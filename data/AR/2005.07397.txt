Submitted to Bernoulli

Contrast estimation of general locally stationary
processes using coupling

JEAN-MARC BARDET 1, PAUL DOUKHAN 2 and OLIVIER WINTENBERGER 3
1University Paris 1 Panthéon-Sorbonne, SAMM, France, E-mail: bardet@univ-paris1.fr
2University Cergy-Pontoise, AGM, France, E-mail: paul.doukhan@u-cergy.fr
3Sorbonne University, LPSM, France, E-mail: olivier.wintenberger@upmc.fr

This paper aims at providing statistical guarantees for a kernel based estimation of time varying parameters driv-
ing the dynamic of very generals classes of local stationary processes. We consider coupling arguments in order
to extend the results of Dahlhaus et al. [13] to the local stationary version of the inﬁnite memory processes in
Doukhan and Wintenberger [21]. The estimators are computed as localized M-estimators of any contrast sat-
isfying appropriate regularity conditions. We prove the uniform consistency and pointwise asymptotic normal-
ity of such kernel based estimators. We apply our results to usual contrasts such as least-square, least absolute
value, or quasi-maximum likelihood contrasts. Various local-stationary processes as ARMA, AR(∞), GARCH,
ARCH(∞), ARMA-GARCH, LARCH(∞), . . . , and integer valued processes are also considered. Numerical
experiments demonstrate the efﬁciency of the estimators on both simulated and real data sets.

MSC 2010 subject classiﬁcations: Primary 62G05; 62G20; secondary 62M05

1. Introduction

Following the seminal paper of Dahlhaus [7] local-stationarity is considered as a natural set of condi-
tions for introducing non-stationarity in times series. The chapter of Dahlhaus [11] yields an exhaustive
survey about new results obtained between 1992 and 2012 on this topics. Dahlhaus and its co-authors
have developed a consistent framework studying deﬁnitions and properties of local stationary time-
varying models (See [14], [12] and [27] for instance) as well as associated statistical issues such as
identiﬁcation and estimation (See [8], [9], [14], [12], [10] and [27]). Note that, except in Dahlhaus and
Subba-Rao [14], the estimators introduced in the previous papers are based on a spectral approxima-
tion of the Gaussian likelihood, i.e. Whittle type estimators. Moreover models considered in this early
literature are linear ﬁlters of independent inputs. More recently a general approach based on derivative
processes has been developed in the important paper of Dahlhaus et al. [13], allowing to get rid off the
linearity condition on the models. Time non-homogeneous Markovian observations are the considered
non-linear time varying models. Under some contraction condition, the observations forget their own
past exponentially fast. Anyway many important models like GARCH-type models are not taken in
consideration in the above considered literature. The reason is that the Markov representation, adjoin-
ing the volatility process, is not observed and, like Hidden Markov Models, do not fall into the setting
considered by Dahlhaus et al. [13]. Indeed such models may be considered as inﬁnite memory pro-
cesses, a general class that also models longer memories than the exponential decaying ones.
The current paper aims at extending the work of Dahlhaus et al. [13] to such an inﬁnite memory setting.
We ﬁrst extend the concept of local stationarity beyond the Markovian case. More precisely, the sta-
tionary models introduced by Doukhan and Wintenberger in [21] are extended to time varying inﬁnite
memory causal processes deﬁned as (X(n)

)1≤t≤n a recursive solution to the equation

t

0
2
0
2

t
c
O
1
2

]
T
S
.
h
t
a
m

[

2
v
7
9
3
7
0
.
5
0
0
2
:
v
i
X
r
a

X(n)

t = F

θ(n)
t

(cid:0)(X(n)

t−k)k∈N∗; ξt

(cid:1),

1 ≤ t ≤ n,

n ∈ N∗ = N \ {0}

1

 
 
 
 
 
 
2

t

)0≤t≤n, n∈N∗ is a deterministic family with θ(n)

where (θ(n)
t ∈ Θ ⊂ Rd for any 0 ≤ t ≤ n and n ∈ N∗,
Fθ is a known real-valued function and the innovations ξt constitute an independent and identically
distributed (i.i.d.) sequence. For ease of writing we will consider X(n)
t = 0 for t ≤ 0, but the arbitrary
choice of any deterministic initial values does not change the asymptotic behavior. In order to make it
tractable, a secondary aim of the paper is to keep the conditions as simple as possible. For instance,
in our setting local-stationarity consists simply in the existence of a function θ∗ such as for u ∈ (0, 1),
θ(n)
t (cid:39) θ∗(u) when t/n (cid:39) u (see the more precise condition (7) in the so-called Assumption (LS(ρ))).
Under this assumption, we deﬁne a kernel based estimator (cid:98)θ(u) of θ∗(u) obtained by the minimization
of a localized sum of contrast Φ (see its deﬁnition in (11)). We then establish the uniform consistency
and the asymptotic normality of this estimator, which is minimax rate optimal, under sharp and general
conditions.
The generality of the setting and the relative simplicity of the conditions allow us to recover existing re-
sults on several classes of examples and extend them to inﬁnite memory processes. Clearly any Markov
process with contractive kernel is an inﬁnite memory model (with memory one if the observations are
Markovian, with exponential decaying memory if there is a hidden state). The inﬁnite memory rep-
resentation for GARCH processes is quite appealing since it holds on the observations whereas the
Markovian representation holds by adjoining the volatility process. Processes with inﬁnite memory
may provide sharp conditions of convergence of estimation procedures in such models. Indeed, as any
contrast is a function of the observations only, the contrast itself has inﬁnite memory (see Bardet and
Wintenberger [5] for a detailed discussion in the stationary setting). We ﬁrst consider least squares and
least absolute values contrasts time-varying LARCH(∞) processes and we notably obtained an efﬁ-
cient asymptotic estimation for these inﬁnite memory chains. Considering quasi log-likelihood contrast
also offers numerous estimation results for time-varying ﬁnite or inﬁnite memory processes: we ob-
tain the uniform consistency and the asymptotic normality for time varying AR(∞) and ARCH(∞).
For ﬁnite memory time varying ARMA(p, q) or GARCH(p, q), our results recover previous ones of
Dahlhaus and co-authors [8], [14] and [13], valid only for ARMA and ARCH models, and extend them
to the important GARCH model class. Dahlhaus et al. [13] used functional dependence conditions, new
coupling arguments developed from Dedecker and Prieur [16] are at the basis of the work, since no
strong mixing condition may be expected and longer memories than exponential decaying ones have
to be tackled. One advantage of our approach is that it goes around the derivative process, a drawback
is that the rates of convergence in the asymptotic normality are sub-optimal. Finally we also apply our
strategy to prove estimation convergence for time varying ARMA-GARCH processes and time varying
integer valued Poisson-GLM type processes.
Numerical studies are also proposed. Firstly, Monte-Carlo experiments show the accuracy of the es-
timator in several cases of time varying processes. However, these simulations also exhibit that such
non-parametric estimate requires sufﬁcient large sample sizes (at least one thousand in many cases).
Secondly, an application to ﬁnancial data (the S&P500 data from October 1990 to October 2020)
demonstrates the evolution of the parameters in case a typical GARCH(1, 1)-model is used.

The forthcoming Section 2 is devoted to the deﬁnition and existence of new non-stationary models. In
Section 3, the deﬁnition of the nonparametric estimator as well as its uniform consistency and asymp-
totic normality are stated, while Section 4 reviews several important cases. Numerical experiments are
proposed in Section 5 and proofs are postponed in Sections 6 and 7.

Contrast estimation of locally stationary processes

3

2. Preliminaries

2.1. Notation

Some standard notation is used:

• The symbol 0 denotes any null vector in any vector space;
• If V is a vector space then V ∞ = {(xn)n∈N ∈ V N; ∃N ∈ N, xk = 0, for all k > N };
• The symbol (cid:107) · (cid:107) denotes the usual Euclidean norm of a vector or the associated norm of a matrix;
• For p ≥ 1 and Z a random vector in Rm, denote: (cid:107)Z(cid:107)p = (cid:2)E((cid:107)Z(cid:107)p)(cid:3)1/p;
• For the measurable vector- or matrix-valued function g deﬁned on some set U , (cid:107)g(cid:107)U =

supu∈U (cid:107)g(u)(cid:107);

• From now on Θ denotes a subset of Rd, and

◦
Θ is the interior of Θ. If V is a Banach space
then C(Θ, V ) denotes the Banach space of V -valued continuous functions on Θ equipped with
the uniform norm (cid:107) · (cid:107)Θ and Lp(C(Θ, V )) (p ≥ 1) denotes the Banach space of random a.e.
continuous functions f such that E(cid:2)(cid:107)f (cid:107)p
Θ

(cid:3) < ∞.
• For θ ∈ Θ and Ψθ : R∞ → V a Borel function with values in a ﬁnite dimensional vector space
θΨθ(x) denotes respectively for k = 0, 1, 2, in case they exist, Ψθ(x), ∂Ψθ(x)/∂θ and

V , ∂k
∂2Ψθ(x)/∂θ2 for x ∈ R∞.

2.2. Stationary inﬁnite memory processes

In all the sequel, we will consider a given real number p ≥ 1.

Set θ ∈ Rd and let a function Fθ be deﬁned as follows

Fθ : (x, y) ∈ R∞ × R (cid:55)→ Fθ(x, y) ∈ R.

(1)

Doukhan and Wintenberger proved in [21] the existence and uniqueness of the stationary solution of
the recurrence equation

Xt = Fθ

(cid:0)(Xt−k)k∈N∗, ξt

(cid:1),

for all t ∈ Z,

(2)

where (Xt) is a process with values in R and where (ξt)t∈Z is a sequence of i.i.d. random variables
(r.v.). This framework provides a parametric representation of models such as nonlinear autoregressive
or conditionally heteroskedastic time series for instance.

The existence of a stationary solution in Lp of the above equation relies on a contraction argument
on the function Fθ. As a consequence, we deﬁne the following family of assumptions (Ak(Θ)) for
k = 0, 1, 2 and some compact subset Θ of Rd:

(Ak(Θ)) For θ ∈ Θ, we assume that the functions ∂k
there exists a sequence (cid:0)b(k)

(Θ)(cid:1)

j

j of nonnegative numbers such that for all x, y ∈ R∞

θFθ exist on R∞ × R for k = 0, 1, 2. Moreover

• Ck(Θ) =

(cid:13)
(cid:13)
(cid:13) sup
θ∈Θ

(cid:13)
(cid:13)∂k

(cid:13)
θFθ(0, ξ0)(cid:13)
(cid:13)
(cid:13)
(cid:13)p

< ∞

(3)

4

•

(cid:13)
(cid:13)
(cid:13) sup
θ∈Θ

(cid:13)
(cid:13)∂k

θFθ(x, ξ0) − ∂k

(cid:13)
θFθ(y, ξ0)(cid:13)
(cid:13)
(cid:13)
(cid:13)p

≤

∞
(cid:88)

j=1

b(k)
j

(Θ) (cid:107)xj − yj(cid:107)p,

(4)

Thus, from [21], under the uniform contraction conditions (A0(Θ)) with B0(Θ) < 1, there exists a
unique stationary solution of (2) in Lp (deﬁned almost surely).

with Bk(Θ) =

∞
(cid:88)

j=1

b(k)
j

(Θ) < ∞.

2.3. Local stationary inﬁnite memory process

If we replace now θ by the time-varying θ(n)
t ∈ Θ, then the uniform contraction con-
ditions (A0(Θ)) with B0(Θ) < 1 ensure the existence of a non-stationary Lp-process. More precisely,
we deﬁne the triangular array (X(n)

such that θ(n)

)1≤t≤n, n∈N∗ such as:

t

t

X(n)

t = F

θ(n)
t

(cid:0)(X(n)

t−k)k∈N∗; ξt

(cid:1),

1 ≤ t ≤ n, n ∈ N∗ ,

(5)

)0≤t≤n, n∈N∗ is a family of real numbers θ(n)
where (θ(n)
In order to make this recursion possible we also set initial conditions

t

t ∈ Θ ⊂ Rd for any 0 ≤ t ≤ n and n ∈ N∗.

X(n)

t = 0,

for t ≤ 0.

(6)

The solution of the above equations is no longer stationary. However, we can establish the following
result (its proof as well as all the other ones are postponed in the Sections 6 and 7):

Lemma 2.1. Let Θ ⊂ Rd such that (A0(Θ)) holds with B0(Θ) < 1. Then, under the assumption (6),
the nonstationary triangular array (X(n)
)0≤t≤n, n∈N∗, solution of (5), remains in Lp and it satisﬁes

t

sup
n∈N∗, 0≤s≤n

(cid:107)X(n)

s (cid:107)p ≤

C0(Θ)
1 − B0(Θ)

.

2.4. The stationary version

We introduce a function u (cid:55)→ θ∗(u) on [0, 1]; this is a continuous time approximation of the trian-
gular array of parameters (θ(n)
)0≤t≤n, n∈N∗ . We consider ρ ∈ (0, 1] and assume the following local-
stationarity assumption:

t

Assumption (LS(ρ)): There exist Kθ > 0 and a continuous function θ∗ : u ∈ [0, 1] (cid:55)→ θ∗(u) ∈ Rd,
such as

(cid:13)
(cid:13)θ(n)

t − θ∗(u)(cid:13)

(cid:13) ≤ Kθ

(cid:12)
(cid:12)
(cid:12)u −

(cid:12)
ρ
(cid:12)
(cid:12)

,

t
n

for any n ∈ N∗ and 1 ≤ t ≤ n.

(7)

This condition describes an Hölder type behavior for the approximation of θ(n)
When t/n −→

θ∗(u) and the behavior of (X(n)

by the function θ∗.
) is similar to its so-called sta-

u then θ(n)

t

t

t

−→
n→+∞

n→+∞

tionary version.

Contrast estimation of locally stationary processes

Deﬁnition 2.1.

If it exists, we deﬁne ( (cid:101)Xt(u))t∈Z as any solution of the recursion

(cid:101)Xt(u) = Fθ∗(u)

(cid:0)( (cid:101)Xt−k(u))k≥1, ξt

(cid:1),

t ∈ Z ,

and we call it the stationary version of (X(n)

t

) at u ∈ [0, 1].

5

(8)

Note that from [21] the existence of the stationary version is implied by contraction assumptions.
Namely, if the function θ∗ satisﬁes θ∗(u) ∈ Θ ⊂ Rd for each u ∈ [0, 1] and is such that (A0(Θ)) holds
with B0(Θ) < 1, then there exists a.s. a unique stationary stationary version ( (cid:101)Xt(u))t∈Z satisfying (8)
and

(cid:107) (cid:101)Xt(u)(cid:107)p ≤

sup
t∈Z

C0(Θ)
1 − B0(Θ)

,

u ∈ [0, 1] .

3. M-estimation for inﬁnite memory processes

3.1. The stationary case

We recall the framework of contrast estimation for inﬁnite memory chain as in Bardet and Wintenberger
[5]. Let (Xt) be a stationary solution of the inﬁnite memory model (2) with parameter θ∗ ∈ Θ ⊂ Rd
such that (A0(Θ)) holds with B0(Θ) < 1. We estimate θ∗ using an M-estimator based on an observed
path (X1, . . . , Xn).

We deﬁne a contrast function Φ(x, θ) that satisﬁes a set of regularity assumptions combined in the
deﬁnition of the space Lip p(Θ) for Θ ⊂ Rd (always with 1 ≤ p):

Space Lip p(Θ): A Borel function h : R∞ × Θ → R belongs to Lip p(Θ) if there exist a sequence
of non-negative numbers (αi(h, Θ))i∈N where (cid:80)∞
s=1 αs(h, Θ) < ∞ and a function g : [0, ∞)2 →
[0, ∞) such as for any sequences U = (Ui)i∈N∗ ∈ (Lp)∞ and V = (Vi)i∈N∗ ∈ (Lp)∞ satisfying
sups≥1{(cid:107)Us(cid:107)p ∨ (cid:107)Vs(cid:107)p} < ∞, one obtains:






E(cid:2) supθ∈Θ
E(cid:2) sup
θ∈Θ

(cid:12)h(0, θ)(cid:12)
(cid:12)
(cid:3) < ∞;
(cid:12)
(cid:12)h(U, θ) − h(V, θ)(cid:12)
(cid:12)
(cid:12)

(cid:3) ≤ g(cid:0) sup
s≥1

(cid:8)(cid:107)Us(cid:107)p ∨ (cid:107)Vs(cid:107)p

(cid:9)(cid:1)

∞
(cid:88)

s=1

αs(h, Θ) (cid:107)Us − Vs(cid:107)p.

(9)

Note that if h ∈ Lip p(cid:48) (Θ) then h ∈ Lip p(Θ) when p ≤ p(cid:48) thanks to Jensen’s inequality. It is possi-
ble (see below the general case for non-stationary models) to prove that if Φ ∈ Lip p(Θ) and if the
stationary solution (Xt) admits ﬁnite p moments, then Φ(cid:0)(X−t)t∈N, θ(cid:1) exists in L1 for any θ ∈ Θ.
The existence of ﬁrst order moments is crucial for ensuring that Φ is a proper score function which is
implied by the following condition:

Assumption (Co(Φ, Θ)): The function Φ ∈ Lip p(Θ) for p ≥ 1 is such that for (Xt)t∈Z satisfying
◦
Θ and with F0 = σ(cid:0)(X−k)k∈N
the inﬁnite memory model (2) with parameter θ∗ ∈

(cid:1),

θ∗ is the unique minimum of the function θ ∈ Θ (cid:55)→ E(cid:2)Φ(cid:0)(X1−k)k∈N, θ(cid:1)|F0

(cid:3) in

◦
Θ .

(10)

6

As a consequence, this condition is depending on the function θ ∈ Θ (cid:55)→ Fθ(·) driving the inﬁnite
memory model (2). Therefore the contrast function Φ will be chosen with respect to this function Fθ.
This is thus natural to deﬁne the M-estimator of θ by

(cid:98)θn = Argmin
θ∈Θ

1
n

n
(cid:88)

t=1

Φ(cid:0)(Xt−i)i∈N, θ(cid:1).

Note that the factor 1/n aims at establishing a Law of Large Numbers. Indeed, if the almost sure
convergence holds, i.e.

(cid:12)
(cid:12)
(cid:12)

1
n

sup
θ∈Θ

n
(cid:88)

t=1

Φ((Xt−i)i∈N, θ) − E(cid:2)Φ((X−i)i∈N, θ)(cid:3)(cid:12)
(cid:12)
(cid:12)

a.s.
−→
n→+∞

0,

then usual arguments imply (cid:98)θn

a.s.
−→
n→+∞

θ∗.

3.2. The local stationary case

We extend the notion of contrast function Φ to the non-stationary process
parameters θ(n)

. The ﬁrst step below is to prove the integrability.

t

(cid:17)

(cid:16)

X(n)
t

t∈Z

for time-varying

t

Lemma 3.1. Let (X(n)
)t∈Z satisfy the non-stationary inﬁnite memory model (5) under condition
(A0(Θ)) with B0(Θ) < 1 and let Φ ∈ Lip p(Θ) with p ≥ 1. Then for any θ ∈ Θ, the sequence of con-
trasts (cid:0)Φ(cid:0)(X(n)
t∈Z exists in L1. Moreover, under Assumption (LS(ρ)), and with ( (cid:101)Xt(u))t∈Z
the stationary version deﬁned in (8), (cid:0)Φ(cid:0)( (cid:101)Xt−k(u))k∈N, θ(cid:1)(cid:1)

t∈Z is a stationary ergodic process.

t−k)k∈N, θ(cid:1)(cid:1)

Under the local stationary assumption (LS(ρ)), we can expect estimating θ∗(u) with 0 < u < 1 deﬁned
in (7) thanks to a M-estimator based on the observations X(n)
for t/n (cid:39) u. The previous M-estimator
has to be localized around t such as t (cid:39) nu using a convolution kernel K with a compact support (for
simplicity):

t

Deﬁnition 3.1. Let a kernel function K : R → R be such as:

• K has a compact support, i.e. there exists c > 0 such as K(x) = 0 for |x| ≥ c;
• K : R → R is piecewise differentiable with (cid:82)

R K(x)dx = 1, CK = supx∈R |K(x)| < ∞.

Then, with a bandwidth sequence (hn)n∈N of positive numbers, we deﬁne the kernel based estimator
of θ∗(u) as

(cid:98)θ(u) = argmin
θ∈Θ

1
nhn

n
(cid:88)

j=1

Φ(cid:0)(X(n)

j−i)i∈N, θ(cid:1) K

(cid:16) j

n − u
hn

(cid:17)

,

u ∈ (0, 1).

(11)

Under weak conditions, this estimator is uniformly consistent:

Contrast estimation of locally stationary processes

7

Theorem 3.1. Let (X(n)
satisﬁes Assumption (A0(Θ)) with B0(Θ) < 1 and (cid:80)∞
with also Assumption (LS(ρ)). If for p ≥ 1, Φ ∈ Lip p(Θ) with (cid:80)
sumption (Co(Φ, Θ)) then, for any u ∈ (0, 1), (cid:98)θ(u) consistently estimates θ∗(u):

)t∈N be the solution of the non-stationary inﬁnite memory model (5) which
t=2 t log(t)bt(Θ) < ∞ and Assumption (A1(Θ)),
s≥0 s αs(Φ, Θ) < ∞ satisfying As-

t

(cid:98)θ(u)

P
−→
n→+∞

θ∗(u),

if hn −→

n→+∞

0

and nhn −→

n→+∞

∞.

Moreover, if p > 1 and n1−1/phn −→
n→+∞
θ∗:

∞ then for any ε > 0 then (cid:98)θ uniformly consistently estimates

sup
u∈[ε,1−ε]

(cid:13)(cid:98)θ(u) − θ∗(u)(cid:13)
(cid:13)
(cid:13)

P
−→
n→+∞

0 .

(12)

Remark 3.1. We notice that the uniform consistency of the kernel estimator was already obtained for
Markov processes by Dahlhaus et al. in [13] under a different set of assumptions. Our extra condition
on the bandwidth n1−1/phn → ∞, n → ∞, corresponds to the extra condition in (ii) of Theorem 5.2
of [13] with M = 0. We notice also that our extra condition implicitly requires that p > 1.

Remark 3.2. Consistency of (cid:98)θ(u) for any u ∈ (0, 1) can be achieved relaxing the uniform contraction
condition B0({Θ) < 1 of Theorem 3.1. Instead assume the pointwise contraction

B0({θ∗(u)}) < 1 ,

u ∈ (0, 1) .

(13)

Then a continuity argument under (A0(Θ)) yeilds for any u ∈ (0, 1) the existence of ε > 0 such that
B0({θ∈Θ : (cid:107)θ − θ∗(u)(cid:107) ≤ ε}) < 1 .

The M-estimator deﬁned in (11) with {θ ∈ Θ : (cid:107)θ − θ∗(u)(cid:107) ≤ ε} in place of Θ consistently estimates
θ∗(u) by an application of Theorem 3.1. That this M-estimator coincides with (cid:98)θ(u) for n sufﬁciently
large follows by a compacity argument of Pfanzagl [28]; The compact set Θ is covered by ﬁnitely many
compact sets of diameter ε among which only one contains θ∗(u). Applying the SLLN of Lemma 7.1
1., the uniqueness in Condition (Co(Φ, Θ)) implies that the minimizer of (11) belongs to the unique
compact set containing θ∗(u) for n large enough. See [5] for details. Note that we were not able to
extend the uniform consistency of (cid:98)θ under pointwise contraction, which is consistent with [13] that
worked under uniform contraction as well. Uniform contraction seems neceassary for proving uniform
moments in Lemma 6.1.

For establishing the asymptotic normality of (cid:98)θ(u), we analogously need extra assumptions on the
differentiability of the contrast Φ and the integrability of its derivatives. We have:

Theorem 3.2. Let the assumptions of Theorem 3.1 hold with Θ a compact set. Assume also that for
any x ∈ R∞, θ ∈ Θ (cid:55)→ Φ(x, θ) is a C2(Θ) function such as ∂θΦ ∈ Lip p(Θ) with (cid:80)∞
s=1 s αs(∂θΦ, Θ) <
∞, and for any u ∈ (0, 1), with ρ ∈ (0, 1] deﬁned in Assumption (LS(ρ)),

• E(cid:2)(cid:13)

(cid:13)∂θΦ(cid:0)( (cid:101)Xk(u))k≤0, θ∗(u)(cid:1)(cid:13)
2(cid:3) < ∞ and Σ(cid:0)θ∗(u)(cid:1) is a deﬁnite positive matrix with
(cid:13)
∂θΦ(cid:0)( (cid:101)X−k(u))k∈N, θ∗(u)(cid:1) (cid:0)∂θΦ(cid:0)( (cid:101)X−k(u))k∈N, θ∗(u)(cid:1)(cid:1)(cid:62)(cid:105)
;

(cid:104)
K2(x)dx · E

Σ(cid:0)θ∗(u)(cid:1) =

(cid:90)

R

8

• Γ(θ∗(u)) = E(cid:2)∂2

θ2Φ(cid:0)( (cid:101)X−k(u))k∈N, θ∗(u)(cid:1)(cid:3) is a positive deﬁnite matrix.

If (hn)n is a sequence of positive numbers such that

nhn −→

n→+∞

∞ and nh1+2ρ

n

−→
n→+∞

0,

then, for any u ∈ (0, 1),

(cid:112)

nhn

(cid:0)

(cid:98)θ(u) − θ∗(u)(cid:1)

(cid:16)

L
−→
n→+∞

Nd

0 , Γ−1(θ∗(u))Σ(cid:0)θ∗(u)(cid:1)Γ−1(θ∗(u))

(cid:17)

.

(14)

(15)

Remark 3.3. A ﬁrst consequence of this result is that the convergence rate of (cid:98)θ(u) is o(n−ρ/(2ρ+1)),
which is just below the classical minimax convergence rate in a non parametric framework for any ρ ∈
(0, 1]. Then the optimal choice of the bandwidth satisﬁes hn = o(n−1/(2ρ+1)) and the estimator is also
uniformly consistent whenever p > (2ρ + 1)/(2ρ). Under additional conditions, Rosenblatt [31] and
Dahlhaus et al. [13] derive expressions for an equivalent of the bias; in this case i.e. nh1+2ρ
(cid:96) (cid:54)=
0, one may use the classical minimax bandwidth and, then, a non-centred Gaussian limit theorem
occurs.

−→
n→+∞

n

Remark 3.4. A precise inspection of the proof of Theorem 3.2 shows that only the consistency of
(cid:98)θ(u), u ∈ (0, 1), is used and not the uniform consistency of (cid:98)θ. Thus the asymptotic normality (15) can
be extending under the pointwise contraction (13).

Remark 3.5. Considering (u1, . . . , um) instead of u, a multidimensional central limit theorem could
also be obtained extending (15). Such a result could be interesting for testing the goodness-of-ﬁt
(H0 : θ∗ = θ0) or the stationarity (H0 : θ∗ = C0 ∈ Rd) of the process. This will be the subject of
a forthcoming paper.

4. Examples

Here we develop several examples of locally stationary inﬁnite memory models with contrast func-
tions Φ ∈ Lip p(Θ) for which the Assumption (Co(Φ, Θ)) is satisﬁed. We also check the conditions of
Theorems 3.1 and 3.2 in order to assert the uniform consistency and the asymptotic normality of the
localized M-estimator.

4.1. Time varying AR(1) processes

In the case of time varying AR(1) (denoted further as tvAR(1)) processes deﬁned by

X(n)

t = θ(n)

t X(n)

t−1 + ξt,

for 1 ≤ t ≤ n, n ∈ N∗,

(16)

with X(n)

t = 0 for any t ≤ 0, and Θ = [−r, r] with 0 < r < 1.

Contrast estimation of locally stationary processes

4.1.1. Least Square contrast

When ΦLS is the Least Square (LS) contrast deﬁned as

ΦLS(x, θ) = (x1 − θ x2)2 ,

9

(17)

we obtain the usual Yule-Walker (or Least Square) estimator of θ∗(u) if the stationary version ( (cid:101)Xt(u))
were observed. Clearly Assumption (Co(Φ, Θ)) holds and using Hölder Inequality, we obtain than
ΦLS ∈ Lip p(Θ) with p = 2,

E(cid:2)| sup
θ∈Θ

(cid:12)ΦLS(U, θ) − ΦLS(V, θ)(cid:12)
(cid:12)
(cid:12)

(cid:3) ≤ (1+r) max
1≤s≤2

(cid:13)Ui(cid:107)2 ∨ (cid:13)
(cid:8)(cid:13)

(cid:13)Vi(cid:107)2}(cid:0)(cid:107)U1−V1(cid:107)2 + r(cid:107)U2−V2(cid:107)2

(cid:1),

and therefore α1(ΦLS, Θ) = 1, α2(ΦLS, Θ) = r and αj(ΦLS, Θ) = 0 for j ≥ 3. From basic calcula-
tion we also have

∂θΦLS(x, θ) = 2x2(θx2 − x1)

and ∂2

θ2ΦLS(x, θ) = 2x2
2.

After elementary algebra, we obtain:

(cid:104)
E

sup
θ∈[−r,r]

(cid:12)∂θΦLS(U, θ) − ∂θΦLS(U, θ)(cid:12)
(cid:12)
(cid:12)

(cid:105)

≤ 4 (cid:0)(cid:107)U1 − V1(cid:107)2 + (cid:107)U2 − V2(cid:107)2

(cid:1) (cid:0)(cid:107)U1(cid:107)2 + (cid:107)V1(cid:107)2 + (cid:107)U2(cid:107)2 + (cid:107)V2(cid:107)2

(cid:1)

from Hölder’s inequality. Analogously,

(cid:104)
E

sup
θ∈[−r,r]

(cid:12)
(cid:12)∂2

θ2ΦLS(U, θ) − ∂2

θ2ΦLS(V, θ)(cid:12)
(cid:12)

(cid:105)

≤ 2 (cid:0)(cid:107)U2 − V2(cid:107)2

(cid:1) (cid:0)(cid:107)U2(cid:107)2 + (cid:107)V2(cid:107)2

(cid:1),

ensuring that ∂θΦLS and ∂2
θ2ΦLS are included in Lip 2
(cid:17)
(cid:16) (cid:82)
Σ(cid:0)θ∗(u)(cid:1) = 4
R K2(x)dx
inite. Then, by an application of Theorem 3.2 we obtain:

σ4
ξ (1 − θ∗(u)2)−1 and Γ(θ∗(u)) = 2σ2

(cid:0)[−r, r](cid:1). If E[ξ2

0] < ∞, both the matrix
ξ (1 − θ∗(u)2)−1 are positive def-

If E[ξ2

0] < ∞ and if θ(n)

t ∈ Θ = [−r, r] satisﬁes Assumption (LS(ρ)), the localized
Corollary 4.1.
least square estimator is asymptotically normal when the sequence (hn)n satisﬁes (14) and we obtain
for any u ∈ (0, 1)

(cid:112)

n hn

(cid:0)

(cid:98)θ(u) − θ∗(u)(cid:1)

(cid:16)

L
−→
n→+∞

N

0 , (cid:0)1 − θ∗(u)2(cid:1)

(cid:17)
K2(x)dx

.

(cid:90)

R

Here, we recover for 0 < ρ ≤ 1 the results on tvAR(1) models obtained by Bardet and Doukhan in [3],
which are also valid for functions u ∈ (0, 1) → (cid:98)θ(u) with Hölderian derivatives.

4.1.2. Least Absolute Value contrast

In the framework of tvAR(1) processes (16) a classical alternative of the LS contrast, known for its
robustness, is the Least Absolute Values (LAV) contrast deﬁned as follows on θ ∈ Θ = [−r, r] with
0 < r < 1,

ΦLAV (x, θ) = (cid:12)

(cid:12)x1 − θ x2

(cid:12)
(cid:12).

(18)

10

If the stationary version ( (cid:101)Xt(u)) were observed, we obtain the usual estimator of θ and Assumption
(Co(ΦLAV , Θ)) holds. In such a case, ΦLAV ∈ Lip p(Θ) for any 1 ≤ p, and we obtain
(cid:3) ≤ (cid:107)U1 − V1(cid:107)p + r (cid:107)U2 − V2(cid:107)p.

(cid:12)ΦLAV (U, θ) − ΦLAV (V, θ)(cid:12)
(cid:12)
(cid:12)

E(cid:2) sup
θ∈Θ

implying α1(ΦLAV , Θ) = 1 and α2(ΦLAV , Θ) = r and αj(ΦLAV , Θ) = 0 for j ≥ 3. Since ΦLAV is
not a differentiable function, we will restrict our purpose to the uniform consistency of (cid:98)θ(u). We obtain
the following result from Theorem 3.1:

Corollary 4.2.
calized LAV estimators (cid:98)θn satisﬁes (12).

If (cid:107)ξ0(cid:107)1 < ∞ and if (θ(n)

t

) ∈ Θ = [−r, r] satisﬁes Assumption (LS(ρ)), then the lo-

4.2. Causal afﬁne processes and Gaussian QMLE

We consider the general class of causal afﬁne processes (Xt) deﬁned by Bardet and Wintenberger in
[5] as

Xt = Mθ

(cid:0)(Xt−i)i≥1
with Θ a compact subset of Rd. We assume the existence of Lipschitz coefﬁcient sequences
(cid:0)βi(f, Θ)(cid:1)

for any t ∈ Z, θ ∈ Θ

i∈N such as for Kθ = fθ or Mθ,

i∈N and (cid:0)βi(M, Θ)(cid:1)

(cid:0)(Xt−i)i≥1

(cid:1) ξt + fθ

(19)

(cid:1),

(cid:12)Kθ(x) − Kθ(y)(cid:12)
(cid:12)

(cid:12) ≤

sup
θ∈Θ

∞
(cid:88)

i=1

βi(K, Θ) (cid:12)

(cid:12)xi − yi

(cid:12)
(cid:12),

(20)

for any x, y ∈ R∞. Then, (Xt) satisﬁes the inﬁnite memory model (2) with Fθ(x, ξ0) = fθ(x) +
ξ0 Mθ(x) and (A0(Θ)) holds when (cid:80)
j βj(f, Θ) < ∞ and (cid:80)

j βj(M, Θ) < ∞ since we have

b(0)
j

(Θ) ≤ βj(f, Θ) + (cid:107)ξ0(cid:107)p βj(M, Θ)

for any j ∈ N∗.

Therefore, (Xt) is a stationary and Lp solution of the causal afﬁne model (19) when

∞
(cid:88)

j=1

βj(f, Θ) + (cid:107)ξ0(cid:107)p βj(M, Θ) < 1.

(21)

(22)

In such a case this is interesting to consider Φ as (−2) times the Gaussian conditional log-density,
inducing

ΦG(x, θ) = log (cid:0)M 2
θ

(cid:0)(xi)i≥2

(cid:1)(cid:1) +

(cid:0)x1 − fθ
M 2
θ

(cid:0)(xi)i≥2
(cid:1)
(cid:0)(xi)i≥2

(cid:1)(cid:1)2

.

(23)

The M-estimator resulting from this contrast is the Gaussian Quasi-Maximum Likelihood estima-
tor (QMLE), notably used for estimating the parameters of GARCH processes, but also for ARMA,
APARCH, ARMA-GARCH processes. . .
As this was already done in [5] (proof of Theorem 1), under identiﬁability conditions on fθ and Mθ
(see below), Assumption (Co(ΦG, Θ)) holds. Moreover, using Bardet et al. [4] (proof of Lemma 6.3),

Contrast estimation of locally stationary processes

11

we obtain with p = 3 assuming the existence of M > 0 such as Mθ ≥ M and with C a constant real
number

(cid:12)ΦG(U, θ) − ΦG(V, θ)(cid:12)
(cid:12)

(cid:12) ≤

sup
θ∈Θ

C (cid:0)1 + |U1|2 + |V1|2 + f 2
θ
(cid:1) − fθ

(cid:0)(Ui)i≥2

|U1 − V1| + (cid:12)

(cid:12)fθ

(cid:0)(Ui)i≥2
(cid:0)(Vi)i≥2

(cid:1) + f 2
θ
(cid:12) + (cid:12)
(cid:1)(cid:12)

(cid:16)

×

(cid:0)(Vi)i≥2

(cid:1)(cid:1)

(cid:0)(Ui)i≥2

(cid:1) − Mθ

(cid:0)(Vi)i≥2

(cid:1)(cid:12)
(cid:12)

(cid:17)

(cid:12)Mθ

hence, there exists a function g such that

E(cid:2) sup
θ∈Θ

(cid:12)ΦG(U, θ) − ΦG(V, θ)(cid:12)
(cid:12)
(cid:12)

(cid:3)

≤ g(cid:0) sup
i≥1

(cid:13)Ui(cid:107)3 ∨ (cid:13)
(cid:8)(cid:13)

(cid:13)Vi(cid:107)3}(cid:1) (cid:16)

(cid:107)U1 − V1(cid:107)3 +

∞
(cid:88)

i=2

b(0)
k (Θ) (cid:107)Ui − Vi(cid:107)3

(cid:17)

,

(24)

using Hölder inequality and with b(0)
the function Fθ given in (A0(Θ)). Therefore, according to (9) with αk(ΦG, Θ) = b(0)
α1(ΦG, Θ) = 1, we check that ΦG ∈ Lip 3(Θ) since (A0(Θ)) holds and B0(Θ) = (cid:80)
Now we consider a time varying causal afﬁne processes, that is the local stationary extension of causal
afﬁne processes deﬁned in (19), i.e.

k (Θ) = βk(fθ, Θ)+(cid:107)ξ0(cid:107)p βk(Mθ, Θ) the Lipschitz coefﬁcients of
k (Θ) for k ≥ 2 and

k (Θ) < ∞.

k b(0)

X(n)

t = M

θ(n)
t

(cid:0)(X(n)

t−i)1≤i

(cid:1) ξt + f

θ(n)
t

(cid:0)(X(n)

t−i)1≤i

(cid:1),

for any t ∈ Z,

(25)

with θ(n)

t ∈ Θ a compact set of Rd and X(n)

t = 0 for t ≤ 0.

In the sequel, we will provide general sufﬁcient conditions for the asymptotic normality of (cid:98)θ(u) in
terms of the functions fθ and Mθ and of their derivatives.

Proposition 4.1. Let (X(n)
Lipschitz inequalities (20) and under Assumption (LS(ρ)). Assume also:

) satisfy (25) where fθ, Mθ, ∂θfθ, ∂θMθ, ∂2

t

θ2fθ and ∂2

θ2Mθ satisfy

1. (cid:107)ξ0(cid:107)4 < ∞ where the probability distribution of ξ0 is absolutely continuous with respect to
(cid:0)βj(fθ, {θ}) +

the Lebesgue measure and Θ is a bounded set included in (cid:8)θ ∈ Rd, (cid:80)∞
j=1
(cid:107)ξ0(cid:107)4 βj(Mθ, {θ})(cid:1) < 1(cid:9);

2. there exists M > 0 such as Mθ ≥ M for any θ ∈ Θ;
3. For all θ, θ(cid:48) ∈ Θ,

(cid:0)fθ = fθ(cid:48)

and Mθ = Mθ(cid:48)

(cid:1) =⇒ θ = θ(cid:48);

(26)

4. We have

(cid:16) d
(cid:88)

j=1

µj

∂
∂θj

fθ∗(u)

(cid:0)( (cid:101)X−k(u))k∈N

(cid:1) = 0 a.s. =⇒ µj = 0, j = 1, . . . , d

(cid:17)

,

12

or

(cid:16) d
(cid:88)

j=1

µj

∂
∂θj

Mθ∗(u)

(cid:0)( (cid:101)X−k(u))k∈N

(cid:1) = 0 a.s. =⇒ µj = 0, j = 1, . . . , d

(cid:17)

.

(27)

Consider Φ = ΦG as (−2) times the Gaussian conditional log-density (23). Then, if

∞
(cid:88)

j=1

j (cid:0)log j (cid:0)βj(f, Θ) + βj(M, Θ)(cid:1) + j (cid:0)βj(∂θf, Θ)(cid:1)

∞
(cid:88)

(cid:16)

+

j=1

βj(∂θM, Θ)(cid:1) + βj(∂2

θ2 f, Θ) + βj(∂2

θ2 M, Θ)

(cid:17)

< ∞,

(28)

with βj(·, Θ) deﬁned in (20), the localized QMLE (cid:98)θ(u) satisﬁes the central limit theorem (15).

Note that the QML contrast Φ depends on fθ and Mθ. This explains why the asymptotic normality
can be obtained from conditions on fθ and Mθ and their derivatives. Note also that the conditions
required in Proposition 4.1 are essentially the same than those requested in Theorem 2 of [5] in the sta-
tionary framework. The asymptotic normality of the (localized) QMLE holds under natural conditions,
nhn in the
the main difference here is the convergence rates which is
non-stationary one, which follows from localisation. The minimax rate is o(n1/3) is obtained for ρ = 1
for local stationary causal afﬁne models; it is smaller than the usual parametric rate O(
n) achieved
by the QMLE in the stationary case.

n in the stationary case but

√

√

√

In the sequel we will state with details the assumptions for three important speciﬁc models, tvAR(∞),
tvARCH(∞) and tvARMA-GARCH models.

4.2.1. Time varying AR(∞) and time varying ARMA(p, q) processes

In such the case of time varying AR(∞) (or tvAR(∞)) or time varying (invertible) ARMA(p, q) pro-
cesses, we have Mθ = σ(θ) > σ > 0 and fθ((xi)i≥1) = (cid:80)∞
j=1 aj(θ) xj, where (aj(θ))j≥1 is a se-
quence of real numbers, implying

X(n)

t = σ(θ(n)

t

) ξt +

∞
(cid:88)

j=1

aj(θ(n)
t

) X(n)
t−j,

for 1 ≤ t ≤ n, n ∈ N∗,

(29)

with X(n)
t = 0 for any t ≤ 0. Thus, the Lipschitz coefﬁcients satisfy βj(f, Θ) = supθ∈Θ |aj(θ)| and
βj(M, Θ) = 0. Then we obtain the asymptotic normality of (cid:98)θ(u) from primitive conditions on func-
tions aj and σ by an application of Proposition 4.1:

t

Corollary 4.3. Let (X(n)
) be a tvAR(∞) process deﬁned in (29). If (cid:107)ξ0(cid:107)4 < ∞, let Θ be a bounded
subset of Rd included in (cid:8)θ ∈ Rd, (cid:80)∞
j=1 aj(θ) < 1(cid:9). If for each j ∈ N∗ the functions θ ∈ Θ (cid:55)→
aj(θ) ∈ R and θ ∈ Θ (cid:55)→ σ(θ) ∈ [σ, ∞) are C2(Θ) functions such as (cid:0)aj(θ) = aj(θ(cid:48)), ∀j ∈ N∗ and
σ(θ) = σ(θ(cid:48))(cid:1) imply (θ = θ(cid:48)), if θ(n)

satisﬁes the assumption of local stationarity (LS(ρ)), and if

t

∞
(cid:88)

j=1

j log j sup
θ∈Θ

(cid:12)
(cid:12)aj(θ)(cid:12)

(cid:12) + j sup
θ∈Θ

(cid:12)
(cid:12)∂θaj(θ)(cid:12)

(cid:12) + sup
θ∈Θ

(cid:12)
θ2aj(θ)(cid:12)
(cid:12)∂2

(cid:12) < ∞,

Contrast estimation of locally stationary processes

13

then the central limit (15) holds for any u ∈ (0, 1) under condition (14).

This result is new, essentially because it deals with two difﬁculties: an inﬁnite memory and also
with a non exponential decrease memory. As an illustrative example consider θ = (µ, κ, σ)(cid:48) and
aj(θ) = µ j−κ for any j ≥ 1, with κ ≥ κ > 2, µ ≤ ((cid:80)∞
j=1 j−κ)−1 and σ ≥ σ > 0. Then the previ-
ous corollary implies the asymptotic normality (15) of (cid:0)
(cid:98)µ(u), (cid:98)κ(u), (cid:98)σ(u)(cid:1) under condition (14) when
(cid:0)µ(n)
t

(cid:1) satisﬁes Assumption (LS(ρ)).

, σ(n)
t

, κ(n)
t

An important subclass of tvAR(∞) models is the one of an invertible tvARMA(p, q) models deﬁned
as

X(n)

1,t X(n)

t + φ(n)
(for 1 ≤ t ≤ n, n ∈ N∗), with X(n)
set of parameters θ(n)
d = p + q + 1 deﬁned by:

t = (cid:0)φ(n)

t−1 + · · · + φ(n)

p,t X(n)

t−p = σ(n)

t

ξt + ψ(n)

1,t ξt−1 + · · · + ψ(n)

q,t ξt−q

(30)

t = 0 for any t ≤ 0, as it was introduced in [7]. We consider the
ARM A of Rd with

(cid:1)(cid:48) and the subset Θ(p,q)

1,t , . . . , ψ(n)

p,t , ψ(n)

q,t , σ(n)

1,t , . . . , φ(n)

t

Θ(p,q)

ARM A =

(cid:110)
(φ1, . . . , φp, ψ1, . . . , ψq, σ) ∈ Rp+q+1,

(cid:111)
1 + φ1 z + · · · + φp zp (cid:54)= 0 and 1 + ψ1 z + · · · + ψq zq (cid:54)= 0 for all |z| ≤ 1
.

t ∈ Θ for any 1 ≤ t ≤ n, n ∈ N∗ and Θ a compact subset of Θ(p,q)

Then if θ(n)
t (cid:107)p <
∞ for any p ≥ 1 when (cid:107)ξ0(cid:107)p < ∞ since X(n)
can be written as a tvAR(∞) process (29) with ﬁnite
sum of absolute values of coefﬁcients. Moreover, from classical analytic arguments it is well known
that the corresponding Lipschitz coefﬁcients βj(f, Θ), βj(∂θf, Θ) and βj(∂2
θ2f, Θ) decrease expo-
nentially fast so that the condition (28) is automatically satisﬁed.

ARM A, then supn,t (cid:107)X(n)

t

As a consequence of Proposition 4.1 we obtain:

If (X(n)
t

Corollary 4.4.
Θ(p,q)
ARM A, if (cid:107)ξ0(cid:107)4 < ∞ and θ(n)
t
plied by the local stationarity (LS(ρ)) of all functions φ(n)
central limit (15) holds for any u ∈ (0, 1) under condition (14).

) is a tvARMA(p, q) process deﬁned in (30), Θ is a bounded subset of

satisﬁes the assumption of local stationarity (LS(ρ)), which is im-

1,t , . . . , φ(n)

p,t , ψ(n)

1,t , . . . , ψ(n)

q,t , σ(n)

t

, then the

This result is a QMLE version of the results obtained by Dahlhaus in [9] for Gaussian tvARMA pro-
cesses (using Whittle likelihood approximation) and by Azrak and Mélard in [1]; we use quasi likeli-
hood contrasts similarly as those authors.

4.2.2. Time varying ARCH(∞) and time varying GARCH(p, q) processes

Time varying ARCH(∞) (tvARCH(∞)) or time varying GARCH(p, q) (tvGARCH(p, q)) processes
(cid:1)1/2, where (aj(θ))j≥0
correspond to fθ((xi)i≥1) = 0 and Mθ((xi)i≥1) = (cid:0)a0(θ) + (cid:80)∞
is a sequence of non negative real numbers and a0(·) ≥ a > 0 implying

j=1 aj(θ) x2
j

X(n)

t = ξt

(cid:16)

a0(θ(n)
t

) +

ai(θ(n)
t

)(cid:0)X(n)
t−i

(cid:1)2(cid:17)1/2

(cid:88)

i≥1

for 1 ≤ t ≤ n, n ∈ N∗,

(31)

14

t = 0 for any t ≤ 0, θ(n)

with X(n)
t ∈ Rd for any 1 ≤ t ≤ n, n ∈ N∗ satisfying Assumption (LS(ρ)).
For more details about stationary ARCH(∞) or GARCH(p, q) processes, or for the transition from
GARCH(p, q) to ARCH(∞), see [22]. We are going to specify again the conditions of Proposition 4.1
in such a case. Firstly, we consider Lipschitz properties on (cid:0)(X(n)
)t as in [5] in
order to recover natural constraint on the parameters set Θ (similar as the moments condition of [24])

t rather than (X(n)

)2(cid:1)

t

t

Θ is a compact subset of

(cid:110)
θ ∈ Rd, (cid:107)ξ0(cid:107)2
4

aj(θ) < 1

(cid:111)

.

∞
(cid:88)

j=1

(32)

i − V 2
Secondly, the Lipschitz coefﬁcients of Φ, ∂θΦ and ∂2
i |
following the same computations than in (24) and in the proof of Proposition 4.1. But since |U 2
i −
i | = |Ui − Vi| |Ui + Vi| and each time (cid:0)Mθ((Ui)i≥1) × Mθ((Vi)i≥1)(cid:1)−1 appears in the function g,
V 2
we deduce that Φ, ∂θΦ and ∂2
θ2Φ are respectively included in Lip 3(Θ), Lip 4(Θ) and Lip 4(Θ) with
coefﬁcients αs(·, Θ) deﬁned in (9) satisfying for s ≥ 2,

θ2Φ can be expressed in terms of |U 2






αs(Φ, Θ)
= supθ∈Θ
αs(∂θΦ, Θ) = supθ∈Θ
αs(∂2
θ2Φ, Θ) = supθ∈Θ

(cid:12)as(θ)(cid:12)
(cid:12)
(cid:12)
(cid:0)(cid:12)
(cid:12)as(θ)(cid:12)
(cid:12)as(θ)(cid:12)
(cid:0)(cid:12)

(cid:12) + (cid:12)
(cid:12) + (cid:12)

(cid:12)∂θas(θ)(cid:12)
(cid:1)
(cid:12)
(cid:12) + (cid:12)
(cid:12)∂θas(θ)(cid:12)

.

(cid:1)

θ2as(θ)(cid:12)
(cid:12)∂2
(cid:12)

From an application of Proposition 4.1 we obtain the asymptotic normality of (cid:98)θ(u) from primitive
conditions on functions aj:

t

Corollary 4.5. Let (X(n)
) be a tvARCH(∞) process deﬁned in (31). We assume that (cid:107)ξ0(cid:107)4 < ∞
(cid:80)∞
. If for j ∈ N∗ the
and we consider Θ a compact subset of
functions θ ∈ Θ (cid:55)→ aj(θ) ∈ [0, ∞) and θ ∈ Θ (cid:55)→ a0(θ) ∈ [a, ∞) are C2(Θ) functions such that
(cid:0)aj(θ) = aj(θ(cid:48)), ∀j ∈ N(cid:1) implies (θ = θ(cid:48)), if θ(n)
satisﬁes the assumption of local stationarity
(LS(ρ)), and if

(cid:110)
θ ∈ Rd, (cid:107)ξ0(cid:107)2
4

j=1 aj(θ) < 1

(cid:111)

t

∞
(cid:88)

j=1

j log j sup
θ∈Θ

(cid:12)aj(θ)(cid:12)
(cid:12)

(cid:12) + j sup
θ∈Θ

(cid:12)∂θaj(θ)(cid:12)
(cid:12)

(cid:12) + sup
θ∈Θ

θ2aj(θ)(cid:12)
(cid:12)
(cid:12)∂2

(cid:12) < ∞,

then the localized QMLE (cid:98)θ(u) is asymptotically normal and (15) holds for any u ∈ (0, 1) and any (hn)
satisfying (14).

To the best of our knowledge, this result is new. In [14] the existence of tvARCH(∞) processes has
been studied and the asymptotic normality has been obtained for tvARCH(p) processes.

We specialized the previous result to the cases where (X(n)
that (cid:107)ξ0(cid:107)4 < ∞ and we consider the model

t

) is a tvGARCH(p, q) process. We assume

(cid:40)

X(n)
t
(cid:0)σ(n)
t

= σ(n)
t
(cid:1)2 = c(n)

ξt
0,t + c(n)

1,t

(cid:0)X(n)
t−1

(cid:1)2 + · · · + c(n)
p,t

(cid:0)X(n)
t−p

(cid:1)2 + d(n)
1,t

(cid:0)σ(n)
t−1

(cid:1)2 + · · · + d(n)
q,t

(cid:0)σ(n)
t−q

(cid:1)2

(33)

Contrast estimation of locally stationary processes

15

i,t )0≤i≤p and (d(n)
t = 0 for any t ≤ 0. Consider θ(n)

j,t )1≤j≤q are non negative real number for any 1 ≤ t ≤ n, n ∈ N∗, with
p,t , d(n)
0,t ≥ c > 0 and

1,t , . . . , d(n)

1,t , . . . , c(n)

t = (cid:0)c(n)

(cid:1)(cid:48) with c(n)

0,t , c(n)

q,t

where (c(n)
X(n)
the subset Θ(p,q)

GARCH of Rp+q+1 deﬁned by:

Θ(p,q)

GARCH =

(cid:110)
(c0, c1, . . . , cp, d1, . . . , dq) ∈ Rp+q+1,

q
(cid:88)

j=1

dj + (cid:107)ξ0(cid:107)2
4

(cid:111)
.

ci < 1

p
(cid:88)

i=1

t (cid:107)4 < ∞. Moreover, (X(n)

t ∈ Θ for any 1 ≤ t ≤ n, n ∈ N∗ with Θ a bounded set included in Θ(p,q)

If θ(n)
supt,n (cid:107)X(n)
coefﬁcients ai(θ(n)
αs(∂2
ing, this implies the following corollary:

GARCH then we have
)t can be written as a tvARCH(∞) (see [22]). Moreover the
) in the tvARCH(∞) decrease exponentially fast. As αs(Φ, Θ), αs(∂θΦ, Θ) and
θ2Φ, Θ) can be expressed from as(·) and their derivatives, which are also exponentially decreas-

t

t

) be a tvGARCH(p, q) process deﬁned in (33) and Θ be a bounded set

t

Corollary 4.6. Let (X(n)
included in Θ(p,q)
which is implied by the local stationarity (LS(ρ)) on all functions c(n)
the central limit (15) holds for any u ∈ (0, 1) under condition (14).

GARCH where (cid:107)ξ0(cid:107)4 < ∞. If θ(n)

t

satisﬁes the assumption of local stationarity (LS(ρ)),
0,t , . . . , c(n)
q,t , then

1,t , . . . , d(n)

p,t , d(n)

This result can be compared for instance with those of [14] for tvARCH(p), which are obtained under
the same procedure but under the condition (cid:107)ξ0(cid:107)4(1+δ) < ∞, or those of [30] for tvGARCH(p, q),
which are obtained from a local polynomial estimation and under the condition (cid:107)ξ0(cid:107)8 < ∞. Note that
[33] also obtained asymptotic normality under very sharp conditions in a special case of tvARCH(p)
process.

4.2.3. Time varying ARMA(p, q)-GARCH(p(cid:48), q(cid:48)) processes

This model was introduced in the stationary framework by [17] and developed in [26]. The model
consists on a tvARMA(p, q) where the pure white noise (ξt) is replaced by a weak white noise (ε(n)
)
that is a tvGARCH(p(cid:48), q(cid:48)) process, i.e. (X(n)

) is deﬁned by

t

t

t−1 − · · · − φ(n)

p,t X(n)

t−p + ε(n)

t + ψ(n)

1,t ε(n)

t−1 + · · · + ψ(n)

q,t ε(n)

t−q

(cid:0)ε(n)
t−1

(cid:1)2 + · · · + c(n)
p(cid:48),t

(cid:0)ε(n)
t−p(cid:48)

(cid:1)2 + d(n)
1,t

(cid:0)σ(n)
t−1

(cid:1)2 + · · · + d(n)
q(cid:48),t

(cid:0)σ(n)
t−q(cid:48)

(cid:1)2

for any 1 ≤ t ≤ n, n ∈ N∗, with X(n)
family of non-negative real numbers. Consider

t = 0 for any t ≤ 0. As previously, c(n)

0,t ≥ c > 0 and (c(n)

(34)
j,t ) is a






X(n)
t
ε(n)
t
(cid:0)σ(n)
t

= −φ(n)
1,t X(n)
= σ(n)
ξt
t
(cid:1)2 = c(n)
0,t + c(n)

1,t

t = (cid:0)φ(n)
θ(n)

1,t , . . . , φ(n)

p,t , ψ(n)

1,t , . . . , ψ(n)

q,t , c(n)

0,t , c(n)

1,t , . . . , c(n)

p(cid:48),t, d(n)

1,t , . . . , d(n)
q(cid:48),t

(cid:1)(cid:48).

If (cid:107)ξ0(cid:107)4 < ∞, deﬁne the subset Θ(p,q,p(cid:48),q(cid:48))

ARM AGARCH of Rd with d = p + q + p(cid:48) + q(cid:48) + 1 by

Θ(p,q,p(cid:48),q(cid:48))

ARM AGARCH =

(cid:110)

θ ∈ Rd,

q(cid:48)
(cid:88)

j=1

dj + (cid:107)ξ0|2
4

p(cid:48)
(cid:88)

j=1

cj < 1,

16

and (cid:0)1 +

p
(cid:88)

j=1

φjzj(cid:1) (cid:0)1 +

q
(cid:88)

j=1

ψjzj(cid:1) (cid:54)= 0 for all |z| ≤ 1

(cid:111)
.

t

t (cid:107)4 < ∞ (see previously) when θ(n)

ARM AGARCH then the GARCH(p(cid:48), q(cid:48)) process (ε(n)

ARM AGARCH , and these coefﬁcients decrease exponentially fast. Therefore, when θ(n)
ARM AGARCH , then supt,n (cid:107)X(n)

Then, if Θ is a bounded set included in Θ(p,q,p(cid:48),q(cid:48))
)t sat-
t ∈ Θ for any 1 ≤ t ≤ n, n ∈ N∗. Moreover, X(n)
isﬁes supt,n (cid:107)ε(n)
can be written as a linear ﬁlter of (ε(n)
)t when the ARMA coefﬁcients satisﬁes the condition required
in Θ(p,q,p(cid:48),q(cid:48))
t ∈ Θ for
any 1 ≤ t ≤ n, n ∈ N∗ with Θ a bounded set included in Θ(p,q,p(cid:48),q(cid:48))
t (cid:107)4 < ∞.
Moreover, following Lemma 2.1. of [2], we know that a stationary ARMA(p, q)-GARCH(p(cid:48), q(cid:48)) pro-
cess is a stationary afﬁne causal process with functions fθ and Mθ satisfying the Lipschitz condition
(20) with Lipschitz coefﬁcients decreasing exponentially fast, as well as their derivatives. This is also
the same case for a time varying ARMA(p, q)-GARCH(p(cid:48), q(cid:48)) process. Therefore, we obtain the fol-
lowing result:

t

t

t

Corollary 4.7. Let (X(n)
) be a time varying ARMA(p, q)-GARCH(p(cid:48), q(cid:48)) process deﬁned in (34)
with (cid:107)ξ0(cid:107)4 < ∞ and Θ be a bounded set included in Θ(p,q,p(cid:48),q(cid:48))
satisﬁes the
assumption of local stationarity (LS(ρ)), which is implied by the same local stationarity property sat-
isﬁed by φ(n)
q,t , c(n)
q(cid:48),t, then the localized QMLE
is asymptotically normal as (15) holds for any u ∈ (0, 1) and (hn) satisfying (14).

ARM AGARCH . Moreover, if θ(n)

1,t , . . . , ψ(n)

1,t , . . . , φ(n)

1,t , . . . , d(n)

1,t , . . . , c(n)

p(cid:48),t, d(n)

p,t , ψ(n)

0,t , c(n)

t

4.3. Time varying LARCH(∞) processes and LS-contrast

Here we consider a LARCH(∞) process introduced by Robinson in [29] and studied intensively by
Giraitis et al. in [25]. The model is deﬁned as

Xt = ξt

(cid:16)

a0(θ) +

∞
(cid:88)

j=1

aj(θ) Xt−j

(cid:17)

for any t ∈ Z,

(35)

where θ ∈ Rd and assume (cid:107)ξ0(cid:107)2 = 1. Assume also that j ∈ N, θ ∈ Rd (cid:55)→ aj(θ) ∈ R are continuous
functions and without lose of generality assume a0(θ) ≥ 0 for any θ ∈ Rd. Moreover, for ensuring the
stationarity of (Xt) and the existence of (cid:107)Xt(cid:107)r with r ≥ 1, assume that for any θ ∈ Θ,

(cid:107)ξ0(cid:107)r

∞
(cid:88)

j=1

|aj(θ)| < 1.

(36)

Even if a LARCH(∞) process is an afﬁne causal process, the Gaussian QML contrast can not be used
for estimating θ. Indeed, the conditional variance of Xt can not be bounded close to 0 and this does
not allow asymptotic results for such contrasts (see more details in Francq and Zakoïan [23]). Even if
weighted least square estimators can also be deﬁned (see [23]), we consider here the following ordinary
LS contrast of square values: for x ∈ R∞, deﬁne

ΦLARCH (x, θ) =

(cid:16)

1 − (cid:0)a0(θ) +
x2

aj(θ) xj+1

(cid:1)2(cid:17)2

.

∞
(cid:88)

j=1

(37)

Contrast estimation of locally stationary processes

17

If the stationary version ( (cid:101)Xt(u)) were observed, for any θ ∈ Θ the score associated to the LS-contrast
is

E(cid:2)ΦLARCH (( (cid:101)X1−k(u))k≥0, θ) | F0

(cid:3) = E(cid:2)|ξ1|4 − 1(cid:3) (cid:16)

a0(θ∗) +

aj(θ∗(u)) (cid:101)X1−j(u)

(cid:17)4

∞
(cid:88)

j=1

(cid:16)(cid:0)a0(θ∗(u)) +

+

∞
(cid:88)

j=1

aj(θ∗(u)) (cid:101)X1−j(u)(cid:1)2 − (cid:0)a0(θ) +

aj(θ) (cid:101)X1−j(u)(cid:1)2(cid:17)2

.

∞
(cid:88)

j=1

We notice that the ﬁrst term at the right side of the last equality does not depend on θ. Then since a0(·)
is supposed to be non negative, if we assume

(cid:16)

a0(θ) +

∞
(cid:88)

j=1

aj(θ) X1−j = a0(θ(cid:48)) +

∞
(cid:88)

j=1

aj(θ) X1−j a.s.

(cid:17)

=⇒ θ = θ(cid:48),

(38)

then E(cid:2)ΦLARCH ((X1−k)k≥0, θ) | F0
holds. Moreover, after computations and use of Hölder Inequalities, if r = 4,

(cid:3) has a unique minimum that is θ∗ and Assumption (Co(ΦLARCH , Θ))

(cid:12)ΦLARCH (U, θ) − ΦLARCH (V, θ)(cid:12)
(cid:12)
(cid:12)

sup
θ∈Θ

(cid:16)

≤

1 + V 2
U 2

1 + (cid:0)a0(θ) +

∞
(cid:88)

j=1

aj(θ) Uj+1

(cid:1)2 + (cid:0)a0(θ) +

aj(θ) Vj+1

(cid:1)2(cid:17)

∞
(cid:88)

j=1

(cid:16)

×

|U1 + V1| |U1 − V1| +

(cid:12)
(cid:12)
(cid:12)2a0(θ) +

∞
(cid:88)

j=1

(cid:12)
(cid:12)
aj(θ) (Uj+1 + Vj+1)
(cid:12)

∞
(cid:88)

j=1

|aj(θ)| |Uj+1 − Vj+1|

(cid:17)

Hence

E(cid:2) sup
θ∈Θ

(cid:12)ΦL(U, θ) − ΦL(V, θ)(cid:12)
(cid:12)
(cid:12)

(cid:3) ≤ g(cid:0) sup
i≥1

(cid:13)Ui(cid:107)4 ∨ (cid:13)
(cid:8)(cid:13)

(cid:13)Vi(cid:107)4}(cid:1)

(cid:16)

×

(cid:107)U1 − V1(cid:107)4 +

∞
(cid:88)

j=2

sup
θ∈Θ

|aj−1(θ)| (cid:107)Uj − Vj(cid:107)4

(cid:17)

,

and therefore ΦLARCH ∈ Lip 4(Θ) with α1(ΦLARCH , Θ) = 1 , and αk(ΦLARCH , Θ) = supθ∈Θ |ak−1(θ)|,
for k ≥ 2 and (cid:80)

k αk(ΦLARCH , Θ) < ∞, from (36).

We consider now the time varying LARCH(∞) process deﬁned by:

X(n)

t = ξt

(cid:16)

a0(θ(n)
t

) +

∞
(cid:88)

i=1

ai(θ(n)
t

) X(n)
t−i

(cid:17)

,

for any t ∈ Z,

(39)

t ∈ Θ a compact set of Rd and X(n)

with θ(n)
t = 0 for t ≤ 0. We also assume that a0(·) is a non negative
function. An application of Theorem 3.1 implies that the localized LS estimator is uniformly consistent

18

when r = 4.
To assert the asymptotic normality, we assume (cid:107)ξ0(cid:107)8 < ∞ and Θ is a bounded subset of the set

ΘLARCH =

(cid:110)
θ ∈ Rd, (cid:107)ξ0(cid:107)8

|ai(θ)| < ∞

(cid:111)
.

∞
(cid:88)

i=1

(40)

Then, using classical computations and Hausdorff Inequalities (see the proof), we obtain the asymptotic
behavior of the estimator:

Proposition 4.2. Assume that θ ∈ Rd (cid:55)→ aj(θ) ∈ R are C2 functions for any j ∈ R, a0(·) ≥ 0 and

1. (cid:107)ξ0(cid:107)8 < ∞ where the probability distribution of ξ0 is absolutely continuous with respect to the

Lebesgue measure and Θ is a bounded set included in ΘLARCH ;

2. For all θ, θ(cid:48) ∈ Θ,

3. For all θ, θ(cid:48) ∈ Θ,

(ai(θ) = ai(θ(cid:48)), for all i ∈ N) =⇒ (θ = θ(cid:48));

(∂θai(θ) = ∂θai(θ(cid:48)), for all i ∈ N) =⇒ (θ = θ(cid:48)).

(41)

(42)

Let (X(n)
Consider Φ = ΦLARCH as in (37). If

t

) be a tvLARCH process deﬁned following (39) where θ(n)

t

satisﬁes Assumption (LS(ρ)).

∞
(cid:88)

j=1

j log j sup
θ∈Θ

|aj(θ| + j sup
θ∈Θ

(cid:107)∂θaj(θ(cid:107) + sup
θ∈Θ

(cid:107)∂2

θ2aj(θ(cid:107) < ∞,

(43)

then (cid:98)θ(u) is asymptotically normal as (15) for any u ∈ (0, 1) and (hn) satisfying (14).

To our knowledge, this result is new, even in its stationary θ(n)
t = θ∗ ∈ Rd version. The particular case
of time varying GLARCH(p, q) process, natural extension of stationary GLARCH(p, q) processes (see
for instance [25]) is also interesting and straightforward:

Corollary 4.8.
respect to the Lebesgue measure and if (X(n)

t

If (cid:107)ξ0(cid:107)8 < ∞ and the probability distribution of ξ0 is absolutely continuous with

) is a tvGLARCH(p, q) process deﬁned by

X(n)

t = ξt σ(n)

t

with σ(n)

t = c(n)

0,t +

p
(cid:88)

i=1

c(n)
i,t X(n)

t−i +

q
(cid:88)

j=1

d(n)
j,t σ(n)
t−j,

for any t ∈ N∗,

with X(n)
in

t = 0 for t ≤ 0 and where θ(n)

t = (c(n)

0,t , . . . , c(n)

i,t , d(n)

j,t , . . . , d(n)

j,t ) ∈ Θ, with Θ a bounded set

(cid:8)(c0, c1, . . . , cp, d1, . . . , dq) ∈ [0, ∞) × Rp+q,

q
(cid:88)

i=1

|di| + (cid:107)ξ0(cid:107)8

p
(cid:88)

i=1

|ci| < 1(cid:9),

which satisﬁes the assumption of local stationarity (LS(ρ)). Then the central limit (15) holds for any
u ∈ (0, 1) under condition (14).

This result is due to the exponential decay of the sequences (αs(ΦLARCH , Θ))s and (αs(∂θΦLARCH , Θ))s
in such as case (see [25]).

Contrast estimation of locally stationary processes

19

4.4. Time varying integer valued processes and Poisson QMLE

Finally, we consider the integer valued process (Xt) deﬁned so that the conditional distribution of Xt
(cid:0)(Xt−i)i≥1
is a Poisson distribution with parameter λθ

(cid:1), i.e.

Xt | ((Xt−i)i≥1)

L
∼ P(λθ

(cid:0)(Xt−i)i≥1

(cid:1)(cid:1),

for any t ∈ Z,

(44)

where for any θ ∈ Θ, U ∈ R∞ (cid:55)→ λθ(U ) ∈ [λ, ∞), λ > 0, is once again a Lipschitz function on Θ sat-
isfying (20) with Lipschitz coefﬁcients (βi(λθ, Θ))i≥1 such that (cid:80)∞
i=1 βi(λθ, Θ) < 1 (see for instance
Doukhan and Kengne [19]).
Then, we consider as a contrast the opposite of the log-likelihood of the process, i.e.

ΦP (x, θ) = −x1 log (cid:0)λθ

(cid:0)(xi)i≥2

(cid:1)(cid:1) + λθ

(cid:0)(xi)i≥2

(cid:1).

(45)

Then, after classical computations we obtain for p = 2:

E(cid:2) sup
θ∈Θ

(cid:12)ΦP (U, θ) − ΦP (V, θ)(cid:12)
(cid:12)
(cid:12)

(cid:3)

(cid:16)

≤ C

sup
i≥1

(cid:13)Ui(cid:107)2 ∨ (cid:13)
(cid:8)(cid:13)

(cid:13)Vi(cid:107)2} (cid:107)U1 − V1(cid:107)2 + (1 + (cid:107)V1(cid:107)2

2)1/2

βi(λθ, Θ) (cid:107)Ui − Vi(cid:107)2

(cid:17)

∞
(cid:88)

i=2

≤ g(cid:0) sup
i≥1

(cid:13)Ui(cid:107)2 ∨ (cid:13)
(cid:8)(cid:13)

(cid:13)Vi(cid:107)2}(cid:1)

∞
(cid:88)

i=1

αi(Φ, Θ) (cid:107)Ui − Vi(cid:107)2,

with α1(ΦP , Θ) = 1 and αi(ΦP , Θ) = βi−1(λθ, Θ) for i ≥ 2, inducing ΦP ∈ Lip 2(Θ).

We extend the structural recursive equation (44) and ΦP to the time varying framework as follows.
This example shows that our results apply to integer valued local stationary processes, which is an
original and interesting extension. Hence, we consider

X(n)
t

| (X(n)

t−i)i≥1

L
∼ P

(cid:16)

λ

θ(n)
t

(cid:0)(X(n)

t−i)i≥1

(cid:1)(cid:17)

,

for any 1 ≤ t ≤ n, and all n ∈ N∗,

(46)

where X(n)
cients (βi(λ, Θ))i≥1, we deﬁne

t = 0 for t ≤ 0. If λθ(·) satisﬁes the uniform Lipschitz property (20) with Lipschitz coefﬁ-

ΘP =

(cid:110)

θ ∈ Rd,

βi(λ, {θ}) < 1

(cid:111)
.

∞
(cid:88)

i=1

(47)

Then, using Theorem 2.1 of Doukhan et al. [18] and Lemma 2.1, we deduce that if θ ∈ ΘP for any
1 ≤ t ≤ n and n ∈ N∗, then supt,n (cid:107)X(n)
t (cid:107)p < ∞, for any p ≥ 1. Using the Poisson QMLE deﬁned by
(45), we obtain the following asymptotic result:

Proposition 4.3. Let (X(n)
(20) and under Assumption (LS(ρ)). Assume also:

) satisfy (25) where λθ, ∂θλθ and ∂2

t

θ2λθ satisfy Lipschitz inequalities

1. Θ is a bounded set included in ΘP ;

20

2. there exists λ > 0 such as λθ ≥ λ for any θ ∈ Θ;
3. For all θ, θ(cid:48) ∈ Θ, λθ = λθ(cid:48) implies θ = θ(cid:48);
4. For any i = 1, . . . , d, ∂
∂θi

(cid:0)( (cid:101)X−k(u))k∈N

λθ∗(u)

(cid:1) (cid:54)= 0 a.s.

Consider Φ = ΦP as it was deﬁned in (45). Then, if

∞
(cid:88)

(j log j) βj(λ, Θ) + j βj(∂θλ, Θ) + βj(∂2

θ2 λ, Θ) < ∞,

(48)

j=1

for any u ∈ (0, 1) and under condition (14).

(cid:112)

n hn

(cid:0)

(cid:98)θ(u) − θ∗(u)(cid:1)

L
−→
n→+∞

N

(cid:16)

0, E

(cid:104) ∂θλθ∗(u)

(cid:0)( (cid:101)X−k(u))k∈N
λ2
θ∗(u)

(cid:1)t∂θλθ∗(u)
(cid:0)( (cid:101)X−k(u))k∈N

(cid:0)( (cid:101)X−k(u))k∈N
(cid:1)

(cid:1)

(cid:105)(cid:17)
.

(49)

This central limit theorem can notably be applied for:

• Time varying integer-valued GARCH(p, q) processes (tvINGARCH(p, q) processes), where

(cid:0)(X(n)

t−j)j≥1

(cid:1) = a(n)

0,t +

λ

θ(n)
t

p
(cid:88)

i=1

a(n)
i,t X(n)

t−i +

q
(cid:88)

i=1

b(n)
i,t λ

θ(n)
t

(cid:0)(X(n)

t−i−j)j≥1

(cid:1),

(50)

for any 1 ≤ t ≤ n and n ∈ N∗, with X(n)

t = 0 for all t ≤ 0. Here
p,t , b(n)
0,t , . . . , a(n)
1,t , . . . , b(n)

q,t

(cid:1).

t = (cid:0)a(n)
θ(n)

Here, following [18], it is possible to consider a sharper set of parameters than the one given by
(47); hence let Θ be a bounded subset of

ΘIN GARCH =

(cid:110)
θ = (cid:0)a0, a1, . . . , ap, b1, . . . , bq

(cid:1) ∈ Rp+q+1,

p
(cid:88)

i=1

ai +

q
(cid:88)

i=1

(cid:111)
.

bi < 1

(51)

In such a case, the Lipschitz coefﬁcients βj(λ, Θ), βj(∂θλ, Θ) and βj(∂2
crease exponentially fast (see [18]), and all the conditions are satisﬁed for obtaining (49).

θ2λ, Θ) exist and de-

• Integer valued threshold GARCH(p, q) processes where, with (cid:96) a positive ﬁxed integer,

(cid:0)(X(n)

t−j)j≥1

(cid:1)

λ

θ(n)
t

= a(n)

0,t +

p
(cid:88)

i=1

a(n)
i,t λ

θ(n)
t

(cid:0)(X(n)

t−i−j)j≥1

(cid:1) +

q
(cid:88)

i=1

(cid:0)b(n)

i,t max(X(n)

t−i − (cid:96), 0) − c(n)

i,t min(X(n)

t−i, (cid:96))(cid:1),

for any 1 ≤ t ≤ n and n ∈ N∗, with X(n)

t = (cid:0)a(n)
θ(n)

0,t , . . . , a(n)

t = 0 for all t ≤ 0. Here
q,t , c(n)
1,t , . . . , b(n)

p,t , b(n)

1,t , . . . , c(n)

q,t

(cid:1).

As previously, the Lipschitz coefﬁcients βj(λ, Θ), βj(∂θλ, Θ) and βj(∂2
crease exponentially fast (see [18]), and all the conditions are satisﬁed for obtaining (49).

θ2λ, Θ) exist and de-

Contrast estimation of locally stationary processes

21

c0

c1

d1

n

1000
3000
10000

(cid:98)cU
0
0.493
0.363
0.259

(cid:98)cE
0
0.455
0.323
0.224

(cid:98)cU
1
0.126
0.081
0.052

(cid:98)cE
1
0.122
0.077
0.048

(cid:98)dU
1

0.230
0.167
0.118

(cid:98)dE
1

0.208
0.146
0.101

Table 1. Square root of the MISE for tvGARCH(1, 1) processes for n = 1000, 3000 and 10000 computed from
1000 independent replications.

5. Numerical experiments

In the sequel we are going to apply our kernel based estimator in several different cases of local sta-
tionary processes.

The window bandwidth hn is a tuning parameter that requires to be chosen. In order to neglect the
bias we chose hn = n−λ with λ = 0.35, inducing n h3
0, which is the uniform consistency
and the asymptotic normality condition required for Lipp-contrast and Cρ functions when p > 3/2 and
ρ = 1.

n −→

n→+∞

5.1. Monte Carlo simulations

Here we will consider three cases:

1. An example of tvGARCH(1, 1). Here, with the notation of equation (33), assume:

0,t = 1 + 0.5 sin (cid:0)5
c(n)

(cid:1),

t
n

1,t = 0.1 + 0.4 cos2 (cid:0)4
c(n)

(cid:1)

t
n

and d(n)

1,t = 0.1 + 0.4

t
n

,

0(u) = 1 + 0.5 sin(5 u), c∗

1(u) = 0.1 + 0.4 cos2(4 u)
1(u) = 0.1 + 0.4 u. Moreover, we assume that (ξt) is a sequence of i.i.d.r.v. following

for any 1 ≤ t ≤ n and n ∈ N∗. Clearly, c∗
and d∗
N (0, 1) distribution.
We independently replicated 1000 trajectories of such process, for n = 2000, 5000 and 10000
and computed the Gaussian QMLE estimators (cid:98)c0(u), (cid:98)c1(u) and (cid:98)d1(u) for u = k/50 with k =
1, . . . , 49. Finally, we used the two well known kernels, the uniform kernel U (x) = 1
2 I1x∈[−1,1]
and the Epanechnikov one E(x) = 3
4 (1 − x2) I1x∈[−1,1] and denote respectively (cid:98)θU (u) and
(cid:98)θE(u).

Table 1 contains the results of these Monte Carlo experiments where we computed the square
root of the mean integrated squared error (RSMISE). In Figure 1 exhibits an example of partic-
ular trajectories of these estimators for n = 10000, while Figure 2 we also present the average
trajectories of (cid:98)cE

1 (u) when n = 5000.

1 (u) and (cid:98)dE

0 (u), (cid:98)cE

2. A tvARCH(∞) example. With the notation of equation (31), assume:

22

(a) Trajectory of (cid:98)cE
0 .

(b) Trajectory of (cid:98)cE
1 .

(c) Trajectory of (cid:98)dE
1 .

Figure 1: Paths of functions c0, c1, d1 (in black), and a path of (cid:98)cE

0 , (cid:98)cE

1 and (cid:98)dE

1 (in red) for n = 10000

(a) Mean of (cid:98)cE
0 .

(b) Mean of (cid:98)cE
1 .

(c) Mean of (cid:98)dE
1 .

Figure 2: Paths of functions c0, c1, d1 (in black), and the mean trajectories over 1000 replications of
0 , (cid:98)cE
(cid:98)cE

1 (in red) for n = 5000

1 and (cid:98)dE

Figure 3: On trajectory of a tvARCH(∞)-process for n = 2000

θ = (cid:0)c0, c1, p(cid:1),

a0(θ) = c0

with c(n)

0,t = 1 + 0.5 sin (cid:0)5

and aj(θ) = c1 j−p
t
n

(cid:1),

1,t = 0.1 + 0.5 cos2 (cid:0)4
c(n)

for j ∈ N∗

(cid:1)

j
n

and p(n)

t = 2.1 +

j
n

.

3,

√

√

0(u) = 1 + 0.5 sin(5 u), c∗

for any 1 ≤ t ≤ n and n ∈ N∗. Therefore c∗
1(u) = 0.1 + 0.5 cos2(4 u)
and p∗(u) = 2.1 + u. Moreover, we assume that (ξt) is a sequence of i.i.d.r.v. following
U([−
As previously, we replicated 1000 trajectories of such process (see for instance one trajectory in
Figure 3), for n = 2000, 5000 and 10000 and computed the Gaussian QMLE estimators with
Epanechnikov kernel (cid:98)cE

1 (u) and (cid:98)pE(u) for u = k/50 with k = 1, . . . , 49.

3]) (uniform) distribution.

0 (u), (cid:98)cE

Table 2 contains the results of these Monte-Carlo experiments where we computed the square
root of mean integrated squared error (RSMISE).

0.00.20.40.60.81.00.60.81.01.21.4uaa00.00.20.40.60.81.00.10.20.30.40.5uaa10.00.20.40.60.81.00.10.20.30.40.5ubb10.00.20.40.60.81.00.60.81.01.21.4uaa00.00.20.40.60.81.00.10.20.30.40.5uaa10.00.20.40.60.81.00.10.20.30.40.5ubb1TimeX0500100015002000−10−50510Contrast estimation of locally stationary processes

23

n

1000
2000
5000
10000

(cid:98)cU
0
0.271
0.227
0.164
0.128
0 , (cid:98)cE

(cid:98)cE
0
0.294
0.261
0.187
0.144
1 , (cid:98)cE
0 , (cid:98)cU

(cid:98)cE
1
0.089
0.069
0.049
0.039

(cid:98)cU
1
0.089
0.066
0.047
0.037

(cid:98)pE
0.770
0.696
0.591
0.517
1 , (cid:98)pU and (cid:98)pE for the tvARCH(∞) processes for n = 1000,

(cid:98)pU
0.789
0.712
0.624
0.555

Table 2. Square root of the MISE of (cid:98)cU
2000, 5000 and 10000 computed from 1000 independent replications.

Figure 4: A trajectory of a tvINGARCH(1, 0)-process for n = 1000

n

1000
2000
5000
10000

(cid:98)aU
0
0.144
0.111
0.079
0.061
0 , (cid:98)aU

(cid:98)aE
0
0.135
0.103
0.073
0.056
1 and (cid:98)aE

(cid:98)aU
1
0.051
0.045
0.032
0.025

(cid:98)aE
1
0.058
0.041
0.030
0.022

Table 3. Square root of the MISE of (cid:98)aU
5000 and 10000 computed from 1000 independent replications.

0 , (cid:98)aE

1 for tvINGARCH(1, 0) processes for n = 1000, 2000,

3. The example of one integer valued process.

A tvINGARCH(1, 0)-process (tvINARCH(1)) as deﬁned in (50). Here we choose

0,t = 1 + 0.5 sin (cid:0)5
a(n)

(cid:1)

t
n

and a(n)

1,t = 0.3 + 0.5

t
n

for any 1 ≤ t ≤ n and n ∈ N∗. Note that we only consider here the Poisson distribution case.
Figure 4 exhibits a trajectory of such a process for n = 1000.
Using the same procedure than in the previous examples, Table 3 contains the RSMISE of the
estimators computed with Uniform and Epanechnikov kernels, (cid:98)aU

1 and (cid:98)aE
1 .

0 , (cid:98)aE

0 , (cid:98)aU

Conclusion: We can globally conclude from these Monte Carlo experiments:

• The consistency of the estimators and their convergence rate are established;
• The Epanechnikov’s kernel is preferable to the uniform one.

TimeX02004006008001000024681024

Figure 5: The log-returns of daily closing values of S&P500 index between October 1990 and October
2020

(a) (cid:98)cU

0 and (cid:98)cE
0

(b) (cid:98)cU

1 and (cid:98)cE
1

(c) (cid:98)dU

1 and (cid:98)dE
1

(d) (cid:98)cU

1 + (cid:98)dU

1 and (cid:98)cE

1 + (cid:98)dE
1

Figure 6: Estimators of c0, c1, d1 and c1 + d1 for the log-returns of daily closing values of S&P500
index from October 1990 and October 2020

5.2. Application to ﬁnancial data

We apply our local non-parametric estimator to a trajectory of ﬁnancial data. More precisely, we con-
sider the log-returns of the daily closing values of S&P500 index between July 1999 and July 2019
(therefore n = 5031, see also Figure 5 for the graph of this trajectory). Many studies have shown that
the GARCH(1, 1) process is a relevant model for this type of data (We refer to the monograph of
Francq and Zakoïan [22] for more details). As a consequence, we used a tvGARCH(1, 1) process (see
(33)) to take into account the changes in economic and ﬁnancial conjectures over 20 years on such a
model (think in particular of the September 2008 crisis and of the spring 2020 COVID crisis). Figure
6 exhibits the evolution of the three estimators computed with Uniform and Epanechnikov kernels, i.e.
0 , (cid:98)cU
(cid:98)cU

1 from Gaussian QMLE.

0 , (cid:98)cE

1 , (cid:98)cE

1 , (cid:98)dE

1 , (cid:98)dU

We draw the evolution of c1 + d1 in order to get a visual indicator of the variability of the S&P500
index. The larger c1 + d1 the worst the moment properties of the tvGARCH(1, 1). The variability may
be seen as an indicator of instability of the ﬁnancial markets and thus of the crisis. Indeed the maximum
of the c1 + d1 is achieved at the chore of the September 2008. More surprisingly, there is also a peak of

1990199520002005201020152020−0.10−0.050.000.050.10timeX1995200020052010201520200e+002e−064e−066e−068e−061e−05dateC0E1995200020052010201520200.050.100.150.200.25dateC1U1995200020052010201520200.750.800.850.900.95dateD1E1995200020052010201520200.920.940.960.981.00dateC1E + D1EContrast estimation of locally stationary processes

25

variability as early as 2003. There the ﬁnancial markets were renewing at their climate between 1998
and 2008 crisis. In order to distinguish between the two peaks of variability, one should observe that
the curves of the coefﬁcients c1 and d1 separately. Then we observe that 2003 corresponds to a higher
value for the coefﬁcient d1 and 2008 to a higher value for the coefﬁcient c1. We note that d1 is the
coefﬁcient of persistence in the volatility whereas c1 transfers external shocks in the volatility. Finally,
also surprisingly, the COVID crisis does not seem to change the last ﬁve years evolution of c1 and d1.

6. Moments and coupling properties of non stationary inﬁnite

memory processes

6.1. Proof of the moments properties in Lemma 2.1

We start this section with the proof of Lemma 2.1 which follows similar arguments than in [21] and
that we give here for completeness.

Proof of Lemma 2.1. Under the assumption (A0(Θ)), for any n ∈ N∗ and 0 ≤ t ≤ n we have

(cid:107)X(n)

t − F

θ(n)
t

(0; ξt)(cid:107)p ≤

∞
(cid:88)

s=1

s (cid:107)X(n)
b(0)

t−s(cid:107)p .

Thus from the triangle inequality we obtain

(cid:107)X(n)

t (cid:107)p ≤

∞
(cid:88)

s=1

s (cid:107)X(n)
b(0)

t−s(cid:107)p + sup
θ∈Θ

(cid:107)Fθ(0, ξ0)(cid:107)p.

As a consequence we get

(cid:107)X(n)

t (cid:107)p ≤

∞
(cid:88)

s=1

b(0)
s max
j≤t−1

(cid:107)X(n)

j (cid:107)p + sup
θ

(cid:107)Fθ(0, ξ0)(cid:107)p ≤ B0(Θ) max
j<t

(cid:107)X(n)

j (cid:107)p + C0(Θ).

With Mt = maxj≤t (cid:107)X(n)
B0(Θ) < 1, which implies with M0 = 0 that for any 0 ≤ t ≤ n,

j (cid:107)p, a recursion entails that Mt ≤ B0(Θ) Mt−1 + C0(Θ) where 0 ≤

Mt ≤ C0(Θ)

t
(cid:88)

k=0

b(0)
k ≤

C0(Θ)
1 − B0(Θ)

< ∞

and this achieves the proof. We refer to [21] for more details.

6.2. Weak dependence properties of the stationary version

Any stationary inﬁnite memory process (2) admits fruitful coupling properties. In this section we quan-
tify them thanks to the τ -weak dependence properties introduced in [16]. The reader is deferred to the
lecture notes [15] for complements and details on coupling, based on the Wasserstein distance between
probabilities. Conditional coupling for stationary time series is deﬁned as follows.

26

Deﬁnition 6.1 ([16]). Let (Ω, C, P) be a probability space, M a σ-subalgebra of C and Z a random
variable with values in E. Denote Λ1(E) the space of 1-Lipschitz functions from E to R. Assume that
(cid:107)Z(cid:107)p < ∞ and deﬁne the coupling coefﬁcient τ (p) as

τ (p)(M, Z) =

(cid:90)

(cid:13)
(cid:13)
(cid:13) sup
f ∈Λ1(E)

(cid:110)(cid:12)
(cid:12)
(cid:12)

f (x)P

Z|M(dx) −

(cid:90)

(cid:12)
f (x)PZ (dx)
(cid:12)
(cid:12)

(cid:111)(cid:13)
(cid:13)
(cid:13)p

.

The dependence between the past of the sequence (Zt)t∈Z and its future k-tuples may be assessed by
using the coupling coefﬁcient τ (p): Consider the norm (cid:107)x − y(cid:107) = (cid:107)x1 − y1(cid:107) + · · · + (cid:107)xk − yk(cid:107) on Ek,
set Mp = σ(Zt, t ≤ p) and deﬁne

τ (p)
Z (r) = sup
k>0

(cid:110)

max
1≤l≤k

1
l

sup

(cid:110)
τ (p)(Mp, (Zj1, . . . , Zjl )) with p + r ≤ j1 < · · · < jl

(cid:111)(cid:111)
.

Finally, the time series (Zt)t∈Z is said to be τ (p)
τ (p)
Z (r) tend to 0 as r tends to inﬁnity.

Z -weakly dependent in case the sequence of coefﬁcients

The τ -dependence coefﬁcients of the stationary process ( (cid:101)Xt(u))t∈Z are bounded above by using
the following coupling scheme. Hence, if (ξ◦
t )t∈Z is an independent replication of (ξt)t∈Z, deﬁne
( (cid:101)X◦

t (u))t∈Z such as:

(cid:101)X◦

t (u) =

(cid:40)

Fθ∗(u)
Fθ∗(u)

Then for s ≥ 0, we have the upper-bound

(cid:0)( (cid:101)X◦
(cid:0)( (cid:101)X◦

t−k(u))k≥1, ξ◦
t
t−k(u))k≥1, ξt

(cid:1), for t ≤ 0;
(cid:1), for t > 0.

τ (p)
(cid:101)X(u)

(s) ≤ (cid:13)

(cid:13) (cid:101)Xs(u) − (cid:101)X◦

s (u)(cid:13)

(cid:13)p .

(52)

(53)

In the following, we mimic the proof of Theorem 3.1 of [21] in order to get an Lp-estimate uni-
form over u ∈ [0, 1] of the approximation of (cid:101)Xs(u) by (cid:101)X◦
s (u). We start by estimating the moments
(cid:107) supu∈[0,1] | (cid:101)Xs(u)|(cid:107)p in the following Lemma.

Lemma 6.1. Let Θ ⊂ Rd be such that (A0(Θ)) holds with B0(Θ) < 1 and assume that (LS(ρ)) also
holds. Then the stationary version ( (cid:101)Xt(u))t∈Z solution of (8) satisﬁes

(cid:13)
(cid:13)
(cid:13) sup
u∈[0,1]

(cid:13)
(cid:13)
| (cid:101)Xt(u)|
(cid:13)p

≤

C0(Θ)
1 − B0(Θ)

,

t ∈ Z .

Proof of Lemma 6.1. We adapt the ﬁxed point approach of [20] to our setting. We refer to [20] for
details. We consider Lp(C([0, 1], R)) the Banach space of random continuous functions H : [0, 1] →
R that admits ﬁnite p moments equipped with the norm H (cid:55)→ (cid:107) supu∈[0,1] |Hu|(cid:107)p. The underlying
probability space is the one of the probability distribution of the iid sequence (ξt)t∈Z, i.e. Hu is a
measurable function of (ξt)t∈Z such that E[supu∈[0,1] |Hu((ξt)t∈Z)|p] < ∞. We denote L the lag
operator on sequences (xt)t∈Z of RZ such that L((xt)t∈Z) = (xt−1)t∈Z. We denote Φ the function
from Lp(C([0, 1], R)) such that

Φ(H)(u) = Fθ∗(u)((Hu ◦ Lj)j≥0, π0) ,

u ∈ [0, 1] ,

Contrast estimation of locally stationary processes

27

(cid:0)(xt)t∈Z

(cid:1) = x0. That u (cid:55)→ Φ(H)(u) is continuous follows from the
where π0 is the projection π0
continuity of θ (cid:55)→ Fθ and u (cid:55)→ θ∗(u) under (A0(Θ)) and (LS(ρ)). That supu∈[0,1] |Φ(H)|(u) admits
ﬁnite moments of order p follows from similar arguments than in Lemma 1 of [20] under (A0(Θ)) that
holds uniformly in u ∈ [0, 1]. One can apply the Picard ﬁxed point theorem to Φ which is a contraction
under B0(Θ) < 1. We derive the existence of (cid:101)Xt(u) in the Banach space Lp(C([0, 1], R)) and the
desired estimate on its norm.

Notice that the same uniform estimate also holds on the coupling version (cid:101)X◦
s (u) so that one can
consider the approximation, for any s ∈ N∗ and any r ∈ N∗. Now let us set the uniform coupling
τ -coefﬁcients as τ (p)
(cid:101)X

(cid:13) supu∈[0,1] | (cid:101)Xs(u) − (cid:101)X◦

(cid:13)p, then,

s (u)|(cid:13)

(s) ≡ (cid:13)

τ (p)
(cid:101)X

(s) ≤ (cid:13)

(cid:13) sup
u∈[0,1]

≤ (cid:13)

(cid:13) sup
u∈[0,1]

(cid:12)
(cid:12)Fθ∗(u)

(cid:0)( (cid:101)Xs−k(u))k≥1, ξt

(cid:1) − Fθ∗(u)

(cid:0)( (cid:101)X◦

s−k(u))k≥1, ξ◦
t

(cid:1)(cid:12)
(cid:12)

(cid:13)
(cid:13)p

(cid:12)
(cid:12)Fθ∗(u)

(cid:0)( (cid:101)Xs−k(u))k≥1, ξs

(cid:1) − Fθ∗(u)

(cid:0)( (cid:101)X◦

s−k(u))k≥1, ξ◦
s

(cid:1)(cid:12)
(cid:12)

(cid:13)
(cid:13)p

≤

∞
(cid:88)

k=1

b(0)
t

(Θ)(cid:13)

(cid:13) sup
u∈[0,1]

(cid:12)
(cid:12) (cid:101)Xs−k(u) − (cid:101)X◦

s−k(u)(cid:12)
(cid:12)

(cid:13)
(cid:13)p

≤ B0(Θ) max

s−r≤t≤s−1

(cid:13)
(cid:13) sup
u∈[0,1]

(cid:12)
(cid:12) (cid:101)Xt(u) − (cid:101)X◦

t (u)(cid:12)
(cid:12)

(cid:13)
(cid:13)p

∞
(cid:88)

+ 2

k=r+1

b(0)
t

(Θ)(cid:13)

(cid:13) sup
u∈[0,1]

| (cid:101)X0(u)|(cid:13)
(cid:13)p.

(54)

By a recursive argument we easily derive that maxt≥0
extend the bound so that

(cid:13)
(cid:13) supu∈[0,1]

(cid:12)
(cid:12) (cid:101)Xt(u) − (cid:101)X◦

t (u)(cid:12)
(cid:12)

(cid:13)
(cid:13)p < ∞. Then we

max
t≥s

(cid:13)
(cid:13) sup
u∈[0,1]

| (cid:101)Xt(u) − (cid:101)X◦

t (u)|(cid:13)
(cid:13)p

≤ B0(Θ) max
t≥s−r

(cid:13)
(cid:13) sup
u∈[0,1]

| (cid:101)Xt(u) − (cid:101)X◦

t (u)|(cid:13)

(cid:13)p + 2

∞
(cid:88)

k=r+1

b(0)
t

(Θ)(cid:13)

(cid:13) sup
u∈[0,1]

| (cid:101)X0(u)|(cid:13)

(cid:13)p .

(55)

Given any r ∈ N∗, a recursive argument on the sequence (cid:0) maxt≥kr
based on (55) together with the coupling properties of the τ -dependent coefﬁcient (53) yields the fol-
lowing Lemma that is analog to Theorem 3.1 of [21]. We refer to [21] for more details.

(cid:13)
(cid:13) supu∈[0,1] | (cid:101)Xt(u)− (cid:101)X◦

t (u)|(cid:13)
(cid:13)p

(cid:1)

k≥0

Lemma 6.2. Let Θ ⊂ Rd such that (A0(Θ)) holds with B0(Θ) < 1 and assume that (LS(ρ)) also hold.
Then the stationary version ( (cid:101)Xt(u))t∈Z is uniformly approximated by its coupling version ( (cid:101)X◦
t (u)) so
that we have

τ (p)
(cid:101)X

(s) ≤ C λs

where λs = inf

1≤r≤s

(cid:16)

B0(Θ)s/r +

∞
(cid:88)

t=r+1

(cid:17)

b(0)
t

(Θ)

for s ≥ 1.

(56)

28

Remark that the bound on the τ -coefﬁcients does not depend on u ∈ (0, 1). Notice also that:

• if b(0)
t
• if b(0)
t

(Θ) = O(cid:0)t−κ(cid:1) with κ > 1, then τ (p)
(cid:101)X
(Θ) = O(cid:0)rt(cid:1) with 0 < r < 1, then τ (p)
(cid:101)X

(s) ≤ λs = O(cid:0)s1−κ log s(cid:1);

√

(s) ≤ λs = O(cid:0)e

s log(r) log(B0(Θ))(cid:1).

(57)

The SLLN in [21] is implied by the summability of the τ -dependence coefﬁcients. We will use the
following Lemma on the τ -dependence coefﬁcients τ (p)
(cid:101)X

(s), s ≥ 1.

Lemma 6.3.

If (cid:80)∞

t=2 t log(t)b(0)

t

(Θ) < ∞ then (cid:80)∞

s=1 λs < ∞.

Proof. Choosing r = (cid:98)s/C log(s)(cid:99) for s ≥ 2 and C > 0 we have

λs ≤ s−C log(1/B0(Θ)) +

∞
(cid:88)

b(0)
t

(Θ).

t=(cid:98)s/C log(s)(cid:99)

For C > 0 sufﬁciently large and since B0(Θ) < 1, we get (cid:80)∞
s > e quote that if t = s/C log(s) we have s > Ct log(t) and inverting of sums yields:

s=1 s−C log(1/B0(Θ)) < ∞. Moreover, for

∞
(cid:88)

∞
(cid:88)

s=3

t=(cid:98)s/C log(s)(cid:99)

b(0)
t

(Θ) ≤ C

∞
(cid:88)

t=1

t log(t)b(0)

t

(Θ) < ∞ ,

and the desired result follows.

6.3. Coupling of local-stationary processes

The τ -dependence properties of the stationary process comes from the coupling scheme where we
uniformly approximate the stationary version ( (cid:101)Xt(u)) with a copy ( (cid:101)X∗
t (u)) that is independent of the
past ( (cid:101)Xt(u))t≤0. The weak dependence notion will be used in order to get the uniform SLLN over
functional of ( (cid:101)Xt(u)). The goal of this section is to extend such coupling approach to non-stationary
processes (X(n)
) in the sense that one approximates (X(n)
) in Lp with a certain coupled version. A
useful remark is that we do not use the stationarity of the coupling process ( (cid:101)X∗
t (u)) for obtaining the
recursion (55). Thus a similar coupling scheme can be extended to the local stationary process (X(n)
)
but locally only. In order to localize, we deﬁne u ∈ [ε, 1 − ε], ε > 0 and n large enough the quantities

t

t

t

in(u) := [n(u − chn)] ≥ 1

and jn(u) := [n(u + chn)] ≤ n,

(58)

where we recall that the compact support of the kernel K is included in [−c, c].

Deﬁnition 6.2 (Coupling process).
cess (X∗
t (u))in≤t≤jn by

In the time-window {in(u), in(u) + 1, . . . , jn(u)} deﬁne the pro-

X∗

t (u) =

(cid:40)

X(n)
,
t
Fθ∗(u)

(cid:0)(X∗

t−k(u))k≥1, ξt

t < in(u),

(cid:1), in(u) ≤ t ≤ jn(u).

(59)

Contrast estimation of locally stationary processes

29

Notice that for the ease of notation and as n is ﬁxed sufﬁciently large in this section, we suppress the
dependence in n on the coupling process. We ﬁrst have to prove the existence of the process (X∗
t (u)).

Lemma 6.4. Let Θ ⊂ Rd be such that (A0(Θ)) holds with B0(Θ) < 1 and assume that (LS(ρ)) also
holds. Then, for any u ∈ (0, 1), there exists a.s. a unique coupling process (X∗
t (u))t∈Z satisfying (59)
and there exists a positive constant C∗ > 0 such that

(cid:13)
(cid:13) sup
u∈[ε,1−ε]

(cid:12)
(cid:12)X∗

in(u)+s(u)(cid:12)
(cid:12)

(cid:13)
(cid:13)p ≤ C∗ n1/p

for all 0 ≤ s ≤ 2c nhn.

(cid:12)Un(ε)(cid:12)

Proof of Lemma 6.4. We use a chaining argument adapted to our framework. We denote (uk) where
uk = (k + 1/2 + chn)/n for k ∈ Un(ε) = (cid:8)[εn − chn − 1/2], . . . , [(1 − ε)n − chn − 1/2](cid:9) a grid of
points of the segment [ε, 1−ε] and therefore (cid:12)
(cid:12) (cid:39) (1−2ε)n ≤ n. Moreover, for each u ∈ [ε, 1−ε]
there exists uk such that |u − uk| ≤ 1/(2n) and therefore in(u) = in(uk) with in(u) deﬁned in (58).
Then a chaining argument shows that
in(u)+s(u)(cid:12)

in(v)+s(v)(cid:12)
(cid:12) .
(60)
Using the inequality max(|x|, |y|) ≤ |x| + |y| and similar argument than in the proof of Lemma 2.1,
we get that

in(uk)+s(uk)(cid:12)
(cid:12) +

sup
u,v: in(u)=in(v)

in(u)+s(u) − X∗

sup
u∈[ε,1−ε]

(cid:12) ≤ max

k∈Un(ε)

(cid:12)
(cid:12)X∗

(cid:12)
(cid:12)X∗

(cid:12)
(cid:12)X∗

(cid:13)
(cid:13) max

k∈Un(ε)

(cid:12)
(cid:12)X∗

in(uk)+s(uk)(cid:12)
(cid:12)

(cid:13)
(cid:13)p ≤

(cid:13)
(cid:13)
(cid:13)

(cid:88)

k∈Un(ε)

(cid:12)
(cid:12)X∗

in(uk)+s(uk)(cid:12)
(cid:12)

(cid:13)
(cid:13)
(cid:13)p

(cid:16) (cid:88)

≤

(cid:13)
(cid:13)X∗

in(uk)+s(uk)(cid:13)
p
(cid:13)
p

(cid:17)1/p

k∈Un(ε)
≤ n1/p C0(Θ)

1 − B0(Θ)

,

(61)

since (cid:107)X∗
(cid:13)
(cid:13)
(cid:13) supu,v: in(u)=in(v)
argument,

in(u)+s(u)(cid:107)p ≤ C0(Θ)/(1 − B0(Θ)) for any u ∈ [ε, 1 − ε] and 0 ≤ s ≤ 2c nhn. Set δn =
. We derive from an application of the chaining

in(u)+s(u) − X∗

in(v)+s(v)(cid:12)
(cid:12)

(cid:12)
(cid:12)X∗

(cid:13)
(cid:13)
(cid:13)p

δn ≤

(cid:13)
(cid:13)
(cid:13)

sup
u,v: in(u)=in(v)

(cid:12)
(cid:12)Fθ∗(in(u)+s)

(cid:0)(X∗

in(u)+s−k(u))k≥1, ξin(u)+s

(cid:1)

−Fθ∗(in(u)+s)

(cid:0)(X∗

in(v)+s−k(v))k≥1, ξin(v)+s

+

(cid:13)
(cid:13)
(cid:13)

sup
u,v: in(u)=in(v)

(cid:12)
(cid:12)Fθ∗(in(u)+s)

(cid:0)(X∗

in(v)+s−k(v))k≥1, ξin(v)+s

(cid:1)

−Fθ∗(in(v)+s)

(cid:0)(X∗

in(v)+s−k(v))k≥1, ξin(v)+s

(cid:1)(cid:12)
(cid:12)

(cid:13)
(cid:13)
(cid:13)p

(cid:1)(cid:12)
(cid:12)

(cid:13)
(cid:13)
(cid:13)p

≤

+

∞
(cid:88)

k=1

b(0)
k (Θ)

(cid:13)
(cid:13)
(cid:13)

sup
u,v: in(u)=in(v)

sup
u,v: in(u)=in(v)
(cid:13)
(cid:13)θ∗(u) − θ∗(v)(cid:13)
(cid:13)

(cid:12)
(cid:12)X∗

in(u)+s−k(u) − X∗

in(v)+s−k(v)(cid:12)
(cid:12)

(cid:13)
(cid:13)
(cid:13)p

30

(cid:16) ∞
(cid:88)

×

k=1

b(1)
k (Θ)

(cid:13)
(cid:13)
(cid:13) sup
u∈[ε,1−ε]

(cid:12)
(cid:12)X∗

in(u)+s−k(u)(cid:12)
(cid:12)

(cid:13)
(cid:13)
(cid:13)p

+

(cid:13)
(cid:13)
(cid:13) sup
θ∈Θ

(cid:13)
(cid:13)∂1

θFθ(0; ξ0)(cid:13)
(cid:13)

(cid:17)

(cid:13)
(cid:13)
(cid:13)p

sup
u,v:in(u)=in(v)

(cid:12)
(cid:12)X∗

in(u)+s−k(u) − X∗

in(v)+s−k(v)(cid:12)
(cid:12)

(cid:13)
(cid:13)
(cid:13)p

≤ B0(Θ)

(cid:13)
(cid:13)
(cid:13)
+ Kθ n−ρ(cid:13)
(cid:13)
(cid:13) sup
θ∈Θ

(cid:13)
(cid:13)∂1

θFθ(0; ξ0)(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)p

+ Kθn−ρB1(Θ)

(cid:16) C0(Θ)n1/p
1 − B0(Θ)

+

(cid:13)
(cid:13)
(cid:13)

sup
u,v:in(u)=in(v)

(cid:12)
(cid:12)X∗

in(u)+s−k(u) − X∗

in(v)+s−k(v)(cid:12)
(cid:12)

(cid:17)

,

(cid:13)
(cid:13)
(cid:13)p

from (7), (60) and (61). Collecting all those bounds for n sufﬁciently large in order that n−ρ is sufﬁ-
ciently small, we get

(cid:13)
(cid:13)
(cid:13)

sup
u,v: in(u)=in(v)

(cid:12)
(cid:12)X∗

in(u)+s(u) − X∗

in(v)+s(v)(cid:12)
(cid:12)

(cid:13)
(cid:13)
(cid:13)p

KθB1(Θ)C0(Θ)
1 − B0(Θ)

n1/p−ρ + Kθ n−ρ(cid:13)
1 − B0(Θ) − Kθ n−ρB1(Θ)

(cid:13) supθ∈Θ |∂1

θFθ(0; ξ0)|(cid:13)
(cid:13)p

.

≤

Finally, applying again the chaining argument we obtain

(cid:107) max

k∈Un(ε)

(cid:12)
(cid:12)X∗

in(uk)+s(uk)(cid:12)

(cid:12)(cid:107)p ≤ n1/p C0(Θ)
1 − B0(Θ)

+ O(n1/p−ρ) .

We point out that (X∗
this is a satisfactory non-stationary approximation of (X(n)
bound:

t (u)) is not a copy of (X(n)

t

t

) as it does not follow the same distribution. However

) because we obtain the following coupling

Lemma 6.5. Under Assumptions (A0(Θ)) with B0(Θ) < 1, (A1(Θ)) and (LS(ρ)), with (X∗
the coupling process deﬁned in (59), there exists a positive constant C(cid:48) > 0 such that

t (u))

(cid:13)
(cid:13) sup
u∈[ε,1−ε]

(cid:12)
(cid:12)X(n)

in(u)+s − X∗

in(u)+s(u)(cid:12)
(cid:12)

(cid:13)
(cid:13)p ≤ C(cid:48) hρ

nn1/p

for all 0 ≤ s ≤ 2c nhn.

(62)

Proof of Lemma 6.5. Deﬁne ∆∗
(cid:12)
(cid:12)X(n)
supu∈[ε,1−ε]

in(u)+s − X∗

in(u)+s(u)(cid:12)

s = 0 for any s ≤ 0 and for 0 ≤ s ≤ 2c nhn set the quantity ∆∗

s =

(cid:12). For 0 ≤ s ≤ 2c nhn we decompose

∆∗

s = sup

u∈[ε,1−ε]

(cid:12)
(cid:12)F

θ(n)
in(u)+s

(cid:0)(X(n)

in(u)+s−k)k≥1, ξin(u)+s

(cid:1)

−Fθ∗(u)

(cid:0)(X∗

in(u)+s−k(u))k≥1, ξin(u)+s

(cid:1)(cid:12)
(cid:12)

≤ sup

u∈[ε,1−ε]

(cid:12)
(cid:12)F

θ(n)
in(u)+s

(cid:0)(X(n)

in(u)+s−k)k≥1, ξin(u)+s

(cid:1)

Contrast estimation of locally stationary processes

31

−F

θ(n)
in(u)+s

(cid:0)(X∗

in(u)+s−k(u))k≥1, ξin(u)+s

(cid:1)(cid:12)
(cid:12)

+ sup

u∈[ε,1−ε]

(cid:12)
(cid:12)F

θ(n)
in(u)+s

(cid:0)(X∗

in(u)+s−k(u))k≥1, ξin(u)+s

(cid:1)

−Fθ∗(u)((X∗

in(u)+s−k(u))k≥1, ξin(u)+s

(cid:1)(cid:12)
(cid:12).

Then we derive

(cid:107)∆∗

s(cid:107)p ≤

(cid:13)
(cid:13)
(cid:13)

sup
u∈[ε,1−ε]

(cid:12)
(cid:12)F

θ(n)
in(u)+s

(cid:0)(X(n)

t−k)k≥1, ξt

(cid:1)

−F

θ(n)
in(u)+s

(cid:0)(X∗

in(u)+s−k(u))k≥1, ξin(u)+s

(cid:1)(cid:12)
(cid:12)

(cid:13)
(cid:13)
(cid:13)p

+

(cid:13)
(cid:13)
(cid:13)

sup
u∈[ε,1−ε]

(cid:12)
(cid:12)F

θ(n)
in(u)+s

(cid:0)(X∗

in(u)+s−k(u))k≥1, ξt

(cid:1)

−Fθ∗(u)((X∗

in(u)+s−k(u))k≥1, ξin(u)+s

(cid:1)(cid:12)
(cid:12)

(cid:13)
(cid:13)
(cid:13)p

≤

(cid:13)
(cid:13)
(cid:13)

∞
(cid:88)

k=1

b(0)
k (Θ)

sup
u∈[ε,1−ε]

(cid:12)
(cid:12)X(n)

in(u)+s−k − X∗

in(u)+s−k(u)(cid:12)
(cid:12)

(cid:13)
(cid:13)
(cid:13)p

+ sup
u∈[ε,1−ε]

(cid:13)
(cid:13)θ(n)

(cid:13)
in(u)+s −θ∗(u)(cid:13)
(cid:13)
(cid:13) sup
(cid:13)
θ∈Θ

sup
u∈[ε,1−ε]

(cid:13)
(cid:13)∂1

θFθ((X∗

(cid:13)
in(u)+s−k(u))k≥1, ξin(u)+s)(cid:13)
(cid:13)
(cid:13)
(cid:13)p

≤

(cid:13)
(cid:13)
(cid:13)

∞
(cid:88)

k=1

b(0)
k (Θ) ∆∗

s−k

(cid:13)
(cid:13)
(cid:13)p

+ sup

u∈[ε,1−ε]

(cid:13)
(cid:13)θ(n)

in(u)+s − θ∗(u)(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

×

sup
u∈[ε,1−ε]

∞
(cid:88)

k=1

k (Θ) (cid:12)
b(1)

(cid:12)X∗

in(u)+s−k(u)(cid:12)

(cid:12) + sup
θ∈Θ

(cid:13)
(cid:13)∂1

(cid:13)
θFθ(0; ξin(u)+s)(cid:13)
(cid:13)
(cid:13)
(cid:13)p

≤

∞
(cid:88)

k=1

k (Θ) (cid:13)
b(0)

(cid:13)∆∗

s−k

(cid:13)
(cid:13)p + sup

u∈[ε,1−ε]

(cid:13)
(cid:13)θ(n)

in(u)+s − θ∗(u)(cid:13)
(cid:13)

(cid:16) ∞
(cid:88)

×

k=1

b(1)
k (Θ)

(cid:13)
(cid:13)
(cid:13) sup
u∈[ε,1−ε]

(cid:12)
(cid:12)X∗

in(u)+s−k(u)(cid:12)
(cid:12)

(cid:13)
(cid:13)
(cid:13)p

+

(cid:13)
(cid:13)
(cid:13) sup
θ∈Θ

(cid:13)
(cid:13)∂1

(cid:13)
θFθ(0; ξ0)(cid:13)
(cid:13)
(cid:13)
(cid:13)p

(cid:17)

by using the Assumptions (A0(Θ)) and (A1(Θ)).

Now deﬁne M ∗
2c nhn it holds

t = maxs≤t

(cid:13)
(cid:13)∆∗
s

(cid:13)
(cid:13)p. We derive from an application of Lemma 6.4 that for any 0 ≤ t ≤

(cid:13)
(cid:13)∆∗
t

(cid:13)
(cid:13)p ≤ B0(Θ) M ∗

t−1 + sup

u∈[ε,1−ε]

(cid:13)
(cid:13)θ(n)

in(u)+t − θ∗(u)(cid:13)

(cid:13) ×

(cid:16)

(cid:17)
C∗ n1/p + C1(Θ)

.

32

We have

sup
u∈[ε,1−ε]

(cid:13)
(cid:13)θ(n)

in(u)+t − θ∗(u)(cid:13)

(cid:13) ≤ c Kθ hρ

n from condition (7) of Assumption (LS(ρ)). As a

consequence, for any 0 ≤ t ≤ 2c nhn, we have

M ∗

t ≤ B0(Θ) M ∗

t−1 + c Kθ

(cid:16)

(cid:17)
C∗n1/p + C1(Θ)

hρ
n.

By deﬁnition, M ∗

0 = 0. Therefore we deduce for any 0 ≤ t ≤ 2c nhn,

M ∗

t ≤

c Kθ
1 − B0(Θ)

(cid:16)

C∗n1/p + C1(Θ)

(cid:17)

hρ
n.

This completes the proof of the Lemma 6.5.

Finally, the coupling process (X∗
the stationary version (cid:101)Xt(u) for t/n (cid:39) u.

t (u))t∈Z is also used for estimating the approximation of X(n)

t with

Lemma 6.6. Under Assumptions (A0(Θ)) with B0(Θ) < 1, (A1(Θ)) and (LS(ρ)) there exists a
positive constant C(cid:48)(cid:48) > 0 such that

(cid:13)
(cid:13) sup
u∈[ε,1−ε]

(cid:12)
(cid:12)X(n)

in(u)+s − (cid:101)Xin(u)+s(u)(cid:12)
(cid:12)

(cid:13)
(cid:13)p ≤ C(cid:48)(cid:48) n1/p(cid:0)hρ

n + λs

(cid:1),

for all 0 ≤ s ≤ 2c nhn.

(63)

Proof of Lemma 6.6. The approximation is derived since the coupling process is a non stationary cou-
pling version of (cid:101)X◦
t (u)) the coupling process deﬁned in (59)
and repeating the same arguments than above, we obtain a similar recursive relation than (55) on the
sequence

t (u) deﬁned in (52). Indeed, using (X∗

(cid:16)

(cid:13)
(cid:13)
(cid:13)

max
t≥kr

sup
u∈[ε,1−ε]

(cid:12)
(cid:12) (cid:101)Xin(u)+t(u) − X∗

in(u)+t(u)(cid:12)
(cid:12)

(cid:17)

(cid:13)
(cid:13)
(cid:13)p

k≥0

given any r ∈ N∗, using Lemma 6.4 and the estimates

(cid:13)
(cid:13)
(cid:13)

sup
u∈[ε,1−ε]

(cid:13)
(cid:12) (cid:101)Xin(u)+s(u)(cid:12)
(cid:12)
(cid:13)
(cid:12)
(cid:13)p

≤

(cid:13)
(cid:13)
(cid:13) max
1≤t≤n

sup
u∈[ε,1−ε]

(cid:12) (cid:101)Xt(u)(cid:12)
(cid:12)
(cid:12)

(cid:13)
(cid:13)
(cid:13)p

(cid:16) (cid:88)

≤

1≤t≤n

(cid:13)
(cid:13)
(cid:13)

sup
u∈[ε,1−ε]

(cid:12) (cid:101)Xt(u)(cid:12)
(cid:12)
(cid:12)

(cid:13)
p
(cid:13)
(cid:13)
p

(cid:17) 1

p ≤ C n

1
p .

We obtain
(cid:13)
(cid:13)
(cid:13) sup
u∈[ε,1−ε]

(cid:12)
(cid:12)X∗

(cid:13)
in(u)+s(u) − (cid:101)Xin(u)+s(u)(cid:12)
(cid:13)
(cid:12)
(cid:13)p

≤ C n1/p λs ,

for

0 ≤ s ≤ 2c nhn,

s with the stationary version (cid:101)Xs(u), namely for for all 0 ≤ s ≤ 2c nhn,

with λs deﬁned in (56). Combining this result with (63) we bound the Lp norm of the approximation
of X(n)
(cid:13)
(cid:13)
(cid:13) sup
u∈[ε,1−ε]
(cid:13)
(cid:13)
(cid:13) sup
u∈[ε,1−ε]

in(u)+s − (cid:101)Xin(u)+s(u)(cid:12)
(cid:12)

in(u)+s(u) − (cid:101)Xin+s(u)(cid:12)
(cid:12)

(cid:13)
(cid:13)
(cid:13) sup
u∈[ε,1−ε]

in(u)+s − X∗

in(u)+s(u)(cid:12)
(cid:12)

(cid:12)
(cid:12)X(n)

(cid:12)
(cid:12)X(n)

(cid:12)
(cid:12)X∗

(cid:13)
(cid:13)
(cid:13)p

(cid:13)
(cid:13)
(cid:13)p

(cid:13)
(cid:13)
(cid:13)p

+

≤

Contrast estimation of locally stationary processes

33

≤ C n1/p(cid:0)hρ

n + λs

(cid:1).

7. Proofs for the Section 3

7.1. Some useful lemmas

Proof of Lemma 3.1. For θ ∈ Θ, t ∈ Z and m ∈ N, deﬁne

φt,m = Φ(cid:0)X(n)

t

, X(n)

t−1, . . . , X(n)

t−m, 0, 0, . . . ; θ(cid:1).

As Φ ∈ Lip p(Θ) the sequence (φt,m)m∈N is a Cauchy sequence in Lq since for any m2 > m1

(cid:13)
(cid:13)φt,m2 − φt,m1

(cid:13)
(cid:13)1 ≤ g(cid:0)

sup
0≤s≤m2

(cid:8)(cid:107)X(n)

t−s(cid:107)p

(cid:9)(cid:1)

m2(cid:88)

k=m1+1

αk(h, Θ) (cid:107)X(n)

t−k(cid:107)p

m2(cid:88)

≤ C

k=m1+1

αk(h, Θ)

(64)

(65)

from Lemma 2.1, since if s < 0 then Xs = 0, thus the corresponding supremum bound extends over
each s ≤ n.
As (cid:80)∞
k=m1+1 αk(h, Θ) ≤ ε for m1 and m2 large
enough. Using the completeness of L1 we deduce the consistency of the sequence (φt,m)m∈N and the
existence in L1 of its limit Φ(cid:0)(X(n)

k=1 αk(h, Θ) < ∞ we deduce that for any ε > 0, (cid:80)m2

t−k)k≥0, θ(cid:1).

When θ(n)
as ( (cid:101)Xt−k(u))k∈N is a stationary ergodic process, this is also the case for (cid:0)Φ(cid:0)( (cid:101)Xt−k(u))k∈N, θ(cid:1)(cid:1)
(see Corollary 2.1.3. in [32]).

t = θ∗(u) for any t, n, we consider Φ(cid:0)( (cid:101)Xt−k(u))k∈N, θ(cid:1) that also exists in L1. Moreover,
t∈Z

Lemma 7.1.

Let in(u)) and jn(u) deﬁned in (58).

1. Let Z(u) = (Zt(u))t∈N be a centered stationary process on a Banach space (B, (cid:107) · (cid:107)) for
any 0 ≤ u ≤ 1. If Z(u) is an ergodic process continuous with respect to u and satisfying
E[sup0≤u≤1 (cid:107)Z0(u)(cid:107)] < ∞ then we have

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n hn

jn(u)
(cid:88)

t=in(u)

sup
0<u<1

Zt(u) K

(cid:17)

(cid:16) t

n − u
hn

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

a.s.
−→
n→+∞

0.

(66)

2. Let Z = (Zt)t∈N be a centered stationary process on Rd such as E(cid:2)(cid:107)Z0(cid:107)2(cid:3) < ∞ and 0 < u < 1.

If 1√
n

(cid:80)n

t=1 Zt

L
−→
n→+∞

Nd

(cid:0)0 , Σ(cid:1) with Σ a positive deﬁnite symmetric matrix, then we have

√

1
n hn

jn(u)
(cid:88)

t=in(u)

Zt K

(cid:17)

(cid:16) t

n − u
hn

L
−→
n→+∞

(cid:16)

0 ,

N

(cid:16) (cid:90)

R

(cid:17)
K2(x) dx

(cid:17)

.

Σ

(67)

34

Proof of Lemma 7.1. Let (cid:96) ∈ N∗, [−c, c] be the compact support of K and let n be large enough
j = (cid:2) − c +
such as n(u − c hn) ≥ 1 and nu + c nhn ≤ n. Then, for j ∈ {1, . . . , (cid:96)}, we denote I((cid:96))
2c j−1
(cid:96)

, −c + 2c j
(cid:96)

(cid:3) , T ((cid:96))

t ∈ N,

j =

∈ Ij

and

t
n −u
hn

(cid:111)

(cid:110)

S((cid:96))
n (u) =

1
nhn

jn(u)
(cid:88)

t=in(u)

Zt(u) K

(cid:17)

(cid:16) t

n − u
hn

and S((cid:96))

n,j(u) =

1
nhn

(cid:88)

t∈T ((cid:96))
j

Zt(u) .

(68)

Below, we will omit the reference to the dependence with respect to u when no confusion will be
possible.
1. We notice that (Zt(u)) is a centered ergodic process on the Banach space L1(C([0, 1], B)). Thus for
any ﬁxed (cid:96) ∈ N∗ such that Card(T ((cid:96))
∞ we apply the uniform ergodic theorem
and since E[Z0(u)] = 0 for any 0 < u < 1 we obtain

) (cid:39) 2cnhn/(cid:96) −→

n→+∞

j

Denote t((cid:96))

j = −c + c 2j−1

(cid:96)

sup
0≤u≤1

(cid:107)S((cid:96))

n,j(u)(cid:107)

a.s.
−→
n→+∞

0.

, the midpoint of T ((cid:96))

j

, then we have

(69)

S((cid:96))
n (u) =

(cid:16)

K

(cid:96)
(cid:88)

j=1

t((cid:96))
j
n − u
hn

(cid:17)

S((cid:96))
n,j(u) +

(cid:96)
(cid:88)

j=1

1
nhn

(cid:88)

(cid:104)
Zt(u)

K

t∈T ((cid:96))
j

(cid:17)

(cid:16) t

n −u
hn

(cid:16)

− K

t((cid:96))
j
n −u
hn

(cid:17)(cid:105)

.

(70)

First, since K is a bounded function and from (69), then for any (cid:96) ∈ N∗ we obtain

(cid:16)

K

(cid:13)
(cid:13)
(cid:13)

(cid:96)
(cid:88)

j=l

t((cid:96))
j
n − u
hn

sup
0≤u≤1

(cid:17)

(cid:13)
S((cid:96))
(cid:13)
n,j(u)
(cid:13)

a.s.
−→
n→+∞

0.

(71)

Second, since K is a C1 function on [−c, c] it holds

sup
0≤u≤1

max
1≤j≤(cid:96)

sup
t∈T ((cid:96))
j

(cid:12)
(cid:12)
(cid:12)K

(cid:16) t

n − u
hn

(cid:17)

(cid:16)

− K

t((cid:96))
j
n − u
hn

(cid:17)(cid:12)
(cid:12)
(cid:12) ≤

c
(cid:96)

(cid:107)K(cid:48)(cid:107)∞

for any (cid:96) ∈ N∗ and any n ∈ N. Then we obtain

sup
0≤u≤1

(cid:13)
(cid:13)
(cid:13)

(cid:96)
(cid:88)

j=1

1
nhn

(cid:88)

Zt(u)

t∈T ((cid:96))
j

(cid:104)

K

(cid:17)

(cid:16) t

n − u
hn

(cid:16)

− K

t((cid:96))
j
n − u
hn

(cid:17)(cid:105)(cid:13)
(cid:13)
(cid:13)

≤ (cid:107)K(cid:48)(cid:107)∞

c
(cid:96)

·

1
nhn

n
(cid:88)

t=1

sup
0<u<1

(cid:107)Zt(u)(cid:107).

Contrast estimation of locally stationary processes

35

The ergodicity of (cid:0) sup0<u<1 (cid:107)Zt(u)(cid:107)(cid:1)
yield

t, its stationarity and E[sup0<u<1 (cid:107)Z0(u)(cid:107)] < ∞ together

1
nhn

jn(u)
(cid:88)

t=in(u)

sup
0<u<1

(cid:107)Zt(u)(cid:107)

(cid:104)
E

a.s.
−→
n→+∞

sup
0<u<1

(cid:105)
(cid:107)Z0(u)(cid:107)

.

Thus, for any ε > 0, there exists a.s. ((cid:96)0, n0) such as for any (cid:96) ≥ (cid:96)0 and n ≥ n0,

sup
0<u<1

(cid:13)
(cid:13)
(cid:13)

(cid:96)
(cid:88)

j=1

1
nhn

(cid:88)

Zt(u)

t∈T ((cid:96))
j

(cid:104)

K

(cid:17)

(cid:16) t

n − u
hn

(cid:16)

− K

t((cid:96))
j
n − u
hn

(cid:17)(cid:105)(cid:13)
(cid:13)
(cid:13) ≤ ε

a.s.

(72)

From (70), (71) and (72), we deduce (66).

2. Consider ﬁrst K = K(cid:96) the piecewise constant function K(cid:96)(x) = (cid:80)(cid:96)
ﬁrst that d = 1 with Σ = σ2 > 0 (unidimensional case).

j=1 aj I1

x∈I ((cid:96))
j

, and also assume

S((cid:96))
n =

(cid:96)
(cid:88)

j=1

aj S((cid:96))
n,j,

with S((cid:96))

n,j deﬁned in (68). Using Card(T ((cid:96))

j

) ∼ 2cnhn/(cid:96) −→

n→+∞

∞, for any j ∈ {1, . . . , (cid:96)},

(cid:16)

(cid:96)
2cnhn

(cid:17)1/2 (cid:88)

(cid:114)

Zt =

t∈T ((cid:96))
j

nhn(cid:96)
2c

S((cid:96))
n,j

L
−→
n→+∞

(cid:16)

0 , σ2(cid:17)

,

N

(73)

(74)

where σ2 = (cid:80)
we have for any j, j(cid:48) ∈ {1, . . . , (cid:96)} such as j (cid:54)= j(cid:48),

t∈Z E[Z0Zt] is such as 0 < (cid:80)

t∈Z E[Z0Zt] < ∞. Moreover, using the stationarity of Z,

nhn Cov (cid:0)S((cid:96))

n,j , S((cid:96))
n,j(cid:48)

(cid:1) =

1
nhn

(cid:88)

(cid:88)

E[ZtZt(cid:48) ] =

t∈T ((cid:96))
j

t(cid:48)∈T ((cid:96))
j(cid:48)

1
nhn

(cid:88)

(cid:88)

E[Z0Zt(cid:48)−t]

t∈T ((cid:96))
j

t(cid:48)∈T ((cid:96))
j(cid:48)

=⇒

(cid:12)
(cid:12)nhn Cov (cid:0)S((cid:96))
(cid:12)

n,j , S((cid:96))
n,j(cid:48)

(cid:1)(cid:12)
(cid:12)
(cid:12) ≤ C

(cid:12)
(cid:12)
(cid:12)

(cid:88)

k>T ((cid:96))
j

E[Z0Zk]

(cid:12)
(cid:12)
(cid:12) −→
n→+∞

0,

as soon as (cid:96) = o(nhn) since (cid:80)
combinations of S((cid:96))

(cid:12)
(cid:12)E[Z0Zt](cid:12)
t∈Z
n,j. E.g. for S((cid:96))
n deﬁned in (73),

(cid:12) < ∞. Hence a central limit theorem holds for any linear

(cid:114)

nhn(cid:96)
2c

S((cid:96))
n

L
−→
n→+∞

(cid:16)

0 ,

N

(cid:96)
(cid:88)

j=1

a2
j

(cid:88)

t∈Z

(cid:17)
E[Z0Zt]

=⇒

(cid:112)

nhn S((cid:96))
n

L
−→
n→+∞

N

(cid:16)

0 , σ2

(cid:90)

R

K2

(cid:17)
(cid:96) (x) dx

,

(75)

36

since for piecewise kernel we have

(cid:90)

R

K2

(cid:96) (x) dx =

2c
(cid:96)

(cid:96)
(cid:88)

j=1

a2
j .

Consider now a general piecewise differentiable kernel K and denote K(cid:96) such as K(cid:96)(x) = (cid:80)(cid:96)
for x ∈ R with aj = K(cid:0) − c + c 2j−1

(cid:1) such that

j=1 aj I1x∈Ij

(cid:96)
(cid:90)

K2

(cid:96) (x) dx −→
(cid:96)→+∞

(cid:90)

R

K2(x) dx.

R

The result will follow from Theorem 3.2 in [6]. For this we check that for any ε > 0

lim
(cid:96)→∞

lim sup
n→∞

P

(cid:16)(cid:12)
(cid:12)S((cid:96))
(cid:12)

n − Sn

(cid:12)
(cid:12)
(cid:12) ≥ ε/

(cid:17)

(cid:112)

nhn

= 0, with

Sn =

1
nhn

n
(cid:88)

t=1

Zt K

(cid:16) t

n − u
hn

(cid:17)

.

From Markov inequality

P

(cid:16)(cid:12)
(cid:12)S((cid:96))
(cid:12)

n − Sn

(cid:12)
(cid:12)
(cid:12) ≥ ε/

(cid:112)

(cid:17)

nhn

≤ nhn

E[∆2
n]
ε2

with

Thus

∆n =

1
nhn

=

1
nhn

jn(u)
(cid:88)

Zt K

t=in(u)

(cid:96)
(cid:88)

(cid:88)

(cid:16)

K

j=1

t∈Tj

(cid:124)

(cid:16) t

n − u
hn

(cid:17)

−

(cid:96)
(cid:88)

j=1

aj S((cid:96))
n,j

(cid:17)

(cid:16) t

n − u
hn

− K(cid:0) − c + 2c

(cid:123)(cid:122)
at,j ((cid:96))

j − 1
(cid:96)

(cid:1)(cid:17)

(cid:125)

Zt .

E[∆2

n] =

1
(nhn)2

≤

1
(nhn)2

(cid:96)
(cid:88)

(cid:88)

(cid:96)
(cid:88)

(cid:88)

j=1

t∈Tj

j(cid:48)=1

t(cid:48)∈T (cid:48)
j

(cid:96)
(cid:88)

(cid:88)

(cid:96)
(cid:88)

(cid:88)

j=1

t∈Tj

j(cid:48)=1

t(cid:48)∈T (cid:48)
j

at,j((cid:96)) at(cid:48),j(cid:48)((cid:96))E[ZtZt(cid:48) ]

|at,j((cid:96))||at(cid:48),j(cid:48) ((cid:96))||E[ZtZt(cid:48) ]| .

Since K is Lipschitz continuous, there exists a constant C > 0 such that

|at,j((cid:96))| ≤ 2

c C
(cid:96)

,

for in ≤ t ≤ jn 1 ≤ j ≤ (cid:96) .

Thus

E[∆2

n] ≤

(cid:19)2

C2
(nhn)2

(cid:18) 2c
(cid:96)

(cid:88)

|E[ZtZt(cid:48) ]| ≤ 8

1≤t,t(cid:48)≤n

C2
nhn

(cid:16) c
(cid:96)

(cid:17)2 (cid:88)

(cid:12)
(cid:12)E[ZtZt(cid:48)](cid:12)
(cid:12) .

t≥0

Then we obtain, again from Markov inequality that, for any ε > 0,

P

(cid:16)(cid:12)
(cid:12)S((cid:96))
(cid:12)

n − Sn

(cid:12)
(cid:12)
(cid:12) ≥ ε/

(cid:112)

nhn

(cid:17)

≤ 2C2 (cid:16) c
(cid:96)

(cid:17)2 (cid:88)

t≥0

(cid:12)
(cid:12)E[ZtZt(cid:48) ](cid:12)

(cid:12) −→
(cid:96)→+∞

0.

Contrast estimation of locally stationary processes

37

Now the extension to d > 1 is standard: consider r = (r1, . . . , rd)T ∈ Rd and a linear combination
(cid:1)T and apply the result obtained for d = 1.
Zt = r1Z(1)
Then the asymptotic covariance matrix rT Σ r > 0 appears to be positive deﬁnite and this implies the
multidimensional central limit theorem.

t where Zt = (cid:0)Z(1)

t + · · · + rdZ(d)

, . . . , Z(d)

t

t

7.2. Proofs of the main results

We will prove (12) in Theorem 3.1 only since the consistency is achieved directly by simple arguments.
We need the following Lemma that is a strong law of large number on the contrast as if the stationary
versions were observed:

Lemma 7.2. Under the assumptions of Theorem 3.1 we have

sup
ε≤u≤1−ε

sup
θ∈Θ

(cid:12)
(cid:12)
(cid:12)

1
nhn

jn(u)
(cid:88)

t=in(u)

Φ( (cid:101)Xt−k(u))k∈N, θ)K(cid:0)

t
n − u
hn

(cid:1) − E(cid:2)Φ(cid:0)( (cid:101)X−k(u))k≥0, θ(cid:1)(cid:3)(cid:12)
(cid:12)
(cid:12)

a.s.
−→
n→+∞

0.

(76)

Proof. The expression I in (76) tends to 0 a.s., if is it the case for I1 and I2 such that,

I1= sup

ε≤u≤1−ε

sup
θ∈Θ

I2= sup

ε≤u≤1−ε

sup
θ∈Θ

(cid:12)
(cid:12)
(cid:12)

1
nhn

(cid:12)
(cid:12)
(cid:12)

1
nhn

jn(u)
(cid:88)

t=in(u)

jn(u)
(cid:88)

t=in(u)

(cid:16)

Φ( (cid:101)Xt−k(u))k∈N, θ) − E(cid:2)Φ(cid:0)( (cid:101)X−k(u))k∈N, θ(cid:1)(cid:3)(cid:17)

K

(cid:16) t

n − u
hn

(cid:17)(cid:12)
(cid:12)
(cid:12)

K

(cid:16) t

n − u
hn

(cid:17)

E(cid:2)Φ(cid:0)( (cid:101)X−k(u))k∈N, θ(cid:1)(cid:3) − E(cid:2)Φ(cid:0)( (cid:101)X−k(u))k∈N, θ(cid:1)(cid:3)(cid:12)
(cid:12)
(cid:12).

1. We use the part 1. of Lemma 7.1 to control I1. For this we deﬁne Z(θ, u) = (Zt(θ, u))t∈Z with
Zt(θ, u) = Φ(cid:0)( (cid:101)Xt−k(u))k∈N, θ) − E(cid:2)Φ(cid:0)( (cid:101)X−k(u))k∈N, θ(cid:1)(cid:3); this is a centered ergodic stationary pro-
cess on the Banach space of the continuous function over Θ × [0, 1] equipped with the uniform norm.
Using E(cid:2) sup(θ,u)∈Θ×[0,1] |Z0(θ, u)|(cid:3) < ∞ since Φ ∈ Lip p(Θ), with Theorem 2.2.1. in [32] we apply
the part 1. of Lemma 7.1 to get

2. For the term I2, notice that

I1

a.s.
−→
n→+∞

0.

I2 ≤ sup

ε≤u≤1−ε

(cid:16)

1 −

(cid:12)
(cid:12)
(cid:12)

1
nhn

sup
θ∈Θ

jn(u)
(cid:88)

t=in(u)

K

(cid:16) t

n − u
hn

(cid:17)(cid:17)

E(cid:2)Φ(cid:0)( (cid:101)X−k(u))k∈N, θ(cid:1)(cid:3)(cid:12)
(cid:12)
(cid:12)

≤ C

sup
ε≤u≤1−ε

(cid:12)
(cid:12)
(cid:12)1 −

1
nhn

jn(u)
(cid:88)

t=in(u)

K

(cid:16) t

n − u
hn

(cid:17)(cid:12)
(cid:12)
(cid:12) ≤

C
nhn

,

(77)

(78)

38

from the usual comparison of a Riemann sum and its integral: indeed K is Lipschitz because it is a
piecewise differentiable function with a compact support.
As a consequence, the proof is complete from (77) and (78).

We also need the uniform approximation of the contrast with its stationary version stated in the next
Proposition.

Proposition 7.1. Under the assumptions of Theorem 3.1 with ( (cid:101)Xt(u))t denoting the stationary pro-
cess deﬁned in (8), we obtain

sup
u∈[ε,1−ε]

sup
θ∈Θ

(cid:12)
(cid:12)
(cid:12)

1
nhn

n
(cid:88)

k=1

Φ(cid:0)(X(n)

k−t)t≥0, θ(cid:1)K

(cid:16) k

n − u
hn

(cid:17)

− E(cid:2)Φ(cid:0)( (cid:101)X−k(u))k≥0, θ(cid:1)(cid:3)(cid:12)
(cid:12)
(cid:12)

P
−→
n→+∞

0.

(79)

Proof of Proposition 7.1. Since Φ ∈ Lip p(Θ) with p ≥ 1, we have

(cid:13)
(cid:13)
(cid:13)

sup
u∈[ε,1−ε]

sup
θ∈Θ

1
nhn

jn(u)
(cid:88)

t=in(u)

(cid:0)Φ(cid:0)(X(n)

t−k))k≥0, θ(cid:1) − Φ(cid:0)( (cid:101)Xt−k(u))k≥0, θ(cid:1)(cid:1) K

(cid:16) t

n − u
hn

(cid:17)(cid:13)
(cid:13)
(cid:13)1

≤

C
nhn

2c nhn(cid:88)

t=0

(cid:12)
(cid:12)
(cid:12)K

(cid:16) in(u)+t
n − u
hn

(cid:17)(cid:12)
(cid:12) g(cid:0)
(cid:12)

sup
s≤jn(u)

(cid:8)(cid:107)X(n)

s (cid:107)p ∨ (cid:107) (cid:101)Xs(u)(cid:107)p

(cid:9)(cid:1)

×

∞
(cid:88)

s=1

αs(Φ, Θ)(cid:13)
(cid:13)

sup
u∈[ε,1−ε]

(cid:12)
(cid:12)X(n)

in(u)+t+1−s − (cid:101)Xin(u)+t+1−s(u)(cid:12)
(cid:12)

(cid:13)
(cid:13)p.

From Assumption (A0(Θ)) and B0(Θ)<1,

(cid:13)
(cid:13)
(cid:13)

sup
u∈[ε,1−ε]

|X(n)

(cid:13)
(cid:13)
in(u)+j − (cid:101)Xin(u)+j(u)|
(cid:13)p

≤ Cn1/p,

for j ≤ 0 using similar arguments as in the proof Lemma 6.6. Moreover with (A1(Θ)) and (LS(ρ)),
we apply 6.6 in order to get

(cid:13)
(cid:13)
(cid:13)

sup
u∈[ε,1−ε]

|X(n)

in(u)+j − (cid:101)Xin(u)+j(u)|

(cid:13)
(cid:13)
(cid:13)p

≤ n1/p(cid:0)hρ

n + λj

(cid:1).

for j ≥ 1. Therefore,

(cid:13)
(cid:13)
(cid:13)

sup
u∈[ε,1−ε]

sup
θ∈Θ

1
nhn

2c nhn(cid:88)

t=0

(cid:0)Φ(cid:0)(X(n)

t−k))k≥0, θ(cid:1) − Φ(cid:0)( (cid:101)Xt−k(u))k≥0, θ(cid:1)(cid:1)K

(cid:16) t

n − u
hn

(cid:17)(cid:13)
(cid:13)
(cid:13)1

≤

C
nhn

2c nhn(cid:88)

CK C∗ (cid:16) t

(cid:88)

t=0

s=1

αs(Φ, Θ) C n1/p(cid:0)hρ

n + λt+1−s

(cid:1) +

t
(cid:88)

s=t+1

αs(Φ, Θ) C n1/p(cid:17)

.

(80)

Then we deduce

(cid:13)
(cid:13)
(cid:13)

sup
u∈[ε,1−ε]

sup
θ∈Θ

1
nhn

2c nhn(cid:88)

t=0

(cid:0)Φ(cid:0)(X(n)

t−k))k≥0, θ(cid:1) − Φ(cid:0)( (cid:101)Xt−k(u))k≥0, θ(cid:1)(cid:1) K

(cid:16) t

n − u
hn

(cid:17)(cid:13)
(cid:13)
(cid:13)1

Contrast estimation of locally stationary processes

39

≤

C n1/p
nhn

jn(u)
(cid:88)

(cid:16) t−in(cid:88)

t=in(u)

s=1

αs(Φ, Θ) (cid:0)λt−s−in + hρ

n

(cid:1) +

∞
(cid:88)

(cid:17)

αs(Φ, Θ)

s=t−in+1

≤

≤

C n1/p
nhn

jn−in
(cid:88)

(cid:16)

k=1

λk

k
(cid:88)

i=1

αi(Φ, Θ) + hρ
n

jn−in
(cid:88)

k
(cid:88)

k=1

i=1

∞
(cid:88)

αi(Φ, Θ) +

(cid:17)

iαi(Φ, Θ)

i=1

Cn1/p
nhn

(cid:16) ∞
(cid:88)

∞
(cid:88)

λk

k=1

i=1

αi(Φ, Θ) + (cid:0)hρ

n + 1(cid:1)

(cid:17)

iαi(Φ, Θ)

∞
(cid:88)

i=1

≤ C

n1/p
nhn

.

(81)

Here we made use of the assumption (cid:80)∞
last bound holds if (cid:80)∞
and the almost sure convergence (76) we obtain the weak consistency result (79).

s=1 sαs(Φ, Θ) < ∞ and of the fact that (cid:80)∞
k=1 λk < ∞. This
t=1 t log(t)bt(Θ) < ∞ and follows from Lemma 6.3. Finally, using (80), (81)

Proof of Theorem 3.1. From the Assumption (Co(Φ, Θ), we have

θ∗(u) = Argmin
θ∈Θ

E(cid:2)Φ(cid:0)( (cid:101)X−t(u))t≥0, θ(cid:1)(cid:3).

The uniform weak law of large numbers implies the uniform convergence, and we need:

sup
u∈[ε,1−ε]

sup
θ∈Θ

(cid:12)
(cid:12)
(cid:12)

1
nhn

n
(cid:88)

k=1

Φ(cid:0)(X(n)

k−t)t≥0, θ(cid:1) K

(cid:16) k

n − u
hn

(cid:17)

− E(cid:2)Φ(cid:0)( (cid:101)X−k(u))k≥0, θ(cid:1)(cid:3)(cid:12)
(cid:12)
(cid:12)

P
−→
n→+∞

0 ,

see the discussion in the Appendix of [13]. From an application of the approximation in Proposition
7.1, this uniform weak law of large number follows from the uniform weak law of large number on the
stochastic version of the contrast, namely

sup
u∈[ε,1−ε]

sup
θ∈Θ

(cid:12)
(cid:12)
(cid:12)

1
nhn

n
(cid:88)

k=1

Φ(cid:0)( (cid:101)Xk−t(u))t≥0, θ(cid:1) K

(cid:16) k

n − u
hn

(cid:17)

− E(cid:2)Φ(cid:0)( (cid:101)X−k(u))k≥0, θ(cid:1)(cid:3)(cid:12)
(cid:12)
(cid:12)

P
−→
n→+∞

0 .

From usual arguments, see for instance [6], this uniform version of the weak law of large number
obtained in Lemma 7.2 will follow from the equicontinuity of the family

(cid:32)

sup
θ∈Θ

(cid:12)
(cid:12)
(cid:12)

1
nhn

n
(cid:88)

k=1

Φ(cid:0)( (cid:101)Xk−t(u))t≥0, θ(cid:1) K

(cid:16) k

n − u
hn

(cid:17)

− E(cid:2)Φ(cid:0)( (cid:101)X−t(u))t≥0, θ(cid:1)(cid:3)(cid:12)
(cid:12)
(cid:12)

(cid:33)

.

u∈[ε,1−ε]

This holds from Markov inequality as Φ ∈ Lip p(Θ) and from the relation

(cid:107) (cid:101)Xt(u) − (cid:101)Xt(u(cid:48))(cid:107)p ≤

(cid:13)
(cid:13)θ∗(u(cid:48)) − θ∗(u)(cid:13)
(cid:13)
1 − B0(Θ)

(cid:16) B1(Θ) C0(Θ)
1 − B0(Θ)

(cid:17)

+ C1(Θ)

,

(82)

as θ∗ is equicontinuous. Indeed, under Ak(Θ), k = 1, 2, we have
(cid:1) − Fθ∗(u(cid:48))

(cid:107) (cid:101)Xt(u) − (cid:101)Xt(u(cid:48))(cid:107)p ≤(cid:107)Fθ∗(u)

(cid:0)( (cid:101)Xt−k(u))k≥1, ξt

(cid:0)( (cid:101)Xt−k(u(cid:48)))k≥1, ξt

(cid:1)(cid:107)p

40

≤ (cid:107)Fθ∗(u)

(cid:0)( (cid:101)Xt−k(u))k≥1, ξt

(cid:1) − Fθ∗(u)

(cid:0)( (cid:101)Xt−k(u(cid:48)))k≥1, ξt

(cid:1) − Fθ∗(u)

(cid:0)( (cid:101)Xt−k(u(cid:48)))k≥1, ξt

(cid:1)(cid:107)p
(cid:0)( (cid:101)Xt−k(u(cid:48)))k≥1, ξt

(cid:1)(cid:107)p

+ (cid:107)Fθ∗(u)
∞
(cid:88)

≤

b(0)
k (Θ) (cid:107) (cid:101)Xt−k(u(cid:48)) − (cid:101)Xt−k(u)(cid:107)p

k=1

+ (cid:13)

(cid:13)θ∗(u(cid:48)) − θ∗(u)(cid:13)

(cid:13) × sup
θ∈Θ

(cid:13)
(cid:13)∂1

θFθ( (cid:101)Xt−1(u), (cid:101)Xt−2(u), . . . , ξt)(cid:13)

(cid:13)p .

We upper-bound

(cid:13)
(cid:13)∂1

θFθ( (cid:101)Xt−1(u), (cid:101)Xt−2(u), . . . , ξt)(cid:13)
(cid:13)p

sup
θ∈Θ

≤

∞
(cid:88)

k=1

b(1)
k (Θ) (cid:107) (cid:101)Xt−k(u)(cid:107)p + sup
θ∈Θ

(cid:13)
(cid:13)∂1

θFθ(0, 0, . . . , ξt)(cid:13)

(cid:13)p .

By a similar argument than in the proof of Lemma 6.5, we deduce that (82) holds.

Now we are in position to prove Theorem 3.2.

(cid:16) ∂
∂θi

Proof of Theorem 3.2. We follow the usual proof of asymptotic normality of a M-estimator. This will
follow from the 3 forthcoming steps:
• I/ We establish that the family ∂θHt

(cid:0)θ∗(u)(cid:1)(cid:17)

(cid:0)θ∗(u)(cid:1) =

Ht

for in ≤ t ≤ jn satis-
1≤i≤d
(cid:0)θ(cid:1) = Φ(cid:0)( (cid:101)Xt−k(u))k∈N, θ(cid:1).
(cid:3) = 0 as θ∗(u) is the unique minimizer of

ﬁes a multidimensional central limit theorem, where we denote Ht
We notice ﬁrst that ∂θE(cid:2)Φ(cid:0)( (cid:101)Xt−k(u))k∈N, θ∗(u)(cid:1) | F0
E(cid:2)Φ(cid:0)( (cid:101)Xt−k(u))k∈N, θ∗(u)(cid:1) | F0
(cid:3) is differentiable under the condition
The function θ ∈ Θ (cid:55)→ E(cid:2)Φ(cid:0)( (cid:101)Xt−k(u))k∈N, θ∗(u)(cid:1) | F0
(cid:13)
(cid:13)∂θΦ(cid:13)
(cid:0)θ∗(u)(cid:1) constitutes a differences of martingale sequence. We also have
(cid:13)∂θΦ(cid:13)
E(cid:2)(cid:13)
2(cid:3) < ∞ and we can apply the CLT for differences of martingale sequences (See for instance
(cid:13)
[6]). As a consequence we can apply the point 2. of Lemma 7.1 and obtain the multidimensional central
limit theorem

(cid:13) ∈ Lip p(Θ). Thus ∂θHt

(cid:3) over the open set

o
Θ.

√

1
nhn

jn(u)
(cid:88)

t=in(u)

∂θΦ(cid:0)( (cid:101)Xt−k(u))k∈N, θ∗(u)(cid:1) K

(cid:17)

(cid:16) t

n − u
hn

L
−→
n→+∞

(cid:16)

0 , Σ(cid:0)θ∗(u)(cid:1)(cid:17)

N

(83)

with Σ(cid:0)θ∗(u)(cid:1) =

(cid:90)

R

K2(x)dx

(cid:88)

(cid:16)

×

t∈Z

Cov (cid:2) ∂
∂θi

Φ(cid:0)( (cid:101)X−k(u))k∈N, θ∗(u)(cid:1) ,

∂
∂θj

Φ(cid:0)( (cid:101)Xt−k(u))k∈N, θ∗(u)(cid:1)(cid:3)(cid:17)

1≤i,j≤d

.

• II/ We use a Taylor-Lagrange expansion for establishing

Contrast estimation of locally stationary processes

41

√

1
nhn

jn(u)
(cid:88)

t=in(u)

∂θΦ(cid:0)( (cid:101)Xt−k(u))k∈N, (cid:98)θ(u)(cid:1) K

(cid:17)

(cid:16) t

n − u
hn

=

√

1
nhn

jn(u)
(cid:88)

t=in(u)

∂θΦ(cid:0)( (cid:101)Xt−k(u))k∈N, θ∗(u)(cid:1) K

(cid:17)

(cid:16) t

n − u
hn

(cid:112)

nhn ·

+

1
nhn

jn(u)
(cid:88)

t=in(u)

θ2Φ(cid:0)( (cid:101)Xt−k(u))k∈N, θ(u)(cid:1)K
∂2

(cid:16) t

n − u
hn

(cid:17)(cid:0)

(cid:98)θ(u) − θ∗(u)(cid:1),

(84)

P
−→
n→+∞

θ∗(u). Moreover, since E(cid:2)(cid:13)

where θ(u) belongs to the segment with extremities θ∗(u) and (cid:98)θ(u). From Theorem 3.1, we have
θ2Φ(cid:0)( (cid:101)Xt−k(u))k∈N, θ(cid:1)(cid:13)
(cid:3) < ∞ for any θ ∈ Θ and θ ∈ Θ (cid:55)→
θ(u)
(cid:13)
θ2Φ(cid:0)( (cid:101)Xt−k(u))k∈N, θ(cid:1) is uniformly continuous because Θ is a bounded set included in Rd, we can
∂2
apply Lemma 7.1 and then:

(cid:13)∂2

1
nhn

jn(u)
(cid:88)

θ2Φ(( (cid:101)Xt−k(u))k∈N,θ(u)) − E(cid:2)∂2
(cid:0)∂2

θ2Φ(( (cid:101)Xt−k(u))k∈N, θ(u))(cid:3)(cid:1)K(cid:0)

t=in(u)

t
n −u
hn

P
(cid:1)
−→
n→+∞

0.

(85)

Thus we get, with Γ(θ∗(u)) = E(cid:2)∂2

θ2Φ(cid:0)( (cid:101)Xt−k(u))k∈N, θ∗(u)(cid:1)(cid:3),

1
nhn

jn(u)
(cid:88)

t=in(u)

θ2Φ(cid:0)( (cid:101)Xt−k(u))k∈N, θ(u)(cid:1) K
∂2

(cid:17)

(cid:16) t

n − u
hn

P
−→
n→+∞

Γ(θ∗(u)).

Moreover, since (cid:98)θ(u) minimizes the contrast function we have

1
nhn

jn(u)
(cid:88)

t=in(u)

∂θΦ(cid:0)(X(n)

t−k(u))k∈N, (cid:98)θ(u)(cid:1) K

(cid:17)

(cid:16) t

n − u
hn

= 0.

(86)

Using the assumptions on the Lipschitz coefﬁcients of ∂θΦ, the same inequalities as (80) and (81) in
the proof of Proposition 7.1 lead for a convenient constant C > 0 to:

(cid:13)
(cid:13)
(cid:13)

1
nhn

(cid:13)
(cid:13)
(cid:13)

1
nhn

jn(u)
(cid:88)

t=in(u)

jn(u)
(cid:88)

t=in(u)

(cid:0)∂θΦ(cid:0)(X(n)

t−k(u))k∈N, (cid:98)θ(u)(cid:1) − ∂θΦ(cid:0)(X∗

t−k(u))k≥0, (cid:98)θ(u)(cid:1)(cid:1)K(cid:0)

(cid:0)∂θΦ(cid:0)(X∗

t−k(u))k≥0, (cid:98)θ(u)(cid:1) − ∂θΦ(cid:0)( (cid:101)Xt−k(u))k∈N, (cid:98)θ(u)(cid:1)(cid:1)K(cid:0)

t
n − u
hn

(cid:1)(cid:13)
(cid:13)1 ≤ Chρ
n,

t
n − u
hn

(cid:1)(cid:13)
(cid:13)1 ≤

C
nhn

.

As a consequence we deduce that:

(cid:13)
(cid:13)
(cid:13)

√

1
nhn

jn(u)
(cid:88)

t=in(u)

∂θΦ(cid:0)( (cid:101)Xt−k(u))k∈N, (cid:98)θ(u)(cid:1) K

(cid:16) t

n − u
hn

(cid:17)(cid:13)
(cid:13)
(cid:13)1

≤ C

(cid:16) 1
√

nhn

(cid:112)

+ hρ
n

nhn

(cid:17)

42

=⇒

√

1
nhn

jn(u)
(cid:88)

t=in(u)

∂θΦ(cid:0)( (cid:101)Xt−k(u))k∈N, (cid:98)θ(u)(cid:1) K

(cid:17)

(cid:16) t

n − u
hn

P
−→
n→+∞

0,

(87)

by using (14). Finally, from (84), using (85), (87), Slutsky Lemma and (83), we deduce:

(cid:112)

nhn Γ(θ∗(u))(cid:0)

(cid:98)θ(u) − θ∗(u)(cid:1)

L
−→
n→+∞

(cid:16)

0 , Σ(cid:0)θ∗(u)(cid:1)(cid:17)
,

N

and this leads to Theorem 3.2.

Proof of Proposition 4.2. We already proved in Section 4.3 that ΦLARCH ∈ Lip 4(Θ) as well as As-
sumption Co(ΦLARCH , Θ) when condition (41) holds. We assumed that θ ∈ Θ (cid:55)→ ai(θ) are C2(Θ)
functions for any i ∈ N. Thus in order to check the conditions of Theorem 3.2, we ﬁrst have to prove
that ∂θΦLARCH ∈ Lip 4(Θ). Indeed we use the estimates
(cid:13)∂θΦLARCH (U, θ) − ∂θΦLARCH (V, θ)(cid:13)
(cid:13)
(cid:13)

(cid:16)

≤ 8

|U1| + |V1| + 2a0(θ) +

|ai(θ)|(cid:0)|Ui+1| + |Vi+1|(cid:1)(cid:17)2

∞
(cid:88)

i=1

(cid:16)(cid:13)
(cid:13)∂θa0(θ)(cid:13)

(cid:13) +

×

∞
(cid:88)

i=1

(cid:13)∂θai(θ)(cid:13)
(cid:13)

(cid:13) |Ui+1|

(cid:17)(cid:16)

|U1 − V1| +

|ai(θ)| |Ui+1 − Vi+1|

(cid:17)

∞
(cid:88)

i=1

(cid:16)

+4

|U1| + |V1| + 2a0(θ) +

|ai(θ)| (cid:0)|Ui+1|+|Vi+1|(cid:1)(cid:17)3

∞
(cid:88)

i=1

∞
(cid:88)

i=1

(cid:13)∂θai(θ)(cid:13)
(cid:13)

(cid:13) |Ui+1 −Vi+1| .

Therefore, using Hölder and Minkowski Inequalities, we obtain

E(cid:2) sup
θ∈Θ

(cid:13)∂θΦLARCH (U, θ) − ∂θΦLARCH (V, θ)(cid:13)
(cid:13)
(cid:3)
(cid:13)

≤ g(cid:0) sup
i≥1

(cid:13)Ui(cid:107)4 ∨ (cid:13)
(cid:8)(cid:13)

(cid:13)Vi(cid:107)4}(cid:1) (cid:16)

|U1 − V1| +

∞
(cid:88)

i=1

(cid:0) sup
θ∈Θ

|ai(θ)| + sup
θ∈Θ

(cid:13)∂θai(θ)(cid:13)
(cid:13)
(cid:13)

(cid:1) (cid:13)

(cid:13)Ui+1 − Vi+1

(cid:17)

.

(cid:13)
(cid:13)4

This inequality implies that ∂θΦLARCH ∈ Lip 4(Θ) under the assumptions of Proposition 4.2.

We also have to establish that under conditions of Proposition 4.2,

and E(cid:2)(cid:13)

(cid:13)∂2

θ2ΦLARCH

E(cid:2)(cid:13)

(cid:13)∂θΦLARCH

(cid:0)( (cid:101)Xk(u))k≤0, θ∗(u)(cid:1)(cid:13)
2(cid:3) < ∞
(cid:13)

(cid:0)( (cid:101)X−k(u))k∈N, θ∗(u)(cid:1)(cid:13)
(cid:3) < ∞. Indeed we have
(cid:13)

∂θΦLARCH

(cid:0)( (cid:101)Xk(u))k≤0, θ∗(u)(cid:1) = −4(ξ2

0 − 1)

(cid:16)

a0(θ∗(u)) +

ai(θ∗(u)) (cid:101)X−i(u))

(cid:17)3

∞
(cid:88)

i=1

(cid:16)
∂θa0(θ∗(u)) +

×

∞
(cid:88)

i=1

∂θai(θ∗(u)) (cid:101)X−i(u))

(cid:17)

. (88)

Contrast estimation of locally stationary processes

43

Therefore, using Hölder and Minkowski Inequalities and independence of ξ0 and ( (cid:101)Xk(u))k≤−1 to-
gether with Eu ≡ E

2(cid:105)
(cid:0)( (cid:101)Xk(u))k≤0, θ∗(u)(cid:1)(cid:13)
(cid:13)

(cid:104)(cid:13)
(cid:13)∂θΦLARCH

we derive that

Eu ≤ 16 E(cid:2)(cid:0)ξ2

0 − 1(cid:1)2(cid:3) (cid:16)

E

(cid:104)(cid:16)

a0(θ∗(u)) +

ai(θ∗(u)) (cid:101)X−i(u))

(cid:17)8(cid:105)(cid:17)3/4

∞
(cid:88)

i=1

(cid:16)

E

(cid:104)(cid:13)
(cid:13)∂θa0(θ∗(u)) +

×

∞
(cid:88)

i=1

8(cid:105)(cid:17)1/4
∂θai(θ∗(u)) (cid:101)X−i(u)(cid:13)
(cid:13)

(cid:16)

≤ C

sup
θ∈Θ

|a0(θ)| + (cid:107) (cid:101)X0(u)(cid:107)8

∞
(cid:88)

i=1

sup
θ∈Θ

(cid:17)6

|ai(θ)|

(cid:16)

×

sup
θ∈Θ

(cid:107)∂θa0(θ)(cid:107) + (cid:107) (cid:101)X0(u)(cid:107)8

∞
(cid:88)

i=1

sup
θ∈Θ

(cid:107)∂θai(θ)(cid:107)

(cid:17)2

.

Thus we obtain E
on (aj)j. The expression for the second derivatives is also derived

2(cid:105)
(cid:0)( (cid:101)Xk(u))k≤0, θ∗(u)(cid:1)(cid:13)
(cid:13)

(cid:104)(cid:13)
(cid:13)∂θΦLARCH

< ∞ with r = 8 under suitable conditions

∂2
θ2ΦLARCH

(cid:16)
(cid:0)( (cid:101)Xk(u))k≤0, θ∗(u)(cid:1) = −4

a0(θ∗(u)) +

ai(θ∗(u)) (cid:101)X−i(u))

(cid:17)2

∞
(cid:88)

i=1

(cid:110)

(ξ2

0 −3)(cid:0)∂θa0(θ∗(u))+

×

∞
(cid:88)

i=1

∂θai(θ∗(u)) (cid:101)X−i(u))(cid:1)(cid:0)∂θa0(θ∗(u))+

∞
(cid:88)

i=1

∂θai(θ∗(u)) (cid:101)X−i(u))(cid:1)(cid:48)

+ (cid:0)a0(θ∗(u))+

∞
(cid:88)

ai(θ∗(u)) (cid:101)X−i(u))(cid:1)(cid:0)∂2

θ2a0(θ∗(u)) +

i=1

As a consequence, similar arguments as previously entail

∞
(cid:88)

i=1

θ2 ai(θ∗(u)) (cid:101)X−i(u))(cid:1)(cid:111)
∂2

.

E

(cid:104)(cid:13)
(cid:13)∂2

θ2ΦLARCH

(cid:105)
(cid:0)( (cid:101)Xk(u))k≤0, θ∗(u)(cid:1)(cid:13)
(cid:13)

< ∞

from Hausdorff and Minkowski inequalities. Finally we checked the conditions of Theorem 3.2 since
the asymptotic covariance matrix Σ(cid:0)θ∗(u)(cid:1) and Γ(θ∗(u)) are positive deﬁnite matrix from (42) using
(88).

Proof of Proposition 4.1. We proved in Section 4 that αk(ΦG, Θ) = b(0)
fθ and Mθ satisfy Lipschitz inequalities (20). But we also have:

k (Θ) and ΦG ∈ Lip 3(Θ) when

∂θΦG(x, θ) =

∂θMθ · (cid:0)(xk)k≥2
(cid:0)(xk)k≥2

Mθ

(cid:1)

(cid:1) + 2 ∂θfθ

(cid:0)(xk)k≥2

(cid:1) ·

fθ

(cid:0)(xk)k≥2
M 2
θ

(cid:0)(xk)k≥2

(cid:1) − x1
(cid:1)

− 2 ∂θMθ

(cid:0)(xk)k≥2

(cid:1) ·

(cid:0)fθ

(cid:0)(xk)k≥2
M 3
θ

(cid:0)(xk)k≥2

(cid:1) − x1
(cid:1)

(cid:1)2

.

44

After computations and using Mθ ≥ M as well as Hölder Inequalities, we obtain

E(cid:2) sup
θ∈Θ

(cid:13)∂θΦG(U, θ) − ∂θΦG(V, θ)(cid:13)
(cid:13)
(cid:3) ≤
(cid:13)

1
M

(cid:13)
(cid:13)∂θMθ

(cid:0)(Uk)k≥2

(cid:1) − ∂θMθ

(cid:0)(Vk)k≥2

(cid:1)(cid:13)
(cid:13)1

(cid:107)∂θMθ

+

(cid:16) (cid:107)fθ

+ 2

(cid:1)(cid:107)2

(cid:1) − U1(cid:107)2

(cid:0)(Vk)k≥2
M
(cid:0)(Uk)k≥2
M 2
(cid:1) − V1(cid:107)3(cid:107)∂θfθ
M 3
(cid:1)(cid:107)2

(cid:107)∂θfθ

+

(cid:0)(Vk)k≥2
M 2

(cid:107)fθ

+ 2 ·

(cid:0)(Vk)k≥2

(cid:16) (cid:107)∂θMθ

(cid:0)(Vk)k≥2

(cid:1)(cid:107)3 (cid:107)fθ

+ 2

(cid:13)
(cid:13)Mθ

(cid:0)(Uk)k≥2

(cid:1) − Mθ

(cid:0)(Vk)k≥2

(cid:1)(cid:13)
(cid:13)2

(cid:13)
(cid:13)∂θfθ

(cid:0)(Uk)k≥2

(cid:1) − ∂θfθ

(cid:0)(Vk)k≥2

(cid:1)(cid:13)
(cid:13)2

(cid:0)(Vk)k≥2

(cid:1)(cid:107)3

(cid:13)
(cid:13)Mθ

(cid:0)(Uk)k≥2

(cid:1) − Mθ

(cid:0)(Vk)k≥2

(cid:1)(cid:13)
(cid:13)3

(cid:0)(cid:13)
(cid:13)fθ

(cid:0)(Uk)k≥2

(cid:1) − fθ

(cid:0)(Vk)k≥2

(cid:1)(cid:17)

(cid:1)(cid:13)
(cid:13)2 + (cid:107)U1 − V1(cid:107)2
(cid:1) − U1 − V1(cid:107)3

(cid:1) + fθ

(cid:0)(Vk)k≥2

(cid:0)(Uk)k≥2
M 3
(cid:0)(Vk)k≥2
(cid:1) − V1(cid:107)2
4

× (cid:0)(cid:13)
(cid:0)(Vk)k≥2

(cid:0)(Uk)k≥2

(cid:1) − fθ
(cid:0)(Vk)k≥2

(cid:13)fθ
(cid:1)(cid:107)4 (cid:107)fθ
M 4

(cid:1)(cid:13)
(cid:13)3 + (cid:107)U1 − V1(cid:107)3

(cid:1)

(cid:0)(cid:13)
(cid:13)Mθ

(cid:0)(Uk)k≥2

(cid:1) − Mθ

(cid:0)(Vk)k≥2

(cid:1)

(cid:1)(cid:13)
(cid:13)4

(cid:107)fθ

+

(cid:0)(Uk)k≥2
M 3

(cid:1) − U1(cid:107)2
3

(cid:13)
(cid:13)∂θMθ

(cid:0)(Uk)k≥2

(cid:1) − ∂θMθ

(cid:0)(Vk)k≥2

(cid:17)

.

(cid:1)(cid:13)
(cid:13)3

(cid:107)∂θMθ

+ 3 ·

Thus

E(cid:2) sup
θ∈Θ

(cid:13)∂θΦG(U, θ) − ∂θΦG(V, θ)(cid:13)
(cid:13)
(cid:3) ≤ g(cid:0) sup
(cid:13)
i≥1

(cid:13)Ui(cid:107)4 ∨ (cid:13)
(cid:8)(cid:13)

(cid:13)Vi(cid:107)4}(cid:1)

(cid:16)(cid:13)
(cid:13)∂θMθ

×

(cid:0)(Uk)k≥2

(cid:1) − ∂θMθ

(cid:13)4 + (cid:13)
(cid:1)(cid:13)

(cid:13)∂θfθ

(cid:0)(Uk)k≥2

(cid:1) − ∂θfθ

(cid:0)(Vk)k≥2

(cid:1)(cid:13)
(cid:13)4

+ (cid:13)

(cid:13)Mθ

(cid:0)(Uk)k≥2

(cid:1) − Mθ

(cid:0)(Vk)k≥2

(cid:13)fθ

(cid:0)(Uk)k≥2

(cid:1) − fθ

(cid:0)(Vk)k≥2

(cid:1)(cid:13)
(cid:13)4 + (cid:107)U1 − V1(cid:107)4

(cid:17)

(cid:0)(Vk)k≥2
(cid:13)4 + (cid:13)
(cid:1)(cid:13)

from Jensen inequality and since we assume that fθ, Mθ, ∂θfθ and ∂θMθ satisfy Lipschitz inequalities
(20). As a consequence we derive

E(cid:2) sup
θ∈Θ

(cid:13)
(cid:13)∂θΦG(U, θ) − ∂θΦG(V, θ)(cid:13)
(cid:13)

(cid:3) ≤ g(cid:0) sup
i≥1

(cid:8)(cid:13)
(cid:13)Ui(cid:107)4 ∨ (cid:13)

(cid:13)Vi(cid:107)4}(cid:1)

(cid:16)

×

(cid:107)U1 − V1(cid:107)4 +

∞
(cid:88)

i=2

(cid:0)βi(f, Θ)+βi(M, Θ) + βi(∂θf, Θ)+βi(∂θM, Θ)(cid:1)(cid:107)Ui − Vi(cid:107)4

(cid:17)

,

therefore ∂θΦG ∈ Lip 4(Θ). From these computations and with the inequality (21) we also deduce that
condition (28) implies B0(Θ) < 1, and (cid:80)∞
s≥0 s αs(Φ, Θ) < ∞,
required in Theorems 3.1 and 3.2. Similar calculations also entail

t=2 t log(t)bt(Θ) < ∞ follows from (cid:80)

E

(cid:104)(cid:13)
(cid:13)∂2

2(cid:105)
θ2Φ(cid:0)( (cid:101)X−k(u))k∈N, θ(cid:1)(cid:13)
(cid:13)

< ∞,

for any

θ ∈ Θ,

Contrast estimation of locally stationary processes

45

since p = 4 and ∂2

θ2fθ and ∂2

θ2Mθ satisfy Lipschitz inequalities (20). We also have

E(cid:2)(cid:13)

(cid:13)∂θΦ(cid:0)( (cid:101)Xk(u))k≤0, θ∗(u)(cid:1)(cid:13)
2(cid:3) ≤
(cid:13)

12
(1 ∨ M )2
(cid:13)∂θfθ∗(u)(( (cid:101)Xk(u))k≤−1)(cid:13)
2
(cid:13)
2

(cid:16)(cid:13)
(cid:13)∂θMθ∗(u)(( (cid:101)Xk(u))k≤−1)(cid:13)
2
(cid:13)
2

(cid:13)
(cid:13)ξ0

2 + (cid:13)
(cid:13)
2
(cid:13)

(cid:13)∂θMθ∗(u)(( (cid:101)Xk(u))k≤−1)(cid:13)
2
(cid:13)
2

(cid:13)
(cid:13)ξ0

(cid:13)
4
(cid:13)
4

(cid:17)
,

+ (cid:13)

since (cid:101)X0(u) − fθ∗(u)(( (cid:101)Xk(u))k≤−1) = Mθ∗(u)(( (cid:101)Xk(u))k≤−1) ξ0, with Mθ∗(u)(( (cid:101)Xk(u))k≤−1) and
ξ0 which are independent. Therefore, we obtain

E(cid:2)(cid:13)

(cid:13)∂θΦ(cid:0)( (cid:101)Xk(u))k≤0, θ∗(u)(cid:1)(cid:13)
2(cid:3) < ∞
(cid:13)

since p = 4. Finally (27) ensures that asymptotic covariance matrix Σ(cid:0)θ∗(u)(cid:1) and Γ(θ∗(u)) are positive
deﬁnite matrix (see [5]) and (26) implies the existence and the uniqueness of θ∗(u) as the minimum
(cid:3) deﬁned in (10) (see also [5]). This ends the checking of the
of θ ∈ Θ (cid:55)→ E(cid:2)Φ(cid:0)( (cid:101)Xk(u))k≤1), θ(cid:1) | F0
conditions of Theorem 3.2.

Aknowledgments The authors wish to thank Rainer Dahlhaus, Thomas Mikosch and Lionel Truquet
for many inspiring discussions.
The work of the second author was funded by CY Initiative of Excellence (grant "Investissements
d’Avenir"ANR- 16-IDEX-0008), Project "EcoDep" PSI-AAP2020-0000000013.

References

[1] AZRAK, R. and MÉLARD, G. (2006). Asymptotic properties of quasi-maximum likelihood esti-
mators for ARMA models with time-dependent coefﬁcients. Statistical Inference for Stochastic
Processes 9 279-330.

[2] BARDET, J. M., BOULAROUK, Y. and DJABALLAH, K. (2017). Asymptotic behavior of the
Laplacian quasi-maximum likelihood estimator of afﬁne causal processes. Electronic Journal of
Statistics 11 452-479.

[3] BARDET, J. M. and DOUKHAN, P. (2018). Non-parametric estimation of time varying AR(1)–

processes with local stationarity and periodicity. Electronic Journal of Statistics 12 2323-2354.

[4] BARDET, J. M., KENGNE, W. and WINTENBERGER, O. (2012). Multiple breaks detection in
general causal time series using penalized quasi-likelihood. Elec. Journal Statist. 435–477.
[5] BARDET, J. M. and WINTENBERGER, O. (2009). Asymptotic normality of the quasi-maximum

likelihood estimator for multidimensional causal processes. Ann. Statist. 37 2730–2759.

[6] BILLINGSLEY, P. (1999). Convergence of probability measures. Second edition, Wiley and sons.
[7] DAHLHAUS, R. (1996). On the Kullback-Leibler information divergence of locally stationary

processes. Stochastic Processes and Applications 62 139-168.

[8] DAHLHAUS, R. (1997). Fitting time series models to nonstationary processes. Annals of Statistics

25 1-37.

[9] DAHLHAUS, R. (2000). A Likelihood Approximation for Locally Stationnary Processes. Annals

of Statistics 28 1762-1794.

[10] DAHLHAUS, R. (2009). Empirical spectral processes for locally stationary time series. Bernoulli

15 1-39.

[11] DAHLHAUS, R. (2012). Locally Stationary Processes. In Time Series Analysis: Methods and Ap-
plications, (T. Subba Rao, S. Subba Rao and C. R. Rao, eds.). Handbook of statistics 30 Elsevier.

46

[12] DAHLHAUS, R. and POLONIK, W. (2006). Nonparametric quasi-maximum likelihood estimation

for Gaussian locally stationary processes. The Annals of Statistics 34 2790–2824.

[13] DAHLHAUS, R., RICHTER, S. and WU, W. B. (2019). Towards a general theory for nonlinear

locally stationary processes. Bernoulli 25 1013–1044.

[14] DAHLHAUS, R. and SUBBA RAO, S. (2006). Statistical Inference for Time Varying ARCH pro-

cesses. The Annals of Statistics 34 1075-1114.

[15] DEDECKER, J., DOUKHAN, P., LANG, G., LEÓN, J. R., LOUHICHI, S. and PRIEUR, C. (2007).
Weak dependence: With Examples and Applications. Lecture Notes in Statistics 190, Springer-
Verlag, New York.

[16] DEDECKER, J. and PRIEUR, C. (2004). Coupling for τ −Dependent Sequences and Applications.

Journal of Theoretical Probability 17 861-885.

[17] DING, Z., GRANGER, C. W. J. and ENGLE, R. (1993). A long memory property of stock market

returns and a new model. J. Empirical Finance 1 83-106.

[18] DOUKHAN, P., FOKIANOS, K. and TJØSTHEIM, D. (2012). On weak dependence conditions for

Poisson autoregressions. Statistics and Probability Letters 82 942-948.

[19] DOUKHAN, P. and KENGNE, W. (2015). Inference and testing for structural change in general

Poisson autoregressive models. Electronic Journal of Statistics 9 1267-1314.

[20] DOUKHAN, P. and TRUQUET, L. (2007). A ﬁxed point approach to model random ﬁelds. ALEA

Lat. Am. J. Probab. Math. Stat. 3 111–132.

[21] DOUKHAN, P. and WINTENBERGER, O. (2008). Weakly dependent chains with inﬁnite memory.

Stochastic Processes and Applications 118 1997-2013.

[22] FRANCQ, C. and ZAKOÏAN, J. M. (2010). GARCH Models: Structure, Statistical Inference and

Financial Applications. Wiley.

[23] FRANCQ, C. and ZAKOÏAN, J.-M. (2010). Inconsistency of the MLE and inference based on

weighted LS for LARCH models. Journal of Econometrics 159 151-165.

[24] GIRAITIS, L., KOKOSZKA, P. and LEIPUS, R. (2000). Stationary ARCH models: dependence

structure and Central Limit Theorem. Econometric Theory 1 3–22.

[25] GIRAITIS, L., LEIPUS, R., ROBINSON, P. and SURGAILIS, D. (2003). LARCH, leverage and

long memory. Journal of Financial Econometrics 2 177–210.

[26] LING, S. and MCALEER, M. (2003). Asymptotic theory for a vector ARMA-GARCH model.

Econometric Theory 19 280-310.

[27] PARASCHAKISA, K. and DAHLHAUS, R. (2012). Frequency and phase estimation in time series

with quasi periodic components. Journal of Time Series Analysis 33 13-31.

[28] PFANZAGL, J. (1969). On the mesurability and consistency of minimum contrast estimates.

Metrika 14 249–334.

[29] ROBINSON, P. (1991). Testing for strong serial correlation and dynamic conditional het-

eroskedasticity in multiple regression. Journal of Econometrics 47 67–84.

[30] ROHAN, N. (2013). A time varying GARCH() model and related statistical inference. Statistics

& Probability Letters 83 1983-1990.

[31] ROSENBLATT, M. (1991). Stochastic curve estimation 3. NSF-CBMS Regional Conference Se-

ries in Probability and Statistics.

[32] STRAUMANN, D. (2005). Estimation in conditionally heteroscedastic time series models. Lecture

Notes in Statistics 181. Springer-Verlag, Berlin.

[33] TRUQUET, L. (2017). Parameter stability and semiparametric inference in time-varying ARCH

models. Journal of Royal Statistical Society Series B 79 1391–1414.

