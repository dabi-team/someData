2
2
0
2

r
a

M
9

]

V
C
.
s
c
[

2
v
1
4
9
1
0
.
3
0
2
2
:
v
i
X
r
a

Autoregressive Image Generation using Residual Quantization

Doyup Lee*
POSTECH, Kakao Brain
doyup.lee@postech.ac.kr

Chiheon Kim*
Kakao Brain
chiheon.kim@kakaobrain.com

Saehoon Kim
Kakao Brain
shkim@kakaobrain.com

Minsu Cho
POSTECH
mscho@postech.ac.kr

Wook-Shin Han†
POSTECH
wshan@postech.ac.kr

Abstract

For autoregressive (AR) modeling of high-resolution im-
ages, vector quantization (VQ) represents an image as a se-
quence of discrete codes. A short sequence length is im-
portant for an AR model to reduce its computational costs
to consider long-range interactions of codes. However,
we postulate that previous VQ cannot shorten the code se-
quence and generate high-ﬁdelity images together in terms
of the rate-distortion trade-off. In this study, we propose the
two-stage framework, which consists of Residual-Quantized
VAE (RQ-VAE) and RQ-Transformer, to effectively generate
high-resolution images. Given a ﬁxed codebook size, RQ-
VAE can precisely approximate a feature map of an image
and represent the image as a stacked map of discrete codes.
Then, RQ-Transformer learns to predict the quantized fea-
ture vector at the next position by predicting the next stack
of codes. Thanks to the precise approximation of RQ-VAE,
we can represent a 256×256 image as 8×8 resolution of
the feature map, and RQ-Transformer can efﬁciently reduce
the computational costs. Consequently, our framework out-
performs the existing AR models on various benchmarks of
unconditional and conditional image generation. Our ap-
proach also has a signiﬁcantly faster sampling speed than
previous AR models to generate high-quality images.

Figure 1. Examples of our conditional generation for 256×256
images. The images in the ﬁrst row are generated from the classes
of ImageNet. The images in the second row are generated from
text conditions (“A cheeseburger in front of a mountain range cov-
ered with snow.” and “a cherry blossom tree on the blue ocean”).
The text conditions are unseen during the training.

1. Introduction

Vector quantization (VQ) becomes a fundamental tech-
nique for autoregerssive (AR) models to generate high-
resolution images [6, 13, 14, 37, 45]. Speciﬁcally, an image
is represented as a sequence of discrete codes, after the fea-
ture map of the image is quantized by VQ and rearranged by
an ordering such as raster scan [34]. After the quantization,
AR model is trained to sequentially predict the codes in the

*Equal contribution
†Corresponding author

sequence. That is, AR models can generate high-resolution
images without predicting whole pixels in an image.

We postulate that reducing the sequence length of codes
is important for AR modeling of images. A short sequence
of codes can signiﬁcantly reduce the computational costs
of an AR model, since an AR uses the codes in previous
positions to predict the next code. However, previous stud-
ies have a limitation to reducing the sequence length of im-
ages in terms of the rate-distortion trade-off [42]. Namely,
VQ-VAE [45] requires an exponentially increasing size of

1

 
 
 
 
 
 
codebook to reduce the resolution of the quantized feature
map, while conserving the quality of reconstructed images.
However, a huge codebook leads to the increase of model
parameters and the codebook collapse problem [10], which
makes the training of VQ-VAE unstable.

In this study, we propose a Residual-Quantized VAE
(RQ-VAE), which uses a residual quantization (RQ) to pre-
cisely approximate the feature map and reduce its spatial
resolution.
Instead of increasing the codebook size, RQ
uses a ﬁxed size of codebook to recursively quantize the
feature map in a coarse-to-ﬁne manner. After D iterations
of RQ, the feature map is represented as a stacked map of
D discrete codes. Since RQ can compose as many vectors
as the codebook size to the power of D, RQ-VAE can pre-
cisely approximate a feature map, while conserving the in-
formation of the encoded image without a huge codebook.
Thanks to the precise approximation, RQ-VAE can further
reduce the spatial resolution of the quantized feature map
than previous studies [14, 37, 45]. For example, our RQ-
VAE can use 8×8 resolution of feature maps for AR mod-
eling of 256×256 images.

In addition, We propose RQ-Transformer to predict
the codes extracted by RQ-VAE. For the input of RQ-
Transformer, the quantized feature map in RQ-VAE is con-
verted into a sequence of feature vectors. Then, RQ-
Transformer predicts the next D codes to estimate the fea-
ture vector at the next position. Thanks to the reduced res-
olution of feature maps by RQ-VAE, RQ-Transformer can
signiﬁcantly reduce the computational costs and easily learn
the long-range interactions of inputs. We also propose two
training techniques for RQ-Transformer, soft labeling and
stochastic sampling for the codes of RQ-VAE. They further
improve the performance of RQ-Transformer by resolving
the exposure bias [38] in the training of AR models. Con-
sequently, as shown in Figure 1, our model can generate
high-quality images.

Our main contributions are summarized as follows.
1) We propose RQ-VAE, which represents an image
as a stacked map of discrete codes, while producing
high-ﬁdelity reconstructed images. 2) We propose RQ-
Transformer to effectively predict the codes of RQ-VAE and
its training techniques to resolve the exposure bias. 3) We
show that our approach outperforms previous AR models
and signiﬁcantly improves the quality of generated images,
computational costs, and sampling speed.

2. Related Work

AR Modeling for Image Synthesis AR models have
shown promising results of image generation [6, 13, 14, 33,
39, 45] as well as text [4] and audio [10] generation. AR
modeling of raw pixels is possible [6, 34, 35, 40], but it is
infeasible for high-resolution images due to the slow speed
and low quality of generated images. Thus, previous stud-

ies incorporate VQ-VAE [14], which uses VQ to represent
an image as discrete codes, and uses an AR model to pre-
dict the codes of VQ-VAE. VQ-GAN [14], improves the
perceptual quality of reconstructed images using adversar-
ial [16, 22] and perceptual loss [28]. However, when the
resolution of the feature map is further reduced, VQ-GAN
cannot precisely approximate the feature map of an image
due to the limited size of codebook.

VQs in Other Applications Composite quantizations
have been used in other applications to represent a vector
as a composition of codes for the precise approximation un-
der a given codebook size [1, 15, 17, 29, 30, 32]. For the
nearest neighbor search, product quantization (PQ) [17] ap-
proximates a vector as the sum of linearly independent vec-
tors in the codebook. As a generalized version of PQ, ad-
ditive quantization (AQ) [1] uses the dependent vectors in
the codebook, but ﬁnding the codes is an NP-hard task [8].
Residual quantization (RQ, also known as stacked quantiza-
tion) [24, 32] iteratively quantizes a vector and its residuals
and represents the vector as a stack of codes, which has been
used for neural network compression [15, 29, 30]. For AR
modeling of images, our RQ-VAE adopts RQ to discretize
the feature map of an image. However, different from pre-
vious studies, RQ-VAE uses a single shared codebook for
all quantization steps.

3. Methods

We propose the two-stage framework with RQ-VAE and
RQ-Transformer for AR modeling of images (see Figure 2).
RQ-VAE uses a codebook to represent an image as a stacked
map of D discrete codes. Then, our RQ-Transformer au-
toregressively predicts the next D codes at the next spatial
position. We also introduce how our RQ-Transformer re-
solves the exposure bias [38] in the training of AR models.

3.1. Stage 1: Residual-Quantized VAE

In this section, we ﬁrst introduce the formulation of VQ
and VQVAE. Then, we propose RQ-VAE, which can pre-
cisely approximate a feature map without increasing the
codebook size, and explain how RQ-VAE represents an im-
age as a stacked map of discrete codes.

3.1.1 Formulation of VQ and VQ-VAE

Let a codebook C be a ﬁnite set {(k, e(k))}k∈[K], which
consists of the pairs of a code k and its code embedding
e(k) ∈ Rnz , where K is the codebook size and nz is the
dimensionality of code embeddings. Given a vector z ∈
Rnz , Q(z; C) denotes VQ of z, which is the code whose
embedding is nearest to z, that is,

Q(z; C) = arg min

k∈[K]

(cid:107)z − e(k)(cid:107)2
2.

(1)

2

Figure 2. An overview of our two-stage image generation framework composed of RQ-VAE and RQ-Transformer. In stage 1, RQ-VAE
uses the residual quantizer to represent an image as a stack of D = 4 codes. After the stacked map of codes is reshaped, RQ-Transformer
predicts the D codes at the next position. More details are available in Section 3.

After VQ-VAE encodes an image into a discrete code
map, VQ-VAE reconstructs the original image from the en-
coded code map. Let E and G be an encoder and a decoder
of VQ-VAE. Given an image X ∈ RHo×Wo×3, VQ-VAE
extracts the feature map Z = E(X) ∈ RH×W ×nz , where
(H, W ) = (Ho/f, Wo/f ) is the spatial resolution of Z,
and f is a downsampling factor. By applying the VQ to
each feature vector at each position, VQ-VAE quantizes Z
and returns its code map M ∈ [K]H×W and its quantized
feature map ˆZ ∈ RH×W ×nz as

Mhw = Q(Zhw; C),

(2)
where Zhw ∈ Rnz is a feature vector at (h, w), and Mhw is
its code. Finally, the input is reconstructed as ˆX = G( ˆZ).

ˆZhw = e(Mhw),

We remark that reducing the spatial resolution of ˆZ,
(H, W ), is important for AR modeling, since the compu-
tational cost of an AR model increases with HW . How-
ever, since VQ-VAE conducts a lossy compression of im-
ages, there is a trade-off between reducing (H, W ) and con-
serving the information of X. Speciﬁcally, VQ-VAE with
the codebook size K uses HW log2 K bits to represent an
image as the codes. Note that the best achievable recon-
struction error depends on the number of bits in terms of the
rate-distortion theory [42]. Thus, to further reduce (H, W )
to (H/2, W/2) but preserve the reconstruction quality, VQ-
VAE requires the codebook of size K 4. However, VQ-VAE
with a large codebook is inefﬁcient due to the codebook col-
lapse problem [10] with unstable training.

3.1.2 Residual Quantization

Instead of increasing the codebook size, we adopt a residual
quantization (RQ) to discretize a vector z. Given a quanti-
zation depth D, RQ represents z as an ordered D codes

RQ(z; C, D) = (k1, · · · , kD) ∈ [K]D,

(3)

3

where C is the codebook of size |C| = K, and kd is the
code of z at depth d. Starting with 0-th residual r0 = z, RQ
recursively computes kd, which is the code of the residual
rd−1, and the next residual rd as

kd = Q(rd−1; C),
rd = rd−1 − e(kd),

(4)

In addition, we deﬁne ˆz(d) =
i=1 e(ki) as the partial sum of up to d code embeddings,

for d = 1, · · · , D.
(cid:80)d
and ˆz := ˆz(D) is the quantized vector of z.

The recursive quantization of RQ approximates the vec-
tor z in a coarse-to-ﬁne manner. Note that ˆz(1) is the closest
code embedding e(k1) in the codebook to z. Then, the re-
maining codes are subsequently chosen to reduce the quan-
tization error at each depth. Hence, the partial sum up to d,
ˆz(d), provides a ﬁner approximation as d increases.

Although we can separately construct a codebook for
each depth d, a single shared codebook C is used for every
quantization depth. The shared codebook has two advan-
tages for RQ to approximate a vector z. First, using separate
codebooks requires an extensive hyperparameter search to
determine the codebook size at each depth, but the shared
codebook only requires to determine the total codebook size
K. Second, the shared codebook makes all code embed-
dings available for every quantization depth. Thus, a code
can be used at every depth to maximize its utility.

We remark that RQ can more precisely approximate a
vector than VQ when their codebook sizes are the same.
While VQ partitions the entire vector space Rnz into K
clusters, RQ with depth D partitions the vector space into
K D clusters at most. That is, RQ with D has the same
partition capacity as VQ with K D codes. Thus, we can in-
crease D for RQ to replace VQ with an exponentially grow-
ing codebook.

Spatial TransformerDepth TransformerContexts 
+ Pos. Emb.EmbeddingsResidual 
QuantizerEncoderReshapeImage (256x256)ReconstructionRQ-VAE (Stage 1)RQ-Transformer (Stage 2)DecoderDepthPos. Emb.Depth<SOS>3.1.3 RQ-VAE

In Figure 2, we propose RQ-VAE to precisely quantize a
feature map of an image. RQ-VAE is also comprised of
the encoder-decoder architecture of VQ-VAE, but the VQ
module is replaced with the RQ module above. Speciﬁ-
cally, RQ-VAE with depth D represents a feature map Z
as a stacked map of codes M ∈ [K]H×W ×D and extracts
ˆZ(d) ∈ RH×W ×nz , which is the quantized feature map at
depth d for d ∈ [D] such that

Mhw = RQ(E(X)hw; C, D),

ˆZ(d)

hw =

d
(cid:88)

d(cid:48)=1

e(Mhwd(cid:48)).

(5)

For brevity, the quantized feature map ˆZ(D) at depth D is
also denoted by ˆZ. Finally, the decoder G reconstructs the
input image from ˆZ as ˆX = G( ˆZ).

Our RQ-VAE can make AR models to effectively gen-
erate high-resolution images with low computational costs.
For a ﬁxed downsampling factor f , RQ-VAE can produce
more realistic reconstructions than VQ-VAE, since RQ-
VAE can precisely approximate a feature map using a given
codebook size. Note that the ﬁdelity of reconstructed im-
ages is critical for the maximum quality of generated im-
ages. In addition, the precise approximation by RQ-VAE
allows more increase of f and decrease of (H, W ) than
VQ-VAE, while preserving the reconstruction quality. Con-
sequently, RQ-VAE enables an AR model to reduce its com-
putational costs, increase the speed of image generation,
and learn the long-range interactions of codes better.

Training of RQ-VAE To train the encoder E and the de-
coder G of RQ-VAE, we use the gradient descent with re-
spect to the loss L = Lrecon + βLcommit with a multiplicative
factor β > 0, The reconstruction loss Lrecon and the com-
mitment loss Lcommit are deﬁned as

Lrecon = (cid:107)X − ˆX(cid:107)2
2,
(cid:104) ˆZ(d)(cid:105)(cid:13)
2
(cid:13)
(cid:13)
2

(cid:13)
(cid:13)
(cid:13)Z − sg

D
(cid:88)

d=1

Lcommit =

(6)

(7)

,

where sg[·] is the stop-gradient operator, and the straight-
through estimator [45] is used for the backpropagation
through the RQ module. Note that Lcommit is the sum of
quantization errors from every d, not a single term (cid:107)Z −
It aims to make ˆZ(d) sequentially decrease the
sg[ ˆZ](cid:107)2
2.
quantization error of Z as d increases. Thus, RQ-VAE ap-
proximates the feature map in a coarse-to-ﬁne manner and
keeps the training stable. The codebook C is updated by the
exponential moving average of the clustered features [45].

4

Adversarial Training of RQ-VAE RQ-VAE is also
trained with adversarial learning to improve the perceptual
quality of reconstructed images. The patch-based adversar-
ial loss [22] and the perceptual loss [23] are used together as
described in the previous study [14]. We include the details
in the supplementary material.

3.2. Stage 2: RQ-Transformer

In this section, we propose RQ-Transformer in Figure 2
to autoregressively predict a code stack of RQ-VAE. After
we formulate the AR modeling of codes extracted by RQ-
VAE, we introduce how our RQ-Transformer efﬁciently
learns the stacked map of discrete codes. We also propose
the training techniques for RQ-Transformer to prevent the
exposure bias [38] in the training of AR models.

3.2.1 AR Modeling for Codes with Depth D

After RQ-VAE extracts a code map M ∈ [K]H×W ×D, the
raster scan order [34] rearranges the spatial indices of M to
a 2D array of codes S ∈ [K]T ×D where T = HW . That is,
St, which is a t-th row of S, contains D codes as

St = (St1, · · · , StD) ∈ [K]D for t ∈ [T ].

(8)

Regarding S as discrete latent variables of an image, AR

models learn p(S) which is autoregressively factorized as

p(S) =

T
(cid:89)

D
(cid:89)

t=1

d=1

p(Std |S<t,d, St,<d).

(9)

3.2.2 RQ-Transformer Architecture

A na¨ıve approach can unfold S into a sequence of length
T D using the raster-scan order and feed it to the conven-
tional transformer [46]. However, it neither leverages the
reduced length of T by RQ-VAE and nor reduces the com-
putational costs. Thus, we propose RQ-Transformer to efﬁ-
ciently learn the codes extracted by RQ-VAE with depth D.
As shown in Figure 2, RQ-Transformer consists of spatial
transformer and depth transformer.

Spatial Transformer The spatial transformer is a stack
of masked self-attention blocks to extract a context vector
that summarizes the information in previous positions. For
the input of the spatial transformer, we reuse the learned
codebook of RQ-VAE with depth D. Speciﬁcally, we deﬁne
the input ut of the spatial transformer as

ut = PET (t) +

D
(cid:88)

d=1

e(St−1,d)

for t > 1,

(10)

where PET (t) is a positional embedding for spatial position
t in the raster-scan order. Note that the second term is equal

to the quantized feature vector of an image in Eq. 5. For
the input at the ﬁrst position, we deﬁne u1 as a learnable
embedding, which is regarded as the start of a sequence.
After the sequence (ut)T
t=1 is processed by the spatial trans-
former, a context vector ht encodes all information of S<t
as

ht = SpatialTransformer(u1, · · · , ut).

(11)

Depth Transformer Given the context vector ht,
the
depth transformer autoregressively predicts D codes
(St1, · · · , StD) at position t. At position t and depth d, the
input vtd of the depth transformer is deﬁned as the sum of
the code embeddings of up to depth d − 1 such that

vtd = PED(d) +

d−1
(cid:88)

d(cid:48)=1

e(Std(cid:48))

for d > 1,

(12)

where PED(d) is a positional embedding for depth d and
shared at every position t. We do not use P ET (t) in vtd,
since the positional information is already encoded in ut.
For d = 1, we use vt1 = PED(1)+ht. Note that the second
term in Eq. 12 corresponds to a quantized feature vector
ˆZ(d−1)
at depth d − 1 in Eq. 5. Thus, the depth transformer
hw
predicts the next code for a ﬁner estimation of ˆZt based
on the previous estimations up to d − 1. Finally, the depth
transformer predicts the conditional distribution ptd(k) =
p(Std = k|S<t,d, St,<d) as

ptd = DepthTransformer(vt1, · · · , vtd).

(13)

RQ-Transformer is trained to minimize LAR, which is the
negative log-likelihood (NLL) loss:

LAR = ESEt,d [− log p(Std|S<t,d, St,<d)] .

(14)

Computational Complexity Our RQ-Transformer can
efﬁciently learn and predict the T × D code maps of RQ-
VAE, since RQ-Transformer has lower computational com-
plexity than the na¨ıve approach, which uses the unfolded
1D sequence of T D codes. When computing T D length
of sequences, a transformer with N layers has O(N T 2D2)
of computational complexity [46]. On the other hand, let
us consider a RQ-Transformer with total N layers, where
the number of layers in the spatial transformer and depth
transformer is Nspatial and Ndepth, respectively. Then, the
spatial transformer requires O(NspatialT 2) and the depth
transformer requires O(NdepthT D2), since the maximum
sequence lengths for the spatial transformer and depth trans-
former are T and D, respectively. Hence, the compu-
tational complexity of RQ-Transformer is O(NspatialT 2 +
NdepthT D2), which is much less than O(N T 2D2). In Sec-
tion 4.3, we show that our RQ-Transformer has a faster
speed of image generation than previous AR models.

3.2.3 Soft Labeling and Stochastic Sampling

The exposure bias [38] is known to deteriorate the perfor-
mance of an AR model due to the error accumulation from
the discrepancy of predictions in training and inference.
During an inference of RQ-Transformer, the prediction er-
rors can also accumulate along with the depth D, since ﬁner
estimation of the feature vector becomes harder as d in-
creases.

Thus, we propose soft labeling and stochastic sampling
of codes from RQ-VAE to resolve the exposure bias. Sched-
uled sampling [2] is a way to reduce the discrepancy. How-
ever, it is unsuitable for a large-scale AR model, since multi-
ple inferences are required at each training step and increase
the training cost. Instead, we leverage the geometric rela-
tionship of code embeddings in RQ-VAE. We deﬁne a cate-
gorical distribution on [K] conditioned by a vector z ∈ Rnz
as Qτ (k|z), where τ > 0 is a temperature

Qτ (k |z) ∝ e−(cid:107)z−e(k)(cid:107)2

2/τ

for k ∈ [K].

(15)

As τ approaches zero, Qτ is sharpened and converges to the
one-hot distribution Q0(k |z) = 1[k = Q(z; C)].

Soft Labeling of Target Codes Based on the distance be-
tween code embeddings, soft labeling is used to improve
the training of RQ-Transformer by explicit supervision on
the geometric relationship between the codes in RQ-VAE.
For a position t and a depth d, let Zt be a feature vector of
an image and rt,d−1 be a residual vector at depth d − 1 in
Eq. 4. Then, the NLL loss in Eq. 14 uses the one-hot label
Q0(·|rt,d−1) as the supervision of Std. Instead of the one-
hot labels, we use the softened distribution Qτ (·|rt,d−1) as
the supervision.

Stochastic Sampling for Codes of RQ-VAE Along with
the soft labeling above, we propose stochastic sampling of
the code map from RQ-VAE to reduce the discrepancy in
Instead of the deterministic code
training and inference.
selection of RQ in Eq. 4, we select the code Std by sam-
pling from Qτ (·|rt,d−1). Note that our stochastic sampling
is equivalent to the original code selection of SQ in the limit
of τ → 0. The stochastic sampling provides different com-
positions of codes S for a given feature map of an image.

4. Experiments

In this section, we empirically validate our model for
high-quality image generation. We evaluate our model on
unconditional image generation benchmarks in Section 4.1
and conditional image generation in Section 4.2. The com-
putational efﬁciency of RQ-Transformer is shown in Sec-
tion 4.3. We also conduct an ablation study to understand
the effectiveness of RQ-VAE in Section 4.4.

5

Figure 3. Generated 256×256 images by our models. First row: LSUN-{cat, bedroom}. Second row: LSUN-church and FFHQ. Third
row: ImageNet and CC-3M. The text conditions of CC-3M are “Mountains and hills reﬂecting over a surface,” “A sunset over the skyline
of a city,” and “Businessman with a paper bag on head,” respectively.

Table 1. Comparison of FIDs for unconditional image generation
on LSUN-{Cat, Bedroom, Church} [48] and FFHQ [25].

VDVAE [7]
DDPM [21]
ImageBART [13]
StyleGAN2 [26]
BigGAN [3]
DCT [33]
VQ-GAN [14]
RQ-Transformer

Cat
-
19.75
15.09
7.25
-
-
17.31
8.64

Bedroom Church

-
4.90
5.51
2.35
-
6.40
6.35
3.04

-
7.89
7.32
3.86
-
7.56
7.81
7.45

FFHQ
28.5
-
9.57
3.8
12.4
13.06
11.4
10.38

For a fair comparison, we adopt the model architec-
ture of VQ-GAN [14]. However, since RQ-VAEs con-
vert 256×256×3 RGB images into 8×8×4 codes, we add
an encoder and decode block to RQ-VAE and further de-
creases the resolution of the feature map by half. All RQ-
Transformers have Nspatial = 24 and Ndepth = 4 except for
the model of 1.4B parameters that has Nspatial = 42 and
Ndepth = 6. We include all details of implementation in the
supplementary material.

4.1. Unconditional Image Generation

The quality of unconditional

image generation is
evaluated on the LSUN-{cat, bedroom, church} [48] and
FFHQ [25] datasets. The codebook size K is 2048 for

FFHQ and 16384 for LSUN. For the FFHQ dataset, RQ-
VAE is trained from scratch for 100 epochs. We also use
early stopping for RQ-Transformer when the validation loss
is minimized, since the small size of FFHQ leads to overﬁt-
ting of AR models. For the LSUN datasets, we use a pre-
trained RQ-VAE on ImageNet and ﬁnetune the model for
one epoch on each dataset. Considering the dataset size,
we use RQ-Transformer of 612M parameters for LSUN-
{cat, bedroom} and 370M parameters for LSUN-church
and FFHQ. For the evaluation measure, we use Frechet In-
ception Distance (FID) [20] between 50K generated sam-
ples and all training samples. Following the previous stud-
ies [13, 14], we also use top-k and top-p sampling to report
the best performance.

Table 1 shows that our model outperforms the other AR
models on unconditional image generation. For small-scale
datasets such as LSUN-church and FFHQ, our model out-
performs DCT [33] and VQ-GAN [14] with marginal im-
provements. However, for a larger scale of datasets such
as LSUN-{cat, bedroom}, our model signiﬁcantly outper-
forms other AR models and diffusion-based models [13,
21]. We conjecture that the performance improvement
comes from the shorter sequence length by RQ-VAE, since
SQ-Transformer can easily learn the long-range interactions
between codes in the short length of the sequence. In the
ﬁrst two rows of Figure 3, we show that RQ-Transformer
can unconditionally generate high-quality images.

6

Table 2. Comparison of FIDs and ISs for class-conditioned image
generation on ImageNet [9] 256×256. † denotes a model without
our stochastic sampling and soft labeling. ‡ denotes the use of
rejection sampling or gradient guidance by pretrained classiﬁer. ∗
denotes the use of RQ-VAE trained for 50 epochs.

Table 4. Comparison of FIDs between ImageNet validation im-
ages and their reconstructed images according to codebook size
(K) and the shape of code map H × W × D. † denotes the repro-
duced performance, and ∗ denotes 50 epochs of training.

FID

Params

101.0
61.6
168.6
203.6
∼45
n/a
74.3
86.8±1.4
95.8±2.1
104.3±1.5
112.4±1.1
119.0±2.5
134.0±3.0

554M 10.94
21.19
3.5B
7.53
164M
6.84
112M
∼31
13.5B
738M
36.5
15.78
1.4B
480M 15.72
821M 14.06
821M 13.11
11.56
1.4B
8.71
1.4B
7.55
3.8B

IS
without rejection sampling or gradient guidance
ADM [11]
ImageBART [13]
BigGAN [3]
BigGAN-deep [3]
VQ-VAE2 [39]
DCT [33]
VQ-GAN [14]
RQ-Transformer
RQ-Transformer†
RQ-Transformer
RQ-Transformer
RQ-Transformer∗
RQ-Transformer∗
with rejection sampling or gradient guidance
ADM‡ [11]
ImageBART‡ [13]
VQ-VAE2‡ [39]
VQ-GAN‡ [14]
RQ-Transformer‡
RQ-Transformer∗‡
RQ-Transformer∗‡
Validation Data

554M
3.5B
13.5B
1.4B
1.4B
1.4B
3.8B
-

4.59
7.44
∼10
5.20
4.45
3.89
3.80
1.62

186.7
273.5±4.1
∼330
280.3±5.5
326.0±3.5
337.5±4.6
323.7±2.8
234.0

Table 3. Comparison of FID and CLIP score [36] on the validation
data of CC-3M [43] for text-conditioned image generation.

VQ-GAN [14]
ImageBART [13]
RQ-Transformer

Params
FID
600M 28.86
22.61
2.8B
654M 12.33

CLIP-s
0.20
0.23
0.26

4.2. Conditional Image Generation

We use ImageNet [9] and CC-3M [43] for a class- and
text-conditioned image generation, respectively. We train
RQ-VAE with K=16,384 on ImageNet training data for 10
epochs and reuse the trained RQ-VAE for CC-3M. For Im-
ageNet, we also use RQ-VAE trained for 50 epochs to ex-
amine the effect of improved reconstruction quality on im-
age generation quality of RQ-Transformer in Table 2. For
conditioning, we append the embeddings of class and text
conditions to the start of input for spatial transformer. The
texts of CC-3M are represented as a sequence of at most 32
tokens using a byte pair encoding [41, 47].

Table 2 shows that our model signiﬁcantly outperforms
previous models on ImageNet. Our RQ-Transformer of
480M parameters is competitive with the previous AR mod-

7

VQ-GAN [14]
VQ-GAN†
VQ-GAN
VQ-GAN
VQ-GAN
RQ-VAE
RQ-VAE
RQ-VAE∗
RQ-VAE
RQ-VAE

H × W × D
16×16×1
16×16×1
8×8×1
8×8×1
8×8×1
8×8×2
8×8×4
8×8×4
8×8×8
8×8×16

K
16,384
16,384
16,384
65,536
131,072
16,384
16,384
16,384
16,384
16,384

rFID
4.90
4.32
17.95
17.66
17.09
10.77
4.73
3.20
2.69
1.83

els including VQ-VAE2 [39], DCT [33], and VQ-GAN [14]
without rejection sampling, although our model has 3× less
parameters than VQ-GAN. In addition, RQ-Transformer
of 821M parameters outperforms the previous AR mod-
els without rejection sampling. Our stochastic sampling
is also effective for performance improvement, while RQ-
Transformer without it still outperforms other AR models.
RQ-Transformer of 1.4B parameters achieves 11.56 of FID
score without rejection sampling. When we increase the
training epoch of RQ-VAE from 10 into 50 and improve
the reconstruction quality, RQ-Transformer of 1.4B param-
eters further improves the performance and achieves 8.71
of FID. Moreover, when we further increase the number of
parameters to 3.8B, RQ-Transformer achieves 7.55 of FID
score without rejection sampling and is competitive with
BigGAN [3]. When ResNet-101 [18] is used for rejec-
tion sampling with 5% and 12.5% of acceptance rates for
1.4B and 3.8B parameters, respectively, our model outper-
forms ADM [11] and achieves the state-of-the-art score of
FID. Figure 3 also shows that our model can generate high-
quality images.

RQ-Transformer can also generate high-quality im-
ages based on various text conditions of CC-3M. RQ-
Transformer shows signiﬁcantly higher performance than
VQ-GAN with a similar number of parameters.
In addi-
tion, although RQ-Transformer has 23% of parameters, our
model signiﬁcantly outperforms ImageBART [13] on both
FID and CLIP score [36] (with ViT-B/32 [12]). The results
imply that RQ-Transformer can easily learn the relationship
between a text and an image when the reduced sequence
length is used for the image. Figure 3 shows that RQ-
Transformer trained on CC-3M can generate high-quality
images using various text conditions. In addition, the text
conditions in Figure 1 are novel compositions of visual con-
cepts, which are unseen in training.

rate-distortion trade-off. Contrastively, note that the rFIDs
are signiﬁcantly improved when we increase the quanti-
zation depth D with a codebook of ﬁxed size K=16,384.
Thus, our RQ-VAE can further reduce the spatial resolution
than VQ-GAN, while conserving the reconstruction qual-
ity. Although RQ-VAE with D > 4 can further improve the
reconstruction quality, we use RQ-VAE with 8×8×4 code
map for AR modeling of images, considering the compu-
tational costs of RQ-Transformer. In addition, the longer
training of RQ-VAE can further improve the reconstruction
quality, but we train RQ-VAE for 10 epochs as the default
due to its increased training time.

Figure 5 and 6 substantiate our claim that RQ-VAE con-
ducts the coarse-to-ﬁne estimation of feature maps. For ex-
ample, Figure 5 shows the reconstructed images G( ˆZ(d))
of a quantized feature map at depth d in Eq. 4. When we
only use the codes at d = 1, the reconstructed image is
blurry and only contains coarse information of the origi-
nal image. However, as d increases and the information
of remaining codes is sequentially added, the reconstructed
image includes more clear and ﬁne-grained details of the
image. We visualize the distribution of the code usage at
each depth d over the norm of code embeddings in Fig-
ure 6. Since RQ conducts the coarse-to-ﬁne approximation
of a feature map, a smaller norm of code embeddings are
used as d increases. Moreover, the overlaps between the
code usage distributions show that many codes are shared
in different levels of depth d. Thus, the shared codebook of
RQ-VAE can maximize the utility of its codes.

5. Conclusion

Discrete representation of visual images is important for
an AR model to generate high-resolution images. In this
work, we have proposed RQ-VAE and RQ-Transformer for
high-quality image generation. Under a ﬁxed codebook
size, RQ-VAE can precisely approximate a feature map of
an image to represent the image as a short sequence of
codes. Thus, RQ-Transformer effectively learns to predict
the codes to generate high-quality images with low compu-
tational costs. Consequently, our approach outperforms the
previous AR models on various image generation bench-
marks such as LSUNs, FFHQ, ImageNet, and CC-3M.

Our study has three main limitations. First, our model
does not outperform StyleGAN2 [26] on unconditional im-
age generation, especially with a small-scale dataset such
as FFHQ, due to overﬁtting of AR models. Thus, regular-
izing AR models is worth exploration for high-resolution
image generation on a small dataset. Second, our study
does not enlarge the model and training data for text-to-
image generation. As a previous study [19, 37] shows that a
huge transformer can effectively learn the zero-shot text-to-
image generation, increasing the number of parameters is
an interesting future work. Third, AR models can only cap-

Figure 4. The sampling speed of RQ-Transformer with 1.4B pa-
rameters according to batch size and code map shape.

4.3. Computational Efﬁciency of RQ-Transformer

In Figure 4, we evaluate the sampling speed of RQ-
Transformer and make a comparison with VQ-GAN. Both
the models have 1.4B parameters. The shape of the in-
put code map for VQ-GAN and RQ-Transformer are set
to be 16×16×1 and 8×8×4, respectively. We use a sin-
gle NVIDIA A100 GPU for each model to generate 5000
samples with 100, 200, and 500 of batch size. The reported
speeds in Figure 4 do not include the decoding time of the
stage 1 model to focus on the effect of RQ-Transformer ar-
chitecture. The decoding time of VQ-GAN and RQ-VAE is
about 0.008 sec/image.

For the batch size of 100 and 200, RQ-Transformer
shows 4.1× and 5.6× speed-up compared with VQ-GAN.
Moreover, thanks to the memory saving from the short se-
quence length of RQ-VAE, RQ-Transformer can increase
the batch size up to 500, which is not allowed for VQ-GAN.
Thus, RQ-Transformer can further accelerate the sampling
speed, which is 0.02 seconds per image, and be 7.3× faster
than VQ-GAN with batch size 200. Thus, RQ-Transformer
is more computationally efﬁcient than previous AR models,
while achieving state-of-the-art results on high-resolution
image generation benchmarks.

4.4. Ablation Study on RQ-VAE

We conduct an ablation study to understand the effect of
RQ with respect to the codebook size (K) and the shape of
the code map (H × W × D). We measure the rFID, which
is FID between original images and reconstructed images,
on ImageNet validation data. Table 4 shows that increasing
the quantization depth D is more effective to improve the
reconstruction quality than increasing the codebook size K.
Here, we remark that RQ-VAE with D = 1 is equivalent to
VQ-GAN. For a ﬁxed codebook size K=16,384, the rFID
signiﬁcantly deteriorates as the spatial resolution H × W is
reduced from 16×16 to 8×8. Even when the codebook size
is increased to K=131,072, the rFID cannot recover the
rFID with 16×16 feature maps, since the restoration of rFID
requires the codebook of size K=16,3844 in terms of the

8

Figure 5. The examples of coarse-to-ﬁne approximation by RQ-VAE. The ﬁrst example is the original image, and the others are recon-
structed from ˆZ(d). As d increases, the reconstructed images become clear and include ﬁne-grained details of the original image.

IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 931–938, 2014. 2

[2] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam
Shazeer. Scheduled sampling for sequence prediction with
recurrent neural networks. arXiv preprint arXiv:1506.03099,
2015. 5

[3] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large
scale GAN training for high ﬁdelity natural image synthe-
In International Conference on Learning Representa-
sis.
tions, 2019. 6, 7

[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-
hini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom
Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,
Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec Radford,
Ilya Sutskever, and Dario Amodei. Language models are
few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell,
M. F. Balcan, and H. Lin, editors, Advances in Neural Infor-
mation Processing Systems, volume 33, pages 1877–1901.
Curran Associates, Inc., 2020. 2

[5] Lars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian Pe-
dregosa, Andreas Mueller, Olivier Grisel, Vlad Niculae,
Peter Prettenhofer, Alexandre Gramfort, Jaques Grobler,
Robert Layton, Jake VanderPlas, Arnaud Joly, Brian Holt,
and Ga¨el Varoquaux. API design for machine learning soft-
In ECML
ware: experiences from the scikit-learn project.
PKDD Workshop: Languages for Data Mining and Machine
Learning, pages 108–122, 2013. 13

[6] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Hee-
woo Jun, David Luan, and Ilya Sutskever. Generative pre-
In International Conference on Ma-
training from pixels.
chine Learning, pages 1691–1703. PMLR, 2020. 1, 2
[7] Rewon Child. Very deep vaes generalize autoregressive mod-
In International

els and can outperform them on images.
Conference on Learning Representations, 2020. 6

[8] Gregory F Cooper. The computational complexity of prob-
abilistic inference using bayesian belief networks. Artiﬁcial
intelligence, 42(2-3):393–405, 1990. 2

Figure 6. The distribution of used codes at each quantization
depth. The blue bar plot represents the code distribution according
to the norm of embeddings. ImageNet validation data is used.

ture unidirectional contexts to generate images compared to
other generative models. Thus, modeling of bidirectional
contexts can further improve the quality of image genera-
tion and enable AR models to be used for image manipula-
tion such as image inpainting and outpainting [13].

Although our study signiﬁcantly reduces the computa-
tional costs for AR modeling of images, training of large-
scale AR models is still expensive, consumes high amounts
of electrical energy, and can leave a huge carbon footprint,
as the scale of model and training dataset becomes large.
Thus, efﬁcient training of large-scale AR models is still
worth exploration to avoid environmental pollution.

6. Acknowledgements

This work was supported by Institute of Information &
communications Technology Planning & Evaluation(IITP)
grant funded by the Korea government(MSIT) (No.2018-
0-01398: Development of a Conversational, Self-tuning
DBMS; No.2021-0-00537: Visual Common Sense).

References

[1] Artem Babenko and Victor Lempitsky. Additive quantiza-
tion for extreme vector compression. In Proceedings of the

[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image

9

database. In 2009 IEEE conference on computer vision and
pattern recognition, pages 248–255. Ieee, 2009. 7, 12
[10] Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook
Kim, Alec Radford, and Ilya Sutskever. Jukebox: A gen-
erative model for music. arXiv preprint arXiv:2005.00341,
2020. 2, 3, 12

[11] Prafulla Dhariwal and Alex Nichol. Diffusion models beat
gans on image synthesis. arXiv preprint arXiv:2105.05233,
2021. 7

[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
arXiv preprint
formers for image recognition at scale.
arXiv:2010.11929, 2020. 7

[13] Patrick Esser, Robin Rombach, Andreas Blattmann, and
Bj¨orn Ommer. Imagebart: Bidirectional context with multi-
nomial diffusion for autoregressive image synthesis, 2021. 1,
2, 6, 7, 9, 13, 26

[14] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming
In Pro-
transformers for high-resolution image synthesis.
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 12873–12883, 2021. 1, 2, 4,
6, 7, 12

[15] Sohrab Ferdowsi, Slava Voloshynovskiy, and Dimche
Kostadinov. Regularized residual quantization: a multi-
layer sparse dictionary learning approach. arXiv preprint
arXiv:1705.00522, 2017. 2

[16] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. Advances in
neural information processing systems, 27, 2014. 2

[17] R. Gray. Vector quantization. IEEE ASSP Magazine, 1(2):4–

29, 1984. 2

[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 770–778, 2016. 7

[19] Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen,
Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B
Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws
arXiv preprint
for autoregressive generative modeling.
arXiv:2010.14701, 2020. 8

[20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. Advances in neural information processing systems,
30, 2017. 6

[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. arXiv preprint arXiv:2006.11239,
2020. 6

[22] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Efros.
Image-to-image translation with conditional adver-
sarial networks. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 1125–1134,
2017. 2, 4

[23] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual
In
losses for real-time style transfer and super-resolution.

European conference on computer vision, pages 694–711.
Springer, 2016. 4, 13

[24] Biing-Hwang Juang and A Gray. Multiple stage vector quan-
In ICASSP’82. IEEE Interna-
tization for speech coding.
tional Conference on Acoustics, Speech, and Signal Process-
ing, volume 7, pages 597–600. IEEE, 1982. 2

[25] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks.
In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 4401–4410, 2019. 6,
12

[26] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,
Jaakko Lehtinen, and Timo Aila. Analyzing and improv-
In Proceedings of the
ing the image quality of stylegan.
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 8110–8119, 2020. 6, 8

[27] Diederik P. Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. In Yoshua Bengio and Yann LeCun,
editors, 3rd International Conference on Learning Represen-
tations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings, 2015. 12

[28] Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero,
Andrew Cunningham, Alejandro Acosta, Andrew Aitken,
Alykhan Tejani, Johannes Totz, Zehan Wang, and Wenzhe
Shi. Photo-realistic single image super-resolution using a
generative adversarial network. In CVPR, 2017. 2

[29] Yue Li, Wenrui Ding, Chunlei Liu, Baochang Zhang, and
Guodong Guo. Trq: Ternary neural networks with residual
quantization. In Proceedings of the AAAI Conference on Ar-
tiﬁcial Intelligence, volume 35, pages 8538–8546, 2021. 2

[30] Zefan Li, Bingbing Ni, Wenjun Zhang, Xiaokang Yang, and
Wen Gao. Performance guaranteed network acceleration via
high-order residual quantization. In Proceedings of the IEEE
international conference on computer vision, pages 2584–
2592, 2017. 2

[31] Ilya Loshchilov and Frank Hutter. Decoupled weight de-
cay regularization. In International Conference on Learning
Representations, 2019. 12

[32] Julieta Martinez, Holger H Hoos, and James J Little. Stacked
arXiv

quantizers for compositional vector compression.
preprint arXiv:1411.2173, 2014. 2

[33] Charlie Nash, Jacob Menick, Sander Dieleman, and Peter
Battaglia. Generating images with sparse representations. In
Marina Meila and Tong Zhang, editors, Proceedings of the
38th International Conference on Machine Learning, vol-
ume 139 of Proceedings of Machine Learning Research,
pages 7958–7968. PMLR, 18–24 Jul 2021. 2, 6, 7

[34] Aaron Van Oord, Nal Kalchbrenner,

and Koray
Kavukcuoglu.
In
Proceedings of The 33rd International Conference on
Machine Learning, 2016. 1, 2, 4

recurrent neural networks.

Pixel

[35] A¨aron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse
Espeholt, Alex Graves, and Koray Kavukcuoglu. Condi-
tional image generation with pixelcnn decoders. In Proceed-
ings of the 30th International Conference on Neural Infor-
mation Processing Systems, pages 4797–4805, 2016. 2
[36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,

10

Polosukhin. Attention is all you need. In Advances in neural
information processing systems, pages 5998–6008, 2017. 4,
5, 12

[47] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chau-
mond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim
Rault, R´emi Louf, Morgan Funtowicz, Joe Davison, Sam
Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama
Drame, Quentin Lhoest, and Alexander M. Rush. Trans-
In
formers: State-of-the-art natural language processing.
Proceedings of the 2020 Conference on Empirical Methods
in Natural Language Processing: System Demonstrations,
pages 38–45, Online, Oct. 2020. Association for Computa-
tional Linguistics. 7

[48] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas
Funkhouser, and Jianxiong Xiao. Lsun: Construction of a
large-scale image dataset using deep learning with humans
in the loop. arXiv preprint arXiv:1506.03365, 2015. 6, 12

Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
Krueger, and Ilya Sutskever. Learning transferable visual
models from natural language supervision. In Marina Meila
and Tong Zhang, editors, Proceedings of the 38th Interna-
tional Conference on Machine Learning, volume 139 of Pro-
ceedings of Machine Learning Research, pages 8748–8763.
PMLR, 18–24 Jul 2021. 7, 13

[37] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.
In Marina Meila and
Zero-shot text-to-image generation.
Tong Zhang, editors, Proceedings of the 38th International
Conference on Machine Learning, volume 139 of Pro-
ceedings of Machine Learning Research, pages 8821–8831.
PMLR, 18–24 Jul 2021. 1, 2, 8, 13, 26

[38] Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli, and
Wojciech Zaremba. Sequence level training with recurrent
neural networks. In Yoshua Bengio and Yann LeCun, edi-
tors, 4th International Conference on Learning Representa-
tions, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016,
Conference Track Proceedings, 2016. 2, 4, 5

[39] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generat-
ing diverse high-ﬁdelity images with vq-vae-2. In Advances
in neural information processing systems, pages 14866–
14876, 2019. 2, 7

[40] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P.
Kingma. Pixelcnn++: A pixelcnn implementation with dis-
cretized logistic mixture likelihood and other modiﬁcations.
In ICLR, 2017. 2

[41] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neu-
ral machine translation of rare words with subword units.
In Proceedings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long Papers),
pages 1715–1725, 2016. 7

[42] Claude E Shannon et al. Coding theorems for a discrete
source with a ﬁdelity criterion. IRE Nat. Conv. Rec, 4(142-
163):1, 1959. 1, 3

[43] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu
Soricut. Conceptual captions: A cleaned, hypernymed, im-
age alt-text dataset for automatic image captioning. In Pro-
ceedings of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), pages
2556–2565, Melbourne, Australia, July 2018. Association
for Computational Linguistics. 7, 12

[44] Karen Simonyan and Andrew Zisserman. Very deep con-
volutional networks for large-scale image recognition.
In
Yoshua Bengio and Yann LeCun, editors, 3rd International
Conference on Learning Representations, ICLR 2015, San
Diego, CA, USA, May 7-9, 2015, Conference Track Proceed-
ings, 2015. 13

[45] Aaron van den Oord, Oriol Vinyals, and koray kavukcuoglu.
Neural discrete representation learning. In I. Guyon, U. V.
Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-
wanathan, and R. Garnett, editors, Advances in Neural Infor-
mation Processing Systems, volume 30. Curran Associates,
Inc., 2017. 1, 2, 4

[46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia

11

Table 5. The hyperparameters for implementing RQ-Transformer. We follow the same notation in the main paper. ne represents the
dimensionality of features in RQ-Transformer, and # heads represents the number of heads in self-attentions of RQ-Transformer.

Dataset
LSUN-cat [48]
LSUN-bedroom [48]
LSUN-church [48]
FFHQ [25]
ImageNet [9]
ImageNet [9]
ImageNet [9]
ImageNet [9]
CC-3M [43]

Nspatial Ndepth

26
26
24
24
12
24
42
42
24

4
4
4
4
4
4
6
6
4

K
# params
16384
612M
16384
612M
16384
370M
2048
370M
16384
480M
16384
821M
1388M 16384
3822M 16384
16384
654M

T D nz
256
4
64
256
4
64
256
4
64
256
4
64
256
4
12
256
4
64
256
4
64
256
4
64
256
4
95

ne
1280
1280
1024
1024
1536
1536
1536
2560
1280

# heads
20
20
16
16
24
24
24
40
20

τ
0.5
0.5
0.5
1.0
0.5
0.5
0.5
0.5
0.5

A. Implementation Details

A.1. Architecture of RQ-VAE

For the architecture of RQ-VAE, we follow the architecture of VQ-GAN [14] for a fair comparison. However, we add two

residual blocks with 512 channels each followed by a down-/up-sampling block to extract feature maps of resolution 8×8.

A.2. Architecture of RQ-Transformer

The RQ-Transformer, which consists of the spatial transformer and the depth transformer, adopts a stack of self-attention
blocks [46] for each compartment. In Table 5, we include the detailed information of hyperparameters to implement our
RQ-Transformers. All RQ-Transformers in Table 5 uses RQ-VAE with 8×8×4 shape of codes. For CC-3M, the length of
text conditions is 32, and the last token in text conditions predicts the code at the ﬁrst position of images. Thus, the total
sequence length (T ) of RQ-Transformer is 95.

A.3. Training Details

For ImageNet, RQ-VAE is trained for 10 epochs with batch size 128. We use the Adam optimizer [27] with β1 = 0.5 and
β2 = 0.9, and learning rate is set 0.00004. The learning rate is linearly warmed up during the ﬁrst 0.5 epoch. We do not
use learning rate decay, weight decaying, nor dropout. For the adversarial and perceptual loss, we follow the experimental
setting of VQ-GAN [14]. In particular, the weight for the adversarial loss is set 0.75 and the weight for the perceptual loss is
set 1.0. To increase the codebook usage of RQ-VAE, we use random restart of unused codes proposed in JukeBox [10]. For
LSUN-{cat, bedroom, church}, we use the pretrained RQ-VAE on ImageNet and ﬁnetune it for one epoch with 0.000004 of
learning rate. For FFHQ, we train RQ-VAE for 150 epochs of training data with 0.00004 of learning rate and ﬁve epochs of
warm-up. For CC-3M, we use the pretrained RQ-VAE on ImageNet without ﬁnetuning.

All RQ-Transformers are trained using the AdamW optimizer [31] with β1 = 0.9 and β2 = 0.95. We use the cosine
learning rate schedule with 0.0005 of the initial learning rate. The RQ-Transformer is trained for 90, 200, 300 epochs for
LSUN-bedroom, -cat, and -church respectively. The weight decay is set 0.0001, and the batch size is 16 for FFHQ and 2048
for other datasets. In all experiments, the dropout rate of each self-attention block is set 0.1 except 0.3 for 3.8B parameters
of RQ-Transformer. We use eight NVIDIA A100 GPUs to train RQ-Transformer of 1.4B parameters, and four GPUs to train
RQ-Transformers of other sizes. The training time is <9 days for LSUN-cat, LSUN-bedroom, <4.5 days for ImageNet, and
CC-3M, and <1 day for LSUN-church and FFHQ. We use the early stopping at 39 epoch for the FFHQ dataset, considering
the overﬁtting of the RQ-Transformer due to the small scale of the dataset.

B. Additional Results of Generated Images by RQ-Transformer

B.1. Additional Examples of Unconditional Image Generation for LSUNs and FFHQ

We show the additional examples of unconditional

trained on LSUN-
{cat, bedroom, church} and FFHQ. Figure 7, 8, 9, and 10 show the results of LSUN-cat, LSUN-bedroom LSUN-church,
and FFHQ, respectively. For the top-k (top-p) sampling, 512 (0.9), 8192 (0.85), 1400 (1.0), and 2048 (0.95) are used respec-
tively.

image generation by RQ-VAEs

12

B.2. Nearest Neighbor Search of Generated Images for FFHQ

For the training of FFHQ, we use early stopping for RQ-Transformer when the validation loss is minimized, since RQ-
Transformer can memorize all training samples due to the small scale of FFHQ. Despite the use of early stopping, we further
examine whether our model memorizes the training samples or generates new images. To visualize the nearest neighbors in
the training images of FFHQ to generated images, we use a KD-tree [5], which is constructed by the VGG-16 features [44]
of training images. Figure 11 shows that our model does not memorize the training data, but generates new face images for
unconditional sample generation of FFHQ.

B.3. Ablation Study on Soft Labeling and Stochastic Sampling

For 821M parameters of RQ-Transformer trained on ImageNet, RQ-Transformer achieves 14.06 of FID score when neither
stochastic sampling nor soft labeling is used. When stochastic sampling is applied to the training of RQ-Transformer, 13.24
of FID score is achieved. When only soft labeling is used without stochastic sampling, RQ-Transformer achieves 14.87
of FID score, and the performance worsens. However, when both stochastic sampling and soft labeling are used together,
RQ-Transformer achieves 13.11 of FID score, which is improved performance than baseline.

B.4. Additional Examples of Class-Conditioned Image Generation for ImageNet

We visualize the additional examples of class-conditional image generation by RQ-Transformer trained on ImageNet.
Figure 12, 13, and 14 show the generated samples by RQ-Transformer with 1.4B parameters conditioned on a few selected
classes. Those images are sampled with top-k 512 and top-p 0.95.In addition, Figure 16 shows the generated samples using
the rejection sampling with ResNet-101 is applied with various acceptance rates. The images in Figure 16 are sampled with
ﬁxed p = 1.0 for top-p sampling and different acceptance rates of the rejection sampling and top-k values. The (top-k,
acceptance rate)s are (512, 0,5), (1024, 0.25), and (2048, 0.05), and their corresponding FID scores are 7.08, 5.62, and
4.45. Figure 15 shows the generated samples of RQ-Transformer with 3.8B parameters using rejection sampling with (4098,
0.125).

B.5. Additional Examples of Text-Conditioned Image Generation for CC-3M

We visualize the additional examples of text-conditioned image generation by RQ-Transformer trained on CC-3M. Figure
17 shows the generated samples conditioned by various texts, which are unseen during training. Speciﬁcally, we manually
choose four pairs of sentences, which share visual content with different contexts and styles, to validate the compositional
generalization of our model. All images are sampled with top-k 1024 and top-p 0.9. Additionally, Figure 18 shows the
samples conditioned on randomly chosen texts from the validation set of CC-3M. For the images in Figure 18, we use the
re-ranking with CLIP similarity score [36] as in [37] and [13] and select the image with the highest CLIP score among 16
generated images.

B.6. The Effects of Top-k & Top-p Sampling on FID Scores

In this section, we show the FID scores of the RQ-Transformer trained on ImageNet according to the choice of k and p
for top-k and top-p sampling, respectively. Figure 19 and 20 shows the FID scores of 821M and 1400M parameters of RQ-
Transformer according to different ks and ps. Although we report the global minimum FID score, the minimum FID score at
each k is not signiﬁcantly deviating from the global minimum. For instance, the minimum FID attained by RQ-Transformer
with 1.4B parameters is 11.58 while the minimum for each k is at most 11.87. When the rejection sampling of generated
images is used to select high-quality images, Figure 21 shows that higher top-k values are effective as the acceptance rate
decreases, since various and high-quality samples can be generated with higher top-k values. Finally, for the CC-3M dataset,
Figure 22 shows the FID scores and CLIP similarity scores according to different top-k and top-p values.

C. Additional Results of Reconstruction Images by RQ-VAE

C.1. Coarse-to-Fine Approximation of Feature Maps by RQ-VAE

In this section, we further explain that RQ-VAE with depth D conducts the coarse-to-ﬁne approximation of a feature map.
Table 6 shows the reconstruction error Lrecon, the commitment loss Lcommit, and the perceptual loss [23], when RQ-VAE
uses the partial sum ˆZ(d) of up to d code embeddings for the quantized feature map of an image. All three losses, which
are the reconstruction and perceptual loss of a reconstructed image, and the commitment loss Lcommit of the feature map,
monotonically decrease as d increases. The results imply that RQ-VAE can precisely approximate the feature map of an

13

Table 6. Results of coarse-to-ﬁne approximation by the RQ-VAE with 8×8×4 shape of M. Reconstruction loss Lrecon, commitment loss
Lcommit, perceptual loss, and reconstruction FID (rFID) are measured on ImageNet validation data.

ˆX
G( ˆZ(1))
G( ˆZ(2))
G( ˆZ(3))
G( ˆZ(4))

Lrecon Lcommit
0.12
0.018
0.10
0.014
0.091
0.012
0.082
0.010

Perceptual loss
0.12
0.090
0.075
0.068

rFID
100.86
22.74
7.66
4.73

image, when RQ-VAE iteratively quantizes the feature map and its residuals. Figure 23 also shows that the reconstructed
images contain more ﬁne-grained information of the original images as d increases. Thus, the experimental results validate
that our RQ-VAE conducts the coarse-to-ﬁne approximation, and RQ-Transformer can learn to generate the feature vector at
the next position in a coarse-to-ﬁne manner.

C.2. The Effects of Adversarial and Perceptual Losses on Training of RQ-VAE

In Figure 24, we visualize the reconstructed images by RQ-VAEs, which are trained without and with adversarial and
perceptual losses. When the adversarial and perceptual losses are not used (the second and third columns), the reconstructed
images are blurry, since the codebook is insufﬁcient to include all information of local details in the original images. However,
despite the blurriness, note that RQ-VAE with D = 4 (the third column) much improves the quality of reconstructed images
than VQ-VAE (or RQ-VAE with D = 1, the second column).

Although the adversarial and perceptual losses are used to improve the quality of image reconstruction, RQ is still impor-
tant to generate high-quality reconstructed images with low distortion. When the adversarial and perceptual losses are used
in the training of RQ-VAEs (the fourth and ﬁfth columns), the reconstructed images are much clear and include ﬁne-grained
details of the original images. However, the reconstructed images by VQ-VAE (or RQ-VAE with D = 1, the fourth column)
include the unrealistic artifacts and the high distortion of the original images. Contrastively, when RQ with D = 4 is used
to encode the information of the original images, the reconstructed images by RQ-VAE (the ﬁfth column) are signiﬁcantly
realistic and do not distort the visual information in the original images.

C.3. Using D Non-Shared Codebooks of Size D/K for RQ-VAE

As mentioned in Section 3.1.2, a single codebook C of size K is shared for every quantization depth D instead of D
non-shared codebooks of size D/K. When we replace the shared codebook of size 16,384 with four non-shared codebooks
of size 4,096, rFID of RQ-VAE increases from 4.73 to 5.73, since the non-shared codebooks can approximate at most
(K/D)D clusters only. In fact, a shared codebook with K=4,096 has 5.94 of rFID, which is similar to 5.73 above. Thus, the
shared codebook is more effective to increase the quality of image reconstruction with limited codebook size than non-shared
codebooks.

14

Figure 7. Additional examples of unconditional image generation by our model trained on LSUN-cat.

15

Figure 8. Additional examples of unconditional image generation by our model trained on LSUN-bedroom.

16

Figure 9. Additional examples of unconditional image generation by our model trained on LSUN-church.

17

Figure 10. Additional examples of unconditional image generation by our model trained on FFHQ.

18

Figure 11. Visualization of nearest neighbors in the FFHQ training samples to our generated samples. In each row, the ﬁrst image is our
generation. The nearest neighbors to the ﬁrst image are visualized according to the similarity of VGG-16 features in descending order.

19

Figure 12. Additional examples of conditional image generation by 1.4B parameters of RQ-Transformer trained on ImageNet. Top: Tench
(0). Middle: Ostrich (9). Bottom: Bald eagle (22).

20

Figure 13. Additional examples of conditional image generation by 1.4B parameters of RQ-Transformer trained on ImageNet. Top:
Lorikeet (90). Middle: Tibetan terrier (200). Bottom: Tiger beetle (300).

21

Figure 14. Additional examples of conditional image generation by 1.4B parameters of RQ-Transformer trained on ImageNet. Top: Coffee
pot (505). Middle: Space shuttle (812). Bottom: Cheeseburger (933).

22

Figure 15. Additional examples of conditional image generation by 3.8B parameters of RQ-Transformer trained on ImageNet. The classes
of images in each line are tench (0), ostrich (9), bald eagle (22), lorikeet (90), tibetan terrier (200), tiger beetle (300), coffee pot (505),
space shuttle (812), and cheeseburger (933), respectively.

23

Figure 16. Additional examples of conditional image generation by 1.4B parameters of RQ-Transformer trained on ImageNet with rejection
sampling. All images are sampled with top-p=1.0 with different acceptance rates of the rejection sampling and top-k values.
(Top)
Acceptance rate=0.5 and top-k=512, Middle: Acceptance rate=0.25 and top-k=1024. Bottom: Acceptance rate=0.05 and top-k=2048.

24

A
of
photograph
crowd of people en-
joying night market.

A
of
photograph
crowd of people under
cherry blossom trees.

A small house in the
wilderness.

A small house on the
shore.

Sunset over the skyline
of a city.

Night landscape of the
skyline of a city.

An illustration of a
cathedral.

A painting of a cathe-
dral.

Figure 17. Additional examples of text-conditional image generation by our model trained on CC-3M. The text conditions are customized
prompts, which are unseen during the training of RQ-Transformer. All images are sampled with top-k 1024 and top-p 0.9.

25

Figure 18. Additional examples of text-conditioned image generation by our model trained on CC-3M. Text prompts are randomly chosen
from the validation data. Best 1 of 16 with re-ranking as in [13, 37].

26

farmer carrying acrate with freshvegetablesisolated on whitebackgrounddecorative waterwheel in thegarden , elementof modernlandscape designa vectorillustration ofcartoon sad cutelittle penguinbald eagle perchedon a frosted pinetree during winterbaked beans in acan with a spoonaudience memberslisten during theforum .a drawing ofsliced limechair - he was adesigner that ledto the use ofgeometric patternsas you can seehere in the woodand fabric .tourists andskiers in front ofa chaletmysteriouscharming womanlooking both sidesand showingsilence gestureisolated overblackroasted chestnutsin a paper bagfruits andvegetablesarranged in seriesin the order ofrainbow colors onblack background .the fountain inthe square .a couple loungesin an open airliving room set ina tropical forestfront view of thetemplemake your journeyto the great widesomewhere evenmore enchantingwith this backpack.pine tree on abackground of theseathe auditoriumlooking towardsthe stage andorganexample of aclassic exteriorhome designtennis playerrises his trophyafter winningsports associationheldroad on themountain duringrainy seasonbanana plantationin presidentialstateview from yourdeck of a localsailing vesseltaken by acustomer .watch : what hasearth looked like?Figure 19. FID of 50K generated samples of RQ-Transformer (821M) against the training and the validation split of ImageNet.

Figure 20. FID of 50K generated samples of RQ-Transformer (1.4B) against the training and the validation split of ImageNet.

Figure 21. FID of rejection-sampled 50K samples of RQ-Transformer (1.4B) against the training and the validation split of ImageNet.

27

0.60.650.70.750.80.850.90.951.0top-p256512102420484096819216384top-k17.4916.3915.3014.5414.0713.5413.1113.1713.5215.9615.0114.2113.7313.2813.2713.5213.9515.2315.1214.2413.6313.3713.5013.8914.9516.8019.6814.6213.7813.4313.5714.0715.3817.4120.5925.0114.3713.7613.6113.7914.4916.5019.6323.1729.0314.0513.7013.5314.0315.2016.9919.9824.3430.3314.1513.5113.6614.0715.0317.1620.1424.1431.05141618202224262830FID (vs. train)0.60.650.70.750.80.850.90.951.0top-p256512102420484096819216384top-k16.5815.4614.3713.5412.9812.3811.8711.7911.9414.9613.9913.1512.5611.9811.8411.8912.1713.1714.0313.0712.3711.9811.9312.1412.9614.4717.0113.4212.5112.0111.9812.2713.3015.0217.8421.8513.1012.4112.0912.0912.5714.2416.9920.2025.5812.8412.3312.0012.2713.1614.6917.3221.2826.8112.9012.1612.1212.3013.0114.8317.4221.0927.471214161820222426FID (vs. valid)0.60.650.70.750.80.850.90.951.0top-p256512102420484096819216384top-k14.9214.2813.3312.7512.3011.9011.5811.5611.9313.8013.0712.5011.9011.6111.6311.7412.5213.5512.9612.4711.9711.7311.9112.4413.3215.0017.6412.4412.1311.9311.7912.5413.5015.4818.4122.7812.3311.9511.8512.1912.9514.7617.2721.0826.5212.2711.9511.8712.2013.2915.4118.2122.2827.8412.2611.8811.8712.2813.3415.4318.2222.3328.26121416182022242628FID (vs. train)0.60.650.70.750.80.850.90.951.0top-p256512102420484096819216384top-k14.3313.6312.7012.0511.5111.0110.6110.4510.6013.1112.3311.6610.9810.5810.4510.4110.9511.7112.1511.5610.9210.5610.5710.8811.5312.9015.2011.5311.0810.7410.4610.9411.6513.3215.9019.8411.3810.8510.5410.7111.2312.7114.8818.3023.3111.2710.8310.5610.7011.5013.2815.7219.4124.5311.2710.7610.5610.7711.5513.2915.7319.4724.9212141618202224FID (vs. valid)256512102420484096819216384top-k1.000.500.250.05acc. ratio11.9313.5517.6422.7826.5227.8428.267.127.088.5911.3413.5314.4014.796.155.405.626.798.008.538.836.715.484.654.454.604.654.66510152025FID (vs. train)256512102420484096819216384top-k1.000.500.250.05acc. ratio10.6011.7115.2019.8423.3124.5324.927.256.737.639.8511.7412.5112.837.135.985.646.377.297.727.968.907.386.175.615.525.485.497.510.012.515.017.520.022.5FID (vs. valid)Figure 22. FID and CLIP score of RQ-Transformer (654M) on CC-3M, evaluated against the validation set. Images are generated condi-
tioned on each sentence in the validation set.

28

0.70.750.80.850.90.951.0top-p2565121024204840968192top-k19.0117.8016.4815.5714.5914.0313.2017.0215.6014.6913.8013.0412.6812.6915.2614.3213.4112.8112.3312.5913.3114.5813.5912.8612.4512.6513.2615.3213.8713.1412.6012.5212.9614.2616.5813.8213.0412.4912.5913.3114.6317.2913141516171819FID (vs. valid)0.70.750.80.850.90.951.0top-p2565121024204840968192top-k0.2580.2580.2580.2590.2590.2590.2580.2580.2580.2590.2590.2580.2580.2580.2590.2580.2590.2580.2580.2570.2550.2590.2590.2580.2580.2570.2560.2530.2580.2580.2580.2570.2560.2540.2520.2590.2580.2580.2570.2560.2540.2510.2520.2530.2540.2550.2560.2570.258CLIP score (valid)Figure 23. Additional examples of coarse-to-ﬁne approximation by RQ-VAE with the 8×8×4 code map. The ﬁrst example in each row is
the original image, and the others are constructed from ˆZ(d) as d increases.

29

Figure 24. Reconstruction images by RQ-VAE with and without adversarial training. The ﬁrst image in each row is the original image.
The second and third images are reconstructed images by RQ-VAE without adversarial training. The second image is reconstructed by
RQ-VAE using 8×8×1 code map, and the third image is reconstructed by RQ-VAE using 8×8×4 code map. The fourth and ﬁfth images
are reconstructed images by RQ-VAE with adversarial training. The fourth images are reconstructed by 8×8×1 code map, and the ﬁfth
images are reconstructed by 8×8×4 code map,

30

