Unsupervised Learning of Depth and Depth-of-Field Effect from Natural Images
with Aperture Rendering Generative Adversarial Networks

Takuhiro Kaneko

NTT Communication Science Laboratories, NTT Corporation

1
2
0
2

n
u
J

4
2

]

V
C
.
s
c
[

1
v
1
4
0
3
1
.
6
0
1
2
:
v
i
X
r
a

Figure 1. Unsupervised learning of depth and depth-of-ﬁeld (DoF) effect from unlabeled natural images. (a) In training, we adopt
only a collection of single-DoF images without any additional supervision (e.g., ground-truth depth, pairs of deep and shallow DoF images,
and pretrained model). (b) Once trained, our model can synthesize tuples of deep and shallow DoF images and depths from random noise.
The generated data are beneﬁcial in training a shallow DoF renderer, which also requires no external supervision. The project page is
available at https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/ar-gan/.

Abstract

1. Introduction

Understanding the 3D world from 2D projected natural
images is a fundamental challenge in computer vision and
graphics. Recently, an unsupervised learning approach has
garnered considerable attention owing to its advantages in
data collection. However, to mitigate training limitations,
typical methods need to impose assumptions for viewpoint
distribution (e.g., a dataset containing various viewpoint
images) or object shape (e.g., symmetric objects). These as-
sumptions often restrict applications; for instance, the ap-
plication to non-rigid objects or images captured from sim-
ilar viewpoints (e.g., ﬂower or bird images) remains a chal-
lenge. To complement these approaches, we propose aper-
ture rendering generative adversarial networks (AR-GANs),
which equip aperture rendering on top of GANs, and adopt
focus cues to learn the depth and depth-of-ﬁeld (DoF) ef-
fect of unlabeled natural images. To address the ambigui-
ties triggered by unsupervised setting (i.e., ambiguities be-
tween smooth texture and out-of-focus blurs, and between
foreground and background blurs), we develop DoF mixture
learning, which enables the generator to learn real image
distribution while generating diverse DoF images. In addi-
tion, we devise a center focus prior to guiding the learning
direction. In the experiments, we demonstrate the effective-
ness of AR-GANs in various datasets, such as ﬂower, bird,
and face images, demonstrate their portability by incorpo-
rating them into other 3D representation learning GANs,
and validate their applicability in shallow DoF rendering.

Natural images are 2D projections of a 3D world. Ad-
dressing the inverse problem, i.e., understanding the 3D
world from natural images, is a fundamental challenge in
computer vision and graphics. Owing to its diverse applica-
tions in various ﬁelds, such as in robotics, content creation,
and photo editing, this challenge has been actively studied.
A direct solution to challenge is learning a 3D predic-
tor in a supervised manner using 2D and 3D data pairs
or multiview image sets. However, obtaining such data
is often impractical or time-consuming. To eliminate this
process, several studies have attempted to learn 3D rep-
resentations from single-view images (i.e., with only a
single view per training instance). However, owing to
the ill-posed nature, several studies required auxiliary in-
formation, such as 2D keypoints [56, 25] or 2D silhou-
ettes [18, 6, 36, 14], to align object positions or extract a
target object from the background. Other studies required
predeﬁned category-speciﬁc shape models (e.g., 3DMM [3]
and SMPL [40]) [24, 63, 12, 50, 51] to obtain clues for re-
construction. Although they have exhibited promising re-
sults, collecting auxiliary information still requires a labo-
rious annotation process, and a shape model requires addi-
tional preparation costs and restricts applicable objects.

To eliminate these disadvantages, fully unsupervised
learning methods that enable 3D representation learning
from single-view images without additional supervision and
shape models have been devised. Although this is a se-

1

(a) Training data(b) Generated dataDeep DoFShallow DoFDepthSingle-DoF images onlyDeep DoFShallow DoFDepthDeep DoFShallow DoFDepth 
 
 
 
 
 
vere setting, previous studies have addressed this challenge
by imposing assumptions for viewpoint distribution (e.g., a
dataset including various viewpoint images) [44, 54, 46] or
object shape (e.g., symmetric objects) [67]. The ﬁrst as-
sumption is required to learn 3D representations by sam-
pling diverse viewpoint images. The second assumption
is required to perform stereo reconstruction using a pair of
mirrored images. Although these assumptions are practical
for objects of a speciﬁc class (e.g., human faces), several ob-
jects do not satisfy these assumptions. For example, these
methods are difﬁcult to apply to non-rigid objects or im-
ages captured from similar viewpoints (e.g., ﬂower or bird
images).

To broaden the application without contradicting previ-
ous achievements, in this study, we consider complemen-
tary cues inherent in photos that have not been actively used
in previous deep generative models (including those above).
In particular, we focus on focus cues, in other words, we
consider the learning depth1 and the depth-of-ﬁeld (DoF)
effect in the defocus process. Speciﬁcally, instead of im-
posing an assumption on the viewpoint distribution, we do
so on the DoF distribution (i.e., a dataset including various
DoF images), and as shown in Figure 1, we attempt to learn
3D representations (particularly depth and DoF effect) from
a collection of single-DoF images (i.e., images with solely
a single DoF setting per training instance).

To achieve this, we propose a novel family of genera-
tive adversarial networks (GANs) [15], referred to as aper-
ture rendering GANs (AR-GANs), which equip aperture ren-
dering (e.g., light ﬁeld aperture rendering [53]) on top of
GANs. Speciﬁcally, AR-GAN initially generates a pair of a
deep DoF image and depth from a random noise, and then
renders a shallow DoF image from the generated deep DoF
image and depth via aperture rendering. With this mecha-
nism, we can synthesize various DoF images using a virtual
camera with an optical constraint on the light ﬁeld.

When AR-GAN is trained in an unsupervised manner us-
ing single-DoF images, two non-trivial challenges are ambi-
guity between the smooth texture and out-of-focus blurs and
ambiguity between the foreground and background blurs, as
we cannot obtain explicit supervision of these relationships.
For the ﬁrst problem, we introduce DoF mixture learning,
which enables the generator to learn the real image distri-
bution while generating various DoF images. This learn-
ing ensures that the generated images (deep and shallow
DoF images) are in real image distribution, and facilitates
the learning of the depth, which is a source of connecting
deep and shallow DoF images. For the second problem,
based on the observed tendency to focus on the center ob-
ject when a focused image is considered, we impose a cen-
ter focus prior, which facilitates the focusing of the center
while guiding the surroundings to be behind the focal plane.
In practice, we adopt this prior solely at the beginning of
training to guide the learning direction.

1In this study, we use depth and disparity interchangeably to indicate

disparity across a camera aperture.

To evaluate the effectiveness of AR-GAN, we ﬁrst con-
ducted experiments with comparative and ablation studies
on diverse datasets, including ﬂower (Oxford Flowers [45]),
bird (CUB-200-2011 [60]), and face (FFHQ [29]) datasets.
A signiﬁcant property of AR-GAN is its portability, which
we validated by incorporating AR-GAN into other 3D rep-
resentation learning GANs (particularly, HoloGAN [44]
and RGBD-GAN [46]). Another signiﬁcant property of
AR-GAN is its ability to synthesize a tuple of deep and
shallow DoF images and depth from a random noise, af-
ter training. We utilize this property to train a shallow DoF
renderer and empirically demonstrate its utility.

Overall, our contributions are summarized as follows:

• We provide unsupervised learning of depth and DoF
effect from unlabeled natural images. This is note-
worthy because it does not impose assumptions on the
viewpoint distribution or object shape, which are re-
quired in conventional unsupervised 3D representation
learning.

• To achieve this, we propose a novel GAN family (AR-
GANs), which generate a deep DoF image and depth
from a random noise and render a shallow DoF image
from them via aperture rendering.

• To address ambiguities caused by a fully unsupervised
setting, we devise DoF mixture learning to enable the
generator to learn real image distribution using gener-
ated diverse DoF images, and develop a center focus
prior to determine the learning direction.

• We validate the effectiveness, portability, and appli-
cability of AR-GANs via extensive experiments. The
project page is available at https://www.kecl.
ntt . co . jp / people / kaneko . takuhiro /
projects/ar-gan/.

2. Related work

Generative adversarial networks. GANs [15] have
achieved remarkable success in 2D image modeling via
a series of advancements (e.g., [5, 29, 30]). A substan-
tial property of GANs is their ability to mimic data dis-
tribution in a random sampling process without explic-
itly deﬁning the data distribution. This allows GANs to
learn various distribution types. For example, recent stud-
ies [64, 66, 44, 18, 54, 46, 37] have made it possible to
learn a 3D-aware image distribution via 3D GAN architec-
tures or 3D representations. Among them, HoloGAN [44]
and RGBD-GAN [46] share a similar motivation with us in
terms of learning 3D representations from natural images in
a fully unsupervised manner; however, the major difference
is that they adopt viewpoint cues, whereas we employ focus
cues. We empirically demonstrate this difference in Sec-
tion 5.2. Owing to this difference, the previous and present
models are not exchangeable but complementary. We verify
their compatibility in Section 5.4 by combining AR-GAN
with HoloGAN and RGBD-GAN.

Another related topic is the application of GANs for un-

2

supervised learning of the foreground and background [58,
71]. Although previous and present studies are relevant in
terms of learning image compositions, they decompose the
image discretely, whereas we learn the continuous depth.
Furthermore, we can learn the DoF effect, which has not
been achieved in previous studies.

Other relevant GANs are GANs with measurements [4,
47, 35, 26, 27], which apply measurements (e.g., mask and
noise) before matching a generated image with a real im-
age. Our aperture rendering functions similarly to those
measurements. However, in the previous work, applicable
measurements were limited to those in a 2D image plane,
and effectiveness was solely demonstrated on synthetically
corrupted images. By contrast, AR-GAN can learn a DoF
effect, which yields a 3D space. In the experiments (Sec-
tion 5), we verify that this effect can be learned from images
taken in real scenarios.

Unsupervised 3D representation learning. As mentioned
in Section 1, the learning of 3D representations from single-
view images has garnered attention owing to its data col-
lection advantage. To address this challenge, several stud-
ies have employed auxiliary information as clues for re-
construction, such as 2D keypoints [56, 25], 2D silhou-
ettes [18, 6, 36, 14], or shape models [24, 63, 12, 50, 51].
By contrast, we attempt to address this challenge with no
additional supervision and no predeﬁned model to reduce
costs from laborious annotation and model preparation.

Recently, some studies [44, 54, 46, 67] have addressed
this; however, their assumptions and objectives differ from
ours. They impose assumptions for the viewpoint distribu-
tion or object shape, whereas we impose an assumption for
the DoF distribution. Owing to this assumption difference,
they can learn 3D meshes [54], depth [46, 67], albedo [67],
texture [54], light [67], and viewpoints [44, 54, 46, 67],
whereas AR-GAN can learn the depth and DoF effect.
Therefore, AR-GAN can be considered a model that can
complement (not replace) previous models. We validate
this statement in Section 5.4 by incorporating AR-GAN into
HoloGAN [44] and RGBD-GAN [46].

Monocular depth estimation. Monocular depth estimation
involves predicting the depth when a single image is given.
A successful approach involves training a depth predictor
using paired or consecutive data, such as image and depth
pairs [9, 38, 33, 32, 70, 10], stereo pairs [11, 13, 68], and
videos [75, 72, 61]. Although this approach is a promising
solution, collecting such data is often impractical or time-
consuming.

In another direction, some studies [53, 16] have proposed
the adoption of focused and all-in-focus image pairs, in-
cluding learning the depth in the process of reconstructing
the focused image from an all-in-focus image. Although
this study is inspired by their success, the main difference
is that they require paired supervision between focused and
all-in-focus images, whereas ours does not need it. How-
ever, owing to this difference, our task is very challenging;
therefore, in this study, we did not attempt to achieve high-

quality depth estimation comparable to supervised meth-
ods. Instead, in the experiments, we compared AR-GAN
with a previous fully unsupervised depth estimation model
(i.e., RGBD-GAN [46]) and demonstrated the utility of AR-
GAN in this challenging setting (Section 5.2).
DoF rendering. The DoF or Bokeh effect is a popular pho-
tography technique, and its synthesis has garnered consid-
erable interest in computer vision and graphics. To achieve
this without prior knowledge of geometry and lightning,
previous studies adopted stereo pairs [1], a stack of images
taken in different camera settings [22, 17], and a segmenta-
tion mask [52, 59] to determine the degree of blur. Although
they have exhibited remarkable results, they are limited ow-
ing to their general dependence on a manually deﬁned DoF
renderer. To address this limitation, end-to-end supervised
learning methods [53, 62, 21, 48], which train a DoF ren-
derer using pairs of shallow and deep DoF images, were
devised. Recently, an unpaired learning method [76] was
also proposed. This method eliminates the requirement for
paired supervision; however, set-level supervision (i.e., su-
pervision of whether each image is a deep or shallow DoF
image) remains necessary. By contrast, we focus on training
a DoF renderer in a fully unsupervised manner. We demon-
strate the effectiveness of our approach in Section 5.5.

3. Preliminaries

3.1. GANs

We brieﬂy introduce two previous works on which our
model is based. The ﬁrst is GAN [15], which learns data
distribution using the following objective:

LGAN = E
+ E

I r∼pr(I)[log C(I r)]
z∼p(z)[log(1 − C(G(z)))],

(1)

where, given a random noise z, a generator G generates an
image I g = G(z) that can deceive a discriminator C by
minimizing this objective, whereas C distinguishes I g from
a real image I r by maximizing this objective. Here, super-
scripts r and g denote the real and generated data, respec-
tively. Using this min-max game, the generative distribution
pg(I) approaches the real distribution pr(I).

3.2. Light ﬁeld aperture rendering

Light ﬁeld aperture rendering [53] is a type of differ-
entiable aperture rendering.2
Its objective is to learn an
aperture renderer R that synthesizes a shallow DoF image
Is(x) = R(Id(x), D(x)), given a deep DoF image Id(x)
and depth D(x).3 Here, x represents the spatial coordinates

2Another representative aperture rendering is compositional aperture
rendering [53], which discretely models disparities using a stack of blur
kernels. In the initial experiments, we determined that light ﬁeld aperture
rendering, which models the light ﬁeld within a camera, is more compati-
ble with our unsupervised learning. This is possibly because the learning
clues are few in our unsupervised learning; therefore, an explicit camera
constraint via light ﬁeld aperture rendering works sufﬁciently.

3In the original study [53], D(x) is estimated from I(x). However,

this estimation is not adopted in AR-GAN; hence, we omitted it here.

3

of the light ﬁeld on the image plane. When Id(x) is directly
warped into the viewpoint in the light ﬁeld based on D(x),
holes can appear in the resulting light ﬁeld. Instead, a train-
able neural network T is adopted to expand D(x) into a
depth map M (x, u) for each view in the light ﬁeld:

M (x, u) = T (D(x)),

(2)

where u denotes the angular coordinates of the light ﬁeld
on the aperture plane. Subsequently, Id(x) is warped into
each view in the light ﬁeld using the depth map M (x, u):4

L(x, u) = Id(x + uM (x, u)),

(3)

where L(x, u) is the simulated camera light ﬁeld. Finally,
it is integrated to render a shallow DoF image Is(x):

Figure 2. Overall pipeline of AR-GAN generator. The AR-GAN
generator ﬁrst generates a deep DoF image I g
d and depth Dg from
a random noise z, and then renders a shallow DoF image I g
s from
I g
d and Dg using the aperture renderer R.

Is(x) =

A(u)L(x, u),

(4)

4.2. Overall pipeline

u
(cid:88)

where A(u) is an indicator that represents the disk-shaped
camera aperture. Hereafter, for simplicity, we omit x and u
when they are not required.

4. Aperture rendering GANs: AR-GANs

4.1. Problem statement

We begin by deﬁning the problem statement. We con-
sider a fully unsupervised setting in which we cannot ob-
tain any supervision or pretrained model except for an im-
age collection. As discussed in Section 2, typical end-to-
end focus-based monocular depth estimation methods (e.g.,
[53, 16]), and DoF rendering methods (e.g., [53, 21, 48, 76])
achieve their objectives using a conditional model (i.e., a
deep DoF image is used as the input, and a depth or shallow
DoF image is estimated based on it). However, in our fully
unsupervised setting, we cannot employ this formulation as
we cannot obtain either ground-truth depth or supervision
of whether each image is a deep or shallow DoF image.

Alternatively, we aim to learn an unconditional generator
G(z) that can generate a tuple of a deep DoF image, depth,
and shallow DoF image, i.e., (I g
s ), from a random
noise z. When the training images are extremely biased
in terms of the DoF (e.g., all images are all-in-focus), it is
difﬁcult to obtain focus cues from the images; hence, we
impose the following assumption on an image distribution:

d , Dg, I g

Assumption 1 The DoF setting is different for each image,
and the dataset includes various DoF images.

Note that we do not have to collect a set/pair of various DoF
images for each training instance. We observed that this as-
sumption is satisﬁed by typical natural image datasets (e.g.,
ﬂower [45], bird [60], and face [29] datasets shown in Fig-
ure 1). Under this assumption, we aim to learn the above-
mentioned generator in a wisdom of crowds approach.

4The depth of the focal plane can be learned explicitly by adding the
parameterized offset ˆm to M in Equation 3. However, we do not do so
under the assumption that it is determined per image Id and internally
represented and optimized in D, which is used in Equation 2. In this case,
the focal plane exists at D = 0, while out-of-focus occurs in |D| > 0.

The overall pipeline of the AR-GAN generator is illus-
trated in Figure 2. When given a random noise z, we ﬁrst
generate a deep DoF image I g

d and depth Dg as follows:

(5)

I g
d = GI (z), Dg = GD(z).
In practice, we share the weights between GI and GD ex-
cept for the last layer because the image and depth exhibit
high correlation. A previous study [39] demonstrated that
this kind of weight sharing is beneﬁcial in learning a joint
distribution between relevant domains. Subsequently, we
render a shallow DoF image I g
d and
Dg using the aperture renderer R described in Section 3.2.
Typical GANs apply a discriminator C to the ﬁnal out-
put of the generator (i.e., I g
s in our case). However, in AR-
GAN, both generators (i.e., GI and GD) and R are train-
able. Hence, without constraints, they could compete for
roles. For example, they can drift into an extreme solution
(e.g., R learns strong out-of-focus, and GI learns an over-
deblurred image). To address this, we develop DoF mixture
learning, which is detailed in the next section.

s from the generated I g

4.3. DoF mixture learning

A possible solution to this problem is regularizing GI
using an explicit distance metric (e.g., L1, L2, or perceptual
loss [23, 8]) such that I g
s . However,
this solution disrupts the depth learning (Section 5.3.1).

d is approximate to I g

Alternatively, we introduce DoF mixture learning. Fig-
ure 3 illustrates the comparison between standard and DoF
mixture learning. In standard GAN training, the generator
attempts to cover the real image distribution using images
without constraints. By contrast, in the DoF mixture learn-
ing, the generator attempts to represent the real image distri-
bution using diverse DoF images whose extent is adjusted
by a scale factor s. More precisely, in our AR-GAN, the
GAN objective (Equation 1) is rewritten as follows:
LAR-GAN = E

I r∼pr(I)[log C(I r)]

+ E

z∼p(z),s∼p(s)[log(1 − C(R(GI (z), sGD(z))))], (6)

where s ∈ [0, 1]; when s = 0, a deep DoF image (almost
equal to I g
d ) is rendered, whereas when s = 1, a shallow

4

zGDGIDg(x)Igd(x)Igs(x)TA(u)Warp&Deep DoFDepthShallow DoFApertureLight field(cid:127)uR(Igd,Dg)Aperture rendererWeight sharingLg(x,u)5. Experiments

5.1. Experimental settings

We conducted four experiments to verify the effective-
ness of AR-GANs from multiple perspectives: a compara-
tive study on unsupervised 3D representation learning (Sec-
tion 5.2), ablation studies on DoF mixture learning and
center focus prior (Section 5.3), portability analysis (Sec-
tion 5.4), and application in shallow DoF rendering (Sec-
tion 5.5). Here, we explain the common settings and present
the details of each in the following sections.
Datasets. We evaluated AR-GANs on three natural im-
age datasets that cover various objects: Oxford Flow-
ers [45] (8189 ﬂower images with 102 categories), CUB-
200-2011 [60] (11788 bird images with 200 categories), and
FFHQ [29] (70000 face images). To efﬁciently examine
various cases, we resized the images to 64 × 64. We also
experimented on 128×128 images in some cases to conﬁrm
the dependency on image resolution (e.g., Figure 1).
Metrics. To evaluate the visual ﬁdelity of the generated im-
ages, we adopted the kernel inception distance (KID) [2],5
which computes the maximum mean discrepancy between
real and generated images within the Inception model [55].
When calculating scores, we generated 20000 images from
each model. Measuring depth and DoF accuracy directly is
non-trivial because we aim to learn an unconditional model
from unpaired and unlabeled natural images, and cannot ob-
tain the ground truth. Alternatively, we evaluated the depth
accuracy by (1) learning the depth estimator using pairs of
images and depths generated by GANs, (2) predicting the
depths of real images using the learned depth estimator, and
(3) comparing the obtained results with the depths predicted
by a state-of-the-art monocular depth estimator [68], which
is trained using stereo pairs in an external dataset.6 We used
scale-invariant depth error (SIDE) [9] to measure the dif-
ference. In both metrics, the performance increased as the
score decreased. In all the experiments, we report the mean
score with the standard deviation over three training runs.
Implementation. We implemented the model based on
HoloGAN [44]. The generator has a StyleGAN-like archi-
tecture [29]. In AR-GANs, 3D convolution used in Holo-
GAN is not required; hence, we replaced it with 2D convo-
lution. The discriminator has instance [57] and spectral [42]
normalizations. The networks were trained using the Adam
optimizer [31] with a non-saturating GAN loss [15].

5.2. Comparative study

First, we conducted a comparative study to clarify the
difference between AR-GAN and previous fully unsuper-
vised 3D representation learning.

5We used KID because it has an unbiased estimator and complements
the ﬂaws of other representative metrics (i.e., Fr´echet inception distance
(FID) [19] and inception score (IS) [19]).

6We used the pretrained model provided by the authors: https:
//github.com/KexianHust/Structure-Guided-Ranking-
Loss.

Figure 3. Comparison of standard and DoF mixture learning.

DoF image (I g
s ) is rendered. Intuitively, the aperture ren-
derer R, which has an optical constraint on the light ﬁeld,
functions as a shallow DoF image prior. Under Assump-
tion 1 (a real image distribution pr(I) includes both deep I r
d
and shallow I r
s DoF images), this prior encourages the gen-
erated deep I g
d and shallow I g
s DoF images to approximate
I r
d and I r
s , respectively. This also facilitates the learning of
Dg, which is a source of the I g
d and I g

s connection.

In practice, we determined that sampling s from a bi-
nomial distribution, i.e., p(s) = B(1, ps), works optimally,
where ps indicates a probability of s = 1. In Section 5.3, we
examine the effect of the ps value. It was manually deter-
mined for simplicity; however, optimizing it in a data-driven
approach is a potential direction for future work.

4.4. Center focus prior

Another challenge unique to unsupervised depth and
DoF effect learning is to the difﬁculty in distinguishing
foreground and background blurs without any constraint or
prior knowledge. Although not all images satisfy this, fo-
cused images tend to be captured when the main targets are
positioned at the center, as shown in Figure 4(a). Based on
this observation, we impose a center focus prior deﬁned by

Dp =

0
−g · (r − rth)

(cid:40)

(r <= rth)
(r > rth),

(7)

where r indicates the distance from the center of the image,
and rth and g denote the hyper-parameters that deﬁne the
focused area and depth gain, respectively. We visualize this
prior in Figure 4(b). As shown in this ﬁgure, the prior facili-
tates the center area focus while promoting the surrounding
area to be behind the focal plane. We apply this prior to the
generated depth Dg as follows:

Lp = λp(cid:107)Dg − Dp(cid:107)2
2,

(8)

where λp represents a weighting parameter. In practice, we
apply this only at the beginning of training to mitigate the
negative effect triggered by the gap between Dr and Dp.

Figure 4. Examples of focused images and center focus prior.
In (b), light color indicates the foreground.

5

RealimagesRealimages(a) Standard learning (baseline)(b) DoF mixture learning (proposed)Functions asa shallow DoF image priorDeep DoFShallow DoF???GGIGDRs(a) Examples of focused images(b) Center focus priorKID×103↓

Oxford Flowers

CUB-200-2011

FFHQ

GAN

11.71 ±0.68

HoloGAN
RGBD-GAN

11.30 ±0.37
12.04 ±0.35

AR-GAN

11.23 ±0.36

15.04 ±0.14

14.68 ±0.51
14.92 ±0.49

14.30 ±0.56

6.97 ±0.30

6.89 ±0.38
6.73 ±0.26

5.75 ±0.19

Table 1. Comparison of KID×103↓ among different GANs.

SIDE×102↓

Oxford Flowers

CUB-200-2011

FFHQ

RGBD-GAN

7.01 ±0.81

AR-GAN

4.46 ±0.03

7.06 ±0.02

3.58 ±0.04

5.81 ±0.40

4.21 ±0.15

Table 2. Comparison of SIDE×102↓ between RGBD-GAN and
AR-GAN. GAN and HoloGAN are not listed here because they
cannot generate depth along with an image.

Figure 6. Examples of predicted depths. † indicates the ablated
model. (c–g) Results obtained in a fully unsupervised setting.

The SIDE comparison is presented in Table 2. We found
that AR-GAN outperforms RGBD-GAN in all datasets. Ex-
amples of the predicted depths are presented in Figure 6.
Although the predicted depths exhibit a lower resolution
than those predicted by the supervised methods (e.g, [68]),7
we found that AR-GAN (c) improves the details (e.g.,
ﬂower details and tree branches) that disappear in [68] (b)
and RGBD-GAN (g), thus beneﬁting from focus cues.

5.3. Ablation study

5.3.1 Ablation study on DoF mixture learning

Metrics. We ﬁrst evaluated the importance of the DoF
mixture learning. As mentioned in Section 5.1, measuring
depth and DoF accuracy directly is non-trivial; therefore,
we further adopted two metrics along with KID and SIDE:
(1) Learned perceptual image patch similarity (LPIPS) [73]
computes the distance between two images in the CNN fea-
ture space and is demonstrated to exhibit a high correlation
with human perceptual similarity [73]. We adopted LPIPS
to measure the perceptual similarity between pairs of I g
d and
I g
s . LPIPS is expected to be moderately small because the
content is preserved before and after the application of aper-
ture rendering. (2) Depth standard deviation (DSD) is the
standard deviation of the generated depths. Our objective is
to learn a meaningful depth that can yield a plausible DoF
effect. When depth learning is successful, DSD is expected
to be sufﬁciently large.

7Note that 64 × 64 is the standard resolution for fully unsupervised
learning methods (e.g., HoloGAN and RGBD-GAN), and applications to
images with complex objects/backgrounds are challenging for them.

Figure 5. Qualitative comparison among HoloGAN, RGBD-
GAN, and AR-GAN. HoloGAN, RGBD-GAN, and AR-GAN
generate images, image and depth pairs, and tuples of deep and
shallow DoF images and depths, respectively.

Comparison models. We compared AR-GAN with Holo-
GAN [44] and RGBD-GAN [46], which are representative
models in this category, as well as standard GAN [15].
As discussed in Section 2, HoloGAN/RGBD-GAN learns
3D representations using viewpoint cues, whereas AR-GAN
achieves this with focus cues. Hence, the applicable datasets
are different, which we veriﬁed by using the three datasets.
Results. Examples of the generated images are presented in
Figure 5. Here, the obtainable 3D representations and appli-
cable datasets differ among the GANs. Although HoloGAN
and RGBD-GAN succeed in learning viewpoint-aware rep-
resentations in FFHQ, they fail to do so in Oxford Flowers
and CUB-200-2011, where viewpoint distributions are bi-
ased and viewpoint cues do not exist sufﬁciently. By con-
trast, AR-GAN succeeds in learning the depth and DoF ef-
fect in all datasets because it can employ focus cues, which
are present across all datasets.

The KID comparison is summarized in Table 1. We
found that AR-GAN achieved comparable performance and
did not incur a negative effect across all datasets.

6

Azimuth(a) HoloGAN (baseline)OxfordFlowersFFHQCUB-200-2011(b) RGBD-GAN (baseline)OxfordFlowersFFHQCUB-200-2011(c) AR-GAN (proposed)OxfordFlowersFFHQCUB-200-2011(     only)OxfordFlowersFFHQCUB-200-2011[68]AR-GANAR-GAN†AR-GAN†AR-GAN†RGBD-GAN(Full)(     only)(w/o      )DpIgdIgs(c)(d)(e)(f)(g)(b)(a)Input(Real)SOTA(Baseline)Oxford Flowers

KID×103↓ SIDE×102↓

LPIPS↓

DSD↑

I g
s only ps = 1
Mixture ps = 0.75
Mixture ps = 0.5
Mixture ps = 0.25
I g
d only ps = 0

12.36 ±0.59
10.97 ±0.26
10.69 ±0.48
11.23 ±0.36
11.58 ±0.37

5.48 ±0.20
4.81 ±0.06
4.65 ±0.05
4.46 ±0.03
4.56 ±0.20

0.229 ±0.027 0.157 ±0.063
0.023 ±0.001 0.657 ±0.006
0.022 ±0.000 0.771 ±0.022
0.028 ±0.001 1.007 ±0.025
0.113 ±0.013 0.446 ±0.065

L1
Double discriminators

11.66 ±0.72
9.74 ±0.31

5.26 ±0.65
6.79 ±2.21

0.033 ±0.001 0.387 ±0.116
0.000 ±0.001 0.032 ±0.046

CUB-200-2011

KID×103↓ SIDE×102↓

LPIPS↓

DSD↑

I g
s only ps = 1
Mixture ps = 0.75
Mixture ps = 0.5
Mixture ps = 0.25
I g
d only ps = 0

13.62 ±0.53
12.68 ±0.61
13.14 ±0.03
14.30 ±0.56
14.58 ±0.56

4.63 ±0.50
3.75 ±0.08
3.55 ±0.02
3.58 ±0.04
5.94 ±0.70

0.125 ±0.037 0.354 ±0.021
0.037 ±0.003 0.748 ±0.072
0.043 ±0.003 0.959 ±0.075
0.059 ±0.002 1.175 ±0.017
0.115 ±0.019 0.193 ±0.012

12.54 ±0.32
Double discriminators 12.50 ±0.12

L1

5.75 ±1.26
4.33 ±0.34

0.042 ±0.001 0.725 ±0.195
0.000 ±0.000 0.001 ±0.000

FFHQ

KID×103↓ SIDE×102↓

LPIPS↓

DSD↑

I g
s only ps = 1
Mixture ps = 0.75
Mixture ps = 0.5
Mixture ps = 0.25
I g
d only ps = 0

5.75 ±0.44
5.67 ±0.23
5.75 ±0.19
6.17 ±0.08
6.85 ±0.13

6.00 ±0.35
4.38 ±0.10
4.21 ±0.15
4.68 ±0.33
4.77 ±0.13

0.097 ±0.011 0.296 ±0.018
0.009 ±0.001 0.757 ±0.177
0.009 ±0.001 0.769 ±0.119
0.010 ±0.001 0.583 ±0.071
0.028 ±0.006 0.202 ±0.003

L1
Double discriminators

5.82 ±0.21
6.20 ±0.08

4.82 ±0.09
5.20 ±0.47

0.015 ±0.004 0.466 ±0.045
0.000 ±0.000 0.000 ±0.000

Figure 7. Comparison of AD with and without Dp.

Oxford Flowers KID×103↓

SIDE×102↓

LPIPS↓

DSD↑

W/ Dp
W/o Dp

11.23 ±0.36
10.69 ±0.24

4.46 ±0.03
6.78 ±1.58

0.028 ±0.001
0.026 ±0.002

1.007 ±0.025
0.915 ±0.137

CUB-200-2011 KID×103↓

SIDE×102↓

LPIPS↓

DSD↑

W/ Dp
W/o Dp

FFHQ

W/ Dp
W/o Dp

14.30 ±0.56
13.96 ±0.63

3.58 ±0.04
4.86 ±1.84

0.059 ±0.002
0.062 ±0.004

1.175 ±0.017
1.183 ±0.059

KID×103↓

SIDE×102↓

LPIPS↓

DSD↑

5.75 ±0.19
5.72 ±0.10

4.21 ±0.15
6.70 ±1.88

0.009 ±0.001
0.009 ±0.001

0.769 ±0.119
0.851 ±0.057

Table 3. Comparison of KID×103↓, SIDE×102↓, LPIPS↓, and
DSD↑ among AR-GANs with different learning methods.

Table 4. Comparison of KID×103↓, SIDE×102↓, LPIPS↓, and
DSD↑ among AR-GANs with and without Dp.

Comparison models. We conducted the analysis from two
perspectives. (1) We evaluated the effect of the ps value,
which indicates the rate of using shallow DoF images in
the DoF mixture learning (Equation 6). (2) We tested two
possible alternatives: L1, which uses L1 loss to guide I g
d
closer to I g
s , and double discriminators, which adopts two
discriminators, for I g
s , respectively. This facilitates
both pg(Id) and pg(Is) to coincide with the overall real dis-
tribution pr(I).
Results. A comparison of the scores is summarized in Ta-
ble 3. Our main ﬁndings are two-fold:

d and I g

(1) Effect of value of ps. We found that some ﬂuctua-
tions exist in the KID; however, in all cases, the scores are
comparable to those of the other GANs presented in Ta-
ble 1. This indicates that AR-GANs can generate plausible
images regardless of ps. By contrast, SIDE, LPIPS, and
DSD are affected by ps. SIDE tends to improve when the
DoF mixture learning is adopted.8 This is because in the
DoF mixture learning, we can encourage I g
s to ap-
proximate I r
s , respectively, as well as facilitate Dg
learning, which is the source that connects them. Exam-
ples of predicted depths (Figure 6 (c–e)) also validate the
effectiveness of DoF mixture learning. Regarding LPIPS
and DSD, as LPIPS increases, and DSD successively de-
creases when ps = 1 or ps = 0. This indicates that the
DoF mixture learning is required to manage LPIPS and
DSD. Among AR-GANs with DoF mixture learning (i.e.,
ps ∈ {0.25, 0.5, 0.75}), there is a trade-off and dataset de-
pendency relative to LPIPS and DSD. Consider the score

d and I g

d and I r

8The sole “I g

d only” case in Oxford Flowers is an exception. In this
case, Dg is not regularized by aperture rendering; however, weight sharing
between GI and GD (Section 4.2) aids the depth learning. This strategy
exhibits dataset dependency and fails in the other datasets.

7

balance, we set ps = 0.25 for Oxford Flowers and CUB-
200-2011 and ps = 0.5 for FFHQ in other experiments.

(2) Comparison with alternatives. Although L1 achieves
reasonable LPIPS, it deteriorates SIDE and DSD more than
those in the DoF mixture learning. This result indicates that
this method disrupts depth learning. Double discriminators
facilitate both pg(Id) and pg(Is) to coincide with pr(I).
Consequently, LPIPS and DSD approach zero, and SIDE
depreciates. This result veriﬁes the importance of mixing
I g
d and I g

s when learning pr(I).

5.3.2 Ablation study on center focus prior

Metrics. We evaluated the necessity of the center focus
prior Dp. To assess the overall tendency of each pixel to
represent the foreground or background blur, we calculated
the average depth (AD), i.e., the pixel-wise average of the
generated depths. To validate the learning consistency, we
compared the results over three training runs.
Results. The results are presented in Figure 7. We found
that we can obtain constant results across training runs when
Dp is adopted. In all results, the center is focused, while the
surroundings are behind the focal plane. By contrast, when
we eliminate Dp, the foreground and background are turned
over, depending on the initialization. These results indicate
that Dp is beneﬁcial in determining the learning direction.
The comparison of the scores is summarized in Table 4.
We found that KID, LPIPS, and DSD are comparable across
all datasets. We deduce that Dp is solely adopted at the be-
ginning of training; therefore, it does not disrupt the entire
training. By contrast, SIDE depreciates when Dp is not im-
plemented. This occurs because the foreground and back-
ground are reversed, as shown in Figure 6(f).

OxfordFlowersFFHQCUB-200-2011Seed 0Seed 1Seed 2Seed 0Seed 1Seed 2(a) WithDp(b) WithoutDpFigure 8. Examples of data generated using AR-HoloGAN and
AR-RGBD-GAN. The viewpoint change in the horizontal direc-
tion is obtained by the HoloGAN/RGBD-GAN function, whereas
the DoF change and depth in the vertical direction are obtained by
the AR-GAN function.

5.4. Portability analysis

As presented in Section 5.2, the obtainable represen-
tations differ between HoloGAN/RGBD-GAN and AR-
GAN. An interesting approach would be learning these rep-
resentations jointly by combining them. A signiﬁcant prop-
erty of AR-GAN is portability, i.e., it is easy to incorporate
into other GANs. Speciﬁcally, we can achieve this simply
by adding aperture rendering on top of HoloGAN/RGBD-
GAN and training it with the DoF mixture training and a
center focus prior. One requirement is that a dataset should
satisfy the assumptions of both models; i.e., a dataset should
include diverse viewpoint images along with various DoF
images. Among the datasets described above, only FFHQ
satisﬁes this requirement. Hence, we solely evaluated AR-
HoloGAN (AR-GAN + HoloGAN) and AR-RGBD-GAN
(AR-GAN + RGBD-GAN) on this dataset.
Results. Examples of generated data are shown in Figure 8.
As shown in this ﬁgure, we can jointly control both view-
points and the DoF effect with the HoloGAN/RGBD-GAN
and AR-GAN functions. As a reference, we also calcu-
lated the KID scores. The scores for AR-HoloGAN and
AR-RGBD-GAN were 5.70 ± 0.32 and 5.43 ± 0.22, re-
spectively. These are better than the scores of the original
AR-GAN, HoloGAN, and RGBD-GAN (Table 1).

5.5. Application in shallow DoF rendering

Finally, we demonstrate the applicability of AR-GAN in
shallow DoF rendering. After training, AR-GAN can syn-
thesize tuples of (I g
s ) from random noise. By uti-
lizing this, we learn a shallow DoF renderer Id → Is using
pairs of (I g
s ). We call this approach AR-GAN-R. As an-
other approach, we learn a depth estimator Id → D using
pairs of (I g
d , Dg). By employing the learned depth estima-

d , Dg, I g

d , I g

Figure 9. Examples of shallow DoF rendering.

tor, we estimate D from Id and then render Is from (Id, D)
using R in AR-GAN. We call this approach AR-GAN-DR.
Dataset. We used Oxford Flowers and AR-GAN-generated
images to train AR-GAN and AR-GAN-R/DR, respectively.
To conﬁrm generality, we conducted a test on a differ-
ent dataset, including ﬂower photos taken by smartphones,
which were used in the CycleGAN study [76].
Comparison model. To the best of our knowledge, no pre-
vious method can learn the DoF effect from natural images
in the same setting as ours (i.e., without additional super-
vision and a predeﬁned model). Therefore, as a baseline,
we used CycleGAN [76], which trains a shallow DoF ren-
derer Id → Is using set-level supervision (i.e., supervision
of whether each image is a deep or shallow DoF image).9
Results. Examples of the rendered images are presented in
Figure 9. We found that CycleGAN often yields unneces-
sary changes (e.g., color change), whereas AR-GAN-R/DR
does not. We infer that the aperture rendering mechanism
in AR-GAN contributes to this phenomenon. In addition,
AR-GAN-DR can estimate the depth simultaneously.

6. Conclusion

We proposed a novel family of GANs, AR-GANs, which
can learn depth and DoF effect from unconstrained natural
images. To achieve this, we incorporated aperture render-
ing into GANs and developed DoF mixture learning and
a center focus prior to address the ambiguities triggered
by the unsupervised setting. Via comparative and abla-
tion studies, we elucidated the differences from previous
GANs and the signiﬁcance of the proposed techniques. We
demonstrated that AR-GANs are compatible and comple-
mentary to previous GANs by combining AR-GANs with
HoloGAN/RGBD-GAN. Finally, we demonstrated the ap-
plicability of AR-GANs in shallow DoF rendering. Despite
their applications in photos, several deep generative models
do not utilize focus cues. In the future, we expect that our
ﬁndings will facilitate further studies on such models.

9We used the pretrained model provided by the authors: https://

github.com/junyanz/pytorch-CycleGAN-and-pix2pix.

8

DeepDoFShallowDoFDepth(a) AR-HoloGAN (AR-GAN + HoloGAN)DeepDoFShallowDoFDepth(b) AR-RGBD-GAN (AR-GAN + RGBD-GAN)AzimuthDeep DoF(d) CycleGAN(a) Input (baseline)(c) AR-GAN-DR (proposed)(b) AR-GAN-R (proposed)Shallow DoFShallow DoFShallow DoFDepthReferences

[1] Jonathan T. Barron, Andrew Adams, YiChang Shih, and Car-
los Hern´andez. Fast bilateral-space stereo for synthetic de-
focus. In CVPR, 2015. 3

[2] Mikołaj Bi´nkowski, Dougal J Sutherland, Michael Arbel,
and Arthur Gretton. Demystifying MMD GANs. In ICLR,
2018. 5, 11

[3] Volker Blanz and Thomas Vetter. A morphable model for the

synthesis of 3D faces. In SIGGRAPH, 1999. 1

[4] Ashish Bora, Eric Price, and Alexandros G. Dimakis. Am-
bientGAN: Generative models from lossy measurements. In
ICLR, 2018. 3

[5] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large
scale GAN training for high ﬁdelity natural image synthesis.
In ICLR, 2019. 2

[6] Wenzheng Chen, Huan Ling, Jun Gao, Edward Smith,
Jaakko Lehtinen, Alec Jacobson, and Sanja Fidler. Learning
to predict 3D objects with an interpolation-based differen-
tiable renderer. In NeurIPS, 2019. 1, 3

[7] Terrance DeVries and Graham W. Taylor. Improved regular-
ization of convolutional neural networks with cutout. arXiv
preprint arXiv:1708.04552, 2017. 24

[8] Alexey Dosovitskiy and Thomas Brox. Generating images
with perceptual similarity metrics based on deep networks.
In NIPS, 2016. 4, 14

[9] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map
prediction from a single image using a multi-scale deep net-
work. In NIPS, 2014. 3, 5, 11

[10] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Bat-
manghelich, and Dacheng Tao. Deep ordinal regression net-
work for monocular depth estimation. In CVPR, 2018. 3
[11] Ravi Garg, Vijay Kumar B G, Gustavo Carneiro, and Ian
Reid. Unsupervised CNN for single view depth estimation:
Geometry to the rescue. In ECCV, 2016. 3

[12] Baris Gecer, Stylianos Ploumpis, Irene Kotsia, and Stefanos
Zafeiriou. GANFIT: Generative adversarial network ﬁtting
for high ﬁdelity 3D face reconstruction. In CVPR, 2019. 1,
3

[13] Cl´ement Godard, Oisin Mac Aodha, and Gabriel J. Bros-
tow. Unsupervised monocular depth estimation with left-
right consistency. In CVPR, 2017. 3

[14] Shubham Goel, Angjoo Kanazawa, and Jitendra Malik.
Shape and viewpoint without keypoints. In ECCV, 2020. 1,
3

[15] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.
2, 3, 5, 6, 24

[16] Shir Gur and Lior Wolf. Single image depth estimation
In CVPR, 2019. 3,

trained via depth from defocus cues.
4

[17] Samuel W. Hasinoff and Kiriakos N. Kutulakos. A layer-
based restoration framework for variable-aperture photogra-
phy. In ICCV, 2007. 3

[18] Philipp Henzler, Niloy Mitra, and Tobias Ritschel. Escap-
ing Plato’s cave using adversarial training: 3D shape from
unstructured 2D image collections. In ICCV, 2019. 1, 2, 3

[19] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, G¨unter Klambauer, and Sepp Hochreiter.
GANs trained by a two time-scale update rule converge to a
Nash equilibrium. In NIPS, 2017. 5

[20] Xun Huang and Serge Belongie. Arbitrary style transfer in
In ICCV,

real-time with adaptive instance normalization.
2017. 23

[21] Andrey Ignatov, Jagruti Patel, and Radu Timofte. Rendering
In CVPR

natural camera bokeh effect with deep learning.
Workshop, 2020. 3, 4

[22] David E. Jacobs, Jongmin Baek, and Marc Levoy. Focal
stack compositing for depth of ﬁeld control. Stanford Com-
puter Graphics Laboratory Technical Report, 1(1):2012,
2012. 3

[23] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual
losses for real-time style transfer and super-resolution.
In
ECCV, 2016. 4, 14

[24] Angjoo Kanazawa, Michael J. Black, David W. Jacobs, and
Jitendra Malik. End-to-end recovery of human shape and
pose. In CVPR, 2018. 1, 3

[25] Angjoo Kanazawa, Shubham Tulsiani, Alexei A. Efros, and
Jitendra Malik. Learning category-speciﬁc mesh reconstruc-
tion from image collections. In ECCV, 2018. 1, 3

[26] Takuhiro Kaneko and Tatsuya Harada. Noise robust genera-

tive adversarial networks. In CVPR, 2020. 3

[27] Takuhiro Kaneko and Tatsuya Harada. Blur, noise, and com-
pression robust generative adversarial networks. In CVPR,
2021. 3

[28] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.
Progressive growing of GANs for improved quality, stability,
and variation. In ICLR, 2017. 24

[29] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks. In
CVPR, 2019. 2, 4, 5, 23

[30] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,
Jaakko Lehtinen, and Timo Aila. Analyzing and improving
the image quality of StyleGAN. In CVPR, 2020. 2

[31] Diederik P. Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. In ICLR, 2015. 5, 23, 24, 25
[32] Yevhen Kuznietsov, Jorg Stuckler, and Bastian Leibe. Semi-
supervised deep learning for monocular depth map predic-
tion. In CVPR, 2017. 3

[33] Iro Laina, Christian Rupprecht, Vasileios Belagiannis, Fed-
erico Tombari, and Nassir Navab. Deeper depth prediction
with fully convolutional residual networks. In 3DV, 2016. 3
[34] Christian Ledig, Lucas Theis, Ferenc Husz´ar, Jose Caballero,
Andrew Cunningham, Alejandro Acosta, Andrew Aitken,
Alykhan Tejani, Johannes Totz, Zehan Wang, and Wenzhe
Shi. Photo-realistic single image super-resolution using a
generative adversarial network. In CVPR, 2017. 14

[35] Steven Cheng-Xian Li, Bo Jiang, and Benjamin Marlin. Mis-
GAN: Learning from incomplete data with generative adver-
sarial networks. In ICLR, 2019. 3

[36] Xueting Li, Sifei Liu, Kihwan Kim, Shalini De Mello, Varun
Jampani, Ming-Hsuan Yang, and Jan Kautz. Self-supervised
single-view 3D reconstruction via semantic consistency. In
ECCV, 2020. 1, 3

[37] Yiyi Liao, Katja Schwarz, Lars Mescheder, and Andreas
Geiger. Towards unsupervised learning of generative models
for 3D controllable image synthesis. In CVPR, 2020. 2
[38] Fayao Liu, Chunhua Shen, Guosheng Lin, and Ian Reid.
Learning depth from single monocular images using deep
convolutional neural ﬁelds. IEEE Trans. Pattern Anal. Mach.
Intell., 38(10):2024–2039, 2015. 3

[39] Ming-Yu Liu and Oncel Tuzel. Coupled generative adversar-

ial networks. In NIPS, 2016. 4

9

[40] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard
Pons-Moll, and Michael J. Black. SMPL: A skinned multi-
person linear model. ACM Trans. Graph., 34(6):1–16, 2015.
1

[41] Andrew L. Maas, Awni Y. Hannun, and Andrew Y. Ng. Rec-
tiﬁer nonlinearities improve neural network acoustic models.
In ICML Workshop, 2013. 23

[42] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and
Yuichi Yoshida. Spectral normalization for generative ad-
versarial networks. In ICLR, 2018. 5, 23

[43] Vinod Nair and Geoffrey E. Hinton. Rectiﬁed linear units
improve restricted Boltzmann machines. In ICML, 2010. 23
[44] Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian
Richardt, and Yong-Liang Yang. HoloGAN: Unsupervised
learning of 3D representations from natural images. In ICCV,
2019. 2, 3, 5, 6, 23

[45] Maria-Elena Nilsback and Andrew Zisserman. Automated
ﬂower classiﬁcation over a large number of classes.
In
ICVGIP, 2008. 2, 4, 5

[46] Atsuhiro Noguchi and Tatsuya Harada. RGBD-GAN: Un-
supervised 3D representation learning from natural image
datasets via RGBD image synthesis. In ICLR, 2020. 2, 3,
6

[47] Arthur Pajot, Emmanuel de Bezenac, and Patrick Gallinari.
In ICLR,

Unsupervised adversarial image reconstruction.
2018. 3

[48] Ming Qian, Congyu Qiao,

Jiamin Lin, Zhenyu Guo,
Chenghua Li, Cong Leng, and Jian Cheng. BGGAN: Bokeh-
glass generative adversarial network for rendering realistic
bokeh. arXiv preprint arXiv:2011.02242, 2020. 3, 4
[49] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:
Convolutional networks for biomedical image segmentation.
In MICCAI, 2015. 24

[50] Soubhik Sanyal, Timo Bolkart, Haiwen Feng, and Michael J.
Black. Learning to regress 3D face shape and expression
from an image without 3D supervision. In CVPR, 2019. 1, 3
[51] Jiaxiang Shang, Tianwei Shen, Shiwei Li, Lei Zhou, Ming-
min Zhen, Tian Fang, and Long Quan. Self-supervised
monocular 3D face reconstruction by occlusion-aware multi-
view geometry consistency. In ECCV, 2020. 1, 3

[52] Xiaoyong Shen, Xin Tao, Hongyun Gao, Chao Zhou, and
Jiaya Jia. Deep automatic portrait matting. In ECCV, 2016.
3

[53] Pratul P. Srinivasan, Rahul Garg, Neal Wadhwa, Ren Ng,
and Jonathan T. Barron. Aperture supervision for monocular
depth estimation. In CVPR, 2018. 2, 3, 4, 23, 24

[54] Attila Szab´o, Givi Meishvili, and Paolo Favaro. Unsuper-
vised generative 3D shape learning from natural images.
arXiv preprint arXiv:1910.00287, 2019. 2, 3

[55] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon
Shlens, and Zbigniew Wojna. Rethinking the Inception ar-
chitecture for computer vision. In CVPR, 2016. 5

[56] Luan Tran and Xiaoming Liu. Nonlinear 3D face morphable

model. In CVPR, 2018. 1, 3

[57] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. In-
stance normalization: The missing ingredient for fast styliza-
tion. arXiv preprint arXiv:1607.08022, 2016. 5, 23

[58] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba.

Generating videos with scene dynamics. In NIPS, 2016. 3

[59] Neal Wadhwa, Rahul Garg, David E. Jacobs, Bryan E.
Feldman, Nori Kanazawa, Robert Carroll, Yair Movshovitz-
Attias, Jonathan T. Barron, Yael Pritch, and Marc Levoy.

Synthetic depth-of-ﬁeld with a single-camera mobile phone.
ACM Trans. Graph., 37(4):1–13, 2018. 3

[60] Catherine Wah, Steve Branson, Peter Welinder, Pietro Per-
ona, and Serge Belongie. The Caltech-UCSD Birds-200-
2011 Dataset. Technical Report CNS-TR-2011-001, Cali-
fornia Institute of Technology, 2011. 2, 4, 5

[61] Chaoyang Wang, Jos´e Miguel Buenaposada, Rui Zhu, and
Simon Lucey. Learning depth from monocular videos using
direct methods. In CVPR, 2018. 3

[62] Lijun Wang, Xiaohui Shen, Jianming Zhang, Oliver Wang,
Zhe Lin, Chih-Yao Hsieh, Sarah Kong, and Huchuan Lu.
DeepLens: Shallow depth of ﬁeld from a single image. ACM
Trans. Graph., 37(6):1–11, 2018. 3

[63] Mengjiao Wang, Zhixin Shu, Shiyang Cheng, Yannis Pana-
gakis, Dimitris Samaras, and Stefanos Zafeiriou. An ad-
versarial neuro-tensorial approach for learning disentangled
Int. J. Comput. Vis., 127(6-7):743–762,
representations.
2019. 1, 3

[64] Xiaolong Wang and Abhinav Gupta. Generative image mod-
eling using style and structure adversarial networks.
In
ECCV, 2016. 2

[65] Zhou Wang, Alan C. Bovik, Hamid R. Sheikh, and Eero P.
Image quality assessment: From error visibil-
IEEE Trans. Image Process.,

Simoncelli.
ity to structural similarity.
13(4):600–612, 2004. 11

[66] Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and
Josh Tenenbaum. Learning a probabilistic latent space of ob-
ject shapes via 3D generative-adversarial modeling. In NIPS,
2016. 2

[67] Shangzhe Wu, Christian Rupprecht, and Andrea Vedaldi.
Unsupervised learning of probably symmetric deformable
In CVPR, 2020. 2,
3D objects from images in the wild.
3

[68] Ke Xian, Jianming Zhang, Oliver Wang, Long Mai, Zhe Lin,
and Zhiguo Cao. Structure-guided ranking loss for single
image depth prediction. In CVPR, 2020. 3, 5, 6, 20

[69] Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical
evaluation of rectiﬁed activations in convolutional network.
In ICML Workshop, 2015. 23

[70] Dan Xu, Elisa Ricci, Wanli Ouyang, Xiaogang Wang, and
Nicu Sebe. Multi-scale continuous CRFs as sequential deep
networks for monocular depth estimation. In CVPR, 2017. 3
[71] Jianwei Yang, Anitha Kannan, Dhruv Batra, and Devi
Parikh. LR-GAN: Layered recursive generative adversarial
networks for image generation. In ICLR, 2017. 3

[72] Zhichao Yin and Jianping Shi. GeoNet: Unsupervised learn-
ing of dense depth, optical ﬂow and camera pose. In CVPR,
2018. 3

[73] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shecht-
man, and Oliver Wang. The unreasonable effectiveness of
deep features as a perceptual metric. In CVPR, 2018. 6, 11,
12

[74] Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song
Han. Differentiable augmentation for data-efﬁcient GAN
training. In NeurIPS, 2020. 24

[75] Tinghui Zhou, Matthew Brown, Noah Snavely, and David G.
Lowe. Unsupervised learning of depth and ego-motion from
video. In CVPR, 2017. 3

[76] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A.
Efros. Unpaired image-to-image translation using cycle-
consistent adversarial networks. In ICCV, 2017. 3, 4, 8

10

Appendix

In this appendix, we provide further analyses (Ap-
pendix A), extended results (Appendix B), and implemen-
tation details (Appendix C) regarding our study.

A. Further analyses

In this section, we provide three additional analyses for
a deeper understanding of metrics and the proposed model.

• Appendix A.1: Further analysis of metrics.

• Appendix A.2: Further analysis of DoF mixture learn-
ing. This is a detailed analysis of the experiments pre-
sented in Section 5.3.1.

• Appendix A.3: Further analysis of the center focus
prior. This is a detailed analysis of the experiments
presented in Section 5.3.2.

A.1. Further analysis of metrics

As discussed in Section 5.1, because of our objective and
formulation, that is, learning an unconditional model from
unpaired and unlabeled natural images, it is nontrivial to
prepare the ground truth or construct a metric that can mea-
sure depth and DoF accuracy directly. As alternatives, in
the ablation study (Section 5.3), we adopted two metrics:
learned perceptual image patch similarity (LPIPS) [73]
and depth standard deviation (DSD), along with kernel in-
ception distance (KID) [2] and scale-invariant depth error
(SIDE) [9]. For further clariﬁcation, in this subsection, we
ﬁrst evaluate the performance of three additional metrics
along with the aforementioned metrics (Section A.1.1), and
then discuss their relationships (Section A.1.2).

A.1.1 Evaluation on additional metrics

Additional metrics. We adopted three additional metrics.

(1) Structural similarity index measure (SSIM) [65].
SSIM is a representative traditional (or non-DNN-based)
metric for predicting perceived quality. This metric mea-
sures the distance between two images; a larger value in-
dicates higher similarity. SSIM is a possible alternative to
LPIPS, which was used in Section 5.3. The reason for using
LPIPS is that a previous study [73] demonstrated that LPIPS
has a higher correlation with human perceptual judgments
than SSIM. We consider this characteristic to be useful for
measuring the validity of the DoF effect where, between
deep and shallow DoF images, the appearance of focused
targets is nearly similar, but that of unfocused regions is
relatively different. To validate this consideration, we will
compare LPIPS and SSIM later.

(2) KIDI g

s . We calculated the KID score between all real
images (i.e., I r ∼ pr(I)) and generated shallow DoF im-
ages (i.e., I g
s = R(GI (z), GD(z)), where z ∼ p(z)). When
d ∼ pr(Id)) and shallow (i.e.,
I r includes both deep (i.e., I r

s ∼ pr(Is)) DoF images, it is expected that KIDI g
I r
s is mod-
erately small but larger than KID, which measures the dis-
tance between all real (i.e., I r ∼ pr(I)) images and all
generated images (i.e., I g ∼ pg(I)).10

d

d

(3) KIDI g

s , and KIDI g

. We computed the KID score between all real
images (i.e., I r ∼ pr(I)) and generated deep DoF images
(i.e., I g
d = GI (z), where z ∼ p(z)). This metric has a
s .
similar characteristic as KIDI g
Results. We applied the aforementioned metrics to the
models presented in Table 3. Table 5 summarizes the re-
sults. Our main ﬁndings are three-fold:
(1) Comparison of KID, KIDI g

among AR-
GANs with different ps (Nos. 1–5). We found that as ps
(i.e., the probability that a shallow DoF image is rendered)
decreases. In par-
decreases, KIDI g
, the KID for
ticular, when comparing KIDI g
the dominant image (i.e., one used at a higher frequency in
training) obtains a smaller value than the KID for the non-
dominant image. The KID for the non-dominant image is
also moderately small when the image is used in training
(i.e., ps ∈ {0.75, 0.5, 0.25} (Nos. 2–4)). By contrast, when
either I g
d is used for training (i.e., ps ∈ {1, 0} (Nos. 1
and 5)), the KID for the unused one is relatively high. These
results imply that DoF mixture learning is useful for match-
ing both pg(Id) and pg(Is) with parts of the real distribu-
tion, which are expected to be pr(Id) and pr(Is), respec-
tively.

s increases and KIDI g

s and KIDI g

s or I g

d

d

d

s , and KIDI g

(2) Comparison of KID, KIDI g

between AR-
GANs with different learning methods (Nos. 6 and 7). When
an explicit distance metric (L1 in this experiment (No. 6)) is
used, it is encouraged to compare generated deep DoF im-
ages (i.e., I g
d ) with generated shallow DoF images (i.e., I g
s ).
However, an adversarial loss is imposed only on I g
s ; con-
sequently, its KIDI g
is signiﬁcantly larger than that of AR-
GANs with DoF mixture learning (Nos. 2–4). This suggests
that, in this case, the visual ﬁdelity of I g
d may not be high,
even when the LPIPS is small.

d

d

s and KIDI g

When the double discriminators (No. 7) are used, both
approach KID. This result indicates that
KIDI g
both pg(Is) and pg(Id) are encouraged to coincide with the
overall real distribution pr(I), which consists of pr(Is) and
pr(Id). This phenomenon can also be explained by the fact
that LPIPS and DSD are almost 0, and SSIM is close to 1.
From these results, we conclude that DoF mixture learn-
ing is more reasonable than L1 and double discriminators
when a dataset includes both deep and shallow DoF images
(Assumption 1), and we aim to selectively represent the real
image distribution according to the DoF strength.

(3) Comparison of LPIPS and SSIM. In most cases, as
LPIPS decreases, SSIM increases. We would like to note
that the similarity is higher when the LPIPS value is small,

10When calculating KID for AR-GANs, we used the DoF mixture set-
d and I g
s

ting that was used in training. Therefore, in Table 3, the rates of I g
that were used to calculate the KID vary according to the model.

11

(a) Oxford Flowers

No.

Learning method

KID×103↓

KIDI

g
s

×103↓

I g
s only ps = 1
1
2 Mixture ps = 0.75
3 Mixture ps = 0.5
4 Mixture ps = 0.25
I g
d only ps = 0
5
L1

6
7 Double discriminators

12.36 ±0.59
10.97 ±0.26
10.69 ±0.48
11.23 ±0.36
11.58 ±0.37

11.66 ±0.72
9.74 ±0.31

12.46 ±0.58
13.74 ±0.47
18.98 ±1.24
30.61 ±2.99
36.63 ±5.11

11.81 ±0.74
9.79 ±0.38

KIDI

g
d

×103↓

293.56 ±44.92
26.50 ±1.59
17.58 ±0.53
12.91 ±0.38
11.51 ±0.41

47.59 ±2.54
9.88 ±0.30

SIDE×102↓

LPIPS↓

SSIM↑

DSD↑

5.48 ±0.20
4.81 ±0.06
4.65 ±0.05
4.46 ±0.03
4.56 ±0.20

5.26 ±0.65
6.79 ±2.21

0.229 ±0.027
0.023 ±0.001
0.022 ±0.000
0.028 ±0.001
0.113 ±0.013

0.033 ±0.001
0.000 ±0.001

0.406 ±0.049
0.943 ±0.002
0.940 ±0.002
0.919 ±0.002
0.870 ±0.020

0.932 ±0.002
0.999 ±0.001

0.157 ±0.063
0.657 ±0.006
0.771 ±0.022
1.007 ±0.025
0.446 ±0.065

0.387 ±0.116
0.032 ±0.046

(b) CUB-200-2011

No.

Learning method

KID×103↓

KIDI

g
s

×103↓

KIDI

g
d

×103↓

SIDE×102↓

LPIPS↓

SSIM↑

DSD↑

I g
s only ps = 1
1
2 Mixture ps = 0.75
3 Mixture ps = 0.5
4 Mixture ps = 0.25
I g
d only ps = 0
5
L1

6
7 Double discriminators

13.62 ±0.53
12.68 ±0.61
13.14 ±0.03
14.30 ±0.56
14.58 ±0.56

12.54 ±0.32
12.50 ±0.12

13.71 ±0.58
12.30 ±0.45
13.20 ±0.43
18.28 ±0.73
28.63 ±2.14

12.59 ±0.45
12.56 ±0.20

72.63 ±25.71
22.64 ±0.37
18.15 ±0.07
16.19 ±0.63
14.85 ±0.72

33.70 ±0.84
12.61 ±0.29

4.63 ±0.50
3.75 ±0.08
3.55 ±0.02
3.58 ±0.04
5.94 ±0.70

5.75 ±1.26
4.33 ±0.34

0.125 ±0.037
0.037 ±0.003
0.043 ±0.003
0.059 ±0.002
0.115 ±0.019

0.042 ±0.001
0.000 ±0.000

0.786 ±0.029
0.921 ±0.006
0.909 ±0.007
0.877 ±0.003
0.908 ±0.010

0.910 ±0.001
1.000 ±0.000

0.354 ±0.021
0.748 ±0.072
0.959 ±0.075
1.175 ±0.017
0.193 ±0.012

0.725 ±0.195
0.001 ±0.000

No.

Learning method

KID×103↓

KIDI

g
s

×103↓

KIDI

g
d

(c) FFHQ
×103↓

SIDE×102↓

LPIPS↓

SSIM↑

DSD↑

I g
s only ps = 1
1
2 Mixture ps = 0.75
3 Mixture ps = 0.5
4 Mixture ps = 0.25
I g
d only ps = 0
5
L1

6
7 Double discriminators

5.75 ±0.44
5.67 ±0.23
5.75 ±0.19
6.17 ±0.08
6.85 ±0.13

5.82 ±0.21
6.20 ±0.08

5.82 ±0.48
6.05 ±0.14
7.53 ±0.43
11.55 ±0.70
10.16 ±0.29

5.83 ±0.23
6.26 ±0.06

74.36 ±7.41
14.50 ±0.65
9.34 ±0.22
7.53 ±0.41
6.94 ±0.10

24.89 ±2.70
6.19 ±0.07

6.00 ±0.35
4.38 ±0.10
4.21 ±0.15
4.68 ±0.33
4.77 ±0.13

4.82 ±0.09
5.20 ±0.47

0.097 ±0.011
0.009 ±0.001
0.009 ±0.001
0.010 ±0.001
0.028 ±0.006

0.015 ±0.004
0.000 ±0.000

0.808 ±0.034
0.967 ±0.006
0.966 ±0.006
0.968 ±0.004
0.976 ±0.003

0.967 ±0.005
1.000 ±0.000

0.296 ±0.018
0.757 ±0.177
0.769 ±0.119
0.583 ±0.071
0.202 ±0.003

0.466 ±0.045
0.000 ±0.000

Table 5. Comparison of KID×103↓, KIDIg
different learning methods. This is an extended version of Table 3.

×103↓, KIDIg

d

s

×103↓, SIDE×102↓, LPIPS↓, SSIM↑, and DSD↑ among AR-GANs with

as well as when the SSIM value is large. However, in some
cases (for example, between Nos. 4 and 5 in CUB-200-
2011), this tendency does not hold. We discuss this in Ap-
pendix A.1.2.

described, LPIPS and DSD reﬂect the success and failure
of the learning of the depth and DoF effect reasonably well.
Therefore, we believe that they are reasonable metrics for
this task.

A.1.2 Relationships among metrics

In the results in Table 5, there are two nontrivial relation-
ships among metrics: (1) relationship between LPIPS and
DSD, and (2) relationship between LPIPS and SSIM. Next,
we discuss the two relationships with examples.

(1) Relationship between LPIPS and DSD. Figure 10
shows the relationship between LPIPS and DSD. We se-
lected three representative models based on the results in
Table 5(a): (a) AR-GAN with double discriminators (No. 7),
for which the LPIPS and DSD are small, (b) AR-GAN with
DoF mixture learning at ps = 0.25 (No. 4), for which the
LPIPS is small and the DSD is large, and (c) AR-GAN with
I g
s only (No. 1), for which the LPIPS is large and the DSD
is small.

As shown in this ﬁgure, when the DSD is small (a, c), the
model fails to learn the depth (i.e., the depth is almost con-
stant). By contrast, when the DSD is large (b), the model
succeeds in learning the meaningful depth. Furthermore,
when the LPIPS is small (a, b), the content is preserved be-
tween the deep and shallow DoF images. By contrast, when
the LPIPS is large (c), the deep DoF image is corrupted. As

(2) Relationship between LPIPS and SSIM. Figure 11
shows the relationship between LPIPS and SSIM. We se-
lected four representative models based on the results in Ta-
ble 5(b): (a) AR-GAN with double discriminators (No. 7),
in which deep and shallow DoF images are similar in terms
of both LPIPS and SSIM, (b) AR-GAN with DoF mixture
learning at ps = 0.25 (No. 4), in which deep and shallow
DoF images are similar in terms of LPIPS but dissimilar in
terms of SSIM, (c) AR-GAN with I g
d only (No. 5), in which
deep and shallow DoF images are similar in terms of SSIM
but dissimilar in terms of LPIPS, and (d) AR-GAN with I g
s
only (No. 1), in which deep and shallow DoF images are
dissimilar in terms of both LPIPS and SSIM.

As shown in this ﬁgure, LPIPS is sensitive to overall blur
(c), in which both the main object and its surroundings are
blurred.11 However, it is insensitive to center focus blur (b),
in which the main object in focus is not blurred but the sur-
rounding area is blurred. By contrast, SSIM is sensitive to
the center focus blur (b) but insensitive to the overall blur
(c). This is because LPIPS measures the distance based on

11This tendency has also been observed in the original study on

LPIPS [73].

12

Figure 10. Relationship between LPIPS and DSD. In each block
(a–c), the three images in the rows represent the generated deep
DoF image (i.e., I g
d ), shallow DoF image (i.e., I g
s ), and depth (i.e.,
Dg), from left to right. The three images in the columns were gen-
erated from different noise z. For ease of viewing, the depth was
normalized using the same values across all results. The models
were trained using Oxford Flowers.

Figure 11. Relationship between LPIPS and SSIM. In each
block (a–d), the three images in the rows represent the generated
deep DoF image (i.e., I g
s ), and depth
(i.e., Dg), from left to right. The three images in the columns were
generated from different noise z. For ease of viewing, the depth
was normalized using the same values across all results. The mod-
els were trained using CUB-200-2011.

d ), shallow DoF image (i.e., I g

the classiﬁer that is sensitive to the difference in the main
target but insensitive to the differences in the irrelevant sur-
roundings. Our objective is to learn the natural DoF effect,
in which the main target and the surroundings tend to be
unchanged and changed, respectively, before and after de-
focusing. To achieve this objective, we believe that LPIPS
is more appropriate than SSIM as a metric that measures
the unexpected dissimilarity caused by factors other than
the DoF effect.

A.2. Further analysis of DoF mixture learning

A.2.1 Effect of sampling methods

As described in Section 4.3, we sampled s (Equation 6)
from a binomial distribution, which is p(s) = B(1, ps).
As demonstrated in the experiments in Section 5, this set-
ting works reasonably well. However, as a further analysis,
in this subsection, we evaluate the performance when s is
sampled from a different distribution, particularly a uniform
distribution, which is p(s) = U (0, 1).

Results. Table 6 summarizes the comparison of KID,
, SIDE, LPIPS, and DSD among AR-GANs
KIDI g
with different sampling methods. When a binomial distri-

s , KIDI g

d

bution is used (i.e., s ∈ {0, 1}), the relationship between
the deep and shallow DoF images is discretely represented.
By contrast, when a uniform distribution is used (i.e., s ∈
[0, 1]), the relationship is continuously represented. In the
latter case, deep and shallow DoF images are represented
continuously, and their total distribution is encouraged to
represent the overall real distribution. Therefore, I g
d (s = 0)
represents the deepest DoF image, and I g
s (s = 1) repre-
sents the most shallow DoF image among the various DoF
images. Consequently, the difference between I g
d and I g
s
becomes larger compared with the use of a binomial distri-
bution. Therefore, in Table 6, the LPIPS for U (0, 1) (No. 6)
is larger than those for B(1, ps) at ps ∈ {0.25, 0.5, 0.75}
(Nos. 2–4) and the DSD for U (0, 1) (No. 6) is comparable
with those for B(1, ps), which achieved the largest DSD
among Nos. 2–4. Because of this characteristic difference,
there are advantages and disadvantages for both a uniform
distribution and a binomial distribution. The choice of sam-
pling methods is therefore task-speciﬁc.

A.2.2 Comparison with another alternative learning

In Section 5.3.1, we tested L1 and double discriminators as
possible alternatives to DoF mixture learning. As a further

13

SmallLargeSmallLargeDSDLPIPS(a) Double discriminators(c)      onlyIgsLPIPS: 0.236, DSD: 0.060LPIPS: 0.000, DSD: 0.001LPIPS: 0.000, DSD: 0.000LPIPS: 0.000, DSD: 0.001LPIPS: 0.187, DSD: 0.076LPIPS: 0.166, DSD: 0.065LPIPS: 0.032, DSD: 1.241LPIPS: 0.048, DSD: 0.836LPIPS: 0.009, DSD: 1.041(b) DoF mixture (               )ps=0.25SimilarDissimilarSSIMLPIPS(c)      onlyIgd(b) DoF mixture (               )ps=0.25LPIPS: 0.039, SSIM: 0.870LPIPS: 0.042, SSIM: 0.848LPIPS: 0.022, SSIM: 0.887LPIPS: 0.102, SSIM: 0.936LPIPS: 0.101, SSIM: 0.912LPIPS: 0.244, SSIM: 0.935LPIPS: 0.000, SSIM: 1.000LPIPS: 0.000, SSIM: 1.000LPIPS: 0.000, SSIM: 1.000(a) Double discriminatorsLPIPS: 0.352, SSIM: 0.784LPIPS: 0.280, SSIM: 0.707LPIPS: 0.212, SSIM: 0.657(d)      onlyIgsDissimilarSimilarNo.

Sampling method

I g
s only B(1, 1)
1
2 Mixture B(1, 0.75)
3 Mixture B(1, 0.5)
4 Mixture B(1, 0.25)
I g
d only B(1, 0)
5
6 Mixture U (0, 1)

No.

Sampling method

I g
s only B(1, 1)
1
2 Mixture B(1, 0.75)
3 Mixture B(1, 0.5)
4 Mixture B(1, 0.25)
I g
d only B(1, 0)
5
6 Mixture U (0, 1)

KID×103↓

12.36 ±0.59
10.97 ±0.26
10.69 ±0.48
11.23 ±0.36
11.58 ±0.37

10.67 ±0.16

KID×103↓

13.62 ±0.53
12.68 ±0.61
13.14 ±0.03
14.30 ±0.56
14.58 ±0.56

12.43 ±0.92

No.

Sampling method

KID×103↓

I g
s only B(1, 1)
1
2 Mixture B(1, 0.75)
3 Mixture B(1, 0.5)
4 Mixture B(1, 0.25)
I g
d only B(1, 0)
5
6 Mixture U (0, 1)

5.75 ±0.44
5.67 ±0.23
5.75 ±0.19
6.17 ±0.08
6.85 ±0.13

5.86 ±0.06

KIDI

g
s

×103↓

12.46 ±0.58
13.74 ±0.47
18.98 ±1.24
30.61 ±2.99
36.63 ±5.11

21.14 ±2.49

KIDI

g
s

×103↓

13.71 ±0.58
12.30 ±0.45
13.20 ±0.43
18.28 ±0.73
28.63 ±2.14

13.38 ±0.70

KIDI

g
s

×103↓

5.82 ±0.48
6.05 ±0.14
7.53 ±0.43
11.55 ±0.70
10.16 ±0.29

7.30 ±0.02

(a) Oxford Flowers

KIDI

g
d

×103↓

293.56 ±44.92
26.50 ±1.59
17.58 ±0.53
12.91 ±0.38
11.51 ±0.41

29.72 ±0.80

SIDE×102↓

LPIPS↓

DSD↑

5.48 ±0.20
4.81 ±0.06
4.65 ±0.05
4.46 ±0.03
4.56 ±0.20

4.31 ±0.11

0.229 ±0.027
0.023 ±0.001
0.022 ±0.000
0.028 ±0.001
0.113 ±0.013

0.157 ±0.063
0.657 ±0.006
0.771 ±0.022
1.007 ±0.025
0.446 ±0.065

0.040 ±0.002

0.857 ±0.118

(b) CUB-200-2011
×103↓

KIDI

g
d

72.63 ±25.71
22.64 ±0.37
18.15 ±0.07
16.19 ±0.63
14.85 ±0.72

25.60 ±1.89

(c) FFHQ

KIDI

g
d

×103↓

74.36 ±7.41
14.50 ±0.65
9.34 ±0.22
7.53 ±0.41
6.94 ±0.10

17.86 ±0.25

SIDE×102↓

LPIPS↓

DSD↑

4.63 ±0.50
3.75 ±0.08
3.55 ±0.02
3.58 ±0.04
5.94 ±0.70

3.62 ±0.04

0.125 ±0.037
0.037 ±0.003
0.043 ±0.003
0.059 ±0.002
0.115 ±0.019

0.354 ±0.021
0.748 ±0.072
0.959 ±0.075
1.175 ±0.017
0.193 ±0.012

0.072 ±0.006

1.142 ±0.038

SIDE×102↓

LPIPS↓

DSD↑

6.00 ±0.35
4.38 ±0.10
4.21 ±0.15
4.68 ±0.33
4.77 ±0.13

4.49 ±0.09

0.097 ±0.011
0.009 ±0.001
0.009 ±0.001
0.010 ±0.001
0.028 ±0.006

0.296 ±0.018
0.757 ±0.177
0.769 ±0.119
0.583 ±0.071
0.202 ±0.003

0.020 ±0.001

1.130 ±0.085

Table 6. Comparison of KID×103↓, KIDIg
sampling methods.

s

×103↓, KIDIg

d

×103↓, SIDE×102↓, LPIPS↓, and DSD↑ among AR-GANs with different

(a) Oxford Flowers

No. Distance metric

KID×103↓

1
2

L1
Perceptual loss

11.66 ±0.72
11.18 ±0.52

KIDI

g
s

×103↓

11.81 ±0.74
11.33 ±0.58

KIDI

g
d

×103↓

47.59 ±2.54
40.35 ±0.56

No. Distance metrics

KID×103↓

1
2

L1
Perceptual loss

12.54 ±0.32
12.78 ±0.21

KIDI

g
s

×103↓

12.59 ±0.45
12.82 ±0.23

KIDI

g
d

×103↓

33.70 ±0.84
36.16 ±2.34

(b) CUB-200-2011

No. Distance metric

KID×103↓

1
2

L1
Perceptual loss

5.82 ±0.21
5.64 ±0.40

KIDI

g
s

×103↓

5.83 ±0.23
5.66 ±0.42

(c) FFHQ
×103↓

KIDI

g
d

24.89 ±2.70
39.37 ±1.46

SIDE×102↓

5.26 ±0.65
4.70 ±0.06

SIDE×102↓

5.75 ±1.26
3.75 ±0.07

SIDE×102↓

4.82 ±0.09
5.31 ±0.07

LPIPS↓

DSD↑

0.033 ±0.001
0.028 ±0.001

0.387 ±0.116
0.589 ±0.041

LPIPS↓

DSD↑

0.042 ±0.001
0.054 ±0.003

0.725 ±0.195
0.754 ±0.053

LPIPS↓

DSD↑

0.015 ±0.004
0.024 ±0.002

0.466 ±0.045
0.439 ±0.010

Table 7. Comparison of KID×103↓, KIDIg
ent distance metrics. Refer to Table 5 for a comparison with the results of other learning methods.

×103↓, KIDIg

d

s

×103↓, SIDE×102↓, LPIPS↓, and DSD↑ between AR-GANs using differ-

alternative, we test perceptual loss [23, 8], which is a vari-
ant of explicit distance metrics and measures the distance
between two images in the CNN feature space. Recent stud-
ies [8, 34] have shown that perceptual loss is more useful
than pixel-level metrics (e.g., L1 and L2) when aiming to
improve visual ﬁdelity.

d

. It is noteworthy that DoF
ﬁnd it difﬁcult to improve KIDI g
mixture learning can address these difﬁculties, as listed in
Table 5 and discussed in Appendix A.1.1. These results in-
dicate that the aforementioned difﬁculties are common in
explicit distance metric-based methods, including L1 and
perceptual loss.

d

s , KIDI g

Results. Table 7 summarizes the comparison of KID,
KIDI g
, SIDE, LPIPS, and DSD between AR-GANs
using different distance metrics. We found that even when
using perceptual loss (No. 2), explicit distance metric-based
methods (Nos. 1 and 2) ﬁnd it difﬁcult to improve both the
LPIPS and DSD simultaneously. Furthermore, they also

A.3. Further analysis of center focus prior

A.3.1 Effect of duration of using center focus prior

In Section 5.3.2, we used the center focus prior only at the
beginning of training to mitigate the negative effect caused
by the gap between the real depth and the predeﬁned center

14

No.

Usage of prior

1 W/ Dp at beginning
2 W/ Dp until end
3 W/o Dp

No.

Usage of prior

1 W/ Dp at beginning
2 W/ Dp until end
3 W/o Dp

KID×103↓

11.23 ±0.36
10.98 ±0.27
10.69 ±0.24

KID×103↓

14.30 ±0.56
13.96 ±0.76
13.96 ±0.63

No.

Usage of prior

KID×103↓

1 W/ Dp at beginning
2 W/ Dp until end
3 W/o Dp

5.75 ±0.19
5.67 ±0.17
5.72 ±0.10

KIDI

g
s

×103↓

30.61 ±2.99
29.33 ±0.52
27.79 ±2.50

KIDI

g
s

×103↓

18.28 ±0.73
16.71 ±1.06
17.40 ±1.01

KIDI

g
s

×103↓

7.53 ±0.43
7.98 ±0.36
7.78 ±0.51

(a) Oxford Flowers

KIDI

g
d

×103↓

12.91 ±0.38
12.70 ±0.50
12.60 ±0.23

(b) CUB-200-2011

KIDI

g
d

×103↓

16.19 ±0.63
15.11 ±0.95
16.01 ±0.66

(c) FFHQ

KIDI

g
d

×103↓

9.34 ±0.22
9.06 ±0.22
9.88 ±0.32

SIDE×102↓

LPIPS↓

DSD↑

4.46 ±0.03
4.54 ±0.01
6.78 ±1.58

0.028 ±0.001
0.019 ±0.001
0.026 ±0.002

1.007 ±0.025
0.259 ±0.001
0.915 ±0.137

SIDE×102↓

LPIPS↓

DSD↑

3.58 ±0.04
5.29 ±0.02
4.86 ±1.84

0.059 ±0.002
0.034 ±0.000
0.062 ±0.004

1.175 ±0.017
0.250 ±0.001
1.183 ±0.059

SIDE×102↓

LPIPS↓

DSD↑

4.21 ±0.15
4.31 ±0.04
6.70 ±1.88

0.009 ±0.001
0.008 ±0.001
0.009 ±0.001

0.769 ±0.119
0.271 ±0.004
0.851 ±0.057

×103↓, SIDE×102↓, LPIPS↓, and DSD↑ among AR-GANs with and
Table 8. Comparison of KID×103↓, KIDIg
without center focus prior. The duration of the center focus prior usage changes between W/ Dp at the beginning (No. 1) and W/ Dp
until the end (No. 2). This is an extended version of Table 4.

×103↓, KIDIg

d

s

Figure 12. Comparison of AD with and without Dp. This is an extended version of Figure 7. Light color indicates the foreground.

as the center focus prior shown in Figure 4(b). These re-
sults also support the aforementioned statement that when
Dp is used until the end of the training, the depth learning
is disturbed.

focus prior. To validate this strategy, we evaluated the per-
formance of the model that used the center focus prior until
the end of training.

d

s , KIDI g

Results. Table 8 summarizes the comparison of KID,
KIDI g
, SIDE, LPIPS, and DSD among AR-GANs
Speciﬁcally, we
with and without center focus prior.
changed the duration of the center focus prior usage be-
tween w/ Dp at beginning (No. 1) and w/ Dp until end
(No. 2). As discussed in Section 5.3.2, when Dp is used
only at the beginning of the training (No. 1), the score dif-
ferences between the models with and without Dp (Nos. 1
and 3) are relatively small. However, when Dp is used un-
til the end of the training (No. 2), the DSD is signiﬁcantly
smaller than the results without Dp (No. 3). These results
indicate that when Dp is used until the end of the training,
the depth learning is disturbed, but when its usage is stopped
in the early phase of training, learning is not prevented.

As further evidence, we show a comparison of the av-
erage depth (AD) in Figure 12. As shown here, when Dp
is used only at the beginning of the training (a), AD is de-
formed depending on the dataset. However, when Dp is
used until the end of the training (b), AD is almost the same

15

(a) W/ center focus prior(b) W/ center focus priorOxfordFlowersFFHQCUB-200-2011Seed 0Seed 1Seed 2Seed 0Seed 1Seed 2(c) W/o center focus priorSeed 0Seed 1Seed 2at beginninguntil endB. Extended results

In this appendix, we provide extended versions of the
ﬁgures presented in the main text. Figure titles and the re-
lationships with the ﬁgures presented in the main text are as
follows:

• Figure 13: Unsupervised learning of depth and DoF
effect from unlabeled natural images. This is an ex-
tended version of Figure 1.

• Figure 14: Linear interpolation in the latent space of
the AR-GAN generator. This is an extended version of
Figure 1.

• Figure 15: Qualitative comparison among HoloGAN,
RGBD-GAN, and AR-GAN. This is an extended ver-
sion of Figure 5.

• Figure 16: Examples of predicted depths. This is an

extended version of Figure 6.

• Figure 17: Examples of data generated using AR-
HoloGAN and AR-RGBD-GAN. This is an extended
version of Figure 8.

• Figure 18: Examples of shallow DoF rendering. This

is an extended version of Figure 9.

16

Figure 13. Unsupervised learning of depth and DoF effect from unlabeled natural images. This is an extended version of Figure 1.
Once trained, our model can synthesize tuples of deep and shallow DoF images and depths from random noise. Here, we show the results
generated using AR-GANs trained with 128 × 128 images. The three images in each image block represent the generated deep DoF image
(i.e., I g
s ), and depth (i.e., Dg), from left to right. The images in the columns are generated from different
noise (i.e., z).

d ), shallow DoF image (i.e., I g

17

Deep DoFShallow DoFDepth(a) Oxford FlowersGenerated dataDeep DoFShallow DoFDepthGenerated data(b) CUB-200-2011Deep DoFShallow DoFDepthGenerated data(c) FFHQFigure 14. Linear interpolation in the latent space of the AR-GAN generator. This is an extended version of Figure 1. In the horizontal
direction, we linearly interpolated the noise z ∈ [z1, z2] in the latent space of the AR-GAN generator. The three images in the columns
represent the deep DoF image (i.e., I g
s ), and depth (i.e., Dg) that were generated from the corresponding
d , Dg) continuously according to z.
noise z, from top to bottom. As shown here, AR-GANs can manipulate the contents of tuples of (I g

d ), shallow DoF image (i.e., I g

s , I g

18

(a) Oxford Flowers(b) CUB-200-2011Deep DoFShallow DoFDepth(c) FFHQDeep DoFShallow DoFDepthDeep DoFShallow DoFDepthz1z2z1z2z1z2Figure 15. Qualitative comparison among HoloGAN, RGBD-GAN, and AR-GAN. This is an extended version of Figure 5. HoloGAN
generates images only, RGBD-GAN generates image and depth pairs, and AR-GAN generates tuples of deep and shallow DoF images
and depths. The respective sets are shown in the same column in each image block. When viewpoint distributions are biased (i.e., Oxford
Flowers and CUB-200-2011), HoloGAN and RGBD-GAN have difﬁculty in learning 3D representations (e.g., in (a), the bird direction does
not change smoothly in HoloGAN on CUB-200-2011), whereas AR-GAN succeeds in learning the depth and DoF effect in all datasets.

19

(a) HoloGAN (baseline)(b) RGBD-GAN (baseline)OxfordFlowersFFHQCUB-200-2011OxfordFlowersFFHQCUB-200-2011OxfordFlowersCUB-200-2011FFHQ(c) AR-GAN (proposed)ElevationDeepDoFShallowDoFDepthDepthImageDepthImageDepthImageDepthImageDepthImageDepthImageImageImageImageImageImageImageAzimuthElevationAzimuthFigure 16. Examples of predicted depths. This is an extended version of Figure 6. (b) Results obtained using a state-of-the-art (SOTA)
monocular depth estimator [68], which was trained using stereo pairs in an external dataset. (c–g) Results obtained in a fully unsupervised
setting. Particularly in (c), AR-GAN with DoF mixture learning and center focus prior was used. In (d–f), † indicates the ablated model,
and either DoF mixture learning or the center focus prior was ablated. In (g), RGBD-GAN was used.

20

(     only)[68]AR-GANAR-GAN†AR-GAN†AR-GAN†RGBD-GAN(Full)(     only)(w/o      )DpIgdIgs(c)(d)(e)(f)(g)(b)(a)Input(Real)SOTA(Baseline)(a) Oxford Flowers(     only)[68]AR-GANAR-GAN†AR-GAN†AR-GAN†RGBD-GAN(Full)(     only)(w/o      )DpIgdIgs(c)(d)(e)(f)(g)(b)(a)Input(Real)SOTA(Baseline)(b) CUB-200-2011(     only)[68]AR-GANAR-GAN†AR-GAN†AR-GAN†RGBD-GAN(Full)(     only)(w/o      )DpIgdIgs(c)(d)(e)(f)(g)(b)(a)Input(Real)SOTA(Baseline)(c) FFHQFigure 17. Examples of data generated using AR-HoloGAN and AR-RGBD-GAN. This is an extended version of Figure 8. The
viewpoint change in the horizontal direction is obtained by the HoloGAN/RGBD-GAN function, whereas the DoF change and depth in
the vertical direction (in the top three rows in each image block) are obtained by the AR-GAN function. With regard to AR-RGBD-GAN,
we also visualized the depth predicted by the RGBD-GAN function in the bottom row. Two types of depths (i.e., the depth predicted by
AR-GAN and the depth predicted by RGBD-GAN) do not completely match because they are calculated based on different principles.
The depth predicted by AR-GAN is based on focus cues, whereas the depth predicted by RGBD-GAN is based on the consistency between
viewpoints.

21

AzimuthDeepDoFShallowDoFDepth(a) AR-HoloGAN (AR-GAN + HoloGAN)(b) AR-RGBD-GAN (AR-GAN + RGBD-GAN)ElevationDeepDoFShallowDoFDepthDeepDoFShallowDoFDepth byRGBD-GANDepth byAR-GANDeepDoFShallowDoFDepth byRGBD-GANDepth byAR-GANAzimuthElevationFigure 18. Examples of shallow DoF rendering. This is an extended version of Figure 9. CycleGAN often yields unnecessary changes
(e.g., color change in the ﬁrst row or overblurring of the main object (i.e., the ﬂower) in the eighth row), whereas AR-GAN-R and AR-
GAN-DR do not. The generated shallow DoF images were almost the same for AR-GAN-R and AR-GAN-DR. The main difference
between them is that AR-GAN-DR can estimate the depth simultaneously, whereas AR-GAN-R cannot.

22

Deep DoF(d) CycleGAN(a) Input (baseline)(c) AR-GAN-DR (proposed)(b) AR-GAN-R (proposed)Shallow DoFShallow DoFShallow DoFDepthC. Implementation details

In this appendix, we provide implementation details for

the following items:

• Appendix C.1: Implementation details of 64 × 64 im-
age generation, which were used in the experiments in
Sections 5.2–5.4.

• Appendix C.2: Implementation details of 128 × 128
image generation, which were used to generate sam-
ples in Figure 1.

• Appendix C.3: Implementation details of shallow DoF
rendering, which were used in the experiments in Sec-
tion 5.5.

Notation. In our description of the network architectures,
we use the following notation.

• Linear: Linear layer
• Conv: Convolutional layer
• Deconv: Deconvolutional (i.e., fractionally strided

convolutional) layer

• ReLU: Rectiﬁed linear unit [43]
• LReLU: Leaky rectiﬁed linear unit [41, 69]
• IN: Instance normalization [57]
• AdaIN: Adaptive instance normalization [20]
• SN: Spectral normalization [42]

In our description of the training settings, we use the
following notation. We used the Adam optimizer [31] for
training.

• α: Learning rate
• β1: First-order momentum parameter
• β2: Second-order momentum parameter

C.1. Details of image generation (Sections 5.2–5.4)

Network architectures. Table 9 provides the AR-GAN ar-
chitecture for 64 × 64 images. This was used in the exper-
iments in Sections 5.2–5.4. As mentioned in Section 5, we
designed the generator (Table 9(a)) and discriminator (Ta-
ble 9(c)) based on HoloGAN [44],12 which was used as a
baseline in the experiments. In HoloGAN, 3D transforma-
tion and 3D convolution were used in the generator. How-
ever, they are not required in AR-GANs. Therefore, in AR-
GAN, we removed the 3D transformation and replaced the
3D convolution with a 2D convolution.

The generator (Table 9(a)) has a StyleGAN-like ar-
chitecture [29], in which the latent vector z is inserted
into the network using adaptive instance normalization
(AdaIN(z)) [20]. In the last layer of the generator, we ad-
justed the scale of D in an instance-dependent manner using
a constant (i.e., 10) and a one-hidden-layer multilayer per-
ceptron (MLP(z)). Table 9(b) provides the architecture of

12https://github.com/thunguyenphuoc/HoloGAN

(a) Generators GI and GD
Input 1: Constant c ∈ R4×4×1024,
Input 2: Random noise z ∈ R128 ∼ N (0, 1)

c, AdaIN(z), ReLU

4 × 4 Deconv up 512, AdaIN(z), ReLU

4 × 4 Deconv up 256, AdaIN(z), ReLU

4 × 4 Deconv up 128, AdaIN(z), ReLU

4 × 4 Deconv up 64, AdaIN(z), ReLU
4 × 4 Conv 3, Tanh → I g
d
4 × 4 Conv 1, Tanh ×10 × MLP(z) → Dg

(b) Depth expansion network T

Input: Warped depth map D(x + uD(x)) ∈ R64×64×25

3 × 3 Conv 25, IN, LReLU

3 × 3 Conv 25, IN, LReLU

3 × 3 Conv 25, IN, LReLU, Add input → M (x, u)

(c) Discriminator C

Input: Image I ∈ R64×64×3

5 × 5 Conv down 64, LReLU

5 × 5 SN Conv down 128, IN, LReLU

5 × 5 SN Conv down 256, IN, LReLU

5 × 5 SN Conv down 512, IN, LReLU

Linear

Table 9. AR-GAN architecture for 64 × 64 images.

the depth expansion network T , which was used in the aper-
ture renderer (Section 3.2). In the aperture renderer, we set
the aperture size to 5 × 5. This was implemented based on a
previous study on aperture rendering [53]. In the discrimi-
nator (Table 9(c)), we used instance normalization (IN) [57]
and spectral normalization (SN) [42], following the imple-
mentation of HoloGAN.12

We implemented HoloGAN (used in Section 5.2) using
the same network architecture that was used in the original
study [44].12 In the generator, the ﬁrst and second 2D de-
convolutional layers in Table 9(a) were replaced with 3D
deconvolutional layers. After this processing, 3D transfor-
mation was performed, and then 1×1 Conv and ReLU were
applied to adjust the number of channels. Strictly, we mod-
iﬁed the number of channels following the ofﬁcial code.12
In addition, we removed a depth estimation layer (which is
presented at the bottom of Table 9(a)) because HoloGAN
cannot optimize it.
In HoloGAN, we used the same dis-
criminator as the one used in AR-GAN (Table 9(c)).

We implemented the generator and discriminator in
RGBD-GAN (used in Section 5.2) using almost the same
network architecture as that of AR-GAN (Table 9). An
exception is that we conducted a modiﬁcation to the
generator (Table 9(a)) to incorporate the viewpoint in-
formation (i.e., azimuth θa and elevation θe) into the
network. More concretely, we concatenated ccyclic =

23

[cos(θa), sin(θa), cos(θe), sin(θe)] to z and used the com-
bination of z and ccyclic as the input of AdaIN in the ﬁrst
and second layers. In the initial experiments, we found that
replacing the constant input c with ccyclic (by converting
ccyclic using a CNN to match the size) is useful for learn-
ing the disentangled representations. Therefore, we applied
this modiﬁcation in the experiments. In addition, we mod-
iﬁed the depth estimation layer at the bottom of Table 9(a)
as “4 × 4 Conv 1, Tanh, Shift 1” because we found that this
setting works reasonably well. In RGBD-GAN, we used the
same discriminator as that used in AR-GAN (Table 9(c)).

When implementing AR-HoloGAN/AR-RGBD-GAN
(used in Section 5.4), we added a depth estimation layer
(shown at the bottom of Table 9(a)) to the last layer of the
HoloGAN/RGBD-GAN generator. Through this modiﬁca-
tion, we generated I g
d and Dg simultaneously, and then ren-
dered the shallow DoF image (i.e., I g
d , Dg) us-
ing the aperture renderer, similar to AR-GAN. In addition,
in AR-HoloGAN and AR-RGBD-GAN, we used the same
discriminator as that used in AR-GAN (Table 9(c)).

s ) from (I g

Training settings. As a GAN objective function, we used
a non-saturating GAN loss [15]. We set the hyperparame-
ters of the center focus prior (Equation 7) as rth = 0.25 and
g = 1, where we scale r (the distance from the center of the
image) such that r = 1 represents half of the image width or
height. We set the weighting parameter λp (Equation 8) to
1. We used the center focus prior only during the initial 5k,
5k, and 50k iterations on Oxford Flowers, CUB-200-2011,
and FFHQ, respectively. Following a previous study [53],
we regularized the depth expansion network T so that the
predicted depth map M (x, u) was close to the warped ver-
sions of D(x):

Ld = λd(cid:107)M (x, u) − D(x + uD(x))(cid:107)1,

(9)

where λd is the weighting parameter and set to 1 in the ex-
periments.

We trained the networks from scratch using the Adam
optimizer [31] with α = 0.0001, β1 = 0.5, β2 = 0.999,
and a batch size of 32. Following the implementation of
HoloGAN, we updated the generator twice for every up-
date of the discriminator. We updated the discriminator for
150k, 250k, and 350k iterations on Oxford Flowers, CUB-
200-2011, and FFHQ, respectively. To stabilize the train-
ing, we applied differentiable augmentation [74], including
color jittering, translation, and cutout [7]. To obtain a stable
performance, we used an exponential moving average [28]
with a decay of 0.999 over the weights to produce the ﬁnal
generator.

When training HoloGAN, RGBD-GAN, AR-HoloGAN,
and AR-RGBD-GAN, we sampled the azimuth and eleva-
tion uniformly within the range of [−50, 50] and [−20, 20],
respectively, because such range values were used as de-
fault values in HoloGAN and RGBD-GAN (particularly, on
a face image dataset).

(a) Generators GI and GD
Input 1: Constant c ∈ R4×4×1024,
Input 2: Random noise z ∈ R128 ∼ N (0, 1)

c, AdaIN(z), ReLU

4 × 4 Deconv up 1024, AdaIN(z), ReLU

4 × 4 Deconv up 512, AdaIN(z), ReLU

4 × 4 Deconv up 256, AdaIN(z), ReLU

4 × 4 Deconv up 128, AdaIN(z), ReLU

4 × 4 Deconv up 64, AdaIN(z), ReLU
4 × 4 Conv 3, Tanh → I g
d
4 × 4 Conv 1, Tanh ×10 × MLP(z) → Dg

(b) Depth expansion network T

Input: Warped depth map D(x + uD(x)) ∈ R128×128×25

3 × 3 Conv 25, IN, LReLU

3 × 3 Conv 25, IN, LReLU

3 × 3 Conv 25, IN, LReLU, Add input → M (x, u)

(c) Discriminator C

Input: Image I ∈ R128×128×3

5 × 5 Conv down 64, LReLU

5 × 5 SN Conv down 128, IN, LReLU

5 × 5 SN Conv down 256, IN, LReLU

5 × 5 SN Conv down 512, IN, LReLU

Linear

Table 10. AR-GAN architecture for 128 × 128 images.

C.2. Details of image generation (Figure 1)

Network architectures. Table 10 provides the AR-GAN
architecture for 128 × 128 images. This was used for gen-
erating samples in Figure 1. This network architecture is
similar to that for 64 × 64 images (Table 9) except that one
layer is added to the generator according to the change in
image size.
Training settings. We used similar training settings as
those for 64 × 64 images (Section C.1) except that the hy-
perparameters of the center focus prior were modiﬁed as
rth = 0.25 and g = 2 according to the change in image size.
We used the center focus prior only during the initial 5k, 5k,
and 5k iterations on Oxford Flowers, CUB-200-2011, and
FFHQ, respectively.

C.3. Details of shallow DoF rendering (Section 5.5)

Network architectures. Table 11 provides the AR-GAN-
DR depth estimator architecture. We used the u-net archi-
tecture [49]. The AR-GAN-R image translator has a similar
architecture to this, except that the number of channels of
the last layer is modiﬁed to three to generate an RGB image.
These models were used in the experiments in Section 5.5.
Training settings. We generated training data (i.e., pairs
of images and depths) using the AR-GAN that was trained

24

ENC CONV0
ENC CONV1
POOL1

ENC CONV2
POOL2

ENC CONV3
POOL3

ENC CONV4
POOL4

ENC CONV5
POOL5

ENC CONV6

UPSAMPLE5
CONCAT5
DEC CONV5A
DEC CONV5B

UPSAMPLE4
CONCAT4
DEC CONV4A
DEC CONV4B

UPSAMPLE3
CONCAT3
DEC CONV3A
DEC CONV3B

UPSAMPLE2
CONCAT2
DEC CONV2A
DEC CONV2B

UPSAMPLE1
CONCAT1
DEC CONV1A
DEC CONV1B
DEC CONV1C

Input: Id ∈ R128×128×3

3 × 3 Conv 48, LReLU
3 × 3 Conv 48, LReLU
2 × 2 Maxpool

3 × 3 Conv 48, LReLU
2 × 2 Maxpool

3 × 3 Conv 48, LReLU
2 × 2 Maxpool

3 × 3 Conv 48, LReLU
2 × 2 Maxpool

3 × 3 Conv 48, LReLU
2 × 2 Maxpool

3 × 3 Conv 48, LReLU

2 × 2 Upsample
Concatenate output of POOL4
3 × 3 Conv 96, LReLU
3 × 3 Conv 96, LReLU

2 × 2 Upsample
Concatenate output of POOL3
3 × 3 Conv 96, LReLU
3 × 3 Conv 96, LReLU

2 × 2 Upsample
Concatenate output of POOL2
3 × 3 Conv 96, LReLU
3 × 3 Conv 96, LReLU

2 × 2 Upsample
Concatenate output of POOL1
3 × 3 Conv 96, LReLU
3 × 3 Conv 96, LReLU

2 × 2 Upsample
Concatenate output of INPUT
3 × 3 Conv 64, LReLU
3 × 3 Conv 32, LReLU
3 × 3 Conv 1 → D

Table 11. AR-GAN-DR depth estimator architecture.

using 128 × 128 images on Oxford Flowers. This AR-GAN
was the same as that used for generating the samples in Fig-
ure 1. We trained the depth estimator for 300k iterations
using the Adam optimizer [31] with α = 0.0003, β1 = 0.9,
β2 = 0.99, and a batch size of 4. The learning rate was kept
constant during the training, except for the last 30% itera-
tions, where the learning rate was smoothly ramped down
to zero.

25

