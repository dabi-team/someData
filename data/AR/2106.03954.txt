1
2
0
2

n
u
J

1
1

]

G
L
.
s
c
[

2
v
4
5
9
3
0
.
6
0
1
2
:
v
i
X
r
a

Evaluating Meta-Feature Selection for the Algorithm
Recommendation Problem

Gean T. Pereira, Moisés R. dos Santos, André C. P. L. F. Carvalho
Computer Science Department, Institute of Mathematics and Computer Sciences (ICMC)
University of São Paulo, São Carlos, Brazil
{geantrinpereira,mmrsantos,edesio}@usp.br
andre@icmc.usp.br

Abstract

With the popularity of Machine Learning (ML) solutions, algorithms and data have
been released faster than the capacity of processing them. In this context, the prob-
lem of Algorithm Recommendation (AR) is receiving a signiﬁcant deal of attention
recently. This problem has been addressed in the literature as a learning task, often
as a Meta-Learning problem where the aim is to recommend the best alternative
for a speciﬁc dataset. For such, datasets encoded by meta-features are explored by
ML algorithms that try to learn the mapping between meta-representations and the
best technique to be used. One of the challenges for the successful use of ML is
to deﬁne which features are the most valuable for a speciﬁc dataset since several
meta-features can be used, which increases the meta-feature dimension. This paper
presents an empirical analysis of Feature Selection and Feature Extraction in the
meta-level for the AR problem. The present study was focused on three criteria:
predictive performance, dimensionality reduction, and pipeline runtime. As we
veriﬁed, applying Dimensionality Reduction (DR) methods did not improve predic-
tive performances in general. However, DR solutions reduced about 80% of the
meta-features, obtaining pretty much the same performance as the original setup
but with lower runtimes. The only exception was PCA, which presented about the
same runtime as the original meta-features. Experimental results also showed that
various datasets have many non-informative meta-features and that it is possible to
obtain high predictive performance using around 20% of the original meta-features.
Therefore, due to their natural trend for high dimensionality, DR methods should
be used for Meta-Feature Selection and Meta-Feature Extraction.

1

Introduction

The machine learning popularity in automated systems is increasing daily [1]. The results in the rise
of machine learning applications in different areas of interest that further expanded to meta-learning
problems [2]. Meta-learning is a sub-area of machine learning that investigates the problem of
learning to learn [3]. The aim is to develop a model that can learn new skills and rapidly adapt to
a new environment, by inspecting the relationship among problems and performance of learning
algorithms [4]. This association is further explored for algorithm selection, which leads to the
algorithm recommendation problem, in which the most suitable algorithm is recommended according
to the dataset [5]. Specialists usually evaluate a group of algorithms for suitability to identify the most
effective recommendation algorithm for a given case. These algorithms are trained using historical
data, and their efﬁciency is assessed using cross-validation methods. The best algorithm is selected
based on having the lowest predictive error or highest accuracy.

There has been a large amount of research on constructing meta-level problems and models [6–8].
Nevertheless, few attention has been given to the usefulness and discriminating power of dataset

 
 
 
 
 
 
characteristics to identify features that are most critical for obtaining good results [5]. To found the
most representative feature in a dataset, Feature Selection (FS) is often employed. In this paper,
we evaluate FS methods for the Algorithm Recommendation Problem. FS is a crucial step in data
mining. It is a Dimensionality Reduction (DR) technique that decreases the number of features to a
convenient size for processing and analysis. FS is different from other DR techniques; it does not
change the original feature set; instead, it selects a subgroup of relevant features by excluding all
other features whose existence in the dataset does not positively affect the learning model [9].

To construct a model, the only source of information used for learning is the set of features. Thus, it is
crucial to ﬁnd the optimal subset of features that represent the original dataset. However, selecting that
optimal subset is not a trivial task; if a large number of features are selected, time and space complexity
increases, causing a high workload on the classiﬁer and a decrease in performance [10]. On the
other hand, if too few features are selected, then there is a possibility of losing useful information,
which again leads to a decrease in model performance. Thus, an optimal subset of relevant features
is needed, which gives the best solution without decreasing the model’s performance [11]. But
there is no single feature selection method that is best all the time. Specialists typically have to
experiment with different methods using a trial and error approach, but the problem is that this is
time-consuming and costly, especially when we have large datasets. This work aims to present an
empirical analysis of feature selection and feature extraction in the meta-level setup for datasets
related to the algorithm recommendation problem. The present study performs three comparison
criteria: predictive performance, dimensionality reduction, and pipeline runtime.

The remaining of this work is organized as follows: Sections 2 and 3 shows the background related to
Meta-Learning for Algorithm Recommendation and Feature Selection methods; Section 4 shows the
experimental setup and methodology; The results and proper discussion are presented in Section 5;
Our conclusion and future work directions are discussed in Section 6.

2 Meta-learning for Algorithm Recommendation

The work [12] was purposed the algorithm selection problem, as know as the algorithm recommenda-
tion problem. The problem can be enunciated as the mapping of a set of data instances to a set of
candidate algorithms for optimizing a speciﬁc performance metric. It seems intuitive that similar data
sets might have a good solution with the same algorithm. However, it is necessary to know if exists
an algorithm that has the best performance for all problem domains. This question was explored
by [13] in the "no free lunch" theorems. For simplicity, one of the theorems demonstrates that, given
a set of algorithms applied to all possible domains, the average performance of the algorithms is
the same. This theorem for supervised machine learning implies that not exists a unique learning
algorithm with performance signiﬁcant superior for all domains.

The best solution is to test all of the algorithm candidates for a speciﬁc task and choose to achieve
the best performance. However, exists many limitations to adopt this approach. One of the main
limitations is the high computational cost that can turn impossible to have a solution on available
time [2]. A possible solution is to use meta-learning for the algorithm recommendation problem [2].
A contemporary deﬁnition for meta-learning or "learn to learning" is the set of methods that consists
of using the knowledge extracted from the tasks, algorithms, or model performance evaluation of past
datasets to perform better, faster, and more efﬁcient tasks for new datasets [4].

Although many approaches exist for meta-learning model selection, in this work, we focus on meta-
feature-based meta-learning [3], where, given a set of datasets and algorithm candidates, the main idea
is to model the algorithm recommendation problem as a supervised machine learning problem [14].
The predictive attributes or meta-features are the set of characteristics extracted from a dataset, and
the target is the algorithm candidate that achieved better performance for this dataset. Thus, the aim
is to build a model able to recommend the best algorithm for new tasks.

Meta-learning has been successfully applied to a large number of learning tasks and domains [15]
[16] [17] [7]. However, with the rising number of purpose meta-features in the literature [18] [19], it
is necessary to study feature selection methods to improve the meta-learning setup efﬁciency.

2

3 Feature Selection Methods

It is common sense that features used for training models have a signiﬁcant inﬂuence on the perfor-
mance. With that said, irrelevant features can harm performance. Thus, feature selection methods
are employed to found the most relevant features in a training dataset, which contribute more to the
target variable. Using feature selection can lead to less redundant data, which lower the probability of
making a decision based on noisy feature, so could reduce overﬁtting and also improve the model’s
accuracy. Dimensions of the training set also decrease because feature selection chooses a relevant
subset of features within a whole dataset; thus, it also reduces the time complexity and algorithms
train faster. There are two types of feature selection methods, model-based feature selection, and
ﬁlter-based ﬁlter selection. This work focuses on ﬁlter-based feature selection because these are
methods with a low computational cost. In the next sections, some methods for ﬁlter-based feature
selection are shortly discussed. For more information read [20] and [21].

3.1 Variance Threshold

This feature selection method removes features with a low variance; thus, it calculates each feature’s
variance, then drops features with variance below some threshold, making sure that the features have
the same scale. Although it is a simple method, it has a compelling motivation, since the idea is
that low variance features contain less relevant information. Moreover, the variance threshold is an
unsupervised method, since it only looks at the feature values and not to the desired output.

3.2 Univariate Analysis

This feature selection method consists of selecting the best features based on statistical tests. It is
often used as a pre-processing step to an estimator, thus improving the performance of a model and
cutting of the time complexity, since it reduces the dimensions of a dataset. These statistical tests
are used to found the relationship between each input feature and the output feature. Those input
feature vectors are kept and used for further analysis, which gives a strong statistical relationship
with the output variable, and remaining input features, whose relationship is weak, are discarded.
There is much statistical test used in the univariate feature selection method i.e., Chi-squared, Pearson
correlation, and ANOVA F-value test, and others. Two of them are described below:

• Chi-squared test: A statistical test that aims to evaluate the dependency between two
variables. Although it has some common similarities with the coefﬁcient of determination,
the chi-square test is only valid for nominal or categorical data, whereas the determination
coefﬁcient is only valid for numeric data. This method has various applications, and one of
them is feature selection. Considering an input feature variable and an output variable, i.e.,
class label, ﬁrst, the Chi-squared statistics of every input feature variable concerning the
output is calculated to get the dependency relationship between them. Afterward, features
that have shown less dependency with output variables are discarded, while the features that
have shown strong dependency with the target variable are kept to training the model.

• ANOVA F-test: This method is based on F-test, which is used only for quantitative data.
ANOVA F-test estimates the degree of linear dependency between an input variable and
the target variable, giving a high score to the features which are highly correlated and low
score to less correlated features. The F-value scores inspect if the means for each group are
considerably different when we group the numerical feature by the target vector.

4 Experimental Methodology

The datasets used in this work were collected from the Aslib repository [22], being 28 meta-datasets
related to the algorithm recommendation problem. These meta-datasets are related to classic opti-
mization problems, such as the Traveling Salesman Problem (TSP) and the Propositional Satisﬁability
Problem (SAT). Table 1 shows the dataset names and basic summary of them.

3

Table 1: Dataset’s statistics.

Dataset
ASP-POTASSCO
BNSL-2016
CPMP-2015
CSP-2010
CSP-Minizinc-Obj-2016
CSP-Minizinc-Time-2016
CSP-MZN-2013
GRAPHS-2015
MAXSAT-PMS-2016
MAXSAT-WPMS-2016
MAXSAT12-PMS
MAXSAT15-PMS-INDU
MIP-2016
OPENML-WEKA-2017
PROTEUS-2014
QBF-2011
QBF-2014
QBF-2016
SAT03-16_INDU
SAT11-HAND
SAT11-INDU
SAT11-RAND
SAT12-ALL
SAT12-HAND
SAT12-INDU
SAT12-RAND
SAT15-INDU
TTP-2016

Instances Features Classes
139
93
22
67
123
117
117
36
44
53
31
57
120
157
32
46
46
67
139
66
67
53
58
70
74
98
87
58

1294
1179
527
2024
100
100
4636
5723
596
630
876
601
214
105
4021
1368
1248
825
2000
296
300
592
1614
767
1167
1362
300
9714

11
8
4
2
3
4
10
6
12
10
6
16
3
3
22
5
13
14
10
9
11
8
29
22
21
9
10
18

Table 2: Classiﬁers and their hyperparameters.

Classiﬁer Hyperparameter
KNN

Num nearest neighboors

Default
3

DT

SVM

RF

Min samples split
Min samples leaf
Max depth

C
Gamma

Nº estimators
Max depth

Range
(1, 51)

(2, 51)
(2, 51)
(2, 31)

2
1
None

1
1/Nº features

(1, 32769)
(1, 32769)

10
None

(1, 1025)
(1, 21)

Feature Selection methods used in this work were implemented using “scikit-learn” [23]. For Variance
Threshold, features that do not change in more than 80, 85, 90, and 95 percent of the observations are
removed. For Chi-squared and ANOVA, features are select according to a percentile with the highest
score for each feature selection method. The percentiles considered were 80, 85, 90, and 95.

For comparison criterion, the feature extraction technique "Principal Component Analysis" (PCA)
adds to this experiment. This technique achieved the best results in the work of [5]. The selected
components correspond to that maintain 80, 85,90, and 95 of the original observations. These values
are stemming from [5], and the PCA implementation is available on the "scikit-learn" package [23].

Table 2 shows the classiﬁers used (Classiﬁer), such as K-Nearest Neighbors (KNN), Decision
Tree (DT), Support Vector Machine (SVM), and Random Forest (RF), their hyperparameters tuned
(Hyperparameter), the default value for the speciﬁc hyperparameter tuned (Default) and the range
values tested during the tuning of that hyperparameter (Range). Also, in Table 2, values “None” in
DT and RF rows are the default values deﬁned by Scikit-learn, which are said to be expanded until
all leaves are pure or until all leaves contain fewer samples than Min samples split parameter.

For model evaluation, all the meta-datasets were scaled between 0 and 1 by Minimum-Maximum
scaler [24]. The pipeline includes feature selection methods, meta-learners, and all parameters for
the hyperparameter tuning process. Hyperparameter tuning is made by the Random Search K-Fold
cross-validation technique in which we split our data into k subsets, and then we tune and train our
models on k − 1 folds, and the performance metric are apply on the left fold. This process is repeated
k times until all the folds have been evaluated. Given a hyperparameter space of a speciﬁc machine
learning model, cross-validation is applied n times with a random variation of hyperparameters space.
Thus, the hyperparameters set with the best performance is selected to be applied to test data [25].
The number of k subsets used were 10, a value widely used on the literature [26].

The models are evaluated in three criteria: predictive performance, proportion of dimensionality
reduction, and time to ﬁt. For performance, uses the balanced accuracy metric [27]. Balanced accuracy
is a fair version of the accuracy metric for multi-class problems. The proportion of dimensionality
reduction is the difference between the number of dimensions of the original meta-dataset and the
dimensions after feature selection or extraction, divided by the number of dimensions of the original
meta-dataset. Time to ﬁt is the average time to induce the feature selection or extraction and the
classiﬁcation model. This time is returned by the random search cross-validation function of the
“scikit-learn” package. For reproducibility, all the paper experiment’s implementation is public on
this repository: https://github.com/moisesrsantos/featselec.

4

5 Results and Discussion

This section discuss the results of applying 4 meta-learners (SVM, KNN, Decision Tree, and Random
Forest) to 28 meta-datasets related to algorithm recommendation, which were also submitted to 3
Feature Selection methods (Chi-squared, F-ANOVA and Variance threshold) and 1 Feature Extraction
method (PCA). Our experiments were performed to answer three research questions: (1) Is there
any predictive performance improvement of applying feature selection to meta-datasets related to
algorithm recommendation? (2) How good is feature selection to reduce the dimensionality of those
meta-datasets? (3) Is there any impact on the pipeline execution time when using feature selection?

The following results are divided in three parts, each one aiming to answer one of the research
questions. We used bar plots and box-plots to discuss the properties of the results distributions, as
well as Critical Difference (CD) diagrams to better show the statistical results between comparisons.
In Figures 2, 4, and 6, the diagrams illustrate the Friedman-Nemenyi test with 95% of conﬁdence
for the Balanced Accuracy, percentage of dimensionality reduction, and pipeline execution times. In
the CD diagrams, the more towards to 1, the better is the method. Therefore, the methods on the left
have better performances than the methods on the right. Besides, it is important to consider the CD
intervals that cross the lines representing the methods. These intervals demonstrate that all the lines
that touch it have statistically equal values, so no method is statistically better than the others and,
thus, they all have comparable performances.

5.1 Predictive Performance

In order to verify whether the DR methods bring some improvement in terms of predictive perfor-
mance, we analyzed the distributions of balanced accuracy obtained by applying each DR method to
each meta-dataset and training with each meta-learner. As shown by box-plots in Figure 1, all DR
methods had similar performances, even when compared to the original datasets. Slightly standing out
from the others, the Variance threshold appeared to reduce the variance of the predictive performance
across datasets and classiﬁers, showing a lower range of values. However, it also generated more
sparse maximum outliers. Speaking of outliers, another interesting thing to observe is that the number
of outliers remains pretty much the same regardless of the DR method applied, and those outliers
remain located in the region of the maximum values only.

CD = 0.58

1

2

3

4

5

Original
F ANOVA

Chi-squared
Variance
PCA

Figure 2: Critical Difference diagram of the perfor-
mances.

Figure 1: Balanced Accuracy comparison be-
tween dimensionality reduction methods.

Even though Variance showed more stable results (except for its outliers), when submitted to a
statistical test, it was veriﬁed that F-ANOVA had the best performance between the DR methods.
However, considering the CD interval shown in Figure 2, no DR method improved the overall
predictive performance in the task of algorithm recommendation with statistical signiﬁcance. Even
so, using DR was as good as using the original data.

5.2 Dimensionality Reduction

For the purpose of verifying how good are the feature selection methods in reducing the dimensionality
of the selected meta-datasets, we focused on observing the performance of each technique in a broader
context. Therefore, reductions in all pipelines involving each meta-dataset and all meta-learners were

5

Chi-squaredFANOVAPCAVarianceOriginal0.10.20.30.40.50.60.70.8Balanced Accuracyconsidered. As seen in Figure 3, DR methods showed a high percentage of reduction in average,
with all of them (except for Variance) achieving more than 80% percent reduction. Chi-squared,
F-ANOVA, and PCA presented similar reduction averages in a stable manner. In turn, Variance
proved to be inferior in average and more unstable, showing higher standard deviation values.

CD = 0.44

1

2

3

4

F ANOVA
PCA

Variance
Chi-squared

Figure 4: Critical Difference diagram of the dimen-
sionality reduction.

Figure 3: Percentage of reduction.

When considering the statistical comparisons seen in Figure 4, it is possible to notice that F-ANOVA,
PCA, and Chi-squared did not differ statistically, being equally good in reducing the dimensions of
the original meta-data. Furthermore, it is clear that Variance is statistically worse than others.

5.3 Pipeline Runtime

To verify if there is any impact on the pipeline runtimes when using feature selection, we collected
runtimes from all the pipelines performed in our experiments. Those runtimes included both the
time to extract the sub-set of meta-features (feature selection) or transform the original space of
meta-features (feature extraction) and the runtime to ﬁt the data. The distributions of the runtimes for
each DR method and the original setup are shown in Figure 5. As seen, the times of the pipelines
were short in general, with maximum values achieving close to 2 seconds. The medians for these
distributions were similar and much closer to the Q1 and the minimum. It is interesting to see that the
variations happened mainly in the region of the maximum values, while the minimum values, Q1,
and Interquartile range were similar.

CD = 0.58

1

2

3

4

5

Variance
Chi-squared

PCA
Original
F ANOVA

Figure 6: Critical Difference diagram regarding
runtimes.

Figure 5: Pipeline runtime comparison.

In summary, these results show that the pipelines were cheap, and the application of DR methods
does not seem to have much impact on time. However, when considering the CD diagram shown in
Figure 6, it is possible to see that the execution times for Variance, Chi-square, and F-ANOVA, are
statistically comparable and shorter than using the original data and PCA. Using Feature Selection
proved to be a better solution for reducing the overall time of pipelines than not using it, showing
that, despite the additional time to lower dimensions, the total time, including the training, is shorter.
On the other hand, using or not using PCA did not statistically change the runtime of the pipelines.

6

0.00.20.40.60.8Proportion of ReductionChi-squaredF ANOVAPCAVarianceChi-squaredF ANOVAPCAVarianceOriginal0.00.51.01.52.0Time (seconds)6 Conclusion and Future Works

We presented a comparative study of Feature Selection and Feature Extraction in the meta-level
setup for datasets related to Algorithm Recommendation. Our goal was to evaluate popular DR
methods according to three criteria: predictive performance, dimensionality reduction, and pipeline
runtime. As we veriﬁed, applying DR did not improve predictive performance in general. However,
it reduced around 80% of the meta-features yet achieving the same performance as the original setup
with a shorter runtime. The only exception was PCA, which showed about the same runtime. Our
experiments showed that these meta-datasets have many non-informative meta-features and that
it is possible to achieve good performance using about 20% of the original features. Traditional
Meta-Learning, the one focused on hand-craft extraction of meta-features, seems to suffer a lot with
the curse of dimensionality. Instead of helping Machine Learning algorithms to achieve better or
fast performance while taking advantage of high-level information (meta-level), it seems that, in
some cases, meta-info hinders the task. We conclude that using DR methods is beneﬁcial to many
meta-datasets since their natural trend for high dimensionality. However, we acknowledge that more
experiments with different numbers and types of meta-features are needed to ensure our claim.

References

[1] Michael I Jordan and Tom M Mitchell. Machine learning: Trends, perspectives, and prospects.

Science, 349(6245):255–260, 2015.

[2] Pavel Brazdil, Christophe Giraud Carrier, Carlos Soares, and Ricardo Vilalta. Metalearning:

Applications to data mining. Springer Science & Business Media, 2008.

[3] Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren. Automatic machine learning: methods,

systems, challenges. Challenges in Machine Learning, 2019.

[4] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adap-
tation of deep networks. In Proceedings of the 34th International Conference on Machine
Learning-Volume 70, pages 1126–1135. JMLR. org, 2017.

[5] Edesio Alcobaça, Rafael G Mantovani, André LD Rossi, and André CPLF de Carvalho. Di-
mensionality reduction for the algorithm recommendation problem. In 2018 7th Brazilian
Conference on Intelligent Systems (BRACIS), pages 318–323. IEEE, 2018.

[6] Gean Trindade Pereira, Moisés dos Santos, Edesio Alcobaça, Rafael Mantovani, and André
Carvalho. Transfer learning for algorithm recommendation. arXiv preprint arXiv:1910.07012,
2019.

[7] Gabriel Jonas Aguiar, Rafael Gomes Mantovani, Saulo M Mastelini, André CPFL de Carvalho,
Gabriel FC Campos, and Sylvio Barbon Junior. A meta-learning approach for selecting image
segmentation algorithm. Pattern Recognition Letters, 128:480–487, 2019.

[8] Gabriel Fillipe Centini Campos, Saulo Martiello Mastelini, Gabriel Jonas Aguiar, Rafael Gomes
Mantovani, Leonimer Flávio de Melo, and Sylvio Barbon. Machine learning hyperparameter
selection for contrast limited adaptive histogram equalization. EURASIP Journal on Image and
Video Processing, 2019(1):59, 2019.

[9] Isabelle Guyon, Steve Gunn, Masoud Nikravesh, and Lofti A Zadeh. Feature extraction:

foundations and applications, volume 207. Springer, 2008.

[10] Peter Flach. Machine learning: the art and science of algorithms that make sense of data.

Cambridge University Press, 2012.

[11] Stephen Marsland. Machine learning: an algorithmic perspective. CRC press, 2015.

[12] John R Rice. The algorithm selection problem. In Advances in computers, volume 15, pages

65–118. Elsevier, 1976.

[13] David H Wolpert. The lack of a priori distinctions between learning algorithms. Neural

computation, 8(7):1341–1390, 1996.

[14] Ricardo Vilalta and Youssef Drissi. A perspective view and survey of meta-learning. Artiﬁcial

intelligence review, 18(2):77–95, 2002.

7

[15] Jorge Y Kanda, Andre CPLF de Carvalho, Eduardo R Hruschka, and Carlos Soares. Using
meta-learning to recommend meta-heuristics for the traveling salesman problem. In 2011 10th
International Conference on Machine Learning and Applications and Workshops, volume 1,
pages 346–351. IEEE, 2011.

[16] Antonio Rafael Sabino Parmezan, Huei Diana Lee, and Feng Chung Wu. Metalearning for
choosing feature selection algorithms in data mining: Proposal of a new framework. Expert
Systems with Applications, 75:1–24, 2017.

[17] Thiyanga S Talagala, Rob J Hyndman, George Athanasopoulos, et al. Meta-learning how to
forecast time series. Monash Econometrics and Business Statistics Working Papers, 6:18, 2018.
[18] Adriano Rivolli, Luís PF Garcia, Carlos Soares, Joaquin Vanschoren, and André CPLF
de Carvalho. Towards reproducible empirical research in meta-learning. arXiv preprint
arXiv:1808.10406, 2018.

[19] Adriano Rivolli, Luís PF Garcia, Carlos Soares, Joaquin Vanschoren, and André CPLF de Car-
valho. Characterizing classiﬁcation datasets: a study of meta-features for meta-learning. arXiv
preprint arXiv:1808.10406, 2018.

[20] Cosmin Lazar, Jonatan Taminau, Stijn Meganck, David Steenhoff, Alain Coletta, Colin Molter,
Virginie de Schaetzen, Robin Duque, Hugues Bersini, and Ann Nowe. A survey on ﬁlter
techniques for feature selection in gene expression microarray analysis. IEEE/ACM Transactions
on Computational Biology and Bioinformatics, 9(4):1106–1119, 2012.

[21] Andrea Bommert, Xudong Sun, Bernd Bischl, Jörg Rahnenführer, and Michel Lang. Benchmark
for ﬁlter methods for feature selection in high-dimensional classiﬁcation data. Computational
Statistics & Data Analysis, 143:106839, 2020.

[22] Bernd Bischl, Pascal Kerschke, Lars Kotthoff, Marius Lindauer, Yuri Malitsky, Alexandre
Fréchette, Holger Hoos, Frank Hutter, Kevin Leyton-Brown, Kevin Tierney, et al. Aslib: A
benchmark library for algorithm selection. Artiﬁcial Intelligence, 237:41–58, 2016.

[23] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,
P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher,
M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine
Learning Research, 12:2825–2830, 2011.

[24] Jiawei Han, Jian Pei, and Micheline Kamber. Data mining: concepts and techniques. Elsevier,

2011.

[25] James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal

of machine learning research, 13(Feb):281–305, 2012.

[26] Stuart J Russell and Peter Norvig. Artiﬁcial Intelligence: A Modern Approach. Prentice Hall,

2009.

[27] Kay Henning Brodersen, Cheng Soon Ong, Klaas Enno Stephan, and Joachim M Buhmann.
The balanced accuracy and its posterior distribution. In 2010 20th International Conference on
Pattern Recognition, pages 3121–3124. IEEE, 2010.

8

