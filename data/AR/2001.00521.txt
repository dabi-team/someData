Lightform: Procedural Effects for Projected AR

Brittany Factura
Lightform, Inc.

Laura LaPerche
Lightform, Inc.

Phil Reyneri
Lightform, Inc.

Brett Jones
Lightform, Inc.

Kevin Karsch
Lightform, Inc.

9
1
0
2
c
e
D
5
2

]

R
G
.
s
c
[

1
v
1
2
5
0
0
.
1
0
0
2
:
v
i
X
r
a

Figure 1: Lightform demonstration showing an automatic effect. From left to right: LF1 with laptop running Lightform Creator
in the background; a scene during LF1 structured light scanning; screenshot of Lightform Creator during effect generation;
the scene with the automatic ”TRON” effect applied (original scene inset).

CCS CONCEPTS
•Computing methodologies →Mixed / augmented reality; Re-
construction;

KEYWORDS
projection mapping, structured light, procedural effects

ACM Reference format:
Brittany Factura, Laura LaPerche, Phil Reyneri, Brett Jones, and Kevin
Karsch. 2018. Lightform: Procedural Effects for Projected AR. In Proceedings
of SIGGRAPH ’18 Studio, Vancouver, BC, Canada, August 12-16, 2018, 3 pages.
DOI: 10.1145/3214822.3214823

1 INTRODUCTION
Projected augmented reality, also called projection mapping or
video mapping, is a form of augmented reality that uses projected
light to directly augment 3D surfaces, as opposed to using pass-
through screens or headsets. The value of projected AR is its ability
to add a layer of digital content directly onto physical objects or
environments in a way that can be instantaneously viewed by
multiple people, unencumbered by a screen or additional setup.

Because projected AR typically involves projecting onto non-flat,
textured objects (especially those that are conventionally not used
as projection surfaces), the digital content needs to be mapped and
aligned to precisely fit the physical scene to ensure a compelling
experience. Current projected AR techniques require extensive
calibration at the time of installation, which is not conducive to
iteration or change, whether intentional (the scene is reconfigured)
or not (the projector is bumped or settles). The workflows are
undefined and fragmented, thus making it confusing and difficult
for many to approach projected AR. For example, a digital artist

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
SIGGRAPH ’18 Studio, Vancouver, BC, Canada
© 2018 Copyright held by the owner/author(s). 978-1-4503-5819-4/18/08. . . $15.00
DOI: 10.1145/3214822.3214823

may have the software expertise to create AR content, but could not
complete an installation without experience in mounting, blending,
and realigning projector(s); the converse is true for many A/V
installation teams/professionals. Projection mapping has therefore
been limited to high-end event productions, concerts, and films,
because it requires expensive, complex tools, and skilled teams
($100K+ budgets).

Lightform provides a technology that makes projected AR ap-
proachable, practical, intelligent, and robust through integrated
hardware and computer-vision software. Lightform brings together
and unites a currently fragmented workflow into a single cohe-
sive process that provides users with an approachable and robust
method to create and control projected AR experiences.

2 LIGHTFORM DESIGN AND

IMPLEMENTATION

Lightform is comprised of two solutions: a small camera/computer
device called the ”LF1” which physically attaches to a projector,
and a desktop software application called ”Lightform Creator” that
allows a user to control the LF1 and also create projected AR content
that can be uploaded and played through the LF1. Using computer
vision and machine learning, we automate and simplify many of
the pain points associated with projected AR, such as (re)alignment
and content creation. We believe that these tools will help make
projected AR faster, easier and cheaper than before.

2.1 Lightform LF1
The LF1 contains mobile processors as well as a 12 megapixel
(4096x3072) RGB imaging sensor. It can be connected to a net-
work over WiFi or Ethernet, and has an HDMI port for connecting
to most any projector. Modern projectors come in a variety of
throw ratios (field of view), and the LF1 supports different lens
configurations to support many of these.

The LF1 acts as a media playback and hosting server, but most
importantly, it is responsible for acquiring structured light scans

 
 
 
 
 
 
SIGGRAPH ’18 Studio, August 12-16, 2018, Vancouver, BC, Canada

Factura, LaPerche, Reyneri, Jones, and Karsch

of the scene, and computing procedural1 AR content (effects). The
data acquired by the LF1 is also sent to Lightform Creator for users
who wish to create more complex AR experiences.

2.1.1

Structured Light Scanning. Structured light scanning is
one of the first and most important steps of the Lightform workflow,
enabling both realignment and content creation. We have imple-
mented a visible structured light algorithm (inspired by Yamazaki
et al. [Yamazaki et al. 2011]) on the LF1 for remotely capturing
detailed scene information. A structured light scan operates by
projecting patterns which are captured by a camera (in this case,
the LF1). This provides a dense pixel correspondence from projector
pixels to/from camera pixels. This mapping can be thought of in
much the same way as stereo camera calibration and reconstruc-
tion [Hartley and Zisserman 2003], and we apply similar techniques
to extract 3D information (disparity and depth) from the scene. One
important benefit that this provides is the ability to reconstruct a
projector image, in other words, a remapping of camera pixels into
the projector’s domain to obtain an image of the scene as if taken
with the projector’s optical parameters and point-of-view. The pro-
jector image is extremely important for creating procedural effects,
as it provides an image that can be understood and processed by
vision techniques (e.g. segmentation, edge detection, etc) and effect-
generating algorithms. A user can author content directly on top
of this image, and it is automatically aligned with the real world,
eliminating the need for traditional mapping workflows.

2.1.2 Procedural Effects: Real World Image Filters. Lightform
uses the structured light scan data to create instant effects, which
makes it easy to produce complex, dynamic motion content with-
out motion graphics or multimedia expertise. Using the projector
image, effects are created by applying various image processing and
computer vision techniques that result in interesting and unique
animations that adapt to the scene. For example, the ”TRON” effect
traces edges present in the scene. This effect first applies Canny
edge detection to the projector image, then uses a shader to perform
edge tracing on the detected edges. The result is an animation in
which the edges of the scene glow and change over time. We show
several of these effects in the accompanying video, and we think
of these as AR filters (in the image processing sense) for the real
world.

Shadertoy. Besides the set of premade effects we have created,
the LF1 supports Shadertoy formatted effects (GLSL shaders that
utilize the same structure and syntax as those found on http://www.
shadertoy.com). This enables custom effects that can be created by
users and shared among the community, or simply downloaded
from Shaderytoy and applied as a projected AR effect.

2.2 Lightform Creator
Lightform Creator is a cross-platform desktop software application.
It can control the LF1 (trigger a scan, stream video/camera images,
play/pause content, etc), but most importantly, it is a tool for easily
creating projected AR content. This tool can be described as a hybrid
between 2D multimedia editing software (e.g. Adobe’s After Effects)
and 3D modeling software (e.g. Autodesk’s Maya), however our
aim is to simplify the user interactions through the use of machine

1Procedural, in this sense, implies that effects adapt to the contents (color and/or
geometry) of a given scene automatically.

learning and computer vision. For the purposes of this paper, we
will focus on Lightform Creator’s procedural effects capabilities.

After capturing a scan and receiving the projector image from
the LF1, Lightform Creator allows a user to quickly create masks
(regions where content should be projected in a scene) with vision-
assisted tools similar to Photoshop’s Quick Select, Magic Wand and
Magnetic Lasso tools. Once masks have been created, the user can
choose from a list of procedural effects to be applied to the scene.
For example, TRON traces the edges of a scene, another distorts a
scene to attract attention, and another makes the scene appear as
if it were a hand-drawn cartoon; many effects are inspired by the
Illumiroom project [Jones et al. 2013]. See the accompanying video
for a demonstration.

3 APPLICATIONS OF LIGHTFORM
In the past, projected AR has been demonstrated primarily in the
academic community and in high-budget concerts and shows. Light-
form encourages and enables these as well as many new AR appli-
cations; different from existing AR technology, these do not require
headsets or additional hardware, and can be consumed by multiple
viewers at the same time.

We are focused on allowing projected AR experiences to be
shared, friction-less, cost-effective, and hidden by presenting an
out-of-home technology that can be scaled to many viewers and
viewed naturally with the naked human eye. We believe in the im-
portance of replacing printed signs or television screens to overlay
digital information onto the real world using a technology that is
invisible, seamless, and unencumbered by screens, thus providing
an out-of-home digital display experience that is novel to those
that experience it. Digital data, art, and ”magic” can be seamlessly
integrated into everyday spaces.

We are particularly interested in using Lightform to communi-
cate and display information or attract attention towards a specific
object or venue (digital signage), create stand-alone art pieces (am-
biance), design a space (place-making and identity), or purely to
provide entertainment and joy (art/performance). Specifically, a
range of projects can be enabled with projected AR: custom digital
content can augment and transform a mural; a cafe menu can be
easily animated with dynamic and updatable items without affect-
ing the aesthetic of a restaurant; compelling effects can animate
onto a 3D sculpture; in general, spaces can be transformed com-
pletely, allowing viewers to engage with their surroundings in new
and meaningful ways.

4 SIGGRAPH DEMONSTRATION
We will demonstrate an interactive tabletop where visitors will
be able to customize their own projected AR scene using various
physical objects and a drawing station. The LF1 will scan the setup
the visitors have created, and users will then have the ability to
apply effects onto their scene. We will also demonstrate a second
LF1 setup running premade effects onto a variety of art pieces and
sculptures within the booth.

REFERENCES
Richard Hartley and Andrew Zisserman. 2003. Multiple View Geometry in Computer

Vision (2 ed.). Cambridge University Press, New York, NY, USA.

Brett R. Jones, Hrvoje Benko, Eyal Ofek, and Andrew D. Wilson. 2013. IllumiRoom:
Peripheral Projected Illusions for Interactive Experiences. In Proceedings of the

Lightform: Procedural Effects for Projected AR

SIGGRAPH ’18 Studio, August 12-16, 2018, Vancouver, BC, Canada

SIGCHI Conference on Human Factors in Computing Systems (CHI ’13). ACM, New
York, NY, USA, 869–878.

S. Yamazaki, M. Mochimaru, and T. Kanade. 2011. Simultaneous self-calibration of a
projector and a camera using structured light. In CVPR 2011 Workshops. 60–67.

