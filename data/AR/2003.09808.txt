0
2
0
2

r
a

M
2
2

]
T
I
.
s
c
[

1
v
8
0
8
9
0
.
3
0
0
2
:
v
i
X
r
a

Tracking an Auto-Regressive Process with Limited
Communication per Unit Time

Rooji Jinan

Parimal Parag Himanshu Tyagi

1

Abstract

Samples from a high-dimensional AR[1] process are observed by a sender which can communicate only ﬁnitely many bits
per unit time to a receiver. The receiver seeks to form an estimate of the process value at every time instant in real-time. We
consider a time-slotted communication model in a slow-sampling regime where multiple communication slots occur between
two sampling instants. We propose a successive update scheme which uses communication between sampling instants to reﬁne
estimates of the latest sample and study the following question: Is it better to collect communication of multiple slots to send
better reﬁned estimates, making the receiver wait more for every reﬁnement, or to be fast but loose and send new information
in every communication opportunity? We show that the fast but loose successive update scheme with ideal spherical codes is
universally optimal asymptotically for a large dimension. However, most practical quantization codes for ﬁxed dimensions do not
meet the ideal performance required for this optimality, and they typically will have a bias in the form of a ﬁxed additive error.
Interestingly, our analysis shows that the fast but loose scheme is not an optimal choice in the presence of such errors, and a
judiciously chosen frequency of updates outperforms it.

I. INTRODUCTION

We consider the setting of real-time decision systems based on remotely sensed observations. In this setting, the decision
maker needs to track the remote observations with high precision and in a timely manner. These are competing requirements,
since high precision tracking will require larger number of bits to be communicated, resulting in larger transmission delay and
increased staleness of information. Towards this larger goal, we study the following problem.

Consider a discrete time ﬁrst-order auto-regressive (AR[1]) process Xt ∈

0. A sensor draws a sample from this
process, periodically once every s time-slots. In each of these time-slots, the sensor can send nR bits to a center. The center
seeks to form an estimate ˆXt of Xt at time t, with small mean square error (MSE). Speciﬁcally, we are interested in minimizing
the time-averaged error

2
2/T to enable timely and accurate tracking of Xt.

We propose and study a successive update scheme where the encoder computes the error in the estimate of the latest sample
at the decoder and sends its quantized value to the decoder. The decoder adds this value to its previous estimate to update the
estimate of the latest sample, and uses it to estimate the current value using a linear predictor. We instantiate this scheme with
a general gain-shape quantizer for error-quantization.

Xt −

ˆXtk

E
k

T
t=1

P

≥

Rn, t

Note that we can send this update several times between two sampling instances. In particular, our interest in comparing a
fast but loose scheme where an update is sent every slot or a slower update every p communication slots. The latter allows
the encoder to use more bits for the update, but the decoder will need to wait longer. We analyze this scheme for a universal
setting and show that the fast but loose successive update scheme, used with an appropriately selected quantizer, is optimal
asymptotic.

To show this optimality, we use a random construction for the quantizer, based on the spherical code given in [1], [2].

Roughly speaking, this ideal quantizer Q yields

for every y uniformly bounded. However, in practice, at ﬁnite n, such quantizers need not exist. Most practical vector quantizers
have an extra additive error, i.e., the error bound takes the form

y

E
k

Q(y)
k

2
y
2 ≤ k

2
22−2R,
k

−

2
y
Q(y)
2 ≤ k
k
We present our analysis for such general quantizers. Interestingly, for such a quantizer (which is all we have at a ﬁnite n),
the optimal choice of p can differ from 1. Our analysis provides a theoretically sound guideline for choosing the frequency of
updates 1/p for practical quantizers.

2θ + nε2.

E
y
k

−

k

2

Our work relates to a large body of literature ranging from real-time compression to control and estimation over networks.
The structure of real-time encoders for source coding has been studied in [3]–[9]. The general structure of real-time encoders
for Markov sources is studied for communication over error-free channels in [3] and over noisy channels in [4], [5]. A similar
structural result for the optimal encoders and decoders which are restricted to be causal is given in [6]. Furthermore, structural
results in the context of optimal zero-delay coding of correlated sources are available in [7]–[9]. The setup in all these works
are different from the problem we consider and the results do not extend to our problem.

The authors are with the Department of Electrical Communication Engineering, Indian Institute of Science, Bangalore 560012, India. Email: {roojijinan,

parimal, htyagi}@iisc.ac.in.

 
 
 
 
 
 
2

The problems of remote estimation under communication constraints of various kinds have been studied in [10]–[15]. This
line of work proposes several Kalman-like recursive estimation algorithms and evaluates their performances. In a related
thread, [16]–[18] study remote estimation under communication constraints and other related constraints using tools from
dynamic programming and stochastic control. However, in all these works the role of channel delay is slightly different
from that in our setting. Furthermore, the speciﬁc problem of choice of quantizer we consider has not been looked at. More
recently, [19] studied remote estimation of Wiener process for channels with random delays and proves that the optimal
sampling policy is a threshold based policy. This work, like some of the works cited above, assumes real-valued transmissions
and does not take into account any quantization effects.

In more information theoretic settings, sequential coding for individual sequences under delay constraints was studied
in [20]–[23]. Closer to our work, the causal (non-anticipatory) rate-distortion function for a stochastic process goes back to
early works [24], [25]. Recent works [26]–[28] consider the speciﬁc case of auto-regressive and Gauss-Markov processes and
use the general formula in these early works to establish asymptotic optimality of simpler information structure for the encoders
(optimal decoder structure is straightforward). We note that related formulations have been studied for simple settings of two
or three iterations in [29], [30] where interesting encoder structures for using previous communication for next sample as
well emerge. Although some of these works propose speciﬁc optimal schemes as well, the key results in this line of research
provide an expression for the rate-distortion function as an optimization problem, solving which will provide guidelines for a
concrete scheme. In contrast, motivated by problems of estimation over an erasure channel, [31] provides an asymptotically
optimal scheme that roughly uses Gaussian codebooks to quantize the innovation errors between the encoder’s observation
and the decoder’s estimation. Our work is closest to [31], but differs in an important aspect from all the works mentioned in
this research thread: We take transmission delays into account. In particular, in our formulation the decoder estimation and
communication delays are on the same time-scales. The decoder must provide an estimate at every time instant, and a longer
codeword will result in a longer delay for the decoder to get complete information.

Nonetheless, our converse bound is derived using similar methods as [32]. Even the achievability part of our proof draws
from [32], but there is technical caveat. Note that after the ﬁrst round of quantization, the error vector need not be Gaussian,
and the analysis in [32] can only be applied after showing a closeness of the error vector distribution to Gaussian in the
Wasserstein distance of order 2. While the original proof [32] overlooks this technical point, this gap can be ﬁlled using a
recent result from [33] if spherical codes are used. But we follow an alternative approach and show a direct analysis using
vector quantizers.

In addition, there is a large body of work on control problems over rate-limited communication channels (cf. [34]–[43]).
This line of work implicitly requires handling of communication delays in construction of estimators. However, the simple
formulation we have seems to be missing, and results in this long line of work do not resolve the questions we raise.

Our main contributions in this paper are as follows: We present a heuristically appealing encoder structure which we show
to be optimal asymptotically in the dimension of the observation. Speciﬁcally, we propose to send successive updates that
reﬁnes the estimates of the latest sample at the decoder. It is important to note that we quantize the estimation error at the
decoder, instead of quantizing the innovation sequence formed at the encoder. Although the optimal MMSE decoder involves
taking conditional expectation, we use a simple decoder which uses a simple linear structure. Yet, we show that this decoder
is asymptotically optimal. Then, we instantiate this general scheme with spherical codes for quantizers to obtain a universal
scheme. In particular, we consider general gain-shape quantizers and develop a framework to analyze their performance. One
interesting result we present shows that the tradeoff between the accuracy and frequency of the updates must be carefully
balanced based on the “bias” (additive error) in the quantizer used.

We present our problem formulation in the next section. Section III presents a discussion on the our achievability scheme
followed by the main results in the subsequent section. Section V provides a detailed analysis of our scheme, which we further
build-on in Section VI to get our asymptotic achievability results. We prove our converse bound in Section VII, and conclude
with a discussion on extensions of our result in the ﬁnal section.

II. PROBLEM FORMULATION

We begin by providing a formal description of our problem; different components of the model are presented in separate
sections. Throughout the remainder of this paper, the set of real numbers is denoted by R, the n-dimensional Euclidean space is
denoted by Rn and the associated Euclidean norm by
k·k2, the set of positive integers is denoted by N, the set of non-negative
integers is denoted by Z+, the set of continuous positive integers until m is denoted by [m] ,
, and an identity
matrix of size n

1, . . . , m
{

n is denoted by In.

}

×

A. Observed random process and its sampling

For α

∈

(0, 1), we consider a discrete time auto-regressive process of order 1 (AR[1] process) in Rn,

Xt = αXt−1 + ξt,

t > 0,

(1)

where (ξt ∈
matrix σ2(1
This implies that the variance of Xt ∈
moment at all times t > 0. Speciﬁcally, let κ > 0 satisfy

Rn, t > 1) is an independent and identically distributed (i.i.d.) random sequence with zero mean and covariance
Rn is a zero mean random variable with covariance matrix σ2In.
α2)In. For simplicity, we assume that X0 ∈
Xtk2 has a bounded fourth

Rn is σ2In for all t > 0. In addition, we assume that

−

k

sup
k∈Z+

1
n q

E
k

4
2 6 κ.

Xkk

3

It is clear that X = (Xt ∈
by Xn and the class of all such processes for different choices of dimension n as X.

Rn, t > 0) is a Markov process. We denote the set of processes X satisfying the assumptions above

This discrete time process is sub-sampled periodically at sampling frequency 1/s, for some s

(Xks ∈

Rn, k > 0).

N, to obtain samples

∈

B. Encoder description

The sampled process (Xks, k > 0) is passed to an encoder which converts it to a bit stream. The encoder operates in
real-time and sends nRs bits between any two sampling instants. Speciﬁcally, the encoder is given by a sequence of mappings
(φt)t≥0, where the mapping at any discrete time t = ks is denoted by

φt : Rn(k+1)

0, 1

nRs .
}

→ {

The encoder output at time t = ks is denoted by the codeword Ct , φt(X0, Xs, . . . , Xks). We represent this codeword by an
nR. For t = ks and
s-length sequence of binary strings Ct = (Ct,0, . . . , Ct,s−1), where each term Ct,i takes values in
}
1, we can view the binary string Ct,i as the communication sent at time t + i. We elaborate on the communication
0
≤
channel next.

0, 1

−

≤

{

s

i

C. Communication channel

The output bit-stream of the encoder is sent to the receiver via an error-free communication channel. Speciﬁcally, we assume
slotted transmission with synchronization where in each slot the transmitter sends nR bits of communication error-free. That is,
we are allowed to send R bits per dimension, per time slot. Note that there is a delay of 1 time-unit (corresponding to one slot) in
transmission of each nR bits. Therefore, the vector Cks,i of nR bits transmitted at time ks+i is received at time instant ks+i+1
for 0 6 i 6 s
1. Throughout we use the notation Ik ,
,
ks, . . . , (k + 1)s
}
respectively, for the set of transmit and receive times for the strings Cks,i, 0

ks + 1, . . . , (k + 1)s
{

and ˜Ik = Ik + 1 =

1.

−

}
i

{

1

s

−
≤

≤

−

D. Decoder description

∈

Ik, for some k

We describe the operation of the receiver at time t

. Upon
}
receiving the codewords Cs, C2s, ..., C(k−1)s and the partial codeword (Cks,0, ..., Cks,i−1) at time t = ks + i, the decoder
estimates the current-state Xt of the process using the estimator mapping
Rn.

}
We denote the overall communication received by the decoder until time instant1 t by Ct−1. Further, we denote by ˆXt|t the
real-time causal estimate ψt(Ct−1) of Xt formed at the decoder at time t. Thus, the overall real-time causal estimation scheme
is described by the mappings (φt, ψt)t>0. It is important to note that the communication available to the decoder at time t
Ik
can only depend on samples Xℓ up to time ℓ 6 ks. As a convention, we assume that ˆX0|0 = 0.

0, . . . , s

ψt :

∈ {

0, 1

ks

→

nRt

−

−

∈

∈

{

1

N, such that i = t

E. Performance metrics

We call the encoder-decoder mapping sequence (φ, ψ) = (φt, ψt)t>0 a tracking code of rate R and sampling period s. The
tracking error of our tracking code at time t for process X is measured by the mean squared error (MSE) per dimension given
by

Dt(φ, ψ, X) ,

1
n

E
k

Xt −

2
ˆXt|tk
2.

Our goal is to design (φ, ψ) with low average tracking error DT (φ, ψ, X) given by

DT (φ, ψ, X) ,

1
T

T −1

Xt=0

Dt(φ, ψ, X).

1The time index t − 1 in Ct−1 corresponds to the transmission time of the codewords, whereby the communication received till time t is denoted by

Ct−1.

For technical reasons, we restrict to a ﬁnite time horizon setting. For the most part, the time horizon T will remain ﬁxed and
will be omitted from the notation. Instead of working with the mean-square error, a more convenient parameterization for us
will be that of accuracy, given by

4

−
Deﬁnition 1 (Maxmin tracking accuracy). The worst-case tracking accuracy for Xn attained by a tracking code (φ, ψ) is given
by

δT (φ, ψ, X) = 1

DT (φ, ψ, X)
σ2

.

The maxmin tracking accuracy for Xn at rate R and sampling period s is given by

δT (φ, ψ, Xn) = inf
X∈Xn

δT (φ, ψ, X).

n (R, s, , Xn) = sup
δT
(φ,ψ)

δT (φ, ψ, Xn),

where the supremum is over all tracking codes (φ, ψ).

The maxmin tracking accuracy δT

of the observations in Xt for X
in n and T . Speciﬁcally, we deﬁne the asymptotic maxmin tracking accuracy as

∈

n (R, s, Xn) is the fundamental quantity of interest for us. Recall that n denotes the dimension
n (R, s, Xn) asymptotically
Xn and T the time horizon. However, we will only characterize δT

δ∗(R, s, X) = lim sup
T →∞

lim sup
n→∞

n (R, s, Xn).
δT

We will provide a characterization of δ∗(R, s, X) and present a sequence of tracking codes that attains it. In fact, the tracking
code we use is an instantiation of our successive update scheme, which we describe in the next section. It is important to note
that our results may not hold if we switch the order of limits above: We need very large codeword lengths depending on a
ﬁxed ﬁnite time horizon T .

III. THE SUCCESSIVE UPDATE SCHEME
In this section, we present our main contribution in this paper, namely the Successive Update tracking code. Before we
describe the scheme completely, we present its different components. In every communication slot, the transmitter gets an
opportunity to send nR bits. The transmitter may use it to send any information about a previously seen sample. There are
various options for the encoder. For instance, it may use the current slot to send some information about a sample it had seen
earlier. Or it may use all the slots between two sampling instants to send a quantized version of the latest sample. Interestingly,
it will be seen (quite straightforwardly) that there are not so many options for the decoder; it gets roughly ﬁxed once the
encoder is chosen.

A. Decoder structure

Once the quantized information is sent by the transmitter, at the receiver end, the decoder estimates the state Xt, using the
codewords received until time t. Since we are interested in forming estimates with small MSE, the decoder simply forms the
minimum mean square error (MMSE) estimate using all the observations till that point. Speciﬁcally, for t
u, denoting by
≥
˜Xu|t the MMSE estimate Xu formed by the communication Ct−1 received before time t, we know (cf. [44])

˜Xu|t = E[Xu|

Ct−1].

The following result presents a simple structure for ˜Xu|t for our AR[1] model.
Lemma 1 (MMSE Structure). The MMSE estimates ˜Xt|t and ˜Xt−i|t, respectively, of samples Xt and Xt−i at any time t
and i = t

ks using communication Ct−1 are related as

Ik

∈

−

Proof: Recalling the notation Ik, we can represent t

Ik as t = ks + i for 0
1, the sample Xks+i can be expressed in terms of the previous sample Xks as

≤

≤

−

s

i

1. From the evolution of the

AR[1] process, for 1

i

s

−

≤

≤

˜Xt|t = αi ˜Xt−i|t = αiE[Xks|
∈

Ct−1].

Xks+i = αiXks +

αi−j ξks+j ,

i

Xj=1

(2)

where the innovation sequence (ξks+j : j > 1) is independent of process samples (X0, . . . , Xks). By our speciﬁcation, the
historical observations Ct−1 at the receiver depend only on the process evolution until time ks, namely Ct−1 is independent
of (ξks+j : j > 1) conditioned on (X0, . . . , Xks). In particular, E[ξks+j |
1. Thus, taking conditional
expectation on both sides of (2), we get

Ct−1] = 0 for every j

≥

˜Xks+i|ks+i = E[Xks+i|

Ct−1] = αiE[Xks|

Ct−1] = αi ˜Xks|t,

5

since ˜Xks|t = E[Xks|
and then scale it to form the estimate of the state at the current time instant.

Ct−1].

Therefore, the optimal strategy for the decoder is to use the communication sent to form an estimate for the latest sample

B. Encoder structure: Reﬁning the error successively

The structure of the decoder exposed in Lemma 1 gives an important insight for encoder design: The communication sent
between two sampling instants is used only to form estimates of the latest sample. In particular, the communication Cks+1,i
transmitted at time t = ks + i must be chosen to reﬁne the previous estimate from E[Xks|
C0, . . . , Cks, Cks+1,0, . . . , Cks+1,i−1]
to E[Xks|
C0, . . . , Cks, Cks+1,0, . . . , Cks+1,i−1, Cks+1,i]. This principle can be applied (as a heuristic) for any other form of
the estimate as follows. Let ˆXks|t denote the estimate for Xks formed at the receiver at time t (which need not be the MMSE
estimate ˜Xks|t). Our encoder computes the error in the receiver estimate of the last process sample at each time instant t.
ˆXks|t, the encoder quantizes this error Yt and sends it as communication
Denoting the error at time t
Cks+1,i.

Ik by Yt , Xks −

∈

Simply speaking, our encoder computes and quantizes the error in the current estimate of the last sample at the decoder,
and sends it to the decoder to enable the reﬁnement of the estimate in the next time slot. While we have not been able to
establish optimality of this encoder structure, our results will show its optimality asymptotically, in the limit as the dimension
n goes to inﬁnity.

Even within this structural simpliﬁcation, a very interesting question remains. Since the process is sampled once in s time
˜Ik, the receiver has access to (C0, . . . , C(k−1)s)
slots, we have, potentially, nRs bits to encode the latest sample. At any time t
and the partial codewords (Cks,0, . . . , Cks,i−1) for i = t
ks. A simple approach for the encoder is to use the complete
codeword to express the latest sample and the decoder can ignore the partial codewords. This approach will result in slow but
very accurate updates of the sample estimates. An alternative fast but loose approach will send nR quantizer codewords to
reﬁne estimates in every communication slot. Should we prefer fast but loose estimates or slow but accurate ones? Our results
will shed light on this conundrum.

−

∈

C. The choice of quantizers

In our description of the encoder structure above, we did not specify a key design choice, namely the choice of the quantizer.
We will restrict to using the same quantizer to quantize the error in each round of communication. The precision of this quantizer
will depend on whether we choose a fast but loose paradigm or a slow but accurate one. However, the overall structure will
remain the same. Roughly speaking, we allow any gain-shape [45] quantizer which separately sends the quantized value of
k2 for input y. Formally, we use the following abstraction.
the gain
. For 0

k2 and the shape y/

1 and 0

y
k

k

y

θ

nR constitutes an nR bit (θ, ε)-quantizer if for every vector y

∞

≤

≤

≤

ε, a quantizer Q with dynamic range
Rn such that

Deﬁnition 2 ((θ, ε)-quantizer family). Fix 0 < M <
M speciﬁed by a mapping Q : Rn
y

→ {

0, 1

2
2 6 nM 2, we have
k

}

k

Further, for a mapping θ : R+ →
{
constitutes an (θ, ε)-quantizer family if for every R the quantizer QR constitutes an nR bit (θ(R), ε)-quantizer.

2
2θ(R) + nε2.
k
[0, 1], which is a decreasing function of rate R, a family of quantizers Q =

Q(y)
k

2
2 6

E
k

y
k

−

y

QR : R > 0

}

∈

The expectation in the previous deﬁnition is taken with respect to the randomness in the quantizer, which is assumed to
be shared between the encoder and the decoder for simplicity. The parameter M , termed the dynamic range of the quantizer,
√nM , the quantizer simply declares a failure,
speciﬁes the domain of the quantizer. When the input y does not satisfy
y
k
. Our tracking code may use any such (θ, ε)-quantizer family. It is typical in any construction of a
which we denote by
gain-shape quantizer to have a ﬁnite M and ε > 0. Our analysis for ﬁnite n will apply to any such (θ, ε)-quantizer family
and, in particular, will bring-out the role of the “bias” ε. However, when establishing our optimality result, we instantiate it
using a random spherical code to get the desired performance.

k2 ≤

⊥

D. Description of the successive update scheme

All the conceptual components of our scheme are ready. We use the structure of Lemma 1 and focus only on updating the
estimates of the latest observed sample Xks at the decoder. Our encoder successively updates the estimate of the latest sample
at the decoder by quantizing and sending estimates for errors Yt.

As discussed earlier, we must decide if we prefer a fast but loose approach or a slow but accurate approach for sending
error estimates. To carefully examine this tradeoff, we opt for a more general scheme where the nRs bits available between
two samples are divided into m = s/p sub-fragments of length nRp bits each. We use an nRp bit quantizer to reﬁne error
estimates for the latest sample Xks (obtained at time t = ks) every p slots, and send the resulting quantizer codewords as

partial tracking codewords (Cks,jp, ..., Cks,(j+1)p−1), 0 6 j
m given by
is divided into m sub-fragments Ik,j , 1

j

≤

≤

m

−

≤

1. Speciﬁcally, the kth codeword transmission interval Ik

6

and (Cks,jp, ..., Cks,(j+1)p−1) is transmitted in communication slots in Ik,j .

Ik,j ,

{

ks + jp, . . . , ks + (j + 1)p

1

}

−

, 0

≤

j

≤

m

−

1,

At time instant t = ks + jp + 1 the decoder receives the jth sub-fragment (Cks,t−ks, t

Ik,j ) of nRp bits, and uses it to
reﬁne the estimate of the latest source sample Xks. Note that the fast but loose and the slow but accurate regimes described
above correspond to p = 1 and p = s, respectively. In the middle of the interval Ik,j , the decoder ignores the partially received
quantization code and retains the estimate ˆXks of Xks formed at time ks + (j
1)p + 1. It forms an estimate of the current
−
state Xks+i by simply scaling ˆXks by a factor of αi, as suggested by Lemma 1.

∈

Finally, we impose one more additional simpliﬁcation on the decoder structure. Instead of using MMSE estimates for the
latest sample, we simply update the estimate by adding to it the quantized value of the error. Thus, the decoder has a simple
linear structure.

We can use any nRp bit quantizer2 Qp for the n-dimensional error vector, whereby this scheme can be easily implemented
in practice if Qp can be implemented. For instance, we can use any standard gain-shape quantizer. The performance of most
quantizers can be analyzed explicitly to render them a (θ, ε)-quantizer family for an appropriate M and function θ. Later,
when analyzing the scheme, we will consider a Qp coming from a (θ, ε)-quantizer family and present a theoretically sound
guideline for choosing p.

Recall that we denote the estimate of Xu formed at the decoder at time t > u by ˆXu|t. We start by initializing ˆX0|0 = 0
and then proceed using the encoder and the decoder algorithms outlined above. Note that our quantizer Qp may declare failure
, in which case the decoder must still yield a nominal estimate. We will simply declare the estimate as3 0 once a
symbol
⊥
failure happens.

We give a formal description of our encoder and decoder algorithms below.

The encoder.

1 Initialize k = 0, j = 0, ˆX0|0 = 0.
2 At time t = ks + jp, use the decoder algorithm (to be described below) to form the estimate ˆXks|t and compute the

error

Yk,j , Xks −

ˆXks|t,

(3)

where we use the latest sample Xks available at time t = ks + jp.

3 Quantize Yk,j to nRp bit as Qp(Yk,j ).
4 If quantize failure occurs and Qp(Yk,j ) =
5 Else, send a binary representation of Qp(Yk,j ) as the communication (Cks,0, ..., Cks,p−1) to the receiver over the next p

to the receiver and terminate the encoder.

, send

⊥

⊥

communication slots4.

6 If j < m

The decoder.

−

1, increase j by 1; else set j = 0 and increase k by 1. Go to Step 2.

1 Initialize k = 0, j = 0, ˆX0|0 = 0.
2 At time t = ks + jp, if encoding failure has not occurred until time t, compute
ˆXks|ks+jp = ˆXks|ks+(j−1)p + Qp(Yk,j−1),

and output ˆXt|t = αt−ks ˆXks|t.

3 Else, if encoding failure has occurred and the

4 At time t = ks + jp + i, for i
5 If j < m

[p

∈

−

1, increase j by 1; else set j = 0 and increase k by 1. Go to Step 2.

1], output5 ˆXt|t = αt−ks ˆXks|ks+jp.

s > t.

−

symbol is received declare ˆXs|t = 0 for all subsequent time instants

⊥

IV. MAIN RESULTS

We present results in two categories. First, we provide an explicit formula for the asymptotic maxmin tracking accuracy
δ∗(R, s, X). Next, we present a theoretically-founded guideline for selecting a good p for the successive update scheme with
a (θ, ε)-quantizer family. Interestingly, the optimal choice may differ from the asymptotically optimal choice of p = 1.

2With an abuse of notation, we will use Qp instead of QRp to denote an nRp bit quantizer.
3In analysis, we account for all these events as error. Only the probability of failure will determine the contribution of this part to the MSE since the process

is mean-square bounded.

4For simplicity, we do not account for the extra message symbol needed for sending ⊥.
5We ignore the partial quantizer codewords received as (Cks,jp+1, Cks,jp+2, . . . , Cks,jp+i−1) till time t.

7

(4)

A. Characterization of the maxmin tracking accuracy

To describe our result, we deﬁne a functions δ0 : R+ →
α2(1
(1
(1
s(1

δ0(R) ,

[0, 1] and g : R+ →
2−2R)
−
α22−2R)
α2s)
α2)

, for all s > 0.

g(s) ,

, for all R > 0;

[0, 1] as

−
−
−

Note that g(s) is a decreasing function of s with g(1) = 1. The result below shows that, for an appropriate choice of
the quantizer, our successive update scheme with p = 1 (the fast but loose version) achieves an accuracy of δ0(R)g(s)
asymptotically, universally for all processes in X.

Theorem 2 (Lower bound for maxmin tracking accuracy: The achievability). For R > 0 and s
tacking accuracy is bounded below as

∈

N, the asymptotic maxmin

δ∗(R, s, X)

δ0(R)g(s).

≥
Furthermore, this bound can be obtained by a successive update scheme with p = 1 and appropriately chosen quantizer Qp.
We provide a proof in Section VI. Note that while we assume that the per dimension fourth moment of the processes in X
is bounded, the asymptotic result above does not depend on that bound. Interestingly, the performance characterized above is
the best possible.

Theorem 3 (Upper bound for maxmin tracking accuracy: The converse). For R > 0 and s
tacking accuracy is bounded above as

∈

N, the asymptotic maxmin

Furthermore, the upper bound is obtained by considering a Gauss-Markov process.

δ∗(R, s, X)

δ0(R)g(s).

≤

We provide a proof in Section VII. Thus, δ∗(R, s, X) = δ0(R)g(s) with the fast but loose successive update scheme being
universally (asymptotically) optimal and the Gauss-Markov process being the most difﬁcult process to track. Clearly, the best
possible choice of sampling period is s = 1 and the highest possible accuracy at rate R is δ0(R), whereby we cannot hope
for an accuracy exceeding δ0(R).

Alternatively, the results above can be interpreted as saying that we cannot subsample at a frequency less than 1/

for attaining a tracking accuracy δ 6 δ0(R).

B. Guidelines for choosing a good p

⌊

g−1(δ/δ0(R))
⌋

∈

The proof of Theorem 2 entails the analysis of the successive update scheme for p = 1. In fact, we can analyze this scheme
N and for any (θ, ε)-quantizer family; we term this tracking code the p-successive update (p-SU) scheme. This

for any p
analysis can provide a simple guideline for the optimal choice of p depending on the performance of the quantizer.
However, there are some technical caveats. A quantizer family will operate only as long as the input y satisﬁes
y
k

M .
and the tracking code encoder, in turn, will

If a y outside this range is observed is observed, the quantizer will declare
declare a failure. We denote by τ the stopping time at which encoder failure occurs for the ﬁrst time, i.e.,

k2 ≤

⊥

Further, denote by At the event that failure does not occur until time t, i.e.,

τ , min

{

ks + jp : Qp(Yk,j ) =

, 0

⊥

≤

k, 0 6 j 6 m

1

.

}

−

We characterize the performance of a p-SU in terms of the probability of encoder failure in a ﬁnite time horizon T .

At ,

τ > t

.

}

{

Theorem 4 (Performance of p-SU). For ﬁxed θ, ε, β
and denote the corresponding tracking code by (φp, ψp). Suppose that for a time horizon T
satisﬁes P (τ

[0, 1], consider the p-SU scheme with an nRp bit (θ, ε)-quantizer Qp,
N, the tracking code (φp, ψp)

β. Then,

T )

∈

∈

≤

≤

T

sup
X∈Xn

D

(φp, ψp, X)

≤

BT (θ, ε, β),

where BT (θ, ε, β) satisﬁes

lim sup
T →∞

BT (θ, ε, β)

σ2

1
h

−

≤

g(s) α2p
1

α2p θ (cid:16)

ε2
σ2 −

θ

1

−

+

(cid:17)i

−

κβg(s)

1
α2s) (cid:16)

−

(1

−

α2(s+p) (1
1
−

θ)
.
−
α2pθ (cid:17)

We remark that β can be made small by choosing M to be large for a quantizer family. Furthermore, the inequality in the
upper bound for the MSE in the previous result (barring the dependence on β) comes from the inequality in the deﬁnition of a

(θ, ε)-quantizer, rendering it a good proxy for the performance of the quantizer. The interesting regime is that of very small β
where the encoder failure doesn’t occur during the time horizon of operation. If we ignore the dependence on β, the accuracy
of the p-SU does not depend either on s or on the bound for the fourth moment κ. Motivated by these insights, we deﬁne the
accuracy-speed curve of a quantizer family as follows.

8

Deﬁnition 3 (The accuracy-speed curve). For α
Q is given by

[0, 1], σ2, and R > 0, the accuracy-speed curve for a (θ, ε)-quantizer family

∈

ΓQ(p) =

α2p
1
α2p θ(Rp) (cid:18)

ε2
σ2 −

θ(Rp)

,

p > 0.

1

−

when ΓQ(p) is larger. Thus, a good choice of p for a given quantizer family Q is the one that maximizes ΓQ(p) for 1

−
By Theorem 4, it is easy to see that the accuracy (precisely the upper bound on the accuracy) of a p-SU scheme is better
s.
We conclude by providing accuracy-speed curves for some illustrative examples. To build some heuristics, note that a uniform
M, M ] has θ(R) = 0 and ε = M 2−R. For a gain-shape quantizer, we express a vector y =
k2ys where
ysk2 = 1. An ideal shape quantizer (which only can be shown to exist asymptotically) using R bits
2−2R, similar to the scalar uniform quantizer. In one of the examples below, we

quantization of [
the shape vector ys has
per dimension will satisfy E
ˆys −
k
consider gain-shape quantizers with such an ideal shape quantizer.

2
2 ≤

ysk

y
k

≤

≤

−

p

k

(cid:19)

Example 1. We begin by considering an ideal quantizer family with θ(R) = 2−2R and ε = 0. In our asymptotic analysis, we
will show roughly that such a quantizer with very small ε exists. For this ideal case, for R > 0, the accuracy-speed curve is
given by

ΓQ(p) =

α2p
1

α2p θ(Rp)
−
α2p θ(Rp)

= 1

−

1

−

1

α2p
α2p2−Rp .
−

−

It can be seen that ΓQ(p) is decreasing in p whereby the optimal choice of p that maximized ΓQ(p) over p
Heuristically, this justiﬁes why asymptotically the fast but loose successive update scheme is optimal.

[s] is p = 1.

∈

Example 2 (Uniform scalar quantization). In this example, we consider a coordinate-wise uniform quantizer. Since we seek
quantizers for inputs y
M √n, M √n] for each
coordinate. For this quantizer, we have θ = 0 and ε2 = nM 22−2R, whereby the accuracy-speed curve is given by ΓQ(p) =
α2p(1

−
nM 22−2R/σ2). Thus, once again, the optimal choice of p that maximizes accuracy is p = 1.

M √n, we can only use uniform quantizer of [

Rn such that

k2 ≤

y
k

∈

−

Example 3 (Gain-shape quantizer). Consider the quantization of a vector y = ays where a =
k2. The vector y is quantized
by a gain-shape quantizer which quantizes the norm and shape of the vector separately to give Q(y) = ˆaˆys. We use a uniform
quantizer within a ﬁxed range [0, M √n] in order to quantize the norm a to ˆa, where an ideal shape quantizer is employed in
quantizing the shape vector ys. Namely, we assume E
6 1. Suppose, that we allot ℓ bits out of
ˆysk
k
the total budget of nR bits for norm quantization and the rest for shape quantization. Then, we see that
2
2 6 2a22−2(R−ℓ/n) + nM 22−2ℓ−1,
Q(y)
k

2
2 6 2−2R and

ys −

ˆysk

E
k

−

k

k

y

y

whereby θ(R) = 2−2(R−ℓ/n)+1 and ǫ2 = M 22−2ℓ−1. Thus, the accuracy-speed curve is given by
α2p

ΓQ(p) =

1

2α2p2−2(Rp−ℓ/n)

2M 22−2ℓ−1
σ2

−

1

(cid:16)

−

2−2(Rp−ℓ/n)+1

.
(cid:17)

−
Note that the optimal choice of p in this case depends on the choice of M .

We illustrated application of our analysis for idealized quantizers, but it can be used to analyze even very practical quantizers,

such as the recently proposed almost optimal quantizer in [46].

V. ANALYSIS OF THE SUCCESSIVE UPDATE SCHEME
From the discussion in section III, we observe that the successive update scheme is designed to reﬁne the estimate of Xks
˜Ik in terms of Dks(φp, ψp, X)

in each interval ˜Ik. This fact helps us in establishing a recursive relation for Dt(φp, ψp, X), t
which is provided next.

∈

Lemma 5. For a time instant t = ks + jp + i, 0
of a p-SU scheme employing an nRp bit (θ, ǫ)-quantizer. Assume that P (Ac

1, 0

m

≤

≤

−

−

≤

≤

p

j

i

Dt(φp, ψp, X) 6 α2(t−ks)θj Dks(φp, ψp, X) + σ2(1

−

α2(t−ks)) +

1 and k > 0, let (φp, ψp) denote the tracking code
t ) 6 β2. Then, we have
θj)ε2

α2(t−ks)(1

+ α2(t−ks)κβ.

−
θ)

(1

−

Proof: From the evolution of the AR[1] process deﬁned in (1), we see that Xt = αt−ksXks +

for the p-SU scheme, we know that ˆXt|t = αt−ks ˆXks|ks+jp at each instant t = ks + jp + i. Therefore, we have

P

t
u=ks+1 αt−uξu. Further

Xt −

ˆXt|t = αt−ks(Xks −

ˆXks|ks+jp) +

t

Xu=ks+1

αt−uξu.

9

Since the estimate ˆXks|ks+jp is a function of samples (X0, . . . , Xks), and the sequence (ξu, u > ks) is independent of the
past, we obtain the per dimension MSE as

Dt(φp, ψp, X) =

α2(t−ks)
n

E
k

Xks −

ˆXks|ks+jpk

2

2 + σ2(1

−

α2(t−ks)).

Further, we divide the error into two terms based on occurrence of the failure event as follows:

Dt(φp, ψp, X) =

α2(t−ks)
n

E[
h

Xks −

k

ˆXks|ks+jpk

2

21At ] + E[

Xks −

k

ˆXks|ks+jpk

2
t ]
21Ac

+ σ2(1

−

i

α2(t−ks)).

(5)

Recall that at each instant t = ks + jp, we reﬁne the estimate ˆXks|ks+(j−1)p of Xks to ˆXks|ks+jp = ( ˆXks|ks+(j−1)p +
Qp(Yk,j−1))1At . Upon substituting this expression for ˆXks|ks+jp, we obtain

Xks −

k

E

(cid:2)

ˆXks|ks+jpk

2

21At ] = E[

Yk,j−1 −

k

Qp(Yk,j−1)
k

2

1At

(cid:3)

k
where the identity uses the deﬁnition of error Yk,j−1 given in (3) and the inequality holds since Qp is a (θ, ε)-quantizer.
Repeating the previous step recursively, we get

6 θE[

Xks −

ˆXks|ks+(j−1)pk

2

21At ] + nε2,

E[

1
n

Xks −

k

ˆXks|ks+jpk

2

21At] 6 θj

which is the same as

6 θj

1
n
1
n

·

·

2
21At] +

E[

k

Xks −

E
k

Xks −

ˆXks|ksk
ˆXks|ksk

2
2 +

1
1

−
−

ε2

θj
θ ·

1
1
θj
θ ·

−
−
ε2,

Moving to the error term E[
0 in the event of an encoder failure. Thus, using the Cauchy-Schwartz inequality, we get

k

E[

1
n
k
Xks −

Xks −
ˆXks|ks+jpk

ˆXks|ks+jpk
2
21Ac

2

21At] 6 θj

Dks(φp, ψp, X) +

−
−
t ] when encoder failure occurs, recall that the decoder sets the estimate to

ε2.

·

1
1

θj
θ ·

1
n

E[

Xks −

k

ˆXks|ks+jpk

2
21Ac

t ] =

E[

1
n
1
n r
6 κβ.

6

2

t ]
1Ac

k

Xksk
E

Xksk

P (Ac
t )

4

i

hk

Substituting the two bounds above in (5), we get the result.

The following recursive bound can be obtained using almost the same proof as that of Lemma 5; we omit the details.

Lemma 6. Let (φp, ψp) denote the tracking code of a p-SU scheme employing an nRp bit (θ, ǫ)-quantizer. Assume that
P (Ac

t ) 6 β2. Then, we have

Dks(φp, ψp, Xn) 6 α2sθmD(k−1)s(φp, ψp, Xn) + σ2(1

−

We also need the following technical observation.

α2s) + α2sε2 (1
(1

θm)
θ)

−
−

+ α2sκβ.

Lemma 7. For a sequence (Xk ∈

R : k

∈

Z+) that satisﬁes sequence of upper bounds

with constants a, b

∈

R such that b is ﬁnite and a

(
−

∈

Xk 6 aXk−1 + b,

∀
1, 1), we have

Z+,

k

∈

lim
K→∞

1
K

K−1

Xk=0

Xk 6

b

−

1

.

a

Xk 6 akX0 + b

ak
a

1
−
1
−

,

Z+.

k

∀

∈

Proof: From the sequence of upper bounds, we can inductively show that

Averaging Xk over the horizon

{

0, . . . , K

1

, we get
}

−
K−1

1
K

Xk 6

Xk=0

1
−
K(1

aK

a) (cid:16)

−

X0 −

1

b

−

+

1

a (cid:17)

b

−

.

a

From the ﬁniteness of X0, b and the fact that
both sides.

a

|

|

< 1, the result follows by taking the limit K growing arbitrarily large on

10

We are now in a position to prove Theorem 4.
Proof of Theorem 4: We begin by noting that, without any loss of generality, we can restrict to T = Ks. This holds since
can
[K]). Therefore, we can write the average MSE per dimension for the p-SU scheme

the contributions of the error term within the ﬁxed interval IK are bounded. For T = Ks, the time duration
be partitioned into intervals (Ik, k + 1
for time-horizon T = Ks as

0, . . . , T
{

∈

}

DT (φp, ψp, X) =

1
Ks

K−1

m−1

p−1

Dks+jp+i(φp, ψp, X).

Xk=0

Xj=0

Xi=0

From the upper bound for per dimension MSE given in Lemma 5, we get

p−1

Xi=0

Dks+jp+i(φp, ψp, X)

α2(jp+i)θjDks(φp, ψp, X) + σ2(1

α2(jp+i)) +

θjDks(φp, ψp, X) +

−
θj)ε2
θ)

(1

−
(1

+ κβ

σ2

−

(cid:17)

(1

−
+ pσ2.

α2(jp+i)(1

θj)ε2

+ α2(jp+i)κβ

i

−
θ)

p−1

≤
Xi=0 h
= α2jp 1
1

α2p
α2

−
−

(cid:16)
0, ..., m

∈ {

(1
s(1

−
−

α2s)
α2) (cid:16)

κβ

σ2 +

−

1

−

θ (cid:17)

and k

−

1

}
ε2

−
0, ..., K

−

∈ {
(1
s(1

−
−

α2p)
α2)

(1
(1

−
−

}
α2sθm)
α2pθ) (cid:16)

Summing the expression above over j

1

, and dividing by T , we get

+

+

1
K

K−1

Xk=0

Dks(φp, ψp, X)

ε2

−

.

θ (cid:17)

−

1

sup
{ak}k≥0∈A

1
K

K−1

Xk=0

ak −

1

ε2

−

,

θ (cid:17)

(6)

DT (φp, ψp, X) 6 σ2 +

It follows by Lemma 6 that

DT (φp, ψp, X) 6 σ2 +

where the

A

denotes the set of

(1
s(1

1

−

ε2

κβ

σ2 +

α2s)
−
α2) (cid:16)
−
ak}k≥0 satisfying
{
ak 6 α2sθmak−1 + σ2(1

−

θ (cid:17)

(1
s(1

−
−

α2p)
α2)

(1
(1

α2sθm)
α2pθ) (cid:16)

−
−

α2s) + α2sκβ + α2sε2 (1
(1

−

lim
K→∞

1
K

K−1

Xk=0

ak 6

1
α2sθm

(cid:16)

σ2(1

−

1

−

{
α2s) + α2sκβ + α2sε2 (1
(1

−
−

.

θm)
θ)
ak}k≥0 ∈ A
θm)
,
θ) (cid:17)

−
−

We denote the right-side of (6) by BT (θ, ǫ, β). Noting that by Lemma 7 any sequence

satisﬁes

we get that

lim sup
T →∞

BT (θ, ǫ, β)

σ2

1
(cid:18)

−

≤

which completes the proof.

α2s)
α2) ·

(1
s(1

−
−

α2p(1
(1

θ)
−
α2pθ) (cid:19)

−

+ ε2 (1
s(1

α2s)
α2) ·

−
−

α2p

α2pθ)

(1

−

+ κβ

s(1

1

−

1
α2) (cid:18)

−

α2(s+p)(1

θ)

(1

−

,

−
α2pθ) (cid:19)
(cid:4)

VI. ASYMPTOTIC ACHIEVABILITY USING RANDOM QUANTIZER

With Theorem 4 at our disposal, the proof of achievability can be completed by ﬁxing p = 1 and showing the existence of
appropriate quantizer. However, we need to handle the failure event, and we address this ﬁrst. The next result shows that the
failure probability depends on the quantizer only through M .

Lemma 8. For ﬁxed T and n, consider the p-SU scheme with p = 1 and an nR bit (θ, ǫ)-quantizer Q with dynamic range
M . Then, for every η > 0, there exists an M0 independent of n such that for all M

M0, we get

P (Ac

T ) 6 η.

≥

Proof: The event AT (of encoder failure not happening until time T for the successive update scheme) occurs when the
T . For brevity, we denote by

1 such that t = ks + j

nM 2, for every k

errors Yk,j satisﬁes
Yt the error random variable Yk,j and Y t−1 = (Y1, ..., Yt−1). We note that

Yk,j k

0 and 0

2
2 ≤

−

≤

≤

≤

≥

k

s

j

11

P (Ac

T ) = P
= P

= P

P

≤

2

+ P (AT −1 ∩
+ P

Ac
T )
YT k
AT −1 ∩ {k
(cid:16)
P
YT k
1AT −1
(cid:16)k
2
E[
YT k
2|
k
nM 2

+ E
(cid:2)
+ E
1AT −1
h
E[

2
21AT −1]

2

.

T −1

T −1

T −1

T −1

(cid:1)

(cid:1)

(cid:1)

(cid:1)

Ac
(cid:0)
Ac
(cid:0)
Ac
(cid:0)
Ac
(cid:0)
Ac
(cid:0)

= P

YT k
nM 2
Q(YT −1) under AT −1, whereby

T −1

+

k

(cid:1)

2 > nM 2
2 > nM 2
Y T −1]

|

}(cid:17)
Y T −1

(cid:17) (cid:3)

i

Note that we have YT = YT −1 −

Denoting by β2

T the probability P (Ac

E[

YT k

2
21AT −1]

k

≤
T ), the previous two inequalities imply

k

·

YT −1k

θ

E[

2

21AT −1 ] + nε2.

β2
T ≤
We saw earlier in the proof of Lemma 5 that E[
until time T

1. Proceeding as in that proof, we get

k

YT −1k

β2
T −1 +

−

E[

θ
nM 2
2
2]/n depends only on the probability β2

YT −1k

2
2] +

ε2
M 2 .

k

T −1 that failure doesn’t occur

β2
T ≤

β2
T −1 +

1
M 2 (c1βT −1 + c2),

where c1 and c2 do not depend on n. Therefore, there exists M0 independent of n such that for all M exceeding M0 we have

β2
T ≤

β2
T −1 + η,

which completes the proof by summing over T .

The bound above is rather loose, but it sufﬁces for our purpose. In particular, it says that we can choose M sufﬁciently large
to make probability of failure until time T less than any β2, whereby Theorem 4 can be applied by designing a quantizer for
this M . Indeed, we can use the quantizer of unit sphere from [1], [2], along with a uniform quantizer for gain (which lies in
M, M ]) to get the following performance. In fact, we will show that a deterministic quantizer with the desired performance
[
−
exists. Note that we already considered such a quantizer in Example 3. But the analysis there was slightly loose, and assumed
the existence of an ideal shape quantizer.

Lemma 9. For every R, ε, γ, M > 0, there exists an nR bit (2−2(R−γ), ε)-quantizer with dynamic range M , for all n
sufﬁciently large.

Proof: We ﬁrst borrow a classic construction from [1], [2], which gives us our desired shape quantizer. Denote by Sn the
. For every γ > 0 and n sufﬁciently large, it was shown in [1], [2] that
1)-dimensional unit sphere

y

(n
there exist 2nR vectors

−

Denoting cos θ = √1

−

{

∈

Rn :
k2 = 1
}
in Sn such that for every y
∈
y, y′

y
k

C

Sn we can ﬁnd y′

satisfying

∈ C

h

>

i

1
p

2−2(R−γ).

−

2−2(R−γ), consider the shape quantizer QR(y) from [2] given by

QR (y) , cos θ

= cos θ

·

·

arg min
y′∈C k
arg max
y′∈C h

y

−
y, y′

y′

2
2
k

,
i

Sn.

y

∀

∈

We append to this shape quantizer the uniform gain quantizer qM : [0, M ]

Note that we shrink the length of y′ by a factor of cos θ, which will be seen to yield the gain over the analysis in Example 3.
[0, M ], which quantizes the interval [0, M ]
. We
ℓ.

→
bit binary representation and denote this mapping by φM : [0, M ]

uniformly into sub-intervals of length ǫ. Speciﬁcally, qM (a) = ε
represent this index using its ℓ ,
y
k

⌋
nM 2, we consider the quantizer

and the corresponding index is given by

log(M/ε)
⌈
⌉
2
2 ≤
k

Rn such that

For every y

⌊
→ {

⌋
0, 1

a/ε

a/ε

∈

}

⌊

Q(y) = √n

qM (cid:18)

·

y
k2
k
√n (cid:19) ·

QR (cid:18)

y
y
k2 (cid:19)
k

.

For this quantizer, for every y

∈

Rn with

M , we have

12

y

k

−

y

2
2 = nB2 such that B
y
k
k
2
2 =
Q(y)
k

2
2 +
k

2
Q(y)
2 −
k
k
= nB2 + n ˆB2 cos2 θ
nB2 + n ˆB2 cos2 θ
≤
= nB2 sin2 θ + n(B

k

≤
2

−

−

−

y, Q(y)
h
i
2nB ˆB cos θ
h
2nB ˆB cos2 θ
ˆB)2 cos2 θ

nB2 sin2 θ + nε2 cos2 θ
nB22−2(R−γ) + nε2,

≤

≤

˜y, QR(˜y)
i

where the ﬁrst inequality uses the covering property of
dynamic range M , for all n sufﬁciently large. Note that this quantizer is a deterministic one.

C

. Therefore, Q constitutes an nR + ℓ bit 2−2(R−γ),ε-quantizer with

Proof of Theorem 2: For any ﬁxed β and ε, we can make the probability of failure until time T less than β by choosing
M sufﬁciently large. Further, for any ﬁxed R, γ > 0, by Lemma 9, we can choose n sufﬁciently large to get an nR bit
(2−2(R−γ), ε)-quantizer for vectors y with
g(s) α2
α22−2(R−γ)

nM 2. Therefore, by Theorem 4 applied for p = 1, we get that

δ∗(R, s, X)

ε2
σ2 −

2−2(R−γ)

2
2 ≤

κβg(s)

σ2(1

y
k

−

≥

−

k

1

.

1
α2s) (cid:16)

−

α2(s+1) 1
1
−

θ
−
α2θ (cid:17)

1
(cid:16)

(cid:17) −

−

The proof is completed upon taking the limits as ε, γ, and β go to 0.

(cid:4)

VII. CONVERSE BOUND : PROOF OF THEOREM 3

The proof is similar to the converse proof in [32], but now we need to handle the delay per transmission. We rely on the
properties of entropy power of a random variable. Recall that for a continuous random variable X taking values in Rn, the
entropy power of X is given by

(X) =

N

1
2πe ·

2(2/n) h(X),

where h(X) is the differential entropy of X.

Consider a tracking code (φ, ψ) of rate R and sampling period s and a process X

at time t is related to the state at time t + i as

Xn. We begin by noting that the state

∈

where the noise

i−1
j=0 αj ξt+i−j is independent of Xt (and the past states). In particular, for t = ks + i, 1 6 i < s, we get

Xt+i = αiXt +

i−1

Xj=0

αjξt+i−j ,

P

E

hk

Xt −

ˆXt|tk

2
2

i

+

2
2

i

i−1

Xj=0

α2j E

2
2

ξt−jk

i

hk

αiXks −

= E

hk
= α2iE

ˆXt|tk
˜Xtk

2
2

Xks −
where we deﬁne ˜Xt := α−i ˆXt|t and the ﬁrst identity uses the orthogonality of noise added in each round from the previous
states and noise. Since the Gaussian distribution has the maximum differential entropy among all continuous random variables
with a given variance, and the entropy power for a Gaussian random variable equals its variance, we get that

+ n(1

hk

−

i

α2i)σ2.

Therefore, the previous bound for tracking error yields

σ2(1

−

α2) >

N

(ξt+i).

Dks+i(φ, ψ, X) > α2i 1
n
= α2i 1
n

E

E

Xks −

Xks −

hk

hk

˜Xks+ik
˜Xks+ik

2
2

2
2

i

i

+

+

(1
(1
(1
(1

α2i)
α2) N
α2i)
α2) N

(ξks+i)

(ξ1),

(7)

−
−
−
−

where the identity uses the assumption that ξt are identically distributed for all t. Taking average of these terms for t = 0, .., T ,
we get

DT (φ, ψ, X) =

1
nKs

>

1
nKs

K−1

(k+1)s−1

Xk=0
K−1

Xi=ks

s−1

E

hk

Xi −

ˆXi|ik

2
2

i

α2iE

Xks −

˜Xks+ik

2
2

i

hk

Xk=0

Xi=0

(ξ1)

+ N
(1

1
α2) (cid:18)

−

−

α2s)
α2) (cid:19)

.

(1
s(1

−
−

Note that ˜Xks+is act as estimates of Xks which depend on the communication received by the decoder until time ks + i.
We denote the communication received at time t by Ct−1, whereby ˜Xks+i depends only on C1, ..., Cks+i−1. In particular, the
communication Cks, ..., Cks+i−1 was sent as a function of Xks, the sample seen at time t = ks.

13

From here on, we proceed by invoking the “entropy power bounds” for the MSE terms. For random variables X and Y such
Y ) = 1/(2πe)22h(X|Y )/n.6 Bounding MSE
that PX|Y has a conditional density, the conditional entropy power is given by
terms by entropy power is a standard step that allows us to track reduction in error due to a ﬁxed amount of communication.
We begin by using the following standard bound (see [47, Chapter 10]):7 For a continuous random variable X and a discrete

(X

N

|

random variable Y taking

0, 1

}

{

nR values, let ˆX be any function of Y . Then, it holds that

1
n

X

E
k

ˆX

k

−

2 > 2−2R

(X).

N

(8)

We apply this result to Xks given Cks−1 in the role of X and the communication Cks, .., Cks+i−1 in the role of Y . The
previous bound and Jensen’s inequality yield

E

1
n

Xks −

hk

˜Xks+ik

2
2

i

> 2−2RiE[

(Xks|

N

Cks−1)].

Next, we recall the entropy power inequality (cf. [47]): For independent X1 and X2,
s−1
Noting that Xks = αsX(k−1)s +
j=0 αjξks−j , where
that Cks−1 is a function of X1, ..., X(k−1)s, we get

(X2).
is an iid zero-mean random variable independent of X(k−1)s, and

(X1) +

(X1 + X2) >

ξi}

P

N

N

N

{

(Xks|

N

Cks−1) >

(αsX(k−1)s|

N

Cks−1) +

(ξks)

N

α2s)
α2)
α2s)
α2)

(1
(1
(1
(1

−
−
−
−

(X(k−1)s|
where the previous identities utilizes the scaling property of differential entropy. Upon combining the bounds given above and
simplifying, we get

(ξ1)

N

N

,

Cks−1) +

= α2s

DT (φ, ψ, X) >

α2s(1
s(1

−
−

α2s2−2Rs)
α22−2R)

1
K

·

K−1

Xk=0
(ξ1)

E[

N

(X(k−1)s|
(1

1 +

−

Cks−1)]

+ N
(1

−

α2) (cid:18)

α2s)(1
s(1

−

α2s2−2Rs)

−
α22−2R)

α2s)
α2) (cid:19)

(1
s(1

−
−

−

.

(9)

Cks−1) are exactly the same as that considered in [32, eqn. 11e] since they correspond
Finally, note that the terms
to recovering X(k−1)s using communication that can depend on it. Therefore, a similar expression holds here, for the sampled
N
process
. Using the recursive bound for the tracking error in (7) and (8), we adapt the results of [32, eqn. 11]
}
for our case to obtain

(X(k−1)s|

Xks : k

N

∈

{

where the quantity d∗

k is given by the recursion

E[

(X(k−1)s|

N

Cks−1)] > d∗

k−1,

k = 2−2Rs
d∗

α2sd∗

k−1 +

(ξ1)

N

(cid:16)

(1
(1

−
−

α2s)
α2) (cid:17)

,

with d∗

0 = 0.

The bound obtain above holds for any given process X

Gaussian random variable, since that would maximize
zero mean and variance σ2 to get
have

(ξ) = σ2(1

N

−

Xn. To obtain the best possible bound we substitute ξ1 to be a
to be a Gaussian random variable with
α2). Thus, taking supremum over all distributions on both sides of (9), we

(ξ1). Speciﬁcally, we set

ξk}

N

∈

{

DT (φ, ψ, X) >

sup
X∈Xn

α2s(1
s(1

−
−

α2s2−2Rs)
α22−2R)

1
K

·

K−1

Xk=0

k−1 + σ2
d∗

1 +

(cid:18)

(1

−

α2s)(1
s(1

−

α2s2−2Rs)

−
α22−2R)

α2s)
α2) (cid:19)

,

(1
s(1

−
−

−

where

k = 2−2Rs
d∗

k−1 + σ2(1

α2sd∗
(cid:0)

−

α2s)
,
(cid:1)

6The conditional differential entropy h(X|Y ) is given by E (cid:2)h(PX|Y )(cid:3).
7It follows simply by noting that Gaussian maximizes differential entropy among all random variables with a given second moment and that h(X) −

h(X|Y ) 6 H(Y ) = nR.

with d∗

0 = 0. For this sequence d∗

k, we can see that (cf. [32, Corollary 1])

lim sup
K→∞

1
K

K−1

Xk=0

d∗
k−1 = lim
K→∞

d∗
k =

σ2(1
(1

α2s)2−2Rs
−
α2s2−2Rs)

−

.

Therefore, we have obtained

lim sup
T →∞

sup
X∈Xn

DT (φ, ψ, X) > σ2

(cid:18)

(1

α2s)α2s2−2Rs
α22−2R)

−
s(1

−

+ 1

−

α2s)
α2)

(1
s(1

−
−

(1

−

+

α2s)(1
s(1

−

α2s2−2Rs)

−
α22−2R)

14

(cid:19)

−
As the previous bound holds for all tracking codes (φ, ψ), it follows that δ∗(R, s, X) 6 g(s)δ0(R).

(cid:19)

= σ2

1
(cid:18)

g(s)δ0(R)

.

VIII. DISCUSSION

We restricted our treatment to an AR[1] process with uncorrelated components. This restriction is for clarity of presentation,
and some of the results can be extended to AR[1] processes with correlated components. In this case, the decoder will be
replaced by a Kalman-like ﬁlter in the manner of [27]. A natural extension of this work is the study of an optimum transmission
strategy for an AR[n] process in the given setting. In an AR[n] process, the strategy of reﬁning the latest sample is clearly
not sufﬁcient as the value of the process at any time instant is dependent on the past n samples. If the sampling is periodic,
even the encoder does not have access to all these n samples unless we take a sample at every instant. A viable alternative is
to take n consecutive samples at every sampling instant. However, even with this structure on the sampling policy, it is not
clear how must the information be transmitted. A systematic analysis of this problem is an interesting area of future research.
Another setting which is not discussed in the current work is where the transmissions are of nonuniform rates. Throughout
our work, we have assumed periodic sampling and transmissions at a ﬁxed rate. For the scheme presented in this paper, it
is easy to see from our analysis that only the total number of bits transmitted in each sampling interval matters, when the
dimension is sufﬁcient large. That is, for our scheme, even framing each packet (sent in each communication slot) using unequal
number of bits will give the same performance as that for equal packet size, if the overall bit-budget per sampling period is
ﬁxed. A similar phenomenon was observed in [31], which allowed the extension of some of their analysis to erasure channels
with feedback. We remark that a similar extension is possible for some of our results, too. This behavior stems from the use
of successive batches of bits to successively reﬁne the estimate of a single sample within any sampling interval, whereby at
the end of the sampling interval the error corresponds to roughly that for a quantizer using the total number of bits sent during
the interval. In general, a study of nonuniform rates for describing each sample, while keeping bits per time-slot ﬁxed, will
require us to move beyond uniform sampling. This, too, is an interesting research direction to pursue.

Finally, we remark that the encoder structure we have imposed, wherein the error in the estimate of the latest sample is
reﬁned at each instant, is optimal only asymptotically and is justiﬁed only heuristically for ﬁxed dimensions. Even for one
dimensional observation it is not clear if this structure is optimal. We believe that this is a question of fundamental interest
which remains open.

The authors would like to thank Shun Watanabe for pointing to the reference [2].

ACKNOWLEDGEMENTS

REFERENCES

[1] A. D. Wyner, “Random packings and coverings of the unit n-sphere,” The Bell System Technical Journal, vol. 46, no. 9, pp. 2111–2118, 1967.
[2] A. Lapidoth, “On the role of mismatch in rate distortion theory,” IEEE Trans. Inf. Theory, vol. 43, no. 1, pp. 38–47, 1997.
[3] H. S. Witsenhausen, “On the structure of real-time source coders,” Bell System Technical Journal, vol. 58, no. 6, pp. 1437–1451, 1979.
[4] D. Teneketzis, “On the structure of optimal real-time encoders and decoders in noisy communication,” IEEE Trans. Inf. Theory, vol. 52, pp. 4017–4035,

2006.

[5] A. Mahajan and D. Teneketzis, “On real-time communication systems with noisy feedback,” in IEEE Inf. Theory Workshop (ITW).

IEEE, 2007, pp.

283–288.

[6] J. C. Walrand and P. Varaiya, “Optimal causal coding - decoding problems,” IEEE Trans. Inf. Theory, vol. 29, pp. 814–819, 1983.
[7] S. Yuksel, “On optimal causal coding of partially observed markov sources in single and multiterminal settings,” IEEE Transactions on Information

Theory, vol. 59, no. 1, pp. 424–437, 2012.

[8] T. Linder and S. Y¨uksel, “On optimal zero-delay coding of vector markov sources,” IEEE Transactions on Information Theory, vol. 60, no. 10, pp.

5975–5991, 2014.

[9] R. G. Wood, T. Linder, and S. Y¨uksel, “Optimal zero delay coding of markov sources: Stationary and ﬁnite memory codes,” IEEE Transactions on

Information Theory, vol. 63, no. 9, pp. 5968–5980, 2017.

[10] W. S. Wong and R. W. Brockett, “Systems with ﬁnite communication bandwidth constraints. i. state estimation problems,” IEEE Transactions on

Automatic Control, vol. 42, no. 9, pp. 1294–1299, 1997.

[11] G. N. Nair and R. J. Evans, “State estimation via a capacity-limited communication channel,” in IEEE Conference on Decision and Control, vol. 1, Dec

1997, pp. 866–871.

[12] ——, “State estimation under bit-rate constraints,” in IEEE Conference on Decision and Control, vol. 1, Dec 1998, pp. 251–256.

15

[13] N. G. Dokuchaev and A. V. Savkin, “Recursive state estimation via limited capacity communication channels,” in Proceedings of the 38th IEEE

Conference on Decision and Control, vol. 5.

IEEE, 1999, pp. 4929–4932.

[14] S. C. Smith and P. Seiler, “Estimation with lossy measurements: jump estimators for jump systems,” IEEE Trans. Autom. Control, vol. 48, no. 12, pp.

2163–2171, 2003.

[15] A. S. Matveev and A. V. Savkin, “The problem of state estimation via asynchronous communication channels with irregular transmission times,” IEEE

Trans. Autom. Control, vol. 48, no. 4, pp. 670–676, 2003.

[16] G. M. Lipsa and N. C. Martins, “Remote state estimation with communication costs for ﬁrst-order lti systems,” IEEE Trans. Autom. Control, vol. 56,

pp. 2013–2025, 2011.

[17] J. Chakravorty and A. Mahajan, “Fundamental limits of remote estimation of autoregressive markov processes under communication constraints,” IEEE

Trans. Autom. Control, vol. 62, pp. 1109–1123, 2017.

[18] A. Nayyar, T. Basar, D. Teneketzis, and V. V. Veeravalli, “Optimal strategies for communication and remote estimation with an energy harvesting sensor,”

IEEE Trans. Autom. Control, vol. 58, pp. 2246–2259, 2013.

[19] Y. Sun, Y. Polyanskiy, and E. Uysal-Biyikoglu, “Remote estimation of the wiener process over a channel with random delay,” in IEEE Inter. Symp. Inf.

Theory (ISIT), Jun. 2017, pp. 321–325.

[20] T. Linder and G. Lugosi, “A zero-delay sequential scheme for lossy coding of individual sequences,” IEEE Trans. Inf. Theory, vol. 47, pp. 2533–2538,

2001.

[21] T. Weissman and N. Merhav, “On limited-delay lossy coding and ﬁltering of individual sequences,” IEEE Trans. Inf. Theory, vol. 48, pp. 721–732, 2002.
[22] T. Weisman and N. Merhav, “Universal prediction of individual binary sequences in the presence of noise,” IEEE Trans. Inf. Theory, vol. 47, pp.

2151–2173, 2001.

[23] S. Matloub and T. Weissman, “Universal zero-delay joint source - channel coding,” IEEE Trans. Inf. Theory, vol. 52, pp. 5240–5249, 2006.
[24] M. S. P. A. K. Gorbunov, “Nonanticipatory and prognostic epsilon entropies and message generation rates,” Problems Inform. Transmission, vol. 9, pp.

184–191, 1973.

[25] ——, “Prognostic epsilon entropy of a gaussian message and a gaussian source,” Problems Inform. Transmission, vol. 10, pp. 93–109, 1974.
[26] P. Stavrou, C. K. Kourtellaris, and C. D. Charalambous, “Information nonanticipative rate distortion function and its applications,” CoRR, vol.

abs/1405.1593, 2014.

[27] P. A. Stavrou, J. Østergaard, and C. D. Charalambous, “Zero-delay rate distortion via ﬁltering for vector-valued gaussian sources,” IEEE Journal of

Selected Topics in Signal Processing, vol. 12, no. 5, pp. 841–856, Oct 2018.

[28] P. A. Stavrou, T. Charalambous, C. D. Charalambous, and S. Loyka, “Optimal estimation via nonanticipative rate distortion function and applications to

time-varying gauss–markov processes,” SIAM Journal on Control and Optimization, vol. 56, no. 5, pp. 3731–3765, 2018.

[29] H. Viswanathan and T. Berger, “Sequential coding of correlated sources,” IEEE Trans. Inf. Theory, vol. 46, no. 1, pp. 236–246, 2000.
[30] N. Ma and P. Ishwar, “On delayed sequential coding of correlated sources,” IEEE Transactions on Information Theory, vol. 57, no. 6, pp. 3763–3782,

June 2011.

[31] A. Khina, V. Kostina, A. Khisti, and B. Hassibi, “Tracking and control of gauss–markov processes over packet-drop channels with acknowledgments,”

IEEE Transactions on Control of Network Systems, vol. 6, no. 2, June 2019.

[32] A. Khina, A. Khisti, V. Kostina, and B. Hassibi, “Sequential coding of Gauss-Markov sources with packet erasures and feedback,” in IEEE Inf. Theory

Workshop (ITW), Nov. 2017, pp. 529–530.

[33] A. Kipnis and G. Reeves, “Gaussian approximation of quantization error for estimation from compressed data,” in IEEE International Symposium on

Information Theory, ISIT, 2019, pp. 2029–2033.

[34] D. F. Delchamps, “Extracting state information from a quantized output record,” Systems and Control Letters, vol. 13, no. 5, pp. 365–372, December

1989.

[35] V. S. Borkar and S. K. Mitter, LQG Control with Communication Constraints. Boston, MA: Springer US, 1997, pp. 365–373.
[36] W. S. Wong and R. W. Brockett, “Systems with ﬁnite communication bandwidth constraints. ii. stabilization with limited information feedback,” IEEE

Transactions on Automatic Control, vol. 44, no. 5, pp. 1049–1053, 1999.

[37] G. N. Nair and R. J. Evans, “Communication-limited stabilization of linear systems,” in Proceedings of the 39th IEEE Conference on Decision and

Control (Cat. No. 00CH37187), vol. 1.

IEEE, 2000, pp. 1005–1010.

[38] D. Liberzon, “On stabilization of linear systems with limited information,” IEEE Transactions on Automatic Control, vol. 48, no. 2, pp. 304–307, 2003.
[39] K. You and L. Xie, “Minimum data rate for mean square stabilization of discrete lti systems over lossy channels,” IEEE Transactions on Automatic

Control, vol. 55, no. 10, pp. 2373–2378, 2010.

[40] S. Yuksel, “Stochastic stabilization of noisy linear systems with ﬁxed-rate limited feedback,” IEEE Transactions on Automatic Control, vol. 55, no. 12,

pp. 2847–2853, 2010.

[41] S. Yuksel and S. P. Meyn, “Random-time, state-dependent stochastic drift for markov chains and application to stochastic stabilization over erasure

channels,” IEEE Transactions on Automatic Control, vol. 58, no. 1, pp. 47–59, 2012.

[42] S. Yuksel, “Characterization of information channels for asymptotic mean stationarity and stochastic stability of nonstationary/unstable linear systems,”

IEEE Transactions on Information Theory, vol. 58, no. 10, pp. 6332–6354, 2012.

[43] ——, “Stationary and ergodic properties of stochastic nonlinear systems controlled over communication channels,” SIAM Journal on Control and

Optimization, vol. 54, no. 5, pp. 2844–2871, 2016.

[44] H. V. Poor, An Introduction to Signal Detection and Estimation (2nd Edition). Berlin, Heidelberg: Springer-Verlag, 1994.
[45] A. Gersho and R. M. Gray, Vector quantization and signal compression. Springer Science & Business Media, 2012, vol. 159.
[46] P. Mayekar and H. Tyagi, “RATQ: A universal ﬁxed-length quantizer for stochastic optimization,” arXiv:1908.08200, 2019.
[47] T. Cover and J. Thomas, Elements of Information Theory, ser. A Wiley-Interscience publication. Wiley, 2006.

