1
2
0
2

c
e
D
4
1

]

G
L
.
s
c
[

2
v
5
7
3
6
1
.
6
0
0
2
:
v
i
X
r
a

Improving Calibration through the Relationship with
Adversarial Robustness

Yao Qin

Xuezhi Wang

Alex Beutel

Ed H. Chi

Google Research
{yaoqin, xuezhiw, alexbeutel, edchi}@google.com

Abstract

Neural networks lack adversarial robustness, i.e., they are vulnerable to adversarial
examples that through small perturbations to inputs cause incorrect predictions.
Further, trust is undermined when models give miscalibrated predictions, i.e., the
predicted probability is not a good indicator of how much we should trust our
model. In this paper, we study the connection between adversarial robustness and
calibration and ﬁnd that the inputs for which the model is sensitive to small pertur-
bations (are easily attacked) are more likely to have poorly calibrated predictions.
Based on this insight, we examine if calibration can be improved by addressing
those adversarially unrobust inputs. To this end, we propose Adversarial Robust-
ness based Adaptive Label Smoothing (AR-AdaLS) that integrates the correlations
of adversarial robustness and calibration into training by adaptively softening labels
for an example based on how easily it can be attacked by an adversary. We ﬁnd
that our method, taking the adversarial robustness of the in-distribution data into
consideration, leads to better calibration over the model even under distributional
shifts. In addition, AR-AdaLS can also be applied to an ensemble model to further
improve model calibration.

1

Introduction

The robustness of machine learning algorithms is becoming increasingly important as ML systems
are being used in higher-stakes applications. In one line of research, neural networks are shown
to lack adversarial robustness – small perturbations to the input can successfully fool classiﬁers
into making incorrect predictions (Szegedy et al., 2014; Goodfellow et al., 2014; Carlini & Wagner,
2017b; Madry et al., 2017; Qin et al., 2020b). In largely separate lines of work, researchers have
studied uncertainty in model’s predictions. For example, models are often miscalibrated where the
predicted conﬁdence is not indicative of the true likelihood of the model being correct (Guo et al.,
2017; Thulasidasan et al., 2019; Lakshminarayanan et al., 2017; Wen et al., 2020; Kull et al., 2019).
The calibration issue is exacerbated when models are asked to make predictions on data different
from the training distribution (Snoek et al., 2019), which becomes an issue in practical settings where
it is important that we can trust model predictions under distributional shift.

Despite robustness, in all its forms, being a popular area of research, the relationship between these
perspectives has not been extensively explored previously. In this paper, we study the correlation
between adversarial robustness and calibration. We discover that input data points that are sensitive
to small adversarial perturbations (are easily attacked) are more likely to have poorly calibrated
predictions. This holds true on a number of network architectures for classiﬁcation and on all
the datasets that we consider: CIFAR-10 (Krizhevsky, 2009), CIFAR-100 (Krizhevsky, 2009) and
ImageNet (Russakovsky et al., 2015). This suggests that the miscalibrated predictions on those
adversarially unrobust data points greatly degrades the performance of model calibration. Based on
this insight, we hypothesize and study if calibration can be improved by giving different supervision
to the model depending on adversarial robustness of each training data.

35th Conference on Neural Information Processing Systems (NeurIPS 2021).

 
 
 
 
 
 
Figure 1: Inputs that are adversarially unrobust are more likely to have poorly calibrated and unstable
predictions on CIFAR-10, CIFAR-100 and ImageNet. Top: Accuracy and conﬁdence of the predicted
class. Bottom: ECE (lower is better) and variance (lower is better) in each adversarial robustness
subset. Higher adversarial robustness level means the input are more adversarially robust (require
larger adversarial perturbations to fool the classiﬁer into wrong predictions).

To this end, we propose Adversarial Robustness based Adaptive Label Smoothing (AR-AdaLS) to
integrate the correlations between adversarial robustness and calibration into training. Speciﬁcally,
AR-AdaLS adaptively smooths the training labels conditioned on how vulnerable an input is to
adversarial attacks. Our method improves label smoothing (Szegedy et al., 2014) by explicitly
teaching the model to differentiate the training data according to their adversarial robustness and
then adaptively smooth their labels. By giving different supervision to the training data, our method
leads to better calibration over the model without an increase of latency during inference.
In
addition, since adversarially unrobust data points can be considered as outliers of the underlying
data distribution (Carlini et al., 2019), our method can even greatly improve model calibration on
held-out shifted data. Further, we propose “AR-AdaLS of Ensemble” to combine our AR-AdaLS
and deep ensembles (Lakshminarayanan et al., 2017; Snoek et al., 2019), to further improve the
calibration performance under distributional shift. Last, we ﬁnd an additional beneﬁt of AR-AdaLS
is improving model stability (i.e., decreasing variance over multiple independent runs), which is
valuable in practical applications where changes in predictions across runs (churn) is problematic.

In summary, our main contributions are as follows:

• Relationship among Robustness Metrics: We ﬁnd a signiﬁcant correlation between adversar-
ial robustness and calibration: inputs that are unrobust to adversarial attacks are more likely to
have poorly calibrated predictions.

• Algorithm: We hypothesize that training a model with different supervision based on adversarial
robustness of each input will make the model better calibrated. To this end, we propose
AR-AdaLS to automatically learn how much to soften the labels of training data based on their
adversarial robustness. Further, we introduce “AR-AdaLS of Ensemble” to show how to apply
AR-AdaLS to an ensemble model.

• Experimental Analysis: On CIFAR-10, CIFAR-100 and ImageNet, we ﬁnd that AR-AdaLS is
more effective than previous label smoothing methods in improving calibration, particularly for
shifted data. Further, we ﬁnd that while ensembling can be beneﬁcial, applying AR-AdaLS to
adaptively calibrate ensembles offers further improvements over calibration.

2 Related Work

Uncertainty estimates How to better estimate a model’s predictive uncertainty is an important
research topic, since many models with a focus on accuracy may fall short in predictive uncertainty.

2

A popular way to improve a model’s predictive uncertainty is to make the model well-calibrated, e.g.,
post-hoc calibration by temperature scaling (Guo et al., 2017), and multi-class Dirichlet calibration
(Kull et al., 2019). In addition, Bayesian neural networks, through learning a posterior distribution
over network parameters, can also be used to quantify a model’s predictive uncertainty, e.g., Graves
(2011); Blundell et al. (2015); Welling & Teh (2011). Dropout-based variational inference (Gal &
Ghahramani, 2016; Kingma et al., 2015) can help DNN models make less over-conﬁdent predictions
and be better calibrated. Recently, mixup training (Zhang et al., 2018) has been shown to improve
both models’ generalization and calibration (Thulasidasan et al., 2019), by preventing the model from
being over-conﬁdent in its predictions. Despite the success of improving uncertainty estimates over
in-distribution data, Snoek et al. (2019) argue that it does not usually translate to a better performance
on data that shift from the training distribution. Among all the methods evaluated by Snoek et al.
(2019) under distributional shift, ensemble of deep neural networks (Lakshminarayanan et al., 2017),
is shown to be most robust to dataset shift, producing the best uncertainty estimates.

Adversarial robustness On the other hand, machine learning models are known to be brittle (Xin
et al., 2017) and vulnerable to adversarial examples (Athalye et al., 2018; Carlini & Wagner, 2017a,b;
He et al., 2018; Qin et al., 2020a). Many defenses have been proposed to improve model’s adversarial
robustness (Song et al., 2017; Yang et al., 2019; Goodfellow et al., 2018), however are further attacked
by more advanced defense-aware attacks (Carlini & Wagner, 2017b; Athalye et al., 2018). Recently,
Carlini et al. (2019); Stock & Cissé (2018) deﬁne adversarial robustness as the minimum distance in
the input domain required to change the model’s output prediction by constructing an adversarial
attack. The most recent work that is close to ours, Carlini et al. (2019), makes the interesting
observation that easily attackable data are often outliers in the underlying data distribution and then
use adversarial robustness to determine an improved ordering for curriculum learning. Our work,
instead, explores the relationship between adversarial robustness and calibration. In addition, we
use adversarial robustness as an indicator to adaptively smooth the training labels to improve model
calibration.

Label smoothing Label smoothing is originally proposed in Szegedy et al. (2016) and is shown to
be effective in improving the quality of uncertainty estimates in Müller et al. (2019); Thulasidasan
et al. (2019). Instead of minimizing the cross-entropy loss between the predicted probability ˆp and
the one-hot label p, label smoothing minimizes the cross-entropy between the predicted probability
and a softened label (cid:101)p = p(1 − (cid:15)) + (cid:15)
Z , where Z is the number of classes in the dataset and (cid:15) is a
hyperparameter which controls the degree of the smoothing effect. Our work makes label smoothing
adaptive and incorporates the correlation with adversarial robustness to further improve calibration.

3 Correlations between Adversarial Robustness and Calibration

To explore the relationship between adversarial robustness and calibration, we ﬁrst introduce the
metrics to evaluate each of them (arrows indicate which direction is better).

Adversarial robustness ↑ Adversarial robustness measures the minimum distance in the input
domain required to change the model’s output prediction by constructing an adversarial attack (Carlini
et al., 2019; Stock & Cissé, 2018). Speciﬁcally, given an input x and a classiﬁer f (·) that predicts
the class for the input, the adversarial robustness is deﬁned as the minimum adversarial perturbation
δ that enables f (x + δ) (cid:54)= f (x). Following the work (Carlini et al., 2019), we construct the (cid:96)2
based CW attack (Carlini & Wagner, 2017b) and then use the (cid:96)2 norm of the adversarial perturbation
(cid:107)δ(cid:107)2 to measure the distance to the decision boundary. Therefore, a more adversarially robust input
requires a larger adversarial perturbation to change the model’s prediction.

Expected calibration error ↓ Model calibration measures the alignment between the predicted
probability and the accuracy. Well calibrated predictions convey the information about how much
we should trust a model’s prediction. We follow the widely used expected calibration error (ECE)
to measure the calibration performance of a network (Guo et al., 2017; Snoek et al., 2019). To
compute the ECE, we need to ﬁrst divide all the data into K buckets sorted by their predicted
probability (conﬁdence) of the predicted class. Let Bk represent the set of data in the k-th conﬁdence
bucket. Then the accuracy and the conﬁdence of Bk are deﬁned as acc(Bk) = 1
1(ˆyi =
|Bk|
ˆpˆyi
yi) and conf(Bk) = 1
i , where ˆy and y represent the predicted class and the true
|Bk|

i∈Bk

i∈Bk

(cid:80)

(cid:80)

3

Table 1: Network architecture and accuracy used for each dataset.

Dataset

Network
Accuracy

CIFAR-10

ResNet-29
91.4%

CIFAR-100

WRN-28-10
79.2%

ImageNet

ResNet-101
77.7%

class respectively, and ˆpˆy is the predicted probability of ˆy. The ECE is then deﬁned as ECE =
(cid:80)K

|Bk|
N |acc(Bk) − conf(Bk)|, where N is the number of data points.

k=1

3.1 Correlations

Based on the evaluation metrics, we can see that adversarial robustness and calibration are measuring
quite different properties: the adversarial robustness measures the property of the data by computing
the adversarial perturbation δ from the input domain, while the calibration metric measures the prop-
erties of the model’s predicted probability in the output space. Although adversarial robustness and
calibration are conceptually different, they are both connected to the decision boundary. Speciﬁcally,
adversarial robustness can be used to measure the distance to the decision boundary: if a data point is
adversarially unrobust, i.e., easy to ﬁnd a small input perturbation to fool the classiﬁer into wrong
classiﬁcations, then this data point is close to the decision boundary. Meanwhile, models should
have relatively less conﬁdent predictions on data points close to the decision boundary. However, as
pointed out by (Guo et al., 2017; Snoek et al., 2019), existing deep neural networks are frequently
over-conﬁdent, i.e., having predictions with high conﬁdence even whey they should be uncertain.
Taking these two together, we hypothesize if examples that can be easily attacked by adversarial
examples are also poorly calibrated.

To test this, we perform experiments on the clean test set across three datasets: CIFAR-10 (Krizhevsky,
2009), CIFAR-100 (Krizhevsky, 2009) and ImageNet (Russakovsky et al., 2015) with different
networks, whose architecture and accuracy are shown in Table 1. We refer to these models as “Vanilla”
for each dataset in the following discussion. The details for training each vanilla network are included
in Appendix A.

To explore the relationship between adversarial robustness and calibration, we start with the relation-
ship between adversarial robustness and conﬁdence together with accuracy. Speciﬁcally, we rank the
input data according to their adversarial robustness and then divide the dataset into R equally-sized
subsets (R = 10 used in this paper). For each adversarial robustness subset, we compute the accuracy
and the average conﬁdence score of the predicted class. As shown in the ﬁrst row in Figure 1, we can
clearly see that both accuracy and conﬁdence increase with the adversarial robustness of the input
data, and conﬁdence is consistently higher than accuracy in each adversarial robustness subset across
three datasets. This indicates that although vanilla classiﬁcation models achieve the state-of-the-art
accuracy, they tend to give over-conﬁdent predictions, especially for those adversarially unrobust
data points.

Taking one step further, we particularly compute the expected calibration error (ECE) in each
adversarial robustness subset, shown in the bottom row of Figure 1. In general, we ﬁnd that data
points falling into lower adversarial robustness levels are more likely to be over-conﬁdent and less
well calibrated (larger ECE). For those adversarially robust examples, there is a better alignment
between the model’s predicted conﬁdence and accuracy, and the ECE over those examples is close to
0. This nicely validates our hypothesis: inputs that are adversarially unrobust are more likely to have
poorly calibrated predictions. On larger-scale ImageNet, while we still see the general trend holds,
the least adversarially robust examples are relatively well calibrated. We hypothesize this may be due
to larger training data and less overﬁtting.

Furthermore, we also ﬁnd an interesting correlation between adversarial robustness and model
stability, which is measured by the variance of the predicted probability across M independent
runs (e.g., M = 5). The variance is computed as σ2 = 1
i=1(ˆpm,i − ¯pi)2, where
ˆpm,i is the m-th model’s predicted probability of the i-th data and ¯pi = 1
m=1 ˆpm,i is the
M
average predicted probability over M runs. As shown in the bottom row of Figure 1, we see that
those adversarially unrobust examples tend to have a much higher variance across all three datasets.
This indicates that inputs that are unrobust to adversarial attacks are more likely to have unstable
predictions.

(cid:80)M

(cid:80)M

(cid:80)N

M −1

m=1

1
N

4

Algorithm 1 Training procedure for AR-AdaLS

Input: number of classes Z, number of training epochs T , number of adversarial robustness subset
R, learning rate of adaptive label smoothing α.
For each adversarial robustness training subset, we initialize the soft label as the one-hot label
(cid:101)pr,t = pr, where the initial soft label for the correct class (cid:101)pz=y
for t = 1 to T do

r,t = 1.

Minimize cross-entropy loss between soft label and predicted probability 1
R
for r = 1 to R do
Update (cid:101)pz=y
Clip (cid:101)pz=y
Update (cid:15)r,t+1 ← ((cid:101)pz=y
Update (cid:101)pr,t+1 ← pr(1 − (cid:15)r,t+1) + (cid:15)r,t+1

r,t+1 ← (cid:101)pz=y
r,t+1 to be within ( 1

Z , 1]
r,t+1 − 1) · Z
1−Z

r,t − α · {conf(Sval

)t − acc(Sval

)t}

Z

r

r

end for

(cid:80)R

r L((cid:101)pr,t, ˆpr,t)

(cid:46) Eqn. (3)

(cid:46) Eqn. (4)
(cid:46) Eqn. (1)

end for

Taking all together, these empirical results nicely build a connection between very different concepts.
In particular, adversarial robustness is measured over the input domain while both calibration and
stability are measured over the output space. Given the strong empirical connection, we now ask: can
we improve model calibration and stability by targeting adversarially unrobust examples?

4 Method

Based on the correlation between adversarial robustness and calibration, we hypothesize and study if
calibration can be improved by giving different supervision to the model depending on the adversarial
robustness of training data. To this end, we propose a method named Adversarial Robustness based
Adaptive Label Smoothing (AR-AdaLS), which performs label smoothing at different degrees to the
training data based on their adversarial robustness. Speciﬁcally, we sort and divide the training data
into R small subsets with equal size according to their adversarial robustness1 and then use (cid:15)r to
soften the labels in each training subset Strain

. The soft labels can be formulated as:

r

(cid:101)pr = pr(1 − (cid:15)r) +

(cid:15)r
Z

,

(1)

r = 1 for the correct class and pz(cid:54)=y

where pr stands for the one-hot vector, e.g., pz=y
r = 0 for the
others, and Z is the number of classes in the dataset. The parameter (cid:15)r controls the degree of
smoothing effect and allows for different levels of smoothing in each adversarial robustness subset.
Generally, a relatively larger (cid:15)r is desirable for lower adversarial robustness levels such that the
model learns to make a lower conﬁdent prediction. Instead of empirically setting the parameter (cid:15)r in
each adversarial robustness subset, we allow it to be adaptively updated according to the calibration
performance on the validation set (discussed in Section 4.1). In this way, we explicitly train a network
with different supervision based on the adversarial robustness of training data.

There are two options to obtain the adversarial robustness. One is “on the ﬂy”: to keep creating the
adversarial attacks during training, which provides precise adversarial robustness ranking but at the
cost of great computing time. The other is to “pre-compute” the adversarial robustness by attacking
a vanilla model with the same network architecture but trained with one-hot labels. This is more
efﬁcient but at the sacriﬁce of the precision of adversarial robustness ranking. In practice, we ﬁnd that
it is sufﬁcient to make the network differentiate the adversarially robust and unrobust data with the
pre-computed adversarial robustness (see more discussion in Section 5.6). Therefore, all experiments
related to “AR-AdaLS” without further speciﬁcation are based on pre-computed adversarial robustness
for efﬁciency.

4.1 Adaptive learning mechanism

To ﬁnd the best hyperparameter (cid:15) for label smoothing, previous methods (Szegedy et al., 2016;
Thulasidasan et al., 2019) sweep (cid:15) in a range and choose the one that has the best validation

1Note, predicted conﬁdence is not a good indicator for splitting the training dataset as the model can easily

overﬁt to the training data and their predicted conﬁdence are all close to 100%.

5

performance. However, in our setting, the number of combinations of (cid:15)r increases exponentially
with the number of adversarial robustness subsets R. To this end, we propose an adaptive learning
mechanism to automatically learn the parameter (cid:15)r in each adversarial robustness subset. The overall
training procedure is summarized in Algorithm 1.
First, we denote the soft label for the correct class in the r-th adversarial robustness subset as (cid:101)pz=y
According to Eqn. (1), we can derive:

r

.

(cid:101)pz=y
r = 1 − (cid:15)r +
Since well-calibrated predicted probability should be aligned with the empirical accuracy, we use the
calibration performance in the validation set to help update (cid:101)pz=y
for the training data. Speciﬁcally,
we ﬁrst rank the adversarial robustness of the validation data and split the validation set into R
equally-sized subsets. Then, we use the difference between conﬁdence and accuracy in the r-th
adversarial robustness validation subset conf(Sval
) to update the soft label for the correct
class of training data in the r-th adversarial robustness training subset Strain

)−acc(Sval

(2)

.

r

r

r

,

(cid:15)r
Z

r,t+1 = (cid:101)pz=y
(cid:101)pz=y

r,t − α · {conf(Sval

r

)t − acc(Sval

r

r
)t},

(3)

|

|

r

r

r

r,t

(cid:80)

(cid:80)

i∈Sval
r

ˆpz=ˆyi
i

) = 1
|Sval
r

) = 1
|Sval
r

are deﬁned as acc(Sval

where (cid:101)pz=y
is the soft label of the correct class in the r-th adversarial robustness training subset at time
step t. The accuracy and the conﬁdence of Sval
1(ˆyi = yi)
and conf(Sval
, where ˆy and y is the predicted class and the true class
respectively, ˆpz=ˆy denotes the the predicted probability of the predicted class. The hyperparameter
α > 0 plays a role as a learning rate to update the soft label (cid:101)pz=y
r,t based on the difference between
the predicted conﬁdence and accuracy in the validation set. Intuitively, if we assign a large (cid:101)pz=y
to
training data, then the network tends to make a high conﬁdent prediction and vice versa. Therefore,
if the conﬁdence is greater than the accuracy (conf(Sval
)) in the validation set, we
to teach the network to be less conﬁdent. Otherwise, we should increase (cid:101)pz=y
should reduce (cid:101)pz=y
.
In addition, we also need to constrain (cid:101)pz=y
Z , 1] after each update as it stands for the
true probability of the correct class, where Z is the number of classes in the dataset.
For a given (cid:101)pz=y

to be within ( 1

) > acc(Sval

i∈Sval
r

r

r

r

r

r

r

r

, we can easily obtain (cid:15)r by reversing Eqn. (2),
(cid:15)r = ((cid:101)pz=y

Z
1 − Z

,

r − 1) ·
and the soft labels for all the classes (cid:101)pr can be computed according to Eqn. (1). We update the soft
labels after each training epoch in our experiments.

(4)

Note that this adaptive learning mechanism can be easily applied to standard label smoothing without
adversarial robustness slicing (R = 1). In this case, we can replace sweeping the hyperparameter (cid:15)
with this adaptive learning method, named as “Adaptive Label Smoothing” (AdaLS). Our proposed
AdaLS and AR-AdaLS do not increase the inference time: we test AdaLS and AR-AdaLS exactly the
same as a vanilla model.

5 Experiments

Datasets We test our method on three datasets CIFAR-10, CIFAR-100 and ImageNet. In addition,
we also report performance on the shifted datasets: CIFAR-10-C, CIFAR-100-C and ImageNet-
C (Hendrycks & Dietterich, 2019), where there are different types of corruptions (19 types for
CIFAR-10, 17 types for CIFAR-100 and 15 types for ImageNet), e.g., noise, blur, weather and digital
categories that are frequently encountered in natural images. Each type of corruption has ﬁve levels
of shift intensity, with higher levels having more corruption.

5.1 How does AR-AdaLS work?

To have a deeper understanding of how AR-AdaLS works, in Figure 2 we visualize the effect of label
smoothing (LS) and our AR-AdaLS. Comparing Figure 2 (a) and (b), AR-AdaLS is better at calibrating
the data than label smoothing, especially on the adversarially unrobust examples (lower adversarial
robustness level). Further, we show plots of ECE and variance in Figure 2 (c) and (d). Both label
smoothing and AR-AdaLS improve model calibration and stability over vanilla model and AR-AdaLS
has the best performance among three methods. This suggests that AR-AdaLS is better at improving
calibration and stability in adversarially unrobust regions, not just on average.

6

Figure 2: Comparison between LS and our AR-AdaLS on the clean test set of CIFAR-10. (a) and (b):
Accuracy and conﬁdence score of the predicted class in each adversarial robustness subset. (c) and
(d): ECE and variance score of Vanilla, LS and AR-AdaLS.

Table 2: ECE (×10−2) on CIFAR-10 and CIFAR-100. AR-AdaLS improves calibration and is only
rivaled by domain-knowledge based data augmentation or larger ensemble models on CIFAR-100.
Adversarial robustness is generated on-the-ﬂy for AR-AdaLS. All the single-model based results are
generated over four independent runs with random initialization.

Method

CIFAR-10

CIFAR-100

Method

CIFAR-10

CIFAR-100

Single-model based

Data-augmentation based

Vanilla
Temperature Scaling
Label Smoothing
AdaLS
AR-AdaLS

2.49±0.10
0.80±0.05
1.07±0.09
1.23±0.02
0.64±0.02

6.11±0.24
4.26±0.07
2.76±0.26
2.65±0.31
2.27±0.16

mixup
CCAT

0.78±0.20
2.37±0.07

1.69±0.08
7.95±0.35

Ensemble based

Mix-n-Match
Ensemble of Vanilla

0.97
0.90

2.80
2.21

5.2 AR-AdaLS improves calibration

Baselines We compare our proposed AR-AdaLS with the following 8 different methods: (1) Vanilla
model trained with one-hot labels, (2) Temperature Scaling (Guo et al., 2017), a post-hoc calibration
method where the predicted logits are divided by a temperature which is tuned on the hold-out
validation set, (3) label smoothing (LS) (Szegedy et al., 2016) that softs labels by sweeping the
hyperparameter (cid:15) (the smoothing degree) in a range to ﬁnd the best hyperparameter (cid:15), (4) Adaptive
Label Smoothing (AdaLS): we use our proposed adaptive learning mechanism introduced in Sec-
tion 4.1 to automatically learn the hyperparameter (cid:15) rather than sweeping to ﬁnd the best (cid:15). (5) mixup,
which is a data augmentation technique originally proposed in (Zhang et al., 2018) and recently
found to be able to improve calibration in (Thulasidasan et al., 2019), (6) Conﬁdence-calibrated
adversarial training (CCAT) (Stutz et al., 2020), a method builds on adversarial training by reducing
the conﬁdence in the labels of adversarial examples. Note that there is a signiﬁcant difference between
our AR-AdaLS and CCAT: CCAT trains a model on the generated adversarial examples to improve
a model’s adversarial robustness. In contrast, our AR-AdaLS, trained on the clean training data, is
proposed to use the correlation between adversarial robustness and calibration to improve a model
calibration performance. (7) “Ensemble of Vanilla” (Lakshminarayanan et al., 2017), an ensemble
of M vanilla models independently trained with random initialization. (8) Mix-n-Match (Zhang
et al., 2020), an ensemble and compositional method proposed for calibration. All the methods are
trained with the same network architecture, i.e., WRN-28-10 (Zagoruyko & Komodakis, 2016) on
both CIFAR-10 and CIFAR-100, and the same training hyperparameters: e.g., learning rate, batch
size, number of training epochs, for fair comparison2. Please refer to Appendix A for all the training
details and hyperparameters.

Results The expected calibration error of all the methods on CIFAR-10 and CIFAR-100 are
displayed in Table 2. We can clearly see that by differentiating the training data based on their
adversarial robustness, AR-AdaLS effectively reduces the calibration error compared to other single-
model based methods without signiﬁcant change in accuracy (see Figure 6 in Appendix) and it is only
rivaled by mixup on CIFAR-100, which uses extra domain knowledge through data augmentation.
Note that AR-AdaLS is only trained on the clean training data without any data augmentation compared
to mixup (Thulasidasan et al., 2019) and CCAT (Stutz et al., 2020).

2The result of Mix-n-Match in Table 2 is from Table 1 reported in the original work (Zhang et al., 2020),

which is trained with the same network architecture, WRN-28-10.

7

Table 3: Mean of ECE (×10−2) across 19 types of shift for CIFAR-10-C and 15 types of shift
for ImageNet-C. Smaller is better. ResNet-29 is used for CIFAR-10 and ResNet-101 is used for
ImageNet. The standard deviation of ﬁve independent runs for each single model is reported. The
best single model and ensemble model in each shift intensity is highlighted in bold.

Single-model based

Ensemble-based

Methods

CIFAR-10-C ImageNet-C

Methods

CIFAR-10-C ImageNet-C

Vanilla
LS
AdaLS

16.7±0.5
10.1±0.4
9.6±0.5

AR-AdaLS

6.4±0.6

10.7±0.5
8.1±0.4
8.0±0.2

6.9±0.2

Ensemble of Vanilla
Ensemble of LS
Ensemble of AdaLS

Ensemble of AR-AdaLS
AR-AdaLS of Ensemble

6.5
4.6
5.2

5.5
4.4

4.2
4.7
4.8

5.1
4.0

5.3

Improve calibration on shifted dataset

Table 3 summarizes the mean calibration error (ECE) on the corrupted datasets: CIFAR-10-C
and ImageNet-C (Hendrycks & Dietterich, 2019). Looking at all the single-model based methods,
we can see that AR-AdaLS signiﬁcantly outperforms other single-model based methods with the
lowest ECE. Contrasting with LS and AdaLS, we see AR-AdaLS beneﬁts greatly from the adversarial
robustness slicing. As a result, our model learns to give smaller soft labels of the correct class to those
adversarially unrobust training data, which can also be considered as outliers of the underlying data
distribution (Carlini et al., 2019). Therefore, when tested on the shifted data that deep networks have
been shown to produce pathologically over-conﬁdent predictions (Hendrycks & Dietterich, 2019),
our model correctly learns to make a relatively lower-conﬁdence prediction, resulting in a better
calibration performance.

In addition, we also compare AR-AdaLS with “Ensemble of Vanilla” (Lakshminarayanan et al., 2017),
which is shown to be the best model for models’ calibration under distributional shift (Snoek et al.,
2019). The result of Ensemble of Vanilla is an ensemble of M = 5 vanilla models independently
trained with random initialization. We can see that AR-AdaLS achieves comparable calibration
performance on CIFAR-10 and the ensemble is better under highly shifted data on ImageNet.

Combination with deep ensembles We further discuss the following two ways to combine
AR-AdaLS with ensembles:

• Ensemble of AR-AdaLS: As in Lakshminarayanan et al. (2017), we ensemble AR-AdaLS by
training multiple independent AR-AdaLS models with random initialization, and average their
predictions at inference.

• AR-AdaLS of Ensemble: Instead of computing soft labels independently for each AR-AdaLS,
we perform AR-AdaLS on the ensembled predictions, i.e., in Eqn (3) we compute conﬁdence
and accuracy based on the average of M = 5 model predictions. Each model is then supervised
with the same soft labels. We will see this slight distinction in training is quite important.

As shown in Table 3, naively combining deep ensembles with AR-AdaLS (Ensemble of AR-AdaLS)
could not effectively improve models’ calibration (see more details in Appendix B). In contrast,
AR-AdaLS of Ensemble, which adaptively adjusts smoothing to keep the ensemble models well
calibrated, performs the best under distributional shift on both CIFAR-10 and ImageNet.

5.4

Improve model stability

Since we observe in Figure 1 that the most adversarially unrobust data points also have very unstable
predictions, we test AR-AdaLS to see if it can help improve model stability, which is of great value in
practice where high variance of a model is bad for churn (Milani Fard et al., 2016). In Figure 3 we
can see that AR-AdaLS can effectively reduce the variance of a model compared to a vanilla model
and label smoothing on CIFAR-10 and ImageNet. Please refer to Table 5 in Appendix for numerical
numbers on both datasets.

8

Figure 3: Variance on clean test and shifted data on CIFAR-10 and ImageNet. For each shift intensity,
we show the results with a box plot summarizing the 25th, 50th, 75th quartiles across 19 shift types
on CIFAR10-C and 15 shift types on ImageNet-C. The error bars indicate the min and max value
across different shift types. ResNet-29 is used for CIFAR-10 and ResNet-101 is used for ImageNet.

Figure 4: Histogram of predictive entropy on out-of-distribution data. Each model is trained on
CIFAR-10 and tested on CIFAR-100. The network architecture is WRN-28-10 and adversarial
robustness in AR-AdaLS is generated on-the-ﬂy.

5.5

Improvements on out-of-distribution data

We further study the performance of AR-AdaLS when predicting on out-of-distribution (OOD) data.
Following (Snoek et al., 2019), we compare the performance of Vanilla, Label Smoothing and
AR-AdaLS by plotting the histogram of the entropy on the OOD data (higher entropy on OOD
is better). As shown in Figure 4, each model is trained on CIFAR-10 dataset and then tested on
CIFAR-100 dataset. We can clearly see that AR-AdaLS signiﬁcantly reduces the number of low-
entropy predictions on OOD data. In addition, using CIFAR-10/CIFAR-100 as in-distribution/out-
of-distribution data, we also report the Area under the ROC curve (AUROC) of label smoothing,
mixup and AR-AdaLS. The AUROC score of standard label smoothing and mixup is 0.832±0.005
and 0.821±0.003 respectively, whereas our AR-AdaLS achieves 0.885±0.003. This demonstrates the
effectiveness of AR-AdaLS even on fully out-of-distribution data.

5.6 Sensitivity analysis

Sensitivity to the number of adversarial robustness subsets We perform a sensitivity analysis
for the number of adversarial robustness subsets R. Speciﬁcally, we plot the calibration error of
AR-AdaLS with a varying R on the clean CIFAR-10 and corrupted CIFAR-10-C in Figure 5. We
can see that there is a signiﬁcant drop in calibration error (ECE) when we increase the number of
adversarial robustness subsets R from 1, where R = 1 denotes AdaLS. Further, the calibration error
is relatively stable when R is chosen within the range [10, 16]. Thus, we choose R = 10 for all
results shown in this paper for AR-AdaLS.

9

Figure 5: ECE on CIFAR-10 and CIFAR-10-C of AR-AdaLS with varying number of adversarial
robustness subset R. Note that when R = 1, AR-AdaLS becomes AdaLS. The results are based on 4
independent runs.

Table 4: Ablation study of AR-AdaLS on CIFAR-100 and CIFAR100-C (corrupted). We report both
accuracy (×10−2) and expected calibration error (×10−2), denoted by Acc and ECE for the clean
test set, and cAcc and cECE for CIFAR100-C. Arrow indicates the better direction; best calibration
is bolded.

Method

Vanilla

Acc/cAcc (↑)
ECE/cECE (↓)

79.2/52.0
6.1/18.2

Label
Smoothing

78.9/51.7
2.8/16.3

Temperature
Scaling

AR-AdaLS
(pre-compute)

79.2/52.0
4.3/14.0

79.3/52.2
2.6/14.2

AR-AdaLS
(on-the-ﬂy)

79.2/52.1
2.3/13.2

Sensitivity to the exactness of adversarial robustness To investigate this, we study the perfor-
mance of AR-AdaLS using adversarial robustness generated via two different ways: One is “on-the-
ﬂy”: we keep creating adversarial attacks during training, which provides a more precise adversarial
robustness ranking but at the cost of great computing time. The other is to “pre-compute” adversarial
robustness by attacking a vanilla model that is trained with one-hot labels. This is more efﬁcient
but at the sacriﬁce of the precision of adversarial robustness ranking. We perform experiments
on CIFAR-100 as an example to compare the performance of AR-AdaLS based on the adversarial
robustness that is “pre-computed” or “on-the-ﬂy”. As shown in Table 4, generating adversarial
robustness “on-the-ﬂy” can further help improve the calibration performance for AR-AdaLS on both
clean and shifted datasets, compared to pre-computing adversarial robustness. Similar patterns are
observed on CIFAR-10.3

Therefore, we can conclude that 1) the exactness of adversarial robustness is helpful for AR-AdaLS,
that is, more precise adversarial robustness leads to a better performance. 2) AR-AdaLS with an
approximation of adversarial robustness (pre-computed) can already signiﬁcantly improve label
smoothing. Hence, all results in this paper related to “AR-AdaLS” without further speciﬁcation are
based on pre-computed adversarial robustness for efﬁciency. This is because our main target is
to show that the idea of differentiating the training data based on their adversarial robustness is
promising to improve model calibration rather than pushing the results to the best.

6 Conclusion

In this paper, we have explored the correlations between adversarial robustness and calibration. We
ﬁnd across three datasets that adversarially unrobust data points, where small adversarial perturbations
to the input are able to fool the classiﬁer into wrong predictions, are more likely to have poorly
calibrated and unstable predictions. Based on this insight, we propose AR-AdaLS to adaptively
smooth the labels of the training data based on their adversarial robustness. In our experiments we
see that AR-AdaLS is more effective than previous label smoothing methods in improving calibration,
particularly for shifted data, and can offer improvements on top of already strong ensembling methods.
We believe this is an exciting new use for adversarial robustness as a means to more generally improve
model trustworthiness, not just by limiting adversarial attacks but also improving calibration and
stability on unexpected data. We hope this spurs further work at the intersection of these areas of
research.

3We did not run on-the-ﬂy AR-AdaLS for ImageNet due to the computational intensity.

10

References

Athalye, A., Carlini, N., and Wagner, D. Obfuscated gradients give a false sense of security:
Circumventing defenses to adversarial examples. In International Conference on Machine Learning,
2018.

Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra, D. Weight uncertainty in neural network.
volume 37 of Proceedings of Machine Learning Research, pp. 1613–1622, Lille, France, 07–09
Jul 2015. PMLR.

Carlini, N. and Wagner, D. Adversarial examples are not easily detected: Bypassing ten detection
methods. In Proceedings of the 10th ACM Workshop on Artiﬁcial Intelligence and Security, AISec
’17, pp. 3–14. Association for Computing Machinery, 2017a. ISBN 9781450352024.

Carlini, N. and Wagner, D. Towards evaluating the robustness of neural networks. In 2017 IEEE

Symposium on Security and Privacy (SP), pp. 39–57. IEEE, 2017b.

Carlini, N., Erlingsson, Ú., and Papernot, N. Distribution density, tails, and outliers in machine

learning: Metrics and applications. ArXiv, abs/1910.13427, 2019.

Gal, Y. and Ghahramani, Z. Dropout as a bayesian approximation: Representing model uncertainty

in deep learning. In International Conference on Machine Learning, pp. 1050–1059, 2016.

Goodfellow, I., Shlens, J., and Szegedy, C. Explaining and harnessing adversarial examples. In

International Conference on Learning Representations, 2014.

Goodfellow, I., Qin, Y., and Berthelot, D. Evaluation methodology for attacks against conﬁdence

thresholding models. 2018.

Graves, A. Practical variational inference for neural networks. In Shawe-Taylor, J., Zemel, R. S.,
Bartlett, P. L., Pereira, F., and Weinberger, K. Q. (eds.), Advances in Neural Information Processing
Systems 24, pp. 2348–2356. Curran Associates, Inc., 2011.

Guo, C., Pleiss, G., Sun, Y., and Weinberger, K. Q. On calibration of modern neural networks. In

International Conference on Machine Learning, 2017.

He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings

of the IEEE conference on Computer Vision and Pattern Recognition, pp. 770–778, 2016a.

He, K., Zhang, X., Ren, S., and Sun, J. Identity mappings in deep residual networks. In European

Conference on Computer Vision, pp. 630–645. Springer, 2016b.

He, W., Li, B., and Song, D. Decision boundary analysis of adversarial examples. In International

Conference on Learning Representations, 2018.

Hendrycks, D. and Dietterich, T. G. Benchmarking neural network robustness to common corruptions

and perturbations. International Conference on Learning Representations, 2019.

Kingma, D. P., Salimans, T., and Welling, M. Variational dropout and the local reparameterization

trick. In Advances in Neural Information Processing Systems, pp. 2575–2583, 2015.

Krizhevsky, A. Learning multiple layers of features from tiny images. Technical report, University of

Toronto, 2009.

Kull, M., Perello Nieto, M., Kängsepp, M., Silva Filho, T., Song, H., and Flach, P. Beyond temperature
scaling: Obtaining well-calibrated multi-class probabilities with dirichlet calibration. In Advances
in Neural Information Processing Systems 32, pp. 12316–12326, 2019.

Lakshminarayanan, B., Pritzel, A., and Blundell, C. Simple and scalable predictive uncertainty
estimation using deep ensembles. In Advances in Neural Information Processing Systems, pp.
6405–6416. Curran Associates Inc., 2017. ISBN 9781510860964.

Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. Towards deep learning models
resistant to adversarial attacks. In International Conference on Learning Representations, 2017.

11

Milani Fard, M., Cormier, Q., Canini, K., and Gupta, M. Launch and iterate: Reducing prediction
churn. In Lee, D. D., Sugiyama, M., Luxburg, U. V., Guyon, I., and Garnett, R. (eds.), Advances in
Neural Information Processing Systems 29, pp. 3179–3187. Curran Associates, Inc., 2016.

Müller, R., Kornblith, S., and Hinton, G. E. When does label smoothing help? In Advances in Neural

Information Processing Systems, pp. 4694–4703, 2019.

Qin, Y., Frosst, N., Raffel, C., Cottrell, G., and Hinton, G. Deﬂecting adversarial attacks. arXiv

preprint arXiv:2002.07405, 2020a.

Qin, Y., Frosst, N., Sabour, S., Raffel, C., Cottrell, G., and Hinton, G. Detecting and diagnosing
adversarial images with class-conditional capsule reconstructions. International Conference on
Learning Representations, 2020b.

Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla,
A., Bernstein, M. S., Berg, A. C., and Li, F.-F. Imagenet large scale visual recognition challenge.
International Journal of Computer Vision, 115:211–252, 2015.

Snoek, J., Ovadia, Y., Fertig, E., Lakshminarayanan, B., Nowozin, S., Sculley, D., Dillon, J., Ren,
J., and Nado, Z. Can you trust your model’s uncertainty? evaluating predictive uncertainty under
dataset shift. In Advances in Neural Information Processing Systems, pp. 13969–13980, 2019.

Song, Y., Kim, T., Nowozin, S., Ermon, S., and Kushman, N. Pixeldefend: Leveraging generative
models to understand and defend against adversarial examples. In International Conference on
Learning Representations, 2017.

Stock, P. and Cissé, M. Convnets and imagenet beyond accuracy: Understanding mistakes and

uncovering biases. In European Conference on Computer Vision, 2018.

Stutz, D., Hein, M., and Schiele, B. Conﬁdence-calibrated adversarial training: Generalizing to
unseen attacks. In International Conference on Machine Learning, pp. 9155–9166. PMLR, 2020.

Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I. J., and Fergus, R.
Intriguing properties of neural networks. In International Conference on Learning Representations,
2014.

Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. Rethinking the inception architecture
for computer vision. 2016 IEEE Conference on Computer Vision and Pattern Recognition, pp.
2818–2826, 2016.

Thulasidasan, S., Chennupati, G., Bilmes, J. A., Bhattacharya, T., and Michalak, S. On mixup
training: Improved calibration and predictive uncertainty for deep neural networks. In Advances in
Neural Information Processing Systems, pp. 13888–13899, 2019.

Welling, M. and Teh, Y. W. Bayesian learning via stochastic gradient langevin dynamics. In Pro-
ceedings of the 28th International Conference on International Conference on Machine Learning,
ICML’11, pp. 681–688. Omnipress, 2011. ISBN 9781450306195.

Wen, Y., Tran, D., and Ba, J. Batchensemble: An alternative approach to efﬁcient ensemble and

lifelong learning. In International Conference on Learning Representations, 2020.

Xin, D., Mayoraz, N., Pham, H., Lakshmanan, K., and Anderson, J. R. Folding: Why good models
sometimes make spurious recommendations. In Proceedings of the Eleventh ACM Conference on
Recommender Systems, pp. 201–209, 2017.

Yang, Y., Zhang, G., Katabi, D., and Xu, Z. Me-net: Towards effective adversarial robustness with

matrix estimation. In International Conference on Machine Learning, 2019.

Zagoruyko, S. and Komodakis, N. Wide residual networks. In British Machine Vision Association,

2016.

Zhang, H., Cissé, M., Dauphin, Y., and Lopez-Paz, D. Mixup: Beyond empirical risk minimization.

In International Conference on Learning Representation, 2018.

Zhang, J., Kailkhura, B., and Han, T. Y.-J. Mix-n-match: Ensemble and compositional methods for
uncertainty calibration in deep learning. In International Conference on Machine Learning, pp.
11117–11128. PMLR, 2020.

12

A Implementation Details

A.1 CIFAR-10

ResNet-29 For all the experimental results on ResNet-29 v2 (He et al., 2016b), we use a batch size
of 256. The network is trained with Adam optimizer (Kingma et al., 2015) for 200 epochs. The
initial learning rate is 10−3 and decayed down to 10−4 after 80 epochs, 10−5 after 120 epochs, 10−6
after 160 epochs and 0.5 × 10−6 after 180 epochs. We adapted the following data augmentation and
training script at https://keras.io/examples/cifar10_resnet/. The training mechanism is
the same for all the methods that we compare in the main paper. We randomly split the training
dataset into training data of 45000 images and 5000 images as the validation set. The test set has
10000 images.

For label smoothing (LS), we sweep the hyperparameter (cid:15) within the range [0, 0.1] with a step
size 0.01 and ﬁnd that the network has the best calibration performance on the validation set when
(cid:15) = 0.02.

For Adaptive Label Smoothing (AdaLS), there is a hyperparameter α which plays a role as learning
rate in the adaptive learning mechanism. We choose hyperparameter α based on the calibration
performance on the validation set. Speciﬁcally, we run experiments with α ∈ {0.005, 0.01, 0.05, 0.1}
and ﬁnd that α = 0.05 achieve the best calibration performance.

Similarly, for Adversarial Robustness based Adaptive Label Smoothing (AR-AdaLS), we choose the
hyperparameter α from the set {0.005, 0.01, 0.05} and empirically set α = 0.005 which has the best
calibration performance on the validation set. We update the training labels after each epoch for all
the experiments related to AR-AdaLS, including the experiments on CIFAR-100 and ImageNet. We
use the same hyperparameter α = 0.005 without further tuning for AR-AdaLS of Ensemble.

All the results of ensemble models are obtained via training 5 independent models with random
initializations.

WRN-28-10 We train a Wide ResNet-28-10 v2 (Zagoruyko & Komodakis, 2016) to obtain the state-
of-the-art accuracy for CIFAR-10 (e.g., Table 2 in the main text). We adapt the same training details
and data augmentation at https://github.com/google/edward2/blob/master/baselines/
cifar/deterministic.py.

For label smoothing, we sweep the hyperparameter (cid:15) within the range [0, 0.1] with a step size 0.01
and ﬁnd that the network has the best calibration performance on the validation set when (cid:15) = 0.02.

For AdaLS and AR-AdaLS, the hyperparameter α is set to be 0.005. For AR-AdaLS that generated with
on-the-ﬂy adversarial examples, we recompute the adversarial robustness for training and validation
sets after 65, 130 epochs.

For mixup (Zhang et al., 2018; Thulasidasan et al., 2019), the mixing parameter of two images is
randomly sampled from a Beta distribution Beta(β, β) at each training iteration. We set β = 0.2 for
best calibration performance on in-distribution data.

For CCAT (Stutz et al., 2020), we observe that training models with adversarial examples bounded
with smaller (cid:96)∞ norm, e.g., ||δ||∞ ≤ 0.01, can beneﬁt more to the calibration with a small accuracy
sacriﬁce on the clean data. Therefore, we train CCAT with PGD attacks bounded by ||δ||∞ ≤ 0.01.
The step size and total iterations to generate PGD attacks is 0.025 and 10 respectively during training.

All the results of ensemble models on WRN-28-10 are obtained via training 4 independent models
with random initializations.

A.2 CIFAR-100

We train a Wide ResNet-28-10 v2 (Zagoruyko & Komodakis, 2016) to obtain the state-of-the-art
accuracy for CIFAR-100. We adapt the same training details and data augmentation at https:
//github.com/google/edward2/blob/master/baselines/cifar/deterministic.py.

For label smoothing, we e sweep the hyperparameter (cid:15) within the range [0, 0.1] with a step size 0.01
and ﬁnd that the network has the best calibration performance on the validation set when (cid:15) = 0.07.

13

All the hyperparameters used for AdaLS, AR-AdaLS, mixup (Zhang et al., 2018; Thulasidasan et al.,
2019) and CCAT (Stutz et al., 2020) are the same as those for CIFAR-10 with WRN-28-10.

All the results of ensemble models on WRN-28-10 are obtained via training 4 independent models
with random initializations.

A.3

ImageNet

All the experiments on ImageNet were obtained via training a ResNet-101 v1 (He et al., 2016a) follow-
ing the training script at https://github.com/google/edward2/blob/master/baselines/
imagenet/deterministic.py. The network is trained with a batch size of 128 for each TPU core
with SGD optimizer for 90 epochs. The input image is normalized (divided by 255) to be within [0,1].
We randomly divide 50000 validation images into validation set with 25000 images and test set with
25000 images. Note that the same dataset and training mechanisms are used for all the methods that
we compare in the main paper.

For Label Smoothing (LS), we sweep the hyperparameter (cid:15) within the range [0, 0.1] with a step
size 0.01 and ﬁnd that the best calibration performance on the validation set is achieved by setting
(cid:15) = 0.02.

For Adaptive Label Smoothing (AdaLS), we sweep the hyperparameter α in the set
{0.005, 0.01, 0.03, 0.05, 0.1} and set it to be α = 0.03 for the best calibration performance on
the validation set.

We empirically set α = 0.001 for AR-AdaLS in the ﬁrst 60 epochs of the training and then increase
it to 0.05 for the next 30 epochs. The same hyperparameter α is used for AR-AdaLS of Ensemble
without further tuning.

All the ensemble models are a combination of 5 independent models with random initializations.

A.4 CW attacks

To compute the adversarial robustness, we construct (cid:96)2 based CW attacks (Carlini & Wagner,
2017b) following the code at https://github.com/tensorflow/cleverhans/blob/master/
cleverhans/attacks/carlini_wagner_l2.py. Speciﬁcally, we set the binary search steps to
be 3, max iterations to be 500 and learning rate to be 0.005. The generated untargeted CW attacks
can achieve 100% success rate for all the datasets that we consider: CIFAR-10, CIFAR-100 and
ImageNet. We set the number of adversarial robustness training subset and validation subset to be
R = 10 respectively.

B Discussion of AR-AdaLS Combined with Ensembles

In Figure 6, we show ECE and accuracy of all the single-models and their corresponding ensembles
on the clean test and shifted CIFAR-10 and ImageNet. At a high level, we see that all the ensemble
models that we compare have similar accuracy, which are higher than single-models. AR-AdaLS of
Ensemble performs the best across both clean test data and all intensities of shifted data in terms of
calibration.

Looking more closely, some trends emerge: all of the ensemble methods perform relatively well
for highly shifted data (intensity 4–5), but Ensemble of LS, Ensemble of AdaLS, Ensemble of
AR-AdaLS perform much worse on less shifted and clean test data. Digging deeper, we display the
conﬁdence of the predicted class and accuracy of each single model and the corresponding ensemble
models on the clean test set of CIFAR-10 and ImageNet in Figure 7. We can clearly see that the
ensemble models generally increase accuracy and decrease conﬁdence compared to a single model,
which results from the disagreement of the prediction of each single model in ensembles. Therefore,
naive deep ensembles can improve calibration on highly shifted data where single-model is over-
conﬁdent but can harm calibration if applied to a well-calibrated single-model. This is made clearer
in Figure 8: while deep ensembles make over-conﬁdent vanilla model well calibrate, it leads the
well calibrated models, e.g., AR-AdaLS, to be under-conﬁdent. From this perspective, AR-AdaLS of
Ensemble avoids this issue by adaptively adjusting smoothing to keep the ensembles well calibrated
on both clean and shifted dataset.

14

Figure 6: Comparison of ensemble models: ECE and Accuracy on both clean test data and shifted
data on CIFAR-10 and ImageNet. For each intensity of shift, we show the results with a box plot
summarizing the 25th, 50th, 75th quartiles across 19 types of shift on CIFAR-10-C and 15 types of
shift on ImageNet-C. The error bars indicate the min and max value across different shift types.

Figure 7: Comparing accuracy and conﬁdence of the predicted class between single model and the
corresponding ensemble model for each method.

Table 5: Mean of variance (×10−2) across 19 types of shift for CIFAR-10-C and 15 types of shift for
ImageNet-C. Best in bold.

Dataset

Shift Intensity

1

Vanilla
LS
AdaLS
AR-AdaLS

7.85
5.54
5.47
4.21

2

9.69
6.95
6.87
5.06

CIFAR10-C

3

11.2
8.11
7.95
5.73

4

13.1
9.65
9.44
6.66

ImageNet-C

5

Mean

1

16.0
11.8
11.5
8.24

11.6
8.41
8.25
5.98

5.28
4.86
4.79
4.53

2

6.39
5.84
5.77
5.49

3

7.37
6.78
6.66
6.12

4

8.23
7.55
7.51
6.76

5

Mean

8.29
7.41
7.56
6.66

7.11
6.49
6.46
5.91

15

Figure 8: Reliability diagram of accuracy versus conﬁdence of single model and ensemble model on
the clean test of CIFAR-10 and ImageNet. The perfect calibrated model should be aligned with the
diagonal dotted line (above is under-conﬁdent, below is over-conﬁdent).

16

