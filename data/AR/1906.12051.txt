A Taxonomy for Virtual and Augmented Reality in Education 

J. Motejlek 
Research Student 
Department of Chemical and Process Engineering 
University of Surrey, Guildford, Surrey, UK  
E-mail: j.motejlek@surrey.ac.uk 

E. Alpay 
Professor / Director of Learning and Teaching 
Department of Chemical and Process Engineering 
University of Surrey, Guildford, Surrey, UK  
E-mail: e.alpay@surrey.ac.uk 

Conference Key Areas: Innovative teaching and learning methods; Engineering Skills 
Keywords: Virtual Reality; Augmented Reality; Taxonomy 

1 

INTRODUCTION 

[3].  Example 

include  military 

training  applications 

Both  virtual  reality  (VR)  and  augmented  reality  (AR)  have  undergone  considerable 
development  in  recent  years.  Even  though  it  seems  that  we  are  still  in  a  primitive 
technological  stage,  it  is  already  recognised  that  VR/AR  can  provide  exciting 
opportunities  to  support  teaching  and  learning  [1].  There  have  been  numerous 
attempts  to  use  this  technology  in  education  contexts  [2],  in  most  cases  showing 
success 
[4],  engineering 
applications through VR laboratories [5], and history [6] and astronomy [7] education. 
The  possibilities  to  use  VR/AR  transcend  to  other  contexts,  such  as  interactive 
performances, theatre, galleries, discovery centres and so on [8]. The advantage of 
VR  as  an  experimental  and  educational  tool  is  the  ability  to  place  the  participant 
inside  any  scene  with  high  degree  of  immersion  [9].  However,  there  are  also 
examples where educational application has only been partially successful, such as 
the use of 3D anatomy models in medical education [10] or skill transfer in VR based 
microsurgery  training  [11].  Greater  understanding  is  needed  as  to  the  features  of 
such  applications 
learning.  More 
fundamentally though, clarity is needed on the classification of the tools to accurately 
describe e.g. function and design.  

that  are  especially  conducive 

to  student 

In  this  paper,  a  taxonomy  for  VR/AR  in  education  is  presented  that  can  help 
differentiate and categorise education experiences and provide indication as to why 
some  applications  of  fail  whereas  others  succeed.  Examples  will  be  presented  to 

1 

 
 
 
 
 
 
 
 
  
  
 
 
 
illustrate the taxonomy, including its use in developing and planning two current VR 
projects  in  our  laboratory.  The  first  project  is  a  VR  application  for  the  training  of 
Chemical Engineering students (and potentially industrial operators) on the use of a 
physical  pilot  plant 
the  use  of  VR 
cinematography  for  enacting  ethics  scenarios  (and  thus  ethical  awareness  and 
development) pertinent to engineering work situations. 

facility.  The  second  project 

involves 

2 

CLASSIFICATION OF AR/VR IN EDUCATION 

Key  factors  of  the  VR/AR  taxonomy  can  be  summarised  as:  (i)  Purpose  of  the 
application,  (ii)  User  experience,  (iii)  Technology  of  the  delivery,  (iv)  Production 
technology, (v) Gamification type, (vi) User interaction and (vii) System interaction. A 
description of each of these factors is given below. 

2.1  Classification by Purpose 

The  most  important  category  in  the  taxonomy  depends  on  the  nature  of  the 
information being accessed and the intended purpose of this information. Specifically, 
purpose may involve: 

A.  Training 

For training purposes the goal of the application is to convey information about how 
to use a specific real device (especially in case of AR) or its digitised equivalent (as 
in  the  case  of  model-based  VR  and  cinematic  VR).  It  is  usually  very  specific  in 
purpose,  with  the  focus  on  training  for  equipment,  machine  or  process  operation 
rather  than  the  understanding  of  the  underlying  principles  of  design.  There  are 
numerous  examples  of  VR/AR  training  applications  in  education  at  the  moment, 
extensively employed in medical training such as dentistry [12], laparoscopy [13] and 
ophthalmoscopy [14]. 

B.  Teaching 

For  teaching  purposes  the  goal  is  to  prepare  the  student  to  retain  and  understand 
knowledge  in  a  general  situation.  The  student  is  being  exposed  to  theory  and 
underlying  principles,  and  such  knowledge  is  expected  to  be  transferable  to  other 
situations  and  environments.  Currently,  the  number  of  examples  of  successful 
teaching applications is relatively low, as often their development is challenging [15] 
with  success  relying  on  effective  scaffolding  [16]  as  well  as  effective  integration  of 
assessment and feedback. However, some examples of effective applications exist in 
areas of language teaching [17], general lab work [5] and agriculture [18]. 

C.  Observing 

For  observing  purposes,  the  primary  goal  is  to  show  or  convey  information  without 
the  need  for  retaining  or  understanding  it.  In  other  words  an  exhibition  purpose. 
Examples  of  such  applications  can  be  seen  in  the  form  of  historical  recreations  of 
sights  [6]  or  artefacts  [19],  the  latter  allowing  for  example  shared  analysis  and 
research of objects between universities. The development of 3D scanners and video 
has played a key role for such observing purposes. 

2 

 
 
2.2  Classification by user experience and delivery technology 

The  two  factors  of  user  experience  and  delivery  technology  are  closely  related  and 
are discussed together in this section. Currently, the taxonomy identifies three distinct 
user experiences: 

A.  Virtual Reality (VR) Experience 

VR experience completely isolates the user from the outside environment inside an 
immersive  world  [20].  The  experience  can  therefore  transport  the  user  into  both 
reality-simulated and hypothetical environments. 

B.  Augmented Reality (AR) Experience 

AR  systems  combine  (overlay)  virtual  content  (e.g.  generated  through  a  model, 
animation or video recording) with real-world imagery [21]. This occurs in real time as 
the  user  engages  with  the  system,  with  aspects  of  the  surroundings  or  other  real-
world objects registered in 3D [22]. In this way, AR can be used to enhance the real-
word  interaction  and  learning  experience,  helping  to  e.g.  better  elucidate  principles 
and concepts. 

C.  Display Experience 

A  virtual  world  can  be  presented  on  a  standard  2D  screen  /  display,  as  well  as 
through 3D visualisation.  Although this paper (and current technology development) 
focuses  on  the  latter  due  to  the  immersive  experience  potential,  a  standard  display 
experience  does  offer  some  advantages  over  3D  immersion,  such  as  the  relative 
clarity of text and general reading experience, and much less prone to causing user 
dizziness, headache or eyestrain [23]. 

The  user  experience  may  be  delivered  through  two  distinct  hardware  devices:  the 
screen and a stereoscopic head mounted display (HMD). Specific features of these 
delivery devices are summarised below: 

A.  HMD 

HMD allows stereoscopic vision in VR [24] as well as in AR [20]. Stereoscopic vision 
arises  when  two  views  of  the  same  scene  with  binocular  disparity  are  presented  to 
each  eye.  The  effect  depends  on  binocular  fusion  in  order  to  yield  perception  of 
depth [25]. In both cases, the user is also hands-free, i.e. the user does not have to 
hold the device in their hands. 

B.  Screen 

The user uses a stationery (e.g. desktop computer) or hand-held (e.g. tablet) device 
[26], or may in fact be surrounded by the screen as in the case of the CAVE system 
[27]. 

Consideration of both the visual experience and the delivery technology creates the 
VR/AR technology matrix shown in Table 1; demonstrative examples of how current 
commercial VR/AR equipment are categorised within this matrix are also shown. The 
delivery and experience are of course closely related: the type of experience defined 
by the technology of delivery and to some extent its production methods (see below). 

3 

 
 
 
Table 1: User’s experience vs. delivery technology 

Screen 

HMD 

Display 

computer monitor 

Google Glass 

VR 

AR 

simulators; 
panoramic videos 

Vive; Oculus 

iPad 

Meta; HoloLens 

Interestingly, the definition of AR as a 3D registered system [22] has a consequence 
for certain types of devices, such as Google Glass which, according to this taxonomy 
is actually a display experience, even though it resembles an AR experience. Since 
Google Glass does not show information related to what is in front of the user, nor is 
registered in 3D (Google Glass technology does not have the necessary sensors for 
that),  it  is  basically  a  screen  showing  information  similar  to  what  the  smart  phone 
does but is strapped onto the user’s head. Therefore, it should not be considered an 
AR device. 

2.3  Classification by Production Technology 

The production technology defines to an extent the type of delivery technology. In this 
taxonomy, it has been identified that it is possible to produce VR/AR experiences with 
3D modelling, cinematography or combination of both.  

C.  3D Modelling 

3D  modelling  and  generated  computer  graphics  is  the  most  common  approach  to 
develop computer games and by extension serious games for education (see section 
2.4  below).  3D  models  can  be  designed  using  tools,  such  as  Blender  or  using 
photogrammetry using 3D scanners [19]. 

D.  Cinematography 

In  terms  of  cinematography,  the  footage  is  filmed  with  a  specific  field  of  view,  most 
commonly 180 or 360 degrees, which then affects how much the user is surrounded 
by  the  image.  The  footage  may  also  be  filmed  stereoscopically,  i.e.  to  provide  the 
illusion of 3D. 

E.  Mixed 

The two approaches can also be mixed. Indeed, embedding 3D objects within filmed 
footage  is  a  common  technique  in  the  film  industry.  A  blended  approach  with  a 
delivery  method  that  is  screen  based  is  not  novel.  However,  the  use  of  HMD  for 
stereoscopic  footage,  that  has  also  been  enhanced  with  3D  models,  is  a  new 
approach,  and  provides  much  design  potential  for  educational  tools.  It  should  be 
noted that theoretically it is also possible to embed cinematography in a 3D model, 
but  the  authors  know  no  such  current  applications.  Such  video  embedment  could 
involve for example the teacher or real process or equipment footage.  

4 

 
 
 
 
2.4  Gamification 

Gamification  describes  game-inspired  techniques  to  engage  students  within  the 
learning  /  interaction  process.  The  purpose  of  gamification  is  to  increase  student 
motivation for learning or skills development. In order to categorise the different types 
of  gamification,  intrinsic  and  extrinsic  motivation  concepts  have  been  considered  in 
this  work,  as  well  as  the  method  of  integration  of  gamification  elements  into  the 
learning content. 

Extrinsic  motivation  can  be  supported  by  rewards,  and  most  gamification  systems 
focus on this by using e.g. points, levels, leader boards, achievements or badges in 
order to motivate students to engage with learning content. The biggest disadvantage 
of this approach is that when the reward stops, the behaviour may also stop unless 
the student has found some other reason to continue. Reward based gamification is 
suitable  for  immediate  and  short  term-change  and  has  been  observed  to  create  a 
short term spike in user engagement [28]. 

Intrinsic motivation is the motivation which is driven by internal rewards and that do 
not depend on external controls, because they are perceived as inherently interesting 
and  enjoyable  by  the  student  [29].  Research  has  shown  that  extrinsic  rewards  can 
undermine  intrinsic  motivation  [30].  Nevertheless,  some  elements  of  extrinsic  cue 
may help students monitor their level of progress through the learning activity, whilst 
not over-riding (or overwhelming) intrinsic drivers for learning. 

It is possible to either embed game elements into the learning environment [31] or to 
integrate  educational  content  into  a  game  [32].  The  latter  are  also  referred  to  as 
serious games. 

From this conceptual framework, two types of gamification can be derived: 

A.  Reward based gamification 

Adding  elements  such  as  leader  boards,  badges  and  achievements  to  the  learning 
content in order to motivate students to progress through it. This on the whole may 
be seen as extrinsic motivators for the learning application.  

B.  Serious games 

Using game elements to increase students’ internal motivation by adding educational 
content to the game.  

According to [28] there are six elements inspired by game design, that can be used 
to  increase  intrinsic  motivation  within  serious  games:  (i)  mimicking  play  to  facilitate 
the freedom to explore and fail within the boundaries of the game; (ii) the creation of 
stories  for  participants  that  are  integrated  with  the  real  world;  (iii)  giving  student’s 
choices  /  options  that  then  dictate  the  game  plot;  (iv)  giving  user  information  that 
connects  concepts  with  real-world  context;  (v)  encouraging  participants  to  discover 
and learn from other interests in the real-world setting;  and (vi) allowing participants 
to  find  connections  to  other  interests  and  past  knowledge  within  the  game  so  as  to 
deepen engagement and consolidate learning.  

5 

 
 
2.5  User Interaction 

The  design  of  the  VR/AR  application  must  also  consider  the  methods  of  user 
interaction  (e.g.  information  selection  or  exchange)  with  the  user.    This  typically 
involve  tracked  controllers  (e.g.  gloves  or  sticks),  or  if  these  are  not  available  or 
desirable, a simpler application control can be employed in form of gaze control [33], 
and are further defined below: 

A.  Tracked controllers 

Uses general-purpose controllers with buttons to interpret the user input. User points 
the controller in a direction and presses the button which causes the desired reaction 
from  the  system.  Alternatively,  the  same  effect  can  be  achieved  by  tracking  user’s 
bare hands and interpreting gestures. 

B.  Gaze control 

User can see a cross hair in the middle of the viewport, and by moving his/her head 
can  position  the  cross  hair  on  the  desired  user  interface  element.  Action  is  evoked 
either by pressing a button on the HMD or by waiting for certain amount of time (i.e. 
fixed gaze for a 1s or so). 

C.  Special controllers 

In terms of this taxonomy a special controller is a controller for input which cannot be 
replicated  using  a  general-purpose  controller  and  often  employ  precise  simulated 
haptic feedback which helps students learn the required skill [13]. These controllers 
are common in medical training and can include for example virtual endoscopes [34], 
simulators  of  dental  procedures  [12]  or  ophthalmoscopes  [14].  Such  controllers  can 
also be simpler, such as turning wheels for drivers [35]. 

2.6  System Interaction 

The way the system communicates with the user is called system interaction in this 
taxonomy.  It  includes  sophisticated  subsystems  embedded  within  the  content,  often 
based  on  research  in  artificial  intelligence.  It  does  not  include  basic  menu  and 
information  that  the  application  might  include  for  the  user  to  be  able  to  operate  its 
functionality. Two types of system interaction have been identified for inclusion in the 
taxonomy: 

A.  Dialog systems 

According  to  explanation-based  constructivist  theories  of  learning,  learning  is  more 
effective  and  deeper  when  the  learner  must  actively  generate  explanations  than 
when  merely  presented  with  information  [36].  This  theory  is  being  used  by  dialog 
systems, which ask the student to provide explanations of the educational context by 
means  of  menus  or  direct  textual  input.  Effectiveness  of  learning  is  reported  to  be 
higher  when  the  student  is  asked  to  answer  questions  via  direct  textual  input  [37]. 
Dialog systems are successfully used in non-VR/AR educational related applications 
[38] but they are not as easily employed in such form in VR/AR because it is harder 
to  implement  an  effective  method  of  input,  especially  when  the  experience  is 
delivered via HMD [39]. 

6 

 
 
 
B.  Intelligent agents 

Intelligent  agents  are  more  sophisticated  than  dialog  systems  and  interact  with  the 
user in a more complex way than just textual or audio information. The user can see 
their  representation  as  an  avatar  which  can  move  in  the  virtual  space  and  operate 
objects  in  the  virtual  world  [40],  which  adds  life  to  the  virtual  world  and  improves 
immersivity  of  the  VR  application  [41].  Intelligent  agents  can  have  the  same 
effectiveness as human tutoring [42]. 

Interestingly,  a  lack  of  intelligent  agents  was  identified  as  one  of  the  problems  of 
sustaining  user  immersion  and  interest  in  educational  VR  applications  [43]  and  a 
number of authors planned to include such intelligent agents in future work (e.g. [44] 
and [6]). The intelligent agent can have at least three distinct functions that fall within 
the taxonomy: 

a.  Intelligent Agents for Training 

Shows how to perform tasks [40]. An example of this in tutoring is STEVE, which 
is  an  interactive  autonomous  system  designed  to  teach  students  tasks  and 
machinery  operation  related  to  naval  engineering  [40].  The  agent  recognises 
student’s  performance  and  can  correct  them  in  case  they  failed  the  task.  The 
system also has ability to work in a team with more than one student [45]. 

b.  Intelligent Agents for Teaching 

Explains  abstract  concepts  [46].  Designing  an  explanation  style  of  education  is 
not  trivial  [15]  and  without  preexisting  scaffolding  students  may  not  be  able 
progress  in  learning  complex  knowledge  [16].  Software  agents  can  have  a 
significant  influence  on  student  motivation  and  it  is  important  to  ensure  that 
agents facilitate, rather than dominate, the learning process [15]. Another factor to 
consider  is  importance  of  intentionality,  orienting  the  learning  activity  around  a 
problem-based  teaching  exercise  which  might  promote  a  more  intentional 
experience [15]. 

c.  Intelligent Agents for Guiding 

This  involves  guiding  students  in  a  complicated  environment  that  they  are 
learning about so that they do not get lost (navigational guidance). The agent can 
also  be  an  attention  guide  directing  the  student’s  gaze  using  pointing  gestures 
[47].  In  order  to  make  the  models  and  environments  immersive  the  agents  fulfil 
relevant tasks as if in the real world [6]. Such agents might not interact with the 
student and just be part of the simulation in order to increase immersivity. 

2.7  Taxonomy overview 

Based on the above review and discussions, an overview of the entire taxonomy is 
given in Table 2. 

7 

 
 
 
 
 
 
 
 
 
1 
Purpose 

2 
Experience 

Table 2: Taxonomy Overview 
5 
4 
3 
Gamification 
Delivery 
Production 
Technology 
Technology 

1.1 
Training 

1.2 
Teaching 

2.1 
VR 

2.2 
AR 

3.1 
3D Modelling 

4.1 
Screen 

3.2 
Cinema-
tography 

4.2 
HMD 

1.3 
Observing 

2.3 
Screen 

3.3 
Mixed (rare) 

5.1 
Embedded game 
elements 

5.2 
Embedded 
educational 
content (serious 
games) 
5.3 
None 

6 
User 
Interaction 

7 
System 
Interaction 

6.1 
General 
purpose 
controller 
6.2 
Gaze control 

7.1 
Dialog 
system 

7.2 
Intelligent 
Agents 

7.3 
None 

6.3 
Special 
controller 
6.4 
None 

3 

TAXONOMY APPLICATIONS 

3.1  Chemical Process Pilot Plant Education Platform 

A VR application of a Chemical process pilot plant has been developed in the VR lab 
of University of Surrey. Its purpose is for operation training of the actual plant within 
the  same  departments  and  providing  a  research  platform  for  VR  use  in  various 
educational settings. In terms of the taxonomy the platform can be used to develop 
applications  for  all  three  purposes,  and  so  far,  has  been  used  for  training. 
Specifically,  the  current  application  allows  orienteering  around  the  plant,  helping 
students to understand the plant layout as well as recognize key items of equipment 
and instrumentation. The tool therefore enables safe and remote interaction with the 
plant. In terms of the developed taxonomy, the application can be classified as:  

Purpose: Training, Experience: VR, Production Technology: 3D Modelling, Delivery Technology: HMD, 
Gamification: None, User Interaction: Tracked Controller, System Interaction: None 

3.2  Stereoscopic Cinematography Storytelling 

A second application has involved the set-up of a custom stereoscopic camera based 
on BlackMagic 4K cinema studio cameras for recording footage that can be viewed 
with  HMD  or  on  screen.  The  camera  rig  supports  a  viewing  angle  of  220º  which 
eliminates  all  problems  related  to  recording  stereoscopic  360º  videos.  Since 
cinematography  has  higher  potential  for  mobile  device  use  due  to  lower  hardware 
requirements, a gaze user interface was developed in order to provide interactivity to 
the video recordings. 

The  project  has  focussed  on  presenting  students  with  a  story  related  to  chemical 
engineering,  allowing  them  to  make  choices  throughout  the  viewing,  and  eventually 
reaching  an  ethical  dilemma  near  the  conclusion  of  the  story.  The  purpose  of  the 
learning  interaction  is  therefore  for  ethical  awareness  within  a  professional  /  work 
context. In terms of the developed taxonomy, the application can be classified as: 

8 

 
 
 
 
 
 
 
 
 
Purpose: Observing, Experience: VR, Production Technology: Cinematography, Delivery Technology: 
HMD  (both  desktop  and  mobile  grade),  Gamification:  None,  User  Interaction:  Gaze,  System 
Interaction: None. 

4 

DISCUSSION 

For the applications described in section 3 above, future work will allow evaluation of 
their effectiveness with respect to the specific taxonomical features. Specific areas of 
development  (and  research  evaluation)  also  arise  through  consideration  of  the 
taxonomy,  including:  (i)  the  use  of  mixed  cinematography  technology;  (ii)  the 
combined use of HMD and display delivery especially in group work situations (e.g. a 
student using the HMD and other group members viewing the student experience to 
facilitate additional discussion and reflection); and (iii) aspects of the application that 
would  benefit  from  gamification  and  system  interaction.  Moreover,  the  taxonomy 
provides indication as to how the base applications can be evolved for other teaching 
/ training / observing scenarios. For example, a greater teaching rather than training 
purpose  may  be  created  through  the  inclusion  of  relevant  system  interaction 
components.  

In a related manner, it is also important to understand the critical definition of purpose 
in VR/AR application design. Imagine the following example: students are presented 
with a chemical engineering rig in VR, containing various instruments (filters, heaters, 
reactor,  pumps,  etc…),  and  are  asked  to  locate  these  instruments.  It  might  be  that 
the  student  is  learning  what  each  instrument  looks  like,  which  would  suggest  the 
application purpose is teaching. It can be argued that the student is actually learning 
the positions of those instruments, which is specific for the given rig, and its purpose 
could  therefore  be  training  for  subsequent  plant  operation.  Whilst  in  real-word 
situations,  both 
training  elements  may  occur  simultaneously 
(complemented by tutor / demonstrator input, reading material or lectures / tutorials), 
care  is  needed  in  VR/AR  design  to  ensure  the  effective  attainment  of  purpose 
through,  e.g.,  appropriate  production  and  delivery  technologies,  gamification  and 
user  and  system  interaction.  This  could  arguably  lead  to  an  8th  category  in  the 
taxonomy,  stand-alone  application  vs.  complementary  resource.  However,  in  Higher 
in 
Education  contexts, 
engineering) are not disparate to other teaching and learning experiences.  

tools  (especially 

teaching  and 

is  envisaged 

that  most 

learning 

it 

5 

CONCLUSION 

A seven-factor taxonomy for VR/AR in education has been presented. This has been 
constructed  through  consideration  of  current  applications  and  literature,  as  well  as 
consideration of aspects of application purpose, design, interaction and engagement. 
The  taxonomy  provides  a  framework  for  categorising  and  verbalising  educational 
applications  in  VR/AR,  as  well  as  for  identifying  areas  for  specific  (and  novel) 
development and research evaluation.  

9 

 
 
 
6 

[1] 

[2] 

[3] 

[4] 

[5] 

[6] 

[7] 

[8] 

[9] 

[10] 

[11] 

[12] 

[13] 

[14] 

[15] 

[16] 

REFERENCE 

M. Zyda, “Why the VR You See Now Is Not the Real VR,” Presence: Teleoperators and 
Virtual Environments, vol. 25, no. 2, pp. 166–168, Nov. 2016. 

R. C. Douglas-Lenders, P. J. Holland, and B. Allen, “Building a better workforce,” Education + 
Training, vol. 59, no. 1, pp. 2–14, Jan. 2017. 

C.-W. Shen, J.-T. Ho, T.-C. Kuo, and T. H. Luong, “Behavioral Intention of Using Virtual 
Reality in Learning,” presented at the the 26th International Conference, New York, New 
York, USA, 2017, pp. 129–137. 

F. Pallavicini, L. Argenton, N. Toniazzi, L. Aceti, and F. Mantovani, “Virtual Reality 
Applications for Stress Management Training in the Military,” The Journal of Strength and 
Conditioning Research, vol. 29, pp. S77–S81, Oct. 2016. 

S. Ren, F. D. McKenzie, S. K. Chaturvedi, R. Prabhakaran, J. Yoon, P. J. Katsioloudis, and 
H. Garcia, “Design and Comparison of Immersive Interactive Learning and Instructional 
Techniques for 3D Virtual Laboratories,” Presence: Teleoperators and Virtual Environments, 
vol. 24, no. 2, pp. 93–112, May 2015. 

J.-B. Barreau, F. Nouviale, R. Gaugne, Y. Bernard, S. Llinares, and V. Gouranton, “An 
Immersive Virtual Sailing on the 18 th-Century Ship Le Boullongne,” Presence: Teleoperators 
and Virtual Environments, vol. 24, no. 3, pp. 201–219, Jul. 2015. 

J.-C. Yen, C.-H. Tsai, and M. Wu, “Augmented Reality in the Higher Education: Students’ 
Science Concept Learning and Academic Achievement in Astronomy,” Procedia - Social and 
Behavioral Sciences, vol. 103, pp. 165–173, Nov. 2013. 

I. Schlacht, A. Mastro, and S. Nazir, “Advances in Applied Digital Human Modeling and 
Simulation,” presented at the AHFE International Conference on Digital Human Modeling and 
Simulation, Florida, USA, 2016, pp. 1–323. 

C. J. Wilson and A. Soranzo, “The Use of Virtual Reality in Psychology: A Case Study in 
Visual Perception,” Computational and Mathematical Methods in Medicine, vol. 2015, no. 1, 
pp. 1–7, 2015. 

C. Moro, Z. Štromberga, A. Raikos, and A. Stirling, “The effectiveness of virtual and 
augmented reality in health sciences and medical anatomy.,” Anat Sci Educ, vol. 10, no. 6, 
pp. 549–559, Nov. 2017. 

C. Våpenstad, E. F. Hofstad, L. E. Bø, E. Kuhry, G. Johnsen, R. Mårvik, T. Langø, and T. N. 
Hernes, “Lack of transfer of skills after virtual reality simulator training with haptic feedback.,” 
Minim Invasive Ther Allied Technol, vol. 26, no. 6, pp. 346–354, Dec. 2017. 
E. Roy, M. M. Bakr, and R. George, “The need for virtual reality simulators in dental 
education: A review,” The Saudi Dental Journal, vol. 29, no. 2, pp. 1–7, Mar. 2017. 

G. Wynn, P. Lykoudis, and P. Berlingieri, “Development and implementation of a virtual 
reality laparoscopic colorectal training curriculum.,” Am. J. Surg., Dec. 2017. 

A. S. Wilson, J. O'Connor, L. Taylor, and D. Carruthers, “A 3D virtual reality ophthalmoscopy 
trainer.,” Clin Teach, vol. 14, no. 6, pp. 427–431, Dec. 2017. 

J. Holmes, “Designing agents to support learning by explaining,” Computers & Education, vol. 
48, no. 4, pp. 523–547, May 2007. 

E. B. Coleman, “Using Explanatory Knowledge During Collaborative Problem Solving in 
Science,” Journal of the Learning Sciences, vol. 7, no. 3, pp. 387–427, Jul. 1998. 

10 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
[17] 

[18] 

[19] 

[20] 

[21] 

Y. Shih and M. Yang, “A Collaborative Virtual Environment for Situated Language Learning 
Using VEC3D,” pp. 1–13, Mar. 2008. 

B. Laurel, “AR and VR: Cultivating the Garden,” Presence: Teleoperators and Virtual 
Environments, vol. 25, no. 3, pp. 253–266, Dec. 2016. 

T. Nicolas, R. Gaugne, C. Tavernier, Q. Petit, V. Gouranton, and B. Arnaldi, “Touching and 
Interacting with Inaccessible Cultural Heritage,” Presence: Teleoperators and Virtual 
Environments, vol. 24, no. 3, pp. 265–277, Jul. 2015. 

D. Schmalstieg and T. Hollerer, “Augmented Reality: Principles and Practice,” pp. 1–84, May 
2016. 

E. Klopfer and K. Squire, “Environmental Detectives—the development of an augmented 
reality platform for environmental simulations,” Education Tech Research Dev, vol. 56, no. 2, 
pp. 203–228, Apr. 2007. 

[22] 

R. T. Azuma, “A Survey of Augmented Reality,” pp. 1–48, Aug. 1997. 

[23] 

[24] 

[25] 

[26] 

[27] 

[28] 

[29] 

[30] 

[31] 

[32] 

J. Penczek, S. G. Satterfield, E. F. Kelley, T. Scheitlin, J. E. Terrill, and P. A. Boynton, 
“Evaluating the Optical Characteristics of Stereoscopic Immersive Display Systems,” 
Presence: Teleoperators and Virtual Environments, vol. 24, no. 4, pp. 279–297, Nov. 2015. 

P. Fuchs, “Virtual Reality Headsets – A Theoretical and Pragmatic Approach,” pp. 1–214, 
Jan. 2017. 

K. S. Hale and K. M. Stanney, Handbook of Virtual Environments, vol. 34, no. 7. CRC Press, 
2014, pp. 1458–754. 

C. Pensieri and M. Pennacchini, “Overview: Virtual Reality in Medicine,” Journal For Virtual 
Worlds Research, vol. 7, no. 1, Jan. 2014. 

G. Bruder, F. Argelaguet, A.-H. Olivier, and A. Lécuyer, “CAVE Size Matters: Effects of 
Screen Distance and Parallax on Distance Estimation in Large Immersive Display Setups,” 
Presence: Teleoperators and Virtual Environments, vol. 25, no. 1, pp. 1–16, Jul. 2016. 

S. Nicholson, “A RECIPE for Meaningful Gamification,” in Gamification in Education and 
Business, no. 1, Cham: Springer International Publishing, 2014, pp. 1–20. 

R. M. Ryan and E. L. Deci, “Toward a Social Psychology of Assimilation: Self-Determination 
Theory in Cognitive Development and Education,” in Self-Regulation and Autonomy, no. 9, B. 
W. Sokol, F. M. E. Grouzet, and U. Muller, Eds. Cambridge: Cambridge University Press, 
2013, pp. 191–207. 

R. M. Ryan and E. L. Deci, “Self-determination theory and the facilitation of intrinsic 
motivation, social development, and well-being.,” American Psychologist, vol. 55, no. 1, pp. 
68–78, 2000. 

B. Monterrat, M. Desmarais, É. Lavoué, and S. George, “A Player Model for Adaptive 
Gamification in Learning Environments,” in A Tutorial Dialogue System for Real-Time 
Evaluation of Unsupervised Dialogue Act Classifiers: Exploring System Outcomes, vol. 9112, 
no. 30, Cham: Springer International Publishing, 2015, pp. 297–306. 

P. Lameras, S. Arnab, I. Dunwell, C. Stewart, S. Clarke, and P. Petridis, “Essential features 
of serious games design in higher education: Linking learning attributes to game mechanics,” 
Br J Educ Technol, vol. 48, no. 4, pp. 972–994, May 2016. 

[33] 

D. Murphy, “Virtual Reality is ‘Finally Here’: A Qualitative Exploration of Formal Determinants 
of Player Experience in VR,” presented at the Proceedings of DiGRA, 2017. 

11 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
[34] 

[35] 

[36] 

[37] 

[38] 

[39] 

[40] 

[41] 

[42] 

[43] 

[44] 

[45] 

[46] 

P. A. Baier, J. A. Baier-Saip, K. Schilling, and J. C. Oliveira, “Simulator for Minimally Invasive 
Vascular Interventions: Hardware and Software,” Presence: Teleoperators and Virtual 
Environments, vol. 25, no. 2, pp. 108–128, Nov. 2016. 

I. Milleville-Pennel and C. Charron, “Driving for Real or on a Fixed-Base Simulator: Is It so 
Different? An Explorative Study,” Presence: Teleoperators and Virtual Environments, vol. 24, 
no. 1, pp. 74–91, Feb. 2015. 

V. Aleven and K. R. Koedinger, “An effective metacognitive strategy - learning by doing and 
explaining with a computer-based Cognitive Tutor.,” Cogn Sci, vol. 26, no. 2, pp. 147–179, 
2002. 

V. Aleven, A. Ogan, O. Popescu, C. Torrey, and K. Koedinger, “Evaluating the Effectiveness 
of a Tutorial Dialogue System for Self-Explanation,” in Intelligent Tutoring Systems, vol. 3220, 
no. 42, Berlin, Heidelberg: Springer, Berlin, Heidelberg, 2004, pp. 443–454. 

A. C. Graesser, P. Chipman, B. C. Haynes, and A. Olney, “AutoTutor: An Intelligent Tutoring 
System With Mixed-Initiative Dialogue,” IEEE Trans. Educ., vol. 48, no. 4, pp. 612–618, Nov. 
2005. 

D. A. Bowman, C. J. Rhoton, and M. S. Pinho, “Text Input Techniques for Immersive Virtual 
Environments: An Empirical Comparison,” Proceedings of the Human Factors and 
Ergonomics Society Annual Meeting, vol. 46, no. 26, pp. 2154–2158, 2002. 

J. Rickel and W. L. Johnson, “STEVE - A Pedagogical Agent for Virtual Reality.,” Agents, pp. 
332–333, 1998. 

M. Ochs, N. Sabouret, and V. Corruble, “Simulation of the Dynamics of Nonplayer 
Characters' Emotions and Social Relations in Games,” IEEE Trans. Comput. Intell. AI 
Games, vol. 1, no. 4, pp. 281–297, Jan. 2010. 

K. VanLEHN, “The Relative Effectiveness of Human Tutoring, Intelligent Tutoring Systems, 
and Other Tutoring Systems,” Educational Psychologist, vol. 46, no. 4, pp. 197–221, Oct. 
2011. 

E. Champion, “Defining Cultural Agents for Virtual Heritage Environments,” Presence: 
Teleoperators and Virtual Environments, vol. 24, no. 3, pp. 179–186, Jul. 2015. 

C. Dede, M. C. Salzman, and B. Loftin, “ScienceSpace: Virtual realities for learning complex 
and abstract scientific concepts,” ieeexplore.ieee.org, Jan. 1996. 

J. Rickel and W. L. Johnson, “Virtual humans for team training in virtual reality,” Citeseer, Jul. 
1999. 

C. Y. Chou, T. W. Chan, C. L. C. Education, 2003, “Redefining the learning companion: the 
past, present, and future of educational agents,” Elsevier, vol. 40, no. 3, pp. 255–269, Apr. 
2003. 

[47] 

J. Rickel, “Intelligent Virtual Agents for Education and Training - Opportunities and 
Challenges.,” IVA, vol. 2190, no. 2, pp. 15–22, 2001. 

12 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
