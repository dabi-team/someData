Integrating Artiﬁcial Intelligence and Augmented Reality in Robotic
Surgery: An Initial dVRK Study Using a Surgical Education Scenario

Yonghao Long, Jianfeng Cao, Anton Deguet, Russell H. Taylor, and Qi Dou

2
2
0
2

r
a

M
3

]

O
R
.
s
c
[

2
v
3
8
3
0
0
.
1
0
2
2
:
v
i
X
r
a

Abstract— Robot-assisted surgery has become progressively
more and more popular due to its clinical advantages. In the
intelligence and augmented reality
meanwhile, the artiﬁcial
in robotic surgery are developing rapidly and receive lots of
attention. However, current methods have not discussed the
coherent integration of AI and AR in robotic surgery. In this
paper, we develop a novel system by seamlessly merging arti-
ﬁcial intelligence module and augmented reality visualization
to automatically generate the surgical guidance for robotic
surgery education. Speciﬁcally, we ﬁrst leverage reinforcement
leaning to learn from expert demonstration and then generate
3D guidance trajectory, providing prior context information
of the surgical procedure. Along with other information such
as text hint, the 3D trajectory is then overlaid in the stereo
view of dVRK, where the user can perceive the 3D guidance
and learn the procedure. The proposed system is evaluated
through a preliminary experiment on surgical education task
peg-transfer, which proves its feasibility and potential as the
next generation of robot-assisted surgery education solution.

I. INTRODUCTION

Artiﬁcial intelligence (AI) and augmented reality (AR)
are two important and increasingly essential techniques to
be developed for next-generation robotic surgery. So far, AI
and AR have individually focused on different perspectives.
In speciﬁc, AI concentrates on recognizing and planning
surgical activities in a way similar to what surgeons could
do, based on computation and analysis of collected sensory
data such as endoscopic videos [1]–[3] and robotic kine-
matics [4]–[6]. Recent advances of AI have substantially
enhanced a number of tasks such as situation awareness
of surgical procedures [7], [8] and automation of some
actions with surgical robots [9], [10]. In the meanwhile,
AR aims to augment the surgical environment in a way to
facilitate surgeon’s operation and decision-making, based on
visualization and integration of additional information that is
computed ofﬂine or in real-time [11], [12]. Equipped with an
immersive view in surgical robotic console, AR has shown
effectiveness for the education of novice surgeons [13]–[15]
and is envisaged to be very helpful if could be adopted intra-
operatively [16].

Unfortunately, to date, the advantages of AI and AR have
not been merged in a sensible way for robotic surgery. The
intriguing combination of AI and AR emerges as a versatile
topic and has been exempliﬁed in a number of application

This project was supported by CUHK Shun Hing Institute of Advanced
Engineering (project MMT-p5-20), Hong Kong RGC TRS Project No.T42-
409/18-R, and InnoHK Multi-Scale Medical Robotics Center.

Y. Long, J. Cao and Q. Dou are with the Department of Computer Science
and Engineering, The Chinese University of Hong Kong. A. Deguet and R.
H. Taylor are with the Department of Computer Science, The Johns Hopkins
University. Corresponding author: Qi Dou (qidou@cuhk.edu.hk)

Fig. 1.
The integrated framework of AI and AR in robotic surgery
on surgical education scenario of peg-transfer. The AI learns policy in a
stimulated environment based on reinforcement learning and predict the
guidance trajectory, which will be further visualized in the dVRK stereo
viewer in the form of AR overlaid trajectory.

scenarios, such as games [17], driver training [18], [19] and
virtual patients [20], [21]. Conferring intelligence on AR
not only can boost the virtual experience but also harnesses
the strong power of learning-based algorithms in demanding
tasks such as surgical education. However, there have been
few attempts in combining AI and AR for surgical robots.
Some authors [22]–[24] have proposed to use computer
vision models to localize the anatomy regions of interest,
and then superimpose the results in the view of camera.
Nonetheless, these solutions only account for presenting ex-
isting clues perceptively, without shedding lights on human-
like decision behaviors. Concurrently, reinforcement learning
(RL) is widely recognized as an effective method for skill
learning [25]–[28], but its potential is not fully exploited in
the surgical robotics domain. One interesting scenario for
exploring these issues is surgical education, in which the
RL-based smart agent is expected to reason about surgical
task and generate constructive guidelines for the novice.
Such embodied intelligence is promising to signiﬁcantly
increase accessibility and reduce the cost of surgical training.
Realisation of the intelligent guidance, in the form of AR
visualization on surgical robotics platforms, could further
enhance usability and user experience, yet how to achieve
this goal remains unclear.

AILearnedPolicyExperimentEnvironmentCameraViewReinforcementLeaningDeploymentAROverlayWorldCoordinateCameraCoordinateInstrumentTipCoordinateWorldCoordinateWorldCoordinateInstrumentTipCoordinateInstrumentTipCoordinatePredictedTrajectoryIn: AugmentedVideo & ControlCommandOut: Video/KinematicsAI+AR Computer(with other input device)Surgical Robotics 
 
 
 
 
 
In this paper, we aim to seamlessly integrate AI and AR,
by augmenting RL-driven instrument movement trajectories
with real-time AR visualizations, and implement the pipeline
for a surgical education scenario using da Vinci Research Kit
(dVRK), as shown in Fig. 1. For the AI-enabled analysis,
reinforcement learning is employed to learn a policy from
the expert demonstration and through the interaction with
the environment. Then the system can reason about the future
action based on the current observation. This behavior can be
embedded into an education process, where the automatically
predicted actions could serve as helpful information to guide
the trainee’s movement step-by-step. Subsequently, how to
effectively visualize the information becomes very important,
especially when incorporating with robotic platforms in real-
time. We will demonstrate the feasibility of overlaying the
3D guidance trajectory within the stereo video via the dVRK
console. By projecting the 3D location of the trajectory
generated from RL policy to the stereo video frames in
the dVRK console, we can vividly observe the surgical
scene with overlaid trajectory for the education purpose.
We have implemented and evaluated our method on the
typical surgical education task of peg-transfer. To the best
of our knowledge, this is the ﬁrst work in exploring synergy
of AI and AR with RL-based prediction and dVRK-based
visualization for surgical education scenario. We hope to
generate discussion and spark the potential of integrated
beneﬁts of AI and AR for surgical education and beyond.

II. RELATED WORK

A. Artiﬁcial Intelligence in Robotic Surgery

The application of artiﬁcial intelligence in robotic surgery
has been actively studied over the last decade [29] since
the da Vinci Surgical System being clinically introduced
in 2000 [30]. Several
topics have been found important
and studied widely such as surgical instrument segmenta-
tion [31], gesture recognition [6], workﬂow recognition [1]
and surgical scene reconstruction [32]. They could support
intra-operative decision [33] and provide valuable database
training and evaluation [34]–[36]. Although
for surgical
promising, these works just provide supplementary infor-
mation without conceiving surgical plan, such as predicting
the trajectories of the surgical
the
emergence of reinforcement learning opens the door to a
new set of policy-based learning strategies [37]. The efﬁcacy
of reinforcement
learning has been revealed in surgical
gesture classiﬁcation [38], [39], surgical scene understanding
[40], [41] and robot learning [27], [42]. Through learning
from expert demonstration, the RL agent could automatically
generate meaningful solutions according to the task at hand.
For example, in [27] and [28], the authors propose to use
Deep Deterministic Policy Gradients (DDPG) with Behavior
Cloning (BC) to conduct surgical task of bimanual needles
regrasping and autonomous blood suction. Both of them
demonstrate encouraging results, showing that RL-based
framework may potentially alleviate the demands on expert’s
guidance [43]–[45].

instrument. Recently,

Apart from the above, there have been some works using
AI for surgical education, such as providing metrics and
performance feedback based on the training records [46]–
[48] and differentiate expertise levels taking the stylistic char-
acteristics into account [49]–[51]. But few of them consider
its application to AR surgical education, where integrated
AR and AI is highly demanded to guide the trainee vividly
and automatically. Given the superiority of RL introduced
above, it is compelling to integrate RL into AR as an essential
AI module, for example, a decision-maker [52], [53], which
encourages the AR system to generate content objectively.

B. Augmented Reality for Robotic Surgery

Augmented reality has been applied to robotic surgery in
a variety of paradigms [12]. In intraoperative applications,
augmentations are superimposed in real-time to offer as-
sistance: (i) enhance depth perception [54]–[56], (ii) com-
pensate tactile sensory [57], (iii) expand ﬁeld of view [58],
(iv) provide more intuitive human-machine interface [54],
and (v) annotate helpful cues [59], [60]. Other applications
utilize augmented reality for robotic surgery training [61].
The common displaying media of augmented reality for
robotic surgery include the da Vinci console, the computer
monitor, and head-mounted display [12]. For the purpose of
surgical education, head-mounted display-based augmented
reality is an advantageous medium as it enables 3D display
and interactions for multiple users and the environment.
Jarc et al. apply augmented reality to a clinical-like training
scenario, where 3D semitransparent tools controlled by the
proctor are augmented and overlaid in the trainee’s console
as guidance [62]. User study involving seven proctor-trainee
pairs is conducted, where they demonstrate the augmented
tools as an effective mentoring approach. However, currently
most of the AR system have not
taken AI as the core
component for generating and creating the context-aware
information.

A. Overall Framework

III. METHOD

The whole system is based on the dVRK [63] platform,
the ﬁrst standard and general da Vinci surgical system
that has been open-sourced and further developed for other
researchers to explore [29]. It has been widely used for the
research on surgical imaging and perception [6], control and
hardware design [64], system simulation [65] and surgical
task automation [66], which substantiate the high reliability
and ﬂexibility of the platform. In this work, we propose
to fully leverage the advantage of dVRK to integrate the
augmented reality and artiﬁcial intelligent to form a robotic
surgery education system as shown in Fig. 2. In total, our
framework consists of several parts as described below:

1) AI+AR Computing Server, as shown in Fig. 2 a): A
high performance computer equipped with a high-end
GPU for AR and AI algorithms deployment, which
can acquire the stereo video and kinematics information
from Patient Side Manipulator (PSM) and Endoscopic
Camera Manipulator (ECM) on dVRK, receives input

Fig. 2. The pipeline of our proposed robot-assisted surgery education system based on dVRK. The computing server a) can acquire the stereo video
and kinematics b) from the dVRK PSM/ECM d) and input the command to control the movement of dVRK PSM. With the AR and AI algorithms, the
computer can overlay the AI and AR information on the video c) and then import the augmented video to the dVRK console e) for user to view.

from other user interface, train and deploy RL policy
or other AI framework and output the augmented video
stream to monitor or other display devices.

2) Endoscope Video Acquisition and Kinematics Control:
As depicted in Fig. 2 b) and d). The dVRK contains
a stereo endoscope on ECM which can capture the
stereo video stream for 3D sensing and perception. In
this case, we collect the video signal from the dVRK
stereo endoscope with a video capture card to convert
the video signal to the USB video stream, which can
be retrieved by the computer. By using the Application
Programming Interface (API) provided by dVRK Robot
Operating System (ROS) packages, users can acquire
the kinematics information (including tool tip position,
velocity, rotation) from two PSMs on dVRK and input
kinematics to control the movement of the PSMs. Also,
the PSMs can be control directly by user using hands
in the dVRK console.

3) Displaying AR video with dVRK console: As depicted
in Fig. 2 c) and e). Equipped with a stereo viewer in
the dVRK console, users can view on the scene with
overlaid AR and AI information from the computer in
the surgical scenario. The stereo viewer can provide
left and right views with disparity, where human can

perceive the surgical video and overlaid information
with 3D feelings. This function is later developed into
TilePro™ [67] in the following generation of da vinci
surgical robot, which showcases the potential possibility
of applying on the real robotic surgery.

B. Generating AI Guidance with Reinforcement Learning

Instead of guiding the novice surgeon by experienced
expert, our work proposes to generate guidance through
advanced reinforcement learning automatically. Considering
the intriguing superiority of SurRoL [65] in leaning from
demonstration and trials on dVRK, we attempt to incorporate
it as our core AI module,
learning and generating the
guidance trajectory using the RL algorithm based on speciﬁc
task.

SurRoL incorporates reinforcement learning to teach an
intelligent agent to take interactive actions so that the cu-
mulative reward can be maximized. While the agent takes
action at ∈ A and interacts with the environment at state
it receives reward rt ∈ R based on whether
st ∈ S,
the agent achieves the goal g at step t. From step t =
0 to t = T ,
the trajectory can be formulated as τ =
{s0, r0, a0, s1, r1, a1..., sT , rT , aT }. Besides, the translation
p(st|st+1) is derived from the underlying physics engine.

SurgicalInstrumentCameraViewdVRKPSM/ECMAugmented Reality1. Text Information Overlay2. RL-powered TrajectoryGuidanceAI+ARComputingServerLeftVideoKinematicsInput: Multi-modal DataOutput: ControlCommanddVRKConsoleStereoEndoscopeUserControldVRKStereo ViewerRightVideoa)b)d)Output: VideowithAROverlayPolicyRCMe)c)xyzis set

the reward rt

For robotic surgery, we deﬁne at as six degrees of freedom,
including position movement (dx, dy, dz) in Cartesian space,
orientation dyaw/dpitch in a top-down/vertical space setting
and j for the open (j ≥ 0) or close (j < 0) status
of the jaw. Reinforcement learning targets on ﬁnding the
policy π to generate action at = π(st). To promote the
to be goal-based,
learning process,
where the success function f (st, g, at) would determine the
reward rt by checking whether at achieves the ground-
truth status, such as 3D positions and 6D poses. Such
scenario is similar to the attempting-feedback procedure in
the surgical education. Finally, the target policy π is learned
by maximizing the experimental expectation Eπ[(cid:80)T
t=0 γtrt],
where γ ∈ [0, 1) denotes the discount factor to balance the
agent’s attention on distant future and immediate future. In
speciﬁc, we opt for a sample efﬁcient learning algorithm
called hindsight experience replay (HER) [68] and combine it
with Q-ﬁltered behavior cloning. In practice, we will leverage
a small amount of demonstration data generated from the
scripted policies as the demonstration for imitation leaning.
It may have great potential to be extensively developed and
learn from large amount of surgery data and then generate
the AI-powered guidance for surgical education.

C. Real-time 3D Visualization by Augmented Reality

To facilitate the visualization result and form a more
intuitive education, we propose to use the AR techniques
to design a 3D visualization system for surgical robots.

1) Different Coordinate System: As an essential part
of the AR system, the coordinates and the transformation
among them serve as the basis of how we locate and align
the AR information to camera view of the stereo endoscope.
In our surgical education scenario, we denote the world
coordinate as {W }, the coordinate of multiple objects in
the experiment environment as {Oi} with i indicating the
index of the objects, Remote Center of Motion (RCM shown
in Fig. 2 (b), which is a ﬁxed base frame of the dVRK
PSM during the operation) as {R}, the instrument tip as
{P } and the endoscope coordinate as {C}. And we use
T dst
src to denote as the transformation from source {src} to
destination coordinate {dst}. During the deployment, T R
W are
ﬁxed and the same, and we manually place the objects in the
same location as in the SurRoL. When given the kinematics
information, we can compute the transformation of T P
R using
the forward kinematics theory and control the PSM to move
the instrument tip to the target pose {P } in the SurRoL and
the real environment as below. If we have the transformation
of the endoscope relative to the world T C
W , we can calculate
and get the position of the object relative to the endoscope
T O
C using the following equation:
C = T O
T O

C = T C
T W
W

W ∗ T W
C ;

(1)

−1

2) Human-involved Flexible Calibration: Calibration pro-
cess is to acquire the coordinate of the endoscope relative
to the world T C
is an very important step [69] for
AR, because it can align the AR information with the
environment. Some methods have been proposed to calibrate

W . It

the hand-eye coordinate [70], [71], but they needs at least
an expert on robotic vision who use the calibration board
to complete the whole process. Using the QR code to
automatically retrieve the coordinate is another attempt and
solution [72], but we need to carefully take the detecting
condition into consideration and it may be restricted in the
surgical scene. In our work, we propose an user-friendly
method to easily calibrate the endoscope coordinate, which
is more suitable for surgical education scenario. Speciﬁcally,
we will ﬁrst deﬁne some speciﬁc points and collect the
3D position [X i
W ] of these points from SurRoL
(i denotes the point index). Then we manually locate the 2D
location [ui, vi] of theses points in the image. We design an
User Interface (UI) as depicted in Fig. 3, where users can
easily locate and record the 2D location by clicking on the
image using the mouse. According to the pinhole model of
the camera, we can have the following equation:

W , Y i

W , Z i





ui
vi
1

K =





fx
0
0


 = K ∗ J ∗ T C
W







(2)







X i
W
Y i
W
Z i
W
1

0
fy
0

cx
cy
1



 ; J =


1
0

0

0
1
0





0 0
0 0
1 0

where J represents the perspective projection model and K
is the camera intrinsic matrix which can be straightforwardly
acquired through [73]. To solve and acquire the solution for
T C
W , we leverage the stable method called iterative solvepnp
algorithm [74]. It is based on Levenberg-Marquardt opti-
mization which minimizes re-projection error and ﬁnds the
best solution. The algorithm needs only 3 points for getting
the solutions (more points contributes to better solution),
which is convenient and efﬁcient for surgical application.

3) Augmented 3D Overlaying: To overlay the 3D infor-
mation on the image, we will ﬁrst place the AR overlay
information (3D) in the environment relative to the world co-
ordinate T AR
W , Z AR
W , where we know the position [X AR
W ].
Then we will project the overlay information to the image us-
ing the projection equation same as the Eqn. (2). Speciﬁcally,
we sometimes may need different views for AR overlaying.
And we can use the proposed calibration method to calibrate
the coordinate and overlay the new generated AR information
relative to the new novel view. This is very promising as in
the surgical education scenario, multiple views may enrich
the information and help the surgeon to learn better.

W , Y AR

D. Integrating AI and AR for Trajectory Guidance

With the AI and AR components introduced above, we
further propose to integrate these two modules to form
a AI-powered trajectory guidance with AR visualization.
Firstly, the dVRK stereo endoscope will capture the stereo
video from the surgical scenario and output for computing
server. Then we will use the proposed calibration method
to calculate and align the coordinate of the endoscope and
world. We will then train the RL policy to learn on the
speciﬁc tasks in the SurRoL. Given a well-learned policy

Fig. 3. Our proposed human-involved calibration process where we can
ﬂexibly locate the speciﬁc points on the peg board for calibrating.

Fig. 4.
transfer (top) and the short range transfer (bottom).

Two trajectories overlay visualization results with long range

and providing with the known state of the environment or
the observation, we can generate the predicted trajectory
from the action, which is represented in the coordinate of
the world. Afterwards, we can project the trajectory points
to the video frames using the projection equation. By lining
up the points, we can ﬁnally generate a trajectory guidance.
With the augmented video displayed in the stereo viewer of
dVRK console, the user can perceive the 3D AR trajectory
guidance for the purpose of robotic surgery education.

IV. EXPERIMENT

To study the preliminary results of the proposed method,
we choose the peg-transfer task as the experimental task in
our work, which is one of the Fundamentals of Laparoscopic
Surgery (FLS) tasks for surgical skills education [75]. In this
experiment, we move the block from one peg to the other
peg with one PSM on the dVRK. The whole process contains
three procedures: 1) lifting the peg, 2) transferring the peg
and 3) placing the peg, as depicted in Fig. 4. We train the
RL policy on this task and generate the block transferring
trajectory based on the trained model. Then we overlay the
peg-transfer path as the AR trajectory on the stereo viewer
for visualization.

We conduct the experiment on a computer using ubuntu
18.04 as the system, which is equipped with 8 cores 3.60GHz
Intel® Xeon(R) W-2123 CPU, NVIDIA® TITAN RTX GPU
and 16G RAM. All the algorithms are written in Python. The
image resolution from the endoscope we use is 640 × 480.

A. Reinforcement Learning on Peg-transfer

In this work, we follow the peg-transfer setting in SurRoL
[65] where the goal tolerate distance is set as 0.5 cm in a
workspace with size of 10 cm2. Each episode lasts for 50
timesteps. When the episode ends, PSM reaches out of the

TABLE I
THE EVALUATION OF Peg-Transfer ON SURROL.

Distance
Short range
Long range
All

Trials
463/500
401/500
864/1000

Success Rate (%)
92.6
80.2
86.4

workspace or the peg-transfer is completed, the environment
will be reset and the positions of initial peg and target peg
will be resampled. Two evaluation settings are proposed, (1)
short range: the block moved by user will not encounter any
obstacle peg during the transfer; (2) long range: the block
needs to avoid 1 to 3 pegs to ﬁnally reach the target peg, as
these two situations are similar to simple and complicated
surgical scenario for education. We randomly generate 100
scripted demonstrations for RL to imitate and train for 100
epochs in the training stage. In the testing stage, for each
experiment, we conduct the peg transfer for 1000 trials (with
500 short range and 500 long range trials) and evaluate
the success rate. We select top 3 performed RL policy and
average their results. The results are shown in the Table I,
where we can ﬁnd that the RL policy can perform well on
long range while achieving a higher performance on the short
range peg-transfer with success rate 92.6%. Overall, our RL
can achieve promising result with average success rate 86.4%
on peg-transfer.

At the deployment stage, we export the complete predicted
trajectory from the RL policy and input the trajectory lo-
cation to control the PSM of real dVRK to conduct peg-
transfer. We show a failure case from the long range setting
in Fig. 5 a), where the block is not placed to the target peg
properly because of the biased trajectory generated from the
RL policy. Although there exists failure trial, the above re-
sults already bring out a lot of possibilities of applying more
advanced RL algorithms on the complex robotic surgery
education scenario.

B. Visualization and Analysis on the AR results

In the calibration process, we choose the top center points
of the twelves pegs as the speciﬁc points [ui, vi] for solving
the coordinate of the endoscope. As we can see in Fig. 3,
the speciﬁc points are located and shown in red dot. The
point locations are shown next to these points with white
text. After pointing out 12 points on the peg-transfer board,
we can ﬁnish calibrating the coordinate.

As shown in Fig. 4, we show two cases from our ex-
periments, with the top one showing the peg-transfer with
long range distance and the bottom one showing the short
range distance. In the experiment, we deploy our RL policy
to generate the 3D trajectory and project the trajectory to the
video frames. As we can see in the ﬁgure, the white dots in
the image frame represent each action step in the trajectory,
and the green lines represent the trajectory. It can be observed
that our method can intuitively and accurately overlay the
trajectory in the video frames, where the instrument tip will
follow the overlaid trajectory visually in the experiment.

To further analyse the stability and precision of the cal-
ibration, we randomly ﬁnd 5 new users with engineering

StartingthecalibrationLocating the specific pointsFinishChoosingallthepointLongrangetrajectoryShortrangetrajectory1. Lifting the Peg2. Transferring the Peg3. Placing the PegTABLE II
THE EVALUATION OF AR CALIBRATION ON Peg-Transfer SCENARIO.

Re-project error (pixel)
Average value
Standard deviation

User1
0.77
0.20

User2
1.02
0.14

User3
0.70
0.12

User4
0.61
0.10

User5
0.71
0.15

(a) Illustration for evaluating video capturing and displaying latency

Fig. 6.
and (b) chart showing the latency related to the point size.

own observation among all the trials, the latency is delicate
enough that the human would not be able to perceive and
will not signiﬁcantly affect the operating.

To further evaluate the overlaying latency, we conduct
an experiment which measures the computing time of over-
laying (projecting 3D trajectory points to the video frames
and visualize them). We evaluate the time cost when given
different size of trajectory points (200-5000), as shown in
Fig. 6 (b), where we randomly generate the 3D locations of
these points and repeat the experiment for 300 times. Finally
we report the average and standard deviation of the time cost.
From the ﬁgure we can observe that when given more points,
the computation time may increase showing exponential
growth. When around or less than 2600 points, we can
achieve the real-time process (> 30 Hz). In our peg transfer
scenario, the RL will generate around 100 trajectory points
which cost just about 1 ms for projecting and overlaying, it is
more than enough for a real-time visualization. According to
the above results, our designed system can efﬁciently overlay
the trajectory on video frames forming a real-time AR
visualization, which has great potential for more complicated
real-time AR robotic surgery education scenario.

V. CONCLUSIONS AND FUTURE WORK

In this work, we confer the intelligence on AR by integrat-
ing AI module as a decision-maker for generating trajectory.
The AI module with reinforcement learning is leveraged to
learn policy-based operation and dynamically generate the
trajectory based on user’s task. To provide an immersive
visualization and facilitate intuitive education, the trajectory
plan is further projected and overlaid onto the stereo video,
assisting the trainee to perceive the intelligent guidelines
in 3D AR. A simple yet effective calibration with human-
computer interaction is also designed to realize coordinate
calibration through an user interface. We conduct the exper-
iment with the surgical training on task peg-transfer, which
demonstrates the feasibility and potential of AI-powered AR
for robotic surgery education.

In our future work, we shall further exploit how to
design and incorporate more AI and AR modules into the
system, such as virtual tool guidance, overlaying the result of
surgical workﬂow recognition and instrument segmentation.
Furthermore, we shall integrate network communication for
realizing a remote surgery education. Methods of measuring
the operation of surgeon compared to the generated ’expert’
trajectory and user study may also be included for evaluating
the effectiveness of the proposed system.

Fig. 5. The failure cases with (a) because of the biased trajectory generated
from RL and (b) due to manually caused calibration error.

background to conduct the calibration process. We calculate
the re-projection error (pixel) [76] based on twelves speciﬁc
points on the pegs. Each user conducts 10 trials and we report
the mean and standard deviation of the error in Table II. From
the table we can ﬁnd that average error is within 1 pixel
(0.76) which is precise for the visualization purpose. Besides,
we only observe one failure trial where the trajectory is
calculated and projected to the wrong location due to the
large error from manually locating the speciﬁc points, as
shown in Fig. 5 b). The above results demonstrate the well
performance of the calibration process and AR visualization
for trajectory guidance on peg-transfer education scenario.

C. System Latency Study

As a systematic assessment of the whole pipeline, latency
is more than important for a reliable and real-time system. In
our work, we mainly focus on studying two different types
of latency that may be introduced by the system. One is the
video capturing and displaying latency, which is often caused
by the delay of the video capture device, video signal transfer
and the monitor. The other is overlaying latency, which is
mainly caused by computing and projecting the points from
3D location to 2D image, as well as the time for drawing
the information on the video frame.

To evaluate the video capturing and displaying delay, we
place a tablet under the camera view of the endoscope
which displays the timestamp on the screen. Then we capture
the video from endoscope camera and display it on the
screen of the computer. Afterwards, we take a photo using
another camera, which will include both table screen and
computer screen. The difference of two timestamps shown
on two screens is the latency. The illustration can be seen in
Fig. 6 (a). Finally, we ﬁnd the average latency is around 161
milliseconds with standard deviation of 17 milliseconds from
100 samples. Which need to be mentioned is that the latency
may vary when given different video capture equipment
or monitor. According to some studies [77], [78] and our

a)b)01020304050607080902008001400200026003200380044005000Point SizeMilliseconds(a)(b)Computer ScreenTablet ScreenTimer (s)EndoscopeREFERENCES

[1] Y. Jin, Y. Long, C. Chen, Z. Zhao, Q. Dou, and P.-A. Heng, “Temporal
memory relation network for workﬂow recognition from surgical
video,” IEEE Transactions on Medical Imaging, 2021.

[2] B. Zhang, A. Ghanem, A. Simes, H. Choi, A. Yoo, and A. Min,
“Swnet: Surgical workﬂow recognition with deep convolutional net-
work,” in Medical Imaging with Deep Learning, 2021.

[3] B. van Amsterdam, M. J. Clarkson, and D. Stoyanov, “Gesture recog-
nition in robotic surgery: A review,” IEEE Transactions on Biomedical
Engineering, vol. 68, no. 6, pp. 2021–2035, 2021.

[4] B. v. Amsterdam, M. J. Clarkson, and D. Stoyanov, “Multi-task
recurrent neural network for surgical gesture recognition and progress
prediction,” in 2020 IEEE International Conference on Robotics and
Automation (ICRA).

IEEE, 2020, pp. 1380–1386.

[5] Y.-H. Su, K. Huang, and B. Hannaford, “Real-time vision-based
tool segmentation with robot kinematics prior,” in 2018
IEEE, 2018,

surgical
International Symposium on Medical Robotics (ISMR).
pp. 1–6.

[6] Y. Long, J. Y. Wu, B. Lu, Y. Jin, M. Unberath, Y.-H. Liu, P. A.
Heng, and Q. Dou, “Relational graph learning on visual and kinematics
embeddings for accurate gesture recognition in robotic surgery,” in
2021 IEEE International Conference on Robotics and Automation
(ICRA).

IEEE, 2021, pp. 13 346–13 353.

[7] C. R. Garrow, K.-F. Kowalewski, L. Li, M. Wagner, M. W. Schmidt,
S. Engelhardt, D. A. Hashimoto, H. G. Kenngott, S. Bodenstedt,
S. Speidel, et al., “Machine learning for surgical phase recognition: a
systematic review,” Annals of Surgery, vol. 273, no. 4, pp. 684–693,
2021.

[8] A. Huaulm´e, D. Sarikaya, K. L. Mut, F. Despinoy, Y. Long, Q. Dou,
C.-B. Chng, W. Lin, S. Kondo, L. Bravo-S´anchez, et al., “Micro-
surgical anastomose workﬂow recognition challenge report,” arXiv
preprint arXiv:2103.13111, 2021.

[9] B. van Amsterdam, M. Clarkson, and D. Stoyanov, “Gesture recogni-
tion in robotic surgery: a review,” IEEE Transactions on Biomedical
Engineering, 2021.

[10] H. Huynhnguyen and U. A. Buy, “Toward gesture recognition in robot-
assisted surgical procedures,” in 2020 2nd International Conference on
Societal Automation (SA).

IEEE, 2021, pp. 1–4.

[11] F. Volont´e, N. C. Buchs, F. Pugin, J. Spaltenstein, B. Schiltz, M. Jung,
M. Hagen, O. Ratib, and P. Morel, “Augmented reality to the rescue
of the minimally invasive surgeon. the usefulness of the interposition
of stereoscopic images in the da vinci™ robotic console,” The Inter-
national Journal of Medical Robotics and Computer Assisted Surgery,
vol. 9, no. 3, pp. e34–e38, 2013.

[12] L. Qian, J. Y. Wu, S. P. DiMaio, N. Navab, and P. Kazanzides,
“A review of augmented reality in robotic-assisted surgery,” IEEE
Transactions on Medical Robotics and Bionics, vol. 2, no. 1, pp. 1–16,
2019.

[13] S. Sheik-Ali, H. Edgcombe, and C. Paton, “Next-generation virtual and
augmented reality in surgical education: a narrative review,” Surgical
technology international, vol. 33, 2019.

[14] E. Z. Barsom, M. Graaﬂand, and M. P. Schijven, “Systematic review
on the effectiveness of augmented reality applications in medical
training,” Surgical endoscopy, vol. 30, no. 10, pp. 4174–4183, 2016.
[15] R. R. McKnight, C. A. Pean, J. S. Buck, J. S. Hwang, J. R. Hsu,
and S. N. Pierrie, “Virtual reality and augmented reality—translating
surgical training into surgical technique,” Current Reviews in Muscu-
loskeletal Medicine, pp. 1–12, 2020.

[16] R. Londei, M. Esposito, B. Diotte, S. Weidert, E. Euler, P. Thaller,
N. Navab, and P. Fallavollita, “Intra-operative augmented reality in
distal locking,” International journal of computer assisted radiology
and surgery, vol. 10, no. 9, pp. 1395–1403, 2015.

[17] E. Turan and G. C¸ etin, “Using artiﬁcial intelligence for modeling of
the realistic animal behaviors in a virtual island,” Computer Standards
& Interfaces, vol. 66, p. 103361, 2019.

[18] J. L. Gabbard, M. Smith, K. Tanous, H. Kim, and B. Jonas, “Ar
drivesim: An immersive driving simulator for augmented reality head-
up display research,” Frontiers in Robotics and AI, vol. 6, p. 98, 2019.
[19] R. G. Boboc, F. Gˆırbacia, and E. V. Butil˘a, “The application of
augmented reality in the automotive industry: A systematic literature
review,” Applied Sciences, vol. 10, no. 12, p. 4259, 2020.

[20] S. Daher, J. Hochreiter, R. Schubert, L. Gonzalez, J. Cendan, M. An-
derson, D. A. Diaz, and G. F. Welch, “The physical-virtual patient
simulator: A physical human form with virtual appearance and behav-
ior,” Simulation in Healthcare, vol. 15, no. 2, pp. 115–121, 2020.

[21] L. Gonzalez, S. Daher, and G. Welch, “Neurological assessment using
a physical-virtual patient (pvp),” Simulation & Gaming, vol. 51, no. 6,
pp. 802–818, 2020.

[22] L. Tanzi, P. Piazzolla, F. Porpiglia, and E. Vezzetti, “Real-time deep
learning semantic segmentation during intra-operative surgery for
3d augmented reality assistance,” International Journal of Computer
Assisted Radiology and Surgery, vol. 16, no. 9, pp. 1435–1445, 2021.
[23] J. Pan, W. Liu, P. Ge, F. Li, W. Shi, L. Jia, and H. Qin, “Real-
time segmentation and tracking of excised corneal contour by deep
neural networks for dalk surgical navigation,” Computer Methods and
Programs in Biomedicine, vol. 197, p. 105679, 2020.

[24] A. H. Sadeghi, A. P. Maat, Y. J. Taverne, R. Cornelissen, A.-M. C.
Dingemans, A. J. Bogers, and E. A. Mahtab, “Virtual reality and artiﬁ-
cial intelligence for 3-dimensional planning of lung segmentectomies,”
JTCVS Techniques, 2021.

[25] A. Amini, I. Gilitschenski, J. Phillips, J. Moseyko, R. Banerjee,
S. Karaman, and D. Rus, “Learning robust control policies for end-to-
end autonomous driving from data-driven simulation,” IEEE Robotics
and Automation Letters, vol. 5, no. 2, pp. 1143–1150, 2020.

[26] K. Kurach, A. Raichuk, P. Sta´nczyk, M. Zajac, O. Bachem, L. Es-
peholt, C. Riquelme, D. Vincent, M. Michalski, O. Bousquet, et al.,
“Google research football: A novel reinforcement learning environ-
ment,” arXiv preprint arXiv:1907.11180, 2019.

[27] Z.-Y. Chiu, F. Richter, E. K. Funk, R. K. Orosco, and M. C. Yip,
“Bimanual regrasping for suture needles using reinforcement learning
for rapid motion planning,” in 2021 IEEE International Conference
on Robotics and Automation (ICRA).

IEEE, 2021, pp. 7737–7743.

[28] F. Richter, R. K. Orosco, and M. C. Yip, “Open-sourced reinforce-
learning environments for surgical robotics,” arXiv preprint

ment
arXiv:1903.02090, 2019.

[29] C. D’Ettorre, A. Mariani, A. Stilli, P. Valdastri, A. Deguet,
P. Kazanzides, R. H. Taylor, G. S. Fischer, S. P. DiMaio, A. Menciassi,
et al., “Accelerating surgical robotics research: Reviewing 10 years of
research with the dvrk,” arXiv preprint arXiv:2104.09869, 2021.
[30] P. Kazanzides, Z. Chen, A. Deguet, G. S. Fischer, R. H. Taylor,
and S. P. DiMaio, “An open-source research kit for the da vinci®
surgical system,” in 2014 IEEE international conference on robotics
and automation (ICRA).

IEEE, 2014, pp. 6434–6439.

[31] Z. Zhao, Y. Jin, X. Gao, Q. Dou, and P.-A. Heng, “Learning motion
ﬂows for semi-supervised instrument segmentation from robotic surgi-
cal video,” in International Conference on Medical Image Computing
and Computer-Assisted Intervention. Springer, 2020, pp. 679–689.

[32] Y. Long, Z. Li, C. H. Yee, C. F. Ng, R. H. Taylor, M. Unberath, and
Q. Dou, “E-dssr: efﬁcient dynamic surgical scene reconstruction with
transformer-based stereoscopic depth perception,” in International
Conference on Medical Image Computing and Computer-Assisted
Intervention. Springer, 2021, pp. 415–425.

[33] T. Blum, H. Feußner, and N. Navab, “Modeling and segmentation of
surgical workﬂow from laparoscopic video,” in International Confer-
ence on Medical Image Computing and Computer-Assisted Interven-
tion. Springer, 2010, pp. 400–407.

[34] N. Padoy, T. Blum, S.-A. Ahmadi, H. Feussner, M.-O. Berger, and
N. Navab, “Statistical modeling and recognition of surgical workﬂow,”
Medical image analysis, vol. 16, no. 3, pp. 632–641, 2012.

[35] A. Nara, K. Izumi, H. Iseki, T. Suzuki, K. Nambu, and Y. Sakurai,
“Surgical workﬂow analysis based on staff’s trajectory patterns,” in
M2CAI workshop, MICCAI, London, 2009.

[36] T. Neumuth, “Surgical process modeling,” Innovative surgical sci-

ences, vol. 2, no. 3, pp. 123–137, 2017.

[37] A. Rakhsha, G. Radanovic, R. Devidze, X. Zhu, and A. Singla,
“Policy teaching via environment poisoning: Training-time adversarial
attacks against reinforcement learning,” in International Conference on
Machine Learning. PMLR, 2020, pp. 7974–7984.

[38] D. Liu and T. Jiang, “Deep reinforcement learning for surgical gesture
segmentation and classiﬁcation,” in International conference on med-
ical image computing and computer-assisted intervention. Springer,
2018, pp. 247–255.

[39] X. Gao, Y. Jin, Q. Dou, and P.-A. Heng, “Automatic gesture recogni-
tion in robot-assisted surgery with reinforcement learning and tree
search,” in 2020 IEEE International Conference on Robotics and
Automation (ICRA).

IEEE, 2020, pp. 8440–8446.

[40] J. Torrents-Barrena, G. Piella, E. Gratacos, E. Eixarch, M. Ceresa, and
M. A. G. Ballester, “Deep q-capsnet reinforcement learning framework
for intrauterine cavity segmentation in ttts fetal surgery planning,”

IEEE transactions on medical imaging, vol. 39, no. 10, pp. 3113–
3124, 2020.

[41] M. Chitsaz and W. C. Seng, “Medical image segmentation by using
reinforcement learning agent,” in 2009 International Conference on
digital Image Processing.

IEEE, 2009, pp. 216–219.

[42] V. M. Varier, D. K. Rajamani, N. Goldfarb, F. Tavakkolmoghaddam,
A. Munawar, and G. S. Fischer, “Collaborative suturing: A reinforce-
ment learning approach to automate hand-off task in suturing for
surgical robots,” in 2020 29th IEEE International Conference on Robot
and Human Interactive Communication (RO-MAN).
IEEE, 2020, pp.
1380–1386.

[43] L. Xie, Y. Miao, S. Wang, P. Blunsom, Z. Wang, C. Chen,
A. Markham, and N. Trigoni, “Learning with stochastic guidance for
robot navigation,” IEEE transactions on neural networks and learning
systems, vol. 32, no. 1, pp. 166–176, 2020.

[44] T. J. Loftus, A. C. Filiberto, Y. Li, J. Balch, A. C. Cook, P. J.
Tighe, P. A. Efron, G. R. Upchurch Jr, P. Rashidi, X. Li, et al.,
“Decision analysis and reinforcement learning in surgical decision-
making,” Surgery, vol. 168, no. 2, pp. 253–266, 2020.

[45] T. Nguyen, N. D. Nguyen, F. Bello, and S. Nahavandi, “A new
tensioning method using deep reinforcement
learning for surgical
pattern cutting,” in 2019 IEEE international conference on industrial
technology (ICIT).
IEEE, 2019, pp. 1339–1344.

[46] C. Sewell, D. Morris, N. H. Blevins, S. Dutta, S. Agrawal, F. Barbagli,
and K. Salisbury, “Providing metrics and performance feedback in a
surgical simulator,” Computer Aided Surgery, vol. 13, no. 2, pp. 63–81,
2008.

[47] R. Nagyn´e Elek and T. Haidegger, “Robot-assisted minimally invasive
surgical skill assessment—manual and automated platforms,” Acta
Polytechnica Hungarica, vol. 16, no. 8, pp. 141–169, 2019.

[48] N. Mirchi, V. Bissonnette, N. Ledwos, A. Winkler-Schwartz, R. Yil-
maz, B. Karlik, and R. F. Del Maestro, “Artiﬁcial neural networks
to assess virtual reality anterior cervical discectomy performance,”
Operative Neurosurgery, vol. 19, no. 1, pp. 65–75, 2020.

[49] M. Ershad, R. Rege, and A. M. Fey, “Meaningful assessment of robotic
surgical style using the wisdom of crowds,” International journal of
computer assisted radiology and surgery, vol. 13, no. 7, pp. 1037–
1048, 2018.

[50] H. Liang and M. Y. Shi, “Surgical skill evaluation model for virtual
surgical training,” in Applied Mechanics and Materials, vol. 40. Trans
Tech Publ, 2011, pp. 812–819.

[51] M. Ershad, R. Rege, and A. M. Fey, “Automatic and near real-time
stylistic behavior assessment in robotic surgery,” International journal
of computer assisted radiology and surgery, vol. 14, no. 4, pp. 635–
643, 2019.

[52] J. Wang, Q. Zhang, D. Zhao, and Y. Chen, “Lane change decision-
making through deep reinforcement learning with rule-based con-
straints,” in 2019 International Joint Conference on Neural Networks
(IJCNN).

IEEE, 2019, pp. 1–6.

[53] A. P. Wierzbicki and J. Wessels, “The modern decision maker,”
in Model-Based Decision Support Methodology with Environmental
Applications. Springer, 2000, pp. 29–46.

[54] R. Wen, C.-B. Chng, and C.-K. Chui, “Augmented reality guidance
with multimodality imaging data and depth-perceived interaction for
robot-assisted surgery,” Robotics, vol. 6, no. 2, p. 13, 2017.

[55] V. Penza, E. De Momi, N. Enayati, T. Chupin, J. Ortiz, and L. S.
Mattos, “Envisors: Enhanced vision system for robotic surgery. a user-
deﬁned safety volume tracking to minimize the risk of intraoperative
bleeding,” Frontiers in Robotics and AI, vol. 4, p. 15, 2017.

[56] F. Cutolo, A. Meola, M. Carbone, S. Sinceri, F. Cagnazzo, E. Denaro,
N. Esposito, M. Ferrari, and V. Ferrari, “A new head-mounted display-
based augmented reality system in neurosurgical oncology: a study on
phantom,” Computer assisted surgery, vol. 22, no. 1, pp. 39–53, 2017.
[57] N. Zevallos, R. A. Srivatsan, H. Salman, L. Li, J. Qian, S. Saxena,
M. Xu, K. Patath, and H. Choset, “A surgical system for automatic
registration, stiffness mapping and dynamic image overlay,” in 2018
International Symposium on Medical Robotics (ISMR).
IEEE, 2018,
pp. 1–6.

[58] Y.-Y. Wang, A. Kumar, K.-C. Liu, S.-W. Huang, C.-C. Huang, W.-
C. Su, F.-L. Hsiao, and W.-N. Lie, “Stereoscopic augmented reality
for single camera endoscopy: a virtual study,” Computer Methods in
Biomechanics and Biomedical Engineering: Imaging & Visualization,
vol. 6, no. 2, pp. 182–191, 2018.

[59] L. Chen, W. Tang, and N. W. John, “Real-time geometry-aware aug-

mented reality in minimally invasive surgery,” Healthcare technology
letters, vol. 4, no. 5, pp. 163–167, 2017.

[60] M. S. Nosrati, R. Abugharbieh, J.-M. Peyrat, J. Abinahed, O. Al-
Alao, A. Al-Ansari, and G. Hamarneh, “Simultaneous multi-structure
segmentation and 3d nonrigid pose estimation in image-guided robotic
surgery,” IEEE Transactions on Medical Imaging, vol. 35, no. 1, pp.
1–12, 2016.

[61] K. S. Tang, D. L. Cheng, E. Mi, and P. B. Greenberg, “Augmented
reality in medical education: a systematic review,” Canadian medical
education journal, vol. 11, no. 1, p. e81, 2020.

[62] A. M. Jarc, A. A. Stanley, T. Clifford, I. S. Gill, and A. J. Hung,
“Proctors exploit three-dimensional ghost tools during clinical-like
training scenarios: a preliminary study,” World journal of urology,
vol. 35, no. 6, pp. 957–965, 2017.

[63] “da vinci research kit research wiki page,” Website, 2021, https:

//research.intusurg.com/index.php/Main Page.

[64] H. Lin, C.-W. V. Hui, Y. Wang, A. Deguet, P. Kazanzides, and
K. S. Au, “A reliable gravity compensation control strategy for dvrk
robotic arms with nonlinear disturbance forces,” IEEE Robotics and
Automation Letters, vol. 4, no. 4, pp. 3892–3899, 2019.

[65] J. Xu, B. Li, B. Lu, Y.-H. Liu, Q. Dou, and P.-A. Heng, “Surrol:
An open-source reinforcement learning centered and dvrk compatible
platform for surgical robot learning,” arXiv preprint arXiv:2108.13035,
2021.

[66] B. Lu, W. Chen, Y.-M. Jin, D. Zhang, Q. Dou, H. K. Chu, P.-
A. Heng, and Y.-H. Liu, “A learning-driven framework with spatial
optimization for surgical suture thread reconstruction and autonomous
grasping under multiple topologies and environmental noises,” in 2020
IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS).

IEEE, 2020, pp. 3075–3082.

[67] W. J. Hyung and Y. Woo, “Tilepro,” in Robotics in General Surgery.

Springer, 2014, pp. 457–460.

[68] M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welin-
der, B. McGrew, J. Tobin, P. Abbeel, and W. Zaremba, “Hindsight ex-
perience replay,” in Proceedings of the 31st International Conference
on Neural Information Processing Systems, 2017, pp. 5055–5065.
[69] A. Tabb and K. M. A. Yousef, “Solving the robot-world hand-eye
(s) calibration problem with iterative methods,” Machine Vision and
Applications, vol. 28, no. 5, pp. 569–590, 2017.

[70] R. Y. Tsai, R. K. Lenz, et al., “A new technique for fully autonomous
and efﬁcient 3 d robotics hand/eye calibration,” IEEE Transactions on
robotics and automation, vol. 5, no. 3, pp. 345–358, 1989.

[71] F. C. Park and B. J. Martin, “Robot sensor calibration: solving ax=
xb on the euclidean group,” IEEE Transactions on Robotics and
Automation, vol. 10, no. 5, pp. 717–721, 1994.

[72] L. Qian, T. Song, M. Unberath, and P. Kazanzides, “Ar-loupe: Mag-
niﬁed augmented reality by combining an optical see-through head-
mounted display and a loupe,” IEEE Transactions on Visualization and
Computer Graphics, 2020.

[73] Z. Zhang, “A ﬂexible new technique for camera calibration,” IEEE
Transactions on pattern analysis and machine intelligence, vol. 22,
no. 11, pp. 1330–1334, 2000.

[74] V. Lepetit, F. Moreno-Noguer, and P. Fua, “Epnp: An accurate o (n)
solution to the pnp problem,” International journal of computer vision,
vol. 81, no. 2, p. 155, 2009.

[75] R. A. Joseph, A. C. Goh, S. P. Cuevas, M. A. Donovan, M. G.
Kauffman, N. A. Salas, B. Miles, B. L. Bass, and B. J. Dunkin, ““chop-
stick” surgery: a novel
technique improves surgeon performance
and eliminates arm collision in robotic single-incision laparoscopic
surgery,” Surgical endoscopy, vol. 24, no. 6, pp. 1331–1335, 2010.

[76] K. Koide and E. Menegatti, “General hand–eye calibration based
on reprojection error minimization,” IEEE Robotics and Automation
Letters, vol. 4, no. 2, pp. 1021–1028, 2019.

[77] R. B. Miller, “Response time in man-computer conversational transac-
tions,” in Proceedings of the December 9-11, 1968, fall joint computer
conference, part I, 1968, pp. 267–277.

[78] N. Tolia, D. G. Andersen, and M. Satyanarayanan, “Quantifying
interactive user experience on thin clients,” Computer, vol. 39, no. 3,
pp. 46–52, 2006.

