LCNN: Lookup-based Convolutional Neural Network

Hessam Bagherinezhad1,2

Mohammad Rastegari2,3

Ali Farhadi1,2,3

1University of Washington

2XNOR.AI

3Allen Institute for AI

hessam, mohammad, ali
{

}

@xnor.ai

7
1
0
2

n
u
J

3
1

]

V
C
.
s
c
[

2
v
3
7
4
6
0
.
1
1
6
1
:
v
i
X
r
a

Abstract

Porting state of the art deep learning algorithms to re-
source constrained compute platforms (e.g. VR, AR, wear-
ables) is extremely challenging. We propose a fast, com-
pact, and accurate model for convolutional neural networks
that enables efﬁcient learning and inference. We introduce
LCNN, a lookup-based convolutional neural network that
encodes convolutions by few lookups to a dictionary that is
trained to cover the space of weights in CNNs. Training
LCNN involves jointly learning a dictionary and a small set
of linear combinations. The size of the dictionary naturally
traces a spectrum of trade-offs between efﬁciency and ac-
curacy. Our experimental results on ImageNet challenge
show that LCNN can offer 3.2
speedup while achieving
55.1% top-1 accuracy using AlexNet architecture. Our
fastest LCNN offers 37.6
speed up over AlexNet while
maintaining 44.3% top-1 accuracy. LCNN not only offers
dramatic speed ups at inference, but it also enables efﬁcient
training. In this paper, we show the beneﬁts of LCNN in
few-shot learning and few-iteration learning, two crucial
aspects of on-device training of deep learning models.

×

×

1. Introduction

In recent years convolutional neural networks (CNN)
have played major roles in improving the state of the art
across a wide range of problems in computer vision, in-
cluding image classiﬁcation [25, 37, 39, 18], object detec-
tion [11, 10, 36], segmentation [34, 32], etc. These mod-
els are very expensive in terms of computation and mem-
ory. For example, AlexNet[25] has 61M parameters and
performs 1.5B high precision operations to classify a single
image. These numbers are even higher for deeper networks,
e.g.,VGG [37]. The computational burden of learning and
inference for these models is signiﬁcantly higher than what
most compute platforms can afford.

Recent advancements in virtual reality (VR by Oculus)
[33], augmented reality (AR by HoloLens) [14], and smart
wearable devices increase the demand for getting our state

1

of the art deep learning algorithm on these portable compute
platforms. Porting deep learning methods to these platforms
is challenging mainly due to the gap between what these
platforms can offer and what our deep learning methods re-
quire. More efﬁcient approaches to deep neural networks is
the key to this challenge.

Recent work on efﬁcient deep learning have focused on
model compression and reducing the computational preci-
sion of operations in neural networks [3, 15, 35]. CNNs suf-
fer from over-parametrization [7] and often encode highly
correlated parameters [22], resulting in inefﬁcient compu-
tation and memory usage[7]. Our key insight is to lever-
age the correlation between the parameters and represent
the space of parameters by a compact set of weight vec-
tors, called dictionary. In this paper, we introduce LCNN,
a lookup-based convolutional neural network that encodes
convolutions by few lookups to a dictionary that is trained
to cover the space of weights in CNNs. Training LCNN
involves jointly learning a dictionary and a small set of
linear combinations. The size of the dictionary naturally
traces a spectrum of trade-offs between efﬁciency and accu-
racy. Our experimental results using AlexNet on ImageNet
challenge show that LCNN can offer 3.2
speedup while
achieving 55.1% top-1 accuracy. Our fastest LCNN offers
37.6
speed up over CNN while maintaining 44.3% top-1
accuracy. In the ResNet-18, the most accurate LCNN offers
5
speedup with 62.2% accuracy and the fastest LCNN of-
×
fers 29.2

speedup with 51.8% accuracy

×

×

In addition, LCNN enables efﬁcient training; almost all
the work in efﬁcient deep learning have focused on efﬁcient
inference on resource constrained platforms [35]. Train-
ing on these platforms is even more challenging and re-
quires addressing two major problems: i. few-shot learn-
ing: the settings of on-device training dictates that there
won’t be enough training examples for new categories. In
fact, most training needs to be done with very few train-
ing examples; ii. few-iteration learning: the constraints in
computation and power require the training to be light and
quick. This imposes hard constraints on the number of it-
erations in training.LCNN offers solutions for both of these
problems in deep on-device training.

×

 
 
 
 
 
 
Few-shot learning, the problem of learning novel cate-
gories from few examples (sometimes even one example),
have been extensively studies in machine learning and com-
puter vision[9]. The topic is, however, relatively new for
deep learning[17], where the main challenge is to avoid
overﬁtting. The number of parameters are signiﬁcantly
higher than what can be learned from few examples. LCNN,
by virtue of having fewer parameters to learn (only around
7% of parameters of typical networks), offers a simple solu-
tion to this challenge. Our dictionary can be learned ofﬂine
from training data where enough training examples per cat-
egory exists. When facing new categories, all we need to
learn is the set of sparse reconstruction weights. Our exper-
imental evaluations show signiﬁcant gain in few-shot learn-
ing; 6.3% in one training example per category.

Few-iteration learning is the problem of getting high-
est possible accuracy in few iterations that a resource con-
strained platform can offer. In a typical CNN, training often
involves hundreds of thousands of iterations. This number
is even higher for recent deeper architectures. LCNN offers
a solution: dictionaries in LCNN are architecture agnostic
and can be transferred across architectures or layers. This
allows us to train a dictionary using a shallow network and
transfer it to a deeper one. As before, all we need to learn
are the few reconstruction weights; dictionaries don’t need
to be trained again. Our experimental evaluations on Im-
ageNet challenge show that using LCNN we can train an
18-layer ResNet with a pre-trained dictionary from a 10-
layer ResNet and achieve 16.2% higher top-1 accuracy on
10K iterations.

In this paper, we 1) introduce LCNN; 2) show state of
the art efﬁcient inference in CNNs using LCNN; 3) demon-
strate possibilities of training deep CNNs using as few as
one example per category 4) show results for few iteration
learning .

2. Related Work

A wide range of methods have been proposed to address
efﬁcient training and inference in deep neural networks.
Here, we brieﬂy study these methods under the topics that
are related to our approach.

Weight compression: Several attempts have been made
to reduce the number of parameters of deep neural net-
works. Most of such methods [13, 46, 3, 15, 38] are based
on compressing the fully connected layers, which contain
most of the weights. These methods do not achieve much
In [21], a small DNN architec-
improvement on speed.
ture is proposed which is fully connected free and has 50x
fewer parameters in compare to AlexNet [26]. However,
their model is slower than AlexNet. Recently [16, 15] re-
duced the number of parameters by pruning. All of these
approaches update a pre-trained CNN, whereas we propose
to train a compact structure that enables faster inference.

Low Rank Assumption: Approximating the weights
of convolutional layers with low-rank tensor expansion has
been explored by [22, 7]. They only demonstrated speedup
in the case of large convolutions. [8] uses SVD for tensor
decomposition to reduce the computation in the lower lay-
ers on a pre-trained CNN. [47] minimizes the reconstruc-
tion error of the nonlinear responses in a CNN, subject to a
low-rank constraint which helps to reduce the complexity of
ﬁlters. Notably, all of these methods are a post processing
on the weights of a trained CNN, and none of them train a
lower rank network from scratch.

Low Precision Networks: A ﬁxed-point implementa-
tion of 8-bit integer was compared with 32-bit ﬂoating point
activations in [41, 20]. Several network quantization meth-
ods are proposed by [13, 1, 29, 29, 19]. Most recently, bi-
nary networks has shown to achieve relatively strong result
on ImageNet [35]. They have trained a network that com-
putes the output with mostly binary operations, except for
the ﬁrst and the last layer. [5] uses the real-valued version
of the weights as a key reference for the binarization pro-
cess.
[4] is an extension of [5], where both weights and
activations are binarized. [23] retrains a previously trained
neural network with binary weights and binary inputs. Our
approach is orthogonal to this line of work. In fact, any of
these methods can be applied in our model to reduce the
precision.

Sparse convolutions: Recently, several attempts have
been made to sparsify the weights of convolutional layers
[31, 45, 44]. [31] shows how to reduce the redundancy in
parameters of a CNN using a sparse decomposition. [45]
proposed a framework to simultaneously speed up the com-
putation and reduce the storage of CNNs.
[44] proposed
a Structured Sparsity Learning (SSL) method to regularize
the structures (i.e., ﬁlters, channels, ﬁlter shapes, and layer
depth) of CNNs. Only in [44] a sparse CNN is trained from
scratch which makes it more similar to our approach. How-
ever, our method provides a rich set of dictionary that en-
ables implementing convolution with lookup operations.

Few-Shot Learning: The problem of learning novel cat-
egories has been studied in [40, 2, 30]. Learning from few
examples per category explored by [17]. [9, 42, 24] pro-
posed a method to learn from one training example per cat-
egory, known as one-shot learning. Learning without any
training example, zero-shot learning, is studied by [27, 28].

3. Our Approach

kw

Overview: In a CNN, each convolutional layer consists of
n cubic weight ﬁlters of size m
kh, where m and n
are the number of input and output channels, respectively,
and kw and kh are the width and the height of the ﬁlter.
Therefore, the weights in each convolutional layer is com-
posed of nkwkh vectors of length m. These vectors are
shown to have redundant information[7]. To avoid this re-

×

×

Figure 1. This ﬁgure demonstrates the procedure for constructing a weight ﬁlter in LCNN. A vector in the weight ﬁlter (the long colorful
cube in the gray tensor W) is formed by a linear combination of few vectors, which are looked up from the dictionary D. Lookup indices
and their coefﬁcients are stored in tensors I and C.

dundancy, we build a relatively small set of vectors for each
layer, to which we refer as dictionary, and enforce each vec-
tor in the weight ﬁlter to be a linear combination of a few
elements from this set. Figure 1 shows an overview of our
model. The gray matrix at the left of the ﬁgure is the dictio-
nary. The dashed lines show how we lookup a few vectors
from the dictionary and linearly combine them to build up
a weight ﬁlter. Using this structure, we devise a fast infer-
ence algorithm for CNNs. We then show that the dictio-
naries provide a strong prior on the visual data and enables
us to learn from few examples. Finally, we show that the
dictionaries can be transferred across different network ar-
chitectures. This allows us to speedup the training of a deep
network by transferring the dictionaries from a shallower
model.

3.1. LCNN

∈

∈

A convolutional layer in a CNN consists of four parts: 1) the
Rm×w×h; where m, w and h are the num-
input tensor X
ber of input channels, the width and the height, respectively,
2) a set of n weight ﬁlters, where each ﬁlter is a tensor
Rm×kw×kh , where kw and kh are the width and the
W
R for each ﬁl-
height of the ﬁlter, 3) a scalar bias term b
Rn×w(cid:48)×h(cid:48)
ter, and 4) the output tensor Y
; where each
X + b. Here
is computed by W
channel Y[i,:,:] ∈
∗
denotes the discrete convolution operation1.
∗
Rk×m as the
For each layer, we deﬁne a matrix D
shared dictionary of vectors. This is illustrated in ﬁgure 1,
on the left side. This matrix contains k row vectors of length
m. The size of the dictionary, k, might vary for different
layers of the network, but it should always be smaller than
nkwkh, the total number of vectors in all weight ﬁlters of
a layer. Along with the dictionary D, we have a tensor for

Rw(cid:48)×h(cid:48)

∈

∈

∈

1The (:) notation is borrowed from NumPy for selecting all entries in a

dimension.

N s×kw×kh

∈

≤k

lookup indices I
, and a tensor for lookup co-
∈
Rs×kw×kh for each layer. For a pair (r, c),
efﬁcients C
I[:,r,c] is a vector of length s whose entries are indices of
the rows of the dictionary, which form the linear compo-
nents of W[:,r,c]. The entries of the vector C[:,r,c] specify
the linear coefﬁcients with which the components should
be combined to make W[:,r,c] (illustrated by a long color-
ful cube inside the gray cub in Figure 1-right). We set s,
the number of components in a weight ﬁlter vector, to be
a small number. The weight tensor can be constructed as
follows:

s

W[:,r,c] =

C[t,r,c] ·

D[I[t,r,c],:] ∀

r, c

(1)

This procedure is illustrated in Figure 1. In LCNN, in-
stead of storing the weight tensors W for convolutional lay-
ers, we store D, I and C, the building blocks of the weight
tensors. As a result, we can reduce the number of param-
eters in a convolutional layer by reducing k, the dictionary
size, and s, the number of components in the linear com-
binations. In the next section, we will discuss how LCNN
uses this representation to speedup the inference.

t=1
(cid:80)

3.1.1 Fast Convolution using a Shared Dictionary

A forward pass in a convolutional layer consists of n con-
volutions between the input X and each of the weight ﬁlters
W. We can write a convolution between an m
kh
weight ﬁlter and the input X as a sum of kwkh separate
(1

1)-convolutions:

kw

×

×

×

W =

X

∗

shiftr,c(X

W[:,r,c])

∗

(2)

kh,kw

r,c
(cid:80)

, where shiftr,c is the matrix shift function along rows and
columns with zero padding relative to the ﬁlter size. Now

!DictionaryW12433mkkwkh!0.20.70.1IC[1,24,33][0.2,0.7,0.1][6,13,40][0.3,0.6,0.4]skwkhskwkh613400.30.60.4Figure 2. S is the output of convolving the dictionary with the input tensor. The left side of this ﬁgure illustrates the inference time
forward pass. The convolution between the input and a weight ﬁlter is carried out by lookups over the channels of S and a few linear
combinations. Direct learning of tensors I and C reduces to an intractable discrete optimization. The right side of this ﬁgure shows an
equivalent computation for training based on sparse convolutions. Parameters P can be trained using SGD. The tiny cubes in P denote the
non-zero entries.

we use the LCNN representation of weights (equation 1) to
rewrite each 1

1 convolution:

×

s

C[t,r,c] ·

D[I[t,r,c],:]))

W =

X

∗

=

shiftr,c(X

s

shiftr,c(

r,c
(cid:80)

∗

(
t=1
(cid:80)

C[t,r,c](X

D[I[t,r,c],:]))

∗

r,c
(cid:80)

t=1
(cid:80)

(3)
Equation 3 suggests that instead of reconstructing the
weight tensor W and convolving with the input, we can
convolve the input with all of the dictionary vectors, and
then compute the output according to I and C. Since the
dictionary D is shared among all weight ﬁlters in a layer,
we can pre-compute the convolution between the input ten-
Rk×w×h be
sor X and all the dictionary vectors. Let S
the output of convolving the input X with all of the dictio-
nary vectors D, i.e.,

∈

S[i,:,:] = X

1
D[i,:] ∀
Once the values of S are computed, we can reconstruct the
output of convolution by lookups over the channels of S
according to I, then scale them by the values in C:

(4)

≤

≤

k

∗

i

W =

kh,kw

s

shiftr,c(

X

∗

C[t,r,c]S[I[t,r,c],:,:])

(5)

r,c
(cid:80)

t=1
(cid:80)
This is shown in Figure 2 (left). Reducing the size of the
dictionary k lowers the cost of computing S and makes the
forward pass faster. Since S is computed by a dense matrix
multiplication, we are still able to use OpenBlas [43] for
fast matrix multiplication. In addition, by pushing the value
of s to be small, we can reduce the number of lookups and
ﬂoating point operations.

3.1.2 Training LCNN

So far we have discussed how LCNN represents a weight
ﬁlter by linear combinations of a subset of elements in a
shared dictionary. We have also shown that how LCNN
performs convolutions efﬁciently in two stages: 1- Small
convolutions: convolving the input with a set of 1
1 ﬁl-
ters (equation 4). 2- Lookup and scale: few lookups over
the channels of a tensor followed by a linear combination
(equation 5) . Now, we explain how one can jointly train
the dictionary and the lookup parameters, I and C. Direct
training of the proposed lookup based convolution leads to a
combinatorial optimization problem, where we need to ﬁnd
the optimal values for the integer tensor I. To get around
this, we reformulate the lookup and scale stage (equation 5)
using a standard convolution with sparsity constraints.

×

∈

Let T

Rk×kw×kh be a one hot tensor, where T[t,r,c] =
1 and all other entries are zero. It is easy to observe that con-
volving the tensor S with T will result in shiftr,c(S[t,:,:]).
We use this observation to convert the lookup and scale
stage (equation 5) to a standard convolution. Lookups and
scales can be expressed by a convolution between the tensor
Rk×w×h, and P[:,r,c] is
S and a sparse tensor P, where P
a s-sparse vector (i.e. it has only s non-zero entries) for all
spatial positions (r, c). Positions of the non-zero entries in
P are determined by the index tensor I and their values are
determined by the coefﬁcient tensor C. Formally, tensor P
can be expressed by I and C:

∈

Pj,r,c =

Ct,r,c,
0,

(cid:40)

t : It,r,c = j

∃
otherwise

(6)

Note that this conversion is reversible, i.e.,we can create
I and C from the position and the values of the non-zero

SS∗P++=⇔=1243312433++=0.20.70.1IC[1,24,33][0.2,0.7,0.1]61340613400.20.70.10.30.60.40.30.60.4[6,13,40][0.3,0.6,0.4]!entries in P. With this conversion, the lookup and scale
stage (equation 5) becomes:

s

shift(r,c)(

C[t,r,c]S[I[t,r,c],:,:]) = S

P

∗

(7)

rc
(cid:88)

t=1
(cid:88)

P[:,r,c](cid:107)
(cid:107)

This is illustrated in Figure 2-right. Now, instead of directly
training I and C, we can train the tensor P with (cid:96)0-norm
(cid:96)0 = s) and then construct I and C
constraints (
from P. However, (cid:96)0-norm is a non-continuous function
with zero gradients everywhere. As a workaround, we relax
it to (cid:96)1-norm. At each iteration of training, to enforce the
sparsity constraint for P[:,r,c], we sort all the entries by their
absolute values and keep the top s entries and zero out the
rest. During training, in addition to the classiﬁcation loss
P[:,r,c](cid:107)
L we also minimize
(cid:96)1, by adding a
(cid:107)

P
(cid:107)

(cid:96)1 =

term λ
to the values in P is computed by:

(cid:96)1 to the loss function. The gradient with respect
(cid:107)

P
(cid:107)

[r,c] (cid:107)
(cid:80)

∂(L + λ

P
(cid:107)
∂P

)

(cid:107)(cid:96)1

=

∂L
∂P

+ λ sign(P)

(8)

where ∂L
∂P is the gradient that is computed through a stan-
dard back-propagation. λ is a hyperparameter that adjusts
the trade-off between the CNN loss function and the (cid:96)1 reg-
ularizer. We can also allow s, the sparsity factor, to be dif-
ferent at each spatial position (r, c), and be determined auto-
matically at training time. This can be achieved by applying
a threshold function,

δ(x) =

x,
0,

(cid:40)

> (cid:15)
x
|
otherwise

|

(9)

over the values in P during training. We also backpropa-
gate through this threshold function to compute the gradi-
ents with respect to P. The derivative of the threshold func-
tion is 1 everywhere except at
< (cid:15), which is 0. Hence,
if any of the entries of P becomes 0 at some iteration, they
stay 0 forever. Using the threshold function, we let each
vector to be a combination of arbitrary vectors. At the end
of the training, the sparsity parameter s at each spatial posi-
tion (r, c) is determined by the number of non-zero values
in P[:, r, c].

x
|

|

Although the focus of our work is to speedup convo-
lutional layers where most of the computations are, our
lookup based convolution model can also be applied on
fully connected (FC) layers. An FC layer that goes from m
inputs to n outputs can be viewed as a convolutional layer
1 and n weight ﬁlters, each of
with input tensor m
size m
1. We take the same approach to speedup fully
1
×
connected layers.

×

×

×

1

After training, we convert P to the indices and the co-
efﬁcients tensors I and C for each layer. At test time, we
follow equation 5 to efﬁciently compute the output of each
convolutional layer.

3.2. Few-shot learning

The shared dictionary in LCNN allows a neural network
to learn from very few training examples on novel cate-
gories, which is known as few-shot learning[17]. A good
model for few-shot learning should have two properties:
a) strong priors on the data, and b) few trainable parame-
ters. LCNN has both of these properties. An LCNN trained
on a large dataset of images (e.g. ImageNet [6]) will have a
rich dictionary D at each convolutional layer. This dictio-
nary provides a powerful prior on visual data. At the time of
ﬁne-tuning for a new set of categories with few training ex-
amples, we only update the coefﬁcients in C. This reduces
the number of trainable parameters signiﬁcantly.

In a standard CNN, to use a pre-trained network to clas-
sify a set of novel categories, we need to reinitialize the
classiﬁcation layer randomly. This introduces a large num-
ber of parameters, on which we don’t have any prior, and
they should be trained solely by a few examples. LCNN,
in contrast, can use the dictionary of the classiﬁcation layer
of the pre-trained model, and therefore only needs to learn
I and C from scratch, which form a much smaller set of
parameters. Furthermore, for all other layers, we only ﬁne-
tune the coefﬁcients C, i.e.,only update the non-zero entries
of P. Note that the dictionary D is ﬁxed across all layers
during the training with few examples.

3.3. Few-iteration learning

Training very deep neural networks are computationally
expensive and require hundreds of thousands of iterations.
This is mainly due to the complexity of these models. In or-
der to constrain the complexity, we should limit the number
of learnable parameters in the network. LCNN has a suit-
able setting that allows us to limit the number of learnable
parameters without changing the architecture. This can be
done by transferring the shared dictionaries D from a shal-
lower network to a deeper one.

Not only we can share a dictionary D across layers, but
we can also share it across different network architectures
Rm×k can be used
of different depths. A dictionary D
in any convolutional layer with input channel size m in any
CNN architecture. For example, we can train our dictionar-
ies on a shallow CNN and reuse in a deeper CNN with the
same channel size. On the deeper CNN we only need to
train the indices and coefﬁcients tensors I and C.

∈

4. Experiments

We evaluate the accuracy and the efﬁciency of LCNN
under different settings. We ﬁrst evaluate the accuracy and
speedup of our model for the task of object classiﬁcation,
evaluated on the standard image classiﬁcation challenge of
ImageNet, ILSRVC2012 [6]. We then evaluate the accu-
racy of our model under few-shot setting. We show that

Model
CNN
Wen et al. [44]
XNOR-Net[35]
LCNN-fast
LCNN-accurate

AlexNet
top-1
56.6
55.4
44.2
44.3
55.1

speedup
1.0×
3.1×2
8.0×3
37.6×
3.2×

top-5
80.2
N/A
69.2
68.7
78.1

Table 1. Comparison of different efﬁcient methods on AlexNet.
The accuracies are classiﬁcation accuracy on the validation set of
ILSVRC2012.

given a set of novel categories with as small as 1 training
example per category, our model is able to learn a classiﬁer
that is both faster and more accurate than the CNN base-
line. Finally we show that the dictionaries trained in LCNN
are generalizable and can be transferred to other networks.
This leads to a higher accuracy in small number of iterations
compared to standard CNN.

4.1. Implementation Details

We follow the common way of initializing the convolu-
tional layers by Gaussian distributions introduced in [12],
including for the sparse tensor P. We set the threshold in
equation 9 for each layer in such a way that we maintain
the same initial sparsity across all the layers. That is, we
set the threshold of each layer to be (cid:15) = c
σ, where c
is constant across layers and σ is the standard deviation of
Gaussian initializer for that layer. We use c = 0.01 for
AlexNet and c = 0.001 for ResNet. Similarly, to maintain
the same level of sparsity across layers we need a λ (equa-
tion 8) that is proportional to the standard deviation of the
Gaussian initializers. We use λ = λ(cid:48)(cid:15), where λ(cid:48) is constant
across layers and (cid:15) is the threshold value for that layer. We
try λ(cid:48)
0.1, 0.2, 0.3
for both AlexNet and ResNet to get
}
different sparsities in P.

∈ {

·

The dictionary size k, the regularizer coefﬁcient λ, and
threshold value (cid:15) are the three important hyperparameters
for gaining speedup. The larger the dictionary is, the more
accurate (but slower) the model becomes. The size of the
the dictionary for the ﬁrst layer does not need to be very
large as it’s representing a 3-dimensional space. We ob-
served that for the ﬁrst layer, a dictionary size as small as 3
vectors is sufﬁcient for both AlexNet and ResNet. In con-
trast, fully connected layers of AlexNet are of higher dimen-
sionality and a relatively large dictionary is needed to cover
the input space. We found dictionary sizes 512 and 1024
to be proper for fully connected layers. In AlexNet we use
the same dictionary size across other layers, which we vary

2They have not reported the overall speedup on AlexNet, but only per

layer speedup. 3.1× is the weighted average of their per layer speedups.

3XNOR-Net gets 32× layer-wise speedup on a 32 bit machine. How-
ever, since they haven’t binarized the ﬁrst and the last layer (which has
9.64% of the computation), their overall speedup is 8.0×.

Model
CNN
XNOR-Net[35]
LCNN-fast
LCNN-accurate

ResNet-18
top-1
69.3
51.2
51.8
62.2

speedup
1.0×
10.6×
29.2×
5×

top-5
90.0
73.2
76.8
84.6

Table 2. Comparison of LCNN and XNOR-Net on ResNet-18.
The accuracies are classiﬁcation accuracy on the validation set of
ILSVRC2012.

from 100 to 500 for different experiments. In ResNet, aside
from the very ﬁrst layer, all the other convolutional layers
are grouped into 4 types of ResNet blocks. The dimension-
ality of input is equal between same ResNet block types,
and is doubled for consecutive different block types. In a
similar way we set the dictionary size for different ResNet
blocks: equal between the same block types, and doubles
for different consecutive block types. We vary the dictio-
nary size of the ﬁrst block from 16 to 128 in different ex-
periments.

4.2. Image Classiﬁcation

In this section we evaluate the efﬁciency and the accu-
racy of LCNN for the task of image classiﬁcation. Our
proposed lookup based convolution is general and can be
applied on any CNN architecture. We use AlexNet [25] and
ResNet [18] architectures in our experiments. We use Ima-
geNet challenge ILSVRC2012 [6] to evaluate the accuracy
of our model. We report standard top-1 and top-5 classiﬁca-
tion accuracy on 1K categories of objects in natural scenes.
To evaluate the efﬁciency, we compare the number of ﬂoat-
ing point operations as a representation for speedup. The
speed and the accuracy of our model depend on two hy-
perparameters: 1) k, the dictionary size and 2) λ, which
controls the sparsity of P; i.e.,the average number of dic-
tionary components in the linear combination . One can

Figure 3. Accuracy vs. speedup. By tuning the dictionary size,
LCNN achieves a spectrum of speedups.

(a) cats, sofas and bicycles excluded

(b) 10 random categories excluded

Figure 4. Comparison between the performance of LCNN and CNN baseline on few-shot learning, for {1, 2, 4} examples per category. In
(a) all cats (7 categories), sofas (1 category) and bicycles (2 categories) are held out for few-shot learning. In (b), 10 random categories are
held out for few-shot learning. We repeat sampling the 10 random categories 5 times to avoid over-ﬁtting to a speciﬁc sampling.

set a trade-off between the accuracy and the efﬁciency of
LCNN by adjusting these two parameters. We compare our
model with several baselines: 1- XNOR-Net [35], which
reduces the precision of weights and outputs to 1-bit, and
therefore multiplications can be replaced by binary opera-
tions. In XNOR-Net, all the layers are binarized except the
ﬁrst and the last layer (in AlexNet, they contain 9.64% of
the computation). 2- Wen et al. [44], which speeds up the
convolutions by sparsifying the weight ﬁlters.

×

Table 1 compares the top-1 and top-5 classiﬁcation ac-
curacy of LCNN with baselines on AlexNet architecture. It
shows that with small enough dictionaries and sparse linear
combinations, LCNN offers 37.6
speedup with the accu-
racy of XNOR-Net. On the other hand, if we set the dic-
tionaries to be large enough, LCNN can be as accurate as
slower models like Wen et al. In LCNN-fast, the dictionary
size of the mid-layer convolutions is 30 and for the fully
connected layers is 512. In LCNN-accurate, the mid-layer
convolutions have a dictionary of size 500 and the size of
dictionary in fully connected layers is 1024. The regural-
izer constant (Section 4.1) λ(cid:48) for LCNN-fast and LCNN-
accurate is 0.3 and 0.1, respectively.

Depending on the dictionary size and λ(cid:48), LCNN can
achieve various speedups and accuracies. Figure 3 shows
different accuracies vs. speedups that our model can
achieve. The accuracy is computed by top-1 measure and
the speedup is relative to the original CNN model. It is in-
teresting to see that the trend is nearly linear. The best ﬁtted
3.08, i.e.,for each one percent accuracy
line has a slope of
that we sacriﬁce in top-1, we gain 3.08 more speedup.

−

We also evaluate the performance of LCNN on ResNet-
18 architecture. ResNet-18 is a compact architecture, which
has 5
fewer parameters in compare to AlexNet while it
achieves 12.7% higher top-1 accuracy. That makes it a

×

much more challenging architecture for further compres-
sion. Yet we show that we can gain large speedups with
a few points drop in the accuracy. Table 2 compares
the accuracy of LCNN, XNOR-Net [35], and the original
model (CNN). LCNN-fast is getting the same accuracy as
XNOR-Net while getting a much larger speedup. Moreover,
LCNN-accurate is getting a much higher accuracy yet main-
taining a relatively large speedup. LCNN-fast has dictio-
naries of size 16, 32, 64, and 128 for different block types.
LCNN-accuracte has larger dictionaries: 128, 256, 512 and
1024 for different block types.

4.3. Few-shot Learning

In this section we evaluate the performance of LCNN on
the task of few-shot learning. To evaluate the performance
of LCNN on this task, we split the categories of ImageNet
challenge ILSVRC2012 into two sets:
i) base categories,
a set of 990 categories which we use for pre-training, and
ii) novel categories, a set of 10 categories that we use for
few-shot learning.We do experiments under 1, 2, and 4 sam-
ples per category. We take two strategies for splitting the
categories. One is random splitting, where we randomly
split the dataset into 990 and 10 categories. We repeat the
random splitting 5 times and report the average over all. The
other strategy is to hold out all cats (7 categories), bicycles
(2 categories) and sofa (1 category) for few-shot learning,
and use the other 990 categories for pre-training. With this
strategy we make sure that base and novel categories do not
share similar objects, like different breeds of cats. For each
split, we repeat the random sampling of 1, 2, and 4 train-
ing images per category 20 times, and get the average over
all. Repeating the random sampling of the few examples is
crucial for any few-shot learning experiment, since a model
can easily overﬁt to a speciﬁc sampling of images.

Table 1CNNLCNN37.9439.9145.0747.7552.1652.78top-1 accuracy3540455055# of training examples per category12452.7847.7539.9152.1645.0737.94CNNLCNN 1Table 1CNN-random1LCNN-random1CNN-random2LCNN-random2CNN-random3LCNN-random3CNN-random4LCNN-random4CNN-random5LCNN-random5CNN-averageLCNN-average63.467.556.662.854.860.550.154.150.662.255.161.4274.177.468.472.764.669.764.166.863.374.966.972.381.782.477.679.876.176.574.076.275.482.576.9679.48top-1 accuracy4555657585# of training examples per category12479.4872.361.4276.9666.955.1CNNLCNNtop-1 accuracy4556.2567.578.7590# of training examples per categoryUntitled 1Untitled 2Untitled 382.477.467.581.774.163.4CNN-random1LCNN-random1top-1 accuracy4556.2567.578.7590# of training examples per categoryUntitled 1Untitled 2Untitled 379.872.762.877.668.456.6CNN-random2LCNN-random2top-1 accuracy4556.2567.578.7590# of training examples per categoryUntitled 1Untitled 2Untitled 376.569.760.576.164.654.8CNN-random3LCNN-random3top-1 accuracy4556.2567.578.7590# of training examples per categoryUntitled 1Untitled 2Untitled 376.266.854.17464.150.1CNN-random4LCNN-random4top-1 accuracy4556.2567.578.7590# of training examples per categoryUntitled 1Untitled 2Untitled 382.574.962.275.463.350.6CNN-random5LCNN-random5 1×

We compare the performance of CNN and LCNN on
few-shot learning in Figure 4. We ﬁrst train an original
AlexNet and an LCNN AlexNet on all training images of
base categories (990 categories, 1000 images per category).
We then replace the 990-way classiﬁcation layer with a ran-
domly initialized 10-way linear classiﬁer. In CNN, this pro-
duces 10
4096 randomly initialized weights, on which we
don’t have any prior. These parameters need to be trained
merely from the few examples.
In LCNN, however, we
transfer the dictionary trained in the 990-way classiﬁcation
layer to the new 10-way classiﬁer. This reduces the number
of randomly initialized parameters by at least a factor of 4.
We use AlexNet LCNN-accurate model (same as the one in
Table 1) for few-shot learning. At the time of ﬁne-tuning for
few-shot categories, we keep the dictionaries in all layers
ﬁxed and only ﬁne-tune the sparse P tensor. This reduces
the total number of parameters that need to be ﬁne-tuned by
. We use different learning rates η and η(cid:48) for
a factor of 14
the randomly initialized classiﬁcation layer (which needs to
be fully trained) and the previous pre-trained layers (which
only need to be ﬁne-tuned). We tried η(cid:48) = η, η(cid:48) = η
10 ,
η(cid:48) = η
100 and η(cid:48) = 0 for both CNN and LCNN, then picked
the best for each conﬁguration.

×

Figure 4 shows the top-1 accuracies of our model and the
baseline in the two splitting strategies of our few-shot learn-
ing experiment. In Figure 4 (a) we are holding out all cat,
sofa, and bicycle categories (10 categories in total) for few-
shot learning. LCNN is beating the baseline consistently
in
examples per category. Figure 4 (b) shows the
comparison in the random splitting strategy. We repeat ran-
domly splitting the categories into 990 and 10 categories 5
times, and report the average over all. Here LCNN gets a
larger improvement in the top-1 accuracy compared to the
baseline for

images per category.

1, 2, 4
{

}

1, 2, 4
{

}

4.4. Few-iteration Learning

In section 3.3 we discussed that the dictionaries in LCNN
can be transferred from a shallower network to a deeper one.
As a result, one can train fewer parameters–only I and C–
in the deeper network with few iterations obtaining a higher
test accuracy compared to a standard CNN. In this experi-
ment we train a ResNet with 1 block of each type, 10 layers
total. We then transfer the dictionaries of each layer to its
corresponding layer of ResNet-18 (with 18 layers). After
transfer, we keep the dictionaries ﬁxed. We show that we
get higher accuracy in small number of iterations compared
to standard CNN. Figure 5 illustrates the learning curves on
top-1 accuracy for both LCNN and standard CNN. The test
accuracy of LCNN is 16.2% higher than CNN at iteration
10K. The solid lines denote the training accuracy and the
dashed lines denote the test accuracy.

Figure 5. LCNN can obtain higher accuracy on few iterations
by transferring the dictionaries D from a shallower architecture.
This ﬁgure illustrates the learning curves on top-1 accuracy for
both LCNN and standard CNN. The accuracy of LCNN is 16.2%
higher than CNN at iteration 10K.

5. Conclusion

With recent advancements in virtual reality, augmented
reality, and smart wearable devices, the need for getting
the state of the art deep learning algorithms onto these re-
source constrained compute platforms increases. Porting
state of the art deep learning algorithms to resource con-
strained compute platforms is extremely challenging. We
introduce LCNN, a lookup-based convolutional neural net-
work that encodes convolutions by few lookups to a dictio-
nary that is trained to cover the space of weights in CNNs.
Training LCNN involves jointly learning a dictionary and a
small set of linear combinations. The size of the dictionary
naturally traces a spectrum of trade-offs between efﬁciency
and accuracy.

×

LCCN enables efﬁcient inference; our experimental re-
sults on ImageNet challenge show that LCNN can offer
3.2
speedup while achieving 55.1% top-1 accuracy us-
ing AlexNet architecture. Our fastest LCNN offers 37.6
×
speed up over AlexNet while maintaining 44.3% top-1 ac-
curacy. LCNN not only offers dramatic speed ups at infer-
ence, but it also enables efﬁcient training. On-device train-
ing of deep learning methods requires algorithms that can
handle few-shot and few-iteration constrains. LCNN can
simply deal with these problems because our dictionaries
are architecture agnostic and transferable across layers and
architectures, enabling us to only learn few linear combi-
nation weights. Our future work involves exploring low-
precision dictionaries as well as compact data structures for
the dictionaries.
Acknowledgments:
This work is in part supported
by ONR N00014-13-1-0720, NSF IIS-1338054, NSF-
1652052, NRI-1637479, Allen Distinguished Investigator
Award, and the Allen Institute for Artiﬁcial Intelligence.

Table 1CNN train lr 0.1CNN test lr 0.1LCNN train lr 0.1LCNN test lr 0.1CNN train lr 0.01CNN test lr 0.01LCNN train lr 0.01LCNN test lr 0.01train difftest diff00.10.10.10.10.10.10.10.10011.692.872.95.160.61.054.447.62.754.7324.985.0410.510.751.511.813.0616.048.081138.268.3317.517.792.252.6819.6619.1611.410.83411.4910.7222.2818.313.123.624.4121.1312.9210.41513.8911.2225.6325.794.024.1927.825.913.9114.68616.4512.7828.2923.944.824.8830.3229.2913.8716.51718.6913.8130.2126.085.635.7832.4530.8313.7617.02820.6217.7232.1828.596.426.2334.2432.1213.6214.4922.8317.333.4630.867.417.2536.1334.4513.317.151024.6919.1634.5231.358.07.6137.2535.3812.5616.221126.2521.1935.4531.558.818.7538.4436.912.1915.711227.7521.7436.5232.929.429.1139.4335.1411.6813.41329.323.2537.1832.6410.269.6740.3338.1411.0314.891430.5826.4137.7535.6211.029.7241.1638.9310.5812.521531.6226.7638.4735.5711.6611.4442.139.8710.4813.111632.7527.5938.9735.5612.2612.4342.3240.599.57131734.0630.1139.4835.7913.0111.0342.6937.058.636.941834.7130.4739.7234.1213.612.4743.7541.349.0410.871936.0130.6840.3938.3414.3413.6144.3841.48.3710.722037.334.0241.2636.2314.9814.5244.8442.557.540000000000018.529999999999992138.6234.6442.238.1115.7314.4445.2642.866.648.222239.7433.5943.1239.5416.5815.7945.8842.96.149.312340.9336.8243.5941.3317.0616.4946.1442.365.215.542442.0137.9844.2140.2917.7615.9846.4343.384.425.400000000000012542.7637.7345.0342.0318.4316.446.9742.394.214.662643.3140.2945.442.918.8818.2247.2544.413.944.122744.2540.9346.0243.4219.5619.0447.7144.323.463.392845.2141.3346.241.920.218.9548.1545.062.943.732946.1642.1546.9143.2220.7818.8148.3545.42.193.253046.6742.4947.4443.3521.4819.3948.6445.481.972.989999999999993147.2443.8747.7344.7321.9619.9848.9246.41.682.533247.8843.1148.2945.0822.4520.8149.4545.331.572.223348.3244.2248.4545.0323.119.9749.2746.460.9500000000000032.243448.5744.349.0146.5423.6121.7649.7547.021.182.720000000000013549.443.8949.3446.3624.1322.7949.7546.950.3500000000000013.063650.043.5949.645.3124.8221.8650.1347.190.1300000000000033.599999999999993750.3945.7149.8843.9125.0722.4550.3547.52-0.03999999999999911.813850.7346.0150.1546.8225.5823.8450.6847.57-0.04999999999999721.563951.347.4250.2147.8726.0224.6651.0848.11-0.2199999999999990.6899999999999984051.946.2550.7947.9226.6524.3551.3248.31-0.5799999999999982.064152.2648.3651.0947.1827.224.1351.4147.42-0.850000000000001-0.9399999999999984252.5748.8551.2647.7227.4525.6151.2748.39-1.3-0.4600000000000014352.9148.1751.7848.1128.126.4551.748.51-1.209999999999990.3399999999999964453.1449.2751.8548.228.4226.9251.7449.02-1.4-0.254553.348.8351.8948.6328.825.4952.0948.34-1.20999999999999-0.4899999999999954653.7448.6352.3249.129.2227.652.3549.1-1.390.4699999999999994754.2349.3252.4749.3529.7726.6452.649.4-1.630.07999999999999834854.650.5952.4949.5830.1327.6752.5949.02-2.01-1.574954.9551.0852.8449.7230.4828.0852.6949.64-2.26000000000001-1.445055.0651.0152.8249.3330.9128.7553.0249.57-2.04-1.445155.551.0753.3350.2331.5228.6753.1749.53-2.33-1.545255.6151.7653.4849.6931.6330.0353.3349.95-2.28-1.815355.6751.4453.4450.4632.0829.1653.3949.93-2.28-1.515456.151.6353.9550.532.4430.1654.0450.3-2.06-1.330000000000015556.3150.7354.0250.4533.0530.4153.8349.95-2.48-0.779999999999994top-1 accuracy0102030405060# of iterations01020304050CNN trainCNN testLCNN trainLCNN test16.2 %x1000 1References

[1] S. Anwar, K. Hwang, and W. Sung. Fixed point optimization
of deep convolutional neural networks for object recognition.
In Acoustics, Speech and Signal Processing (ICASSP), 2015
IEEE International Conference on, pages 1131–1135. IEEE,
2015. 2

[2] H. Azizpour, A. Sharif Razavian, J. Sullivan, A. Maki, and
S. Carlsson. From generic to speciﬁc deep representations
In Proceedings of the IEEE Con-
for visual recognition.
ference on Computer Vision and Pattern Recognition Work-
shops, pages 36–45, 2015. 2

[3] W. Chen, J. T. Wilson, S. Tyree, K. Q. Weinberger, and
Y. Chen. Compressing neural networks with the hashing
trick. In ICML, 2015. 1, 2

[4] M. Courbariaux and Y. Bengio. Binarynet: Training deep
neural networks with weights and activations constrained to
+1 or -1. CoRR, 2016. 2

[5] M. Courbariaux, Y. Bengio, and J.-P. David. Binaryconnect:
Training deep neural networks with binary weights during
propagations. In Advances in Neural Information Processing
Systems, pages 3105–3113, 2015. 2

[6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. Imagenet: A large-scale hierarchical image database. In
CVPR09, 2009. 5, 6

[7] M. Denil, B. Shakibi, L. Dinh, M. Ranzato, and N. de Freitas.
Predicting parameters in deep learning. In NIPS, 2013. 1, 2
[8] E. L. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fer-
gus. Exploiting linear structure within convolutional net-
works for efﬁcient evaluation. In NIPS, 2014. 2

[9] L. Fei-Fei, R. Fergus, and P. Perona. One-shot learning of
object categories. IEEE transactions on pattern analysis and
machine intelligence, 28(4):594–611, 2006. 2

[10] R. Girshick. Fast r-cnn. In Proceedings of the IEEE Inter-
national Conference on Computer Vision, pages 1440–1448,
2015. 1

[11] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-
ture hierarchies for accurate object detection and semantic
In Proceedings of the IEEE conference on
segmentation.
computer vision and pattern recognition, pages 580–587,
2014. 1

[12] X. Glorot and Y. Bengio. Understanding the difﬁculty of
In AISTATS,

training deep feedforward neural networks.
2010. 6

[13] Y. Gong, L. Liu, M. Yang, and L. Bourdev. Compress-
ing deep convolutional networks using vector quantization.
arXiv preprint arXiv:1412.6115, 2014. 2

[14] M. Gottmer. Merging reality and virtuality with microsoft

hololens. 2015. 1

[15] S. Han, H. Mao, and W. J. Dally. Deep compression: Com-
pressing deep neural network with pruning, trained quanti-
zation and huffman coding. In ICLR, 2015. 1, 2

[16] S. Han, J. Pool, J. Tran, and W. Dally. Learning both weights
and connections for efﬁcient neural network. In Advances in
Neural Information Processing Systems, pages 1135–1143,
2015. 2

[17] B. Hariharan and R. Girshick. Low-shot visual object recog-

nition. arXiv preprint arXiv:1606.02819, 2016. 2, 5

[18] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. CoRR, 2015. 1, 6

[19] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and
Y. Bengio. Quantized neural networks: Training neural net-
works with low precision weights and activations. arXiv
preprint arXiv:1609.07061, 2016. 2

[20] K. Hwang and W. Sung. Fixed-point feedforward deep neu-
In Signal
ral network design using weights+ 1, 0, and- 1.
Processing Systems (SiPS), 2014 IEEE Workshop on, pages
1–6. IEEE, 2014. 2

[21] F. N. Iandola, M. W. Moskewicz, K. Ashraf, S. Han, W. J.
Dally, and K. Keutzer. Squeezenet: Alexnet-level accuracy
with 50x fewer parameters and ¡1mb model size. CoRR,
abs/1602.07360, 2016. 2

[22] M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up
convolutional neural networks with low rank expansions. In
British Machine Vision Conference (BMVC), 2014. 1, 2
[23] M. Kim and P. Smaragdis. Bitwise neural networks. arXiv

preprint arXiv:1601.06071, 2016. 2

[24] G. Koch, R. Zemel, and R. Salakhutdinov. Siamese neu-
ral networks for one-shot image recognition. In ICML Deep
Learning Workshop, 2015. 2

[25] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
classiﬁcation with deep convolutional neural networks.
In
Advances in neural information processing systems, pages
1097–1105, 2012. 1, 6

[26] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, 2012. 2

Imagenet
In

[27] C. H. Lampert, H. Nickisch, and S. Harmeling. Attribute-
based classiﬁcation for zero-shot visual object categoriza-
tion. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 36(3):453–465, 2014. 2

[28] J. Lei Ba, K. Swersky, S. Fidler, et al. Predicting deep zero-
shot convolutional neural networks using textual descrip-
tions. In Proceedings of the IEEE International Conference
on Computer Vision, pages 4247–4255, 2015. 2

[29] Z. Lin, M. Courbariaux, R. Memisevic, and Y. Bengio.
Neural networks with few multiplications. arXiv preprint
arXiv:1510.03009, 2015. 2

[30] E. Littwin and L. Wolf. The multiverse loss for robust trans-
fer learning. arXiv preprint arXiv:1511.09033, 2015. 2
[31] B. Liu, M. Wang, H. Foroosh, M. Tappen, and M. Pensky.
Sparse convolutional neural networks. In CVPR, 2015. 2
[32] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 3431–3440, 2015. 1

[33] V. Oculus. Oculus rift-virtual reality headset for 3d gaming.

URL: http://www. oculusvr. com, 2012. 1

[34] P. O. Pinheiro, R. Collobert, and P. Dollar. Learning to seg-
ment object candidates. In Advances in Neural Information
Processing Systems, pages 1990–1998, 2015. 1

[35] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. Xnor-
net: Imagenet classiﬁcation using binary convolutional neu-
ral networks. In ECCV, 2016. 1, 2, 6, 7

[36] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards
real-time object detection with region proposal networks. In
Advances in Neural Information Processing Systems, pages
91–99, 2015. 1

[37] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 1

[38] S. Srinivas and R. V. Babu. Data-free parameter pruning for
deep neural networks. In British Machine Vision Conference
(BMVC), 2015. 2

[39] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
pages 1–9, 2015. 1

[40] S. Thrun. Is learning the n-th thing any easier than learning
the ﬁrst? Advances in neural information processing sys-
tems, pages 640–646, 1996. 2

[41] V. Vanhoucke, A. Senior, and M. Z. Mao.

Improving the
In Deep Learning and

speed of neural networks on cpus.
Unsupervised Feature Learning NIPS Workshop, 2011. 2
[42] O. Vinyals, C. Blundell, T. Lillicrap, K. Kavukcuoglu, and
D. Wierstra. Matching networks for one shot learning. arXiv
preprint arXiv:1606.04080, 2016. 2

[43] Q. Wang, X. Zhang, Y. Zhang, and Q. Yi. Augem: automati-
cally generate high performance dense linear algebra kernels
on x86 cpus. In Proceedings of the International Conference
on High Performance Computing, Networking, Storage and
Analysis, 2013. 4

[44] W. Wen, C. Wu, Y. Wang, Y. Chen, and H. Li. Learning
structured sparsity in deep neural networks. In NIPS, 2016.
2, 6, 7

[45] J. Wu, C. Leng, Y. Wang, Q. Hu, and J. Cheng. Quantized
convolutional neural networks for mobile devices. arXiv
preprint, 2015. 2

[46] Z. Yang, M. Moczulski, M. Denil, N. de Freitas, A. Smola,
L. Song, and Z. Wang. Deep fried convnets. In ICCV, 2015.
2

[47] X. Zhang, J. Zou, X. Ming, K. He, and J. Sun. Efﬁcient
and accurate approximations of nonlinear convolutional net-
works. In CVPR, 2015. 2

