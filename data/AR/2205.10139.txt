2
2
0
2

y
a
M
0
2

]

G
L
.
s
c
[

1
v
9
3
1
0
1
.
5
0
2
2
:
v
i
X
r
a

Towards efﬁcient feature sharing in MIMO architectures

R´emy Sun1,3,* Alexandre Ram´e1

Cl´ement Masson3

Nicolas Thome2 Matthieu Cord1,4

1MLIA, ISIR, Sorbonne Universit´e
2Vertigo, CEDRIC, Conservatoire National des Arts et M´etiers

3Thales Land and Air Systems, Elancourt, France

4Valeo.ai

Abstract

Multi-input multi-output architectures propose to train
multiple subnetworks within one base network and then av-
erage the subnetwork predictions to beneﬁt from ensembling
for free. Despite some relative success, these architectures
are wasteful in their use of parameters. Indeed, we highlight
in this paper that the learned subnetwork fail to share even
generic features which limits their applicability on smaller
mobile and AR/VR devices. We posit this behavior stems
from an ill-posed part of the multi-input multi-output frame-
work. To solve this issue, we propose a novel unmixing step
in MIMO architectures that allows subnetworks to prop-
erly share features. Preliminary experiments on CIFAR 100
show our adjustments allow feature sharing and improve
model performance for small architectures.

1. Introduction

The last decade has seen large deep architectures take
over many machine learning domains [3, 5] previously
solved by more traditional algorithms. As such, deep learn-
ing has become ever more present in practical applications.
It is therefore now especially important to ﬁnd ways to max-
imize model performance [12, 16].

A well known way to obtain better performances given
already trained models is to ensemble the predictions given
by multiple models [6]. Indeed, predictions from indepen-
dently trained models have been shown to complement each
other such that the aggregated predictions largely outper-
form the individual model predictions on a test set.

Unfortunately, this increase in performance comes at the
cost of dramatically increased overhead : to ensemble mul-
tiple models, one must have access to multiple trained mod-
els [1,6]. This is an untenable cost in many real world appli-
cations where networks must ﬁt on tiny embedded chips in

*Corresponding author

mailto: remy.sun@isir.upmc.fr

(a) Previous MIMO models train subnetworks that fail to share features.

(b) MixShare introduces mechanisms that allow feature sharing.

Figure 1. The lack of feature sharing in MIMO models is a
missed opportunity for parameter efﬁciency on smaller networks.
Mixshare introduces unmixing to allow feature sharing among
subnetworks in multi-input multi-output models, improving them
on smaller networks previous methods struggle on.

mobile and AR/VR devices. Signiﬁcant emphasis has there-
fore been put in the ensembling literature on ﬁnding ways to
minimize the inherent cost of ensembling, typically through
some degree of parameter sharing between models [7, 13].
Multi-input multi-output (MIMO) strategies [2, 11] pro-
vide an interesting solution to this conundrum by ensem-
bling for virtually free. Through their multiple inputs and
outputs, MIMO frameworks train independent subnetworks
within a base network. Thanks to the sparse nature of large
neural networks [8], the resulting subnetworks yield strong
and diverse predictions that can be ensembled. As shown

1

 
 
 
 
 
 
on Fig. 1a with M = 2, the M inputs are embedded by M
subnetworks with no structural differences. Thus, we have
M (inputs, labels) pairs in training: {(xi, yi)}0≤i<M . More
precisely, M images are fed to the network at once. The
M inputs are encoded by M distinct convolutional layers
{ci}0≤i<M into a shared latent space before being aggre-
gated (either through summation [2] or more complex mix-
ing operations [11]). This representation is then processed
by the core network into a single feature vector, which is
classiﬁed by M dense layers {di}0≤i<M . Diverse subnet-
works appear as di learns to classify yi from input xi. At
inference, we can ensemble M predictions by feeding the
same image M times to the model.

MixMo [11] has however recently highlighted signiﬁ-
cant limitations of such architectures: multi-input multi-
output architectures require large base models and struggle
to ﬁt more than 2 subnetworks. Indeed, [11] shows a signif-
icant drop in performance on CIFAR 100 when going from
2 subnetworks to 4 subnetworks.

This scaling issue is explained by analyzing the features
inside the network, as we show at the beginning of this pa-
per by extending [2, 11]’s study of subnetwork behavior.
Our analysis shows that the aforementioned scaling issues
stem from how subnetworks share no features in the base
network (see Fig. 1a): each channel or feature is almost ex-
clusively used by one subnetwork. As such, we can explain
the scaling issue since each additional subnetwork signif-
icantly reduces the effective size of the individual subnet-
works. Beyond causing issues on smaller architectures or
harder datasets, this leads to very wasteful use of network
parameters. This is especially unfortunate as the subnet-
works could at the very least share generic features in the
ﬁrst layers. We see this as a missed opportunity, one that
can signiﬁcantly improve multi-input multi-output models’
applicability to real world settings like mobile devices.

In this paper, we ﬁrst carefully study in Sec. 2 how sub-
networks use the base network’s features. After showing
the lack of feature sharing, we discuss the impact of this on
parameter efﬁciency and model performance. Secondly, we
propose Mixshare in Sec. 3 to address the issues preventing
feature sharing (see Fig. 1b). In particular, we introduce a
novel unmixing mechanism (Sec. 3.1) to allow sharing and
discuss in Sec. 3.2 how proper network initialization is nec-
essary to improve model performance.

2. MIMO Subnetworks do not share features

In this section, we strive to pinpoint the cause of multi-
input multi-output architectures’ scaling issues. To this end,
we consider the following question: how do subnetworks
behave in multi-input multi-output architectures ?

Following [11], we check how the inputs are organized in
the C feature maps of the mixing space by considering the
L1 norm of the C encoder kernels for each subnetwork (see

Figure 2. We study the inﬂuence of features in the input and out-
put block on the subnetworks (L1 norm ﬁgured by bars). For the
input block, we consider the L1 norm of the feature kernels for the
relevant encoders. For the output block, we look at the L1 norm
of columns in the classiﬁer weights matrices. On the ﬁgure, this
shows us the ﬁrst input block kernel is mostly used by the ﬁrst
(red) subnetwork (7 vs. 1). Similarly, the ﬁrst feature of the output
block proves important only to the ﬁrst (red) classiﬁer.

Fig. 2). This tells us whether a feature map contains more
information about one input, and we can visualize which
maps are used by which subnetwork through histograms h0
and h1 of feature inﬂuence for each subnetwork. Quantita-
tively, we can approximate the feature sharing rate through
the ratio of min(h0,h1)
max(h0,h1) . In the same spirit, we consider the
L1 norm of the columns of classiﬁer weight matrices to
quantify the importance of each feature to each classiﬁer.

We conduct our study on a WideResNet-28-2 [15] using
the more realistic batch repetition 2 setting from [11] on
the CIFAR 100 dataset [4] (see Appendix). We choose to
consider this situation as it perfectly showcases the issues
encountered by MIMO methods on smaller architectures.
To complement this, we also show results on the slightly
larger WideResNet-28-5 later on in the paper.

Fig. 3 shows the subnetworks are fully independent in
the core network: each channel in the input block encodes
information about only one input, as the corresponding ker-
nel of the other encoder’s L1 is very low. A similar behavior
is observed in the output block, and further analysis of input
inﬂuence on intermediary feature maps shows this behavior
remains consistent within the network (See Appendix).

Multi-input multi-output architectures’ scaling issues be-
come much easier to understand in light of this: the amount
of weights available to each of the underlying subnetworks
decreases quadratically with the number of subnetwork. In-

2

(a) Encoder kernels L1 norm.

(b) Classiﬁer columns L1 norm.

(a) Encoder kernels L1 norm.

(b) Classiﬁer columns L1 norm.

Figure 3. Features are used by one subnetwork or the other, never
both at the same time: the overlap (orange) is very low.

Figure 5. Applying unmixing leads to features being used by both
subnetworks: the overlap (orange) is very high.

3.1. Unmixing: extracting features for each input

We build upon an intuition put forth in MixMo [11]: the
lack of feature sharing is caused by the need for individual
classiﬁer at the end of the network to extract class infor-
mation for one input speciﬁcally. Indeed, the M classiﬁers
have access to the exact same set of extracted features. If
two classiﬁers use the same feature, that feature needs to
describe the state of two different inputs. This is an issue
when one accounts for the fact inputs are in fact drawn inde-
pendently and there can therefore be no meaningful feature
describing the state of two inputs simultaneously.

Let us now consider how the classiﬁers should ideally
behave on shared features. Since each classiﬁer is paired
to one of the input pathways, they should be able to extract
two different interpretations of the shared features that still
encode the same functional information (see Fig. 4). For
instance, the shared feature should encode for the presence
of ﬂowers but each classiﬁer should be able to infer from
the feature whether its personal input contains ﬂowers.

While this is not the case in traditional CNNs, MixMo
[11] introduces a modiﬁcation to the seminal MIMO ar-
chitecture that causes feature maps to encode information
about the different inputs separately. Indeed, since MixMo
mixes inputs according to some binary mixing augmenta-
tion scheme (typically CutMix [14]), each pixel on the ﬁnal
feature maps encodes information about one of the inputs.
This is fortunate as it provides us with a fairly natural
solution: unmixing. Unmixing (illustrated in Fig. 4) recy-
cles the binary masks generated for input mixing in order to
ﬁlter the feature maps so that only information relevant to
a speciﬁc input is contained in the unmixed version. This
way, a single feature map can describe each of the inputs.

Fig. 5 shows that applying unmixing causes the subnet-
works to share features, both in the input and output block.
In fact, every feature in the unmixed model is used by all
subnetworks which proves unmixing indeed solves the core
obstacle to feature sharing in MIMO networks.

Introducing unmixing however leads to unstable and
generally worse performance as seen in Tab. 1. Crucially,
even individual subnetwork accuracy suffers from unmix-
ing which suggests an underlying issue.

Figure 4. Two steps are necessary to allow feature sharing in net-
works: 1) Ensure the subnetworks share a “common language” by
initializing the convolutional encoders to be close to each other. 2)
Extract descriptions of each input from model features.

deed, since feature maps of different subnetworks cannot
communicate, only 1
M weights can be non-zero. This frac-
tion of non-zero weights must then be distributed between
the M subnetworks. Furthermore, the subnetworks likely
extract similar generic features, at least in the ﬁrst layers.
Since the subnetworks share no features, this means those
features are unnecessarily replicated for each subnetwork.

This is not wholly surprising or undesirable behavior as
MIMO strives to train M independent subnetworks to ob-
tain diverse ensembles. By avoiding overlap between sub-
networks, the subnetworks act as a standard ensemble of
smaller models, with the base model size acting as hard cap
on the number of the parameters used by the ensemble.

While it is true not sharing any features ensures subnet-
works’ independence, it seems unnecessary.
Indeed, the
subnetworks are highly unlikely to extract completely dif-
ferent features. As such, subnetworks should beneﬁt from
sharing features at least in the early layers even if the clas-
siﬁer still consider fairly different features.

At ﬁrst blush, nothing in the MIMO training protocol
explicitly requires the subnetworks not share any features.
Why do the subnetworks avoid sharing features ? How
could we encourage them to share some parameters ?

3. How can subnetworks share features ?

We discuss here the obstacles preventing feature sharing
in multi-input multi-output architectures, and propose solu-
tions to correct this behavior.

3

Method

28-2

28-5

Acc. ens.

Acc. Ind.

Classiﬁer share rate

Acc. ens.

Acc. Ind.

Classiﬁer share rate

MixMo
MixMo + Unmix
MixMo + Unmix + kernel init.

74.8 ± 0.2
69.7 ± 15.4
79.0 ± 0.1

71.6 ± 0.2
69.7 ± 15.4
79.0 ± 0.1

MixShare (partial on 25% features)
MixShare (fadeout to 100 epochs)

73.3 ± 0.5
79.0 ± 0.1

71.5 ± 1.0
76.7 ± 1.3

4.9 ± 0.4
99.1 ± 0.4
99.4 ± 0.1

60.6 ± 6.5
64.4 ± 6.0

81.9 ± 0.1
79.4 ± 2.7
82.1 ± 0.2

79.4 ± 0.2
79.4 ± 2.7
82.1 ± 0.2

79.9 ± 0.3
82.4 ± 0.3

78.8 ± 0.4
81.6 ± 0.5

8.2 ± 0.1
98.8 ± 0.7
99.3 ± 0.1

60.0 ± 2.3
62.7 ± 8.3

Table 1. Overall ensemble accuracy (%), average subnetwork accuracy (%) and classiﬁer (output block) sharing rate (see Sec. 2) for MixMo
variants, mean ± std reported over 3 runs. Mixshare with fadeout unmixing yields both strong individual models and ensemble gains.

3.2. Aligning encoder kernels to allow efﬁcient fea-

ture sharing

Intuitively, feature sharing should at the very least lead to
higher individual subnetwork accuracy as the subnetworks
use more parameters. As such, we now investigate why un-
mixing degrades performance so dramatically.

By extracting multiple possible interpretations of a sin-
gle feature, unmixing introduces a new problem in the
model.
Indeed, we need our interpretations of the same
feature to encode the same functional characteristics (e.g.
ﬂower detection). The issue is that a randomly initialized
multi-input multi-output network typically leads to having
multiple interpretations of the same feature.

Indeed, the encoders computing the mixed representa-
tions are very different. For an input feature, the mixed fea-
ture map could contain information about horizontal bor-
ders on input 1 and vertical borders on input 2. As such,
there is no consistent interpretation for our mixed features.
We can unify the interpretation of unmixed features at
the start by simply aligning the kernels of the encoders.
Indeed, as long as each feature encodes the same sort of
information for each encoder, there should be no ambiguity
introduced by the unmixing process.

Tab. 1 shows that ﬁxing the initialization scheme of the
encoders to the same value does indeed lead the model to
outperform normal mixmo models.

3.3. Towards partial feature sharing

While proper unmixing does allow feature sharing in
multi-input multi-output networks, Tab. 1 and Fig. 5 show it
leads to subnetworks sharing all features: the subnetworks
are identical. This is even less desirable than fully separated
subnetworks as it makes ensembling pointless [9, 10].

Ideally, subnetworks would share some parameters but
still remain distinct functionally. This way, we would be
able to strike a compromise between fully separated and
fully shared subnetworks. The issue with this however, is
that removing obstacles to feature sharing makes it unnec-
essary for subnetworks to separate in any way.

In this preliminary work, we discuss two solutions: par-
tial unmixing and fadeout unmixing. Partial unmixing is

a straightforward solution where we only apply unmixing to
a ﬁxed subset of the ﬁnal feature maps (e.g. 25%). In Fade-
out unmixing we start training the network with proper un-
mixing but progressively reduce the strength of unmixing so
that there is no unmixing towards the end of the procedure.
For instance, we use the unmixing mask M + r(1 − M )
(instead of M ) with r = min(1, epoch/100) if we want to
stop unmixing by epoch 100. As such, fadeout unmixing
initializes the network in a shared state and progressively
pushes the subnetworks to develop independent features.

We now propose the full MixShare framework by com-
bining proper kernel initialization and partial/fadeout un-
mixing along with slight adjustments to standard MIMO
procedures like input repetition [2] and loss balancing [11]
(see Appendix). Tab. 1 shows that both MixShare variants
succeed in causing partial feature sharing. Partial fails to
train strong individual subnetworks, but still showcases en-
semble beneﬁts. Fadeout on the other hand leads to strong
performances and retains signiﬁcant ensembling beneﬁts on
medium sized networks like a WideResNet 28-5.

4. Conclusion

We have shown multi-input multi-output models in-
duce fully separated subnetworks because of a difﬁculty
in matching outputs to inputs for the neural network. We
have proposed an unmixing mechanism and encoder initial-
ization for MixMo [11] architectures and demonstrated it
allows multi-input multi-output architectures to share fea-
tures. Our preliminary experiments show this corrected
architecture outperforms standard multi-input multi-output
architectures on smaller networks with a proper unmixing
scheme. We hope that by highlighting the main issue at the
crux of these architectures’ inefﬁciency, our work will lead
to further research on MIMO architectures that will lead to
their deployment smaller mobile and AR/VR devices.

Acknowledgments This work was conducted using
from GENCI–IDRIS (Grant 2021-
HPC resources
AD011013208), and under a CIFRE grant between Thales
Land and Air Systems and Sorbonne University.

4

References

[1] Lars Kai Hansen and Peter Salamon. Neural network en-
sembles. IEEE transactions on pattern analysis and machine
intelligence, 1990. 1

[16] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and
David Lopez-Paz. mixup: Beyond empirical risk minimiza-
tion. In International Conference on Learning Representa-
tions, 2018. 1

Jenatton,

[2] Marton Havasi, Rodolphe

Stanislav Fort,
Jeremiah Zhe Liu, Jasper Snoek, Balaji Lakshminarayanan,
Andrew Mingbo Dai, and Dustin Tran. Training indepen-
In International
dent subnetworks for robust prediction.
Conference on Learning Representations, 2021. 1, 2, 4, 6
[3] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Identity mappings in deep residual networks. In European
Conference on Computer Vision, 2016. 1

[4] Alex Krizhevsky et al. Learning multiple layers of features

from tiny images. 2009. 2

[5] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works. In Advances in Neural Information Processing Sys-
tems, 2012. 1

[6] Balaji Lakshminarayanan, Alexander Pritzel, and Charles
Blundell. Simple and scalable predictive uncertainty esti-
mation using deep ensembles. In Advances in neural infor-
mation processing systems, 2017. 1

[7] Stefan Lee, Senthil Purushwalkam, Michael Cogswell,
David J. Crandall, and Dhruv Batra. Why M heads are bet-
ter than one: Training a diverse ensemble of deep networks.
Arxiv preprint, 2015. 1

[8] Eran Malach, Gilad Yehudai, Shai Shalev-Schwartz, and
Ohad Shamir. Proving the lottery ticket hypothesis: Prun-
ing is all you need. In International Conference on Machine
Learning, 2020. 1

[9] Tianyu Pang, Kun Xu, Chao Du, Ning Chen, and Jun Zhu.
Improving adversarial robustness via promoting ensemble
diversity. In International Conference on Machine Learning,
2019. 4

[10] Alexandre Rame and Matthieu Cord. Dice: Diversity in
deep ensembles via conditional redundancy adversarial es-
In International Conference on Learning Repre-
timation.
sentations, 2021. 4

[11] Alexandre Rame, Remy Sun, and Matthieu Cord. Mixmo:
Mixing multiple inputs for multiple outputs via deep sub-
networks. In International Conference on Computer Vision,
2021. 1, 2, 3, 4, 6

[12] Divya Shanmugam, Davis Blalock, Guha Balakrishnan, and
John Guttag. Better aggregation in test-time augmentation.
In International Conference on Computer Vision, 2021. 1

[13] Yeming Wen, Dustin Tran, and Jimmy Ba. Batchensemble:
an alternative approach to efﬁcient ensemble and lifelong
In International Conference on Learning Repre-
learning.
sentations, 2019. 1

[14] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk
Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu-
larization strategy to train strong classiﬁers with localizable
features. In International Conference on Computer Vision,
2019. 3, 6

[15] Sergey Zagoruyko and Nikos Komodakis. Wide residual net-
works. In British Machine Vision Conference, 2016. 2

5

Appendix

A. Experimental details

The code for this work was directly adapted from the of-
ﬁcial MixMo [11] codebase: https://github.com/
alexrame/mixmo-pytorch.

We followed similar experimental settings on CIFAR
100 as MixMo [11] and present here the adapted setting de-
scription:

We used standard architecture WRN-28-w, with a fo-
cus on w = 2. We re-use the hyper-parameters con-
ﬁguration from MIMO [2] with batch repetition 2 (bar2).
The optimizer is SGD with learning rate of 0.1
b × batch-size
,
batch size 64, linear warmup over 1 epoch, decay rate 0.1
at steps {75, 150, 225}, l2 regularization 3e-4. We follow
standard MSDA practices [14] and set the maximum num-
ber of epochs to 300. Our experiments ran on a single
NVIDIA 12Go-TITAN X Pascal GPU.

128

All experiments were run three times on three ﬁxed seeds
from the same version of the codebase. Qualitative results
presented in Fig. 3 and Fig. 5 are obtained by visualizing
results for the ﬁrst set of random seeds. Quantitative results
presented in Tab. 1 are given in the form of mean ± std
over the three runs.

B. Complementary adjustments to MIMO pro-

cedures in MixShare

MIMO methods use a number of auxiliary procedures
to train strong subnetworks. However, as MixShare differs
signiﬁcantly from standard MIMO frameworks, it does not
use these frameworks to the same extent.

CutMix probability in the input block MixMo [11] only
uses cutmix mixing in its input block about half the time,
using a basic summing operation on the two encoded inputs
the rest of the time. This is because the model will use a
summing operation at test time. Therefore, the use of cut-
mix at training induces an strong train/test gap that needs to
be bridged by the use of summing during training.

We cannot afford to use summing half the time as un-
mixing relies on the use of cutmix in the input block. How-
ever, since our two encoders are very similar (due to our ker-
nel alignment), cutmix and summing (or averaging) behave
very similarly and the train/test gap is therefore minimal.

Input Repetition A slight train/test gap still remains
however since the model is rarely presented the same image
as input to both subnetworks at training time. We solve this
by reprising a procedure introduced in the seminal MIMO
paper [2]: input repetition. In our case, we ensure 10% of
inputs of our batches are made of repetition of the same im-
age during training.

Loss rebalancing MixMo [11] introduced a re-weighting
function of the subnetwork training losses that rescales the
mixing ratios used in the inputs block. These ratios are
rescaled to be less lopsided (closer to an even 50/50 split)
before being applied to their relevant subnetwork losses.
This rescaling is necessary as it ensures all parameters re-
ceive sufﬁcient training signal.

We however ﬁnd in our experiments it is more beneﬁcial
to do away with this re-balancing and keep the original mix-
ing ratios, which we explain by the large amount of features
shared between subnetworks. Since features are shared, we
do not need to worry about some features receiving too little
training signal.

C. A more nuanced discussion on kernel align-

ment

While MixShare uses the exact same initialization of the
encoder kernels for simplicity, it is interesting to note much
weaker versions of kernel alignment are sufﬁcient to obtain
similar results.

Indeed, we found in our experiments that initializing the
kernels to be simply co-linear is more than enough to en-
sure proper feature sharing. In fact, this leads to the exact
same performance as using the same initialization and the
encoder kernels quickly converge to similar values.

This further validates our intuition that MIMO models
need a “common language” to beneﬁt from sharing features:
all that is required is for encoder kernels to extract the same
“type” of features.

D. Analysis of subnetwork features within the

core network

Sec. 2 studies what features each subnetwork uses in the
input block and output block of the multi-input multi-output
model. Studying the importance of features within the core
networks for each subnetworks is more difﬁcult as it is not
possible to consider the model weights. Reprising an anal-
ysis conducted in the Appendix of [11], we identify the in-
ﬂuence of intermediate features on subnetworks with the
variance of the feature with respect to the relevant input.

For the ﬁrst subnetwork, if we consider the intermedi-
ate feature map (at one point in the network f ) Mint =
fint(Dtest, d), such that Mint is of shape N × C × H × W
with Dtest the test set, d a ﬁxed input, N the size of the test
set, C the number of intermediate feature maps and H × W
the spatial coordinates. We compute the importance of each
of the C feature map with respect to the ﬁrst subnetwork as
M ean(V ar(Mint, dim = 0), dim = (1, 2)). The impor-
tance of intermediate features for the second subnetwork is
obtained similarly by considering Mint = fint(d, Dtest).

Fig. 6 shows the resulting feature importance maps at
after each of the three residual blocks in the core network.

6

(a) Feature variance after block 1.

(b) Feature variance after block 2.

(c) Feature variance after block 3.

Figure 6. Checking the variance of feature maps w.r.t. the two inputs at different levels of the network shows clear separation of features
in standard multi-input multi-output architectures.

As can be observed, the subnetworks remain consistently
separated in the core network.

7

