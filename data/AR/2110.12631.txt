Time series signal recovery methods: comparative
study

Firuz Kamalov
Department of Electrical Engineering
Canadian University Dubai
Dubai, UAE
ﬁruz@cud.ac.ae

Hana Sulieman
Department of Mathematics and Statistics
American University of Sharjah
Sharjah, UAE
hsulieman@aus.edu

1
2
0
2

t
c
O
5
2

]

O
C

.
t
a
t
s
[

1
v
1
3
6
2
1
.
0
1
1
2
:
v
i
X
r
a

Abstract—Signal data often contains missing values. Effective
replacement (imputation) of the missing values can have sig-
niﬁcant positive effects on processing the signal. In this paper,
we compare three commonly employed methods for estimating
missing values in time series data: forward ﬁll, backward ﬁll,
and mean ﬁll. We carry out a large scale experimental analysis
using 3,600 AR(1)-based simulated time series to determine the
optimal method for estimating missing values. The results of the
numerical experiments show that the forward and backward ﬁll
methods are better suited for times series with large positive
correlations, while the mean ﬁll method is better suited for
times series with low or negative correlations. The extensive
and exhaustive nature of the numerical experiments provides
a deﬁnitive answer to the comparison of the three imputation
methods.

Index Terms—time series, ﬁlling methods,

imputation, AR,

PACF, autoregression

I. INTRODUCTION

One of the common issues that plagues real life data is
missing values. The problem of missing data is present in
many applications including signal processing, ﬁnance, meteo-
rology, medicine, and others. Errors in collection and recording
can lead to incomplete data which can lead to false results.
Analysis and conclusions derived based on incomplete data
can be misleading. Therefore, the development of effective
solutions for dealing with incomplete data is an important area
of research. In this paper, we focus on handling incomplete
data in the context of time series. The process of replacing the
missing values is commonly referred to as imputation. Dealing
with time series data poses its unique challenges. Our goal is
to compare various approaches to ﬁll (impute) missing time
series values and identify the most effective solution.

In certain cases simply ignoring the missing values may
be acceptable. It is the easiest and most convenient approach
that is frequently utilized by data scientist. However, a more
nuanced approach is required when dealing with time series
data. Temporal relationships are affected when data is dropped
from a time series. For instance, seasonal patterns can be
obfuscated when missing time series values are ignored.

© 20XX IEEE. Personal use of this material is permitted. Permission from
IEEE must be obtained for all other uses, in any current or future media,
including reprinting/republishing this material for advertising or promotional
purposes, creating new collective works, for resale or redistribution to servers
or lists, or reuse of any copyrighted component of this work in other works.

Stochastic models such as autoregressive and moving average
assume a continuous sequence of ordered values and can
produce delusive results in case of dropped values. As a result,
missing time series data requires replacement.

There exists a number of approaches to deal with incomplete
data [13]. In the present study, we consider three of the most
commonly employed methods for estimating missing values
in time series: forward ﬁll, backward ﬁll, and mean ﬁll. In
the forward ﬁll method, the missing values are replaced with
the preceding observations, that is, if xt is missing then it is
replaced with xt−1. In the backward ﬁll method, the missing
values are replaced with the following observations, that is, if
xt is missing then it is replaced with xt+1. In the mean ﬁll
method, the missing values are replaced with the average value
of the sample series. The forward and backward ﬁll methods
have the advantage in scenarios where there is a strong positive
correlation between the time series values whereas mean ﬁll
yields better performance in more volatile data. The three
imputation methods are simple and easy to implement making
them a very popular tool among the practitioners. Unlike the
more exotic imputation techniques that require signiﬁcant time
and effort and do not necessarily produce optimal results,
the classic ﬁlling methods considered in our study provide
a fast and reliable avenue for replacing missing times series
values. As a result, they remain popular and highly relevant.
Our paper presents an exhaustive study of the methods’
performance under a range of scenarios. We carry out a large
scale numerical analysis based on 3,600 simulated time series
to determine the most effective replacement technique.

In this paper, we focus on the autoregressive (AR) process
of order 1. It is the most commonly used time series model.
The results of the AR(1) process can be suitably extended to
the general family of AR(p) processes. Recall that the AR(1)
process is given by the equation

xt = φxt−1 + wt,

(1)

where xt is the value of the time series at time t and wt
is white noise. In our experiments, we simulate the AR(1)
process and artiﬁcially remove a portion of the values. Then
we reconstruct
the missing values using the three ﬁlling
techniques mentioned above. Our goal is to identify the ﬁlling
method that produces sample partial autocorrelation function

 
 
 
 
 
 
(PACF) values that are closest to the theoretical PACF values.
Our study is based on a total of 3,600 simulated time series us-
ing different mode coefﬁcients. The results of the experiments
show that mean ﬁll outperforms the other methods.

Time series forecasting plays an important role in a number
of applications including ﬁnance, medicine, engineering and
others [4]–[6]. The issue of missing times series values can
lead to inaccurate forecasts. Therefore, a good understanding
of imputation methods is crucial. We believe that our study
would serve as a useful reference to interested parties both
inside and outside of academia.

II. LITERATURE

There exists a number of methods for dealing with incom-
plete time series data. An overview of the classic imputation
methods along with their advantages and disadvantages can
be found in [13]. Imputation methods and algorithms are
extensively implemented in statistical packages such as R [11].
A recent trend in imputation research involves the use of
neural networks to estimate the missing time series values.
For instance, the authors in [10] employ generative adversarial
networks whereas in [16] the authors use Bayesian networks
to impute missing values in multivariate time series. In [3],
the authors propose a deep sequential latent variable model to
address the issue of interpretability. The authors employ non-
linear dimensionality reduction using a VAE approach together
with structured variational approximation.

Domain-speciﬁc imputation methods have also been pro-
posed. In [9], the authors employ a multiview learning method
to replace missing values in trafﬁc-related time series data.
The model combines LSTM and SVR algorithms together with
collaborative ﬁltering techniques. The proposed method is able
to account for the local and global variation in temporal and
spatial views to capture more information from the existing
data. The issue of missing data in transportation time series
was also addressed in [15]. The authors utilized an improved
kNN-based imputation method to improve the accuracy by
34%. The authors in [2] modify the pattern sequence algorithm
to to simultaneously forecast and backcast missing values for
imputation. Although test results were positive more extensive
experiments are required to conﬁrm the efﬁcacy of the algo-
rithm. Imputation methods have also been proposed in other
domain such estimating forest biomass [12] and water level
forecasting [17]. A bagging algorithm to improve the existing
imputation methods was proposed in [1]. The test results show
that bagging has a positive effect on the performance of the
imputation algorithms.

III. NUMERICAL EXPERIMENTS

In this section, we carry out a range of numerical exper-
iments to determine the best technique to estimate missing
values in time series. Our focus is on the AR(1) based time
series. We compare three approaches: forward ﬁll, backward
ﬁll, and mean ﬁll. The results of the experiments reveal that
backward ﬁll produces the most accurate results among all the
tested methods.

A. Methodology

To analyze the performances of the ﬁlling models we
measure their accuracy on AR(1) generated time series. To
this end, we consider a range of AR(1) model coefﬁcients φ:
0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9. For each value of φ, we
generate 100 different time series. Next, for each time series,
we randomly drop a portion of the values. The resulting time
series simulates a real life scenario of incomplete data. Then
we estimate and replace the missing values using the above
ﬁlling techniques. The sample PACF values are calculated for
each restored time series. The ﬁlling methods are evaluated
based on the difference between the theoretical PACF of the
original process and the sample PACF based on the restored
time series. For each value of φ, we aggregate the results
of the experiments on 100 simulated time series. We report
the overall mean of the difference between theoretical and
sample PACF values. One of the key factors in the analysis
of incomplete data is the quantity of missing values. In our
experiments, we study the performance of ﬁlling algorithms
under different drop rates: 5%, 10%, 20%, and 30% of
the series values. As a result, we obtain a comprehensive
perspective of the model performance. A summary of the
methodology is provided below.

Algorithm

For a ﬁxed value of φ, dropout rate p, and ﬁlling method M.
1. Randomly generate 100 different sample time series

based on the AR(1) model (Eq. 1).

2. For each time series xt, calculate the accuracy score using

the following steps
i. Drop a fraction p of the time series values.

ii. Replace the missing values using the method M.

iii. Calculate the sample PACF at lag h = 1 based on the

restored time series.

iv. Calculate the accuracy score as the difference between

the sample PACF and the theoretical PACF.

3. Calculate the mean and standard deviation of accuracy

scores over the 100 sample time series.

The numerical experiments are implemented in Python
using the statsmodels package [14]. We used Google
Colab to carry out the experiments.

B. Numerical experiments

A range of numerical experiments with various parameter
settings and dropout rates were performed to measure the
performance of the ﬁlling methods. A total of 3,600 simulated
sample time series were analyzed to obtain the ﬁnal results.

To illustrate the experiments consider a single sample time
series generated using the AR(1) process with parameter
φ = 0.4 (Figure 1a). Note that in this case the theoretical
PACF is equal to 0.329. To mimic real-life incomplete data,
we drop 20% of the time series values as shown in Figure

1a. Then the ﬁlling methods are used to replace the missing
values. As shown in Figures 1b-1d, the restored series are close
to the original sample time series. For each ﬁlling method, we
compute the sample PACF (Figures 1b-1d). To measure the
performance of a ﬁlling method we take the absolute difference
between the theoretical PACF (0.329) and the sample PACF
obtained from the restored time series. Thus, the accuracy
score of the forward ﬁll method is |0.329 − 0.464| = 0.135
(Figure 1b). The accuracy scores of the backward and mean
ﬁll methods are 0.163 and 0.014 respectively. This experiment
is repeated 100 times for each combination of the model
parameter φ and dropout rate values.

The results of our numerical experiments are reported
according dropout rates. Concretely, for each dropout rate, we
repeat the experiment described in Figure 1 100 times for each
value of φ and calculate the average difference between the
theoretical and restored PACF. The results of the experiments
are presented in Figures 2-5. First, we consider the case when
10% of the time series values are missing. As shown in
Figure 2, the forward and backward ﬁll methods produce small
errors (difference) at large positive values of φ. On the other
hand, mean ﬁll produces relatively larger errors. Note that at
large positive values of φ the time series has a strong trend.
Therefore, the forward and backward methods that follow the
the trend produce better results. However, the performance of
mean ﬁll improves as the value of φ decreases. Eventually,
beyond the value of φ = 0.5 , mean ﬁll produces signiﬁcantly
better results than the forward and backward ﬁll methods. At
the lower values of φ, the time series moves more sporadically
with frequent changes in the direction of movement. As a
result, the forward and backward ﬁll methods overestimate
the time series values. The mean ﬁll method produces more
conservative estimates that are less likely to miss the true
time series value by a large margin. When evaluating the
performances of the ﬁlling methods across the range of values
of φ mean ﬁll yields undoubtedly better results.

The performance of the ﬁlling methods on data with higher
dropout rate is consistent with that of 10% dropout rate.
As shown in Figures 3-5,
the forward and backward ﬁll
methods perform well at large positive values of φ. On the
other hand, mean ﬁll produces better results as the value
of φ decreases. Concretely, mean ﬁll outperforms the other
methods for all values φ ≤ 0.5. It is interesting to observe
that while the forward and backward ﬁll methods achieve
a better performance with positive values of φ, mean ﬁll
achieves better results with negative values of φ. We also note
that the performances of all three methods deteriorate as the
dropout rate increases. For instance, the PACF error of mean
ﬁll increases from under 0.10 to over 0.20 at φ = 0.9 as the
dropout rate increase from 10% to 25%. Similarly, the PACF
error of forward ﬁll increases from under 0.35 to almost 0.70
at φ = −0.9 as the dropout rate increase from 10% to 25%
(Figures 2-5).

(a) The original and corrupted times series. The corrupted series is
missing 20% of the original values. The original sample PACF at lag
1 is 0.329.

(b) The restored times series using forward ﬁll method and the
corresponding sample PACF plot (0.464).

(c) The restored times series using backward ﬁll method and the
corresponding sample PACF plot (0.492) .

(d) The restored times series using mean ﬁll method and the corre-
sponding sample PACF plot (0.315).

Fig. 1: The original sample time series is generated based on
the AR(1) process with coefﬁcient φ = 0.4. Then 20% of
the time series values were dropped to obtain an incomplete
sample. The three ﬁlling methods are applied to restore the
series.

IV. CONCLUSION

In this paper, we analyzed the performance of three methods
to ﬁll the missing values in a time series data. Concretely,
we studied the performance of the forward, backward, and
mean ﬁll methods in restoring the missing values from AR(1)
generated time series sample. We carried out a total of 3,600
simulations with a range of dropout rates and model parameter
values. The performance of the ﬁlling methods was measured
based on the difference between the true and estimated PACF

050100150200250300t3210123xtOriginal time series050100150200250300t3210123xtCorrupted time series050100150200250300t3210123xtRestored time series051015202530lag0.20.00.20.40.60.81.0PACFPACF050100150200250300t3210123xtRestored time series051015202530lag0.00.20.40.60.81.0PACFPACF050100150200250300t3210123xtRestored time series051015202530lag0.20.00.20.40.60.81.0PACFPACFFig. 2: The average true and estimated PACF difference for
times series with 10% missing values.

Fig. 4: The average true and estimated PACF difference for
times series with 20% missing values.

Fig. 3: The average true and estimated PACF difference for
times series with 15% missing values.

Fig. 5: The average true and estimated PACF difference for
times series with 25% missing values.

values. The results of the numerical experiments show that
mean ﬁll signiﬁcantly outperforms the other methods at values
φ ≤ 0.5. The results are consistent across different dropout
rates. Forward and backward ﬁll achieve better results at large
positive values of φ. The performance of forward ﬁll is due
to the positive trend in the time series for positive values
of φ. As a result, forward ﬁll achieves accurate results by
following the trend estimates. Conversely, mean ﬁll achieves
better results for value φ ≤ 0.5 which is explained by higher
time series stochasticity. The mean ﬁll method produces more
conservative estimates that are better suited for frequently
alternating series.

We conclude that the forward and backward ﬁll methods are

better suited for time series with strong positive correlations
between its values. Mean ﬁll is better suited for time series
with low positive correlation or negative correlations between
its values. The results hold across different dropout rates.
Since the AR(1) process is one of the commonly encountered
models, the outcomes of this study can be reasonably extended
to other stochastic processes. Given the importance of time
series forecasting [7], [8], our study would be a valuable
reference to both academics and industry practitioners.

REFERENCES

[1] Andiojaya, A., & Demirhan, H. (2019). A bagging algorithm for the
imputation of missing values in time series. Expert Systems with
Applications, 129, 10-26.

0.90.80.70.60.50.40.000.020.040.060.080.10PACF diffforwardbackwardmean-0.9-0.8-0.7-0.6-0.5-0.40.000.050.100.150.200.250.30PACF diffforwardbackwardmean0.90.80.70.60.50.40.000.020.040.060.080.100.120.14PACF diffforwardbackwardmean-0.9-0.8-0.7-0.6-0.5-0.40.00.10.20.30.4PACF diffforwardbackwardmean0.90.80.70.60.50.40.0000.0250.0500.0750.1000.1250.1500.175PACF diffforwardbackwardmean-0.9-0.8-0.7-0.6-0.5-0.40.00.10.20.30.40.5PACF diffforwardbackwardmean0.90.80.70.60.50.40.000.050.100.150.20PACF diffforwardbackwardmean-0.9-0.8-0.7-0.6-0.5-0.40.00.10.20.30.40.50.60.7PACF diffforwardbackwardmean[2] Bokde, N., Beck, M. W.,

´Alvarez, F. M., & Kulat, K. (2018). A
novel imputation methodology for time series based on pattern sequence
forecasting. Pattern recognition letters, 116, 88-96.

[3] Fortuin, V., Baranchuk, D., R¨atsch, G., & Mandt, S. (2020, June).
Gp-vae: Deep probabilistic time series imputation. In International
Conference on Artiﬁcial Intelligence and Statistics (pp. 1651-1661).
PMLR.

[4] Hamilton, J. D. (2020). Time series analysis. Princeton university press.
[5] Kamalov, F., Smail, L., & Gurrib, I. (2020, December). Forecasting with
Deep Learning: S&P 500 index. In 2020 13th International Symposium
on Computational Intelligence and Design (ISCID) (pp. 422-425). IEEE.
[6] Kamalov, F. (2020). Forecasting signiﬁcant stock price changes using
neural networks. Neural Computing and Applications, 32(23), 17655-
17667.

[7] Kamalov, F., Smail, L., & Gurrib, I. (2020, November). Stock price fore-
cast with deep learning. In 2020 International Conference on Decision
Aid Sciences and Application (DASA) (pp. 1098-1102). IEEE.

[8] Kamalov, F., & Gurrib, I. (2020). Machine learning based forecasting
of signiﬁcant daily returns in foreign exchange markets. arXiv preprint
arXiv:2009.10065.

[9] Li, L., Zhang, J., Wang, Y., & Ran, B. (2018). Missing value imputation
for trafﬁc-related time series data based on a multi-view learning
method. IEEE Transactions on Intelligent Transportation Systems, 20(8),
2933-2943.

[10] Luo, Y., Cai, X., Zhang, Y., Xu, J., & Yuan, X. (2018, December). Mul-
tivariate time series imputation with generative adversarial networks. In
Proceedings of the 32nd International Conference on Neural Information
Processing Systems (pp. 1603-1614).

[11] Moritz, S., & Bartz-Beielstein, T. (2017). imputeTS: time series missing

value imputation in R. R J., 9(1), 207.

[12] Nguyen, T. H., Jones, S., Soto-Berelov, M., Haywood, A., & Hislop,
S. (2018). A comparison of imputation approaches for estimating forest
biomass using landsat time-series and inventory data. Remote Sensing,
10(11), 1825.

[13] Pratama, I., Permanasari, A. E., Ardiyanto, I., & Indrayani, R. (2016,
October). A review of missing values handling methods on time-series
data. In 2016 International Conference on Information Technology
Systems and Innovation (ICITSI) (pp. 1-6). IEEE.

[14] Seabold, S., & Perktold, J. (2010, June). Statsmodels: Econometric and
statistical modeling with python. In Proceedings of the 9th Python in
Science Conference (Vol. 57, p. 61).

[15] Sun, B., Ma, L., Cheng, W., Wen, W., Goswami, P., & Bai, G. (2017,
October). An improved k-nearest neighbours method for trafﬁc time
series imputation. In 2017 Chinese Automation Congress (CAC) (pp.
7346-7351). IEEE.

[16] Susanti, S. P., & Azizah, F. N. (2017, November). Imputation of missing
value using dynamic Bayesian network for multivariate time series data.
In 2017 International Conference on Data and Software Engineering
(ICoDSE) (pp. 1-5). IEEE.

[17] Yang, J. H., Cheng, C. H., & Chan, C. P. (2017). A time-series water
level forecasting model based on imputation and variable selection
method. Computational intelligence and neuroscience, 2017.

