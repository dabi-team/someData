Relationship Oriented Affordance Learning through
Manipulation Graph Construction

Chao Tang1, Jingwen Yu1,2, Weinan Chen1, Member, IEEE, and Hong Zhang1 Fellow, IEEE

1
2
0
2

v
o
N
1

]

O
R
.
s
c
[

2
v
7
3
1
4
1
.
0
1
1
2
:
v
i
X
r
a

Abstract— In this paper, we propose Manipulation Rela-
tionship Graph (MRG), a novel affordance representation
which captures the underlying manipulation relationships of an
arbitrary scene. To construct such a graph from raw visual ob-
servations, a deep nerual network named AR-Net is introduced.
It consists of an Attribute module and a Context module, which
guide the relationship learning at object and subgraph level
respectively. We quantitatively validate our method on a novel
manipulation relationship dataset named SMRD. To evaluate
the performance of the proposed model and representation,
both visual perception and physical manipulation experiments
are conducted. Overall, AR-Net along with MRG outperforms
all baselines, achieving the success rate of 88.89% on task
relationship recognition (TRR) and 73.33% on task completion
(TC).

I. INTRODUCTION

Humans possess rich prior knowledge as for recognizing
what function or interaction an object affords in order to
accomplish a certain task. Similarly, robots operating in
the unconstructed environment ought to be equipped with
the ability to reason about object affordances to plan task-
oriented manipulation sequences. Affordance, ﬁrst mentioned
in [1], enables robots to identify its potential interactions
with humans or environments and informs the subsequent
robotic manipulation [2]–[4]. It places humans and robots
on the same page in terms of cognitive reasoning and scene
understanding. This paper is concerned with the following
question: given an arbitrary scene consisting of a set of
objects, how does the robot ﬁgure out what to do with them.
For example, a cooking robot should be able to recognize a
spatula can be used to scoop the pot or handover appropriate
kitchen tools to a human chef during cooking. Due to the
importance of affordance learning, extensive research has
been conducted.

Most works in computer vision community cast affordance
learning as a segmentation problem [5]–[8]. Object parts
sharing the same functionality are segmented and grouped at
the pixel level before assigned with corresponding affordance
category labels. These methods treat affordance learning
as detecting the property of an isolated object and do not
consider the interactions among objects. For example, a robot
predicts the affordance of a knife as “cut” (blade) and “grasp”
(handle) but has no idea as to “what to cut” or “where to
cut”. Contrary to the segmentation based approach, another
approach known as knowledge graph relies on a symbolic

1Department of Electronic and Electrical Engineering, Southern Univer-

sity of Science and Technology, China.

2Department of Electronic and Computer Engineering, Hong Kong

University of Science and Technology, Hong Kong, China.

Fig. 1. Robot reasons about the manipulation relationship graph (MRG)
over a set of kitchen objects.

representation of a scene where detected objects are ﬁrst
described in an embedding space. Then a knowledge graph
is constructed whose nodes denote a set of concepts known
to the robot (e.g. apple, mug, bowl) and whose edges denote
a set of possible relations such as affordances [4], [9]–
[12]. Now, to query the learned knowledge graph, the robot
must recognize the object category ﬁrst using pretrained
object detectors such as [13], which makes the closed world
assumption, and is unable to generalize with respect to novel
object instances.

To extend the previous works in affordance learning, we
introduce manipulation relationship graph (MRG), a novel
representation which interprets affordance as the manip-
ulation relationships between pairs of objects, and show
how an MRG is constructed from raw visual observations
without object category information. Rather than asking
the question of what functionality an object
is able to
afford or provide, our affordance framework addresses the
question of how objects in a scene are related in terms
of functionalities, i.e., how pairs of objects are related to
each other through affordance properties. As is depicted in
Figure 1, when the robot ﬁrst perceives a scene consisting
of a set of objects, it abstracts the scene to an MRG whose
nodes denote the object observations and edges indicate the
potential manipulation relationships between pairs of objects.
In general, each manipulation relationship is written as a
triplet (subject, relationship, object), where subject and
object are represented by an index (e.g. with four objects,
indexes runs from 1 to 4). Such an MRG guides robotic
manipulation by informing “what to use” (subject), “how to
use”(relationship) and “what to act on” (object).

To construct the MRG, we contribute AR-Net, a novel af-

 
 
 
 
 
 
fordance learning framework which captures the underlying
manipulation relationships of an arbitrary scene. It consists
of an Attribute module and a Context module to facilitate
model’s generalization to intra-class and inter-class object
variations. The manipulation relationships are learnt under
the supervision of a novel dataset, which will be introduced
in detail shortly. In this paper, we use the word object to
either refer to the physical entities being manipulated or the
recipient of an action in a (subject, relationship, object)
triplet. Which interpretation is relevant should be clear from
the context. Our contributions in this paper are three-fold:

• We introduce MRG, a novel affordance representation
which captures the underlying manipulation relation-
ships between pairs of objects directly from raw visual
observations.

• We propose AR-Net, a deep neural network to infer the
manipulation relationships and construct the MRG.
• A novel dataset, semantic manipulation relationship de-
tection (SMRD) dataset, consisting of six manipulation
relationship classes, 13 tool categories, 1171 images,
and 4428 relationship triplets, is presented.

The rest of this paper is organized as follows. We review
related literature in Section II and provide the formal def-
inition of MRG in Section III. The detailed explanation of
the proposed AR-Net is given in Section IV. Experimental
setup and results for both visual perception and physical
manipulation experiments are presented in Section V and
VI respectively. We discuss the limitations and provide a
conclusion in Section VII.

II. RELATED WORKS

In this paper, we propose to learn affordance by con-
structing MRG. With respect to prior research, three most
related topics are reviewed in this section including affor-
dance learning in robotics, visual relationship detection, and
manipulation using a graph.

A. Affordance Learning in Robotics

Affordance provides valuable information as to what func-
tion an object or environment affords for the purpose of
achieving a speciﬁed goal by robots [1]. There are currently
three affordance learning approaches: segmentation based,
knowledge-graph based and interaction based, each of which
presents a unique way to interpret affordance. The segmenta-
tion based approach segments the object part by functionality
at the pixel level. [5] starts with segmenting tool affordance
regions from local shape and geometry primitives. Built
on prior works, [6] utilizes deep learning to achieve state-
of-the-art performance on affordance segmentation. While
the aforementioned methods operate on 2D or 2.5D data,
[8] segments objects in the 3D domain. The segmentation
based approach usually focuses on detecting the properties
of isolated objects and does not consider the interactions
among them.

Another way to learn affordance is through the knowledge
graph framework, which captures the relationships among
symbols in an embedding space. [4] learns multi-relational

embeddings to jointly reason about object affordances, lo-
cations, and materials. To keep the knowledge base up-to-
date, [11] proposes an incremental robotics knowledge graph
framework by leveraging continual learning. These methods
are usually trained on large knowledge sources such as [10]
[12]. A knowledge-graph based method models affordance as
a set of relationships among objects but relies on a pretrained
object detector to convert raw visual observations to a set of
symbols and is therefore unable to generalize to novel object
instances.

The third affordance learning approach discovers affor-
dances through physical
interactions between robots and
objects. [3] and [14] learn affordances from goal-directed
tool manipulation via a set of predicted keypoints in sim-
ulation. Similarly, [15] reasons about the long-term effects
of actions through modeling what actions are afforded in
the future through trial-and-error. These models are usually
task-speciﬁc and are not able to handle general situations.
In contrast to the three affordance learning approaches, our
proposed method deals with various tasks and is in principle
capable of generalizing to any arbitrary scene without the
need for object category information, which is particularly
helpful in human robot interaction.

B. Visual Relationship Detection

Visual relationship detection is deﬁned as the task of
describing the relationships between objects within a scene
using visual input. [16] ﬁrst proposes to represent objects,
attributes and relationships between objects using a scene
graph. [17] improves on [16] by leveraging language priors
from semantic word embedding to ﬁne-tune the likelihood
of a predicted relationship. After that, numerous methods
have been proposed by utilizing recurrent architecture [18],
attention mechanism [19] and reinforcement learning [20].

In robotics, [21] detects the support relationships between
surface from RGBD data for the purpose of robot navigation
while [22] detects the evolution of the spatial relationships
between involved objects during manipulation.Our proposed
method extends the task of describing the scene with spatial
or action relationships to predicting potential manipulation
relationships among objects.

C. Manipulation Using A Graph

Since robotic manipulation necessitates robot executing a
sequence of actions in order to achieve a goal, graph becomes
a natural spatio-temporal representation. [23] and [24] focus
on the problem of robotic grasping. They simultaneously
detect objects and recognize the low-level manipulation
relationships as a graph to help robots ﬁnd the right order in
which the objects should be grasped.

Graph has also been widely used in task-and-motion
planning (TAMP). [25] solves the long-horizon planning
problem by representing subgoals as graphs for rigid-body
manipulation. Rather than focusing on symbolic information,
[26] jointly predicts a sequence of discrete actions and the
parameter values of their associated low-level controllers
without prior geometric knowledge of the environment.

Combining symbolic and geometric reasoning, [27] builds a
two-level scene graph representation including geometric and
symbolic scene graph for long-horizon manipulation tasks.
Following the practice, we use the form of graph to capture
the underlying manipulation relationships, but the proposed
method focuses on high level scene understanding and uses
a graph to capture the diverse manipulation relationships.

III. MANIPULATION RELATIONSHIP GRAPH

Given the observation of a scene, our objective is to
abstract it to a directed graph which captures the underlying
manipulation relationships. For each scene, let there be N
objects. A manipulation relationship graph G = (V, E)
consists of the vertices, V = {vi}N
the
observed N objects in the scene, and edges E = {eij| if
the manipulation relationship exists from objects i to j} for
i, j ∈ {1, ..., N }. R classes of manipulation relationships are
deﬁned. Examples of scene and the corresponding MRGs are
shown in Figure 2.

i=1 to represent

Each node vi ∈ V in the graph has a feature vector φi(v),
which contains the identity, spatial and appearance informa-
tion. Since category label is not used, object identity is a 1-
dimensional index. Spatial information is represented as a 5-
dimensional vector indicating the bounding box coordinates
and a conﬁdence score. Appearance information is a feature
vector extracted from a visual patch at corresponding spatial
location. Edge types and weights (i.e. relationship classes and
conﬁdence scores) are inferred using φi(v) of all objects.

Fig. 2. Examples of scene (ﬁrst row) and corresponding MRGs (second
row). In our case, R = 6. Indexes are color-coded and manipulation
relationships are represented as follow: {1-scoop, 2-pour, 3-cut, 4-contain,
5-wipe, 6-dump}.

IV. RELATIONSHIP ORIENTED AFFORDANCE LEARNING

To address the problem of affordance learning in the form
of MRG constrcution, we present AR-Net, which takes as
input the observation of an arbitrary scene and outputs the
manipulation relationships among objects as an MRG. An
overview of the proposed method is shown in Figure 3. The
entire process can be summarized as follows: (1) given the
RGB input, category-agnostic object proposals are grouped
into pairs to form a fully-connected graph, where each pair
is mutually connected with directed edges; (2) subgraphs
are created by taking the union box of two proposals; (3)
subgraphs referring to the same manipulation relationship
are clustered; (4) object and subgraph features are reﬁned

through Context and Attribute modules of AR-Net; (5) ma-
nipulation relationships are predicted based on corresponding
subject, object and subgraph features to construct an MRG.
Each step will be explained in detail for the rest of this
section.

A. FC Graph Construction and Subgraph Clustering

RPN [13] is ﬁrst adopted to generate category-agnostic ob-
ject proposals. Without prior knowledge, each pair of objects
has possibly mutual manipulation relationships. Therefore,
a directed fully-connected graph is constructed as shown in
Figure 3 (b), where each object is connected to all the others.
Each edge represents a potential manipulation relationship
(or no relationship) between subject and object. And each
relationship triplet is described as (subject, relationship,
object).

To obtain the representation of a candidate relationship
triplet, the union box of subject and object is taken. The
corresponding subgraph features are then extracted from the
spatial location of the union box. Since many subgraphs
share the same subject, object and manipulation relation-
ship, subgraph clustering [19] is deployed to merge these
subgraphs (Figure 3 (c)). It helps keep the overall graph
representation concise and meanwhile save the computation
cost. After clustering, the fully-connected graph consists of
two parts: object proposals and subgraphs. Fully-connected
layers and 2-D convolution layers are then applied to object
and subgraph features respectively to obtain the object fea-
ture vectors and 2-D subgraph feature maps. Each subgraph
is connected to all the objects within it, and each candidate
relationship triplet refers to one subgraph with the corre-
sponding subject and object.

B. Attribute Learning as An Auxiliary Task

To support the open-world assumption, the object category
detection branch is modiﬁed to an objectness detection
branch. Instead of predicting the category label, it predicts
the object existence label as binary classiﬁcation (“0” for
non-object and “1” for object), making the proposed method
category-agnostic. However, removing the category label
leads to a considerable drop in relationship prediction per-
formance as it provides important contextual information.

To resolve the dilemma, Attribute module inspired by [7]
is introduced. The main idea is that the key of predicting the
category label of either subject or object is to recognize its
commonly used functions or attributes. Correctly predicting
the attributes seamlessly reduces the searching space of
potential manipulation relationships in which the subject
or object may be engaged. Meanwhile, attribute is more
transferable than category as objects from different categories
may share the same attribute. Combining these two leads
to better manipulation relationship recognition capability
towards novel object instances.

The Attribute module explicitly predicts the attributes of
a given object as a multi-label classiﬁcation problem with R
outputs, where R is the number of manipulation relationship
classes (in our case, R=6). As shown in Figure 4, bowl

Fig. 3. Overview of the proposed AR-Net. (a) a RGB image capturing a set of objects is used as input. (b) Objects proposals (blue) generated by RPN
are grouped into pairs to construct a fully-connected graph, where each pair is mutually connected by directed edges. (c) Subgraphs (green) are created
by taking the union box of two proposals and those referring to the same manipulation relationship are clustered and merged. (d) Object and subgraph
features are reﬁned through Context module and Attribute module. (e) Manipulation relationships are predicted to build MRG. (f) Robot executes the task
relationship using computed grasp and effect points.

Fig. 4. Attribute prediction as an auxiliary task

possesses the attributes of “pour” and “contain” as humans
usually put food in it or use bowl to transfer liquid to other
containers. Spoon is solely used for scooping, so it has single
attribute. Based on the fact that visually similar parts share
similar functions, Attribute module guides object feature
learning by connecting the visual structure to attributes. It
helps the proposed method recognize attributes of novel
objects and therefore correctly predict the manipulation rela-
tionships. Attribute learning works as an auxiliary task during
training and is dropped during inference. Non-affordance
objects (e.g. apple) are not considered. The overall detection
branch consists of an objectness detection branch, a bounding
box regression branch and an attribute prediction branch.

Attribute learning can be interpreted as novel object de-
tection, which is still an open problem. In the current setup,
the model is studied in the limited context of six commonly
used attributes among kitchen tools. Ideally, extra attribute
categories can be incorporated in order to handle more di-
verse and challenging scenarios. Different from adding many
more tool categories, adding one more attribute category may
potentially enable the robot to reason about the usage of a
large set of tool categories with similar functions.

C. Subgraph Feature Aggregation with Context

Object features offer detailed contextual cues such as
appearance and spatial location information to facilitate the
understanding of how each candidate relationship region (i.e.
subgraph) interacts with its corresponding subject and object.
In other words, a candidate relationship region might contain
several objects due to dense layout or inaccurate bounding
box localization. But the manipulation relationship is only

determined by the designated subject and object out of all
objects within that region.

However, the use of raw subgraph features fails to capture
the interactions between the subgraph and objects within
it. To resolve this problem, Context module from [19] is
modiﬁed and adopted. It is designed based on the attention
mechanism [28]. It ﬁrst guides the model to “look at” the
potential locations where subject and object are located, and
then aggregate subgraph features using the object features
from the corresponding spatial location. Meanwhile, since
different objects exist at different spatial locations, weights
should be spatial-dependent when incorporating object fea-
tures into subgraph features.

The detailed description is as follows. Let there be K
objects within a subgraph. The weighted object features at
location (x, y) are expressed as below:

ˆO(x, y) =

K
(cid:88)

i=1

W (oi, S, x, y) · Vproj(oi)

(1)

where S denotes the subgraph features. W (oi, S, x, y) is the
attention weight of object feature oi at (x, y), and ˆO(x, y)
represents the weighted object features at the same location.
Vproj projects oi to the target subgraph domain. The attention
weight is computed as:

W (oi, S, x, y) =

exp(Kproj(oi) · Qproj(S(x, y)))
i=1 exp(Kproj(oi) · Qproj(S(x, y)))

(cid:80)N

(2)

where S(x, y) denotes the subgraph feature at (x, y). Kproj
and Qproj are projection functions. Then we represent the
aggregated subgraph features as:

ˆS =

(cid:88)

x,y

S(x, y) + α · ˆO(x, y)

(3)

where α is a learnable scale factor.

D. Manipulation Relationship Graph Construction

After obtaining the reﬁned object and subgraph features,
a manipulation relationship is predicted using AR-Net by
taking the addition of the corresponding subject, object and
subgraph features as follows:

pij = F C(ReLu(oi + ˆS + oj))

(4)

where pij denotes the relationship (from i to j) distribution
over R classes of manipulation relationships plus background
(no relationship). F C is the fully-connected layer and ReLu
is the activation function.

Top-1 prediction will be used to form the manipulation
relationship triplet as (subject, relationship, object), where
subject and object are represented by their indexes. Triplet
score is computed by taking the product of subject, object
and relationship conﬁdence probabilities. Redundant triplets
and those with low scores will be eliminated with triplet
NMS [29]. After that, an MRG is constructed by treating
objects as nodes and relationships between pairs of objects
as edges with triplet scores being edge weights.

Overall, the proposed AR-Net uses VGG16 [30] pretrained
on ImageNet [31] as backbone and Adam [32] as optimizer
with 0.9 momentum and 0.00001 weight decay. Initial learn-
ing rate is set as 0.001 and get multiplied by 0.1 every 5
epochs. Subgraph clustering threshold is set to 0.5. RPN is
trained ﬁrst and then the whole model is trained jointly for
15 epochs. The model is designed based on [13] [19].

V. EXPERIMENTAL SETUP

In this section, we will ﬁrst discuss the details of a novel
manipulation relationship dataset that is used to evaluate the
proposed AR-Net and other baselines. The setup for both
visual perception and physical manipulation experiments will
then be presented. Finally, the evaluation metrics used for
experiments will be given.

A. SMRD Dataset

TABLE I. SUMMARY OF SMRD DATASET

Relationship class

scoop, pour, cut, contain, wipe, dump

Tool class

Image split

Relationship split

pan, spatula, plate, knife, bowl, cloth,
fork, mug, spoon, brush, cup, pot, can

988 for training, 183 for testing,
1171 images in total

3563 for training, 865 for testing,
4428 relationships in total

In order for a robot to learn diverse manipulation relation-
ships, a dataset annotated with ground truth manipulation
relationships is needed. Not aware of such a dataset, Seman-
tic Manipulation Relationship Detection (SMRD) dataset is
created. A summary of SMRD is presented in Table I. It
includes 13 kitchen tool classes as shown in Figure 5 (b) and
six manipulation relationship classes. Non-affordance objects
(e.g. vegetables, fruits) are also collected since manipulation
relationships commonly exist between tools with speciﬁc

Fig. 5.
(c) Intra-class objects; (d) Inter-class objects.

(a) Experiment platform; (b) Kitchen tools used in SMRD dataset;

affordances and non-affordance objects (e.g. (knif e, cut,
apple)). Overall, SMRD consists of 1171 images and 4428
relationships, averaging 3.78 relationships per image. For
each image, the GT relationships are organized in the form
of an MRG. The dataset does not consider the category label
since the label is not used during relationship inference.

B. Visual Perception Experiment

To gain insights into our proposed model and show
the effectiveness of each component, visual perception
experiment
to three
baselines:

is conducted. We compare AR-Net

• Base uses raw object and subgraph features. It does
not consider the attribute of each object and isolate the
contextual connection between relationship region and
objects within it.

• Base+ATT reﬁnes object features with Attribute mod-
ule. It guides the relationship learning by explicitly
predicting the attributes of objects.

• Base+CT is equipped with Context module. It aggre-
gates subgraph features with object features from the
corresponding spatial location and therefore guides the
model to “look at” the positions where subject and
object are located.

• Base+ATT+CT (AR-Net) takes advantage of both At-
incorporates
information to obtain

tribute module and Context module. It
attribute learning and context
better transferability and prediction accuracy.

C. Physical Manipulation Experiment

We then evaluate the feasibility of the proposed method
in robotic manipulation experiment. Figure 5 (a) shows the
experiment platform. It consists of a 6-DOF Kinova Gen3
Lite robotic arm and an Intel Realsense D435i RGBD camera
with eye-to-hand calibration. In addition to AR-Net, two
types of affordance learning approaches are presented for
comparison:

• AS represents segmentation-based affordance learning
methods [2], [5]–[8]. In these methods, object parts
sharing the same functionality are segmented and

Fig. 6.
AffordanceNet [6]

Grasp point (yellow) and effect point (white) computed using

grouped at the pixel level. In the following compari-
son, we select state-of-the-art affordance segmentation
architecture [6] for comparison.

• DKG ﬁrst detects the object categories in the given
scene and then query the trained knowledge base to
infer the manipulation relationships. [9] is reproduced
as a baseline approach. It consists of an object detector
pretrained on SMRD in conjunction with a knowledge
graph constructed from GT manipulation relationships
for fair comparison.

Since each method above deals with a unique set of affor-
dance categories, four shared affordances including “scoop”,
“pour”, “cut”, “contain” are extracted for testing. Besides, for
the purpose of robotic manipulation, grasp point on subject
and effect point on object are computed using AffordanceNet
[6]. It is done by ﬁrst performing affordance segmentation
on subject and object. Grasp and effect points are then
computed by taking the midpoint of selected affordance
regions. As is shown in Figure 6, AffordanceNet segments
to a “contain” region (red) and a “wrap-grasp”
the pot
region (brown). Spatula is segmented into a “scoop” region
(blue) and a “grasp” region (purple). Grasp point (yellow)
is the midpoint of “grasp” region on spatula (subject) while
effect point (white) is the midpoint of “contain” region on
pot (object). Finally, manipulation action is executed using
motion primitives.

D. Evaluation Metric

Two sets of evaluation metrics are provided for visual per-
ception and physical manipulation experiments respectively.
For the former, Recall@1 (denoted as R@1) and Recall@5
(denoted as R@5) are used. We evaluate the performance
under the tasks of phrase detection (denoted as P) and rela-
tionship detection (denoted as R). The former task coarsely
localizes the entire relationship as one union bounding box
having at least 0.5 overlap with the ground truth box and
the latter requires the localization of both the subject and
the object with respect to their ground truth bounding boxes
and predicts the correct relationship simultaneously. More
details can be found in [17]. In all, four metrics including
PR@1, PR@5, RR@1, RR@5 are used in visual perception
experiment. For physical manipulation experiment, the robot
needs to localize each object precisely for the purpose of
manipulation. Therefore RR@1 and RR@3 are used. To test
the task execution capability, task relationship recognition
rate (denoted as TRR) and task completion rate (denoted as
TC) are recorded. TRR examines robot’s recognition ability
towards a given task relationship and TC evaluates the task
completion capability.

Fig. 7. Examples of manipulation relationships executed by the robot

VI. EXPERIMENTS

This section details the experiments performed and their
outcomes. We ﬁrst show the results from visual perception
experiment by comparing the proposed method to three
baselines. Three sets of physical manipulation experiments
are then conducted on the seen, intra-class and inter-class
objects respectively.

A. Visual perception experiment results

TABLE II. PERFORMANCE ON VISUAL EXPERIMENTS (%)

Method

Base

Base+ATT

Base+CT

Base+ATT+CT (proposed)

PR@1

28.704

35.764

35.995

37.384

PR@5

74.884

82.060

81.597

RR@1

17.130

19.792

19.907

84.259

20.370

RR@5

59.375

67.593

67.246

68.059

Visual perception experiment

is performed on SMRD.
Four models follow the same training and inference strate-
gies. As shown in Table II, the proposed Base+ATT+CT
outperforms all three baselines across all four metrics. This
highlights the model’s ability to accurately recognize manip-
ulation relationships using attribute and context information.
Base shows the lowest recall since it does not utilize object
attribute information and does isolate the connection between
relationship region and objects within it. Base+ATT gains a
performance boost compared to Base. It shows the effec-
tiveness of proposed Attribute module. Instead of predicting
the category label, it guides the feature learning by inferring
what attributes an object possesses. Base+CT also shows
competitive performance as it bridges the gap between rela-
tionship region and objects within it. With Context module,
the model learns both “where to look at” (spatial info) and
“what to look at” (appearance info).

B. MRG guided task execution

For the following three physical tasks, we examine each
method’s ability to recall the manipulation relationships and
physically manipulate objects with motion primitives. Shown
in Figure 7 are examples of manipulation relationships
executed by the robot. Seen objects from SMRD are ﬁrst
tested.

TABLE III. IN-CLASS TASK EXECUTION (%)

Method

RR@1

RR@3

AS

DKG

AR-Net (Ours)

23.73

29.29

29.26

45.41

78.41

80.64

TRR

33.33

86.67

93.33

TC

33.33

66.67

80.00

TABLE V. INTER-CLASS GENERALIZATION (%)

Method

RR@1

RR@3

AS

DKG

AR-Net (Ours)

8.73

27.06

28.17

13.02

51.35

62.30

TRR

20.00

66.67

80.00

TC

20.00

46.67

60.00

In this task, a set of objects covering all four manipulation
relationships (i.e. scoop, pour, cut, contain) are selected
from the SMRD and 15 scenes are constructed. Each scene
consists of 3-7 relationships, and one task relationship is
randomly selected from GT relationships for physical exe-
cution. The robot is required to (1) recall all the manipulation
relationships within the given scene (RR@k), (2) recognize
the selected task relationship (TRR), (3) physically execute
the task with motion primitives (TC). Results are presented in
Table III. AS performs the worst. It is because segmentation-
based approach only deals with objects with speciﬁc af-
fordances such as tools whereas most manipulation actions
happen between tools and non-affordance objects. For ex-
ample, AS fails to recognize and execute the task (knif e,
cut, apple) since apple has no affordance. Therefore, it fails
in all “cut” and “contain” tasks. DKG gives competitive
result, leading in RR@1. The model is able to output robust
detection results and query the knowledge base accordingly.
However, the misclassiﬁcations lead to inferior performance
to the proposed method across three metrics as AR-Net does
not rely on object categories to make predictions.

C. Intra-class generalization

Fig. 8. Overall performance (%) of three methods in physical experiments

V. The proposed method outperforms all other baselines
across all four metrics by signiﬁcant margins. Large inter-
class variation causes considerable misclassiﬁcations to DKG
and therefore leads to inferior performance. As performs
the worst as the model does not generalize well to inter-
class objects and meanwhile is not able to recognize non-
affordance objects.

An overall evaluation of three methods is presented in
Figure 8 by combining the statistics from three sets of
physical experiments. The proposed method achieves the
success rate of 88.89% on TRR and 73.33% on TC, showing
clear superiority under each evaluation metric.

TABLE IV. INTRA-CLASS GENERALIZATION (%)

VII. DISCUSSION AND CONCLUSION

Method

RR@1

RR@3

AS

DKG

AR-Net (Ours)

18.50

26.83

26.28

44.17

68.33

72.72

TRR

26.67

86.67

93.33

TC

26.67

73.33

80.00

In this task, we investigate whether each model can
generalize the learned relationships to unseen objects from
the same class.

As shown in Figure 5 (c), a set of objects sharing the
same category with SMRD is collected but they vary in color,
material and size. Table IV details the results. Similarly, AS
gives the lowest recall and task completion rate. DKG serves
as a strong baseline and performs slightly better than AR-
Net in terms of RR@1. A drop occurs on RR@3 for both
DKG and AR-Net, showing the large intra-class variation.
The proposed method achieves superior RR@3, TRR and
TC due to the category-agnostic design, leading to better
task execution ability.

D. Inter-class generalization

Figure 5 (d) shows the objects used for inter-class gen-
eralization experiment, which is the most challenging task
out of three. These objects differs largely from SMRD in
usage, appearance and material. Results are shown in Table

Fig. 9. Human annotated (GT) grasp point (green) and computed grasp
point (yellow)

A signiﬁcant drop from TRR to TC is observed from
Figure 8, showing the gap between relationship recognition
and successful manipulation. The drop is mainly caused by
two reasons. First, a manipulation action is implemented
by way of pre-designed motion primitives, which do not
consider the task and environment constraints. Low-level task
and motion planning (TAMP) algorithm can be combined
to generate ﬂexible manipulation actions. Another typical
failure case is due to sub-optimal grasp points. As is shown in
Figure 9, grasp point computed using [6] drifts away from

the GT grasp point annotated by human, leading to sub-
optimal grasp poses and potential manipulation failure. This
indicates that task-oriented grasping (TOG) module [3] [33]
is needed.

To conclude, this work addresses the problem of affor-
dance learning in the task of robotic manipulation. We
introduce a novel affordance representation which captures
the potential manipulation relationships of an arbitrary scene.
A deep neural network is designed to construct the proposed
MRG from raw visual observations. To train the model,
a novel dataset is collected and annotated, consisting of
six manipulation relationship classes and 13 tool categories.
Visual perception and physical manipulation experiments
demonstrate the superiority of proposed method and repre-
sentation against all baselines. For the next step, TAMP and
TOG modules will be incorporated to generate task-oriented
manipulation trajectories and grasp poses in order to enhance
the physical manipulation capability.

ACKNOWLEDGMENT

We thank members of Robotics Perception and Intel-
ligence Lab at SUSTech for their helpful feedback and
discussion.

REFERENCES

[1] J. J. Gibson, “The theory of affordances,” Hilldale, USA, vol. 1, no. 2,

pp. 67–82, 1977.

[2] R. Xu, F.-J. Chu, C. Tang, W. Liu, and P. A. Vela, “An affordance
keypoint detection network for robot manipulation,” IEEE Robotics
and Automation Letters, vol. 6, no. 2, pp. 2870–2877, 2021.

[3] Z. Qin, K. Fang, Y. Zhu, L. Fei-Fei, and S. Savarese, “Keto: Learning
keypoint representations for tool manipulation,” in 2020 IEEE Interna-
tional Conference on Robotics and Automation (ICRA).
IEEE, 2020,
pp. 7278–7285.

[4] A. Daruna, W. Liu, Z. Kira, and S. Chetnova, “Robocse: Robot
common sense embedding,” in 2019 International Conference on
Robotics and Automation (ICRA).

IEEE, 2019, pp. 9777–9783.

[5] A. Myers, C. L. Teo, C. Ferm¨uller, and Y. Aloimonos, “Affordance
detection of tool parts from geometric features,” in 2015 IEEE
International Conference on Robotics and Automation (ICRA).
IEEE,
2015, pp. 1374–1381.

[6] T.-T. Do, A. Nguyen, and I. Reid, “Affordancenet: An end-to-end
deep learning approach for object affordance detection,” in 2018 IEEE
international conference on robotics and automation (ICRA).
IEEE,
2018, pp. 5882–5889.

[7] F.-J. Chu, R. Xu, C. Tang, and P. A. Vela, “Recognizing object
affordances to support scene reasoning for manipulation tasks,” arXiv
preprint arXiv:1909.05770, 2019.

[8] S. Deng, X. Xu, C. Wu, K. Chen, and K. Jia, “3d affordancenet: A
benchmark for visual object affordance understanding,” in Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 2021, pp. 1778–1787.

[9] A. Saxena, A. Jain, O. Sener, A. Jami, D. K. Misra, and H. S. Koppula,
“Robobrain: Large-scale knowledge engine for robots,” arXiv preprint
arXiv:1412.0691, 2014.

[10] M. Beetz, D. Beßler, A. Haidu, M. Pomarlan, A. K. Bozcuo˘glu, and
G. Bartels, “Know rob 2.0—a 2nd generation knowledge processing
framework for cognition-enabled robotic agents,” in 2018 IEEE In-
ternational Conference on Robotics and Automation (ICRA).
IEEE,
2018, pp. 512–519.

[11] A. Daruna, M. Gupta, M. Sridharan, and S. Chernova, “Continual
learning of knowledge graph embeddings,” IEEE Robotics and Au-
tomation Letters, vol. 6, no. 2, pp. 1128–1135, 2021.

[12] H. Liu and P. Singh, “Conceptnet—a practical commonsense reasoning

tool-kit,” BT technology journal, vol. 22, no. 4, pp. 211–226, 2004.

[13] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-
time object detection with region proposal networks,” Advances in
neural information processing systems, vol. 28, pp. 91–99, 2015.
[14] D. Turpin, L. Wang, S. Tsogkas, S. Dickinson, and A. Garg, “Gift:
Generalizable interaction-aware functional tool affordances without
labels,” arXiv preprint arXiv:2106.14973, 2021.

[15] D. Xu, A. Mandlekar, R. Mart´ın-Mart´ın, Y. Zhu, S. Savarese, and
L. Fei-Fei, “Deep affordance foresight: Planning through what can be
done in the future,” arXiv preprint arXiv:2011.08424, 2020.

[16] J. Johnson, R. Krishna, M. Stark, L.-J. Li, D. Shamma, M. Bernstein,
and L. Fei-Fei, “Image retrieval using scene graphs,” in Proceedings
of the IEEE conference on computer vision and pattern recognition,
2015, pp. 3668–3678.

[17] C. Lu, R. Krishna, M. Bernstein, and L. Fei-Fei, “Visual relationship
detection with language priors,” in European conference on computer
vision. Springer, 2016, pp. 852–869.

[18] D. Xu, Y. Zhu, C. B. Choy, and L. Fei-Fei, “Scene graph generation
by iterative message passing,” in Proceedings of the IEEE conference
on computer vision and pattern recognition, 2017, pp. 5410–5419.

[19] Y. Li, W. Ouyang, B. Zhou, J. Shi, C. Zhang, and X. Wang, “Fac-
torizable net: an efﬁcient subgraph-based framework for scene graph
generation,” in Proceedings of the European Conference on Computer
Vision (ECCV), 2018, pp. 335–351.

[20] X. Liang, L. Lee, and E. P. Xing, “Deep variation-structured rein-
forcement learning for visual relationship and attribute detection,” in
Proceedings of the IEEE conference on computer vision and pattern
recognition, 2017, pp. 848–857.

[21] M. Y. Yang, W. Liao, H. Ackermann, and B. Rosenhahn, “On support
relations and semantic scene graphs,” ISPRS journal of photogramme-
try and remote sensing, vol. 131, pp. 15–25, 2017.

[22] K. Zampogiannis, Y. Yang, C. Ferm¨uller, and Y. Aloimonos, “Learning
the spatial semantics of manipulation actions through preposition
grounding,” in 2015 IEEE international conference on robotics and
automation (ICRA).

IEEE, 2015, pp. 1389–1396.

[23] H. Zhang, X. Lan, X. Zhou, Z. Tian, Y. Zhang, and N. Zheng, “Vi-
sual manipulation relationship network for autonomous robotics,” in
2018 IEEE-RAS 18th International Conference on Humanoid Robots
(Humanoids).

IEEE, 2018, pp. 118–125.

[24] D. Park, Y. Seo, D. Shin, J. Choi, and S. Y. Chun, “A single multi-task
deep neural network with post-processing for object detection with
reasoning and robotic grasp detection,” in 2020 IEEE International
Conference on Robotics and Automation (ICRA).
IEEE, 2020, pp.
7300–7306.

[25] A. Simeonov, Y. Du, B. Kim, F. R. Hogan, J. Tenenbaum, P. Agrawal,
and A. Rodriguez, “A long horizon planning framework for manipulat-
ing rigid pointcloud objects,” arXiv preprint arXiv:2011.08177, 2020.
[26] D. Driess, J.-S. Ha, R. Tedrake, and M. Toussaint, “Learning geometric
reasoning and control for long-horizon tasks from visual input,” in
Proc. of the IEEE International Conference on Robotics and Automa-
tion (ICRA), 2021.

[27] Y. Zhu, J. Tremblay, S. Birchﬁeld, and Y. Zhu, “Hierarchical planning
for long-horizon manipulation with geometric and symbolic scene
graphs,” arXiv preprint arXiv:2012.07277, 2020.

[28] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in
Advances in neural information processing systems, 2017, pp. 5998–
6008.

[29] Y. Li, W. Ouyang, X. Wang, and X. Tang, “Vip-cnn: Visual phrase
guided convolutional neural network,” in Proceedings of the IEEE
conference on computer vision and pattern recognition, 2017, pp.
1347–1356.

[30] K. Simonyan and A. Zisserman, “Very deep convolutional networks
for large-scale image recognition,” arXiv preprint arXiv:1409.1556,
2014.

[31] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁca-
tion with deep convolutional neural networks,” Advances in neural
information processing systems, vol. 25, pp. 1097–1105, 2012.
[32] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimiza-

tion,” arXiv preprint arXiv:1412.6980, 2014.

[33] W. Liu, A. Daruna, and S. Chernova, “Cage: Context-aware grasping
engine,” in 2020 IEEE International Conference on Robotics and
Automation (ICRA).

IEEE, 2020, pp. 2550–2556.

