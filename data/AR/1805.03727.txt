1
2
0
2

y
a
M
8
2

]

C
D
.
s
c
[

2
v
7
2
7
3
0
.
5
0
8
1
:
v
i
X
r
a

ARES: Adaptive, Reconfigurable, Erasure coded, Atomic Storage

NICOLAS NICOLAOU, Algolysis Ltd, Limassol, Cyprus
VIVECK CADAMBE, Pennsylvania State University, US
N. PRAKASH, Intel Corp.
ANDRIA TRIGEORGI, University of Cyprus, Nicosia, Cyprus
KISHORI M. KONWAR, MURIEL MEDARD, and NANCY LYNCH, Massachusetts Institute of

Technology, USA

Emulating a shared atomic, read/write storage system is a fundamental problem in distributed computing. Replicating atomic objects

among a set of data hosts was the norm for traditional implementations (e.g., [8]) in order to guarantee the availability and accessibility

of the data despite host failures. As replication is highly storage demanding, recent approaches suggested the use of erasure-codes to

offer the same fault-tolerance while optimizing storage usage at the hosts. Initial works focused on a fix set of data hosts. To guarantee

longevity and scalability, a storage service should be able to dynamically mask hosts failures by allowing new hosts to join, and failed

host to be removed without service interruptions. This work presents the first erasure-code based atomic algorithm, called ARES, which

allows the set of hosts to be modified in the course of an execution. ARES is composed of three main components: (i) a reconfiguration

protocol, (ii) a read/write protocol, and (iii) a set of data access primitives. The design of ARES is modular and is such to accommodate

the usage of various erasure-code parameters on a per-configuration basis. We provide bounds on the latency of read/write operations,

and analyze the storage and communication costs of the ARES algorithm.

ACM Reference Format:

Nicolas Nicolaou, Viveck Cadambe, N. Prakash, Andria Trigeorgi, Kishori M. Konwar, Muriel Medard, and Nancy Lynch. 2021. ARES:

Adaptive, Reconfigurable, Erasure coded, Atomic Storage . 1, 1 (May 2021), 34 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn

1 INTRODUCTION

Distributed Storage Systems (DSS) store large amounts of data in an affordable manner. Cloud vendors deploy hundreds

to thousands of commodity machines, networked together to act as a single giant storage system. Component failures

of commodity devices, and network delays are the norm, therefore, ensuring consistent data-access and availability

at the same time is challenging. Vendors often solve availability by replicating data across multiple servers. These

services use carefully constructed algorithms that ensure that these copies are consistent, especially when they can be

accessed concurrently by different operations. The problem of keeping copies consistent becomes even more challenging

This work was partially funded by the Center for Science of Information NSF Award CCF-0939370, NSF Award CCF-1461559, AFOSR Contract Number:
FA9550-14-1-0403, NSF CCF-1553248 and RPF/POST-DOC/0916/0090.
Authorsâ€™ addresses: Nicolas Nicolaou, nicolas@algolysis.comAlgolysis Ltd, Limassol, Cyprus; Viveck Cadambe, vxc12@engr.psu.eduPennsylvania State
University, US; N. Prakash, prakashn@mit.eduIntel Corp.; Andria Trigeorgi, aatrige01@cs.ucy.ac.cyUniversity of Cyprus, Nicosia, Cyprus; Kishori M.
Konwar, kishori@csail.mit.edu; Muriel Medard, medard@mit.edu; Nancy Lynch, lynch@csail.mit.eduMassachusetts Institute of Technology, USA.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or
distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work
owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to
lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.

Â© 2021 Association for Computing Machinery.
Manuscript submitted to ACM

Manuscript submitted to ACM

1

 
 
 
 
 
 
2Nicolas Nicolaou, Viveck Cadambe, N. Prakash, Andria Trigeorgi, Kishori M. Konwar, Muriel Medard, and Nancy Lynch

when failed servers need to be replaced or new servers are added, without interrupting the service. Any type of service

interruption in a heavily used DSS usually translates to immense revenue loss.

The goal of this work is to provide an algorithm for implementing strongly consistent (i.e., atomic/linearizable),

fault-tolerant distributed read/write storage, with low storage and communication footprint, and the ability to reconfigure

the set of data hosts without service interruptions.

Replication-based Atomic Storage. A long stream of work used replication of data across multiple servers to implement

atomic (linearizable) read/write objects in message-passing, asynchronous environments where servers (data hosts) may

crash fail [7, 8, 17â€“19, 21, 22, 34]. A notable replication-based algorithm appears in the work by Attiya, Bar-Noy and

Dolev [8] (we refer to as the ABD algorithm) which implemented non-blocking atomic read/write data storage via logical

timestamps paired with values to order read/write operations. Replication based strategies, however, incur high storage

and communication costs; for example, to store 1,000,000 objects each of size 1MB (a total size of 1TB) across a 3 server
system, the ABD algorithm replicates the objects in all the 3 servers, which blows up the worst-case storage cost to 3TB.
Additionally, every write or read operation may need to transmit up to 3MB of data (while retrieving an object value of
size 1MB), incurring high communication cost.

Erasure Code-based Atomic Storage. Erasure Coded-based DSS are extremely beneficial to save storage and communi-
cation costs while maintaining similar fault-tolerance levels as in replication based DSS [13]. Mechanisms using an [ğ‘›, ğ‘˜]
erasure code splits a value ğ‘£ of size, say 1 unit, into ğ‘˜ elements, each of size 1
ğ‘˜ units, creates ğ‘› coded elements of the
ğ‘›
ğ‘˜ units. So the [ğ‘› = 3, ğ‘˜ = 2] code in the
same size, and stores one coded element per server, for a total storage cost of
previous example will reduce the storage cost to 1.5TB and the communication cost to 1.5MB (improving also operation
latency). Maximum Distance Separable (MDS) codes have the property that value ğ‘£ can be reconstructed from any ğ‘˜ out
of these ğ‘› coded elements; note that replication is a special case of MDS codes with ğ‘˜ = 1. In addition to the potential
cost-savings, the suitability of erasure-codes for DSS is amplified with the emergence of highly optimized erasure coding

libraries, that reduce encoding/decoding overheads [3, 9, 38]. In fact, an exciting recent body of systems and optimization

works [4, 28, 38, 41â€“44, 46] have demonstrated that for several data stores, the use of erasure coding results in lower

latencies than replication based approaches. This is achieved by allowing the system to carefully tune erasure coding

parameters, data placement strategies, and other system parameters that improve workload characteristics â€“ such as load

and spatial distribution. A complementary body of work has proposed novel non-blocking algorithms that use erasure

coding to provide an atomic storage over asynchronous message passing models [10, 12, 13, 16, 29, 30, 45]. Since erasure

code-based algorithms, unlike their replication-based counter parts, incur the additional burden of synchronizing the
access of multiple pieces of coded-elements from the same version of the data object, these algorithms are quite complex.

Reconfigurable Atomic Storage. Configuration refers to the set of storage servers that are collectively used to host
the data and implement the DSS. Reconfiguration is the process of adding or removing servers in a DSS. In practice,

reconfigurations are often desirable by system administrators [6], for a wide range of purposes, especially during system

maintenance. As the set of storage servers becomes older and unreliable they are replaced with new ones to ensure

data-durability. Furthermore, to scale the storage service to increased or decreased load, larger (or smaller) configurations

may be needed to be deployed. Therefore, in order to carry out such reconfiguration steps, in addition to the usual read
and write operations, an operation called reconfig is invoked by reconfiguration clients. Performing reconfiguration
of a system, without service interruption, is a very challenging task and an active area of research. RAMBO [33] and

DynaStore [5] are two of the handful of algorithms [14, 20, 23, 27, 39, 40] that allows reconfiguration on live systems; all

these algorithms are replication-based.

Manuscript submitted to ACM

ARES: Adaptive, Reconfigurable, Erasure coded, Atomic Storage

3

Despite the attractive prospects of creating strongly consistent DSS with low storage and communication costs, so far,

no algorithmic framework for reconfigurable atomic DSS employed erasure coding for fault-tolerance, or provided any

analysis of bandwidth and storage costs. Our paper fills this vital gap in algorithms literature, through the development of
novel reconfigurable approach for atomic storage that use erasure codes for fault-tolerance. From a practical viewpoint,

our work may be interpreted as a bridge between the systems optimization works [4, 28, 38, 41â€“44, 46] and non-blocking
erasure coded based consistent storage [10, 12, 13, 16, 29, 30, 45]. Specifically, the use of our reconfigurable algorithms

would potentially enable a data storage service to dynamically shift between different erasure coding based parameters

and placement strategies, as the demand characteristics (such as load and spatial distribution) change, without service

interruption.

Our Contributions. We develop a reconfigurable, erasure-coded, atomic or strongly consistent [25, 32] read/write

storage algorithm, called ARES. Motivated by many practical systems, ARES assumes clients and servers are separate
processes * that communicate via logical point-to-point channels.

In contrast to the, replication-based reconfigurable algorithms [5, 14, 20, 23, 27, 33, 39, 40], where a configuration

essentially corresponds to the set of servers that stores the data, the same concept for erasure coding need to be much
more involved. In particular, in erasure coding, even if the same set of ğ‘› servers are used, a change in the value of ğ‘˜
defines a new configuration. Furthermore, several erasure coding based algorithms [12, 16] have additional parameters

that tune how many older versions each server store, which in turn influences the concurrency level allowed. Tuning of

such parameters can also fall under the purview of reconfiguration.

To accommodate these various reconfiguration requirements, ARES takes a modular approach. In particular, ARES
uses a set of primitives, called data-access primitives (DAPs). A different implementation of the DAP primitives may be

specified in each configuration. ARES uses DAPs as a â€œblack boxâ€ to: (i) transfer the object state from one configuration

to the next during reconfig operations, and (ii) invoke read/write operations on a single configuration. Given the DAP
implementation for each configuration we show that ARES correctly implements a reconfigurable, atomic read/write

storage.

Algorithm

CASGC [11]

SODA [29]
ORCAS-A [16]
ORCAS-B [16]
ABD [8]
RAMBO [33]
DYNASTORE [5]
SMARTMERGE [27]

ARES (this paper)

#rounds
/write
3

#rounds
/read
2

2
3
3
2
2
â‰¥ 4
2

2

2
â‰¥ 2
3
2
2
â‰¥ 4
2

2

Reconfig.

No

No
No
No
No
Yes
Yes
Yes

Yes

Repl. or
EC
EC

EC
EC
EC
Repl.
Repl.
Repl.
Repl.

EC

Storage cost

read bandwidth

write bandwidth

(ğ›¿ + 1) ğ‘›
ğ‘˜
ğ‘›
ğ‘˜
ğ‘›
âˆ
ğ‘›
â‰¥ ğ‘›
â‰¥ ğ‘›
â‰¥ ğ‘›
(ğ›¿ + 1) ğ‘›
ğ‘˜

ğ‘›
ğ‘˜
(ğ›¿ + 1) ğ‘›
ğ‘˜
ğ‘›
âˆ
2ğ‘›
â‰¥ ğ‘›
â‰¥ ğ‘›
â‰¥ ğ‘›
(ğ›¿ + 1) ğ‘›
ğ‘˜

ğ‘›
ğ‘˜
ğ‘›2
ğ‘˜
ğ‘›
âˆ
ğ‘›
â‰¥ ğ‘›
â‰¥ ğ‘›
â‰¥ ğ‘›
ğ‘›
ğ‘˜

Table 1. Comparison of ARES with previous algorithms emulating atomic Read/Write Memory for replication (Repl.) and
erasure-code based (EC) algorithms. ğ›¿ is the maximum number of concurrent writes with any read during the course of an
execution of the algorithm. In practice, ğ›¿ < 4 [13].

The DAP primitives provide ARES a much broader view of the notion of a configuration as compared to replication-

based algorithms. Specifically, the DAP primitives may be parameterized, following the parameters of protocols used

for their implementation (e.g., erasure coding parameters, set of servers, quorum design, concurrency level, etc.). While

transitioning from one configuration to another, our modular construction, allows ARES to reconfigure between different

*In practice, these processes can be on the same node or different nodes.

Manuscript submitted to ACM

4Nicolas Nicolaou, Viveck Cadambe, N. Prakash, Andria Trigeorgi, Kishori M. Konwar, Muriel Medard, and Nancy Lynch

sets of servers, quorum configurations, and erasure coding parameters. In principle, ARES even allows to reconfigure

between completely different protocols as long as they can be interpreted/expressed in terms of the primitives; though in

this paper, we only present one implementation of the DAP primitives to keep the scope of the paper reasonable. From a

technical point of view, our modular structure makes the atomicity proof of a complex algorithm (like ARES) easier.

An important consideration in the design choice of ARES, is to ensure that we gain/retain the advantages that come

with erasure codes â€“ cost of data storage and communication is low â€“ while having the flexibility to reconfigure the system.

Towards this end, we present an erasure-coded implementation of DAPs which satisfy the necessary properties, and are
used by ARES to yield the first reconfigurable, erasure-coded, read/write atomic storage implementation, where read
and write operations complete in two-rounds. We provide the atomicity property and latency analysis for any operation
in ARES, along with the storage and communication costs resulting from the erasure-coded DAP implementation. In

particular, we specify lower and upper bounds on the communication latency between the service participants, and
we provide the necessary conditions to guarantee the termination of each read/write operation while concurrent with
reconfig operations.

Table 1 compares ARES with a few well-known erasure-coded and replication-based (static and reconfigurable) atomic

memory algorithms. From the table we observe that ARES is the only algorithm to combine a dynamic behavior with the

use of erasure codes, while reducing the storage and communcation costs associated with the read or write operations.

Moreover, in ARES the number of rounds per write and read is at least as good as in any of the remaining algorithms.

Document Structure. Section 2 presents the model assumptions and Section 3, the DAP primitives. In Section 4, we

present the implementation of the reconfiguration and read/write protocols in ARES using the DAPs. In Section 5, we

present an erasure-coded implementation of a set of DAPs, which can be used in every configuration of the ARES

algorithm. Section 7 provides operation latency and cost analysis, and Section 8 the DAP flexibility. Section 9 presents an

experimental evaluation of the proposed algorithms. We conclude our work in Section 10. Due to lack of space omitted

proofs can be found in [36].

2 MODEL AND DEFINITIONS

A shared atomic storage, consisting of any number of individual objects, can be emulated by composing individual atomic
memory objects. Therefore, herein we aim in implementing a single atomic read/write memory object. A read/write object
takes a value from a set V. We assume a system consisting of four distinct sets of processes: a set W of writers, a set R
of readers, a set G of reconfiguration clients, and a set S of servers. Let I = W âˆª R âˆª G be the set of clients. Servers host
data elements (replicas or encoded data fragments). Each writer is allowed to modify the value of a shared object, and

each reader is allowed to obtain the value of that object. Reconfiguration clients attempt to introduce new configuration of

servers to the system in order to mask transient errors and to ensure the longevity of the service. Processes communicate
via messages through asynchronous, and reliable channels.

Configurations. A configuration, with a unique identifier from a set C, is a data type that describes the finite set of
servers that are used to implement the atomic storage service. In our setting, each configuration is also used to describe
the way the servers are grouped into intersecting sets, called quorums, the consensus instance that is used as an external

service to determine the next configuration, and a set of data access primitives that specify the interaction of the clients
and servers in the configuration (see Section 3). More formally, a configuration, ğ‘ âˆˆ C, consists of: (ğ‘–) ğ‘.ğ‘†ğ‘’ğ‘Ÿğ‘£ğ‘’ğ‘Ÿğ‘  âŠ† S: a
set of server identifiers; (ğ‘–ğ‘–) ğ‘.ğ‘„ğ‘¢ğ‘œğ‘Ÿğ‘¢ğ‘šğ‘ : the set of quorums on ğ‘.ğ‘†ğ‘’ğ‘Ÿğ‘£ğ‘’ğ‘Ÿğ‘ , s.t. âˆ€ğ‘„1, ğ‘„2 âˆˆ ğ‘.ğ‘„ğ‘¢ğ‘œğ‘Ÿğ‘¢ğ‘šğ‘ , ğ‘„1, ğ‘„2 âŠ† ğ‘.ğ‘†ğ‘’ğ‘Ÿğ‘£ğ‘’ğ‘Ÿğ‘ 
and ğ‘„1 âˆ© ğ‘„2 â‰  âˆ…; (ğ‘–ğ‘–ğ‘–) ğ·ğ´ğ‘ƒ (ğ‘): the set of primitives (operations at level lower than reads or writes) that clients in I may
Manuscript submitted to ACM

ARES: Adaptive, Reconfigurable, Erasure coded, Atomic Storage

5

invoke on ğ‘.ğ‘†ğ‘’ğ‘Ÿğ‘£ğ‘’ğ‘Ÿğ‘ ; and (ğ‘–ğ‘£) ğ‘.ğ¶ğ‘œğ‘›: a consensus instance with the values from C, implemented and running on top of the
servers in ğ‘.ğ‘†ğ‘’ğ‘Ÿğ‘£ğ‘’ğ‘Ÿğ‘ . We refer to a server ğ‘  âˆˆ ğ‘.ğ‘†ğ‘’ğ‘Ÿğ‘£ğ‘’ğ‘Ÿğ‘  as a member of configuration ğ‘. The consensus instance ğ‘.ğ¶ğ‘œğ‘› in
each configuration ğ‘ is used as a service that returns the identifier of the configuration that follows ğ‘.

Executions. An algorithm ğ´ is a collection of processes, where process ğ´ğ‘ is assigned to process ğ‘ âˆˆ I âˆª S. The state,
of a process ğ´ğ‘ is determined over a set of state variables, and the state ğœ of ğ´ is a vector that contains the state of each
process. Each process ğ´ğ‘ implements a set of actions. When an action ğ›¼ occurs it causes the state of ğ´ğ‘ to change, say
from some state ğœğ‘ to some different state ğœ â€²
ğ‘ . We call the triple âŸ¨ğœğ‘, ğ›¼, ğœ â€²
ğ‘ âŸ© a step of ğ´ğ‘ . Algorithm ğ´ performs a step,
when some process ğ´ğ‘ performs a step. An action ğ›¼ is enabled in a state ğœ if âˆƒ a step âŸ¨ğœ, ğ›¼, ğœ â€²âŸ© to some state ğœ â€². An
execution is an alternating sequence of states and actions of ğ´ starting with the initial state and ending in a state. An
execution ğœ‰ fair if enabled actions perform a step infinitely often. In the rest of the paper we consider executions that are
fair and well-formed. A process ğ‘ crashes in an execution if it stops taking steps; otherwise ğ‘ is correct or non-faulty. We
assume a function ğ‘.F to describe the failure model of a configuration ğ‘.

Reconfigurable Atomic Read/Write Objects. A reconfigurable atomic object supports three operations: read(),
write(ğ‘£) and reconfig(ğ‘). A read() operation returns the value of the atomic object, write(ğ‘£) attempts to modify the value
of the object to ğ‘£ âˆˆ V, and the reconfig(ğ‘) that attempts to install a new configuration ğ‘ âˆˆ C. We assume well-formed
executions where each client may invoke one operation (read(), write(ğ‘£) or reconfig(ğ‘)) at a time.

An implementation of a read/write or a reconfig operation contains an invocation action (such as a call to a procedure)
and a response action (such as a return from the procedure). An operation ğœ‹ is complete in an execution, if it contains both
the invocation and the matching response actions for ğœ‹; otherwise ğœ‹ is incomplete. We say that an operation ğœ‹ precedes
an operation ğœ‹ â€² in an execution ğœ‰, denoted by ğœ‹ â†’ ğœ‹ â€², if the response step of ğœ‹ appears before the invocation step of ğœ‹ â€²
in ğœ‰. Two operations are concurrent if neither precedes the other. An implementation ğ´ of a read/write object satisfies
the atomicity (linearizability [25]) property if the following holds [32]. Let the set Î  contain all complete read/write
operations in any well-formed execution of ğ´. Then there exists an irreflexive partial ordering â‰º satisfying the following:

A1. For any operations ğœ‹1 and ğœ‹2 in Î , if ğœ‹1 â†’ ğœ‹2, then it cannot be the case that ğœ‹2 â‰º ğœ‹1.
A2. If ğœ‹ âˆˆ Î  is a write operation and ğœ‹ â€² âˆˆ Î  is any read/write operation, then either ğœ‹ â‰º ğœ‹ â€² or ğœ‹ â€² â‰º ğœ‹.
A3. The value returned by a read operation is the value written by the last preceding write operation according to â‰º (or

the initial value if there is no such write).

Storage and Communication Costs. We are interested in the complexity of each read and write operation. The complexity
of each operation ğœ‹ invoked by a process ğ‘, is measured with respect to three metrics, during the interval between the
invocation and the response of ğœ‹: (ğ‘–) number of communication round, accounting the number of messages exchanged
during ğœ‹, (ğ‘–ğ‘–) storage efficiency (storage cost), accounting the maximum storage requirements for any single object at the
servers during ğœ‹, and (ğ‘–ğ‘–ğ‘–) message bit complexity (communication cost) which measures the size of the messages used
during ğœ‹.

We define the total storage cost as the size of the data stored across all servers, at any point during the execution of

the algorithm. The communication cost associated with a read or write operation is the size of the total data that gets

transmitted in the messages sent as part of the operation. We assume that metadata, such as version number, process ID,

etc. used by various operations is of negligible size, and is hence ignored in the calculation of storage and communication
cost. Further, we normalize both costs with respect to the size of the value ğ‘£; in other words, we compute the costs under
the assumption that ğ‘£ has size 1 unit.

Manuscript submitted to ACM

6Nicolas Nicolaou, Viveck Cadambe, N. Prakash, Andria Trigeorgi, Kishori M. Konwar, Muriel Medard, and Nancy Lynch

Erasure Codes. We use an [ğ‘›, ğ‘˜] linear MDS code [26] over a finite field Fğ‘ to encode and store the value ğ‘£ among
the ğ‘› servers. An [ğ‘›, ğ‘˜] MDS code has the property that any ğ‘˜ out of the ğ‘› coded elements can be used to recover
(decode) the value ğ‘£. For encoding, ğ‘£ is divided into ğ‘˜ elements ğ‘£1, ğ‘£2, . . . ğ‘£ğ‘˜ with each element having size 1
ğ‘˜ (assuming
size of ğ‘£ is 1). The encoder takes the ğ‘˜ elements as input and produces ğ‘› coded elements ğ‘’1, ğ‘’2, . . . , ğ‘’ğ‘› as output,
i.e., [ğ‘’1, . . . , ğ‘’ğ‘›] = Î¦([ğ‘£1, . . . , ğ‘£ğ‘˜ ]), where Î¦ denotes the encoder. For ease of notation, we simply write Î¦(ğ‘£) to mean
[ğ‘’1, . . . , ğ‘’ğ‘›]. The vector [ğ‘’1, . . . , ğ‘’ğ‘›] is referred to as the codeword corresponding to the value ğ‘£. Each coded element ğ‘ğ‘–
also has size 1
ğ‘˜ . In our scheme we store one coded element per server. We use Î¦ğ‘– to denote the projection of Î¦ on to the ğ‘–th
output component, i.e., ğ‘’ğ‘– = Î¦ğ‘– (ğ‘£). Without loss of generality, we associate the coded element ğ‘’ğ‘– with server ğ‘–, 1 â‰¤ ğ‘– â‰¤ ğ‘›.

Tags. We use logical tags to order operations. A tag ğœ is defined as a pair (ğ‘§, ğ‘¤), where ğ‘§ âˆˆ N and ğ‘¤ âˆˆ W, an ID of a
writer. Let T be the set of all tags. Notice that tags could be defined in any totally ordered domain and given that this
domain is countably infinite, then there can be a direct mapping to the domain we assume. For any ğœ1, ğœ2 âˆˆ T we define
ğœ2 > ğœ1 if (ğ‘–) ğœ2.ğ‘§ > ğœ1.ğ‘§ or (ğ‘–ğ‘–) ğœ2.ğ‘§ = ğœ1.ğ‘§ and ğœ2.ğ‘¤ > ğœ1.ğ‘¤.

3 DATA ACCESS PRIMITIVES

In this section we introduce a set of primitives, we refer to as data access primitives (DAP), which are invoked by the
clients during read/write/reconfig operations and are defined for any configuration ğ‘ in ARES. The DAPs allow us: (ğ‘–) to
describe ARES in a modular manner, and (ğ‘–ğ‘–) a cleaner reasoning about the correctness of ARES.

We define three data access primitives for each ğ‘ âˆˆ C: (ğ‘–) ğ‘.put-data(âŸ¨ğœ, ğ‘£âŸ©), via which a client can ingest the tag value
pair âŸ¨ğœ, ğ‘£âŸ© in to the configuration ğ‘; (ğ‘–ğ‘–) ğ‘.get-data(), used to retrieve the most up to date tag and vlaue pair stored in the
configuration ğ‘; and (ğ‘–ğ‘–ğ‘–) ğ‘.get-tag(), used to retrieve the most up to date tag for an object stored in a configuration ğ‘.
More formally, assuming a tag ğœ from a set of totally ordered tags T , a value ğ‘£ from a domain V, and a configuration ğ‘
from a set of identifiers C, the three primitives are defined as follows:

DEFINITION 1 (DATA ACCESS PRIMITIVES). Given a configuration identifier ğ‘ âˆˆ C, any non-faulty client process
ğ‘ may invoke the following data access primitives during an execution ğœ‰, where ğ‘ is added to specify the configuration
specific implementation of the primitives:

ğ·1: ğ‘.get-tag() that returns a tag ğœ âˆˆ T ;
ğ·2: ğ‘.get-data() that returns a tag-value pair (ğœ, ğ‘£) âˆˆ T Ã— V,
ğ·3: ğ‘.put-data(âŸ¨ğœ, ğ‘£âŸ©) which accepts the tag-value pair (ğœ, ğ‘£) âˆˆ T Ã— V as argument.

In order for the DAPs to be useful in designing the ARES algorithm we further require the following consistency

properties. As we see later in Section 6, the safety property of ARES holds, given that these properties hold for the DAPs

in each configuration.

PROPERTY 1 (DAP CONSISTENCY PROPERTIES). In an execution ğœ‰ we say that a DAP operation in an execution ğœ‰
is complete if both the invocation and the matching response step appear in ğœ‰. If Î  is the set of complete DAP operations
in execution ğœ‰ then for any ğœ™, ğœ‹ âˆˆ Î :

C1 If ğœ™ is ğ‘.put-data(âŸ¨ğœğœ™, ğ‘£ğœ™ âŸ©), for ğ‘ âˆˆ C, âŸ¨ğœğœ™, ğ‘£ğœ™ âŸ© âˆˆ T Ã— V, and ğœ‹ is ğ‘.get-tag() (or ğ‘.get-data()) that returns

ğœğœ‹ âˆˆ T (or âŸ¨ğœğœ‹ , ğ‘£ğœ‹ âŸ© âˆˆ T Ã— V) and ğœ™ completes before ğœ‹ is invoked in ğœ‰, then ğœğœ‹ â‰¥ ğœğœ™ .

C2 If ğœ™ is a ğ‘.get-data() that returns âŸ¨ğœğœ‹ , ğ‘£ğœ‹ âŸ© âˆˆ T Ã— V, then there exists ğœ‹ such that ğœ‹ is ğ‘.put-data(âŸ¨ğœğœ‹ , ğ‘£ğœ‹ âŸ©) and ğœ™

did not complete before the invocation of ğœ‹. If no such ğœ‹ exists in ğœ‰, then (ğœğœ‹ , ğ‘£ğœ‹ ) is equal to (ğ‘¡0, ğ‘£0).

Manuscript submitted to ACM

ARES: Adaptive, Reconfigurable, Erasure coded, Atomic Storage

7

In Section 5 we show how to implement a set of DAPs, where erasure-codes are used to reduce storage and communi-

cation costs. Our DAP implementation satisfies Property 1.

As noted earlier, expressing ARES in terms of the DAPs allows one to achieve a modular design. Modularity enables

the usage of different DAP implementation per configuration, during any execution of ARES, as long as the DAPs
implemented in each configuration satisfy Property 1. For example, the DAPs in a configuration ğ‘ may be implemented
using replication, while the DAPs in the next configuration say ğ‘ â€², may be implemented using erasure-codes. Thus, a
system may use a scheme that offers higher fault tolerance (e.g. replication) when storage is not an issue, while switching

to a more storage efficient (less fault-tolerant) scheme when storage gets limited.

In Section 8, we show that the presented DAPs are not only suitable for algorithm ARES, but can also be used to
implement a large family of atomic read/write storage implementations. By describing an algorithm ğ´ according to a
simple algorithmic template (see Algorithm 7), we show that ğ´ preserves safety (atomicity) if the used DAPs satisfy
Property 1, and ğ´ preserves liveness (termination), if every invocation of the used DAPs terminate, under the failure
model assumed.

4 THE ARES PROTOCOL

In this section, we describe ARES. In the presentation of ARES algorithm we decouple the reconfiguration service

from the shared memory emulation, by utilizing the DAPs presented in Section 3. This allows ARES, to handle both

the reorganization of the servers that host the data, as well as utilize a different atomic memory implementation per

configuration. It is also important to note that ARES adopts a client-server architecture and separates the reader, writer and

reconfiguration processes from the server processes that host the object data. More precisely, ARES algorithm comprises
of three major components: (ğ‘–) The reconfiguration protocol which consists of invoking, and subsequently installing new
configuration via the reconfig operation by recon clients. (ğ‘–ğ‘–) The read/write protocol for executing the read and write
operations invoked by readers and writers. (ğ‘–ğ‘–ğ‘–) The implementation of the DAPs for each installed configuration that
respect Property 1 and which are used by the reconfig, read and write operations.

4.1 Implementation of the Reconfiguration Service.

In this section, we describe the reconfiguration service in ARES. The service relies on an underlying sequence of

configurations (already proposed or installed by reconfig operations), in the from of a â€œdistributed listâ€, which we refer to
as the global configuration sequence (or list) Gğ¿. Conceptually, Gğ¿ represents an ordered list of pairs âŸ¨ğ‘, ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘¢ğ‘ âŸ©, where ğ‘
is a configuration identifier (ğ‘ âˆˆ C), and a binary state variable ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘¢ğ‘  âˆˆ {ğ¹, ğ‘ƒ } that denotes whether ğ‘ is finalized (ğ¹ ) or is
still pending (ğ‘ƒ). Initially, Gğ¿ contains a single element, say âŸ¨ğ‘0, ğ¹ âŸ©, which is known to every participant in the service.
To facilitate the creation of Gğ¿, each server in ğ‘.ğ‘†ğ‘’ğ‘Ÿğ‘£ğ‘’ğ‘Ÿğ‘  maintains a local variable ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶ âˆˆ {C âˆª {âŠ¥}} Ã— {ğ‘ƒ, ğ¹ }, which
is used to point to the configuration that follows ğ‘ in Gğ¿. Initially, at any server ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶ = âŸ¨âŠ¥, ğ¹ âŸ©. Once ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶ it is set to a
value it is never altered. As we show below, at any point in the execution of ARES and in any configuration ğ‘, the ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶
variables of the non-faulty servers in ğ‘ that are not equal to âŠ¥ agree, i.e., {ğ‘ .ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶ : ğ‘  âˆˆ ğ‘.ğ‘†ğ‘’ğ‘Ÿğ‘£ğ‘’ğ‘Ÿğ‘  âˆ§ ğ‘ .ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶ â‰  âŠ¥} is either
empty of has only one element.

Clients discover the configuration that follows a âŸ¨ğ‘, âˆ—âŸ© in the sequence by contacting a subset of servers in ğ‘.ğ‘†ğ‘’ğ‘Ÿğ‘£ğ‘’ğ‘Ÿğ‘  and
collecting their ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶ variables. Every client in I maintains a local variable ğ‘ğ‘ ğ‘’ğ‘ that is expected to be some subsequence
ğ‘¥ (a caret over some name) to denote state
of Gğ¿. Initially, at every client the value of ğ‘ğ‘ ğ‘’ğ‘ is âŸ¨ğ‘0, ğ¹ âŸ©. We use the notation (cid:98)
variables that assume values from the domain {C âˆª {âŠ¥}} Ã— {ğ‘ƒ, ğ¹ }.

Manuscript submitted to ACM

8Nicolas Nicolaou, Viveck Cadambe, N. Prakash, Andria Trigeorgi, Kishori M. Konwar, Muriel Medard, and Nancy Lynch

Reconfiguration clients may introduce new configurations, each associated with a unique configuration identifier from
C. Multiple clients may concurrently attempt to introduce different configurations for same next link in Gğ¿. ARES uses
consensus to resolve such conflicts: a subset of servers in ğ‘.ğ‘†ğ‘’ğ‘Ÿğ‘£ğ‘’ğ‘Ÿğ‘ , in each configuration ğ‘, implements a distributed
consensus service (such as Paxos [31], RAFT [37]) , denoted by ğ‘.ğ¶ğ‘œğ‘›.

The reconfiguration service consists of two major components: (ğ‘–) sequence traversal, responsible of discovering a

recent configuration in Gğ¿, and (ğ‘–ğ‘–) reconfiguration operation that installs new configurations in Gğ¿.

Algorithm 1 Sequence traversal at each process ğ‘ âˆˆ I of algorithm ARES.

procedure read-config(ğ‘ ğ‘’ğ‘)

until âˆƒğ‘„, ğ‘„ âˆˆ ğ‘.ğ‘„ğ‘¢ğ‘œğ‘Ÿğ‘¢ğ‘šğ‘  s.t. ğ‘Ÿğ‘’ğ‘ğ‘– receives ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶ğ‘  from

2:

4:

6:

8:

ğœ‡ = max( { ğ‘— : ğ‘ ğ‘’ğ‘ [ ğ‘— ].ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘¢ğ‘  = ğ¹ })
ğ‘ â† ğ‘ ğ‘’ğ‘ [ğœ‡ ]
(cid:98)
while (cid:98)
ğ‘ â†get-next-config((cid:98)
(cid:98)
if (cid:98)

ğ‘ â‰  âŠ¥ do

ğ‘.ğ‘ ğ‘“ ğ‘”)

ğ‘ â‰  âŠ¥ then
ğœ‡ â† ğœ‡ + 1
ğ‘ ğ‘’ğ‘ [ğœ‡ ] â† (cid:98)
ğ‘
put-config(ğ‘ ğ‘’ğ‘ [ğœ‡ âˆ’ 1].ğ‘ ğ‘“ ğ‘”, ğ‘ ğ‘’ğ‘ [ğœ‡ ])

10:

end while
return ğ‘ ğ‘’ğ‘

12: end procedure

âˆ€ğ‘  âˆˆ ğ‘„

16:

18:

if âˆƒğ‘  âˆˆ ğ‘„ s.t. ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶ğ‘  .ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘¢ğ‘  = ğ¹ then

return ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶ğ‘ 

else if âˆƒğ‘  âˆˆ ğ‘„ s.t. ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶ğ‘  .ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘¢ğ‘  = ğ‘ƒ then

return ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶ğ‘ 

20:

else

return âŠ¥
22: end procedure

procedure put-config(ğ‘, ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶)

24:

send (WRITE-CONFIG, ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶) to each ğ‘  âˆˆ ğ‘.ğ‘†ğ‘’ğ‘Ÿ ğ‘£ğ‘’ğ‘Ÿğ‘ 
until âˆƒğ‘„, ğ‘„ âˆˆ ğ‘.ğ‘„ğ‘¢ğ‘œğ‘Ÿğ‘¢ğ‘šğ‘  s.t. ğ‘Ÿğ‘’ğ‘ğ‘– receives ACK from

procedure get-next-config(ğ‘)

âˆ€ğ‘  âˆˆ ğ‘„

14:

send (READ-CONFIG) to each ğ‘  âˆˆ ğ‘.ğ‘†ğ‘’ğ‘Ÿ ğ‘£ğ‘’ğ‘Ÿğ‘ 

26: end procedure

Sequence Traversal. Any read/write/reconfig operation ğœ‹ utilizes the sequence traversal mechanism to discover the
latest state of the global configuration sequence, as well as to ensure that such a state is discoverable by any subsequent
operation ğœ‹ â€². See Fig. 1 for an example execution in the case of a reconfig operation. In a high level, a client starts
by collecting the ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶ variables from a quorum of servers in a configuration ğ‘, such that âŸ¨ğ‘, ğ¹ âŸ© is the last finalized
configuration in that clientâ€™s local ğ‘ğ‘ ğ‘’ğ‘ variable (or ğ‘0 if no other finalized configuration exists). If any server ğ‘  returns
a ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶ variable such that ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶.ğ‘ ğ‘“ ğ‘” â‰  âŠ¥, then the client (ğ‘–) adds ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶ in its local ğ‘ğ‘ ğ‘’ğ‘, (ğ‘–ğ‘–) propagates ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶ in a
quorum of servers in ğ‘.ğ‘†ğ‘’ğ‘Ÿğ‘£ğ‘’ğ‘Ÿğ‘ , and (ğ‘–ğ‘–ğ‘–) repeats this process in the configuration ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶.ğ‘ ğ‘“ ğ‘”. The client terminates when
all servers reply with ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶.ğ‘ ğ‘“ ğ‘” = âŠ¥. More precisely, the sequence parsing consists of three actions (see Alg. 1):

get-next-config(ğ‘): The action get-next-config returns the configuration that follows ğ‘ in Gğ¿. During get-next-config(ğ‘),
a client sends READ-CONFIG messages to all the servers in ğ‘.ğ‘†ğ‘’ğ‘Ÿğ‘£ğ‘’ğ‘Ÿğ‘ , and waits for replies containing ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶ from a
quorum in ğ‘.ğ‘„ğ‘¢ğ‘œğ‘Ÿğ‘¢ğ‘šğ‘ . If there exists a reply with ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶.ğ‘ ğ‘“ ğ‘” â‰  âŠ¥ the action returns ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶; otherwise it returns âŠ¥.

put-config(ğ‘, ğ‘ â€²): The put-config(ğ‘, ğ‘ â€²) action propagates ğ‘ â€² to a quorum of servers in ğ‘.ğ‘†ğ‘’ğ‘Ÿğ‘£ğ‘’ğ‘Ÿğ‘ . During the action,
the client sends (WRITE-CONFIG, ğ‘ â€²) messages, to the servers in ğ‘.ğ‘†ğ‘’ğ‘Ÿğ‘£ğ‘’ğ‘Ÿğ‘  and waits for each server ğ‘  in some quorum
ğ‘„ âˆˆ ğ‘.ğ‘„ğ‘¢ğ‘œğ‘Ÿğ‘¢ğ‘šğ‘  to respond.

read-config(ğ‘ ğ‘’ğ‘): A read-config(ğ‘ ğ‘’ğ‘) sequentially traverses the installed configurations in order to discover the latest
state of the sequence Gğ¿. At invocation, the client starts with the last finalized configuration âŸ¨ğ‘, ğ¹ âŸ© in the given ğ‘ ğ‘’ğ‘ (Line
A1:2), and enters a loop to traverse Gğ¿ by invoking get-next-config(), which returns the next configuration, assigned
ğ‘ is appended at the end of the sequence ğ‘ ğ‘’ğ‘; (b) a put-config action is invoked to inform a
to (cid:98)
quorum of servers in ğ‘.ğ‘†ğ‘’ğ‘Ÿğ‘£ğ‘’ğ‘Ÿğ‘  to update the value of their ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶ variable to (cid:98)
ğ‘ = âŠ¥ the loop terminates and the action
read-config returns ğ‘ ğ‘’ğ‘.
Manuscript submitted to ACM

ğ‘ â‰  âŠ¥, then: (a) (cid:98)

ğ‘. While (cid:98)

ğ‘. If (cid:98)

ARES: Adaptive, Reconfigurable, Erasure coded, Atomic Storage

9

Algorithm 2 Reconfiguration protocol of algorithm ARES.

at each reconfigurer ğ‘Ÿğ‘’ğ‘ğ‘–

2: State Variables:

ğ‘ğ‘ ğ‘’ğ‘ []ğ‘ .ğ‘¡ .ğ‘ğ‘ ğ‘’ğ‘ [ ğ‘— ] âˆˆ C Ã— {ğ¹, ğ‘ƒ } with members:

return ğ‘ ğ‘’ğ‘

20: end procedure

4: Initialization:

ğ‘ğ‘ ğ‘’ğ‘ [0] = âŸ¨ğ‘0, ğ¹ âŸ©

6: operation reconfig(c)
if ğ‘ â‰  âŠ¥ then

8:

10:

ğ‘ğ‘ ğ‘’ğ‘ â†read-config(ğ‘ğ‘ ğ‘’ğ‘)
ğ‘ğ‘ ğ‘’ğ‘ â† add-config(ğ‘ğ‘ ğ‘’ğ‘, ğ‘)
update-config(ğ‘ğ‘ ğ‘’ğ‘)
ğ‘ğ‘ ğ‘’ğ‘ â† finalize-config(ğ‘ğ‘ ğ‘’ğ‘)

12: end operation

procedure add-config(ğ‘ ğ‘’ğ‘, ğ‘)

14:

16:

18:

ğœˆ â† |ğ‘ ğ‘’ğ‘ |
ğ‘â€² â† ğ‘ ğ‘’ğ‘ [ğœˆ ].ğ‘ ğ‘“ ğ‘”
ğ‘‘ â† ğ‘â€².ğ¶ğ‘œğ‘›.ğ‘ğ‘Ÿğ‘œğ‘ğ‘œğ‘ ğ‘’ (ğ‘)
ğ‘ ğ‘’ğ‘ [ğœˆ + 1] â† âŸ¨ğ‘‘, ğ‘ƒ âŸ©
put-config(ğ‘â€², âŸ¨ğ‘‘, ğ‘ƒ âŸ©)

procedure update-config(ğ‘ ğ‘’ğ‘)

22:

ğœ‡ â† max( { ğ‘— : ğ‘ ğ‘’ğ‘ [ ğ‘— ].ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘¢ğ‘  = ğ¹ })
ğœˆ â† |ğ‘ ğ‘’ğ‘ |

24: ğ‘€ â† âˆ…

for ğ‘– = ğœ‡ : ğœˆ do

26:

28:

âŸ¨ğ‘¡, ğ‘£ âŸ© â† ğ‘ ğ‘’ğ‘ [ğ‘– ].ğ‘ ğ‘“ ğ‘”.get-data()
ğ‘€ â† ğ‘€ âˆª { âŸ¨ğœ, ğ‘£ âŸ© }

âŸ¨ğœ, ğ‘£ âŸ© â† maxğ‘¡ { âŸ¨ğ‘¡, ğ‘£ âŸ© : âŸ¨ğ‘¡, ğ‘£ âŸ© âˆˆ ğ‘€ }
ğ‘ ğ‘’ğ‘ [ğœˆ ].ğ‘ ğ‘“ ğ‘”.put-data( âŸ¨ğœ, ğ‘£ âŸ©)

30: end procedure

32:

34:

procedure finalize-config(ğ‘ ğ‘’ğ‘)

ğœˆ = |ğ‘ ğ‘’ğ‘ |
ğ‘ ğ‘’ğ‘ [ğœˆ ].ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘¢ğ‘  â† ğ¹
put-config(ğ‘ ğ‘’ğ‘ [ğœˆ âˆ’ 1].ğ‘ ğ‘“ ğ‘”, ğ‘ ğ‘’ğ‘ [ğœˆ ])
return ğ‘ ğ‘’ğ‘

36: end procedure

Algorithm 3 Server protocol of algorithm ARES.

at each server ğ‘ ğ‘– in configuration ğ‘ğ‘˜

8: end receive

2: State Variables:

ğœ âˆˆ N Ã— W, initially, âŸ¨0, âŠ¥âŸ©

4: ğ‘£ âˆˆ ğ‘‰ , intially, âŠ¥

ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶ âˆˆ C Ã— {ğ‘ƒ, ğ¹ }, initially âŸ¨âŠ¥, ğ‘ƒ âŸ©

6: Upon receive (READ-CONFIG) ğ‘ ğ‘– , ğ‘ğ‘˜ from ğ‘

send ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶ to ğ‘

10:

12:

Upon receive (WRITE-CONFIG, ğ‘ ğ‘“ ğ‘”ğ‘‡ğ‘–ğ‘›) ğ‘ ğ‘– , ğ‘ğ‘˜ from ğ‘
if ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶.ğ‘ ğ‘“ ğ‘” = âŠ¥ âˆ¨ ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶.ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘¢ğ‘  = ğ‘ƒ then

ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶ â† ğ‘ ğ‘“ ğ‘”ğ‘‡ğ‘–ğ‘›

send ACK to ğ‘

end receive

Reconfiguration operation. A reconfiguration operation reconfig(ğ‘), ğ‘ âˆˆ C, invoked by any reconfiguration client ğ‘Ÿğ‘’ğ‘ğ‘– ,
attempts to append ğ‘ to Gğ¿. The set of server processes in ğ‘ are not a part of any other configuration different from ğ‘. In
a high-level, ğ‘Ÿğ‘’ğ‘ğ‘– first executes a sequence traversal to discover the latest state of Gğ¿. Then it attempts to add the new
configuration ğ‘, at the end of the discovered sequence by proposing ğ‘ in the consensus instance of the last configuration in
the sequence. The client accepts and appends the decision of the consensus instance (that might be different than ğ‘). Then
it attempts to transfer the latest value of the read/write object to the latest installed configuration. Once the information is
transferred, ğ‘Ÿğ‘’ğ‘ğ‘– finalizes the last configuration in its local sequence and propagates the finalized tuple to a quorum of
servers in that configuration. The operation consists of four phases, executed consecutively by ğ‘Ÿğ‘’ğ‘ğ‘– (see Alg. 2):

read-config(ğ‘ ğ‘’ğ‘): The phase read-config(ğ‘ ğ‘’ğ‘) at ğ‘Ÿğ‘’ğ‘ğ‘– , reads the recent global configuration sequence as described in

the sequence traversal.

add-config(ğ‘ ğ‘’ğ‘, ğ‘): The add-config(ğ‘ ğ‘’ğ‘, ğ‘) attempts to append a new configuration ğ‘ to the end of ğ‘ ğ‘’ğ‘ (clientâ€™s view
of Gğ¿). Suppose the last configuration in ğ‘ ğ‘’ğ‘ is ğ‘ â€² (with status either ğ¹ or ğ‘ƒ), then in order to decide the most recent
configuration, ğ‘Ÿğ‘’ğ‘ğ‘– invokes ğ‘ â€².ğ¶ğ‘œğ‘›.ğ‘ğ‘Ÿğ‘œğ‘ğ‘œğ‘ ğ‘’ (ğ‘), on the consensus object associated with configuration ğ‘ â€². Let ğ‘‘ âˆˆ C be
Manuscript submitted to ACM

10Nicolas Nicolaou, Viveck Cadambe, N. Prakash, Andria Trigeorgi, Kishori M. Konwar, Muriel Medard, and Nancy Lynch

Fig. 1. Illustration of an execution of the reconfiguration steps.
the configuration identifier decided by the consensus service. If ğ‘‘ â‰  ğ‘, this implies that another (possibly concurrent)
reconfiguration operation, invoked by a reconfigurer ğ‘Ÿğ‘’ğ‘ ğ‘— â‰  ğ‘Ÿğ‘’ğ‘ğ‘– , proposed and succeeded ğ‘‘ as the configuration to follow
ğ‘ â€². In this case, ğ‘Ÿğ‘’ğ‘ğ‘– adopts ğ‘‘ as it own propose configuration, by adding âŸ¨ğ‘‘, ğ‘ƒâŸ© to the end of its local ğ‘ğ‘ ğ‘’ğ‘ (entirely
ignoring ğ‘), using the operation put-config(ğ‘ â€², âŸ¨ğ‘‘, ğ‘ƒâŸ©), and returns the extended configuration ğ‘ ğ‘’ğ‘.

update-config(ğ‘ ğ‘’ğ‘): Let us denote by ğœ‡ the index of the last configuration in the local sequence ğ‘ğ‘ ğ‘’ğ‘, at ğ‘Ÿğ‘’ğ‘ğ‘– , such
that its corresponding status is ğ¹ ; and ğœˆ denote the last index of ğ‘ğ‘ ğ‘’ğ‘. Next ğ‘Ÿğ‘’ğ‘ğ‘– invokes update-config(ğ‘ğ‘ ğ‘’ğ‘), which
gathers the tag-value pair corresponding to the maximum tag in each of the configurations in (cid:155)ğ‘ğ‘ ğ‘’ğ‘[ğ‘–] for ğœ‡ â‰¤ ğ‘– â‰¤ ğœˆ,
and transfers that pair to the configuration that was added by the add-config action. The get-data and put-data DAPs
are used to transfer the value of the object to the new configuration, and they are implemented with respect to the
configuration that is accessed. Suppose âŸ¨ğ‘¡ğ‘šğ‘ğ‘¥ , ğ‘£ğ‘šğ‘ğ‘¥ âŸ© is the tag value pair corresponding to the highest tag among the
responses from all the ğœˆ âˆ’ ğœ‡ + 1 configurations. Then, âŸ¨ğ‘¡ğ‘šğ‘ğ‘¥ , ğ‘£ğ‘šğ‘ğ‘¥ âŸ© is written to the configuration ğ‘‘ via the invocation of
(cid:156)ğ‘ğ‘ ğ‘’ğ‘ [ğœˆ].ğ‘ ğ‘“ ğ‘”.put-data(âŸ¨ğœğ‘šğ‘ğ‘¥ , ğ‘£ğ‘šğ‘ğ‘¥ âŸ©).

finalize-config(ğ‘ğ‘ ğ‘’ğ‘): Once the tag-value pair is transferred, in the last phase of the reconfiguration operation, ğ‘Ÿğ‘’ğ‘ğ‘–
executes finalize-config(ğ‘ğ‘ ğ‘’ğ‘), to update the status of the last configuration in ğ‘ğ‘ ğ‘’ğ‘, say ğ‘‘ = (cid:156)ğ‘ğ‘ ğ‘’ğ‘ [ğœˆ].ğ‘ ğ‘“ ğ‘”, to ğ¹ . The
reconfigurer ğ‘Ÿğ‘’ğ‘ğ‘– informs a quorum of servers in the previous configuration ğ‘ = (cid:156)ğ‘ğ‘ ğ‘’ğ‘[ğœˆ âˆ’ 1].ğ‘ ğ‘“ ğ‘”, i.e. in some ğ‘„ âˆˆ
ğ‘.ğ‘„ğ‘¢ğ‘œğ‘Ÿğ‘¢ğ‘šğ‘ , about the change of status, by executing the put-config(ğ‘, âŸ¨ğ‘‘, ğ¹ âŸ©) action.

Server Protocol. Each server responds to requests from clients (Alg. 3). A server waits for two types of messages:
READ-CONFIG and WRITE-CONFIG. When a READ-CONFIG message is received for a particular configuration ğ‘ğ‘˜ , then
the server returns ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶ variables of the servers in ğ‘ğ‘˜ .ğ‘†ğ‘’ğ‘Ÿğ‘£ğ‘’ğ‘Ÿğ‘ . A WRITE-CONFIG message attempts to update the ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶
variable of the server with a particular tuple ğ‘ ğ‘“ ğ‘”ğ‘‡ğ‘–ğ‘›. A server changes the value of its local ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶.ğ‘ ğ‘“ ğ‘” in two cases: (i)
ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶.ğ‘ ğ‘“ ğ‘” = âŠ¥, or (ii) ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶.ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘¢ğ‘  = ğ‘ƒ.

Fig. 1 illustrates an example execution of a reconfiguration operation recon(ğ‘5). In this example, the reconfigurer ğ‘Ÿğ‘’ğ‘ğ‘–
goes through a number of configuration queries (read-next-config) before it reaches configuration ğ‘4 in which a quorum
of servers replies with ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶.ğ‘ ğ‘“ ğ‘” = âŠ¥. There it proposes ğ‘5 to the consensus object of ğ‘4 (ğ‘4.ğ¶ğ‘œğ‘›.ğ‘ğ‘Ÿğ‘œğ‘ğ‘œğ‘ ğ‘’ (ğ‘5) on arrow
10), and once ğ‘5 is decided, recon(ğ‘5) completes after executing finalize-config(ğ‘5).

4.2 Implementation of Read and Write operations.

The read and write operations in ARES are expressed in terms of the DAP primitives (see Section 3). This provides the

flexibility to ARES to use different implementation of DAP primitives in different configurations, without changing the

Manuscript submitted to ACM

ARES: Adaptive, Reconfigurable, Erasure coded, Atomic Storage

11

Algorithm 4 Write and Read protocols at the clients for ARES.

2:

4:

6:

8:

10:

12:

14:

16:

18:

20:

22:

Write Operation:

at each writer ğ‘¤ğ‘–
State Variables:
ğ‘ğ‘ ğ‘’ğ‘ []ğ‘ .ğ‘¡ .ğ‘ğ‘ ğ‘’ğ‘ [ ğ‘— ] âˆˆ C Ã— {ğ¹, ğ‘ƒ } with members:
Initialization:
ğ‘ğ‘ ğ‘’ğ‘ [0] = âŸ¨ğ‘0, ğ¹ âŸ©

24: Read Operation:

26:

28:

at each reader ğ‘Ÿğ‘–
State Variables:
ğ‘ğ‘ ğ‘’ğ‘ []ğ‘ .ğ‘¡ .ğ‘ğ‘ ğ‘’ğ‘ [ ğ‘— ] âˆˆ C Ã— {ğ¹, ğ‘ƒ } with members:
Initialization:
ğ‘ğ‘ ğ‘’ğ‘ [0] = âŸ¨ğ‘0, ğ¹ âŸ©

operation write(ğ‘£ğ‘ğ‘™), ğ‘£ğ‘ğ‘™ âˆˆ ğ‘‰
ğ‘ğ‘ ğ‘’ğ‘ â†read-config(ğ‘ğ‘ ğ‘’ğ‘)
ğœ‡ â† max( {ğ‘– : ğ‘ğ‘ ğ‘’ğ‘ [ğ‘– ].ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘¢ğ‘  = ğ¹ })
ğœˆ â† |ğ‘ğ‘ ğ‘’ğ‘ |
for ğ‘– = ğœ‡ : ğœˆ do

ğœğ‘šğ‘ğ‘¥ â† max(ğ‘ğ‘ ğ‘’ğ‘ [ğ‘– ].ğ‘ ğ‘“ ğ‘”.get-tag(), ğœğ‘šğ‘ğ‘¥ )

âŸ¨ğœ, ğ‘£ âŸ© â† âŸ¨âŸ¨ğœğ‘šğ‘ğ‘¥ .ğ‘¡ğ‘  + 1, ğœ”ğ‘– âŸ©, ğ‘£ğ‘ğ‘™ âŸ©
ğ‘‘ğ‘œğ‘›ğ‘’ â† ğ‘“ ğ‘ğ‘™ğ‘ ğ‘’
while not ğ‘‘ğ‘œğ‘›ğ‘’ do

ğ‘ğ‘ ğ‘’ğ‘ [ğœˆ ].ğ‘ ğ‘“ ğ‘”.put-data( âŸ¨ğœ, ğ‘£ âŸ©)
ğ‘ğ‘ ğ‘’ğ‘ â†read-config(ğ‘ğ‘ ğ‘’ğ‘)
if |ğ‘ğ‘ ğ‘’ğ‘ | = ğœˆ then
ğ‘‘ğ‘œğ‘›ğ‘’ â† ğ‘¡ğ‘Ÿğ‘¢ğ‘’

else

ğœˆ â† |ğ‘ğ‘ ğ‘’ğ‘ |

end while
end operation

30:

operation read( )

ğ‘ğ‘ ğ‘’ğ‘ â†read-config(ğ‘ğ‘ ğ‘’ğ‘)
ğœ‡ â† max( { ğ‘— : ğ‘ğ‘ ğ‘’ğ‘ [ ğ‘— ].ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘¢ğ‘  = ğ¹ })
ğœˆ â† |ğ‘ğ‘ ğ‘’ğ‘ |
for ğ‘– = ğœ‡ : ğœˆ do

âŸ¨ğœ, ğ‘£ âŸ© â† max(ğ‘ğ‘ ğ‘’ğ‘ [ğ‘– ].ğ‘ ğ‘“ ğ‘”.get-data(), âŸ¨ğœ, ğ‘£ âŸ©)

ğ‘‘ğ‘œğ‘›ğ‘’ â† false
while not ğ‘‘ğ‘œğ‘›ğ‘’ do

ğ‘ğ‘ ğ‘’ğ‘ [ğœˆ ].ğ‘ ğ‘“ ğ‘”.put-data( âŸ¨ğœ, ğ‘£ âŸ©)
ğ‘ğ‘ ğ‘’ğ‘ â†read-config(ğ‘ğ‘ ğ‘’ğ‘)
if |ğ‘ğ‘ ğ‘’ğ‘ | = ğœˆ then
ğ‘‘ğ‘œğ‘›ğ‘’ â† ğ‘¡ğ‘Ÿğ‘¢ğ‘’

else

ğœˆ â† |ğ‘ğ‘ ğ‘’ğ‘ |

end while
return ğ‘£
end operation

32:

34:

36:

38:

40:

42:

44:

46:

basic structure of ARES. At a high-level, a write (or read) operation is executed where the client: (ğ‘–) obtains the latest
configuration sequence by using the read-config action of the reconfiguration service, (ğ‘–ğ‘–) queries the configurations,
in ğ‘ğ‘ ğ‘’ğ‘, starting from the last finalized configuration to the end of the discovered configuration sequence, for the latest
âŸ¨ğ‘¡ğ‘ğ‘”, ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’âŸ© pair with a help of get-tag (or get-data) operation as specified for each configuration, and (ğ‘–ğ‘–ğ‘–) repeatedly
propagates a new âŸ¨ğ‘¡ğ‘ğ‘”â€², ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’ â€²âŸ© pair (the largest âŸ¨ğ‘¡ğ‘ğ‘”, ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’âŸ© pair) with put-data in the last configuration of its local
sequence, until no additional configuration is observed. In more detail, the algorithm of a read or write operation ğœ‹ is as
follows (see Alg. 4):

A write (or read) operation is invoked at a client ğ‘ when line Alg. 4:8 (resp. line Alg. 4:31) is executed. At first, ğ‘

issues a read-config action to obtain the latest introduced configuration in Gğ¿, in both operations.

If ğœ‹ is a write ğ‘ detects the last finalized entry in ğ‘ğ‘ ğ‘’ğ‘, say ğœ‡, and performs a ğ‘ğ‘ ğ‘’ğ‘ [ ğ‘—].ğ‘ğ‘œğ‘›ğ‘“ .get-tag() action, for
ğœ‡ â‰¤ ğ‘— â‰¤ |ğ‘ğ‘ ğ‘’ğ‘| (line Alg. 4:9). Then ğ‘ discovers the maximum tag among all the returned tags (ğœğ‘šğ‘ğ‘¥ ), and it increments
the maximum tag discovered (by incrementing the integer part of ğœğ‘šğ‘ğ‘¥ ), generating a new tag, say ğœğ‘›ğ‘’ğ‘¤. It assigns âŸ¨ğœ, ğ‘£âŸ©
to âŸ¨ğœğ‘›ğ‘’ğ‘¤, ğ‘£ğ‘ğ‘™âŸ©, where ğ‘£ğ‘ğ‘™ is the value he wants to write (Line Alg. 4:13).

if ğœ‹ is a read, ğ‘ detects the last finalized entry in ğ‘ğ‘ ğ‘’ğ‘, say ğœ‡, and performs a ğ‘ğ‘ ğ‘’ğ‘[ ğ‘—].ğ‘ğ‘œğ‘›ğ‘“ .get-data() action, for
ğœ‡ â‰¤ ğ‘— â‰¤ |ğ‘ğ‘ ğ‘’ğ‘| (line Alg. 4:32). Then ğ‘ discovers the maximum tag-value pair (âŸ¨ğœğ‘šğ‘ğ‘¥ , ğ‘£ğ‘šğ‘ğ‘¥ âŸ©) among the replies, and
assigns âŸ¨ğœ, ğ‘£âŸ© to âŸ¨ğœğ‘šğ‘ğ‘¥ , ğ‘£ğ‘šğ‘ğ‘¥ âŸ©.

Once specifying the âŸ¨ğœ, ğ‘£âŸ© to be propagated, both reads and writes execute the ğ‘ğ‘ ğ‘’ğ‘[ğœˆ].ğ‘ ğ‘“ ğ‘”.put-data(âŸ¨ğœ, ğ‘£âŸ©) action,
where ğœˆ = |ğ‘ğ‘ ğ‘’ğ‘|, followed by executing read-config action, to examine whether new configurations were introduced
in Gğ¿. The repeat these steps until no new configuration is discovered (lines Alg. 4:15â€“21, or lines Alg. 4:37â€“43). Let

Manuscript submitted to ACM

12Nicolas Nicolaou, Viveck Cadambe, N. Prakash, Andria Trigeorgi, Kishori M. Konwar, Muriel Medard, and Nancy Lynch

ğ‘ğ‘ ğ‘’ğ‘â€² be the sequence returned by the read-config action. If |ğ‘ğ‘ ğ‘’ğ‘â€²| = |ğ‘ğ‘ ğ‘’ğ‘| then no new configuration is introduced, and
the read/write operation terminates; otherwise, ğ‘ sets ğ‘ğ‘ ğ‘’ğ‘ to ğ‘ğ‘ ğ‘’ğ‘â€² and repeats the two actions. Note, in an execution of
ARES, two consecutive read-config operations that return ğ‘ğ‘ ğ‘’ğ‘â€² and ğ‘ğ‘ ğ‘’ğ‘â€²â€² respectively must hold that ğ‘ğ‘ ğ‘’ğ‘â€² is a prefix of
ğ‘ğ‘ ğ‘’ğ‘â€²â€², and hence |ğ‘ğ‘ ğ‘’ğ‘â€²| = |ğ‘ğ‘ ğ‘’ğ‘â€²â€²| only if ğ‘ğ‘ ğ‘’ğ‘â€² = ğ‘ğ‘ ğ‘’ğ‘â€²â€². Finally, if ğœ‹ is a read operation the value with the highest tag
discovered is returned to the client.

Discussion ARES shares similarities with previous algorithms like RAMBO [24] and the framework in [40]. The

reconfiguration technique used in ARES ensures the prefix property on the configuration sequence (resembling a blockchain

data structure [35]) while the array structure in RAMBO allowed nodes to maintain an incomplete reconfiguration history.

On the other hand, the DAP usage, exploits a different viewpoint compared to [40], allowing implementations of atomic

read/write registers without relying on strong objects, like ranked registers [15].

5 IMPLEMENTATION OF THE DAPS

In this section, we present an implementation of the DAPs, that satisfies the properties in Property 1, for a configuration ğ‘,
with ğ‘› servers using a [ğ‘›, ğ‘˜] MDS coding scheme for storage. We implement an instance of the algorithm in a configuration
of ğ‘› server processes. We store each coded element ğ‘ğ‘– , corresponding to an object at server ğ‘ ğ‘– , where ğ‘– = 1, Â· Â· Â· , ğ‘›. The
implementations of DAP primitives used in ARES are shown in Alg. 5, and the serversâ€™ responses in Alg. 6.

Algorithm 5 DAP implementation for ARES.

at each process ğ‘ğ‘– âˆˆ I

2: procedure c.get-tag()

4:

6:

send (QUERY-TAG) to each ğ‘  âˆˆ ğ‘.ğ‘†ğ‘’ğ‘Ÿ ğ‘£ğ‘’ğ‘Ÿğ‘ 
until ğ‘ğ‘– receives âŸ¨ğ‘¡ğ‘  âŸ© from (cid:6) ğ‘›+ğ‘˜
2
ğ‘¡ğ‘šğ‘ğ‘¥ â† max( {ğ‘¡ğ‘  : received ğ‘¡ğ‘  from ğ‘  })
return ğ‘¡ğ‘šğ‘ğ‘¥
end procedure

(cid:7) servers in ğ‘.ğ‘†ğ‘’ğ‘Ÿ ğ‘£ğ‘’ğ‘Ÿğ‘ 

8: procedure c.get-data()

send (QUERY-LIST) to each ğ‘  âˆˆ ğ‘.ğ‘†ğ‘’ğ‘Ÿ ğ‘£ğ‘’ğ‘Ÿğ‘ 
until ğ‘ğ‘– receives ğ¿ğ‘–ğ‘ ğ‘¡ğ‘  from each server ğ‘  âˆˆ Sğ‘” s.t. |Sğ‘” | =

10:

(cid:7) and Sğ‘” âŠ‚ ğ‘.ğ‘†ğ‘’ğ‘Ÿ ğ‘£ğ‘’ğ‘Ÿğ‘ 

(cid:6) ğ‘›+ğ‘˜
2
ğ‘‡ ğ‘ğ‘”ğ‘  â‰¥ğ‘˜

âˆ— = set of tags that appears in ğ‘˜ lists

12:

14:

16:

ğ‘‘ğ‘’ğ‘ = set of tags that appears in ğ‘˜ lists with values

ğ‘‡ ğ‘ğ‘”ğ‘  â‰¥ğ‘˜
ğ‘šğ‘ğ‘¥ â† maxğ‘‡ ğ‘ğ‘”ğ‘  â‰¥ğ‘˜
ğ‘¡ âˆ—
âˆ—
ğ‘šğ‘ğ‘¥ â† maxğ‘‡ ğ‘ğ‘”ğ‘  â‰¥ğ‘˜
ğ‘¡ğ‘‘ğ‘’ğ‘
ğ‘‘ğ‘’ğ‘
if ğ‘¡ğ‘‘ğ‘’ğ‘
ğ‘šğ‘ğ‘¥ = ğ‘¡ âˆ—
ğ‘šğ‘ğ‘¥ then
ğ‘£ â† decode value for ğ‘¡ğ‘‘ğ‘’ğ‘
ğ‘šğ‘ğ‘¥

return âŸ¨ğ‘¡ğ‘‘ğ‘’ğ‘
18: end procedure

ğ‘šğ‘ğ‘¥ , ğ‘£ âŸ©

procedure c.put-data(âŸ¨ğœ, ğ‘£ âŸ©))

20:

22:

ğ‘ğ‘œğ‘‘ğ‘’-ğ‘’ğ‘™ğ‘’ğ‘šğ‘  = [ (ğœ, ğ‘’1), . . . , (ğœ, ğ‘’ğ‘›) ], ğ‘’ğ‘– = Î¦ğ‘– (ğ‘£)
send (PUT-DATA, âŸ¨ğœ, ğ‘’ğ‘– âŸ©) to each ğ‘ ğ‘– âˆˆ ğ‘.ğ‘†ğ‘’ğ‘Ÿ ğ‘£ğ‘’ğ‘Ÿğ‘ 
until ğ‘ğ‘– receives ACK from (cid:6) ğ‘›+ğ‘˜
2

(cid:7) servers in ğ‘.ğ‘†ğ‘’ğ‘Ÿ ğ‘£ğ‘’ğ‘Ÿğ‘ 

end procedure

Each server ğ‘ ğ‘– stores one state variable, ğ¿ğ‘–ğ‘ ğ‘¡, which is a set of up to (ğ›¿ + 1) (tag, coded-element) pairs. Initially the set

at ğ‘ ğ‘– contains a single element, ğ¿ğ‘–ğ‘ ğ‘¡ = {(ğ‘¡0, Î¦ğ‘– (ğ‘£0)}. Below we describe the implementation of the DAPs.

tags in their ğ¿ğ‘–ğ‘ ğ‘¡ğ‘ , and awaits responses from

ğ‘.get-tag(): A client, during the execution of a ğ‘.get-tag() primitive, queries all the servers in ğ‘.ğ‘†ğ‘’ğ‘Ÿğ‘£ğ‘’ğ‘Ÿğ‘  for the highest
servers. A server upon receiving the GET-TAG request, responds to

(cid:108) ğ‘›+ğ‘˜
2
the client with the highest tag, as ğœğ‘šğ‘ğ‘¥ â‰¡ max(ğ‘¡,ğ‘) âˆˆğ¿ğ‘–ğ‘ ğ‘¡ ğ‘¡. Once the client receives the tags from
the highest tag ğ‘¡ and returns it .

(cid:109)

(cid:108) ğ‘›+ğ‘˜
2

servers, it selects

(cid:109)

ğ‘.put-data(âŸ¨ğ‘¡ğ‘¤, ğ‘£âŸ©): During the execution of the primitive ğ‘.put-data(âŸ¨ğ‘¡ğ‘¤, ğ‘£âŸ©), a client sends the pair (ğ‘¡ğ‘¤, Î¦ğ‘– (ğ‘£)) to
each server ğ‘ ğ‘– âˆˆ ğ‘.ğ‘†ğ‘’ğ‘Ÿğ‘£ğ‘’ğ‘Ÿğ‘ . When a server ğ‘ ğ‘– receives a message (PUT-DATA, ğ‘¡ğ‘¤, ğ‘ğ‘– ) , it adds the pair in its local ğ¿ğ‘–ğ‘ ğ‘¡, trims
the pairs with the smallest tags exceeding the length (ğ›¿ + 1) of the ğ¿ğ‘–ğ‘ ğ‘¡ , and replies with an ack to the client. In particular,
ğ‘ ğ‘– replaces the coded-elements of the older tags with âŠ¥, and maintains only the coded-elements associated with the (ğ›¿ + 1)
Manuscript submitted to ACM

ARES: Adaptive, Reconfigurable, Erasure coded, Atomic Storage

13

Algorithm 6 The response protocols at any server ğ‘ ğ‘– âˆˆ S in ARES for client requests.

at each server ğ‘ ğ‘– âˆˆ S in configuration ğ‘ğ‘˜

end receive

2: State Variables:

ğ¿ğ‘–ğ‘ ğ‘¡ âŠ† T Ã— Cğ‘  , initially { (ğ‘¡0, Î¦ğ‘– (ğ‘£0)) }

4:

Upon receive (QUERY-TAG) ğ‘ ğ‘– , ğ‘ğ‘˜ from ğ‘

ğœğ‘šğ‘ğ‘¥ = max(ğ‘¡,ğ‘ ) âˆˆğ¿ğ‘–ğ‘ ğ‘¡ ğ‘¡
Send ğœğ‘šğ‘ğ‘¥ to ğ‘

6: end receive

Upon receive (QUERY-LIST) ğ‘ ğ‘– , ğ‘ğ‘˜ from ğ‘

Send ğ¿ğ‘–ğ‘ ğ‘¡ to ğ‘

8:

10:

12:

14:

16:

Upon receive (PUT-DATA, âŸ¨ğœ, ğ‘’ğ‘– âŸ©) ğ‘ ğ‘– , ğ‘ğ‘˜ from ğ‘

ğ¿ğ‘–ğ‘ ğ‘¡ â† ğ¿ğ‘–ğ‘ ğ‘¡ âˆª { âŸ¨ğœ, ğ‘’ğ‘– âŸ© }
if |ğ¿ğ‘–ğ‘ ğ‘¡ | > ğ›¿ + 1 then

ğœğ‘šğ‘–ğ‘› â† min{ğ‘¡ : âŸ¨ğ‘¡, âˆ—âŸ© âˆˆ ğ¿ğ‘–ğ‘ ğ‘¡ }

/* remove the coded value and retain the tag */

ğ¿ğ‘–ğ‘ ğ‘¡ â† ğ¿ğ‘–ğ‘ ğ‘¡ \ { âŸ¨ğœ, ğ‘’ âŸ© : ğœ = ğœğ‘šğ‘–ğ‘› âˆ§ âŸ¨ğœ, ğ‘’ âŸ© âˆˆ ğ¿ğ‘–ğ‘ ğ‘¡ }
ğ¿ğ‘–ğ‘ ğ‘¡ â† ğ¿ğ‘–ğ‘ ğ‘¡ âˆª { (ğœğ‘šğ‘–ğ‘›, âŠ¥) }

Send ACK to ğ‘

18: end receive

highest tags in the ğ¿ğ‘–ğ‘ ğ‘¡ (see Line Alg. 6:16). The client completes the primitive operation after getting acks from
servers.

(cid:109)

(cid:108) ğ‘›+ğ‘˜
2

ğ‘.get-data(): A client, during the execution of a ğ‘.get-data() primitive, queries all the servers in ğ‘.ğ‘†ğ‘’ğ‘Ÿğ‘£ğ‘’ğ‘Ÿğ‘  for their
local variable ğ¿ğ‘–ğ‘ ğ‘¡, and awaits responses from
servers, it selects
the highest tag ğ‘¡, such that: (ğ‘–) its corresponding value ğ‘£ is decodable from the coded elements in the lists; and (ğ‘–ğ‘–) ğ‘¡ is the
highest tag seen from the responses of at least ğ‘˜ ğ¿ğ‘–ğ‘ ğ‘¡ğ‘  (see lines Alg. 5:11-14) and returns the pair (ğ‘¡, ğ‘£). Note that in the
case where anyone of the above conditions is not satisfied the corresponding read operation does not complete.

servers. Once the client receives ğ¿ğ‘–ğ‘ ğ‘¡ğ‘  from

(cid:108) ğ‘›+ğ‘˜
2

(cid:108) ğ‘›+ğ‘˜
2

(cid:109)

(cid:109)

5.1 Safety (Property 1) proof of the DAPs

Correctness. In this section we are concerned with only one configuration ğ‘, consisting of a set of servers ğ‘.ğ‘†ğ‘’ğ‘Ÿğ‘£ğ‘’ğ‘Ÿğ‘ . We
assume that at most ğ‘“ â‰¤ ğ‘›âˆ’ğ‘˜
servers from ğ‘.ğ‘†ğ‘’ğ‘Ÿğ‘£ğ‘’ğ‘Ÿğ‘  may crash. Lemma 2 states that the DAP implementation satisfies
2
the consistency properties Property 1 which will be used to imply the atomicity of the ARES algorithm.

THEOREM 2 (SAFETY). Let Î  a set of complete DAP operations of Algorithm 5 in a configuration ğ‘ âˆˆ C, c.get-tag,

c.get-data and c.put-data, of an execution ğœ‰. Then, every pair of operations ğœ™, ğœ‹ âˆˆ Î  satisfy Property 1.

PROOF. As mentioned above we are concerned with only configuration ğ‘, and therefore, in our proofs it suffices to
examine only one configuration. Let ğœ‰ be some execution of ARES, then we consider two cases for ğœ‹ for proving property
ğ¶1: ğœ‹ is a get-tag, or ğœ‹ is a get-data primitive.

(cid:109)

(cid:108) ğ‘›+ğ‘˜
2

servers that responds to ğ‘ğœ™ , during ğœ™. Denote by ğ‘†ğœ‹ the set of

Case (ğ‘): ğœ™ is ğ‘.put-data(âŸ¨ğœğœ™, ğ‘£ğœ™ âŸ©) and ğœ‹ is a ğ‘.get-tag() returns ğœğœ‹ âˆˆ T . Let ğ‘ğœ™ and ğ‘ğœ‹ denote the clients that invokes
(cid:109)
(cid:108) ğ‘›+ğ‘˜
2

ğœ™ and ğœ‹ in ğœ‰. Let ğ‘†ğœ™ âŠ‚ S denote the set of
servers that responds to ğ‘ğœ‹ , during ğœ‹. Let ğ‘‡1 be a point in execution ğœ‰ after the completion of ğœ™ and before the invocation
of ğœ‹. Because ğœ‹ is invoked after ğ‘‡1, therefore, at ğ‘‡1 each of the servers in ğ‘†ğœ™ contains ğ‘¡ğœ™ in its ğ¿ğ‘–ğ‘ ğ‘¡ variable. Note that,
once a tag is added to ğ¿ğ‘–ğ‘ ğ‘¡, it is never removed. Therefore, during ğœ‹, any server in ğ‘†ğœ™ âˆ© ğ‘†ğœ‹ responds with ğ¿ğ‘–ğ‘ ğ‘¡ containing
ğ‘¡ğœ™ to ğ‘ğœ‹ . Note that since |ğ‘†ğœ™ | = |ğ‘†ğœ‹ | =
ğ‘šğ‘ğ‘¥ at ğ‘ğœ‹ , during ğœ‹ is at least as large as
ğ‘¡ğœ™ , i.e., ğ‘¡ğœ‹ â‰¥ ğ‘¡ğœ™ . Therefore, it suffices to prove our claim with respect to the tags and the decodability of its corresponding
value.

implies |ğ‘†ğœ™ âˆ© ğ‘†ğœ‹ | â‰¥ ğ‘˜, and hence ğ‘¡ğ‘‘ğ‘’ğ‘

(cid:108) ğ‘›+ğ‘˜
2

(cid:109)

Case (ğ‘): ğœ™ is ğ‘.put-data(âŸ¨ğœğœ™, ğ‘£ğœ™ âŸ©) and ğœ‹ is a ğ‘.get-data() returns âŸ¨ğœğœ‹ , ğ‘£ğœ‹ âŸ© âˆˆ T Ã— V. As above, let ğ‘ğœ™ and ğ‘ğœ‹ be
the clients that invokes ğœ™ and ğœ‹. Let ğ‘†ğœ™ and ğ‘†ğœ‹ be the set of servers that responds to ğ‘ğœ™ and ğ‘ğœ‹ , respectively. Arguing
Manuscript submitted to ACM

14Nicolas Nicolaou, Viveck Cadambe, N. Prakash, Andria Trigeorgi, Kishori M. Konwar, Muriel Medard, and Nancy Lynch

as above, |ğ‘†ğœ™ âˆ© ğ‘†ğœ‹ | â‰¥ ğ‘˜ and every server in ğ‘†ğœ™ âˆ© ğ‘†ğœ‹ sends ğ‘¡ğœ™ in response to ğ‘ğœ™ , during ğœ‹, in their ğ¿ğ‘–ğ‘ ğ‘¡â€™s and hence
ğ‘¡ğœ™ âˆˆ ğ‘‡ ğ‘ğ‘”ğ‘  â‰¥ğ‘˜
. Now, because ğœ‹ completes in ğœ‰, hence we have ğ‘¡ âˆ—
ğ‘‘ğ‘’ğ‘ so
âˆ—
ğ‘¡ğœ‹ â‰¥ maxğ‘‡ ğ‘ğ‘”ğ‘  â‰¥ğ‘˜
ğ‘‘ğ‘’ğ‘ = maxğ‘‡ ğ‘ğ‘”ğ‘  â‰¥ğ‘˜
âˆ— â‰¥ ğ‘¡ğœ™ . Note that each tag is always associated with its corresponding value ğ‘£ğœ‹ , or the
corresponding coded elements Î¦ğ‘  (ğ‘£ğœ‹ ) for ğ‘  âˆˆ S.

ğ‘šğ‘ğ‘¥ . Note that maxğ‘‡ ğ‘ğ‘”ğ‘  â‰¥ğ‘˜

âˆ— â‰¥ maxğ‘‡ ğ‘ğ‘”ğ‘  â‰¥ğ‘˜

ğ‘šğ‘ğ‘¥ = ğ‘¡ğ‘‘ğ‘’ğ‘

Next, we prove the ğ¶2 property of DAP for the ARES algorithm. Note that the initial values of the ğ¿ğ‘–ğ‘ ğ‘¡ variable in
each servers ğ‘  in S is {(ğ‘¡0, Î¦ğ‘  (ğ‘£ğœ‹ ))}. Moreover, from an inspection of the steps of the algorithm, new tags in the ğ¿ğ‘–ğ‘ ğ‘¡
variable of any servers of any servers is introduced via put-data operation. Since ğ‘¡ğœ‹ is returned by a get-tag or get-data
operation then it must be that either ğ‘¡ğœ‹ = ğ‘¡0 or ğ‘¡ğœ‹ > ğ‘¡0. In the case where ğ‘¡ğœ‹ = ğ‘¡0 then we have nothing to prove. If ğ‘¡ğœ‹ > ğ‘¡0
then there must be a put-data(ğ‘¡ğœ‹ , ğ‘£ğœ‹ ) operation ğœ™. To show that for every ğœ‹ it cannot be that ğœ™ completes before ğœ‹, we
adopt by a contradiction. Suppose for every ğœ‹, ğœ™ completes before ğœ‹ begins, then clearly ğ‘¡ğœ‹ cannot be returned ğœ™, a
â–¡
contradiction.

Liveness. To reason about the liveness of the proposed DAPs, we define a concurrency parameter ğ›¿ which captures all
the put-data operations that overlap with the get-data, until the time the client has all data needed to attempt decoding
a value. However, we ignore those put-data operations that might have started in the past, and never completed yet, if
their tags are less than that of any put-data that completed before the get-data started. This allows us to ignore put-data
operations due to failed clients, while counting concurrency, as long as the failed put-data operations are followed by
a successful put-data that completed before the get-data started. In order to define the amount of concurrency in our
specific implementation of the DAPs presented in this section the following definition captures the put-data operations
that overlap with the get-data, until the client has all data required to decode the value.

DEFINITION 3 (VALID get-data OPERATIONS). A get-data operation ğœ‹ from a process ğ‘ is valid if ğ‘ does not crash

until the reception of

responses during the get-data phase.

(cid:109)

(cid:108) ğ‘›+ğ‘˜
2

DEFINITION 4 (put-data CONCURRENT WITH A VALID get-data). Consider a valid get-data operation ğœ‹ from a
process ğ‘. Let ğ‘‡1 denote the point of initiation of ğœ‹. For ğœ‹, let ğ‘‡2 denote the earliest point of time during the execution
responses. Consider the set Î£ = {ğœ™ : ğœ™ is any put-data operation that completes before
when ğ‘ receives all the
ğœ‹ is initiated}, and let ğœ™ âˆ— = arg maxğœ™ âˆˆÎ£ ğ‘¡ğ‘ğ‘”(ğœ™). Next, consider the set Î› = {ğœ† : ğœ† is any put-data operation that starts
before ğ‘‡2 such that ğ‘¡ğ‘ğ‘”(ğœ†) > ğ‘¡ğ‘ğ‘”(ğœ™ âˆ—)}. We define the number of put-data concurrent with the valid get-data ğœ‹ to be the
cardinality of the set Î›.

(cid:108) ğ‘›+ğ‘˜
2

(cid:109)

Termination (and hence liveness) of the DAPs is guaranteed in an execution ğœ‰, provided that a process no more than
ğ‘“ servers in ğ‘.ğ‘†ğ‘’ğ‘Ÿğ‘£ğ‘’ğ‘Ÿğ‘  crash, and no more that ğ›¿ put-data may be concurrent at any point in ğœ‰. If the failure model is
satisfied, then any operation invoked by a non-faulty client will collect the necessary replies independently of the progress
of any other client process in the system. Preserving ğ›¿ on the other hand, ensures that any operation will be able to decode
a written value. These are captured in the following theorem:

THEOREM 5 (LIVENESS). Let ğœ‰ be well-formed and fair execution of DAPs, with an [ğ‘›, ğ‘˜] MDS code, where ğ‘› is
the number of servers out of which no more than ğ‘›âˆ’ğ‘˜
2 may crash, and ğ›¿ be the maximum number of put-data operations
concurrent with any valid get-data operation. Then any get-data and put-data operation ğœ‹ invoked by a process ğ‘
terminates in ğœ‰ if ğ‘ does not crash between the invocation and response steps of ğœ‹.

PROOF. Note that in the read and write operation the get-tag and put-data operations initiated by any non-faulty client
always complete. Therefore, the liveness property with respect to any write operation is clear because it uses only get-tag
Manuscript submitted to ACM

ARES: Adaptive, Reconfigurable, Erasure coded, Atomic Storage

15

(cid:109)

(cid:108) ğ‘›+ğ‘˜
2

(cid:109)

(cid:108) ğ‘›+ğ‘˜
2

and put-data operations of the DAP. So, we focus on proving the liveness property of any read operation ğœ‹, specifically,
the get-data operation completes. Let ğœ‰ be and execution of ARES and let ğ‘ğœ” and ğ‘ğœ‹ be the clients that invokes the write
operation ğœ” and read operation ğœ‹, respectively.

Let ğ‘†ğœ” be the set of

servers that responds to ğ‘ğœ” , in the put-data operations, in ğœ”. Let ğ‘†ğœ‹ be the set of

servers that responds to ğ‘ğœ‹ during the get-data step of ğœ‹. Note that in ğœ‰ at the point execution ğ‘‡1, just before the execution
of ğœ‹, none of the write operations in Î› is complete. Observe that, by algorithm design, the coded-elements corresponding
to ğ‘¡ğœ” are garbage-collected from the ğ¿ğ‘–ğ‘ ğ‘¡ variable of a server only if more than ğ›¿ higher tags are introduced by subsequent
writes into the server. Since the number of concurrent writes |Î›|, s.t. ğ›¿ > |Î›| the corresponding value of tag ğ‘¡ğœ” is not
garbage collected in ğœ‰, at least until execution point ğ‘‡2 in any of the servers in ğ‘†ğœ” .

Therefore, during the execution fragment between the execution points ğ‘‡1 and ğ‘‡2 of the execution ğœ‰, the tag and
coded-element pair is present in the ğ¿ğ‘–ğ‘ ğ‘¡ variable of every in ğ‘†ğœ” that is active. As a result, the tag and coded-element pairs,
(cid:109)
(ğ‘¡ğœ”, Î¦ğ‘  (ğ‘£ğœ” )) exists in the ğ¿ğ‘–ğ‘ ğ‘¡ received from any ğ‘  âˆˆ ğ‘†ğœ” âˆ© ğ‘†ğœ‹ during operation ğœ‹. Note that since |ğ‘†ğœ” | = |ğ‘†ğœ‹ | =
hence |ğ‘†ğœ” âˆ© ğ‘†ğœ‹ | â‰¥ ğ‘˜ and hence ğ‘¡ğœ” âˆˆ ğ‘‡ ğ‘ğ‘”ğ‘  â‰¥ğ‘˜
ğ‘‘ğ‘’ğ‘ , the set of decode-able tag, i.e., the value ğ‘£ğœ” can be decoded by ğ‘ğœ‹ in
ğœ‹, which demonstrates that ğ‘‡ ğ‘ğ‘”ğ‘  â‰¥ğ‘˜
ğ‘šğ‘ğ‘¥ via a contradiction: we assume
ğ‘‘ğ‘’ğ‘ . Now, consider any tag ğ‘¡, which exists due to our assumption, such that, ğ‘¡ âˆˆ ğ‘‡ ğ‘ğ‘”ğ‘  â‰¥ğ‘˜
âˆ— > maxğ‘‡ ğ‘ğ‘”ğ‘  â‰¥ğ‘˜
maxğ‘‡ ğ‘ğ‘”ğ‘  â‰¥ğ‘˜
,
ğ‘¡ âˆ‰ ğ‘‡ ğ‘ğ‘”ğ‘  â‰¥ğ‘˜
ğ‘‘ğ‘’ğ‘ and ğ‘¡ > ğ‘¡ğ‘‘ğ‘’ğ‘
ğ‘šğ‘ğ‘¥ in their ğ¿ğ‘–ğ‘ ğ‘¡ variables to ğ‘ğœ‹ .
Note that since ğ‘˜ > ğ‘›/3 hence |ğ‘†ğœ” âˆ© ğ‘†ğ‘˜
ğœ‹ â‰  âˆ…. Then ğ‘¡ must be in some servers in ğ‘†ğœ” at
ğ‘‡2 and since ğ‘¡ > ğ‘¡ğ‘‘ğ‘’ğ‘
ğ‘šğ‘ğ‘¥ â‰¥ ğ‘¡ğœ” . Now since |Î›| < ğ›¿ hence (ğ‘¡, âŠ¥) cannot be in any server at ğ‘‡2 because there are not enough
concurrent write operations (i.e., writes in Î›) to garbage-collect the coded-elements corresponding to tag ğ‘¡, which also
holds for tag ğ‘¡ âˆ—
â–¡
ğ‘‘ğ‘’ğ‘ , a contradiction.

ğœ‹ âŠ‚ ğ‘† be any subset of ğ‘˜ servers that responds with ğ‘¡ âˆ—

ğ‘‘ğ‘’ğ‘ â‰  âˆ…. Next we want to argue that ğ‘¡ âˆ—

ğ‘šğ‘ğ‘¥ . In that case, ğ‘¡ must be in ğ‘‡ ğ‘ğ‘” â‰¥ğ‘˜

(cid:7) â‰¥ 1, i.e., ğ‘†ğœ” âˆ© ğ‘†ğ‘˜

ğ‘šğ‘ğ‘¥ . Let ğ‘†ğ‘˜

ğ‘šğ‘ğ‘¥ = ğ‘¡ğ‘‘ğ‘’ğ‘

+ (cid:6) ğ‘›+1
3

(cid:108) ğ‘›+ğ‘˜
2

(cid:108) ğ‘›+ğ‘˜
2

ğœ‹ | â‰¥

(cid:109)

âˆ—

6 CORRECTNESS OF ARES

In this section, we prove that ARES correctly implements an atomic, read/write, shared storage service. The correctness

of ARES highly depends on the way the configuration sequence is constructed at each client process. Also, atomicity is
ensured if the DAP implementation in each configuration ğ‘ğ‘– satisfies Property 1. Thus, we begin by showing some critical
properties preserved by the reconfiguration service proposed in ARES in subsection 6.1, and then we proof the correctness

of ARES in subsection 6.2 when those properties hold and the DAPs used in each configuration satisfy Property 1.

We proceed by first introducing some definitions and notation, that we use in the proofs that follow.

Notations and definitions. For a server ğ‘ , we use the notation ğ‘ .ğ‘£ğ‘ğ‘Ÿ |ğœ to refer to the value of the state variable ğ‘£ğ‘ğ‘Ÿ , in
ğ‘ , at a state ğœ of an execution ğœ‰. If server ğ‘  crashes at a state ğœğ‘“ in an execution ğœ‰ then ğ‘ .ğ‘£ğ‘ğ‘Ÿ |ğœ â‰œ ğ‘ .ğ‘£ğ‘ğ‘Ÿ |ğœğ‘“ for any state
variable ğ‘£ğ‘ğ‘Ÿ and for any state ğœ that appears after ğœğ‘“ in ğœ‰.

We define as the tag of a configuration ğ‘ the smallest tag among the maximum tags found in each quorum of ğ‘. This
is essentially the smallest tag that an operation may witness when receiving replies from a single quorum in ğ‘. More
formally:

DEFINITION 6 (TAG OF A CONFIGURATION). Let ğ‘ âˆˆ C be a configuration, ğœ be a state in some execution ğœ‰ then we
define the tag of ğ‘ at state ğœ as ğ‘¡ğ‘ğ‘”(ğ‘)|ğœ â‰œ minğ‘„ âˆˆğ‘.ğ‘„ğ‘¢ğ‘œğ‘Ÿğ‘¢ğ‘šğ‘  maxğ‘  âˆˆğ‘„ (ğ‘ .ğ‘¡ğ‘ğ‘”|ğœ ). We drop the suffix |ğœ , and simply denote
as ğ‘¡ğ‘ğ‘”(ğ‘), when the state is clear from the context.

Next we provide the notation to express the configuration sequence witnessed by a process ğ‘ in a state ğœ (as ğ‘.ğ‘ğ‘ ğ‘’ğ‘|ğœ ),

ğ‘
ğ‘
the last finalized configuration in that sequence (as ğœ‡ (c
ğœ )), and the length of that sequence (as ğœˆ (c
ğœ )). More formally:

Manuscript submitted to ACM

16Nicolas Nicolaou, Viveck Cadambe, N. Prakash, Andria Trigeorgi, Kishori M. Konwar, Muriel Medard, and Nancy Lynch

ğ‘
ğœ for ğ‘.ğ‘ğ‘ ğ‘’ğ‘|ğœ , i.e., the
DEFINITION 7. Let ğœ be any point in an execution of ARES and suppose we use the notation c
ğ‘
ğ‘
ğ‘
ğ‘
ğœ ) â‰œ |c
ğœ ) â‰œ max{ğ‘– : c
ğ‘ğ‘ ğ‘’ğ‘ variable at process ğ‘ at the state ğœ. Then we define as ğœ‡ (c
ğœ [ğ‘–].ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘¢ğ‘  = ğ¹ } and ğœˆ (c
ğœ |, where
ğ‘
ğ‘
ğœ | is the length of the configuration vector c
|c
ğœ .

Last, we define the prefix operation on two configuration sequences.

DEFINITION 8 (PREFIX ORDER). Let x and y be any two configuration sequences. We say that x is a prefix of y,

denoted by x âª¯ğ‘ y, if x[ ğ‘—].ğ‘ ğ‘“ ğ‘” = y[ ğ‘—].ğ‘ ğ‘“ ğ‘”, for all ğ‘— such that x[ ğ‘—] â‰  âŠ¥.

6.1 Reconfiguration Protocol Properties

In this section we analyze the properties that we can achieve through our reconfiguration algorithm. In high-level, we do

show that the following properties are preserved:

i configuration uniqueness: the configuration sequences in any two processes have identical configuration at any

place ğ‘–,

ii sequence prefix: the configuration sequence witnessed by an operation is a prefix of the sequence witnessed by

any succeeding operation, and

iii sequence progress: if the configuration with index ğ‘– is finalized during an operation, then a configuration ğ‘—, for

ğ‘— â‰¥ ğ‘–, will be finalized for a succeeding operation.

The first lemma shows that any two configuration sequences have the same configuration identifiers in the same

indexes.

LEMMA 9. For any reconfigurer ğ‘Ÿ that invokes an reconfig(ğ‘) action in an execution ğœ‰ of the algorithm, If ğ‘Ÿ chooses
to install ğ‘ in index ğ‘˜ of its local ğ‘Ÿ .ğ‘ğ‘ ğ‘’ğ‘ vector, then ğ‘Ÿ invokes the ğ¶ğ‘œğ‘›ğ‘  [ğ‘˜ âˆ’ 1].ğ‘ğ‘Ÿğ‘œğ‘ğ‘œğ‘ ğ‘’ (ğ‘) instance over configuration
ğ‘Ÿ .ğ‘ğ‘ ğ‘’ğ‘ [ğ‘˜ âˆ’ 1].ğ‘ ğ‘“ ğ‘”.

PROOF. It follows directly from the algorithm.

â–¡

LEMMA 10. If a server ğ‘  sets ğ‘ .ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶ to âŸ¨ğ‘, ğ¹ âŸ© at some state ğœ in an execution ğœ‰ of the algorithm, then ğ‘ .ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶ = âŸ¨ğ‘, ğ¹ âŸ©

for any state ğœ â€² that appears after ğœ in ğœ‰.

PROOF. Notice that a server ğ‘  updates its ğ‘ .ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶ variable for some specific configuration ğ‘ğ‘˜ in a state ğœ only when it
receives a WRITE-CONF message. This is either the first WRITE-CONF message received at ğ‘  for ğ‘ğ‘˜ (and thus ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶ = âŠ¥),
or ğ‘ .ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶ = âŸ¨âˆ—, ğ‘ƒâŸ© and the message received contains a tuple âŸ¨ğ‘, ğ¹ âŸ©. Once the tuple becomes equal to âŸ¨ğ‘, ğ¹ âŸ© then ğ‘  does
not satisfy the update condition for ğ‘ğ‘˜ , and hence in any state ğœ â€² after ğœ it does not change âŸ¨ğ‘, ğ¹ âŸ©.
â–¡

LEMMA 11 (CONFIGURATION UNIQUENESS). For any processes ğ‘, ğ‘ âˆˆ I and any states ğœ1, ğœ2 in an execution ğœ‰, it

ğ‘
must hold that c
ğœ1

ğ‘
[ğ‘–].ğ‘ ğ‘“ ğ‘” = c
ğœ2

ğ‘
[ğ‘–].ğ‘ ğ‘“ ğ‘”, âˆ€ğ‘– s.t. c
ğœ1

ğ‘
[ğ‘–].ğ‘ ğ‘“ ğ‘”, c
ğœ2

[ğ‘–].ğ‘ ğ‘“ ğ‘” â‰  âŠ¥.

ğ‘
PROOF. The lemma holds trivially for c
ğœ1

ğ‘
[0].ğ‘ ğ‘“ ğ‘” = c
ğœ2

[0].ğ‘ ğ‘“ ğ‘” = ğ‘0. So in the rest of the proof we focus in the case

where ğ‘– > 0. Let us assume w.l.o.g. that ğœ1 appears before ğœ2 in ğœ‰.

According to our algorithm a process ğ‘ sets ğ‘.ğ‘ğ‘ ğ‘’ğ‘ [ğ‘–].ğ‘ ğ‘“ ğ‘” to a configuration identifier ğ‘ in two cases: (i) either it
received ğ‘ as the result of the consensus instance in configuration ğ‘.ğ‘ğ‘ ğ‘’ğ‘ [ğ‘– âˆ’ 1].ğ‘ ğ‘“ ğ‘”, or (ii) ğ‘ receives ğ‘ .ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶.ğ‘ ğ‘“ ğ‘” = ğ‘
from a server ğ‘  âˆˆ ğ‘.ğ‘ğ‘ ğ‘’ğ‘ [ğ‘– âˆ’ 1].ğ‘ ğ‘“ ğ‘”.ğ‘†ğ‘’ğ‘Ÿğ‘£ğ‘’ğ‘Ÿğ‘ . Note here that (i) is possible only when ğ‘ is a reconfigurer and attempts
to install a new configuration. On the other hand (ii) may be executed by any process in any operation that invokes the

read-config action. We are going to proof this lemma by induction on the configuration index.
Manuscript submitted to ACM

ARES: Adaptive, Reconfigurable, Erasure coded, Atomic Storage

17

Base case: The base case of the lemma is when ğ‘– = 1. Let us first assume that ğ‘ and ğ‘ receive ğ‘ğ‘ and ğ‘ğ‘, as the
result of the consensus instance at ğ‘.ğ‘ğ‘ ğ‘’ğ‘ [0].ğ‘ ğ‘“ ğ‘” and ğ‘.ğ‘ğ‘ ğ‘’ğ‘ [0].ğ‘ ğ‘“ ğ‘” respectively. By Lemma 9, since both processes
want to install a configuration in ğ‘– = 1, then they have to run ğ¶ğ‘œğ‘›ğ‘  [0] instance over the configuration stored in their
local ğ‘ğ‘ ğ‘’ğ‘ [0].ğ‘ ğ‘“ ğ‘” variable. Since ğ‘.ğ‘ğ‘ ğ‘’ğ‘ [0].ğ‘ ğ‘“ ğ‘” = ğ‘.ğ‘ğ‘ ğ‘’ğ‘[0].ğ‘ ğ‘“ ğ‘” = ğ‘0 then both ğ¶ğ‘œğ‘›ğ‘  [0] instances run over the same
configuration ğ‘0 and thus by the aggreement property the have to decide the same value, say ğ‘1. Hence ğ‘ğ‘ = ğ‘ğ‘ = ğ‘1 and
ğ‘.ğ‘ğ‘ ğ‘’ğ‘ [1].ğ‘ ğ‘“ ğ‘” = ğ‘.ğ‘ğ‘ ğ‘’ğ‘ [1].ğ‘ ğ‘“ ğ‘” = ğ‘1.

Let us examine the case now where ğ‘ or ğ‘ assign a configuration ğ‘ they received from some server ğ‘  âˆˆ ğ‘0.ğ‘†ğ‘’ğ‘Ÿğ‘£ğ‘’ğ‘Ÿğ‘ .
According to the algorithm only the configuration that has been decided by the consensus instance on ğ‘0 is propagated
to the servers in ğ‘0.ğ‘†ğ‘’ğ‘Ÿğ‘£ğ‘’ğ‘Ÿğ‘ . If ğ‘1 is the decided configuration, then âˆ€ğ‘  âˆˆ ğ‘0.ğ‘†ğ‘’ğ‘Ÿğ‘£ğ‘’ğ‘Ÿğ‘  such that ğ‘ .ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶ (ğ‘0) â‰  âŠ¥, it
holds that ğ‘ .ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶ (ğ¶0) = âŸ¨ğ‘1, âˆ—âŸ©. So if ğ‘ or ğ‘ set ğ‘.ğ‘ğ‘ ğ‘’ğ‘ [1].ğ‘ ğ‘“ ğ‘” or ğ‘.ğ‘ğ‘ ğ‘’ğ‘[1].ğ‘ ğ‘“ ğ‘” to some received configuration, then
ğ‘.ğ‘ğ‘ ğ‘’ğ‘ [1].ğ‘ ğ‘“ ğ‘” = ğ‘.ğ‘ğ‘ ğ‘’ğ‘ [1].ğ‘ ğ‘“ ğ‘” = ğ‘1 in this case as well.

ğ‘
[ğ‘˜] = c
ğœ2

[ğ‘˜] for some ğ‘˜, ğ‘˜ â‰¥ 1.

ğ‘
Hypothesis: We assume that c
ğœ1
Induction Step: We need to show that the lemma holds for ğ‘– = ğ‘˜ + 1. If both processes retrieve ğ‘.ğ‘ğ‘ ğ‘’ğ‘[ğ‘˜ + 1].ğ‘ ğ‘“ ğ‘”
and ğ‘.ğ‘ğ‘ ğ‘’ğ‘ [ğ‘˜ + 1].ğ‘ ğ‘“ ğ‘” through consensus, then both ğ‘ and ğ‘ run consensus over the previous configuration. Since
ğ‘
[ğ‘˜] then both process will receive the same decided value, say ğ‘ğ‘˜+1, and
according to our hypothesis c
ğœ1
hence ğ‘.ğ‘ğ‘ ğ‘’ğ‘[ğ‘˜ + 1].ğ‘ ğ‘“ ğ‘” = ğ‘.ğ‘ğ‘ ğ‘’ğ‘ [ğ‘˜ + 1].ğ‘ ğ‘“ ğ‘” = ğ‘ğ‘˜+1. Similar to the base case, a server in ğ‘ğ‘˜ .ğ‘†ğ‘’ğ‘Ÿğ‘£ğ‘’ğ‘Ÿğ‘  only receives the
configuration ğ‘ğ‘˜+1 decided by the consensus instance run over ğ‘ğ‘˜ . So processes ğ‘ and ğ‘ can only receive ğ‘ğ‘˜+1 from some
server in ğ‘ğ‘˜ .ğ‘†ğ‘’ğ‘Ÿğ‘£ğ‘’ğ‘Ÿğ‘  so they can only assign ğ‘.ğ‘ğ‘ ğ‘’ğ‘[ğ‘˜ + 1].ğ‘ ğ‘“ ğ‘” = ğ‘.ğ‘ğ‘ ğ‘’ğ‘[ğ‘˜ + 1].ğ‘ ğ‘“ ğ‘” = ğ‘ğ‘˜+1 at Line A2:8. That completes
â–¡
the proof.

ğ‘
[ğ‘˜] = c
ğœ2

Lemma 11 showed that any two operations store the same configuration in any cell ğ‘˜ of their ğ‘ğ‘ ğ‘’ğ‘ variable. It is not
known however if the two processes discover the same number of configuration ids. In the following lemmas we will
show that if a process learns about a configuration in a cell ğ‘˜ then it also learns about all configuration ids for every index
ğ‘–, such that 0 â‰¤ ğ‘– â‰¤ ğ‘˜ âˆ’ 1.

ğ‘
LEMMA 12. In any execution ğœ‰ of the algorithm , If for any process ğ‘ âˆˆ I, c
ğœ [ğ‘–] â‰  âŠ¥ in some state ğœ in ğœ‰, then
ğ‘
ğœâ€² [ğ‘–] â‰  âŠ¥ in any state ğœ â€² that appears after ğœ in ğœ‰.
c

ğ‘
âˆ— [ğ‘–] either after the invocation of a consensus instance, or while executing the
PROOF. A value is assigned to c
read-config action. Since any configuration proposed for installation cannot be âŠ¥ (A2:7), and since there is at least one
configuration proposed in the consensus instance (the one from ğ‘), then by the validity of the consensus service the
ğ‘
ğ‘
decision will be a configuration ğ‘ â‰  âŠ¥. Thus, in this case c
âˆ— [ğ‘–] is
âˆ— [ğ‘–] cannot be âŠ¥. Also in the read-config procedure, c
ğ‘
ğœ [ğ‘–] â‰  âŠ¥ at state ğœ then it cannot become âŠ¥ in any
assigned to a value different than âŠ¥ according to Line A2:8. Hence, if c
state ğœ â€² after ğœ in execution ğœ‰.
â–¡

ğ‘
LEMMA 13. Let ğœ1 be some state in an execution ğœ‰ of the algorithm. Then for any process ğ‘, if ğ‘˜ = ğ‘šğ‘ğ‘¥ {ğ‘– : c
ğœ1

[ğ‘–] â‰  âŠ¥},

ğ‘
then c
ğœ1

[ ğ‘—] â‰  âŠ¥, for 0 â‰¤ ğ‘— < ğ‘˜.

ğ‘
[ğ‘˜] â‰  âŠ¥, then ğ‘ assigned ğ‘ğ‘˜ to c
ğœ1

ğ‘
PROOF. Let us assume to derive contradiction that there exists ğ‘— < ğ‘˜ such that c
ğœ1

[ ğ‘— + 1] â‰  âŠ¥.
ğ‘
[ğ‘˜], say ğ‘ğ‘˜ . Since
Consider first that ğ‘— = ğ‘˜ âˆ’ 1 and that ğœ1 is the state immediately after the assignment of a value to c
ğœ1
ğ‘
[ğ‘˜] in one of the following cases: (i) ğ‘ğ‘˜ was the result of the consensus instance,
c
ğœ1
or (ii) ğ‘ received ğ‘ğ‘˜ from a server during a read-config action. The first case is trivially impossible as according to
ğ‘
Lemma 9 ğ‘ decides for ğ‘˜ when it runs consensus over configuration c
[ğ‘˜ âˆ’ 1].ğ‘ ğ‘“ ğ‘”. Since this is equal to âŠ¥, then we
ğœ1
Manuscript submitted to ACM

ğ‘
[ ğ‘—] = âŠ¥ and c
ğœ1

18Nicolas Nicolaou, Viveck Cadambe, N. Prakash, Andria Trigeorgi, Kishori M. Konwar, Muriel Medard, and Nancy Lynch

ğ‘
assigned c
ğœ1

ğ‘
[ğ‘˜] = ğ‘ğ‘˜ in Line A1:8. The
cannot run consensus over a non-existent set of processes. In the second case, ğ‘ assigns c
ğœ1
ğ‘
value ğ‘ğ‘˜ was however obtained when ğ‘ invoked get-next-config on configuration c
[ğ‘˜ âˆ’ 1].ğ‘ ğ‘“ ğ‘”. In that action, ğ‘ sends
ğœ1
ğ‘
[ğ‘˜ âˆ’ 1].ğ‘ ğ‘“ ğ‘”.ğ‘†ğ‘’ğ‘Ÿğ‘£ğ‘’ğ‘Ÿğ‘  and waits until a quorum of servers replies. Since we
READ-CONFIG messages to the servers in c
ğœ1
[ğ‘˜] = ğ‘ğ‘˜ it means that get-next-config terminated at some state ğœ â€² before ğœ1 in ğœ‰, and thus: (a) a quorum
ğ‘
ğœâ€² [ğ‘˜ âˆ’ 1].ğ‘ ğ‘“ ğ‘”.ğ‘†ğ‘’ğ‘Ÿğ‘£ğ‘’ğ‘Ÿğ‘  replied, and (b) there exists a server ğ‘  among those that replied with ğ‘ğ‘˜ . According
of servers in c
ğ‘
[ğ‘˜ âˆ’ 1] = âŠ¥ at ğœ1. So if state ğœ â€² is before ğœ1 in ğœ‰, then by Lemma 12, it follows that
to our assumption however, c
ğœ1
ğ‘
ğœâ€² [ğ‘˜ âˆ’ 1] = âŠ¥. This however implies that ğ‘ communicated with an empty configuration, and thus no server replied to ğ‘.
c
This however contradicts the assumption that a server replied with ğ‘ğ‘˜ to ğ‘.

Since any process traverses the configuration sequence starting from the initial configuration ğ‘0, then with a simple
â–¡

ğ‘
induction and similar reasoning we can show that c
ğœ1

[ ğ‘—] â‰  âŠ¥, for 0 â‰¤ ğ‘— â‰¤ ğ‘˜ âˆ’ 1.

We can now move to an important lemma that shows that any read-config action returns an extension of the configura-
tion sequence returned by any previous read-config action. First, we show that the last finalized configuration observed by
any read-config action is at least as recent as the finalized configuration observed by any subsequent read-config action.

ğ‘
ğœ ) = ğ‘˜ for some process ğ‘, then for any element
LEMMA 14. If at a state ğœ of an execution ğœ‰ of the algorithm ğœ‡ (c

ğ‘
ğ‘
ğ‘
0 â‰¤ ğ‘— < ğ‘˜, âˆƒğ‘„ âˆˆ c
ğœ [ ğ‘— + 1].
ğœ [ ğ‘—].ğ‘ ğ‘“ ğ‘”) = c
ğœ [ ğ‘—].ğ‘ ğ‘“ ğ‘”.ğ‘„ğ‘¢ğ‘œğ‘Ÿğ‘¢ğ‘šğ‘  such that âˆ€ğ‘  âˆˆ ğ‘„, ğ‘ .ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶ (c

PROOF. This lemma follows directly from the algorithm. Notice that whenever a process assigns a value to an element

of its local configuration (Lines A1:8 and A2:17), it then propagates this value to a quorum of the previous configuration
ğ‘
ğœâ€² [ ğ‘—] in some state ğœ â€² in ğœ‰, then ğ‘ may assign a
(Lines A1:9 and A2:18). So if a process ğ‘ assigned ğ‘ ğ‘— to an element c
ğ‘
ğ‘
ğ‘
ğœâ€² [ ğ‘—]) occurs. During put-config action, ğ‘
ğœâ€² [ ğ‘— âˆ’ 1].ğ‘ ğ‘“ ğ‘”, c
ğœâ€²â€² [ ğ‘— + 1] only after put-config(c
value to the ğ‘— + 1 element of c
ğ‘
ğ‘
ğ‘
ğ‘
ğœâ€² [ ğ‘—], for
ğœ [ğ‘˜] â‰  âŠ¥, then ğ‘ propagated each c
ğœâ€² [ ğ‘— âˆ’ 1].ğ‘ ğ‘“ ğ‘”.ğ‘„ğ‘¢ğ‘œğ‘Ÿğ‘¢ğ‘šğ‘ . Hence, if c
ğœâ€² [ ğ‘—] in a quorum ğ‘„ âˆˆ c
propagates c
ğ‘
â–¡
0 < ğ‘— â‰¤ ğ‘˜ to a quorum of servers ğ‘„ âˆˆ c
ğœâ€² [ ğ‘— âˆ’ 1].ğ‘ ğ‘“ ğ‘”.ğ‘„ğ‘¢ğ‘œğ‘Ÿğ‘¢ğ‘šğ‘ . And this completes the proof.

LEMMA 15 (SEQUENCE PREFIX). Let ğœ‹1 and ğœ‹2 two completed read-config actions invoked by processes ğ‘1, ğ‘2 âˆˆ I
respectively, such that ğœ‹1 â†’ ğœ‹2 in an execution ğœ‰. Let ğœ1 be the state after the response step of ğœ‹1 and ğœ2 the state after
the response step of ğœ‹2. Then c

âª¯ğ‘ c

ğ‘1
ğœ1

ğ‘2
ğœ2 .

PROOF. Let ğœˆ1 = ğœˆ (c
ğ‘2
[ğ‘–].ğ‘ ğ‘“ ğ‘” = c
ğœ2

ğ‘1
c
ğœ1
if we can show that ğœˆ1 â‰¤ ğœˆ2 then the lemma follows.

) and ğœˆ2 = ğœˆ (c

ğ‘2
ğœ2

ğ‘1
ğœ1

[ğ‘–].ğ‘ ğ‘“ ğ‘”. Also from Lemma 13 we know that for 0 â‰¤ ğ‘— â‰¤ ğœˆ1, c

). By Lemma 11 for any ğ‘– such that c

ğ‘1
ğœ1

[ğ‘–] â‰  âŠ¥ and c

ğ‘2
ğœ2
[ ğ‘—] â‰  âŠ¥, and 0 â‰¤ ğ‘— â‰¤ ğœˆ2, c

[ğ‘–] â‰  âŠ¥, then
ğ‘2
[ ğ‘—] â‰  âŠ¥. So
ğœ2

ğ‘1
ğœ1

ğ‘1
ğœ1

Let ğœ‡ = ğœ‡ (c

[ ğ‘—].ğ‘ ğ‘“ ğ‘”.ğ‘„ğ‘¢ğ‘œğ‘Ÿğ‘¢ğ‘šğ‘ , for 0 â‰¤ ğ‘— < ğœˆ1, such that âˆ€ğ‘  âˆˆ ğ‘„, ğ‘ .ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶ = c

ğ‘2
ğœâ€² ) be the last finalized element which ğ‘2 established in the beginning of the read-config action ğœ‹2 (Line
A2:2) at some state ğœ â€² before ğœ2. It is easy to see that ğœ‡ â‰¤ ğœˆ2. If ğœˆ1 â‰¤ ğœ‡ then ğœˆ1 â‰¤ ğœˆ2 and the lemma follows. Thus, it
remains to examine the case where ğœ‡ < ğœˆ1. Notice that since ğœ‹1 â†’ ğœ‹2 then ğœ1 appears before ğœ â€² in execution ğœ‰. By
[ ğ‘— + 1]. Since
Lemma 14, we know that by ğœ1, âˆƒğ‘„ âˆˆ c
ğœ‡ < ğœˆ1, then it must be the case that âˆƒğ‘„ âˆˆ c
[ğœ‡ + 1]. But by Lemma
ğ‘2
ğœâ€² [ğœ‡].ğ‘ ğ‘“ ğ‘”. Let ğ‘„ â€² be the quorum that replies to the read-next-config occurred in ğ‘2, on
11, we know that c
[ğœ‡ + 1] to
configuration c
ğ‘1
ğ‘2 during ğœ‹2. Since c
[ğœ‡ + 1], and repeats the process in the configuration
ğœ1
ğ‘2
[ ğ‘—].ğ‘ ğ‘“ ğ‘”, for ğœ‡ â‰¤ ğ‘— < ğœˆ1, has a quorum of servers with ğ‘ .ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶, then by a
âˆ— [ğœ‡ + 1].ğ‘ ğ‘“ ğ‘”. Since every configuration c
c
simple induction it can be shown that the process will be repeated for at least ğœˆ1 âˆ’ ğœ‡ iterations, and every configuration
ğ‘2
[ ğ‘—], for 0 â‰¤ ğ‘— â‰¤ ğœˆ1. Hence ğœˆ1 â‰¤ ğœˆ2 and the lemma follows
ğœâ€²â€² [ ğ‘—] = c
c
â–¡
in this case as well.

ğ‘2
ğœâ€² [ğœ‡].ğ‘ ğ‘“ ğ‘”. By definition ğ‘„ âˆ© ğ‘„ â€² â‰  âˆ…, thus there is a server ğ‘  âˆˆ ğ‘„ âˆ© ğ‘„ â€² that sends ğ‘ .ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶ = c

[ğœ‡].ğ‘ ğ‘“ ğ‘”.ğ‘„ğ‘¢ğ‘œğ‘Ÿğ‘¢ğ‘šğ‘  such that âˆ€ğ‘  âˆˆ ğ‘„, ğ‘ .ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶ = c

[ ğ‘—], at some state ğœ â€²â€² before ğœ2. Thus, c

[ğœ‡ + 1] â‰  âŠ¥ then ğ‘2 assigns c

ğ‘2
âˆ— [ğœ‡ + 1] = c

[ğœ‡].ğ‘ ğ‘“ ğ‘” = c

[ ğ‘—] = c

ğ‘1
ğœ1

ğ‘1
ğœ1

ğ‘1
ğœ1

ğ‘1
ğœ1

ğ‘1
ğœ1

ğ‘2
ğœ2

ğ‘1
ğœ1

ğ‘1
ğœ1

ğ‘1
ğœ1

ğ‘1
ğœ1

Manuscript submitted to ACM

ARES: Adaptive, Reconfigurable, Erasure coded, Atomic Storage

19

Thus far we focused on the configuration member of each element in ğ‘ğ‘ ğ‘’ğ‘. As operations do get in account the status
of a configuration, i.e. ğ‘ƒ or ğ¹ , in the next lemma we will examine the relationship of the last finalized configuration as
detected by two operations. First we present a lemma that shows the monotonicity of the finalized configurations.

LEMMA 16. Let ğœ and ğœ â€² two states in an execution ğœ‰ such that ğœ appears before ğœ â€² in ğœ‰. Then for any process ğ‘ must

ğ‘
ğ‘
ğœ ) â‰¤ ğœ‡ (c
hold that ğœ‡ (c
ğœâ€²).

ğ‘
ğœ [ğ‘˜].ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘¢ğ‘  = ğ¹ at a state ğœ, then ğ‘
PROOF. This lemma follows from the fact that if a configuration ğ‘˜ is such that c
ğ‘
ğ‘
ğœâ€² [ ğ‘—].ğ‘ ğ‘“ ğ‘” such that ğ‘— â‰¥ ğ‘˜. But c
ğœâ€² [ ğ‘—].ğ‘ ğ‘“ ğ‘” is the last finalized
will start any future read-config action from a configuration c
ğ‘
ğ‘
configuration at ğœ â€² and hence ğœ‡ (c
â–¡
ğœâ€²) â‰¥ ğœ‡ (c
ğœ ).

LEMMA 17 (SEQUENCE PROGRESS). Let ğœ‹1 and ğœ‹2 two completed read-config actions invoked by processes
ğ‘1, ğ‘2 âˆˆ I respectively, such that ğœ‹1 â†’ ğœ‹2 in an execution ğœ‰. Let ğœ1 be the state after the response step of ğœ‹1 and ğœ2 the
state after the response step of ğœ‹2. Then ğœ‡ (c

) â‰¤ ğœ‡ (c

).

ğ‘1
ğœ1

ğ‘2
ğœ2

) and hence ğœ‡1 â‰¤ ğœ‡ (c

) as well.

ğ‘2
ğœ2

ğ‘1
ğœ1

PROOF. By Lemma 15 it follows that c

ğ‘1
ğœ1

), such that ğœ‡1 â‰¤ ğœˆ1, be the last element in c

ğœ‡1 = ğœ‡ (c
element which ğ‘2 obtained in Line A1:2 during ğœ‹2 such that c
and since ğœ2 is after ğœ â€², then by Lemma 16 ğœ‡2 â‰¤ ğœ‡ (c

ğ‘2
ğœ2

ğ‘1
ğœ1 is a prefix of c

ğ‘1
ğœ1 , where c

ğ‘1
ğ‘2
ğœ2 . Thus, if ğœˆ1 = ğœˆ (c
ğœ1

ğ‘2
ğœ2
[ğœ‡1].ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘¢ğ‘  = ğ¹ . Let now ğœ‡2 = ğœ‡ (c

), ğœˆ1 â‰¤ ğœˆ2. Let
ğ‘2
ğœâ€² ), be the last
ğ‘2
ğœâ€² [ğœ‡2].ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘¢ğ‘  = ğ¹ in some state ğœ â€² before ğœ2. If ğœ‡2 â‰¥ ğœ‡1,

) and ğœˆ2 = ğœˆ (c

ğ‘1
ğœ1

ğ‘1
ğœ1

ğ‘1
ğœ1

ğ‘1
ğœ1 is a prefix in terms of configurations of the c

It remains to examine the case where ğœ‡2 < ğœ‡1. Process ğ‘1 sets the status of c

ğ‘1
ğœ1
[ğœ‡1].ğ‘ ğ‘“ ğ‘”, ğ¹ âŸ© to a quorum of servers in c

[ğœ‡1] to ğ¹ in two cases: (i) either when
[ğœ‡1].ğ‘ ğ‘“ ğ‘”, ğ¹ âŸ© from some server ğ‘  during a read-config
finalizing a reconfiguration, or (ii) when receiving an ğ‘ .ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶ = âŸ¨c
[ğœ‡1 âˆ’ 1].ğ‘ ğ‘“ ğ‘” before completing.
action. In both cases ğ‘1 propagates the âŸ¨c
ğ‘2
We know by Lemma 15 that since ğœ‹1 â†’ ğœ‹2 then c
ğœ2 . So it must be the
ğ‘2
case that ğœ‡2 < ğœ‡1 â‰¤ ğœˆ (c
). Thus, during ğœ‹2, ğ‘2 starts from the configuration at index ğœ‡2 and in some iteration performs
ğœ2
[ğœ‡1 âˆ’ 1].ğ‘ ğ‘“ ğ‘”. Since ğœ‹1
get-next-config in configuration c
completed before ğœ‹2, then it must be the case that ğœ1 appears before ğœ â€² in ğœ‰. However, ğ‘2 invokes the get-next-config
operation in a state ğœ â€²â€² which is either equal to ğœ â€² or appears after ğœ â€² in ğœ‰. Thus, ğœ â€²â€² must appear after ğœ1 in ğœ‰. From that it
follows that when the get-next-config is executed by ğ‘2 there is already a quorum of servers in c
[ğœ‡1 âˆ’ 1].ğ‘ ğ‘“ ğ‘”, say ğ‘„1,
[ğœ‡1].ğ‘ ğ‘“ ğ‘”, ğ¹ âŸ©from ğ‘1. Since, ğ‘2 waits from replies from a quorum of servers from the same configuration,
that received âŸ¨c
say ğ‘„2, and since the ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶ variable at each server is monotonic (Lemma 10), then there is a server ğ‘  âˆˆ ğ‘„1 âˆ© ğ‘„2, such
) â‰¥ ğœ‡1 in this case
that ğ‘  replies to ğ‘2 with ğ‘ .ğ‘›ğ‘’ğ‘¥ğ‘¡ğ¶ = âŸ¨c
â–¡
as well. This completes our proof.

[ğœ‡1 âˆ’ 1]. According to Lemma 11, c

[ğœ‡1].ğ‘ ğ‘“ ğ‘”, ğ¹ âŸ©, and hence ğœ‡ (c

[ğœ‡1].ğ‘ ğ‘“ ğ‘”, ğ¹ âŸ©. So, c

[ğœ‡1 âˆ’ 1].ğ‘ ğ‘“ ğ‘” = c

[ğœ‡1] gets âŸ¨c

ğ‘1
ğœ1

ğ‘2
ğœ2

ğ‘2
ğœ2

ğ‘2
ğœ2

ğ‘1
ğœ1

ğ‘2
ğœ2

ğ‘2
ğœ2

ğ‘1
ğœ1

ğ‘1
ğœ1

Using the previous Lemmas we can conclude to the main result of this section.

THEOREM 18. Let ğœ‹1 and ğœ‹2 two completed read-config actions invoked by processes ğ‘1, ğ‘2 âˆˆ I respectively, such
that ğœ‹1 â†’ ğœ‹2 in an execution ğœ‰. Let ğœ1 be the state after the response step of ğœ‹1 and ğœ2 the state after the response step of
ğœ‹2. Then the following properties hold:
ğ‘2
(ğ‘) Configuration Consistency: c
ğœ2
ğ‘2
(ğ‘) Sequence Prefix: c
ğœ2 , and
(ğ‘) Sequence Progress: ğœ‡ (c
) â‰¤ ğœ‡ (c

[ğ‘–].ğ‘ ğ‘“ ğ‘”, for 1 â‰¤ ğ‘– â‰¤ ğœˆ (c

[ğ‘–].ğ‘ ğ‘“ ğ‘” = c

ğ‘1
ğœ1

ğ‘1
ğœ1

ğ‘1
ğœ1

),

)

âª¯ğ‘ c
ğ‘1
ğœ1

ğ‘2
ğœ2

PROOF. Statements (ğ‘), (ğ‘) and (ğ‘) follow from Lemmas 11, 15, and 16.

â–¡

Manuscript submitted to ACM

20Nicolas Nicolaou, Viveck Cadambe, N. Prakash, Andria Trigeorgi, Kishori M. Konwar, Muriel Medard, and Nancy Lynch

6.2 Atomicity Property of ARES

Given the properties satisfied by the reconfiguration algorithm of ARES in any execution, we can now proceed to examine

whether our algorithm satisfies the safety (atomicity) conditions. The propagation of the information of the distributed

object is achieved using the get-tag, get-data, and put-data actions. We assume that the DAP used satisfy Property 1 as

presented in Section 3, and we will show that, given such assumption, ARES satisfies atomicity.

We begin with a lemma stating that if a reconfiguration operation retrieves a configuration sequence of length ğ‘˜ during

its read-config action, then it installs/finalizes the ğ‘˜ + 1 configuration in the global configuration sequence.

LEMMA 19. Let ğœ‹ be a complete reconfiguration operation by a reconfigurer ğ‘Ÿğ‘ in an execution ğœ‰ of ARES. if ğœ1 is the
) at a state ğœ2

state in ğœ‰ following the termination of the read-config action during ğœ‹, then ğœ‹ invokes a finalize-config(cğ‘Ÿğ‘
ğœ2
in ğœ‰, with ğœˆ (cğ‘Ÿğ‘
ğœ2

) = ğœˆ (cğ‘Ÿğ‘
ğœ1

) + 1.

PROOF. This lemma follows directly from the implementation of the reconfig operation. Let ğœ‹ be a reconfiguration
operation reconfig(ğ‘). At first, ğœ‹ invokes a read-config to retrieve a latest value of the global configuration sequence, cğ‘Ÿğ‘
ğœ1 ,
in the state ğœ1 in ğœ‰. During the add-config action, ğœ‹ proposes the addition of ğ‘, and appends at the end of cğ‘Ÿğ‘
ğœ1 the decision
ğ‘‘ of the consensus protocol. Therefore, if cğ‘Ÿğ‘
ğœ1 is extended by âŸ¨ğ‘‘, ğ‘ƒâŸ© (Line A 2:17), and hence the add-config action returns
a configuration sequence cğ‘Ÿğ‘
does not change during the update-config action,
ğœâ€²
1
) = ğœˆ (cğ‘Ÿğ‘
then cğ‘Ÿğ‘
) + 1 and
ğœâ€²
ğœâ€²
1
1
â–¡
the lemma follows.

is passed to the finalize-config action at state ğœ2, and hence cğ‘Ÿğ‘
ğœ2

with length ğœˆ (cğ‘Ÿğ‘
ğœâ€²
1

) + 1. As ğœˆ (cğ‘Ÿğ‘
ğœâ€²
1

. Thus, ğœˆ (cğ‘Ÿğ‘
ğœ2

) = ğœˆ (cğ‘Ÿğ‘
ğœ1

) = ğœˆ (cğ‘Ÿğ‘
ğœ1

= cğ‘Ÿğ‘
ğœâ€²
1

The next lemma states that only some reconfiguration operation ğœ‹ may finalize a configuration ğ‘ at index ğ‘— in a
configuration sequence ğ‘.ğ‘ğ‘ ğ‘’ğ‘ at any process ğ‘. To finalize ğ‘, the lemma shows that ğœ‹ must witness a configuration
sequence such that its last finalized configuration appears at an index ğ‘– < ğ‘— in the configuration sequence ğ‘.ğ‘ğ‘ ğ‘’ğ‘. In other
words, reconfigurations always finalize configurations that are ahead from their latest observed final configuration, and it

seems like â€œjumpingâ€ from one final configuration to the next.

ğ‘
ğœ [ ğ‘—].ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘¢ğ‘  = ğ¹ for some process ğ‘ âˆˆ I,
LEMMA 20. Suppose ğœ‰ is an execution of ARES. For any state ğœ in ğœ‰, if c
ğœâ€²) during ğœ‹ at

then there exists a reconfig operation ğœ‹ by a reconfigurer ğ‘Ÿğ‘ âˆˆ G, such that (i) ğ‘Ÿğ‘ invokes finalize-config(cğ‘Ÿğ‘
some state ğœ â€² in ğœ‰, (ii) ğœˆ (cğ‘Ÿğ‘

ğœâ€²) = ğ‘—, and (iii) ğœ‡ (cğ‘Ÿğ‘

ğœâ€²) < ğ‘—.

PROOF. A process sets the status of a configuration ğ‘ to ğ¹ in two cases: (i) either during a finalize-config(ğ‘ ğ‘’ğ‘) action
such that ğœˆ (ğ‘ ğ‘’ğ‘) = âŸ¨ğ‘, ğ‘ƒâŸ© (Line A2:33), or (ii) when it receives âŸ¨ğ‘, ğ¹ âŸ© from a server ğ‘  during a read-next-config action.
Server ğ‘  sets the status of a configuration ğ‘ to ğ¹ only if it receives a message that contains âŸ¨ğ‘, ğ¹ âŸ© (Line A3:10). So, (ii) is
possible only if ğ‘ is finalized during a reconfig operation.

ğ‘
ğœ [ ğ‘—].ğ‘ ğ‘“ ğ‘”. To do so, process ğ‘Ÿğ‘ invokes
Let, w.l.o.g., ğœ‹ be the first reconfiguration operation that finalizes c
ğœâ€² [ ğ‘—].ğ‘ ğ‘“ ğ‘”.
ğœâ€² and hence ğœˆ (cğ‘Ÿğ‘
ğœâ€²) = ğ‘—. Also, by Lemma 20 it follows that
ğœâ€²â€² in some state ğœ â€²â€² that appeared before ğœ â€² in ğœ‰, such that
ğœâ€²â€² to
â–¡

ğ‘
finalize-config(cğ‘Ÿğ‘
ğœ [ ğ‘—].ğ‘ ğ‘“ ğ‘” = cğ‘Ÿğ‘
) during ğœ‹, at some state ğœ â€² that appears before ğœ in ğœ‰. By Lemma 11, c
ğœâ€²
1
Since, ğ‘Ÿğ‘ finalizes cğ‘Ÿğ‘
ğœâ€² [ ğ‘—], then this is the last entry of cğ‘Ÿğ‘
the read-config action of ğœ‹ returned a configuration cğ‘Ÿğ‘
ğœâ€²). Since by definition, ğœ‡ (cğ‘Ÿğ‘
ğœˆ (cğ‘Ÿğ‘
ğœâ€²â€²) = ğœ‡ (cğ‘Ÿğ‘
result in cğ‘Ÿğ‘

ğœâ€²â€²), then ğœ‡ (cğ‘Ÿğ‘
ğœâ€²) < ğ‘— as well and the lemma follows.

ğœâ€²â€²) < ğ‘—. However, since only âŸ¨ğ‘, ğ‘ƒâŸ© is added to cğ‘Ÿğ‘

ğœâ€²â€²) â‰¤ ğœˆ (cğ‘Ÿğ‘
ğœâ€²). Therefore, ğœ‡ (cğ‘Ÿğ‘

ğœâ€², then ğœ‡ (cğ‘Ÿğ‘

ğœâ€²â€²) < ğœˆ (cğ‘Ÿğ‘

We now reach an important lemma of this section. By ARES, before a read/write/reconfig operation completes

it propagates the maximum tag it discovered by executing the put-data action in the last configuration of its local
configuration sequence (Lines A2:18, A4:16, A4:38). When a subsequent operation is invoked, it reads the latest

Manuscript submitted to ACM

ARES: Adaptive, Reconfigurable, Erasure coded, Atomic Storage

21

configuration sequence by beginning from the last finalized configuration in its local sequence and invoking read-data to
all the configurations until the end of that sequence. The lemma shows that the latter operation will retrieve a tag which is

higher than the tag used in the put-data action of any preceding operation.

LEMMA 21. Let ğœ‹1 and ğœ‹2 be two completed read/write/reconfig operations invoked by processes ğ‘1 and ğ‘2 in I,
in an execution ğœ‰ of ARES, such that, ğœ‹1 â†’ ğœ‹2. If ğ‘1.put-data(âŸ¨ğœğœ‹1
, ğ‘£ğœ‹1 âŸ©) is the last put-data action of ğœ‹1 and ğœ2 is the
state in ğœ‰ after the completion of the first read-config action of ğœ‹2, then there exists a ğ‘2.put-data(âŸ¨ğœ, ğ‘£âŸ©) action in some
), such that (i) it completes in a state ğœ â€² before ğœ2 in ğœ‰, and (ii)
configuration ğ‘2 = c
ğœ â‰¥ ğœğœ‹1 .

[ğ‘˜].ğ‘ ğ‘“ ğ‘”, for ğœ‡ (c

) â‰¤ ğ‘˜ â‰¤ ğœˆ (c

ğ‘2
ğœ2

ğ‘2
ğœ2

ğ‘2
ğœ2

ğ‘1
ğœ1

ğ‘2
ğœ2

[ğœˆ (c

) â‰¤ ğœˆ (c

PROOF. Note that from the definitions of ğœˆ (Â·) and ğœ‡ (Â·), we have ğœ‡ (c
, ğ‘£ğœ‹1 âŸ©) and ğœ â€²

ğ‘2
). Let ğœ1 be the state in ğœ‰ after the
ğœ2
1 be the state in ğœ‰ following the response step of ğœ‹1. Since any operation
completion of ğ‘1.put-data(âŸ¨ğœğœ‹1
ğ‘1
ğœ1 , and hence ğ‘1 =
executes put-data on the last discovered configuration then ğ‘1 is the last configuration found in c
ğ‘2
ğ‘1
ğ‘1
), since ğœ‹2 (and thus
) â‰¤ ğœ‡ (c
c
ğœ2
ğœ1
ğœ1
its first read-config action) is invoked after ğœ â€²
1 (and thus after the last read-config action during ğœ‹1). Hence, combining
ğ‘2
the two implies that ğœ‡ (c
).
ğœ2
Therefore, it remains to examine whether the last finalized configuration witnessed by ğ‘2 appears before or after ğ‘1, i.e.:
(ğ‘) ğœ‡ (c

). Now from the last implication and the first statement we have ğœ‡ (c

)].ğ‘ ğ‘“ ğ‘”. By Lemma 16 we have ğœ‡ (c

) and by Lemma 17 we have ğœ‡ (c

) and (ğ‘) ğœ‡ (c

) â‰¤ ğœ‡ (c

) â‰¤ ğœˆ (c

) â‰¤ ğœ‡ (c

) â‰¤ ğœˆ (c

) > ğœˆ (c

ğ‘1
ğœâ€²
1

ğ‘1
ğœâ€²
1

ğ‘1
ğœ1

ğ‘2
ğœ2

ğ‘1
ğœ1

).

ğ‘2
ğœ2

ğ‘1
ğœ1

ğ‘1
ğœ1

ğ‘2
ğœ2

ğ‘1
ğœ1

ğ‘2
ğœ2 value returned by read-config at ğ‘2 during the execution of ğœ‹2
Case (ğ‘): Since ğœ‹1 â†’ ğœ‹2 then, by Theorem 18, c
ğ‘1
ğ‘1
ğ‘2
) â‰¤ ğœˆ (c
). Since ğ‘1 is the last
) â‰¤ ğœˆ (c
ğœ2 . Therefore, ğœˆ (c
âª¯ğ‘ c
satisfies c
ğœ1
ğœ1
ğ‘1
ğ‘1
). So if we take ğ‘2 = ğ‘1 then the ğ‘1.put-data(âŸ¨ğœğœ‹1
ğœ1 , then it has index ğœˆ (c
, ğ‘£ğœ‹1 âŸ©) action trivially satisfies
configuration in c
ğœ1
both conditions of the lemma as: (i) it completes in state ğœ1 which appears before ğœ2, and (ii) it puts a pair âŸ¨ğœ, ğ‘£âŸ© such that
ğœ = ğœğœ‹1 .

), and hence in this case ğœ‡ (c

) â‰¤ ğœˆ (c

ğ‘2
ğœ2

ğ‘2
ğœ2

ğ‘2
ğœ2

Case (ğ‘): This case is possible if there exists a reconfiguration client ğ‘Ÿğ‘ that invokes reconfig operation ğœŒ, during which
it executes the finalize-config(cğ‘Ÿğ‘
). Let ğœ be the state immediately
after the read-config of ğœŒ. Now, we consider two sub-cases: (ğ‘–) ğœ appears before ğœ1 in ğœ‰, or (ğ‘–ğ‘–) ğœ appears after ğœ1 in ğœ‰.

âˆ— ) that finalized configuration with index ğœˆ (cğ‘Ÿğ‘

âˆ— ) = ğœ‡ (c

ğ‘2
ğœ2

ğ‘1
ğœ1

ğ‘2
ğœ2

ğœ = c

ğœ â‰ºğ‘ c

ğ‘1
ğœ1 , or cğ‘Ÿğ‘

ğœ ) + 1 â‰¤ ğœˆ (c

âˆ— ) = ğœˆ (cğ‘Ÿğ‘

Now suppose, that cğ‘Ÿğ‘

ğ‘1
ğœ1 . Then it follows that ğœˆ (cğ‘Ÿğ‘

ğ‘1
ğœ1 due to Lemma 15. Suppose cğ‘Ÿğ‘

Subcase (ğ‘)(ğ‘–): Since read-config at ğœ completes before the invocation of last read-config of operation ğœ‹1 then, either
ğ‘1
cğ‘Ÿğ‘
ğœ1 , then according to Lemma 19 ğ‘Ÿğ‘ executes finalize-config
ğœ â‰ºğ‘ c
ğ‘2
on configuration sequence cğ‘Ÿğ‘
âˆ— with ğœˆ (cğ‘Ÿğ‘
ğœ ) + 1. If however,
ğœ2
ğ‘1
ğ‘1
cğ‘Ÿğ‘
ğœ1 , then ğœˆ (cğ‘Ÿğ‘
ğœ ) < ğœˆ (c
) â‰¤ ğœˆ (c
) which contradicts our
ğœ â‰ºğ‘ c
ğœ1
initial assumption for this case that ğœ‡ (c

ğœ ) + 1. Since ğœˆ (cğ‘Ÿğ‘
ğ‘1
ğœ1

), then ğœ‡ (c
ğ‘2
ğœ2

) = ğœˆ (cğ‘Ÿğ‘
ğ‘1
ğœ1

) and thus ğœˆ (cğ‘Ÿğ‘
ğ‘2
) > ğœˆ (c
ğœ2

âˆ— ) = ğœ‡ (c
). This implies that ğœ‡ (c
). So this sub-case is impossible.
ğ‘2
ğœ2

) + 1 in this case. Since ğœ1 is
) = ğœˆ (c
ğœ = c
the state after the last put-data during ğœ‹1, then if ğœ â€²
1 is the state after the completion of the last read-config of ğœ‹1 (which
ğ‘1
. So, during its last read-config process ğ‘1 does not read the
= c
follows the put-data), it must be the case that c
ğœ1
ğ‘1
)+1. This means that the put-config completes in ğœŒ at state ğœğœŒ after ğœ â€²
configuration indexed at ğœˆ (c
1 and the update-config
ğœ1
ğœŒ after ğœğœŒ with a configuration sequence cğ‘Ÿğ‘
operation is invoked at state ğœ â€²
. During the update operation ğœŒ invokes get-data
ğœâ€²
ğœŒ
operation in every configuration cğ‘Ÿğ‘
). Notice that ğœˆ (cğ‘Ÿğ‘
) â‰¤ ğ‘– â‰¤ ğœˆ (cğ‘Ÿğ‘
) = ğœ‡ (c
ğœâ€²
ğœâ€²
ğœâ€²
ğœŒ
ğœŒ
ğœŒ
moreover the last configuration of cğ‘Ÿğ‘
ğœâ€²
ğœŒ
and hence ğœ‡ (cğ‘Ÿğ‘
ğœâ€²
ğœŒ

[ğ‘–].ğ‘ ğ‘“ ğ‘”, for ğœ‡ (cğ‘Ÿğ‘
) = ğœˆ (c
ğœâ€²
ğœŒ
was just added by ğœŒ and it is not finalized. From this it follows that ğœ‡ (cğ‘Ÿğ‘
ğœâ€²
ğœŒ

). Therefore, ğœŒ executes get-data in configuration cğ‘Ÿğ‘
ğœâ€²
ğœŒ

) + 1 and
),
). Since ğ‘1 invoked

ğ‘1
ğœ1
) < ğœˆ (cğ‘Ÿğ‘
ğœâ€²
ğœŒ

[ ğ‘—].ğ‘ ğ‘“ ğ‘” for ğ‘— = ğœˆ (c

), and that ğœ‡ (c

ğœ ) = ğœˆ (c

) â‰¤ ğœˆ (c

ğ‘1
ğœâ€²
1

ğ‘1
ğœ1

ğ‘1
ğœ1

ğ‘2
ğœ2

ğ‘1
ğœ1

ğ‘1
ğœ1

Manuscript submitted to ACM

22Nicolas Nicolaou, Viveck Cadambe, N. Prakash, Andria Trigeorgi, Kishori M. Konwar, Muriel Medard, and Nancy Lynch

, ğ‘£ğœ‹1 âŸ©) at the same configuration ğ‘1, and completed in a state ğœ1 before ğœ â€²

put-data(âŸ¨ğœğœ‹1
ğœŒ , then by C1 of Property 1, it
follows that the get-data action will return a tag ğœ â‰¥ ğœğœ‹1 . Therefore, the maximum tag that ğœŒ discovers is ğœğ‘šğ‘ğ‘¥ â‰¥ ğœ â‰¥ ğœğœ‹1 .
ğ‘2
Before invoking the finalize-config action, ğœŒ invokes ğ‘1.put-data(âŸ¨ğœğ‘šğ‘ğ‘¥ , ğ‘£ğ‘šğ‘ğ‘¥ )âŸ©. Since ğœˆ (cğ‘Ÿğ‘
), and since by
ğœ2
ğœâ€²
ğœŒ
ğ‘2
Lemma 11, then the action put-data is invoked in a configuration ğ‘2 = c
[ ğ‘—].ğ‘ ğ‘“ ğ‘” such that ğ‘— = ğœ‡ (c
). Since the
ğœ2
read-config action of ğœ‹2 observed configuration ğœ‡ (c
), then it must be the case that ğœ2 appears after the state where the
finalize-config was invoked and therefore after the state of the completion of the put-data action during ğœŒ. Thus, in this
case both properties are satisfied and the lemma follows.

) = ğœ‡ (c

ğ‘2
ğœ2

ğ‘2
ğœ2

Subcase (ğ‘)(ğ‘–ğ‘–): Suppose in this case that ğœ occurs in ğœ‰ after ğœ1. In this case the last put-data in ğœ‹1 completes before the
invocation of the read-config in ğœŒ in execution ğœ‰. Now we can argue recursively, ğœŒ taking the place of operation ğœ‹2, that
ğœ‡ (cğ‘Ÿğ‘
). Note that there are
finite number of operations invoked in ğœ‰ before ğœ‹2 is invoked, and hence the statement of the lemma can be shown to hold
â–¡
by a sequence of inequalities.

ğœ ) and therefore, we consider two cases: (ğ‘) ğœ‡ (cğ‘Ÿğ‘

) and (ğ‘) ğœ‡ (cğ‘Ÿğ‘

ğœ ) â‰¤ ğœˆ (cğ‘Ÿğ‘

ğœ ) â‰¤ ğœˆ (c

ğœ ) > ğœˆ (c

ğ‘1
ğœ1

ğ‘1
ğœ1

The following lemma shows the consistency of operations as long as the DAP used satisfy Property 1.

LEMMA 22. Let ğœ‹1 and ğœ‹2 denote completed read/write operations in an execution ğœ‰, from processes ğ‘1, ğ‘2 âˆˆ I
respectively, such that ğœ‹1 â†’ ğœ‹2. If ğœğœ‹1 and ğœğœ‹2 are the local tags at ğ‘1 and ğ‘2 after the completion of ğœ‹1 and ğœ‹2 respectively,
then ğœğœ‹1 â‰¤ ğœğœ‹2 ; if ğœ‹1 is a write operation then ğœğœ‹1

< ğœğœ‹2 .

PROOF. Let âŸ¨ğœğœ‹1

, ğ‘£ğœ‹1 âŸ© be the pair passed to the last put-data action of ğœ‹1. Also, let ğœ2 be the state in ğœ‰ that follows the
completion of the first read-config action during ğœ‹2. Notice that ğœ‹2 executes a loop after the first read-config operation
ğ‘2
ğ‘2
and performs ğ‘.get-data (if ğœ‹2 is a read) or ğ‘.get-tag (if ğœ‹2 is a write) from all ğ‘ = c
).
ğœ2
ğœ2
By Lemma 21, there exists a ğ‘ â€².put-data(âŸ¨ğœ, ğ‘£âŸ©) action by some operation ğœ‹ â€² on some configuration ğ‘ â€² = c
[ ğ‘—].ğ‘ ğ‘“ ğ‘”, for
ğ‘2
), that completes in some state ğœ â€² that appears before ğœ2 in ğœ‰. Thus, the get-data or get-tag invoked
ğœ‡ (c
ğœ2
[ ğ‘—].ğ‘ ğ‘“ ğ‘”, occurs after state ğœ2 and thus after ğœ â€². Since the DAP primitives used satisfy C1 and C2 of Property
by ğ‘2 on c
1, then the get-tag action will return a tag ğœ â€²
ğœ‹2 or a get-data action will return a pair âŸ¨ğœ â€²
â‰¥ ğœ. As ğ‘2 gets
ğœ‹2
the maximum of all the tags returned, then by the end of the loop ğ‘2 will retrieve a tag ğœğ‘šğ‘ğ‘¥ â‰¥ ğœ â€²
ğœ‹2

) â‰¤ ğ‘— â‰¤ ğœˆ (c
ğ‘2
ğœ2

) â‰¤ ğ‘– â‰¤ ğœˆ (c
ğ‘2
ğœ2

[ğ‘–].ğ‘ ğ‘“ ğ‘”, for ğœ‡ (c

âŸ©, with ğœ â€²
ğœ‹2

â‰¥ ğœ â‰¥ ğœğœ‹1 .

, ğ‘£ â€²
ğœ‹2

ğ‘2
ğœ2

ğ‘2
ğœ2

If now ğœ‹2 is a read, it returns âŸ¨ğœğ‘šğ‘ğ‘¥ , ğ‘£ğ‘šğ‘ğ‘¥ âŸ© after propagating that value to the last discovered configuration. Thus,
ğœğœ‹2 â‰¥ ğœğœ‹1 . If however ğœ‹2 is a write, then before propagating the new value the writer increments the maximum timestamp
> ğœğœ‹1 in this
discovered (Line A4:13) generating a tag ğœğœ‹2
â–¡
case.

> ğœğ‘šğ‘ğ‘¥ . Therefore the operation ğœ‹2 propagates a tag ğœğœ‹2

And the main result of this section follows:

THEOREM 23 (ATOMICITY). In any execution ğœ‰ of ARES, if in every configuration ğ‘ âˆˆ Gğ¿, ğ‘.get-data(), ğ‘.put-data(),

and ğ‘.get-tag() satisfy Property 1, then ARES satisfy atomicity.

As algorithm ARES handles each configuration separately, then we can observe that the algorithm may utilize a

different mechanism for the put and get primitives in each configuration. So the following remark:

REMARK 24. Algorithm ARES satisfies atomicity even when the implementaton of the DAPs in two different configu-
rations ğ‘1 and ğ‘2 are not the same, given that the ğ‘ğ‘– .get-tag, ğ‘ğ‘– .get-data, and the ğ‘ğ‘– .put-data primitives in each ğ‘ğ‘– satisfy
Property 1.

Manuscript submitted to ACM

ARES: Adaptive, Reconfigurable, Erasure coded, Atomic Storage

23

7 PERFORMANCE ANALYSIS OF ARES

A major challenge in reconfigurable atomic services is to examine the latency of terminating read and write operations,

especially when those are invoked concurrently with reconfiguration operations. In this section we provide an in depth

analysis of the latency of operations in ARES. Additionally, a storage and communication analysis is shown when ARES

utilizes the erasure-coding algorithm presented in Section 5, in each configuration.

7.1 Latency Analysis

Liveness (termination) properties cannot be specified for ARES, without restricting asynchrony or the rate of arrival

of reconfig operations, or if the consensus protocol never terminates. Here, we provide some conditional performance
analysis of the operation, based on latency bounds on the message delivery. We assume that local computations take

negligible time and the latency of an operation is due to the delays in the messages exchanged during the execution. We
measure delays in time units of some global clock, which is visible only to an external viewer. No process has access to
the clock. Let ğ‘‘ and ğ· be the minimum and maximum durations taken by messages, sent during an execution of ARES, to
reach their destinations. Also, let ğ‘‡ (ğœ‹) denote the duration of an operation (or action) ğœ‹. In the statements that follow,
we consider any execution ğœ‰ of ARES, which contains ğ‘˜ reconfig operations. For any configuration ğ‘ in an execution
of ARES, we assume that any ğ‘.ğ¶ğ‘œğ‘›.propose operation, takes at least ğ‘‡ğ‘šğ‘–ğ‘› (ğ¶ğ‘ ) time units.

Let us first examine what is the action delays based on the boundaries we assume. It is easy to see that actions
put-config, read-next-config perform two message exchanges thus take time 2ğ‘‘ â‰¤ ğ‘‡ (ğœ™) â‰¤ 2ğ·. From this we can derive
the delay of a read-config action.

LEMMA 25. Let ğœ™ be a read-config operation invoked by a non-faulty reconfiguration client ğ‘Ÿğ‘, with the input
ğœ ) + 1) â‰¤ ğ‘‡ (ğœ™) â‰¤

ğœâ€² respectively. Then the delay of ğœ™ is: 4ğ‘‘ (ğœˆ (cğ‘Ÿğ‘

ğœâ€²) âˆ’ ğœ‡ (cğ‘Ÿğ‘

ğœ and cğ‘Ÿğ‘

argument and returned values of ğœ™ as cğ‘Ÿğ‘
4ğ· (ğœˆ (cğ‘Ÿğ‘
ğœ ) + 1).

ğœâ€²) âˆ’ ğœ‡ (cğ‘Ÿğ‘

From Lemma 25 it is clear that the latency of a read-config action depends on the number of configurations installed

since the last finalized configuration known to the recon client.

Given the latency of a read-config, we can compute the minimum amount of time it takes for ğ‘˜ configurations to be

installed.

The following lemma shows the maximum latency of a read or a write operation, invoked by any non-faulty client.

From ARES algorithm, the latency of a read/write operation depends on the delays of the DAPs operations. For our

analysis we assume that all get-data, get-tag and put-data primitives use two phases of communication. Each phase
consists of a communication between the client and the servers.

LEMMA 26. Suppose ğœ‹, ğœ™ and ğœ“ are operations of the type put-data, get-tag and get-data, respectively, invoked by
some non-faulty reconfiguration clients, then the latency of these operations are bounded as follows: (ğ‘–) 2ğ‘‘ â‰¤ ğ‘‡ (ğœ‹) â‰¤ 2ğ·;
(ğ‘–ğ‘–) 2ğ‘‘ â‰¤ ğ‘‡ (ğœ™) â‰¤ 2ğ·; and (ğ‘–ğ‘–ğ‘–) 2ğ‘‘ â‰¤ ğ‘‡ (ğœ“ ) â‰¤ 2ğ·.

In the following lemma, we estimate the time taken for a read or a write operation to complete, when it discovers ğ‘˜

configurations between its invocation and response steps.

LEMMA 27. Consider any execution of ARES where at most ğ‘˜ reconfiguration operations are invoked. Let ğœğ‘  and ğœğ‘’
be the states before the invocation and after the completion step of a read/write operation ğœ‹, in some fair execution ğœ‰ of
ARES. Then we have ğ‘‡ (ğœ‹) â‰¤ 6ğ· (ğ‘˜ + 2) to complete.

Manuscript submitted to ACM

24Nicolas Nicolaou, Viveck Cadambe, N. Prakash, Andria Trigeorgi, Kishori M. Konwar, Muriel Medard, and Nancy Lynch

PROOF. Let ğœğ‘  and ğœğ‘’ be the states before the invocation and after the completion step of a read/write operation ğœ‹ by ğ‘
respectively, in some execution ğœ‰ of ARES. By algorithm examination we can see that any read/write operation performs
the following actions in this order: (ğ‘–) read-config, (ğ‘–ğ‘–) get-data (or get-tag), (ğ‘–ğ‘–ğ‘–) put-data, and (ğ‘–ğ‘£) read-config. Let
ğœ1 be the state when the first read-config, denoted by read-config1, action terminates. By Lemma 25 the action will take
time:

ğ‘
ğ‘‡ (read-config1) â‰¤ 4ğ· (ğœˆ (c
ğœ1

ğ‘
) âˆ’ ğœ‡ (c
ğœğ‘  ) + 1)

ğ‘
ğ‘
The get-data action that follows the read-config (Lines Alg. 4:34-35) also took at most (ğœˆ (c
) âˆ’ ğœ‡ (c
ğœğ‘  ) + 1) time units,
ğœ1
given that no new finalized configuration was discovered by the read-config action. Finally, the put-data and the second
ğ‘
ğ‘
read-config actions of ğœ‹ may be invoked at most (ğœˆ (c
ğœğ‘’ ) âˆ’ ğœˆ (c
) + 1) times, given that the read-config action discovers
ğœ1
one new configuration every time it runs. Merging all the outcomes, the total time of ğœ‹ can be at most:

ğ‘
ğ‘
ğ‘
ğœğ‘  ) + 1) + 2ğ· (ğœˆ (c
ğ‘‡ (ğœ‹) â‰¤ 4ğ· (ğœˆ (c
) âˆ’ ğœ‡ (c
ğœ1
ğœ1
(cid:105)
(cid:104)
ğ‘
ğ‘
ğœğ‘’ ) âˆ’ ğœ‡ (c
ğœˆ (c
ğœğ‘  ) + 2

â‰¤ 6ğ·

â‰¤ 6ğ· (ğ‘˜ + 1)

ğ‘
ğ‘
ğ‘
ğœğ‘’ ) âˆ’ ğœˆ (c
ğœğ‘  ) + 1) + (4ğ· + 2ğ·)(ğœˆ (c
) âˆ’ ğœ‡ (c
ğœ1

) + 1)

ğ‘
ğ‘
ğœğ‘  ) â‰¤ ğ‘˜ + 1 since there can be at most ğ‘˜ new configurations installed. and the result of the lemma
ğœğ‘’ ) âˆ’ ğœ‡ (c
where ğœˆ (c
â–¡
follows.

It remains now to examine the conditions under which a read/write operation may â€œcatch upâ€ with an infinite number

of reconfiguration operations. For the sake of a worst case analysis we will assume that reconfiguration operations suffer
the minimum delay ğ‘‘, whereas read and write operations suffer the maximum delay ğ· in each message exchange. We first
show how long it takes for ğ‘˜ configurations to be installed.

LEMMA 28. Let ğœ be the last state of a fair execution of ARES, ğœ‰. Then ğ‘˜ configurations can be installed to cğœ , in time

ğ‘‡ (ğ‘˜) â‰¥ 4ğ‘‘ (cid:205)ğ‘˜

ğ‘–=1

ğ‘– + ğ‘˜ (ğ‘‡ğ‘šğ‘–ğ‘› (ğ¶ğ‘ ) + 2ğ‘‘) time units.

PROOF. In ARES a reconfig operation has four phases: (ğ‘–) read-config(ğ‘ğ‘ ğ‘’ğ‘), reads the latest configuration se-
quence, (ğ‘–ğ‘–) add-config(ğ‘ğ‘ ğ‘’ğ‘, ğ‘), attempts to add the new configuration at the end of the global sequence Gğ¿, (ğ‘–ğ‘–ğ‘–)
update-config(ğ‘ğ‘ ğ‘’ğ‘), transfers the knowledge to the added configuration, and (ğ‘–ğ‘£) finalize-config(ğ‘ğ‘ ğ‘’ğ‘) finalizes the
added configuration. So, a new configuration is appended to the end of the configuration sequence (and it becomes visible

to any operation) during the add-config action. In turn, the add-config action, runs a consensus algorithm to decide on the
added configuration and then invokes a put-config action to add the decided configuration. Any operation that is invoked
after the put-config action observes the newly added configuration.

Notice that when multiple reconfigurations are invoked concurrently, then it might be the case that all participate to the

same consensus instance and the configuration sequence is appended by a single configuration. The worst case scenario

happens when all concurrent reconfigurations manage to append the configuration sequence by their configuration. In

brief, this is possible when the read-config action of each reconfig operation appears after the put-config action of another
reconfig operation.

More formally we can build an execution where all reconfig operations append their configuration in the configuration
sequence. Consider the partial execution ğœ‰ that ends in a state ğœ. Suppose that every process ğ‘ âˆˆ I knows the same
ğ‘
ğœ = cğœ . Also let the last finalized operation in cğœ be the last configuration of the sequence, e.g.
configuration sequence, c
ğ‘
ğœ0 . We extend ğœ‰0 by a series of reconfig
ğœ‡ (cğœ ) = ğœˆ (cğœ ). Notice that cğœ can also be the initial configuration sequence c
operations, such that each reconfiguration ğ‘Ÿğ‘ğ‘– is invoked by a reconfigurer ğ‘Ÿğ‘– and attempts to add a configuration ğ‘ğ‘– . Let

Manuscript submitted to ACM

ARES: Adaptive, Reconfigurable, Erasure coded, Atomic Storage

25

Fig. 2. Successful reconfig operations.

ğ‘Ÿğ‘1 be the first reconfiguration that performs the following actions without being concurrent with any other reconfig
operation:

â€¢ read-config starting from ğœ‡ (cğœ )
â€¢ add-config completing both the consensus proposing ğ‘1 and the put-config action writing the decided configuration

Since ğ‘Ÿğ‘1 its not concurrent with any other reconfig operation, then is the only process to propose a configuration in
ğœ‡ (cğœ ), and hence by the consensus algorithm properties, ğ‘1 is decided. Thus, cğœ is appended by a tuple âŸ¨ğ‘1, ğ‘ƒâŸ©.

Let now reconfiguration ğ‘Ÿğ‘2 be invoked immediately after the completion of the add-config action from ğ‘Ÿğ‘1. Since
the local sequence at the beginning of ğ‘Ÿğ‘2 is equal to cğœ , then the read-config action of ğ‘Ÿğ‘2 will also start from ğœ‡ (cğœ ).
Since, ğ‘Ÿğ‘1 already propagated ğ‘1 to ğœ‡ (cğœ ) during is put-config action, then ğ‘Ÿğ‘2 will discover ğ‘1 during the first iteration
of its read-config action, and thus it will repeat the iteration on ğ‘1. Configuration ğ‘1 is the last in the sequence and thus
the read-config action of ğ‘Ÿğ‘2 will terminate after the second iteration. Following the read-config action, ğ‘Ÿğ‘2 attempts to
add ğ‘2 in the sequence. Since ğ‘Ÿğ‘1 is the only reconfiguration that might be concurrent with ğ‘Ÿğ‘2, and since ğ‘Ÿğ‘1 already
completed consensus in ğœ‡ (cğœ ), then ğ‘Ÿğ‘2 is the only operation to run consensus in ğ‘1. Therefore, ğ‘2 is accepted and ğ‘Ÿğ‘2
propagates ğ‘2 in ğ‘1 using a put-config action.

So in general we let configuration ğ‘Ÿğ‘ğ‘– to be invoked after the completion of the add-config action from ğ‘Ÿğ‘ğ‘–âˆ’1. As a
result, the read-config action of ğ‘Ÿğ‘ğ‘– performs ğ‘– iterations, and the configuration ğ‘ğ‘– is added immediately after configuration
ğ‘ğ‘–âˆ’1 in the sequence. Figure 2 illustrates our execution construction for the reconfiguration operations.

It is easy to notice that such execution results in the worst case latency for all the reconfiguration operations
ğ‘Ÿğ‘1, ğ‘Ÿğ‘2, . . . , ğ‘Ÿğ‘ğ‘– . As by Lemma 25 a read-config action takes at least 4ğ‘‘ time to complete, then as also seen in
Figure 2, ğ‘˜ reconfigs may take time ğ‘‡ (ğ‘˜) â‰¥ (cid:205)ğ‘˜
ğ‘–=1 [4ğ‘‘ âˆ— ğ‘– + (ğ‘‡ğ‘šğ‘–ğ‘› (ğ¶ğ‘ ) + 2ğ‘‘)]. Therefore, it will take time ğ‘‡ (ğ‘˜) â‰¥
4ğ‘‘ (cid:205)ğ‘˜
â–¡

ğ‘– + ğ‘˜ (ğ‘‡ğ‘šğ‘–ğ‘› (ğ¶ğ‘ ) + 2ğ‘‘) and the lemma follows.

ğ‘–=1

The following theorem is the main result of this section, in which we define the relation between ğ‘‡ğ‘šğ‘–ğ‘› (ğ¶ğ‘ ), ğ‘‘ and ğ·

so to guarantee that any read or write issued by a non-faulty client always terminates.

THEOREM 29. Suppose ğ‘‡ğ‘šğ‘–ğ‘› (ğ¶ğ‘ ) â‰¥ 3(6ğ· âˆ’ ğ‘‘), then any read or write operation ğœ‹ completes in any execution ğœ‰ of

ARES for any number of reconfiguration operations in ğœ‰.

Manuscript submitted to ACM

26Nicolas Nicolaou, Viveck Cadambe, N. Prakash, Andria Trigeorgi, Kishori M. Konwar, Muriel Medard, and Nancy Lynch

PROOF. By Lemma 28, ğ‘˜ configurations may be installed in: ğ‘‡ (ğ‘˜) â‰¥ 4ğ‘‘ (cid:205)ğ‘˜
ğ‘– + ğ‘˜ (ğ‘‡ğ‘šğ‘–ğ‘› (ğ¶ğ‘ ) + 2ğ‘‘). Also by Lemma
ğ‘–=1
(cid:17)
(cid:16)
ğ‘
ğ‘
ğ‘
ğ‘
27, we know that operation ğœ‹ takes at most ğ‘‡ (ğœ‹) â‰¤ 6ğ·
ğœğ‘’ ) âˆ’ ğœ‡ (c
. Assuming that ğ‘˜ = ğœˆ (c
ğœğ‘’ ) âˆ’ ğœ‡ (c
ğœˆ (c
ğœğ‘  ), the
ğœğ‘  ) + 2
total number of configurations observed during ğœ‹, then ğœ‹ may terminate before a ğ‘˜ + 1 configuration is added in the
configuration sequence if 6ğ· (ğ‘˜ + 2) â‰¤ 4ğ‘‘ (cid:205)ğ‘˜
. And that
â–¡
completes the lemma.

ğ‘– + ğ‘˜ (ğ‘‡ğ‘šğ‘–ğ‘› (ğ¶ğ‘ ) + 2ğ‘‘) then we have ğ‘‘ â‰¥ 3ğ·

ğ‘‡ğ‘šğ‘–ğ‘› (ğ¶ğ‘ )
2(ğ‘˜+2)

ğ‘˜ âˆ’

ğ‘–=1

7.2 Storage and Communication Costs for ARES.

Storage and Communication costs for ARES highly depends on the DAP that we use in each configuration. For our

analysis we assume that each configuration utilizes the algorithms and the DAPs presented in Section 5.

Recall that by our assumption, the storage cost counts the size (in bits) of the coded elements stored in variable ğ¿ğ‘–ğ‘ ğ‘¡ at
each server. We ignore the storage cost due to meta-data. For communication cost we measure the bits sent on the wire

between the nodes.

LEMMA 30. The worst-case total storage cost of Algorithm 5 is (ğ›¿ + 1) ğ‘›
ğ‘˜ .

PROOF. The maximum number of (tag, coded-element) pair in the ğ¿ğ‘–ğ‘ ğ‘¡ is ğ›¿ + 1, and the size of each coded element is
â–¡

ğ‘˜ while the tag variable is a metadata and therefore, not counted. So, the total storage cost is (ğ›¿ + 1) ğ‘›
1
ğ‘˜ .

We next state the communication cost for the write and read operations in Aglorithm 5. Once again, note that we ignore

the communication cost arising from exchange of meta-data.

LEMMA 31. The communication cost associated with a successful write operation in Algorithm 5 is at most ğ‘›
ğ‘˜ .

PROOF. During read operation, in the get-tag phase the servers respond with their highest tags variables, which are
ğ‘˜ each, and hence the
ğ‘›
ğ‘˜ . Therefore, we have the worst case communication cost of a write operation is
â–¡

metadata. However, in the put-data phase, the reader sends each server the coded elements of size 1
total cost of communication for this is
ğ‘›
ğ‘˜ .

LEMMA 32. The communication cost associated with a successful read operation in Algorithm 5 is at most (ğ›¿ + 2) ğ‘›
ğ‘˜ .

PROOF. During read operation, in the get-data phase the servers respond with their ğ¿ğ‘–ğ‘ ğ‘¡ variables and hence each such
ğ‘˜ . In the put-data phase, the reader
ğ‘›
ğ‘˜ . Therefore,
â–¡

list is of size at most (ğ›¿ + 1) 1
sends each server the coded elements of size 1
ğ‘˜ each, and hence the total cost of communication for this is
we have the worst case communication cost of a read operation is (ğ›¿ + 2) ğ‘›
ğ‘˜ .

ğ‘˜ , and then counting all such responses give us (ğ›¿ + 1) ğ‘›

From the above Lemmas we get.

THEOREM 33. The ARES algorithm has: (i) storage cost (ğ›¿ + 1) ğ‘›

ğ‘˜ , (ii) communication cost for each write at most to

ğ‘˜ , and (iii) communication cost for each read at most (ğ›¿ + 2) ğ‘›
ğ‘›
ğ‘˜ .

8 FLEXIBILITY OF DAPS

In this section, we argue that various implementations of DAPs can be used in ARES. In fact, via reconfig operations,
one can implement a highly adaptive atomic DSS: replication-based can be transformed into erasure-code based DSS;

increase or decrease the number of storage servers; study the performance of the DSS under various code parameters, etc.
The insight to implementing various DAPs comes from the observation that the simple algorithmic template ğ´ (see Alg. 7)
Manuscript submitted to ACM

ARES: Adaptive, Reconfigurable, Erasure coded, Atomic Storage

27

for reads and writes protocol combined with any implementation of DAPs, satisfying Property 1 gives rise to a MWMR

atomic memory service. Moreover, the read and writes operations terminate as long as the implemented DAPs complete.

Algorithm 7 Template ğ´ for the client-side read/write steps.

2:

4:

operation read()

âŸ¨ğ‘¡, ğ‘£ âŸ© â† ğ‘.get-data()
ğ‘.put-data( âŸ¨ğ‘¡, ğ‘£ âŸ©)
return âŸ¨ğ‘¡, ğ‘£ âŸ©
end operation

6: operation write(ğ‘£)
ğ‘¡ â† ğ‘.get-tag()
ğ‘¡ğ‘¤ â† ğ‘–ğ‘›ğ‘ (ğ‘¡ )
ğ‘.put-data( âŸ¨ğ‘¡ğ‘¤, ğ‘£ âŸ©)

8:

10: end operation

A read operation in ğ´ performs ğ‘.get-data() to retrieve a tag-value pair, âŸ¨ğœ, ğ‘£âŸ© from a configuration ğ‘, and then it
performs a ğ‘.put-data(âŸ¨ğœ, ğ‘£âŸ©) to propagate that pair to the configuration ğ‘. A write operation is similar to the read but
before performing the put-data action it generates a new tag which associates with the value to be written. The following
result shows that ğ´ is atomic and live, if the DAPs satisfy Property 1 and live.

THEOREM 34 (ATOMICITY OF TEMPLATE ğ´). Suppose the DAP implementation satisfies the consistency properties
ğ¶1 and ğ¶2 of Property 1 for a configuration ğ‘ âˆˆ C. Then any execution ğœ‰ of algorithm ğ´ in configuration ğ‘ is atomic and
live if each DAP invocation terminates in ğœ‰ under the failure model ğ‘.F .

PROOF. We prove the atomicity by proving properties ğ´1, ğ´2 and ğ´3 presented in Section 2 for any execution of the

algorithm.

Property ğ´1: Consider two operations ğœ™ and ğœ‹ such that ğœ™ completes before ğœ‹ is invoked. We need to show that it

cannot be the case that ğœ‹ â‰º ğœ™. We break our analysis into the following four cases:

Case (ğ‘): Both ğœ™ and ğœ‹ are writes. The ğ‘.put-data(âˆ—) of ğœ™ completes before ğœ‹ is invoked. By property C1 the tag ğœğœ‹
returned by the ğ‘.get-data() at ğœ‹ is at least as large as ğœğœ™ . Now, since ğœğœ‹ is incremented by the write operation then ğœ‹ puts
a tag ğœ â€²

ğœ‹ and hence we cannot have ğœ‹ â‰º ğœ™.

ğœ‹ such that ğœğœ™ < ğœ â€²

Case (ğ‘): ğœ™ is a write and ğœ‹ is a read. In execution ğœ‰ since ğ‘.put-data(âŸ¨ğ‘¡ğœ™, âˆ—âŸ©) of ğœ™ completes before the ğ‘.get-data()
of ğœ‹ is invoked, by property C1 the tag ğœğœ‹ obtained from the above ğ‘.get-data() is at least as large as ğœğœ™ . Now ğœğœ™ â‰¤ ğœğœ‹
implies that we cannot have ğœ‹ â‰º ğœ™.

Case (ğ‘): ğœ™ is a read and ğœ‹ is a write. Let the id of the writer that invokes ğœ‹ we ğ‘¤ğœ‹ . The ğ‘.put-data(âŸ¨ğœğœ™, âˆ—âŸ©) call of ğœ™
completes before ğ‘.get-tag() of ğœ‹ is initiated. Therefore, by property C1 get-tag(ğ‘) returns ğœ such that, ğœğœ™ â‰¤ ğœ. Since ğœğœ‹
is equal to ğ‘–ğ‘›ğ‘ (ğœ) by design of the algorithm, hence ğœğœ‹ > ğœğœ™ and we cannot have ğœ‹ â‰º ğœ™.

Case (ğ‘‘): Both ğœ™ and ğœ‹ are reads. In execution ğœ‰ the ğ‘.put-data(âŸ¨ğ‘¡ğœ™, âˆ—âŸ©) is executed as a part of ğœ™ and completes before

ğ‘.get-data() is called in ğœ‹. By property C1 of the data-primitives, we have ğœğœ™ â‰¤ ğœğœ‹ and hence we cannot have ğœ‹ â‰º ğœ™.

Property ğ´2: Note that because the tag set T is well-ordered we can show that A2 holds by first showing that every
write has a unique tag. This means that any two pair of writes can be ordered. Note that a read can be ordered w.r.t. any

write operation trivially if the respective tags are different, and by definition, if the tags are equal the write is ordered

before the read.

Observe that two tags generated from different writers are necessarily distinct because of the id component of the tag.
Now if the operations, say ğœ™ and ğœ‹ are writes from the same writer then, by well-formedness property, the second operation
will witness a higher integer part in the tag by property C1, and since the ğ‘.get-tag() is followed by ğ‘.put-data(âˆ—). Hence
ğœ‹ is ordered after ğœ™.

Manuscript submitted to ACM

28Nicolas Nicolaou, Viveck Cadambe, N. Prakash, Andria Trigeorgi, Kishori M. Konwar, Muriel Medard, and Nancy Lynch

Property ğ´3: By C2 the ğ‘.get-data() may return a tag ğœ, only when there exists an operation ğœ‹ that invoked a
ğ‘.put-data(âŸ¨ğœ, âˆ—âŸ©). Otherwise it returns the initial value. Since a write is the only operation to put a new tag ğœ in the
â–¡
system then Property ğ´3 follows from C2.

8.1 Representing Known Algorithms in terms of data-access primitives

A number of known tag-based algorithms that implement atomic read/write objects (e.g., ABD [8], FAST[17] ), can be

expressed in terms of DAP. In this subsection we demonstrate how we can transform the very celebrated ABD algorithm

[8].

MWABD Algorithm. The multi-writer version of the ABD can be transformed to the generic algorithm Template ğ´.
Algorithm 8 illustrates the three DAP for the ABD algorithm. The get-data primitive encapsulates the query phase of
MWABD, while the put-data primitive encapsulates the propagation phase of the algorithm.

Algorithm 8 Implementation of DAP for ABD at each process ğ‘ using configuration ğ‘

2:

4:

6:

8:

10:

20:

22:

24:

26:

Data-Access Primitives at process ğ‘:
procedure c.put-data(âŸ¨ğœ, ğ‘£ âŸ©))

send (WRITE, âŸ¨ğœ, ğ‘£ âŸ©) to each ğ‘  âˆˆ ğ‘.ğ‘†ğ‘’ğ‘Ÿ ğ‘£ğ‘’ğ‘Ÿğ‘ 
until âˆƒğ‘„,ğ‘„âˆˆğ‘.ğ‘„ğ‘¢ğ‘œğ‘Ÿğ‘¢ğ‘šğ‘  s.t. ğ‘ receives ACK from âˆ€ğ‘  âˆˆ ğ‘„

end procedure

procedure c.get-tag()

send (QUERY-TAG) to each ğ‘  âˆˆ ğ‘.ğ‘†ğ‘’ğ‘Ÿ ğ‘£ğ‘’ğ‘Ÿğ‘ 
until âˆƒğ‘„ , ğ‘„ âˆˆ ğ‘.ğ‘„ğ‘¢ğ‘œğ‘Ÿğ‘¢ğ‘šğ‘  s.t.

ğ‘ receives âŸ¨ğœğ‘  , ğ‘£ğ‘  âŸ© from âˆ€ğ‘  âˆˆ ğ‘„

ğœğ‘šğ‘ğ‘¥ â† max( {ğœğ‘  : ğ‘ received âŸ¨ğœğ‘  , ğ‘£ğ‘  âŸ© from ğ‘  })

Primitive Handlers at server ğ‘ ğ‘– in configuration ğ‘:

Upon receive (QUERY-TAG) from ğ‘

send ğœ to ğ‘

end receive

Upon receive (QUERY) from ğ‘

send âŸ¨ğœ, ğ‘£ âŸ© to ğ‘

end receive

12:

14:

16:

18:

28:

30:

32:

return ğœğ‘šğ‘ğ‘¥
end procedure

procedure c.get-data()

send (QUERY) to each ğ‘  âˆˆ ğ‘.ğ‘†ğ‘’ğ‘Ÿ ğ‘£ğ‘’ğ‘Ÿğ‘ 
until âˆƒğ‘„ , ğ‘„ âˆˆ ğ‘.ğ‘„ğ‘¢ğ‘œğ‘Ÿğ‘¢ğ‘šğ‘  s.t.

ğ‘ receives âŸ¨ğœğ‘  , ğ‘£ğ‘  âŸ© from âˆ€ğ‘  âˆˆ ğ‘„

ğœğ‘šğ‘ğ‘¥ â† max( {ğœğ‘  : ğ‘Ÿğ‘– received âŸ¨ğœğ‘  , ğ‘£ğ‘  âŸ© from ğ‘  })
return { âŸ¨ğœğ‘  , ğ‘£ğ‘  âŸ© : ğœğ‘  = ğœğ‘šğ‘ğ‘¥ âˆ§ ğ‘ received âŸ¨ğœğ‘  , ğ‘£ğ‘  âŸ© from ğ‘  }

end procedure

Upon receive (WRITE, âŸ¨ğœğ‘–ğ‘›, ğ‘£ğ‘–ğ‘› âŸ©) from ğ‘

if ğœğ‘–ğ‘› > ğœ then

âŸ¨ğœ, ğ‘£ âŸ© â† âŸ¨ğœğ‘–ğ‘›, ğ‘£ğ‘–ğ‘› âŸ©

send ACK to ğ‘

end receive

Let us now examine if the primitives satisfy properties C1 and C2 of Property 1. We begin with a lemma that shows

the monotonicity of the tags at each server.

LEMMA 35. Let ğœ and ğœ â€² two states in an execution ğœ‰ such that ğœ appears before ğœ â€² in ğœ‰. Then for any server ğ‘  âˆˆ S it

must hold that ğ‘ .ğ‘¡ğ‘ğ‘”|ğœ â‰¤ ğ‘ .ğ‘¡ğ‘ğ‘”|ğœâ€².

PROOF. According to the algorithm, a server ğ‘  updates its local tag-value pairs when it receives a message with a
â–¡

higher tag. So if ğ‘ .ğ‘¡ğ‘ğ‘”|ğœ = ğœ then in a state ğœ â€² that appears after ğœ in ğœ‰, ğ‘ .ğ‘¡ğ‘ğ‘”|ğœâ€² â‰¥ ğœ.

In the following two lemmas we show that property C1 is satisfied, that is if a put-data action completes, then any

subsequent get-data and get-tag actions will discover a higher tag than the one propagated by that put-data action.

LEMMA 36. Let ğœ™ be a ğ‘.put-data(âŸ¨ğœ, ğ‘£âŸ©) action invoked by ğ‘1 and ğ›¾ be a ğ‘.get-tag() action invoked by ğ‘2 in a

configuration ğ‘, such that ğœ™ â†’ ğ›¾ in an execution ğœ‰ of the algorithm. Then ğ›¾ returns a tag ğœğ›¾ â‰¥ ğœ.
Manuscript submitted to ACM

ARES: Adaptive, Reconfigurable, Erasure coded, Atomic Storage

29

PROOF. The lemma follows from the intersection property of quorums. In particular, during the ğ‘.put-data(âŸ¨ğœ, ğ‘£âŸ©)
action, ğ‘1 sends the pair âŸ¨ğœ, ğ‘£âŸ© to all the servers in ğ‘.ğ‘†ğ‘’ğ‘Ÿğ‘£ğ‘’ğ‘Ÿğ‘  and waits until all the servers in a quorum ğ‘„ğ‘– âˆˆ ğ‘.ğ‘„ğ‘¢ğ‘œğ‘Ÿğ‘¢ğ‘šğ‘ 
reply. When those replies are received then the action completes.

During a ğ‘.get-data() action on the other hand, ğ‘2 sends query messages to all the servers in ğ‘.ğ‘†ğ‘’ğ‘Ÿğ‘£ğ‘’ğ‘Ÿğ‘  and waits until
all servers in a quorum ğ‘„ ğ‘— âˆˆ ğ‘.ğ‘„ğ‘¢ğ‘œğ‘Ÿğ‘¢ğ‘šğ‘  (not necessarily different than ğ‘„ğ‘– ) reply. By definition ğ‘„ğ‘– âˆ© ğ‘„ ğ‘— â‰  âˆ…, thus any
server ğ‘  âˆˆ ğ‘„ğ‘– âˆ© ğ‘„ ğ‘— reply to both ğœ™ and ğ›¾ actions. By Lemma 35 and since ğ‘  received a tag ğœ, then ğ‘  replies to ğ‘2 with a tag
â–¡
ğœğ‘  â‰¥ ğœ. Since ğ›¾ returns the maximum tag it discovers then ğœğ›¾ â‰¥ ğœğ‘  . Therefore ğœğ›¾ â‰¥ ğœ and this completes the proof.

With similar arguments and given that each value is associated with a unique tag then we can show the following

lemma.

LEMMA 37. Let ğœ‹ be a ğ‘.put-data(âŸ¨ğœ, ğ‘£âŸ©) action invoked by ğ‘1 and ğœ™ be a ğ‘.get-data() action invoked by ğ‘2 in a
configuration ğ‘, such that ğœ‹ â†’ ğœ™ in an execution ğœ‰ of the algorithm. Then ğœ™ returns a tag-value âŸ¨ğœğœ™, ğ‘£ğœ™ âŸ© such that ğœğœ™ â‰¥ ğœ.

Finally we can now show that property C2 also holds.

LEMMA 38. If ğœ™ is a ğ‘.get-data() that returns âŸ¨ğœğœ‹ , ğ‘£ğœ‹ âŸ© âˆˆ T Ã— V, then there exists ğœ‹ such that ğœ‹ is a

ğ‘.put-data(âŸ¨ğœğœ‹ , ğ‘£ğœ‹ âŸ©) and ğœ™ â†› ğœ‹.

PROOF. This follows from the facts that (i) servers set their tag-value pair to a pair received by a put-data action, and
(ii) a get-data action returns a tag-value pair that it received from a server. So if a ğ‘.get-data() operation ğœ™ returns a
tag-value pair âŸ¨ğœğœ‹ , ğ‘£ğœ‹ âŸ©, there should be a server ğ‘  that replied to that operation with âŸ¨ğœğœ‹ , ğ‘£ğœ‹ âŸ©, and ğ‘  received âŸ¨ğœğœ‹ , ğ‘£ğœ‹ âŸ© from
â–¡
some ğ‘.put-data(âŸ¨ğœğœ‹ , ğ‘£ğœ‹ âŸ©) action, ğœ‹. Thus, ğœ‹ can proceed or be concurrent with ğœ™, and hence ğœ™ Ì¸â†’ ğœ‹.

9 EXPERIMENTAL EVALUATION

The theoretical findings suggest that ARES is an algorithm to provide robustness and flexibility on shared memory

implementations, without sacrificing strong consistency. In this section we present a proof-of-concept implementation of

ARES and we run preliminary experiments to get better insight on the efficiency and adaptiveness of ARES. In particular,

our experiments measure the latency of each read, write, and reconfig operations, and examine the persistence of
consistency even when the service is reconfigured between configurations that utilize different shared memory algorithms.

9.1 Experimental Testbed

We ran experiments on two different setups: (i) simulated locally on a single machine, and (ii) on a LAN. Both type of
experiments run on Emulab [2], an emulated WAN environment testbed used for developing, debugging, and evaluating

the systems. We used nodes with two 2.4 GHz 64-bit 8-Core E5-2630 "Haswell" processors and 64 GB RAM. In both

setups we used an external implementation of Raft[] consensus algorithms, which was used for the service reconfiguration

and was deployed on top of small RPi devices. Small devices introduced further delays in the system, reducing the speed

of reconfigurations and creating harsh conditions for longer periods in the service.

Local Experimental Setup: The local setup was used to have access to a global synchronized clock (the clock of the

local machine) in order to examine whether our algorithm preserves global ordering and hence atomicity even when

using different algorithms between configurations. Therefore, all the instances are hosted on the same physical machine

avoiding the skew between computer clocks in a distributed system. Furthermore, the use of one clock guarantees that

when an event occurs after another, it will assign a later time.

Manuscript submitted to ACM

30Nicolas Nicolaou, Viveck Cadambe, N. Prakash, Andria Trigeorgi, Kishori M. Konwar, Muriel Medard, and Nancy Lynch

Distributed Experimental Setup: The distributed experiments in Emulab enabled the examination of the performance

of the algorithm in a close to real environment. For the deployment and remote execution of the experimental tasks on the
Emulab, we used Ansible Playbooks [1]. All physical nodes were placed on a single LAN using a DropTail queue with

the default traffic parameters, i.e. 100 Mb bandwidth, and no delay or packet loss. Each physical machine runs one server

or client process. This guarantees a fair communication delay between a client and a server node.

Node Types: In all experiments, we use three distinct types of nodes, writers, readers and servers. Their main role is

listed below:

â€¢ writer ğ‘¤ âˆˆ ğ‘Š âŠ† ğ¶ : a client that sends write requests to all servers and waits for a quorum of the servers to reply
â€¢ reader ğ‘Ÿ âˆˆ ğ‘… âŠ† ğ¶: a client that sends read requests to servers and waits for a quorum of the servers to reply
â€¢ reconfigurer ğ‘” âˆˆ ğº âŠ† ğ¶: a client that sends reconfiguration requests to servers and waits for a quorum of the

servers to reply

â€¢ server ğ‘  âˆˆ ğ‘†: a server listens for read and write requests, it updates its object replica according to the atomic shared

memory and replies to the process that originated the request.

Performance Metric: The metric for evaluating the algorithms is the operational latency. This includes both commu-

nication and computational delays. The operation latency is computed as the average of all clientsâ€™ average operation

latencies. For better estimations, each experiment in every scenario was repeated 3 times.

9.2 Experimental Scenarios

In this section, we describe the scenarios we constructed and the settings for each of them. In our scenarios we constructed

the DAPs and used two different atomic storage algorithms in ARES: (i) the erasure coding based algorithm presented in

Section 5, and (ii) the ABD algorithm (see Section 8.1).

Erasure Coding: The type of erasure coding we use is (n,k)-Reed-Solomon code, which guarantees that any k of n coded

fragments is enough to reassemble the original object. The parameter k is the number of encoded data fragments, n is the

total number of servers and m is the number of parity fragments, i.e. n-k. A high number of m and consequently a small

number of k means less redundancy.

Fixed Parameters: In all scenarios the number of servers is fixed to 10. The number of writers and the value of delta are

set to 5; delta being the maximum number of concurrent put-data operations. The parity value of the EC algorithm is set

to 2, in order to minimize the redundancy; leading to 8 data servers and 2 parity servers.

It is worth mentioning that the quorum size of the EC algorithm is (cid:6) 10+8
2

(cid:7) = 9, while the quorum size of ABD algorihtm
(cid:5) + 1 = 6. In relation to the EC algorithm, we can conclude that the parameter k is directly proportional to the

is (cid:4) 10
2
quorum size. But as the value of k and quorum size increase, the size of coded elements decreases.
Distributed Experiments: For the distributed experiments we use a stochastic invocation scheme in which readers,

writers and reconfigurers pick a random time between intervals to invoke their next operations. Respectively the intervals
are [1...ğ‘Ÿğ¼ğ‘›ğ‘¡], [1..ğ‘¤ğ¼ğ‘›ğ‘¡] and [1..ğ‘Ÿğ‘’ğ‘ğ‘œğ‘›ğ¼ğ‘›ğ‘¡], where ğ‘Ÿğ¼ğ‘›ğ‘¡, ğ‘¤ğ¼ğ‘›ğ‘¡ = 2ğ‘ ğ‘’ğ‘ and ğ‘Ÿğ‘’ğ‘ğ‘œğ‘›ğ¼ğ‘›ğ‘¡ = 15ğ‘ ğ‘’ğ‘. In total, each writer performs
60 writes, each reader 60 reads and the reconfigurer if any 6 reconfigurations.

In particular, we present four types of scenarios:

â€¢ File Size Scalability (Emulab): The first scenario is made to evaluate how the read and write latencies are affected
by the size of the shared object. There are two separated runs, one for each examined storage algorithm. The

number of readers is fixed to 5, without any reconfigurers. The file size is doubled from 1 MB to 128 MB.

Manuscript submitted to ACM

ARES: Adaptive, Reconfigurable, Erasure coded, Atomic Storage

31

â€¢ Reader Scalability (Emulab): This scenario is constructed to compare the read and write latency of the system
with two different storage algorithms, while the readers increase. In particular, we execute two separate runs, one

for each storage algorithm. We use only one reconfigurer which requests recon operations that lead to the same

shared memory emulation and server nodes. The size of the file used is 4 MB.

â€¢ Changing Reconfigurations (Emulab): In this scenario, we evaluate how the read and write latencies are affected
when increasing the number of readers, while also changing the storage algorithm. We run two different runs

which differ in the way the reconfigurer chooses the next storage algorithm: (i) the reconfigurer chooses randomly

between the two storage algorithms, and (ii) the reconfigurer switches between the two storage algorithms. The

size of the file used, in both scenarios, is 4 MB.

â€¢ Consistency Persistence (Local): In this scenario we run multiple client operations in order to check if the data is
consistent across servers. The number of readers is set to 5. The readers and writers invoke their next operations
without any time delay, while the reconfigurer waits 15ğ‘ ğ‘’ğ‘ for the next invocation. In total, each writer performs
500 writes, each reader 500 reads and the reconfigurer 50 reconfigurations. The size of the file used is 4 MB.

9.3 Experimental Results

In this section, we present and explain the evaluation results of each scenario. As a general observation, the ARES

algorithm with the EC storage provides data redundancy with a lower communicational and storage cost compared to the

ABD storage that uses a strict replication technique.

File Size Scalability Results: Fig. 3(a) shows the results of the file size scalability experiments. The read and write

latencies of both storage algorithms remain in low levels until 16 MB. In bigger sizes we observe the latancies of all

operations to grow significantly. It is worth noting that the fragmentation applied by the EC algorithm, benefits its write

operations which follow a slower increasing curve than the rest of the operations. From the rest the reads seem to the one

to suffer the worst delay hit, as they are engaged in more communication phases. Nevertheless, the larger messages sent

by ABD result in slower read operations.

Reader Scalability Results: The results of reader scalability experiments can be found in Fig. 3(b). The read and write

latencies of both algorithms remain almost unchanged, while the number of readers increases. This indicates that the

system does not reach a state where it can not handle the concurrent read operations. Still, the reduced message size of

read and write operation in EC keep their latencies lower than the coresponding latencies of ABD. On the other hand, the

reconfiguration latency in both algorithms witnesses wild fluctuations between about 1 sec and 4 sec. This is probably due

to the unstable connection in the external service which handles the reconfigurations.

Changing Reconfigurations Results: Fig. 3(c) illustrates the results of experiments with the random storage change.

While, in Fig. 3(d), we can find the results of the experiments when the reconfigurer switches between storage algorithms.

During both experiments, there are cases where a single read/write operation may access configurations that implement

both ABD and EC algorithms, when concurrent with a recon operation. Thus, the latencies of such operations are

accounted in both ABD and EC latencies. As we mentioned earlier, our choice of k minimizes the coded fragment size

but introduces bigger quorums and thus larger communication overhead. As a result, in smaller file sizes, the ARES may

not benefit so much from the coding, bringing the delays of the two storage algorithms closer to each other. It is again

obvious that the reconfiguration delays is higher than the delays of all other operations.

Consistency Persistence Results: Though ARES protocol is provably strongly consistent, it is important ensure that our

implementation is correct. Validating strong consistency of an execution requires precise clock synchronization across all

processes, so that one can track operations with respect to a global time. This is impossible to achieve in a distributed

Manuscript submitted to ACM

32Nicolas Nicolaou, Viveck Cadambe, N. Prakash, Andria Trigeorgi, Kishori M. Konwar, Muriel Medard, and Nancy Lynch

(a)

(c)

(b)

(d)

Fig. 3. Simulation results.

system where clock drift is inevitable. To circumvent this, we deploy all the processes in a single beefy machine so that

every process observers the same clock running in the same physical machine.

Our checker gathers data regarding an execution, and this data includes start and end times of all the operations, as well

as other parameters like logical timestamps used by the protocol. The checker logic is based on the conditions appearing

in Lemma 13.16 [32], which provide a set of sufficient conditions for guaranteeing strong consistency. The checker

validates strong consistency property for every atomic object individually for the execution under consideration.

10 CONCLUSIONS

We presented an algorithmic framework suitable for reconfigurable, erasure code-based atomic memory service in

asynchronous, message-passing environments. We provided experimental results of our initial prototype that can store

multiple objects. We also provided a new two-round erasure code-based algorithm that has near optimal storage cost,

and Moreover, this algorithm is suitable specifically where during new configuration installation the object values passes

directly from servers in older configuration to those Future work will involve adding efficient repair and reconfiguration

using regenerating codes.

Manuscript submitted to ACM

ARES: Adaptive, Reconfigurable, Erasure coded, Atomic Storage

33

REFERENCES

[1] Ansible. https://www.ansible.com/overview/how-ansible-works.
[2] Emulab network testbed. https://www.emulab.net/.
[3] Intel storage acceleration library (open source version). https://goo.gl/zkVl4N.
[4] ABEBE, M., DAUDJEE, K., GLASBERGEN, B., AND TIAN, Y. Ec-store: Bridging the gap between storage and latency in distributed erasure coded

systems. In 2018 IEEE 38th International Conference on Distributed Computing Systems (ICDCS) (July 2018), pp. 255â€“266.

[5] AGUILERA, M. K., KEIDAR, I., MALKHI, D., AND SHRAER, A. Dynamic atomic storage without consensus. In Proceedings of the 28th ACM

symposium on Principles of distributed computing (PODC â€™09) (New York, NY, USA, 2009), ACM, pp. 17â€“25.

[6] AGUILERA, M. K., KEIDARY, I., MALKHI, D., MARTIN, J.-P., AND SHRAERY, A. Reconfiguring replicated atomic storage: A tutorial. Bulletin of

the EATCS 102 (2010), 84â€“081.

[7] ANTA, A. F., NICOLAOU, N., AND POPA, A. Making â€œfastâ€ atomic operations computationally tractable. In International Conference on Principles

Of Distributed Systems (2015), OPODISâ€™15.

[8] ATTIYA, H., BAR-NOY, A., AND DOLEV, D. Sharing memory robustly in message passing systems. Journal of the ACM 42(1) (1996), 124â€“142.
[9] BURIHABWA, D., FELBER, P., MERCIER, H., AND SCHIAVONI, V. A performance evaluation of erasure coding libraries for cloud-based data stores.

In Distributed Applications and Interoperable Systems (2016), Springer, pp. 160â€“173.

[10] CACHIN, C., AND TESSARO, S. Optimal resilience for erasure-coded byzantine distributed storage.
International Conference on (Los Alamitos, CA, USA, 2006), IEEE Computer Society, pp. 115â€“124.

In Dependable Systems and Networks,

[11] CADAMBE, V. R., LYNCH, N., MÃ‰DARD, M., AND MUSIAL, P. A coded shared atomic memory algorithm for message passing architectures. In

Network Computing and Applications (NCA), 2014 IEEE 13th International Symposium on (Aug 2014), pp. 253â€“260.

[12] CADAMBE, V. R., LYNCH, N. A., MÃ‰DARD, M., AND MUSIAL, P. M. A coded shared atomic memory algorithm for message passing architectures.

Distributed Computing 30, 1 (2017), 49â€“73.

[13] CHEN, Y. L. C., MU, S., AND LI, J. Giza: Erasure coding objects across global data centers. In Proceedings of the 2017 USENIX Annual Technical

Conference (USENIX ATC â€™17) (2017), pp. 539â€“551.

[14] CHOCKLER, G., GILBERT, S., GRAMOLI, V., MUSIAL, P. M., AND SHVARTSMAN, A. A. Reconfigurable distributed storage for dynamic networks.

Journal of Parallel and Distributed Computing 69, 1 (2009), 100â€“116.

[15] CHOCKLER, G., AND MALKHI, D. Active disk paxos with infinitely many processes. Distributed Computing 18, 1 (2005), 73â€“84.
[16] DUTTA, P., GUERRAOUI, R., AND LEVY, R. R. Optimistic erasure-coded distributed storage. In DISC â€™08: Proceedings of the 22nd international

symposium on Distributed Computing (Berlin, Heidelberg, 2008), Springer-Verlag, pp. 182â€“196.

[17] DUTTA, P., GUERRAOUI, R., LEVY, R. R., AND CHAKRABORTY, A. How fast can a distributed atomic read be? In Proceedings of the 23rd ACM

symposium on Principles of Distributed Computing (PODC) (2004), pp. 236â€“245.

[18] FAN, R., AND LYNCH, N. Efficient replication of large data objects. In Distributed algorithms (2003), F. E. Fich, Ed., vol. 2848 of Lecture Notes in

Computer Science, pp. 75â€“91.

[19] FERNÃNDEZ ANTA, A., HADJISTASI, T., AND NICOLAOU, N. Computationally light â€œmulti-speedâ€ atomic memory. In International Conference on

Principles Of Distributed Systems (2016), OPODISâ€™16.

[20] GAFNI, E., AND MALKHI, D. Elastic Configuration Maintenance via a Parsimonious Speculating Snapshot Solution. In International Symposium on

Distributed Computing (2015), Springer, pp. 140â€“153.

[21] GEORGIOU, C., NICOLAOU, N. C., AND SHVARTSMAN, A. A. On the robustness of (semi) fast quorum-based implementations of atomic shared
memory. In DISC â€™08: Proceedings of the 22nd international symposium on Distributed Computing (Berlin, Heidelberg, 2008), Springer-Verlag,
pp. 289â€“304.

[22] GEORGIOU, C., NICOLAOU, N. C., AND SHVARTSMAN, A. A. Fault-tolerant semifast implementations of atomic read/write registers. Journal of

Parallel and Distributed Computing 69, 1 (2009), 62â€“79.

[23] GILBERT, S. RAMBO II: Rapidly reconfigurable atomic memory for dynamic networks. Masterâ€™s thesis, MIT, August 2003.
[24] GILBERT, S., LYNCH, N., AND SHVARTSMAN, A. RAMBO II: Rapidly reconfigurable atomic memory for dynamic networks. In Proceedings of

International Conference on Dependable Systems and Networks (DSN) (2003), pp. 259â€“268.

[25] HERLIHY, M. P., AND WING, J. M. Linearizability: a correctness condition for concurrent objects. ACM Transactions on Programming Languages

and Systems 12, 3 (1990), 463â€“492.

[26] HUFFMAN, W. C., AND PLESS, V. Fundamentals of error-correcting codes. Cambridge university press, 2003.
[27] JEHL, L., VITENBERG, R., AND MELING, H. Smartmerge: A new approach to reconfiguration for atomic storage. In International Symposium on

Distributed Computing (2015), Springer, pp. 154â€“169.

[28] JOSHI, G., SOLJANIN, E., AND WORNELL, G. Efficient redundancy techniques for latency reduction in cloud systems. ACM Transactions on

Modeling and Performance Evaluation of Computing Systems (TOMPECS) 2, 2 (2017), 12.

[29] KONWAR, K. M., PRAKASH, N., KANTOR, E., LYNCH, N., MÃ‰DARD, M., AND SCHWARZMANN, A. A. Storage-optimized data-atomic algorithms
for handling erasures and errors in distributed storage systems. In 2016 IEEE International Parallel and Distributed Processing Symposium (IPDPS)
(May 2016), pp. 720â€“729.

Manuscript submitted to ACM

34Nicolas Nicolaou, Viveck Cadambe, N. Prakash, Andria Trigeorgi, Kishori M. Konwar, Muriel Medard, and Nancy Lynch

[30] KONWAR, K. M., PRAKASH, N., LYNCH, N., AND MÃ‰DARD, M. Radon: Repairable atomic data object in networks. In The International Conference

on Distributed Systems (OPODIS) (2016).

[31] LAMPORT, L. The part-time parliament. ACM Transactions on Computer Systems 16, 2 (1998), 133â€“169.
[32] LYNCH, N. Distributed Algorithms. Morgan Kaufmann Publishers, 1996.
[33] LYNCH, N., AND SHVARTSMAN, A. RAMBO: A reconfigurable atomic memory service for dynamic networks. In Proceedings of 16th International

Symposium on Distributed Computing (DISC) (2002), pp. 173â€“190.

[34] LYNCH, N. A., AND SHVARTSMAN, A. A. Robust emulation of shared memory using dynamic quorum-acknowledged broadcasts. In Proceedings of

Symposium on Fault-Tolerant Computing (1997), pp. 272â€“281.

[35] NAKAMOTO, S. Bitcoin: A peer-to-peer electronic cash system.
[36] NICOLAOU, N., CADAMBE, V., KONWAR, K., PRAKASH, N., LYNCH, N., AND MÃ‰DARD, M. Ares: Adaptive, reconfigurable, erasure coded,

atomic storage. CoRR abs/1805.03727 (2018).

[37] ONGARO, D., AND OUSTERHOUT, J. In search of an understandable consensus algorithm. In Proceedings of the 2014 USENIX Conference on

USENIX Annual Technical Conference (Berkeley, CA, USA, 2014), USENIX ATCâ€™14, USENIX Association, pp. 305â€“320.

[38] RASHMI, K., CHOWDHURY, M., KOSAIAN, J., STOICA, I., AND RAMCHANDRAN, K. Ec-cache: Load-balanced, low-latency cluster caching with

online erasure coding. In OSDI (2016), pp. 401â€“417.

[39] SHRAER, A., MARTIN, J.-P., MALKHI, D., AND KEIDAR, I. Data-centric reconfiguration with network-attached disks. In Proceedings of the 4th

Intâ€™l Workshop on Large Scale Distributed Sys. and Middleware (LADIS â€™10) (2010), p. 22â€“26.

[40] SPIEGELMAN, A., KEIDAR, I., AND MALKHI, D. Dynamic Reconfiguration: Abstraction and Optimal Asynchronous Solution. In 31st International

Symposium on Distributed Computing (DISC 2017) (2017), vol. 91, pp. 40:1â€“40:15.

[41] WANG, S., HUANG, J., QIN, X., CAO, Q., AND XIE, C. Wps: A workload-aware placement scheme for erasure-coded in-memory stores. In

Networking, Architecture, and Storage (NAS), 2017 International Conference on (2017), IEEE, pp. 1â€“10.

[42] XIANG, Y., LAN, T., AGGARWAL, V., AND CHEN, Y.-F. R. Multi-tenant latency optimization in erasure-coded storage with differentiated services.

In 2015 IEEE 35th International Conference on Distributed Computing Systems (ICDCS) (2015), IEEE, pp. 790â€“791.

[43] XIANG, Y., LAN, T., AGGARWAL, V., CHEN, Y.-F. R., XIANG, Y., LAN, T., AGGARWAL, V., AND CHEN, Y.-F. R. Joint latency and cost

optimization for erasure-coded data center storage. IEEE/ACM Transactions on Networking (TON) 24, 4 (2016), 2443â€“2457.

[44] YU, Y., HUANG, R., WANG, W., ZHANG, J., AND LETAIEF, K. B. Sp-cache: load-balanced, redundancy-free cluster caching with selective partition.
In Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis (2018), IEEE Press, p. 1.
[45] ZHANG, H., DONG, M., AND CHEN, H. Efficient and available in-memory kv-store with hybrid erasure coding and replication. In 14th USENIX

Conference on File and Storage Technologies (FAST 16) (Santa Clara, CA, 2016), USENIX Association, pp. 167â€“180.

[46] ZHOU, P., HUANG, J., QIN, X., AND XIE, C. Pars: A popularity-aware redundancy scheme for in-memory stores. IEEE Transactions on Computers

(2018), 1â€“1.

Manuscript submitted to ACM

