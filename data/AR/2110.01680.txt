1
2
0
2

t
c
O
4

]

V
C
.
s
c
[

1
v
0
8
6
1
0
.
0
1
1
2
:
v
i
X
r
a

How You Move Your Head Tells What You Do: Self-supervised Video
Representation Learning with Egocentric Cameras and IMU Sensors

Satoshi Tsutsui*
Indiana University
stsutsui@indiana.edu

Ruta Desai
Facebook Reality Labs
rutadesai@fb.com

Karl Ridgeway
Facebook Reality Labs
karl.ridgeway@fb.com

Abstract

1. Introduction

Understanding users’ activities from head-mounted
cameras is a fundamental task for Augmented and Virtual
Reality (AR/VR) applications. A typical approach is to train
a classiﬁer in a supervised manner using data labeled by
humans. This approach has limitations due to the expensive
annotation cost and the closed coverage of activity labels.
A potential way to address these limitations is to use self-
supervised learning (SSL). Instead of relying on human an-
notations, SSL leverages intrinsic properties of data to learn
representations. We are particularly interested in learning
egocentric video representations beneﬁting from the head-
motion generated by users’ daily activities, which can be
easily obtained from IMU sensors embedded in AR/VR de-
vices. Towards this goal, we propose a simple but effective
approach to learn video representation by learning to tell
the corresponding pairs of video clip and head-motion. We
demonstrate the effectiveness of our learned representation
for recognizing egocentric activities of people and dogs.

Figure 1: Our SSL model maximizes agreement between
the representations of video and head-motion captured by a
head-mounted camera with IMU sensors. We extract CNN
features of a video clip and a head-motion clip, and max-
imize the similarity between these features, if they are the
corresponding pair.

*Research conducted during an internship at Facebook Reality Labs.

AR/VR technology is ﬁnally starting to ﬂourish with
the advent of consumer head-mounted devices like Oculus,
Ray-Ban Stories, HoloLens, etc. These devices have the
potential to fundamentally revolutionize our daily lives and
our society – in a manner similar to what smartphones did in
the previous decades. To enable such AR/VR applications,
one of the fundamental challenges that requires to be solved
is egocentric action recognition – machine understanding of
users’ activities from head-mounted cameras.

With the progress of modern computer vision technolo-
gies, the now-familiar approach for action recognition is to
train convolutional neural networks (CNNs) in a supervised
manner using millions of video clips that are manually cat-
egorized into egocentric actions. This approach, however,
has at least two limitations. First, annotating large enough
video clips to train CNNs is very expensive. Second, even
if we had unlimited budgets, we would not be able to cover
all of the actions that humans could do.

One of the promising ways to address these limitations is
to train CNNs using self-supervised learning (SSL) [1, 2],
which has been making rapid progress these days.
In-
stead of relying on human annotations, SSL utilizes intrin-
sic properties existing in the data (e.g., invariance over data-
augmentation [2], multi-modality of the data [1], etc.) for
training representations for various downstream tasks in-
cluding recognition. Inspired by these, we are particularly
interested in using head-motion data as a self-supervision
signal for egocentric action recognition. Our intuition to
leverage head-motion for SSL of egocentric video represen-
tations is based on two main factors. First, head-motion is
inherently related to users’ activities. For instance, head
and gaze usually precede picking up/putting down actions;
similarly head-motion can also give away movements and
change of focus (see Figure 2). Secondly, head-motion data
can be easily accessible in AR/VR application scenarios via
affordable, on-board IMU sensors on egocentric devices.

To harness the potential of head-motion data for SSL of
egocentric video representations, a few fundamental ques-

 
 
 
 
 
 
(a) Walk/Move

(b) Putting an object down

(c) Change views/focuses

Figure 2: Head-motion captured synchronously with egocentric videos by the IMU sensors embedded in a head-mounted
camera. Head-motion can capture a lot of information about the camera wearer’s actions such as walking/moving (a), moving,
reaching to an object, putting an object down (b), or changing view/focus (c).

tions need to be answered – Does head-motion data have
unique information that is not captured by the egocentric
video representation? If so, what is an effective way to uti-
lize the useful signals in head-motion to beneﬁt egocentric
video representation learning? Finally, do the learned repre-
sentations work better than those trained with SSL on video
only data? In this work, we systematically answer these
research questions. We empirically show that head-motion
can provide additional advantages over video even for fully
supervised learning. We then design a simple but effective
SSL approach to learn egocentric video representations by
classifying pairs of videos and head-motion data based on
their correspondence (Figure 1). We train our model on the
EPIC-KITCHENS dataset using this approach and show the
effectiveness of resultant representation for the downstream
task of classifying actions in kitchen tasks. Furthermore,
we also leverage the same representation to recognize dog-
centric activities induced by dogs’ head motion, demon-
strating that our learned representation generalizes beyond
the training domain.

2. Method

SSL task formulation Inspired by the limitations of la-
beled datasets, we aim to learn egocentric video representa-
tions using SSL for AR/VR applications. In particular, we
want to leverage the multimodal data available in AR/VR
– egocentric video and head-motion captured by a head-
mounted camera with IMU sensors. SSL usually utilizes
a proxy task to train representations without human annota-
tions. For instance, we can learn image representations with
a contrastive loss by maximizing the agreement between
two different augmented views of the same image [2]. That
is, given a pair of randomly augmented images, their repre-
sentations are encouraged to be similar if they are from the
same image, and not if from different images. An extension
of this for a multimodal case is to train on the correspon-
dence between two modalities such as audio and video [1].
Inspired by this audio-visual SSL framework, we propose a

binary classiﬁcation task to match the correspondence be-
tween egocentric video and IMU signals of head-motion
captured by a head-mounted camera for learning egocentric
video representations in AR/VR.

SSL loss To train representations using the above SSL
task, we randomly sample a batch of short (2 seconds in our
experiments) video clips synchronized with head-motion
signals captured by head-mounted IMU sensors. We then
extract the feature vectors of video and IMU, compute the
pairwise similarities, and encourage the similarities to be
high only if they are from the same clip (Figure 1). Specif-
ically, given {(vi, mi)}N
i=1 – a batch with N pairs of video
and head-motion feature vectors from CNNs, we minimize
the following contrastive loss function L.

L =

1
N 2

N
(cid:88)

N
(cid:88)

(cid:32)

i=1

j=1

(cid:80)N

exp(sim(vi, mj))
k=1 exp(sim(vi, mk))
(cid:33)

+

exp(sim(vi, mj))
k=1 exp(sim(vk, mj))

(cid:80)N

(1)

,

where sim() is the cosine similarity i.e. sim(vi, mj) =
(cid:62)mj
vi
(cid:107)vi(cid:107)(cid:107)mj (cid:107) .

After the SSL training, we can use the video representa-
tion vi (and also head-motion representation mi if needed)
for downstream tasks such as action recognition.

3. Experiments

3.1. Dataset and backbone details

We use the EPIC-KITCHENS [3] dataset for all of ex-
periments except the last one. For the ﬁnal experiment,
we use the DogCentric Activity dataset [4] in order to
show generalization of our approach beyond the training
dataset. For the EPIC-KITCHENS dataset, we select the
video clips accompanied by the corresponding IMU signals
of head (camera) motion, and made our own data split of

2

Action
(Verb)

Take
Put
Wash
Open
Close

Model

Video Head-Motion Ensemble

74.73
63.89
68.35
69.35
47.21

52.75
37.71
41.73
9.58
18.78

76.92
66.88
70.36
65.90
42.13

Table 1: Head-motion helps action recognition: Fully-
supervised action classiﬁcation accuracy (%) from the video
only model, the head-motion only, and their ensemble.

train:validation:test = 30044:3032:4379 based on video ids
with no overlapping subjects among the splits. This split
has 65 unique test verbs, which means the random guess
baseline can achieve the accuracy of 1.5%. However, due
to the biased distribution of actions, the major action (take)
dominates the 27% of testing set. For experiments using
the DogCentric Activity dataset, we choose the activity cat-
egories related to actions with head-motion: Walk, Shake,
Look at left, and Look at right. These four actions are al-
most balanced and the majority class of Walk dominates the
30% of the dataset. This dataset is small (total 216 video
clips and 86 after our selection), so we split into the half-
and-half based on dog ids and perform 2-fold cross valida-
tion and report the mean accuracy.

In order to train representations with SSL loss described
in eq. 1, we use SlowFast50 as the backbone CNN repre-
sentation for video and VGG16 for representing the head-
motion IMU signals. The spatiotemporal input size of the
video CNN is 256 × 256 × 48, corresponding to the width,
height, and frame size (with the frame rate of 24fps), respec-
tively. A raw IMU clip is represented with a matrix shaped
with 396 × 6, corresponding to time (with the frequency of
198Hz) and channels (XYZs for accelerometers and gyro-
scopes), respectively. Our ways of handling IMU signals
are based on [5], where ordinary image classiﬁcation CNNs
can be used after extracting spectrograms with nf f t = 64.

3.2. Evaluating the utility of head-motion signals

Our goal is to utilize head-motion to learn better egocen-
tric video representation for action recognition. However,
since video is a rich modality with high ﬁdelity of informa-
tion, is there any room left for the head motion signals to
improve the video representation for action recognition? To
answer this question, we perform two preliminary experi-
ments.

The ﬁrst experiment is to train an action classiﬁer from
head-motion signals and compare with a video only classi-
ﬁer. We expect that the video-based classiﬁer will achieve
the higher action classiﬁcation accuracy. However, if some

Action
(Verb)

Take
Put
Wash
Open
Close

Video
Only

384
365
191
161
73

Correctly Classiﬁed by
Head-Motion
Only

Both Neither

124
120
59
5
17

500
233
148
20
20

175
218
98
75
87

Table 2: Head-motion helps action recognition: The num-
ber of correctly classiﬁed action clips from video only,
head-motion only, both video and head-motion, and neither
of them. Even thought video is usually superior due to the
richer content, there exits action clips that can be classiﬁed
using head-motion only.

Video CNN Pretrained on
Kinetics EPIC-KITCHENS

Freeze Video CNN
Update Video CNN

87.33
92.8

86.44
94.73

Table 3: Head-motion information is not inherently cap-
tured in existing video representations: ROC-AUC (%) on
the SSL correspondence task of matching egocentric video
and head-motion. Irrespective of how the video representa-
tion is pretrained, updating video CNN will give some gain
on the task, which suggests a room for improving the video
representation by incorporating head-motion. Note that the
head-motion CNN is always updated.

classes can be correctly classiﬁed only by head-motion sig-
nals, then this would imply that head-motion indeed has
an advantage over video at least for certain classes. We
show the classiﬁcation results of the top ﬁve frequent ac-
tions (verbs) in Table 1 and 2. The classiﬁer from video
has higher accuracy on average, which is what we expected.
However, some action clips are correctly classiﬁed only
from head-motion (Table 2). Moreover, we also add a sim-
ple ensemble model by averaging the probability vectors
(i.e. outputs after the softmax function) of the two classi-
ﬁers, and conﬁrm the improvement on the overall accuracy
(Table 1). These results demonstrate an advantage of the
head-motion signals over the video.

The second experiment is to see if existing video rep-
resentations (e.g. CNN features pretrained on Kinetics) al-
ready capture head-motion information or not. This ques-
tion is important because,
if video representations pre-
trained without head-motion already contain all the infor-
mation that can be extracted from head-motion, we cannot
add any additional value into the video representation by
using head-motion. To answer this question, we initialize

3

Representation

Acc.

Representation

Acc.

Fully-Supervised on Kinetics
Self-Supervised on EPIC-KITCHENS
Fully-Supervised on EPIC-KITCHENS

36.58
41.94 (Ours)
55.61

Table 4: Head-motion-based SSL pretraining improves
downstream classiﬁcation accuracy: Action classiﬁcation
accuracy (%) on EPIC-KITCHENS dataset using represen-
tations learned in different ways.

Fully-Supervised on Kinetics
Self-Supervised on EPIC-KITCHENS

46.98
54.21 (Ours)

Table 5: Video representations pretrained using head-
motion-based SSL on kitchen domain are also useful for out
of domain tasks: Action classiﬁcation accuracy (%) on Dog-
Centric Activity dataset using a linear classiﬁer on different
representations.

the video CNN using pretrained weights from Kinetics or
EPIC-KITCHENS, and compare the accuracy of our SSL
task of matching the correspondence between video and
head-motion in two different settings. In the ﬁrst setting,
we train our model (Figure 1) with frozen pretreind video
CNN and only update the head-motion CNN weights. In the
second setting, we update both the video and head-motion
CNN weights. We compare the resulting ROC-AUC accu-
racies of SSL correspondence classiﬁcation task for both the
settings – without and with the update of the video CNN
weights (Table 3). We see an increased performance for
both CNNs pretrained on Kinetics and EPIC-KITCHENS.
Our interpretation is that updating the video CNN weights
will not provide any gain on the accuracy if the head-motion
information is already embedded in the pretrained video
representation. The increased performance indicates that
we still have room to improve the video representation by
utilizing head-motion. Note that we use ROC-AUC instead
of the plain accuracy because most of the pairs are negative
correspondence (i.e. always classifying as negative achieves
the high plain accuracy).

3.3. Leveraging head-motion for action recognition

3.3.1 EPIC-KITCHEN action classiﬁcation using SSL

pretrained representation

After training our model (Figure 1) using our SSL task
(eq. 1), we can leverage the learned video CNN as a generic
video representation backbone for downstream tasks such
as egocentric action classiﬁcation. To test the effectiveness
of our video representations learned using SSL, we trained a
linear classiﬁer of multiclass logistic regression (or softmax
regression) on top of the learned video representation. We
also train the same linear classiﬁer on top of the representa-
tions learned using fully supervised training for action clas-
siﬁcation of Kinetics and EPIC-KITCHENS and compare
the results (Table 4). The classiﬁer that uses the represen-
tation learned using SSL achieves the accuracy of 41.94%.
This is higher than the accuracy (27.01%) of Kinetics rep-
resentation, and lower than the fully supervised counterpart
(55.61%) of EPIC-KITCHENS. While our SSL representa-
tion pre-training is thus effective, it is still behind the fully-

4

supervised counterpart. We therefore wish to close this gap
in our future work.

3.3.2 Generalization to DogCentric actions

We also want to see if the representation learned using our
SSL task (eq. 1) generalizes beyond the training domain
of kitchens. To test this, we train a linear classiﬁer (mul-
ticlass logistic/softmax regression) on DogCentric Activ-
ity Dataset using our pretrained SSL representation from
EPIC-KICTHEN. We show the results in Table 5. While the
classiﬁer based on Kinetics representation has the accuracy
of 46.98%, our SSL representation achieves 54.21%. This
indicates the effectiveness of our SSL approach beyond the
training domain.

4. Conclusion

We explored a self-supervised learning (SSL) of video
representation by leveraging multimodal egocentric video
streams and head-motion captured by IMU sensors for
AR/VR applications. While video has much richer informa-
tion, there’s still room to improve the video representation
using head-motion information. Our SSL task, which is tell
a simple video-motion correspondence, can train represen-
tations effective for egocentric action recognition.

References

[1] Relja Arandjelovic and Andrew Zisserman. Objects that

sound. In ECCV, 2018.

[2] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geof-
frey Hinton. A simple framework for contrastive learning of
visual representations. In ICML, 2020.

[3] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, , An-
tonino Furnari, Jian Ma, Evangelos Kazakos, Davide Molti-
santi, Jonathan Munro, Toby Perrett, Will Price, and Michael
Wray. Rescaling Egocentric Vision. IJCV, 2021.

[4] Yumi Iwashita, Asamichi Takamine, Ryo Kurazume, and
Michael S Ryoo. First-person animal activity recognition
from egocentric videos. In ICPR, 2014.

[5] Gierad Laput and Chris Harrison. Sensing ﬁne-grained hand

activity with smartwatches. In CHI, 2019.

