2
2
0
2

l
u
J

6

]
T
S
.
h
t
a
m

[

1
v
6
4
5
2
0
.
7
0
2
2
:
v
i
X
r
a

ADAPTIVE DEEP LEARNING FOR NONPARAMETRIC TIME SERIES
REGRESSION

DAISUKE KURISU, RIKU FUKAMI, AND YUTA KOIKE

Abstract. In this paper, we develop a general theory for adaptive nonparametric estimation of
mean functions of nonstationary and nonlinear time series using deep neural networks (DNNs). We
ﬁrst consider two types of DNN estimators, non-penalized and sparse-penalized DNN estimators,
and establish their generalization error bounds for general nonstationary time series. We then derive
minimax lower bounds for estimating mean functions belonging to a wide class of nonlinear autore-
gressive (AR) models that include nonlinear generalized additive AR, single index, and threshold
AR models. Building upon the results, we show that the sparse-penalized DNN estimator is adap-
tive and attains the minimax optimal rates up to a poly-logarithmic factor for many nonlinear
AR models. Through numerical simulations, we demonstrate the usefulness of the DNN methods
for estimating nonlinear AR models with intrinsic low-dimensional structures and discontinuous or
rough mean functions, which is consistent with our theory.

1. Introduction

Motivated by the great success of deep neural networks (DNNs) in several applications such
as pattern recognition and natural language processing, there has been an increasing interest in
revealing the reason why DNNs work well from the statistical point of view. In the past few years,
many researchers have contributed to understand theoretical advantages of DNN estimates for non-
parametric regression models. See, for example, Bauer and Kohler (2019), Imaizumi and Fukumizu
(2019), Schmidt-Hieber (2019, 2020), Suzuki (2019), Hayakawa and Suzuki (2020), Nakada and
Imaizumi (2020), Kohler and Langer (2021), Suzuki and Nitanda (2021), Tsuji and Suzuki (2021),
and references therein.

In contrast to the recent progress of DNNs, theoretical results on statistical properties of DNN
methods for stochastic processes are scarce. As exceptional studies, we refer to Kohler and Krzyzak
(2020), Ogihara (2021), and Oga and Koike (2021). Kohler and Krzyzak (2020) consider a time
series prediction problem and investigate the convergence rate of a deep recurrent neural network
estimate. Ogihara (2021) considers DNN estimation for the diﬀusion matrices and studies their
estimation errors as misspeciﬁed parametric models. Oga and Koike (2021) investigate nonpara-
metric drift estimation of a multivariate diﬀusion process. Notably, there seems no theoretical
results on the statistical properties of feed-forward DNN estimators for nonparametric estimation
of the mean function of a nonlinear and possibly nonstationary time series.

Date: First version: January 17, 2022. This version: July 7, 2022.
Key words and phrases. nonparametric regression, time series, adaptive estimation, deep neural network
MSC2020 subject classiﬁcations: 62G08, 62M10, 68T07.
D. Kurisu is partially supported by JSPS KAKENHI Grant Number 20K13468. Y. Koike is partially supported
by JST CREST Grant Number JPMJCR2115 and JSPS KAKENHI Grant Number 19K13668. The authors would
like to thank Taiji Suzuki for his helpful comments.

1

 
 
 
 
 
 
The goal of this paper is to develop a general theory for adaptive nonparametric estimation of
the mean function of a nonlinear time series using DNNs. The contributions of this paper are as
follows.

First, we provide bounds of (i) generalization error (Lemma A.1) and (ii) expected empirical
error (Lemma A.2) of general estimators of the mean function of a nonlinear and possibly nonsta-
tionary time series. These results are of independent theoretical interest since they can be useful
to investigate asymptotic properties of nonparametric estimators including DNNs. Building upon
the results, we establish a generalization error bound of DNN estimators (Theorem 3.1).

Second, we consider a sparse-penalized DNN estimator which is deﬁned as a minimizer of an
empirical risk with penalization by the clipped L1-norm and develop its asymptotic properties. In
particular, we establish a generalization error bound of the sparse penalized DNN estimator (The-
orem 3.2). This enables us to estimate mean functions of nonlinear time series models adaptively.
Our work can be viewed as extensions of the results in Schmidt-Hieber (2020) and Ohn and Kim
(2022) for independent observations to nonstationary time series. From the technical point of view,
our analysis is related to the strategy in Schmidt-Hieber (2020). Due to the existence of temporal
dependence, the extensions are nontrivial and we achieve this by developing a new strategy to obtain
generalization error bounds for dependent data using a blocking technique for β-mixing processes
and exponential inequalities for self-normalized martingale diﬀerence sequences. It shall be noted
that our approach is also quite diﬀerent from that of Ohn and Kim (2022) since their approach
strongly depends on the independence of observations and our generalization error bounds of the
sparse penalized DNN estimator improve the power of the logs in their bounds. More detailed
diﬀerences are discussed in Section 3.3. Our approach to derive generalization error bounds paves
a way to new techniques for studying statistical properties of machine learning methods for more
richer classes of models for dependent data including time series and spatial data.

Third, we establish that the sparse-penalized DNN estimators achieve minimax rates of con-
vergence up to a poly-logarithmic factor over a wide class of nonlinear AR(d) processes including
generalized additive AR models and functional coeﬃcient AR models introduced in Chen and Tsay
(1993) that allow discontinuous mean functions. When the mean function belongs to a class of
suitably smooth functions (e.g., H¨older space), one can use other nonparametric estimators for
adaptively estimating the mean function (see Hoﬀmann (1999) for example). Similar assumptions
on the smoothness of the mean functions have been made in most papers that investigate non-
parametric estimation of the mean function of a nonlinear time series regression model (Robinson
(1983), Tran (1993), Truong (1994), Masry (1996a,b), Hoﬀmann (1999), Fan and Yao (2008), Zhao
and Wu (2008), Hansen (2008), and Liu and Wu (2010)). However, the methods in those papers
cannot be applied for estimating nonlinear time series models with possibly discontinuous mean
functions. Our results show that the sparse-penalized DNN estimation is a uniﬁed method for adap-
tively estimating both smooth and discontinuous mean functions of time series regression models.
Further, we shall note that the sparse-penalized DNN estimators attain the parametric rate of
convergence up to a logarithmic factor when the mean functions belong to an (cid:96)0-bounded aﬃne
class that include (multi-regime) threshold AR processes (Theorems 4.3 and 4.4).

In addition to the theoretical results, we also conduct simulation studies to investigate the ﬁnite
sample performance of the DNN estimators. We ﬁnd that the DNN methods work well for the
models with (i) intrinsic low-dimensional structures and (ii) discontinuous or rough mean functions.
These results are consistent with our main results.

2

To summarize, this paper contributes to the literature of nonparametric estimation of nonlinear
and nonstationary time series by establishing (i) the theoretical validity of non-penalized and sparse-
penalized DNN estimators for the adaptive nonparametric esitmation of the mean function of
nonlinear time series regression and (ii) show the optimality of the sparse-penalized DNN estimator
for a wide class of nonlinear AR processes.

The rest of the paper is organized as follows. In Section 2, we introduce nonparametric regression
models considered in this paper. In Section 3, we provide generalization error bounds of (i) the
non-penalized and (ii) the sparse-penalized DNN estimators. In Section 4, we present the minimax
optimality of the sparse-penalized DNN estimators and show that the estimators achieve the mini-
max optimal convergence rate up to a logarithmic factor over (i) composition structured functions
and (ii) (cid:96)0-bounded aﬃne classes. In Section 5, we investigate ﬁnite sample properties of the DNN
estimators and compare their performance with other estimators (kernel ridge regression, k-nearest
neighbors, and random forest) via numerical simulations. Section 6 concludes and discusses possible
extensions. All the proofs are included in Appendix.

1.1. Notations. For any a, b ∈ R, we write a ∨ b = max{a, b} and a ∧ b = min{a, b}. For x ∈ R,
(cid:98)x(cid:99) denotes the largest integer ≤ x. Given a function f deﬁned on a subset of Rd containing
[0, 1]d, we denote by f |[0,1]d the restriction of f to [0, 1]d. When f is real-valued, we write (cid:107)f (cid:107)∞ :=
supx∈[0,1]d |f (x)| for the supremum on the compact set [0, 1]d. Also, let supp(f ) denote the support
of the function f . For a vector or matrix W , we write |W | for the Frobenius norm (i.e.
the
Euclidean norm for a vector), |W |∞ for the maximum-entry norm and |W |0 for the number of
non-zero entries. For any positive sequences an, bn, we write an (cid:46) bn if there is a positive constant
C > 0 independent of n such that an ≤ Cbn for all n, an (cid:16) bn if an (cid:46) bn and bn (cid:46) an.

2. Settings

Let (Ω, G, {Gt}t≥0, P) be a ﬁltered probability space. Consider the following nonparametric time

series regression model:

Yt = m(Xt) + η(Xt)vt, t = 1, . . . , T,

(2.1)

where T ≥ 3, (Yt, Xt) ∈ R × Rd, and {Xt, vt}T
t=1 is a sequence of random vectors adapted to
the ﬁltration {Gt}T
In this paper we investigate
nonparametric estimation of the mean function m on the compact set [0, 1]d, that is, f0 := m1[0,1]d.
The model (2.1) covers a range of nonlinear time series models.

t=1. We assume Cη := supx∈[0,1]d |η(x)| < ∞.

Example 2.1 (Nonlinear AR(p)-ARCH(q) model). Consider a nonlinear AR model

Yt = (cid:101)m(Yt−1, . . . , Yt−p) + (γ0 + γ1Y 2

t−1 + · · · + γqY 2

t−q)1/2vt,

where γ0 > 0, γi ≥ 0, i = 1, . . . , q with 1 ≤ p, q ≤ d. This example corresponds to the model (2.1)
with Xt = (Yt−1, . . . , Yt−d)(cid:48), m(x1, . . . , xd) = (cid:101)m(x1, . . . , mp) and η(x1, . . . , xd) = (γ0 + γ1x2
1 + · · · +
γqx2

q)1/2.

Example 2.2 (Multivariate nonlinear time series). Consider the case that we observe multivariate
t=1 and {Xt = (X1,t, . . . , Xd,t)(cid:48)}T
time series {Yt = (Y1,t, . . . , Yp,t)(cid:48)}T
Yj,t = mj(Xt) + ηj(Xt)vj,t, j = 1, . . . , p.

(2.2)
The model (2.2) corresponds to (i) multivariate nonlinear AR model when Xt = (Y(cid:48)
t−q)(cid:48)
for some q ≥ 1 and (ii) multivariate nonlinear time series regression with exogenous variables
3

t=1 such that

t−1, . . . , Y(cid:48)

t=1 is uncorrelated with {vt = (v1,t, . . . , vp,t)(cid:48)}T
when ηj(·) = 1 and {Xt}T
t=1. If one is interested in
estimating the mean function m = (m1, . . . , mp)(cid:48) : Rd → Rp, then it is enough to estimate each
component mj. In this case, the problem of estimating mj is reduced to that of estimating the
mean function m of the model (2.1).

Example 2.3 (Time-varying nonlinear models). Consider a nonlinear time-varying model

Yt = m

(cid:18) t
T

, Yt−1, . . . , Yt−p

+ η

(cid:19)

, Yt−1, . . . , Yt−q

vt,

(cid:19)

(cid:18) t
T

(2.3)

where 1 ≤ p, q ≤ d−1. This example corresponds to the model (2.1) with Xt = (t/T, Yt−1, . . . , Yt−(d−1))(cid:48)
as well as m and η regarded as functions on Rd in the canonical way. The model (2.3) covers, for in-
stance, time-varying AR(p)-ARCH(q) models when m(u, x1, . . . , xp) = m0(u) + (cid:80)p
j=1 mj(u)xj and
η(u, x1, . . . , xq) = (η0(u)+(cid:80)q
j )1/2 with some functions mj : [0, 1] → R, ηj : [0, 1] → [0, ∞).

j=1 ηj(u)x2

See also Section 4 for other examples of the model (2.1).

3. Main results

In this section, we provide generalization error bounds of (i) the non-penalized and (ii) the

sparse-penalized DNN estimators. We assume the following conditions.

Assumption 3.1.

(i) The random variables vt are conditionally centered and sub-Gaussian,
t )|Gt−1] ≤ 2 for some constant Kt > 0. Moreover,

that is, E[vt | Gt−1] = 0 and E[exp(v2
E[v2

t /K2
t |Gt−1] = 1. Deﬁne K = max1≤t≤T Kt.

(ii) The process X = {Xt}T

t=1 is exponentially β-mixing, i.e. the β-mixing coeﬃcient βX (t) of

X satisﬁes βX (t) ≤ C1,β exp(−C2,βt) with some constants C1,β and C2,β for all t ≥ 1.

(iii) The process X is predictable, that is, Xt is measurable with respect to Gt−1.

Condition (i) is used to apply exponential inequalities for self-normalized processes presented in
de la Pe˜na et al. (2004). Since E[E[exp(v2
t )], Condition (i) also implies
that each vt is sub-Gaussian. Condition (ii) is satisﬁed for a wide class of nonlinear time series.
t=1 can be nonstationary. When Xt = (Yt−1, . . . , Yt−d)(cid:48), Chen
In particular, the process X = {Xt}T
and Chen (2000) provide a set of suﬃcient conditions for the process X to be strictly stationary
and exponentially β-mixing (Theorem 1 in Chen and Chen (2000)):

t )|Gt−1]] = E[exp(v2

t /K2

t /K2

(i) {vt} is a sequence of i.i.d. random variables and has an everywhere positive and continuous

density function, E[vt] = 0, and vt is independent of Xt−s for all s ≥ 1.
(ii) The function m is bounded on every bounded set, that is, for every Γ ≥ 0,

(iii) The function σ satisﬁes, for every Γ ≥ 0,

|m(x)| < ∞.

sup
|x|≤Γ

0 < η1 ≤ inf
|x|≤Γ

η(x) ≤ sup
|x|≤Γ

η(x) < ∞,

where η1 is a constant.

(iv) There exist constants cm,i ≥ 0, cη,i ≥ 0 (i = 0, . . . , d) and M > 0 such that

|m(x)| ≤ cm,0 +

d
(cid:88)

i=1

cm,i|xi|, for |x| ≥ M ,

4

η(x) ≤ cη,0 +

d
(cid:88)

i=1

cη,i|xi|, for |x| ≥ M , and

d
(cid:88)

i=1

(cm,i + cη,iE[|v1|]) < 1.

We also refer to Tjøstheim (1990), Bhattacharya and Lee (1995), Lu and Jiang (2001), Cline and
Pu (2004) and Vogt (2012) for other suﬃcient conditions for the process X being strictly or locally
stationary and exponentially β-mixing.

3.1. Deep neural networks. To estimate the mean function m of the model (2.1), we ﬁt a deep
neural network (DNN) with a nonlinear activation function σ : R → R. The network architecture
(L, p) consists of a positive integer L called the number of hidden layers or depth and a width vector
p = (p0, . . . , pL+1) ∈ NL+2. A DNN with network architecture (L, p) is then any function of the
form

f : Rp0 → RpL+1, x (cid:55)→ f (x) = AL+1 ◦ σL ◦ AL ◦ σL−1 ◦ · · · ◦ σ1 ◦ A1(x),
(3.1)
where A(cid:96) : Rp(cid:96)−1 → Rp(cid:96) is an aﬃne linear map deﬁned by A(cid:96)(x) := W(cid:96)x + b(cid:96) for given p(cid:96)−1 × p(cid:96)
weight matrix W(cid:96) and a shift vector b(cid:96) ∈ Rp(cid:96), and σ(cid:96) : Rp(cid:96) → Rp(cid:96) is an element-wise nonlinear
activation map deﬁned as σ(cid:96)(z) := (σ(z1), . . . , σ(zp(cid:96)))(cid:48). We assume that the activation function σ
is C-Lipschitz for some C > 0, that is, there exists C > 0 such that |σ(x1) − σ(x2)| ≤ C|x1 − x2|
for any x1, x2 ∈ R. Examples of C-Lipschitz activation functions include the rectiﬁed linear unit
(ReLU) activation function x (cid:55)→ max{x, 0} and the sigmoid activation function x (cid:55)→ 1/(1 + e−x).
For a neural network of the form (3.1), we deﬁne

θ(f ) := (vec(W1)(cid:48), b(cid:48)

1, . . . , vec(WL+1)(cid:48), b(cid:48)

L+1)(cid:48)

where vec(W ) transforms the matrix W into the corresponding vector by concatenating the column
vectors.

We let Fσ,p0,pL+1 be the class of DNNs which take p0-dimensional input to produce pL+1-
dimensional output and use the activation function σ : R → R. Since we are interested in real-valued
function on Rd, we always assume that p0 = d and pL+1 = 1 in the following.

For a given DNN f , we let depth(f ) denote the depth and width(f ) denote the width of f (i.e.

width(f ) = max1≤(cid:96)≤L p(cid:96)). For positive constants L, N, B, and F , we set

Fσ(L, N, B) := {f ∈ Fσ,d,1 : depth(f ) ≤ L, width(f ) ≤ N, |θ(f )|∞ ≤ B}

and

Fσ(L, N, B, F ) :=

(cid:110)

f 1[0,1]d : f ∈ Fσ(L, N, B), (cid:107)f (cid:107)∞ ≤ F

(cid:111)

.

Moreover, we deﬁne a class of sparsity constrained DNNs with sparsity level S > 0 by

Fσ(L, N, B, F, S) := {f ∈ Fσ(L, N, B, F ) : |θ(f )|0 ≤ S} .

(3.2)

(3.3)

3.2. Non-penalized DNN estimator. Let (cid:98)fT be an estimator which is a real-valued random
function on Rp such that the map (ω, x) (cid:55)→ (cid:98)fT (ω, x) is measurable with respect to the product of
t=1 and the Borel σ-ﬁeld of Rd. In this section, we provide ﬁnite
the σ-ﬁeld generated by {Yt, Xt}T
sample properties of a DNN estimator (cid:98)fT ∈ Fσ(L, N, B, F, S) of f0.
In particular, we provide bounds for the generalization error

R( (cid:98)fT , f0) = E

(cid:34)

1
T

where {X ∗

t }T

t=1 is an independent copy of X.

T
(cid:88)

( (cid:98)fT (X ∗

t ) − f0(X ∗

t ))2

(cid:35)

,

t=1

5

Let F be a pointwise measurable class of real-valued functions on Rd (cf. Example 2.3.4 in

van der Vaart and Wellner (1996)). Deﬁne

(cid:20)

ΨF

T ( (cid:98)fT ) := E

QT ( (cid:98)fT ) − inf
¯f ∈F

QT ( ¯f )

(cid:21)

,

where QT (f ) is the empirical risk of f deﬁned by QT (f ) := 1
T
ΨF

T ( (cid:98)fT ) measures a gap between (cid:98)fT and an exact minimizer of QT (f ) subject to f ∈ F. Deﬁne

(cid:80)T

t=1(Yt − f (Xt))2. The function

(cid:98)fT,np ∈

arg min
f ∈Fσ(L,N,B,F,S)

QT (f )

and we call (cid:98)fT,np as the non-penalized DNN estimator.
The next result gives a generalization bound of (cid:98)fT,np.

Theorem 3.1. Suppose that Assumption 3.1 is satisﬁed. Consider the nonparametric time series
regression model (2.1) with unknown regression function m satisfying (cid:107)f0(cid:107)∞ ≤ F where f0 =
m1[0,1]d for some F ≥ 1. Let (cid:98)fT be any estimator taking values in the class F = Fσ(L, N, B, F, S)
with B ≥ 1. Then for any ρ > 1, there exits a constant Cρ, only depending on (Cη, C1,β, C2,β, K, ρ),
such that

(cid:18)

R( (cid:98)fT , f0) ≤ ρ

ΨF

T ( (cid:98)fT ) + inf
f ∈F

R(f, f0)

(cid:19)

+ CρF 2 S(L + 1) log ((L + 1)(N + 1)BT ) (log T )

T

.

Theorem 3.1 is an extension of Theorem 2 in Schmidt-Hieber (2020) to possibly nonstationary
β-mixing sequence and the process {vt} can be non Gaussian and dependent. The result follows
from Lemmas A.1 and A.2 in Appendix A.1. Note that Lemmas A.1 and A.2 are of independent
interest since they are general results so that the estimator (cid:98)fT do not need to take values in
Fσ(L, N, B, F, S). Hence the results would be useful to investigate generalization error bounds of
other nonparametric estimators.

Let f0 = m1[0,1]d belong to composition structured functions F0 = G(cid:0)q, d, t, β, A(cid:1) for exam-
ple (see Section 4.1 for the deﬁnition). By choosing σ(x) = max{x, 0} and the parameters of
Fσ(LT , NT , BT , F, ST ) as LT (cid:16) log T , NT (cid:16) T , BT ≥ 1, F > (cid:107)f0(cid:107), and ST (cid:16) T κ/(κ+1) log T with
κ = maxi=0,...,q ti/(2β∗
i ), one can show that the non-penalized DNN estimator achieves the minimax
convergence rate over F0 up to a logarithmic factor. However, the sparsity level ST depends on the
characteristics t and β of f0. Therefore, the non-penalized DNN estimator is not adaptive since
we do not know the characteristics in practice. In the next subsection, we provide a generalization
error bound of sparse-penalized DNN estimators which plays an important role to show that the
sparse-penalized DNN estimators can estimate f0 adaptively.

3.3. Sparse-penalized DNN estimator. Deﬁne ¯QT (f ) as a penalized version of the empirical
risk:

¯QT (f ) :=

1
T

T
(cid:88)

t=1

(Yt − f (Xt))2 + JT (f ),

where JT (f ) is the sparse penalty given by

JT (f ) := JλT ,τT (f ) := λT (cid:107)θ(f )(cid:107)clip,τT
6

for tuning parameters λT > 0 and τT > 0. Here, (cid:107) · (cid:107)clip,τ denotes the clipped L1 norm with a
clipping threshold τ > 0 (Zhang (2010)) deﬁned as

p
(cid:88)

(cid:107)θ(cid:107)clip,τ :=

(cid:18) |θj|
τ

(cid:19)

∧ 1

j=1
for a p-dimensional vector θ = (θ1, . . . , θp)(cid:48). In this section, we provide ﬁnite sample properties of
the sparse-penalized DNN estimator deﬁned as

(cid:98)fT,sp ∈

arg min
f ∈Fσ(L,N,B,F )

¯QT (f ).

Further, for any estimator (cid:98)fT ∈ F = Fσ(L, N, B, F ) of f0, we deﬁne

(cid:20)

¯ΨF

T ( (cid:98)fT ) := E

¯QT ( (cid:98)fT ) − inf
¯f ∈F

¯QT ( ¯f )

(cid:21)

.

The next result provides a generalization error bound of the sparse-penalized DNN estimator.

Theorem 3.2. Suppose that Assumption 3.1 is satisﬁed. Consider the nonparametric time series
regression model (2.1) with unknown regression function m satisfying (cid:107)f0(cid:107)∞ ≤ F where f0 =
m1[0,1]d for some F ≥ 1. Let (cid:98)fT be any estimator taking values in the class F = Fσ(LT , NT , BT , F )
where LT , NT , and BT are positive values such that LT ≤ CL logν0 T , NT ≤ CN T ν1, 1 ≤ BT ≤
CBT ν2 for some positive constants CL, CN , CB, ν0, ν1, and ν2. Moreover, we assume that the tuning
parameters λT and τT of the sparse penalty function JT (f ) satisfy λT = (F 2ιλ(T ) log2+ν0 T )/T with
a strictly increasing function ιλ(x) such that ιλ(x)/ log x → ∞ as x → ∞ and τT (LT + 1)((NT +
1)BT )LT +1 ≤ Cτ T −1 with some positive constant Cτ for any T . Then,

R( (cid:98)fT , f0) ≤ 6

(cid:18)

¯ΨF

T ( (cid:98)fT ) + inf
f ∈F

(R(f, f0) + JT (f ))

(cid:19)

+ CF 2

(cid:18) 1 + log T
T

(cid:19)

,

where C is a positive constant only depending on (Cη, C1,β, C2,β, CL, CN , CB, Cτ , ν0, ν1, ν2, K, ιλ).

Theorem 3.2 is an extension of Theorem 1 in Ohn and Kim (2022), which considers i.i.d. ob-
servations. Here we explain some diﬀerences between their result and ours. First, Theorem 3.2
can be applied to nonstationary time series since we only assume the process X to be β-mixing.
Second, our approach to proving Theorem 3.2 is diﬀerent from that of Ohn and Kim (2022). Their
proofs heavily depends on the theory for i.i.d. data in Gy¨orﬁ et al. (2002) so extending their ap-
proach to our framework seems to require substantial work. In contrast, our approach is based
on other technical tools such as the blocking technique of β-mixing processes in Rio (2013) and
exponential inequalities for self-normalized martingale diﬀerence sequences. In particular, consid-
ering continuous time embedding of a martingale diﬀerence sequence and applying the results on
(super-) martingales in Barlow et al. (1986), we can allow the process {vt}T
t=1 to be conditionally
centered and circumvent additional conditions on its distribution such as conditional Gaussianity or
symmetry (see also Lemma C.2 and the proof of Lemma C.3 in Appendix). As a result, our result
improve the power of logs in their generalization error bound. Third, (a) the upper bound of the
depth of the sparse penalized DNN estimator L can glow by a power of log T and (b) we take the
tuning parameter λT to depend on F 2. Particularly, (a) enables us to estimate f0 adaptively when
f0 belongs to an (cid:96)0-bounded aﬃne class as well as composition structured functions (see Sections
4.1 and 4.2 for details) and (b) enables (cid:98)fT,sp to be adaptive with respect to (cid:107)f (cid:107)∞ ≤ F . See also
the comments on Proposition 4.1 on the improvement of the upper bound.

7

4. Minimax optimality in nonlinear AR models

In this section, we show the minimax optimality of the sparse-penalized DNN estimator (cid:98)fT,sp. In
particular, we show that (cid:98)fT,sp achieves the minimax convergence rate over (i) composition structured
functions and (ii) (cid:96)0-bounded aﬃne class. We note that these classes of functions include many
nonlinear AR models such as (generalized) additive AR models, single-index models, (multi-regime)
threshold AR models, and exponential AR models.

We consider the observation {Yt}T

t=1 generated by the following nonlinear AR(d) model:




Yt = m(Yt−1, . . . , Yt−d) + vt,
(Y0, Y−1, . . . , Y−d+1)(cid:48) ∼ ν,
vt
Here, ν is a (ﬁxed) probability measure on Rd such that (cid:82)
unknown function to be estimated.

i.i.d.∼ N (0, 1).



t = 1, . . . , T,

(4.1)

Rd |x|ν(dx) < ∞, and m : Rd → R is an

Our results in this section can be extended to more general AR(d) model that allows non-
Gaussianity of the distribution of vt and non-constant volatility function η(·). To simplify our
argument, we focus on the model (4.1).

Let c = (c0, c1, . . . , cd) ∈ (0, ∞)d+1 satisfy (cid:80)d

able functions m : Rd → R satisfying |m(x)| ≤ c0 + (cid:80)d
shows that the process Y = {Yt}T

i=1 ci < 1. We denote by M0(c) the set of measur-
i=1 ci|xi| for all x ∈ Rd. The following lemma

t=1 is exponentially β-mixing “uniformly” over m ∈ M0(c).

Lemma 4.1. Consider the nonlinear AR(d) model (4.1) with m ∈ M0(c). There are positive
constants Cβ and C(cid:48)

β depending only on c, d and ν such that

βY (t) ≤ C(cid:48)

βe−Cβ t

for all t ≥ 1.

(4.2)

The next result gives the generalization error for a family of functions that can be approximated

with a certain degree of accuracy by DNNs.

Proposition 4.1. Consider the nonlinear AR(d) model (4.1) with m ∈ M0(c). Let F , (cid:98)fT , F, LT ,
NT , BT , λT and τT as in Theorem 3.2. Suppose that there are constants κ, r ≥ 0, C0 > 0 and
CS > 0 such that

inf
f ∈Fσ(LT ,NT ,BT ,F,ST )

(cid:107)f − f0(cid:107)2

L2([0,1]d) ≤

with ST := CST κ/(κ+1) logr T . Then,

C0
T 1/(κ+1)

R( (cid:98)fT , f0) ≤ 6 ¯ΨF

T ( (cid:98)fT ) + C(cid:48)F 2 ιλ(T ) log2+ν0+r T

,

T 1/(κ+1)

where C(cid:48) is a positive constant only depending on (c, d, ν, CL, CN , CB, Cτ , ν0, ν1, ν2, K, ιλ, κ, r, C0, CS).

If (cid:98)fT = (cid:98)fT,sp, then the generalization error bound in Proposition 4.1 is reduced to

R( (cid:98)fT , f0) ≤ C(cid:48)F 2 ιλ(T ) log2+ν0+r T
When ν0 = 1 and ιλ(T ) = logν3 T with ν3 ∈ (1, 2), one can see that our result improves the power of
logs in the generalization error bound in Theorem 2 in Ohn and Kim (2022). Moreover, our result
allows the generalization error bound to depend explicitly on F . Combining this with the results in
the following sections implies that the sparse-penalized DNN estimator can be adaptive concerning
the upper bound of (cid:107)f0(cid:107)∞ (by taking F (cid:16) logν4 T with ν4 > 0 for example) and hence Proposition
8

T 1/(κ+1)

.

4.1 is useful for the computation of (cid:98)fT,sp since the upper bound F is unknown in practice as well
as other information about the shape of f0.

4.1. Composition structured functions. In this subsection, we consider nonparametric estima-
tion of the mean function f0 when it belongs to a class of composition structured functions which
is deﬁned as follows.

For p, r ∈ N with p ≥ r, β, A > 0 and l < u, we denote by Cβ

r ([l, u]p, A) the set of functions

f : [l, u]p → R satisfying the following conditions:
(i) f depends on at most r coordinates.
(ii) f is of class C(cid:98)β(cid:99) and satisﬁes

(cid:88)

(cid:107)∂αf (cid:107)∞ +

(cid:88)

α:|α|1<β

α:|α|1=(cid:98)β(cid:99)

sup
x,y∈[l,u]p:x(cid:54)=y

|∂αf (x) − ∂αf (y)|
|x − y|β−(cid:98)β(cid:99)

∞

≤ A,

where we used multi-index notation, that is, ∂α = ∂α1 · · · ∂αp with α = (α1, . . . , αp) ∈ Zp
≥0
and |α|1 := (cid:80)p

j=1 αj.

Let d = (d0, . . . , dq+1) ∈ Nq+2 with d0 = d and dq+1 = 1, t = (t0, . . . , tq) ∈ Nq+1 with ti ≤ di
for all i and β = (β0, . . . , βq) ∈ (0, ∞)q+1. We deﬁne G(cid:0)q, d, t, β, A(cid:1) as the class of functions
f : [0, 1]d → R of the form

f = gq ◦ · · · ◦ g0,
where gi = (gij)j : [li, ui]di → [li+1, ui+1]di+1 with gij ∈ Cβi
ti
i = 0, . . . , q.

(4.3)
(cid:0)[li, ui]di, A(cid:1) for some |li+1|, |ui+1| ≤ A,

Denote by M(cid:0)c, q, d, t, β, A(cid:1) the class of functions in M0(c) whose restrictions to [0, 1]d belong

to G(cid:0)q, d, t, β, A(cid:1). Also, deﬁne

q
(cid:89)

β∗
i := βi

(β(cid:96) ∧ 1),

(cid:96)=i+1

φT := max
i=0,...,q

T

−

2β∗
i
+ti .

2β∗
i

Example 4.1 (Nonlinear additive AR model). Consider a nonlinear AR model:

Yt = m1(Yt−1) + · · · + md(Yt−d) + vt,

where m1, . . . , md are univariate measurable functions.
In this case, the mean function can be
written as a composition of functions m = g1 ◦ g0 with g0(x1, . . . , xd) = (m1(x1), . . . , md(xd))(cid:48)
and g1(x1, . . . , xd) = (cid:80)d
1 ([0, 1], A) for j = 1, . . . , d. Note that
g1 ∈ Cγ
d ([−A, A]d, (A + 1)d) for any γ > 1. Then we can see that m|[0,1]d : [0, 1]d → [−Ad, Ad] and

j=1 xj. Suppose that mj|[0,1] ∈ Cβ

m|[0,1]d ∈ G(cid:0)1, (d, d, 1), (1, d), (β, (β ∨ 2)d), (A + 1)d(cid:1).

Hence φT = T − 2β

2β+1 in this case.

Example 4.2 (Nonlinear generalized additive AR model). Consider a nonlinear AR model:

Yt = φ(m1(Yt−1) + · · · + md(Yt−d)) + vt,
where φ : R → R is some unknown link function. In this case, the mean function can be written
as a composition of functions m = g2 ◦ g1 ◦ g0 with g0 and g1 as in Example 4.1 and g2 = φ.
Suppose that φ ∈ Cγ
1 ([−Ad, Ad], A) and take mj and g1 as in Example 4.1. Then we can see that
m|[0,1]d : [0, 1]d → [−A, A] and

m|[0,1]d ∈ G(cid:0)2, (d, d, 1, 1), (1, d, 1), (β, (β ∨ 2)d, γ), (A + 1)d(cid:1).

9

Hence φT = T − 2β(γ∧1)

2β(γ∧1)+1 ∨ T − 2γ

2γ+1 in this case.

Example 4.3 (Single-index model). Consider a nonlinear AR model:

Yt = φ0(Zt) + φ1(Zt)Yt−1 + · · · + φd(Zt)Yt−d + vt,

Zt = b0 + b1Yt−1 + · · · + bdYt−d,

where, for j = 0, 1, . . . , d, φj : R → R is an unknown function and bj is an unknown constant. In
this case, the mean function can be written as a composition of functions m = g2 ◦ g1 ◦ g0 with
g0(x1, . . . , xd) = (b0+b1x1+· · ·+bdxd, x1, . . . , xd)(cid:48), g1(z, x1, . . . , xd) = (φ0(z), . . . , φd(z), x1, . . . , xd)(cid:48),
and g2(w0, w1, . . . , wd, x1, . . . , xd) = w0+w1x1+· · ·+wdxd. Suppose that φ0, . . . , φd ∈ Cβ
1 ([−A, A], A)
for some constants β ≥ 1 and A ≥ 1 ∨ (cid:80)d

j=0 |bj|. Then we have

m|[0,1]d ∈ G(cid:0)2, (d, d + 1, 2d + 1, 1), (d, 1, 2d + 1), (βd, β, β(2d + 1)), (A + 1)(1 + d + dA)(cid:1).

Hence φT = T − 2β

2β+1 in this case.

Below we show the minimax lower bound for estimating f0 ∈ M(c, q, d, t, β, A).

Theorem 4.1. Consider the nonlinear AR(d) model (4.1) with m ∈ M(c, q, d, t, β, A). Suppose
that c0 ≥ A and tj ≤ min{d0, . . . , dj−1} for all j. Then, for suﬃciently large A,

lim inf
T →∞

φ−1
T inf
(cid:98)fT

sup
m∈M(c,q,d,t,β,A)

R( (cid:98)fT , f0) > 0,

where the inﬁmum is taken over all estimators (cid:98)fT .

Theorem 4.1 and the next result imply that the sparse-penalized DNN estimator (cid:98)fT,sp is rate
optimal since it attains the minimax lower bound up to a poly-logarithmic factor. We write ReLU
for the ReLU activation function, i.e. ReLU(x) = max{x, 0}.

Theorem 4.2. Consider the nonlinear AR(d) model (4.1) with m ∈ M(c, q, d, t, β, A). Let F ≥
1 ∨ A be a constant, LT (cid:16) logr T for some r > 1, NT (cid:16) T , BT , λT and τT as in Theorem 3.2 with
ν0 = r, and (cid:98)fT a minimizer of ¯QT (f ) subject to f ∈ FReLU(LT , NT , BT , F ). Then

sup
m∈M(c,q,d,t,β,A)

R( (cid:98)fT , f0) = O (cid:0)φT ιλ(T ) log3+r T (cid:1)

as T → ∞.

4.2. (cid:96)0-bounded aﬃne class. In this subsection, we consider nonparametric estimation of the
mean function f0 when it belongs to an (cid:96)0-bounded aﬃne class I 0
Φ. This class was introduced in
Hayakawa and Suzuki (2020) and is deﬁned as follows.

Deﬁnition 4.1. Given a set Φ of real-valued functions on Rd with (cid:107)ϕ(cid:107)L2([0,1]d) = 1 for each ϕ ∈ Φ
along with constants ns ∈ N and C > 0, we deﬁne an (cid:96)0-bounded aﬃne class I 0

Φ as

I 0
Φ(ns, C) :=

(cid:40) ns(cid:88)

i=1

θiϕi(Ai · −bi) : Ai ∈ Rd×d, bi ∈ Rd, θi ∈ R, ϕi ∈ Φ,

| det Ai|−1 ∨ |Ai|∞ ∨ |bi|∞ ∨ |θi| ≤ C, i = 1, . . . , ns

(cid:9) .

By taking the set Φ suitably, the class of functions I 0

Φ includes many nonlinear AR models such
as threshold AR (TAR) models and we can show that the sparse-penalized DNN estimator attains
the convergence rate O(T −1) up to a poly-logarithmic factor (Theorem 4.4).

10

Example 4.4 (Threshold AR model). Consider a two-regime TAR(1) model:

Yt =

(cid:40)

a1Yt−1 + vt
a2Yt−1 + vt

if Yt−1 ≤ r,
if Yt−1 > r,

where a1, a2, r are some constants. This model corresponds to (4.1) with d = 1 and m(y) =
(a11(−∞,r](y) + a21(r,∞)(y))y. Note that the mean function m can be discontinuous and this m can
be rewritten as

m(y) = −a1 ReLU(r − y) + a1r1[0,∞)(r − y) + a2 ReLU(y − r) + a2r1[0,∞)(y − r).

Φ(ns, C) with Φ = {

Hence m ∈ I 0
3 ReLU, 1[0,∞)}, ns ≥ 4 and C ≥ max{|a1|, |a2|, |r|}. This
argument can be extended to a multi-regime (self-exciting) TAR model of any order in an obvious
manner.

√

We set M0

Φ(c, ns, C) := M0(c) ∩ I 0

Φ(ns, C). Below we show the minimax lower bound for

estimating f0 ∈ M0

Φ(c, ns, C).

Theorem 4.3. Consider the nonlinear AR(d) model (4.1) with m ∈ M0
there is a function ϕ ∈ Φ such that supp(ϕ) ⊂ [0, 1]d and (cid:107)ϕ(cid:107)∞ ≤ c0. Then,

Φ(c, ns, C). Suppose that

lim inf
T →∞

T inf
(cid:98)fT

sup
Φ(c,ns,C)

m∈M0

R( (cid:98)fT , f0) > 0,

where the inﬁmum is taken over all estimators (cid:98)fT .

Now we extend the argument in Example 4.4. For this, we introduce the function class APσ,d(C1, C2, D, r)

which can be approximated by “light” networks.

Deﬁnition 4.2. For C1, C2, D > 0 and r ≥ 0, we denote by APσ,d(C1, C2, D, r) the set of functions
ϕ : Rd → R satisfying that, for each ε ∈ (0, 1/2), there exist parameters Lε, Nε, Bε, Sε > 0 such
that

• Lε ∨ Nε ∨ Sε ≤ C1{log2(1/ε)}r and Bε ≤ C2/ε hold;
• there exists an f ∈ Fσ(Lε, Nε, Bε) such that |θ(f )|0 ≤ Sε and (cid:107)f − ϕ(cid:107)2

L2([−D,D]d) ≤ ε.

Depending on the value of r, APσ,d(C1, C2, D, r) contains various functions such as step functions

(0 ≤ r), polynomials (r = 1), and very smooth functions (r = 2).

Example 4.5 (Piecewise linear functions). For σ = ReLU, we evidently have ReLU ∈ APσ,1(C1, C2, D, r)
for any C1, C2, D ≥ 2 and r ≥ 0. In this case we also have 1[0,∞) ∈ APσ,1(C1, C2, D, r) if C1, C2 ≥ 7.
In fact, for any ε ∈ (0, 1/2), the function

(cid:18)

fε(x) = σ

σ(x + 1) − σ(x) −

(cid:19)

σ(−x)

,

1
ε

x ∈ R,

satisﬁes (cid:107)fε − 1[0,∞)(cid:107)2

L2([−D,D]) ≤ ε.

Example 4.6 (Polynomial functions). Take σ = ReLU and consider a polynomial function ϕ(x) =
(cid:80)p
i=0 aixi for some constants a0, . . . , ap ∈ R. Then, given D > 0, we have ϕ ∈ APσ,1(C1, 1/2, D, 1)
for some constant C > 0 depending only on maxi=0,...,p |ai|, p and D by Proposition III.5 in
Elbr¨achter et al. (2021).

11

Example 4.7 (Very smooth functions). Take σ = ReLU again. Let ϕ : R → R be a C∞ function
such that there are constants A ≥ 1 and D > 0 satisfying supx∈[−D,D] |ϕ(n)(x)| ≤ n!A for all
n ∈ Z≥0. Then, by Lemma A.6 in Elbr¨achter et al. (2021), A−1ϕ ∈ APσ,1(C1, 1, D, 2) for some
constants C1 > 0 depending only on D. Hence ϕ ∈ APσ,1(C1, A, D, 2). The condition on ϕ is
satisﬁed e.g. when there is a holomorphic function Ψ on {z ∈ C : |z| < D + 1} such that |Ψ| ≤ A
and Ψ(x) = ϕ(x) for all x ∈ [−D, D]. This follows from Cauchy’s estimates (cf. Theorem 10.26 in
Rudin (1987)).

Example 4.8 (Product with an indicator function). Again consider the ReLU activation function
σ = ReLU. Let ϕ ∈ APσ,1(C1, C2, D, r) for some constants C1, C2, D > 0 and r ≥ 1, and assume
supx∈[−D,D] |ϕ(x)| ≤ A for some constant A ≥ 1. Then ϕ1[0,∞) ∈ APσ,1(C3, C3, D, r) for some
constant C3 depending only on C1, C2, D, A. To see this, ﬁx ε ∈ (0, 1/2) arbitrarily and take
Lε, Nε, Bε, Sε and f as in Deﬁnition 4.2. Also, let fε deﬁned as in Example 4.5. By Proposition
III.3 in Elbr¨achter et al. (2021), there is an f1 ∈ Fσ(C4 log(1/ε), 5, 1) with C4 > 0 depends only
on A such that supx,y∈[−A,A] |f1(x, y) − xy| ≤ ε. Then, by Lemmas II.3–II.4 and A.7 in Elbr¨achter
et al. (2021), there is an f2 ∈ Fσ(C5{log(1/ε)}r, C5{log(1/ε)}r, C5/ε) with C5 > 0 depending only
on C1, C2, A such that f2(x) = f1(f (x), fε(x)) for all x ∈ R and (cid:107)θ(f2)(cid:107)∞ ≤ C5{log(1/ε)}r. For
this f2, we have

(cid:107)f2 − ϕ1[0,∞)(cid:107)L2([−D,D])
≤ (cid:107)f2 − f fε(cid:107)L2([−D,D]) + (cid:107)(f − ϕ)fε(cid:107)L2([−D,D]) + (cid:107)ϕ(fε − 1[0,∞))(cid:107)L2([−D,D])
≤ (D + 1 + A)

√

ε.

Applying this argument to ε/

√

D + 1 + A instead of ε, we obtain the desired result.

Theorem 4.3 and the next result imply that the sparse-penalized DNN estimator (cid:98)fT,sp attains

the minimax optimal rate over M0

Φ(c, ns, C) up to a poly-logarithmic factor.

Theorem 4.4. Consider the nonlinear AR(d) model (4.1) with m ∈ M0
Φ(c, ns, C). Suppose that
Φ ⊂ APReLU,d(C1, C2, D, r) for some constants C1, C2 > 0, D ≥ (d + 1)C and r ≥ 0. Let F ≥ 1 + c0
be a constant, LT (cid:16) logr(cid:48)
T for some r(cid:48) > r, NT (cid:16) T , BT (cid:16) T ν for some ν > 1, λT and τT as
in Theorem 3.2 with ν0 = r(cid:48), and (cid:98)fT a minimizer of ¯QT (f ) subject to f ∈ FReLU(LT , NT , BT , F ).
Then

sup
Φ(c,ns,C)

m∈M0

R( (cid:98)fT , f0) = O

(cid:32)

ιλ(T ) log2+r+r(cid:48)
T

(cid:33)

T

as T → ∞.

Example 4.9. By Examples 4.4 and 4.5, the sparse-penalized DNN estimator adaptively achieve
the minimax rate of convergence up to a logarithmic factor for threshold AR models. Thanks
to Examples 4.6–4.8, this result can be extended to some threshold AR models with nonlinear
coeﬃcients.

Example 4.10 (Functional coeﬃcient AR model). Examples 4.7 and 4.8 also imply that Theorem
4.4 can be extended to some functional coeﬃcient AR (FAR) models introduced in Chen and Tsay
(1993):

Yt = f1(Y∗

t−1)Yt−1 + · · · + fd(Y∗

t−1)Yt−d + vt

t−1 = (Yt−1, . . . , Yt−d)(cid:48) and fj : Rd → R are measurable functions. This model include
where Y∗
many nonlinear AR models such as (1) TAR models (when fj are step functions), (2) exponential
AR (EXPAR) models proposed in Haggan and Ozaki (1981) (when fj are exponential functions),
12

and (3) smooth transition AR (STAR) models (e.g. Granger and Ter¨asvirta (1993) and Ter¨asvirta
(1994)). Note that some classes of FAR models such as EXPAR and STAR models can be written
as a composition of functions so Theorem 4.2 can be applied to those examples.

5. Simulation results

In this section, we conduct a simulation experiment to assess the ﬁnite sample performance of
DNN estimators for the mean function of nonlinear time series. Following Ohn and Kim (2022),
we compare the following ﬁve estimators in our experiment: Kernel ridge regression (KRR), k-
nearest neighbors (kNN), random forest (RF), non-penalized DNN estimator (NPDNN), and sparse-
penalized DNN estimator (SPDNN).

For kernel ridge regression, we used a Gaussian radial basis function kernel and selected the tuning
parameters by 5-fold cross-validation as in Ohn and Kim (2022). We determined the search grids
for selection of the tuning parameters following Exterkate (2013). The tuning parameter k for k-
nearest neighbors was also selected by 5-fold cross-validation with the search grid {5, 7, . . . , 41, 43}.
For random forest, unlike Ohn and Kim (2022), we did not tune the number of the trees but ﬁx it
to 500 following discussions in (Hastie et al., 2009, Section 15.3.4) as well as the analysis of Probst
and Boulesteix (2018). Instead, we tuned the number of variables randomly sampled as candidates
at each split. This was done by the R function tuneRF of the package randomForest.

For the DNN based estimators, we set the network architecture (L, p) as L = 3 and p1 = p2 =
p3 = 128 along with the ReLU activation function σ(x) = max{x, 0}. Supposing that data were
appropriately scaled, we ignored the restriction to [0, 1]d of observations when constructing (and
evaluating) the DNN based estimators. The network weights were trained by Adam (Kingma and
Ba, 2015) with learning rate 10−3 and minibatch size of 64. To avoid overﬁtting, we determined the
number of epochs by the following early stopping rule: First, we train the network weights using
the ﬁrst half of observation data and evaluate its mean square error (MSE) using the second half
of the data at each epoch. We stop the training when the MSE is not improved within 5 epochs.
After determining the number of epochs by this rule, we trained the network weights using the full
sample. For the sparse-penalized DNN estimator, we also need to select the parameters λT and
τT . We set τT = 10−9. λT was selected from { Sy log3
, 2Sy log3
10 T
} to
T
minimize the MSE in the above early stopping rule. Here, Sy is the sample variance of {Yt}T

, Sy log3
T

, Sy log3
2T

, Sy log3
4T

10 T

10 T

10 T

10 T

8T

t=1.

We consider the following non-linear AR models for data-generating processes. Throughout this

section, {εt}T
EXPAR: Yt = a1(Yt−1)Yt−1 + a2(Yt−1)Yt−2 + 0.2εt with

t=1 denote i.i.d. standard normal variables.

a1(y) = 0.138 + (0.316 + 0.982y)e−3.89y2
,
a2(y) = −0.437 − (0.659 + 1.260y)e−3.89y2

.

TAR: Yt = b1(Yt−1)Yt−1 + b2(Yt−1)Yt−2 + εt with

b1(y) = 0.4 · 1(−∞,1](y) − 0.8 · 1(1,∞)(y),
b2(y) = −0.6 · 1(−∞,1](y) + 0.2 · 1(1,∞)(y).

FAR:

Yt = −Yt−2 exp(−Y 2

t−2/2) +

1
1 + Y 2
13

t−2

cos(1.5Yt−2)Yt−1 + 0.5εt.

AAR:

SIM:

Yt = 4

Yt−1
1 + 0.8Y 2

t−1

+

exp{3(Yt−2 − 2)}
1 + exp{3(Yt−2 − 2)}

+ εt.

Yt = exp(−8Z2

t ) + 0.5 sin(2πZt)Yt−1 + 0.1εt, Zt = 0.8Yt−1 + 0.6Yt−2 − 0.6.

SIMv: For v ∈ {0.5, 1.0, 5.0},

Yt = {Φ(−vZt) − 0.5}Yt−1 + {Φ(2vZt) − 0.6}Yt−2 + εt,
Zt = Yt−1 + Yt−2 − Yt−3 − Yt−4,

where Φ is the standard normal distribution function.

The ﬁrst four models, EXPAR, TAR, FAR and AAR, are taken from Chapter 8 of Fan and Yao
(2008); see Examples 8.3.7, 8.4.7 and 8.5.6 ibidem. The models SIM and SIMv are respectively
taken from (Xia and Li, 1999, Example 1) and (Xia et al., 2007, Example 3.2) to cover the single-
index model (cf. Example 4.3). Since the model SIMv has a parameter v varying over {0.5, 1, 5},
we consider totally eight models. We generated observation data {Yt}T
t=1 with T = 400 and burn-in
period of 100 observations.

As in Ohn and Kim (2022), we evaluate the performance of each estimator by the empirical L2
error computed based on newly generated 105 simulated data. Figure 1 shows the boxplots of the
empirical L2 errors of the ﬁve estimators over 500 Monte Carlo replications for eight models. As the
ﬁgure reveals, the performances of KRR, NPDNN and SPDNN are superior to those of KNN and
RF. Moreover, except for FAR, the DNN based estimators are comparable or better than KRR:
For models with intrinsic low-dimensional structures such as AAR, SIM and SIM0.5, the DNN
based estimators perform slightly better than KRR. For models with discontinuous or rough mean
functions such as TAR, SIM1 and SIM5, the performances of the DNN based estimators dominate
that of KRR. These observations are in line with theoretical results developed in this paper.

6. Concluding remarks

In this paper, we have advanced statistical theory of feed-forward deep neural networks (DNN)
for dependent data. For this, we investigated statistical properties of DNN estimators for nonpara-
metric estimation of the mean function of a nonstationary and nonlinear time series. We established
generalization bounds of both non-penalized and sparse-penalized DNN estimators and showed that
the sparse-penalized DNN estimators can estimate the mean functions of a wide class of the nonlin-
ear autoregressive (AR) models adaptively and attain the minimax optimal convergence rates up to
a logarithmic factor. The class of nonlinear AR models covers nonlinear generalized additive AR,
single index models, and popular nonlinear AR models with discontinuous mean functions such as
multi-regime threshold AR models.

It would be possible to extend the results in Section 4 to other function classes such as piece-
wise smooth functions (Imaizumi and Fukumizu (2019)), functions with low intrinsic dimensions
(Schmidt-Hieber (2019) and Nakada and Imaizumi (2020)), and functions with varying smoothness
(Suzuki (2019) and Suzuki and Nitanda (2021)). We leave such extensions as future research.

14

Figure 1. Boxplots of empirical L2 errors

Appendix A. Proofs for Section 3
For random elements X and Y , we write X L= Y if they have the same law. Let F be a pointwise
measurable class of real-valued functions on Rd. For δ > 0, a ﬁnite set G ⊂ F is called a δ-covering
of F with respect to (cid:107) · (cid:107)∞ if for any f ∈ F there exists g ∈ G such that (cid:107)f − g(cid:107)∞ ≤ δ. The
minimum cardinality of a δ-covering of F with respect to (cid:107) · (cid:107)∞ is called the covering number of
F with respect to (cid:107) · (cid:107)∞ and denoted by N (δ, F, (cid:107) · (cid:107)∞). Let σ(Z) be the σ-ﬁeld generated by the
15

0.0030.0100.030KRRKNNRFNPDNNSPDNNEXPAR0.10.30.5KRRKNNRFNPDNNSPDNNTAR0.0050.0100.0300.050KRRKNNRFNPDNNSPDNNFAR0.030.100.301.003.00KRRKNNRFNPDNNSPDNNAAR0.0010.0030.0100.030KRRKNNRFNPDNNSPDNNSIM0.050.100.300.50KRRKNNRFNPDNNSPDNNSIM0.50.050.100.300.50KRRKNNRFNPDNNSPDNNSIM10.10.31.0KRRKNNRFNPDNNSPDNNSIM5random element Z. Let (cid:98)fT be an estimator taking values in F and deﬁne its expected empirical
error by

(cid:98)R( (cid:98)fT , f0) = E

(cid:34)

1
T

T
(cid:88)

( (cid:98)fT (Xt) − f0(Xt))2

(cid:35)

.

t=1

A.1. Proof of Theorem 3.1. The following Lemmas A.1 and A.2 are modiﬁcations of Lemmas 4.1
and 4.2 in Oga and Koike (2021). We give their proofs for self-containedness since Oga and Koike
(2021) focus on nonparametric estimation of diﬀusion processes and so there are some diﬀerences
in our framework.

Lemma A.1. Let δ > 0 and suppose that there exists an integer NT such that NT ≥ N (δ, F, (cid:107) ·
(cid:107)∞) ∨ exp(10). Also, let aT be a positive number such that µT := (cid:98)T /(2aT )(cid:99) > 0. In addition,
suppose that there is a number F ≥ 1 such that (cid:107)f (cid:107)∞ ≤ F for all f ∈ F ∪ {f0}. Then, for all
ε ∈ (0, 1],

R( (cid:98)fT , f0) ≤ (1 + ε) (cid:98)R( (cid:98)fT , f0) +

21(1 + ε)2
ε

F 2 log NT
µT

+

4F 2
µT

+ 4(2 + ε)F 2β(aT ) + 4(2 + ε)F δ.

Proof. Let {f1, . . . , fNT } be a δ-covering of F with respect to (cid:107) · (cid:107)∞ and deﬁne a random variable
J taking values in {1, . . . , NT } such that (cid:107) (cid:98)fT − fJ (cid:107)∞ ≤ δ.

Step 1 (Reduction to independence) We rely on the coupling technique for β-mixing sequences to
construct independent blocks; cf. Rio (2013). For (cid:96) = 0, . . . , µT − 1, let I1,(cid:96) = {2(cid:96)aT + 1, . . . , (2(cid:96) +
1)aT }, I2,(cid:96) = {(2(cid:96) + 1)aT + 1, . . . , 2((cid:96) + 1)aT }. Deﬁne

(cid:101)g(cid:96) := ((cid:101)g1,(cid:96), . . . , (cid:101)gNT ,(cid:96))(cid:48) =

(cid:96) := ((cid:101)g∗
(cid:101)g∗

1,(cid:96), . . . , (cid:101)g∗

NT ,(cid:96))(cid:48) =





(cid:88)

t∈I1,(cid:96)





(cid:88)

t∈I1,(cid:96)

(f1(Xt) − f0(Xt))2, . . . ,



(cid:48)

(fNT (Xt) − f0(Xt))2



,

(cid:88)

t∈I1,(cid:96)

(f1(X ∗

t ) − f0(X ∗

t ))2, . . . ,

(fNT (X ∗

t ) − f0(X ∗

t ))2


(cid:48)



.

(cid:88)

t∈I1,(cid:96)

In the following, we extend the probability space if necessary and assume that there is a sequence
{U(cid:96)}∞

(cid:96)=1 of i.i.d. uniform random variables over [0, 1] independent of X.

We will show that there exist two sequences of independent RNT -valued random variables

{g(cid:96)}µT −1
(cid:96)=0

and {g∗

(cid:96) }µT −1

(cid:96)=0

such that for all (cid:96) = 0, . . . , µT − 1,

|E[gj,(cid:96)] − E[(cid:101)gj,(cid:96)]| ≤ 4F 2aT β(aT ),
j,(cid:96)](cid:12)
(cid:12)
(cid:12) ≤ 4F 2aT β(aT ).
(cid:12)E[g∗
j,(cid:96) be the j-th component of g(cid:96) and g∗

j,(cid:96)] − E[(cid:101)g∗

where gj,(cid:96) and g∗
proof of (A.2) is similar.

(cid:96) , respectively. We only prove (A.1) since the

(A.1)

(A.2)

First we will show that there exist a sequences {g(cid:96)}µT −1
(cid:96)=0

of independent random vectors in RNT

such that

g(cid:96)

L= (cid:101)g(cid:96), P(g(cid:96) (cid:54)= (cid:101)g(cid:96)) ≤ β(aT ) for 0 ≤ (cid:96) ≤ µT − 1.

For all (cid:96)1, (cid:96)2, deﬁne the σ-ﬁeld A((cid:96)1, (cid:96)2) generated by {Xt}t∈I((cid:96)1,(cid:96)2) where I((cid:96)1, (cid:96)2) := (cid:83)(cid:96)2
I1,(cid:96).
From the deﬁnition of (cid:101)g(cid:96), we ﬁnd that σ((cid:101)g(cid:96)) ⊂ A((cid:96), (cid:96)) for all (cid:96). Applying Lemma C.1, there exists a
L= (cid:101)g(cid:96), independent of A(0, (cid:96) − 1), and P(g(cid:96) (cid:54)= (cid:101)g(cid:96)) ≤ β(aT ). Moreover,
random vector g(cid:96) such that g(cid:96)
g(cid:96) is measurable with respect to the σ-ﬁeld generated by A(0, (cid:96)) and U(cid:96). Therefore, for any (cid:96), g(cid:96) is
16

(cid:96)=(cid:96)1

independent of {g(cid:96)(cid:48)}(cid:96)−1
A(0, (cid:96)1) and U(cid:96)1. This implies that {g(cid:96)}µT −1
(cid:96)=0

(cid:96)(cid:48)=1, since for (cid:96)1, (cid:96)2 with (cid:96)1 < (cid:96)2, g(cid:96)2 is independent of the σ-ﬁeld generated by

is a sequence of independent random variables.

Next we will show (A.1). By deﬁnition we have 0 ≤ (cid:101)gj,(cid:96) ≤ 4F 2aT for all j. Since g(cid:96) has the same

law as (cid:101)g(cid:96), we also have 0 ≤ gj,(cid:96) ≤ 4F 2aT a.s. for all j. Consequently, we have

|E[gj,(cid:96)] − E[(cid:101)gj,(cid:96)]| ≤ E (cid:2)|gj,(cid:96) − (cid:101)gj,(cid:96)| 1{gj,(cid:96) (cid:54)= (cid:101)gj,(cid:96)}(cid:3) ≤ 4F 2aT P(gj,(cid:96) (cid:54)= (cid:101)gj,(cid:96)) ≤ 4F 2aT β(aT ).
Step 2 (Bounding the diﬀerence of the sum of independent blocks) In this step, we will show

(cid:34)µT −1
(cid:88)

(cid:35)

g∗
J,(cid:96)

E

(cid:96)=0

≤ (1 + ε)E

(cid:34)µT −1
(cid:88)

(cid:35)

gJ,(cid:96)

+

(cid:96)=0

21(1 + (cid:15))2
ε

F 2aT log NT .

(A.3)

for all ε ∈ (0, 1].

For j = 1, . . . , NT , deﬁne

(cid:32)

rj :=

µT −1
(cid:88)

(cid:33)1/2

E[g∗

j,(cid:96)]

,

1
µT

∨

4F 2aT log NT
µT
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:80)µT −1

(cid:96)=0 (g∗
2F rj

(cid:96)=0
j,(cid:96) − gj,(cid:96))

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

B := max
1≤j≤NT

By deﬁnition, we have
(cid:34)µT −1
(cid:88)

(cid:12)
(cid:12)
(cid:12)
E
(cid:12)
(cid:12)

(cid:35)

g∗
J,(cid:96)

− E

(cid:34)µT −1
(cid:88)

(cid:96)=0

gJ,(cid:96)

(cid:35)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ E

(cid:34)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

µT −1
(cid:88)

(g∗

J,(cid:96) − gJ,(cid:96))

(cid:96)=0

(cid:12)
(cid:35)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ 2F E [rJ B] .

(cid:96)=0

By the Cauchy-Schwarz inequality, we have



(cid:32)

E[rJ B] ≤ E



(cid:33)1/2



E[g∗

J,(cid:96)]

B

 +

1
µT

µT −1
(cid:88)

(cid:96)=0

(cid:18) 4F 2aT log NT
µT

(cid:19)1/2

E [B]

≤ E

(cid:34)

1
µT

µT −1
(cid:88)

(cid:96)=0

(cid:35)1/2

E[g∗

J,(cid:96)]

E[B2]1/2 + 2F

= E

(cid:34)µT −1
(cid:88)

(cid:96)=0

E[g∗

J,(cid:96)]

(cid:35)1/2 (cid:18) E[B2]
µT

(cid:19)1/2

+ 2F

(cid:115)

(cid:115)

aT log NT
µT

E [B]

aT log NT
µT

E [B] .

Therefore, we have

(cid:12)
(cid:12)
(cid:12)
E
(cid:12)
(cid:12)

(cid:34)µT −1
(cid:88)

(cid:96)=0

(cid:35)

g∗
J,(cid:96)

− E

(cid:34)µT −1
(cid:88)

(cid:96)=0

gJ,(cid:96)

(cid:35)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ 2E

(cid:34)µT −1
(cid:88)

(cid:96)=0

E[g∗

J,(cid:96)]

(cid:35)1/2 (cid:18) F 2E[B2]

(cid:115)

(cid:19)1/2

+ 4F 2

aT log NT
µT

E [B] .

µT

Hence, using (C.4) in Schmidt-Hieber (2020), we have
(cid:115)

(cid:35)

(cid:35)

g∗
J,(cid:96)

≤ (1 + ε)E

gJ,(cid:96)

+ 4F 2(1 + ε)

(cid:34)µT −1
(cid:88)

E

(cid:34)µT −1
(cid:88)

(cid:96)=0

(cid:96)=0

Now we show

aT log NT
µT

E [B] +

(1 + ε)2
ε

F 2E[B2]
µT

.

(A.4)

E[B] ≤ 3(cid:112)aT µT log NT , E[B2] ≤ 9aT µT log NT .

(A.5)

17

√

√

Deﬁne γ := 1+
For all j, (cid:96), we have by construction

and α := γ

37

3

aT µT log NT . Note that γ solves the equation 3γ2 − 2γ − 12 = 0.

(cid:12)
(cid:12)
(cid:12)
(cid:12)

g∗
j,(cid:96) − gj,(cid:96)
2F rj

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

4F 2aT
(cid:113) 4F 2aT log NT
µT

2F

=

(cid:114) aT µT
log NT

and

Then

Var

(cid:18) g∗

j,(cid:96) − gj,(cid:96)
2F rj

(cid:19)

≤

2E[(g∗

j,(cid:96))2]

4F 2r2
j

≤

8F 2aT E[g∗
(cid:80)µT −1

j,(cid:96)]
(cid:96)(cid:48)=0 E[g∗

j,(cid:96)(cid:48)]

4F 2 1
µT

=

2aT µT E[g∗
j,(cid:96)]
(cid:80)µT −1
(cid:96)(cid:48)=0 E[g∗
j,(cid:96)(cid:48)]

.

µT −1
(cid:88)

(cid:96)=0

Var

(cid:19)

(cid:18) g∗

j,(cid:96) − gj,(cid:96)
2F rj

≤ 2aT µT .

Using Bernstein’s inequality (cf. Lemma 2.2.9 in van der Vaart and Wellner (1996)), we have for
all x ≥ α,

(cid:32)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

P

(cid:80)µT −1

(cid:96)=0 (g∗
2F rj

j,(cid:96) − gj,(cid:96))

(cid:33)



≥ x

≤ 2 exp

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
2 x2
2aT µT + 1
3

(cid:113) aT µT
log NT





x



≤ 2 exp

−

√

1
2 γ
2aT µT + 1
3

(cid:18)

= 2 exp

−

3γ2
12 + 2γ

log NT
α

aT µT log NT

√

γ

(cid:113) aT µT
log NT
(cid:19)

aT µT log NT
(cid:18)

x

= 2 exp

−

(cid:19)

x

.

log NT
α



x



Thus,

P (B ≥ x) ≤

NT(cid:88)

j=1

P

(cid:32)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:80)µT −1

(cid:96)=0 (g∗
2F rj

j,(cid:96) − gj,(cid:96))

(cid:33)

(cid:18)

≥ x

≤ 2NT exp

−

(cid:19)

x

.

log NT
α

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Hence we have

E[B] =

(cid:90) ∞

0

P(B ≥ x)dx ≤ α +

(cid:90) ∞

P (B ≥ x)dx

α
≤ α + 2NT
log NT
≤ 3(cid:112)aT µT log NT

(cid:18)

exp

−

α
log NT
α

(cid:19)

(cid:18)

α

=

1 +

(cid:19)

2
log NT

γ(cid:112)aT µT log NT

and

E[B2] = 2

(cid:90) ∞

0

x P(B ≥ x)dx ≤ 2

(cid:90) α

xdx + 2

(cid:90) ∞

(cid:18)

x exp

−

0
log NT
α

(cid:19)

x

dx

(cid:90) ∞

α

x P(B ≥ x)dx

+

α2
log2 NT

(cid:19)

(cid:18)

exp

−

(cid:19)

α

log NT
α

≤ 2α2 + 4NT

≤ α2 + 4NT

α
(cid:18) α2

log NT

≤ 1.44γ2aT µT log NT ≤ 9aT µT log NT .

18

Combining (A.4) and (A.5), we have

(cid:34)µT −1
(cid:88)

(cid:35)

g∗
J,(cid:96)

E

(cid:96)=0

≤ (1 + ε)E

≤ (1 + ε)E

(cid:34)µT −1
(cid:88)

(cid:96)=0
(cid:34)µT −1
(cid:88)

(cid:96)=0

(cid:35)

gJ,(cid:96)

+ 12(1 + ε)F 2aT log NT +

9(1 + ε)2
ε

F 2aT log NT

(cid:35)

gJ,(cid:96)

+

21(1 + (cid:15))2
ε

F 2aT log NT ,

where the last inequality follows from 1 ≤ (1 + ε)/ε.

Step 3 (Conclusion) From (A.2) and (A.3), we have

(cid:34)µT −1
(cid:88)

E

(cid:96)=0

(cid:35)

(cid:101)g∗

J,(cid:96)

≤ E

(cid:34)µT −1
(cid:88)

(cid:96)=0

(cid:35)

g∗
J,(cid:96)

+ 4F 2aT µT β(aT )

≤ (1 + ε)E

≤ (1 + ε)E

(cid:34)µT −1
(cid:88)

(cid:96)=0
(cid:34)µT −1
(cid:88)

(cid:96)=0

(cid:35)

gJ,(cid:96)

+

(cid:35)

(cid:101)gJ,(cid:96)

+

21(1 + (cid:15))2
ε

21(1 + (cid:15))2
ε

F 2aT log NT + 4F 2aT µT β(aT )

F 2aT log NT + 4(2 + ε)F 2aT µT β(aT ).

(A.6)

Additionally, deﬁne

(cid:101)hJ,(cid:96) :=

(cid:88)

t∈I2,(cid:96)

(fJ (Xt) − f0(Xt))2, (cid:101)h∗

J,(cid:96) :=

(cid:88)

t∈I2,(cid:96)

(fJ (X ∗

t ) − f0(X ∗

t ))2.

Then a similar argument to derive (A.6) yields

(cid:34)µT −1
(cid:88)

E

(cid:35)

(cid:101)h∗
J,(cid:96)

(cid:96)=0
Now, note that

≤ (1 + ε)E

(cid:35)

(cid:101)hJ,(cid:96)

+

(cid:34)µT −1
(cid:88)

(cid:96)=0

21(1 + (cid:15))2
ε

F 2aT log NT + 4(2 + ε)F 2aT µT β(aT ).

(A.7)

R(fJ , f0) =

(cid:98)R(fJ , f0) =

1
T

1
T

E

E





µT −1
(cid:88)

(cid:96)=0





µT −1
(cid:88)

(cid:96)=0

(cid:101)g∗
J,(cid:96) +

(cid:101)gJ,(cid:96) +

µT −1
(cid:88)

(cid:96)=0

µT −1
(cid:88)

(cid:96)=0

T
(cid:88)

(cid:101)h∗
J,(cid:96) +

(fJ (X ∗

t ) − f0(X ∗

t ))2



 ,

t=2aT µT +1

T
(cid:88)

(cid:101)hJ,(cid:96) +



(fJ (Xt) − f0(Xt))2

 .

t=2aT µT +1

Together with (A.6) and (A.7), we have

R(fJ , f0) ≤

1
T

+




(1 + ε)E



(cid:34)µT −1
(cid:88)

(cid:96)=0

(cid:101)gJ,(cid:96) +

µT −1
(cid:88)

(cid:96)=0

(cid:35)

(cid:101)hJ,(cid:96)

+ E





T
(cid:88)

(fJ (X ∗

t ) − f0(X ∗

t ))2

t=2aT µT +1

2
T

(cid:26) 21(1 + (cid:15))2
ε

F 2aT log NT + 4(2 + ε)F 2aT µT β(aT )

(cid:27)










≤ (1 + ε) (cid:98)R(fJ , f0) +

+

1
T



E



T
(cid:88)

t=2aT µT +1

21(1 + ε)2
ε

F 2 log NT
µT

+ 4(2 + ε)F 2β(aT )

(cid:8)(fJ (X ∗

t ) − f0(X ∗

t ))2 − (fJ (Xt) − f0(Xt))2(cid:9)

19



 .

Since

(cid:12)
(cid:12)(fJ (Xt) − f0(Xt))2 − (fJ (X ∗

t ))2(cid:12)
(cid:12)( (cid:98)fT (x) − f0(x))2 − (fJ (x) − f0(x))2(cid:12)
(cid:12)
(cid:12) = | (cid:98)fT (x) − fJ (x)|| (cid:98)fT (x) + fJ (x) − 2f0(x)|
≤ 4F δ,

t ) − f0(X ∗

(cid:12) ≤ 4F 2,

(cid:12)
(cid:12)

(A.8)

we have

R( (cid:98)fT , f0) ≤ 4F δ + (1 + ε)

(cid:110)

(cid:98)R( (cid:98)fT , f0) + 4F δ

(cid:111)

+

21(1 + ε)2
ε

F 2 log NT
µT

+ 4(2 + ε)F 2β(aT ) +

4F 2 · 2aT
T

≤ (1 + ε) (cid:98)R( (cid:98)fT , f0) +

21(1 + ε)2
ε

F 2 log NT
µT

+

4F 2
µT

+ 4(2 + ε)F 2β(aT ) + 4(2 + ε)F δ.

(cid:3)

Lemma A.2. Let Y = {Yt}t≥1 be a time series satisfying (2.1), and set f0 := m1[0,1]d. Also, let
δ > 0 and assume NT := N (δ, F, (cid:107) · (cid:107)∞) < ∞. Suppose that there is a number F ≥ 1 such that
(cid:107)f (cid:107)∞ ≤ F for all f ∈ F ∪ {f0}. Suppose also that supp(f ) ⊂ [0, 1]d for all f ∈ F. Then, under
Assumption 3.1, for all ε ∈ (0, 1) there exists a constant Cε depending only on (Cη, ε, K) such that

(cid:98)R( (cid:98)fT , f0) ≤

1
1 − ε

ΨF

T ( (cid:98)fT ) +

1
1 − ε

inf
f ∈F

R(f, f0) + CεF 2γδ,T ,

where

γδ,T := δ +

(log T )(log NT )
T

+

1
T

.

Proof. Let {f1, . . . , fNT } be a δ-covering of F with respect to (cid:107) · (cid:107)∞ and deﬁne a random variable
J taking values in {1, . . . , NT } such that (cid:107) (cid:98)fT − fJ (cid:107)∞ ≤ δ.
Step 1 In this step, we will show that for any ¯f ∈ F,

(cid:98)R( (cid:98)fT , f0) = ΨT ( (cid:98)fT , ¯f ) + (cid:98)R( ¯f , f0) + 2E

(cid:34)

1
T

T
(cid:88)

t=1

( (cid:98)fT (Xt) − f0(Xt))η(Xt)vt

,

(A.9)

(cid:35)

where ΨT ( (cid:98)fT , ¯f ) = E

(cid:105)
(cid:104)
QT ( (cid:98)fT ) − QT ( ¯f )

. As Yt = m(Xt) + η(Xt)vt, we have

t − f 2
Y 2

0 (Xt) = (Yt − (cid:98)fT (Xt))2 − ( (cid:98)fT (Xt) − f0(Xt))2 + 2 (cid:98)fT (Xt)(Yt − f0(Xt))
= (Yt − (cid:98)fT (Xt))2 − ( (cid:98)fT (Xt) − f0(Xt))2 + 2 (cid:98)fT (Xt)η(Xt)vt.

For the second equation, we have used the fact supp( (cid:98)fT ) ⊂ [0, 1]d. Likewise,

t − f 2
Y 2

0 (Xt) = (Yt − ¯f (Xt))2 − ( ¯f (Xt) − f0(Xt))2 + 2 ¯f (Xt)η(Xt)vt.

Since

we have

E (cid:2)(f0(Xt) − ¯f (Xt))η(Xt)vt

(cid:3) = E (cid:2)(f0(Xt) − ¯f (Xt))η(Xt)E [vt|Gt−1](cid:3) = 0,

(cid:98)R( (cid:98)fT , f0) = ΨT ( (cid:98)fT , ¯f ) + (cid:98)R( ¯f , f0) + 2E

20

(cid:34)

1
T

T
(cid:88)

t=1

( (cid:98)fT (Xt) − f0(Xt))η(Xt)vt

(cid:35)

+ 2E

(cid:34)

1
T

T
(cid:88)

t=1

(f0(Xt) − ¯f (Xt))η(Xt)vt

(cid:35)

= ΨT ( (cid:98)fT , ¯f ) + (cid:98)R( ¯f , f0) + 2E

(cid:34)

1
T

T
(cid:88)

t=1

( (cid:98)fT (Xt) − f0(Xt))η(Xt)vt

.

(cid:35)

Step 2 In this step, we will show

E

(cid:34)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
T

T
(cid:88)

t=1

(fJ (Xt) − f0(Xt))η(Xt)vt

(cid:12)
(cid:35)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ 4CηK

(cid:32)

(log T )(log NT + 1

2 log 2)

T

(cid:33)1/2 (cid:18)

(cid:98)R( (cid:98)fT , f0) + F δ +

(cid:19)1/2

.

4F 2
T

(A.10)

For every j = 1, . . . , NT , deﬁne

Aj :=

Bj :=

T
(cid:88)

(fj(Xt) − f0(Xt))η(Xt)vt,

t=1
(cid:32) T

(cid:88)

t=1

(fj(Xt) − f0(Xt))2η2(Xt)(v2

t + 1)

(cid:33)1/2

and

ξj :=

Aj
j + E[B2
J ]

(cid:113)
2

B2

,

where ξj := 0 if the denominator equals 0. By the Cauchy-Schwarz inequality,

(cid:34)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
T

E

T
(cid:88)

(fJ (Xt) − f0(Xt))η(Xt)vt

t=1

(cid:12)
(cid:35)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

≤

2
T

2
T

(cid:104)
|ξJ | (cid:0)B2

E

J + E[B2

J ](cid:1)1/2(cid:105)

E (cid:2)ξ2
J

(cid:3)1/2 (cid:0)2E[B2

J ](cid:1)1/2

.

(A.11)

If E[B2

J ] > 0, from Assumption 3.1, Lemmas C.3 and C.4 with y =

(cid:113)

E[B2

J ], we have



E



(cid:113)

E[B2
J ]
j + E[B2
J ]

B2

(cid:113)



exp (cid:0)2ξ2
j

(cid:1)

 ≤ 1.

Hence



E



(cid:113)

E[B2
J ]

(cid:113)

B2

J + E[B2
J ]





exp (cid:0)2ξ2
J

(cid:1)

 ≤ E

 max
1≤j≤NT

(cid:113)

E[B2
J ]
j + E[B2
J ]

B2

(cid:113)



exp (cid:0)2ξ2
j

(cid:1)

 ≤ NT .

Therefore, by the Cauchy-Schwarz inequality, we obtain

E[exp(ξ2

J )] ≤

(cid:118)
(cid:117)
(cid:117)
(cid:117)
(cid:116)E





(cid:113)

E[B2
J ]

(cid:113)

B2

J + E[B2
J ]





exp (cid:0)2ξ2
J

(cid:1)

 E



21

(cid:113)

J + E[B2
B2
J ]
(cid:113)

E[B2
J ]



 ≤

(cid:118)
(cid:117)
(cid:117)
(cid:117)
(cid:116)NT E





(cid:113)

J + E[B2
B2
J ]
(cid:113)

E[B2
J ]



.

Since



E



(cid:113)

we conclude E[exp(ξ2
J )] ≤ 21/4
ξJ = 0 in such a case. Then by Jensen’s inequality,
J )(cid:3) ≤

(cid:3) ≤ log E (cid:2)exp(ξ2

E (cid:2)ξ2
J



(cid:115)

 ≤

E

(cid:20) B2

J + E[B2
J ]
E[B2
J ]

(cid:21)

√

2,

≤

B2
J + E[B2
J ]
(cid:113)

E[B2
J ]

√

N T . This inequality also holds when E[B2

J ] = 0 because we have

1
2

log NT +

1
4

log 2.

(A.12)

Using Assumption 3.1, (A.8) and supp(f ) ⊂ [0, 1]d for all f ∈ F,

(fJ (Xt) − f0(Xt))2(v2

t + 1)

(cid:35)

(cid:35)

( (cid:98)fT (Xt) − f0(Xt))2(v2

t + 1)

+ 4C2

η F δE

(cid:34) T

(cid:88)

(v2

t + 1)

( (cid:98)fT (Xt) − f0(Xt))2v2
t

(cid:35)

+ C2

η (cid:98)R( (cid:98)fT , f0) + 8C2

η F T δ.

t=1

(cid:35)

(A.13)

E[B2

J ] ≤ C2

η E

≤ C2

η E

= C2

η E

(cid:34) T

(cid:88)

t=1
(cid:34) T

(cid:88)

t=1
(cid:34) T

(cid:88)

t=1

(cid:35)

Decompose
(cid:34) T

(cid:88)

E

( (cid:98)fT (Xt) − f0(Xt))2v2
t

t=1

(cid:34) T

(cid:88)

( (cid:98)fT (Xt) − f0(Xt))2v2

t 1{|vt|≤K

≤ E

t=1

(cid:35)

√

2 log T }

+ E

(cid:34) T

(cid:88)

t=1

( (cid:98)fT (Xt) − f0(Xt))2v2

t 1{|vt|>K

√

2 log T }

(cid:35)

≤ 2K2T (log T ) (cid:98)R( (cid:98)fT , f0) + 4F 2T E

(cid:20)
2K2 ·

v2
t
2K2 1{|vt|>K

√

(cid:21)

2 log T }

≤ 2K2T (log T ) (cid:98)R( (cid:98)fT , f0) + 8F 2K2T E

(cid:20) v2
t
2K2 1{|vt|>K

√

(cid:21)

.

2 log T }

Since vt are sub-Gaussian, we have
(cid:20) v2
t
2K2 1{|vt|>K

E

√

2 log T }

(cid:21)

(cid:20)

≤ E

exp

(cid:20)

≤ E

exp

(cid:20)

≤ E

exp

(cid:19)

(cid:18) v2
t
2K2
(cid:19)

1{|vt|>K
(cid:18)

(cid:21)

√

2 log T }

(cid:18) v2
t
K2
(cid:18) v2
t
K2

exp

−

(cid:19)

(cid:18)

exp

−

(cid:21)

√

2 log T }

(cid:19)

v2
t
2K2
2K2 log T
2K2

1{|vt|>K
(cid:19)(cid:21)

≤

2
T

.

Then by (A.13)-(A.15) and (A.33),

J ] ≤ 4C2
Combining (A.11), (A.12), and (A.16), we have (A.10).

η K2T (log T ) (cid:98)R( (cid:98)fT , f0) + 16C2

E[B2

η F 2K2 + 4C2

η F K2T δ.

Step 3 In this step, we complete the proof. By (A.10) and the AM-GM inequality,

E

(cid:34)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
T

T
(cid:88)

t=1

(fJ (Xt) − f0(Xt))η(Xt)vt

22

(cid:35)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

ε
2

(cid:98)R( (cid:98)fT , f0) + γε.

(A.14)

(A.15)

(A.16)

where

γε =
Combining this with (A.9), we have for any ¯f ∈ F,

εT

16C2

η K2(log T )(2 log NT + log 2)

+

F δε
2

+

2F 2ε
T

.

(cid:98)R( (cid:98)fT , f0) = ΨT ( (cid:98)fT , ¯f ) + (cid:98)R( ¯f , f0) + 2E

(cid:34)

1
T

T
(cid:88)

t=1

( (cid:98)fT (Xt) − fJ (Xt))η(Xt)vt

(cid:35)

+ 2E

(cid:34)

1
T

T
(cid:88)

t=1

(fJ (Xt) − f0(Xt))η(Xt)vt

(cid:35)

≤ ΨF

T ( (cid:98)fT ) + (cid:98)R( ¯f , f0) +

2Cηδ
T

T
(cid:88)

t=1

E[|vt|] + ε (cid:98)R( (cid:98)fT , f0) + 2γε

≤ ΨF

T ( (cid:98)fT ) + (cid:98)R( ¯f , f0) + 2Cηδ + ε (cid:98)R( (cid:98)fT , f0) + 2γε.

Since (cid:98)R( ¯f , f0) = R( ¯f , f0), we have

where γ(cid:48)

T ( (cid:98)fT ) + R( ¯f , f0) + γ(cid:48)
(1 − ε) (cid:98)R( (cid:98)fT , f0) ≤ ΨF
ε,
ε = 2Cηδ + 2γε. Taking the inﬁmum over ¯f ∈ F, we conclude
R( ¯f , f0) +

(cid:98)R( (cid:98)fT , f0) ≤

T ( (cid:98)fT ) +

ΨF

1
1 − ε

inf
f ∈F

1
1 − ε

1
1 − ε

Noting F ≥ 1, we obtain the desired result.

γ(cid:48)
ε.

(cid:3)

Proof of Theorem 3.1. From Lemmas A.1 and A.2 with F = Fσ(L, N, B, F, S), δ = T −1 and aT =
C−1

2,β log T , we have

R( (cid:98)fT , f0) ≤

1 + ε
1 − ε

1 + ε
1 − ε

inf
f ∈F

R(f, f0)

ΨF

+

T ( (cid:98)fT ) +
(cid:18) 21(1 + ε)2
ε

(cid:19) F 2
µT
+ Cε(1 + ε)F 2 (log T )(log NT )

log NT + 4

.

T

+ (2Cε(1 + ε) + 8(2 + ε))

F 2
T

We may assume µT = (cid:98) T
2aT
Lemma C.5,

(cid:99) > 0; otherwise T < 2aT and thus C2,β/2 < log T

T . Moreover, using

log N (cid:0)T −1, Fσ(L, N, B, F, S), (cid:107) · (cid:107)∞
(cid:1) ∨ exp(10), there exits a universal con-
Therefore, letting NT := N (cid:0)T −1, Fσ(L, N, B, F, S), (cid:107) · (cid:107)∞
stant c1 > 0 such that log NT ≤ c1S(L + 1) log ((L + 1)(N + 1)BT ). Combining all the estimates
(cid:3)
above, we obtain the desired result.

(cid:1) ≤ 2S(L + 1) log ((L + 1)(N + 1)BT ) .

A.2. Proof of Theorem 3.2. Without loss of generality, we may assume (1 + Cτ )T −1 < 1;
otherwise, the desired bound holds with C = 4(1 + Cτ ) because R( (cid:98)fT , f0) ≤ 4F 2. Then we have
δT := T −1 + τT (LT + 1)((NT + 1)BT )LT +1 ≤ (1 + Cτ )T −1 < 1.

(A.17)

Next, for k ≥ 0 and s > 0, we deﬁne

FT,k,s :=

(cid:110)
f ∈ F : 2k−1s1{k(cid:54)=0} ≤ JT (f ) < 2ks

(cid:111)

, ΩT,k,s :=

(cid:110)

(cid:98)fT ∈ FT,k,s

(cid:111)

.

23

Note that Ω = (cid:83)∞
N (δT , FT,k,s, (cid:107) · (cid:107)∞) and let {f k
struction, we can deﬁne a random variable Jk taking values in {1, . . . , Nk} such that (cid:107) (cid:98)fT − f k
Jk
δT on ΩT,k,s.

k=0 ΩT,k,s and ΩT,k1,s ∩ ΩT,k2,s = ∅ for k1 (cid:54)= k2. For each k, set Nk :=
} be a δT -covering of FT,k,s with respect to (cid:107) · (cid:107)∞. By con-
(cid:107)∞ ≤

1 , . . . , f k
Nk

Step 1 (Reduction to independence) Let aT be a positive number such that 2aT < T , aT → ∞
and aT /ιλ(T ) → 0 as T → ∞, and set µT := (cid:98)T /2aT (cid:99) > 0. For (cid:96) = 0, . . . , µT − 1, let I1,(cid:96) =
{2(cid:96)aT + 1, . . . , (2(cid:96) + 1)aT }, I2,(cid:96) = {(2(cid:96) + 1)aT + 1, . . . , 2((cid:96) + 1)aT }. For each k, we deﬁne

(cid:48)



(cid:96) := ((cid:101)gk
(cid:101)gk

1,(cid:96), . . . , (cid:101)gk

Nk,(cid:96))(cid:48) =

(cid:101)g∗,k

(cid:96)

:= ((cid:101)g∗,k

1,(cid:96) , . . . , (cid:101)g∗,k

Nk,(cid:96))(cid:48) =

(cid:88)



t∈I1,(cid:96)





(cid:88)

(f k

1 (Xt) − f0(Xt))2, . . . ,

(cid:88)

t∈I1,(cid:96)

(f k
Nk

(Xt) − f0(Xt))2



,

(f k

1 (X ∗

t ) − f0(X ∗

t ))2, . . . ,

(cid:88)

(f k
Nk

t∈I1,(cid:96)

(X ∗

t ) − f0(X ∗

t ))2



(cid:48)



.

t∈I1,(cid:96)

From a similar argument in Step1 of the proof of Lemma A.1, we can show that there exist two
sequences of independent RNk -valued random variables {gk
such that for all
(cid:96) = 0, . . . , µT − 1,

(cid:96) }µT −1

and {g∗,k

(cid:96) }µT −1

(cid:96)=0

(cid:96)=0

(cid:96) , P(gk

gk
(cid:96)
g∗,k
(cid:96)
j,(cid:96) ≤ 4F 2aT and 0 ≤ g∗,k
j,(cid:96) ≤ 4F 2aT a.s., where gk

) ≤ β(aT ).
j,(cid:96) and g∗,k
, respectively. Further, let kT,s be an integer such that (cid:80)

(cid:96) ) ≤ β(aT ),
(cid:54)= (cid:101)g∗,k

L= (cid:101)gk
L= (cid:101)g∗,k

(cid:96) (cid:54)= (cid:101)gk

, P(g∗,k

(cid:96)

(cid:96)

(cid:96)

with 0 ≤ gk
and g∗,k
(cid:96) = 0, . . . , µT − 1, we deﬁne
(cid:16)

(cid:96)

(cid:101)g(cid:96) :=

(cid:16)

g(cid:96) :=

(cid:96) )(cid:48), . . . , ((cid:101)gkT,s
((cid:101)g0
(cid:96) )(cid:48), . . . , (gkT,s
(g0

(cid:96)

(cid:96)

)(cid:48)(cid:17)(cid:48)
)(cid:48)(cid:17)(cid:48)

, (cid:101)g∗
, g∗

(cid:96) :=

(cid:96) :=

(cid:16)

(cid:16)

(cid:96) )(cid:48), . . . , ((cid:101)g∗,kT,s
((cid:101)g∗,0
(cid:96) )(cid:48), . . . , (g∗,kT,s
(g∗,0

(cid:96)

(cid:96)

)(cid:48)(cid:17)(cid:48)
)(cid:48)(cid:17)(cid:48)

,

.

j,(cid:96) are the j-th components of gk
(cid:96)
k≥kT,s+1 P(ΩT,k,s) ≤ 1/µT . For

We can also assume that for all (cid:96) = 0, . . . , µT − 1,

g(cid:96)

g∗
(cid:96)

L= (cid:101)g(cid:96), P(g(cid:96) (cid:54)= (cid:101)g(cid:96)) ≤ β(aT ),
L= (cid:101)g∗
(cid:96) (cid:54)= (cid:101)g∗
(cid:96) ) ≤ β(aT ).

(cid:96) , P(g∗

Likewise, deﬁne

(cid:96) := ((cid:101)hk
(cid:101)hk

1,(cid:96), . . . , (cid:101)hk

Nk,(cid:96)) =





(cid:88)

(f k

1 (Xt) − f0(Xt))2, . . . ,


(cid:48)

(f k
Nk

(Xt) − f0(Xt))2



,

(cid:88)

t∈I2,(cid:96)

(cid:101)h∗,k
(cid:96)

:= ((cid:101)h∗,k

1,(cid:96) , . . . , (cid:101)h∗,k

Nk,(cid:96)) =

t∈I2,(cid:96)





(cid:88)

t∈I2,(cid:96)

(f k

1 (X ∗

t ) − f0(X ∗

t ))2, . . . ,

(cid:88)

(f k
Nk

t∈I2,(cid:96)

(X ∗

t ) − f0(X ∗

t ))2


(cid:48)



and there exist two sequences of independent RNk -valued random variables {hk
such that for all (cid:96) = 0, . . . , µT − 1,

(cid:96) }µT −1

(cid:96)=0 and {h∗,k

(cid:96) }µT −1

(cid:96)=0

hk
(cid:96)
h∗,k
(cid:96)

L= (cid:101)hk
L= (cid:101)h∗,k

(cid:96)

(cid:96) , P(hk

(cid:96) (cid:54)= (cid:101)hk

, P(h∗,k

(cid:96)

24

(cid:96) ) ≤ β(aT ),
(cid:54)= (cid:101)h∗,k

(cid:96) ) ≤ β(aT ).

For (cid:96) = 0, . . . , µT − 1, we deﬁne

(cid:16)

(cid:16)

(cid:101)h(cid:96) :=

h(cid:96) :=

((cid:101)h0

(h0

(cid:96) )(cid:48), . . . , ((cid:101)hkT,s
(cid:96) )(cid:48), . . . , (hkT,s

(cid:96)

(cid:96)

)(cid:48)(cid:17)(cid:48)
)(cid:48)(cid:17)(cid:48)

, (cid:101)h∗

(cid:96) :=

, h∗

(cid:96) :=

(cid:16)

(cid:16)

((cid:101)h∗,0

(cid:96) )(cid:48), . . . , ((cid:101)h∗,kT,s
(cid:96) )(cid:48), . . . , (h∗,kT,s

(cid:96)

(cid:96)

(h∗,0

)(cid:48)(cid:17)(cid:48)
)(cid:48)(cid:17)(cid:48)

,

.

We can also assume that for all (cid:96) = 0, . . . , µT − 1,

h(cid:96)

h∗
(cid:96)

L= (cid:101)h(cid:96), P(h(cid:96) (cid:54)= (cid:101)h(cid:96)) ≤ β(aT ),
L= (cid:101)h∗
(cid:96) (cid:54)= (cid:101)h∗

(cid:96) ) ≤ β(aT ).

(cid:96) , P(h∗

Step 2 (Bounding the generalization error) In this step, we will show

(cid:18)

R( (cid:98)fT , f0) ≤ 3

(cid:98)R( (cid:98)fT , f0) +

(cid:19)

E[JT ( (cid:98)fT )]

+

1
3



1 +

32F 2aT
T

+ 24F δT +

s
4

+

32F 2
µT

+ 48F 2β(aT )

(cid:16)

25/4 exp

(cid:16)

1 − exp

− sT

128F 2aT

− sT

256F 2aT

(cid:17)





(cid:17)

(A.18)

for T ≥ T0, where T0 > 0 is a constant depending only on (CL, CN , CB, ν0, ν1, ν2, ιλ). Deﬁne

D := R( (cid:98)fT , f0) − (cid:98)R( (cid:98)fT , f0) −

E[JT ( (cid:98)fT )],

(cid:34)

(cid:32)

Dk := E

1ΩT,k,s

1
T

T
(cid:88)

t=1

∆t( (cid:98)fT ) −

(cid:33)(cid:35)

JT ( (cid:98)fT )

,

1
2

1
2

where ∆t(f ) = (f (X ∗
∆t(f k
Jk

)| ≤ 8F δT , we have

t ) − f0(X ∗

t ))2 − (f (Xt) − f0(Xt))2. Note that D = (cid:80)∞

k=0 Dk. Since |∆t( (cid:98)fT ) −

(cid:32)

(cid:34)
1ΩT,k,s

Dk ≤ E

(cid:32)

(cid:34)
1ΩT,k,s

≤ E

1
T

1
T

T
(cid:88)

t=1

T
(cid:88)

t=1

∆t(f k
Jk

) −

∆t(f k
Jk

) −

1
2

1
2

(cid:33)(cid:35)

(cid:34)

(cid:32)

JT ( (cid:98)fT )

+ E

1ΩT,k,s

(cid:12)
(cid:12)∆t( (cid:98)fT ) − ∆t(f k
(cid:12)

Jk

(cid:12)
(cid:12)
)
(cid:12)

(cid:33)(cid:35)

1
T

T
(cid:88)

t=1

(cid:33)(cid:35)

JT ( (cid:98)fT )

+ 8F δT P(ΩT,k,s).

T
(cid:88)

(cid:33)(cid:35)
)

∆t(f k
Jk

Observe that
(cid:32)

(cid:34)

E

1ΩT,k,s

1
T

(cid:34)
1ΩT,k,s

≤ E

(cid:34)
1ΩT,k,s

≤ E

1
T

1
T

t=1
(cid:32)µT −1
(cid:88)

(cid:96)=0
(cid:32)µT −1
(cid:88)

µT −1
(cid:88)

(cid:101)g∗,k
Jk,(cid:96) +

µT −1
(cid:88)

(cid:101)h∗,k
Jk,(cid:96) −

µT −1
(cid:88)

(cid:101)gk
Jk,(cid:96) −

(cid:96)=0

(cid:96)=0

(cid:96)=0

(cid:33)(cid:35)



(cid:101)hk
Jk,(cid:96)

+E

1ΩT,k,s





1
T

T
(cid:88)





|∆t(f k
Jk

)|




t=2aT µT +1

µT −1
(cid:88)

(cid:101)g∗,k
Jk,(cid:96) +

µT −1
(cid:88)

(cid:101)h∗,k
Jk,(cid:96) −

µT −1
(cid:88)

(cid:101)gk
Jk,(cid:96) −

(cid:101)hk
Jk,(cid:96)

(cid:96)=0

(cid:96)=0

(cid:96)=0

(cid:96)=0

(cid:33)(cid:35)

+

8F 2
µT

P(ΩT,k,s).

Further,

1
T

µT −1
(cid:88)

∞
(cid:88)

(cid:96)=0

k=0

(cid:104)
1ΩT,k,s

E

(cid:12)
Jk,(cid:96) − g∗,k
(cid:12)(cid:101)g∗,k
(cid:12)
Jk,(cid:96)

(cid:12)
(cid:12)
(cid:12)

(cid:105)

≤

4F 2aT
T

µT −1
(cid:88)

∞
(cid:88)

k=0

(cid:96)=0
25

(cid:20)

E

1ΩT,k,s1(cid:110)

(cid:21)

(cid:111)

Jk ,(cid:96)(cid:54)=g∗,k
(cid:101)g∗,k
Jk ,(cid:96)

1ΩT,k,s1{(cid:101)g∗

(cid:96) }
(cid:96) (cid:54)=g∗

(cid:105)

E

(cid:104)
1ΩT,k,s1{(cid:101)g∗

(cid:96) }
(cid:96) (cid:54)=g∗

E

(cid:104)

1{(cid:101)g∗

(cid:96) }
(cid:96) (cid:54)=g∗

(cid:105)

+

∞
(cid:88)

k=kT,s+1

µT −1
(cid:88)

∞
(cid:88)

(cid:104)

E

(cid:96)=0

µT −1
(cid:88)

k=0


kT,s
(cid:88)

(cid:96)=0

k=0





µT −1
(cid:88)

(cid:96)=0

(cid:18)

≤

4F 2aT
T

≤

4F 2aT
T

≤

≤

4F 2aT
T

4F 2aT
T

(cid:105)

+

∞
(cid:88)

E (cid:2)1ΩT,k,s


(cid:3)


k=kT,s+1

(cid:3)


E (cid:2)1ΩT,k,s

· µT

β(aT ) +

(cid:19)

1
µT

≤ 2F 2

(cid:18)

β(aT ) +

(cid:19)

1
µT

. (A.19)

Therefore, similar arguments to obtain (A.19) yield

(cid:34)

(cid:32)

E

1ΩT,k,s

∞
(cid:88)

k=0

1
T

T
(cid:88)

t=1

∆t(f k
Jk

) −

1
2

(cid:33)(cid:35)

JT ( (cid:98)fT )

(cid:32)

(cid:34)
1ΩT,k,s

E

≤

∞
(cid:88)

k=0

(cid:34)
1ΩT,k,s

E

+

∞
(cid:88)

k=0

1
T

(cid:32)

µT −1
(cid:88)

(g∗,k

Jk,(cid:96) − gk

Jk,(cid:96)) −

(cid:96)=0

(cid:33)(cid:35)

1
4

JT ( (cid:98)fT )

1
T

µT −1
(cid:88)

(h∗,k

Jk,(cid:96) − hk

Jk,(cid:96)) −

(cid:96)=0

(cid:33)(cid:35)

JT ( (cid:98)fT )

+

1
4

16F 2
µT

+ 8F 2β(aT ).

Deﬁne

bj,k := 1ΩT,k,s

µT −1
(cid:88)

(g∗,k

j,(cid:96) − gk

j,(cid:96))2,

¯bk := E [bJk,k] ,

(cid:113)

rj,k := 2

bj,k + ¯bk,

Bj,k := 1ΩT,k,s

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:96)=0
(cid:80)µT −1

(cid:96)=0 (g∗,k
j,(cid:96) − gk
rj,k

j,(cid:96))

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

Bk := BJk,k,

where Bj,k := 0 if the denominator equals 0. Then we have

(cid:32)

(cid:34)
1ΩT,k,s

E

1
T

µT −1
(cid:88)

(g∗,k

Jk,(cid:96) − gk

Jk,(cid:96)) −

(cid:96)=0

(cid:33)(cid:35)

JT ( (cid:98)fT )

≤ E

1
4

(cid:20)(cid:18) 1
T

rJk,kBk − 1ΩT,k,s

(cid:19)(cid:21)

JT ( (cid:98)fT )

.

1
4

Applying the AM-GM inequality, we have

(cid:34)

E

1ΩT,k,s

(cid:32)

1
T

µT −1
(cid:88)

(g∗,k

Jk,(cid:96) − gk

Jk,(cid:96)) −

(cid:33)(cid:35)

JT ( (cid:98)fT )

1
4

≤ E

(cid:34) r2
Jk,k
64F 2T aT

(cid:96)=0
(cid:35)

+ E

=

8
64F 2T aT
=: Ir,k + IB,k.

E [bJk,k] + E

B2

(cid:20) 16F 2aT
T
(cid:20) 16F 2aT
T

k − 1ΩT,k,s

(cid:21)

JT ( (cid:98)fT )

1
4

B2

k − 1ΩT,k,s

(cid:21)

JT ( (cid:98)fT )

1
4

Now we evaluate Ir,k. Observe that

E[bJk,k]

26

≤ 4F 2aT E

1ΩT,k,s

(cid:34)

(cid:34)

≤ 4F 2aT E

1ΩT,k,s

(cid:12)
(cid:12)g∗,k
Jk,(cid:96) − gk
(cid:12)

Jk,(cid:96)

(cid:12)
(cid:12)(cid:101)g∗,k
Jk,(cid:96) − (cid:101)gk
(cid:12)

Jk,(cid:96)

(cid:35)

(cid:35)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

µT −1
(cid:88)

(cid:96)=0
µT −1
(cid:88)

(cid:96)=0

(cid:34)

+ 4F 2aT E

1ΩT,k,s

µT −1
(cid:88)

(cid:96)=0

(cid:16)(cid:12)
Jk,(cid:96) − g∗,k
(cid:12)(cid:101)g∗,k
(cid:12)
Jk,(cid:96)

(cid:12)
(cid:12)
(cid:12) +

(cid:12)
Jk,(cid:96) − gk
(cid:12)(cid:101)gk
(cid:12)

Jk,(cid:96)

(cid:12)
(cid:12)
(cid:12)

(cid:35)

(cid:17)



≤ 4F 2aT E

1ΩT,k,s

µT −1
(cid:88)

(cid:88)

( (cid:98)fT (X ∗

t ) − f0(X ∗

t ))2


 + 4F 2aT E



1ΩT,k,s

µT −1
(cid:88)

(cid:88)

(cid:96)=0

t∈I1,(cid:96)

( (cid:98)fT (Xt) − f0(Xt))2

(cid:96)=0

µT −1
(cid:88)

(cid:96)=0

µT −1
(cid:88)

t∈I1,(cid:96)
(cid:12)
(cid:12)
(cid:12)
(cid:101)g∗,k
Jk,(cid:96) −
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:101)gk
Jk,(cid:96) −
(cid:12)
(cid:12)
(cid:12)
(cid:20)

(cid:96)=0

µT −1
(cid:88)

(cid:96)=0



+ 4F 2aT E

1ΩT,k,s



+ 4F 2aT E

1ΩT,k,s

+ 4F 2aT · 8F 2aT



≤ 4F 2aT E

1ΩT,k,s

( (cid:98)fT (X ∗

t ) − f0(X ∗

t ))2

( (cid:98)fT (Xt) − f0(Xt))2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)




(cid:88)

t∈I1,(cid:96)

(cid:88)

t∈I1,(cid:96)

(cid:18)

E

1ΩT,k,s

1(cid:110)

Jk ,(cid:96)(cid:54)=g∗,k
(cid:101)g∗,k
Jk ,(cid:96)

(cid:111) + 1(cid:110)

Jk ,(cid:96)(cid:54)=gk
(cid:101)gk

(cid:19)(cid:21)

(cid:111)

Jk ,(cid:96)



 + 4F 2aT E

1ΩT,k,s

µT −1
(cid:88)

(cid:88)

(cid:96)=0

t∈I1,(cid:96)

( (cid:98)fT (X ∗

t ) − f0(X ∗

t ))2

µT −1
(cid:88)

(cid:88)

(cid:96)=0

t∈I1,(cid:96)

( (cid:98)fT (Xt) − f0(Xt))2









+ 4F 2aT (4F aT µT δT + 4F aT µT δT )P(ΩT,k,s)

+ 32F 4a2
T

µT −1
(cid:88)

E

(cid:96)=0

(cid:20)

(cid:18)

1ΩT,k,s

1(cid:110)

Jk ,(cid:96)(cid:54)=g∗,k
(cid:101)g∗,k
Jk ,(cid:96)

(cid:111) + 1(cid:110)

Jk ,(cid:96)(cid:54)=gk
(cid:101)gk

Jk ,(cid:96)

(cid:19)(cid:21)
(cid:111)

.

(A.20)

For the above inequalities, we used the fact that

(cid:12)
Jk,(cid:96) − g∗,k
(cid:12)(cid:101)g∗,k
(cid:12)
Jk,(cid:96)

(cid:12)
(cid:12) ≤ 4F 2aT ,
(cid:12)

(cid:12)
Jk,(cid:96) − gk
(cid:12)(cid:101)gk
(cid:12)

Jk,(cid:96)

(cid:12)
(cid:12) ≤ 4F 2aT a.s.
(cid:12)

and on ΩT,k,s, (cid:98)fT ∈ FT,k,s and

(cid:12)
(cid:12)(f k
(cid:12)

Jk

(x) − f0(x))2 − ( (cid:98)fT (x) − f0(x))2(cid:12)
(cid:12)
(cid:12) ≤

(cid:12)
(cid:12) (cid:98)fT (x) − f k
(cid:12)

Jk

(x)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12) (cid:98)fT (x) + f k
(cid:12)

Jk

(cid:12)
(cid:12)
(cid:12) ≤ 4F δT .
(x) − 2f0(x)

Hence we obtain

∞
(cid:88)

k=0

Ir,k ≤

4F 2aT
8F 2T aT



E





µT −1
(cid:88)

(cid:88)

( (cid:98)fT (X ∗

t ) − f0(X ∗

t ))2



 + E





µT −1
(cid:88)

(cid:88)

( (cid:98)fT (Xt) − f0(Xt))2









(cid:96)=0

+

t∈I1,(cid:96)
32F 4 · 4a2

T µT β(aT )

8F 2T aT

µT −1
(cid:88)

(cid:88)

( (cid:98)fT (X ∗

t ) − f0(X ∗

t ))2

32F 3a2

T µT δT

8F 2T aT

+

1
2

=





E



1
T

(cid:96)=0

t∈I1,(cid:96)
+ 2F δT + 8F 2β(aT ).

(cid:96)=0

t∈I1,(cid:96)





 + E



1
T

µT −1
(cid:88)

(cid:88)

(cid:96)=0

t∈I1,(cid:96)





( (cid:98)fT (Xt) − f0(Xt))2





(A.21)

27

Now we evaluate IB,k. If ¯bk > 0, applying Lemmas C.2 and C.4 with y =

(cid:112)¯bk, we obtain



E



(cid:113)

(cid:112)¯bk
bj,k + ¯bk



exp (cid:0)2B2
j,k

(cid:1)

 ≤ 1.

Hence



E



(cid:112)¯bk
bJk,k + ¯bk

(cid:113)





exp (cid:0)2B2

Jk,k

(cid:1)

 ≤ E

 max
1≤j≤Nk

(cid:112)¯bk
bj,k + ¯bk

(cid:113)



exp (cid:0)2B2
j,k

(cid:1)

 ≤ Nk.

Therefore, by the Cauchy-Schwarz inequality, we obtain

E[exp(B2

k)] ≤

(cid:118)
(cid:117)
(cid:117)
(cid:117)
(cid:116)E





(cid:112)¯bk
bJk,k + ¯bk

(cid:113)

(cid:16)

exp

2B2

Jk,k

(cid:17)





 E



(cid:113)

bJk,k + ¯bk
(cid:112)¯bk



 ≤

(cid:118)
(cid:117)
(cid:117)
(cid:117)
(cid:116)NkE





(cid:113)

bJk,k + ¯bk
(cid:112)¯bk



.

Since

we conclude



E



(cid:113)

bJk,k + ¯bk
(cid:112)¯bk



(cid:115)

 ≤

E

(cid:20) bJk,k + ¯bk
¯bk

(cid:21)

√

2,

≤

(A.22)
This inequality also holds when ¯bk = 0 because we always have Bk = 0 in such a case. Thus, for
k ≥ 1, we have

Nk.

E[exp(B2

k)] ≤ 21/4(cid:112)

(cid:20) 16F 2aT
T

E

B2

k − 1ΩT,k,s

(cid:21)
JT ( (cid:98)fT )

≤

1
4

(cid:90) ∞

P

0

(cid:18) 16F 2aT
T
(cid:90) ∞

(cid:19)

B2

k − 1ΩT,k,s

JT ( (cid:98)fT ) > x

1
4
T (x + 2k−3s)
16F 2aT
(cid:18)

2k−3sT
16F 2aT

exp

−

(cid:19)

dx

(cid:19)

,

dx

(A.23)

(cid:18)

−

exp

≤ 21/4(cid:112)

Nk

= 21/4(cid:112)

Nk ·

0
16F 2aT
T

where the second inequality follows from Markov’s inequality and (A.22). Recall that δT is deﬁned
by (A.17). Then by Lemma C.6,

log Nk ≤

2ks
λT

(LT + 1) log

(cid:18) (LT + 1)(NT + 1)BT
T −1

(cid:19)

≤ C1

2ks
λT

log1+ν0 T,

(A.24)

where C1 is a positive constant depending only on (CL, CN , CB, ν0, ν1, ν2). Since aT /ιλ(T ) → 0
as T → ∞, there is a constant T0 depending only on C1 and ιλ such that (C1 log1+ν0 T )/λT ≤
T /(128F 2aT ) whenever T ≥ T0. For such T , we have

21/4(cid:112)

Nk ·

16F 2aT
T

(cid:18)

exp

−

(cid:19)

2k−3sT
16F 2aT

≤

=

21/4 · 16F 2aT
T
21/4 · 16F 2aT
T

exp

exp

For k = 0,

(cid:18) 2ksT

(cid:18)

256F 2aT
2ksT
256F 2aT

−

(cid:19)

−

2ksT
128F 2aT
(cid:19)

.

(cid:20) 16F 2aT
T

E

B2

0 − 1ΩT,0,s

(cid:21)

JT ( (cid:98)fT )

≤

1
4

16F 2aT
T

28

E[B2

0] =

16F 2aT
T

log(exp(E[B2

0]))

≤

0)])

log(E[exp(B2

16F 2aT
T
16F 2aT
T
8F 2aT
T
8F 2aT
T
For the second inequality, we used Jensen’s inequality and for the last inequality, we used log N0 ≤
sT /(128F 2aT ). Combining (A.23) and (A.25), we have

log(21/4(cid:112)

(1 + log N0)

(A.25)

s
16

N0)

≤

≤

≤

+

.

∞
(cid:88)

k=0

IB,k ≤

8F 2aT
T

+

s
16

≤

s
16

+

8F 2aT
T

Therefore, (A.21) and (A.26) yield

+



21/4 · 16F 2aT
T

(cid:16)

25/4 exp

1 +

(cid:16)

1 − exp

∞
(cid:88)

exp

(cid:18)

k=1
− sT

128F 2aT

− sT

256F 2aT

(cid:19)

−

2ksT
256F 2aT


(cid:17)

 .

(cid:17)

(A.26)

(cid:34)

E

1ΩT,k,s

(cid:32)

1
T

∞
(cid:88)

k=0

∞
(cid:88)

≤

(Ir,k + IB,k)

µT −1
(cid:88)

(g∗,k

Jk,(cid:96) − gk

Jk,(cid:96)) −

(cid:96)=0

(cid:33)(cid:35)

JT ( (cid:98)fT )

1
4

k=0




≤

E



1
2

1
T

µT −1
(cid:88)

(cid:88)

(cid:96)=0

t∈I1,(cid:96)

( (cid:98)fT (X ∗

t ) − f0(X ∗

t ))2





 + E



1
T

+ 2F δT + 8F 2β(aT ) +

s
16

+

8F 2aT
T



1 +

µT −1
(cid:88)

(cid:88)





( (cid:98)fT (Xt) − f0(Xt))2





t∈I1,(cid:96)

(cid:96)=0
(cid:16)

25/4 exp

(cid:16)

1 − exp

− sT

128F 2aT

− sT

256F 2aT

(cid:17)



 .

(cid:17)

Likewise, we have

(cid:34)

E

1ΩT,k,s

(cid:32)

1
T

∞
(cid:88)

k=0

µT −1
(cid:88)

(h∗,k

Jk,(cid:96) − hk

Jk,(cid:96)) −

(cid:96)=0

(cid:33)(cid:35)

JT ( (cid:98)fT )

1
4





E



1
T

≤

1
2

µT −1
(cid:88)

(cid:88)

(cid:96)=0

t∈I2,(cid:96)

( (cid:98)fT (X ∗

t ) − f0(X ∗

t ))2





 + E



1
T

+ 2F δT + 8F 2β(aT ) +

s
16

+

8F 2aT
T



1 +

Hence,

D =

≤

∞
(cid:88)

k=1
(cid:16)
1
2

Dk

R( (cid:98)fT , f0) + (cid:98)R( (cid:98)fT , f0)

(cid:17)

29

µT −1
(cid:88)

(cid:88)





( (cid:98)fT (Xt) − f0(Xt))2





t∈I2,(cid:96)

(cid:96)=0
(cid:16)

25/4 exp

(cid:16)

1 − exp

− sT

128F 2aT

− sT

256F 2aT

(cid:17)



 .

(cid:17)

+ 12F δT + 16F 2β(aT ) +

s
8

+

16F 2aT
T



1 +

(cid:16)

25/4 exp

(cid:16)

1 − exp

− sT

128F 2aT

− sT

256F 2aT

(cid:17)



 +

(cid:17)

16F 2
µT

+ 8F 2β(aT ).

Since D = R( (cid:98)fT , f0) − (cid:98)R( (cid:98)fT , f0) − 1

2 E[JT ( (cid:98)fT )], we obtain (A.18).
Step 3 (Bounding the expected empirical error) In this step, we will show that for any ¯f ∈ F,

(cid:98)R( (cid:98)fT , f0) +

1
3

E[JT ( (cid:98)fT )] ≤ 2

(cid:17)
(cid:16) ¯ΨT ( (cid:98)fT , ¯f ) + R( ¯f , f0) + JT ( ¯f )


(cid:16)

+

96K2C2
η (log T )
T

25/4 exp

−

1 +

1 − exp

(cid:16)

−

+

22F 2
3T (log T )

5sT
1152K2C2

η (log T )

5sT
2304K2C2

η (log T )

(cid:17)



 + 4CηδT +

(cid:17)

5
12

s

(A.27)

for T ≥ T1, where ¯ΨT ( (cid:98)fT , ¯f ) = E
(CL, CN , CB, ν0, ν1, ν2, K, Cη, ιλ).

For each t = 1, . . . , T and for any ¯f ∈ F, we have

(cid:104) ¯QT ( (cid:98)fT ) − ¯QT ( ¯f )

(cid:105)

and T1 > 0 is a constant depending only on

E[( ¯f (Xt) − Yt)2 + JT ( ¯f )] − E[( (cid:98)fT (Xt) − Yt)2 + JT ( (cid:98)fT )]
= E[( ¯f (Xt) − f0(Xt))2] + JT ( ¯f ) − E[( (cid:98)fT (Xt) − f0(Xt))2] − E[JT ( (cid:98)fT )]

+ 2E[( (cid:98)fT (Xt) − f0(Xt))η(Xt)vt],

where we used the fact E[ ¯f (Xt)η(Xt)vt] = E[ ¯f (Xt)η(Xt)E[vt|Gt−1]] = 0. Then we have

(cid:98)R( (cid:98)fT , f0) +

1
6

E[JT ( (cid:98)fT )] = ¯ΨT ( (cid:98)fT , ¯f ) + R( ¯f , f0) + JT ( ¯f ) −

5
6

E[JT ( (cid:98)fT )]
(cid:35)

+ 2E

(cid:34)

1
T

T
(cid:88)

t=1

( (cid:98)fT (Xt) − f0(Xt))η(Xt)vt

.

(A.28)

Observe that
(cid:34)
T
(cid:88)

2E

1
T

t=1

( (cid:98)fT (Xt) − f0(Xt))η(Xt)vt

(cid:35)

=

=

2
T

2
T

∞
(cid:88)

k=0

∞
(cid:88)

k=0

(cid:34)
1ΩT,k,s

E

(cid:34)
1ΩT,k,s

E

T
(cid:88)

t=1

T
(cid:88)

t=1

( (cid:98)fT (Xt) − f0(Xt))η(Xt)vt

(cid:35)

(cid:35)

(f k
Jk

(Xt) − f0(Xt))η(Xt)vt

+

2
T

∞
(cid:88)

k=0

(cid:34)
1ΩT,k,s

E

T
(cid:88)

t=1

( (cid:98)fT (Xt) − f k
Jk

(Xt))η(Xt)vt

.

(cid:35)

Since supx | (cid:98)fT (x) − f k
Jk

(x)| ≤ δT on ΩT,k,s and
(cid:34) T

(cid:35)

2CηδT
T

E

(cid:88)

t=1

|vt|

≤

2CηδT
T

(cid:113)

E (cid:2)v2
t

(cid:3) = 2CηδT ,

T
(cid:88)

t=1

we have
(cid:34)

2E

1
T

T
(cid:88)

t=1

( (cid:98)fT (Xt) − f0(Xt))η(Xt)vt

(cid:35)

≤

2
T

∞
(cid:88)

k=0

(cid:34)
1ΩT,k,s

E

T
(cid:88)

t=1

(f k
Jk

(Xt) − f0(Xt))η(Xt)vt

+ 2CηδT .

(cid:35)

(A.29)

30

Deﬁne

Uj,k :=

T
(cid:88)

t=1

(f k

j (Xt) − f0(Xt))η(Xt)vt,

Vj,k := 1ΩT,k,s

(cid:32) T

(cid:88)

t=1

(f k

j (Xt) − f0(Xt))2η2(Xt)(v2

t + 1)

(cid:33)1/2

,

ηj,k := 1ΩT,k,s

2

(cid:113)

Uj,k
j,k + E[V 2
V 2

Jk,k]

,

ηk := ηJk,k,

where ηj,k := 0 if the denominator equals 0. Observe that

2
T

≤

≤

(cid:34)

E

1ΩT,k,s

T
(cid:88)

(f k
Jk

(Xt) − f0(Xt))η(Xt)vt

(cid:35)

t=1
(cid:16)

(cid:104)
1ΩT,k,s

2

2
T

E

(cid:113)

Jk,k + E[V 2
V 2

1
η T (log T )

6K2C2

E (cid:2)V 2

Jk,k

(cid:3) +

(cid:17)(cid:105)

Jk,k]ηk
96K2C2
η (log T )
T

E[1ΩT,k,sη2
k].

(A.30)

Combining (A.29) and (A.30), we have

2E

(cid:34)

1
T

T
(cid:88)

t=1

( (cid:98)fT (Xt) − f0(Xt))η(Xt)vt

−

(cid:35)

∞
(cid:88)

≤

k=0
∞
(cid:88)

k=0

=:

1
η T (log T )

E (cid:2)V 2

Jk,k

(cid:3) +

∞
(cid:88)

k=0

E

6K2C2

IV,k +

∞
(cid:88)

k=0

Iη,k + 2CηδT .

E[JT ( (cid:98)fT )]

5
6
(cid:34)
1ΩT,k,s

(cid:32)

96K2C2
η (log T )
T

η2
k −

5
6

(cid:33)(cid:35)

JT ( (cid:98)fT )

+ 2CηδT

(A.31)

Now we evaluate IV,k. Observe that
E (cid:2)V 2

(cid:3)

Jk,k

≤ C2

η E

= C2

η E

(cid:34)
1ΩT,k,s

(cid:34)
1ΩT,k,s

T
(cid:88)

t=1

T
(cid:88)

t=1

(f k
Jk

(Xt) − f0(Xt))2(v2

t + 1)

(cid:35)

( (cid:98)fT (Xt) − f0(Xt))2(v2

t + 1)

+ C2

η E

1ΩT,k,s

(cid:35)

(cid:34)

(cid:34)

+ 2C2

η E

1ΩT,k,s

T
(cid:88)

( (cid:98)fT (Xt) − f0(Xt))(f k
Jk

t=1

(Xt) − (cid:98)fT (Xt))(v2

t + 1)

(cid:34)
1ΩT,k,s

T
(cid:88)

≤ C2

η E

( (cid:98)fT (Xt) − f0(Xt))2(v2

t + 1)

(cid:35)

t=1
(cid:34)

+ 2F C2

η δT E

1ΩT,k,s

(cid:35)

(v2

t + 1)

+ 4F C2

η δT E

T
(cid:88)

t=1

(cid:34)
1ΩT,k,s

T
(cid:88)

t=1

(cid:35)

(v2

t + 1)

31

(Xt) − (cid:98)fT (Xt))2(v2

t + 1)

(cid:35)

T
(cid:88)

(f k
Jk

t=1

(cid:35)

(cid:34)
1ΩT,k,s

= C2

η E

T
(cid:88)

t=1

( (cid:98)fT (Xt) − f0(Xt))2(v2

t + 1)

+ 6F C2

η δT E

(cid:35)

(cid:34)
1ΩT,k,s

T
(cid:88)

t=1

(cid:35)

(v2

t + 1)

.

From the same arguments to obtain (A.16) in the proof of Lemma A.2, we have

C2

η E

(cid:34) T

(cid:88)

t=1

( (cid:98)fT (Xt) − f0(Xt))2v2
t

(cid:35)

≤ 2C2

η K2T (log T ) (cid:98)R( (cid:98)fT , f0) + 16C2

η F 2K2.

Hence we have
∞
(cid:88)

IV,k ≤

k=0

1
η T (log T )

6K2C2

(cid:16)

C2

η (2K2T (log T ) + 1) (cid:98)R( (cid:98)fT , f0) + 16C2

η F 2K2 + 6F C2

η T δT

≤

1
2

(cid:98)R( (cid:98)fT , f0) +

11F 2
3(log T )

δT .

For the second inequality, we used the inequalities T δT ≥ 1, F ≥ 1 and
E[v2

K2

t (E[exp(v2

t /K2

t )] − 1) ≤ K.

1 = max
1≤t≤T

t ] ≤ max
1≤t≤T

(cid:17)

(A.32)

(A.33)

Now we evaluate Iη,k. If E[V 2

Jk,k] > 0, applying Lemmas C.3 and C.4 with y =

(cid:113)

E[V 2

Jk,k], we

obtain

Hence



E





E



(cid:113)

(cid:113)

E[V 2

Jk,k]
j,k + E[V 2
V 2

Jk,k]



exp (cid:0)2η2
j,k

(cid:1)

 ≤ 1.

(cid:113)

E[V 2

Jk,k]
Jk,k + E[V 2
V 2

Jk,k]

(cid:113)





exp (cid:0)2η2

Jk,k

(cid:1)

 ≤ E

 max
1≤j≤Nk

(cid:113)

E[V 2

Jk,k]
j,k + E[V 2
V 2

Jk,k]

(cid:113)



exp (cid:0)2η2
j,k

(cid:1)

 ≤ Nk.

Therefore, by the Cauchy-Schwarz inequality, we obtain

E[exp(η2

k)] ≤

(cid:118)
(cid:117)
(cid:117)
(cid:117)
(cid:116)NkE





(cid:113)

Jk,k]

Jk,k + E[V 2
V 2
(cid:113)
E[V 2

Jk,k]



.

(cid:113)

Jk,k]

V 2
Jk,k + E[V 2
(cid:113)
E[V 2

Jk,k]



 ≤

(cid:118)
(cid:117)
(cid:117)
(cid:116)E

(cid:34) V 2

Jk,k]

Jk,k + E[V 2
E[V 2
Jk,k]

(cid:35)

√

2,

≤

Since

we conclude



E



This inequality also holds when E[V 2
for k ≥ 1, we have

k)] ≤ 21/4(cid:112)

E[exp(η2

(A.34)
Jk,k] = 0 because we always have ηk = 0 in such a case. Thus,

Nk.

(cid:34)

E

1ΩT,k,s

(cid:32)

(cid:32)

96K2C2
η (log T )
T
(cid:32)

≤

(cid:90) ∞

0

P

1ΩT,k,s

(cid:33)(cid:35)

η2
k −

5
6

JT ( (cid:98)fT )

96K2C2
η (log T )
T

(cid:33)

(cid:33)

JT ( (cid:98)fT )

> x

dx

η2
k −

5
6

32

≤ 21/4(cid:112)

Nk

(cid:90) ∞

0



(cid:16)

T

x + 5·2k−2s

(cid:17)



96K2C2

3
η (log T )

 dx

exp

−

=

21/4 · 96K2C2

η (log T )

T

(cid:112)

Nk exp

(cid:18)

−

5 · 2ksT

1152K2C2

η (log T )

(cid:19)

,

where the second inequality follows from Markov’s inequality and (A.34). Since ιλ(x) → ∞
as x → ∞ and F ≥ 1, there is a constant T1 depending only on C1, K, Cη and ιλ such that
(C1 log1+ν0 T )/λT ≤ T /(128F 2) whenever T ≥ T1. For such T , we have by (A.24)
21/4 · 96K2C2

21/4 · 96K2C2

(cid:19)

(cid:18)

(cid:18)

5 · 2ksT

5 · 2ksT

η (log T )

η (log T )

(cid:112)

Nk exp

−

1152K2C2

η (log T )

≤

T

exp

−

2304K2C2

η (log T )

T

For k = 0,

(cid:19)

.

(cid:34)

(cid:32)

E

1ΩT,0,s

96K2C2
η (log T )
T

η2
k −

5
6

JT ( (cid:98)fT )

(cid:33)(cid:35)

≤

≤

≤

96K2C2
η (log T )
T

E[η2
0]

48K2C2
η (log T )
T
48K2C2
η (log T )
T

(1 + log N0)

+

5s
24

.

Then we have
∞
(cid:88)

Iη,k ≤

k=0

48K2C2
η (log T )
T

+

5s
24

≤

5s
24

+

48K2C2
η (log T )
T

+



21/4 · 96K2C2

η (log T )

T

(cid:18)

exp

−

∞
(cid:88)

k=1

5 · 2ksT

(cid:19)

2304K2C2

η (log T )

1 +

25/4 exp

1 − exp

(cid:16)

−

(cid:16)

−

5sT
1152K2C2

η (log T )

5sT
2304K2C2

η (log T )

(cid:17)



 .

(cid:17)

(A.35)

Combining (A.28), (A.31), (A.32), and (A.35), we obtain (A.27).
Step 4 (Conclusion)
From (A.18) and (A.27) with aT = C−1

2,β + 1)(F 2 ∨ K2C2

η )(log T )/T , we have

R( (cid:98)fT , f0) ≤ 6

(cid:16) ¯ΨT ( (cid:98)fT , ¯f ) + R( ¯f , f0) + JT ( ¯f )

(cid:17)

2,β log T and s = (C−1
22F 2
log T
(cid:1)
(cid:1)

δT

(cid:33)

+

288K2C2
η (log T )
T

32F 2C−1
2,β(log T )
T

(cid:32)

1 +

(cid:32)

1 +

25/4 exp (cid:0)− 5
1152
1 − exp (cid:0)− 5
2304
(cid:1)
25/4 exp (cid:0)− 1
128
(cid:1)
1 − exp (cid:0)− 1
256

64F 2C−1
2,β(log T )
T

+

96F 2C1,β
T

+

+

+

+ 12CηδT +

5(C−1

2,β + 1)(F 2 ∨ K2C2

η )(log T )

4T

(cid:33)

+ 24F δT +

(C−1

2,β + 1)(F 2 ∨ K2C2

η )(log T )

4T

whenever T ≥ T0 ∨T1. Here, we used the fact that e−2x/(1−e−x) is decreasing for x > 0. Therefore,
(cid:3)
we obtain the desired result.

Appendix B. Proofs for Section 4

Throughout this section, we write (cid:107) · (cid:107)2 = (cid:107) · (cid:107)L2([0,1]d) for short.

33

B.1. Proof of Lemma 4.1. The proof is based on Theorem 1.3 in Hairer and Mattingly (2011).
We begin by introducing some general notation. The total variation measure of a signed measure
µ is denoted by |µ|. For x ∈ Rd, δx denotes the Dirac measure at x. Given a Markov kernel
P on Rd and a probability measure µ on Rd, we deﬁne the probability measure µP on Rd by
(µP)(·) = (cid:82)
Rd P(x, ·)µ(dx). Moreover, we deﬁne Markov kernels P n, n = 1, 2, . . . , inductively as
follows. For n = 1, we set P 1 := P. For n ≥ 2, we deﬁne P n(x, A) := (P n−1(x, ·)P)(A) for x ∈ Rd
and a Borel set A in Rd.

Next, we rewrite model (4.1) to a Markov chain. Let Xt = (Yt−1, . . . , Yt−d)(cid:48) and ¯vt = (vt, 0, . . . , 0)(cid:48)

for t = 1, . . . , T . Deﬁne the function ¯m : Rd → Rd as

¯m(x) = (m(x), x1, . . . , xd−1)(cid:48),

x ∈ Rd.

Then the process X = {Xt}T

t=1 satisﬁes
(cid:26) Xt+1 = ¯m(Xt) + ¯vt,

X1 ∼ ν.

t = 1, . . . , T,

(B.1)

Hence X is a Markov chain. Let Pm be the transition kernel associated with X. We are going to
apply Theorem 1.3 in Hairer and Mattingly (2011) to P d
m.

First we check Assumption 1 in Hairer and Mattingly (2011) (geometric drift condition). Let

b1 = 1. Take positive numbers b2, . . . , bd satisfying the following conditions:

d
(cid:88)

j=i

cj < bi < bi−1 − ci−1,

i = 2, . . . , d.

(B.2)

Thanks to the condition (cid:80)d
deﬁne the function V : Rd → [0, ∞) as

i=1 ci < 1, we can indeed take such numbers by induction. Then, we

V (x) =

d
(cid:88)

i=1

bi|xi|,

x ∈ Rd.

Denote by g the standard normal density. We have for any x ∈ Rd

(cid:90)

Rd

V (y)Pm(x, dy) =

(cid:90) ∞

−∞

|y|g(y − m(x))dy +

d
(cid:88)

i=2

bi|xi−1|

≤ m(x) + 1 +

d−1
(cid:88)

i=1

bi+1|xi|

≤ c0 + 1 +

d
(cid:88)

i=1

(ci + bi+1)|xi| ≤ γV (x) + c0 + 1,

where bd+1 := 0 and

Since γ < 1 by (B.2), we obtain

γ := max
i=1,...,d

ci + bi+1
bi

.

(cid:90)

Rd

V (y)P d

m(x, dy) ≤ γdV (x) + (c0 + 1)

1 − γd
1 − γ

.

(B.3)

Hence P d

m satisﬁes Assumption 1 in Hairer and Mattingly (2011).

34

Next we check Assumption 2 in Hairer and Mattingly (2011) (minorization condition). Set

R :=

3(c0 + 1)
1 − γ

and

C := {x ∈ Rd : V (x) ≤ R}.

Note that C is compact. A straightforward computation shows that P d
given by

m has the transition density

pm(x, y) =

d
(cid:89)

i=1

Then, for any x, y ∈ Rd,

g(yi − m(yi+1, . . . , yd, x1, . . . , xi)),

x, y ∈ Rd.

d
(cid:88)

pm(x, y) ≥

1
(2π)d/2

≥

1
(2π)d/2

(cid:32)

exp

−



exp

−

(y2

i + m(yi+1, . . . , yd, x1, . . . , xi)2)

(cid:33)

i=1

d
(cid:88)

i=1

y2
i −



c0 +

d
(cid:88)

i=1

d
(cid:88)

j=i+1

cj−i|yj| +



2

cd−i+j|xj|



 .

i
(cid:88)

j=1

Using the Cauchy-Schwarz inequality and (cid:80)d

i=1 ci < 1, we obtain



c0 +

d
(cid:88)

j=i+1

cj−i|yj| +



2

cd−i+j|xj|



i
(cid:88)

j=1



√

√

c0

c0 +

d
(cid:88)

√

√

cj−i

cj−i|yj| +

i
(cid:88)

√

√

cd−i+j


2

cd−i+j|xj|



=





≤

c0 +

d
(cid:88)

j=i+1


≤ (c0 + 1)

c0 +

j=i+1

cj−i +

i
(cid:88)

j=1

j=1





cd−i+j



c0 +

cj−iy2

j +



cd−i+jx2
j



i
(cid:88)

j=1

d
(cid:88)

j=i+1


d
(cid:88)

j=i+1

cj−iy2

j +

i
(cid:88)

j=1

cd−i+jx2
j

 .

Hence

pm(x, y) ≥

1
(2π)d/2

=

≥

1
(2π)d/2

1
(2π)d/2

Therefore, setting

we obtain



exp

−



exp

−

d
(cid:88)

i=1

d
(cid:88)

i=1

y2
i − (c0 + 1)



c0 +

d
(cid:88)

i=1

d
(cid:88)

j=i+1

cj−iy2

j +





cd−i+jx2
j





i
(cid:88)

j=1

y2
i − dc0(c0 + 1) −

d
(cid:88)

j−1
(cid:88)

j=2

i=1

cj−iy2

j −



cd−i+jx2
j



d
(cid:88)

d
(cid:88)

j=1

i=j

exp (cid:0)−2|y|2 − dc0(c0 + 1) − |x|2(cid:1) .

α :=

1
4d inf

x∈C

exp(−dc0(c0 + 1) − |x|2),

inf
x∈C

pm(x, y) ≥ αϕ(y)

35

for any y ∈ Rd,

(B.4)

where ϕ is the density of the d-dimensional normal distribution with mean 0 and covariance matrix
4−1Id. This implies that P d

m satisﬁes Assumption 2 in Hairer and Mattingly (2011).

Consequently, we have by Theorem 1.3 in Hairer and Mattingly (2011)

ρβ(µ1P d

m, µ2P d

m) ≤ ¯αρβ(µ1, µ2)

(B.5)

for any probability measures µ1 and µ2 on Rd, where β > 0 and ¯α ∈ (0, 1) depend only on c and
d, and

ρβ(µ1, µ2) :=

(cid:90)

Rd

(1 + βV (x))|µ1 − µ2|(dx).

Now, applying (B.5) repeatedly, we obtain

ρβ(µ1P dn

m , µ2P dn

m ) ≤ ¯αnρβ(µ1, µ2)

for n = 1, 2, . . . .

(B.6)

Therefore, for any integer n ≥ 1,

βX (dn) = sup
t≥1

≤ sup
t≥1

(cid:90)

Rd

(cid:90)

Rd
(cid:90)

(cid:107)δxP dn

m − ηP t+dn

m (cid:107)νP t

m(dx)

ρβ(δxP dn

m , (νP t

m)P dn

m )νP t

m(dx)

≤ ¯αn sup
t≥1

Rd
(cid:18)

≤ 2¯αn sup
t≥0
(cid:18)

≤ 2¯αn

1 + β

ρβ(δx, νP t

m)νP t

m(dx)

1 + β

(cid:19)

V (x)νP t

m(dx)

(cid:90)

Rd

(cid:90)

Rd

V (x)ν(dx) +

(c0 + 1)
1 − γ

(cid:19)

,

where the ﬁrst equality follows from (Davydov, 1973, Proposition 1) and the last inequality follows
from (B.3), respectively. Finally, note that βY (t) ≤ βX (t) ≤ βX (d(cid:98)t/d(cid:99)) for any t ≥ 1. Thus we
(cid:3)
complete the proof of (4.2).

B.2. Proof of Proposition 4.1. First, one can easily check that, whenever t > d, Yt has density
bounded by 1. Hence

R(f, f0) ≤

4F 2d
T

+ (cid:107)f − f0(cid:107)2
2

(B.7)

for all measurable f : Rd → R with (cid:107)f (cid:107)∞ ≤ F . Next, since JT (f ) ≤ λT |θ(f )|0, we have

inf
f ∈Fσ(LT ,NT ,BT ,F )

(R(f, f0) + JT (f )) ≤

≤

≤

(R(f, f0) + λT |θ(f )|0)

inf
f ∈Fσ(LT ,NT ,BT ,F,ST,εT )
4F 2d
T

C0
T 1/(κ+1)

+

+ λT ST

4F 2d + C0 + CSιλ(T ) log2+ν0+r T
T 1/(κ+1)

,

where the second inequality follows from (B.7) and assumption. Combining this with Theorem 3.2
(cid:3)
and Lemma 4.1 gives the desired result.

36

B.3. Proof of Theorem 4.1. We begin by reducing the problem to establishing a lower bound
on the minimax L2-estimation error.

Lemma B.1. Let {aT }T ≥1 be a sequence of positive numbers such that aT = O(T ) as T → ∞.
Then, there is a constant ρ > 0 such that

lim inf
T →∞

aT inf
(cid:98)fT

sup
m∈M

R( (cid:98)fT , f0) ≥ ρ lim inf
T →∞

aT inf
(cid:98)fT

sup
m∈M

E[(cid:107) (cid:98)fT − f0(cid:107)2
2]

(B.8)

for any M ⊂ M0(c).

Proof. Throughout the proof, we will use the same notation as in Section B.1. First, by the proof of
Lemma 4.1 and (Hairer and Mattingly, 2011, Theorem 3.2), P d
m has the invariant distribution Πm
for all m ∈ M0(c). Next, ﬁx an estimator (cid:98)fT arbitrarily. Set ¯c0 := c0 + 1 and deﬁne (cid:101)fT := {(−¯c0) ∨
(cid:98)fT } ∧ ¯c01[0,1]d. Since (cid:107)f0(cid:107)∞ ≤ (cid:80)d
i=0 ci < ¯c0 and supp(f0) ⊂ [0, 1]d, we have | (cid:101)fT − f0| ≤ | (cid:98)fT − f0|.
Hence

R( (cid:98)fT , f0) ≥ E

(cid:34)

1
T

T
(cid:88)

t=1

( (cid:101)fT (X ∗

t ) − f0(X ∗

t ))2

(cid:35)

=

1
T

T
(cid:88)

t=1

E

(cid:20)(cid:90)

(cid:26)(cid:90)

Rd

Rd

( (cid:101)fT (y) − f0(y))2P t

m(x, dy)

(cid:27)

(cid:21)

ν(dx)

.

For any integer 1 ≤ t0 ≤ T , we have

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
T

T
(cid:88)

t=t0

E

(cid:20)(cid:90)

(cid:26)(cid:90)

Rd

Rd

( (cid:101)fT (y) − f0(y))2P t

m(x, dy)

(cid:27)

ν(dx) −

(cid:90)

Rd

| (cid:101)fT (y) − f0(y)|2Πm(dy)

(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ 4¯c2
0

1
T

T
(cid:88)

(cid:90)

Rd

t=t0

(cid:107)δxP t

m − Πm(cid:107)ν(dx) ≤ 4¯c2
0

1
T

T
(cid:88)

t=t0

¯α(cid:98)t/d(cid:99)

(cid:90)

Rd

ρβ(δx, Πm)ν(dx),

where the last inequality follows from (B.6). We have

1
T

T
(cid:88)

t=t0

¯α(cid:98)t/d(cid:99) ≤

1
T ¯α

T
(cid:88)

t=t0

¯αt/d ≤

¯αt0/d
T ¯α(1 − ¯α1/d)

and

(cid:90)

(cid:90)

(cid:90)

ρβ(δx, Πm)ν(dx) ≤ 2 + β

V (x)ν(dx) + β

V (x)Πm(dx).

Rd
One can easily derive the following estimate from (B.3) (cf. (Hairer, 2006, Proposition 4.24)):

Rd

Rd

(cid:90)

Rd

V (x)Πm(dx) ≤

c0 + 1
1 − γ

.

(B.9)

Combining these estimates, we obtain
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

( (cid:101)fT (y) − f0(y))2P t

T
(cid:88)

1
T

(cid:26)(cid:90)

(cid:20)(cid:90)

E

Rd

Rd

t=t0

m(x, dy)

(cid:27)

ν(dx) −

(cid:90)

Rd

| (cid:101)fT (y) − f0(y)|2Πm(dy)

(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ C1

¯αt0/d
T

,

where C1 > 0 depends only on c, d and ν. Consequently,

R( (cid:98)fT , f0) ≥

1
T

T
(cid:88)

t=t0

E

(cid:20)(cid:90)

(cid:26)(cid:90)

Rd

Rd

( (cid:101)fT (y) − f0(y))2P t

m(x, dy)

(cid:27)

(cid:21)

ν(dx)

37

≥

T − t0 + 1
T

E

(cid:20)(cid:90)

Rd

| (cid:101)fT (y) − f0(y)|2Πm(dy)

(cid:21)

− C1

¯αt0/d
T

.

Choosing t0 = (cid:98)

√

T (cid:99) and noting aT = O(T ), we obtain

lim inf
T →∞

aT inf
(cid:98)fT

sup
m∈M

R( (cid:98)fT , f0) ≥ lim inf
T →∞

aT inf
(cid:98)fT

sup
m∈M

E

(cid:20)(cid:90)

Rd

| (cid:98)fT (y) − f0(y)|2Πm(dy)

(cid:21)

.

(B.10)

Now, using the deﬁnition of Πm, we can easily check that Πm has the density given by

We have by (B.4)

πm(y) =

(cid:90)

Rd

pm(x, y)Πm(dx),

y ∈ Rd.

inf
y∈[0,1]d

πm(y) ≥ α inf

y∈[0,1]d

ϕ(y)Πm(C).

By Markov’s inequality and (B.9), we obtain

1 − Πm(C) = Πm(V > R) ≤

1
R

(cid:90)

Rd

V (x)Πm(dx) ≤

1
3

.

Hence we conclude

inf
y∈[0,1]d

πm(y) ≥

α
3

inf
y∈[0,1]d

ϕ(y).

Consequently, there is a constant ρ > 0 depending only on c, d and ν such that

(cid:34)(cid:90)

E

[0,1]d

(cid:35)
| (cid:98)fT (y) − f0(y)|2Πm(dy)

≥ ρE

(cid:34)(cid:90)

[0,1]d

| (cid:98)fT (y) − f0(y)|2dy

(B.11)

(cid:35)

for any estimator (cid:98)fT . Combining (B.10) with (B.11) gives the desired result.
Proof of Theorem 4.1. We write MA = M(cid:0)c, q, d, t, β, A(cid:1) for short. For each m ∈ MA and T ∈ N,
we denote by Pm,T the law of the random vector YT := (Y−d+1, . . . , YT )(cid:48) when Yt are deﬁned by
(4.1). Moreover, we denote by Em,T [·] the expectation under Pm,T .
Now, note that any estimator based on the observation {Yt}T

t=1 is also an estimator based on
YT . Therefore, according to Theorem 2.7 in Tsybakov (2009) and Lemma B.1, it suﬃces to show
that there is a constant A > 0 having the following property: For suﬃciently large T ∈ N, there
are an integer M ≥ 1 and functions m0, m1, . . . , mM ∈ MA such that

(cid:3)

and

and

(cid:107)mj − mk(cid:107)2

2 ≥ κφT

for all 0 ≤ j < k ≤ M

Pj (cid:28) P0

for all j = 1, . . . , M

1
M

M
(cid:88)

j=1

(cid:20)

Emj ,T

log

(cid:21)

≤

dPj
dP0

1
9

log M,

(B.12)

(B.13)

(B.14)

where κ > 0 is a constant independent of T and Pj := Pmj ,T for j = 0, 1, . . . , M .

By the proof of (Schmidt-Hieber, 2020, Theorem 3), there is a constant A > 0 having the following
property: For any T ∈ N, there are an integer M ≥ 1 and functions f(0), . . . , f(M ) ∈ G(cid:0)q, d, t, β, A(cid:1)
satisfying the following condition:

38

((cid:63)) For all 0 ≤ j < k ≤ M ,

and

T (cid:107)f(j) − f(k)(cid:107)2

2 ≤

log M
9

(cid:107)f(j) − f(k)(cid:107)2

2 ≥ κφT ,

where κ > 0 is a constant depending only on t and β.
For each j = 1, . . . , M , we deﬁne the function mj : Rd → R as

mj(x) =

(cid:40)

f(j)(x)
0

if x ∈ [0, 1]d,
otherwise.

(B.15)

(B.16)

It is evident that m0, m1, . . . , mM ∈ MA when c0 ≥ A. In the following we show that these mj
satisfy (B.12)–(B.14).

First, (B.12) immediately follows from (B.16). Next, it is straightforward to check (B.13) and

dPj
dP0

(YT ) =

T
(cid:89)

t=1

g(Yt − mj(Yt−1, . . . , Yt−d))
g(Yt − m0(Yt−1, . . . , Yt−d))

for every j = 1, . . . , d, where g is the standard normal density. Hence, with Xt = (Yt−1, . . . , Yt−d)(cid:48),

(cid:20)

Emj ,T

log

(cid:21)

(YT )

=

dPj
dP0

=

1
2

1
2

T
(cid:88)

t=1

T
(cid:88)

t=1

Emj ,T

(cid:2)m0(Xt)2 − mj(Xt)2 + 2Yt(mj(Xt) − m0(Xt))(cid:3)

Emj ,T

(cid:2)(mj(Xt) − m0(Xt))2(cid:3) .

When t > d, conditional on Xt−d, Xt has the density given by

d
(cid:89)

i=1

g(yi − m(yi+1, . . . , yd, Yt−d−1, . . . , Yt−d−i)),

y ∈ Rd,

which is bounded by 1. Thus

(cid:20)

Emj ,T

log

(cid:21)
(YT )

dPj
dP0

≤ 2A2d +

T
2

(cid:90)

Rd

(mj(x) − m0(x))2dx ≤ 2A2d +

log M
18

,

where the last inequality follows from (B.15). Also, by (B.15) and (B.16), κT φT ≤ (log M )/9.
Since T φT → ∞ as T → ∞, we have 2A2d ≤ (log M )/18 for suﬃciently large T . For such T , we
(cid:3)
have (B.14). This completes the proof.

B.4. Proof of Theorem 4.2. Let κ = maxi=0,...,q ti/(2β∗
Hieber (2020), there exist constants C0, CS > 0 such that

i ). By the proof of Theorem 1 in Schmidt-

sup
m∈M(c,q,d,t,β,A)

inf
f ∈Fσ(LT ,NT ,BT ,F,ST )

(cid:107)f − m(cid:107)2

∞ ≤ C0φT = C0T −1/(κ+1)

with ST := CST κ/(κ+1) log T . So the desired result follows by Proposition 4.1.

(cid:3)

39

B.5. Proof of Theorem 4.3. For each m ∈ M0(c) and T ∈ N, we denote by Pm,T the law of
the random vector YT := (Y−d+1, . . . , YT )(cid:48) when Yt are deﬁned by (4.1). Moreover, we denote by
Em,T [·] the expectation under Pm,T .

Now, note that any estimator based on the observation {Yt}T

t=1 is also an estimator based on

YT . Therefore, by Lemma B.1, it suﬃces to prove

lim inf
T →∞

T inf
(cid:98)fT

sup
Φ(c,ns,C)

m∈M0

E[(cid:107) (cid:98)fT − f0(cid:107)2

2] > 0.

M0

For each T = 1, 2, . . . , deﬁne m±
Φ(c, ns, C) by assumption. Also, by construction,
1
√
T

T := ± 1
√
2

T − m−

T (cid:107)2 =

(cid:107)m+

T

(cid:107)ϕ(cid:107)2 =

1
√
T

.

ϕ and write P± = Pm±

T ,T . Note that m±

T ∈

Moreover, as in the proof of Theorem 4.1, we can show that P+ (cid:28) P− and

Em+

T ,T

(cid:20)

log

dP+
dP−

(cid:21)
(YT )

≤ 2c2

0d +

T
2

(cid:90)

Rd

(m+

T (x) − m−

T (x))2dx = 2c2

0d +

1
2

,

where we used the assumptions supp(ϕ) ⊂ [0, 1]d and (cid:107)ϕ(cid:107)∞ ≤ c0. Consequently, by Eq.(2.9) and
Theorem 2.2 in Tsybakov (2009),

Since

lim inf
T →∞

inf
(cid:98)fT

sup
Φ(c,ns,C)

m∈M0

(cid:18)

Pm,T

(cid:107) (cid:98)fT − f0(cid:107)2 ≥

(cid:19)

1
√

2

T

> 0.

lim inf
T →∞

T inf
(cid:98)fT

sup
Φ(c,ns,C)

m∈M0

E[(cid:107) (cid:98)fT − f0(cid:107)2
2]

≥

1
4

lim inf
T →∞

inf
(cid:98)fT

sup
Φ(c,ns,C)

m∈M0

(cid:18)

Pm,T

(cid:107) (cid:98)fT − f0(cid:107)2 ≥

(cid:19)

,

1
√
2

T

we complete the proof.

(cid:3)

B.6. Proof of Theorem 4.4. We are going to apply Proposition 4.1. Fix m ∈ M0
arbitrarily. By deﬁnition, m is of the form

Φ(c, ns, C)

m(x) =

ns(cid:88)

i=1

θiϕi(Aix − bi),

where Ai ∈ Rd×d, bi ∈ Rd, θi ∈ R and ϕi ∈ Φ with | det Ai|−1 ∨ |Ai|∞ ∨ |bi|∞ ∨ |θi| ≤ C for
i = 1, . . . , ns. Since Φ ⊂ APReLU,d(C1, C2, D, r) by assumption, for every i, there exist parameters
Li, Ni, Bi, Si > 0 such that Li ∨ Ni ∨ Si ≤ C1(log2 T )r and Bi ≤ C2T hold and there exists an
fi ∈ FReLU(Li, Ni, Bi) such that |θ(fi)|0 ≤ Si and (cid:107)fi − ϕi(cid:107)2

L2([−D,D]d) ≤ 1/T. Deﬁne

f (x) =

ns(cid:88)

i=1

θifi(Aix − bi),

x ∈ Rd.

Then

(cid:107)f − m(cid:107)L2([0,1]d) ≤ C

(cid:115)(cid:90)

ns(cid:88)

i=1

[0,1]d

|fi(Aix − bi) − ϕi(Aix − bi)|2dx

40

(cid:115)(cid:90)

ns(cid:88)

≤ C

i=1

[−(d+1)C,(d+1)C]d

≤ C3/2nsT −1/2,

|fi(y) − ϕi(y)|2| det Ai|−1dx

i=0 ci ≤ F because m ∈ M0(c) and (cid:80)d

where we used the assumption D ≥ (d + 1)C for the last inequality. Also, note that (cid:107)m(cid:107)∞ ≤
(cid:80)d
i=1 ci < 1. Hence, with (cid:101)f = (−F ) ∨ (f ∧ F ), we have
| (cid:101)f − m| ≤ |f − m|. Thus (cid:107) (cid:101)f − m(cid:107)2
s/T . Therefore, the proof is completed once
we show that there exists a constant CS > 0 such that (cid:101)f ∈ FReLU(LT , NT , BT , F, CS logr T ) for
suﬃciently large T .

L2([0,1]d) ≤ C3n2

By Lemmas II.3–II.4 and A.8 in Elbr¨achter et al. (2021), there exists a constant C(cid:48)

S > 0 such
S logr T for suﬃciently large T . Also, note that
that f ∈ FReLU(LT , NT , BT ) and |θ(f )|0 ≤ C(cid:48)
x ∧ F = − ReLU(F − x) + F and x ∨ (−F ) = ReLU(x + F ) − F for all x ∈ R. Thus we have
S logr T + 20) for suﬃciently large T by Lemma II.3 in Elbr¨achter
(cid:101)f ∈ FReLU(LT + 4, NT , BT , F, 4C(cid:48)
(cid:3)
et al. (2021).

Appendix C. Technical tools

Here we collect technical tools we used in the proofs. Let A and B be two σ-ﬁelds of a probability

space (Ω, T , P). The β-mixing coeﬃcient between A and B is deﬁned by

β(A, B) =

1
2

sup




(cid:88)

(cid:88)



i∈I

j∈J

|P(Ai ∩ Bj) − P(Ai) P(Bj)|






,

where the maximum is taken over all ﬁnite partitions {Ai}i∈I ⊂ A and {Bj}j∈J ⊂ B of Ω.

Lemma C.1 (Lemma 5.1 in Rio (2013)). Let A be a σ-ﬁeld in a probability space (Ω, T , P) and X
be a random variable with values in some Poilsh space. Let U be a random variable with uniform
distribution over [0, 1], independent of the σ-ﬁeld generated by X and A. Then there exists a random
variable X ∗, with the same law as X, independent of X, such that P(X (cid:54)= X ∗) = β(A, σ(X)) where
σ(X) denote the σ-ﬁeld generated by X. Furthermore X ∗ is measurable with respect to the σ-ﬁeld
generated by A and (X, U ).

Lemma C.2 (Lemma 1.4 in de la Pe˜na et al. (2004)). Let {di} be a sequence of variables adapted
to an increasing sequence of σ-ﬁelds {Fi}. Assume that the di’s are conditionally symmetric
(i.e. L(di|Fi−1) = L(−di|Fi−1), where L(di|Fi−1) is the conditional law of di given Fi−1). Then
exp (cid:0)λ (cid:80)n

i /2(cid:1), n ≥ 1, is a supermartingale with mean ≤ 1, for all λ ∈ R.

i=1 di − λ2 (cid:80)n

i=1 d2

Lemma C.3. Let {di} be a martingale diﬀerence sequence with respect to a ﬁltration {Fi}. Assume
E[d2

i ] < ∞ for all i. Then

(cid:34)

(cid:32)

E

exp

λ

n
(cid:88)

i=1

di −

λ2
2

(cid:32) n
(cid:88)

i=1

d2
i +

n
(cid:88)

i=1

(cid:33)(cid:33)(cid:35)

E[d2

i | Fi−1]

≤ 1

for all n ≥ 1 and λ ∈ R.
Proof. Deﬁne a process M = {Mt}t∈[0,∞) as Mt = (cid:80)(cid:98)t(cid:99)
i=1 λdi for t ≥ 0. It is straightforward to
check that M is an {F(cid:98)t(cid:99)}-martingale and its continuous martingale part is identically equal to
0. Moreover, the compensator of the process {(cid:80)(cid:98)t(cid:99)
i=1 E[{(−λdi) ∨ 0}2 |

i=1{(−λdi) ∨ 0}2}t≥0 is {(cid:80)(cid:98)t(cid:99)

41

Fi−1]}t≥0 by Eq.(3.40) of (Jacod and Shiryaev, 2003, Ch. I). Therefore, by Proposition 4.2.1 in
Barlow et al. (1986), the process




exp





Mt −





1
2

(cid:98)t(cid:99)
(cid:88)

{(λdi) ∨ 0}2 +

i=1

(cid:98)t(cid:99)
(cid:88)

i=1

E[{(−λdi) ∨ 0}2 | Fi−1]














t∈[0,∞)

is an {F(cid:98)t(cid:99)}-supermartingale. Hence
(cid:34)

(cid:32)

1 ≥ E

exp

λ

di −

n
(cid:88)

i=1

1
2

(cid:32) n
(cid:88)

i=1

{(λdi) ∨ 0}2 +

n
(cid:88)

i=1

E[{(−λdi) ∨ 0}2 | Fi−1]

.

(cid:33)(cid:33)(cid:35)

Since {(λdi)∨0}2 ≤ λ2d2
of the exponential function.

i and {(−λdi)∨0}2 ≤ λ2d2

i , the desired result follows from the monotonicity
(cid:3)

Lemma C.4 (Theorem 1.2 in de la Pe˜na et al. (2004)). Let B ≥ 0 and A be two random variables
satisfying E

≤ 1 for all λ ∈ R. Then for all y > 0,

λA − λ2

exp

(cid:16)

(cid:104)

2 B2(cid:17)(cid:105)
(cid:34)

E

y
(cid:112)B2 + y2

exp

(cid:18)

A2
2(B2 + y2)

(cid:19)(cid:35)

≤ 1.

Lemma C.5 (Proposition 8 in Ohn and Kim (2022)). Let L ∈ N, N ∈ N, B ≥ 1, F > 0, and
S > 0. Then for any δ ∈ (0, 1),

log N (δ, Fσ(L, N, B, F, S), (cid:107) · (cid:107)∞) ≤ 2S(L + 1) log (cid:0)(L + 1)(N + 1)Bδ−1(cid:1) .
Lemma C.6 (Proposition 10 in Ohn and Kim (2022)). Let L ∈ N, N ∈ N, B ≥ 1, F > 0, and
τ > 0. Let

ˇFσ,τ (L, N, B, F, S) := {f ∈ Fσ(L, N, B, F ) : (cid:107)θ(f )(cid:107)clip,τ ≤ S} .

Then for any δ ∈ (τ (L + 1)((N + 1)B)L+1, 1),

log N (cid:0)δ, ˇFσ(L, N, B, F, S), (cid:107) · (cid:107)∞

(cid:1) ≤ 2S(L + 1) log

(cid:18)

(L + 1)(N + 1)B
δ − τ (L + 1)((N + 1)B)L+1

(cid:19)

.

References

Barlow, M. T., S. D. Jacka, and M. Yor (1986). Inequalities for a pair of processes stopped at a

random time. Proceedings of the London Mathematical Society 52 (3), 142–172.

Bauer, B. and M. Kohler (2019). On deep learning as a remedy for the curse of dimensionality in

nonparametric regression. Annals of Statistics 47 (4), 2261–2285.

Bhattacharya, R. and C. Lee (1995). On geometric ergodicity of nonlinear autoregressive models.

Statistics & Probability Letters 22 (4), 311–315.

Chen, M. and G. Chen (2000). Geometric ergodicity of nonlinear autoregressive models with

changing conditional variances. Canadian Journal of Statistics 28 (3), 605–614.

Chen, R. and R. S. Tsay (1993). Functional-coeﬃcient autoregressive models. Journal of the

American Statistical Association 88 (421), 298–308.

Cline, D. B. and H.-M. H. Pu (2004). Stability and the Lyapounov exponent of threshold AR-ARCH

models. Annals of Applied Probability 14 (4), 1920–1949.

Davydov, Y. A. (1973). Mixing conditions for Markov chains. Theory of Probability and Its

Applications 18 (2), 321–338.

42

de la Pe˜na, V. H., M. J. Klass, and T. L. Lai (2004). Self-normalized processes: exponential
inequalities, moment bounds and iterated logarithm laws. Annals of Probability, 1902–1933.
Elbr¨achter, D., D. Perekrestenko, P. Grohs, and H. B¨olcskei (2021). Deep neural network approxi-

mation theory. IEEE Transactions on Information Theory 67 (5), 2581–2623.

Exterkate, P. (2013). Model selection in kernel ridge regression. Computational Statistics and Data

Analysis 68, 1–16.

Fan, J. and Q. Yao (2008). Nonlinear Time Series: Nonparametric and Parametric Methods.

Springer.

Granger, C. W. and T. Ter¨asvirta (1993). Modelling Nonlinear Economic Relationships. Oxford

University Press.

Gy¨orﬁ, L., M. Kohler, A. Krzyzak, and H. Walk (2002). A Distribution-Free Theory of Nonpara-

metric Regression. Springer.

Haggan, V. and T. Ozaki (1981). Modelling nonlinear random vibrations using an amplitude-

dependent autoregressive time series model. Biometrika 68 (1), 189–196.

Hairer, M. (2006). Ergodic properties of Markov processes. Lecture notes. http://www.hairer.

org/notes/Markov.pdf.

Hairer, M. and J. C. Mattingly (2011). Yet another look at Harris’ ergodic theorem for Markov
chains. In Seminar on Stochastic Analysis, Random Fields and Applications VI, pp. 109–117.
Springer.

Hansen, B. E. (2008). Uniform convergence rates for kernel estimation with dependent data.

Econometric Theory 24 (3), 726–748.

Hastie, T., R. Tibshirani, and J. Friedman (2009). The Elements of Statistical Learning (second

ed.). Springer.

Hayakawa, S. and T. Suzuki (2020). On the minimax optimality and superiority of deep neural

network learning over sparse parameter spaces. Neural Networks 123, 343–361.

Hoﬀmann, M. (1999). On nonparametric estimation in nonlinear AR(1)-models. Statistics &

Probability Letters 44 (1), 29–45.

Imaizumi, M. and K. Fukumizu (2019). Deep neural networks learn non-smooth functions eﬀec-

tively. In Proceedings of Machine Learning Research, Volume 89, pp. 869–878. PMLR.

Jacod, J. and A. Shiryaev (2003). Limit Theorems for Stochastic Processes (second ed.). Springer.
Kingma, D. P. and J. L. Ba (2015). Adam: A method for stochastic optimization. In Y. Bengio
and Y. LeCun (Eds.), 3rd International Conference on Learning Representations, ICLR 2015,
San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.

Kohler, M. and A. Krzyzak (2020). On the rate of convergence of a deep recurrent neural network

estimate in a regression problem with dependent data. arXiv:2011.00328 .

Kohler, M. and S. Langer (2021). On the rate of convergence of fully connected deep neural network

regression estimates. Annals of Statistics 49 (4), 2231–2249.

Liu, W. and W. B. Wu (2010). Simultaneous nonparametric inference of time series. Annals of

Statistics 38 (4), 2388–2421.

Lu, Z. and Z. Jiang (2001). L1 geometric ergodicity of a multivariate nonlinear AR model with an

ARCH term. Statistics & Probability Letters 51 (2), 121–130.

Masry, E. (1996a). Multivariate local polynomial regression for time series: uniform strong consis-

tency and rates. Journal of Time Series Analysis 17 (6), 571–599.

43

Masry, E. (1996b). Multivariate regression estimation local polynomial ﬁtting for time series.

Stochastic Processes and their Applications 65 (1), 81–101.

Nakada, R. and M. Imaizumi (2020). Adaptive approximation and generalization of deep neural

network with intrinsic dimensionality. Journal of Machine Learning Research 21, 174–1.

Oga, A. and Y. Koike (2021). Drift estimation for a multi-dimensional diﬀusion process using deep

neural networks. arXiv:2112.13332 .

Ogihara, T. (2021). Misspeciﬁed diﬀusion models with high-frequency observations and an appli-

cation to neural networks. Stochastic Processes and their Applications 142, 245–292.

Ohn, I. and Y. Kim (2022). Nonconvex sparse regularization for deep neural networks and its

optimality. Neural Computation 34 (2), 476–517.

Probst, P. and A.-L. Boulesteix (2018). To tune or not to tune the number of trees in random

forest. Journal of Machine Learning Research 18, 1–18.

Rio, E. (2013). Inequalities and limit theorems for weakly dependent sequences. https://cel.

archives-ouvertes.fr/cel-00867106v2.

Robinson, P. M. (1983). Nonparametric estimators for time series. Journal of Time Series Analy-

sis 4 (3), 185–207.

Rudin, W. (1987). Real and Complex Analysis (third ed.). McGraw-Hill.
Schmidt-Hieber, J. (2019). Deep ReLU network approximation of functions on a manifold.

arXiv:1908.00695 .

Schmidt-Hieber, J. (2020). Nonparametric regression using deep neural networks with ReLU acti-

vation function. Annals of Statistics 48 (4), 1875–1897.

Suzuki, T. (2019). Adaptivity of deep ReLU network for learning in Besov and mixed smooth
Besov spaces: optimal rate and curse of dimensionality. International Conference on Learning
Representations. https://openreview.net/forum?id=H1ebTsActm.

Suzuki, T. and A. Nitanda (2021). Deep learning is adaptive to intrinsic dimensionality of model
smoothness in anisotropic Besov space. Advances in Neural Information Processing Systems 34.
Ter¨asvirta, T. (1994). Speciﬁcation, estimation, and evaluation of smooth transition autoregressive

models. Journal of the American Statistical Association 89 (425), 208–218.

Tjøstheim, D. (1990). Non-linear time series and Markov chains. Advances in Applied Probabil-

ity 22 (3), 587–611.

Tran, L. T. (1993). Nonparametric function estimation for time series by local average estimators.

Annals of Statistics 21 (2), 1040–1057.

Truong, Y. K. (1994). Nonparametric time series regression. Annals of the Institute of Statistical

Mathematics 46 (2), 279–293.

Tsuji, K. and T. Suzuki (2021). Estimation error analysis of deep learning on the regression problem

on the variable exponent Besov space. Electronic Journal of Statistics 15 (1), 1869–1908.

Tsybakov, A. B. (2009). Introduction to Nonparametric Estimation. Springer.
van der Vaart, A. W. and J. A. Wellner (1996). Weak Convergence and Empirical Processes with

Applications to Statistics. Springer.

Vogt, M. (2012). Nonparametric regression for locally stationary time series. The Annals of

Statistics 40 (5), 2601–2633.

Xia, Y. and W. K. Li (1999). On single-index coeﬃcient regression models. Journal of the American

Statistical Association 94 (448), 1275–1285.

44

Xia, Y., W.-K. Li, and H. Tong (2007). Threshold variable selection using nonparametric methods.

Statistica Sinica 17, 265–287.

Zhang, T. (2010). Analysis of multi-stage convex relaxation for sparse regularization. Journal of

Machine Learning Research 11 (3), 1081–1107.

Zhao, Z. and W. B. Wu (2008). Conﬁdence bands in nonparametric time series regression. Annals

of Statistics 36 (4), 1854–1878.

(D. Kurisu) Graduate School of International Social Sciences, Yokohama National University, 79-4

Tokiwadai, Hodogaya-ku, Yokohama 240-8501, Japan.

Email address: kurisu-daisuke-jr@ynu.ac.jp

(R. Fukami) Graduate School of Mathematical Science, The University of Tokyo, 3-8-1 Komaba,

Meguro-ku, Tokyo 153-8914, Japan.

Email address: rick.h.azuma@gmail.com

(Y. Koike) Graduate School of Mathematical Science, The University of Tokyo, 3-8-1 Komaba,

Meguro-ku, Tokyo 153-8914, Japan.

Email address: kyuta@ms.u-tokyo.ac.jp

45

