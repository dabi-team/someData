7
1
0
2

r
a

M
8

]

R
G
.
s
c
[

2
v
2
0
4
0
0
.
0
1
6
1
:
v
i
X
r
a

DYNAMIC POLYGON CLOUDS: REPRESENTATION AND COMPRESSION FOR VR/AR
MICROSOFT RESEARCH TECHNICAL REPORT MSR-TR-2016-59

(DRAFT AS OF JANUARY 4, 2017 — SEE ARXIV.ORG FOR UPDATES)

Philip A. Chou1, Eduardo Pavez 2, Ricardo L. de Queiroz3, and Antonio Ortega2

1Microsoft Research, Redmond, WA, USA
2University of Southern California, Los Angeles, CA, USA
3Universidade de Brasilia, Brasilia, Brazil

ABSTRACT

1. INTRODUCTION

We introduce the polygon cloud, also known as a polygon set
or soup, as a compressible representation of 3D geometry (in-
cluding its attributes, such as color texture) intermediate be-
tween polygonal meshes and point clouds. Dynamic or time-
varying polygon clouds, like dynamic polygonal meshes and
dynamic point clouds, can take advantage of temporal redun-
dancy for compression, if certain challenges are addressed. In
this paper, we propose methods for compressing both static
and dynamic polygon clouds, speciﬁcally triangle clouds. We
compare triangle clouds to both triangle meshes and point
clouds in terms of compression, for live captured dynamic
colored geometry. We ﬁnd that triangle clouds can be com-
pressed nearly as well as triangle meshes, while being far
more robust to noise and other structures typically found in
live captures, which violate the assumption of a smooth sur-
face manifold, such as lines, points, and ragged boundaries.
We also ﬁnd that triangle clouds can be used to compress
point clouds with signiﬁcantly better performance than pre-
viously demonstrated point cloud compression methods. In
particular, for intra-frame coding of geometry, our method
improves upon octree-based intra-frame coding by a factor of
5-10 in bit rate. Inter-frame coding improves this by another
factor of 2-5. Overall, our dynamic triangle cloud compres-
sion improves over the previous state-of-the-art in dynamic
point cloud compression by 33% or more.

Index Terms— Polygon soup, dynamic mesh, point
cloud, augmented reality, motion compensation, compres-
sion, graph transform, octree

P. A. Chou is with Microsoft Research, Redmond, WA, USA, e-mail:

pachou@ieee.org.

E. Pavez is with the Department of Electrical Engineering, University of

Southern California, Los Angeles, CA, USA, e-mail: pavezcar@usc.edu

R. L. de Queiroz is with the Computer Science Department at Universi-

dade de Brasilia, Brasilia, Brazil, e-mail: queiroz@ieee.org.

Antonio Ortega is with the Department of Electrical Engineering at the
University of Southern California, Los Angeles, CA, USA, e-mail: anto-
nio.ortega@sipi.usc.edu

With the advent of virtual and augmented reality comes the
birth of a new medium:
live captured 3D content that can
be experienced from any point of view. Such content ranges
from static scans of compact 3D objects, to dynamic captures
of non-rigid objects such as people, to captures of rooms in-
cluding furniture, public spaces swarming with people, and
whole cities in motion. For such content to be captured at
one place and delivered to another for consumption by a vir-
tual or augmented reality device (or by more conventional
means), the content needs to be represented and compressed
for transmission or storage. Applications include gaming,
tele-immersive communication, free navigation of highly pro-
duced entertainment as well as live events, historical artifact
and site preservation, acquisition for special effects, and so
forth. This paper presents a novel means of representing and
compressing the visual part of such content.

Until this point, two of the more promising approaches
to representing both static and time-varying 3D scenes have
been polygonal meshes and point clouds, along with their as-
sociated color information. However, both approaches have
drawbacks. Polygonal meshes represent surfaces very well,
but they are not robust to noise and other structures typi-
cally found in live captures, such as lines, points, and ragged
boundaries that violate the assumptions of a smooth surface
manifold. Point clouds, on the other hand, have a hard time
modeling surfaces as compactly as meshes.

We propose a hybrid between polygonal meshes and point
clouds: polygon clouds. Polygon clouds are sets of polygons,
often called a polygon soup. The polygons in a polygon cloud
are not required to represent a coherent surface. Like the
points in a point cloud, the polygons in a polygon cloud can
represent noisy, real-world geometry captures without any as-
sumption of a smooth 2D manifold. In fact, any polygon in a
polygon cloud can be collapsed into a point or line as a special
case. The polygons may also overlap. On the other hand, the
polygons in the cloud can also be stitched together into a wa-
tertight mesh if desired to represent a smooth surface. Thus

 
 
 
 
 
 
polygon clouds generalize both point clouds and polygonal
meshes.

For concreteness we focus on triangles instead of arbi-
trary polygons, and we develop an encoder and decoder for
sequences of triangle clouds. We assume a simple group of
frames (GOF) model, where each group of frames begins
with an Intra (I) frame, also called a reference frame or a
key frame, which is followed by a sequence of Predicted (P)
frames, also called inter frames. The triangles are assumed to
be consistent across frames. That is, the triangles’ vertices are
assumed to be tracked from one frame to the next. The trajec-
tories of the vertices are not constrained. Thus the triangles
may change from frame to frame in location, orientation, and
proportion. For geometry encoding, redundancy in the vertex
trajectories is removed by a spatial othogonal transform fol-
lowed by temporal prediction, allowing low latency. For color
encoding, the triangles in each frame are projected back to
the coordinate system of the reference frame. In the reference
frame we voxelize the triangles in order to ensure that their
color textures are sampled uniformly in space regardless of
the sizes of the triangles, and in order to construct a common
vector space in which to describe the color textures and their
evolution from frame to frame. Redundancy of the color vec-
tors is removed by a spatial orthogonal transform followed
by temporal prediction, similar to redundancy removal for
geometry. Uniform scalar quantization and entropy coding
matched to the spatial transform are employed for both color
and geometry.

We compare triangle clouds to both triangle meshes and
point clouds in terms of compression, for live captured dy-
namic colored geometry. We ﬁnd that triangle clouds can be
compressed nearly as well as triangle meshes, while being far
more ﬂexible in representing live captured content. We also
ﬁnd that triangle clouds can be used to compress point clouds
with signiﬁcantly better performance than previously demon-
strated point cloud compression methods.

The organization of the paper is as follows. Following a
summary of related work in Section 2, preliminary material is
presented in Section 3. Components of our compression sys-
tem are presented in Section 4, while the core of our system
is presented in Section 5. Experimental results are presented
in Section 6. The conclusion is in Section 7.

2. RELATED WORK

2.1. Mesh compression

3D mesh compression has a rich history, particularly from
the 1990s forward. Overviews may be found in [1, 2, 3].
Fundamental is the need to code mesh topology, or connec-
tivity, such as in [4, 5]. Beyond coding connectivity, coding
the geometry, i.e., the positions of the vertices, is also fun-
damental. Many approaches have been taken, but one sig-
niﬁcant and practical approach to geometry coding is based

on “geometry images” [6] and their temporal extension, “ge-
ometry videos” [7]. In these approaches, the mesh is parti-
tioned into patches, the patches are projected onto a 2D plane
as charts, non-overlapping charts are laid out in a rectangu-
lar atlas, and the atlas is compressed using a standard image
or video coder, compressing both the geometry and the tex-
ture (i.e., color) data. For dynamic geometry, the meshes are
assumed to be temporally consistent (i.e., connectivity is con-
stant frame-to-frame) and the patches are likewise temporally
consistent. Geometry videos have been used for representing
and compressing free-viewpoint video of human actors [8].
Other key papers on mesh compression of human actors in
the context of tele-immersion include [9, 10].

2.2. Motion estimation

A critical part of dynamic mesh compression is the ability to
track points over time. If a mesh is deﬁned for a keyframe,
and the vertices are tracked over subsequent frames, then the
mesh becomes a temporally consistent dynamic mesh. There
is a huge body of literature in the 3D tracking, 3D motion es-
timation or scene ﬂow, 3D interest point detection and match-
ing, 3D correspondence, non-rigid registration, and the like.
We are particularly inﬂuenced by [11, 12, 13], all of which
produce in real time, given data from one or more RGBD sen-
sors for every frame t, a parameterized mapping fθt : R3 →
R3 that maps points in frame t to points in frame t+1. Though
corrections may need to be made at each frame, chaining the
mappings together over time yields trajectories for any given
set of points. Compressing these trajectories is similar to
compressing motion capture (mocap) trajectories, which has
been well studied. [14] is a recent example with many refer-
ences. Compression typically involves an intra-frame trans-
form to remove spatial redundancy and either temporal pre-
diction (if low latency is required) or a temporal transform
(if the entire clip or group of frames is available) to remove
temporal redundancy, as in [15].

2.3. Graph signal processing

Graph Signal Processing (GSP) has emerged as an extension
of the theory of linear shift invariant signal processing to the
processing of signals on discrete graphs, where the shift oper-
ator is taken to be the adjacency matrix of the graph, or alter-
natively the Laplacian matrix of the graph [16, 17]. GSP was
extended to critically sampled perfect reconstuction wavelet
ﬁlter banks in [18, 19]. These constructions were used for
dynamic mesh compression in [20, 21].

2.4. Point cloud compression using octrees

Sparse Voxel Octrees (SVOs) were developed in the 1980s to
represent the geometry of three-dimensional objects [22, 23].
Recently SVOs have been shown to have highly efﬁcient im-
plementations suitable for encoding at video frame rates [24].

In the guise of occupancy grids, they have also had signiﬁcant
use in robotics [25, 26, 27]. Octrees were ﬁrst used for point
cloud compression in [28]. They were further developed for
progressive point cloud coding, including color attribute com-
pression, in [29]. Octrees were extended to coding of dynamic
point clouds (i.e., point cloud sequences) in [30]. The focus
of [30] was geometry coding; their color attribute coding re-
mained rudimentary. Their method of inter-frame geometry
coding was to take the exclusive-OR (XOR) between frames
and code the XOR using an octree. Their method was imple-
mented in the Point Cloud Library [31].

2.5. Color attribute compression for static point clouds

To better compress the color attributes in static voxelized
point clouds, Zhang, Florˆencio, and Loop used transform
coding based on the Graph Fourier Transform (GFT), re-
cently developed in the theory of Graph Signal Processing
[32]. While transform coding based on the GFT has good
compression performance, it requires eigen-decompositions
for each coded block, and hence may not be computation-
ally attractive. To improve the computational efﬁciency,
while not sacriﬁcing compression performance, Queiroz and
Chou developed an orthogonal Region-Adaptive Hierarchical
Transform (RAHT) along with an entropy coder [33]. RAHT
is essentially a Haar transform with the coefﬁcients appropri-
ately weighted to take the non-uniform shape of the domain
(or region) into account. As its structure matches the Sparse
Voxel Octree, it is extremely fast to compute. Other ap-
proaches to non-uniform regions include the shape-adaptive
DCT [34] and color palette coding [35]. Further approaches
based on non-uniform sampling of an underlying stationary
process can be found in [36], which uses the KLT matched to
the sample, and in [37], which uses sparse representation and
orthogonal matching pursuit.

2.6. Dynamic point cloud compression

Thanou, Chou, and Frossard [38, 39] were the ﬁrst to deal
fully with dynamic voxelized points clouds, by ﬁnding
matches between points in adjacent frames, warping the
previous frame to the current frame, predicting the color at-
tributes of the current frame from the quantized colors of
the previous frame, and coding the residual using the GFT-
based method of [32]. Thanou et al. used the XOR-based
method of Kammerl et al. [30] for inter-frame geometry
compression. However, the method of [30] proved to be inef-
ﬁcient, in a rate-distortion sense, for anything except slowly
moving subjects, for two reasons. First, the method “pre-
dicts” the current frame from the previous frame, without
any motion compensation. Second, the method codes the
geometry losslessly, and so has no ability to perform a rate-
distortion trade-off. To address these shortcomings, Queiroz
and Chou [40] used block-based motion compensation and
rate-distortion optimization to select between coding modes

(intra or motion-compensated coding) for each block. Fur-
ther, they applied RAHT to coding the color attributes (in
intra-frame mode), color prediction residuals (in inter-frame
mode), and the motion vectors (in inter-frame mode). They
also used in-loop deblocking ﬁlters. Mekuria et al. [41] in-
dependently proposed block-based motion compensation for
dynamic point cloud sequences. Although they did not use
rate-distortion optimization, they used afﬁne transformations
for each motion-compensated block, rather than just trans-
lations. Unfortunately, it appears that block-based motion
compensation of dynamic point cloud geometry tends to
produce gaps between blocks, which are perceptually more
damaging than indicated by objective metrics such as the
Haussdorf-based metrics commonly used in geometry com-
pression [42].

2.7. Key learnings

Some of the key learnings from the previous work, taken as a
whole, are that

• Point clouds are preferable to meshes for resilience
to noise and non-manifold signals measured in real
world signals, especially for real time capture where
the computational cost of heavy duty pre-processing
(e.g., surface reconstruction,
topological denoising,
charting) can be prohibitive.

• For geometry coding in static scenes, point clouds ap-
pear to be more compressible than meshes, even though
the performance of point cloud geometry coding seems
to be limited by the lossless nature of the current octree
methods. In addition, octree processing for geometry
coding is extremely fast.

• For color attribute coding in static scenes, both point
clouds and meshes appear to be well compressible. If
charting is possible, compressing the color as an image
may win out due to the maturity of image compression
algorithms today. However, direct octree processing for
color attribute coding is extremely fast, as it is for ge-
ometry coding.

• For both geometry and color attribute coding in dy-
namic scenes (or inter-frame coding), temporally con-
sistent dynamic meshes are highly compressible. How-
ever, ﬁnding a temporally consistent mesh can be chal-
lenging from a topological point of view as well as from
a computational point of view.

In our work, we aim to achieve the high compression ef-
ﬁciency possible with intra-frame point cloud compression
and inter-frame dynamic mesh compression, while simulta-
neously achieving the high computational efﬁciency possible
with octree-based processing, as well as its robustness to real-
world noise and non-manifold data.

3. PRELIMINARIES

3.1. Notation

Notation is given in Table 1.

symbol
[N ]
t
vi or v(t)
i
fm or f (t)
m
cn or c(t)
n
ai or a(t)
i
V or V (t)
F or F (t)
C or C(t)
A or A(t)
T or T (t)
P or P (t)
V or V(t)
F or F(t)
C or C(t)
A
TA
M,Mv,M1
W,Wv,Wrv
I, Iv, Irv
ˆV, ˆC, ˆA,. . .
ˆVv or ˆV(t)
v
Vr
ˆVrv or ˆV(t)
rv
Cr = C
Crv orC(t)
rv
J
U
∆motion
∆color,intra
∆color,inter

description
set of integers {1, 2, · · · , N }
time or frame index
3D point with coordinates xi, yi, zi
face with vertex indices im, jm, km
color with components Yn, Un, Vn
generic attribute vector ai1, . . . , ain
set of Np points {v1, . . . , vNp }
set of Nf faces {f1, . . . , fNf }
set of Nc colors {c1, . . . , cNc}
set of Na attribute vectors {a1, . . . , aNa }
triangle cloud (V, F, C) or (V, F, A)
point cloud (V, C) or (V, A)
Np × 3 matrix with i-th row [xi, yi, zi]
Nf × 3 matrix with m-th row [im, jm, km]
Nc × 3 matrix with n-th row [Yn, Un, Vn]
list (i.e., matrix) of attributes
list of transformed attributes
lists of Morton codes
lists of weights
lists of indices
lists of quantized or reproduced quantities
list of voxelized vertices
list of reﬁned vertices
list of voxelized reﬁned vertices
list of colors of reﬁned vertices
list of colors of voxelized reﬁned vertices
octree depth
upsampling factor
motion quantization stepsize
intra-frame color quantization stepsize
inter-frame color quantization stepsize

Table 1: Notation

3.2. Dynamic triangle clouds

A dynamic triangle cloud is a numerical representation of a
time changing 3D scene or object. We denote it by a sequence
{T (t)} where T (t) is a triangle cloud at time t. Each individ-
ual frame T (t) has geometry (shape and position) and color
information.

The geometry information consists of a list of vertices
: i = 1, · · · , Np}, where each vertex v(t)
i =
] is a point in 3D, and a list of triangles (or

V (t) = {v(t)
i
, z(t)
, y(t)
[x(t)
i
i
i

m , j(t)

faces) F (t) = {f (t)
m : m = 1, · · · , Nf }, where each face
f (t)
m = [i(t)
m , k(t)
m ] is a vector of indices of vertices from
V (t). We denote by V(t) the Np × 3 matrix whose i-th row
is the point v(t)
, and similarly we denote by F(t) the Nf × 3
matrix whose m-th row is the triangle f (t)
m . The triangles in a
triangle cloud do not have to be adjacent or form a mesh, and
they can overlap. Two or more vertices of a triangle may have
the same coordinates, thus collapsing into a line or point.

i

n = [Y (t)

The color information consists of a list of colors C(t) =
{c(t)
n : n = 1, · · · , Nc}, where each color c(t)
n , U (t)
n ,
V (t)
n ] is a vector in YUV space (or other convenient color
space). We denote by C(t) the Nc × 3 matrix whose n-th row
is the color c(t)
n . The list of colors represents the colors across
the surfaces of the triangles. To be speciﬁc, c(t)
n is the color
of a “reﬁned” vertex v(t)
r (n), where the reﬁned vertices are
obtained by uniformly subdividing each triangle in F (t) by
upsampling factor U , as shown in Figure 1b for U = 4. We
denote by V(t)
the Nc ×3 matrix whose nth row is the reﬁned
r
r (n). V(t)
vertex v(t)
r can be computed from V (t) and F (t), so
we do not need to encode it, but we will use it to compress the
color information. Note that Nc = Nf (U +1)(U +2)/2. The
upsampling factor U should be high enough so that it does not
limit the color spatial resolution obtainable by the color cam-
eras. In our experiments, we set U = 10 or higher. Setting
U higher does not typically affect the bit rate signiﬁcantly,
though it does affect memory and computation in the encoder
and decoder.

Thus frame t can be represented by the triple V(t), F(t),
C(t). We use a Group of Frames (GOF) model, in which the
sequence is partitioned into GOFs. The GOFs are processed
independently. Without loss of generality, we label the frames
in a GOF t = 1 . . . , N . There are two types of frames: refer-
ence and predicted. In each GOF, the ﬁrst frame (t = 1) is a
reference frame and all other frames (t = 2, . . . , N ) are pre-
dicted. Within a GOF, all frames must have the same number
of vertices, triangles, and colors: ∀t ∈ [N ], V(t) ∈ RNp×3,
F(t) ∈ [Np]Nf ×3 and C(t) ∈ RNc×3. The triangles are as-
sumed to be consistent across frames so that there is a cor-
respondence between colors and vertices within the GOF. In
Figure 1b we show an example of the correspondences be-
tween two consecutive frames in a GOF. Across GOFs, the
GOFs may have a different numbers of frames, vertices, tri-
angles, and colors.

In the following two subsections, we outline how to obtain
a triangle cloud from an existing point cloud or an existing
triangular mesh.

3.2.1. Converting a dynamic point cloud to a dynamic trian-
gle cloud

A dynamic point cloud is a sequence of point clouds {P (t)},
where each P (t) is a list of [x, y, z] coordinates each with an
attribute attached to it, like color. To produce a triangle cloud,

(a) Man mesh.

(b) Correspondences between two consecutive frames.

Fig. 1: Triangle cloud geometry information.

we need a way to ﬁt a point cloud to a set of triangles in such
a way that we produce GOFs with consistent triangles. One
way of doing that is the following.

1. Decide if frame in P (t) is reference or predicted.

2. If reference frame:

(a) Fit triangles to point cloud to obtain V(1), F(1),
where V(1) is a list of vertices and F(1) is a list of
triangles.

(b) Subdivide each triangle, and project each vertex
of the subdivision to the closest point in the cloud
to obtain C(1).

3. If predicted frame:

(a) Deform triangle cloud of previous reference
frame to ﬁt point cloud to obtain V(t), such that
the ith point v(t)
in V(t) corresponds to the ith
i
point v(1)
in V(1).

i

(b) Subdivide each triangle, and project each vertex
of the subdivision to the closest point in the cloud
to obtain C(t).

(c) Go to step 1.

This process will introduce geometric distortion and a change
in the number of points. All points will be forced to lie in a
uniform grid on the surface of a triangle. The triangle ﬁtting
can be done using triangular mesh ﬁtting and tracking tech-
niques such as in [11, 12, 13].

3.2.2. Converting a dynamic triangular mesh to a dynamic
triangle cloud

The geometry of a triangular mesh is represented by a list of
key points or vertices and their connectivity, given by an ar-
ray of 3D coordinates V and faces F. The triangles are con-
strained to form a smooth surface without holes. For color,
the mesh representation typically includes an array of 2D tex-
ture coordinates T ∈ RNp×2 and a texture image. The color
at any point on a face can be retrieved (for rendering) by in-
terpolating the texture coordinates at that point on the face
and sampling the image at the interpolated coordinates. The
sequence of triangular meshes is assumed to be temporally
consistent, meaning that within a GOF, the meshes of the pre-
dicted frames are deformations of the reference frame. The
sizes and positions of the triangles may change but the de-
formed mesh still represents a smooth surface. The sequence
of key points V(t) thus can be traced from frame to frame
and the faces are all the same. To convert the color informa-
tion into the dynamic triangle cloud format, for each frame
and each triangle, the mesh sub-division function can be ap-
plied to obtain texture coordinates of reﬁned triangles. Then
the texture image can be sampled and a color matrix C can be
formed for each frame.

3.3. Compression system overview

In this section we provide an overview of our system for com-
pressing dynamic triangle clouds. We compress consecutive
GOFs sequentially and independently, so we focus on the sys-
tem for compressing an individual GOF (V(t), F(t), C(t)) for
t ∈ [N ].

For the reference frame, we voxelize the vertices V(1),
v using octree en-

and then encode the voxelized vertices V(1)

coding. We encode the connectivity F(1) with a lossless en-
tropy coder.
(We could use method such as EdgeBreaker
or TFAN [4, 5], but for simplicity for this small amount of
data in our experiments we use the lossless universal encoder
gzip.) We code the connectivity only once per GOF (i.e.,
for the reference frame), since the connectivity is consistent
across the GOF, i.e., F(t) = F(1) for t ∈ [N ]. We voxelize
the colors C(1), and encode the voxelized colors C(1)
rv using
a transform coding method that combines the region adaptive
hierarchical transform (RAHT) [33], uniform scalar quantiza-
tion, and adaptive Run-Length Golomb-Rice (RLGR) entropy
coding [43]. At the cost of additional complexity, the RAHT
transform could be replaced by transforms with higher per-
formance [36, 37].

v − ˆV(t−1)

For predicted frames, we compute prediction residuals
from the previously decoded frame. Speciﬁcally, for each pre-
dicted frame t > 1 we compute a motion residual ∆V(t)
v =
rv − ˆC(t−1)
and a color residual ∆C(t)
V(t)
,
where we have denoted with a hat a quantity that has been
compressed and decompressed. These residuals are encoded
using again RAHT followed by uniform scalar quantization
and entropy coding.

rv = C(t)

rv

v

It is important to note that we do not directly compress the
list of vertices V(t) or the the list of colors C(t) (or their pre-
diction residuals). Rather, we voxelize them ﬁrst with respect
to their corresponding vertices in the reference frame, and
then compress them. This ensures that 1) if two or more ver-
tices or colors fall into the same voxel, they receive the same
representation and hence are encoded only once, and 2) the
colors (on the set of reﬁned vertices) are resampled uniformly
in space regardless of the density or shapes of triangles.

In the next section, we detail the basic elements of the sys-
tem: reﬁnement, voxelization, octrees, and transform coding.
In the section after that, we detail how these basic elements
are put together to encode and decode a sequence of triangle
clouds.

4. REFINEMENT, VOXELIZATION, OCTREES, AND
TRANSFORM CODING

4.1. Reﬁnement

Given a list of faces F, its corresponding list of vertices V,
and upsampling factor U , a list of “reﬁned” vertices Vr can
be produced using Algorithm 1. Step 1 (in Matlab notation)
assembles three equal-length lists of vertices (each as an Nf ×
3 matrix), containing the three vertices of every face. Step 5
appends a linear combinations of the faces’ vertices to a grow-
ing list of reﬁned vertices.

We assume that the list of colors C is in 1-1 correspon-
dence with the list of reﬁned vertices Vr. Indeed, to obtain
the colors C from a textured mesh, the 2D texture coordi-
nates T can be linearly interpolated in the same manner as
the 3D position coordinates V to obtain “reﬁned” texture co-

Algorithm 1 Reﬁnement (reﬁne)
Input: V, F, U

1: Vi = V(F(:, i), :), i = 1, 2, 3 // ith vertex of all faces
2: Initialize Vr = empty list
3: for i = 0 to U do
4:
5:

Vr = [Vr; V1+(V2−V1)i/U +(V3−V1)j/U ]

for j = 0 to U − i do

end for

6:
7: end for
Output: Vr

ordinates Tr which may then be used to lookup appropriate
color Cr = C in the texture map.

4.2. Morton codes and voxelization

A voxel is a volumetric element used to represent the at-
tributes of an object in 3D over a small region of space.
Analogous to 2D pixels, 3D voxels are deﬁned on a uniform
grid. We assume the geometric data live in the unit cube
[0, 1)3, and we uniformly partition the cube into voxels of
size 2−J × 2−J × 2−J .

Now consider a list of points V = [vi] and an equal-
length list of attributes A = [ai], where ai is the real-valued
attribute (or vector of attributes) of vi. (These may be, for
example, the list of reﬁned vertices Vr and their associated
colors Cr = C as discussed above.) In the process of vox-
elization, the points are partitioned into voxels, and the at-
tributes associated with the points in a voxel are averaged.
The points within each voxel are quantized to the voxel cen-
ter. Each occupied voxel is then represented by the voxel cen-
ter and the average of the attributes of the points in the voxel.
Moreover, the occupied voxels are put into Z-scan order, also
known as Morton order [44]. The ﬁrst step in voxelization is
to quantize the vertices and to produce their Morton codes.
The Morton code m for a point (x, y, z) is obtained simply
by interleaving (or “swizzling”) the bits of x, y, and z, with
x being higher order than y, and y being higher order than z.
For example, if x = x4x2x1, y = y4y2y1, and z = z4z2z1
(written in binary), then the Morton code for the point would
be m = x4y4z4x2y2z2x1y1z1. The Morton codes are sorted,
duplicates are removed, and all attributes whose vertices have
a particular Morton code are averaged.

The procedure is detailed in Algorithm 2. Vint is the list
of vertices with their coordinates, previously in [0, 1), now
mapped to integers in {0, . . . , 2J − 1}. M is the correspond-
ing list of Morton codes. Mv is the list of Morton codes,
sorted with duplicates removed, using the Matlab function
unique. I and Iv are vectors of indices such that Mv = M(I)
and M = Mv(Iv), in Matlab notation. (That is, the ivth ele-
ment of Mv is the I(iv)th element of M and the ith element
of M is the Iv(i)th element of Mv.) Av = [¯aj] is the list of

attribute averages

¯aj =

1
Nj

(cid:88)

ai,

i:M(i)=Mv(j)

(1)

where Nj is the number of elements in the sum. Vv is the list
of voxel centers. The algorithm has complexity O (N log N ),
where N is the number of input vertices.

Algorithm 2 Voxelization (voxelize)
Input: V, A, J
1: Vint = f loor(V ∗ 2J ) // map coords to {0, . . . , 2J − 1}
2: M = morton(Vint) // generate list of morton codes
3: [Mv, I, Iv] = unique(M) // ﬁnd unique codes, and sort
4: Av = [¯aj], where ¯aj = mean(A(M = Mv(j)) is the
average of all attributes whose Morton code is the jth
Morton code in the list Mv

5: Vv = (Vint(I, :) + 0.5) ∗ 2−J // compute voxel centers
Output: Vv (or equivalently Mv), Av, Iv.

4.3. Octree encoding

Any set of voxels in the unit cube, each of size 2−J × 2−J ×
2−J , designated occupied voxels, can be represented with an
octree of depth J [22, 23]. An octree is a recursive subdivi-
sion of a cube into smaller cubes, as illustrated in Figure 2.
Cubes are subdivided only as long as they are occupied (i.e.,
contain any occupied voxels). This recursive subdivision can
be represented by an octree with depth J, where the root cor-
responds to the unit cube. The leaves of the tree correspond
to the set of occupied voxels.

There is a close connection between octrees and Morton
codes. In fact, the Morton code of a voxel, which has length
3J bits broken into J binary triples, encodes the path in the
octree from the root to the leaf containing the voxel. More-
over, the sorted list of Morton codes results from a depth-ﬁrst
traversal of the tree.

Each internal node of the tree can be represented by one
byte, to indicate which of its eight children are occupied. If
these bytes are serialized in a depth-ﬁrst traversal of the tree,
the serialization (which has a length in bytes equal to the num-
ber of internal nodes of the tree) can be used as a description
of the octree, from which the octree can be reconstructed.
Hence the description can also be used to encode the ordered
list of Morton codes of the leaves. This description can be fur-
ther compressed using a context adaptive arithmetic encoder.
However, for simplicity in our experiments, we use gzip in-
stead of an arithmetic encoder.

In this way, we encode any set of occupied voxels in a

canonical (Morton) order.

Fig. 2: Cube subdivision. Blue cubes represent occupied re-
gions of space.

Fig. 3: One level of RAHT applied to a cube of eight voxels,
three of which are occupied.

4.4. Transform coding

In this section we describe the region adaptive hierarchi-
cal transform (RAHT) [33] and its efﬁcient implementation.
RAHT can be described as a sequence of orthonormal trans-
forms applied to attribute data living on the leaves of an
octree. For simplicity we assume the attributes are scalars.
This transform processes voxelized attributes in a bottom
up fashion, starting at the leaves of the octree. The inverse
transform reverses this order.

Consider eight adjacent voxels, three of which are occu-
pied, having the same parent in the octree, as shown in Fig-
ure 3. The colored voxels are occupied (have an attribute) and
the transparent ones are empty. Each occupied voxel is as-
signed a unit weight. For the forward transform, transformed
attribute values and weights will be propagated up the tree.

One level of the forward transform proceeds as follows.
Pick a direction (x, y, z), then check whether there are two
occupied cubes that can be processed along that direction. In
the leftmost part of Figure 3 there are only three occupied
cubes, red, yellow, and blue, having weights wr, wy, and wb,
respectively. To process in the direction of the x axis, since
the blue cube does not have a neighbor along the horizontal
direction, we copy its attribute value ab to the second stage
and keep its weight wb. The attribute values ay and ar of
the yellow and red cubes can be processed together using the
orthonormal transformation

(cid:21)

(cid:20)a0
g
a1
g

=

√

1
wy + wr

(cid:20) √
wy
√
wr
−

√
√

wr
wy

(cid:21)

(cid:21) (cid:20)ay
ar

,

(2)

where the transformed coefﬁcients a0
g respectively
represent low pass and high pass coefﬁcients appropriately

g and a1

Fig. 4: Transform coding system for voxelized point clouds.

weighted. Both transform coefﬁcients now represent infor-
mation from a region with weight wg = wy + wr (green
cube). The high pass coefﬁcient is stored for entropy coding
along with its weight, while the low pass coefﬁcient is further
processed and put in the green cube. For processing along
the y axis, the green and blue cubes do not have neighbors,
so their values are copied to the next level. Then we process
in the z direction using the same transformation in (2) with
weights wg and wb.

This process is repeated for each cube of eight subcubes
at each level of the octree. After J levels, there remains one
low pass coefﬁcient that corresponds to the DC component;
the remainder are high pass coefﬁcients. Since after each pro-
cessing of a pair of coefﬁcients, the weights are added and
used during the next transformation, the weights can be inter-
preted as being inversely proportional to frequency. The DC
coefﬁcient is the one that has the largest weight, as it is pro-
cessed more times and represents information from the entire
cube, while the high pass coefﬁcients, which are produced
earlier, have smaller weights because they contain informa-
tion from a smaller region. The weights depend only on the
octree (not the coefﬁcients themselves), and thus can provide
a frequency ordering for the coefﬁcients. We sort the trans-
formed coefﬁcients by decreasing magnitude of weight.

Finally, the sorted coefﬁcients are quantized using uni-
form scalar quantization, and are entropy coded using adap-
tive Run Length Golomb-Rice coding [43]. The pipeline is
illustrated in Figure 4.

Efﬁcient implementations of RAHT and its inverse are de-
tailed in Algorithms 4 and 5, respectively. Algorithm 3 is a
prologue to each. Algorithm 6 is our uniform scalar quantiza-
tion.

5. ENCODING AND DECODING

In this section we describe in detail encoding and decoding
of dynamic triangle clouds. First we describe encoding and
decoding of reference frames. Following that, we describe
encoding and decoding of predicted frames. For both refer-
ence and predicted frames, we describe ﬁrst how geometry
is encoded and decoded, and then how color is encoded and
decoded. The overall system is shown in Figure 5.

5.1. Encoding and decoding of reference frames

For reference frames, encoding is summarized in Algo-
rithm 7, while decoding is summarized in Algorithm 8.

Algorithm 3 Prologue to Region Adaptive Hierarchical
Transform (RAHT) and its Inverse (IRAHT) (prologue)
Input: V, J

1: M1 = morton(V) // morton codes
2: N = length(M1) // number of points
3: for (cid:96) = 1 to 3J do // deﬁne (I(cid:96), M(cid:96), W(cid:96), F(cid:96)), ∀(cid:96)
4:
5:
6:

if (cid:96) = 1 then // initialize indices of coeffs at layer 1
I1 = (1 : N )T // vector of indices from 1 to N

else // deﬁne indices of coeffs at layer (cid:96)

I(cid:96) = I(cid:96)−1(¬[0; F(cid:96)−1]) // left sibs and singletons

end if

7:
8:
9: M(cid:96) = M1(I(cid:96)) // morton codes at layer (cid:96)
10: W(cid:96) = [I(cid:96)(2 : end); N + 1] − I(cid:96) // weights
11:

D = M(cid:96)(1 : end − 1) ⊕ M(cid:96)(2 : end) // path diffs
F(cid:96) = (D ∧ (23J − 2(cid:96))) = 0 // left sibling ﬂags

12:
13: end for
Output: {(I(cid:96), W(cid:96), F(cid:96)) : (cid:96) = 1, . . . , 3J}, and N

Algorithm 4 Region Adaptive Hierarchical Transform
(RAHT)
Input: V, A, J

1: [{(I(cid:96), W(cid:96), F(cid:96))}, N ] = prologue(V, J)
2: TA = A // perform transform in place
3: W = 1 // initialize to N -vector of unit weights
4: for (cid:96) = 1 to 3J − 1 do
5:
6:
7:

i0 = I(cid:96)([F(cid:96); 0] == 1) // left sibling indices
i1 = I(cid:96)([0; F(cid:96)] == 1) // right sibling indices
w0 = W(cid:96)([F(cid:96); 0] == 1) // left sibling weights
w1 = W(cid:96)([0; F(cid:96)] == 1) // right sibling weights
x0 = TA(i0, :) // left sibling coefﬁcients
x1 = TA(i1, :) // right sibling coefﬁcients
a = repmat(sqrt(w0./(w0 +w1)), 1, size(TA, 2))
b = repmat(sqrt(w1./(w0+w1)), 1, size(TA, 2))
TA(i0, :) = a . ∗ x0 + b . ∗ x1
TA(i1, :) = −b . ∗ x0 + a . ∗ x1

8:
9:
10:
11:
12:

13:
14:
15: W(i0) = W(i0) + W(i1)
16: W(i1) = W(i0)
17: end for
Output: TA, W

Fig. 5: Encoder (left) and decoder (right). The switches are in the t = 1 position, and ﬂip for t > 1.

Algorithm 5 Inverse Region Adaptive Hierarchical Trans-
form (IRAHT)
Input: V, TA, J

1: [{(I(cid:96), W(cid:96), F(cid:96))}, N ] = prologue(V, J)
2: A = TA // perform inverse transform in place
3: for (cid:96) = 3J − 1 down to 1 do
4:
5:
6:
7:
8:

i0 = I(cid:96)([F(cid:96); 0] == 1) // left sibling indices
i1 = I(cid:96)([0; F(cid:96)] == 1) // right sibling indices
w0 = W(cid:96)([F(cid:96); 0] == 1) // left sibling weights
w1 = W(cid:96)([0; F(cid:96)] == 1) // right sibling weights
x0 = TA(i0, :) // left sibling coefﬁcients
x1 = TA(i1, :) // right sibling coefﬁcients
a = repmat(sqrt(w0./(w0 +w1)), 1, size(TA, 2))
b = repmat(sqrt(w1./(w0+w1)), 1, size(TA, 2))
TA(i0, :) = a . ∗ x0 − b . ∗ x1
TA(i1, :) = b . ∗ x0 + a . ∗ x1

9:
10:
11:
12:
13:
14: end for
Output: A

Algorithm 6 Uniform scalar quantization (quantize)

Input: A, step, midriseORmidstep

1: if midriseORmidstep = midstep then
ˆA = round(A/step) ∗ step
2:
3: else // midriseORmidstep = midrise
4:
5: end if
Output: ˆA

ˆA = [round(A/step − 0.5) + 0.5] ∗ step

v ] = voxelize( ˆV(1), V(1), J) s.t. ˆV(1) =

r

v , I(1)

(from system input)

v , V(1)
v (I(1)
v )

Algorithm 7 Encode reference frame (I-encoder)
Input: J, U , ∆color,intra (from system parameters)
Input: V(1), F(1), C(1)
1: // Geometry
2: ˆV(1) = quantize(V(1), 2−J , midrise)
3: [ ˆV(1)
ˆV(1)
4: // Color
5: ˆV(1)
r = ref ine( ˆV(1), F(1), U )
rv , C(1)
6: [ ˆV(1)
rv (I(1)
ˆV(1)
rv )
rv , W(1)
7: [TC(1)
(1)
rv = quantize(TC(1)
rv , (cid:100)TC
v ), code(I(1)
(to reference frame decoder)

8: (cid:100)TC
9: ˆC(1)
Output: code( ˆV(1)

rv ] = voxelize( ˆV(1)

rv = IRAHT ( ˆV(1)

rv ] = RAHT ( ˆV(1)

rv , C(1)

r , C(1)

(1)
rv , J)

rv , I(1)

rv , J)
rv , ∆color,intra, midstep)

v ), code(F(1)), code((cid:100)TC

Output: ˆV(1), ˆV(1)
r
v , ˆC(1)
Output: ˆV(1)
rv (to reference frame buffer)

(to predicted frame encoder)

r , J) s.t. ˆV(1)

r =

(1)
rv )

Algorithm 8 Decode reference frame (I-decoder)
Input: J, U , ∆color,intra (from system parameters)
Input: code( ˆV(1)

v ), code(I(1)

v ), code(F(1)), code((cid:100)TC

(1)
rv )

(from reference frame encoder)

v (I(1)
v )

rv ] = voxelize( ˆV(1)

1: // Geometry
2: ˆV(1) = ˆV(1)
3: // Color
4: ˆV(1)
r = ref ine( ˆV(1), F(1), U )
5: [ ˆV(1)
rv , I(1)
6: W(1)
rv = RAHT ( ˆV(1)
7: ˆC(1)
rv = IRAHT ( ˆV(1)
8: ˆC(1)
r = ˆC(1)
rv (I(1)
rv )
Output: ˆV(1), F(1), ˆC(1)
r
Output: ˆV(1)
v , ˆV(1)
rv , I(1)
Output: ˆV(1)
rv (to reference frame buffer)

v , I(1)
v , ˆC(1)

rv , J)
rv , (cid:100)TC

r , J) s.t. ˆV(1)

(1)
rv , J)

(to renderer)
rv (to predicted frame decoder)

r = ˆV(1)

rv (I(1)
rv )

5.1.1. Geometry encoding and decoding

We assume that the vertices in V(1) are in Morton order. If
not, we put them into Morton order and permute the indices in
F(1) accordingly. The lists V(1) and F(1) are the geometry-
related quantities in the reference frame transmitted from the
encoder to the decoder. V(1) will be reconstructed at the de-
coder with some loss as ˆV(1), and F(1) will be reconstructed
losslessly. We now describe the process.

v (I(1)

At the encoder, the vertices in V(1) are ﬁrst quantized to
the voxel grid, producing a list of quantized vertices ˆV(1),
the same length as V(1). There may be duplicates in ˆV(1),
because some vertices may have collapsed to the same grid
point. ˆV(1) is then voxelized (without attributes), the effect
of which is simply to remove the duplicates, producing a pos-
along with a list of indices I(1)
sibly slightly shorter list ˆV(1)
v
v
v ). Since ˆV(1)
such that (in Matlab notation) ˆV(1) = ˆV(1)
v
has no duplicates, it represents a set of voxels. This set can
be described by an octree. The byte sequence representing
the octree can be compressed with any entropy encoder; we
use gzip. The list of indices I(1)
v , which has the same length
as ˆV(1), indicates, essentially, how to restore the duplicates,
which are missing from ˆV(1)
in-
crease in unit steps for all vertices in ˆV(1) except the dupli-
cates, for which there is no increase. The list of indices is
thus a sequence of runs of unit increases alternating with runs
of zero increases. This binary sequence of increases can be
encoded with any entropy encoder; we use gzip on the run
lengths. Finally the list of faces F(1) can be encoded with any
entropy encoder; we again use gzip, though algorithms such
as [4, 5] might also be used.

v . In fact, the indices in I(1)
v

The decoder entropy decodes ˆV(1)

then recovers ˆV(1) = ˆV(1)
sion of V(1), to obtain both ˆV(1) and F(1).

v (I(1)

v , I(1)

v , and F(1), and
v ), which is the quantized ver-

5.1.2. Color encoding and decoding

Let V(1)
r = ref ine(V(1), F(1), U ) be the list of “reﬁned ver-
tices” obtained by upsampling, by factor U , the faces F(1)
whose vertices are V(1). We assume that the colors in the list
C(1)
r = C(1) correspond to the reﬁned vertices in V(1)
r . In
particular, the lists have the same length. Here, we subscript
the list of colors by an ‘r’ to indicate that it corresponds to the
list of reﬁned vertices.

r

When the vertices V(1) are quantized to ˆV(1), the reﬁned
vertices change to ˆV(1)
r = ref ine( ˆV(1), F(1), U ). The list
of colors C(1)
can also be considered as indicating the colors
on ˆV(1)
is the color-related quantity in the
reference frame transmitted from the encoder to the decoder.
The decoder will reconstruct C(1)
r . We
now describe the process.

r with some loss ˆC(1)

r . The list C(1)
r

r

r = ˆV(1)

At the encoder, the reﬁned vertices ˆV(1)

rv has the same length as ˆV(1)

are obtained as
r
described above. Then the vertices ˆV(1)
and their associated
color attributes C(1)
are voxelized to obtain a list of voxels
r
rv , and the list of indices I(1)
rv , the list of voxel colors C(1)
ˆV(1)
rv
rv (I(1)
such that (in Matlab notation) ˆV(1)
rv ). The list
of indices I(1)
r , and contains for
each vertex in ˆV(1)
the index of its corresponding vertex in
ˆV(1)
rv . Particularly if the upsampling factor U is large, there
may be many reﬁned vertices falling into each voxel. Hence
rv may be signiﬁcantly shorter than the list ˆV(1)
the list ˆV(1)
(and the list I(1)
rv ). However, unlike the geometry case, in this
case the list I(1)
rv need not be transmitted.

r

r

The list of voxel colors ˆC(1)

rv , each with unit weight, is
transformed by RAHT to an equal-length list of transformed
colors TC(1)
rv . The transformed
colors then quantized with stepsize ∆color,intra to obtain

rv and associated weights W(1)

(1)
(cid:100)TC
rv . The quantized RAHT coefﬁcients are entropy coded
as described in Section 4.4 using the associated weights,

(1)
and are transmitted. Finally, (cid:100)TC
rv is inverse transformed
by RAHT to obtain ˆC(1)
rv . These represent the quantized
voxel colors, and will be used as a reference for subsequent
predicted frames.

r

r

At the decoder, similarly, the reﬁned vertices ˆV(1)

are ob-
tained by upsampling, by factor U , the faces F(1) whose ver-
tices are ˆV(1) (both of which have been decoded already in
the geometry step). ˆV(1)
is then voxelized (without attributes)
rv and list of indices I(1)
to produce the list of voxels ˆV(1)
rv such
that ˆV(1)
rv ). The weights W(1)
rv are recovered by
using RAHT to transform a null signal on the vertices ˆV(1)
r ,
(1)
each with unit weight. Then (cid:100)TC
rv is entropy decoded using
the recovered weights and inverse transformed by RAHT to
obtain the quantized voxel colors ˆC(1)
rv . Finally, the quantized
r = ˆC(1)
reﬁned vertex colors can be obtained as ˆC(1)

r = ˆV(1)

rv (I(1)

rv (I(1)

rv ).

5.2. Encoding and decoding of predicted frames

We assume that all N frames in a GOP are aligned. That
is, the lists of faces, F(1), . . . , F(N ), are all identical. More-
over, the lists of vertices, V(1), . . . , V(N ), all correspond in
the sense that the ith vertex in list V(1) (say, v(1)(i) = v(1)
)
i
corresponds to the ith vertex in list V(t) (say, v(t)(i) = v(t)
),
for all t = 1, . . . , N . (v(1)(i), . . . , v(N )(i)) is the trajectory
of vertex i over the GOF, i = 1, . . . , Np, where Np is the
number of vertices.

i

Similarly, when the faces are upsampled by factor U to
create new lists of reﬁned vertices, V(1)
r — and
their colors, C(1)
r , . . . , C(N )
r — the irth elements of these lists
also correspond to each other across the GOF, ir = 1, . . . , Nc,
where Nc is the number of reﬁned vertices, or the number of
colors.

r , . . . , V(N )

r

r (ir), . . . , c(N )

r (ir), . . . , v(N )
r

The trajectory {(v(1)(i), . . . , v(N )(i)) : i = 1, . . . , Np}
can be considered an attribute of vertex v(1)(i), and likewise
the trajectories {(v(1)
(ir)) : ir = 1, . . . , Nc}
and {(c(1)
(ir)) : ir = 1, . . . , Nc} can be con-
sidered attributes of reﬁned vertex v(1)
r (ir). Thus the trajecto-
ries can be partitioned according to how the vertex v(1)(i) and
the reﬁned vertex v(1)
r (ir) are voxelized. As for any attribute,
the average of the trajectories in each cell of the partition is
used to represent all trajectories in the cell. Our scheme codes
these representative trajectories. This could be a problem if
trajectories diverge from the same, or nearly the same, point,
for example, when clapping hands separate. However, this
situation is usually avoided by restarting the GOF by insert-
ing a key frame, or reference frame, whenever the topology
changes, and by using a sufﬁciently ﬁne voxel grid.

In this section we show how to encode and decode the
predicted frames, i.e., frames t = 2, . . . , N , in each GOF.
The frames are processed one at a time, with no look-ahead,
to minimize latency. The encoding is detailed in Algorithm 9,
while decoding is detailed in Algorithm 10.

5.2.1. Geometry encoding and decoding

At the encoder, for frame t, as for frame 1, the vertices V(1),
or equivalently the vertices ˆV(1), are voxelized. However, for
frame t > 1 the voxelization occurs with attributes V(t). In
this sense, the vertices V(t) are projected back to the refer-
ence frame, where they are voxelized like attributes. As for
frame 1, this produces a possibly slightly shorter list ˆV(1)
v
v (I(1)
along with a list of indices I(1)
v ).
v
In addition, it produces an equal-length list of representative
attributes, V(t)
v . Such a list is produced every frame. There-
fore the previous frame can be used as a prediction. The
v = V(t)
prediction residual ∆V(t)
is transformed,
quantized with stepsize ∆motion, inverse transformed, and
added to the prediction to obtain the reproduction ˆV(t)
v , which
goes into the frame buffer. The quantized transform coefﬁ-

such that ˆV(1) = ˆV(1)

v − ˆV(t−1)

v

v

v

v ] = voxelize( ˆV(1), V(t), J) s.t. ˆV(1) =

v , I(1)

r
, ˆC(t−1)
rv

(from previous frame buffer)

v − ˆV(t−1)
v ] = RAHT ( ˆV(1)

Algorithm 9 Encode predicted frame (P-encoder)
Input: J, ∆motion, ∆color,inter (from system parameters)
Input: V(t), C(t)
(from system input)
r
Input: ˆV(1), ˆV(1)
(from reference frame encoder)
Input: ˆV(t−1)
1: // Geometry
2: [ ˆV(1)
v , V(t)
ˆV(1)
v (I(1)
v )
3: ∆V(t)
v = V(t)
4: [T∆V(t)
v , W(1)
(t)
5: (cid:92)T∆V
v = quantize(T∆V(t)
v , (cid:92)T∆V
6: (cid:100)∆V
7: ˆV(t)
8: // Color
rv , C(t)
9: [ ˆV(1)
rv (I(1)
ˆV(1)
rv )
rv = C(t)
10: ∆C(t)
rv , W(1)
11: [T∆C(t)
(t)
12: (cid:92)T∆C
rv = quantize(T∆C(t)
rv ,(cid:92)T∆C

rv − ˆC(t−1)
rv ] = RAHT ( ˆV(1)

v = IRAHT ( ˆV(1)

rv ] = voxelize( ˆV(1)

v , ∆motion, midstep)

r , J) s.t. ˆV(1)

rv , ∆color,inter, midstep)

v = ˆV(t−1)

v , ∆V(t)

rv , ∆C(t)

r , C(t)

(t)
v , J)

rv , I(1)

+ (cid:100)∆V

v , J)

rv , J)

r =

(t)
v

(t)
rv , J)

(t)

(t)

rv

v

rv = IRAHT ( ˆV(1)
rv + (cid:100)∆C

13: (cid:100)∆C
rv = ˆC(t−1)
14: ˆC(t)
Output: code((cid:92)T∆V

(t)
rv

(t)

v ), code((cid:92)T∆C

(t)
rv ) (to predicted frame

decoder)
Output: ˆV(t)

v , ˆC(t)

rv (to previous frame buffer)

Algorithm 10 Decode predicted frame (P-decoder)
Input: J, U , ∆motion, ∆color,inter (from system parame-

ters)

(t)
v ),

code((cid:92)T∆C

(t)
rv )

(from predicted

rv (from reference frame decoder)

(from previous frame buffer)

rv , I(1)

v = ˆV(t−1)

v , ˆV(1)
, ˆC(t−1)
rv

Input: code((cid:92)T∆V
frame encoder)
v , I(1)
v

Input: ˆV(1)
Input: ˆV(t−1)
1: // Geometry
v = RAHT ( ˆV(1)
2: W(1)
v , J)
(t)
v = IRAHT ( ˆV(1)

3: (cid:100)∆V
4: ˆV(t)
+ (cid:100)∆V
v
5: ˆV(t) = ˆV(t)
v (I(1)
v )
6: // Color
rv = RAHT ( ˆV(1)
7: W(1)
rv , J)
(t)
rv = IRAHT ( ˆV(1)
rv + (cid:100)∆C
rv (I(1)
rv )

8: (cid:100)∆C
rv = ˆC(t−1)
9: ˆC(t)
r = ˆC(t)
10: ˆC(t)
Output: ˆV(t), F(1), ˆC(t)
r
Output: ˆV(t)

v , ˆC(t)

(t)
v

(t)
rv

v , (cid:92)T∆V

(t)
v , J)

rv ,(cid:92)T∆C

(t)
rv , J)

(to renderer)
rv (to previous frame buffer)

cients are entropy coded. We use adaptive RLGR as the en-
tropy coder.

At the decoder, the entropy code for the quantized trans-
form coefﬁcients of the prediction residual is received, en-
tropy decoded, inverse transformed, inverse quantized, and
added to the prediction to obtain ˆV(t)
v , which goes into the
frame buffer. Finally ˆV(t) = ˆV(t)
v ) is sent to the ren-
derer.

v (I(1)

5.2.2. Color encoding and decoding

r = ˆV(1)

r , are voxelized with attributes C(t)

At the encoder, for frame t > 1, as for frame t = 1, the reﬁned
vertices ˆV(1)
r . In this sense,
the colors C(t)
are projected back to the reference frame,
r
where they are voxelized. As for frame t = 1, this produces
a signiﬁcantly shorter list ˆV(1)
rv along with a list of indices
rv (I(1)
rv such that ˆV(1)
I(1)
rv ). In addition, it produces a
list of representative attributes, C(t)
rv . Such a list is produced
every frame. Therefore the previous frame can be used as a
rv − ˆC(t−1)
prediction. The prediction residual ∆C(t)
is transformed, quantized with stepsize ∆color,inter, inverse
transformed, and added to the prediction to obtain the repro-
duction ˆC(t)
rv , which goes into the frame buffer. The quantized
transform coefﬁcients are entropy coded. We use adaptive
RLGR as the entropy coder.

rv = C(t)

rv

At the decoder, the entropy code for the quantized trans-
form coefﬁcients of the prediction residual is received, en-
tropy decoded, inverse transformed, inverse quantized, and
added to the prediction to obtain ˆC(t)
rv , which goes into the
frame buffer. Finally ˆC(t)
rv ) is sent to the ren-
derer.

r = ˆC(t)

rv (I(1)

5.3. Rendering for visualization and distortion computa-
tion

t=1

The decompressed dynamic triangle cloud { ˆV(t), ˆC(t)
r , F(t)}N
may have varying density across triangles resulting in some
holes or transparent looking regions, which are not satis-
factory for visualization. We apply the triangle reﬁnement
function on the set of vertices and faces from Algorithm 1 and
produce the redundant representation { ˆV(t)
t=1.
This sequence consists of a dynamic point cloud { ˆV(t)
r }N
whose colored points lie in the surfaces of triangles given by
{ ˆV(t)
t=1. This representation is further reﬁned using a
similar method to increase the spatial resolution by adding a
linear interpolation function for the color attributes as shown
in Algorithm 11. The output is a denser point cloud, de-
noted by { ˆV(t)
t=1. We use this denser point cloud for
visualization and distortion computation in the experiments
described in the next section.

r }N
r , ˆC(t)

r , ˆC(t)

rr , ˆC(t)

r , F(t)

r , F(t)

r }N

rr }N

t=1,

Algorithm 11 Reﬁnement and Color Interpolation
Input: Vr, Cr, Fr, Uinterp

1: Vi = Vr(Fr(:, i), :), i = 1, 2, 3 // ith vertex of all faces
2: Ci = Cr(Fr(:, i), :), i = 1, 2, 3 // color on i-th vertex
3: Initialize Vrr = Crr = empty list
4: for i = 0 to Uinterp do
5:
6:

Vrr = [Vrr; V1 + (V2 − V1)i/Uinterp + (V3 −

for j = 0 to Uinterp − i do

V1)j/Uinterp]

7:

Crr = [Crr; C1 + (C2 − C1)i/Uinterp + (C3 −

C1)j/Uinterp]
end for

8:
9: end for

Output: Vrr, Crr

6. EXPERIMENTS

In this section, we evaluate the RD performance of our sys-
tem, for both intra-frame and inter-frame coding, for both
color and geometry, under a variety of different error metrics.
Our baseline for comparison to previous work is intra-frame
coding of colored voxels using octree coding for geometry
[23, 45, 28, 30] and RAHT coding for colors [33].

6.1. Dataset

We use triangle cloud sequences derived from the Microsoft
HoloLens Capture (HCap) mesh sequences Man, Soccer, and
Breakers1. The initial frame from each sequence is shown in
Figures 6a-c. In the HCap sequences, each frame is a trian-
gular mesh. The frames are partitioned into groups of frames
(GOFs). Within each GOF, the meshes are consistent, i.e.,
the connectivity is ﬁxed but the positions of the triangle ver-
tices evolve in time. We construct a triangle cloud from each
mesh at time t as follows. For the vertex list V(t) and face
list F(t), we use the vertex and face lists directly from the
mesh. For the color list C(t), we upsample each face by fac-
tor U = 10 to create a list of reﬁned vertices, and then sample
the mesh’s texture map at the reﬁned vertices. The geometric
data are scaled to ﬁt in the unit cube [0, 1]3. Our voxel size
is 2−J × 2−J × 2−J , where J = 10 is the maximum depth
of the octree. All sequences are 30 frames per second. The
overall statistics are described in Table 2.

6.2. Distortion metrics

Comparing algorithms for compressing colored 3D geometry
poses some challenges because there is no single agreed upon
metric or distortion measure for this type of data. Even if one
attempts to separate the photometric and geometric aspects
of distortion, there is often an interaction between the two.

1formally known as 2014 04 30 Test 4ms, 2014 11 07 Soccer Guy tra-
ditional Take4, and 2014 11 14 Breakers modern minis Take4, respectively

(a) Man

(b) Soccer

(c) Breakers

Fig. 6: Initial frames of datasets Man, Soccer, and Breakers.

Sequence
Man
Soccer
Breakers

# frm # GOF
200
493
496

7
159
156

|V|/f
11027
18187
12702

|F|/f
19978
33349
23178

voxels/f
561198
505803
411162

Table 2: Dataset statistics. Number of frames, number of
GOFs (i.e., number of reference frames), and average num-
ber of vertices and faces per reference frame, in the origi-
nal HCap datasets, and average number of occupied voxels
per frame after voxelization with respect to reference frames.
All sequences are 30 fps. For voxelization, all HCap meshes
were upsampled by a factor of U = 10, normalized to a
1 × 1 × 1 bounding cube, and then voxelized into voxels of
size 2−J × 2−J × 2−J , J = 10.

We consider several metrics for both color and geometry to
evaluate different aspects of our compression system.

6.2.1. Projection distortion

One common approach to evaluating the distortion of com-
pressed colored geometry relative to an original is to render
both the original and compressed versions of the colored ge-
ometry from a particular point of view, and compare the ren-
dered images using a standard image distortion measure such
as PSNR.

One question that arises with this approach is which view-
point, or set of viewpoints, should be used. Another question
is which method of rendering should be used. We choose
to render from six viewpoints, by voxelizing the colored ge-
ometry of the reﬁned and interpolated dynamic point cloud
{ ˆV(t)
t=1 described in Section 5.3, and projecting the

rr , ˆC(t)

rr }N

voxels onto the six faces of the bounding cube, using orthog-
onal projection. For a cube of size 2J × 2J × 2J voxels,
the voxelized object is projected onto six images each of size
2J ×2J pixels. If multiple occupied voxels project to the same
pixel on a face, then the pixel takes the color of the occupied
voxel closest to the face, i.e., hidden voxels are removed. If
no occupied voxels project to a pixel on a face, then the pixel
takes a neutral gray color. The mean squared error over the
six faces and over the sequence is reported as PSNR sepa-
rately for each color component: Y, U, and V. We call this the
projection distortion.

The projection distortion measures color distortion di-
rectly, but it also measures geometry distortion indirectly.
Thus we will report the projection distortion as a function
of the motion stepsize (∆motion) for a ﬁxed color stepsize
(∆color), and vice versa, to understand the independent ef-
fects of geometry and color compression on this measure of
quality.

6.2.2. Matching distortion

A matching distortion is a generalization of the Hausdorff dis-
tance commonly used to measure the difference between ge-
ometric objects [42]. Let S and T be source and target sets of
points, and let s ∈ S and t ∈ T denote points in the sets, with
color components (here, luminances) Y (s) and Y (t), respec-
tively. For each s ∈ S let t(s) be a point in T matched (or
assigned) to s, and likewise for each t ∈ T let s(t) be a point
in S assigned to t. The functions t(·) and s(·) need not be in-
vertible. Commonly used functions are the nearest neighbor

assignments

t∗(s) = arg min
t∈T
s∗(t) = arg min
s∈S

d2(s, t)

d2(s, t)

(3)

(4)

where d2(s, t) is a geometric distortion measure such as the
squared error d2(s, t) = ||s − t||2
2. Given matching functions
t(·) and s(·), the forward (one-way) mean squared matching
distortion has geometric and color components

d2
G(S → T ) =

d2
Y (S → T ) =

1
|S|

1
|S|

(cid:88)

s∈S
(cid:88)

s∈S

||s − t(s)||2
2

|Y (s) − Y (t(s))|2
2

(5)

(6)

while the backward mean squared matching distortion has ge-
ometric and color components

d2
G(S ← T ) =

d2
Y (S ← T ) =

1
|T |

1
|T |

(cid:88)

t∈T
(cid:88)

t∈T

||t − s(t)||2
2

|Y (t) − Y (s(t))|2
2

(7)

(8)

and the symmetric mean squared matching distortion has ge-
ometric and color components

d2
G(S, T ) = max{d2
Y (S, T ) = max{d2
d2

G(S → T ), d2
Y (S → T ), d2

G(S ← T )}
(9)
Y (S ← T )}. (10)

In the event that the sets S and T are not ﬁnite, the averages in
(5)-(8) can be replaced by integrals, e.g., (cid:82)
2dµ(s)
for an appropriate measure µ on S.

S ||s−t(s)||2

The forward, backward, and symmetric Hausdorff match-
ing distortions are similarly deﬁned, with the averages re-
placed by maxima (or integrals replaced by suprema).

Though there can be many variants on these measures,
for example using averages in (9)-(10) instead of maxima, or
using other norms or robust measures in (5)-(8), these deﬁni-
tions are consistent with those in [42] when t∗(·) and s∗(·) are
used as the matching functions. (Though we do not use them
here, matching functions other than t∗(·) and s∗(·), which
take color into account and are smoothed, such as in [40],
may yield distortion measures that are better correlated with
subjective distortion.) In this paper, for consistency with the
literature, we use the symmetric mean squared matching dis-
tortion with matching functions t∗(·) and s∗(·).

For each frame t, we compute the matching distortion
between sets S(t) and T (t), which are obtained by the sam-
pling the texture map of the original HCap data to obtain a
high resolution point cloud (V(t)
rr ) with J = 10 and
U = 40. We compare its colors and vertices to the decom-
pressed and color interpolated high resolution point cloud
( ˆV(t)
rr ) with interpolation factor Uinterp = 4 described

rr , ˆC(t)

rr , C(t)

in Section 5.32. We then voxelize both point clouds and com-
pute the mean squared matching distortion over all frames
as

¯d2
G =

¯d2
Y =

1
N

1
N

N
(cid:88)

t=1

N
(cid:88)

t=1

G(S(t), T (t))
d2

Y (S(t), T (t))
d2

(11)

(12)

and we report the geometry and color components of the
matching distortion in dB as

P SN RG = −10 log10

P SN RY = −10 log10

¯d2
G
3W 2
¯d2
Y
2552

(13)

(14)

(15)

where W = 1 is the width of the bounding cube.

Note that even though the geometry and color components
of the distortion measure are separate, there is an interac-
tion: The geometry affects the matching, and hence affects
the color distortion. Thus we will report the color compo-
nent of the matching distortion as a function of the color step-
size (∆color) for a ﬁxed motion stepsize (∆motion), and vice
versa, to understand the independent effects of geometry and
color compression on color quality. We report the geometry
component of the matching distortion as a function only of
the motion stepsize (∆motion), since color compression does
not affect the geometry under the assumed matching functions
t∗(·) and s∗(·).

6.2.3. Triangle cloud distortion

In our setting, the input and output of our system are the tri-
angle clouds (V(t), F(t), C(t)) and ( ˆV(t), F(t), ˆC(t)). Thus
natural measures of distortion for our system are

P SN RG = −10 log10

(cid:32)

1
N

P SN RY = −10 log10

(cid:32)

1
N

N
(cid:88)

||V(t)

r ||2
2

r

r − ˆV(t)
3W 2N (t)
r − ˆY(t)
2552N (t)

r

r ||2
2

(cid:33)

(16)

(cid:33)

,

(17)

||Y(t)

t=1

N
(cid:88)

t=1

is the ﬁrst (i.e, luminence) column of the N (t)

where Y(t)
r × 3
r
matrix of color attributes C(t)
and W = 1 is the width of
r
the bounding cube. These represent the average geometric
and luminance distortions across the faces of the triangles.
P SN RU and P SN RV can be similarly deﬁned.

2Note that the original triangle cloud was obtained by sampling the HCap
data with upsampling factor U = 10. Thus by interpolating the decom-
pressed triangle cloud with Uinterp = 4, the overall number of vertices and
triangles is the same as obtained by sampling the original HCap data with
upsampling factor U = 40.

However for rendering we use higher resolution versions
of the triangles, in which both the vertices and the colors are
interpolated up from V(t)
r using Algorithm 11 to ob-
tain higher resolution vertices and colors V(t)
rr . We
use the following distortion measures as very close approxi-
mations of (16) and (17):

rr and C(t)

r and C(t)

P SN RG = −10 log10

(cid:32)

1
N

P SN RY = −10 log10

(cid:32)

1
N

N
(cid:88)

||V(t)

rr ||2
2

rr − ˆV(t)
3W 2N (t)
rr
rr − ˆY(t)
2552N (t)
rr

rr ||2
2

(cid:33)

(18)

(cid:33)

,

(19)

||Y(t)

t=1

N
(cid:88)

t=1

rr is the ﬁrst (i.e, luminence) column of the N (t)

where Y(t)
rr × 3
matrix of color attributes C(t)
rr and W = 1 is the width of
the bounding cube. P SN RU and P SN RV can be similarly
deﬁned.

6.2.4. Transform coding distortion

For the purposes of rate-distortion optimization, and other
rapid distortion computations, it is more convenient to use an
internal distortion measure: the distortion between the input
and output of the tranform coder. We call this the transform
coding distortion, deﬁned in dB as

P SN RG = −10 log10

(cid:32)

1
N

P SN RY = −10 log10

(cid:32)

1
N

N
(cid:88)

||V(t)

v ||2
2

v − ˆV(t)
3W 2N (t)
v
rv − ˆY(t)
2552N (t)
rv

rv ||2
2

(cid:33)

(20)

(cid:33)

,

(21)

||Y(t)

t=1

N
(cid:88)

t=1

rv is the ﬁrst (i.e, luminence) column of the N (t)
where Y(t)
rv × 3
matrix C(t)
rv . P SN RU and P SN RV can be similarly de-
ﬁned. Unlike (18)-(19), which are based on system inputs
and outputs V(t), C(t) and ˆV(t), ˆC(t), (20)-(21) are based on
v , C(t)
the voxelized quantities V(t)
rv , which are
deﬁned for reference frames in Algorithm 7 (Steps 3, 6, and
9) and for predicted frames in Algorithm 9 (Steps 2, 7, 9,
and 14). The squared errors in the two cases are essentially
the same, but are weighted differently: one by face and one
by voxel.

rv and ˆV(t)

v , ˆC(t)

6.3. Rate metrics

As with the distortion, we report bit rates for compression of
a whole sequence, for geometry and color. We compute the
bit rate averaged over a sequence, in megabits per second, as

RM bps =

bits
10242N

30 [Mbps]

(22)

information of the sequence. Also, we report the bit rate in
bits per voxel as

Rbpv =

bits
t=1 N (t)

rv

(cid:80)N

[bpv]

(23)

where N (t)
rv is the number of occupied voxels in frame t and
again bits is the total number of bits used to encode color or
geometry for the whole sequence. The number of voxels of a
given frame N (t)
rv depends on the voxelization used. For ex-
ample in our triangle cloud encoder, within a GOF all frames
have the same number of voxels, because the voxelization of
attributes is done with respect to the reference frame. For our
triangle encoder in all intra mode, each frame will have a dif-
ferent number of voxels.

6.4. Intra-frame coding

We ﬁrst examine intra-frame coding of triangle clouds, and
compare it to intra-frame coding of voxelized point clouds.
To obtain the voxelized point clouds, we voxelize the original
mesh-based sequences Man, Soccer, and Breakers by reﬁning
each face in the original sequence by upsampling factor U =
10, and voxelizing to level J = 10. For each sequence, and
each frame t, this produces a list of occupied voxels V(t)
rv and
their colors Crv.

6.4.1. Intra-frame coding of geometry

We compare our method for coding geometry in reference
frames with the previous state-of-the-art for coding geome-
try in single frames. The previous state-of-the art for coding
the geometry of voxelized point clouds [23, 45, 28, 30] codes
the set of occupied voxels V(t)
rv by entropy coding the octree
description of the set. In contrast, our method ﬁrst approx-
imates the set of occupied voxels by a set of triangles, and
then codes the triangles as a triple (V(t)
v ). The ver-
tices V(t)
v are coded using octrees plus gzip, the faces F(t) are
coded directly with gzip, and the indices I(t)
v are coded using
run-length encoding plus gzip as described in Section 5.1.1.
When the geometry is smooth, relatively few triangles need
to be used to approximate it. In such cases, our method gains
because the list of vertices V(t)
v is much shorter than the list of
occupied voxels V(t)
rv , even though the list of triangle indices
F(t) and the list of repeated indices I(t)

v must also be coded.

v , F(t), I(t)

Taking all bits into account, Table 3 shows the bit rates for
both methods in megabits per second (Mbps) and bits per oc-
cupied voxel (bpv) averaged over the sequences. Our method
reduces the bit rate needed for intra-frame coding of geome-
try by a factor of 5-10, breaking through the 2.5 bpv rule-of-
thumb for octree coding.

where N is the number of frames in the sequence, and bits is
the total number of bits used to encode the color or geometry

While it is true that approximating the geometry by trian-
gles is generally not lossless, in this case the process is loss-

Previous

Ours

Sequence Mbps
50.7
37.6
43.7

Man
Soccer
Breakers

bpv Mbps
5.24
3.20
6.39
2.61
4.88
3.28

bpv
0.33
0.44
0.36

Table 3: Intra-frame coding of the geometry of voxelized
point clouds. “Previous” refers to our implementation of the
octree coding approach described in [23, 45, 28, 30].

less because our ground truth datasets are already described
in terms of triangles.

6.4.2. Intra-frame coding of color

Our method of coding color in reference frames is identical
with the state-of-the art for coding color in single frames, us-
ing transform coding based on RAHT, described in [33]. For
reference, the rate-distortion results for color intra-frame cod-
ing are shown in Figure 13 (where we compare to color inter-
frame coding).

6.5. Inter/intra-frame coding: transform coding distor-
tion rate curves

We next examine hybrid inter-frame plus intra-frame coding
(here called inter/intra-frame coding) of triangle clouds using
the transform coding distortion, and compare it to intra-frame
only coding of triangle clouds. We show that temporal pre-
diction provides substantial gains for geometry across all se-
quences, and signiﬁcant gains for color on one of the three
sequences.

6.5.1. Inter/intra-frame coding of geometry

Figure 7 shows the geometry transform coding distortion
P SN RG (20) as a function of the bit rate needed for geom-
etry information in inter/intra-frame coding of the sequences
Man, Soccer, and Breakers. It can be seen that the geometry
PSNR saturates, at relatively low bit rates, at the highest ﬁ-
delity possible for a given voxel size 2−J , which is 71 dB for
J = 10. In Figure 8 we show on the Breakers sequence that
quality within 0.5 dB of this limit appears to be sufﬁciently
close to that of the original voxelization without quantization.
At this quality, for Man, Soccer, and Breakers sequences,
the encoder in inter/intra (hybrid) mode has geometry bit
rates of about 1.2, 2.7, and 2.2 Mbps (0.07, 0.19, 0.17 bpv),
respectively. For comparison, the encoder in all-intra mode
has geometry bit rates of 5.24, 6.39, and 4.88 Mbps (0.33,
0.44, 0.36 bpv), respectively, as shown in Table 3. Thus the
intra-inter mode has a geometry bit rate savings of a factor of
2-5 over our intra-frame coding only, and a factor of 13-45
over previous intra-frame octree coding.

A temporal analysis is provided in Figures 9 and 10. Fig-
ure 9 shows the number of kilobits per frame needed to en-
code the geometry information for each frame. The number
of bits for the reference frames are dominated by their oc-
tree descriptions, while the number of bits for the predicted
frames depends on the quantization stepsize for motion resid-
uals, ∆motion. We observe that a signiﬁcant bit reduction can
be achieved by lossy coding of residuals. For ∆motion = 4,
there is more than a 3x reduction in bit rate for inter-frame
coding relative to intra-frame coding.

Figure 10 shows the mean squared quantization error

M SEG =

1
N

N
(cid:88)

t=1

||V(t)

v ||2
2

v − ˆV(t)
3W 2N (t)
v

,

(24)

which corresponds to the P SN RG in (20). Note that for ref-
erence frames, the mean squared error is well approximated
by

||V(1)

v ||2
2

v − ˆV(1)
3W 2N (t)
v

≈

2−2J
12

∆= (cid:15)2.

(25)

Thus for reference frames, the M SEG falls to (cid:15)2, while for
predicted frames, the M SEG rises from (cid:15)2 depending on the
motion stepsize ∆motion.

6.5.2. Intra/inter-frame coding of color

To evaluate color coding, ﬁrst we consider separate quantiza-
tion stepsizes for reference and predicted frames ∆color,intra
and ∆color,inter respectively. Both take values in {1, 2, 4, 8,
16, 32, 64}.

Figures 11 and 12 shows the color transform coding dis-
tortion P SN RY (21) as a function of the bit rate (Mbps and
bpv respectively) needed for all (Y , U , V ) color information
for inter/intra-frame coding of the sequences Man, Soccer,
and Breakers, for different combinations of ∆color,intra and
∆color,inter, where each colored curve corresponds to a ﬁxed
value of ∆color,intra. It can be seen that the optimal RD curve
is obtained by choosing ∆color,intra = ∆color,inter, as shown
in the dashed line.

Next, we consider equal quantization stepsizes for ref-
erence and predicted frames, hereafter designated simply
∆color.

Figure 13 shows the color transform coding distortion
P SN RY (21) as a function of the bit rate needed for all
(Y , U , V ) color information for inter/intra-frame coding and
intra-frame only coding on the sequences Man, Soccer, and
Breakers. We observe that inter/intra-frame coding outper-
forms intra-frame only coding by 2-3 dB for the Breakers
sequence. However, for the Man and Soccer sequences, their
RD performances are similar. Further investigation is needed
on when and how gains can be achieved by predictive coding
of color.

A temporal analysis is provided in Figures 14 and 15.
In Figure 14 we show the bit rates (Kbit) to compress the

color information for the ﬁrst 100 frames of all sequences.
We observe that, as expected, for smaller values of ∆color
the bit rates are higher, for all frames. For Man and Soc-
cer sequences we observe that the bit rates do not vary much
from reference frames to predicted frames; however in the
Breakers sequence, it is clear that for all values of ∆color
the reference frames have much higher bit rates compared to
predicted frames, which conﬁrms the results from Figure 13,
where inter/intra-frame coding provides gains with respect to
intra-frame only coding of triangle clouds for the Breakers
sequence, but not for the Man and Soccer sequences. In Fig-
ure 15 we show the MSE of the Y color component for the
ﬁrst 100 frames of all sequences. For ∆color ≤ 4 the error is
uniform across all frames and sequences.

6.5.3. Comparison to dynamic mesh compression

We now compare our results to the dynamic mesh compres-
sion in [21], which uses a distortion measure similar to the
transform coding distortion measure, and reports results on a
version of the Man sequence.

For geometry coding, Figure 5 in [21] shows that when
their geometry distortion is 70.5 dB, their geometry bit rate is
about 0.45 bpv. As shown in Figure 7, at the same distortion,
our bit rate is about 0.07 bpv, which is lower than their bit rate
by a factor of 6x or more.

For color coding, Figure 5 in [21] shows that when their
color distortion is 40 dB, their color bit rate is about 0.8 bpv.
As shown in Figure 13, at the same distortion, our bit rate is
about 1.8 bpv.

Overall, their bit rate would be about 0.45 + 0.8 = 1.3
bpv, while our bit rate would be about 0.07 + 1.8 = 1.9 bpv.
However it should be cautioned that the sequence compressed
in [21] is not the original Man sequence used in our work
but rather a smooth mesh ﬁt to a low-resolution voxelization
(J = 9) of the sequence. Hence it has smoother color as well
as smoother geometry, and should be easier to code. Never-
theless, it is a point of comparison.

6.6. Inter/intra-frame coding: triangle cloud, projection,
and matching distortion-rate curves

In this section we show distortion rate curves using the tri-
angle cloud, projection, and matching distortion measures.
All distortions in this section are computed from high reso-
lution triangle clouds generated from the original HCap data,
and from the decompressed triangle clouds. For computa-
tional complexity reasons, we show results only for the Man
sequence, and consider only its ﬁrst four GOFs (120 frames).

6.6.1. Geometry coding

First we analyze the triangle cloud distortion and matching
distortion of geometry as a function of geometry bit rate. The

RD plots are shown in Figure 16. We observe that both dis-
tortion measures start saturating at the same point as for the
transform coding distortion: around ∆motion = 4. How-
ever for these distortion measures the saturation is not as pro-
nounced. This suggest that these distortion measures are quite
sensitive to small amounts of geometric distortion.

Next we study the effect of geometry compression on
color quality. In Figure 17 we show the Y component PSNR
for the projection and matching distortion measures. The
color has been compressed at the highest bit rate considered,
using the same quantization step for intra and inter color
coding, ∆color = 1. Surprisingly, we observe a signiﬁcant
inﬂuence of the geometry compression on these color distor-
tion measures, particular for ∆motion > 4. This indicates
very high sensitivity to geometric distortion of the projection
distortion measure and the color component of the matching
distortion measure. This hyper-sensitivity can be explained
as follows. For the projection distortion measure, geometric
distortion causes local shifts of the image. As is well known,
PSNR, as well as other image distortion measures including
SSIM, fall apart upon image shifts. For the matching met-
ric, since the matching functions s∗ and t∗ depend only on
geometry, geometric distortion causes inappropriate matches,
which affect the color distortion across those matches.

6.6.2. Color coding

Finally we analyze the RD curve for color coding as a func-
tion of color bit rate. We plot Y component PSNR for the
triangle cloud, projection, and matching distortion measures
in Figure 18. For this experiment we consider the color quan-
tization steps equal for intra and inter coded frames. The mo-
tion step is set to ∆motion = 1. For all three distortion mea-
sures, the PSNR saturates very quickly. Apparently, this is
because the geometry quality severely limits the color quality
under any of the these three distortion measures, even when
the geometry quality is high (∆motion = 1). In particular,
when ∆motion = 1, for color quantization stepsizes smaller
than ∆color = 8, color quality does not improve signiﬁcantly
under these distortion measures, while under the transform
coding distortion measure, the PSNR continues to improve,
as shown in Figures 11 and 12. Whether the hyper-sensitivity
of the color projection and color matching distortion measures
to geometric distortion are perceptually justiﬁed is question-
able, but open to further investigation.

6.6.3. Comparison to dynamic point cloud compression

Notwithstanding possible issues with the color projection dis-
tortion measure, it provides an opportunity to compare our
results on dynamic triangle cloud compression to the results
on dynamic point cloud compression in [40]. Like us, [40]
reports results on a version of the Man sequence, using the
projection distortion measure.

Figure 12 shows that for triangle cloud compression, the
projection distortion reaches 38.5 dB at around 2 bpv.
In
comparison, Figure 10a in [40] shows that for dynamic point
cloud compression, the projection distortion reaches 38.5 dB
at around 3 bpv. Hence it seems that our dynamic triangle
cloud compression may be more efﬁcient than point cloud
compression under the projection distortion measure. How-
ever it should be cautioned that the sequence compressed in
[40] is a lower resolution (J = 9) version of the Man se-
quence rather than the higher resolution version (J = 10)
used in our work. Moreover, Figure 12 in our paper reports
the distortion between the original signal (with uncoded color
and uncoded geometry) to the coded signal (with coded color
and coded geometry), while Figure 10a in [40] reports the
distortion between the signal with uncoded color and coded
geometry to the signal with coded color and identically coded
geometry. In the latter case, the saturation of the color mea-
sure due to geometric coding is not apparent.

7. CONCLUSION

When coding for video, the representation of the input to the
encoder and the representation of the output of the decoder are
clear: sequences of rectangular arrays of pixels. Furthermore,
distortion measures between the two representations are well
accepted in practice.

In contrast, when coding for augmented reality, as of yet,
the representation of the input to the encoder and the represen-
tation of the output of the decoder are not yet widely agreed
upon in the research community. This is because the there
are many types of sensing scenarios and rigs, each requiring a
different process for fusing raw camera data into the encoder
input representation. Likewise, there are many varieties of
display scenarios and devices, each requiring a different pro-
cess for rendering the decoder output representation. Natu-
rally, distortion measures between any such representations
are also not yet widely agreed upon.

Two leading candidates for the codec’s representation for
augmented reality to this point have been dynamic meshes
and dynamic point clouds. Each has its advantages and disad-
vantages. Dynamic meshes ﬁt well into the traditional graphic
pipeline and have high compression efﬁciency. However, they
do not accommodate well the noise and non-surface topolo-
gies typically present in real time live capture. Conversely,
dynamic point clouds are well-suited for representing noise
and non-surface topologies, but are difﬁcult to interpolate in
space and time, making them difﬁcult to compress by exploit-
ing spatial and temporal redundancies.

In this paper, we proposed dynamic polygon clouds,
which have the advantages of both meshes and point clouds,
without their disadvantages. We provided detailed algorithms
on how to compress them, and we used a variety of distortion
measures to evaluate their performance.

For intra-frame coding of geometry, we showed that com-

pared to the previous state-of-the-art for intra-frame coding
of the geometry of voxelized point clouds, our method re-
duces the bit rate by a factor of 5-10 with negligible (but non-
zero) distortion, breaking through the 2.5 bpv rule-of-thumb
for lossless coding of geometry in voxelized point clouds. In-
tuitively, these gains are achieved by reducing the representa-
tion from a dense list of points to a less dense list of vertices
and faces.

For inter-frame coding of geometry, we showed that com-
pared to our method of intra-frame coding of geometry, we
can reduce the bit rate by a factor of 3 or more. For inter/intra-
frame (hybrid) coding, this results in a geometry bit rate sav-
ings of a factor of 2-5 over intra-frame coding only.
Intu-
itively, these gains are achieved by coding the motion predic-
tion residuals. Multiplied by the 5-10 x improvement of our
intra-frame coding compared to previous octree-based intra-
frame coding, we have demonstrated a 13-45 x reduction in
bit rate over previous octree-based intra-frame coding.

For inter-frame coding of color, we showed that compared
to our method of intra-frame coding of color (which is the
same as the current state-of-the-art for intra-frame coding of
color [33]), our method reduces the bit rate by about 30% or
alternatively increases the PSNR by about 2 dB (at the rele-
vant level of quality) for one of our three sequences. For the
other two sequences, we found little improvement in perfor-
mance relative to intra-frame coding of color. This is a matter
for further investigation, but one hypothesis is that the gain
is dependent upon the quality of the motion estimation. In-
tuitively, gains are achieved by coding the color prediction
residuals, and the color prediction is accurate only if the mo-
tion estimation is accurate.

We compared our results on triangle cloud compression
to recent results in dynamic mesh compression and dyanmic
point cloud compression. The comparisons are imperfect due
to somewhat different datasets and distortion measures, which
likely favor the earlier work. However, they indicate that com-
pared to dynamic mesh compression, our geometry coding
may have a bit rate 6x lower, while our color coding may
have a bit rate 2.25x higher. At the same time, compared to
dynamic point cloud compression, our overall bit rate may be
about 33% lower.

Our work also revealed the hyper-sensitivity of distortion
measures such as the color projection and color matching dis-
tortion measures to geometry coding.

Future work includes better transforms and better en-
tropy coders, RD optimization, better motion compensation,
and more perceptually relevant distortion measures and post-
processing ﬁltering.

8. ACKNOWLEDGMENT

The authors would like to thank the Microsoft HoloLens Cap-
ture (HCap) team for making their data available to this re-
search, and would also like to thank the Microsoft Research

Interactive 3D (I3D) team for many discussions.

9. REFERENCES

[1] P. Alliez and C. Gotsman, “Recent advances in com-
pression of 3d meshes,” in Advances in Multiresolution
for Geometric Modeling, N. A. Dodgson, M. S. Floater,
and M. A. Sabin, Eds., pp. 3–26. Springer Berlin Hei-
delberg, Berlin, Heidelberg, 2005.

[2] J. Peng, Chang-Su Kim, and C. C. Jay Kuo, “Technolo-
gies for 3d mesh compression: A survey,” Journal of
Vis. Comun. and Image Represent., vol. 16, no. 6, pp.
688–733, Dec. 2005.

[3] A. Maglo, G. Lavou´e, F. Dupont, and C. Hudelot, “3d
mesh compression: survey, comparisons and emerging
trends,” ACM Computing Surveys, vol. 9, no. 4, 2013.

[4] J. Rossignac, “Edgebreaker: Connectivity compression
IEEE Trans. Visualization and
for triangle meshes,”
Computer Graphics, vol. 5, no. 1, pp. 47–61, Jan. 1999.

[5] K. Mamou, T. Zaharia, and F. Prˆeteux, “TFAN: A low
complexity 3d mesh compression algorithm,” Computer
Animation and Virtual Worlds, vol. 20, 2009.

[6] Xianfeng Gu, Steven J. Gortler, and Hugues Hoppe,
ACM Trans. Graphics (SIG-

“Geometry images,”
GRAPH), vol. 21, no. 3, pp. 355–361, July 2002.

[7] H. Brice˜no, P. Sander, L. McMillan, S. Gortler, and
H. Hoppe, “Geometry videos: a new representation for
3d animations,” in Symp. Computer Animation, 2003.

[8] A. Collet, M. Chuang, P. Sweeney, D. Gillett, D. Evseev,
D. Calabrese, H. Hoppe, A. Kirk, and S. Sullivan,
“High-quality streamable free-viewpoint video,” ACM
Trans. Graphics (SIGGRAPH), vol. 34, no. 4, pp. 69:1–
69:13, July 2015.

[9] R. Mekuria, M. Sanna, E. Izquierdo, D. C. A. Bulter-
man, and P. Cesar, “Enabling geometry-based 3-d tele-
immersion with fast mesh compression and linear rate-
less coding,” IEEE Transactions on Multimedia, vol. 16,
no. 7, pp. 1809–1820, Nov 2014.

[10] A. Doumanoglou, D. S. Alexiadis, D. Zarpalas, and
P. Daras, “Toward real-time and efﬁcient compression
of human time-varying meshes,” IEEE Transactions on
Circuits and Systems for Video Technology, vol. 24, no.
12, pp. 2099–2116, Dec 2014.

[11] R. A. Newcombe, D. Fox, and S. M. Seitz, “Dynamic-
fusion: Reconstruction and tracking of non-rigid scenes
in real-time,” in 2015 IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2015, pp.
343–352.

[12] M. Dou, J. Taylor, H. Fuchs, A. Fitzgibbon, and S. Izadi,
“3d scanning deformable objects with a single rgbd sen-
sor,” in 2015 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2015, pp. 493–501.

[13] M. Dou, S. Khamis, Y. Degtyarev, P. Davidson, S. R.
Fanello, A. Kowdle, S. Orts Escolano, C. Rhemann,
D. Kim, J. Taylor, P. Kohli, V. Tankovich, and S. Izadi,
“Fusion4d: real-time performance capture of challeng-
ing scenes,” ACM Transactions on Graphics (TOG), vol.
35, no. 4, pp. 114, 2016.

[14] J. Hou, L. P. Chau, N. Magnenat-Thalmann, and Y. He,
“Human motion capture data tailored transform cod-
ing,” IEEE Transactions on Visualization and Computer
Graphics, vol. 21, no. 7, pp. 848–859, July 2015.

[23] D. Meagher, “Geometric modeling using octree encod-
ing,” Computer Graphics and Image Processing, vol.
19, no. 2, pp. 129 – 147, 1982.

[24] C. Loop, C. Zhang, and Z. Zhang,

“Real-time
high-resolution sparse voxelization with application to
in Proc. of the 5th High-
image-based modeling,”
Performance Graphics Conference, New York, NY,
USA, 2013, pp. 73–79.

[25] H. P. Moravec, “Sensor fusion in certainty grids for mo-
bile robots,” AI Magazine, vol. 9, no. 2, pp. 61–74, 1988.

[26] A. Elfes, “Using occupancy grids for mobile robot per-
ception and navigation,” IEEE Computer, vol. 22, no. 6,
pp. 46–57, 1989.

[15] J. Hou, L.-P. Chau, N. Magnenat-Thalmann, and Y. He,
“Low-latency compression of mocap data using learned
spatial decorrelation transform,” Comput. Aided Geom.
Des., vol. 43, no. C, pp. 211–225, Mar. 2016.

[27] K. Pathak, A. Birk, J. Poppinga, and S. Schwertfeger,
“3d forward sensor modeling and application to occu-
pancy grid based sensor fusion,” in Proc. IEEE/RSJ Int’l
Conf. Intelligent Robots and Systems (IROS), Oct. 2007.

[16] A. Sandryhaila and J. M. F. Moura, “Discrete signal
processing on graphs,” IEEE Transactions on Signal
Processing, vol. 61, no. 7, pp. 1644–1656, April 2013.

[28] R. Schnabel and R. Klein, “Octree-based point-cloud
compression,” in Eurographics Symp. on Point-Based
Graphics, July 2006.

[17] D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega,
and P. Vandergheynst, “The emerging ﬁeld of signal
processing on graphs: Extending high-dimensional data
analysis to networks and other irregular domains,” IEEE
Signal Process. Mag., vol. 30, no. 3, pp. 83–98, May
2013.

[18] S. K. Narang and A. Ortega, “Perfect reconstruction
two-channel wavelet ﬁlter banks for graph structured
data,” IEEE Transactions on Signal Processing, vol. 60,
no. 6, pp. 2786–2799, June 2012.

[19] S. K. Narang and A. Ortega,

“Compact support
biorthogonal wavelet ﬁlterbanks for arbitrary undirected
graphs,” IEEE Transactions on Signal Processing, vol.
61, no. 19, pp. 4673–4685, Oct 2013.

[20] H. Q. Nguyen, P. A. Chou, and Y. Chen, “Compres-
sion of human body sequences using graph wavelet ﬁl-
in 2014 IEEE International Conference
ter banks,”
on Acoustics, Speech and Signal Processing (ICASSP),
May 2014, pp. 6152–6156.

[21] A. Anis, P. A. Chou, and A. Ortega,

“Compression
of dynamic 3d point clouds using subdivisional meshes
and graph wavelet transforms,” in 2016 IEEE Interna-
tional Conference on Acoustics, Speech and Signal Pro-
cessing (ICASSP), March 2016, pp. 6360–6364.

[29] Y. Huang, J. Peng, C. C. J. Kuo, and M. Gopi,

“A
generic scheme for progressive point cloud coding.,”
IEEE Trans. Vis. Comput. Graph., vol. 14, no. 2, pp.
440–453, 2008.

[30] J. Kammerl, N. Blodow, R. B. Rusu, S. Gedikli,
M. Beetz, and E. Steinbach, “Real-time compression
of point cloud streams,” in IEEE Int. Conference on
Robotics and Automation, Minnesota, USA, May 2012.

[31] R. B. Rusu and S. Cousins, “3d is here: Point cloud
library (PCL),” in In Robotics and Automation (ICRA),
2011 IEEE International Conference on. pp. 1–4, IEEE.

[32] C. Zhang, D. Florˆencio, and C. Loop,

“Point cloud
attribute compression with graph transform,” in 2014
IEEE International Conference on Image Processing
(ICIP), Oct 2014, pp. 2066–2070.

[33] R. L. de Queiroz and P. A. Chou, “Compression of 3d
point clouds using a region-adaptive hierarchical trans-
form,” IEEE Transactions on Image Processing, vol. 25,
no. 8, pp. 3947–3956, Aug 2016.

[34] R. A. Cohen, D. Tian, and A. Vetro, “Attribute compres-
sion for sparse point clouds using graph transforms,” in
2016 IEEE International Conference on Image Process-
ing (ICIP), Sept 2016, pp. 1374–1378.

[22] C. L. Jackins and S. L. Tanimoto, “Oct-trees and their
use in representing three-dimensional objects,” Com-
puter Graphics and Image Processing, vol. 14, no. 3,
pp. 249 – 270, 1980.

[35] B. Dado, T. R. Kol, P. Bauszat, J.-M. Thiery, and
E. Eisemann, “Geometry and Attribute Compression
for Voxel Scenes,” Eurographics Computer Graphics
Forum, 2016.

[36] R. L. de Queiroz and P. A. Chou, “Transform coding
for point clouds using a Gaussian process model,” IEEE
Trans. Image Processing, 2016, submitted.

[37] J. Hou, L.-P. Chau, Y. He, and P. A. Chou, “Sparse rep-
resentation for colors of 3d point cloud via virtual adap-
tive sampling,” in 2017 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP),
2017, to appear.

[38] D. Thanou, P. A. Chou, and P. Frossard, “Graph-based
motion estimation and compensation for dynamic 3d
point cloud compression,” in Image Processing (ICIP),
2015 IEEE International Conference on, Sept 2015, pp.
3235–3239.

[39] D. Thanou, P. A. Chou, and P. Frossard, “Graph-based
compression of dynamic 3d point cloud sequences,”
IEEE Transactions on Image Processing, vol. 25, no. 4,
pp. 1765–1778, April 2016.

[40] R. L. de Queiroz and P. A. Chou, “Motion-compensated
compression of dynamic voxelized point clouds,” IEEE
Trans. Image Processing, 2016, submitted.

[41] R. Mekuria, K. Blom, and P. Cesar, “Design, implemen-
tation and evaluation of a point cloud codec for tele-
immersive video,” IEEE Transactions on Circuits and
Systems for Video Technology, vol. PP, no. 99, pp. 1–1,
2016.

[42] R. Mekuria, Z. Li, C. Tulvan, and P. Chou, “Evalua-
tion criteria for pcc (point cloud compression),” output
document n16332, ISO/IEC JTC1/SC29/WG11 MPEG,
May 2016.

[43] H. S. Malvar, “Adaptive run-length/Golomb-Rice en-
coding of quantized generalized gaussian sources with
unknown statistics,” in Data Compression Conference
(DCC’06), March 2006, pp. 23–32.

[44] G. M Morton, “A computer oriented geodetic data base;
and a new technique in ﬁle sequencing,” Technical re-
port, IBM, Ottawa, Canada, 1966.

[45] T. Ochotta and D. Saupe,

“Compression of Point-
Based 3D Models by Shape-Adaptive Wavelet Coding
in Proc. of the First Euro-
of Multi-Height Fields,”
graphics Conference on Point-Based Graphics, 2004,
pp. 103–112.

(a) RD curves for motion compression.

(b) RD curves for motion compression.

Fig. 7: RD curves for geometry compression. Rates include
all geometry information.

(a) original

(b) 62 dB (1.6 Mbps for all geometry information)

(c) 70.5 dB (2.2 Mbps for all geometry information)

Fig. 8: Visual quality of geometry compression.

(a) Man

(b) Soccer

(c) Breaker

Fig. 9: Kilobits/frame required to code the geometry information for each frame for different values of the motion residual
using octree coding plus gzip and encode I(1)
quantization stepsize ∆motion ∈ {1, 2, 4, 8}. Reference frames encode V(1)
v
v
using run-length coding plus gzip. Predicted frames encode their motion residuals ∆V(t) using transform coding.

(a) Man

(b) Soccer

(c) Breaker

Fig. 10: Mean squared quantization error required to code the geometry information for each frame for different values of the
motion residual quantization stepsize ∆motion ∈ {1, 2, 4, 8}. Reference frames encode V(1)
v using octrees; hence the distortion
is due to quantization error is (cid:15)2. Predicted frames encode their motion residuals ∆V(t) using transform coding.

Fig. 11: Luminance (Y) component rate-distortion perfor-
mances of (top) Man, (middle) Soccer and (bottom) Break-
ers sequences, for different intra-frame stepsizes ∆color,intra.
Rate includes all (Y , U , V ) color information.

Fig. 12: Luminance (Y) component rate-distortion perfor-
mances of (top) Man, (middle) Soccer and (bottom) Break-
ers sequences, for different intra-frame stepsizes ∆color,intra.
Same as Figure 11 but rate is in bits-per-voxel.

Fig. 13: Inter/intra-frame coding vs. intra-frame only cod-
ing. The bit rate contains all (Y , U , V ) color information,
although the distortion is only the luminance (Y ) PSNR.

(a) Man

(b) Soccer

(c) Breaker

Fig. 14: Kilobits/frame required to code the color information for each frame for different values of the color residual quantiza-
tion stepsize ∆color ∈ {1, 2, 4, 8}. Reference frames encode their colors C(1)
rv and predicted frames encode their color residuals
∆C(t)

rv using transform coding.

(a) Man

(b) Soccer

(c) Breaker

Fig. 15: Mean squared quantization error required to code the color information for each frame for different values of the color
residual quantization stepsize ∆color ∈ {1, 2, 4, 8}. Reference frames encode their colors C(1)
rv and predicted frames encode
their color residuals ∆C(t)

rv using transform coding.

(a) Geometry distortion vs geometry bit rate [Mbps]

(a) Color distortion vs geometry bit rate [Mbps]

(b) Geometry distortion vs geometry bit rate [bpv]

(b) Color distortion vs geometry bit rate [bpv]

Fig. 16: RD curves for geometry triangle cloud and matching
distortion vs. geometry bit rates.

Fig. 17: RD curves for color triangle cloud and matching dis-
tortion vs. geometry bit rates. The color stepsize is set to
∆color = 1.

(a) Color distortion vs color bit rate [Mbps]

(b) Color distortion vs color bit rate [bpv]

Fig. 18: RD curves for color triangle cloud, projection, and
matching distortion vs color bit rates. The motion stepsize is
set to ∆motion = 1.

