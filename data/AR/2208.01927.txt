MOMENT ESTIMATOR FOR AN AR(1) MODEL WITH NON-ZERO MEAN
DRIVEN BY A LONG MEMORY GAUSSIAN NOISE

YANPING LU

Abstract. In this paper, we consider an inference problem for the ﬁrst order autoregressive
process with non-zero mean driven by a long memory stationary Gaussian process. Suppose
that the covariance function of the noise can be expressed as |k|2H−2 times a positive constant
when k tends to inﬁnity, and the fractional Gaussian noise and the fractional ARIMA model
are special examples that satisfy this assumption. We propose moment estimators and prove
the strong consistency, the asymptotic normality and joint asymptotic normality.

Keywords: Gaussian process; Fourth moment theorems; Asymptotic normality.

1. Introduction

For the ﬁrst order autoregressive model (Xt, t ∈ N) driven by a given noise sequence ξ =

(ξt, t ∈ Z):

Xt = α + θXt−1 + ξt,

t ∈ N

(1.1)

with X0 = 0, the inference problem regarding the parameter θ has been extensively studied
in probability and statistics literatures. For when ξ is independent identical distribution or
a martingale diﬀerence sequence, this problem has been widely studied over the past decades
(see [1], [2] and the references therein). Parameter estimation of AR(1) model driven by i.i.d.
sequence with non-zero mean was considered in [3].

The maximum likelihood estimator of AR(p) was investigated in [5] for regular stationary
Gaussian noise, in which they transform the observation model into an "equivalent" model with
Gaussian white noise. In [5], it is pointed out that for the strongly dependent noises the least
square estimator is generally not consistent. In case of long-memory noise, the detection of a
change of the above parameter θ is studied by means of the likelihood ratio test [6]. [7] consider
second moment estimator of θ in AR(1) model with zero mean driven by a long memory Gaussian
noise.

In this paper, we will discuss the long-range dependence Gaussian noise case and propose
moment estimators of θ and α. First, we ﬁnd that it is very convenient to construct moment
estimators when we restrict the domain of the parameter θ in

Θ = {θ ∈ R | 0 < θ < 1}.

It seems that this restriction is very reasonable for real-world context sometimes. In fact, |θ| < 1
is an assumption to ensure the model (1.1) to have a stationary solution. We rule out the case
1

2
2
0
2

g
u
A
3

]
T
S
.
h
t
a
m

[

1
v
7
2
9
1
0
.
8
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
2MOMENT ESTIMATOR FOR AN AR(1) MODEL WITH NON-ZERO MEAN DRIVEN BY A LONG MEMORY GAUSSIAN NOISE

of −1 < θ < 0 in which the series tends to oscillate rapidly. We also rule out the case of θ = 0
in which Xt is not an autoregressive model any more.

Next, we assume that the stationary Gaussian noise ξ satisﬁes the following Hypothesis 1.1:

HYPOTHESIS 1.1. The covariance function Rξ(k) = E(ξ0ξk) for any k ∈ Z satisﬁes

Rξ(k) → C|k|2H−2,

as k → ∞,

(1.2)

where H ∈ ( 1

2 , 1) and C is a position constant.

We mentioned that in order to simplify the calculation in this paper, we decided to omit the
slowly varying function L; in fact, while this function is sometimes tedious to deal with from a
mathematical point of view, it is of less importance from a philosophical point of view.
It is well-known that Eq. (1.2) is equivalent to the spectral density of ξ satisfying

hξ(λ) (cid:118) CH |λ|1−2H ,

as λ → 0,

with CH = π−1Γ(2H − 1) sin(π − πH). Please refer to [8] or Lemma 2.2 below.

We will see that the fractional Gaussian noise and the fractional ARIMA model driven by

Gaussian white noise are special examples satisfying Hypothesis 1.1.

We will replace Xt in an AR(1) model with zero mean to Xt − µ, then

Xt − µ = θ(Xt−1 − µ) + ξt,

t ∈ N.

Here, µ = α
Denote

1−θ .

Yt :=

∞
(cid:88)

j=0

θjξt−j,

(1.3)

When |θ| < 1, the solution to the model (1.1) with initial value X0 = 0 can be represented as:

Xt = µ + Yt + θtζ,

(1.4)

where µ + Yt is the stationary solution and ζ is a normal random variable with zero mean.

It is clear that the second moment of Yt is:

f (θ) := E(Y 2

t ) =

∞
(cid:88)

i,j=0

θi+jRξ(i − j).

When 0 < θ < 1, f (θ) is positive and strictly increasing (see [7] (5)). Denote ¯X := 1
n
and f −1(·) is the inverse function of f (·).

(cid:80)n

t=1 Xt

We propose the second moment estimator of θ as:

ˆθn = f −1

(cid:32)

1
n

n
(cid:88)

(Xt − ¯X)2

(cid:33)

t=1

and the moment estimator of α as

ˆαn = (1 − ˆθn) ¯X.

(1.5)

(1.6)

MOMENT ESTIMATOR FOR AN AR(1) MODEL WITH NON-ZERO MEAN DRIVEN BY A LONG MEMORY GAUSSIAN NOISE3

In this paper, we will show the strong consistency and give the asymptotic distribution for ˆαn
and ˆθn. Moreover, we also give the joint asymptotic normality. These results are stated in the
following theorems:

Theorem 1.2. Under Hypothesis 1.1, the estimator ˆθn and ˆαn is strongly consistent, i.e.,

lim
n→∞

ˆθn = θ

a.s.;

lim
n→∞

ˆαn = α a.s..

Theorem 1.3. Under Hypothesis 1.1 and suppose that H ∈ ( 1
asymptotic distribution as n → ∞:
(1) asymptotic normality of ˆθn

2 , 3

4 ). Then, we have the following

√

n(ˆθn − θ) law−−→ N

(cid:18)

0,

σ2
H
[f (cid:48)(θ)]2

(cid:19)

,

where σ2

H = 2 (cid:80)
(2) asymptotic normality of ˆαn

k∈Z R2

Y (k), RY (k) = E(YtYt+k) and f (cid:48)(θ) is the derivative of f (θ);

n1−H (ˆαn − α) law−−→ N (0, σ2

1),

where σ2

1 = [H(2H −1)]−1π−1B(2H −1, 2−2H) sin(2πH −π) and B is the Beta function;
n(ˆθn −θ) and Gn,2 := n1−H (ˆαn −α), then we have the joint asymptotic

√

(3) Denote Gn,1 :=
normality

where

(Gn,1, Gn,2) law−−→ N2(0, Σ),

Σ =

(cid:32) σ2

H
[f (cid:48)(θ)]2
0

(cid:33)

.

0
σ2
1

Next, we give some sequences that satisfy Hypothesis 1.1.

Example 1.4. The fractional Gaussian noise with covariance function

Rξ =

1
2

(|k + 1|2H + |k − 1|2H − 2|k|2H ),

k ∈ Z

satisﬁes Hypothesis 1.1 when H > 1

2 . It is well-known that Rξ (cid:118) H(2H − 1)|k|2H−2 as k → ∞.

Example 1.5. The fractional ARIMA(0,d,0) model driven by a Gaussian white noise satisﬁes
Hypothesis 1.1 when d ∈ (0, 1
2 ). It is well-known that its the covariance function spectral density
satisfy

Rξ(k) = σ

Γ(k + d)Γ(1 − 2d)
Γ(k + 1 − d)Γ(1 − d)Γ(d)

,

k ≥ 0,

hξ(λ) =

σ
2π

|2 sin(λ/2)|−2d, λ → 0,

where Γ(·) is Gamma function.

Here, we give the ﬁgure.1 of the function y = f (θ) in the interval θ ∈ (0, 1) and the inverse
function θ = f −1(θ) with y > 0 for fGn with Hurst parameter H = 0.58 and ARFIMA(0, d, 0)
with d = 0.08 respectively.

In the remainder of this paper, C will be a generic positive constant independent of n the

value of which may diﬀer from line to line.

4MOMENT ESTIMATOR FOR AN AR(1) MODEL WITH NON-ZERO MEAN DRIVEN BY A LONG MEMORY GAUSSIAN NOISE

(a) a

(b) b

(c) c

(d) d

Figure 1. The ﬁgures of the function y = f (θ) and the inverse function θ =
f −1(θ) for fGn with Hurst parameter H = 0.58 ((a) and (b), respectively) and
ARFIMA(0, d, 0) with d = 0.08 ((c) and (d), respectively) respectively.

2. Preliminary

In this section, we list the main deﬁnitions and theorems that is used to show our results. The

following deﬁnition is cited from Deﬁnition 1.2 of [8] respectively.

Deﬁnition 2.1. Let {ξt} be a second-order stationary process with autocovariance function
Rξ(k)(k ∈ Z) and spectral density

hξ(λ) = (2π)−1

∞
(cid:88)

k=−∞

Rξ(k) exp(−ikλ), λ ∈ [−π, π].

Then {ξt} is said to exhibit linear long-range dependence if

hξ(λ) = CH |λ|1−2H ,

where H ∈ ( 1

2 , 1).

The following theorem is well-known and is cited from Theorem 1.3 of [8]:

Theorem 2.2. Let Rξ(k)(k ∈ Z) and h(λ)(λ ∈ [−π, π]) be the autocovariance function and
spectral density respectively of a second-order stationary process {ξt}. Then the following holds:

(1) If

Rξ(k) = C|k|2H−2,

k ∈ Z,

MOMENT ESTIMATOR FOR AN AR(1) MODEL WITH NON-ZERO MEAN DRIVEN BY A LONG MEMORY GAUSSIAN NOISE5

where H ∈ ( 1

2 , 1), then

where

(2) If

where H ∈ ( 1

2 , 1), then

where

h(λ) (cid:118) CH |λ|1−2H ,

λ → 0,

CH = π−1Γ(2H − 1) sin(π − πH).

h(λ) = C (cid:48)|λ|1−2H ,

0 < λ < π,

Rξ(k) (cid:118) C (cid:48)

H |k|2H−2,

k → ∞,

C (cid:48)

H = 2Γ(2 − 2H) sin(πH −

1
2

π).

The following theorem is well-known Wick formula (see [12] p.202 (A.2.13) and [13] Theorem

1.28).

Theorem 2.3. Given a ﬁnite set b. Denote by P(b) the class of all partitions of b. Assume that
(Y1, · · · , Yn) is a zero mean jointly normal random vector, then
(cid:40) (cid:80)

(cid:81)

π={ik,jk}∈P(n)

k E[Yik Yjk ],

E[Y1 · · · Yn] =

0,

n is even ;
n is odd .

Observe that, on the right-hand side, the sum is taken over all partitions π of n such that each
block of π contains exactly two elements.

We denote by Md(R) the collection of all real d × d matrices and Nd(0, C) the law of an

Rd-valued Gaussian vector with zero mean and covariance matrix C.

Denote H⊗pand H(cid:12)p as the pth tensor product and the pth symmetric tensor product of the
Hilbert space H. Let Hp be pth Wiener chaos with respect to G. It is deﬁned as the closed linear
subspace of L2(Ω) generated by the random variables {Hp(G(h)) : h ∈ H, (cid:107) h (cid:107)H= 1}, where Hp
is the pth Hermite polynomial deﬁned by

Hp(x) =

(−1)p
p!

x2
2

e

dp
dxp e− x2

2 ,

p ≥ 1.

and H0(x) = 1. We have the identity Ip(h⊗p) = Hp(G(h)) for any h ∈ H. where Ip(·) is the
generalized Wiener-Itô stochastic integral. Then the map Ip provides a linear isometry between
H(cid:12)p (equipped with norm 1√

p! (cid:107) · (cid:107)H⊗p ) and Hp. Here H0 = R and I0(x) = x, by convention.

The following theorem is multivariate fourth-moment theorem, see [14] Theorem 6.5 or [12]

Theorem 6.2.3.

Theorem 2.4. Let d ≥ 2 and qd, ..., q1 ≥ 1 be some ﬁxed integers. Consider vectors

Fn = (F1,n, · · · , Fd,n) = (Iq1(f1,n), · · · , Iqd (fd,n)), n ≥ 1,

where fi,n ∈ L2(Rqi ) is symmetric function. Let C ∈ Md(R) be a symmetric matrix. Assume
that

lim
n→∞

E[Fi,nFj,n] = Ci,j,

1 ≤ i, j ≤ d.

6MOMENT ESTIMATOR FOR AN AR(1) MODEL WITH NON-ZERO MEAN DRIVEN BY A LONG MEMORY GAUSSIAN NOISE

as n → ∞, the following two conditions are equivalent:

(1) Fn converges in law to Nd(0, C);
(2) For every 1 ≤ i ≤ d, we have Fi,n converges in law to N (0, Ci,i).

3. Proofs of The Strong Consistency

Proof of Theorem 1.2 First, we consider the strong consistency of ¯X. From Theorem 1.2 in [7],
then (Yt, t ∈ N) is a stationary and ergodic Gaussian process with zero mean. The ergodicity
theorem implies

1
n

n
(cid:88)

t=1

Yt −→ E(Yt) = 0 a.s., n → ∞.

lim
n→∞

1
n

ζ

n
(cid:88)

t=1

θt = 0

Since

and (1.4), we have

¯X −→ µ a.s., n → ∞.

(3.1)

To show the strong consistency of ˆθn, we will consider 1
n
n
(cid:88)

n
(cid:88)

n
(cid:88)

(Xt − µ)2 − ( ¯X − µ)2 =

(Xt − ¯X)2 =

1
n

t=1

1
n

t=1

1
n

t=1

(cid:80)n

t=1(Xt − ¯X)2. It is clear that

(Yt + ζθt)2 − ( ¯X − µ)2.

(3.2)

By Theorem 1.2 in [7], we have

1
n

n
(cid:88)

t=1

(Xt − µ)2 −→ f (θ) a.s., n → ∞.

Substituting last formula and (3.1) into (3.2), we obtain

1
n

n
(cid:88)

t=1

(Xt − ¯X)2 −→ f (θ) a.s., n → ∞.

By the second moment estimator of θ (see (1.5))

ˆθn = f −1

(cid:32)

1
n

n
(cid:88)

(Xt − ¯X)2

(cid:33)

,

t=1

we deduce that f −1(·) is a continue function. The continuous mapping theorem implies

lim
n→∞

ˆθn = θ

a.s.

(3.3)

Finally, (3.1) and (3.3) imply

Let

( ¯X, ˆθn) −→ (µ, θ) a.s., n → ∞.

g(µ, θ) = (1 − θ)µ,

Again by the continuous mapping theorem, we conclude that ˆαn is strongly consistent.

MOMENT ESTIMATOR FOR AN AR(1) MODEL WITH NON-ZERO MEAN DRIVEN BY A LONG MEMORY GAUSSIAN NOISE7

4. The Asymptotic Normality

Deﬁne

Vn :=

1
√
n

See Lemma 3.3-3.4 in [7].

n
(cid:88)

[Y 2

k − E(Y 2

k )], n ≥ 1.

k=1

(4.1)

Proposition 4.1. Under Hypothesis 1.1, Yt given in (1.3) exhibits linear long-range dependence.
Namely,

hY (λ) (cid:118) Cθ,H |λ|1−2H ,

as λ → 0,

where Cθ,H = Γ(2H−1) sin(π−πH)

(1−θ)2π

. If 1

2 < H < 1, then the covariation of Yt satisﬁes

RY (k) (cid:118) Cθ,H |k|2H−2,

as k → ∞,

where Cθ,H = B(2H−1,2−2H) sin(2πH−π)

(1−θ)2π

and B is Beta function.

(4.2)

(4.3)

1 be given in (1.6) and Theorem 1.3(2) respectively. If H > 1

2 ,

Proposition 4.2. Let ¯X and σ2
then as n → ∞,

n1−H ( ¯X − µ) law−−→ N

See [14] Theorem 7.3(1) and [15].

Proof. Deﬁne

(cid:18)

0,

σ2
1
(1 − θ)2

(cid:19)

.

Thanks to (1.4), we have

It is clear that

Zn := n−H

n
(cid:88)

i=1

Yi.

n1−H ( ¯X − µ) = Zn + n−H ζ

n
(cid:88)

t=1

θt.

n−H ζ

n
(cid:88)

t=1

θt −→ 0, n → ∞.

Hence, we need only show that

We claim that as n → ∞

Zn

law−→ N

(cid:18)

0,

(cid:19)

σ2
1
(1 − θ)2

n → ∞.

E(Z 2

n) =

1
n2H

n
(cid:88)

k,j=1

RY (k − j) −→

σ2
1
(1 − θ)2 .

Indeed,decompose the left-hand side as

1
n2H

(cid:88)

RY (k − j) +

k,j=1,··· ,n
|k−j|≤2

1
n2H

(cid:88)

RY (k − j).

k,j=1,··· ,n
|k−j|≥3

(4.4)

(4.5)

(4.6)

(4.7)

8MOMENT ESTIMATOR FOR AN AR(1) MODEL WITH NON-ZERO MEAN DRIVEN BY A LONG MEMORY GAUSSIAN NOISE

Using |RY (k)| ≤ f (θ), we obtain the ﬁrst term is bounded by C 1
zero as n → ∞ (recall that 1

2 < H < 1).Concerning the second term, one has, (4.3) implies

n2H−1 and therefore tends to

where

Then

where

RY (k) (cid:118) Cθ,H |k|2H−2,

as k → ∞,

Cθ,H =

B(2H − 1, 2 − 2H) sin(2πH − π)
(1 − θ)2π

=

H(2H − 1)σ2
1
(1 − θ)2

.

1
n2H

(cid:88)

k,j=1,··· ,n
|k−j|≥3

RY (k − j) (cid:118) H(2H − 1)σ2

1

(1 − θ)2

ln,

(cid:88)

ln =

k,j=1,··· ,n
|k−j|≥3

(cid:12)
(cid:12)
(cid:12)
(cid:12)

k − j
n

(cid:12)
(cid:12)
(cid:12)
(cid:12)

2H−2 1
n2 .

Therefore, there exists ln → l∞ for all x ∈ (0, 1) , where

l∞ = 2

(cid:90) 1

0

(1 − x)x2H−2dx.

If g(x) = (1 − x)x2H−2 with H ∈ ( 1

2 , 1), it is obvious that it is decreasing monotonically at
(0, 1). Let us show that ln is dominated by an integral function and the convergence rate is
obtained

|ln − l∞| = 2

= 2

≤ 2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

j=3

(cid:90) 2

n

0

n
(cid:88)

(cid:18)

1 −

j=3

(cid:19) (cid:18) j
n

(cid:19)2H−2 1
n

j
n

−

(cid:90) 1

0

g(x)dx

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

(cid:90) j

n

j−1
n

(cid:19)

(cid:18) j
n

g

dx −

n
(cid:88)

(cid:90) j

n

j=1

j−1
n

g(x)dx

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

f (x)dx +

n
(cid:88)

(cid:90) j

n

j=3

j−1
n

(cid:19)

(cid:20)
g

(cid:18) j − 1
n

− g

(cid:19)(cid:21)

(cid:18) j
n

dx

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n2H−1 .
Therefore, ln → l∞, as n → ∞. And this concludes the proof of (4.7).

= C

Characteristic function of Zn

E(eivZn ) = exp

(cid:18)

−

1
2

(cid:19)

v2E[Z 2
n]

n→∞−−−−→ exp

(cid:18)

−

1
2

v2

σ2
1
(1 − θ)2

(cid:19)

,

which implies the desired (4.6).

(cid:3)

Proof of Theorem 1.3 The limiting distribution of ˆθn is similar to AR(1) model with zero mean.
Consider

1
√
n

n
(cid:88)

n=1

(cid:0)(Xt − ¯X)2 − f (θ)(cid:1) =

1
√
n

n
(cid:88)

n=1

(cid:0)(Xt − µ)2 − f (θ)(cid:1) −

√

n( ¯X − µ)2.

(4.8)

MOMENT ESTIMATOR FOR AN AR(1) MODEL WITH NON-ZERO MEAN DRIVEN BY A LONG MEMORY GAUSSIAN NOISE9

By Theorem 1.3 in [7], we have

1
√
n

n
(cid:88)

n=1

(cid:0)(Xt − µ)2 − f (θ)(cid:1) law−−→ N (0, σ2

H ), n → ∞.

For H ∈ ( 1

2 , 3

4 ), Proposition 4.2 implies
√

n( ¯X − µ)2 = n2H− 3

2 (cid:0)n1−H ( ¯X − µ)(cid:1)2 n→∞−−−−→ 0.

Substituting (4.9) and (4.10) into (4.8), Slutsky’s theorem implies

1
√
n

n
(cid:88)

t=1

(cid:0)(Xt − ¯X)2 − f (θ)(cid:1) law−−→ N (0, σ2

H ), n → ∞.

Delta method and (3.3) imply

(cid:32)

(cid:34)
f −1

1
n

n
(cid:88)

t=1

(cid:33)(cid:35)(cid:48)

(Xt − ¯X)2

=

1
f (cid:48)(ˆθn)

n→∞−−−−→

1
f (cid:48)(θ)

a.s.

Thus,

√

n(ˆθn − θ) law−−→ N

(cid:18)

0,

σ2
H
[f (cid:48)(θ)]2

(cid:19)

.

(4.9)

(4.10)

(4.11)

Second, consider the asymptotic normality of ˆαn. The diﬀerence of ˆαn and α are represented

as the follow:

ˆαn − α = (1 − ˆθn) ¯X − (1 − θ)µ

= (1 − ˆθn) ¯X − (1 − θ) ¯X + (1 − θ) ¯X − (1 − θ)µ
= (θ − ˆθn) ¯X + (1 − θ)( ¯X − µ).

If H > 1

2 , then

lim
n→∞

n1−H (ˆθn − θ) ¯X = lim
n→∞

Therefore,

√

n
nH− 1

2

(ˆθn − θ) ¯X = µ lim
n→∞

√

n
nH− 1

2

(ˆθn − θ) = 0.

lim
n→∞

n1−H (ˆαn − α) = (1 − θ) lim
n→∞

n1−H ( ¯X − µ).

(4.12)

Proposition 4.2 and Slutsky’s theorem imply

n1−H (ˆαn − α) law−−→ N (0, σ2

1), n → ∞.

Now, consider the jointly asymptotic normality of ˆθn and ˆαn. Zn and Vn are given by (4.4)

and (4.1) respectively. Theorem 2.3 implies


E(ZnVn) = n− 1

2 −H

n
(cid:88)

n
(cid:88)



E(YiY 2

j ) − nf (θ)E(Yi)

 = 0.



i=1

j=1

Then, Zn and Vn are independent. Combining Theorem 2.4 with proof of Theorem 1.3 in [7],
(4.6), we obtain

(Vn, Zn) law−−→ N2(0, Σ1), n → ∞,

10MOMENT ESTIMATOR FOR AN AR(1) MODEL WITH NON-ZERO MEAN DRIVEN BY A LONG MEMORY GAUSSIAN NOISE

where

Denote

and

Σ =

(cid:32)

σ2
H
0

(cid:33)

.

0
σ2
1
(1−θ)2

Gn,3 :=

1
√
n

n
(cid:88)

t=1

(cid:0)(Xt − ¯X)2 − f (θ)(cid:1)

Gn,4 := n1−H ( ¯X − µ).

(4.13)

Slutsky’s theorem together with (3.11)-(3.12) in [7], (4.5), (4.10) implies

(Gn,3, Gn,4) law−−→ N2(0, Σ1), n → ∞.

Next, since the derivability of f (θ) at θ, we have

f (cid:48)(θ)(ˆθn − θ) + (ˆθn − θ) =

1
n

n
(cid:88)

t=1

(Xt − ¯X)2 − f (θ),

√

f (cid:48)(θ)

n(ˆθn − θ) +

√

n(ˆθn − θ)

o(ˆθn − θ)
ˆθn − θ

=

1
√
n

n
(cid:88)

t=1

(cid:0)(Xt − ¯X)2 − f (θ)(cid:1).

Theorem 1.3(1) implies that

√

n(ˆθn − θ) converges in law. Thanks to (3.3), we have

o(ˆθn − θ)
ˆθn − θ

−→ 0 a.s.

√

n(ˆθn − θ)

o(ˆθn − θ)
ˆθn − θ

law−−→ 0.

Then

Therefore,

√

lim
n→∞

n(ˆθn − θ) = lim
n→∞

1
f (cid:48)(θ)

1
√
n

n
(cid:88)

t=1

(cid:0)(Xt − ¯X)2 − f (θ)(cid:1).

Using Cramér-Wold Device ([16] Theorem 3.9.5) together with (4.12), we conclude

lim
n→∞

E

(cid:16)

ei(v1Gn,1+v2Gn,2)(cid:17)

(cid:16)

= lim
n→∞

E

1
f (cid:48) (θ)

Gn,3+v2(1−θ)Gn,4)(cid:17)

ei(v1
(cid:20)

exp

−

= lim
n→∞
(cid:20)

(cid:18)

v2
1

1
2

1

[f (cid:48)(θ)]2 E[G2
(cid:19)(cid:21)

(cid:18)

v2
1

1
2

σ2
[f (cid:48)(θ)]2 + v2
H

2σ2
1

.

= exp

−

Then

n,3] + v2

2(1 − θ)2E[Gn,4]2

(cid:19)(cid:21)

(Gn,1, Gn,2) law−−→ N2(0, Σ), n → ∞.

MOMENT ESTIMATOR FOR AN AR(1) MODEL WITH NON-ZERO MEAN DRIVEN BY A LONG MEMORY GAUSSIAN NOISE11

5. Simulation

In this section, we will test the performance of estimators ˆθn and ˆαn under limited samples,
and test their joint distribution performance. By the delta method, we know that the asymptotic
normal of the moment estimator ˆθn is equivalent to that of the random variable
n
(cid:88)

1
√
n

n=1

(cid:0)(Xt − ¯X)2 − f (θ)(cid:1),

please refer to (4.11). Since the inverse function θ = f −1(·) does not have an analytic expression,
it is convenience to plot the ﬁgure of the above random variable to show the asymptotic normality.
The fGn noise and the ARFIMA(0 ,d ,0) noise data used in this section are all generated by the
R package "longmemo" in [4].

In this experiment, the data generated with θ = 0.6, n = 3000, and each group of data
will be copied M = 10000 times by Monte Carlo method.In order to better test the estimator,
we simulate fGn noise with H = 0.58 (see subscript (a) of all the following ﬁgures) and the
ARFIMA(0 ,d ,0) noise with d = 0.08 (see subscript (b) of all the following ﬁgures) by Monte
Carlo method. We have the asymptotic normality for that random variable, as shown in Figure.2.
And we have the asymptotic normality for that the estimator ˆαn, as shown in Figure.3. Thus
the joint asymptotic normality of the two moment estimators ˆθn and ˆαn, as shown in Figure.4.

(a)

(b)

Figure 2. The asymptotic normality of 1
n

(cid:80)n

t=1(Xt − ¯X)2.

(a)

(b)

Figure 3. The asymptotic normality of ˆαn.

12MOMENT ESTIMATOR FOR AN AR(1) MODEL WITH NON-ZERO MEAN DRIVEN BY A LONG MEMORY GAUSSIAN NOISE

(a)

(b)

Figure 4. The joint asymptotic distributions of 1
n

(cid:80)n

t=1(Xt − ¯X)2 and ˆαn.

References

[1] Anderson T W, Taylor J B. Strong consistency of least squares estimates in dynamic models[J]. Ann. Statis,

1979, 7(3): 484-489.

[2] Lai T L, Wei C Z. Asymptotic properties of general autoregressive models and strong consistency of least

squares estimates of their parameters[J].J. Multivar. Anal, 1983, 13(1): 1-23.

[3] Mami T, Yousfate A.On parameters estimation of stationary AR(1) with nonzero mean alpha-stable innova-

tions in the case α ∈ [1, 2][J]. Acta Universitatis Apulensis, 2013, 35

[4] Beran J. Statistics for long-memory processes[M]. Monographs on Statistics and Applied Probability, 1994.
[5] Brouste A, Cai C, Kleptsyna M. Asymptotic properties of the MLE for the autoregressive process coeﬃcients

under stationary Gaussian noise[J]. Mathematical Methods of Statistics, 2014, 23(2): 103-115.

[6] Brouste A, Cai C, Soltane M, Wang L. Testing for the change of the mean-reverting parameter of an autore-

gressive model with stationary Gaussian noise[J]. Stat Inference Stoch Process, 2020, 23: 301-318.

[7] Chen Y, Tian L, Li Y. Second moment estimator for AR(1) model driven by a long memory Gaussian noise[J].

Journal of Statistical Planning and Inference, 2023, 222:94-107.

[8] Beran J, Feng Y, Ghosh S, and Kulik R. Long-memory processes: probabilistic properties and statistical

methods[M]. Berlin: Springer-Verlag, 2013.

[9] Giraitis L, Koul H L, Surgailis D. Asymptotic normality of regression estimators with long memory errors[J].

Statistics and Probability Letters, 1996, 29(4): 317-335.

[10] Zygmund A. Trigonometric series[M]. Cambridge:Cambridge University Press, 1968.
[11] Bingham N H, Goldie C M, Teugels J L. Regular Variation[M]. Cambridge:Cambridge University Press, 1989.
[12] Nourdin I, Peccati G. Normal approximations with Malliavin calculus: from Stein??s method to universal-

ity[M]. Cambridge:Cambridge University Press, 2012.

[13] Janson S. Gaussian Hilbert Spaces[M]. Cambridge:Cambridge University Press, 1997.

[14] Nourdin I.Selected aspects of fractional Brownian motion[M]. Milan:Springer, 2012.
[15] Taqqu M S. Weak convergence to fractional Brownian motion and to the Rosenblatt process[J]. Wahrschein-

lichkeitstheorie verw. Gebiete, 1975, 31: 287-302.

[16] Durrett R. Probability: theory and examples[M]. Cambridge:Cambridge University Press, 2010.

School of Mathematics and Statistics, Jiangxi Normal University, Nanchang, 330022, Jiangxi,

China

Email address: luyp@jxnu.edu.cn

