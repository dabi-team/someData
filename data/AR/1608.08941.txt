6
1
0
2

g
u
A
1
3

]
E
M

.
t
a
t
s
[

1
v
1
4
9
8
0
.
8
0
6
1
:
v
i
X
r
a

Penalised complexity priors for stationary autoregressive
processes

Sigrunn Holbek Sørbye1 and H˚avard Rue2

1Department of Mathematical Sciences, UiT The Arctic University of Norway, 9037
Tromsø, Norway. e-mail: sigrunn.sorbye@uit.no
2Department of Mathematical Sciences, Norwegian University of Science and Technology,
7491 Trondheim, Norway. e-mail: hrue@math.ntnu.no

September 1, 2016

Abstract

The autoregressive process of order p (AR(p)) is a central model in time series analysis.
A Bayesian approach requires the user to deﬁne a prior distribution for the coefﬁcients of the
AR(p) model. Although it is easy to write down some prior, it is not at all obvious how to
understand and interpret the prior, to ensure that it behaves according to the users prior knowl-
edge. In this paper, we approach this problem using the recently developed ideas of penalised
complexity (PC) priors. These priors have important properties like robustness and invariance to
reparameterisations, as well as a clear interpretation. A PC prior is computed based on speciﬁc
principles, where model component complexity is penalised in terms of deviation from simple
base model formulations. In the AR(1) case, we discuss two natural base model choices, cor-
responding to either independence in time or no change in time. The latter case is illustrated in
a survival model with possible time-dependent frailty. For higher-order processes, we propose
1) model
a sequential approach, where the base model for AR(p) is the corresponding AR(p
expressed using the partial autocorrelations. The properties of the new prior are compared with
the reference prior in a simulation study.

−

Keywords: AR(p), R-INLA, prior selection, robustness.

1 Introduction

Autoregressive (AR) processes are widely applied to model time-varying stochastic processes, for
example within ﬁnance, biostatistics and natural sciences (Brockwell and Davis, 2002; Chatﬁeld,
2003; Prado and West, 2010). Applications also include Bayesian model formulations, often com-
bined with Markov chain Monte Carlo computations to perform posterior and predictive inference
(Albert and Chib, 1993; Chib, 1993; Barnett et al., 1996). Particularly, AR processes are useful
to model underlying latent dependency structure and they make up important building blocks in
complex hierarchical models, for example analysing spatial data (Lesage, 1997; Sahu et al., 2007;
Sahu and Bakar, 2012).

In ﬁtting an AR(p) process using a Bayesian approach, it is necessary to select priors for all model
parameters. A simple choice is to assign uniform priors to the regression coefﬁcients (Zellner, 1971;
DeJong and Whiteman, 1991), but this is not optimal neither for the ﬁrst-order nor higher-order pro-
cesses (Berger and Yang, 1994). A more reasonable approach is given by Liseo and Macaro (2013),

1

 
 
 
 
 
 
who provide a general framework to compute both Jeffreys and reference priors using the well-
known partial autocorrelation function (PACF) parameterisation (Barndorff-Nielsen and Schou, 1973).
Stationarity of the AR(p) process is equivalent to choosing the partial autocorrelations within a p-
dimensional unit hypercube. In general, Jeffreys priors are invariant to reparameterisations, while
reference priors are not. Liseo and Macaro (2013) recommend reference priors, at least when the
order of the AR process is smaller or equal to 4. For higher-order processes, calculation of the
reference prior is numerically cumbersome and requires extensions of their suggested numerical
approximation.

This paper derives and investigates penalised complexity (PC) priors (Simpson et al., 2016) for
the partial autocorrelations of stationary AR processes of any ﬁnite order. In general, a PC prior is
computed based on speciﬁc underlying principles, in which a model component is seen as a ﬂexible
parameterisation of a simple base model structure. The main idea is to assign a prior to a measure
of divergence from the ﬂexible version of the component to its base model and the PC prior for the
relevant parameter is derived by transformation. In the AR(1) case, this implies that the PC prior for
the ﬁrst-lag coefﬁcient φ can be derived using white noise (φ = 0) as a base model. Alternatively,
we can view the limiting random walk case as a base model (φ = 1), representing no change in time.
Which of these base models that represent a natural choice depends on the relevant application.

In the higher-order AR(p) case, we introduce a sequential approach to construct a PC prior for
1) process as a base model. The
the pth partial autocorrelation, using the corresponding AR(p
resulting joint prior for the partial autocorrelations is consistent under marginalisation, and each of
the marginals can be adjusted according to a user-deﬁned scaling criterion. The scaling is important
and prescribes the degree of informativeness of the prior. Here, we suggest to incorporate a scaling
criterion using the variance of the one-step ahead forecast error, also allowing for different rates
of shrinkage for each of the partial autocorrelations. The resulting priors have good robustness
properties and are also seen to have comparable frequentistic properties with reference priors.

−

The plan of this paper is as follows. PC priors and their properties are reviewed in Section 2.
We derive PC priors for the coefﬁcient of an AR(1) process in Section 3, using the two mentioned
base models. PC priors are designed to prevent overﬁtting and this property is demonstrated for
a real data example in Section 4, where an AR(1) process is used to model time-dependent frailty
in a Cox proportional hazard model. Contrary to previous results (Fleming and Harrington, 2005;
Yau and McGilchrist, 1998), the given data on chronic granulomatous disease do not seem to support
the additional introduction of a time-varying frailty. Extension of the PC priors to higher-order AR
processes is given in Section 5, including incorporation of interpretable scaling parameters to adjust
the rate of shrinkage. Section 6 contains simulation results, comparing the performance of the PC
and reference priors, while concluding remarks are given in Section 7.

2 Penalised complexity priors and their properties

The framework of PC priors (Simpson et al., 2016) represents a systematic and uniﬁed approach to
compute priors for parameters of model components with an inherit nested structure. A simple ver-
sion of the model component is referred to as a base model, typically characterised by a ﬁxed value
of the relevant parameter, while the ﬂexible version is seen as a function of the random parameter.
The PC prior is computed to penalise deviation from the ﬂexible model to the ﬁxed base model. This
section gives a brief review on PC priors and their properties in the context of AR(p) processes.

2

2.1 A brief review on the principles underlying PC priors

The informativeness of PC priors is speciﬁed in terms of four main principles, stated in Simpson et al.
(2016). These principles are useful both to compute priors in a uniﬁed way and to understand
their properties. The principles, summarised below, express support to Occam’s razor, penalisa-
tion of model complexity using the Kullback-Leibler divergence, a constant rate penalisation and
user-deﬁned scaling, see Simpson et al. (2016) for a thorough description of PC priors and their
applications.

|

1. Let f1 = π(x

ξ) denote the density of a model component x in which we aim to ﬁnd a
prior for the parameter ξ. A simpler structure of this model component is characterised by
the density f0 = π(x
ξ = ξ0), where ξ0 is a ﬁxed value. In accordance with the principle
of parsimony expressed by Occam’s razor, the prior for ξ should be designed to give proper
shrinkage to ξ0 and decay with decreasing complexity of f1.

|

2. In order to characterise the complexity of f1 compared with f0, we calculate a measure of
complexity between these two densities. PC priors are derived using the Kullback-Leibler
divergence (Kullback and Leibler, 1951),

KLD(f1 k

f0) =

f1(x) log

Z

f1(x)
f0(x)

(cid:18)

(cid:19)

dx,

which measures the information lost when the ﬂexible model f1 is approximated with the
simpler model f0. For zero-mean multi-normal densities, calculation of the Kullback-Leibler
divergence simpliﬁes to performing simple matrix computations on the covariance matrices as

KLD(f1 k

f0) =

1
2

(cid:18)

tr(Σ−1
0

Σ1)

n

−

−

ln

Σ1|
|
Σ0| (cid:19)(cid:19)
|

(cid:18)

where fi ∼
Kullback-Leibler divergence is transformed to a unidirectional distance measure

N (0, Σi), i = 0, 1, while n is the dimension. To facilitate interpretation, the

d(ξ) = d(f1 k

f0) =

2KLD(f1 k

f0).

(1)

This is not a distance metric in the ordinary sense, but a quantity which is interpretable as a
measure of distance from the ﬂexible model f1 to the base model f0.

p

3. In choosing a prior for the distance measure d(ξ), it is natural to assume that the mode should
be located at the base model while the density decays as the distance from the base model
increases. The PC prior is derived based on a principle of constant rate penalisation,

π(d(ξ) + δ)
π(d(ξ))

= rδ,

d(ξ), δ

0,

≥

(2)

∈

where r
(0, 1). This implies that the relative change in the prior for d(ξ) is independent of
the actual distance. This is a reasonable choice as it is complicated to properly characterise
different decay rates for different distances. The resulting prior is exponentially distributed,
λ) and the corresponding PC prior for ξ
π(d(ξ)) = λ exp(
follows by a standard change of variable transformation.

λd(ξ)), λ > 0, where r = exp(

−

−

4. The rate λ characterises the shrinkage properties of the prior and it is important that this
parameter can be chosen (implicitly) in an intuitive and interpretable way, for example by a

3

user-deﬁned probability statement for the parameter of interest. Simpson et al. (2016) suggest
to determine λ by incorporating a probability statement of tail events, e.g.

P (Q(ξ) > U ) = α,

(3)

where U represents an assumed upper limit for an interpretable transformation Q(ξ), while
α is a small probability. However, other scaling suggestions might be just as reasonable,
depending on the speciﬁc application.

2.2 Important properties of PC priors in the context of AR processes

The given four principles provide a strategy to calculate priors for model parameters in a systematic
way, rather than turning to ad-hoc prior choices still often made in Bayesian literature. Also, the
principles can be helpful to interpret the assumed prior information and how this inﬂuences posterior
results.

A ﬁrst important property of PC priors is invariance to reparameterisations. This follows auto-
matically as the prior is derived based on a measure of divergence between models, which does not
depend on the speciﬁc model parameterisation. We consider the invariance property to be partic-
ularly useful in the case of autoregressive processes, as these are typically parameterised either in
terms of the regression coefﬁcients, or by using the partial autocorrelations. The great beneﬁt of
using the partial autocorrelations is that these give an unconstrained set of parameter values, ensur-
ing a positive deﬁnite correlation matrix. In contrast, the valid parameter space for the regression
coefﬁcients is rather complicated, especially for higher-order processes (p > 3).

Second, the PC priors are designed to shrink towards well-deﬁned base models. In the setting of
autoregressive processes, this implies that the priors will prevent overﬁtting, for example in terms of
selecting an unnecessarily high order of the process. In addition, the base model can be chosen to
reﬂect different simple structures of a model component, depending on the given application. For an
AR(1) process, it is relevant to assume either no dependency, or no change in time, as simple base
model formulations. For higher-order processes, we could also choose no correlation as a base model
but this might cause too much shrinkage in many applications. As an alternative, we introduce a new
sequential approach which deﬁnes a sequence of base models, reﬂecting the additional complexity
in increasing the order of the ﬁtted AR process.

Third, PC priors are computationally simple and are already implemented within the R-INLA
framework (Rue et al., 2009; Martins et al., 2013), for different latent Gaussian model components.
The priors are designed to have a clear interpretation as the informativeness of the priors is adjusted
by user-deﬁned scaling. Here, we will take advantage of this to allow for different rates of shrinkage
for priors assigned to partial autocorrelations of different lags. In contrast, objective priors simply
aim to incorporate as little information to the inference as possible.

3 PC priors for AR(1) using two different base models

A ﬁrst-order autoregressive process can be deﬁned by

xt = φxt−1 + wt, wt ∼

N (0, κ−1),

t = 2, . . . , n,

φ2).
where x1 is assumed to be normally distributed with mean 0 and marginal precision τ = κ(1
This process represents an important special case of general autoregressive processes, in which the
dependency structure is governed by the coefﬁcient φ. Using the framework of penalised complexity
priors, φ is viewed as a ﬂexibility parameter reﬂecting deviation from simple ﬁxed base model

−

4

formulations. In this section, we derive PC priors for φ both using no autocorrelation (φ = 0) and no
change in time (φ = 1) as base models, and we suggest how these priors can be scaled. A real-data
application using the latter base model is included in Section 4.

Note that we also use a penalised complexity prior for the precision parameter τ . Following
Simpson et al. (2016), this prior is derived using inﬁnite precision as a base model, which gives the
type-2 Gumbel distribution

π(τ ) =

λ
2

τ −3/2 exp(

−

λτ −1/2), λ > 0.

(4)

The rate λ is inferred using the probability statement P (1/√τ > U ) = α, where α is a small
probability. The prior is scaled by specifying an upper limit U for the marginal standard deviation
1/√τ , in which the corresponding rate is λ =
log(α)/U . To make an intuitive choice for U , one
can consider the marginal standard deviation after the precision τ is integrated out. For example, if
α = 0.01 this standard deviation is 0.31U (Simpson et al., 2016).

−

3.1 Base model: No dependency in time

In general, the correlation matrix of the ﬁrst-order autoregressive process is Σ1 =
. Choos-
ing no autocorrelation (φ = 0) as a base model, the resulting process is white noise with correlation
matrix equal to the identity matrix, Σ0 = I. By simple matrix calculations, the distance function (1)
φ2). Using the principle of constant rate penalisation (2),
is seen to equal d(φ) =
−
an exponential prior is assigned to d(φ) with rate λ = θ/√n
1. The resulting prior is invariant to n
and by the ordinary transformation of variable formula, the PC prior for the one-lag autocorrelation
is

n) log(1

φ|i−j|

p

(1

−

−

(cid:1)

(cid:0)

π(φ) =

exp

θ

−

ln(1

−

−

φ2)

θ
2

(cid:16)

p

(1

φ2)

−

(cid:17)

φ
|
|
ln(1

−

−

,

φ2)

φ
|
|

< 1, θ > 0.

(5)

The rate parameter θ is important as it inﬂuences how fast the prior shrinks towards the white
noise base model. To infer θ, we need a sensible criterion which facilitates the interpretation of
this parameter. Simpson et al. (2016) suggest to use a probability statement for an interpretable
transformation of the parameter of interest, for example in terms of tail events as deﬁned by (3).
When the base model is φ = 0, a reasonable alternative is to deﬁne such a tail event as large absolute
correlations being less likely, i.e.

p

Prob(
φ
|
|

> U ) = α.

−

ln(1

ln(α)/

U 2). The interpretation of this criterion is intuitive in the
This implies that θ =
ﬁrst-order case, but we ﬁnd it difﬁcult to use in practice for higher-order processes. An alternative
scaling idea is presented in Section 5.2, where we consider the variance of the one-step forecast
error as the order of the AR process is increased. We recommend the latter approach, as this is more
intuitively implemented for general AR(p) processes.

p

−

−

3.2 Base model: No change in time

An alternative base model for the AR(1) process is to assume that the process does not change in
time (φ = 1). This represents a limiting random walk case, being a non-stationary and singular
process. Consequently, a limiting argument is needed to derive the PC prior for φ.
φ|i−j|
0

where φ0 is close to 1 and φ < φ0. In this case, the

and Σ0 =

Let Σ1 =

φ|i−j|

Kullback-Leibler divergence is
(cid:0)

(cid:16)

(cid:17)

KLD(f1(φ)

f0) =

k

(cid:1)
1
2

1

1

(cid:18)

−

φ2
0

(n

−

2(n

−

1)φ0φ + (n

2)φ2
0)

−

n

−

−

(n

−

1) ln

1
1

−
−

(cid:18)

φ2
φ2

0 (cid:19)(cid:19)

.

5

Considering the limiting value as φ0 →

1, the distance

d(φ) = lim
φ0→1

2KLD(f1(φ)

p

f0) = lim

φ0→1 s

k

2(n

φ)

−

−
1

1)(1
φ2
0

−

= c

1

φ,

−

< 1,

φ
|
|

p

for a constant c that does not depend on φ. Since 0
distribution to d(φ) with rate θ = λ/c and the resulting PC prior for φ is

d(φ)

≤

≤

c√2, we assign a truncated exponential

π(φ) =

θ exp

exp

1

−

θ√1

−
√2θ

(cid:0)
−

−

φ
2√1
(cid:1)

−

,

φ

< 1.

φ
|
|

(6)

Again, we need to suggest an intuitive criterion to scale the prior in terms of θ. This case requires
(cid:1)(cid:1)
separate consideration, as it cannot be seen as a special case of the approach in Section 5. One option
is to make use of (3), and determine (U, α) in terms of the probability statement Prob(φ > U ) = α.
The solution to this equation is given implicitly by

(cid:0)

(cid:0)

provided that α is larger than the lower limit

1

−
1

exp

θ√1

−
exp
(cid:0)

−

−
√2θ

U

(cid:1)

= α,

−
(cid:0)
(1

(cid:1)
U )/2.

−

3.3 The PC priors versus the reference prior

p

The two alternative PC priors for the ﬁrst-lag coefﬁcient of an AR(1) process are illustrated in Fig-
ure 1, using rate parameter θ = 2 in (5) and (6), respectively. For comparison, we also illustrate
the reference prior deﬁned by π(φ) = 1
φ2)−1/2,
< 1 (Barndorff-Nielsen and Schou, 1973;
Berger and Yang, 1994; Liseo and Macaro, 2013).

φ
|
|

π (1

−

In general, reference priors are designed to give objective Bayesian inference in the sense of
being least informative in a certain information-theoretic sense (Berger et al., 2009). This implies
that the data are given a maximum effect on the posterior estimates. In general, the reference prior
In the given
is calculated to maximise a measure of divergence from the posterior to the prior.
AR(1) case, the reference prior for φ is calculated to maximise an asymptotic version of the ex-
pected Kullback-Leibler divergence, in practice performed using an asymptotic version of the Fisher
information matrix (Barndorff-Nielsen and Schou, 1973; Liseo and Macaro, 2013). The resulting
reference prior is seen to be similar to the Jeffreys prior which is deﬁned (up to a constant) by the
square root of the determinant of the Fisher information matrix (Liseo and Macaro, 2013). Using
a small rate parameter, the PC prior with base φ = 0 will be quite similar to the reference/Jeffreys
prior but for increasing rate parameters, the effect of shrinkage to 0 is increased. Note that a PC prior
1 as the base model can be derived similarly as using the φ = 1 base model.
using φ =

−

4 Application: Modeling time-varying frailty with AR(1)

To demonstrate the use of the PC prior for the lag-one autocorrelation, we consider an example of
a Cox proportional hazard model with time-varying frailty. The Cox proportional hazard model is
a popular type of survival model that can be ﬁtted to recurrent event data. It assumes that the time-
varying hazard for the ith subject can be expressed as h(t; i) = h0(t) exp(ηi), where the combined
risk variable ηi in most cases depends on subject-speciﬁc covariates zi and contributions from ran-
dom effects/frailty. The function h0(t) is the baseline hazard, see Fleming and Harrington (2005)
for further details and applications of the model. In the given example, our main focus is on the
inclusion of a subject-speciﬁc and possibly time-dependent frailty term in ηi.

6

0
.
2

5
.
1

)

(

0
.
1

5
.
0

0
.
0

−1.0

−0.5

0.0

0.5

1.0

Figure 1: The PC priors for the coefﬁcient φ of AR(1), using φ = 0 (solid thick line) and φ = 1
(dashed line) as base models. The rate parameters θ in (5) and (6) are set equal to 2 in both cases.
For comparison we also include the reference prior for φ (dotted line).

4.1 Dependent Gaussian random effects

A full Bayesian analysis of the Cox proportional hazard model requires a model for the baseline
hazard. A natural choice is to consider the log baseline hazard as a piecewise constant function
on small time intervals, and impose smoothness to penalise deviations from a constant, see for
example Fahrmeir and Tutz (2001, Sec 8.1.1) and Rue and Held (2005, Sec. 3.3.1). Let [0, T ] be
the time interval of interest, and divide that interval into n equidistant (for simplicity) intervals
< tn−1 < T . Let hj, j = 1, . . . , n denote the log baseline hazard in the jth
0 < t1 < t2 <
interval. The ﬁrst order random walk (RW1) model imposes smoothing among neighbour hi’s,

· · ·

π(h

τ )

|

∝

(τ τ ∗)(n−1)/2 exp

τ τ ∗
2



−

(hj −

hj−1)2

.



n

Xj=2




This is a ﬁrst-order intrinsic Gaussian Markov random ﬁeld with a covariance matrix on the form
1. The parameter τ ∗ is a positive
τ −1R, where the correlation matrix R is singular and of rank n
scaling constant which is added such that the generalised variance (the geometric mean of the diag-
onal elements of R−1), is 1. This is needed to make the model invariant to the size of n and to unify
the interpretation of τ , which then represents the precision of the (marginal) deviation from the null
space of R, see Sørbye and Rue (2014) and Simpson et al. (2016) for further details. To separate
i hi = 0. The base model is a
the baseline hazard from the intercept, we impose the constraint
constant (in time) baseline hazard, which corresponds to inﬁnite smoothing, τ =
. The resulting
penalised complexity prior for τ is given by (4).

P

∞

−

An interesting extension to the commonly used subject speciﬁc frailty model is to allow the frailty
term to depend on time (Yau and McGilchrist, 1998), leading to a time-dependent combined risk
variable ηi(t). Anticipating a positive correlation in time, it is natural to model this time dependent
risk using a continuous-time Ornstein-Uhlenbeck process or its discrete time version given by AR(1).

7

f
p
f
The stationary AR(1) model for subject i’s speciﬁc frailty is given by

vit | {

vis, s < t

} ∼ N

(φvi,t−1, 1/(τv(1

φ2))),

−

parameterised so that τv is the marginal precision and φ is the lag-one correlation. For this model
component, the natural base model (keeping the marginal precision constant) is a time-constant
frailty, in which we use the PC prior for φ in (6). For a ﬁxed correlation φ, the base model for the
precision τv is the constant zero which gives the type-2 Gumbel prior in (4).

4.2 Analysis of chronic granulomatous disease data

We end this section by analysing data on chronic granulomatous disease (CGD) (Fleming and Harrington,
2005) available in R as the cgd dataset in the survival package. This data set consists of 128
patients from 13 hospitals with CGD. These patients participated in a double-blinded placebo con-
trolled randomised trial, in which a treatment using gamma interferon (γ-IFN) was used to avoid
or reduce the number of infections suffered by the patients. The recorded number of CGD infec-
tions for each patient ranged from zero to a maximum of seven, and the survival times are given
as the times between recurrent infections on each patient. We follow Yau and McGilchrist (1998)
and introduce a deterministic time dependent covariate for each patient, given as the time since the
ﬁrst infection (if any). Additionally, we include the covariates treatment (placebo or γ-IFN), inherit
(pattern of inheritance), age (in years), height (in cm), weight (in kg), propylac (use of prophylactic
antibiotics at study entry), sex, region (US or Europe), and steroids (presence of corticosteroids)
(Manda and Meyer, 2005; Yau and McGilchrist, 1998). The covariates age, height and weight were
scaled before the analysis.

The computations were performed using the R-INLA package, by rewriting the model into a
larger Poisson regression, see Fahrmeir and Tutz (2001) for a more general discussion and Martino et al.
(2010) for R-INLA speciﬁc details. The prior speciﬁcations are as follows. We used a constant prior
for the intercept and independent zero mean Gaussian prior with low precision, i.e. 0.001, for all
the ﬁxed effects. For the log baseline hazard with n = 25 segments, we used the type-2 Gumbel
prior with parameters (U = 0.15/0.31, α = 0.01) giving a marginal standard deviation for the log
baseline hazard of about 0.15. This seems adequate as we do not expect the log baseline hazard
to be highly variable. The time-dependent frailty was assigned a type-2 Gumbel prior for the pre-
cision with parameters (U = 0.3/0.31, α = 0.01) giving a marginal standard deviation of about
0.3, hence we allow for moderate subject speciﬁc variation. For the derived prior (6) for φ, we used
the parameters (U = 1/2, α = 0.75), which puts most of the prior mass for high values of φ as
P (φ > 1/2) = 0.75. This corresponds to using a rate parameter θ

1.55 in (6).

≈

Figure 2 (a) shows the prior (dashed) and posterior (solid) densities for the autocorrelation co-
efﬁcient of the AR(1) model for the frailty. The data hardly alters the prior at all, showing that
there is not much information in the data available for this parameter, and we cannot conclude any-
thing about the time-varying frailty. This is contrary to the ﬁndings in Manda and Meyer (2005) and
Yau and McGilchrist (1998). Figure 2 (b) displays the log baseline hazard, showing an increasing
trend (additional to the deterministic time dependent covariate), but the wide point-wise credible
bands give no clear evidence for a time-dependent baseline hazard. With the new prior we are more
conﬁdent that we do not overﬁt the data using the more ﬂexible model for the log baseline hazard, as
we do control the amount of deviation and its shrinkage towards it. The given conclusions are robust
to changes in the parameter choices (U, α) for the different model components.

8

y
t
i
s
n
e
D

0
1

8

6

4

2

0

d
r
a
z
a
h
.
e
n

i
l

e
s
a
b
g
o
L

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

2
.
0
−

4
.
0
−

6
.
0
−

0.0

0.2

0.4

0.6

0.8

1.0

0

100

200

300

400

Lag−one correlation

(a)

Relative time

(b)

Figure 2: Panel (a) displays the posterior density (solid) and prior density (dashed) for the lag-
one autocorrelation φ in the AR(1) model for the time-dependent frailty. Panel (b) displays the
log baseline hazard, mean (solid), median (dashed-dotted), lower (0.025, dashed) and upper (0.975,
dotted) quantiles.

5 Deriving PC priors for higher-order AR processes

Deﬁne an autoregressive process of order p by

xt = φ1xt−1 +

· · ·

+ φpxt−p + ǫt,

ǫt

iid
∼ N

(0, κ−1),

(7)

where x = (x1, . . . , xn) is an n-dimensional vector, t = p, . . . , n, and κ is the precision of the
p correlation matrix Σp is Toeplitz (Gray, 2002) with elements
innovations. The corresponding p
that can be expressed as Σij = σ|i−j|, where σ0 = 1. Although (7) is a natural parameterisation
for known parameter values φp = (φ1, . . . , φp), it is an awkward parameterisation when these are
unknown, as the positive deﬁniteness requirement of the correlation matrix makes the space of valid
φp complicated for p > 3. This implies that it is necessary to impose a number of non-linear
constraints on these coefﬁcients to deﬁne a stationary process.

×

A good alternative is to make use of the invariance property of the PC prior and deﬁne the prior
for φp implicitly. The basic idea, which is commonly used when estimating AR(p) parameters, is
to assign the prior to the partial autocorrelations ψp = (ψ1, . . . , ψp)
1.
This gives a useful unconstrained set of parameters for this problem. Furthermore, there is a smooth
bijective mapping between the partial autocorrelations and the autocorrelations in Σp, given by the
Levinson-Durbin recursions (Monahan, 1984; Golub and van Loan, 1996).

1, 1]q, where q = p

[
−

−

∈

5.1 A sequential approach to construct PC priors

In deriving PC priors for the partial autocorrelations of an AR(p) process, we suggest to use a
sequential approach, augmenting the partial autocorrelations one by one. Deﬁne ψ0 = 0 and assume
that ψp = (ψp−1, ψp) for p = 1, 2, . . . . We calculate the Kullback-Leibler divergence, conditional
on the terms already included in the model,

KLD(f1(ψp)

f0(ψp−1)) =

k

1
2

(cid:18)

tr(Σ−1
p−1

Σp)

n

−

−

ln

Σp|
|
Σp−1| (cid:19)(cid:19)
|

,

(cid:18)

9

 
where Σ0 = I and f1 and f0 represent the densities of the AR(p) and AR(p
1) processes, respec-
tively. Notice that by augmenting the partial autocorrelations ψp−1 with one (or several) terms, the
1 elements of the corresponding AR(p) process remains
correlation structure between the ﬁrst p
1) process is a band matrix of order
unchanged. As the inverse correlation matrix of the AR(p
2p

1, we immediately notice that

−

−

−

−

Σ−1
p

Σp+r = I,

r = 1, 2 . . . ,

and tr(Σ−1
p−1

Σp) = n. Also,

ln

Σp|
|
Σp−1| (cid:19)
|

= ln

(cid:18)

  Q
Q
of the pth order partial autocorrelation, i.e.,

p
i=1(1
p−1
i=1 (1

i )n−i
ψ2
ψ2
i )n−i !

−

−

= (n

−

p) ln(1

ψ2

p).

−

The resulting measure of distance from the AR(p) model to its base AR(p

1), is only a function

−

d(ψp) =

2KLD(f1(ψp)

f0(ψp−1)) =

k

(n

−

−

p) ln(1

ψ2

p).

−

q

Applying the principle of constant rate penalisation (2), an exponential density is assigned to d(ψp)
with rate λp = θp/√n

p. The resulting prior for the pth partial autocorrelation is

q

−

π(ψp) =

θp
2

exp

θp

−

(cid:16)

−

q

ln(1

−

ψ2
p)

(cid:17)

(1

−

ψ2
p)

ln(1

ψp|
|
−

q

,

ψ2
p)

−

< 1,

ψp|
|

(8)

where the parameter θp > 0 inﬂuences how fast the prior shrinks towards the base model.

The given formulation allows us to derive interpretable conditional priors for each of the partial
autocorrelations ψp, given the previous parameters ψp−1. As the resulting priors are conditionally
independent, the partial autocorrelations are seen to be consistent under marginalisation (as dis-
cussed in West (1991) in the context of kernel density estimation). Also, the marginal for an AR(q)
process is not inﬂuenced by higher-order partial autocorrelations when these are 0, i.e. for q

p:

≤

π(ψq) =

Z

π(ψp)dψ−q = π(ψq |

ψq+1 = 0, . . . , ψp = 0).

5.2 Controlling shrinkage properties

The given sequential approach implies that the prior for partial autocorrelations of different lags have
the same functional form, but potentially different rate parameters. The next step is to determine a
reasonable criterion to choose the rate θp in (8). Our suggestion is motivated by the conditional
variance of the one-step ahead forecast error for an AR(p) with ﬁxed p,

Var ((xt+1 −

ˆxt+1)

xs≤t, τ ) = τ −1(1

ψ2

1)(1

ψ2
2)

(1

ψ2

p),

|
ψ2

−
k is an non-decreasing function with k. We assume that

· · ·

−

−

and the observation that often 1

−
k) = 1

ψ2

E(1

−

(1

−

−

a)bk−1,

a, b

∈

[0, 1],

k = 1, . . . , p,

so the one-step ahead prediction, a priori, is non-decreasing with k. This reduces the prior speciﬁca-
tion into two parameters a and b, which have to be speciﬁed by the user. The parameter a represents
1) = a. The choice b = 1 induces the same shrinkage for all ψk while
the initial expectation E(1

ψ2

−

10

b < 1 gives increasing shrinkage for increasing k. For given values of a and b, the corresponding
value for the rate parameter in (8) is found by solving

E(1

−

ψ2

k) =

θk√π
2

θ2
k
4

exp

+ log

erfc

θk
2

= 1

(1

−

−

a)bk−1

(9)

(cid:18)
for each k = 1, . . . , p, where erfc(z) denotes the complementary error function

(cid:19)(cid:19)(cid:19)

(cid:18)

(cid:18)

erfc(z) =

∞

e−t2

dt.

2
√π

z

Z

6 Simulation results

To illustrate the properties of PC priors for the partial autocorrelations of autoregressive processes,
we conduct a simulation study in which an AR(3) process is ﬁtted to six different test cases. Except
for the two ﬁrst cases, the test examples are similar to the ones used in Liseo and Macaro (2013). In
each case, we ﬁt an AR(3) model to generated time series of length n = 50.

0.6, 0)

−

0.2,

Test cases
PC prior (a = b = 0.5)
1. ψ = (0, 0, 0)
2. ψ = (0.7, 0, 0)
3. ψ = (0.2, 0.3, 0)
4. ψ = (
5. ψ = (0.5,
−
6. ψ = (0.5,
−
Reference prior
1. ψ = (0, 0, 0)
2. ψ = (0.7, 0, 0)
3. ψ = (0.2, 0.3, 0)
4. ψ = (
5. ψ = (0.5,
6. ψ = (0.5,

−
0.3, 0)
0.3,

−
0.3, 0)
0.3,

0.2,

−

−

0.6, 0)

−
−

0.1)

−

0.1)

Root mean squared error
rmse1
rmse2
rmse3

Coverage (95%)
ˆζ1

ˆζ2

ˆζ3

0.133
d
0.106
0.174
0.070
0.093
0.092

0.146
0.101
0.185
0.070
0.092
0.088

0.123
d
0.118
0.150
0.123
0.136
0.146

0.151
0.143
0.143
0.111
0.133
0.143

0.111
d
0.103
0.106
0.108
0.107
0.118

0.135
0.126
0.130
0.133
0.130
0.133

0.928
0.912
0.888
0.953
0.938
0.937

0.911
0.911
0.879
0.949
0.939
0.938

0.919
0.961
0.882
0.921
0.918
0.892

0.901
0.932
0.920
0.933
0.923
0.916

0.956
0.968
0.968
0.959
0.965
0.950

0.931
0.944
0.929
0.934
0.938
0.928

Table 1: The root mean squared error and the frequentistic coverage of 95% highest posterior den-
sity intervals for each of the estimated partial autocorrelations of AR(3) processes, using PC priors
with a = b = 0.5 and the reference prior, respectively. The given results are averaged over 1000
simulations, and the time series length in each simulation is n = 50.

The results using m = 1000 simulations, are displayed in Table 1, where the average root-mean

squared error is

rmsei =

1
m

m

( ˆψi −

Xj=1

ψi)2,

i = 1, 2, 3.

v
u
u
t

d

We also report frequentistic coverage, ˆζi, i = 1, 2, 3, of the estimated 95% highest posterior density
intervals. In all test examples, the PC prior was implemented with scaling a = b = 0.5. By solving
(9), this corresponds to using rate parameters (θ1, θ2, θ3)
(0.87, 1.94, 3.33) in estimating the three
partial autocorrelations.

≈

11

As expected, the results illustrate that the use of PC priors avoid overﬁtting. In the ﬁrst test case
of simulating white noise, the PC prior is seen to give both smaller root mean squared error and
better frequentistic coverage, compared with using the reference prior. We also notice that using
PC priors gives better results in estimating ˆψ3 for all the test cases. For the other parameters, the
PC and reference priors are seen to have quite comparable performance. This implies that the PC
prior seems like a promising alternative to reference priors in estimating the partial autocorrelations
of AR(p) processes. The main advantage of PC priors is that these are easy to compute, also for
higher-order processes, and more ﬂexible than the reference prior, allowing for individual scaling.
In comparing the two priors, we also considered the forecast error and coverage of 95% highest
posterior density intervals for one-step ahead predictions. The results were very similar using the
PC and reference priors and we do not report these here.

The given results are not surprising. Especially, the approach of scaling the PC priors is designed
to reﬂect decreasing partial autocorrelations as the order of the process is increased.
If we have
reasons to believe that the partial autocorrelations do not decrease with higher order, we suggest to
scale the priors for the partial autocorrelations of all lags similarly, using b = 1. We have chosen to
report results only using a = b = 0.5 but have also tried several other combinations of the scaling
parameters a and b. The main impression is that the PC priors are quite robust to different choices
of a and b. Also, it is easy to understand how changes in these parameter will induce changes in the
estimates. A larger value of a and/or smaller value of b give more shrinkage to 0. In general, we
recommend that a is chosen to be less or equal to 0.5 as higher values of a might impose too must
shrinkage for the ﬁrst-lag partial autocorrelation. Also, values of b less than 0.5 might impose too
much shrinkage for the partial autocorrelations of higher lags.

7 Discussion

An important aspect of statistical model ﬁtting is to select models that are ﬂexible enough to capture
true underlying structure but do not overﬁt. Among competing models we would prefer the more
parsimonious one, for example in terms of having fewer assumptions, fewer model components or a
simpler structure of model components. Hawkins (2004) describes overﬁtting in terms of violating
the principle of parsimony given by Occam’s razor, the models and procedures used should contain
all that is necessary for the modeling but nothing more. The given PC priors obey this principle,
ensuring shrinkage to speciﬁc base models chosen to reﬂect the given application.

The PC priors represent a weakly informative alternative to existing prior choices for autoregres-
sive processes, allowing for user-deﬁned scaling to adjust the informativeness of the priors. The PC
priors are computationally simple and are easily implemented for any ﬁnite order p of the autore-
gressive process. The priors are available within the R-INLA framework, in which AR processes
can be used as building blocks within the general class of latent Gaussian models (Rue et al., 2009).
This class of models have many applications, among others including analysis of temporal and spa-
tial data. A natural extension in time series applications is to derive PC priors also for autoregressive
(integrated) moving average processes. Other useful model extensions would include vector au-
toregressive models (Sims, 1980), frequently used to analyse multivariate time series, for example
within the ﬁelds of econometrics.

In this paper, we have only considered the stationary case. Previous controversy (Phillips, 1991)
in assigning a prior to the lag-one autocorrelation of an AR(1) process relates to whether the station-
arity condition
< 1 is included, or not. Phillips (1991) argued that objective ignorance priors, like
the Jeffreys prior, should be used for AR(1) processes when no stationarity assumptions are made,
while uniform priors would give inference biased towards stationarity. One of the problem seen with
Jeffreys prior is that it puts most of its probability mass on regions of the parameter space giving a

φ
|
|

12

non-stationary process (Liseo and Macaro, 2013). The reference prior was originally only deﬁned
for stationary process but has been extended in a symmetric way for
> 1 (Berger and Yang,
1994), in which it is seen to have a more reasonable shape than Jeffreys prior (Robert, 2007). A
relevant future project is to study the use of PC priors also for non-stationary AR processes.

φ
|
|

References

Albert, J. H. and Chib, S. (1993). Bayesian inference via Gibbs sampling of autoregressive time
series subject to Markov mean and variance shifts. Journal of Business and Economic Statistics,
11:1–15.

Barndorff-Nielsen, O. and Schou, G. (1973). On the parametrization of autoregressive models by

partial autocorrelations. Journal of Multivariate Analysis, 3:408–419.

Barnett, G., Kohn, R., and Sheather, S. (1996). Bayesian estimation of an autoregressive model

using Markov chain Monte Carlo. Journal of Econometrics, 74:237–254.

Berger, J. O., Bernardo, J. M., and Sun, D. (2009). The formal deﬁnition of reference priors. The

Annals of Statistics, 37:905–938.

Berger, J. O. and Yang, R. (1994). Noninformative priors and Bayesian testing for the AR(1) model.

Econometric Theory, 10:461–482.

Brockwell, P. J. and Davis, R. A. (2002). Introduction to Time Series and Forecasting. Springer-

Verlag, New Work, 2nd edition.

Chatﬁeld, C. (2003). The Analysis of Time Series: An Introduction. Chapman & Hall/CRC, 6th

edition.

Chib, S. (1993). Bayes regression with autoregressive errors: A Gibbs sampling approach. Journal

of Econometrics, 58:275–294.

DeJong, D. N. and Whiteman, C. H. (1991). Reconsidering ‘Trends and random walks in macroe-

conomic time series’. Journal of Monetary Economics, 28:221–254.

Fahrmeir, L. and Tutz, G. (2001). Multivariate Statistical Modelling Based on Generalized Linear

Models. Springer Science + Business Media, New York, 2nd edition.

Fleming, T. R. and Harrington, D. P. (2005). Counting Processes and Survival Analysis. Wiley
Series in Probability and Statistics (Book 625). John Wiley & Sons, Inc., New Jersey, 2nd edition.

Golub, G. H. and van Loan, C. F. (1996). Matrix Computations. Johns Hopkins University Press,

Baltimore, 3rd edition.

Gray, R. M. (2002). Toeplitz and circulant matrices: A review.

Free book available from

http://ee.stanford.edu/

∼

gray, Department of Electrical Engineering, Stanford University.

Hawkins, D. M. (2004). The problem of overﬁtting. Journal of Chemical Information and Computer

Sciences, 44:1–12.

Kullback, S. and Leibler, R. A. (1951). On information and sufﬁciency. The Annals of Mathematical

Statistics, 22:79–86.

13

Lesage, J. P. (1997). Bayesian estimation of spatial autoregressive models. International Regional

Science Review, 20:113–129.

Liseo, B. and Macaro, C. (2013). Objective priors for causal AR(p) with partial autocorrelations.

Journal of Statistical Computation and Simulation, 83:1613–1628.

Manda, S. O. M. and Meyer, R. (2005). Bayesian inference for recurrent events data using time-

dependent frailty. Statistics in Medicine, 24:1263–1274.

Martino, S., Akerkar, R., and Rue, H. (2010). Approximate Bayesian inference for survival models.

Scandinavian Journal of Statistics, 38:514–528.

Martins, T. G., Simpson, D., Lindgren, F., and Rue, H. (2013). Bayesian computing with INLA:

New features. Computational Statistics and Data Analysis, 67:68–83.

Monahan, J. F. (1984). A note on enforcing stationarity in autoregressive-moving average models.

Biometrika, 71:403–404.

Phillips, P. C. B. (1991). To criticize the critics: An objective Bayesian analysis of stochastic trends.

Journal of Applied Econometrics, 6:333–364.

Prado, R. and West, M. (2010). Time Series - Modeling, Computation and Inference. Chapman &

Hall/CRC, Boca Raton.

Robert, C. R. (2007). The Bayesian choice. Springer Science+Business Media, LLC, New York.

Rue, H. and Held, L. (2005). Gaussian Markov Random Fields: Theory and Applications, volume

104 of Monographs on Statistics and Applied Probability. Chapman & Hall, London.

Rue, H., Martino, S., and Chopin, N. (2009). Approximate Bayesian inference for latent Gaussian
models by using integrated nested Laplace approximations (with discussion). Journal of the Royal
Statistical Society, Series B, 71:319–392.

Sahu, S. K. and Bakar, K. S. (2012). Hierarchical Bayesian autoregressive models for large space-
time data with applications to ozone concentration modelling. Applied Stochastic Models in Busi-
ness and Industry, 28:395–415.

Sahu, S. K., Gelfand, A. E., and Holland, D. M. (2007). High resolution space-time ozone modeling

for assessing trends. Journal of the American Statistical Association, 102:1221–1234.

Simpson, D., Rue, H., Riebler, A., Martins, T. G., and Sørbye, S. H. (2016). Penalising model
component complexity: A principled, practical approach to constructing priors (with discussion).
To appear in Statistical Science.

Sims, C. A. (1980). Macroeconomics and reality. Econometrica, 48:1–48.

Sørbye, S. H. and Rue, H. (2014). Scaling intrinsic Gaussian Markov random ﬁeld priors in spatial

modelling. Spatial Statistics, 8:39–51.

West, M. (1991). Kernel density estimation and marginalization consistency. Biometrika, 78:421–

425.

Yau, K. K. W. and McGilchrist, C. A. (1998). ML and REML estimation in survival analysis with

time dependent correlated frailty. Statistics in Medicine, 17:1201–1213.

Zellner, A. (1971). Introduction to Bayesian inference in econometrics. John Wiley & Sons, Inc.,

New York.

14

