Accent Recognition with Hybrid Phonetic Features

Zhan Zhanga, Xi Chena, Yuehai Wanga,∗, Jianyi Yanga

a Department of Information and Electronic Engineering, Zhejiang University, China

1
2
0
2

y
a
M
5

]
S
A
.
s
s
e
e
[

1
v
0
2
9
1
0
.
5
0
1
2
:
v
i
X
r
a

Abstract

The performance of voice-controlled systems is usually inﬂuenced by accented speech. To make these sys-
tems more robust, the frontend accent recognition (AR) technologies have received increased attention in
recent years. As accent is a high-level abstract feature that has a profound relationship with the language
knowledge, AR is more challenging than other language-agnostic audio classiﬁcation tasks. In this paper,
we use an auxiliary automatic speech recognition (ASR) task to extract language-related phonetic features.
Furthermore, we propose a hybrid structure that incorporates the embeddings of both a ﬁxed acoustic model
and a trainable acoustic model, making the language-related acoustic feature more robust. We conduct sev-
eral experiments on the Accented English Speech Recognition Challenge (AESRC) 2020 dataset. The results
demonstrate that our approach can obtain a 6.57% relative improvement on the validation set. We also get
a 7.28% relative improvement on the ﬁnal test set for this competition, showing the merits of the proposed
method.

Keywords: accent recognition, audio classiﬁcation, accented English speech recognition

1. Introduction

With the quick growth of voice-controlled sys-
tems,
speech-related technologies are becoming
part of our daily life. However, the variability of
speech poses a serious challenge to these technolo-
gies. Among the many factors that can inﬂuence
the speech variability, accent is a typical one that
will cause degradation in recognition accuracy[1]
[2][3].

Accent is a diverse pronouncing behavior under
certain languages, which can be inﬂuenced by so-
cial environment, education, residential zone, and
so on. As analyzed in [4], English speakers are con-
structed by not only about 380 million natives, but
also by close to 740 million non-native speakers. In-
ﬂuenced by their native language, the speakers may
have a very wide variety of accents.

To analyze the accent attribute in the collected
speech and make the whole voice-controlled system
more generalized, accent recognition (AR) or accent

∗Corresponding author
Email addresses: zhan_zhang@zju.edu.cn (Zhan

Zhang), chen__xi@zju.edu.cn (Xi Chen),
wyuehai@zju.edu.cn (Yuehai Wang), yangjy@zju.edu.cn
(Jianyi Yang)

classiﬁcation technologies can be applied to cus-
tom the downstream subsystems. Thus, AR tech-
nologies have received increased attention in recent
years.

From the point of deep learning, as accent is an
utterance-level attribute, AR is also a classiﬁcation
task that converts an audio sequence into a certain
class. In this respect, the audio classiﬁcation tasks,
including audio scene classiﬁcation, speaker recog-
nition, and AR, can share similar ideas on network
structures. However, AR is a more challenging task.
Generally, the acoustic scene classiﬁcation or
speaker recognition task can be ﬁnished by using
certain low-level discriminative features. For exam-
ple, speaker recognition can be completed by recog-
nizing the unique timbre (such as the frequency) of
the speaker, which is unrelated to the language they
speak. Thus, both acoustic scene classiﬁcation and
speaker recognition can be language-agnostic tasks.
In contrast, we generally realize someone has a
certain accent when hearing that a speciﬁc pronun-
ciation is diﬀerent from the standard one. There-
fore, for the AR task, to judge whether someone has
a diﬀerent accent, the knowledge of that language is
needed. The discriminative feature of the accented
pronunciation is also more subtle. As a result, we

Preprint submitted to Journal of LATEX Templates

May 6, 2021

 
 
 
 
 
 
think that AR diﬀers from acoustic scene classiﬁ-
cation or speaker recognition because AR requires
a more ﬁne-grained and language-related feature,
which is more diﬃcult compared to the language-
agnostic tasks.

In this paper, ﬁrstly, we review the related AR
papers and systems in Sec.2. We give the proposed
method a detailed description in Sec.3. We use a
CNN-based ASR frontend to encode the language-
related information. Then we append extra self-
attention layers and an aggregation layer to capture
the time sequence relevance and conduct classiﬁca-
tion. We apply the ASR loss as an auxiliary multi-
task learning (MTL)[5] target to extract language-
related acoustic features. To make the acoustic fea-
tures more robust, we fuse the embedding of a train-
able acoustic model that is trained on the accented
dataset and a ﬁxed acoustic model trained on the
standard dataset for a hybrid aggregation.

We conduct dense experiments to compare the
model architecture and analyze diﬀerent training
methods in Sec.4. Finally, we give our conclusion
in Sec.5.

The contributions of this paper are summarized

as follows.

• We propose a novel hybrid structure to solve
the AR task. Our method adopts an ASR
backbone and fuses the language-related pho-
netic features to perform the AR task along
with ASR MTL.

• We investigate the relationship between ASR
and AR. Speciﬁcally, we ﬁnd that without
the auxiliary ASR task, AR may easily over-
ﬁt to the speaker label in the trainset. The
proposed ASR MTL alleviates such a phe-
nomenon. Moreover, the phonetic acoustic fea-
tures extracted by the hybrid acoustic models
can make the AR model more robust.

• We

experiments

conduct dense

on Ac-
cented English Speech Recognition Challenge
(AESRC) 2020 dataset[6]. The results show
that the recognition accuracy of the proposed
method is 6.57% better than the oﬃcial base-
line on the validation set. We also get a 7.28%
relative improvement on the ﬁnal test set in
the related AESRC competition1.

1Competition

at
https://www.datatang.ai/interspeech2020,
or
[6]. Our team code is Z2 and we rank the 3rd. A detailed
description of the dataset is also available in both of them.

available

Track1

results

are

2. Related Work

In this section, we start from reviewing the re-
lated AR papers. Next, we analyse the other sys-
tems submitted to the AESRC 2020 evaluation.

2.1. Accent Recognition

To solve the AR task, conventional methods in-
cluding [7, 8] adopt Hidden Markov Model (HMM)
to model the time features. [9] integrates both the
speech attribute features and the acoustic cues to
better capture foreign accents.
[10] applies Fisher
Vector (FV) and Vector of Locally Aggregated De-
scriptors (VLAD) methods for the language identi-
ﬁcation task. [11] gives a thorough study about the
encoding methods and loss functions for the lan-
guage recognition task.

Besides the aforementioned works that focus on
the recognition task alone, there are other works
that apply AR as an auxiliary task to boost the
performance of the other main tasks.
[12, 13] ex-
plore the relationship between accent and the ASR
task, and show that the accent recognition task can
lead to a more robust performance of the main ASR
task.

Although research in this area has shown that
AR can be an auxiliary task to boost the ASR per-
formance, when ASR performs as the auxiliary task
for AR, the eﬀects have not been closely examined.
How to combine AR with the ASR task for a better
language-related feature still remains to be investi-
gated. This paucity inspires us to design our novel
hybrid AR model combined with ASR MTL.

2.2. AESRC Systems

As summarized in [6], Transformer[14]

is the
mainstream backbone for the systems submitted to
AESRC. There are also teams that use ResNet[15]
or TDNN[16] to model the accent. Besides the deep
learning methods, a combination of neural network
and support vector machine (SVM)[17] is also ap-
plied for AR by a team.

As reported, data augmentation plays an impor-
tant role in the AR training process. SpecAug[18]
is used by many teams.
In addition, pitch shift,
noise augment, and speed perturb are also applied.
It is worth noticing that the 1st team uses the
provided data to train an accented text-to-speech
(TTS) system[19]. They synthesize extra accented
training audio clips with this TTS system, and the
accuracy of AR is improved by 10% absolutely.

2

Figure 1: Proposed hybrid structure for accent recognition. We list the dataset used for the acoustic model (AM ) in the bracket
and use the gray color to indicate the ﬁxed acoustic model does not participant in the AR training process. The auxiliary
branch plotted in dash line (the green block) is used only during training.

Module
FBank
AMt
Linear
AMf
Fusion Block
Conv1D(kernelsize=1)
Self-Attention Aggregation
Linear

Output Shape Output Description
Tin × df bank
T × demb
T × dphoneme
T × demb
T × demb
T × dattn
1 × dattn
1 × daccent

Input feature, df bank = 40
Trainable embedding, Aasr, demb = 1024
Phoneme prediction, ˆw, dphoneme = 40
Reference embedding, AR
Merged embedding, AM
asr
Reshaped embedding for aggregation, dattn = 256
Aggregated feature for classiﬁcation, Ac
Predicted accent class, ˆc, daccent = 8

asr

Table 1: Output summary of the proposed model. Tin is the length of the input sequence. Note that the acoustic model will
downsample the feature sequence by 2 on the time domain, i.e., Tin = 2T .

The submitted systems also show that

the
language-related information is of vital importance
to train the AR task. For example, the 1st team
uses phone posteriorgrams (PPG) features gener-

ated by a Kaldi[20] ASR system as the model input.
The 2nd team concatenates the accent label with
the text label sequence for a sequence-to-sequence
training.

3

Aggregation ModelTrainableAcoustic Model𝐴𝐴𝑀𝑀𝑡𝑡(AESRC+Librispeech)Fixed Acoustic Model𝐴𝐴𝑀𝑀𝑓𝑓(Librispeech)FBankInputTrainableEmbedding𝐴𝐴𝑙𝑙𝑙𝑙𝑙𝑙ReferenceEmbedding𝐴𝐴𝑙𝑙𝑙𝑙𝑙𝑙𝑅𝑅MergedEmbedding𝐴𝐴𝑙𝑙𝑙𝑙𝑙𝑙𝑀𝑀Transformer EncoderMean Std PoolingPositional EncodingClassificationFeature𝐴𝐴𝑐𝑐GTAccent Class𝑐𝑐CE LossGTPhonemes𝑤𝑤CTC LossFeature ExtractorGround TruthTrainable WeightInput & OutputFixed WeightConv1DFusion BlockMain Branch (AR)Auxiliary Branch (ASR)LinearPredictedPhonemes�𝑤𝑤PredictedAccent Clasŝ𝑐𝑐LinearASR MTLweight=𝜆𝜆(a)

(b)

(c)

Figure 2: Fusion block with diﬀerent structures. (a) Adding-based fusion. (b) Concatenation-based fusion. (c) Concatenation-
ChannelAttention-based fusion.
In (b)(c), a Conv1D layer with kernelsize=1 is adopted to cast the concatenated channels
2 ∗ demb back to demb.

Among these teams, our system ranked the 3rd.
We also adopt a phoneme classiﬁcation task to ex-
tract the language-related information. Besides, we
pay more attention to the relationship between AR
and ASR. We propose a novel hybrid structure to
fuse the phonetic features for AR, which will be
discussed in the next section.

3. Proposed Method

In this section, we ﬁrst start from a baseline that
directly models the relationship between the input
spectrum and the accent label. However, such a
baseline does not make full use of the spoken words.
We analyze that making use of the text information
can help the accent modeling in the next subsection.
We further propose our ASR MTL method which
can incorporate the extra text information. Finally,
we analyze the ASR learning target and propose
to use hybrid acoustic models for a more robust
prediction. The whole model structure is shown in
Fig.1.

3.1. Network Structure

To summarize, the acoustic scene classiﬁcation,
speaker recognition, and AR tasks discussed in
Sec.1 all process a sequence into a certain label.

As the input feature is generally spectrums, which
can be viewed as images, intuitively, we may con-
sider using computer vision (CV) models including
VGG[21] or ResNet[15] for classiﬁcation. However,
as the AR task needs more ﬁne-grained features, we
choose to adopt an ASR-based backbone to extract
the high-level features for each frame, and then ag-
gregate them for the ﬁnal accent prediction.

The attention mechanism can be a good choice
to aggregate the extracted acoustic features, and
there are also attention-based aggregation meth-
ods successfully applied to the accent classiﬁcation
task, including [10] and the self-attention pooling
layer [11]. However, a single self-attention pooling
layer appended to the acoustic model may not be
enough. As we also perform an auxiliary ASR task
(discussed in the next subsection) for the acoustic
model, the acoustic model still needs to adapt from
In other words, a
the ASR task to the AR task.
single pooling layer may be too shallow to transfer
the task.

For our method, we tend to keep the aggregation
model independent from the acoustic model so that
the ASR loss applied to the acoustic model can be
more eﬃcient. Thus, we choose to stack the multi-
head self-attention layers for a high-level extraction
of the accent attribute for each frame. This extrac-

4

TrainableEmbeddingReferenceEmbeddingLinearLinearMergedEmbedding𝑇𝑇×𝑑𝑑𝑚𝑚𝑚𝑚𝑒𝑒𝑇𝑇×𝑑𝑑𝑚𝑚𝑚𝑚𝑒𝑒𝑇𝑇×𝑑𝑑𝑚𝑚𝑚𝑚𝑒𝑒TrainableEmbeddingReferenceEmbeddingLinearLinearMergedEmbeddingCatConv1D𝑇𝑇×𝑑𝑑𝑚𝑚𝑚𝑚𝑒𝑒𝑇𝑇×𝑑𝑑𝑚𝑚𝑚𝑚𝑒𝑒𝑇𝑇×(2∗𝑑𝑑𝑚𝑚𝑚𝑚𝑒𝑒)𝑇𝑇×𝑑𝑑𝑚𝑚𝑚𝑚𝑒𝑒TrainableEmbeddingReferenceEmbeddingLinearLinearMergedEmbeddingCatConv1D𝑇𝑇×𝑑𝑑𝑚𝑚𝑚𝑚𝑒𝑒𝑇𝑇×𝑑𝑑𝑚𝑚𝑚𝑚𝑒𝑒𝑇𝑇×(2∗𝑑𝑑𝑚𝑚𝑚𝑚𝑒𝑒)ChannelAttention1×(2∗𝑑𝑑𝑚𝑚𝑚𝑚𝑒𝑒)𝑇𝑇×𝑑𝑑𝑚𝑚𝑚𝑚𝑒𝑒tor is, in fact, the encoder part of Transformer[14].
Formally, the attention mechanism can be de-

scribed as,

Attention(Q, K, V ) = sof tmax

(cid:19)

(cid:18) QK T
√
dk

V,

(1)

(cid:17)

(cid:16) QKT
√
dk

where sof tmax
is the dot-product similar-
ity of the query matrix Q and key matrix K T
), constructing the weight of the
(scaled by
value matrix V . The multi head attention (MHA)
is described as,

1√

dk

M HA(Q, K, V ) = concat(hd1, ..., hdh)W O,

(2)

, V W V

i , KW K
i

where hdi = Attention(QW Q
i ). h is
the head number, and W Q, W K, W V , W O is the
projections matrices for query, key, value, and out-
put, correspondingly. For the self-attention case,
Q = K = V = x, where x is the input features for
each attention layer.

Compared with the self-attention pooling layer,
multiple attention heads help the model to capture
diﬀerent representation subspaces at diﬀerent posi-
tions of the accent.

After the stacked multi-head self-attention lay-
ers, we use a statistic pooling layer[22] to compute
the mean and standard deviation across the time
dimension of the acoustic sequence. Finally, we use
a fully-connected layer to project the aggregated
features into the ﬁnal accent prediction.

In this paper, we use a CNN-based acous-
tic model called Jasper[23] to extract deep pho-
netic features from the input 40-dim FBank spec-
trum. Jasper is constructed by multi Jasper blocks,
and each Jasper block is further constructed by
multi CNN-based subblocks (1D Conv, BatchNorm,
ReLU, dropout) with residual connections. We use
the Jasper 5x3 version (5 Jasper blocks, each Jasper
block has 3 sub-blocks) for our experiments. For
the aggregation model, we set the attention dim
dattn = 256, the feed-forward dim df f = 1024, the
head number h = 4, and the number of stacked
multi-head attention layers nlayers = 3.

3.2. Multi Task Learning

As discussed in the aforementioned Sec.1, AR re-
quires a more ﬁne-grained feature compared with
acoustic scene classiﬁcation and speaker recogni-
tion. Meanwhile, acoustic scene classiﬁcation and
speaker recognition can be a language-agnostic

5

task, which is easier compared to the language-
related AR task. Consequently, the AR task may
degrade to the speaker recognition task if we do not
set extra constrains to the learning targets.

Formally, if we denote the model parameters as
θ, the acoustic features for classiﬁcation as Ac, the
accent label as c, the predicted accent class ˆc can
be described as,

ˆc = argmaxcP (c|Ac; θ), c ∈ {ci, ..., cn},

(3)

where n is the number of accents in the dataset.

We use the cross-entropy (CE) loss as the classi-
ﬁcation loss function between the predicted accent
class ˆc and the ground truth label c,

lc = CE(c, ˆc).

(4)

However, we should note that for the training
dataset, the accent attribute is labelled for each
speaker. In other words, each accent label cj can
be inferred from the speaker label si,

cj = Accent(si), 1 ≤ i ≤ m, 1 ≤ j ≤ n,

(5)

where m is the total number of speakers in the
training dataset.

As a result, if we only apply the cross-entropy loss
for the accent classiﬁcation, a possible solution for
the network is to simply memorizes each speaker
s in the training dataset and learn the mapping
between the predicted speaker ˆs(cid:48) and the accent
label ˆc(cid:48) by using the following Eq.6 and Eq.5:

ˆs(cid:48) = argmaxsP (s|Ac; θ), s ∈ {si, ..., sm}.

(6)

In other words, the network will overﬁt to the
speaker label on the trainset. For unseen speakers,
the AR performance will be severely degraded. We
demonstrate this phenomenon in Sec.4.3.

We assume that accent recognition using speaker-
invariant features is more accurate than the method
via speaker recognition, and we force the network
to learn the language-related information. On the
one hand, the text transcription is independent of
the speaker information, and ASR MTL is suitable
for this task. On the other hand, ASR MTL can
oﬀer complementary information for the main AR
task. The ASR task forces the model to learn the
language-related information, and the ﬁne-grained
phonetic feature can contribute to the accent recog-
nition. We build another branch of the proposed
model in Fig.1 to perform the ASR task during

training. The auxiliary ASR branch tries to pre-
dict the pronounced phonemes ˆw based on the ASR
acoustic features Aasr and the parameters of the
ASR acoustic model θasr,

ˆw = argmaxwP (w|Aasr; θasr).

(7)

We convert the text from word-level to phoneme-
level using a grapheme to phoneme (G2P) tool2 and
use CTC[24] as the ASR loss function,

lasr = CT C(w, ˆw).

(8)

For the proposed model, the feature for accent
classiﬁcation, Ac, is aggregated from the feature for
ASR, Aasr,

Ac = AggregationM odel(Aasr).

(9)

We use the parameter λ to balance the weight be-
tween the ASR task and the AR task,

l = lc + λlasr.

(10)

3.3. Hybrid Phonetic Features

Although the AR performance can be boosted by
the auxiliary ASR task, we should note that there
is a diﬀerence between the ASR target and the AR
target.

Given a certain speech, ASR allows mispronunci-
ations caused by accents and corrects them to the
target text. For example, non-native speakers with
diﬀerent accents may pronounce the target word
“AE P AH L” (APPLE) to “AE P OW L” or “AE
P AO L”. We denote this pronounced phoneme as
AHOW or AHAO. There is no need for ASR mod-
els to distinguish from diﬀerent accents, and ASR
models will ignore diﬀerent accents and still map
both AHOW and AHAO accented speech into the
non-accented transcription of this dataset, AH. In
other words, ASR models will not work hard to ex-
plore the diﬀerences between diﬀerent accents.

On the contrary, AR needs to ﬁnd the diﬀerences
between the AHOW and AHAO referring to the
original AH. This conﬂict inspired us to use an-
other acoustic model trained on the non-accented
dataset for a ﬁxed reference.

For the proposed method, two Jasper acoustic
models are used. The ﬁrst one is trained on the non-
accented dataset (Librispeech). The second one is

2https://github.com/Kyubyong/g2p

6

ﬁrst trained on the non-accented dataset and then
trained on the accented dataset (AESRC, these two
datasets will be introduced later). We pretrain
these two acoustic models with the CTC loss and
keep the model with the lowest validation phone er-
ror rate (PER) for further experiments. We freeze
the weight parameters of the non-accented acoustic
model (AM) and call it the “ﬁxed AM” (denoted as
AMf ). The accented one is further ﬁne-tuned to-
gether with the aggregation model using Eq.10, and
we call it the “trainable AM” (denoted as AMt). As
illustrated in Fig.1, we merge the reference phonetic
embedding of the ﬁxed AM into the trainable one
with a fusion block.

In this paper, we consider three diﬀerent fusion
blocks as illustrated in Fig.2. First, a linear projec-
tion is applied to the embeddings,

A

(cid:48)

asr = linear (Aasr) ,

asr = linear (cid:0)AR
AR(cid:48)

asr

(cid:1) .

(11)

(12)

Adding-based fusion. As illustrate in Fig.2a,
an add function is applied to merge the processed
embeddings.

AM

asr = A

(cid:48)

asr + AR(cid:48)
asr.

(13)

However, simply adding them may lose the infor-
mation of the raw data.

Concatenation-based fusion.

Instead, we
stack the embeddings on the channel domain and
use a Conv1D(kernelsize=1) to half the channels
of the concatenated embedding AC
asr. This struc-
ture is illustrated in Fig.2b. This process can be
described as,

AC

asr = ChannelCat

(cid:16)

A

(cid:48)

asr, AR(cid:48)

asr

(cid:17)

,

asr = Conv (cid:0)AC
AM
asr ∈ RT ×2demb and AM

asr
asr ∈ RT ×demb .

(cid:1) ,

where AC

(14)

(15)

Concatenation-ChannelAttention-based
fusion. Furthermore, we adopt channel-attention
to better control the importance of the refer-
ence embedding as illustrated in Fig.2c. The
channel-attention is obtained by the squeeze-excite
block[25][26]. Based on Eq.14, the concatenated
feature AC
asr goes through a global MaxPooling
and a global AveragePooling on the time domain,

C = T imeM ax(AC

asr) + T imeAve(AC

asr),

(16)

where C ∈ R1×2demb . C is further squeezed and
excited,

Csqueeze = ReLU (linear(C)),

Cexcite = ReLU (linear(Csqueeze)),

(17)

(18)

where Csqueeze ∈ R1× 2demb
We choose r = 16.

r

and Cexcite ∈ R1×2demb .

The channel-attention CA is obtained by the sig-

moid activation of Cexcite,

CA = Sigmoid(Cexcite),

(19)

and used as the scaling factor of AC
domain,

asr on the channel

asr = Conv (cid:0)CA · AC
AM

asr

(cid:1) .

(20)

Finally, for these three fusion methods, we ag-
gregate the merged ASR feature AM
asr instead of
the original Aasr in Eq.9 and optimize the whole
model with the aforementioned MTL loss (Eq.10)
for AR classiﬁcation. We use 39 English phonemes
plus (cid:104)BLANK(cid:105) for CTC classiﬁcation, and there
are 8 accents included in the AESRC dataset. The
detailed model description is summarized in Tab.1.

4. Experiments

In this section, we ﬁrst give a detailed descrip-
tion of the dataset and the experiment environ-
ment. We show the AR results in the following
subsection. Next, we conduct a speaker recognition
test to demonstrate that the auxiliary ASR task can
be of vital importance to keep the AR task from
overﬁtting of the speaker recognition task. Finally,
we explore the relationship between the given tran-
scription for the ASR task and the performance of
the AR task to test the robustness.

4.1. Dataset and Environment

We use two speciﬁed datasets as required by

AESRC for training in our experiments.

The ﬁrst one is Librispeech[27], which does not
contain accent labels. This dataset is constructed
by approximately 1000 hours of 16kHz read English
speech.

The second one is the oﬃcially released dataset
by AESRC, which is composed of 8 diﬀerent
accents,
Indian (IN),
Japanese (JP), Korean (KR), American (US),
British (UK), Portuguese (PT), Russian (RU).

including Chinese (CN),

Each accent is made up of 20 hours of speech data
collected from around 60 speakers on average. The
speech recordings are collected in relatively quiet
environments with cellphones and provided in Mi-
crosoft wav format (16kHz, 16bit and mono). This
dataset is split into trainset, valset, and testset
for the challenge of AR (track1) and ASR (track2)
tasks. Utterances read by certain speciﬁc speakers
are kept for the valset.

We use Pytorch[28] to build our systems. We use
the 40-dim FBank spectrum features extracted by
Kaldi toolkit[20] for the input. Furthermore, we ap-
ply SpecAug[18] to perform the data augmentation.
We conduct the experiments on a server with Intel
Xeon E5-2680 CPU, and 4 NVIDIA P100 GPUs.
We use the Adam optimizer (lr = 10−4) for both
the ASR pretraining and the AR task.

4.2. Accent Recognition Test

We show the accent recognition results on the
validation set of diﬀerent models in Tab.2. We
ﬁrst start from the oﬃcial AESRC baseline sys-
tems. The oﬃcial baseline systems use the en-
coder of Transformer and a statical pooling to per-
form the AR task. In Tab.2, they are denoted as
Transformer-XL, where X represents the number of
the encoder layers. As we can see from Tab.2, a big-
ger model (from Id 1 to Id 3) will lead to overﬁtting
and a degraded classiﬁcation accuracy. However,
as we can observe from both the baseline models
and our models, this overﬁtting phenomenon can
be alleviated by the ASR task. ASR pretraining on
Librispeech and AESRC dataset (Id 3, Id 5) can
greatly improve the AR accuracy compared with
the model without pretraining (Id 2, Id 4, corre-
spondingly). Furthermore, the proposed MTL ver-
sion (Id 6-8) is better than ASR initiation only. We
obtain the best result of the MTL-based models
by setting the ASR weight to λ = 0.1. By merg-
ing the outputs of the ﬁxed AM and the trainable
AM, the proposed hybrid methods (Id 9-11) show
a better performance compared with the MTL ver-
sion. The Concatenation-ChannelAttention-based
fusion method (Id 11) shows the best performance
among these models, which results in a 6.57% rel-
ative improvement over the best oﬃcial ASR ini-
tialized baseline (Id 3). Note that the performance
of US data is relative low. We think this is be-
cause the accented dataset supposes that the data
collected in each country belongs to one type of ac-
cent. Thus, the accent label is decided by which
country the speaker belongs to, rather than their

7

(a)

(b)

(c)

(d)

Figure 3: Diﬀerent training methods. (a) Directly train the whole model for the AR task. (b) Pretrain the acoustic model
for ASR and then train the whole model for AR. (c) Pretrain the acoustic model for ASR and then train the whole model for
AR and ASR MTL (weight=λ). (d) The proposed hybrid method. Based on (c), the non-accented embedding is merged for
reference.

native language. As speakers in the US are various,
the performance for US can be downgraded.

we compare the speaker recognition performances
of these two models.

4.3. Speaker Recognition Test

As discussed above, the model with ASR MTL
is better than the one without the ASR pretraining
(Id 6, Id 4 in Tab.2). To validate our assumption in
Sec.3.2, that directly training the network for AR
may overﬁt to the speaker label on the trainset, and
ASR MTL can help the model to learn a speaker-
invariant feature thus alleviating this phenomenon,

To probe whether the output of these two models
contains speaker information, we freeze the weight
parameters of these two models and replace the ﬁ-
nal linear layer of the aggregation model to perform
the speaker recognition task. We only train this lin-
ear layer to see how the original feature extracted by
each model correlates with the speaker information.
We train the linear layer with the cross-entropy loss

8

Aggregation Model𝐴𝐴𝑀𝑀𝑡𝑡(No Pretraining)FBankInput𝐴𝐴𝑙𝑙𝑙𝑙𝑙𝑙Feature ExtractorAggregation Model𝐴𝐴𝑀𝑀𝑡𝑡(AESRC+Librispeech)FBankInput𝐴𝐴𝑙𝑙𝑙𝑙𝑙𝑙Feature ExtractorASR MTLAggregation Model𝐴𝐴𝑀𝑀𝑡𝑡(AESRC+Librispeech)FBankInput𝐴𝐴𝑙𝑙𝑙𝑙𝑙𝑙Feature Extractorweight=𝜆𝜆ASR MTLAggregation Model𝐴𝐴𝑀𝑀𝑡𝑡(AESRC+Librispeech)𝐴𝐴𝑀𝑀𝑓𝑓(Librispeech)FBankInput𝐴𝐴𝑙𝑙𝑙𝑙𝑙𝑙𝐴𝐴𝑙𝑙𝑙𝑙𝑙𝑙𝑅𝑅𝐴𝐴𝑙𝑙𝑙𝑙𝑙𝑙𝑀𝑀Feature ExtractorFusion Blockweight=𝜆𝜆Id Model
0
Trans 3L
1
Trans 6L
2
Trans 12L
3
Trans 12L(ASR initialized)
4
Ours(No pretraining, Fig.3a)
5
Ours(ASR initialized, Fig.3b)
6
Ours(MTL, Fig.3c, λ = 0.1)
7
Ours(MTL, Fig.3c, λ = 0.2)
8
Ours(MTL, Fig.3c, λ = 0.3)
Ours(Hybrid, Fig.3d,2a,λ = 0.1)*
9
10 Ours(Hybrid, Fig.3d,2b,λ = 0.1)
11 Ours(Hybrid, Fig.3d,2c,λ = 0.1)

US UK CN IN JP KR PT RU Total
48.5
45.7
44.0
30.6
42.7
21.2
73.2
60.2
45.7
45.7
76.3
40.3
71.2
68.6
70.6
57.8
66.1
60.9
72.1
50.4
71.9
61.8
73.9
63.1

70.0
74.5
85.0
93.9
81.0
93.7
91.8
92.0
85.3
95.0
88.9
92.3

56.2
50.9
38.2
67.0
40.1
75.0
86.9
79.5
85.1
83.3
89.6
88.7

30.0
34.0
49.6
75.7
63.6
76.0
76.0
66.2
63.2
77.0
74.5
73.7

45.0
43.7
26.0
55.6
37.8
52.1
56.6
71.3
69.6
73.5
66.0
66.3

54.1
52.2
47.8
76.1
60.0
75.1
79.9
77.5
76.0
80.6
80.8
81.1

57.2
65.7
51.8
85.5
84.5
88.3
89.5
84.7
80.8
92.9
95.1
95.3

83.5
75.2
66.1
97.0
79.1
97.3
99.1
98.5
98.9
99.4
98.9
98.3

Table 2: Validation accuracy on AESRC. (* Our team submitted this adding-based hybrid version for the ﬁnal test of AESRC.)

Transcription

0% Degraded(θ = 0)

50% Degraded(θ = 0.5)

100% Degraded(θ = 1)

Random

Id Model US UK CN IN JP KR PT RU Total
99.1
12 MTL
98.3
13 Hybrid
99.6
14 MTL
98.6
15 Hybrid
99.5
16 MTL
98.3
17 Hybrid
73.3
18 MTL
88.6
19 Hybrid

89.5
95.3
79.6
94.6
76.7
97.5
73.3
86.6

56.6
66.3
67.2
63.7
58.4
59.6
34.9
43.0

86.9
88.7
91.9
88.3
86.6
78.6
33.6
61.8

68.6
63.1
54.5
50.1
49.0
52.6
28.8
46.6

71.2
73.9
62.6
72.9
56.6
72.7
39.1
49.7

79.9
81.1
76.8
79.3
74.7
77.5
51.5
64.8

76.0
73.7
66.2
73.3
77.1
68.9
45.1
68.1

91.8
92.3
93.7
91.8
93.6
90.9
81.9
73.5

Table 3: Validation accuracy for the robustness test under diﬀerent transcription situations. All models use λ = 0.1. For the
hybrid version, we use the channel-attention based fusion (Fig.2c). Note that Id 12, 13 is in fact the same as Id 6, 11 in Tab.2,
correspondingly.

(a)

(b)

Figure 4: Speaker recognition test. (a) Cross-entropy loss curve for the speaker label while training the models with/without
the ASR task. (b) Speaker recognition accuracy curve on the validation dataset while training the models with/without the
ASR task.

and Adam optimizer (lr = 10−4). We should note
that for the original AR experiments, the training
and validation dataset is split by diﬀerent speakers.

To perform the speaker recognition task, we did an-
other splitting by utterances, and a certain speaker
appears in both the training set and the validation

9

0100200300400500600700Step12345LossSpeaker Recoginition CrossEntropy Loss (Train)ModelOurs w/ ASROurs w/o ASR0100200300400500600700Step0.100.150.200.250.300.350.40AccuracySpeaker Recoginition Accuracy (Validate)ModelOurs w/ ASROurs w/o ASR(a)

(b)

Figure 5: 2D embeddings of the accent features. (a) The baseline version without the auxiliary ASR task. (b) The MTL
version.

set.

We show the training process in Fig.4a and the
validation process in Fig.4b. The model without
ASR pretraining has a lower training cross-entropy
loss for the speaker label and a much higher speaker
recognition accuracy compared to the ASR MTL
version. Such a phenomenon suggests that adding
the ASR task indeed helps the model to learn a
speaker-invariant feature, and this feature is more
powerful to solve the AR task as shown in Tab.2.
We also plot the embeddings of the predictions on
the validation dataset in Fig.5 using t-SNE[29]. The
embeddings learned by the MTL version are located
in a more reasonable subspace.

4.4. Robustness Test

As noted in Sec.3.3, if the ASR dataset is con-
structed by non-native speakers with accents, the
transcription of certain phonemes may be labelled
as the text that the speakers are asked to read,
rather than the pronounced one.
In other words,
pronunciations with diﬀerent accents are merged
and all labelled as the same text. If the model is
dominated by the auxiliary ASR task, it will be
hard for the model to ﬁnd the uniqueness for diﬀer-
ent accents. As a result, the performance may be
degraded for the AR task. In this subsection, we
set experiments to validate the performance of the
proposed hybrid model under diﬀerent ASR situa-
tions.

To simulate the transcription confusion intro-
duced by accents, we randomly map a certain
phoneme w to the upper-level group it belongs to
(denoted as G(w)), with the probability p sampled
from a uniform distribution U (0, 1). The hierar-
chy of phonemes in English language can be seen in
Fig.6, and is commonly used by [30][31][32]. For-
mally, this process can be denoted as follows,

(cid:40)

w(cid:48) =

G(w)
w

if p < θ
otherwise

, p ∼ U (0, 1).

(21)

As θ increases, there will be more pronunciations
that are not labelled as the original phonemes, but
are messed into the upper group. We use the
group index for classiﬁcation instead of the origi-
nal phoneme index.

Meanwhile, to test the eﬀect of the phonetic in-
formation for AR, we also test an extreme situa-
tion that the text transcriptions are randomly gen-
erated. We still use the MTL version (Fig.3c) for
experiment, but the trainable acoustic model AMt
is not pretrained on AESRC or Librispeech. Under
this situation, the whole model learns the wrong
phonetic information of the accented speech. We
also test the proposed hybrid version (Fig.3d, AMt
is also not pretrained) for comparison.

As we can see from Tab.3, how precise the tran-
scription is indeed has an impact on the AR accu-
racy. For the simulated dataset, if more phonemes
are messed, the performances of both models will

10

10.07.55.02.50.02.55.07.510.010.07.55.02.50.02.55.07.510.0Acc=60.0%USUKCNINJPKRPTRU10.07.55.02.50.02.55.07.510.010.07.55.02.50.02.55.07.510.0Acc=79.9%USUKCNINJPKRPTRUFigure 6: The hierarchy of phonemes in English language.

be downgraded. For the MTL version, learning the
wrong phonetic information (Id 18) is even worse
than the one without ASR pretraining (Id 4 in
Tab.2), suggesting that the phonetic information
is helpful for the accent recognition. Meanwhile,
for the proposed hybrid method, the ﬁxed AM can
perform the ASR task for the accented speech in-
dependently, which is not aﬀected by the messed
text, making the model more robust. To see the
eﬀect, we plot the channel-attention of the normal
situation (Id 13) and the extreme situation (Id 19)
in Fig.7. As the random transcription is not helpful
for the AR task, the channel-attention mechanism
learns to pay more attention to the reference em-
bedding in Fig.7b compared to the normal situation
in Fig.7a.

5. Conclusion

In this paper, we propose a novel hybrid struc-
ture along with ASR MTL to solve the AR task.
The auxiliary ASR task can force the model to
extract speaker-invariant and language-related fea-
tures, which prevents the model from overﬁtting
to the easier speaker recognition task. Further-
more, the hybrid structure is designed to fuse
the embeddings of two acoustic models.
The
Concatenation-ChannelAttention-based fusion can
make the extracted features more robust. The pro-
posed method is 6.57% better than the oﬃcial base-
line on the validation set and gets a 7.28% rela-
tive improvement on the ﬁnal test set in the re-
lated AESRC competition, showing the merits of
our method.

(a) ChannelAttention for the model trained with normal tran-
scription. Summed attention weight for the reference embedding
dividing summed attention weight for the trainable embedding
is ((cid:80)2demb

CAc)/((cid:80)demb−1

CAc) = 1.02.

c=0

c=demb

(b) ChannelAttention for the model trained with randomly gen-
erated transcription. Summed attention weight for the reference
embedding dividing summed attention weight for the trainable
embedding is ((cid:80)2demb

CAc)/((cid:80)demb−1

CAc) = 1.63.

c=0

c=demb

Figure 7: Channel attention weight (CA ∈ R1×2demb ) un-
der diﬀerent training conditions. When the transcription
becomes unreliable, the channel-attention block pays more
attention to the reference embedding.

11

ObstruentPhonemeAspirateFricativeAffricateStopSonorantsNasalVowelSemivowelLiquidAA AE AHUWAO AWAY UHEHER EY OY IH IY OWW YL RM N NGB D GK P TCH JHDH F S SH TH V Z ZHHH0102320470.20.40.60.80102320470.20.40.60.8References

[1] X. Chu, E. Combs, A. Wang, M. Picheny, Accented
speech recognition inspired by human perception.
URL https://arxiv.org/pdf/2104.04627

[2] C. Huang, T. Chen, S. Li, E. Chang, J. Zhou, Analysis
of speaker variability, EUROSPEECH 2001 - SCANDI-
NAVIA - 7th European Conference on Speech Commu-
nication and Technology (49) (2001) 1377–1380.

[3] J. Levis, T. Barriuso, Nonnative speakers’ pronuncia-
tion errors in spoken and read english, Proceedings of
Pronunciation in Second Language Learning and Teach-
ing (PSLLTP 3 (2011) 187–194.

[4] T. Viglino, P. Motlicek, M. Cernak, End-to-end ac-
cented speech recognition, Proceedings of the Annual
Conference of the International Speech Communication
Association, INTERSPEECH 2019-Septe (2019) 2140–
2144. doi:10.21437/Interspeech.2019-2122.

[5] M. Crawshaw, Multi-task learning with deep neural net-

works: A survey.
URL https://arxiv.org/pdf/2009.09796

[6] X. Shi, F. Yu, Y. Lu, Y. Liang, Q. Feng, D. Wang,
Y. Qian, L. Xie, The accented english speech recogni-
tion challenge 2020: Open datasets, tracks, baselines,
results and methods.
URL https://arxiv.org/pdf/2102.10233

[7] K. Mannepalli, P. N. Sastry, M. Suman, Mfcc-gmm
based accent recognition system for telugu speech sig-
nals, International Journal of Speech Technology 19 (1)
(2016) 87–93. doi:10.1007/s10772-015-9328-y.

[8] H. Behravan, V. Hautamaki, S. M. Siniscalchi, T. Kin-
nunen, C. H. Lee, I-vector modeling of speech attributes
for automatic foreign accent recognition, IEEE/ACM
Transactions on Audio Speech and Language Process-
ing 24 (1) (2016) 29–41.
doi:10.1109/TASLP.2015.
2489558.

[9] S. M. Siniscalchi, T. Kinnunen, C.-h. Lee, H. Behra-
van, V. Hautam, Introducing attribute features to for-
eign accent recognition school of computing , university
of eastern ﬁnland , ﬁnland faculty of architecture and
engineering , university of enna “ kore ”, italy (2014)
5332–5336.

[10] J. Chen, W. Cai, D. Cai, Z. Cai, H. Zhong, M. Li, End-
to-end language identiﬁcation using netfv and netvlad,
2018 11th International Symposium on Chinese Spo-
ken Language Processing, ISCSLP 2018 - Proceed-
ings (August) (2018) 319–323. doi:10.1109/ISCSLP.
2018.8706687.

[11] W. Cai, J. Chen, M. Li, Exploring the encoding layer
and loss function in end-to-end speaker and language
recognition system, arXiv (April).
doi:10.21437/
odyssey.2018-11.

[12] M. Najaﬁan, M. Russell, Automatic accent identiﬁca-
tion as an analytical tool for accent robust automatic
speech recognition, Speech Communication 122 (June)
(2020) 44–55. doi:10.1016/j.specom.2020.05.003.
[13] F. Weninger, Y. Sun, J. Park, D. Willett, P. Zhan, Deep
learning based mandarin accent identiﬁcation for accent
robust asr, Proceedings of the Annual Conference of
the International Speech Communication Association,
INTERSPEECH 2019-Septe (2019) 510–514. doi:10.
21437/Interspeech.2019-2737.

[14] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,
L. Jones, A. N. Gomez, L. Kaiser, I. Polosukhin, At-

12

tention is all you need.
URL http://arxiv.org/pdf/1706.03762v5

[15] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning

for image recognition.
URL https://arxiv.org/pdf/1512.03385

[16] K. J. Lang, A. H. Waibel, G. E. Hinton, A time-delay
neural network architecture for isolated word recog-
nition, Neural Networks 3 (1) (1990) 23–43.
doi:
10.1016/0893-6080(90)90044-L.

[17] N. Cristianini, J. Shawe-Taylor, An introduction to
Support Vector Machines: And other kernel-based
learning methods / Nello Cristianini and John Shawe-
Taylor, Cambridge University Press, Cambridge, 2000.
[18] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph,
E. D. Cubuk, Q. V. Le, Specaugment: A simple data
augmentation method for automatic speech recogni-
tion, in: Interspeech 2019, ISCA, ISCA, 15-19 Septem-
ber 2019, pp. 2613–2617. doi:10.21437/Interspeech.
2019-2680.

[19] H. Huang, X. Xiang, Y. Yang, R. Ma, Y. Qian,
Aispeech-sjtu accent identiﬁcation system for the ac-
cented english speech recognition challenge.
URL https://arxiv.org/pdf/2102.09828

[20] D. Povey, A. Ghoshal, G. Boulianne, L. Burget,
O. Glembek, N. Goel, M. Hannemann, P. Motl´ıˇcek,
Y. Qian, P. Schwarz, J. Silovsk´y, G. Stemmer, K. Vesel,
The kaldi speech recognition toolkit, IEEE 2011 Work-
shop on Automatic Speech Recognition and Under-
standing.

[21] K. Simonyan, A. Zisserman, Very deep convolutional

networks for large-scale image recognition.
URL https://arxiv.org/pdf/1409.1556

[22] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey,
S. Khudanpur, X-vectors : Robust dnn embeddings for
speaker recognition david snyder , daniel garcia-romero
, gregory sell , daniel povey , sanjeev khudanpur center
for language and speech processing & human language
technology center of excellence the johns hopkins un,
Icassp2018 (2018) 5329–5333.

[23] J. Li, V. Lavrukhin, B. Ginsburg, R. Leary,
O. Kuchaiev, J. M. Cohen, H. Nguyen, R. T. Gadde,
Jasper: An end-to-end convolutional neural acoustic
model.
URL http://arxiv.org/pdf/1904.03288v3

[24] A. Graves, S. Fern´andez, F. Gomez, J. Schmidhuber,
Connectionist temporal classiﬁcation: Labelling unseg-
mented sequence data with recurrent neural networks,
ACM International Conference Proceeding Series 148
(2006) 369–376. doi:10.1145/1143844.1143891.
[25] J. Hu, L. Shen, G. Sun, Squeeze-and-excitation net-
works, in: 2018 IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR 2018), IEEE,
Piscataway, NJ, 2018, pp. 7132–7141. doi:10.1109/
CVPR.2018.00745.

[26] S. Woo, J. Park, J.-Y. Lee,

I. S. Kweon, Cbam:
Convolutional block attention module, in: V. Ferrari,
M. Hebert, C. Sminchisescu, Y. Weiss (Eds.), Com-
puter vision – ECCV 2018. Part VII, Vol. 11211 of
LNCS sublibrary. SL 6, Image processing, computer
vision, pattern recognition, and graphics, Springer,
Cham, Switzerland, 2018, pp. 3–19.
doi:10.1007/
978-3-030-01234-2{\textunderscore}1.

[27] V. Panayotov, G. Chen, D. Povey, S. Khudanpur, Lib-
rispeech: An asr corpus based on public domain au-
dio books, ICASSP, IEEE International Conference on

Acoustics, Speech and Signal Processing - Proceedings
2015-Augus (2015) 5206–5210. doi:10.1109/ICASSP.
2015.7178964.

[28] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Brad-
bury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein,
L. Antiga, A. Desmaison, A. K¨opf, E. Yang, Z. De-
Vito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner,
L. Fang, J. Bai, S. Chintala, Pytorch: An imperative
style, high-performance deep learning library.
URL http://arxiv.org/pdf/1912.01703v1

[29] L. V. D. Maaten, Geoﬀrey E. Hinton, Visualizing data
using t-sne, Journal of Machine Learning Research 9
(2008) 2579–2605.

[30] H. Hamooni, A. Mueen, Dual-domain hierarchical clas-
siﬁcation of phonetic time series, in: 2014 IEEE Inter-
national Conference on Data Mining, IEEE, 122014, pp.
160–169. doi:10.1109/ICDM.2014.92.

[31] O. Dekel, J. Keshet, Y. Singer, An online algorithm for
hierarchical phoneme classiﬁcation, in: D. Hutchison,
T. Kanade, J. Kittler, J. M. Kleinberg, F. Mattern,
J. C. Mitchell, M. Naor, O. Nierstrasz, C. Pandu Ran-
gan, B. Steﬀen, M. Sudan, D. Terzopoulos, D. Tygar,
M. Y. Vardi, G. Weikum, S. Bengio, H. Bourlard (Eds.),
Machine Learning for Multimodal Interaction, Vol. 3361
of Lecture Notes in Computer Science, Springer Berlin
Heidelberg, Berlin, Heidelberg, 2005, pp. 146–158. doi:
10.1007/978-3-540-30568-2{\textunderscore}13.
[32] K.-F. Lee, H.-W. Hon, Speaker-independent phone
recognition using hidden markov models, IEEE Trans-
actions on Acoustics, Speech, and Signal Processing
37 (11) (1989) 1641–1648. doi:10.1109/29.46546.

13

