PERIODNET: A NON-AUTOREGRESSIVE WAVEFORM GENERATION MODEL WITH A
STRUCTURE SEPARATING PERIODIC AND APERIODIC COMPONENTS

Yukiya Hono, Shinji Takaki, Kei Hashimoto, Keiichiro Oura, Yoshihiko Nankaku, and Keiichi Tokuda

Department of Computer Science, Nagoya Institute of Technology, Nagoya, Japan

1
2
0
2

b
e
F
5
1

]
S
A
.
s
s
e
e
[

1
v
6
8
7
7
0
.
2
0
1
2
:
v
i
X
r
a

ABSTRACT
We propose PeriodNet, a non-autoregressive (non-AR) waveform
generation model with a new model structure for modeling periodic
and aperiodic components in speech waveforms. The non-AR wave-
form generation models can generate speech waveforms parallelly
and can be used as a speech vocoder by conditioning an acoustic
feature. Since a speech waveform contains periodic and aperiodic
components, both components should be appropriately modeled to
generate a high-quality speech waveform. However, it is difﬁcult to
decompose the components from a natural speech waveform in ad-
vance. To address this issue, we propose a parallel model and a series
model structure separating periodic and aperiodic components. The
features of our proposed models are that explicit periodic and ape-
riodic signals are taken as input, and external periodic/aperiodic de-
composition is not needed in training. Experiments using a singing
voice corpus show that our proposed structure improves the natu-
ralness of the generated waveform. We also show that the speech
waveforms with a pitch outside of the training data range can be
generated with more naturalness.

Index Terms— Neural vocoder, generative adversarial network,
singing voice synthesis, text-to-speech synthesis, signal processing

1. INTRODUCTION

In recent years, speech synthesis technology has rapidly improved
with the introduction of deep neural networks.
In particular,
WaveNet [1], which has an autoregressive (AR) structure, directly
models the distributions of waveform samples and has demonstrated
remarkable performance. WaveNet can be used as a speech vocoder
by conditioning auxiliary features such as Mel-spectrogram and
acoustic features extracted by conventional signal processing-based
vocoder [2]. This is also used in state-of-the-art speech synthe-
sis systems, which greatly contributes to improving the quality of
synthesized speech [3, 4]. However, WaveNet suffers from slow
inference speed because of the AR mechanism and huge network ar-
chitectures. Although compact AR models [5,6] have been proposed
to accelerate inference speed, it is limited because audio samples
must be generated sequentially. Thus, such models are not suited for
real-time TTS applications.

Recently, signiﬁcant efforts have been devoted to building
non-AR models to resolve this problem. Parallel WaveNet [7]
and ClariNet [8] introduce the teacher-student knowledge distilla-
tion. This framework transfers the knowledge from an AR teacher
WaveNet to an inverse autoregressive ﬂow (IAF)-based non-AR
student model [9]. The IAF student model is highly parallelizable
and can synthesize high-quality waveforms. However, the training
procedure is complicated because it requires a well-trained teacher
model as well as a mix of distilling and other perceptual training
criteria. WaveGlow [10] and FloWaveNet [11] with ﬂow-based gen-
erative models have been proposed as well. Although these models

can be directly learned by minimizing the negative log-likelihood of
the training data, they need a huge number of parameters and require
many GPU resources to obtain optimal results for a single speaker
model.

Another approach for parallel waveform generation is to use
generative adversarial networks (GANs) [12]. A GAN is a pow-
erful generative model that has been successfully used in various
research ﬁelds such as image generation [13], speech synthesis [14],
and singing voice synthesis [15]. GAN-based models have also been
proposed for waveform generation [16, 17]. Since these training
frameworks enable models to effectively capture the time-frequency
distribution of the speech waveform and improve training stability,
these GAN-based models are much easier to train than the conven-
tional non-AR methods described above.

A neural vocoder can generate high-ﬁdelity waveforms since it
can restore missing information from the acoustic feature in a data-
driven fashion and is less limited by the knowledge and assumptions
of the conventional vocoder [18, 19]. However, this also results in a
lack of acoustic controllability and robustness. In fact, it is difﬁcult
for a neural vocoder to generate a speech waveform with accurate
pitches outside the range of the training data. Some methods with
explicit periodic signals [20,21] and methods with a pitch-dependent
convolution mechanism [22] address this problem.

It is known that both periodic and aperiodic components are
mixed in speech waveforms. Although neural vocoders often model
speech waveforms as single signals without considering these mixed
components, it is important to take them into account to model
speech waveforms more effectively. In particular, when the neural
vocoder is used in a singing voice synthesis system [23, 24], the
accuracy of pitch and breath sound reproduction has a signiﬁcant
effect on quality and naturalness. Several methods for decompos-
ing the periodic and aperiodic components contained in the speech
waveform have been proposed [25, 26]. However, it is still difﬁcult
to decompose them, and it is not optimal to use decomposed wave-
forms, including decomposition errors, as the training data for the
neural vocoders.

In this paper, we consider speech waveform modeling in terms
of the model structures and propose PeriodNet, a non-autoregressive
neural vocoder for better speech waveform modeling. We introduce
two versions with different model structures, a parallel model and
a series model, assuming that the periodic and aperiodic waveforms
can be generated from the explicit periodic and aperiodic signals,
such as a sine wave and a noise sequence, respectively. Our pro-
posed methods also can generate a waveform that includes a pitch
outside the range of the training data. Moreover, our models have
the robustness of the input pitch since these generate the periodic
and aperiodic waveforms with two separate neural networks.

 
 
 
 
 
 
2. WAVEFORM MODELING

2.1. Autoregressive neural vocoder

In neural vocoders with an AR structure [1, 5, 6], a speech wave-
form at each timestep is modeled as a probability distribution con-
ditioned on past speech samples and auxiliary features such as Mel-
spectrograms and acoustic features. An overview of the AR neural
vocoder is shown in Fig. 1(a). In this paper, we use WaveNet [1]
as a neural network to generate waveforms (referred to as a genera-
tor in this paper). WaveNet has a stack of dilated causal convolution
with a gated activation function, and it is capable of modeling speech
waveforms with complex periodicity. However, there is a problem
that it cannot make a parallel inference and takes time to generate
waveform because of the AR structure.

2.2. Non-autoregressive neural vocoder

In non-AR neural vocoders [7, 8, 10, 11, 16, 17], the neural network
represents the mapping function from a pre-generated input signal,
such as Gaussian noise, to the speech waveform. Hence, all wave-
form samples can be generated in parallel without incurring the ex-
pense of having to make predictions autoregressively. However, it
is difﬁcult to predict a speech waveform with autocorrelation from a
noise sequence without autocorrelation properly. Prior studies [20,
21] have proposed methods that use explicit periodic signals such
as sine waves. These methods provide high pitch accuracy and can
synthesize waveforms with a pitch not included in the training data.
In this paper, following these attempts, we use the sine wave, noise,
and voiced/unvoiced (V/UV) sequence as input signals, as shown in
Fig 1(b). Note that this V/UV sequence is smoothed in advance.
Various architectures can be used for the generator in Fig 1(b); we
use a Parallel WaveGAN [16]-based architecture. Details of model
architectures will be described in Sec. 3.2.

3. PROPOSED MODEL STRUCTURES SEPARATING
PERIODIC AND APERIODIC COMPONENTS

3.1. Model structures

A speech waveform contains periodic and aperiodic waveforms. In
the structure shown in Fig. 1(b), the generation process of the pe-
riodic and aperiodic waveforms is represented by a single model.
However, this structure is not always optimal for waveform model-
ing, especially when the accuracy of pitch and breath sound repro-
duction has a signiﬁcantly affects quality and naturalness, such as in
singing voice synthesis. We assume that the speech waveform is the
sum of periodic and aperiodic components. The periodic and aperi-
odic components are expected to be easily created from the periodic
and aperiodic signal (such as the sine waves and noise sequences),
respectively. Thus, in this paper, we propose a parallel mode struc-
ture and a series model structure based on these assumptions.

The parallel model structure is shown in Fig 1(c). This structure
assumes that the periodic and aperiodic waveforms are independent
of each other. An explicit periodic signal consisting of a sine wave
and V/UV sequence is used to predict the periodic waveform, and an
explicit aperiodic signal consisting of noise and V/UV sequence is
used to predict the aperiodic waveform.

The series model structure is shown in Fig 1(d). In this struc-
ture, we assume that the aperiodic waveform depends on the pe-
riodic waveform, considering the possibility that there is an aperi-
odic waveform corresponding to the phase of the periodic waveform.
Speciﬁcally, we introduce a residual connection between two gener-

(a) AR model

(b) Non-AR baseline model

(c) Non-AR parallel model

(d) Non-AR series model

Fig. 1: Structures for speech waveform modeling

ators so that the latter generator can predict the aperiodic component
taking into account the dependence of the periodic component.

In the parallel model and the series model, different acoustic
features can be selected for the auxiliary features of the periodic and
aperiodic generators, making it possible to obtain more robust neural
vocoders with proper conditioning.

3.2. Model details

In this paper, we incorporate Parallel WaveGAN [16]-based frame-
work into our non-AR baseline and proposed models, as shown in
Fig. 1(b), Fig. 1(c), and Fig. 1(d). Each generator has the same archi-
tecture as the generator of [16], which is a modiﬁed WaveNet-based
model with non-causal convolution. On the other hand, for the dis-
criminators, we utilize a multi-scale architecture with three discrimi-
nators that have identical network structures but operate on different
audio scales, following [17]. Each discriminator has the same ar-
chitecture as the discriminator of [16]. These models are trained by
optimizing the combination of multi-resolution short-time Fourier
transform loss and adversarial loss in the same fashion as [16].

In the training vocoder with the parallel and series model struc-
tures, the ﬁnal output sequence, which is the sum of two generators’
output sequence, is only evaluated. This is the same as the baseline
model with the single model structure. From the assumptions pre-
sented in Sec. 3.1, by inputting the sine wave and noise sequence
separately, each generator should be trained to predict periodic and
aperiodic waveforms, respectively.

4. EXPERIMENTS

4.1. Experimental conditions

Seventy Japanese children’s songs (total: 70 min) performed by one
female singer were used for the experiments. Sixty songs were used
for training, and the rest were used for testing. Singing voice sig-
nals were sampled at 48 kHz, and each sample was quantized by 16
bits. The auxiliary features consisted of 50-dimensional WORLD
mel-cepstral coefﬁcients [19], 25-dimensional mel-cepstral analysis
aperiodicity measures, one-dimensional continuous log fundamental
frequency (F0) value, and one-dimensional voiced/unvoiced binary
code. Feature vectors were extracted with a 5-ms shift, and the fea-
tures were normalized to have zero mean and unit variance before
training.

In the training stage, the sine waves for the input of the non-
AR neural vocoder were generated based on the glottal closure point
extracted from a natural speech using REAPER [27]. The purpose of

PreviousoutputGeneratorOutputAuxiliary featureSineNoiseV/UVGeneratorOutputAuxiliary featureSineV/UVPeriodicGeneratorAperiodicGeneratorOutput+NoiseV/UVAuxiliary featureAuxiliary featurePeriodicGeneratorAperiodicGeneratorOutput+SineV/UVNoiseV/UVAuxiliary featureAuxiliary feature(a) Waveform of the periodic generator’s output

(b) Waveform of the aperiodic generator’s output

(c) Waveform after the sum of two signals

Fig. 2: Spectrograms of generated waveform by non-AR parallel model

(a) Waveform of the periodic generator’s output

(b) Waveform of the aperiodic generator’s output

(c) Waveform after the sum of two signals

Fig. 3: Spectrograms of generated waveform by non-AR series model

this is to input a sine wave that is close in phase to the target’s natural
speech during training. Meanwhile, the sine waves were generated
based on the F0 values in the synthesis stage.

The following seven systems were compared.

• WN: The AR WaveNet [1].

• BM1: The non-AR baseline model, as shown in Fig. 1(b) that
used noise and a V/UV signal as the generator input and is
conditioned on all auxiliary features.

• BM2: The non-AR baseline model, as shown in Fig. 1(b) that
used a sine wave and a V/UV signal as the generator input
and is conditioned on all auxiliary features.

• BM3: The non-AR baseline model, as shown in Fig. 1(b) that
used noise, a sine wave, and a V/UV signal as the generator
input and is conditioned on all auxiliary features.

• PM1: The non-AR parallel model, as shown in Fig. 1(c). The
periodic generator takes a sine wave and a V/UV signal as in-
put, and the aperiodic generator takes noise and a V/UV sig-
nal as input. Both generators are conditioned on all auxiliary
features.

• PM2: The non-AR parallel model, as shown in Fig. 1(c). Un-
like PM1, the aperiodic generator is conditioned by auxiliary
features other than F0.

• SM: The non-AR series model, as shown in Fig. 1(d). The
periodic generator takes a sine wave and a V/UV signal as
input, and the aperiodic generator takes noise, a V/UV signal,
and the output signal of the periodic generator as input. Both
generators are conditioned on all auxiliary features.

WN consisted of 30 layers of dilated residual convolution
blocks with causal convolution. The dilations of WN were set to
1, 2, 4, . . . , 512, and the 10 dilation layers were stacked three times.
The channel size for dilation, residual block, and skip-connection in
WN was set to 256, and the ﬁlter size in WN was set to two. The
singing voice waveforms to train WN were quantized from 16 bits
to 8 bits by using the µ-law algorithm [28].

The generators of BM1, BM2, and BM3, and periodic genera-
tor of PM1, PM2, and SM consisted of 30 layers of dilated residual
convolution blocks with three dilation cycles, the same as WN. The
aperiodic generators of PM1, PM2, and SM consisted of 10 lay-
ers of dilated residual convolution blocks without dilation cycles.
The channel size for dilation, residual block, and skip-connection
was set to 64, and the ﬁlter size was set to three. The discrimina-
tors of BM1, BM2, BM3, PM1, PM2, and SM had the multi-scale
architecture with three discriminators. The discriminators took 48
kHz full-resolution waveforms, and 24 kHz and 16 kHz downsam-
pled waveforms. The downsampling was performed using average
pooling. Each discriminator consisted of 10 non-causal dilated con-
volutions with leaky ReLU activation function. We applied weight
normalization [29] to all convolutional layers.

All models were trained using the RAdam optimizer [30] with
1000K iterations. Speciﬁcally, in BM1, BM2, BM3, PM1, PM2,
and SM, the discriminators were ﬁxed for the ﬁrst 100K iterations,
and then both the generator and discriminator were jointly trained
afterward.

4.2. Comparison of spectrograms

Fig. 2 and Fig. 3 show the spectrograms in PM1 and SM, respec-
tively. Each ﬁgure has three spectrograms of the waveform of the
periodic generator’s output, the aperiodic generator’s output, and the
sum of two predicted signals. Fig. 2(a) and Fig. 2(b) show that the
waveform of the periodic generator contains many harmonic com-
ponents, and that of the aperiodic generator contains the other fre-
quency components. As seen in the highlighted boxes on the left
and in the center, which represent parts of the breath and unvoiced
plosives “/t/”, respectively, it can be seen that the spectra of these
unvoiced sounds only appear in the output of the aperiodic gener-
ator. These tendencies can also be seen in Fig. 3(a) and Fig. 3(b).
These results indicate that two generators in the parallel model and
the series model work on modeling the transformation from the sine
waves and the noise sequence to the periodic and the aperiodic wave-
forms. Comparing the highlighted box in the lower right of Fig. 2(b)

0.01.02.03.04.05.0Time [s]05101520Frequency [kHz]0.01.02.03.04.05.0Time [s]05101520Frequency [kHz]0.01.02.03.04.05.0Time [s]05101520Frequency [kHz]0.01.02.03.04.05.0Time [s]05101520Frequency [kHz]0.01.02.03.04.05.0Time [s]05101520Frequency [kHz]0.01.02.03.04.05.0Time [s]05101520Frequency [kHz]Fig. 4: Subjective evaluation results of experiment 1

Fig. 6: Subjective evaluation results of experiment 3

4.3.2. Comparison of model structures of non-AR neural vocoders

To compare the model structures of non-AR neural vocoders, we
conducted two subjective evaluation experiments using BM3, PM1,
PM2, and SM. In these experiments, the samples were generated by
four vocoders conditioned on two different F0 scales: original and
double scale. In the experiment with the original F0 scale, we also
used the natural waveform NAT for comparison.

The results are presented in Fig. 5 and Fig. 6. These ﬁgures show
that PM1, PM2, and SM attained higher naturalness than BM3. This
indicates that it is effective for the non-AR neural vocoders using the
explicit periodic signal to introduce a parallel or series structure. Al-
though the difference between PM1, PM2, and SM was negligible
when conditioning on the original F0 as shown in Fig. 5, PM2 was
the best performance when conditioning on the doubled F0 as shown
in Fig. 6. The waveform samples generated by BM3, PM1, and SM
tended to contain more aperiodic waveforms than those generated
by PM2. In BM3, the period and aperiodic components were not
modeled separately, and speech waveforms were generated from a
single generator conditioned by auxiliary features including F0. In
PM1 and SM, although the networks for modeling these compo-
nents were separate, both aperiodic generators were conditioned on
auxiliary features including F0. In particular, the aperiodic generator
in SM also depended on periodic waveforms predicted by the peri-
odic generator. Therefore, it was assumed that BM3, PM1, and SM
could not generate aperiodic waveforms when these vocoders took
out-of-range F0 as the acoustic features in the synthesis stage. PM2
is more robust for an unseen F0 outside the F0 range of the training
data because the aperiodic generator in PM2 does not depend on the
periodic signal or F0.

5. CONCLUSIONS

We introduced PeriodNet, a non-AR neural vocoder with new model
structures, to appropriately modeling the periodic and aperiodic
components in the speech waveform. Each generator in the paral-
lel or series model structure can model the periodic and aperiodic
waveforms without the use of decomposition techniques. The ex-
perimental results showed that the proposed methods were able to
generate high-ﬁdelity speech waveforms and improve the ability to
generate waveforms with a pitch outside the range of the training
data. Future work includes investigating the effect of proposed meth-
ods on different datasets, such as a multi-speaker and multi-singer
dataset.

6. ACKNOWLEDGEMENTS

This work was supported by JSPS KAKENHI Grant Number
JP19H04136 and JP18K11163.

Fig. 5: Subjective evaluation results of experiment 2

and Fig. 3(b), the output waveform of the aperiodic generator in SM
contains more harmonic components than in PM1. This suggests
that the periodic waveform as the input of the aperiodic generator in
SM may have leaked into to the output of the aperiodic generator
because the output waveform of the periodic generator fed into the
aperiodic generator. It should be noted that some harmonic compo-
nents are also included in Fig. 2(b) since the periodic and aperiodic
waveforms were not explicitly decomposed in the training stage.

4.3. Subjective evaluations

4.3.1. Comparison of AR/non-AR neural vocoders and the input sig-
nals

We conducted a listening test using WN, BM1, BM2, BM3, and
NAT to compare neural vocoders the with and without the AR struc-
ture and the input signals for the non-AR neural vocoder. Note that
NAT indicates a recorded natural waveform. The naturalness of the
synthesized singing voice was assumed using the mean opinion score
(MOS) test method. The participants were sixteen native Japanese
speakers, and each participant evaluated ten phrases randomly se-
lected from the test data. After listening to each test sample in the
MOS test, the participants were asked to score the naturalness of the
sample out of ﬁve (1 = Bad; 2 = Poor; 3 = Fair; 4 = Good; and 5 =
Excellent).

The results of the subjective evaluation are shown in Fig. 4.
BM1 yielded a lower MOS value than WN, indicating that it is difﬁ-
cult to generate high-quality singing voices from noise. On the other
hand, BM2 showed the same score as WN. By inputting a periodic
signal, the neural vocoder can appropriately synthesize waveforms
with periodicity the lack of the AR structure. However, the wave-
form of WN contains quantization noise, so the quality of BM2 was
insufﬁcient. BM3, which inputs both explicit periodic and aperiodic
signals, has reached the MOS value close to NAT. This indicates the
effectiveness of using both explicit periodic and aperiodic signals as
inputs for non-AR neural vocoders.

WNBM1BM2BM3NAT2345Mean opinion score3.562.403.614.454.53BM3PM1PM2SMNAT2345Mean opinion score4.304.444.414.484.56BM3PM1PM2SM2345Mean opinion score2.502.743.082.667. REFERENCES

[1] A. van den Oord, S. Dieleman, H. Zen, K. Simonyan,
O. Vinyals, A. Graves, N. Kalchbrenner, A. W. Senior, and
K. Kavukcuoglu, “WaveNet: A generative model for raw au-
dio,” arXiv preprint arXiv:1609.03499, 2016.

[2] A. Tamamori, T. Hayashi, K. Kobayashi, K. Takeda, and
T. Toda, “Speaker-dependent WaveNet vocoder.” in Proccd-
ings of Interspeech, 2017, pp. 1118–1122.

[3] J. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang,
Z. Chen, Y. Zhang, Y. Wang, R. Skerrv-Ryan et al., “Natural
TTS synthesis by conditioning WaveNet on mel spectrogram
predictions,” in Proceedings of ICASSP, 2018, pp. 4779–4783.

[4] W. Ping, K. Peng, A. Gibiansky, S. O. Arik, A. Kannan,
S. Narang, J. Raiman, and J. Miller, “Deep voice 3: Scaling
text-to-speech with convolutional sequence learning,” arXiv
preprint arXiv:1710.07654, 2017.

[5] N. Kalchbrenner, E. Elsen, K. Simonyan, S. Noury,
N. Casagrande, E. Lockhart, F. Stimberg, A. v. d. Oord,
S. Dieleman, and K. Kavukcuoglu, “Efﬁcient neural audio syn-
thesis,” arXiv preprint arXiv:1802.08435, 2018.

[6] J.-M. Valin and J. Skoglund, “LPCNet:

Improving neural
speech synthesis through linear prediction,” in Proceedings of
ICASSP, 2019, pp. 5891–5895.

[7] A. van den Oord, Y. Li,

I. Babuschkin, K. Simonyan,
O. Vinyals, K. Kavukcuoglu, G. Driessche, E. Lockhart,
L. Cobo, F. Stimberg et al., “Parallel WaveNet: Fast high-
ﬁdelity speech synthesis,” in Proceedings of ICML, 2018, pp.
3918–3926.

[8] W. Ping, K. Peng, and J. Chen, “ClariNet:

Parallel
wave generation in end-to-end text-to-speech,” arXiv preprint
arXiv:1807.07281, 2018.

[9] D. P. Kingma, T. Salimans, R. Jozefowicz, X. Chen,
I. Sutskever, and M. Welling, “Improved variational inference
with inverse autoregressive ﬂow,” in Advances in Neural Infor-
mation Processing Systems, 2016, pp. 4743–4751.

[10] R. Prenger, R. Valle, and B. Catanzaro, “WaveGlow: A ﬂow-
based generative network for speech synthesis,” in Proceedings
of ICASSP, 2019, pp. 3617–3621.

[11] S. Kim, S.-g. Lee,

and S. Yoon,
“FloWaveNet: A generative ﬂow for raw audio,” arXiv
preprint arXiv:1811.02155, 2018.

J. Song,

J. Kim,

[12] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-
Farley, S. Ozair, A. Courville, and Y. Bengio, “Generative ad-
versarial nets,” in Advances in Neural Information Processing
Systems, 2014, pp. 2672–2680.

[13] S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and
H. Lee, “Generative adversarial text to image synthesis,” Pro-
ceedings of ICML, vol. 48, pp. 1060–1069, 20–22 Jun 2016.

[14] Y. Saito, S. Takamichi, and H. Saruwatari, “Statistical paramet-
ric speech synthesis incorporating generative adversarial net-
works,” IEEE/ACM Transactions on Audio, Speech, and Lan-
guage Processing, vol. 26, no. 1, pp. 84–96, 2018.

[15] Y. Hono, K. Hashimoto, K. Oura, Y. Nankaku, and K. Tokuda,
“Singing voice synthesis based on generative adversarial net-
works,” in Proceedings of ICASSP, 2019, pp. 6955–6959.

[16] R. Yamamoto, E. Song, and J.-M. Kim, “Parallel WaveGAN: A
fast waveform generation model based on generative adversar-
ial networks with multi-resolution spectrogram,” in Proceed-
ings of ICASSP, 2020, pp. 6199–6203.

[17] K. Kumar, R. Kumar, T. de Boissiere, L. Gestin, W. Z. Teoh,
J. Sotelo, A. de Br´ebisson, Y. Bengio, and A. C. Courville,
“MelGAN: Generative adversarial networks for conditional
waveform synthesis,” in Advances in Neural Information Pro-
cessing Systems, 2019, pp. 14 910–14 921.

[18] H. Kawahara, I. Masuda-Katsuse, and A. De Cheveigne, “Re-
structuring speech representations using a pitch-adaptive time–
frequency smoothing and an instantaneous-frequency-based
F0 extraction: Possible role of a repetitive structure in sounds,”
Speech communication, vol. 27, no. 3, pp. 187–207, 1999.

[19] M. Morise, F. Yokomori, and K. Ozawa, “WORLD: A vocoder-
based high-quality speech synthesis system for real-time ap-
plications,” IEICE Transactions on Information and Systems,
vol. 99, no. 7, pp. 1877–1884, 2016.

[20] X. Wang, S. Takaki, and J. Yamagishi, “Neural source-ﬁlter
waveform models for statistical parametric speech synthesis,”
IEEE/ACM Transactions on Audio, Speech, and Language
Processing, vol. 28, pp. 402–415, 2019.

[21] K. Oura, K. Nakamura, K. Hashimoto, Y. Nankaku, and
K. Tokuda, “Deep neural network based real-time speech
vocoder with periodic and aperiodic inputs,” in Procedings of
ISCA SSW10, 2019, pp. 13–18.

[22] Y.-C. Wu, T. Hayashi, T. Okamoto, H. Kawai, and T. Toda,
“Quasi-periodic Parallel WaveGAN: A non-autoregressive raw
waveform generative model with pitch-dependent dilated con-
volution neural network,” arXiv preprint arXiv:2007.12955,
2020.

[23] M. Nishimura, K. Hashimoto, K. Oura, Y. Nankaku, and
K. Tokuda, “Singing voice synthesis based on deep neural net-
works,” in Proccdings of Interspeech, 2016, pp. 2478–2482.

[24] Y. Hono, S. Murata, K. Nakamura, K. Hashimoto, K. Oura,
Y. Nankaku, and K. Tokuda, “Recent development of the
DNN-based singing voice synthesis system – Sinsy,” in Pro-
ceedings of APSIPA, 2018, pp. 1003–1009.

[25] X. Serra and J. Smith, “Spectral modeling synthesis: A
sound analysis/synthesis system based on a deterministic plus
stochastic decomposition,” Computer Music Journal, vol. 14,
no. 4, pp. 12–24, 1990.

[26] P. Zubrycki and A. Petrovsky, “Accurate speech decomposi-
tion into periodic and aperiodic components based on discrete
harmonic transform,” in Proceedings of European Signal Pro-
cessing Conference, 2007, pp. 2336–2340.

[27] “REAPER: Robust epoch and pitch estimator,” https://github.

com/google/REAPER.

[28] “Pulse code modulation (PCM) of voice frequencies,” in ITU-T

Recommendation G.711, 1988.

[29] T. Salimans and D. P. Kingma, “Weight normalization: A sim-
ple reparameterization to accelerate training of deep neural net-
works,” in Advances in Neural Information Processing Sys-
tems, 2016, pp. 901–909.

[30] L. Liu, H. Jiang, P. He, W. Chen, X. Liu, J. Gao, and J. Han,
“On the variance of the adaptive learning rate and beyond,” in
Proceedings of ICLR, April 2020.

