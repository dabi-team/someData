P2A: A Dataset and Benchmark for Dense Action Detection from
Table Tennis Match Broadcasting Videos

Jiang Bian, Qingzhong Wang, Haoyi Xiong, Jun Huang, Chen Liu,
Xuhong Li, Jun Cheng, Jun Zhao, Feixiang Lu, Dejing Dou
Baidu Inc.
Beijing, China

2
2
0
2

l
u
J

6
2

]

V
C
.
s
c
[

1
v
0
3
7
2
1
.
7
0
2
2
:
v
i
X
r
a

ABSTRACT
While deep learning has been widely used for video analytics, such
as video classification and action detection, dense action detection
with fast-moving subjects from sports videos is still challenging.
In this work, we release yet another sports video dataset P2A for
Ping Pong-Action detection, which consists of 2,721 video clips
collected from the broadcasting videos of professional table tennis
matches in World Table Tennis Championships and Olympiads.
We work with a crew of table tennis professionals and referees to
obtain fine-grained action labels (in 14 classes) for every ping-pong
action appeared in the dataset, and formulate two sets of action
detection problems—action localization and action recognition. We
evaluate a number of commonly-seen action recognition (e.g., TSM,
TSN, Video SwinTransformer, and Slowfast) and action localization
models (e.g., BSN, BSN++, BMN, TCANet), using P2A for both
problems, under various settings. These models can only achieve
48% area under the AR-AN curve for localization and 82% top-one
accuracy for recognition, since the ping-pong actions are dense
with fast-moving subjects but broadcasting videos are with only
25 FPS. The results confirm that P2A is still a challenging task and
can be used as a benchmark for action detection from videos.

CCS CONCEPTS
• Computing methodologies → Activity recognition and un-
derstanding; Video segmentation.

KEYWORDS
datasets, video analysis, action recognition and localization, table
tennis

1 INTRODUCTION
Videos have become one of the most popular media in our every day
life and video understanding draws much attention of researchers in
recent years, including video tagging [1, 56, 66], retrieval [13, 16, 29],
action recognition [15, 21, 59] and localization [10, 37, 62]. There
are many applications of video understanding, such as recommen-
dation systems [7, 30, 63]. One of the most attractive applications
is understanding sports videos, which is able to benefit coaching
[3, 52], player training [4, 46] and sports broadcasting [28, 53, 53].
To better understanding sports videos, localizing and recognizing
the actions in untrimmed videos play crucial roles, however, they
challenging tasks with unsolved problems.

First, there are many types of sports, such as team sports like
football, basketball, volleyball, and individual sports like tennis,
table tennis, golf, and gymnastics, and each of them has specific
actions. Therefore, it is difficult to build a dataset covering all sports

and their specific actions. Moreover, the duration of actions in
different sports are diverse. For example, the strokes in tennis and
table tennis are extremely fast, usually less than one second, while
actions in soccer could endure for several seconds, such as long
passes and corner ball. Hence, it is difficult to model the actions
with various lengths using one model.

Second, data annotation of sports actions is challenging. Com-
pared with common actions in our daily life like walking, riding
bicycles that can be easily recognized by annotators, it could be
difficult to identify the actions in sports videos, for example, Axel
jump, Flip jump and Lutz jump in figure skating, hence, professional
players should be involved in data annotation. In addition, play-
ers normally perform deceptive actions, for example, table tennis
players can perform similar actions, but serve balls with different
kinds of spin (top spin, back spin and side spin), which is difficult
for annotators to distinguish one action from the others.

Recently, researchers pay much attention to sports videos to
address the challenges, including building datasets [12, 20, 24, 35,
42] and proposing new models [23, 36, 48]. In this paper, we focus
on a specific sport – table tennis and propose a dataset – Ping Pang
Action (P2A) for action recognition and localization to facilitate
researches on fine-grained action understanding. The properties of
P2A are as follows:

• We annotate each stroke in videos, including the category of the
stroke and the indices of the starting and end frames. Plus, the
stroke labels are confirmed by professional players, including
Olympic table tennis players.

• To the best of our knowledge, P2A is the largest dataset for
table tennis analysis, composed of 2,721 untrimmed broadcasting
videos, and the total length is 272 hours. Though [24, 35] propose
table tennis datasets, the number of videos is smaller and they
use self-recorded videos where there is only one player, which
is much easier than using broadcasting videos. Moreover, the
datasets proposed in [24, 35] are only for action recognition,
whereas P2A can also be used for action localization.

• The actions are fast and dense. The action length ranges from 0.3
seconds to 3 seconds, but more than 90% actions are less than 1
second. And typically, there are around 15 actions in 10 seconds,
which is challenging for localization.

With the properties, P2A is a rich dataset for research on action
recognition and localization, and a benchmark to assess models (see
section 3 for more details). We further benchmark several existing
widely used action recognition and localization models using P2A,
finding that P2A is relatively challenging for both recognition and
localization.

 
 
 
 
 
 
To sum up, the main contributions of this work are twofold. First,
we develop a new challenging dataset – P2A for fast action recog-
nition and dense action localization, which provides high-quality
annotations confirmed by professional table tennis players. Com-
pared with existing table tennis datasets, P2A uses broadcasting
videos instead of self-recorded ones, making it more challenging
and flexible. In addition, P2A is the largest one for table tennis
analysis. Second, we benchmark a number of existing recognition
and localization models using P2A, finding that it is difficult to
localize dense actions and recognize actions with imbalance cat-
egories, which could inspire researchers to come up with novel
models for dense action detection in the future.

2 RELATED WORK

Action Localization. The task of action localization is to find
the beginning and the end frames of actions in untrimmed videos.
Normally, there are three steps in an action localization model. The
first step is video feature extraction using deep neural networks.
Second, classification and finally, suppressing the noisy predictions.
Z. Shou et al.[43] propose a temporal action localization model
based on multi-stage CNNs, where a deep proposal network is
employed to identify the candidate segments of a long untrimmed
video that contains actions, then a classification network is applied
to predict the action labels of the candidate segments and finally
the localization network is fine-tuned. Alternatively, J. Yuan et
al.[65] employ Improved Dense Trajectories (IDT) to localize actions,
however, IDT is time-consuming and requires more memory. In
contrast, S. Yeung et al.[64] treat action localization as decision
making process, where an agent observes video frames and decides
where to look next and when to make a prediction. Similar to Faster-
RCNN [41], X. Dai et al.[10] propose a temporal context network
(TCN) for active localization, where proposal anchors are generated
and the proposals with high scores are fed into the classification
network to obtain the action categories. To generate better temporal
proposals, T. Lin et al.[27] propose a boundary sensitive network
(BSN), which adopts a local-to-global fashion, however, BSN is not
a unified framework for action localization and the proposal feature
is too simple to capture temporal context. To address the issues
of BSN, T. Lin et al.[26] propose a boundary matching network
(BMN). Recently, M. Xu et al.[62] introduce the pre-training-fine-
tuning paradigm into action localization, where the model is first
pre-trained on a boundary-sensitive synthetic dataset and then fine-
tuned using the human annotated untrimmed videos. Instead of
using CNNs to extract video features, M. Nawhal et al.[37] employ
a graph transformer, which significantly improves the performance
of temporal action localization.

Action Recognition. Action recognition lies at the heart of
video understanding, an elementary module that draws much atten-
tion of researchers. A simple deep learning based model is proposed
by A. Karpathy [21], where a 2D CNN is independently applied to
each video frame. To capture temporal information, the fusion of
frame features is used, however, this simple approach cannot suffi-
ciently capture the motion of objects. A straightforward approach
to mitigate this problem is to directly introduce motion informa-
tion into action recognition models. Hence, K. Simonyan et al.[44]
proposed a two-stream framework, where the spatial stream CNN

takes a single frame as input and the temporal steam CNN takes
a multi-frame optical flow as input. However, the model proposed
by [44] only employs shallow CNNs, while L. Wang et al.[55] in-
vestigate different architectures and the prediction of a video is the
fusion of the segments’ predictions. To capture temporal informa-
tion without introducing extra computational cost, J. Lin et al.[25]
propose a temporal shift module, which can fuse the information
from the neighboring frames via using 2D CNNs. Another family of
action recognition models is 3D based. D. Tran et al.[49] propose a
deep 3D CNN, termed C3D, which employs 3D convolution kernels.
However, it is difficult to optimize a 3D CNN, since there are much
more parameters than a 2D CNN. While J. Carreira et al.[8] employs
mature architecture design and better-initialized model weights,
leading to better performance. All above mentioned models adopt
the same frame rate, while C. Feichtenhofer et al.[15] propose a
SlowFast framework, where the slow path uses a low frame rate
and the fast path employs a higher temporal resolution and then
the features are fused in the successive layers.

Recently, researchers pay more attention to video transformer –
a larger model with multi-head self-attention layers. G Bertasius
et al.[5] propose a video transformer model, termed TimeSformer,
where video frames are divided into patches and then space-time
attention is imposed on patches. TimeSformer has a larger capacity
than CNNs, in particular using a large training dataset. Similarly, A.
Arnab et al.[2] propose a transformer-based model, namely ViViT,
which divides video frames into non-overlapping tubes instead of
frame patches. Another advantage of the transformer-based model
is that it is easy to use self-supervised learning approaches to pre-
train the model. C. Wei et al.[59] propose a pre-training method,
where the transformer-based model is to predict the Histograms of
Oriented Gradients (HoG) [11] of the masked patches and then the
model is fine-tuned on downstream tasks. Using the pre-training-
fine-tuning paradigm, we can further improve the performance of
action recognition.

In this paper, we adopt multiple widespread action recognition
and localization models to conduct experiments on our proposed
dataset – P2A, showing that P2A is relatively challenging for both
action recognition and localization since the action is fast and the
categories are imbalance (see sections 3 and 4 for more details).

3 P2A DATASET
Our goal for establishing the PingPangAction dataset is to intro-
duce a challenging benchmark with professional and accurate an-
notations to the intensive and dense human action understanding
regime.

3.1 Dataset Construction
Preparation. The procedure of data collection takes the following
steps. First, we collect the raw broadcasting video clips of Inter-
national/Chinese top-tier championships and tournaments from
the ITTF (International Table Tennis Federation) Museum & China
Table Tennis Museum. We are authorized to download video clips
from 2017 to 2021 (five years), including the Tokyo 2020 Summer
Olympic table tennis competition, World Cups, World Champi-
onships, etc.

2

As a convenience for further processing (e.g., storing and an-
notating), we split the whole game videos into 6-minute chunks,
while ensuring the records are complete, distinctive and of standard
high-resolution, e.g., 720P (1280 × 720) and 1080p (1920 × 1080).
Then, the short video chunks are ready for annotation in the next
step.

Annotation. Since the actions (i.e., the ping pong strokes) in
video chunks are extremely dense and fast-moving, it is challenging
to recognize and localize all of them accurately. Thus, we cooperate
with a team of table tennis professionals and referees to regroup the
actions into broad types according to the similarity of their char-
acteristics, which are the so-called 14-classes (14c) and 8-classes
(8c). As the name implies, 14c categorizes all the strokes in our col-
lected video chunks into 14 classes, which are shown in Fig. 1 (right
side). Compared to 14c, we further refine and combine the classes
into eight higher-level ones, where the mapping relationships are
revealed accordingly. Specifically, since there are seven kinds of
service classes at the beginning of each game round (i.e., the first
stroke by the serving player), we reasonably combine these ser-
vice classes aside from those non-serving classes. Note that, all the
actions/strokes we mentioned here are presented in the formal turn-
s/rounds either conducted by the front-view player or back-view
player from the broadcasting cameras (the blue-shirt and red-shirt
player in Fig. 1), and others presented in highlights (e.g., could be in
a zoom-in scale with slow motions), Hawk-Eye replays (e.g., with
the focus on the ball tracking), and game summaries (e.g., possibly
in a zoom-out scale with a scoreboard) are ignored as redundant
actions/strokes. For each of the action types in 14c/8c, we define a
relatively clear indicator for the start frame and the end frame to
localize the specific action/stroke. For example, we set the label of
the non-serving stroke in a +/- 0.5 seconds window based on the
time instance when the racket touches the ball, where the serving
action/stroke may take a long time before the touching moment so
that we label it in a +0.5/-1.5 seconds window as the adjustment.
The window size is used as the reference for the annotator and can
be varied according to the actual duration of the specific action. We
further show the detailed distribution of the action/stroke classes
in Section 3.2.

Once we have established the well-structured classes of actions,
the annotators then engage to label all the collected broadcasting
video clips. Unfortunately, the first edition of annotated dataset
achieves around 85% labeling precision under sampling inspection
by experts (i.e., international table tennis referees). It is reasonable
that even with the trained annotators, labeling the actions/strokes
of table tennis players is still challenging, which lies in the existence
of 1) the fast-moving and dense actions; 2) the deceptive actions;
3) the entire/half-sheltered actions. To improve the quality of the
labeled dataset, we work with a crew of table tennis professionals
to further correct the wrong or missing samples in the first edition.
The revised edition can achieve about 95% labeling precision on
average, which is confirmed by the invited international table tennis
referee [39].

Note that, beyond labeling the segment of each action/stroke, we
also label some additional information along with it (after finishing
each action), which includes the placement of the ball on the table,
the occasion of winning or losing a point, the player who committed
the action (the name with the status of front view or back view in

that stroke), whether it is forehand or backhand, and the serving
condition (serve or not). The full list of labeling items is attached
in the open-sourced dataset repository on Github1.

Calibration. The last step for annotation is to clean the dataset
for further research purposes. Since the whole 6-minute video
dataset has some chunks without valid game rounds/turns (e.g., the
opening of the game, the intermission and break, and the award
ceremony), we filter out those unrelated chunks and focus on only
action-related videos. Then, to accomplish the main goals of the
research, which are tasks of recognition and localization, we further
screen the video chunks to reserve the qualified ones meeting the
following criteria:
• Camera Angle – The current most-common camera angle for
broadcasting is the top view, where the cameras are hosted on a
relatively high position upon the game court and overlook from
the back of two sides of players. As shown in Fig. 1, we only select
the videos recorded in the standard top view and remove those
with other views (e.g., side view, “bird’s eye” view, etc.) to keep the
whole dataset consistent. Note that, the broadcasting recordings
with non-standard broadcasting angles rarely (less than 5%)
appear in our target pool of competition videos, where only a
few WTT (World Table Tennis Championships) broadcasting
videos experimentally use the other types of camera angles.
• Game Type – Considering the possible mutual interference in
double or mixed double games, we only include the single games
in P2A. In this case, there at most two players appear simulta-
neously in each frame, where usually one player is located near
the top of the frame and another is at the bottom of the frame.
• Audio Track – Although we do not leverage the audio informa-
tion for the localization and recognition tasks, the soundtracks
are along with the videos and awaiting to be explored or incorpo-
rated in the future research. We also remove those with broken
or missing soundtracks.

• Video Resolution – We select the broadcasting videos with a
resolution equal to or higher than 720P (1280 × 720) in MP4
format and drop those with low frame quality.

• Action Integrity – Since we trim the original broadcasting video
into 6-minute chunks, some actions/strokes may be cut off when
delivering the splitting. In this case, we make efforts to label those
broken actions (about 0.2%) and it is optional to incorporate the
broken actions or not in a specific task.

In the next subsection, we introduce the basic statistics of cali-
brated P2A dataset and provide some guidance on pre-processing
that might be helpful in specific tasks.

Figure 1: The camera angles & the classes of stroke/action in
P2A dataset.

1The link will be released later.

3

Non-serveServeSide SpinSpin/No-SpinReverse PendulumSquattingHookNeutralNormalDrop ShotDriveStep-around DriveLong PushFlipBackhand FlipControlFigure 2: An example of “Side Spin Serve” action in the consecutive frames from P2A dataset.

Table 1: Comparison of existing sports video datasets for action detection and related purposes.

Dataset

Duration

Samples

Segments Classes

Tasks†

Density‡ Action Length

Source

Year

Olympic [38]
UCF-Sports [45]
ActivityNet [6]∗
TenniSet [14]
OpenTTGames [51]
FineGym [42]
P2A

20 h
15 h
648 h
9 h
5 h
708 h

272 h

800
150
14,950
380
12
303

2,721

800
2,100
23,064
3,568
4,271
4,885

139,075

16
10
200
10
3
530

14

Rec + Loc
Rec + Loc
Rec + Loc
Rec + Loc
Rec + Seg
Rec + Loc

Rec + Loc

1.00
14.00
1.54
9.40
355.92
16.12

52.70

5s ∼ 20s
2s ∼ 12s
10s ∼ 100s
1.08s ∼ 2.84s
0.12s ∼ 0.13s
8s ∼ 55s
0.32s ∼ 3s

YouTube

2010
Broadcast TV 2014
2015
YouTube
2017
YouTube
Self Recorded
2020
Broadcast TV 2020

Broadcast TV 2022

†We denote the task of recognition, localization, and segmentation as “Rec”, “Loc”, and “Seg”.
‡We calculate the Density here as the value of Segments/Samples.
∗We also include some popular video-based action detection datasets, which are not only limited to sports-related actions, but with a considerable amount of them in sub-genre.

3.2 Dataset Analysis
In this section, we conduct a comprehensive analysis of the statis-
tics of P2A, which is the foundation of the follow-up research and
also serves as the motivation for establishing P2A in the video anal-
ysis domain. The analysis lies in four parts, which are (1) general
statistics, (2) category distribution, (3) densities, and (4) discussion.
In the rest of this subsection, we go through each part in detail.
General Statistics. The P2A dataset consists of 2,721 annotated
6-minutes-long video clips, containing 139,075 labeled action seg-
ments and last 272 hours in total. They are extracted from over 200
table tennis competitions, involving almost all the top-tier ones
during 2017-2021. These events include the World Table Tennis
Championships, the Table Tennis World Cup, the Olympics, the
ITTF World Tour, the Asia Table Tennis Championships, the Na-
tional Games of China, as well as the China Table Tennis Super
League. Since we intend to establish an action-focused table tennis
dataset, the quality and standard of actions are expected to achieve
a relatively high level, where the official HD broadcasting videos
selected from the above-mentioned top-tier competitions lay a good
foundation as the source of the target actions.

For the general statistics of actions, Table 1 presents the basic sta-
tistics of P2A compared with the popular streams of sports-related
datasets. The length of action for P2A varies from 0.32 to 3 seconds,
which is significantly shorter than most of the other datasets ex-
cept the OpenTTGames and the TenniSet. Since the OpenTTGames
targets only the in-game events (i.e. ball bounces, net hits, or empty
event targets) without any stroking annotations, the action length
is even shorter and with a considerably higher Density. Compared
to these two table tennis related datasets, our P2A dataset has a
longer Duration (over 30 times, and with more samples and seg-
ments), where it means a lot for accurately reflecting the correlation
and distribution of actions/strokes in different classes. Furthermore,
compared to the sources of the OpenTTGames and the TenniSet,

P2A leverages the Broadcast TV games with the proper calibration
by the international table tennis referee, which are more official
and professional. Compared to the commonly used datasets such
as ActivityNet and FineGym, P2A focuses on the actions/strokes
with the dense and fast-moving characteristics. Specifically, the
action length of P2A is around 30 times shorter and density is 5 to
50 higher than these two datasets, which means P2A introduces a
series action/strokes in a totally different granularity. We later show
that the SOTA models confront a huge challenge when applying on
P2A. Although the number of classes of P2A is relatively small due
to the nature of the table tennis sports itself, the action recognition
and localization tasks are barely easy on P2A for most of the main-
stream baselines. In the next subsection, we show the distribution
of categories in P2A, which is one of the vital components causing
the aforementioned difficulties.
Category Distribution. Figure 1 right side shows the targeting
categories of actions/stokes in P2A. The P2A dataset has two main
branches of strokes – Serve and Non-serve, where each of them
owns 7 types of sub-strokes. Since the strokes are possibly taken
by different players in the same video frames, we annotate strokes
from both the front-view and the back-view players without time
overlaps. For example, Figure 2 represents consecutive frames of
annotations on a front-view player, where a player is producing
a “Side Spin Serve” at the start of a turn. Notice that this player
stands at the far-end of the game table and turns around from the
side-view to the front-view, then keeps the front-view in the rest
of this turn. A similar annotating is taken to record the player’s
strokes at the near-end of the game table.

As aforementioned, the P2A dataset consists of two versions,
which are 14c (14 classes) and 8c (8 classes). The 14c flattens all the
predefined classes in Figure 1 and treats them equally in the future
tasks. We measure the number of strokes for each of the classes in
Figure 3. Overall, the drive category contains more than half of the

4

Figure 3: Sorted distribution of the number of actions (with duration) from each class (Ver. 14c) in the P2A dataset.

(b) Non-Serving Actions (7 classes)
Figure 4: Sorted distribution of the number of actions from the serving, non-serving, combined classes (Ver. 8c) in the P2A
dataset.

(c) Combined Actions (8 classes)

(a) Serving Actions (7 classes)

samples (we sort the categories in descending order), which means
there exists unbalancing category phenomenon in P2A. We also
observe an interesting point that the non-serve categories generally
last a shorter duration than the serve categories (as shown from
the blue bars). This is because of the long preparation of the serve
strokes and could be an important feature to distinguish the serve
and the non-serve strokes. Another fact is that the non-serve cate-
gories dominate the whole dataset, where there are seven non-serve
categories out of the left-most eight categories (in the descending
order). This unbalancing phenomenon leads to the creation of the
second version – 8C to combine all the serve categories into one
unified “Serve” category.

Figure 4 measures the number of actions in the serve, non-
serve , and combined 8c categories separately. As a result, the sub-
categories are unbalanced, where “Side Spin” and “Drive” dominates
the serving and non-serving actions in (a)&(b). In the combined 8c
dataset, “Drive” unsurprisingly takes a large proportion among all
eight categories due to its frequent appearances in modern table ten-
nis competitions. Thus, it is necessary to adopt additional sampling
techniques to mitigate the side effects, and the implementation
details are introduced in Section 4.

(a) All Classes

(b) Serve vs Non-serve Classes

Figure 5: Duration distribution of actions/strokes in P2A
dataset.
Densities. As one of the unique characteristics compared to other
datasets, the high density of actions/strokes plays a vital role in P2A.

5

We analyze the density of actions/strokes in two aspects, where
the first one is the duration distribution of each actions/strokes
category. Figure 5(a) shows the duration distribution of all classes
of strokes in P2A. We can observe that most of the action types
have relatively stable distributions (with short tails) except the
“Control” and the “Neutral” types. The “Neutral” is one of the serving
strokes representing those irregular/unknown actions in the serving
process, where the duration of it widely ranges from 0.4 to 3 seconds
in Figure 5(a). It is similar that the “Control” category stands for
the non-serving strokes which cannot be categorized to any other
classes, and has a long tail of 2 seconds at most. On the whole, the
average duration of strokes sticks to around 0.5 seconds, which
demonstrates the actions in P2A is fast-moving compared to other
datasets in Table 1. We also compare the duration distribution
between the serve and non-serve classes in Figure 5(b), where the
stacked plot of both classes presents that the serve class has a
slightly longer tail than the non-serve class.

For the second respect, we measure the action frequency and
the appearance density in each turn of the game. One turn in a
game starts from a serving action by one player and ends with
one point acquired by either of the players. Thus, the action fre-
quency can reflect the nature of the table tennis competition (e.g.,
the strategy and the pace of the current table tennis game) that
how many strokes could be delivered in a single turn. As shown
in Figure 6(a), most of the turns have around 3 ∼ 5 actions/strokes
and the distribution is also long-tailed to a maximum 28 action-
s/strokes per turn. It is reasonable that the pace of the game today
becomes faster than ever, and a point could be outcome within
three strokes for both players (i.e., summed up within 6 strokes).
To further differentiate from other sports datasets, we additionally
measure the appearance density in 10 seconds, which is usually the
time boundary for long-duration and short-duration sports actions.
Figure 6(b) depicts the histogram of the action counts in 10-second
consecutive video frames. The results follow a normal-like distri-
bution with average counts as 15, which means the actions appear

DriveSide SpinDrop ShotStep-around DriveControlBackhand FlipLong PushFlipReverse PendulumSpin/No-SpinNormalHookSquattingNeutral01000020000300004000050000Number of Actions0.000.250.500.751.001.251.501.752.00Average Duration (s)Side SpinReverse PendulumSpin/No-SpinNormalHookSquattingNeutral02500500075001000012500150001750020000Number of Actions0.000.250.500.751.001.251.501.752.00Average Duration (s)DriveDrop ShotStep-around DriveControlBackhand FlipLong PushFlip01000020000300004000050000Number of Actions0.000.250.500.751.001.251.501.752.00Average Duration (s)DriveServeDrop ShotStep-around DriveControlBackhand FlipLong PushFlip01000020000300004000050000Number of Actions0.000.250.500.751.001.251.501.752.00Average Duration (s)0.00.51.01.52.02.53.0Duration0.00.20.40.60.81.0DensityClassSide SpinDrop ShotStep-around DriveSpin/No-SpinDriveNormalBackhand FlipControlFlipLong PushReverse PendulumSquattingNeutralHook0.00.51.01.52.02.53.0Duration0.000.250.500.751.001.251.501.752.00DensityClassServeNon-Servedensely and frequently in a short time in the most of the time in
P2A. The density can be achieved as about 1.5 actions per second
on average and it demonstrates the characteristics of dense actions
for the P2A dataset.

(a) The Action Frequency (b) The Appearance Den-

sity

Figure 6: Density of actions/strokes in each turn of game.

Summary. In this section, we comprehensively report the statis-
tics of P2A. Specifically, we leverage rich measurements to have a
thorough view of the volume of the dataset, the category distribu-
tion, the duration of samples, and the densities. In conclusion, the
P2A dataset is unique comparing to the other mainstream sports
datasets in the following sides,
• Comparing to the table tennis datasets [14, 51], P2A has an ade-
quate sample size (10 times ∼ 200 times) and more subtle set of
categories.

• Comparing to the large datasets in other sports domain [6, 38,
42, 45], P2A focuses on the dense (5 times ∼ 50 times) and fast-
moving (around 0.1 times) actions.

• Comparing to most of the aforementioned sports datasets, P2A
includes two players’ actions back-to-back (the action appears in
turns with a fast pace), which is more complicated and mutually
interfering when analyzing.
In the next section, we investigate the performance of the state-
of-the-art models and solutions for the predefined action recogni-
tion and action localization tasks on our P2A dataset. Then, we
observe the challenges those algorithms confront and demonstrate
the research potential of the P2A dataset.

4 BENCHMARK EVALUATION
In this section, we present the evaluation of various methods on our
P2A dataset and show the difficulties when applying the original
design of those methods. Then, we provide several suggestions to
mitigate the ineffectiveness and tips to improve the current models.

4.1 Baselines
We adopt two groups of baseline algorithms to separately tackle
the action recognition and action localization tasks. For Action
Recognition, we includes the following trending algorithms,
• Temporal Segment Network (TSN) [55] is a classic 2D-CNN-
based solution in the field of video action recognition. This
method mainly solves the problem of long-term behavior recogni-
tion of video, and replaces dense sampling with sparsely sampling
video frames, which can not only capture the global information
of the video, but also remove redundancy and reduce the amount
of calculation.

6

• Temporal Shift Module (TSM) [25] is a popular video action
recognition model with shift operation, which can achieve the
performance of 3D CNN but maintain 2D CNN’s complexity. The
method of moving through channels greatly improves the uti-
lization ability of temporal information without increasing any
additional number of parameters and calculation costs. TSM is
accurate and efficient: it ever ranked first place on the Something-
Something [17] leaderboard. Here, we adopt the industrial level
variation from the deep learning platform PaddlePaddle [34]. We
improve the original design with additional knowledge distilla-
tion, where the model is pretrained on Kinetics400 [8] dataset.
• SlowFast [15] involves a Slow pathway operating at a low frame
rate to capture spatial semantics, and another Fast pathway op-
erating at a high frame rate to capture motions in the temporal
domain. SlowFast is a powerful action recognition model, which
ranks in second place on AVA v2.1 datasets [18].

• Video-Swin-Transformer (Vid) [31] is a video classification
model based on Swin Transformer [32]. It utilizes Swin Trans-
former’s multi-scale modeling and the efficient local attention
module. Vid shows competitive performances in video action
recognition on various datasets, including the Something-Something
and Kinetic-series datasets.

For Action Localization, we investigate the following classical

or trending algorithms,

• Boundary Sensitive Network (BSN) [27] is an effective pro-
posal generation method, which adopts "local to global" fashion.
BSN has already achieved high recalls and temporal precision
on several challenging datasets, such as ActivityNet-1.3 [6] and
THUMOS14’ [54].

• BSN++ [47] is a new framework which exploits complementary
boundary regressor and relation modeling for temporal proposal
generation. It ranked first in the CVPR19 - ActivityNet challenge
leaderboard on the temporal action localization task.

• Boundary-Matching Network (BMN) [26] introduces the Boundary-
Matching (BM) mechanism to evaluate confidence scores of densely
distributed proposals, which leads to generating proposals with
precise temporal boundaries as well as reliable confidence scores
simultaneously. Combining with existing feature extractors (e.g.,
TSM), BMN can achieve state-of-the-art temporal action detec-
tion performance.

• Self-Supervised Learning for Semi-Supervised Temporal
Action Proposal (SSTAP) [58] is one of the self-supervised
methods to improve semi-supervised action proposal generation.
SSTAP leverages two crucial branches, i.e., the temporal-aware
semi-supervised branch and relation-aware self-supervised branch
to further refine the proposal generating model.

• Temporal Context Aggregation Network (TCANet) [40] is
the championship model in the CVPR 2020 - HACS challenge
leaderboard, which can generate high-quality action proposals
through "local and global" temporal context aggregation and com-
plementary as well as progressive boundary refinement. Since it
is a newly released model designed for several specific datasets
(the original design shows unstable performance on P2A), we
slightly modify the architecture to incorporate a BMN block
ahead of TCANet. The revised model is denoted as TCANet+ in
our experiments.

051015202530The number of actions in each turn0100020003000400050006000Count051015202530Appearances in 10 seconds (actions)02004006008001000Count4.2 Experimental Setup
Datasets. As aforementioned, we establish two versions – 14-
classes (14c) and 8-classes (8c) – in P2A. The difference is that
we combine all the serving strokes/actions in a single “Serve” cat-
egory in 8c version. We evaluate the baseline action recognition
algorithms on both versions and compare the performances side
by side. Since the performances of the current action localization
algorithms on 14c are far from satisfactory, we only report the
results on 8c for reference.
Metrics. We use the Top-1 and Top-5 accuracy as the metric for
evaluating the performance of baselines in the action recognition
tasks. For the action localization task, We adopt the area under the
Average Recall vs. Average Number of Proposals per Video (AR-AN)
curve as the evaluation metric. A proposal is a true positive if it
has a temporal intersection over union (tIoU) with a ground-truth
segment that is greater than that or equal to a given threshold (e.g,
tIoU>0.5). AR is defined as the mean of all recall values using tIoU
between 0.5 and 0.9 (inclusive) with a step size of 0.05. AN is defined
as the total number of proposals divided by the number of videos in
the testing subset. We consider 100 bins for AN, centered at values
between 1 and 100 (inclusive) with a step size of 1, when computing
the values on the AR-AN curve.
Data Augmentation. The category distribution analysis in Sec-
tion 3.2 reveals that P2A is an imbalanced dataset, where it might
cause trouble to the recognition task. To fully utilize the proposed
P2A dataset, we design a simple yet effective data augmentation
method to train the baseline models. Specifically, we introduce an
up/down-sampling procedure to balance the sample size of each
category. First, we calculate the mean size of all the samples (de-
noted 𝑁 samples) by the defined number of categories (e.g., 8c or
14c) as 𝑀, where 𝑀 = 𝑁 /8 or 𝑁 /14. Then, we sample the actual
number of action segments 𝑆𝐴 within a range 𝑀 ≤ 𝑆𝐴 ≤ 2𝑀.
For those categories with less number of segments than 𝑀, we
up-sample by the random duplication. For those with much more
samples over 2𝑀, we random down-sample from the original sam-
ple set and try to make the sampled actions cover all the video
clips (i.e., uniform sampling from each 6-minute chunk). We apply
this data augmentation strategy to all the recognition tasks and the
significant performance gains. As shown in Figure 7, we report the
ablation results of action recognition baseline TSM on P2A with-
/without the data augmentation. In Figure 7(a), the confusion matrix
illustrates that TSM falsely classifies most of the strokes/actions
into the “Drive” category without the class balancing, while this
kind of misclassification is significantly alleviated after we apply the
data augmentation. Note that, since the localization tasks normally
require feature extractor networks as the backbones (e.g., TSM is
one of the backbones), localization performance could also be bene-
fited from the designed data augmentation. The baseline algorithms
in the following results section also involve data augmentation.

4.3 Results
This section first presents the performance results of action recog-
nition tasks. Table 2 provides a summary of these results in terms of
top1/top5 accuracy on both 8c and 14c datasets. As we can observe,
the top5 accuracy is unsurprisingly higher than the top1 accuracy
for all the methods. For comparing the 8c and 14c datasets in the

7

(a) without class balancing

(b) with class balancing

Figure 7: Confusion matrix of action recognition results

Table 2: Performance of different action recognition meth-
ods on P2A in terms of validation accuracy.

Actions

Face

Method
TSN
TSM
SlowFast
Vid

Face&Back TSM (Best)

Acc1@14c Acc5@14c Acc1@8c Acc5@8c

59.77
69.97
63.19
68.85

59.46

90.13
98.08
92.79
98.16
87.11

73.84
82.35
77.41
81.03

68.39

94.85
99.06
96.07
98.96

91.73

same setting, the performances of baselines on 8c are relatively
better than on 14c, which confirms that the combination of serving
classes is reasonable and actually eases the recognition task. In
general, our refined TSM (PaddlePaddle version) method achieves
the highest accuracy in all four settings except the top5 accuracy on
the 14c datasets. The popular transformer-based method Vid also
shows competitive performances in the second place as a whole.
Note that, since the same strokes/actions done by the player facing
the camera and with back towards the camera differs drastically
(e.g., the blocked views and the opposite actions), we mainly focus
on the facing players’ actions/strokes as the recognition targets. As
for the back view player’s actions/strokes, the performances are
poor for all methods. Alternatively, we report the performance of
the mixed (i.e., face and back) one, the results are far below the
expectation, where even the best performer, TSM, only achieves
59.46 top 1 accuracy on the 14c and 68.39 top5 accuracy on the 8c
dataset. Although the top5 accuracy of all the methods appears to
be promising, considering the total number of categories in P2A,
which is 14 or 8 accordingly, the results are trivial to some degree.
In conclusion, for the action recognition task with the mainstream
baseline algorithms, P2A is considerably a challenging dataset and
there is a vast room and potential for improvement and research
involvement.

For the action localization task, we summarize the performances
in Figure 9. We selected several representative state-of-the-art ac-
tion localization methods that are publicly available and retrained
them on our P2A dataset. We report the area under the AR-AN
curve for the baselines as the measurements of the localization
performance. For example, in Figure 9(a), we draw the AR-AN
curves with a varying value of tIoU from 0.5 to 0.95 separately and
one mean curve (solid blue curve) to represent the overall perfor-
mance of BSN on P2A. Among all the methods, TCANEt+ shows
the highest overall AUC score which is 47.79 in Figure 9(f) and
also outperforms other baselines in each tIoU level. To straightfor-
wardly compare the localization results, we visualize the located
action segments from each method in Figure 8. The top figure is on

ServeDriveControlDrop ShotFlipBackhand FlipLong PushStep-around DriveServeDriveControlDrop ShotFlipBackhand FlipLong PushStep-around Drive137112090000161870921600009610000019118139000054108000007100000001900000001100100030255075100125150175Drop ShotDriveLong PushServeStep-around DriveBackhand FlipControlFlipDrop ShotDriveLong PushServeStep-around DriveBackhand FlipControlFlip61671072000021181072800054618000028127121091600092131900021144580003791200001213900080020406080100Figure 8: Example of localisation results on P2A dataset.

(a) BSN

(b) BSN++
(e) BMN
Figure 9: Performance of different action localization methods on P2A.

(d) TCANet+

(c) SSTAP

(f) ALL

a large scale to show the predicted segments compared with the
ground-truth segments in a complete 6-minute video chunk, where
the bottom one zooms in a single turn for a series of consecutive
actions/strokes by the player facing the camera. From such visual
observations, we can find that TCANet+ can almost reproduce the
short action segments from the ground-truth labeling. However,
the first period of serving action is barely located and TCANet+
mis-predicts with a much shorter one instead. Compared to the
action recognition task, the action localization seems more difficult
on P2A since the best AUC score is even in a massive gap from the
average achievement of these baselines on ActivityNet [6] (47.79
≪ 68.25 on average from top35 solutions in the 2018 leader-board).
Summary. From the above experimental results, we can conclude
that the P2A is still a very challenging dataset for no matter the
action recognition and the action localization tasks. Compared to
the current release sports datasets, the representative baselines
hardly adapt to the proposed P2A dataset and the performance is
far lower than our expectation. Thus, the P2A dataset provides a
good opportunity for researchers to further explore the solution in
a domain of fast-moving and dense action detection. Note that, the
P2A dataset is not limited to the action recognition and localization
tasks only, where it is also potential to be used for video segmen-
tation and summarizing. Actually, we are on the way to explore
more possibilities of P2A and will update the released version to
include more use cases. Due to the limited space, we will release the

implementation details for all the baselines in our Github repository
with the dataset link once the paper gets published.

5 CONCLUSION
In this paper, we present a new large-scale dataset P2A, which is
currently the largest publicly available table tennis action detection
dataset to the best of our knowledge. We have evaluated several
state-of-the-art algorithms on the introduced dataset. Experimental
results show that existing approaches are heavily challenged by
fast-moving and dense actions and uneven action distribution in
video chunks. In the future, we hope that P2A could become a
new benchmark dataset for fast-moving and dense action detection
especially in the sports area. Through a comprehensive introduction
and analysis, we intend to help the follow-up users and researchers
to better acknowledge the characteristics and the statistics of P2A,
so as to fully utilize the dataset in action recognition and action
localization tasks. We believe the P2A dataset will make an early
attempt in driving the sports video analysis industry to be more
intelligent and effective.

A SUPPLEMENTARY
Due to the page limit in the main body of the manuscript, we
supplement the complete implementation details and evaluation
results with brief discussions in this supplementary section.

8

Recognition Method

Backbone

Sampling

Num_Seg

Target Size

Others

Table 3: Key parameters of implemented algorithms.

TSN
TSM
SlowFast
TimeSformer
ViD
Attention-LSTM
MoViNet

ResNet50-D
ResNet50-D
ResNet50-D

TenCrop
Dense
Time-stride
ViT-base-patch16-224 UniformCrop
UniformCrop
RandomNoise
UniformCrop

SwinT-base-1k-224
Inception-ResNet-v2
MoViNet-A0

8
8
8
8
32
8
50

224
224
224
224
224
224
172

Distillation with ResNet152-CSN
Distillation with ResNet152-CSN
Mutigrid speedup [60]
Linspace when testing
-
-
AutoAugment [9]

Localization Method

Backbone

Sampling

NMS

Window Size

Others

BSN
BSN++
SSTAP
BMN
TCANet+

TSM (ResNet50-D)
TSM (ResNet50-D)
TSM (ResNet50-D)
TSM (ResNet50-D)
TSM (ResNet50-D)

-
-
-
-
RandomNoise

Soft
Soft
Soft
Soft
Soft

100
100
100
100
300

-
Proposal extension [47], Self-attention
Self-Supervised Learning
Proposal extension [47]
LGTE, SEBlock [40]

Remarks. In this paper, we mainly target the 2D video action
recognition algorithms considering the following sakes,

• Traditional 3D/4D video action recognition algorithms such as
I3D [8], R3D [19], S3D [61], and Non-local [57] are widely ac-
knowledged to be inefficient in computation and optimization
compared to the 2D algorithms (i.e., it is barely affordable and
scalable in some real world applications), even though the 3D/4D
algorithms generally have a more robust performance [67] on
video recognition tasks.

• Analyzing table tennis broadcasting video is not only limited to
academic purposes, which includes the research on dense, fast-
moving, and noisy (e.g., multiple action sources from players)
action recognition and localization, it also benefits table tennis
sports affairs, for example, real-time competition analysis, and
actions/events summarizing on stream broadcasting videos. Thus,
the effectiveness and efficiency are of equal importance for the
solutions in industrial practices, where the 2D and the lightweight
video action recognition algorithms have received a great deal of
industrial and research attention in recent years.

• We also include one 3D action recognition algorithm – Slow-
Fast [15]. Compared to the traditional 3D networks, SlowFast
does not heavily rely on the concatenation of 3D CNNs, where its
fast pathway could be very lightweight by reducing its channel
capacity, so as to largely improve the overall efficiency. Further-
more, SlowFast is proved to be competitive with other full-size
3D recognition algorithms on several benchmark datasets [67].

Notice that the proposed P2A is not only available for the ac-
tion recognition and the action localization tasks. For the general
action detection task, which incorporates the recognition and lo-
calization in a unified objective, the P2A dataset is still challeng-
ing and remains explored in future study. As shown in Figure 10,
we conduct the experiments for the action detection task on P2A
using a pipeline with the components of (1) TSM (PaddlePaddle
variation), (2) BMN, and (3) Attention-LSTM. Specifically, we first
extract the frame features using a TSM module (TSM module is
pre-trained on a recognition task on the P2A dataset). Once we
have the well-represented features, we then feed the features into

a BMN module to generate the action proposals. The last step is to
train an Attention-LSTM module to obtain the action class within
each proposal. We further provide ablation tests in Table 4. As we
can observe, several optimization strategies improve the average
mAP (mean average precision) of the proposed pipeline, which
includes the proposal extension and data augmentation. Since the
proposal generated by the BMN module sometimes cannot cover
the entire action, we intend to extend the proposal duration before
and after for 1 second. However, we find that not all the actions
benefit from this kind of extension, where we additionally design a
partial proposal extension strategy for the target actions. Although
applying the above mentioned strategies, we hardly obtain a satis-
fied average mAP (IoU from 0.5 to 0.9 with a step size of 0.05), where
the value is only 49.74 for the best model after the fine-tuning. It
makes sense with the unsatisfactory result that the performance of
action detection on P2A depends on the performance of both the
action recognition and the action localization tasks, which have
been shown as two challenging tasks in this paper.

Figure 10: The pipeline of action detection on the P2A
dataset.

Table 4: Ablation experiments of the action detection
pipeline on P2A. (“EPE/PPE” stands for the entire/partial pro-
posal extension. “DA” represents the data augmentation.)

mAP

Pipelines

IoU

0.5

0.75

0.95 Average

TSN + BMN + LSTM
TSM + BMN + LSTM
TSM + BMN + LSTM + EPE
TSM + BMN + LSTM + PPE
TSM + DA + BMN + LSTM + PPE

37.71
40.39
45.14
46.03
49.75

16.33
22.01
22.94
25.79
30.68

4.10
4.94
5.89
7.17
8.03

18.99
20.34
22.06
24.35
33.35

9

FramesVideosFeaturesFeaturesTSMProposalsBMNLSTMActionsAdditional recognition algorithms. Beyond the most represen-
tative action recognition algorithms introduced in the main body,
we also conduct the experiments using the following algorithms,
• Vanilla-TSN/TSM: the original version of TSN [55] and TSM [25]
models. Note that, the revised TSM (denoted as TSM) includes (1)
better backbones: ResNet50-D series; (2) Two-stage knowledge
distillation: TSM adopt a semi-supervised distillation (SSD) strat-
egy for the first stage and leverages the CSN [50] as the teacher
network with a ResNet152 backbone in the second stage.

• Attention-LSTM [33]: recurrent neural networks (RNN) are often
used in the processing of sequence data, which can model the
sequence information of consecutive frames, and are commonly
used methods in the field of video classification. As one of the
RNNs, Attention-LSTM uses a two-way long and short-term
memory network with attention layers to encode all the frame
features of the video in sequence.

• MoViNet [22]: Movinet is a mobile video network developed
by Google research. It uses causal convolution operators with
stream buffer and temporal ensembles to improve the accuracy of
video classification. It is a lightweight and efficient video model
designed for online video stream reasoning.

• TimeSformer [5]: TimeSformer is a video classification model
based on a vision transformer, which equips with the global
receptive field, and strong time series modeling without con-
volutions. At present, it has achieved SOTA accuracy on the
Kinetics-400 data set, approximating the performance of clas-
sic 3D-CNN based video classification models, while it has a
considerably short training time.

As shown in Table 5, we present the full performance results of
the implemented video action recognition algorithms in different
settings. The TSM (revised) algorithm outperforms the other log-
arithms in most cases. We also observe that the recognition task
of Face&Back is even more challenging than the recognition task
of Face only, where it is reasonable since the actions/strokes from
the back-view player are commonly blocked partially/entirely by
the body of the player and the mixed actions/strokes from both
players in the overlapped frames might affect the learning process
of the baseline models. The overall performance of the baseline al-
gorithms demonstrates the action recognition task on P2A has a lot
of room to be explored and improved in terms of the performance.
Implementations. We list the key hyper-parameters of all the im-
plemented action recognition and action localization algorithms in
Table 3. Note that, the TSN and TSM stands for the revised versions
compared to the released version from the original paper. For other
baselines, we follow the original design of the critical components
from each algorithm and adjust specific hyper-parameters to fit
them on the P2A dataset.

REFERENCES
[1] Sami Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Paul Natsev, George Toderici,
Balakrishnan Varadarajan, and Sudheendra Vijayanarasimhan. 2016. Youtube-8m:
A large-scale video classification benchmark. arXiv preprint arXiv:1609.08675
(2016).

[2] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lučić, and
Cordelia Schmid. 2021. Vivit: A video vision transformer. In Proceedings of the
IEEE/CVF International Conference on Computer Vision. 6836–6846.

[3] Hongshu Bao and Xiang Yao. 2021. Dynamic 3D image simulation of basketball
movement based on embedded system and computer vision. Microprocessors and
microsystems 81 (2021), 103655.

Table 5: Performance of different action recognition meth-
ods on P2A in terms of validation accuracy.

Actions

Face

Face&Back

Method
TSN
Vanilla-TSN
TSM
Vanilla-TSM
SlowFast
Vid
Attention-LSTM
MoViNet
TimeSformer

TSN
Vanilla-TSN
TSM
Vanilla-TSM
SlowFast
Vid
Attention-LSTM
MoViNet
TimeSformer

Acc1@14c Acc5@14c Acc1@8c Acc5@8c

59.77
57.28
69.97
63.33
63.19
68.85
53.19
54.79
64.01

53.31
50.47
59.46
55.72
56.39
58.54
51.06
52.45
56.49

90.13
89.36
98.08
93.21
92.79
98.16
81.98
85.31
94.87

78.03
76.69
87.11
84.05
87.98
87.20
75.39
77.19
86.31

73.84
72.07
82.35
76.79
77.41
81.03
73.34
74.09
77.39

54.13
53.94
68.39
63.39
65.10
67.28
52.79
54.12
64.93

94.85
93.89
99.06
95.63
96.07
98.96
89.31
88.93
95.56

82.46
80.31
91.73
86.32
89.93
91.05
80.67
82.07
90.30

[4] Gedas Bertasius, Hyun Soo Park, Stella X Yu, and Jianbo Shi. 2017. Am I a baller?
basketball performance assessment from first-person videos. In Proceedings of
the IEEE international conference on computer vision. 2177–2185.

[5] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. 2021. Is Space-Time At-
tention All You Need for Video Understanding?. In International Conference on
Machine Learning. PMLR, 813–824.

[6] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles.
2015. Activitynet: A large-scale video benchmark for human activity understand-
ing. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition. 961–970.

[7] Desheng Cai, Shengsheng Qian, Quan Fang, Jun Hu, Wenkui Ding, and Chang-
sheng Xu. 2022. Heterogeneous Graph Contrastive Learning Network for Person-
alized Micro-video Recommendation. IEEE Transactions on Multimedia (2022).
[8] Joao Carreira and Andrew Zisserman. 2017. Quo vadis, action recognition? a new
model and the kinetics dataset. In proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition. 6299–6308.

[9] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le.
2018. Autoaugment: Learning augmentation policies from data. arXiv preprint
arXiv:1805.09501 (2018).

[10] Xiyang Dai, Bharat Singh, Guyue Zhang, Larry S Davis, and Yan Qiu Chen. 2017.
Temporal context network for activity localization in videos. In Proceedings of
the IEEE International Conference on Computer Vision. 5793–5802.

[11] Navneet Dalal and Bill Triggs. 2005. Histograms of oriented gradients for human
detection. In 2005 IEEE computer society conference on computer vision and pattern
recognition (CVPR’05), Vol. 1. Ieee, 886–893.

[12] Adrien Deliege, Anthony Cioppa, Silvio Giancola, Meisam J Seikavandi, Jacob V
Dueholm, Kamal Nasrollahi, Bernard Ghanem, Thomas B Moeslund, and Marc
Van Droogenbroeck. 2021. Soccernet-v2: A dataset and benchmarks for holis-
tic understanding of broadcast soccer videos. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. 4508–4519.

[13] Jianfeng Dong, Xirong Li, Chaoxi Xu, Xun Yang, Gang Yang, Xun Wang, and
Meng Wang. 2021. Dual encoding for video retrieval by text. IEEE Transactions
on Pattern Analysis and Machine Intelligence (2021).

[14] Hayden Faulkner and Anthony Dick. 2017. TenniSet: A Dataset for Dense Fine-
Grained Event Recognition, Localisation and Description. In 2017 International
Conference on Digital Image Computing: Techniques and Applications (DICTA).
IEEE, 1–8.

[15] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. 2019. Slow-
fast networks for video recognition. In Proceedings of the IEEE/CVF international
conference on computer vision. 6202–6211.

[16] Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. 2020. Multi-
modal transformer for video retrieval. In European Conference on Computer Vision.
Springer, 214–229.

[17] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska,
Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos,
Moritz Mueller-Freitag, et al. 2017. The" something something" video database
for learning and evaluating visual common sense. In Proceedings of the IEEE
international conference on computer vision. 5842–5850.

[18] Chunhui Gu, Chen Sun, David A Ross, Carl Vondrick, Caroline Pantofaru, Yeqing
Li, Sudheendra Vijayanarasimhan, George Toderici, Susanna Ricco, Rahul Suk-
thankar, et al. 2018. Ava: A video dataset of spatio-temporally localized atomic

10

visual actions. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition. 6047–6056.

[19] Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. 2018. Can spatiotemporal
3d cnns retrace the history of 2d cnns and imagenet?. In Proceedings of the IEEE
conference on Computer Vision and Pattern Recognition. 6546–6555.

[20] Yudong Jiang, Kaixu Cui, Leilei Chen, Canjin Wang, and Changliang Xu. 2020.
Soccerdb: A large-scale database for comprehensive video understanding. In
Proceedings of the 3rd International Workshop on Multimedia Content Analysis in
Sports. 1–8.

[21] Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Suk-
thankar, and Li Fei-Fei. 2014. Large-scale video classification with convolutional
neural networks. In Proceedings of the IEEE conference on Computer Vision and
Pattern Recognition. 1725–1732.

[22] Dan Kondratyuk, Liangzhe Yuan, Yandong Li, Li Zhang, Mingxing Tan, Matthew
Brown, and Boqing Gong. 2021. Movinets: Mobile video networks for efficient
video recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition. 16020–16030.

[23] Maria Koshkina, Hemanth Pidaparthy, and James H Elder. 2021. Contrastive
learning for sports video: Unsupervised player classification. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 4528–4536.
[24] Kaustubh Milind Kulkarni and Sucheth Shenoy. 2021. Table tennis stroke recog-
nition using two-dimensional human pose estimation. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. 4576–4584.
[25] Ji Lin, Chuang Gan, and Song Han. 2019. Tsm: Temporal shift module for efficient
video understanding. In Proceedings of the IEEE/CVF International Conference on
Computer Vision. 7083–7093.

[26] Tianwei Lin, Xiao Liu, Xin Li, Errui Ding, and Shilei Wen. 2019. Bmn: Boundary-
matching network for temporal action proposal generation. In Proceedings of the
IEEE/CVF International Conference on Computer Vision. 3889–3898.

[27] Tianwei Lin, Xu Zhao, Haisheng Su, Chongjing Wang, and Ming Yang. 2018.
Bsn: Boundary sensitive network for temporal action proposal generation. In
Proceedings of the European conference on computer vision (ECCV). 3–19.
[28] Wu Liu, Chenggang Clarence Yan, Jiangyu Liu, and Huadong Ma. 2017. Deep
learning based basketball video analysis for intelligent arena application. Multi-
media Tools and Applications 76, 23 (2017), 24983–25001.

[29] Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zisserman. 2019. Use
what you have: Video retrieval using representations from collaborative experts.
arXiv preprint arXiv:1907.13487 (2019).

[30] Yang Liu, Cheng Lyu, Zhiyuan Liu, and Dacheng Tao. 2019. Building effective
short video recommendation. In 2019 IEEE International Conference on Multimedia
& Expo Workshops (ICMEW). IEEE, 651–656.

[31] Ze Liu. 2021. Video swin transformer. https://arxiv.org/pdf/2106.13230v1.pdf
[32] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin,
and Baining Guo. 2021. Swin transformer: Hierarchical vision transformer using
shifted windows. In Proceedings of the IEEE/CVF International Conference on
Computer Vision. 10012–10022.

[33] Xiang Long, Chuang Gan, Gerard De Melo, Jiajun Wu, Xiao Liu, and Shilei Wen.
2018. Attention clusters: Purely attention based local feature integration for
video classification. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition. 7834–7843.

[34] Yanjun Ma, Dianhai Yu, Tian Wu, and Haifeng Wang. 2019. PaddlePaddle: An
open-source deep learning platform from industrial practice. Frontiers of Data
and Domputing 1, 1 (2019), 105–115.

[35] Pierre-Etienne Martin, Jenny Benois-Pineau, Renaud Péteri, and Julien Morlier.
2018. Sport action recognition with siamese spatio-temporal cnns: Application
to table tennis. In 2018 International Conference on Content-Based Multimedia
Indexing (CBMI). IEEE, 1–6.

[36] Pierre-Etienne Martin, Jenny Benois-Pineau, Renaud Péteri, and Julien Morlier.
2021. Three-Stream 3D/1D CNN for Fine-Grained Action Classification and
Segmentation in Table Tennis. In Proceedings of the 4th International Workshop
on Multimedia Content Analysis in Sports. 35–41.

[37] Megha Nawhal and Greg Mori. 2021. Activity graph transformer for temporal

action localization. arXiv preprint arXiv:2101.08540 (2021).

[38] Juan Carlos Niebles, Chih-Wei Chen, and Li Fei-Fei. 2010. Modeling temporal
structure of decomposable motion segments for activity classification. In European
conference on computer vision. Springer, 392–405.

[39] Olympedia. 2012. Referee Biographical Information. http://www.olympedia.org/

athletes/5004924

[40] Zhiwu Qing, Haisheng Su, Weihao Gan, Dongliang Wang, Wei Wu, Xiang Wang,
Yu Qiao, Junjie Yan, Changxin Gao, and Nong Sang. 2021. Temporal context
aggregation network for temporal action proposal refinement. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 485–494.
[41] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015. Faster r-cnn:
Towards real-time object detection with region proposal networks. Advances in
neural information processing systems 28 (2015).

[42] Dian Shao, Yue Zhao, Bo Dai, and Dahua Lin. 2020. Finegym: A hierarchical video
dataset for fine-grained action understanding. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition. 2616–2625.

11

[43] Zheng Shou, Dongang Wang, and Shih-Fu Chang. 2016. Temporal action lo-
calization in untrimmed videos via multi-stage cnns. In Proceedings of the IEEE
conference on computer vision and pattern recognition. 1049–1058.

[44] Karen Simonyan and Andrew Zisserman. 2014. Two-stream convolutional net-
works for action recognition in videos. Advances in neural information processing
systems 27 (2014).

[45] Khurram Soomro and Amir R Zamir. 2014. Action recognition in realistic sports

videos. In Computer vision in sports. Springer, 181–208.

[46] Panyawut Sri-Iesaranusorn, Felan Carlo Garcia, Francis Tiausas, Supatsara Wat-
tanakriengkrai, Kazushi Ikeda, and Junichiro Yoshimoto. 2021. Toward the Perfect
Stroke: A Multimodal Approach for Table Tennis Stroke Evaluation. In 2021 Thir-
teenth International Conference on Mobile Computing and Ubiquitous Network
(ICMU). IEEE, 1–5.

[47] Haisheng Su, Weihao Gan, Wei Wu, Yu Qiao, and Junjie Yan. 2020. Bsn++:
Complementary boundary regressor with scale-balanced relation modeling for
temporal action proposal generation. arXiv preprint arXiv:2009.07641 (2020).
[48] Haritha Thilakarathne, Aiden Nibali, Zhen He, and Stuart Morgan. 2021. Pose is
all you need: The pose only group activity recognition system (POGARS). arXiv
preprint arXiv:2108.04186 (2021).

[49] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri.
2015. Learning spatiotemporal features with 3d convolutional networks. In
Proceedings of the IEEE international conference on computer vision. 4489–4497.

[50] Du Tran, Heng Wang, Lorenzo Torresani, and Matt Feiszli. 2019. Video classi-
fication with channel-separated convolutional networks. In Proceedings of the
IEEE/CVF International Conference on Computer Vision. 5552–5561.

[51] Roman Voeikov, Nikolay Falaleev, and Ruslan Baikulov. 2020. TTNet: Real-time
temporal and spatial video analysis of table tennis. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition Workshops. 884–885.
[52] Nick Wadsworth, Lewis Charnock, Jamie Russell, and Martin Littlewood. 2020.
Use of video-analysis feedback within a six-month coach education program at
a professional football club. Journal of Sport Psychology in Action 11, 2 (2020),
73–91.

[53] Bin Wang, Wei Shen, FanSheng Chen, and Dan Zeng. 2019. Football match
intelligent editing system based on deep learning. KSII Transactions on Internet
and Information Systems (TIIS) 13, 10 (2019), 5130–5143.

[54] Limin Wang, Yu Qiao, Xiaoou Tang, et al. 2014. Action recognition and detection
by combining motion and appearance features. THUMOS14 Action Recognition
Challenge 1, 2 (2014), 2.

[55] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and
Luc Van Gool. 2016. Temporal segment networks: Towards good practices for
deep action recognition. In European conference on computer vision. Springer,
20–36.

[56] Shangfei Wang, Longfei Hao, and Qiang Ji. 2019. Knowledge-augmented mul-
timodal deep regression bayesian networks for emotion video tagging.
IEEE
Transactions on Multimedia 22, 4 (2019), 1084–1097.

[57] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. 2018. Non-local
neural networks. In Proceedings of the IEEE conference on computer vision and
pattern recognition. 7794–7803.

[58] Xiang Wang, Shiwei Zhang, Zhiwu Qing, Yuanjie Shao, Changxin Gao, and
Nong Sang. 2021. Self-supervised learning for semi-supervised temporal action
proposal. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition. 1905–1914.

[59] Chen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan Yuille, and Christoph
Feichtenhofer. 2021. Masked Feature Prediction for Self-Supervised Visual Pre-
Training. arXiv preprint arXiv:2112.09133 (2021).

[60] Chao-Yuan Wu, Ross Girshick, Kaiming He, Christoph Feichtenhofer, and Philipp
Krahenbuhl. 2020. A multigrid method for efficiently training video models. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.
153–162.

[61] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy. 2018.
Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video
classification. In Proceedings of the European conference on computer vision (ECCV).
305–321.

[62] Mengmeng Xu, Juan-Manuel Pérez-Rúa, Victor Escorcia, Brais Martinez, Xiatian
Zhu, Li Zhang, Bernard Ghanem, and Tao Xiang. 2021. Boundary-sensitive
pre-training for temporal localization in videos. In Proceedings of the IEEE/CVF
International Conference on Computer Vision. 7220–7230.

[63] Huan Yan, Chunfeng Yang, Donghan Yu, Yong Li, Depeng Jin, and Dah Ming
Chiu. 2019. Multi-site user behavior modeling and its application in video
recommendation. IEEE Transactions on Knowledge and Data Engineering 33, 1
(2019), 180–193.

[64] Serena Yeung, Olga Russakovsky, Greg Mori, and Li Fei-Fei. 2016. End-to-end
learning of action detection from frame glimpses in videos. In Proceedings of the
IEEE conference on computer vision and pattern recognition. 2678–2687.

[65] Jun Yuan, Bingbing Ni, Xiaokang Yang, and Ashraf A Kassim. 2016. Temporal
action localization with pyramid of score distribution features. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition. 3093–3102.

[66] Joe Yue-Hei Ng, Matthew Hausknecht, Sudheendra Vijayanarasimhan, Oriol
Vinyals, Rajat Monga, and George Toderici. 2015. Beyond short snippets: Deep
networks for video classification. In Proceedings of the IEEE conference on computer
vision and pattern recognition. 4694–4702.

[67] Yi Zhu, Xinyu Li, Chunhui Liu, Mohammadreza Zolfaghari, Yuanjun Xiong,
Chongruo Wu, Zhi Zhang, Joseph Tighe, R Manmatha, and Mu Li. 2020. A com-
prehensive study of deep video action recognition. arXiv preprint arXiv:2012.06567
(2020).

12

