September 8, 2021 0:38 WSPC/INSTRUCTION FILE

main

Random Matrices: Theory and Applications
© World Scientiﬁc Publishing Company

Spectral properties of sample covariance matrices
arising from random matrices
with independent non identically distributed columns

Cosme Louart∗
GIPSA-lab, 11 rue des Mathématiques, 38401 St Martin d’Hères
cosmelouart@gmail.com

Romain Couillet†
LIG-lab, 700 avenus Centrale, 38401 St Martin d’Hères
romain.couillet@gipsa-lab.grenoble-inp.fr

Received (Day Month Year)
Revised (Day Month Year)

k

A

−

∈ M

zIp)−1 and A

n XX T
∗/√n). Here, we show that
k

Given a random matrix X = (x1, . . . , xn)
p,n with independent columns and sat-
isfying concentration of measure hypotheses and a parameter z whose distance to the
spectrum of 1
n XX T should not depend on p, n, it was previously shown that the function-
als Tr(AR(z)), for R(z) = ( 1
p deterministic, have a standard
∈ M
E[R(z)]
O(1/√n),
deviation of order O(
k
where ˜R(z) is a deterministic matrix depending only on z and on the means and covari-
ances of the column vectors x1, . . . , xn (that do not have to be identically distributed).
This estimation is key to providing accurate ﬂuctuation rates of functionals of X of
interest (mostly related to its spectral properties) and is proved thanks to the in-
n(H) of diagonal matrices with
troduction of a semi-metric ds deﬁned on the set
n(H):
complex entries and positive imaginary part and satisfying, for all D, D′
ds(D, D′) = maxi∈[n] |
(Di)
Possibly most importantly, the underlying concentration of measure assumption on
the columns of X ﬁnds an extremely natural ground for application in modern statistical
machine learning algorithms where non-linear Lipschitz mappings and high number of
classes form the base ingredients.

i))1/2.

Di −

kF ≤

D′
i|

˜R(z)

∈ D

(D′

/(

−

D

ℑ

ℑ

Keywords: random matrix theory ; concentration of measure ; convariance matrices ;
contractivity and stability of quadratic equations.

Mathematics Subject Classiﬁcation 2000: 15A52, 60B12, 62J10

1
2
0
2

p
e
S
6

]

R
P
.
h
t
a
m

[

1
v
4
4
6
2
0
.
9
0
1
2
:
v
i
X
r
a

Notations

For any integer d
complex variable z

≥
∈

∗GIPSA-lab.
†LIG-lab, GIPSA-lab.

1, we will note for simplicity [d]
C, we denote ¯z =

. Given a
1, . . . , d
}
(z) its conjugate. We denote

≡ {

(z)

i

ℜ

−

ℑ

1

 
 
 
 
 
 
September 8, 2021 0:38 WSPC/INSTRUCTION FILE

main

2

z

}

{

}

0

x

∈

ℑ

≥

x
k

(z) > 0

and H =

2)1/2. We denote

∈
{
k · k
n
i=1 |

R+ =
denote
= (

C,
R, x
the Euclidean norm on Cp deﬁned for any x = (x1, . . . , xp)
Mp,n the set of real matrices of size p
xi|

k
the set of squared matrices of size p and
Given a set A
set of diagonal matrices with entries in A. Given a diagonal matrix ∆
denote ∆1, . . . , ∆n its diagonal elements. We introduce on
the spectral norm
as

Mp,n(A) is the set of matrices with entries in A and

, the hyperbolic space. We
Cp as
n,
Mp
Op ⊂ Mp the set of orthogonal matrices.
Dn(A) the
∈ Dn(A), we
Dn(C))
∈ Mp,n(A)

k · kF deﬁned for any M

Mp,n(C) (and on

and the Frobenius norm

∈
×

k · k

C,

P

⊂

M

k

k

= sup
x

k

k≤

1 k

M x
k

and

M

kF =

k

q

Tr(M ¯M T ).

(A), the set of functions from A to R and

Given two sets A, B, we note
the set of functions from A to B.

F

(A, B),

F

Introduction

Considering a sample covariance matrix 1
is the data matrix, we note Sp( 1
distribution of 1
≡
its Stieltjes transform expressed as:

n XX T , denoted µ

∈

n XX T ) the spectrum of 1

n XX T , where X = (x1, . . . , xn)

∈ Mp,n
n XX T . The spectral
n XX T ) δλ, is classically studied through

Sp( 1

λ

P
XX T

g : C

Sp

\

1
n

(cid:18)
z

C

(cid:19)

−→

7−→

dµ(λ)
z
λ

.

−

R

Z

The relevance of the Stieltjes transform has been extensively justiﬁed in some sem-
inal works [MP67,Sil86] by the Cauchy integral that provides for any analytical
mapping f deﬁned on a neighborhood of a subset B

n XX T ) the identity:

Sp( 1

⊂

f (λ)dµ(λ) =

f (z)g(z)dz,

1
2iπ

\

C

→

Iγ

ZB
Sp( 1
n XX T ) is a closed path on which f is deﬁned and whose
where γ : [0, 1]
Sp( 1
n XX T ). But we can go further and
interior Iγ satisﬁes Iγ ∩
∩
approximate linear functionals of the eigenvectors thanks to the resolvent. If we
note EB the random eigenspace associated to the eigenvalues of 1
n XX T belonging
to B, and ΠB the orthogonal projector on EB, then for any deterministic matrix
A

n XX T ) = B

Sp( 1

∈ Mp:

1
2iπ

Zγ

Tr(ΠBA) =

Tr(AR(z))dz

with R(z)

(cid:19)
(cid:18)
The matrix R(z) is commonly called the resolvent of 1
n XX T . It satisﬁes in partic-
Sp( 1
p Tr(R(z)). It thus naturally becomes
ular that, for all z
the central element of the study of the spectral distribution. One of the ﬁrst tasks

n XX T ), g(z) = 1

C

∈

\

1
n

XX T

−

≡

1

−

.

zIp

(0.1)

September 8, 2021 0:38 WSPC/INSTRUCTION FILE

main

3

in random matrix theory is to devise a so called “deterministic equivalent” for R(z)
([HLN07]), that we will denote here ˜R(z). Speciﬁcally, we look for a deterministic
matrix computable from the ﬁrst statistics of our problem (the means and covari-
ances of the xi’s) and close to E[R]. Two questions then arise:

(1) Is R(z) close to E[R(z)] ?
(2) What does this notion of closeness really mean ?

The ﬁrst questions relate to concentrations properties on R(z) that arise from
concentration properties on X. The study of random matrices originally studied
with i.i.d. entries ([MP67], [Yin86]), mere Gaussian hypotheses ([BKV96]), or with
weaker hypotheses concerning the ﬁrst moments of the entries (supposed to be
independent or at least independent up to an aﬃne transformation). Some more
recent works showed the concentration of the spectral distribution of Wishart or
Wigner matrices with log-concave hypotheses that relax some independence as-
sumptions ([AC15]) or very light hypotheses on the quadratic functionals of the
columns ([BZ08]), improved in ([Yas16]) or on the norms of the columns and the
rows ([Ada11]). In the present work, we adopt similar hypotheses (slightly more
general than [AC15] and more precise than [BZ08] and [Ada11]) originating from
the concentration of measure theory. The concentration hypothesis on X is de-
scribed in Assumption 0.2: for simplicity of exposition in the introduction, we will
just assume here that the matrix X is a λ-Lipschitz transformation of a Gaussian
vector Z
n, p
(we will write λ
O(1)). We also require the columns x1, . . . , xn of X to be inde-
pendent, but the entries of the columns may have intricate dependencies, as long
as they remain λ-Lipschitz transformations of a Gaussian vector. This represents
a wide range of random vectors and, among the most commonly studied random
vectors, this mainly merely excludes the heavy tailed distributions and the discrete
distributions.a Some large description of the concentration of measure phenomenon
can be found in [Led05] and [BLM13] and more speciﬁcally, some elementary appli-
cations to random matrices are provided in [Tao12], [Ver18] and [LC19]. We further
provide in subsection “Practical implications” a series of arguments demonstrating
the relevance of this approach in classical problems of statistical machine learning.
The concentration inequality on X in a sense “propagates” to the resolvent that
1

N but for a Lipschitz parameter λ

(0, Id) for any given d

then satisﬁes, for all deterministic matrices A such that
and for all z not too close to the spectrum of 1

Tr(AAT )

kF ≡

∼ N

≪

≤

≤

A

∈

k

n XX T :

P (
|

Tr (A (R(z)

E[R(z)]))

t)

≤

| ≥

−

cnt2

Ce−

+ Ce−

p
cn,

(0.2)

for some numerical constants C, c independent of n, p. We see from (0.2) the im-
portant beneﬁt gained with our concentration hypothesis on X: it provides simple

aSome of these random vectors still satisfy what is referred to as “convex concentration” hypotheses
and their sample covariance matrix may still be studied but to the expense of more advanced
control (see [LC19] and a coming follow-up work).

September 8, 2021 0:38 WSPC/INSTRUCTION FILE

main

4

.
→ ∞
The condition

quasi-asymptotic results on the convergence of the resolvent, while most of the re-
sults on random matrices are classically expressed in the limiting regime where
n, p

k

A

kF ≤

1 answers our second question: a speciﬁcity of our ap-
proach is to control the convergence of the resolvent with the Frobenius norm at a
speed of order O(1/√n). The concentration inequality (0.2) means that all linear
forms of R(z), which are 1-Lipschitzb for the Frobenius norm, have a standard de-
viation of order O(1/√n); this is crucial to be able to estimate quantities expressed
in (0.1). Generally, the only studied linear forms of the resolvent are the Stieltjes
1
p Tr(R(z)) (it is 1/√p-Lipschitz so its standard deviation is of
transform g(z) =
order O(1/√pn) which is a classical result although not exactly under a concentra-
tion of measure assumption) or projections on deterministic vectors uT R(z)u, for
which only the concentration in spectral norm with a speed of order O(1/√n) is
needed.

−

Those remarks gain a real importance when we are able to estimate the expecta-
tion of R(z) with a deterministic equivalent (that we can compute). In this article,
we look for a closeness relation in Frobenius norm:

˜R(z)
(cid:13)
(cid:13)
We may then replace in (0.2) the term “E[R(z)]” by ˜R(z) which we are able to
(cid:13)
compute from the expectations and covariances of the columns x1, . . . , xn.

E[R(z)]
(cid:13)
(cid:13)
(cid:13)

F ≤

−

O

(cid:19)

(cid:18)

.

1
√n

Note, and this is one of our important contributions, that we do not assume
that the columns are identically distributed: in particular, the means and covari-
ances can be all diﬀerent (although they have to satisfy some boundedness prop-
erties expressed in Assumptions 0.4 and 0.5). This remark may be related to the
studies made of matrices X with a variance proﬁle: but this is here even more gen-
eral because the laws of the columns are not solely deﬁned from their means and
covariances (although the spectral distribution of 1
n XX T just depends on these
quantities).

The main issue raised by this hypothesis is that the deterministic equivalent is

then deﬁned from the n diagonal entries of a diagonal matrixc ˜Λz = Diag(˜Λz
Dn(C) solution to:

i )i
∈

[n] ∈

[n] : ˜Λz

i = z

i
∀

∈

1
n

−

Tr

˜Λz

Σi ˜Q

with ˜Q

˜Λz

(cid:16)

(cid:17)

Ip −

≡  

1
n

1

−

Σi
˜Λz

i !

n

i=1
X

,

(0.3)

in which Σi = E[xixT
i ]. The diﬃculties are then (i) to prove the existence and
uniqueness of ˜Λz and (ii) to ensure some stability propertiesd on this equation

O(1)

bor λ-Lipschitz with λ
≤
cThe interest to resort to a diagonal matrix of
later – mainly to employ Proposition 1.4 in a natural formalism.
dConceptually, it means that if we have a diagonal matrix L
z

(cid:17) then L is “close” to ˜Λz.

n Tr (cid:16)Σi ˜QL

M

1

−

n rather than to a vector of Rn will be clearer

∈ M

n satisfying

i
∀

[n] : Li ≡

∈

September 8, 2021 0:38 WSPC/INSTRUCTION FILE

main

5

k

−

˜R(z)

E[R(z)]

O(1/√n), where ˜R(z)

kF ≤

eventually allowing us to assert that
≡
˜Q ˜Λz
1
. Those two diﬃculties disappear when most of the Σ1, . . . , Σn are equal; in
z
other words, when we have a ﬁnite number of distinct distributions for the xi’s
(see [LC19]). When they are all diﬀerent, our solution consists in introducing a
convenient semi-metrice ds on which the ﬁxed point equation satisﬁed by ˜Λz is
contractive, leading (after still some work since a semi-metric is not as easy to treat
as if ds were a true metric) to existence, uniqueness and stability properties. This
semi-metric, quite similar to the one already introduced in [LC20] to study robust
∈ Dn(H) as:
estimators, is deﬁned for any D, D′
D

D′

ds(D, D′) =

−
(D)

ℑ

ℑ

.

(D′) (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

p
The semi-metric appears as a central object in random matrix theory that al-
ready gained some visibilty in a squared form in [AKE16], where it relates to the
hyperbolic metric through the identity dH(z, z′) = cosh(ds(z, z′)2
1). In particu-
lar, we can show that any Stieltjes transform is 1-Lipschitz for this semi-metric
(see Appendix A). Once the appropriate semi-metric is identiﬁed, contractivity
properties are not suﬃcient to prove the existence and uniqueness to (0.3): one
also needs to introduce the correct space over which the mapping is contractive
(
DI z

z ∈ Dn(H)
}

∈ Dn(H), D

Let us now present more precisely our assumptions and main results.

≡ {

D

−

).

Assumptions and Main Results
Let us start with the deterministic results concerning the diagonal matrices ˜Λz,
which do not require any particularly constraining assumption.

Theorem 1. Given n nonnegative symmetric matrices Σ1, . . . , Σn ∈ Mp, for all
z
∈

H, the equation:

i
∀

∈

[n], Li = z

Tr

Σi



Ip −

1
n

−

n

1
n

i=1
X

∈ Dn(H) that we denote ˜Λz.

1

1

−

Σi
Li !





(0.4)

admits a unique solution L

Letting ˜R : z

, we can construct with ˜Λz a Stieltjes
transform whose associated distribution converges towards the spectral distribution
(cid:17)
of 1

P
n XX T , where X = (x1, . . . , xn) and for all i

[n], Σi = E[xixT

zΣi
Li −

zIp

i ].

7→

n
i=1

1
n

(cid:16)

−

∈

Theorem 2. The mapping ˜g : z
˜µ of compact support ˜S

R+.

⊂

1

p Tr( ˜R) is the Stieltjes transform of a measure

→

eA semi metric is deﬁned as a metric that does not satisﬁes the triangular inequality.

 
September 8, 2021 0:38 WSPC/INSTRUCTION FILE

main

6

Let us now set the stage for the results on X. We want to track the speed of
convergence of our various quantities of interest as a function of n to provide a
quasi asymptotic result that can be used for suﬃciently large values of n. We will
then assume that all our quantities depend on n, X = Xn but also p = pn that
should not be too large compared to n:

Assumption 0.1. There exists a constant K > 0 such that

N, p

n

∀

∈

Kn.

≤

The concentration hypothesis on X = Xn is a bit involved: it is issued from
the concentration of measure theory and concerns a wide range of random vectors,
N, and Φ :
in particular any random matrix X = Φ(Z) with Z
(Rd,

k · kF ), λ-Lipschitz. For more examples, see [Led05].
Assumption 0.2. There exist two constants C, c > 0 such that for all n
for all 1-Lipschitz mapping f :

N and

∈

Mp,n,
(

(0, Id), d

∼ N

k · k

R:

→

∈

)

Mp,n →

| ≥
In the previous example where X = Φ(Z), C = 2 and c = λ√2 which do not

≤

−

f (X)

E[f (X)]

t)

Ce−

(t/c)2

.

P (
|

depend on d that can be taken as large as necessary.

A third natural and fundamental hypothesis is to assume that the n columns
of X = (x1, . . . , xn) are independent. Again, we do not assume that x1, . . . , xn
are identically distributed: we can possibly have n diﬀerent distributions for the
columns of X.

Assumption 0.3. X has independent columns x1, . . . , xn ∈

Rp.

Let us note for simplicity, for any i

[n]:

∈

E[xi]

µi ≡

Σi ≡

E[xixT
i ]

and

Ci ≡

Σi −

µiµT
i .

It is easy to deduce from Assumption 0.2 (see [LC19]) that there exists a constant
N,
K > 0 such that for all n
O(1). But to have the best convergence
bounds, we also need to impose:f

Cik ≤

∈

k

Assumption 0.4.

K > 0 such that

n

∃

∈
We conclude with a last assumption which is likely central to precisely ap-
proximate the support of the spectral distribution.g Although we are unsure of its
importance, our line of arguments could not avoid it; it is nonetheless quite a weak
constraint in view of the practical use of our result.

∈

∀

k

N,

i
∀

[n] :

µik ≤

K.

Assumption 0.5.

K > 0 such that

∃

N,

n

∀

∈

i
∀

[n] : Σi ≥

∈

KIp.

f In [LC19], we only supposed that
O(√n), however, then we only had an approximation
of the resolvent with the spectral norm, here we will provide a similar convergence result with the
Frobenius norm.
gthe values of the Stieltjes distribution g(z) can be approximated for z
the real axis or for z

R suﬃciently far from the support without this assumption.

C suﬃciently far from

µik ≤
k

∈

∈

September 8, 2021 0:38 WSPC/INSTRUCTION FILE

main

7

Recalling that our goal is to estimate the spectral distribution of 1

n XX T , we

denote λ1 ≥ · · · ≥

λp ≥

0 the p eigenvalues of 1

n XX T . We wish to estimate:

µ =

1
p

p

i=1
X

δλi.

Now, let us remark that the spectrum of 1

n XX T is closely related to the spec-

trum of 1

n X T X via the equivalence:

λ

Sp

∈

1
n

XX T

0

\ {

}

⇐⇒

λ

Sp

∈

1
n

X T X

0

.

\ {

}

(cid:18)

(cid:19)
As a consequence, one of the two matrices has
supplementary zeros in its
spectrum. Those zeros not carrying any speciﬁc information about the distribution
of 1
n XX T , we will remove them from our study to avoid unnecessary complications.
The min(p, n) ﬁrst entries of σ( 1
n X T X) are the same (and some of
them can cancel), we thus naturally introduce the sets:

n XX T ) and σ( 1

−

(cid:18)

(cid:19)

n

p

|

|

S

E[λi], i

[n]

and

}

∈

≡ {
(we have the inclusion S
0
}
on zero, instead of studying, as it is usually done, the resolvent R(z) = ( 1
zIp)−

−
, but possibly, 0

1, we rather look at:

∈
0). To avoid the issue

n XX T

0 ∪ {

⊂

−

∈

S

S

}

−

−

S

0 ≡ {

E[λi], i

[min(p, n)]

,

Ip −
(cid:18)
Qz
that has the advantage of satisfying

≡

Qz

Given a set T

introduce the semi-norm

k

k ≤

C, we note for any ε > 0, T ε =
−0, deﬁned for any f

⊂

1
zn

XX T

1

−

,

(cid:19)

0.
T,

z

S

O(1), for all z
∈
−
C,
t
∈
{
∈
∃
(C) as:
∈ F
.

f (z)
|

|

k · kSε
f

kSε

−0

k

= sup
Sε
z

C

∈

\

−0

t

z

|

−

| ≤

ε

}

. We

Theorem 3. Under Assumptions 0.1-0.5, given ε > 0, there exist two constants
C 1-Lipschitz for the norm
C, c > 0, such that for all linear mapping u :
k · kSε

(C)

−0:

→

F

P (
|

u(g

−

˜g)

| ≥

t)

≤

cnpt2

Ce−

+ Ce−

cn.

In particular, following the preliminary inferences of the introduction, for any

C, since the integration on bounded paths of C

Sε
−

0

\

−0, we can approximate:

analytical mapping f : C
is Lipschitz for the norm

P

f (λ)dµ(λ)

→
k · kSε
1
2iπ

−

Z

(cid:18)(cid:12)
(cid:12)
(cid:12)
for any close path Sε
O(1). There
(cid:12)
−
exists a correspondence between a distribution and its Stieltjes transform: denoting

0 with length lγ satisfying lγ ≤

Sε
−

0 ⊂

(cid:12)
(cid:12)
(cid:12)
(cid:12)

⊂

(cid:19)

C

γ

\

Iγ

f (z)g(z)dz

t

≥

≤

cnpt2

Ce−

+ Ce−

cn,

September 8, 2021 0:38 WSPC/INSTRUCTION FILE

main

8

µ the spectral distribution of 1

n XX T , we have indeed for any real a < b:

µ([a, b]) = lim
0
→

b

1
π

(g(x + iy))dx.

0

y

→

1
π

a ℑ
Z
As seen on Figure 1, this measure is naturally close to ˜µ deﬁned for any real a < b
as ˜µ([a, b]) = limy
(˜g(x + iy))dx. For computation issues, the quantities p
and n are chosen relatively small; the convergence of the iteration of the ﬁxed point
equation (0.4) deﬁning ˜Λ is in particular very slow when the covariances Σ1, . . . , Σn
are all diﬀerent from one another. Although the contractivity of (0.4) for the semi
metric ds ensures the convergence of the iterations, the Lipschitz parameter is very
close to 1 when z = x + iy is close to the spectrum (that happens naturally when
y is chosen close to 0 to estimate d˜µ).

b
a ℑ
R

to the approximation of the mapping R : z
be far from zero when 0
S
−
(C,
semi-norm

We even have a stronger result that provides inferences on eigenvectors thanks
. Here z must
7→
0). For this result, we introduce the
(cid:1)
(cid:0)
Mp) as:
kF .
f (z)

S (even if 0 /
∈
∈ F
kF,Sε = sup

∈
k · kF,Sε, deﬁned for every f

n XX T

zIp

−

k

f

−

C

1

1

Sε k

z

∈

\

Theorem 4. Under Assumptions 0.1-0.5, given ε > 0, there exist two constants
R, 1-Lipschitz for the norm
C, c > 0, such that for all linear form u :
k · kF,Sε:

Mp →

P

u

R

˜R

−

(cid:16)

(cid:16)(cid:12)
(cid:12)
(cid:12)

t

≥

(cid:17)

(cid:17)(cid:12)
(cid:12)
(cid:12)

cnt2

Ce−

+ Ce−

cn.

≤

The projections on deterministic vectors provide us with good estimates on iso-
lated eigenvectors, but a concentration in spectral norm would have been suﬃcient
for this kind of result. A key consequence of Theorem 4 lies in its also providing
Rp; this is
accurate estimates of projections on high dimensional subspaces F
shownh in Figure 2 that depicts some of these projections with increasing num-
n XX T , EA the
bers of classes. If we let A
eigenspace associated to those eigenvalues and ΠA and ΠF , respectively the orthog-
O(√p)):
onal projection on EA and F , we can bound (since

R be a subset of the eigenvalues of 1

dim(F )

⊂

⊂

ΠF kF =

k

≤

Tr(ΠF ΠA)

1
2ipπ

−

Iγ

Tr(ΠF ˜R(z))dz

P

1
p

(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)

t

≥

≤

(cid:19)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

p
cnpt2

Ce−

+ Ce−

cn.

hIt must be noted that that the setting of Figure 2 does not exactly fall under the hypotheses
O(√p)), as the amplitude of the signals must be suﬃciently
of the paper (since here
large for the resulting eigenvalues to isolate from the bulk of the distribution when the number of
classes is high (√p
14 is not so large). However, even in this extreme setting the prediction are
good.

E[xi]
k

k ≥

≈

September 8, 2021 0:38 WSPC/INSTRUCTION FILE

main

)
λ
(
µ
d

y
t
i
s
n
e
d

l
a
r
t
c
e
p
s

0.4

0.2

0

9

empirical distribution
prediction from ˜Λ

0.4

0.2

0

0

5

10
eigenvalues λ

15

0

5

10
eigenvalues λ

15

1, . . . , 20

n XX T and its deterministic estimate obtained from ˜Λ for n = 160
Fig. 1. Spectral distribution of 1
and p = 80. Introducing P an orthogonal matrix chosen randomly and Σ
p such that for
(0, Σ)
j
i
∀
and (right)
[n]. The
histograms would have been similar for any other concentrated vectors x1, . . . , xn having the
same covariances and comparable observation diameter (see Deﬁnition 1.1)

(0, Σi), where Σ1 = Σ and Σi+1 = P T ΣiP for all i

, Σj = 1 and for j
}
[n], xi ∼ N
i
∀

, Σj = 8, we chose (left)
}

[n], xi ∼ N
∈

∈ D
∈

21, . . . , 80

∈ {

∈ {

∈

Practical implications

Results on the large dimensional behavior of sample covariance matrix models are
far from a novelty and the present article may at ﬁrst sight be seen as yet another
variation stacked on the pile of previous generalizations to the seminal work of
Mar˘cenko and Pastur [MP67].

The core motivation for the present development of a concentration of measure
approach to random matrices does not arise as a mere additional mathematical ex-
ercise but more fundamentally as a much needed new toolbox to address the mod-
ern problem of eﬃcient big data processing. Speciﬁcally, the question of mastering
artiﬁcial intelligence and automated data processing in the era of the data deluge
comes along with two structural diﬃculties: (i) eﬃcient machine learning algorithms
(starting with neural networks) inherently rely on a succession of non-linear oper-
ators which, as it appears, guarantee algorithm stability when these operators are
Lipschitz ; (ii) while hitting several instances of the curse of dimensionality (which
make modern neural networks extremely resource consuming), algorithm stabil-
ity also appears to be guaranteed when large and numerous data are considered;
besides, and possibly most importantly, as evidenced in recent works [SLTC20],
assuming the large dimensional data to be instances of concentrated random vectors
is a satisfying model to theoretically track the behavior of real algorithms applied
on real data (precisely, it can be shown, as with the example of artiﬁcial images
produced with generative adversarial networks, that real data of practical interest
– such as images, sounds, or modern natural language embeddings – are akin to
concentrated random vectors).

September 8, 2021 0:38 WSPC/INSTRUCTION FILE

main

10

)
λ
(
µ
d

y
t
i
s
n
e
d

l
a
r
t
c
e
p
s

)
λ
(
µ
d

y
t
i
s
n
e
d

l
a
r
t
c
e
p
s

0.8

0.6

0.4

0.2

0

0.8

0.6

0.4

0.2

0

empirical distribution
prediction from ˜Λ
integration path (in C)

k = 20

EA

0.8

0.6

0.4

0.2

0

k = 42

EA

0

5

10

0

5

10

eigenvalues λ

eigenvalues λ

100

prediction with ˜Λz
value from drawing of X
y = k

50

T ΠEA 1

1

tr(ΠEA U U ′)

k = 75

EA

0

5

10

eigenvalues λ

0

0

20
60
40
number of classes k

80

∈

∈

N
[20], xj+lnk ∼ N

Fig. 2. Prediction of the alignement of the signals in the data towards the eigen space of the
biggest eigen values of 1
n XX T for p = 200, n = knk where nk = 20 and k is the number of classes
taking the values k = 10, 15, 20, 30, 35, 42, 50, 62, 75. The signals u1, . . . , uk are drawn randomly
(0, Ip), and we let U = (u1, . . . , uk)
∈ Mp,k. Then, for
and independently following a law
(uj , Ip). (top and bottom left) representation of the
all j
[k] and l
n XX T and its prediction with ˜Λz.(bottom right) representation (with
spectral distribution of 1
T ΠEA 1 and their prediction (smooth line) with
marks) of the quantities Tr(ΠEA U U T ) and 1
)) and
1) on the path drawn in red on
the integration of, respectively,
the other graphs. The line y = k represents instances of Tr(ΠA) = k that can be approximated
integrating 1
). The projection Tr(ΠEA U U T ) is a little lower than k because the eigen
vectors associated to the highest eigen values of 1
n XX T are not perfectly aligned with the signal
due to the randomness of X.

zπ Tr( ˜Q ˜Λz
( 1

zπ Tr( ˜Q ˜Λz

( 1
zπ 1

T ˜Q ˜Λz

ℑ

ℑ

These two features of modern data processing forcefully call for a revision of
large dimensional statistics, starting with random matrix theory, towards a generic
framework of concentration of measure theory for large dimensional vectors and
matrices. The present work, buttressed on our detailed contribution [LC20], pro-
vides the mathematical base ground for applying this framework to elementary (yet
already quite rich) models of large dimensional “realistic sample” covariance matri-

September 8, 2021 0:38 WSPC/INSTRUCTION FILE

main

11

ces. Note in particular that one important contribution of this paper is to generalize
the results to unlimited number of classes (in classiﬁcation tasks for instance) which
is a classical question raised in machine learning problems.
Among other applications, our results are applicable to:

•

•

track the behavior of the successive non-linear layers of neural networks applied
on (concentrated random vector-modelled) real data; the main interest of our
framework in this setting is that concentration of measure propagates seamlessly
from layer to layer (which would not be valid if assumptions of independence
between the data vector entries were made);
evaluate the performance of most standard machine learning algorithms for
classiﬁcation and regression, all based on Lipschitz operators: from hyperplane-
based (support vector machines) to graph-based (semi-supervised learning)
classiﬁcations.

We consequently believe that the present work may serve, in the long run, as
a springboard to simplify and systematize the analysis, improvement, and cost
eﬃciency (thus reduce the carbon footprint) of modern large data processing algo-
rithms.

The remainder of the article is structured as follows. We ﬁrst provide some basic
notations and results to handle concentrated random vectors (Section 1). Second,
we explain why and how we have to place ourselves on an event of overwhelming
probability where Qz is bounded to show our convergence results (Section 2).
Third, we prove the concentration of the resolvent (Section 3) and provide a ﬁrst,
intermediary, deterministic equivalent which at this stage will not be numerically
satisfying (Section 4). Then we introduce the semi-metric ds and prove Theorem 1
(the existence and uniqueness of ˜Λ) (Section 5) and devise our second determin-
istic equivalent ˜Q ˜Λ that we can compute from ˜Λ and that satisﬁes Theorem 4
(Section 6). Eventually, we show that ˜g is a Stieltjes transform (Theorem 2) and
that it approaches the Stieltjes transform of our spectral distribution (Theorem 3)
(Section 7).

September 8, 2021 0:38 WSPC/INSTRUCTION FILE

main

12

1. Concentration of Measure notations and general inferences

∈ Mp,n or even for the column vectors xi ∈

Concentration of measure is a phenomenon that appears for random vector in high
Rp. The
dimensions, in our case, for X
idea is to track the concentration of real "observations" (or "functionals") of those
random vectors through dimensionality (here when n gets big since p is seen as
an integer sequence depending on n). More precisely, we say that a random vector
R 1-Lipschitz satisﬁes
Z
interesting concentration inequality detailed below.

Rn is concentrated if all the f (Z) for any f : Rn

→

∈

In what follows, we simplify greatly the approach, for a larger coverage of these
notions, we advice the reader to consult our previous works in [LC19,LC21]. To stay
in the setting that interest us, we keep the index notation n, the next deﬁnition
introduce the notation for concentrated vectors depending on n, the vector spaces
Mpn,n, Rpn ,
Dn and vector spaces
will be in the next section either
of functions taking value in those vector spaces. In all those cases, the associated
norm (or semi-norm) must be speciﬁed, but it will generally be the euclidean norm
(i.e. the Frobenius norm on matricial spaces).

Mpn,

Mn,

To control the convergence speeds, we will extensively use the notation O(an)
or o(an) in inequalities, they are deﬁned followingly, for two given sequences
(an)n

N, (bn)n

N

RN
+:

∈

∈

∈
O(an)
O(an)
o(an)

bn ≤
bn ≥
bn ≤

⇐⇒

⇐⇒

⇐⇒

∃

n

K > 0,
an ≤
ε > 0,
∀

∀
∈
O(bn)
n

∃

N, bn ≤

Kan

N, bn ≤

εan

∈

k · kn)n

0, a sequence of random vectorsi (Zn)n

Deﬁnition 1.1. Given a sequence of normed (or semi-normed) vector spaces
0 En, a sequence
(En,
≥
of positive reals (σn)n
+ and a parameter q > 0, we say that Zn is concen-
trated with an observable diameter of order O(σn) iﬀ there exist C, c > 0 such that
for all n

0 ∈

0 ∈

RN

R:

Q

≥

≥

≥

n

∈

N, for all 1-Lipschitz mapping f : En →
t)
f (Zn)

E[f (Zn)]

t > 0 : P (
|
∀

−
∝ Eq(σ). When σn ≤

(t/cσn)q

Ce−

≤

| ≥
O(1), we have Z

(1.1)

∝ Eq(1), that we

We note in that case Z
∝ Eq.
note more simply Z

With this new notation, we can rewrite Assumption 0.2 followingly:

X

∝ E2.

iA random vector Z of E is a measurable function from a probability space (Ω,
vector space (E,
but we abusively simply denote Z

) (endowed with the Borel σ-algebra); one should indeed write Z : Ω

, P) to the normed
E,

k · k

E.

→

F

∈

September 8, 2021 0:38 WSPC/INSTRUCTION FILE

main

We deduce immediately from this deﬁnition, that once he have a concentrated
random vector (for instance Z
(0, In)) then we can construct an inﬁnite number
of them thanks to Lipschitz transformations as explained in next proposition.

∝ N

13

Proposition 1.1. In the setting of Deﬁnition 1.1, if we are given a second sequence
of normed vector spaces (E′n,
k·k
RN
+ and a sequence of λn-Lipschitz transformation φn : En →
implication:

0 ∈
E′n, then we have the

0, a sequence of positive parameters (λn)n

′n)n

≥

≥

Z

∝ Eq(σ)

⇐⇒

φ(Z)

∝ Eq(λσ).

We pursue this ﬁrst deﬁnition with an important notion called the deterministic
equivalent, which is basically a relevant approximation of the expectation of a con-
centrated vector. It is in particular important when we look at linear forms on the
random vector (our main objective is to ﬁnd a deterministic equivalent for R(z)).

∈

Deﬁnition 1.2. In the setting of Deﬁnition 1.1 and given a sequence of determinis-
tic vectors ˜Z
En, we say that Z is linearly concentrated around the deterministic
equivalent ˜Z with an observable diameter of order σ iif there exist two constants
N and all u : En →
C, c > 0 such that for all n
˜Zn)]
u(Zn −
t
(cid:16)(cid:12)
(cid:12)
(cid:12)

∈
t > 0 : P
∀
˜Z

R 1-Lipschitz and linear :

We note then Z

(t/cσn)q

Ce−

≤

≥

(cid:12)
(cid:12)
(cid:12)

(cid:17)

.

± Eq(σ).

∈

This notation will be extensively employed on random variables (depending on
X and thus on n) for which the notion of Lipschitz concentration and of linear
concentration is equivaent. Of course, Z
± Eq(σ). We have
in particular the simple but important result:

∝ Eq(σ) =
⇒

E[Z]

Z

∈

Lemma 1 ([LC19], Lemma 2.6). In the setting of Deﬁnition 1.1, given two
deterministic vectors ˜Z, ˜Z ′
O(σn), then we have the equiv-
Zn −
alence:

Z ′nkn ≤

En, if

∈

k

Z

˜Z

± Eq(σ)

Z

˜Z ′

± Eq(σ).

∈

∈

⇐⇒
This Lemma gives the global scheme of our paper, we ﬁrst show that R(z)
E[R(z)]

∝
E2(1/√n), then we look for a deterministic matrix ˜R(z) such that
−
˜R(z)
O(1/√n). We provide then a useful control on the norm of linearly con-
centrated matrices (the same bound is true for vectors of Rn since they can be seen
as matrices and the spectral norm then coincides with the euclidean norm). While
being a stronger property, the lipschitz concentration described in Deﬁnition 1.1
does not allow us to get a better bound.

kF ≤

k

Proposition 1.2 ([LC19], Corollary 2.13). In the setting of Deﬁnition 1.2, if
En =

˜Z

± E2(σ)

=

⇒

E[

Z

k

˜Z

]

k

−

≤

O(√nσ)

∈

Mpn,n:
Z

September 8, 2021 0:38 WSPC/INSTRUCTION FILE

main

14

The concentration Qz
∝ E2(1/√n) can not be rigorously stated with Proposi-
tion 1.1, because when 1
nz XX T is too close to zero, then Qz diverges and is far
from a Lipschitz transformation of X (see the beggining of subsection 2 for more
details on this issue). We need here to remove the highly unprobable event that
some eigen values of 1
cn appearing in
Theorem 3 and 4). This little correction is made possible thanks to next lemma.

nz XX T get close to 1 (this is the term Ce−

An)
≥
∝ Eq(σ)

Lemma 2. In the setting of Deﬁnition 1.1, if we are given a sequence of events
An such that P(
Z

O(1), then:

=

)
| A
In particular, that implies that if a functional f : En →
F )

a subset F
the existence of two constants C, c > 0, such that for all n

En, and P(Z

(Z

⇒

≥

⊂

∈

O(1), then the concentration Z

N:

∝ Eq(σ).

R is only λ-Lipschitz on
∝ Eq(σ) implies

t > 0 : P (
|
∀

f (Z)

−

E[f (Z)

Z

|

∈

F ]

| ≥

Z

t

|

∈

F )

Ce−

(t/cλσ)q

.

∈

≤

mf }

P (
|

Proof. We know from [LC19] that there exists an equivalent deﬁnition of Deﬁni-
R, the existence of
tion 1.1 where we assume for all 1-Lipschitz mapping f : En →
a constant af such thatj:

t > 0 : P (
|
∀

f (Zn)

af | ≥

−

t)

≤

Ce−

(t/cσn)q

(it replaces inequality (1.1) in Deﬁnition 1.1). An equivalent deﬁniton is also ob-
tained replacing inequality (1.1) with:

t > 0 : P (
|
∀
where mf is a median of f (Zn).

f (Zn)

mf | ≥

−

t)

≤

Ce−

(t/cσn)q

,

Given such a mapping f , and such a median mf , let us note S
, since f is 1-Lipschitz, we can bound:

z

E

|

∈

≡ {

f (z)

≤

f (Zn)

mf | ≥

t

| An)

−

≤

≤

≤

1
An)
e−

P(
2C
K

(t/cσn)q

,

P (d(Zn, S)

t or d(Zn, Sc)

≥
(P (d(Zn, S)

≥

t

| An)
≥
t) + P ( d(Zn, Sc)

t))

≥

for some constants C, c > 0, since g : z
1-Lipschitz and 0 is

7→

d(z, S) and g′

: z

d(z, Sc) are

7→

We end this preliminary section with two important results of concentration of

measure theory that will give us the basis for the approximation of E[Qz].

jIn other word, if the observations are concentrated around a constant, then, they are also con-
centrated around their means (the converse is clear). The approach with the expectation is not
adapted here because the expectation depends on the event on which it is taken, so we would
actually have to bound P (
n]
.
| A
|

n) to show the concentration of X

E[f (Z ′

f (Zn)

| A

| A

| ≥

n)

−

t

September 8, 2021 0:38 WSPC/INSTRUCTION FILE

main

15

Proposition 1.3 (Hanson-Wright, [LC21], Proposition 8). Given two ran-
dom matrices X
O(1), then for
all deterministic matrix A

∈ Mp,n, if we assume that X

∝ E2 and

kF ≤

E[X]
∈ Mp, we have the concentrationk:
in (
E1(
± E2(
k
k

kF ) +
A

A
k

k

)

Mn,

k · kF )

O(1) and any 1-Lipschitz and linear mapping u :

X T AX

∈

E[X T AX]

In particular, given any r

R:

Mn →

≤

E

u

X T AX

E[X T AX]

r

−

O(
k

A
k

≤

r
F )

(cid:0)

i

(cid:1)(cid:12)
(cid:12)

h(cid:12)
Proposition 1.4 ([LC21], Proposition 11). Given two random matrices D
(cid:12)
∈ Mp,n and a deterministic matrix ˜D
Dn, X = (x1, . . . , xn)
˜D
k · kF ),
± E2 in (
Dn,
D
xi ∝ E2 in (Rp,
),
k · k
E[xi]
O(1),
supi
[n] k
∈
we can bound:

∈ Dn, such that:

•
•
•

k ≤

∈

∈

1
n

h

(cid:13)
(cid:13)
(cid:13)

E

X(D

˜D)X T

−

O (1)

F ≤

i(cid:13)
(cid:13)
(cid:13)

2. Resorting to a “concentration zone” for Qz
Before studying the matricial case, let us ﬁrst place ourselves in R. We consider
R, a Gaussian random variable with zero mean and variance equal to σ2
X
(0, σ2)). In particular, although we work with unidimensional variables,
(X
∝ E2. The
there can still be a possible dependence on n, and we can write X
random variable Q
= 1 and its law fQ can be
computed on R
1

1/(1
−
and satisﬁes:

X) is only deﬁned if X

∈
∼ N

≡
}

\ {

fQ(q) =

1

q )2/σ2

(1

e−

−
√2πσq2

.

≤
≤

o(1)), it can be interesting to consider the event
Ce−

Thus Q is clearly not exponentially concentrated (when q
therefore the expectation of Q is not even deﬁned). However, if σ is small enough (at
least σ
satisfying
P(
, 1
c
2 ], one
Q)
A
∈ E2. Following this setting, in the matricial cases, we also need
sees that (Q
AQ where the ﬁxed point Q is deﬁned;
to place ourselves in a concentration zone
suﬃciently small to retrieve an exponential concentration with Q
| AY but large
enough to be highly probable.

σ2/2. The mapping f : z
| AQ)

≤
z being 4-Lipschitz on (

1
2 }
−∞

AQ ≡ {

, fQ(q)

→ ∞

7→

X

∼

−

1

1

e−1/σ2
q2

The same resort to a concentration zone will take place for the resolvent matrix
1, for that purpose, we introduce in this section an event of

1
zn XX T )−

Qz = (Ip −

kThe notation
1(
k
E
E
(1.1) but with Ce−(t/ckAkF )2
+ Ce−t/ckAk,replacing the right-hand term

kF ) +

2(
k

) means that we have the same concentration inequality as in

A

A

k

6
September 8, 2021 0:38 WSPC/INSTRUCTION FILE

main

16

high probability,
Sε.
all z

∈

AQ, under which the eigen values of 1
X

Let us start with a bound on

, Assumption 0.4 leads us to:

k
k
√n sup
1
≤
then, we deduce from Proposition 1.2 applied to Assumption 0.2 that E[
E[X]

]
k
O(√n). Now, introducing a constant ε > 0 such that O(1)

+ O(√n)

O(√n)

E[xi]

E[X]

k ≤

k ≤

n k

i
≤

X

k

k

zn XX T are far from 1, for

≤
≤

k

≤
O(1) and ν > 0 deﬁned with:

k
ε

≤

√ν

E

≡

1
√n k

X

(cid:20)

≤

k

(cid:21)

O(1),

we note:

X

Since
k
k
such that P
Aν)
X(

/√n

∈
c
Q
A
(cid:0)

1
n

XX T

Aν ≡

(cid:26)

≤

ν + ε

.

(cid:27)

(2.1)

ν

(cid:13)
(cid:13)
cn. The mapping M

± E2(1/√n), we know that there exist two constants C, c > 0
n M M T is O(1/√n) Lipschitz on
Ce−
≤

→

(cid:13)
(cid:13)

1

(cid:1)

⊂ Mp,n and therefore, thanks to Lemma 2:
1
1
√n
n

| Aν ∝ E2

XX T

(cid:19)
Rp, the mapping that associates to any matrix the sequence
Let us note σ :
of its eigen values in decreasing order. It has been shown in [] that σ is 1-Lipschitz
(from (
) and therefore if we note λ1, . . . , λp the eigen values
of 1

Mp →
k · kF ) to (Rp,
k · k
n XX T such that λ1 ≥ · · · ≥

λn, we have the concentration:

Mp,

(cid:18)

(λ1, . . . , λp)

Aν [λ1], . . . , E
Considering a constant ε > 0, independent with n that will be ﬁxed for the
O(1)). We show in the next lemma that the

Aν [λp])

| Aν ∈

± E2

(E

(cid:19)

(cid:18)

ε

.

1
√n

whole paper (we note O(1)
event

≤

≤

i
AQ ≡ Aν ∩ {∀

[min(p, n)] : λi ∈

∈

Sε
−

0}

has an overwhelming probability.

Lemma 3. There exist C, c > 0 such that

n

∀

Proof. Starting from the identity Sε
−
that Sε
−
and j1 ≤ · · · ≤

jd in [min(p, n)] such that:

∪i
∈
0 is a union of, say, d intervals of R. There exists 2d indexes i1 ≤ · · · ≤

Aν [λi] + ε], we see
id

0 =

−

ε, E

Ce−

cn.

≤

N P(

∈
[p] [E

c
Q)
A
Aν [λi]

Since E

Aν [λi1 ]

≥

0 and E

Sε
−

0 =

[E

Aν [λik ]

−

ε, E

Aν [λjk ] + ε] .

[k
[d]
∈
Aν [λjd ] = ν
E
Aν [λjd ] + 2ε
2εd

≤

≤

O(1),

≤

O(1), we can bound:

September 8, 2021 0:38 WSPC/INSTRUCTION FILE

main

17

O(1). We can then bound
That implies in particular that d
thanks to the concentration of the 2d random variables λi1 , . . . , λid and λj1 , . . . , λjd :

O(1) because ε

≥

≤

P(

c
A

Q) = P (
∃

k

d

[d],

|

∈

P (
|

≤

Xk=1

λik −

λik −
E
Aν [λik ]

E
Aν [λik ]

> ε or

|

|

> ε) + P (
|

|

λjk −

> ε)

|

E
Aν [λjk ]

λjk −
E
Aν [λjk ]

> ε)

|

≤

2dCe−

cnε2
4

,

where C and c are the two constants appearing in the concentration inequality of
(λ1, . . . , λp) = σ( 1

n XX T ).

At that point of the paper we know that the non zero eigen values of 1

n XX T

are most likely lying in the union of compact intervals Sε
−

0 ⊂

[0, ν].

3. Concentration of the resolvent Qz = (In − 1
∈ Mp,n(C), we note

= √A ¯AT (A ¯AT is a nonnegative Her-
Given a matrix A
A
|
|
mitian matrix). With a simple diagonalization precedure, one can show for any
Hermitian matrix A the simple characterization of the spectrum of

nz XX T )−1.

:

A
|

|

Sp(
|

A
|

) =

, λ

λ
|

{|

∈

Sp(A)
}
Qz

(3.1)

O(1) when z is close to
It is possible to go further than the mere bound
Qz
0 and p
+ 1)). When p
n
/(
|
no such bound is true and it is then convenient to rather look at the coresolvent
ˇQz

1 that satisﬁes in that regime

n, we are then able to show that

≡
To formalize this approach, we introduce two quantities that will appear in our

1
nz X T X)−

| ≤
z
O(
|
|

(Ip −

|
| ≤

+ 1)).

O(
|

/(
|

ˇQz

| ≤

≤

≥

z

z

z

|

|

|

|

|

convergence speeds:

κz ≡ 


z
|
1 +

|
|

z

|
1 if n

if p

n

≤

p

≤

1 if p

n

≤

ˇκz ≡ 


z
|
1 +

if n

p

≤

|
|

z

|



note that both of them are bounded by 1 but they can tend to zero with
pending on the sign of p
quantities of the paper, is varying with n (we do not assume that O(1)
like ε. It is not a "constant").


de-
n. Note than in our formalism, the parameter z, as most
O(1)

≤ |

| ≤

−

z

z

|

|

Lemma 4. Under

AQ, Qz and ˇQz can be deﬁned on 0 and for any z
Qz

O (κz) Ip

and

ˇQz

∈
\
O (ˇκz) Ip

C

O (ˇκz) Ip ≤ |

| ≤

| ≤

Sε
−

0:

O (κz) Ip ≤ |

Proof. We can diagonalize the nonnegative symmeric matrix: 1
with D = Diag(λ1,
such that λq+1 =
q < min(p, n)). Then, if we note P0 ∈ Mp,d the matrix composed of the p

∈ Op, an orthogonal matrix. There exists q
q, λi 6

n XX T = P T DP ,
[p]
= 0 (possibly q = p or
q

, λp) and P
= λp = 0, and for all i

· · ·
· · ·

−

≤

∈

September 8, 2021 0:38 WSPC/INSTRUCTION FILE

main

18

last columns of P , P+ the matrix composed of the rest of the q columns, and
D+ = Diag(λ1, . . . , λq), we can decompose:

Qz = P T

(cid:18)

+ = 0 and P+P T

D

P = P T

zIp
zIp −
0 = 0, we have:

(cid:19)

0 P0 + P T
+

2 = P T

Qz

|

|

P+.

D+ (cid:19)

zId
zId −
(cid:18)
0 P0 + P T
+

z
|
|
zId−

2Id
D+|

|

2

(cid:16)

(cid:17)

P+. We

z

≤

z

|

|
+ ν + ε ≤

P T
+

(cid:18)

| (cid:19)
Therefore, in all cases (p
we can precise those bounds:

≤

|

|

n or p

(cid:18)

≥

Id
z
P+ ≤
|
|
D+| (cid:19)
zId −
|
Qz
n) O(1)

z
|
|
d(z, S

O

|
|
O(1). However, when p

0) ≤

(cid:18)

−

z

z
|
1 +

≤ |

| ≤

| (cid:19)

n,

≤

Since P0P T
can bound:

O

z
|
1 +

|
|

z

z
|1+
|
z
|

|

(cid:16)

≤

(cid:17)

z

|

| ≥

O(1), then P0 is empty, P+ = P and we see that O

•

•

if E

AQ [λmin(p,n)]
O

z
|1+
|
z
|

Qz
|
| ≤
if E
AQ [λmin(p,n)]
(cid:16)
(cid:17)
O(1) and O(κz) = O( |

≤

|

≥
.

o(1), the bound d(z, S

0)

ε

O(1) implies that

−

≥
O(κz), therefore:

≥

O(1)

z
|1+
z
|

)

|

≤
≤
O (min (1, κz)) Ip ≤ |

O (κz)

≤

Qz

O (1 + κz) Ip ≤

| ≤

O (κz) .

The inequalities on ˇQz are proven the same way.

Proof. Noting Φ :

Proposition 3.1. Given z
E2(κz/√n) in (

Mp,

∈

C
\
k · kF ) and ˇQz
Mp,n → Mp(C) and ˇΦ :

0, we have the concentrations Qz

Sε
−
| AQ ∝ E2(ˇκz/√n) in (

Mn,

k · kF ).
Mp,n → Mn(C) deﬁned as:
M T M
zn

ˇΦ(M ) =

In −
(cid:18)

(cid:19)

Φ(M ) =

M M T
zn

1

−

(cid:19)

Ip −
(cid:18)

and

1

−

,

| AQ ∝

it is suﬃcient to show that Φ (resp. ˇΦ) is O(κz/√n)-Lipschitz (resp. O(ˇκz/√n)-
∈ Mp,n, we can
Lipschitz) on
bound

M
(ν + ε)√n

AQ
n,p and any H

∈ M

AQ
n,p

X(

M

≡

AQ)l. For any M
O(√n) and:
≤

k ≤

k

dΦ M ·

k

H

kF =

Φ (M )

1
nz

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(M H T + HM T )Φ (M )
(cid:13)
(cid:13)
(cid:13)
(cid:13)

F

We can now distingish two diﬀerent cases:

if p

≤

•

n then:

dΦ M ·

k

H

kF ≤

O

(cid:18)

H
k
(1 +

z
|
)2√n

kF |
z
|
|

H

k

O

(cid:18)

≤

(cid:19)

z
kF |
1 +

/√n
|
z
|

|

.

(cid:19)

l

AQ
n,p ⊂ {

M

M

∈ M

n,p, 1

M M T

n k

1

ε

}

−

k ≤

September 8, 2021 0:38 WSPC/INSTRUCTION FILE

main

•

if p
O( |

≥
z
|1+
z
|

n, we know that
). We employ then the classical identity:

O(κz)

Φ(M )

k ≤

≤

k

O(1) and

ˇΦ(M )

k ≤

k

O(ˇκz)

19

≤

|

Φ(M )M = M T ˇΦ(M )

(3.2)

to be able to bound:

dΦ M ·

k

H

kF ≤

H

2(ν + ε)
k
√n
z
|
H
kF
√n

|
k

O

≤

kF

ˇΦ(M )

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Φ (M )
k
k

O

(cid:18)

k
(1 +

H
kF
)√n
z
|

|

(cid:19)

(cid:18)
AQ), Qz is a O(κz/√n)-Lipschitz transformation of
Thus, in all cases, under X(
∝ E2(1) and as such, it satisﬁes the concentration inequality of the proposition
X
thanks to Lemma 2. The same holds for ˇQz.

(cid:19)

4. A ﬁrst deterministic equivalent

One is often merely working with linear functionals of Qz, and since Proposition 3.1
n, one naturally wants to estimate the
implies that Qz
e−
AQ [Q]. In [LC19] is provided a deterministic equivalent ˜Qz
∈ Mp(C)
expectation E
R−, we are going to show below a
Qz
satisfying
−
stronger result,

| AQ ∈
˜Qz

O(1/√n) for any z

E
AQ Qz

± E2 |

k ≤

∈

k

•
•
•

with a Frobenius norm replacing the spectral norm,
Sε
for any complex z
−
for random vectors x1, . . . , xn having possibly diﬀerent distributions (it was
assumed in [LC19] that there was a ﬁnite number of classes)

0,

C

∈

\

An eﬃcient approach, developed in particular in [Sil86], is to look for a deter-
Rn

ministic equivalent of Qz depending on a deterministic diagonal matrix ∆
and having the form:

∈

˜Q∆ =

Ip −
(cid:0)

(cid:1)

Σ∆

1

−

where

Σ∆

1
n

≡

Σi
∆i

=

1
n

E[X∆−

1X T ].

One can then express the diﬀerence with the expectation of Qz under
followingly:

AQ, E

AQ [Qz]

n

i=1
X

E
AQ [Qz]

−

˜Q∆ = E

XX T

˜Q∆

1
n

Qz

(cid:20)

(cid:18)

AQ
n

=

1
n

E

AQ

Qz

(cid:20)

(cid:18)

i=1
X

Σ∆

−
xixT
i
z −

(cid:19)
Σi
∆i (cid:19)

(cid:21)
˜Q∆

.

(cid:21)

To pursue the estimation of the expectation, one needs to control the dependence
between Qz and xi. For that purpose, one uses classically the Schur identities:

Qz = Qz
−

i +

1
n

1

Qz
ixixT
−
1
zn xT
−

i Qz
i
−
i Qz
ixi
−

and

Qzxi =

Qz
ixi
−
1
i Qz
zn xT
−

,

ixi

1

−

(4.1)

September 8, 2021 0:38 WSPC/INSTRUCTION FILE

main

20

−

i)−

iX T
−

i = (In −

1 (recall that X

1
for Qz
zn X
−
Mp,n). Then introducing the notation Λz
express thanks to the independence between Qz
−
xixT
i
Λz
i −

E
AQ [Qz]

˜Q∆ =

Qz
−

1
n

AQ

≡

−

E

n

i

i = (x1, . . . , xi
−
Diag1

n(z

i
≤
≤
i and xi:

−

Σi
∆i (cid:19)

˜Q∆

(cid:21)

1, 0, xi+1, . . . , xn)
n xT

∈
ixi), one can

−
1

i Qz
−

i=1
X

+

1
zn2

(cid:18)
1
∆i

E

AQ

(cid:20)
n

i=1
X

= ε1 + δ1 + δ2 + ε2

QzxixT
h

i Qz
−

iΣi ˜Q∆

i

(4.2)

ε1 =

1
n

E

ε2 =

1
zn2

QzX

(cid:20)

(cid:18)

AQ
n

1
∆i

E

AQ

n

i=1
X
E

i=1
X
n

AQ

Qz
−

i

(cid:20)

(cid:18)

E

AQ

Qz
−

i

δ1 =

1
n

δ2 =

1
n

∆i −

Λz
i
z∆i (cid:19)
QzxixT
h

i Qz
−

X T ˜Q∆

(cid:21)
iΣi ˜Q∆

i −

xixT

i
E
AQ [xixT
i ]
∆i
E
AQ [xixT
i ]
∆i

Σi

−

˜Q∆

(cid:19)

(cid:21)

˜Q∆

,

with :






i=1
X
E
AQ[Λz]
From this decomposition, one is enticed into choosing, in a ﬁrst step ∆
∈
Dn(C) so that ε1 would be small. We will indeed take for ∆, the deterministic
diagonal matrix:

≈

(cid:19)

(cid:21)

(cid:18)

(cid:20)

Lemma 5. Given z

ˆΛz

C

z

≡
Sε
\
−
(Λz

∈

1
n
−
0, (Λz
| AQ)

Tr(ΣiE

∈ Dn(C).
AQ [Qz])
∝ E2(κz/√n) in (
± E2(κz/√n).

| AQ)
ˆΛz

∈

Dn(C),

k · kF ) and:

To prove this lemma, we need two preliminary results which aim is to show that
i i close to Qz.

Qz
−

Lemma 6 (Independence under AQ). Given two mappings f : Rp
→ Mp and
O(κg),
AQ,
g :
i)
k ≤
we can approximate:

Mp,n → Mp such that under

O(κf ) and

kF ≤

f (xi)

g(X

k

k

−

E
AQ[f (xi)g(X

i)]

−
O(1).

−

AQ [f (xi)]E
E

AQ [g(X

−

i)]

O

κf κge−

cn

,

(cid:0)

(cid:1)

F ≤

(cid:13)
(cid:13)

for some constant c

(cid:13)
(cid:13)

≥

Proof. Let us continue f xi(
Rp and M
deﬁning for any x

AQ) and g X−i(
∈ Mp,n:

∈

AQ) respectively on Rp and on

Mp,n

˜f (x) =

f (x) if x

xi(
∈
otherwise

(

0

AQ)

and

˜g(M ) =

(

g(M ) if M

∈
0 otherwise

X

AQ)
i(

−

September 8, 2021 0:38 WSPC/INSTRUCTION FILE

main

We can then estimate thanks to the independence between X

i and xi

m:

−

21

E
AQ [f (X

−

i)g(xi)] =

E

1X

i)

−

(cid:2)
˜f (xi)˜g(X

P(

∈AQf (xi)g(X
AQ)
i)
i
E [˜g(X

+

−

h

P(
P(

(cid:3)
c
Q)
A
AQ)
i)] + O

−

= E

= E

= E

˜f (xi)
h
i
AQ [f (xi)] E

−

AQ [g(X
E
AQ [Q

[n],

k

−

i)
i

E

˜f (xi)˜g(X
h
k·kF (κf κge−
i)] + O

cn)

k·kF (κf κge−

cn)

O(κz/n)

Q

i]

−

−

kF ≤
AQ, X
∈ Mp,n such that

−

i and xi are
1,
kF ≤

A

k

Lemma 7. Given z

C

Sε
−

\

∈

0, for all i

∈

Proof. As we saw with Lemma 6, we can consider that under
almost independent. Given a deterministic matrix A
that allows us to bound:

xT
i Q

ixi

−

iAQ
Λi

−

Tr

AE

AQ [Q

Q

i]

−

−

(cid:0)

(cid:12)
(cid:12)

(cid:1)(cid:12)
(cid:12)

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)
≤ (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E

AQ

E

AQ

O

≤

thanks to Proposition 1.3 (
k

1

(cid:20)

n

"

ˆΛi| (cid:12)

(cid:12)
AQ

|
E

1
n

+

,

(cid:12)
κz
(cid:12)
(cid:12)
(cid:12)
n
(cid:16)
(cid:17)
E[xi]
kF ≤

(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)
iAQ

ixi −

−

E
AQ[xT

i Q

−

iAQ

−

ixi]

xT
i Q

−

1
Λi (cid:21)

(cid:20)

Tr

ΣiE

AQ

xT
i Q

iAQ

ixi

−

−

(cid:0)

(cid:2)

(cid:12)
(cid:12)
(cid:3)(cid:1)
(cid:12)
(cid:12)

O(1) and

Q

k

−

iAQ

O(κz)).

#(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
+ O
(cid:12)
(cid:16)

κz
n

(cid:17)

Sε
−

\

Proof. [Proof of Lemma 5] The concentration of Λz = z
C

0 is easy to treat since, under

xi/√n

AQ,

k

k ≤

and therefore it can be shown as in Proposition 3.1 that X
Lipschitz transformation of X.

→

i Qz
ixi for all z
∈
−
Qz
O(κz)
ik ≤
k
−
Λz is a O(κz/√n)-

−

ikF ≤
1
n xT
O(1) and

−

± E2(κz/√n), we just have to bound thanks to Lemmas 6
≤

O(1/n):

O(ne−

cn)

To show that Λz
∈
and 7 and the bound
k
√n

E
AQ[Λz]

F ≤

−

ˆΛz
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

≤

ˆΛz
Σi −
ˆΛz
(cid:13)
(cid:13)
sup
(cid:13)
[n]
i
∈

1
√n

+

(cid:12)
(cid:12)

−

k ≤

E
AQ [xixT
i ]
E
AQ [Λz]
(cid:13)
(cid:13)
E
AQ [xixT
(Σi −
(cid:13)
(cid:0)
E

Tr

(cid:12)
(cid:12)
Tr

xixT
i

E
AQ [Q

AQ

Let us then try and bound Λz and ˆΛz.

(cid:0)

(cid:2)

(cid:3)

i ])E

AQ [Q]

+

Tr

ΣiE

AQ [Q

Q

i]

−

−

(cid:1)(cid:12)
(cid:12)
(cid:12)
(cid:12)
E
AQ [xixT

(cid:0)
i Q

−

i]

−

−

O

≤

(cid:18)

i]

(cid:1)(cid:12)
(cid:12)

κz
√n

(cid:1)(cid:12)
(cid:12)

(cid:19)

.

mfor xp, yp
O(ap)

∈ M

p and (ap)p∈N

∈

RN

+ the notation xp = yp +Ok·kF (ap) signiﬁes that

xp
k

−

yp

kF ≤

September 8, 2021 0:38 WSPC/INSTRUCTION FILE

main

22

Lemma 8. Given z

C

∈
) In ≤

\

O

0:

Sε
−
z
|
|
ˇκz (cid:19)

(cid:18)

Λz, ˆΛz

≤

O

≤

z
|
|
ˇκz (cid:19)

(cid:18)

≤

O (1 +

) In

z

|

|

z

O (
|

|

Proof. Starting with (3.2) and the identity:

we deduce from the Schur identities (4.1) that:

1
zn

ˇQzXX T = ˇQz

In,

−

(4.3)

z
Λz = Diagi

1
1
i Qz
zn xT
−
Now we know from the characterization (3.1) and Lemma 4 that:

= In +

ixi !

1
zn

−

[n]

1

∈

Diag(X T QzX) = Diag( ˇQz)

|

=

≤

≤ |

O(ˇκz)

[n](eT
i

ˇQzei)

inf
[n]
i
∈

Sp( ˇQz)

Diagi
∈

Diag( ˇQz)
|
where e1, . . . , en are the n vectors of the canonical basis of Rn (ei ∈
except in the ith entry where there is a 1).
Λz
AQ on those
. Taking the expectation under
|
AQ [Λz], and we can then deduce the control on

That gives us the bounds on
|
bounds we obtain the control on E
ˆΛz thanks to Lemma 5:

Rn is full of 0

sup
[n]
i
∈

Sp( ˇQz)

O(ˇκz)

| ≤

≤

E
AQ [Λz]

−

O

≤

κz
n

o

≤

z
|
|
ˇκz (cid:19)

.

(cid:18)

ˆΛz
(cid:13)
(cid:13)
(cid:13)

(cid:16)
We can now prove the main result of this subsections that allows us to set that

(cid:17)

(cid:13)
(cid:13)
(cid:13)

˜Q ˆΛz

is a deterministic equivalent of Qz (thanks to Lemma 1).

Proposition 4.1. Given z

C

Sε
−

0:

\

∈

O(κz)

and

E
AQ [Qz]

≤

ˆΛz

˜Q

−

O

F ≤

κz
√n

.

(cid:18)

(cid:19)

(cid:13)
(cid:13)
To prove this proposition, we will bound the diﬀerent elements of the decompo-
(cid:13)
sition (4.2). And to bound ε1, we will need:

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

ˆΛz

˜Q
(cid:13)
(cid:13)
(cid:13)

Lemma 9. Given z

C

Sε
−

\

∈

0, under

1

z√n QzX

O(1) and:

k ≤

AQ,
k
X T ˇQz

1
z√n

QzX =

1
z√n

| AQ ∝ E2 (1)

n, it is more convenient to work with the expression 1

Proof. We follow the steps of the proof of Proposition 3.1. Depending on the sign
of p
n)
−
or with 1
look at the variations of the mapping Ψ :

n). We just treat here the case p

z√n X T ˇQz (when p

z√n QzX (when p

n and therefore

≥

≤

≤

Mp,n → Mp,n(C) deﬁned as:

Ψ(M ) =

1
z√n

Ip −
(cid:18)

M M T
zn

−

(cid:19)

1

M.

 
September 8, 2021 0:38 WSPC/INSTRUCTION FILE

main

23

to show the concentration of

1

z√n QzX = Ψ(X). For all H, M

(and with the notation Φ(M ) =
tion 3.1):

(cid:16)

MM T
zn

Ip −

1

−

(cid:17)

AQ)
∈ M
given in the proof of Proposi-

X(

≡

AQ
n,p

dΨ M ·

k

Thus, under

H

k ≤

(cid:13)
(cid:13)
(cid:13)
O
(cid:13)

Ψ (M )

(M H T + HM T )Ψ (M )

1
nz
H
k
k
√n(1 +
|
|
AQ Ψ is O(1/√n)-Lispchitz and therefore

k
(1 +

H
z

+ O

)2

≤

(cid:18)

(cid:18)

(cid:19)

z

|

k
)√n
|

M
z√n

+

Ψ (M )

H
z√n

O

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
≤

H
k
k
√n

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:18)
(cid:19)
(cid:19)
1
z√n QzX
∝ E2(1/√n).

.

Proof. [Proof of Proposition 4.1] Let us note for simplicity κ ˜Q ≡ k
at decomposition (4.2) we can start with the bound:

˜Q ˆΛz

k

. Looking

ε1kF =

k

1
n

E

AQ

"

QzX

ˆΛz

i −
z ˆΛz

Λz
i
i !

applying Proposition 1.4 with the hypotheses:

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

O

≤

(cid:18)

κ ˜Q
√n

(cid:19)

X T ˜Q∆

#(cid:13)
F
(cid:13)
(cid:13)
(cid:13)
(cid:13)

X
1
z
|
|
Λz

| AQ ∝ E2 given by Assumption 0.2,
QzX
| AQ ∝ E2 given by Lemma 9,
ˆΛz
given by Lemma 5,
± E2

κz
√n

∈

•
•

•

(cid:17)

(cid:16)
and bounding thanks to Lemma 8 κz
∈ Mp(C) satisfying
any matrix A
Schwarz inequality:

ˆΛz ≤
A
k

kF ≤

) = O(

O( κz ˇκz
O(1). Second, for
z
≤
|
|
1, let us bound thanks to Cauchy-

1
1+

)

z

|

|

E
AQ [Tr(Aε2)]

=

(cid:12)
(cid:12)

(cid:12)
(cid:12)

1
z2n2

E

AQ

r

Tr

AQzX

h
n

(cid:16)

ˆΛz

n|−

|

E

AQ

Tr

˜Q ˆΛz Σi|

i=1
X
2
F

supi
∈

(cid:16)
h
[n] Tr (Σ2
n

· v
u
u
t

1
n2

A
k
n

2X T ¯Qz ¯AT

Qz
−

i|

2Σi

(cid:17)i
¯˜Q ˆΛz

(cid:17)i

i ) κ ˜Q

O



≤

κ ˜Q
√n

(cid:18)

(cid:19)



O

≤



s

k



thanks to the bounds provided by our assumptions, and Lemmas 4, 8 and 9.

Third, we bound easily with Lemma 6 the quantity:

E
AQ [Tr(Aδ1)]

(cid:13)
(cid:13)

1
n

≡

n

i=1
X

|

(cid:13)
(cid:13)

1
ˆΛz

Tr

ˆΛz

˜Q

AE

AQ

Qz
−

ixixT
i

i | (cid:13)
(cid:13)
(cid:13)

(cid:16)

Tr

−

ˆΛz

˜Q
(cid:16)

AE

(cid:2)

AQ

Qz
−

i

(cid:3)(cid:17)
E

AQ

xixT
i

(cid:2)
since

(cid:3)
Σi −

k

(cid:3)(cid:17)(cid:13)
(cid:2)
(cid:13)
E
(cid:13)
AQ [xixT
i ]

k ≤

O

≤

.

(cid:19)

κ ˜Q
√n

(cid:18)
O( 1
n )

And we can bound

E
AQ [Tr(Aδ2)]

O

≤

(cid:13)
(cid:13)

(cid:13)
(cid:13)

κ ˜Q
n

(cid:16)

(cid:17)

 
September 8, 2021 0:38 WSPC/INSTRUCTION FILE

main

24

Taking the supremum on A
E
F and
AQ [ε2]
(cid:13)
(cid:13)

E
AQ [δ1]

F ,
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

∈ Mp,n(C) and putting the bounds on
E
F together, we obtain:
AQ [δ2]
(cid:13)
(cid:13)
ˆΛz
˜Q

O

κ ˜Q
√n

F ≤

(cid:13)
(cid:13)
E
AQ [Qz]

−

E
AQ[ε1]

F ,
(cid:13)
(cid:13)

(cid:13)
(cid:13)

So, in particular κ ˜Q ≡
E
AQ [Qz]
O(κz) as
k
k
of the Proposition.

(cid:18)

(cid:13)
(cid:13)
(cid:13)
E
AQ [Qz]

(cid:19)
(cid:13)
(cid:13)
(cid:13)
˜Q ˆΛz
, which implies that κ ˜Q ≤
since 1
(cid:13)
(cid:13)
√n = o(1). We obtain then directly the second bound
(cid:17)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

κ ˜Q
√n

+ O

(cid:13)
(cid:13)

(cid:13)
(cid:13)

≤

(cid:16)

5. Introduction of the semi-metric ds and proof of Theorem 1

Proposition 4.1 slightly simpliﬁed the problem because while we initially had to
estimate the expectation of the whole matrix Qz, now, we just need to approach the
diagonal matrix ˆΛz
E
AQ [xiQz
[n]. One is tempted to introduce
from the pseudo identity:

ixi])i
∈

Diag(z

−

≡

1
n

−

ˆΛz

i ≈

z

−

1
n

Tr(Σi ˆQz
−

i)

z

−

≈

1
n

Tr(Σi ˜Q

ˆΛz

)

(5.1)

a ﬁxed point equation whose solution would be a natural estimate for ˆΛz. This
equation is given in Theorem 1: we chose ˜Λz satisfying

˜Λz

i = z

1
n

−

Tr

Σi ˜Q
(cid:16)

˜Λz

.

(cid:17)

We are now going to prove that ˜Λz is well deﬁned for any z
that H

) to prove Theorem 1.

C, Im(z) > 0

z

H (where we recall

∈

≡ {

}
Introducing the mapping:

∈

L

∀

∈ Dn(H) :

I

z(L)

zIn −

≡

Diag

1
n

Tr

(cid:18)

Σi ˜QL)
(cid:16)

(cid:17)(cid:19)1
≤

,

n

i
≤

we want to show that I z admits a unique ﬁxed point. For that purpose, let us adapt
tools already introduced in [LC20] that rely on the introduction of a semi-metric
ds deﬁnedn for any D, D′

∈ Dn(H) as:

ds(D, D′) = sup
n
i
≤

≤

1

Di −
|
(Di)
ℑ

D′i|
(D′i)
ℑ

(it lacks the triangular inequality to be a true metric). This semi-metric is intro-
duced to set Banach-like ﬁxed point theorems.

p

nIn [LC20], ds is only deﬁned on
i instead of
then
properties of the semi-metric; the only objective with this new choice is to be able to set that
is contractive for this semi-metric.

n(R+) and the denominator appearing in the deﬁnition is
i). The present adaptation does not change the fundamental
z

DiD′

D
(D′

(Di)

pℑ

p

ℑ

I

September 8, 2021 0:38 WSPC/INSTRUCTION FILE

main

25

Dn(H)) (or more simply
λ
Deﬁnition 5.1. Given λ > 0, we denote
s (
Dn(H)
there is no ambiguity), the class of functions f :
∈ Dn(H):
the semi-metric ds; i.e. satisfying for all D, D′
λds(D, D′).

λ
s when
→ Dn(H), λ-Lipschitz for

ds(f (D), f (D′))

C

C

≤

when λ < 1, we say that f is contracting for the semi-metric ds.

This class presents an important number of stability properties that we list
in the next proposition (the stability of the class through the sum is proven in
Appendix A, it was already done with a slightly diﬀerent setting in [LC20]):

θ
s ,

∈ C
1
−
f ∈ C

Proposition 5.1. Given three parameters α, λ, θ > 0 and two mappings f
and g

λ
s

∈ C

λ
s ,

αf

λ
s ,

∈ C

f

g

◦

∈ C

λθ
s ,

and

f + g

max(λ,θ)
s

.

∈ C

We can now present our ﬁxed point theorem that has been demonstrated once

again in [LC20]:

Dn(H) and a map-
Theorem 5 ([LC20] Theorem 3.13). Given a subset
ping f :
Df ), if
Df → Df with an imaginary part bounded from above and below (in
it is furthermore contracting for the stable semi-metric ds on Df , then there exists
a unique ﬁxed point ∆∗

Df of

∈ Df satisfying ∆∗ = f (∆∗).

We are going to employ this theorem on the mapping

z for z

I

∈

H. We ﬁrst

restrict our study on a subset of

∈ Dn(H)
}

DI z

≡ {
H, I z(

∈
H, and L

Dn(H):
∈ Dn(H), D/z
D
⊂ DI z .
DI z )
∈ DI z and i
n
i=1 ℑ

Σi ˜QL

∈
(z) + 1

n Tr

[n], note that:
¯˜QL

> 0

∈
(Lj )Σi
Lj|
n
i=1 ℑ

2

|

P

2 Tr

|

(cid:16)
Σi ˜QL
(cid:16)

(cid:16)

(z) +

P
ℑ

(cid:17)
(Lj /z)Σi
Lj/z

2

|

|

¯˜QL

> 0

(cid:17)

(cid:17)

Lemma 10. For any z

Proof. Considering z

(
I
(
I

• ℑ

• ℑ

z (L)i) =
z (L)i /z) = 1
z
n
|

ℑ

Let us now express the Lipschitz parameter of I z for the semi metric ds.

Proposition 5.2. For any z
ds on

∈

DI z and satisﬁes for any L, L′
ds(I z(L), I z(L′))
(1
H and D

≤
p
∈ DI z :

∈

where for any w

φ(w, L) =

sup1

≤

Proof.

H, the mapping I z is 1-Lipschitz for the semi-metric

∈ DI z :

φ(z, L))(1

−

−

φ(z, L′))ds(L, L′),

(w)
(
I

ℑ
n ℑ

i
≤

w(L))i ∈

(0, 1).

September 8, 2021 0:38 WSPC/INSTRUCTION FILE

main

26

Let us bound for any L, L′

∈ DI z :

I z(L)i −

I z(L′)i =

1
n

Tr

Σi ˜Qz(L)



1
n



n

Lj −
(Lj)
ℑ

ℑ

L′j
ℑ
(L′j) q

(L′j)

(Lj)

ℑ
LjL′j

Σj


˜Qz(L′)





j=1
X



q
Σi ˜Qz(L)



1
n

Tr

≤

ds(L, L′)

v
u
u
t

n

i=1
X

ℑ

(Lj)Σi
2
Lj|

|

!

¯˜Qz(L)

!

1
n

· v
u
u
t

Tr

Σi ˜Qz(L′)

n

ℑ

(L′j)Σi
2
L′j|

¯˜Qz(L′)
!

!

|
i=1
X
(I z(L′)i)

ds(L, L′)

≤

(
ℑ

(I z(L)i)

(z)) (

ℑ

− ℑ

(z)),

− ℑ

(5.2)

thanks to Cauchy-Schwarz inequality and the identity

p

1
n

0

≤

Tr

Σi ˜Qz(L)

n

i=1
X

ℑ

(L′j)Σi
2
L′j|

|

!

¯˜Qz(L)

!

(I z(L)i)

=

ℑ

(z)

− ℑ

issued from the proof of Lemma 10.
Dividing both sides of (5.2) by

Lipschitz parameter.

p

(I z(L)i)

ℑ

ℑ

(I z(L′)i), we retrieve the wanted

The contractivity of I z is not fully stated in the previous proposition because,
the Lipschitz parameter depends on the values of L, L′ and we want a bound uniform
on

DI z . This will be done thanks to next lemmas.

Lemma 11 (Commutation between inversion and modulus of matrices).
Given an invertible matrix M
M −

2)−

M

1.

1

Proof. introducing A, B

1 = A + iB, we have the identity:

1

M −

|

|

2 = A2 + B2 = M −

1 = ( ¯M M )−

1 = (
|

M

|

2)−

1.

|

∈ Mp,

2 = (
|
|
∈ Mp(R) such that M −
1 ¯M −

1 = M −

1M −

|

Lemma 12. Given L

∈ DI z , we can bound:
I z(L)
O
(z)In ≤ |

| ≤

ℑ

z

|

z
|
(z)

+ |
ℑ

(cid:19)

In

|
(cid:18)

and:

O

ℑ
1 +

(cid:18)

(z)

(z)

ℑ

(cid:19)

Ip ≤

QI z(L)

(cid:12)
(cid:12)
(cid:12)

Ip
z
|
(z)

≤

(cid:12)
(cid:12)
(cid:12)

.

|
ℑ
∈ DI z , then we know that

Proof. The lower bound of I z(L) is immediate. If L
L/z

∈ Dn(H), and therefore, noting that:
( ˜QL/z)−

=

n

1

zIp −

ℑ  

1
n

Σi
Li/z !

(z)Ip +

=

ℑ

1
n

n

ℑ

i=1
X

|

i=1
X

(Li/z)Σi
Li/z

2 ≥ ℑ

(z)Ip,

|

(cid:17)

ℑ

(cid:16)

 
 
 
 
 
 
September 8, 2021 0:38 WSPC/INSTRUCTION FILE

main

27

z
(z) which gives us directly the upper
|
|
ℑ
.

O

z
|
(z)

z

+ |
ℑ
ν, we can bound:

(cid:16)

(cid:17)

|

|

we can deduce from Lemma 11 that

bound on I z(L) since

Finally, since

1
n

k

|

1 + 1
z
n
|
n
(cid:16)
i=1 Σik ≤
1
n

In −

P

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

˜QL
k ≤
supi∈[n] Tr(Σi)
(z)

k

ℑ
1
n

E

n
(cid:2)

i=1
X

XX T

(cid:17)

≤

≤

(cid:3)

1 +

(cid:13)
(cid:13)
Σi
(cid:13)
(cid:13)
I z(L)i (cid:13)
≤
(cid:13)
(cid:13)
(cid:13)
since O
(cid:13)

˜QI z(L)
|

|

ν
(z)

,

ℑ

which provides the lower bound on

(cid:16)
We now have all the elements to prove Theorem 1:

ℑ
1+

(z)

(z)

ℑ

1
1+ ν

ℑ(z)

.

≤

(cid:17)

DI z ), the mapping I z is bounded
Proof. [Proof of Theorem 1] On the domain I z(
and contracting for the semimetrix ds thanks to Proposition 5.2 and Lemma 12. The
hypotheses of Theorem 5 are thus satisﬁed, and we know that there exists a unique
DI z ) such that I z(˜Λz) = ˜Λz. There can not exist a second
diagonal matrix ˜Λz
I z(
∈
∈ Dn(H) such that Λ′ = I z(Λ′) because then Proposition 5.2
diagonal matrix Λ′
Dn(H)) would imply that ds(Λ′, ˜Λz) < ds(Λ′, ˜Λz).
(true on the whole domain

We end this section with an interesting result on ˜Λz that is proven in Appendix B

since it will not be of any use for our main results.

Lemma 13. supi
∈

[n] |

Λz

i | ≤

O(1 +

).

z

|

|

6. Convergence of ˆΛz towards ˜Λz, proof of Theorem 4
To show the convergence of ˆΛz towards ˜Λz = I z(˜Λz), we need an important result
bounding the distance to a ﬁxed point of a contracting mapping for the semimetric
ds. This sets what we call the stability of the equation. First allowing us to bound
˜Λz. In the
ˆΛz
k
former application, the convergence parameter is n, while in the latter application
C in the neighbourhood of 0. For the sake of generality, we
it is a parameter t
N to track the convergence (we will then
thus introduce here a new notation s
consider s = n or sequences (ts)s
∈

, it will then be employed to show the continuity of z

˜Λz

7→

−

∈

k

N

∈

∈
CN depending on the applications).
Dns(H), (f s)m

N, each
Proposition 6.1. Let us consider a family of mappings of
f s being λ-Lipschitzo for the semi-metric ds with λ < 1 and admitting the ﬁxed
point ˜Γs = f s(˜Γs) and a family of diagonal matrices Γs. If one assumes that p

∈

oActually, f s does not need to be λ-Lipschitz on the whole set
to bound for all s

N:

ns (H), but we need to be able

D

∈

ds(f s(˜Γs), f s(Γs))

λds(˜Γs, Γs)

≤

pUsually the notations O(1) and o(1) are used for quasi-asymptotic studies when n tends to inﬁnity
but in this proposition, the relevent parameter is s, thus ds(f s(Γs), Γs)
os→∞(1) means that
S, ds(f s(Γs), Γs)
for all K > 0, there exists S

N such that for all s

K.

≤

∈

≥

≤

September 8, 2021 0:38 WSPC/INSTRUCTION FILE

main

28

(Γs),

ds(

ℑ

ℑ

(f s(Γs)))

os

→∞

≤

(1), then:





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
∈ Dn(H):
(Γ2))

ℑ

(Γ1),

.

ds(Γs, ˜Γs)

Os

f s(Γs)

Γs

−

(cid:13)
(cid:13)
(cid:13)
(cid:13)

This proposition is just a mere application of the following elementary result.
(cid:13)
(cid:13)

→∞ 

(˜Γs)

(Γs)

q

≤

ℑ

ℑ

Lemma 14. Given three diagonal matrices Γ1, Γ2, Γ3

Proof. Let us simply bound:

1 + ds(

ℑ

(cid:0)

−
(Γ2)

ℑ
(Γ1)

p
ℑ

Γ3
(Γ1) (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

ℑ

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

p

≤ (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

p

Γ3
(Γ2) (cid:13)
ℑ
(cid:13)
(cid:13)
(cid:13)
Γ3
(cid:13)
(Γ2) (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

ℑ

+

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

p

≤ (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
≤ (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

ℑ

Γ3
(Γ2) (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(Γ2)

ℑ

p

ℑ
p
Γ3
(Γ2) (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(Γ1)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
ℑ
(Γ1)

(Γ2)

ℑ

(cid:16)p

(Γ1)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
can then conclude with the bound
(cid:13)

p

p

ℑ

ℑ

p

we

Γ3
(Γ1) (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

ℑ

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

p

(cid:1)

(Γ1)

− ℑ
(Γ2) +

ℑ

ℑ

p

p
(Γ2) +

ℑ

p

ℑ

(cid:16)p

(Γ1)

ℑ

ℑ

(Γ2).

p
Proof. [Proof of Proposition 6.1] Let us simply bound thanks to Lemma 14:

,

≥

(Γ1)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:17)
(cid:13)
(Γ1)
(cid:13)
(cid:17)

ds(˜Γs, Γs)

˜Γs

f s(Γs)

−
(˜Γs)

(Γs)

ℑ

ℑ

q

˜Γs, f s(Γs)
(cid:17)
(cid:16)

≤ (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
ds

≤

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
q
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(1 + ds(

f s(Γs)

Γs

−

(˜Γs)

(Γs)

ℑ

ℑ

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(Γs),

ℑ

ℑ

(f s(Γs))) +

≤

≤

λds

˜Γs, Γs

(cid:16)

(cid:17)

(1 + ds(

ℑ

(Γs),

ℑ

(f s(Γs))) +

1

λ

√
(cid:13)
ℑ
(cid:13)
λds(
(cid:13)
−
(cid:13)
(f s(Γs)))

−

f s(Γs)
(˜Γs)

−

Γs
(Γs)

ℑ
(Γs),

ℑ

o(1)

≤

≤

O

(cid:13)
(cid:13)
(f s(Γs)) ≤
(cid:13)
ℑ
(cid:13)
λ
1
2λ for s suﬃciently big.
−

q





ℑ

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

since ds(

ℑ

(Γs),

ℑ

f s(Γs)

Γs

−

(˜Γs)

(Γs)

(cid:13)
(cid:13)
(cid:13)
ℑ
(cid:13)
q
(cid:13)
f s(Γs)
(cid:13)

ℑ
Γs

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(˜Γs)

(Γs)

−

ℑ

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

ℑ

(cid:13)
(cid:13)
(cid:13)
(cid:13)
q
(cid:13)
(cid:13)
f s(Γs)

Γs

−

(˜Γs)

(Γs)

ℑ





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

To be employ Proposition 6.1 on the matrices ˜Γn = ˜Λz and Γn = ˆΛz and on the

mapping f n = I z, we ﬁrst need to set:

Proposition 6.2. Given z

∈

ds

ℑ

(cid:16)

C such that d(z, Sε)
(I z(ˆΛz)),

≥
O

ℑ

(ˆΛz)
(cid:17)

≤

O(1):
κz
n

(cid:16)

(cid:17)

September 8, 2021 0:38 WSPC/INSTRUCTION FILE

main

Proof. We can bound thanks to Lemmas 7 and 15:

29

(cid:17)

,

ds

ℑ

(cid:16)

(I z(ˆΛz)),

ℑ

(ˆΛz)
(cid:17)

= sup
1

i
≤

≤

1
n Tr

Σiℑ
(cid:16)

˜Q ˆΛz
(cid:16)
(ˆΛz
i )

ℑ

ℑ
q
( ˜Q ˆΛz
)
(z) −

ℑ

ℑ

EAQ [Qz]

−
(I z(ˆΛz)i)
EAQ [Qz]
(z)

(cid:0)

(cid:1)

+ O

κz
n

(cid:16)

+ O

κz
n

(cid:17)(cid:17)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:13)
F !
(cid:13)
(cid:13)
(cid:13)
(cid:13)

O

≤

n (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1
(cid:12)
√n (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

Qz

since

ℑ
ℑ
O(√n). The identity 1
zn QzXX T = Q
In allows us to write
ΣikF ≤
k
(EAQ [Qz ])
¯Qz]). We already know how to
EAQ[
EAQ [
1
1
2
Qz
=
ℑ
¯z
z
(z)
−
ℑ
AQ[Qz] thanks to Proposition 4.1, we are thus left to estimate EAQ[
estimate E
2].
We do not give the full justiﬁcations that are closely similar to those presented in
the proof of Proposition 4.1 – mainly application of Proposition 1.4 and Lemma 5.
To complete this estimation, we consider a deterministic matrix A
∈ Mp, and we
try and estimate:

Qz](=

Qz

−

−

−

−

(cid:17)

(cid:16)

|

|

|

|

|

|

Tr

AEAQ

¯Qz

Qz

ˆΛz

˜Q

)

−

(cid:16)
=

=

1
n

1
n

n

h

(cid:16)

Tr

i=1
X
n

AEAQ

Tr

AEAQ

n

i=1
X
1
n2

+

"

"

(cid:17)i(cid:17)

¯QzQzΣi ˜Q ˆΛz
ˆΛz
i
iΣi ˜Q ˆΛz
¯QzQz
−
ˆΛz
i
¯QzxixT
i |

Tr

AEAQ

i=1
X
1
n Tr

"
A ¯QzX∆zX T ˜Q ˆΛz
(cid:16)

z ˆΛz
i

EAQ

=

h

i=1
X
n
1
n

i=1
X
˜Q ˆΛz
2xixT
i

#! −
Qz
i|
−
z ˆΛz
i

+ O

(cid:17)i

κz
√n

(cid:18)
2xi)

(cid:19)

1
n

#! −

n

Tr

AEAQ

¯QzQz
−

ixixT
i
Λz
i

"

˜Q ˆΛz

Tr

AEAQ

¯Qz
−

iQz
ixixT
i
−
ˆΛz
i

"

#!
˜Q ˆΛz

#!

+ O

#!

κz
√n

(cid:18)

(cid:19)

ˆ∆z
with the notation ∆z
± E2(κz/√n), with the clas-
Diagi
≡
∈
sical notation ˆ∆z = E
AQ [∆z]. Returning to our estimation, we obtain thanks to
Proposition 1.4:

n xT
i |

[n]( 1

Qz
−

i|

∈

Tr

AEAQ
n

¯Qz
h

(cid:16)
=

1
n

=

1
n

(cid:16)
ˆ∆z
i
Tr
ˆΛz
i
ˆ∆z
i
ˆΛz
i

i=1
X
n

i=1
X

Qz

ˆΛz

˜Q

)

−

AEAQ

˜Q ˆΛz

(cid:17)i(cid:17)
¯Qz
−

"

ixixT
i
¯Λz
i

+ O

#!

2 Tr

AEAQ
(cid:16)

¯Qz
−
h

ˆΛz

iΣi ˜Q

+ O

i(cid:17)

(cid:18)

κz
√n

(cid:18)
κz
√n

(cid:19)

Now, taking the expectation on the identity:

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(Λz) =

ℑ

(z)

ℑ

−

1
n ℑ

(xT

i Qzxi) =

ℑ

(z)

1 +

(cid:18)

1
nz

Qz

xT
i |

2xi −

|

i Qzxi
xT

(cid:0)

(cid:19)

(cid:19)
(cid:1)

= ℑ

(z)
z

(∆z + Λz) ,

 
 
 
 
 
 
 
September 8, 2021 0:38 WSPC/INSTRUCTION FILE

main

30

we deduce that ˆ∆z = z
ℑ
˜Q ˆΛz

(z) ℑ
k·kF (κz/√n), we can estimate:

2 + O

(ˆΛz)

−

|

|

ˆΛz. Therefore, with the identity E

AQ [ ¯Qz ˜Q ˆΛz

] =

Tr

AEAQ

(cid:0)

Qz

2

|

|

(cid:2)

(cid:3)(cid:1)

In conclusion:

=

1
n

=

ℑ

n






i=1
X
z
(z) ℑ

z
(z)

(ˆΛz
i )
2 −

ℑ
ˆΛz
i

1
¯ˆΛz

i

ℑ

(cid:12)
ˆΛz
(cid:12)
(cid:12)

(cid:12)
(cid:12)
A ˜Q
(cid:12)
(cid:16)

(cid:17)(cid:17)

+

1
n

Tr

(cid:16)

Tr





Tr

ˆΛz

A ˜Q
(cid:16)

(cid:17)

+ O

κz
√n

(cid:18)

(cid:19)

AEAQ
(cid:16)

¯Qz
−
h

ˆΛz

iΣi ˜Q

+ Tr

i(cid:17)

(cid:16)

˜Q

A
|

ˆΛz

2

|

(cid:17)

+ O

κz
√n

(cid:18)

(cid:19)

(E

ℑ

AQ [Qz])
(z)
ℑ

− ℑ

( ˜Q ˆΛz

)

O

≤

(z)κz
ℑ
√n
z
|
|

(cid:18)

≤

(cid:19)

O

κz
√n

,

(cid:19)

(cid:18)

and we retrieve the result of the proposition.

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

F

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

And to show that the mapping I z is contracting, we will need:

Lemma 15. Given z

C

Sε
−

0:

\

∈

(z)

ℑ

≤

inf
i
∈

[n] ℑ

ˆΛz
i

(cid:16)

(cid:17)

≤

sup
i
∈

[n] ℑ

ˆΛz
i
(cid:16)

(cid:17)

O (

ℑ

≤

(z))

Proof. The lower bound is obvious since for all i

[n]:

∈

(ˆΛz

i ) =

ℑ

(z)
(z) + ℑ
2n2
z
|

|

ℑ

E
AQ [xT

i Qz
−

iX

iX T
−

i

¯Qz
−

−

ixi]

(z).

≥ ℑ

The upper bound is obtained thanks
Qz
−

O(1) provided by Lemma 9.

√n

iX

i/

z

−

≤

|

|

to the bound, valid under

AQ,

We now have all the elements to show:

Proposition 6.3. For any z

H such that d(z, S

∈

0)

−

≥

O(1):

ˆΛz

k

−

˜Λz

k ≤

O

κz
n

(cid:16)

(cid:17)

and

O

z
|
|
ˆκz (cid:19)

(cid:18)

˜Λz

O

| ≤

≤ |

z
|
|
ˆκz (cid:19)

(cid:18)

Proof.

We already know from Proposition 6.2 that ds
sides, the Lipschitz parameter λ of I z on the set
(cid:16)
1

O(1). Recall indeed from Proposition 5.2 that:

ℑ

λ

{

(I z(ˆΛz)),
˜Λz, ˆΛz, n

o(1). Be-

(ˆΛz)
≤
N
(cid:17)
is such that

ℑ

∈

}

−

≥

1

−

λ

≤ v
u
u
t

(z)

ℑ
[n] ℑ

supi
∈

(˜Λz

i ) !  

1

−

(z)

ℑ
[n] ℑ

supi
∈

(ˆΛz

i ) ! ≤

O(1)

1

−

1

−

≤

O(1),

p

 
September 8, 2021 0:38 WSPC/INSTRUCTION FILE

main

thanks to Lemma 15. Therefore, we can employ Proposition 6.1 to set that:

31





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(˜Λz
i )
(˜Λz
i )

= ds(ˆΛz, ˜Λz)

O

≤

ˆΛz

p

˜Λz
−
ˆΛz ˜Λz (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

which implies thanks to Lemma 15 that:

ˆΛz

I z(ˆΛz)

−
(ˆΛz)

(˜Λz)

ℑ

ℑ

q

,





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

˜Λz

supi
[n] ℑ
∈
inf i
[n] ℑ
∈
We reach here the only point of the whole paper where we will employ Assump-
tion 0.5. It is to set that:

I z(ˆΛz)
(cid:13)
(cid:13)
(cid:13)

ˆΛz
(cid:13)
(cid:13)
(cid:13)

ˆΛz
(cid:13)
(cid:13)
(cid:13)

v
u
u
t

(cid:13)
(cid:13)
(cid:13)









−

−

≤

O

.

(˜Λz

i ) =

inf
i
∈

[n] ℑ

(z) + inf
[n]

i
∈

ℑ

(z) +

≥ ℑ

n

j=1
X

As a conclusion:

n

ℑ
n
j=1
|
X
(˜Λj)
˜Λj|
2

ℑ
n
|

(˜Λj)
˜Λj|
2

Tr

˜Λz

Σi ˜Q

Σj

˜Λz

¯˜Q

(cid:16)

O

Tr

˜Λz

˜Q

Σj

˜Λz

¯˜Q

(cid:16)

(cid:16)

(cid:17)(cid:17)

(cid:17)

O

≥

sup
i
∈

[n] ℑ

(˜Λz
i )

!

ˆΛz
(cid:13)
(cid:13)
(cid:13)

˜Λz

−

O

≤

ˆΛz

−

(cid:13)
(cid:13)
(cid:13)

(cid:16)(cid:13)
(cid:13)
(cid:13)

1
√n

O

≤

(cid:18)

(cid:17)

I z(ˆΛz)
(cid:13)
(cid:13)
(cid:13)

We can then deduce a result on the resolvent:

Corollary 1. For any z

C such that d(z, Sε)

∈

ˆΛz

˜Q

−

ˆQ
(cid:13)
(cid:13)
(cid:13)

(cid:19)

(cid:13)
(cid:13)
(cid:13)

O

≤

κz
n

.

(cid:17)

(cid:16)

O(1),

˜Q ˜Λz

k

k ≤

O(κz) and:

E
AQ [Qz]

˜Λz

˜Q

−

F ≤

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

≥

O

κz
√n

(cid:18)

(cid:19)

E
AQ [Qz]

˜Q ˆΛz

−

F ≤

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
˜Q

Proof. We already know from Proposition 4.1 that

O(κz/√n), thus we are left to bound:

ˆΛz

˜Λz

˜Q

−

˜Q
(cid:13)
(cid:13)
(cid:13)

ˆΛz

˜Q
F ≤ (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
sup
i
∈

≤

n

1
n

ˆΛz
i −
ˆΛz
i

˜Λz
i
˜Λz
i

Σi

!

i=1
X
˜Λz
ˆΛz
i
i −
˜Λz
ˆΛz
[n] (cid:12)
i
i
(cid:12)
(cid:12)
z ˇκ2
κ3
z√p
(cid:12)
(cid:12)
2n
z

(cid:18)

|

|

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
≤
(cid:19)

ΣikF
k

O

(cid:18)

ˆΛz

˜Q

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
κz
(cid:13)
√n

(cid:19)

˜Λz

F

(cid:13)
(cid:13)
(cid:13)
(cid:13)
˜Λz
˜Q
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

We can then deduce that:

O

≤

˜Λz

kF +
We can now prove our second main Theorems (on the linear concentration of

O(κz).

kF ≤

F ≤ k

−

k

ˆΛz

˜Q

˜Λz

˜Q

ˆΛz

˜Q

(cid:13)
(cid:13)
(cid:13)

˜Q
(cid:13)
(cid:13)
(cid:13)
˜Q ˜Λz
):

1

z Qz around 1

z

 
 
September 8, 2021 0:38 WSPC/INSTRUCTION FILE

main

32

Proof. [Proof of Theorem 3 ] We saw in the proof of Proposition 3.1 that the
Sε as
mapping R
k · kF,Sε) deﬁned, under
1
z Qz has a Lipschitz parameter bounded by:
R(z) =

AQ, for any z

Mp),

(
F

Sε,

(C

C

∈

∈

\

\

−

sup
C

∈

\

Sε

z

O

κz
√n
z

|

(cid:19)

(cid:18)

|

= O

1
√n

(cid:18)

(cid:19)

(since when n
therefore inf z
E2, (R
| AQ)
bound:

∈

p, κz = |

≥
Sε 1/
C

z

, and when n < p, κz = 1 but E[λp] = 0 and

z
|1+
z
|
O(1)). As a O(1/√n)-Lipschitz transformation of X

|

|

\

∝
∝ E2(1/√n), we can then conclude thanks to Corollary 1 with the

| ≥

1
z

E
AQ [R]

−

˜R

F,Sε ≤

z

sup
C

Sε

E
AQ [Qz]

−

˜Λz

˜Q

O

sup
C

Sε

≤

z

|

\

∈

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

| (cid:13)
(cid:13)
(cid:13)
Although it is not particularly needed for practical use, we are now going to
p Tr( ˜R(z)) is a Stieltjes transform. One of the main
show that the mapping ˜g : z
result of next section is also to show the convergence of the Stieltjes transform g
towards ˜g on C
Sε on which we approximated
R(z)).

0 (which is a bigger set than C

Sε
−

(cid:13)
(cid:13)
(cid:13)

→

\

\

∈

\

1

|

| (cid:19)

(cid:18)

κz
√n

z

O

≤

1
√n

(cid:18)

(cid:19)

7. Approach with the Stieltjes formalism, proofs of Theorems 2

and 3

We start with an interesting identity that gives a direct link between the Stieltjes
transforms g and ˜g with the diagonal matrix Λz and ˜Λz. From the equality Qz
1
nz XX T Qz = Ip, and the Schur identities (4.1) we can deduce that:

−

g(z) =

1
pz

−

Tr(Qz) =

1
z −

1
npz2

−

=

1
z −

1
npz

ixi

xT
i Q
−
Λz
i

=

1
z

n

i=1
X

n

i Qzxi
xT

i=1
X
n
p −

(cid:18)

1
p

1

−

(cid:19)

with the notation gDz

, deﬁned for any mapping D : H

n

= gΛz

(z),

1
Λz
i

Dz

∈ Dn(H) asq:

i=1
X
z

7→

∋
1
Dz .

n

gDz

: z

1
z

n
p −

1

1
p

−

7−→

Interestingly enough, if we note ˜g
1
equality ˜g =
the following well known theorem that can be found for instance in [Bol97]:

(cid:18)
g ˜Λ, then one can rapidly check that we have the
). To show that ˜g is a Stieltjes transform, we will employ

pz Tr( ˜Q ˜Λz

i=1
X

−

≡

(cid:19)

qThe Steiltjes transform of the spectral distribution of 1
z

n(H), ˇgDz

Dz

: z

7−→ −

n
1
i=1
p P

1
Dz

7→

∈ D

n X T X is ˇg = ˇgΛz

, where for all D : H

∋

September 8, 2021 0:38 WSPC/INSTRUCTION FILE

main

33

Theorem 6. Given an analytic mapping f : H
iyf (iy) = 1
then f is the Stieltjes transform of a probability measure µ that satisﬁes the two
reciprocal formulas:

H, if limy

∞ −

→

→

+

µ(dλ)
z ,
λ

f (z) =
for any continuous pointr a < b: µ([a, b]) = limy

−

R

•

•

1
π

0+

→

b
a ℑ

(f (x + iy))dx.

If, in particular,
z
∀
continuation on C
\

H, zf (z)
).
0

∈
(R+ ∪ {

}

H, then µ(R−) = 0 and f admits an analytic
R

∈

The ﬁrst hypothesis to verify in order to employ Theorem 6 is the analycity of ˜g,
˜Λz. We can show with limiting arguments
that originates from the analycity of z
that ˜Λz is analytical as a limit of a sequence of analytical mappings. However,
although it is slightly more laborious, we prefer here to prove the analicity from the
original deﬁnition. First let us show the continuity with Proposition 6.1.

→

Proposition 7.1. The mapping z

˜Λz is continuous on C+.

7→

|

N

}

∈

∈

∈

∈

w

C

H

→∞

∈ {

w + z

H, we consider a sequence (ts)s
∈

such
Proof. Given z
ts = 0. Let us verify the assumption of Proposition 6.1 where for all
that lims
N, f s = I z+ts, ˜Γs = ˜Λz+ts and Γs = ˜Λz (it does not depend on s). We already
s
know from Proposition 5.2 that f s are all contracting for the stable semi-metric
with a Lipschitz parameter λ < 1 that can be chosen independent from s for s big
N and any i
enough. Let us express for any s
[n]:
∈
∈
i = I z+ts(˜Λz)i −
f s(Γs)i −
Γs
(z)
(I z+ts(˜Λz)) =
4 ,
Noting that for s suﬃciently big,
≥
we see that ds(
0. Therefore, the assumptions of
i ))
Proposition 6.1 are satisﬁed and we can conclude that there exists K > 0 such that
for all s

(ts)
ℑ
(z) −→s
ℑ
→∞

( ˜Λz )
4 ≥

(f s(Γs)i),

i = ts

(ts) +

(˜Λz)

ℑ
≤

(7.1)

(Γs

˜Λz

N:

ℑ

ℑ

ℑ

ℑ

ℑ

ℑ

4

∈

˜Λz

˜Λz+ts

−
(˜Λz+ts)

(˜Λz)

ℑ

ℑ

K

ts|
|
(˜Λz+ts)

2Kts
(z)

ℑ

.

(˜Λz) ≤

ℑ

≤

inf i
∈

[n]

ℑ

q

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

q
Besides, we can also bound:

(˜Λz+ts)

ℑ

q

2

(˜Λz)
ℑ
(z)

q
ℑ

(
ℑ

≤

(˜Λz) + Kts)

O(1),

≤

That directly implies that
is continuous on H.

k

˜Λz+ts

˜Λz

−

k ≤

O(ts)

−→s
→∞

0, and consequently, z

˜Λz

7→

rWe can add the property
in a, b, we need µ(

a

R, µ(
x
∈
∀
{
) = 0
b
) = µ(
}
{

{

}

x

) = limy→0+ y
}

ℑ

(f (x + iy)), here for µ to be continuous

September 8, 2021 0:38 WSPC/INSTRUCTION FILE

main

34

Let us now show that z
7→
f t = I z+t, we can decompose:

˜Λz it is diﬀerentiable. Employing again the notation

˜Λz+t

−

(cid:16)

˜Λz

=

f t(˜Λz+t)

f t(˜Λz) + f t(˜Λz)

−

(cid:17)

(cid:16)
= Diagi
∈

1
n

Tr



[n] 

ΣiR(˜Λz+t)

f 0(˜Λz)
(cid:17)
˜Λz+t
j −
˜Λz+t
j

n

˜Λz
j
˜Λz
j

−

1
n

j=1
X





Now, if we introduce the vector a(t) =
Dn(H), the matrix:

˜Λz

i −

˜Λz+t
i

(cid:16)

1

≤

i
≤

n ∈

(cid:17)

ΣjR(˜Λz)



+ tIn







Rn, and for any D, D′

∈

Ψ(D, D′) =

1
n

Tr (ΣiR(D)Σj R(D′))
DjD′j

∈ Mn

!1

≤

i,j

n

≤

We have the equation:

a(t) = Ψ(˜Λz, ˜Λz+t)a(t) + t1.

(7.2)

To be able to solve this equation we need:

Lemma 16. Given any z, z′

H, In −

∈

Ψ(˜Λz, ˜Λz′

) is invertible.

Proof. We are going to show the injectivity of In −
Rn such that x = Ψ(˜Λz, ˜Λz′
vector x
inequality, with similar calculus as in the proof of Proposition 5.2:

). Let us introduce a
)x. We can bound thanks to Cauchy-Schwatz

∈

Ψ(˜Λz, ˜Λz′

n

ΣiR(˜Λz)

Tr

=

xi|

|





1
n

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
sup
j

≤

[n] (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
therefore, if we note

∈

ℑ

q

xj
(˜Λz)

ℑ

j=1
X

ℑ

q

(˜Λz′ )

q

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
[n]

(cid:12)
(cid:12)
(cid:12)
(cid:12)
˜Λz′ , ˜Λz′
q

x
k

x
k

k

˜Λz′ , ˜Λz′

which directly implies

ℑ
sup1≤i≤n ℑ

(w)
(I w( ˜Λz( ′) ))i ∈

(0, 1).

xjΣj
(˜Λz)

ℑ

ℑ

q

(˜Λz′ )

(˜Λz′ )

(˜Λz)
˜Λz
j

ℑ
˜Λz′
j

R(˜Λz′

(˜Λz
i )

− ℑ

ℑ

(z)

(˜Λz′
i )

(z′)

− ℑ

ℑ

q

)




(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

x
k

k

˜Λ ≡

supi
∈

xj
( ˜Λz′ )

√

ℑ

( ˜Λz′ )

ℑ

, we have then the bound:
(cid:12)
(cid:12)
(cid:12)
(cid:12)

φ(z, ˜Λz′ ))

(1

φ(z, ˜Λz′))(1

≤ k
that x = 0 since we know that φ(z, ˜Λz(′ )

−

−

) =

From the

continuity

of

limit
= 0 (see the proof of Proposition 7.5 in Appendix B), we

(z, z′)

and

the

7→

),

Ψ(˜Λz, ˜Λz′

limx
can deduce as a side result from Lemma 16:

Ψ(˜Λx, ˜Λx)
k

→∞ k

Lemma 17. Given any z, z′

H,

k

∈

Ψ(˜Λz, ˜Λz′

< 1

)
k

 
September 8, 2021 0:38 WSPC/INSTRUCTION FILE

main

35

The continuity of z

7→

˜Λz given by Proposition 7.1 and the continuity of the
Ψ(˜Λz, ˜Λz) which is invertible), allows

inverse operation on matrices (around In −
us to let t tend to zero in the equation

1
t

a(t) = (In −

Ψ(˜Λz, ˜Λz+t))−

1

1,

to obtain:

Proposition 7.2. The mapping z

7→

˜Λz is analytic on H, and satisﬁes:

(In −
We can then conclude ﬁrst that for all i

= Diag

(cid:16)

∂ ˜Λz
∂z

Ψ(˜Λz, ˜Λz))−

1

1

[n], the mappings z

(cid:17)

∈

1
Λz
i

→

are Stieltjes

transforms.

Proposition 7.3. For all i
∈
whose Stieltjes transform is z

[n], there exists a distribution ˜µi with support on R+

1
˜Λz
i

7→ −

z
∀

H:

∈
1
˜Λz

Proof. We just check the hypotheses of Theorem 6. We already know that z

is analytical thanks to Proposition 7.2 and the lower bound ˜Λz

7→
(z) > 0.

i ≥ ℑ

1
˜Λz
−
i
Besides,

(˜Λz
i )
˜Λz
i |

ℑ  −

i !

> 0

= ℑ
|

= ℑ
|
∈ DIz . Finally recalling from Lemma 12 that for all y

ℑ  −

and

i !

since ˜Λz
iy
(iy) = 1, we directly see that for all j
|
|
ℑ

[n]:

∈

z
˜Λz

> 0,

(˜Λz
˜Λz

i /z)
i /z

|
R+,

∈

˜Q ˜Λiy

k

k ≤

˜Λiy
j
iy

= 1 +

1
iyn

Tr(Σj ˜Q

˜Λiy

)

1.

−→y
+
→

∞

we can then conclude with Theorem 6.

We can then deduce easily that ˜g is also a Stieltjes transform (it is the Theo-

rem 2) with an interesting characterization of its measure with the ˜µi.

Proposition 7.4. The mapping ˜g is the Stieltjes transform of the measure:

˜µ =

n
p −

(cid:18)

1

δ0 +

(cid:19)

1
p

n

i=1
X

˜µi,

where δ0 is the Dirac measure on 0 (if p > n, then the measures ˜µ1, . . . , ˜µn contains
Dirac weights on zero that cancel the term

p

n
p δ0).
−

Recall that ˜µ, satisﬁes ˜g(z) =

λ
−
This formula implies that ˜g is analytic on C
\

∞

R

+
0

1

−
z d˜µ(λ), and let us note ˜S, its support.
˜S, ˜g(¯z) = ˜g(z).
˜S and that for all z

C

∈

\

September 8, 2021 0:38 WSPC/INSTRUCTION FILE

main

36

To precise the picture let us provide a result of compactness of ˜S proven in the
Appendix.

Proposition 7.5. The measure ˜µ has a compact support ˜S
O(1).

⊂

R+ and sup ˜S

≤

We end this section with the proof of the convergence of g towards ˜g that will

close our paper.

Proof. [Proof of Theorem 3] Be careful that we want here a concentration for the
inﬁnite norm on C

0. Two cases are under study:

Sε
−

\

•

z
|1+
z
|

≤

n, κz = |

When p
|
postition 3.1 that the mapping g deﬁned for any z
identity g(z) =
−
of X, thus (g
| AQ)
thanks to the bound:

, and therefore we can show as in the proof of Pro-
(Sε
) with the
\
−
AQ a O(1/√pn)-Lipschitz transformation
−0). We can then conclude

∝ E2(1/√pn) in (
F

1
pz Tr(Qz) is, under

k · kSε

0 ∪ {

(C),

C

∈

}

0

sup
C
Sε

∈

\

−0

z

E
AQ [g(z)]

−

˜g(z)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

≤

z

≤

z

sup
C
Sε

∈

\

−0

sup
C
Sε

∈

\

−0

1
zp

Tr

O

z

(cid:18)

|

(cid:16)
κz
√np

|

E
AQ [Qz]

−

˜Λz

˜Q

(cid:17)

1
√np

.

(cid:19)

O

(cid:18)

≤

(cid:19)

•

When p
≥
that then
|

n ˇκz = |
Λz

z
|1+
z
|

|

| ≥

, and we are going to employ the fact (see Lemma 8)

O(1). Indeed, thanks to this lower bound and the identity:

g(z) =

n
p −

1

1
z

(cid:18)

−

(cid:19)

1
p

n

i=1
X

1
Λz
i

,

we can show as in the proof of Lemma 5 that g is, under
Lipschitz transformation of X, and as such (g
kSε

| AQ)
−0). And this time we conclude with the bound:

AQ, a O(1/√pn)-
∝ E2(1/√pn) in (
k ·
F

(C),

sup
C
Sε

−0

∈

\

z

E
AQ [g(z)]

−

˜g(z)

≤

z

(cid:12)
(cid:12)

(cid:12)
(cid:12)

≤

z

1
p

O

n

i=1
X
1
p

O

sup
C
Sε

−0

∈

\

sup
C
Sε

−0

∈

\
1
p

O

≤

≤

(cid:19)

(cid:18)

E




n

˜Λz
i

Λz
(cid:12)
(cid:12)
(cid:12)

i −
Λz
i

˜Λz
i

(cid:12)
(cid:12)
E
(cid:12)
AQ

(cid:12)
(cid:12)
(cid:12)





(cid:12)
(cid:12)
Λz
(cid:12)

i −

h(cid:12)
(cid:12)
(cid:12)

i=1
X
1
√pn

(cid:18)

(cid:19)

ˆΛz
i

+

i

(cid:12)
(cid:12)
(cid:12)

˜Λz
i

i −

ˆΛz
(cid:12)
(cid:12)
(cid:12)

!

(cid:12)
(cid:12)
(cid:12)

We retrieve then the concentration inequality of the theorem thanks to the bound
on P(

AQ) given in Lemma 3.

 
September 8, 2021 0:38 WSPC/INSTRUCTION FILE

main

37

Appendix A. Properties of the semi metric ds and consequences

in random matrix theory

In this appendix, we will employ the semi-metric ds indiﬀerently on diagonal matri-
Dn(H) or vectors of Hn or more simply with variables of H as in next proposition.
ces
Proposition Appendix A.1. All the Stieltjes transforms are 1-Lipschitz for the
semi-metric ds on H.

Proof. We consider a Stieltjes transform g : z
R. Given z, z′

H, we can bound thanks to Cauchy-Schwarz inequality:

z for a given measure µ on

→

dµ(t)
t
−

g(z)

|

−

g(z′)

| ≤

R

z′
−
(z)

z

z′
−
z)(t

(t

−

z′
−
(z)

ℑ
ℑ
(g(z))

z
(z′) (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ℑ

p
ℑ

ℑ

−

z′)

dµ(t)
(cid:12)
(cid:12)
(cid:12)
(z)
(cid:12)
z

≤ (cid:12)
(cid:12)
(cid:12)
p
(cid:12)
(cid:12)
2 dµ(t)
|
(g(z′))ds(z, z′)

ℑ
t
−

sZ

|

Z

∈

Z

(cid:12)
(cid:12)
(cid:12)
(cid:12)
≤ (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
p

=

ℑ

z
ℑ
(t
(z′) (cid:12)
(cid:12)
Z p
−
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(z′)
(cid:12)
(cid:12)
z′|

2 dµ(t)

ℑ
t
−

|

(z)
ℑ
z)(t
−

(z′)
z′)

dµ(t)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

We did not ﬁnd any particular use of this proposition (the idea could be to solve

g(z) = z) but the stability it introduces looks interesting.

Let us start with some preliminary lemmas before proving the fourth point
of Proposition 5.1, i.e. the stability towards the sum of the class of mappings, 1-
Lipschitz for the semi-metric ds. They are mere adaptation of results of [LC20].

Lemma 18. Given four positive numbers a, b, c, d

R+:

∈

√ab +

αβ

≤

(a + α)(b + β)

p

p

Lemma 19. Given four diagonal matrices ∆, ∆′, D, D′

max

and

a + α
b + β ≤
∈ Dn(H):
max(ds(∆, ∆′), ds(D, D′)).

a
b

,

α
β

(cid:18)

(cid:19)

ds(∆ + D, ∆′ + D′)

≤

Proof. [Proof of Property 19] For any ∆, ∆′, D, D′
that:

+

n , there exists i0 ∈

∈ D

[n] such

ds(∆ + D, ∆′ + D′) =

≤

≤

thanks to Lemma 18.

D′i0
(∆′i0 + D′i0)
(cid:12)
(cid:12)
Di0 −
(cid:12)
(cid:12)
ℑ
q

(Di0 )

+

∆′i0 + Di0 −
ℑ

∆i0 −
(∆i0 + Di0)
(cid:12)
(cid:12)
ℑ
∆i0 −
ℑ
∆i0 −
(∆i0 )
(cid:12)
(cid:12)
ℑ

(∆i0 )
(cid:12)
(cid:12)



ℑ

∆′i0
(∆′i0 ) +

(cid:12)
(cid:12)

∆′i0
(∆′i0 )
(cid:12)
(cid:12)

ℑ

,

D′i0

q

q

max



q

(D′i0 ))
(cid:12)
(cid:12)
ℑ
Di0 −
(Di0 )
(cid:12)
(cid:12)
ℑ

ℑ

D′i0
(D′i0 ) 
(cid:12)
(cid:12)


q

September 8, 2021 0:38 WSPC/INSTRUCTION FILE

main

38

Lipschitz for ds then f + g is also λ-Lipschitz.

We deduce directly from this lemma that if f, g :

→ Dn(H) are λ-
This property gives a very fast proof to show the existence and uniqueness of
solutions to the equations studied in [AKE16] (however their proof is not much
longer). We start with a preliminary lemma:

Dn(H)

Proposition Appendix A.2. Given a matrix S
Cp

+ and it is 1-Lipschitz for the semi-metric ds.

+ to Cn

∈ Mn,p(R+) , z

Sz goes from

7→

Proof. For any z
(Sz) = S
If we note s1, . . . , sn, the columns of S, we can decompose Sz =
πi(z) = zi. Each mapping siπi is 1-Lipschitz for ds, since we have for any z, z′

+ since all the entries of S are positive.
n
i=1 siπi(z), where
Cp
+:

(z)

ℑ

ℑ

∈

∈

Cp
+,

Rp

ds(sjπi(z), sjπi(z′)) = sup

([si]jz′i) (cid:12)
(cid:12)
(cid:12)
therefore, as a sum of 1-Lipschitz operators, we know that z
(cid:12)
(cid:12)
for ds.

[p] (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

p

ℑ

∈

j

[si]jz′i

[si]jzi −
([si]jzi)
ℑ

=

→

P
zi −
(zi)
ℑ

z′i

∈

,

(z′i)
Sz is also 1-Lipschitz
p

ℑ

Proposition Appendix A.3. Given any a
the equation:

∈

Rn and any matrixs S

∈ Mn(R+),

admits a unique solution m

= z1 + a + Sm

1
m
−
Cn
+.

∈

Proof. Let us introduce I : x
x . To employ Theorem 5, let us
z1 + a
ﬁrst show that the imaginary part of I(x) is bounded from below and above for
all x
(z), we can
+ we see straightforwardly that
+. Given x
furthermore bound:

(I(x))

≥ ℑ

S 1

Cn

Cn

→

−

ℑ

∈

∈

(I(x))

ℑ

≤ ℑ

(x)
(z)1 + S ℑ
x
|
|

2 ≤ ℑ

(z)1 + S 1

(z)Ip +

(x) ≤

ℑ

(cid:18)

ℑ

1
(z)

S

ℑ

(cid:19)

κI .

1 ≡

Besides, we already know from Propostion 5.1 that I is 1-Lipschitz for ds but we
Hn we can bound thanks to
need a Lipschitz parameter lower than 1. Given x, y
Proposition 5.1 and Appendix A.2:

∈

I(x)

|

−

I(y)

| ≤

1
x −

1
y

S

(cid:18)

1
x

S

(cid:18)

(cid:18)

ℑ

(cid:19)(cid:19)

S

(cid:18)

(cid:18)

1
y

≤ sℑ

ds(x, y)

(cid:19)(cid:19)

that implies that the Lipschitz parameter of I is lower than:

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
s(cid:18)

−

(z)
ℑ
(I(x))

ℑ

1
(cid:19) (cid:18)

−

(z)
ℑ
(I(y))

ℑ

(cid:19)

1

−

≤

(z)
ℑ
κI

< 1

We conclude then with Theorem 5 that there exists a unique x
x = I(x), from which we deduce the existence and uniqueness of m = 1
x .

∈

Hn such that

sNote that unlike in [] we do not suppose that S is symmetric.

September 8, 2021 0:38 WSPC/INSTRUCTION FILE

main

Appendix B. Proofs of the non necessary results

39

Proof. [Proof of Lemma 13]If we assume that
i
[n],
i | ≥
∀
|
n
that 1
1
1
2 , and therefore,
2ν and
i=1
n
consequence,

i | ≤

n
i=1

[n]:

Σi
˜Λz

1
n

∈

|

˜Λz

2ν, then we deduce
2. As a

˜Q ˜Λz

|

| ≤

1
˜Λz
i | ≤
|
i
∈
∀

P

˜Λz
i

(cid:12)
(cid:12)
We can conclude that:
(cid:12)

(cid:12)
(cid:12)
(cid:12)

z

|

≤ |

P
1
n

+

˜Λz

˜Q

Tr

Σi|
(cid:16)

|

(cid:17)

+

z

|

≤ |

2pν
n

˜Λz

i | ≤

sup
i
∈

[n] |

max

(cid:18)

ν
2

+

,

z

|

|

2pν
n

O(1 +

).

z

|

|

≤

(cid:19)

Proof. [Proof of Proposition 7.5] We are going to show that for x suﬃciently big,
(g(x + iy)) = 0, which will allow us to conclude thanks to the relation
limy
R and
between ˜µ and ˜g given in Theorem 6. Considering z = x + iy
such that

H, for x, y

ℑ

0+

∈

∈

→

x

x0 ≡

≥

max

8
n

sup
[n]
i
∈

Tr(Σi), 4ν

!

let us show ﬁrst that

z is stable on

I

i
∈
∀
t
A ≡ Dn(
{

(˜Λz
[n],
i )
ℜ
≥
+ iR∗+)
x
2 }

≥

ℜ

1

( ˜QL)−
(cid:16)

(cid:17)

= Ip −

1
n

n

ℜ

i=1
X
DI z ,

ℑ

and as we already know, since
then bound:

∈ A

x0
2 . This is a consequence of the fact that
∩ DI z . Indeed, given L
Σi
(Λi)Σi
(Li) ≥
Li|
( ˜QL)−

0, therefore,

2. We can

Ip −

2 ≥

i=1
X

˜QL

1
n

1
2

ℜ

n

1

|

k

k ≤

≥

(cid:16)

(cid:17)

z(L)i) = x

(
I

ℜ

−

x

−

≥

1
n

4
n

Σi ˜QL

Tr



1


−


Tr (Σi)


x
.
2

≥

1
n

n

ℜ

j=1
X

(Lj)Σj
Lj|

|

2 



¯˜QL





Thus as a limit of elements of
Besides, let us bound:

, Λz

A

, and

∈ A

[n],

i
∀

∈

ℜ

(Λz
i )

≥

x
2 .

ΣiR(Λz)

Tr



1
n

(Λz

i ) = y +

ℑ

y +

≤

1
n

4
n


Tr (Σi)

n

ℑ

j=1
X

j )Σj
2

(Λz
Λz
j |

|

¯R(Λz)





(Λz
j )
ℑ
(Λz
j )2
ℜ

sup
[n]
j

∈

1
n

n

j=1
X

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Σj(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

 
September 8, 2021 0:38 WSPC/INSTRUCTION FILE

main

40

and with the bounds
(Λz
j )

supj

[n] ℑ

∈

1
n
y + 2ν
(cid:13)
x supj
P
(cid:13)
(cid:13)

≤

∈

n
j=1 Σj

ν,

(Λz

j )2

xx0
4 , we can eventually bound

ℜ

≥

≤
(Λz
(cid:13)
j ), which implies, for x suﬃciently big:
[n] ℑ
(cid:13)
(cid:13)
(Λz
j )

0.

y

sup
j

[n] ℑ

∈

≤

1

2ν
x

−

−→y
0+
→

We can then conclude letting y tend to 0 in the formulation ˜g = g ˜Λ:

(˜g(x + iy)) =

ℑ

y
x2 + y2

n
p −

(cid:18)

1

+

(cid:19)

1
p

n

i=1
X

ℜ

References

(Λz
i )
ℑ
i )2 +

ℑ

(Λz

(Λz

i )2 −→y
0+

→

0

[AC15]

Radosław Adamczak and Djalil Chafaï. Circular law for random matrices
with unconditional log-concave distribution. Communications in Contemporary
Mathematics, 17(4), 2015.

[Ada11] Radoslaw Adamczak. On the marchenko-pastur and circular laws for some
classes of random matrices with dependent entries. Electronic Journal of Prob-
ability, 16:1065–1095, 2011.

[AKE16] Oskari Ajanki, Torben Krüger, and Laszlo Eedös. Singularities of solutions to
quadratic vector equations on the complex upper half-plane. Communications
on Pure and Applied Mathematics, 70(9):1672–1705, 2016.

[BKV96] A. Boutet de Monvel, A. Khorunzhy, and V. Vasilchuk. Limiting eigenvalue
distribution of random matrices with correlated entries. Markov Processes and
Related Fields, 2(4):607–636, 1996.

[BZ08]

[Bol97]

[BLM13] Stéphane Boucheron, Gábor Lugosi, and Pascal Massart. Concentration In-
equalities: A Nonasymptotic Theory of Independence. Oxford University Press,
2013.
Vladimir Bolotnikov. On a general moment problem on the half axis. Linear
algebra and its application, 255:57–112, 1997.
Zhidong Bai and Wang Zhou. Large sample covariance matrices without inde-
pendence structures in columns. Statistica Sinica, 18:425–442, 2008.
[HLN07] W. Hachem, P. Loubaton, and J. Najim. Deterministic equivalents for certain
functionals of large random matrices. Annals of Applied Probability, 17(3):875–
930, 2007.
Cosme Louart and Romain Couillet. Concentration of measure and large ran-
dom matrices with an application to sample covariance matrices. submitted to
Random Matrices: Theory and Applications, 2019.
Cosme Louart and Romain Couillet. A concentration of measure and random
matrix approach to large dimensional robust statistics. submitted, 2020.
Cosme Louart and Romain Couillet. Concentration of measure and generalized
product of random vectors with an application to hanson-wright-like inequali-
ties. arXiv preprint arXiv:2102.08020, 2021.

[LC21]

[LC19]

[LC20]

[Led05] Michel Ledoux. The concentration of measure phenomenon. Number 89. Amer-

[MP67]

[Sil86]

ican Mathematical Soc., 2005.
V. A. Mar˘cenko and L. A. Pastur. Distribution of eigenvalues for some sets of
random matrices. Math USSR-Sbornik, 1(4):457–483, 1967.
J. W. Silverstein. Eigenvalues and eigenvectors of large dimensional sample
covariance matrices. Random Matrices and their Applications, pages 153–159,
1986.

September 8, 2021 0:38 WSPC/INSTRUCTION FILE

main

41

[Tao12]

[SLTC20] Mohamed El Amine Seddik, Cosme Louart, Mohamed Tamaazousti, and Ro-
main Couillet. Random matrix theory proves thatdeep learning representations
of gan-databehave as gaussian mixtures. ICML (submitted), 2020.
Terence Tao. Topics in random matrix theory, volume 132. American Mathe-
matical Soc., 2012.
Roman Vershynin. High-Diensional Probability. Cambridge University Press,
2018.
Pavel Yaskov. A short proof of the marchenko–pastur theorem. Comptes Rendus
Mathematique, 354, 3:319–322, 2016.
Y. Yin. Limiting spectral distribution for a class of random matrices. Journal
of Multivariate Analysis, 20:50–68, 1986.

[Yin86]

[Yas16]

[Ver18]

