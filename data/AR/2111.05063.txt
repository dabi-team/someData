Tightening the Approximation Error of Adversarial Risk
with Auto Loss Function Search

PENGFEI XIA, University of Science and Technology of China
ZIQIANG LI, University of Science and Technology of China
BIN LI, University of Science and Technology of China

2
2
0
2

r
p
A
9

]

G
L
.
s
c
[

2
v
3
6
0
5
0
.
1
1
1
2
:
v
i
X
r
a

Despite achieving great success, Deep Neural Networks (DNNs) are
vulnerable to adversarial examples. How to accurately evaluate the
adversarial robustness of DNNs is critical for their deployment in
real-world applications. An ideal indicator of robustness is adver-
sarial risk. Unfortunately, since it involves maximizing the 0-1 loss,
calculating the true risk is technically intractable. The most common
solution for this is to compute an approximate risk by replacing the
0-1 loss with a surrogate one. Some functions have been used, such
as Cross-Entropy (CE) loss and Difference of Logits Ratio (DLR) loss.
However, these functions are all manually designed and may not
be well suited for adversarial robustness evaluation. In this paper,
we leverage AutoML to tighten the error (gap) between the true
and approximate risks. Our main contributions are as follows. First,
AutoLoss-AR, the first method to search for surrogate losses for
adversarial risk, with an elaborate search space, is proposed. The
experimental results on 10 adversarially trained models demonstrate
the effectiveness of the proposed method: the risks evaluated using
the best-discovered losses are 0.2% to 1.6% better than those evalu-
ated using the handcrafted baselines. Second, 5 surrogate losses with
clean and readable formulas are distilled out and tested on 7 unseen
adversarially trained models. These losses outperform the baselines
by 0.8% to 2.4%, indicating that they can be used individually as
some kind of new knowledge. Besides, the possible reasons for the
better performance of these losses are explored.

Keywords: Deep Neural Networks, Adversarial Examples, Adver-
sarial Risk, Approximation Error, Auto Loss Function Search.

1

INTRODUCTION

Numerous studies [6, 16, 31, 40, 49] have demonstrated that
Deep Neural Networks (DNNs) are vulnerable to adversarial
examples: by adding a small, imperceptible perturbation to
a clean input, the model’s prediction can be easily misled.
With the growing demand for DNNs in security-sensitive
applications, such as autonomous vehicles [17] and biometric
identification systems [39], this vulnerability has attracted

Authors’ addresses: Pengfei Xia, xpengfei@mail.ustc.edu.cn, University of
Science and Technology of China, Hefei, China; Ziqiang Li, iceli@mail.ustc.
edu.cn, University of Science and Technology of China, Hefei, China; Bin Li,
binli@ustc.edu.cn, University of Science and Technology of China, Hefei,
China.

2022. XXXX-XXXX/2022/4-ART $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

widespread attention. A key issue in this field is how to accu-
rately evaluate the adversarial robustness of DNNs. Unfor-
tunately, as revealed by recent studies [4, 5, 8, 41, 42], this is
not an easy task to tackle, and an inappropriate evaluation
method may lead to a false sense of robustness.

One factor that affects the quality of the evaluation is the
surrogate loss. Specifically, the adversarial robustness of a
DNN 𝑓 can be measured by its adversarial risk [16, 31, 42, 48],
that is:

𝑅(𝑓 , 𝐷, 𝐵, 𝜖) =

E
(𝑥,𝑦)∼𝐷

max
𝑥 ′ ∈𝐵 (𝑥,𝜖)

ℓ0−1(𝑓 (𝑥 ′), 𝑦)

,

(1)

(cid:20)

(cid:21)

where (𝑥, 𝑦) ∼ 𝐷 denote the input and its ground-truth label
sampled from the joint distribution 𝐷, 𝐵(𝑥, 𝜖) denotes the
perceptually similar set of 𝑥 with the hyperparameter 𝜖, e.g.,
within a 𝐿𝑝 -norm sphere ||𝑥 ′ − 𝑥 ||𝑝 ≤ 𝜖, and ℓ0−1 denotes
the 0-1 loss. This formula defines the worst-case risk of 𝑓
under given 𝐷, 𝐵, and 𝜖, and is a solid indicator of adversarial
robustness. However, since optimizing the discontinuous 0-
1 loss is computationally intractable [3], it is common to
approximate the true risk by replacing the 0-1 loss with a
surrogate function ℓ𝑠 , that is:

(cid:20)

(cid:21)

ℓ0−1(𝑓 (𝑥 ′), 𝑦)

𝑅 ′(𝑓 , 𝐷, 𝐵, 𝜖, ℓ𝑠 ) =

E
(𝑥,𝑦)∼𝐷
s.t. 𝑥 ′ = argmax
𝑥 ′ ∈𝐵 (𝑥,𝜖)
In fact, it is still challenging to compute 𝑅 ′, because obtaining
the global maximum on 𝑓 is generally impossible. Thus, one
has to compromise further: using gradient descent [31] or
other algorithms [2, 38] to find a suboptimal solution, that is:

ℓ𝑠 (𝑓 (𝑥 ′), 𝑦)

(2)

.

(cid:20)

𝑅 ′′(𝑓 , 𝐷, 𝐵, 𝜖, ℓ𝑠, 𝑚) =

E
(𝑥,𝑦)∼𝐷
s.t. 𝑥 ′ = 𝑚(𝑓 , 𝑥, 𝑦, 𝐵, 𝜖, ℓ𝑠 )

ℓ0−1(𝑓 (𝑥 ′), 𝑦)

(cid:21)

,

(3)

where 𝑚 denotes a specific algorithm. We define the approxi-
mation error (gap) between 𝑅 and 𝑅 ′′ as:

𝐸 (𝑓 , 𝐷, 𝐵, 𝜖, ℓ𝑠, 𝑚) = 𝑅(𝑓 , 𝐷, 𝐵, 𝜖) − 𝑅 ′′(𝑓 , 𝐷, 𝐵, 𝜖, ℓ𝑠, 𝑚).
(4)
The impact of the surrogate loss on 𝐸 is twofold. First, it
determines the theoretical gap between 𝑅 and 𝑅 ′. Second,
the degree of matching between ℓ𝑠 and 𝑚 affects the gap
between 𝑅 ′ and 𝑅 ′′. Many functions have been used as surro-
gate losses to reduce the approximation error. For example,

, Vol. 1, No. 1, Article . Publication date: April 2022.

 
 
 
 
 
 
2

• Pengfei Xia, Ziqiang Li, and Bin Li

several studies [11, 16, 31, 40] adopt Cross-Entropy (CE) loss
to construct adversarial examples to evaluate DNNs. Carlini
and Wagner [6] tested and compared the performance of 7
handcrafted (CW) functions. Croce and Hein [8] proposed
Difference of Logits Ratio (DLR) loss to alleviate the gradient
masking problem [4, 32] that often occurs in the evaluation.
However, the above functions are all manually designed
and may not be well suited for adversarial robustness evalua-
tion. Inspired by recent advances [26–28] in AutoML, we won-
der if it is possible to automatically design surrogate losses
that are suitable for adversarial risk. Li et al. [27] propose
AutoLoss-Zero, a tree-based framework for finding losses for
generic learning tasks, such as semantic segmentation [30]
and object detection [14]. We customize it to evaluate the
adversarial robustness of DNNs and propose AutoLoss-AR.
The main contributions of this paper are:

• AutoLoss-AR, the first method to search for surrogate
losses for adversarial robustness evaluation, is pro-
posed. To ensure the effectiveness and efficiency of
the search, a search space and a fitness evaluation pro-
cedure are customized. The experimental results on 10
adversarially trained models show that the proposed
method can find functions with smaller approximation
errors than the handcrafted baselines.

• 5 surrogate losses with clean and readable formulas
are distilled out. These losses outperform the hand-
crafted baselines on 7 unseen adversarially trained mod-
els, which suggests that they can be used independently
of AutoLoss-AR, as some kind of new knowledge. Be-
sides, the possible reasons why these losses perform
better than the baselines are explored by visualizing
the local loss landscape.

The rest of this paper is organized as follows. The next
section briefly reviews the relevant studies. In Section 3, we
detail the proposed AutoLoss-AR. Our experimental settings
and results are depicted in Section 4. Section 5 concludes this
paper.

2 RELATED WORK
2.1 Adversarial Examples

By adding almost imperceptible perturbations to clean inputs,
adversarial examples can mislead DNNs with high confidence.
Such examples were first noticed by Szegedy et al. [40] and
have attracted much attention. Goodfellow et al. [16] attrib-
uted the existence of adversarial examples to the linear nature
of DNNs and proposed a single-step attack named Fast Gra-
dient Sign Method (FGSM) to construct them. Several studies
[11, 25, 31] extended this single-step attack to multi-step iter-
ations. Among them, Project Gradient Descent (PGD) attack
proposed by Madry et al. [31] is the most typical one and
has shown a strong attack capability. Carlini and Wagner [6]

, Vol. 1, No. 1, Article . Publication date: April 2022.

developed a powerful class of target attacks, named C&W
attacks, by transforming the constrained problem into an
unconstrained one with a penalty item. Su et al. [38] demon-
strated that DNNs can be fooled by modifying only one pixel
in the input image.

One of the primary principles for constructing adversarial
examples is that the instances with or without perturbations
should be perceptually similar to humans. The methods men-
tioned above satisfy this principle by limiting the 𝐿𝑝 -norm
of the difference between a clean sample and its adversary to
a small value. Some other methods maintain high perceptual
similarity without the 𝐿𝑝 -norm restriction. For example, En-
gstrom et al. [13] found that DNNs are fragile to simple image
transformations, such as rotation and translation. Xiao et al.
[50] proposed to construct adversarial examples by slightly
flowing the position of each pixel in the input images.

Regardless of the form used to generate adversarial exam-
ples, these attack methods evaluate the adversarial robustness
of a DNN by computing an approximation of the true adver-
sarial risk, which involves the choice of the surrogate loss.

2.2 Adversarial Training

To improve the adversarial robustness of DNNs, many de-
fense methods have been developed, such as data prepro-
cessing [18, 22], feature squeezing [51], and gradient regu-
larization [35, 47]. Among these methods, the most intuitive
and effective one is to reuse adversarial examples as aug-
mented data to train DNNs, i.e., Adversarial Training (AT).
The original AT can be formulated as:

𝜃 ∗ = argmin

𝜃

E
(𝑥,𝑦)∼𝐷

(cid:20)

max
𝑥 ′ ∈𝐵 (𝑥,𝜖)

(cid:21)

ℓ (𝑓𝜃 (𝑥 ′), 𝑦)

(5)

where 𝜃 denotes the parameters of 𝑓 and ℓ denotes a loss
function. This type of method was first introduced by Good-
fellow et al. [16]. Madry et al. [31] developed it by training
DNNs with stronger adversarial examples generated by PGD.
Subsequent studies have mainly focused on accelerating the
training [37, 52] or improving the resistance [46, 53].

In this paper, we use sixteen adversarially trained models
to verify the effectiveness of AutoLoss-AR and the discovered
surrogate losses.

2.3 Auto Loss Function Search

Auto loss function search, which belongs to AutoML [19, 55],
has raised the interest of researchers in recent years. At
present, all studies concentrate on how to find a loss function
to train a model with better performance. For example, Li
et al. [26] and Wang et al. [43] proposed to optimize a loss
function for face recognition, where the search is performed
on specific hyper-parameters in a fixed formula. Li et al. [28]
and Liu et al. [29] introduced this idea to semantic segmenta-
tion and object detection, respectively. More recently, Li et al.

Tightening the Approximation Error of Adversarial Risk with Auto Loss Function Search •

3

Fig. 1. Pipeline of AutoLoss-AR. The search space and the search algorithm are two important parts of AutoLoss-AR, where the former is
composed of two sets and the latter is composed of 5 steps.

[27] proposed AutoLoss-Zero, a general framework for learn-
ing tasks. They verified the effectiveness of AutoLoss-Zero
on four tasks, i.e., semantic segmentation, object detection,
instance segmentation, and pose estimation.

We customize AutoLoss-Zero to tighten the approximation
error of adversarial risk and propose AutoLoss-AR. To better
fit the evaluation task, we design a search space and a fitness
evaluation procedure.

3 METHODOLOGY
3.1 Problem Formulation
Our goal is to find a surrogate loss ℓ𝑠 that minimizes the
approximation error 𝐸. It can be formulated as:

ℓ𝑠 = argmin

ℓ𝑠 ∈S
= argmin
ℓ𝑠 ∈S

𝐸 (𝑓 , 𝐷, 𝐵, 𝜖, ℓ𝑠, 𝑚)

𝑅(𝑓 , 𝐷, 𝐵, 𝜖) − 𝑅 ′′(𝑓 , 𝐷, 𝐵, 𝜖, ℓ𝑠, 𝑚)

,

(6)

where S denotes the search space of the loss function. Since
under given 𝑓 , 𝐷, 𝐵, and 𝜖, 𝑅 is an unknown constant, and
𝑅 ≥ 𝑅 ′′, optimizing Equation 6 is equivalent to optimizing:

ℓ𝑠 = argmax

ℓ𝑠 ∈S

𝑅 ′′(𝑓 , 𝐷, 𝐵, 𝜖, ℓ𝑠, 𝑚).

(7)

3.2 AutoLoss-AR

To solve the above problem, each function is expressed as
a tree, and Genetic Programming (GP) [23] is adopted to
find a suitable solution in the search space. Our method
is customized from AutoLoss-Zero [27], and is dubbed as
AutoLoss-AR, which stands for Auto Loss function search

for Adversarial Risk. The main difference is that AutoLoss-
AR uses a search space and a fitness evaluation procedure
specifically designed for adversarial robustness evaluation
to ensure the effectiveness and efficiency of the search. The
pipeline of AutoLoss-AR is shown in Figure 1, where the
search space and the search algorithm are two important
parts. More details are described in the following.

Search Space. The search space, consisting of the input
3.2.1
node set and the primitive operation set, determines the do-
main that an algorithm searches. To design a suitable space,
we refer to some handcrafted loss functions that are often
used for adversarial robustness evaluation, including CE loss
[16, 31, 40], CW losses [6], Margin Logit (ML) loss [42], and
DLR loss [8]. Besides these two sets, another factor that af-
fects the search space is the maximum depth of a tree.

1) Input Node Set: We consider to evaluate the adversarial
robustness of classification models in this paper. The input
node set contains two variables, 𝑝 and 𝑞, and two constants,
0 and 1, where 𝑝 denotes the logits of the model output 𝑓 (𝑥)
and 𝑞 denotes the one-hot form of the true label 𝑦. 𝑝 and 𝑞 are
of the same shape (1, 𝐶), where 𝐶 is the number of categories.
The purpose of using the logits instead of 𝑓 (𝑥) and adding
two constants is to expand the search space for a possible
better solution. Since 𝑓 (𝑥) = softmax(𝑝), to ensure that the
losses with 𝑓 (𝑥) as input are easy to explore, we have added
Softmax to the primitive operation set.

2) Primitive Operation Set: The primitive operations, includ-
ing the element-wise and aggregation operations, are listed
in Table 1. Since the loss calculation is usually performed
on a batch, we define the inputs of each operation to be of

, Vol. 1, No. 1, Article . Publication date: April 2022.

AddMulNegAbsInvSqrtSquareExpLogMaxSumSoftmaxPrimitive Operation Set1𝑝𝑞0Input Node SetSearch SpaceInitializationEvaluationSoftmax𝑝𝑞MulAbs𝑜Add𝑝𝑞MulAddSquare1ExpNeg𝑜…Population0.80.70.1Add𝑝𝑞ExpAddSoftmax1MulNeg𝑜Softmax𝑝𝑞MulAbs𝑜Add𝑝𝑞MulAddSquare1ExpNeg𝑜…PopulationAdd𝑝𝑞ExpAddSoftmax1MulNeg𝑜PopulationSoftmax𝑝𝑞MulAbs𝑜…0.80.7Add𝑝𝑞ExpAddSoftmax1MulNeg𝑜Softmax𝑝𝑞MulAbs𝑜…Offspring PopulationAdd𝑝𝑞SqrtAddSoftmax1MulNeg𝑜AbsMul𝑝𝑞MulSqrtAdd1ExpNegSoftmaxExp𝑜CrossoverandMutationUpdateSelection4

• Pengfei Xia, Ziqiang Li, and Bin Li

Table 1. Primitive operation set. The shape of 𝑎 and 𝑏 is (𝑁 , 𝐶) or
(𝑁 , 1), where 𝑁 denotes the batch size and 𝐶 denotes the number
of categories. 𝛾 is a small constant to avoid dividing by zero.

Operation Expression

Arity

Add
Mul
Neg
Abs
Inv
Sqrt
Square
Exp
Log

Max
Sum
Softmax

𝑎 + 𝑏
𝑎 × 𝑏
−𝑎
|𝑎|
sign(𝑎)/(|𝑎| + 𝛾)
sign(𝑎) × √︁|𝑎| + 𝛾
𝑎2
exp(𝑎)
sign(𝑎) × log(|𝑎| + 𝛾)
max(𝑎)
sum(𝑎)
softmax(𝑎)

2
2
1
1
1
1
1
1
1

1
1
1

shape (𝑁 , 𝐶) or (𝑁 , 1), where 𝑁 denotes the batch size. For
the element-wise operations, including Add, Mul, Neg, Abs,
Inv, Sqrt, Square, Exp, and Log, the output dimension is the
same as the input dimension. The aggregation operations,
including Max, Sum, and Softmax, are performed on the sec-
ond dimension and keep that dimension without reduction.
It is worth noting that for each expression coded by a tree,
the output 𝑜 is a matrix of shape (𝑁 , 𝐶) or (𝑁 , 1), so it needs
to be calculated to a real number by:

ℓ𝑠 =

1
𝑁

𝑁
∑︁

𝐶 or 1
∑︁

𝑛=1

𝑐=1

𝑜𝑛𝑐 .

(8)

A surrogate loss comprises a tree-encoded expression and
fixed aggregation operations in Equation 8.

Search Algorithm. We adopt GP, a population-based
3.2.2
algorithm, to find a suitable surrogate loss to tighten the
approximation error in the search space. As shown in Fig-
ure 1, the search algorithm consists of 5 steps: initialization,
evaluation, selection, crossover, and mutation.

1) Initialization: In the initialization step, a number of ex-
pression trees are randomly generated to form the initial
population.

2) Evaluation: The fitness evaluation procedure is shown
in Figure 2. During the evaluation process, the generated
expressions may be infeasible to compute, such as invalid
values NaN, Inf, etc. At this point, we set their fitness to 0.
Since the optimization objective is Equation 7, we directly
calculate 𝑅 ′′ for each valid tree as its fitness, where the value
is between 0 and 1. The calculation of 𝑅 ′′ is shown in Equa-
tion 3, and we choose PGD-10 [31] as the algorithm 𝑚. To
further reduce the time consumption, the approximate risk

, Vol. 1, No. 1, Article . Publication date: April 2022.

Fig. 2. Fitness evaluation procedure used in AutoLoss-AR.

of an expression is calculated over 1000 images during the
search. When testing the losses discovered, we calculate the
approximate risk on the entire test set.

3) Selection: We adopt the tournament [15] as the selection
strategy in AutoLoss-AR. It involves holding several tourna-
ments among a few individuals randomly chosen from the
population. The winner of each tournament, i.e., the one with
the maximum fitness, is selected for the next step. Two hyper-
parameters, i.e., the size of the tournament and the number
of times it is held, are used to control the selection process.
4) Crossover and Mutation: The crossover and mutation
steps are used to generate the offspring population. These ex-
pression trees are randomly subjected to one-point crossover
and independent mutation, where the probabilities of mating
and mutating are two hyperparameters.

4 EXPERIMENTS
4.1 Purpose

The purpose of our experiments is to answer:

• Under given conditions, i.e., 𝑓 , 𝐷, 𝐵, and 𝜖, can AutoLoss-
AR obtain a better solution than the handcrafted losses
for tightening the approximation error?

• Can the best-searched losses be used in other condi-

tions, such as an unseen model 𝑓 ?

• if the searched surrogate losses perform better than the

handcrafted baselines, then why?

4.2 Setup

To test the performance of AutoLoss-AR and the searched sur-
rogate losses, we evaluate the robustness of 17 adversarially
trained models1, using CIFAR-10 [24] and CIFAR-100 [24]
as datasets. Of these, 10 models are used for white-boxing

1All the adversarially trained models used in this paper are obtained from
https://robustbench.github.io.

DecodingIs it valid?Expression treeEvaluatedbyPGDFitness = 0YNTightening the Approximation Error of Adversarial Risk with Auto Loss Function Search •

5

Table 2. Adversarial accuracy of 10 adversarially trained models in white-box setting. The PGDBS column is the results of the best-searched
losses with PGD-100. The Diff. column is the differences between the best-handcrafted losses and the best-searched losses, where the
negative values indicate that the searched losses perform better than the handcrafted baselines.

Methods

Clean

PGDCE PGDCW APGDCE APGDCW APGDDLR

PGDBS

Diff.

CIFAR-10, 𝐿∞, 𝜖 = 8/255

Addepalli2021 [1]
Cui2021 [9]
Huang2020 [21]
Sehwag2021[36]
Wu2020[46]
Zhang2019 [53]

80.24
88.22
83.48
84.38
85.36
84.92

55.78
53.97
55.92
57.42
58.80
54.91

51.70
53.77
53.95
56.43
56.82
53.58

55.78
53.89
55.83
57.30
58.86
54.79

51.72
53.75
53.97
56.37
56.80
53.54

51.75
56.05
54.42
56.92
56.90
53.69

51.31
52.79
53.10
54.76
56.32
52.94

-0.39
-0.96
-0.85
-1.61
-0.48
-0.60

CIFAR-10, 𝐿2, 𝜖 = 0.5
Sehwag2021 [36]
Wu2020[46]

89.52
88.51

74.13
74.74

74.09
73.91

74.15
74.73

74.15
74.73

74.26
73.91

73.48
73.69

-0.60
-0.22

CIFAR-100, 𝐿∞, 𝜖 = 8/255

Addepalli2021 [1]
Cui2021 [9]

62.02
70.25

32.83
30.13

27.98
28.20

32.87
29.90

27.95
28.10

28.05
29.59

27.36
27.16

-0.59
-0.94

Table 3. Hyperparameters of AutoLoss-AR used in this paper.

Hyperparameter

Value

Number of generations
Maximum depth of a tree
Population size
Tournament size
Crossover rate
Mutation rate

50
25
100
3
0.5
0.3

testing, i.e., searching for losses directly on these models us-
ing AutoLoss-AR. The other 7 models are used for black-box
testing, i.e., their robustness is evaluated using the previously
searched functions.

Both searching for surrogate losses and using the searched
losses for adversarial robustness evaluation involve the selec-
tion of an algorithm 𝑚. In the search process of AutoLoss-AR,
considering the time consumption, we choose 10-step PGD
(PGD-10) [31] as the algorithm 𝑚. Once the search is com-
pleted, the best-searched losses are combined with 100-step
PGD (PGD-100) [31] and 100-step Auto-PGD (APGD-100) [8]
to evaluate the adversarial robustness of a model.

To provide a comparison, CE loss [11, 16, 31], CW loss [6],
and DLR loss [8] are selected as the handcrafted baselines.
Carlini and Wagner [6] tested 7 different losses, and here
we choose only the one with the best performance. Since

our goal is to find a surrogate loss suitable for adversarial
robustness evaluation, the most straightforward measure for
the performance of a loss is the approximate risk evaluated
using this function, i.e., 𝑅 ′′. However, to be consistent with
previous studies [5, 8, 16, 31], we use adversary accuracy as
the measure, which is equal to 1 − 𝑅 ′′.

AutoLoss-AR is based on tree coding and GP algorithm.
Some hyperparameters of AutoLoss-AR used in this paper
are shown in Table 3. Our experiments are implemented with
PyTorch [33] and DEAP [10], and run on 4 Tesla V100 GPUs.

4.3 White-box Results
Under given 𝑓 , 𝐷, 𝐵, and 𝜖, can AutoLoss-AR obtain a bet-
ter solution than the handcrafted losses for tightening the
approximation error? To answer this question, we use the
proposed method to find surrogate losses on 10 adversarially
trained models. The results are shown in Table 2. As we can
see, the adversarial accuracy evaluated using the searched
losses outperforms the accuracy evaluated using the base-
lines for all models. The improvements are around 0.2% to
1.6%. These results suggest that the answer to the question
is: yes, AutoLoss-AR can obtain better surrogate losses than
the manual design baselines for closer approximations of the
true adversarial risk.

Another point of interest is the search efficiency of AutoLoss-

AR. We perform 15 independent searches with the proposed
method on six models trained using CIFAR-10 and 𝐿∞-norm

, Vol. 1, No. 1, Article . Publication date: April 2022.

6

• Pengfei Xia, Ziqiang Li, and Bin Li

Table 4. Formulas of the simplified best-searched surrogate losses.

Searched Loss

BS1
BS2
BS3
BS4
BS5

Formula
exp(10 × softmax(𝑝)/max(softmax(𝑝)))
exp(− max(softmax(𝑝 + 2 × softmax(5 × 𝑝))))
softmax(− softmax(exp(𝑝) × 2 × 𝑝)) × (softmax(2 × 𝑝) + 2 × 𝑞)
(softmax(softmax(2 × 𝑝) + 𝑝 − 𝑞) − 𝑞)2
exp(− max(softmax(exp(softmax(exp(𝑝) + 𝑝) + 1) + 𝑝) + 1))

the adversarial robustness of 7 unseen adversarially trained
models, and the results are shown in Figure 5.

Some observations are summarized as follows:

• The searched losses performed better than the hand-
crafted baselines in the vast majority of cases. Of the
210 adversarial accuracies evaluated using the searched
losses, only 3 performed worse than the manual ones.
The best-discovered losses outperform the best base-
lines by 0.8% to 2.4%.

• BS5, BS3, and BS2 are the top 3 best-performing losses
among these 5 simplified functions. BS5 and BS2 have
similar formulas, as shown in Table 4, where they both
contain exp(− max(softmax(· · · ))).

• The loss functions searched by using PGD-10 as 𝑚 can
also be well suitable for PGD-100 and APGD-100.
• The loss functions searched by using 8/255 as 𝜖 can

also be well suitable for 10/255 and 12/255.

The above results and observations illustrate that the sim-
plified 5 losses can be used individually as some kind of new
knowledge. The robustness of the model evaluated using
them is in most cases more accurate than the hand-designed
baselines.

4.5 Attribution Study

If the searched surrogate losses perform better than the hand-
craft ones, then why? To answer this question, we visualize
the local loss landscapes, as shown in Figure 4. We first gen-
erate the adversarial examples, i.e., 𝑥 ′
BS of the clean
input 𝑥 by the handcraft losses and the searched losses, re-
spectively, and keep 𝑓 (𝑥 ′
BS) ≠ 𝑦. Then, we
calculate the loss values by:
ℓ (𝛼, 𝛽) = clamp

HC) = 𝑦 and 𝑓 (𝑥 ′

HC and 𝑥 ′

HC − 𝑥) + 𝛽 × (𝑥 ′

BS − 𝑥)),

(𝛼 × (𝑥 ′

(9)

0,1

where 0 ≤ 𝛼 ≤ 1 and 0 ≤ 𝛽 ≤ 1. In Figure 4, ℓ0−1(𝛼, 𝛽),
ℓCE (𝛼, 𝛽), ℓCW (𝛼, 𝛽), ℓBS3(𝛼, 𝛽), and ℓBS5(𝛼, 𝛽) are drawn.

The results indicate that there may be two reasons. The first
one is the local consistency between the searched loss and the
0-1 loss is better than the handcrafted loss. For example, as
shown in Figure 4(a), maximizing CE loss does not maximize
the 0-1 loss. The second one is the specific algorithm 𝑚, such
as PGD-100 used here, cannot guarantee to find the global

Fig. 3. Adversarial accuracy of the losses searched by AutoLoss-
AR in 15 independent runs. The green lines represent the best-
handcrafted baselines.

adversarial training, and the results are shown in Figure 3.
The run rates that can search for surrogate losses better than
the best-handcrafted baselines are 9/15, 12/15, 12/15, 12/15,
8/15, 12/15 for the six models, respectively.

4.4 Black-box Results

Can the searched loss function be used in other conditions?
This problem is of practical interest because it determines
when evaluating the adversarial robustness of a new model,
whether the search losses can be used independently as some
new knowledge without going through a time-consuming
search process. We try to figure it out in this section.

First, we simplify the searched functions in white-box set-
ting and extract 5 clear and readable formulas, as shown in
Table 4. It can be seen that these functions have some com-
mon features, i.e., they frequently use exp, max, and softmax
as the operator units. Then, these losses are used to evaluate

, Vol. 1, No. 1, Article . Publication date: April 2022.

Addepalli2021,L∞,(cid:15)=8/255Cui2021,L∞,(cid:15)=8/255Huang2020,L∞,(cid:15)=8/255Sehwag2021,L∞,(cid:15)=8/255Wu2020,L∞,(cid:15)=8/255Zhang2019,L∞,(cid:15)=8/255556065AdversarialaccuracySearchedHandcrafted.

5

,

4

,

3

,

2

,

1
=

i

e
r
e
h
w

,

0
0
1
-
D
G
P
h
t
i

w
s
s
o
l

i
S
B
e
h
t

f
o
s
t
l
u
s
e
r

e
h
t

s
i

n
m
u
l
o
c

i
S
B
D
G
P
e
h
T

.

i

g
n
tt
e
s

x
o
b
-
k
c
a
l
b
n

i

s
l
e
d
o
m
d
e
n
i
a
r
t
y

l
l
a
i
r
a
s
r
e
v
d
a

7

f
o
y
c
a
r
u
c
c
a

l
a
i
r
a
s
r
e
v
d
A

.

5

e
l

b
a
T

,
s
e
s
s
o
l

d
e
h
c
r
a
e
s
-
t
s
e
b
e
h
t
d
n
a

s
e
s
s
o
l

d
e
ft
a
r
c
d
n
a
h
-
t
s
e
b
e
h
t
n
e
e
w
t
e
b
s
e
c
n
e
r
e
ff
d
e
h
t

i

s
i

n
m
u
l
o
c

.

i

ff
D
e
h
T

.

0
0
1
-
D
G
P
A
h
t
i

w
s
s
o
l

i
S
B
e
h
t

f
o
s
t
l
u
s
e
r

e
h
t

s
i

n
m
u
l
o
c

i
S
B
D
G
P
A
e
h
T

Tightening the Approximation Error of Adversarial Risk with Auto Loss Function Search •

7

.

i

ff
D

5
S
B
D
G
P
A

4
S
B
D
G
P
A

3
S
B
D
G
P
A

2
S
B
D
G
P
A

1
S
B
D
G
P
A

5
S
B
D
G
P

4
S
B
D
G
P

3
S
B
D
G
P

2
S
B
D
G
P

1
S
B
D
G
P

R
L
D
D
G
P
A

W
C
D
G
P
A

E
C
D
G
P
A

W
C
D
G
P

E
C
D
G
P

n
a
e
l
C

s
d
o
h
t
e
M

0
8
0
-

.

0
7
1
-

.

6
2
1
-

.

3
5
1
-

.

9
8
0
-

.

0
3
2
-

.

8
7
0
-

.

2
5
1
-

.

4
4
1
-

.

8
9
1
-

.

6
9
1
-

.

4
3
1
-

.

8
0
2
-

.

8
2
1
-

.

9
8
1
-

.

7
8
0
-

.

3
1
2
-

.

7
3
2
-

.

0
4
1
-

.

2
4
1
-

.

1
3
1
-

.

0
8
9
5

.

2
1
0
5

.

7
1
5
5

.

6
8
3
5

.

1
3
7
5

.

2
6
3
4

.

3
7
3
5

.

2
9
8
4

.

6
3
8
3

.

7
6
4
4

.

2
5
3
4

.

5
5
7
4

.

4
1
3
3

.

5
9
4
4

.

4
8
8
3

.

6
5
8
2

.

5
2
4
3

.

3
4
3
3

.

1
9
7
3

.

2
6
3
2

.

7
3
6
3

.

1
8
9
5

.

7
1
0
5

.

6
1
5
5

.

8
8
3
5

.

3
7
7
5

.

2
7
3
4

.

8
8
3
5

.

3
9
8
4

.

6
6
8
3

.

7
5
4
4

.

2
4
3
4

.

3
9
7
4

.

4
3
3
3

.

2
0
5
4

.

1
9
8
3

.

2
0
9
2

.

0
4
4
3

.

6
4
3
3

.

7
0
8
3

.

3
9
3
2

.

3
3
6
3

.

2
8
9
5

.

1
4
0
5

.

3
1
5
5

.

0
8
3
5

.

3
2
7
5

.

2
7
3
4

.

2
7
3
5

.

7
9
8
4

.

6
8
8
3

.

7
6
4
4

.

6
5
3
4

.

9
5
7
4

.

3
5
3
3

.

2
9
4
4

.

6
1
9
3

.

2
3
9
2

.

8
5
4
3

.

0
7
3
3

.

2
7
7
3

.

0
1
4
2

.

7
3
6
3

.

2
8
9
5

.

7
0
0
5

.

0
2
5
5

.

1
8
3
5

.

4
5
7
5

.

1
6
3
4

.

1
8
3
5

.

9
8
8
4

.

8
3
8
3

.

9
5
4
4

.

9
4
3
4

.

4
7
7
4

.

5
2
3
3

.

7
9
4
4

.

6
8
8
3

.

9
6
8
2

.

4
2
4
3

.

4
4
3
3

.

7
9
7
3

.

3
6
3
2

.

4
3
6
3

.

8
9
9
5

.

5
6
0
5

.

7
2
5
5

.

5
1
4
5

.

5
3
7
5

.

9
0
4
4

.

3
7
3
5

.

5
2
9
4

.

4
1
9
3

.

2
8
4
4

.

4
8
3
4

.

2
6
7
4

.

5
7
3
3

.

3
1
5
4

.

1
4
9
3

.

0
5
9
2

.

2
8
4
3

.

2
9
3
3

.

9
8
7
3

.

2
4
4
2

.

3
5
6
3

.

4
8
9
5

.

7
1
0
5

.

7
1
5
5

.

3
8
3
5

.

8
4
7
5

.

7
6
3
4

.

7
7
3
5

.

0
0
9
4

.

2
6
8
3

.

9
7
4
4

.

4
6
3
4

.

3
8
7
4

.

2
3
3
3

.

9
9
4
4

.

2
0
9
3

.

9
7
8
2

.

6
3
4
3

.

9
4
3
3

.

0
2
8
3

.

6
8
3
2

.

3
4
6
3

.

2
8
9
5

.

8
1
0
5

.

2
2
5
5

.

0
9
3
5

.

0
8
7
5

.

6
7
3
4

.

2
9
3
5

.

1
0
9
4

.

0
8
8
3

.

0
6
4
4

.

4
5
3
4

.

5
0
8
4

.

6
4
3
3

.

2
1
5
4

.

4
9
8
3

.

2
1
9
2

.

5
5
4
3

.

5
6
3
3

.

2
3
8
3

.

8
9
3
2

.

5
5
6
3

.

0
9
9
5

.

2
4
0
5

.

1
2
5
5

.

1
8
3
5

.

0
3
7
5

.

3
7
3
4

.

4
7
3
5

.

2
1
9
4

.

5
0
9
3

.

6
7
4
4

.

5
6
3
4

.

9
5
7
4

.

0
6
3
3

.

3
9
4
4

.

8
2
9
3

.

4
5
9
2

.

5
6
4
3

.

0
9
3
3

.

3
0
8
3

.

7
2
4
2

.

1
5
6
3

.

0
8
9
5

.

1
1
0
5

.

0
2
5
5

.

7
8
3
5

.

4
5
7
5

.

5
6
3
4

.

2
8
3
5

.

3
0
9
4

.

0
6
8
3

.

4
6
4
4

.

3
5
3
4

.

1
8
7
4

.

8
3
3
3

.

0
0
5
4

.

5
8
8
3

.

3
8
8
2

.

0
3
4
3

.

2
5
3
3

.

8
1
8
3

.

1
7
3
2

.

3
5
6
3

.

5
9
9
5

.

4
6
0
5

.

0
3
5
5

.

3
1
4
5

.

3
4
7
5

.

7
1
4
4

.

7
7
3
5

.

5
2
9
4

.

7
2
9
3

.

4
9
4
4

.

3
8
3
4

.

1
8
7
4

.

1
8
3
3

.

8
1
5
4

.

0
6
9
3

.

5
5
9
2

.

7
9
4
3

.

8
0
4
3

.

1
0
8
3

.

2
6
4
2

.

7
6
6
3

.

7
8
0
6

.

1
1
3
5

.

8
1
7
5

.

4
0
6
5

.

5
9
8
5

.

5
1
7
4

.

7
7
4
5

.

8
8
0
5

.

3
5
2
4

.

0
7
7
4

.

8
4
6
4

.

2
1
0
5

.

0
4
7
3

.

7
5
6
4

.

3
3
1
4

.

6
9
2
3

.

4
3
8
3

.

5
1
7
3

.

3
5
0
4

.

4
1
8
2

.

5
9
7
3

.

4
6
0
6

.

4
3
2
5

.

9
3
6
5

.

3
3
5
5

.

2
1
8
5

.

5
9
5
4

.

1
5
4
5

.

0
5
0
5

.

9
0
1
4

.

5
5
6
4

.

8
3
5
4

.

9
8
8
4

.

6
8
5
3

.

0
2
6
4

.

1
8
0
4

.

8
3
1
3

.

6
8
6
3

.

0
8
5
3

.

2
1
9
3

.

4
5
6
2

.

4
6
7
3

.

5
8
1
6

.

7
7
1
5

.

0
2
7
5

.

2
9
6
5

.

4
7
1
6

.

1
9
5
4

.

8
7
6
5

.

0
0
1
5

.

0
8
9
3

.

3
3
7
4

.

2
7
6
4

.

0
0
2
5

.

2
2
5
3

.

5
2
8
4

.

3
7
0
4

.

3
4
9
2

.

7
3
6
3

.

4
1
6
3

.

9
8
1
4

.

4
0
5
2

.

1
5
9
3

.

0
6
0
6

.

0
4
2
5

.

4
5
6
5

.

8
3
5
5

.

0
3
8
5

.

6
9
5
4

.

0
5
4
5

.

1
4
0
5

.

8
2
1
4

.

9
6
6
4

.

3
5
5
4

.

5
0
9
4

.

4
9
5
3

.

0
3
6
4

.

1
0
1
4

.

9
6
1
3

.

5
0
7
3

.

9
9
5
3

.

1
2
9
3

.

5
7
6
2

.

0
5
4
5

.

7
8
1
6

.

5
8
1
5

.

5
2
7
5

.

1
0
7
5

.

1
8
1
6

.

1
0
6
4

.

8
7
6
5

.

9
1
1
5

.

8
0
0
4

.

1
4
7
4

.

2
8
6
4

.

5
3
2
5

.

8
2
5
3

.

2
3
8
4

.

7
9
0
4

.

5
6
9
2

.

2
5
6
3

.

4
1
6
3

.

0
2
2
4

.

4
2
5
2

.

1
8
7
3

.

9
6
9
8

.

3
0
7
8

.

1
1
7
8

.

4
3
5
8

.

0
5
7
8

.

4
3
3
8

.

2
5
4
8

.

]
0
2
[

9
1
0
2
s
k
c
y
r
d
n
e
H

]
2
1
[

9
1
0
2
m
o
r
t
s
g
n
E

]
7
[

9
1
0
2
n
o
m
r
a
C

]
4
4
[

9
1
0
2
g
n
a
W

]
5
4
[

0
2
0
2
g
n
o
W

]
4
5
[

0
2
0
2
g
n
a
h
Z

]
4
3
[

0
2
0
2
e
c
i
R

9
6
9
8

.

3
0
7
8

.

1
1
7
8

.

4
3
5
8

.

0
5
7
8

.

4
3
3
8

.

2
5
4
8

.

]
0
2
[

9
1
0
2
s
k
c
y
r
d
n
e
H

]
2
1
[

9
1
0
2
m
o
r
t
s
g
n
E

]
7
[

9
1
0
2
n
o
m
r
a
C

]
4
4
[

9
1
0
2
g
n
a
W

]
5
4
[

0
2
0
2
g
n
o
W

]
4
5
[

0
2
0
2
g
n
a
h
Z

]
4
3
[

0
2
0
2
e
c
i
R

5
5
2
/
0
1
=

𝜖

,

∞
𝐿

,

0
1
-
R
A
F
I
C

5
5
2
/
2
1
=

𝜖

,

∞
𝐿

,

0
1
-
R
A
F
I
C

5
5
2
/
8
=

𝜖

,

∞
𝐿

,

0
1
-
R
A
F
I
C

9
6
9
8

.

3
0
7
8

.

1
1
7
8

.

4
3
5
8

.

0
5
7
8

.

4
3
3
8

.

2
5
4
8

.

]
0
2
[

9
1
0
2
s
k
c
y
r
d
n
e
H

]
2
1
[

9
1
0
2
m
o
r
t
s
g
n
E

]
7
[

9
1
0
2
n
o
m
r
a
C

]
4
4
[

9
1
0
2
g
n
a
W

]
5
4
[

0
2
0
2
g
n
o
W

]
4
5
[

0
2
0
2
g
n
a
h
Z

]
4
3
[

0
2
0
2
e
c
i
R

.
s
e
n

i
l
e
s
a
b
d
e
ft
a
r
c
d
n
a
h
e
h
t
n
a
h
t

r
e
tt
e
b
m
r
o
f
r
e
p
s
e
s
s
o
l

d
e
h
c
r
a
e
s

e
h
t

t
a
h
t

e
t
a
c
i
d
n

i

s
e
u
l
a
v

e
v
i
t
a
g
e
n
e
h
t

e
r
e
h
w

, Vol. 1, No. 1, Article . Publication date: April 2022.

8

• Pengfei Xia, Ziqiang Li, and Bin Li

(a) CE loss, BS3 loss

(b) CW loss, BS3 loss

(c) CE loss, BS5 loss

(d) CW loss, BS5 loss

Fig. 4. Local loss landscapes on Wong2020 [45].

optimum, although the handcrafted loss has good consistency
with the 0-1 loss, such as Figure 4(b). The searched surrogate
is easier for the algorithm to find a more suitable adversarial
example. Interestingly, we find that both BS3 and BS5 are
doing their best to smoothly simulate the steep 0-1 loss (for
BS5, the part that rises from 𝛼 = 0 and 𝛽 = 0 looks like this).
Perhaps such losses are better fit with the algorithm 𝑚 and
easier to find adversarial examples.

4.6 Qualitative Results

Some qualitative examples are shown in Figure 5, where the
adversarial examples generated using BS5 loss can misled the
model and are visually similar to clean images.

5 CONCLUSION

We establish the tightening of the approximation error as an
optimization problem and solve it with AutoML. Specifically,

, Vol. 1, No. 1, Article . Publication date: April 2022.

we focus on the surrogate loss in adversarial robustness eval-
uation and propose AutoLoss-AR to find a possible solution
with tree coding and GP algorithm. The experiments on 10
adversarially trained models are conducted to verify the effec-
tiveness and efficiency of the proposed method. The results
show that the risks evaluated using the best-discovered losses
are 0.2% to 1.6% better than those evaluated using the hand-
crafted baselines. Meanwhile, 5 surrogate losses with clean
and readable formulas are distilled out. The experimental
results demonstrate that they perform well on unseen adver-
sarially trained models. Besides, we also identify two reasons
why the searched losses are better than the baselines by vi-
sualizing the local loss landscapes: (1) the local consistency
between the searched loss and the 0-1 loss is better, and (2)
the searched loss is easier to optimize to find a deceptive
adversarial example.

CElossdirection(α)0.00.51.0BS3lossdirection(β)0.00.51.0CElossCElossdirection(α)0.00.51.0BS3lossdirection(β)0.00.51.00-1lossCElossdirection(α)0.00.51.0BS3lossdirection(β)0.00.51.0BS3loss0.00.51.0CElossdirection(α)0.00.51.0BS3lossdirection(β)CEloss0.00.51.0CElossdirection(α)0.00.51.0BS3lossdirection(β)BS3loss‘0−1=0‘0−1=1‘sCWlossdirection(α)0.00.51.0BS3lossdirection(β)0.00.51.0CWlossCWlossdirection(α)0.00.51.0BS3lossdirection(β)0.00.51.00-1lossCWlossdirection(α)0.00.51.0BS3lossdirection(β)0.00.51.0BS3loss0.00.51.0CWlossdirection(α)0.00.51.0BS3lossdirection(β)CWloss0.00.51.0CWlossdirection(α)0.00.51.0BS3lossdirection(β)BS3loss‘0−1=0‘0−1=1‘sCElossdirection(α)0.00.51.0BS5lossdirection(β)0.00.51.0CElossCElossdirection(α)0.00.51.0BS5lossdirection(β)0.00.51.00-1lossCElossdirection(α)0.00.51.0BS5lossdirection(β)0.00.51.0BS5loss0.00.51.0CElossdirection(α)0.00.51.0BS5lossdirection(β)CEloss0.00.51.0CElossdirection(α)0.00.51.0BS5lossdirection(β)BS5loss‘0−1=0‘0−1=1‘sCWlossdirection(α)0.00.51.0BS5lossdirection(β)0.00.51.0CWlossCWlossdirection(α)0.00.51.0BS5lossdirection(β)0.00.51.00-1lossCWlossdirection(α)0.00.51.0BS5lossdirection(β)0.00.51.0BS5loss0.00.51.0CWlossdirection(α)0.00.51.0BS5lossdirection(β)CWloss0.00.51.0CWlossdirection(α)0.00.51.0BS5lossdirection(β)BS5loss‘0−1=0‘0−1=1‘sTightening the Approximation Error of Adversarial Risk with Auto Loss Function Search •

9

Fig. 5. Qualitative examples of adversarial examples generated with CW loss and BS5 loss.

ACKNOWLEDGMENT

The work was supported in part by the National Natural
Science Foundation of China under Grands U19B2044 and
61836011.

REFERENCES

[1] Sravanti Addepalli, Samyak Jain, Gaurang Sriramanan, and
Venkatesh Babu Radhakrishnan. 2021. Towards Achieving Adversarial
Robustness Beyond Perceptual Limits. (2021).

[2] Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani
Srivastava, and Kai-Wei Chang. 2018. Generating natural language
adversarial examples. arXiv preprint arXiv:1804.07998 (2018).

[3] Sanjeev Arora, László Babai, Jacques Stern, and Z Sweedyk. 1997. The
hardness of approximate optima in lattices, codes, and systems of linear
equations. J. Comput. System Sci. 54, 2 (1997), 317–331.

[4] Anish Athalye, Nicholas Carlini, and David Wagner. 2018. Obfuscated
gradients give a false sense of security: Circumventing defenses to
adversarial examples. In International conference on machine learning.
PMLR, 274–283.

[5] Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel,
Jonas Rauber, Dimitris Tsipras, Ian Goodfellow, Aleksander Madry, and
Alexey Kurakin. 2019. On evaluating adversarial robustness. arXiv
preprint arXiv:1902.06705 (2019).

[6] Nicholas Carlini and David Wagner. 2017. Towards evaluating the
robustness of neural networks. In 2017 ieee symposium on security and
privacy (sp). IEEE, 39–57.

[7] Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C Duchi, and
Percy S Liang. 2019. Unlabeled data improves adversarial robustness.
Advances in Neural Information Processing Systems 32 (2019).

[8] Francesco Croce and Matthias Hein. 2020. Reliable evaluation of adver-
sarial robustness with an ensemble of diverse parameter-free attacks.
In International conference on machine learning. PMLR, 2206–2216.
[9] Jiequan Cui, Shu Liu, Liwei Wang, and Jiaya Jia. 2021. Learnable
boundary guided adversarial training. In Proceedings of the IEEE/CVF
International Conference on Computer Vision. 15721–15730.

[10] François-Michel De Rainville, Félix-Antoine Fortin, Marc-André Gard-
ner, Marc Parizeau, and Christian Gagné. 2012. Deap: A python frame-
work for evolutionary algorithms. In Proceedings of the 14th annual
conference companion on Genetic and evolutionary computation. 85–92.
[11] Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin
Hu, and Jianguo Li. 2018. Boosting adversarial attacks with momentum.
In Proceedings of the IEEE conference on computer vision and pattern
recognition. 9185–9193.

[12] Logan Engstrom, Andrew Ilyas, Shibani Santurkar, and Dimitris
Tsipras. [n.d.]. Robustness (python library), 2019. URL https://github.
com/MadryLab/robustness 4, 4 ([n. d.]), 4–3.

[13] Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, and
Aleksander Madry. 2018. A rotation and a translation suffice: Fooling
cnns with simple transformations. (2018).

[14] Ross Girshick. 2015. Fast r-cnn. In Proceedings of the IEEE international

conference on computer vision. 1440–1448.

[15] David E Goldberg and Kalyanmoy Deb. 1991. A comparative analysis of
selection schemes used in genetic algorithms. In Foundations of genetic
algorithms. Vol. 1. Elsevier, 69–93.

[16] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014.
arXiv preprint

Explaining and harnessing adversarial examples.
arXiv:1412.6572 (2014).

[17] Sorin Grigorescu, Bogdan Trasnea, Tiberiu Cocias, and Gigel Macesanu.
2020. A survey of deep learning techniques for autonomous driving.
Journal of Field Robotics 37, 3 (2020), 362–386.

[18] Chuan Guo, Mayank Rana, Moustapha Cisse, and Laurens Van
Der Maaten. 2017. Countering adversarial images using input transfor-
mations. arXiv preprint arXiv:1711.00117 (2017).

[19] Xin He, Kaiyong Zhao, and Xiaowen Chu. 2021. AutoML: A Survey of
the State-of-the-Art. Knowledge-Based Systems 212 (2021), 106622.
[20] Dan Hendrycks, Kimin Lee, and Mantas Mazeika. 2019. Using pre-
training can improve model robustness and uncertainty. In International
Conference on Machine Learning. PMLR, 2712–2721.

[21] Lang Huang, Chao Zhang, and Hongyang Zhang. 2020. Self-adaptive
training: beyond empirical risk minimization. Advances in neural infor-
mation processing systems 33 (2020), 19365–19376.

[22] Connie Kou, Hwee Kuan Lee, Ee-Chien Chang, and Teck Khim Ng. 2019.
Enhancing transformation-based defenses against adversarial attacks
with a distribution classifier. In International Conference on Learning
Representations.

[23] John R Koza. 1992. Genetic programming: on the programming of com-

puters by means of natural selection. Vol. 1. MIT press.

[24] Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning multiple layers

of features from tiny images. (2009).

[25] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. 2016. Adversarial
machine learning at scale. arXiv preprint arXiv:1611.01236 (2016).
[26] Chuming Li, Xin Yuan, Chen Lin, Minghao Guo, Wei Wu, Junjie Yan,
and Wanli Ouyang. 2019. Am-lfs: Automl for loss function search.
In Proceedings of the IEEE/CVF International Conference on Computer
Vision. 8410–8419.

[27] Hao Li, Tianwen Fu, Jifeng Dai, Hongsheng Li, Gao Huang, and Xizhou
Zhu. 2021. AutoLoss-Zero: Searching Loss Functions from Scratch for
Generic Tasks. arXiv preprint arXiv:2103.14026 (2021).

, Vol. 1, No. 1, Article . Publication date: April 2022.

dogfrogbirdcatautomobiletruckdeerairplaneshiphorsedogfrogbirdcatautomobiletruckdeerairplaneshiphorsehorsetruckcatbirdtruckshipdogautomobileautomobiletruckCleanCW lossBS5 loss[46] Dongxian Wu, Shu-Tao Xia, and Yisen Wang. 2020. Adversarial weight
perturbation helps robust generalization. Advances in Neural Informa-
tion Processing Systems 33 (2020), 2958–2969.

[47] Pengfei Xia and Bin Li. 2021.

Improving resistance to adversarial
deformations by regularizing gradients. Neurocomputing 455 (2021),
38–46.

[48] Pengfei Xia, Ziqiang Li, Hongjing Niu, and Bin Li. 2021. Understand-
ing the error in evaluating adversarial robustness. arXiv preprint
arXiv:2101.02325 (2021).

[49] Pengfei Xia, Hongjing Niu, Ziqiang Li, and Bin Li. 2021. On the re-
ceptive field misalignment in CAM-based visual explanations. Pattern
Recognition Letters 152 (2021), 275–282.

[50] Chaowei Xiao, Jun-Yan Zhu, Bo Li, Warren He, Mingyan Liu, and Dawn
Song. 2018. Spatially transformed adversarial examples. arXiv preprint
arXiv:1801.02612 (2018).

[51] Weilin Xu, David Evans, and Yanjun Qi. 2017. Feature squeezing:
Detecting adversarial examples in deep neural networks. arXiv preprint
arXiv:1704.01155 (2017).

[52] Dinghuai Zhang, Tianyuan Zhang, Yiping Lu, Zhanxing Zhu, and Bin
Dong. 2019. You only propagate once: Accelerating adversarial training
via maximal principle. arXiv preprint arXiv:1905.00877 (2019).

[53] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent
El Ghaoui, and Michael Jordan. 2019. Theoretically principled trade-
off between robustness and accuracy. In International Conference on
Machine Learning. PMLR, 7472–7482.

[54] Jingfeng Zhang, Xilie Xu, Bo Han, Gang Niu, Lizhen Cui, Masashi
Sugiyama, and Mohan Kankanhalli. 2020. Attacks which do not kill
training make adversarial learning stronger. In International conference
on machine learning. PMLR, 11278–11287.

[55] Barret Zoph and Quoc V Le. 2016. Neural architecture search with
reinforcement learning. arXiv preprint arXiv:1611.01578 (2016).

10

• Pengfei Xia, Ziqiang Li, and Bin Li

[28] Hao Li, Chenxin Tao, Xizhou Zhu, Xiaogang Wang, Gao Huang, and
Jifeng Dai. 2020. Auto Seg-Loss: Searching Metric Surrogates for Se-
mantic Segmentation. arXiv preprint arXiv:2010.07930 (2020).

[29] Peidong Liu, Gengwei Zhang, Bochao Wang, Hang Xu, Xiaodan Liang,
Yong Jiang, and Zhenguo Li. 2021. Loss Function Discovery for Object
Detection via Convergence-Simulation Driven Search. arXiv preprint
arXiv:2102.04700 (2021).

[30] Jonathan Long, Evan Shelhamer, and Trevor Darrell. 2015. Fully con-
volutional networks for semantic segmentation. In Proceedings of the
IEEE conference on computer vision and pattern recognition. 3431–3440.
[31] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris
Tsipras, and Adrian Vladu. 2017. Towards deep learning models resis-
tant to adversarial attacks. arXiv preprint arXiv:1706.06083 (2017).
[32] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha,
Z Berkay Celik, and Ananthram Swami. 2017. Practical black-box
attacks against machine learning. In Proceedings of the 2017 ACM on
Asia conference on computer and communications security. 506–519.

[33] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward
Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga,
and Adam Lerer. 2017. Automatic differentiation in pytorch. (2017).

[34] Leslie Rice, Eric Wong, and Zico Kolter. 2020. Overfitting in adver-
sarially robust deep learning. In International Conference on Machine
Learning. PMLR, 8093–8104.

[35] Andrew Ross and Finale Doshi-Velez. 2018. Improving the adversarial
robustness and interpretability of deep neural networks by regularizing
their input gradients. In Proceedings of the AAAI Conference on Artificial
Intelligence, Vol. 32.

[36] Vikash Sehwag, Saeed Mahloujifar, Tinashe Handina, Sihui Dai, Chong
Xiang, Mung Chiang, and Prateek Mittal. 2021. Improving adversarial
robustness using proxy distributions. arXiv preprint arXiv:2104.09425
(2021).

[37] Ali Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John Dickerson,
Christoph Studer, Larry S Davis, Gavin Taylor, and Tom Goldstein.
2019. Adversarial training for free! arXiv preprint arXiv:1904.12843
(2019).

[38] Jiawei Su, Danilo Vasconcellos Vargas, and Kouichi Sakurai. 2019. One
pixel attack for fooling deep neural networks. IEEE Transactions on
Evolutionary Computation 23, 5 (2019), 828–841.

[39] Kalaivani Sundararajan and Damon L Woodard. 2018. Deep learning
for biometrics: A survey. ACM Computing Surveys (CSUR) 51, 3 (2018),
1–34.

[40] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Du-
mitru Erhan, Ian Goodfellow, and Rob Fergus. 2013. Intriguing proper-
ties of neural networks. arXiv preprint arXiv:1312.6199 (2013).

[41] Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander
Madry. 2020. On adaptive attacks to adversarial example defenses.
arXiv preprint arXiv:2002.08347 (2020).

[42] Jonathan Uesato, Brendan O’donoghue, Pushmeet Kohli, and Aaron
Oord. 2018. Adversarial risk and the dangers of evaluating against
weak attacks. In International Conference on Machine Learning. PMLR,
5025–5034.

[43] Xiaobo Wang, Shuo Wang, Cheng Chi, Shifeng Zhang, and Tao Mei.
2020. Loss function search for face recognition. In International Con-
ference on Machine Learning. PMLR, 10029–10038.

[44] Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and
Quanquan Gu. 2019. Improving adversarial robustness requires revis-
iting misclassified examples. In International Conference on Learning
Representations.

[45] Eric Wong, Leslie Rice, and J Zico Kolter. 2020. Fast is better than free:
Revisiting adversarial training. arXiv preprint arXiv:2001.03994 (2020).

, Vol. 1, No. 1, Article . Publication date: April 2022.

