Learned Monocular Depth Priors in
Visual-Inertial Initialization

Yunwen Zhou, Abhishek Kar, Eric Turner, Adarsh Kowdle, Chao X. Guo,
Ryan C. DuToit, and Konstantine Tsotsos

Google AR
{verse,abhiskar,elturner,adarshkowdle,chaoguo,rdutoit,ktsotsos}@google.com

Abstract. Visual-inertial odometry (VIO) is the pose estimation back-
bone for most AR/VR and autonomous robotic systems today, in both
academia and industry. However, these systems are highly sensitive to
the initialization of key parameters such as sensor biases, gravity direc-
tion, and metric scale. In practical scenarios where high-parallax or vari-
able acceleration assumptions are rarely met (e.g. hovering aerial robot,
smartphone AR user not gesticulating with phone), classical visual-
inertial initialization formulations often become ill-conditioned and/or
fail to meaningfully converge. In this paper we target visual-inertial ini-
tialization speciﬁcally for these low-excitation scenarios critical to in-
the-wild usage. We propose to circumvent the limitations of classical
visual-inertial structure-from-motion (SfM) initialization by incorporat-
ing a new learning-based measurement as a higher-level input. We lever-
age learned monocular depth images (mono-depth) to constrain the rel-
ative depth of features, and upgrade the mono-depths to metric scale
by jointly optimizing for their scales and shifts. Our experiments show a
signiﬁcant improvement in problem conditioning compared to a classical
formulation for visual-inertial initialization, and demonstrate signiﬁcant
accuracy and robustness improvements relative to the state-of-the-art
on public benchmarks, particularly under low-excitation scenarios. We
further extend this improvement to implementation within an existing
odometry system to illustrate the impact of our improved initialization
method on resulting tracking trajectories.

Keywords: Visual-inertial
inertial structure from motion

initialization, Monocular depth, Visual-

1

Introduction

Monocular visual-inertial odometry (VIO) enables accurate tracking of metric
3D position and orientation (pose) using just a monocular camera and iner-
tial measurement unit (IMU) providing linear acceleration and rotational ve-
locity. These techniques have unlocked an economical and near-ubiquitous solu-
tion for powering complex scene understanding in augmented or virtual reality
(AR/VR) experiences (e.g. [15]) on commodity platforms (e.g, Google’s AR-
Core and Apple’s ARKit), alongside other robotic applications such as aerial

2

Y. Zhou et al.

(a) First Row: Intensity image inputs. Second Row: Mono-depth images. Third Row: Metric-
depth images, recovered after joint motion, scale, and shift optimization. Stable metric-depth is
recovered after the optimization from initial inconsistent and inaccurate mono-depth. Green Tracks
on First Row: Inlier feature-tracks for mono depth constraints. Red Tracks on First Row: Outlier
feature-tracks due to temporally inconsistent associated mono-depth values (see Sec. 3.3)

(b) Left: Initialization trajectory under a limited motion scenario in meters. Trajectory recovery is
improved with tight coupling between VI-SFM and mono-depth (note incorrect scale in blue trajec-
tory). Right: Mono-depth coupling improves problem conditioning, potentially reducing uncertainty
of estimates and increasing accuracy.
Fig. 1: At top, demonstration of depth constraints over an initialization window. At
bottom, demonstration of trajectories estimated with and without mono depth on the
sequence shown at top, illustration of feature position uncertainty.

delivery drones. A precondition of successful operation in these scenarios is suc-
cessful (and accurate) initialization of key system parameters such as scale, ini-
tial velocity, accelerometer and gyro biases, and initial gravity direction. Poor
initialization typically leads to tracking divergence, unacceptable transients, low-
accuracy operation, or outright failures, especially of downstream modules (e.g.
drone navigation software). Unfortunately, visual-inertial initialization routines
have a very common failure mode in these realistic scenarios: insuﬃcient motion
for the system’s motion and calibration states to be unambiguously resolvable
[29,33,35,47,60]. This occurs, for example, if the user of a phone-based AR game
moves with very little parallax relative to the visible scene or when a drone must
initialize while hovering. These are extremely common in practice. To improve
VIO initialization in these scenarios on commodity hardware we must optimize
for the total (user-visible) latency to initialization and accuracy of the resulting
trajectories, while not violating real-time operation. For example, a phone-based
AR user may expect a responsive (< 500ms) startup of their game, regardless of

Learned Monocular Depth Priors in Visual-Inertial Initialization

3

how they moved their phone, and without taking noticeable compute resources
from the primary AR application.

Due to its impact, many recent works have focused on formulating fast and
accurate initialization algorithms for robust monocular VIO [8, 34, 43, 47, 50, 62].
These works rely on sparse visual feature tracks to constrain relative pose (up
to scale) in the visual-inertial structure-from-motion (VI-SFM) problem. Under
low parallax initialization scenarios, any classical depth estimation approach for
these features in the VI-SFM problem will be susceptible to large uncertainty,
such as in the sequence in Fig. 1a. This uncertainty (illustrated in Fig. 1b) makes
the overall system ill-conditioned, often resulting in poor or failed initializations.
This ambiguity is exacerbated if the inertial measurements lack enough variation
to reliably recover metric scale [47].

Inspired by the robustness achievements of depth-enabled visual SLAM sys-
tems [13, 17, 48, 59] and recent advances in generalized learning-based monoc-
ular depth (mono-depth) [51, 52], we propose a novel formulation of monocu-
lar VIO initialization. We incorporate depth measurements from a mono-depth
model directly into a classical VI-SFM framework as measurements. Our pro-
posed method operates in real-time on a mobile phone and is able to accurately
initialize in traditionally challenging low parallax or limited acceleration scenar-
ios, without requiring an additional dedicated sensor for estimating depth (e.g.
LiDAR, Time-of-Flight). Our primary contributions are:

– We apply learned monocular depth priors for VIO initialization. To the best
of our knowledge, we are the ﬁrst to leverage the power of learned depth for
this problem through coupling with classical methods.

– We propose a novel residual function which tightly couples scale and shift
invariant monocular depth measurements within a traditional VI-SFM for-
mulation.

– We propose a gradient-based residual weighting function and an outlier re-

jection module to eﬀectively deal with noisy depth predictions.

– We demonstrate robust and accurate initialization relative to the state-of-
the-art on public benchmarks when embedded within an existing tracking
system, particularly under low-excitation scenarios (i.e. when accelerometer
readings or velocity do not signiﬁcantly change across the initialization win-
dow). We achieve all of the above while maintaining real-time performance
on 10Hz image streams on resource constrained devices.

2 Related Work

Visual-inertial odometry [31, 53] is a well-studied problem in both the computer
vision and robotics communities and many works [6, 18, 20, 30, 39, 44, 49, 56, 57]
have focused speciﬁcally on accurate initial estimation of states required by the
inertial sensor. These works can be roughly classiﬁed into two categories - 1)
jointly solving a visual-inertial SFM problem directly in closed form or as a
bundle adjustment problem [7, 43, 47] and 2) cascaded approaches which solve

4

Y. Zhou et al.

a pure visual SFM for up to scale pose followed by metric scale recovery using
inertial observations [8,42,50,62]. Both approaches typically use a visual-inertial
bundle adjustment (VI-BA) step to further reﬁne their solution.

Feature-based visual odometry (VO) plays a key role in VIO initialization but
often exhibits large uncertainty in low parallax and motion scenarios. Addition-
ally, the VO prior requires enough non-zero inertial measurements for observing
metric scale [47] to initialize VIO. A recent state-of-the-art method [8] (used as
the initialization routine for the popular ORBSLAM3 system [6]) still requires
around 2 seconds (at 10Hz) to initialize and only succeeds with reasonable mo-
tion excitation. Our proposed method aims to initialize with lower (user-visible)
latency (i.e. less data collection time) even in challenging low-motion scenar-
ios. Some prior works have explored using higher order visual information such
as lines [42] for increased system observability in monocular VIO. Additionally,
RGB-D SLAM systems [13, 17, 48] have been tremendously successful in a num-
ber of domains (AR/VR, self driving cars, etc.) and can inherently initialize
faster given direct metric depth observations. For example, [25] demonstrated
that the inclusion of a depth sensor signiﬁcantly reduces the required number
of feature observations. However, in spite of their advantages, depth sensors can
signiﬁcantly increase the cost and/or complexity of a device. Our work is focused
on improving VIO initialization for commodity devices equipped with only an
IMU and single camera.

With the advent of deep learning, there has been signiﬁcant interest in end-
to-end learning for VIO [2,10,12,26,41,58]. However, the proposed methods often
lack the explainability and modular nature of traditional VIO systems, have al-
ternative end-goals (e.g. self supervised depth/optical ﬂow/camera pose estima-
tion), or are too expensive to operate on commodity hardware without custom
accelerators. Moreover, end-to-end methods don’t explicitly consider in-motion
initialization and often benchmark on datasets with the trajectory starting at
stationary point
[5, 22]. Prior works have also explored learning methods in
purely inertial [9,28,46] or visual systems [4,36,54]. CodeVIO [61] demonstrated
that incorporating a diﬀerentiable depth decoder into an existing VIO system
(OpenVINS) [23] can improve tracking odometry accuracy. Note that CodeVIO
does not tackle the VIO initialization problem and relies on tracking landmarks
from already-initialized VIO. It uses the OpenVINS initialization solution which
only initializes after observing enough IMU excitation following a static period.
However, CodeVIO does demonstrate an eﬀective and modular integration of
learned priors within VIO and inspires us to deliver similar improvements to
VIO initialization, while operating under realtime performance constraints.

3 Methodology

Our proposed system is composed of two modules as shown in Fig. 2: 1) monoc-
ular depth inference which infers (relative) depth from each RGB keyframe,
and 2) a VIO initialization module which forms a visual-inertial structure-from-
motion (VI-SFM) problem, with the relative depth constraints from the inferred

Learned Monocular Depth Priors in Visual-Inertial Initialization

5

Fig. 2: Overall initialization diagram composed of monocular depth inference module
running on each keyframe, and the visual-inertial bundle adjustment module. Initialized
states are then fed into our VIO for tracking.

monocular depth. This VI-SFM problem aims to estimate keyframe poses, ve-
locity, and calibration states, which are then used as the initial condition for a
full VIO system.

Like most VIO initialization algorithms [7, 8, 62], our VIO initialization con-
sists of a closed-form solver, whose solution is then reﬁned with visual-inertial
bundle adjustment (VI-BA). In this section, we ﬁrst brieﬂy describe our mono-
depth model. Then, we detail our contribution on employing mono-depth con-
straints in VI-BA reﬁnement.

3.1 Light-weight Monocular Depth Model

Our key contribution in this work is to incorporate prior-driven monocular depth
constraints within a classical VIO initialization framework for better tracking ini-
tialization. For the ﬁnal system to be practical, we require the mono-depth model
to generalize to a wide variety of scenes and operate under a small compute bud-
get. We follow recent state-of-the-art monocular depth estimation models [52]
and train a lightweight mono-depth network. Speciﬁcally, we use the robust
scale-shift invariant loss [52] alongside various edge-sensitive depth losses [45,52]
and train a small UNet model on a variety of datasets including ScanNet [14],
MannequinChallenge [45] as well as pseudo-ground truth disparity maps gener-
ated on the OpenImages [37] dataset using large pretrained publicly available
models [52]. For datasets with metric depth ground truth (e.g. ScanNet), we
also add a loose metric depth loss term (Charbonnier loss [3] between prediction
and inverse metric depth) to inform the scale and shift priors in Eq. (5). We
trained our model on gravity-aligned (or “upright”) images to avoid having it
learn depth maps for “sideways” images and better use its limited model ca-
pacity. Our ﬁnal model is fast (Tab. 4), light-weight (∼ 600K parameters) and
predicts relative (inverse) depth maps as shown in Fig. 1a.

Given the scale-shift invariant nature of our training losses, the metric in-
verse depth, z, can be expressed as a scaled and shifted version of the model
prediction, d, as z = ad + b, where a and b are the scale and shift parameters
respectively. Moreover, as our model is trained on gravity aligned (“upright”)
images, we rotate the input image in 90-degree increments before inferring depth.

6

Y. Zhou et al.

Since only 45-degree accuracy is required to get the best rotation, for simplicity
we use accelerometer measurements rotated through pre-calibrated IMU-camera
extrinsics as an estimate of gravity in the camera frame.

3.2 VI-BA with Monocular Depth Constraints

We aim to solve for the following state parameters, X , in our VI-BA problem

X = [X0; . . . ; XN −1; Cj f0; . . . ; Cj fM −1; S0; . . . ; SN −1]

(1)

where

– Xk represents the kth IMU keyframe state among N keyframes in total,
which is [qk; pk; vk; ba
k ]. qk and pk are the kth IMU keyframe pose pa-
rameterized as quarternion and translation w.r.t the global frame {G} in
which we assume the direction of gravity is known. vk is the velocity in {G}
and ba

k are the accelerometer and gyro biases at the kth keyframes.

k, bω

k; bω

– Cj fi represents the ith feature point parameterized in local inverse depth
[uij, vij, wij]T with respect to the jth keyframe’s camera coordinates. uij
and vij lie on normalized image XY plane and wij is the inverse depth [11].
– Sk = [ak; bk] following Sec. 3.1, which are scale and shift for recovering

metric depth from the raw mono-depth at the kth keyframe.

– The IMU-camera extrinsics (qC, pC) and 3D-2D projection parameters
P roj(·) are not estimated due to lack of information in such a small ini-
tialization window. We adopt pre-calibrated values as is customary.

We initialize the state X using a standard closed-form solver [43] for a VI-
SFM problem formulated with reprojection error. Its formulation and derivation
are presented in the supplemental material. Given keyframes K, with up to scale
and shift mono inverse depth, feature points F, and L(⊂ F) feature points with
mono inverse depth measurements, the VI-BA minimizes the following objective
function:

ˆX = argmin

(cid:88)

(cid:107)rIij (cid:107)2

Σij

+

(cid:88)

(cid:88)

ρ((cid:107)rFik(cid:107)2

ΣF

)

X

(cid:88)

+

(i,j)∈K
(cid:124)
(cid:125)
(cid:123)(cid:122)
Inertial Constraints
(cid:88)
λikρ((cid:107)rLik (cid:107)2)

k∈K

i∈F
(cid:124)

(cid:123)(cid:122)
Visual Constraints

(cid:125)

+ (cid:107)r0(cid:107)2
Σ0

+

(cid:88)

(cid:107)rSi (cid:107)2
ΣS

(2)

k∈K

i∈L
(cid:124)
(cid:125)
(cid:123)(cid:122)
Mono-Depth Constraints

(cid:124)

i∈K
(cid:123)(cid:122)
Prior Constraints

(cid:125)

where rIij is the IMU preintegration residual error [19] corresponding to IMU
measurements between two consecutive keyframes, rFik is the standard visual
reprojection residual resulting from subtracting a feature-point’s pixel measure-
ment from the projection of fi into the kth keyframe [27], rLik is an inverse
depth temporal consistency residual for incorporating mono-depth, and rSi is
a residual relative to a prior for scale and shift (Sec. 3.3). r0 is a prior for the

Learned Monocular Depth Priors in Visual-Inertial Initialization

7

Fig. 3: A factor graph illustration of the VI-SFM depth reﬁnement problem Eq. (2).
Circled nodes represent X in Eq. (1) to be estimated. They are connected by constraints
illustrated in the graph. The pink dashed box is the traditional VI-SFM problem.
The green dashed box represents the new proposed constraints to maintain relative
feature depth consistency across keyframes. Feature points and poses are constrained
through the scale-shift parameters S.

bias estimates of the 0th keyframe and Σ0, Σij, ΣF , ΣS are the corresponding
measurement covariance matrices. λik is a scalar weight for each depth residual
and ρ(.) refers the huber-loss function [32].

The factor graph resulting from (2) is illustrated in Fig. 3. (rIij , rFik , r0)
forms the traditional VI-SFM problem as highlighted in the pink dashed box.
The following sections detail the proposed depth constraints (rLik , rSi) which
are grouped by green dashed box.

3.3 Weighted Mono-Depth Constraints

As illustrated in Fig. 3, depth constraints relate observed feature-point depth
with that keyframe’s scale-shift parameters, Sk. Hence only 2 additional pa-
rameters are needed to model the hundreds of mono-depth residual equations
for each keyframe-landmark pair. As demonstrated in Sec. 4, this improves the
system conditioning under motion restricted scenarios.

The depth constraints comprise three major components - the residual
function, the weight for each residual and the outlier rejection module to
reject inconsistent mono-depth measurements across keyframes.

Inverse Depth Residual Function. Inspired by the loss functions em-
ployed in monocular deep depth estimation [16], our proposed depth residual for
keyframe k and feature point i takes the form of the log of the ratio between the
measured depth scaled/shifted by Sk and the feature point’s estimated depth:
rLik = log (cid:0)(akdik + bk) · Ω(Cjfi, qj, pj, qk, pk

(3)

(cid:1))

Where Ω(·) is the depth of the feature point i (which is parameterized with
respect to keyframe j) in keyframe k. If k = j then Ω(·) can be simpliﬁed to

8

Y. Zhou et al.

w−1
ij . This is how we tie mono-depth parameters to multiple features and poses
to better constrain the problem. The derivation details for Ω(·) are presented in
supplemental material.

It is well known that this residual can lead to a degenerate solution of scale
going to zero or a negative value [21]. To avoid this, we adopt the common
technique of deﬁning the scale parameter ak as

ak = ε + log(esk + 1)

(4)

where ε = 10−5, which prevents ak from being either negative or zero, allowing
us to optimize sk freely.

Scale-shift Prior. Reiterating Sec. 3.1, the ML model is trained on certain
metric depth datasets with a loss where the scale is supposed to be 1 and shift
is 0. We deﬁne prior residuals for scale and shift at the ith frame as

(cid:3)T

rSi = (cid:2)1 − ai −bi
Since metric depth is not observable from the ML model, in practice we assign
a very large covariance ΣS to these scale-shift priors terms (0.3 for scale, 0.2 for
shift), which keeps parameters bounded to the regime in which model training
occurred, and in degenerate situations such as zero-acceleration, allows us to
converge to a sensible scale.

(5)

Fig. 1a shows the eﬀectiveness of the depth constraints and scale-shift priors.
With them, we are able to upgrade the learned depth to metric level. The better-
conditioned problem then yields a more accurate trajectory, illustrated in Fig. 1b.
Edge Awareness Weight. The ML model doesn’t explicitly yield predic-
tion uncertainty, however, we empirically observe the uncertainty is larger near
depth edges and propose a loss weight, λik, which modulates the residual with
gradients of image Ik and depth Dk as follows

λik = e−(α|∇2Φ(Ik(uik,vik))|+|∇2Φ(Dk(uik,vik))|)

(6)

where ∇2 is the laplacian operator, Φ(·) is a bilateral ﬁlter for sharpening image
and depth edges, α is a hyperparameter for relative weighting of image/depth
gradients and (uik, vik) is the pixel location of the feature point in keyframe
k. This weight diminishes the eﬀect of depth constraints on feature points near
image/depth edges and favors non-edge regions where the depth and image gra-
dients are in agreement.

Outlier Rejection for Depth Measurements. The weighting function
Eq. (6) helps mitigate eﬀects of erroneous mono-depth measurements at a given
keyframe, but cannot reconcile inconsistency in depth measurements across
keyframes. For a short initialization window (< 2s), keyframe images tend not
to vary drastically. Given this, we expect the mono-depth output to not vary
signiﬁcantly as well (even though they are up to an unknown scale and shift).
For example, if the mono-depth model predicts a feature point to have small
depth w.r.t the rest of the scene in one keyframe but large depth in another,
the mono-depth residuals for this given feature are likely to be unreliable and
should not be included in the ﬁnal optimization.

Learned Monocular Depth Priors in Visual-Inertial Initialization

9

Algorithm 1 Outlier Depth Measurements Rejection

Input: Mono-depth residuals rLik, i ∈ L, k ∈ K; thresholds

σmin, σmax

Output: Set of inlier mono-depth residuals
1: σL ← {}
2: for i ∈ L do
3:
4: end for
5: if percentile(σL, 25) > σmax then

Append σi =

k(rik−ˆri)
N −1

to σL

(cid:113) (cid:80)

return {}

6: else if percentile(σL, 85) < σmin then

return {rLik, ∀i ∈ L, ∀k ∈ K}

7: else

return {rLik|σi < percentile(σL, 85)}

8: end if

Thus, we devise an outlier-rejection scheme detailed in Algorithm 1. This
algorithm ﬁrst evaluates the standard deviations of residuals involving a given
feature point, σL = {σi, ∀i ∈ L}. Then depending on the distribution of σL we
choose the inlier set. (i) If the 25th percentile of σL is larger than a maximum
threshold, we reject all mono-depth constraints. This scenario occurs when the
ML inference is highly unstable and typically does not yeild useful constraints.
(ii) When mono-depth constraints are generally self-consistent (the 85th per-
centile of σL is smaller than a minimum threshold) we accept all mono-depth
constraints. (iii) In all other cases, we reject residuals corresponding to σi in
upper 15th percentile of σL, removing the least self-consistent constraints. Such
a scenario is depicted in Fig. 1a, where the mono-depth residuals involving red
feature tracks are rejected.

In practice, we require an up-to-scale accurate estimate of camera pose and
feature position to evaluate rLik for input to Algorithm 1. Therefore, we ﬁrst
solve the VI-BA without mono-depth (i.e., the pink rectangle portion of Fig. 3).
Finally after convergence of the depth-less cost-function, we add the depth con-
straints as detailed in this section, and solve Eq. (2).

4 Experiments

We perform two sets of experiments on the popular EuRoC dataset [5], contain-
ing visual and inertial data from a micro air vehicle (MAV) along with accurate
motion ground truth. To generate reliable correspondences for visual and mono-
depth constraints, our front-end uses gyro measurements as a prior for frame-to-
frame rotations following 2-pt RANSAC [55]. We ﬁrst exhaustively evaluate VIO
initialization performance on the whole trajectory by running our initialization
routine in windows sampled throughout each trajectory in the dataset, which is
commonly done in a variety initialization works [8, 42, 62]. Additionally, we also

10

Y. Zhou et al.

evaluate the eﬀect of initialization on tracking performance by employing our
method on a baseline similar to OpenVINS [23] in 10s time windows distributed
uniformly across datasets. In both cases, we compare against ground truth poses
captured by a VICON system present in the dataset.

4.1 Exhaustive Initialization Evaluation

Following prior related initialization works [8, 42, 62], we exhaustively create
VIO initialization events across the whole trajectory to evaluate performance
across diﬀerent motion and visual scenarios. For a fair comparison, we split
each dataset into segments evenly and attempt to initialize all methods on the
same set of segments. We collect poses from all successful initializations for the
evaluation, though note: not all trials are successful due to internal validation
steps of the respective algorithms and success does not necessarily mean that the
initialization poses are qualiﬁed for tracking. Accuracy may be poor (measured
by scale error or RMSE), in which case tracking may diverge.

Our baseline method consists of a closed-form initialization [43] followed by
VI-BA [40] with only the VI-SFM portion of residuals present (pink rectangle
in Fig. 3). We also compare against the state-of-the-art VI-initialization method
Inertial-only [8], implementation of which is obtained from the open-sourced
SLAM method [6]. Given N keyframes, Inertial-only uses up-to-scale visual
odometry as the prior in a MAP framework to recover the metric scale, gravity
vector, and IMU biases, followed by a VI-BA reﬁnement step. Inertial-only’s
visual front-end performs RANSAC with PnP [38].

We conﬁgured all three methods to operate on 10Hz image streams following
previous works [8, 50, 62]. We treat each image as a keyframe and use either 5 or
10 keyframes (KFs) for initialization. In the 5KFs setting, we split datasets into
0.8s initialization windows evenly. For practical applications, faster initialization
is preferred. So we speciﬁcally highlight a 5KFs experiment to further exacerbate
issues of insuﬃcient baseline/motion, which are commonplace in deployment
scenarios (e.g. MAVs, AR/VR). Other detailed experimental results for 10KFs
under 10Hz/4Hz settings (also studied in [8]) are presented in the supplemental
material.

We were able to generate 1078, 1545, 1547, initialization trajectories re-
spectively for Inertial-only, baseline, and our proposed method over all EuRoC
datasets from 1680 initialization attempts. The average initialization trajectory
latency for the three methods were 0.592s, 0.399s, and 0.399s respectively. For
our 10KFs setting, we split datasets into 1.6s windows. We generated 571, 809,
815 initialization trajectories for the three methods with an average trajectory
latency of 1.367, 0.897 and 0.897 from 839 initialization attempts. Since Inertial-
only uses visual odometry as the prior, to better align with the resulting expecta-
tions across diﬀerent methods, we rejected those trajectories with poor resulting
reprojection error of each visual constraint for the baseline and our proposed
method. We observed that Inertial-only had longer initialization latency and
typically led to fewer successful initializations because it requires mean trajec-
tory acceleration larger than 0.5% of gravity ( ¯||a|| > 0.005G) as stated in [8].

Learned Monocular Depth Priors in Visual-Inertial Initialization

11

To measure trajectory accuracy, we perform a Sim(3) alignment against the
ground truth trajectory to get scale error and position RMSE for each initializa-
tion. Since the global frames of the IMU sensor should be gravity-aligned, the
gravity RMSE (in degrees) is computed from the global z axis angular devia-
tion in the IMU frame. Following past work [8], we omit scale errors when the
mean trajectory acceleration ¯||a|| < 0.005G, however gravity and position RMSE
are still reported. Finally, we also empirically compute the condition number of
the problem hessian in the most challenging of sequences (mean acceleration
¯||a|| < 0.005G) to evaluate problem conditioning with the added mono-depth
constraints. We present our aggregated results for the 5KFs setting in Tab. 1.
We signiﬁcantly outperform state-of-the-art Inertial-only in all metrics, achieving
on average a 43% reduction in scale error, 61% reduction in position RMSE, and
21% reduction in gravity RMSE for the challenging 5KF setting at an initial-
ization latency of 0.4s. Furthermore, our formulation leads to a lower condition
number compared to the baseline, indicating improved problem conditioning.

Table 1: Exhaustive initialization benchmark results per dataset from Inertial-only,
our baseline, and our proposed method using 5 KFs with 10Hz image data. For each
metric, lower is better.

Scale Error (%)
¯||a|| > 0.005G

Position RMSE
(meters)

Gravity RMSE
(degrees)

log(Condition Num)
¯||a|| < 0.005G

Dataset Inertial-only Baseline Ours Inertial-only Baseline Ours Inertial-only Baseline Ours Baseline

Ours

mh 01
mh 02
mh 03
mh 04
mh 05
v1 01
v1 02
v1 03
v2 01
v2 02
v2 03

Mean

41.34
38.80
57.44
74.29
70.35
55.44
56.86
56.93
42.40
41.27
59.64

54.07

43.65
41.41
59.09
56.26
54.64
54.25
45.12
38.55
40.84
34.31
36.42

31.11
34.98
34.65
48.40
44.52
25.59
26.12
20.01
23.51
19.33
27.87

45.87

30.55

0.047
0.048
0.145
0.179
0.145
0.056
0.106
0.097
0.035
0.035
0.116

0.092

0.035
0.033
0.091
0.090
0.078
0.038
0.069
0.048
0.026
0.026
0.044

0.025
0.026
0.055
0.075
0.063
0.021
0.038
0.025
0.015
0.015
0.033

0.053

0.036

1.38
1.33
3.09
2.38
2.13
3.47
3.77
5.36
1.49
2.92
4.10

2.86

2.43
2.04
3.73
2.69
2.77
3.73
3.86
3.59
1.78
2.66
2.81

2.92

1.82
1.81
2.89
2.31
2.30
3.36
2.44
2.37
1.35
1.96
2.24

13.97
13.31
13.83
13.42
13.66
12.93
13.26
12.62
13.45
12.20
13.30

2.26

13.27

13.16
12.50
12.73
11.27
12.51
11.43
11.67
12.03
12.84
12.27
11.17

12.14

To demonstrate the importance of the scale/shift priors, edge weighting, and
outlier rejection introduced in this work, we present results of an ablation study
in Tab. 2. This study shows each component signiﬁcantly improves the overall
performance of the system.

In Fig. 4, we plot the cumulative distributions for the metrics above for both
the 10KFs (top) and 5KFs (bottom) settings. We can see that while we do
better than the baseline and Inertial-only in the 10KFs setting, the gains are
greater in the more challenging 5 KFs setting with low-excitation, highlighting
the beneﬁt of the mono-depth residuals. In order to gain insights into where
our method outperforms others, we visualize a dataset with trajectory color
coded by acceleration magnitude and scale error for the various methods in
Fig. 5. We outperform both Inertial-only and the baseline almost across the
whole trajectory but more speciﬁcally so in low acceleration regions which are

12

Y. Zhou et al.

Table 2: Aggregated exhaustive initialization benchmark ablation study of our pro-
posed method using 5 KFs with 10Hz image data for all EuRoC datasets. For each
metric, lower is better.

Metrics

Ours

Ours w/o Ours w/o

Ours w/o
Prior Weight Outlier Rejection Everything

Ours w/o

Scale Error (%) ¯||a|| > 0.005G

Position RMSE (meters)

31.23

35.47

0.036

0.041

Gravity RMSE (degrees)
2.26
log(Condition Num) ¯||a|| < 0.005G 12.14

2.53

13.24

34.22

0.041

2.46

13.23

36.59

0.039

2.46

13.18

37.55

0.044

2.57

13.49

Fig. 4: Cumulative distribution plots for primary error metrics. First row: Results
with 10 keyframes. Second row: Results with 5 keyframes. For each plot, the X axis
denotes a threshold for error metric and the Y axis shows the fraction of initializa-
tion attempts with the respective error metric smaller than the threshold on the X
axis. Note: 1) Improved gains in the 5KF (i.e. less motion) setting where mono-depth
residuals show greater impact. 2) Recall doesn’t converge to 100% due to initialization
failures among attempts.

traditionally the hardest for classical VIO initialization methods. This further
validates our hypothesis that the added mono-depth constraints condition the
system better with direct (up to scale/shift) depth measurement priors in low-
excitation scenarios, which is critical for today’s practical applications of VIO.

4.2 Visual-inertial Odometry Evaluation

To better illustrate our method’s in-the-wild applicability, we conduct experi-
ments quantifying the impact of our method when used in-the-loop with odom-
etry. Considering the additional challenge of 5KFs initialization, we focus our
experiments there instead of typical 10KFs [8] and evaluate the accuracy of ﬁ-
nal tracking trajectories. The evaluation is performed with a baseline similar
to OpenVINS [23], which is a state-of-the-art VIO system commonly used in

Learned Monocular Depth Priors in Visual-Inertial Initialization

13

Fig. 5: Acceleration and scale error visualizations for the v2 01 dataset (best viewed
in color). Left: Trajectory colored by acceleration magnitude as %G (lighter indicates
low acceleration). Right: Segments of poses colored by scale error magnitude for each
initialization window in the dataset (lighter is better). Segments colored black indicate
failed initializations for the respective methods. We outperform other methods over
the entire trajectory on scale error, especially in low acceleration regions, e.g, left side
of the plot, where our method performs signiﬁcantly better.

compute-limited use-cases (e.g, mobile AR/VR, drones). Similar to Sec. 4.1, we
create initialization events periodically but evaluate the tracking trajectories in-
stead. We split the datasets evenly into 10s segments and initialize and perform
VIO using the same 10s of information for both methods.

As in Sec. 4.1, our baseline is tracking initialized with VI-SFM only. We gen-
erated a total of 142 trajectories using our protocol over all EuRoC datasets for
each method and report aggregated position and gravity RMSE for each dataset.
The aggregated results are shown in Tab. 3 where we see an 84% improvement
in position RMSE and 46% improvement in gravity RMSE over the baseline
method. This suggests a signiﬁcant expected improvement in downstream uses
of odometry, such as rendering virtual content, depth estimation, or navigation.
Computation Cost. We ran our system on a Pixel4XL mobile phone using
only CPU cores. The computation cost (in milliseconds) for diﬀerent initializa-
tion modules is shown in Tab. 4. The closed-form initialization problem is solved
using Eigen [24] and the subsequent VI-BA is solved with the Ceres Solver [1]

Table 3: Visual-inertial odometry benchmark results over all EuRoC datasets with
and without mono-depth constraints used in initialization. VIO runs at 10Hz and is
initialized with 5KFs.

Position RMSE (m) Gravity RMSE (deg)

Dataset Baseline Ours Diﬀ(%) Baseline Ours Diﬀ(%)

mh 01
mh 02
mh 03
mh 04
mh 05
v1 01
v1 02
v1 03
v2 01
v2 02
v2 03

1.560
0.604
2.466
0.526
3.204
3.438
2.846
2.649
1.824
2.615
2.939

0.543 -65.19
0.071 -88.24
1.299 -47.32
0.124 -76.42
0.910 -71.59
0.082 -97.61
0.097 -96.59
0.059 -97.77
0.046 -97.47
0.060 -97.70
0.567 -80.70

Mean

2.243

0.351 -84.35

2.21
1.65
2.88
2.01
3.44
4.66
3.57
3.19
2.19
3.42
3.99

3.02

1.55 -29.86
1.31 -20.60
2.29 -20.48
1.01 -49.75
1.88 -45.34
2.69 -42.27
1.22 -65.82
1.28 -59.87
1.08 -50.68
1.25 -63.45
2.06 -48.37

1.61 -46.68

14

Y. Zhou et al.

Table 4: Computation duration of key modules in milliseconds.

Mono depth Closed-form Initialization VI-BA Solver (baseline) VI-BA Solver (ours)

71.64

0.73

16.2

39.8

using Levenberg–Marquardt. We run ML inference on the CPU in its own thread
and hence achieve real-time performance (within 100ms for the 10Hz conﬁgu-
ration) on a mobile phone. While we do observe that adding depth constraints
increases the computational cost of the VI-SFM problem, we still improve in
terms of overall initialization speed by producing a satisfactory solution with
only 5KFs (0.5s of data) as opposed to 10KFs typically required by the base-
line and Inertial-only.

5 Conclusion

In this paper, we introduced a novel VIO initialization method leveraging learned
monocular depth. We integrated the learned depth estimates, with alignment
parameters, into a classical VI-SFM formulation. Through the learned image
priors, our method gains signiﬁcant robustness to typical degenerate motion
conﬁgurations for VI-SFM, such as low parallax and low excitation (near-zero)
acceleration. This method only requires a lightweight ML model and additional
residuals (with associated states) to be added to a standard pipeline and does
not signiﬁcantly impact runtime, enabling application on mobile devices. Our
experiments demonstrated signiﬁcant improvements to accuracy, problem con-
ditioning, and robustness relative to the state-of-the-art, even when signiﬁcantly
reducing the number of keyframes used and exacerbating the problem of low
excitation. Our method could serve as a straightforward upgrade for most tradi-
tional pipelines. There are several key limitations and directions for future work
to call out:

– We do not claim any direct upgrades to VI system observability. While the
use of a prior on scale and shift and the training of the mono-depth network
(assuming scale and shift being 1 and 0) may provide some direct scale
information, our work’s primary contribution is to problem conditioning and
behaviour under limited motion, not zero motion.

– Mono-depth has generalization limitations due to biases in its training data,
learning scheme, and model structure. It is crucial to note that we did not
re-train our network for EuRoC. It was used oﬀ the shelf after training
on general imagery which are very diﬀerent from EuRoC. With a network
trained speciﬁcally for the problem domain (or optimized in the loop at test
time per initialization window) we expect an even greater improvement.

Acknowledgements. We thank Josh Hernandez and Maksym Dzitsiuk for

their support in developing our real-time system implementation.

Learned Monocular Depth Priors in Visual-Inertial Initialization

15

References

1. Agarwal, S., Mierle, K., Others: Ceres solver. http://ceres-solver.org
2. Almalioglu, Y., Turan, M., Sari, A.E., Saputra, M.R.U., de Gusm˜ao, P.P.B.,
Markham, A., Trigoni, N.: Selfvio: Self-supervised deep monocular visual-inertial
odometry and depth estimation. CoRR abs/1911.09968 (2019), http://arxiv.
org/abs/1911.09968

3. Barron, J.T.: A general and adaptive robust loss function. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4331–
4339 (2019)

4. Bloesch, M., Czarnowski, J., Clark, R., Leutenegger, S., Davison, A.J.:
Codeslam—learning a compact, optimisable representation for dense visual slam.
In: Proceedings of the IEEE/CVF conference on Computer Vision and Pattern
Recognition. pp. 2560–2568 (2018)

5. Burri, M., Nikolic, J., Gohl, P., Schneider, T., Rehder, J., Omari, S., Achtelik,
M.W., Siegwart, R.: The euroc micro aerial vehicle datasets. The International
Journal of Robotics Research 35(10), 1157–1163 (2016)

6. Campos, C., Elvira, R., Rodr´ıguez, J.J.G., Montiel, J.M., Tard´os, J.D.: Orb-slam3:
An accurate open-source library for visual, visual–inertial, and multimap slam.
IEEE Transactions on Robotics (2021)

7. Campos, C., Montiel, J.M.M., Tard´os, J.D.: Fast and robust initialization for
visual-inertial SLAM. CoRR abs/1908.10653 (2019), http://arxiv.org/abs/
1908.10653

8. Campos, C., Montiel, J.M., Tard´os, J.D.: Inertial-only optimization for visual-
inertial initialization. In: 2020 IEEE International Conference on Robotics and
Automation (ICRA). pp. 51–57. IEEE (2020)

9. Chen, C., Lu, X., Markham, A., Trigoni, N.: Ionet: Learning to cure the curse of
drift in inertial odometry. In: Proceedings of the AAAI Conference on Artiﬁcial
Intelligence (2018)

10. Chen, C., Rosa, S., Miao, Y., Lu, C.X., Wu, W., Markham, A., Trigoni, N.: Selective
sensor fusion for neural visual-inertial odometry. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. pp. 10542–10551 (2019)
11. Civera, J., Davison, A.J., Montiel, J.M.: Inverse depth parametrization for monoc-

ular slam. IEEE Transactions on Robotics 24(5), 932–945 (2008)

12. Clark, R., Wang, S., Wen, H., Markham, A., Trigoni, N.: Vinet: Visual-inertial
odometry as a sequence-to-sequence learning problem. In: Proceedings of the AAAI
Conference on Artiﬁcial Intelligence (2017)

13. Concha, A., Civera, J.: RGBDTAM: A cost-eﬀective and accurate RGB-D tracking
and mapping system. CoRR abs/1703.00754 (2017), http://arxiv.org/abs/
1703.00754

14. Dai, A., Chang, A.X., Savva, M., Halber, M., Funkhouser, T., Nießner, M.: Scannet:
Richly-annotated 3d reconstructions of indoor scenes. In: Proc. Computer Vision
and Pattern Recognition (CVPR), IEEE (2017)

15. Du, R., Turner, E., Dzitsiuk, M., Prasso, L., Duarte, I., Dourgarian, J., Afonso, J.,
Pascoal, J., Gladstone, J., Cruces, N., et al.: Depthlab: Real-time 3d interaction
with depth maps for mobile augmented reality. In: Proceedings of the 33rd Annual
ACM Symposium on User Interface Software and Technology. pp. 829–843 (2020)
16. Eigen, D., Puhrsch, C., Fergus, R.: Depth map prediction from a single image
using a multi-scale deep network. CoRR abs/1406.2283 (2014), http://arxiv.
org/abs/1406.2283

16

Y. Zhou et al.

17. Endres, F., Hess, J., Sturm, J., Cremers, D., Burgard, W.: 3-d mapping with an

rgb-d camera. IEEE transactions on robotics 30(1), 177–187 (2013)

18. Fei, X., Soatto, S.: Xivo: An open-source software for visual-inertial odometry.

https://github.com/ucla-vision/xivo (2019)

19. Forster, C., Carlone, L., Dellaert, F., Scaramuzza, D.: On-manifold preintegration
theory for fast and accurate visual-inertial navigation. CoRR abs/1512.02363
(2015), http://arxiv.org/abs/1512.02363

20. Forster, C., Pizzoli, M., Scaramuzza, D.: Svo: Fast semi-direct monocular visual
odometry. In: 2014 IEEE international conference on robotics and automation
(ICRA). pp. 15–22. IEEE (2014)

21. Garg, R., Wadhwa, N., Ansari, S., Barron, J.T.: Learning single camera depth
estimation using dual-pixels. CoRR abs/1904.05822 (2019), http://arxiv.org/
abs/1904.05822

22. Geiger, A., Lenz, P., Stiller, C., Urtasun, R.: Vision meets robotics: The kitti

dataset. International Journal of Robotics Research (IJRR) (2013)

23. Geneva, P., Eckenhoﬀ, K., Lee, W., Yang, Y., Huang, G.: Openvins: A research
platform for visual-inertial estimation. In: 2020 IEEE International Conference on
Robotics and Automation (ICRA). pp. 4666–4672. IEEE (2020)

24. Guennebaud, G., Jacob, B., et al.: Eigen v3. http://eigen.tuxfamily.org (2010)
25. Guo, C.X., Roumeliotis, S.I.: Imu-rgbd camera 3d pose estimation and extrinsic
calibration: Observability analysis and consistency improvement. In: 2013 IEEE
International Conference on Robotics and Automation. pp. 2935–2942 (2013).
https://doi.org/10.1109/ICRA.2013.6630984

26. Han, L., Lin, Y., Du, G., Lian, S.: Deepvio: Self-supervised deep learning of monoc-
ular visual inertial odometry using 3d geometric constraints. In: 2019 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS). pp. 6906–
6913. IEEE (2019)

27. Hartley, R.I., Zisserman, A.: Multiple View Geometry in Computer Vision. Cam-

bridge University Press, ISBN: 0521540518, second edn. (2004)

28. Herath, S., Yan, H., Furukawa, Y.: Ronin: Robust neural inertial navigation in
the wild: Benchmark, evaluations, amp; new methods. In: 2020 IEEE Interna-
tional Conference on Robotics and Automation (ICRA). pp. 3146–3152 (2020).
https://doi.org/10.1109/ICRA40945.2020.9196860

29. Hernandez, J., Tsotsos, K., Soatto, S.: Observability, identiﬁability and sensitiv-
ity of vision-aided inertial navigation. In: 2015 IEEE International Conference on
Robotics and Automation (ICRA). pp. 2319–2325. IEEE (2015)

30. Huai, Z., Huang, G.: Robocentric visual-inertial odometry. In: 2018 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS). pp. 6319–
6326. IEEE (2018)

31. Huang, G.: Visual-inertial navigation: A concise review.

In: 2019 Interna-
tional Conference on Robotics and Automation (ICRA). pp. 9572–9582 (2019).
https://doi.org/10.1109/ICRA.2019.8793604

32. Huber, P.J.: Robust estimation of a location parameter. In: Breakthroughs in statis-

tics, pp. 492–518. Springer (1992)

33. Jones, E., Vedaldi, A., Soatto, S.: Inertial structure from motion with autocalibra-

tion. In: Workshop on Dynamical Vision. vol. 25, p. 11 (2007)

34. Kaiser, J., Martinelli, A., Fontana, F., Scaramuzza, D.: Simultaneous
inertial aided
IEEE Robotics and Automation Letters 2(1), 18–25 (2017).

state initialization and gyroscope bias calibration in visual
navigation.
https://doi.org/10.1109/LRA.2016.2521413

Learned Monocular Depth Priors in Visual-Inertial Initialization

17

35. Kelly, J., Sukhatme, G.S.: Visual-inertial sensor fusion: Localization, mapping and
sensor-to-sensor self-calibration. The International Journal of Robotics Research
30(1), 56–79 (2011)

36. Kopf, J., Rong, X., Huang, J.B.: Robust consistent video depth estimation. In:
IEEE/CVF Conference on Computer Vision and Pattern Recognition (2021)
37. Krasin, I., Duerig, T., Alldrin, N., Ferrari, V., Abu-El-Haija, S., Kuznetsova,
A., Rom, H., Uijlings, J., Popov, S., Kamali, S., Malloci, M., Pont-Tuset, J.,
Veit, A., Belongie, S., Gomes, V., Gupta, A., Sun, C., Chechik, G., Cai, D.,
Feng, Z., Narayanan, D., Murphy, K.: Openimages: A public dataset for large-
scale multi-label and multi-class image classiﬁcation. Dataset available from
https://storage.googleapis.com/openimages/web/index.html (2017)

38. Lepetit, V., Moreno-Noguer, F., Fua, P.: Epnp: An accurate o (n) solution to the

pnp problem. International journal of computer vision 81(2), 155 (2009)

39. Leutenegger, S., Lynen, S., Bosse, M., Siegwart, R., Furgale, P.: Keyframe-based
visual–inertial odometry using nonlinear optimization. The International Journal
of Robotics Research 34(3), 314–334 (2015)

40. Leutenegger, S., Lynen, S., Bosse, M., Siegwart, R., Furgale, P.: Keyframe-based
visual–inertial odometry using nonlinear optimization. The International Journal
of Robotics Research 34(3), 314–334 (2015)

41. Li, C., Waslander, S.L.: Towards end-to-end learning of visual inertial odometry
with an ekf. In: 2020 17th Conference on Computer and Robot Vision (CRV). pp.
190–197. IEEE (2020)

42. Li, J., Bao, H., Zhang, G.: Rapid and robust monocular visual-inertial initial-
ization with gravity estimation via vertical edges. In: 2019 IEEE/RSJ Interna-
tional Conference on Intelligent Robots and Systems (IROS). pp. 6230–6236 (2019).
https://doi.org/10.1109/IROS40897.2019.8968456

43. Li, M., Mourikis, A.I.: A convex formulation for motion estimation using visual
and inertial sensors. In: Proceedings of the Workshop on Multi-View Geometry,
held in conjunction with RSS. Berkeley, CA (July 2014)

44. Li, M., Mourikis, A.I.: High-precision, consistent ekf-based visual-inertial odome-

try. The International Journal of Robotics Research 32(6), 690–711 (2013)

45. Li, Z., Dekel, T., Cole, F., Tucker, R., Snavely, N., Liu, C., Freeman, W.T.: Learning
the depths of moving people by watching frozen people. In: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR) (2019)

46. Liu, W., Caruso, D., Ilg, E., Dong, J., Mourikis, A.I., Daniilidis, K., Kumar, V.,
Engel, J.: Tlio: Tight learned inertial odometry. IEEE Robotics and Automation
Letters 5(4), 5653–5660 (2020)

47. Martinelli, A.: Closed-form solution of visual-inertial structure from motion. Inter-

national journal of computer vision 106(2), 138–152 (2014)

48. Mur-Artal, R., Tard´os, J.D.: Orb-slam2: An open-source slam system for monoc-
ular, stereo, and rgb-d cameras. IEEE Transactions on Robotics 33(5), 1255–1262
(2017). https://doi.org/10.1109/TRO.2017.2705103

49. Qin, T., Li, P., Shen, S.: Vins-mono: A robust and versatile monocular visual-
inertial state estimator. CoRR abs/1708.03852 (2017), http://arxiv.org/abs/
1708.03852

50. Qin, T., Shen, S.: Robust
robots.

timation on aerial
ence on Intelligent Robots and Systems
https://doi.org/10.1109/IROS.2017.8206284

2017

initialization of monocular visual-inertial es-
International Confer-
In:
(IROS). pp. 4225–4232 (2017).

IEEE/RSJ

51. Ranftl, R., Bochkovskiy, A., Koltun, V.: Vision transformers for dense prediction.

ArXiv preprint (2021)

18

Y. Zhou et al.

52. Ranftl, R., Lasinger, K., Hafner, D., Schindler, K., Koltun, V.: Towards robust
monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer.
IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) (2020)
[tutorial].
(2011).

Fraundorfer,
Automation

D.,
Robotics

F.:
Magazine

53. Scaramuzza,

odometry

Visual

18(4),

80–92

IEEE
https://doi.org/10.1109/MRA.2011.943233

54. Tang, C., Tan, P.: Ba-net: Dense bundle adjustment networks. In: International

Conference on Learning Representations (2018)

55. Troiani, C., Martinelli, A., Laugier, C., Scaramuzza, D.: 2-point-based outlier re-
jection for camera-imu systems with applications to micro aerial vehicles. In: 2014
IEEE International Conference on Robotics and Automation (ICRA). pp. 5530–
5536 (2014). https://doi.org/10.1109/ICRA.2014.6907672

56. Tsotsos, K., Chiuso, A., Soatto, S.: Robust inference for visual-inertial sensor fu-
sion. In: 2015 IEEE International Conference on Robotics and Automation (ICRA).
pp. 5203–5210. IEEE (2015)

57. Von Stumberg, L., Usenko, V., Cremers, D.: Direct sparse visual-inertial odom-
etry using dynamic marginalization. In: 2018 IEEE International Conference on
Robotics and Automation (ICRA). pp. 2510–2517. IEEE (2018)

58. Wang, S., Clark, R., Wen, H., Trigoni, N.: Deepvo: Towards end-to-end visual
odometry with deep recurrent convolutional neural networks. In: 2017 IEEE Inter-
national Conference on Robotics and Automation (ICRA). pp. 2043–2050. IEEE
(2017)

59. Whelan, T., Leutenegger, S., Salas-Moreno, R., Glocker, B., Davison, A.: Elas-
ticfusion: Dense slam without a pose graph. In: Robotics: Science and Systems
(2015)

60. Wu, K.J., Guo, C.X., Georgiou, G., Roumeliotis, S.I.: Vins on wheels. In: 2017
IEEE International Conference on Robotics and Automation (ICRA). pp. 5155–
5162. IEEE (2017)

61. Zuo, X., Merrill, N., Li, W., Liu, Y., Pollefeys, M., Huang, G.: Codevio: Visual-
inertial odometry with learned optimizable dense depth. In: 2021 IEEE Interna-
tional Conference on Robotics and Automation (ICRA). pp. 14382–14388. IEEE
(2021)

62. Zu˜niga-No¨el, D., Moreno, F.A., Gonzalez-Jimenez, J.: An analytical solution to the
imu initialization problem for visual-inertial systems. IEEE Robotics and Automa-
tion Letters 6(3), 6116–6122 (2021). https://doi.org/10.1109/LRA.2021.3091407

