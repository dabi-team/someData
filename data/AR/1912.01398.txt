1
2
0
2

t
c
O
0
1

]
h
p
-
p
m
o
c
.
s
c
i
s
y
h
p
[

2
v
8
9
3
1
0
.
2
1
9
1
:
v
i
X
r
a

TeaNet: universal neural network interatomic potential inspired by iterative
electronic relaxations

So Takamotoa,∗, Satoshi Izumia, Ju Lib

a Department of Mechanical Engineering
The University of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8656
b Department of Nuclear Science and Engineering and Department of Materials Science and Engineering
Massachusetts Institute of Technology
Cambridge, MA 02139

Abstract

A universal interatomic potential for an arbitrary set of chemical elements is urgently needed in computational
materials science. Graph convolution neural network (GCN) has rich expressive power, but previously was
mainly employed to transport scalars and vectors, not rank ≥ 2 tensors. As classic interatomic potentials were
inspired by tight-binding electronic relaxation framework, we want to represent this iterative propagation of
rank ≥ 2 tensor information by GCN. Here we propose an architecture called the tensor embedded atom
network (TeaNet) where angular interaction is translated into graph convolution through the incorporation of
Euclidean tensors, vectors and scalars. By applying the residual network (ResNet) architecture and training
with recurrent GCN weights initialization, a much deeper (16 layers) GCN was constructed, whose ﬂow is
similar to an iterative electronic relaxation. Our traning dataset is generated by density functional theory
calculation of mostly chemically and structurally randomized conﬁgurations. We demonstrate that arbitrary
structures and reactions involving the ﬁrst 18 elements on the periodic table (H to Ar) can be realized
satisfactorily by TeaNet, including C-H molecular structures, metals, amorphous SiO2, and water, showing
surprisingly good performance (energy mean absolute error 19 meV/atom) and robustness for arbitrary
chemistries involving elements from H to Ar.

Keywords: Neural Network Potential, Molecular Dynamics, Interatomic potential, Graph Neural Network

1. Introduction

A universal interatomic potential for atomistic simulations of arbitrary chemical species, structures, transfor-
mations and reactions would considerably extend the reach of computational materials. While historically we
have used simple analytical expressions [1, 2, 3], machine learning (ML) interatomic potentials [4, 5, 6, 7, 8]
are increasingly invoked to parametrize interatomic interactions.

Deep neural networks (DNN) have proved to be successful in various ML tasks when large datasets are
provided. The convolution operation, where identical set of weights are used for nodes “belonging” to diﬀerent
spatial locations, achieves eﬃcient compression. The convolutional weight depends on the relative distance,
and not the absolute positions (“translational invariance”). This idea of parametrizing interactions by spatial
relationships can be generalized to graphs. The ﬁeld of graph convolution-based neural networks (GCN)
has been expanding rapidly [9, 10, 11], in particular for molecular systems, where atoms and bonds are

∗Corresponding author
Email addresses:

takamoto.so@fml.t.u-tokyo.ac.jp ( So Takamoto ), izumi@fml.t.u-tokyo.ac.jp ( Satoshi Izumi

), liju@mit.edu ( Ju Li )

Preprint submitted to Elsevier

October 12, 2021

 
 
 
 
 
 
represented by the nodes and edges of the graph. Such network architectures appear natural to both atomistic
and electronic-structure modelers. Indeed, as all the atoms/ions of the same chemical type/valence state
and isotopic mass are “indistinguishable particles” in quantum mechanics, the GCN weights assigned to
atoms/bonds of the same chemical type(s) but diﬀerent integer labels i or j, where i, j = 1, 2, ..., N is the
(arbitrarily) assigned index of an atom in the simulation, should obviously also be identical (“permutational
invariance”). However, sometimes, like in multi-identical-Fermion wavefunction, there can be a “minus sign”
issue. Such “minus sign” can show up in some bond-centered quantities, e.g. if xij ≡ xi − xj, then xij = −xji,
and how to store certain “bond-centered” quantities thus necessitates the usage of notation [ij] where the
order of i, j in the bracket matters, unlike rij ≡ |xij| where the order of i, j does not matter, for which we
use the notation (ij). So we use notation x[ij] ≡ xij to denote a vector that belongs to directed edge labeled
by [ij], and r(ij) ≡ rij to denote a scalar that belongs to undirected edge labeled by (ij), for “bond-centered”
quantities, that can be scalar (rank-0 tensor), vector (rank-1 tensor), matrix (rank-2 tensor), etc. Note in
this paper we take “bond” to mean i, j pair relations where r(ij) can nanometers, and not necessarily the
so-called ﬁrst nearest neighbors.

While GCN architecture exploiting translational and permutational invariances remove the dependence on an
arbitrary observation-frame origin and an arbitrary atomic indexing scheme, how “rotational invariance”, that
is, how arbitrary observation-frame orientations aﬀect or not aﬀect certain results, needs to be discussed. In
any atomistic calculation of the stress tensor, heat ﬂux vector, etc. based on for instance the Tersoﬀ potential
[12], or in assembling the electronic overlap integral and Hamiltonian matrix in the tight-binding / linear
combination of atomic orbitals (LCAO) model [13], one has plenty of scalars (rank 0), vectors (rank 1) and
matrices (rank 2) in the data ﬂow of a code. In an iterative electronic relaxation or explicit time-dependent
density functional theory (TDDFT) [14] calculation, this kind of tensorial data ﬂow can sometime even carry
into the (pesudo)time-domain. In all these calculations, the observation-frame orientation does not really
matter, as all physical quantities are expressed in rank-M tensors ˜Tα1,α2,...,αM , with tensor transformation
law

˜Tα(cid:48)

1,α(cid:48)

2,...,α(cid:48)
M

= Qα(cid:48)

1α1Qα(cid:48)

2α2...Qα(cid:48)

M αM Tα1,α2,...,αM

(1)

α

where ˜T is the same physical object read in a diﬀerent observation frame, Qα(cid:48)α is the rotation matrix between
two observation frames, and Einstein summation rule is used. In this paper we use α, β = 1, 2, 3 to label
Cartesian axes, and i, j, k = 1, 2, ..., N to label atoms. Thus, T [ij]
denotes a rank-1 tensor (vector) that
belongs to a bond, or pair of atoms [ij], where the order matters (directed edge), and T (ij)
αβ is a rank-2 tensor
(matrix) that belongs to the bond or pair of atoms (ij) where the order does not matter. Similarly, T i
α is a
rank-1 tensor (vector) that belongs to the atom i, and T i
αβ is a rank-2 tensor (matrix) that belongs to the
atom i. One could certainly come up with more complex notations like T (ijk)
αβ where the permutation orders
of i, j, k does not matter, or something like T [(ij),(kl)]
where i, j order does not matter, k, l order does not
matter, but (ij) and (kl) order matters. Generally speaking, in this notation the superscript denotes the
“owner” of the tensor whose Cartesian indices are in the subscript. In this paper we will only be limited to
M ≤ 2, and owners either i, (ij), or [ij], as these covers the data types of most of the legacy codes. One can
thus imagine these kinds of “tensor-typed” and “ownership-stamped” data ﬂowing in respective legacy codes
to represent interatomic or electronic-structure interactions.

αβγδ

In addition to the stable molecular structures, currently several GCN models have also succeeded in reproducing
the dynamics of speciﬁc molecules [7, 6, 15, 16, 17]. However, a universal IP describing bond formation,
bond breaking and recombination for arbitrary structures with arbitrary number of elements remains at
the developmental stage. Inspired by the nonlinear iterative data ﬂows in a DFT calculation in achieving
charge-density convergence, we believe the performance of GCN can be signiﬁcantly improved by allowing
M ≥ 2 quantities (“tensors” in “tensor embedded atom network (TeaNet)”) to ﬂow in the network, in addition
to the M = 0 (scalars) and occasional M = 1 (vectors) quantities that ﬂow in conventional GCN.

Physically, embedded atom method (EAM) potential incorporates the concept of electron density of metal,
while Tersoﬀ-type and modiﬁed embedded atom method (MEAM) potentials incorporate the concept of bond
order and angular dependence, which can be derived from the tight-binding approximation of the electronic

2

wave function, using local combination of (quasi)atomic orbitals [13]. These IPs have been widely used for
simulating extended defects, mechanical deformation and damage, and phase transitions. However, individual
potential parameter set is developed to reproduce a certain systems (e.g. FCC metals, silica, organic molecules,
etc.). In this paper, we propose a NNIP architecture (see section 2) that can be considered a superset of
MEAM potentials while mimicking electronic total-energy relaxation [18] in a local orbital (tight-binding)
basis [13, 19, 20]. We call this approach the tensor embedded atom network (TeaNet). We modify the
architecture of GCN with new components (edge-associated in addition to node-associated variables) that
fully represent the corresponding physics-based IP. Rank-2 tensors as well as rank-1 vectors are introduced
in the network, so the model can naturally represent propagation of orientation-dependent Hamiltonian
information. We have also adopted residual NN architecture, with recurrent parameter model initially. Such
ResNet architecture and recurrent GCN initialization to accelerate computations are found to be quite
eﬀective in getting rapid reduction of traning error.

Our method is related to previous NNIP eﬀorts. Embedded Atom Neural Network Potential (EANN)
[17] extends the EAM potential using NN. This model combines physics-based representation (electron
density) and NN-based embedding function F (ρ). This physics-related model provides excellent accuracy
for bulk systems while retaining simplicity. There are several works implementing higher-order geometric
information into NN architecture through spherical harmonics representation [21, 22, 23]. The key idea is to
use Clebsch-Gordan coeﬃcients to hold invariances by any rotations in SO(3) group. In addition, the idea
and the theoretical study of using tensor values in the interatomic potential was investigated in Moment
Tensor Potentials (MTP) [24].

In section 4, we show the training results of our model for elements 1-18 (H-Ar) on the periodic table, where
random combination of these elements in mostly highly disordered structures are used as the training set.
We also performed sensitivity analysis and discussed the importance of the diﬀerent features of our model. In
section 5, we show the general applicability of our method to a wide range of materials including chemical
reaction processes. We will demonstrate that our model performs well for liquid water, amorphous silica as
well as simple metals and hydrocarbons.

2. TeaNet architecture

We ﬁrst introduce the notation used in our drawings. In the line-drawing ﬁgures, values are illustrated as
circles. The ﬁlled colors corresponds to the types of the values, where scalar, vector, and tensor are illustrated
as gray, light blue, and blue circles, respectively. Operations are illustrated as rectangles. Here, we write
the linear layer as lin(x), the nonlinear activation layer as act(x), the concatenation function as con(x, y, ...),
vector L2-norm as norm(x), and the cutoﬀ function as cut(x). We use subscript to denote the dimension of
stacked variables, for example s128 means 128 scalars, v32 means 32 vectors (total 48 real numbers), and t16
means 16 matrices, each 3 × 3. The 128, 32, and 16 are called number of channels.

lin(x) is always applied channel by channel. It is noted that each lin(x) appeared in the following equations
has diﬀerent parameters. It is also noted that those parameters are learnable network parameters like in
ordinary neural networks.

While the output of TeaNet is the total energy of the system (a scalar), the network is trained to simultaneously
compute the atomic forces, providing useful data for training. The atomic forces are calculated by a
backpropagation process, and so the training process becomes a double backpropagation. The molecular
dynamics simulation requires a smooth activation function. In this study, we employed the integral of the
softplus function, which to our knowledge we were ﬁrst to propose as an activation function. The integral is
calculated as follows:

f (x) ≡

(cid:90) x

−∞

log (1 + exp (t)) dt

= −Li2 (− exp (x)) ,

3

(2)

Figure 1: Overview of the TeaNet NNIP.

where Li2 is a second-order polylogarithm function. This function approaches 0 as x tends to −∞ and
approaches the curve of x2 + C at large x, where C is a constant. When this function is applied to the edge
arrays, the activation functions are shifted so that f (0) becomes 0. Thus

act(x) ≡ f (x) − f (0).

(3)

Using the activation function, we can train a softplus-type network in the second backpropagation process. If
the polylogarithm function is replaced by the softplus function, the second backpropagation process results in
a sigmoid-type network. The function shape is shown in Fig. 2. The eﬀect of this change to the prediction
accuracy is presented in the section 4.

The cutoﬀ function cut(x) is a smoothly decaying function. In this work, we use the same function as act(x)
shifted by linear function,

cut(x) ≡ act(lin(x)) + (c1x + c0)

(4)

where c0 and c1 (linear function part) are set to satisfy cut(x) and its derivative are zero when x equals to
the cutoﬀ distance.

In TeaNet architecture, the inputs are the list of element label and the list of position of atom. Other

4

OverviewLocal interaction blockInputsOutput value (energy)2Local means atoms within certain physical cutoff distance will interactPreprocessPostprocessLocal interaction blockLocal interaction blockInputs are element label and position of atoms4 ~ 16 layersFigure 2: Left: comparison of activation functions. Softplus and ELU (α = 1) functions[25] are also shown. They are shifted so
that f (0) becomes 0. Middle: derivative of the activation functions. Right: second derivative of activation functions. In softplus
and ELU, second gradient value f (cid:48)(cid:48)(x) vanishes when x is large.

predeﬁned information such as bonding or atomic charges are not required. The output value is single scalar
value, which corresponds to the energy. The force of the atoms are calculated using normal back propagation.
There are three parts in TeaNet. The ﬁrst part is preprocess. It receives the input values and creates various
values which is used for the graph convolution layers. The second part is the internal graph convolution
layers which we call local interaction block. The input values and output values of the single layer have the
same shapes. Therefore we can stack the layers by arbitrary numbers. The last part is postprocess part,
which receives the ouput values of the graph convolution layer and output single scalar value.

2.1. Preprocess and postprocessing

2.1.1. Preprocessing

Here, we use the character a as atom-related values (corresponding nodes) and b as bond-related values
(corresponding bonds).

Bonds are counted only for pair of atoms whose distance is smaller than the cutoﬀ distance. In this paper,
the cutoﬀ distance is set to be 6 Å.

There are three types of values for atom-related values which are scalar, vector, and rank-2 tensor. We use the
symbols as, av, and at for them. It is noted that each types of values have multiple channels. For example,
in this paper, we use 256 dimensions (usually called channels in neural network context) for scalar value and
16 dimensions for both vector and rank-2 tensor value. Therefore, if the number of atoms in the system is 64
and the number of dimension of the space is 3, the shapes of as, av, and at are 64 × 128, 64 × 3 × 16, and
64 × 3 × 3 × 16, respectively.

Bonds have scalar and vector values. We use the symbols bs, bv as well. In addition, two special constant
values for bond-related values are also introduced. One is relative position vector rv. It is deﬁned by the
diﬀerence of the position of two corresponding atoms. Another one is bond length rs, which can be calculated
by the l2-norm of rv. It should be noted that the sign of rv depends on the order of corresponding two atoms,
which is described as "minus sign" issue at the introduction section. Careful consideration is required to use
rv in the following calculations since the output value should not depend on the order of atoms. We use the
character i and j for the label of those two atoms.

Atom scalar as is initialized by look-up table. To imitate the occupancy of electron orbitals, the values
corresponding to the atomic number are divided by 2 and packed by 1 from the top of the array. The list is
shown in table 1. The remaining channels are set to zero. Atom vector av and rank-2 tensor at are initialized
by zero.

5

-10123456-3-2-10123f(x)xSoftplusELUProposed00.10.20.30.40.50.60.70.80.91-3-2-10123f''(x)xSoftplusELUProposed00.511.522.533.5-3-2-10123f'(x)xSoftplusELUProposedFigure 3: Preprocess and postprocessing

Bond scalar bs are initialized by Eq. 5,

bs = exp {−lin (rs)} + (c(cid:48)

1rs + c(cid:48)

0) ,

(5)

where c(cid:48)
1 (linear function part) are set to satisfy bs and its derivative with respect to rs are zero when
rs equals to the cutoﬀ distance. Eq. (5) is expected to behave like the distance term of the Morse-style IP.

0 and c(cid:48)

Bond vector bv is also initialized by zero. Unlike rv, we make bv does not depend on the order of atom i and
j. The example of physical value corresponding bv is local electric dipole.

To reiterate, in the preprocess part, as, av, at, bs, bv, rs, and rv are initialized.

2.1.2. Postprocessing

For the postprocessing part, only as and bs are used to calculate energy. It is noted that as and bs have
multiple channels (multiple scalar values for single atom and single bond). First, single scalar values for each
atom and bond are calculated by,

alast = lin(as),

blast = lin(bs),

(6)

where the number of channels of alast and blast are one.

6

Preprocess/PostprocessVector NormElementasActivation(exp)bsAtom typeBond scalar arrayavatbvAtom vector arrayAtom tensor arrayBond vector arrayRelative position vectorLength scalarasbsAtom Summation+EOutput value (energy)PreprocessPostprocessColor scheme:Fixed constantVector variableTensor variableShapes:ValuesOperationsrsrvPositioniPositionj-Atom positionsAtom scalar array3oooLookup tableThe cutoff distance is set to 6 ÅNotation:Length of arrays128, v16, t16s128s128Bond Summationv16v16t16oConcats8s120s128s1s1s128s1LinearChannel mixing+bLinearChannel mixing+bTable 1: List of the input values of ns.
Element

ns

H
He
Li
Be
B
C
N
O
F
Ne
Na
Mg
Al
Si
P
S
Cl
Ar

[0.5, 0, 0, 0, 0, 0, 0, 0, 0]
[1, 0, 0, 0, 0, 0, 0, 0, 0]
[1, 0.5, 0, 0, 0, 0, 0, 0, 0]
[1, 1, 0, 0, 0, 0, 0, 0, 0]
[1, 1, 0.5, 0, 0, 0, 0, 0, 0]
[1, 1, 1, 0, 0, 0, 0, 0, 0]
[1, 1, 1, 0.5, 0, 0, 0, 0, 0]
[1, 1, 1, 1, 0, 0, 0, 0, 0]
[1, 1, 1, 1, 0.5, 0, 0, 0, 0]
[1, 1, 1, 1, 1, 0, 0, 0, 0]
[1, 1, 1, 1, 1, 0.5, 0, 0, 0]
[1, 1, 1, 1, 1, 1, 0, 0, 0]
[1, 1, 1, 1, 1, 1, 0.5, 0, 0]
[1, 1, 1, 1, 1, 1, 1, 0, 0]
[1, 1, 1, 1, 1, 1, 1, 0.5, 0]
[1, 1, 1, 1, 1, 1, 1, 1, 0]
[1, 1, 1, 1, 1, 1, 1, 1, 0.5]
[1, 1, 1, 1, 1, 1, 1, 1, 1]

Then, alast and blast are summed along all atoms and bonds. The obtained single scalar value is the output
value (total energy E) of this model.

E =

(cid:88)

atoms

alast +

(cid:88)

bonds

blast.

(7)

2.2. Local interaction block: overview

This section shows the overview of the calculation ﬂow of local interaction block. The detail of each calculation
block will be described in the later sections.

First, several operations are applied to the atom-wise inputs (as, av, at) and the bond-wise inputs (bs, bv).
Those newly created values during the local interaction block are named as1 or av1.

Then, atom-wise values are distributed to the corresponding bonds. It is noted that there are always two
atom-wise values for single bond. Those distributed values are concatenated with bond-wise values with
keeping required invariances. The bond shape values (rs, rv) are also used here. Then, the new bond-wise
value named ytot is created using those values.

After that, new atom-wise variables and bond-wise variables are created using ytot. Those values are added
to the atom input values and bond input values. Finally, the same shapes of values as the input values (as,
av, at, bs, bv) are returned.

7

Figure 4: Local interaction block: overview

2.3. Local interaction block: preprocessing

As described before, the local interaction block receives as, av, at, bs, and bv as inputs. First, several linear
and nonlinear functions are applied for each values.

as1 = lin(act(lin(con(as, norm(av))))),
av1 = lin(av),
at1 = lin(at),
bs1 = act(lin(con(bs, norm(bv)))).

(8)

Here, concat means the values are concatenated along the channel axis.

For linear channel mixing, the linear operation is not applied along the space dimension axis but along
the channel axis. It is noted that the raw components of vector and tensor values should not be summed,
multiplied independently, or combined with other scalar values since the those components depend on the
basis vectors of the coordination system. On the other hand, linear function along channel axis, scalar
multiplication, inner product (including vector norm), and tensor product are allowed operations.

In this paper, vector norm means the L2-norm of vector along dimension axis. The result values can be
treated as the scalar values.

2.4. Local interaction block: distribution

The atom-wise variables as1, av1, and at1 are distributed to the corresponding bonds. It is noted that there
are always two atom-wise values for single bond. We labeled them by i and j as described before.

8

Figure 5: Local interaction block: preprocessing

To clarify that the distributed values corresponds to the bonds, we name the distributed atom-type values as
βs1, βv1, and βt1. Since there are two corresponding atoms (i and j) for single bond, there are two β values
such as βs1i and βs1j. We write βs1{i,j} when the same operations are applied along i and j.

We now have βs1{i,j}, βv1{i,j}, βt1{i,j}, bs1, and bv. Independently, we have rs and rv. All of those values are
bond-wise values.

2.5. Create bond-wise values: preparation

Tensor value βt1{i,j} is squashed into vector values by taking inner product with rv, and then summed to
βv1{i,j}.

βv2{i,j} = βv1{i,j} ±ij βt1{i,j} · rv.

(9)

It is noted that the sign of rv depends on the order of the atomic label (i or j). Therefore, to keep the i-j
order invariance, the sign should be ﬂipped when the operation is applied to j-related values. We use the
symbol ±ij for that case. In the ﬁgure, the i-j order sensitive values are highlighted as blue characters and
lines.

9

Figure 6: Local interaction block: distribution

2.6. Create bond-wise values: create various intermediate values

Various bond-type scalar values are calculated by taking the inner products of vector values.

x0{i,j} = βs{i,j}cut(rs),
x1{i,j} = ±ijβv2{i,j} · rvcut(rs),
x2{i,j} = βv2{i,j} · bv,

x3 = βv2i · βv2jcut(rs).

(10)

It is noted that ±ij is used for rv part again. The cutoﬀ function cut(rs) is multiplied for x0{i,j}, x1{i,j}, and
x3 to ensure that all values are zero when the bond distance equals to the cutoﬀ distance. It does not be
applied to x2{i,j} since bond-related value bv is assumed to have the same nature.

2.7. Create bond-wise values: concatenation

The goal of this section is to create the uniﬁed bond-wise value ytot from the previously created values. The
obtained scalar values are x0{i,j}, x1{i,j}, x2{i,j}, x3, and bs1.

The thing left to be done is to eliminate i-j order dependence. It is noted that the values of x0{i,j} swap if
we swap atom i and j. In this architecture, we ﬁrst calculate the summation and diﬀerence (x0i + x0j and
x0i − x0j). The former one does not have order dependence and the latter one has order dependence only on
its sign. Therefore, applying the even function for the latter one removes the order dependence. Here, we use

10

Figure 7: Create bond-wise values: preparation

the square function. The same treatment is carried out for x1{i,j} and x2{i,j}.

ysym = lin(con(x0i + x0j, x1i + x1j, x2i + x2j, x3, es1)),
yasym = lin(con(x0i − x0j, x1i − x1j, x2i − x2j)),

ytot = act(ysym) + (yasym)2 ,

(11)

where (yasym)2 means element-by-element square. ytot is considered to represent the state of the bond.

In the ﬁgure, we highlighted the order-sensitive calculation ﬂow as blue characters and lines.

2.8. Local interaction block: create atomic values for update

Using ytot, various values which will be accumulated to atom-wise values and bond-wise values are created.
Atom-type variables are calculated by,

βs3{i,j} = lin(ytot),
βv3{i,j} = lin(ytot)lin(bv) ±ij lin(ytot)rv,
βt3{i,j} = lin(ytot)rv ⊗ rv ±ij lin(bv) ⊗ rv.

(12)

11

Figure 8: Create bond-wise values: create various intermediate values

2.9. Local interaction block: create bond values for update

In the same manner to the atom-wise values, bond-wise values are calculated using ytot.

bs3 = lin(ytot),
bv3 = lin(ytot)lin(yasym)rv + lin(βv2i + βv2j)cut(rs).

(13)

For creating bond vector value bv3, yasym is introduced to eliminate the i-j order dependence.

2.10. Local interaction block: aggregation

βs3{i,j}, βv3{i,j}, βt3{i,j} are intended to update the atom-wise values. However, those values are still bond-
wise values and needed to be aggregated to the corresponding atoms. This is done by taking the summation of
neighboring bond-wise values to the atoms. This is the inverse calculation ﬂow to the distribution described
in the section 2.4

We name the summed atomic values as as3, av3, and at3.

12

Figure 9: Create bond-wise values: concatenation

2.11. Local interaction block: create output values

Finally, node and edge variables are updated by ResNet-style bypass function.

˜as4 = as + lin(as) + lin(as)as3,
˜av4 = av + lin(av) + lin(as)av3,
˜at4 = at + lin(at) + lin(as)at3 + lin(as)I,
˜bs4 = bs + lin(bs) + bs3,
˜bv4 = bv + lin(bv) + bv3,

(14)

where I is the identity tensor which is used as the bias term. The ﬁrst term is a residual part and the second
term is the structure-independent value update part.

It is noted that lin(as) is multiplied to atom-wise update part. It is considered to work as a node convolution
gate function. These variables are the ﬁnal output of the interaction block and used as the input variables of
the next block.

Those ﬁve values (˜as4, ˜av4, ˜at4, ˜bs4, and ˜bv4) are the output values of the local interaction block. They are
used for the input values of the next local interaction block or the postprocess layer.

3. TeaNet Philosophy and Training

With the detailed network laid out in section 2, we now zoom out and discuss the underlying philosophy of
TeaNet. We would like to show the correspondence between existing physics-based potentials (EAM and
Tersoﬀ-type angular-dependent potentials) and GCN, in section 3.1 and 3.2, respectively. We show that

13

Figure 10: Local interaction block: create atomic values for update

the Tersoﬀ-type angular-dependent bond-order potential can also be rewritten as the graph convolution by
incorporating the Euclidean tensor variables into GCN architecture. This means that the rank-2 tensors
empower GCN to treat the spatial information naturally while keeping frame-rotation, reﬂection, and
translation equivariances. We also show the necessity of tensor values for transferring spatial information in
graph convolution architecture. Then in section 3.3, we introduce the constraint which enables the model to
be stacked deeper and to improve the accuracy. Then we explain the analogy of this constraint with the
energy relaxation procedure of the charge-transfer-type IP, which is known as charge equilibration (QEq)
method[26]

3.1. Rewriting EAM potential as graph convolution

The EAM potential [1] incorporates the concept of electron density in a shallow 1-layer network. In EAM,
the total energy, E, is calculated as:

E =

ρi =

1
2

(cid:88)

(cid:88)

i

j(cid:54)=i

φij (rij) +

(cid:88)

i

Fi (ρi) ,

(cid:88)

j(cid:54)=i

fj (rij) ,

(15)

where i, j are the atom labels and φij, Fi, fj, and rij are functions describing the two-body energy, the
embedding energy, the electron charge, and the interatomic distance, respectively. In EAM potential, ρi
which corresponds to the background electron density at atom i is calculated by the summation of pairwise
function. It can be expressed as a single-layer graph convolution (see Fig. 16).

The EAM potential can be translated as a shallow GCN as follows: The atomic information (on the nodes) is
distributed to the corresponding bonds rij. Then, the bond-wise values (φij (rij) , fi (rij)) are calculated. A
part of them (fi (rij)) are summed to the corresponding atoms and atom-site nonlinear function (Fi (ρ)) is
applied. It is noted that EAM potential has the required invariances such as permutation, pair order, and
isometry.

14

Figure 11: Local interaction block: create bond values for update

The calculation ﬂow of our graph convolution layer follows the idea of the EAM potential. First, the atom-wise
values are calculated. Then, they are distributed into the corresponding bonds and the bond-wise values are
calculated by combining atom-wise values and bond-wise values. After that, the calculated bond-wise values
are transferred into the corresponding atoms and update the atom-wise values.

To accumulate the edge information (fj (rij)) into nodes, the embedding function (Fi) plays an important
role. In EAM potential, Fi represents the interaction between certain atoms and the surrounding electron
density. Therefore, from a physics standpoint, the embedding function is essential in the network architecture.
We call this the “node gate” function. The eﬀect of the node gate function on prediction accuracy is presented
in section 4.

3.2. Translating bond angle interaction into graph convolution and embedding vector and tensor values

Generally speaking, atomic interactions depend on the bond angle between interacting atoms. For example,
H2O and NH3 molecules are stabilized at a certain bond angle. Diamond comprises a tetrahedral network.
These angular dependencies are generated by the interaction between electron orbitals [13].

When embedding spatial information in the network architecture, satisfying invariance requirements can
be challenging. The energy should be invariant to the rotation of the basis vectors. Invariance is handled
diﬀerently in diﬀerent models. One solution is to limit the input data to only the bond length. SchNet [7] and
PhysNet [16] uses bond length only. Deep tensor neural networks (DTNN) [6] and deep potential molecular
dynamics (DPMD) [15] also maintain the rotational invariance by using bond length. However, bond-bond
interactions usually depends on the higher-order geometric information such as bond angle or dihedral angle,
and its relation to the bond length can be weak. The detailed discusssion is in Appendix Appendix .1. Since
the solution of using the raw values of vector components as the input values loses the rotation invariance, it
is not appropriate for a molecular dynamics simulation.

Many existing IPs involve bond angles directly. For example, the Stillinger-Weber potential [27] has a
three-body energy function. Bond-order-type potentials, such as the Tersoﬀ potential [28, 2], possess a
bond-order term consisting of the three-body angular-dependent term. Some machine learning-based models

15

Figure 12: Local interaction block: aggregation

give similar solutions. The Behler-Parrinello neural network (BPNN) [4] calculates the three-body symmetry
functions.

However, the bond angles correspond to neither nodes nor edges but rather to three-body atom combinations.
Therefore, they should be combined and converted into representative node and edge values during the
convolution operation, which requires the use of ad-hoc functions such as symmetry functions. Another
problem is the lack of long-range interaction in the three-body angle term. In GCN, local information can
transfer to farther nodes through the convolutional layers. Transfer of the angle information is also desirable.
For example, the directional electronic orbitals of the π bonds can be extensively spread. However, convolution
of the angular information at the node crushes the angle information and prevents its propagation.

Here, we show that the angle-dependent three-body convolution algorithm can be naturally expressed as a
normal node-and-edge convolution operation using Euclidean vector and second-order tensor values. This
means that the model can have local spatial information and propagate it to farther nodes, and to interact
with them at nodes while keeping rotational invariances. This is achieved by rewriting the Tersoﬀ-type
angle-dependent bond-order function as a convolution operation.

The Tersoﬀ-type angle-dependent term ζij can be written as

E =

1
2

(cid:88)

i,j(cid:54)=i

φA (rij) +

1
2

(cid:88)

i,j(cid:54)=i

b (ζij) φB (rij) ,

ζij =

(cid:88)

k(cid:54)=i,j

G (θijk) H (rij, rik) ,

(16)

where i, j, and k are the atom labels; θijk is the angle between bonds ij and ik; rij and rik are the bond
lengths, and φA, φB, b, G, and H are various functions. In some Tersoﬀ-type potentials [29, 3], the ζij term
is expressed as

ζij =

(cid:88)

(cid:104)

c + d {h − cos (θijk)}2(cid:105)

fc (rik) exp [λ (rij − rik)] ,

(17)

k(cid:54)=i,j

16

Figure 13: Local interaction block: output 1

where fc is the cutoﬀ function and c, d, h, and λ are the parameters. After expanding the factors and
converting the parameters, Eq. (17) is transformed to

ζij = exp (λrij)

= exp (λrij)

= exp (λrij)

= exp (λrij)

(cid:88)

k(cid:54)=i,j
(cid:88)

k(cid:54)=i,j
(cid:88)

(cid:2)g0 + g1 cos (θijk) + g2 cos2 (θijk)(cid:3) fc (rik) exp (−λrik)

(cid:104)

g0 + g1ˆrij · ˆrik + g2 (ˆrij · ˆrik)2(cid:105)

fc (rik) exp (−λrik)

[g0 + g1ˆrij · ˆrik + g2 (ˆrij ⊗ ˆrij) : (ˆrik ⊗ ˆrik)] fc (rik) exp (−λrik)

k(cid:54)=i,j
(cid:88)

[g0 + g1ˆrij · ˆrik + g2 (ˆrij ⊗ ˆrij) : (ˆrik ⊗ ˆrik)] fc (rik) exp (−λrik)

k(cid:54)=i
− (g0 + g1 + g2) fc (rij)




= g0 exp (λrij)

(cid:88)



k(cid:54)=i

fc (rik) exp (−λrik)



+ g1 exp (λrij) ˆrij ·



ˆrikfc (rik) exp (−λrik)





(cid:88)



k(cid:54)=i

+ g2 exp (λrij) (ˆrij ⊗ ˆrij) :

− (g0 + g1 + g2) fc (rij) ,

(ˆrik ⊗ ˆrik) fc (rik) exp (−λrik)







(cid:88)



k(cid:54)=i

(18)

where ˆrij and ˆrik are the unit vectors. The symbols “·,” “:,” and “⊗” denote the inner product, the Frobenius
inner product, and the tensor product (dyad) of two vectors, respectively. Since all summation terms
are written without j, they can be calculated by the convolution operation. As a result, the Tersoﬀ-type

17

Figure 14: Local interaction block: output 2

potential function can be written as a two-layered neural network. The necessity of the Rank-2 tensors for
the angle interaction using convolution operation and its physical meaning and comparison with spherical
harmonics-based methods are shown in the Appendix Appendix .2.

Based on this discussion, we introduce both vectors and tensors into the network. Each node array contains
scalar, vector, and tensor values, whereas each edge array contains scalar and vector values. A relative
position vector is also provided as an input value. The eﬀects of tensor values on prediction accuracy are
presented in the section 4. See section 2.1 for the details of the implementation.

3.3. Improvement of stacked GCN accuracy inspired of iterative energy minimization process

Like in existing GCNs, the local interaction block can be stacked multiple times. However, as frequently seen
in NN training, we observed the increase of the number of layers always brings the instability during the
learning procedure. Therefore, it was hard to improve the accuracy by increasing the number of layers of our
model in practice.

Here, we found a method to reduce this instability signiﬁcantly. The key idea is to initialize and to make a
constraint that all middle layers have the same NN parameters at the initial stage of training. One can ﬁnd
similarities to the recurrent GCN architecture [9]. Another essential point is to apply the residual network
(ResNet) architecture. Interestingly, we found that the accuracy was improved by increasing the number of
layers up to 16. (see architectural details in section 4). It is said that improving the expression power of
GCN is hard by increasing the depth size [30]. This was also true in atomistic system in practice. Many
GCN models in atomistic system also have up to 6 convolution layers [7, 16, 15]. In addition, making the
constraint to set the all middle layers have the same parameters can be thought to enforce them to behave
the identical nonlinear transformation, which seems to reduce the expressive ability of the entire network.
Therefore, it is not strange to think that this method does not contribute to the accuracy. It is noted that
this constraint was came from an analogy with physics. In this section, we explain the analogy and introduce
the insight why the deeply stacked model can improve the accuracy even it is GCN.

Limiting the number of GCN layers to one means the node’s information can be determined only by the

18

Figure 15: Local interaction block: output 3

neighboring nodes. In the analogy with GCN and EAM potential, this corresponds to the assumption that the
electron state (density) of the atom can be calculated only by surrounding atoms. Although the assumption
works well for certain systems, it is not physically correct picture in general, as seen by the long-ranged nature
of the dielectric response function in DFT[31]. The charge transfer eﬀect plays important roles in chemical
reactions. The actual electron states are determined so that the energy of the entire system is minimized.
DFT calculates the ground state of the electron density by an iterative procedure. To incorporate such
long-ranged propagation of information, charge-transfer-type IPs [26, 32, 33, 29], which model the deviation
of the electron density and minimize the energy of the system with respect to the charge distribution, are
being actively developed.

In charge-transfer-type IPs, the energy minimization involves implicit matrix-vector equations solved by matrix
inverse calculation [26, 32] or solved by repeatedly updating the charge distribution using the gradient-based
method [33]. If the number of iterations is ﬁxed, this iterative procedure could be written as a feed-forward
data ﬂow model. It is noted that iterative total energy minimization reproduces the physically reasonable
long-range interactions. A well-known example is the Green’s function solution that can be represented by a
matrix-vector equation Ax = b: even though A is a sparse matrix (local interactions), the inverse A−1 is
dense and resembles long-range interactions. However, by iteratively solving Ax = b with Krylov subspace
method {b, Ab, A2b, A3b, ..., Anb}, one can achieve excellent approximant to the long-range interaction,
which is akin to an n-layer neural network with identical weights.

It should be noted the importance of the residual network architecture in the above discussion. The residual
network (ResNet) [34] have recently emerged in the ﬁelds of image recognition, as have other machine-learning
tasks, including object detection [35], machine translation [36], and speech synthesis [37]. ResNet’s core idea is
to “bypass” the output values from the middle layers and add them directly to the lower layer to avoid gradient
disappearance during back propagation. Interestingly, previous studies interpreted the ResNet architecture
as an explicit Euler method of ordinary and partial diﬀerential equations [38, 39, 40]. In this section, we
associated the stack of the local interaction blocks using residual network connection with charge-transfer
energy minimization calculation of IPs.

19

Figure 16: EAM potential represented as a graph convolution. Left: Schematic of the summation operation. Right: Corresponding
network model.

3.4. Data collection

Since our target to develop an universal IP with applicability to arbitrary structures, the dataset is required
to cover the wide range of phase space as much as possible. One solution is to increase the number of data
points. Another requirement is to secure the diversity of data points.

The dataset is created as follows. First, the simulation box is ﬁlled with tens of atoms. The element type is
randomly selected from the ﬁrst three rows of the periodic table (from H to Ar). The number of element
types and their ratio in one sample is also widely distributed. The system is heated to high temperature
(e.g. 10,000 K), melted for approximately 100 femtoseconds, cooled to a setting temperature, then further
annealed for another 100 femtoseconds by classical MD to obtain a snapshot. The timestep is 1 femtosecond.
This process is repeated for various temperatures (up to 5,000 K) and volumes. Then, the reference energy
and atomic forces are obtained by DFT calculations of the snapshots. We consider that this dataset consists
of highly disordered structures, including many types of local atomic conﬁgurations, and thus presents a
challenging task. Furthermore, most of the conﬁgurations are far from stable.

In addition, to include realistic structure, we create another dataset by heating the structures of the
molecular dataset of the Materials Project repository [41] up to 3,000 K. In this work, we merged those
two datasets. The entire dataset contains approximately 294,000 structures. The size of the dataset at the
double backpropagation process (the corresponding atomic forces of the 294,000 structures) is approximately
7,375,000. Two-hundred randomly selected structures (including 4962 × 3 atomic force data) are used for the
test dataset exclusively.

We used VASP for DFT calculation. To increase the number of data points, the relatively fast settings were
used. GGA-PBE was used for the exchange-correlation energy. The Gaussian smearing was used. Spin
polarization is considered. The smearing width σ was 0.2 eV. The PREC setting in VASP (used to determine
energy cutoﬀ) was set to Medium. One k-point was applied. We used the same settings among structures
to ensure the energy surface is consistent. Further expansion of the dataset (e.g. increasing the number of
elements, increasing the number of structures, improvement of the computational accuracy) is a future task.

The details of dataset is shown in Appendx Appendix .6.

20

3.5. Training procedure

The NN hyperparameters are set as follows. The length of the scalar node and edge arrays is set to be 128.
The length of the vector node, rank-2 tensor node, and vector edge arrays is each set to 16. The cutoﬀ
distance is set to 6 Å. The minibatch size is 100.

The network is trained by optimizing the combined absolute loss function (energies and atomic forces) using
the Adam optimizer [42]. As the number of layers increased, frequent ﬂuctuations were observed in the
training error. This instability may be explained, at least in part, by the roughness of the DFT-calculated
potential energy surface, which is the ground truth of this task. Small atomic displacements, such as the
approaching of two neighboring atoms, can potentially cause abrupt energy increases.

To resolve this problem, we constrained the parameters of all intermediate layers in the network to the same
values at the initial stage of the training, as described in section 3.3.

Finally, the models were trained by stochastic gradient descent with a small learning rate (0.1). The numbers
of iterations were set to 450,000 (initial), 450,000 (main), and 20,000 (ﬁnal) in all cases.

4. Training results

4.1. Dependence of accuracy on the number of NN layers

There are several datasets for atomic systems, without reaction barrier information. For example, QM7
(GDB7-12) [43] and QM9 (GDB9-14) [44] are composed of equilibrium molecular data. In contrast, to
reproduce the wide range of energy surface, the model should reproduce a wide range of structures. Therefore,
evaluations of highly disordered structures including dangling bonds, overcoordinated atoms, and various
disordered bond lengths are required. Therefore, we prepare our own dataset of highly disordered structures
using molecular dynamics simulations. The dataset consists of the ﬁrst three rows of the periodic table (from
H to Ar). The details of the data preparation are shown in section 3.4.

We trained networks of diﬀerent depths (2, 4, 8, and 16 layers). The hyperparameters and other settings
for training are shown in section 3.5. The results are depicted in Table 2. Increasing the number of layers
improved the network accuracy. No overﬁtting was observed in any system. In the best-performing network
(with 16 layers), the mean absolute error (MAE) of the energy was 19.3 meV/atom. Our proposed method
enables to construct deeper model which has higher accuracy in the ﬁeld of GCN.

Table 2: Regression accuracy of trained networks with various numbers of layers.

# layers # params

2
4
8
16

87, 000
235, 000
529, 000
1, 120, 000

Test loss
function [unitless]

Energy MAE
[meV/atom]

Force MAE
[eV/Å]

2.54
1.92
1.65
1.62

32.5
23.9
21.4
19.3

0.213
0.167
0.143
0.142

For further evaluation, we also trained our model for datasets of locally stable atomic conﬁgurations (QM9
dataset and Materials Project molecule dataset). In addition, we evaluated the applicability of previous
works using our dataset. The results are shown in Appendix Appendix .3.

4.2. Eﬀects of the proposed components of the network

To investigate the eﬀects of the components in our proposed network architecture, we systematically removed
their corresponding functions and checked each component eﬀect. The results are presented in Table 3.

21

First, the network was run without inputting the tensor values (“w/o tensor” row in Table 3). To conduct a
fair test, the number of scalar values was increased to maintain the original number of parameters in the
network. Then, the network was run without the node convolution gate (“w/o gate” row in Table 3). The
number of scalar values was again increased to oﬀset the reduction in the number of parameters. Finally, the
proposed activation function was replaced by the softplus function (“Softplus” row in Table 3). A four-layer
network without the initial 450,000 iterations was used for comparison.

Table 3: Comparison between the baseline and the four-layer network with one removed component.

Test loss
function

Energy MAE
[meV/atom]

Force MAE
[eV/Å]

Original four layers
w/o tensor
w/o gate
Softplus

1.84
2.15
1.99
1.89

22.6
25.5
24.5
24.1

0.161
0.190
0.174
0.165

The largest decrease in accuracy is seen in the case without a tensor value. The second largest decrease is in
the case where the node convolution gate was not inserted. Interestingly, the proposed activation function
outperformed the softplus function.

5. Materials Applications

5.1. Overview

The universal NNIP should be applicable to arbitrary 3D atomic conﬁgurations with any bond types,
crystal/molecular structures, and element type (up to Ar in this dataset). We have tested various systems
including molecular systems, inorganic crystal structures, water, and aqueous solutions.

In this section, we used the four-layer version of the neural network in consideration of the calculation cost of
MD simulations. This is like the embedded-atom potential with embedding applied four times, and with
tensors and vectors propagating inside as well. The same parameter set is used throughout this section.

5.2. Intramolecular structure

We tested the reproducibility of the structures of small C-H molecules. The bond lengths and bond angles of
typical small hydrocarbon molecules were compared, and the results are listed in Table 4.

Our model can reproduce both the bond lengths and angles with good accuracy. In particular, a variety of
C-C bonding (sp, sp2, and sp3) is well reproduced. It is noted that ethene forms a planar structure and that
ethane forms a staggered conformation. This indicates that our model captures the dihedral angle (4-node)
interactions by passing vector and tensor information through the C-C bond. In addition, we conﬁrmed that
benzene forms a planar structure while cyclohexene forms a chair-type structure, which is a typical diﬀerence
in bonding nature between aromaticity and a single bond.

5.3. Bulk properties of metal and semiconductor

Metals have delocalized dielectric response, while materials with bandgap can have exponentially localized
response [13]. Table 4 shows TeaNet predictions of Na, Al, and Si. Several crystal structure polymorphs of
the same element were evaluated.

22

Table 4: Top: Structural accuracy on small hydrocarbon molecules. Bottom: calculated lattice constants and cohesive energies
of diﬀerent phases of Na, Al, and Si. The cohesive energies corresponding to the most stable structure are shown in bold.

C-C length [Å] C-H length [Å] H-C-C angle [degree]
DFT TeaNet DFT TeaNet DFT

TeaNet

Acetylene (C2H2)
Ethene (C2H4)
Ethane (C2H6)
Benzene (C6H6)
Cyclohexene (C6H12)

1.21
1.33
1.53
1.40
1.53

1.21
1.34
1.53
1.40
1.55

1.07
1.09
1.10
1.09
1.10

1.06
1.09
1.10
1.09
1.10

180◦
122◦
112◦
120◦
110◦

180◦
121◦
112◦
120◦
110◦

Lattice constant [Å] Cohesive energy [eV/atom]
DFT
DFT

TeaNet

TeaNet

5.30
4.22
7.62
4.05
3.23
6.05
3.91
3.17
5.47

5.39
4.30
7.29
4.11
3.26
6.30
4.26
3.37
5.47

1.10
1.09
0.76
3.42
3.27
2.79
3.97
3.93
4.64

1.16
1.15
0.77
3.43
3.38
2.75
4.43
4.40
4.76

Al

Na FCC
BCC
Diamond
FCC
BCC
Diamond
FCC
BCC
Diamond

Si

5.4. Amorphous silicon dioxide

Since SiO2 amorphous structure has various bond angles and various coordination numbers, it is treated
as a benchmark of the IPs [45, 29]. Amorphous SiO2 conﬁguration including 648 atoms is obtained by a
melt-quench process. The obtained structure and the partial radial distribution functions are shown in Fig.
17. The result is in good agreement with those of previous studies [46, 29]. Detailed comparison for silica
polymorphs with the other IPs [45, 47] are shown in Appendix Appendix .4.

5.5. Properties of water

Water is ubiquitous in chemistry and biochemistry. Atomistic simulation of polar and protic solvent is,
therefore, essential for chemistry, biochemistry and electrochemistry. First, the ice (ice Ih) crystal structure
was created. The calculated density of ice at 200 K was 0.93 g/cm3. Second, liquid properties were investigated.
As an initial structure, an MD cell having 512 H2O molecules was prepared. It was melted at 800 K for 1 ps
under NVT ensemble and then annealed at 300 K and 1 bar for 3 ns under NPT ensemble. The density of
liquid water was 1.00 g/cm3. These values are in good agreement with the experimental values (0.92 g/cm3
at 200 K, 1.00 g/cm3 at 300 K), and we conﬁrmed that the density of water is higher than that of ice [50].
Figure 17 shows a snapshot of TeaNet simulation of a system of water molecules at 300 K, and the partial
radial distribution function (RDF) of water predicted by our model compared to the experiment [48]. It is
noted that there are IPs which can reproduce the liquid water and ice structures. For example, the calculated
density of liquid water and ice using ReaxFF potential [49] are 1.01 g/cm3 and 0.96 g/cm3, respectively. In
addition, O–O partial RDF of ReaxFF potential is shown in Fig. (17).

Another important property of water is its high dielectric constant. In MD simulation, the dielectric constant

23

Figure 17: Top left: obtained SiO2 amorphous structure. Top right: comparison of partial radial distribution function of
amorphous SiO2 with DFT[46] and conventional IP[29]. Bottom left: snapshot of water. Bottom right: partial radial distribution
function of water at 300 K. The experimental data is derived from the merged X-ray and neutron scattering data [48]. It is
noted that the intramolecular bonds of H2O (within 1.20 Å for O–H and 1.77 Å for H–H) are not shown. For O–O, ReaxFF
potential result [49] is also shown.

(cid:15) can be calculated from the ﬂuctuation of the total dipole moment by [51]

(cid:15) = 1 +

4π
3V kBT

(cid:16)(cid:10)M 2(cid:11) − (cid:104)M (cid:105)2(cid:17)

,

(19)

where M , V , kB, and T are the dipole moment, volume, Boltzmann constant, and temperature, respectively.
(cid:104)(cid:105) corresponds to the time average operation. The dipole moment of a single H2O molecule is set to 1.8546
Debye in this simulation. The calculated dielectric constant was around 52 (Experimental value: 78 at 298 K
[50]).

In this simulation, the calculation speed was about 0.14 second/step for 1536 atoms (512 H2O molecules)
using single NVIDIA Titan V GPU.

24

051015202530350123456Si-OSi-SiO-OPartial RDF g(r)r [Angstrom]DFTconventional IPTeaNet01234567012345H-HO-HO-OPartial RDF g(r)r [Angstrom]Exp.TeaNetReaxFF5.6. Ion dissociation and the Grotthuss proton diﬀusion mechanism

Next, we investigate ion dissociation, proton transport, and the Grotthuss mechanism by simulating HCl
in H2O. As a result, the HCl molecule dissociated and a single Cl atom and H3O molecule were created.
Here, Cl and H3O are shown without +/- signs because the charge deviation eﬀect cannot be extracted
explicitly. After this, occasionally one H atom in the H3O was observed to hop to another neighboring O
atom, as shown in Figure 18. This proton transfer process, known as the Grotthuss mechanism, plays an
important role in proton diﬀusion. But previously there was no bonded IP that can reproduce the Grotthuss
mechanism. In TeaNet MD, the calculated eﬀective diﬀusion coeﬃcient of H3O is 1.5 Å2/ps, which is in good
agreement with the previous DFT study (DFT: 1.3 Å2/ps, experiment: 0.93 Å2/ps [52]). It should be noted
that ReaxFF potential [49] can reproduce the diﬀusion coeﬃcient (1.0 Å2/ps). The ﬁgures focusing on the Cl
atom is shown in Appendix Appendix .5.

Figure 18: Snapshots of hopping of H between H2O molecules. H, 2-coordinate O, and 3-coordinate O are shown by blue, yellow,
and green spheres, respectively. (Left): In water, H in H2O and H3O are oriented to neighboring O atoms. (Left to middle): An
H in H3O hopped to another O. (Middle to right): Another H in the H3O molecule hopped to the other H2 O molecule. As a
whole, these events were considered as the Grotthuss diﬀusion of H.

6. Conclusion

In this paper, we provided a uniﬁed view of GCN and physics-based interatomic potentials. Based on the
ﬁndings, we proposed a new network model, named the tensor embedded atom network (TeaNet). In this
network, the graph convolution is associated with EAM potential and the stacked network model is associated
with the iterative electronic total energy relaxation calculation. The Euclidean vectors and tensor values are
incorporated into the model to reproduce the propagation of orientation-dependent Hamiltonian information.
TeaNet mimics the information ﬂow of nonlinear iterative electronic relaxations (truncating at 5 iterations at
present). The proposed model shows great performance for the ﬁrst 18 elements on the periodic table (H
to Ar) even for highly disordered structures. We showed that it can reproduce a diverse range of material
properties including C-H molecular structures, metals, amorphous SiO2, liquid water and ice.

Data availability

The raw data required to reproduce these ﬁndings are available to download from [Code Ocean (https://codeocean.com/),
data available if this manuscript is accepted, or upon request during the review process]. The processed data
required to reproduce these ﬁndings are available to download from [Code Ocean (https://codeocean.com/),
data available if this manuscript is accepted, or upon request during the review process].

25

Conﬂict of interest

Authors declare that there are no conﬂicts of interest.

Acknowledgments

JL acknowledges support from the US DOE Oﬃce of Nuclear Energy’s NEUP Program under Grant No.
DE-NE0008751. ST acknowledges support from a Grant-in-Aid for JSPS Fellows. We thank Zhe Shi and
David Allan Bloore for commenting on the manuscript.

Contributions

ST performed the research. SI and JL discussed results and revised the manuscript.

References

[1] Murray S. Daw and M. I. Baskes. Embedded-atom method: Derivation and application to impurities,

surfaces, and other defects in metals. Phys. Rev. B, 29:6443–6453, Jun 1984.

[2] J. Tersoﬀ. Modeling solid-state chemistry: Interatomic potentials for multicomponent systems. Phys.

Rev. B, 39:5566–5568, Mar 1989.

[3] So Takamoto, Takahiro Yamasaki, Jun Nara, Takahisa Ohno, Chioko Kaneta, Asuka Hatano, and Satoshi
Izumi. Atomistic mechanism of graphene growth on a sic substrate: Large-scale molecular dynamics
simulations based on a new charge-transfer bond-order type potential. Phys. Rev. B, 97:125411, Mar
2018.

[4] Jörg Behler and Michele Parrinello. Generalized neural-network representation of high-dimensional

potential-energy surfaces. Phys. Rev. Lett., 98:146401, Apr 2007.

[5] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message

passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.

[6] Kristof T Schütt, Farhad Arbabzadah, Stefan Chmiela, Klaus R Müller, and Alexandre Tkatchenko.
Quantum-chemical insights from deep tensor neural networks. Nature communications, 8:13890, 2017.

[7] Kristof Schütt, Pieter-Jan Kindermans, Huziel Enoc Sauceda Felix, Stefan Chmiela, Alexandre
Tkatchenko, and Klaus-Robert Müller. Schnet: A continuous-ﬁlter convolutional neural network for
modeling quantum interactions. In Advances in Neural Information Processing Systems, pages 992–1002,
2017.

[8] A. Bartók-Pártay. The Gaussian Approximation Potential: An Interatomic Potential Derived from First

Principles Quantum Mechanics. Springer Theses. Springer Berlin Heidelberg, 2010.

[9] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural network

model. IEEE Transactions on Neural Networks, 20(1):61–80, Jan 2009.

[10] Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geometric
deep learning: going beyond euclidean data. IEEE Signal Processing Magazine, 34(4):18–42, 2017.

[11] Aditya Grover, Aaron Zweig, and Stefano Ermon. Graphite: Iterative generative modeling of graphs.

arXiv preprint arXiv:1803.10459, 2018.

[12] J Li, L Porter, and S Yip. Atomistic modeling of ﬁnite-temperature properties of crystalline beta-sic - ii.

thermal conductivity and eﬀects of point defects. J. Nucl. Mater., 255:139–152, 1998.

26

[13] XF Qian, J Li, L Qi, CZ Wang, TL Chan, YX Yao, KM Ho, and S Yip. Quasiatomic orbitals for ab

initio tight-binding analysis. Phys. Rev. B, 78:245112, 2008.

[14] XF Qian, J Li, X Lin, and S Yip. Time-dependent density functional theory with ultrasoft pseu-
dopotentials: Real-time electron propagation across a molecular junction. Phys. Rev. B, 73:035408,
2006.

[15] Linfeng Zhang, Jiequn Han, Han Wang, Roberto Car, and Weinan E. Deep potential molecular dynamics:
A scalable model with the accuracy of quantum mechanics. Phys. Rev. Lett., 120:143001, Apr 2018.

[16] Oliver T. Unke and Markus Meuwly. Physnet: A neural network for predicting energies, forces, dipole
moments, and partial charges. Journal of Chemical Theory and Computation, 15(6):3678–3693, 2019.
PMID: 31042390.

[17] Yaolong Zhang, Ce Hu, and Bin Jiang. Embedded atom neural network potentials: Eﬃcient and accurate
machine learning with a physically inspired representation. The Journal of Physical Chemistry Letters,
10(17):4962–4967, 2019. PMID: 31397157.

[18] MC Payne, MP Teter, DC Allan, TA Arias, and JD Joannopoulos. Iterative minimization techniques
for abinitio total-energy calculations - molecular-dynamics and conjugate gradients. Rev. Mod. Phys.,
64:1045–1097, 1992.

[19] CZ Wang, GD Lee, J Li, S Yip, and KM Ho. Atomistic simulation studies of complex carbon and silicon
systems using environment-dependent tight-binding potentials. Sci. Model. Simul., 15:97–121, 2008.

[20] CZ Wang, WC Lu, YX Yao, J Li, S Yip, and KM Ho. Tight-binding hamiltonian from ﬁrst-principles

calculations. Sci. Model. Simul., 15:81–95, 2008.

[21] Risi Kondor, Zhen Lin, and Shubhendu Trivedi. Clebsch–gordan nets: a fully fourier space spherical
convolutional neural network. In Advances in Neural Information Processing Systems, pages 10117–10126,
2018.

[22] Brandon Anderson, Truong Son Hy, and Risi Kondor. Cormorant: Covariant molecular neural networks.

In Advances in Neural Information Processing Systems, pages 14510–14519, 2019.

[23] Nathaniel Cabot Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoﬀ, and Patrick
Riley. Tensor ﬁeld networks: Rotation- and translation-equivariant neural networks for 3d point clouds,
2018.

[24] Alexander V. Shapeev. Moment tensor potentials: A class of systematically improvable interatomic

potentials. Multiscale Modeling & Simulation, 14(3):1153–1173, 2016.

[25] Djork-Arné Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning

by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.

[26] Anthony K. Rappe and William A. Goddard. Charge equilibration for molecular dynamics simulations.

The Journal of Physical Chemistry, 95(8):3358–3363, 1991.

[27] Frank H. Stillinger and Thomas A. Weber. Computer simulation of local order in condensed phases of

silicon. Phys. Rev. B, 31:5262–5271, Apr 1985.

[28] J. Tersoﬀ. New empirical approach for the structure and energy of covalent systems. Phys. Rev. B,

37:6991–7000, Apr 1988.

[29] So Takamoto, Tomohisa Kumagai, Takahiro Yamasaki, Takahisa Ohno, Chioko Kaneta, Asuka Hatano,
and Satoshi Izumi. Charge-transfer interatomic potential for investigation of the thermal-oxidation
growth process of silicon. Journal of Applied Physics, 120(16):165109, 2016.

27

[30] Nicolas Papernot, Martín Abadi, Úlfar Erlingsson, Ian J. Goodfellow, and Kunal Talwar. Semi-supervised
knowledge transfer for deep learning from private training data. In 5th International Conference on
Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.
OpenReview.net, 2017.

[31] LX He and D Vanderbilt. Exponential decay properties of wannier functions and related quantities.

Phys. Rev. Lett., 86:5341–5344, 2001.

[32] Adri C. T. van Duin, Siddharth Dasgupta, Francois Lorant, and William A. Goddard. Reaxﬀ: A reactive

force ﬁeld for hydrocarbons. The Journal of Physical Chemistry A, 105(41):9396–9409, 2001.

[33] Jianguo Yu, Susan B. Sinnott, and Simon R. Phillpot. Charge optimized many-body potential for the

Si/sio2 system. Phys. Rev. B, 75:085311, Feb 2007.

[34] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.

[35] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Computer Vision

(ICCV), 2017 IEEE International Conference on, pages 2980–2988. IEEE, 2017.

[36] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation system:
Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.

[37] Aaron Van Den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal
Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio.
arXiv preprint arXiv:1609.03499, 2016.

[38] Yiping Lu, Aoxiao Zhong, Quanzheng Li, and Bin Dong. Beyond ﬁnite layer neural networks: Bridging

deep architectures and numerical diﬀerential equations. arXiv preprint arXiv:1710.10121, 2017.

[39] Bo Chang, Lili Meng, Eldad Haber, Lars Ruthotto, David Begert, and Elliot Holtham. Reversible
architectures for arbitrarily deep residual neural networks. In Thirty-Second AAAI Conference on
Artiﬁcial Intelligence, 2018.

[40] Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary diﬀerential

equations. In Advances in neural information processing systems, pages 6571–6583, 2018.

[41] Anubhav Jain, Shyue Ping Ong, Geoﬀroy Hautier, Wei Chen, William Davidson Richards, Stephen Dacek,
Shreyas Cholia, Dan Gunter, David Skinner, Gerbrand Ceder, and Kristin A. Persson. Commentary: The
materials project: A materials genome approach to accelerating materials innovation. APL Materials,
1(1):011002, 2013.

[42] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980, 2014.

[43] Matthias Rupp, Alexandre Tkatchenko, Klaus-Robert Müller, and O. Anatole von Lilienfeld. Fast and
accurate modeling of molecular atomization energies with machine learning. Phys. Rev. Lett., 108:058301,
Jan 2012.

[44] Raghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole Von Lilienfeld. Quantum

chemistry structures and properties of 134 kilo molecules. Scientiﬁc data, 1:140022, 2014.

[45] Shinji Munetoh, Teruaki Motooka, Koji Moriguchi, and Akira Shintani. Interatomic potential for si–o

systems using tersoﬀ parameterization. Computational Materials Science, 39(2):334–339, 2007.

[46] Johannes Sarnthein, Alfredo Pasquarello, and Roberto Car. Model of vitreous sio 2 generated by an ab

initio molecular-dynamics quench from the melt. Physical Review B, 52(17):12690, 1995.

28

[47] Adri C.T. Van Duin, Alejandro Strachan, Shannon Stewman, Qingsong Zhang, Xin Xu, and William
A. Goddard. Reaxﬀsio reactive force ﬁeld for silicon and silicon oxide systems. Journal of Physical
Chemistry A, 107(19):3803–3811, May 2003.

[48] Alan K Soper. The radial distribution functions of water as derived from radiation total scattering

experiments: Is there anything we can say for sure? ISRN Physical Chemistry, 2013, 2013.

[49] Weiwei Zhang and Adri C.T. Van Duin. Second-generation reaxﬀ water force ﬁeld: Improvements in the
description of water density and oh-anion diﬀusion. Journal of Physical Chemistry B, 121(24):6021–6032,
June 2017.

[50] William M Haynes. CRC handbook of chemistry and physics. CRC press, 2014.

[51] Martin Neumann. Dipole moment ﬂuctuation formulas in computer simulations of polar systems.

Molecular Physics, 50(4):841–858, 1983.

[52] Mauro Boero, Tamio Ikeshoji, and Kiyoyuki Terakura. Density and temperature dependence of proton
diﬀusion in water: A ﬁrst-principles molecular dynamics study. ChemPhysChem, 6(9):1775–1779, 2005.

[53] Th Demuth, Y Jeanvoine, J Hafner, and JG Angyan. Polymorphism in silica studied in the local density
and generalized-gradient approximations. Journal of Physics: Condensed Matter, 11(19):3833, 1999.

29

Appendix

Appendix .1. Examples on the weak correlation between bond length and interactions of atoms

With ﬁnite radial cutoﬀ distance, using bond length information only sometimes makes it hard to estimate
the interactions of atoms. The simple example is ethylene. The rotation of C–C bond is ﬁxed because of
pi-bonding. However, with respect to C–C bond rotation, all angles of the chemical bonds which share the
same atom do not change. This is interpreted as the dihedral angle interaction which has important role in
organic molecules. It is noted that if the cutoﬀ distance is long enough, there can be seen a diﬀerence in
H–H distance where two hydrogen atoms are connected to the other side of C atoms. However, the change of
H–H distance with respect to the rotation of C–C bond is relatively subtle. In addition, in this case, the
length-based method should represent the pi-bond interaction as the distance of H–H length, while there is
little direct interaction between them.

Figure .19: Schematic illustration of the rotation of C–C bond in ethylene. Only H-H distance corresponding opposite site
(illustrated by dotted line) is diﬀerent.

Another example is small cluster consisting of three atoms arranged in an equilateral triangle. Accounting
the nearest neighbor atoms only, the numbers of neighbor atoms are identical to the inﬁnite chain structure.
It means that the length-based model with short cutoﬀ distance cannot tell whether the structure is triangle
or chain no matter how many the convolution layer is, while their bond angles are quite diﬀerent. In other
words, the length-based model should represent the angle-dependent interaction by the existence of the second
nearest neighbor atoms.

Figure .20: Schematic illustration of triangle cluster and inﬁnite chain. They have the same connectivity.

30

CCHHHHCCHHHHCABCAPeriodic boundariesABCAppendix .2. The necessity of Rank-2 tensors and its physical meaning

Rank-2 tensors are essential to express the edge-edge interaction through their angle by graph convolution
operation. This can be demonstrated in the following example. Let the nodes and edges contain only vector
values, and suppose that two edges are connected to a center node, that has point-group symmetry as shown
in Fig. .21. After the convolution, the summed vector values at the node are always 0, and the node loses
its directional information. If the third edge is connected to the node, no angle dependence is represented.
However, if the second-order tensor values are introduced, the point-group symmetric edge pairs have identical
(no sign reversal) tensor values; therefore, the directional information can be accumulated on the node. It
should be noted that the vector and tensor values are not merely mathematical tricks but express various
physical quantities related to the electronic structure. For example, the local charge deviation is expressed by
the electric dipole moment. Since the electron orbit of a π bond extends perpendicularly to the bond direction,
the dihedral bending is prevented. Polarizability can be expressed by tensor as well. These properties can be
naturally expressed using the vector and tensor variables. Higher-order tensor values can also be introduced
in the same manner.

Figure .21: Example of the vanishment of directional information when convoluting with vector values only. If a pair of atoms
(shown in dark green circles) having the same properties are located on opposite sides of the center atom (shown in orange
circle), any vector values summed at the center atom will vanish. Thus, the angular-dependent interaction between another
neighbor atom (shown in white circle) and dark green atoms, corresponding to θ1 and θ2, cannot be incorporated in the model.

The spherical harmonic-based methods [21, 22] also have the ability to represent the higher-order geometric
information while holding the rotation and translation invariances from the perspective of SO(3) group. It
should be noted that the tensor product-based representation and the spherical harmonics-based representation
are diﬀerent. The signiﬁcant diﬀerence appears in mirror transformation. Since SO(3) do not hold mirror
transformation invariance, the corresponding calculations behave diﬀerent under mirror-symmetry structure.
On the other hand, our method outputs the same value in any O(n) transformations.

One of this example is outer product of two vectors. Since outer product operation introduces pseudo-vector
and breaks mirror symmetry, the tensor product-based method cannot reproduce outer product operation.
Instead, Rank-2 rotation tensor which holds O(n) symmetry can be created. This characteristic introduces
the desired inductive bias in the ﬁeld of physics.

31

Appendix .3. Numerical experiments using existing dataset and applicability of other models for our dataset

Although our aim is to reproduce the potential energy of highly disordered atomic conﬁgurations, we also
evaluated our model for datasets of locally stable atomic conﬁgurations. First, the QM9 dataset was used.
Since QM9 contains only stable structures, it is possible to increase accuracy by retraining. We retrained
the four-layer version of the network with the stochastic gradient descent (SGD) optimizer while gradually
decreasing the learning rate. The mean squared error of the energy was used as the loss. In this case, we use
the original QM9 validation dataset as the test dataset. The MAE of the energy was 13 meV per molecule
(1.2 meV/atom) among the QM9 validation dataset. This is similar to the current top scores (14 meV [7], 8
meV [16]), and the other methods (19-130 meV) [5]. It is noted that the error of the dataset with locally
stable structures is one magnitude smaller than that of highly disordered structures shown in Table 2.

Second, the Materials Project molecule dataset, which consists of elements in the ﬁrst three rows of the
periodic table, was used. We recalculated the energy of the dataset by DFT to adjust the diﬀerence in the
method of DFT. We trained the network in the same way as with QM9. The resulting MAE of the energy
was 3.1 meV/atom. Our model well succeeds in estimating the energy of locally stable atomic conﬁgurations.
It is noted that our model does not require the bond types as the input and that we use a relatively short
cutoﬀ distances (6 Å).

For further comparison, we discuss the applicability of the other models for our highly disordered dataset.
First, we would like to note that symmetry function-based methods (BPNN [4] and its derivations) is not
suitable for this task. Since symmetry function explicitly treats the three-body term of each element, the
number of parameters increases dramatically by increasing the number of elements in the dataset. This
behavior makes it hard to train the model.

On the other hand, GNN-based model can be applied to this dataset. We use SchNet [7] as the current
length-based milestone method. To focus on the reproducibility of dynamics properties, we mainly focus on
the atomic forces. We use SchNetPack for the evaluation. The speciﬁed parameters are below. To align the
conditions, we set the number of layer to 4 (original model: 3 and 6) and the cutoﬀ distance to 6.0 Å. We set
the parameter ρ (weight of losses) for the energy and the force to 0.001 and 0.999, respectively. The result is
shown in Table .5.

Table .5: Model comparison using proposed highly disordered dataset.

# params

Energy MAE
[meV/atom]

Force MAE
[eV/Å]

SchNet [7]
ours

310, 000
235, 000

29.7
23.9

0.638
0.167

There is a large diﬀerence on estimating atomic forces between SchNet and ours. It should be noted that the
structures of this dataset is not limited to speciﬁc molecular systems. It means the model is requested to
reproduce the properties of structures which is not supposed to exist in the training dataset. This task will
be further diﬃcult as compared to the existing dynamics benchmarks using speciﬁc molecular systems.

32

Appendix .4. Silica polymorphs reproducibility

Additional experiments for the reproducibility of silica polymorphs (α-quartz, α-cristobalite, β-tridymite,
stishovite) are carried out. The result is shown in table .6. The snapshots are shown in Fig. .22. Overall, our
model well reproduces the silica polymorphs including the diﬀerence of the energies and the densities. In
addition, our model well estimates the diﬀerence of the energy of stishovite, which Tersoﬀ-type potential
estimates two times larger. In stishovite crystal structure, one Si atom is connected to 6 O atoms and one O
atom is connected to 3 Si atoms. It means that the local environment of each atom is far from the usual
tetrahedral silica crystal structures. It indicates that our model is robust for the change of local environments
of atoms.

Table .6: Calculated cohesive energy, relative energy to α-quartz, and density of silica polymorphs. a-Q, a-C, and b-T correspond
to α-quartz, α-cristobalite, β-tridymite, respectively.

DFT [53]

Tersoﬀ [45]

ReaxFF [47]

Ours

Crystal

a-Q
a-C
b-T
stishovite
a-Q
a-C
b-T
stishovite
a-Q
a-C
b-T
stishovite
a-Q
a-C
b-T
stishovite

Cohesive energy
[eV/atom]

Relative energy
[eV/atom]

Density
[g/cm3]

−0.011
−0.008
0.212

0.001
0.002
0.502

0.001
−0.006
0.279

0.003
0.009
0.254

2.48
2.13
2.06
4.11
2.42
2.16
2.08
3.89
2.55
2.22
2.09
4.29
2.44
2.19
1.92
4.12

7.942
7.953
7.950
7.730
6.698
6.697
6.696
6.196
-
-
-
-
6.720
6.717
6.711
6.466

33

Figure .22: Illustration of SiO2 crystals (1) α-quartz. (2) α-cristobalite. (3) β-tridymite. (4) stishovite. The optimized structure
using TeaNet are shown. It is noted that Si and O atoms in stishovite have 6 neighboring atoms and 3 neighboring atoms,
respectively.

34

(1)(2)(3)(4)Appendix .5. Cl atom observation in water

In the simulation of ion dissociation and proton diﬀusion of water, one HCl molecule was added into H2O. In
this section, the behavior of Cl atom was observed. The snapshots are shown in Fig. .23. As the HCl molecule
dissociated in the water, the individual Cl atom was observed during the MD simulation. The interaction of
Cl atom and surrounding water molecules was also shown. Although they are not bonded strictly, H atoms in
the surrounding water molecule tend to get closer to the Cl atom. This is in good agreement with the picture
of anions in water. It should be noted that those eﬀects were reproduced without preparing any explicit
water-Cl DFT simulations in advance.

Figure .23: (1), (2), and (3): Snapshots of water in which HCl molecule was inserted. The green sphere corresponds to Cl atom.
H3O molecule can be found in the blue box shown in (2). (4): Close snapshot of (3). H-Cl long-range bonds are also shown
(visual cutoﬀ distance for H-Cl was set to 3 Å).

35

(1)(2)(3)(4)Appendix .6. Details of dataset

The details of the test dataset is shown. Since it was made by randomly selecting from the entire dataset, it
can be considered to reﬂect the trend of the entire dataset.

Table .7 shows the amount of each element in the test dataset. The calculated energy is also shown. The
structures of the ﬁrst 20 samples in table .7 are shown in Fig. .24. Table .8 shows the number of pairs in the
test dataset. As described in the main text, the dataset consists of highly disordered structures.

Table .7: Content of the test dataset. The number of each element
in the test dataset is shown. E corresponds to the total energy
of the system calculated by DFT. The zero point of the energy is
deﬁned as the sum of the energies of atoms separated in a vacuum.
The unit of energy is eV.

H He Li Be B

C N O F Ne Na Mg Al

Si

P

S Cl Ar

E

E /atom

0
1
0
0
0
0
0
0
0
0
0
3
0
0
2
16
9
0
0
0
0
2
1
2
0
0
0
10
1
1
1
0
2
0
0
4
1

1
0
1
1
0
0
0
0
0
1
0
0
0
0
0
0
6
0
0
0
0
0
2
1
0
0
0
0
1
1
1
3
0
0
0
1
1

1
0
1
1
8
0
0
0
0
0
0
2
1
4
2
0
10
9
0
1
8
3
2
2
0
0
16
0
0
0
1
1
2
0
0
2
0

0
0
1
1
0
0
0
0
0
2
0
0
0
0
0
0
12
0
2
0
0
1
0
1
0
0
0
0
2
0
0
0
2
0
0
2
1

0
0
2
0
0
0
1
1
0
0
0
0
1
0
1
0
7
0
0
1
0
1
0
0
0
0
0
0
1
1
2
0
0
0
1
0
2

3
0
2
1
0
0
1
0
0
0
0
1
0
0
3
0
12
0
1
0
0
1
1
0
1
0
0
12
0
0
1
0
2
0
0
0
1

0
0
0
2
0
3
2
1
0
1
0
0
0
0
0
0
9
0
1
0
0
1
1
0
0
0
0
2
1
1
0
1
0
0
1
1
1

1
2
2
1
0
1
0
0
0
0
0
0
1
0
1
0
3
9
0
0
0
0
1
0
0
0
0
0
2
1
0
3
0
0
1
1
0

3
1
2
1
0
0
4
0
0
1
0
2
1
0
0
32
7
0
1
0
8
2
0
3
0
0
0
0
1
1
2
0
1
0
0
1
0

1
0
1
1
0
1
0
0
9
0
2
1
2
4
0
0
9
0
0
0
0
0
0
3
0
1
0
0
0
0
1
1
2
9
1
0
1

0
1
0
1
0
0
0
0
9
0
0
0
1
0
0
16
6
0
0
0
0
0
0
0
0
0
0
0
2
0
2
0
2
0
0
1
2

36

0
1
1
1
0
1
1
0
0
1
0
0
4
0
1
0
10
0
2
0
0
0
3
0
0
15
0
0
1
0
1
2
0
0
2
1
0

3
0
2
0
8
0
2
0
0
2
0
0
1
0
1
0
5
0
0
0
0
3
1
2
0
0
0
0
1
0
0
0
2
0
0
1
1

0
0
0
1
0
0
1
1
0
0
1
1
1
0
1
0
3
0
0
0
0
0
0
0
0
0
0
0
1
1
2
0
1
0
1
0
1

0
0
1
1
0
1
2
0
0
1
1
3
0
0
3
0
3
0
0
0
0
1
0
1
0
0
0
0
1
2
1
1
2
9
0
1
1

1
1
0
1
0
0
0
2
0
0
0
1
0
0
0
0
7
0
1
0
0
1
0
1
0
0
0
0
0
0
1
2
0
0
0
0
2

2
0
0
1
0
1
1
2
0
0
0
2
0
0
0
0
6
0
0
14
0
0
2
0
0
0
0
0
1
1
1
2
0
0
1
0
0

2
−32.57
1
−22.14
0
−58.83
1
−53.44
0
−30.25
0
−18.42
1
−64.72
1
−22.55
0
0.97
1
−25.22
0
−2.54
0
−47.47
3
−26.56
0
−2.99
−55.86
1
0 −197.82
4 −394.97
−62.79
0
−23.62
0
−28.70
0
−58.89
0
−48.22
0
−27.45
2
−41.19
0
7
1.25
−0.22
0
0
−28.89
0 −117.52
−56.39
0
−36.41
0
−45.35
1
−36.98
0
−34.90
2
−20.54
0
−22.94
0
−35.49
0
−32.36
3

−1.81
−2.77
−3.68
−3.34
−1.89
−2.30
−4.05
−2.82
0.05
−2.52
−0.63
−2.97
−1.66
−0.37
−3.49
−3.09
−3.09
−3.49
−2.95
−1.79
−3.68
−3.01
−1.72
−2.57
0.16
−0.01
−1.81
−4.90
−3.52
−3.64
−2.52
−2.31
−1.74
−1.14
−2.87
−2.22
−1.80

H He Li Be B

C N O F Ne Na Mg Al

Si

1
4
0
2
0
1
1
0
0
0
0
0
0
1
1
1
5
0
0
6
0
2
0
0
0
0
0
0
0
0
0
2
0
1
5
0
0
2
0
1
0
8
1
1
0
0
0
0
11
0
6
1

0
5
2
0
0
2
2
0
0
0
0
0
0
0
0
1
5
0
0
0
0
0
0
0
2
0
1
0
2
0
0
0
3
0
6
0
0
1
0
1
1
0
0
1
1
2
0
0
8
0
0
0

1
2
1
0
0
0
0
0
15
2
0
0
0
0
0
0
4
0
0
10
0
1
0
0
2
0
2
0
1
0
0
0
1
1
7
0
0
1
0
2
1
0
1
3
0
1
8
0
8
0
0
0

0
2
1
0
0
0
0
0
0
2
0
0
0
0
1
1
4
0
0
0
0
0
0
0
2
0
1
0
0
0
9
0
3
1
10
0
0
1
1
2
1
0
1
1
1
1
0
0
6
0
0
1

0
4
1
0
2
0
0
0
0
0
4
1
0
1
0
0
3
0
0
0
1
1
0
0
1
8
0
0
2
0
0
0
0
0
8
0
16
0
1
3
1
0
0
0
0
1
16
0
8
0
0
0

2
5
1
0
0
2
1
0
0
1
0
0
0
0
2
1
3
16
0
0
0
1
0
0
0
0
0
0
1
0
0
1
0
0
12
0
0
1
0
0
0
8
3
0
2
1
0
0
11
0
8
1

1
3
0
0
0
2
0
0
0
1
0
1
0
1
0
2
3
0
0
0
1
1
0
8
2
0
0
0
0
0
9
1
0
0
14
0
0
1
0
0
0
0
5
1
1
1
8
0
6
0
2
1

0
5
0
1
0
2
0
0
0
0
0
1
0
0
0
1
6
0
0
0
0
0
0
0
1
0
2
0
3
1
0
1
0
1
2
0
0
0
2
0
3
0
0
2
1
0
16
0
3
0
0
0

0
4
1
0
0
1
0
0
0
2
0
0
8
0
0
1
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
2
0
6
6
0
0
1
1
0
0
1
0
1
1
0
0
6
0
0
0

0
1
0
0
0
1
1
0
0
3
0
1
0
2
1
0
5
0
0
0
0
1
9
0
1
0
0
0
2
0
0
1
2
0
3
0
0
0
1
0
1
0
1
0
0
1
16
15
6
16
0
2

0
2
1
0
0
0
0
0
0
0
0
1
0
1
0
1
1
0
4
0
2
1
0
0
1
0
0
0
1
2
0
0
0
1
8
0
0
1
0
0
1
0
1
2
0
1
0
0
6
0
0
3

0
1
1
0
0
1
0
0
0
0
0
0
0
0
1
2
3
0
0
0
0
1
0
0
1
8
0
0
1
0
0
0
1
0
9
1
0
2
2
0
0
0
1
0
0
2
8
0
5
0
0
2

0
6
1
2
0
1
1
0
0
0
0
0
8
1
0
0
4
0
0
0
0
0
0
8
2
0
0
1
1
0
0
0
0
0
10
0
0
1
0
0
1
0
0
1
0
0
8
0
12
0
0
0

1
2
0
0
0
0
0
0
1
0
4
1
0
0
0
0
4
0
4
0
0
1
0
0
0
0
0
0
0
0
0
1
0
2
5
0
0
3
1
1
0
0
1
1
1
0
0
0
9
0
0
0

37

P

0
2
0
0
0
1
1
0
0
0
0
0
0
0
1
0
0
0
0
4
0
0
0
0
0
0
0
0
1
0
0
2
2
0
6
0
0
0
3
0
0
0
1
0
0
0
32
0
6
0
0
1

S Cl Ar

E

E /atom

2
9
0
0
0
0
1
10
0
1
0
1
0
0
0
0
3
0
0
0
0
1
0
0
0
0
2
1
1
0
0
2
1
1
9
0
0
1
0
2
0
0
1
1
2
1
8
1
4
0
0
2

0
5
0
2
0
2
0
10
0
3
0
1
0
1
0
0
4
0
0
0
0
1
9
0
1
0
0
1
2
0
0
1
1
0
3
1
0
0
0
2
0
0
1
1
1
0
8
0
6
0
0
0

0
−25.29
2 −220.63
−23.16
0
−10.70
1
−3.60
0
−60.44
0
−11.70
0
−41.19
0
−31.25
0
−44.95
1
−21.54
0
−26.63
0
−5.74
0
−10.77
0
−25.65
1
1
−46.32
6 −206.37
−22.43
0
−16.87
0
−36.95
0
−13.41
0
−34.68
0
−45.46
0
−41.27
0
−43.28
0
−80.56
0
−24.73
0
−1.31
1
−61.66
2
−10.50
1
−71.42
0
−42.65
4
−39.80
0
0
−22.08
5 −427.13
−15.13
0
−88.14
0
−39.07
1
−42.05
0
−39.07
1
−32.15
0
−71.87
0
−66.54
1
−46.73
1
−38.19
1
3
−46.10
0 −461.06
−44.16
0
7 −374.90
−39.70
0
−75.53
0
−56.83
2

−3.16
−3.45
−2.32
−1.34
−1.80
−3.78
−1.46
−2.06
−1.95
−2.81
−2.69
−3.33
−0.36
−1.35
−3.21
−3.86
−3.22
−1.40
−2.11
−1.85
−3.35
−2.89
−2.53
−2.58
−2.71
−5.03
−3.09
−0.33
−3.08
−2.62
−3.97
−2.67
−2.49
−2.76
−3.34
−1.89
−5.51
−2.44
−3.50
−2.44
−3.22
−4.49
−3.33
−2.92
−3.18
−2.88
−3.60
−2.76
−2.93
−2.48
−4.72
−3.55

H He Li Be B

C N O F Ne Na Mg Al

Si

1
0
0
0
0
1
4
0
0
0
0
2
14
0
0
0
4
0
0
3
1
0
16
2
0
0
0
8
0
0
0
1
0
7
1
0
0
0
0
0
7
0
1
0
1
0
0
1
3
0
0
0

1
0
0
2
0
2
0
0
0
1
0
2
7
0
1
0
7
0
4
3
3
0
8
2
2
1
0
8
0
0
0
0
1
0
0
8
0
0
0
0
0
2
2
0
2
0
2
0
0
2
0
0

0
1
0
0
0
3
0
0
0
0
8
0
4
0
0
0
7
0
0
1
1
0
16
1
0
0
0
2
0
2
0
3
0
0
1
0
0
1
0
2
0
2
1
16
0
0
0
0
1
0
0
0

0
1
0
1
10
1
0
1
0
0
0
1
0
0
0
0
4
0
0
1
2
1
0
1
0
0
0
8
0
1
0
2
0
0
2
0
0
0
0
0
0
0
0
0
2
0
0
1
0
0
0
0

0
1
0
0
0
0
0
2
0
0
0
2
2
0
0
0
4
0
0
1
0
2
0
0
0
2
0
8
8
3
7
1
0
0
2
0
2
0
0
1
0
2
1
0
1
0
1
0
0
1
1
8

0
0
0
1
10
1
4
0
0
2
0
1
5
9
0
0
4
0
0
2
2
1
0
1
0
0
0
6
0
1
0
1
0
7
1
0
1
0
0
2
0
0
0
0
1
8
0
0
1
2
0
0

1
1
8
1
0
1
2
3
0
1
0
0
13
0
0
9
7
0
0
2
2
2
8
0
0
1
0
10
0
0
1
1
0
0
1
0
1
0
0
0
9
1
2
8
1
0
0
1
0
2
0
0

2
0
0
1
0
0
2
0
0
3
8
0
8
0
0
0
3
1
0
3
1
0
8
1
0
0
0
2
0
2
0
0
0
1
0
0
1
1
0
0
4
2
1
0
3
0
1
3
0
0
0
0

0
0
0
1
0
1
0
1
0
3
0
0
10
0
0
0
3
0
0
3
1
2
0
1
0
2
0
3
0
0
0
0
0
0
1
0
2
0
0
1
0
0
0
8
1
0
0
2
1
2
0
0

0
0
0
2
0
0
0
0
0
0
0
1
5
0
0
0
3
0
0
1
0
2
24
1
0
0
0
11
0
0
0
0
0
0
2
0
1
0
0
2
0
0
2
0
2
0
2
0
1
0
0
0

0
0
0
1
0
2
0
1
4
1
0
2
9
0
0
0
3
0
0
1
1
3
8
0
0
1
0
6
0
1
0
0
0
0
2
0
1
0
0
1
0
0
1
8
1
0
0
0
0
1
0
0

2
0
0
0
0
0
0
1
0
1
0
0
8
0
7
0
0
0
0
3
1
0
0
0
0
1
0
7
0
1
0
1
0
0
1
0
0
0
0
0
0
5
0
8
0
0
2
1
0
0
0
0

1
1
0
1
0
2
0
0
4
2
0
1
7
0
0
0
5
0
0
33
0
0
24
0
0
2
0
8
0
1
0
1
6
0
0
0
1
0
0
1
0
2
0
0
2
0
1
1
1
1
0
8

1
2
0
0
0
1
0
0
0
0
0
1
5
0
0
9
2
0
0
2
1
2
0
0
0
0
0
7
0
0
0
0
0
0
1
0
0
1
9
2
0
0
0
0
0
0
1
2
0
0
0
0

38

P

0
0
0
1
0
0
0
2
0
0
0
0
11
0
0
0
2
0
4
2
0
1
0
3
16
2
0
5
0
1
0
0
1
0
1
0
0
0
0
1
0
1
0
0
2
0
1
1
2
0
0
0

S Cl Ar

E

E /atom

2
0
0
0
0
2
0
1
0
0
0
0
7
9
0
0
2
0
4
0
2
0
0
2
0
0
16
9
8
1
0
0
0
1
1
8
0
0
0
2
0
0
0
0
0
0
1
0
2
0
0
0

5
0
0
4
0
2
0
0
0
0
0
1
7
0
0
0
3
1
0
0
0
0
8
0
0
0
0
14
0
0
0
1
0
3
1
0
1
0
9
1
0
0
1
16
0
8
3
1
4
0
15
0

−32.44
0
−7.19
1
−40.96
0
−37.51
0
−93.71
0
−44.09
1
−51.94
0
−53.71
0
−11.15
0
−50.47
2
−57.67
0
2
−29.53
6 −378.64
−74.44
0
−6.89
0
0
−46.17
1 −204.67
−2.18
6
−19.82
4
−92.94
3
2
−51.51
−59.71
0
8 −266.13
−45.98
1
−44.08
0
−24.05
0
0
−47.83
6 −383.19
−76.73
0
−59.94
2
−33.89
0
−29.74
0
−0.06
0
−74.36
0
−54.96
2
−22.66
0
−31.12
1
−5.95
0
−47.85
0
−52.83
0
−60.71
0
−49.48
1
−40.21
0
0 −185.75
−61.69
1
−59.34
0
−42.74
1
−37.49
2
−39.57
0
−33.80
1
−29.37
0
−39.13
0

−2.03
−0.90
−5.12
−2.34
−4.69
−2.20
−4.33
−4.48
−1.39
−3.15
−3.60
−1.85
−2.96
−4.14
−0.86
−2.56
−3.20
−0.27
−1.24
−1.45
−2.58
−3.73
−2.08
−2.87
−2.45
−2.00
−2.99
−2.99
−4.80
−3.75
−4.24
−2.48
−0.01
−3.91
−2.75
−1.42
−2.59
−1.98
−2.66
−3.30
−3.04
−2.75
−3.35
−2.90
−3.08
−3.71
−2.67
−2.34
−2.47
−2.82
−1.84
−2.45

H He Li Be B

C N O F Ne Na Mg Al

1
0
0
13
0
9
0
0
0
0
0
0
9
0
1
0
0
0
0
1
1
0
0
0
0
8
0
0
0
1
0
0
8
0
0
1
0
0
0
5
1
0
0
0
0
1
1
0
0
1
0
1

2
0
8
0
1
0
0
1
0
0
0
2
8
0
0
0
0
0
6
1
2
0
2
1
1
0
0
0
0
1
0
0
8
0
2
0
0
0
0
6
0
8
3
0
0
3
0
8
0
0
0
0

0
16
8
0
0
0
0
0
1
0
0
0
9
9
1
0
4
0
0
0
2
1
0
0
0
0
1
2
0
0
0
8
0
0
0
3
1
0
0
6
1
0
0
0
0
0
1
16
0
1
0
0

2
0
8
0
0
0
2
1
1
0
0
0
8
0
1
0
0
0
0
0
0
1
0
0
2
0
0
1
0
0
4
16
0
0
1
1
0
0
32
7
0
16
0
0
0
2
1
0
0
0
0
3

0
16
16
0
2
0
1
0
3
0
1
1
7
0
2
18
0
0
0
0
1
0
1
0
2
0
1
1
0
1
0
16
0
15
0
0
0
0
0
6
2
8
3
0
64
1
1
0
0
0
0
0

0
0
8
7
4
0
0
1
1
0
1
1
10
0
0
0
0
16
0
0
1
0
2
0
2
0
1
0
0
1
0
8
16
0
0
0
0
0
0
6
0
0
3
0
0
1
0
8
0
1
0
0

1
0
24
1
0
0
1
1
0
0
0
1
8
0
0
0
0
0
0
0
2
0
1
0
0
0
2
1
0
1
0
8
0
0
0
1
0
0
0
7
2
0
3
0
0
1
3
0
0
0
0
0

1
0
8
1
0
0
0
0
0
0
0
1
8
0
0
0
0
0
0
0
1
1
1
0
1
8
1
1
18
0
0
16
16
0
2
1
2
0
0
9
2
0
2
0
64
1
0
8
0
0
0
0

2
0
8
0
1
0
0
1
1
0
0
3
7
9
0
0
0
0
0
0
0
0
2
1
0
0
0
1
0
0
0
0
16
1
2
1
3
0
0
5
0
0
0
0
0
0
0
16
0
1
0
1

0
0
0
0
1
0
1
0
1
0
0
0
8
0
0
0
0
32
0
0
0
0
2
0
2
0
2
0
0
2
4
8
16
0
0
0
3
0
0
12
1
8
0
0
0
1
0
0
0
0
0
0

1
0
0
0
0
0
1
0
2
0
0
1
4
0
1
0
4
0
0
0
1
1
0
0
1
0
1
0
0
2
0
8
0
0
0
0
0
0
0
5
0
0
2
0
0
3
0
0
8
1
0
1

0
0
8
0
1
0
1
0
0
0
0
1
6
0
0
0
0
0
0
1
0
0
1
0
1
0
2
1
2
3
0
16
0
0
2
1
0
0
0
13
1
0
4
0
0
0
0
0
0
2
10
0

0
0
16
0
2
0
2
0
3
0
0
1
2
0
0
0
0
0
0
1
0
1
1
0
1
0
0
2
0
0
8
0
8
0
0
3
0
0
0
8
4
0
1
0
0
0
0
8
0
2
10
0

39

Si

1
16
8
0
3
0
0
0
0
0
2
0
6
0
0
0
0
0
1
1
4
2
2
0
1
0
1
2
0
0
0
16
0
0
2
1
1
0
32
7
1
0
0
32
0
0
1
0
0
0
0
1

P

S Cl Ar

E

E /atom

0
0
8
0
0
9
1
1
1
9
2
1
5
0
0
0
0
0
0
0
0
1
0
0
1
0
0
1
0
1
0
0
0
0
0
1
2
0
0
3
1
0
6
0
0
1
0
0
0
2
0
0

2
0
0
0
1
0
1
0
1
9
0
1
10
0
1
0
0
0
0
1
3
0
0
6
1
0
1
1
0
1
0
0
16
0
1
0
2
15
0
5
1
8
1
0
0
0
2
0
8
0
0
0

1
0
0
0
0
0
0
0
1
0
0
1
8
0
1
0
0
16
1
0
2
0
1
0
0
0
1
0
0
1
0
0
16
0
3
2
2
1
0
8
1
8
1
32
0
1
0
0
0
2
0
1

2
−45.19
16 −170.52
0 −483.13
−88.00
0
−56.86
0
−38.97
0
−33.08
1
−17.18
2
−45.47
2
−45.55
0
−12.91
2
1
−44.00
5 −370.09
−11.95
0
−24.14
0
−83.57
2
−31.34
0
0 −248.64
−3.70
0
−7.90
2
−59.84
0
−27.50
0
−37.31
0
−19.17
0
−56.88
0
−49.09
0
−49.73
2
−42.56
2
−57.64
0
−39.10
1
0
−18.93
8 −484.78
8 −414.13
−81.78
0
−34.70
1
−35.55
2
−50.47
0
0
−43.11
0 −230.94
10 −326.74
0
−51.42
8 −138.38
−68.00
3
0 −219.32
0 −796.72
−51.38
0
0
−41.80
0 −130.89
−45.30
0
−35.13
3
−7.75
0
−14.94
0

−2.82
−2.66
−3.77
−4.00
−3.55
−2.17
−2.76
−2.15
−2.53
−2.53
−1.61
−2.75
−2.89
−0.66
−3.02
−4.18
−3.92
−3.88
−0.46
−0.99
−2.99
−3.44
−2.33
−2.40
−3.55
−3.07
−3.11
−2.66
−2.88
−2.44
−1.18
−3.79
−3.24
−5.11
−2.17
−1.98
−3.15
−2.69
−3.61
−2.55
−2.86
−2.16
−2.13
−3.43
−6.22
−3.21
−4.18
−2.05
−2.83
−2.20
−0.39
−1.87

H He Li Be B

C N O F Ne Na Mg Al

Si

P

S Cl Ar

E

E /atom

0
0
1
0
0
0
0

8
0
0
0
0
1
0

8
0
0
0
4
1
1

8
0
1
10
0
1
1

8
0
0
0
0
0
1

0
0
0
0
0
0
0

0
0
1
0
0
0
1

0
0
1
6
0
1
1

0
4
1
0
0
0
0

0
0
1
4
0
1
0

0
0
0
0
0
2
1

8
0
1
0
0
1
2

8
0
0
0
4
0
2

8
0
0
0
0
2
1

0
4
0
0
0
1
0

8
0
0
0
0
3
0

0
0
0
0
0
1
0

0 −148.50
−24.20
0
−13.42
1
−75.76
0
−17.00
0
−42.95
1
−27.97
1

−2.32
−3.02
−1.68
−3.79
−2.13
−2.68
−2.33

Table .8: The number of atom pairs in the test dataset.

H

981

He

403
455

Li

Be

B

C

N

O

622
582
2091

300
554
522
1208

277
518
705
671
3324

916
594
543
766
505
1333

684
601
697
929
739
681
1068

1002
527
1313
590
3514
538
759
2745

F

564
401
662
401
659
393
536
387
815

Ne

Na Mg

Al

Si

P

S

535
637
676
606
886
554
927
758
499
1201

438
384
509
414
459
477
825
404
520
717
741

327
598
902
411
474
599
586
520
369
588
451
917

537
586
741
621
625
841
525
625
402
668
374
468
1515

341
477
802
1391
824
460
678
462
415
621
393
489
516
1098

741
500
502
283
572
396
576
505
488
778
295
302
475
297
1155

401
780
432
491
751
868
433
414
519
350
346
482
599
412
716
1600

Cl

469
539
779
385
448
676
622
506
503
498
527
630
1106
889
408
738
1413

Ar

372
501
460
442
551
490
416
455
372
553
321
383
510
451
342
464
469
421

H
He
Li
Be
B
C
N
O
F
Ne
Na
Mg
Al
Si
P
S
Cl
Ar

40

Figure .24: The structures of the ﬁrst 20 samples in table .7 are shown (order: top left to right). The colors of the atoms
correspond to the element number (H: blue, Ar: red). It is noted that the structures with small box are drawn at a size of 2 × 2.
See table .7 for the details of component.

41

5 Å