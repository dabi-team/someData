JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST20151PartialDomainAdaptationwithoutDomainAlignmentWeikaiLiandSongcanChenAbstract—Unsuperviseddomainadaptation(UDA)aimstotransferknowledgefromawell-labeledsourcedomaintoadifferentbutrelatedunlabeledtargetdomainwithidenticallabelspace.Currently,themainworkhorseforsolvingUDAisdomainalignment,whichhasprovensuccessful.However,itisoftendifﬁculttoﬁndanappropriatesourcedomainwithidenticallabelspace.Amorepracticalscenarioisso-calledpartialdomainadaptation(PDA)inwhichthesourcelabelsetorspacesubsumesthetargetone.Unfortunately,inPDA,duetotheexistenceoftheirrelevantcategoriesinsourcedomain,itisquitehardtoobtainaperfectalignment,thusresultinginmodecollapseandnegativetransfer.Althoughseveraleffortshavebeenmadebydown-weightingtheirrelevantsourcecategories,thestrategiesusedtendtobeburdensomeandriskysinceexactlywhichirrelevantcategoriesareunknown.ThesechallengesmotivateustoﬁndarelativelysimpleralternativetosolvePDA.Toachievethis,weﬁrstprovideathoroughtheoreticalanalysis,whichillustratesthatthetargetriskisboundedbybothmodelsmoothnessandbetween-domaindiscrepancy.ConsideringthedifﬁcultyofperfectalignmentinsolvingPDA,weturntofocusonthemodelsmoothnesswhilediscardtheriskierdomainalignmenttoenhancetheadaptabilityofthemodel.Speciﬁcally,weinstantiatethemodelsmoothnessasaquitesimpleintra-domainstructurepreserving(IDSP).Toourbestknowledge,thisistheﬁrstnaiveattempttoaddressthePDAwithoutdomainalignment.Finally,ourempiricalresultsonmultiplebenchmarkdatasetsdemonstratethatIDSPisnotonlysuperiortothePDASOTAsbyasigniﬁcantmarginonsomebenchmarks(e.g.,∼+10%onCl→Rwand∼+8%onAr→Rw),butalsocomplementarytodomainalignmentinthestandardUDA.IndexTerms—PartialDomainAdaptation,StructurePreserving,Semi-supervisedLearning,manifoldLearningF1INTRODUCTIONCURRENTLY,unsuperviseddomainadaptation(UDA)hascaughttremendousattentioninthemachinelearn-ingcommunity,whichlearnsaclassiﬁerfortheunlabeledtargetdomainbyusingalabeledsourcedomain.Mostexist-ingworksassumethatthesourceandtargetdomainsshareanidenticallabelsetorspace[1],[2].Inthisspeciﬁccontext,domainalignmentcomestothemainhtospotforsolvingUDA,i.e.,instancere-weighting[3],featurealignment[4],[5],[6]ormodeladaptation[7],[8],[9].However,inpractice,ittendstobeextremelyburdensomeandquitedifﬁculttoﬁndanidealsourcedomainwithidenticallabelspace[10].Incontrast,inthecontextofbigdata,amorepracticalalternativeistoaccessalarge-scalesourcedomainwhileworkingonarelativesmall-scaletargetdomaintocoverthetargetlabelspace,whichisalsoknownasthepartialdomainadaptation(PDA)[11],[12],[13],[14],[15],[16].Unfortunately,insuchascenario,theconventionaldomainalignmentoftenfailsasaresultoftheirrelevantsourcesub-classescanbemixedwithtargetdata,resultinginmodecollapseandnegativetransfer[13].Toalleviatethisissue,almostallexistingPDAstudiesfocusondiminishingthenegativeimpactofirrelevantsourcecategoriesbetweentwodomainsbyanelaboratelydesignedreweightingapproachandinturnlearningthedomaininvariantmodel/representationinthesharedlabelspace[11],[12],[16],[17].Unfortunately,fromanalgorith-•TheauthorsarewithCollegeofComputerScienceandTechnology,Nan-jingUniversityofAeronauticsandAstronauticsof(NUAA),Nanjing,211106,China.E-mail:{leeweikai;s.chen}@nuaa.edu.cn.•CorrespondingauthorisSongcanChen.ManuscriptreceivedApril19,XXXX;revisedAugust26,XXXX.micperspective,suchanadditionalreweightingapproachcanbequiterisky,sinceexactlywhichirrelevantcategoriesareunknown.Moreover,theseapproacheshighlyrelyonthepseudolabelsoftargetdomain.However,thetargetsamplesmightbewronglypredictedintotheirrelevantcategoriesinthebeginningunderthedomainshift,asshowninFig.1(b)[18],[19],[20],[21],[22],makingithardtoobtainaperfectalignment.Theexperimentalresultsofthesemethodsalsoconﬁrmthatitisdifﬁculttoaccuratelyidentifytheirrelevantcategories[11],[12],[17]insourcedomain.Further,eveniftheirrelevantsourcecategoriesarecorrectlyﬁltered,itisstillhardtoobtainaperfectalignmenttoguaranteethatthetargetdomaincanbelabeledcorrectly[22],[23],asshowninFig.1(c).Fromatheoreticalperspective,severalrecentlyproposedtheoreticalworksrevealthatthedomainalign-menthurtsthetransfer-abilityofrepresentations/model[21],[22].Consequently,thesedrawbackspushustoseekanalternativeforsolvingPDAwithoutdomainalignment.Recently,severaleffortshaveindeedbeendedicatedtoaddressingUDAwithoutdomainalignment,evenifthedomainsinvolvedsharethesamelabelset.Forexample,Lietal,progressivelyanchorsthetargetsamplesandinturnreﬁnethesharedsubspaceforknowledgetransfer[19].Lietal,optimizesthepredictivebehaviorinthetargetdomaintoaddressUDA[23].Liuetal,enforcesthepseudolabelstogeneralizeacrossdomains[18].Asoneproverbgoes,”thelesserofthetwoevils”,discardingthedomainalignmentissometimesawisechoicewheninducedmisalignmentdoesmoreharmthangoodasvalidatedbytheseworks.Nonetheless,tothebestofourknowledge,sofar,therehasbeennoattempttoaddressPDAwithoutdomainalignment,wherethemisalignmentmorelikelyoccurs.Atthesametime,thetheoreticalaspectofhowtotransferknowledgearXiv:2108.12867v1  [cs.CV]  29 Aug 2021JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST20152withoutdomainalignmenthasnotbeenwellstudiedyet.Toaddresstheseissues,wederiveanovelgeneralizationerrorboundfordomainadaptationwhichcombinesthemodelsmoothnessandthebetween-domaindiscrepancy.ConsideringboththedifﬁcultyofperfectalignmentandtheriskofmisalignmentinsolvingPDA,wesimplybypassthedomainalignmentandfocusonthemodelsmoothness,whichguidesustooptimizethetargetlabeltohaveasmoothstructure.Speciﬁcally,asaproofofconcept,thispaperpresentsaquitesimplePDAframeworktoencouragetheadaptationabilityofthemodel,whichinstantiatesthemodelsmoothnessasacommonly-usedmanifoldstructurepreserving[9],[24].Incontrasttotheconventionaldo-mainalignmentwhichindirectlylearnsadomaininvariantrepresentation/modelforadaptation,wedirectlyoptimizethetargetlabelstoenhancetheadaptationperformanceofthemodel.DoingsoisalsoquiteconsistentwithVapnik’sphilosophy,i.e.,anydesiredproblemshouldbesolvedinadirectway[25].Furthermore,onamountoftheexistenceofthedomainshift,themanifoldstructuretendstovarysharplyacrossdomains.Forexample,severalsource-privatesamplescanbelocatednearthetargetsamples,andsomeofthesourcesampleswithcommonlabelscanbefarawayfromthetargetsamplesasshowninFig.1(e).Also,thestructureinformationoftheirrelevantcategoriesinthesourcedomaineasilyincorporatebiasinlearningtheclassiﬁer.Thus,itcanbequiteperiloustoleveragethestructureknowledgeacrossdomains.Tothisend,weonlyconsidertheintradomainstructurepreserving(IDSP)toconstraintheclassiﬁeronthetargetdomaintoalleviatethenegativetransfercausedbythedomaingapasshowninFig.1(f).Notably,fromboththeoreticalandempiricalviews,IDSPanddomainalignmentcancomplementeachother,whenidealalignmentisrelativelyeasytobeconducted,especiallyinUDA.Inaddition,wewouldliketoemphasizethattheIDSPalsocanworkwellinsolvingUDAcomparedtotheexistingPDAmethods.Forfacilitatingeffortstoreplicateourresults,ourimplementationisavailableonGitHub1.Insummary,thisworkmakesthefollowingcontributions:•Anovelgeneralizationerrorboundisderived,whichprovidesanalternative(i.e.,modelsmoothness)forsolvingPDA;•AsimpleyeteffectivePDAmethodbyintra-domainstructurepreserving(IDSP)isproposed,whichistheﬁrstattempt,toourbestknowledge,toaddressPDAwithoutdomainalignment;•TheexperimentalresultsrevealthattheproposedIDSPcaneffectivelyenhancetheadaptationabilityofthemodel,whichadditionallyprovidesaneffectivebenchmarkforPDA;•TheIDSPcanbecomplementarytodomainalign-mentinUDA,whendomainalignmentisrelativelylessriskyinsuchasetting,whichagaintellstheneedoftargetingatasafetransferofknowledge.Therestofthispaperisorganizedasfollows.InSection2,webrieﬂyoverviewUDAandPDA.InSection3,weelab-orateontheproblemformulation,andprovideanovelgen-eralizationerrorboundfordomainadaptation.InSection1.https://github.com/Cavin-Lee/IDSP.Fig.1.MotivationofIDSP.(a)Partialdomainadaptation,inwhichthelabelsetorspaceofthesourcedomainsubsumesthetargetone.(b)Itisriskytodownweighttheirrelevantcategories.(c)Thetargetsamplesmightbewronglypredictedeveniftheirrelevantcategoriesareﬁltered.(d)Classiﬁerlearnedfromthesourcedomain.(e)Themanifoldstructureacrossdomainstendstovarysharplyduetothedomainshift.(f)Weonlyusetheintra-domainstructurepreservingtoenhancetheabilityoftheadaptationabilityoftheclassiﬁer.4,wepresenttheIDSPmodelandoptimizationalgorithmindetail.InSection5,wepresenttheexperimentalresultsandthecorrespondinganalysis.Intheend,weconcludetheentirepaperwithfutureresearchdirectionsinSection62RELATEDWORKSInthissection,wepresentthemostrelatedresearchonUDA/PDAandhighlightthedifferencesbetweenthemandourmethod.2.1UnsupervisedDomainAdaptationRecentpracticesonUDAusuallyattempttominimizethedomaindiscrepancyforborrowingtheexistingwell-establishedsourcedomainknowledge.Followingthis,mul-tipledomainadaptationtechniqueshavebeendeveloped,includinginstancere-weighting[3],[26],[27],featurealign-ment[4],[5],[6]andclassiﬁeradaptation[7],[8],[9].Theinstancere-weightingmethodsfocusoncorrectingthedis-tributionbiasesinthedatasamplingprocedurethroughreweightingtheindividualsamplestominimizetheA-distance[27],MaximumMeanDiscrepancy(MMD)[3],orKL-divergence[26].Thefeaturealignmentmethodsgen-eratethedomain-invariantfeaturetoreducethedistribu-tiondifferencesacrossdomains,suchasMMD[4],CentralMomentDiscrepancy[28],Bregmandivergence[29],JointMMD[7],[30],A-distance[6],MaximumClassiﬁerDiscrep-ancy[31],Wassersteindistance[32],∆-distance[33],orthedistancebetweenthesecond-orderstatistics(covariance)ofthesourceandtargetfeatures[34].Theclassiﬁeradaptationmethodsadaptthemodelparameterofsourcedomaintotargetdomainbyimposingtheadditionalconstraintsforalignment[7],[8],[9].TheproposedIDSPcanbesummarizedintotheclassiﬁeradaptation.However,IDSPdoesnotalignthetargetandsourcedomainsbutusestheintra-domainstructurepre-servingtoenhancetheadaptationabilityofthemodel.Inaddition,alloftheexistingUDAmethodsassumethesourcelabelspaceandtargetlabelspaceareidentical,whichisJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST20153oftentoostricttobesatisﬁedinthereal-worldapplications.TheproposedIDSPrelaxesthisassumptionandaimstoaddressamorechallengingPDAproblem.2.2PartialDomainAdaptationPDAisamorepracticalscenarioinwhichthesourcelabelspacesubmergesthetargetone[11].Owingtotheexistenceoftheirrelevantcategoriesinthesourcedomain,theriskofmis-alignmentandmodecollapseishighlyincreased.Toaddresssuchissues,theexistingapproachesmainlyfocusonmitigatingthepotentialnegativetransfercausedbyirrelevantclassesinthesourcedomainbyclass-reweighting[11],[14],[16],[35]orexample-reweighting[12],[13].Speciﬁcally,theclass-reweightingworksfocusonalle-viatingthenegativetransfercausedbythesource-privateclasses.Toachievethis,PartialAdversarialDomainAdap-tation(PADA)alleviatesthenegativetransferbydown-weightingthedataofirrelevantclassesinthesourcedomainwiththeguidanceofdomaindiscriminator[11].SelectiveAdversarialNetwork(SAN)[35]extendsPADAtomaxi-mallymatchthedatadistributionsinthesharedclassspacebyaddingmultipledomaindiscriminators.DeepResidualCorrectionNetwork(DRCN)plugsoneresidualblockintothesourcenetworktoenhancetheadaptationfromsourcetotargetandexplicitlyweakenstheinﬂuencefromtheirrelevantsourceclasses[14].ConditionalandLabelShift(CLS)introduceaclass-wisebalancingparametertoalignbothmarginalandconditionaldistributionbetweensourceandtargetdomains[16].Incontrasttothementionedclassre-weightingscheme,ImportanceWeightedAdversarialNets(IWAN)followstheideaofinstancere-weightingtoﬁlterouttheinﬂuenceoftheirrelevantsamplesinthesourcedomain.TwoWeightedInconsistency-reducedNetworks(TWINs)designsanin-consistencylosstodown-weightingtheoutliersampleinthesourcedomain[13].ExampleTransferNetwork(ETN)integratesthediscriminativeinformationintothesample-levelweightingmechanism[12].AlmostalloftheexistingPDAmethodsrelyonvariousadversarialstrategiesanddeepnetworkstolearnthetar-getmodelwiththeguidanceofthespeciﬁcallydesignedclass/samplereweightingapproaches.However,suchanapproachcouldbevulnerableintermsofadaptationduetotheagnosticlabelspace.Besides,theweightedadversar-ialtrainingbasedmethodsmaysufferfromtheissuesoftraininginstabilityandmodecollapse,sincetheyarehighlybaseontheunreliablepseudolabels[19].Incontrast,IDSPrequiresneitheradversarialtrainingnorreweightingandthushighlymitigatestheseissues.Inaddition,theIDSPdoesnotﬁlterouttheoutlierclassesandthuscanalsoworkwellonUDA.3ATHEORETICALANALYSISFORDOMAINADAP-TATIONCurrenttheoreticalworksondomainadaptationen-couragesdomainalignmentinsolvingdomainadaptation[1],[2].Unfortunately,itisoftenhardtoobtainan’ideal’alignmentindomainadaptation.Toﬁndarelativelysimpleralternative,wenowattempttogenerateanovelgeneraliza-tionerrorboundforsolvingdomainadaptation.3.1NotionsandDeﬁnitionsInthispaper,wefocusonthePDAandUDAscenarios.WeuseX∈RdandY∈Rtodenotetheinputandoutputspace,respectively.ThePDAandUDAscenariosconstitutealabeledsourcedomainDs={xsi,ysi}ni=1withnsamples,andanunlabeledtargetdomainDt={xti}n+mi=n+1withmsamples.Speciﬁcally,xi∈Xandyi∈Yarethefeatureandlabelofi-thsample,respectively.ThesourceandtargetdomainsfollowthedistributionsPandQ,respectively,andP6=Q.LetXs,XtandYs,Ytdenotethefeatureandlabelspacesofsourceandtargetdomains,respectively.Weassumethatthesourceandtargetdomainssharethesamefeaturespacei.e.,Xs=Xt.ForPDA,thetargetlabelspaceYtissubmergedbythesourcelabelspaceYs,i.e.,Yt⊂Ys.Here,wefurtherhavePs6=Q,wherePsisthedistributionoftheshareclassesinthesourcedomain.ForUDA,thesourceandtargetdomainssharetheidenticallabelspace,i.e.,Ys=Yt.ThegoalofbothPDAandUDAistotransferthediscriminativeinformationfromthesourcedomain.Inaddition,ourgeneralizationboundsarebasedonthefollowingTotalVariationdistance[36]andmodelsmoothness.Deﬁnition1.TotalVariationdistance[36]:Giventwodistribu-tionsPandQ.TheTotalVariation-distanceTV(P,Q)betweendistributionsPandQisdeﬁnedas:TV(P,Q)=12ZX|dP(x)−dQ(x)|.(1)Deﬁnition2.ModelSmoothness:Amodelfisr-coverwith(cid:15)smoothnessondistributionP,ifEP"supkδk∞≤r|f(w,x+δ)−f(w,x)|#≤(cid:15).(2)3.2GeneralizationErrorBoundwithSmoothnessInspiredbyarecenttheoreticalstudy[37],weassumethatboththesourceandtargetdomainshaveacompactsupportX∈Rd.Thus,thereexistsD>0,suchthat∀u,v∈X,ku−vk<D.LetL(f(x),y)bethelossfunc-tion.Werepresenttheexpectedriskofmodelfoverdistri-butionPasEP(f)=E{x,y}∼PL(f(x),y).Inaddition,wealsoassumethat0≤L(f(x),y)≤MforconstantMwithoutlossofgenerality.Theorem1.GiventwodistributionsPandQ,ifamodelfis2r-coverwith(cid:15)smoothnessoverdistributionsPandQ,withprobabilityatleast1−θ,wehave:JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST20154EQ(f)≤EP(f)+2(cid:15)+2MTV(P,Q)+Ms(2d)2(cid:15)2Dr2+1log2+2log(cid:0)1θ(cid:1)m+Ms(2d)2(cid:15)2Dr2+1log2+2log(cid:0)1θ(cid:1)n+Mslog(1/θ)2m.(3)TheproofisgiveninAppendixA.ThisgeneralizationerrorboundensuresthatthetargetriskEQisboundedbythesourceriskEP(f),themodelsmoothness(cid:15)andthedomaindiscrepancyTV(P,Q).WhilethemisalignmenttendstoberiskyforsolvingPDA,weturntofocusondecreasingthe(cid:15)toguaranteealowtargetrisk.Inotherwords,iftwopointsx1,x2∈Xareclose,thentheconditionaldistributionsP(y|x1)andP(y|x2)shouldbesimilar.4INTRADOMAINSTRUCTUREPRESERVINGInthissection,weexploitthetheoreticalresultsintro-ducedabovetoderiveasimpleandpracticalPDAalgorithmforproofofconcept.Speciﬁcally,wefocusonmodelsmooth-nessandproposeanIntraDomainStructurePreserving(IDSP)regularizertoaddressPDA.Inthefollowing,wegothroughthedetailsofIDSP.4.1MainIdeaOurgoalistolearnanadaptiveclassiﬁerforthetargetdomainDt.Tobeginwith,wesupposetheclassiﬁerbef=w|φ(x),whereφ(·)denotesthefeaturemappingfunctionthatprojectstheoriginalfeaturespacetotheHilbertspaceHandwistheparameteroftheclassiﬁer.Then,weinduceastandardstructuralriskminimizationprinciple[38]as:f=argminf∈HKL(f(x),y)+λR(f),(4)wheretheﬁrsttermrepresentsthelossondatasamples,whichrepresentstheempiricalriskofthetrainingdata.Thesecondtermreferstotheregularizationterm.HKistheHilbertspaceinducedbykernelfunctionK(·,·).Themainideaofthispaperistoincorporateanappropriateregularizationtermtolearnaclassiﬁerffortargetdomain.Speciﬁcally,weattempttodiscardthedomainalign-ment,sincemisalignmenttendstodomoreharmthangoodinPDA[11]andeveninUDA[22].Motivatedbythetheo-reticalanalysis,weturntofocusonthemodelsmoothness.Asaproofofconcept,weassumethatthedataliesonamanifoldandfollowarecentstudy[9]toaddaLaplacianregularizationtermtoreducethemodelsmoothness(cid:15)2.2.Here,weemphasizethatsuchastrategyisonlyasimpleattempt,whichmeansthatanyothermodelsmoothnesstricksuchasconsis-tencyregularization[39],[40]canalsobeadoptedtoslovePDA/UDA.4.2LearningClassiﬁerTolearntheclassiﬁerf,weonlycalculatetheempiricalriskonthesourcedomain,sincethetargetdomainhasnola-belinformation.Inaddition,weadoptthemostcommonly-usedsquareloss,i.e.,l2astheempiricalrisk.Then,fcanberepresentedas:f=argminf∈HKnXi=1(yi−f(xi))2+λkfk2K.(5)Moreover,weincorporateanadditionalkfk2KconstraintfollowedbyRegularizedLeastSquares(RLS)[41].4.3IntraDomainStructurePreservingForasafetransferofknowledge,weonlyfocusonmodelsmoothnessandmodelitastheintradomainstructurepreserving.Speciﬁcally,asthemanifoldstructuremayvarysharplycrossdomainsduetothedomainshiftandthestructureinformationoftheirrelevantcategoriesinthesourcedomaineasilyintroducebiasinthelearnedclassiﬁer,ittendstoberiskytoleveragethestructureinformationofthesourcedomain.Thus,weonlyaddaLaplacianregularizationtermontargetdomaintopreserveitsintradomainstructure.Inthefollowing,wepresentthegraphconstructionandLaplacianregularizationofIDSP.4.3.1GraphConstructionThecoreforstructurepreservingisthegraphconstruc-tion.Thepair-wiseafﬁnitymatrixˆGofthegraphcanbeformulatedasfollows:ˆGij=(cid:26)sim(xi,xj),xi∈Np(xj)0,otherwise,(6)wheresim(·,·)denotesapropersimilaritymeasurement(thispaperusethecosinedistance)betweentwosamples.Nprepresentsthesetofp-nearestneighborsofpointxi.Sinceweonlyconsidertheintradomainstructurepreserv-ingoftargetdomain,wefurtherhave:Gij=(cid:26)ˆGij,xiandxj∈Dt0,otherwise.(7)Notably,ifwefurtherhaveGij=ˆGijwherexiandxj∈Ds,wewillpreservetheintrastructureofbothsourceandtargetdomain(ST).Inaddition,if∀i,j,Gij=ˆGij,themodelwilldegeneratetotheconventionalmanifoldstructurepreserving(CST).4.3.2LaplacianRegularizationByintroducingLaplacianmatrixL=D−G,whereDii=Pn+mi=1Gijisthediagnoalmatrix,theLaplacianregularizationRL(f)canbeexpressedbyRL(f)=n+mXi,j=1(f(xi)−f(xj))2Gij=n+mXi,j=1f(xi)Lijf(xj).(8)JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST201554.4OverallReformulationSubstitutingwithequations5and8inequation4,theoverallreformulationcanbereformulatedas:f=argminf∈HKnXi=1(yi−f(xi))2+λkfk2K+γn+mXi,j=1f(xi)Lijf(xj),(9)whereλandγareregularizationhyper-parametersaccord-ingly.4.5LearningAlgorithmThemajordifﬁcultyoftheoptimizationliesinthatthekernelmappingφ:X→Hmayhaveinﬁnitedimensions.TosolveEq.9effectively,wereformulateitbyusingthefollowingrevisedRepresentertheorem.Theorem2.RepresenterTheorem:TheparameterW∗=[w∗1,···,w∗h]fortheoptimizedsolutionfofEq.9canbeexpressedintermsofthecross-domainlabeledandunlabeledexamples,f(x)=n+mXi=1αiK(xi,x)andw=n+mXi=1αiφ(xi),(10)whereKisakernelinducedbyφ,αiisaweightingcoefﬁcient.TheproofisgiveninAppendixB.ByincorporatingEq.7intoEq.9,weobtainthefollowingobjective3:α=argminα∈Rn+m(cid:13)(cid:13)(cid:13)(cid:16)Y−αTK(cid:17)V(cid:13)(cid:13)(cid:13)2F+tr(cid:16)λαTKα+γαTKLKα(cid:17).(11)whereVisthelabelindicatormatrixwithVii=1ifi∈Ds,otherwiseVii=0.Settingderivativeofobjectivefunctionas0leadsto:α=((V+γL)K+λI)−1VYT.(12)ThelearningalgorithmsaresummarizedinAlgorithm1.Algorithm1LearningalgorithmforIDSPInput:nsourcelabeleddatasetsDs={xsi,yi}ni=1mtargetunlabeleddatasetsDt={xsi}mi=1Hyper-parametersλ,γ,p;Output:PredictiveClassiﬁerf1:CalculatethegraphLaplacianLbyEq.(7)2:ConstructkernelKbyaspeciﬁckernelfunction;3:ComputeαbyEq.(12);4:ReturnClassiﬁerfbyEq.(10);3.Weneedtostatethatalthoughourframeworkisbasedonkernelmethod,itcanbeeasilyappliedtothedeepmodelbyreformulatingtheφ(x).TABLE1StatisticsofthebenchmarkdatasetsDataset#Sample#Class#DomainOfﬁce-Home1550065Ar,Cl,Pr,RwImage-Clef720012C,I,POfﬁce-31465231A,W,D4.6ComplexityAnalysisThecomputationalcomplexityforsolvingIDSPconsistsofthreeparts.Wedenotesastheaveragenumberofnon-zerofeaturesofeachsampleandwehaves≤d,p(cid:28)min(n+m,d).SolvingtheEq.12byLUdecompositionrequiresO((n+m)3).ForconstructingthegraphLaplacianmatrixL,IDSPrequiresO(sm2).Forconstructingtheker-nelmatrixL,IDSPrequiresO((n+m)2).Thus,thetotalcomplexityofIDSPisO((n+m)3+sm2+(n+m)2).Itshouldbenotedthatitisnotdifﬁculttospeedupthealgorithmusingconjugategradientmethod[7]orkernelapproximationmethod[42],whichisbeyondthescopeofthiswork.5EXPERIMENTSToevaluatetheperformanceofIDSP,weconductmulti-pleexperimentsoverboththePDAandtheUDAsettingsonthemostwidely-usedbenchmarkdatasetsincludingOfﬁce-Home,Image-ClefandOfﬁce-31.Table1liststhestatisticsofthethreedatasets.5.1IDSPonPDA5.1.1DatasetsforPDAWeﬁrstevaluatetheperformanceofIDSPonthewidely-adoptedPDAbenchmarks,i.e.,Ofﬁce-31andOfﬁce-Home.Thedetailsofthedatasetsaregivenasfollows:Ofﬁce-31[43]contains4652imageswith31categoriesinthreevisualdomainsincludingAmazon(A),DSLR(D)andWebcam(W).ForthesettingofPDA,wefollowthesamesplitsusedinrecentPDAstudies[11],[35].Speciﬁcally,thetargetdomainsonlycontain10categories,whichissharedwithCaltech-256.Ofﬁce-HomeisreleasedatCVPR’17[44],whichcontains65differentobjectsfrom4domainsincluding15588images:Artisticimages(Ar),Clipartimages(Cl),Productimages(Pr)andReal-worldimages(Rw).ReferencetothePDAsettinginrecentstudies[11],[35],theﬁrst25categories(inalphabeticalorder)aretakenasthetargetcategories,whiletheothersasthesourceprivatecategories.5.1.2ExperimentalSetupInordertoevaluatetheperformanceofIDSPonthePDAsetting,wecompareIDSPwithseveralPDASOTAs:PartialAdversarialDomainAdaptation(PADA)[11],SelectiveAd-versarialNetwork(SAN)[35],TwoWeightedInconsistency-reducedNetworks(TWINs)[13],ImportanceWeightedAd-versarialNetwork(IWAN)[17],ExampleTransferNetwork(ETN)[12],DeepResidualCorrectionNetwork(DRCN)[14]andConditionalandLabelShift(CLS)[16].ToillustratethedifﬁcultyofdomainalignmentinsolvingPDA,wefurtherJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST20156TABLE2Accuracy(%)onOfﬁce-HomeforPDAfrom61classesto25classesUDAMethodsPDAMethodsResNetDANDANNMEDAPASPADADRCNIWANSANETNIDSPAr→Cl38.644.444.952.155.052.054.053.944.459.260.8Ar→Pr60.861.854.173.875.767.076.454.568.777.080.8Ar→Rw75.274.569.078.983.978.783.078.174.679.587.3Cl→Ar39.941.836.357.966.352.262.161.367.562.969.3Cl→Pr48.145.234.361.873.353.864.548.065.065.776.0Cl→Rw52.954.145.271.174.759.071.063.377.875.080.2Pr→Ar49.746.944.159.765.752.670.854.259.868.374.7Pr→Cl30.938.138.048.555.543.249.852.044.755.459.2Pr→Rw70.868.468.777.679.478.880.581.380.184.485.3Rw→Ar65.464.453.068.671.873.777.576.572.275.777.8Rw→Cl41.845.434.753.054.356.659.156.750.257.761.3Rw→Pr70.468.846.578.582.077.179.982.978.785.585.7AVE53.754.547.465.169.862.169.0.63.665.370.574.9AllPDAmethodsareachievedbytheweighteddomainalignmentapproachexceptIDSP,whichisthesamewiththefollowingTable3.BothMEDAandIDSPincorporatethemanifoldregularizationterm.BothPASandIDSParethenon-alignedmethods.Thebestaccuracyispresentedinboldandthesecondbestisunderlined,similarlyhereinafter.TABLE3Accuracy(%)onOfﬁce-31forPDAfrom31classesto10classesUDAMethodsPDAMethodsResNetDANDANNMEDAPASPADATWINsDRCNIWANSANETNCLSIDSPA→W54.546.441.479.397.086.586.090.889.293.994.599.699.7D→W94.653.646.897.099.399.399.310099.399.310010099.7W→D94.358.638.999.410010010010099.4100100100100A→D65.642.741.485.398.482.286.886.090.582.295.097.399.4D→A73.265.741.392.094.692.794.795.695.692.796.297.995.1W→A71.765.344.791.694.495.494.595.894.395.494.698.395.7AVE75.655.442.490.797.292.793.694.394.792.796.798.298.3compareseveraltraditionallearningandUDAbenchmarksincludingResNet[45],ManifoldEmbeddedDistributionAlignment(MEDA)[24],DeepAdaptationNetwork(DAN)[46]andDomainAdversarialNeuralNetworks(DANN)[6].Forthenon-alignedmethod,weonlycomparedPro-gressiveAdaptationofSubspaces(PAS)[19],sinceonlyPASperformedthePDAexperimentintheoriginalpaper.Forafaircomparison,weusethe2048-dimensionaldeepfeature(extractedusingResNet50pre-trainedonImageNet)forbothIDSPandothershallowUDAapproach(i.e.,MEDAandPAS).Theoptimalparametersofallcomparedmethodsaresetfollowingtheiroriginalpapers.Notethatseveralresultsaredirectlyobtainedfromthepublishedpapersifwefollowthesamesetting.AsforIDSP,weempiricallysetthehyper-parametersλ=0.1,γ=5andp=10forthePDAsetting.Toevaluatetheperformance,wefollowthewidelyusedaccuracyasameasurement.5.1.3ExperimentalResultsTheclassiﬁcationresultsof12PDAtasksonOfﬁce-Homedatasetand6PDAtasksonOfﬁce-31datasetaregiveninTable2andTable3,respectively.Speciﬁcally,onbothtwodatasets,ourapproachachievesthesuperiorresultswithav-erageaccuraciesof74.9%onOfﬁce-31datasetand98.3%onOfﬁce-31datasetbyusingasimplelaplacianregularizationterm.IDSPoutperformsthestateoftheartbyasigniﬁcantmarginonseveraltask(+10%onCl→Rw,+8%onAr→Rwand+6%onCl→Ar).Inaddition,IDSPachievesthebestresultsonall12tasksatOfﬁce-Homedatasetandachievesthebest/secondbestonall6tasksatOfﬁce-31dataset.Notably,theUDAmethods(e.g.,DAN,DANN)workevenworsethanthebaselinemethodwithoutDA(i.e.,ResNet),thereasonisthattheriskofmodecollapseandmis-alignmentishighlyincreasedinthePDAsetting.Moreover,itshouldbenotedthatMEDAconsistsofbothstructurepreservinganddomainalignment,whoseresultshaveahugegapbetweenIDSP,whichillustratesthatthedomainalignmenttendstohurttheadaptationandcausesthenegativetransferinthePDAsetting.Also,MEDAachievesbetterresultsthanPADAinOfﬁce-Homedataset,whichalsovalidatestheperformancegainbroughtbythemodelsmoothness.Inaddition,PASalsoachievesaquitecomparableresultsonPDA,whichrevealsthatdiscardingdomainalignmentisawisechoiceinPDA.Tosumup,theresultsrevealtheeffectivenessoftheIDSPforsolvingPDA,whileperfectalignmentisquitehardtoobtaininPDAsetting.5.2IDSPonUDA5.2.1DatasetWealsovalidatetheperformanceofIDSPontheUDAsetting.TwodatasetsincludingOfﬁce-HomeandImage-Clefareadopted,whichareboththecommonly-usedbenchmarkdatasetsfortheclosed-setUDAandwidelyadoptedinthemostexistingworkssuchas[4],[6],[30],[44],[46].WeselectallcategoriesintheOfﬁce-HomefortheUDAsetting.ThestatisticinformationoftheImage-CLEFisgivenasfollows.Image-CLEF[30]derivesfromImage-CLEF2014domainadaptationchallenge,andisorganizedbyselecting12objectJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST20157TABLE4Accuracy(%)onOfﬁce-HomeforUDAwithclosed-setResNet1NNTCATJMCORALGFKSADANDANNJANCDANPASMEDAIDSPAr→Cl34.945.338.338.142.238.943.643.645.645.946.652.255.255.0Ar→Pr50.057.058.758.459.157.163.357.059.361.265.972.976.274.5Ar→Rw58.045.761.762.064.960.168.067.970.168.973.476.977.376.3Cl→Ar37.457.039.338.446.438.747.745.847.050.455.758.458.059.1Cl→Pr41.958.752.452.956.353.160.756.558.559.762.768.173.771.0Cl→Rw46.248.156.055.558.355.561.960.460.961.064.269.771.970.4Pr→Ar38.542.942.641.545.442.248.244.046.145.851.858.359.360.4Pr→Cl31.242.937.537.841.237.641.543.643.743.449.147.452.452.7Pr→Rw60.468.964.165.068.564.670.067.768.570.374.576.677.977.6Rw→Ar53.960.852.653.060.153.759.463.163.263.968.267.168.268.9Rw→Cl41.248.341.742.048.242.347.451.551.852.456.953.557.556.5Rw→Pr59.974.770.571.473.170.674.674.376.876.880.777.681.882.1AVE46.156.451.351.355.351.257.256.357.658.362.864.967.567.0TABLE5Accuracy(%)onImage-ClefforUDAwithclosed-setResNet1NNTCATJMCORALGFKSADANDANNJANCDANPASMEDAIDSPC→I78.083.589.390.083.086.388.286.387.089.591.390.592.791.2C→P65.571.374.575.071.573.374.369.274.374.274.275.579.174.7I→C91.589.093.294.288.793.094.592.896.294.797.795.196.295.7I→P74.874.877.576.273.775.576.874.575.076.877.778.380.278.5P→C91.276.283.785.372.082.393.589.891.591.794.395.595.895.7P→I83.974.080.880.371.378.088.382.286.088.090.792.091.591.5AVE80.778.183.283.576.781.485.982.585.085.887.787.889.387.9(a)λ(b)γ(c)#neighborpFig.2.classiﬁcationaccuracyw.r.t.p,λandγ,respectivelycategoriessharedinthethreefamousreal-worlddatasets,ImageNetILSVRC2012(I),PascalVOC2012(P),Caltech-256(C).Itincludes50imagesineachcategoryandtotally600imagesforeachdomain.5.2.2ExperimentalSetupWecompareIDSPrespectivelywithseveralUDASO-TAs:1NearestNeighbor(1NN),TransferComponentAnal-ysis(TCA)[4],TransferJointMatching(TJM)[7],Correla-tionAlignment(CORAL)[34],GeodesicFlowKernel(GKF)[47],SubspaceAlignment(SA)[5],ResNet50[45],DAN[46],DANN[6],JointAdaptationNetworks(JAN)[30],Con-ditionalAdversarialNetworks(CDAN)[33],MEDA[24]andPAS[19].Theresultsofthedeep-learning-basedap-proaches(e.g.,DAN,DANN,JANandCDAN)areobtaineddirectlyfromtheexistingworks[6],[30],[33],[46].Forfaircomparison,weusethe2048-dimensionaldeepfeature(extractedusingResNet50pre-trainedonImageNet)forbothIDSPandothershallowUDAapproaches.Theoptimalparametersofallcomparedmethodsaresetaccordingtotheiroriginalpapers.AsforIDSP,weempiricallysetthehyper-parametersλ=0.1,γ=1andp=10fortheUDAsetting.5.2.3ExperimentalResultsTheclassiﬁcationresultsofthe12UDAtasksonOfﬁce-Homedatasetand6UDAtasksonImage-ClefdatasetaregiveninTable4andTable5,respectively.Onbothtwodatasets,ourapproachachievescomparableresultswithaverageaccuracyof67.0%onOfﬁce-Homedatasetand87.9%onImage-Clefdataset.Speciﬁcally,theIDSPachievesthebestorthesecond-bestresultson11tasksatOfﬁce-Homedatasetandachievesthebest/second-beston4tasksatOfﬁce-31dataset.TheresultsillustratetheeffectivenessandtheJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST20158Fig.3.PerformanceonUDA/PDAwithdifferentstructurepreserving:nostructurepreserving(nP),interandintrastructureofsourceandtargetdomain(CST),intrastructureofbothsourceandtargetdomain(ST)andintrastructureoftargetdomain(T)TABLE6Accuracy(%)ofIDSPandIDSP-JDADataSetsIDSPIDSP-JDAOfﬁce-Home(UDA)67.068.0Image-Clef(UDA)87.989.3Ofﬁce-Home(PDA)74.963.3Ofﬁce-31(PDA)98.389.3ﬂexibilityofmodelsmoothnessforsolvingUDA.Notably,MEDAincorporatesbothmanifoldanddomainalignmentregularizationterms.ThesuperiorresultsofMEDAfurthershowthatmodelsmoothnessanddomainalignmentcancomplementeachothersincetheperfectalignmenttendsmucheasiertoobtaininUDA.5.3JointWorkwithDomainAlignmentTovalidatetheinﬂuenceofdomainalignmentforIDSP,wefurtherconductexperimentsbyaddinganadditionaljointdistributionadaptation(JDA)term[48]onUDAandPDAsettings.TheresultsaregiveninTable6.Asweobserve,intheUDAsetting,theresultsof(IDSP+JDA)achievesuperiorresultswiththeaccuracyof68.0%onOfﬁce-Homedatasetand89.3%onImage-Clefdataset.TheresultsillustratethatdomainalignmentandIDSPcanbecomplementarywitheachotheratasafetransferofknowl-edge.Incontrast,inthePDAsetting,wecanﬁndthattheadaptionperformancewillbesigniﬁcantlydecreasedifweincorporatethedomainalignment.MoredetailscanbefoundinAppendixC.TheresultsrevealthatitisbeneﬁcialtodiscarddomainalignmentinPDAatleastinmoreriskysettingswhereperfectalignmentishardtobeachieved.5.4SensitivityAnalysisTheproposedIDSPmethodinvolvesthreehyper-parameters(i.e.,λforl2-regularization,γforlapla-cianregularizationandneighborp).Toinvestigatethesensitivityofthesehyper-parametersonperfor-mance,weconductexperimentsonofﬁce-Home,Image-ClefandOfﬁce-31datasets.Speciﬁcally,werunIDSPbysearchingλ∈{0.01,0.02,0.05,0.1,0.2,0.5,1,2,5,10},γ∈{0.01,0.02,0.05,0.1,0.2,0.5,1,2,5,10}andp∈{1,2,3,4,5,6,7,8,9,10}.AsweobserveinFigure2(a-c),theIDSPperformsrobustlyandinsensitivelyonboththeclosed-setUDAandPDAtasksonawiderangeofparametervaluesofp,λandγ.5.5EffectivenessofIntraDomainStructurePreservingWeverifyeffectivenessofIDSPbyinspectingtheimpactsofdifferentstructurepreservingconstraints.Speciﬁcally,weusenostructurepreserving(nP,i.e.,γ=0),conventionalmanifoldstructurepreserving(CST),intrastructureofbothsourceandtargetdomain(ST)andintrastructurepreserv-ingoftargetdomain(T,i.e.,IDSP),whoseresultsontheOfﬁce-Home,Image-ClefandOfﬁce-31aregiveninFigure3.MoredetailsaregiveninAppendixD.ItcanbeobservedthattheIDSPoutperformsthesebaselines.FromFigure3,weeasilyﬁndthatthestructurepreservingcaneffec-tivelyenhancetheadaptationabilityoftheclassiﬁerlearnedfromthesourcedomain,whichconﬁrmsthesuperiorityofmodelsmoothness.Also,wecanﬁndthatwithmoresourcestructureinformationconsidered(i.e.,STandCST),theperformancearereduced,especiallyinPDA.Thereasonisthatthemanifoldstructureacrossdifferentdomainsmayvarysharply(especiallywhentheirrelevantcategoriesexist)alongwiththedomainshift,resultinginanegativetransfer.Theresultsillustratethenecessitytoconsidertheintradomainstructurepreserving.6CONCLUSIONInthispaper,consideringthedifﬁcultyoftheperfectalignmentbetweendomainswhilesolvingPDA,ween-deavortoaddressthePDAbygivingupdomainalignment.Toachieveit,anovelgeneralizationerrorboundisﬁrstderivedandthenatheoretically-motivatedPDAapproachisproposedbyenforcingintradomainstructurepreserving(IDSP).Theexperimentalresultsdemonstratetheeffective-nessoftheproposedIDSP,whichconﬁrmsthatIDSPschemecanbeapplicabletoenhancetheadaptationabilityinsolv-ingPDA.ThisstudyalsoindicatesthatIDSPandconven-tionaldomainalignmentcanbecomplementarywitheachotherintheUDAsetting.Inaddition,itshouldbenotedthatIDSPnaturallybeneﬁtsthesource-freeUDA,sinceitonlyconsidersthetargetstructure,whichwillbefurtherstudiedJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST20159inourfuturework.Intheend,wewouldliketoemphasizethatundoubtedlydomainalignmentremainsimportantfordomainadaptation.Thus,howtoobtainharmlessorperfectalignmentwiththeguidanceofmodelsmoothnessisstillournextpursuit.ACKNOWLEDGEMENTTheauthorswouldliketothankDr.JingjingGuandDr.YunyunWangfortheproofreadingofthismanuscript.ThisworkissupportedinpartbytheNSFCunderGrantNo.62076124.REFERENCES[1]S.Ben-David,J.Blitzer,K.Crammer,F.Pereira,etal.,“Analysisofrepresentationsfordomainadaptation,”Advancesinneuralinformationprocessingsystems,vol.19,p.137,2007.[2]S.Ben-David,J.Blitzer,K.Crammer,A.Kulesza,F.Pereira,andJ.W.Vaughan,“Atheoryoflearningfromdifferentdomains,”Machinelearning,vol.79,no.1-2,pp.151–175,2010.[3]M.Sugiyama,T.Suzuki,S.Nakajima,H.Kashima,P.vonB¨unau,andM.Kawanabe,“Directimportanceestimationforcovariateshiftadaptation,”AnnalsoftheInstituteofStatisticalMathematics,vol.60,no.4,pp.699–746,2008.[4]S.J.Pan,I.W.Tsang,J.T.Kwok,andQ.Yang,“Domainadaptationviatransfercomponentanalysis,”IEEETransactionsonNeuralNetworks,vol.22,no.2,pp.199–210,2010.[5]B.Fernando,A.Habrard,M.Sebban,andT.Tuytelaars,“Unsu-pervisedvisualdomainadaptationusingsubspacealignment,”inProceedingsoftheIEEEinternationalconferenceoncomputervision,pp.2960–2967,2013.[6]Y.GaninandV.Lempitsky,“Unsuperviseddomainadaptationbybackpropagation,”inInternationalconferenceonmachinelearning,pp.1180–1189,2015.[7]M.Long,J.Wang,G.Ding,S.J.Pan,andS.Y.Philip,“Adaptationregularization:Ageneralframeworkfortransferlearning,”IEEETransactionsonKnowledgeandDataEngineering,vol.26,no.5,pp.1076–1089,2013.[8]M.Baktashmotlagh,M.T.Harandi,B.C.Lovell,andM.Salzmann,“Unsuperviseddomainadaptationbydomaininvariantprojec-tion,”inProceedingsoftheIEEEInternationalConferenceonComputerVision,pp.769–776,2013.[9]M.Belkin,P.Niyogi,andV.Sindhwani,“Manifoldregularization:Ageometricframeworkforlearningfromlabeledandunlabeledexamples,”Journalofmachinelearningresearch,vol.7,no.Nov,pp.2399–2434,2006.[10]J.Hoffman,E.Tzeng,T.Park,J.-Y.Zhu,P.Isola,K.Saenko,A.Efros,andT.Darrell,“Cycada:Cycle-consistentadversarialdomainadaptation,”inInternationalconferenceonmachinelearning,pp.1989–1998,PMLR,2018.[11]Z.Cao,L.Ma,M.Long,andJ.Wang,“Partialadversarialdomainadaptation,”inProceedingsoftheEuropeanConferenceonComputerVision(ECCV),pp.135–150,2018.[12]Z.Cao,K.You,M.Long,J.Wang,andQ.Yang,“Learningtotransferexamplesforpartialdomainadaptation,”inProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition,pp.2985–2994,2019.[13]T.Matsuura,K.Saito,andT.Harada,“Twins:Twoweightedinconsistency-reducednetworksforpartialdomainadaptation,”arXivpreprintarXiv:1812.07405,2018.[14]S.Li,C.H.Liu,Q.Lin,Q.Wen,L.Su,G.Huang,andZ.Ding,“Deepresidualcorrectionnetworkforpartialdomainadapta-tion,”IEEETransactionsonPatternAnalysisandMachineIntelligence,vol.43,no.7,pp.2329–2344,2021.[15]Y.Kim,S.Hong,S.Yang,S.Kang,Y.Jeon,andJ.Kim,“Associativepartialdomainadaptation,”arXivpreprintarXiv:2008.03111,2020.[16]X.Liu,Z.Guo,S.Li,F.Xing,J.You,C.C.J.Kuo,G.E.Fakhri,andJ.Woo,“Adversarialunsuperviseddomainadaptationwithconditionalandlabelshift:Infer,alignanditerate,”2021.[17]J.Zhang,Z.Ding,W.Li,andP.Ogunbona,“Importanceweightedadversarialnetsforpartialdomainadaptation,”inProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition,pp.8156–8164,2018.[18]H.Liu,J.Wang,andM.Long,“Cycleself-trainingfordomainadaptation,”arXivpreprintarXiv:2103.03571,2021.[19]W.LiandS.Chen,“Unsuperviseddomainadaptationwithpro-gressiveadaptationofsubspaces,”arXivpreprintarXiv:2009.00520,2020.[20]F.D.Johansson,D.Sontag,andR.Ranganath,“Supportandinvertibilityindomain-invariantrepresentations,”inThe22ndIn-ternationalConferenceonArtiﬁcialIntelligenceandStatistics,pp.527–536,PMLR,2019.[21]H.Zhao,R.T.DesCombes,K.Zhang,andG.Gordon,“Onlearninginvariantrepresentationsfordomainadaptation,”inIn-ternationalConferenceonMachineLearning,pp.7523–7532,PMLR,2019.[22]V.Bouvier,P.Very,C.Chastagnol,M.Tami,andC.Hudelot,“Ro-bustdomainadaptation:Representations,weightsandinductivebias,”arXivpreprintarXiv:2006.13629,2020.[23]B.Li,Y.Wang,T.Che,S.Zhang,S.Zhao,P.Xu,W.Zhou,Y.Bengio,andK.Keutzer,“Rethinkingdistributionalmatchingbaseddomainadaptation,”arXivpreprintarXiv:2006.13352,2020.[24]J.Wang,W.Feng,Y.Chen,H.Yu,M.Huang,andP.S.Yu,“Visualdomainadaptationwithmanifoldembeddeddistributionalignment,”inProceedingsofthe26thACMinternationalconferenceonMultimedia,pp.402–410,2018.[25]V.Vapnik,Thenatureofstatisticallearningtheory.Springerscience&businessmedia,2013.[26]Y.Tsuboi,H.Kashima,S.Hido,S.Bickel,andM.Sugiyama,“Directdensityratioestimationforlarge-scalecovariateshiftadaptation,”JournalofInformationProcessing,vol.17,pp.138–155,2009.[27]J.Huang,A.Gretton,K.Borgwardt,B.Sch¨olkopf,andA.J.Smola,“Correctingsampleselectionbiasbyunlabeleddata,”inAdvancesinneuralinformationprocessingsystems,pp.601–608,2007.[28]W.Zellinger,T.Grubinger,E.Lughofer,T.Natschl¨ager,andS.Saminger-Platz,“Centralmomentdiscrepancy(cmd)fordomain-invariantrepresentationlearning,”arXivpreprintarXiv:1702.08811,2017.[29]S.Si,D.Tao,andB.Geng,“Bregmandivergence-basedregu-larizationfortransfersubspacelearning,”IEEETransactionsonKnowledgeandDataEngineering,vol.22,no.7,pp.929–942,2009.[30]M.Long,H.Zhu,J.Wang,andM.I.Jordan,“Deeptransferlearningwithjointadaptationnetworks,”inInternationalconferenceonmachinelearning,pp.2208–2217,2017.[31]K.Saito,K.Watanabe,Y.Ushiku,andT.Harada,“Maximumclassiﬁerdiscrepancyforunsuperviseddomainadaptation,”inProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition,pp.3723–3732,2018.[32]N.Courty,R.Flamary,andD.Tuia,“Domainadaptationwithregularizedoptimaltransport,”inJointEuropeanConferenceonMachineLearningandKnowledgeDiscoveryinDatabases,pp.274–289,Springer,2014.[33]M.Long,Z.Cao,J.Wang,andM.I.Jordan,“Conditionalad-versarialdomainadaptation,”inAdvancesinNeuralInformationProcessingSystems,pp.1640–1650,2018.[34]B.Sun,J.Feng,andK.Saenko,“Returnoffrustratinglyeasydomainadaptation,”inThirtiethAAAIConferenceonArtiﬁcialIntelligence,pp.–,2016.[35]Z.Cao,M.Long,J.Wang,andM.I.Jordan,“Partialtransferlearningwithselectiveadversarialnetworks,”inProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition,pp.2724–2732,2018.[36]C.Villani,Optimaltransport:oldandnew,vol.338.Springer,2009.[37]M.Yi,L.Hou,J.Sun,L.Shang,X.Jiang,Q.Liu,andZ.-M.Ma,“Improvedoodgeneralizationviaadversarialtrainingandpre-training,”arXivpreprintarXiv:2105.11144,2021.[38]V.N.Vapnik,“Anoverviewofstatisticallearningtheory,”IEEEtransactionsonneuralnetworks,vol.10,no.5,pp.988–999,1999.[39]T.Chen,S.Kornblith,M.Norouzi,andG.Hinton,“Asimpleframeworkforcontrastivelearningofvisualrepresentations,”inInternationalconferenceonmachinelearning,pp.1597–1607,PMLR,2020.[40]K.He,H.Fan,Y.Wu,S.Xie,andR.Girshick,“Momentumcontrastforunsupervisedvisualrepresentationlearning,”inProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.9729–9738,2020.[41]R.Rifkin,G.Yeo,T.Poggio,etal.,“Regularizedleast-squaresclas-siﬁcation,”NatoScienceSeriesSubSeriesIIIComputerandSystemsSciences,vol.190,pp.131–154,2003.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST201510[42]A.Rahimi,B.Recht,etal.,“Randomfeaturesforlarge-scalekernelmachines.,”inNIPS,vol.3,p.5,Citeseer,2007.[43]K.Saenko,B.Kulis,M.Fritz,andT.Darrell,“Adaptingvisualcat-egorymodelstonewdomains,”inEuropeanconferenceoncomputervision,pp.213–226,Springer,2010.[44]H.Venkateswara,J.Eusebio,S.Chakraborty,andS.Panchanathan,“Deephashingnetworkforunsuperviseddomainadaptation,”inProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition,pp.5018–5027,2017.[45]K.He,X.Zhang,S.Ren,andJ.Sun,“Deepresiduallearningforimagerecognition,”inProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pp.770–778,2016.[46]M.Long,Y.Cao,J.Wang,andM.Jordan,“Learningtransferablefeatureswithdeepadaptationnetworks,”inInternationalconfer-enceonmachinelearning,pp.97–105,2015.[47]B.Gong,Y.Shi,F.Sha,andK.Grauman,“Geodesicﬂowkernelforunsuperviseddomainadaptation,”in2012IEEEConferenceonComputerVisionandPatternRecognition,pp.2066–2073,IEEE,2012.[48]M.Long,J.Wang,G.Ding,J.Sun,andP.S.Yu,“Transferjointmatchingforunsuperviseddomainadaptation,”inProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pp.1410–1417,2014.WeikaiLireceivedhisB.S.degreeinInforma-tionandComputingSciencefromChongqingJiaotongUniversityin2015.In2018,hecom-pletedhisM.S.degreeincomputerscienceandtechniqueatChongqingJiaotongUniversity.HeiscurrentlypursuingthePh.D.degreewiththeCollegeofComputerScience&Technol-ogy,NanjingUniversityofAeronauticsandAs-tronautics.Hisresearchinterestsincludepatternrecognitionandmachinelearning..SongcanChenreceivedhisB.S.degreeinmathematicsfromHangzhouUniversity(nowmergedintoZhejiangUniversity)in1983.In1985,hecompletedhisM.S.degreeincomputerapplicationsatShanghaiJiaotongUniversityandthenworkedatNUAAinJanuary1986.TherehereceivedaPh.D.degreeincommunicationandinformationsystemsin1997.Since1998,asafull-timeprofessor,hehasbeenwiththeCollegeofComputerScience&TechnologyatNUAA.Hisresearchinterestsincludepatternrecogni-tion,machinelearningandneuralcomputing.HeisalsoanIAPRFellow.APPENDIXAGENERALIZATIONERRORTheorem1.GiventwodistributionsPandQ,wedenoter=maxkxi−xjk2whereWij>0.Ifamodelfis2r-coverwith(cid:15)smoothnessoverdistributionsPandQ,withprobabilityatleast1−θ,wehave:EQ(f)≤EP(f)+2(cid:15)+2MTV(P,Q)+Ms(2d)2(cid:15)2Dr2+1log2+2log(cid:0)1θ(cid:1)m+Ms(2d)2(cid:15)2Dr2+1log2+2log(cid:0)1θ(cid:1)n+Mrlog(1/θ)2m(1)ProofLetBW(P,r)={P:W∞(P,P)≤r},whereW∞isthe∞-thWasserstein-distance[?].LetPr=argminp∈BW(P,r)Ep(f)andQr=argminq∈BW(Q,r)Eq(f)EQ(f)=EQ(f)−EQr(f)+EQr(f)−EP(f)+EP(f)≤EP(f)+|EQr(f)−EQ(f)|+|EQr(f)−EP(f)|≤EP(f)+|EQr(f)−EQ(f)|+|EQr(f)−EPr(f)|+|EPr(f)−EP(f)|(2)Then,accordingtotheTheorems1and5inrecentstudy[?],wehave|EPr(f)−EP(f)|≤(cid:15)+Ms(2d)2(cid:15)2Dr2+1log2+2log(cid:0)1θ(cid:1)m(3)|EQr(f)−EQ(f)|≤(cid:15)+Ms(2d)2(cid:15)2Dr2+1log2+2log(cid:0)1θ(cid:1)n(4)and|EQr(f)−EPr(f)|≤2MTV(P,Q)+Mrlog(1/θ)2m(5)BypluggingtheEqs.3,4and5intoEq,2,wehave:EQ(f)≤EP(f)+2(cid:15)+2MTV(P,Q)+Ms(2d)2(cid:15)2Dr2+1log2+2log(cid:0)1θ(cid:1)m+Ms(2d)2(cid:15)2Dr2+1log2+2log(cid:0)1θ(cid:1)n+Mrlog(1/θ)2m(6)Q.E.DAPPENDIXBREPRESENTERTHEOREMTheorem2.RepresenterTheorem:TheparameterW∗=[w∗1,···,w∗h]fortheoptimizedsolutionfofEq.??canbeexpressedintermsofthecross-domainlabeledandunlabeledexamples,f(x)=n+mXi=1αiK(xi,x)andw=n+mXi=1αiφ(xi)(7)whereKisakernelinducedbyφ,αiisacoefﬁcient.ProofItiseasytoprovebycontradiction.Denotespan(φ(xi),1≤i≤n+m)asthelinearspacespannedbyvectorsφ(xi):1≤i≤n+m.Then,eachWjcanbeexpressedaswj=wkj+w⊥j(8)wherewkjisthecomponentalongthelinearspacespan(φ(xi),1≤i≤n+m)andw⊥jisthecomponentalongitsorthogonalcomplementspace.LettheoptimalsolutionW∗=W∗k+W∗⊥(9)whereW∗k=hw∗k1,w∗k2,···,w∗kmiandW∗⊥=(cid:2)w∗⊥1,w∗⊥2,···,w∗⊥m(cid:3).Then,weeasilyhave(cid:0)W∗⊥(cid:1)TW∗k=0.WeassumethatW∗⊥6=0,anddenoteJ(·)astheobjectivevalueofIDSP,thenwehave:J(W∗)=nXi=1(yi−f(xi))2+λkfk2K+γn+mXi,j=1f(xi)Lijf(xj)=nXi=1(yi−w∗|φ(xi))2+λtr(w∗|w∗)+γn+mXi,j=1w∗|φ(xi)Lijw∗|φ(xj)=nXi=1(cid:16)yi−w∗k|φ(xi)(cid:17)2+λ(cid:16)tr(cid:16)w∗k|w∗k(cid:17)+tr(cid:0)w∗⊥|w∗⊥(cid:1)(cid:17)+γn+mXi,j=1w∗k|φ(xi)Lijw∗k|φ(xj)+γn+mXi,j=1w∗⊥|φ(xi)Lijw∗⊥|φ(xj)(10)whileW∗⊥6=0,andListheLaplacianmatrix,weeasilyhaveJ(w∗k)≤J(w∗).Thus,W∗kismoreoptimalthanW∗,whichiscontradictorywiththefact.ThisyieldstheW∗=W∗k.Q.E.DarXiv:2108.12867v1  [cs.CV]  29 Aug 2021APPENDIXCINTRADOMAINSTRUCTUREPRESERVINGWITHJOINTDOMAINADAPTATIONFORUDAItshouldbenotedthattheriskyofdomainalignmenttendstobelowerinUDA.Wefurtherconductexperimentsbyaddinganadditionaljointdistributionadaptation(JDA)term[?].Theentireobjectiveisgivenasfollows:f=argminf∈HKnXi=1(yi−f(xi))2+λkfk2K+γn+mXi,j=1f(xi)Lijf(xj)+ηRJDA(f)(11)whereRJDA(f)istheJDAterm,whichisdenotedas:RJDA(f)=(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)1nnXi=1f(xi)−1mn+mXj=n+1f(xj)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2H+(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)1n(c)Xxi∈D(c)sf(xi)−1m(c)Xxj∈D(c)tf(xj)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2H(12)whereD(c)s={xi:xi∈Ds∧y(xi)=c}isthesetofthesourcedatabelongingtoclassc.andn(c)=(cid:12)(cid:12)(cid:12)D(c)S(cid:12)(cid:12)(cid:12).Correspondingly,D(t)s={xj:xj∈Dt∧ˆy(xj)=c},whereˆy(xj)isthepseudo(predicted)labelofxjandm(c)=(cid:12)(cid:12)(cid:12)D(c)t(cid:12)(cid:12)(cid:12).LearningAlgorithmTheorem3.Theminimizerofoptimizationproblem11admitsanexpansionf(x)=n+mXi=1αiK(xi,x)andw=n+mXi=1αiφ(xi)(13)whereKisakernelinducedbyφ,αiisacoefﬁcient.ProofTheproofissimilartoTheorem2.Hence,weomittheproofforTheorem3.ByincorporatingEq.13intoEq.11,weobtainthefollowingobjective:α=argminα∈Rn+m(cid:13)(cid:13)(cid:0)Y−αTK(cid:1)V(cid:13)(cid:13)2F+tr(cid:0)λαTKα+αTK(γL+ηM)Kα(cid:1).(14)whereVisthelabelindicatormatrixwithVii=1ifi∈Ds,otherwiseVii=0.MistheMMDmatrixwhichcanbecomputedas:(MC)ij=1n(c)n(c),xi,xj∈D(c)s1m(c)m(c),xi,xj∈D(c)t−1n(c)m(c),(xi∈D(c)s,xj∈D(c)txj∈D(c)s,xi∈D(c)t0,otherwise(15)Algorithm1LearningalgorithmforPASInput:nsourcelabeleddatasetsDs={xsi,yi}ni=1mtargetunlabeleddatasetsDt={xsi}mi=1Hyper-parametersλ,γ,pandη;Output:PredictiveClassiﬁerf1:InitializethePseudoLabelˆy2:CalculatethegraphLaplacianL3:ConstructkernelKbyaspeciﬁckernelfunction4:whilenotconvergedo5:ConstructMMDmatrixM;6:Computeα;7:UpdatethePseudoLabelˆy8:endwhile9:ReturnClassiﬁerf.(a)Ofﬁce-Home(UDA)(b)Image-Clef(UDA)(c)Ofﬁce-Home(PDA)(d)Ofﬁce-31(PDA)Fig.1.classiﬁcationaccuracyw.r.t.ηandγ,respectivelyForclarity,wecanalsocomputeM0withEq.15ifsubsti-tutingn(0)=n,m(0)=m,D(0)s=Ds,D(0)t=DtSettingderivativeofobjectivefunctionas0leadsto:α=((V+γL+ηM)K+λI)−1VYT.(16)ThelearningalgorithmsaresummarizedinAlgorithm1.SensitivityAnalysisTheproposedIDSP+JDAmethodinvolvesoneaddi-tionalmorehyper-parametesr(i.e.,η).Toinvestigatethesensitivityofthesmoothnessandthedomainalignment,weconductexperimentsonofﬁce-Home,Image-ClefandOfﬁce-31datasets.Speciﬁcally,weranIDSPbysearch-ingλ∈{0.01,0.02,0.05,0.1,0.2,0.5,1,2,5,10},η∈{0.01,0.02,0.05,0.1,0.2,0.5,1,2,5,10}withλ=0.1and(a)Ofﬁce-Home(UDA)(b)Image-Clef(UDA)(c)Ofﬁce-31(PDA)(d)Ofﬁce-Home(ODA)Fig.2.classiﬁcationaccuracyofIDSPandIDSP+JDAp=10.AsweobservedinFigure1,theIDSPperformsrobustlyandinsensitivelyonboththeclosed-setUDAonawiderangeofparametervaluesofηandγ.Tosumup,theperformanceofIDSPstaysrobustwithawiderangeofregularizationparameterchoice.Theycanbeselectedwithoutknowledgeinrealapplications.Further,theperformancewillbedecreasewithηincreasedinPDA.TheresultsillustratesthatdomainalignmentdonotsuitthePDAsetting.ClassiﬁcationResultsTheclassiﬁcationresultsofIDSPandIDSP+JDA((γ=2,η=0.5forUDAandγ=10,η=0.01forPDA)aregiveninFigure2andTableI.AswecanobservedinFigure2andTableI,withJDAincorporatesinlearningclassier,theTABLEIACCURACY(%)OFIDSPANDIDSP-JDADataSetsIDSPIDSP-JDAOfﬁce-Home(UDA)67.068.0Image-Clef(UDA)87.989.3Ofﬁce-Home(PDA)74.963.3Ofﬁce-31(PDA)98.389.3performanceareincreasedinalmostalltasksintheUDAsetting.Incontrast,theperformancearesigniﬁcantlydecreasedinalltasksinthePDAsetting.TheresultsillustratethatthedomainalignmentmayintroducenegativetransferinPDA,sincePDAdonotsatisfytheassumptionofdomainalignment.Incontrast,modelsmoothnesscandowellonbothPDAandUDAsettings.APPENDIXDGRAPHCONSTRUCTIONInordertodemonstratetheeffectivenessoftheproposedIntraDomainStructurePreserving,weperformanablationstudybylearningaclassiﬁerwithdifferentstructurepreservingconstraints.Speciﬁcally,weusenostructurepreserving(nP,i.e.,γ=0),manifoldstructurepreservingofsourceandtargetdomain(CST),intrastructureofbothsourceandtargetdomain(ST)andintrastructurepreservingoftargetdomain(T).Thepair-wiseafﬁnitymatrixˆG(T)ofthegraphcanbeformulatedasfollows:G(T)ij=(cid:26)ˆGij,xiandxj∈Dt0,otherwise(17)Thepair-wiseafﬁnitymatrixˆG(ST)ofthegraphcanbeformulatedasfollows:G(ST)ij=(cid:26)ˆGij,(xi,xj)∈Dtor(xi,xj)∈Ds0,otherwise(18)Thepair-wiseafﬁnitymatrixˆG(CST)ofthegraphcanbeformulatedasfollows:G(ST)ij=ˆGij(19)