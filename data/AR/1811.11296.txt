Taking Control of Intra-class Variation in Conditional GANs
Under Weak Supervision

Richard Marriott1,2, Sami Romdhani2 and Liming Chen1
1 Ecole Centrale de Lyon, Ecully, France
2 IDEMIA, Courbevoie, France

0
2
0
2
c
e
D
9
1

]

V
C
.
s
c
[

2
v
6
9
2
1
1
.
1
1
8
1
:
v
i
X
r
a

Abstract— Generative Adversarial Networks (GANs) are able
to learn mappings between simple, relatively low-dimensional,
random distributions and points on the manifold of realistic
images in image-space. The semantics of this mapping, however,
are typically entangled such that meaningful image properties
cannot be controlled independently of one another. Conditional
GANs (cGANs) provide a potential solution to this problem,
allowing speciﬁc semantics to be enforced during training. This
solution, however, depends on the availability of precise labels,
which are sometimes difﬁcult or near impossible to obtain,
e.g. labels representing lighting conditions or describing the
background. In this paper we introduce a new formulation
of the cGAN that is able to learn disentangled, multivariate
models of semantically meaningful variation and which has the
advantage of requiring only the weak supervision of binary
attribute labels. For example, given only labels of ambient /
non-ambient lighting, our method is able to learn multivariate
lighting models disentangled from other factors such as the
identity and pose. We coin the method intra-class variation
isolation (IVI) and the resulting network the IVI-GAN. We
evaluate IVI-GAN on the CelebA dataset and on synthetic 3D
morphable model data, learning to disentangle attributes such
as lighting, pose, expression, and even the background.

I. INTRODUCTION
Generative Adversarial Networks (GANs) [11] are capable
of generating highly realistic synthetic data-samples. This
capability is a direct result of the insightful training objective
of the generative network: to produce synthetic data that
cannot be distinguished from the real thing by a second
adversarial network aiming to classify samples as either real
or fake. Recent advances such as use of the Wasserstein loss
[1] and various forms of regularisation [12], [19], [5] and
training techniques [14] mean that training of GANs is now
stable enough to generate data-samples with thousands of
dimensions. For example, deep convolutional GANs (DC-
GANs) can now produce convincing images at mega-pixel
resolution or higher [14], [6].

While classical DC-GANs can generate realistic images,
the precise form of these images cannot be easily controlled.
The subject of the images is dependent in some way on the
values of the random input vector, but a priori we do not
know in what way. An obvious solution to this problem is
the Conditional GAN (cGAN), in which the GAN’s discrim-
inator is trained to distinguish real image+label pairs from
fake image+label pairs, thereby encouraging the generator
to produce images that correctly correspond to the label
upon which it is conditioned. In their typical form, cGANs
are trained under the strong supervision of these labels:
given binary category labels, images belonging to certain

Fig. 1.
Lighting conditions manipulated by IVI-GAN for four synthetic
identities. Left-hand column: ρlighting = 04 (ambient); the other columns
show the effect of assigning a value of 3.0 or −3.0 to individual elements
of the lighting vector while keeping other parameters constant.

categories can be generated (but whose speciﬁc form is
still governed by the entangled, real-valued random vector);
given continuous, real-valued labels, cGANs can be forced to
obey more speciﬁc semantics. Previous works, for example,
have conditioned on binary hair-colour labels [9], and on
continuous expression action unit magnitudes [23]. However,
the annotation of training images with descriptions of multi-
dimensional phenomena such as expression and lighting, is
notoriously difﬁcult.

In this paper we propose a new formulation of cGAN
which is able to learn disentangled, multivariate representa-
tions of variation in images using only the weak supervision
of simple, binary labels indicating the presence or absence of
that form of variation, e.g. “ambient lighting / non-ambient
lighting”. Training data need not be labelled with precise
parameter-estimations of prior models of the image property.
We coin the method intra-class variation isolation (IVI) and
the resulting network the IVI-GAN. A video demonstrating
the continuous variation of various isolated parameter sets
can be viewed at https://youtu.be/hoWOFeADwdY.

 
 
 
 
 
 
Our contributions are

• An investigation of weakly supervised learning in con-
ditional GANs and their ability to learn disentangled
semantics given only binary labels;

• Proposition of a novel, physically motivated method of
employing the GAN’s generator which acts to enforce
the separation of lighting from other forms of variation;
• A novel conditional GAN formulation that allows disen-
tangled, multivariate models of variation to be learned
with only the weak supervision of binary labels whilst
preserving the identity: the IVI-GAN.

The rest of the paper is organised as follows: In Section
II we discuss related work; in Section III we formalise our
novel conditional GAN formulation; in Section IV we give
speciﬁc details of our implementation; and in Section V we
present an analysis of results produced using the IVI-GAN.

II. RELATED WORK

Most recent works aiming to manipulate the semantics
of synthesised images take the form of image-translation
networks or auto-encoders with adversarial losses added to
help ensure that images look realistic. In [15] face images
are translated from being “at pose” to frontal while a
discriminator ensures realism and that expression remains
constant. The identity and other image properties, however,
are only preserved via a pixel-wise comparison of the gener-
ated image with a ground-truth frontal image. Since such
corresponding pairs of images are rarely available, other
researchers have instead proposed auto-encoding methods
such as CycleGAN [27], whose image-shaped latent space
is trained to resemble the distribution of a different class
of images, not necessarily paired with the input
image.
For example, in both [9] and [4], a discriminator is used
to encourage generated images to belong to different ex-
pression categories, while the cycle-consistency loss ensures
that the general structure of the image remains unaffected,
thus implicitly preserving the identity. In [23], continuous
expression action unit labels are used to provide continuous
control over the transformed images, rather than just control
over the expression category. The downside, however, is that
precise, real-valued labels are difﬁcult to obtain. In addition,
methods relying on cycle-consistency losses cannot be easily
used to manipulate pose since this involves more signiﬁcant
alterations to image structure, thereby destroying the implicit
constraint on identity.

Other works consider identity preservation explicitly by
adding biometric losses. In [16] a pre-trained biometric
network is added as a constraint during manipulation of
expression. In [2] a biometric network is trained in parallel
with the generative network whereas in [26] the GAN’s
discriminator itself is trained to classify the identity as a
secondary task. Both [2] and [26] are able to convincingly
modify the pose of input images. All of these methods, how-
ever, require strong supervision to control image properties:
in [26] ﬁne-grained pose-category labels are needed and in
[16], similar to [23], continuous expression labels are needed.

InfoGAN [7] is the only work of which we are aware
that attempts to disentangle control of image properties in
an entirely unsupervised manner. By maximising the mutual
information between a small subset of input parameters
and the generated images, those parameters are attributed
control over the most signiﬁcant forms of variation in the
image dataset. In practice these forms of variation tend to
resemble semantics such as pose and lighting. However,
there is no guarantee of which semantics will be learned,
nor that identity will be preserved. Rather than an entirely
unsupervised approach, in this work we propose a weakly
supervised alternative, compromising between the unpre-
dictable semantics of InfoGAN and the strong requirement
for precise labels of typical conditional generation methods.
Our weakly supervised technique allows the learning of
speciﬁc forms of variation but does not require a prior model
of that variation.

Concurrent with our own work, [25] demonstrates an alter-
native method for achieving continuous control over image
properties using only binary labels. Rather than training a
conditional GAN on labelled training images,
labels are
used retrospectively to identify the principal directions of
variation of various attributes in the latent space of a pre-
trained GAN. In their work, no special attention is paid to
preserving identity. Also, in the absence of multiple labels,
their method is not capable of identifying multi-directional
forms of variation such as illumination or the conﬁguration of
the background. In Section V we add a comparison with [25]
since, despite these differences, it is the work most closely
related to our own.

III. INTRA-CLASS VARIATION ISOLATION

Before describing intra-class variation isolation, we intro-
duce the conditional GAN. Our best results are achieved
using Wasserstein GANs and so we present this version.

A. Conditional GANs

A conditional GAN (cGAN) [18] consists of two networks
that are trained alternately: the discriminator, D, is trained to
discriminate fake images from real ones; and the generator,
G, is trained to generate images that will better fool the
discriminator. The function that is minimised in order to train
the discriminator is

LθD = E(x,y)∼pdata [D(x, y; θD)]

− Ez∼N ,ρ∼pdata [D(G(z, ρ; θG), ρ; θD)]

(1)

where x is an image and y ∈ Rn the associated vector
of labels drawn from the distribution of real data; z ∈
N m is a random Gaussian vector but could alternatively
be drawn from any other real-valued distribution; ρ ∈ Rn
is a vector of labels used to condition the generator, here
selected at random from the training dataset; and θD and
θG are the parameters of the discriminator and generator
networks respectively. In the Wasserstein GAN, the output
of D is a single, unbounded scalar value; i.e. no squashing-
function is applied. Provided that D remains k-Lipschitz

to θD
continuous, minimising equation (1) with respect
yields an estimator of the Wasserstein distance (to a scalar,
k) between the distributions of real, labelled data and ρ-
parameterised, generated data [1]. Lipschitz continuity is
usually achieved by adding an additional gradient-penalty
term [12] to the discriminator loss in equation (1).

The trained discriminator is then used as the generator loss

and equation (2) is minimised with respect to θG.

LθG = Ez∼N ,ρ∼pdata [D(G(z, ρ; θG), ρ; θD)]

(2)

Since generated images are never compared directly with the
training images, the generator learns a smooth distribution of
realistic images from which new ones can be sampled.

B. Intra-class variation isolation

In order to control some attribute of a generated image in
a continuous fashion, for example the pose seen in a face-
image, a typical cGAN requires each training image to be
labelled with a precise and accurate pose-estimation. Our
method, intra-class variation isolation (IVI), requires only
the weak supervision of binary category labels indicating
whether or not a particular form of variation is present. The
IVI-GAN is then allowed to learn its own multivariate model
of that form of variation.

Technically, the changes to the standard cGAN are very
simple and involve modifying only the form of the labels
provided to the cGAN. The function to be minimised to train
the IVI-GAN’s discriminator is

LθD = E(x,y)∼pdata [D(x, y; θD)]

− Ez∼N ,β∼pdata [D(G(z, ρ; θG), β; θD)]

(3)

where y ∈ {0, 1}n are now binary labels for n categories,
and β ∈ {0, 1}n are binary labels sampled from the same
distribution as y (but, as before, do not necessarily have to
be the same). The novel aspect of the loss is the way in
which the random parameters, ρ, are chosen.

All variation in images generated by the IVI-GAN must
be derived from the combination of z and ρ. The values of
z are independent of β and so can always be used freely by
the generator, irrespective of the labels. The parameter sets
forming ρ, however, are only available when certain forms
of variation are labelled as being present. The idea is that the
generator will then only use those parameters to describe that
form of variation since the presence of non-zero parameters
cannot be relied upon to describe anything else.

When labelling training images, it is best that βi = 0 be
used to describe a unique image property. For example, in the
case of expression, βexp = 0 should correspond to a neutral
expression and βexp = 1 to all non-neutral expressions. If
βexp = 0 were chosen to represent a non-unique property,
such as “not smiling”, then all of the different ways of not
smiling would necessarily end up being encoded by z. In
the case of lighting, we chose βlighting = 0 to represent
“ambient lighting”. This means that the colour and intensity
of ambient light in our images is encoded by z but that all
non-ambient lighting phenomena are encoded by ρlighting.
We ﬁnd that the IVI mechanism does indeed encourage
disentanglement of labelled variation. This is aided by the
natural parsimony of the generator which must ﬁnd efﬁcient
ways of representing the training image distribution despite
having only limited capacity. To do this, common features
are, of course, reused by the generator to form different
images. When making only subtle modiﬁcation to images,
e.g. adding lighting effects, the generator tends to leave
the general structure of images unchanged, thus implicitly
preserving the identity along with other image attributes.
However, when making more signiﬁcant changes to images,
such as modifying the pose, this implicit identity preservation
cannot be relied upon. We therefore introduce an explicit
constraint on the identity which is described in the following
section.

C. The biometric identity-constraint

ρ =








p1
p2
...
pn








where pi ∈

(cid:40)

N qi ,
0qi,

if βi = 1
if βi = 0

(4)

Here, N qi is a random Gaussian vector of length qi, and 0qi
is a vector of zeros. Analogous to the cGAN, the function
then minimised to train the generator is

LθG = Ez∼N ,β∼pdata [D(G(z, ρ; θG), β; θD)]

(5)

N.B. The mechanism used to form ρ in (4) could be
interpreted as a function f (z(cid:48), β) = (z, ρ), where z(cid:48)
is
an extended random vector providing the additional ran-
dom values used to form ρ. Since this function could be
implemented as part of a modiﬁed generator, G(cid:48) (which
is permitted to have an arbitrary architecture), we have
G(z, ρ) = G(f (z(cid:48), β)) = G(cid:48)(z(cid:48), β); i.e. equations (3) and
(5) are mathematically equivalent to (1) and (2) but for binary
labels. Figure 2 depicts this intra-class variation isolation
mechanism as part of our IVI-GAN.

To ensure that identity remains constant upon adjusting
other image-properties, we have added a term to the gen-
erator loss of IVI-GAN involving a pre-trained biometric
network [13] that accepts a facial
image as input and
produces a 128-dimensional encoding of the identity.
(cid:107)B(G(z, ρ)) − B(G(z, ρ2))(cid:107)2(cid:105)
(cid:104)

LID = Ez∼N

(6)

where B is the biometric network and ρ2 is a second set
of random label-parameters. (N.B. we have dropped the θ
for convenience of notation.) By running the generator twice
for the same z but different ρ, and constraining the identity
encodings to remain close, we ensure that the identity is
encoded as part of z and not affected by changes to ρ. The
full generator loss is then

LθG = Ez∼N ,β∼pdata [D(G(z, ρ), β)] + λIDLID

(7)

where λID is a hyper-parameter to be tuned.

The biometric identity constraint is depicted on the right-

hand side Figure 2.

Fig. 2. An illustration of IVI-GAN. The real-valued parameter-vector, ρ, is formed by masking sections of an extended, random vector using the randomly
selected, binary label-vector, β. It is β (not ρ) that is then fed to the discriminator with the generated image.

D. An additional, structural constraint for lighting

Despite the natural tendency of the generator to leave the
structure of images unaffected when modifying properties
such as lighting, subtle unwanted changes can still be seen,
e.g. small changes to pose. To avoid this, we introduce a
constraint on the image structure to be used when modifying
lighting conditions.

Lighting is an additive phenomenon. For example, an
image of a scene with two light sources is equivalent to
the sum of two images of the same scene with the two
light sources acting on it separately. One way to generate
an image of a face under a particular lighting condition,
therefore,
images of
is to add together two constituent
the same subject. By re-formulating our generator in this
way, the constraint that the composite image must appear
realistic to the discriminator ensures that features in the two
constituent images are of the same general structure. If not,
the composite image would appear blurry and unrealistic due
to misalignment and other inconsistencies. We propose that
the generator be replaced with the following:

G(z, ρ)comp = G(z, 0) + G(z, ρ)

(8)

where ρ represents lighting parameters only and 0 is a vector
of zeros the same length as ρ indicating that ambient lighting
should be generated. In our IVI-GAN, G(z, ρ)comp replaces
G in both equation (3) and (5). By choosing G(z, 0) as the
second constituent image, we straightforwardly ensure that
G(z, 0)comp generates ambiently lit images. We also ﬁnd that
this formulation can be used to help constrain facial structure
when modifying the appearance of the background.

IV. IMPLEMENTATION

Our implementation is built upon the stable and efﬁcient
progressive GAN of [14], a Tensorﬂow implementation of
which was made publicly available by Nvidia. The progres-
sive GAN begins by generating images of 4 × 4 resolution
and then progressively fades in new convolutional upscaling
layers until
the desired resolution is reached. There has
been much recent work on improving the quality of GAN-
generated images published in the literature. We tested a

selection of these enhancements and found that our best
results were produced by a progressive Wasserstein GAN
with the standard gradient penalty (GP) term of [12] where
the weight of the GP term was allowed to evolve throughout
training based on an adaptive lambda scheme similar to that
in [8]. In the generator we use orthogonal initialisation of
weights and replace the pixel-wise feature normalisation used
in [14] with the orthogonal regularisation of [5] using the
suggested weight of 0.0001.

A. Conditioning the GAN

The way in which labels and label-parameters are used to
condition GANs is an open area of research. For example,
[20] ﬁnds that, given certain assumptions about the form of
the distribution of data, the optimal method of conditioning
the discriminator should be to learn some inner-product of
the label-vector with the channels at each pixel; in [10] the
generator network is conditioned via instance-normalisation
parameters. We have used the more straight-forward method
of concatenating label-vectors with inputs but expect that
these more sophisticated methods of conditioning may be
used to improve results. We concatenate our IVI parameter
vector, ρ, with the random vector on input to the generator,
and on input to the discriminator we concatenate the binary
labels, y and β, as additional channels repeated at each pixel
of the real and generated images respectively.

An analysis of where best to concatenate conditioning
vectors was performed in [22]. They predicted that earlier in
the network should be better since the discriminator would
be afforded more learning interactions with the information.
In fact they conclude that it is best to concatenate the condi-
tioning with the ﬁrst hidden layer. However, in a progressive
architecture, the ﬁrst convolutional layer of the discriminator
is not faded in until the ﬁnal stages of training. In our case, it
therefore makes more sense to concatenate the conditioning
with the image where it is then scaled down and fed to each
layer of the progressive network as they are being faded in.
Ultimately, once all layers of the progressive discriminator
are active, the conditioning information and image are only
fed to the ﬁrst layer of the discriminator.

Fig. 3. A continuous eye-wear model learned by IVI-GAN. The style of
eye-wear is controlled by the direction of a two-dimensional unit vector.
Setting the length of the vector to zero removes the eye-wear (top row).

Fig. 4. A colourful selection of images demonstrating the capability of
IVI-GAN to generate a range of different backgrounds while preserving
identity and other image attributes.

Many applications make use of auxiliary classiﬁers (ACs)
[21] as a way of ensuring that conditional parameters are not
ignored during generation of images. We tested this method
in conjunction with IVI-GAN using the auxiliary classiﬁer
already implemented in Nvidia’s progressive GAN code.
However, we found results to be unsatisfactory. As noted in
[19], auxiliary classiﬁers encourage the generator to produce
images that are easy to classify; a goal which is not in
alignment with the principal training objective of the GAN.
We found that, given a large weight in the discriminator
loss, the AC-term caused mode-collapse, squeezing variation
into narrow, well-separated categories. For example, upon
varying continuous, conditional pose parameters we observed
a discrete jump in the generated pose between frontal and
large poses. Giving less weight to the AC-term ameliorated
the discrete jumps in pose. However, more subtle artefacts
remained, such as broken noses pointing in one direction
or the other; a feature obviously used by the discriminator
to help classify slightly non-frontal poses. See Figure 5 for
examples of this behaviour.

V. RESULTS

We have evaluated IVI-GAN on the CelebA dataset [17],
and on a dataset of synthetic face images generated using
the Basel 3D morphable model (3DMM) [3]. In Section V-
A we present the results for CelebA, including a qualitative
comparison with similar results taken from [25]. In Section
V-B we investigate pose changes in images generated from
CelebA to give an idea of the form and consistency of
multivariate models learnt via weak supervision. We then
show results demonstrating the efﬁcacy of our biometric

Fig. 5.
auxiliary classiﬁer (not used in IVI-GAN).

Examples of broken nose features generated during tests of an

identity constraint in Section V-C and ﬁnally, in Section V-D,
we show additional results for a balanced dataset of synthetic
3DMM images.

A. Taking control of variation in CelebA

We trained IVI-GAN on a 100k image subset of the
CelebA dataset. Images were prepared in a similar way
to the CelebA-HQ dataset of [14] but super-resolution was
not used. Our network was trained progressively up to a
resolution of 128 × 128 and was conditioned on a selection
of the attribute labels available with CelebA. These attributes
were complemented with binary labels for lighting, the back-
ground, and for pose. Lighting and background labels were
found by hand labelling a set of 10k images as containing
either ambient or non-ambient lighting, and having either
plain or busy/coloured backgrounds. Two simple classiﬁers
were then trained to label the remaining images. Pose labels
were found by applying an off-the-shelf pose detector and
categorising all images with yaw and pitch greater than three
degrees as being non-frontal.

Figures 1, 3, 4, 6 and 7 show the effect of varying
the multi-dimensional parameter vectors learned for lighting
(4 parameters), eye-wear (2 parameters normalised to unit

Fig. 6.
Images demonstrating the effect of varying one of the pose
parameters, used by IVI-GAN to represent yaw-like variation. The parameter
is varied between −3.0 and 3.0. In the middle column ρpose = (0, 0).

Fig. 8.

Images taken from Shen et. al. (2019) [25].

despite [25] having taken no explicit steps to achieve this.
We suspect that this would not be the case, however, if pitch
were to be varied using the same method. With fewer images
in the training set exhibiting pitch variations, correlation of
large pitches with certain identities can lead to shifts towards
those identities. In contrast to [25], IVI-GAN simultaneously
learns a multivariate representation of both yaw and pitch. It
also explicitly ensures that identity-drift is kept to a minimum
via its biometric identity constraint.

B. Weak learning of multivariate models

IVI-GAN is able to learn its own models of variation given
only binary labels that indicate the presence or absence of
that form of variation. A desirable property of such a model
is that the same parameter values result in the same seman-
tic properties irrespective of the identity and other image
properties. In Table I we compare the consistency of poses
generated by IVI-GAN (without ID constraint) with those
generated by a cGAN conditioned on precise, real-valued
pose labels. The only difference between the cGAN and the
IVI-GAN is that, for the cGAN, ρpose are pose labels instead
of random parameters, and are fed to the discriminator in
place of the binary labels, βpose. We generated 100 random
identities for each of the pose parameter conﬁgurations given
in Table I and then used a pose-detector to ﬁnd the mean
generated pose and the standard deviation of angles from that
direction. Note that, for a more straightforward comparison,
the pose parameter conﬁgurations that were fed to the cGAN
are the mean poses detected in the images generated by IVI-
GAN. We see that, despite the absence of strong supervision,
IVI-GAN is able to generate poses with a consistency close
to that of a standard cGAN. (Note that the consistency of
the cGAN statistics may be artiﬁcially high since the same
detector was used to label the training images.) The form of
the pose model learned by IVI-GAN (with ID constraint) is
depicted in Figure 9 and examples of the images analysed
in Table I are given in Figures 10 and 11. It can be seen
that the visual quality of images generated by IVI-GAN is
similar to that of the cGAN.

C. Efﬁcacy of the biometric constraint

As previously mentioned, modiﬁcations to images that
signiﬁcantly affect their general structure, such as changes
to pose, can lead to identity shift and shifts in other image
properties. Changes to the property of pitch seem to be

Images demonstrating the effect of varying the other pose parameter,
Fig. 7.
used by IVI-GAN to represent pitch-like variation. The parameter is varied
between −3.0 and 3.0. In the middle column ρpose = (0, 0).

length), background (10 parameters) and pose (2 parameters).
Each row of images in Figure 3 corresponds to a particular
conﬁguration of ρglasses. We use a vector of two parameters
normalised to unit length. Only four instances of variation are
shown but the style of glasses can be varied continuously by
rotating the unit vector, with each style morphing smoothly
into the next. Glasses can be removed completely by setting
ρglasses = (0, 0) (top row). We see that modiﬁcations to the
style of glasses are well disentangled from the other image
parameters and from the identity.

In Figure 4, each row corresponds to a different random
instantiation of ρbackground. (Setting ρbackground = 010
results in the same set of images as shown in the ﬁrst
row of Figure 3.) Again, we see that modiﬁcations to the
background leave other image properties and the identity
largely unaffected. However, we notice that certain features
of the background have a tendency to be present in most
images of certain identities. We believe this effect is due to
unwanted, spurious correlations in the training dataset.

Although unguided by precise labels, Figures 6 and 7
show that IVI-GAN has learned to use one pose parameter
to represent yaw-like variation and the second to represent
pitch-like variation. Other image properties such as lighting,
the background and the identity remain consistent. These
results (and also those of Figure 3) can be compared with
those in Figure 8 which have been taken from Shen et.
al. (2019) [25]. Note that in [25], images were generated
at higher resolution. Here, we have down-sampled them to
128×128 resolution for closer comparison with our own. The
identities in Figure 8 seem to be reasonably well preserved

TABLE I
STATISTICS OF POSES DETECTED IN IMAGES OF 100 RANDOM

IDENTITIES FOR THE GIVEN PARAMETER-CONFIGURATIONS.

GAN conﬁguration

Type

IVI-GAN

cGAN

ρ
(0.0, 1.0)
(0.0, 2.0)
(0.0, 3.0)
(10.2◦, −1.3◦)
(23.8◦, −6.5◦)
(33.7◦, −12.6◦)

Mean pose

yaw
pitch
10.2◦
−1.3◦
−6.5◦
23.8◦
33.7◦ −12.6◦
0.8◦
9.3◦
23.5◦
−5.2◦
32.5◦ −11.4◦

StdDev
pose
7.5◦
8.8◦
8.8◦
4.8◦
6.3◦
6.9◦

Fig. 10.
Examples of images analysed in Table I generated by an IVI-
GAN. Five random identities are shown in frontal poses (top row) and with
pose parameters prescribed as ρpose = (0.0, 2.0) (bottom row).

Fig. 9. Detected poses in images generated by an IVI-GAN. Horizontal and
vertical axes indicate detected yaw and pitch for a selection of parameter
values (indicated in the plot) averaged over 100 identities.

particularly prone to this issue. (See Figure 12.) To counter
these problems, IVI-GAN incorporates the explicit, biometric
identity constraint described in Section III-C. The biomet-
ric network [13] was pre-trained on images of resolution
96 × 96 and so we only activate the additional ID-loss
during the ﬁnal stabilisation period of the training of the
progressive GAN. We performed experiments with λID =
[1.0, 0.1, 0.01, 0.001, 0.0001] and ﬁnally use λID = 0.0001.
Higher values were found to inhibit pose-variation too much.
Comparisons were made between biometric encodings of
100 random identities generated at a frontal pose and the
same identities (same z vectors) generated using the pose
parameter conﬁgurations given in Table II. The ﬁnal three
rows of Table II show that LID has the desired effect
of ensuring that images generated at larger poses match
their frontal counterparts. With the ID-constraint enabled,
the lowest mean detected cosine similarity is now 0.78 at an
average pose of (−13.6◦, −14.4◦). Note that the biometric
network used to produce these matching scores was not the
same as that used to constrain the IVI-GAN. Figures 6 and 7
show examples of the extent to which identity is constrained
as pose parameters are varied.

Fig. 11. Examples of images analysed in Table I generated by the cGAN.
Five random identities are shown in frontal poses (top row) and with pose
parameters prescribed as ρpose = (23.8◦, −6.5◦) (bottom row).

by a 3D morphable model (3DMM) [3] and lit using a
spherical harmonic lighting model [24]. Identities and ex-
pressions were sampled from random Gaussian distributions,
and lighting and pose from uniform distributions. Figure 13
demonstrates that, given adequate data, IVI-GAN is able
to generate high-quality results for the full range of poses,
i.e. for yaws in the range [−90◦, 90◦] (top) and pitches in
the range [−45◦, 45◦] (bottom). We note that, since there
is no explicit constraint on expression, the expression we
selected for the frontal image is lost during the disruptive
pose changes. The desired expression can often be recovered,
however, by readjusting expression parameters afterwards,
as has been done in the bottom row of Figure 14. Since
adjusting the expression is a more subtle image-modiﬁcation,
it does not affect the pose.

With full control over the synthetic dataset, we were
able to easily generate labels of neutral/non-neutral expres-
sion. (Similar labels were not available during our tests
on CelebA.) The top row of Figure 14 shows the range
of expressions learnt by IVI-GAN, whilst the bottom row
shows some of the more distinctive lighting modes that were
learned. Note that the lighting condition remains consistent
as expression is varied and vice-versa.

VI. CONCLUSIONS

D. Learning from a balanced, synthetic dataset

The quality of generated images at large poses and con-
taining other extreme conditions is limited by the availability
of such images in training datasets. As a cleaner test of
IVI, we trained IVI-GAN on synthetic face images generated

We have shown that it is possible to adapt a conditional
GAN in order to gain continuous, disentangled control over
image attributes without the need for extensive labelling.
Only simple binary labels, indicating whether an attribute
is present in one form or another, are required. The GAN

Fig. 12. Results demonstrating drift in identity when varying a pitch-like
parameter in IVI-GAN without biometric identity constraint. Each row of
images was generated from the same z vector.

Fig. 13.
Results demonstrating pose-variation in images generated by
IVI-GAN trained on a synthetic dataset. The two rows show the effect of
varying the two, uniform pose parameters between −3.0 and 3.0. All other
parameters were kept the same.

TABLE II
IDENTITY-MATCHING STATISTICS FOR IMAGES GENERATED AT POSE VS.
FRONTAL (ρ = (0.0, 0.0)). MATCHINGS CONSIDERED TO BE FAILURES
(< 0.7) ARE INDICATED IN RED.

GAN conﬁguration
Type

ρ
(0.0, 1.0)
(0.0, 2.0)
(0.0, 3.0)
(0.0, 1.0)
(0.0, 2.0)
(0.0, 3.0)

IVI

IVI
+LID

Cosine
similarity
0.83
0.66
0.54
0.94
0.86
0.78

StdDev
cosine sim.
0.19
0.17
0.11
0.10
0.16
0.17

is then allowed to discover its own, multivariate way of
modelling the variation present within that attribute category
in an unsupervised fashion. To the best of our knowledge,
IVI-GAN is the ﬁrst network to achieve this separation in a
weakly supervised manner. In conjunction with an explicit
identity constraint and a further, physically motivated image-
structure constraint used when manipulating lighting and the
background, IVI has been shown to be highly effective via
evaluation on CelebA and on a synthetic dataset of 3DMM
images. Potential applications of IVI-GAN include dataset-
augmentation and image-manipulation.

REFERENCES

[1] M. Arjovsky, S. Chintala and L. Bottou. Wasserstein generative

adversarial networks. In ICML, 2017.

[2] J. Bao, D. Chen, F. Wen, H. Li and G. Hua. Towards open-set identity

preserving face synthesis. In CVPR, 2018.

[3] V. Blanz and T. Vetter. A morphable model for the synthesis of 3D

faces. In SIGGRAPH, 1999.

[4] B. Bozorgtabar, M. S. Rad, H. Kemal Ekenel and J. Thiran. Using
photorealistic face synthesis and domain adaptation to improve facial
expression analysis. In FG, 2019.

[5] A. Brock, T. Lim, J.M. Ritchie and N. Weston. Neural photo editing

with introspective adversarial networks. In ICLR, 2017.

[6] A. Brock, J. Donahue and K. Simonyan. Large scale gan training for

high ﬁdelity natural image synthesis. In ICLR, 2019.

[7] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever and P.
Abbeel. Infogan: Interpretable representation learning by information
maximizing generative adversarial nets. In NIPS, 2016.

[8] Y.S. Chen, Y.C. Wang, M.H. Kao and Y.Y. Chuang. Deep photo
enhancer: Unpaired learning for image enhancement from photographs
with gans. In CVPR, 2018.

[9] Y. Choi, M. Choi, M. Kim, J.W. Ha, S. Kim and J. Choo. Stargan:
Uniﬁed generative adversarial networks for multi-domain image-to-
image translation. In CVPR, 2018.

[10] V. Dumoulin, J. Shlens and M. Kudlur. A learned representation for

artistic style. In ICLR, 2017.

Results demonstrating expression and lighting variation in
Fig. 14.
images generated by IVI-GAN trained on a synthetic dataset. The left-hand
images show neutral expression (ρexp = 08, top) and ambient lighting
(ρlighting = 09, bottom). The other images show the effects of activating
individual, expression and lighting parameters with values of −3.0 or 3.0.

[11] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville and Y. Bengio. Generative adversarial nets. In
NIPS, 2014.

[12] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin and A.C. Courville.

Improved training of wasserstein gans. In NIPS, 2017.

[13] A. Hasnat, J. Bohn´e, J. Milgram, S. Gentric and L. Chen. Deepvisage:
Making face recognition simple yet with powerful generalization
skills. In ICCV, 2017.

[14] T. Karras, T. Aila, S. Laine and J. Lehtinen. Progressive growing of

GANs for improved quality, stability, and variation. In ICLR, 2018.

[15] Y.H. Lai and S.H. Lai. Emotion-preserving representation learning
via generative adversarial network for multi-view facial expression
recognition. In FG, 2018.

[16] A. Lindt, P. Barros, H. Siqueira and S. Wermter. Facial expression

editing with continuous emotion labels. In FG, 2019.

[17] Z. Liu, P. Luo, X. Wang and X. Tang. Deep learning face attributes

in the wild. In CVPR, 2015.

[18] M. Mirza and S. Osindero. Conditional generative adversarial nets.

arXiv preprint arXiv:1411.1784, 2014.

[19] T. Miyato, T. Kataoka, M. Koyama and Y. Yoshida. Spectral normal-

ization for generative adversarial networks. In ICLR, 2018.

[20] T. Miyato and M. Koyama. cGANs with projection discriminator. In

ICLR, 2018.

[21] A. Odena, C. Olah and J. Shlens. Conditional image synthesis with

auxiliary classiﬁer gans. In ICML, 2017.

[22] G. Perarnau, J. Van De Weijer, B. Raducanu and J.M.

´Alvarez.
Invertible conditional gans for image editing. In NIPS Workshops,
2016.

[23] A. Pumarola, A. Agudo, A.M. Martinez, A. Sanfeliu and F. Moreno-
Noguer. Ganimation: Anatomically-aware facial animation from a
single image. In ECCV, 2018.

[24] R. Ramamoorthi and P. Hanrahan. An efﬁcient representation for

irradiance environment maps. In SIGGRAPH, 2001.

[25] Y. Shen, J. Gu, X. Tang and B. Zhou. Interpreting the latent space
of gans for semantic face editing. arXiv preprint arXiv:1907.10786,
2019.

[26] L. Tran, X. Yin and X. Liu. Disentangled representation learning gan

for pose-invariant face recognition. In CVPR, 2017.

[27] J.Y. Zhu, T. Park, P. Isola and A.A. Efros. Unpaired image-to-image
translation using cycle-consistent adversarial networks. In ICCV,
2017.

