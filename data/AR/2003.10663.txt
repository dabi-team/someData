0
2
0
2

r
a

M
4
2

]

V
C
.
s
c
[

1
v
3
6
6
0
1
.
3
0
0
2
:
v
i
X
r
a

MODELING CROSS-VIEW INTERACTION CONSISTENCY
FOR PAIRED EGOCENTRIC INTERACTION RECOGNITION

Zhongguo Li∗, Fan Lyu∗, Wei Feng∗, Song Wang∗†

∗College of Intelligence and Computing, Tianjin University, China
†Department of Computer Science and Engineering, University of South Carolina, USA
{lizhongguo,fanlyu}@tju.edu.cn, wfeng@ieee.org, songwang@cec.sc.edu

ABSTRACT

A

A

B

With the development of Augmented Reality (AR), egocen-
tric action recognition (EAR) plays an important role in ac-
curately understanding demands from the user. However,
EAR is designed to help recognize human-machine interac-
tion in single egocentric view, thus difﬁcult to capture inter-
actions between two face-to-face AR users. Paired egocen-
tric interaction recognition (PEIR) is the task to collabora-
tively recognize the interactions between two persons with the
videos in their corresponding views. Unfortunately, existing
PEIR methods always directly use linear decision function to
fuse the features extracted from two corresponding egocentric
videos, which ignore the consistency of interaction in paired
egocentric videos. The consistency of interactions in paired
videos, and features extracted from them, are correlated to
each other. On top of that, we propose to derive the relevance
between two views using bilinear pooling, which captures the
consistency of two views in feature-level. Speciﬁcally, each
neuron in the feature maps from one view connects to the neu-
rons from the other view, which enforces the compact consis-
tency between two views and then all possible paired neurons
are used for PEIR. To be efﬁcient, we use compact bilinear
pooling with Count Sketch to avoid directly computing outer
product. Experimental results on the PEV dataset shows the
superiority of the proposed methods on the task PEIR.

Index Terms— Paired egocentric interaction recognition,

bilinear pooling, action recognition

1. INTRODUCTION

Due to the advance of Augmented Reality (AR) techniques,
wearable AR devices like Microsoft HoloLens allow users to
interact with the real world via gestures or voice commands.
Egocentric action recognition (EAR) [1, 2, 3, 4] is the task
to recognize the action or gesture of users to achieve intelli-
gent human-machine interaction. In this paper, we study the
further problem of Paired Egocentric Interaction Recognition

This work was supported, in part, by the National Natural Science Foun-
dation of China NSFC-U1803264, NSFC-61672376, and NSFC-61671325.

B

Fig. 1: An illustration of the consistency of interactions be-
tween a paired of egocentric views. Here the right hand of
person A, labeled by red boxes, are recorded in both the views
of A and B.

(PEIR) that recognizes the interactions between face-to-face
AR users [5, 6, 7]. Different from EAR that only considers
one egocentric video, PEIR needs to simultaneously consider
the paired face-to-face egocentric videos. Utilizing paired
egocentric videos recorded from face-to-face views can ob-
tain more precise recognitions than egocentric videos from
single views [8]. Enabling AR systems to understand the in-
teractions between persons can provide more precise assis-
tance in daily life. For examples, while one user point at one
object and the other one shares his/her attention, AR system
could read the interaction and response without explicit com-
mands.

Previous works make many efforts on EAR [1, 2, 3, 4].
Ryoo et al. [4] tries to capture both entire scene dynamics and
salient local motions observed in videos to predict interac-
tions. Li et al. [1] tries to model the relationship of the camera
wearer and the interactor. From EAR methods, a naive solu-
tion to PEIR is directly fusing the features from two views for
the sake of one head output. For instance, one can concate-
nate the features extracted from paired egocentric videos and
directly use the generated feature for linear classiﬁcation. Till
now, few works try to solve the PEIR problem. In [8], Yo-
netani et al. adopt a linear decision function to combine two
kinds of handcrafted feature for PEIR.

In our view, the naive feature fusion from two views ig-
nores the interaction consistency in two videos taken from
two persons’ views respectively. As shown in Fig. 1, we con-
sider two persons of A and B and their paired views are shown
as A and B respectively. On the one hand, the head’s tilting
of A could be recorded in the view of B, and also leads to the

978-1-7281-1331-9/20/$31.00 c(cid:13)2020 IEEE

 
 
 
 
 
 
shift of A’s viewpoints, because cameras are mounted over
their heads. On the other hand, there may be interactions oc-
curred in common areas recorded in both views of A and B.
In the both cases, there are explicit or implicit information
consistency of interactions between the views of A and B.

Due to the consistency of interactions in two views, neu-
rons of feature maps corresponding to two views should de-
scribe the same information. We propose to classify interac-
tions based on the consistent information represented by all
possible paired neurons. Speciﬁcally, we use bilinear pool-
ing, a second order polynomial kernel function [9], to capture
the compact consistent information of interaction in all pairs.
To avoid directly computing expensive outer product, we pro-
pose to use compact bilinear pooling to reduce the compu-
tation cost by using Count Sketch. We ﬁrst extract features
from paired egocentric videos, followed by obtaining sketch
mentioned above and transforming them into Fourier domain.
Finally we compute element-wise product of them and trans-
form the result into real domain for linear classiﬁcation. Ex-
perimental results on the PEV dataset shows that the proposed
method outperforms other methods using naive fusions.

2. RELATED WORKS

Paired egocentric interaction recognition (PEIR), ex-
tended from egocentric action recognition (EAR), aims at
recognizing the interaction between two face-to-face persons
from both of their views. Along with the successes of deep
learning in image-level tasks [10, 11], in recent years, EAR
makes a great progress by using deep neural networks. For
example Li et al. [1] models the relationship of the camera
wearer and the interactor. However, EAR assumes there ex-
ists only one person with wearable camera. PEIR was ﬁrst
proposed by Yonetani et al. [8], where interactions like subtle
motion of head or small hand actions people used in commu-
nications are recognized, by using their paired face-to-face
views. The video from each view is collected by the camera
over the head. To recognize interaction, Yonetain et al. [8]
combines two kinds of hand-crafted features – PoTCD [12]
for head and iDT [13] for body – for two views respectively
with linear decision models. However, it ignores the consis-
tency of interactions in two views and prefers to rely on only
one view for prediction. Different from naive fusion like con-
catenation, we propose to leverage bilinear pooling to model
the consistency between the paired egocentric videos.
Bilinear pooling models [14] was proposed for ﬁne-grained
image classiﬁcation. Bilinear methods have been used to fuse
two kind of features extracted by deep neural networks. Given
two features f 1, f 2 ∈ RC, bilinear methods compute the outer
product of them by

T
f 1 ⊗ f 2 = [f 1 × (f 2)

] ∈ RC 2

,

(1)

sorted by orders in new vector. Then the features generated
by bilinear methods are directly used for classiﬁcation. How-
ever when C is large, the generated features are of dimension
C2. High-dimensional features make the direct computing of
the outer product for linear classiﬁcation very expensive. Gao
et al. [9] reduces the dimension of generated feature from C2
into D, which is far less than C2. Following this idea, Fukui
et al. [15] fuse the visual features and the text features for
visual question answering and visual grounding.

3. METHOD

3.1. Problem formulation

Denote the person who starts the interaction as A, the one
who receives the interaction as B, as shown in Fig. 1 and their
recorded videos (taken by the wearable cameras mounted
over their heads) as vA and vB, respectively. PEIR tries to
learn to recognize the interaction l∗ from such paired egocen-
tric videos based on the hypothesis l∗ = h(vA, vB).

The naive method to tackle PEIR is directly fusing the
information from two views, such as [8], and this method can
be formulated as

f A = CNN(vA),
f B = CNN(vB),
g = Φ(f A, f B),
p(l|vA, vB) = σ(W⊤g + b),

(2)

(3)

(4)

(5)

where f A, f B ∈ RC are the ﬂattened feature maps extracted
by Convolutional Neural Network (CNN) and g ∈ RD is the
fused feature of f A and f B, σ is the softmax activation func-
tion. The key step is the design of the fusing function Φ to
effectively combine the information from two views. Com-
mon choices of the fusing function Φ include
1) concatenation: Φ(x, y) = [x, y],
2) element-wise summation: Φ(x, y) = x + y, and
3) element-wise product: Φ(x, y) = x ⊙ y.
These three common fusing methods are easy to imple-
ment but do not consider the possible interaction consistency
between two views.
In this following, we propose a new
method to address this problem.

3.2. Main Method

The consistency of interactions exist in paired egocentric
videos as the common visual information of co-viewed ob-
jects, i.e., f A and f B are correlated. We ﬁrst enumerate all
pairs between f A and f B by

gk = φ(f A

i , f B

j ),

(6)

where ⊗ means outer product of tensors. [ · ] operator vector-
izes a matrix into a vector which means elements in matrix is

where g ∈ RC 2
represents the consistent information in the
form of pairs and k = i · C + j represents the k-th pair be-

2

224x224x3

View A

View B

224x224x3

CNN model

Cx1

Dx1

Dx1

Transpose

e

1xC

´

Transpose

DFFT

Cx1
h1
h2
h3
h4

Cx1

e
l
p
m
a
s
y
l
l
a
s
r
e
v
i
n
u

}
1
,
1
-
{

m
o
r
f

e
l
p
m
a
s
y
l
l
a
s
r
e
v
i
n
u

}
D

.
.
.
3
,
2
,
1
{

m
o
r
f

11,h1

0
0 0 0 0 0
0 0 0
0
44,h1

0 0 0 0 0 0
0 0
22,h1
0 0 0 0
0 0 0 0 0 0

33,h1

...

hC

0 0 0 0

cc,h1

0 0 0

Dx1

Dx1

FC Layer

CxD

e

-1

DFFT

Target

e

Transpose

1xC

´

Transpose

DFFT

CNN model

Cx1

Dx1

Dx1

Calculate sketches for paired videos’ features

Obtain sketch of features’ outer product

Fig. 2: Framework of the proposed method. Given paired egocentric videos from views A and B, we ﬁrst extract features from
both videos, then calculate the sketches of them, ﬁnally we obtain the sketch of outer product and transform it into Fourier
domain for classiﬁcation. DFFT is the discrete fast Fourier transform.

tween f A and f B. fi is the i-th element in f and φ is element-
wise form of Φ. By Eq. (6), we construct element-wise in-
formation correlation between view A and view B, which
yields ﬁne-grained interaction consistency representation g.
In Eq. (6), if the product operator is adopted for φ, g is ac-
tually the outer product of f A and f B, and Φ becomes the bi-
linear method as described in Eq. (1). The procedure of the
compact bilinear method is shown in Alg. 1 in the appendix.
To reduce the computation cost, we use the compact bilinear
method [16, 9, 15] to shrink the dimension of g from C2 to D,
where D ≪ C2, and the full framework is shown in Fig. 2.
Gao et al. [9] proves that bilinear method is actually a sec-
ond order polynomial kernel function and uses Count Sketch
(called sketch in the remainder of this paper) [9, 17] to reduce
the computation cost.

Detailedly, we ﬁrst calculate the sketch of feature f A and
f B. We deﬁne h, s as following: each element in h ∈ NC
+
is universal randomly sampled from {1, 2, 3, · · · , D} and ev-
ery element in s ∈ ZC is universal randomly sampled from
{−1, 1}. We denote H as a C ×D matrix generated according
to h: each element Hi,j is deﬁned as

After that, we transform sketches into Fourier domain using
discrete fast Fourier transform (DFFT) and obtain element-
wise product result of two sketches in Fourier domain. It has
been proved [16] that the sketch of outer product f A ⊗ f B in
Fourier domain is just the element-wise product of f A’s sketch
sketchA and f B’s sketch sketchB both in Fourier domain,
and the computation of outer product could be replaced by
computing element-wise product in Fourier domain. Finally
we obtain g in the original domain by transforming the result
in Fourier domain back using inverse DFFT (DFFT−1), i.e.,
g = DFFT−1(DFFT(sketchA) ⊙ DFFT(sketchB)). (10)

3.3. Discussion

Although the concatenation method and element-wise opera-
tions can also be written in the form of Eq. (2), they cannot
describe the consistency of interaction in two views. The rea-
sons are discussed as follows.
Concatenation. For concatenation methods, the fusing func-
tion Φ in Eq. (4) is to concatenate f A and f b, and the logits of
the PEIR prediction can be written as

Hi,j =

if j = hi,
1,
0, otherwise.

(cid:26)

(7)

p(l|vA, vB) ∝

For the feature f , we ﬁrst compute element-wise product of
f and s, similar to traditional Random Maclaurin [18] for ap-
proximating the polynomial kernel. Then we compute matrix
product of the above element-wise product and matrix H to
get sketch of feature f , which projects f from RC into RD to
provide bounds on the variance of estimates to guarantee the
reliability [16]:

sketchA = (sA ⊙ f A)HA,
sketchB = (sB ⊙ f B)HB.

(8)

(9)

3

2C

Yi=1
C

exp

i gi

W⊤
(cid:0)

(cid:1)

,

2C

(11)

=

exp

Yi=1

i f A
i

W⊤
(cid:0)

Yi=C+1

(cid:1)

exp

i f B
j

W⊤
(cid:0)

(cid:1)

. (12)

Since the ﬁrst C elements of g ∈ R2C generated by the con-
catenation method are all from feature f A and the rest are
all from f B, we could rewrite the predicted probability of the
model deﬁned in Eq. (5) as Eq. (12). Obviously, in Eq. (12),
the feature f A is independent of f B for ﬁnal prediction. Thus
concatenation method does not consider the correlation be-
tween f A and f B.

 
 
 
 
0.8

0.6

0.4

0.2

0.0

Element-wise product. Element-wise product is similar to
Eq. (6), and the predicted probability could be written as

C

p(l|vA, vB) ∝

exp

Yi=1

i f A

i f B
i

W⊤
(cid:0)

,

(cid:1)

(13)

where only feature elements with the same index is correlated.
Element-wise summation. If we adopt element-wise sum-
mation as φ, we could see

Table 2: Mean accuracy of TwoStream nets.

Modality View

Method

PoTCD+IDT A+B Yonetani et al.[8]
TwoStream A+B
TwoStream A+B
TwoStream A+B
TwoStream A+B
TwoStream A+B
TwoStream A+B

Svm
Concat
Ours
Product
Sum
Ours

D Avg Acc
-
-
2048
2048
1024
1024
1024

69.2
87.27
87.72
88.91
85.17
84.41
88.45

C

p(l|vA, vB) ∝

exp

Yi=1
C

=

exp

Yi=1

W⊤
(cid:0)

i (f A

i + f B
i )
(cid:1)

i f A
i

W⊤
(cid:0)

C

Yi=1

(cid:1)

exp

i f B
i

W⊤
(cid:0)

(cid:1)

.

(14)

Pointing Attention Passing Receiving Positive Negative Gesture

Pointing

0.84

0.02

0.02

0.00

0.00

0.02

0.06

Similar to the concatenation-based fusion, in this case fea-
tures f A and f B are independent of each other for ﬁnal predic-
tion.
Ours. We adopt bilinear pooling operation as Φ, which means
the product operator is selected as φ in Eq. ((6)), i.e.,

n
o
i
t
c
i
d
e
r
P

Attention

0.01

0.94

0.00

0.01

0.02

0.05

0.00

Passing

0.01

0.00

0.94

0.06

0.00

0.00

0.02

Receiving

0.00

0.01

0.02

0.92

0.00

0.00

0.00

Positive

0.01

0.02

0.00

0.00

0.90

0.03

0.08

p(l|vA, vB) ∝

exp

C

Yi=1,j=1

i f A

i f B
j

W⊤
(cid:0)

,

(cid:1)

where the logits of PEIR are predicted based on the paired
elements between f A and f B. Each element in f A is correlated
with all the elements in f B for ﬁnal prediction.

Table 1: Accuracy in validation set with RGB or OpticalFlow.

Avg
Svm
Concat
Ours

Modality View Method
A
B
A+B
A+B
A+B
A+B
A
B
A+B
A+B
A+B
A+B
A+B
A+B Product
A+B
A+B
A+B Product
A+B

RGB
RGB
RGB
RGB
RGB
RGB
OF
OF
OF
OF
OF
OF
RGB
RGB
RGB
OF
OF
OF

Avg
Svm
Concat
Ours
Sum

Ours
Sum

Ours

D
-
-
-
-
2048
2048
-
-
-
-
2048
2048
1024
1024
1024
1024
1024
1024

Split1
78.50
73.52
80.50
84.96
82.25
84.35
84.96
79.74
86.01
89.40
87.37
88.10
82.99
84.35
86.08
87.24
87.30
89.33

Split2
77.94
68.73
80.55
78.70
80.89
83.09
81.35
78.00
82.62
83.42
87.56
89.06
79.14
79.10
81.34
87.26
88.44
88.81

Split3 Avg Acc
76.19
71.75
76.80
80.40
79.30
83.47
84.24
78.46
85.86
87.97
88.21
89.12
79.09
72.81
83.78
87.64
88.34
88.37

77.54
71.33
79.28
81.35
80.81
83.63
83.51
78.73
84.83
86.93
87.71
88.76
80.41
78.75
83.73
87.78
88.03
88.84

4. EXPERIMENTS

4.1. Experiments details

Dataset and evaluation metric. PEV dataset is proposed
in [8], which contains 1,226 pairs of videos in 7 cate-
Each paired video is collected by the cameras
gories.

4

(15)

Negative

0.00

0.01

0.00

0.00

0.00

0.85

0.01

Gesture

0.14

0.00

0.03

0.01

0.08

0.05

0.83

GroundTruth

Fig. 3: Confusion matrix of our method, averaged over the
three validation sets.

mounted to the heads of two persons standing face to face.
The 7 categories of interactions present in PEV dataset[8]
are: P ointing, Attention, P ositive, N egative, P assing,
Receiving, Gesture. We adopt the three-fold cross valida-
tion for evaluation. Results in all three validation splits are
reported in Table 1.

Model settings. We adopt I3D [19] as the backbone, which
is pretrained on Kinetic-400. C, the dimension of f A and f B,
is set to 1,024 as used in backbone. For fair comparison ,
D, the dimension of g, is set to 2,048 as in the concatenation
method, or 1,024 as in the element-wise product or summa-
tion methods. Cross entropy loss is chosen as the loss func-
tion. Standard SGD is used to train neural networks in this
paper, and the learning rate is set to 0.1.

f A and f B shall be normalized to be in
Normalization.
the form of one-hot encoding, since they are batch normal-
ized [20] in the backbone and fed into ReLU [21] layer. We
only scale them by a constant factor.

Data Processing. We sample one frame for every three
frames. Sampled frames are randomly cropped and scaled
to 224 × 224. Horizontal ﬂipping is then used for data aug-
mentation. The Length of clip is set to 32, and the last frame
will be repeated if clip’s length is less than 32.

1.0

0.9

0.8

0.7

0.6

0.5

RGB

Pointing Attention Passing Receiving Positive Negative Gesture

A+B Avg
A+B Svm
A+B Cat
A+B Sum
A+B Product
A+B Ours

1.0

0.9

0.8

0.7

0.6

0.5

OF

Pointing Attention Passing Receiving Positive Negative Gesture

Fig. 4: Accuracy for each category while input is RGB (left) or OpticalFlow (right). ‘A+B’ means both views are used.

Pointing

Attention

Passing

Receiving

Positive

Negative

Gesture

A

B

Fig. 5: Top 10 percent integrated gradients of our model in validation sets. The yellow dots show the model’s focuses.

4.2. Results

We explore different fusion methods under RGB or Opti-
calFlow modality. We choose feature-fusion methods includ-
ing concatenation (Concat), element-wise product (Product)
and element summation (Sum) as baselines. We also report
results of the methods using average (Avg) or SVM for late-
fusion, i.e., training classiﬁer for each view ﬁrst and then fus-
ing scores as the ﬁnal result. Results are shown in Table 1.

Comparisons. All the methods using two views together
outperform the models that only use one view. We can
see that, SVM is always a better choice for fusing deci-
sion scores. Element-wise product achieves a score higher
than element-summation in OpticalFlow modality, but lower
in RGB modality. While under RGB modality, the end-
to-end concatenation method does not outperform the score
fusion method, e.g., SVM, the result is opposite when the
input becomes OpticalFlow. Under OpticalFlow modality,
element-wise product is surprisingly better than concatena-
tion or element-wise summation, with half parameters in the
last fully connected layer compared with the concatenation-

based method. Our method performs best in both RGB
and OpticalFlow modalities – it outperforms the second best
method by 2.28% in RGB modality, and by 1.05% in Opti-
calFlow modality, with D set as 2,048. When D is set to
1,024, our proposed method outperforms the second best by
3.32% in RGB modality, and by only 0.81% (over element-
wise product) in OpticalFlow modality. A possible expla-
nation is that only motion information is recorded in Opti-
calFlow modality and the consistency involving appearance
cannot be built in OpticalFlow modality. When following
the two-stream net [22] of using both RGB and OpticalFlow
modalities as input, our method still achieves highest score
than other fusing methods, as shown in Table. 2.

Confusion matrix and category-grained accuracy. Confu-
sion matrix of our method is shown in Fig. 3 and Fig. 4. The
proposed method performs best or pretty close to the best.
Other methods is unstable compared with ours. We believe
this is because our proposed method makes ﬁnal prediction
only if evidence is found in both paired egocentric videos.
For RGB modality, N egative action is the most difﬁcult to
be recognized, where the score-fusion SVM method performs

5

the best, followed by our method as the second. For Opti-
calFlow modality, the most difﬁcult action is Gesture and
the proposed method achieves the highest score.

4.3. Visualization

We use Integrated Gradients [23] to visualize the model, as
shown in Fig. 5. The yellow dots in frames represent the fo-
cus of the model. In P ointing category, person A’s hand is
focused, and other dots in the edges of frame represent the
model focus on these pixels to catch shift of attentions, and
similar results can be seen in Attention category. The vi-
sualization for P assing and Receiving actions shows that
the model successfully focuses on the hand and object. For
P ositive and N egative actions, the focus is changed into A
and B’s upper body. The interesting result occurs for Gesture
action, the model focuses on B’s head, possibly because B re-
sponds to A with a subtle shaking of head.

5. CONCLUSION

In this paper, we proposed to build the relevance between
paired egocentric videos for interaction recognition. We ob-
served that the consistency of interactions exists in paired
videos and features extracted from them are correlated to each
other. We proposed to use bilinear pooling to capture all pos-
sible consistent information represented in pairs of elements
between the features from two views. Moreover we used the
compact bilinear pooling with Count Sketch to reduce the al-
gorithm computation complexity. Experiments showed that
our method achieves the state-of-the-art performance on the
PEV dataset.

6. REFERENCES

[1] Haoxin Li, Yijun Cai, and Wei-Shi Zheng,

“Deep
dual relation modeling for egocentric interaction recog-
nition,” in CVPR, 2019.

[2] Vasileios Choutas, Philippe Weinzaepfel, Jerome Re-
vaud, and Cordelia Schmid, “Potion: Pose motion rep-
resentation for action recognition,” in CVPR, 2018.
[3] Swathikiran Sudhakaran, Sergio Escalera, and Oswald
Lanz, “LSTA: Long short-term attention for egocentric
action recognition,” in CVPR, 2019.

[4] Michael Ryoo, Brandon Rothrock, and Larry Matthies,
in
“Pooled motion features for ﬁrst-person videos,”
CVPR, 2015.

[5] Mohammad Syahputra, Siti Fatimah, and Romi Rahmat,
“Interaction on Augmented Reality with Finger Detec-
tion and Hand Movement Recognition,” in AVR, 2018.
[6] Ahmad Karambakhsh, Aouaidjia Kamel, Bin Sheng,
Ping Li, Po Yang, and David Feng, “Deep gesture inter-
action for augmented anatomy learning,” International

6

Journal of Information Management, vol. 45, pp. 328–
336, APR 2019.

[7] Xueni Pan and Antonia Hamilton, “Why and how to
use virtual reality to study human social interaction:
The challenges of exploring a new research landscape,”
British Journal of Psychology, vol. 109, no. 3, pp. 395–
417, AUG 2018.

[8] Ryo Yonetani, Kris M Kitani, and Yoichi Sato, “Recog-
nizing micro-actions and reactions from paired egocen-
tric videos,” in CVPR, 2016.

[9] Yang Gao, Oscar Beijbom, Ning Zhang, and Trevor Dar-
rell, “Compact bilinear pooling,” in CVPR, 2016.
[10] Chaorui Deng, Qi Wu, Qingyao Wu, Fuyuan Hu, Fan
Lyu, and Mingkui Tan, “Visual Grounding via Accu-
mulated Attention,” in CVPR, 2018.

[11] Fan Lyu, Qi Wu, Fuyuan Hu, Qingyao Wu, and Mingkui
Tan, “Attend and Imagine: Multi-Label Image Classiﬁ-
cation With Visual Attention and Recurrent Neural Net-
works,” IEEE Transactions on Multimedia, vol. 21, no.
8, pp. 1971–1981, AUG 2019.

[12] Yair Poleg, Chetan Arora, and Shmuel Peleg, “Temporal
segmentation of egocentric videos,” in CVPR, 2014.
[13] Heng Wang and Cordelia Schmid, “Action recognition

with improved trajectories,” in ICCV, 2013.

[14] Tsung-Yu Lin, Aruni RoyChowdhury, and Subhransu
“Bilinear cnn models for ﬁne-grained visual

Maji,
recognition,” in CVPR, 2015.

[15] Akira Fukui, Dong Huk Park, Daylen Yang, Anna
Rohrbach, Trevor Darrell, and Marcus Rohrbach, “Mul-
timodal compact bilinear pooling for visual question an-
swering and visual grounding,” in EMNLP, 2016.
[16] Ninh Pham and Rasmus Pagh, “Fast and scalable poly-
nomial kernels via explicit feature maps,” in SIGKDD,
2013.

[17] Rasmus Pagh, “Compressed matrix multiplication,” in

ITCS, 2012.

[18] Purushottam Kar and Harish Karnick, “Random feature
maps for dot product kernels,” in AISTATS, 2012.
[19] Joao Carreira and Andrew Zisserman, “Quo vadis, ac-
tion recognition? a new model and the kinetics dataset,”
in CVPR, 2017.

[20] Sergey Ioffe and Christian Szegedy, “Batch normaliza-
tion: Accelerating deep network training by reducing
internal covariate shift,” in ICML, 2015.

[21] Xavier Glorot, Antoine Bordes, and Yoshua Bengio,

“Deep sparse rectiﬁer neural networks,” in AIS, 2011.

[22] Karen Simonyan and Andrew Zisserman,

“Two-
stream convolutional networks for action recognition in
videos,” in NIPS, 2014.

[23] Mukund Sundararajan, Ankur Taly, and Qiqi Yan, “Ax-
iomatic attribution for deep networks,” in ICML, 2017.

