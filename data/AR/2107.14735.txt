1
2
0
2
c
e
D
9

]

V
C
.
s
c
[

4
v
5
3
7
4
1
.
7
0
1
2
:
v
i
X
r
a

Neural Relighting and Expression Transfer on Video Portraits

Youjia Wang*
wangyj2@shanghaitech.edu.cn

Taotao Zhou*
zhoutt@shanghaitech.edu.cn

Minzhang Li
limzh@shanghaitech.edu.cn

Teng Xu
xuteng@shanghaitech.edu.cn

Lan Xu
xulan1@shanghaitech.edu.cn

Jingyi Yu
jingyiyu@shanghaitech.edu.cn

ShanghaiTech University

Abstract

Photo-realistic video portrait reenactment beneﬁts vir-
tual production and numerous VR/AR experiences. The task
remains challenging as the reenacted expression should
match the source while the lighting should be adjustable to
new environments. We present a neural relighting and ex-
pression transfer technique to transfer the head pose and
facial expressions from a source performer to a portrait
video of a target performer while enabling dynamic relight-
ing. Our approach employs 4D reﬂectance ﬁeld learning,
model-based facial performance capture and target-aware
neural rendering. Speciﬁcally, given a short sequence of
the target performer’s OLAT, we apply a rendering-to-video
translation network to ﬁrst synthesize the OLAT result of
new sequences with unseen expressions. We then design a
semantic-aware facial normalization scheme along with a
multi-frame multi-task learning strategy to encode the con-
tent, segmentation, and motion ﬂows for reliably inferring
the reﬂectance ﬁeld. This allows us to simultaneously con-
trol facial expression and apply virtual relighting. Extensive
experiments demonstrate that our technique can robustly
handle challenging expressions and lighting environments
and produce results at a cinematographic quality.

1. Introduction

The popularity of mobile cameras has witnessed the
rapid development of digital facial portrait photography.
Further synthesizing and editing the video portraits sug-
gests different content, which enables numerous applica-
tions in virtual cinematography, movie post-production, vi-
sual effects and telepresence, among others. Generating
such video portraits conveniently still remains challenging
and has attracted substantive attention of both the computer
vision and graphics communities.

In this paper, we focus on automatically and conve-
niently synthesizing a photo-realistic video portrait of a tar-

Figure 1. Top: our lighting dome for acquiring the short OLAT
sequence of the target performer. Bottom: a different performer
can transfer his expressions, unseen from the target sequence, with
new lighting conditions at a cinematographic quality.

get actor where the facial expression, the lighting condi-
tion and background are fully disentangled and controllable
through various source actors. It is particularly challeng-
ing since humans are highly sensitive to facial inconsistency
and the video portraits are inﬂuenced by the complex in-
teraction of environment, lighting patterns and various fa-
cial attributes like material and specular reﬂection. The
most common practice for manipulating video portraits in
the ﬁlm industry still relies on tedious manual labor and so-
phisticated devices for reﬂectance ﬁeld acquisition [11] or
chroma-keying [70]. The recent neural techniques [31, 56]
bring huge potential for convenient and highly-realistic
manipulations, enabling compelling visual quality in vari-
ous applications, such as visual dubbing [24, 30], telepres-
ence [36], video post-editing [16, 73] or audio-driven vir-
tual assistant [60], even in real-time [63,65]. However, they
still suffer from pre-baked shading of the speciﬁc training
scenes without manipulating the illumination, which is crit-
ical for relighting applications in portrait photography like
virtual cinematography. On the other hand, various neu-
ral schemes with controllable portrait’s lighting conditions
have been proposed using high-quality pore-level 4D facial

 
 
 
 
 
 
reﬂectance ﬁelds [11] acquired under multi-view and multi-
lit conﬁgurations. However, they are limited to image-based
relighting [40, 48], a subset of the reﬂectance ﬁeld learn-
ing [37,49] or relightable model and appearance reconstruc-
tion [4, 21, 38]. Researchers pay less attention to combine
the background and lighting manipulation with face-interior
reenactment into a universal data-driven framework.

In this paper, we attack the above challenges and present
the ﬁrst approach to generating relightable neural video por-
traits for a speciﬁc performer. As illustrated in Fig. 1, with
the aid of immediate and high-quality facial reﬂectance ﬁeld
inference, our approach endows the entire photo-realistic
video portraits synthesis with the ability of fully disentan-
glement and control of various head poses, facial expres-
sions, backgrounds, and lighting conditions. This unique
capability also enables numerous photo-realistic visual edit-
ing effects.

Our key idea is to combine high-quality 4D reﬂectance
ﬁeld learning, model-based facial performance capture and
target-aware neural rendering into a consistent framework,
so as to support convenient, realistic and controllable edit-
ing of neural video portraits. To this end, for the tempo-
ral ground-truth supervision, we ﬁrst build up a light stage
setup to collect the dynamic facial reﬂectance ﬁelds of the
target performer with one-light-at-a-time (OLAT) images
under various sequential poses and expressions. Then, with
such target-speciﬁc OLAT data, the core of our approach
is a novel rendering-to-video translation network design
with the explicit head pose, facial expression, lighting and
background disentanglement for highly controllable neural
video portrait synthesis. After training, reliable head pose
and facial expression editing are obtained by applying the
same facial capture and normalization scheme to the source
video input, while our explicit OLAT output enables high-
quality relit effect. This relit effect can not only match the
portrait with the environment, but also simulate the rim-
light to improve the matting quality for a better background
replacement. An encoder-decoder architecture is further
adopted to encode the illumination weighting parameters
of OLAT images from the corresponding relit images, en-
abling automatic lighting editing from natural source im-
ages. Thus, our approach enables automatically and con-
veniently control of facial expression, lighting condition
and background of the performer’s video portrait with high
photo-realism for various visual editing effects. To summa-
rize, our main contributions include:

• We demonstrate the new capability of simultaneous
relighting and and expression transfer, which enables
photo-realistic video portraits synthesis for various vi-
sual editing effects, with full disentanglement of ex-
pressions, backgrounds and lighting conditions.

• We introduce a rendering-to-video translation network
to transfer model-based input into high-ﬁdelity facial

reﬂectance ﬁelds, with a multi-frame multi-task learn-
ing strategy to encode content, segmentation and tem-
poral information.

• We propose to utilize hybrid model-based facial cap-
ture with a carefully designed semantic-aware facial
normalization scheme for reliable disentanglement.

2. Related Work

Face capture and reconstruction. Face reconstruc-
tion methods aim to reconstruct the geometry and appear-
ance of 3D face models from visual data and we refer to
two recent reports on monocular 3D face reconstruction
and applications [14, 77] for a comprehensive overview.
Early solutions adopt optimization schemes by ﬁtting a
3D template model of only the face regions into vari-
ous visual input, such as a single image [5, 6], a tempo-
ral consistent video [8, 17, 20, 22, 50, 71] or even unstruc-
tured image sets [28, 29, 43]. The recent deep learning
techniques bring huge potential for face modeling, which
learn to predict the 3D face shape, geometry or appear-
ance [7, 26, 41, 42, 52, 58, 59, 72, 76] However, these method
above still cannot create a photo-realistic model in a con-
trollable manner, especially for those ﬁne-grained facial re-
gions like hair, mouth interior or eye gaze. The recent
work [18] enables controllable implicit facial modeling and
rendering using neural radiance ﬁeld, but it still relies on
per-scene training and suffers from pre-baked illumination
of the training scenes.
In contrast, our approach enables
more disentanglement and control of background, lighting
condition as well as facial capture parameters for photo-
realistic video portrait generation.

Face reenactment and replacement. Facial reenact-
ment re-generates the face content of a target actor in a
portrait video by transferring facial expression and pose
from a source actor. Many recent reenactment approaches
model-based expression capturing and the expressions are
transferred via dense motion ﬁelds [3, 34, 51] or facial
parameters [19, 32, 62–64, 66]. The recent neural ap-
proaches [31,56] replace components of the standard graph-
ics pipeline by learned components, which bring huge po-
tential for convenient and highly-realistic manipulations,
enabling compelling visual quality in various applications,
such as visual dubbing [24, 30], telepresence [36], video
post-editing [16, 73] or audio-driven virtual assistant [60],
even in real-time [63, 65]. Inspired by pix2pix [23], Deep
Video Portraits [31] proposes an rendering-to-image trans-
lation approach that converts synthetic rendering input to a
scene-speciﬁc realistic image output. However,
[31] and
the work followed by, such as
[24, 30, 61] suffers from
pre-baked shading of the speciﬁc training scenes without
manipulating the illumination which is critical for relight-
ing applications like virtual cinematography. Some recent

Figure 2. The algorithm pipeline of our relightable neural video portrait. Our pipeline mainly consists of three stages: Portrait parsing
disentangles portrait parameters explicitly and normalizes them into the target actor’s distribution for conditioning feature map genera-
tion; Then the proposed network synthesizes 4D reﬂectance ﬁelds via sequential conditioning feature maps; Our approach can achieve
simultaneous relighting and reenactment effects on the portrait video of the target actor.

work enables lighting or facial expression control during the
facial portrait image generation [9, 55] or editing [53, 54]
based on styleGAN [25]. However, these methods mainly
focus on the task of portrait image generation and suffers
from unrealistic interpolation for neural video portrait reen-
actment. Differently, we take advantage of learning from
the temporally consistent OLAT dataset of the target per-
former to allow for larger changes in facial expression reen-
actment, background as well as the lighting condition, while
maintaining high photo-realism. Facial portrait relight-
ing. High quality facial portrait relighting requires the
modeling of pore-level 4D reﬂectance ﬁelds. Debevec et
al. [11] invent Light Stage to capture the reﬂectance ﬁeld
of human faces, which has enabled high-quality 3D face
reconstruction and illuminations rendering, advancing the
ﬁlm’s special effects industry. Some subsequent work has
also achieved excellent results by introducing deep learn-
ing [21, 37, 38, 48, 74]. Some work follows the pipeline of
color transfer to achieve the relighting effects [10, 45–47],
which usually needs another portrait image as the facial
color distribution reference. With the advent of deep neural
networks and neural rendering, some methods [35, 44, 75]
adopt Spherical Harmonics (SH) lighting model to manip-
ulate the illumination. Several works [2, 13, 67] jointly es-
timate the 3D face and SH [5, 34] parameters and achieved
relighting by recovering the facial geometry and modify the
parameters of the SH lighting model. Explicitly modeling
the shadow and specular [39,69] achieve excellent results in
directional light source relighting. Mallikarjunr et al. [57]
take a single image portrait as input to predict OLAT(one-
light-at-a-time) as Reﬂectance Fields, which can be relit to
other lighting via image-based rendering. Sun et al. [48]
choose environment map as lighting model and use light
stage captured OLAT data to generate realistic training data
and train relighting networks in an end-to-end fashion. Sim-
ilarly, we use the target-aware temporal OLAT images to

Figure 3. The illustration of capture timing and frame alignment.
our method computes optical ﬂows between full-lit images and
warps OLAT images accordingly. The ”overlapping” strategy al-
low us to reuse a same OLAT image in different OLAT imageset
so that we can achieve higher capture frame rate.

generate training data for high-quality lighting disentangle-
ment. Differently, we further combine the background and
lighting manipulation with face-interior reenactment into a
universal data-driven framework.

3. Neural Light and Expression Transfer on

Video Portraits

We present relightable neural video portrait, a simulta-
neous relighting and reenactment scheme that transfers fa-
cial expressions from a source actor to the portrait video of
a target actor with arbitrary new backgrounds and lighting
conditions. Fig. 2 illustrates our technical pipeline, which
combines 4D reﬂectance ﬁeld learning, model-based facial
performance capture and target-aware neural rendering. We
ﬁrst describe our dynamic OLAT dataset capture scheme in
Sec. 3.1, and further introduce a portrait parsing scheme in
Sec. 3.2 to generate hybrid conditioning feature maps for
fully controllable portrait video synthesis. Speciﬁcally, our
approach parses the input video of the source actor for the

..................Capture Time Lineoptical flowFull-lit imageOLAT imageAligned imageA OLAT imageset(96 OLAT frames)6 aligned OLAT images 6  OLAT imagesrendering-to-video network, where nt is the number of the
frames.

A conditioning feature maps Ft consists of a diffuse
color image Id
t and a coordinate image Ic
t . We use
FLAME [33], a parametric 3D head model described by a
function G(β, θ, φ) as our representation of portrait, where
the coefﬁcient parameters β, θ, and φ represent head shape,
head pose and facial expression, respectively. Inspired by
DVP [31], we use diffuse color and pncc as textures for ren-
dering, and use them as the coordinate images Id
t and Ic
t
respectively.

These images provide a coarse estimation of the detailed
facial geometry and spatial information for result synthesis.
Face parameters β, θ, and φ are extracted from this im-
age patch via the DECA method [15], while the 2D land-
marks are estimated using a regression trees approach [26].
Note that when the source and target actors are differ-
ent, their facial geometry will be different even in similar
facial expressions, which will manifest in facial character-
istics, such as eye size, nose length, mouth curvature, etc.
Therefore, we performed normalization operations during
the expression transfer process. For details, please refer to
the supplementary material.

3.3. Reﬂectance Field Generation

Our conditioning rendering-to-video translation network
takes the composed sequence of conditioning feature maps
as input. Here, the generated reﬂectance ﬁeld consists of
96 portrait images in the same head pose and facial expres-
sion from an identical camera view but are lit one light at
a time. Such design in our approach enables more con-
trollable high-quality relighting than directly predicting the
portrait image under speciﬁc environment lighting condi-
tions. Our explicit relighting strategy based on OLAT im-
agesets owns better generalization ability than those end-
to-end learning ones which are highly relied on the diver-
sity of training data. Meanwhile, our target-aware neural
rendering strategy can render not only photo-realistic facial
output even for those regions complying with head motion
such as hair. As illustrated in Fig. 2, our translation network
adopts a multi-frame multi-task learning framework, which
will be described in detail below.

Sequential input. We gather the adjacent 11 frames to
be a sequential input Ftc = {Ft}tc
t=tc−10 for the network
inference, where tc is the current time. This sequential input
enables the network to extract the rich temporal information
in our dynamic OLAT dataset and output stabilized image
sequence.

Translation network. The encoder extracts multi-scale
latent representations of conditioning feature maps, while
the decoder module generate the reﬂectance ﬁeld ,as shown
in Fig. 2. Many works have shown that learning geomet-
ric information while learning reﬂectance helps the neural

Figure 4. The demonstration of our capturing system and sam-
ples of captured data. Left: cameras and lights are arranged on a
spherical structure; (a)-(e): samples of OLAT images; (f) a full-lit
frame image. Subﬁgures on the bottom-left of (a)-(f) illustrates
the corresponding lighting conditions on our system.

explicit rigid head pose, and facial expression, to support
explicit editing and disentanglement. In sec. 3.3, we intro-
duce our rendering-to-video translation network to synthe-
size high-quality OLAT imagesets from sequential condi-
tioning feature maps. We adopt a multi-frame multi-task
learning strategy to encode content, segmentation and tem-
poral information simultaneously for reﬂectance ﬁeld infer-
ence. Finally, various novel visual editing applications are
as described in Sec. 3.4.

3.1. Data Acquisition

To recover the 4D reﬂectance ﬁelds of dynamic portrait,
we build up a light stage, so as to provide ﬁne-grained fa-
cial perception. Our hardware architecture is demonstrated
in Fig. 12, which is a spherical dome of a radius of 1.3 me-
ters with 96 fully programmable LEDs and a 4K ultra-high-
speed PCC (Phantom Capture Camera) as shown in Fig. 12.
To densely acquire dynamic sets of facial expressions,
captured targets were required to perform natural conversa-
tions and a range-of-motion sequence with translation and
rotation.

However, one of the most challenging issue is that the
motion of the captured target will cause mis-alignments,
leading to blurriness. We conquer such limitations by opti-
cal ﬂow algorithm and further obtain results at higher frame
rate.

Inspired by the approach [38], we additionally capture an
full-bright image for tracking purposes every 6 images, as
shown in Fig. 14. Then, we align the OLAT data between 14
consecutive groups of full-bright frames with optical ﬂow.
We will discuss the capture setting in supplementary mate-
rials for details.

3.2. Portrait Parsing

Our Portrait parsing module aims to generate a set
t=1 as inputs to our

of conditioning feature map {Ft}nt

Figure 5. Relighting and expression transfer. The ﬁrst row is the reference for expression and the reference for lighting. Each following
row shows the result of a target actor under the reference expression and lighting. For columns 1 and 4, we use an ambient light to relit, to
show our expression transfer result. For each expression, we present two additional lighting condition. In columns 2 and 3, we use HDR
environment illumination to relit the target actor, and use the corresponding region in the environment map as the background. In columns
5 and 6, we show our neural reﬂectance ﬁeld’s simulation of studio lighting, and use a suitable picture as the background.

network to disambiguate the relationship between light and
albedo. Therefore, we design the network to explicitly learn
the information of albedo and normal. Such a multi-task
framework enforces the encoder and the decoder to learn
contextual information of the target actor, so as to produce
more detailed reﬂectance ﬁelds. Please refer to supplemen-
tary for more details

Training details. We leverage captured dynamic OLAT
sequences of the target actor to train our target-aware trans-
lation network. We synthesize a fully-illuminated image via
relighting the corresponding OLAT imageset Yt for each
frame. Portrait parsing will compute {βt}nt
t=1,

t=1, {θt}nt

t=1, and {ˆIl

{φt}nt
t}nt
into conditioning feature maps {Ft}nt
tion 3.2.

t=1 and further render and compose them
t=1 as described in Sec-

(cid:80)nt

So we compute the average head shape parameter ¯β =
t=1 βt
representing the actor’s head shape which will be
nt

used in synthesizing conditioning inputs.

We utilize Mean Square Error(MSE) loss and the Multi-
scale Structural Similarity(MS-SSIM) Index [68] loss to pe-
nalize the differences between the synthesized reﬂectance
ﬁeld and the ground truth, which are combined together as

a photometric loss Lcolor:

Lcolor(α) = EF ,Y[(cid:107)Y − Ψo(F; α)(cid:107)2

2 + fmss(Y, Ψo(F; α))],

(1)
where fmss(·) is a differentiable MS-SSIM function [68]; α
is the network weights of the proposed translation network;
Ψo(·) outputs the predicted reﬂectance ﬁeld of the network.
We designed loss for the two structural information of

albedo and normal:
Lnormal(α) =EN[1 − fangle(N, Ψn(α))],
Lalbedo(α) =EA[fvgg(A, Ψa(α)) + fmss(A, Ψa(α))],
(2)
where fangle(·) denotes the per-pixel cosine distance be-
tween the two vector [1], fvgg(·) denotes the output feature
of the third-layer of pretrained VGG-19; Ψn(·), Ψa(·) out-
puts the predicted normal map, predicted albedo map of the
network respectively.

Meanwhile, we deploy an adversarial loss LGAN to en-
hance the similarity of the distribution between predicted
reﬂectance ﬁeld and the ground truth so that the proposed
network can produce photo-realistic results. Our discrimi-
nator D(·) is inspired by the PatchGAN [12] classiﬁer and
has a similar architecture but difference in inputs. D(·)
conditions on the input, the conditioning feature maps F,
and either the predicted Ψo(F; α) or the ground-truth re-
ﬂectance ﬁeld Y. The adversarial loss LGAN has the fol-
lowing form:

LGAN (α, ω) =EF ,Y[log D(F, Y; ω)]

+EF [log(1 − D(F, Ψo(F; α); ω))],

(3)

where ω is the network weights of the discriminator D(·).

The color reﬂectance ﬁeld branch directly outputs a re-
ﬂectance ﬁeld with 96 portrait images. The lack of con-
straints on these images let the network’s outputs have slight
facial geometry differences, which is not expected in our
task and will affect the relighting quality. We propose
a context loss to alleviate this inconsistency in the pre-
dicted reﬂectance ﬁeld. Speciﬁcally, we re-render a fully-
illuminated portrait image from the predicted reﬂectance
ﬁeld which is a differentiable linear combination of output
OLAT images. Next, we extract the head shape parameters
β(Ψo(F; α)) of the re-rendered portrait image and expect it
to be as close to ¯β as possible. The context loss Lcontext is
formulated as:

Lcontext(α) = EF ((cid:107)β(Ψo(F; α)) − ¯β(cid:107)1).

(4)

We linearly combine these loss function as the following
objective function to ﬁnd the optimal weights of translation
network:

α∗ = arg min

α

max
ω

λ1LGAN (α, ω) + Lcolor(α)

+ Lnormal(α) + Lalbedo(α) + λ2Lcontext(α),

(5)

Figure 6. Application of virtual production.

where λ1 and λ2 are weights to balance the contribution of
these terms. We set λ1 = λ2 = 0.1 in our implementation.

3.4. Applications

The explicit disentanglement of the head pose, facial ex-
pression and lighting condition in NeRVP that produces
a photo-realistic video portrait of the target actor enables
many visual effects. Here, we introduce following basic
operations of our approach, whose combination empowers
various applications as demonstrated in Section 4.

Relighting.

Since our approach synthesizes 4D re-
ﬂectance ﬁelds for the target actor, we can relight the por-
trait video by linearly combining the RGB channels of
OLAT images according to arbitrary target environment
map. Such kind of explicit approach provides more reliable
and realistic outputs than learning-based methods which
easily produce low-contrast results.

Matting and Background Replacement. We generated
4d reﬂectance ﬁelds to simulate the lighting on the side and
back very well, which allows us to rim-lit the portrait by
relighting method to make the contour of the portrait more
distinguishable from the background. We use ModNet [27]
for matting of portraits, and we show the important role of
rim-light in the matting process in Fig. 11.

Face reenactment. Given a video clip of the source ac-
tor, our approach can transfer the pose and the facial expres-
sion to the portrait video of target actor. Finally, the target
actor’s face in the synthesized portrait video will reenact the
source actor.

4. Experimental Results

In this section, we evaluate our approach in a variety of

challenging scenarios.

4.1. Qualitative result

As we can see in the gallery Fig. 5, our approach can
transfer the facial expression vividly. In addition, the net-
work learns the OLAT frames explicitly, which makes it
possible to relight the portraits in high-dynamic-range. This
also enables us to imitate the photography studio lighting
and show an artistic relighting effect. We chose Rembrandt

Source ActorWith Lighting andExpression TransferWith Expression Transfer OnlyFigure 7. High frame rate dynamic OLAT generation.

Figure 9. Qualitative comparison on OLAT test set.

Comparison with other relighting methods

PSNR(↑)
Method
DVP+MTP
19.299±1.709
DVP+SIPR 17.655±1.854
27.271±1.659
Ours

SSIM (↑)
0.702±0.056
0.675±0.043
0.862±0.016

MAE (↓)
0.064±0.009
0.088±0.020
0.029±0.004

Table 1. Quantitative comparision. ↑ means the larger is better,
while ↓ means smaller is better.
In terms of three metrics, our
method outperforms the others.

lighting portraits are interfered by the interaction of the en-
vironment. Since our method is a novel approach to gen-
erating relightable neural video portrait, to demonstrate our
outstanding performance, we compare our method against
the one based on DVP (Deep Video Portraits) [31], which
suffers a lack of relightable capacity. Therefore, for fair
comparisons, we further add another state-of-the-art meth-
ods for relighting, Single Image Portrait Relight (SIPR) and
MassTransport approach (MTP), respectively.

As shown in both Fig. 8 and Fig. 9, our approach sur-
passes all other methods in visual effects. DVP+SIR suffers
from over-smoothness, leading to insufﬁciency of details
and unevenly distributed light on the target portrait face.
Moreover, DVP+MTP shows severe ﬂickering along with
varying illumination conditions from various angles of de-
ﬂection. In contrast, our approach achieves the perfect ren-
dering result in terms of photo-realism without any artifact.

For further quantitative comparison, we evaluate our ren-
dering results via three various metrics: signal-to-noise ra-
tio (PSNR), structural similarity index(SSIM) and mean
absolute error (MAE) to compare with existing state-of-art
approaches. We generate a sequence of relighted portraits
from all reference views which are evenly spaced in range
of illumination angles. As shown in the Tab. 1, our method
shows the outstanding performance on the photo-realistic
synthesis and controllable video editing in terms of all three
metrics with even less standard deviation.

Figure 8. Qualitative comparison on in-the-wild videos.

lighting and cyberpunk lighting for demonstration. To imi-
tate these two studio lighting, we need to have a good per-
formance under various challenging lighting conditions, in-
cluding rim light, hard directional light, backlight, top light
and ambient light. Therefore, it is very difﬁcult to imitate
these two types of artistic lighting conditions, where our
method shows superiority.

Our method also has many other applications. First, we
can generate high frame rate dynamic OLAT. Since the net-
work learns a mapping from facial parameters to reﬂection
ﬁelds, we can interpolate lighting by interpolating expres-
sion parameters. This makes it possible to generate 1000fps
OLAT data, which can be used in various scenes such as
slow motion ﬁll light in movies.
In reality, a 100,000fps
high-speed camera is required to collect such data, which
cannot be achieved by current technology. We show this
application in Fig. 7.

Our method can also be directly used in the ﬁeld of vir-
tual production, as illustrated in Fig.6. Our algorithm sup-
ports transferring such informative lighting and expressions
from the given source actor to the target one, enabling ﬁlm
making with more ﬂexibility out of the limitation of actors
and surrounding illumination conditions.

4.2. Comparisons

With the aid of the immediate and high-quality dynamic
facial reﬂectance ﬁeld inference, our novel synthesized neu-
ral video portraits endow the entire capacity of combin-
ing dynamic facial expressions, various illumination con-
ditions and changeable environmental backgrounds.
It is
particularly of competitive advantages over cases that re-

Relit resultsOLATRelit resultsRelit resultsOLATRelit resultsFigure 10. For the target person on the left, we use another person
to animate. The results for our full pipeline are shown in the ﬁrst
row, and the w/o normalization test result is shown in the second
row.

4.3. Ablation Study

In this subsection, we evaluate the performance of our
approach by comparing different designs of our method
components in quality and quantity. We use w/o normal-
ization to denote the variations of our approach without
normalizing the parameters of the extracted 3D face model.
As shown in Fig. 10, the bottom row illustrates the blinking
process without normalization on the parameters of the face
model. In contrast, the upper row shows the reenactment
of the same eye blinking motion after normalization. We
can observe that when the source person’s eyes are much
smaller than the target person’s, the animation results look
like that the target person cannot open the eyes, which is
absurd and unnatural. The key problem is that the source
person and target person have different facial geometries,
when we simply transfer the facial motions of the source
person, the size of eyes and mouth will not match the face
of the target person. Normalization maps the distribution
of the parameters of the source person to the target person,
which ensures that we can transfer facial motions in a rea-
sonable and photo-realistic way.

When replacing the background, a good alpha map is
necessary. We demonstrate the process of getting the alpha
map in Fig. 11 , in which we used the method of MOD-
Net [27]. We use w/ rim-light to denote using rim-light
to lit the contour of the target actor, and use w/o rim-light
to denote not using rim-light. The result shows that our
generated neural reﬂectance ﬁeld can simulate photorealis-
tic rim-light, which greatly improves the quality of matting,
especially for the ﬁne structure such as hair.

4.4. Limitations

We have demonstrated the compelling capability of si-
multaneous relighting and reenactment for neural video por-
trait with a variety of fancy visual editing effects. Never-
theless, as the ﬁrst trial for such disentanglement of facial
expression, background and lighting conditions for neural

Figure 11. We showed the simulation of rim-light by generated
neural reﬂectance ﬁeld and demonstrated its importance in mat-
ting.

portrait generation, our approach is subject to some limita-
tions. First, the output of our neural network is 96 OLAT
imagesets, leading to a huge GPU memory occupancy to
support the training process. This is the key limitation on
the spatial resolution of our generated results. Furthermore,
to fully reenact the expression of the source person, we need
to obtain a rich OLAT dataset of the target person, which is
very expensive to train the person-speciﬁc neural network
with a such a large-scale dataset. In addition, our method is
limited by the accuracy of the estimation of the facial para-
metric model, which means we cannot accurately estimate
the representative exaggerated expression of each person.
This makes our driving effect not accurate enough under
some expressions.

Our approach also do not utilizes ﬁne-grained 3D models
for other regions such as hair. Thus, we can occasionally
observe that the movements of these parts look unnatural
and may violate the laws of physics.

5. Conclusion

We have presented the ﬁrst approach to generate re-
lightable neural video portraits of a target performer, which
enables simultaneous relighting and reenactment via high-
quality facial reﬂectance ﬁeld learning. Our novel pipeline,
using multi-frame multi-task learning strategy,combines 4D
reﬂectance ﬁeld learning, model-based facial performance
capture and target-aware neural rendering, so as to trans-
fer the facial expressions from a source actor to a portrait
video of a target actor with arbitrary new background s and
lighting conditions. Extensive experimental results demon-
strate the effectiveness of our approach for simultaneous re-
lighting and reenactment with various visual editing effects
under high realism, which compares favorably to the state-
of-the-art. We believe that our approach is a critical step

Target Ground Truthw/o normalizationOursZoomed-inReferenceFrameAlpha Mattew/o Rim Lightw/Rim Lightfor the highly realistic synthesis of portrait video into var-
ious environments under the control of various meaningful
attributes, with many potential applications for fancy visual
effects in VR/AR, gaming, ﬁlming or entertainment.

References

[1] Victoria Fernandez Abrevaya, Adnane Boukhayma,
Philip H.S. Torr, and Edmond Boyer. Cross-modal deep face
normals with deactivable skip connections. In Proceedings
of
the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), June 2020. 6

[2] Oswald Aldrian and William AP Smith. Inverse rendering
of faces with a 3d morphable model. IEEE transactions on
pattern analysis and machine intelligence, 35(5):1080–1093,
2012. 3

[3] Hadar Averbuch-Elor, Daniel Cohen-Or, Johannes Kopf, and
Michael F Cohen. Bringing portraits to life. ACM Transac-
tions on Graphics (TOG), 36(6):1–13, 2017. 2

[4] Sai Bi, Stephen Lombardi, Shunsuke Saito, Tomas Simon,
Shih-EnWei, Kevyn McPhail, Ravi Ramamoorthi, Yaser
Sheikh, and Jason Saragih. Deep relightable appearance
models for animatable faces. ACM Trans. Graph. (Proc. SIG-
GRAPH), 40(4), 2021. 2

[5] Volker Blanz, Kristina Scherbaum, Thomas Vetter, and
In Com-
Hans-Peter Seidel. Exchanging faces in images.
puter Graphics Forum, volume 23, pages 669–676. Wiley
Online Library, 2004. 2

[6] Volker Blanz and Thomas Vetter. A morphable model for
In Proceedings of the 26th an-
the synthesis of 3d faces.
nual conference on Computer graphics and interactive tech-
niques, pages 187–194, 1999. 2

[7] Chen Cao, Derek Bradley, Kun Zhou, and Thabo Beeler.
Real-time high-ﬁdelity facial performance capture. ACM
Transactions on Graphics (ToG), 34(4):1–9, 2015. 2

[8] Chen Cao, Qiming Hou, and Kun Zhou. Displaced dynamic
expression regression for real-time facial tracking and ani-
mation. ACM Transactions on graphics (TOG), 33(4):1–10,
2014. 2

[9] Anpei Chen, Ruiyang Liu, Ling Xie, and Jingyi Yu. A free
viewpoint portrait generator with dynamic styling. arXiv
preprint arXiv:2007.03780, 2020. 3

[10] Xiaowu Chen, Mengmeng Chen, Xin Jin, and Qinping Zhao.
Face illumination transfer through edge-preserving ﬁlters. In
CVPR 2011, pages 281–287. IEEE, 2011. 3

[11] Paul Debevec, Tim Hawkins, Chris Tchou, Haarm-Pieter
Duiker, Westley Sarokin, and Mark Sagar. Acquiring the
In Proceedings of the
reﬂectance ﬁeld of a human face.
27th annual conference on Computer graphics and interac-
tive techniques, pages 145–156, 2000. 1, 2, 3

[12] Ugur Demir and G¨ozde B.

Patch-based image
inpainting with generative adversarial networks. CoRR,
abs/1803.07422, 2018. 6

¨Unal.

[13] Bernhard Egger, Sandro Sch¨onborn, Andreas Schnei-
der, Adam Kortylewski, Andreas Morel-Forster, Clemens
Blumer, and Thomas Vetter. Occlusion-aware 3d morphable
models and an illumination prior for face image analysis.

International Journal of Computer Vision, 126(12):1269–
1287, 2018. 3

[14] Bernhard Egger, William AP Smith, Ayush Tewari, Stefanie
Wuhrer, Michael Zollhoefer, Thabo Beeler, Florian Bernard,
Timo Bolkart, Adam Kortylewski, Sami Romdhani, et al.
3d morphable face models—past, present, and future. ACM
Transactions on Graphics (TOG), 39(5):1–38, 2020. 2
[15] Yao Feng, Haiwen Feng, Michael J Black, and Timo Bolkart.
Learning an animatable detailed 3d face model from in-the-
wild images. arXiv preprint arXiv:2012.04012, 2020. 4
[16] Ohad Fried, Ayush Tewari, Michael Zollh¨ofer, Adam Finkel-
stein, Eli Shechtman, Dan B Goldman, Kyle Genova, Zeyu
Jin, Christian Theobalt, and Maneesh Agrawala. Text-based
editing of talking-head video. ACM Transactions on Graph-
ics (TOG), 38(4):1–14, 2019. 1, 2

[17] Graham Fyffe, Andrew Jones, Oleg Alexander, Ryosuke
Ichikari, and Paul Debevec. Driving high-resolution facial
scans with video performance capture. ACM Transactions
on Graphics (TOG), 34(1):1–14, 2014. 2

[18] Guy Gafni, Justus Thies, Michael Zollh¨ofer, and Matthias
Dynamic neural radiance ﬁelds for monoc-
arXiv preprint

reconstruction.

Nießner.
ular 4d facial avatar
arXiv:2012.03065, 2020. 2

[19] Pablo Garrido, Levi Valgaerts, Ole Rehmsen, Thorsten
Thormahlen, Patrick Perez, and Christian Theobalt. Auto-
In Proceedings of the IEEE con-
matic face reenactment.
ference on computer vision and pattern recognition, pages
4217–4224, 2014. 2

[20] Pablo Garrido, Michael Zollh¨ofer, Dan Casas, Levi Val-
gaerts, Kiran Varanasi, Patrick P´erez,
and Christian
Theobalt. Reconstruction of personalized 3d face rigs from
monocular video. ACM Transactions on Graphics (TOG),
35(3):1–15, 2016. 2

[21] Kaiwen Guo, Peter Lincoln, Philip Davidson, Jay Busch,
Xueming Yu, Matt Whalen, Geoff Harvey, Sergio Orts-
Escolano, Rohit Pandey, Jason Dourgarian, et al. The re-
lightables: Volumetric performance capture of humans with
realistic relighting. ACM Transactions on Graphics (TOG),
38(6):1–19, 2019. 2, 3

[22] Alexandru Eugen Ichim, Soﬁen Bouaziz, and Mark Pauly.
Dynamic 3d avatar creation from hand-held video input.
ACM Transactions on Graphics (ToG), 34(4):1–14, 2015. 2
[23] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Efros.
Image-to-image translation with conditional adver-
sarial networks. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 1125–1134,
2017. 2

[24] Xinya Ji, Hang Zhou, Kaisiyuan Wang, Wayne Wu,
Chen Change Loy, Xun Cao, and Feng Xu. Audio-driven
emotional video portraits. arXiv preprint arXiv:2104.07452,
2021. 1, 2

[25] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks.
In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 4401–4410, 2019. 3

[26] Vahid Kazemi and Josephine Sullivan. One millisecond face
alignment with an ensemble of regression trees. In Proceed-

ings of the IEEE conference on computer vision and pattern
recognition, pages 1867–1874, 2014. 2, 4

IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 5124–5133, 2020. 3

[27] Zhanghan Ke, Kaican Li, Yurou Zhou, Qiuhua Wu, Xi-
angyu Mao, Qiong Yan, and Rynson W.H. Lau. Is a green
screen really necessary for real-time portrait matting? ArXiv,
abs/2011.11961, 2020. 6, 8
[28] Ira Kemelmacher-Shlizerman.

Internet based morphable
model. In Proceedings of the IEEE international conference
on computer vision, pages 3256–3263, 2013. 2

[29] Ira Kemelmacher-Shlizerman, Aditya Sankar, Eli Shecht-
In Eu-
man, and Steven M Seitz. Being john malkovich.
ropean Conference on Computer Vision, pages 341–353.
Springer, 2010. 2

[30] Hyeongwoo Kim, Mohamed Elgharib, Michael Zollh¨ofer,
Hans-Peter Seidel, Thabo Beeler, Christian Richardt, and
Christian Theobalt. Neural style-preserving visual dubbing.
ACM Transactions on Graphics (TOG), 38(6):1–13, 2019. 1,
2

[31] Hyeongwoo Kim, Pablo Garrido, Ayush Tewari, Weipeng
Xu, Justus Thies, Matthias Niessner, Patrick P´erez, Chris-
tian Richardt, Michael Zollh¨ofer, and Christian Theobalt.
Deep video portraits. ACM Transactions on Graphics (TOG),
37(4):1–14, 2018. 1, 2, 4, 7

[32] Kai Li, Qionghai Dai, Ruiping Wang, Yebin Liu, Feng Xu,
and Jue Wang. A data-driven approach for facial expres-
sion retargeting in video. IEEE Transactions on Multimedia,
16(2):299–310, 2013. 2

[33] Tianye Li, Timo Bolkart, Michael J Black, Hao Li, and Javier
Romero. Learning a model of facial shape and expression
from 4d scans. ACM Trans. Graph., 36(6):194–1, 2017. 4

[34] Kang Liu and Joern Ostermann. Realistic facial expression
In 2011 IEEE
synthesis for an image-based talking head.
International Conference on Multimedia and Expo, pages 1–
6. IEEE, 2011. 2

[35] Yang Liu, Alexandros Neophytou, Sunando Sengupta, and
Eric Sommerlade. Relighting images in the wild with a
self-supervised siamese auto-encoder. In Proceedings of the
IEEE/CVF Winter Conference on Applications of Computer
Vision, pages 32–40, 2021. 3

[36] Stephen Lombardi, Jason Saragih, Tomas Simon, and Yaser
Sheikh. Deep appearance models for face rendering. ACM
Transactions on Graphics (TOG), 37(4):1–13, 2018. 1, 2
[37] Abhimitra Meka, Christian Haene, Rohit Pandey, Michael
Zollh¨ofer, Sean Fanello, Graham Fyffe, Adarsh Kowdle,
Xueming Yu, Jay Busch, Jason Dourgarian, et al. Deep
reﬂectance ﬁelds: high-quality facial reﬂectance ﬁeld infer-
ence from color gradient illumination. ACM Transactions on
Graphics (TOG), 38(4):1–12, 2019. 2, 3

[38] Abhimitra Meka, Rohit Pandey, Christian H¨ane, Sergio Orts-
Escolano, Peter Barnum, Philip David-Son, Daniel Erickson,
Yinda Zhang, Jonathan Taylor, Soﬁen Bouaziz, et al. Deep
relightable textures: volumetric performance capture with
neural rendering. ACM Transactions on Graphics (TOG),
39(6):1–21, 2020. 2, 3, 4, 12

[39] Thomas Nestmeyer, Jean-Franc¸ois Lalonde, Iain Matthews,
and Andreas Lehrmann. Learning physics-guided face re-
In Proceedings of the
lighting under directional light.

[40] Rohit Kumar Pandey, Sergio Orts Escolano, Chloe LeGen-
dre, Christian Haene, Soﬁen Bouaziz, Christoph Rhemann,
Paul Debevec, and Sean Fanello. Total relighting: Learning
to relight portraits for background replacement. 2021. 2
[41] Elad Richardson, Matan Sela, and Ron Kimmel. 3d face re-
construction by learning from synthetic data. In 2016 fourth
international conference on 3D vision (3DV), pages 460–
469. IEEE, 2016. 2

[42] Elad Richardson, Matan Sela, Roy Or-El, and Ron Kimmel.
Learning detailed face reconstruction from a single image.
In Proceedings of the IEEE conference on computer vision
and pattern recognition, pages 1259–1268, 2017. 2

[43] Joseph Roth, Yiying Tong, and Xiaoming Liu. Adaptive
3d face reconstruction from unconstrained photo collections.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 4197–4206, 2016. 2

[44] Soumyadip Sengupta, Angjoo Kanazawa, Carlos D Castillo,
and David W Jacobs. Sfsnet: Learning shape, reﬂectance
and illuminance of facesin the wild’. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 6296–6305, 2018. 3

[45] YiChang Shih, Sylvain Paris, Connelly Barnes, William T
Freeman, and Fr´edo Durand. Style transfer for headshot por-
traits. 2014. 3

[46] Zhixin Shu, Sunil Hadap, Eli Shechtman, Kalyan Sunkavalli,
Sylvain Paris, and Dimitris Samaras. Portrait lighting trans-
fer using a mass transport approach. ACM Transactions on
Graphics (TOG), 36(4):1, 2017. 3

[47] Yibing Song, Linchao Bao, Shengfeng He, Qingxiong Yang,
and Ming-Hsuan Yang. Stylizing face images via multi-
ple exemplars. Computer Vision and Image Understanding,
162:135–145, 2017. 3

[48] Tiancheng Sun, Jonathan T Barron, Yun-Ta Tsai, Zexiang
Xu, Xueming Yu, Graham Fyffe, Christoph Rhemann, Jay
Busch, Paul E Debevec, and Ravi Ramamoorthi. Single
image portrait relighting. ACM Trans. Graph., 38(4):79–1,
2019. 2, 3

[49] Tiancheng Sun, Zexiang Xu, Xiuming Zhang, Sean Fanello,
Christoph Rhemann, Paul Debevec, Yun-Ta Tsai, Jonathan T
Light stage super-
Barron, and Ravi Ramamoorthi.
ACM
resolution: continuous high-frequency relighting.
Transactions on Graphics (TOG), 39(6):1–12, 2020. 2
[50] Supasorn Suwajanakorn, Ira Kemelmacher-Shlizerman, and
Steven M Seitz. Total moving face reconstruction.
In
European conference on computer vision, pages 796–812.
Springer, 2014. 2

Steven M Seitz,

[51] Supasorn Suwajanakorn,

and Ira
Kemelmacher-Shlizerman. What makes tom hanks look
In Proceedings of the IEEE International
like tom hanks.
Conference on Computer Vision, pages 3952–3960, 2015. 2
[52] Ayush Tewari, Florian Bernard, Pablo Garrido, Gaurav
Bharaj, Mohamed Elgharib, Hans-Peter Seidel, Patrick
P´erez, Michael Zollhofer, and Christian Theobalt. Fml:
In Proceedings of the
Face model learning from videos.
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 10812–10822, 2019. 2

[53] Ayush Tewari, Abdallah Dib, Tim Weyrich, Bernd Bickel,
Hans-Peter Seidel, Hanspeter Pﬁster, Wojciech Matusik,
Louis Chevallier, Mohamed Elgharib, Christian Theobalt,
et al. Photoapp: Photorealistic appearance editing of head
portraits. arXiv preprint arXiv:2103.07658, 2021. 3

[54] Ayush Tewari, Mohamed Elgharib, Florian Bernard, Hans-
Peter Seidel, Patrick P´erez, Michael Zollh¨ofer, and Chris-
tian Theobalt. Pie: Portrait image embedding for semantic
control. ACM Transactions on Graphics (TOG), 39(6):1–14,
2020. 3

[55] Ayush Tewari, Mohamed Elgharib, Gaurav Bharaj, Florian
Bernard, Hans-Peter Seidel, Patrick P´erez, Michael Zoll-
hofer, and Christian Theobalt.
Stylerig: Rigging style-
gan for 3d control over portrait images. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 6142–6151, 2020. 3

[56] Ayush Tewari, Ohad Fried, Justus Thies, Vincent Sitzmann,
Stephen Lombardi, Kalyan Sunkavalli, Ricardo Martin-
Brualla, Tomas Simon, Jason Saragih, Matthias Nießner,
Rohit Pandey, Sean Fanello, Gordon Wetzstein, Jun-Yan
Zhu, Christian Theobalt, Maneesh Agrawala, Eli Shechtman,
Dan B. Goldman, and Michael Zollh¨ofer. State of the Art on
Neural Rendering. Computer Graphics Forum, 2020. 1, 2

[57] Ayush Tewari, Tae-Hyun Oh, Tim Weyrich, Bernd Bickel,
Hans-Peter Seidel, Hanspeter Pﬁster, Wojciech Matusik,
Mohamed Elgharib, Christian Theobalt, et al. Monocular re-
construction of neural face reﬂectance ﬁelds. arXiv preprint
arXiv:2008.10247, 2020. 3

[58] Ayush Tewari, Michael Zollh¨ofer, Pablo Garrido, Florian
Bernard, Hyeongwoo Kim, Patrick P´erez, and Christian
Theobalt. Self-supervised multi-level face model learning
In Proceed-
for monocular reconstruction at over 250 hz.
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 2549–2559, 2018. 2

[59] Ayush Tewari, Michael Zollhofer, Hyeongwoo Kim, Pablo
Garrido, Florian Bernard, Patrick Perez, and Christian
Theobalt. Mofa: Model-based deep convolutional face au-
toencoder for unsupervised monocular reconstruction.
In
Proceedings of the IEEE International Conference on Com-
puter Vision Workshops, pages 1274–1283, 2017. 2

[60] Justus Thies, Mohamed Elgharib, Ayush Tewari, Christian
Theobalt, and Matthias Nießner. Neural voice puppetry:
In European Conference
Audio-driven facial reenactment.
on Computer Vision, pages 716–731. Springer, 2020. 1, 2

[61] Justus Thies, Michael Zollh¨ofer, and Matthias Nießner. De-
ferred neural rendering: Image synthesis using neural tex-
tures. ACM Transactions on Graphics (TOG), 38(4):1–12,
2019. 2

[62] Justus Thies, Michael Zollh¨ofer, Matthias Nießner, Levi Val-
gaerts, Marc Stamminger, and Christian Theobalt. Real-
time expression transfer for facial reenactment. ACM Trans.
Graph., 34(6):183–1, 2015. 2

[63] Justus Thies, Michael Zollhofer, Marc Stamminger, Chris-
tian Theobalt, and Matthias Nießner. Face2face: Real-time
In Proceed-
face capture and reenactment of rgb videos.
ings of the IEEE conference on computer vision and pattern
recognition, pages 2387–2395, 2016. 1, 2

[64] Justus Thies, Michael Zollh¨ofer, Marc Stamminger, Chris-
tian Theobalt, and Matthias Nießner. Facevr: Real-time
gaze-aware facial reenactment in virtual reality. ACM Trans-
actions on Graphics (TOG), 37(2):1–15, 2018. 2

[65] Justus Thies, Michael Zollh¨ofer, Christian Theobalt, Marc
Stamminger, and Matthias Nießner. Headon: Real-time reen-
actment of human portrait videos. ACM Transactions on
Graphics (TOG), 37(4):1–13, 2018. 1, 2

[66] Daniel Vlasic, Matthew Brand, Hanspeter Pﬁster, and Jovan
In ACM

Popovic. Face transfer with multilinear models.
SIGGRAPH 2006 Courses, pages 24–es. 2006. 2

[67] Yang Wang, Zicheng Liu, Gang Hua, Zhen Wen, Zhengyou
Zhang, and Dimitris Samaras. Face re-lighting from a single
image under harsh lighting conditions. In 2007 IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
1–8. IEEE, 2007. 3

[68] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli.
from error visibility to struc-
IEEE Transactions on Image Processing,

Image quality assessment:
tural similarity.
13(4):600–612, 2004. 5, 6

[69] Zhibo Wang, Xin Yu, Ming Lu, Quan Wang, Chen Qian, and
Feng Xu. Single image portrait relighting via explicit mul-
tiple reﬂectance channel modeling. ACM Transactions on
Graphics (TOG), 39(6):1–13, 2020. 3

[70] Steve Wright. Digital compositing for ﬁlm and video. Rout-

ledge, 2006. 1

[71] Chenglei Wu, Derek Bradley, Markus Gross, and Thabo
Beeler. An anatomically-constrained local deformation
model for monocular face capture. ACM transactions on
graphics (TOG), 35(4):1–12, 2016. 2

[72] Shugo Yamaguchi, Shunsuke Saito, Koki Nagano, Yajie
Zhao, Weikai Chen, Kyle Olszewski, Shigeo Morishima, and
Hao Li. High-ﬁdelity facial reﬂectance and geometry infer-
ence from an unconstrained image. ACM Transactions on
Graphics (TOG), 37(4):1–14, 2018. 2

[73] Xinwei Yao, Ohad Fried, Kayvon Fatahalian, and Maneesh
Agrawala. Iterative text-based editing of talking-heads using
neural retargeting, 2020. 1, 2

[74] Xiuming Zhang, Sean Fanello, Yun-Ta Tsai, Tiancheng Sun,
Tianfan Xue, Rohit Pandey, Sergio Orts-Escolano, Philip
Davidson, Christoph Rhemann, Paul Debevec, et al. Neu-
ral light transport for relighting and view synthesis. ACM
Transactions on Graphics (TOG), 40(1):1–17, 2021. 3
[75] Hao Zhou, Sunil Hadap, Kalyan Sunkavalli, and David W Ja-
cobs. Deep single-image portrait relighting. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, pages 7194–7202, 2019. 3

[76] Xiangyu Zhu, Xiaoming Liu, Zhen Lei, and Stan Z Li. Face
IEEE
alignment in full pose range: A 3d total solution.
transactions on pattern analysis and machine intelligence,
41(1):78–92, 2017. 2

[77] Michael Zollh¨ofer, Justus Thies, Pablo Garrido, Derek
Bradley, Thabo Beeler, Patrick P´erez, Marc Stamminger,
Matthias Nießner, and Christian Theobalt. State of the art
on monocular 3d face reconstruction, tracking, and applica-
tions. In Computer Graphics Forum, volume 37, pages 523–
550. Wiley Online Library, 2018. 2

Supplementary Materials

Figure 12. The demonstration of our capturing system. Cameras
and lights are arranged on a spherical structure.

6. Hardware architecture and capture settings

To recover the 4D reﬂectance ﬁelds of dynamic portrait,
we build up a light stage device with 96 LED light sources
and a stationary 4K ultra-high-speed camera at 1000 fps,
resulting in a temporal OLAT image set at 25 fps, so as to
provide ﬁne-grained facial perception.

Under such a dynamic capture setting, the target per-
former can freely speak, translate and rotate in a certain
range to provide a continuous and dynamic OLAT im-
age sets sequence. However, one of the most challeng-
ing issues is that the motion of the captured target along
with data acquisition will cause misalignment, leading to
blurriness in the OLAT image sets, making the data post-
processing more challenging. We conquer such limitations
using an optical ﬂow algorithm and further retrieve results
at a higher frame rate using densely-acquired homogeneous
full-lit frames.

Our hardware architecture is demonstrated in Fig. 12,
which is a spherical dome of a radius of 1.3 meters with 96
fully programmable LEDs and a 4K ultra-high-speed cam-
era. The LEDs are independently controlled via a synchro-
nization control system and evenly localized on the doom
to provide smooth lighting conditions.

Single PCC (Phantom Capture Camera) is leveraged
with a global shutter setting, generating roughly 6 TB of
data in total. For each data session, we collect video se-
quences in 25 seconds rather than isolated frames with 700
us exposure time and 1000 EI, which allows a lower noise

Figure 13. The samples of our captured data. (a) to (e) are samples
of OLAT images; (f) is a full-lit frame image for calculating the
optical-ﬂow. The bottom-left illustrates the corresponding lighting
conditions on our system.

ﬂoor. In practice, we can simultaneously control the PCC
along with the LEDs system.

During capture, all the 96 LEDs follow the patterns

shown in Fig. 13.

During the capture period, the high-speed camera syn-
chronizes with the 96 LEDs at 1000 fps and outputs an
8-bit Bayer pattern color image stream at a resolution of
2048×1440.

Note that it is required at least 0.1s to acquire a complete
dynamic session with 96 LEDs. Such duration causes mis-
alignment, making it challenging to handle blurriness in the
dataset and the low frame rate gives rise to inconsistent dy-
namic facial capture results. Inspired by the approach [38],
we interleave “tracking frames” into the capture sequence
during every six images rather than a complete session to
conquer such drawbacks and cast the tracking frames as ref-
erences, as shown in Fig. 14.

Instead of capturing an image with homogeneous illumi-
nation for every 96 images, we capture an image for track-
ing purposes every 6 images. This capture strategy allows
us to align the OLAT data between 14 consecutive groups
of full-bright frames in any pose to the middle frame with
optical ﬂow. It is equivalent to that the image between every
two homogeneous illumination frames is multiplexed thir-
teen times to enhance the ﬁnal optical ﬂow result.

7. Normalization Operation Process

Note that when the source and target actors are different,
their facial geometry will be different even in similar facial
expressions, which will manifest in facial characteristics,
such as eye size, nose length, mouth curvature, etc. We ex-

9. Quantitative comparisons

For quantitative comparison, we evaluate our render-
ing results via three various metrics: signal-to-noise ratio
(PSNR), structural similarity index(SSIM) and mean abso-
lute error (MAE) to compare with existing state-of-art ap-
proaches. For comparison, we generate a sequence of re-
lighted portraits from all reference views which are evenly
spaced in range of illumination angles. As shown in Fig. 16
due to the high-quality reﬂectance ﬁeld inference and fully
disentanglement, our method enables entire photo-realistic
video portraits synthesis under various illumination con-
ditions, making the rendering quality of any temporal se-
quence surpassing other baselines with higher PSNR, SSIM
and lower MAE.

10. Application: Virtual Conference

By utilizing our generated dynamic facial reﬂectance
ﬁeld, we can achieve a relightable virtual conference as
shown in Fig. 17. No matter whether the user is dressing
casually with messy hair, or lying on the sofa with a cup
of coffee, the user can appear decently in front of others in
suits. The conference background can be changed arbitrar-
ily, the most impressive effect is that the light conditions on
the generated user in suits will perfectly match the environ-
ment. Furthermore, if the user is walking on the street in a
rush with a shaking camera, or even he is not looking and
the camera, by explicitly controlling the pose parameters,
our approach can always let the user look like attending the
conference naturally.

Figure 14. The illustration of capture timing and frame alignment.
our method computes optical ﬂows between full-lit images and
warps OLAT images accordingly. The ”overlapping” strategy al-
low us to reuse a same OLAT image in different OLAT image set
so that we can achieve higher capture frame rate.

pect the distribution of conditioning feature maps generated
from the source actor is similar to one of the target actor, so
as to preserve target-aware appearance rendering results.

Speciﬁcally, we assume parameters as random variables
which follow the normal distributions. For each parameter,
we normalize both the mean and variance of the source ac-
tor’s distribution Xs ∼ N (µs, σ2
s ) to be the same as the
target actor’s distribution Xt ∼ N (µt, σ2
t ). Thus, the nor-
malized parameter ˆXs can be formulated as:

ˆXs = (σt (cid:11) σs) ◦ (Xs − µs) + µt ∼ N (µt, σ2

t ),

(6)

where (cid:11) and ◦ are element-wise division and product re-
spectively. In our implementation, we regard the head pose
θ and facial expression φ of FLAME parameters as two in-
dividual random variables and use the Eqn. 6 for normaliza-
tion.

Thus, the conditioning feature maps Ft at time t are for-

mulated as:

t ,˜Ic

Ft = {˜Id

t ,˜Il
(7)
t = Π(G(β, ˜θ, ˜φ), H) and Π is the rasterization
where ˜Id
and texture mapping function; ˜θ and ˜φ are normalized pa-
rameters.

t ,˜Ic

t},

8. Network Architecture

We adopt the U-Net architecture to the proposed trans-
lation network as illustrated in Fig. 15. Our network in-
ferences both the facial reﬂectance ﬁeld, the normal, and
the albedo simultaneously to facilitate portrait video com-
position applications. The proposed network consists of an
encoder and a decoder module. The encoder extracts multi-
scale latent representations of conditioning feature maps,
while the decoder module generates the reﬂectance ﬁeld,
the normal, and the albedo.

Such a multi-task framework enforces the network to
learn contextual information of the target actor, so as to pro-
duce more detailed reﬂectance ﬁelds.

..................Capture Time Lineoptical flowFull-lit imageOLAT imageAligned imageA OLAT imageset(96 OLAT frames)6 aligned OLAT images 6  OLAT imagesFigure 15. Our multi-frame multi-task architecture design for our rendering-to-video network.

Figure 16. Quantitative comparisons on the test reference data with ground truth. Our results are more realistic and closer to ground truth.

Figure 17. We demonstrated the application of our algorithm in the virtual conference. Users can participate in video conferences on
formal occasions in a widerrange of scenarios. Through the control of pose, we can realize that the user still presents a decent participation
state in scenes such as walking and notlooking at the camera.

Conv2DLeaky ReLUBatch NormSkip ConnectionT-Conv2DDropoutReLUBatch NormConv2DReLUBatch NormConv2DDropoutReLUBatch NormDropout11 framesTexturePNCCOLAT9596 imagesReflectance FieldAlbedoNormalEncoder641282565125125125125125125125122561286496*3512Decoder