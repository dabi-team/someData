2
2
0
2

y
a
M
1
3

]

G
L
.
s
c
[

1
v
4
9
8
5
1
.
5
0
2
2
:
v
i
X
r
a

VQ-AR: Vector Quantized Autoregressive Probabilistic
Time Series Forecasting

Kashif Rasul
kr95698@navercorp.com

Young-Jin Park
NAVER CLOVA & NAVER AI Lab
young.j.park@navercorp.com

Max Nihlén Ramström
NAVER CLOVA
max.nihlen.ramstrom@navercorp.com

Kyung-Min Kim
NAVER CLOVA & NAVER AI Lab
kyungmin.kim.ml@navercorp.com

Abstract

Time series models aim for accurate predictions of the future given the past, where
the forecasts are used for important downstream tasks like business decision mak-
ing. In practice, deep learning based time series models come in many forms, but
at a high level learn some continuous representation of the past and use it to output
point or probabilistic forecasts. In this paper, we introduce a novel autoregressive
architecture, VQ-AR, which instead learns a discrete set of representations that are
used to predict the future. Extensive empirical comparison with other competitive
deep learning models shows that surprisingly such a discrete set of representations
gives state-of-the-art or equivalent results on a wide variety of time series datasets.
We also highlight the shortcomings of this approach, explore its zero-shot general-
ization capabilities, and present an ablation study on the number of representations.
The full source code of the method will be available at the time of publication
with the hope that researchers can further investigate this important but overlooked
inductive bias for the time series domain.

1

Introduction

Time series forecasting is an important business and scientiﬁc machine learning problem which is
typically used to support decision making for down-stream tasks. Classical methods like in [22]
model each time series in a dataset individually using hand-crafted features and are used extensively
as they providing strong baselines.

Recently deep learning based methods offer an alternative approach by utilizing a shared model
trained over the whole dataset of time series, without the need for explicit feature engineering. These
methods are competitive, especially when dealing with a large amount of data, against the classical
methods, but can also suffer from overﬁtting or learn spurious correlations due to their ﬂexibility.

Often the forecasts are point valued, however it is much more useful for the decision making process
to also incorporate the inherent (aleatoric) uncertainty present in the observations via probabilistic
forecasts. In the univariate setting this is usually done by learning the parameters of some chosen
distribution, or via quantile regression or non-parametric and semi-parametric models which explicitly
learn the conditional quantiles of the next time step given the past [39]. Other approaches include
using Conformal Predictions [49, 54]. In the multivariate setting [50] however, one needs to resort
to some approximation of the full multivariate distribution [47] due to computational tractability or
learn the conditional distribution via Normalizing Flows [46, 10], Energy Based Methods [45], or
Generative Adversarial Networks (GANs) [30].

Preprint. Under review.

 
 
 
 
 
 
As in most supervised deep learning methods, these models learn some representation of the history
of a time series in order to best forecast, which can be thought of as a kind of self-supervised learning
task (not unlike GPT [7] or Bert [11] in the Natural Language Processing (NLP) setting). Thus in
order to provide better historical representations we have a large collection of architectural choices
we can make for the problem or dataset at hand.

The core insight of this work is the realization that there might only be a discrete collection of
representations of time series histories which could potentially be used for the prediction task.
The success of modern NLP on the back of discrete token representations leads us to consider if
something similar would also work in the time series domain. Thus, in this paper we take inspiration
from the Vector Quantized-Variational AutoEncoder (VQ-VAE) [51] model to learn a set of latent
representations in order to best forecast in an autoregressive fashion.

Thus, the main contribution of this paper is a novel autoregressive time series forecasting method
which is trained end-to-end and provides excellent inductive bias for the probabilistic forecasting
task. We highlight the model’s performance and properties via extensive experimentation and show
that it achieves comparable results against ensembles of tree-based methods without extensive feature
engineering.

2 Background

We introduce some notation and go over some background material needed for the model ﬁrst.

In this work we assume the univariate forecasting setup of DeepAR [48], however the method also
applies to the multivariate setting. Formally we assume we are given a training dataset of D ≥ 1
t ∈ R or in N. The
time series Dtrain = {xi
problem will require us to predict P ≥ 1 steps into the future and thus will come with a back-testing
test set of these D time series denoted by Dtest = {xi
Ti+1:Ti+P }. Note that even though we are
denoting the time index t by an incremental counter here, in reality each t has a date-time associated
with it, which increments regularly based on the frequency of the dataset. Often we will have all the
Ti be the same date-time for the time series of a particular dataset we use.

} where i ∈ {1, . . . , D} and at each time t we have xi

1:Ti

2.1 Probabilistic Forecasting

In the probabilistic forecasting problem, we wish to learn the unknown future distribution of the data
given its past. Rather than considering the whole history of each time series i in a dataset (which
might not be of the same size) we can instead consider some ﬁxed context window of size C ≥ 1 of
our choosing and try to learn this potentially complex unknown distribution of the future values given
the context window sized past denoted by
pX (xi

(1)

Ti+1:Ti+P |xi

Ti+1−C:Ti

).

Thus, if we denote the parameters of our deep learning model by θ, we can approximate (1) by an
autoregressive model which we can write via the chain-rule of probability as

pX (xi

Ti+1:Ti+P |xi

Ti+1−C:Ti

; θ) = ΠTi+P

t=Ti+1pX (xi

t|xi

Ti+1−C:t−1; θ).

And as we can see at a high level the probabilistic forecasting problem reduces down to learning some
representation of the context window past together with a distribution model of the next time step(s)
given this representation. For example, the DeepAR model uses a Recurrent Neural Network (RNN)
(like the LSTM [20] or GRU [8]) to represent the context window history together with associated
covariates, denoted by real-valued RF sized vectors ci

ht = RNN(concat(xi

1:Ti+P (known for all times), as
t−1, ci

t), ht−1; θ),

with ht ∈ RH and h0 = (cid:126)0, to learn pX (xi
parameters of some chosen distribution, e.g.
maximize the log-likelihood of the distribution with respect to the ground-truth xi
and t in Dtrain.

t|ht; θ). The distribution head in DeepAR learns the
the mean and variance of a Gaussian and we can
t via SGD for all i

By incorporating different architectural inductive biases to learn the historic representation together
with different distribution heads, researchers have come up with a zoo of models to solve the

2

probabilistic forecasting problem in the deep learning domain, for not only the univariate and
multivariate or regular and irregular time series setting, but also for spatial-temporal problems as
well.

2.2 VQ-VAE

Leaving time series forecasting aside for the moment we can consider the fact that latent represen-
tations can potentially also be discrete. Thus instead of learning a continuous valued distributional
representation as in Variational AutoEncoders (VAE) [28], the VQ-VAE model learns a ﬁxed size or
discrete set of representations of the data via an encoder-decoder bottleneck.

Formally the model consists of an encoder that maps its input onto a ﬁxed sized set of latent variables
and a decoder that reconstructs the input from one of these ﬁxed number of latents. Both the encoder
and decoder use this shared codebook of vectors. Thus if we denote the encoding representation
by henc ∈ RE, then this vector is quantized based on its distance to the prototype vectors in the
codebook {z1, . . . , zJ } such that the henc is replaced by the closest prototype vector:
Quantize(henc) := zn where n = arg min

(cid:107)henc − zj(cid:107)2,

(2)

j

for a codebook size J ≥ 1 of our choosing.

The mappings are learned by back-propagating the gradient of a reconstruction error via the decoder to
the encoder using the Straight-Through gradient estimator [19, 4]. Apart from this loss the VQ-VAE
model also has two terms that encourages the alignment of the vector space of the codebook with the
output of the encoder, namely the codebook loss which pushes the selected codebook entry z closer
to the encoder representation and the commitment loss which encourages the output of the encoder to
stay close to the chosen codebook vector to stop it switching too frequently from one code vector to
another. The “stop-gradient” or “detach” operator sg(·) blocks the gradients from ﬂowing into its
argument and β is the hyperparameter which penalizes the code vector ﬂuctuating, then the two extra
entries of the loss are given by

(cid:107)sg(henc) − z(cid:107)2

2 + β(cid:107)sg(z) − henc(cid:107)2
2.

(3)

Since the optimal code would be the average of the representations, [51] presents an exponential
moving average update scheme of the latents instead of the codebook loss (ﬁrst term of (3)) during
training. The SoundStream [55] paper proposes to initialize the codebook by the k-means centroids
of the representations from the ﬁrst batch. Additionally, as proposed in Jukebox [12], we deploy the
heuristic where we replace codebook vectors that have an exponential moving average cluster size
less than some threshold Q by a random vector from the batch.

3 VQ-AR Method

We motivate this method with the observation that time series data in a dataset visually looks similar
with repeated patterns, and thus we ask if instead of a continuous representation of the histories as
learned by most deep learning models, could a discrete set of representations sufﬁce? With this in
mind, we introduce the VQ-AR model which incorporates an encoder-decoder RNN together with a
Vector Quantizer (VQ) bottleneck.

Unlike in the DeepAR setting, where we learn the temporal representation of each time series’
context window to predict the distribution of the next step, we instead ﬁrst encode it to a quantized
representation zn given by (2) using an encoding-RNN’s state denoted by

t = RNNenc(concat(xi
henc

t−1, ci

t), henc

t−1; θ),

with henc
pX (xi

t
t|zn; θ) using a decoding-RNN

∈ RE and henc

0 = (cid:126)0. Subsequently we model the distribution of the next time step

t = RNNdec(Quantize(henc
hdec

t

), hdec

t−1; θ),

which only takes the Vector-Quantized latent zn (2) as input, with hdec
Figure 1 for a schematic of the model.

t ∈ RH and hdec

0 = (cid:126)0. See

3

Figure 1: VQ-AR model schematic at time point t − 1 for time series i. During training the model takes
as input the target xi
t and outputs the parameters of some chosen distribution to
minimize the negative log-likelihood and Vector Quantizer loss (5); and during inference (dashed) it
allows for sampling of the next time point ˆxi

t−1 and covariates ci

t, in an autoregressive fashion.

Because the prior p(zj) = 1/J is a uniform categorical distribution and the posterior

q(zn|xi

1:t−1, ci

2:t; θ) =

(cid:26)1
0

if n = arg minj (cid:107)henc
otherwise,

t − zj(cid:107)2

(4)

is a categorical distribution parametrized by one-hot probability vectors, the KL-divergence term
in the Evidence Lower Bound (ELBO) [6] is constant (log J), and since the ELBO’s expectation
term is over a deterministic distribution (4), we can simply train the whole model by maximizing the
log-likelihood term of the next time step: log pX (xi
Thus, similar to the DeepAR model we minimize the negative log-likelihood of some chosen paramet-
ric distribution and train the whole model end-to-end together with the VQ loss (3). For example,
to model Gaussian emissions the distribution head consist of a linear layer which outputs the mean
µt ∈ R and standard deviation σt ∈ R+, making sure that the standard deviation is always positive
via the softplus(·) non-linearity. These parameters are used to instantiate the distribution N (µt, σt)
from which we calculate the negative log-likelihood: − log (cid:96)N (xi
t). The full loss of the model for
time point t − 1 in the training window and series i is thus denoted by

t|zn, hdec

t−1; θ).

Li

t−1(θ) := − log (cid:96)N (xi

t) + (cid:107)sg(henc

t

) − zn(cid:107)2

2 + β(cid:107)sg(zn) − henc

t (cid:107)2
2.

(5)

3.1 Training

We construct batches B of context and subsequent prediction window of total size T + 1, by sampling
a random time series (and corresponding covariates) from Dtrain and then sample this window
randomly within. We will then minimize the mini-batch loss, which is the mean of all the individual
losses (5) denoted by

L(θ) :=

1
|B|T

(cid:88)

T
(cid:88)

Li

t(θ),

xi

1:T +1∈B

t=1

via the Adam [27] SGD optimizer at each step of training with an appropriate batch size |B| and
learning rate.

3.2

Inference

At inference time we go over the last context sized window of each time series in the training
dataset Dtrain and then start forecasting for the P prediction steps by sequentially sampling from
the distribution head and autoregressively passing this value back into the model (together with the
covariates which are known for all times). In fact, we can repeat this process S number of times (e.g.

4

Enc-RNNVectorQuantizerDec-RNNdistributionheadlog_prob/samplelatent +  commitmentloss......codebookS = 100) to then report any empirical probability interval of interest as well as probabilistic metrics,
like Continuous Ranked Probability Score (CRPS) [15, 37], with respect to the ground-truth values in
Dtest. Point forecasting metrics can also be evaluated with respect to the empirical median or mean
from the S samples at each time step of the prediction.

Note, unlike in some generative modeling use cases, we do not sample with a reduced temperature
distribution to obtain higher quality samples, nor do we Beam-search or do Nucleus sampling as in
NLP settings during inference.

3.3 Covariates

As mentioned in section 2.1, we can incorporate covariates for each time point of the time series
(as well as for all future times) by creating time features given the granularity of the time series in
question. For example, for daily data we could consider the day of the week, week of the month,
month of the year, etc., features. Although not strictly necessary, but certainly helpful, we can take
inspiration from classical methods and construct time series covariates, such as lagged features,
moving averages, difference to previous time points, exponential moving averages, and moving
percentiles, etc., of xi (and/or of temporal covariates) depending on the frequency of the time series
in question.

Where applicable holidays or other recurring events can also be turned into temporal features. A
running “age” counter can also serve as an indication of the length of a time series. Apart from these
temporal features, one can embed the categorical identity i ∈ {1, . . . , D} of each time series into
a vector via a trainable embedding layer whose time independent output is copied over for all time
points being considered. If the time series are grouped then their categorical tree structure can also be
represented via similar embeddings.

Unless explicitly stated, for all our experiments we only use time, age, lagged features, and categorical
identities as covariates.

3.4 Scaling

Entities in time series data can potentially have arbitrary magnitudes and so to be able to train a
shared deep learning model with respect to inputs of any scale we can utilize the following heuristic:
for each context window we can calculate the mean value of the data in the interval, speciﬁcally
νi = (cid:80)T
t/T (if it is not zero or 1 otherwise) and divide the time series by this before using it in
the model. The outputs from the distribution heads are then multiplied by this νi when sampling or
we rescale in the parameter space of the distribution (e.g. when we require integer samples from a
Poisson or Negative-Binomial head).

t=1 xi

We omit stating this heuristic in all our derivations and treat it as an implementation detail.

4 Experiments

We test the performance of VQ-AR for the forecasting task in terms of performance and also highlight
the limitations of the model and generalization properties of the representations in this section.
For our experiments we use Exchange [31], Solar [31], Elec.1, Traffic2, Taxi3, Wikipedia4
open datasets, preprocessed exactly as in [47], as well as M5 [35] and proprietary E-Commerce dataset,
with their properties listed in Table 5 in the appendix B. As can be noted in the table, we do not need
to normalize scales for the Traffic dataset. From the names of the datasets, we see that we cover a
number of time series domains including ﬁnance, weather, energy, logistics, page-views, as well as
E-commerce.

1https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014
2https://archive.ics.uci.edu/ml/datasets/PEMS-SF
3https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page
4https://github.com/mbohlkeschneider/gluon-ts/tree/mv_release/datasets

5

As indicated above, for evaluation we employ the CRPS metric which measures the compatibility of
a cumulative distribution function (CDF) F with the ground-truth observation x as

CRPS(F, x) =

(cid:90)

R

(F (y) − I{x ≤ y})2 dy,

where I{x ≤ y} is the indicator function which is one if x ≤ y and zero otherwise. CRPS is a proper
scoring function, meaning CRPS attains its minimum when the predictive distribution F and the
data distribution are equal. Employing the empirical CDF of F , i.e. ˆF (y) = 1
I{x(s) ≤ y}
S
using S samples x(s) ∼ F as a natural approximation of the predictive CDF, CRPS can be directly
computed from sampled predictions from the model at each time point [26]. The ﬁnal metric is
averaged over all the prediction time steps and time series in a dataset.

(cid:80)S

s=1

4.1 Forecasting

We will compare VQ-AR with the following deep learning baseline probabilistic univariate models

• DeepAR [48]: an RNN based probabilistic model which learns the parameters of some

chosen distribution for the next time point;

• MQCNN [53]: a Convolutional Neural Network model which outputs chosen quantiles of the

forecast upon which we regress the ground truth via Quantile loss;

• SQF-RNN [14]: an RNN based non-parametric method which models the quantiles via linear

splines and also regresses the Quantile loss;

• IQN-RNN [16]: combines an RNN model with an Implicit Quantile Network (IQN) [9] head

to learn the distribution similar to SQF-RNN;

• LSF [18]: a method that transform a point-estimator, coming from for example gradient

boosting methods, into a probabilistic one;

as well as the classical ETS [23] which is an exponential smoothing method using weighted averages
of past observations with exponentially decaying weights as the observations get older together with
Gaussian additive errors (E) modeling trend (T) and seasonality (S) effects separately.

We evaluate the model on the datasets detailed above and follow the recommendations of the M4
competition [34] regarding forecasting performance metrics. Thus, we also report the mean scale
interval score [15] (MSIS5) for a 95% prediction interval, the 50-th and 90-th quantile percentile loss
(QL50 and QL90 respectively), as well as the CRPS score. The point-forecasting performance of
models is measured by the normalized root mean square error (NRMSE), the mean absolute scaled
error (MASE) [24], and the symmetric mean absolute percentage error (sMAPE) [33]. For pointwise
metrics, we use sampled medians with the exception of NRMSE, where we take the mean over
our prediction samples. The results of our extensive experiments are detailed in Table 1. Note that
the VQ-AR is essentially compressing the time series histories to discrete tokens which could have
applications in edge-computing, by trading off performance with space.

The M5 competition was a twin competition measuring both point forecasting accuracy [35] as well
as probabilistic uncertainty [36] and the top solutions were dominated by tree-based methods. In fact,
the winning solution to the accuracy benchmark is based on an ensemble of 220 gradient boosting
tree (GBT) models while the winner of the uncertainty challenge is an ensemble of 126 GBTs with
hundreds of hand-crafted features. In Table 2 we provide a comparison between tree-based methods
and neural methods. As detailed in [25] it is still an open problem to improve deep-learning models
with respect to tree-based methods and as can be seen, the VQ-VR method provides competitive
metrics via a simple single model without any extensive feature engineering.

4.2 Robustness

We check the robustness of VQ-AR with respect to the noise added to the context window inputs (but
not covariates) when forecasting. Our hypothesis is that due to the quantization nature of the method,
the added noise (up to a certain level) will correspond to the same codebook vector and thus this

5http://www.unic.ac.cy/test/wp-content/uploads/sites/2/2018/09/

M4-Competitors-Guide.pdf

6

Table 1: Comparison metrics using different methods: SQF-RNN with 50 knots, DeepAR and VQ-AR
with Student-T (-t), Negative Binomial (-nb) or IQN (-iqn) emission heads, ETS, MQCNN, and
IQN-RNN on the datasets.

Dataset

Method

CRPS QL50 QL90 MSIS NRMSE sMAPE MASE

Exchange

Solar

Electricity

Traffic

Taxi

Wikipedia

M5

E-Commerce

SQF-RNN-50
DeepAR-t
ETS
IQN-RNN
MQCNN
VQ-AR-t

SQF-RNN-50
DeepAR-t
ETS
IQN-RNN
MQCNN
VQ-AR-iqn

SQF-RNN-50
DeepAR-t
ETS
IQN-RNN
MQCNN
VQ-AR-t

SQF-RNN-50
DeepAR-t
ETS
IQN-RNN
MQCNN
VQ-AR-t

SQF-RNN-50
DeepAR-nb
ETS
IQN-RNN
MQCNN
VQ-AR-nb

SQF-RNN-50
DeepAR-nb
DeepAR-t
ETS
IQN-RNN
MQCNN
VQ-AR-iqn

SQF-RNN-50
DeepAR-nb
ETS
IQN-RNN
MQCNN
VQ-AR-nb

SQF-RNN-50
DeepAR-nb
ETS
IQN-RNN
MQCNN
VQ-AR-iqn

0.010
0.012
0.008
0.007
0.015
0.010

0.330
0.418
0.646
0.373
0.928
0.320

0.078
0.062
0.076
0.060
0.129
0.054

0.153
0.172
0.373
0.139
1.220
0.138

0.286
0.299
1.059
0.295
1.262
0.286

0.283
0.321
0.235
0.788
0.221
0.398
0.231

0.558
0.539
0.838
0.539
0.574
0.527

0.545
0.531
2.605
0.534
0.649
0.483

14.15
69.29
15.89
17.37
60.04
18.10

5.65
7.33
18.55
5.99
73.58
5.64

8.66
6.79
9.99
8.74
30.54
5.88

8.40
8.02
17.67
7.11
116.69
7.79

5.53
5.44
12.24
6.51
48.61
5.43

23.71
26.48
23.77
61.68
21.78
38.79
22.09

8.26
8.01
23.67
8.26
8.04
7.20

0.020
0.022
0.015
0.014
0.026
0.019

0.929
1.072
1.112
1.037
1.920
0.885

0.632
0.687
0.838
0.543
1.230
0.653

0.401
0.472
0.647
0.433
0.723
0.409

0.570
0.610
2.147
0.583
2.645
0.572

2.24
2.354
2.15
3.261
2.102
2.202
2.106

1.634
1.547
2.560
1.511
1.775
1.628

24.82
38.12
117.50
32.92
27.56
22.33

8.492
8.160
31.236
7.588
9.336
6.702

0.006
0.007
0.005
0.004
0.011
0.007

0.175
0.254
0.383
0.165
1.535
0.174

0.044
0.046
0.050
0.040
0.132
0.036

0.117
0.117
0.287
0.117
2.005
0.113

0.188
0.203
0.617
0.201
0.488
0.193

0.321
0.361
0.267
0.836
0.251
0.327
0.252

0.466
0.469
0.696
0.475
0.497
0.457

0.648
0.638
2.457
0.590
0.712
0.555

0.013
0.016
0.010
0.010
0.016
0.013

0.431
0.543
0.661
0.491
0.960
0.414

0.097
0.078
0.100
0.074
0.148
0.068

0.186
0.216
0.386
0.168
0.563
0.164

0.362
0.379
1.297
0.370
1.451
0.362

0.328
0.383
0.27
0.440
0.254
0.453
0.266

0.708
0.679
1.051
0.677
0.725
0.694

0.627
0.612
2.991
0.627
0.751
0.562

7

0.013
0.030
0.011
0.013
0.045
0.015

1.342
1.393
1.546
1.356
1.838
1.346

0.144
0.117
0.156
0.138
0.240
0.107

0.243
0.244
0.489
0.171
0.636
0.185

0.609
0.582
1.159
0.629
0.912
0.570

0.261
0.327
0.219
0.301
0.193
0.379
0.208

1.556
1.550
1.560
1.602
1.616
1.535

1.653
1.409
1.791
1.650
1.677
1.629

1.800
9.980
1.517
3.041
5.440
2.658

1.004
1.275
1.938
1.15
2.248
0.969

1.051
0.849
1.247
0.897
2.000
0.717

0.76
0.89
1.543
0.656
2.712
0.641

0.741
0.771
1.552
0.758
3.041
0.741

1.44
1.852
1.295
2.214
1.214
2.336
1.261

0.912
0.915
3.161
0.898
0.921
0.895

1.246
1.071
10.270
1.076
1.325
1.074

Table 2: Weighted Quantile Loss (lower is better) for M5 test set predictions using different methods
for speciﬁed quantiles and their average.

Quantile

M5
winner

M5
runner-up

DeepAR-t

DeepAR-nb

MQCNN

LSF

VQ-AR-nb

0.005
0.025
0.165
0.25
0.50
0.75
0.835
0.975
0.995

Mean

0.010
0.050
0.312
0.444
0.698
0.687
0.585
0.182
0.055

0.336

0.042
0.086
0.337
0.461
0.690
0.722
0.598
0.196
0.076

0.357

0.023
0.075
0.319
0.458
0.766
0.821
0.754
0.430
0.310

0.440

0.016
0.061
0.319
0.450
0.705
0.699
0.600
0.202
0.074

0.347

0.010
0.051
0.320
0.451
0.712
0.709
0.609
0.213
0.084

0.012
0.054
0.316
0.451
0.724
0.728
0.607
0.218
0.086

0.351

0.355

0.013
0.057
0.316
0.445
0.697
0.690
0.591
0.189
0.063

0.340

method would be robust to this, as opposed to for example DeepAR. However, too much noise could
also cause the encoder to ﬂip to nearby codebook vectors, and cause the predictions to be completely
off since there is no semantic meaning in nearby codebook vectors that is explicitly built into VQ-AR.

For some noise level l = {0.2, 0.4, . . . , 1.0} we
calculate the standard deviation of the context
window target σcontext and add to each time point
noise sampled from N (0, l × σcontext) and then
pass this together with the covariates to the ap-
propriate RNN.

Table 3 details the metrics for different levels
of noise. As we can see with a small amount
of noise the VQ-AR method is more robust than
IQN-RNN, however as we increase the noise the
selected codebook vectors become too different
leading to worse performance than with an RNN
based method with smooth representations. The
MSIS metric which measures the upper 95% and
lower 5% probability interval however is more
robust.

4.3 Generalization

Table 3: Comparison of test set performance of
IQN-RNN and VQ-AR on Solar dataset with differ-
ent levels of Gaussian noise added to the context
window input at the start of inference. The best
metric for each noise level l is highlighted in bold.

Method

Level CRPS MSIS

sMAPE MASE

IQN-RNN

VQ-AR-iqn

0.0
0.2
0.4
0.6
0.8
1.0

0.0
0.2
0.4
0.6
0.8
1.0

0.373
0.397
0.431
0.479
0.536
0.610

0.320
0.388
0.466
0.540
0.623
0.712

5.99
8.22
11.97
17.49
22.71
29.42

5.64
8.95
12.76
15.25
17.66
21.18

1.356
1.358
1.364
1.370
1.382
1.399

1.346
1.362
1.384
1.410
1.429
1.451

1.115
1.182
1.234
1.324
1.453
1.612

0.969
1.144
1.340
1.564
1.801
2.042

The VQ-AR method is essentially an information
bottleneck that serves as a form of regulariza-
tion. Thus our hypothesis is that the learned
cookbook representations {zj} would need to
be somewhat universal time series representations in comparison to methods that can potentially
learn continuous representations without such a constraint. In order to test this we measure the
zero-shot prediction performance of VQ-AR against DeepAR by ﬁrst training these two models on the
Electricity dataset (without any categorical time series covariates) and then testing the trained
model’s performance on the test set of Solar and Traffic.

Table 4 highlights the prediction metrics on unseen datasets and as can be noted, the representations
learned as discrete entities tend to perform better in the zero-shot setting especially since the datasets
come from dissimilar domains.

4.4 Ablation

For our ablation study, we wish to test the performance of the model with respect to the number
of codebook vectors J. In this experiment for the M5 and E-Commerce datasets we train and test
the performance of VQ-AR using J = {2, 4, . . . , 512}. We record the metrics in Figure 2 and to our

8

(a)

(b)

Figure 2: Test set CRPS of VQ-AR-iqn on (a) M5 and (b) E-Commerce datasets with different number
of codebook vectors (J). Surprisingly, the decoding RNN can still give valid predictions given some
sequence of just two codebook vectors.

surprise, we see that we get decent forecasts using only two codebook vectors for all the datasets
considered (and not only for the ones in the ﬁgure). More codebook vectors tend to give better
performance. As J → ∞ we approach the DeepAR method albeit with more RNN layers.

5 Related Work

Table 4: Comparison of test set performance of
DeepAR and VQ-AR trained on the Electricity
dataset and then tested on Solar and Traffic in
a zero-shot setting. The best metric for each test
dataset is given in bold.

The use of Vector Quantization (VQ) in time se-
ries forecasting has not been explored, as far as
we are aware. However, generative models like
VQ-VAE have been used extensively for sequen-
tial modeling problems like for videos [43] and
audio/speech [12, 55, 2] generation. Typically
these models are trained in a two-stage process
where a VQ-VAE model is ﬁrst trained and then
a sequential generative model is trained on top
of the codebook vectors. The VQ-Wav2Vec [2]
uses a causal convolution to ﬁrst learn a repre-
sentation of an audio signal some steps into the
future for further downstream task. This model
is not autoregressive, takes bounded audio inputs only and is not able to be used as a forecaster which
would need to account for the inductive bias of scale and additional covariates.

Solar
Traffic

Solar
Traffic

sMAPE MASE

CRPS MSIS

1.432
1.270

1.413
0.972

16.60
12.91

0.522
0.267

1.441
0.393

1.435
0.300

0.511
0.214

14.69
10.52

DeepAR-t

VQ-AR-t

Method

Test set

The SOM-VAE [13] model is related in that it uses VQ to learn interpretable discrete representation
of sequential data for clustering. Also in the paper [42], the authors discretize the inputs to time
series models via different binning transformations, as a way to mitigate the scaling issue discussed
in section 3.4, and investigate performance given this input transformation on different architectures.

Neural forecasting methods [5] are related with the caveat that all of them, as far as we are aware, can
be thought of as some component that learns a representation of the context window in a continuous
fashion together with an emission component, be it point forecasting or probabilistic. CNN [3] based
methods can typically be replaced by RNNs or causal Transformers [52] and most works try to
introduce some time series or problem speciﬁc inductive bias into the architectural choices [32] or
emission models [44] or inputs [42] rather than explicitly on the representations themselves.

6 Summary and Discussion

We have presented VQ-AR a conceptually simple and novel yet powerful autoregressive time series
forecasting method based on learning discrete historic representations of time series histories in order
to forecast, trained in an end-to-end fashion. We have shown not only does it perform better or similar
to the competitive methods considered, but is also able to achieve good performance in comparison
to ensembles of hundreds of tree-based models which typically dominate forecasting benchmarks, all
via a single model. The architectural principle applied in this method can be adapted for multivariate

9

248163264128256512The number of codebook vectors (J)0.5250.5300.5350.5400.545CRPS0.5410.5300.5320.5290.5300.5300.5300.5270.530M5248163264128256512The number of codebook vectors (J)0.480.500.520.54CRPS0.5400.5170.5090.5310.5060.5240.5100.5050.483E-Commerceforecasting and the underlying RNN based encoder-decoder can be replaced by a Transformer if so
desired.

For future work, we would like to test the inductive bias introduced in this work for the multivariate
setting and also incorporate it within the Transformer architecture especially for very long time series
sequences. As mentioned in the appendix A, the gradient estimator considered can also be replaced
by its smooth-relaxation variant or by methods with less bias to help improve the VQ training.

Finally, we have also highlighted a critique of VQ-AR especially in terms of its robustness to noisy
inputs, and in the future, we would like to investigate how to mitigate this potential issue.

Acknowledgments

We wish to acknowledge and thank the authors and contributors of the many open source libraries
that were used in this work, in particular: GluonTS [1], NumPy [17], Pandas [38], Matplotlib [21],
Jupyter [29] and PyTorch [40].

References

[1] Alexander Alexandrov, Konstantinos Benidis, Michael Bohlke-Schneider, Valentin Flunkert, Jan
Gasthaus, Tim Januschowski, Danielle C. Maddix, Syama Rangapuram, David Salinas, Jasper
Schulz, Lorenzo Stella, Ali Caner Türkmen, and Yuyang Wang. GluonTS: Probabilistic and
Neural Time Series Modeling in Python. Journal of Machine Learning Research, 21(116):1–6,
2020.

[2] Alexei Baevski, Steffen Schneider, and Michael Auli. vq-wav2vec: Self-supervised learning
of discrete speech representations. In International Conference on Learning Representations,
2020.

[3] Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolu-

tional and recurrent networks for sequence modeling, 2018.

[4] Yoshua Bengio, Nicholas Léonard, and Aaron C. Courville. Estimating or propagating gradients
through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.

[5] Konstantinos Benidis, Syama Sundar Rangapuram, Valentin Flunkert, Bernie Wang, Danielle
Maddix, Caner Turkmen, Jan Gasthaus, Michael Bohlke-Schneider, David Salinas, Lorenzo
Stella, Laurent Callot, and Tim Januschowski. Neural forecasting: Introduction and literature
overview, 2020.

[6] Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006.

[7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,
Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya
Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle,
M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information
Processing Systems, volume 33, pages 1877–1901. Curran Associates, Inc., 2020.

[8] Kyunghyun Cho, Bart van Merriënboer, Dzmitry Bahdanau, and Yoshua Bengio. On the
properties of neural machine translation: Encoder–decoder approaches. In Proceedings of
SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages
103–111, Doha, Qatar, October 2014. Association for Computational Linguistics.

[9] Will Dabney, Georg Ostrovski, David Silver, and Remi Munos. Implicit Quantile Networks for
Distributional Reinforcement Learning. In Jennifer Dy and Andreas Krause, editors, Proceed-
ings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of
Machine Learning Research, pages 1096–1105, Stockholmsmässan, Stockholm Sweden, 2018.
PMLR.

10

[10] Emmanuel de Bézenac, Syama Sundar Rangapuram, Konstantinos Benidis, Michael Bohlke-
Schneider, Richard Kurle, Lorenzo Stella, Hilaf Hasson, Patrick Gallinari, and Tim
Januschowski. Normalizing kalman ﬁlters for multivariate time series analysis. In H. Larochelle,
M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information
Processing Systems, volume 33, pages 2995–3007. Curran Associates, Inc., 2020.

[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of
deep bidirectional transformers for language understanding. In Proceedings of the 2019 Confer-
ence of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis,
Minnesota, June 2019. Association for Computational Linguistics.

[12] Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya

Sutskever. Jukebox: A generative model for music. CoRR, abs/2005.00341, 2020.

[13] Vincent Fortuin, Matthias Hüser, Francesco Locatello, Heiko Strathmann, and Gunnar Rätsch.
In ICLR (Poster).

Som-vae: Interpretable discrete representation learning on time series.
OpenReview.net, 2019.

[14] Jan Gasthaus, Konstantinos Benidis, Yuyang Wang, Syama Sundar Rangapuram, David Salinas,
Valentin Flunkert, and Tim Januschowski. Probabilistic Forecasting with Spline Quantile
Function RNNs. In Kamalika Chaudhuri and Masashi Sugiyama, editors, Proceedings of the
22nd International Conference on Artiﬁcial Intelligence and Statistics AISTATS 2019, Naha,
Okinawa, Japan, volume 89 of Proceedings of Machine Learning Research, pages 1901–1910.
PMLR, 2019.

[15] Tilmann Gneiting and Adrian E Raftery. Strictly proper scoring rules, prediction, and estimation.

Journal of the American Statistical Association, 102(477):359–378, 2007.

[16] Adèle Gouttes, Kashif Rasul, Mateusz Koren, Johannes Stephan, and Toﬁgh Naghibi. Proba-
bilistic Time Series Forecasting with Implicit Quantile Networks. In Time Series Workshop @
ICML 2021, 2021.

[17] Charles R. Harris, K. Jarrod Millman, St’efan J. van der Walt, Ralf Gommers, Pauli Virtanen,
David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert
Kern, Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane,
Jaime Fern’andez del R’ıo, Mark Wiebe, Pearu Peterson, Pierre G’erard-Marchant, Kevin
Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E.
Oliphant. Array programming with NumPy. Nature, 585(7825):357–362, September 2020.

[18] Hilaf Hasson, Bernie Wang, Tim Januschowski, and Jan Gasthaus. Probabilistic forecasting: A
level-set approach. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors,
Advances in Neural Information Processing Systems, 2021.

[19] Geoffrey Hinton, Nitish Srivastava, Kevin Swersky, Tijmen Tieleman, and Abdel-rahman
Mohamed. Neural networks for machine learning: Lecture 15b deep autoencoders. https:
//www.cs.toronto.edu/~hinton/coursera_lectures.html, 2012.

[20] S. Hochreiter and J. Schmidhuber. Long Short-Term Memory. Neural Computation, 9(8):1735–

1780, November 1997.

[21] J. D. Hunter. Matplotlib: A 2D graphics environment. Computing in Science & Engineering,

9(3):90–95, 2007.

[22] R.J. Hyndman and G. Athanasopoulos. Forecasting: Principles and practice. OTexts, 2021.

[23] Rob J. Hyndman and Yeasmin Khandakar. Automatic time series forecasting: The forecast

package for R. J. Stat. Soft., 27(3):1–22, 2008.

[24] Rob J. Hyndman and Anne B. Koehler. Another look at measures of forecast accuracy. Interna-

tional Journal of Forecasting, 22(4):679–688, 2006.

[25] Tim Januschowski, Yuyang Wang, Kari Torkkola, Timo Erkkilä, Hilaf Hasson, and Jan Gasthaus.

Forecasting with trees. International Journal of Forecasting, 2021.

11

[26] Alexander Jordan, Fabian Krüger, and Sebastian Lerch. Evaluating Probabilistic Forecasts with

scoringRules. Journal of Statistical Software, Articles, 90(12):1–37, 2019.

[27] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua
Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.

[28] Diederik P. Kingma and Max Welling. An Introduction to Variational Autoencoders. Founda-

tions and Trends in Machine Learning, 12(4):307–392, 2019.

[29] Thomas Kluyver, Benjamin Ragan-Kelley, Fernando Pérez, Brian Granger, Matthias Bussonnier,
Jonathan Frederic, Kyle Kelley, Jessica Hamrick, Jason Grout, Sylvain Corlay, Paul Ivanov,
Damián Avila, Saﬁa Abdalla, and Carol Willing. Jupyter notebooks – a publishing format for
reproducible computational workﬂows. In F. Loizides and B. Schmidt, editors, Positioning and
Power in Academic Publishing: Players, Agents and Agendas, pages 87–90. IOS Press, 2016.

[30] Alireza Koochali, Andreas Dengel, and Sheraz Ahmed. If you like it, gan it—probabilistic

multivariate times series forecast with gan. Engineering Proceedings, 5(1), 2021.

[31] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling Long- and Short-
Term Temporal Patterns with Deep Neural Networks. In The 41st International ACM SIGIR
Conference on Research & Development in Information Retrieval, SIGIR ’18, pages 95–104,
New York, NY, USA, 2018. ACM.

[32] Bryan Lim, Sercan Ö. Arık, Nicolas Loeff, and Tomas Pﬁster. Temporal fusion transformers
for interpretable multi-horizon time series forecasting. International Journal of Forecasting,
37(4):1748–1764, 2021.

[33] Spyros Makridakis. Accuracy measures: theoretical and practical concerns. International

Journal of Forecasting, 9(4):527–529, 1993.

[34] Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. The M4 Competition:
100,000 time series and 61 forecasting methods. International Journal of Forecasting, 36(1):54–
74, 2020. M4 Competition.

[35] Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. The m5 competition:

Background, organization, and implementation. International Journal of Forecasting, 2021.

[36] Spyros Makridakis, Evangelos Spiliotis, Vassilios Assimakopoulos, Zhi Chen, Anil Gaba,
Ilia Tsetlin, and Robert L. Winkler. The m5 uncertainty competition: Results, ﬁndings and
conclusions. International Journal of Forecasting, 2021.

[37] James E. Matheson and Robert L. Winkler. Scoring Rules for Continuous Probability Distribu-

tions. Management Science, 22(10):1087–1096, 1976.

[38] The Pandas development team. pandas-dev/pandas: Pandas, February 2020.

[39] Youngsuk Park, Danielle Maddix, François-Xavier Aubet, Kelvin Kan, Jan Gasthaus, and
Yuyang Wang. Learning quantile functions without quantile crossing for distribution-free time
series forecasting. In Gustau Camps-Valls, Francisco J. R. Ruiz, and Isabel Valera, editors,
Proceedings of The 25th International Conference on Artiﬁcial Intelligence and Statistics,
volume 151 of Proceedings of Machine Learning Research, pages 8127–8150. PMLR, 28–30
Mar 2022.

[40] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-
performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alché
Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32,
pages 8026–8037. Curran Associates, Inc., 2019.

12

[41] Max B. Paulus, Chris J. Maddison, and Andreas Krause. Rao-blackwellizing the straight-through
gumbel-softmax gradient estimator. In International Conference on Learning Representations,
2021.

[42] Stephan Rabanser, Tim Januschowski, Valentin Flunkert, David Salinas, and Jan Gasthaus. The
effectiveness of discretization in forecasting: An empirical study on neural time series models.
In 6th Workshop on Mining and Learning from Time Series, 2020.

[43] Ruslan Rakhimov, Denis Volkhonskiy, Alexey Artemov, Denis Zorin, and Evgeny Burnaev.
Latent video transformer. In Giovanni Maria Farinella, Petia Radeva, José Braz, and Kadi
Bouatouch, editors, VISIGRAPP (5: VISAPP), pages 101–112. SCITEPRESS, 2021.

[44] Syama Sundar Rangapuram, Matthias W Seeger, Jan Gasthaus, Lorenzo Stella, Yuyang Wang,
and Tim Januschowski. Deep state space models for time series forecasting. In S. Bengio,
H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in
Neural Information Processing Systems 31, pages 7796–7805. Curran Associates, Inc., 2018.

[45] Kashif Rasul, Calvin Seward, Ingmar Schuster, and Roland Vollgraf. Autoregressive denoising
diffusion models for multivariate probabilistic time series forecasting. In Marina Meila and
Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning,
volume 139 of Proceedings of Machine Learning Research, pages 8857–8868. PMLR, 18–24
Jul 2021.

[46] Kashif Rasul, Abdul-Saboor Sheikh, Ingmar Schuster, Urs Bergmann, and Roland Vollgraf.
Multivariate Probabilistic Time Series Forecasting via Conditioned Normalizing Flows. In
International Conference on Learning Representations 2021 (Conference Track), 2021.

[47] David Salinas, Michael Bohlke-Schneider, Laurent Callot, Roberto Medico, and Jan Gasthaus.
High-dimensional multivariate forecasting with low-rank Gaussian Copula Processes.
In
H. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alché Buc, E. Fox, and R. Garnett, editors,
Advances in Neural Information Processing Systems 32, pages 6824–6834. Curran Associates,
Inc., 2019.

[48] David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. DeepAR: Probabilistic
forecasting with autoregressive recurrent networks. International Journal of Forecasting, 2019.

[49] Kamil˙e Stankeviˇci¯ut˙e, Ahmed Alaa, and Mihaela van der Schaar. Conformal time-series
In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors,

forecasting.
Advances in Neural Information Processing Systems, 2021.

[50] Ruey S. Tsay. Multivariate Time Series Analysis: With R and Financial Applications. Wiley

Series in Probability and Statistics. Wiley, 2014.

[51] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation
learning. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and
R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran
Associates, Inc., 2017.

[52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Ł ukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In I. Guyon, U.V. Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural
Information Processing Systems 30, pages 5998–6008. Curran Associates, Inc., 2017.

[53] Ruofeng Wen, Kari Torkkola, and Balakrishnan Narayanaswamy. A multi-horizon quantile
recurrent forecaster. In Vitaly Kuznetsov, Oren Anava, Scott Yang, and Azadeh Khaleghi,
editors, NIPS 2017 Time Series Workshop, 2017.

[54] Chen Xu and Yao Xie. Conformal prediction interval for dynamic time-series. In Marina
Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine
Learning, volume 139 of Proceedings of Machine Learning Research, pages 11559–11569.
PMLR, 18–24 Jul 2021.

[55] Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi.
Soundstream: An end-to-end neural audio codec. IEEE/ACM Transactions on Audio, Speech,
and Language Processing, 2021.

13

Figure 3: A taxonomy of different gradient estimator methods, adapted from [41], ordered chronolog-
ically with the method used in this paper marked with a tick.

Table 5: Number of time series, domain, frequency, total training time steps and prediction length
properties of the training datasets used in the experiments.

Dataset

Exchange
Solar
Elec.
Traffic
Taxi
E-Commerce
Wikipedia
M5

D

Dom.
R≥0
8
R≥0
137
R≥0
320
(0, 1)
862
N≥0
1, 214
N≥0
2, 731
N≥0
9, 535
30, 490 N≥0

Freq.

Time step

Pred. len.

day
hour
hour
hour
30-min
day
day
day

6, 071
7, 009
15, 782
14, 036
1, 488
887
762
1, 941

30
24
24
24
24
28
30
28

A Gradient Estimators

Since the sampling and quantization in (4) are not differentiable the VQ-VAE paper applies a Straight-
Through gradient estimator which propagates the gradients with respect to the codebook vectors
through to the output of the encoding network. If the codebook vector and encoding representation
are close then this is an adequate approximation but there is no bound on the bias of the gradients
with this scheme and thus it falls under the biased-low variance taxonomy [41] of gradient estimators
as shown in Figure 3.

B Experiment Details

To help reduce confusion we also collect the different hyperparameters in a single location with their
description and default values listed in Table 6.

14

GradientEstimatorssingle lossevaluationmultiple lossevaluationsmodified forward computationunmodified forward computationunbiased high variancebiased low varianceGumbel-Softmax(Maddison et al., 2016) (Jang et al., 2016)RELAX (Garthwohl et al., 2018)NVIL (Mnih & Gregor, 2014)REINFORCE (Glynn et al., 1990) (Williams et al., 1992)REBAR (Tucker et al., 2017)VIMCO (Mnih & Rezende, 2016)Straight-ThroughGumbel-Softmax(Jang et al., 2016)Straight-Through (Hinton, 2012)(Bengio et al., 2013)FouST(Pervez et al., 2020)Gumbel-Rao Monte Carlo (Paulus et al., 2021)MuProp(Gu et al., 2016)✔Table 6: Glossary of the main hyperparameters, their description and default values unless speciﬁed.

Parameter Description

D
P
C
F

E

H
J
β
Q
S

Number of time series in a dataset
Prediction horizon length
Context window length
Size of covariate vector
Encoding RNN’s hidden size and
size of codebook vectors
Decoding RNN’s hidden size
Number of codebook vectors
Commitment cost
Threshold to replace codebook vector
Number of samples

Default

P × 6

64

40
128
0.25
2
100

For our experiments we selected the default options for the models as implemented in the GluonTS
[1] library. In particular for VQ-AR, we modiﬁed the base implementation of DeepAR and thus
kept the parameters of the models to be similar where applicable. We trained each model for
max_epochs=50 with the batch_size=256 and a default learning_rate=1e-3. For the evalua-
tion we used quantiles=[0.1, 0.2, ..., 0.9] to calculate the CRPS metric.

15

