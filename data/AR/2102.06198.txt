1

Millimeter Wave MIMO based Depth Maps

for Wireless Virtual and Augmented Reality

Abdelrahman Taha1, Qi Qu2, Sam Alex2, Ping Wang2, William L. Abbott2, and

Ahmed Alkhateeb1

1 Arizona State University, {a.taha, alkhateeb}@asu.edu

2 Facebook, Inc., {qqu, sampalex, pingwang, billabbott}@fb.com

Abstract

Augmented and virtual reality systems (AR/VR) are rapidly becoming key components of the

wireless landscape. For immersive AR/VR experience, these devices should be able to construct accurate

depth perception of the surrounding environment. Current AR/VR devices rely heavily on using RGB-

D depth cameras to achieve this goal. The performance of these depth cameras, however, has clear

limitations in several scenarios, such as the cases with shiny objects, dark surfaces, and abrupt color

transition among other limitations. In this paper, we propose a novel solution for AR/VR depth map

construction using mmWave MIMO communication transceivers. This is motivated by the deployment

of advanced mmWave communication systems in future AR/VR devices for meeting the high data rate

demands and by the interesting propagation characteristics of mmWave signals. Accounting for the

constraints on these systems, we develop a comprehensive framework for constructing accurate and

high-resolution depth maps using mmWave systems. In this framework, we developed new sensing

beamforming codebook approaches that are speciﬁc for the depth map construction objective. Using

these codebooks, and leveraging tools from successive interference cancellation, we develop a joint beam

processing approach that can construct high-resolution depth maps using practical mmWave antenna

arrays. Extensive simulation results highlight the potential of the proposed solution in building accurate

depth maps. Further, these simulations show the promising gains of mmWave based depth perception

compared to RGB-based approaches in several important use cases.

Abdelrahman Taha and Ahmed Alkhateeb are with the School of Electrical, Computer and Energy Engineering, Arizona State

University. Qi Qu, Sam Alex, Ping Wang, and Bill Abbott are with Facebook, Inc. This material is based upon work supported

by Facebook, Inc.

1
2
0
2

b
e
F
3
1

]
P
S
.
s
s
e
e
[

2
v
8
9
1
6
0
.
2
0
1
2
:
v
i
X
r
a

 
 
 
 
 
 
2

I. INTRODUCTION

Wireless augmented and virtual reality (AR/VR) applications are recently attracting increasing

interest. Realizing wireless AR/VR in practice can open the door for a wide range of interesting

applications and use cases. Enabling Immersive AR/VR experience, however, requires high

resolution and accurate depth perception. This can potentially allow the wireless AR/VR users

to move freely within their indoor or outdoor environment. Current depth perception approaches

for AR/VR systems rely mainly on RGB-D (depth) cameras for constructing the depth maps.

While RGB-D based depth map construction approaches can generally provide good accuracy,

they suffer from critical limitations in scenarios with bright shiny or transparent surfaces, dark

objects, and large rooms among others. These limitations stem from the fundamental properties

of the way visible light propagate and interact with the different surfaces.

In order to overcome these limitations, we propose to leverage mmWave systems and

signals for improving the depth map estimation accuracy. This is motivated by the interesting

characteristics of mmWave signals and by the note that mmWave systems will be deployed in

future AR/VR devices anyway for meeting the wireless communication requirements [1]. In

terms of the mmWave signal characteristics, the propagation of these signals is not affects

by the interference from the light sources which makes mmWave systems capable of detecting

bright and dark objects. Further, the mmWave diffuse scattering and specular reﬂection properties

could help in detecting transparent objects as well as rough surfaces. These aspects among

others motivate exploring the potential of leveraging mmWave transceivers for complementing

the RGB-D depth-maps in AR/VR systems, which is the focus of this paper.

A. Prior Work

Previous depth map construction approaches focused on leveraging: (i) monocular images

using RGB cameras [2], (ii) passive/active stereo images using either RGB-D depth cameras [3],

[4] or infrared (IR) stereo cameras [5], [6], and (iii) gated images using active gated imaging

cameras [7], [8]. In [2], a monocular depth estimation approach capable of capturing the object

boundaries is proposed. In [3], RGB images along with sparse depth samples, acquired from

depth cameras or computed via Simultaneous Localization and Mapping (SLAM) algorithms,

are used jointly to reconstruct the depth maps. An alternative approach for depth estimation

was proposed in [4], where a monocular structured-light camera — a calibrated stereo set-up

with one camera and one laser projector— is leveraged for estimating the disparity. As for the

3

active stereo systems, in [5], IR projected pattern from stereo IR cameras is utilized for depth

estimation through active stereo matching. The IR images are acquired from the Intel Realsense

camera [9]. Also, the IR pattern characteristics needed for active stereo matching are described

in [6]. In addition, high-resolution depth images can be achieved for far objects using active

gated imaging systems, as in [7], [8].

These depth map construction approaches [2]–[8], [10], however, have several important

limitations complications as follows. (i) First, these depth map construction approaches normally

fail to sense the depth for shiny, dark, transparent, and distant surfaces. While there are some

attempts in solving these challenges using IR stereo cameras [5] or excessive processing of

the RGB-D images [11], there is no complete and general solution yet to this problem. (ii)

Further, these IR and RGB-D based depth map construction algorithms suffer from a critical

limitation, which is the depth ambiguity for far objects/surfaces. The depths for distant surfaces

can not be resolved by the algorithms in [5], [11]. (iii) Another key challenge is the additional

bill of materials (BOM) cost incurred from integrating the IR stereo camera systems in the

wireless AR/VR device architectures. On the contrary, the existing mmWave systems in the

wireless AR/VR device architectures incurs no additional BOM cost when leveraged for depth

map estimation purposes jointly with the primary purpose of wireless communications. (iv) The

ﬁeld of view coverage is also a main challenge. The depth map coverage is limited by the camera

ﬁeld of view. The camera ﬁeld of view is constrained by the camera lens and by the light sensor.

The ﬁeld of view in mmWave MIMO systems, however, is constrained by the array radiation

pattern, as will be explained in Section VI. By contrast, the typical ﬁeld of view in mmWave

MIMO systems can be larger than the typical camera ﬁeld of view.

These challenges motivate the research for other technologies to complement the RGB-D

cameras in accurately sensing the VR/AR environment. One promising technology for this goal

is employing wireless millimeter wave (mmWave) systems. Since mmWave antenna arrays will

be used to satisfy the communication high data rate demands of wireless VR/AR, it is interesting

to investigate if they could also be useful for VR/AR-relevant sensing functions, such as depth

estimation. Initial studies for using mmWave communication arrays for radar and sensing were

presented in [12], [13]. These studies, however, focused only on the ranging problem (of one

or multiple targets), not on the depth map construction problem. Other mmWave sensing and

tracking work that was not restricted to communications hardware was presented in [14], [15].

The research in [14], [15], though, targeted tracking a single object in a small distance, and

4

cannot be directly applied to depth estimation of surrounding surfaces in VR/AR. Further, the

work in [12]–[15], did not study the trade-offs between estimation accuracy and different system

parameters, such as number of antennas and adopted bandwidth, and did not compare between

the system performance under transceiver architectures constraints, such as those imposed on

the analog phased-array transceiver architectures. By contrast, interesting research challenges

are accompanying the mmWave MIMO based scene depth map construction framework ranging

from beam codebook design challenges to scene depth estimation challenges. These challenges

will be addressed in this work and will be explained in detail in Section V.

B. Contribution

In this paper, we consider the mmWave MIMO based depth map construction problem for

AR/VR systems, adopting mmWave communication hardware and frame structure. The contri-

butions of this paper can be summarized as follows.

• mmWave MIMO depth map construction framework: We formulate the mmWave MIMO

depth map construction problem and propose a general framework for building depth maps

under the constraints imposed by mmWave communication hardware and frame structure.

• A design for depth-map suitable sensing beamforming codebook: We deﬁne the charac-

teristics of the desirable mmWave sensing beamforming codebook for efﬁcient depth map

construction and develop a codebook construction approach that meets these characteristics.

• High-resolution depth map construction approach: Given the designed beamforming code-

book, we develop a novel signal processing approach for jointly processing the signals

received by the sensing beams and building high-resolution depth maps.

The proposed solution is extensively evaluated using accurate ray-tracing channels generated from

Wireless InSite [16], and ground truth depth images generated from Blender [17]. The simulation

results show the promise of mmWave MIMO sensing in becoming a viable depth estimation

solution for communication-constrained sensing systems, either as a standalone approach or as

an integrated approach with RGB-D depth cameras. These simulation results can be of great

usefulness for various applications; they can be generally applied to AR/VR devices, smart

home devices, or auto drive devices.

Notation: We use the following notation throughout this paper: A is a matrix, a is a vector,

a is a scalar, and A is a set. (cid:107)a(cid:107)p is the p-norm of a. |A| is the determinant of A, (cid:107)A(cid:107)F

is its Frobenius norm, whereas AT , AH, A∗, A−1, A† are its transpose, Hermitian (conjugate

5

Fig. 1: The considered setup where the mmWave communication system, deployed at the AR/VR device, is jointly

leveraged for sensing and depth map construction. This ﬁgure is generated using Blender [17] with 3D models

downloaded from [18]–[21].

transpose), conjugate, inverse, and pseudo-inverse respectively. Ap is the pth element of the set
A. [A]r,: and [A]:,c are the rth row and cth column of the matrix A, respectively. diag(a) is a

diagonal matrix with the entries of a on its diagonal. I is the identity matrix. 1N and 0N are

the N -dimensional all-ones and all-zeros vector respectively. A ⊗ B is the Kronecker product of

A and B, A ◦ B is their Khatri-Rao product, and A (cid:12) B is their Hadamard product. N (m, R)
is a complex Gaussian random vector with mean m and covariance R. E [·] is used to denote

expectation. vec(A) is a vector whose elements are the stacked columns of matrix A.

II. SYSTEM AND CHANNEL MODELS

In this section, the system model for the adopted communication-constrained sensing frame-

work is ﬁrst formulated, followed by of the characterization of the adopted channel model.

A. System Model

In this paper, we propose to reuse the same AR/VR mmWave communication system/circuits to

do the sensing and depth map construction, as shown in Fig. 1. Hence, we adopt a sensing model

that accounts for the mmWave communication system/circuit constraints. This communication-

constrained sensing model consists of a transmitter and a receiver; both are connected through a

self-isolation circuitry to a shared N antenna array, as depicted in Fig. 2. This type of operation

Communication LinkWall mounted transceiverAR/VR device6

Fig. 2: A block diagram of the communication-constrained sensing model is illustrated. The sensing framework,

Π, consists of (a) the beam codebook design P and (b) the post-processing design g (., P), to estimate the scene

depth map (cid:98)D. The upper path represents the transmitter path while the lower path represents the receiver path.

is commonly referred to as MIMO in-band full-duplex operation [22]. We assume that the

transmitter and receiver chains are well-isolated by an isolation circuitry to avoid any self-

interference. This assumption is reasonable with the recent developments of self-interference

systems. One example of these systems is the magnetic-free non-reciprocal circulators (i) based

on coupled-resonator loops [23] or (ii) based on CMOS circulators operating in the 28GHz

mmWave band [24]. Another example is the receiver with integrated magnetic-free non-reciprocal

circulator and baseband self-interference cancellation operating in the Sub-6 GHz band [25]. A

third example is the magnetic-free SOI CMOS circulator operating in the 60GHz mmWave

band [26]. Accounting for this self-interference, however, is an important direction for future

extensions.

Further, and for the sake of having low-cost and power consumption mmWave transceivers, we

adopt an analog-only architecture for the N -antenna array used for transmission and reception,

[27], [28], where the beamforming/combining is done in the analog domain using a network of

phase shifters. Next, we summarize the transmit and receive signal models.

Transmit Signal Model: We consider a wideband single-carrier waveform consisting of

multiple time frames. These frames are transmitted over an aggregated time interval of T seconds

during which the environment is assumed to be relatively static. This time interval is commonly

referred to as a coherent processing interval (CPI) [29]. Each frame consists of both data and

preamble sequences designed for the wireless communication function. The co-existing sensing

model also uses these preamble sequences to sense the environment and build the depth maps,

Depth MapRF chainRF chainBeamforming-combining pairsBeam codebookPassband ModulationPulse ShapingPassband DemodulationPreamble SymbolsPost-ProcessingAR/VR mmWaveTransmitterAR/VR mmWaveReceiverSelf-isolation circuitry7

as will be explained in detail in the following sections. This can be achieved by either splitting

the frames between sensing and communication or by designing the sensing and communication

beam training operations to share the same preamble sequences. Next, for ease of exposition,

we assume that M frames/preamble sequences are dedicated for sensing. If sm[n] denotes the
nth transmitted symbol at the mth frame, with E (cid:2)|sm[n]|2(cid:3) = 1, then the complex-baseband
representation of the transmit waveform can be written as [30]

a(t) =

(cid:112)

Es

M −1
(cid:88)

Nm−1
(cid:88)

m=0

n=0

sm[n] δ(t − nTS − mTF),

(1)

where Es represents the average energy per symbol, TS is the symbol time, and TF is the frame
duration. Nm is the number of symbols in the mth frame, which is divided into a preamble
sequence of length N p and a set of data symbols of length N d

m. Further, we assume that the
same preamble sequence sp[n], n ∈ {1, . . . , N p}, is transmitted in the ﬁrst N p symbols of each

frame. Note that for the sake of simplifying the transmit and receive signal representation, we

incorporated the transmit pulse shaping and receive ﬁltering functions into the channel model.
Finally, if a beamforming vector f ∈ CN ×1 is used to transmit the signal at the AR/VR device,

the complex-baseband representation of the transmitted signal can expressed as

x(t) = f a(t).

(2)

This transmitted signal will interact with the environment (through reﬂection, scattering, etc.)

and will be received back by the AR/VR device. Next, we describe the receive signal model.

Receive Signal Model: Let Gtar denote the number of targets/scatterers in the environment.

Then, focusing on the preamble sequence transmission/reception (i.e., the ﬁrst N p symbols of

each frame), the receive sensing signal of the mth frame can be written as

ym[n] =

Gtar(cid:88)

Ld−1
(cid:88)

g=1

d=0

(cid:112)

EswHHd,gf sp[n − d] + wHvm[n],

(3)

where w ∈ CN ×1 is the combining vector at the AR/VR, and vm[n] ∼ NC (0
¯
noise with variance σ2

nI) is the receive
n. Hd,g ∈ CN ×N , d ∈ {1, . . . , Ld−1}, is the delay-d channel matrix between
the transmission from and the reception by the AR/VR antenna array, which is described in the

, σ2

following subsection.

B. Channel Model

Given that the depth sensing problem highly relies on the accurate modeling of the surrounding

environment and its geometry, we adopt a geometric channel model in this work. More speciﬁ-

8

cally, we consider the extended Saleh-Valenzuela wideband geometric channel model [31]–[34].

Based on that, the gth target contribution in the delay-d channel, Hd,g, can be modeled as

Hd,g = (cid:112)Gg

Lray
(cid:88)

(cid:96)=1

α(cid:96)e−j2πfcτ(cid:96)p (dTs − τ(cid:96)) aR

(cid:0)φR

(cid:96),g, θR
(cid:96),g

(cid:1) aH

T

(cid:0)φT

(cid:96),g, θT
(cid:96),g

(cid:1) ,

(4)

where Lray is the number of channel clusters; each cluster is contributing with one ray of

(cid:96),g, θT

(cid:96),g, θR

arrival, φT

(cid:96),g and φR

complex channel coefﬁcient α(cid:96), time delay τ(cid:96), and azimuth/elevation angles of departure and
(cid:1) represent the transmit
(cid:0)φT

(cid:1) and aR
and receive array response vectors associated with the angles φT

(cid:96),g. The transmit
and receive pulse shaping signals are included within p(t) such that p(t) = pT(t) ∗ pR(t). The
path gain associated with the gth target is denoted by Gg and can be expressed as

(cid:96),g, θR
(cid:96),g
(cid:96),g and φR

(cid:96),g, respectively. aH
T

(cid:0)φR
(cid:96),g, θT

(cid:96),g, θT
(cid:96),g

(cid:96),g, θR

Gg =

GTGRλ2σRCS
(4π)3(ρg)2PL ,

g

(5)

where GT and GR are the transmitter and receiver gains, λ is the operating wavelength, PL is

the path loss exponent. Finally, ρg denotes the distance (range) between the AR/VR device and

the gth target/scatterer and σRCS

g

denotes the radar cross section of this target.

III. PROBLEM DEFINITION

Our objective in this paper is to efﬁciently estimate the depth/range map of the surrounding

environment using the communication-constrained mmWave MIMO sensing model in Section II.

Before delving into the formal problem deﬁnition, it is important to distinguish between the range

and depth of a certain target. As depicted in Fig. 3, the range of a target with respect to the

AR/VR camera (which is aligned with the AR/VR antenna array) is the linear radial distance from

the camera center (focal point) to the target. For the depth, it is measured by the z-coordinate

of the camera center (focal point) with respect to the x-y plane of the target. Given that the

range and depth can be calculated from one another, we focus our formulation on the depth

estimation problem. Next, we deﬁne the depth map of the surrounding environment with respect

to an AR/VR device.

Deﬁnition (Depth Map): We deﬁne the depth map Dmap of resolution Mh × Mw as an image

of Mh pixels high and Mw pixels wide, where the value of each pixel represents the smallest

depth between the AR/VR device and the targets/objects in this pixel.

In this paper, we express this depth map as an Mh × Mw matrix Dmap ∈ RMh×Mw. Further,

we use Mres = MhMw to denote the total number of pixels in the depth map. The range map

9

Fig. 3: This ﬁgure shows the conventional single target range estimation problem, where one target exists in free

space in line-of-sight (LoS) with the AR/VR device. This device steers perfectly one beam towards that target to

estimate the range.

Rmap ∈ RMh×Mw is similarly deﬁned. Now, given the system and channel models in Section II,

the AR/VR device constructs the estimated mmWave-based depth map through two main steps:

(i) sensing the environment using several beamforming and combining sensing vectors and (ii)

post-processing the receive sensing signal to construct the estimated depth map. More formally,

if a beamforming-combining pair (fm, wm) is used to transmit and receive the N p symbols of

the mth preamble sequence, then the receive sensing signal can be expressed as

ym[n] =

Ld−1
(cid:88)

d=0

(cid:112)

EswH

mHdfmsp[n − d] + wHvm[n],

n ∈ {0, 1, . . . , N p + Ld − 1},

(6)

where Hd = (cid:80)Gtar
g=1 Hd,g. By stacking the N p + Ld receive symbols (from transmitting the
preamble sequence), we get ym = [ym[0], . . . , ym[N p + Ld − 1]]T , which represents the receive

sensing vector of one preamble sequence using one beamforming-combining pair. Next, if M

preamble sequences are used to sense the environment via M beamforming-combining pairs

(fm, wm) , m ∈ {1, . . . , M }, then the aggregated receive sensing signal can be written as

Y = [y1, . . . , yM ].

(7)

For ease of exposition, we deﬁne the sensing beamforming codebook P as the codebook that

includes the M beamforming-combining pairs, i.e., P = {(fm, wm) : m ∈ {1, . . . , M }}. Finally,

given the receive sensing matrix Y, a post-processing is applied for estimating the depth map.

Small TargetAR/VR deviceDepthRange<latexit sha1_base64="f2yzimwbR/Dgjzp6tZ360fHRqNI=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit><latexit sha1_base64="l29WxoUb9DEbvmhLG7jHtZ0OU24=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4KokIeix68diC/YA2lM120q7dbMLuRgihv8CLB0W8+pO8+W/ctjlo64OBx3szzMwLEsG1cd1vZ219Y3Nru7RT3t3bPzisHB23dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5m/mdJ1Sax/LBZAn6ER1JHnJGjZWa2aBSdWvuHGSVeAWpQoHGoPLVH8YsjVAaJqjWPc9NjJ9TZTgTOC33U40JZRM6wp6lkkao/Xx+6JScW2VIwljZkobM1d8TOY20zqLAdkbUjPWyNxP/83qpCW/8nMskNSjZYlGYCmJiMvuaDLlCZkRmCWWK21sJG1NFmbHZlG0I3vLLq6R9WfPcmte8qtZvizhKcApncAEeXEMd7qEBLWCA8Ayv8OY8Oi/Ou/OxaF1zipkT+APn8wfnvYz9</latexit><latexit sha1_base64="HDzXchlsPlmuEyZZ/9zFJ+iVC6I=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N0IN/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB6UGM/g==</latexit>10

If g(.) denotes the post-processing function, the estimated depth map (cid:98)Dmap ∈ RMh×Mw can be
written as

(cid:98)Dmap = g(Y; P).

(8)

Our objective in this paper then is to design the sensing beamforming codebook P and the

post-processing g(·) to efﬁciently estimate the depth map (cid:98)Dmap to be as close as possible to the

actual depth map Dmap. To evaluate the performance of the proposed approaches, we will adopt

the root-mean squared estimation error (RMSE) and the mean absolute error (MAE) between

the depth maps, which are deﬁned as

∆RMSE =

∆MAE =

(cid:107)Dmap − g (Y; P)(cid:107)2
2,

(cid:114) 1
K
(cid:107)Dmap − g (Y; P)(cid:107)2
1 .

1
K

(9)

(10)

In Section V, we will present the general framework of our proposed depth map estimation

approach. This will be followed by a detailed description of the two main components in this

framework, namely the beamforming codebook design P and the post-processing solution g(.),

in Sections VI and VII.

IV. BACKGROUND

Before going into the proposed framework for estimating depth/range maps using mmWave

MIMO, we provide a brief background on the basics of the single-target range estimation

problem. For a preliminary model, consider one target in the free space with a Line-of-sight

(LoS) path to the AR/VR device. Further, consider the case when one mmWave beam is perfectly

steered towards that target, as depicted in Fig. 3. Adopting this preliminary model, the target range

estimation accuracy bound will be ﬁrst examined. Then, a description of the main algorithms

used in the literature to approach this problem is provided.

A. Target Range Estimation Accuracy

Our main objective is to ﬁnd the fundamental limit for mmWave MIMO based depth estima-

tion, which can be considered as range estimation at every possible eyesight direction, i.e. at

every azimuth angle φ ∈ [0, 2π[ and every elevation angle θ ∈ [0, π[. For the range estimation

accuracy, one useful metric is the Cramer-Rao lower bound (CRLB) on the range estimation.

For white Gaussian noise, the CRLB provides a lower bound on the mean-squared-error of any

11

unbiased estimator, hence it is used as a benchmark for the performance analysis of parameter

estimation [35]. Considering the case of range estimation for a single target, the CRLB of this

single target range is formulated as [29], [35], [36]
ς 2
8Pint η2B2 SNRrad

σ2
ˆρ ≥

,

(11)

where ς is the speed of light, B is the transmission bandwidth, Pint is the integration gain

and is equal to the number of symbols used for preamble estimation, and η depends on the

power spectral density shape of a(t) over the preamble duration. Under the assumption of a ﬂat
spectral density for a(t), η2 = (2π)2 /12. The radar signal-to-noise ratio for this target can then

be expressed as SNRrad = EsGrad/σ2

n, where Grad denotes the path gain associated with the

target.

B. Target Range Estimation Algorithms

Estimating the round trip delay ˆτ is equivalent to ﬁnding the range estimate ˆρ, since they

are directly related through ˆτ = 2ˆρ/ς. Given the extensive research on delay estimators in

the literature [29], [37], we will restrict the scope of this paper on the magnitude based delay

estimators in [38] for simplicity. In a general sense, given a known transmit preamble sequence

x0[n] and the received baseband sequence z[n], the receiver can estimate the round-trip delay by

maximizing an objective function, the cross-correlation function between the two time-sequences,

over a range of possible delays. Based on this notion, two delay estimators are formulated as

follows [38].

1) Basic Correlator: The basic correlator is a coarse delay estimator that performs the

maximization at the same sampling frequency, fS, tuned by the AR/VR communication system.

Assume that the length of the received baseband sequence, z[n], is Lz samples, where the last

Nz samples are non-zeros. The range estimate can then be formulated as

ˆρBC =

ςTS
2

arg max
q:q∈Q

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Lz−1
(cid:88)

n=Lz−Nz

2

(cid:12)
(cid:12)
x0[n] × z∗[n − q]
(cid:12)
(cid:12)
(cid:12)

.

(12)

where TS = 1
B denotes the sampling time, Q represents the set of possible discrete
fS
sample delays, and the optimal q solution is denoted by qBC. Unfortunately, the accuracy of

= 1

this range estimate is limited by the sampling frequency fS. One attempt of improving the

estimation accuracy is by performing the maximization at a higher sampling frequency. This

attempt, however, increases the computational complexity dramatically, which motivates the role

of the upcoming delay estimator, the massive correlator [38].

12

2) Massive Correlator: The primary function of the massive correlator is to perform the

maximization of the objective function at a higher sampling frequency without the computational

burden of computing the shift in real time. For this reason, [38], [39] introduced the solution of

pre-designing a speciﬁc correlator bank that contains shifted versions of the reference sequence,

x0[n]. The receiver will then multiply the received sequence by the correlator bank to compute

the objective function.

We describe the steps of the massive correlator algorithm as follows [38], [39]. (a) Upsample

x0[n] with a sampling frequency higher than fS, denoted as fest. (b) Deﬁne the correlator bank

matrix, X0, where each row of this matrix is a shifted version of the upsampled x0[n]. Let the

number of rows in X0 be equal to (2δ + 1), where δ is the largest lag/advance discrete fractional
delay in the receive sequence, such that δ = fest
2fS

. (c) Downsample independently each row of the

correlator matrix to the lower sampling frequency fS; let the resulting matrix be named as B0.

The reason for this step is to test delays at the higher sampling frequency, fest, but only apply

multiplications at the lower sampling frequency, fS. (d) Shift back the receive sequence, z[n],

with the coarse discrete delay estimate, qBC, such that ¯z[n] = z[n + qBC], and then concatenate

the sequence into one row vector, ¯z. (e) Calculate the fractional range estimate, ˆρ(cid:48), such that

(cid:16)

ˆρ(cid:48) = ς

2fest

−(δ + 1) + arg maxq(cid:48) [g]q(cid:48)

(cid:17)

ˆρMC, such that ˆρMC = ˆρBC + ˆρ(cid:48).

, where g = ¯z×BH

0 . (f) Calculate the ﬁne range estimate,

With an end goal of constructing depth maps, these range estimation algorithms will then be

leveraged by the mmWave MIMO based depth estimation as explained in the upcoming sections.

In the next section, we formulate a general framework for scene depth estimation.

V. GENERAL FRAMEWORK FOR SCENE DEPTH ESTIMATION

In this section, we highlight the key elements of the proposed depth map estimation approach,

namely the sensing beamforming codebook P and post-processing g(.), and discuss the chal-

lenges associated with designing these elements. As depicted in Fig. 4, we ﬁrst design the sensing

beamforming codebook P ofﬂine based on the desired AR/VR properties such as the ﬁeld of

view, the scene aspect ratio, and the number of horizontal and vertical beams covering the scene

view. To build the depth map of a certain scene, the beam pairs of the designed codebook are

used to sense the environment and acquire the receive sensing matrix Y in (6). This receive

signals are then jointly processed using the post-proposed approach to build the depth map. In

the remaining of this section, we explain the challenges associated with designing the codebook

13

Fig. 4: The ﬁgure summarizes the proposed sensing framework for mmWave MIMO based depth estimation, which

involves sensing the scene using the designed beamforming codebook P and applying the proposed post-processing

operations g(.; P) to the receive signal to construct the estimated depth map (cid:98)Dmap.

and the post-processing operations. Then, we will present how our proposed solutions overcome

these challenges in Sections VI and VII.

A. Codebook Design Challenges

To effectively sense the surrounding environment and build efﬁcient depth maps, the beams

of the sensing codebooks should be designed to scan the full scene. Since the mmWave MIMO

based depth maps will likely complement the RGB-D based maps, our objective is to build a

beamforming codebook that scans the full rectangular grid of the typical depth sensors of the

AR/VR cameras. However, the classical beam steering codebooks such as the DFT codebooks

[40], that independently sample the azimuth and elevation directions, do not normally ﬁt a

rectangular grid. They instead form a parabolic grid, i.e., for a ﬁxed elevation angle, the grid

line of these codebook beams are parabolic curves as shown in Fig. 5(a). This mismatch between

the mmWave MIMO-based and camera-based depth grids could lead to clear distortion in the

joint mmWave/RGB-D depth map construction and make it hard to complement the RGB-D

depth map using mmWave MIMO sensing.

One possible solution is to estimate the depths on the parabolic grid using the classical

beamforming codebook and then interpolate/extrapolate to calculate the rectangular depth map.

The main disadvantage of this solution, however, is that the interpolation can potentially lead

to considerable loss in the depth map accuracy as the changes of the depth are not normally

Beamforming-combiningPair Codebook DesignmmWaveSensing with Overlapped BeamsSuccessive Interference CancellationDepth Map ConstructionCodebook SweepJoint Processing SolutionField of ViewAspect RatioHorizontal ResolutionVertical ResolutionBeam codebookPost-ProcessingDepth MapDesign ParametersAR/VRProposed Depth Map Sensing Framework14

(a) Parabolic shape of the classical codebook grid

(b) Classical codebook grid mismatch

Fig. 5: (a) The intersections between the classical codebook beam directions and the x-z depth plane form the

parabolic shape of the classical codebook grid. (b) The mismatch between the classical codebook grid of a 16 × 16

UPA and the desirable rectangular grid for a depth map is illustrated at a y = 13.32mm depth plane, for a scene

of 100◦ ﬁeld of view, 16/9 aspect ratio.

smooth in nature. Hence, in order to avoid the interpolation loss, the more persuasive solution

is to develop a depth map compatible beamforming codebook that ﬁts exactly the desirable

rectangular sensor grid. With this motivation, we propose a beamforming/combining design

approach in Section VI to overcome the codebook mismatch challenge.

B. Scene Depth Estimation Challenges

The sensing beamforming codebook is used to sense the surrounding environment. Now, given

the receive sensing matrix Y, the objective of the post-processing is to construct an accurate

depth map of the facing scene. This process, however, has several challenges. In order to explain

these challenges, let’s ﬁrst consider the case when the environment has only a single target. In

this case, the sensing/scanning beam that is directed towards the region that includes this target

will result in some backscattering signal. This signal can be used for calculating the round-trip

time of ﬂight and consequently the range of this target, leveraging the MIMO radar concepts

[29], [41] and the algorithms detailed in Section IV-B. In terms of the range/depth map, the pixel

that includes the region of this target will simple have the value of the estimated range/depth. In

practice, however, the environment has several targets/surfaces and the mmWave arrays have strict

constraints on their hardware: power budget, computational complexity, etc. These limitations

lead to critical challenges for our objective of building accurate depth and range maps of the

AR/VR DeviceDepth planeCodebook beams atone elevation angle <latexit sha1_base64="f2yzimwbR/Dgjzp6tZ360fHRqNI=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit><latexit sha1_base64="l29WxoUb9DEbvmhLG7jHtZ0OU24=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4KokIeix68diC/YA2lM120q7dbMLuRgihv8CLB0W8+pO8+W/ctjlo64OBx3szzMwLEsG1cd1vZ219Y3Nru7RT3t3bPzisHB23dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5m/mdJ1Sax/LBZAn6ER1JHnJGjZWa2aBSdWvuHGSVeAWpQoHGoPLVH8YsjVAaJqjWPc9NjJ9TZTgTOC33U40JZRM6wp6lkkao/Xx+6JScW2VIwljZkobM1d8TOY20zqLAdkbUjPWyNxP/83qpCW/8nMskNSjZYlGYCmJiMvuaDLlCZkRmCWWK21sJG1NFmbHZlG0I3vLLq6R9WfPcmte8qtZvizhKcApncAEeXEMd7qEBLWCA8Ayv8OY8Oi/Ou/OxaF1zipkT+APn8wfnvYz9</latexit><latexit sha1_base64="HDzXchlsPlmuEyZZ/9zFJ+iVC6I=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N0IN/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB6UGM/g==</latexit>-0.025-0.02-0.015-0.01-0.00500.0050.010.0150.020.025Horizontal direction [m]-0.025-0.02-0.015-0.01-0.00500.0050.010.0150.020.025Vetical direction [m]Classical beamforming/combining codebook gridTarget rectangular grid for scene depth maps15

Fig. 6: The multipath estimation challenge for scene range estimation is illustrated. The design challenge is how

the sensing framework can detect and estimate the range through the desired channel path (path 1 in blue) and

avoid making faulty estimation because of the other undesired paths (paths 2-4) in the environment.

environment. More speciﬁcally, if we adopt the approach that scans the surrounding scene using

a beamforming codebook and processed the receive sensing signal of each beam independently to

estimate the depth of the region deﬁned by this beam, then this approach will have the following

key drawbacks.

• Low-resolution depth-maps: The low resolution drawback is mainly due to (i) the lim-

itation on the number of AR/VR antennas, which is controlled by many factors in the

AR/VR device such as the device dimensions, computational complexity, circuit routing,

power consumption, etc., and (ii) the number of beams in the sensing codebook P, which

is limited by the time allocated for the depth estimation process.

• Inter-target interference: The constraints on the number of antennas at the AR/VR de-

vice limit the system spatial resolution. This makes it hard to differentiate between the

ranges/depths of the different targets/surfaces that are close to each other. In other words,

when measuring the depth of the object in a particular region/ direction, multiple ob-

jects/surfaces may reﬂect the incident signal at the same time. The interference between these

reﬂected/scattered signals may highly affect the accuracy of the range/ depth estimation.

Hence, if a certain pixel has multiple objects/surfaces, it will be difﬁcult to estimate the

shortest depth of the objects in this pixel (to follow the depth map deﬁnition in Section III).

• Inter-path interference: When sensing the range/depth of a certain target, the optimal

situation (in terms of depth estimation accuracy) is when the target backscatters a single

AR/VR devicePath 1back-scattering desired pathPath 2back-scattering main lobePath 3back-scattering side lobePath 4two reflections 16

ray to the receiving array. In practice, however, the signal incident on a certain target

may experience more than one phenomenon, such as scattering, reﬂection, diffraction, etc.,

which results in multiple rays. More than one of these rays could traverse the environment

in different ways/directions, especially in indoor environments, before reaching the receiver.

This means that they may reach the receiving array from multiple angles and with different

time of ﬂights. This causes an inter-path interference which makes it hard to accurately

estimate the range/depth of the target of interest. For example, if the receiver estimates the

range/depth based on a wrong path, this may noticeably degrade the accuracy of the depth

map estimation. This challenge is depicted in Fig. 6. As illustrated, the challenge is how

to design the sensing framework (the codebook and post-processing) to detect the desired

channel path (the path in blue) while ﬁltering out all the undesired channel paths. Examples

of undesired paths are the paths 2-4. Path 2 is transmitted and received within the main

lobe. Path 3 is transmitted and received within the side lobe. Path 4 experiences multiple

reﬂections instead of back-scattering, before reaching back the receiver.

In the next two sections, we efﬁciently design the two elements of our proposed depth map

sensing framework, namely the sensing codebook and the post-processing, to address these

challenges.

VI. DEPTH MAP BASED DESIGN FOR SENSING CODEBOOKS

As discussed in Section V-A, our objective is to design a sensing beamforming codebook

that ﬁts the rectangular grid of the depth camera. In this section, we ﬁrst present our codebook

design that achieves this objective. Then, we incorporate a new side-lobe reduction approach to

ameliorate the inter-path interference problem.

A. Proposed Codebook Design

Since the objective from the beamforming-combining pair codebook design is for the codebook

grid to match the desired rectangular grid of a range/depth scene, we start with the relevant

camera geometry equations. The scene deﬁnition starts by deﬁning the key quantities of the

ﬁeld of view, FoV, and the scene aspect ratio, AR. Let the ﬁeld of view be centered around the

boresight antenna array direction. It is worth noting that the separation distance of the camera

plane away from the antenna array reference point, aka the focal length, is irrelevant in our

17

codebook design. This is based on the notice that the beamforming/combining codebook design

normally depends on angles rather than distances.

In a general sense, for any chosen value of focal length, the sensor grid points’ coordinates are

ﬁrst calculated to determine the codebook angles accordingly. More speciﬁcally, assume that the

focal length is set to a certain value, FL. The camera plane width, aka the sensor grid width in

the horizontal dimension, SH, and camera plane height, aka the sensor grid height in the vertical

dimension, SV, can be calculated as

SH = 2FL tan (FoV/2) , and SV = SH/AR.

(13)

For designing a beamforming-combining pair codebook, let N = NV × NH, where NV and

NH denote the number of UPA antennas on the elevation (vertical) and azimuth (horizontal)

dimensions, respectively. Consider an oversampled beamforming codebook of M = N VN H
beams, where N V = NVF OS

H denoting the oversampling
factors in the elevation and azimuth dimensions, respectively. The grid spacing in the vertical

V and N H = NHF OS

H with F OS

V and F OS

and horizontal directions are expressed as QV = SV/N V and QH = SH/N H. Notice that the

codebook resolution of N VN H beams will be mapped at the end to the desired up-scaled depth

image resolution of Mres = Mh × Mw pixels.

Let the x- and z-axes be in the direction of the sensor grid width and height, respectively, and

let the y-axis be the direction of the depth. The (x, y, z) rectangular coordinates of the sensor

grid points on the camera plane can then be deﬁned as

(x, y, z) ∈ C, C = X × Y × Z,

X = (cid:8)x : x ∈ (cid:8) −SH

2 + QH

2 , −SH

2 + 3QH

2 , . . . , SH

2 − QH

2

Y = {y : y = FL} ,
Z = (cid:8)z : z ∈ (cid:8) −SV

2 + QV

2 , −SV

2 + 3QV

2 , . . . , SV

2 − QV

2

(cid:9)(cid:9) ,

(cid:9)(cid:9) ,

(14)

where we note that |C| = N VN H = M . After deﬁning the (x, y, z) coordinates of every grid

point on the camera plane, their M corresponding (θz, θx) angles with respect to the z- and

x-axes can now be calculated using the mapping from rectangular to spherical coordinates, such

that

(cid:26)

O =

(θz, θx) : θz =

(cid:20)

π
2 − arctan

(cid:18)

z√

x2+y2

(cid:19)(cid:21)

(cid:20)

, θx =

π
2 − arctan

(cid:18)

x√

(−z)2+y2

(cid:19)(cid:21)

(cid:27)

, (x, y, z) ∈ C

.

(15)

18

Fig. 7: The comparison between (a) the classical (on the left side) and the proposed (on the right side) beam

codebook design is demonstrated for a scene of 100◦ ﬁeld of view and 16/9 aspect ratio, using 16 × 16 UPAs.

The proposed codebook eliminates any grid mismatch distortion. The top ﬁgures are the 3D codebook radiation

patterns while the bottom ﬁgures are the 2D codebook grids at a plane within 13.32mm depth.

Finally, after calculating the (θz, θx) angles for each and every grid point, the beamforming

codebook, F , for an NH × NV transmit UPA, is then expressed as

(cid:110)

F =

f ∈ CN ×1 : f = (cid:101)bV (θz) ◦ (cid:101)bH (θx) , (θz, θx) ∈ O

(cid:111)

,

(cid:101)bV (θz) = (cid:2)1, e−jκds cos(θz), e−j2κds cos(θz), . . . , e−j(NV−1)κds cos(θz)(cid:3)T
(cid:101)bH (θx) = (cid:2)1, e−jκds cos(θx), e−j2κds cos(θx), . . . , e−j(NH−1)κds cos(θx)(cid:3)T

,

,

(16)

where κ = 2π
λ is the wave number, λ is the operating wavelength, and ds is the antenna element
spacing between adjacent UPA elements in meters. (cid:101)bH ∈ CNH×1 and (cid:101)bV ∈ CNV×1 are the
horizontal and vertical basic vectors used for constructing the beamforming codebook. We will

call these vectors, (cid:101)bH and (cid:101)bV, the constituent horizontal and vertical beamforming vectors,

respectively. In our depth estimation problem, the receive combining codebook, W, can be

Horizontal direction [m]Horizontal direction [m](a)(b)Vertical direction [m]Vertical direction [m]19

similarly deﬁned for the NH × NV receive UPA. For such case, the cardinalities of the sets are
equal, |W| = |F | = |C| = |O| = M . Further, let F ∈ CN ×M and W ∈ CN ×M be the matrices

that consist of the codebooks beams of F and W. Then, the proposed sensing beamforming-

combining pair codebook P can be expressed as

P =

(cid:110)
(fm, wm) ∈ CN ×1 × CN ×1 : fm = [F]:,m , wm = [W]:,m , m ∈ {1, . . . , M }

(cid:111)

.

(17)

A comparison between the classical and the proposed beam codebook design is demonstrated

in Fig. 7 for a scene of 100◦ ﬁeld of view and 16/9 aspect ratio, using 16 × 16 UPAs. The top

ﬁgures are the 3D codebook radiation patterns while the bottom ﬁgures are the 2D codebook

grids at a plane within 13.32mm depth. As shown, the proposed beam codebook eliminates any

grid mismatch distortion.

B. Sidelobe Reduction Approach

As discussed in Section V-B, to rectify the inter-path interference problem, the sensing frame-

work needs to ﬁlter out the undesired channel paths. As illustrated in Fig. 6, one type of undesired

channel paths is the type of paths transmitted from/received by the sidelobes of a codebook beam.

For this reason, we propose an efﬁcient sidelobe reduction (SLR) approach. In [42], [43], an

SLR approach was proposed for low sidelobe beamforming in uniform circular arrays. Inspired

by their work, we propose a new efﬁcient sidelobe reduction approach to uniform planar arrays

(UPAs) to reduce beamforming/combining sidelobe levels.

The key idea of this approach is when applying different weights on the beamforming/combining

vector elements, the sensing framework can control the beam radiation pattern in a way to in-
crease the power difference between the mainlobe and the sidelobes. Speciﬁcally, let cH ∈ RNH×1
and cV ∈ RNV×1 represent the horizontal and vertical weight vectors for sidelobe reduction. Let

bH and bV denote the horizontal and vertical constituent beamforming vectors after sidelobe

reduction. The updated beamforming codebook, F , for an NH × NV transmit UPA, can then be

rewritten as

F = (cid:8)f ∈ CN ×1 : f = bV (θz) ◦ bH (θx) , (θz, θx) ∈ O(cid:9) ,

bV (θz) = (cid:101)bV (θz) (cid:12) cV, bH (θx) = (cid:101)bH (θx) (cid:12) cH,

[cV]rV

= e

[cH]rH

= e

−(rV−µV)2
2σ2
V

−(rH−µH)2
2σ2
H

, µV =

, µH =

NV
2

NH
2

, σV =

, σH =

NV
δV
NH
δH

, rV ∈ {1, 2, . . . , NV},

, rH ∈ {1, 2, . . . , NH},

(18)

20

Fig. 8: Normalized power radiation pattern comparison between (a) the case without the sidelobe reduction (SLR)

approach, (b) the case with the SLR approach where δH = δV = 3, and (c) where δH = δV = 4. As shown,

increasing the values of the control variables (the deltas) increases the gap between the mainlobe level and the

sidelobes levels. The top ﬁgures are the 3D views of the patterns while the bottom ﬁgures are the top views.

where δH, δV are the sidelobe reduction control variables; the higher the values, the greater

reduction in the sidelobe power levels compared to the mainlobe power level.

Fig. 8 illustrates the radiation pattern in dB for one beamforming vector out of the updated

beamforming codebook, F , for different values of the sidelobe reduction control variables, δH, δV.

As depicted, increasing the values of the control variables increases the power gap between the

mainlobe level and the sidelobes levels. To take into consideration the phase quantization of

the RF phase shifters in the AR/VR transceiver architecture previously shown in Fig. 2, we

examine the effect of 2-bit phase quantization on the power radiation pattern. The 2-bit discrete
(cid:9). Fig. 9 compares the power radiation pattern between the case
phase shift set is (cid:8)0, π

2 , π, 3π

2

of continuous phase shifts and the case of 2-bit quantized phase shifts. As depicted, the phase

quantization affects the beam pattern shape of the sidelobes.

One main advantage of this approach is its computational efﬁciency; as formulated, only

two element-wise multiplication between the weight vectors and the constituent beamforming

vectors, (cid:101)bH, (cid:101)bV, are needed to update the beam radiation pattern. By contrast, reducing the

(a)  No SLR.(b) SLR, deltas = 3.(c) SLR, deltas = 4.21

(a) No SLR and continuous phase shifts

(b) SLR deltas = 4 and continuous phase

shifts

(c) No SLR and 2-bit phase shifts

(d) SLR deltas = 4 and 2-bit phase shifts

Fig. 9: Normalized power radiation pattern comparison between the case with no phase quantization and the case

with 2-bit phase quantization, for two scenarios: without the sidelobe reduction (SLR) approach and with the SLR

approach where δH = δV = 4. The phase quantization affects the beam pattern shape of the sidelobes.

sidelobe levels dramatically increases the beamwidth of the mainlobe, as depicted in Fig. 8. The

increased mainlobe beamwidth, however, can be mitigated by the other solutions proposed for

rectifying the inter-path interference problem, e.g. the successive interference cancellation (SIC)

algorithm and the joint processing (JP) solution, as will be described in the following section.

VII. PROPOSED SCENE RANGE/DEPTH ESTIMATION

In this section, given a pre-designed beamforming/combining codebook, P, we propose an

efﬁcient approach for the scene range/depth estimation in AR/VR devices. As depicted in Fig. 4,

once the beamforming/combining codebook has been designed, the AR/VR transmits the sensing

signal while sweeping over all the beamforming-combining vector pairs. Speciﬁcally, for a

beamforming-combining vector pair (fm, wm), where m ∈ {1, . . . , M }, the receive sensing

22

signal, ym ∈ CN p+Ld, can be modeled as in (6) and (7). After reception, the acquired sensing

signals are processed to estimate the range and depth maps, as will be thoroughly explained

in this section. Our proposed post-processing solution has three main elements: (i) The use of

oversampled/overlapped beams, (ii) the successive interference cancellation based management of

inter-target and inter-path interference, and (iii) the joint processing of the signals received using

the codebook beams to realize high-resolution and accurate depth maps. Next, we explain these

three elements in Sections VII-A-VII-C before presenting the scene range/depth map construction

approach in Section VII-D.

A. Overlapped Beams

With the objective of increasing the resolution of the mmWave MIMO based depth maps,

we propose to adopt oversampled sensing codebooks to scan the surrounding environment.

In particular, for the sensing codebook, we adopt the developed codebook in Section VI-A

with oversampling factors of F OS

V in the azimuth and elevation directions. While the
oversampled codebook has the potential of enhancing the depth map resolution, it is important to

H and F OS

note that advanced post-processing (for the receive signals using these oversampled beams) needs

to be incorporated to achieve this goal. The reason mainly goes back to the wide beamwidth (and

low spatial resolution) of the codebook beams, which is fundamentally limited by the number of

AR/VR antennas. This wide beamwidth leads to a number of challenges: (i) The spatial regions

scanned by the oversampled beams have high overlap. This makes it hard to differentiate between

the depths of the different objects in the depth map pixels, which challenges the objective of

realizing high-resolution depth maps. (ii) Since the codebook beams still have wide beamwidth,

the inter-target interference problem discussed in Section V-B still exists.

To address these challenges, we propose a novel post-processing approach based on successive

interference cancellation and joint-beam processing. This approach in summarized in two main

steps as follows. In the ﬁrst step, a successive interference cancellation (SIC) based algorithm is

used to detect the most dominant channel paths contributing to the range/depth estimation of the

region covered by each codebook beam. These paths form a set of candidate ranges/depths for

the scene range/depth estimation. In the second step, a developed joint-beam processing solution

selects one range/depth out of the set of candidate ranges/depths formed by the SIC algorithm.

These two sequential algorithms are discussed in detail in the following two subsections.

Algorithm 1 Successive Interference Cancellation
Inputs: Receive sensing signal ym[n], transmit preamble signal sp[n], threshold level ATH,

beamforming-combining pair codebook P.

Outputs: Candidate delay set for each beam, Tm, ∀m ∈ {1, . . . , M },.

1: for m = 1 to M do

(cid:46) For each beamforming and combining vector pair (fm, wm).

23

2:

3:

4:

5:

6:

7:

8:

Initialize: Updated signal ˜ym[n] ← ym[n], solution set Tm ← ∅, ∀m, and ˜A ← ATH.
while ˜A ≥ ATH do

(cid:46) Compute the delay of each dominant channel path.

Calculate the delay of the path with the maximum cross-correlation,

˜q ← arg max

q:q∈Q





˜A ←

Ly−1
(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Ly−1
(cid:88)

n=Ly−Ny

sp[n] × (˜ym[n − q])∗

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

sp[n] × (˜ym[n − ˜q])∗



 .

n=Ly−Ny

Add the candidate delay to the solution set, Tm ← Tm ∪ {˜q} .

Calculate the energy of the transmit signal up to this delay value,

EQ ←

Ly−˜q−1
(cid:88)

n=Ly−Ny

|sp[n]|2 .

Perform interference cancellation at this delay value,

˜ym[n] ← ˜ym[n] −

˜A
EQ

sp[n − ˜q].

Calculate the next maximum in the cross-correlation,
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

˜A ← max
q:q∈Q

Ly−1
(cid:88)

sp[n] × (˜ym[n − q])∗

n=Ly−Ny

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

B. Successive Interference Cancellation

The main goal of the successive interference cancellation (SIC) algorithm is to detect all the

dominant paths that might contribute to the range estimation of the region of interest. This is

motivated by its good performance in multi-target detections problems [44]. The SIC algorithm

is applied in the discrete-time domain and is summarized in Algorithm 1. The algorithm is

described as follows. Let the length of the receive sensing sequence ym[n] be Ly = N p + Ld

24

Fig. 10: The operation of the successive interference cancellation (SIC) algorithm is illustrated. The delay position

of the maximum cross-correlation is ﬁrst detected. The SIC algorithm then encodes a signal shifted at this delay

position and subtracted it from the receive signal. After that, the algorithm repeats itself until all the local maxima

above the threshold value are detected.

symbols. First, as shown in Fig. 10, for every codebook beam, the delay position of the maximum

cross-correlation magnitude value is detected. Q is the set of possible delays. Second, the SIC

algorithm encodes the transmit preamble signal to be shifted to this delay position and subtracted

it from the received signal. Afterwards, the algorithm repeats itself to detect the second local

maximum above the threshold value. Finally, The SIC algorithm stops iterating when all the

local maxima above the threshold value are detected. The output of this algorithm is a set

of candidate delays for every codebook beam. These sets pass as input to the next algorithm,

the joint processing solution, as will be explained in the next part. In Fig. 10, note that the

cross-correlation magnitude plot appears to be drawn as a continuous plot, only for illustration

purposes. The actual cross-correlation magnitude, however, is expressed in discrete time delays.

C. Joint Processing Solution

The purpose of the joint processing (JP) solution between the overlapped beams is to estimate

the transitions in depth/range maps more accurately. The proposed JP solution is summarized

in Algorithm 2. The algorithm is described in detail as follows. First, the JP solution works
on the candidate delay sets, the output from the SIC algorithm, {Tm}M

m=1, to choose one range
estimate out of the candidate delay set. This processing, however, is employed relative to the 2D

codebook grid, as illustrated in Fig. 11. Following this notion, the linear indices in Tm is now

converted into matrix subscripts Th,v through the transformation m = (v − 1)N H + h, such that

Tm = Th,v, where v is the elevation beam index (vertical grid index) and h is the azimuth beam

index (horizontal grid index). The objective is to calculate the scene range estimates across all

Cross-correla�on magnitudeTime delay00TH110Cross-correla�on magnitude0Time delayTH1212Cross-correla�on magnitude0Time delay0TH1231230Cross-correla�on magnitudeTime delay0TH123123Iteration 1Iteration 2Iteration 3Iteration 425

Fig. 11: This ﬁgure illustrates the basic operation of the joint processing (JP) solution for overlapped beams. The

JP solution sweeps from left to right, then from top to bottom. The JP solution decides on which path to choose

from the current candidate set by a simple comparison with the sets of the surrounding grid points.

Algorithm 2 Joint Processing Solution

Inputs: Candidate delay set for each beam, Th,v, ∀h ∈ {1, . . . , N H}, ∀v ∈ {1, . . . , N V}.
Outputs: Scene range estimate (ˆρh,v)SRE , ∀h, v.

1: Initialize: Common adjacent set Nh,v ← ∅, ∀h, v, difference set Mh,v ← ∅, ∀h, v.

2: for v = 1 to N V do

3:

4:

5:

6:

7:

8:

9:

for h = 1 to N H do

Construct the common adjacent set, Nh,v ← (Th−1,v ∪ Th,v−1 ∪ Th−1,v−1 ∪ Th+1,v−1).

Construct the difference set, Mh,v ← Th,v \ Nh,v.

if Mh,v (cid:54)= ∅ then

Choose the least delay from the difference set, (ˆρh,v)SRE ← ςTS

2 min Mh,v.

else

Choose the least delay from the candidate set, (ˆρh,v)SRE ← ςTS

2 min Th,v.

beam directions, (ˆρh,v)SRE , ∀h, v.

As shown in Fig. 11, the JP solution sweeps from left to right, then from top to bottom. For

each grid point, the JP solution uses (i) the set of the current grid point, named as the ”current

set”, and (ii) the sets of the previous adjacent grid points to construct a ”common adjacent set”.

This common adjacent set is the union of the sets of all previous adjacent grid points. Then,

to investigate if a new object/surface transition appears, this current set is compared with the

𝑣ℎ1ഥ𝑁𝐻ഥ𝑁𝑉⋯⋯1Adjacent setsCurrent candidate set26

Algorithm 3 mmWave MIMO Sensing Based Range/Depth Estimation Framework

Inputs: Field of view FoV, aspect ratio AR, number of horizontal and vertical beams N H, N V.

Outputs: Range map (cid:98)Rmap, depth map (cid:98)Dmap.

1: Design the beamforming-combining pair codebook, P, following Section VI.

2: for m = 1 to M do

(cid:46) For each beamforming and combining vector pair (fm, wm).

3:

Acquire receive sensing signal, ym[n], ∀n ∈ {0, 1, . . . , N p + Ld − 1}, as in (6).

4: Calculate the candidate delay set for each beam, Tm, ∀m ∈ {1, . . . , M }, as in Algorithm 1.
5: Calculate the scene range estimate for each beam, (ˆρm)SRE , ∀m, as in Algorithm 2.
6: Calculate ﬁne range estimates, (ˆρm)MC ← (ˆρm)SRE + (ˆρm)(cid:48) , ∀m, following Section IV-B2.

7: Construct the range map, (cid:98)Rmap, from (20).

8: Construct the depth map, (cid:98)Dmap, from (21).

common adjacent set to detect if there is any set difference. This is based on the notion that the

difference set can probably be the new edges that will appear in the range map while sweeping.

If the set difference is not empty, then the solution chooses the path with the least time-of-ﬂight

from the set difference. Otherwise, if the set difference is empty, then the solution chooses the

path with the least time-of-ﬂight from the current set.

D. Range/Depth Map Construction

In this section, we formulate the depth map construction approach, the ﬁnal step in Fig. 4. In

summation of the broader view, the mmWave MIMO sensing based range/depth map estimation

framework is outlined in Algorithm 3. The algorithm steps are summarized as follows. Step 1

refers to the design of the beamforming-combining pair codebook P was covered in Section VI.

Step 4 refers to the successive interference cancellation described in Section VII-B. Step 5 refers

to the joint processing solution detailed in Section VII-C. After that, in Step 6, the ﬁne range
estimate can be calculated, such that (ˆρm)MC = (ˆρm)SRE + (ˆρm)(cid:48), where (ˆρm)(cid:48) is computed

from the algorithm described in Section IV-B2. Next, after calculating the range estimates, the

upcoming steps (Steps 7,8) are focused on constructing the range and depth maps. Note that the

range to an object is actually the radial distance in spherical coordinates. Fortunately, the (x, y, z)

rectangular coordinates of the sensor grid points on the camera plan were already calculated

27

Fig. 12: This ﬁgure demonstrates the adopted simulation framework for scene depth estimation. The framework

consists of: designing the indoor setup, generating the ground truth range/depth maps, and constructing the estimated

maps for performance evaluation. For more complex setups, designing the indoor scenarios jointly in Wireless InSite

and Blender can be more effective.

for the design of the beamforming-combining pair codebook using (14). These rectangular

coordinates in (14) can then be converted to spherical coordinates, such that

(cid:40)

S =

(θz, Φ) : θz =

(cid:20)

π
2 − arctan

(cid:19)(cid:21)

(cid:18)

z√

x2+y2

, Φ = arctan (cid:0) y

x

(cid:1) , (x, y, z) ∈ C

(cid:41)
.

(19)

In order to construct the matrices for the range and depth maps, let Θ, Φ ∈ RN V×N H be

the matrices that represent the angles of the spherical coordinates (θz, Φ) ∈ S, respectively.
Following Step 6 in Algorithm 3, the range map estimate (cid:98)Rmap ∈ RN V×N H can be expressed as

(cid:104)
(cid:98)Rmap

(cid:105)

v,h

= (ˆρm)MC , m = (v − 1)N H + h, ∀m ∈ {1, . . . , M },

(20)

where h ∈ {1, . . . , N H}, v ∈ {1, . . . , N V}. Given the angles in spherical coordinates and the
range map estimate, the depth map estimate (cid:98)Dmap ∈ RN V×N H can then be expressed as
(cid:104)

(cid:105)

(cid:105)

= |ˆρ sin (θz) sin (Φ)| , ˆρ =

(cid:98)Rmap

, θz = [Θ]v,h , Φ = [Φ]v,h , ∀v, h.

(21)

(cid:104)
(cid:98)Dmap

v,h

v,h

Finally, since the range and depth map resolutions are set to N H ×N V, two-dimensional image

interpolation can be employed to scale the maps to the desired resolutions, Mh × Mw. Examples

of interpolation methods are the nearest neighbor interpolation and the bicubic interpolation.

Although the bicubic interpolation can probably be the interpolation method of choice for

achieving more estimation accuracy, the nearest neighbor interpolation is more computationally

efﬁcient. In the simulation results of Section VIII, we evaluate the two interpolation approaches

for our mmWave MIMO based depth map construction problem.

Design the indoor scenarioSubdivide the objects into multiple facesGenerate the ground truth depth mapSet the object materials and generate the channel pathsGenerate the delay-𝒅MIMO channel matrix followed by the receive radar sequencesmmWaveMIMO based range/depth estimationResize the ground truth depth map to the codebook resolutionPerformance EvaluationWireless InSiteWireless InSiteBlenderMATLAB28

VIII. SIMULATION RESULTS

In this section, we evaluate the performance of the proposed mmWave based depth estima-

tion approach. First, we describe the adopted simulation framework in Section VIII-A before

extensively studying the estimation accuracy of the proposed approach under various scenarios

and system parameters. The simulation results presented can be of great usefulness for various

applications; they can be generally applied to AR/VR devices, smart home devices, or auto drive

devices.

A. Simulation Framework

Since the depth estimation heavily depends on the environment under test, it is crucial to

evaluate the performance of the proposed solution based on realistic channels. This motivates

using channels generated by accurate ray-tracing to capture the sensing dependence on the

environment geometry, scatterers’ materials, AR/VR position, etc. This is why we designed the

simulations models using Remcom Wireless InSite [16], which is an accurate 3D ray-tracing

simulator. Further, to efﬁciently incorporate diffuse scattering models, we need to have highly

detailed ﬂoor plans with a sufﬁcient number of faces. To achieve this objective, we resorted to

the high-ﬁdelity game engine, Blender [17], to build accurate ﬂoor plans. These plans/models

are then exported to Wireless InSite to obtain the ray-tracing outputs, and ﬁnally to MATLAB

to construct the channel models in (4) and implement the proposed depth estimation approach.

The proposed evaluation framework is illustrated in Fig. 12. For benchmarking, we also use the

Blender ﬂoor plans to obtain the ground truth depth maps, which are essential to evaluate the

accuracy of our solutions. The ground truth maps are generated by placing a Blender camera

at the same position of the UPA reference antenna element, and adjusting the Blender camera

parameters to capture the same ﬁeld of view.

Signal model: We adopt the signal model described in Section II with a focus on the sensing

system performance. The AR/VR device is assumed to be ﬁxed in position. Unless otherwise

mentioned, the UPA size is 16 × 16 antennas (NH = NV = 16) at the mmWave 60GHz

operating band with transmission bandwidth of 2GHz. The antenna elements have a gain of

0dBi with half-wavelength antenna spacing. The transmit power is set to 30dBm. The preamble

sequence is the same as the one in the single carrier PHY packet preamble of the IEEE 802.11ad

standard (3328 symbols). M preamble sequences are used to sense the environment via M

beamforming-combining pairs. For the sake of calculating a rough estimate of the time allocated

29

TABLE I: The adopted diffuse scattering parameters for different materials

Diffuse Scattering Parameter

Concrete Ceilingboard Wood

Floorboard

Drywall Glass

Scattered to incident electric ﬁeld ratio

Forward to backward scattering power ratio

Cross-polarization ratio

Narrowness of the scattering lobes

40%

75%

40%

40%

30%

75%

40%

40%

15%

75%

40%

40%

15%

75%

40%

40%

10%

75%

40%

40%

0%

75%

40%

40%

for environment sensing through transmission and reception, assume that all the M preamble

sequences are transmitted sequentially with guard intervals in between. The highest M value

reported in the upcoming simulation results is 4096 beams. Assuming a sampling rate of 2Gsps,

the estimate of the longest sensing time is then ≈ 7ms.

Channel generation: The channel matrix, Hd, is generated in two steps. The ﬁrst step is

generating the channel rays using the ray-tracing software, Wireless InSite. The Wireless InSite

propagation model is set to ’X3D’ with 0.1◦ ray-spacing and enabled mode of diffuse scattering.

Up to three reﬂections, one diffraction, and one transmission properties are allowed for each

ray in the Wireless InSite simulation. The diffuse scattering model used is “directive with

backscatter”; this model is ﬁxed across all materials in all the testing scenarios. The chosen

diffuse scattering model creates two scattering lobes; a forward lobe of diffuse scattered power

centered on the direction of specular reﬂection and a backward lobe centered on the opposite

direction of incidence. The diffuse scattering parameters of the different materials are summarized

in Table I. The values reported in Table I follow the ITU default parameter values at 60GHz.

The second step in the sensing channel generation is calculating the delay-d channel matrix out

of the channel paths using the DeepMIMO dataset generation code [45]. Using these channels

and following (4)-(3), the noisy receive sensing sequences are generated. The noise power is

calculated based on a 2GHz bandwidth and a receiver noise ﬁgure of 7dB.

mmWave based depth estimation parameters: The beamforming-combining pair codebook

is designed based on a 100◦ ﬁeld of view centered around the antenna array boresight, a 16/9

scene aspect ratio, and horizontal and vertical oversampling factors of unity. The ground truth

depth maps are generated from Blender using a Blender camera with a 100◦ ﬁeld of view, a focal

length of 13.43mm corresponding to a sensor width of 32mm. The ground truth depth map image

quality is set to 1080p resolution; i.e., 1920 × 1080 pixels. Concerning the massive correlator,

30

Fig. 13: The maps for the one wall scenario are depicted for a separation distance of 7 meters from the AR/VR

device with 16 × 16 UPAs. The depicted maps are the estimated maps (at the top), ground truth maps (at the

bottom), range maps (on the left side), and 1080p depth maps (on the right side). Comparing (a) with (b), the range

map estimation error: MAE = 0.098m. Comparing (c) with (d), the depth map estimation error: MAE = 0.12m.

fest is set to 100 multiple of the sampling frequency fS; i.e., δ = fest
2fS

= 50. Unless mentioned

otherwise, the massive correlator is adopted for range estimation. Throughout this paper, two

performance metrics are used: (i) root-mean-square-error (RMSE) between the estimated map and

the ground truth map to indicate the standard deviation of the estimation error, and (ii) mean-

absolute-error (MAE) to denote the expected value of the estimation error. The two metrics

are deﬁned in (9). Next, we evaluate the performance of our proposed mmWave MIMO depth

estimation approach in four main scenarios: (i) A one wall scenario in Section VIII-B, (ii) a two

walls scenario in Section VIII-C; (iii)) a room with two pillars scenario in Section VIII-D, and

(iv) a conference room scenario in Section VIII-E.

B. One wall scenario

The one wall scenario consists of an AR/VR transceiver facing a wall in free space propagation.

Unless otherwise mentioned, the separation distance between the wall and the transceiver is 7

(a)Estimated range map(c)Estimated depth map(b) Ground truth range map(d) Ground truth depth mapRange RMSE = 0.127m Range MAE    = 0.098mDepth RMSE = 0.153m Depth MAE    = 0.120m31

Fig. 14: The depth maps for the one wall scenario are depicted for different antenna conﬁgurations and codebook

resolutions, for a separation distance of 7 meters. Figures (a), (b), and (c) illustrate the estimated 1080p maps for

8 × 8, 16 × 8, and 16 × 16 UPAs. Figures (d) illustrate the ground truth maps. The top maps are with no codebook

oversampling while the bottom maps are with codebook oversampling factors of two.

meters and the wall building material is concrete. In Fig. 13, we show the estimated range

and depth maps for the one wall scenario compared to the ground truth maps. Fig. 13(a) and

Fig. 13(b) show that the range map estimation error has an average MAE of 0.098m and RMSE

of 0.127m. Further, the depth map estimation error Fig. 13(c) and Fig. 13(d) has an average

MAE of 0.12m and RMSE of 0.153m. Overall, these ﬁgures show that the proposed approaches

can accurately estimate the range/depth maps for a wall at 7m distance from the AR/VR device

with around 10cm error, which highlights the effectiveness of this approach.

Impact of the important system parameters: Next, we brieﬂy evaluate the impact of the

various system parameters on the performance of the proposed mmWave depth map estimation

solution.

• Number of antennas and sensing codebook beams: In Fig. 14, we plot the estimated

and ground-truth depth maps for different number of antennas and codebook oversampling

1.a) 8x81.b) 16x81.d) Ground truthDepth RMSE = 0.208m Depth MAE   =  0.161m1.c) 16x16Depth RMSE = 0.153m Depth MAE   =  0.120mDepth RMSE = 0.395m Depth MAE   =  0.304m2.b) 16x82.d) Ground truthDepth RMSE = 0.202m Depth MAE   =  0.153m2.c) 16x16Depth RMSE = 0.148m Depth MAE   =  0.111m2.a) 8x8Depth RMSE = 0.329m Depth MAE   =  0.239m32

(a) 12x2

(b) 8x3

(c) 6x4

(d) Ground truth

Depth RMSE = 0.505m

Depth RMSE = 0.512m

Depth RMSE = 0.473m

Depth MAE = 0.392m

Depth MAE = 0.397m

Depth MAE = 0.372m

Fig. 15: The 1080p depth maps for the one wall scenario are depicted at different antenna conﬁgurations, for

a separation distance of 7 meters. The same number of antenna elements is used (24 elements) and codebook

oversampling factors of four are employed. Figures (a), (b), and (c) illustrate the estimated maps for 12 × 2, 8 × 3,

and 6 × 4 UPAs. Figures (d) illustrate the ground truth depth map.

(a) Continuous phase shifts

(b) 2-bit discrete phase shifts

(c) Ground truth

Depth RMSE = 0.148m

Depth RMSE = 0.149m

Depth MAE = 0.1106m

Depth MAE = 0.1111m

Fig. 16: The 1080p depth maps for the one wall scenario at 7m separation distance are estimated for two cases of

the RF phase shifters at the AR/VR device: (a) continuous phase shifts and (b) 2-bit quantized phase shifts. 16 × 16

UPA is employed with codebook oversampling factors of two. Figures (c) illustrate the ground truth depth map.

factors. As illustrated, the depth estimation accuracy can generally improve by increasing

the number of antennas and/or the codebook oversampling factors. This comes with the

cost of deploying more antennas at the AR/VR device or employing more beams, which

translates to a longer sensing time. In Fig. 15, we plot the estimated and ground-truth depth

maps for different antenna conﬁgurations using the same number of antenna elements. As

depicted, the depth estimation accuracy depends on the UPA conﬁguration, with the best

2004006008001000120014001600180010020030040050060070080090010005.566.577.52004006008001000120014001600180010020030040050060070080090010005.566.577.52004006008001000120014001600180010020030040050060070080090010005.566.577.52004006008001000120014001600180010020030040050060070080090010005.566.577.52004006008001000120014001600180010020030040050060070080090010006.46.66.877.27.47.62004006008001000120014001600180010020030040050060070080090010006.46.66.877.27.47.62004006008001000120014001600180010020030040050060070080090010006.46.66.877.27.47.633

(a)

(b)

Fig. 17: For the one wall scenario, the error performance of the proposed mmWave MIMO based depth estimation

solution is evaluated under different error metrics in (a) and is evaluated for different preamble sequence lengths

in (b). The wall is 7 meters away from the AR/VR device with 16 × 16 UPAs. The ﬁgures show the robustness

of the developed approach under relatively low SNR regime. Note that the displayed transmit power range in (b)

corresponds to average SNR range of −20.7dB to −0.7dB.

conﬁguration being the 6 × 4 UPA because of its closeness to the 1080p aspect ratio.

• RF phase shift quantization: As previously described in Section VI-B, the phase quanti-

zation of the RF phase shifters in the AR/VR transceiver architecture produces a noticeable

change in the radiation pattern shape of the sidelobes. To examine the effect of this phase

quantization on the estimated depth maps, Fig. 16 shows the comparison of the estimated

depth maps for two cases of the RF phase shifters at the AR/VR device: (a) continuous phase

shift and (b) 2-bit quantized phase shifts. As depicted, the phase quantization contributes

with a small negative impact on the depth map estimation accuracy for the one wall scenario

at a separation distance of 7 meters.

• Transmit sensing power: In Fig. 17a, we investigate the effect of changing the transmit

power on the depth map estimation accuracy. The SNR value of 0dB corresponds to a

transmit power of 15dBm. This ﬁgure shows that a transmit power of 5dBm (SNR of

−10dB) could be sufﬁcient to reach around 10cm error for the depth estimation accuracy.

• Preamble sequence length: The estimation error versus transmit power is depicted in

-20-15-10-5010-1100101-505101510-110010134

Fig. 18: The error performance of the proposed mmWave MIMO based depth estimation solution is evaluated across

different separation distances for the one wall scenario. The estimation error starts from ≈ 1.5m at a 1m distance

and reaches around 10cm at a 7 meters distance.

Fig. 17b for different values of preamble sequence lengths, namely preambles with 50,

100, 1000, and 3000 symbols. As shown in this ﬁgure, increasing the preamble sequence

length improves the depth estimation accuracy at the expense of increased sensing time and

post-processing complexity.

• Separation distance between the AR/VR device and the facing wall: Fig. 18 investigates

the impact of increasing the depth value on the depth estimation accuracy. As shown in

this ﬁgure, the larger the distance between the AR/VR device and the facing surface, the

larger the error in the depth estimate, which is expected. This ﬁgure also highlights some

advantage for the bicubic interpolation compared to the other interpolation methods.

• The surface material: Now, we evaluate the performance of the proposed approach for

different surface materials. More speciﬁcally, we summarize in Table II the range map

MAE for different candidates of the wall material. Overall, we can notice some correlation

between the estimation accuracy and the scattered to incident power ratio property of the

materials, which are summarized in Table I.

1234567Separation distance (meters)10-210-1Estimation error (meters)1080p depth map RMSE [constant interp.]1080p depth map RMSE [lanczos3 interp.] 1080p depth map RMSE [bicubic interp.] 1080p depth map MAE [constant interp.] 1080p depth map MAE [lanczos3 interp.] 1080p depth map MAE [bicubic interp.]35

(a)

(b)

Fig. 19: (a) The adopted two walls scenario is illustrated. (b) The maps for the two walls scenario are depicted.

The AR/VR device is employed with 16 × 16 UPAs. The depicted maps are the estimated maps (at the top), ground

truth maps (at the bottom), range maps (on the left side), and 1080p depth maps (on the right side). Comparing

(a) with (b), the range map estimation error: MAE = 0.052m. Comparing (c) with (d), the depth map estimation

error: MAE = 0.046m.

TABLE II: The estimation error results of the one wall scenario for different wall materials

Estimation Error (meters) Concrete Ceilingboard Wood

Floorboard Layered Drywall

Glass

Basic Correlator

Massive Correlator

0.101

0.0983

0.099

0.097

0.101

0.0983

0.0984

0.0983

0.103

0.101

12.697

15.498

C. Two walls scenario

The two walls scenario consists of one AR/VR device facing two walls in free space propa-

gation as depicted in Fig. 19a. The separation distance between the front wall and the AR/VR

device is 1m while the separation between the back wall and the AR/VR device is 2m. The

walls’ building material is concrete. Each wall consists of 2, 048 faceted faces, and each face

contributing with at most one backscattered ray. The purpose behind studying this scenario is to

test the alignment of the estimated map compared to the ground truth depth map. The results of

<latexit sha1_base64="f2yzimwbR/Dgjzp6tZ360fHRqNI=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit><latexit sha1_base64="l29WxoUb9DEbvmhLG7jHtZ0OU24=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4KokIeix68diC/YA2lM120q7dbMLuRgihv8CLB0W8+pO8+W/ctjlo64OBx3szzMwLEsG1cd1vZ219Y3Nru7RT3t3bPzisHB23dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5m/mdJ1Sax/LBZAn6ER1JHnJGjZWa2aBSdWvuHGSVeAWpQoHGoPLVH8YsjVAaJqjWPc9NjJ9TZTgTOC33U40JZRM6wp6lkkao/Xx+6JScW2VIwljZkobM1d8TOY20zqLAdkbUjPWyNxP/83qpCW/8nMskNSjZYlGYCmJiMvuaDLlCZkRmCWWK21sJG1NFmbHZlG0I3vLLq6R9WfPcmte8qtZvizhKcApncAEeXEMd7qEBLWCA8Ayv8OY8Oi/Ou/OxaF1zipkT+APn8wfnvYz9</latexit><latexit sha1_base64="HDzXchlsPlmuEyZZ/9zFJ+iVC6I=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N0IN/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB6UGM/g==</latexit>Small TargetAR/VR deviceDepthRangeAR/VR device1mDepth1mDepth(a)Estimated range map(c)Estimated depth map(b)Ground truth range map(d)Ground truth depth mapRange RMSE = 0.100m Range MAE   = 0.052mDepth RMSE = 0.085m Depth MAE     = 0.046m36

(a) The room with two pillars

(b) The room scene from the door position

Fig. 20: Figure (a) illustrates the bird view of the room with two pillars. Figure (b) shows the scene from the

AR/VR device position, centered at the front door. The 5m×5m room consists of a concrete ﬂoor plan with two

wood pillars in the middle of the room. The wood pillars are at 2 meters distance from the AR/VR device.

this test are illustrated in Fig. 19b, where the estimated range and depth maps are compared to

the ground truth maps. As shown in Fig. 19b, the two edges of the front wall in the estimated

maps align reasonably well with the one displayed in the ground truth maps. This highlights the

promising performance of proposed mmWave based depth estimation solution.

D. A room with two pillars

In this scenario, we consider a 5m×5m room where one AR/VR device is centered at the

front door of the room, as depicted in Fig. 20. The room consists of a concrete ﬂoor plan

with two wood pillars in the middle of the room. The wood pillars are at 2 meters distance

from the AR/VR transceiver. The ﬂoor plan consists of 15, 488 faceted faces whereas each of

the wood pillars consists of 3, 072 faceted faces. Note that the ceiling of the ﬂoor plan is set

to the invisible mode for visibility purposes only. For the estimation error assessment of the

indoor space scenario, Fig. 21 shows the comparison between estimated and ground truth maps

for 16 × 16 UPA antennas with a codebook oversampling factors of four in both azimuth and

elevation dimensions. First, Fig. 21(a) with Fig. 21(b) show the estimate and ground truth range

maps, which have a MAE of 0.139m and RMSE of 0.355m. For the depth maps, Fig. 21(c) with

Fig. 21(d) represent 1080p maps with estimation error of(i) 0.126m for the MAE and 0.356m

for the RMSE with nearest neighbor interpolation, and (ii) 0.123m for the MAE and 0.328m

37

Fig. 21: The maps for the room with two pillars are depicted. 16 × 16 UPAs are employed with codebook

oversampling factors of four. The depicted maps are the estimated maps (at the top), ground truth maps (at the

bottom), range maps (on the left side), and 1080p depth maps (on the right side). Comparing (a) with (b), the range

map estimation error: MAE = 0.139m. Comparing (c) with (d), the depth map estimation error: MAE = 0.126m.

for the RMSE with bicubic interpolation. From observing the difference in maps, the mmWave

reasonably recover most of the depth information of the scene with low codebook resolution

(16 × 16) compared to the ground truth 1080p resolution. With narrower transmit and receive

beams, i.e. more antenna elements, the estimation accuracy is expected to further improve.

The depth map estimation accuracy for this scenario is also evaluated at different SNRs in

Fig. 22. In this ﬁgure, we adopt the model and system parameters used in Fig. 21 with 16 × 16

UPAs and oversampling factors of four. It is also worth mentioning that 0dB SNR corresponds

to −20dBm transmit power in our setup. As shown in Fig. 22, the estimated depth maps have

MAE of almost 10cm at 0dB, which highlights the promising performance of our proposed depth

map estimation approach at relatively low SNRs and in indoor room with several surfaces and

(a)Estimated range map(c)Estimated depth map(b)Ground truth range map(d)Ground truth depth mapRange RMSE = 0.355mRange MAE   = 0.139mDepth RMSE = 0.356m Depth MAE    = 0.126m38

Fig. 22: For the room with two pillars, the error performance of the proposed mmWave MIMO based depth

estimation is evaluated for different error metrics. 16 × 16 UPAs are employed with a codebook oversampling

factors of four in both dimensions. This ﬁgure shows the robustness of the proposed mmWave MIMO based depth

estimation under relatively low SNR regime.

different materials. This will be further emphasized in the following subsection.

E. Conference room scenario

In this scenario, we consider the conference room shown in Fig. 23. The ceiling of the indoor

space is set to the invisible mode for visibility purpose only. The 10m×10m indoor space has a

6m×6m conference room with glass walls. The indoor space walls are made of layered drywall,

the ceiling is made of ceiling board, and the ﬂoor is made of ﬂoorboard. The conference room

chairs and tables are made of wood. The conference room door opening is 1m in width and 2.7m

in height. The number of facets for each item in the indoor space is as follows: 2, 048 facets for

the layered drywall, 2, 048 facets for the ﬂoorboard, and 2, 048 facets for the ceiling board. In

addition, the number of facets for each item in the conference room is as follows: 1, 568 facets

for the glass wall, 4, 446 facets for the table, 21, 192 facets for the ofﬁce chairs. The conference

room scenario consists of two AR/VR devices for two scenes under study — the ﬁrst device is

centered at the front door of the conference room while the second transceiver is placed outside

-20-15-10-505Signal-to-noise ratio (dB)10-1100Estimation error (meters)Radial distance map RMSE1080p depth map RMSE [bicubic interp.]Radial distance map MAE1080p depth map MAE [bicubic interp.]39

Fig. 23: (a) the bird view of the conference room scenario; (b) and (c) the scenes under study. The 10m×10m

indoor space contain a 6m×6m conference room in glass. the indoor space walls are made from layered drywall,

the ceiling is made from ceiling board and the ﬂoor is made from ﬂoorboard. The conference room chairs and

tables are made from wood.

of the conference room facing the other glass facet. The scenes captured by the AR/VR camera

for the two cases are shown in Fig. 23(b) and Fig. 23(c).

One main motivation for leveraging mmWave MIMO to estimate the depth maps (compared to

RGB based depth estimation approaches) is the expected higher efﬁciency in detecting transparent

and dark objects. In Fig. 24, we compare our mmWave MIMO based depth estimation approach

with the RGB based depth estimation approach, detailed in [2], for the two considered conference

room scenarios. It’s worth emphasizing here that the algorithms in [2] achieve considerably good

depth accuracy when tested on the NYU depth V2 dataset [46]. As shown in Fig. 24, the mmWave

MIMO based estimator outperforms the RGB based estimator in recognizing transparent and dark

objects. For the ﬁrst scene, the glass wall was not detected by the RGB estimator. Also, in the

presence of a scene with low illumination, the mmWave MIMO based estimator performance

shows robustness in the estimation accuracy compared to the RGB based estimator. Figure 1.c)

and 2.c) were generated with the aid of the SLR approach in Section VI-B, with δH = 2, δV = 3.

As for the second scene, the RGB based estimator is unable to detect the transparent glass

(a)Conference room scenario(b)First scene(c)Second scene40

Fig. 24: For the conference room scenario, the proposed mmWave MIMO based depth estimation is compared with

the RGB based depth estimation in [2]. 16 × 16 UPAs are employed with codebook oversampling factors of four.

The depicted maps are the maps of the ﬁrst scene with lights on/off (The top two rows) and the second scene (the

bottom row). (a) The scenes under study; (b) the estimated maps from monocular RGB images; (c) the estimated

maps from our proposed solution; (d) the ground truth depth maps.

compared to the mmWave MIMO based estimator. Interestingly, despite the fact that the glass

scattering ratio is 0% based on Table I, the conference room glass wall is partially recovered

by the mmWave MIMO based estimator due to the boresight reﬂection path. This makes the

wireless AR/VR experience safer by providing the ability to detect transparent surfaces. All these

promising results highlight the potential of leveraging the proposed mmWave MIMO based depth

map estimation approaches for immersive AR/VR experience.

1.a) First scene1.b) RGB1.d) Ground truthDepth RMSE = 0.974mDepth MAE   = 0.767m1.c) mmWave MIMODepth RMSE = 0.997m Depth MAE   = 0.518mDepth RMSE = 1.162mDepth MAE   = 0.921mDepth RMSE = 2.283m Depth MAE   = 1.919mDepth RMSE = 0.634m Depth MAE   = 0.381m2.b) RGB2.d) Ground truth3.b) RGB3.c) mmWave MIMO3.d) Ground truth2.c) mmWave MIMODepth RMSE = 0.997m Depth MAE   = 0.518m2.a) First scene3.a) Second scene(lights on)(lights off)41

IX. CONCLUSION

In this paper, we considered the problem of estimating accurate depth maps for AR/VR devices,

which is an essential goal for immersive mixed-reality experience. For this problem, we proposed

leveraging the mmWave communication systems that are deployed on the AR/VR devices to

estimate and build high-resolution depth maps. We formulated the communication-constrained

depth map sensing problem and proposed a comprehensive framework for realizing this objective.

The proposed framework includes (i) the construction of depth map speciﬁc sensing codebooks

using practical mmWave antenna arrays and (ii) the development of efﬁcient post-processing so-

lutions for jointly processing the receive signals from the multiple sensing beams and estimating

high-resolution depth maps. Simulations using accurate 3D ray-tracing models conﬁrmed the

promising accuracy of our proposed mmWave based depth map estimation approach in various

environment scenarios. In particular, the results show that the proposed approach can construct

relatively high-resolution depth maps with less than 10cm error using practical mmWave systems.

This highlights the potential of leveraging this solution to complement RGB-D based depth maps

and realize immersive depth perception for wireless virtual/augmented reality systems.

X. ACKNOWLEDGMENT

This work is sponsored by the Facebook Reality Lab. The authors would like to thank our

WSLCL group members, Muhammad Alrabeiah and Andrew Hredzak, for their valuable help

in building the Blender models and benchmarking the performance with the RGB based depth

estimation approaches.

REFERENCES

[1] VIVE, “VIVE Wireless Adapter.” [Online]. Available: https://www.vive.com/us/wireless-adapter/

[2] J. Hu, M. Ozay, Y. Zhang, and T. Okatani, “Revisiting Single Image Depth Estimation: Toward Higher Resolution Maps

With Accurate Object Boundaries,” in Proc. of the IEEE Winter Conference on Applications of Computer Vision (WACV),

2019, pp. 1043–1051.

[3] F. Mal and S. Karaman, “Sparse-to-Dense: Depth Prediction from Sparse Depth Samples and a Single Image,” in Proc.

of International Conference on Robotics and Automation (ICRA).

IEEE, 2018, pp. 1–8.

[4] G. Riegler, Y. Liao, S. Donne, V. Koltun, and A. Geiger, “Connecting the Dots: Learning Representations for Active

Monocular Depth Estimation,” in Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),

2019, pp. 7624–7633.

[5] Y. Zhang, S. Khamis, C. Rhemann, J. Valentin, A. Kowdle, V. Tankovich, M. Schoenberg, S. Izadi, T. Funkhouser, and

S. Fanello, “Activestereonet: End-to-end Self-supervised Learning for Active Stereo Systems,” in Proc. of the European

Conference on Computer Vision (ECCV), 2018, pp. 784–801.

42

[6] C. Cheng, Y. Wang, C. Wei, C. Chen, and L. Lin, “IR Pattern Characteristics For Active Stereo Matching,” US Patent

16/359,699, Oct. 3, 2019.

[7] T. Gruber, M. Kokhova, W. Ritter, N. Haala, and K. Dictmayer, “Learning Super-resolved Depth from Active gated

Imaging,” in Proc. of International Conference on Intelligent Transportation Systems (ITSC).

IEEE, 2018, pp. 3051–

3058.

[8] T. Gruber, F. Julca-Aguilar, M. Bijelic, and F. Heide, “Gated2depth: Real-time Dense Lidar From Gated Images,” in Proc.

of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 1506–1516.

[9]

Intel, “Intel RealSense Depth Camera D435.” [Online]. Available: https://www.intelrealsense.com/depth-camera-d435/

[10] Y. Wang, W.-L. Chao, D. Garg, B. Hariharan, M. Campbell, and K. Q. Weinberger, “Pseudo-lidar From Visual Depth

Estimation: Bridging the Gap in 3D Object Detection for Autonomous Driving,” in Proc. of the IEEE Conference on

Computer Vision and Pattern Recognition (CVPR), 2019, pp. 8445–8453.

[11] Y. Zhang and T. Funkhouser, “Deep Depth Completion of a Single RGB-D Image,” in Proc. of the IEEE Conference on

Computer Vision and Pattern Recognition (CVPR), June 2018.

[12] J. A. Zhang, A. Cantoni, X. Huang, Y. J. Guo, and R. W. Heath, “Joint communications and sensing using two steerable

analog antenna arrays,” in 2017 IEEE 85th Vehicular Technology Conference (VTC Spring), June 2017, pp. 1–5.

[13] P. Kumari, J. Choi, N. Gonz´alez-Prelcic, and R. W. Heath, “Ieee 802.11ad-based radar: An approach to joint vehicular

communication-radar system,” IEEE Transactions on Vehicular Technology, vol. 67, no. 4, pp. 3012–3027, April 2018.

[14] J. Lien, N. Gillian, M. E. Karagozler, P. Amihood, C. Schwesig, E. Olson, H. Raja, and I. Poupyrev, “Soli: Ubiquitous

gesture sensing with millimeter wave radar,” ACM Trans. Graph., vol. 35, no. 4, Jul. 2016. [Online]. Available:

https://doi.org/10.1145/2897824.2925953

[15] T. Wei and X. Zhang, “Mtrack: High-precision passive tracking using millimeter wave radios,” in Proceedings of the 21st

Annual International Conference on Mobile Computing and Networking, ser. MobiCom ’15. New York, NY, USA:

Association for Computing Machinery, 2015, p. 117–129. [Online]. Available: https://doi.org/10.1145/2789168.2790113

[16] Remcom, “Wireless insite.” [Online]. Available: http://www.remcom.com/wireless-insite

[17] “Blender 2.80.” [Online]. Available: http://www.blender.org

[18] “Blenderkit.” [Online]. Available: https://www.blenderkit.com/

[19] “Blender demo ﬁles.” [Online]. Available: https://www.blender.org/download/demo-ﬁles/

[20] “Turbosquid.” [Online]. Available: https://www.turbosquid.com/Search/3D-Models

[21] “Free3d.” [Online]. Available: https://free3d.com/

[22] A. Sabharwal, P. Schniter, D. Guo, D. W. Bliss, S. Rangarajan, and R. Wichman, “In-Band Full-Duplex Wireless: Challenges

and Opportunities,” IEEE Journal on Selected Areas in Communications, vol. 32, no. 9, pp. 1637–1652, 2014.

[23] N. A. Estep, D. L. Sounas, J. Soric, and A. Al`u, “Magnetic-Free Non-Reciprocity and Isolation Based on Parametrically

Modulated Coupled-Resonator Loops,” Nature Physics, vol. 10, no. 12, pp. 923–927, 2014.

[24] T. Dinc and H. Krishnaswamy, “A 28GHz Magnetic-Free Non-Reciprocal Passive CMOS Circulator Based on Spatio-

Temporal Conductance Modulation,” in Proc. of IEEE International Solid-State Circuits Conference (ISSCC).

IEEE,

2017, pp. 294–295.

[25] J. Zhou, N. Reiskarimian, and H. Krishnaswamy, “Receiver with Integrated Magnetic-Free N-Path-Filter-Based Non-

Reciprocal Circulator and Baseband Self-Interference Cancellation for Full-Duplex Wireless,” in Proc. of IEEE International

Solid-State Circuits Conference (ISSCC).

Institute of Electrical and Electronics Engineers Inc., 2016, pp. 178–180.

[26] A. Nagulu and H. Krishnaswamy, “Non-Magnetic 60GHz SOI CMOS Circulator Based on Loss/Dispersion-Engineered

Switched Bandpass Filters,” in Proc. of IEEE International Solid-State Circuits Conference (ISSCC).

IEEE, 2019, pp.

446–448.

43

[27] R. W. Heath, N. Gonz´alez-Prelcic, S. Rangan, W. Roh, and A. M. Sayeed, “An overview of signal processing techniques

for millimeter wave MIMO systems,” IEEE Journal of Selected Topics in Signal Processing, vol. 10, no. 3, pp. 436–453,

April 2016.

[28] A. Alkhateeb, J. Mo, N. Gonzalez-Prelcic, and R. Heath, “MIMO precoding and combining solutions for millimeter-wave

systems,” IEEE Communications Magazine,, vol. 52, no. 12, pp. 122–131, Dec. 2014.

[29] M. A. Richards, J. A. Scheer, and W. A. Holm, Principles of Modern Radar: Basic Principles. Raleigh, NC, USA:

SciTech Publishing, 2010.

[30] P. Kumari, S. A. Vorobyov, and R. W. Heath Jr, “Adaptive Virtual Waveform Design for Millimeter-Wave Joint

Communication-Radar,” arXiv preprint arXiv:1904.05516, 2019.

[31] Q. Spencer, B. Jeffs, M. Jensen, and A. Swindlehurst, “Modeling the Statistical Time and Angle of Arrival Characteristics

of an Indoor Multipath Channel,” IEEE Journal on Selected Areas in Communications, vol. 18, no. 3, pp. 347–360, 2000.

[32] P. F. Smulders, “Statistical Characterization of 60-GHz Indoor Radio Channels,” IEEE Transactions on Antennas and

Propagation, vol. 57, no. 10, pp. 2820–2829, 2009.

[33] O. El Ayach, S. Rajagopal, S. Abu-Surra, Z. Pi, and R. Heath, “Spatially sparse precoding in millimeter wave MIMO

systems,” IEEE Transactions on Wireless Communications, vol. 13, no. 3, pp. 1499–1513, Mar. 2014.

[34] A. Guerra, F. Guidi, D. Dardari, A. Clemente, and R. D’Errico, “A Millimeter-Wave Indoor Backscattering Channel Model

for Environment Mapping,” IEEE Transactions on Antennas and Propagation, vol. 65, no. 9, pp. 4935–4940, 2017.

[35] S. M. Kay, Fundamentals of Statistical Signal Processing: Estimation Theory. Prentice Hall, 1993.

[36] P. Kumari, J. Choi, N. Gonz´alez-Prelcic, and R. W. Heath, “IEEE 802.11ad-Based Radar: An Approach to Joint Vehicular

Communication-Radar System,” IEEE Transactions on Vehicular Technology, vol. 67, no. 4, pp. 3012–3027, 2018.

[37] P. Bidigare, U. Madhow, R. Mudumbai, and D. Scherber, “Attaining Fundamental Bounds on Timing Synchronization,”

in Proc. of International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2012, pp. 5229–5232.

[38] A. Herschfelt, H. Yu, S. Wu, H. Lee, and D. W. Bliss, “Joint Positioning-Communications System Design: Leveraging

Phase-Accurate Time-of-Flight Estimation and Distributed Coherence,” in Proc. of Asilomar Conference on Signals,

Systems, and Computers (ACSSC), 2018, pp. 433–437.

[39] A. Herschfelt, “Simultaneous Positioning and Communications: Hybrid Radio Architecture, Estimation Techniques, and

Experimental Validation,” Ph.D. dissertation, Arizona State University, 2019.

[40] A. Alkhateeb and R. W. Heath Jr, “Frequency selective hybrid precoding for limited feedback millimeter wave systems,”

submitted to IEEE Transactions on Communications, arXiv preprint arXiv:1510.00609, 2015.

[41] W. L. Melvin and J. A. Scheer, Principles of Modern Radar Vol. II: Advanced Techniques. Edison, NJ, USA: SciTech

Publishing, 2013.

[42] M. I. Dessouky, H. A. Sharshar, and Y. A. Albagory, “Efﬁcient Sidelobe Reduction Technique for Small-sized Concentric

Circular Arrays,” Progress In Electromagnetics Research, vol. 65, pp. 187–200, 2006.

[43] Y. A. Albagory, M. Dessouky, and H. Sharshar, “An Approach for Low Sidelobe Beamforming in Uniform Concentric

Circular Arrays,” Wireless Personal Communications, vol. 43, no. 4, pp. 1363–1368, 2007.

[44] E. Grossi, M. Lops, and L. Venturino, “Detection and Localization of Multiple Targets in IEEE 802.11ad Networks,” in

Proc. of Asilomar Conference on Signals, Systems, and Computers (ACSSC), 2019.

[45] A. Alkhateeb, “Deepmimo: A generic deep learning dataset for millimeter wave and massive MIMO applications,” in Proc.

of Information Theory and Applications Workshop (ITA), San Diego, CA, Feb 2019, pp. 1–8.

[46] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus, “Indoor Segmentation and Support Inference from RGBD Images,” in

Proc. of European Conference on Computer Vision (ECCV), 2012.

