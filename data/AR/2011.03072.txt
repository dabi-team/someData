0
2
0
2

v
o
N
5

]
L
C
.
s
c
[

1
v
2
7
0
3
0
.
1
1
0
2
:
v
i
X
r
a

ALIGNMENT RESTRICTED STREAMING RECURRENT NEURAL NETWORK
TRANSDUCER

Jay Mahadeokar, Yuan Shangguan, Duc Le, Gil Keren, Hang Su, Thong Le, Ching-Feng Yeh
Christian Fuegen, Michael L. Seltzer

Facebook AI
{jaym,yuansg,duchoangle,gilkeren,suhang,tmle,cfyeh,fuegen,mikeseltzer}@fb.com

ABSTRACT
There is a growing interest in the speech community in de-
veloping Recurrent Neural Network Transducer (RNN-T)
models for automatic speech recognition (ASR) applications.
RNN-T is trained with a loss function that does not enforce
temporal alignment of the training transcripts and audio.
As a result, RNN-T models built with uni-directional long
short term memory (LSTM) encoders tend to wait for longer
spans of input audio, before streaming already decoded ASR
tokens. In this work, we propose a modiﬁcation to the RNN-
T loss function and develop Alignment Restricted RNN-T
(Ar-RNN-T) models, which utilize audio-text alignment in-
formation to guide the loss computation. We compare the
proposed method with existing works, such as monotonic
RNN-T, on LibriSpeech and in-house datasets. We show that
the Ar-RNN-T loss provides a reﬁned control to navigate the
trade-offs between the token emission delays and the Word
Error Rate (WER). The Ar-RNN-T models also improve
downstream applications such as the ASR End-pointing by
guaranteeing token emissions within any given range of la-
tency. Moreover, the Ar-RNN-T loss allows for bigger batch
sizes and 4 times higher throughput for our LSTM model
architecture, enabling faster training and convergence on
GPUs.

Index Terms— streaming, ASR, RNN-T, end-pointer, la-

tency, token emission delays

does not enforce on the temporal alignment of the training
transcripts and audio. As a result, RNN-T suffers from token
emission delays - time from when the token is spoken to
when the transcript of the token is emitted [12, 13]. Delayed
emissions of tokens adversely affects user experiences and
downstream applications such as the end-pointer.

Some existing work tried to mitigate the token emission
delays in streaming RNN-Ts. We introduce them in Section 2.
Other works utilized semi-streaming or non-streaming mod-
els to predict better token emission time, at the cost of the
overall latency of the transcripts. In this work, we propose
a novel loss function for streaming RNN-T, and the resul-
tant trained model is called Alignment Restricted RNN-T (Ar-
RNN-T). It utilizes audio-text alignment information to guide
the loss computation. In Section 3, we show that theoretically,
Ar-RNN-T loss function is faster to compute and results in
better audio-token alignment. In Section 4, we empirically
compare our proposed method with existing works such as
monotonic RNN-T training [12] on two data set: LibriSpeech
and voice command.
In the results section, Section 5, we
show improvement in training speed and that when used in
tandem with an end-pointer, Ar-RNN-T provides an unprece-
dentedly reﬁned control over the latency-WER trade-offs of
RNN-T models.

2. RELATED WORK

1. INTRODUCTION

Streaming Automatic Speech Recognition (ASR) researches
have made their way into our everyday products. Smart
speakers can now transcribe utterances in a streaming fash-
ion, allowing users and downstream applications to see instant
output in terms of partial transcriptions [1, 2, 3, 4, 5, 6]. There
is a growing interest in the community to develop end-to-end
(E2E) streaming ASR models, because they can transcribe
accurately and run compactly on edge devices [2, 7, 8, 9].
Amongst these streaming E2E models, Recurrent Neural
Network Transducer (RNN-T) is a candidate for many appli-
cations [10, 11]. RNN-T is trained with a loss function that

The RNN-T model consists of an encoder, a predictor and a
joint network. The encoder processes incoming audio acous-
tic features and the predictor processes the previously emitted
tokens [10, 11]. The joint representation of the encoder and
predictor is then fed into the joint network to predict the next
likely token or a ‘blank’ symbol. Section 3.1 explains the loss
function used to train the standard RNN-T model. Our work
uses an RNN-T architecture setting similar to the RNN-T pre-
sented in [2].

Several works have observed that the RNN-T loss func-
tion does not enforce alignment between audio and token
emission time. Tripathi et al. noticed that the RNN-T models
sometimes do not emit anything for a while before outputting

 
 
 
 
 
 
multiple tokens all together [12]. To mitigate audio-token
timing misalignment,
the authors proposed Mono-RNN-T
models that requires the model to output at most one label
at a frame. Similarly, Sak et al. proposed Neural Network
Aligner (RNA) [14] to train RNN models that do not out-
put multiple labels without processing one input vector. The
HAT [15] variant of the RNN-T loss function is also proposed
to introduce an acoustic-language modality separation in the
ASR pipeline.
In all these works, the exact alignment be-
tween spoken token and emitted token is implicitly modeled.
Our proposed Ar-RNN-T training imposes more restriction
to the audio-token and emission token alignment than the
above-mentioned RNN-T variants. By leveraging audio-text
alignment, we strictly enforce that the intervals between emit-
ted tokens should be bound by a reasonable span of time.

Zeyer et al. also explored the idea of incorporating exter-
nal alignment and using approximations [14] to compute the
RNN-T loss function efﬁciently [16]. They reported train-
ing time and WER of non-streaming bidirectional-LSTM
(BLSTM) models with their approximation framework,
whereas we focus on training streaming RNN-T models,
constructed with single-directional LSTMs. Our empirical
results show – similar to what [13] has discovered – that
the BLSTM-based RNN-T models with more future audio
context behave differently from the single-directional LSTM-
based RNN-T models in their token emission delays. We
analyze the time emission delay of each token, as well as the
end-pointing effects of the Ar-RNN-T training, giving users a
detailed trade-off between WER and token emissions delays.
Token emission delays are also detrimental for the RNN-
T model to decide when to end-point. Li et al.
trained the
RNN-T to predict a special end-of-query symbol [17], (cid:104)/s(cid:105),
to aid the end-pointer [18]. They added penalty losses over
the early and late emissions of the (cid:104)/s(cid:105) token to ensure that it
is emitted at the right time. Their idea of early-emission and
late-emission penalties is similar to Ar-RNN-T loss function.
However, instead of applying these penalties only to the (cid:104)/s(cid:105)
token, Ar-RNN-T imposes such penalty to all tokens.

Other works introduced audio-text alignment informa-
tion into the RNN-T encoder by incorporating an additional
alignment restrictive loss function into model pre-training
processes.
In [19], for instance, Hu et al. used the Cross
Entropy (CE) loss function to pre-train the RNN-T encoder.
Instead of imposing alignment in the pre-training step, our
work directly enforces the alignment during RNN-T training.
Besides RNN-T, there exist works sharing similar inspi-
rations to tackle delayed emission problem for other types of
model loss functions. Senior et al. [20] investigated apply-
ing constraints on the Connectionist Temporal Classiﬁcation
(CTC) alignment to reduce the latency in decoding CTC
based models by limiting the set of search paths used in
the forward-backward algorithm and Povey et al. [21] used
lattice-based technique for constraining phone emission time.
Our algorithm applies to the RNN-T loss lattice to align

word-piece emission times.

3. METHODOLOGY

The Recurrent Neural Network Transducer model (RNN-
T) [10, 11] processes a sequence of acoustic input features
x = (x1, ..., xT ) while predicting a sequence of output labels
y = (y1, ..., yU ) such as characters or word-pieces [22]. The
model consist of three main components. The acoustic model
sub-network processes the sequence x to emit temporal rep-
resentations x = (x1, ..., xT ) such that T ≤ T . The predictor
sub-network consumes previously emitted preﬁx of y to pro-
duce a representation summarizing the current decoding state
y = (y1, ..., yU ). Finally, a joiner component combines the
two representations to output a probability distribution over
possible output tokens k, for every xt and yu:

z(k, t, u) = Pr(k|xt, yu).

(1)

3.1. Standard RNN-T Loss

Similar to the CTC model training processes [23], RNN-T
outputs a set of tokens that consist of predeﬁned units, of-
ten word-pieces [22], and a special blank symbol (cid:15), such that
z((cid:15), t, u) represents the probability of not emitting any new
sub-word unit given t, u.

We deﬁne an alignment h = ((k1, t1), ..., (kT , tT )) as a
sequence of T tokens emissions and deﬁne the probability of
h as:

(cid:89)

Pr(h) =

z(ki, xti , ylast(i)),

(2)

i

where last(i) is the index of the last non-blank label among
k1, ..., ki−1. The RNN-T model then deﬁnes the probability
of a given transcription y to be

Pr(y|x) =

(cid:88)

Pr(h),

(3)

h∈B−1(y)

where B−1(y) is the set of all alignments that correspond to
the transcription y. During training, the RNN-T model at-
tempts to maximize the probability assigned to a correct tran-
scription y∗, by deﬁning the loss as

L = − log(Pr(y∗|x)).

(4)

For gradient and loss computations, [24] deﬁnes the forward
variable α(t, u) to be the probability of emitting y∗
[1:u] during
the ﬁrst t time steps:

α(t, u) = α(t − 1, u)z((cid:15), t − 1, u)+
α(t, u − 1)z(y∗
u, t, u − 1).

(5)

Similarly, the backward variable β(t, u) is deﬁned to be the
probability of emitting y∗

[u+1:U ] from time step t onwards.

The product α(t, u)β(t, u) is the probability of align-
ments that result in emitting the correct target sequence, while

(a)

(b)

Fig. 1: Alignment for RNN-T models. (a) shows token emission delays of a standard RNN-T on an utterance. The gap between
the “token appears in 1-best” and the “ground truth end-time” is the token emission delay; (b) shows the Ar-RNN-T loss
restricted regions for each token. bl is the left-buffer region and br is the right-buffer measured from the spoken token end time.

emitting exactly the ﬁrst u symbols up to time step t. Since
z(k, t, u) only affects the probability of such alignments, it
holds that

∂P r(y∗|x)
∂z(k, t, u)

=

∂α(t, u)β(t, u)
∂z(k, t, u)



= α(t, u)

β(t, u + 1)
β(t + 1, u)
0

otherwise.



if k = yu+1,
if k = (cid:15),

(6)

The above equation allows the derivation of the RNN-T loss
gradient with respect to the joiner outputs, which is then fur-
ther propagated to compute the gradient of all network param-
eters.

When computing the RNN-T loss, we sum the probabil-
ities of all possible alignments. In an extreme example, the
loss function may consider an alignment in which all tokens
are emitted at the last time step and therefore, model learns
to wait before emitting tokens. Empirically, Figure 1a shows
that standard RNN-T has a long delay between the token (spo-
ken) end time and the emission time.

3.2. Alignment Restricted RNN-T Loss

In order to have more control on token emissions, we pro-
pose restricting the set of alignments that are considered for
optimization in Eq 3. More formally, we augment the tran-
scription tokens y1, ..., yU with additional alignment labels
a1, ..., aU . Alignment labels can be obtained from a boot-
strap hybrid ASR model, as described in Section 3.3. In ad-
dition, we deﬁne two hyper-parameters bl and br, which de-
termine the left and right boundaries of valid alignments (see
Figure 1b). An alignment h = (k1, t1), ..., (kT , tT ) is con-
sidered valid by the Ar-RNN-T loss if it results in the correct
transcription y∗, and for every i such that ki (cid:54)= (cid:15), meaning
ki = yu for some u, it holds that au − bl ≤ ti ≤ au + br. We
deﬁne the valid time ranges for u to be (vlu, vru) such that
vlu = au − bl and vru = au + br.

The above formulation also allows a convenient computa-

tion of the loss value and the gradient. We deﬁne

z(yu+1, t, u) =

(cid:40)

z(yu+1, t, u) if vlu ≤ t ≤ vru,
0 otherwise.

(7)

z((cid:15), t, u) = z((cid:15), t, u).

We replace z in Equation 5 for the forward α(t, u) computa-
tion with z, and likewise for the backward β(t, u) computa-
tion. Once the forward and backward variables are replaced
with their alignment restricted counterparts, the computation
of the loss and gradient in Eq. 6 is identical to the standard
RNN-T loss case.

3.3. Alignment Label Generation

To obtain alignment labels a1, ..., aU we perform forced
alignment using a traditional hybrid acoustic model trained
using cross-entropy criterion on chenone targets [25]. Chenone
model’s alignment contains the silence token (SIL), which we
ignore during ArRNN-T training because it is equivalent to
moving along the time-dimension of the RNN-T lattice with-
out emitting new tokens. The hybrid model provides word
level start and end times (sj, ej) for each word wj in the tran-
scription. In contrast, our RNN-T models use word-pieces.
We propose two strategies for obtaining word-piece align-
ments. The ﬁrst strategy (AS1) is to simply use ai = ej
for all i, j such that the word-piece yi is part of the word
wj. The seconds strategy (AS2) is to evenly split the emitted
(sj, ej) between the word-pieces that belong to the appropri-
ate word, in the following manner: assume the word wj is
comprised of r(cid:48) word-pieces yi, ..., yi+r(cid:48)−1. We then set for
every 1 ≤ r ≤ r(cid:48): ai+r−1 = sj + r
r(cid:48) (ej − sj). Section 4
contains results with both strategies.

3.4. Training Optimizations

3.4.1. Optimizing Memory

Let B denote the batch size containing training samples
S = {s1, s2...sB}; T , U be the maximum number of encoder

time-steps and output symbols for any sample si; D be the
number of word-piece targets. Our baseline implementation
uses function merging memory optimizations described in
[26] which fuses the loss with softmax. Implementation wise
for R-NNT loss we must compute and store α(t, u), β(t, u),
z(k, t, u) where k ∈ {yu, (cid:15)} which take O(B × T × U )
memory and gradients(b, t, u, d) take O(B × T × U × D)
memory.

As described in 3.2 to compute Ar-RNN-T loss we only
need to consider α(t, u) and β(t, u) which fall within the re-
striction constraints. For an efﬁcient implementation, during
training for every sample si we pre-compute the valid time
ranges tiu ∈ (vlu, vru) for every yu ∈ si. Let Vsi denote the
number of valid time-steps to be considered for si. Similar to
ideas described in [26] Sec 3.1 we then concatenate Vsi for
si ∈ S in one dimension to obtain a 2D tensor of dimensions
((cid:80)
i Vsi is O(B × (T + U × (bl + br))).
This memory optimization is applicable to the joiner and all
subsequent operations including loss and it allows us to scal-
ing the model training with 4 times the batch-size on GPU
clusters.

i Vsi, D) where (cid:80)

3.4.2. Optimizing GPU Compute

A naive implementation that uses valid tensor locations tiu
can be achieved using slice operation in a deep-learning li-
brary like Pytorch [27]. To further improve throughput, we
implement a custom join operation that invokes O((cid:80)
i Vsi ×
D) cuda threads to update the joined output tensor for forward
operation, each thread requires O(B + U ) time to execute us-
ing pre-computed indexes. For backward operation we invoke
O(B × (T + U ) × D) cuda threads to update encoder and
predictor gradients. We perform similar optimizations while
computing z(k, t, u) values inside the loss. Overall combined
with an increased batch size, this optimization enables us to
scale training throughput by 4 times using the 37M parameter
LSTM architecture described in 4.1.

Other optimization techniques like mixed precision train-
ing [28] are known to improve training throughput, but come
at a cost of sacriﬁcing accuracy or training stability. The
proposed approach does not involve any approximations, and
can be combined with mixed-precision training for added
throughput improvements.

4. EXPERIMENTAL SETUP

We trained and evaluated Ar-RNN-T models on a publicly
available data set, the Librispeech [29] corpus, as well as on
an in-house, voice-search based corpus.

4.1. Librispeech

The Librispeech corpus contains 970 hours of labeled speech.
We extract 40-channel ﬁlterbanks features computed from a

25ms window with a stride of 10ms. We apply spectrum aug-
mentation [30] with mask parameter (F = 27), and ten time
masks with maximum time-mask ratio (pS = 0.05), and speed
perturbation (perturbation ratio 0.9, 1.0, 1.1).

We implemented an RNN-T model architecture similar to
that in [2]. The encoder consists of 8 layers of LSTMs with
Layer Norm with 640 hidden units, with a two times time
dilation step after the second and fourth LSTM layers. The
decoder consists of 2 layers of LSTM with 256 hidden units.
Both encoder and decoder project embeddings of 1024 di-
mensions. The joint layer consists of a simple DNN layer and
a softmax layer, predicting a word-piece output of size 4096.
We experiment with the amount of model look-ahead
for the LSTM cells in the RNN-T. Model look-ahead is the
amount of future context (or audio frames) that are provided
as input to the model. Standard LSTM layers do not re-
quire any model look-ahead. We stack 10 frames with a
stride of 1 as input to the standard LSTM based RNN-T en-
coder to providing a total of 100ms audios per decoding step.
When latency-controlled Bidirectional LSTM (LC-BLSTM)
cells [31] are used to build the RNN speech encoder [25],
it operates on 1.28s chunks with 200ms look-ahead. Bidi-
rectional LSTM (BLSTM) cells have access to the entire
incoming audio (thus maximum look-ahead). Thus, BLSTM
based RNN-T models are non-streaming.

4.2. Voice Command

The voice command data set combines two sources. The ﬁrst
source is in-house, human transcribed data recorded via mo-
bile devices by 20k crowd-sourced workers. The data is in the
voice assistant domain, and is anonymized with personally
identiﬁable information (PII) removed. We distort the col-
lected audio using simulated reverberation and add randomly
sampled additive background noise extracted from publicly
available videos. The second source came from 1.2 million
voice commands (1K hours), sampled from production trafﬁc
of smart speakers. The content of the voice commands in-
clude communication, media or music requests etc, with PII
removed, audio anonymized and morphed. Speed perturba-
tions [32] are applied to this dataset to create two additional
training data at 0.9 and 1.1 times the original speed. We ap-
plied distortion and additive noise to the speed perturbed data,
resulting in a corpus of 38.6M utterances (31K hours) in total.
Our evaluation data consists of 5K hand-transcribed
anonymized utterances from volunteer participants in Smart
speaker’s in-house pilot program. To measure end-pointing
performance thoroughly, we append 2 second silence to the
end of all utterances in the voice command evaluation dataset
to mitigate no end-point circumstances. The end-pointing
locations are manually annotated by transcribers.

The RNN-T model for voice command has the same ar-
chitecture described in Section 4.1. The targets for the RNN-
T training are word-pieces from a pre-trained sentence piece

model [33].

4.4.2. End-pointing metrics

4.3. Token End Time and Finalization delay

We analyze the delay in the emission time of ASR words dur-
ing inference from the actual time when the word has been
spoken in audio. We described the token timing alignment in
Section 3.3. We deﬁne Ground Truth End Time (ETgt) as the
end time of token as per the alignment using a hybrid ASR
model.

During inference, a breadth-ﬁrst beam search algorithm
explores several paths in the RNN-T decoding lattice, where
each path is a sequence of ASR tokens (including blanks).
Each such path can deﬁne an alignment for the predicted
hypothesis text and audio. We deﬁne ASR Token end time
(ETasr) to be the time when the token / word was emit-
ted according to the most probable path encountered during
RNN-T decoding. Note that we use ETasr for downstream
applications like static end-pointing. We also deﬁne the To-
ken Finalization time (F Tasr) as the ﬁrst time when the token
appeared in 1-best partial hypothesis.

Given above deﬁnitions, we deﬁne the token end-time de-
lay (ED) as ED = ETasr − ETgt and token ﬁnalization
delay (F D) as F D = F Tasr − ETgt.

4.4. ASR End-pointing

To compare to previous work [18], we also study the impact of
Ar-RNN-T training on the behavior of the end-pointer, which
determines if the user has stopped speaking during inference.
Accurate and timely end-pointing improves user experience
while early or aggressive end-pointing may lead to the user
being cut-off and deletion errors in the ASR hypotheses.

4.4.1. End-pointers

We study the behaviors of streaming RNN-T combined with
the following end-pointing solutions:

Static End-pointer is a module that makes end-point de-
cisions by looking at 1-best ASR partial hypothesis. An end-
point decision is made when the length of trailing silence
(ETasr) exceeds the static threshold Tstatic.

Neural End-pointer (NEP) is a neural-network which
takes in audio features as inputs and predicts the end-of-query
(eoq) directly, as described in [34]. The neural end-pointer
uses parameters alphaeoq and Teoq to determine an end-point,
i.e. end-pointing when P (eoq) >= alphaeoq for at-least Teoq
consecutive milli-seconds.

As described in [18], [17] we can also train a RNN-T sys-

tem to predict end of sentence (EOS) symbol.

E2E End-pointer uses the probability of EOS symbol to
make end-pointing decisions. More concretely, we use tun-
able parameters alphae2e and Te2e and make end-point deci-
sions only when the P (eos) >= alphae2e for at-least Te2e
consecutive milli-seconds.

For applications that require real-time feedback, an ideal end-
pointer should end-point as soon as a user stops speaking, so
that results can be immediately processed to interact with the
user. Since we evaluate our solution ofﬂine, we track End-
point Latency [34] which is deﬁned as the difference be-
tween the time end-pointer makes end-pointing decision and
the time user stops speaking as annotated by transcriber. To
study the distribution of end-pointing decisions, we track Av-
erage latency (Lavg) and latency at 90 percentile (Lp90).

To measure the quality of end-pointing solutions, we track
two more metrics. Early cutoff percentage (Early Cut%) is
the percentage of utterances in our which end-pointed ear-
lier than the hand-labelled end time. No end-point percentage
N oEPpct is the percentage of utterances where end-pointer
failed to end-point at all. For optimal user experience, we aim
to minimize Early Cut% and N oEPpct.

5. RESULTS

5.1. LibriSpeech

RNN-
T
Loss

A
B Standard
C
D Mono

Encoder

Avg
ED

Avg
F D

-0.26 N/A
BLSTM
LC-BLSTM -0.13 N/A
0.33
LSTM
0.27

0.26
0.23

LSTM

Test-
clean
WER
2.74
3.3
4.66
4.69

Test-
other
WER
6.9
8.4
11.7
12.0

Table 1: WERs and token emission delay trade-offs for dif-
ferent encoders trained with standard RNN-T on LibriSpeech.
See Section 4.3 for ED and F D deﬁnitions.

Align-
ment

D AS1
E
F AS2
G
H

br

15
15
10
5
0

Avg
ED

0.29
0.26
0.17
0.07
-0.01

Avg
F D

0.36
0.33
0.25
0.16
0.13

Test-
clean
WER
4.8
4.61
4.91
6.38
9.31

Test-
other
WER
11.85
11.89
12.7
15.01
19.53

Table 2: WERs and token emission delays on LibriSpeech for
LSTM-based RNN-T models trained with Ar-RNN-T loss. D
is trained using alignment strategy AS1 (assign word end time
to all sub-words) while E-H are trained with AS2 (equally
split word time among sub-words)

As described in Table 1 and Figure 2, as the WER is
better the more model look-ahead is provided and hence
BLSTM outperforms LC-BLSTM, which in turn is better
than LSTMs. More interestingly, we also study token end-
time delay (ED) for these models and notice that LSTM

Model

EOS

RNN-T

N

Mono-RNN-T N

Ar-RNN-T

Ar-RNN-T

N

Y

End-
pointer
Static
NEP
Static
NEP
Static
NEP
E2E

WER DEL

7.5
18.5
7.6
13.2
7.8
8.3
7.4

4.1
15.5
4.5
10.3
4.2
4.9
4.1

Early
Cut%
1.42
2.06
1.25
1.77
2.53
3.19
2.31

No
EP%
0.33
0.33
0.43
0.68
0.14
0.37
0.12

Avg
ED
0.60

Avg
FD
0.74

0.46

0.52

0.19

0.31

0.19

0.26

Lavg

LP 90

1604
692
1462
609
1228
546
393

1840
1120
1695
1040
1400
990
530

Table 3: Comparing RNN-T with Mono-RNN-T and Ar-RNN-T with different end-pointing models on Voice Command
dataset. We show WER, deletion rate, early cutoffs, percentages with no end-pointing, latency of end-pointing alongside
the Avg ED and Avg F D.

els are trained with bl=0 and br=10. For static end-pointing
we use Tstatic = 1sec. Both the neural end-pointer (NEP)
and the E2E end-pointers use static end-pointer as a fall-back
solution especially when background noise is excessive.

As shown in Table 3, the baseline RNN-T model with the
static end-pointer has high Lavg(1604ms) and LP 90(1840ms),
mainly due to delayed token emissions. Moreover when we
use a neural end-pointer we see a signiﬁcant increase in WER
(7.5% to 18.53%) although the early cutoffs do not increase.
This is mainly because stand-alone neural end-pointer cuts
off utterances before the RNN-T model emits trailing tokens
due to emission delays which leads to an increase in the dele-
tion error (4.1% to 15.5%). Ar-RNN-T solves this problem
as shown in row 6 of Table 3, and we get WER 8.28 and Lavg
546ms. However LP 90 is still high (990ms) mainly because
static end-pointer kicks in for these 90+ percentile utterances.
Similar to [17], we also train the RNN-T model with an
EOS symbol, but we observed that the EOS symbol is ex-
tremely delayed and the model tends to emit it towards the
end of audio. [18] solves this problem by imposing early /
late penalty on EOS. With Ar-RNN-T, we require no such
penalty for timely EOS emission. With E2E end-pointer
Ar-RNN-T can achieve best trade-offs for Lavg (393ms) and
WER (7.44). Moreover, the E2E end-pointer with Ar-RNN-T
model signiﬁcant improves the LP 90 to 530ms.

6. CONCLUSION

In this work, we present a detailed analysis of RNN-T model’s
token emission delays and its impact on the downstream ap-
plications. We propose a modiﬁcation to the RNN-T loss that
uses alignment information to restrict the paths being opti-
mized during training. We call our solution Alignment Re-
stricted RNN-T and show that we can control the token delays
for RNN-T models systematically using tunable parameters
during training, while also signiﬁcantly improving training
throughput. Using our proposed solution, we show that we
can improve the accuracy of downstream applications such as
the ASR end-pointing system and signiﬁcantly reduce latency
and early cut-offs.

Fig. 2: On LibriSpeech, Ar-RNN-T models allow reﬁned con-
trol over the trade-offs between WER and average token end-
time delays (ED).

model has much higher ED compared to BLSTM models.
The BLSTM model’s Avg ED is negative, which means the
most probable path emission times on average are earlier than
ground truth end times ETgt. This is likely because RNN-T
loss is able to use the future context built into model and
conﬁdently emit tokens earlier. However, for LSTM model
the Avg ED and Avg F D is 263ms and 333ms respectively
indicating that the model learns to wait until it sees future
frames before it conﬁdently emits the ASR tokens.

For Ar-RNN-T, we run experiments using a ﬁxed bl of 0
and sweep br. We observe that Ar-RNN-T with br = 15 has
better WER than the standard RNN-T. WER degrades as we
impose more constraints on the buffer size. As expected we
are also able to limit the emission delays with br. We plot the
WER vis-`a-vis average ED trade-off in Figure 2. Ar-RNN-
T, with a conﬁgurable left-right buffer, provides a knob for
researchers to tune and slide their model along the trade-off
curve.

5.2. Voice Command and End-pointing

On the voice command dataset, we ﬁrst train an RNN-T
model as described in 4.2. We then ﬁne-tune the RNN-T
model with Ar-RNN-T and Mono-RNN-T losses; we also
ﬁne-tune the RNN-T model with Ar-RNN-T loss and EOS
symbol appended to targets for comparison. Ar-RNN-T mod-

7. REFERENCES

[1] Surabhi Punjabi, Harish Arsikere, Zeynab Raeesy,
Chander Chandak, Nikhil Bhave, Ankish Bansal,
Markus M¨uller, Sergio Murillo, Ariya Rastrow, Sri
Garimella, et al.,
“Streaming end-to-end bilingual
asr systems with joint language identiﬁcation,” arXiv
preprint arXiv:2007.03900, 2020.

[2] Yanzhang He, Tara N Sainath, Rohit Prabhavalkar, Ian
McGraw, Raziel Alvarez, Ding Zhao, David Rybach,
Anjuli Kannan, Yonghui Wu, Ruoming Pang, et al.,
“Streaming end-to-end speech recognition for mobile
in ICASSP 2019-2019 IEEE International
devices,”
Conference on Acoustics, Speech and Signal Processing
(ICASSP). IEEE, 2019, pp. 6381–6385.

[3] Kanishka Rao, Has¸im Sak, and Rohit Prabhavalkar,
“Exploring architectures, data and units for streaming
end-to-end speech recognition with rnn-transducer,” in
2017 IEEE Automatic Speech Recognition and Under-
standing Workshop (ASRU). IEEE, 2017, pp. 193–199.

[4] Chunyang Wu, Yongqiang Wang, Yangyang Shi, Ching-
Feng Yeh, and Frank Zhang, “Streaming transformer-
based acoustic models using self-attention with aug-
arXiv preprint arXiv:2005.08042,
mented memory,”
2020.

[5] Tara N Sainath, Yanzhang He, Bo Li, Arun Narayanan,
Ruoming Pang, Antoine Bruguier, Shuo-yiin Chang,
Wei Li, Raziel Alvarez, Zhifeng Chen, et al., “A stream-
ing on-device end-to-end model surpassing server-side
in ICASSP
conventional model quality and latency,”
2020-2020 IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP). IEEE,
2020, pp. 6059–6063.

[6] Shinji Watanabe, Takaaki Hori, Suyoun Kim, John R
Hershey, and Tomoki Hayashi, “Hybrid ctc/attention
architecture for end-to-end speech recognition,” IEEE
Journal of Selected Topics in Signal Processing, vol. 11,
no. 8, pp. 1240–1253, 2017.

[7] Chung-Cheng Chiu, Wei Han, Yu Zhang, Ruom-
ing Pang, Sergey Kishchenko, Patrick Nguyen, Arun
Narayanan, Hank Liao, Shuyuan Zhang, Anjuli Kan-
nan, et al., “A comparison of end-to-end models for
in 2019 IEEE Auto-
long-form speech recognition,”
matic Speech Recognition and Understanding Workshop
(ASRU). IEEE, 2019, pp. 889–896.

[8] Yuan Shangguan, Jian Li, Liang Qiao, Raziel Alvarez,
and Ian McGraw, “Optimizing speech recognition for
the edge,” arXiv preprint arXiv:1909.12408, 2019.

[9] Kwangyoun Kim, Kyungmin Lee, Dhananjaya Gowda,
Junmo Park, Sungsoo Kim, Sichen Jin, Young-Yoon
Lee, Jinsu Yeo, Daehyun Kim, Seokyeong Jung, et al.,
“Attention based on-device streaming speech recogni-
in 2019 IEEE Auto-
tion with large speech corpus,”
matic Speech Recognition and Understanding Workshop
(ASRU). IEEE, 2019, pp. 956–963.

[10] Alex Graves,

“Sequence transduction with recurrent

neural networks,” CoRR, vol. abs/1211.3711, 2012.

[11] Alex Graves, Abdel-rahman Mohamed, and Geoffrey
Hinton, “Speech recognition with deep recurrent neu-
ral networks,” in 2013 IEEE international conference
on acoustics, speech and signal processing. IEEE, 2013,
pp. 6645–6649.

[12] Anshuman Tripathi, Han Lu, Hasim Sak, and Hagen
“Monotonic recurrent neural network trans-
Soltau,
in 2019 IEEE Auto-
ducer and decoding strategies,”
matic Speech Recognition and Understanding Workshop
(ASRU). IEEE, 2019, pp. 944–948.

[13] Jinyu Li, Rui Zhao, Zhong Meng, Yanqing Liu, Wen-
ning Wei, Sarangarajan Parthasarathy, Vadim Mazalov,
Zhenghao Wang, Lei He, Sheng Zhao, et al., “Devel-
oping rnn-t models surpassing high-performance hybrid
models with customization capability,” arXiv preprint
arXiv:2007.15188, 2020.

[14] Hasim Sak, Matt Shannon, Kanishka Rao,

and
“Recurrent neural aligner: An
Franc¸oise Beaufays,
encoder-decoder neural network model for sequence to
sequence mapping.,” in Interspeech, 2017, vol. 8, pp.
1298–1302.

[15] Ehsan Variani, David Rybach, Cyril Allauzen, and
Michael Riley, “Hybrid autoregressive transducer (hat),”
in ICASSP 2020-2020 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP).
IEEE, 2020, pp. 6139–6143.

[16] Albert Zeyer, Andr´e Merboldt, Ralf Schl¨uter, and Her-
mann Ney, “A new training pipeline for an improved
neural transducer,” arXiv preprint arXiv:2005.09319,
2020.

[17] S. Chang, R. Prabhavalkar, Y. He, T. N. Sainath, and
G. Simko, “Joint endpointing and decoding with end-
to-end models,” in ICASSP 2019 - 2019 IEEE Interna-
tional Conference on Acoustics, Speech and Signal Pro-
cessing (ICASSP), 2019, pp. 5626–5630.

[18] Bo Li, Shuo-yiin Chang, Tara N Sainath, Ruoming
Pang, Yanzhang He, Trevor Strohman, and Yonghui Wu,
“Towards fast and accurate streaming end-to-end asr,”
in ICASSP 2020-2020 IEEE International Conference

[28] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,
Sam Gross, Nathan Ng, David Grangier, and Michael
Auli, “fairseq: A fast, extensible toolkit for sequence
modeling,” 2019.

[29] Vassil Panayotov, Guoguo Chen, Daniel Povey, and San-
jeev Khudanpur, “Librispeech: an asr corpus based on
in 2015 IEEE Interna-
public domain audio books,”
tional Conference on Acoustics, Speech and Signal Pro-
cessing (ICASSP). IEEE, 2015, pp. 5206–5210.

[30] Daniel S Park, William Chan, Yu Zhang, Chung-Cheng
Chiu, Barret Zoph, Ekin D Cubuk, and Quoc V Le,
“Specaugment: A simple data augmentation method
arXiv preprint
for automatic speech recognition,”
arXiv:1904.08779, 2019.

[31] Yu Zhang, Guoguo Chen, Dong Yu, Kaisheng Yaco,
Sanjeev Khudanpur, and James Glass, “Highway long
short-term memory rnns for distant speech recognition,”
in 2016 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP). IEEE, 2016,
pp. 5755–5759.

[32] Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-
jeev Khudanpur, “Audio augmentation for speech recog-
nition,” in Sixteenth Annual Conference of the Interna-
tional Speech Communication Association, 2015.

[33] Taku Kudo and John Richardson, “Sentencepiece: A
simple and language independent subword tokenizer
arXiv
and detokenizer for neural text processing,”
preprint arXiv:1808.06226, 2018.

[34] Matt Shannon, Gabor Simko, Shuo-Yiin Chang, and
Carolina Parada, “Improved end-of-query detection for
streaming speech recognition.,” in Interspeech, 2017,
pp. 1909–1913.

on Acoustics, Speech and Signal Processing (ICASSP).
IEEE, 2020, pp. 6069–6073.

[19] Hu Hu, Rui Zhao, Jinyu Li, Liang Lu, and Yifan Gong,
“Exploring pre-training with alignments for rnn trans-
ducer based end-to-end speech recognition,” in ICASSP
2020-2020 IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP). IEEE,
2020, pp. 7079–7083.

[20] A. Senior, H. Sak, F. de Chaumont Quitry, T. Sainath,
and K. Rao, “Acoustic modelling with cd-ctc-smbr lstm
rnns,” in 2015 IEEE Workshop on Automatic Speech
Recognition and Understanding (ASRU), 2015, pp. 604–
609.

[21] Daniel Povey, Vijayaditya Peddinti, Daniel Galvez, Pe-
gah Ghahremani, Vimal Manohar, Xingyu Na, Yim-
ing Wang, and Sanjeev Khudanpur, “Purely sequence-
trained neural networks for asr based on lattice-free
mmi,” in Interspeech 2016, 2016, pp. 2751–2755.

[22] Kazuki Irie, Rohit Prabhavalkar, Anjuli Kannan, An-
toine Bruguier, David Rybach, and Patrick Nguyen, “On
the choice of modeling unit for sequence-to-sequence
speech recognition,” arXiv preprint arXiv:1902.01955,
2019.

[23] Alex Graves, Santiago Fern´andez, Faustino Gomez, and
J¨urgen Schmidhuber, “Connectionist temporal classi-
ﬁcation: labelling unsegmented sequence data with re-
current neural networks,” in Proceedings of the 23rd in-
ternational conference on Machine learning, 2006, pp.
369–376.

[24] Alex Graves,

neural networks,”
2012.

“Sequence transduction with recurrent
arXiv preprint arXiv:1211.3711,

[25] Duc Le, Xiaohui Zhang, Weiyi Zheng, Christian F¨ugen,
Geoffrey Zweig, and Michael L Seltzer, “From senones
to chenones: Tied context-dependent graphemes for
in 2019 IEEE Auto-
hybrid speech recognition,”
matic Speech Recognition and Understanding Workshop
(ASRU). IEEE, 2019, pp. 457–464.

[26] Jinyu Li, Rui Zhao, Hu Hu, and Yifan Gong,

“Im-
proving rnn transducer modeling for end-to-end speech
recognition,” 2019.

[27] Adam Paszke, Sam Gross, Francisco Massa, Adam
Lerer,
James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga,
et al., “Pytorch: An imperative style, high-performance
deep learning library,” in Advances in neural informa-
tion processing systems, 2019, pp. 8026–8037.

