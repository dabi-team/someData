7
1
0
2

l
u
J

7
1

]
T
S
.
h
t
a
m

[

1
v
9
7
3
5
0
.
7
0
7
1
:
v
i
X
r
a

Eﬃcient semiparametric estimation in time-varying regression
models

Lionel Truquet ∗†

Abstract

We study semiparametric inference in some linear regression models with time-varying coeﬃcients,
dependent regressors and dependent errors. This problem, which has been considered recently by
Zhang and Wu (2012) under the functional dependence measure, is interesting for parsimony reasons
or for testing stability of some coeﬃcients in a linear regression model.
In this paper, we propose
a diﬀerent procedure for estimating non time-varying parameters at the rate √n, in the spirit of the
method introduced by Robinson (1988) for partially linear models. When the errors in the model are
martingale diﬀerences, this approach can lead to more eﬃcient estimates than the method considered
in Zhang and Wu (2012). For a time-varying AR process with exogenous covariates and conditionally
Gaussian errors, we derive a notion of eﬃcient information matrix from a convolution theorem adapted
to triangular arrays. For independent but non identically distributed Gaussian errors, we construct an
asymptotically eﬃcient estimator in a semiparametric sense.

1 Introduction

Regression models with time-varying parameters have been widely studied in the literature. This type of
models includes standard regression models, some linear time series models such as the locally stationary
AR process studied by Dahlhaus (1997) or some time series models with covariates considered for instance
in Chen and Hong (2012). In Zhang and Wu (2012), the authors study a general linear regression model
with time-varying coeﬃcients and dependent errors. This model writes

yi = x′iβ(i/n) + ei,

1

i

≤

≤

n, E (ei

xi) = 0,

|

(1)

n are triangular arrays of dependent random vectors and β is a smooth function
n and (ei)1
where (xi)1
i
i
≤
≤
≤
≤
which deﬁnes the time-varying coeﬃcients in the regression model. Model (1) is quite general under the
assumptions used in this paper. For instance, the regressors and the noise component can be stationary
processes or locally stationary processes in the sense of Dahlhaus (1997). An interesting and natural question
for model (1) is to reduce model complexity by assuming that some covariates are associated to a constant
parameter. In Zhang and Wu (2012), a testing procedure is given for detecting some stable (or non time-
varying) coeﬃcients. Under the null hypothesis, one faces to a semiparametric model of type

yi = x′1,iβ1 + x′2,iβ2(i/n) + ei,

1

i

≤

≤

n,

(2)

with xi =
′. Semiparametric models are often an interesting alternative when the number of re-
(cid:17)
gressors is important or when the sample size is moderate. Moreover, semiparametric models are often a

x′i,1, x′i,2

(cid:16)

∗UMR 6625 CNRS IRMAR, University of Rennes 1, Campus de Beaulieu, F-35042 Rennes Cedex, France and
†ENSAI, Campus de Ker-Lann, rue Blaise Pascal, BP 37203, 35172 Bruz cedex, France. Email: lionel.truquet@ensai.fr.

1

 
 
 
 
 
 
ﬁrst step to modify a parametric model by taking in account of a particular inhomogeneity in the data. For
instance, Gao and Hawthorne (2006) investigated a regression model in which the time-varying trend is the
single nonparametric component. In the same spirit, in Truquet (2016), we studied semiparametric versions
of locally stationary ARCH models with a particular emphasis on the case of a time-varying unconditional
variance and non time-varying lag parameters. The square of the ARCH process being a weak AR process,
estimation of the ARCH coeﬃcients is a particular case of statistical inference in model (1).

As shown in Zhang and Wu (2012), under some general assumptions, non time-varying parameters in
model (1) can be estimated at the rate √n. The technique used in their paper consists in averaging the
nonparametric estimator of the full vector β(
·

) for the coordinates corresponding to the stable parameters.

In this paper, we generalize the approach studied in Truquet (2016) for estimating stable parameters
in ARCH models to model (1). Our method is based on the idea originally used by Robinson (1988)
or Speckman (1988) for partially linear models and consists in removing the nonparametric part of the
regression function by using partial regressions. This approach is also known as proﬁle least squares method
in the literature. Under some general assumptions, we show that β1 can be estimated at the rate √n. But the
main contribution of this paper addresses the question of eﬃciency. When the noise process (ei)1
n is a
i
≤
≤
martingale diﬀerence, we show that both methods requires the estimation of the conditional variance of the
noise to improve eﬃciency. However, the lower bound for the asymptotic variance obtained by the averaging
method of Zhang and Wu (2012) is larger than the method based on partial regressions. Moreover if the noise
is conditionally Gaussian and the regressors exogenous (except possible lag values of yi), the lower bound
obtained with our approach can be interpreted as a lower bound in semiparametric estimation. Estimating
the conditional variance and using a plug-in method to get an eﬃcient estimate of β1 is a very diﬃcult task
in general. This is why we only consider the case of independent errors with a time-varying unconditional
variance to construct an asymptotically optimal estimator which is also eﬃcient in the Gaussian case.

Let us mention that our conclusions for eﬃciency were already known for the case of a linear regres-
sion model with semi-varying coeﬃcients, a model in which some of the coeﬃcients depend on a covariate
and the others are constant. See Fan and Zhang (2008) for a survey of regression models with varying
coeﬃcients and for a discussion of the non eﬃciency of the averaging method. The case of time-varying co-
eﬃcients could be seen as a particular case of a semi-varying regression models with the time as a covariate.
However, this particular case requires a special attention due to the dependence between the observations
and the asymptotic with triangular arrays which lead to more technical arguments. Moreover, it is not
possible to use some available results for the semiparametric eﬃciency such as that of Chamberlain (1992).
Indeed, working with triangular arrays and the inﬁll asymptotic inherent to the local stationarity assumptions
requires a special attention.

−

In this paper, we will not study the nonparametric estimation of β2(
·

) because using a plug-in of β1 with
a √n
consistent estimator in (2) is asymptotically equivalent to the nonparametric estimation of parameter
β2 with known coeﬃcients β1. The latter problem has been extensively studied in the literature. Then the
most challenging problem is the estimation of the parametric part of the regression function, a problem we
focus on in this paper.

The paper is organized as follows. In Section 2, we introduce our model assumptions and we study the
asymptotic properties of our estimators. In Section 3, we consider an autoregressive process with exoge-
nous covariates and we give a discussion about asymptotic semiparametric eﬃciency when the errors are
conditionally Gaussian. Proofs of our two main results are postponed to the last section of the paper.

2

2 Semiparametric estimation in time-varying regression models

Let us introduce the model considered in this paper which is quite similar to the regression model studied in
Zhang and Wu (2012). We assume that

yi = x′1,iβ1 + x′2,iβ2(i/n) + ei,

1

i

≤

≤

n,

(3)

where ei is a random variable taking values in R, x1,i and x2,i are two random vectors of size p1 and p2
Rp1, β2 is a function deﬁned on the unit interval [0, 1] and taking values in Rp2. In
respectively and β1 ∈
the sequel, we set p = p1 + p2. Now we introduce the dependence structure of
′ as well as
(cid:17)
the functional dependence measure used in Zhang and Wu (2012). Let (ξk)k
Z a sequence of i.i.d random
k = σ
variables taking values in an arbitrary measurable space (E,
. We
ξ j : j
E
also assume that there exist measurable functions Gn,i : (Rp)N
(cid:16)
R such that

∈
Z, we set
Rp and Hn,i : RN

x′i,1, x′i,2, ei

). For k

≤

∈

k

(cid:17)

(cid:16)

→

F
→

If

F0,k = σ

ξ0, . . . , ξ
(cid:16)

k+1, ξ′
k, ξ
−
−

xi =

x1,i
x2,i!

= Gn,i (

F

i) ,

ei = Hn,i (

F

i) .

N, we set

for k

1, . . .
k
−
−
(cid:17)
δk,q(G) = sup
n
1
≥

∈
max
n k
i
1
≤
≤

Gn,i (

F0)

Gn,i

−

q,
k

F0,k
(cid:0)

(cid:1)

Θm,q(G) =

δk,q(G).

Xk
m
≥

1,1

n
i
≤
≤

Gn,i
k

In the sequel, we will often use some conditions of type Θ0,q(G) <
. In this case, it will be assumed
q <
. Note that model (3) is quite general because it contains standard
implicitly that maxn
k
≥
regression models, the autoregressive processes studied in Zhang and Wu (2012) and more generally autore-
gressive processes with exogenous covariates. The latter case will be studied in Section 3. The assumptions
we will use in the sequel allow to consider locally stationary processes for the covariates and the noise com-
ponent. This includes the moving averages considered in Dahlhaus (1997), the ARCH processes considered
in Dahlhaus and Subba Rao (2006) or other Markov processes studied in Subba Rao (2006) and more re-
cently in Dahlhaus et al. (2017). In particular the noise component ei can be conditionally heteroscedastic.

∞

∞

2.1 Estimation of β1

Throughout this paper, K will denote a probability density supported on [
We set for 1

i, j

n,

−

1, 1] and Lipschitz continuous.

≤

≤

We set

We have

ki, j =

1
nb

i
j
1
nb K
−
nb
(cid:16)
n
ℓ=1 K

(cid:17)
ℓ
i
−
nb

.

(cid:17)

,

s3,i = E

(cid:16)

x2,ix′2,i

,

(cid:17)

(cid:16)

P

(cid:16)

s1,i = E

x2,iyi

,

s2,i = E

x2,ix′1,i

(cid:0)

(cid:1)
1
q1,i = s−
3,i s1,i,
x′2,iq1,i,

−

yi = yi

(cid:17)
1
q2,i = s−
3,i s2,i,

x1,i = x1,i

q′2,i x2,i.

−

e

e
s1,i = s2,iβ1 + s3,iβ2(i/n).

3

 
This yields to the relations

and

q1,i = q2,iβ1 + β2(i/n)

yi =

x′1,iβ1 + ei.

The quantities

yi and

x1,i will be estimated by

e

e

e

e

ˆyi = yi

−

x′2,i ˆq1,i,

ˆx1,i = x1,i

ˆq′2,i x2,i,

−

where

n

ˆs1,i =

ki, j x2, jy j,

ˆs2,i =

Xj=1

n

Xj=1

ki, j x2, jx′1, j,

ˆs3,i =

n

Xj=1

ki, j x2,ix′2,i,

1
ˆq1,i = ˆs−
3,i ˆs1,i,

1
ˆq2,i = ˆs−
3,i ˆs2,i.

We now deﬁne the estimator of parameter β1.

n

Our estimator can be interpreted in two ways.

ˆβ1 =

1 n

−

Xi=1

ˆx1,i ˆyi.

ˆx1,i ˆx′1,i



Xi=1



•

The ﬁrst interpretation concerns partial regressions. The terms x′2,iq1,i and x′2,iq2,i can be interpreted
as the orthogonal projection of yi and x1,i respectively onto the linear L2 space generated by the
components of x2,i. Then
x1,i are the residuals components of these two regressions and a
new regression is made for estimating β1. In practice, it is necessary to estimate non parametrically
these quantities by ˆyi and ˆx1,i respectively. Then our estimator has the same interpretation than in the
partially linear models studied in Robinson (1988) or Speckman (1988), except that the response yi
and the regressors x1,i are projected onto the subspace of linear functions in x2,i.

yi and

e

e

•

The second interpretation is the proﬁle least squares method studied for instance in Fan and Huang
(2005) for semi-varying coeﬃcients models. The quantity ˆq1,i
ˆq1,iβ1 is the Nadaraya-Watson esti-
mator of the function β2 for the model

−

Then ˆβ1 is deﬁned as a minimizer of

yi

−

x′1,iβ1 = x′2,iβ2(i/n) + ei.

n

α

7→

yi

Xi=1 (cid:16)

x′1,iβ1 −

−

x′2,i

ˆq1,i
(cid:2)

−

ˆq1,iβ1

2

.

(cid:3)(cid:17)

On can also use local polynomials in the ﬁrst step if more regularity is assumed for β2, see the remark
after Theorem 1.

Let d be any positive integer. For a vector v of Rd or a square matrix A of size d
the euclidean norm of v and by
v
|
|
U or arbitrary dimension, we write U
assumptions.

d, we will denote by
the corresponding operator norm of the matrix A. For a random vector
r) <
. We will use the following
|

r = E1/r (
U
|
k

Lr, r > 0 if

U
k

∞

×

A

∈

|

|

4

A1 β is a Lipschitz function.

A2 There exist two measurable functions G : [0, 1]

constant C such that, if xi(u) = G (u,

F

→
× (cid:16)
i) and ei(u) = H (u,
F

(cid:17)

N

Rd

Rp, H : [0, 1]
i),

×

RN

→

R and a positive

max
n
i
1
≤
≤

xi

[
k

−

xi(i/n)

k2 +

xiei

k

−

xi(i/n)ei(i/n)

k2] = O

1
n !

,

and

−
k
q = 1
A3 There exist q > 8/7 and q1 > 16/7, q2 > 2 such that 1
q1

−

k

+ 1
q2

xi(u)

xi(v)

k2 +

xi(u)ei(u)

xi(v)ei(v)

.
v
|

C

u
|

−

k2 ≤
and Θ0,2q1(G) + Θ0,q2(L) <

Ln,i = Hn,iGn,i.

, with

∞

A4 For each u

∈

[0, 1], we have det E

k) G (u,

G (u,
(cid:2)

F

, 0.

k)′

F

(cid:3)

Notes

1. Without additional assumptions, the quantity ˆs3,i can be degenerate for ﬁnite samples (e.g discrete
covariates with the value 0 in the support of the distribution). Assumption A4 guarantees it is not the
case asymptotically. However one can always replace ˆs3,i by ˆs3,i + µnIp2 where Ip2 denotes the identity
matrix of size p2 and (µn)n is a sequence of positive real numbers and decreasing to 0 at a rate which
is suﬃciently fast to not modify our asymptotic results (e.g µn = 1/n). The new matrix is now positive
deﬁnite and then invertible. Since this modiﬁcation is only technical and complicates our notations,
we will not use it.

2. Assumption A2 is a local stationarity property. This assumption is satisﬁed for time-varying autore-
gressive processes or moving averages with time-varying coeﬃcients. With respect to Zhang and Wu
(2012), we directly use the functional dependence coeﬃcient for the triangular arrays Gn,i, Hn,i in-
stead of their local stationary approximations G (i/n,
i). With this choice, we avoid
to state a second result ensuring that the triangular arrays can be replaced in our estimators without
)), we recover
changing their asymptotic behavior. Note that when
the functional dependence coeﬃcients used in Zhang and Wu (2012).

i) and H (i/n,

= (G (i/n,

) , H (i/n,

F

F

·

·

Gn,i, Hn,i
(cid:0)

(cid:1)

3. The dependence/moment conditions given in assumption A3 is slightly more restrictive than that of
Zhang and Wu (2012) which is guaranteed if there exists q > 1 such that Θ0,4(G) + Θ0,2q(L) <
.
∞
For instance, taking q1 = q2 = 2q, our conditions are satisﬁed provided Θ0,4q(G) + Θ0,2q(L) <
for
q > 8
7 .

∞

4. Suﬃcient conditions for A2

k

−

xi

A3 are given by

xi(u)

k4 +

ei
k

−

−

ei(u)

k4 ≤

C

u
"|

−

i
n |

+

1
n #

,

and Θ0,4q(G) + Θ0,4q(H) <

xi(u)

k
.

∞

xi(v)

k4 +

ei(u)
k

−

−

ei(v)

k4 ≤

C

u
|

v
|

−

5

 
For ℓ = 1, 2, let us also set xℓ,i(u) = Gℓ (u,

ei(u). We also set

i), ei(u) = H (u,

F

F

i) and yi(u) = x1,i(u)′β1 + x2,i(u)′β2(u) +

q1(u) = E−
1

x2,i(u)x2,i(u)′

E

(cid:2)

(cid:3)

(cid:2)

x2,i(u)yi(u)
(cid:3)

,

q2(u) = E−
1

x2,i(u)x2,i(u)′

E

x2,i(u)x1,i(u)′

,

x1,i(u) = x1,i(u)

(cid:2)

q2(u)′ x2,i(u).

−

(cid:3)

(cid:2)

(cid:3)

Theorem 1. Assume A1

−
c with 1/4 < c < min

n−

e

A4, Θn,2q1(G) = O
n−
1 , 3
3
1
(cid:0)
q−
4 −
4 −

ν1

for ν1 > 1
1
q1
2 −
2
1
(cid:1)
(4/3) , 2
2 , 2
q−
q
∧

−

n

ν2

and Θn,q2(L) = O
(8/3) , 1

n−
(cid:0)
, we have
o

q1

∧

4

2

−

for ν2 > 1

2 −

1
q2

.

(cid:1)

√n

ˆβ1 −

β1

(cid:16)

(cid:17) ⇒ N (cid:16)

1
1
1 Σ2Σ−
0, Σ−
i

,

(cid:17)

1

E

Σ1 =

Z
0

x1,i(u)

x1,i(u)′

du,

Σ2 =

(cid:2)
e

e

(cid:3)

1

Z

0 Xℓ
Z
∈

Cov

x1,0(u)e0(u),

(cid:2)
e

du.

x1,ℓ(u)eℓ(u)
(cid:3)

e

Then if b

≍

with

Notes

1. The optimal bandwidth for estimating Lipschitz time-varying parameters is b

q2 = 2q and q > 6/5, this choice is allowed for applying Theorem 1.

n−

1/3. When q1 =

≍

2. It is also possible to use local polynomials of order 1 for removing the nuisance term x′i,2β2(i/n). To

this end, let us deﬁne for ℓ = 0, 1, 2,

S ℓ,i =

n

Xj=1

ki, j x2, jx′2, j

ℓ

j
i
−
nb (cid:19)

.

(cid:18)

Then we deﬁne ˆq1,i and ˆq2,i as the ﬁrst components of the two block matrices

1
−

S 0,i S 1,i
S 1,i S 2,i!

n
j=1 ki, j x2, jy j
j
i
n
j=1 ki, j x2, jy j
nb 
P
−

q1,i and




P

and

1
S 0,i S 1,i
−
S 1,i S 2,i!

n
j=1 ki, j x2, jx′1, j
j
i
n
j=1 ki, j x2, j x′1, j
P
−
nb





respectively. Moreover, let us denote by

q2,i the ﬁrst components of the two block matrices

ES 0,i ES 1,i
ES 1,i ES 2,i!

1
−

n

j=1 ki, jE
j=1 ki, jE
n
P

e
x2, jy j

h
x2, jy j

i
j
i
−
nb

e
,

ES 0,i ES 1,i
ES 1,i ES 2,i!

1
−

j=1 ki, jE
n
j=1 ki, jE
n
P

x2, jx′1, j

h
x2, jx′1, j

h
respectively. Then, under the assumptions of Theorem 1, one can show that

P

P

i

h









i
j
i
−
nb





i





P





q1,i

max
n
i
1
≤
≤

q2,i
(cid:12)(cid:12)(cid:12)e
The term 1/n corresponds to the deviation with respect to stationarity, i.e the order of the diﬀerences
q2,iβ1 and if the second derivative
q1,i
of β2 exists and is continuous, we get a smaller bias for β2:

(cid:12)(cid:12)(cid:12)
q2(i/n). Moreover we have β2(i/n) = q1,i

q1(i/n) and q2,i

q1,i
(cid:12)(cid:12)(cid:12)e

+ max
n
i
1
≤
≤

= O

b +

q2,i

−

−

−

−

−

(cid:12)(cid:12)(cid:12)

.

1
n !

q1,i

−

−

max
n
i
1
≤
≤

q1,i
(cid:12)(cid:12)(cid:12)e

−

q2,i
(cid:0)
e
6

q2,i

β1

= O

b2 +

(cid:1)

(cid:12)(cid:12)(cid:12)

1
n !

.

 
 
 
 
 
 
If we deﬁne an estimator of β1 with these new expressions for ˆq1,i and ˆq2,i, we obtain the proﬁle least
squares estimators studied in Fan and Huang (2005) for semi-varying coeﬃcients regression models.
Under the assumptions of Theorem 1, the estimator ˆβ1 is √n
consistent but some assumptions can
−
be relaxed. Inspection of the proof of Theorem 1 shows that the only change due to this smaller bias
concerns the term N2 and the bandwidth condition nb4
0 (the bias
is of order b for ˆq2,i and of order b2 for ˆri). This leads to the condition c > 1/6 for the bandwidth.
Moreover, on can choose q > 12/11 and q1 > 24/11 in assumption A3 to get a compatible upper
bound for c in Theorem 1.

0 can be replaced by nb6

→

→

2.2 Improving eﬃciency when the noise process is a martingale diﬀerence

In this subsection, we assume the following martingale diﬀerence structure for the noise component ei:

E

ei
(cid:16)

i
1
−

= 0, Var

ei

= σ2 (i/n, xi) ,

i
1
−

where σ : [0, 1]
1). For
i
−
simplicity of notations, we set σi = σ (i/n, xi). Now let pi = p (i/n, xi) a weight function. One can write a
second regression model by multiplying the ﬁrst one by the weights pi,

F
(cid:12)(cid:12)(cid:12)
R∗+ is an unknown function. We also assume that Gn,i (
F

i) = Gn,i (

F
(cid:12)(cid:12)(cid:12)

Rp

→

F

×

(cid:17)

(cid:17)

(cid:16)

piyi = pi x′1,iβ1 + pi x′2,iβ2(i/n) + piei,

1

i

≤

≤

n.

(4)

In the sequel, we assume that the assumptions A1
asymptotic variance given in Theorem 1 has a simpler form. In particular,

−

A4 are satisﬁed for this new regression model. The

Σ2 = E

1

Z
0

p2
i (u)σ2

i (u)

px1,i(u)

px1,i(u)′du,

where pi(u) = p (u, xi(u)), σi(u) = σ (u, xi(u)) and px denotes the vector (pi xi)1
n. Now, if we set
i
≤
≤
pi(u)2 x2,i(u)x1,i(u)′

pi(u)2 x2,i(u)x2,i(u)′

q2(u) = E−
1

E

,

f

f

1
q∗2(u) = E−

and changing our notations with

h
σi(u)−
h

i ·
2 x2,i(u)x2,i(u)′

h
E

σi(u)−
h

i ·

i
2 x2,i(u)x1,i(u)′

i

x1,i(u) = x1,i(u)

−

q2(u)′ x2,i(u),

x∗1,i(u) = x1,i(u)

q∗2(u)′ x2,i(u),

−

we have

e

e

Σ1 = E

1

Z
0

pi(u)2

x1,i(u)

x1,i(u)′du = E

1

Z
0

pi(u)2

x1,i(u)

x∗1,i(u)′du = E

e
e
From the Cauchy-Schwarz inequality, we have for x, y

e

e
Rp1,

1

Z
0

pi(u)2σi(u)

x1,i(u)

e

x∗1,i(u)′
σi(u)
e

du.

2

x′Σ1y
(cid:1)

(cid:0)

≤

where

Σ∗ = E

1

x∗1,i(u)

x∗1,i(u)′

Z

0 e

σi(u)2
e

du.

∈
x′Σ2x

y′Σ∗y,

·

7

(5)

1
1 z and y = (Σ∗)−
Now setting x = Σ−

1 z for z

Rp1, we get

∈

1 z
−
(cid:1)
1 is the asymptotic variance corresponding to the choice pi = σ−
1
i

1
1
1 Σ2Σ−
z′Σ−
1 z.

Σ∗
(cid:0)

z′

≤

Note that (Σ∗)−
. This means that weighted
least squares can improve eﬃciency of our estimator, provided to ﬁrst estimate the conditional variance of
the noise. In the subsection 2.4, we will construct an eﬃcient estimator when the errors are independent but
have a time-varying unconditional variance.

2.3 Comparison with a related method

Zhang and Wu (2012) have proposed another method for the estimation of parameter β1. Their method
) of coeﬃcients using local polynomials and then averaging
consists in ﬁrst estimating the full vector β(
·
ˆβ1(u)du. They have shown that under some assumptions,
the ﬁrst component of this estimator, i.e ˆβ1 =
ˆβ1 is root-n consistent. Comparing eﬃciency of both methods seems diﬃcult for a general noise process.
Nevertheless, one can get interesting properties when the noise is a martingale diﬀerence. To this end, we
keep the notations of the previous subsection. Application of the method of Zhang and Wu (2012) in the
regression model (4) leads to an asymptotic variance S given by (see Theorem 3.1 of that paper):

1
0
R

S =

1

Z
0

where

AS 1(u)−

1S 2(u)S 1(u)−

1A′du,

S 1(u) = E
and A is a matrix of size p1 ×
1
when pi = σ−
i

,

S 2(u) = E

pi(u)2 xi(u)xi(u)′
h
p and which associates the p1 ﬁrst components of a vector of Rp. Note that
. Using

pi(u)4σi(u)2 xi(u)xi(u)′

1A′du where S ∗(u) = E

i

i

h

1
0 AS ∗(u)−

, the asymptotic variance reduces to

the formula for the inverse of a matrix deﬁned by blocs, one can check that AS ∗(u)−
Using Lemma 6, we have

R

xi(u)xi(u)′
σi(u)2
h
1A′ = E−
1

i
h∗i (u)h∗i (u)′
σi(u)2

.
(cid:21)

(cid:20)

1

Z
0

AS ∗(u)−

1A′

(cid:23)

Σ∗
(cid:0)

1 .
−
(cid:1)

(6)

Moreover, the matrix
from the Cauchy-Schwarz inequality

AS ∗(u)−

R

1
0

1A′ is optimal for the estimator of Zhang and Wu (2012). Indeed we have

2

x′S 2(u)x

x′S 1(u)x
(cid:1)
1z. Then we obtain the following important
1z and y = S ∗(u)−
and the result follows by tacking x = S 1(u)−
conclusion for a martingale diﬀerence noise: removing the nonparametric component with partial regres-
sions can lead to a more eﬃcient estimator than averaging directly the nonparametric estimator. In the next
section, we show that an AR process with a conditionally Gaussian noise and some strongly exogenous
covariates, the matrix (Σ∗)−

1 is a lower bound in semiparametric estimation.

y′S ∗(u)y

≤

(cid:0)

·

8

2.4 Weighted least squares and asymptotic eﬃciency improvement

In this subsection, we consider the simple case ei = σ(i/n) f (ξi) with E f (ξi) = 0 and Var f (ξi) = 1. In this
case, we ﬁrst derive an estimator of σ2(i/n). A natural candidate is

n

ˆσ2(i/n) =

ki, j

y j
h

−

x′j

ˆβ(i/n)
i

Xj=1

2

,

where ˆβ is an estimate of β. We will use the simple Nadaraya-Watson estimator of β, i.e

n

Xj=1

ˆβ(i/n) = 


ki, j x j x′j


1 n
−

Xj=1

ki, j x jy j.

Another solution is to take the semiparametric estimator ˆβ1 of β1 which is √n
tions of Theorem 1 and to deﬁne an estimator of β2(i/n) as follows:

consistent under the assump-

−

ˆβ2(i/n) = arg min
Rp2

α

∈

n

ki, j

yi
h

Xj=1

x′i,1

ˆβ1 −

−

x′i,2α

2

.

i

But this second choice will not improve our asymptotic results and for simplicity we only consider the
Nadaraya-Watson estimator for the plug-in. Our plug-in estimator is deﬁned as follows. For simplicity we
will simply note σi (resp. ˆσi) for σ(i/n) (resp. ˆσ(i/n)). We set

ˆs∗1,i =

n

Xj=1

ki, j

x2, jy j
ˆσ2
j

,

ˆs∗2,i =

n

Xj=1

ki, j

x2, j x′1, j
ˆσ2
j

,

ˆs∗3,i =

n

Xj=1

ki, j

x2, jx′2, j
ˆσ2
j

,

and

ˆq∗1,i =

ˆs∗3,i

(cid:17)

(cid:16)

1
−

ˆs∗1,i,

ˆq∗2,i =

ˆs∗3,i

(cid:17)

(cid:16)

1
−

ˆs∗2,i

−
The optimal estimator of β1 is deﬁned by

ˆy∗i = yi

x′2,i ˆq∗1,i,

ˆx∗1,i = x1,i

ˆq∗2,i

− (cid:16)

′ x2,i.
(cid:17)

n

ˆx∗1,i

ˆβ∗1 = 


Xi=1

′
(cid:17)

ˆx∗1,i
(cid:16)
ˆσ2
i

1
−

n

Xi=1





ˆx∗1,i ˆy∗i
ˆσ2
i

.

We have the following result.

Theorem 2. Assume that all the assumptions A1-A4 hold true with q2 > 4 and σ is positive and Lipschitz
over [0, 1]. Then

where Σ∗ is deﬁned in (5).

√n

ˆβ∗1 −
(cid:16)

β1

0,
(cid:17) ⇒ N (cid:16)

Σ∗

(cid:0)

9

1
−
(cid:1)

,

(cid:17)

3 Asymptotic semiparametric eﬃciency for an autoregressive process with

exogenous covariates

3.1 An autoregressive process with covariates

n such that zi = G(2)
n,i (
We consider a process (zi)1
i
≤
≤
We deﬁne a process (yi)1
n recursively by
i
≤
≤

F

i) and an error process (ei)1
n deﬁned as in Section 2.
i
≤
≤

yi =

q

Xj=1

a j(i/n)yi
−

j + z′i γ(i/n) + ei

and we set xi =
q, z′i
1, . . . , yi
yi
−
−
(cid:16)
parameter of interest. The following assumptions will be needed.

′ and p = q + d. Finally, we set β(u) =
(cid:17)

(cid:16)

a1(u), . . . , aq(u), γ(u)′

′ for the
(cid:17)

1

−

P

B1 The function u

β(u) is Lipschitz. Moreover, for each u

7→

q
j=1 a j(u)z j are outside the unit disc.

[0, 1], the roots of the polynomial

∈

u(z) =

P

B2 There exists a measurable function M : [0, 1]

EN

×

→

R such that if zi(u) = M (u,

i),

F

and

max
n
i
1
≤
≤

zi
[
k

−

zi(i/n)

k4 +

ei
k

−

ei(i/n)

k4] = O

1
n !

,

zi(u)
k

−

zi(v)

k4 +

ei(u)
k

−

ei(v)

k4 ≤

C

u
|

.
v
|

−

B3 There exist q > 8

7 such that Θ0,4q (M) + Θ0,4q(H) <

.

∞
Under the previous assumptions, we show that (yi)1
n can be approximated locally by the stationary pro-
i
≤
≤
cess

q

yi(u) =

a j(u)yi
−

Xj=1

j(u) + zi(u)′γ(u) + ei(u),

Z, u

i

∈

∈

[0, 1].

For initialization, we assume that for i

exists an application Nn,i :
Rd

N : [0, 1]

N

× (cid:16)

(cid:17)

→

N

Rd
(cid:16)

(cid:17)

→

R such that yi(u) = N (u,

i).

F

0, yi = yi(0). Note that for integers n

n, there
≥
i). Moreover, there exists also an application

1 and 1

≤

≤

i

R such that yi = Nn,i (

≤

F

Proposition 1. Under the assumptions B1-B3, there exists a constant C > 0 such that for all (u, v, i)
[0, 1]2

,

∈

1, 2, . . . , n
}

× {

yi
k

−

yi(u)

k4 ≤

C

u
"|

−

i
n |

+

1
n #

,

yi(u)
k

−

yi(v)

k4 ≤

C

u
|

−

.
v
|

Moreover, we have Θ0,4q(N) <

.

∞

Note. Assumptions B1-B3 entail assumptions A1-A3. There is no general arguments for checking A4.
q
However, if A4 is not valid then we have
[0, 1] and some
j=1 λ jyi
−
constants λ j and µ. This type of equality cannot hold in general. For instance, assumption A4 is automatic
P
for a non degenerate noise such that the e′is are independent and independent from the z′is.

j(u) + µ′zi(u) = 0 a.s for some u

∈

10

 
Proof of Proposition 1 Let us introduce the following notations. For i

n, we set

≤

Yi =

1, . . . , yi
yi, yi
−
−
(cid:16)

q+1

(cid:17)

′ , Bi =

z′i γ(i/n) + ei, 0′1,q
1
−
(cid:16)

(cid:17)
= (γ(0)′, ei(0), zi(0)) when i

′ ,

with the convention that yi = yi(0) and
by A(i/n) the companion matrix associated to the polynomial
Yi = A(i/n)Yi
1 + Bi. From the assumption B2, for each u
−
smaller than 1. Then, one can consider

γ(i/n)′, ei, z′i

∈

(cid:16)

(cid:17)

0. Moreover we denote
n, we have
[0, 1], the spectral radius of the matrix A(u) is

i/n. Then, for q + 1

P

≤

≤

≤

i

mu = inf

k

1 :

A(u)k

≥

n

(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)

< 1
o

<

.

∞

O

u =

[0, 1] :

A(v)mu
v
Now, let
{
|
h
unit interval, we have [0, 1] =
i=1O
∪
common multiple of the integers mu1, . . . , muh, we have maxu
∈
of the application (u1, . . . , uh)
A(uh), we have, if n is large enough,

u is an open subset of [0, 1]. From the compactness of the
< 1
}
ui for suitable points u1, . . . , uh in [0, 1]. Then if m denotes the lowest
< 1. From the uniform continuity

. The set

A(u)m

[0,1] |

A(u1)

O

∈

|

|

7→

· · ·

ρ = max
k
m
1
≤
≤
−

n (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
k4 <

∞

A

k
n !

A

k

1
−
n ! · · ·

A

k

−

m + 1
n

< 1.

!(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

. From our assumptions, we deduce that there exists a positive

Moreover, we have supu
∈
real number D such that,

[0,1] k

Yi(u)

Yi
k

−

Yi(u)

k4 ≤

ρ
Yi
k
−

m

−

m(u)
Yi
−

k4 + D
hℓ
−
n

+

+

1
n #

u

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

i
n −

"(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

1
n #

,

∞

ρh

D

Xh=0

≤

u

i

−

"(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

which leads to the ﬁrst part of the proposition, the bound for
way. For the second part, we use the expansion

Yi(u)
k

−

Yi(v)

k4 can be obtained in the same

Yi = Bi +

∞

Xh=1

A

i
n (cid:19) · · ·

(cid:18)

A

i

−

h + 1
n

h.
Bi
−

!

Then we obtain Θ0,4q(N) <
.(cid:3)
Θ0,4q(H) <

∞

∞

by using the contraction coeﬃcient ρ and the assumption Θ0,4q(M) +

3.2 Gaussian inputs and eﬃcient information matrix

∼ N

f (ξi) = ξ(1)
i

In this subsection, we assume that ei = σ(i/n, zi) f (ξi) with f (ξi)
Z are independent. This independence property occurs for example when ξi =
and ( f (ξi))i
∈
(cid:16)
ξ(1) and ξ(2) independent,
and zi only depending on
. This type of condition on the
covariates is called strong exogeneity (sometimes Granger or Sims causality in the literature) and is very
useful to get a simple expression of the conditional likelihood function. We will assume that the conditional
standard deviation σi = σ (i/n, zi) satisﬁes infn
n σi
. One could also
i
≥
≤
≤
allow σi to depend on past values of yi but it is necessary to introduce additional regularity conditions to
deﬁne the model. This is why we prefer to consider conditional heteroscedasticity only with respect to the
covariates.

(0, 1) and the two processes (zi)1
n
i
≤
≤
, with
(cid:17)

> 0 a.s. for a constant σ

ξ(2)
j
(cid:16)

, ξ(2)
i

ξ(1)
i

i
j
≤

σ

1,1

≥

(cid:17)

−

−

11

 
 
 
 
We assume that the parameter β(u) writes as (β1, β2(u)) with β1 ∈

Rp1 is constant. The coordinates of β1
can contain some of the a′js or some of the coordinates of γ. For instance, β1 can contains all the parameters
except a time-varying trend β2. One can also consider the case of constant lag coeﬃcients a j and a covariate
zi with time-varying coeﬃcients.

We will ﬁrst prove a LAN expansion of the likelihood ratio conditionally to some initial values y0, y1, . . . , y
−
′. Note that µn does
(cid:17)
y−0 , z1:n
the density of
0. Using the exogeneity property of

not depending on n. Let µn be the probability distribution of the vector
not depend on a parameter of interest in this problem. We also denote by Ln,β
(y1, y2, . . . , yn) conditionally to zi and y j for 1
the covariates, we have the expression

z′1, z′2, . . . , z′n

n and

q + 1

(cid:16)·|

≤

−

≤

≤

≤

(cid:16)

(cid:17)

i

j

q+1

n

Ln,β

y1:n
(cid:16)

y−0 , z1:n
|

(cid:17)

=

2πσ2
i

Yi=1 (cid:16)

1/2

−

(cid:17)

yi
exp 
− (cid:12)(cid:12)(cid:12)


2

−

x′i β(i/n)
(cid:12)(cid:12)(cid:12)
2σ2
i

.





Then we set

y−0 , z1:n
y1:n
|
(cid:16)
where λn denotes the Lebesgue measure on Rn. We introduce the following local parametrization. Let us
denote by H the Hilbert space Rp1

Lp1([0, 1]) endowed with the scalar product

dµn (z1:n) dλn (y1:n) ,

y1:n, z1:n
(cid:16)

dPn,β

y−0
|

= Ln,β

(cid:17)

(cid:17)

×
1

g)H =
(h
|

Z
0

h(u)′ M(u)h(u)du, M(u) = E

x0(u)x0(u)′
σ2 (u, x0(u)) #

.

"

ﬁrst components of h(u) and g(u) do
We set h(u), g(u) for simplicity of notations. It is intended that the p1−
not depend on u. Note that, in a parametric problem, M(u) corresponds to the Fisher information matrix for
the estimation of the parameter β(u) in the model

yi(u) = x′i (u)β(u) + σ (u, zi(u)) f (ξi) .

We also set

= Rp1

p2 where

H

× L

L

∈ L × L

β, h

, the log likelihood ratio obtained after shifting parameter β is given by

denotes the set of Lipschitz functions deﬁned over [0, 1]. Then if

log

dPn,β+h/ √n
dPn,β

n

y1:n, z1:n
(cid:16)

y−0
|

(cid:17)

=

Xi=1

2

x′i β
−
(cid:12)(cid:12)(cid:12)
2σ2
i

yi
(cid:12)(cid:12)(cid:12)

yi
− (cid:12)(cid:12)(cid:12)

−

x′i(β + h)
(cid:12)(cid:12)(cid:12)
2σ2
i

2





Let us ﬁrst derive a LAN expansion for this likelihood ratio.

.





Proposition 2. Under the assumptions B1

B3, we have

−

where

log

dPn,β+h/ √n
dPn,β

y1:n, z1:n

y−0
|

(cid:17)

(cid:16)

= ∆n,β,h

1
2
H + oP(1),
h
2 k
k

−

∆n,β,h :=

1
√n

n

Xi=1

f (ξi) x′i h(i/n)
σi

0,

2
h
H
k
k

.

(cid:17)

⇒ N (cid:16)

12

Proof of Proposition 2 We have

log

dPn,β+h/ √n
dPn,β

y1:n, z1:n
(cid:16)

y−0
|

(cid:17)

= ∆n,β,h

1
2

−

∆′n,h,

where

∆′n,h =

n

1
n

h′(i/n)xi x′ih(i/n)
σ2
i

.

Xi=1
1, . . . , y
First, one can note that the initial values y0, y
q+1 are forgotten exponential fast. More precisely if yi
−
−
q+1(0), the L4
norm of the diﬀerence is bounded
is initialized with the random variables y0(0), y
1(0), . . . , y
−
−
−
by Cρi with ρ
n is initialized with (yi(0))
(0, 1). Hence, one can assume that (yi)1
0. Using the
i
q+1
i
∈
≤
≤
≤
≤
B3 and Proposition 1, there exists a positive constant C such that for all n
assumptions B1
n
1, 1
≥
and u

[0, 1],

−

≤

≤

−

i

E

2
σ−
i xix′i

(cid:16)
4 = O(1). Then the
Moreover, the application u
|
convergence of ∆n,β,h follows from the central limit theorem for triangular arrays of martingale diﬀerences.
(cid:17)
Moreover, using Lemma A1 of Zhang and Wu (2012) with q = 2, we have

7→

xi

E

|

σi(u)−
(cid:16)
2 xi(u)xi(u)′

2 xi(u)xi(u)′

≤

u
C [
|

i/n
|
n Eσ−
4
is Lipschitz and max1
i
i
≤
≤

+ 1/n] .

(cid:17)(cid:12)(cid:12)(cid:12)(cid:12)

−

E

(cid:17) −
σi(u)−
(cid:16)

(cid:12)(cid:12)(cid:12)(cid:12)

∈

∆′n,h −

1
n

n

Xi=1

h(i/n)′E

2
σ−
i xix′i
h

i

h(i/n) = OP

.

1/ √n
(cid:17)
(cid:16)

2
Then we deduce from the properties stated above that ∆′n,h → k
h
H. This completes the proof of Proposition
k
2.(cid:3)
From this LAN expansion, we deduce the following result.

Corollary 1. The limit distribution of any regular estimator of parameter β1 equals the distribution of L1+L2
of independent random vectors such that

L1 ∼ N

p1

0,

(cid:16)

,

Σ∗
(cid:0)

1
−
(cid:1)

(cid:17)

1

E

Σ∗ =

Z

0

2
σi(u)−
(cid:16)

x∗1,i(u)

x∗1,i(u)′

e

e

du,

(cid:17)

with

x∗1,i(u) = x1,i(u)

q∗2(u)′ x2,i(u),

q∗2(u) = E−
1

σi(u)−

2 x2,i(u)x2,i(u)′

E

2 x2,i(u)x1,i(u)′

.

e

h
Note. The matrix Σ∗ plays the rule of an eﬃcient information matrix. Its deﬁnition involves the whole
family of stationary approximations

of our triangular array.

(yi(u), zi(u), ei(u)) : u

i

i

σi(u)−
h

−

{

[0, 1]
}

∈

Proof of Corollary 1 We use the general convolution theorem given in van der Vaart and Wellner (1996),
theorem 3.11.2, which is compatible with triangular arrays. To this end, we consider the sequence of pa-
deﬁned by κn(h) = β1 + h1/ √n where h1 denotes the vector of constant parameters.
rameters
}
This sequence of parameters is regular, we have

κn(h) : h

H

∈

{

The adjoint operator ˙κ∗ : Rp1

→

H of the projection operator ˙κ is deﬁned by the equalities,

√n (κn(h)

−

κn(0)) = h1 := ˙κ(h).

˙κ(h)′v =

H ,
h, ˙κ∗v
(cid:1)
(cid:0)

13

Rp1, h

v

∈

H.

∈

First, we replace the two random vectors x1,i(u)/σi(u) and x2,i(u)/σi(u) by orthogonal vectors in L2. We
have

x1,i(u)
σi(u)

=

x∗1,i(u)
σi(u)
e

+ q∗2(u)′

x2,i(u)
σi(u)

.

Then we have

M(u) =

Ip1
0p2,p1

Then if ˙κ∗v = (g′1, g2(u)′)′, we get

q∗2(u)′
Ip2 ! · 


E

x∗1,i(u)

x∗1,i(u)′

(cid:20) e

σi(u)2
e
0p2,p1

(cid:21)

0p1,p2
x2,i(u)x2,i(u)′
σi(u)2

E

h





i

Ip1
q∗2(u)

·  

0p1,p2
Ip2 !

.

1

h′1v = h′1Σg1 +

Z

0 (cid:16)
1v and g2(u) =

Then we get g1 = Σ−

h′1q∗2(u)′ + h2(u)′

E

x2,i(u)x2,i(u)′
σi(u)2

"

(cid:17)

q∗2(u)g1 + g2(u)
(cid:17)

# (cid:16)

du.

q∗2(u)Σ−

1v. We deduce that

−

H = v′Σ−
˙κ∗v
k
k

1v

and the convolution theorem gives the result, from the LAN expansion given in Proposition 2.(cid:3)

Note. Under the assumptions of Theorem 2 and when σi = σ(i/n), we deduce that the estimator ˆβ∗1 is
asymptotically eﬃcient in a semiparametric sense. Rigorously, it is necessary to show that this estimator is
regular in the sense of van der Vaart and Wellner (1996). This step does not present any diﬃculty, provided
to follow carefully the proof of Theorem 2. Details are omitted.

4 Proofs of Theorem 1 and Theorem 2

4.1 Additional results for the proof of Theorem 1

In the sequel, we set

qℓ,i = E−
1

ˆs3,i

E

ˆsℓ,i

for ℓ = 1, 2.

Lemma 1. Assume that the assumptions of Theorem 1 hold true. Setting mi = E ˆs3,i, we have

e

(cid:0)

(cid:1)

(cid:0)

(cid:1)

1. max1
n
i
≤
≤

1
m−
i k
k
2. for ℓ = 1, 2,

= O(1), max1
n
i
≤
≤

1
ˆs−
3,i k

k

= OP(1),

ˆqℓ,i

qℓ,i

−

−

1
m−
i

max
i
1
≤
≤

n (cid:12)(cid:12)(cid:12)(cid:12)

ˆsℓ,1 −

h

E ˆsℓ, j

i

1
+ m−
i

ˆs3,i

(cid:2)

−

E ˆs3,i

1
m−
i

E ˆsℓ,i

(cid:3)

= oP

1
√n !

.

(cid:12)(cid:12)(cid:12)(cid:12)

3. for ℓ = 1, 2, max1
n
i
≤
≤
4. For ℓ = 1, 2, we have

e
ˆqℓ,i
(cid:12)(cid:12)(cid:12)

−

qℓ,i

= oP

1/4

n−

.

(cid:12)(cid:12)(cid:12)

e

(cid:16)

(cid:17)

max
n
i
1
≤
≤

−

qℓ,i
(cid:12)(cid:12)(cid:12)e

qℓ,i

= o

1/4

n−

(cid:16)

(cid:12)(cid:12)(cid:12)

, max
n
i
1
≤
≤

(cid:17)

qℓ,i

−

(cid:12)(cid:12)(cid:12)

qℓ(i/n)
(cid:12)(cid:12)(cid:12)

= O(1/n).

14

 
 
Proof of Lemma 1

1. Using Lemma A1 in Zhang and Wu (2012), there exists C > 0 such that

E ˆs3,i

ˆs3,i

−

max
n
i
1
≤
≤

k

(cid:12)(cid:12)(cid:12)

k2 ≤

(cid:12)(cid:12)(cid:12)

C √nanΘ0,4(G),

where

an = max
n 
i
1
≤
≤

ki,1 +



Using our assumptions, we deduce that

n
1
−

Xj=1

ki, j+1 −
|

ki, j

= O

1
nb !

.

|


E ˆs3,i

= oP(1).

max
n
i
1
≤
≤

ˆs3,i

−

(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)

Moreover, setting G = (G1, G2) and g2(u) = EG2 (u,

max
n
i
1
≤
≤

E ˆs3,i
(cid:12)(cid:12)(cid:12)

−

g2(i/n)
(cid:12)(cid:12)(cid:12)

= O(b + 1/n).

i) G2 (u,

F

F

i)′, we have from assumption A2,

(7)

(8)

ˆs3,i
From (7) and (8), we conclude that max1
n
i
−
≤
≤
assumptions A2 and A4 which guarantee that max1
n
i
≤
≤

g2(i/n)
1
(cid:12)(cid:12)(cid:12)
g2(i/n)−

(cid:12)(cid:12)(cid:12)

2. One can check that the quantity inside the norm is equal to

= O(1).

(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)

= oP(1). Then the result follows from

1
di = ˆs−
3,i
1
(cid:0)
+ m−
i

ˆs3,i
−
E ˆs3,i
(cid:0)

E ˆs3,i
(cid:1)
ˆs3,i

−

1
m−
i
1
(cid:0)
m−
i

ˆs3,i

ˆsℓ,i

−

−

E ˆs3,i
(cid:1)
E ˆsℓ,i

1
m−
i

ˆsℓ,i

.

(cid:1)
Using Lemma 3 and the previous point of this lemma, we get

(cid:1)

(cid:0)

= OP

di

max
n |
i
1
≤
≤

|

v2
n
(nb)2 !

, where vn = n1/q1 + n1/q2 +

nb log n

p

and the result follows from our bandwidth conditions.

3. The result follows from the previous point, Lemma 3 and our bandwidth conditions.

4. As in the proof of point 1., for ℓ = 1, 2, 3, we have from assumption A2

E ˆsℓ,i

max
n
i
1
≤
≤

n

−

Xj=1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

ki, j sℓ( j/n)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

= O (1/n) , max
n
i
1
≤
≤

Xj=1

ki, j sℓ( j/n)

−

n

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

= O(b).

sℓ(i/n)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

.(cid:3)
(cid:17)

1/4

n−

(cid:16)

Then the result follows from assumption A4 and the condition b = o

15

 
 
4.2 Proof of Theorem 1

We set Dn =

n
i=1 ˆx1,i ˆx′1,i. Using the decomposition

P

ˆyi = ˆx′1,iβ1 + x′2,iˆri + ei,

ˆri =

we get

ˆq2,i
(cid:0)

−

q2,i

β1 −

ˆq1,i
(cid:0)

−

(cid:1)

q1,i

,

(cid:1)

Using now the decomposition ˆx1,i =

1
β1 = D−
n

n

ˆx1,i

x′2,i ˆri + ei

.

i

Xi=1

q2,i

h
′ x2,i, we have
ˆq2,i
−
−
(cid:0)
(cid:1)
1
β1 = D−
n [N1 −

N2 + N3 −

N4] ,

ˆβ1 −
x1,i
ˆβ1 −
e

where

N1 =

n

Xi=1

x1,i x′2,iˆri, N2 =

e
n

n

ˆq2,i

Xi=1 (cid:0)
n

N3 =

x1,iei, N4 =

ˆq2,i

The proof of Theorem 1 will follow from the following points.

Xi=1

e

Xi=1 (cid:0)

−

−

q2,i

′ x2,i x′2,iˆri,
(cid:1)

q2,i

′ x2,iei.
(cid:1)

We ﬁrst show that the leading term N3
√n
A2 and Lemma 1 (4.), we have

•

= 1
√n

n
i=1

x1,iei converges in distribution. Using assumption

P

e

x1,i(i/n)ei(i/n)

k2 = O

−

1
√n !

.

1
√n

n

Xi=1

x1,iei

k
e

e

As in the proof of Theorem 1 given in Zhang and Wu (2012), using m
is possible to show that

dependent approximations, it

−

1
√n

Using Lemma 1, we have

n

x1,i(i/n)ei(i/n)

Xi=1

e

(0, Σ2) .

⇒ N

•

•

1
√n |

N2| ≤

C

1
√n

n

Xi=1 (cid:12)(cid:12)(cid:12)

x2,ix′2,i

max
ℓ=1,2

·

max
n
i
1
≤
≤

(cid:12)(cid:12)(cid:12)

qℓ,i

−

ˆqℓ,i
(cid:12)(cid:12)(cid:12)

2 = oP(1).
(cid:12)(cid:12)(cid:12)

Next, we show that N4
√n
Lemma 1, it is only necessary to prove that 1
√n

= oP(1). With the same arguments, one can also show that N1
√n

= oP(1). Using

n
i=1 A′i x2,iei = oP(1) when

Ai =

q2,i

−

1
q2,i, Ai = m−
i

ˆs2,i

1
, Ai = m−
i

ˆs3,i

E ˆs3,i

1
m−
i

E ˆs2,i.

−

(cid:0)

(cid:1)

P
E ˆs2,i

−

(cid:1)

(cid:0)

e

The ﬁrst case follows from Lemma 1, Lemma A.1 in Zhang and Wu (2012) and our bandwidth con-
ditions. The second and third case follows from Lemma 5, applied with r1 = q1, r2 = q2 and using
our bandwidth conditions. One can also note that for the proof of N1 = o
, we have to show that
(cid:17)
n
1
i, j=1 ki, jziw j = oP(1) when zi and w j are elements of the matrices xi x′i and x jx′j respectively. One
√n
can apply Lemma 5 with r1 = r2 = q1 and the result follows from our bandwidth conditions.

√n

P

(cid:16)

16

 
To end the proof of Theorem 1, it remains to show that Dn

n = Σ1 + oP(1). We ﬁrst note that

•

n

Xi=1
n

Dn =

+

x1,i

x′1,i +

n

Xi=1 (cid:0)

ˆq2,i

−

e
e
ˆq2,i

Xi=1 (cid:0)

−

q2,i

′ x2,i
(cid:1)

(cid:1)
n

Xi=1

q2,i

x2,ix′2,i

ˆq2,i
(cid:0)

−

q2,i

′
(cid:1)

x′1,i +

x1,ix′2,i

e

e

ˆq2,i
(cid:0)

−

q2,i

.

(cid:1)

It has already been shown in the previous points that

Moreover, using the moment bound given by Lemma A.1 in Zhang and Wu (2012), we have

e

e

Dn
n

=

1
n

n

Xi=1

x1,i

x′1,i + oP(1).

1
n

n

x1,i

Xi=1 h

e

x′1,i −
e

E

x1,i

x′1,i

(cid:16)
e

e

= oP(1).

(cid:17)i

Then using assumption A2 and the convergence of Riemann sums, we get

1
n

Dn =

1
n

n

E

x1,0(i/n)

x1,0(i/n)′

+ oP(1)

Xi=1
= Σ1 + oP(1).(cid:3)

(cid:2)
e

(cid:3)

e

4.3 Additional results for the proof of Theorem 2

Lemma 2. Assume that the assumptions of Theorem 2 hold true. Then the following properties are valid.

(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)

1. max1
n
i
≤
≤

2. max1
n
i
≤
≤

3. max1
n
i
≤
≤

4. We have

5. Deﬁne

= OP

b +

√log(n)
√nb

+ 1
1
n

−

1
q2 b !

= oP

ˆβ(i/n)

−

n
j=1 ki, j

β(i/n)
(cid:12)(cid:12)(cid:12)

P
ˆσ2

i −

P

(cid:12)(cid:12)(cid:12)(cid:12)

x j

2 = OP(1) and max1
n
i
|
|
≤
≤
n
j=1 ki, je2
j

= oP

P
and max1
n
i
≤
≤

1
√n (cid:19)

(cid:18)

n
j=1 ki, j

x je j

|

|
n
j=1 ki, je2

j −

P

(cid:12)(cid:12)(cid:12)(cid:12)

1/4

.

n−
(cid:16)
= OP(1).

(cid:17)

max
i
1
≤
≤

n (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

1
ˆσ2
i −

1
σ2
i

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

= oP

1/4

n−
(cid:16)

(cid:17)

, max
i
1
≤
≤

n (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

σ2

i −

1
ˆσ2
i −

1
σ2
i −

σ2
i

= oP

1/4

n−

.

(cid:17)

(cid:16)

(cid:12)(cid:12)(cid:12)(cid:12)
n
j=1 ki, je2
j
P
σ4
i

= oP

1/2

n−
(cid:16)

.

(cid:17)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

n

ˇs1,i =

ki, j

Xj=1

x1, jy j
σ2
j

,

s1,i =

n

Xj=1

σ2

j −

n

Xt=1

ki, j 


x2, jy j
σ4
j

,

k j,te2
t 


with similar deﬁnition for ˇs2,i, ˇs3,i and s2,i, s3,i. Then we have for ℓ = 1, 2, 3,

Moreover max1
n
i
≤
≤

sℓ,i

= oP

1/4

n−

(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)

(cid:16)

(cid:17)

max
n
i
1
≤
≤

−

ˆs∗ℓ,i −

ˇsℓ,i

(cid:12)(cid:12)(cid:12)
and max1
n
i
≤
≤

1/2

n−
(cid:16)

.

(cid:17)

= OP(1).

sℓ,i

= oP

(cid:12)(cid:12)(cid:12)
ˆs∗3,i

(cid:16)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

1
−
(cid:17)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

17

 
6. We set m∗i = E

ˇs3,i

and for ℓ = 1, 2,

q∗ℓ,i =

(cid:0)
max
i
1
≤
≤

(cid:1)
ˆq∗ℓ,i −
n (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

e
E ˇs3,i
h
Moreover, if s∗ℓ,i and q∗ℓ,i are deﬁned as sℓ,i and qℓ,i but with a division by σ2
expectations, we have also

q∗ℓ,i −
e

ˆs∗ℓ,i −

1
−
(cid:17)

1
−
(cid:17)

m∗i

m∗i

m∗i

i (cid:16)

−

−

(cid:16)

(cid:16)

h

(cid:0)
1
−
(cid:17)

E ˇsℓ,i

i(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

= oP

1/2

n−
(cid:16)

.

(cid:17)

i inside all the mathematical

1 E
−
(cid:17)

ˇsℓ,i

. Then,
(cid:1)

m∗i

(cid:16)
ˆs∗3,i

Proof of Lemma 2

ˆq∗ℓ,i −

max
i
1
≤
≤

n h(cid:12)(cid:12)(cid:12)

q∗ℓ,i

+

(cid:12)(cid:12)(cid:12)

q∗ℓ,i

q∗ℓ,i −
(cid:12)(cid:12)(cid:12)e

i
(cid:12)(cid:12)(cid:12)

= oP

1/4

n−

.

(cid:16)

(cid:17)

1. For the ﬁrst point, we set di =

n
j=1 ki, j x jx′j and we use the decomposition

ˆβ(i/n)

−

P
1
β(i/n) = d−
i

n

Xj=1

1
ki, j x je j + d−
i

n

Xj=1

ki, j x j x′j

β( j/n)

(cid:2)

−

.

β(i/n)
(cid:3)

= OP(1). Then the result follows from
As in the proof of Lemma 1, point 1, we have max1
n
i
≤
≤
the Lipschitz properties of β, Lemma 3, the second point of the present lemma and our bandwidth
conditions.

1
d−
i
(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)

2. For the second point, we use the fact that

(2012),

1,1

n E
i
≤
≤

n
≥
P

xi

2 <
|

|

∞

and from Lemma A.1 in Zhang and Wu

3. We have

max
n
i
1
≤
≤

n

Xj=1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

ˆσ2(i/n)

n

−

Xj=1

ki, je2
j

= OP

1
√nb !

.

x j

2
|

E

x j

2
|

|

−

ki, j

h|

i

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

n

=

ki, j

Xj=1
= Ai

−

(cid:12)(cid:12)(cid:12)(cid:12)

2Bi.

x′j

β( j/n)
(cid:16)

−

n

2

−

Xj=1

2

ˆβ(i/n)
(cid:17)(cid:12)(cid:12)(cid:12)(cid:12)

ki, je j x′j

ˆβ(i/n)
(cid:16)

−

β( j/n)
(cid:17)

We ﬁrst deal with Ai. We have

Ai

|

| ≤

2

n

Xj=1

ki, j

x j

|

2
|

β( j/n)
(cid:20)|

β(i/n)
|

−

2 +

ˆβ(i/n)

(cid:12)(cid:12)(cid:12)

2

.

(cid:21)

−

β(i/n)
(cid:12)(cid:12)(cid:12)

From the two ﬁrst points and using our bandwidth conditions, we have

max
n |
i
1
≤
≤

Ai

|

= OP

b2

+ oP

(cid:16)

(cid:17)

1/2

n−
(cid:16)

Next, we have Bi = Bi1 + Bi2 with

= oP

1/2

n−

.

(cid:16)

(cid:17)

(cid:17)

n

Bi1 =

n

Xj=1

ki, je j x′j

ˆβ(i/n)
h

−

β(i/n)
i

, Bi2 =

ki, je j x′j

β(i/n)
(cid:2)

−

.

β( j/n)
(cid:3)

Xj=1

18

 
Under our bandwidth condition c < 3

n
j=1 ki, j x je j

3 max1
n
i
≤
≤
(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)
P
Finally, we have Bi2 = b

1/4

= oP
n−
(cid:16)
n
j=1 e j x′j

1
q2

4 −
. From point 1., we deduce that max1
n
i
≤
≤
(cid:17)

and the assumption on Θn,q2(L), we get from Lemma
Bi1|
.
(cid:17)
β( j/n)). One can note that the
1
nb

ki, j = ki, jb−

1 (β(i/n)

n−
(cid:16)

= oP

= O

and

ki, j

1/2

e

i, j

|

ki, j with

weights

−
ki, j satisﬁes similar properties as ki, j. Indeed, we have max1
e
≤

P

n
≤

e

max
n
i
1
≤
≤

max
n
j
1
−
≤
≤

ki, j

1 (cid:12)(cid:12)(cid:12)(cid:12)e

ki, j+1

−

e

(cid:12)(cid:12)(cid:12)(cid:12)

= O

1
n2b2 !

.

(cid:12)(cid:12)(cid:12)(cid:12)e

(cid:12)(cid:12)(cid:12)(cid:12)

(cid:16)

(cid:17)

Then, we claim that Lemma 3 can be applied by replacing the weights ki, j by the weights
ki, j be-
cause the two previous properties on the weights are suﬃcient to get this result (see also the proof of
e
=
Lemma A.3 in Zhang and Wu (2012)). Using our bandwidth conditions, we easily get max1
Bi2|
n
i
|
≤
≤
oP
. This completes the proof of the ﬁrst assertion. The second assertions easily follows Lemma
(cid:17)
3, the Lipschitz property of σ and our bandwidth conditions.

n−
(cid:16)

1/2

4. Using the previous point, we have

1
ˆσ2
i −

1
σ2
i −

ˆσ2
i

σ2

i −
σ4
i

ˆσ2

σ2
i −
i
σ4
i ˆσ2
i

− (cid:16)

max
n
i
1
≤
≤

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

Using point 3 and the positivity of σ, we get the result.

5. Setting z1,i = x2,iyi, z2,i = x2,ix′1,i and z3,i = x2,ix′2,i, we have

= oP

1/2

n−
(cid:16)

.

(cid:17)

2

(cid:17)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

ˆs∗ℓ,i = ˇsℓ,i +

n

Xj=1

1
ˆσ2
i −

1
σ2
i

ki, jzℓ,i 


σ2

i −

n

Xj=1

ki, je2

sℓ,i

max
n
i
1
≤
≤

max
n
i
1
≤
≤

≤

(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

.




j (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

Then the ﬁrst assertion follows from the previous point of the present lemma. For the second assertion,
we note that

and the result follows from the point 3. Finally, the last assertion follows from the ﬁrst one, by using

max
i
1
≤
≤

n (cid:12)(cid:12)(cid:12)(cid:12)

ˇs3,i

E

2
σ−
i x2,i x′2,i
(cid:16)

−

= oP(1), max
i
1
≤
≤

E

2
σ−
i x2,i x′2,i
(cid:16)

(cid:17) −

E

2
σ−
i x2(i/n)x2(i/n)′
(cid:16)

= O(1/n)

(cid:17)(cid:12)(cid:12)(cid:12)(cid:12)

n (cid:12)(cid:12)(cid:12)(cid:12)

(cid:17)(cid:12)(cid:12)(cid:12)(cid:12)

and assumption A4.

6. The ﬁrst assertion follows from the previous point, using the same decomposition as in Lemma 1 for

the diﬀerence ˆq∗ℓ,i −

q∗ℓ,i. The other assertions can be shown as in Lemma 1, points 3 and 4.(cid:3)

e
4.4 Proof of Theorem 2

As in the proof of Theorem 1, we set Dn =

1
ˆσ2
i

n
i=1

P

ˆy∗i =

ˆx∗1,i

′ β1 + x′2,i ˆr∗i + ei,
(cid:17)

(cid:16)

ˆx∗1,i

(cid:16)
ˆr∗i =

ˆx∗1,i

′ and we use the decomposition
(cid:17)
ˆq∗2,i −
(cid:16)

β1 − (cid:16)

ˆq∗1,i −

q∗2,i

q∗1,i

(cid:17)

(cid:17)

.

19

 
We get

ˆβ∗1 −

β1 =

Using now the decomposition ˆx∗1,i =
e
ˆβ∗1 −

x∗1,i − (cid:16)
β1 =

where

n

Xi=1

1
ˆσ2
i

ˆx∗1,i

h

q∗2,i

′ x2,i, with

x′2,iˆr∗i + ei

.

i
x∗1,i = x1,i

(cid:17)
N∗1 −

e
N∗2 + N∗3 −

N∗4

,

i

D∗n

1
−
(cid:1)
(cid:0)
ˆq∗2,i −
D∗n

(cid:0)

1
−
(cid:1)

h

q∗2,i

′ x2,i, we have
(cid:17)

− (cid:16)

N∗1 =

n

Xi=1

N∗3 =

1
ˆσ2
i

n

Xi=1

x∗1,ix′2,i ˆr∗i , N∗2 =

e
1
ˆσ2
i

x∗1,iei, N∗4 =

n

Xi=1
n

Xi=1

1
ˆσ2

1
ˆσ2

ˆq∗2,i −

i (cid:16)

ˆq∗2,i −

i (cid:16)

q∗2,i

′ x2,ix′2,i ˆr∗i ,
(cid:17)

q∗2,i

′ x2,iei.
(cid:17)

e
We ﬁrst consider the leading term 1
√n

•

N∗3. Using Lemma 2, we have

1
√n

N∗3 =

1
√n

n

x∗1,iei
σ2
i

+

1
√n

n

σ2

i −

Xi=1

n
j=1 ki, je2
j
P
σ4
i

x∗1,iei + oP(1).

Xi=1 e

e

As in the proof of Theorem 1, one can show that the ﬁrst term is asymptotically Gaussian with mean
0 and variance Σ∗. Note that one can use directly the central limit theorem for martingale diﬀerences
for this step. For the second term, we decompose

n

σ2

i −

Xi=1

n
j=1 ki, je2
j
P
σ4
i

x∗1,iei = S 1 + S 2,

e

with S 1 =

n
i, j=1 ki, j

σ2
i

σ2
(cid:16)

j −

= oP

x∗1,iei
σ4
i
n
i, j=1 ki, j

(cid:17) e

P

(2012), Lemma A.1 and S 2 =
r1 = 2 and r2 = 4.

P

√n

by using the moment inequality given in Zhang and Wu

(cid:16)

(cid:17)
e2
j −
(cid:16)

σ2
j

x∗1,iei
σ4
i

(cid:17) e

= oP

√n
(cid:17)

(cid:16)

by using Lemma 5 with r = 4/3,

•

•

From Lemma 2, we easily get N∗2 = oP

.

√n
(cid:17)

(cid:16)

Next, we show that N∗4 = oP

(cid:16)

√n
. Using Lemma 2, it is easily seen that
(cid:17)

n

N∗4 =

ˆq∗2,i −

q∗2,i

Xi=1 (cid:16)

x2,iei
σ2
i

′
(cid:17)

+ oP

.

√n
(cid:17)

(cid:16)

Moreover, using Lemma A.1 in Zhang and Wu (2012), we have

Using Lemma 2(6), it remains to show that

N∗4 =

n

Xi=1 (cid:16)

ˆq∗2,i −

q∗2,i

e
n
i=1 z′i

zi =

1
−

m∗i
(cid:16)

(cid:17)

ˆs∗3,i

mi
h

−

P
m∗i

i (cid:16)

1
−
(cid:17)

20

x2,iei
σ2
i

′
(cid:17)

+ oP

.

√n
(cid:17)

(cid:16)

x2,iei
σ2
i

= oP

(cid:16)

or zi =

√n
(cid:17)
1
−
(cid:17)

when

ˆs∗2,i −

h

E ˇs2,i

.

i

m∗i
(cid:16)

Let us detail the ﬁrst case. We have m∗i zim∗i = z1,i + z2,i with z1,i = ˇs3,i
one can use Lemma 5 with r1 = q1 and r2 = q2. For the z2,i, we have

−

E ˇs3,i and z2,i = s3,i. For z1,i,

n

m∗i

1
−
(cid:17)

z′2,i

m∗i
(cid:16)

(cid:17)

1 x2,iei
−
σ2
i

Xi=1 (cid:16)

Cn max
j
1
≤
≤

≤

n

Xi=1

ki, j

x′2,iei
σ2
i

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

n (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

n

Xt=1

max
j
1
≤
≤

·

n (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

k j,te2

t −

σ2

.

j(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

Then the result follows from Lemma 3. Note that the ﬁrst maximum involves the weights ki, j instead
of k j,i but it does not change the result and this maximum is a oP
. Moreover, the second
(cid:17)
ˆs∗2,i −
i
h
√n
. Details are omitted.
(cid:17)

n−
(cid:16)
1
maximum is oP
m∗i
−
(cid:17)
then omitted. Using analog arguments, one can also show that N∗1 = oP

by applying Lemma 2(3). The case zi =

is similar and

n−
(cid:16)

E ˇs2,i

1/4

1/4

(cid:17)

(cid:16)

(cid:16)

Finally, we show that D∗n/n

Σ∗ in probability. Using the decomposition

•

→

n

Xi=1
n

D∗n =

+

Xi=1 (cid:16)

1
ˆσ2
i

x∗1,i

e
ˆq∗2,i −

x∗1,i

(cid:16)
e
q∗2,i

(cid:17)

′

(cid:17)

n

′ +

Xi=1 (cid:16)
x2,i
x′1,i
ˆσ2
e
i

+

ˆq∗2,i −

q∗2,i

n

(cid:17)
x1,i x′2,i
ˆσ2
i

Xi=1 e

x2,i x′2,i
ˆσ2
i

ˆq∗2,i −

q∗2,i

.

(cid:17)

(cid:16)

ˆq∗2,i −
(cid:16)

q∗2,i

′

(cid:17)

Using the previous points and Lemma 2(4), we get

D∗n
n

=

1
n

n

x∗1,i

Xi=1 e

′
(cid:17)

x∗1,i
(cid:16)
ˆσ2
e
i

+ oP(1) =

1
n

n

x∗1,i

Xi=1 e

′

(cid:17)

x∗1,i
(cid:16)
σ2
e
i

= oP(1).

As for the end of the proof of Theorem 1, we get n−

1D∗n = Σ∗ + oP(1). (cid:3)

4.5 Auxiliary results

We ﬁrst recall a lemma given in Zhang and Wu (2012) for uniform convergence of kernel smoothers.

Lemma 3. Let zn,i = Jn,i (
zn,i
sup1
i
1 k
≥
≤
≤

r <
k

∞

n,n

F

and Θn,r(J) = O

for ν > 1

2 −

1
r . Then,

i) be a centered triangular array. We assume that there exists r > 2 such that

u

1
nb

K

sup
[0,1]

u
∈

n

Xi=1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

ν
n−
(cid:0)
i/n
−
b

!

(cid:1)

zn,i(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

=

OP(vn)
nb

, where vn = n1/r +

nb log n.

p

The second lemma is useful for getting convergence rates of quadratic forms.

Lemma 4. Let zn,i = J1,n,i (
and q1, q2 > 2 two real numbers such that 1
q1

i) and wn,i = J2,n,i (

F

F
+ 1
q2

i) be two centered triangular arrays. Let q
q and Θ0,q1 (J1) + Θ0,q2 (J2) <

= 1

(1, 2]
and Qn =

∈

∞

i< j

n αi, jzn,iwn, j, where for 1
≤

≤

≤

i < j

n, αi, j is a real number. Then we have

1
≤
P

where

Qn

k

−

EQn

q
k

≤

Cn1/q

Θ0,q1(J1) + Θ0,q2(J2)
i
h

2

an,

a2
n = 
max
i<n
1

≤

n

Xj=i+1

αi, j
|

2
|

max
1< j
n
≤

∨ 






j
1
−

Xi=1

αi, j
|

2
|

.





21

 
Proof of Lemma 4 We follows the proof of Proposition 1 in Liu and Wu (2010) which gives a control of
Lq norms of quadratic forms when q
2. See also Lemma A.2 in Zhang and Wu (2012) for a similar result.
We set

≥

un, j =

j
1
−

Xi=1

n

αi, jzn,i,

v(k)
n,i =

αi, jw(k)
n, j,

Xj=i+1

Z, w(k)

n, j is deﬁned as wn, j but the innovation ξk is replaced with ξ′k in

j (ξ′ is a copy of ξ).

F

where for k
Deﬁning

∈

we have

k = E (

k)

−

E (

·|F

k

1) ,
−

·|F

P

αi, j

zn,iwn, j
h

−

n,i w(k)
z(k)

n, j

q

i k

wn, j
h

−

w(k)
n, j

q +
k

k

i

n
1
−

Xi=1

v(k)
n,i

zn,i
h

−

z(k)
n,i

q
k

i

kQn

kP

q
k

≤ k X1
i< j
n
≤
≤
n

un, j

≤ k

Xj=2
= Ik + IIk.

Using the moment bound given in Lemma A.1 in Zhang and Wu (2012), we have

Ik

≤

≤

n

Xj=2

un, j
k

wn, j

q1 · k
k

w(k)
n, jk

q2

−

Can

n

Xj=2

k,q2 (J2) Θ0,q1 (J1) .
δ j
−

Using similar arguments, we get

Since

we get

kQn

kP

q
q ≤
k

≤

IIk

≤

Can

n
1
−

Xi=1

k,q1 (J1) Θ0,q2 (J2) .
δi
−

n

Xj=2

δ j
k,2q(J2)
−

≤

Θ0,q2(J2),

n
1
−

Xi=1

δi
k,2q(J1)
−

≤

Θ0,q1(J1),

k + IIq
Iq

k

i

C

h
Caq
n

Θ0,q1(J1) + Θ0,q2(J2)
(cid:17)

(cid:16)

2q

1
−

n

Xj=2





k,2q(J2) +
δ j
−

n
1
−

Xi=1

.

δi
k,2q(J1)
−


22

Finally, using Burkholder’s inequality, we get

Qn

k

−

EQn

q =
k

≤

≤

≤

n

k

Xk=
−∞

CE1/q

n

kQn

P

q
k

n

Xk=
−∞



(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)


kQn]2

[
P

1/q

q/2

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)





C 
Xk=

−∞
Cann1/q

E

kQn

q
|



|P


Θ0,q1(J1) + Θ0,q2(J2)
i
h

2

.

This completes the proof of the lemma.(cid:3)

i) and wi = J2,n,i (

Lemma 5. Let zi = J1,n,i (
r
for d = min

(1, 2] such that Θ0,r1(J1) + Θ0,r2(J2) <
,
(cid:17)

r , 1

2
(cid:16)

F

−

∞

∈

2

2

1
√n

n

n

zi

Xi=1

·

Xj=1

ki, jwi = oP(1).

= O(1), we have

ki,iziwi = OP

1
√nb !

= oP(1).

n E
Proof of Lemma 5 Since max1
i
≤
≤

1
√n

Moreover, we have

ziwi
|

|

n

Xi=1

n

Using Lemma 4, we get under our bandwidth conditions

max
i<n 
1
≤

Xj=i+1

k2
i, j



∨ 




j
1
−

Xi=1

max
1< j
n
≤

= O

1
nb !

.

k2
i, j



i) be two centered triangular arrays. Assume that there exists
F
where r1, r2 > 2 are such that 1

. Then, if ndb

r = 1
r1

+ 1
r2

→ ∞

1
√n X1
i, j
n
≤
≤

ki, jziw j = OP

n1/r
n √b !

= oP(1).

Finally, we have

1
√n X1
i, j
n
≤
≤

ki, jE

= O

ziw j
(cid:16)

(cid:17)

1
√nb !

.

To show (9), we bound the covariances as follows. If i

j, we have

≤

(9)

E

(cid:12)(cid:12)(cid:12)(cid:12)

ziw j
(cid:16)

(cid:17)(cid:12)(cid:12)(cid:12)(cid:12)

E

ℓzi

ℓw j

· P

(cid:16)P
(cid:17)(cid:12)(cid:12)(cid:12)(cid:12)
ℓ,r2 (J2).
ℓ,r1 (J1)δ j
δi
−
−

≤ Xℓ
i (cid:12)(cid:12)(cid:12)(cid:12)
≤
≤ Xℓ
i
≤

23

 
 
 
 
Hence, we obtain

E

ki, j

1
√n X1
(cid:12)(cid:12)(cid:12)(cid:12)
i, j
n
≤
≤
n ki, j = O
≤

i, j

ziw j
(cid:16)

√n max
i, j
n
1
≤
≤

≤

ki, jΘ0,r1(J1)Θ0,r2(J2),

(cid:17)(cid:12)(cid:12)(cid:12)(cid:12)
. The proof of Lemma 5 is now complete.(cid:3)

which leads to (9) since max1
≤
+
d be a continuous application taking values in the space

Lemma 6. Let Σ : [0, 1]
of size d, symmetric and positive deﬁnite. Assume that for each u
the inequality

→ S

∈

(cid:17)

(cid:16)

1
nb

+
d of square matrices
[0, 1], Σ(u) is invertible. Then we have

S

(cid:22) Z
0
denotes the order relation for nonnegative deﬁnite matrices. Moreover, the equality between these

 Z
0

where
two matrices holds if and only if the application Σ is constant.

(cid:22)

1
−

1

1

Σ(u)du
!

Σ(u)−

1du,

Proof of Lemma 6 For x, y

∈

Rd, we have from the Cauchy-Schwarz inequality,

y)2 =

(x′

·

1

x′

·

Σ(u)−

1/2Σ(u)1/2

2

·

ydu

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

x′

≤

· Z
0

Then setting y =

1
0
(cid:18)R

Σ(u)du
(cid:19)

x, we get

Z
0

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

1
−

·

x′

1

Σ(u)du
!

·  Z
0

1
−

x

·

≤

x′

· Z
0

1

1

Σ(u)−

1du

1

y′

x

·

·

· Z
0

Σ(u)du

y.

·

Σ(u)−

1du

x,

·

which shows the inequality. Moreover, if the two matrices are equal, the equality condition in the Cauchy-

Schwarz inequality entails that for each x

, there exists a real number λx such that x = λxΣ(u)

∈
Integrating the previous relation with respect to u, we ﬁnd that λx = 1 and then Σ(v) =

\{

Rd

0
}

1
0
R

Σ(u)du, v

∈

1
−

x.

Σ(u)du
(cid:19)

1
0
(cid:18)R
[0, 1].(cid:3)

References

G. Chamberlain. Eﬃciency bounds for semiparametric regression. Econometrica, 60:567–596, 1992.

B. Chen and Y. Hong. Testing for smooth structural changes in time series models via nonparametric

regression. Econometrica, 80:1157–1183, 2012.

R. Dahlhaus. Fitting time series models to nonstationary processes. Ann. Statist., 25:1–37, 1997.

R. Dahlhaus and S. Subba Rao. Statistical inference for time-varying arch processes. Ann. Statist., 34:

1075–1114, 2006.

R. Dahlhaus, S. Richter, and W.B. Wu. Towards a general theory for non-linear locally stationary processes.

arXiv:1704.02860, 2017.

J. Fan and T. Huang. Proﬁle likelihood inferences on semiparametric varying-coeﬃcient partially linear

models. Bernoulli, 11:1031–1057, 2005.

24

J. Fan and W. Zhang. Statistical methods with varying coeﬃcient models. Statistics and Its Interface, 1:

179–195, 2008.

J. Gao and K. Hawthorne. Semiparametric estimation and testing of the trend of temperature series. Econom.

J., 9:332–355, 2006.

W. Liu and W.B. Wu. Asymptotics of spectral density estimates. Econometric Theory, 26:1218–1245, 2010.

P.M. Robinson. Root-n-consistent semiparametric regression. Econometrica, 56:931–954, 1988.

P. Speckman. Kernel smoothing in partial linear models. J. Roy. Statist. Soc. B, 50:413–436, 1988.

S. Subba Rao. On some nonstationary, nonlinear random processes and their stationary approximations.

Adv. in App. Probab., 38:1155–1172, 2006.

L. Truquet. Parameter stability and semiparametric inference in time-varying arch models. Forthcoming in

J. R. Stat. Soc. Ser. B, 2016.

A.W. van der Vaart and J.A. Wellner. Weak convergence and empirical processes. Springer Verlag, New-

York, 1996.

T. Zhang and W.B. Wu. Inference of time-varying regression models. Ann. Statist., 40:1376–1402, 2012.

25

