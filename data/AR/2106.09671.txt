1
2
0
2

n
u
J

7
1

]
I
S
.
s
c
[

1
v
1
7
6
9
0
.
6
0
1
2
:
v
i
X
r
a

An Attract-Repel Decomposition of Undirected
Networks

Alexander Peysakhovich
Facebook AI Research

Leon Bottou
Facebook AI Research

Abstract

Dot product latent space embedding is a common form of representation learning
in undirected graphs (e.g. social networks, co-occurrence networks). We show that
such models have problems dealing with ‘intransitive’ situations where A is linked
to B, B is linked to C but A is not linked to C. Such situations occur in social
networks when opposites attract (heterophily) and in co-occurrence networks when
there are substitute nodes (e.g. the presence of Pepsi or Coke, but rarely both, in
otherwise similar purchase baskets). We present a simple expansion which we call
the attract-repel (AR) decomposition: a set of latent attributes on which similar
nodes attract and another set of latent attributes on which similar nodes repel. We
demonstrate the AR decomposition in real social networks and show that it can
be used to measure the amount of latent homophily and heterophily. In addition,
it can be applied to co-occurrence networks to discover roles in teams and ﬁnd
substitutable ingredients in recipes.

1

Introduction

Network data is ubiquitous across many disciplines ranging from the natural sciences (Jeong et al.,
2001; Barabasi and Oltvai, 2004) to the social sciences (Granovetter, 1985; Easley et al., 2010;
Jackson, 2010). However, networks are high dimensional objects and thus can be difﬁcult to work
with. Finding easy representations of how nodes are connected is an important topic of study. In the
symmetric (or undirected) case a workhorse method are dot product models (Lov´asz and Vesztergombi,
1999; Ng et al., 2002; Perozzi et al., 2014; Tang et al., 2015; Grover and Leskovec, 2016; Athreya
et al., 2017; Lerer et al., 2019). In dot product models each node in a network is associated with
an embedding (a.k.a latent vector) in Euclidean space and dot product between vectors reﬂects the
strength of an edge between two nodes.

We show that the dot product model can fail in the sense that while the network has simple structure,
the dot product model requires a high dimensionality to represent the network well. Such failure
will occur whenever networks exhibit a lack of ‘transitivity’ A is strongly connected to B and B is
strongly connected to C but A is not connected to C. In such cases the dot product model struggles as
representing this triangle requires us to construct vectors where A and B are close, B and C are close
but A and C are far. When networks have many such combinations, representing such a network with
high ﬁdelity will require high dimensional vectors.

Such ‘forbidden triads’ (Granovetter, 1973) – or more general versions of this pattern – are ubiquitous
in networks. They can occur in social networks where they can be signals of heterophily (individuals
with similar attributes are less likely to be friends) or ‘enemies’ (there is clearly something going on
in a citation network if two scientists cite all of the same colleagues but never cite each other). In
co-occurrence networks they can indicate that nodes play similar ‘roles’ such the formation of teams
or the combination of ingredients in recipes. Thus, a lossy model which has trouble describing these
types of structures is likely missing something important about the network when the embedding
dimension is much lower than the node count, as in most applications of network embeddings.

Preprint. Under review.

 
 
 
 
 
 
We show that a minimal way to expand the dot product model is to consider two sets of latent
attributes: ones on which nodes attract and ones on which nodes repel. We refer to this as an attract-
repel (AR) decomposition. We show how to construct AR decompositions from a combination of
nuclear norm minimization and eigendecomposition. We then perform several experiments using the
AR decomposition.

In experiment 1 we use it to measure latent homophily (similar types attract) and heterophily (similar
types repel) in real world social networks both at the network and individual level. We show that in
real social networks there is a substantial heterophilic component and so AR embeddings can more
efﬁciently reproduce network structure than dot product models. Furthermore, we show that in a
network with known community structure the relative magnitude of an individual’s repel vs. attract
vectors tells us what fraction of the individual’s links are within their own community vs. outside of it.
We also show that the repulsion component of an AR decompositions can be used to ﬁnd individuals
which ﬁll similar roles in a team (experiment 2) or ingredients which can be substituted for one
another in recipes (experiment 3).

2 Related Work

Recently Seshadhri et al. (2020) show that dot product random models cannot reproduce various
macro features of real world social networks. Building on this insight (Chanpuriya et al., 2020)
consider factorizing adjacency matrices with two vectors per node, a ‘target’ and a ‘context’ vector
showing that such embeddings are able to reproduce the macro features. Ruiz et al. (2020) applies the
concept of 2 embeddings per item to their dataset of a series of ‘baskets’ (items purchased at a single
time) to ﬁnd complements and substitutes.Word2vec (Mikolov et al., 2013), a workhorse model in
natural language processing, trains two embeddings per word.

The two embedding approach is overparameterized when the underlying matrix is symmetric
since the symmetry imposes n equalities across dot products of the vectors (targeti · contextj =
targetjcontexti). The AR decomposition takes advantage of this symmetry to give a simpler decom-
position.

The two embedding approach is inspired by word2vec (Mikolov et al., 2013), a workhorse model in
natural language processing. Since word2vec predicts co-occurrences within a window, if a co-occurs
with b then b co-occurs with a so it may seem that word2vec is a symmetric model and so amenable
to an AR decomposition. However, in practice, the word2vec objective is optimized using negative
sampling which downsamples common words and upsamples rare words which means the implied
matrix is not exactly symmetric (Levy and Goldberg, 2014).

Real similarity/dissimilarity matrices (e.g. similarity between words as rated by psychology experi-
ment participants) can often can be non-metric. Laub and M¨uller (2004) shows in several exploratory
analyses that the negative eigenspaces appear to have interpretable characteristics. Van der Maaten
and Hinton (2012) focuses on visualizing such matrices using multiple embeddings similar to our
AR decomposition. This work is related to ours though differs in that in dissimilarity matrices the
diagonal is held ﬁxed (all elements are 0 dissimilar to themselves) whereas a key part of our approach
is that we treat self-edges as a nuisance parameter rather than one that needs to be represented by the
embedding.

3 The AR Decomposition of Symmetric Networks

We begin with a set of nodes V = 1, 2, . . . , n. There is an edge weight between any two nodes pij.
We let Pij be the n × n matrix of edge weights.

For this paper we are interested in looking at symmetric networks, so Pij = Pji. This includes, for
example, undirected social networks (e.g. friendship graphs, coauthorship networks), co-occurrence
graphs, or trade ﬂows.

We begin with the following question: is there a succinct representation of the network P ?

We ﬁrst introduce the following proposition:
Proposition 1. Any symmetric matrix M has a family of decompositions of the form

M = D + AA(cid:48) − RR(cid:48)

2

where D is a diagonal matrix.

The decomposition above is overparametrized - there are many choices of D, A, R that satisfy the
equation above. This naturally leads to the question is which decomposition from the family is the
‘correct’ one.

Our main idea is that when we consider network analysis the self-edges of the original matrix P (i.e.
the elements Pii) are typically not considered as meaningful edges. Therefore, we consider treating
the diagonal above as a nuisance parameter and choosing the ‘simplest’ decomposition in terms of
A and R. In other words, we are looking for the simplest decomposition of only the off-diagonal
elements.
Deﬁnition 1. The AR decomposition of a matrix M is the solution to the optimization problem:
||A||2

min
D,A,R

F + ||R||2
F
s.t. M = D + AA(cid:48) − RR(cid:48)

(1)

The rank of the AR decomposition is deﬁned as rank(A) + rank(R).

We can further show that choosing the D, A, R matrices as in the optimization problem 1 is equivalent
to choosing a diagonal for the matrix M while keeping the off diagonal terms to ﬁnd the simplest
possible matrix in terms of nuclear norm || · ||∗. Under some mild conditions on the matrix the nuclear
norm is a convex envelope of the matrix’s rank (Recht et al., 2010) and so it has been suggested to use
nuclear norm minimization as a way of ﬁnding low rank solutions to matrix completion problems.
Proposition 2. Let A, R be solutions to 1. For a matrix M let M OD refer to the off diagonal terms.
Then the matrix ˆM = AA(cid:48) − RR(cid:48) a the solution to

min
˜M
s.t.

|| ˜M ||∗
˜M OD = M OD

(2)

The equivalence between choosing a diagonal to minimize the nuclear norm and choosing the simplest
AR decomposition in terms of Frobenius norm gives us a way to compute the AR decomposition
using convex optimization. We refer to the solution produced by this as the decomposition of the
matrix.

1. Construct the augmented matrix ˆP whose off-diagonal (OD) is deﬁned by the graph and

solve the convex problem:

|| ˆP ||∗ s.t. ˆPij = pij∀i (cid:54)= j

min
ˆP

2. Compute the eigendecomposition of ˆP = Q(cid:48)DQ. Any symmetric matrix has such a decom-

position with real eigenvalues.

3. (Optional) To use a k dimensional embedding, truncate the n − k smallest eigenvalues to 0
4. Let D− be the negative eigenvalues and D+ be the positive ones. Let Q− correspond
to the eigenvectors with negative eigenvalues and Q+ be the eigenvectors with positive
eigenvalues.
5. Let A = Q+

D+ and let R = Q−

−D−

√

√

Note that we can modify the algorithm to construct an A-only decomposition, which we will show
in the next section always exists, by adding a constraint to the nuclear norm minimization problem
in step 1 such that ˆP is positive semi-deﬁnite (PSD) which will guarantee that all eigenvalues are
weakly positive and so the R component will have rank 0.

4 Motivating Examples

We have shown that every matrix has an AR decomposition where the diagonal is treated as a
nuisance parameter. In social network analysis a commonly used model is the dot product random
graph which models the probability of a node i being connected to node j as proportional to g(vi · vj)
where g is a link function (Young and Scheinerman, 2007; Sussman et al., 2012). We can now show

3

that the R matrix is in some sense not needed at all - the dot product random graph can perfectly
represent the off-diagonal elements given enough dimensions.

Proposition 3. Any symmetric matrix M has a family of decompositions of the form

where D is a diagonal matrix.

M = D + AA(cid:48)

However, we will now show that these representations can be unsatisfying in the sense that very
simple networks can require very high dimensional embeddings to faithfully represent them.

1

2

3

4

5

6

7

8

Figure 1: This graph has simple structure
but requires high dimensionality to be
represented faithfully by a dot product
model.

Consider the graph in Figure 1. There is a simple structure
to the graph: purple nodes connect to green nodes. We say
that a dot product model represents the graph if vi · vj = 1
for all linked nodes and 0 for all unlinked nodes. Consider
the general version of this graph where there are nper
nodes of each type. We can show the following result:

Proposition 4. Any vectors {v1, . . . , vk} which represent
the generalized version of the graph in Figure 1 must have
dimension ≥ 2nper − 1.

We relegate the full proof to the Appendix, however, we
can give the intuition quite simply: the dot product model
in low dimensions is ‘transitive’ in that if A is close to B
and B is close to C, A must also be close to C. In order
to express such unclosed triangles the dot product model
needs additional free parameters or, in other words, extra
dimensions.

By contrast for any nper there is a simple AR representa-
tion of the following form: let ai = 1√
for all nodes i. Let
2
for green nodes and ri = − 1√
ri = 1√
for purple nodes.
2
2
The ﬁgure above shows that dot product models have a problem representing heterophily a.k.a. the
fact that sometimes ‘opposites attract.’ In the Appendix, we expand this analysis to a popular class of
network generating models: stochastic block models (Holland et al., 1983).

Such issues are not relegated to social networks. A similar problem for the dot product model appears
in another canonical setting: matrix factorization based recommender systems (Koren et al., 2009). In
such systems we have as set of users and a set of items. In the simplest case we have a set of entries
eij which are 1 if user i purchased product j and 0 otherwise. We want to approximate eij as a low
dimensional vector ui for the user and mi for the item.

Consider a purchase dataset constructed from a hypothetical sandwich shop. The shop has M breads
and M lunch meats. Individuals come into the shop and order their favorite meat on their favorite
bread. Bread and meat preferences are uniform in the population and there is no correlation between
an individual’s meat preference and their bread preference. This gives us a bread-meat co-purchase
dataset where each row is a 2M dimensional vector with a single 1 in the ﬁrst M columns and a
single 1 in the second M columns. Let D be the co-purchase dataset which includes 1 of each legal
combination.

Proposition 5. Any vectors {u1, . . . , uN }, {m1, . . . , mM } which represent the sandwich dataset D
must have dimension ≥ 2M − 1.

A major use of recommender systems is to construct recommendations of the form “users who
purchased X also purchased Y ”. This is a statement about the co-purchase matrix, not necessarily
the full data matrix. Looking at the co-purchase graph between items shows the connection between
the issue in the social network case and the sandwich case. Here all breads are co-purchased with
all meats but never with each other, yielding the same graph as in Figure 1. Thus, the same 2-d
representation of items sufﬁces to consider all interesting interactions (i.e. excluding the ‘item with
itself’ interaction).

4

5 Experiments

5.1 Heterophily in Real Social Networks

The AR decomposition lets us decompose a social nework into its heterophilic and homophilic parts.
In particular, we can compare the variance in A to the variance in R to look at the amount of latent
homophily vs. latent heterophily present in the network.

We ﬁrst consider the anonymized ego-networks (an ego network takes a focal ego, takes all of their
friends, and maps the friendships between them) of 627 users of a music social network.1 We consider
users with at least 50 friends (mean ego network size = 81.6).

(a) Reconstruction error as function of rank

(b) Rank required to achieve desired reconstruction error

Figure 2: In the ego network dataset we see that much of the variance can be explained via a low rank
reconstruction (panel a) and that the AR decomposition is much more efﬁcient at reconstructing the
network. Panel b shows this more starkly, it can take 30 − 70% more dimensionality to reach the
same level of reconstruction ﬁdelity using the A over the AR decomposition.

Denote by ˆP k the k dimensional approximation to ˆP . We look at the rank k error as e(k) =
(cid:80)
i(cid:54)=j ( ˆpij)2 and the reconstruction ﬁdelity

ij)2. The normalized reconstruction error is

i(cid:54)=j(ˆpij − ˆpk

e(k)

is 1− the reconstruction error.
Another choice of diagonal has di = (cid:80)
j(cid:54)=i Aij. This is the unsigned graph Laplacian. We look at
the off diagonal reconstruction error of this diagonal choice as well referring to it as the Unsigned
Spectral Decomposition (USpectral).

(cid:80)

We compute the A and AR decompositions using the nuclear norm based procedure described earlier.
To compute the approximate nuclear norm minimizing diagonal we use singular value thresholding
(SVT Cai et al. (2010)) from the R package ﬁlling (You, 2020).

We also use the generalized Gabriel bi-cross-validation (BCV) procedure proposed in (Owen et al.,
2009) to construct an estimate of each network’s optimal approximation rank. In BCV the row and
column indices are split into folds, one fold of the matrix, is held out while the rest of the matrix is
used to ﬁt a low-rank approximation. The rank chosen is the one which minimizes average held out
loss. We use a split of 10 folds. The average BCV chosen normalized rank is approximately 9% of
the full rank indicated by a gray line on the plot.

1These networks were collected via a the network’s public API (Rozemberczki et al., 2020) and are available
on the Stanford Network Analysis Project https://snap.stanford.edu/data/deezer_ego_nets.
html.

5

0.000.250.500.751.000.000.250.500.751.00Relative RankOff Diagonal  Reconstruction ErrorEmbeddingA OnlyARUSpectralllllllllllllllllllllllllllllllllllllllll0.10.20.30.50.60.70.80.9Desired Reconstruction PrecisionRelative Rank RequiredEmbeddingllA OnlyARFigure 3: The fraction of an individual’s variance explained by negative eigenvalue vectors (||ri||),
a.k.a. the individual level latent heterophily, is strongly correlated with a observed measure of
heterophily, fraction of e-mail communications outside one’s own department.

k

Figure 2 panel a shows the reconstruction error averaged over the 627 networks by the normalized
Rows(P ) ) of the approximation. The AR decomposition attains a much lower error than the A
rank (
only dcomposition. The USpectral decomposition does quite poorly at reconstruction of the non-self
edges of the graph compared to the AR or even the nuclear norm minimizing A decomposition.
However, we note that the Laplacian embeddings have other properties and are not intended simply
for optimal representation of the network.

Panel b shows the A vs AR comparison in a different way. We plot how large of an embedding we
need to recover a ﬁxed ﬁdelity decomposition on average across the 627 networks. We see that the A
decomposition requires a ∼ 50% higher dimensionality to recover the network with the same ﬁdelity
as the BCV chosen AR decomposition.

So far we have focused on the variance explained by the negative eigenvalues as a measure of
||ri||
heterophily in the whole network. This concept can also be applied at the individual level -
||ai||+||ri||
is a proxy for what fraction of the variance in node i’s adjacency matrix is explained by the repel-
component.

We consider the EU e-mail network dataset (Leskovec et al., 2007; Yin et al., 2017). This network
consists of 1005 individuals at a European research institution. We use a symmetric version of
the network where an edge exists between two individuals if they have ever emailed each other.
Each individual also belongs to one of 42 different research departments. We construct the AR
decomposition as above with BCV selecting a 57 dimensional representation.

Figure 3 shows that there is a strong relationship between
edges that are outside of their own research department.

||ri||

||ai||+||ri|| and the fraction of an individual’s

5.2 Structural Roles in Networks

There are two notions of ‘similarity’ in the graph embedding literature. First order similarity refers
to whether two nodes are connected. Second order similarity refers to whether nodes have similar
neighborhoods (i.e. are linked to similar nodes). Sometimes second order similarity is referred to as
‘structural equivalence’ (Lorrain and White, 1971).2

2These two similarities can reﬂect different information about the graph and some popular embedding
approaches (e.g. LINE (Tang et al., 2015)) actually produce two separate embeddings one which reﬂect ﬁrst-
order and one which reﬂects second-order similarity and then concatenate them together to use them as node
embeddings. A simple way to capture both types of similarity is to train two embeddings per node, a ‘target’ and
a ‘context’, to learn pij = ti · cj. One way to construct such embeddings is via an SVD (or logistic SVD) of the

6

llllllllll0.250.500.751.001510|| r || / (|| r || + || a ||) DecileFraction of Across  Department EdgesEU EmailsThe AR decomposition makes it easy to ﬁnd both ﬁrst and second order similarities. This can be
deﬁned as 1Simij = aiaj − rirj. First order similarity is just the implied strength of an edge between
two nodes. Second order similarity can also be deﬁned as 2Simij = aiaj + rirj. Note that nodes that
have a high 2Sim score are precisely those which have similar patterns of connections to other nodes.

We can look at how these similarities together tell us things that they cannot individually. An
interesting case is nodes which have low 1Sim, in other words they are only weakly directly connected
but have high 2Sim. In co-occurrence networks these are nodes which occur in the same context but
rarely together - in other words, they have similar roles or, in economic parlance, they are substitutable
(Ruiz et al., 2020).

A simple way to discover substitutes for a node i is to compute 2Simij − 1Simij. However, plugging
in the deﬁnitions shows us that ﬁnding substitutes given an AR decomposition is as simple as ﬁnding
nearest R neighbors.

5.3 Similar Team Roles

We begin by looking at data from the online game DotA2. In this game individuals are placed in a
team of 5, each individual chooses one of ∼ 120 ‘heroes’, and the team competes against another
team of 5.

Heroes in DotA are different and specialized. To be a successful team, a group needs to have a
balanced set of heroes. Each hero can ﬁll one or more of 9 possible roles. These roles are Carry,
Disabler, Durable, Escape, Initiator, Jungler, Nuker, Pusher, and Support. See https://dota2.
fandom.com/wiki/Role for more details. Heroes usually can ﬁll more than one role, though
role types are correlated (for example, most Nuker heroes are usually not Durable).

Figure 4: In our DotA data we see that similarity in A vectors does not predict similarity in roles very
well but similarity in R vectors (produced without knowing roles) does.

We use a publicly available Kaggle dataset3 of ∼ 39, 000 DotA matches. From this data we construct
a co-occurrence matrix for the heroes. Letting cij be the co-occurrence between i and j. Because the
co-occurences are extremely right skewed, we consider the matrix of log(cij + 1) though qualitatively
all our results go through using the raw co-occurrence counts as well.

We take the AR decomposition of this co-occurrence matrix. We again use the BCV procedure to
select the dimensionality of the embedding. The BCV procedure selects a 10 dimensional represen-
tation and we ﬁnd that 5 of these dimensions are associated with negative eigenvalues and 5 are
associated with positive ones.

adjacency matrix which constructs both row and column embeddings (Chanpuriya et al., 2020). For symmetric
networks this is overparametrized as there are extra constraints: pij = pji implies that ticj = tjci.

3Available

here:

overview.

https://www.kaggle.com/c/mlcourse-dota2-win-prediction/

7

lllllllllllllllllllllllllllllllA SimilarityR Similarity−4−2024−2.50.02.50.30.40.50.60.70.20.30.40.50.6Normalized SimilarityRole SimilarityDotA2 RolesWe take what are (as of the time of this paper) the roles the DotA-wiki states that each hero can play
and construct a 9 dimensional vector with a 1 if the hero can play that role and a 0 otherwise. We
then compute the cosine similarity between the role vectors of any two heroes. Let θij denote this
cosine similarity. A higher cosine similarity means that heroes occupy more similar roles.

Given that a well balanced team should cover many roles, we expect that heroes with similar role
vectors should be less likely to co-occur. Figure 4 shows that there is a strong correlation between the
θij and vi · vj but not so much between θij and ui · uj.

5.4 Substitutable Ingredients

We now investigate a different task. We
use a dataset of 180, 000+ recipes avail-
able on Kaggle4. We construct the log
co-occurrence matrix of the 1000 most
common ingredients in these recipes. We
compute the AR decomposition of this
matrix using the same methodology as
the experiment above. We use the rank
chosen by BCV (k = 125).

We then look at some commonly substi-
tuted cooking ingredients.5 We did not
use all examples on the site for two rea-
sons. First, some of them were not in
the top 1000 most commonly used in-
gredients. Second, some substitutions ex-
plicitly require a mixture of multiple (3
or more) other ingredients, which is not
achievable in the current version of our
model.

In Table 1 we take some focal ingredients
and show their 3 nearest R neighbors
using the cosine similarity. We see that
using R-similarity as a substitutability
metric seems to yield qualitatively good
results in this dataset.

We see that some ingredients have good
substitutes while others do not. For ex-
ample, nearest neighbors of many items
have R similaries between .7 and .9 and
seem sensible (canola oil for vegetable
oil or greek yogurt for yogurt or bak-
ing cocoa for unsweetened chocolate).
However, beer’s closest R neighbors are
the less sensible apple juice and corn
oil with much lower similarity scores
(around .3 − .4).

target
baking mix
baking mix
baking mix
baking powder
baking powder
baking powder
beer
beer
beer
brown sugar
brown sugar
brown sugar
buttermilk
buttermilk
buttermilk
chicken broth
chicken broth
chicken broth
lemon
lemon
lemon
onion
onion
onion
orange juice
orange juice
orange juice
parmesan cheese
parmesan cheese
parmesan cheese
parsley
parsley
parsley
pecan
pecan
pecan
red wine
red wine
red wine
unsalted butter
unsalted butter
unsalted butter
unswtd chocolate
unswtd chocolate
unswtd chocolate
vegetable oil
vegetable oil
vegetable oil
vinegar
vinegar
vinegar
yogurt
yogurt
yogurt

substitute
bisquick
biscuit mix
bisquick mix
baking soda
whole wheat ﬂour
all-purpose ﬂour
apple juice
mango
corn oil
sugar
honey
light brown sugar
skim milk
soymilk
chickpea
chicken stock
vegetable broth
vegetable stock
fresh lemon juice
lemon, juice of
lemon juice
red onion
scallion
yellow onion
honey
orange
lemon
mozzarella
cheddar
olive oil
fresh parsley
ﬂat leaf parsley
dried parsley
walnut
nut
sliced almond
dry red wine
dry white wine
white wine
butter
margarine
heavy cream
unswtd choc square
baking cocoa
unswtd cocoa
oil
canola oil
olive oil
cider vinegar
white vinegar
apple cider vinegar
plain yogurt
greek yogurt
vanilla yogurt

R score
0.78
0.73
0.70
0.85
0.51
0.42
0.40
0.39
0.39
0.74
0.69
0.68
0.52
0.48
0.39
0.85
0.63
0.61
0.76
0.71
0.66
0.71
0.71
0.68
0.61
0.50
0.47
0.64
0.62
0.53
0.93
0.65
0.59
0.85
0.75
0.65
0.79
0.68
0.61
0.74
0.67
0.48
0.83
0.74
0.71
0.96
0.88
0.67
0.89
0.87
0.82
0.73
0.70
0.53

Table 1: Substitutes for various focal ingredients found by
looking at cosine similarity neighbors in the R component.

In addition, we note that the notion of
substitution discovered here does not
always mean a substitution which pre-
serves the same character of the dish.
For example, pancakes can be made with
milk or with buttermilk, and indeed we see that milk is a substitute for buttermilk. However, milk

4The

dataset

is

available

at

https://www.kaggle.com/shuyangli94/

food-com-recipes-and-user-interactions, it originally appeared in Majumder et al. (2019)

5The list from which we draw is available at https://www.allrecipes.com/article/

common-ingredient-substitutions/.

8

pancakes will taste very different from buttermilk pancakes as buttermilk is acidic while regular milk
is not.

Nevertheless, we see that R similarity, which is high when 2Sim is high but 1Sim is low, seems to be
a good way to detect substitutable nodes in co-occurrence networks.

6 Conclusion

We have shown that the workhorse dot product (or A-only) model of networks has trouble modeling
a lack of triadic closure or heterophily in networks. We have shown that the AR decomposition is an
extension which allows us to model these issues in a parsimonious way.

We have shown that real world ego networks have substantial heterophilic components. In addition,
we have shown that nodes that are not linked to each other yet are linked to similar nodes can
be interpreted as having a special relationship (e.g. they are ‘substitutes’ when the network is a
co-occurrence graph).

The implied generative model behind the AR decomposition when applied to social networks is,
as with standard latent position random graph models, one where edge formation is independent
conditional on the latent positions of the nodes. This is a reasonable assumption for some cases but
not others. Thus, expanding beyond this assumption is an interesting avenue for future work.

In addition, scaling the AR decomposition is a problem in itself. Our proposed construction of
the AR decomposition requires the solution to a nuclear norm optimization. Though nuclear norm
minimization is a convex problem, it scales poorly when graphs grow to thousands or millions of
nodes (Recht et al., 2010). For this reason, studying whether AR decompositions can be easily
computed via stochastic gradient descent or other scalable optimization methods is an important next
step.

9

References

Emmanuel Abbe. 2017. Community detection and stochastic block models: recent developments.

The Journal of Machine Learning Research 18, 1 (2017), 6446–6531.

Edoardo Maria Airoldi, David M Blei, Stephen E Fienberg, and Eric P Xing. 2008. Mixed membership

stochastic blockmodels. Journal of machine learning research (2008).

Edoardo M Airoldi, David M Blei, Stephen E Fienberg, Eric P Xing, and Tommi Jaakkola. 2006.
Mixed membership stochastic block models for relational data with application to protein-protein
interactions. In Proceedings of the international biometrics society annual meeting, Vol. 15.

Avanti Athreya, Donniell E Fishkind, Minh Tang, Carey E Priebe, Youngser Park, Joshua T Vogelstein,
Keith Levin, Vince Lyzinski, and Yichen Qin. 2017. Statistical inference on random dot product
graphs: a survey. The Journal of Machine Learning Research 18, 1 (2017), 8393–8484.

Albert-Laszlo Barabasi and Zoltan N Oltvai. 2004. Network biology: understanding the cell’s

functional organization. Nature reviews genetics 5, 2 (2004), 101–113.

Jian-Feng Cai, Emmanuel J Cand`es, and Zuowei Shen. 2010. A singular value thresholding algorithm

for matrix completion. SIAM Journal on optimization 20, 4 (2010), 1956–1982.

Sudhanshu Chanpuriya, Cameron Musco, Konstantinos Sotiropoulos, and Charalampos Tsourakakis.
2020. Node Embeddings and Exact Low-Rank Representations of Complex Networks. Advances
in Neural Information Processing Systems 33 (2020).

David Easley, Jon Kleinberg, et al. 2010. Networks, crowds, and markets. Vol. 8. Cambridge

university press Cambridge.

Mark Granovetter. 1985. Economic action and social structure: The problem of embeddedness.

American journal of sociology 91, 3 (1985), 481–510.

Mark S Granovetter. 1973. The strength of weak ties. American journal of sociology 78, 6 (1973),

1360–1380.

Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for networks. In
Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and
data mining. 855–864.

Paul W Holland, Kathryn Blackmond Laskey, and Samuel Leinhardt. 1983. Stochastic blockmodels:

First steps. Social networks 5, 2 (1983), 109–137.

Matthew O Jackson. 2010. Social and economic networks. Princeton university press.

Hawoong Jeong, Sean P Mason, A-L Barab´asi, and Zoltan N Oltvai. 2001. Lethality and centrality in

protein networks. Nature 411, 6833 (2001), 41–42.

Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization techniques for recom-

mender systems. Computer 42, 8 (2009), 30–37.

Julian Laub and Klaus-Robert M¨uller. 2004. Feature discovery in non-metric pairwise data. The

Journal of Machine Learning Research 5 (2004), 801–818.

Adam Lerer, Ledell Wu, Jiajun Shen, Timothee Lacroix, Luca Wehrstedt, Abhijit Bose, and Alex
Peysakhovich. 2019. Pytorch-biggraph: A large-scale graph embedding system. arXiv preprint
arXiv:1903.12287 (2019).

Jure Leskovec, Jon Kleinberg, and Christos Faloutsos. 2007. Graph evolution: Densiﬁcation and
shrinking diameters. ACM transactions on Knowledge Discovery from Data (TKDD) 1, 1 (2007),
2–es.

Omer Levy and Yoav Goldberg. 2014. Neural word embedding as implicit matrix factorization.

Advances in neural information processing systems 27 (2014), 2177–2185.

Francois Lorrain and Harrison C White. 1971. Structural equivalence of individuals in social networks.

The Journal of mathematical sociology 1, 1 (1971), 49–80.

10

L´aszl´o Lov´asz and Katalin Vesztergombi. 1999. Geometric representations of graphs. Paul Erdos

and his Mathematics 2 (1999).

Bodhisattwa Prasad Majumder, Shuyang Li, Jianmo Ni, and Julian McAuley. 2019. Generating

personalized recipes from historical user preferences. EMNLP (2019).

Rahul Mazumder, Trevor Hastie, and Robert Tibshirani. 2010. Spectral regularization algorithms
for learning large incomplete matrices. The Journal of Machine Learning Research 11 (2010),
2287–2322.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed
representations of words and phrases and their compositionality. arXiv preprint arXiv:1310.4546
(2013).

Andrew Y Ng, Michael I Jordan, Yair Weiss, et al. 2002. On spectral clustering: Analysis and an

algorithm. Advances in neural information processing systems 2 (2002), 849–856.

Art B Owen, Patrick O Perry, et al. 2009. Bi-cross-validation of the SVD and the nonnegative matrix

factorization. The annals of applied statistics 3, 2 (2009), 564–594.

Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning of social
representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge
discovery and data mining. 701–710.

Benjamin Recht, Maryam Fazel, and Pablo A Parrilo. 2010. Guaranteed minimum-rank solutions of
linear matrix equations via nuclear norm minimization. SIAM review 52, 3 (2010), 471–501.

Benedek Rozemberczki, Oliver Kiss, and Rik Sarkar. 2020. Karate Club: An API Oriented Open-
source Python Framework for Unsupervised Learning on Graphs. In Proceedings of the 29th
ACM International Conference on Information and Knowledge Management (CIKM ’20). ACM,
3125–3132.

Francisco JR Ruiz, Susan Athey, David M Blei, et al. 2020. Shopper: A probabilistic model of
consumer choice with substitutes and complements. Annals of Applied Statistics 14, 1 (2020),
1–27.

C Seshadhri, Aneesh Sharma, Andrew Stolman, and Ashish Goel. 2020. The impossibility of low-
rank representations for triangle-rich complex networks. Proceedings of the National Academy of
Sciences 117, 11 (2020), 5631–5637.

Daniel L Sussman, Minh Tang, Donniell E Fishkind, and Carey E Priebe. 2012. A consistent
adjacency spectral embedding for stochastic blockmodel graphs. J. Amer. Statist. Assoc. 107, 499
(2012), 1119–1128.

Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. Line: Large-
scale information network embedding. In Proceedings of the 24th international conference on
world wide web. 1067–1077.

Laurens Van der Maaten and Geoffrey Hinton. 2012. Visualizing non-metric similarities in multiple

maps. Machine learning 87, 1 (2012), 33–55.

Hao Yin, Austin R Benson, Jure Leskovec, and David F Gleich. 2017. Local higher-order graph
clustering. In Proceedings of the 23rd ACM SIGKDD international conference on knowledge
discovery and data mining. 555–564.

Kisung You. 2020. ﬁlling: Matrix Completion, Imputation, and Inpainting Methods. https:

//CRAN.R-project.org/package=filling R package version 0.2.2.

Stephen J Young and Edward R Scheinerman. 2007. Random dot product graph models for social
networks. In International Workshop on Algorithms and Models for the Web-Graph. Springer,
138–149.

11

7 Appendix to “An Attract-Repel Decomposition of Undirected Networks”

7.1 Stochastic Block Models

We now look at the relationship of the AR decomposition to a popular model in the literature: the
stochastic block model (SBM) (Holland et al., 1983). These models are used in network analysis in
ﬁelds as different as looking at human social networks to modeling protein interactions (Airoldi et al.,
2006; Abbe, 2017). The SBM works as follows: there are d blocks. Each node belongs to one block.6
For blocks i, j there is a probability bij that a node from i is connected to a node from j. Let B be
the matrix of node connection probabilities. We assume B has rank d. Let PB be the implied matrix
of probability of connections in a population of n nodes. Again, ignore the diagonal.

It is known that if B is positive semi-deﬁnite and rank k then there exists an A decomposition of rank
k which represents the SBM.

We can generalize this result to the AR decomposition as well as generalize our example in Figure 1:
Proposition 6. If B has rank d then PB has the following properties:

1. PB has AR decomposition of rank d

2. If B is positive semi-deﬁnite then an A decomposition of PB can have rank d

3. If B has negative eigenvalues then any A decomposition of PB has rank > d.

Proof of Theorem 6. We ﬁrst prove that PB has an AR decomposition of rank k. To do this, we will
explicitly construct the decomposition. Since B is symmetric and real it has an AR decomposition of
B = XX (cid:48) − Y Y (cid:48). We give each node the embedding corresponding to the embedding of it’s block.
This is an AR decomposition of PB.

From this the second point follows directly.

The third point follows by contradiction. Suppose that PB has an A decomposition of rank d (or
less). Let this be X. By construction it must be that all nodes with the same block k have the same
embedding, call it xk. But then B = XX (cid:48). But this implies that B is positive semi-deﬁnite, which
we know it is not.

Lemma 1. The following SBMs will not be positive semi-deﬁnite:

1. Any SBM with heterophily across blocks: There exist some blocks i, j where nodes in i are
more likely to connect to j than to i and nodes in j are more likely to connect to i than j.

2. Any SBM with a triangle: There are 3 blocks i, j, k with the properties that

bii(bjjbkk − b2

jk) − bij(bijbkk − bjkbki) + bik(bijbjk − bjjbik) < 0.

Proof of Lemma 1. By Sylvester’s Criterion for a full rank matrix to be positive semi-deﬁnite, any
principal minor (submatrix of the same indexed rows and columns) must have non-negative determi-
nant. A 2 × 2 submatrix of i, j has the determinant

biibjj − b2
ij
where the square is there due to symmetry of bij = bji. If bij > bii, bjj this determinant is negative
meaning that B cannot be positive semi-deﬁnite and thus must have negative eigenvalues.

The proof for the second statement in the Lemma is the same, since the equation in the Lemma is in
the fact that determinant of a 3x3 principal minor.

The three-wise case is particularly interesting as we will see later that unclosed triangles play a special
role in co-occurrence networks (note the condition in the Lemma is satisﬁed with a basic triangle
where i, j connect to k but not to each other bii = bjj = bjk = 1 and bij = 1, bjk = 1 but bij = 0).

It is possible to construct higher order interaction conditions that guarantee that B is not easily A
decomposable but these conditions become harder to interpret.

6The extension to a mixed membership model (Airoldi et al., 2008) is straightforward and would not change

our main result.

12

7.2 Proofs of Main Text Results

Proof of Proposition 1. Any symmetric matrix M has a decomposition of the form A(cid:48)A − R(cid:48)R. To
construct this:

1. Compute the eigendecomposition of ˆP = Q(cid:48)DQ. Any symmetric matrix has such a decom-

position with real eigenvalues.

2. Let D− be the negative eigenvalues and D+ be the positive ones. Let Q− correspond
to the eigenvectors with negative eigenvalues and Q+ be the eigenvectors with positive
eigenvalues.

3. Let A = Q+

√

D+ and let R = Q−

√

−D−

Since given any diagonal matrix D the matrix M + D is still symmetric, the decomposition continues
to exist. The family of AR decompositions is thus equivalent to any choice of diagonal for M .

Proof of Proposition 2. By Lemma 6 of Mazumder et al. (2010) we know that for a matrix X we can
write

||X||∗ = min
U V =X

1
2

(||U ||2

F + ||V ||2

F ||).

With the minimum being attained at the factor decomposition X = U V .
By construction of our matrix ˜M it has the factor decomposition U = [A, R] and V = [A, −R]
where [·] denotes column-wise concatenation.

Substituting the deﬁnition of the factors gets
1
2

|| ˜M ||∗ =

(2

(cid:88)

a2
ij + 2

(cid:88)

r2
ij)

ij

ij

which simpliﬁes to

|| ˜M ||∗ = ||A||2

F + ||R||2
F .

Recall the construction of ˜M is by ﬁnding the smallest A, R in terms of ||A||2
F to ﬁt all
the off diagonal elements. Thus, these A, R also solve the constrained nuclear norm minimization
problem.

F + ||R||2

Proof of Proposition 3. We know that any symmetric matrix C that is positive semi-deﬁnite has a
decomposition of the form C = A(cid:48)A. Thus it is sufﬁcient to show there exists a choice of diagonal
D such that M − D is positive semi-deﬁnite. Let ∆ be the eigenvalues of M . If all ∆ are weakly
positive we are done since then M is positive semi-deﬁnite. Let δ be the largest negative eigenvalue
otherwise. Construct the diagonal Dδ as the constant −δ.

Given a matrix M the eigenvalues of M + D are simply the eigenvalues of M shifted up by D.
Therefore, by construction M − Dδ has non-negative eigenvalues. But then it is positive semi-deﬁnite.
Let Aδ be the decomposition of M − Dδ. Then M = Dδ + A(cid:48)

δAδ as required.

Proof of Proposition 4. In essence, we want to determine the rank, that is, determine the dimension
of the nullspace of a matrix

M =

(cid:19)

(cid:18) A R
R B

where A is an n × n diagonal matrix with coefﬁcients a1 . . . an > 0, B is an n × n diagonal matrix
with coefﬁcients b2 . . . bn > 0, and R is an n × n matrix of all 1. Let’s solve!

M ×

(cid:19)

(cid:18) u
v

= 0 ⇐⇒ ∀i

13

(cid:26) aiui + (cid:80)
bivi + (cid:80)

j vj = 0
j uj = 0

Since ai > 0 and bi > 0, this implies

∀i ui = −

1
ai

(cid:88)

j

vj

vi = −

1
bi

(cid:88)

uj

j

(3)

If we were free to set (cid:80)
j uj as we please, the two equations (3) would describe a 2-
dimensional space. Therefore the nullspace of M has dimension at most 2. However we can also use
the ﬁrst of these equations to write

j vj and (cid:80)

(cid:88)

ui = −

(cid:88)

(cid:88)

vj

(4)

1
ai

i
Therefore our nullspace has dimension at most 1. But we can continue and use the second equations
from (3) to replace vj above:

j

i

(cid:32)

(cid:88)

i

(cid:33) 


(cid:88)

j

1
ai



v]

 =

(cid:32)

(cid:88)

i

(cid:33) 


(cid:88)

j

1
ai





1
bj

(cid:88)

uk

k

(cid:88)

ui = −

i
(cid:16)(cid:80)
i

(cid:17) (cid:16)(cid:80)
j

(cid:17)

1
ai

Therefore, if r =
ui = vj = 0: the matrix is nonsingular. On the other hand, if r = 1, then I can choose (cid:80)
to any non zero value, deduce (cid:80)
described a one-dimensional nullspace.

j vj = 0 which means that
j vj equal
j ui using (4), compute ui and vi using (3), and verify that we have

(cid:54)= 1, then we must have (cid:80)

i ui = (cid:80)

1
bj

1
In conclusion: if r =
ai
2n − 1. This is the case, for instance, when ai = bj = n.

1
bj

(cid:16)(cid:80)
i

(cid:17) (cid:16)(cid:80)
j

(cid:17)

(cid:54)= 1, the matrix has full rank. If r = 1 the matrix has rank

Proof of Proposition 5. Let D be the data matrix for all legal combinations of the sandwich problem.
Consider the matrix C = D(cid:48)D which is the 2M × 2M co-occurrence matrix of all items that are
purchased. We know that rank(C) ≤ rank(D).

It is easy to check that if i be a meat and j be a bread (or vice versa) then cij = 1. If i, j are
the same item type, then cij = 0. For any cii = M . This is precisely the matrix described in the
proof of Proposition 4. This matrix has rank 2M − 1. Therefore the matrix D also has rank at least
2M − 1.

14

