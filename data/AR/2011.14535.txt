ArXiv
(PRE-PRINT)

Beyond LunAR: An augmented reality UI for deep-space
exploration missions

Sarah Radway, Anthony Luo, Carmine Elvezio, Jenny Cha, Sophia Kolak, Elijah Zulu, Sad Adib

0
2
0
2

v
o
N
0
3

]

C
H
.
s
c
[

1
v
5
3
5
4
1
.
1
1
0
2
:
v
i
X
r
a

Abstract As space exploration eﬀorts shift to deep
space missions, new challenges emerge regarding astro-
naut communication and task completion. While the
round trip propagation delay for lunar communications
is 2.6 seconds, the time delay increases to nearly 22 min-
utes for Mars missions [1]. This creates a need for as-
tronaut independence from earth-based assistance, and
places greater signiﬁcance upon the limited communi-
cations that are able to be delivered. To address this is-
sue, we prototyped an augmented reality user interface
for the new xEMU spacesuit, intended for use on plan-
etary surface missions. This user interface assists with
functions that would usually be completed by ﬂight con-
trollers in Mission Control, or are currently completed
in manners that are unnecessarily diﬃcult. We accom-
plish this through features such as AR model-based task
instruction, sampling task assistance, note taking, and
telemetry monitoring and display.

Keywords augmented reality · user interface design ·
deep-space communications · spacesuit interface ·
head-mounted display · space exploration

1 Introduction

Human space exploration has always been heavily sup-
ported by the people in Mission Control, who commu-
nicate information to astronauts through voice commu-
nication channels, known as voice loops [2].

Astronauts rely on these communications for mis-
sion critical information, such as task instructions, teleme-
try status, or commands in unexpected circumstances.

Sarah Radway, Anthony Luo, Carmine Elvezio, Jenny Cha,
Sophia Kolak, Elijah Zulu, Sad Adib
Department of Computer Science
Columbia University
New York, NY 10027, USA
E-mail: sar2236@columbia.edu

As space exploration moves into deep-space, however,
communication delays from Earth can be up to 22 min-
utes on Mars [1], making the current means of informa-
tion transfer unsuitable.

As NASA prepares to send astronauts into more re-
mote parts of space for longer periods of time [3], there
has been signiﬁcant research into the eﬀects these de-
lays will have on communication between astronauts
and Mission Control. Love and Reagan [1] ﬁnd that
when delays are introduced, audio communication is
hindered by disordered sequences, interrupted calls, im-
paired ability to provide relevant information, slow re-
sponse to events, and reduced situational awareness.

In response to the issues caused by delayed audio
transmissions, NASA and commercial partners are de-
veloping spacesuits with augmented reality (AR) en-
abled glass, that allow graphics to be displayed in the
astronaut’s visual ﬁeld [4]. By utilizing this ability to
display graphics in the ﬁeld of view of the astronauts,
these spacesuits can respond to many of the aforemen-
tioned problems introduced by deep-space communica-
tion. NASA’s Exploration Extravehicular Activity Mo-
bility Unit (xEMU) spacesuits will likely include AR
enabled glass for the Artemis 2024 Mission and future
missions to Mars, aiding astronauts with diﬃcult tasks
such as navigation, science sampling, and critical repair
tasks in ways previously unimaginable [4]. For instance,
instead of being told how to repair a rover, an astronaut
could be shown how to preform the repair through 3D
graphics overlaid on the vehicle itself similar to Hen-
derson and Feiner [5].

User interfaces (UIs) must mediate the interaction
between the astronaut and this information, making de-
cisions about when and how to show text and/or graph-
ics. In the context of general AR UIs, Olsson et al.
[6] demonstrated that certain approaches do not en-
hance the user’s experience. In safety critical situations
such as Extra-Vehicular Activities (EVAs), it is of the

 
 
 
 
 
 
2

Sarah Radway et al.

utmost importance that astronauts have a thorough
understanding of instructions and their surroundings.
Thus, while AR has the capacity to enrich communi-
cation in space, its eﬀectiveness is largely determined
by the quality of the UI, which integrates this sensory
information into the work environment.

Because the xEMU spacesuit and its new graphic ca-
pabilities have not yet been utilized in a real mission, it
is still unclear how graphical elements should be added
into the communication loop between Mission Control
and astronauts during EVAs. Furthermore, UIs have
not yet been developed to cope with the delays between
Earth and Mars, which is an important consideration
as we enter the era of crewed deep-space exploration.
In this paper, we present an AR system designed to
address the speciﬁc challenges faced by astronauts on
deep-space exploration missions. In particular, we aim
to alleviate the burden of communication delays, pro-
vide a helpful on-demand source of assistance, and fa-
cilitate task completion with greater eﬃcacy through
hybrid graphics and text instructions.

Compared to prior work, such as that of Feiner et
al. [7], our system focuses less on deep-space task man-
agement design and implementation, but instead intro-
duces an AR system for EVA assistance for astronauts.
Furthermore, our system incorporates elements of AR
such as 3D overlaid instructions, like those of D’Souza
[8], which brings about signiﬁcant user beneﬁts. In con-
trast to these works, we introduce a voice-based con-
trol system for ease of use (given the restrictive nature
of spacesuits) and automatic telemetry monitoring and
alerting. Our system is speciﬁcally designed to increase
astronaut independence from MCC, which is especially
important given the time-delay of deep-space missions.
In this paper, we focus on how multimodal systems
can be utilized to mitigate the eﬀects of delayed commu-
nication in deep space. Our design focuses on minimal
physical interaction, intuitiveness, and conciseness, so
as not to unnecessarily distract the user. Therefore, we
chose a voice-navigation technique to avoid adding to
the intense physical strain astronauts face, and imple-
mented an instruction method involving they overlay of
AR models on top of the physical environment. Our in-
struction method aims to eliminate the cognitive strain
and confusion of text-only instructions, and instead of-
fer a more intuitive and user friendly combination of
text instructions with overlaid AR visualizations. Mind-
ful of the limitations in the way of deep-space commu-
nications bandwidth, we established a data ﬂow that
reduces the need for task instruction data transmission
by at least two orders of magnitude in comparison with
a standard approach. Through this project, we hope to
contribute to the consideration of the unique needs of

astronauts completing lunar and deep-space missions,
and propose a solution to a number of the challenges
they face.

2 Background and Related Work

We begin by describing the necessary background for
understanding and evaluating our approach.

2.1 Augmented Reality (AR)

Augmented Reality (AR) is an emerging technology which
allows computer systems to add virtual content to the
visual ﬁeld of the user [9]. Contrary to virtual reality
(VR), which immerses the user in a diﬀerent environ-
ment, AR systems create a layer on top of one’s pre-
existing visual frame. Popular games such as Pok´emon
Go [10] and Ingress [11] are examples of this technology.

2.2 Deep-Space Exploration

Deep-space exploration, as the name suggests, refers to
the exploration of remote regions of outer space. Specif-
ically, these regions must lie outside of lower-earth or-
bit, which includes missions orbiting around the earth
such as the International Space Station, and outside of
cis-lunar space, which includes lunar orbiting missions
such as the planned Lunar Gateway project [12]. Ex-
amples of deep-space exploration include missions to
Mars and Venus. As of now, all deep-space exploration
has consisted of crewed missions; however, NASA has
shifted its future mission objectives towards manned
deep-space exploration, speciﬁcally of Mars.

Signiﬁcant work has been completed investigating
the applications of AR to astronaut task assistance.
From astronaut training [13] to astronaut work support
[14], it has been demonstrated that the use of AR im-
proves user experience and productivity. Example im-
plementations include providing system/environment warn-
ings, procedural task assistance incorporating 2D and
3D elements [14], and informational resources [15]. AR
is being used on board space missions to assist astro-
nauts, and solve issues that prevent task completion
[16].

Additionally, the issue of delayed communications
accompanying deep-space exploration has been shown
to have a signiﬁcant impact on astronaut performance;
NASA astronaut study participants have directly stated
that communication delays will represent a massive chal-
lenge for long duration missions [17]. Researchers have

AR UI for Deep Space Missions

3

attempted to mitigate the negative eﬀects of this de-
lay, utilizing techniques ranging from analog testing
to behavioral implementations. Further, Love and Rea-
gan’s investigation showed that astronauts preferred to
receive non-critical information from Mission Control
via text and data communication methods, rather than
voice communication methods, when in situations in-
volving signiﬁcant communication latency [1]. It is im-
portant to consider how this general preference can be
applied to astronauts’ daily tasks in deep-space mis-
sions. We were unable to ﬁnd any academic research
examining the application of this preference to astro-
naut on-board assistance through an AR UI.

3 Approach

3.1 Voice Recognition for User Interface Navigation

The average astronaut suit has a mass of 127 kg and
signiﬁcantly restricts movement, which makes gestures
as simple as scrolling with one’s ﬁnger highly strenu-
ous [3]. Thus, when it came time to determine how to
navigate through our UI, this special condition ruled
out many of the more typical methods: namely navi-
gation techniques requiring physical movement. Astro-
nauts typically communicate with the Mission Control
Center (MCC) over voice loops. By building upon this
preexisting voice loop framework in the MCC, astro-
nauts would no longer need to halt task completion to
provide a status update—ﬂight controllers in Mission
Control would be able to access voice data as astro-
nauts work their way through assignments.

As crewed missions go further into deep space, the
communication delays between MCC and on-board crew
will become more and more signiﬁcant, and inevitably
increase the level of distraction during the task caused
by ﬂight controller interruption [18]; this ability for a
ﬂight controller to passively observe astronaut status
could greatly beneﬁt astronaut productivity. The use of
voice control as a mode of UI navigation in aerospace-
related tasks has been thoroughly established and sup-
ported in the work of Helin et al. [14] and Markov-
Vetter and Staadt [15]. Voice control, built upon the
preexisting voice loop infrastructure, seems to be the
least demanding and most comfortable UI navigation
technique that possesses an acceptable accuracy level.
We use Microsoft Azure Cognitive Speech Services
speech-to-text functionality [19], in order to implement
the process control diagram displayed in Figure 1. We
establish a key phrase for each of the UI’s basic tasks,
which the system identiﬁes when said by the user. The
UI also displays the most likely relevant key phrase(s)

during tasks, to assist the user. For example, the UI in-
structs the user to say the key word “next” to scroll to
the next instruction. The system then recognizes that
tag word, and advances the UI to the next step’s de-
scription. This system allows a user to eﬀectively open,
close, scroll forwards and scroll backwards through UI
elements.

Additionally, we used voice recognition to assist in
tasks involving astronaut note-taking. This feature, when
activated, records the user’s speech to text content on
the onscreen notepad display, and stores a ﬁle version
to send back to the MCC. This proves especially help-
ful for sampling tasks: while harvesting moon rocks on
the lunar surface, typically an astronaut would need to
record information about the rock samples with a pa-
per and pencil [20] or the modern equivalent of a tablet
and stylus. The traditional writing approach, aside from
being ineﬃcient, would prove uncomfortable for astro-
nauts in a full spacesuit, with limited hand dexterity.
This speech functionality allows the user to carry out
the sampling process without this same need for physi-
cal movement; in our implementation, when a rock sam-
ple is collected, a virtual notepad was displayed, sequen-
tially asking the user a series of questions, and record-
ing their answers on screen. The UI displays the ques-
tion “What color is the sample?”, and the user simply
needs to speak to record the color, saying, for instance,
“gray”. When the user has answered the question, and
can clearly see that their answer is recorded correctly,
they say “next”, and can continue working their way
through the sample collection, entirely eliminating the
need for additional physical burdens.

While we understand that using a cloud computing
service would be infeasible for deep space (for the same
reasons as MCC communication), we believe that a ma-
jority of our implementation can be replicated without
the need for a service like Microsoft Azure. The vocab-
ulary integral to this UI is very sparse, with fewer than
25 necessary key terms for navigation purposes. Beyond
that, for sampling tasks, the required vocabulary is also
relatively limited. Given the relatively bounded expec-
tations for the voice control system, it should be pos-
sible to create a system capable of functioning with-
out an Earth-bound computing service, for example by
creating a voice control system based on HMMs (Hid-
den Markov Models), speciﬁcally limited in its applica-
tion and scope, as implemented in Markov-Vetter and
Staadt[15].

3.2 Sampling

The sampling task consists of two main goals: to display
and store sampling information and to obtain visual

4

Sarah Radway et al.

Fig. 1 Voice Control Basic Outline

information of the environment. Given the fact that
sampling tasks can be achieved with limited vocabu-
lary, our main focus is to simplify the procedure for
the user, as well as to store and display this data in
a way that minimized cognitive load and space usage.
To trigger the sampling voice commands, the user says
“begin sampling”. The procedure is broken down into
a sequence of atomic expressions (true/false) that are
triggered by key phrases and words (“collect sample”,
“stop”, “exit”). Once the “begin sampling” key phrase
has been recognized, the system creates an on-device
folder for the environment and each sample in order
to organize information for both the user and MCC.
The system then listens for voice input; the astronaut’s
observations about the sample along with the date and
time are recorded into a text ﬁle, which can be displayed
in the UI immediately and easily sent to MCC. To ob-
tain visual information, we utilize Unity’s [21] built in
PhotoCapture API, which is integrated with 3D envi-
ronments, and captures images every 0.5 seconds. We
consider both photo and video capture; however our im-
plementation uses just photo capture due to the smaller
ﬁle size and the fact that the environment does not
change enough to require constant surveillance.

3.3 Telemetry

When it comes to working in outer space, the immedi-
acy and clarity of alerts is a matter of life or death. For
this reason, we created a telemetry monitoring and re-
porting system, intended to deal with emergency situa-
tions during an EVA. Our system is constantly monitor-
ing and updating telemetry as it ﬂows in, as in the on-
board astronaut health analytic system demonstrated
in McGregor [22]. Upon receiving telemetry data, it
compares each value against a standard value range.
If the value is in a normal range, this information will
never be communicated to the user, with the exception

of the suit’s estimated oxygen, battery, and water time
remaining values, which are constantly displayed in the
top left corner of the UI. If the value is not in a normal
range, however, the system has a series of responses.

For abnormalities in critical telemetry, such as wa-
ter or oxygen level, the entire UI will ﬂash a mostly-
transparent shade of red so as to not obstruct the user’s
vision, while ensuring the astronaut understands the
urgent need to return immediately to base. The dis-
play warning will continue to appear in red at the top
left corner of the UI, making it almost impossible to
miss, but once again, not obstructing the astronauts
ﬁeld of view. For abnormalities in telemetry considered
important, but not immediately life threatening, such
as a slight increase in environmental pressure, the sys-
tem will still display the warning in red at the left cor-
ner, however the entire UI will not ﬂash a red hue as
the emphasis may serve to distract more than alert. In
this way, our system attempts to balance emphasis with
relevance, ensuring astronauts are always immediately
aware when a dangerous situation arises and remain
informed of the functionality of their equipment.

3.4 Object Recognition and Task Instructions

Because astronauts have a high cognitive load during
an EVA, we aimed to make task instructions as simple
and intuitive as possible. Therefore we choose to dis-
play text instructions in a helmet-ﬁxed manner, mean-
ing that it is in the ﬁeld of view of the user at all times,
and ﬁxed to the coordinate system of the helmet. We
accompanied text instructions with an AR overlay on
relevant machinery, in order to demonstrate features of
the task, similar to that in Markov-Vetter and Staadt
[15] and Feiner et al. [23].

To demonstrate the feasibility and accuracy of this
system, we implemented instructions for changing a tire
on the Multi-Mission Space Exploration Vehicle (MM-

AR UI for Deep Space Missions

5

SEV) rover. An example instruction from this task set
may say, “Turn the wrench counterclockwise to loosen
a bolt that holds the tire in place”; and accordingly,
a large red box would be overlaid around the speciﬁc
bolt, to clarify which bolt the user should loosen, and
an arrow would be projected circling around the bolt,
indicating to the user which direction they should turn
the wrench.

Only two pieces of information are required in or-
der to achieve this overlay-based instruction schema:
the 6DOF pose of the target object (in our case, the
rover) onto which the instructional models will be pro-
jected, and a set of predetermined coordinates of the
instructional models in reference to the target object.
To meet the ﬁrst requirement, our system uses the
Vuforia Engine [24] in Unity to identify an object and
provide its position and orientation. We began by es-
tablishing a Model Target, a concept in Vuforia rep-
resenting the base object for use in object and pose
recognition, relative to which the augmentations will be
placed. In the tire-changing example, the model target
would be the rover, as the system is intended to project
instructions in reference to the rover as the user goes
through the task. With Vuforia, a user ﬁrst aligns the
model target with a guide view to begin tracking; to
begin the tire-changing task, a user would stand away
from the rover, allowing the outline of the guide view
to align with the rover as shown in Figure 2.

Fig. 3 Overlayed animation of removal of MMSEV Rover
tire in red.

the mission, which could be rigorously tested as neces-
sary on-site. For each step of the task, our system takes
as input sets of Quaternion rotation values and posi-
tion coordinates, which are given in relation to a model
target located at the origin. It is important to mention
the need to use Quaternion rotation values, as opposed
to Euler angles, to avoid gimbal lock and ensure that
the movement of models will execute correctly. With
this information, the system can create moving over-
lays, or highlight speciﬁc areas of the rover at the ori-
gin, by providing one or a series of positions and rota-
tions through which the projected model should appear.
This ability is demonstrated in Figure 3, where the tire
is overlaid onto the rover, demonstrating the process
of its removal. Because we know the location of the
model target, in this case the rover, and the location of
a speciﬁc model relative to when this rover is at the ori-
gin, we can transform these coordinates. Through this
transformation, the instructional models can be prop-
erly aligned with the model target; a bolt holding a tire
in place on the target rover could be highlighted, or the
process of removing the bolt could be projected onto
the model.

Fig. 2 Guide View for Tracking of MMSEV Rover

This allows for the system to understand the loca-
tion and orientation of the rover so that instructional
models may be inserted in reference to this model tar-
get.

For the second requirement, the 6DOF pose is deter-
mined as a part of creating the task instructions. These
coordinates direct the animation of task sequences, through
providing instructions on how to align the projected
graphics with the model target. Their sequence is prede-
termined, so the astronaut trainers would create pack-
ages of reference coordinates for task sequences prior to

3.5 Model Referential Communications

Due to the limited bandwidth of deep-space transmis-
sion, a solution must be introduced to display and trans-
fer models/instructions. This is particularly challenging
given the average 3D model size is two or more or-
ders of magnitude larger than the data size that can
be transferred in a reasonable time frame (say 1–3 min-
utes) given the bandwidth speed available [25] and thus
transferring or downloading models in real time is in-
feasible.

As such, we take inspiration from multiplayer games
and introduce a model-referential communication sys-
tem wherein commonly used 3D models are stored lo-
cally on the deep-space device. To relay live AR instruc-

6

Sarah Radway et al.

tions with interactive 3D models, the relative model
position, rotation, and scale information is sent, as op-
posed to the whole model. This reduces the amount of
information needed to be sent to just 11.8 KB for our
entire instruction set, as opposed to over 54 MB for just
a single rover model. This also increases the ﬂexibility of
instructions that can be sent from Mission Control: As
long as no new 3D models are needed, new instructions
or demonstrations can easily be sent within a matter of
minutes.

We implemented our model referential communica-
tion task using MongoDB [26] with Unity. We chose
MongoDB over SQL [27] databases for our implementa-
tion, as it uses a document-based model suitable for in-
struction sets or 3D model collections. We created pre-
existing AssetBundles containing 3D models of commonly-
used tools/vehicles stored oﬄine on-device. Instruction
sets were created in BSON [28] form with each instruc-
tion sub-step containing a dynamic link to the required
3D models (stored on-device), as well as the required
position and orientations for those models. When the
instruction set (in BSON form) is received, the BSON-
string is ﬁrst serialized to a C# object in Unity. We
then ﬁnd the corresponding AssetBundle(s) for the C#
object, along with the position and orientation for the
given 3D models and project it onto the actual relative
position and orientation of the rover in AR. This is used
to provide step-by-step instructions projected onto the
rover along with 3D demonstrations.

For our proof of concept, we used a MongoDB At-
las cluster hosted on Amazon Web Services (AWS) [29]
to simulate a data downlink, as shown in Figure 4. We
created a Java program that takes instructions in CSV
format and automatically converts them into Instruc-
tionSet objects. The program also automatically dese-
rializes these objects into MongoDB BSON document
form and uploads them to our MongoDB Atlas instance.
This simulates Mission Control sending new instruction
sets to the astronauts via a data uplink. We were able
to demonstrate the usability and feasibility of this ap-
proach both from a technical standpoint and from a
limited bandwidth standpoint. As shown in Figure 5,
the peak average bandwidth usage on a single day was
only a mere 286 bytes/second, with other days clocking
in at only 20–80 bytes/second. This is well within the
bandwidth of the direct-to-Earth data rate in previous
deep-space spacecraft (for example, the Curiosity rover
had a direct-to-Earth data rate of 62.5-4000 bytes/sec).

4 Outcome and Lessons Learned

Voice Control Implementation and Performance: We
were pleasantly surprised by the performance of our

voice control implementation; updates were consistently
available in ﬁve seconds or less. We found that short,
simple commands seemed to work best, as they are eas-
iest to recognize, and are intuitive for the user (e.g.,
“Open Instructions” and “Next”). Based upon our ﬁnd-
ings, a good direction for further work would be to pur-
sue a method of interaction that is more natural, where
the user can ask questions of the interface and expect
responses, or move in nonlinear paths through tasks.

AR Implementation and Testing: Through the course
of this project, we discovered it was helpful to develop
AR aspects of this work in a manner compatible with
VR (Virtual Reality) for testing purposes; we found
that the unique challenges of the lunar surface were
easier to mimic in VR. Additionally, we developed for
a Microsoft HoloLens AR headset, but an interesting
direction for future research would be to use eye track-
ing in combination with voice for navigation on an AR
headset capable of this, such as the Microsoft HoloLens
2.

Model Referential Task Instructions: The model ref-
erential task instruction system we developed was very
successful from both a technical and ease of use stand-
point. A ﬂight controller or Mission Control on Earth
could easily write new instructions in CSV format us-
ing Microsoft Excel [30], which can be automatically
parsed, uploaded, and sent to the astronaut’s AR de-
vice using the instruction parsing program we created.
We found MongoDB BSON and MongoDB Atlas to be
well suited to this task, as we could upload/download
instructions in near real-time, and the MongoDB Unity
driver worked well with our system. Finally, our model
referential task instruction system oﬀered huge savings
from a data bandwidth perspective, as only the 6DOF
poses of 3D models and their corresponding instructions
were needed. Future work in this area would include a
more seamless and user friendly way to upload instruc-
tions and also provide an intelligent mechanism for au-
tomatically scheduling data-heavy instructions during
low-bandwidth usage periods.

Object Recognition: We were unable to evaluate the
accuracy of our object recognition implementation, due
to COVID-19 restricting travel to the location of the
rover. Therefore, in the ﬁnal stages of our work and test-
ing, we used Vuforia Simulator Play Mode [31], Vufo-
ria’s tracking simulator, in order to test with our model
targets. From our exploration, we would strongly rec-
ommend Vuforia as the best option for this type of task.
We examined several other object recognition solutions,
but Vuforia required the least work for the same perfor-
mance. In particular, Vuforia oﬀers an Object Scanner
which can create model targets for training.

AR UI for Deep Space Missions

7

Fig. 4 Model Referential System Design

Fig. 5 Data/Bandwidth usage of our MongoDB Atlas Instance

Sampling: Given the high accuracy of the voice recog-
nition system for an experimental system with no spe-
cialized training (we used Microsoft Cognitive Services
which has a Word Error Rate of < 10% [32]), we rec-
ommend voice recording as a method for taking ﬁeld
notes in the future. In terms of visual data, we consid-
ered collecting videos and photos. However, ﬁle size is a
concern with video capture, and video quality may not
be ideal given the lighting conditions. The photo ap-
proach did not pose such problems, and we concluded
that there would not be enough real-time changes in the
environment to require constant visual feedback. Thus,
we recommend the photo approach in order to mini-
mize space usage and preserve visual data quality. Of
course, if storage capacity of the ﬁnal device permits, a
hybrid solution of both photos and videos could be used
for increased data ﬁdelity. For future work, we believe
there is potential to enhance the information conveyed
by associating a sample with its location. Some sug-
gestions are to use a predeﬁned topographic map and
then “pin” the location of the sample using some sort of
visual marker. Another option is to recreate the topog-
raphy of a sample site using radar and LiDAR, and then
display the ﬁeld notes in the context of this generated
site.

5 Conclusions

In this paper, we have presented an AR system with
several design considerations for potential challenges
faced in deep-space missions. Our contributions to this
area are four-fold. First, we build upon prior art and
introduce 3D graphical overlay instructions in AR that
are designed to facilitate task completion and under-
standing by astronauts. We incorporate state-of-the-art
object recognition technologies to allow our system to
overlay graphics and animations with their associated
instructions onto the target object directly. We hope
that this combination of text and graphics increases
astronauts’ productivity and performance accuracy, es-
pecially given a foreign environment, novel instructions,
and potentially poor lighting conditions.

Second, we considered the physical limitations of
space suits, and designed our AR system with a voice
control system. This aims to reduce astronaut fatigue,
while providing a relatively reliable means of opera-
tion for non-critical tasks. By leveraging existing speech
recognition models (with a Word Error Rate of less than
10%), our system is able to reliably recognize instruc-
tions and commands. Furthermore, for tasks that re-
quire the astronaut to take notes of some form, we intro-
duce a voice-based note-taking and compiling system,
to reduce the need for a physical writing implement.

Model Referential System DesignMongoDB AtlasObject Creation and SerializationObject DeserializationMongoDB BSON DocumentCSV InstructionsMission ControlAstronaut AR DeviceUplinkDownlinkMongoDB BSON DocumentOffline 3D ModelsDynamic LinkingAfﬁne Transformations and Scale3D Model Dynamic LinksProject  to User (AR)8

Sarah Radway et al.

Third, we introduce an unobtrusive, automatic teleme-

try monitoring system that will alert the astronauts to
any anomalies and other serious issues. We hope that
this system will allow astronauts to focus on the task-
at-hand, without being worried about manually moni-
toring vitals or other telemetry streams. We believe this
is especially important in deep-space missions, where
the time delay between the astronaut and MCC will be
too large to safely rely on MCC for anomalous teleme-
try monitoring. Finally, we present a model referential
communications model to allow for an eﬃcient method
of transferring and linking large 3D data models (re-
quired for use in our graphical instructions) within the
speciﬁcation of deep-space data rate limitations.

Our future work will focus on improvements on mul-
tiple fronts. First, we hope to expand our voice control
system and introduce an intelligent assistant capable of
handling more complex user requests without the need
for cloud computing. Second, we intend to introduce
an unsupervised machine learning based anomaly de-
tection system, that will triage and cluster anomalies
observed in telemetry data and provide helpful sugges-
tions on solutions. Third, we aim to make our system
more contextually aware from both a user standpoint
and from an environmental standpoint (taking into ac-
count terrain, lighting, obstacles, and other data).

6 Acknowledgments

This work would not have been possible without the
eﬀort and advice of Professor Steve Feiner. We are ex-
tremely grateful for his support and enthusiasm. Addi-
tionally, we would like to extend our sincerest gratitude
to the Columbia University Fu Foundation School of
Engineering and Applied Science, for academic and ﬁ-
nancial support, the NASA SUITS Challenge Program
for providing us with a platform to create and share
our work, and The Columbia Space Initiative, for their
constant encouragement and tireless passion.

References

1. S. G.

Love

and M.

L. Reagan,

“De-
communication,” Acta Astronau-
layed voice
tica, vol. 91, pp. 89 – 95, 2013.
[Online].
Available: http://www.sciencedirect.com/science/
article/pii/S0094576513001537

2. E. Patterson, J. Watts-Englert, and D. Woods,
“Voice loops as coordination aids in space shuttle
mission control,” Computer supported cooperative
work : CSCW : an international journal, vol. 8,
pp. 353–71, 02 1999.

3. B. Dunbar, Moon to Mars Overview, 2020, https:
//www.nasa.gov/topics/moon-to-mars/overview.
4. P. Mitra, Human Systems Integration of an Ex-
travehicular Activity Space Suit Augmented Reality
Display System. Mississippi State University, 2018.
5. S. J. Henderson and S. K. Feiner, “Augmented real-
ity in the psychomotor phase of a procedural task,”
in Proc. 10th IEEE International Symposium on
Mixed and Augmented Reality, 2011, pp. 191–200.
6. T. Olsson, T. K¨arkk¨ainen, E. Lagerstam, and
L. Vent¨a-Olkkonen, “User evaluation of mobile aug-
mented reality scenarios,” JAISE, vol. 4, pp. 29–47,
01 2012.

7. M. Baltrusaitis and K. Feigh, “The development
of a user interface for mixed-initiative plan man-
agement for human spaceﬂight,” in 2019 IEEE
Aerospace Conference.

IEEE, 2019, pp. 1–9.

8. G. V. D’souza, “The eﬀectiveness of augmented
reality for astronauts on lunar missions: An ana-
log study,” Master’s thesis, Embry-Riddle Aero-
nautical University, 1 Aerospace Boulevard Day-
tona Beach, FL 32114-3900, 12 2019.

9. R. T. Azuma, “A survey of augmented reality,”
Presence: Teleoperators and Virtual Environments,
vol. 6, no. 4, pp. 355–385, 1997. [Online]. Available:
https://doi.org/10.1162/pres.1997.6.4.355
[Online]. Available:

10. “Pok´emon

https:

go!”
//www.pokemongo.com/en-us/

11. “Ingress.”

[Online].

Available:

https://

www.ingress.com/

12. K. Mars, Deep Space, 2018, https://www.nasa.gov/

johnson/exploration/deep-space.

13. A. M. Braly, B. Nuernberger, and S. Y. Kim,
“Augmented reality improves procedural work on
an international space station science instrument,”
Human Factors, vol. 61, no. 6, pp. 866–878,
2019, pMID: 30694084. [Online]. Available: https:
//doi.org/10.1177/0018720818824464

14. K. Helin, T. Kuula, C. Vizzi, J. Karjalainen,
and A. Vovk, “User experience of augmented
reality system for
astronaut’s manual work
support,” Frontiers in Robotics and AI, vol. 5,
2018. [Online]. Available: https://doi.org/10.3389/
frobt.2018.00106

15. D. Markov-Vetter and O. Staadt, “A pilot study for
augmented reality supported procedure guidance to
operate payload racks on-board the international
space station,” in 2013 IEEE International Sympo-
sium on Mixed and Augmented Reality (ISMAR).
IEEE, 2013, pp. 1–6.

AR UI for Deep Space Missions

9

“Nasa microsoft

collaborate,”
https:

16. S. Ramsey,
2015.

[Online].

Available:

Jun
//www.nasa.gov/press-release/nasa-microsoft-
collaborate-to-bring-science-ﬁction-to-science-fact
17. N. M. Kintz, C.-P. Chou, W. B. Vessey, L. B. Leve-
ton, and L. A. Palinkas, “Impact of communication
delays to and from the international space station
on self-reported individual and team behavior and
performance: A mixed-methods study,” Acta As-
tronautica, vol. 129, pp. 193 – 200, 2016. [Online].
Available: http://www.sciencedirect.com/science/
article/pii/S009457651630697X

18. N. J. Jim Taylor, Deep Space Communications,
https://descanso.jpl.nasa.gov/monograph/

2014,
series13/DeepCommoOverall--141030A ama.pdf.

19. “Cognitive

services.”

Avail-
able: https://azure.microsoft.com/en-us/services/
cognitive-services/

[Online].

20. C. Wong, The Saga of Writing

in Space,
2017, https://airandspace.si.edu/stories/editorial/
saga-writing-space.

21. U. Technologies.
unity.com/

[Online]. Available: https://

22. C. McGregor, “A platform for real-time online
health analytics during spaceﬂight,” in 2013 IEEE
Aerospace Conference, 2013, pp. 1–8.

23. S. Feiner, B. Macintyre, and D. Seligmann,
“Knowledge-based augmented reality,” Communi-
cations of the ACM, vol. 36, no. 7, pp. 53–62, 1993.
https://

Available:

24. “Vuforia.”

[Online].
developer.vuforia.com/
25. “Communications with

2019.
[Online]. Available: https://mars.nasa.gov/msl/
mission/communications/
[Online].

earth,” Aug

Available:

https://

26. “Mongodb.”

www.mongodb.com/

27. “Microsoft

sql.” [Online]. Available: https://

www.microsoft.com/en-us/sql-server

28. “Bson.” [Online]. Available: http://bsonspec.org/
[Online].
free
29. M. Hunt,
Available: https://aws.amazon.com/free/?all-free-
tier.sort-by=item.additionalFields.SortRank&all-
free-tier.sort-order=asc

“Aws

tier,”

1994.

30. Microsoft Corporation, “Microsoft excel.” [Online].
Available: https://oﬃce.microsoft.com/excel
31. “Vuforia play mode.” [Online]. Available: https:
//library.vuforia.com/content/vuforia-library/en/
articles/Solution/vuforia-play-mode-in-unity.html
custom speech accuracy.” [Online].
https://docs.microsoft.com/en-us/

32. “Evaluate
Available:
azure/cognitive-services/speech-service/how-to-
custom-speech-evaluate-data

