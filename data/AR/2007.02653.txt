Teacher-to-classroom assignment

and student achievement

Bryan S. Graham∗, Geert Ridder†, Petra Thiemann‡, Gema Zamarro§¶

September 2, 2020

Abstract

We study the eﬀects of counterfactual teacher-to-classroom assignments on average
student achievement in elementary and middle schools in the US. We use the Measures
of Eﬀective Teaching (MET) experiment to semiparametrically identify the average re-
allocation eﬀects (AREs) of such assignments. Our ﬁndings suggest that changes in
within-district teacher assignments could have appreciable eﬀects on student achieve-
ment. Unlike policies which require hiring additional teachers (e.g., class-size reduction
measures), or those aimed at changing the stock of teachers (e.g., VAM-guided teacher
tenure policies), alternative teacher-to-classroom assignments are resource neutral; they
raise student achievement through a more eﬃcient deployment of existing teachers.

JEL codes: I20, I21, I24

Keywords: teacher quality, teacher assignment, education production, student achievement,
average reallocation eﬀects, K–12

∗Department of Economics, University of California - Berkeley, and National Bureau of

Economic Research, e-mail: bgraham@econ.berkeley.edu

0
2
0
2

p
e
S
1

]

M
E
.
n
o
c
e
[

2
v
3
5
6
2
0
.
7
0
0
2
:
v
i
X
r
a

†Department of Economics, University of Southern California, e-mail: ridder@usc.edu
Lund
‡Department

Economics,

University,

IZA,

and

e-mail:

of

petra.thiemann@nek.lu.se

§Department of Education Reform, University of Arkansas, and CESR, e-mail:

gzamarro@uark.edu

¶We thank seminar audiences at the 2017 All-California Econometrics Conference at Stanford University,
UC Riverside, NESG, University of Duisburg-Essen, Tinbergen Institute Amsterdam, Lund University, IFN
Stockholm, the University of Bristol, and the University of Southern California for helpful feedback. We thank
Tommy Andersson, Kirabo Jackson, Magne Mogstad, Hessel Oosterbeek, Daniele Paserman, and Hashem
Pesaran for useful comments and discussions. All the usual disclaimers apply. Financial support for Graham
was provided by the National Science Foundation (SES #1357499, SES #1851647). Thiemann was aﬃliated
with USC Dornsife INET while working on this paper.

 
 
 
 
 
 
1

Introduction

Approximately four million teachers work in the public elementary and secondary education
system in the United States. These teachers provide instruction to almost ﬁfty million stu-
dents, enrolled in nearly one hundred thousand schools, across more than thirteen thousand
school districts (McFarland et al., 2019, Snyder et al., 2017). Diﬀerences in measured stu-
dent achievement are substantial across US schools as well as across classrooms within these
schools. Beginning with Hanushek (1971), a large economics of education literature attributes
cross-classroom variation in student achievement to corresponding variation in (largely) la-
tent teacher attributes. These latent attributes are sometimes referred to as teacher quality
or teacher value-added.

The implications of value-added measures (VAM) for education policy are controver-
sial both within the academy and outside it.1 The most contentious applications of VAM
involve their use in teacher tenure and termination decisions (cf., Chetty et al., 2012, Darling-
Hammond, 2015). The premise of such applications is that changes in the stock of existing
teachers – speciﬁcally rooting out teachers with low VAMs and retaining those with high
ones – could lead to large increases in student achievement and other life outcomes.

In this paper we pose an entirely diﬀerent question: is it possible to raise student achieve-
ment, without changes to the existing pool of teachers, by changing who teaches whom?
Schools and school districts are the loci of teacher employment. To keep our analysis policy-
relevant, we therefore focus on the achievement eﬀects of diﬀerent within-school and within-
district teacher-to-classroom assignment policies.

For teacher assignment policies to matter, teachers must vary in their eﬀectiveness in
teaching diﬀerent types of students. For example, some teachers may be especially good
at teaching English language learners, minority students, or accelerated learners (c.f., Dee,
2004, Loeb et al., 2014). Formally educational production must be non-separable in some
teacher and student attributes (Graham et al., 2007, 2014, 2020).

Using data collected in conjunction with the Measures of Eﬀective Teaching (MET)
project, we present experimental evidence of such non-separabilities. We expand upon stan-
dard models of educational production, which typically assume that the eﬀects of teacher and
student inputs are separable. Our identiﬁcation strategy relies on the random assignment of
teachers to classrooms in the MET project. Speciﬁcally, we study assignments based upon
(i) an observation-based, pre-experiment measure of teaching practice – Danielson’s (2011)
Framework for Teaching (FFT) instrument – and (ii) students’ and classroom peers’ baseline
test scores, also measured pre-experiment.

Alternative teacher assignments change the joint distribution of these variables. When

1See, for example, the March 2015 special issue of Educational Researcher on value-added research and
policy or the American Statistical Association’s statement on value-added measures (Morganstein and Wasser-
stein, 2014).

1

the educational production function is non-separable in these variables, these changes can
aﬀect average achievement. Focusing on assignments based on teacher FFT and student
baseline achievement, we ﬁnd that within-district changes in teacher-to-classroom assign-
ments could increase average classroom student achievement by as much as 0.04 standard
deviations. This eﬀect corresponds to the estimated diﬀerence in average achievement across
the teacher-to-classroom assignment which maximizes aggregate achievement versus the one
which minimizes it. Perhaps more realistically, comparing the status quo assignment in MET
schools – which was generated by random assignment of teachers to classrooms – with the
optimal one generates an estimated increase in average test scores of 0.02 standard deviations.
To benchmark these eﬀects consider a policy which removes the bottom τ ×100 percent of
teachers from classrooms – sorted according to their VAM – and replaces them with average
teachers (i.e., teachers with VAMs of zero). Assuming a Gaussian distribution for teacher
value-added, the eﬀect of such an intervention would be to increase the mean of the student
test score distribution by

(1 − τ ) σ

(cid:1)
φ (cid:0) qτ
1 − Φ (cid:0) qτ

σ

σ

(cid:1)

standard deviations.2 Here σ corresponds to the standard deviation of teacher value-added
and qτ to its τ th quantile.

Rockoﬀ (2004, Table 2) and Rothstein (2010, Table 6) estimate a standard deviation
of teacher value-added of between 0.10 and 0.15. Taking the larger estimate and setting
τ = 0.05 (0.10) generates an expected increase in student test scores of 0.015 (0.026) stan-
dard deviations; smaller than, albeit comparable to, our reallocation eﬀects. We note that
replacing ﬁve (ten) percent of teachers would be diﬃcult to do in practice. It would be even
more diﬃcult to correctly identify the bottom ﬁve (ten) percent of teachers (according to VA)
and replace them with average ones. We conclude that the achievement eﬀects of teacher
assignment policies are meaningful.

In contrast to policies which replace teachers, and/or require the hiring of new ones,
the within-school and within-district teacher reassignment policies we explore in this paper
are resource neutral. Districts require no new teachers or ﬁnancial resources to implement
them. Of course, reassigning teachers across classrooms, and especially across schools within
a district, may involve other types of costs. For example, many school districts operate under
collective bargaining agreements which give senior teachers partial control over their school
assignment (e.g., Cohen-Vogel et al., 2013).3

2As is common practice, we abstract from the eﬀects of possible behavioral responses to VAM-guided
teacher retention policies. For example, teachers might become demoralized by VAM retention systems and
teach less eﬀectively as a result; or they could be motivated to teach more eﬀectively.

3 Moreover, many teachers have preferences over school and student characteristics. Thus, one might
need to pay bonuses to move teachers to certain schools or classrooms (e.g., classrooms with high fractions
of disadvantaged students).

2

Our decision to use teacher FFT, students’ and peers’ baseline test scores as assignment
variables was driven by their availability in the MET dataset. Of course, observational
measures of teaching practices, such as the FFT, are routinely used for teacher evaluation,
promotion, and retention decisions. Its seems reasonable, therefore, to explore whether they
might also aid in determining a teacher’s classroom assignment.

It seems likely, however, that assignments based upon a diﬀerent pair of teacher and
classroom attributes would lead to greater achievement gains.4 Working with appropriately
chosen linear combinations of multiple student and teacher attributes could generate even
larger achievement gains. We leave explorations along these lines to future research.

Readers may reasonably raise questions about the external validity of the results reported
below. We believe such skepticism is warranted and encourage readers to view what is
reported below as provisional. We do, however, hope our results are suﬃciently compelling
to motivate additional research and experimentation with teacher-to-classroom assignment
policies.

Work by Susanna Loeb and co-authors suggests that US school districts have de facto
teacher-to-classroom assignment policies (Kalogrides et al., 2011, Grissom et al., 2015). For
example, they ﬁnd that less experienced, minority, and female teachers are more likely to be
assigned to predominantly minority classrooms. They also present evidence that principals
use teacher assignments as mechanisms for retaining teachers – as well as for encouraging
less eﬀective teachers to leave – and that more experienced teachers exert more inﬂuence on
classroom assignment decisions.

Our work helps researchers and policy-makers understand the achievement eﬀects of such
policies and the potential beneﬁts of alternative ones. The ﬁndings presented below suggest
that teacher-to-classroom assignment policies are consequential and that changes to them
could meaningfully increase average student achievement.

In addition to our substantive results, we present new identiﬁcation results for average
reallocation eﬀects (AREs). Identiﬁcation and estimation of AREs under (conditional) exo-
geneity is considered by Graham et al. (2014, 2020). Unfortunately these results do not apply
directly here. Although teachers were randomly assigned to classrooms as part of the MET
experiment, compliance was imperfect. Furthermore some students moved across classrooms
after the random assignment of teachers, which raises concerns about bias due to endogenous
student sorting. We develop a semiparametric instrumental variables estimator (e.g., Ai and
Chen, 2003) which corrects for student and teacher non-compliance. Our analysis highlights
how complex identiﬁcation can be in the context of multi-population matching models where
agents sort endogenously.

In independent work, Aucejo et al. (2019) also use the MET data to explore comple-
mentarity between student and teacher attributes in educational production. They do not

4That is, we did not consider the universe of possible assignment variables.

3

focus on adjusting for teacher and student non-compliance as we do, emphasizing instead
an intent-to-treat type analysis. Their paper includes an interesting analysis of the diﬀerent
sub-components of the FFT measure; something not done here. Despite these methodolog-
In earlier work, Dee
ical diﬀerences, their results are qualitatively concordant with ours.
(2004, 2005) uses Project STAR data to study complementarity between teacher and student
minority status.

2 Model and identiﬁcation

Our goal is to identify the average achievement eﬀects of alternative assignments of teachers to
MET classrooms. These are average reallocation eﬀects (AREs), as introduced by Graham
et al. (2007, 2014). The identiﬁcation challenge is to use the observed MET teacher-to-
classroom assignments and outcomes to recover these AREs.

Our analysis is based upon experimentally generated combinations of student and teacher
attributes, that is, it exploits the random assignment of teachers to classrooms in the MET
experiment.5 Like in many other ﬁeld experiments, various deviations from MET’s intended
protocol complicate our analysis. In this section we outline a semiparametric model of educa-
tional production and consider its identiﬁcation based upon the MET project as implemented,
using the MET data as collected.

It is useful, however, to ﬁrst explore nonparametric identiﬁcation of reallocation eﬀects
under an ideal implementation of the MET project (henceforth MET as designed ). Such an
approach clariﬁes how the extra restrictions introduced below allow for the identiﬁcation of
reallocation eﬀects despite non-compliance, attrition, and other deviations from the intended
experimental protocol.

Nonparametric identiﬁcation under ideal circumstances

Our setting features two populations, one of students and the other of teachers. Each student
is distinguished by an observed attribute Xi, in our case a measure of baseline academic
achievement, and an unobserved attribute, say “student ability,” Vi. Similarly, each teacher
is characterized by an observed attribute Wi, in our case an observation-based measure of
teaching pedagogy, and an unobserved attribute, say “teacher quality,” Ui.6

Let i = 1, . . . , N index students. Let C be the total number of MET classrooms or
equivalently teachers. We deﬁne Gi to be a C × 1 vector of classroom assignment indicators.
The cth element of Gi equals one if student i is in classroom c ∈ {1, . . . , C} and zero otherwise.

5To be precise, the randomization was carried out within schools. See Section 3 for details.
6We use “ability” as a shorthand for latent student attributes associated with higher test scores; likewise

we use “quality” as a shorthand for latent teacher attributes associated with higher test scores.

4

The indices of student i(cid:48)s peers or classmates are therefore given by the index set

p (i) = {j : Gi = Gj, i (cid:54)= j} .

Next we deﬁne the peer average attribute as ¯Xp(i) = 1
characteristic X across student i(cid:48)s peers). We deﬁne ¯Vp(i) similarly.

|p(i)|

(cid:80)

j∈p(i) Xj (i.e., the average of the

The MET project protocol did not impose any requirements on how students, in a given
school-by-grade cell, were divided into classrooms. Evidently schools followed their existing
procedures for dividing students within a grade into separate classrooms. An implication of
this observation is that the MET experiment implies no restrictions on the joint density

fXi,Vi, ¯Xp(i), ¯Vp(i)

(x, v, ¯x, ¯v),

(1)

beyond the requirement that the density be feasible.7 For example, if most schools tracked
students by prior test scores, then we would expect Xi and ¯Xp(i) to positively covary. If,
instead, students were randomly assigned to classrooms and hence peers, we would have,
ignoring ﬁnite population issues, the factorization

fXi,Vi, ¯Xp(i), ¯Vp(i)

(x, v, ¯x, ¯v) = fXi,Vi(x, v)f ¯Xp(i), ¯Vp(i)

(¯x, ¯v).8

Our analysis allows for arbitrary dependence between own and peer attributes, both observed
and unobserved, and consequently is agnostic regarding the protocol used to group students
into classrooms.

Two implications of this agnosticism are (i) our analysis is necessarily silent about the
presence and nature of any peer group eﬀects, and (ii) it is likely that more complicated poli-
cies, involving simultaneously regrouping students into new classes and reassigning teachers
to them, could raise achievement by more than what is feasible via reassignments of teachers
to existing classrooms alone, which is the class of policies we consider.9

Although nothing about the MET protocol generates restrictions on the joint density (1),

random assignment of teachers to classrooms – however formed – ensures that

fXi,Vi, ¯Xp(i), ¯Vp(i),Wi,Ui(x, v, ¯x, ¯v, w, u) = fXi,Vi, ¯Xp(i), ¯Vp(i)

(x, v, ¯x, ¯v)fWi,Ui(w, u).

(2)

Here Wi and Ui denote the observed and unobserved attributes of the teacher assigned to
the classroom of student i. A perfect implementation of MET as designed would ensure

7We cannot, for example, have an assignment which implies all students have above-average peers.
8Indeed additional restrictions on the second density to the right of the equality above would also hold;

see Graham et al., 2010, and Graham, 2011, for additional discussion and details.

9Learning about the eﬀects of policies which simultaneously regroup students and reassign teachers would
require double randomization. See Graham (2008) for an empirical example and Graham et al. (2010, 2020)
for a formal development.

5

that student and teacher attributes vary independently of each other. Our research design is
fundamentally based upon restriction (2).

Let Yi be an end-of-year measure of student achievement, generated according to

Yi = g (cid:0)Xi, ¯Xp(i), Wi, Vi, ¯Vp(i), Ui

(cid:1) .

(3)

Other than the restriction that observed and unobserved peer attributes enter as means,
equation (3) imposes no restrictions on educational production.10 Under restriction (2) the
conditional mean of the outcome given observed own, peer, and teacher attributes equals

E (cid:2)Yi| Xi = x, ¯Xp(i) = ¯x, Wi = w(cid:3) =

(cid:90) (cid:90) (cid:90) (cid:104)

g (x, ¯x, w, v, ¯v, u) f Vi, ¯Vp(i)|Xi,Xp(i)

(v, ¯v| x, ¯x)

×f Ui|Wi (ui| wi)(cid:3) dvd¯vdu
= mamf (x, ¯x, w) .

(4)

Equation (4) coincides with (a variant of) the Average Match Function (AMF) estimand
discussed by Graham et al. (2014, 2020). The AMF can be used to identify AREs. Our
setting – which involves multiple students being matched to a single teacher – is somewhat
more complicated than the one-to-one matching settings considered by Graham et al. (2014,
2020). One solution to this “problem” would be to average equation (3) across all students in
the same classroom and work directly with those averages. As will become apparent below,
however, working with a student-level model makes it easier to deal with non-compliance
and attrition, which have distinctly student-level features. It also connects our results more
directly with existing empirical work in the economics of K-to-12 education, where student-
level modelling predominates, and results in greater statistical power.

The decision to model outcomes at the student level makes the analysis of teacher re-
assignments a bit more complicated, at least superﬁcially. To clarify the issues involved it is
helpful to consider an extended example. Assume there are two types of students, Xi ∈ {0, 1},
and two types of teachers, Wi ∈ {0, 1}. For simplicity assume that the population fractions
of type Xi = 1 students and type Wi = 1 teachers both equal one-half, that is, half of the
students are of type 1 and half of the students are taught by a teacher of type 1. Assume,
again to keep things simple, that classrooms consist of three students each.

Table 1 summarizes this basic set-up. Column 1 lists classroom types. For example, a
000 classroom consists of all type-0 students. There are four possible classroom types, each
assumed to occur with a frequency of one-fourth.11 The status quo mechanism for grouping
students into classrooms induces a joint distribution of own and peer average attributes.

10A fully nonparametric model would allow achievement to vary with any exchangeable function of peer

attributes; see Graham et al. (2010) for more details.
11Within classrooms students are exchangeable.

6

Table 1: Feasible teacher reassignments

Classroom Type Pr (cid:0) Wi = 1| Xi, ¯Xp(i)

(cid:1) Pr

Status quo

Counterfactual
(cid:12)
(cid:16) ˜Wi = 1
(cid:12) Xi, ¯Xp(i)
(cid:12)

(cid:17)

Xi

f (cid:0)Xi, ¯Xp(i)

(cid:1)

1
3

1
2

1
2

1
2

011

001

000

0
0
0
0
0
1
0
1
1
1
1
1
Note: The population fraction of type Xi = 1 students is 1
2 and that of type Wi = 1 teachers is also 1
2 .
Classrooms of three students each are formed, such that the frequency of each of the four possible classroom
conﬁgurations is 1
4 in the population of classrooms of size 3. Under the status quo teachers are assigned to
classrooms at random; in the counterfactual teachers are assigned more assortatively. See the main text for
more information.

1
4
1
4
1
4
1
6
1
6
1
12
1
12
1
6
1
6
1
4
1
4
1
4

111

1
2

1
2

2
3

1
2

¯Xp(i)
0
0
0
1
2
1
2
0
1
1
2
1
2
1
1
1

4 (3 out
(0, 0) = 1
4,
6 (2 out of 12) of the students are in a classroom with one type-0 and one type-1 peer,
6. The MET experiment implies no restrictions on the joint density

This joint distribution is given in the right-most column of Table 1. For instance, 1
of 12) of the students are in a classroom with two type-0 peers, so that fXi, ¯Xp(i)
and 1
so that fXi, ¯Xp(i)
fXi, ¯Xp(i)

(x, ¯x), consequently we only consider policies which leave it unchanged.

2) = 1

(0, 1

Next assume, as was the case in the MET experiment, that under the status quo teachers
are randomly assigned to classrooms. This induces the conditional distribution of Wi given
Xi and ¯Xp(i) reported in column 2 of Table 1. Of course, from this conditional distribution,
and the marginal for Xi and ¯Xp(i), we can recover the joint distribution of own type, peer
average type, and teacher type (i.e., of Xi, ¯Xp(i) and Wi).

2. Inspecting Table 1, this subpopulation represents 1

Now consider the AMF: mamf (x, ¯x, w). Consider the subpopulation of students with Xi =
1 and ¯Xp(i) = 1
6 of all students (right-
most column of Table 1). If we assign to students in this subpopulation a teacher of type
Wi = 1, then the expected outcome coincides with mamf (cid:0)1, 1
2, 1(cid:1). Under random assignment
of teachers the probability of assigning a type-1 teacher is the same for all subpopulations of
students.

Finally consider a counterfactual assignment of teachers to classrooms. Since we leave the
(x, ¯x) is left unmodiﬁed. The counterfactual as-

composition of classrooms unchanged, fXi, ¯Xp(i)
signment therefore corresponds to a conditional distribution for teacher type, ˜f ˜Wi|Xi, ¯Xp(i)

(w| x, ¯x)

7

which satisﬁes the feasibility condition:

(cid:90) (cid:90)

˜f ˜Wi|Xi, ¯Xp(i)

(w| x, ¯x) fXi, ¯Xp(i)

(x, ¯x) dxd¯x = f (w)

(5)

for all w ∈ W. Here ˜f denotes a counterfactual distribution, while f denotes a status
quo one. We use ˜Wi to denote an assignment from the counterfactual distribution. Note
D= Wi marginally, but will diﬀer conditional on
that by feasibility of an assignment ˜Wi
student attributes. Condition (5), as discussed by Graham et al. (2014), allows for degenerate
conditional distributions, as might occur under a perfectly positive assortative matching.
Average achievement under a counterfactual teacher-to-classroom assignment equals:

(cid:17)
βare (cid:16) ˜f

=

(cid:90) (cid:90) (cid:20)(cid:90)

mamf (x, ¯x, w) ˜f ˜Wi|Xi, ¯Xp(i)

(cid:21)

(w| x, ¯x) dw

fXi, ¯Xp(i)

(x, ¯x) dxd¯x.

(6)

Since all the terms to the right of the equality are identiﬁed, so too is the ARE. Conceptually
we ﬁrst – see the inner integral in equation (6) – compute the expected outcome in each type
of classroom (e.g., Xi = x and ¯Xp(i) = ¯x) given its new teacher assignment (e.g., to type
˜Wi = w). We then – see the outer two integrals in equation (6) – average over the status quo
distribution of Xi, ¯Xp(i), which is left unchanged. This yields average student achievement
under the new assignment of teachers to classrooms.

In addition to the feasibility condition (5) we need to also rule out allocations that assign
diﬀerent teachers to students in the same classrooms. Note that mamf (x, ¯x, w) is the average
outcome for the subpopulation of students of type Xi = x with peers ¯Xp(i) = ¯x. For example,
in Table 1 classroom 001 has students from two subpopulations so deﬁned. Assignment of
teachers to subpopulations of students opens up the possibility that a classroom is assigned
to teachers of diﬀerent types for its constituent subgroups of students.
If, as indicated
in Table 1, the teacher-type assignment probability is the same for all subpopulations of
students represented in a classroom, then the ARE in equation (6) coincides with one based on
direct assignment of teachers to classrooms. This implicit restriction on teacher assignments
provides a link between models for individual outcomes and classroom-level reallocations.

Semiparametric identiﬁcation under MET as implemented

In the MET experiment as implemented not all teachers and students appear in their assigned
classrooms. This occurs both due to attrition (e.g., when a student changes schools prior
to follow-up) as well as actual non-compliance (e.g., when a teacher teaches in a classroom
diﬀerent from their randomly assigned one).

In this section we describe our approach to identifying AREs in MET as implemented.
Relative to the idealized analysis of the previous subsection we impose two types of additional

8

restrictions. First, we work with a semiparametric, as opposed to a nonparametric, educa-
tional production function. Second, we make behavorial assumptions regarding the nature
of non-compliance. Both sets of assumptions are (partially) testable.

Educational production function

Our ﬁrst set of restrictions involve the form of the educational production function. A key re-
striction we impose is that unobserved student, peer, and teacher attributes enter separably.
Although this assumption features in the majority of economics of education empirical work
(e.g., Chetty et al., 2014a,b), it is restrictive. We also discretize the observed student and
teacher attribute. This allows us to work with a parsimonously parameterized educational
production function that nevertheless accommodates complex patterns of complementarity
between student and teacher attributes. Discretization also allows us to apply linear program-
ming methods to study counterfactual assigments (c.f., Graham et al., 2007, Bhattacharya,
2009).

Speciﬁcally we let Xi be a vector of indicators for each of K “types” of students. Types
correspond to intervals of baseline test scores. Our preferred speciﬁcation works with K = 3
types of students: those with low, medium, and high baseline test scores.12 In this case Xi is
a 2 × 1 vector of dummies for whether student i’s baseline test score was in the medium or
high range (with the low range being the omitted group). This deﬁnition of Xi means that
¯Xp(i) equals the 2 × 1 vector of fractions of peers in the medium and high baseline categories
(with the fraction low range omitted).

We discretize the distribution of the teacher attribute similarly: Wi is a vector of indicators
for L diﬀerent ranges of FFT scores. In our preferred speciﬁcation we also work with L = 3
types of teachers: those with low, medium, and high FFT scores. Hence Wi is again a 2 × 1
vector of dummies for whether the teacher of student i’s FFT score was in the medium or
high range (with the low range again being the omitted group).

We assess the sensitivity of our results to coarser and ﬁner discretizations of the baseline
test score and FFT distributions. Speciﬁcally we look at K = L = 2 and K = L = 4
dicretizations.

We posit that end-of-school year achievement for student i is generated according to

Yi = α + X (cid:48)
iβ + Vi
(cid:123)(cid:122)
(cid:125)
(cid:124)
Student Ability
(cid:0)Xi ⊗ ¯Xp(i)
(cid:124)
(cid:123)(cid:122)
Student-Peer Complementarity

+ ¯X (cid:48)
(cid:124)
(cid:1)(cid:48) ζ
(cid:125)

+

+ W (cid:48)
(cid:124)

p(i)γ + ρ ¯Vp(i)
i δ + Ui
(cid:123)(cid:122)
(cid:125)
(cid:123)(cid:122)
(cid:125)
Teacher Quality
Peer Eﬀect
+ (Xi ⊗ Wi)(cid:48) η + (cid:0)Wi ⊗ ¯Xp(i)

(cid:124)

(cid:123)(cid:122)
Student-Teacher Complementarity

(cid:1)(cid:48) λ
(cid:125)

(7)

Observe that – as noted above – own, Vi, peer, ¯Vp(i), and teacher, Ui, unobservables enter

12Precise variable deﬁnitions are given below.

9

equation (7) additively. The labelled grouping of terms in equation (7) highlights the ﬂex-
ibility of our model relative to those typically employed by researchers. As in traditional
models, observed and unobserved student and teacher attributes are posited to directly in-
ﬂuence achievement. We add to this standard set-up the possibility of complementarity
between own and peer attributes, and complementarity between own and teacher attributes.
Additionally our model allows for both observed and unobserved peer attributes to inﬂuence
achievement.

Conditional on working with a discrete student and teacher type space, equation (7) is
unrestrictive in how own and teacher attributes interact to generate achievement. In contrast,
equation (7) restricts the eﬀect of peers’ observed composition on the outcome. Partition
ζ = (ζ1, . . . , ζK−1) and similarly partition λ = (λ1, . . . , λL−1). The (K − 1) × 1 gradient of
student i’s outcome with respect to peer composition is

∂Yi
∂ ¯Xp(i)

= γ +

K−1
(cid:88)

k=1

Xkiζk +

L−1
(cid:88)

l=1

Wliλl,

(8)

which is constant in ¯Xp(i), although varying heterogenously with student and teacher type.
Put diﬀerently convexity/concavity in ¯Xp(i) is ruled out by equation (7). It should be noted
that the MET data, in which the assignment of peers is not random, are not suitable for
estimating peer eﬀects (non-linear or otherwise).

For completeness we also include the interaction of teacher type with peer composition
– the (cid:0)Wi ⊗ ¯Xp(i)
(cid:1) regressor in equation (7) – although λ is poorly identiﬁed in practice.13
Due to our limited sample size, we do not include the third order interactions of own, peer,
and teacher types.14

Relative to a standard “linear-in-means” type model typically ﬁtted to datasets like ours

(e.g., Hanushek et al., 2004):

Yi = α + X (cid:48)

iβ + ¯X (cid:48)

p(i)γ + W (cid:48)

i δ + Vi + Ui,

(9)

It allows for rich interactions in observed own, peer, and
equation (7) is rather ﬂexible.
teacher attributes and is explicit in that both observed and unobserved peer attributes may
inﬂuence own achievement. The “linear-in-means” model (9) presumes homogenous eﬀects

13Admittedly, the (cid:0)Wi ⊗ ¯Xp(i)

(cid:1) term is not entirely straightforward to interpret; however it seemed ad hoc
to include some second-order interactions while a priori excluding others. We also report speciﬁcations which
exclude this term below (as its coeﬃcient is always insigniﬁcantly diﬀerent from zero).

14The educational production function in equation (7) includes J = dim(α) + dim(β) + dim(γ) + dim(δ) +
dim(ζ) + dim(η) + dim(λ) = 1 + 2 + 2 + 2 + 4 + 4 + 4 = 19 parameters. A fully interacted model would
introduce 8 additional parameters, for 3 × 3 × 3 = 27 in total. Even this model would not be fully ﬂexible
since, although constructed from averages of binary indicators, ¯Xp(i) is not binary-valued. A more ﬂexible
model would therefore also include, for example, interactions of Xi with the squares of the elements of ¯Xp(i)
(and so on.)

10

and does not explicitly incorporate unobserved peer attributes.15

As mentioned above, a student’s assigned teacher and peers may deviate from her realized
ones due to attrition and non-compliance. To coherently discuss our assumptions about these
i and ¯Xp∗(i) denote student i(cid:48)s assigned
issues we require some additional notation. Let W ∗
teacher and peer attribute (here p∗ (i) is the index set of i(cid:48)s assigned classmates). Random
assignment of teachers to classrooms ensures that a student’s assigned teacher’s attributes
are independent of her own unobservables:

E (cid:2)Vi| Xi, ¯Xp∗(i), W ∗

i

(cid:3) = E (cid:2)Vi| Xi, ¯Xp∗(i)

(cid:3) def

≡ g1

(cid:0)Xi, ¯Xp∗(i)

(cid:1) .

(10)

Here g1(x, ¯x) is unrestricted. Under double randomization, with students additionally grouped
into classes at random, we would have the further restriction

E (cid:2)Vi| Xi, ¯Xp∗(i)

(cid:3) = E [Vi| Xi] .

However, since the MET experiment placed no restrictions on how students were grouped
into classrooms, we cannot rule out the possibility that a student’s peer characteristics, ¯Xp∗(i),
predict her own unobserved ability, Vi. Consequently our data are necessarily silent about the
presence and nature of any peer group eﬀects in learning. This limitation does not limit our
ability to study the eﬀects of teacher reallocations, because we leave the student composition
of classrooms – and hence the “peer eﬀect” – ﬁxed in our counterfactual experiments.

Finally, even with double randomization, we would still have E [ Vi| Xi] (cid:54)= 0. Observed
and unobserved attributes may naturally covary in any population (e.g., for example, average
hours of sleep, which is latent in our setting, plausibly covaries with baseline achievement
and also inﬂuences the outcome). Such covariance is only a problem if, as is true in the
traditional program evaluation setting, the policies of interest induce changes in the marginal
distribution of Xi – and hence the joint distribution of Xi and Vi.16 This is not the case here:
any reallocations leave the joint distribution of Xi and Vi unchanged.

The MET protocol also ensures that assigned peer unobservables, ¯Vp∗(i), are independent

of the observed attributes of one’s assigned teacher:

E (cid:2) ¯Vp∗(i)

(cid:12)
(cid:12) Xi, ¯Xp∗(i), W ∗

i

(cid:3) = E (cid:2) ¯Vp∗(i)

(cid:12)
(cid:12) Xi, ¯Xp∗(i)

(cid:3) def

≡ g2

(cid:0)Xi, ¯Xp∗(i)

(cid:1) ,

(11)

with g2

(cid:0)Xi, ¯Xp∗(i)

(cid:1), like g1

(cid:0)Xi, ¯Xp∗(i)

(cid:1), unrestricted.

Random assignment of teachers to classrooms also ensures independence of the unobserved

15Manski (1993) did allow for unobserved peer attributes as did Graham (2008); but these cases are

exceptional.

16A prototypical example is a policy which increases years of completed schooling. Such a policy necessarily
changes the joint distribution of schooling and unobserved labor market ability relative to its status quo
distribution.

11

attribute of a student’s assigned teacher and observed student and peer characteristics:

E (cid:2)U ∗

i | Xi, ¯Xp∗(i), W ∗

i

(cid:3) = E [U ∗

i | W ∗

i ] = 0.

(12)

The second equality is just a normalization.17

Under MET as designed we could identify AREs using equations (10), (11), and (12).
i and ¯Xp(i) = ¯Xp∗(i) for
To see this let, as would be true under perfect compliance, Wi = W ∗
all i = 1, . . . , N . Using equations (10), (11), and (12) yields, after some manipulation, the
partially linear regression model (e.g., Robinson, 1988):

Yi = W (cid:48)

i δ + (Xi ⊗ Wi)(cid:48) η + (cid:0)Wi ⊗ ¯Xp(i)

(cid:1)(cid:48) λ + h (cid:0)Xi, ¯Xp(i)

(cid:1) + Ai

with E (cid:2)Ai| Xi, ¯Xp(i), Wi

(cid:3) = 0 for

def

≡ (cid:2)Vi − g1

Ai

(cid:0)Xi, ¯Xp(i)

(cid:1)(cid:3) + ρ (cid:2) ¯Vp(i) − g2

(cid:0)Xi, ¯Xp(i)

(cid:1)(cid:3) + Ui,

and where the nonparametric regression component equals

(13)

(14)

h (cid:0)Xi, ¯Xp(i)

(cid:1) def

≡ α + X (cid:48)

iβ + g1

(cid:0)Xi, ¯Xp(i)

(cid:1) + ¯X (cid:48)

p(i)γ + ρg2

(cid:0)Xi, ¯Xp(i)

(cid:1) + (cid:0)Xi ⊗ ¯Xp(i)

(cid:1)(cid:48) ζ.

(15)

(cid:1) and g2

(cid:0)Xi, ¯Xp(i)

Note, even under this perfect experiment, we cannot identify β, γ, and ζ; these terms are
(cid:1) and hence absorbed into the nonparametric
confounded by g1
component of the regression model. This lack of identiﬁcation reﬂects the inherent inability
of the MET experiment to tell us anything about peer group eﬀects. Nevertheless, as detailed
below, knowledge of δ, η, and λ is suﬃcient to identify the class of reallocation eﬀects we
focus upon.

(cid:0)Xi, ¯Xp(i)

Patterns of non-compliance

Unfortunately, we do not observe student outcomes under full compliance. Non-compliance
may induce correlation between Ai and Xi, ¯Xp(i) and Wi in regression model (13). Our
solution to this problem is to construct instrumental variables for observed teacher and peer
attributes, Wi and ¯Xp(i) – which necessarily reﬂect any non-compliance and attrition on the
part of teachers and students – from the assigned values, W ∗

i and ¯Xp∗(i).

Rigorously justifying this approach requires imposing restrictions on how, for example,
realized and assigned teacher quality relate to one another. The ﬁrst assumption we make
along these lines is:

17Reallocations leave the joint distribution of Ui and Wi unchanged, so we are free to normalize this mean

to zero.

12

Assumption 1. (Idiosyncratic Teacher Deviations)

E (cid:2)Ui − U ∗

i | Xi, ¯Xp∗(i), W ∗

i

(cid:3) = 0.

(16)

Assumption 1 implies that the diﬀerence between realized and assigned (unobserved)
teacher “quality” cannot be predicted by own and assigned peer and teacher observables.
While Assumption 1 is not directly testable, we can perform the following plausibility test.
Let Ri − R∗
i be the diﬀerence between the realized and assigned value of some observed
teacher attribute other than Wi (e.g., years of teaching experience). Under equation (16),
i , and ¯Xp∗(i), a test for the joint
if we compute the OLS ﬁt of this diﬀerence onto 1, Xi, W ∗
signiﬁcance of the non-constant regressors should accept the null of no eﬀect. Finding that,
for example, students assigned to classrooms with low average peer prior-year achievement,
tend to move into classrooms with more experienced teachers suggests that Assumption 1
may be implausible.

Assumption 1 and equation (12) above yield the mean independence restriction

E (cid:2)Ui| Xi, W ∗

i , ¯Xp∗(i)

(cid:3) = 0.

(17)

This equation imposes restrictions on the unobserved attribute of student i’s realized teacher.
It is this latent variable which drives the student outcome actually observed.

Our second assumption involves the relationship between the unobserved attributes of a
student’s assigned peers and those of her realized peers. These two variables will diﬀer if
some students switch out of their assigned classrooms.

Assumption 2. (Conditionally Idiosyncratic Peer Deviations)

E (cid:2) ¯Vp(i) − ¯Vp∗(i)

(cid:12)
(cid:12) Xi, ¯Xp∗(i), W ∗

i

(cid:3) = E (cid:2) ¯Vp(i) − ¯Vp∗(i)

(cid:12)
(cid:12) Xi, ¯Xp∗(i)

(cid:3)

(18)

Assumption 2 implies that the diﬀerence between realized and assigned unobserved peer
“quality” cannot be predicted by assigned teacher observables. We do allow for these devia-
tions to covary with a student’s type and the assigned composition of her peers. Assumption
2 and equation (11) yield a second mean independence restriction of

E (cid:2) ¯Vp(i)

(cid:12)
(cid:12) W ∗

i , Xi, ¯Xp∗(i)

(cid:3) = g∗

2

(cid:0)Xi, ¯Xp∗(i)

(cid:1) ,

(19)

(cid:1) def

(cid:0)Xi, ¯Xp∗(i)

≡ E (cid:2) ¯Vp(i) − ¯Vp∗(i)

(cid:12)
(cid:1) is unrestricted. We can
(cid:12) Xi, ¯Xp∗(i)
where g∗
2
also pseudo-test Assumption 2 using observed peer attributes. Finding, for example, that
– conditional on own type, Xi, and assigned peers’ average type, ¯X ∗
p(i) – assigned teacher
quality, W ∗
i , predicts diﬀerences between the realized and assigned values of other observed
peer attributes provides evidence against Assumption 2.

(cid:0)Xi, ¯Xp∗(i)

(cid:3) + g2

13

The experiment-generated restrictions – equations (10), (11), and (12) above – in conjunc-
tion with our two (informally testable) assumptions about deviations from the experiment
protocol – Assumptions 1 and 2 above – together imply the following conditional moment
restriction:

E (cid:2)Ui + Vi + ρ ¯Vp(i)

(cid:12)
(cid:12) W ∗

i , Xi, ¯Xp∗(i)

(cid:3) = g1

(cid:0)Xi, ¯Xp∗(i)

(cid:1) + ρg∗

2

(cid:0)Xi, ¯Xp∗(i)

(cid:1) .

(20)

We wish to emphasize two features of restriction (20). First, the conditioning variables
are assigned peer and teacher attributes, not their realized counterparts. This reﬂects our
strategy of using assignment constructs as instruments. Second, any function of W ∗
i , as well
as interactions of such functions with functions of Xi and ¯Xp∗(i) do not predict the composite
error Ui + Vi + ρ ¯Vp(i) conditional on Xi and ¯Xp∗(i); hence such terms are valid instrumental
variables.

More speciﬁcally we redeﬁne h to equal

h (cid:0)Xi, X p∗(i), X p(i)

(cid:1) def

≡ α + X (cid:48)

(cid:0)Xi, X p∗(i)

(cid:1) + X

iβ + g1
(cid:0)Xi, X p∗(i)

(cid:1) + (cid:0)Xi ⊗ X p(i)

(21)

ζ

(cid:48)
p(i)γ
(cid:1)(cid:48)

and Ai to equal

+ ρg∗
2

def

≡ (cid:0)Vi − g1

Ai

(cid:0)Xi, X p∗(i)

(cid:1)(cid:1) + ρ (cid:0)V p(i) − g∗

2

(cid:0)Xi, X p∗(i)

(cid:1)(cid:1) + Ui.

(22)

Equations (7), (21), and (22) yield an outcome equation of

Yi = W (cid:48)

i δ + (Xi ⊗ Wi)(cid:48) η + (cid:0)Wi ⊗ ¯Xp(i)

(cid:1)(cid:48) λ + h (cid:0)Xi, ¯Xp∗(i), X p(i)

(cid:1) + Ai.

(23)

Condition (20) implies that Ai is conditionally mean zero given Xi, ¯Xp∗(i) and W ∗
i .

Summarizing, the experimentally-induced restrictions (10), (11), and (12), and our As-

sumptions 1 and 2 together imply that:

E (cid:2)Ai| Xi, ¯Xp∗(i), W ∗

i

(cid:3) = 0.

(24)

The estimation simpliﬁes if we impose a restriction on the peer attrition/non-compliance

that is similar to Assumption 2, but is on the observable peer average:

E (cid:2) ¯Xp(i) − ¯Xp∗(i)

(cid:12)
(cid:12) Xi, ¯Xp∗(i), W ∗

i

(cid:3) = E (cid:2) ¯Xp(i) − ¯Xp∗(i)

(cid:12)
(cid:12) Xi, ¯Xp∗(i)

(cid:3) .

(25)

This restriction is directly testable. By equation (25),

E (cid:2) ¯Xp(i)

(cid:12)
(cid:12) Xi, ¯Xp∗(i), W ∗

i

(cid:3) = E (cid:2) ¯Xp(i)

(cid:12)
(cid:12) Xi, ¯Xp∗(i)

(cid:3) .

14

If this restriction holds the outcome equation is as in (23), but with a redeﬁned nonparametric
h that is a function of Xi and ¯Xp∗(i) only,

h (cid:0)Xi, X p∗(i)

(cid:1) def

≡ α + X (cid:48)

iβ + g1
(cid:0)Xi, X p∗(i)

(cid:1) + E (cid:2) ¯Xp(i)

(cid:0)Xi, X p∗(i)
(cid:1) + (cid:0)Xi ⊗ E (cid:2) ¯Xp(i)

(cid:12)
(cid:12) Xi, ¯Xp∗(i)
(cid:3)(cid:1)(cid:48) ζ,

(cid:12)
(cid:12) Xi, ¯Xp∗(i)

(cid:3)(cid:48) γ+

ρg∗
2

(26)

and Ai is

def

≡ (cid:0)Vi − g1

Ai

(cid:0)Xi, X p∗(i)
(cid:12)
(cid:12) Xi, ¯Xp∗(i)

(cid:1)(cid:1) + ρ (cid:0)V p(i) − g∗

(cid:1)(cid:1) + Ui+
(cid:0)Xi, X p∗(i)
(cid:3)(cid:1)(cid:48) γ + (cid:0)Xi ⊗ (cid:0) ¯Xp(i) − E (cid:2) ¯Xp(i)

2

(cid:0) ¯Xp(i) − E (cid:2) ¯Xp(i)

(cid:12)
(cid:12) Xi, ¯Xp∗(i)

(cid:3)(cid:1)(cid:1)(cid:48) ζ.

(27)

The conditional moment restriction in equation (24) also holds for this error.
Equations (23) and (24) jointly deﬁne a partially linear model with an endogenous para-
metric component. This is a well-studied semiparametric model (see, for example, Chen
et al., 2003). The parameters δ, η, and λ are identiﬁed; h (cid:0)Xi, ¯Xp∗(i)
(cid:1) is a nonparametric
nuisance function.

We implement the partial linear IV estimator using the following approximation for

h(x, ¯x):

h (cid:0)Xi, ¯Xp∗(i)

(cid:1) ≈ X (cid:48)

ib + ¯X (cid:48)

p∗(i)d + (cid:0)Xi ⊗ ¯Xp∗(i)

(cid:1)(cid:48) f.

For this approximation we estimate δ, η, and λ by linear IV ﬁt of Yi onto a constant,
Xi, ¯Xp∗(i), (cid:0)Xi ⊗ ¯Xp∗(i)
(cid:1) using the excluded instruments
(cid:1). Note that both assigned and realized peer groups enter
i , (Xi ⊗ W ∗
W ∗
the main equation.

(cid:1), Wi, (Xi ⊗ Wi), and (cid:0)Wi ⊗ ¯Xp(i)
i ⊗ ¯Xp∗(i)

i ), and (cid:0)W ∗

As in the case with perfect compliance, we do not identify β, γ, and ζ, again reﬂecting
the inherent inability of the MET experiment to tell us anything about peer group eﬀects.
Nevertheless knowledge of δ, η, and λ is suﬃcient to identify the class of reallocation eﬀects
we focus upon.

3 Data and tests of identifying assumptions

The Measures of Eﬀective Teaching (MET) study

The MET study was conducted during the 2009/10 and 2010/11 school years in elementary,
middle, and high schools located in six large urban school districts in the United States.18

18The school districts are: Charlotte-Mecklenburg (North Carolina), Dallas Independent School District
(Texas), Denver Public Schools (Colorado), Hillsborough County Public Schools (Florida), Memphis City
Schools (Tennessee), and the New York City Department of Education (New York). The participation of
school districts and schools in the MET study was voluntary.

15

The goal of the study was to examine determinants and consequences of teacher quality and
teaching practices. In the ﬁrst year of the study, the MET researchers collected detailed back-
ground information on teaching practices for each teacher as well as background performance
In the second year of the study, MET researchers conducted
measures for each student.
a ﬁeld experiment, randomly assigning teachers to classrooms within school-by-grade-by-
subject cells (“randomization blocks”). The randomization blocks typically consisted of two
to three classrooms each. Classroom composition was not manipulated as part of the study.
For more details about the study design, see White et al. (2019) and Kane et al. (2013).

The dataset contains detailed information on the students, their classroom teachers, and
their classroom peers. As our key measure of teaching practices, we use Danielson’s (2011)
“Framework for Teaching” (FFT) measure, collected at baseline (school year 2009/10). The
FFT is designed to provide feedback to teachers, but has also been used to study the impact
of teaching practices on student outcomes (c.f., Garrett and Steinberg, 2015, Aucejo et al.,
2019). The FFT is an observational measure: teachers are video-taped several times during
a school year and subsequently rated – using a specially designed rubric – by trained raters
(often former teachers).

For our main outcome measure we use students’ 2010/11 end-of-year standardized state
test scores in the subjects mathematics and English language arts (ELA). The dataset also
includes background characteristics from school district records for students (age, gender,
race/ethnicity, special-education status, free/reduced-price lunch eligibility, gifted status,
and whether a student is an English language learner) and for teachers (education, teaching
experience in the district). Through a section identiﬁer, we can match a student to her
classroom peers.

Sample and summary statistics

In constructing our estimation sample, we closely follow Garrett and Steinberg (2015), who
investigate the impact of FFT on students’ test score outcomes in the MET study. We
restrict our sample to all elementary- and middle-school students (grades 4–8) who took part
in the randomization. Furthermore, we only include students with non-missing information
on baseline and ﬁnal test score outcomes as well as students with non-missing information on
the characteristics of the assigned and realized teacher, and the characteristics of the assigned
and realized classroom peers. Details of the data construction from the MET data ﬁles are
provided in Appendix B.

Our ﬁnal sample consists of about 8,500 students and 614 teachers in math and of about
9,600 students and 649 teachers in ELA.19 The majority of the students (60 percent) are ele-
mentary school students (grades 4–5), and the remaining students are middle school students

19Appendix Table A.1 summarizes all student and teacher variables.

16

(grades 6–8). Nearly 60 percent of the students are economically disadvantaged, that is, el-
igible for free/reduced-price lunch.20 The student sample is diverse, with equal proportions
of white, black, and Hispanic students.

Student test scores are centered at the relevant district-level mean and standardized by
the relevant district-level standard deviation. MET students exceed their district average
test score by about 0.15 standard deviations on average. The majority of the teachers are
female (about 85 percent), and about two-thirds of them are white. Most of the teachers
have substantial teaching experience – seven years on average – and about 40 percent of the
teachers have graduated from a Master’s program.21

Teaching practices

The MET data includes ratings of teaching practices for two domains of the FFT, “Classroom
Environment” and “Instruction.” These domains are divided into four components each,22
and each component is rated on a four-point scale by each rater (unsatisfactory, basic, pro-
ﬁcient, distinguished), with high scores in a category indicating that a teacher is closer to
an ideal teaching practice according to the FFT. The teachers are video-taped at least twice
during the school year during lessons, and each video is rated independently by at least two
raters. In our analysis, we average the scores across all videos, raters, components, and do-
mains. This aggregation of the scores is the most common one in the literature (cf., Garrett
and Steinberg, 2015, Aucejo et al., 2019).

The FFT is on average 2.5 in math and 2.6 in ELA, which corresponds to a rating between
“basic” and “proﬁcient”; for the purpose of our analysis, we create three categories of FFT
(see Appendix Figure A.1). We set the cutoﬀs at FFTs of 2.25 and 2.75. In our sample, low-
FFT teachers have “basic” teaching practices on average (average FFT of 2.1 in both math
and ELA), and high-FFT teachers have “proﬁcient” teaching practices on average (average
FFT of 2.9 in both math and ELA); 18 percent of the teachers in math and 14 percent of
the teachers in ELA are classiﬁed as having a low FFT, 62 percent of the teachers in math
and 58 percent of the teachers in ELA are classiﬁed as having a middle FFT, and 20 percent
of the teachers in math and 28 percent of the teachers in ELA are classiﬁed as having a high
FFT. Our results are not sensitive to the exact position of the cutoﬀs, and we investigate
a model with two or four, instead of three, FFT categories in our sensitivity checks below.
in
This categorization also creates variation in FFT levels within randomization blocks:

20This variable is missing for 30 percent of the students in the math sample and for 25 percent of students

in the ELA sample.

21Teaching experience and teacher education are missing for about 30 percent of teachers.
22The components of the domain “Classroom Environment” are: creating an environment of respect and
rapport; establishing a culture for learning; managing classroom procedures; managing student behavior. The
components of the domain “Instruction” are: communicating with students; using questioning and discussion
techniques; engaging students in learning; using assessment in instruction.

17

our sample, 65 percent of randomization blocks in math, and 70 percent of randomization
blocks in ELA include teachers with diﬀerent levels of FFT (for example, both a low- and a
middle-FFT teacher).

Test score outcomes

The end-of-year test scores provided in the MET data are z-scores, i.e., they are standardized
such that district-wide, they will be mean zero with unit standard deviation. Since MET
schools are not representative at the district level, the mean test scores may – and do –
deviate from zero. We use the 2010/11 z-score as our outcome variable.

To identify reallocation eﬀects, we split 2009/10 baseline test scores into three bins,
corresponding to terciles of the within-district z-score distribution. In our estimation sample,
27 percent of the students in math, and 26 percent in ELA, have “low” baseline test scores,
36 percent of the students in math, and 35 percent of the students in ELA, have “middle”
baseline test scores, and 36 percent of the students in math, and 39 percent of the students
in ELA, have “high” baseline test scores.23 To include classroom peers into the analysis, we
compute the fraction of each student’s classmates with high, middle, and low baseline test
scores (leave-own-out means).

Non-compliance

As outlined in Section 2, some teachers and students switched classrooms or schools before
the start of the school year, which we take into account in our identiﬁcation strategy. In
our sample, 69 percent of the students in math, and 73 percent of the students in ELA, are
actually taught by their randomly assigned teachers. This level of non-compliance is high
enough to make analyses which ignore it potentially problematic.

We also observe changes in classroom peers after the randomization but before the be-
ginning of the school year. Changes in assigned peers were driven by students who leave
schools, repeat a grade or, in some cases, by the need to adjust their schedule. On average,
students’ realized peers, however, are not appreciably diﬀerent from their assigned ones. The
diﬀerence in baseline z-scores between the assigned and the realized peers amounts to just
0.02 standard deviations on average both in math and in ELA.

Tests of identifying assumptions and restrictions

In this section, we report the results of the series of speciﬁcation tests discussed earlier; specif-
ically tests designed to assess the plausibility of Assumptions 1 and 2. These two assumptions
impose restrictions on the nature of non-compliance by students and teachers. We further

23These numbers are not exactly 33 percent in each category, because the students in our sample have on

average higher baseline test scores than the full student population in a district.

18

directly test restriction (25), which is a restriction on the nature of non-compliance by peers.
We also assess the quality of the initial MET randomization of teachers to classrooms.

To test whether the randomization was successful in balancing student characteristics
across teachers with diﬀerent levels of FFT we regress the FFT of a student’s assigned teacher
on the student’s own characteristics, controlling for randomization block ﬁxed eﬀects. None
of the student characteristics predict assigned teacher’s FFT, individually or jointly, which
conﬁrms that the randomization indeed “worked” (see Appendix Table A.2).

Covariate balance, however, is not a suﬃcient condition to identify reallocation eﬀects
under non-compliance by both students and teachers (see Section 2). Assumption 1 requires
that own and assigned peer and teacher observables should not predict the diﬀerence between
realized and assigned unobserved teacher quality. Since this assumption involves a statement
about unobserved variables, we cannot test it directly.
Instead we “test” it indirectly as
described in Section 2. Speciﬁcally we use those teacher background characteristics that are
not part of the the model as replacements for the unobserved quality of a teacher: a teacher’s
demographics, experience, and education. We regress the diﬀerence between realized and
assigned teacher characteristics on the student’s baseline test score, the FFT of the assigned
teacher, and the average baseline test score of the assigned peers. Consistent with Assumption
1, these variables do not jointly predict diﬀerences between the characteristics of the assigned
and realized teacher in any of the regression ﬁts (see Appendix Table A.3).

Assumption 2 states that diﬀerences between realized and assigned unobserved peer qual-
ity cannot be predicted by assigned teacher observables, conditional on own baseline achieve-
ment and the baseline achievement of assigned peers. Again, we can only perform an indirect
test of this assumption. To do so we regress diﬀerences between the assigned and realized
characteristics of classroom peers onto the FFT of the assigned teacher, controlling for own
baseline test scores and assigned peers’ average baseline test scores. We do not ﬁnd, consis-
tent with Assumption 2, that teacher FFT predicts diﬀerences between the characteristics of
the assigned and realized peers (see Appendix Tables A.4 and A.5).

Finally, we directly assess restriction (25), which implies that the baseline test scores
of realized peers cannot be predicted by assigned teacher FFT. We test this restriction by
regressing the baseline test scores of realized peers onto a student’s baseline test score, the
baseline test scores of her assigned peers, as well as the FFT of her assigned teacher. We
ﬁnd that this restriction also holds in our data (see Appendix Table A.6).

4 Computing reallocation eﬀects

To construct estimates of the average eﬀect of reassigning teachers across classrooms we
proceed in three steps. First, as outlined in Section 2, we estimate (a subset of) the param-
eters of the educational production function. Second, we feed these parameters into a linear

19

programming problem to compute an optimal assignment (i.e., an assignment that maxi-
mizes aggregate achievement). Third, we compare the aggregate outcome under the optimal
assignment to the aggregate outcome under the status quo assignment, as well as to that
under the worst assignment (i.e., the one which minimizes aggregate achievement). These
comparisons provide a sense of the magnitude of achievement gains available from improved
teacher assignment policies.

Estimation of the education production function

The speciﬁcation that we use to estimate the parameters of the educational production func-
tion coincides with equation (23). Since randomization was carried out within randomization
blocks, we additionally include randomization block ﬁxed eﬀects in this regression model. We
then estimate the model’s parameters by the method of instrumental variables (IV) as de-
scribed in Section 2. Tests for instrument relevance following Sanderson and Windmeijer
(2016) conﬁrm that our IV estimates are unlikely to suﬀer from weak-instrument bias (see
Appendix Table A.7).

Computing and characterizing an optimal assignment

The aim of the analysis is to ﬁnd an assignment of teachers to classrooms that improves
student outcomes. What is meant by an “optimal” assignment depends, of course, on the
objective function.

In this paper we choose to maximize aggregate outcomes (i.e., the sum of all students’ test
scores). This is the “simplest” objective we can consider, it is straightforward to compute,
justiﬁable from a utilitarian perspective, and easy to interpret. We would like to emphasize,
however, that our analysis can be modiﬁed to accommodate other objective functions – in
some cases easily, in others with more diﬃculty. In practice, for example, the social planner
may care about both the aggregate outcome as well as inequality, especially across identiﬁable
subgroups.

Our intention is not to advocate for maximization of the aggregate outcome in practice,
although doing so is appropriate in some circumstances; rather this objective provides a
convenient starting point and makes our analysis comparable to that of other educational
policy evaluations (e.g., the typical regression-based class-size analysis provides an estimate
of the eﬀect of class-size on average achievement).

To determine the optimal allocation, we feed the estimated parameters from equation (23),
speciﬁcally ˆδ, ˆη, and ˆλ, into a linear program. Given these parameters, we can compute, for
each student, her predicted outcome when taught by a low-, middle-, or high-FFT teacher,
leaving the original classroom composition unchanged.

20

Formally, for each student i, we compute three counterfactual outcomes (cid:98)Yi(w), w ∈
{wL, wM , wH}, corresponding to an assignment to a low-, middle- or high-FFT teacher. Ag-
gregated to the classroom level this yields, in an abuse of notation, three counterfactual
classroom-level test score aggregates, (cid:98)Yc(w) = (cid:80)
i∈s(c) (cid:98)Yi(w), with w ∈ {wL, wM , wH} (here
s (c) denotes the set of indices for students in classroom c).

By aggregating the counterfactual outcomes to the classroom level, we transform the as-
signment problem from a many-to-one matching problem to a one-to-one matching problem.
This approach is only suitable because the conﬁguration of students across classrooms re-
mains ﬁxed; we only consider the eﬀects of reassigning teachers across existing classrooms of
students.24 The one-to-one matching problem is a special linear program, a transportation
problem, which is easily solvable.25

There are a few additional constraints we impose to make the reallocation exercise real-
istic. First, we do not allow teachers to be reassigned across districts or across school types
(i.e., elementary or middle school). We also present, as a sensitivity check, a version of the
allocation where we only allow teachers to be reassigned within their randomization block.
Second, there are a few teachers that teach several sections of a class. In this case, we treat
these sections as clusters, and allocate one teacher to all sections in each such cluster.

When computing counterfactual outcomes for each student, we omit the nonparametric
component, h(x, ¯x), as well as the randomization block ﬁxed eﬀects and the main eﬀects of
own baseline achievement. This is appropriate because we report the diﬀerence between the
sum of aggregate outcomes for two allocations where the nonparametric component, the ﬁxed
eﬀects, and the main eﬀects of own baseline achievement cancel out.

In addition to the optimal assignment, we also compute a “worst” possible assignment, i.e.,
the assignment that minimizes aggregate test scores. The diﬀerence between the aggregate
outcome for the best and worst assignment is the maximal reallocation gain. This provides
an upper bound on the magnitude of student achievement gains that teacher reassignments
might yield in practice.

Average Reallocation Eﬀects

An individual reallocation eﬀect, or reallocation gain, is deﬁned as the diﬀerence between
an individual student’s outcome under two assignments. For example, one can compute, for
), i.e., the predicted outcome under the optimal allocation, where ˜W opt
each student, (cid:98)Yi( ˜W opt
takes the values wL, wM , or wH, depending on whether the student would be assigned to
a low-, middle-, or high-FFT teacher in an optimal allocation. Similarly, one can compute

i

i

24As noted earlier the eﬀects of classroom composition changes on student achievement are unidentiﬁable

here in any case.

25Appendix C contains the details on how we specify the transportation problem. To compute the optimal

assignment, we use the function lpSolve in R.

21

the same parameter for each student under the status quo, which we denote as (cid:98)Yi(Wi). In
MET schools the status quo assignment was induced by random assignment of teachers to
classrooms within randomization blocks. The allocation of teachers across schools within a
given district was, of course, non-random, and possibly non-optimal from the standpoint of
maximizing the aggregate outcome. An individual reallocation gain can then be computed
as the diﬀerence between these two outcomes, (cid:98)Yi( ˜W opt
) − (cid:98)Yi(Wi). The optimal assignment is
not the assignment that maximizes the predicted outcome for student i, but the assignment
that maximizes the aggregate outcome across all the classrooms in the sample (subject to the
various constraints we impose on the problem – like ruling out reassignments across school
districts).

i

These individual gains can be aggregated in many ways in order to create policy-relevant
parameters. We start by deﬁning our key parameter of interest, the average reallocation
eﬀect, as

(cid:91)ARE =

1
N

N
(cid:88)

i=1

(cid:104)
(cid:98)Yi( ˜W opt

i

(cid:105)
) − (cid:98)Yi(Wi)

.

(28)

We also consider conditional average reallocation eﬀects, that is, average reallocation
eﬀects for students with varying baseline characteristic x (e.g., students with low, middle,
and high baseline test scores):

(cid:91)ARE(x) =

1
i=1 1(Xi = x)

(cid:80)N

N
(cid:88)

i=1

1(Xi = x)

(cid:104)

(cid:98)Yi( ˜W opt

i

(cid:105)
) − (cid:98)Yi(Wi)

.

(29)

Below we show that only about one-half of MET students would experience a teacher
change when switching from the status quo teacher assignment to an optimal one. The
reallocation eﬀect for students in classrooms not assigned a diﬀerent teacher is, of course,
zero. This motivates a focus on the average achievement gain among those students who are
assigned a diﬀerent teacher. We deﬁne the reallocation eﬀect conditional on being reassigned
as:

(cid:92)AREC(x) =

1
i=1 1(Xi = x)1(Wi (cid:54)= ˜W opt

i

(cid:80)N

)

×

N
(cid:88)

i=1

1(Xi = x)1(Wi (cid:54)= ˜W opt

i

)

(cid:104)

(cid:98)Yi( ˜W opt

i

(cid:105)
) − (cid:98)Yi(Wi)

.

(30)

We can, of course, not condition on x as well.
Finally, the maximal reallocation gain is obtained if we compare the average outcome

under an optimal assignment with that under a worst assignment

22

max

(cid:91)ARE

(x) =

1
i=1 1(Xi = x)

(cid:80)N

N
(cid:88)

i=1

(cid:104)

1(Xi = x)

(cid:98)Yi( ˜W opt

i

) − (cid:98)Yi( ˜W worst

i

(cid:105)
)

,

(31)

where (cid:98)Yi( ˜W worst
aggregate test scores.

i

) is the predicted outcome for student i under an allocation that minimizes

Inference on reallocation eﬀects

We use the Bayesian bootstrap to quantify our (posterior) uncertainty about AREs. We treat
each teacher-classroom pair as an i.i.d. draw from some unknown (population) distribution.
Following Chamberlain and Imbens (2003), we approximate this unknown population by
a multinomial, to which we assign an improper Dirichlet prior. This leads to a posterior
distribution which (i) is also Dirichlet and (ii) conveniently only places probability mass on
data points observed in our sample.

Mechanically, we draw C standard exponential random variables and weigh each student
in section/classroom c = 1, . . . , C with the cth weight (i.e., all students in the same section
have the same weight). Using this weighted sample, we compute the IV regression ﬁt, solve
for the optimal (worst) assignment, and compute the various reallocation eﬀects. We repeat
this procedure 1,000 times. This generates 1,000 independent draws from the posterior
distribution of the ARE.

Formally this approach to inference is Bayesian. Consequently the “standard errors” we
present for our ARE estimates summarize dispersion in the relevant posterior distribution
(not variability across repeated samples).26 An alternative, frequentist, approach to inference
is provided by Hsieh et al. (2018). They transform the problem of inference on the solution
to a linear program into inference on a set of linear moment inequalities.
If the binding
constraints are the same over the bootstrap distribution, then inference based on the Bayesian
bootstrap will be similar to that based on moment inequalities (see also Graham et al., 2007
and Bhattacharya, 2009).

26The “standard errors” for the AREs are constructed as follows. Let ˆθ(b) be the estimate of the reallocation
eﬀect in bth the bootstrap sample (or equivalently the bth draw from the posterior distribution for θ) and
ˆθ the reallocation eﬀect in the original sample. Consider the centered statistic ˆθ(b) − ˆθ; let q(0.025) and
q(0.975) be the 0.025 and 0.975 quantiles of its bootstrap/posterior distribution. We then construct the
interval [ˆθ − q(0.975), ˆθ − q(0.025)] (see Hansen, 2020). We compute the corresponding standard error by
dividing this interval by 2Φ−1(0.975) ≈ 3.92, where Φ(.) is the standard normal CDF.

23

5 Results

Regression results

Before reporting parameter estimates for our preferred model – equation (23) above with
three categories of FFT, own and peer baseline achievement each – we present those for a
more conventional model with teacher FFT, student and peer average baseline test scores
all entering linearly. For this speciﬁcation we leave the FFT and baseline test scores undis-
cretized. These initial results replicate and expand upon the prior work of Garrett and
Steinberg (2015), who study the impact of FFT on student achievement in (approximately)
the same sample.27

These initial results indicate that teacher FFT does not aﬀect students’ test scores on
average (see Table 2, Panel A). This is consistent with the ﬁndings of Garrett and Steinberg
(2015). Table 2 presents estimates, using the instrumental variables described in Section
2. Here this involves using the FFT score of a student’s randomly assigned teacher as an
instrument for that of her realized teacher (Panels A–C), and the average baseline test score
of her assigned peers as an instrument for the average baseline test score of her realized peers
(Panel B).

OLS estimates of the same model are reported in Appendix Table A.8. The coeﬃcient
on FFT is statistically signiﬁcant and positive in the OLS results. The discrepancy between
the OLS and IV results may reﬂect the impact of correcting for non-compliance, as described
above, or simply reﬂect the greater sampling variability of the IV estimates.

Next we add the interactions of teacher FFT with both the baseline student, and peer
average, test scores. This provides an initial indication of whether any complementarity
between teacher FFT and student baseline achievement is present. These results suggest
that high-FFT teachers are more eﬀective at teaching students with higher baseline scores
(see Table 2, Panels B and C). This result is signiﬁcant at the 5-percent level in the math
sample, but only weakly signiﬁcant in the ELA sample. The magnitudes and precision of
the teacher-student match eﬀects are similar across the IV and OLS estimates (the latter
reported in Appendix Table A.8). The coeﬃcient on the interaction between teacher FFT
and peer average baseline achievement (Panel B) is poorly determined (whether estimated
by IV or OLS).

In Table 3 we report IV estimates of our preferred 3 × 3 model. This corresponds to
equation (23) with both Wi and Xi consisting of dummy variables for middle and high FFT
and baseline test scores respectively.
In this speciﬁcation, the teacher FFT main eﬀects
remain insigniﬁcant. We do, however, ﬁnd positive match eﬀects between teacher FFT and
student baseline scores. These are signiﬁcant for the high-FFT teachers (i.e., teachers with

27Aucejo et al. (2019) study the impact of FFT in the MET data, but focus on math classrooms in

elementary schools.

24

Table 2: IV regression results of the linear model. Dependent variables: student test score outcomes

(2)
(1)
A. Only teacher
eﬀects

Math
0.013
(0.093)

ELA
0.079
(0.076)

FFT

FFT × baseline

FFT × avg. peer baseline

Baseline

0.749***
(0.011)

0.693***
(0.011)

(3)
(4)
B. Full model

Math
-0.004
(0.091)
0.096**
(0.042)
0.016
(0.103)
0.505***
(0.109)

ELA
0.104
(0.075)
0.073*
(0.039)
-0.172
(0.197)
0.512***
(0.099)

(5)

(6)

C. Without
teacher × peer
interactions

Math
-0.004
(0.091)
0.098**
(0.046)

ELA
0.076
(0.075)
0.049
(0.040)

0.498***
(0.119)

0.565***
(0.103)

0.700
8,534

0.632
9,641

0.700
8,534

0.621
9,641

0.700
8,534

0.632
9,641

δ

η

λ

β

R2
N

Note: The dependent variables are subject-speciﬁc test score outcomes in math and ELA. The speciﬁcations
include linear terms for FFT, individual and peer baseline test scores. The instrumental variables are based
on assigned teacher FFT (Panels A–C) and assigned peer baseline test scores (Panel B). All regressions
control for the h(x, ¯x) function (see Section 2) and for randomization block ﬁxed eﬀects. Analytic standard
errors, clustered by randomization block, are in parentheses.
** signiﬁcant at the 1%-level ** signiﬁcant at the 5%-level * signiﬁcant at the 10%-level.

a “proﬁcient” score on average). Students with middle or high baseline test score levels
score signiﬁcantly higher on end-of-year achievement tests when matched with a high-FFT
teacher, compared to students with low baseline test scores (see Table 3, Panels B and C).28
The OLS estimation results, reported in the appendix, are qualitatively similar to the IV
ones; although the interactions of the middle-FFT dummy variable with the middle- and
high-baseline student test score dummies are also signiﬁcant when estimated by OLS (see
Appendix Table A.9).

To compute optimal assignments and average reallocation eﬀects, we use the IV estimates
presented in columns 5–6 of Table 3. These speciﬁcations omit the FFT-by-peer composition
interaction terms, whose coeﬃcients are poorly determined in all speciﬁcations (whether
ﬁtted by IV or OLS). Omitting these terms has little eﬀect on either the location or the
precision of the coeﬃcients on the FFT-by-baseline interactions. In Appendix Tables A.15
and A.16 we also present average rellocation eﬀects based upon the speciﬁcations in columns
3–4 of Table 3. These ARE estimates are larger, albeit less precisely determined.

28In contrast, the coeﬃcients on interactions of the teacher FFT dummy variables with the peer composition

variables are imprecisely determined.

25

Table 3: IV regression results of the 3 × 3 model. Dependent variables: student test score outcomes

(2)
(1)
A. Only teacher
eﬀects

(4)
(3)
B. Full model

(5)

(6)

C. Without
teacher × peer
interactions

Math

ELA

Math

ELA

Math

ELA

0.069
(0.053)
0.038
(0.067)

-0.029
(0.048)
-0.058
(0.065)

δ

FFT middle

FFT high

η

FFT middle

× baseline middle

× baseline high

FFT high

× baseline middle

× baseline high

λ

FFT middle

× fraction peers middle

× fraction peers high

FFT high

× fraction peers middle

× fraction peers high

0.027
(0.060)
-0.155
(0.103)

0.052
(0.060)
0.050
(0.076)

0.196**
(0.084)
0.265**
(0.100)

-0.082
(0.059)
-0.154*
(0.083)

0.067
(0.063)
0.097
(0.082)

0.127*
(0.072)
0.149
(0.095)

-0.145
(0.138)
-0.547
(0.380)

0.040
(0.060)
0.015
(0.084)

0.184**
(0.082)
0.226**
(0.099)

0.318
(0.299)
0.239
(0.205)

0.641
(0.513)
0.460
(0.409)

-0.219
(0.173)
-0.137
(0.203)

0.057
(0.067)
0.076
(0.081)

0.150**
(0.074)
0.187**
(0.092)

0.288
(0.401)
0.161
(0.288)

0.227
(0.389)
-0.355
(0.343)

β

Baseline middle

Baseline high

R2
N

0.888***
(0.077)
1.622***
(0.103)

0.739***
(0.084)
1.555***
(0.105)

0.843***
(0.092)
1.578***
(0.124)

0.699***
(0.096)
1.503***
(0.118)

0.840***
(0.092)
1.550***
(0.119)

0.682***
(0.094)
1.470***
(0.115)

0.617
8,534

0.554
9,641

0.616
8,534

0.552
9,641

0.617
8,534

0.555
9,641

Note: The dependent variables are subject-speciﬁc test score outcomes in math and ELA. The instrumental
variables are based on assigned teacher FFT (Panels A–C) and assigned peer baseline test scores (Panel
B). All regressions control for the h(x, ¯x) function (see Section 2) and for randomization block ﬁxed eﬀects.
Analytic standard errors, clustered by randomization block, are in parentheses.
*** signiﬁcant at the 1%-level ** signiﬁcant at the 5%-level * signiﬁcant at the 10%-level.

26

Characterization of an optimal reassignment

If an optimal assignment is similar to the status quo one, then large reallocation eﬀects
are unlikely. Therefore, before quantifying any reallocation eﬀects, we ﬁrst discuss how an
optimal assignment and a worst assignment diﬀer from the status quo.

Our primary reassignment policy considers reassignments of teachers across classrooms
district-wide (that is, we allow teachers to move to a diﬀerent school within their district).
We do restrict teachers to teach at the same level (e.g., elementary school teachers do not
move to middle school classrooms). Under this scenario we ﬁnd that, when moving from the
status quo to an optimal assignment, 49 percent of the students in the math sample, and 47
percent of the students in the ELA sample, are assigned to a new teacher. The balance of
students remain with their status quo teacher.

It is interesting to examine how the reallocation changes the “assortativeness” of the
assignment. An assignment is characterized as positive assortative if students with higher
baseline test scores are more frequently matched with higher-FFT teachers, compared to a
random assignment. Indeed, we observe this in our data for the optimal assignment. Figure 1
displays, for each level of FFT, the distribution of student baseline test scores in the average
class a teacher of that FFT level is assigned to. In both the math and in the ELA sample,
the optimal allocation is more assortative than the status quo. In the status quo in math,
for instance, a high-FFT teacher has on average 23 percent of students with low baseline test
scores and 40 percent of students with high baseline test scores. In the optimal allocation,
the fraction of students with low baseline test scores drops to 9 percent, and the fraction
of students with high baseline test scores increases to 61 percent on average. The optimal
allocation in the ELA sample displays a similar pattern. The positive assortativeness arises
in the optimal allocation because students with high baseline test scores beneﬁt more from
a high-FFT teacher, compared to students with low baseline test scores (see Table 3). The
worst allocation displays negative assortativeness (see Appendix Figure A.2).

Average reallocation eﬀects

Tables 4 and 5 present ARE estimates. In Panel A an optimal assignment is compared with
the status quo; while in Panel B optimal assignments are compared with “worst” assignments.
We presents results for all students (Panels A.I and B.I), as well as for just those students
who experience a teacher change as part of the reallocation (Panels A.II and B.II).

In the math sample (Table 4), the optimal allocation improves average test scores by 1.7
percent of a test score standard deviation compared to the status quo. This eﬀect is precisely
determined with a Bayesian bootstrap standard error of 0.6 percent. The reported eﬀects are
largely driven by students with high baseline test scores. These students gain 2.8 percent of
a test score standard deviation on average; students with middle and low baseline test scores,

27

Figure 1: Assortativeness of the optimal allocation in comparison with the status quo

Note: The ﬁgure compares the status quo allocation (Panels 1 and 3) with the optimal allocation (Panels 2
and 4). The bars represent the fractions of students with low, middle, and high baseline test scores that a
teacher with low, middle, and high FFT is assigned to on average under each of the allocations. For each
teacher type, the fractions add up to 1. The optimization is carried out within school types (elementary or
middle school) and districts.

28

0.330.360.300.270.360.370.230.370.400.2.4.6Fraction of studentsFFT lowFFT middleFFT high(1) Math: status quo0.410.400.190.330.390.280.090.300.610.2.4.6Fraction of studentsFFT lowFFT middleFFT high(2) Math: optimal allocation0.290.380.340.270.350.390.220.360.430.2.4.6Fraction of studentsFFT lowFFT middleFFT high(3) ELA: status quo0.590.340.060.240.370.400.130.310.570.2.4.6Fraction of studentsFFT lowFFT middleFFT high(4) ELA: optimal allocationBaseline lowBaseline middleBaseline highin contrast, gain only 1.2 and 1.4 percent of a test score standard deviation, respectively (on
average).

Since only half of the students experience a change in their teacher, the average eﬀect
represents an equal-weighted mixture of a zero eﬀect and a positive eﬀect on those students
who do experience a change in teachers. The average eﬀect for the latter group is 3.6 percent
of a test score standard deviation (Panel A.II, SE = 1.2 percent).

Table 4: Average reallocation gains in math. Gains expressed in test score standard deviations

(1)

(2)

(3)

(4)

(5)

(6)

(7)

(8)

Panel A. Optimal versus status quo

A.I Full sample

A.II Conditional on being reallocated

all students
0.017
(0.006)
8,534

high
0.028
(0.014)
2,332

middle
0.012
(0.008)
3,108

low
0.014
(0.011)
3,094

Gain
SE
N

all students
0.036
(0.012)
4,107

high
0.059
(0.029)
1,121

middle
0.028
(0.019)
1,380

low
0.026
(0.019)
1,606

Panel B. Optimal versus worst allocation

B.I Full sample

B.II Conditional on being reallocated

all students
0.040
(0.012)
8,534

high
0.072
(0.034)
2,332

middle
0.019
(0.015)
3,108

low
0.038
(0.026)
3,094

Gain
SE
N

all students
0.060
(0.019)
5,746

high
0.103
(0.048)
1,626

middle
0.032
(0.022)
1,912

low
0.053
(0.037)
2,208

Note: The table shows the average reallocation gains from implementing the optimal assignment instead
of the random assignment (status quo) in Panel A, and the average reallocation gains from implementing
the optimal assignment instead of the worst assignment in Panel B. The gains are expressed in test score
standard deviations. The computations are based on a 3 × 3 model without teacher-by-peer interactions
(see Table 3, column 5). The results are presented separately for the full sample of students (columns 1–4),
and for the sample of classrooms that would get a new teacher as a result of the reallocation (columns 5–8).
The reassignments are carried out within school types and districts. Standard errors are in parentheses and
computed using the Bayesian bootstrap with 1,000 replications (see Section 4). High/middle/low: students
in the top/middle/bottom tercile of the baseline test score distribution.

The comparison of an optimal allocation with a worst allocation yields improvements that
are about twice as large. Relative to a worst allocation, an optimal allocation improves test
scores by 4.0 percent of a standard deviation on average (SE = 1.2 percent). The gains are
7.2 percent of a standard deviation for students with high baseline test scores, and 1.9 and
3.8 percent of a standard deviation for students with middle and low baseline test scores,
respectively. If one considers only those students who are reassigned to a new teacher, the
reallocation eﬀect amounts 6.0 percent of a standard deviation on average (Panel B.II, SE =
1.9 percent).

One way to benchmark the magnitude of these AREs is to compare them with the eﬀects
of hypothetical policies aimed at improving teacher value-added measures (VAMs). As noted

29

in the introduction, such policies are controversial, as is the evidence marshalled to support
them. Here we oﬀer no commentary on the advisability of actually adopting VAM-guided
teacher personnel policies; nor do we oﬀer an assessment of VAM studies. Rather we simply
use these studies, and the policy thought experiments they motivate, to benchmark our ARE
ﬁndings.

Teacher value-added is typically conceptualized as an invariant intercept-shifter, which
uniformly raises or lowers the achievement of all students in a classroom. In this framework
replacing a low value-added teacher with a high one will raise achievement for all students
in a classroom. Rockoﬀ (2004) estimates that the standard deviation of the population
distribution of teacher value-added (in a New Jersey school district) is around 0.10 test
score standard deviations in both math and reading. Recent studies ﬁnd somewhat higher
estimates: Chetty et al. (2014a) estimates that the standard deviation of teacher value-added
is 0.16 in math and 0.12 in reading; similarly, Rothstein (2017) ﬁnds values of 0.19 in math
and 0.12 in reading.

Using a standard deviation of 0.15 we can consider the eﬀect of a policy which removes
the bottom τ × 100 percent of teachers, sorted by VAM, and replaces them with teachers
at the ˜τ th quantile of the VAM distribution. Under normality the eﬀect of such a policy on
average student achievement is to increase test scores by

(1 − τ ) σ

(cid:1)
φ (cid:0) qτ
1 − Φ (cid:0) qτ

σ

σ

(cid:1) + σΦ−1 (˜τ )

standard deviations. Setting τ = 0.05 and ˜τ = 0.75 this expression gives an estimate of the
policy eﬀect of 0.021 (i.e., 2.1 percent of a test score standard deviation). This is comparable
to the average eﬀect on math achievement associated with moving from the status quo MET
assignment to an optimal one. In practice correctly identifying, and removing from class-
rooms, the bottom ﬁve percent of teachers would be diﬃcult to do. Replacing them with
teachers in the top quartile of the VAM distribution perhaps even more so. Contextualized
in this way the AREs we ﬁnd are large.

An attractive feature of the policies we consider is that they are based on measurable
student and teacher attributes, not noisily measured latent ones. At the same time we are
mindful that most school districts would not ﬁnd it costless to reallocate teachers freely across
classrooms and schools.

Another way to calibrate the size of the eﬀects we ﬁnd is as follows. We ﬁnd that when
implementing an optimal teacher-to-classroom assignment, only about one half of students
experience a change in teachers. We ﬁnd that test scores increase by about 3.6 percent of a
standard deviation for these students. This is comparable to increasing the average VAM of
these students’ teachers from zero (i.e., the median) to the 0.6 quantile of the teacher VAM
distribution. Again, increasing the VAM of half of all teachers by such an amount may be

30

diﬃcult to do in practice.29

For English language arts (ELA) achievement we ﬁnd smaller reallocation eﬀects. Mov-
ing from the status quo to an optimal allocation is estimated to raise achievement by 0.8
percent of a test score standard deviation (SE = 0.6 percent). As with math, these gains
are concentrated among students with high baseline scores who are assigned a new teacher.
These students experience an average gain of 5 percent of a test score standard deviation
(SE = 2.5 percent).

Table 5: Average reallocation gains in ELA. Gains expressed in test score standard deviations.

(1)

(2)

(3)

(4)

(5)

(6)

(7)

(8)

Panel A. Optimal versus status quo

A.I Full sample

A.II Conditional on being reallocated

all students
0.008
(0.006)
9,641

high
0.024
(0.011)
2,480

middle
0.002
(0.007)
3,402

low
0.004
(0.010)
3,759

Gain
SE
N

all students
0.017
(0.012)
4,529

high
0.050
(0.025)
1,167

middle
0.004
(0.013)
1,627

low
0.008
(0.020)
1,735

Panel B. Optimal versus worst allocation

B.I Full sample

B.II Conditional on being reallocated

all students
0.018
(0.010)
9,641

high
0.057
(0.028)
2,480

middle
0.003
(0.011)
3,402

low
0.005
(0.019)
3,759

Gain
SE
N

all students
0.025
(0.015)
6,675

high
0.080
(0.038)
1,770

middle
0.004
(0.017)
2,229

low
0.007
(0.024)
2,676

Note: The table shows the average reallocation gains from implementing the optimal assignment instead
of the random assignment (status quo) in Panel A, and the average reallocation gains from implementing
the optimal assignment instead of the worst assignment in Panel B. The gains are expressed in test score
standard deviations. The computations are based on a 3 × 3 model without teacher-by-peer interactions
(see Table 3, column 6). The results are presented separately for the full sample of students (columns 1–4),
and for the sample of classrooms that would get a new teacher as a result of the reallocation (columns 5–8).
The reassignments are carried out within school types and districts. Standard errors are in parentheses and
computed using the Bayesian bootstrap with 1,000 replications (see Section 4). High/middle/low: students
in the top/middle/bottom tercile of the baseline test score distribution.

In sum implementing an optimal assignment generates improvements in test score out-
comes across the distribution of baseline achievement. Yet, while the gains are large for
students with high baseline test scores – up to 10 percent of a standard deviation for stu-
dents with high baseline test scores in math – for students with middle and low baseline test
scores, the gains are smaller. Our results suggest that reallocations matter more in math,

29One may also use eﬀect sizes of educational interventions in general as point of comparison. Based on a
meta-study by Kraft (forthcoming), an eﬀect size of 4 percent of a standard deviation in math qualiﬁes as a
“medium eﬀect,” corresponding to the 40th percentile of eﬀect sizes found across 750 educational interventions
in pre-K–12 settings in the US (see Table 1 in Kraft, forthcoming).

31

compared to ELA. This is in line with the value-added literature, which consistently reports
higher value-added in math compared to ELA.

Our results provide strong evidence of complementarity between student baseline test
scores and teacher practices. This complementarity, if taken into account when assigning
teachers to classrooms, appears to make a diﬀerence in students’ performance across the
distribution of baseline achievement. Teacher reassignments that maximize the average test
score, however, will not close the achievement gap between students with low versus high
baseline achievement levels. They actually widen this achievement gap. A diﬀerent objective
function, one that values equity as well as “eﬃciency,” may be preferred in practice.

It is also possible that assignments based on other student/teacher attributes might both
raise average achievement and narrow achievement gaps. For example, if literacy in a stu-
dent’s native language helps develop literacy in English, then perhaps bilingual teachers could
help to raise the achievement levels of English language learners. Many other examples might
merit exploration.

6 Sensitivity checks

Teacher and student categories

Working with a discrete categorization of baseline achievement and teacher FFT provides a
simple, but also highly ﬂexible, way of capturing non-linearities in educational production.
We tested the sensitivity of our results to alternative categorizations of teachers and students.
Speciﬁcally, we test (a) a coarser speciﬁcation with two levels of teacher FFT (cutoﬀ point
at 2.5) and two levels of student baseline test scores and (b) a ﬁner speciﬁcation with four
levels of teacher FFT (cutoﬀ points at 2.25, 2.5, and 2.75) and four levels of student baseline
test scores.

In the coarser speciﬁcation, we do not detect any signiﬁcant match eﬀects between teacher
FFT and student baseline test scores (see Appendix Table A.10): the model with only two
levels of teacher FFT does not capture the positive eﬀect of the high-FFT teachers on students
with middle or high baseline test scores. By contrast, the results of the ﬁner speciﬁcation,
with four levels of FFT, are very similar to the results of our preferred speciﬁcation (see
Appendix Table A.11).

Measure of teaching practices

We also assess the sensitivity of our results to the measure of teaching practices that we use
in the analysis. The MET data also contains an alternative measure, the CLASS (Classroom
Assessment Scoring System), for a subset of the sample (6,320 observations in the math
sample and 6,999 observations in the ELA sample). The CLASS is a teacher observation

32

protocol that uses diﬀerent domains and evaluation criteria than the FFT (see Section B.4 in
the Data Appendix for details on the observation protocol and on how we process the data).
The CLASS is also widely used in research on teacher quality (e.g. Araujo et al., 2016). We
ﬁnd that our results are similar when using the CLASS instead of the FFT (see Appendix
Table A.12).

Restrictions on the reassignment process

In our preferred optimization procedure, we optimize the assignment of teachers to classrooms
within types of schools (elementary or middle school) and school districts. We choose these
restrictions because teachers might not be willing or able to teach in a diﬀerent school type
or a diﬀerent school district. As a sensitivity check, we also calculated the results of both a
more restrictive and a less restrictive optimization procedure.

Our least restrictive allocation optimizes the assignment within school types, but allows
for reassignments across districts. The magnitudes of the reallocation eﬀects in this case
are only slightly larger than those where reallocations are within districts (see Appendix
Tables A.13 and A.14, columns 1–4). This ﬁnding may reﬂect the similarity of the distribu-
tions of baseline student achievement and teacher FFT in MET school districts. It would
be interesting to repeat our analysis in a metro area consisting of an urban core district and
multiple suburban ones. In such a setting is it seems plausible that moving teachers across
school districts might raise average achievement.

Our most restrictive allocation reassigns teachers only within school-grade-subject cells
(i.e., randomization blocks). This restriction is strong because each randomization block
typically contains only two or three sections. Under this restriction, the reallocation gains
are overall negligible (see Appendix Tables A.13 and A.14, columns 5–8).

Accounting for teacher-peer match eﬀects in the assignment

Our main results for the reallocation eﬀects are based on a model without teacher-to-peer
match eﬀects (i.e., in a model which imposes the restriction that λ = 0), since these eﬀects
are very noisily estimated using our data. Using a model which does not impose this restric-
tion generates reallocation eﬀects about twice as large. These eﬀects are also more noisily
measured (see Appendix Tables A.15 and A.16).

7 Conclusion

We provide an econometric framework that allows us to semiparametrically characterize
complementarity between teaching practices and student baseline test scores. Our framework

33

exploits the random assignment of teachers to classrooms available in the MET dataset, while
formally dealing with non-compliance by both teachers and students.

Our results show that the potential gains associated with an outcome-maximizing assign-
ment of teachers to classrooms are large. They are comparable to fairly large (hypothetical)
interventions to raise teacher VAM. An attractive feature is that they are, at least in theory,
resource neutral. No new teachers are required to implement the policies we consider.

Our focus on the objective of maximizing the population average test score leads to an
optimal assignment that increases the gap between less and more prepared students. We
could also consider objective functions that do no focus on average test scores, but instead
on test score gaps or proﬁciency levels. We could, for instance, choose an assignment which
maximizes the number of students who reach a “proﬁcient” level on their end-of-year assess-
ment. It is possible that this objective function would suggest a less assortative assignment:
students with high baseline scores would likely reach the proﬁcient level regardless of their
teacher’s FFT, while the incremental eﬀect of having a high-FFT teacher on the probability
of attaining proﬁciency may be larger for students with lower baseline test scores. Outcomes
other than math and ELA achievement (e.g., socio-emotional skills) may also be of interest.
We consider this paper as a ﬁrst pass that establishes the feasibility of recovering match
eﬀects from imperfect experimental data and shows that the resulting reallocation eﬀects
that depend upon these match eﬀects and the supply of teachers are substantial.

34

References

Ai, C. and X. Chen (2003): “Eﬃcient Estimation of Models with Conditional Moment

Restrictions Containing Unknown Functions,” Econometrica, 71, 1795 – 1843.

Araujo, M. C., P. Carneiro, Y. Cruz-Aguayo, and N. Schady (2016): “Teacher
quality and learning outcomes in kindergarten,” The Quarterly Journal of Economics, 131,
1415 – 1453.

Aucejo, E., P. Coate, J. C. Fruehwirth, S. Kelly, and Z. Mozenter (2019):

“Teacher Eﬀectiveness and Classroom Composition,” Manuscript.

Bhattacharya, D. (2009): “Inferring optimal peer assignment from experimental data,”

Journal of the American Statistical Association, 104, 486 – 500.

Chamberlain, G. and G. W. Imbens (2003): “Nonparametric applications of Bayesian

inference,” Journal of Business and Economic Statistics, 21, 12 – 18.

Chen, X., O. Linton, and I. Van Keilegom (2003): “Estimation of semiparametric

models when the criterion function is not smooth,” Econometrica, 71, 1591 – 1608.

Chetty, R., J. N. Friedman, and J. E. Rockoff (2012): “Great Teaching,” Education

Next, 12, 59 – 64.

——— (2014a): “Measuring the Impacts of Teachers I: Evaluating Bias in Teacher Value-

Added Estimates,” American Economic Review, 104, 2593 – 2632.

——— (2014b): “Measuring the Impacts of Teachers II: Teacher Value-Added and Student

Outcomes in Adulthood,” American Economic Review, 104, 2633 – 2679.

Cohen-Vogel, L., L. Feng, and L. Osborne-Lampkin (2013): “Seniority provisions
in collective bargaining agreements and the ‘teacher quality gap’,” Educational Evaluation
and Policy Analysis, 35, 324 – 343.

Danielson, C. (2011): “The Framework for Teaching Evaluation Instrument, 2011 Edi-
from https://danielsongroup.org/downloads/

tion,” Retrieved on May 31, 2020,
2011-framework-teaching-evaluation-instrument.

Darling-Hammond, L. (2015): “Can Value Added Add Value to Teacher Evaluation?”

Educational Researcher, 44, 132 – 137.

Dee, T. S. (2004): “Teachers, race, and student achievement in a randomized experiment,”

Review of Economics and Statistics, 86, 195 – 210.

35

——— (2005): “A Teacher like Me: Does Race, Ethnicity, or Gender Matter?” The American

Economic Review, 95, 158 – 165.

Garrett, R. and M. P. Steinberg (2015): “Examining Teacher Eﬀectiveness Using
Classroom Observation Scores: Evidence From the Randomization of Teachers to Stu-
dents,” Educational Evaluation and Policy Analysis, 37, 224 – 242.

Graham, B. (2008): “Identifying social interactions through conditional variance restric-

tions,” Econometrica, 76, 643 – 660.

Graham, B. S. (2011): “Econometric methods for the analysis of assignment problems in
the presence of complementarity and social spillovers,” in Handbook of social economics,
ed. by J. Benhabib, A. Bisin, and M. O. Jackson, Amsterdam: North-Holland, vol. 1B,
965 – 1052.

Graham, B. S., G. W. Imbens, and G. Ridder (2007): “Redistributive eﬀects for
discretely-valued inputs,” IEPR Working Paper 07.7, University of Southern California.

——— (2010): “Measuring the eﬀects of segregation in the presence of social spillovers: a

nonparametric approach,” Working Paper 16499, NBER.

——— (2014): “Complementarity and aggregate implications of assortative matching: a

nonparametric analysis,” Quantitative Economics, 5, 29 – 66.

——— (2020): “Identiﬁcation and Eﬃciency Bounds for the Average Match Function Under
Conditionally Exogenous Matching,” Journal of Business & Economic Statistics, 38, 303
– 316.

Grissom, J. A., D. Kalogrides, and S. Loeb (2015): “The micropolitics of educational
inequality: The case of teacher–student assignments,” Peabody Journal of Education, 90,
601 – 614.

Hansen, B. E. (2020): “Econometrics,” Retrieved on July 4, 2020, from https://www.

ssc.wisc.edu/~bhansen/econometrics/.

Hanushek, E. (1971): “Teacher Characteristics and Gains in Student Achievement: Esti-

mation Using Micro Data.” American Economic Review, 61, 280 – 288.

Hanushek, E. A., J. F. Kain, and S. G. Rivkin (2004): “Disruption versus Tiebout
improvement: The costs and beneﬁts of switching schools,” Journal of Public Economics,
88, 1721 – 1746.

Hsieh, Y.-W., X. Shi, and M. Shum (2018): “Inference on estimators deﬁned by math-

ematical programming,” Manuscript.

36

Kalogrides, D., S. Loeb, and T. Beteille (2011): “Power Play? Teacher Char-
acteristics and Class Assignments,” Working Paper 59, National Center for Analysis of
Longitudinal Data in Education Research.

Kane, Thomas, J., D. F. McCaffrey, T. Miller, and D. O. Staiger (2013):
“Have we identiﬁed eﬀective teachers? Validating measures of eﬀect teaching using random
assignment,” Met project research paper, Bill & Melinda Gates Foundation.

Kraft, M. A. (forthcoming): “Interpreting Eﬀect Sizes of Education Interventions,” Edu-

cational Researcher.

Loeb, S., J. Soland, and L. Fox (2014): “Is a Good Teacher a Good Teacher for
All? Comparing Value-Added of Teachers with Their English Learners and Non-English
Learners,” Educational Evaluation and Policy Analysis, 36, 457 – 475.

Manski, C. F. (1993): “Identiﬁcation of endogenous social eﬀects: the reﬂection problem,”

Review of Economic Studies, 60, 531 – 542.

McFarland, J. et al. (2019): “The Condition of Education 2019,” NCES 2019-144,

National Center for Education Statistics.

Morganstein, D. and R. Wasserstein (2014): “ASA Statement on Value-Added Mod-

els,” Statistics and Public Policy, 1, 108 – 110.

Robinson, P. M. (1988): “Root-N-Consistent Semiparametric Regression,” Econometrica,

56, 931 – 954.

Rockoff, J. E. (2004): “The Impact of Individual Teachers on Student Achievement:

Evidence from Panel Data,” American Economic Review, 94, 247 – 252.

Rothstein, J. (2010): “Teacher Quality in Educational Production: Tracking, Decay, and

Student Achievement 2010,” Quarterly Journal of Economics, 125, 175 – 214.

——— (2017): “Measuring the impacts of teachers: Comment,” American Economic Review,

107, 1656–84.

Sanderson, E. and F. Windmeijer (2016): “A weak instrument F-test in linear IV
models with multiple endogenous variables,” Journal of Econometrics, 190, 212 – 221.

Snyder, T. D. et al. (2017): “Digest of Education Statistics,” NCES 2018-070, National

Center for Education Statistics.

Stock, J. H. and M. Yogo (2005): “Testing for weak instruments in Linear IV regres-
sion,” in Identiﬁcation and Inference for Econometric Models: Essays in Honor of Thomas
Rothenberg, Cambridge University Press, 80 – 108.

37

White, M., B. Rowen, G. Alter, L. Blankenship, C. Greene, and S. Windish

(2019): User Guide to Measures of Eﬀective Teaching Longitudinal Database (MET LDB),
Ann Arbor: Inter-University Consortium for Political and Social Research, The University
of Michigan.

38

A Figures and Tables

Figure A.1: Distribution of teacher FFT and student baseline test scores

Note: Distribution of teacher FFT (Panels 1 and 3) and student baseline test scores (Panels 2 and 4) in the
estimation sample. Student baseline test scores are section averages.

39

0.05.1.15.2Fraction of teachers1.522.533.5FFTNumber of teachers = 614, mean = 2.52(1) FFT - math teachers0.05.1.15Fraction of sections-2-1012Baseline test scores (section averages)Number of sections = 792, mean = .02(2) Math baseline test scores0.05.1.15.2Fraction of teachers1.522.533.5FFTNumber of teachers = 649, mean = 2.58(3) FFT - ELA teachers0.05.1.15Fraction of sections-2-1012Baseline test scores (section averages)Number of sections = 796, mean = .07(4) ELA baseline test scoresFigure A.2: Assortativeness of the worst allocation in comparison with the status quo

Note: The ﬁgure compares the status quo allocation (Panels 1 and 3) with the worst allocation (Panels 2
and 4). The bars represent the fractions of students with low, middle, and high baseline test scores that a
teacher with low, middle, and high FFT is assigned to on average under each of the allocations. For each
teacher type, the fractions add up to 1. The optimization is carried out within school types (elementary or
middle school) and districts.

40

0.330.360.300.270.360.370.230.370.400.2.4.6Fraction of studentsFFT lowFFT middleFFT high(1) Math: status quo0.180.320.500.220.400.380.570.330.100.2.4.6Fraction of studentsFFT lowFFT middleFFT high(2) Math: worst allocation0.290.380.340.270.350.390.220.360.430.2.4.6Fraction of studentsFFT lowFFT middleFFT high(3) ELA: status quo0.070.280.650.260.370.370.370.370.260.2.4.6Fraction of studentsFFT lowFFT middleFFT high(4) ELA: worst allocationBaseline lowBaseline middleBaseline highTable A.1: Summary statistics of the estimation samples

Student characteristics

Age
Fourth grade
Fifth grade
Sixth grade
Seventh grade
Eighth grade
Male
Gifted
Special education
ELL
Free/reduced-price lunch
White
Black
Hispanic
Asian
Other race
Subject test score 2010 (baseline)
Subject test score 2011 (outcome)

Teacher characteristics

Male
White
Black
Hispanic
Other race
Years in district
Master’s or higher
FFT

Classroom characterisics

Class size
Sample size
Students
Teachers
Schools

(1)

(2)

(3)

(4)

Math sample

ELA sample

Mean

SD

Mean

SD

10.32
26%
35%
16%
10%
12%
49%
6%
7%
15%
58%
27%
31%
31%
8%
3%
0.11
0.15

14%
65%
28%
6%
2%
7.39
40%
2.53

(1.56)
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
(0.89)
(0.90)

-
-
-
-
-
(6.73)
-
(0.30)

10.28
26%
35%
15%
12%
12%
49%
11%
7%
13%
57%
28%
30%
33%
7%
3%
0.17
0.17

11%
64%
29%
6%
1%
7.36
37%
2.59

(1.48)
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
(0.93)
(0.91)

-
-
-
-
-
(6.33)
-
(0.30)

25.54

(5.48)

25.60

(5.44)

8,534
614
153

9,641
649
160

Note: Summary statistics of the estimation samples. Standard deviations are in parentheses. ELL: English
language learner. FFT: Framework for teaching. For details on the sample construction, see Appendix B.

41

Table A.2: Balancing tests. Dependent variable: FFT of the assigned teacher

(1)

(2)

(3)

(4)

Math

ELA

Baseline test score
Age
Male
Gifted
Special education
ELL
Black
Hispanic
Asian
Other race
Free/reduced-price lunch

coeﬀ
0.004
-0.003
0.001
-0.015
0.013
-0.004
-0.004
-0.013
0.009
0.014
0.005

SE
(0.005)
(0.006)
(0.005)
(0.015)
(0.012)
(0.011)
(0.008)
(0.009)
(0.014)
(0.012)
(0.010)

joint

for

F-test
cance (p-value)
R2
N

signiﬁ-

0.465

0.598
8,518

coeﬀ
0.000
-0.006
-0.002
0.010
-0.002
-0.014
-0.010
0.002
0.016
0.010
-0.002

SE
(0.004)
(0.005)
(0.003)
(0.016)
(0.009)
(0.011)
(0.007)
(0.006)
(0.011)
(0.010)
(0.006)

0.241

0.635
9,630

Note: The table presents results of OLS regressions of the FFT of the assigned teacher on individual student
characteristics. Free/reduced-price lunch eligibility is coded as 0 for students who do not have information on
this variable in the dataset. All regressions control for randomization block ﬁxed eﬀects. Analytic standard
errors, clustered by randomization block, are in parentheses. ELL: English language learner. FFT: Framework
for teaching.

42

Table A.3: Tests of Assumption 1. Dependent variables: Diﬀerences between assigned and realized

teacher characteristics

(1)

(2)

(3)

(4)

(5)

(6)

Panel I. Math sample

Male

0.009
(0.060)
0.003
(0.005)
0.027
(0.076)

Teacher’s race
black

hispanic

Experience Master’s
degree

(years)

white

-0.039
(0.107)
-0.003
(0.005)
0.037
(0.058)

-0.061
(0.068)
0.005
(0.004)
0.007
(0.034)

0.059
(0.059)
-0.002
(0.002)
-0.037
(0.041)

-2.727
(2.236)
0.071
(0.082)
0.738
(0.931)

-0.018
(0.099)
-0.006
(0.006)
-0.052
(0.089)

0.956

0.799

0.537

0.731

0.635

0.849

0.149
8,163

0.176
8,098

0.108
7,689

0.039
7,689

0.163
5,585

0.154
6,083

Panel II. ELA sample

Male

0.008
(0.067)
0.003
(0.002)
-0.008
(0.027)

Teacher’s race
black

hispanic

Experience Master’s
degree

(years)

white

0.036
(0.085)
-0.000
(0.004)
0.102**
(0.046)

-0.037
(0.071)
0.002
(0.003)
-0.041
(0.031)

0.031
(0.028)
-0.002
(0.002)
-0.056*
(0.029)

0.500
(0.799)
-0.008
(0.042)
0.220
(0.436)

-0.003
(0.060)
0.001
(0.004)
0.049
(0.049)

0.300

0.116

0.414

0.063

0.837

0.779

0.090
9,183

0.130
9,183

0.053
9,183

0.050
9,183

0.142
6,791

0.111
6,963

FFT, assigned teacher

Baseline test score

Avg. peer baseline test score,
assigned peers

F-test for joint signiﬁcance
(p-value)
R2
N

FFT, assigned teacher

Baseline test score

Avg. peer baseline test score,
assigned peers

F-test for joint signiﬁcance
(p-value)
R2
N

Note: The table presents results of OLS regressions of diﬀerences between assigned and realized teacher
characteristics on individual baseline test scores, the FFT of the assigned teacher, and the average baseline
test scores of the assigned peers. Each column represents a regression with a diﬀerent dependent variable. All
regressions control for randomization block ﬁxed eﬀects. Analytic standard errors, clustered by randomization
block, are in parentheses. Teachers with ethnicity ”white” and ethnicity ”other” are pooled into one category.
FFT: Framework for teaching.
**signiﬁcant at the 5%-level, *signiﬁcant at the 10%-level.

43

Table A.4: Tests of Assumption 2 in the math sample. Dependent variables: diﬀerences between assigned and realized peer characteristics

Diﬀerence between assigned

and realized peer characteristics

FFT, assigned teacher

Baseline test score

Avg. peer baseline test score,
assigned peers

R2
N

(1)

Age

(2)

(3)

(4)

Male

Gifted

Special

(5)

ELL

(6)

FRL

(7)

(8)

(9)

(10)

Race/ethnicity

education

White

Black Hispanic Asian

0.008
(0.019)
-0.006
(0.003)
-0.030*
(0.017)

0.124
8,525

0.015
(0.014)
0.000
(0.001)
-0.006
(0.013)

0.264
8,534

0.006
(0.009)
0.004**
(0.002)
-0.013
(0.009)

0.222
8,534

-0.017
(0.011)
-0.001
(0.001)
0.009
(0.008)

0.240
8,527

0.006
(0.012)
0.000
(0.001)
0.033**
(0.011)

0.200
8,534

-0.005
(0.016)
0.000
(0.002)
-0.001
(0.014)

0.241
5,976

0.004
(0.011)
0.001
(0.001)
-0.012
(0.010)

0.207
8,534

-0.010
(0.011)
-0.002
(0.002)
0.012
(0.011)

0.232
8,534

0.006
(0.013)
0.000
(0.001)
-0.007
(0.011)

0.247
8,534

-0.001
(0.007)
0.001*
(0.001)
0.008
(0.005)

0.185
8,534

4
4

Note: The table presents results of OLS regressions of diﬀerences between assigned and realized peer characteristics on the FFT of the assigned teacher,
individual baseline test scores, and the average baseline test scores of the assigned peers. Each column represents a regression with a diﬀerent dependent
variable. All regressions control for randomization block ﬁxed eﬀects. Analytic standard errors, clustered by randomization block, are in parentheses. ELL:
English language learner, FRL: Free/reduced-price lunch eligible, FFT: Framework for teaching.
** signiﬁcant at the 5%-level * signiﬁcant at the 10%-level.

Table A.5: Tests of Assumption 2 in the ELA sample. Dependent variables: diﬀerences between assigned and realized peer characteristics

Diﬀerence between assigned

and realized peer characteristics

FFT, assigned teacher

Baseline test score

Avg. peer baseline test score,
assigned peers

R2
N

(1)

Age

(2)

(3)

(4)

Male

Gifted

Special

(5)

ELL

(6)

FRL

(7)

(8)

(9)

(10)

Race/ethnicity

education

White

Black Hispanic Asian

0.000
(0.020)
-0.005**
(0.003)
-0.009
(0.013)

0.098
9,635

0.008
(0.014)
-0.001
(0.001)
0.000
(0.011)

0.241
9,641

0.008
(0.009)
0.002
(0.001)
-0.029***
(0.007)

0.147
9,641

-0.012
(0.011)
0.000
(0.001)
0.013
(0.009)

0.208
9,636

0.022
(0.017)
-0.001
(0.001)
0.028**
(0.009)

0.178
9,641

-0.003
(0.014)
-0.002
(0.001)
0.007
(0.011)

0.238
7,270

-0.008
(0.012)
0.001
(0.001)
-0.002
(0.008)

0.203
9,641

-0.001
(0.013)
0.000
(0.001)
0.001
(0.008)

0.254
9,641

0.012
(0.014)
-0.001
(0.001)
0.001
(0.010)

0.218
9,641

-0.002
(0.009)
0.001
(0.000)
0.000
(0.006)

0.215
9,641

4
5

Note: The table presents results of OLS regressions of diﬀerences between assigned and realized peer characteristics on the FFT of the assigned teacher,
individual baseline test scores, and the average baseline test scores of the assigned peers. Each column represents a regression with a diﬀerent dependent
variable. All regressions control for randomization block ﬁxed eﬀects. Analytic standard errors, clustered by randomization block, are in parentheses. ELL:
English language learner, FRL: Free/reduced-price lunch eligible, FFT: Framework for teaching.
** signiﬁcant at the 5%-level * signiﬁcant at the 10%-level.

Table A.6: Tests of restriction (25). Dependent variables: average peer baseline test scores of

realized peers

(1)

(2)

Average peer baseline test score,
realized peers

Math sample

ELA sample

FFT, assigned teacher

Baseline test score

Avg. peer baseline test score,
assigned peers

R2
N

-0.015
(0.029)
0.031***
(0.008)
0.744
(0.034)

0.881
8,534

0.036
(0.029)
0.023***
(0.005)
0.769
(0.030)

0.904
9,641

Note: The table presents results of OLS regressions of the average peer baseline test scores of the realized
peers on individual baseline test scores, FFT of the assigned teacher, and the average peer baseline test scores
of the assigned peers in the math and ELA samples. All regressions control for randomization block ﬁxed
eﬀects. Analytic standard errors, clustered by randomization block, are in parentheses. FFT: Framework for
teaching.
*** signiﬁcant at the 1%-level.

46

Table A.7: Tests for instrument relevance

Panel A. Math

(1)

(2)

(3)

First-stage F-statistics

unconditional

SW

Variable

F(10,238) p-value

F(1, 238)

FFT middle
FFT high
FFT middle × baseline middle
FFT high × baseline middle
FFT middle × baseline high
FFT high × baseline high
FFT middle × fraction peers middle
FFT high × fraction peers middle
FFT middle × fraction peers high
FFT middle × fraction peers high

Panel B. ELA

17.57
11.62
82.49
33.30
39.59
36.92
17.47
12.85
8.07
12.01

(0.000)
(0.000)
(0.000)
(0.000)
(0.000)
(0.000)
(0.000)
(0.000)
(0.000)
(0.000)

60.17
57.95
498.96
387.27
322.43
378.59
113.27
102.70
182.12
137.55

First-stage F-statistics

unconditional

SW

Variable

F(10,238) p-value

F(1, 238)

FFT middle
FFT high
FFT middle × baseline middle
FFT high × baseline middle
FFT middle × baseline high
FFT high × baseline high
FFT middle × fraction peers middle
FFT high × fraction peers middle
FFT middle × fraction peers high
FFT middle × fraction peers high

26.21
22.34
146.86
121.72
128.34
88.44
25.06
21.57
13.9
13.21

(0.000)
(0.000)
(0.000)
(0.000)
(0.000)
(0.000)
(0.000)
(0.000)
(0.000)
(0.000)

75.61
102.87
836.59
1500.06
1226.48
1724.38
164.19
213.78
362.49
477.11

Note: The table presents weak instrument F-tests following Sanderson and Windmeijer (2016). Columns 1
and 2 present the unconditional F-statistics from ﬁrst-stage regressions of equation (23). Column 3 presents
the Sanderson-Windmeijer conditional F-statistics for each of the endogenous variables. The F-statistics in
column 3 need to be compared to the critical values of the Stock-Yogo weak identiﬁcation F-test (Stock and
Yogo, 2005). All of the F-statistics exceed the Stock-Yogo critical values for a 5% maximal IV relative bias
(critical value is 20.74).

47

Table A.8: OLS regression results of the linear model. Dependent variables: student test score

outcomes

(2)
(1)
A. Only teacher
eﬀects

Math
0.088**
(0.040)

ELA
0.118**
(0.045)

FFT

FFT × baseline

FFT × avg. peer baseline

Baseline

0.749***
(0.011)

0.690***
(0.011)

(3)
(4)
B. Full model

Math
0.081**
(0.040)
0.060*
(0.031)
-0.006
(0.066)
0.595***
(0.079)

ELA
0.116**
(0.045)
0.088**
(0.032)
-0.066
(0.081)
0.460***
(0.081)

(5)

(6)

C. Without
teacher × peer
interactions

Math
0.080**
(0.040)
0.059**
(0.028)

ELA
0.116**
(0.044)
0.078**
(0.030)

0.598***
(0.073)

0.487***
(0.078)

0.700
8,534

0.633
9,641

0.701
8,534

0.633
9,641

0.701
8,534

0.633
9,641

δ

η

λ

β

R2
N

Note: The dependent variables are subject-speciﬁc test score outcomes in math and ELA. The speciﬁcations
include linear terms for FFT, individual and peer baseline test scores. All regressions control for the h(x, x)
function (see Section 2) and for randomization block ﬁxed eﬀects. Analytic standard errors, clustered by
randomization block, are in parentheses. FFT: Framework for teaching.
*** signiﬁcant at the 1%-level ** signiﬁcant at the 5%-level * signiﬁcant at the 10%-level.

48

Table A.9: OLS regression results of the 3 × 3 model. Dependent variables: student test score

outcomes

(1)
(2)
A. Only teacher
eﬀects

(3)
(4)
B. Full model

(5)

(6)

C. Without
teacher × peer
interactions

Math

ELA

Math

ELA

Math

ELA

0.062*
(0.032)
0.066
(0.041)

0.028
(0.034)
0.048
(0.043)

δ

FFT middle

FFT high

η

FFT middle

× baseline middle

× baseline high

FFT high

× baseline middle

× baseline high

λ

FFT middle

× fraction peers middle

× fraction peers high

FFT high

× fraction peers middle

× fraction peers high

0.032
(0.038)
-0.016
(0.052)

0.033
(0.045)
0.053
(0.055)

0.098*
(0.059)
0.104
(0.067)

-0.127**
(0.045)
-0.141**
(0.054)

0.102**
(0.051)
0.137**
(0.062)

0.094
(0.059)
0.193**
(0.072)

0.032
(0.097)
-0.060
(0.113)

0.030
(0.047)
0.039
(0.060)

0.100*
(0.059)
0.120*
(0.068)

-0.024
(0.198)
-0.045
(0.187)

0.155
(0.224)
0.460
(0.409)

-0.058
(0.091)
-0.043
(0.125)

0.108**
(0.054)
0.136**
(0.067)

0.113*
(0.060)
0.225**
(0.075)

-0.204
(0.187)
-0.126
(0.154)

-0.187
(0.232)
-0.355
(0.343)

β

Baseline middle

Baseline high

R2
N

0.911***
(0.072)
1.678***
(0.105)

0.823***
(0.075)
1.636***
(0.101)

0.782***
(0.076)
1.551***
(0.111)

0.713***
(0.093)
1.443***
(0.113)

0.783***
(0.075)
1.544***
(0.106)

0.723***
(0.095)
1.454***
(0.116)

0.618
8,534

0.557
9,641

0.570
8,534

0.523
9,641

0.570
8,534

0.523
9,641

Note: The dependent variables are subject-speciﬁc test score outcomes in math and ELA. All regressions
control for the h(x, x) function (see Section 2) and for randomization block ﬁxed eﬀects. Analytic standard
errors, clustered by randomization block, are in parentheses. FFT: Framework for teaching.
*** signiﬁcant at the 1%-level ** signiﬁcant at the 5%-level * signiﬁcant at the 10%-level.

49

Table A.10: IV regression results of the 2 × 2 model. Dependent variables: student test score

outcomes

(1)
(2)
A. Only teacher
eﬀects

Math
0.072*
(0.038)

ELA
-0.060
(0.043)

FFT high

× baseline high

× fraction peers high

Baseline high

0.935***
(0.058)

0.927***
(0.054)

(3)
(4)
B. Full model

Math
0.088
(0.087)
0.039
(0.053)
-0.068
(0.149)
0.920***
(0.058)

ELA
0.170
(0.107)
-0.014
(0.055)
-0.422*
(0.229)
0.938***
(0.060)

(5)

(6)

C. Without
teacher × peer
interactions

Math
0.056
(0.047)
0.030
(0.052)

ELA
-0.027
(0.046)
-0.062
(0.055)

0.924***
(0.057)

0.956***
(0.060)

0.545
8,534

0.485
9,641

0.545
8,534

0.481
9,641

0.545
8,534

0.485
9,641

δ

η

λ

β

R2
N

Note: The dependent variables are subject-speciﬁc test score outcomes in math and ELA. The instrumental
variables are based on assigned teacher FFT (Panels A–C) and assigned peer baseline test scores (Panel B). All
regressions control for the h(x, x) function (see Section 2) and for randomization block ﬁxed eﬀects. Analytic
standard errors, clustered by randomization block, are in parentheses. FFT: Framework for teaching.
*** signiﬁcant at the 1%-level ** signiﬁcant at the 5%-level * signiﬁcant at the 10%-level.

50

Table A.11: IV regression results of the 4 × 4 model. Dependent variables: student test score

outcomes

(1)
(2)
A. Only teacher
eﬀects

(3)
(4)
B. Full model

(5)

(6)

C. Without
teacher × peer
interactions

δ

FFT lower middle

FFT upper middle

FFT high

η

FFT lower middle
× baseline lower middle

× baseline upper middle

× baseline high

FFT upper middle
× baseline lower middle

× baseline upper middle

× baseline high

FFT high
× baseline lower middle

× baseline upper middle

× baseline high

Math
0.146**
(0.063)
0.029
(0.059)
0.017
(0.069)

ELA
-0.037
(0.048)
0.013
(0.055)
-0.006
(0.060)

Math
0.029
(0.307)
-0.656*
(0.386)
-1.622**
(0.815)

0.111
(0.096)
0.028
(0.104)
0.053
(0.125)

0.101
(0.087)
0.001
(0.087)
0.110
(0.115)

0.269**
(0.120)
0.136
(0.111)
0.266**
(0.130)

ELA
0.097
(0.188)
-0.222
(0.268)
-0.044
(0.213)

0.084
(0.075)
0.023
(0.087)
0.028
(0.107)

0.103
(0.070)
0.141*
(0.084)
0.214**
(0.092)

0.144*
(0.075)
0.232**
(0.089)
0.251**
(0.097)

Math
0.114
(0.092)
-0.035
(0.078)
-0.162
(0.122)

0.083
(0.095)
-0.001
(0.107)
0.016
(0.124)

0.096
(0.078)
0.015
(0.083)
0.111
(0.100)

0.200*
(0.104)
0.140
(0.113)
0.253**
(0.124)

ELA
-0.056
(0.059)
-0.114
(0.072)
-0.132
(0.086)

0.074
(0.072)
0.007
(0.079)
0.002
(0.104)

0.103
(0.069)
0.142*
(0.080)
0.222**
(0.093)

0.118
(0.074)
0.181**
(0.085)
0.182*
(0.099)

β

Baseline lower middle

Baseline upper middle

Baseline high

λ
R2
N

0.635***
(0.131)
1.228***
(0.160)
1.913***
(0.172)

0.780***
(0.098)
1.273***
(0.116)
2.170***
(0.141)

0.649
8,534

0.592
9,641

0.517**
(0.195)
1.143***
(0.218)
1.750***
(0.246)
(cid:88)
0.638
8,534

0.747***
(0.105)
1.246***
(0.124)
2.121***
(0.148)
(cid:88)
0.589
9,641

0.563***
(0.156)
1.201***
(0.177)
1.835***
(0.201)

0.721***
(0.107)
1.203***
(0.127)
2.072***
(0.151)

0.648
8,534

0.593
9,641

Note: The dependent variables are subject-speciﬁc test score outcomes in math and ELA. The instrumental
variables are based on assigned teacher FFT (Panels A–C) and assigned peer basedline test scores (Panel B).
All regressions control for the h(x, x) function (see Section 2) and for randomization block ﬁxed eﬀects. The
results for the coeﬃcients on teacher-peer match eﬀects (Panel B) are omitted. Analytic standard errors,
clustered by randomization block, are in parentheses. FFT: framework for teaching.
*** signiﬁcant at the 1%-level ** signiﬁcant at the 5%-level * signiﬁcant at the 10%-level.

51

Table A.12: IV regression results of the 3 × 3 model with CLASS as measure of teaching practices.

Dependent variables: student test score outcomes

(2)
(1)
A. Only teacher
eﬀects

(4)
(3)
B. Full model

(5)

(6)

C. Without
teacher × peer
interactions

Math

ELA

Math

ELA

Math

ELA

0.039
(0.061)
-0.050
(0.079)

0.071
(0.067)
0.010
(0.088)

δ

CLASS middle

CLASS high

η

CLASS middle
× baseline middle

× baseline high

CLASS high
× baseline middle

× baseline high

λ

CLASS middle
× fraction peers middle

× fraction peers high

CLASS high
× fraction peers middle

× fraction peers high

0.008
(0.103)
-0.211*
(0.117)

0.074
(0.098)
0.019
(0.109)

0.198**
(0.100)
0.240**
(0.106)

0.038
(0.072)
-0.099
(0.107)

0.058
(0.067)
0.048
(0.091)

0.168**
(0.077)
0.137
(0.088)

-0.214
(0.489)
-0.521
(0.520)

0.075
(0.105)
0.008
(0.115)

0.201*
(0.103)
0.253**
(0.114)

0.451
(0.934)
0.153
(0.373)

0.741
(0.916)
0.036
(0.442)

0.115
(0.235)
0.085
(0.262)

0.054
(0.070)
0.040
(0.092)

0.177**
(0.076)
0.157*
(0.085)

-0.165
(0.473)
-0.040
(0.306)

-0.168
(0.375)
-0.324
(0.382)

β

Baseline middle

Baseline high

R2
N

0.929***
(0.095)
1.686***
(0.118)

0.870***
(0.087)
1.645***
(0.115)

0.819***
(0.148)
1.572***
(0.158)

0.798***
(0.100)
1.584***
(0.122)

0.820***
(0.143)
1.562***
(0.152)

0.798***
(0.099)
1.586***
(0.124)

0.607
6,320

0.557
6,999

0.603
6,320

0.557
6,999

0.607
6,320

0.558
6,999

Note: The dependent variables are subject-speciﬁc test score outcomes in math and ELA. The instrumental
variables are based on assigned teacher CLASS (Panels A–C) and assigned peer baseline test scores (Panel
B). For details on the CLASS measure of teaching practices, see Appendix B. All regressions control for the
h(x, x) function (see Section 2) and for randomization block ﬁxed eﬀects. Analytic standard errors, clustered
by randomization block, are in parentheses.
***signiﬁcant at the 1%-level, **signiﬁcant at the 5%-level, *signiﬁcant at the 10%-level.

52

Table A.13: Average reallocation gains in math: sensitivity to restrictions on possible assignments.

Gains expressed in test score standard deviations.

(1)

(2)

(3)
Panel A. Optimal versus status quo

(4)

(5)

(6)

(7)

(8)

A.I Within school type
middle
0.015
(0.010)
3,108

high
0.031
(0.015)
2,332

all students
0.019
(0.007)
8,534

low
0.015
(0.012)
3,094

A.II Within randomization block

all students
0.005
(0.002)
8,534

high
0.009
(0.004)
2,332

middle
0.004
(0.003)
3,108

low
0.003
(0.003)
3,094

Panel B. Optimal versus worst allocation

all students
0.048
(0.015)
8,534

B.I Within school type
middle
0.025
(0.019)
3,108

high
0.089
(0.039)
2,332

low
0.041
(0.028)
3,094

B.II Within randomization block

all students
0.011
(0.003)
8,534

high
0.020
(0.009)
2,332

middle
0.008
(0.006)
3,108

low
0.008
(0.007)
3,094

Gain
SE
N

Gain
SE
N

Note: The table shows the average reallocation gains from implementing the optimal assignment instead
of the random assignment (status quo) in Panel A, and the average reallocation gains from implementing
the optimal assignment instead of the worst assignment in Panel B. The gains are expressed in test score
standard deviations. The computations are based on the 3 × 3 model without teacher-by-peer interactions
(see Table 3, column 5). Panels A.I and B.I display the reallocation eﬀects when the reallocation is carried
out within school types (elementary or middle school) and across school districts. Panels A.II and B.II
display the reallocation eﬀects when the reallocation is carried out within randomization blocks. Standard
errors are in parentheses and computed using the Bayesian bootstrap with 1,000 replications (see Section 4).
High/middle/low: students in the top/middle/bottom tercile of the baseline test score distribution.

53

Table A.14: Average reallocation gains in ELA: sensitivity to restrictions on possible assignments.

Gains expressed in test score standard deviations

(1)

(2)

(3)
Panel A. Optimal versus status quo

(5)

(4)

(6)

(7)

(8)

A.I Within school type
middle
0.002
(0.007)
3,402

high
0.027
(0.013)
2,480

all students
0.009
(0.007)
9,641

low
0.004
(0.012)
3,759

A.II Within randomization block

all students
0.002
(0.002)
9,641

high
0.007
(0.004)
2,480

middle
0.001
(0.002)
3,402

low
0.001
(0.003)
3,759

Panel B. Optimal versus worst allocation

all students
0.021
(0.013)
9,641

B.I Within school type
middle
0.004
(0.013)
3,402

high
0.066
(0.034)
2,480

low
0.006
(0.022)
3,759

B.II Within randomization block

all students
0.005
(0.004)
9,641

high
0.017
(0.009)
2,480

middle
0.001
(0.004)
3,402

low
0.002
(0.007)
3,759

Gain
SE
N

Gain
SE
N

Note: The table shows the average reallocation gains from implementing the optimal assignment instead
of the random assignment (status quo) in Panel A, and the average reallocation gains from implementing
the optimal assignment instead of the worst assignment in Panel B. The gains are expressed in test score
standard deviations. The computations are based on the 3 × 3 model without teacher-by-peer interactions
(see Table 3, column 6). Panels A.I and B.I display the reallocation gains when the reallocation is carried out
within school types (elementary or middle school) and across school districts. Panels A.II and B.II display
the reallocation eﬀects when the reallocation is carried out within randomization blocks. Standard errors
are in parentheses and computed using the Bayesian bootstrap with 1,000 replications. High/middle/low:
students in the top/middle/bottom tercile of the baseline test score distribution.

54

Table A.15: Average reallocation gains in math: sensitivity to inclusion of teacher-by-peer interactions. Gains expressed in test score standard

deviations

(1)

(2)

(3)

(4)

A.I Within school type
middle
0.026
(0.017)
3,108

high
0.059
(0.028)
2,332

all students
0.035
(0.020)
8,534

all students
0.098
(0.048)
8,534

B.I Within school type
middle
0.070
(0.043)
3,108

high
0.170
(0.093)
2,332

low
0.027
(0.026)
3,094

low
0.073
(0.053)
3,094

Gain
SE
N

Gain
SE
N

5
5

(5)

(8)
Panel A. Optimal versus status quo

(7)

(6)

A.II Within school type and district
low
0.025
(0.031)
3,094

all students
0.034
(0.022)
8,534

high
0.057
(0.030)
2,332

middle
0.025
(0.018)
3,108

Panel B. Optimal versus worst allocation

B.II Within school type and district
low
0.064
(0.054)
3,094

all students
0.085
(0.047)
8,534

middle
0.061
(0.046)
3,108

high
0.146
(0.089)
2,332

(9)

(10)

(11)

(12)

A.III Within randomization block

all students
0.009
(0.006)
8,534

high
0.016
(0.008)
2,332

middle
0.008
(0.006)
3,108

low
0.006
(0.006)
3,094

B.III Within randomization block

all students
0.022
(0.012)
8,534

high
0.035
(0.018)
2,332

middle
0.019
(0.013)
3,108

low
0.015
(0.012)
3,094

Note: The table shows the average reallocation gains from implementing the optimal assignment instead of the random assignment (status quo) in Panel
A, and the average reallocation gains from implementing the optimal assignment instead of the worst assignment in Panel B. The gains are expressed in
test score standard deviations. The computations are based on the 3 × 3 model with teacher-by-peer interactions (see Table 3, column 3). Columns 1–4
display the reallocation eﬀects when the reallocation is carried out within school types (elementary or middle school) and across districts, columns 5–8
display the reallocation eﬀects when the reallocation is carried out within school types and districts, and columns 9–12 display the reallocation eﬀects when
the reallocation is carried out within randomization blocks. Standard errors are in parentheses and computed using the Bayesian bootstrap with 1,000
replications. High/middle/low: students in the top/middle/bottom tercile of the baseline test score distribution.

Table A.16: Average reallocation gains in ELA: sensitivity to inclusion of teacher-by-peer interactions. Gains expressed in test score standard

deviations

(1)

(2)

(3)

(4)

A.I Within school type
middle
0.043
(0.024)
3,402

high
0.042
(0.023)
2,480

all students
0.044
(0.027)
9,641

all students
0.091
(0.052)
9,641

B.I Within school type
middle
0.080
(0.043)
3,402

high
0.070
(0.037)
2,480

low
0.045
(0.044)
3,759

low
0.116
(0.085)
3,759

Gain
SE
N

Gain
SE
N

5
6

(5)

(8)
Panel A. Optimal versus status quo

(7)

(6)

A.II Within school type and district
low
0.042
(0.037)
3,759

all students
0.039
(0.023)
9,641

high
0.034
(0.019)
2,480

middle
0.040
(0.020)
3,402

Panel B. Optimal versus worst allocation

B.II Within school type and district
low
0.101
(0.069)
3,759

all students
0.080
(0.044)
9,641

middle
0.070
(0.035)
3,402

high
0.061
(0.031)
2,480

(9)

(10)

(11)

(12)

A.III Within randomization block

all students
0.012
(0.008)
9,641

high
0.011
(0.005)
2,480

middle
0.011
(0.007)
3,402

low
0.014
(0.011)
3,759

B.III Within randomization block

all students
0.027
(0.016)
9,641

high
0.023
(0.011)
2,480

middle
0.021
(0.012)
3,402

low
0.035
(0.026)
3,759

Note: The table shows the average reallocation gains from implementing the optimal assignment instead of the random assignment (status quo) in Panel
A, and the average reallocation gains from implementing the optimal assignment instead of the worst assignment in Panel B. The gains are expressed in
test score standard deviations. The computations are based on the 3 × 3 model with teacher-by-peer interactions (see Table 3, column 4). Columns 1–4
display the reallocation eﬀects when the reallocation is carried out within school types (elementary or middle school) and across districts, columns 5–8
display the reallocation eﬀects when the reallocation is carried out within school types and districts, and columns 9–12 display the reallocation eﬀects when
the reallocation is carried out within randomization blocks. Standard errors are in parentheses and computed using the Bayesian bootstrap with 1,000
replications. High/middle/low: students in the top/middle/bottom tercile of the baseline test score distribution.

B Data Appendix

B.1 Construction of the dataset from the MET ﬁles

Our dataset combines eight diﬀerent data ﬁles from the MET study (2018 release): the
randomization ﬁle, the teacher ﬁle, the class section ﬁle, the student ﬁle, two classroom
observation score ﬁles (the CLASS and the FFT ﬁle), as well as the district-wide ﬁles for the
school years 2009/10 and 2010/11.

The basis of our data construction is the randomization ﬁle. It contains identiﬁers for
all students who were randomly assigned to a teacher in the second year of the MET study,
and an identiﬁer (identiﬁers) for their assigned teacher(s). Through the teacher identiﬁers,
we merge this dataset to the FFT and CLASS ﬁles, and thus obtain the FFT and CLASS
of the assigned teachers. Moreover, we use the teacher identiﬁers to merge the data to the
teacher ﬁle, in order to obtain teacher background characteristics of the assigned teachers.

Student characteristics and test score outcomes come from the student ﬁle and the district-
wide ﬁles. Through a student identiﬁer, we ﬁrst merge the randomization ﬁle with the
student ﬁle, which contains individual information for all students who were part of the
MET study and still had a MET teacher at the end of the school year; i.e., the student
ﬁle does not contain any information on students who switch to a non-MET teacher within
the same school, a non-MET school, or a non-MET district. We use the student ﬁle to
extract students’ demographic characteristics, baseline test scores (test scores in school year
2009/10), and test score outcomes (test scores in school year 2010/11) in math and ELA.
While there are no missings for student background characteristics, some of the students
have missing baseline test scores or missing test score outcomes. Therefore, we obtain the
missing test score information from the district-wide ﬁles.

To construct information on the peer group composition in the assigned classroom, we
average the student background characteristics and baseline test scores at the level of the
assigned teacher and randomization block, since each teacher can only be assigned to one
classroom within a randomization block. To be precise, we construct the leave-own-out mean,
i.e., the classroom mean excluding the student herself.

Information on the realized teacher and the realized peers are constructed based on the
student ﬁle. This ﬁle contains an identiﬁer for the section that the student attended in school
year 2010/11, as well as an identiﬁer for the teacher who taught the section. Based on this
teacher identifer, we merge information on the FFT, CLASS, and background characteristics
of the realized teacher. Furthermore, we construct leave-own-out means of peer characteristics
based on the section identiﬁers. We add information on the size of the realized classroom
from the class section ﬁle.

B.2 Construction of the estimation sample

In our analysis, we focus on students in grades 4-8 who were randomized to a teacher before
the start of the school year. We create separate samples for math and ELA. In total, the
randomization sample contains information on about 16,000 students who were randomized
to a teacher in math and ELA in grades 4-8.

57

Table B.1: Sample construction from the randomization ﬁle

Restrictions

(1)

(2)

(3)

(4)

Math

ELA

N

1. Students in randomization ﬁle
2. Record in the student ﬁle
3. At least two classrooms per randomization

15,749
10,268
9,824

block (base sample)

percent
of base
sample

-
-
100%

N

16,252
11,271
10,856

percent
of base
sample

-
-
100%

4. Baseline test scores and test score outcomes

9,245

94%

10,136

available

5. FFT of the assigned teacher available
6. FFT of the realized teacher available
Information on the assigned peers available
7.
8.
Information on the realized peers available
9. At least two classrooms per randomization

9,066
8,724
8,718
8,717
8,534

block after applying all restrictions

92%
89%
89%
89%
87%

10,057
9,767
9,762
9,761
9,641

93%

93%
90%
90%
90%
89%

Note: Sample restrictions used to create the estimation sample from MET data ﬁles, school year 2010/11.

Table B.1 details further restrictions that we apply to construct our estimation dataset.
We require each student to have a record in the student ﬁle, since we use this ﬁle to identify
the realized teacher, the realized peer group, and the student background variables. About
two-thirds of the students in the randomization sample can be identiﬁed in the student ﬁle.
Moreover, we restrict our sample to only randomization blocks with at least two classrooms.
The resulting dataset forms our base sample.

After constructing our base sample, we remove observations with missing information
on baseline test scores or test score outcomes, with missing information on the FFT of
the assigned or realized teacher, and with missing information on the baseline test scores
of the assigned or realized peers. We further remove all randomization blocks with only
one classroom after applying these restrictions. Our resulting estimation sample contains
information on about 8,500 students in math and 9,600 students in ELA. Thus, we retain
about 87 percent of observations from the base sample in math, and 89 percent of observations
from the base sample in ELA.

Our sample size is close to the sample size reported by Garrett and Steinberg (2015).
We obtain a slightly larger sample size because we complete missing test score information
in the student ﬁle with information from the district-wide ﬁles. These ﬁles are part of the
2018 MET release and were not available when Garrett and Steinberg (2015) published their
study.

Our estimation dataset does not contain any missing information for the main variables
used in the analysis; however, it contains some missings in teacher and student demographics.
With the exception of free/reduced-price lunch eligibility (30 percent missings in math and 25
percent missings in ELA), the student background variables contain (virtually) no missings.
Further missings occur in the teacher demographics: teacher gender and race/ethnicity are
missing for 3–4 percent of the estimation sample. Teacher experience and teachers’ education

58

are missing for about 30 percent of the estimation sample each, because one school district
did not provide information on teachers’ education, and another district did not provide in-
formation on teachers’ experience. Full information on teacher demographics is only available
for 42 percent of the math sample, and for 48 percent of the ELA sample.

B.3 Sample comparisons

This section investigates in how far the randomization sample diﬀers from the estimation
sample. It compares the distribution of student baseline test scores and teacher FFT in the
original randomization sample with the distribution in the estimation sample.

The randomization sample consists of about 15,700 students in math and 16,300 students
in ELA; baseline test scores are available for about 13,900 students in math and for about
14,400 students in ELA (see Figure B.1). The remaining student observations can neither
be matched to the district-wide ﬁles nor be matched to the student ﬁle. Our estimation
sample contains only students in the student ﬁle – because we use this ﬁle to identify realized
teachers and peers – and is thus considerably smaller (about 8,500 students in math and
about 9,600 students in ELA). Students in the estimation dataset have on average higher
baseline test scores than the students in the randomization dataset. The diﬀerence amounts
to 0.06 standard deviations in math and to 0.05 standard deviations in ELA.

Figure B.1: Distribution of baseline test scores in the randomization sample and the estimation

sample

Note: The ﬁgure compares the distributions of student baseline test scores across the randomization sample
and the estimation sample for math (Panels 1 and 2) and ELA (Panels 3 and 4).

Teacher FFT does not diﬀer appreciably between the randomization sample and the
estimation sample (see Figure B.2). In the math randomization sample for grades 4–8, 666

59

0.02.04.06.08.1Fraction of students-4-2024Baseline test scoresNumber of students = 13920, mean = .05(1) Math baseline test scores- randomization sample0.02.04.06.08.1Fraction of students-4-2024Baseline test scoresNumber of students = 8534, mean = .11(2) Math baseline test scores- estimation sample0.02.04.06.08.1Fraction of students-4-2024Baseline test scoresNumber of students = 14424, mean = .12(3) ELA baseline test scores- randomization sample0.02.04.06.08.1Fraction of students-4-2024Baseline test scoresNumber of students = 9641, mean = .17(4) ELA baseline test scores- estimation sampleteachers have non-missing information on FFT; the estimation sample includes 614 teachers.
In both samples, the average FFT is 2.52. In the ELA randomization sample for these grades,
705 teacher have non-missing FFT, and the estimation sample contains 649 teachers. The
average FFT in both samples are nearly identical with 2.57 and 2.58.

Figure B.2: Distribution of FFT in the randomization sample and the estimation sample

Note: The ﬁgure compares the distributions of teacher FFT across the randomization sample and the esti-
mation sample for math teachers (Panels 1 and 2) and ELA teachers (Panels 3 and 4). The red lines denote
the FFT cutoﬀs we use to classify teachers in our preferred speciﬁcation.

B.4

Information on the CLASS measure

Information from the MET User Guide. The following information about the CLASS
(Classroom Assessment Scoring System) protocol is taken from the MET User Guide (White
et al., 2019, p. 32–33):

“CLASS is an observational protocol designed to measure the extent to which teachers
eﬀectively support children’s social and academic development. Two diﬀerent versions of
CLASS were used in the MET Study: the Upper Elementary (Grades 4-5) and the Secondary
(Grades 6-9).

“The CLASS instrument is divided into three broad domains of measurement: Emotional
Support, Classroom Organization, and Instructional Support. Each domain, in turn, is mea-
sured by a number of dimensions. The domain “Emotional Support,” for example, refers to
the emotional tone in a classroom, which can be measured along four dimensions: positive cli-
mate, negative climate, teacher sensitivity, and regard for student perspectives. The domain
“Classroom Organization” refers to the ways a classroom is structured to manage students’
behavior, time, and attention, which can be measured along three dimensions: behavior

60

0.05.1.15.2Fraction of teachers1.522.533.5FFTNumber of teachers = 666, mean = 2.52(1) FFT (math) - randomization sample0.05.1.15.2Fraction of teachers1.522.533.5FFTNumber of teachers = 614, mean = 2.52(2) FFT (math) - estimation sample0.05.1.15.2Fraction of teachers1.522.533.5FFTNumber of teachers = 705, mean = 2.57(3) FFT (ELA) - randomization sample0.05.1.15.2Fraction of teachers1.522.533.5FFTNumber of teachers = 649, mean = 2.58(4) FFT (ELA) - estimation samplemanagement, productivity, and instructional learning formats. The domain “Instructional
Supports” refers to the ways a teacher provides supports to encourage student conceptual
understanding and student problem solving and can be measured along four dimensions:
content understanding, analysis and problem solving, instructional dialogue, and quality of
feedback. [...]

“CLASS scoring is done using a detailed scoring rubric. In this rubric, a classroom is
scored on each instructional dimension at 15-minute intervals using a 7-point scale. For the
MET Study, only the ﬁrst 30 minutes of each video was scored. Scores are assigned based
on anchor descriptions of what is to be observed in order for a classroom to be scored at
“high,” “mid,” and “low” points on the 7-point scale. In the MET Study, dimension scores
are often aggregated to higher levels of analysis simply by averaging raters’ scores to get
a single segment score and then calculating the harmonic mean of segment scores across
all segments for a particular target of measurement (e.g., a day, a class section, a teacher).
Standard errors of measurement for these derived scores are not generally reported.”

Use of the CLASS measure in our study. We use the CLASS measure to test the
sensitivity of our results to the classroom observation protocol used. To construct a unique
measure for each teacher, we take the average across the three CLASS domains.

Our estimation sample contains 466 teachers in the math sample and 495 teachers in the
ELA sample. In the math sample, CLASS ranges from 2.54 to 5.58, with a mean of 4.34. In
the ELA sample, CLASS ranges from 2.95 to 5.58, with a mean of 4.39. We split teachers
in three categories according to this measure. We choose the cutoﬀ values of 4 and 4.5 to
carry out the split. Both in the math and in the ELA sample, 20 percent of the teachers are
classiﬁed as low, 42 percent as middle, and 38 percent as high.

61

C Optimal allocation: linear program

The linear program aims at maximizing the aggregate test score outcomes in the data. We
use (cid:98)Yc(w) to denote the predicted aggregate outcome of classroom c when assigned a teacher
of level w, where w = wL when assigned a low-, w = wM when assigned a middle-, and
w = wH when assigned a high-FFT teacher (see Section 4 for details). We use C to denote
the total number of teachers in the dataset, and Cw to denote the number of teachers of level
w in the dataset. We deﬁne an indicator variable αcw, which takes the value 1 if classroom c
is taught by a teacher of level w, and 0 otherwise. We also deﬁne an assignment matrix A,
which contains all αcw. The linear program can then be written as:

subject to

max
A

C
(cid:88)

(cid:88)

c=1

w∈{wL,wM ,wH }

αcw (cid:98)Yc(w)

(cid:88)

αcw = 1 ∀ c ∈ C

w∈{wL,wM ,wH }

C
(cid:88)

c=1

αcw = Cw

for w ∈ {wL, wM , wH}

αcw ∈ {0, 1}

This is a transportation problem with C + 3 + (3 × C) constraints. We solve the trans-

portation problem in R using lpSolve.

62

