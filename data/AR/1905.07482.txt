Learning to Reconstruct 3D Manhattan Wireframes from a Single Image

Yichao Zhou1,2,âˆ— Haozhi Qi1 Yuexiang Zhai1 Qi Sun2 Zhili Chen2 Li-Yi Wei2 Yi Ma1

1UC Berkeley

2Adobe Research

1
2
0
2

r
p
A
6
1

]

V
C
.
s
c
[

2
v
2
8
4
7
0
.
5
0
9
1
:
v
i
X
r
a

Abstract

In this paper, we propose a method to obtain a compact
and accurate 3D wireframe representation from a single im-
age by eï¬€ectively exploiting global structural regularities.
Our method trains a convolutional neural network to simul-
taneously detect salient junctions and straight lines, as well
as predict their 3D depths and vanishing points. Compared
with the state-of-the-art learning-based wireframe detection
methods, our network is simpler and more uniï¬ed, leading
to better 2D wireframe detection. With global structural
priors from parallelism, our method further reconstructs
a full 3D wireframe model, a compact vector representa-
tion suitable for a variety of high-level vision tasks such
as AR and CAD. We conduct extensive evaluations on a
large synthetic dataset of urban scenes as well as real im-
ages. Our code and datasets have been made public at
https://github.com/zhou13/shapeunity.

1. Introduction

Recovering 3D geometry of a scene from RGB images is
one of the most fundamental and yet challenging problems
in computer vision. Most existing oï¬€-the-shelf commer-
cial solutions to obtain 3D geometry still requires active
depth sensors such as structured lights (e.g., Apple ARKit
and Microsoft Mixed Realty Toolkit) or LIDARs (popular
in autonomous driving). Although these systems can meet
the needs of speciï¬c purposes, they are limited by the cost,
range, and working conditions (indoor or outdoor) of the
sensors. The representations of ï¬nal outputs are typically
dense point clouds, which are not only memory and compu-
tation intense, but also may contain noises and errors due to
transparency, occlusions, reï¬‚ections, etc.

On the other hand, traditional image-based 3D recon-
struction methods, such as Structure from Motion (SfM) and
visual SLAM, often rely on local features. Although the ef-
ï¬ciency and reliability have been improving (e.g., Microsoft
Hololens, Magic Leap), they often need multiple cameras
with depth sensors [13] for better accuracy. The ï¬nal scene

âˆ—This work was done when Y. Zhou was an intern at Adobe Research.

(a) Input image

(b) 3D wireframe

(c) Novel view

Figure 1. Column (a) shows the input images overlaid with the
groundtruth wireframes, in which the red and blue dots represent
the C- and T-type junctions, respectively. Column (b) shows the
predicted 3D wireframe from our system, with grayscale visualiz-
ing depth. Column (c) shows alternative views of (b). Note that
our system recovers geometrically salient wireframes, without be-
ing aï¬€ected by the textural lines, e.g., the vertical textural patterns
on the Big Ben facade.

representation remains quasi-dense point clouds, which are
typically incomplete, noisy, and cumbersome to store and
share. Consequently, complex post-processing techniques
such as plane-ï¬tting [11] and mesh reï¬nement [14, 19] are
required. Such traditional representations can hardly meet
the increasing demand for high-level 3D modeling, content
editing, and model sharing from hand-held cameras, mobile
phones, and even drones.

Unlike conventional 3D geometry capturing systems, the
human visual system does not perceive the world as uni-
formly distributed points.
Instead, humans are remark-
ably eï¬€ective, eï¬ƒcient, and robust in utilizing geometrically
salient global structures such as lines, contours, planes, and
smooth surfaces to perceive 3D scenes [1]. However, it re-
mains challenging for vision algorithms to detect and utilize
such global structures from local image features, until recent
advances in deep learning which makes learning high-level
features possible from labeled data. The examples include

1

 
 
 
 
 
 
detecting planes [29, 18], surfaces [9], 2D wireframes [12],
room layouts [34], key points for mesh ï¬tting [30, 28], and
sparse scene representations from multiple images [5].

In this work, we infer global 3D scene layouts from
learned line and junction features, as opposed to local corner-
like features such as SIFT [7], ORB [21], or line segments
[10, 4, 23] used in conventional SfM or visual SLAM sys-
tems. Our algorithm learns to detect a special type of wire-
frames that consist of junctions and lines representing the
corners and edges of buildings. We call our representation
the geometric wireframe and demonstrate that together with
certain global priors (such as globally or locally Manhattan
[2, 7, 23]), the wireframe representation allows eï¬€ective and
accurate recovery of the sceneâ€™s 3D geometry, even from a
single input image. Our method trains a neural network to
estimate global lines and two types of junctions with depths,
and constructs full 3D wireframes using the estimated depths
and geometric constraints.

Previously, there have been eï¬€orts trying to understand
the indoor scenes with the help of the 3D synthetic datasets
such as the SUNCG [24, 31]. Our work aims at natural urban
environments with a variety of geometries and textures. To
this end, we build two new datasets containing both synthetic
and natural urban scenes. Figure 1 shows the sampled results
of the reconstruction and Figure 2 shows the full pipeline of
our system.
Contributions of this paper. Comparing to existing wire-
frame detection algorithms such as [12], our method

â€¢ jointly detects junctions, lines, depth, and vanishing
points with a single neural network, exploiting the tight
relationship among those geometric structures;

â€¢ learns to diï¬€erentiate two types of junctions: the physi-
cal intersections of lines and planes â€œC-junctionsâ€, and
the occluding â€œT-junctionsâ€;

â€¢ recovers a full 3D wireframe of the scene from the lines

and junctions detected in a single RGB image.

2. Methods

As depicted in Figure 2, our system starts with a neural
network that takes a single image as input and jointly predicts
multiple 2D heatmaps, from which we vectorize lines and
junctions as well as estimate their initial depth values and
vanishing points. We call this intermediate result a 2.5D
wireframe. Using both the depth values and vanishing points
estimated from the same network as the prior, we then lift
the wireframe from the 2.5D image-space into the full 3D
world-space.

2.1. Geometric Representation

In a geometric wireframe W = (V, E) of the scene, V
and E âŠ† V Ã— V are the junctions and lines. Speciï¬cally, E
represents lines from physical intersections of two planes

while V represents (physical or projective) intersections of
lines among E. Unlike [10, 12], our E totally excludes
planar textural lines, such as the vertical textures of Big Ben
in Figure 1. The so-deï¬ned W aims to capture global scene
geometry instead of local textural details.1 By ruling out
planar textural lines, we can group the junctions into two
categories. Let ğ½ğ’— âˆˆ {ğ¶, ğ‘‡ } be the junction type of ğ’—, in
which each junction can either be a C-junction (ğ½ğ’— = ğ¶)
or a T-junction (ğ½ğ’— = ğ‘‡). Corner C-junctions are actual
intersections of physical planes or edges, while T-junctions
are generated by occlusion. Examples of T-junctions (in
blue) and C-junctions (in red) can be found in Figure 1. We
denote them as two disjoint sets V = Vğ¶ âˆªVğ‘‡ , in which Vğ¶ =
{ğ’— âˆˆ V | ğ½ğ’— = ğ¶} and Vğ‘‡ = {ğ’— âˆˆ V | ğ½ğ’— = ğ‘‡ }. We note that
the number of lines incident to a T-junction in E is always
1 rather than 3 because a T-junction do not connect to the
two foreground vertices in 3D. Junction types are important
for inferring 3D wireframe geometry, as diï¬€erent 3D priors
will be applied to each type.2 For each C-junction ğ’—ğ‘ âˆˆ Vğ¶ ,
deï¬ne ğ‘§ğ’—ğ‘ as the depth of vertex ğ’—ğ‘, i.e., the ğ‘§ coordinate
of ğ’—ğ‘ in the camera space. For each occlusional T-junction
ğ’—ğ‘¡ âˆˆ Vğ‘‡ , we deï¬ne ğ‘§ğ’—ğ‘¡ as the depth on the occluded line in the
background because the foreground line depth can always be
recovered from other junctions. With depth information, 3D
wireframes that are made of C-junctions, T-junctions, and
lines give a compact representation of the scene geometry.
Reconstructing such 3D wireframes from a single image is
our goal.

2.2. From a Single Image to 2.5D Representation

Our ï¬rst step is to train a neural network that learns the
desired junctions, lines, depth, and vanishing points from our
labeled datasets. We ï¬rst brieï¬‚y describe the desired outputs
from the network and the architecture of the network. The
associated loss functions for training the network will be
speciï¬ed in detail in the next sections.

Given the image ğ¼ of a scene, the pixel-wise outputs of our
neural network consist of ï¬ve outputs âˆ’ junction probability
ğ½, junction oï¬€set ğ‘¶, edge probability ğ¸, junction depth D,
and vanishing points ğ‘½:

ğ‘Œ (cid:17) (ğ½, ğ‘¶, ğ¸, D, ğ‘½),

Ë†ğ‘Œ (cid:17) ( Ë†ğ½, Ë†ğ‘¶, Ë†ğ¸, Ë†D, Ë†ğ‘½),

(1)

where symbols with and without hats represent the ground
truth and the prediction from the neural network, respec-
tively. The meaning of each symbol is detailed in Sec-
tion 2.2.2.

1In urban scenes, lines from regular textures (such as windows on a
facade) do encode accurate scene geometry [32]. The neural network can
still use them for inferring the wireframe but only not to keep them in
the ï¬nal output, which is designed to give a compact representation of the
geometry only.

2There is another type of junctions which are caused by lines intersecting

with the image boundary. We treat them as C-junctions for simplicity.

Figure 2. Overall pipeline of the proposed method.

2.2.1 Network Design
Our network structure is based on the stacked hourglass
network [22]. The input images are cropped and re-scaled
to 512 Ã— 512 before entering the network. The feature-
extracting module, the ï¬rst part of the network, includes
strided convolution layers and one max pooling layer to
downsample the feature map to 128 Ã— 128. The following
part consists of ğ‘† hourglass modules. Each module will
gradually downsample then upsample the feature map. The
stacked hourglass network will gradually reï¬ne the output
map to match the supervision from the training data. Let the
output of the ğ‘—th hourglass module given the ğ‘–th image be
ğ¹ğ‘— (ğ¼ğ‘–). During the training stage, the total loss to minimize
is:

ğ¿total (cid:17)

ğ‘
âˆ‘ï¸

ğ‘†
âˆ‘ï¸

ğ‘–=1

ğ‘—=1

ğ¿ (ğ‘Œ ( ğ‘—)
ğ‘–

, Ë†ğ‘Œğ‘–) =

ğ‘
âˆ‘ï¸

ğ‘†
âˆ‘ï¸

ğ‘–=1

ğ‘—=1

ğ¿ (ğ¹ğ‘— (ğ¼ğ‘–), Ë†ğ‘Œğ‘–),

where ğ‘– represents the index of images in the training dataset;
ğ‘— represents the index of the hourglass modules; ğ‘ repre-
sents the number of training images in a batch; ğ‘† represents
the number of stacks used in the neural network; ğ¿(Â·, Â·) rep-
resents the loss of an individual image; ğ‘Œ ( ğ‘—)
represents the
predicted intermediate representation of image ğ¼ğ‘– from the
ğ‘—th hourglass module, and Ë†ğ‘Œğ‘– represents the ground truth
intermediate representation of image ğ¼ğ‘–.

ğ‘–

The loss of an individual image is a superposition of the

loss functions ğ¿ ğ‘˜ speciï¬ed in the next section:

ğ¿ (cid:17) âˆ‘ï¸
ğ‘˜

ğœ†ğ‘˜ ğ¿ ğ‘˜ , ğ‘˜ âˆˆ {ğ½, ğ‘¶, ğ¸, D, ğ‘½}.

The hyper-parameters ğœ†ğ‘˜ represents the weight of each sub-
loss. During experiments, we set ğœ† so that ğœ†ğ‘˜ ğ¿ ğ‘˜ are of
similar scales.
2.2.2 Output Maps and Loss Functions
Junction Map ğ½ and Loss ğ¿ ğ½ . The ground truth junction
map Ë†ğ½ is a down-sampled heatmap for the input image,
whose value represents whether there exists a junction in
that pixel. For each junction type ğ‘¡ âˆˆ {ğ¶, ğ‘‡ }, we estimate
its junction heatmap

(cid:40)

Ë†ğ½ğ‘¡ ( ğ’‘) =

1 âˆƒğ’— âˆˆ Vğ‘¡ : ğ’‘ = (cid:98) ğ’—
4 (cid:99)
0 otherwise

, ğ‘¡ âˆˆ {ğ¶, ğ‘‡ }.

where ğ’‘ is the integer coordinate on the heatmap and ğ’— is
the coordinate of a junction with type ğ‘¡ in the image space.
Following [22], the resolution of the junction heatmap is 4
times less than the resolution of the input image.

Because some pixels may contain two types of junctions,
we treat the junction prediction as two per-pixel binary clas-
siï¬cation problems. We use the classic softmax cross en-
tropy loss to predict the junction maps:

ğ¿ ğ½ (ğ½, Ë†ğ½) (cid:17) 1
ğ‘›
ğ‘¡ âˆˆ {ğ¶,ğ‘‡ }

âˆ‘ï¸

ğ’‘

âˆ‘ï¸

CrossEntropy (cid:0)ğ½ğ‘¡ ( ğ’‘), Ë†ğ½ğ‘¡ ( ğ’‘)(cid:1) ,

where ğ‘› is the number of pixels of the heatmap. The resulting
ğ½ğ‘¡ (ğ‘¥, ğ‘¦) âˆˆ (0, 1) represents the probability whether there
exists a junction with type ğ‘¡ at [4ğ‘¥, 4ğ‘¥ + 4) Ã— [4ğ‘¦, 4ğ‘¦ + 4) in
the input image.
Oï¬€set Map ğ‘¶ and Loss ğ¿ğ‘¶. Comparing to the input im-
age, the lower resolution of ğ½ might aï¬€ect the precision of

Feature Extraction & Hourglass x 4CONVsDepth MapsJunction HeatmapsEdge MapsVanishing PointsWireframe Vectorization3D LiftingNeural Network2.5D InferenceCONVsCONVsCONVsInput Imagejunction positions. Inspired by [27], we use an oï¬€set map
to store the diï¬€erence vector from Ë†ğ½ to its original position
with sub-pixel accuracy:

Ë†ğ‘¶ğ‘¡ ( ğ’‘) =

(cid:40) ğ’—
4 âˆ’ ğ’‘ âˆƒğ’— âˆˆ Vğ‘¡ : ğ’‘ = (cid:98) ğ’—
4 (cid:99)
otherwise
0

, ğ‘¡ âˆˆ {ğ¶, ğ‘‡ }.

We use the â„“2-loss for the oï¬€set map and use the heatmap
as a mask to compute the loss only near the actual junctions.
Mathematically, the loss function is written as

ğ¿ğ‘¶ (ğ‘¶, Ë†ğ‘¶) (cid:17) âˆ‘ï¸

(cid:205)

ğ‘¡ âˆˆ {ğ¶,ğ‘‡ }

(cid:12)ğ‘¶ğ‘¡ ( ğ’‘) âˆ’ Ë†ğ‘¶ğ‘¡ ( ğ’‘)(cid:12)
(cid:12)
(cid:12)
ğ’‘ Ë†ğ½ğ‘¡ ( ğ’‘) (cid:12)
2
(cid:12)
(cid:12)
(cid:12)
2
(cid:205)
ğ’‘ Ë†ğ½ğ‘¡ ( ğ’‘)

,

where ğ‘¶ğ‘¡ ( ğ’‘) is computed by applying a sigmoid and con-
stant translation function to the last layer of the oï¬€set branch
in the neural network to enforce ğ‘¶ğ‘¡ ( ğ’‘) âˆˆ [0, 1)2. We nor-
malize ğ¿ğ‘¶ by the number of junctions of each type.
Edge Map ğ¸ and Loss ğ¿ğ¸ . To estimate line positions,
we represent them in an edge heatmap. For the ground
truth lines, we draw them on the edge map using an anti-
aliasing technique [33] for better accuracy. Let dist( ğ’‘, ğ‘’) be
the shortest distance between a pixel ğ’‘ and the nearest line
segment ğ‘’. We deï¬ne the edge map to be

(cid:40)

Ë†ğ¸ ( ğ’‘) =

maxğ‘’ 1 âˆ’ dist( ğ’‘, ğ‘’) âˆƒğ‘’ âˆˆ E : dist( ğ’‘, ğ‘’) < 1,
0

otherwise.

Intuitively, ğ¸ ( ğ’‘) âˆˆ [0, 1] represents the probability of a
line close to point ğ’‘. Because the range of the edge map
is always between 0 and 1, we can treat it as a probability
distribution and use the sigmoid cross entropy loss on the ğ¸
and Ë†ğ¸:

ğ¿ğ¸ (ğ¸, Ë†ğ¸) (cid:17) 1
ğ‘›

âˆ‘ï¸

ğ’‘

CrossEntropy (cid:0)ğ¸ ( ğ’‘), Ë†ğ¸ ( ğ’‘)(cid:1) .

Junction Depth Maps D and Loss ğ¿ D. To estimate the
depth ğ‘§ğ’— for each junction ğ’—, we deï¬ne the junction-wise
depth map as

Ë†Dğ‘¡ ( ğ’‘) =

(cid:40)ğ‘§ğ’— âˆƒğ’— âˆˆ Vğ‘¡ : ğ’‘ = (cid:98) ğ’—
4 (cid:99)
otherwise
0

, ğ‘¡ âˆˆ {ğ¶, ğ‘‡ }.

In many datasets with unknown depth units and camera
intrinsic matrix ğ¾, ğ‘§ğ’— remains a relative scale instead of ab-
solute depth. To remove the ambiguity from global scaling,
we use scale-invariant loss (SILog) which has been intro-
duced in the single image depth estimation literature [3]. It
removes the inï¬‚uence of the global scale by summing the
log diï¬€erence between each pixel pair.

ğ¿ğ· (D, Ë†D) (cid:17) âˆ‘ï¸

ğ‘¡

âˆ‘ï¸

âˆ’

ğ‘¡

1
ğ‘›ğ‘¡

1
ğ‘›2
ğ‘¡

âˆ‘ï¸

(cid:0) log Dğ‘¡ ( ğ’‘) âˆ’ log Ë†Dğ‘¡ ( ğ’‘)(cid:1) 2

ğ’‘ âˆˆVğ‘¡
(cid:0) âˆ‘ï¸

ğ’‘ âˆˆVğ‘¡

log Dğ‘¡ ( ğ’‘) âˆ’ log Ë†Dğ‘¡ ( ğ’‘)(cid:1) 2.

Vanishing Point Map ğ‘½ and Loss ğ¿ğ‘½ . Lines in man-
made outdoor scenes often cluster around the three mutually
orthogonal directions. Let ğ‘– âˆˆ {1, 2, 3} represent these three
directions. In perspective geometry, parallel lines in direc-
tion ğ‘– will intersect at the same vanishing point (ğ‘‰ğ‘–,ğ‘¥, ğ‘‰ğ‘–,ğ‘¦)
in the image space, possibly at inï¬nity. To prevent ğ‘‰ğ‘–,ğ‘¥ or
ğ‘‰ğ‘–,ğ‘¦ from becoming too large, we normalize the vector so
that

ğ‘½ğ‘– =

1
ğ‘‰ 2
ğ‘–,ğ‘¥ + ğ‘‰ 2

ğ‘–,ğ‘¦ + 1

(cid:2)ğ‘‰ğ‘–,ğ‘¥, ğ‘‰ğ‘–,ğ‘¦, 1(cid:3)ğ‘‡ .

(2)

Because the two horizontal vanishing points ğ‘½1 and ğ‘½2 are
order agnostic from a single RGB image, we use the Chamfer
â„“2-loss for ğ‘½1 and ğ‘½2, and the â„“2-loss for ğ‘½3 (the vertical
vanishing point):

ğ¿ğ‘½ (ğ‘½, Ë†ğ‘½) (cid:17) min((cid:107)ğ‘½1 âˆ’ Ë†ğ‘½1(cid:107), (cid:107)ğ‘½2 âˆ’ Ë†ğ‘½1(cid:107))

+ min((cid:107)ğ‘½1 âˆ’ Ë†ğ‘½2(cid:107), (cid:107)ğ‘½2 âˆ’ Ë†ğ‘½2(cid:107)) + (cid:107)ğ‘½3 âˆ’ Ë†ğ‘½3 (cid:107)2
2

.

2.3. Heatmap Vectorization

As seen from Figure 2, the outputs of the neural network
are essentially image-space 2.5D heatmaps of the desired
wireframe. Vecterization is needed to obtain a compact
wireframe representation.
Junction Vectorization. Recovering the junctions V from
the junction heatmaps ğ½ is straightforward. Let ğœ—ğ¶ and ğœ—ğ‘‡
be the thresholds for ğ½ğ¶ and ğ½ğ‘‡ . The junction candidate sets
can be estimated as

Vğ‘¡ â† { ğ’‘ + ğ‘¶ğ‘¡ ( ğ’‘) | ğ½ğ‘¡ ( ğ’‘) â‰¥ ğœ—ğ‘¡ }, ğ‘¡ âˆˆ {ğ¶, ğ‘‡ }.

(3)

Line Vectorization. Line vectorization has two stages.
In the ï¬rst stage, we detect and construct the line candi-
dates from all the corner C-junctions. This can be done
by enumerating all the pairs of junctions ğ’–, ğ’˜ âˆˆ Vğ¶ , con-
necting them, and testing if their line conï¬dence score is
greater than a threshold ğ‘(ğ’–, ğ’˜) â‰¥ ğœ—ğ¸ . The conï¬dence
score of a line with two endpoints ğ’– and ğ’˜ is given as
ğ‘(ğ’–, ğ’˜) = 1
ğ’‘ âˆˆğ‘ƒ (ğ’–,ğ’˜) ğ¸ ( ğ’‘) where ğ‘ƒ(ğ’–, ğ’˜) represents
|ğ’–ğ’˜ |
the set of pixels in the rasterized line (cid:174)ğ’–ğ’˜, and | (cid:174)ğ’–ğ’˜| represents
the number of pixels in that line.

(cid:205)

In the second stage, we construct all the lines between
â€œT-Tâ€ and â€œT-Câ€ junction pairs. We repeatedly add a T-
junction to the wireframe if it is tested to be close to a
detected line. Unlike corner C-junctions, the degree of a
T-junction is always one. So for each T-junction, we ï¬nd
the best edge associated with it. This process is repeated
until no more lines could be added. Finally, we run a post-
processing procedure to remove lines that are too close or
cross each other. By handling C-junctions and T-junctions
separately, our line vectorization algorithm is both eï¬ƒcient
and robust for scenes with hundreds of lines. A more detailed
description is discussed in the supplementary material.

2.4. Image-Space 2.5D to World-Space 3D

So far, we have obtained vectorized junctions and lines in
2.5D image space with depth in a relative scale. However, in
scenarios such as AR and 3D design, absolute depth values
are necessary for 6DoF manipulation of the 3D wireframe.
In this section, we present the steps to estimate them with
our network predicted vanishing points.
2.4.1 Calibration from Vanishing Points
In datasets such as MegaDepth [16], the camera calibration
matrix ğ¾ âˆˆ R3Ã—3 of each image is unknown, although it is
critical for a full 3D wireframe reconstruction. Fortunately,
calibration matrices can be inferred from three mutually or-
thogonal vanishing points if the scenes are mostly Manhat-
tan. According to [20], if we transform the orthogonal van-
ishing points ğ‘½ğ‘– to the calibrated coordinates Â¯ğ‘½ğ‘– (cid:17) ğ¾ âˆ’1ğ‘½ğ‘–,
then Â¯ğ‘½ğ‘– should be mutually orthogonal, i.e.,

ğ‘½ğ‘–ğ¾ âˆ’ğ‘‡ ğ¾ âˆ’1ğ‘½ ğ‘— = 0, âˆ€ğ‘–, ğ‘— âˆˆ {1, 2, 3}, ğ‘– â‰  ğ‘— .

These equations impose three linearly independent con-
straints on ğ¾ âˆ’ğ‘‡ ğ¾ âˆ’1 and would enable solving up to three
unknown parameters in the calibration matrix, such as the
optical center and the focal length.
2.4.2 Depth Reï¬nement with Vanishing Points
Due to the estimation error, the predicted depth map may
not be consistent with the detected vanishing points ğ‘½ğ‘–. In
practice, we ï¬nd the neural network performs better on esti-
mating the vanishing points than predicting the 2.5D depth
map. This is probably because there are more geometric
cues for the vanishing points, while estimating depth re-
quires priors from data. Furthermore, the unit of the depth
map might be unknown due to the dataset (e.g., MegaDepth)
and the usage of SILog loss. Therefore, we use the vanishing
points to reï¬ne the junction depth and determine its absolute
value. Let Ëœğ‘§ğ’— (cid:17) Dğ½ğ’— (ğ’—) be the predicted depth for junction
ğ’— from our neural network. We design the following convex
objective function:

min
ğ‘§, ğ›¼

3
âˆ‘ï¸

ğ‘–=1

âˆ‘ï¸

(cid:13)
(cid:13)(ğ‘§ğ’– Â¯ğ’– âˆ’ ğ‘§ğ’— Â¯ğ’—) Ã— Â¯ğ‘½ğ‘–

(cid:13)
(cid:13)2

(ğ’–,ğ’—) âˆˆAğ‘–
+ ğœ†ğ‘…

âˆ‘ï¸

(cid:107)ğ‘§ğ’— âˆ’ ğ›¼ Ëœğ‘§ğ’— (cid:107)2
2

(4)

subject to

ğ’— âˆˆV
ğ‘§ğ’— â‰¥ 1, âˆ€ğ’— âˆˆ V,
ğœ†ğ‘§ğ’– + (1 âˆ’ ğœ†)ğ‘§ğ’— â‰¤ ğ‘§ğ’˜,
âˆ€ğ’˜ âˆˆ Vğ‘‡ , (ğ’–, ğ’—) âˆˆ E : ğ’˜ = ğœ†ğ’– + (1 âˆ’ ğœ†)ğ’—,

(5)

(6)

where Ağ‘– represents the set of lines corresponding to van-
ishing point ğ‘–; ğ›¼ resolves the scale ambiguity in the depth
dimension; Â¯ğ’– (cid:17) ğ¾ âˆ’1 [ğ‘¢ ğ‘¥ ğ‘¢ ğ‘¦ 1]ğ‘‡ is the vertex position in the
calibrated coordinate. The goal of the ï¬rst term in Equa-
tion (4) is to encourage the line (ğ‘§ğ’– Â¯ğ’–, ğ‘§ğ’˜ Â¯ğ’˜) parallel to van-
ishing point Â¯ğ‘½ğ‘– by penalizing over the parallelogram area

spanned by those two vectors. The second term regularizes
ğ‘§ğ’— so that it is close to the networkâ€™s estimation Ëœğ‘§ğ’— up to a
scale. Equation (5) prevents the degenerating solution ğ’› = 0.
Equation (6) is a convex relaxation of ğœ†
, the
ğ‘§ğ’–
depth constraint for T-junctions.

+ 1âˆ’ğœ†
ğ‘§ğ’˜

â‰¥ 1
ğ‘§ğ’—

3. Datasets and Annotation

One of the bottlenecks of supervised learning is inad-
equate dataset for training and testing. Previously, [12]
develops a dataset for 2D wireframe detection. However,
their dataset does not contain the 3D depth or the type of
junctions. To the best of our knowledge, there is no public
image dataset that has both wireframe and 3D information.
To validate our approach, we create a hybrid dataset with a
larger number of synthetic images of city scenes and smaller
number of real images. The former has accurate 3D geome-
try and automatically annotated ground truth 3D wireframes
from mesh edges, while the latter is manually labelled with
less accurate 3D information.
SceneCity Urban 3D Dataset (SU3). To obtain a large
number of images with accurate geometrical wireframes,
we use a progressively generated 3D mesh repository,
SceneCity3. The dataset is made up of simple polygons
with artist-tuned materials and textures. We extract the
C-junctions from the vertices of the mesh and compute
T-junctions using computational geometry algorithms and
OpenGL. Our dataset includes 230 cities, each containing
8 Ã— 8 city blocks. The cities have diï¬€erent building arrange-
ments and lighting conditions by varying the sky maps. We
randomly generate 100 viewpoints for each city based on cri-
teria such as the number of captured buildings to simulate
hand-held and drone cameras. The synthetic outdoor images
are then rendered through global illumination by Blender,
which provides 23, 000 images in total. We use the images
of the ï¬rst 227 cities for training and the rest 3 cities for
validation.
Realistic Landmark Dataset. The MegaDepth v1 dataset
[17] contains real images of 196 landmarks in the world. It
also contains the depth maps of these images via structure
from motion. We select about 200 images that approxi-
mately meet the assumptions of our method, manually label
their wireframes, and register them with the rough 3D depth.
In our experiments, we pretrain our network on the SU3
dataset, and then use 2/3 of the real images to ï¬netune the
model. The remaining 1/3 is for testing.

4. Experiments

We conduct extensive experiments to evaluate our method
and validate the design of our pipeline with ablation studies.
In addition, we compare our method with the state-of-the-
art 2D wireframe extraction approaches. We then evaluate

3https://www.cgchan.com/

the performance of our vanishing point estimation and depth
reï¬nement steps. Finally, we demonstrate the examples of
our 3D wireframe reconstruction.

4.1. Implementation Details

Our backbone is a two-stack hourglass network [22].
Each stack consists of 6 stride-2 residual blocks and 6 nearest
neighbour upsamplers. After the stacked hourglass feature
extractor, we insert diï¬€erent â€œheadâ€ modules for each map.
Each head contains a 3 Ã— 3 convolutional layer to reduce
the number of channels followed by a 1 Ã— 1 convolutional
layer to compute the corresponding map. For vanishing
point regression, we use a diï¬€erent head with two consecu-
tive stride-2 convolution layers followed by a global average
pooling layer and a fully-connected layer to regress the po-
sition of the vanishing points.

During the training, the ADAM [15] optimizer is used.
The learning rate and weight decay are set to 8 Ã— 10âˆ’4
and 1 Ã— 10âˆ’5. All the experiments are conducted on four
NVIDIA GTX 1080Ti GPUs, with each GPU holding 12
mini-batches. For the SceneCity Urban 3D dataset, we train
our network for 25 epochs. The loss weights are set as
ğœ† ğ½ = 2.0, ğœ†ğ‘¶ = 0.25 ğœ†ğ¸ = 3.0, and ğœ† D = 0.1 so that all the
loss terms are roughly equal. For the real-world dataset, we
initialize the network with the one trained on the SU3 dataset
and use a 10âˆ’4 learning rate to train for 5 epochs. We hori-
zontally ï¬‚ip the input image as data-augmentation. Unless
otherwise stated, the input images are cropped to 512 Ã— 512.
The ï¬nal output is of stride 4, i.e., with size 128 Ã— 128.
During heatmap vectorization, we use the hyper-parameter
ğœ—ğ¶ = 0.2, ğœ—ğ‘‡ = 0.3, and ğœ—ğ¸ = 0.65.

4.2. Evaluation Metrics

We use the standard AP (average precision) from object
detection [6] to evaluate our junction prediction results. Our
algorithm produces a set of junctions and their associated
scores. The prediction is considered correct if its â„“2 distance
to the nearest ground truth is within a threshold. By this cri-
terion, we can draw the precision-recall curve and compute
the mean AP (mAP) as the area under this curve averaging
over several diï¬€erent thresholds of junction distance.

In our implementation, mAP is averaged over thresh-
olds 0.5, 1.0, and 2.0. In practical applications, long edges
between junctions are typically preferred over short ones.
Therefore, we weight the mAP metric by the sum of the
length of the lines connected to that junction. We use APğ¶
and APğ‘‡ to represent such weighted mAP metric for C-
junctions and T-junctions, respectively. We use the inter-
section over union (IoU) metric to evaluate the quality of
line heatmaps. For junction depth map, we evaluate it on
the positions of the ground truth junctions with the scale
invariant logarithmic error (SILog) [3, 8].

supervisions
ğ¸
ğ‘¶

ğ½
CE â„“1 â„“2 CE SILog Ord APğ¶ APğ‘‡ IoUğ¸ SILog

D

D

ğ½

metrics
ğ¸

(a) (cid:88)
(b) (cid:88) (cid:88)
(c) (cid:88)
(d)
(e) (cid:88)
(f)
(g)
(h) (cid:88)

(cid:88)

(cid:88)

(cid:88) (cid:88)

(cid:88) (cid:88)

65.4 57.1
69.3 55.8
72.8 60.1

/

/

74.3 61.0

/
/

/
/

74.4 61.2

/
/
/

73.3

74.2

/
/
74.3

/
/
/

/

/

3.59
4.14
3.04

(cid:88)
(cid:88)

(cid:88)

(cid:88)

Table 1. The columns under â€œsupervisionsâ€ indicate what losses and
supervisions are used during training; the columns under â€œmetricsâ€œ
indicate the performance given such supervision during evaluation.
The second row shows the symbols of the feature maps; the third
row shows the loss function names of the corresponding maps.
â€œCEâ€ stands for the cross entropy loss, â€œSILogâ€ loss is proposed
by [3], and â€œOrdâ€ represents the ordinary loss in [16]. â€œ/â€ indicates
that the maps are not generated and thus not evaluable.

4.3. Ablation on Joint Training and Loss Functions

We run a series of experiments to investigate how diï¬€er-
ent feature designs and multi-task learning strategies aï¬€ect
the wireframe detection accuracy. Table 1 presents our abla-
tion studies with diï¬€erent combinations of tasks to research
the eï¬€ects of joint training. We also evaluate the choice of
â„“1- and â„“2-losses for oï¬€set regression and the ordinary loss
[16] for depth estimation. We conclude that:

1. Regressing oï¬€set is signiï¬cantly important for local-
izing junctions (7.4 points for APğ¶ and 3 points for
APğ‘‡ ), by comparing rows (a-c). In addition, â„“2 loss is
better than â„“1 loss, probably due to its smoothness.
2. Joint training junctions and lines improve in both tasks.
Rows (c-e) show improvements with about 1.5 points in
APğ¶ , and 0.9 point in APğ‘‡ and line IoU. This indicates
the tight relation between junctions and lines.

3. For depth estimation, we test the ordinal loss from [16].
To our surprise, it does not improve the performance on
our dataset (rows (f-g)). We hypothesis that this is be-
cause the relative orders of sparsely annotated junctions
are harder to predict than the foreground/background
relationship in [16].

4. According to rows (f) and (h), joint training with junc-
tions and lines slightly improves the performance of
depth estimation by 0.55 SILOG point.

4.4. Comparison with 2D Wireframe Extraction

One recent work related to our system is [12], which
extracts 2D wireframes from single RGB images. However,
it has several fundamental diï¬€erences from ours: 1) It does
not diï¬€erentiate between corner C-junctions and occluding

Figure 3. Comparison with [12] on 2D wireframe detection. We
improve the baseline method by 4 points.

avg[ğ¸V] med[ğ¸V]
2.69â—¦
4.65â—¦

1.55â—¦
0.14â—¦

avg[ğ¸f] med[ğ¸f]
2.3%
4.02% 1.38%
12.40% 0.21% 20.0%

failures

Ours
[4, 26]

Table 2. Performance comparison between our method and LSD/J-
linkage [4, 26] for vanishing point detection. ğ¸V represents the
angular error of ğ‘½ğ‘– in degree, ğ¸f represents the relative error of
the recovered camera focal lengths, and â€œfailuresâ€ represents the
percentage of cases whose ğ¸V > 8â—¦.

T-junctions. 2) Its outputs are only 2D wireframes while
ours are 3D. 3) It trains two separated networks for detecting
junctions and lines. 4) It detects texture lines while ours only
detects geometric wireframes.

In this experiment, we compare the performance with
[12]. The goal of this experiment is to validate the impor-
tance of joint training. Therefore we follow the exact same
training procedure and vectorization algorithms as in [12]
except for the uniï¬ed objective function and network struc-
ture. Figure 3 shows the comparison of precision and recall
curves evaluated on the test images, using the same evalu-
ation metrics as in [12]. Note that due to diï¬€erent network
designs, their model has about 30M parameters while ours
only has 19M. With fewer parameters, our system achieves
4-point AP improvement over [12] on the 2D wireframe
detection task.

As a sanity check, we also train our network separately
for lines and junctions, as shown by the green curve in
Figure 3. The result is only slightly better than [12]. This
experiment shows that our performance gain is from jointly
trained objectives instead of neural network engineering.

4.5. Vanishing Points and Depth Reï¬nement

In Section 2.4, vanishing point estimation and depth re-
ï¬nement are used in the last stage of the 3D wireframe
representation. Their robustness and precision are critical
to the ï¬nal quality of the system output. In this section, we
conduct experiments to evaluate the performance of these

(a) Ground truth

(b) Before reï¬nement

(c) After reï¬nement

Figure 4. (b) shows a rendering of the wireframe from Ëœğ‘§ğ’— from a
slightly diï¬€erent view, while (c) shows the wireframe improved by
the optimization in Section 2.4.2.

methods.

For vanishing point detection, Table 2 shows the per-
formance comparison between our neural network-based
method and the J-Linkage clustering algorithm [26, 25] with
the LSD line detector [4] on the SU3 dataset. We ï¬nd that
our method is more robust in term of the percentage of
failures and average error, while the traditional line cluster
algorithm is more accurate when it does not fail. This is
because LSD/J-linkage applies a stronger geometric prior,
while the neural network learns the concept from the data.
We choose our method for its simplicity and robustness, as
the focus of this project is more on the 3D wireframe repre-
sentation side, but we believe the performance can be further
improved by engineering a hybrid algorithm or designing a
better network structure.

We also compare the error of the junction depth before
and after depth reï¬nement in term of SILog. We ï¬nd that on
65% of the testing cases, the error is smaller after the reï¬ne-
ment. This shows that the geometric constraints from van-
ishing points does help improve the accuracy of the junction
depth in general. As shown in Figure 4, the depth reï¬nement
also improves the visual quality of the 3D wireframe. On
the other hand, the depth reï¬nement may not be as eï¬€ective
when the vanishing points are not precise enough, or the
scene is too complex so that there are many erroneous lines
in the wireframe. Some failure cases can be found in the
supplementary material.

4.6. 3D Wireframe Reconstruction Results

We test our 3D wireframe reconstruction method on both
the synthetic dataset and the real images. Examples illustrat-
ing the visual quality of the ï¬nal reconstruction are shown
in Figures 5 and 6. A video demonstration can be found
in http://y2u.be/l3sUtddPJPY. We do not show the
ground truth 3D wireframes for the real landmark dataset
due to its incomplete depth maps.

Acknowledgement

This work is partially supported by Sony US Research
Center, Adobe Research, Berkeley BAIR, and Bytedance
Research Lab.

0.40.50.60.70.80.9Recall0.20.30.40.50.60.70.80.9Precisionf=0.4f=0.5f=0.5f=0.6f=0.7f=0.8PR Curve for AP and f-measure(AP=67.5, f=72.1) [12](AP=67.8, f=72.6) ours-sep(AP=71.0, f=74.3) ours-jointGround truth 2D Our Inferred 2D [12] Inferred 2D

Ground truth 3D

Inferred 3D

Ground truth 3D

Inferred 3D

Figure 5. Left group: comparison of 2D results between the ground truth (column 1), our predictions (column 2), and the results from
wireframe parser [12] (column 3). Middle (columns 4-5) and right groups (columns 6-7): novel views of the ground truths and our
reconstructions to demonstrate the 3D representation of the scene. The color of the wireframes visualizes depth.

Ground truth

Inferred

Novel views

Ground truth

Inferred

Novel views

Figure 6. Test results on real images from MegaDepth.

References

[1] Marco Bertamini, Mai Helmy, and Daniel Bates. The vi-
sual system prioritizes locations near corners of surfaces (not
just locations near a corner). Attention, Perception, & Psy-
chophysics, 75(8):1748â€“1760, Nov 2013. 1

[2] James M Coughlan and Alan L Yuille. Manhattan world:
Compass direction from a single image by bayesian inference.
In ICCV, volume 2, pages 941â€“947, 1999. 2

[3] David Eigen, Christian Puhrsch, and Rob Fergus. Depth
map prediction from a single image using a multi-scale deep
network. In NIPS, 2014. 4, 6

[4] Jakob Engel, Thomas SchÃ¶ps, and Daniel Cremers. LSD-
SLAM: Large-scale direct monocular slam. In ECCV. 2014.
2, 7

[5] S. M. Ali Eslami, Danilo Jimenez Rezende, Frederic Besse,
Fabio Viola, Ari S. Morcos, Marta Garnelo, Avraham Ruder-
man, Andrei A. Rusu, Ivo Danihelka, Karol Gregor, David P.
Reichert, Lars Buesing, Theophane Weber, Oriol Vinyals,
Dan Rosenbaum, Neil Rabinowitz, Helen King, Chloe Hillier,
Matt Botvinick, Daan Wierstra, Koray Kavukcuoglu, and
Demis Hassabis. Neural scene representation and rendering.
Science, 2018. 2

[6] Mark Everingham, Luc Van Gool, Christopher KI Williams,
John Winn, and Andrew Zisserman. The Pascal visual object
classes (VOC) challenge. International journal of computer
vision, 88(2):303â€“338, 2010. 6

[7] Yasutaka Furukawa, Brian Curless, Steven M Seitz, and
Richard Szeliski. Manhattan-world stereo. In CVPR, 2009.
2

[8] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel
Urtasun. Vision meets robotics: The KITTI dataset. The
International Journal of Robotics Research, 2013. 6

[9] Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan
Russell, and Mathieu Aubry. AtlasNet: A papier-mÃ¢chÃ© ap-
proach to learning 3D surface generation. In CVPR, 2018.
2

[10] Manuel Hofer, Michael Maurer, and Horst Bischof. Eï¬ƒcient
3D scene abstraction using line segments. Computer Vision
and Image Understanding, Apr. 2017. 2

[11] Jingwei Huang, Angela Dai, Leonidas Guibas, and Matthias
Niessner. 3Dlite: Towards commodity 3D scanning for con-
tent creation. ACM Trans. Graph., 2017. 1

[12] Kun Huang, Yifan Wang, Zihan Zhou, Tianjiao Ding,
Shenghua Gao, and Yi Ma. Learning to parse wireframes
in images of man-made environments. In CVPR, 2018. 2, 5,
6, 7, 8

[13] Shahram Izadi, David Kim, Otmar Hilliges, David
Molyneaux, Richard Newcombe, Pushmeet Kohli, Jamie
Shotton, Steve Hodges, Dustin Freeman, Andrew Davison,
et al. KinectFusion: real-time 3D reconstruction and inter-
action using a moving depth camera. In Proceedings of the
24th annual ACM symposium on User interface software and
technology, pages 559â€“568. ACM, 2011. 1

[14] Michael Kazhdan, Matthew Bolitho, and Hugues Hoppe.
Poisson surface reconstruction. In Proceedings of the fourth
Eurographics symposium on Geometry processing, volume 7,
2006. 1

[15] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980,
2014. 6

[16] Zhengqi Li and Noah Snavely. MegaDepth: Learning single-
In Computer

view depth prediction from internet photos.
Vision and Pattern Recognition (CVPR), 2018. 5, 6

[17] Zhengqi Li and Noah Snavely. MegaDepth: Learning single-
view depth prediction from internet photos. In CVPR, 2018.
5

[18] Chen Liu, Jimei Yang, Duygu Ceylan, Ersin Yumer, and Yasu-
taka Furukawa. PlaneNet: Piece-wise planar reconstruction
from a single RGB image. In CVPR, 2018. 2

[19] William E Lorensen and Harvey E Cline. Marching cubes:
A high resolution 3D surface construction algorithm.
In
ACM siggraph computer graphics, volume 21, pages 163â€“
169. ACM, 1987. 1

[20] Yi Ma, Stefano Soatto, Jana Kosecka, and S. Shankar Sas-
try. An Invitation to 3D Vision: From Images to Geometric
Models. SpringerVerlag, 2003. 5

[21] RaÃºl Mur-Artal, JMM Montiel, and Juan D TardÃ³s. ORB-
SLAM: A versatile and accurate monocular SLAM system.
IEEE Transactions on Robotics, 2015. 2

[22] Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hour-
glass networks for human pose estimation. In ECCV, 2016.
3, 6

[23] Srikumar Ramalingam and Matthew Brand. Lifting 3D man-
hattan lines from a single image. In Proceedings of the IEEE
International Conference on Computer Vision, pages 497â€“
504, 2013. 2

[24] Shuran Song, Fisher Yu, Andy Zeng, Angel X. Chang, Mano-
lis Savva, and Thomas Funkhouser. Semantic scene comple-
tion from a single depth image. In CVPR, 2017. 2

[25] Jean-Philippe Tardif. Non-iterative approach for fast and
accurate vanishing point detection. In 2009 IEEE 12th Inter-
national Conference on Computer Vision, pages 1250â€“1257.
IEEE, 2009. 7

[26] Roberto Toldo and Andrea Fusiello. Robust multiple struc-
tures estimation with J-linkage. In European conference on
computer vision, pages 537â€“547. Springer, 2008. 7

[27] Xinggang Wang, Kaibing Chen, Zilong Huang, Cong Yao,
and Wenyu Liu. Point linking network for object detection.
arXiv, 2017. 4

[28] Jiajun Wu, Tianfan Xue, Joseph J Lim, Yuandong Tian,
Joshua B Tenenbaum, Antonio Torralba, and William T Free-
In European
man. Single image 3D interpreter network.
Conference on Computer Vision, pages 365â€“382. Springer,
2016. 2

[29] Fengting Yang and Zihan Zhou. Recovering 3D planes from
a single image via convolutional neural networks. In ECCV,
2018. 2

[30] Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao.
Joint face detection and alignment using multitask cascaded
IEEE Signal Processing Letters,
convolutional networks.
23(10):1499â€“1503, 2016. 2

[31] Yinda Zhang, Shuran Song, Ersin Yumer, Manolis Savva,
Joon-Young Lee, Hailin Jin, and Thomas Funkhouser.
Physically-based rendering for indoor scene understanding
using convolutional neural networks. In CVPR, 2017. 2

[32] Zhengdong Zhang, Arvind Ganesh, Xiao Liang, and Yi Ma.
International

Tilt: Transform invariant low-rank textures.
journal of computer vision, 99(1):1â€“24, 2012. 2

[33] Alois Zingl. A rasterizing algorithm for drawing curves,

2012. 4

[34] Chuhang Zou, Alex Colburn, Qi Shan, and Derek Hoiem.
LayoutNet: Reconstructing the 3D room layout from a single
RGB image. In CVPR, 2018. 2

