0
2
0
2

r
a

M
4

]

O
R
.
s
c
[

2
v
8
3
1
0
1
.
7
0
9
1
:
v
i
X
r
a

IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JANUARY, 2020

1

Reﬂective-AR Display: An Interaction Methodology
for Virtual-to-Real Alignment in Medical Robotics

Javad Fotouhi1,2 Tianyu Song2 Arian Mehrfard2 Giacomo Taylor1,2,4 Qiaochu Wang2 Fengfan Xian2 Alejandro
Martin-Gomez3 Bernhard Fuerst4 Mehran Armand5 Mathias Unberath1,2 Nassir Navab1,2,3

reduction. Robotic systems can augment the surgeon’s abilities
with stereo endoscopic imaging and intuitive control which
help the surgeon’s hand-eye coordination and reduce physical
workload during surgery [1]. Furthermore, robotic surgery has
beneﬁts over traditional laparoscopic techniques with patients
experiencing reduced blood loss and shorter post-operative
hospital stays [2].

Abstract—Robot-assisted minimally invasive

surgery has
shown to improve patient outcomes, as well as reduce com-
plications and recovery time for several clinical applications.
While increasingly conﬁgurable robotic arms can maximize reach
and avoid collisions in cluttered environments, positioning them
appropriately during surgery is complicated because safety reg-
ulations prevent automatic driving. We propose a head-mounted
display (HMD) based augmented reality (AR) system designed to
guide optimal surgical arm set up. The staff equipped with HMD
aligns the robot with its planned virtual counterpart. In this user-
centric setting, the main challenge is the perspective ambiguities
hindering such collaborative robotic solution. To overcome this
challenge, we introduce a novel registration concept for intuitive
alignment of AR content to its physical counterpart by providing
a multi-view AR experience via reﬂective-AR displays that
simultaneously show the augmentations from multiple viewpoints.
Using this system, users can visualize different perspectives
while actively adjusting the pose to determine the registration
transformation that most closely superimposes the virtual onto
the real. The experimental results demonstrate improvement
in the interactive alignment of a virtual and real robot when
using a reﬂective-AR display. We also present measurements
from conﬁguring a robotic manipulator in a simulated trocar
placement surgery using the AR guidance methodology.

Index Terms—Surgical Robotics: Laparoscopy, Computer Vi-

sion for Medical Robotics, Augmented Reality.

I. INTRODUCTION

R OBOTIC-ASSISTED minimally invasive surgery is be-

coming increasingly common due to its associated ben-
include higher accuracy, and tremor and fatigue

eﬁts that

Manuscript received: 09, 10, 2019; Revised 12, 21, 2019; Accepted 01, 17,

2020.

This paper was recommended for publication by Editor Pietro Valdastri

upon evaluation of the Associate Editor and Reviewers’ comments.

1 J. Fotouhi, G. Taylor, M. Unberath, and N. Navab are with the De-
partment of Computer Science, Johns Hopkins University, 3400 N. Charles
Street, Baltimore, USA javad.fotouhi@jhu.edu, gzt@jhu.edu,
unberath@jhu.edu, nassir.navab@jhu.edu

2 J. Fotouhi, T. Song, A. Mehrfard, Q. Wang, F. Xian, G. Taylor, M.
Unberath, and N. Navab are with the Laboratory for Computer Aided
Medical Procedures, Johns Hopkins University, 3400 N. Charles Street,
Baltimore, USA javad.fotouhi@jhu.edu, tsong11@jhu.edu,
amehrfa1@jhu.edu, qwang85@jhu.edu, fxian1@jhu.edu,
gzt@jhu.edu, unberath@jhu.edu, nassir.navab@jhu.edu

3 A. Martin-Gomez and N. Navab are with the Laboratory for
Computer Aided Medical Procedures, Technical University of Mu-
nich, 3 Boltzmannstr, Munich, Germany alejandro.martin@tum.de,
nassir.navab@jhu.edu
and B.
Inc.,
are with Verb
Pkwy, Mountain View, USA gzt@jhu.edu,

4 G. Taylor
2450 Bayshore
benfuerst@verbsurgical.com

Surgical

Fuerst

5 M. Armand is with the Applied Physics Laboratory, Johns Hop-
kins University, 11100 Johns Hopkins Road, Laurel, Maryland, USA
mehran.armand@jhuapl.edu

Digital Object Identiﬁer (DOI): see top of this page.

Fig. 1. AR-assisted robot arm positioning

Quick and accurate set up of robotic systems leading up to
surgery remains a major challenge in the endeavor of making
robotic surgery the standard of care. After a patient has been
positioned, anesthetized, and trocars inserted, the robotic arms
must be positioned and docked before operation can begin.
This procedure is a crucial step of workﬂow and grows more
complex the more joints the robotic arms have. While many
different conﬁgurations of the robot’s joints may allow the
robotic arm to dock, sub-optimal positioning increases the like-
lihood of collisions and inadequate reach during teleoperation.
Any repositioning or undocking necessary to adjust the robot
signiﬁcantly decreases operating room efﬁciency [3]. For many
procedures including minimally invasive gastrectomy, ”junk
time”, the time taken to set up or reposition robotic arms, is
often the sole reason for increased procedure time in robotic
procedures over purely laparoscopic approach [4].

Optimal set up of robotic arms, consequently, is critical

 
 
 
 
 
 
2

IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JANUARY, 2020

to increasing the efﬁciency of robotic surgery and foster
acceptance. Due to safety and regulatory concerns, having a
robot automatically drive itself to a pre-operative position is
infeasible. In modern surgical robotic systems, the set up of the
arms can be supported by lasers, as shown in Fig. 2. Though
lasers assist the staff in aligning the robot, it can still exhibit
challenges in a complex system with joint redundancies, as
it does not directly show the desired conﬁguration of all
joints. As most manual methods are error prone and induce a
steep learning curve to operating room staff unfamiliar with
a system, we investigate an augmented reality (AR) solution
for guidance during robotic set up. Using an optical see-
through head-mounted display (OST-HMD), setup staff can be
interactively guided through joint-by-joint steps to optimally
position the robot in an efﬁcient manner.

The works by Qian et al. are similar to our solution in the
spirit of using AR for robotic surgery [5], [6], [7]; however, the
focus of their works were on optimal instrument insertion and
manipulation by showing the extension of the arms inside the
abdomen using AR. Our methodology addresses the alignment
of robot arms for optimal reach and minimum collision. Other
early applications of AR in robot-assisted surgery focused
on multi-modal registration of medical imaging data with the
endoscopic view [8], [9].

Several studies have discussed the challenges of aligning
virtual and real objects, and have emphasized the importance
of this step for various room-scale and spatially-aware AR
solutions [10], [11], [12], [13]. Nuernberger et al. suggested a
semi-automatic alignment strategy to register virtual and real
spaces [14]. Their work relied on scene content and environ-
ment constraints such as edges and surfaces for snapping the
virtual content to real. In a different study, various rendering
and visualization techniques were compared for alignment of
different virtual models in fully immersive environments [15].
Results indicated that static visualization techniques which
exhibited lower occlusion in a single view yielded better
alignment.

In order to properly augment virtual assistance on a physical
robot, we require an intuitive and fast approach to align the
AR environment provided by a OST-HMD to the robot. This
registration must be robust to perceptual ambiguities that arise
during AR alignment [16], [17]. To this end, we propose
virtual-real active alignment (ViRAAl) to register a virtual
model of the robot to its real counterpart. Our method enables
the user to create and view multiple AR mirrors which show
the current 3D scene (including real and virtual robot) from
different viewpoints. By providing this overlay from multiple
perspectives simultaneously, users can actively adjust the 6
degree-of-freedom (DOF) transformation parameters that best
align the virtual and real objects in all views.

We summarize the contributions of the present work as
1) reﬂective-AR displays as the multi-view and marker-free
paradigm for co-registration between the virtual and real
spaces, hence enabling spatially-aware AR, and 2) using AR
for assistance during robot set up (Fig. 1).

Fig. 2. Da Vinci Xi surgical robot uses multiple lasers to assist the positioning
and docking of the robotic arms.

Fig. 3. ViRAAl strategy estimates the virtuality to reality transformation
RTV.

II. METHODOLOGY

An important step in many AR scenarios is to bring the
virtual content that lives in a controlled environment into align-
ment with the physical reality that is present in the unmodeled
environment [16]. In this work, to enable seamless interaction
of a surgical robot manipulator and its virtual representation
during an AR experience, we introduce reﬂective-AR displays
that enable multi-view visualization and interactive alignment
of virtual and real objects. In Sec. II-A, we present the problem
formulation for registering virtual-to-real. A key contribution
is presented in
of this work, which is the AR reﬂectors,

Origin of the virtualworldOrigin of the realworldRTVFOTOUHI et al.: REFLECTIVE-AR

3

B. Reﬂective-AR Display

Due to the projective property of human visual system and
the differences in perceptual cues in virtuality and reality,
the scale and depth between real and virtual objects are
easily misjudged [17]. To overcome depth ambiguities and
enhance 3D perception during an AR experience, we introduce
reﬂective-AR displays that allow simultaneous visualization
of the scene from various viewpoints. The reﬂective-AR
displays shown in Fig. 4 are constructed by displaying images
from the integrated camera sensor of the OST-HMD as if
the user observed the real scene from different viewpoints
simultaneously, and are augmented with the projections of
the 3D virtual objects. To compute a geometrically relevant
pose for displaying these images, we compute the associated
observer poses to the coordinate frame of the AR scene in the
operating room (OROR,O tOR) via simultaneous-localization
and mapping (SLAM).

The observer imaging geometry in Fig. 5 is formulated as:
(cid:20)OROR
0(cid:62)

Po = Ko P

OtOR
1

(4)

(cid:21)

,

where Ko is the matrix of intrinsic parameters and P is the
projection operator. Next, to simulate a mirror-like view, we
construct a reversed frustum as (Fig. 5):







Rx(π)

0(cid:62)





0
0
D
1











,

(5)

Pm = Km P

(cid:20)OROR
0(cid:62)

(cid:21)

OtOR
1

Km =


1
0
0 −1

0
0



 Ko.

0
0
1

In Eq. 5, the optical center of the observer frustum is rotated
by the amount π around the x axis, and translated by the
amount D along the principle ray of the frustum. Distance D
is approximated as the Euclidean distance between the camera
center, and an arbitrary point on the surface of the robot that
is acquired by colliding the gaze cursor with the spatial map
of the AR scene. The distance is merely used as a reference to
position the optical center of the reversed frustum, and does
not affect the rendering content in the reﬂective-AR display.
To compute the reversed frustum’s intrinsic matrix Km, the y-
axis of the image plane is ﬂipped according to Eq. 5. Lastly, to
give rise to a mirror-like AR display, the 3D virtual structures
are projected into the image plane with the imaging geometry
Pm, thus, enabling joint visualization of real and virtual in the
reﬂective display.

Since the reﬂective AR displays are constructed based on
the imaging geometry of the observer frustum, we adopted the
same axis convention used in the computer vision community.
We set the z-axis in the direction pointing away from the
camera, along the principal ray, connecting the origin to the
principal point on the image plane.

Fig. 4. Reﬂective-AR displays enable simultaneous alignment from multiple
views.

Sec. II-B. Finally,
in Sec. II-C, we suggest AR guidance
to facilitate robot set up during surgical interventions. It is
important to note that in Sec. II-A and II-B we discuss the
problem of ”virtual-to-real” alignment to enable spatially-
aware AR, and in Sec. II-C we discuss ”real-to-virtual”
alignment to provide spatially-aware AR guidance.

A. Virtual-Real Active Alignment (ViRAAl)

To estimate the virtual-to-real 6 DOF alignment shown in
Fig. 3, we estimate the transformation RTV = ( ¯R, ¯t) via
interactively registering a robot with its virtual model at N
pre-deﬁned joint conﬁgurations. Each time a rigid-body trans-
formation {(Ri, ti)}N
i=1 is obtained, where (Ri, ti) ∈ SE(3),
and SE is the Special Euclidean group.

We hypothesize that the average transformation computed
from these N estimates, can yield a closer approximation of
the true virtual-to-real alignment compared to each individual
N transformations. Hence, from these N estimates, we seek to
compute the mean rotation and translation. The mean rotation
matrix ¯R is computed on the Special Orthogonal group SO(3)
by minimizing:

arg min
¯R∈SO(3)

N
(cid:88)

i=1

d(Ri, ¯R)2,

(1)

where d(.) denotes a distance function on the Riemannian
manifold. To establish d(.), the rotation matrix is expressed in
the Lie algebra (tangent space) of the Lie group as R = e ˆw.
The tangent space w is then obtained as log(R) = ˆw, such that
ˆw is the skew-symmetric matrix constructed from the vector
w. Consequently, the mean rotation is estimated as [18]:

arg min
¯R∈SO(3)

N
(cid:88)

i=1

(cid:13)
(cid:13)log(R(cid:62)
i

¯R)(cid:13)
2
F ,
(cid:13)

(2)

where (cid:107).(cid:107)2
computed in Euclidean space as:

F is the Frobenius norm. The mean translation ¯t is

¯t =

1
N

N
(cid:88)

i=1

ti.

C. Augmented Reality Assistance for Robot Set Up

(3)

After the registration transformation is established between
real and virtual worlds, a collision-free and safe virtual robot

AR MirrorAR MirrorRTVOR Coordinate SystemVirtualRobotRealRobotM1TORM2TORVTOR4

IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JANUARY, 2020

Fig. 5.

Imaging geometries of the observed and reversed frustums in relation to the robotic manipulator

conﬁguration can be presented to the medical assistant. The
desired conﬁguration can either be estimated via the inverse
kinematics of the robot, or can be adjusted interactively using
the virtual robot and the patient position on the surgical bed.
The robot set up is then performed in joint-by-joint steps,
following the virtual planning.

III. EXPERIMENTAL RESULTS

We ﬁrst evaluate the ViRAAl strategy using a virtual and
real robotic manipulator. Next, in a simulated surgical setup,
we assess the errors in moving the robot joints to achieve a
desired joint conﬁguration using AR guidance where a trocar
must be inserted at a mannequin’s umbilicus (Fig. 6). The
umbilicus is commonly chosen as a robotic port and remote
center of motion (RCM) for abdominal surgery. Training for
port and trocar placement in umbilicus and optimal docking
of the robot has a steep learning curve [19], [20].

A. System and Design

For the experiments we used a 7 DOF KUKA LBR In-
telligent Industrial Work Assistant (iiwa) 7 R800 redundant
robot manipulator (KUKA AG, Augsburg, Germany). The
joint conﬁguration and end-effector pose of the robot was
obtained through a ROS interface [21]. It is important to note
that our solution is designed to address challenges in surgical
settings, and the KUKA arm was merely used as an exemplary
robot that was available for this research. The AR environment
was delivered by a ﬁrst-generation Microsoft HoloLens OST-
HMD (Microsoft, Redmond, WA).

each iteration we located three pairs of distinct 3D landmarks
on the surfaces of both real and virtual robots. These points
were identiﬁed interactively by intersecting rays from the OST-
HMD to the landmark using a gaze cursor.

We deﬁne each ray i using the position of the user’s head hi,
and the unit direction vector ui from the head to the annotated
landmark on the spatial map of the environment. The landmark
x∗
i is estimated in a least-squares fashion from the intersection
of two rays as:

x∗

i = arg min
x∈R3

2
(cid:88)

i=1

(cid:107)(I3 − uiu(cid:62)

i )(x − hi)(cid:107)2.

(6)

To quantify the error of our ground-truth measurement
mechanism using 3D landmarks, we computed the Euclidean
distance between different sets of targets on an optical table
for a total of 12 times. We selected four combinations of land-
marks which were 5 cm, 10 cm, 15 cm, and 20 cm apart. The
average error for measuring distances using AR annotations
was 3.6 mm.

For virtual-to-real object alignment, the results in Table I
indicate a total error of 16.5 ± 11.0 mm when using the
reﬂective-AR display, and 30.2 ± 23.9 mm when using AR
without the additional mirror view. To demonstrate the change
in alignment error when averaging multiple alignment trans-
formations on the SE(3) manifold as presented in Sec. II-A,
we computed the average transformation given Eq. 2 and Eq. 3
when using the AR reﬂective display. This experiment yielded
a total error of 11.3 ± 1.01 mm, which is lower than each
individual alignment trial.

B. Alignment of Virtual-to-Real

C. Augmented Reality for Robot Set Up: Accuracy Analysis

The ViRAAl strategy is evaluated by aligning the virtual
and real robots with and without a reﬂective-AR display. Each
experiment is repeated 10 times for 4 users. The error mea-
surements are presented in Table I. We did not incorporate an
external marker-based tracking approach as the base-line since
marker tracking exhibits high errors due to propagation, and
does not include the user in the loop, i.e. it only determines the
registration error and not the augmentation error in the user’s
view. Instead, to quantify the amount of misalignment, for

During a simulated robot-assisted trocar placement, U =
8 users moved the robot joints to achieve different target
joint conﬁgurations. Each user performed this task with the
guidance from 1) AR, 2) AR with reﬂective display, and
3) interactive ﬁxed display, all in randomized orders. The
ﬁxed external monitor was used as a non-immersive baseline,
displaying the desired robot conﬁgurations to the user at all
time during the execution, and allowing the users to interact
with the visualization by rotating or scaling (Fig. 7). The users

Reversed FrustumObserved FrustumzxyOptical center of the observed viewOptical center of the reflected viewRx(π)Axis flipDFOTOUHI et al.: REFLECTIVE-AR

5

Fig. 6. During the surgical AR experience, the virtual model of the robot is ﬁrst visualized at a known conﬁguration (a). The alignment between the real and
virtual is established in multiple views via reﬂective-AR displays (b). Once the 6 DOF rigid-body transformation is identiﬁed between the real and virtual
content, a virtual robot is rendered into the scene at a safe surgical conﬁguration (c). The robot assistant can then align the robot with the virtual counterpart
(d), and dock it to the trocar (e).

TABLE I
MEAN AND STANDARD DEVIATION OF MISALIGNMENT ERRORS IN MM.

Alignment Method
ViRAAl
ViRAAl + Reﬂective-AR Display

(tx, σtx )
(17.4, 16.1)
(9.00, 5.64)

(ty, σty )
(11.9, 6.24)
(10.3, 7.45)

(tz, σtz )
(21.6, 16.5)
(9.18, 5.77)

(cid:13)t(cid:13)
((cid:13)
(cid:13)2 , (cid:107)σt(cid:107)2)
(30.2, 23.9)
(16.5, 11.0)

Fig. 7.

Interactive ﬁxed display demonstrating two different target joint conﬁgurations

performed each test three times, and each time with a different
target joint. To set up AR we used the average Euclidean
transformation over N = 3 trials as described in Eq. 2 and
Eq. 3. Errors in joint angles are demonstrated in Fig. 8. The
total errors are shown in Fig. 9. The violin plots show the
distribution of error within each guidance method.

highlight the errors for twisting and revolving joints. A joint is
characterized as twisting if the axis of rotation is parallel to the
robot link, and revolving if the axis of rotation is orthogonal
to the robot link.

D. Augmented Reality for Robot Set Up: Time Analysis

The statistics for the overall error in joint positioning using
each guidance techniques is compared in Table II. Statistical
signiﬁcance measures are shown in Table III. In Fig. 10 we
present a comparison between the errors from each joint, and

Table IV presents the observed time for all eight users.
Guidance using AR and AR reﬂector both required registration
between the virtual and real content. Therefore, each time the
users were given a unique joint conﬁguration target, prior to

a)b)c)d)e)Reflective AR Displays from Two Views6

IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JANUARY, 2020

TABLE II
COMPARISON OF THE ERROR FOR RE-POSITIONING THE ROBOT JOINTS IN
DEGREE UNITS

Joint Error
Reﬂective AR Display
AR
Fixed Display

Mean Median Min Max
188
23.7
197
26.8
249
71.4

0.02
0.00
0.64

4.93
5.88
50.6

Std
41.1
42.4
61.8

Fig. 8. Distribution of errors evaluated for each joint separately when guided
by 1) non-immersive ﬁxed display, 2) AR calibrated without reﬂective-AR
display, and 3) AR calibrated with reﬂective-AR display.

Fig. 9. Total error distribution for all joints using guidance by 1) non-
immersive ﬁxed display, 2) AR calibrated without reﬂective-AR display, and
3) AR calibrated with reﬂective-AR display.

moving the real robot, the user aligned the virtual model of
the robot with its real counterpart three times (cid:8)RTV(i)(cid:9)3
i=1.
Using the ViRAAl approach presented in Sec. II-A,
the
transformation RTV which expressed the geometric mean in
SE(3) was computed. The average transformation was applied
to the AR scene to register the origins of the real and virtual
environments, and enable AR guidance. Table IV also presents
the time required for ViRAAl with and without AR reﬂectors
for all users.

Fig. 10. The plot demonstrates an abstract comparison between the errors
contributed by each joint. Since AR-based approaches yielded substantially
smaller errors, we used Logarithmic scale for optimal visualization and
comparison of errors with different orders of magnitude. Revolving joints
with even indexes are shown in blue, and twisting joints with odd indexes are
shown in red.

IV. DISCUSSION

The experimental results indicated an improved alignment
when using a reﬂective-AR display. The L2-norm average
misalignment error in this case was 16.5 ± 11.0 mm, and
showed improvement compared to 30.2 ± 23.9 mm error when
no reﬂective displays were used. Averaging the transforma-
tions on SE(3) manifold yielded an even lower error of
11.3 ± 1.01 mm. This alignment error does not seem sufﬁcient
for tasks that require high accuracy such as deﬁning biopsy tar-
gets in AR, however seems acceptable for providing intuition
during robotic arm set up. Using more than one reﬂective-
AR display did not improve the alignment due to two main
reasons. First, the limited ﬁeld of view of Microsoft Hololens
prohibited optimal view of multiple mirrors in their frustums
simultaneously when standing in close proximity to the robot.
Second, the poor quality of SLAM-based tracking and the
unreliable spatial map of the HMD resulted in drifts, hence
achieving alignment consensus in all views became challeng-
ing. Larger ﬁeld of view, reliable head tracking, enhanced form
factor, enhanced gesture input, and eye tracking capabilities
can greatly improve the current limitations.

We evaluated the results in Table I based on a novel user-in-
the-loop concept. Alternatively, these errors can be evaluated
by using an external navigation system and ﬁducials. However,
the latter will exclude the user and will only evaluate the reg-
istration error, instead of the augmentation error. Considering
the large improvement (> 45%) reported across multiple trials
in Table I, we expect the conclusion will not change when
using optical navigation.

For modern surgical platforms, the ﬁrst assistants are trained
to set up, dock, tear down, and re-conﬁgure the robot using
extensive pre-operative E-learning or instructor-led training. In

Fixed DisplayAR calibratedAR calibratedw/ single view  w/ reflective-AR050100150200250Joint Angle Error (deg)Joint 1 ErrorMeanMedianFixed DisplayAR calibratedAR calibratedw/ single view  w/ reflective-AR050100150200250Joint Angle Error (deg)Joint 2 ErrorMeanMedianFixed DisplayAR calibratedAR calibratedw/ single view  w/ reflective-AR050100150200250Joint Angle Error (deg)Joint 3 ErrorMeanMedianFixed DisplayAR calibratedAR calibratedw/ single view  w/ reflective-AR050100150200250Joint Angle Error (deg)Joint 4 ErrorMeanMedianFixed DisplayAR calibratedAR calibratedw/ single view  w/ reflective-AR050100150200250Joint Angle Error (deg)Joint 5 ErrorMeanMedianFixed DisplayAR calibratedAR calibratedw/ single view  w/ reflective-AR050100150200250Joint Angle Error (deg)Joint 6 ErrorMeanMedianFixed DisplayAR calibratedAR calibratedwith single view          with reflective-AR display050100150200250Joint Angle Error (deg)Total Joint ErrorMeanMedian210 mm130 mm200 mm200 mm200 mm200 mm126 mmJoint 1:TwistingJoint 2:RevolvingJoint 3:TwistingJoint 4:RevolvingJoint 5: TwistingJoint 6: RevolvingJoint 7:TwistingFOTOUHI et al.: REFLECTIVE-AR

7

TABLE III
P-VALUES FOR EACH INDIVIDUAL JOINT, AS WELL AS FOR ALL JOINTS COMBINED.

P-value
Reﬂective AR Display / AR
Reﬂective AR Display / Fixed Display
AR / Fixed Display

Joint 1
0.61
0.12e−6
0.16e−6

Joint 2
0.50
0.15e−1
0.39e−1

Joint 3
0.23
0.24e−6
0.62e−4

Joint 4
0.71
0.77
0.87

Joint 5
0.59
0.22e−6
0.12e−6

Joint 6
0.27
0.76e−4
0.58e−7

Total
0.54
0.12e−15
0.20e−17

TABLE IV
TIME REQUIRED FOR VIRAAL AND RE-POSITIONING THE ROBOT JOINTS IN MINUTE:SECOND UNITS

Reﬂective AR Display
AR
Fixed Display

Mean Median
1 : 50
2 : 00
1 : 23
1 : 31
1 : 35
1 : 34

Execution Time
Min
0 : 39
0 : 22
0 : 27

Max
3 : 47
5 : 06
3 : 20

Std
0 : 50
0 : 59
0 : 48

Mean Median
4 : 42
4 : 32
1 : 52
2 : 29
-
-

Alignment Time
Min
2 : 06
1 : 11
-

Max
6 : 59
4 : 26
-

Std
1 : 41
1 : 12
-

addition to the training that they receive by the manufacturer,
general guidelines, and demonstrations in the form of text
or visualizations are available in the operating room. In our
experiments, to exclude the bias of training, we substituted
the conventional training-based approach by recruiting users
with no background in setting up surgical robots and focused
on demonstrating the effectiveness of intra-operative AR guid-
ance. Comparison with base-line training is a subject of future
work, which requires randomized studies in clinical settings
with larger populations.

The alignment between the real and virtual, which estab-
lishes the registration, is an entirely user-dependent step as
the registration chain implicitly takes into account the internal
relations between the users eyes, AR camera on the headset,
and the AR displays. Since human visual systems are different,
these relations differ, and consequently cannot be done in a
user-out-the-loop setting. Therefore, automatic approaches will
not generalize.

Fig. 9 presented the misalignment errors for bringing the
real robot to a desired conﬁguration. The expected accuracy
for the robotic set up depends on the design, number of
arms, and number of joints of the particular surgical system.
These parameters all vary depending on the surgical use case.
The higher the number of joints and arms, the higher the
chance for collision; therefore, higher accuracy is demanded
for a more complex system. This step during the AR-assisted
workﬂow, which involves the alignment of a real object to
virtual, can in future leverage from multi-view strategies by
using collaborative AR devices or external cameras.

Several AR publications have shown that time-saving of
AR cannot be quantiﬁed immediately with dedicated user
studies [22], partly because of the unfamiliar interface and ex-
posure to additional information. Time-saving only manifests
after the user is proﬁcient with the system. We hypothesize
that while there is overhead in setting up AR, the rate of
failure/collision would drop leading to a net reduction of
overall junk-time. The Riemannian averaging in Eq. 2 and
Eq. 3 can increase this set up time, but is a one-time process
which takes place before intervention and can result in a more
accurate fusion of information and improved AR experience.
We hope AR assistance minimizes the training time and allows
operators to verify and inspect the proper alignment of robot
arms, both quantitatively and visually.

HMD-based AR is challenged by recurrent estimation of
transformations and corrections such as user-to-display and
drift, respectively. There is a great wealth of literature directed
towards addressing these issues [23], which our solution
beneﬁts from as new hardware becomes available. It should
be noted that drift will be quite limited because the working
volume near the patient bed and the robot is restricted.

Our proposed approach enables the co-registration between
the real and virtual spaces and delivers spatially-aware AR. We
also demonstrated the application of ViRAAl for AR guidance
during minimally-invasive robotic surgery. The estimation of
the overall registration greatly beneﬁts from the averaging
strategy presented in Eqs. 1-3, which are suggested to compute
the mean transformation that satisﬁes the properties of SE(3)
manifold. This average estimate is a rigid transformation that
has the shortest distance to all other estimates around the true
pose.

The range of errors exhibited by all intra-operative guidance
methods, particularly by the non-immersive ﬁxed display,
prove the complexity and importance of this problem for robot
manipulation. We computed p-values and compared all pairs
of methods in Table III to identify the most effective assistive
approach. Statistical signiﬁcance was considered if p < 0.05.
Results suggested that guidance using AR with and without
reﬂective display yielded signiﬁcantly lower errors compared
to non-immersive ﬁxed display. The AR guidance approach
using reﬂective displays outperformed the AR system with
no mirrors, however in this comparison statistical signiﬁcance
was not achieved.

In Fig. 10, we compared the error contributed by revolv-
ing and twisting joints separately. Results indicated that re-
positioning of revolving joints in all three guidance methods
are consistently more accurate than the twisting joints. We
hypothesize that the higher error is the result of the inherent
symmetry in twisting motion that may lead to ambiguities.

V. CONCLUSION

In this manuscript we presented a novel multi-view strategy
to align virtual and real content, and demonstrated an appli-
cation of it for improving surgical robotic workﬂows. The
reﬂective-AR displays were introduced to eliminate the 3D
scale ambiguities and improve the AR scene realism. We have
demonstrated an AR interface that accommodates multiple

8

IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JANUARY, 2020

[8] O. Mohareri, C. Schneider, T. K. Adebar, M. C. Yip, P. Black, C. Y.
Nguan, D. Bergman, J. Seroger, S. DiMaio, and S. E. Salcudean,
“Ultrasound-based image guidance for robot-assisted laparoscopic radi-
cal prostatectomy: initial in-vivo results,” in International Conference on
Information Processing in Computer-Assisted Interventions. Springer,
2013, pp. 40–50.

[9] P. Pessaux, M. Diana, L. Soler, T. Piardi, D. Mutter, and J. Marescaux,
“Towards cybernetic surgery: robotic and augmented reality-assisted
liver segmentectomy,” Langenbeck’s archives of surgery, vol. 400, no. 3,
pp. 381–385, 2015.

[10] D. Reiners, D. Stricker, G. Klinker, and S. M¨uller, “Augmented reality
for construction tasks: Doorlock assembly,” Proc. IEEE and ACM IWAR,
vol. 98, no. 1, pp. 31–46, 1998.

[11] S. J. Henderson and S. K. Feiner, “Augmented reality in the psychomotor
phase of a procedural task,” in 2011 10th IEEE International Symposium
on Mixed and Augmented Reality.

IEEE, 2011, pp. 191–200.

[12] S. Feiner, B. Macintyre, and D. Seligmann, “Knowledge-based aug-
mented reality,” Communications of the ACM, vol. 36, no. 7, pp. 53–62,
1993.

[13] T. P. Caudell and D. W. Mizell, “Augmented reality: An application
of heads-up display technology to manual manufacturing processes,”
in Proceedings of the twenty-ﬁfth Hawaii international conference on
system sciences, vol. 2.

IEEE, 1992, pp. 659–669.

[14] B. Nuernberger, E. Ofek, H. Benko, and A. D. Wilson, “Snaptoreality:
Aligning augmented reality to the real world,” in Proceedings of the
2016 CHI Conference on Human Factors in Computing Systems. ACM,
2016, pp. 1233–1244.

[15] A. Martin-Gomez, U. Eck, and N. Navab, “Visualization Techniques for
Precise Alignment in VR. A Comparative Study,” in 2019 IEEE Virtual
Reality (VR).

IEEE, 2019, pp. 0–0.

[16] P. Milgram and D. Drascic, “Perceptual effects in aligning virtual and
real objects in augmented reality displays,” in Proceedings of the Human
Factors and Ergonomics Society Annual Meeting, vol. 41, no. 2. SAGE
Publications Sage CA: Los Angeles, CA, 1997, pp. 1239–1243.
[17] P. Willemsen, A. A. Gooch, W. B. Thompson, and S. H. Creem-Regehr,
“Effects of stereo viewing conditions on distance perception in vir-
tual environments,” Presence: Teleoperators and Virtual Environments,
vol. 17, no. 1, pp. 91–101, 2008.

[18] M. Moakher, “Means and averaging in the group of rotations,” SIAM
journal on matrix analysis and applications, vol. 24, no. 1, pp. 1–16,
2002.

[19] C. Chang, Z. Steinberg, A. Shah, and M. S. Gundeti, “Patient positioning
and port placement for robot-assisted surgery,” Journal of endourology,
vol. 28, no. 6, pp. 631–638, 2014.

[20] P. Iranmanesh, P. Morel, O. J. Wagner, I. Inan, F. Pugin, and M. E.
Hagen, “Set-up and docking of the da vinci R(cid:13) surgical system: prospec-
tive analysis of initial experience,” The international journal of medical
robotics and computer assisted surgery, vol. 6, no. 1, pp. 57–60, 2010.
[21] M. Safeea and P. Neto, “Kuka sunrise toolbox: Interfacing collaborative
robots with matlab,” IEEE Robotics & Automation Magazine, vol. 26,
no. 1, pp. 91–96, 2019.

[22] A. D¨unser and M. Billinghurst, “Evaluating augmented reality systems,”
in Handbook of augmented reality. Springer, 2011, pp. 289–307.
[23] M. Tuceryan, Y. Genc, and N. Navab, “Single-point active alignment
method (spaam) for optical see-through hmd calibration for augmented
reality,” Presence: Teleoperators & Virtual Environments, vol. 11, no. 3,
pp. 259–276, 2002.

[24] J. Fotouhi, M. Unberath, T. Song, W. Gu, A. Johnson, G. Osgood,
M. Armand, and N. Navab, “Interactive ﬂying frustums (iffs): spatially
aware surgical data visualization,” International journal of computer
assisted radiology and surgery, pp. 1–10, 2019.

reﬂective displays, and allows the users to scale the images
within their viewing frustum [24].

The virtual-to-real registration approach, ViRAAl,

is an
interactive and user-speciﬁc method that calibrates the real
and virtual worlds directly to the user’s display. No external
camera or tracking system, other than the HMD itself, is used
in order to keep maximum ﬂexibility and transferability of the
system into different surgical environments.

Seamless overlays of virtual content onto the reﬂective AR
displays are achieved by placing virtual cameras at the optical
centers where the images were acquired, hence allowing to
render virtual and real from an identical imaging geometry.
The reﬂective displays require a static scene, therefore, are
suited for aligning virtual-to-real, and not vice-versa.

The focus of this work is beyond the KUKA robot and its
redundant design; it is instead on complex surgical platforms
with multiple arms and various joints. The task of alignment is
expected to be more difﬁcult for redundant manipulators with
more joints. Nonetheless, we expect AR to provide an effective
guidance mechanism to reconﬁgure complex redundant arms
at the bedside.

Surgical robots are only certiﬁed to be controlled by the
surgeon in its active mode, and due to safety reasons, their
set up by the surgical staff are performed entirely manually.
Our solution is designed around this concept of full manual
interaction. It should be noted that our contribution is not
on computing joint conﬁgurations that minimize collision, but
instead we show that if such conﬁguration exists, then with
the support of AR it can be manually achieved by the ﬁrst
assistant.

A user interface such as the AR reﬂectors, can accelerate
interaction during surgery. By measuring the exact time of the
staff this could be validated in future work. We believe that
the proposed alignment strategy can extend to other realms
of computer-assisted surgery, namely for surgical training and
AR guidance during image-guided therapies.

REFERENCES

[1] R. v. van der Schatte Olivier, C. vant Hullenaar, J. Ruurda, and
I. Broeders, “Ergonomics, user comfort, and performance in standard
and robot-assisted laparoscopic surgery,” Surgical endoscopy, vol. 23,
no. 6, p. 1365, 2009.

[2] J. Jayakumaran, S. D. Patel, B. K. Gangrade, D. M. Narasimhulu, S. R.
Pandian, and C. Silva, “Robotic-assisted laparoscopy in reproductive
surgery: a contemporary review,” Journal of robotic surgery, vol. 11,
no. 2, pp. 97–109, 2017.

[3] R. M. Juza, J. R. Lyn-Sue, and E. M. Pauli, “Intraoperative considera-
tions for robotic repair,” in Laparoscopic and Robotic Incisional Hernia
Repair. Springer, 2018, pp. 103–115.

[4] H. Liu, T. Kinoshita, A. Tonouchi, A. Kaito, and M. Tokunaga, “What
are the reasons for a longer operation time in robotic gastrectomy than
in laparoscopic gastrectomy for stomach cancer?” Surgical Endoscopy,
vol. 33, no. 1, pp. 192–198, 2019.

[5] L. Qian, A. Deguet, Z. Wang, Y.-H. Liu, and P. Kazanzides, “Augmented
reality assisted instrument insertion and tool manipulation for the ﬁrst
in robotic surgery,” in 2019 International Conference on
assistant
Robotics and Automation (ICRA).

IEEE, 2019, pp. 5173–5179.

[6] L. Qian, X. Zhang, A. Deguet, and P. Kazanzides, “Aramis: Augmented
reality assistance for minimally invasive surgery using a head-mounted
display,” in International Conference on Medical Image Computing and
Computer-Assisted Intervention. Springer, 2019, pp. 74–82.

[7] L. Qian, A. Deguet, and P. Kazanzides, “Arssist: augmented reality
on a head-mounted display for the ﬁrst assistant in robotic surgery,”
Healthcare technology letters, vol. 5, no. 5, pp. 194–200, 2018.

