NON-AUTOREGRESSIVE MANDARIN-ENGLISH
CODE-SWITCHING SPEECH RECOGNITION

Shun-Po Chuang†, Heng-Jui Chang†, Sung-Feng Huang, Hung-yi Lee

College of Electrical Engineering and Computer Science, National Taiwan University

1
2
0
2

t
c
O
2

]
L
C
.
s
c
[

3
v
8
5
2
2
0
.
4
0
1
2
:
v
i
X
r
a

ABSTRACT

Mandarin-English code-switching (CS) is frequently used
among East and Southeast Asian people. However,
the
intra-sentence language switching of the two very differ-
ent languages makes recognizing CS speech challenging.
Meanwhile, the recent successful non-autoregressive (NAR)
ASR models remove the need for left-to-right beam decod-
ing in autoregressive (AR) models and achieved outstanding
performance and fast inference speed, but it has not been ap-
plied to Mandarin-English CS speech recognition. This paper
takes advantage of the Mask-CTC NAR ASR framework to
tackle the CS speech recognition issue. We further propose to
change the Mandarin output target of the encoder to Pinyin for
faster encoder training and introduce the Pinyin-to-Mandarin
decoder to learn contextualized information. Moreover, we
use word embedding label smoothing to regularize the de-
coder with contextualized information and projection matrix
regularization to bridge that gap between the encoder and
decoder. We evaluate these methods on the SEAME corpus
and achieved exciting results.

Index Terms— non-autoregressive, code-switching, end-

to-end speech recognition

1. INTRODUCTION

Code-switching (CS) is a phenomenon of using two or more
languages within a sentence in text or speech. This practice is
common in regions worldwide, especially in East and South-
east Asia, where people frequently use Mandarin-English CS
speech in daily conversations. Thus, developing technologies
to recognize this type of speech is critical. Although humans
can easily recognize CS speech, automatic speech recogni-
tion (ASR) technologies nowadays perform poorly since these
systems are primarily trained with monolingual data. More-
over, very few CS speech corpora are publicly available [1–4],
making training high-performance ASR models more chal-
lenging for CS speech.

† Equal contribution.
We thank National Center for High-performance Computing (NCHC) of
National Applied Research Laboratories (NARLabs) in Taiwan for providing
computational and storage resources.

Various approaches were studied to tackle the CS speech
recognition problem,
including language identity recogni-
tion [5–8] and data-augmentation [9–12]. Many prior works
exploited the powerful end-to-end ASR technologies like
listen, attend and spell [13] and RNN transducers [14]. How-
ever, these methods mostly require autoregressive (AR) left-
to-right beam decoding, leading to longer processing time and
higher latency. Some studies also demonstrated that process-
ing each language with its encoder or decoder obtained better
performance [15–17]. This multi-encoder-decoder architec-
ture and AR decoding require signiﬁcantly more computation
time, thus less feasible for real-world applications. There-
fore, in this paper, we leverage non-autoregressive (NAR)
ASR technology to tackle this issue.

Unlike the successful AR ASR models [18, 19], NAR
ASR eliminates the left-to-right dependency by directly pre-
dicting the whole sequence at once or reﬁne the output se-
quence in a constant amount of iterations [20–22]. NAR ASR
can thus exploit parallel computing technologies for faster
inference. This paper primarily adopts Mask-CTC [23, 24], a
NAR ASR framework with a NAR conditional masked lan-
guage model (CMLM) [25] for iterative decoding, possessing
competing performance with AR ASR baselines.

Although Mask-CTC provided exciting results in numer-
ous ASR tasks, recognizing CS speech remains challenging.
The CS task suffers from data scarcity [26, 27], using deep
learning-based methods with millions of parameters like
transformer easily lead to overﬁtting. Thereby, we use word
embedding label smoothing [28] to incorporate additional
textual knowledge, bringing more precise semantic and con-
textual information for better regularization. Second, the
current Mask-CTC framework propagates decoder errors to
the encoder, but we wish to increase the connection between
the two modules for better performance. We suggest con-
straining the encoder’s output projection layer similar to the
input embedding layer of the decoder to bridge the gap be-
tween the encoder and decoder. Furthermore, we propose to
change the encoder’s output target from Chinese characters to
Pinyin symbols [29]. Each Chinese character can be mapped
to at least one Pinyin symbol, representing its pronunciation.
Using Pinyin symbols as the target allows the encoder to
focus on acoustic modeling and reduces the vocabulary size.
Before this paper, Naowarat et al. [30] used contextual-

 
 
 
 
 
 
ized CTC to force NAR ASR to learn contextual informa-
tion in CS speech recognition. In contrast, we utilize CMLM
to better modeling linguistic information. Also, Huang et
al. [31] developed context-dependent label smoothing meth-
ods by removing impossible labels given the previous output
token, while our word embedding label smoothing is more
straightforward and leverages knowledge from the additional
text data. Song et al. [28] proposed word similarity-based la-
bel smoothing very similar to our work. Instead, they applied
this method to monolingual RNN-LM.

We conducted extensive experiments on the SEAME CS
corpus [1] to evaluate our methods. Next, we showed that
the proposed Pinyin-based Mask-CTC and the regularization
methods beneﬁt CS speech recognition and offer comparable
performance to the AR baseline. Moreover, error analyses
prove our methods useful and provided insights to future im-
provements of CS speech recognition.

2. METHOD

2.1. Mask-CTC

The Mask-CTC [23,24] model is a NAR ASR framework that
adopts the transformer encoder-decoder structure, but the de-
coder is a CMLM for NAR decoding.

First, in the training phase, given transcribed audio-text
pair (X, Y ), the acoustic feature sequence X is ﬁrst encoded
with a conformer encoder model [18], and linearly projected
to probability distributions over all possible vocabularies V to
minimize the CTC loss [32]. Then, some tokens Ymask ⊂ Y
are randomly replaced with the special token <mask>. The
decoder is trained to predict the masked tokens conditioned on
the observed tokens Yobs = Y \Ymask and the encoder output.
The Mask-CTC model is thus trained to maximize the log-
likelihood

log PNAR(Y |X) = α log PCTC(Y |X)

+ (1 − α) log PCMLM(Ymask|Yobs, X),
(1)

where 0 ≤ α ≤ 1 is a tunable hyper-parameter.

At the decoding stage, the encoder output sequence is ﬁrst
transformed by CTC greedy decoding, denoted as ˆY . Next,
tokens with probability lower than a speciﬁed threshold Pthres
in ˆY are masked with <mask> for the CMLM to predict, de-
noted as ˆYmask. Then, the masked tokens are gradually recov-
ered by the CMLM in K iterations. In the kth iteration, the
most (cid:98)| ˆYmask|/K(cid:99) probable tokens in the masked sequence
are recovered by calculating

yu = arg max

y∈V

PCMLM(yu = y| ˆY (k)

obs , X),

(2)

where yu is the uth output token and ˆY (k)
obs is the sequence
including the unmasked tokens and the masked tokens recov-

Fig. 1.
(a) The original Mask-CTC [23] framework and
(b) the proposed Mask-CTC + Pinyin-to-Mandarin decoder
framework. The example ”我(wo)很(hen) happy” means ”I
am very happy.” However, the character ”很(hen)” is mis-
spelled as ”狠(hen)” which have the same pronunciation but
different meanings. The ﬁnal transformer decoders can re-
cover the wrong character.

ered in the ﬁrst (k−1) iterations. The Mask-CTC ASR frame-
work achieves performance close to AR ASR models but with
signiﬁcantly faster decoding and lower latency [23, 24].

2.2. Pinyin as Output Target

In this paper, we introduce the Pinyin-to-Mandarin (P2M) de-
coder for dealing with CS data, as shown in Fig. 1. One of the
challenges of training Mandarin-English CS speech recogni-
tion models is that jointly modeling the two very different lan-
guages is tricky. Roughly 5k characters are frequently used in
Mandarin, consisting of approximately 400 pronunciations or
1500 with tones involved. Thereby having many characters
with identical pronunciation, increasing ambiguity and mak-
ing ASR models prone to predict incorrect characters with the
correct pronunciations. Moreover, a large vocabulary size is
difﬁcult for CTC training. We thus propose changing the en-
coder’s targets from Chinese character to Pinyin to separate
the process of acoustic and language modeling.

Each Chinese character can be mapped to at least one
Pinyin symbol, representing the pronunciation, similar to
phoneme representations. Replacing characters with Pinyin
signiﬁcantly reduces vocabulary size and allows the encoder
to focus on learning acoustic modeling. The Pinyin system’s

P2M Decoderwo henhappy我狠happyConformer EncoderTransformer Decoder我很happyCTC我狠happyConformer EncoderTransformer Decoder我很happyCTC(a) Mask-CTC(b) Mask-CTC+ P2M Decoderpronunciation rules are similar to English since Pinyin sym-
bols can be represented in Latin letters, providing a more
intuitive way to utilize a pre-trained English ASR model for
initialization.
P2M Decoder. Next, we propose a P2M decoder to map
Pinyin back to Chinese characters using a single-layer de-
coder, as shown in the middle of Fig. 1. The P2M model
is trained with Pinyin-Mandarin character sequence pairs
conditioned on the encoder’s output. Based on acoustic-
the P2M model can translate
contextualized information,
Pinyin-English mixed sequence to Mandarin-English because
typical phonetic sequences in Mandarin are usually differ-
ent from English. The original Mask-CTC uses Mandarin-
English sentences to train the decoder; this approach further
applies the triplet pair (Pinyin, Mandarin, English) for train-
ing, providing more information to train the ﬁnal Mandarin-
English decoder. We encourage the P2M model to learn a
Pinyin-to-Mandarin character translation with a randomly
masking technique.

2.3. Word Embedding Label Smoothing Regularization

In this section, we introduce a label smoothing [33] method
using pre-trained word embedding. As mentioned earlier, the
scarcity of transcribed CS data makes CS ASR models eas-
ily overﬁt and unable to generalize. Hence, to improve the
performance and prevent the CMLM decoder from overconﬁ-
dence, we wish to regularize the model parameters and bring
more textual knowledge via label smoothing.

Conventionally, label smoothing reduces the ground truth
label’s probability to 1 − (cid:15) for a small constant 0 < (cid:15) < 1,
while all the other labels are equally assigned with a small
constant probability. Although label smoothing is useful in
regularization, it is incapable of exploiting the target’s mean-
ing and similarity with other targets. Therefore, we distill
knowledge from semantic-rich word embedding pre-trained
from a large amount of text data. Word embedding reﬂects
semantic-similar and contextual-relevant words that would
have similar word representations. By leveraging such prop-
erties would allow the model to learn the semantic-contextual
information implicitly.

In this paper, we determine the possible labels by calcu-
lating the cosine similarity between word embedding. The
original method proposed by Song et al. [28] chooses similar
words with cosine similarity higher than a threshold τ . For a
ground truth label ˆy, the set of similar words are chosen as

D = {y ∈ V\{ˆy} | cos (e(ˆy), e(y)) ≥ τ } ,

(3)

where e denotes pre-trained word embedding and cos(·, ·)
returns the cosine similarity between the two inputs. How-
ever, determining the best value of τ for each dataset requires
multiple experiments, which is problematic since training
an ASR model is time-consuming and thus unsuitable in our
case. Moreover, datasets with different languages, vocabulary

sizes, or word embedding types should set different thresh-
olds to ﬁnd a proper D for each token. To mitigate this issue,
we propose setting the size of D to a ﬁxed value N . In this
case, we guarantee top N similar words can be collected for
each token in the vocabulary. The set of similar words can
thus be obtained by

D = arg topN
y∈V\{ˆy}

cos (e(ˆy), e(y)) ,

(4)

where the arg topN operator returns a set of N indices indi-
cating the labels with the top N highest scores. The probabil-
ities of each label can be written as

P (y) =






y = ˆy
1 − (cid:15),
(cid:15)
N ,
y ∈ D
0, otherwise

.

(5)

We expect the label smoothing method provides a better
regularization effect. Moreover, the word embeddings can be
trained with additional text data other than CS data, introduc-
ing richer information to the decoder.

2.4. Projection Matrix Regularization

Here, we propose regularizing the projection matrices in
the ASR model to bridge the gap between the encoder and
decoder. The Mask-CTC model’s components are trained
jointly, and the errors of the decoder are backpropagated
through the encoder. During training,
the decoder takes
masked ground-truth labels as its input; nevertheless, during
inference, the decoder’s input is the erroneous output of the
encoder, making it more challenging to decode correctly.
This mismatch might lead to degraded performance of the
ASR. Thus, we propose to mitigate this problem by making
the encoder output projection matrix and the decoder input
embedding matrix similar to each other.

Another purpose of this method is to fuse the knowledge
in the embedding layers. The encoder mainly focuses on
acoustic modeling, making the output embeddings contain
more phonetic information and easily output wrong tokens
with similar pronunciation.
In contrast, the decoder input
embedding contains linguistic information, regularizing these
embedding spaces makes the decoder input embedding in-
volve more phonetic information. Therefore, the CMLM
can learn to predict correct sentences based on the phonetic-
linguistic mixed information.

The computation of the projection matrix regularization
loss is shown as follows. The last layer of the encoder is a ma-
trix WCTC ∈ Rd×|V| for linearly projecting encoded features
of dimension d to a probability distribution over all |V| possi-
ble output labels. The ﬁrst layer of the decoder is the embed-
ding layer that transforms one-hot vectors representing text
tokens to continuous hidden features with a matrix WT
emb ∈
Rd×|V|. Matrices WCTC and WT
emb can be respectively writ-
ten as [wCTC,1 . . . wCTC,|V|] and [wemb,1 . . . wemb,|V|],

Table 1.
The duration and the composition of Man-
darin, English, and code-switching utterances in the train-
ing/validation/testing sets of the SEAME corpus.

train

val

devman

devsge

Duration (hours)
Mandarin
English
Code-switching

7.5

114.6
6.0
19.3% 19.1% 19.9%
23.8% 24.4% 12.4%
56.9% 56.5% 67.7%

3.9
8.9%
49.8%
41.3%

where w are column vectors of dimension d. Here we wish
the two matrices have similar behavior, and thus apply cosine
embedding loss to constrain the two matrices as

LMatReg =

1
|V|

|V|
(cid:88)

v=1

[1 − cos (wCTC,v, wemb,v)] .

(6)

This loss function can also be used to build a relation between
the output layer of the P2M decoder and the input embed-
ding layer of CMLM. Additionally, an alternative solution is
to share the same weights in WCTC and WT
emb; however, we
found this solution severely damaged the performance since
the two matrices still have some different functionalities.

The combined loss function is

L = − α log PCTC(Y pyin|X)

− (1 − α) log PP2M(Y char
− (1 − α) log PCMLM(Y char
+ βLMatReg,

mask|Y pyin

obs , X)

mask|Y char

obs , X)

(7)

where Y pyin and Y char respectively represent using Pinyin
and character as the Mandarin tokens. In this paper, the log
probabilities PP2M and PCMLM include either conventional
or word embedding label smoothing.

3. EXPERIMENT

3.1. Data

To evaluate the proposed methods for CS speech recognition,
we used a CS corpus and three monolingual corpora.
SEAME Corpus: SEAME [1] is a Mandarin-English CS
conversational speech corpus collected in Singapore. This
corpus is composed of spontaneous and noisy conversations
in interviews. The two evaluation sets, devman and devsge,
are respectively biased toward Mandarin and English. We ex-
cluded all the testing data from the SEAME corpus and used
the remaining for the train/validation set. We used all train-
ing data except for the ablation study in Sec. 3.4, which only
used 10% of data. The statistics are listed in Table 1. The
baseline model used subword units [34] for English, Chinese
characters for Mandarin, and an additional <noise> token,

resulting in a vocabulary size of 5751. We used 2704 Chinese
characters and were reduced to 390 Pinyin tokens for training
the P2M decoder. The pypinyin package was used for Pinyin-
Mandarin mapping. 1
Monolingual Datasets: We used the LibriSpeech [35] En-
glish corpus of approximately 960 hours of training data for
model pre-training. Because text data in SEAME are noisy
and scarce, we combined SEAME, the TEDLIUM2 English
corpus [36], and the AISHELL1 Mandarin corpus [37] to
train a 64-dimensional skip-gram model using the fastText
toolkit [38]. We chose TEDLIUM2 rather than LibriSpeech
for training word embedding since it has shorter and more
spontaneous utterances similar to SEAME.

3.2. Model

All experiments were based on the ESPnet toolkit [39] and
followed its LibriSpeech training recipe. Audio features were
extracted to global mean-variance normalized 83-dimensional
log-Mel ﬁlterbank and pitch features. Speed perturbation [40]
and SpecAugment [41] were added throughout the training
process for data augmentation. Conventional label smoothing
was applied to all experiments except for those with the word
embedding label smoothing technique. The encoder architec-
ture for both AR and NAR models was a 12-layer conformer
encoder with a dimension of 512 and 8 attention heads per
layer [18]. The P2M decoder was a single-layer transformer
decoder, and the CMLM decoder was a 6-layer transformer
decoder [42], both decoders had a feed-forward dimension of
2048. The ASR models in our experiments were all initialized
with a pre-trained conformer ASR provided by ESPnet.

We evaluated the performance of our models with token
error rate (TER) and real-time factor (RTF), where the to-
kens refer to Chinese characters and English words. RTF is
an indicator for demonstrating the advantage of NAR mod-
els’ fast inference speed over AR models, which is calculated
as dividing the total decoding time by the total audio signal
length. Therefore, lower RTF equals lower computation time
and more desirable for real-world applications. We set the
hyper-parameters in Eq. (7) to α = 0.3 and β = 10−4, and
set (cid:15) = 0.1 and N = 10 for the label smoothing loss.

We found iterative reﬁnement in Mask-CTC provided lit-
tle improvement in our experiments, probably because the
spontaneous and code-switched SEAME corpus. Compared
with the clean and monolingual corpora used in previous stud-
ies [23,24], SEAME made the CMLM decoder of Mask-CTC
more challenging to learn. Therefore, we directly predicted
the output with a single pass through the decoder.

3.3. Pinyin Decoder and Regularization Methods

This section investigated the effectiveness of the proposed
model and regularization methods using all training data in

1https://pypi.org/project/pypinyin/

Table 2. TERs(%) and RTF of the AR and NAR ASR mod-
els trained with all data from SEAME. The Reg Methods in
row (e) are the word embedding label smoothing and the pro-
jection matrix regularization methods applied jointly. Model
averaging was applied to rows (f) and (i).

Table 3.
Comparison of regularization methods in low-
resource scenario in TER(%). EmbLS and MatReg respec-
tively denote word embedding label smoothing and projection
matrix regularization. The overall TER is in the All column.

Method

devman

devsge RTF

(I) Non-autoregressive

(a) CTC
(b) Mask-CTC
(c) + M2M (w/o Pinyin)
(d) + P2M (w/ Pinyin)
(e) + Reg Methods
+ Model Avg
(f)

24.2
16.5
16.6
16.3
16.0
15.3

(II) Autoregressive

(g) Transformer-T [16]
(h) Multi-Enc-Dec [15]
(i) Conformer (Ours)

18.5
16.7
14.3

35.0
24.4
24.4
24.0
24.1
22.3

26.3
23.1
20.6

0.01
0.02
0.02
0.02
0.02
0.02

−
−
1.03

SEAME. We chose models with the lowest validation losses
for evaluation, and the results are listed in Sec. (I) of Table 2.
We set three baselines: conformer CTC (row (a)), Mask-
CTC (row (b)), and the proposed architecture but used Chi-
nese characters instead of Pinyin tokens as the intermediate,
denoted as the Mandarin-to-Mandarin (M2M) decoder (row
(c)). First, the Mask-CTC architecture obtained better per-
(a)),
formance than the original CTC model (rows (b) v.s.
indicating that leveraging the CMLM decoder was beneﬁcial
for CS speech recognition. Next, adding the M2M decoder to
the Mask-CTC framework slightly damaged the ASR perfor-
mance (rows (c) v.s. (b)). However, switching the encoder’s
target to Pinyin improved the ASR performance (rows (d)
v.s. (b)), indicating that introducing Pinyin as the intermedi-
ate representation helped recognition of Chinese characters.
Moreover, the regularization methods achieved better results
on devman and obtained comparable results on devsge (rows
(e) v.s. (d)). To decrease the variance of the model’s predic-
tion, we selected the top ﬁve checkpoints with higher valida-
tion accuracy for averaging [43], resulting in the best NAR
ASR performance (row (f)).

To highlight the beneﬁts of using NAR ASR models, we
listed the performance of AR ASR models in Sec. (II) of Ta-
ble 2 for comparison. Our AR baseline incorporated an RNN-
LM by shallow fusion with a beam size of 10. The model
surpassed the previous SOTA Transformer-transducer [16]
and Multi-encoder-decoder [15] (rows (i) v.s.
(g)(h)), pro-
viding a solid reference. The best NAR model offered good
performance with a small gap between the best AR model
(rows (f) v.s. (i)) but with a signiﬁcantly 50× speedup (the
RTF column), implying the NAR model could recognize CS
speech while possessing fast inference speed. Overall, we

EmbLS MatReg

devman

devsge All

(a)

(b)
(c)
(d)

(cid:37)

(cid:34)
(cid:37)
(cid:34)

(cid:37)

(cid:37)
(cid:34)
(cid:34)

42.2

42.0
42.2
41.7

50.7

50.1
50.4
50.2

45.3

44.9
45.2
44.8

have shown that NAR ASR models incorporated with the
proposed methods achieved exciting results.

3.4. Regularization Methods for Low-resource Setting

To verify the efﬁcacy of the proposed regularization meth-
ods, we conducted ablation studies under the low-resource
scenario with only 10% of training data. 2 The performance
of the baseline model is shown in row (a) of Table 3. Improve-
ment was brought by only applying the word embedding label
smoothing (rows (b) v.s. (a)), showing that leveraging textual
knowledge from additional text data was beneﬁcial under the
low-resource scenario. With the proposed projection matrix
regularization applied, the model obtained less improvements
(rows (c) v.s. (a)). Finally, applying both methods (row (d))
achieved the best performance. We showed that the projec-
tion matrix regularization technique beneﬁted from the label
smoothing method and ﬁlled the gap between the encoder and
decoders. The ablation study showed that both methods were
compatible with each other and improved ASR performance.

3.5. Error Analyses

This section analyzed each method’s errors and showed how
the proposed methods decreased the error rates. The results
over devman + devsge are shown in Table 4. We analyzed
the source of TERs, including substitution, deletion, and in-
sertion. We provided the Pinyin error rates (PER, column
(iii)), by mapping all Chinese characters to their correspond-
ing Pinyin tokens, and evaluated TER at Pinyin level. PER
shows the models’ ability to produce Chinese characters with
correct pronunciations. We conducted t-test and McNemar
test to compare the signiﬁcance of improvements.

We ﬁrst inspected the methods without model averaging
in rows (a) to (e). Most results in columns (i) to (iv) were very
similar to Table 2, but more information could be extracted
with separated TERs of Mandarin and English symbols. First,
the Mandarin and English errors contributed similarly to the

2Due to space limitation, we only show results in low-resource settings.

Table 4. A more detailed error analysis of different methods on the combination of devman and devsge evaluation sets. All
values are in percentage (%). The overall TER are divided into (i) all tokens, (ii) Mandarin characters, (iii) Pinyin tokens, and
(iv) English words. The Sub, Del, and Ins columns respectively indicate substitution, deletion, and insertion. The deletion rates
are also divided into (vi) all tokens, (vii) Mandarin characters, and (viii) English words.

Method

TER

Sub

Del

Ins

(i) All

(ii) Man

(iii) Pinyin

(iv) Eng

(v) All

(vi) All

(vii) Man

(viii) Eng

(ix) All

(a) CTC
(b) Mask-CTC
(c) + M2M
(d) + P2M
(e) + Reg Methods

28.1
19.4
19.4
19.1
18.9

+ Model Avg

(f)
(g) AR

17.8
16.6‡(cid:5)

13.2
9.0
9.0
8.9
8.8

8.4
7.9(cid:5)

11.8
8.0
7.9
7.7
7.6

7.3
6.9(cid:5)

14.9
10.4
10.5
10.2
10.1

9.5
8.6‡(cid:5)

17.4
12.5
12.7
12.7
12.7

11.8
11.0‡(cid:5)

8.9
5.1
4.7†∗
4.5†∗
4.3†∗

4.2
3.8‡(cid:5)

4.5
2.8
2.5†∗
2.5†∗
2.3†∗

2.4
2.2‡

4.3
2.3
2.2
2.1†∗
2.0†∗

1.8
1.6‡(cid:5)

1.8
1.8
1.9∗
1.9
1.9

1.8
1.7(cid:5)

† Passed t-test with p < 0.01 compared with row (b). ‡ Passed t-test with p < 0.01 compared with row (f).
∗ Passed McNemar test with p < 0.05 compared with row (b). (cid:5) Passed McNemar test with p < 0.05 compared with row (f).

lower deletion rates. Moreover, the proposed P2M decoder
passed t-test and McNemar tests in English deletion while the
M2M decoder failed (column (viii), rows (d) v.s. (c)), show-
ing that using Pinyin as the intermediate beneﬁted English
word prediction. Replacing Chinese characters with Pinyin
symbols forced the encoder to learn acoustic modeling better;
thus, it predicted more accurate sequence lengths rather than
struggling to predict correct Chinese characters directly.

Finally, we compared the differences between our best
NAR model and the AR baseline (rows (f)(g)). In most error
terms, the AR baseline surpassed the best NAR model with
high conﬁdence, showing the large gaps between the NAR
and AR ASR models. Therefore, we should pay more atten-
tion to these gaps, including English TER, substitution rates,
and deletion rates. In contrast, the Mandarin and Pinyin TERs
and the insertion rate failed to pass the t-test, showing that the
NAR model’s performance was similar to the AR model in
these metrics. These results provided some insights to im-
proving NAR CS speech recognition models. For instance, to
decrease the deletion rate of a NAR ASR, more efforts can be
made in the decoding stage to predict more precise length of
the output [24]. Overall, the results have shown the proposed
methods offered powerful performance compared to the AR
baseline, while remains some space to improve.

4. CONCLUSION

This paper introduces a novel non-autoregressive ASR frame-
work by adding a Pinyin-to-Mandarin decoder in the Mask-
CTC ASR to solve the Mandarin-English code-switching
speech recognition problem. We also propose word embed-
ding label smoothing for including contextual information
to conditional masked language model and projection matrix
regularization method to bridge the gap between the encoder

Fig. 2. Samples of different ASR models’ predictions of the
same utterance from devman. Del and Sub here indicate the
number of required deletions and substitutions, respectively.
Fewer deletions have to be made for the proposed methods.

total TERs. With the proposed P2M decoder and regulariza-
tion methods respectively added, both Mandarin and English
TERs were reduced (rows (d)(e) v.s. (b)), showing that our
approaches beneﬁted the two languages simultaneously. An-
other observation was that the ratio between PERs and Man-
darin TERs ((iii) ÷ (ii)) gradually dropped from 89.5% (row
(a)) to 85.9% (row (d)) when adding the proposed methods,
indicating that the models tend to predict Chinese characters
with correct pronunciation with our methods.

Next, we investigated the contributions of substitution,
deletion, and insertion to the overall TER (columns (v) to
(ix)). We found that the number of substitutions and in-
the same among different methods
sertions were almost
(columns (v) and (ix)), but deletions had signiﬁcant im-
provements (columns (vi) to (viii)). Results indicated that
our approaches made the ASR output sequence length closer
to the ground truth. An actual sample is shown in Fig. 2,
Mask-CTC prone to predict shorter sentences on the SEAME
corpus, but our methods provided more accurate predictions
in both Chinese characters and English words, resulting in

Ground truth不是講traditional lah 我比較喜歡有自己的空間啊space ahMask-CTC(Del = 5, Sub = 1)不是講我比較喜歡有自己的space的+ M2M(Del = 4)不是講lah我比較喜歡有自己的空間+ P2M (Del = 3)不是講traditional lah 我比較喜歡有自己間啊ah+ P2M + Reg (Del = 0)不是講traditional lah 我比較喜歡有自己的空間啊space ahand decoders. We demonstrated the effectiveness of our meth-
ods with exciting performance on the SEAME corpus. The
new ASR framework and regularization methods have the
potential to improve various speech recognition scenarios.

[12] Ching-Ting Chang, Shun-Po Chuang, and Hung yi Lee,
“Code-switching sentence generation by generative ad-
versarial networks and its application to data augmenta-
tion,” in Interspeech, 2019.

5. REFERENCES

[1] Dau-Cheng Lyu, Tien-Ping Tan, Eng Siong Chng, and
“SEAME: a Mandarin-English code-
Haizhou Li,
switching speech corpus in South-East Asia,” in Inter-
speech, 2010.

[2] Han-Ping Shen, Chung-Hsien Wu, Yan-Ting Yang, and
Chun-Shan Hsu, “CECOS: A Chinese-English code-
switching speech database,” in O-COCOSDA, 2011.

[3] Ying Li, Yue Yu, and Pascale Fung,

“A mandarin-

english code-switching corpus.,” in LREC, 2012.

[4] Timo Baumann, Arne K¨ohn, and Felix Hennig, “The
spoken wikipedia corpus collection: Harvesting, align-
ment and an application to hyperlistening,” Language
Resources and Evaluation, vol. 53, no. 2, 2019.

[5] Shuai Zhang, Jiangyan Yi, Zhengkun Tian, Jianhua
Tao, and Ye Bai,
“RNN-transducer with language
bias for end-to-end Mandarin-English code-switching
speech recognition,” in ISCSLP, 2021.

[6] Ke Li, Jinyu Li, Guoli Ye, Rui Zhao, and Yifan Gong,
“Towards code-switching ASR for end-to-end CTC
models,” in ICASSP, 2019.

[7] Changhao Shan, Chao Weng, Guangsen Wang, Dan Su,
Min Luo, Dong Yu, and Lei Xie, “Investigating end-
to-end speech recognition for Mandarin-English code-
switching,” in ICASSP, 2019.

[8] Zhiping Zeng, Yerbolat Khassanov, Van Tung Pham,
“On
Haihua Xu, Eng Siong Chng, and Haizhou Li,
the end-to-end solution to Mandarin-English code-
switching speech recognition,” in Interspeech, 2019.

[9] Chenpeng Du, Hao Li, Yizhou Lu, Lan Wang, and Yan-
“Data augmentation for end-to-end code-

min Qian,
switching speech recognition,” in SLT, 2021.

[13] William Chan, Navdeep Jaitly, Quoc V. Le, and Oriol
Vinyals, “Listen, attend and spell: A neural network for
large vocabulary conversational speech recognition,” in
ICASSP, 2016.

[14] Alex Graves,

“Sequence transduction with recurrent
neural networks,” in ICML Workshop on Representation
Learning, 2012.

[15] Xinyuan Zhou, Emre Yılmaz, Yanhua Long, Yijie Li,
and Haizhou Li, “Multi-encoder-decoder transformer
for code-switching speech recognition,” in Interspeech,
2020.

[16] Siddharth Dalmia, Yuzong Liu, Srikanth Ronanki, and
Katrin Kirchhoff, “Transformer-transducers for code-
switched speech recognition,” in ICASSP, 2021.

[17] Yizhou Lu, Mingkun Huang, Hao Li, Jiaqi Guo, and
Yanmin Qian,
“Bi-encoder transformer network for
Mandarin-English code-switching speech recognition
using mixture of experts,” in Interspeech, 2020.

[18] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki
Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang,
Zhengdong Zhang, Yonghui Wu, and Ruoming Pang,
“Conformer: Convolution-augmented transformer for
speech recognition,” in Interspeech, 2020.

[19] Chunxi Liu, Frank Zhang, Duc Le, Suyoun Kim,
Yatharth Saraf, and Geoffrey Zweig, “Improving RNN
in SLT,
transducer based ASR with auxiliary tasks,”
2021.

[20] Ethan A Chi,

Julian Salazar, and Katrin Kirch-
hoff, “Align-reﬁne: Non-autoregressive speech recog-
arXiv preprint
nition via iterative realignment,”
arXiv:2010.14233, 2020.

[21] Nanxin Chen, Shinji Watanabe, Jes´us Villalba, and Na-
“Listen and ﬁll in the missing letters:
jim Dehak,
Non-autoregressive transformer for speech recognition,”
arXiv preprint arXiv:1911.04908, 2019.

[10] Yanhua Long, Yijie Li, Qiaozheng Zhang, Shuang Wei,
Hong Ye, and Jichen Yang, “Acoustic data augmenta-
tion for mandarin-english code-switching speech recog-
nition,” Applied Acoustics, vol. 161, 2020.

[22] William Chan, Chitwan Saharia, Geoffrey Hinton, Mo-
hammad Norouzi, and Navdeep Jaitly, “Imputer: Se-
quence modelling via imputation and dynamic program-
ming,” in ICML, 2020.

[11] Yash Sharma, Basil Abraham, Karan Taneja, and Preethi
Jyothi, “Improving low resource code-switched ASR
using augmented code-switched TTS,” in Interspeech,
2020.

[23] Yosuke Higuchi, Shinji Watanabe, Nanxin Chen, Tetsuji
Ogawa, and Tetsunori Kobayashi, “Mask CTC: Non-
autoregressive end-to-end ASR with CTC and mask pre-
dict,” in Interspeech, 2020.

[24] Yosuke Higuchi, Hirofumi Inaguma, Shinji Watanabe,
Tetsuji Ogawa, and Tetsunori Kobayashi,
“Improved
mask-CTC for non-autoregressive end-to-end ASR,” in
ICASSP, 2021.

[36] Anthony Rousseau, Paul Del´eglise, and Yannick Est`eve,
“Enhancing the TED-LIUM corpus with selected data
for language modeling and more TED talks,” in LREC,
2014.

[25] Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and
Luke Zettlemoyer,
“Mask-predict: Parallel decoding
of conditional masked language models,” in EMNLP-
IJCNLP, 2019.

[37] Hui Bu, Jiayu Du, Xingyu Na, Bengu Wu, and Hao
Zheng, “AISHELL-1: An open-source mandarin speech
in O-
corpus and a speech recognition baseline,”
COCOSDA, 2017.

[38] Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov, “Enriching word vectors with subword
information,” arXiv preprint arXiv:1607.04606, 2016.

[39] Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki
Hayashi, Jiro Nishitoba, Yuya Unno, Nelson Enrique
Yalta Soplin, Jahn Heymann, Matthew Wiesner, Nanxin
Chen, Adithya Renduchintala, and Tsubasa Ochiai,
“ESPnet: End-to-end speech processing toolkit,” in In-
terspeech, 2018.

[40] Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-
jeev Khudanpur, “Audio augmentation for speech recog-
nition,” in Interspeech, 2015.

[41] Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng
Chiu, Barret Zoph, Ekin D. Cubuk, and Quoc V. Le,
“SpecAugment: A simple data augmentation method for
automatic speech recognition,” in Interspeech, 2019.

[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser,
and Illia Polosukhin, “Attention is all you need,” in
NeurIPS, 2017.

[43] Shigeki Karita, Nanxin Chen, Tomoki Hayashi, Takaaki
Hori, Hirofumi Inaguma, Ziyan Jiang, Masao Someki,
Nelson Enrique Yalta Soplin, Ryuichi Yamamoto, Xi-
aofei Wang, et al., “A comparative study on transformer
vs RNN in speech applications,” in ASRU, 2019.

[26] Yash Sharma, Basil Abraham, Karan Taneja, and Preethi
Jyothi, “Improving Low Resource Code-Switched ASR
Using Augmented Code-Switched TTS,” in Interspeech,
2020.

[27] Shun-Po Chuang, Tzu-Wei Sung, and Hung-yi Lee,
“Training code-switching language model with mono-
lingual data,” in ICASSP, 2020.

[28] Minguang Song, Yunxin Zhao, Shaojun Wang, and
Mei Han, “Word similarity based label smoothing in
RNNLM training for ASR,” in SLT, 2021.

[29] William Chan and Ian Lane, “On online attention-based
speech recognition and joint Mandarin character-Pinyin
training.,” in Interspeech, 2016.

[30] Burin Naowarat, Thananchai Kongthaworn, Korrawe
Karunratanakul, Sheng Hui Wu, and Ekapol Chuang-
suwanich, “Reducing spelling inconsistencies in code-
switching ASR using contextualized CTC loss,” arXiv
preprint arXiv:2005.07920, 2020.

[31] Zheying Huang, Peng Li, Ji Xu, Pengyuan Zhang,
and Yonghong Yan, “Context-dependent label smooth-
ing regularization for attention-based end-to-end code-
switching speech recognition,” in ISCSLP, 2021.

[32] Alex Graves, Santiago Fern´andez, Faustino Gomez, and
J¨urgen Schmidhuber, “Connectionist temporal classiﬁ-
cation: Labelling unsegmented sequence data with re-
current neural networks,” in ICML, 2006.

[33] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,
Jon Shlens, and Zbigniew Wojna, “Rethinking the in-
in CVPR,
ception architecture for computer vision,”
2016.

[34] Taku Kudo and John Richardson, “SentencePiece: A
simple and language independent subword tokenizer
and detokenizer for neural text processing,” in EMNLP:
System Demonstrations, 2018.

[35] Vassil Panayotov, Guoguo Chen, Daniel Povey, and San-
jeev Khudanpur, “Librispeech: An ASR corpus based
on public domain audio books,” in ICASSP, 2015.

