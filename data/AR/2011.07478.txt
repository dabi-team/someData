Towards Understanding the Regularization of Adversarial Robustness on
Neural Networks

Yuxin Wen * 1 2 Shuai Li * 1 Kui Jia 1 2

0
2
0
2

v
o
N
5
1

]

G
L
.
s
c
[

1
v
8
7
4
7
0
.
1
1
0
2
:
v
i
X
r
a

Abstract

The problem of adversarial examples has shown
that modern Neural Network (NN) models could
be rather fragile. Among the more established
techniques to solve the problem, one is to require
the model to be (cid:15)-adversarially robust (AR); that
is, to require the model not to change predicted la-
bels when any given input examples are perturbed
within a certain range. However, it is observed
that such methods would lead to standard per-
formance degradation, i.e., the degradation on
In this work, we study the
natural examples.
degradation through the regularization perspec-
tive. We identify quantities from generalization
analysis of NNs; with the identiﬁed quantities we
empirically ﬁnd that AR is achieved by regulariz-
ing/biasing NNs towards less conﬁdent solutions
by making the changes in the feature space (in-
duced by changes in the instance space) of most
layers smoother uniformly in all directions; so to a
certain extent, it prevents sudden change in predic-
tion w.r.t. perturbations. However, the end result
of such smoothing concentrates samples around
decision boundaries, resulting in less conﬁdent so-
lutions, and leads to worse standard performance.
Our studies suggest that one might consider ways
that build AR into NNs in a gentler way to avoid
the problematic regularization.

1. Introduction

Despite the remarkable performance (Krizhevsky et al.,
2012) of Deep Neural Networks (NNs), they are found to
be rather fragile and easily fooled by adversarial examples

*Equal contribution 1School of Electronic and Information En-
gineering, South China University of Technology, Guangzhou,
Guangdong 510640, China 2Pazhou Lab, Guangzhou, 510335,
China. Correspondence to: Shuai Li <lishuai918@gmail.com>,
Yuxin Wen <wen.yuxin@mail.scut.edu.cn>, Kui Jia <kui-
jia@scut.edu.cn>.

Proceedings of the 37 th International Conference on Machine
Learning, Online, PMLR 119, 2020. Copyright 2020 by the au-
thor(s).

(Szegedy et al., 2014). More intriguingly, these adversarial
examples are generated by adding imperceptible noise to
normal examples, and thus are indistinguishable for humans.
NNs that are more robust to adversarial examples tend to
have lower standard accuracy (Su et al., 2018), i.e., the accu-
racy measured on natural examples. The trade-off between
robustness and accuracy has been empirically observed in
many works (Fawzi et al., 2018; Kurakin et al., 2017; Madry
et al., 2018; Tsipras et al., 2019), and has been theoretically
analyzed under the context of simple models, e.g., linear
models (Tsipras et al., 2019), quadratic models (Fawzi et al.,
2018), but it is not clear whether the analysis generalizes to
NNs. For example, Tsipras et al. (2019) show that for linear
models, if examples are close to decision boundaries, ro-
bustness provably conﬂicts with accuracy, though the proof
seems unlikely to generalize to NNs. Arguably, the most
widely used remedy is developed to require NNs to be (cid:15)-
adversarially robust (AR), e.g., via Adversarial Training
(Madry et al., 2018), Lipschitz-Margin Training (Tsuzuku
et al., 2018); that is, they require the model not to change
predicted labels when any given input examples are per-
turbed within a certain range. In practice, such AR methods
are found to lead to worse performance measured in stan-
dard classiﬁcation accuracy. Alternatives to build AR into
NNs are also being developed. For instance, Zhang et al.
(2019) show that a gap exists between surrogate risk gap
and 0-1 risk gap if many examples are close to decision
boundaries, and better robustness can be achieved by push-
ing examples away from decision boundaries. But pushing
examples away again degrades NN performance in their
experiments. But they are yet to be widely adopted by the
community.

We investigate how adversarial robustness built into NNs
by the arguably most established method, i.e., Adversar-
ial Training (Madry et al., 2018), inﬂuences the behaviors
of NNs to make them more robust but have lower perfor-
mance through the lens of regularization. In an earlier time
(Szegedy et al., 2014), adversarial training has been sug-
gested as a form of regularization: it augments the training
of NNs with adversarial examples, and thus might improve
the generalization of the end models. Note that such a hard
requirement that the adversarial examples need to be clas-
siﬁed correctly is different from the methods that increase

 
 
 
 
 
 
Towards Understanding the Regularization of Adversarial Robustness on Neural Networks

adversarial robustness by adding a soft penalty term to the
risk function employed by Lyu et al. (2015) and Miyato
et al. (2018), or a penalty term through curvature reduc-
tion (Moosavi-Dezfooli et al., 2019), or local linearization
(Qin et al., 2019) (more discussion in appendix A). In these
works, regularization is explicitly enforced by a penalty
term, while in adversarial training, it is not clear that how
training with augmented adversarial examples regularizes
NNs. For example, if adversarial training does work as a
regularizer, how does a possible improvement in general-
ization by using more data end up degrading performance?
Even such a basic problem does not have a clear answer. To
understand the regularization effects of AR on NNs, we go
beyond simple linear or quadratic models and undertake a
comprehensive generalization analysis of AR by establish-
ing a rigorous generalization bound on NNs, and carrying
out a series of empirical studies theoretically guided by the
bound.

Technically, improved generalization implies the reduction
in gap between training errors and test errors. Regulariza-
tion achieves the gap reduction by reducing the size of the
hypothesis space, which reduces the variance, but mean-
while increases the bias of prediction made — a constant
classiﬁer can have zero generalization errors, but also have
low test performance. Thus, when a hypothesis space is im-
properly reduced, another possible outcome is biased poorly
performing models with reduced generalization gaps.

Key results. Through a series of theoretically motivated
experiments, we ﬁnd that AR is achieved by regulariz-
ing/biasing NNs towards less conﬁdent solutions by making
the changes in the feature space of most layers (which are
induced by changes in the instance space) smoother uni-
formly in all directions; so to a certain extent, it prevents
sudden change in prediction w.r.t. perturbations. However,
the end result of such smoothing concentrates examples
around decision boundaries and leads to worse standard
performance. We elaborate the above statement in details
shortly in section 1.1.

Implications. We conjecture that the improper reduction
comes from the indistinguishability of the change induced
in the intermediate layers of NNs by adversarial noise and
that by inter-class difference. To guarantee AR, NNs are
asked to smoothe out difference uniformly in all directions
in a high dimensional space, and thus are biased towards
less conﬁdent solutions that make similar/concentrated pre-
dictions. We leave the investigation of the conjecture as
future works.

1.1. AR leads to less conﬁdent NNs with more

indecisive misclassiﬁcations

This section elaborates the key results we brieﬂy present
previously.

AR reduces the perturbations in the activation/outputs — the
perturbations that are induced by perturbations in the in-
puts fed into the layer — of most layers. Through a series of
theoretically motivated experiments, the results prompt us to
look at the singular value distributions of the weight matrix
of each layer of the NNs. Shown in ﬁg. 1a, we ﬁnd that
overall the standard deviation (STD) of singular values asso-
ciated with a layer of the NN trained with lower AR strength
4 is larger than that of the NN with higher AR strength 16
1 — the green dots are mostly below the red dots. Note that
given a matrix W and an example x, singular values of
W determine how the norm ||W x|| is changed comparing
with ||x||. More speciﬁcally, let σmin, σmax be the mini-
mal and maximal singular values. If x is not in the null
space of W , then we have ||W x|| ∈ [σmin||x||, σmax||x||],
where || · || denotes 2-norm. This applies to norm ||δx||
of a perturbation as well; that is, given possible changes
δx of x of the same norm ||δx|| = c, where c is a con-
stant, the variance of σ(W ) roughly determines the vari-
ance of ||W δx||, where σ(W ) denotes all singular values
{σi} of W . In more details, note that by SVD decomposi-
tion, W δx = (cid:80)
i δx, thus σi determines how the
component vT
i δx in the direction of vi is ampliﬁed. To
see an example, suppose that σmin = σmax = σ0, then
the variance of σ(W ) is zero, and ||W δx|| = σ0||δx||.
In this case, the variance of ||W δx|| (given an ensemble
of perturbations δx of the same norm c) is zero as well.
The conclusion holds as well for ReLU(W δx), where W
here is a weight matrix of a layer of a NN, and ReLU de-
notes Rectiﬁer Linear Unit activation function (proved by
applying Cauchy interlacing law by row deletion (Chafai)
in lemma 3.1). Consequently, by reducing the variance of
singular values of weight matrix of a layer of the NN, AR
reduces the variance of the norms of layer activations, or
informally, perturbations in the activations, induced by input
perturbations.

i σiuivT

The perturbation reduction in activations concentrates ex-
amples, and it empirically concentrates them around deci-
sion boundaries; that is, predictions are less conﬁdent. The
reduced variance implies that the outputs of each layer of
the NN are more concentrated, but it does not tell where they
are concentrated. Note that in the previous paragraph, the
variance relationship discussed between ||W δx|| and ||δx||
equally applies to ||W x|| and ||x||, where x is an actual
example instead of perturbations. Thus, to ﬁnd out the con-
centration of perturbations, we can look at the concentration
of samples. Technically, we look at margins of examples. In
a multi-class setting, suppose a NN computes a score func-
tion f : Rd → RL, where L is the number of classes; a way

1The AR strength is characterized by the maximally allowed
l∞ norm of adversarial examples that are used to train the NNs —
we use adversarial training (Madry et al., 2018) to build adversarial
robustness into NNs. Details can be found in appendix B.1

Towards Understanding the Regularization of Adversarial Robustness on Neural Networks

(a) STD of Singular Values

(b) Margin Distribution

(c) Accuracy

Figure 1. Experiment results on ResNet56 (He et al., 2016) trained on the CIFAR10 dataset. For the details of the experiments, refer to
section 4. (a) The standard deviation of singular values of each layer of NNs with adversarial robustness (AR) strength 4, 16 (AR strength
8 is dropped for clarity of the plot). To emphasize, the x-axis is the layer index — overall 56 layers are involved. (b) The probability
distribution of margins of NNs with AR strength 4, 8, 16. (c) The standard and adversarial accuracy of NNs with AR 4, 8, 16.

to convert this to a classiﬁer is to select the output coordinate
with the largest magnitude, meaning x (cid:55)→ arg maxi fi(x).
The conﬁdence of such a classiﬁer could be quantiﬁed by
margins. It measures the gap between the output for the cor-
rect label and other labels, meaning fy(x) − maxi(cid:54)=y fi(x).
Margin piece-wise linearly depends on the scores, thus the
variance of margins is also in a piece-wise linear relation-
ship with the variance of the scores, which are computed
linearly from the activation of a NN layer. Thus, the con-
sequence of concentration of activation discussed in the
previous paragraph can be observed in the distribution of
margins. More details of the connection between singular
values and margins are discussed in section 4.2.2, after we
present lemma 3.1. A zero margin implies that a classiﬁer
has equal propensity to classify an example to two classes,
and the example is on the decision boundary. We plot the
margin distribution of the test set of CIFAR10 in ﬁg. 1b, and
ﬁnd that margins are increasingly concentrated around zero
— that is, the decision boundaries — as AR strength grows.

The sample concentration around decision boundaries
smoothes sudden changes induced perturbations, but also in-
creases indecisive misclassiﬁcations. The concentration of
test set margins implies that the induced change in margins
by the perturbation in the instance space is reduced by AR.
Given two examples x, x(cid:48) from the test set, δx = x − x(cid:48)
can be taken as a signiﬁcant perturbation that changes the
example x to x(cid:48). The concentration of overall margins im-
plies the change induced by δx is smaller statistically in
NNs with higher AR strength. Thus, for an adversarial per-
turbation applied on x, statistically the change of margins
is smaller as well — experimentally it is reﬂected in the
increased adversarial robustness of the network, as shown in
the increasing curve in ﬁg. 1c. That is, the sudden changes
of margins originally induced by adversarial perturbations

are smoothed (to change slowly). However, the cost of such
smoothness is lower conﬁdence in prediction, and more test
examples are slightly/indecisively moved to the wrong sides
of the decision boundaries — incurring lower accuracy, as
shown in the decreasing curve in ﬁg. 1c.

Lastly, we note that experiments in this section are used
to illustrate our main arguments in this section. Further
consistent quality results are reported in section 4 by con-
ducting experiments on CIFAR10/100 and Tiny-ImageNet
with networks of varied capacity. And more corroborative
experiment results are presented in the appendices, and out-
lined in section 1.2.

1.2. Outline and contributions

This work carries out generalization analysis on NNs with
AR. The quantities in the previous section are identiﬁed by
the generalization errors (GE) upper bound we establish at
theorem 3.1, which characterizes the regularization of AR
on NNs. The key result is obtained at the end of a series of
analysis, thus we present the outline of the analysis here.

Outline. After presenting some preliminaries in section 2,
we proceed to analyze the regularization of AR on NNs, and
establish a GE upper bound in section 3. The bound prompts
us to look at the GE gaps in experiments. In section 4.1, we
ﬁnd that for NNs trained with higher AR strength, the surro-
gate risk gaps (GE gaps) decrease for a range of datasets, i.e.,
CIFAR10/100 and Tiny-ImageNet. It implies AR effectively
regularizes NNs. We then study the ﬁner behavior change
of NNs that might lead to such a gap reduction. Again, we
follow the guidance of theorem 3.1. We look at the margins
in section 4.2.1, then at the singular value distribution in
section 4.2.2, and discover the main results described in
section 1.1. More corroborative experiments are present

Towards Understanding the Regularization of Adversarial Robustness on Neural Networks

in appendix B.4 and appendix B.6 to show that such phe-
nomenon exists in a broad range of NNs with varied capacity
and adversarial training techniques. More complementary
results are present in appendix B.3 to explain some seem-
ingly abnormal observations, and in appendix B.5 to quanti-
tatively demonstrate the smoothing effects of AR discussed
in section 1.1. Related works are present in appendix A.

Contributions. Overall, the core contribution in this work
is to show that adversarial robustness (AR) regularizes NNs
in a way that hurts its capacity to learn to perform in test.
More speciﬁcally:

• We establish a generalization error (GE) bound that char-
acterizes the regularization of AR on NNs. The bound
connects margin with adversarial robustness radius (cid:15) via
singular values of weight matrices of NNs, thus suggest-
ing the two quantities that guide us to investigate the
regularization effects of AR empirically.

• Our empirical analysis tells that AR effectively regularizes
NNs to reduce the GE gaps. To understand how reduced
GE gaps turns out to degrade test performance, we study
variance of singular values of layer-wise weight matrices
of NNs and distributions of margins of samples, when
different strength of AR are applied on NNs.

• The study shows that AR is achieved by regulariz-
ing/biasing NNs towards less conﬁdent solutions by mak-
ing the changes in the feature space of most layers (which
are induced by changes in the instance space) smoother
uniformly in all directions; so to a certain extent, it pre-
vents sudden change in prediction w.r.t. perturbations.
However, the end result of such smoothing concentrates
samples around decision boundaries and leads to worse
standard performance.

2. Preliminaries

Assume an instance space Z = X ×Y, where X is the space
of input data, and Y is the label space. Z := (X, Y ) are
the random variables with an unknown distribution µ, from
which we draw samples. We use Sm = {zi = (xi, yi)}m
i=1
to denote the training set of size m whose examples are
drawn independently and identically distributed (i.i.d.) by
sampling Z. Given a loss function l, the goal of learning is
to identify a function T : X (cid:55)→ Y in a hypothesis space (a
class T of functions) that minimizes the expected risk

R(l ◦ T ) = EZ∼µ [l (T (X), Y )] ,

Since µ is unknown, the observable quantity serving as the
proxy to the expected risk R is the empirical risk

Rm(l ◦ T ) =

1
m

m
(cid:88)

i=1

l (T (xi), yi) .

times termed as generalization gap in the literature

GE(l ◦ T ) = |R(l ◦ T ) − Rm(l ◦ T )|.

(1)

A NN is a map that takes an input x from the space X , and
builds its output by recursively applying a linear map Wi
followed by a pointwise non-linearity g:

xi = g(W ixi−1),

where i indexes the times of recursion, which is denoted as
a layer in the community, i = 1, . . . , L, x0 = x, and g de-
notes the activation function. which is restricted to Rectiﬁer
Linear Unit (ReLU) (Glorot et al., 2011) or max pooling
operator (B´ecigneul, 2017) in this paper. To compactly
summarize the operation of T , we denote

T x = g(W Lg(W L−1 . . . g(W 1x))).

(2)

Deﬁnition 1 (Covering number). Given a metric space
(S, ρ), and a subset ˜S ⊂ S, we say that a subset ˆS of
˜S is a (cid:15)-cover of ˜S, if ∀˜s ∈ ˜S, ∃ˆs ∈ ˆS such that ρ(˜s, ˆs) ≤ (cid:15).
The (cid:15)-covering number of ˜S is

N(cid:15)( ˜S, ρ) = min{| ˆS| : ˆS is an (cid:15)-covering of ˜S}.

Various notions of adversarial robustness have been studied
in existing works (Madry et al., 2018; Tsipras et al., 2019;
Zhang et al., 2019). They are conceptually similar; in this
work, we formalize its deﬁnition to make clear the object
for study.
Deﬁnition 2 ((ρ, (cid:15))-adversarial robustness). Given a multi-
class classiﬁer f : X → RL, and a metric ρ on X , where L
is the number of classes, f is said to be adversarially robust
w.r.t. adversarial perturbation of strength (cid:15), if there exists an
(cid:15) > 0 such that ∀z = (x, y) ∈ Z and δx ∈ {ρ(δx) ≤ (cid:15)},
we have

fˆy(x + δx) − fi(x + δx) ≥ 0,
where ˆy = arg maxj fj(x) and i (cid:54)= ˆy ∈ Y. (cid:15) is called ad-
versarial robustness radius. When the metric used is clear,
we also refer (ρ, (cid:15))-adversarial robustness as (cid:15)-adversarial
robustness.

Note that the deﬁnition is an example-wise one; that is, it
requires each example to have a guarding area, in which all
examples are of the same class. Also note that the robust-
ness is w.r.t. the predicted class, since ground-truth label is
unknown for a x in test.

We characterize the GE with ramp risk, which is a typical
risk to undertake theoretical analysis (Bartlett et al., 2017;
Neyshabur et al., 2018b).
Deﬁnition 3 (Margin Operator). A margin operator M :
RL × {1, . . . , L} → R is deﬁned as

Our goal is to study the discrepancy between R and Rm,
which is termed as generalization error — it is also some-

M(s, y) := sy − max
i(cid:54)=y

si

Towards Understanding the Regularization of Adversarial Robustness on Neural Networks

Deﬁnition 4 (Ramp Loss). The ramp loss lγ : R → R+ is
deﬁned as

lγ(r) :=






0
1 + r/γ
1

r < −γ
r ∈ [−γ, 0]
r > 0

Deﬁnition 5 (Ramp Risk). Given a classiﬁer f , ramp risk
is the risk deﬁned as

Rγ(f ) := E(lγ(−M(f (X), Y ))),

where X, Y are random variables in the instance space Z
previously.

We will use a different notion of margin in theorem 3.1, and
formalize its deﬁnition as follows. We reserve the unqual-
iﬁed word “margin” speciﬁcally for the margin discussed
previously — the output of margin operator for classiﬁca-
tion. We call this margin to-be-introduced instance-space
margin (IM).

Deﬁnition 6 (Smallest Instance-space Margin). Given an
element z = (x, y) ∈ Z, let v(x) be the distance from x to
its closest point on the decision boundary, i.e., the instance-
space margin (IM) of example x. Given a covering set ˆS of
Z, let

vmin =

min
x∈{x∈X |∃x(cid:48)∈ ˆSm,||x−x(cid:48)||2≤(cid:15)}

v(x),

(3)

where ˆSm := {x(cid:48) ∈ ˆS|∃xi ∈ Sm, ||xi − x(cid:48)||2 ≤ (cid:15)}. vmin
is the smallest instance-space margin of elements in the
covering balls that contain training examples.

3. Theoretical instruments for empirical

studies on AR

In this section, we rigorously establish the bound mentioned
in the introduction. We study the map T deﬁned in section 2
as a NN (though technically, T now is a map from X to RL,
instead of to Y, such an abuse of notation should be clear
in the context). To begin with, we introduce an assumption,
before we state the generalization error bound guaranteed
by adversarial robustness.

Assumption 3.1 (Monotony). Given a point x ∈ X , let
x(cid:48) be the point on the decision boundary of a NN T that
is closest to x. Then, for all x(cid:48)(cid:48) on the line segment x +
t(x(cid:48) − x), t ∈ [0, 1], the margin M(T x(cid:48)(cid:48), y) decreases
monotonously.

The assumption is a regularity condition on the classiﬁer
that rules out undesired oscillation between x and x(cid:48). To see
how, notice that the margin deﬁned in deﬁnition 3 reﬂects
how conﬁdent the decision is made. Since x(cid:48) is on the
decision boundary, it means the classiﬁer is unsure how it

should be classiﬁed. Thus, when the difference x(cid:48) − x is
gradually added to x, ideally we want the conﬁdence that
we have on classifying x to decrease in a consistent way to
reﬂect the uncertainty.

Theorem 3.1. Let T denote a NN with ReLU and MaxPool-
ing nonlinear activation functions (the deﬁnition is put at
eq. (2) for readers’ convenience), lγ the ramp loss deﬁned at
deﬁnition 4, and Z the instance space assumed in section 3.
Assume that Z is a k-dimensional regular manifold that
accepts an (cid:15)-covering with covering number ( CX
(cid:15) )k, and
assumption assumption 3.1 holds. If T is (cid:15)0-adversarially
robust (deﬁned at deﬁnition 2), (cid:15) ≤ (cid:15)0, and denote vmin
the smallest IM margin in the covering balls that contain
training examples (deﬁned at deﬁnition 6), σi
min the smallest
singular values of weight matrices W i, i = 1, . . . , L − 1
of a NN, {wi}i=1,...,|Y| the set of vectors made up with ith
rows of W L (the last layer’s weight matrix), then given an
i.i.d. training sample Sm = {zi = (xi, yi)}m
i=1 drawn from
Z, its generalization error GE(l ◦ T ) (deﬁned at eq. (1))
satisﬁes that, for any η > 0, with probability at least 1 − η

GE(lγ ◦ T ) ≤ max{0, 1 −

umin
γ

}

(cid:114)

+

2 log(2)C k
X
(cid:15)km

+

2 log(1/η)
m

(4)

where

umin = min

y,ˆy∈Y,y(cid:54)=ˆy

||wy − w ˆy||2

L−1
(cid:89)

i=1

σi
minvmin

(5)

is a lower bound of margins of examples in covering balls
that contain training samples.

The proof of theorem 3.1 is in appendix C. The bound iden-
tiﬁes quantities that would be studied experimentally in sec-
tion 4 to understand the regularization of AR on NNs. The
ﬁrst term in eq. (4) in theorem 3.1 suggests that quantities
related to the lower bound of margin umin might be useful
to study how (cid:15)-adversarial robustness ((cid:15)-AR) regularizes
NNs. However, (cid:15)-AR is guaranteed in the instance space
that determines the smallest instance-space margin vmin. To
relate GE bound with (cid:15)-AR, we characterize in eq. (5) the
relationship between margin with IM, via smallest singular
values of NNs’ weight matrices, suggesting that quantities
related to singular values of NNs’ weight matrices might
be useful to study how AR regularizes NNs as well. An
illustration on how AR could inﬂuence generalization of
NNs through IM is also given in ﬁg. 2a. The rightmost term
in eq. (4) is a standard term in robust framework (Xu &
Mannor, 2012) in learning theory, and is not very relevant
to the discussion. The remaining of this paper are empir-
ical studies that are based on the quantities, e.g., margin
distributions and singular values of NNs’ weight matrices,
that are related to the identiﬁed quantities, i.e., umin, σi
min.

Towards Understanding the Regularization of Adversarial Robustness on Neural Networks

(a)

(b)

Figure 2. (a) Illustration of the regularization effect of adversarial robustness. If a NN T is (cid:15)-adversarially robust, for a given example x
(drawn as ﬁlled squares or circles) and points x(cid:48) in the yellow ball {x(cid:48) | ρ(x, x(cid:48)) ≤ (cid:15)} around x, the predicted labels of x, x(cid:48) should be
the same, and the loss variation is potentially bigger as x(cid:48) moves from the center to the edge, as shown as intenser yellow color at the
edge of a ball. Collectively, the adversarial robustness of each example requires an instance-space margin (IM) to exist for the decision
boundary, shown as the shaded cyan margin. As normally known, margin is related to generalization ability that shrinks the hypothesis
space. In this case, the IM required by adversarial robustness would weed out hypotheses that do not have an adequate IM, such as the
red dashed line shown in the illustration. (b) Illustration of lemma 3.1. Given a NN with ReLU activation function, the feature map
I l at layer l is divided into regions where I l(x) is piecewise linear w.r.t. x. The induced linear map W q
1 is given by diag(τ1(q))W 1,
where diag(τl(q)) is a diagonal matrix whose diagonal entries are given by a vector τ1(q) that has 0-1 values. For example, in region
p, I 1 = W p
1x and distance are horizontally
elongated. Thus given x, x(cid:48), the difference ||I l(x) − I l(x(cid:48))|| between I l(x) and I l(x(cid:48)) is the length of the transformed line segment
x − x(cid:48) drawn, of which each segment is linearly transformed in a different way.

1x and distance between instances x are vertical elongated, while in region q, I 1 = W q

These studies aim to illuminate with empirical evidence on
the phenomena that AR regularizes NNs, reduces GE gaps,
but degrades test performance. 2

Before turning into empirical study, we further present a
lemma to illustrate the relation characterized in eq. (5) with-
out the need to jump into proof of theorem 3.1. It would
motivate our experiments later in section 4.2.2. We state the
following lemma that relates distances between elements
in the instance space with those in the feature space of any

2Note that in the previous paragraph, though we identiﬁes
quantities umin and σi
min related to the upper bound of GE, the
quantities we actually would study empirically are margin dis-
tribution and all singular values that characterize the GE of all
samples, not just the extreme case (upper bound). The analytic
characterization of the GE of all samples is not possible since we
do not have enough information (we do not know the true distri-
bution of samples). That’s why to arrive at close-form analytic
characterization of GE, we resort to the extreme non-asymptotic
large-sample behaviors. The analytic form is a neat way to present
how relevant quantities inﬂuence GE. In the rest of the paper, we
would carry on empirical study on the distributions of margins and
singular values to investigate AR’s inﬂuence on GE of all samples.

intermediate network layers.
Lemma 3.1. Given two instances x, x(cid:48) ∈ X , let I l(x) be
the activation g(W lg(W l−1 . . . g(W 1x))) at layer l of x,
then there exist n ∈ N sets of matrices {W qj
i }i=1...l, j =
1 . . . n, that each of the matrix W qj
is obtained by setting
i
some rows of W i to zero, and {qj}j=1...n are arbitrary
distinctive symbols indexed by j that index W qj
i , such that

||I l(x) − I l(x(cid:48))|| =

n
(cid:88)

(cid:90) ej

j=1

sj

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

l
(cid:89)

i=1

W qj

(cid:12)
(cid:12)
i dt(x − x(cid:48))
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

where s1 = 0, sj+1 = ej, en = 1, sj, ej ∈ [0, 1] — each
[sj, ej] is a segment in the line segment parameterized by t
that connects x and x(cid:48).

Its proof is in appendix C, and an illustration is given in
ﬁg. 2b. Essentially, it states that difference in the feature
space of a NN, induced by the difference between elements
in the instance space, is a summation of the norms of the
linear transformation ((cid:81)l
i=1 W qj
i ) applied on segments of
the line segment that connects x, x(cid:48) in the instance space.

Towards Understanding the Regularization of Adversarial Robustness on Neural Networks

(a)

(b)

Figure 3. Experiment results on CIFAR10/100, and Tiny-ImageNet. The unit of x-axis is the adversarial robustness (AR) strength of NNs,
c.f. the beginning of section 4. (a) Plots of loss gap (and error rate gap) between training and test datasets v.s. AR strength. (b) Plots of
losses (and error rates) on training and test datasets v.s. AR strength.

Since W qj
is obtained by setting rows of W i to zero, the
i
singular values of these induced matrices are intimately
related to weight matrices W i of NN by Cauchy interlac-
ing law by row deletion (Chafai). Since the margin of an
example x is a linear transform of the difference between
IL−1(x) and the IL−1(x(cid:48)) of an element x(cid:48) on the decision
boundary, singular values of {Wi}i=1...L−1 determine the
ampliﬁcation/shrinkage of the IM x − x(cid:48).

4. Empirical studies on regularization of

adversarial robustness

In this section, guided by theorem 3.1, we undertake empir-
ical studies to explore AR’s regularization effects on NNs.
We ﬁrst investigate the behaviors of off-the-shelf architec-
tures of ﬁxed capacity on various datasets in section 4.1
and section 4.2. More corroborative controlled studies that
explore the regularization effects of AR on NNs with varied
capacity are present in appendix B.3.

4.1. Adversarial robustness effectively regularizes NNs

on various datasets

This section aims to explore whether AR can effectively
reduce generalization errors — more speciﬁcally, the sur-
rogate risk gaps. We use adversarial training (Madry et al.,
2018) to build adversarial robustness into NNs. The AR
strength is characterized by the maximally allowed l∞ norm
of adversarial examples that are used to train the NNs. De-
tails on the technique to build adversarial robustness into
NNs is given in appendix B.1.

Our experiments are conducted on CIFAR10, CIFAR100,
and Tiny-ImageNet (ImageNet, 2018) that represent learn-

ing tasks of increased difﬁculties. We use ResNet-56 and
ResNet-110 (He et al., 2016) for CIFAR10/100, and Wide
ResNet (WRN-50-2-bottleneck) (Zagoruyko & Komodakis,
2016) for Tiny-ImageNet (ImageNet, 2018). These net-
works are trained with increasing AR strength. Results are
plotted in ﬁg. 3.

Regularization of AR on NNs. We observe in ﬁg. 3a
(shown as blue lines marked by circles) that GE gaps (the
gaps between training and test losses) decrease as strength
of AR increase; we also observe in ﬁg. 3a that training losses
increase as AR strength increase; these results (and more
results in subsequent ﬁg. 6) imply that AR does regularize
training of NNs by reducing their capacities to ﬁt training
samples. Interestingly, in the CIFAR10/100 results in ﬁg. 3b,
the test losses show a decreasing trend even when test error
rates increase. It suggests that the network actually performs
better measured in test loss as contrast to the performance
measured in test error rates. This phenomenon results from
that less conﬁdent wrong predictions are made by NNs
thanks to adversarial training, which will be explained in
details in section 4.2, when we carry on ﬁner analysis. We
note that on Tiny-ImageNet, the test loss does not decrease
as those on CIFAR10/100.
It is likely because the task
is considerably harder, and regularization hurts NNs even
measured in test loss.

Trade-off between regularization of AR and test error
rates. The error rate curves in ﬁg. 3b also tell that the end
result of AR regularization leads to biased-performing NNs
that achieve degraded test performance. These results are
consistent across datasets and networks.

Towards Understanding the Regularization of Adversarial Robustness on Neural Networks

Seemingly abnormal phenomenon. An seemingly ab-
normal phenomenon in CIFAR10 observed in ﬁg. 3a is that
the error rate gap actually increases. It results from the same
underlying behaviors of NNs, which we would introduce in
section 4.2, and an overﬁtting phenomenon that AR cannot
control. Since it would be a digress to explain, it is put in
appendix B.3.

We ﬁnally note that the adversarial robustness training re-
produced is relevant, of which the defense effect is com-
parable with existing works. One may refer to ﬁg. 12 in
appendix D.2 for the details. We can see from it that similar
adversarial robustness to Madry et al. (2018) and Li et al.
(2018) is achieved for CIFAR10/100, Tiny-ImageNet in the
NNs we reproduce.

4.2. Reﬁned analysis through margins and singular

values

The experiments in the previous sections conﬁrm that AR re-
duces GE, but decreases accuracy. We study the underlying
behaviors of NNs to analyze what have led to it here. More
speciﬁcally, we show that adversarial training implements
(cid:15)-adversarial robustness by making NNs biased towards less
conﬁdent solutions; that is, the key ﬁnding we present in
section 1.1 that explains both the prevented sudden change
in prediction w.r.t. sample perturbation (i.e., the achieved
AR), and the reduced test accuracy.

4.2.1. MARGINS THAT CONCENTRATE MORE AROUND

ZERO LEAD TO REDUCED GE GAP

To study how GE gaps are reduced, theorem 3.1 suggests
we ﬁrst look at the margins of examples — a lower bound of
margins is umin in eq. (5). The analysis on margins has been
a widely used tool in learning theory (Bartlett et al., 2017).
It reﬂects the conﬁdence that a classiﬁer has on an example,
which after being transformed by a loss function, is the sur-
rogate loss. Thus, the loss difference between examples are
intuitively reﬂected in the difference in conﬁdence character-
ized by margins. To study how AR inﬂuences generalization
of NNs, distributions of samples which are obtained by train-
ing ResNet-56 on CIFAR10 and CIFAR100 with increased
AR strength (the same setting as for ﬁg. 3). Applying the
same network of ResNet-56 respectively on CIFAR-10 and
CIFAR-100 of different learning difﬁculties creates learning
settings of larger- and smaller-capacity NNs.

Concentration and reduced accuracy.
In ﬁg. 4, we can
see that in both CIFAR10/100, the distributions of margins
become more concentrated around zero as AR grows. The
concentration moves the mode of margin distribution to-
wards zero and more examples slightly across the decision
boundaries, where the margins are zero, which explains the

(a) CIFAR10 Test

(b) CIFAR100 Test

(c) CIFAR10 Training

(d) CIFAR100 Training

Figure 4. Margin distributions of NNs with AR strength 4, 8, 16
on Training and Test sets of CIFAR10/100.

reduced accuracy 3.

Concentration and reduced loss/GE gap. The concen-
tration has different consequences on training and test losses.
Before describing the consequences, to directly relate the
concentration to loss gap, we further introduce estimated
probabilities of examples. This is because though we use
ramp loss in theoretical analysis, in the experiments, we
explore the behaviors of more practically used cross entropy
loss. The loss maps one-to-one to estimated probability, but
not to margin, though they both serve as a measure of conﬁ-
dence. Suppose p(x) is the output of the softmax function
of dimension L (L is the number of target classes), and y is
the target label. The estimated probability of x would be the
y-th dimension of (p(x)), i.e., (p(x))y. On the training
sets, since the NNs are optimized to perform well on the
sets, only a tiny fraction of them are classiﬁed wrongly. To
concentrate the margin distribution more around zero, is to

3We remark a possibly confusing phenomenon here about the
margin. The bound eq. (4) might give the impression that a smaller
margin might lead to a larger generalization error, while the empir-
ical study instead shows that the NNs with a smaller margin have
a smaller generalization error. The hypothesized confusion is a
misunderstanding of the generalization bound analysis. The upper
bound is a worst case analysis of GE. However, in practice, the in-
teresting object is the average gap between the training losses and
the test losses, i.e., the GE. Unfortunately, the average gap cannot
be analyzed analytically (cf. footnote 2). Thus, we, and also the
statistical learning community, resort to worst case analysis to ﬁnd
an upper bound on GE to identify quantities that might inﬂuence
GE. In this case, the phenomenon suggests that the bound might be
loose, though this is a problem that plagues the statistical learning
community (Nagarajan & Kolter, 2019). But our focus in this work
is not to derive tight bounds, or reach deﬁnite conclusions from
bounds alone, but to guide experiments with the bound.

Towards Understanding the Regularization of Adversarial Robustness on Neural Networks

make almost all of predictions that are correct less conﬁ-
dent. Thus, a higher expected training loss ensues. On the
test sets, the estimated probabilities of the target class con-
centrate more around middle values, resulting from lower
conﬁdence/margins in predictions made by NNs, as shown
in ﬁg. 5a (but the majority of values are still at the ends).
Note that wrong predictions away from decision boundaries
(with large negative margins) map to large loss values in the
surrogate loss function. Thus, though NNs with larger AR
strength have lower accuracy, they give more predictions
whose estimated probabilities are at the middle (compared
with NNs with smaller AR strength). These predictions,
even if relatively more of them are wrong, maps to smaller
loss values, as shown in ﬁg. 5b, where we plot the histogram
of loss values of test samples. In the end, it results in ex-
pected test losses that are lower, or increase in a lower rate
than the training losses on CIFAR10/100, Tiny-ImageNet,
as shown in ﬁg. 3b. The reduced GE gap results from the
increased training losses, and decreased or less increased
test losses.

4.2.2. AR MAKES NNS SMOOTHE PREDICTIONS W.R.T.
INPUT PERTURABTIONS IN ALL DIRECTIONS

(a) Prob. Histogram

(b) Loss Histogram

To begin with, we show that the singular values of the weight
matrix of each layer determine the perturbation in margins
of samples induced by perturbations in the instance space.
Such a connection between singular values and the perturba-
tion of outputs of a single layer, i.e., ReLU(W δx), has been
discussed in section 1.1. In the following, with lemma 3.1,
we describe how the relatively more complex connection
between margins and singular values of each weight matrix
of layers of NNs holds. Observe that margins are obtained
by applying a piece-wise linear mapping (c.f. the margin
operator in deﬁnition 3) to the activation of the last layer
of a NN. It implies the perturbations in activation of the
last layer induce changes in margins in a piece-wise linear
way. Meanwhile, the perturbation in the activation of the
last layer (induced by perturbation in the instance space) is
determined by the weight matrix’s singular values of each
layer of NNs. More speciﬁcally, this is explained as follows.
Lemma 3.1 shows that the perturbation δI induced by δx,
(cid:12)
is given by (cid:80)n
(cid:12)
(cid:12). Note that for
each i, W qi
is a matrix. By Cauchy interlacing law by row
i
deletion (Chafai), the singular values of W i, the weight
matrix of layer i, determine the singular values of W qj
i .
Thus, suppose l = 1, we have the change (measured in
(cid:12)
1 δxdt(cid:12)
norm) induced by perturbation as (cid:80)n
(cid:12).
(cid:12)
The singular values of W 1 would determine the variance
(of norms) of activation perturbations induced by perturba-
tions δx, similarly as explained in section 1.1 except that
the norm perturbation now is obtained by a summation of
(cid:12)
n terms (cid:12)
(cid:12) (each of which is the exact form dis-
(cid:12)
cussed in section 1.1) weighted by 1/(ej − sj). Similarly,
for the case where l = 2 . . . L − 1, the singular values of
W l determine the variance of perturbations in the output
of layer l that induced by the perturbations in the output
of the previous layer (the input to layer l), i.e., layer l − 1.
Consequently, we choose to study these singular values.

(cid:12)
(cid:12)
i δxdt
(cid:12)

i=1 W qj

1 δxdt(cid:12)
(cid:12)

(cid:12)
(cid:12)W qj

(cid:12)
(cid:12)W qj

(cid:82) ej
sj

(cid:82) ej
sj

(cid:81)l

j=1

j=1

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

(c) CIFAR10

(d) CIFAR100

Figure 5. (a)(b) are histograms of estimated probabilities and
losses of the test set sample of NNs trained with AR strength
4, 8, 16. We plot a subplot of a narrower range inside the plot
of the full range to better show the histograms of examples that
are around the middle values induced by AR. (c)(d) are standard
deviations of singular values of weight matrices of NNs at each
layer trained on CIFAR10/100 with AR strength 4, 16. The AR
strength 8 is dropped for clarity.

The observation in section 4.2.1 shows that AR make NNs
just less conﬁdent by reducing the variance of predictions
made and concentrate margins more around zero. In this
section, we study the underlying factors of AR that make
NNs become less conﬁdent.

We show the standard deviation of singular values of each
layer of ResNet56 trained on CIFAR10/100 earlier in ﬁg. 5c
ﬁg. 5d. Overall, the standard deviation of singular values
associated with a layer of the NN trained with AR strength
4 is mostly larger than that of the NN with AR strength
16. The STD reduction in CIFAR100 is relatively smaller
than CIFAR10, since as observed in ﬁg. 4b, the AR induced
concentration effect of margin distributions is also relatively
less obvious than that in ﬁg. 4a. More quantitative analysis
is given in appendix B.2. This leads us to our key results
described in section 1.1.

Acknowledgements

This work is supported in part by National Natural Sci-
ence Foundation of China (Grant No. 61771201), Program
for Guangdong Introducing Innovative and Enterpreneurial
Teams (Grant No. 2017ZT07X183), Guangdong R&D

Towards Understanding the Regularization of Adversarial Robustness on Neural Networks

Key Project of China (Grant No. 2019B010155001) and
Guangzhou Key Laboratory of Body Data Science (Grant
No. 201605030011).

References

Attias, I., Kontorovich, A., and Mansour, Y.

Improved
generalization bounds for robust learning. Technical re-
port, 2018. URL https://arxiv.org/pdf/1810.
02180.pdf.

Bartlett, P., Foster, D. J., and Telgarsky, M. Spectrally-
normalized margin bounds for neural networks. In NIPS,
pp. 6240–6249, 2017.

B´ecigneul, G. On the effect of pooling on the geometry
of representations. Technical report, mar 2017. URL
http://arxiv.org/abs/1703.06726.

Chafai, D. Singular Values Of Random Matrices. Technical

report.

Cullina, D., Bhagoji, A. N., and Mittal, P. PAC-learning in
the presence of evasion adversaries. In NIPS, 2018.

Fawzi, A., Fawzi, O., and Frossard, P. Analysis of classiﬁers’
robustness to adversarial perturbations. Mach. Learn., 107
(3):481–508, 2018. doi: 10.1007/s10994-017-5663-3.

Glorot, X. and Bengio, Y. Understanding the difﬁculty of
training deep feedforward neural networks. In AISTATS,
2010.

Glorot, X., Bordes, A., and Bengio, Y. Deep Sparse Rectiﬁer

Neural Networks. In AISTATS, 2011.

Goodfellow, I. J., Shlens, J., and Szegedy, C. Explaining
and Harnessing Adversarial Examples. In ICLR, 2015.

He, K., Zhang, X., Ren, S., and Sun, J. Identity Mappings

in Deep Residual Networks. In ECCV, 2016.

Ilyas, A., Santurkar, S., Tsipras, D., Engstrom, L., Tran, B.,
and Madry, A. Adversarial examples are not bugs, they
are features. In NeuIPS, pp. 125–136, 2019.

ImageNet, T. Tiny imagenet, 2018. URL https://

tiny-imagenet.herokuapp.com/.

Jia, K., Li, S., Wen, Y., Liu, T., and Tao, D. Orthogonal
Deep Neural Networks. Technical report, 2019. URL
http://arxiv.org/abs/1905.05929.

Kannan, H., Kurakin, A., and Goodfellow, I. Adversarial
logit pairing. Technical report, 2018. URL http://
arxiv.org/abs/1803.06373.

Khim, J. and Loh, P.-L. Adversarial Risk Bounds for Binary
Classiﬁcation via Function Transformation. Technical re-
port, 2018. URL http://arxiv.org/abs/1810.
09519.

Krizhevsky, A., Sutskever, I., and Hinton, G. E. ImageNet
Classiﬁcation with Deep Convolutional Neural Networks.
In NIPS, 2012.

Kurakin, A., Goodfellow, I. J., and Bengio, S. Adversarial

Machine Learning at Scale. In ICLR, 2017.

Lee, C.-y., Xie, S., and Gallagher, P. W. Deeply-Supervised

Nets. In AISTATS, 2015.

Li, Y., Min, M. R., Yu, W., Hsieh, C.-J., Lee, T. C. M.,
and Kruus, E. Optimal Transport Classiﬁer: Defend-
ing Against Adversarial Attacks by Regularized Deep
Embedding. Technical report, 2018. URL http:
//arxiv.org/abs/1811.07950.

Lyu, C., Huang, K., and Liang, H. N. A uniﬁed gradient
regularization family for adversarial examples. In ICDM,
2015.

Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and
Vladu, A. Towards Deep Learning Models Resistant
to Adversarial Attacks. In ICLR, 2018.

Miyato, T., Maeda, S. I., Ishii, S., and Koyama, M. Vir-
tual Adversarial Training: A Regularization Method for
Supervised and Semi-Supervised Learning. PAMI, pp.
1–16, 2018. ISSN 19393539. doi: 10.1109/TPAMI.2018.
2858821.

Moosavi-Dezfooli, Mohsen, S., Fawzi, A., Uesato, J., and
Frossard, P. Robustness via curvature regularization, and
vice versa. In CVPR, pp. 9070–9078, 2019.

Nagarajan, V. and Kolter, J. Z. Uniform convergence may
be unable to explain generalization in deep learning. In
NeuIPS, pp. 11615–11626, 2019.

Neyshabur, B., Bhojanapalli, S., and Srebro, N. A PAC-
Bayesian Approach to Spectrally-Normalized Margin
Bounds for Neural Networks. In ICLR, 2018a.

Neyshabur, B., Li, Z., Bhojanapalli, S., LeCun, Y., and
Srebro, N. The Role Of Over-parametrization In General-
ization Of Neural Networks. In ICLR, 2018b.

Pfeiffer, F. W. Automatic differentiation in prose. In ICLR

Workshop, 2017.

Qin, C., Martens, J., Gowal, S., Krishnan, D., Dvijotham, K.,
Fawzi, A., De, S., Stanforth, R., and Kohli, P. Adversarial
Robustness through Local Linearization. In NeurIPS, pp.
13847–13856, 2019.

Towards Understanding the Regularization of Adversarial Robustness on Neural Networks

Schmidt, L., Santurkar, S., Tsipras, D., Talwar, K., and
Madry, A. M. Adversarially Robust Generalization Re-
quires More Data. In NIPS, pp. 5014–5026, 2018.

Yin, D. and Bartlett, P. Rademacher Complexity for Adver-

sarially Robust. In ICML, 2018.

Zagoruyko, S. and Komodakis, N. Wide Residual Networks.

Sedghi, H., Gupta, V., and Long, P. M. The Singular Values

In BMVC, 2016.

Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
Understanding deep learning requires rethinking general-
ization. In ICLR, 2016.

Zhang, H., Dauphin, Y. N., and Ma, T. Fixup Initializa-
tion: Residual Learning Without Normalization. In ICLR,
2018.

Zhang, H., Yu, Y., Jiao, J., Xing, E. P., Ghaoui, L. E., and
Jordan, M. I. Theoretically Principled Trade-off between
Robustness and Accuracy. In ICML, 2019.

of Convolutional Layers. In ICLR, 2018.

Shalev-Shwartz, S. and Ben-David, S. Understanding Ma-
chine Learning: From Theory to Algorithms. Understand-
ing Machine Learning: From Theory to Algorithms. Cam-
bridge University Press, 2014. ISBN 9781107057135.

Sinha, A., Namkoong, H., and Duchi, J. Certifying Some
Distributional Robustness with Principled Adversarial
Training. In ICLR, 2018.

Sokolic, J., Giryes, R., Sapiro, G., and Rodrigues, M. R. D.
IEEE
Robust Large Margin Deep Neural Networks.
Transactions on Signal Processing, 65(16):4265–4280,
aug 2017. ISSN 1053-587X. doi: 10.1109/TSP.2017.
2708039.

Su, D., Zhang, H., Chen, H., Yi, J., and Aug, C. V.

Is
Robustness the Cost of Accuracy ? – A Comprehensive
Study on the Robustness of. In ECCV, 2018.

Szegedy, C., Zaremba, W., and Sutskever, I.

Intriguing

properties of neural networks. In ICLR, 2014.

Tram`er, F., Kurakin, A., Papernot, N., Goodfellow, I.,
Boneh, D., and McDaniel, P. Ensemble adversarial train-
ing: Attacks and defenses. Technical report, 2017. URL
http://arxiv.org/abs/1705.07204.

Tsipras, D., Santurkar, S., Engstrom, L., Turner, A., and
Ma¸dry, A. Robustness May Be at Odds with Accuracy.
In ICLR, 2019.

Tsuzuku, Y., Sato, I., and Sugiyama, M. Lipschitz-Margin
Training : Scalable Certiﬁcation of Perturbation Invari-
ance for Deep Neural Networks. In NIPS, pp. 6541–6550,
2018.

Verma, N. Distance Preserving Embeddings for General
n-Dimensional Manifolds. Journal of Machine Learning
Research, 14:2415–2448, 2013.

Wang, J. and Zhang, H. Bilateral adversarial training: To-
wards fast training of more robust models against adver-
sarial attacks. In ICCV, pp. 6629–6638, 2019.

Xie, C., Wu, Y., Maaten, L. v. d., Yuille, A. L., and He, K.
Feature denoising for improving adversarial robustness.
In CVPR, pp. 501–509, 2019.

Xu, H. and Mannor, S. Robustness and generalization. Ma-
chine Learning, 86(3):391–423, 2012. ISSN 08856125.
doi: 10.1007/s10994-011-5268-1.

Towards Understanding the Regularization of Adversarial Robustness on Neural Networks

Appendices

A. Related works

Generalization and robustness. Robustness in machine
learning models is a large ﬁeld. We review some more
works that analyze robustness from the statistical perspec-
tive. The majority of works that study adversarial robustness
from the generalization perspective study the generalization
behaviors of machine learning models under adversarial
risk. The works that study adversarial risk include Attias
et al. (2018); Schmidt et al. (2018); Cullina et al. (2018); Yin
& Bartlett (2018); Khim & Loh (2018); Sinha et al. (2018).
The bounds obtained under the setting of adversarial risk
characterize the risk gap introduced by adversarial examples.
Thus, it is intuitive that a larger risk gap would be obtained
for a larger allowed perturbation limit (cid:15), which is roughly
among the conclusions obtained in those bounds. That is to
say, the conclusion normally leads to a larger generalization
error as an algorithm is asked to handle more adversarial
examples, for that it focuses on characterizing the error of
adversarial examples, not that of natural examples. How-
ever, adversarial risk is not our focus. In this paper, we
study when a classiﬁer needs to accommodate adversarial
examples, what is the inﬂuence that the accommodation has
on generalization behaviors of empirical risk of natural data.

Hard and soft adversarial robust regularization. We
study the behaviors of NNs that are trained in the way that
adversarial examples are required to be classiﬁed correctly.
We note that the adversarial robustness required can also be
built in NNs in a soft way by adding a penalty term in the
risk function. Relevant works includes Lyu et al. (2015) and
Miyato et al. (2018). This line of works is not our subject of
investigation. They focus on increasing test performance in-
stead of defense performance. The focus of our works is to
study the behaviors that lead to standard performance degra-
dation when a network is trained to has a reasonable defense
ability to adversarial examples. For example, a 50% accu-
racy on adversarial examples generated by PGD methods
(Madry et al., 2018) in ﬁg. 12 is a defense ability that can
serve as a baseline for a reasonable defense performance. It
is natural that in the setting where the requirement to defend
against adversarial examples is dropped, the regularization
can be weakened (added as a penalty term) to only aim to
improve the test performance of the network. In this case,
no performance degradation would occur, but the defense
performance is also poor.

Explicit regularization that increases robustness of NNs
by imposing smoothness through a penalty term. The
smoothing effect of adversarial training on the loss sur-
face has been observed in contemporary works (Moosavi-
Dezfooli et al., 2019; Qin et al., 2019). And based on such an
observation, explicit regularization is formulated by adding

a penalty term to the risk function to increase NNs’ ro-
bustness. The message of this work is different from the
insights of (Moosavi-Dezfooli et al., 2019; Qin et al., 2019)
related to regularization. They (Moosavi-Dezfooli et al.,
2019; Qin et al., 2019) show that if the output of NNs is
explicitly smoothed through a penalty term thorough curva-
ture regularization (Moosavi-Dezfooli et al., 2019), or local
linearization (Qin et al., 2019), then a certain degree of ad-
versarial robustness (AR) can be achieved. The penalty term
works as a regularizer because it is explicitly formulated
that way. It is not clear whether adversarial training, which
is a different and arguably the most widely used technique,
has the effect of a regularizer. This is the issue that is inves-
tigated in this work, and we show that adversarial training
effectively regularizes NNs, which is not clear previously.
In addition, this work has shown that adversarial training
has a smoothing effect on features of all layers, instead of
just the loss surface. Such a ﬁne-grained analysis is possi-
ble because of the theoretical instruments developed in this
work, and is absent previously.

B. Further empirical studies on adversarial

robustness

B.1. Technique to build adversarial robustness

To begin with, we describe the technique that we use to
build AR into NNs. As mentioned in the caption of ﬁg. 1,
we choose arguably the most well received technique, i.e.,
the adversarial training method (Madry et al., 2018). Specif-
ically, we use l∞-PGD (Madry et al., 2018) untargeted at-
tack adversary, which creates an adversarial example by
performing projected gradient descent starting from a ran-
dom perturbation around a natural example. Then, NNs
are trained with adversarial examples. NNs with different
AR strength are obtained by training them with increasingly
stronger adversarial examples. The adversarial strength of
adversarial examples is measured in the l∞ norm of the per-
turbation applied to examples. l∞-norm is rescaled to the
range of 0 − 255 to present perturbations applied to differ-
ent datasets in a comparable way; that means in ﬁg. 1 ﬁg. 3
ﬁg. 4 ﬁg. 5 and ﬁg. 6 ﬁg. 12, AR is measured in this scale.
We use 10 steps of size 2/255 and maximum of = [4/255,
8/255, 16/255] respectively for different defensive strength
in experiments. For example, a NN with AR strength 8 is
a NN trained with adversarial examples generated by per-
turbations whose l∞ norm are at most 8. Lastly, we note
that although adversarial training could not precisely guar-
antee an adversarial robustness radius of (cid:15), a larger l∞ norm
used in training would make NNs adversarially robust in
a larger ball around examples. Thus, though the precise
adversarial robustness radius is not known, we know that
we are making NNs adversarially robust w.r.t. a larger (cid:15).
Consequently, it enables us to study the inﬂuence of (cid:15)-AR

Towards Understanding the Regularization of Adversarial Robustness on Neural Networks

on NNs by studying NNs trained with increasing l∞ norm.

B.2. Quantitative analysis of variance reduction in

singular values

Here, we provide more quantitative analysis on ﬁg. 5c and
ﬁg. 5d, as noted previously in section 4.2.2.

i σ4

i σ16

i − σ16

i − σ4

Quantitatively, we can look at the accumulated standard
deviation (STD) difference in all layers. We separate the
layers into two group: the group that the STD (denoted
σ4
i ) of singular values of layer i (of the NN trained) with
AR strength 4 that is larger than that (denoted σ16
i ) of AR
strength 16; and the group that is smaller. In CIFAR10, for
the ﬁrst group, the summation of the difference/increments
of STD of the two networks ((cid:80)
i ) is 4.7465, and
the average is 0.1158. For the second groups, the summa-
tion ((cid:80)
i ) is 0.4372, and the average is 0.0312. In
CIFAR100, the summation of the ﬁrst group is 3.7511, and
the average is 0.09618; the summation of the second group
is 0.4372, and the average is 0.1103. The quantitative com-
parison shows that the accumulated STD decrease in layers
that have their singular value STDs decreased (comparing
STD of the NN with AR strength 16 with STD of the NN
with AR strength 4) is a magnitude larger the accumulated
STD increase in the layers that have their singular value
STDs increased. The magnitude difference is signiﬁcant
since the STDs of singular values of most layers are around
1.

B.3. Discrepancy between trends of loss and error rate

gaps in large capacity NNs

In section 4.1, we have noted an inconsistent behaviors
of CIFAR10, compared with that of CIFAR100 and Tiny-
ImageNet: the error gap reduces for CIFAR100 and Tiny-
ImageNet, but increases for CIFAR10. It might suggest
that AR does not effectively regularize NNs in the case
of CIFAR10. However, we show in this section that the
abnormal behaviors of CIFAR10 are derived from the same
margin concentration phenomenon observed in section 4.2.1
due to capacity difference, and compared with the error
gaps, the GE/loss gaps are more faithfully representatives of
the generalization ability of the NNs. Thus, the seemingly
abnormal phenomenon corroborate, not contradict, the key
results present in section 1.

Using CIFAR10 and CIFAR100 as examples and evidence
in the previous sections, we explain how the discrepancy
emerges from AR’s inﬂuence on margin distributions of the
same network trained on tasks with different difﬁculties.
Further evidence that the discrepancy arises from capac-
ity difference would be shown at appendix B.3, where we
run experiments to investigate GE gap of NNs with varied
capacities on the same task/dataset.

1. On CIFAR10, the margin distribution of training sets
not only concentrate more around zero, but also skews
towards zero. As shown in the margin distribution on
training sets of CIFAR10 in ﬁg. 4c, we ﬁnd that the large
error gap is caused by the high training accuracy that
is achieved with a high concentration of training sam-
ples just slightly beyond the decision boundary. This
phenomenon does not happen in CIFAR100. Compar-
ing margin distribution on the test set in ﬁg. 4(a) in
ﬁg. 4a, the margin distribution on the training set in
ﬁg. 4c is highly skewed, i.e., asymmetrically distributed
w.r.t. mean. While the margin distributions of CIFAR100
training set in ﬁg. 4d is clearly less skewed, and looks
much more like a normal distribution, as that of the mar-
gin distribution on the test set.

2. The high skewness results from the fact that the NN
trained on CIFAR10 is of large enough capacity to overﬁt
the training set. As known, CIFAR100 is a more difﬁcult
task w.r.t. CIFAR10 with more classes and less training
examples in each class. Thus, relatively, even the same
ResNet56 network is used, the capacity of the network
trained on CIFAR10 is larger than the one trained on
CIFAR100. Recall that NNs have a remarkable ability to
overﬁt training samples (Zhang et al., 2016). And note
that though AR requires in a ball around an example,
the examples in the ball should be of the same class,
since the ball is supposed only to include imperceptible
perturbation to the example, few of the training samples
are likely in the same ball. Thus, the ability to overﬁt
the training set is not regularized by AR: if NNs can
overﬁt all training samples, it can still overﬁt some more
examples that are almost imperceptibly different. For CI-
FAR10, since NNs have enough capacity, the NN simply
overﬁts the training set.

3. However, as shown in the observed overﬁtting phe-
nomenon in ﬁg. 4c, the high training accuracy is made up
of correct predictions with relatively lower conﬁdence
(compared with NNs with lower AR), which is bad and
not characterized by the error rate; and the low test ac-
curacy are made up of wrong predictions with relatively
lower conﬁdence as well (as explain in section 4.2.1),
which is good, and not characterized by error rate as well.
Thus, the error gap in this case does not characterize
the generalization ability (measured in term of predic-
tion conﬁdence) of NNs well, while the GE gap more
faithfully characterizes the generalization ability, and
show that AR effectively regularizes NNs. In the end,
AR still leads to biased poorly performing solutions —
since the overﬁtting in training set does not prevent the
test margin distribution concentrating more around zero,
which leads to higher test errors of CIFAR10 as shown
in ﬁg. 3b. It further suggests that the damage AR done
to the hypothesis space is not recovered by increasing
capacity, however the ability of NNs to ﬁt arbitrary labels

Towards Understanding the Regularization of Adversarial Robustness on Neural Networks

(a) Gap Curves

(b) Error Rate & Loss Curves

Figure 6. The four plots from upper left to lower bottom (in each subﬁgure) are NNs with increasingly smaller spectral complexity, where
“Spectral Norm 1” means for each weight matrix of the NN, its spectral norm is at most 1. (a) Plots of training/test loss gap (and error gap)
against adversarial robustness strength. (b) Training/test losses and error rates against increased strength of adversarial robustness.

is not hampered by AR.

B.4. Further evidence of regularization effects on NNs

with varied capacity

In previous sections, we observe AR consistently effectively
regularizes NNs; meanwhile, we also observe that in the
case where a NN has a large capacity, it can spuriously
overﬁt training samples and lead to an increased error gap.
In this section, we present additional results by applying
AR to networks of varied capacities. The effects of adver-
sarial training on a larger NNs, i.e., ResNet 110 is given
in appendix B.4.1. Then, AR applied on NNs with con-
trolled capacities through spectral normalization is given in
appendix B.4.2. This is to ensure that our observations and
analysis in previous sections exist not just at some singular
points, but also in a continuous area in the hypothesis space.

B.4.1. REGULARIZATION EFFECTS ON NNS WITH

LARGER CAPACITY

To preliminarily validate that the regularization effects ob-
served in section 4.1 manifest in NNs with varied capacities,
we investigate the regularization effects of AR on a larger
NNs, i.e., ResNet 110. The results are shown in ﬁg. 7. The
observed phenomenon is the same with that of ResNet56
presented in section 4.1, and thus corroborates our results.

B.4.2. REGULARIZATION EFFECTS ON NNS WITH

CONTROLLED CAPACITIES

To control capacities of NNs quantitatively, we choose
the measure based on spectral norm (Bartlett et al., 2017;
Neyshabur et al., 2018a). In spectral norm based capac-

ity measure bound (Bartlett et al., 2017; Neyshabur et al.,
2018a), the NN capacity is normally proportional to a quan-
tity called spectral complexity (SC), which is deﬁned as
follows.
Deﬁnition 7 (Spectral Complexity). Spectral complexity
SC(T ) of a NN T is the multiplication of spectral norms of
weight matrices of layers in a NN.

SC(T ) =

L
(cid:89)

i=1

||W i||2

where {W i}i=1...L denotes weight matrices of layers of the
NN.

To control SC, we apply the spectral normalization (SN)
(Sedghi et al., 2018) on NNs. The technique renormalizes
the spectral norms of the weight matrices of a NN to a
designated value after certain iterations. We carry out the
normalization at the end of each epoch.

We train ResNet56 with increasingly strong AR and with
increasingly strong spectral normalization. The results are
shown in ﬁg. 6.

As can be seen, as the capacity of NNs decreases (from
upper left to bottom right in each sub-ﬁgure), the error
gap between training and test gradually changes from an
increasing trend to a decreasing trend, while the loss gap
keeps a consistent decreasing trend. It suggests that the
overﬁtting phenomenon is gradually prevented by another
regularization techniques, i.e., the spectral normalization.
As a result, the regularization effect of AR starts to emerge
even in the error gap, which previously manifests only in
the loss gap. The other curves corroborate our previous
observations and analysis as well.

Towards Understanding the Regularization of Adversarial Robustness on Neural Networks

(a)

(b)

Figure 7. Experiment results on CIFAR10/100 with ResNet-110 (He et al., 2016). The unit of x-axis is the adversarial robustness (AR)
strength of NNs, c.f. the beginning of section 4. (a) Plots of loss gap (and error rate gap) between training and test datasets v.s. AR
strength. (b) Plots of losses (and error rates) on training and test datasets v.s. AR strength.

B.5. Further evidence on the smoothing effect of

adversarial robustness

We quantitatively measure the smoothing effect around
examples here by measuring the average maximal loss
change/variation induced by the perturbation (of a ﬁxed
inﬁnity norm) applied on examples. We found that the loss
variation decreases as networks become increasingly adver-
sarially robust. Note that the loss of an example is a proxy
to the conﬁdence of the example — it is the logarithm of
the estimated probability (a characterization of conﬁdence)
of the NN classiﬁer.

For a given maximal perturbation range characterized by the
inﬁnity norm, we generate adversarial examples within that
norm for all test samples. For each example, the maximal
loss variation/change of the adversarial example w.r.t. the
natural example is computed for networks with different
adversarial strength. To obtain statistical behaviors, we
compute the average and standard deviation of such maxima
of all test samples. The results are shown in ﬁg. 8. The
exact data can be found in table 1.

We can see that the average loss variation decreases with ad-
versarial robustness. The standard deviation decreases with
network adversarial robustness as well. The phenomenon
that the standard deviation is comparably large with the
mean might need some explanation. This is because differ-
ent examples have different losses, thus the loss varies in
relatively different regimens — the more wrongly classiﬁed
examples vary in a larger magnitude, and vice versa for
more correctly classiﬁed examples. This phenomenon leads
to the large standard deviation of the loss variation.

Figure 8. Average maximal loss variation induced by adversar-
ial examples in networks with increasing adversarial robustness.
The experiments are carried on CIFAR10/100. (cid:15) represents the
maximal perturbation can be applied on natural test examples to
generate adversarial examples. It is measured in the inﬁnity norm.
The larger the (cid:15), the stronger the perturbation is. The error bars
represent standard deviation.

B.6. Further experiments on using FGSM in
adversarial training to build adversarial
robustness

We explain the choice of PGD as the representative of ad-
versarial training techniques here. Various adversarial train-
ing methods are variant algorithms that compute ﬁrst order
approximation to the point around the input example that
minimizes the label class conﬁdence. The difference is
how close the approximation is. Recent works on adver-
sarial examples exclusively only use PGD in experiments

Towards Understanding the Regularization of Adversarial Robustness on Neural Networks

Table 1. Data of the smoothing effect of PGD adversarial training in ﬁg. 8.

Dataset

Attack Strength

4

Defensive Strength
8

16

CIFAR10

CIFAR100

(cid:15) = 1
(cid:15) = 2
(cid:15) = 4

(cid:15) = 1
(cid:15) = 2
(cid:15) = 4

0.0273 ± 0.0989
0.0590 ± 0.1637
0.1337 ± 0.2494

0.0236 ± 0.0778
0.0477 ± 0.1337
0.1006 ± 0.2137

0.0215 ± 0.0588
0.0443 ± 0.1102
0.0888 ± 0.1816

0.0550 ± 0.1276
0.1072 ± 0.2138
0.1868 ± 0.2946

0.0430 ± 0.1043
0.0839 ± 0.1802
0.1563 ± 0.2712

0.0379 ± 0.0886
0.0732 ± 0.1568
0.1355 ± 0.2494

results. Here we mainly present counterparts of the results
analyzed there.

Adversarial robustness reduces the standard deviation
of singular values of weight matrices in the network.
In section 4.2.2, we ﬁnd that for NNs with stronger adversar-
ial robustness, the standard deviation of singular values of
weight matrices is smaller in most layers. The phenomenon
has been consistently observed in NNs trained with FGSM
on CIFAR10/100, as shown in ﬁg. 11c and ﬁg. 11d. Please
refer to section 1.1 and section 4.2.2 for the analysis of the
results. Here we mainly present counterparts of the results
analyzed there.

In conclusion, all key empirical results have been consis-
tently observed in NNs trained with FGSM.

Table 2. Data of ﬁg. 9

Dataset

CIFAR10

CIFAR100

Defensive Strength
16
8
4

89.32
0.038
0.467
0.429

62.01
0.469
1.776
1.307

86.67
0.086
0.495
0.409

59.78
0.656
1.723
1.067

82.83
0.252
0.637
0.385

56.30
0.822
1.797
0.975

Test Acc.
Trn Loss
Test Loss
∆ Loss

Test Acc.
Trn Loss
Test Loss
∆ Loss

(Kannan et al., 2018; Schmidt et al., 2018; Xie et al., 2019;
Ilyas et al., 2019; Wang & Zhang, 2019). It is also a very
strong multi-step attack method that improves over many of
its antecedents: NNs trained by FGSM could have no de-
fense ability to adversarial examples generated by PGD, as
shown in Table 5 in Madry et al. (2018); multi-step methods
prevent the pitfalls of adversarial training with single-step
methods that admit a degenerate global minimum (Tram`er
et al., 2017). Thus, we believe the observations in this work
is representative for various adversarial training techniques.
Yet, even in the worst case, this work at least makes a ﬁrst
step to understand a representative approach of the approxi-
mation.

To corroborate the analysis, we also use FGSM (Goodfellow
et al., 2015) in the adversarial training to build adversarial
robustness into NNs. The results are consistent with the
results obtained using PGD. The experiments are carried on
CIFAR10/100. We present key plots that support the results
obtained in the main con- tent here. All the setting are same
with that described in appendix B.1 of PGD, except that we
replace PGD with FGSM.

Adversarial robustness reduces generalization gap and
standard test performance.
In section 4.1, we ﬁnd that
NNs with stronger adversarial robustness tend to have
smaller loss/generalization gap between training and test
sets. Consistent phenomenon has been observed in net-
works adversarially trained with FGSM on CIFAR10/100,
as shown in ﬁg. 9a. Consistent standard test performance
degradation has been observed in adversarially trained with
FGSM on CIFAR10/100 as well, as shown in ﬁg. 9b. The
exact data can be found in table 2.

Adversarial robustness concentrates examples around
decision boundaries.
In section 4.2.1, we ﬁnd that the
distributions of margins become more concentrated around
zero as AR grows. The phenomenon has been observed
consistently in networks adversarially trained with FGSM
on CIFAR10/100, as shown in ﬁg. 10. Phenomenon in ﬁg. 5a
and ﬁg. 5b are also reproduced consistently in ﬁg. 11a and
ﬁg. 11b. Please refer to section 4.2.1 for the analysis of the

Towards Understanding the Regularization of Adversarial Robustness on Neural Networks

(a)

(b)

Figure 9. Experiment results on CIFAR10/100. The network is
ResNet-56 (He et al., 2016). The unit of x-axis is the adversarial
robustness (AR) strength of NNs, c.f. the beginning of section 4.
(a) Plots of loss gap between training and test datasets v.s. AR
strength. (b) Plots of error rates on training and test datasets v.s.
AR strength.

(a) CIFAR10 Test

(b) CIFAR100 Test

(c) CIFAR10 Training

(d) CIFAR100 Training

Figure 10. Margin distributions of NNs with AR strength 4, 8, 16
on Training and Test sets of CIFAR10/100.

(a) Prob. Histogram

(b) Loss Histogram

(c) CIFAR10

(d) CIFAR100

Figure 11. (a)(b) are histograms of estimated probabilities and
losses respectively of the test set sample of NNs trained AR
strength 4, 8, 16. We plot a subplot of a narrower range inside
the plot of the full range to show the histograms of examples that
are around the middle values to show the change induced by AR
that induces more middle valued conﬁdence predictions. (c)(d) are
standard deviations of singular values of weight matrices of NNs
at each layer trained on CIFAR10/100 with AR strength 4, 16. The
AR strength 8 is dropped for clarity.

Towards Understanding the Regularization of Adversarial Robustness on Neural Networks

C. Proof of theorem 3.1

C.2. Proof

C.1. Algorithmic Robustness Framework

In order to characterize the bound to the GE, we build on the
algorithmic robustness framework (Xu & Mannor, 2012).

We introduce the framework below.

Deﬁnition 8 ((K, (cid:15)(·))-robust). An algorithm is (K, (cid:15)(·))
robust, for K ∈ N and (cid:15)(·) : Z m (cid:55)→ R, if Z can be parti-
tioned into K disjoint sets, denoted by C = {Ck}K
k=1, such
that the following holds for all si = (xi, yi) ∈ Sm, z =
(x, y) ∈ Z, Ck ∈ C:

∀si = (xi, yi) ∈ Ck, ∀z = (x, y) ∈ Ck
=⇒ |l(f (xi), yi) − l(f (x), y)| ≤ (cid:15)(Sm).

The gist of the deﬁnition is to constrain the variation of loss
values on test examples w.r.t. those of training ones through
local property of the algorithmically learned function f .
Intuitively, if s ∈ Sm and z ∈ Z are “close” (e.g., in the
same partition Ck), their loss should also be close, due to
the intrinsic constraint imposed by f .

For any algorithm that is robust, Xu & Mannor (Xu & Man-
nor, 2012) proves

Theorem C.1 (Xu & Mannor (Xu & Mannor, 2012)). If
a learning algorithm is (K, (cid:15)(·))-robust and L is bounded,
a.k.a. L(f (x), y) ≤ M ∀z ∈ Z, for any η > 0, with
probability at least 1 − η we have

GE(fSm ) ≤ (cid:15)(Sm) + M

(cid:114)

2K log(2) + 2 log(1/η)
m

. (6)

To control the ﬁrst term, an approach is to constrain the varia-
tion of the loss function. Covering number (Shalev-Shwartz
& Ben-David, (Shalev-Shwartz & Ben-David, 2014), Chap-
ter 27) provides a way to characterize the variation of the
loss function, and conceptually realizes the actual number
K of disjoint partitions.

For any regular k-dimensional manifold embedded in space
equipped with a metric ρ, e.g., the image data embedded in
L2(R2), the square integrable function space deﬁned on R2,
it has a covering number N (X ; ρ, (cid:15)) of (CX /(cid:15))k (Verma,
2013), where CX is a constant that captures its “intrinsic”
properties, and (cid:15) is the radius of the covering ball. When
we calculate the GE bound of NNs, we would assume the
data space is a k-dimensional regular manifold that accepts
a covering.

Adversarial robustness makes NNs a (K, (cid:15)(·))-robust algo-
rithm, and is able to control the variation of loss values
on test examples. Building on covering number and theo-
rem C.1, we are able to prove theorem 3.1.

Proof of lemma 3.1 . By theorem 3 in Sokolic et al. (2017),
we have

||Il(x) − Il(x(cid:48))|| =

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:90) 1

0

J (x − t(x(cid:48) − x))dt(x − x(cid:48))

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(7)

where J (x) denotes the Jacobian of Il(x) at x.

By lemma 3.2 in Jia et al. (2019), when we only have max
pooling layers and ReLU as nonlinear layer in NNs, J (x) is
a linear operator at a local region around x. For terminology
concerning regions, we follow the deﬁnitions in Jia et al.
(2019). More speciﬁcally, we have

J (x) =

l
(cid:89)

i=1

W x
i

where W x
i is the linear mapping (matrix) induced by J (x)
at x. It is a matrix obtained by selectively setting certain
rows of W i to zero. For the more concrete form of W x
i ,
refer to lemma 3.2 in Jia et al. (2019). In Jia et al. (2019), it
is noted as W q

i , where q is a region where x is in.

Suppose that from x to x(cid:48), the line segment x − x(cid:48) passes
through regions {qj}j=1,...,n. The line segment is illustrated
in ﬁg. 2b as the boldest black line segment at the upper half
of the ﬁgure. In the illustration, x − x(cid:48) passes through
three regions, colored coded as gray, dark yellow, light blue
respectively. The line segment is divided into three sub-
segments. Suppose l(t) = x + t(x(cid:48) − x). Then the three
sub-segments can be represented by l(t) as l(s1) to l(e1),
l(s2) to l(e2), and l(s3) to l(e3) respectively, as noted on
the line segment in the illustration. Originally, the range
of the integration in eq. (7) is from 0 to 1, representing the
integration on the line segment l(0) to l(1) in the instance
space. Now, since for each of these regions trespassed by
the line segment, the Jacobian J (x) is a linear operator,
denoted as W qj
i , the integration in eq. (7) from 0 to 1 can
be decomposed as a summation of integration on segments
l(s1) to l(e1) etc. In each of these integration, the Jaco-
bian J (x) is the multiplication of linear matrices W qj
i , i.e.,
(cid:81)l
i . Thus, eq. (7) can be written as

i=1 W qj

n
(cid:88)

(cid:90) ej

j=1

sj

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

l
(cid:89)

i=1

W qj

(cid:12)
(cid:12)
i dt(x − x(cid:48))
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

where sj, ej denotes the start and end of the segment
[sj, ej] ⊂ [0, 1] of the segment [0, 1] that passes through
the region qj.

In the cases that a linear operator is applied on the feature
map Il(x) without any activation function, we can also

Towards Understanding the Regularization of Adversarial Robustness on Neural Networks

obtain a similar conclusion. Actually, such cases are just
degenerated cases of feature maps that have activation func-
tions.
Corollary C.1. Given two elements x, x(cid:48), and Il(x) =
W lg(W l−1 . . . g(W 1x)), we have

||Il(x) − Il(x(cid:48))|| =

n
(cid:88)

(cid:90) ej

j=1

sj

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

W l

l−1
(cid:89)

i=1

W qj

i dt(x − x(cid:48))

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

where symbols are deﬁned similar as in Proof of lemma 3.1.

Now, we are ready to prove theorem C.1.

X /(cid:15)k, denoting K = C k

Proof of theorem C.1. Similar with the proof of theo-
rem C.1, we partition space Z into the (cid:15)-cover of Z, which
by assumption is a k-dimension manifold. Its covering num-
ber is upper bounded by C k
X /(cid:15)k,
and ˆCi the ith covering ball. For how the covering ball
is obtained from the (cid:15)-cover, refer to theorem 6 in Xu &
Mannor (2012). We study the constraint/regularization that
adversarial robustness imposes on the variation of the loss
function. Since we only have (cid:15)-adversarial robustness, the
radius of the covering balls is at most (cid:15) — this is why we
use the same symbol. Beyond (cid:15), adversarial robustness does
not give information on the possible variation anymore. Let
T (cid:48) denotes the NN without the last layer.

First, we analyze the risk change in a covering ball Ci. The
analysis is divided into two cases: 1 all training samples
in Ci are classiﬁed correctly; 2) all training samples in Ci
are classiﬁed wrong. Note that no other cases exist, for
that the radius of Ci is restricted to be (cid:15), and we work on
(cid:15)-adversarial robust classiﬁers. It guarantees that all samples
in a ball are classiﬁed as the same class. Thus, either all
training samples are all classiﬁed correctly, or wrongly.

satisfy the above inequality. The observation leads to the
constraint on the loss difference (cid:15)(·) deﬁned earlier in deﬁ-
nition 8 in the following.

Given any training example z := (x, y) ∈ Ci, and any
element z(cid:48) := (x(cid:48), y(cid:48)) ∈ Ci, where Ci is the covering ball
that covers x, we have

|lγ(x, y) − lγ(x(cid:48), y(cid:48))|

=| max{0, 1 −

≤ max{0, 1 −

} − max{0, 1 −

u(x(cid:48))
γ

}|

u(x)
γ
umin
γ

}.

(8)

Now we relate the margin to the margin in the instance
space.

Given z := (x, y) ∈ Z, and z(cid:48), of which x(cid:48) is the closest
points to x (measured in Euclidean norm) on the decision
boundary, we can derive the inequality below.

u(x) = u(x) − u(x(cid:48))
(cid:90) 1

=

J (x − t(x − x(cid:48)))dt(x − x(cid:48))

(9)

(10)

0
(cid:90) 1

0

(cid:90) 1

0

=

=

(wy − w ˆy)T

L−1
(cid:89)

i=1

W x−t(x−x(cid:48))

i

dt(x − x(cid:48))

(cid:12)
(cid:12)
(wy − w ˆy)T
(cid:12)
(cid:12)
(cid:12)

L−1
(cid:89)

i=1

W x−t(x−x(cid:48))

i

(cid:12)
(cid:12)
dt(x − x(cid:48))
(cid:12)
(cid:12)
(cid:12)
(11)

=

n
(cid:88)

(cid:90) ej

j=1

sj

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(wy − w ˆy)T

L−1
(cid:89)

i=1

W qj

(cid:12)
(cid:12)
i dt(x − x(cid:48))
(cid:12)
(cid:12)
(cid:12)

(12)

(cid:90) 1

dt

0
(13)

L−1
(cid:89)

i=1

L−1
(cid:89)

i=1

min||x − x(cid:48)||2
σi

min||x − x(cid:48)||2
σi

We ﬁrst study case 1). Given any example z = (x, y) ∈ Ci,
let ˆy = arg maxi(cid:54)=y wT

i T (cid:48)x. Its ramp loss is

≥ min

y,ˆy∈Y,y(cid:54)=ˆy

||wy − w ˆy||2

lγ(x, y) = max{0, 1 −

1
γ

(wy − w ˆy)T T (cid:48)x}.

Note that within Ci, (wy − w ˆy)T T (cid:48)x ≥ 0, thus lγ(x, y) is
mostly 1, and we would not reach the region where r > 0
in deﬁnition 4. Let u(x) := (wy − w ˆy)T T (cid:48)x, and ui
min =
min∀x∈Ci u(x). We have

lγ(x, y) ≤ max{0, 1 −

ui
min
γ

} ≤ max{0, 1 −

umin
γ

},

where umin denotes the smallest margin among all parti-
tions.

The inequality above shows adversarial robustness requires
that T (cid:48)x should vary slowly enough, so that in the worst
case, the loss variation within the adversarial radius should

≥ min

y,ˆy∈Y,y(cid:54)=ˆy

||wy − w ˆy||2

where J (x) denotes the Jacobian of Il(x) at x. eq. (10) can
be reached by theorem 3 in Sokolic et al. (2017). eq. (11)
can be reached because (wy − w ˆy)W x−t(x−x(cid:48))
(x − x(cid:48))
is the actually classiﬁcation score u(x), u(x(cid:48)) difference
between x, x(cid:48), and by assumptions assumption 3.1, they are
positive throughout. eq. (12) is reached due to corollary C.1
— in this case, the matrix W l in corollary C.1 is of rank one.

i

To arrive from eq. (12) to eq. (13), we observe that x(cid:48) is
the closest point to x on the decision boundary. Being the
closest means x − x(cid:48) ⊥ N ((wy − w ˆy)T (cid:48)). If the difference
x(cid:48) − x satisﬁes x − x(cid:48) (cid:54)⊥ N (T (cid:48)), we can always remove

Towards Understanding the Regularization of Adversarial Robustness on Neural Networks

the part in the N (T (cid:48)), which would identify a point that is
closer to x, but still on the decision boundary, which would
be a contradiction. Then if x − x(cid:48) is orthogonal to the
null space, we can bound the norm using the least singular
values. We develop the informal reasoning above formally
in the following.

Similarly in lemma 3.4 in Jia et al. (2019), by Cauchy inter-
lacing law by row deletion, assuming x ⊥ N ((cid:81)L−1
i=1 W qj
i )
(N denotes the null space; the math statement means x is
orthogonal to the null space of J (x)), we have

||

L−1
(cid:89)

i=1

W qj

i x||2 ≥

L−1
(cid:89)

i=1

σi
min||x||2

(14)

where σi
min is the smallest singular value of W i. Then
conclusion holds as well for multiplication of matrices
(cid:81)L−1
i , since the multiplication of matrices are also a

i=1 W qj

matrix.

parameterized on vmin, as follows

max{0,1 −

umin
γ

} ≤ max{0,

1 −

miny,ˆy∈Y,y(cid:54)=ˆy ||wy − w ˆy||2

γ

(cid:81)L−1

i=1 σi

minvmin

}.

Notice that only because (cid:15)0-adversarial robustness, we can
guarantee that vmin is non-zero, thus the bound is inﬂuenced
by AR.

Then, we study case 2), in which all training samples z ∈ Ci
are classiﬁed wrong. In this case, for all z ∈ Ci, the ˆy given
by ˆy = arg maxi(cid:54)=y wT
i T (cid:48)x in the margin operator is the
same, for that ˆy is the wrongly classiﬁed class. Its ramp loss
is

Notice that in each integral in eq. (12), we are integrating
over constant. Thus, we have it equates to

lγ(x, y) = max{0, 1 −

1
γ

(wy − w ˆy)T T (cid:48)x}.

n
(cid:88)

j=1

(ej − sj)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(wy − w ˆy)T

L−1
(cid:89)

i=1

W qj

i (x − x(cid:48))

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

i=1 W qj

in each operand, x − x(cid:48) ⊥
Now we show that
N ((wy − w ˆy)T (cid:81)L−1
i ). Denote Tqj as N ((wy −
w ˆy)T (cid:81)L−1
i=1 W qj
i ). Suppose that it does not hold. Then
we can decompose x − x(cid:48) into two components ∆1, ∆2,
where ∆1 ⊥ Tqj , ∆2 (cid:54)⊥ Tqj . We can ﬁnd a new point
x(cid:48)(cid:48) = x + ∆1 that is on the boundary. However, in this case

||x − x(cid:48)(cid:48)||2 = ||∆1||2 ≤ ||∆1||2 + ||∆2||2 = ||x − x(cid:48)||2

Recall that x(cid:48) is the closest point to x on the decision bound-
ary. This leads to a contradiction. Repeat this argument for
all j = 1, . . . , n, then we have x − x(cid:48) be orthogonal to all
N (Tqj ). Thus, by the inequality eq. (14) earlier, we can
arrive at eq. (13) — notice that wy − w ˆy is a matrix with
one column, thus also satisﬁes the above reasoning.

Through the above inequality, we can transfer the margin
to margin in the instance space. Let v(x) be the shortest
distance in || · ||2 norm from an element x ∈ X to the
decision boundary. For a covering ball Ci, let vi
min be
minx∈Ci v(x). Let vmin be the smallest vi
min among all
covering balls Ci that contain at least a training example.
We have that

umin ≥ min

y,ˆy∈Y,y(cid:54)=ˆy

||wy − w ˆy||2

L−1
(cid:89)

i=1

σi
minvmin

Consequently, we can obtain an upper bound of eq. (8)

Note that in the case 1), it is the y that stays ﬁxed, while ˆy
may differ from example to example; while in the case 2), it
is the ˆy stays ﬁxed, while y may differ.

Similarly, within Ci as required by adversarial robustness,
(wy − w ˆy)T T (cid:48)x ≤ 0, thus we always have 1 − 1
γ (wy −
w ˆy)T T (cid:48)x ≥ 1, implying

lγ(x, y) = 1.

Thus, ∀z = (x, y), z(cid:48) = (x(cid:48), y(cid:48)) ∈ Ci

|lγ(x, y) − lγ(x(cid:48), y(cid:48))| = 0.

(15)

Since only these two cases are possible, by eq. (8) and
eq. (15), we have ∀z, z(cid:48) ∈ Ci

|lγ(x, y) − lγ(x(cid:48), y(cid:48))| ≤ max{0, 1 −

umin
γ

}.

(16)

The rest follows the standard proof in algorithmic robust
framework.

Let Ni be the set of index of points of examples that fall into
Ci. Note that (|Ni|)i=1...K is an IDD multimonial random

Towards Understanding the Regularization of Adversarial Robustness on Neural Networks

variable with parameters m and (|µ(Ci)|)i=1...K. Then

D. Implementation Details

We summarize the details of the experiments in this section.
The experiments are run with PyTorch (Pfeiffer, 2017).

|R(l ◦ T ) − Rm(l ◦ T )|

=|

≤|

K
(cid:88)

i=1

K
(cid:88)

i=1

EZ∼µ[l(T X, Y )]µ(Ci) −

1
m

m
(cid:88)

i=1

l(T xi, yi)|

EZ∼µ[l(T X, Y )]

|Ni|
m

−

1
m

m
(cid:88)

i=1

l(T xi, yi)|

EZ∼µ[l(T X, Y )]µ(Ci) −

K
(cid:88)

i=1

EZ∼µ[l(T X, Y )]

|Ni|
m

|

K
(cid:88)

i=1

K
(cid:88)

+ |

≤|

1
m

(cid:88)

i=1

j∈Ni

max
z∈Ci

|l(T x, y) − l(T xj, yj)|

+ | max
z∈Z

|l(T x, y)|

K
(cid:88)

i=1

|

|Ni|
m

− µ(Ci)||.

Remember that z = (x, y).

(17)

(18)

(16) we have eq.
γ )}.

By eq.
than max{0, 2(1 − umin
Huber-Carol
(cid:113) log(2)2k+1Ck

X

+ 2 log(1/η)
m

.

γkm

inequality, eq. (18) is less or equal

(17)

is equal or
less
By Breteganolle-
to

The proof is ﬁnished.

Figure 12. The plot of accuracy on adversarial examples v.s. ad-
versarial defense strength built in NNs. The dotted line of which
the intersections are marked by stars are adversarial accuracy in
Madry et al. (2018) (CIFAR10), in Li et al. (2018) (Tiny ImageNet)
under similar adversarial attack strength.

D.1. Datasets

CIFAR10/100. Each CIFAR dataset consists of 50, 000
training data and 10, 000 test data. CIFAR-10 and CIFAR-
100 have 10 and 100 classes respectively. Our data aug-
mentation follows the standard manner in Lee et al. (2015):
during training, we zero-pad 4 pixels along each image side,
and sample a 32 × 32 region cropped from the padded im-
age or its horizontal ﬂip; during testing, we use the original
non-padded image.

Tiny-ImageNet. Tiny-ImageNet is a subset of ImageNet
dataset, which contains 200 classes rather than 1, 000
classes. Each class has 500 training images and 50 vali-
dation images. Images in the Tiny-ImageNet dataset are
of 64 × 64 pixels, as opposed to 256 × 256 in the full Im-
ageNet set. The data augmentation is straightforward: an
input image is 56 × 56 randomly cropped from a resized
image using the scale, aspect ratio augmentation as well as
scale jittering. A single 56 × 56 cropped image is used for
testing.

D.2. Experiments in section 4.1

CIFAR10/100 Models and Training. The models for CI-
FAR10/100 are the same as the ones in appendix B.3, ex-
cept that we do not use spectral normalization anymore.
CIFAR100 has 100 output neurons instead of 10.

Tiny-ImageNet Model. For Tiny ImageNet dataset, we use

Towards Understanding the Regularization of Adversarial Robustness on Neural Networks

Table 3. Raw data of CIFAR10 dataset for plots in ﬁg. 3, ﬁg. 7 and
ﬁg. 12.

Table 4. Raw data of CIFAR100 dataset for the plot in ﬁg. 3, ﬁg. 7
and ﬁg. 12.

Method

ResNet-56
+ Adv Trn

ResNet-110
+ Adv Trn

Defensive Strength
16
8
4

99.51
88.86
10.65
0.014
0.683
0.669
65.92

99.95
89.20
10.75
0.002
0.825
0.823
58.02

98.45
87.51
10.94
0.043
0.649
0.606
65.24

99.62
87.09
12.53
0.010
0.813
0.803
66.94

95.97
84.89
11.08
0.105
0.650
0.545
72.16

98.42
85.02
13.40
0.044
0.729
0.685
72.40

Trn Acc.
Test Acc.
∆Acc.
Trn Loss
Test Loss
∆Loss
PGD

Trn Acc.
Test Acc.
∆Acc.
Trn Loss
Test Loss
∆Loss
PGD

Method

ResNet-56
+ Adv Trn

ResNet-110
+ Adv Trn

Defensive Strength
16
8
4

88.73
61.31
27.42
0.357
2.063
1.706
30.52

96.91
61.48
35.43
0.098
2.645
2.547
33.33

86.97
60.87
26.10
0.413
2.106
1.693
40.99

94.55
61.26
33.29
0.171
2.413
2.241
42.08

82.17
59.43
22.74
0.570
1.978
1.408
48.81

90.90
59.56
31.34
0.278
2.323
2.045
50.99

Trn Acc.
Test Acc.
∆Acc.
Trn Loss
Test Loss
∆Loss
PGD

Trn Acc.
Test Acc.
∆Acc.
Trn Loss
Test Loss
∆Loss
PGD

50-layered wide residual networks with 4 groups of resid-
ual layers and [3, 4, 6, 3] bottleneck residual units for each
group respectively. The 3×3 ﬁlter of the bottleneck residual
units have [64 × k, 128 × k, 256 × k, 512 × k] feature maps
with the widen factor k = 2 as mentioned in Zagoruyko &
Komodakis (2016). We replace the ﬁrst 7 × 7 convolution
layer with 3 × 3 ﬁlters with stride 1 and padding 1. The
max pooling layer after the ﬁrst convolutional layer is also
removed to ﬁt the 56 × 56 input size. Batch normalization
layers are retained for this dataset. The weights of convo-
lution layers for Tiny ImageNet are initialized with Xavier
uniform (Glorot & Bengio, 2010). Again, all dropout layers
are omitted.

Tiny-ImageNet Training. The experiments on the Tiny-
ImageNet dataset are based on a mini-batch size of 256
for 90 epochs. The initial learning rate is set to be 0.1
and decayed at 10 at 30 and 60 epochs respectively. All
experiments are trained on the training set with stochastic
gradient descent with the momentum of 0.9.

Results. The data for ﬁg. 3 and ﬁg. 7 are given in table 3,
table 4 and table 5. More speciﬁcally, the data on CIFAR10
are given in table 3. The result on CIFAR100 are given in
table 4. The result on Tiny-ImageNet are given in table 5.

Adversarial Robustness Attack Method. The adversarial
accuracy is evaluated against l∞-PGD (Madry et al., 2018)
untargeted attack adversary, which is one of the strongest
white-box attack methods. When considering adversarial
attack, they usually train and evaluate against the same per-
turbation. And for our tasks, we only use the moderate
adversaries that generated by 10 iterations with steps of size
2 and maximum of 8. When evaluating adversarial robust-
ness, we only consider clean examples classiﬁed correctly
originally, and calculate the accuracy of the adversarial ex-

Table 5. Raw data of Tiny-ImageNet dataset for the plot in ﬁg. 3,
ﬁg. 7 and ﬁg. 12.

Method

Defensive Strength

0

4

8

Wide ResNet
+ Adv Trn

Trn Acc.
Test Acc.
∆Acc.
Trn Loss
Test Loss
∆Loss
PGD

79.12
63.43
15.69
0.874
1.561
0.687
0.00

73.71
62.09
11.62
1.080
1.637
0.557
32.26

66.17
61.09
5.08
1.384
1.689
0.305
41.20

16

60.73
57.36
3.37
1.641
1.806
0.165
53.12

amples generated from them that are still correctly classiﬁed.
The adversarial accuracy is given in table 3 table 4 table 5,
the row named “PGD”, and plotted in ﬁg. 12.

D.3. Experiments in appendix B.3

Models. We use ResNet-type networks (Zhang et al., 2018).
Given that we need to isolate factors that inﬂuence spectral
complexity, we use ResNet without additional batch normal-
ization (BN) layers. To train ResNet without BN, we rely
on the ﬁxup initialization proposed in Zhang et al. (2018).
The scalar layers in Zhang et al. (2018) are also omitted,
since it changes spectral norms of layers. Dropout layers
are omitted as well. Following Sedghi et al. (2018), we clip
the spectral norm every epoch rather than every iteration.

Training. The experiments on CIFAR10 datasets are based
on a mini-batch size of 256 for 200 epochs. The learning
rate starts at 0.05, and is divided by 10 at 100 and 150
epochs respectively. All experiments are trained on training
set with stochastic gradient descent based on the momentum
of 0.9.

Towards Understanding the Regularization of Adversarial Robustness on Neural Networks

Table 6. Raw data for ﬁg. 6. SP denotes spectral norm.

Strength of
Spectral Normalization

Defensive Strength
16
8
4

SP 1

SP 3

SP 5

SP Uncontrolled

Trn Acc.
Test Acc.
∆Acc.
Trn Loss
Test Loss
∆Loss
PGD

Trn Acc.
Test Acc.
∆Acc.
Trn Loss
Test Loss
∆Loss
PGD

Trn Acc.
Test Acc.
∆Acc.
Trn Loss
Test Loss
∆Loss
PGD

Trn Acc.
Test Acc.
∆Acc.
Trn Loss
Test Loss
∆Loss
PGD

96.91
90.47
6.44
0.092
0.316
0.224
57.93

99.65
90.02
9.63
0.010
0.606
0.596
56.83

99.57
89.53
10.04
0.012
0.649
0.638
54.91

99.51
88.86
10.65
0.014
0.683
0.669
65.92

94.38
88.87
5.51
0.159
0.353
0.194
69.98

98.51
88.07
10.44
0.039
0.580
0.541
67.73

98.33
88.09
10.24
0.045
0.602
0.557
65.96

98.45
87.51
10.94
0.043
0.649
0.606
65.24

90.58
85.82
4.76
0.265
0.432
0.168
75.98

95.94
85.43
10.51
0.107
0.577
0.470
73.41

95.96
85.32
10.64
0.105
0.611
0.506
72.37

95.97
84.89
11.08
0.105
0.650
0.545
72.16

Results. The data for ﬁg. 6 are given in table 6.

