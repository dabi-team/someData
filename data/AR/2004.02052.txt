gDLS*: Generalized Pose-and-Scale Estimation Given Scale and Gravity Priors

Victor Fragoso
Microsoft
victor.fragoso@microsoft.com

Joseph DeGol
Microsoft
joseph.degol@microsoft.com

Gang Hua1
Wormpex AI
ganghua@gmail.com

0
2
0
2

r
p
A
5

]

V
C
.
s
c
[

1
v
2
5
0
2
0
.
4
0
0
2
:
v
i
X
r
a

Abstract

Many real-world applications in augmented reality (AR),
3D mapping, and robotics require both fast and accurate es-
timation of camera poses and scales from multiple images
captured by multiple cameras or a single moving camera.
Achieving high speed and maintaining high accuracy in a
pose-and-scale estimator are often conﬂicting goals. To si-
multaneously achieve both, we exploit a priori knowledge
about the solution space. We present gDLS*, a generalized-
camera-model pose-and-scale estimator that utilizes rota-
tion and scale priors. gDLS* allows an application to ﬂexi-
bly weigh the contribution of each prior, which is important
since priors often come from noisy sensors. Compared to
state-of-the-art generalized-pose-and-scale estimators (e.g.
gDLS), our experiments on both synthetic and real data
consistently demonstrate that gDLS* accelerates the esti-
mation process and improves scale and pose accuracy.

1. Introduction

Estimating the pose and scale from multiple images
taken from multiple cameras or multiple images from one
moving camera (e.g., a SLAM [6, 18, 19, 34, 46] trajectory)
is an essential step in many augmented reality (AR) [32, 39,
41, 49, 47], 3D mapping [3, 11, 31, 38, 42, 44], and robotics
applications [15, 20, 21, 30, 50, 54]. Consider hologram
sharing services (e.g., Azure Spatial Anchors [1]) as an
example. These services have a reference map and need
to localize query images accurately (so that holograms are
positioned correctly) and quickly (to maintain a nice user
experience). However, as Figure 1 shows, current meth-
ods leave room for improvement in terms of both accuracy
and processing time. In this work, we propose gDLS*, a
multi-camera pose-and-scale estimator that exploits scale
and gravity priors to improve accuracy and speed. Despite
using additional information, gDLS* computes its param-
eters with linear complexity in the number of points and
multiple optimal solutions in a single shot, avoiding itera-

1 This work was done while at Microsoft.
2 josephdegol.com/pages/GDLSStar_CVPR20.html
Published at CVPR 2020.

Figure 1. Estimating the pose and scale accurately and quickly is
essential in many applications in AR, 3D mapping, and robotics.
We introduce gDLS*, a pose-and-scale estimator that exploits
scale and/or gravity priors to improve accuracy and speed. Com-
pared to state-of-the-art estimators (e.g., gDLS+++ [44] and
gP+s [48]), gDLS* achieves more accurate estimates in less time
when registering a set of cameras to an existing 3D reconstruc-
tion. The right image above shows an existing 3D reconstruc-
tion (gray) with aligned cameras and points for gDLS* (green),
gDLS+++ (red), and gP+s (orange). The left image shows a
zoomed view of the positions of the aligned cameras. The white
points are the expected camera positions and the green/red/orange
points are the estimate positions for each method. The cyan line
between indicates a match, where longer lines indicate more error.

tive optimization procedures.

Using single camera pose estimators (e.g., [2, 7, 16, 23,
26, 29, 33, 53]) to develop a multi-camera pose-and-scale
estimator is cumbersome, and their estimates tend to be in-
accurate [22, 43].
Instead, many multi-camera pose-and-
scale estimators [22, 43, 44] use the generalized camera
model [12, 37] to elegantly treat the collection of cameras as
one generalized camera, yielding accuracy improvements.
Despite their improvements, these estimators often produce
erroneous results due to noisy input data and numerical in-
stabilities in their underlying polynomial solvers.

Given the need of accurate pose and scale estimates by
many applications in AR, 3D mapping, and robotics, some
algorithms [41, 42, 52] exploit inertial measurements (e.g.,
gravity directions). Most of these approaches assume that
the gravity or down directions are reliable, and include
this extra knowledge as part of their mathematical deriva-
tion to simplify the problem. However, the gravity direc-

1

 
 
 
 
 
 
tions can still be noisy due to the nature of these sensors
and can affect the accuracy of the estimates. In contrast,
gDLS* adopts a generalized-camera model with regular-
izers that encode scale and rotation priors (e.g., gravity di-
rection). These regularizers allow a user to independently
control the contribution of each individual prior, which is
beneﬁcial to reduce the effect of noise present in each prior.
We show using synthetic data that gDLS* is numer-
ically stable and resilient to noise. We demonstrate this
by (1) varying pixel noise and sample size and show-
ing that gDLS* estimates transformations with errors that
are no worse than current estimators; and (2) varying the
noise in the scale and gravity priors and showing that
gDLS* maintains accuracy and speed. We then use real
data (i.e. [10, 40]) to evaluate gDLS* when registering a set
of cameras to an existing 3D reconstruction. Our extensive
experiments show that gDLS* is signiﬁcantly faster and
slightly more accurate than current pose-and-scale estima-
tors (i.e., [22, 43, 44, 48]). Moreover, the experiments show
that a rotation prior based on gravity directions improves ro-
tation and translation estimates while achieving signiﬁcant
speed-ups. On the other hand, a scale prior mainly improves
scale estimates while modestly enhancing translation esti-
mates and speed.
In summary,

the contributions of this work are (1)
gDLS*, a novel and generalized formulation of gDLS [43]
that includes scale and gravity priors that computes its pa-
rameters with an O(n) complexity; (2) a novel evaluation
protocol for pose-and-scale estimators that reports rotation,
translation, and scale errors; and (3) extensive experimen-
tal results showing that gDLS* consistently improves pose
accuracy in less time.

2. Related Work

Estimating the position and orientation of a camera is
crucial for many applications because they need to accu-
rately register computer-generated content into the real-
world, localize an agent (e.g., a visually impaired person)
within an environment, and autonomously navigate (e.g.,
self-driving cars). While these applications use camera
pose estimators to operate, most of the estimators have fo-
cused on localizing single cameras. Although these esti-
mators [2, 8, 23, 25, 26, 29, 51, 53] have achieved im-
pressive performance and accuracy, many applications [21]
have started to adopt multi-camera systems. This is because
a multi-camera system can provide additional information
that allows an application to estimate its pose more accu-
rately. For this reason, this section reviews existing work
on multi-camera pose and pose-and-scale estimators.

2.1. Multi-Camera Pose Estimators

Chen and Chang [4] and Nister and Stewenius [35] pro-
posed gP3P, a minimal estimator which requires three 2D-

Table 1. gDLS* compares favorably to existing state-of-the-art
pose-and-scale estimators because it maintains all the properties of
other estimators while also being the only estimator that enables
the use of gravity and scale priors.

Reference

gP+s gDLS gDLS+++ UPnP Ours
[44]
[48]

[43]

[22]

-

2014
(cid:88)
(cid:88)

Year
Generalized Camera
Geometric Optimality
Linear Complexity
(cid:88)
Multiple Solutions
Similarity Transformation (cid:88)
(cid:88)
Singularity-Free Rotation
Gravity Prior
Scale Prior

2014
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

2016
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

2014
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)

2019
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

3D correspondences to estimate the pose of a multi-camera
system. gP3P computes up to eight solutions by ﬁnding the
intersections of a circle and a ruled quartic surface. Lee et
al. [14] also introduced a minimal estimator that utilizes
Pl¨ucker lines to estimate the depth of each point. Subse-
quently, it estimates the position of each point w.r.t. to the
frame of reference of the multi-camera system. Then it es-
timates the absolute pose of the multi-camera system.

Unlike previous minimal solvers, Kneip et al. [22] in-
troduced UPnP, an efﬁcient minimal and non-minimal pose
estimator derived from a least-squares reprojection-error-
based cost function. Inspired by DLS [16], UPnP reformu-
lates the cost function as one depending only on a unit-norm
quaternion. UPnP ﬁnds the optimal rotation by solving a
polynomial system that encodes the vanishing of the cost
gradient at the optimal unit-norm quaternion via a Gr¨obner-
basis solver [24, 28].

2.2. Pose-and-Scale Estimators

Different from multi-camera pose estimators, pose-and-
scale estimators compute the pose of a multi-camera system
and a scale value; this value scales the positions of the cam-
eras in order to align a 3D representation into the frame of
reference of the multi-camera system more accurately.

Ventura et al. [48] proposed gP+s, a minimal pose-and-
scale estimator that requires four 2D-3D correspondences
and a Gr¨obner basis polynomial solver [24, 28]. However,
gP+s can also work with more than four points. Kukelova et
al. [27] introduced another minimal pose-and-scale estima-
tor that avoids using a Gr¨obner basis polynomial solver,
leading to impressive speed-ups but a decrease in accuracy.
Unlike previous estimators, Sweeney et al. [43] pre-
sented gDLS, an estimator derived from a least-squares
reprojection-error cost function. gDLS derives a cost func-
tion that depends only on a rotation matrix.
Inspired
by DLS [16], gDLS solves a polynomial system that en-
codes the vanishing of the cost gradient at the optimal
Cayley-Gibbs-Rodrigues angle-axis vector using the DLS
polynomial solver (a Macaulay solver). Unfortunately, its

Macaulay solver can be slow since it requires obtaining the
eigenvectors of a 27 × 27 action matrix. To alleviate this
issue, Sweeney et al. [44] introduced gDLS+++, a gDLS-
based estimator using a unit-norm quaternion. Thanks to
the rotation representation of gDLS+++, it can use the efﬁ-
cient UPnP polynomial solver.

Different from previous methods, gDLS* is one of the
ﬁrst estimators to incorporate scale and rotation priors. As
we show in Section 4, these priors improve both speed and
accuracy. Moreover, gDLS* maintains many of the de-
sirable properties of current solvers:
(1) uses a general-
ized camera model which elegantly simpliﬁes the formu-
lation; (2) computes multiple optimal solutions in a sin-
gle shot, avoiding iterative optimization procedures; (3)
scales linearly when building its parameters; and (4) uses
a singularity-free rotation representation. See Table 1 for a
brief comparison of estimator properties.

3. Pose-and-Scale Estimation using Priors

The goal of gDLS*

is to provide hints about

the
scale and rotation parameters of the similarity transfor-
mation using a generalized pose-and-scale estimator (e.g.,
gDLS [43]). Thanks to the prevalence of inertial sensors
in mobile devices, these priors are readily available. For
instance, a rotation prior can be obtained from the gravity
direction using measurements from inertial sensors, and a
scale prior can be obtained from the IMU [36], GPS, or
known landmark sizes [5].

One of the design considerations of gDLS* is the abil-
ity to control the contribution of each of the priors inde-
pendently. This allows the user to either disable or enable
each of the priors. When enabling the priors, gDLS* al-
lows a user to set a weight for each prior to control their
conﬁdence. In Section 4, we test a range of weights, but
we plan to explore in future work how to set these weights
automatically using the variance of the noise of the sensors.
Because gDLS* is based on the pose-and-scale formula-
tions of gDLS [43, 44], we ﬁrst describe the pose-and-scale
formulation and then present our modiﬁcations that enable
the use of scale and rotation priors.

3.1. gDLS - A Pose-and-Scale Estimator Review

Given n 2D-3D correspondences, gDLS computes the
scale and pose of a non-central camera by minimizing the
following least-squares cost function:

J(R, t, s, α) =

n
(cid:88)

i=1

(cid:107)αiri − (Rpi + t − sci) (cid:107)2,

(1)

where ri is a unit-vector indicating the direction from the
position of the camera ci to a 3D point pi; αi is the depth
of the point pi with respect to the camera position ci; α is a
vector holding the depths; R ∈ SO(3) is a rotation matrix;

Figure 2. Estimating the pose of a multi-camera system Q requires
the estimation of R and t, while the scale s adjusts the camera po-
sitions ci so that W and Q use the same metric scale. gDLS* can
use the gravity directions g to impose a rotation prior and a scale-
prior s0 to place the cameras at the right scale.

t ∈ R3 is a translation vector; and s ∈ R is the scale; see
Fig. 2 for a visual representation of Eq. (15).

The pose-and-scale formulation shown in Eq. (15) accu-
mulates the errors between the transformed i-th 3D point
(Rpi + t − sci) and the same point described with respect
to the camera αiri. The rotation R, the translation t, and
sci transform a 3D point from a world coordinate system to
the coordinate system of a generalized camera.

To ﬁnd the minimizer (R(cid:63), t(cid:63), s(cid:63), α(cid:63)), gDLS [43] ﬁrst
rewrites J(R, t, s, α) as a function that only depends on
the rotation matrix. As Hesch and Roumeliotis [16] and
Sweeney et al. [43, 44] demonstrated, the translation t,
scale s, and depth αi can be written as a linear function
of the rotation matrix R. Thus, it is possible to re-write the
pose-and-scale least-squares cost formulation as follows:

J(R) =

n
(cid:88)

i=1

(cid:107)αi(R)ri − (Rpi + t(R) − s(R)ci) (cid:107)2

(2)

= vec(R)(cid:124)M vec(R),

where vec(R) is a vectorized form of the rotation matrix,
and M is a square matrix capturing the constraints from the
input 2D-3D correspondences; the dimensions of M depend
on the vectorization and representation of vec(R).

Given the cost function J(R), gDLS ﬁnds the optimal
rotation R(cid:63) by solving a polynomial system representing
the constraint that the gradient ∇qJ(R(cid:63)) = 0 is null with
respect to the rotation parameters q, and rotation-parameter
constraints (e.g., ensuring a unit-norm quaternion).

3.2. Incorporating Priors via Regularizers

In order to impose scale and rotation priors to Eq. (15),
gDLS* uses regularizers. Adding these regularizers leads
to the following least-squares cost function:

J (cid:48) = J(R, t, s, α) + λs (s0 − s)2 + λg(cid:107)gQ × RgW (cid:107)2,
(3)

where s0 is the scale prior; gQ and gW are the gravity di-
rections of the multi-camera setting and world, respectively;

the symbol × represents the cross-product operator; and λs
and λg are weights controlling the contribution of the scale
and rotation priors, respectively. These weights (i.e., λs and
λg) must be greater than or equal to zero.

The scale regularizer λs (s0 − s)2 imposes a penalty by
deviating from the scale prior s0. On the other hand, the
rotation prior λg(cid:107)gQ × RgW (cid:107)2 imposes a misalignment
penalty between the transformed world gravity direction
RgW and the query gravity direction gQ.

As discussed earlier, the ﬁrst step to solve for pose and
scale is to re-write the cost J (cid:48) as a function that only de-
pends on the rotation matrix. To do so, it is mathematically
convenient to deﬁne

x = (cid:2)α1

. . . αn

s

t(cid:124)(cid:3)(cid:124)

.

(4)

The gradient evaluated at the optimal x(cid:63) must satisfy the
following constraint: ∇xJ (cid:48)(cid:12)
(cid:12)x=x(cid:63) = 0. From this con-
straint, we obtain the following relationship:

x = (A(cid:124)A + P )−1 A(cid:124)W b + (A(cid:124)A + P )−1 P x0

relationships for depth, scale, and translation obtained by
Sweeney et al. for gDLS [43, 44] when λs = 0.

In order to re-write the regularized least-squares cost

function (i.e., Eq. (14)) as clearly as possible, we deﬁne

ei = αi(R)ri − (Rpi + t(R) − s(R)ci)

= ηi + ki
(cid:124)
ηi = u
i W bri − Rpi − V W b + SW bci
ki = λss0 (lir − lt + ln+1ci) .

(8)

The residual ei is divided into two terms: ηi, the resid-
ual part considering the unconstrained terms; and ki the
residual part considering the scale-prior-related terms. Note
again that when λs = 0, ki becomes null and ei becomes
the residual corresponding to gDLS [43, 44].

Using the deﬁnitions from Eq. (8), and the scale, depth,
and translation relationships shown in Eq. (32), we can now
re-write the regularized least-squares cost function shown
in Eq. (14) as follows:



 W b + λssol





U
S
V

=

where



r1

A =




. . .

rn



0n×n

P =



λs




 , b =











p1
...
pn

c1 −I
...
...
cn −I


 , W =

03×3



R




. . .




 ,

R

(5)

(6)

(cid:3)(cid:124)

n

s0

(cid:124)
0
3

and x0 = (cid:2)0(cid:124)
Inspired by gDLS [43] and
.
DLS [16], we partition (A(cid:124)A + P )−1 A(cid:124) into three matri-
ces U, S, and V such that the depth, scale, and translation
parameters are functions of U, S, and V, respectively. These
matrices and the vector l can be computed in closed form
by exploiting the sparse structure of the matrices A and P ;
see appendix for the full derivation.

Eq. (5) provides a linear relationship between the depth,
scale, and translation and the rotation matrix. Consequently,
these parameters are computed as a function of the rotation
matrix as follows:

(cid:124)
i W b + λssoli
αi(R) = u
s(R) = SW b + λssoln+1
t(R) = V W b + λssolt,

(7)

(cid:124)
i is the i-th row of matrix U , lj is the j-th entry
where u
of the vector l, and lt corresponds to the last three entries
of the vector l. Note that we can obtain the exact same

J (cid:48) = J (cid:48)

gDLS + J (cid:48)

s + J (cid:48)
g
= vec(R)(cid:124)M vec(R) + 2d(cid:124)vec(R) + k

(9)

where

J (cid:48)
gDLS =

n
(cid:88)

i=1

(cid:124)
e
i ei =

n
(cid:88)

i=1

η

(cid:124)
i ηi + 2k

(cid:124)
(cid:124)
i ki
i ηi + k

(cid:124)
gDLSvec(R) + kgDLS

= vec(R)(cid:124)MgDLSvec(R) + 2d
s = λs (s0 − S(R))2
J (cid:48)
= vec(R)(cid:124)Msvec(R) + 2d(cid:124)
s vec(R) + ks
g = λg(cid:107)gQ × RgW (cid:107)2 = vec(R)(cid:124)Mgvec(R)
J (cid:48)
M = MgDLS + Ms + Mg
d = dgDLS + ds
k = kgDLS + ks.

(10)

The parameters of Eq. (9) (i.e., MgDLS, Ms, Mg, dgDLS, ds,
kgDLS, and ks) can be computed in closed form and in O(n)
time; see appendix for the closed form solutions of these
parameters.

An important observation is that Eq. (9) generalizes
the unconstrained quadratic function of gDLS shown in
Eq. (15). When both priors are disabled, i.e., λg = λs = 0,
then J (cid:48)(R) = J(R). Also, note that the weights λg and λs
allow the user to control the contribution of each of the pri-
ors independently. This gives gDLS* great ﬂexibility since
it can be adapted to many scenarios. For instance, these
weights can be adjusted so that gDLS* reﬂects the conﬁ-
dence on certain priors, reduces the effect of noise present in
the priors, and fully disables one prior but enables another.

Figure 3. Rotation, translation, and scale errors as a function of (a) pixel noise when estimating pose and scale using a minimal sample of
correspondences, and (b) sample size. s-gDLS*, g-gDLS*, and sg-gDLS* produce comparable errors to that of gDLS [43, 44]. On the
other hand, gP+s [48] produces the highest errors.

3.3. Solving for Rotation

Given that the prior-based pose-and-scale cost function
(i.e., Eq. (14)) depends only on the rotation matrix, the next
step is to ﬁnd R such that it minimizes Eq. (9). To achieve
this, gDLS* represents the rotation matrix R using a quater-
nion q = (cid:2)q1 q2 q3 q4
. To compute all the minimizers of
Eq. (9), gDLS* follows [16, 22, 43, 44] and builds a poly-
nomial system that encodes the ﬁrst-order optimality condi-
tions and the unit-norm-quaternion constraint, i.e.,

(cid:3)(cid:124)

= 0,

(cid:40) ∂J (cid:48)
∀j = 1, . . . , 4
∂qj
qj (q(cid:124)q − 1) = 0, ∀j = 1, . . . , 4

.

(11)

The polynomial system shown in Eq. (11) encodes the unit-
norm-quaternion constraint with
∂ (q(cid:124)q − 1)2
∂qj

= qj (q(cid:124)q − 1) = 0, ∀j.

(12)

Eq. (12) yields efﬁcient elimination templates and small ac-
tion matrices, which delivers efﬁcient polynomial solvers as
Kneip et al. [22] shows. In fact, gDLS* adopts the efﬁcient
polynomial solver of Kneip et al. [22] as we leverage their
rotation representation
vec(R) = (cid:2)q2
3 q2
2 q2

.
(13)
Given this representation, the dimensions of the parame-
ters of the regularized least-squares cost function shown in
Eq. (9) become M ∈ R10×10, d ∈ R10, and k ∈ R.

4 q1q2 q1q3 q1q4 q2q3 q2q4 q3q4

1 q2

(cid:3)(cid:124)

Because gDLS* uses the solver of Kneip et al. [22], it
efﬁciently computes eight rotations. After computing these
solutions, gDLS* discards quaternions with complex num-
bers, and then recovers the depth, scale, and translation us-
ing Eq. (32). Finally, gDLS* uses the computed similarity
transformations to discard solutions that map the input 3D
points behind the camera.

Our gDLS* derivation can be generalized.

Impos-
ing scale and translation priors via the regularizers is gen-
eral enough to be adopted by least-squares-based estimators
(e.g., DLS [16] and UPnP [22]). This is because the regu-
larizers are quadratic functions that can be added without
much effort into their derivations.

Figure 4. (a) gDLS*’s accuracy slowly degrades as the noise in-
creases in the scale prior. (b) gDLS*’s accuracy is barely affected
by noise in the gravity prior. (c) Time is not affected when using
noisy scale (left) and gravity (right) priors. The orange line shows
the best timing of gDLS+++ [44], the second fastest estimator in
our experiments.

4. Experiments

This section presents experiments that use (i) synthetic
data to demonstrate the numerical stability and robustness
of gDLS* and (ii) real data to show the performance of
gDLS* in registering a SLAM trajectory to a pre-computed
point cloud. We test three gDLS* conﬁgurations: scale-
only-regularized (s-gDLS*), gravity-only-regularized (g-
gDLS*), and scale-gravity-regularized (sg-gDLS*). For all
experiments except the ablation study, λs and λg are ﬁxed
to 1. We compare to several state-of-the-art pose-and-scale
estimators: gP+s [48], gDLS [43], gDLS+++ [44], and
UPnP [22]. All implementations are integrated into Theia-
SfM [45]. For all experiments, we use one machine with
two 2.10 GHz Intel Xeon CPUs and 32 GB of RAM.

Datasets. For the SLAM trajectory registration, the ex-
periments use two publicly available SLAM datasets: the
TUM RGBD dataset [40] and the KITTI dataset [10]. These

0510Pixel Noise Std. Dev.10-1100101Rotation Error [degrees]0510Pixel Noise Std. Dev.10-1100101Translation Error [m]0510Pixel Noise Std. Dev.10-310-210-1Scale ErrorgDLSgDLS+++gP+ss-gDLS*g-gDLS*sg-gDLS*05001000Number of Correspondences10-210-1100Rotation Error [degrees]05001000Number of Correspondences10-210-1100Translation Error [m]05001000Number of Correspondences10-410-310-2Scale ErrorgDLSgDLS+++gP+ss-gDLS*g-gDLS*sg-gDLS*(a) Robustness to pixel noise with minimal samples(b) Accuracy as a function of sample size(a) Noisy Scale Priors(b) Noisy Gravity PriorsRotation ErrorTranslation ErrorScale ErrorRotation ErrorTranslation ErrorScale Error(c) Timings Given Noisy PriorsTable 2. Estimation times in seconds for gP+s [48], gDLS [43], gDLS+++ [44], UPnP [22], s-gDLS* (s column), g-gDLS* (g column), and
sg-gDLS* (sg column) for rigid (s0 = 1) and similarity (s0 = 2.5) transformations. The top six rows show results for the TUM dataset,
and the last six rows show results for the KITTI dataset. A gravity prior tends to deliver fast estimates, while a scale prior modestly slows
down the estimation. On the other hand, scale and gravity priors tend to be modestly faster than gDLS+++.

Rigid Transformation [s0 = 1]

Similarity Transformation [s0 = 2.5]

[48]

[43]

[44]

Fr1 Desk
Fr1 Room
Fr2 LargeNoLoop
Fr1 Desk2
Fr2 Pioneer SLAM
Fr2 Pioneer SLAM 2
Drive 1 (cid:0)10−2 [sec](cid:1)
Drive 9
Drive 19
Drive 22
Drive 23 (cid:0)10−2 [sec](cid:1)
Drive 29

10.77
8.86
5.63
4.99
21.06
3.39

1.66
0.32
0.51
0.12
3.40
1.15

15.38
12.23
8.10
7.72
27.77
3.49

2.32
0.49
0.29
0.22
4.72
1.95

5.79
4.51
2.76
2.96
10.82
1.93

0.93
0.23
0.80
0.12
2.14
0.76

[22]

9.60
5.52
2.23
4.20
8.27
0.88

1.17
0.14
0.27
0.07
2.18
1.19

s

8.11
6.66
2.52
3.89
12.08
1.17

0.90
0.34
0.29
0.12
2.19
1.13

g

3.39
2.85
2.01
1.67
6.48
1.02

0.58
0.23
0.26
0.06
1.45
0.77

sg

5.80
4.70
2.10
3.65
8.08
1.40

0.90
0.28
0.29
0.11
2.04
1.11

[48]

[43]

10.54
8.55
6.46
5.04
17.10
3.16

1.31
0.35
0.57
0.12
3.25
1.16

15.38
11.98
8.25
7.47
25.42
3.66

1.79
0.48
0.90
0.19
4.71
1.96

[44]

5.79
4.61
2.82
2.59
9.34
1.96

1.19
0.22
0.39
0.07
2.07
0.75

s

8.12
7.96
2.49
3.99
11.84
1.17

1.0
0.50
0.29
0.11
2.34
1.14

g

3.82
3.24
2.40
1.86
7.41
1.09

0.61
0.22
0.26
0.06
1.55
0.74

sg

6.89
5.40
2.39
3.38
9.70
0.99

0.87
0.26
0.29
0.12
1.94
1.12

datasets provide per-frame accelerometer estimates, which
we use to compute one gravity direction for each SLAM
trajectory. Speciﬁcally, we low pass ﬁlter and smooth the
accelerations (because the gravity acceleration is constant
within the high frequency noise) to get an estimate of the
gravity vector for each image. Then, we take the mean of
all these estimates to get a ﬁnal gravity vector estimate. The
ﬁnal result is one gravity vector for each trajectory.

Error Metrics. All the experiments report rotation,
translation, and scale errors. The rotation error is the an-
gular distance [13, 17] between the expected and the esti-
mated rotation matrix. The translation error is the L2 norm
between the expected and the estimated translation. Lastly,
the scale error is the absolute difference between the ex-
pected and the estimated scale values.

4.1. Robustness to Noisy Synthetic Data

This experiment consists of three parts:

(1) measur-
ing robustness to pixel noise with minimal samples (i.e.,
four 2D-3D correspondences); (2) measuring accuracy as
a function of the size of a non-minimal sample (i.e., more
than four 2D-3D correspondences); and (3) testing how
noise in scale and gravity priors effects solution accuracy
and run time. For all experiments, we execute 1, 000 tri-
als using 10 randomly positioned cameras within the cube
[−10, 10] × [−10, 10] × [−10, 10], and 300 random 3D
points in the cube [−5, 5]×[−5, 5]×[10, 20]. For each trial,
we transform the 3D points by the inverse of a randomly
generated ground truth similarity transformation (i.e., a ran-
dom unit vector direction and random rotation angle be-
tween 0◦ and 360◦, random translation between 0 and 5 in
(x, y, z), and random scale between 0 and 5).

gDLS* is robust to pixel noise with minimal sam-
ples. We generate random minimal samples with zero-mean
Gaussian noise added to the pixel positions. We vary the
noise standard deviation between 0 and 10 and measure

Figure 5. Evaluation Protocol:
(1) Reconstruct a scene using
Theia-SfM; (2) Split the reconstruction into Query (Q) and Refer-
ence (W) parts; (3) Transform Q using a similarity transform S;
and (4) Estimate similarity transform S(cid:63) aligning Q and W.

the rotation, translation, and scale errors. The results of
this experiment can be seen in Fig. 3(a). We observe that
s-gDLS*, g-gDLS*, and sg-gDLS* perform similarly to
gDLS [43] and gDLS+++ [44] when comparing their rota-
tion and translation errors. Also, we see that the rotation
and translation errors produced by gP+s [48] are the high-
est. All methods produce similar scale errors.

Accuracy of gDLS* improves with non-minimal sam-
ples. For this experiment, we vary the size of the sam-
ple from 5 to 1000 2D-3D correspondences and ﬁx the
standard deviation to 0.5 for the zero-mean Gaussian pixel
noise. Fig. 3(b) shows that s-gDLS*, g-gDLS*, and sg-
gDLS* produce comparable rotation, translation, and scale
errors to that of gDLS and gDLS+++. On the other hand,
gP+s produced the largest errors.

gDLS*

is numerically stable.

From Fig. 3, we
sg-
conclude that s-gDLS* (green), g-gDLS* (red),
gDLS* (cyan) are numerically stable because the errors are
similar to that of gDLS+++.

gDLS* is robust to noise in scale and gravity priors.
For this experiment, we gradually increase the noise in the
scale and gravity priors. In Fig. 4(a), we see that noise in
the scale prior slowly increases the rotation, translation, and

Table 3. Rotation, translation, and scale errors of gP+s [48], gDLS [43], gDLS+++ [44], UPnP [22], and gDLS* using a unit scale (i.e.,
s0 = 1) and gravity priors. The ﬁrst six rows show results for the TUM dataset, and the last six rows show results for the KITTI dataset.
The smallest errors are shown in bold. We observe that gDLS* and UPnP perform equivalently when comparing rotation and translation
errors. However, gDLS* produces the lowest errors among the pose-and-scale estimators (i.e., gP+s, gDLS, and gDLS+++).

Rerror [deg] (10−1)

[48]

2.78
2.05
1.90
2.34
1.50
1.63

0.44
1.15
3.42
0.66
0.74
1.00

[43]

2.58
1.95
1.61
2.13
1.51
1.49

0.40
1.10
3.57
0.62
0.58
1.06

[44]

2.72
1.99
1.62
2.26
1.47
1.51

0.42
1.15
3.30
0.66
0.62
1.06

[22]

1.89
1.05
1.35
1.63
0.76
0.97

0.34
0.64
2.48
0.31
0.84
0.56

Ours

1.57
0.98
1.32
1.25
0.87
1.08

0.33
1.13
3.04
0.63
0.56
0.82

Fr1 Desk
Fr1 Room
Fr2 LargeNoLoop
Fr1 Desk2
Fr2 Pioneer SLAM
Fr2 Pioneer SLAM 2

Drive 1
Drive 9
Drive 19
Drive 22
Drive 23
Drive 29

Rigid Transformation [s0 = 1]
terror (10−2)

[48]

2.02
1.09
6.77
2.03
1.29
1.93

0.27
0.43
0.83
0.28
0.19
0.34

[43]

1.92
1.05
5.77
1.85
1.18
1.77

0.25
0.40
0.85
0.27
0.18
0.35

[44]

1.91
1.09
6.01
1.90
1.19
1.80

0.24
0.44
0.80
0.30
0.19
0.35

[22]

1.36
0.55
4.70
1.38
0.59
1.22

0.17
0.13
0.63
0.16
0.12
0.21

Ours

1.19
0.52
4.45
1.06
0.70
1.34

0.16
0.20
0.73
0.27
0.09
0.26

serror (10−3)

[48]

1.46
1.48
4.04
1.35
2.77
6.81

0.72
6.27
4.99
1.90
1.28
1.60

[43]

1.33
1.40
4.15
1.15
2.73
7.48

0.66
5.79
5.64
1.70
1.16
3.39

[44]

1.29
1.39
4.39
1.26
2.67
7.44

0.61
6.14
5.54
1.67
1.28
1.73

Ours

0.68
0.32
1.39
0.49
0.87
0.88

0.02
0.05
0.01
0.94
0.03
0.75

Table 4. Rotation, translation, and scale errors of gP+s [48], gDLS [43], gDLS+++ [44], and gDLS* using a scale prior of s0 = 2.5 and
gravity priors. The ﬁrst six rows show results for the TUM dataset, and the last six rows show results for the KITTI dataset. The smallest
errors are shown in bold. We see that gDLS* produces the smallest errors in almost every case.

Rerror [deg] (10−1)

Similarity Transformation [s0 = 2.5]
terror (10−2)

serror

(cid:0)10−3(cid:1)

Fr1 Desk
Fr1 Room
Fr2 LargeNoLoop
Fr1 Desk2
Fr2 Pioneer SLAM
Fr2 Pioneer SLAM 2

Drive 1
Drive 9
Drive 19
Drive 22
Drive 23
Drive 29

[48]

2.77
2.02
1.90
2.29
1.43
1.84

0.43
1.17
3.60
0.64
0.75
1.00

[43]

2.59
1.99
1.57
2.13
1.48
1.49

0.40
1.09
3.27
0.62
0.59
1.09

[44]

2.72
1.99
1.62
2.26
1.47
1.51

0.42
1.15
3.57
0.66
0.62
1.06

Ours

2.23
1.29
1.49
1.77
1.17
1.27

0.33
1.13
3.09
0.62
0.57
0.82

[48]

5.50
2.79
13.9
4.37
3.67
5.23

0.47
1.11
2.13
0.77
0.58
0.80

[43]

5.21
2.78
12.3
4.14
3.44
4.49

0.44
1.06
1.91
0.75
0.51
0.89

[44]

5.22
2.79
12.8
4.32
3.50
4.58

0.43
1.16
2.06
0.82
0.56
0.88

Ours

4.55
1.71
10.1
3.28
2.42
3.69

0.28
0.50
1.80
0.75
0.24
0.65

[48]

3.59
3.74
10.7
3.22
6.98
15.8

1.73
15.7
14.6
4.40
3.13
3.81

[43]

3.27
3.41
10.8
2.88
6.81
18.7

1.62
14.4
14.0
4.27
2.87
4.46

[44]

3.23
3.48
11.0
3.14
6.68
18.6

1.54
15.3
14.1
4.19
3.20
4.33

Ours

1.56
0.91
3.66
1.37
1.65
2.42

0.05
0.12
0.04
2.34
0.14
1.86

scale errors. Conversely, in Fig. 4(b), noise in the gravity
prior has little effect on the ﬁnal accuracy. Lastly, Fig. 4(c)
shows that noise has a minimal effect on the solution time.

4.2. SLAM Trajectory Registration

The goal of this experiment is to measure the accuracy
of an estimated similarity transformation which registers a
SLAM trajectory (a collection of images from a moving
camera) to a pre-computed 3D reconstruction. This exper-
iment uses both scale and gravity priors for gDLS*. Part
of this experiment considers a unit-scale similarity transfor-
mation, which makes it equivalent to a rigid transformation.
In the latter case, the experiment also includes UPnP [22],
a state-of-the-art multi-camera pose estimator that only es-
timates a rigid transformation (i.e., no scale estimation).

For each dataset and method combination, we run 100
trials. Each estimator is wrapped in RANSAC [9] to esti-

mate the transformations and the same parameters are used
for all of the scale-and-pose experiments. RANSAC labels
correspondences with more than 4 pixels of reprojection er-
ror as outliers. Because we use RANSAC, all methods tend
to converge to accurate solutions (Tables 3 and 4); however,
the speed of convergence can differ signiﬁcantly (Table 2).

While there exist datasets and clear methods to evalu-
ate visual-based localization or SfM reconstructions (e.g.,
[5, 38]), there is not a well established methodology to eval-
uate pose-and-scale estimators. Previous evaluation proce-
dures (e.g., [43, 44]) mostly show camera position errors,
but discard orientation and scale errors. To evaluate the reg-
istration of a SLAM trajectory, we propose a novel evalu-
ation procedure as illustrated in Fig. 5: (1) reconstruct the
trajectory using Theia-SfM; (2) remove a subset of images
with their corresponding 3D points and tracks to create a
new query set (the remaining images, points, and tracks are

Figure 6. Average rotation, translation, and scale errors of the best baseline (best performing baseline for a given metric) and gDLS* as a
function of λs and λg on (a) KITTI and (b) TUM datasets. A gravity prior (g-gDLS*) tends to reduce rotation and translation errors and
modestly improves scale errors. A scale prior (s-gDLS*) tends to improve scale accuracy and modestly reduces translation errors. The
combination of scale and gravity priors (sg-gDLS*) tends to reduce translation and scale errors and improves rotation estimates (see (b)).

the reference reconstruction); (3) apply a similarity trans-
formation to describe the reconstruction in a different frame
of reference with a different scale; and (4) estimate the sim-
ilarity transformation. To compute the input 2D-3D corre-
spondences, the evaluation procedure matches the features
from the query images to the features of the reference re-
construction and geometrically veriﬁes them. From these
matches and reconstruction, the procedure builds the 2D-
3D correspondences by ﬁrst computing the rays pointing to
the corresponding 3D points using the camera positions.

The gravity prior signiﬁcantly improves speed. Ta-
ble 2 shows the average estimation times for both rigid
(s0 = 1) and similarity (s0 = 2.5) transformations. We
observe that both priors help the estimators ﬁnd the solu-
tion much faster than many baselines (see sg columns). In
particular, a gravity only prior (see g columns) can speed
up gDLS* signiﬁcantly while producing good estimates
(see Sec. 4.3). On the other hand, a scale only prior (see
s columns) can modestly accelerate gDLS*.

Incorporating scale and rotation priors consistently
improves accuracy. Tables 3 and 4 present the average ro-
tation, translation, and scale errors of 100 trials, each es-
timating rigid and similarity transformations, respectively.
Both Tables show six TUM trajectories at the top and six
KITTI trajectories at the bottom. The scale priors s0 are
shown at the top of both Tables. Note that UPnP does not
estimate scales, so it is not included in similarity transfor-
mation sections. Table 3 shows that gDLS* and UPnP pro-
duce the most accurate rotation and translation estimates,
and that gDLS* produces the most accurate scale estimates.
Table 4 shows that gDLS* tends to produce the most accu-
rate rotation and translation estimates, and that gDLS* pro-
duces the most accurate scale estimates.

4.3. Ablation Study

This study aims to show the impact on the estimator ac-
curacy of the weights λs and λg as they vary. We use the
same TUM and KITTI datasets and RANSAC conﬁguration
as in previous experiments. We vary the weights from 0.25
to 3 using increments of 0.25 and run 100 trials for each
weight. To summarize the results, we average the rotation,

translation, and scale errors.

The priors improve accuracy and speed when used
individually or together. Fig. 6 shows the results of this
study. We see that on average a gravity prior (g-gDLS*)
signiﬁcantly improves rotation and translation errors, while
modestly improving scale errors. On the other hand, a scale
prior (s-gDLS*) on average signiﬁcantly improves the scale
errors, while modestly improving translation errors. Finally,
both gravity and scale priors improve translation and scale
errors and can help the estimator improve rotation errors.

From these results, we can conclude that accurate priors
can greatly improve accuracy estimates (thereby also im-
proving speed). However, we know from Fig. 4 that noisy
priors can also degrade accuracy. Thus, for future work, we
will explore how to automatically set λs and λg based on
the noise of the priors to maximize accuracy and speed.

5. Conclusion

This work presents gDLS*, a novel pose-and-scale es-
timator that exploits scale and/or gravity priors to improve
accuracy and speed. gDLS* is based on a least-squares
re-projection error cost function which facilitates the use of
regularizers that impose prior knowledge about the solution
space. This gDLS* derivation is general because these reg-
ularizers are quadratic functions that can easily be added to
other least-squares-based estimators. Experiments on both
synthetic and real data show that gDLS* improves speed
and accuracy of the pose-and-scale estimates given sufﬁ-
ciently accurate priors. The gravity prior is particularly ef-
fective, but the scale prior also improves the translation and
scale estimates. These ﬁndings make gDLS* an excellent
estimator for many applications where inertial sensors are
available such as AR, 3D mapping, and robotics.

Acknowledgment

Gang Hua was supported in part by the National Key
R&D Program of China Grant 2018AAA0101400 and
NSFC Grant 61629301.

123s/g Parameter Value0.050.10.150.20.25Rotation Error [deg]123s/g Parameter Value0.0050.010.0150.02Translation Error [m]123s/g Parameter Value10-3Scale Error123s/g Parameter Value0.050.10.150.20.25Rotation Error [deg]123s/g Parameter Value0.0050.010.0150.02Translation Error [m]123s/g Parameter Value10-3Scale Error123s/g Parameter Value0.050.10.150.20.25Rotation Error [deg]123s/g Parameter Value0.0050.010.0150.02Translation Error [m]123s/g Parameter Value10-3Scale Error(a) KITTI 123s/g Parameter Value0.160.180.2Rotation Error [deg]123s/g Parameter Value0.040.0450.050.0550.06Translation Error [m]123s/g Parameter Value2468Scale Error10-3123s/g Parameter Value0.160.180.2Rotation Error [deg]123s/g Parameter Value0.040.0450.050.0550.06Translation Error [m]123s/g Parameter Value2468Scale Error10-3123s/g Parameter Value0.160.180.2Rotation Error [deg]123s/g Parameter Value0.040.0450.050.0550.06Translation Error [m]123s/g Parameter Value2468Scale Error10-3(b) TUM  Best Baselines-gDLS*g-gDLS*sg-gDLS*

,


(cid:125)

(23)

(24)

A. Incorporating Priors via Regularizers

Section 3 of the main submission describes the proposed
formulation to include scale and rotation priors using regu-
larizers. This formulation is:

J (cid:48) = J(R, t, s, α) + λs (s0 − s)2
(cid:125)

(cid:124)

(cid:123)(cid:122)
Js

+ λg(cid:107)gQ × RgW (cid:107)2
,
(cid:125)
(cid:123)(cid:122)
Jg

(cid:124)

(14)

where

J(R, t, s, α) =

n
(cid:88)

i=1

(cid:107)αiri − (Rpi + t − sci) (cid:107)2

(15)

by combining Equations (17) and (18). Rearranging terms
of Eq. (21) yields Eq. (5) in the main submission, which is

x = (A(cid:124)A + P )−1 A(cid:124)W b + (A(cid:124)A + P )−1 P x0.

(22)

In order to obtain the simpliﬁed version of Eq. (22)
(5) of the main submission and inspired

shown in Eq.
by [43], we rewrite

A(cid:124)A + P =

=

4×4

n×4

(cid:20) In×n B(cid:48)
B(cid:48)(cid:124)
n×4 D(cid:48)
(cid:20) In×n Bn×4
(cid:124)
n×4 D4×4
B
(cid:123)(cid:122)
A(cid:124)A

(cid:124)

(cid:21)

(cid:21)

(cid:125)



0n×n

+



(cid:124)

λs

(cid:123)(cid:122)
P

03×3

is the gDLS [43] pose-and-scale re-projection error cost
function from the n 2D-3D correspondences, Js is the scale
prior regularizer, and Jg is the gravity direction constraint
imposing a rotation prior.

where

A.1. Cost Function Depending Only on Rotation

As mentioned in Section 3, the ﬁrst step to obtain a cost
function that we can minimize in a single shot is to rewrite
Eq. (14) as a function of the rotation matrix R. To do so, we
deﬁne

x = (cid:2)α1

. . . αn

s

t(cid:124)(cid:3)(cid:124)

,

(16)

a vector holding the depths for each i-th point αi, the scale
s, and translation vector t. We know that the optimal
depths, translation, and scale x(cid:63) vanish the gradient ∇xJ (cid:48)
of the cost function J (cid:48), i.e.,
∇xJ (cid:48)(cid:12)

(cid:12)x=x(cid:63) = [∇xJ + ∇xJs]x=x(cid:63) = 0.

(17)

To satisfy this constraint, we calculate each of the gradients
∇xJ and ∇xJs:






(cid:124)
(cid:124)
r
1 c1 −r
1
...
...
ncn −r(cid:124)
r(cid:124)
(cid:20)(cid:80)n
(cid:124)
i=1 c
i ci
(cid:80)n
i=1 −ci

n






(cid:80)n

i=1 −c
nI

(cid:21)

(cid:124)
i

,

B =

D =

and I is the identity matrix. It is important to mention that
A(cid:124)A is exactly the same as that of gDLS [43], and thus

A(cid:124)A + P =

D(cid:48)

4×4 =

(cid:21)

(cid:20) In×n Bn×4
(cid:124)
n×4 D(cid:48)
B
(cid:20)λs + (cid:80)n
(cid:80)n

4×4
(cid:124)
i ci
i=1 c
i=1 −ci

(25)

(cid:80)n

i=1 −c
nI

(cid:21)

(cid:124)
i

.

Eq. (22) requires the inverse of (A(cid:124)A + P )−1. To com-
pute a closed form relationship, we use the following block
matrix expression

∇xJ = 2A(cid:124)Ax − 2AW b
∇xJs = 2P x − 2P x0,

(18)

(A(cid:124)A + P )−1 =

(cid:20)En×n Fn×4
G4×n H4×4

(cid:21)

.

(26)

where



r1

A =




. . .

rn



0n×n

P =



λs




 , b =











p1
...
pn

c1 −I
...
...
cn −I


 , W =

03×3



R




. . .




 ,

R

(19)

and x0 = (cid:2)0(cid:124)

(cid:124)
n so 0
3

(cid:3)(cid:124)

. Note that

(cid:124)
Js = λs (s0 − s)2 = (x − x0)

P (x − x0) .

(20)

Thus, we can rewrite Eq. (17) into

∇xJ (cid:48) = A(cid:124)Ax − AW b + P x − P x0 = 0,

(21)

Through block matrix inversion, we obtain the following
closed-form block matrices:

E = I + BHB(cid:124)
F = −BH
G = −HB(cid:124)
H = (D(cid:48) − Y )−1
(cid:20)(cid:80)n
(cid:124)
(cid:124)
i ci
i rir
i=1 c
(cid:124)
(cid:80)n
i ci
i=1 −rir

Y =

(27)

(cid:21)

(cid:124)
i

(cid:80)n

i=1 −c
(cid:80)n

i=1 rir

(cid:124)
i rir
(cid:124)
i

Like in gDLS [43], we use matrices U , S, and V to sim-

plify Eq. (22), i.e.,

(A(cid:124)A + P )−1A(cid:124) =



 ,

(28)





U
S
V

where

(cid:124)
1


r



U =

(cid:21)

(cid:20)S
V

= −HB(cid:124)




 + B

(cid:21)

(cid:20)S
V



. . .

r(cid:124)
n

(cid:124)
r
1



. . .


 + H

r(cid:124)
n
. . . c(cid:124)
. . .

= H

(cid:20)c

(cid:124)
1 − c1r1r
(cid:124)
c1c
1 − I

(cid:124)
1

n − cnrnr(cid:124)
cnc(cid:124)
n − I

n

(cid:21)

.

(29)

We can simplify Eq. (22) further. To do this, we focus

on simplifying the term encoding the scale prior, yielding

(A(cid:124)A + P )−1P x0 =

(cid:21)

(cid:20)E F
G H


0


(cid:124)

λs

(cid:123)(cid:122)
P





(cid:125)

0

= λss0

(cid:21)

(cid:20)F1
B1
(cid:124) (cid:123)(cid:122) (cid:125)
l









0
s0
0
(cid:124) (cid:123)(cid:122) (cid:125)
x0

, (30)

where F1 and B1 are the ﬁrst column of the matrix F and
B, respectively. Combining Equations (30) and (28) allows
us to rewrite Eq. (22) as follows:

x =





U
S
V



 W b + λss0l,

(31)

which is the bottom part of Eq.
(5) in the main submis-
sion. Eq. (31) provides a linear relationship between depths,
scale, and translation and the rotation matrix. The explicit
relationships are the following

Eq. (14) and Eq. (33)) to rewrite the main cost function as
one depending only on rotation parameters. To do so as
clearly as possible, we deﬁne

(cid:20) c
(cid:124)
1
−I

(cid:21)

c(cid:124)
. . .
n
. . . −I

ei = αi(R)ri − (Rpi + t(R) − s(R)ci)
(cid:124)
i W b + λs0Fi,1) ri − Rpi
= (u

− (V W b + λs0H2:4,1)
+ (SW b + λs0H1,1) ci

(cid:124)
i W bri − Rpi − V W b + SW bci
= u
.
(cid:125)
(cid:123)(cid:122)
(cid:124)
ηi

(34)

+ λs0 (Fi,1ri − H2:4,1 + H1,1qi)
(cid:123)(cid:122)
(cid:125)
ki

(cid:124)

= ηi + ki

As noted in gDLS [43] paper, ηi can be factored out as

follows:

(cid:124)
i − I) (Rpi − SW bci + V W b)
ηi = (rir
i − I) (cid:2)L(pi) −ciSL(b) V L(b)(cid:3)
(cid:124)
= (rir
(cid:125)
(cid:124)

(cid:123)(cid:122)
Mi

vec(R),

(35)

where vec(R) vectorizes a rotation matrix R, and L(z)
is a function that computes a matrix such that Rz =
L(z)vec(R). Since we use the rotation representation of
Upnp [22], i.e.,

vec(R) = (cid:2)q2

1 q2

2 q2

3 q2

4 q1q2 q1q3 q1q4 q2q3 q2q4 q3q4

(cid:3)(cid:124)

,
(36)

then the function L(·) is

(cid:124)
αi(R) = u
i W b + λssoli
s(R) = SW b + λssoln+1
t(R) = V W b + λssolt,

(32)



(cid:124)
i is the i-th row of matrix U , lj is the j-th entry of
where u
the vector l, and lt corresponds to the last three entries of
the vector l. Speciﬁcally, the entries of vector l are

L(z)(cid:124) =

l =










l1
...
ln
ln+1
lt










=










F1,1
...
Fn,1
H1,1
H2:4,1










,

(33)

z2
z3
z1
−z2 −z3
z1
−z3
z2
−z1
z3
−z1 −z2
2z2
−2z3
−2z1
0
0
2z1
0
2z1
2z1
0
2z2
2z3

0
2z3
−2z2
2z2
2z3
0

































.

(37)

where H2:4,1 represent the last three entries of the ﬁrst col-
umn of H. We can use these explicit relationships (i.e.,

By substituting the relationships shown in Eq. (32) and
the factorizations shown in Eq. (35) into Eq. (14), we obtain

the following relationships:

J (cid:48)
gDLS =

=

n
(cid:88)

i=1
n
(cid:88)

i=1

(cid:124)
i ei =
e

n
(cid:88)

i=1

η

(cid:124)
i ηi + 2k

(cid:124)
i ηi + k

(cid:124)
i ki

vec(R)(cid:124)M

(cid:124)
i Mivec(R) + 2k

(cid:124)
i Mivec(R) + k

(cid:124)
i ki

= vec(R)(cid:124)

(cid:32) n
(cid:88)

M

(cid:124)
i Mi

vec(R)+

(cid:33)

(cid:125)

;

i=1

(cid:124)

(cid:123)(cid:122)
MgDLS
(cid:33)

(cid:32) n
(cid:88)

2

(cid:124)
i Mi
k

(cid:124)

i=1

(cid:123)(cid:122)
(cid:124)
d
gDLS

(cid:125)

vec(R) +

n
(cid:88)

k

(cid:124)
i ki

i=1
(cid:124)

= vec(R)(cid:124)MgDLSvec(R) + 2d

(cid:125)

(cid:123)(cid:122)
kgDLS
(cid:124)
gDLSvec(R) + kgDLS

(38)

s = λs (s0 − s(R))2
J (cid:48)
= λs (SL(b)vec(R) + λss0H1,1 − s0)2
= vec(R)(cid:124) (λsL(b)(cid:124)S(cid:124)SL(b))
vec(R)+
(cid:125)

(cid:124)

(cid:123)(cid:122)
Ms

2 λs (s0 − λss0H1,1) SL(b)
(cid:125)
(cid:123)(cid:122)
(cid:124)
d
s

(cid:124)

vec(R)+

; and

(39)

λs (λss0H1,1 − s0)2
(cid:124)
(cid:123)(cid:122)
(cid:125)
ks
= vec(R)(cid:124)Msvec(R) + 2d(cid:124)

s vec(R) + ks

J (cid:48)
g = λg(cid:107)gQ × RgW (cid:107)2
×(cid:98)gQ(cid:99)×L(gW )(cid:1)
= vec(R)(cid:124) (cid:0)λgL(gW )(cid:124)(cid:98)gQ(cid:99)
(cid:123)(cid:122)
(cid:125)
Mg

(cid:124)

(cid:124)

= vec(R)(cid:124)Mgvec(R)

vec(R)
.

(40)

The symbol (cid:98)·(cid:99)× indicates the skew symmetric matrix. By
putting together the components of the cost, we end up with
the ﬁnal cost function

J (cid:48) = J (cid:48)

gDLS + J (cid:48)

s + J (cid:48)
g,

(41)

which is Eq. (10) in the main submission.

References

[1] Azure

spatial

anchors.

https://azure.microsoft.com/en-

us/services/spatial-anchors/. 1

[2] Martin Bujnak, Zuzana Kukelova, and Tomas Pajdla. A gen-
eral solution to the p4p problem for camera with unknown
focal length. In Proc. of the IEEE Conf. on Computer Vision
and Pattern Recognition (CVPR), 2008. 1, 2

[3] Federico Camposeco, Andrea Cohen, Marc Pollefeys, and
Torsten Sattler. Hybrid camera pose estimation. In Proc. of
the IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR), 2018. 1

[4] Chu-Song Chen and Wen-Yan Chang. Pose estimation for
generalized imaging device via solving non-perspective n
point problem. In Proc. of the IEEE International Confer-
ence on Robotics and Automation (ICRA), 2002. 2

[5] Joseph DeGol, Timothy Bretl, and Derek Hoiem. Improved
In
structure from motion using ﬁducial marker matching.
Proc. of the European Conf. on Computer Vision (ECCV),
2018. 3, 7

[6] Jakob Engel, Thomas Sch¨ops, and Daniel Cremers. Lsd-
In Proc. of the
slam: Large-scale direct monocular slam.
European Conference on Computer Vision (ECCV), 2014. 1
[7] Luis Ferraz, Xavier Binefa, and Francesc Moreno-Noguer.
Very fast solution to the pnp problem with algebraic outlier
In Proc. of the IEEE Conf. on Computer Vision
rejection.
and Pattern Recognition (CVPR), 2014. 1

[8] Luis Ferraz, Xavier Binefa, and Francesc Moreno-Noguer.
Very fast solution to the pnp problem with algebraic outlier
In Proc. of the IEEE Conf. on Computer Vision
rejection.
and Pattern Recognition (CVPR), 2014. 2

[9] Martin A Fischler and Robert C Bolles. Random sample
consensus: a paradigm for model ﬁtting with applications to
image analysis and automated cartography. Communications
of the ACM, 24(6):381–395, 1981. 7

[10] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel
Urtasun. Vision meets robotics: The kitti dataset. Intl. Jour-
nal of Robotics Research (IJRR), 2013. 2, 5

[11] Marcel Geppert, Peidong Liu, Zhaopeng Cui, Marc Polle-
Efﬁcient 2d-3d matching
ArXiV preprint

feys, and Torsten Sattler.
for multi-camera visual
arXiv:1809.06445, 2018. 1

localization.

[12] M. D. Grossberg and S. K. Nayar. A general imaging model
and a method for ﬁnding its parameters. In Proc. of the IEEE
Intl. Conf. on Computer Vision (ICCV), 2001. 1

[13] Richard Hartley, Jochen Trumpf, Yuchao Dai, and Hongdong
Intl. Journal of Computer Vision

Li. Rotation averaging.
(IJCV), 103(3):267–305, 2013. 6

[14] Gim Hee Lee, Bo Li, Marc Pollefeys, and Friedrich Fraun-
dorfer. Minimal solutions for pose estimation of a multi-
camera system. In Robotics Research: The 16th Intl. Sympo-
sium ISRR, pages 521–538. Springer, 2016. 2

[15] Lionel Heng, Benjamin Choi, Zhaopeng Cui, Marcel Gep-
pert, Sixing Hu, Benson Kuan, Peidong Liu, Rang Nguyen,
Project autovi-
Ye Chuan Yeo, Andreas Geiger, et al.
sion: Localization and 3d scene perception for an au-
ArXiV
tonomous vehicle with a multi-camera system.
preprint arXiv:1809.05477, 2018. 1

[16] Joel A Hesch and Stergios I Roumeliotis. A direct least-
squares (DLS) method for PnP. In Proc. of the IEEE Intl.
Conf. on Computer Vision (ICCV), 2011. 1, 2, 3, 4, 5
[17] Du Q Huynh. Metrics for 3d rotations: Comparison and
Journal of Mathematical Imaging and Vision,

analysis.
35(2):155–164, 2009. 6

[18] Eagle S Jones and Stefano Soatto. Visual-inertial navigation,
mapping and localization: A scalable real-time causal ap-

proach. The Intl. Journal of Robotics Research, 30(4):407–
430, 2011. 1

[19] Georg Klein and David Murray. Parallel tracking and map-
ping on a camera phone. In Proc. of the IEEE Intl. Sympo-
sium on Mixed and Augmented Reality (ISMAR), 2009. 1
[20] Laurent Kneip, Margarita Chli, and Roland Y Siegwart. Ro-
bust real-time visual odometry with a single camera and an
In Proc. of the British Machine Vision Conference
imu.
(BMVC), 2011. 1

[21] Laurent Kneip, Paul Furgale, and Roland Siegwart. Using
multi-camera systems in robotics: Efﬁcient solutions to the
npnp problem. In Proc. of the IEEE Intl. Conf. on Robotics
and Automation (ICRA), 2013. 1, 2

[22] Laurent Kneip, Hongdong Li, and Yongduek Seo. Upnp:
An optimal o(n) solution to the absolute pose problem with
In Proc. of the European Conf. on
universal applicability.
Computer Vision (ECCV), 2014. 1, 2, 5, 6, 7, 10

[23] Laurent Kneip, Davide Scaramuzza, and Roland Siegwart.
A novel parametrization of the perspective-three-point prob-
lem for a direct computation of absolute camera position and
orientation. In Proc. of the IEEE Conf. Computer Vision and
Pattern Recognition (CVPR), 2011. 1, 2

[24] Zuzana Kukelova, Martin Bujnak, and Tomas Pajdla. Auto-
matic generator of minimal problem solvers. In Proc. of the
European Conf. on Computer Vision (ECCV), 2008. 2
[25] Zuzana Kukelova, Martin Bujnak, and Tomas Pajdla.
Closed-form solutions to minimal absolute pose problems
with known vertical direction. In Proc. of the Asian Conf.
on Computer Vision (ACCV), 2010. 2

[26] Zuzana Kukelova, Martin Bujnak, and Tomas Pajdla. Real-
time solution to the absolute pose problem with unknown
radial distortion and focal length. In Proc. of the IEEE Intl.
Conf. on Computer Vision (ICCV), 2013. 1, 2

[27] Zuzana Kukelova, Jan Heller, and Andrew Fitzgibbon. Efﬁ-
cient intersection of three quadrics and applications in com-
puter vision. In Proc. of the IEEE Conf. on Computer Vision
and Pattern Recognition (CVPR), 2016. 2

[28] Viktor Larsson, Magnus Oskarsson, Kalle Astrom, Alge
Wallis, Zuzana Kukelova, and Tomas Pajdla. Beyond grob-
ner bases: Basis selection for minimal solvers. In Proc. of
the IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR), 2018. 2

[29] Vincent Lepetit, Francesc Moreno-Noguer, and Pascal Fua.
Epnp: An accurate o(n) solution to the pnp problem. Intl.
Journal of Computer Vision (IJCV), 81(2):155, 2009. 1, 2

[30] Jesse Levinson, Michael Montemerlo, and Sebastian Thrun.
Map-based precision vehicle localization in urban environ-
ments. In Robotics: Science and Systems, 2007. 1

[31] Yi Ma, Stefano Soatto, Jana Kosecka, and S Shankar Sastry.
An invitation to 3-d vision: from images to geometric models,
volume 26. Springer Science & Business Media, 2012. 1
[32] Pierre Martin, Eric Marchand, Pascal Houlier, and Isabelle
Marchal. Mapping and re-localization for mobile augmented
reality. In Proc. of the IEEE Intl. Conf. on Image Processing
(ICIP), 2014. 1

[33] P. Miraldo and H. Araujo. A simple and robust solution to
the minimal general pose estimation. In Proc. of the IEEE In-
ternational Conference on Robotics and Automation (ICRA),
2014. 1

[34] Raul Mur-Artal and Juan D Tard´os. Orb-slam2: An open-
source slam system for monocular, stereo, and rgb-d cam-
IEEE Transactions on Robotics, 33(5):1255–1262,
eras.
2017. 1

[35] David Nist´er and Henrik Stew´enius. A minimal solution to
the generalised 3-point pose problem. Journal of Mathemat-
ical Imaging and Vision, 27(1):67–79, 2007. 2

[36] Gabriel N¨utzi, Stephan Weiss, Davide Scaramuzza, and
Roland Siegwart. Fusion of imu and vision for absolute
scale estimation in monocular slam. Journal of Intelligent
& Robotic Systems, 2011. 3

[37] Robert Pless. Using many cameras as one.

In Proc. of
the IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR), 2003. 1

[38] Torsten Sattler, Will Maddern, Carl Toft, Akihiko Torii,
Lars Hammarstrand, Erik Stenborg, Daniel Safari, Masatoshi
Okutomi, Marc Pollefeys, Josef Sivic, et al. Benchmark-
ing 6dof outdoor visual localization in changing conditions.
In Proc. of the IEEE Conf. on Computer Vision and Pattern
Recognition (CVPR), 2018. 1, 7

[39] Dieter Schmalstieg and Tobias Hollerer. Augmented reality:
principles and practice. Addison-Wesley Professional, 2016.
1

[40] J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cre-
mers. A benchmark for the evaluation of rgb-d slam sys-
tems. In Proc. of the Intl. Conf. on Intelligent Robot Systems
(IROS), 2012. 2, 5

[41] Chris Sweeney,

John Flynn, Benjamin Nuernberger,
Matthew Turk, and Tobias H¨ollerer. Efﬁcient computation of
absolute pose for gravity-aware augmented reality. In Proc.
of the IEEE Intl. Symposium on Mixed and Augmented Real-
ity (ISMAR), 2015. 1

[42] Chris Sweeney, John Flynn, and Matthew Turk. Solving for
relative pose with a partially known rotation is a quadratic
eigenvalue problem. In Proc. of the Intl. Conf. on 3D Vision
(3DV), 2014. 1

[43] Chris Sweeney, Victor Fragoso, Tobias H¨ollerer, and
Matthew Turk. gDLS: A scalable solution to the general-
ized pose and scale problem. In Proc. of the European Conf.
on Computer Vision (ECCV), 2014. 1, 2, 3, 4, 5, 6, 7, 9, 10

[44] Chris Sweeney, Victor Fragoso, Tobias H¨ollerer, and
Matthew Turk. Large scale sfm with the distributed camera
model. In Proc. of the IEEE Intl. Conf. on 3D Vision (3DV),
2016. 1, 2, 3, 4, 5, 6, 7

[45] Christopher Sweeney, Tobias Hollerer, and Matthew Turk.
Theia: A fast and scalable structure-from-motion library. In
Proc. of the ACM Intl. Conf. on Multimedia, 2015. 5
[46] K. Tsotsos, A. Chiuso, and S. Soatto. Robust inference for
visual-inertial sensor fusion. In Proc. of the Intl. Conference
on Robotics and Automation (ICRA). 2015. 1

[47] Jonathan Ventura, Clemens Arth, Gerhard Reitmayr, and Di-
eter Schmalstieg. Global localization from monocular slam
on a mobile phone. IEEE Transactions on Visualization and
Computer Graphics, 20(4):531–539, 2014. 1

[48] Jonathan Ventura, Clemens Arth, Gerhard Reitmayr, and Di-
eter Schmalstieg. A minimal solution to the generalized
pose-and-scale problem. In Proc. of the IEEE Conf. on Com-
puter Vision and Pattern Recognition (CVPR), 2014. 1, 2, 5,
6, 7

[49] Jonathan Ventura and Tobias H¨ollerer. Wide-area scene
In Proc. of the IEEE
mapping for mobile visual tracking.
Intl. Symposium on Mixed and Augmented Reality (ISMAR),
2012. 1

[50] Stephan Weiss, Markus W Achtelik, Simon Lynen,
Michael C Achtelik, Laurent Kneip, Margarita Chli, and
Roland Siegwart. Monocular vision for long-term micro
aerial vehicle state estimation: A compendium. Journal of
Field Robotics, 30(5):803–831, 2013. 1

[51] Changchang Wu. P3. 5p: Pose estimation with unknown
focal length. In Proc. of the IEEE Conf. on Computer Vision
and Pattern Recognition (CVPR), 2015. 2

[52] Bernhard Zeisl, Torsten Sattler, and Marc Pollefeys. Cam-

era pose voting for large-scale image-based localization. In
Proc. of the IEEE Intl. Conf. on Computer Vision (ICCV),
2015. 1

[53] Yinqiang Zheng, Yubin Kuang, Shigeki Sugimoto, Kalle As-
trom, and Masatoshi Okutomi. Revisiting the pnp problem:
A fast, general and optimal solution. In Proc. of the IEEE
Intl. Conf. on Computer Vision (ICCV), 2013. 1, 2

[54] Julius Ziegler, Henning Lategahn, Markus Schreiber,
Christoph G Keller, Carsten Kn¨oppel, Jochen Hipp, Martin
Haueis, and Christoph Stiller. Video based localization for
bertha. In Proc. of the IEEE Intelligent Vehicles Symposium,

2014. 1

