Subgeometrically ergodic autoregressions with
autoregressive conditional heteroskedasticity∗

Mika Meitz

Pentti Saikkonen

University of Helsinki

University of Helsinki

May 2022

Abstract

In this paper, we consider subgeometric ergodicity of univariate nonlinear autoregressions
with autoregressive conditional heteroskedasticity (ARCH). The notion of subgeometric
ergodicity was introduced in the Markov chain literature in 1980s and it means that the
transition probability measures converge to the stationary measure at a rate slower than
geometric; this rate is also closely related to the convergence rate of β-mixing coeﬃ-
cients. While the existing literature on subgeometrically ergodic autoregressions assumes
a homoskedastic error term, this paper provides an extension to the case of conditionally
heteroskedastic ARCH-type errors, considerably widening the scope of potential appli-
cations. Speciﬁcally, we consider suitably deﬁned higher-order nonlinear autoregressions
with possibly nonlinear ARCH errors and show that they are, under appropriate condi-
tions, subgeometrically ergodic at a polynomial rate. An empirical example using energy
sector volatility index data illustrates the use of subgeometrically ergodic AR–ARCH mod-
els.

JEL classiﬁcation: C22.

MSC2020 classiﬁcations: 60J05, 37A25.

Keywords: Nonlinear autoregressive model, autoregressive conditional heteroskedastic-
ity, ARCH, subgeometric ergodicity, polynomial ergodicity, Markov chain, β-mixing.

2
2
0
2

y
a
M
4
2

]

M
E
.
n
o
c
e
[

1
v
3
5
9
1
1
.
5
0
2
2
:
v
i
X
r
a

∗The authors thank the Academy of Finland (MM and PS), Foundation for the Advancement of Finnish
Securities Markets (MM), and OP Group Research Foundation (MM) for ﬁnancial support. Contact addresses:
Mika Meitz, Department of Economics, University of Helsinki, P. O. Box 17, FI–00014 University of Helsinki,
Finland; e-mail: mika.meitz@helsinki.ﬁ. Pentti Saikkonen, Department of Mathematics and Statistics, Univer-
sity of Helsinki, P. O. Box 68, FI–00014 University of Helsinki, Finland; e-mail: pentti.saikkonen@helsinki.ﬁ.

1

 
 
 
 
 
 
1 Introduction

Let Xt (t = 0, 1, 2, . . .) be a Markov chain on the state space X and initialized from an X0
following some initial distribution. If the n-step probability measures P n(x ; ·) = Pr(Xn ∈ · |
X0 = x) converge in total variation norm (cid:107) · (cid:107)T V to the stationary probability measure π at
rate rn (for some r > 1), that is,

lim
n→∞

rn(cid:107)P n(x ; ·) − π(·)(cid:107)T V = 0,

π a.e.,

(1)

the Markov chain is said to be geometrically ergodic. When the convergence in (1) takes place
at a suitably deﬁned rate r(n) slower than geometric, that is,

lim
n→∞

r(n)(cid:107)P n(x ; ·) − π(·)(cid:107)T V = 0,

π a.e.,

(2)

the Markov chain is called subgeometrically ergodic. Examples of common rates (where c
denotes a positive constant) include geometric (or exponential) when r(n) = ecn = rn (r > 1),
subexponential when r(n) = ecnγ (0 < γ < 1), polynomial when r(n) = (1+n)c, and logarithmic
when r(n) = (1 + ln(n))c. The authoritative and classic reference to Markov chain theory is
the monograph of Meyn and Tweedie (2009), while an up-to-date treatment of subgeometric
ergodicity can be found in Chapters 16 and 17 of Douc, Moulines, Priouret, and Soulier (2018).
To give some background, the notion of subgeometric ergodicity was introduced in the
Markov chain literature in the 1980s when Nummelin and Tuominen (1983) and Tweedie (1983)
obtained the ﬁrst subgeometric ergodicity results for general state space Markov chains. Sub-
sequent work by Tuominen and Tweedie (1994), Fort and Moulines (2000), Jarner and Roberts
(2002), Fort and Moulines (2003), and Douc, Fort, Moulines, and Soulier (2004) lead to a for-
mulation of a so-called drift condition to ensure subgeometric ergodicity, paralleling the use of a
Foster-Lyapunov drift condition to establish geometric ergodicity (see, e.g., Meyn and Tweedie,
2009, Ch 15). Various topics in probability theory and statistics have also been considered
under subgeometric assumptions; for instance, Douc, Guillin, and Moulines (2008) considered
the central limit theorem and Berry-Esseen bounds, Atchadé and Fort (2010) the convergence
of Markov chain Monte Carlo algorithms, Merlevède, Peligrad, and Rio (2011) a Bernstein-
type inequality, and Meitz and Saikkonen (2021) the rate of β-mixing. In this paper we are
interested in autoregressive time series models. Results regarding the subgeometric ergodicity
of ﬁrst-order autoregressions were obtained by Tuominen and Tweedie (1994), Veretennikov
(2000), Fort and Moulines (2003), Douc et al. (2004), Klokov and Veretennikov (2004, 2005),
and Klokov (2007), among others, whereas results for more general higher-order autoregressions
were obtained by Meitz and Saikkonen (2020).

In this paper we consider subgeometric ergodicity of autoregressive models with autore-
gressive conditional heteroskedasticity (ARCH; Engle, 1982). The previous works on subge-
ometrically ergodic autoregressions listed above only considered the case of independent and
identically distributed (IID) errors, and allowing for conditionally heteroskedastic errors con-
siderably widens the scope of potential applications. This is particularly important in applica-
tions using economic and ﬁnancial time series data. In the subgeometrically ergodic AR–ARCH
models we consider, the conditional mean is similar to the (homoskedastic) AR models already

2

considered in Meitz and Saikkonen (2020). The precise model formulation will be given and
motivated further in Section 2, but we already note that the models we consider accommodate
for behavior similar to a unit root process for large values of the observed series but almost
no restrictions are placed on their dynamics for moderate values of the observed series. The
conditional variance is allowed to follow a rather general nonlinear ARCH process. In our main
result, we show that the considered AR–ARCH processes are, under appropriate conditions,
subgeometrically ergodic at a polynomial rate (details of polynomial ergodicity and polynomial
rates will be discussed later).

The inclusion of ARCH (instead of IID) errors considerably complicates the proofs of
(sub)geometric ergodicity of nonlinear autoregressions. Papers considering subgeometric er-
godicity of homoskedastic autoregressions were already listed above. Geometric ergodicity of
nonlinear autoregressive models with ARCH (or generalized ARCH) errors has previously been
considered by numerous authors; see, e.g., Cline and Pu (2004), Meitz and Saikkonen (2008,
2010), and the many references therein. Compared to these two strands of previous literature,
the combination of the subgeometrically ergodic type of nonlinear dynamics in the conditional
mean with ARCH errors leads to additional complications in the proofs. To appropriately
separate these two sources of dynamics we make use of a (relatively unknown) extension of
Bernoulli’s inequality due to Feﬀerman and Shapiro (1972) (combined with Young’s inequal-
ity), and to control terms arising due to conditional heteroskedasticity we devise a special
matrix norm that is of a more complicated type than the norms typically used when analysing
the stability of nonlinear time series models.

The rest of the paper is organized as follows. Section 2 introduces the nonlinear AR–ARCH
model considered and states the assumptions we employ. Results on subgeometric ergodicity
are given in Section 3. In Section 4 we consider an empirical application of our model to a daily
time series of an energy sector volatility index. Section 5 concludes. All proofs are collected in
an Appendix.

2 Model

2.1 Conditional mean

We consider the univariate process yt (t = 1, 2, . . .) generated by

yt = π1yt−1 + · · · + πp−1yt−p+1 + g(ut−1) + σtεt,

(3)

where p ≥ 1 is the autoregressive order, ut = yt − π1yt−1 − · · · − πp−1yt−p+1, g is a real-valued
function, εt is an IID error term, and σt = σ(yt−1) is a positive volatility term that depends on
p + q lagged values of yt, yt−1 = (yt−1, . . . , yt−p−q), where q ≥ 1 is an ARCH order. For now,
one concrete example of the volatility term is a linear ARCH process, where σt satisﬁes

t = ω + α1e2
σ2

t−1 + · · · + αqe2

t−q

(4)

and et = yt − π1yt−1 − · · · − πp−1yt−p+1 − g(ut−1), ω > 0, and αi ≥ 0 (i = 1, . . . , q); a more
general formulation for the conditional variance will be considered below. Note that a compact

3

expression for et is et = ut −g(ut−1) so that equation (3) can be expressed as ut = g(ut−1)+σtεt.
If π1 = · · · = πp−1 = 0 in equation (3), we have ut = yt so that the autoregressive order p reduces
to one and equation (3) reduces to yt = g(yt−1) + σtεt.

Our ﬁrst assumption contains basic requirements for the error term εt and makes clear that

the squared volatility, σ2

t , is the conditional variance of yt.

Assumption 1. {εt, t = 1, 2, . . .} is a sequence of IID random variables that is independent of
(y0, . . . , y1−p−q), has zero mean and unit variance, and the distribution of ε1 has a (Lebesgue)
density that is bounded away from zero on compact subsets of R.

Later on we introduce an assumption on the conditional variance σ2
moments of εt.

t which further restricts the

To further describe the conditional mean of the autoregressions we consider, we next spec-
ify the conditions needed for the function g in equation (3). The following assumption is a
simpliﬁcation of Assumption 1 in Meitz and Saikkonen (2020) (the somewhat more general
formulation used therein is brieﬂy discussed at the end of this subsection).

Assumption 2.
(i) The roots of the polynomial (cid:36)(z) = 1 − π1z − · · · − πp−1zp−1 lie outside the unit circle.
(ii) The function g : R → R in (3) is measurable, locally bounded, and satisﬁes |g(u)| → ∞ as
|u| → ∞, and there exist positive constants r, M0, K0, and 0 < ρ < 2 such that for all u ∈ R

|g(u)| ≤

(cid:40)

(1 − r |u|−ρ) |u|
K0

for |u| ≥ M0,
for |u| ≤ M0.

(5)

Assumption 2(i) corresponds to the conventional stationarity condition of a linear autoregres-
sion in that it requires the roots of the polynomial (cid:36)(z) to lie outside the unit circle. In the
ﬁrst-order case p = 1, this condition becomes redundant because then π1 = · · · = πp−1 = 0.
Assumption 2(ii) is needed to prove the subgeometric ergodicity of the process yt, as already
done by Fort and Moulines (2003) and Douc et al. (2004) in the ﬁrst-order case p = 1 and by
Meitz and Saikkonen (2020) for higher-order autoregressions.

We next provide some intuition and motivation for our model in (3). To clarify the role of
inequality (5) restricting the function g(·), suppose Assumptions 1 and 2(i) hold but instead of
Assumption 2(ii) suppose the function g(·) were linear with g(u) = π0u and π0 ∈ [−1, 1]. Using
the lag operator L, equation (3) could then be written as

ut − π0ut−1 = (1 − π0L)(1 − π1L − · · · − πp−1Lp−1)yt = σtεt,

(6)

that is, as the familiar linear AR(p) model (with autoregressive heteroskedasticity). Given
Assumptions 1 and 2(i), the case π0 ∈ (−1, 1) corresponds to geometric ergodicity of yt and the
cases π0 = ±1 to non-ergodicity. Nonlinear functions g(·) satisfying Assumption 2(ii) provide
a middle ground between these extreme cases of geometric ergodicity and non-ergodicity. For
instance, if g(u) = (1 − r |u|−ρ)u for |u| > r1/ρ and g(u) = 0 otherwise (r > 0, 0 < ρ < 2), then
for any ﬁxed π0 ∈ (−1, 1) and for all u suﬃciently large in absolute value (i.e., for the values of
u that are crucial for determining ergodicity),

|π0||u| < |g(u)| < |u|.

4

The subgeometrically ergodic autoregressions we consider thus provide one possibility for mod-
eling small departures from unit root autoregressions. Assumption 2(ii) implies that for large
values of |ut−1|, the conditional mean of model (3) is close to that of an integrated process (of
order one). On the other hand, as inequality (5) restricts the function g(·) only for large values
of its argument, no restrictions (apart from the boundedness condition in (5)) are imposed
when the argument takes values inside some bounded set of values. Thus the autoregressions
we consider may exhibit rather arbitrary (stationary, unit root, explosive, nonlinear, etc.) be-
havior for moderate values of the observed series. For further discussion on models related to
ours, see Lieberman and Phillips (2020) and the introduction of Meitz and Saikkonen (2020).
Homoskedastic subgeometrically ergodic autoregressions satisfying (a somewhat more gen-
eral version of) Assumption 2 were already considered by Meitz and Saikkonen (2020). As
many time series in economics, ﬁnance, and other ﬁelds exhibit conditional heteroskedasticity,
in this paper we consider an extension to ARCH errors. In the homoskedastic case considered
in Meitz and Saikkonen (2020), the term g(ut−1) in (3) was replaced with the more general
formulation ut−1 + ˜g(yt−1, . . . , yt−p) (with ˜g a real-valued function) to allow for more general
dependence on the past through the variables yt−1, . . . , yt−p (and not only through the linear
combination ut−1 = yt−1 − π1yt−2 − · · · − πp−1yt−p). The present simpler formulation worked
well in the empirical application of Section 4 and in some other examples we tried out, and
leads to more transparent assumptions and streamlined proofs.

2.2 Companion form

To establish ergodicity, we need the companion form of the (p + q)-dimensional process yt =
(y1,t, y2,t) with a p-dimensional y1,t = (yt, . . . , yt−p+1) and a q-dimensional y2,t = (yt−p, . . . , yt−p−q+1).
First we formulate the p-dimensional companion form related to equation (3), which reads as











yt
yt−1
...
...
yt−p+1











=











π1 π2
0
1
. . .
. . .
· · ·

0
...
0



· · · πp−1 0
0
0
· · ·


...
...
. . .



. . .

0
0

0
1
0











yt−1
yt−2
...
...
yt−p











+ g(ut−1)





















1
0
...
...
0

+ σtεt





















1
0
...
...
0

or, denoting the matrix in this equation with Φ and setting ιp = (1, 0, . . . , 0) (p × 1), as

y1,t = Φy1,t−1 + g(ut−1)ιp + σtεtιp

(7)

(when p = 1, Φ = 0 and ut−1 = yt−1). As σt = σ(yt−1) depends on the whole (p+q)-dimensional
vector yt−1, we have to expand (7) to the (p + q)-dimensional companion form

(cid:34)

(cid:21)

=

(cid:20) y1,t
y2,t

Φ 0p×q

0q×(p−1)

Iq 0q×1

(cid:21)

(cid:35) (cid:20) y1,t−1
y2,t−1

+ g(ut−1)ιp+q + σtεtιp+q,

(8)

where Iq is the (q × q) identity matrix and 0∗×∗ denotes a matrix of zeros with the indicated
dimensions (and ιp+q is deﬁned in the obvious way). This shows that yt is a Markov chain on
Rp+q.

5

In order to establish ergodicity we further transform the p-dimensional companion form (7)
in a way already used in Meitz and Saikkonen (2020, Sec 4). To this end we deﬁne the matrices

A =

1
. . .


1 −π1 −π2 · · · −πp−1
0 · · ·
0


...
. . .
. . .


. . .
. . .

0

0
· · ·
0 · · ·

0
...
0
1










and Π = AΦA−1 =









0

0 · · ·
0 0
1 π1 π2 · · · πp−1
0 · · ·
0 1
...
. . .
. . .
. . .
1
0 · · · 0

0
...
0









=

(cid:20) 0 01×(p−1)
ιp−1 Π1

(cid:21)

,

(9)
where A is nonsingular and Π1 is the (p − 1) × (p − 1) dimensional lower right hand corner of
Π (when p = 1, A = 1 and Π = 0). With these deﬁnitions equation (7) can be transformed
into

Ay1,t = ΠAy1,t−1 + g(ut−1)ιp + σtεtιp,

(10)

where Ay1,t = (ut, yt−1, . . . , yt−p+1). Now, for any p-dimensional vector x1, form the partition
x1 = (x1,1, . . . , x1,p) = (x1,1, x1,2) and deﬁne

z(x1) =

(cid:21)

(cid:20)z1(x1)
z2(x1)

= Ax1 =

(cid:20)x1,1 − π1x1,2 − · · · − πp−1x1,p
x1,2

(cid:21)

(11)

(when p = 1, x1,2 and z2(x1) are dropped). Using this notation equation (10) can be expressed
as z(y1,t) = Πz(y1,t−1) + g(z1(y1,t−1))ιp + σ(yt−1)εtιp, that is, as

(cid:21)

(cid:20)z1(y1,t)
z2(y1,t)

=

=

(cid:21)

(cid:21) (cid:20)z1(y1,t−1)
(cid:20) 0 01×(p−1)
z2(y1,t−1)
ιp−1 Π1
(cid:20) g(z1(y1,t−1)) + σ(yt−1)εt
Π1z2(y1,t−1) + z1(y1,t−1)ιp−1

+ g(z1(y1,t−1))ιp + σ(yt−1)εtιp

(cid:21)

.

(12)

Here the ﬁrst equation is in a form where the autoregressive order is one and the volatility
term is a function of the (p + q)-dimensional vector yt−1 = (y1,t−1, y2,t−1) whereas the second
equation involves the p-dimensional vector y1,t−1 only.

By Assumption 2(i), the roots of the polynomial (cid:36)(z) lie outside the unit circle, so that the
eigenvalues of the matrix Π1 in the second equation in (12) are smaller than one in absolute
value. As is well known, this implies the existence of a matrix norm of Π1 that is also smaller
than one. Speciﬁcally, for any vector norm (cid:107) · (cid:107), denote by ||| · ||| the corresponding induced
matrix norm (Horn and Johnson, 2013, Defn 5.6.1); that is, for any conformable square matrix
A, set

|||A||| = max
(cid:107)x(cid:107)=1

(cid:107)Ax(cid:107).

Then we obtain the following result (Horn and Johnson, 2013, Lemma 5.6.10).

Lemma 1. There exists a vector norm (cid:107) · (cid:107)∗ and a corresponding induced matrix norm ||| · |||∗
such that |||Π1|||∗ = (cid:36) < 1.

The existence of an induced matrix norm with the property in the above lemma is essential
in our proofs. (When p = 1, Assumption 2(i) and Lemma 1 are redundant.) The norms (cid:107) · (cid:107)∗
and ||| · |||∗ are deﬁned on Rp−1 and R(p−1)×(p−1), respectively, and they have been commonly

6

used in time series models. In the next subsection we introduce norms which are of a diﬀerent
type.

2.3 Conditional variance

The root condition of Assumption 2(i) and inequality (5) of Assumption 2(ii) are of major
importance for establishing the stability of our model. However, as these conditions only con-
cern the conditional mean, we need additional assumptions restricting the conditional variance
σ2
t . As an extension of the basic ARCH model (4) we consider a nonlinear formulation of the
conditional variance deﬁned as

t = ζ0,t−1ω + α1ζ1,t−1e2
σ2

t−1 + · · · + αqζq,t−1e2

t−q,

(13)

where ζi,t−1 = ζi(yt−1) is a function of yt−1 (i = 0, . . . , q) and otherwise the notation is as in
equation (4) (including the conditions ω > 0 and α1, . . . , αq ≥ 0). When the functions ζi,t−1 are
the same for all i = 0, . . . , q we remove the index i and use the notations ζt−1 and ζ(·). This is
the case in our empirical example where ζt−1 = ζ(yt−1) = 1/(1+e−γ(yt−1−a)) is a logistic function
depending only on yt−1. For possible alternatives we consider a more general formulation and
introduce the following assumption.

Assumption 3. In equation (13), the following conditions are assumed. (i) The parameters
ω, α1, . . . , αq satisfy ω > 0, α1, . . . , αq ≥ 0, and (cid:80)q
i=1 αi < 1. (ii) For each i = 0, . . . , q, the
function ζi takes values in (0, 1].

The above assumption includes the case ζi ≡ 1 for all i, which corresponds to the linear ARCH
model (4). It covers also the above-mentioned logistic function.

0, . . . , e2

Consider the q-dimensional process ξt = (e2

t−q+1) (t ≥ 1) with initial values
ξ0 = (e2
−q+1 are functions of y0. Inspired by Cline and Pu (2004,
Example 4.2) we now introduce the following equation which is a straightforward implication
of equation (13) and the fact σtεt = et:

−q+1) where e2

t−1, . . . , e2

0, . . . , e2

t , e2



















e2
t
e2
t−1
...
...

e2
t−q+1

=










α1ζ1,t−1ε2
1

t α2ζ2,t−1ε2
t
0

0
...
0

1
. . .
· · ·

· · · αq−1ζq−1,t−1ε2
· · ·
. . .
. . .
0

0
...
0
1

t αqζq,t−1ε2
t
0
...
0
0




























e2
t−1
e2
t−2
...
...
e2
t−q

+










t ω

ζ0,t−1ε2
0
...
...
0










(t = 1, 2, . . .) or, more brieﬂy,

ξt = Λζ,tξt−1 + ωζ,t,

t = 1, 2, . . . ;

(14)

as ξt is a function of yt, we occasionally write ξt = ξ(yt). For later purposes we also note that
due to the identities σtεt = et and e2
qξ(yt) we have

t = ι(cid:48)

t = σ2(yt−1) = E[ι(cid:48)
σ2

qξ(yt) | yt−1].

(15)

7

When there is need to make the dependence of Λζ,t on yt−1 explicit we use the notation Λζ,t(yt−1)
and replace the (random) argument yt−1 by a ﬁxed counterpart when needed. Speciﬁcally,
Λζ,t(x) means that the functions ζi,t−1 = ζi,t−1(yt−1) used in Λζ,t(yt−1) are replaced by ζi,t(x)
for all i = 1, . . . , q, and the notations σ2(x) and ξ(x) are used similarly.

We also deﬁne the matrices

Λt =










α1ε2
1

t α2ε2
t
0

0
...
0

1
. . .
· · ·

· · · αq−1ε2
· · ·
. . .
. . .
0

0
...
0
1

t αqε2
t
0
...
0
0










and Λ = E[Λt] =










α1 α2
0
1

0
...
0

1
. . .
· · ·

· · · αq−1 αq
0
0
· · ·
...
...
. . .
. . .
0
0
0
1
0










.

(16)

Note that Λt is obtained from the matrix Λζ,t by choosing ζi,t = 1 for all i = 1, . . . , q. Similarly,
we denote ωt = (ωt, 0, . . . , 0)(cid:48) with ωt = ωε2

t and ω = E[ωt] = (ω, 0, . . . , 0)(cid:48).

In our proofs we need to appropriately control the size of the random matrix Λt, and not
just the size of the non-random matrix Λ = E[Λt]. This is the reason why we next consider
vector and matrix norms more complicated than those in Lemma 1. To this end, we ﬁrst recall
the deﬁnition of an Lp-norm (for convenience, in this subsection only, we use the notation p in
Lp-norms; elsewhere in the paper p stands for the autoregressive order in model (3)). If (cid:107) · (cid:107) is
any vector norm on Rq and v is a q-dimensional random vector, equation

(cid:107)v(cid:107)Lp = (E[(cid:107)v(cid:107)p])1/p

(1 ≤ p < ∞)

deﬁnes an Lp-norm on the set of (equivalence classes of almost surely equal) q × 1 random
vectors that are p-integrable (see, e.g., Dudley, 2004, Secs 5.1 and 5.2). It may be worth noting
that for nonrandom vectors there is no diﬀerence between the norms (cid:107) · (cid:107) and (cid:107) · (cid:107)Lp but for
random vectors the outcome of (cid:107) · (cid:107) is random and that of (cid:107) · (cid:107)Lp is nonrandom. This Lp-norm
can be used to induce a norm for random matrices; for the conventional non-random matrix
case and for the terminology used below, see Horn and Johnson (2013, Def. 5.6.1 and Sec 5.6.).
Speciﬁcally, to deﬁne a generalized (non-submultiplicative) matrix norm ||| · |||Lp, for any q × q
random matrix A set

|||A|||Lp = max
(cid:107)x(cid:107)Lp =1

(cid:107)Ax(cid:107)Lp = max
(cid:107)x(cid:107)=1

(cid:107)Ax(cid:107)Lp

(x ∈ Rq),

(17)

where the latter equality holds as x is nonrandom. This deﬁnes a generalized matrix norm1 on
the set of (equivalence classes of almost surely equal) q × q random matrices with p-integrable
entries; moreover, the norms (cid:107) · (cid:107), (cid:107) · (cid:107)Lp, and ||| · |||Lp are related by the inequality2

(cid:107)Ax(cid:107)Lp ≤ |||A|||Lp(cid:107)x(cid:107)

(x ∈ Rq).

(18)

We next state a high-level condition that assumes the existence of a vector norm on Rq with

1Axioms (1), (1a), (2), and (3) of a generalized matrix norm (see Horn and Johnson, 2013, pp. 340–341) can
be checked similarly as in the proof of Theorem 5.6.2(c) of the same reference (replacing the norms (cid:107) · (cid:107) and ||| · |||
therein with (cid:107) · (cid:107)Lp and ||| · |||Lp , replacing appropriate statements therein with their almost sure counterparts,
and using Minkowski’s inequality as an additional justiﬁcation for axiom (3)).

2Inequality (18) can be veriﬁed analogously to Theorem 5.6.2(b) of Horn and Johnson (2013).

8

particular additional properties. (Primitive conditions ensuring this high-level assumption will
be given momentarily.) One of these properties is monotonicity in the sense of Deﬁnition 5.4.18
of Horn and Johnson (2013): a vector norm (cid:107) · (cid:107) is monotone if x, y ∈ Rq satisfying |xi| ≤ |yi|
for i = 1, . . . , q always implies that (cid:107)x(cid:107) ≤ (cid:107)y(cid:107). For clarity, we use notation (cid:107) · (cid:107)• for the speciﬁc
vector norm in the assumption below; similarly, we denote the related Lp-norm by (cid:107) · (cid:107)•Lp and
the generalized matrix norm by ||| · |||•Lp. We also introduce two constants, s0 ≥ 1 and b ≥ 1,
such that b = 1 when s0 = 1 and b > (2s0 − ρ)/[s0(2 − ρ)] > 1 when s0 > 1 (recall from
Assumption 2 that ρ ∈ (0, 2) so that 2s0 > ρ). These constants are used in the next section
where we establish our ergodicity result and there the size of s0 will have an eﬀect on the rate of
convergence obtained and the order of moments that are ﬁnite. The rather complex conditions
required from the constant b are due to the connection between the conditional mean and ARCH
errors (this connection disappears when s0 = 1 as it also does in subgeometric homoskedastic
autoregressions).

Assumption 4. Suppose there exists a vector norm (cid:107) · (cid:107)• on Rq that is (i) monotone and (ii)
such that |||Λt|||•Lbs0 = λ < 1, where b and s0 are as described above.

This assumption tacitly requires that E[|εt|2bs0] is ﬁnite, thereby strengthening Assumption 1
(when s0 > 1). Assumption 4 is formulated in a way that is convenient in our proofs but is not
very transparent. The following lemma gives primitive conditions ensuring that Assumption 4
holds (for a proof, see the appendix).

Lemma 2. Suppose that Assumptions 1 and 3 hold and also that the parameters α1, . . . , αq
in Assumption 3 satisfy (cid:80)q
1|bs0])1/bs0. Then Assumption 4
holds.

i=1 αi < 1/¯µ2bs0 where ¯µ2bs0 = (E[|ε2

3 Subgeometric ergodicity

We now consider the stability of the model introduced in the previous section. We begin with
a brief account of some necessary Markov chain concepts (for more comprehensive discussions,
see Meyn and Tweedie (2009) and Douc et al. (2018) and also Meitz and Saikkonen (2020, Sec
2)). Let Xt (t = 0, 1, 2, . . .) be a Markov chain on a general measurable state space (X, B(X))
(with B(X) the Borel σ-algebra) and let P n(x ; A) = Pr(Xn ∈ A | X0 = x) signify its n-step
transition probability measure. For an arbitrary ﬁxed measurable function f : X → [1, ∞) and
for any signed measure µ, deﬁne the f -norm (cid:107)µ(cid:107)f as

(cid:107)µ(cid:107)f = sup

f0:|f0|≤f

|µ(f0)| ,

(19)

where µ(f0) = (cid:82)
x∈X f0(x)µ(dx) and the supremum in (19) runs over all measurable functions
f0 : X → R such that |f0(x)| ≤ f (x) for all x ∈ X (when f ≡ 1, the f -norm (cid:107)µ(cid:107)f reduces
to the total variation norm (cid:107)µ(cid:107)T V = supf0:|f0|≤1 |µ(f0)| used in (1) and (2)). When the n-step
probability measures P n(x ; ·) converge in f -norm and at rate r(n) to the stationary probability

9

measure π satisfying π(f ) < ∞, that is,

lim
n→∞

r(n)(cid:107)P n(x ; ·) − π(cid:107)f = 0

for π-almost all x ∈ X, 3

(20)

we say that the Markov chain Xt is (f, r)-ergodic; this implicitly entails the existence of π as well
as certain moments as π(f ) < ∞. In the conventional geometrically ergodic case, r(n) = rn for
some r > 1. To establish (f, r)-ergodicity, we use a so-called drift condition deﬁned as follows
(here 1S(x) denotes the indicator function taking value one when x belongs to the set S and
zero elsewhere).

Condition D. There exist a measurable function V : X → [1, ∞), a concave increasing
continuously diﬀerentiable function φ :
[1, ∞) → (0, ∞), a measurable set C, and a ﬁnite
constant ˜b such that

E [V (X1) | X0 = x ] ≤ V (x) − φ (V (x)) + ˜b1C(x),

x ∈ X.

(21)

The idea is to verify this condition with suitable functions V and φ, which together with some
additional conditions ensures the (f, r)-ergodicity of the process Xt; for more details, see Meitz
and Saikkonen (2020, Thm 1).

Now consider the stability of the Markov chain yt on Rp+q given in (8). To deﬁne the
function V in (21), we use the functions z1(·), z2(·), and ξ(·) in (11)–(12) and (14) and the
norms (cid:107)·(cid:107)∗ and (cid:107)·(cid:107)• in Lemma 1 and Assumption 4. Set x = (x1, . . . , xp+q) ∈ Rp+q and
decompose x to its p- and q-dimensional components as x = (x1, x2). We deﬁne the function
V as

V (x) = 1 + |z1(x1)|2s0 + s1(cid:107)z2(x1)(cid:107)2s0α

∗ + s2(cid:107)ξ(x)(cid:107)bs0
• ,

(22)

where s0 and b are deﬁned above Assumption 4, s1 and s2 are positive constants to be speciﬁed
later (with s1 small and s2 large), and α = 1 − ρ/2s0 (recall from Assumption 2 that ρ ∈ (0, 2)
so that α ∈ (0, 1)); when p = 1, we can set s1 = 0 and drop z2(x1). To verify Condition D, we
need to consider the conditional expectation

E [V (y1) | y0 = x] = 1 + E (cid:2)|z1(y1,1)|2s0 | y0 = x(cid:3) + s1E (cid:2)(cid:107)z2(y1,1)(cid:107)2s0α

∗

+ s2E (cid:2)(cid:107)ξ(y1)(cid:107)bs0

•

| y0 = x(cid:3) ,

| y0 = x(cid:3)

(23)

bound the conditional expectations on the right hand side of (23), and express these bounds in
a way which conforms to inequality (21) with the function φ satisfying the conditions required
in Condition D. These considerations, combined with the checking of some additional technical
conditions, lead to the following theorem (the proof can be found in the Appendix).

Theorem 1. Consider the Markov chain yt deﬁned in (8). Suppose that Assumptions 1–4 hold
and that V (x) is as in (22). Then yt is (f, r)-ergodic with the polynomial convergence rate
r(n) = nδ−1 and the function f given by f (x) = V (x)1−δρ/2s0; this result holds for any choice
of δ ∈ [1, 2s0/ρ] and for some (small enough) s1 > 0 and some (large enough) s2 > 0.

3That is, the convergence in (20) is required to hold for all x ∈ X except for those x in a set that has

probability zero with respect to the stationary measure π.

10

Theorem 1 provides the ﬁrst subgeometric ergodicity results for autoregressions with au-
toregressive conditional heteroskedasticity. In this theorem, the convergence rate r(n) shows
the speed at which the n-step transition probability measures of the process yt converge to the
stationary probability measure. Due to the polynomial convergence rate we therefore call the
process yt polynomially ergodic. The choice of δ in Theorem 1 allows for a trade-oﬀ between
the rate of convergence and the size of the f -norm. Theorem 1 also implies that the process yt
is β-mixing (and hence α-mixing) and the rate at which β-mixing coeﬃcients decay is given by
the fastest convergence rate, that is, limn→∞ n2s0/ρ−1β(n) = 0. A further corollary of Theorem
1 is that the stationary distribution of yt has ﬁnite moments up to order 2s0 − ρ. (For details
of these results, see Meitz and Saikkonen, 2020, Sec 2.) It may also be clarifying to note that
when p = 1, model (3) reduces to yt = g(yt−1) + σtεt and the function V in (22) becomes
V (x) = 1 + |x1|2s0 + s2(cid:107)ξ(x)(cid:107)bs0
• .

Theorem 1 remains valid also in the homoskedastic case (obtained by setting α1 = · · · =
αq = 0). Previous polynomial ergodicity results for homoskedastic autogressions were obtained
by Fort and Moulines (2003, Sec 2.2) and Meitz and Saikkonen (2020, Thm 3), and the above
Theorem 1 provides partial improvements over these earlier results in certain cases. Assump-
tions and notation are slightly diﬀerent in all the papers, but (in the notation of the present
paper) Theorem 1 improves earlier results when 1 ≤ ρ < 2 and 1 < s0 < 2.

The proof of Theorem 1 is also somewhat diﬀerent from the previous polynomial ergodicity
results in Fort and Moulines (2003, Sec 2.2) and Meitz and Saikkonen (2020, Thm 3). A rather
obvious diﬀerence is that these earlier results deal with homoskedastic autoregressions whereas
our model contains a nonlinear ARCH term, the size of which is controlled with the special
matrix norm deﬁned in Assumption 4. Regarding the conditional expectation, the mentioned
earlier results rely on Lemma 3 in Fort and Moulines (2003) while our proof of Theorem 1 avoids
the use of this lemma, and instead makes use of a (relatively unknown) extension of Bernoulli’s
inequality due to Feﬀerman and Shapiro (1972) (combined with Young’s inequality).

Theorem 1 concerns only polynomial ergodicity of subgeometric AR–ARCH models, and
does not consider subexponential ergodicity (where the rate r(n) in (2) equals, say, ecnγ with
In the homoskedastic case, the
c > 0 and 0 < γ < 1). There are good reasons for this.
previous results of Douc et al. (2004, Sec 3.3) and Meitz and Saikkonen (2020, Thm 2) on
subexponential ergodicity of nonlinear autoregressions imply that the observed process yt has
ﬁnite moments of all orders. This is in stark contrast to ARCH-type models. For instance, in
the simplest ARCH(1) model (yt = σtεt, σ2
t−1, and εt IID N(0,1)), the ﬁniteness
of moments of order 2r for the observed process (E[|yt|2r] < ∞) is known to require the
1E[|εt|2r] < 1 (see, e.g., Ling and McAleer, 2002, Thm 2.1 and Ling, 1999, Example
condition αr
6.1). For large values of r this restricts the parameter α1 in a way that is not realistic in
empirical applications of ARCH-type models. Similar problems occur also in more complicated
(G)ARCH and AR–(G)ARCH models (see, e.g., Meitz and Saikkonen, 2008a, Thm 2 and Meitz
and Saikkonen, 2008b, Thm 1, respectively). In the polynomial ergodicity result of Theorem 1
above only the mild condition α1 < 1 (when q = s0 = 1) is needed to obtain moments of order
2 − ρ; a comparable subexponential ergodicity result might place unreasonable restrictions on
α1, and is therefore not pursued in this paper.

t = ω + α1y2

The conditional mean of the model we have so far discussed is very general, and we next
consider some concrete illustrating examples. The following two special cases were introduced

11

in Meitz and Saikkonen (2020, Sec 5) in the case of a homoskedastic error term. We ﬁrst
consider a model with a time-varying intercept term based on a logistic function and speciﬁed
as

yt = ν1L(ut−1; γ, a1) + ν2(1 − L(ut−1; γ, a2)) + yt−1 + πt−1∆yt−1 + · · · + πp−1∆yt−p+1 + σtεt, (24)

where L(u; γ, a) = 1/(1 + e−γ(u−a)) is the logistic function and the parameters γ, a1, a2 are
assumed to satisfy γ > 0 and a1 ≤ a2, and ν1, ν2 are assumed to satisfy ν1 < 0 < ν2. Moreover,
∆ signiﬁes the diﬀerence operator (so that ∆yt−1 = yt−1 − yt−2) and the remaining notation is
as in model (3). Arguments similar to those in Meitz and Saikkonen (2020, proof of Proposition
1) can now be used to prove the following result (for details, see the Appendix).

Proposition 1. Consider the process yt deﬁned in equation (24) and suppose that Assumptions
1,2(i), 3, and 4 hold. Then, yt is polynomially ergodic with convergence rate r(n) = n2s0−1 and
ﬁnite moments up to order 2s0 − 1.

The convergence rate presented in Proposition 1 also shows the rate of β-mixing coeﬃcients.
As another special case, we consider a model with a time-varying slope term deﬁned as

yt = πt−1yt−1 + · · · + πp−1yt−p+1 + S(ut−1)ut−1 + σtεt,

(25)

where S(ut−1) is either S1(ut−1) = 1−r0/h(ut−1) or S2(ut−1) = exp{−r0/h(ut−1)} (with r0 > 0)
and the function h : R → (0, ∞) as deﬁned in Proposition 2 of Meitz and Saikkonen (2020,
Sec 5.2). In addition to a general formulation of the function h that proposition provides six
special cases of which two are h(u) = 1 + |u − a|ρ and h(u) = (1 + (u − a)2)ρ/2 (where a ∈ R
and ρ ∈ (0, 2); see Assumption 2). Regarding the remaining notation, it is as in model (3).

The following result can be established by using arguments similar to those in the proof of

Proposition 2 in Meitz and Saikkonen (2020, Sec 5.2) (for details, see the Appendix).

Proposition 2. Consider the process yt deﬁned in equation (25) and suppose that Assumptions
1,2(i), 3, and 4 hold. Then, yt is polynomially ergodic with convergence rate r(n) = n2s0/ρ−1
and ﬁnite moments up to order 2s0 − ρ.

The rate of β-mixing coeﬃcients coincides with the rate given in the proposition. As the
function h depends on the parameter ρ ∈ (0, 2), the convergence rate in Proposition 2 diﬀers
from that obtained in Proposition 1 except in the case ρ = 1.

4 Empirical application

We next illustrate the use of subgeometrically ergodic AR–ARCH models in a small empirical
example. The data we employ consists of daily observations on the Chicago Board Options
Exchange energy sector volatility index (fred.stlouisfed.org/series/VXXLECLS) over the
period 16 March 2011 through 31 December 2021 (a total of 2719 observations). This data
series reﬂects energy sector risk and is displayed in the top left graph of Figure 1 (the solid
graph; the dashed horizontal line shows the estimate ˆa = 25.366, see (26) and (27) below). The
time series plot shows signs of strong persistence, which is also reﬂected in the autocorrelation
function of the data shown in the top right graph of Figure 1.

12

Figure 1: Top row: Daily observations on the Chicago Board Options Exchange energy sector
volatility index, 16 March 2011 – 31 December 2021 (left); the corresponding autocorrelation
function (right). Middle row: The function I(x) = −νL(x; γ, a) + ν(1 − L(x; γ, a)) (left) and
the corresponding time-varying intercept term I(yt−1) (right), based on parameter estimates in
(27). Bottom row: The estimated volatility series ˆσt (left) and residual series ˆεt (right), based
on parameter estimates in (27).

We model this data series using the parametric speciﬁcation in (24). As for the error
distribution, after some experimentation a skew version of the t-distribution due to Jones and
Faddy (2003) was found convenient. The density function of this distribution is

f (x; c, d) = C −1
c,d

(cid:26)

1 +

x
(c + d + x2)1/2

(cid:27)c+1/2(cid:26)

1 −

x
(c + d + x2)1/2

(cid:27)d+1/2

,

where c and d are positive parameters and Cc,d = 2c+d−1B(c, d)(c + d)1/2 (with B(·, ·) denoting
the beta function); the case c = d results in a symmetric t-distribution with 2c degrees of
freedom and the cases c < d and c > d imply skewness to the left and right, respectively. In our
application, we use this distribution centralized to have mean zero and standardized to have
unit variance (i.e., in the density function x is replaced by sx + m and C −1
c,d where m
and s2 denote the mean and variance, see Jones and Faddy, 2003, Sec 2.1; this requires that
c > 1 and d > 1, for a moment of order k is ﬁnite when c > k/2 and d > k/2).

c,d by sC −1

13

05001000150020002500204060801200501001502002500.00.20.40.60.81.0020406080100120140−0.2−0.10.00.105001000150020002500−0.20−0.100.000.10050010001500200025000510152005001000150020002500−4−20246We estimate the model parameters using the method of maximum likelihood and employ
optimization routines in R. (We simply assume that standard properties of maximum likeli-
hood estimators hold and calculate standard errors based on the standard formulas.) Some
experimentation leads to model (24) with order p = 1 and with a nonlinear ARCH term of
order q = 3. Speciﬁcally, the considered model is

yt = yt−1 − νL(yt−1; γ, a) + ν(1 − L(yt−1; γ, a)) + σtεt
t−2 + α3e2
t = (ω + α1e2
σ2

t−3)L(yt−1; γ, a),

t−1 + α2e2

(26)

where the errors εt are IID(0, 1) and follow the above described (centralized and standardized)
skew t-distribution, L(y; γ, a) = 1/(1 + e−γ(y−a)) is the logistic function, the parameters ν and γ
are positive, and a ∈ R. (We also tried a model where the logistic functions in the conditional
expectation and in the ARCH term were diﬀerent but this extension had only a minor eﬀect
on the results.) ML estimation leads to the following results:

yt = yt−1 − 0.187
(0.040)

L(yt−1; 0.171
(0.018)

ˆσ2
t = (3.259
(0.493)

+ 0.406
(0.081)

e2
t−1 + 0.310
(0.066)

, 25.366
(1.434)
e2
t−2 + 0.149
(0.052)

) + 0.187
(0.040)
e2
t−3)L(yt−1; 0.171
(0.018)

(1 − L(yt−1; 0.171
(0.018)

, 25.366
(1.434)

),

, 25.366
(1.434)

)) + ˆσt ˆεt

(27)

where the numbers in parenthesis are standard errors; estimates for the parameters in the error
distribution are ˆc = 3.551 (0.422) and ˆd = 2.138 (0.197).

To illustrate the conditional mean of the estimated model, consider the function I(x) =
−νL(x; γ, a)+ν(1−L(x; γ, a)) and the corresponding time-varying intercept term I(yt−1) based
on the above parameter estimates. These are shown in the middle row of Figure 1. In the left
panel, the two horizontal dashed lines show the minimum and maximum I(x) attains, while
the three vertical dashed lines indicate the minimum of the observed data series yt (11.71),
the estimate ˆa = 25.366, and the maximum of yt (130.61). On the right, the three horizontal
dashed lines show the minimum and maximum I(yt−1) attains (−0.187 and 0.154) and the
origin. Intuitively, when yt−1 is close to ˆa the time-varying intercept term I(yt−1) is close to
zero and the conditional mean of (27) corresponds to unit root type behavior (without drift);
when yt−1 takes values clearly below/above ˆa the intercept I(yt−1) is positive/negative and
behavior akin to a unit root process with increasing/decreasing drift occurs.

The left panel in the bottom row of Figure 1 displays the estimated volatility series ˆσt. The
variation of the volatility over time is strong, and the large spikes in the volatility series coincide
with the large values in the observed series. The logistic formulation of the conditional variance
in (27) makes it possible for large observations to amplify volatility more than a standard linear
ARCH model would allow for.

The right panel in the bottom row of Figure 1 shows the residual series ˆεt. Four additional
graphs analyzing the residuals are available in Figure 2 in Appendix B: autocorrelation functions
of the residuals and of the squared residuals, together with a histogram and a Q-Q plot. The
autocorrelation functions reveal that the very strong persistence present in the original series has
been quite well captured by the estimated subgeometrically ergodic AR–ARCH model (only
three of the shown 100 autocorrelation coeﬃcients are barely outside the displayed critical
values). The histogram and the Q-Q plot indicate that the employed skew version of the t-
distribution ﬁts well as only a few outlying observations deviate from the estimated density

14

function and the 45 degree line.

Note also that the estimated AR–ARCH model satisﬁes the requirements of a stationary
and (subgeometrically) ergodic process. It may be interesting to note that estimation attempts
using standard linear ARMA(1,1)–GARCH(1,1) models (with skew t errors) lead to estimated
autoregressive coeﬃcients in excess of 0.999, reﬂecting the very persistent nature of the data
series apparent from the time series and autocorrelation plots in the top row of Figure 1.

The primary purpose of this small empirical example was to demonstrate what kind of time
series could be modeled with subgeometrically ergodic AR–ARCH models. It is worth pointing
out that such models may work well even in cases where the graphs of the employed time series
and related autocorrelation functions look very diﬀerent from those displayed in Figure 1.

5 Conclusions

In this paper, we examined the subgeometric ergodicity of nonlinear autoregressive models with
autoregressive conditional heteroskedasticity. We provided conditions that ensured polynomial
ergodicity of the considered AR–ARCH models. Our results generalized existing results that
assumed the error terms to be IID. The use of subgeometrically ergodic AR–ARCH models was
illustrated in an empirical example using energy sector volatility index data.

Several future research topics could be entertained. In this paper we have only considered
ARCH-type conditional heteroskedasticity, and extending the results to the generalized ARCH
(GARCH) case would be of interest. Subgeometric ergodicity of multivariate autoregressions
with autoregressive conditional heteroskedasticity is another interesting topic left for future
work.

15

Appendix A

Appendix A contains the proofs of Lemma 2, Theorem 1, and Propositions 1 and 2.

Proof of Lemma 2. Deﬁne the vector (¯α1, . . . , ¯αq) = (α1 ¯µ2s0b, . . . , αq ¯µ2s0b) and let ¯Λ denote
the q × q matrix obtained by replacing the ﬁrst row of the matrix Λt by (¯α1, . . . , ¯αq). By
assumption, ¯α1, . . . , ¯αq ≥ 0 and (cid:80)q
i=1 ¯αi < 1. These conditions ensure that the polynomial
p(t) = tq − ¯α1tq−1 − · · · − ¯αq has all its roots inside the unit circle (if a root t with |t| ≥ 1
existed, the contradiction 1 = ¯α1/t+· · ·+ ¯αq/tq ≤ ¯α1+· · ·+ ¯αq would follow); this in turn implies
that the matrix ¯Λ has spectral radius ρ(¯Λ) < 1 (see Horn and Johnson, 2013, pp. 194–195).
Therefore the matrix Iq − ¯Λ is invertible with (Iq − ¯Λ)−1 = (cid:80)∞
¯Λi. Set 1q = (1, . . . , 1)
(q × 1) and let (x)abs = (|x1|, . . . , |xq|) (q × 1) denote the elementwise absolute value of a vector
q(Iq − ¯Λ)−1(x)abs. Note that
x ∈ Rq. We deﬁne the vector norm (cid:107) · (cid:107)• on Rq as (cid:107)x(cid:107)• = 1(cid:48)
as (Iq − ¯Λ)−1 = (cid:80)∞
¯Λi with ¯Λ having nonnegative (and also some strictly positive) entries,
(cid:107)x(cid:107)• = 1(cid:48)
q(x)abs = (cid:107)x(cid:107)1 whenever x (cid:54)= 0 (here (cid:107) · (cid:107)1 denotes the usual l1
vector norm).

q(Iq − ¯Λ)−1(x)abs > 1(cid:48)

i=0

i=0

Now let x (cid:54)= 0 be arbitrary and consider (cid:107)Λtx(cid:107)•. To this end, note that |α1ε2
t xq| ≤ α1ε2

t x1 + · · · +
t |xq|, which implies that the elementwise inequality (Λtx)abs ≤

αqε2
Λt(x)abs holds (with probability one; note that only the ﬁrst elements diﬀer). Thus also

t |x1| + · · · + αqε2

(cid:107)Λtx(cid:107)• = 1(cid:48)

q(Iq − ¯Λ)−1(Λtx)abs ≤ 1(cid:48)

q(Iq − ¯Λ)−1Λt(x)abs.

As bs0 ≥ 1, Minkowski’s inequality and the deﬁnition of the vector (¯α1, . . . , ¯αq) yield

E[{1(cid:48)

q(Iq − ¯Λ)−1Λt(x)abs}bs0]1/bs0 ≤ 1(cid:48)

q(Iq − ¯Λ)−1 ¯Λ(x)abs,

where, as (Iq − ¯Λ)−1 ¯Λ = (Iq − ¯Λ)−1 − Iq and (cid:107)x(cid:107)• > (cid:107)x(cid:107)1,

q(Iq − ¯Λ)−1 ¯Λ(x)abs = (cid:107)x(cid:107)• − (cid:107)x(cid:107)1 = (cid:107)x(cid:107)•(1 − (cid:107)x(cid:107)1/(cid:107)x(cid:107)•) < (cid:107)x(cid:107)•.
1(cid:48)

These derivations establish that

(cid:107)Λtx(cid:107)•Lbs0 = (E[(cid:107)Λtx(cid:107)bs0

• ])1/bs0 < (cid:107)x(cid:107)•

and that

|||Λt|||•Lbs0 = max

(cid:107)x(cid:107)

•Lbs0 =1

(cid:107)Λtx(cid:107)•Lbs0 = max
(cid:107)x(cid:107)•=1

(cid:107)Λtx(cid:107)•Lbs0 < 1.

Finally, by its deﬁnition, it is clear that the vector norm (cid:107) · (cid:107)• is monotone.

(cid:4)

Proof of Theorem 1: For clarity, we break down the long proof into several intermediate
steps.

Step 1: Preliminaries. We ﬁrst consider the function V deﬁned in (22) and the conditional
expectation E[V (y1) | y0 = x]. As before, we decompose an x ∈ Rp+q to its p- and q-
dimensional components as x = (x1, x2); similarly, we decompose y1 as y1 = (y1,1, y2,1). For

16

any x ∈ Rp+q , it is convenient to deﬁne

V1(x1) = |z1(x1)|2s0,

V2(x1) = s1(cid:107)z2(x1)(cid:107)2s0α

∗

,

and

V3(x) = s2(cid:107)ξ(x)(cid:107)bs0
•

so that V (x) = 1 + V1(x1) + V2(x1) + V3(x) (when p = 1, we can set s1 = 0 and drop z2(x1)
and V2). We next consider the three conditional expectations

E[V1(y1,1) | y0 = x] = E[|z1(y1,1)|2s0 | y0 = x]
E[V2(y1,1) | y0 = x] = E[s1(cid:107)z2(y1,1)(cid:107)2s0α
E[V3(y1) | y0 = x] = E[s2(cid:107)ξ(y1)(cid:107)bs0
•

∗
| y0 = x]

| y0 = x]

(28)

(29)

(30)

related to functions V1, V2, and V3.
expectations can be bounded from above using the following upper bounds

In Steps 2–4 below we establish that these conditional

E[|z1(y1,1)|2s0 | y0 = x] ≤ |z1(x1)|2s0 − ˜r|z1(x1)|2s0α + C(cid:107)ξ(x)(cid:107)bs0

E[s1(cid:107)z2(y1,1)(cid:107)2s0α
E[s2(cid:107)ξ(y1)(cid:107)bs0
•

∗

| y0 = x] ≤ s1 (cid:107)z2(x1)(cid:107)2s0α
| y0 = x] ≤ s2 (cid:107)ξ(x)(cid:107)bs0

∗ − ˜(cid:36)sα

• − ˜λs2 (cid:107)ξ(x)(cid:107)bs0

1 (cid:107)z2(x1)(cid:107)2s0α2
∗
• + C,

• + C
(31)
+ ˜s1 |z1(x1)|2s0α + C (32)
(33)

where ˜r, ˜(cid:36), ˜s1, ˜λ > 0 with ˜λ < 1 and where ˜s1 can be made as close to zero as desired by
choosing a small enough s1 (and ˜s1 = 0 when p = 1). Moreover, here and in what follows, for
simplicity we use C to denote a ﬁnite positive constant whose value may change from occurence
to occurence (alternatively, we could use C1, C2, . . .). For brevity, we also often (but not always)
drop the argument x1 from z1(x1) and z2(x1) and simply write z1 and z2.

For ease of reference, we also note here that Assumption 4 allows us to bound the conditional
variance as follows. By the deﬁnition of σ2
t in (13), Assumption 3, and deﬁnition of ξ(yt−1) in
t−1 + · · · + e2
(14), σ2
t−q = ω + (cid:107)ξ(yt−1)(cid:107)1 (with (cid:107) · (cid:107)1 denoting the usual l1 vector norm).
The equivalence of vector norms on Rq and the fact that ω is a (ﬁnite) constant implies that
(for some ﬁnite constant C)

t ≤ ω + e2

t = σ2(yt−1) ≤ C(1 + (cid:107)ξ(yt−1)(cid:107)•) a.s.
σ2

and σ2(x) ≤ C(1 + (cid:107)ξ(x)(cid:107)•) for all ﬁxed x. (34)

Step 2: Upper bound for V1. Using (12) the conditional expectation in (28) can be ex-
pressed as

E[V1(y1,1) | y0 = x] = E[|g(z1(x1)) + σ(x)ε1|2s0].
For any positive real number Z, deﬁne the set S1(Z) = {x ∈ Rp+q : |z1(x1)| ≤ Z} and let
Sc
1(Z) denote the complement of this set.

First consider values of x such that x ∈ Sc

1(Z) so that |z1| = |z1(x1)| > Z. Choose Z large
enough to ensure that g(z1) (cid:54)= 0 (Assumption 2) so that |g(z1) + σ(x)ε1|2s0 can be written as

|g(z1)|2s0 |1 + σ(x)ε1/g(z1)|2s0.

(35)

We ﬁrst bound the latter term in this expression using the following extension of Bernoulli’s
inequality due to Feﬀerman and Shapiro (1972): for any a ≥ 2, there exists positive numbers

17

A and B such that

|1 + u|a ≤ 1 + au + Au2 + B|u|a

(36)

for all u ∈ R. Using (36), the latter term in (35) is dominated by

1 + 2s0

σ(x)
g(z1)

ε1 + A

σ(x)2
g(z1)2 ε2

1 + B

σ(x)2s0
|g(z1)|2s0

|ε1|2s0.

This upper bound, (35), the facts E[ε1] = 0 and E[ε2
µ2s0 = E[|ε1|2s0], now yield

1] = 1 (Assumption 1), and the notation

E[V1(y1,1) | y0 = x] ≤ |g(z1)|2s0 + A|g(z1)|2s0−2σ(x)2 + Bσ(x)2s0µ2s0.

Choose Z large enough to ensure that 0 < 1 − r|z1|−ρ < 1 and |g(z1)| ≤ (1 − r|z1|−ρ)|z1|
(Assumption 2). Using the elementary inequalities (1 − u)a1 ≤ 1 − u and (1 − u)a2 ≤ 1 for all
0 < u < 1, a1 ≥ 1, and a2 ≥ 0, and recalling that s0 ≥ 1 and σ2(x) ≤ C (1 + (cid:107)ξ(x)(cid:107)•) (see
(34)), we obtain

E[V1(y1,1) | y0 = x] ≤ |z1|2s0 − r|z1|2s0−ρ + C|z1|2s0−2 + C|z1|2s0−2(cid:107)ξ(x)(cid:107)• + C(cid:107)ξ(x)(cid:107)s0

• µ2s0

for some positive C (by choosing Z large enough, the constant term on the dominant side has
been absorbed into |z1|2s0). To merge the terms −r|z1|2s0−ρ and C|z1|2s0−2, by choosing Z large
enough to ensure that C/r|z1|2−ρ < 1 we have

−r|z1|2s0−ρ + C|z1|2s0−2 = −r|z1|2s0−ρ(1 − C/r|z1|2−ρ) ≤ −ˆr|z1|2s0−ρ

for some positive constant ˆr. Hence,

E[V1(y1,1) | y0 = x] ≤ |z1|2s0−ˆr|z1|2s0−ρ+C|z1|2s0−2(cid:107)ξ(x)(cid:107)•+C(cid:107)ξ(x)(cid:107)s0

1(Z).
(37)
Now consider values of x such that x ∈ S1(Z). As inequality (5) implies that g(z1) is

• µ2s0 for all x ∈ Sc

bounded on S1(Z), triangle inequality and the elementary inequality

(cid:12)
(cid:12)
(cid:12)
(cid:12)

m
(cid:88)

i=i

r

(cid:12)
(cid:12)
(cid:12)
(cid:12)

ai

≤ cr

m
(cid:88)

i=i

|ai|r where cr = 1 for 0 < r ≤ 1 and cr = mr−1 for r > 1

(38)

for any real numbers a1, . . . , am (see, e.g., Davidson, 1994, p. 140) imply that |g(z1) + σ(x)ε1|2s0
is dominated by C(1 + σ2s0(x)|ε1|2s0) (for some C > 0; we omit this statement from now on)
for all x ∈ S1(Z). As µ2s0 = E[|ε1|2s0] and σ2(x) ≤ C (1 + (cid:107)ξ(x)(cid:107)•), it is seen that

E[V1(y1,1) | y0 = x] ≤ C(1 + (cid:107)ξ(x)(cid:107)s0

• µ2s0)

for all x ∈ S1(Z).

(39)

Combining (37) and (39), noting that 2s0 − ρ = 2s0α (see the discussion following (22)),

and merging constants, we can conclude that for all x ∈ Rp+q,

E[V1(y1,1) | y0 = x] ≤ |z1|2s0 − ˆr|z1|2s0α + C|z1|2s0−2(cid:107)ξ(x)(cid:107)• + C(cid:107)ξ(x)(cid:107)s0

• + C.

(40)

18

For future developments, it is convenient to further manipulate this upper bound. First, con-
sider the product |z1|2s0−2(cid:107)ξ(x)(cid:107)• appearing in (40) and momentarily focus on the case s0 > 1
(when also b > 1). Using Young’s inequality (with exponents bs0/(bs0 − 1) and bs0) yields

|z1|2s0−2(cid:107)ξ(x)(cid:107)• ≤

bs0 − 1
bs0

|z1|2s0b(s0−1)/(bs0−1) +

1
bs0

(cid:107)ξ(x)(cid:107)bs0
• .

Simple calculations show that the assumption b > (2s0 − ρ)/[s0(2 − ρ)] implies that 2s0b(s0 −
1)/(bs0 − 1) < 2s0α. Therefore for some small positive ς

|z1|2s0−2(cid:107)ξ(x)(cid:107)• ≤ C(1 + |z1|2s0α−ς + (cid:107)ξ(x)(cid:107)bs0

• );

(41)

clearly this upper bound also holds in the case s0 = 1.

Second, consider the terms involving |z1|2s0α in (40) and |z1|2s0α−ς in (41). By considering

values of |z1| larger and smaller than some large bound, it is straightforward to see that

C|z1|2s0α−ς − ˆr|z1|2s0α = −ˆr(1 − C/[ˆr|z1|ς])|z1|2s0α ≤ C − ˜r|z1|2s0α

for some positive constant ˜r. Third, the term (cid:107)ξ(x)(cid:107)s0
by a term of the form C + (cid:107)ξ(x)(cid:107)bs0
• .

• appearing in (40) is clearly dominated

Inequality (40) together with these additional manipulations leads to the ﬁnal upper bound

E[V1(y1,1) | y0 = x] ≤ |z1|2s0 − ˜r|z1|2s0α + C(cid:107)ξ(x)(cid:107)bs0

• + C

(42)

which holds for all x ∈ Rp+q.

Step 3: Upper bound for V2. Using (12), we can express the conditional expectation in
(29) as

E[V2(y1,1) | y0 = x] = s1(cid:107)Π1z2(x1) + z1(x1)ιp−1(cid:107)2s0α

∗

.

Recall that α = 1 − ρ/2s0 ∈ (0, 1) (because s0 ≥ 1 and ρ ∈ (0, 2) by assumption) and that
|||Π1|||∗ ≤ (cid:36) for some 0 < (cid:36) < 1 (by Lemma 1). These facts together with elementary
inequalities (and dropping the argument x1 from z1(x1) and z2(x1)) imply that

(cid:107)Π1z2 + z1ιp−1(cid:107)α

∗ ≤ (cid:107)Π1z2(cid:107)α

∗ + (cid:107)z1ιp−1(cid:107)α

∗ ≤ (cid:36)α (cid:107)z2(cid:107)α

∗ + (cid:107)ιp−1(cid:107)α

∗ |z1|α .

This, together with the convexity of the function |x| (cid:55)→ |x|2s0 (recall that s0 ≥ 1 by assumption),
imply that for any τ1 ∈ (0, 1) and τ2 = 1 − τ1,

s1 (cid:107)Π1z2 + z1ιp−1(cid:107)2s0α

∗ ≤

τ2

(cid:32)

s1/2s0
1 (cid:36)α
τ2

(cid:107)z2(cid:107)α

∗ + τ1

≤ τ2

s1(cid:36)2s0α
τ 2s0
2

(cid:107)z2(cid:107)2s0α

∗ + τ1

s1/2s0
1

(cid:107)ιp−1(cid:107)α
∗
τ1
s1 (cid:107)ιp−1(cid:107)2s0α
τ 2s0
1

∗

(cid:33)2s0

|z1|α

|z1|2s0α .

(43)

Consider the former term on the dominant side of (43). Fix a τ2 such that τ2 ∈ ((cid:36)α, 1) and

19

set ˜(cid:36) = 1 − ((cid:36)α/τ2)2s0 ∈ (0, 1). Then the former term on the dominant side of (43) satisﬁes

τ2

s1(cid:36)2s0α
τ 2s0
2

(cid:107)z2(cid:107)2s0α

∗ = τ2s1(1 − ˜(cid:36)) (cid:107)z2(cid:107)2s0α

∗ < s1 (cid:107)z2(cid:107)2s0α

∗ − ˜(cid:36)s1 (cid:107)z2(cid:107)2s0α

∗

.

(44)

Suppose now that s1 is any ﬁxed (but potentially arbitrarily small) positive number. If (cid:107)z2(cid:107)∗
1 (cid:107)z2(cid:107)2s0α2
is large enough to ensure that s1 (cid:107)z2(cid:107)2s0α
as α ∈ (0, 1)
and the right side of (44) is dominated by s1 (cid:107)z2(cid:107)2s0α
. On the other hand, if
s1 (cid:107)z2(cid:107)2s0α

∗ < 1 the right side of (44) is bounded by a constant. Therefore

≥ sα
1 (cid:107)z2(cid:107)2s0α2

≥ 1, then s1 (cid:107)z2(cid:107)2s0α

∗ − ˜(cid:36)sα

∗

∗

∗

∗

τ2

s1(cid:36)2s0α
τ 2s0
2

(cid:107)z2(cid:107)2s0α

∗ < s1 (cid:107)z2(cid:107)2s0α

∗ − ˜(cid:36)sα

1 (cid:107)z2(cid:107)2s0α2

∗

+ C.

Now consider the latter term on the dominant side of (43). Choosing a small enough ﬁxed
s1, this term can be made smaller than ˜s1 |z1|2s0α where ˜s1 can be chosen as close to zero as
desired. To summarize, it holds that

E[V2(y1,1) | y0 = x] ≤ s1 (cid:107)z2(cid:107)2s0α

∗ − ˜(cid:36)sα

1 (cid:107)z2(cid:107)2s0α2

∗

+ ˜s1 |z1|2s0α + C

(45)

where ˜(cid:36) ∈ (0, 1) and the value of ˜s1 > 0 can be chosen as close to zero as desired.

Step 4: Upper bound for V3. By the deﬁnition of the function ξ in (14), ξ(y1) =
Λζ,1(y0)ξ(y0) + ωζ,1. We start by bounding both terms on the right hand side of this equality
and, for simplicity, remove the argument y0 and instead use the notations Λζ,1 and ξ0.

First denote vζ,1 = ε2

1−q). Using
the deﬁnitions of the matrices Λζ,1 and Λ1 and the vector ξ0 (see (14) and (16)) we then have

0 + · · · + αqζq,0e2

1−q) and v1 = ε2

0 + · · · + αqe2

1(α1ζ1,0e2

1(α1e2

Λζ,1ξ0 = (vζ,1, e2

0, . . . , e2

1−q) and Λ1ξ0 = (v1, e2

0, . . . , e2

1−q),

where all components of both vectors are nonnegative. As ζi,0 ∈ (0, 1] for all i = 1, . . . , q by as-
sumption, we have vζ,1 ≤ v1 (a.s.). The monotonicity of the norm (cid:107) · (cid:107)• required in Assumption
4 now implies that (cid:107)Λζ,1ξ0(cid:107)• ≤ (cid:107)Λ1ξ0(cid:107)• (a.s.) (see the discussion preceding Assumption 4).
Regarding the vector ωζ,1, its ﬁrst component is ζ0,0(y0)ε2
1ω (a.s.) and the other compo-
nents are zero, so that the monotonicity of the norm (cid:107)·(cid:107)• shows that (cid:107)ωζ,1(cid:107)• ≤ (cid:107)ω1(cid:107)• = ε2
1(cid:107)ω(cid:107)•
(a.s.) where ω = (ω, 0, . . . , 0).

1ω ≤ ε2

The preceding discussion together with the triangle inequality now yields, with probability

one,

(cid:107)ξ(y1)(cid:107)• = (cid:107)Λζ,1(y0)ξ(y0) + ωζ,1(cid:107)• ≤ (cid:107)Λζ,1(y0)ξ(y0)(cid:107)• + (cid:107)ωζ,1(cid:107)• ≤ (cid:107)Λ1ξ(y0)(cid:107)• + ε2

1(cid:107)ω(cid:107)•.

Using the notation ¯µ2bs0 = (E[|ε0|2bs0])1/bs0 and Minkowski’s inequality we ﬁnd that

(cid:0)E[(cid:107)ξ(y1)(cid:107)bs0

•

| y0 = x](cid:1)1/bs0 ≤ (cid:0)E[(cid:107)Λ1ξ(x)(cid:107)bs0

• ](cid:1)1/bs0 + ¯µ2bs0(cid:107)ω(cid:107)•.

20

By inequality (18) and Assumption 4, the ﬁrst term on the dominant side satisﬁes

(cid:0)E[(cid:107)Λ1ξ(x)(cid:107)bs0

• ](cid:1)1/bs0 = (cid:107)Λ1ξ(x)(cid:107)•Lbs0 ≤ |||Λ1|||•Lbs0 (cid:107)ξ(x)(cid:107)• = λ(cid:107)ξ(x)(cid:107)•

with λ < 1. The preceding steps imply that

(cid:0)E[(cid:107)ξ(y1)(cid:107)bs0

•

| y0 = x](cid:1)1/bs0 ≤ λ(cid:107)ξ(x)(cid:107)• + ¯µ2bs0(cid:107)ω(cid:107)• = λ(cid:107)ξ(x)(cid:107)•

(cid:18)

1 +

(cid:19)

¯µ2bs0(cid:107)ω(cid:107)•
λ(cid:107)ξ(x)(cid:107)•

and, using (30) and the notation ˆλ = λbs0 < 1, we obtain

E[V3(y1) | y0 = x] ≤ s2

ˆλ(cid:107)ξ(x)(cid:107)bs0
•

(cid:18)

1 +

(cid:40)

(cid:19)bs0

¯µ2bs0(cid:107)ω(cid:107)•
λ(cid:107)ξ(x)(cid:107)•
(cid:18)

= s2(cid:107)ξ(x)(cid:107)bs0

• −

1 − ˆλ

1 +

¯µ2bs0(cid:107)ω(cid:107)•
λ(cid:107)ξ(x)(cid:107)•

(cid:19)bs0(cid:41)

s2 (cid:107)ξ(x)(cid:107)bs0
•

.

As ˆλ ∈ (0, 1), we can choose a ξ > 0 such that the term in curly brackets is larger than some
˜λ ∈ (0, 1) whenever (cid:107)ξ(x)(cid:107)• > ξ. Therefore, whenever (cid:107)ξ(x)(cid:107)• > ξ, we have

E[V3(y1) | y0 = x] ≤ s2 (cid:107)ξ(x)(cid:107)bs0

• − ˜λs2 (cid:107)ξ(x)(cid:107)bs0

•

.

On the other hand, whenever (cid:107)ξ(x)(cid:107)• ≤ ξ the previous derivations also make it clear that
E[V3(y1) | y0 = x] is bounded by some constant. Therefore for all x ∈ Rp+q,

E[V3(y1) | y0 = x] ≤ s2 (cid:107)ξ(x)(cid:107)bs0

• − ˜λs2 (cid:107)ξ(x)(cid:107)bs0

• + C.

(46)

Step 5: Upper bound for V . We next combine the upper bounds (42), (45), and (46)
derived in Steps 2–4 to obtain

E[V (y1) | y0 = x] ≤ 1 + |z1|2s0 − ˜r|z1|2s0α + C(cid:107)ξ(x)(cid:107)bs0
•
+ ˜s1 |z1|2s0α

+ s1 (cid:107)z2(cid:107)2s0α
+ s2(cid:107)ξ(x)(cid:107)bs0

∗ − ˜(cid:36)sα

1 (cid:107)z2(cid:107)2s0α2
• − ˜λs2(cid:107)ξ(x)(cid:107)bs0

∗

• + C.

(47)

To combine the terms involving |z1|2s0α, set ¯r = ˜r − ˜s1 so that

−˜r|z1|2s0α + ˜s1|z1|2s0α = −¯r|z1|2s0α;

recalling that ˜s1 can be chosen as close to zero as desired, we have ¯r > 0 by a suitable choice
of ˜s1. On the other hand, to manipulate the terms involving (cid:107)ξ(x)(cid:107)bs0
• , choose s2 large enough
to ensure that ˜λ − C/s2 ∈ (0, 1) and set ¯λ = ˜λ − C/s2. As now −¯λs2 = C − ˜λs2, the terms
involving (cid:107)ξ(x)(cid:107)bs0

in (47) can be written as

•

s2(cid:107)ξ(x)(cid:107)bs0

• + C(cid:107)ξ(x)(cid:107)bs0

• − ˜λs2(cid:107)ξ(x)(cid:107)bs0

• = s2(cid:107)ξ(x)(cid:107)bs0

• − ¯λs2(cid:107)ξ(x)(cid:107)bs0
• .

(48)

Whenever s2(cid:107)ξ(x)(cid:107)bs0
s2(cid:107)ξ(x)(cid:107)bs0

• ≥ sα

2 (cid:107)ξ(x)(cid:107)bs0α

•

• < 1, (48) is bounded by the constant 1; for s2(cid:107)ξ(x)(cid:107)bs0

• ≥ 1, we have
as α ∈ (0, 1). Thus the expression in (48) is always bounded by

21

1 + s2(cid:107)ξ(x)(cid:107)bs0
from (47) that

• − ¯λsα

2 (cid:107)ξ(x)(cid:107)bs0α

•

. As V (x) = 1 + |z1|2s0 + s1 (cid:107)z2(cid:107)2s0α

∗ + s2(cid:107)ξ(x)(cid:107)bs0

• , we obtain

E[V (y1) | y0 = x] ≤ V (x) − (1 + ¯r|z1|2s0α + ˜(cid:36)sα

1 (cid:107)z2(cid:107)2s0α2

∗

+ ¯λsα

2 (cid:107)ξ(x)(cid:107)bs0α

•

) + C.

The inequality in (38) implies that

[V (x)]α = (1 + |z1|2s0 + s1 (cid:107)z2(cid:107)2s0α

∗ + s2(cid:107)ξ(x)(cid:107)bs0

• )α ≤ 1 + |z1|2s0α + sα

1 (cid:107)z2(cid:107)2s0α2

∗

+ sα

2 (cid:107)ξ(x)(cid:107)bs0α

•

so that, setting e = min{¯r, ˜(cid:36), ¯λ} ∈ (0, 1), we have

e[V (x)]α ≤ 1 + ¯r|z1|2s0α + ˜(cid:36)sα

1 (cid:107)z2(cid:107)2s0α2

∗

+ ¯λsα

2 (cid:107)ξ(x)(cid:107)bs0α

•

.

Therefore, setting ˜e = e/2,

E[V (y1) | y0 = x] ≤ V (x) − ˜e[V (x)]α + {C − ˜e[V (x)]α} .

Now, deﬁne the set

AN = (cid:8)x ∈ Rp+q : |z1(x1)|2s0 ≤ N,

(cid:107)z2(x1)(cid:107)2s0α

∗ ≤ N,

(cid:107)ξ(x)(cid:107)bs0α

• ≤ N (cid:9),

(49)

N if either |z1(x1)|2s0 > N , (cid:107)z2(x1)(cid:107)2s0α

where N is so large that AN is nonempty (see (4)). The complement of AN is denoted by Ac
N
so that x ∈ Ac
∗ > N or (cid:107)ξ(x)(cid:107)bs0α
• > N . By choosing a
N it holds that C − ˜e[V (x)]α < 0 so that E[V (y1) | y0 = x] ≤
large enough N , for all x ∈ Ac
N . On the other hand, the function V (x) − e[V (x)]α + C is clearly
V (x) − ˜e[V (x)]α for all x ∈ Ac
bounded by some positive constant ˜b on AN . Therefore we can conclude that there exists an
N and a positive constant ˜b such that

E[V (y1) | y0 = x] ≤ V (x) − φ1 (V (x)) + ˜b1AN (x),

(50)

where φ1(v) = ˜evα. This implies that Condition D holds with φ = φ1 and C = AN .

Step 6: Showing that AN is petite. We ﬁrst note that the deﬁnition of a petite set and
other Markov chain concepts we refer to below can be found in Meyn and Tweedie (2009, Chs
4–6). The idea is to establish that the set AN in (49) is petite for any N ≥ 1 so large that AN
is nonempty. To this end, we show below that there exists an MN < ∞ such that

E[(cid:107)yp+q(cid:107)2s0α

1

| y0 = x] < M 2s0α
N ,

sup
x∈AN

(51)

where (cid:107) · (cid:107)1 denotes the usual l1 vector norm. Next note that Theorem 2.2(ii) of Cline and
Pu (1998) along with our Assumption 1 shows that the Markov chain yt is a ψ-irreducible and
aperiodic T-chain (see also Example 2.1 of the aforementioned paper). Therefore the compact
set BN = {x ∈ Rp+q : (cid:107)x(cid:107)1 ≤ MN } is small (see Meyn and Tweedie, 2009, Thms 6.2.5(ii) and

22

5.5.7). Moreover, due to Markov’s inequality,

inf
x∈AN

Pr[yp+q ∈ BN | y0 = x] = 1 − sup
x∈AN
≥ 1 − sup
x∈AN

Pr[(cid:107)yp+q(cid:107)1 ≥ MN | y0 = x]

E[(cid:107)yp+q(cid:107)2s0α

1

| y0 = x]/M 2s0α
N ,

where the last expression is positive due to (51). Proposition 5.2.4(i) of Meyn and Tweedie
(2009) now implies that the set AN is small. Proposition 5.5.3 of the same reference therefore
implies that the set AN is also petite.

To complete the proof of petiteness of AN , it remains to establish (51). First we introduce
some notation. We let ||| · |||1 denote the maximum column sum norm deﬁned for real square
matrices (this norm is induced by the l1 vector norm, see Horn and Johnson, 2013, Sec 5.6).
For brevity, we denote zt = z(y1,t) = Ay1,t and also partition the p-dimensional zt as zt =
(z1,t, z2,t) (see (11)). This allows us to write the companion form (12) as

zt =

(cid:21)

(cid:20)z1,t
z2,t

(cid:20)

=

g(z1,t−1) + σtεt
Π1z2,t−1 + z1,t−1ιp−1

(cid:21)

.

(52)

Finally, we set (cid:126)z1,p+q = (z1,p+q, . . . , z1,1) and (cid:126)z2,p+q = (z2,p+q, . . . , z2,1).

Now consider the norm (cid:107)yp+q(cid:107)1 in (51). Using properties of the norms (cid:107) · (cid:107)1 and ||| · |||1 we

can write

(cid:107)yp+q(cid:107)1 ≤

p+q
(cid:88)

t=1

(cid:107)y1,t(cid:107)1 =

p+q
(cid:88)

(cid:107)A−1zt(cid:107)1 ≤ |||A−1|||1

t=1

p+q
(cid:88)

t=1

(cid:107)zt(cid:107)1 = |||A−1|||1((cid:107)(cid:126)z1,p+q(cid:107)1 + (cid:107)(cid:126)z2,p+q(cid:107)1)

implying that (cid:107)yp+q(cid:107)1 ≤ C((cid:107)(cid:126)z1,p+q(cid:107)1 + (cid:107)(cid:126)z2,p+q(cid:107)1). Adding terms and making use of inequality
(38) and the fact that α ∈ (0, 1) we obtain

(cid:107)yp+q(cid:107)2s0α

1 ≤ C((1 + (cid:107)(cid:126)z1,p+q(cid:107)1)2s0α + (cid:107)(cid:126)z2,p+q(cid:107)2s0α
≤ C((1 + (cid:107)(cid:126)z1,p+q(cid:107)1)2s0 + (cid:107)(cid:126)z2,p+q(cid:107)2s0α
)
≤ C(1 + (cid:107)(cid:126)z1,p+q(cid:107)2s0
).

1 + (cid:107)(cid:126)z2,p+q(cid:107)2s0α

1

1

1

)

(53)

To obtain an upper bound for the term (cid:107)(cid:126)z1,p+q(cid:107)2s0

, consider the equality z1,t = g(z1,t−1)+σtεt
from (52) and note that, by Assumption 2, |g(u)| ≤ K0 for |u| ≤ M0 and |g(u)| ≤ (1 −
r |u|−ρ)|u| ≤ |u| for |u| ≥ M0, so that |g(u)| ≤ K0 + |u| for all u ∈ R (note that Assumption
2 requires M0 to be so large that r |u|−ρ ∈ (0, 1) for |u| ≥ M0). Using these inequalities,
|z1,t| ≤ K0 + |z1,t−1| + σt|εt| (t = 1, . . . , p + q), so that

1

|z1,1| ≤ K0 + |z1,0| + σ1|ε1|,
|z1,2| ≤ 2K0 + |z1,0| + σ1|ε1| + σ2|ε2|,

...

|z1,p+q| ≤ (p + q)K0 + |z1,0| + σ1|ε1| + · · · + σp+q|εp+q|.

23

Thus (cid:107)(cid:126)z1,p+q(cid:107)1 ≤ C (cid:0)1 + |z1,0| + (cid:80)p+q

i=1 σi|εi|(cid:1) and, making use of inequality (38),

(cid:32)

(cid:107)(cid:126)z1,p+q(cid:107)2s0

1 ≤ C

1 + |z1,0|2s0 +

(cid:33)

σ2s0
i

|εi|2s0

.

p+q
(cid:88)

i=1

(54)

Next, to bound the term (cid:107)(cid:126)z2,p+q(cid:107)2s0α

1

, consider z2,t = Π1z2,t−1+z1,t−1ιp−1 (see (52)). Setting

κ = |||Π1|||1 and using the fact (cid:107)ιp−1(cid:107)1 = 1 we obtain

(cid:107)z2,t(cid:107)1 ≤ |||Π1|||1(cid:107)z2,t−1(cid:107)1 + |z1,t−1|(cid:107)ιp−1(cid:107)1 = κ(cid:107)z2,t−1(cid:107)1 + |z1,t−1|

and furthermore

(cid:107)z2,1(cid:107)1 ≤ κ(cid:107)z2,0(cid:107)1 + |z1,0|,
(cid:107)z2,2(cid:107)1 ≤ κ2(cid:107)z2,0(cid:107)1 + κ|z1,0| + |z1,1|,

...

(cid:107)z2,p+q(cid:107)1 ≤ κp+q(cid:107)z2,0(cid:107)1 + κp+q−1|z1,0| + · · · + |z1,p+q−1|.

This implies that

(cid:107)(cid:126)z2,p+q(cid:107)1 ≤ C((cid:107)z2,0(cid:107)1 + 1 + |z1,0| + (cid:107)(cid:126)z1,p+q(cid:107)1).

As the norms (cid:107)·(cid:107)1 and (cid:107)·(cid:107)∗ are equivalent, it holds that (cid:107)z2,0(cid:107)1 ≤ C(cid:107)z2,0(cid:107)∗. Making use of
inequality (38) and the fact that α ∈ (0, 1) we obtain

(cid:107)(cid:126)z2,p+q(cid:107)2s0α

1 ≤ C((cid:107)z2,0(cid:107)2s0α
≤ C((cid:107)z2,0(cid:107)2s0α
≤ C(1 + (cid:107)z2,0(cid:107)2s0α

∗ + (1 + |z1,0| + (cid:107)(cid:126)z1,p+q(cid:107)1)2s0α)
∗ + (1 + |z1,0| + (cid:107)(cid:126)z1,p+q(cid:107)1)2s0)
∗ + |z1,0|2s0 + (cid:107)(cid:126)z1,p+q(cid:107)2s0
1 ).

(55)

Now combine (53) with the upper bounds obtained for (cid:107)(cid:126)z1,p+q(cid:107)2s0
1
and (55), and recall that z1,0 = z1(y1,0) and z2,0 = z2(y1,0), to obtain

and (cid:107)(cid:126)z2,p+q(cid:107)2s0α

1

in (54)

(cid:32)

(cid:107)yp+q(cid:107)2s0α

1 ≤ C

1 + (cid:107)z2(y1,0)(cid:107)2s0α

∗ + |z1(y1,0)|2s0 +

(cid:33)

σ2s0
i

|εi|2s0

.

p+q
(cid:88)

i=1

As µ2s0 = E[|ε1|2s0] is ﬁnite, this implies that

E[(cid:107)yp+q(cid:107)2s0α

1

| y0 = x] ≤ C

1 + (cid:107)z2(x1)(cid:107)2s0α

∗ + |z1(x1)|2s0 + µ2s0

(cid:32)

(cid:33)

E[σ2s0
i

| y0 = x]

. (56)

p+q
(cid:88)

i=1

Next consider the terms in (56) involving conditional expectations of the σ2s0

’s. We ﬁrst
derive an inequality which is similar to inequality (11) in Meitz and Saikkonen (2010). Using
repeated substitution and the equality ξ(yt) = Λζ,tξ(yt−1) + ωζ,t we obtain, for any ﬁxed t ≥ 1,
that

i

t−1
(cid:89)

t−2
(cid:88)

k
(cid:89)

ξ(yt) =

Λζ,t−kξ(y0) + ωζ,t +

Λζ,t−lωζ,t−k−1.

k=0

k=0

l=0

24

Now consider the vector norm (cid:107)·(cid:107)• in Assumption 4. Denote by ||| · |||• the matrix norm induced
by the vector norm (cid:107) · (cid:107)•; that is, for any q × q matrix A, set

|||A|||• = max
(cid:107)x(cid:107)•=1

(cid:107)Ax(cid:107)•

(x ∈ Rq).

(For clarity, note that ||| · |||• above and ||| · |||•Lp deﬁned in (17) coincide for nonrandom matrices
but diﬀer for random ones.) As (cid:107)·(cid:107)• in Assumption 4 is assumed to be monotone, it follows from
Problems 5.6.P41(c) and 5.6.P42 in Horn and Johnson (2013, p. 368) that the induced matrix
norm ||| · |||• is monotone on the positive orthant, meaning that any q × q matrices A and B that
satisfy the (entrywise) inequalities 0 ≤ A ≤ B also satisfy the inequality |||A|||• ≤ |||B|||•. Usual
properties of vector norms and matrix norms in conjunction with inequality (38) therefore yield

(cid:107)ξ(yt)(cid:107)s0

• ≤ C

t−1
(cid:89)

k=0

|||Λζ,t−k|||s0

• (cid:107)ξ(y0)(cid:107)s0

• + C(cid:107)ωζ,t(cid:107)s0

• + C

t−2
(cid:88)

k
(cid:89)

k=0

l=0

|||Λζ,t−l|||s0

• (cid:107)ωζ,t−k−1(cid:107)s0
• .

By the monotonicity properties of the norms (cid:107) · (cid:107)• and ||| · |||• and the deﬁnitions of the matrices
Λζ,t and Λt in (14) and (16) we also obtain |||Λζ,t|||• ≤ |||Λt|||• and (cid:107)ωζ,t(cid:107)• ≤ (cid:107)ωt(cid:107)• = ε2
t (cid:107)ω(cid:107)•
(a.s.) for all t = 1, 2, . . ., implying that

(cid:107)ξ(yt)(cid:107)s0

• ≤ C

t−1
(cid:89)

k=0

|||Λt−k|||s0

• (cid:107)ξ(y0)(cid:107)s0

• + C

(cid:32)

|εt|2s0 +

t−2
(cid:88)

k
(cid:89)

k=0

l=0

|||Λt−l|||s0

• |εt−k−1|2s0

(cid:107)ω(cid:107)s0
• .

(cid:33)

Now, denote the expectation E [|||Λt|||s0

• ] by χ (this expectation is ﬁnite due to Assumption

4). Using the independence of the Λt’s and independence of |||Λt−l|||•’s and εt−k−1’s, yields

E [(cid:107)ξ(yt)(cid:107)s0

• | y0 = x] ≤ Cχt(cid:107)ξ(x)(cid:107)s0

• + C

(cid:32)

1 +

t−2
(cid:88)

k=0

(cid:33)

χk+1

(cid:107)ω(cid:107)s0
• .

(57)

Inequality (34) in conjunction with (38) show that σ2s0
i = 1, . . . , p + q. From (57) it then follows that E[σ2s0
together with (56) implies

i

i ≤ C(1 + (cid:107)ξ(yi−1)(cid:107)s0

| y0 = x] ≤ C(1 + (cid:107)ξ(x)(cid:107)s0

• ) (a.s.) for all
• ) which

E[(cid:107)yp+q(cid:107)2s0α

1

| y0 = x] ≤ C (cid:0)1 + |z1(x1)|2s0 + (cid:107)z2(x1)(cid:107)2s0α

∗ + (cid:107)ξ(x)(cid:107)s0
•

(cid:1) .

For any x ∈ AN , the dominant side is bounded by C(1 + 2N + N 1/bα) and thus we can ﬁnd a
ﬁnite MN such that (51) holds.

Step 7: Completing the proof. We are now ready to complete the proof by applying
Theorem 1(iii) in Meitz and Saikkonen (2020). To this end, in the beginning of Step 6 we
already noted that the Markov chain yt is ψ-irreducible and aperiodic. That Condition D
holds was shown in (50) in Step 5. Petiteness of the set AN was shown in Step 6. We also
V (x) < ∞; this inequality is a straightforward consequence of the
need to verify that supx∈AN
deﬁnitions of the set AN and the function V . Thus, applying Theorem 1(iii) in Meitz and
(cid:4)
Saikkonen (2020) we can complete the proof.

25

Proofs of Propositions 1 and 2. For Proposition 1, note that model (24) can be written as
ut = ut−1 +ν1L(ut−1; γ, a1)+ν2(1−L(ut−1; γ, a2))+σtεt so that the function g(·) in Assumption
2(ii) takes the form g(u) = u + ν1L(u; γ, a1) + ν2(1 − L(u; γ, a2)). Arguments used in the proof
of Proposition 1 in Meitz and Saikkonen (2020) now show that Assumption 2(ii) holds with
ρ = 1. Applying Theorem 1 with δ = 2s0 yields the polynomial ergodicity result, and the
moment result follows from the remarks made after Theorem 1. As for Proposition 2, model
(25) can be written as ut = S(ut−1)ut−1 + σtεt so that now g(u) = S(u)u. Assumption 2(ii)
can be veriﬁed as in the proof of Proposition 2 in Meitz and Saikkonen (2020), and the result
(cid:4)
follows from Theorem 1 (with δ = 2s0/ρ).

Appendix B

Appendix B contains Figure 2 which displays further analysis of the residuals of model (27).

Figure 2: Further analysis of the residuals shown in (the bottom right graph of) Figure 1:
autocorrelation function of ˆεt (top left), autocorrelation function of ˆε2
t (top right), histogram
along with the estimated error density (bottom left), and a Q-Q plot (bottom right). The dashed
√
T ≈ ±0.038
lines in the autocorrelation function graphs show the conventional bounds ±1.96/
(T = 2715; the ﬁrst four observations are used as initial values).

26

010203040500.00.20.40.60.81.0010203040500.00.20.40.60.81.0−4−2024680.00.20.40.6−4−20246810−4−20246References

Atchadé, Y. and G. Fort (2010). Limit theorems for some adaptive MCMC algorithms with

subgeometric kernels. Bernoulli 16, 116–154.

Cline, D. B. H. and H. H. Pu (1998). Verifying irreducibility and continuity of a nonlinear time

series. Statistics & Probability Letters 40, 139–148.

Cline, D. B. H. and H. H. Pu (2004). Stability and the Lyapounov exponent of threshold

AR-ARCH models. Annals of Applied Probability 14, 1920–1949.

Davidson, J. (1994). Stochastic Limit Theory. Oxford: Oxford University Press.

Douc, R., G. Fort, E. Moulines, and P. Soulier (2004). Practical drift conditions for subgeometric

rates of convergence. Annals of Applied Probability 14, 1353–1377.

Douc, R., A. Guillin, and E. Moulines (2008). Bounds on regeneration times and limit theo-
rems for subgeometric Markov chains. Annales de l’Institut Henri Poincaré – Probabilités et
Statistiques 44, 239–257.

Douc, R., E. Moulines, P. Priouret, and P. Soulier (2018). Markov Chains. Cham: Springer.

Dudley, R. M. (2004). Real Analysis and Probability. Cambridge: Cambridge University Press.

Engle, R. F. (1982). Autoregressive conditional heteroscedasticity with estimates of the variance

of United Kingdom inﬂation. Econometrica 50, 987–1007.

Feﬀerman, C. and H. S. Shapiro (1972). A planar face on the unit sphere of the multiplier

space Mp, 1 < p < ∞. Proceedings of the American Mathematical Society 36, 435–439.

Fort, G. and E. Moulines (2000). V-subgeometric ergodicity for a Hastings–Metropolis algo-

rithm. Statistics & Probability Letters 49, 401–410.

Fort, G. and E. Moulines (2003). Polynomial ergodicity of Markov transition kernels. Stochastic

Processes and their Applications 103, 57–99.

Horn, R. A. and C. R. Johnson (2013). Matrix Analysis (2nd ed.). Cambridge University Press.

Jarner, S. F. and G. O. Roberts (2002). Polynomial convergence rates of Markov chains. Annals

of Applied Probability 12, 224–247.

Jones, M. C. and M. J. Faddy (2003). A skew extension of the t-distribution, with applications.

Journal of the Royal Statistical Society: Series B 65, 159–174.

Klokov, S. A. (2007). Lower bounds of mixing rate for a class of Markov processes. Theory of

Probability and Its Applications 51, 528–535.

Klokov, S. A. and A. Yu. Veretennikov (2004). Sub-exponential mixing rate for a class of

Markov chains. Mathematical Communications 9, 9–26.

27

Klokov, S. A. and A. Yu. Veretennikov (2005). On subexponential mixing rate for Markov

processes. Theory of Probability and Its Applications 49, 110–122.

Lieberman, O. and P. C. B. Phillips (2020). Hybrid stochastic local unit roots. Journal of

Econometrics 215, 257–285.

Ling, S. (1999). On the probabilistic properties of a double threshold ARMA conditional

heteroskedastic model. Journal of Applied Probability 36, 688–705.

Ling, S. and M. McAleer (2002). Necessary and suﬃcient moment conditions for the
GARCH(r,s) and asymmetric power GARCH(r,s) models. Econometric Theory 18, 722–
729.

Meitz, M. and P. Saikkonen (2008a). Ergodicity, mixing, and existence of moments of a class
of Markov models with applications to GARCH and ACD models. Econometric Theory 24,
1291–1320.

Meitz, M. and P. Saikkonen (2008b). Stability of nonlinear AR–GARCH models. Journal of

Time Series Analysis 29, 453–475.

Meitz, M. and P. Saikkonen (2010). A note on the geometric ergodicity of a nonlinear AR–

ARCH model. Statistics & Probability Letters 80, 631–638.

Meitz, M. and P. Saikkonen (2020). Subgeometrically ergodic autoregressions. Econometric

Theory, advance online publication, doi:10.1017/S0266466620000419.

Meitz, M. and P. Saikkonen (2021). Subgeometric ergodicity and β-mixing. Journal of Applied

Probability 58, 594–608.

Merlevède, F., M. Peligrad, and E. Rio (2011). A Bernstein type inequality and moderate
deviations for weakly dependent sequences. Probability Theory and Related Fields 151, 435–
474.

Meyn, S. P. and R. L. Tweedie (2009). Markov Chains and Stochastic Stability (2nd ed.).

Cambridge: Cambridge University Press.

Nummelin, E. and P. Tuominen (1983). The rate of convergence in Orey’s theorem for Harris
recurrent Markov chains with applications to renewal theory. Stochastic Processes and their
Applications 15, 295–311.

Tuominen, P. and R. L. Tweedie (1994). Subgeometric rates of convergence of f -ergodic Markov

chains. Advances in Applied Probability 26, 775–798.

Tweedie, R. L. (1983). Criteria for rates of convergence of Markov chains, with application to
queueing and storage theory. In J. F. C. Kingman and G. E. H. Reuter (Eds.), Probability,
Statistics and Analysis, pp. 260–276. Cambridge: Cambridge University Press.

Veretennikov, A. Yu. (2000). On polynomial mixing and convergence rate for stochastic diﬀer-
ence and diﬀerential equations. Theory of Probability and Its Applications 44, 361–374.

28

