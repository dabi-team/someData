9
1
0
2

t
c
O
7
1

]

G
L
.
s
c
[

1
v
7
3
7
7
0
.
0
1
9
1
:
v
i
X
r
a

Autoregressive Models: What Are They Good For?

Murtaza Dalal∗
mdalal@berkeley.edu

Alexander C. Li∗
alexli1@berkeley.edu

Rohan Taori∗
rohantaori@berkeley.edu

Abstract

Autoregressive (AR) models have become a popular tool for unsupervised learning,
achieving state-of-the-art log likelihood estimates. We investigate the use of AR
models as density estimators in two settings – as a learning signal for image transla-
tion, and as an outlier detector – and ﬁnd that these density estimates are much less
reliable than previously thought. We examine the underlying optimization issues
from both an empirical and theoretical perspective, and provide a toy example that
illustrates the problem. Overwhelmingly, we ﬁnd that density estimates do not
correlate with perceptual quality and are unhelpful for downstream tasks.

1

Introduction

Autoregressive (AR) models are a class of likelihood models that attempt to model the data
distribution by estimating the data density. They approach the maximum likelihood objective
Ex~pdata(x)[log pθ(x)] by factorizing pθ(x) over the dimensions of x via the chain
θ∗ = arg maxθ
rule. They learn each conditional probability forming pθ(x) = Πd
i=1pθ(xi|x1, ..., xi−1), and this
decomposition helps them achieve negative log-likehood (NLL) scores superior to other methods
such as VAEs [11] or ﬂow models [5, 1].

PixelCNN [4] was the ﬁrst to introduce a convolutional AR architecture, achieving a NLL of 3.00
bits/dim on CIFAR-10. Over the past few years, a ﬂurry of further modiﬁcations [14, 15, 17, 8, 3]
has pushed the score down to 2.85 bits/dim, the best known reported NLL on CIFAR-10 to date.
However, despite these advancements, their uses outside of compression have not been well explored.
Samples from these models are considerably worse than state-of-the-art GANs [2], and they do not
provide a compact latent space representation, an important piece for use in downstream tasks.

Prior work [19] has shown that AR metrics such as log-likelihood and Parzen window estimates are
poor indicators of the AR model’s performance on speciﬁc tasks. We investigate this empirically
in two scenarios: using the log-density as a learning signal for image translation, and for outlier
detection. Our results show that density estimates neither correlate with perceptual quality, nor are
useful for downstream tasks.

2 Log-Likelihood Estimates as Learning Signal

We ﬁrst explore using NLL scores for image-to-image translation. CycleGAN [20], a popular
unpaired image translation method, learns mappings between domains X and Y using GANs [7].
However, GANs are known to be unstable during training due to their adversarial framework, and
they also lack an evaluation metric for the perceptual quality of generated images, so there is no way
to evaluate the learned mapping other than visually inspecting the samples.

Replacing the CycleGAN discriminator with NLL estimates from an AR model seems like a natural
solution to these problems. Powerful AR models provide log-likelihood estimates of our samples
throughout training that we can compare across methods, and optimization could be easier since there

∗Authors contributed equally to this work, ordered alphabetically by surname.

Preprint. Under review.

 
 
 
 
 
 
is no longer an adversarial minimax problem, which could help learn more general cross-domain
mappings.

2.1 ARCycle Formulation

For the mapping functions F : X → Y and G : Y → X, we borrow the same cycle consistency loss
from [20]:

Lcyc(G, F ) = Ey∼pdata(y)[||F (G(y)) − y||1] + Ex∼pdata(x)[||G(F (x)) − x||1]
Instead of an adversarial loss, we use a generative loss with the negative log-likelihood of the
generated image under our autoregressive model. This autoregressive model is trained purely on real
images from its domain. For the mapping function F : X → Y and the density model P on Y , we
express the objective as:

(1)

LN LL(P, F, X, Y ) = Ex∼p(x)[− log P (F (x))]

(2)

Thus, our overall ARCycle objective that we minimize is

L(F, G, PX , PY ) = LN LL(PY , F, X, Y ) + LN LL(PX , G, Y, X) + βLcyc(G, F )

(3)

2.2 Experiments and Discussion

(a) LN LL + Lcyc

(b) Just LN LL

(c) Just Lcyc

(d) LN LL + Lcyc,
Gaussian blur on
generator output.

(e) ARCycle start-
ing with pretrained
generators.

Figure 1: ARCycle trained in different settings. The left columns are real images from colored-
MNIST dataset, the middle columns contain mappings of the images to the MNIST domain, and the
right columns show reconstruction with the reverse mapping back to the colored-MNIST domain.

(a) Iteration 0

(b) Iteration 25

(c) Iteration 50

(d) Iteration 75

(e) Iteration 100

Figure 2: AR Cycle quickly learns to output lines in the MNIST space when optimizing LN LL +Lcyc.

We trained ARCycle with the loss in Equation 3, as well as several ablations. We used a pre-trained
PixelCNN++ to compute LN LL and set β so that LN LL is on the same order as the reconstruction
loss. After training to convergence, we observed the results in Figure 1.

The reconstructions in Figure 1a are perfect, but the translated images in the MNIST domain have
collapsed to a degenerate solution. The network encodes enough information into the translation to
perfectly reconstruct the original image, but hits a local optimum for LN LL and does not learn to
produce realistic MNIST digits. The negative log prob of the translations starts at about 8 bits/dim
and steadily goes down to 3 bits/dim, far from the .8 bits/dim that PixelCNN++ achieves on the
MNIST test set. Even if we remove competing losses and only optimize LN LL as in Figure 1b, or
blur the transformed images to remove high-frequency patterns as in Figure 1d, our mappings still fail
to produce realistic MNIST digits. Most interestingly, even if we initialize training with mappings F

2

and G pretrained using CycleGAN, the ARCycle training procedure manages to corrupt the mappings
by producing faint lines in the background.

To analyze how the network learns to arrive at the degenerate solution, we plot reconstructions over
iterations in Figure 2. Here, we see an interesting phenomenon: with more and more updates, the
translated images look like a set of lines, while at the same time the reconstructions get more accurate.
A hypothesis is that the network is learning to embed information in high-frequency signals that
aren’t apparent to the human eye, a phenomenon also mentioned in [20].

LN LL proved to be difﬁcult to optimize in a variety of settings, and these results suggest that using
AR density estimates as learning signal for optimization may be ﬂawed, which we investigate in the
next section.

3 Optimization With AR Models

(a) Optimized samples at iteration 0, 5000, and 10000.

(b) Loss curve over 10k iter-
ations.

Figure 3: Result of directly maximizing images’ PixelCNN++ log-likelihood estimate by applying
gradients to the image pixels themselves.

In this section, we investigate the optimization process and explain why an AR log-likelihood loss
can be difﬁcult to optimize in general.

3.1 Directly Maximizing Image Log-Likelihood

One key observation is that the gradient of the ARCycle loss with respect to our generator F ’s
parameters is entirely dependent – by the gradient chain rule – on the gradient of the log-likelihood
with respect to the generated image. If that pixel-level gradient vanishes, gradient descent cannot help
our generator produce better images. Thus, we want to see if it’s possible to directly optimize the
image pixels themselves using gradient descent on the negative log-likelihood output of PixelCNN++.

Figure 3 shows the result of applying gradient descent on 3 MNIST digits from the test set, 3 images
of random noise, and black, gray, and white images. Every image begins to accumulate noise, even
the true MNIST digits, which should have already been in a local minimum. Both the noisy and gray
images form lines, which indicates that PixelCNN++ gradients lead us to images that locally resemble
digits, rather than images that gradually form digits as training progresses. Not only does the model
encourage local texture over global structure, but the optimization problem seems ill-conditioned.
The true MNIST digits start with 0.6 bits/dim, but jump up to 7 bits/dim after a few gradient steps.
Since the digits remain visually identical, log-likelihood has no bearing on the quality of a sample.
Overall, minimizing log-likelihood is difﬁcult to do and is not guaranteed to produce good results.

3.2 Harder Optimization Problem

Training PixelCNN++ on CIFAR-10 or ImageNet achieves good performance for a wide range of
hyperparameter choices. Why, then, is it so hard to maximize the log-likelihood of samples under a
trained PixelCNN++ model?
The PixelCNN++ objective is minθ Ex∼Pdata [− log Pθ(x)]. As long as Pθ is initialized with support
almost everywhere and the parameters θ are closely tied to the density estimate, the gradient of this is
nonzero and should yield improvements to the AR model’s ability to model the distribution.

Ex∼Pgen[− log Pθ(x)]
In contrast, by trying to directly optimize our samples, our objective is minPgen
Instead of trying to increase our probability of data points by adapting our model, we have to ﬁnd

3

(a) True data distribution.

(b) Learned probability with
discretized Gaussian.

(c) Gradient norm heatmap.

Figure 4: 2D toy example illustrating the optimization problem that arises when the data lies on a
lower-dimensional manifold.

samples that have high likelihood under a ﬁxed AR model. In certain scenarios, this proves to be an
impossible task. Assuming that the data lies on a lower-dimensional manifold within the pixel-space,
a powerful enough AR model will put all of its probability mass on the manifold, leaving none
elsewhere. Thus, as the model has probability zero almost everywhere, the gradients are also zero
almost everywhere, so gradient descent cannot improve the log-likelihood of points.

We visualize this with a 2-dimensional toy problem in Figure 4. The true data distribution, shown
in Figure 4a, always has x1 = 0, while x2 ∼ N (0, 1). We use a MADE AR model [6] to learn
a discretized Gaussian log-likelihood, an analogue of the discretized mixture of logistics used by
PixelCNN++. As seen in Figure 4b, the trained model is able to ﬁt the true distribution fairly closely.
Internally, the Gaussian distribution in the x1-direction is becoming narrower, as it tries to place as
much mass as possible in the bin for x1 = 0. This leads to the gradient heatmap in Figure 4c, which
is taken with respect to x1 and x2. The gradient is nonzero in only 2 thin strips, which will converge
to two inﬁnitely thin lines as the AR model ﬁts the true distribution better and better. Optimizing a set
of samples on this landscape would be impossible, as only the points on those thin, bright gradient
strips would have a nonzero gradient. Thus, the ability of powerful autoregressive models to represent
any distribution hurts downstream optimization by creating vanishing gradients almost everywhere.

4 Correlation between Log-Likelihood and Perceptual Features

4.1 Do Realistic Images Have High Log-Likelihood?

Figure 5: Left: samples from WGAN-GP trained on CIFAR-10. Right: plot of the zero-centered
inception score and PixelCNN++ negative log probability of WGAN-GP samples over the WGAN
training process.

Do images that look like the training dataset have high log probability under an AR model for that
dataset? We propose the following experiment: Train a WGAN-GP [9] model to produce highly
realistic CIFAR-10 images, compute the bits/dim of those samples under a PixelCNN++, and compare
that quantity to the bits/dim of the CIFAR-10 test set under the same PixelCNN++.

Our WGAN-GP samples, which are visually indistinguishable from real images, obtained 6.52
bits/dim, signiﬁcantly higher than the CIFAR-10 test set, which had 2.92 bits/dim. This suggests that
perceptual similarity may not be correlated well with the AR log probability. To further investigate,
we compare the AR log probability to the inception score, a metric purported to correlate well with

4

Dataset
CIFAR-10 Test
WGAN-GP CIFAR-10 Samples
SVHN Test
Noise
All Black
All White

AR-2SD AR-1SD AR-One-sided
68%
44.9%
44.9%
0%
0%
0%

CCG
94.7% 92.3%
0% 69.7%
100% 37.3%
1.1%
0%
0%

92.5%
0%
90.9%
0%
0%
0%

0%
100%
100%

Table 1: The percent of the dataset classiﬁed to be in CIFAR-10, i.e. classiﬁed as not an outlier.

human perception [16]. Figure 5 shows that the log probability remains relatively constant, even
though the inception score and sample perceptual quality progressively increase. This further implies
that log probability is not a good metric to evaluate whether a given image is similar to a certain
distribution of images.

4.2 Do Images with High Log-Likelihood Look Realistic?

We seek to better understand whether images that have high log probability under an AR model
trained on a dataset look like images from that dataset. To test this, we evaluated how well AR
models fare against other methods of outlier detection for high dimensional data, where using simple
strategies such as z-scores is not particularly useful. All the methods we test only assume knowledge
of the CIFAR-10 training set and attempted to classify new images as CIFAR-10.

We tried three strategies for outlier detection with AR models. We computed the mean µ and
standard deviation σ bits/dim over the CIFAR-10 training set using the PixelCNN++’s NLL. We then
constructed 3 intervals of the following form: 2 Standard Deviation Interval (AR-2SD): [µ − 2σ, µ +
2σ], 1 Standard Deviation Interval (AR-1SD): [µ − σ, µ + σ], and One-Sided Interval (AR-One-sided):
(−∞, µ + 2σ]. We classify an image as an outlier if its NLL in bits/dim lies outside the interval.

We compare against a Class-Conditional Gaussians (CCG), a method for high-dimensional outlier
detection using deep classiﬁers [12]. CCG is as follows: Train a classiﬁer on the dataset (Inception-v1
[18] on CIFAR-10), strip off the output layer, and ﬁt class conditional Gaussians to the feature vectors
given by evaluating the classiﬁer on the dataset. For a new test image, compute its feature vector and
evaluate the feature vector’s probability under each class conditional Gaussian; if the probability is
less than some threshold, classify as an outlier.

We test the effectiveness of the outlier detection methods on six different datasets: CIFAR-10 test
set, samples from WGAN-GP trained on CIFAR-10, SVHN test set, random noise images from
{0, ..., 255}D, completely black images, and completely white images. Intuitively, we would expect
a good outlier detector to classify CIFAR-10 test set images correctly as CIFAR, WGAN-GP samples
as CIFAR most of the time, and SVHN, noise, all black and all white images as outliers.

Table 1 shows that CCG largely does what a good outlier detector should do, but the AR model
does not. In fact, the AR model often classiﬁes SVHN to be CIFAR just as often as it does actual
CIFAR-10 images. As found earlier by [13], PixelCNN++ actually assigns higher log probability to
images from SVHN than actual CIFAR-10 images – SVHN achieves 2.1 bits/dim, while CIFAR-10
only gets 2.92 bits/dim. Additionally, the model assigns even high log probability to all black and all
white images: 0.008 and 0.16 bits/dim respectively. These failures on outlier detection indicate that
log probability under an AR model is not usefully correlated with our notion of perceptual quality.

5 Conclusions and Future Work

We investigate the usefulness of the density estimates of autoregressive models. We apply them to 2
tasks: as a learning signal for unsupervised image translation with ARCycle, and outlier detection, and
ﬁnd that the density estimates are not informative in both settings. We also perform an analysis on the
underlying optimization issues, ﬁnding that optimizing using AR models leads to degenerate solutions
due to vanishing gradients. Their log-likelihood estimates also don’t correlate with perceptual quality,
which explains their poor performance at outlier detection. Given that these models achieve superior
log-likelihood, our ﬁndings call into question the overall utility of likelihood-based learning. In this
work, we examined PixelCNN++ in detail; one interesting avenue of future work is investigating if
our results hold for other autoregressive models as well.

5

References

[1] John R. Bach and George Tewﬁk.

Convolutions Diederik.
9115 (Print)\r0894-9115 (Linking).
doi:
http://arxiv.org/abs/1807.03039https://insights.ovid.com/crossref?an=
00002060-200704000-00008http://www.ncbi.nlm.nih.gov/pubmed/17413543.

In NIPS, volume 86, pages 301–3, 2007.

Glow: Generative Flow with Invertible 1x1x1
ISBN 0894-
URL

10.1097/PHM.0b013e318038d39c.

[2] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large Scale GAN Training for High
Fidelity Natural Image Synthesis. 9 2018. URL http://arxiv.org/abs/1809.11096.

[3] Xi Chen, Nikhil Mishra, Mostafa Rohaninejad, and Pieter Abbeel. PixelSNAIL: An Improved
Autoregressive Generative Model. Proceedings of Machine Learning Research, 12 2017. URL
http://arxiv.org/abs/1712.09763.

[4] Aaron Courville, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel Recurrent Neural Networks.
Proceedings of The 33rd International Conference on Machine Learning, 48:1747–1756, 2016.
URL http://proceedings.mlr.press/v48/oord16.html.

[5] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using Real NVP. 9

2016. URL http://arxiv.org/abs/1605.08803.

[6] Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. MADE: Masked Autoen-
coder for Distribution Estimation. 9 2015. URL http://arxiv.org/abs/1502.03509.

[7] Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative Adversarial Networks. 9 2014. URL
http://arxiv.org/abs/1406.2661.

[8] Scott Gray, Alec Radford, and Diederik P Kingma. GPU Kernels for Block-Sparse Weights.

Technical report. URL https://github.com/openai/blocksparse.

[9] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville.
Improved Training of Wasserstein GANs. 9 2017. URL http://arxiv.org/abs/1704.
00028.

[10] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-Image Translation with

Conditional Adversarial Networks. 9 2016. URL http://arxiv.org/abs/1611.07004.

[11] Diederik P Kingma and Max Welling. Auto-Encoding Variational Bayes. 9 2013. URL

http://arxiv.org/abs/1312.6114.

[12] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A Simple Uniﬁed Framework for
Detecting Out-of-Distribution Samples and Adversarial Attacks. 9 2018. URL http://arxiv.
org/abs/1807.03888.

[13] Eric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, Dilan Gorur, and Balaji Lakshminarayanan.
Do Deep Generative Models Know What They Don’t Know? ICLR 2019, 10 2018. URL
http://arxiv.org/abs/1810.09136.

[14] Aaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and
Koray Kavukcuoglu. Conditional Image Generation with PixelCNN Decoders. 6 2016. URL
http://arxiv.org/abs/1606.05328.

[15] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Łukasz Kaiser, Noam Shazeer, and Alexander
Ku. Image Tranformer. In ICML Submission, 2018. URL http://arxiv.org/abs/1802.
05751.

[16] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
ISSN 09246495. doi: arXiv:1504.01391. URL

Improved DCGAN. pages 1–10, 2016.
http://arxiv.org/abs/1606.03498.

[17] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. PixelCNN++: Improving
the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modiﬁcations. 9 2017.
URL http://arxiv.org/abs/1701.05517.

6

[18] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov,
Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions.
In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern
Recognition, volume 07-12-June-2015, pages 1–9. IEEE Computer Society, 9 2015. ISBN
9781467369640. doi: 10.1109/CVPR.2015.7298594.

[19] Lucas Theis, Aäron van den Oord, and Matthias Bethge. A note on the evaluation of generative

models. 11 2015. URL http://arxiv.org/abs/1511.01844.

[20] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired Image-to-Image
Translation using Cycle-Consistent Adversarial Networks. 9 2017. URL http://arxiv.org/
abs/1703.10593.

7

