1
2
0
2

n
a
J

1
1

]
T
S
.
h
t
a
m

[

1
v
0
1
2
4
0
.
1
0
1
2
:
v
i
X
r
a

General Hannan and Quinn Criterion for Common Time
Series

BY Kare KAMILA∗

January 13, 2021

SAMM, Université Paris 1 Panthéon-Sorbonne, FRANCE

Abstract

∞

This paper aims to study data driven model selection criteria for a large class
) processes, as well as GARCH or
of time series, which includes ARMA or AR(
∞
ARCH(
), APARCH and many others processes. We tackled the challenging issue of
designing adaptive criteria which enjoys the strong consistency property. When the
observations are generated from one of the aforementioned models, the new criteria,
select the true model almost surely asymptotically. The proposed criteria are based on
the minimization of a penalized contrast akin to the Hannan and Quinn’s criterion and
then involved a term which is known for most classical time series models and for more
complex models, this term can be data driven calibrated. Monte-Carlo experiments
and an illustrative example on the CAC 40 index are performed to highlight the
obtained results.

Key words: Time series, Model selection, consistency, data driven, HQ criterion.

1 Introduction

A common solution in model selection is to choose the model, minimizing a penalized based
criterion which is the sum of two terms: the ﬁrst one is the empirical risk (least squares,
likelihood) that measures the goodness of ﬁt and the second one is an increasing function
of the complexity which aims to penalize large models and control the bias.
Therefore a challenging task when designing a penalized criterion is the speciﬁcation of the
penalty term. Considering leading model selection criteria (BIC, AIC, Cp, HQ to name
a few), one can see that the penalty term is a product of the model dimension with a
sequence which is speciﬁc to the criteria. Indeed, a criterion is designed according to the
goal one would like to achieve. The classical properties for model selection criteria include
consistency, eﬃciency (oracle inequality, asymptotic optimality), adaptative in the mini-
max sense.

In this paper, we focus on consistency property which aims at identifying the data
generating process with high probability or almost surely. Hence, it requires the assumption
whereby there exists a true model in the set of competitive models and the goal is to select
In [BKK20],
this with probability approaches one as the sample size tends to inﬁnity.
they studied model selection criteria regarding consistency in a large class of time series,
which is the interest of this paper. The leading criterion obtained in this framework is
the BIC; with a relatively heavy penalty, it ensures the selection of quite simple models.
Moreover, several papers have established the consistency property in particular settings.
For instance, [HQ79] shows that the Hannan and Quinn (HQ) penalty c log log n with

∗This author has received funding from the European Union’s Horizon 2020 research and innovation

programme under the Marie Sklodowska-Curie grant agreement No 754362.

1

 
 
 
 
 
 
c > 2 leads to a consistent choice of the true order in the framework of AR type models.
One year later, [Han80] (or [HD12]) extended this result for ARMA models.

Also, it has been proven in several contexts, that the BIC criterion [Sch78] enjoys
the consistency property:
[Shi86] in the density estimation using hypothesis testing for
autoregressive moving average models, [LMH04] in density estimation for independent
observations, [BKK20] for a general class of time series, to name a few.

Compare to HQ penalty, the BIC penalty does not have the slowest rate of increase
and then it can very often choose very simple models possible wrongs for small samples
[HQ79]. Moreover, the HQ criterion has been derived for linear time series: AR models in
[HQ79], ARMA models in [Han80] and [HD12]. Is the HQ penalty still strongly consistent
for heteroscedastic nonlinear models such as GARCH, APARCH or ARMA-GARCH? And
what about a general class including linear and non linear models as well?

That raises a challenging question of designing robust penalties for most classical time
series models enjoying the model selection consistency. This is the issue we want to address
in this paper for a general class of times series called aﬃne causal and deﬁned below.

Class

AC

(M, f ) : A process X = (Xt)t∈Z belongs to

Xt = M

(Xt−i)i∈N∗

ξt + f

AC
(Xt−i)i∈N∗

(M, f ) if it satisﬁes:

for any t

Z.

∈

(1.1)

(cid:0)
(cid:0)
where (ξt)t∈T is a sequence of zero-mean independent identically distributed random vec-
tors (i.i.d.r.v) satisfying E(
R are two measurable
ξ0|
|
functions.

(cid:1)
1 and M , f : R∞

with r

r) <

→

∞

≥

(cid:1)

For instance,

• if M

(Xt−i)i∈N∗

= σ and f

(Xt−i)i∈N∗

=

process;
(cid:0)

(cid:1)

• if M

(Xt−i)i∈N∗

=

(cid:0)
a0 + a1X 2

t−1 +

(Xt)t∈Z is an ARCH(p) process.
(cid:1)

q

(cid:0)

∞
i=1 φiXt−i, then (Xt)t∈Z is an AR(

∞

)

(cid:1)

· · ·

P
+ apX 2

t−p and f

(Xt−i)i∈N∗

= 0, then

(cid:0)

(cid:1)

Note that, numerous classical time series models such as ARMA(p, q), GARCH(p, q),
ARMA(p, q)-GARCH(p, q) (see [DGE93] and [LM03]) or APARCH(δ, p, q) processes (see
[DGE93]) belongs to
The study of this type of process more often requires the classical regularity conditions on
the functions M and f , which are not restrictive at all and remain valid in various time
serie models. Let us recall these conditions for Ψθ = fθ or Mθ and Θ a compact set.

(M, f ).

AC

Hypothesis A(Ψθ, Θ): Assume that
negative real numbers

αk(Ψθ, Θ)

Ψθ(0)
k
k≥1 such that

kΘ <

∞
∞
k=1 αk(Ψθ, Θ) <

and there exists a sequence of non-

satisfying:

∞

(cid:0)

Ψθ(x)
k

−

Ψθ(y)

(cid:1)
kΘ ≤

∞

Xk=1

P
xk −
αk(Ψθ, Θ)
|

yk|

f or all x, y

R∞.

∈

In addition, if the noise ξ0 admits r-order moments (for r

1), let us deﬁne:

≥

Θ(r) =

θ

n

Rd, A(fθ,

θ
{

∈

) and A(Mθ,
}

∞

θ
{

) hold with
}

αk(fθ,

θ
{

) +
}

ξ0kr
k

Xk=1

∞

Xk=1

αk(Mθ,

θ
{

) < 1
}

o

.

(1.2)

Under this assumption, [DW08] showed that there exists a stationary and ergodic solution
to (1.1) with r-order moment for any θ
Θ(r). Moreover, [BW09] studied the consistency
and the asymptotic normality of the QMLE of θ∗ for

(Mθ∗, fθ∗) .

∈

AC

2

The main contribution of this paper is the generalization of the HQ criterion to aﬃne
causal class: we provide a minimal multiplicative penalty term cmin so that all penalties
of the form 2 c log log n Dm with c
cmin ensure the strong consistency property for aﬃne
causal models under some mild conditions on the Lipschitz coeﬃcients of functions Mθ, fθ
(Dm denotes the size of the model m). Monte Carlo experiments have been conducted in
order to attest the accuracy of our new criteria.

≥

The paper is organized as follows. The model selection consistency along with notations
and assumptions are described in Section 2. Numerical results are presented in Section 3
and Section 4 contains the proofs.

2 Model Selection Consistency

2.1 Model Selection Procedure

Let assume (X1, . . . , Xn) be a trajectory of a stationary aﬃne causal process m∗ :=
(Mθ∗ , fθ∗), where θ∗ is unknown. The goal of the consistency property is to come

AC
up with this true model given a set of candidate model

such that m∗

M

.
∈ M

∈

⊂

∈ M

Θ(m)

can be viewed as a set of causal functions (Mθ, fθ)

RDm. Θ(m) is the parameter set of the model m.

A Dm-dimensionnal model m
with θ
The consistency property will be study using the MLE ﬁrst. Extension to QMLE will be
done afterwards.
) log-
The MLE is derived from the conditional (with respect to the ﬁltration σ
likelihood of (X1, . . . , Xn) when (ξt) is supposed to be a Gaussian standard white noise.
Due to the linearity of a causal aﬃne process, we deduce that this conditional log-likelihood
(up to an additional constant) Ln is deﬁned for all θ

(Xt)t≤0

Θ by:

(cid:8)

(cid:9)

Ln(θ) :=

1
2

−

n

t=1
X

qt(θ) , with qt(θ) :=

∈
(Xt −
H t
θ

θ)2
f t

+ log(H t
θ)

(2.1)

), M t

where f t

· · ·

θ := fθ(Xt−1, Xt−2,

θ := Mθ(Xt−1, Xt−2,

θ =
From now on, we drop the Gaussian assumption of the noise. Let
(cid:0)
M

a ﬁnite family
(cid:1)
of candidate models containing the true one m∗. According to Proposition 1 in [BKK20],
all these models can be included into a big one with parameter space Θ. For each speciﬁc
model m

, we deﬁne the Gaussian MLE

θ(m) as

· · ·

) and H t

M t
θ

2.

∈ M

θ(m) = argmax
b
θ∈Θ(m)

Ln(θ).

(2.2)

b
∈ M

, we consider a penalized contrast C(m) ensuring a
To select the true model m
trade-oﬀ between
2 times the maximized log-likelihood, which decreases with the size of
the model, and a penalty increasing with the size of the model. Therefore, the choice of
the "best" model
m among the estimated can be performed by minimizing the following
criteria

−

m = argmin
m∈M

b

C(m) with C(m) =

−

2 Ln

θ(m)

+ κn(m)

(2.3)

where (κn)n an increasing sequence depending on the number of observations n and the
dimension Dm. There exist several possible choices of κn(m) including

b

b

(cid:0)

(cid:1)

• κn(m) = 2c Dm log log n with c > 1, we retrieve the HQ criterion [HQ79];

• κn = Dm log n, C yields to BIC criterion [Sch78];

• κn = 2 Dm, C is the AIC criterion [Aka73].

3

Basically the principle is that by increasing the size, the likelihood increases also. The
question is whether this increase in complexity is oﬀset by a suﬃcient increase in likelihood.
If the answer is no, then the least complex model is used, even if it is less likely. If the
answer is aﬃrmative, then we accept to work with a more complex model. Of course,
all the diﬃculty lies in the choice of weights between likelihood and complexity, and thus
ultimately in the speciﬁcation of the penalty multiplicative term κn.

What is the better weighting term of the model complexity? The aim here is by
leveraging the increasing rate of the likelihood, to propose a data driven κn in order to
guarantee the strong consistency property to our model selection procedure i.e.

2.2 Assumptions

m a.s.
−→n→∞

m∗.

b

(2.4)

Some mild conditions will be required to prove the consistency of the considered model
selection criteria.

The following assumption is well-known as the identiﬁability one and is always required in
order to guarantee the unicity of the global maximum of the MLE at the true parameter
θ∗. That is:

Assumption A1: For all θ, θ′

∈

Θm, (f 0

θ = f 0

θ′) and (M 0

θ = M 0

θ′) =

⇒

θ = θ′.

Another required assumption concerns the diﬀerentiability of Ψθ = fθ or Mθ on Θ. This
type of assumption has already been considered in order to apply the QMLE procedure
(see [BW09], [SM06], [Whi82]).

The following condition provides the invertibility of the Fisher’s information matrix of
(X1, . . . , Xn) and was used to prove the asymptotic normality of the QMLE (see [BW09]).

Assumption A2: One of the families (∂f t
linearly independent.

θ/∂θ(i))1≤i≤Dm∗ or (∂H t

θ/∂θ(i))1≤i≤Dm∗ is a.e.

Note that the deﬁnitions of the conditional log-likelihood requires that their denomi-
nators do not vanish. Hence, we will suppose in the sequel that the lower bound of
2 (which is reached since Θ is compact) is strictly positive:
) =
Hθ(
·

)
Mθ(
·

Assumption A3:
(cid:0)

(cid:1)

h > 0 such that inf
θ∈Θ

∃

(Hθ(x))

h for all x

≥

R∞.

∈

Next we assume the existence of the eighth order moment of the noise.
Assumption A4: E[ξ8

.

0] <

∞

We end the list of assumptions by assuming a suitable relation between the Fisher Infor-
mation matrix G(θ∗
m) deﬁned
as follows

m) and the limiting Hessian matrix of the log-likelihood F (θ∗

F (θ∗

m)

i,j = E

m)

∂2q0(θ∗
∂θi∂θj

i

and (G(θ∗

m))i,j = E

m)

∂q0(θ∗
∂θi

m)

∂q0(θ∗
∂θj

,

i

h

with θ∗

(cid:0)

h
m := (θ∗, 0, . . . , 0)⊤

(cid:1)

Θ(m).

∈

Assumption A5: There exist absolutes constants α1 and α2 such that for any m
verifying m∗

m,

⊂

where D1
RDm, Σθ∗

m and D2
m := G(θ∗

m1m = α1 D1
1⊤
mΣθ∗
m are two integers such that D1
m)1/2F (θ∗

m)−1G(θ∗

m)1/2.

m + α2 D2
m
m + D2

m = Dm,

∈ M

(2.5)

1m := (1, 1, . . . , 1)⊤

∈

4

For most classical aﬃne causal models, A5 is veriﬁed (see Proposition 2). However, for

more complex models such as ARMA-GARCH with µ4 6
2.3 Consistency Result

= 3, Σθ∗

m is hard to handle.

Before stating the main result of this section, we give important intermediate results. All
proof of the results stated in this subsection can be found in Section 4.

The following Proposition suggests the existence of a term that will be the keystone of

this work.

Proposition 1. Let m∗ any aﬃne causal model. For any model m with θ∗
under A1-A5, there exist α1, α2, D1

m such that

m, D2

m ∈

Θ(m), and

lim sup
n→∞

Ln

θ(m)

Ln(θ∗

m)

−

(cid:0)

2 log log n
(cid:1)
b

=

1
4

(cid:0)

α1 D1

m + α2 D2
m

a.s.

(2.6)

(cid:1)

For every m
times

, let us denote by cmin(m) the following term that will be used several

∈ M

cmin(m) :=

1
4

α1 D1

m + α2 D2
m

(2.7)

Now we state a result which provides the values of both α1 and α2 for most classical aﬃne
causal models.

(cid:1)

(cid:0)

Proposition 2. Under the assumptions and notation of Proposition 1, we have

• If µ4 = E[ξ4
cmin(m) = 1

0] = 3 (for instance for Gaussian noise), then α1 = 2, α2 = 2 and
2 Dm;

• If the parameter θ identifying an aﬃne causal model Xt = M t

composed as θ = (θ1, θ2)′ with f t
and

θ =

f t
θ1

and M t

M t
θ2

θ ξt + f t
θ can be de-
1
, then α1 = 2, α2 = µ4 −

1
e
cmin(m) =
2

D1

m +

θ =
µ4 −
4

1
f

D2
m

The second conﬁguration in Proposition 2 includes classical time series

• GARCH(p, q), APARCH(δ, p, q) type models and related ones, cmin(m) = µ4−1

4 Dm;

• ARMA(p, q) models, cmin(m) = Dm
2
2 + µ4−1

cmin(m) = Dm−1

otherwise.

4

if the variance of the noise is known and

We can now state the ﬁrst main result of this paper.

Theorem 2.1. Let (X1, . . . , Xn) be an observed trajectory of an aﬃne causal process X
RDm∗ . Let
(Mθ∗, fθ∗) where θ∗ is an unknown vector belonging to Θ(r)
belonging to
⊂
. If assumptions A1-A5
also
∈ M
α1
4 , α2
hold, there exist α1, α2, and a minimal constant cmin := max

be a ﬁnite family of candidate models such that m∗

such that

AC

M

4

for any κn(m) = 2 c Dm log log n with

(cid:0)

(cid:1)

it holds for the selected model

m according to (2.3)

cmin

c

≥

b

a.s.
−→n→∞

m∗,

m

b

5

(2.8)

(2.9)

Remark 1.

1. For classical conﬁgurations as seen in Proposition 2, this result gives a

generalization of Hannan and Quinn criterion.

2. For more complex models, the values of α1 and α2 are unknowns (at least until a
better relationship between matrix F (θ∗
m) is found) and so cmin is also
unknown. In these cases, we propose to use adaptive methods such as slope heuristic
algorithm or dimension jump [AM09] to calibrate cmin.

m) and G(θ∗

Let us mention that our result generalizes the strong consistency obtained by [HQ79] for AR
models as the aﬃne causal class also contains GARCH models. It furthermore generalizes
the result [HD12] for ARMA models.

Theorem 2.1 gives a theoretical guarantee on the consistency of the model selection pro-
cedure. However, it does not say anything about the convergence (and its rate) of the
parameter estimate resulting from the model selection
m. The following results shows that
θ bm is consistent and veriﬁes a CLT.
the ﬁnal estimate

Theorem 2.2. Under the assumptions of Theorem 2.1, it holds

b

b

θ(

m)

P
−→n→∞

θ∗.

b

b

Moreover

√n

θ(

m)

with Σθ∗,m∗ := F (θ∗)−1G(θ∗)F (θ∗)−1

b

(cid:16)(cid:0)

b

(cid:1)

(θ∗)i

i −

i∈m∗

(cid:17)

D

−→n→∞ N|m∗|

0 , Σθ∗,m∗

(cid:0)

(cid:1)

(2.10)

(2.11)

In this subsection, we have used the MLE contrast without any distribution assumption
on the noise to derive a consistency property. However, the contrast Ln as in (2.1) depends
on all the past values of the process X, which are unobserved. In the next subsection, we
will propose an extension of Theorem 2.1 based on QMLE which does not require knowledge
of the initial values of the process.

2.4

Extension of Theorem 2.1 when using QMLE.

The goal of this subsection is to sharpen the conditions on the sequence κn found in
[BKK20]. Before stating the result, let recall a little bit some deﬁnitions and notations
Ln is
used in [BKK20] about QMLE. Following the deﬁnition of Ln, the quasi-likelihood

Ln(θ) :=

1
2

−

t=1
X
fθ(Xt−1, Xt−2, . . .),

b

f t
θ :=

where
For each speciﬁc model m
b

b

∈ M

n

qt(θ) , with

qt(θ) :=

θ)2
f t

(Xt −
H t
θ

b

b

θ :=

b
M t
Mθ(Xt−1, Xt−2, . . .) and
, we deﬁne the Gaussian QMLE
c
c
θ(m) = argmax
θ∈Θ(m)

Ln(θ).

b

+ log(

H t
θ)

b
(2.12)

b
M t
θ

c

H t
θ =
θ(m) as
(cid:0)
b
b

2.

(cid:1)

(2.13)

The choice of the "best" model
the penalized contrast

C(m)

e

b

m among the estimated can be performed by minimizing

m = argmin
b
m∈M

b
C(m) with

C(m) =

−

2

Ln

θ(m)

+ κn(m).

(2.14)

b

b

(cid:0)

e

b

(cid:1)

b

6

In this framework, we do not consider long memory process and then we deﬁne the
(Mθ, fθ) in which every process has Lispchitz coeﬃcients

(Mθ, fθ) a subset of
class
satisfying the following conditions

AC

H

αj(fθ) + αj(Mθ) + αj(∂θfθ) + αj(∂θMθ) = O(j−γ) with γ > 2.

It is then straightforward to see that every process in the class

(Mθ, fθ) veriﬁes the

H

following condition:

Condition K(Θ):

1
log log k

Xk≥e

Xj≥k

αj(fθ, Θ) + αj(Mθ, Θ) + αj(∂θfθ, Θ) + αj(∂θMθ, Θ) <

.

∞

This ﬁnding allows to propose a sharpen generalization of both Theorem 3.1 in our

previous paper [BKK20] and a similar result in [Ken20].

Before stating the result using QMLE, it is important as in [BKK20] or [BW09], to distin-
guish the special case of NLARCH(
) processes which includes for instance GARCH(p, q)
or ARCH(

∞
) processes. In such case, let us deﬁne the class:

∞

Class

AC

Hθ): A process X = (Xt)t∈Z belongs to
(

Hθ) if it satisﬁes:
(

AC

f

e

Xt = ξt

Hθ

(X 2

t−i)i∈N∗

f
e
for any t

Z.

∈

(2.15)

q
= Hθ

(cid:0)

(cid:1)
=

(Xt−i)i∈N∗
(Xt−i)i∈N∗
e
Hθ), we will use the assumption A(
(

Hθ

(X 2

(cid:1)

(cid:0)

(cid:1)

(cid:0)

e

then,

t−i)i∈N∗
AC
Hθ, Θ). The new set of stationary
f

Hθ) =
(

AC

e

(cid:1)

(Mθ, 0).

θ
{

) holds with
}

2

ξ0kr
k
(cid:0)

(cid:1)

e
∞

Xk=1

αk(

Hθ,

θ
{

e

) < 1
}

o

.

(2.16)

Hθ,

e

Therefore, if M 2
θ
In case of the class
(cid:0)
solutions is for r
≥

Θ(r) =

θ

n

e

∈

AC
2:
f
e
Rd, A(

Finally, we propose to restrict class
sidering all the process checking the condition

Hθ) to
(

AC

Hθ) as done with
(

H

(Mθ, fθ) by con-

H

αj(Hθ) + αj(∂θHθ) = O(j−γ) with γ > 2

f

e

e

e

so that

1
log log k

Xk≥e

Xj≥k

αj(Hθ) + αj(∂θHθ) <

.

∞

We can now state the second main result.

Theorem 2.3. Let (X1, . . . , Xn) be an observed trajectory of an aﬃne causal process
Hθ∗)) where θ∗ is an unknown vector belonging to
(
(Mθ∗, fθ∗) (or
X belonging to
RDm∗ ). Let also
be a ﬁnite family of candidate models
Θ(r)
Θ(r)
⊂
such that m∗
e
. If assumptions A1-A5 hold, there exist α1, α2, and a minimal con-
∈ M
α1
4 , α2
e
stant cmin := max

e
such that

H
RDm∗ (or

M

H

⊂

4

(cid:0)
for any κn(m) = 2 c Dm log log n with

(cid:1)

it holds for the selected model

m according to (2.14)

cmin

c

≥

b

m

b

a.s.
−→n→∞

m∗.

7

(2.17)

(2.18)

All the comments made about the Theorem 2.1 remain valid here. Moreover, re-
cently, [Ken20] requires heavy penalties to ensure the strong consistency for the process
(Mθ∗, fθ∗). Indeed, according to [Ken20], it is necessary that κn veriﬁed
in the class
to obtain (2.18) which is a stronger condition since the HQ criterion
κn/ log log n
does not fulﬁll this condition and it is well known that HQ criterion is strongly consistent
(see for instance [HQ79]). Moreover, the new penalties found in this paper does not satisfy
this condition, yet we ensure strongly consistency.

AC
−→n→∞ ∞

Also, let us mention that for small samples, heavy penalties such as those in [Ken20]

can very often choose very simple models possible wrongs [HQ79].

Remark 2. Our results show that, asymptotically heavy penalties such as BIC penalty will
ensure the consistency property. However, in practise, these penalties are used for ﬁxed n
and most for small samples and it is important to point out their drawbacks. Very often,
′).
in the family of competitive models
′, making the penalty,
Since the diﬀerence
θm) + 2
heavy could oﬀset this positivity and can lead to the selection of some underﬁtted models
b
and then wrong models. To be more convincing of that, see the simulations (DGP III)
experiments in Section 3.

, there are misspeciﬁed and underﬁtted models (

θm∗ ) > 0 for every m

M
Ln(

∈ M

Ln(

M

−

b

b

b

2

2.5 Algorithm of Calibration of the minimal constant

There exist several ways to calibrate the minimal constant cmin including the dimension
jump (presented below) and the data-driven slope estimation. Indeed, once an estimation
of cmin is obtained, many studies advocates the choice of 2
cmin which turn out to be
optimal ([Mas07], [AM09] among others). Now we present the dimension jump algorithm.

Algorithm 1. Dimension Jump [AM09]

b

1. Compute the selected model

m(c) as a function of c > 0

m(c) = argmin
m∈M

Ln

θm

+ c penshape(m);
b
(cid:1)

(cid:0)
b
b
cmin such that D bm(c) is "huge" for c <

b

2. Find
cmin;

penshape(m) = Dm log log n

cmin and "reasonnably small" for c

≥

b
3. Select the model

b

m :=

m(2

cmin).

b

This algorithm has been implemented in [BMM12] which gives several details including

the grid for c values.

b

b

b

Let us notice that, in view of obtaining penalties, there is no need to calibrate the cmin
constant for most classical time series models. However, since the fourth order moment
of the noise is unknown, a consistent estimate of this term is required. To do that, we
proceed as in the estimation of the variance of the noise as in the Mallows Cp.

A consistent estimator

µm,4 of µ4 = E[ξ4

0] can be :

1
n

n

b

t=1
X

(cid:0)

µm,4 :=

b

ξt(m)

4 with

ξt(m) :=

−1

M t
b
θm

(cid:1)

b

b

(cid:0)

c

(cid:1)

Xt −
(cid:0)

f t
b
θm

(cid:1)

b

where we suppose that m is the "largest" model in the family
, typically the largest
order of a family of time series. As a result an estimator of the cmin constant to consider
in the penalty κn is

M

•

bµm,4−1
4

for GARCH family and related ones;

• 1

2 for ARMA family with known variance.

8

3 Numerical Experiments

In this section, several numerical experiments are conducted to assess the consistency
property (Section 2) of our new criteria.

3.1 Monte Carlo: Consistency

This subsection studies the performance of the model selection criteria found in Section 2.
We have considered three diﬀerent Data Generating Process (DGP):

DGP I

DGP II

DGP III

Xt = 0.5 Xt−1 + 0.2 Xt−2 + ξt,
t−1 + 0.2 X 2

Xt =
Xt = 0.1( Xt−1 + Xt−2 + . . . + Xt−6) + ξt,

0.2 + 0.4 X 2

1/2 ξt,

t−2

(cid:0)

(cid:1)

where (ξt) will be a white Gaussian noise with variance one at ﬁrst and a Student with 5
degrees of freedom on the other hand. For the ﬁrst and the second DGP, we considered as
competitive models all the models in the family

deﬁned by:

M

=

ARMA(p, q) or GARCH(p′, q′) processes with 0

p, q, p′

5, 1

q′

≤

≤

5

.

≤

≤

M

(cid:8)

Therefore, there are 66 candidate models as in [BKK20]. The goal is to compare the ability
of selecting the true model for κn = log n Dm, κn = 2
cmin log log n Dm ( in accordance
with the condition (2.17) and κn = 2
cmin log log n Dm. Moreover, from Theorem
cmin does not need to be estimated and worth one half for Gaussian noise. But for
2.2
for DGP II.
Student noise,
b
The Table 1 presents the results of the selection procedure. As we can notice, the three
penalties have a good consistency property. Moreover, for n relatively small, the penalty
κn = 2
cmin log log n is
the best the penalty to consider.

cmin log log n Dm is better than both others. For larger n, κn =

for the DGP I and

cmin = max

bµm,4−1
4

bµm,4−1
4

cmin =

1
2 ,

×

b

b

b

b

2

(cid:0)

(cid:1)

(cid:9)

b

b

Table 1: Percentage of selected order based on 500 independent replications depending on
cmin and log n, where and W, T, O refers to wrong,
sample’s length for the penalty
true and overﬁtted selection.

cmin, 2

n

b

100
cmin log n

b

cmin 2

cmin 2

W 58.2
b
DGP I
T
32.6
Gaussian O
9.2

W 70.0
DGP II
T
29.8
Gaussian O
0.2

DGP I
Student

DGP II
Student

W 70.1
T
19.8
O
10.1

W 75.0
T
22.4
O
2.6

80.8
b
19
0.2

84.8
15.2
0

86.8
13.2
0

94.6
5.4
0

94.0
6.0
0

97.5
2.5
0.

84.2 88.8
11.2
15.0
0.0
0.8

23.6
b
64.2
12.2

17.4
81.0
1.6

38.3
59.7
2.0

45.6
49.0
4.4

500
cmin log n

1000
cmin log n

cmin 2

2000
cmin log n

cmin 2

26
b
73.4
0.6

21.6
78.4
0

53.1
46.9
0

57.2
41.2
1.6

36.2
63.8
0

35.4
64.6
0.

35.0
65.9
0.1

55.4
44.6
1.0

18.8
b
71.6
9.6

16.4
83.4
0.2

16.0
83.9
0.1

25.6
71.0
3.4

17.6
b
82.2
0.2

17.8
82.0
0.2

15.8
84.2
0

19.5
80.5
0

32.6
66.8
0.6

15.8
84.2
0.0

16.3
82.7
0

26.2
73.8
0.0

4.2
b
89.4
6.4

3.6
96.4
0

18
81.4
0.6

14
85.0
1.0

7.4
b
92.2
0.5

3.6
96.4
0

22.0
78.0
0

16.0
84.0
0

7.8
92.2
0

5
95
0

18
82.0
0

13.0
87.0
0

For DGP III, as we want to exhibit the possible "non consistency" of BIC for small samples,
we have considered as the competitive set, the hierarchical family of AR models

′ =

M

(cid:8)

AR(1), . . . , AR(15)

9

(cid:9)

for n = 100, 200, 400, 500, 1000, 2000.
based on 1000 independent replications are presented for the above three penalties.

In Table 2 below the percentage of selected order

Table 2: Percentage of selected order based on 1000 independent replications depending
on sample’s length using penalty terms κ1
n = log n, for DGP
n =
III.

cmin and κ3

cmin, κ2

n = 2

n

100
n κ2
κ1

n κ3
n

200
n κ3
n κ2
κ1
n

κ1
n

b

400
n κ3
κ2
n

500
κ2
n

κ1
n

b

κ3
n

κ1
n

1000
n κ3
κ2
n

2000
κ2
n

κ3
n

κ1
n

p < 6
p = 6
p > 6

61.5 96 99
15
3.0 1.0
23.5 1.0 0

46 83.5 97
25 13
2.5
29 3.5 0.5

29.5 59 87.5
44 37.5 12.5
26.5 3.5 0

19.5 45.5 76.5
50.5 49.5 22.5
1.0
30 5.0

3.0 9.5 38.5
68 86.5 61
29 4.0 0.5

0
0.5 5.0
72.5 96 94.5
27.5 3.5 0.5

These results invite us to be cautious when using the BIC for small sample sizes, whereas
the proposed adaptive penalty is more robust, as it at least allow us to recover an overﬁtted
model that is less harmful than a wrong model most often chosen by the BIC.

3.2 Real Data Analysis: ﬁnancial time series

CAC 40 is a benchmark French stock market index and is highly regarded in many statis-
tical studies . Let consider the daily closing prices of the CAC 40 index from January 1st,
2010 to December 31st, 2019 plotted in Figure 1. Over the period under review, the CAC
40 increased.

To analyze this type of data, it is common to consider the returns (see Figure 2). We
can see that the return values display some small auto-correlations. Also, from Figure
3, the squared returns of CAC 40 are strongly auto-correlated. These facts suggest that
the strong white noise assumption cannot be sustained for this log-returns sequence of the
CAC 40 index.

Hence, let consider the competitive set of models
in order to propose the best suitable model for these data:

M

used in the previous subsection

=

ARMA(p, q) or GARCH(p′, q′) processes with 0

p, q, p′

5, 1

q′

≤

≤

5

.

≤

≤

M

(cid:8)

Using the adaptive penalty and the BIC criterion, we ﬁnd out that the GARCH(1, 1) is the
with respect to both criteria. This fact is in accordance with [FZ10]
best model over
which found the same result using the returns of the CAC 40 index from March 2, 1990 to
December 29, 2006.

M

(cid:9)

4 Proofs

4.1 Proof of Theorem 2.1

Usually, this proof is divided into two parts: one has to show that as n tends to inﬁnity
the probability of overﬁtting goes to zero and so is the probability of misspeciﬁcation.

4.1.1 Overﬁtting Case

Let m
We have

∈ M

such as m∗

⊂

m. We want to show that C(m∗)

C(m) a.s. asymptotically.

≤

C(m∗)

C(m)

≤

⇐⇒ −
Ln

⇐⇒

2 Ln

θ(m∗)

Ln(θ∗)
θ(m)
(cid:1)
(cid:0)
b
2 log log n
(cid:1)
b

−

(cid:0)

+ 2c log log n Dm∗
θ(m∗)

Ln

≤ −

−

2 log log n
(cid:1)

θ(m)

2 Ln
Ln(θ∗)
(cid:0)
(cid:1)
+ c (Dm −
b

+ 2c log log n Dm

Dm∗ ) (4.1)

b

≤

(cid:0)

10

0
0
0
6

0
0
5
5

0
0
0
5

0
0
5
4

0
0
0
4

0
0
5
3

0
0
0
3

0

500

1000

1500

2000

2500

Time

Figure 1: Daily closing CAC 40 index (January 1st, 2010 to December 31 st, 2019).

5
0
.
0

0
0
.
0

5
0
.
0
−

0
.
1

8
.
0

6
.
0

2
.
0

0
.
0

0

500

1000

1500

2000

2500

0

10

Time

20

Lag

30

4(cid:0)

(a) Time plot of log returns.

(b) Correlograms of log returns.

Figure 2: Daily closing CAC 40 index (January 4th, 2010 to December 31 st, 2019).

therefore, a necessary and suﬃcient condition to avoid overﬁtting can be stated by taking
on both sides of the inequality (4.1); that is by virtue of deﬁnition (2.7)
lim sup
n→∞

cmin(m)

−

cmin(m∗)

c (Dm −

≤

Dm∗ )

for m∗

m,

⊂

(4.2)

i.e.,

α1
4

(D1

m −

D1

m∗ ) +

α2
4

(D2

m −

D2

m∗ )

c (Dm −

≤

Dm∗ )

which is fulﬁlled for any constant c such as in (2.8). Indeed, cmin = max

4 , α2
α1

4

and

α1
4

(D1

m −

D1

m∗ ) +

α2
4

(D2

m −

D2

m∗ )

D1
cmin
m −
≤
(cid:16)
= cmin(Dm −

D1

m∗ + D2

(cid:0)
m −

Dm∗ )

D2

m∗

(cid:1)

(cid:17)

where the inequality holds since m∗
the associated criterion κn will avoid overﬁtting.

⊂

m that implies D1

m ≥

D1

m∗ and D2

m ≥

D2

m∗ . Hence

11

0
(cid:1)
(cid:2)
0

.

1

8
0

.

6
0

.

2
0

.

0
0

.

0

10

20

Lag

30

(cid:3)(cid:4)

Figure 3: Sample autocorrelations of squared returns of the CAC 40 index (January 1st,
2010 to December 31 st, 2019).

4.1.2 Misspeciﬁcation/Underﬁtting Case

All misspeciﬁed/underﬁtted models are contained in the set

′ =

M

m

∈ M

: (m∗

m)

∪

6⊆

(m

⊂

m∗)

.

n

o

The proof is exactly as the one done in [BKK20]. But for the sake of completeness, we
give here some important steps of the proof.
The goal is to show that for every m

′, it holds

∈ M

C(m∗) < C(m) a.s.

(4.3)

′. From Proposition 2 in [BKK20] and using continuous mapping Theorem

∈ M

1
n

Ln

θ(m∗)

−

h

(cid:0)

b
A(m) := L(θ∗)

(cid:1)

(cid:0)

b
L(θ∗(m))

−
Xt−1, Xt−2,

(cid:0)
L(θ∗)

L(θ) =

−

· · ·
1
2

(cid:1)
E

Ln

θ(m)

= A(m0) + oa.s(1)

(4.4)

(cid:1)i

with L(θ) =

1
2

−

E[q0(θ)].

E

q0(θ)

h

(cid:2)

q0(θ∗)

−

| F0

.

(cid:3)i

(4.5)

Let us denote by

Ft := σ

. Using conditional expectation, we obtain

Let m

where

But,

E

q0(θ)

q0(θ∗)

−

| F0

(cid:2)

(cid:3)

θ∗)2
f 0

(X0 −
H 0
θ∗

= E

h
= log

(cid:16)

= log

θ )2
f 0

(X0 −
H 0
θ

E

+

(cid:17)

H 0
θ
H 0
θ∗
H 0
θ
H 0
θ∗

=

(cid:16)
H 0
θ∗
H 0
θ −

(cid:17)
log

+ log(H 0
θ )

(X0 −
(cid:2)

E

−
θ )2
f 0
H 0
θ
(X0 −

1 +

−
H 0
θ∗
H 0
θ

(cid:16)

(cid:2)

−

(cid:17)

1 +

| F0

−
(cid:3)
θ∗ + f 0
f 0
θ∗
H 0
θ
θ )2
f 0

−
H 0
θ

(f 0
θ∗

| F0

| F0

i

(cid:3)

log(H 0

θ∗)

E

−
(X0 −

(cid:2)
θ )2
f 0

−

θ∗)2
f 0
H 0
θ∗
| F0

(cid:3)

12

(cid:5)
(cid:6)
(cid:7)
Thus from (4.5),

2 A(m) = E

1 +

H 0

H 0

(cid:16)

−

log

H 0

H 0
θ∗
θ∗(m) −
H 0
θ∗
θ∗(m) i
(cid:16)
1 > 0 for any x > 0, x

H 0
θ∗
θ∗(m) (cid:17)
H 0
θ∗
E
θ∗(m) i(cid:17)

H 0

log

−

h

h

E

≥

h
log(x)

(f 0
θ∗

θ∗(m))2
f 0

−
H 0

θ∗(m)
(f 0
θ∗

1 + E

−

h

−

Since x
that

−

−

• If f 0
θ∗

= f 0

θ∗(m) then E

• Otherwise, then

(f 0

θ∗ −f 0
θ∗
H 0
θ∗

(m)

(m))2

> 0 and A(m) > 0.

h

i

i
θ∗(m))2
f 0

−
H 0

θ∗(m)

by Jensen Inequality.

i

= 1 and x

log(x)

−

1 = 0 for x = 1, we deduce

A(m) = E

H 0
θ∗
θ∗(m) −

H 0

log

h

H 0

H 0
θ∗
θ∗(m) (cid:17)
θ∗ = f 0

−

(cid:16)

1

,

i

and from assumption A1, since θ∗ /
∈

Θ(m) and f 0

θ∗(m), we necessarily have

H 0
θ∗

= H 0

θ∗(m) so that

H 0
θ∗
H 0
(m) 6
θ∗

= 1. Then A(m) > 0.

As a consequence,

C(m∗)

C(m)

−
n

= A(m) +

2 c κn
n

(Dm −

Dm∗) + oa.s(1).

That establishes (4.3) by virtue of (2.8) and as all the considered models are ﬁnite dimen-
(cid:4)
sional.

In the sequel, we state and prove several lemmas to which we referred to when proving
above main results.

4.1.3 Proof of Theorem 2.3

The proof follows mutatis mutandis from the Theorem 2.1’s proof after replacing line by
C respectively and
line Ln,
(cid:4)
applying Lemma 2 instead of Proposition 1.

θ(m) and the criterion C by their equivalent

θ(m) and

Ln,

b

b

e

b

4.1.4 Proof of Proposition 1

Proof. Applying a second order Taylor expansion of Ln around
large such that θ(m)
θ(m)) = 0 since
∂Ln(

∈
θ(m) is a local extremum):

Θ(m) which are between θ∗

m := (θ∗, 0, . . . , 0)⊤ and

θ(m) for n suﬃciently
θ(m) yields (as

b

b

θ(m)

θ∗
m

−

:= I1(m).

(cid:0)

b

(cid:1)

Ln(θ∗)

b

−

Ln(

θ(m)) = Ln(θ∗

m)

b

b

=

1
2

θ(m)

−

−

Ln(

θ∗
m

θ(m))
⊤ ∂2Ln(θ(m))
b
∂θ2

From the mean value Theorem, and for large n, there exists θm,i between (θ∗
such that, 1

i

Dm:

m)i and (

θ(m))i

(cid:0)

b

(cid:1)

≤

≤

0 =

θm)

∂Ln(
∂θi
b

=

m)

∂Ln(θ∗
∂θi

+

∂2Ln(θm,i)
∂θ∂θi

θm −
(
b

Also, using Lemma 4 of [BW09] and continuous mapping Theorem, we deduce that:

θ∗
m)

b

(4.6)

2
n

∂2Ln(θm,i)
∂θ∂θi

Fn :=

−

(cid:16)

1≤i≤Dm

(cid:17)

a.s.
−→n→+∞

F (θ∗

m) = E

m)

∂2q0(θ∗
∂θ2

.

i

h

(4.7)

13

6
6
6
On the other hand, under A2 condition, F (θ∗
suﬃciently large such that Fn is invertible. Therefore, from (4.6), it follows

m) is an invertible matrix and there exists n

I1(m)
2 log log n

=

=

=

1
4 log log n
1
4 log log n

(cid:0)

(cid:16)

1
2n log log n

−

θ∗
m

θ(m)

(cid:1)
⊤

m)

−
∂Ln(θ∗
b
∂θ
(cid:17)
∂Ln(θ∗
m)
∂θ

⊤ ∂2Ln(θ(m))
∂θ2
2
n

F −1
n

−
(cid:16)
⊤

(cid:17)
m)−1
F (θ∗

θ(m)

θ∗
m

−

(cid:1)

b

(cid:0)
∂2Ln(θ(m))
∂θ2
(cid:16)
∂Ln(θ∗
m)
∂θ

×

(cid:16)

×

(cid:17)

The next step of the proof consists in handling the quadratic form

∂Ln(θ∗
∂θ

m)

by applying the law of iterated logarithm (LIL).

We claim that
(cid:16)

(cid:17)

2
n

−

F −1
n

(cid:17)
1 + o(1)

m)

∂Ln(θ∗
∂θ

a.s.

(cid:0)
∂Ln(θ∗
∂θ

m)

⊤

(cid:17)

(cid:16)

(cid:1)
F (θ∗

×

m)−1

×

lim sup
n→∞

1
√2n log log n

2 G(θ∗

m)−1/2 ∂Ln(θ∗
m)

∂θ

=

1, . . . , 1

⊤.

(4.8)

(cid:0)
Proof of the claim: First, since the covariance matrix of 2 ∂Ln(θ∗
m)
m)−1/2 ∂Ln(θ∗
that the covariance matrix of the vector Zm := 2 G(θ∗
identity matrix. Moreover, as

∂θ

∂θ

m)

(cid:1)
is G(θ∗

m), it follows
Dm

is the Dm ×

m)

∂Ln(θ∗
∂θ

=

1
2

−

m)

∂qt(θ∗
∂θ

=

1
2

−

n

t=1
X

∂qt(θ∗)
∂θ

,

n

t=1
X

where

and

∂qt(θ∗)
∂θ

E

h

σ(Xt−1, Xt−2, . . . , X1)
(cid:12)
i
(cid:12)
(cid:12)
E

2

∂q1(θ∗)
∂θi

<

∞

h(cid:16)

(cid:17)

i

= 0

(4.9)

(4.10)

hold from [BW09] under A4. Finally, one can see that the ith element of Zm can be
rewritten as

where ζ i

t =

−

Dm
j=1

P

(cid:0)

Dm

2 G(θ∗

m)−1/2

Xj=1
(cid:0)
m)−1/2

G(θ∗

∂qt(θ∗)
∂θj

i,j

m)

∂Ln(θ∗
∂θj

=

i,j

(cid:1)

n

ζ i
t

t=1
X

. By virtue of (4.9), we have

= 0.

E

(cid:1)
ζ i
t

h

σ(Xt−1, Xt−2, . . . , X1)
(cid:12)
i
(cid:12)
(cid:12)
1
√2n log log n  

2 G(θ∗

m)−1/2 ∂Ln(θ∗
m)
∂θ !i

= 1.

lim sup
n→∞

Hence, any component of Zm veriﬁes the LIL. That is for any i = 1, . . . , Dm,

This fact concludes the proof of the claim (4.8).

Hence writing

1
2n log log n

m)

∂Ln(θ∗
∂θ

⊤

F (θ∗

m)−1 ∂Ln(θ∗
m)

=

∂θ
⊤

1
√2n log log n

it follows

2G(θ∗

m)−1/2 ∂Ln(θ∗
m)
∂θ !

G(θ∗

m)−1G(θ∗
m)1/2F (θ∗
4

m)1/2

lim sup
n→∞

Ln

(cid:0)

Ln(θ∗

m)

−

θ(m)
2 log log n
(cid:1)
b

= 1⊤

m Σθ∗

m 1m.

14

1
√2n log log n

2G(θ∗

m)−1/2 ∂Ln(θ∗
m)
∂θ !

(cid:4)

 
 
∂H 0
θ∗
m
∂θj

i
(4.12)

.

4.1.5 Proof of Proposition 2

It is suﬃcient to show that

Σθ∗

m := G(θ∗

m)1/2F (θ∗

m)−1G(θ∗

m)1/2 =

(cid:18)

2 ID1
OD2

m,D1
m
m,D1

OD1

m,D2
m
1) ID2
m,D2

m (cid:19)

m (µ4 −

,

(4.11)

where I is the identity matrix and O the null matrix. From [BW09], we have for m∗
and i, j

m:

∈

m

⊂

G(θ∗

m)

i,j = E

h

(cid:0)

(cid:1)
m)

F (θ∗

i,j = E

m)

∂q0(θ∗
∂θi
∂2q0(θ∗
∂θi∂θj

m)

m)

∂q0(θ∗
∂θj

= E

i
2(H 0
θ∗
m
h

= E

i

(cid:0)
1/ If µ4 = 3, then G(θ∗

(cid:1)

h

4(H 0
θ∗
m
h
)−1

∂f 0
θ∗
m
∂θi

)−1

∂f 0
θ∗
m
∂θi

∂f 0
θ∗
m
∂θj

∂f 0
θ∗
m
∂θj

+ (H 0
θ∗
m

+ (µ4 −
∂H 0
θ∗
m
∂θi

)−2

1) (H 0
θ∗
m

)−2

∂H 0
θ∗
m
∂θi

∂H 0
θ∗
m
∂θj

i

m) = 2 F (θ∗

m) and the result is straightforward.

2/ For the second conﬁguration, from [BKK21], we have

G(θ∗

m) =

(G(θ∗

1≤i,j≤D1
m

m)
OD2
m,D1
(cid:1)
m

(µ4 −
and G(θ∗

OD1
1) (G(θ∗

m,D2
m
m)

1≤i,j≤D2

m)F (θ∗

(cid:1)
m)−1 =

m !
2 ID1
OD2

m,D1
m
m,D1

OD1

m,D2
m
1) ID2
m,D2

m (µ4 −

As a covariance matrix, G(θ∗

of G(θ∗

m) is unique and blocks diagonal. Thus,

m (cid:19)
m) is positive deﬁnite. Therefore the square root G(θ∗

(cid:18)

m)1/2

Σθ∗

m = G(θ∗
= G(θ∗

m)−1/2
(cid:16)
m)−1,
m)F (θ∗

G(θ∗

m)F (θ∗

m)−1

G(θ∗

m)1/2

(cid:17)

which gives (4.11).

4.2 Technical Lemmas

Lemma 1. Let X
Assume that assumption A3 holds. Then for i = 0, 1, 2, it holds

(Mθ∗, fθ∗) (or

(Hθ∗)) and Θ

∈ H

H

⊆

Θ(r) (or Θ

(cid:4)

Θ(r)) with r

4.

≥

⊆

1
log log n

∂(i)

e
Ln(θ)
∂θi
b

(cid:13)
(cid:13)
(cid:13)

Θ

(cid:13)
(cid:13)
(cid:13)

∂(i)Ln(θ)
∂θi

−

e

a.s.
−→n→+∞

0.

(4.13)

Proof. This Lemma has already been proved in [BKK20] in a more general framework. Let
us prove the result for i = 0. Other cases can be deduced by using a similar reasoning.

We have for any θ

Θ,

Ln(θ)
|

−

∈

Ln(θ)

| ≤

1
log log n

b
Ln(θ)

P

Ln(θ)

−

Θ ≤

n
t=1 |

qt(θ)

1
b
log log n

By Corollary 1 of [KW69], (4.13) is established when:

(cid:13)
(cid:13)b

(cid:13)
(cid:13)

. Then,
qt(θ)
|

−
n

Xt=1

qt(θ)

−

qt(θ)

Θ.

(cid:13)
(cid:13)b

(cid:13)
(cid:13)

1
log log k

E

qk(θ)

qk(θ)

−

Θ <

.

∞

(4.14)

Xk≥1

(cid:13)
(cid:13)b

(cid:13)
(cid:13)

From [BW09] and [BKK20], there exists a constant C such that
1/ If X

(Mθ, fθ), we deduce

∈ H

15

 
E

(cid:2)

qt(θ)
(
k

−

qt(θ)

kΘ

C

≤

b

(cid:3)

Hence,

αj(fθ, Θ) +

αj(Mθ, Θ)

.

(4.15)

(cid:16) Xj≥t

Xj≥t

(cid:17)

1
log log k

E

qk(θ)
k
(cid:2)
which is ﬁnite by deﬁnition of the class

qk(θ)

Xk≥1

kΘ

−

b

(cid:3)

≤

C

H

1
log log k

αj(fθ, Θ) + αj(Mθ, Θ)

,

Xk≥1
(Mθ, fθ), and this achieves the proof.

(cid:16) Xj≥k

(cid:17)

2/ If X

Hθ),
(

∈

H

E

e

e

This fact along with Corollary 1 of [KW69] enable us to conclude the proof in this case. (cid:4)

b

qt(θ)

kΘ

−

C

≤

qt(θ)
(
k
(cid:2)

(cid:3)

(cid:16) Xj≥t

(cid:17)

αj(Hθ, Θ)

.

(4.16)

Lemma 2. Under the assumptions of Theorem 2.3, for any model m
it holds

with θ∗

∈

∈ M

o
^
Θ(m),

lim sup

n→∞  

Ln

b

Ln(θ∗)

−

θ(m)
2 log log n
(cid:1)
e
b

(cid:0)

= cmin(m) a.s.

(4.17)

!

Proof. Applying a second order Taylor expansion of
large such that θ(m)
θ(m)) = 0 since
∂

∈
θ(m) is a local extremum):

Θ(m) which are between θ∗

Ln(

Ln around

m := (θ∗, 0, . . . , 0)⊤ and

e

e

θ(m) for n suﬃciently
θ(m) yields (as

b

e

Ln(θ∗)

Ln
e

−

θ(m)

b

(cid:0)

e

b

(cid:1)

But I2(m) can be rewritten as

I2(m)
2 log log n

=

1
4

θ(m)

−

⊤

θ∗
m

(cid:0)
e
+

(cid:1)

1
log log n

1
4

m)

=

Ln(θ∗
1
b
2
:= I2(m).
(cid:0)

=

θ(m)

e

Ln

(cid:0)
θ∗
b
m

−

−

θ(m)
⊤ ∂2
e

Ln(θ(m))
(cid:1)
∂θ2

(cid:1)

b

e

θ(m)

θ∗
m

−

(cid:0)

e

(cid:1)

1
log log n "

∂2

Ln(θ(m))
∂θ2

−

∂2Ln(θ(m))
∂θ2

θ(m)

θ∗
m

−

b
⊤ ∂2Ln(θ(m))
∂θ2

θ(m)

θ∗
m

−

#

θ(m)

θ∗
m

−

(cid:0)

e

(cid:1)

(cid:0)

e

(cid:1)

(cid:0)

e

(cid:1)=: I21(m) + I22(m).

First, as

θ(m)

a.s.
−→n→+∞

θ∗
m along side with Lemma 1, it follows

e

I21(m)

a.s.
−→n→+∞

0.

Writing a counterpart of (4.6) using the quasi-functions, we have

θ(m)

−

θ∗
m =

Hence I22(m) can be rewritten as

e

∂2

Ln(θ(m))

(cid:16)

b

(cid:17)

−1 ∂

m)

Ln(θ∗
∂θ

b

(4.18)

θ(m)

θ∗
m

⊤ ∂2Ln(θ(m))
∂θ2

θ(m)

θ∗
m

−

I22(m) =

=

=

1
4 log log n

1
4 log log n

(cid:0)

∂
e

(cid:16)

1
2 n log log n

−

(cid:1)
⊤

m)

−
Ln(θ∗
∂2
∂θ
(cid:17)
(cid:16)
∂Ln(θ∗
m)
∂θ

b

⊤

(cid:16)

(cid:17)

(cid:1)
∂2Ln(θ(m))
(cid:16)

m)

∂Ln(θ∗
∂θ

×

(cid:0)

−1
e

Ln(θ(m))

(cid:17)
m)−1

F (θ∗

b
×

16

∂2

Ln(θ(m))

−1 ∂

(cid:17) (cid:16)
b
1 + o(1)

(cid:17)

a.s.

(cid:0)

(cid:1)

m)

Ln(θ∗
∂θ

b

(4.19)

since from (4.7), it holds

n
2

−

and along with Lemma 1, it also holds

∂2Ln(θ(m))
(cid:17)

(cid:16)

−1

a.s.
−→n→+∞

F (θ∗

m)−1

log log n

∂2

Ln(θ(m))

−1

n
2

−

a.s.
−→n→+∞

F (θ∗

m)−1.

(cid:16)
As a consequence, the chain of following equalities holds a.s.

(cid:17)

b

∂Ln(θ∗

m)
∂θ !

×

(cid:4)

lim sup

n→∞  

Ln

(cid:0)

b

Ln(θ∗)

−

θ(m)
2 log log n
(cid:1)
e
b

= lim sup

n→∞   −

!

= cmin(m)

1
2 n log log n

m)

∂Ln(θ∗
∂θ

⊤

(cid:17)

×

(cid:16)

F (θ∗

m)−1

That ends the proof of (4.17).

5 Acknowledgements

The author thanks Jean-Marc BARDET for proofreads and helpful discussions.

17

References

[Aka73] H Akaike. Information theory and an extension of the maximum likelihood principle.
Proceedings of the 2nd international symposium on information, Akademiai Kiado, Bu-
dapest, 1973.

[AM09]

S. Arlot and P. Massart. Data-driven calibration of penalties for least-squares regression.
Journal of Machine learning research, 10:245–279, 2009.

[BKK20] J.-M. Bardet, K. Kamila, and W. Kengne. Consistent model selection criteria and
goodness-of-ﬁt test for common time series models. Electronic Journal of Statistics,
14(1):2009–2052, 2020.

[BKK21] J.-M. Bardet, K. Kamila, and W. Kengne. Eﬃcient and consistent data-driven model

selection for time series. Preprint, 2021.

[BMM12] J.-P. Baudry, C. Maugis, and B. Michel. Slope heuristics: overview and implementation.

Statistics and Computing, 22(2):455–470, 2012.

[BW09]

J.-M. Bardet and O. Wintenberger. Asymptotic normality of the quasi-maximum
likelihood estimator for multidimensional causal processes. The Annals of Statistics,
37(5B):2730–2759, 2009.

[DGE93] Z. Ding, C. Granger, and R.F. Engle. A long memory property of stock market returns

and a new model. Journal of empirical ﬁnance, 1(1):83–106, 1993.

[DW08] P. Doukhan and O. Wintenberger. Weakly dependent chains with inﬁnite memory.

Stochastic Processes and their Applications, 118(11):1997–2013, 2008.

[FZ10]

C. Francq and J.-M. Zakoian. GARCH models: structure, statistical inference and
ﬁnancial applications. John Wiley & Sons, 2010.

[Han80] E.J. Hannan. The estimation of the order of an arma process. The Annals of Statistics,

8(5):1071–1081, 1980.

[HD12]

[HQ79]

Edward James Hannan and Manfred Deistler. The statistical theory of linear systems.
SIAM, 2012.

Edward J Hannan and Barry G Quinn. The determination of the order of an autoregres-
sion. Journal of the Royal Statistical Society: Series B (Methodological), 41(2):190–195,
1979.

[Ken20] William Kengne. Strongly consistent model selection for general causal time series.

Statistics & Probability Letters, page 109000, 2020.

[KW69] E.G. Kounias and T. Weng. An inequality and almost sure convergence. The Annals of

Mathematical Statistics, 40(3):1091–1093, 1969.

[LM03]

S. Ling and M. McAleer. Asymptotic theory for a vector arma-garch model. Econometric
theory, 19(2):280–310, 2003.

[LMH04] E. Lebarbier and T. Mary-Huard. Le critère BIC: fondements théoriques et interpréta-

tion. PhD thesis, INRIA, 2004.

[Mas07] P. Massart. Concentration inequalities and model selection. Springer, 2007.

[Sch78] G. Schwarz. Estimating the dimension of a model. The Annals of Statistics, 6:461–464,

1978.

[Shi86]

[SM06]

Ritei Shibata. Consistency of model selection and parameter estimation. Journal of
Applied Probability, pages 127–141, 1986.

D. Straumann and T. Mikosch. Quasi-maximum-likelihood estimation in conditionally
heteroscedastic time series: A stochastic recurrence equations approach. The Annals of
Statistics, 34(5):2449–2495, 2006.

[Whi82] H. White. Maximum likelihood estimation of misspeciﬁed models. Econometrica, pages

1–25, 1982.

18

