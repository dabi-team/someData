1

EasyCom: An Augmented Reality Dataset to Support
Algorithms for Easy Communication in Noisy Environments

https://github.com/facebookresearch/EasyComDataset

Jacob Donley∗, Vladimir Tourbabin∗, Jung-Suk Lee∗, Mark Broyles∗,
Hao Jiang∗, Jie Shen†, Maja Pantic†, Vamsi Krishna Ithapu∗, Ravish Mehra∗

∗Facebook Reality Labs Research, USA.

†Facebook AI Applied Research, UK.

1
2
0
2

t
c
O
8
1

]

D
S
.
s
c
[

2
v
4
7
1
4
0
.
7
0
1
2
:
v
i
X
r
a

Abstract—Augmented Reality (AR) as a platform has the
potential to facilitate the reduction of the cocktail party effect.
Future AR headsets could potentially leverage information from
an array of sensors spanning many different modalities. Training
and testing signal processing and machine learning algorithms
on tasks such as beam-forming and speech enhancement require
high quality representative data. To the best of the author’s
knowledge, as of publication there are no available datasets
that contain synchronized egocentric multi-channel audio and
video with dynamic movement and conversations in a noisy
environment. In this work, we describe, evaluate and release a
dataset that contains over 5 hours of multi-modal data useful for
training and testing algorithms for the application of improving
conversations for an AR glasses wearer. We provide speech
intelligibility, quality and signal-to-noise ratio improvement re-
sults for a baseline method and show improvements across all
tested metrics. The dataset we are releasing contains AR glasses
egocentric multi-channel microphone array audio, wide ﬁeld-of-
view RGB video, speech source pose, headset microphone audio,
annotated voice activity, speech transcriptions, head bounding
boxes, target of speech and source identiﬁcation labels. We have
created and are releasing this dataset to facilitate research in
multi-modal AR solutions to the cocktail party problem.

Index Terms—augmented reality (AR), cocktail party, beam-
forming, speech enhancement, egocentric, audio, video, dataset

I. INTRODUCTION

T HE ability to communicate in noisy environments is a

challenge for many people and has been a popular research
topic for many decades. The difﬁculty that arises is often
referred to as the cocktail party effect [1], [2]. As our world
moves towards a new era of computing in augmented reality
(AR) and virtual reality (VR) there are many beneﬁts to be
had from an all day wearable computing device, including the
potential to eliminate the cocktail party effect once and for all.
The biggest obstacle to developing systems to overcome this
effect is a lack of realistic data that reﬂects the speciﬁc scenario
and provides multi-modal information. Machine learning and
signal processing algorithms can beneﬁt from leveraging high
quality multi-sensor data, however, until now, there has not
been a single dataset that provides all relevant sensor data from
an AR headset in an environment that induces the cocktail
party effect.

A critically differentiating aspect of an AR headset-based
dataset is its egocentric nature and the associated dynamic

Please send correspondence to: EasyComDataset@fb.com

movement. There are few egocentric datasets publicly available
that also provide data useful for solving the cocktail party
effect. The Epic Kitchens dataset [3], [4], originally released
in 2018 and an extended version released in 2020, contains
egocentric video and single channel audio recordings of
activities that took place in kitchens. The environments that
were recorded do not contain noise or conversations that are
useful for solving the cocktail party problem. The COSINE
speech dataset [5], released in 2009, contains egocentric audio
recordings with seven microphones and in naturally noisy
environments. However, the dataset does not contain other
sensor modalities, such as video, nor does it contain annotated
labels or data from a head mounted device. The EgoCom
dataset [6], released in 2020, contains egocentric video and
two channel audio recordings of conversations from pairs of
glasses. However, the dataset does not contain acoustic noise
necessary for the problem, there is no clock synchronization
between devices and the multi-channel microphone data is lossy
compressed. The Amazon Dinner Party Corpus (DiPCo) [7],
released in 2019, contains 39 channels of audio as well as
annotations to speciﬁcally help address the cocktail party
problem in the form of a dinner party. The CHiME-5 [8]
and CHiME-6 [9] challenge datasets, released in 2018 and
re-run in 2019, respectively, contain 32 audio channels of 4
participants having a dinner party in private homes. The DiPCo,
CHiME-5 and CHiME-6 datasets, however, do not contain any
video or rigid wearable microphone array recordings. The lack
of egocentric perspective limits the use of the datasets and
does not facilitate solving the cocktail party problem for users
of wearable devices.

In order to reduce the cocktail party effect, it is desirable
to address the problem from many different domains and
types of systems, such as voice/speech activity detectors,
speaker diarization algorithms, direction of arrival estimators,
multi-channel beamforming algorithms, single-channel audio
and audio-visual speech enhancement algorithms, automatic
speech recognition algorithms, and more. To train and test
algorithms and machine learning models for these systems,
it is beneﬁcial to use high-quality realistic multi-sensor data
that has been recorded in environments that closely match
that of where the cocktail party effect is present. Given the
limitation of the datasets mentioned earlier, we have collected
and are releasing a dataset that contains egocentric multi-
channel microphone audio, video, positional data, voice activity,

 
 
 
 
 
 
2

TABLE I
TYPES OF DATA INCLUDED IN EXISTING DATASETS AND EASYCOM.

Name

COSINE [5]
CHiME-5&6 [8], [9]
DiPCo [7]
Epic Kitchens [3], [4]
EgoCom [6]
EasyCom

Year

2009
2018/19
2019
2018/20
2020
2021

# Mics
(sync)

7
32
39
1
2
8 to 10

Video

Egocentric

Mics

Camera

Pose

Transcribed

Noisy
Speech

(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)

(cid:88)

(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)

(cid:88)

speech transcriptions and face bounding boxes. The data was
collected in a noisy restaurant-like room with loudspeakers
placed around the room. Participants sitting around a table
attempt to have natural conversations with each other. The
video data can be used for training and testing face detection
and recognition algorithms, pose estimation techniques, lip
reading, conversational dynamics predictors and more. Sensor
fusion of the audio and video modalities can be performed to
solve complex problems more efﬁciently, such as audio visual
speech enhancement. The multi-channel audio and annotations
provide high quality ground truth data and labels for intrusive
metrics used as loss/cost functions or for performance analysis.
We illustrate the differences between existing datasets and the
dataset released in this work in table I, which shows the key
features that exist in each of them, and we summarize the
types of problems that each dataset can investigate in table II.
To benchmark different systems using the dataset for
the application of mitigating the cocktail party effect, we
propose the following task; enhance a target speech source by
improving the signal to noise ratio (SNR), speech quality and
speech intelligibility whilst not distorting the original speech.
Algorithms running on this dataset would also need to cope
with the dynamic movement of the sensors that are worn by the
participants and should run in real-time, or at least be causal.
As a baseline starting point, we show that by building
a multi-channel acoustic beamformer that is optimized to
isotropic/diffuse noise we can obtain good improvements across
all three benchmarking measures with minimal distortions in
the target speech. The beamformer dynamically steers to the
desired speech source of interest, is used on all different wearers
of the AR glasses in the dataset and can be run in real-time.
In this paper, we ﬁrst describe an example task for which we
use the dataset in section II, which we follow with a general
description of the dataset in section III and where it can be
obtained in section IV. The baseline method used to for the
task is explained in section V. Finally, we present and discuss
the results for the baseline method in section VI and conclude
the paper in VII.

II. CONVERSATIONAL FOCUS TASK
The main goal to be accomplished by using this dataset is
that of conversational focus. The aim is for an algorithm to
operate on unseen data and enhance the speech of the person
who is intended to be heard by the AR glasses wearer. The
enhancement of speech is deﬁned such that there is no noise

or interference and that the target speech is undistorted from
that of the close microphone signals, or at least, perceptually
undistorted. Competing speech should be suppressed to either
an inaudible level or a level that is not distracting to the intended
conversation. It is also an aim to simultaneously remove all
noise. Temporal and spectral distortions or modulations should
not be apparent in the enhanced speech and any switching
between target speech sources should be unnoticeable.

In order for any solution to this task be practical, it is
imperative that the system run causally, in real-time and with
minimal latency. When considering the physical playback
mechanism, for an AR application, the levels and delay between
the sound originating from the scene and from the playback
device should be perceptually justiﬁed.

This task is deﬁned and driven by the motivation to improve
conversations in noisy environments, such as restaurants, bars,
social gatherings, etc. The cocktail party problem that motivates
this task is a problem that occurs for both normal hearing and
hard of hearing. It is, therefore, also the goal to derive a solution
that is equally performant for people at all levels of hearing
ability.

III. THE EASYCOM DATASET

In this section, we describe the dataset’s general setting,
contents and features. The dataset was collected in a room
approximately 6m × 7m × 3m (width×depth×height) with a
reverberation time to 60 dB (RT60) of 645 ms. A table was
located in the center of the room and between 3 to 5 participants
sat around the table for a session duration of approximately
30 min that was hosted by a single individual, referred to as the
‘server’. During each session 10 loudspeakers placed around
the room at varying heights played uncorrelated restaurant-
like noise where each channel signal was assigned a different
loudspeaker each session. The noise was played back at a sound
pressure level (SPL) of approximately 71 dB as measured at
the positions of the seated participants. There are 12 sessions
included in the dataset totaling approximately 5 h and 18 min
of conversational recordings. Additionally, there are 3 extra
sessions included, which contain minor errors, such as data
frame drops.

One participant per session was provided with an AR glasses
headset that recorded multi-channel audio and video, and was
tracked with tracking markers. The multi-channel microphone
array consisted of two binaural microphones (one in each ear)
and an additional 4 microphones distributed around the glasses,

3

TABLE II
TYPES OF PROBLEMS THAT CAN BE INVESTIGATED USING EXISTING DATASETS AND EASYCOM.

Use Case

COSINE
[5]

Epic Kitchens
[3], [4]

CHiME-5&6
[8], [9]

DiPCo
[7]

EgoCom
[6]

EasyCom

Voice/Speech Activity Detection
Speaker Diarization
Automatic Speech Recognition (ASR)
Noise Robust ASR
Single-Ch Speech Enhancement
Single-Ch Speech Separation
Single-Ch Noise Reduction
Multi-Ch Speech Enhancement
Multi-Ch Speech Separation
Multi-Ch Noise Reduction
Acoustic Beamforming
Audio-Visual Speech Enhancement
Audio-Visual Speech Separation
Audio-Visual Noise Reduction
Silent Video Speech Reconstruction
Moving-Array Acoustic Beamforming
Direction of Arrival Estimation
Sound Source Localization and Tracking
Acoustic SLAM
Visual Face Tracking
Visual Body Tracking
Audio-Visual Sound Source Tracking
Conversational Dynamics

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

see Fig. 2 for approximate positions. The microphones recorded
audio at a sampling rate of 48 kHz with a bit depth of 32. The
video was recorded with a wide-angle camera at a resolution
of 1920 × 1080 and frame rate of 20 frames per second. The
camera lens provided an approximate horizontal ﬁeld of view
(FOV) of 120◦, vertical FOV of 66◦ and diagonal FOV of
139◦. All other participants in the session, except the ‘server’,
were each provided with faux glasses that contained tracking
markers and a headset microphone. Every participant, except
the ‘server’, had their head pose optically tracked with three
dimensional positions and rotations recorded at a rate of 20
frames per second. The headset microphones were recorded at
a sampling rate of 48 kHz with a bit depth of 32.

A. Methods and Procedure

The high level procedure undergone for recording the
dataset was to initially set up the room, initialize software
at the beginning of each session, prepare the hardware for
participants, record participant information, start the recording
process, run through several interaction tasks and stop/ﬁnish
the recordings. The initial room set up consisted of adjusting
acoustic panels on walls to approximate the reverberation level
of a typical occupied restaurant [10], [11] and the installation of
an OptiTrack1 optical tracking system. A table with surrounding
chairs and a separate area with a central recording computer
were also set up.

1https://optitrack.com/

Prior to recording each session the recording computer
software was initialized and prepared for recording. The
participants were seated based on the total number of par-
ticipants for the given session (e.g. see Fig. 3). The AR glasses
hardware containing the active sensors was ﬁtted to one of
the participants. All other participants were then ﬁtted with a
headset Rode microphone and faux AR glasses mounted only
with OptiTrack infra-red reﬂectors (optical tracking markers).
The participant wearing the AR glasses was asked to face each
other participant while an image of the participants faces were
captured. All participants were provided with faux names and
occupations, and the interactions procedure was described to
them. The sound level of the restaurant-like noise was gradually
increased to the speciﬁed SPL to approximate that of a typical
occupied restaurant [11] and then the data recording was started
immediately.

The participants began the interactions tasks when noise
started, coinciding with the beginning of the recording. The
interactions tasks were completely improvised. The ﬁrst task the
participants undertook was an introduction to each other using
their provided faux names and occupations. These conversations
were allowed to continue anywhere up to approximately 3.5
minutes or until the ‘server’ determined that there was a natural
end in the conversation, whichever occurred ﬁrst. The ‘server’
then proceeded to take everyone’s order from a faux menu,
provided to them at that time, by asking participants what they
would like to eat and drink from the menu. During pilot phases
of the data collection, it was not uncommon for participants
to order in a short time, so to facilitate longer interactions

4

B. Annotated Labels

We used human annotators to label Voice Activity (VA),
transcribe speech, and label Target of Speech (ToS) for the
dataset and used face and head detection models to generate
face and head bounding boxes for each participant in the
scene. To help aid annotations, we embedded photos of each
participant with a corresponding identiﬁcation (ID) number (1-
7) into the videos. In order to reduce the background restaurant-
like noise, and provide annotators with a clearer audio signal
for labeling, we mixed the original audio captured by the AR
glasses’ microphones with the headset microphone audio worn
by each participant.

1) Voice Activity: To perform annotations, we used Face-
book’s internal annotation platform, and utilized a video time
segmentation tool for all human annotations. The video time
segmentation tool used included an audio wave form display
and allowed annotators to focus to millisecond precision in
order to label the times of VA. Customizable labels and text
input were utilized for speech transcription and ToS labeling.
We deﬁned VA as any utterance by the participant. This
included any sound generated by the participant, including
speech, laughing, coughing, and hesitations. VA annotations
consisted of time segments of activity and a label to designate
the corresponding participant. Annotators were instructed to
watch the video and label VA for each participant sequentially
by ID number, limiting their focus to labeling one participant at
a time. Rejection options were included, which annotators used
to designate videos in which personally identiﬁable information,
inappropriate language or profanity was spoken.

2) Speech Transcriptions and Target of Speech: For speech
transcription and ToS annotations we used the VA annotation
results as predeﬁned responses, which allowed annotators to
append additional labels onto the VA annotations. Annotators
were instructed to watch the video, enter the transcribed speech
into a text box for each time segment of activity and designate
a label corresponding to the ToS. In addition to the participant
ID numbers (labeled as numbers 1-7) we included ‘Group-
Targeted Speech’ (labeled as 0) for when the participant was
addressing everyone in the scene and ‘Non-Targeted Speech’
(labeled as 8) when the participant did not address anyone in
the group (e.g. when reading sentences aloud). In instances
where more than one participant was a target of the speech,
but not the whole group, we allowed annotators to designate
secondary targets using extra labels.

Fig. 1. Example image frames from the dataset videos.

Fig. 2. The approximate microphone positions on the AR glasses are shown
with corresponding channel number. Channels 5 and 6 are binaural microphones.
This ﬁgure and the microphone positions are not a depiction of any future
product.

the ‘server’ would judge and follow up with further questions
about the menu order. The aim was for the menu ordering
to last approximately 3 minutes. After ordering, participants
menus were collected and they were all provided with a single
puzzle. The participants were encouraged to think out loud
to facilitate conversation or to ask for a new puzzle if they
were stuck. The ‘server’ would provide hints on the puzzles if
participants appeared stuck. After solving a puzzle the ‘server’
would provide a new puzzle and this was repeated until either
10 minutes had passed solving puzzles or the participants solved
them all, whichever came ﬁrst. Following the puzzle solving
task, all puzzles were collected and participants were asked
to play a game of “I spy”, chosen to facilitate simultaneous
discussion and head movement. If any of the participants did
not know the game, one of those who did was encouraged to
explain it to the rest and if not a single participant knew the
game, the ‘server’ would explain the game. The game of “I spy”
was allowed to continue for approximately 5 minutes, after
which the ‘server’ would interrupt for the next task. The ﬁnal
task of the recording procedure was to ask each participant to
read a set of 20 random command-like sentences from a list
(chosen for automatic speech recognition -like tasks). Once
the sentences had ﬁnished being read by each participant the
recording was stopped/ﬁnished, the devices were removed from
participants and they were allowed to leave.

After

recording,

the dataset underwent

several post-
processing stages which are outlined in Appendix A. Cali-
bration data for the inter- and intra- modality relationships
is included in the dataset and is described in more detail in
Appendix B.

5

voting result of the IDs automatically assigned at each frame.
We then select one representative image from each tracklet
based on head pose and facial landmark detection results given
by FAN [14] and send these images for manual annotation.
Comparing to frame-level annotation, this setup reduces the
total workload by a factor of 15. Two raters were recruited
to perform this manual annotation task. In cases when their
results differed, a third rater was enlisted to make the ﬁnal
decision.

4) Head Bounding Boxes: We track people’s heads simulta-
neously in the dataset. Our method detects the potential targets
in images and then we associate the detections through time
to generate target trajectories. However, the head detectors are
not perfect. Our tracker thus needs to deal with the missing
detections and the false alarms. For each video, head tracking
gives a set of person trajectories, which which we automatically
assign IDs using the corresponding face IDs.

The proposed method maintains the ID record of each
potential trajectory and tries to extend it to match the current
head detections. We detect head candidates using Yolo-v3
object detection deep network [15] trained on the Open Images
dataset [16]. Matching the trajectories and the observations
can be formulated as the following optimization problem:

(cid:88)

min

(ci,j − t)xi,j

(i,j)

(cid:88)

s.t.

xi,j ≤ 1,

i

(cid:88)

j

xi,j ≤ 1

xi,j is binary ∀i, j.

Here, xi,j is a binary variable that indicates whether trajectory
i matches head candidate j; if there is a match xi,j = 1,
otherwise xi,j = 0. ci,j is the cost of matching trajectory i
with head candidate j. Then matching cost ci,j is deﬁned as

ci,j = (cid:13)

(cid:13)pj − H(pi)(cid:13)

(cid:13)2 + αd(bj, bi),

where p is a 4-element vector whose elements are the x and
y coordinates of the top-left corner and bottom-right corner of
the head bounding boxes. For a trajectory, its p is determined
by its last position. To compensate for the large motion of the
camera in egocentric videos, we estimate the 2×2 homography
H between the current video frame and the previous video
frame using optical ﬂow [17]. b is a deep feature that can
distinguish between head appearances of different people. The
CNN is a modiﬁed resnet-18 with input size of 128 × 128 and
output size of 128. It is trained on the Voxceleb2 [18] dataset
using triplet contrastive loss with a margin of 1. The distance
d(·, ·) is L2 distance.

To avoid trivial all zero solutions, we introduce a threshold,
t. There is a potential match if the matching cost is less than
the threshold t. All the positive coefﬁcients x have to be zero.
Otherwise, they can be set to zero to lower the objective. The
matching for all the negative coefﬁcient x must be maximum.
The above optimization problem can be solved efﬁciently using
the primal-dual method [19].

We extend the trajectory, i, to include the head detection,
j, if the corresponding xi,j = 1. The trajectory i’s age is

Fig. 3. The physical layout of the recording setup is shown along with the
participant ID numbers. The sets of IDs/locations for each given number
of participants per session is also shown. The concentric rings indicate
approximate loudspeaker locations around the room. The distance from
participant ID 2 to ID’s 3, 4, 5, 6 and 7 was approximately 1.0 m, 1.2 m,
1.1 m, 1.2 m and 1.0 m, respectively. Dimensions are not to scale.

We provided annotators with speech transcription guide-
lines that would result in transcriptions similar to closed-
captions. The guidelines included capitalization of proper
nouns, capitalization of the ﬁrst word of every sentence and
used end-of-sentence punctuation. All words and numbers
were spelled out and we used coding conventions for laughter,
coughing, throat clear, unintelligible speech, hesitations, and
labeled as ‘L’, ‘C’, ‘T’, ‘U’, ‘H’ and ‘-’,
interruptions,
respectively. For mispronunciations, we enclosed the utterance
with asterisks (e.g. ‘*cabernet sauvignon*’). Upon completion
of the speech transcriptions and ToS labeling annotations we
completed a quality check where annotators reviewed each
other’s transcriptions and ToS labels to ensure accuracy and
consistency.

3) Face Bounding Boxes: We provide per-frame face ID
annotation for all videos in the dataset. We use RetinaFace [12]
to detect all faces from the videos and try to match the detected
faces to the ID of the glasses worn by the participants. During
the experiment, OptiTrack was used to capture the glasses’
locations in the 3D space at all time. In order to project the
glasses’ 3D coordinates to the 2D image plane, we calibrate the
camera using OpenCV2 to estimate its intrinsics and distortion
coefﬁcients. In each frame, we compute the pairwise Euclidean
distance between the detected faces and all glasses, and then
use constrained Hungarian algorithm to obtain the best match
between them. Although this process yields accurate results
in most cases, manual correction is still needed because: 1)
the face detector could produce false positives; 2) there is no
OptiTrack data for the person acting as the ‘server’; and 3) the
distances computed on the image plane could be misleading
when one face is occluding another. However, due to the large
size of the dataset, it is impractical to perform manual correction
on a frame-by-frame basis. As a remedy, we use an intersection-
over-union (IoU) -based tracker [13] to group the detected
faces into tracklets and perform the annotation on the tracklet
level. For each tracklet, we initialize its ID to be the majority

2https://opencv.org/

6

data points. The sampling rate for each data type, and further
details about the exact structure of the dataset, are included in
the root directory.

V. BASELINE METHOD

The baseline method to achieve improved conversational
focus of the AR glasses wearer aims to utilize noisy signals
captured by microphones of the acoustic array to generate
single-channel enhanced speech of the desired participant.

Given that the enhanced speech is required to be distortion-
less, we analyze a beamforming algorithm commonly referred
to as the maximum directivity index (DI) beamformer as the
baseline method. The maximum DI beamformer maximizes
the directvity index (and consequently the directivity factor),
which is an indicator of how well diffuse noise is suppressed.
The beamformer does this while keeping the signal arriving in
the target direction undistorted. The maximum DI beamformer
derivation can be formulated as an optimization problem with
the following

min
h(ω)

hH(ω)R(ω)h(ω),

s.t. hH(ω)d(ω) = g

(1)

where ω denotes angular frequency and (·)H is a Hermitian
transpose. Considering an N-microphone acoustic array, h(ω)
denotes a beamformer coefﬁcient vector of size N × 1 and d(ω)
is a vector of size N × 1 that represents the acoustic transfer
functions from the desired participant to the microphones on
the acoustic array. R(ω) is the N × N multichannel covariance
matrix of a spherically isotropic noise ﬁeld with unit power
spectral density. The solution of the optimization problem
above is a coefﬁcient vector of the maximum DI beamformer
hmaxDI(ω), which can be obtained with

hmaxDI(ω) =

g∗R−1(ω)d(ω)
dH (ω)R−1(ω)d(ω)

(2)

where g is the constraint imposed on the beamformer output
to satisfy the distortionless condition of the maximum DI
beamformer and (·)∗ indicates conjugation. If g is set to be
a transfer function that represents an acoustic path from the
desired talker to one of the microphones on the AR glasses,
then the beamformer will retrieve the speech captured by that
speciﬁc microphone without distortion.

In order to compute hmaxDI(ω), two main components, R(ω)
and d(ω), need to be consistently available while processing
the microphone signals. Theoretically R(ω) can be derived by
spatial integration over a sphere, such as
(cid:90)

(cid:90)

d(φ, θ, ω)dH(φ, θ, ω)sin(θ)dθdφ

(3)

R(ω) =

1
4π

φ

θ

where φ and θ are azimuthal and inclination angles, respectively.
Using the set of array transfer functions (ATFs) that are readily
available from the dataset (see Appendix B), R(ω) can be
approximated with

˜R(ω) =

1
NAT F

(cid:88)

(cid:88)

n

m

dAT F (φn, θm, ω)dH

AT F (φn, θm, ω)

(4)

Fig. 4. Annotations and labels are shown for an example frame in the dataset.
The tracked poses have been projected into the image space.

increased by 1 and its life is restored to the maximum value,
e.g. 20. If xi,j = 0 ∀j, and trajectory i’s life is greater than
0, then it extends to a predicted position based on M −1(pi),
where M is the homography estimated based on optical ﬂow
and M −1 is its inverse. Trajectory i’s age is increased by 1
and its life is decreased by 1. We remove the trajectories who’s
life is less than zero.

To deal with false alarm head detections, we only output
the trajectories whose lengths are longer than a number, e,g. 5
frames. This can effectively remove false alarms because they
are usually not stable and tend to generate short trajectories.
To assign an ID to each trajectory, we match each head
bounding box in a trajectory to a face detection that has
the maximum IoU with the head detection, and record the
corresponding face ID. A trajectory’s ID is then determined by
the majority vote of the matched face IDs; each head detection
in the trajectory is labeled with the trajectory ID.

IV. DATASET AVAILABILITY

The dataset can be downloaded from the following address:

https://github.com/facebookresearch/EasyComDataset

Please send correspondence to EasyComDataset@fb.com. The
entire dataset is available for free under the Creative Commons
Attribution Non-Commercial 4.0 International public license
agreement (CC-BY-NC-4.0)3.

The dataset is approximately 79 GB in size and contains
separate directories for each type of data. The video in the
dataset at the provided link is the compressed video (please
contact us for details on uncompressed video). The ﬁles are
split into one minute durations for all data types. There are
323 one minute segments totaling 5 h and 18 min of data. All
data and labels totals 3245 ﬁles and approximately 1 trillion

3https://creativecommons.org/licenses/by-nc/4.0/

7

where NAT F is the total number of ATFs in the set. φn and
θm are the azimuthal and the inclination angles associated with
a set of discrete points sampling the sphere for which an ATF
vector dAT F (φn, θm, ω) is measured.

As both the desired participant and the AR glasses wearer
are moving constantly relative to one another, d(ω) is assumed
to change over time. Hence the estimate of d(ω) needs
to be updated over time as opposed to R(ω) whose value
approximates a weakly time-varying value. We use the output
of the OptiTrack tracking system as the estimated positions
of the desired participant and the AR glasses wearer. We then
calculate the relative angle of the desired participant from
the AR glasses wearer. Once the relative angle is estimated,
dAT F (ω) that is closest to the relative angle is chosen from
the set of ATFs and used as the estimate of d(ω).

The processing of the baseline method is conducted on
a frame by frame basis. At every frame, the beamformer
coefﬁcient vector, hmaxDI(ω), is updated and frames of multi-
channel audio samples are ﬁltered by hmaxDI(ω) in a weighted
overlap-add (WOLA) procedure.

VI. BASELINE RESULTS AND DISCUSSION

To analyze the baseline method performance, we consider
signal-to-noise ratios, speech intelligibility and speech quality.
We use 11 intrusive instrumental objective metrics as measures
of the performance. Three metrics are related to speech quality,
four metrics are related to speech intelligibility and four are
related to SNR.

We use SNR as one of the metrics [20], which is well
established in the literature. We deﬁne the SNR as the ratio of
the desired target source signal to all other sounds, where we
consider all other sounds as undesired noise. Another metric
used is segmental SNR (SegSNR), which is similar to SNR
but the mean SNR is only computed over segments where the
target source signal is active [20]. The Signal to Distortion
Ratio (SDR) metric is also used and follows a similar deﬁnition
to SNR but is computed using the implementation described
in [21]. The Scale-Invariant SDR (SI-SDR) is the last of the
SNR related metrics used [22]. The SI-SDR is a variant of
SDR that was designed to address some assumptions made in
the SDR implementation from [21].

Speech intelligibility was investigated using the Short-
Time Objective Intelligibility (STOI) metric, which has high
correlation with time-frequency weighted noisy speech and is
computed on short-time segments. A more recent version of
STOI, known as Extended STOI (ESTOI) was also used in the
evaluation. ESTOI has been reported to, additionally, accurately
predict intelligibility when highly modulated noise is present.
The Hearing-Aid Speech Perception Index (HASPI) version 2
was another intelligibility metric used [25]. HASPI is a metric
that estimates intelligibility using a model of the auditory
periphery and is valid for normal-hearing and hearing-impaired
listeners. We compute HASQI assuming all participants in
the dataset have normal hearing and are speaking at levels
of approximately 71 dB SPL. The last of the intelligibility
metrics used is Speech Intelligibility In Bits (SIIB) [26], which
estimates the amount of information shared between a talker

and a listener in bits. SIIB has been show to have higher
correlation to intelligibility than STOI, ESTOI and HASPI
version 1 [30] across many different datasets [31].

For speech quality estimation we use the Perceptual Evalua-
tion of Speech Quality (PESQ) metric [27], which is widely
available and commonly used for speech quality evaluations,
although it was originally designed for telephony applications.
The Hearing-Aid Speech Quality Index (HASQI) version 2
was also used to evaluate speech quality and, like HASPI,
is based on a model of the auditory periphery, taking into
account the effects of hearing loss. We compute HASQI with
the same assumptions we make with HASPI. Finally, the last
metric used in the evaluation is the Virtual Speech Quality
Objective Listener (ViSQOL) version 3 metric [29], which has
been shown to have higher correlation to speech quality than
PESQ on several datasets [32].

The metric scores are obtained on the EasyCom dataset for all
possible target cases (each participant is marked as the desired
participant at least once) and the average scores are shown
in Table III. ‘Reference Mic’ and ‘Baseline Method’ denote
the case where the unprocessed reference microphone signal
is used as the degraded signal for each of the metrics and the
case where the baseline method output is used as the degraded
signal input for each of the metrics, respectively. Two subsets
of signals are evaluated as test cases for all metrics, namely
‘Noise’ and ‘Noise + Interferer’. For the ‘Noise’ test case, the
desired participant’s portions of the signals (as determined by
the VA labels of the dataset) are used as inputs for the metrics
only when there is no competing talker present, and noise is
always present. For the ‘Noise + Interferer’ test case, portions
of the signal when the desired participant is active are used
regardless of whether there is a competing talker present or
not, and noise is always present. We ignore cases where the
participant wearing the AR glasses is actively talking. In all
evaluations, a minimally-noisy clean speech reference signal is
used, which is the close microphone signals that are positioned
next to the participant’s mouths. These signals are not identical
to the true clean speech component in the noisy speech signals
captured by the AR glasses microphones and so the reported
metric values may not be meaningful on their own.

Table III shows the results where we can see that in all
test cases the baseline method increases all metrics over the
‘Reference Mic’, except one. The only metric that does not
result in an improvement over the ‘Reference Mic’ is PESQ
and occurs for the ‘Noise + Interferer’ test case, where the
PESQ value for the ‘Reference Mic’ and ‘Baseline Method’
are 1.172 and 1.168, respectively. This is likely due to the
presence of interfering speech in the reference signal that is
provided to the metric for the speciﬁc test case. In Table III we
see a 1.2 dB to 3.7 dB of average improvement in the SNR-
based metrics. A 0.05 to 0.11 improvement is made on average
in most intelligibility-based metrics, except SIIB, which uses
units of bits and shows 32 bits to 36 bits of improvement. We
disregarded the units while making relative comparisons. Of
the improvements that are made in the quality-based metrics,
we see a range of average improvement of 0.03 to 0.10.

The challenging conditions of the dataset make it difﬁcult to
improve over the reference microphone signal, however, there

8

TABLE III
SCORES FOR THE REFERENCE MICROPHONE SIGNAL AND THE BASELINE METHOD OUTPUT.

Test Case

SNR
[20]

SegSNR
[20]

SDR
[21]

SI-SDR
[22]

STOI
[23]

ESTOI
[24]

HASPI
[25]

Reference Mic
Baseline Method

Noise

Reference Mic
Baseline Method

Noise +
Interferer

−9.27
−6.62

−13.3
−10.1

−14.2
−10.7

−15.9
−12.2

−17.5
−8.98
−7.79 −14.7

−26.2
−14.3
−12.9 −23.4

0.504
0.590

0.462
0.544

0.321
0.408

0.303
0.379

0.876
0.927

0.720
0.830

SIIB
[26]

110
146

107
139

PESQ
[27]

HASQI
[28]

ViSQOL
[29]

1.17
1.27

1.17
1.17

0.268
0.319

0.197
0.249

1.64
1.68

1.65
1.68

are many other types of signals that could be leveraged to
outdo the baseline method’s results as reported here. We call
on the research community to improve on these results using
the dataset for the tasks outlined in this paper.

VII. CONCLUSIONS

In this work we have discussed and shown that existing
datasets do not provide sufﬁcient data for solving the cocktail
party problem from a multi-modal and egocentric point of
view, which could be common place in head mounted AR
devices. Existing egocentric datasets are missing either the
visual modality, pose information and/or noisy environments.
We have closed the gap in the literature and the gap in
available datasets by releasing a dataset with more than ﬁve
hours of synchronized multi-modal egocentric noisy recordings
of natural conversations. To facilitate accelerated research in
the area we have open-sourced the high quality dataset with
various annotated labels in the different modalities.

We have proposed a benchmarking task for the dataset and
have analyzed a baseline method to address the task. The base-
line method leverages acoustic and positional information to
enhance targeted speech in real-time. Other included modalities
could also be leveraged when addressing the benchmarking
task. Our baseline method results in substantial and consistent
improvements in speech quality, intelligibility and signal-to-
noise ratio -based metrics whilst not distorting the target speech.
Our method can also instantaneously switch between target
speech sources.

We challenge the research community to outperform our
baseline using the benchmarking task on the provided dataset
to help solve the cocktail party problem for AR.

APPENDIX A
POST-PROCESSING

After the recording of the raw data, several post-processing
steps took place to improve the quality of the dataset. The post-
process reduced the size of the dataset, aligned clean speech
reference signals and facilitated more accurate annotations.

A. Video Compression

The storage size of the raw uncompressed videos, at several
terabytes, is substantially larger than that of the visually lossless
(lossy) compressed version, at approximately 42 gigabytes. The
video compression was performed using FFMPEG. The videos
were encoded from uncompressed MPEG in an AVI container
to an MPEG4 ﬁle format. The MPEG4 video was compressed

using the x264 library with settings using the ‘veryslow’ preset
and a Constant Rate Factor (CRF) of 17.

The adaptive B-frame decision method was optimal with 8 B-
frames between I and P. The direction motion vector prediction
was automatic and integer pixel motion estimation method
was an uneven multi-hexagon search. The maximum motion
vector search range was 24. All partitions of the P, B and I
-macroblocks were considered. The number of reference frames
was 16. The sub-pixel motion estimation and mode decision
was with quantization parameter and trellis RD quantization,
enabled on all mode decisions. There were 60 frames for the
frame-type lookahead.

Two channels of the recorded audio were embedded in
the compressed video. The binaural microphone audio from
channel ﬁve (left) and channel six (right) of the microphone
array was embedded.

B. Audio Alignment

The AR glasses microphone array and headset microphones
were sampled with two different hardware clocks. Each
frame of 2400 samples of audio was timestamped using a
central clock. The headset microphone recordings were ﬁrst
coarsely aligned with the time stamps. The alignment was
then further reﬁned using the absolute peak of a generalized
cross-correlation with phase transform (GCC-PHAT) [33] using
the headset microphone signals and a reference microphone
on the AR glasses.

APPENDIX B
CALIBRATION DATA

To facilitate effective use of the data across different
modalities, calibration data is provided. The calibration data
allows for efﬁcient use of the acoustic array, linking the acoustic
and visual modalities, reducing optical distortions and reﬁning
tracking estimates. None of the provided calibration data that is
described in the following is a depiction of any future product.

A. Acoustic Array Transfer Functions

In order to effectively compute spatial enhancement ﬁlters
or beamforming ﬁlters, the transfer function from each mi-
crophone to a point in far-ﬁeld space is required. This set of
transfer functions is commonly called the steering vector, array
manifold or array transfer functions (ATFs), the latter is the
term used in this work. The ATFs included in this dataset
were measured on a head and torso simulator in an anechoic
chamber for a discrete set of positions on a sphere.

B. Spatial Acoustic-Optical Alignment

The acoustic array has a spatial response and, hence, the
ability to localize sound sources in space. The camera spatially
samples photons from the surrounding space and can be
correlated with the acoustic space if both spaces are aligned. In
order to align these two modalities, the AR glasses were placed
on a head and torso acoustic simulator in a specialized acoustic
setup. The discrete acoustic source locations in azimuth and
elevation were marked in the camera ﬁeld of view and labeled
as the corresponding image pixel coordinates.

C. Optical Camera Intrinsics

The camera lens path introduces typical lens distortions,
which change the way the ﬂat image sensor maps to world
coordinates. To compensate for this lens warping, the camera
intrinsics and lens de-warp ﬁlter parameters were measured.
Images of a standard checkerboard were taken with the camera
and then used to produce the de-warping ﬁlters.

D. Tracking Marker Locations

The OptiTrack tracking system used multiple passive infra-
red reﬂective markers to track the participants in the scene.
Multiple tracking markers were used per pair of glasses, ﬁve
for the AR glasses recording the scene and four for the other
glasses. The resulting tracked position included in the dataset is
the center of mass of the tracked marker locations. In order to
re-align the tracked positions to a speciﬁc marker, the relative
positions of the markers and the center of mass are provided
in the dataset.

ACKNOWLEDGMENTS

We would like to thank the research assistant for their
excellent work helping collect this dataset (whose name is
anonymous for privacy reasons). We would also like to thank
everyone who gave valuable feedback on the dataset. Lastly,
we extend our thanks to all participants involved in the data
collection.

REFERENCES

[1]

E. C. Cherry, “Some experiments on the recognition of speech, with
one and with two ears,” J. Acoust. Soc. Am., vol. 25, no. 5, pp. 975–979,
Sep. 1953.

[2] A. W. Bronkhorst, “The cocktail party phenomenon: A review
of research on speech intelligibility in multiple-talker conditions,”
Acustica, vol. 86, no. 1, pp. 117–128, 2000.

[3] D. Damen et al., “Scaling egocentric vision: The epic-kitchens dataset,”

in Eur. Conf. Comput. Vision (ECCV), 2018.

[4] D. Damen et al., “Rescaling egocentric vision,” 2020. arXiv: 2006.13

256 [cs.CV].

[5] A. Stupakov, E. Hanusa, J. Bilmes, and D. Fox, “COSINE - a corpus
of multi-party conversational speech in noisy environments,” in IEEE
Int. Conf. on Acoust., Speech and Signal Process. (ICASSP), 2009,
pp. 4153–4156.

[6] C. Northcutt, S. Zha, S. Lovegrove, and R. Newcombe, “EgoCom: A
multi-person multi-modal egocentric communications dataset,” IEEE
Trans. Pattern Anal. and Mach. Intell., pp. 1–1, 2020.

[7] M. V. Segbroeck et al., DiPCo – dinner party corpus, 2019. arXiv:

[8]

1909.13447 [eess.AS].
J. Barker, S. Watanabe, E. Vincent, and J. Trmal, The ﬁfth ‘CHiME’
speech separation and recognition challenge: Dataset,
task and
baselines, 2018. arXiv: 1803.10609 [cs.SD].

9

[9]

[12]

[10]

[11]

S. Watanabe et al., CHiME-6 challenge: Tackling multispeaker speech
recognition for unsegmented recordings, 2020. arXiv: 2004.09249
[cs.SD].
J. H. Rindel, “Acoustical capacity as a means of noise control in eating
establishments,” Proc. Baltic-Nordic Acoust. Meeting, vol. 2429, 2012.
J. Rindel, “Restaurant acoustics–verbal communication in eating
establishments,” Acoust. Pract., vol. 7, no. 1-14, 2019.
J. Deng et al., “Retinaface: Single-shot multi-level face localisation
in the wild,” in Proc. IEEE/CVF Conf. Comput. Vision and Pattern
Recognit., 2020, pp. 5203–5212.
E. Bochinski, V. Eiselein, and T. Sikora, “High-speed tracking-by-
detection without using image information,” in IEEE Int. Conf. Adv.
Video and Signal Based Surveillance (AVSS), 2017, pp. 1–6.
[14] A. Bulat and G. Tzimiropoulos, “How far are we from solving the
2d & 3d face alignment problem?(and a dataset of 230,000 3d facial
landmarks),” in Proc. IEEE Int. Conf. Comput. Vision, 2017, pp. 1021–
1030.
J. Redmon and A. Farhadi, “YOLOv3: An incremental improvement,”
2018. arXiv: 1804.02767 [cs.CV].

[15]

[13]

[16] A. Kuznetsova et al., “The open images dataset v4,” Int. J. Comput.

Vision, pp. 1–26, 2020.

[17] C. Zach, T. Pock, and H. Bischof, “A duality based approach for
realtime TV-L1 optical ﬂow,” in Joint pattern recognition symposium,
Springer, 2007, pp. 214–223.
J. S. Chung, A. Nagrani, and A. Zisserman, “VoxCeleb2: Deep speaker
recognition,” Proc. Interspeech, pp. 1086–1090, 2018.

[18]

[19] C. H. Papadimitriou and K. Steiglitz, Combinatorial Optimization:

Algorithms and Complexity. Courier Corporation, 1998.

[20] M. Brookes, “Voicebox: Speech processing toolbox for matlab,”
ic. ac. uk/hp/staff/dmb/voice-

Software, available from www. ee.
box/voicebox. html, vol. 47, 1997.
E. Vincent, R. Gribonval, and C. Fevotte, “Performance measurement
in blind audio source separation,” IEEE Trans. Audio, Speech, Lang.
Process., vol. 14, no. 4, pp. 1462–1469, 2006.
J. L. Roux, S. Wisdom, H. Erdogan, and J. R. Hershey, “SDR –
half-baked or well done?” In IEEE Int. Conf. on Acoust., Speech and
Signal Process. (ICASSP), 2019, pp. 626–630.

[21]

[22]

[25]

[24]

[23] C. H. Taal, R. C. Hendriks, R. Heusdens, and J. Jensen, “An algorithm
for intelligibility prediction of time–frequency weighted noisy speech,”
IEEE Trans. Audio, Speech, and Lang. Process., vol. 19, no. 7,
pp. 2125–2136, 2011.
J. Jensen and C. H. Taal, “An algorithm for predicting the intelligibility
of speech masked by modulated noise maskers,” IEEE/ACM Trans.
Audio, Speech, Lang. Process., vol. 24, no. 11, pp. 2009–2022, 2016.
J. M. Kates and K. H. Arehart, “The hearing-aid speech perception
index (HASPI) version 2,” Speech Commun., vol. 131, pp. 35–46,
2021, ISSN: 0167-6393.
S. Van Kuyk, W. B. Kleijn, and R. C. Hendriks, “An instrumental
intelligibility metric based on information theory,” IEEE Signal Process.
Lett., vol. 25, no. 1, pp. 115–119, 2018.
Perceptual evaluation of speech quality (PESQ). Int. Telecommun.
Union (ITU), ITU-T Rec. P.862, 2003.
J. M. Kates and K. H. Arehart, “The hearing-aid speech quality index
(HASQI) version 2,” J. Audio Eng. Soc., vol. 62, no. 3, pp. 99–117,
2014.

[28]

[27]

[26]

[30]

[29] M. Chinen et al., “ViSQOL v3: An open source production ready
objective speech and audio metric,” in Int. Conf. Quality Multimedia
Experience (QoMEX), IEEE, 2020, pp. 1–6.
J. M. Kates and K. H. Arehart, “The hearing-aid speech perception
index (HASPI),” Speech Commun., vol. 65, pp. 75–93, 2014, ISSN:
0167-6393.
S. Van Kuyk, W. B. Kleijn, and R. C. Hendriks, “An evaluation of
intrusive instrumental intelligibility metrics,” IEEE/ACM Trans. Audio,
Speech, Lang. Process., vol. 26, no. 11, pp. 2153–2166, 2018.
[32] A. Hines, J. Skoglund, A. C. Kokaram, and N. Harte, “ViSQOL: An
objective speech quality model,” EURASIP J. Audio, Speech, Music
Process., vol. 2015, no. 1, pp. 1–18, 2015.

[31]

[33] C. Knapp and G. Carter, “The generalized correlation method for
estimation of time delay,” IEEE Trans. Acoust., Speech, and Signal
Process., vol. 24, no. 4, pp. 320–327, 1976.

