0
2
0
2

y
a
M
3
1

]
E
M

.
t
a
t
s
[

2
v
7
5
9
7
0
.
6
0
9
1
:
v
i
X
r
a

Estimation of Markovian-regime-switching
models with independent regimes

Nigel Bean1,2, Angus Lewis∗1,2, and Giang T. Nguyen1,2

1School of Mathematical Sciences, The University of Adelaide,
Australia
2Australian Research Council Centre of Excellence in
Mathematical and Statistical Frontiers (ACEMS) aﬃliated
authors.

May 14, 2020

Abstract

Markovian-regime-switching (MRS) models are commonly used for
modelling economic time series, including electricity prices where inde-
pendent regime models are used, since they can more accurately and suc-
cinctly capture electricity price dynamics than dependent regime MRS
models can. We can think of these independent regime MRS models for
electricity prices as a collection of independent AR(1) processes, of which
only one process is observed at each time; which is observed is determined
by a (hidden) Markov chain. Here we develop novel, computationally fea-
sible methods for MRS models with independent regimes including for-
ward, backward and EM algorithms. The key idea is to augment the
hidden process with a counter which records the time since the hidden
Markov chain last visited each state that corresponding to an AR(1) pro-
cess.

Keywords: Electricity price model, forward-backward algorithm, hidden Markov
model, Markov-switching time series

1 Introduction

A commonly used model for economic time series is the Markovian-regime-
switching (MRS) model whereby multiple stochastic processes are interweaved

∗This research was supported by the provision of an Australian Government Research
Training Program Scholarship, a University of Adelaide postgraduate research scholarship, and
an Australian Research Council Discover Project, Grant/Award Number: ARC DP180103106.
email:

angus.lewis@adelaide.edu.au

1

 
 
 
 
 
 
by a Markov chain. The general idea is that there exist multiple regimes under-
lying the observation process, and depending on which regime the system is in,
diﬀerent characteristics are displayed. For example, for stock prices we could
suppose that there is a bull regime where prices trend upward and are compar-
atively non-volatile, and a bear regime where prices trend downward and are
relatively volatile. Our motivating application is electricity prices where it is
common to model prices with an MRS model. Due to the fact that electricity
cannot currently be stored eﬃciently, electricity prices show characteristics not
seen in typical commodity markets, for example mean reversion, prices spikes,
drops and negative prices. MRS models are able to capture these behaviours,
and have been popular tools for modelling randomness in electricity markets.
Typically MRS models with two or three regimes are used [9, 8, 13, 14]: a base
regime where prices are relatively non-volatile, a spike regime where prices are
volatile and high, and sometimes a drop regime is included, where prices are
volatile and low. More broadly, models with Markovian switching ﬁnd appli-
cation in biology [1], weather modelling [23, 27], speech recognition [21] and
more.

The earliest applications of MRS models for electricity prices are [9] and
[8]. Their models specify that prices decay back to base levels following a spike
according to an autoregressive process of order 1 (AR(1)). However, this is not
consistent with observations from the market where a more immediate return
to base levels is observed [16, 2]. For this reason, [14] introduce a three-regime
MRS model which separates the behaviour of base and spike prices. They
use one regime to capture base prices, one regime to capture spikes, and one
regime to return prices to base levels following a spike. Motivated by the need
to capture the distinct and abrupt price spikes in electricity markets, it has
become popular to specify MRS models with independent regimes, which are
able to capture this behaviour without the addition of the extra regime required
by [14]. We may think of a dependent regime MRS models as a single process,
where the dynamics of the process is governed by the hidden Markov chain,
whereas we may think of an independent regime MRS model as a collection
of independent AR(1) processes, and at each time t, the hidden Markov chain
chooses which process is observed.

We say a model has independent regimes if, given the hidden regime se-
quence, the observations generated from each regime are independent of obser-
vations generated from any other regime; and we say a model has dependent
Independent regime MRS models were introduced in [13]
regimes otherwise.
and have since been popular [25, 17, 24].
In these models, typically at least
one regime is speciﬁed as an AR(1) process. AR(1) processes have a depen-
dence structure between values at successive times which, coupled with the in-
dependent regimes assumption in an MRS model, complicates the dependence
structure between an observation at time t and all prior observations – the de-
pendence between prices is governed by the hidden regime process and is there-
fore random. It is for this reason that the forward, backward and expectation-
maximisation (EM) algorithms for traditional (dependent regime) MRS models
or hidden Markov models do not apply. For each time t, the forward algorithm

2

evaluates the probabilities that the hidden process is in each regime given the
observed values up to time t. The backward algorithm then uses the output
of the forward algorithm to calculate the probabilities that the hidden process
is in each regime given all observations. The EM algorithm is an iterative op-
timisation algorithm, iterating between an E-step and an M-step, used to ﬁnd
the maximum likelihood estimates of parameters for models with missing data
or latent variables – the E-step is computed by the backward algorithm.

For simplicity, in this work, we focus on independent regime MRS models
with AR(1) and i.i.d. regimes only, since these are the type of models used in
the electricity price modelling literature. However, we believe the methods de-
veloped here are more general and apply to Markov-switching processes with
regimes that are discrete-time Markov chains generally, and can also be extended
to more general autoregressive processes. The most popular method of infer-
ence for the models used for electricity pricing is an approximation to the EM
algorithm introduced by [17], which we show can be unreliable (see [22] also).
Here, a novel, computationally feasible, and exact likelihood-based framework
to solve this problem is developed. This work is related to the forward, back-
ward, and EM algorithms for traditional MRS models, and, more closely, to the
same algorithms for hidden semi-Markov models, where the idea of augmenting
the hidden process with a counter is also used [26]. The novel algorithms have
complexity
is the total number of regimes in the
model, T is the length of the observed data set, and k is the number of AR(1)
regimes in the model. As
may be impractically large depending
on the values of T and k, we also present an approximation to our algorithm
(cid:1)
that is
where D > 0 is the maximum length of the memory of
the AR(1) processes, that is, we specify that the observations xt may depend
only on xt−ℓ for ℓ

, and is independent of any xt−ℓ for ℓ > D.

M 2T k+1kk

M 2T k+1kk

M 2T Dkkk

where M <

(cid:1)
O

∞

O

O

(cid:0)

(cid:0)

(cid:0)

This paper is structured as follows. We formally introduce the MRS model,
in particular the independent-regime type in Section 1.1, and discuss the ap-
proximate parameter inference algorithm of [17] in Section 1.2. A novel forward
algorithm, which is used to evaluate the likelihood and ﬁltered state probabilities
for these models, is presented in Section 2. Using the outputs of the forward
algorithm we develop a novel backward algorithm in Section 3. The backward
algorithm is used to evaluate the smoothed state probabilities, which are applied
in Section 4 to construct an EM algorithm. We introduce the truncated approx-
imations of our algorithms at the end of Section 4. Section 5 provides simulation
evidence that our EM algorithm is consistent and that the truncation approxi-
mations are reasonable, while in Section 6 we apply our algorithms to estimate
MRS models for the South Australian wholesale electricity market. Finally, we
make concluding remarks in Section 7.

(cid:1)
1, 2, ..., D
∈ {

}

1.1 MRS models: A brief introduction

An MRS model is built from two pieces, an unobservable regime sequence,
t∈N, which is a ﬁnite-state Markov Chain, and an observation sequence,
Rt
}
=
t∈N. Let us denote the state space of the hidden regime sequence as
Xt
}

{
{

S

3

Simulation of a dependent regime MRS model

Simulation of an independent regime MRS model

15

10

5

0

-5

0

X
t

B

t

S
t

10

20

30

40

50

15

10

5

0

-5

0

X
t

B

t

S
t

Unobserved

10

20

30

40

50

(Left) Simulation of the dependent regime MRS model example.
Figure 1:
(Right) Simulation of the independent regime MRS model example. Notice
that after a change of regime the independent regime model shows a distinct
change in characteristics.

{

∞}

1, 2, . . . , M <

, and the transition matrix as P = [pij]i,j∈S .
The simplest MRS model is the hidden Markov model (HMM) where obser-
vations Xt take values in a discrete set, and Xt is independent of Xt−1, . . . , X0
and Xt+1, Xt+2, . . . given the regime at time t, Rt. In general, MRS models are
speciﬁed in terms of distributions that allow dependence on past observations,
given the current regime. That is, the model deﬁnes distributions,

F Rt,

|{

Xt

Rt, Xt−1, Xt−2, . . . , X0} ∼
for some distribution F Rt. The MRS model, as introduced by [10, 11], speciﬁes
that Xt
follows some time-series model (an autore-
gressive process of order p, for example) with dependence on a ﬁnite number
of past observations, but not on R0, .., Rt−1. That is, the dependence structure
does not take into account which regime the past observations belong to. For
example, the following is a dependent regime MRS model.

Rt, Xt−1, Xt−2, . . . , X0}

|{

Example 1 (An MRS model with dependent regimes). Let
p11 = p22 = 0.9, and specify

=

1, 2

}

{

S

, and

Xt

Xt

Rt = 1, Xt−1, Xt−2, . . . , X0}
Rt = 2, Xt−1, Xt−2, . . . , X0}

|{

|{

= 0.6Xt−1 + ε(1)
= 1 + 0.9Xt−1 + ε(2)

,

t

t

.

t ∼

for ε(i)
i.i.d. N(0,1) for i = 1, 2. So Xt follows AR(1) dynamics in both
Regimes 1 and 2. This is a dependent-regime MRS model since Xt depends
on Xt−1 regardless of which regime the lagged observation, Xt−1, came from.
Figure 1 (Left) shows a simulation of this model.

In this paper we relax the assumption that the current observation Xt is
conditionally independent of R0, .., Rt−1, in order to increase ﬂexibility in these
models. In particular, we consider models where, given Rt = i, Xt depends only
on lagged values from Regime i, thus the dependence structure is random as it
is a function of R0, . . . , Rt−1. The following is an example of an independent
regime MRS model.

4

Example 2 (An MRS model with independent regimes). Let
p11 = p22 = 0.9, and deﬁne the following AR(1) processes

=

1, 2

{

}

S

, and

Bt = 0.6Bt−1 + εB
t ,
St = 1 + 0.9St−1 + εS
t ,

where εB
struct the MRS model as follows

t and εS

t are sequences of i.i.d. N(0,1) random variables. Then, con-

Xt =

Bt,
St,

(

if Rt = 1,
if Rt = 2.

Figure 1 (Right) shows a simulation of this model.

A precise deﬁnition of dependent and independent regime models is the
. We say that a model
,
Xt : t

following. Deﬁne the sets
has independent regimes if, given the regime sequence, the sets
i

, are independent. Otherwise, it is a dependent regime model.

N : Rt = i

i
∈ A

i :=

∈ S

, i

A

∈

{

}

{

}

t

∈ S

1.2 Existing methods

For the simplest form of MRS model, the HMM, likelihood evaluation and max-
imisation algorithms were ﬁrst developed in a series of papers, [5], [4], and [6],
and subsequent work on MRS models is typically closely related to this. The
ﬁrst algorithms for the more general dependent regime MRS model were pre-
sented by [10, 11]. The main issue for maximum likelihood estimation of models
with hidden regimes is that the regime sequence is unobserved, thus to naively
evaluate the likelihood requires calculation of the marginal distribution

L (θ) := f θ

X (x) =

f θ
X,R (x, R) =

R∈S T +1
X

R∈S T +1
X

f θ
X|R (x

|

R) f θ

R (R) ,

(1)

Y (
·

where f θ
) denotes the distribution function of a random vector Y with pa-
rameters θ, x = (x0, . . . , xT ) is a sequence of observed values, and
T +1 is
the space of all possible regime sequences of length T + 1, R = (R0, ..., RT ).
T +1 is M T +1 which, for most realistic datasets,
The number of sequences in
is computationally infeasible to evaluate in this form. In the context of HMMs,
the sum (1) is made computationally feasible by the forward algorithm [6], and
the maximisation of the likelihood is commonly performed via the Baum-Welch
algorithm [6], which is a speciﬁc case of the EM algorithm [7] and uses the
backward algorithm [6].

S

S

The works of [10, 11] extend the methods for HMMs to MRS models with
dependent regimes by adapting the forward algorithm, developing a new algo-
rithm to replace the backward algorithm and constructing an EM algorithm.
[20] reﬁnes the work of Hamilton, developing a more eﬃcient implementation of
Hamilton’s smoothing algorithm. Kim’s algorithm is similar to the backward
algorithm for HMMs.

5

Relevant to this paper, [17] extend Hamilton’s work and develop an approx-
imate algorithm for MRS models with independent regimes, which we label the
EM-like algorithm since it resembles Hamilton’s EM algorithm. However, it is
not an example of the EM algorithm and so none of the EM theory holds. In
Sections 1.2.1–1.2.3, we brieﬂy review the work of [11], [20] and [17] in order to
provide the motivation and background for our work.

1.2.1 Likelihood evaluation for dependent-regime models: The for-

ward algorithm

T

Deﬁne xr:s = (xr, xr+1, . . . , xs) for r
f θ
X0 (x0)
and f θ
the likelihood or loglikelihood:

t=1
Q
Xt|X 0:t−1

f θ
Xt|X0:t−1

(xt

|

s and write the likelihood as L (θ) =
x0:t−1) . The forward algorithm [11] calculates f θ
X0

≤

for t = 1, 2, . . . , T, from which it is straightforward to calculate

Algorithm 1: The forward algorithm [11]

Step 1. Initialise the algorithm with values Pθ (R0 = i) = πi, which may be as-
sumed to be known a priori, or left as parameters to be inferred.

Step 2. The term, f θ

X0 (x0), is calculated as f θ

X0 (x0) =

f θ
X0|R0

(x0|

i) Pθ (R0 = i) ,

is known from the model speciﬁcation.

i∈S
P

where the density f θ

X0|R0

Step 3. For t = 1, . . . , T :

f

θ
Xt|X0:t−1 (xt

x0:t−1)

|

θ
Xt|Rt,X 0:t−1 (xt

=

f

i∈S
X

i, x0:t−1)

|

j∈S
X

Pθ

(Rt−1 = j

x0:t−1) pji,

|

Xt|Rt,X0:t−1

where f θ
more, the probabilities Pθ (Rt−1 = j
Theorem,

|

is also known from the model speciﬁcation. Further-
x0:t−1) can be calculated using Bayes’

Pθ (Rt−1 = j

x0:t−1)

|
f θ
Xt−1|Rt−1,X0:t−2

=

≥

pijPθ (Rt−2 = i

j, x0:t−2)

(xt−1|
f θ
Xt−1|X0:t−2

i∈S
P
(xt−1|

x0:t−2)

x0:t−2)

|

,

(2)

for t

2. These are known as the forward/ﬁltered probabilities.

The quantities Pθ (Rt = i

x0:t−1) =

|

pjiPθ (Rt−1 = j

|

x0:t−1) are known

as the prediction probabilities. In some applications, the forward and prediction
probabilities may be quantities of interest in their own right, and they also
appear as inputs to the backward algorithm (see Section 1.2.2).

j∈S
P

6

1.2.2 Maximum likelihood for dependent-regime models: The EM

algorithm

Hamilton’s forward algorithm [11] is a computationally feasible way to evalu-
ate the loglikelihood, from which it is possible to use black-box optimisation
methods to ﬁnd the MLEs. However, it is common to use the EM algorithm [7]
instead, particularly when the E-step and M-step of the algorithm are available
in closed form. The EM algorithm proceeds by iterating between the E-step,
log f θ
constructing the function Q (θ, θn) = E
x; θn
, and the M-
step, maximising Q with respect to θ
Θ, where Θ is the parameter space.
(cid:2)
This results in a sequence
n∈N that converges to a local maximiser of the
loglikelihood.

X,R (x, R)

θn

∈

}

{

(cid:3)

|

The EM algorithm for MRS models with dependent regimes proceeds as
follows [11]. Deﬁne the random variable ηij as the number of transitions from
state i to state j in the sequence R = (R0, . . . , RT ) and let I (
) be the indicator
·
function. The joint log-density of x and R can be written as

log f θ

X,R (x, R)

=

I (R0 = j) log f θ

X0|R0 (x0|

j)

j∈S
X
T

+

+

I (Rt = j) log f θ

Xt|Rt,X 0:t−1 (xt

j, x0:t−1)

|

t=1
X

j∈S
X
ηij log pij +

i,j∈S
X

j∈S
X

I (R0 = j) log πj ,

where πj denotes Pθ (R0 = i). In the nth iteration, n
0, for the E-step, taking
the conditional expectation given parameters θn and observed values x0:T yields

≥

Q (θ, θn) =

Pθn (R0 = j

x0:T ) log f θ

X0|R0 (x0|

|

j)

j∈S
X
T

+

j∈S
X

t=1
X
+

i,j∈S
X

Pθn (Rt = j

|

x0:T ) log f θ

Xt|Rt,X 0:t−1 (xt

j, x0:t−1)

|

E[ηij

|

x0:T ; θn] log pij +

Pθn (R0 = j

j∈S
X

x0:T ) log πj,

|

where the expectation E[ηij
densities f θ

and f θ

X0|R0

Xt|Rt,X 0:t−1

x0:T ; θn] =

|

The smoothed probabilities, Pθn (Rt = j

, are given by the model speciﬁcation.

t=1
P
x0:T ) and Pθn (Rt = j, Rt−1 = i

|

Pθn (Rt = j, Rt−1 = i

x0:T ). The

required to construct Q are obtained using a backward recursion after running
the forward algorithm with parameters θn, and storing the forward and predic-
tion probabilities. Developed by [20], this backward recursion is in Algorithm
2, below.

x0:T ),

|

T

|

7

Algorithm 2: The backward algorithm [20]

Step 1. Evaluate Pθn (RT = j

|

Step 2. For t = T

1, . . . , 0,

−

x0:T ) using the forward algorithm (Algorithm 1).

Pθn (Rt = j, Rt+1 = i

Pθn (Rt = j

|

|

x0:T ) = p(n)
ji

Pθn (Rt = j

x0:t) Pθn (Rt+1 = i

|

Pθn (Rt+1 = i

x0:T )
,

|

x0:T ) =

Pθn (Rt = j, Rt+1 = i

i∈S
X

|

x0:t)
|
x0:T ) ,

where p(n)

ij means the pij parameter under θn.

After executing Kim’s backward algorithm, we can construct the function Q.
, θn) are found. In the dependent regime
In the M-step, the maximisers of Q (
·
model, if the process is in Regime j at time t, then the observations evolve
according to Xt = αj + φj Xt−1 + σj εt where αj, φj and σj are parameters, and
N (0, 1). Recall that, for the dependent regime model, Xt−1 is the last
εt
{
observed value (regardless of which regime generated it). The maximiser of Q
at the (n + 1)th iteration of the EM algorithm, θn+1, for n
0, is the following
system of equations [11, 17]:

} ∼

≥

φ(n+1)
j

=

α(n+1)
j

=

T

t=1
P
T

t=1
P
T

t=1
P

T

(n+1)

σ2
j

=

t=1
P

(cid:0)

(cid:1)

where

Pθn (Rt = j

x0:T ) xt−1B(i)
1,t

|

,

Pθn (Rt = j

x0:T ) xt−1B(i)
2,t

|

Pθn (Rt = j

x0:T )

xt

T

(cid:16)
Pθn (Rt = j

x0:T )

|

t=1
P

Pθn (Rt = j

φ(n+1)
j

xt−1

−

,

(cid:17)

x0:T )

α(n+1)
j

xt

−

−

φ(n+1)
j

xt−1

(cid:16)

Pθn (Rt = j

x0:T )

|

2

,

(cid:17)

B(i)

1,t = xt

xt−1 −

−

T

s=1
P

Pθn (Rs = j

x0:T ) (xs

|

−

xs−1)
,

Pθn (Rs = j

x0:T )

|

T

s=1
P

and B(i)

2,t =

T

Pθn (Rs = j

x0:T ) xs−1

s=1
P

T

s=1
P

Pθn (Rs = j

x0:T )

|

8

xt−1.

−

t=1
P

|

|
T

|

In general, the switching probabilities are updated using the following [20]

p(n+1)
ij

=

T

t=1
P

Pθn (Rt = j

x0:T )

|

p(n)
ij

Pθn (Rt−1 = i
Pθn (Rt = j

|
x0:t−1)

x0:t−1)

|

,

(3)

Pθn (Rt−1 = i

x0:T )

|

T

t=1
P

which rely on the smoothed, forward and prediction probabilities. For i.i.d. regimes
the M-step can often be derived analytically; however, such expressions are not
required for our discussion since there is no dependence on lagged values in these
regimes and so they are omitted.

Thus, we implement the EM algorithm by initialising it with a guess of
the true parameters, then alternating between the forward and backward al-
gorithms (the E-step) and calculating the maximisers of Q (the M-step). The
algorithm terminates when the step size is below a prespeciﬁed tolerance, i.e.
θn+1 −
1.2.3 Approximate maximum likelihood for independent-regime mod-

|∞ < e where e is some small tolerance.

θn

|

els: The EM-like algorithm

Xt|Rt,X 0:t−1

For independent-regime MRS models the EM algorithm is computationally in-
feasible if the densities f θ
, t = 1, . . . , T , are computed naively,
(M t) calculation for each t = 1, ..., T , where M <
as this is a
is the num-
ber of regimes in the model. On the other hand, if we use our proposed foward
tk+1
algorithm, introduced in Section 2), calculation of these densities is
for each t = 1, ..., T , which may be computationally feasible when T and k are
(cid:1)
not too large.

, Rt

∈ S

∞

O

O

(cid:0)

Developed by [17], the EM-like algorithm is an approximation to the EM
algorithm. For independent regime MRS models, the EM-like algorithm over-
comes the problem of computational infeasibility by replacing lagged values for
Regime i (assuming this is an AR(1) regime) with approximations, ˜b(n)
t−1,i. These
are described as the expectations [17], E[Bi
t|
= i)

t := I (Rt = i) xt + I (Rt

x0:t; θn], where

αi + φiBi

t−1 + σiεi
t

Bi

.

Simply put, wherever xt−1 appears verbatim in an expression related to Regime
i in the EM algorithm in Sections 1.2.1-1.2.2, it is replaced with ˜b(n)
t−1,i at the
nth iteration. The ˜b(n)
t,i are calculated recursively as

(cid:1)

(cid:0)

t,i = ˜Pθn (Rt = i
˜b(n)

x0:t) xt + ˜Pθn (Rt

|
x0:t) and ˜Pθn (Rt

x0:t−1)

= i

|

αi + φi˜b(n)
(cid:16)

t−1,i

(cid:17)

,

(4)

|

where ˜Pθn (Rt = i
x0:t) are given by the forward algorithm
|
which is part of the EM-like procedure. Janczura and Weron, [17], conduct sim-
ulation studies and show that this algorithm seems to work well for the datasets
they generate. However, no theoretical results are available that show conver-
gence of, or error bounds for, the EM-like algorithm; in particular, there is no

= i

9

6
6
6
1

0.5

0

-0.5

-1

2

1

0

1

0.5

0

MLE

EM-like

MLE

EM-like

MLE

EM-like

2

1

0

-1

2

1.5

1

MLE

EM-like

1

0.8

0.6

0.4

1

0.8

0.6

MLE

EM-like

MLE

EM-like

MLE

EM-like

Figure 2: Boxplots of the parameters recovered by the EM-like algorithm (Right)
and the MLEs recovered by our EM algorithm (Left) for Example 3. The blue
line represents the true parameter value. Notice that the EM-like algorithm is
not able to recover the parameters, while the EM algorithm performs relatively
well.

guarantee that the parameter estimates produced by the EM-like algorithm are
consistent. In contrasts, our algorithms rest on the theory of the EM algorithm.
We can construct examples of independent-regime MRS models where the

EM-like algorithm fails to get close to the true parameter values.

Example 3. Consider the following independent-regime MRS model,

Xt =

Bt,
Yt,

(

if Rt = 1,
if Rt = 2,

(5)

{

}

{

}

εt

Rt

t∈N is a Markov chain with state space

where Bt is an AR(1) process, Bt = 0.95Bt−1 + √0.2εt, with
being a
sequence of i.i.d. N(0, 1) random variables, Yt is an i.i.d. sequence of N(2, 1)
random variables, and
,
}
transition matrix entries p11 = 0.5 and p22 = 0.8, and initial probability dis-
tribution (1, 0), so the process always starts in Regime 1. We simulated 20
realisations of length T = 2000 from this model and used the EM-like algorithm
to try to recover the true parameters. To give the algorithm the best chance
of converging to the true parameters, we initialise the EM-like algorithm at the
true parameter values. The parameters recovered by the EM-like algorithm are
summarised in Figure 2. For comparison, the MLEs obtained using our EM
algorithm (Section 4) are also shown. Notice in Figure 2 that the EM-like algo-
rithm performs poorly, while our exact method performs much better.

1, 2

=

S

{

2 A novel forward algorithm

The general idea of the novel algorithms presented in this paper is to augment
the hidden Markov chain with counters that record the last time each AR(1)

10

regime was visited. This augmented process is a Markov chain, and similar
arguments to those used to construct the forward-backward algorithm for MRS
models with dependent regimes can be used to construct a forward and back-
ward algorithms for these models. Our methods are related to the forward and
backward algorithms for hidden semi-Markov models, where the hidden pro-
cess is also augmented with a counter and the augmented hidden process is
a Markov chain [26]. Though similar, the algorithms for hidden semi-Markov
models (HSMMs) are not applicable to the models considered here since for
HSMMs the counter counts the number of time steps since the last change of
regime, whereas here the counters count the number of transitions since the last
visit to a regime. Furthermore, in HSMMs, the counters do not appear in the
conditional densities of the observations, as they do here.

2.1 The augmented hidden Markov chain

For the following, suppose that the ﬁrst k states,
,
}
correspond to AR(1) processes and all other regimes are i.i.d. That is, we have
the following independent regime MRS model

1, . . . , k < M

AR =

S

{

Xt =

if Rt = 1,

if Rt = k,
if Rt = k + 1,

,

if Rt = M,

B1
t ,
...

Bk
t ,
Sk+1
t
...
SM
t




,

t = αi + φiBi

t are AR(1) and Sj
where Bi
t are i.i.d. Our arguments also
hold for k = M with only slight modiﬁcation, but here we treat the case k < M
only, since these are the types of models relevant to our application.

t−1 + σiεi

Now, deﬁne another Markov chain

Ht

t∈N :=

(N t, Rt)

t∈N =

(Nt,1, . . . , Nt,k, Rt)

t∈N,

}

}

{

{

}

∈

∈ {

{
N+ counts the number of time steps since the process

where Nt,j
was
last in Regime j before time t, for each AR(1) regime j = 1, . . . , k. When there
with Rτ = j, then we set Nt,j = t + 1. Thus the
is no time τ
0, 1, . . . , t
t∈N lives on the state space Nk
augmented Markov chain
let
describe
n := (n1, . . . , nk)
= ns
for r
= s or, at time t, nr = ns = t + 1 (it is possible that neither state r nor
state s have been visited by time t). Also, deﬁne 1 to be a row vector of ones
of length k, ei to be a row vector of length k with all entries being 0 except the
ith entry which is 1, and n(−i) := n

−
}
{
H t
transitions
the Markov
}
+ be an arbitrary vector of counters, with nr

niei = (n1, . . . , ni−1, 0, ni+1, . . . , nk) .

+ × S

1
}
H t

chain

the

Nk

To

Rt

of

∈

}

{

{

,

.

−

11

6
6
The transition probabilities of

H t

are

H t = (nt, i))

Pθ

}

{
(H t+1 = (nt+1, j)
|
c
AR, j
AR, j

for i
for i
otherwise.

∈ S
∈ S

pij
pij
0

= 


, nt+1 = nt + 1,
, nt+1 = n(−i)

t + 1,

∈ S
∈ S

(6)



In words, when the current state is H t = (nt, i) and i is not an AR(1) regime (so
there is no counter associated with state i), then, at time t + 1, Rt transitions
to state j with probability pij and all the counters are advanced by 1 to nt+1 =
nt + 1, since there has been one more time step since
was last in any state
{
AR). When the current state is H t = (nt, i), where
with a counter (any state in
S
with probability
i is an AR(1) regime, then Rt transitions to any state j
pij, the counter for Regime i, nt+1,i, is set to 1, since the last time in state i was
t, and all other counters are advanced by 1. All other transition probabilities
for

∈ S

Rt

}

H t

t∈N is initialised and evolves, many states are inaccessible for
0, 1, . . . , T <

is countably inﬁnite. However, due to the way
,
}
, and this makes our algorithm computationally feasi-
is initialised with the

{
t
ble. Speciﬁcally, we suppose that the Markov chain
probability distribution

}
∈ {

H t

H t

∞}

{

}

{

}

{

H t
are 0.
{
The state space of
H t

}

P (H 0 = (n0,1, . . . , n0,k, j)) =

πj ,
0,

(

, n0,i = 1, for all i

for j
otherwise.

∈ S

AR,

∈ S

(7)

The distribution π := (π1, . . . , πM ) can be any proper probability distribu-
tion. However, in line with existing algorithms for dependent regime MRS
models, it can be either the stationary distribution of
, or a point mass
on a single state, or, when used as part of the EM algorithm, the probabilities
x0:T ) calculated at the previous iteration of the EM algorithm. The
Ht

following lemma gives all the states that

can be in at time t > 0.

Pθn (H 0|

Rt

{

}

{

}

Lemma 4. Deﬁne
{
number of AR(1) regimes. For each t = 1, 2, . . . , T , let
vectors nt := (nt,1, . . . , nt,k) such that, for j, m

as a vector of 1’s of length k where k is the
(t) be the set of all

1
}

AR,

S

S

(0) :=

∈ S

(i) nt,j

∈ {

1, 2, . . . , t + 1

,

}

(ii) there are at most min (t, k) elements of nt with nt,j

= t + 1,

(iii) nt,j

= nt,m for all j

= m, unless nt,j = nt,m = t + 1.

H t

Given
H t to reach states (nt, i) where nt
min(t,k)

is initialised with the distribution in Equation (7), it is possible for
(t) and i
only. The cardinality of

∈ S

∈ S

}

{

(t) is

S

(t)

|S

|

=

m=0
P

t
m

k
m

m!.

(cid:0)

(cid:1)(cid:0)

(cid:1)

12

6
6
6
Proof. First, we explain why
H t for t

0.

At time t = 0 the chain,

≥

tion in Equation (7), so

S
At time t > 0, either

S
H t
{
(0) :=
Rt

{

}

(t) contains all possible values of the counters of

{

(N t, Rt)
}

=
}
.
1
{
}
has never visited state j

, is initialised with the distribu-

AR, in which case

∈ S

{

}

}

Rt

; this is part (i). Since the process

last visited j at time tj, in which case nt,j = t

nt,j = t + 1, or
1, 2, . . . , t
{
at a time, it follows that nt,j
= nt,m for j
which is part (iii) of the deﬁnition. Also, at time t, the regime chain
}
could only possibly have visited min (t, k) possible states; this is part (ii) of the
deﬁnition.

∈
can only be in one regime
= m (unless nt,j = nt,m = t + 1),

Rt

Rt

−

tj

{

}

{

(t). The elements of

Now, to prove the cardinality of

(t) are of the
S
form (n1, . . . , nk). At time t, let m be the possible number of counters that
are not equal to t + 1, so m is an element of
. For each
k
m
ways of choosing which m of the k
m
counters are not equal to t + 1. Next, each counter takes a distinct value in
ways of choosing the value of the m counters. There
{
are m! possible permutations to allocate the chosen values to the counters. So,

0, 1, . . . , min (t, k)
}
t
m

0, 1, . . . , min (t, k)
}

, so there are

, there are

1, . . . , t

∈ {

S

}

{

(cid:0)

(cid:1)

(cid:0)
min(t,k)

(cid:1)
t
m

m=0
P

in total there are

k
m

m! elements in

(t).

S

(cid:1)
Lemma 4 says that if nt /

(cid:1)(cid:0)

(cid:0)

(t) then Pθ (H t = (nt, j)) = 0 for any j

. Therefore the elements of the set
H t

∈
(t) partition the space of all counters
S
that the process
has positive probability of reaching. Thus, for any
(measurable) set A and any t, the law of total probability can be applied as
Pθ (H t = (nt, j) , A) . We will use this fact multiple times
Pθ (A) =

∈ S

S

{

}

j∈S
P
to construct the forward algorithm.

nt∈S (t)
P

2.2 Constructing the forward algorithm

The forward algorithm is multi-purpose. It can be used to evaluate the like-
lihood, and also to evaluate the ﬁltered and prediction probabilities, which in
turn are inputs to the backward algorithm. For clarity of exposition, we ﬁrst
present a simple, but impractical due to underﬂow, algorithm (Lemma 5) to
calculate the likelihood for independent regime MRS models, then address the
underﬂow issue later with a normalised version of the algorithm (Lemma 7).
Deﬁne

for t = 0, 1, . . . , T , nt

Ht,X0:t ((nt, j) , x0:t)

nt (j) := f θ
α(t)
(t), and j

∈ S

.

∈ S

Lemma 5 (A simple forward algorithm). First, for j

calculate

∈ S
(H 0 = (n0, j)) .

(8)

α(0)

n0 (j) = f

θ

X0|H0 (x0|

(n0, j)) Pθ

13

6
6
Then for t = 1, 2, . . . , T , nt

(t), j

, calculate

f θ
Xt|Ht,X0:t−1

∈ S
(xt

∈ S
(nt, j) , x0:t−1)

|

f θ
Xt|Ht,X0:t−1

(xt

|

(nt, j) , x0:t−1) pℓj

pijα(t−1)

nt−1 (i)

AR

i∈S c
P
= 1, for all ℓ

if nt,ℓ

AR,

∈ S

α(t−1)
nt−1+meℓ (ℓ)

t

m=1
P

if nt,ℓ = 1, for some ℓ

AR.

∈ S

α(t)

nt (j) =






Then the likelihood is given by

L (θ) =

α(T )

nT (j) .

nT ∈S (T )

j∈S X
X

Proof. First, from the law of total probability, we have

(9)

(10)

nt (j) := f θ
α(t)
=

Ht,X0:t ((nt, j) , x0:t)

f

θ

Ht−1,Ht,X 0:t ((nt−1, i) , (nt, j) , x0:t) .

(11)

nt−1∈S (t−1)
X

i∈S
X

−

1, . . . , k

Now, if any nt,ℓ = 1 for ℓ
nt−1 = nt

∈ {
1+meℓ for some m
t

}
1, . . . , t
∈ {
}
f θ
H t−1,H t,X 0:t ((nt
in Equation (11) simpliﬁes to
Otherwise, all elements of nt are greater than 1, in which case Rt−1 /
nt−1 = nt

1, so the double sum in Equation (11) simpliﬁes to

, then it must be that Rt−1 = ℓ and
. Thus, in this case, the double sum
1 + meℓ, ℓ) , (nt, j) , x0:t) .

AR and

m=1
P

∈ S

−

−

f

θ

Ht−1,Ht,X0:t ((nt

−

1, i) , (nt, j) , x0:t) .

i∈S c
X

AR

For both cases the following arguments are the same, so for notational conve-
nience we will use n to be either nt
AR, or
−
1 + meℓ when nt,ℓ = 1 for some ℓ
nt
∈ S

1 when nt,ℓ > 1 for all ℓ

−
We can write the summands as

∈ S

AR.

θ

Ht−1,Ht,X0:t ((n, i) , (nt, j) , x0:t)
f
= f θ

H t−1,H t,X 0:t−1 (H t−1 = (n, i) , H t = (nt, j) , x0:t−1)

(n, i) , (nt, j) , x0:t−1)

×

f θ
Xt|Ht−1,Ht,X0:t−1 (xt
= Pθ (H t = (nt, j)
H t−1 = (n, i) , x0:t−1)
f θ
H t−1,X 0:t−1 (H t−1 = (n, i) , x0:t−1)
θ
Xt|Ht,X0:t−1 (xt

(nt, j) , x0:t−1) ,

×

f

|

|

×

|

and Equation (9) follows after noting that

Pθ (H t = (nt, j)

H t−1 = (n, i) , x0:t−1) = pij,

|
(i). Equation (10) is just an application of the

and from the deﬁnition of α(t−1)
law of total probability.

n

14

6
Lemma 6. The complexity of the simple forward algorithm, as given by the
total number of multiplications, is less than

M 2T k+1kk

.

O

Proof. For t = 0, calculating α(0)
tions in total. For each t
all ℓ

AR. Fix t. Noting that

∈ {

}

∈ S

n0 (j) by (8) for all j

∈ S
, ﬁrst consider the case where nt,ℓ

requires M multiplica-
= 1 for

1, . . . , T

(cid:0)

(cid:1)

nt

1 : nt

(t), nt,ℓ > 1 for all ℓ

∈ S}

=

S

(t−1),

∈ S

−
(t−1)

{
then there are
sary pijα(t−1)
then collapses this to
f θ
Xt|H t,X 0:t−1

(xt

|S

|

nt−1 (i) terms since i
(t−1)

−

(M

k) M multiplications to calculate all the neces-
(t−1). The sum
−
∈ S
∈ S
M terms, each of which is then multiplied by
(nt, j) , x0:t−1) which requires

M multiplications.

and nt

c
AR, j

∈ S

(t−1)

|S

1

|

|
Now consider the case where nt,ℓ = 1 for some ℓ

|S

|

AR. Fix t and ℓ.
α(t−1)
nt−1+meℓ (ℓ) for each nt where nt,ℓ = 1 and store

∈ S

Compute the sums

t

m=1
P
them. After computing the sums, there are
stored terms.
Keep t ﬁxed, but allow ℓ to vary. Each of the stored sums is multiplied by
pℓj and f θ
which gives
AR and j
|
a total of 2M k
multiplications. Thus, the total number of
| − |S
multiplications required is

(nt, j) , x0:t−1) for ℓ
(t−1)

Xt|H t,X0:t−1
(t)

|S
(cid:0)
∈ S

(cid:1)
∈ S

| − |S

(t−1)

(xt

|S

(t)

|

|

(cid:0)

(cid:1)

C = M +

T

|S

t=1 h
X
T

(t−1)

(M

|

−

k) M +

(t−1)

|S

|

M +

≤

= M +

M 2

t=1 h(cid:0)
X
T

M 2

3M k + M

(t−1)

|S

|

+ 2M k

(cid:1)

min(t−1,k)

3M k + M

t

−

−

(cid:16)

(t)

|

i
k
m

|S

1

−
m

(cid:19)(cid:18)

m!

(cid:19)

M + 2M k

(t)

(t−1)

|S

| − |S

|

(cid:17)i

t=1
X


(cid:0)


+ 2M k

m=0 (cid:18)
X

(cid:1)

min(t,k)

m=0 (cid:18)
X

t
m

k
m

(cid:19)(cid:18)

m!

(cid:19)



min(t−1,k)



M +

≤

T

t=1
X


(cid:0)


M 2

3M k + M

−

m=0
X

(cid:1)

(t

1)m

−
m!

km
m!

m!

+ 2M k

min(t,k)

m=0
X

tm
m!

km
m!

m!





15

6
tm
m! . This can then be bounded by

by the result

t
m

(cid:0)

(cid:1)

≤

M + T

M 2

3M k + M

−

(cid:0)

M + T (k + 1)

M 2

−

min(T −1,k)

m=0
X

(cid:1)

3M k + M

(T

1)m

−
m!

km
m!

m! + 2T M k

min(T,k)

T m
m!

km
m!

m!

(T

1)k

kk
k!

−
k!

k! + 2T M k(k + 1)

k!

m=0
X

T k
kk
k!
k!
T k+1
(k

1)!

−

kk.

≤

≤

M + (k + 1)

(cid:0)
M 2

−

3M k + M

(cid:0)

(cid:1)

(cid:1)
T (T
(k

1)k
1)!

−
−

kk−1 + 2M (k + 1)

From which we see the complexity is bounded by

M 2T k+1kk

.

O

To overcome possible underﬂow issues, we consider a normalised version of

(cid:1)

(cid:0)

the algorithm. Deﬁne

α(t)

nt (j) :=

(

f θ
H 0,X0 ((n0, j) , x0)
f θ
H t,Xt|X0:t−1

((nt, j) , xt

for t = 0,
for t = 1, . . . T.

x0:t−1)

|
n0 = α(0)
a(0)

n0 (j) from the simple algo-

e

Lemma 7 (A normalised algorithm). Set
rithm. Then, for t = 1, . . . , T calculate

f θ
Xt|Ht,X0:t−1

(xt

f θ
Xt|Ht,X0:t−1

(xt

|

|

e
(nt, j) , x0:t−1)

AR

i∈S c
P

(nt, j) , x0:t−1) pij

pij

α(t−1)
nt−1 (i)

α(t−1)
n

(ℓ)

ℓ∈S
P
if nt,k

e
n∈S (t−1)
P
= 1, for all k
α(t−1)
nt−1+mei (i)
α(t−1)
n

e

AR,

∈ S

(ℓ)

e
n∈S (t−1)
P
otherwise.

ℓ∈S
P

e

t

m=1
P

α(t)

nt (j) :=

e






Then the loglikelihood is given by

T

L(θ) =

log





t=0
X

nt∈S (t)
X

i∈S
X

e
Proof. The deﬁnition of conditional densities gives



α(t)

nt (i)



.

(12)

nt (j) := f θ
α(t)
= f θ

Ht,Xt|X 0:t−1 ((nt, j) , xt
Xt|H t,X 0:t−1 (xt

|

x0:t−1)

(nt, j) , x0:t−1) Pθ (H t = (nt, j)

|

e
where

x0:t−1) ,

|

Pθ (H t = (nt, j)

|

=

x0:t−1)

nt−1∈S (t−1)
X

i∈S
X

Pθ (H t−1 = (nt−1, i) , H t = (nt, j)

x0:t−1) .

|

(13)

16

6
Using the same arguments as in the proof of Lemma 5, the right-hand side of
(13) simpliﬁes to

t

m=1
X

Pθ

(H t−1 = (nt

−

1 + mei, i) , H t = (nt, j)

x0:t−1)

|

when nt,i = 1 for some i

AR,

∈ S
Pθ (H t−1 = (nt

1, i) , H t = (nt, j)

x0:t−1)

|

−

(14)

(15)

i∈S c
X
AR
= 1 for all ℓ

when nt,ℓ
in the form
Pθ

(H t = (nt, j)

AR. The summands in (14) and (15) can be written

∈ S

|

H t−1 = (nt−1, i) , x0:t−1) Pθ
((nt−1, i) , xt−1|

f θ
Ht−1,Xt−1|X0:t−2
f θ
Ht−1,Xt−1|X0:t−2

((n, ℓ) , xt−1|

x0:t−2)

x0:t−2)

(H t−1 = (nt−1, i)

x0:t−1)

|

= pij

= pij

ℓ∈S
P

n∈S (t−1)
P
α(t−1)
nt−1 (i)

α(t−1)
n

,
(ℓ)

ℓ∈S
P

e
n∈S (t−1)
P

which proves the result for the iterations. Equation (12) holds from the law of
total probability.

e

Lemma 8. The complexity of the normalised forward algorithms, as given by
the total number of multiplications, is less than

M 2T k+1kk

.

O

Proof. The normalised algorithm is the same as the forward algorithm except
(cid:1)
(ℓ). The most eﬃcient way to do
that each term is divided by

α(t−1)
n

(cid:0)

n∈S (t−1)
P
this extra step is to do the division pij/
e
ℓ∈S
P

ℓ∈S
P

which results in an additional T M 2 multiplications in total.

n∈S (t−1)
P

α(t−1)
n

(ℓ) for i, j

ﬁrst,

∈ S

e

3 A novel backward algorithm

The goal of the backward algorithm is to calculate the smoothed probabilities

nt (i) := Pθ
γ(t)

(H t = (nt, i)

x0:T ) ,

|

for t = 0, 1, . . . , T , nt = (n1,t, . . . , nk,t)
. The smoothed
probabilities are often of interest in their own right, but are also typically used to
construct an EM algorithm. Recall that as a byproduct of the forward algorithm
we obtain the ﬁltered probabilities

(t) and i

∈ S

∈ S

nt (j) := Pθ (H t = (nt, j)
α(t)

x0:t) =

|

b

e
nt∈S (t)
P

ℓ∈S
P

17

e

α(t)
nt (j)

,
α(t)
nt (ℓ)

6
as well as the prediction probabilities

nt (j) = Pθ (H t = (nt, j)
φ(t)

x0:t−1) ,

|
(t), and all t = 0, . . . , T . These are the inputs to the backward

, nt

for j
algorithm.

∈ S

∈ S

Lemma 9 (A backward algorithm). The smoothed probabilities can be calculated
using the following procedure. Set γ(T )
(T ).
nT (i) =
2, . . . , 0 and for nt
1, T
Then, for t = T

α(T )
nT (i) , for all i
(t) calculate

, nT

∈ S

∈ S

−

−

γ(t)
nt (i) =

α(t)
nt (i)

b
α(t)
nt (i)






b

pij

pij

j∈S
P

j∈S
P

∈ S
b
γ(t+1)
nt+1 (j)
φ(t+1)
nt+1 (j)
γ(t+1)
t +1 (j)
n(−i)
φ(t+1)
t +1 (j)
n(−i)

for i

c
AR,

∈ S

for i

AR.

∈ S

Proof. Consider the event H t = (nt, i). By the deﬁnition of nt, when i
then nt+1 = n(−i)
H t = (nt, i) is known, then nt+1 is also known. As a result,

AR,
AR, then nt+1 = nt + 1. Thus, when

t + 1, and when i

∈ S

∈ S

c

nt (i) = Pθ
γ(t)
=

(H t = (nt, i)

x0:T )

|

Pθ (H t = (nt, i) , H t+1 = (nt+1, j)

x0:T )

|

nt+1∈S (t+1)

j∈S X
X

Pθ

H t = (nt, i) , H t+1 =

(cid:16)

Pθ (H t = (nt, i) , H t+1 = (nt + 1, j)

(cid:16)

n(−i)

t + 1, j

x0:T

|
(cid:17)
x0:T )

|

(cid:17)

for i

AR,

∈ S

for i

c
AR,

∈ S

(16)

= 


j∈S
P
j∈S
P



1, nt
for t = 0, 1, . . . , T
the same for both cases, i
n take the value n(−i)
The summands on the right hand side of (16) can be written as

. Since the following arguments are
c
AR, for notational convenience, let
c
AR.

AR and the value nt + 1 when i

(t) and i
AR and i

t + 1 when i

∈ S
∈ S

∈ S

∈ S

∈ S

∈ S

−

f θ
Ht,Ht+1,X t+1:T |X0:t

((nt, i) , (n, j) , xt+1:T

x0:t)

|

f θ
Xt+1:T |X 0:t
(H t = (nt, i)

x0:t) Pθ

(xt+1:T

x0:t)

|

= Pθ

|

f θ
X t+1:T |Ht,H t+1,X 0:t

×

f θ
X t+1:T |X0:t
x0:t) pij

= Pθ (H t = (nt, i)
f θ
X t+1:T |Ht,H t+1,X 0:t

|

×

f θ
X t+1:T |X0:t

(H t+1 = (n, j)
(xt+1:T

|

(nt, i) , (n, j) , x0:t)

H t = (nt, i) , x0:t)

|
(xt+1:T

x0:t)

|

(xt+1:T

|
(xt+1:T

(nt, i) , (n, j) , x0:t)
,

x0:t)

|

(17)

18

where the last equality holds since H t+1 is independent of x0:t given H t. Now,
noting that xt+1:T is independent of H t given H t+1 and x0:t, then the right-
hand side of (17) equals
Pθ (H t = (nt, i)
f θ
X t+1:T |X 0:t

f θ
Xt+1:T |Ht+1,X0:t (xt+1:T

(n, j) , x0:t) ,

|

|

|
(xt+1:T
Pθ (H t = (nt, i)
f θ
Xt+1:T |X 0:t
Pθ (H t = (nt, i)
f θ
Xt+1:T |X 0:t

x0:t) pij
x0:t)
x0:t) pij
x0:t)
|
x0:t) pij
x0:t)

|
(xt+1:T

|
(xt+1:T

|

=

=

= Pθ (H t = (nt, i)

x0:t) pij

|

f θ
X t+1:T ,Ht+1|X 0:t

(xt+1:T , H t+1 = (n, j)

x0:t)

|

f θ
X t+1:T |X0:t

x0:t)
x0:t) Pθ (H t+1 = (n, j)

Pθ (H t+1 = (n, j)
(xt+1:T
Pθ (H t+1 = (n, j)

|

|

x0:t)

|

x0:T )

|

Pθ (H t+1 = (n, j)
Pθ (H t+1 = (n, j)

x0:T )
x0:t)

|
|

=

α(t)

nt (i) pij

γ(t)
n (j)
.
φ(t)
n (j)

Writing out n explicitly for the two cases completes the proof.

b

Lemma 10. The total complexity of the backward algorithm in Lemma 9, as
measured by the total number of multiplications, is less than

M 2T k+1kk

.

O

Proof. First, for each t

T

∈ {

−

1, ..., 0

we need to calculate the ratio

(cid:0)

}
γ(t+1)
nt+1 (j)
φ(t+1)
nt+1 (j)

(cid:1)

for every corresponding nt
multiplications.
This quantity is independent of i, thus only needs to be done once for a given t
if we save the results.

. This costs M

(t) and j

∈ S

∈ S

|S

(t)

|

Now consider t, i and nt ﬁxed. The multiplication of pij and

γ(t+1)
nt+1 (j)
φ(t+1)
nt+1 (j)

is done for every j
results in a single term, which is then multiplied by the corresponding
and this costs an additional 1 multiplication. We do this for all i
nt

which costs M multiplications. The sum over j
∈ S
α(t)
nt (i),
and

∈ S

(t)

∈ S
b

(t), which costs (M + 1) M
∈ S
So, for a given t we execute M

multiplications.
+ (M + 1) M

(t)

multiplications. This

|S
1, so the total number of multiplications is

|

|S
|S

(t)

|
|

is done for every t = 0, . . . , T

−

T −1

M

t=0 (cid:16)
X

(t)

|S

|

+ (M + 1) M

(t)

|S

|

(cid:17)

=

M 2 + 2M

(cid:0)
M 2 + 2M

(cid:1)

T −1

(t)

|S

t=0
X
T (T

−

≤

=

M 2T k+1kk

(cid:1)

,

(cid:0)
O

|
1)k kk−1 (k + 1)
1)!
(k

−

where we have used similar arguments to Lemma 6 to bound the complexity.

(cid:0)

(cid:1)

19

Of importance to the next section, note that we can obtain from the back-

ward algorithm, the smoothed probabilities

Pθ (Rt = i, Nt,i = ℓ

x0:T ) =

|

Pθ (H t = (nt, i)

x0:T ) .

|

Xnt∈S (t):
nt,i=ℓ

4 A novel EM algorithm

Here we show how the output from the backward algorithm can be used to
implement an exact, computationally feasible EM algorithm for MRS models
with independent regimes.

4.1 The E-step

Recall that the EM algorithm [7] is an iterative procedure, alternating between
an expectation step and a maximisation step. In the expectation step the func-
tion Q (θ, θn) is constructed as

Q (θ, θn) = E[log f θ
θ
= E[log f

X0:T ,R (x0:T , R)
|
X0:T ,H0,...,H T (x0:T , H 0, . . . , H T )

x0:T ; θn]

x0:T ; θn],

|

(18)

where R = (R0, . . . , RT ) is a sequence of the hidden Markov chain
, and
(H 0, . . . , H T ) is a sequence of the corresponding augmented hidden process
. The information contained in the sequences R and (H 0, . . . , H T ) is
H t
{
entirely equivalent, but we opt for the latter representation to remain consistent
with, and emphasise the place of, the work in the previous sections. In the M-
step of the algorithm, the maximisers arg max

Q (θ, θn) are found.

Rt

}

}

{

For MRS models Q (θ, θn) can be written as

θ∈Θ

Q (θ, θn) = E

log f θ

X0:T |H0,...,HT (x0:T
|
log Pθ (H 0, . . . , H T )

H 0, . . . , H T )
x0:T ; θn

.

x0:T ; θn

|

i

(19)

h
+ E

(cid:2)

|
H t

(cid:3)

Using the augmented hidden Markov chain,
t∈N, (19) can be written in
such a way that the function Q is computationally feasible. First note that,
given H t and X 0:t−1, Xt is independent of H s for s
= t which allows the

}

{

20

6
(x0:T

H 0, . . . , H T ) to be written as

function log f θ

X0:T |H 0,...,HT

log f θ

X0:T |H0,...,HT (x0:T

|

= log f θ

X0|H 0 (x0|

H 0) +

|
H 0, . . . , H T )
T

t=1
X

log f θ

Xt|Ht,X0:t−1 (xt

H t, x0:t−1)

|

f θ
X0|H0 (x0|

(n0, j))

I(H0=(n0,j))




f θ
Xt|Ht,X 0:t−1 (xt


(nt, j) , x0:t−1)

I(Ht=(nt,j))

|

nt∈S (t)

j∈S Y
Y



I (H 0 = (n0, j)) log f θ


X0|H 0 (x0|

(n0, j))

(20)






= log

n0∈S (0)


j∈S Y

Y
T


+

log

t=1
X

=

n0∈S (0)

j∈S X
X
T

+

t=1
X

nt∈S (t)
j∈S X
X

I (H t = (nt, j)) log f θ

Xt|H t,X 0:t−1 (xt

(nt, j) , x0:t−1) .

|

(21)

Since f θ
similarly for f θ

Xt|Ht,X0:t−1

(xt
|
X0|H0 (x0|

(nt, j) , x0:t−1) = f θ
(nt,j, j)), the expression (21) simpliﬁes to

Xt|Nt,j ,Rt,X0:t−1

(xt

|

nt,j, j, x0:t−1), and

I (N0,j = 1, R0 = j) log f θ

X0|N0,j,R0 (x0|

j∈SAR
X
T

t

1, j)

(22)

I (Nt,j = m, Rt = j) log f θ

Xt|Nt,j ,Rt,X 0:t−1 (xt

m, j, x0:t−1)

|

+

+

+

AR

j∈S c
X
T

t=1
X

j∈S c
X

AR

t=1
X

m=1
X

j∈SAR
X
I (R0 = j) log f θ

X0|R0 (x0|

j)

I (Rt = j) log f

θ
Xt|Rt (xt

j) .

|

(23)

θn
H0,...,H T |X0:T

Taking the expectation of (23) with respect to the distribution f

21

Pθn (Nt,j = m, Rt = j

x0:T ) log f θ

Xt|Nt,j ,Rt,X0:t−1 (xt

|

m, j, x0:t−1)

|

(equivalently the distribution f

θn
R|X0:T

) gives

θ

E

log f

h
=

X0:T ,H 0,...,HT |X 0:T (x0:T , H 0, . . . , H T )

x0:T ; θn

Pθn (N0,j = 1, R0 = j

x0:T ) log f θ

|

|
i
1, j)
X0|N0,j ,R0 (x0|

j∈SAR
X
T

t

+

+

+

t=1
X

m=1
j∈SAR
X
X
Pθn (R0 = j

j∈S c
X
AR
T

x0:T ) log f θ

X0|R0 (x0|

|

j)

Pθn (Rt = j

x0:T ) log f θ

Xt|Rt (xt

|

j) .

|

AR

j∈S c
X

t=1
X
Using similar arguments, E

log Pθ (H 0, . . . , H T )

is found to be

x0:T ; θn

|

(cid:3)

(cid:2)
log Pθ (H 0, . . . , H T )

E

x0:T ; θn

(cid:20)

= E

=





i∈S
X

(cid:12)
(cid:12)
(cid:12)
(cid:12)

I(R0=i)
π
i

log




i∈S
Y
Pθn (R0 = i



|

i,j∈S
Y
x0:T ) log πi +

(cid:21)

pηij
ij 


(cid:12)
(cid:12)
(cid:12)
(cid:12)

i,j∈S
X

x0:T ; θn



E [ηij


x0:T ; θn] log pij,
|

where ηij is the random variable counting the number of transitions from state
Rt−1 = i to state Rt = j in the sequence R. The expectation E [ηij
x0:T ; θn]
can be calculated as

|

T

E [ηij

|

x0:T ; θn] = E

I (Rt−1 = i, Rt = j)

"
T

t=1
X
Pθn (Rt−1 = i, Rt = j

=

x0:T ; θn
(cid:12)
(cid:12)
(cid:12)
(cid:12)
x0:T ) .

#

|

t=1
X

22

So, the function Q is
Q (θ, θn)

j∈SAR
X

j∈S c
X
AR
T

=

+

+

+

+

t=1
X
T

j∈SAR
X

m=1
X
Pθn (Rt = j

AR

j∈S c
X
Pθn (R0 = i

t=1
X

i∈S
X

Pθn (N0,j = 1, R0 = j

x0:T ) log f θ

X0|N0,j ,R0 (x0|

|

1, j)

Pθn (R0 = j

x0:T ) log f

|

θ

X0|R0 (x0|

j)

t

Pθn (Nt,j = m, Rt = j

x0:T ) log f

θ
Xt|Nt,j ,Rt,X0:t−1 (xt

|

m, j, x0:t−1)

|

x0:T ) log f

θ
Xt|Rt (xt

|

j)

|

x0:T ) log πi +

|

T

i,j∈S
X

t=1
X

Pθn (Rt−1 = i, Rt = j

x0:T ) log pij.

|

(24)

Lemma 11. The joint probabilities are given by
Pθn (Rt−1 = i, Rt = j
x0:T )

|

Pθn (Nt,i = 1, Rt = j

x0:T )

|
Pθn (Rt = j, N t = nt

x0:T )

|

when i

AR,

∈ S

p(n)
ij

Pθn (Rt−1 = i, N t−1 = nt
p(n)
kj

1
−
Pθn (Rt−1 = k, N t−1 = nt

|

x0:t−1)

x0:t−1)

1

|

−

, when i

c
AR.

∈ S

(25)

=






nt−1∈S (t−1)
P

×

k∈S c
P

AR

This proof follows similar arguments to those in [20], which develops algo-

rithms for MRS models with dependent regimes.

AR, note that Nt,i = 1 if and only if Rt−1 = i and we

∈ S

c

AR all counters in nt are diﬀerent from 1, so nt

1

−

∈ S

(t−1).

Proof. For the case i
are done.

When i

∈ S

Thus
Pθn (Rt−1 = i, Rt = j

x0:T )

|

=

=

=

nt−1∈S (t−1)
X

nt−1∈S (t−1)
X

nt−1∈S (t−1)
X
Pθn (Rt−1 = i

×

|

Pθn (N t = nt, Rt−1 = i, Rt = j

x0:T )

|

Pθn (N t = nt, Rt = j

Pθn (N t = nt, Rt = j

x0:T ) Pθn (Rt−1 = i

|

N t = nt, Rt = j, x0:T )

x0:T )

|

|

N t = nt, Rt = j, x0:t−1) .

(26)

23

The last equality holds since, given Rt and N t, then xt:T is independent of
Rt−1. Focusing on the right-most term in Equation (26),

Pθn (Rt−1 = i
|
Pθn (Rt = j

N t = nt, Rt = j, x0:t−1)
N t = nt, Rt−1 = i, x0:t−1) Pθn (Rt−1 = i

|

Pθn (Rt = j

N t = nt, x0:t−1)

N t = nt, x0:t−1)

|

=

=

p(n)
ij

Pθn (N t = nt, Rt−1 = i
Pθn (N t = nt, Rt = j

|
x0:t−1)

|
x0:t−1)

,

|

where p(n)
ij
Rt−1, then Rt is independent of N t and X 0:t−1.

is the parameter pij in θn; the second equality holds since, given

Now, notice that

Pθn (N t = nt, Rt−1 = i

|

x0:t−1) = Pθn (N t−1 = nt

1, Rt−1 = i

−

x0:t−1) ,

|

since i

c
AR, and that

∈ S

Pθn (N t = nt, Rt = j

x0:t−1) =

|

p(n)
kj

Pθn (N t−1 = nt

1, Rt−1 = k

−

x0:t−1) ,

|

k∈S c
X

AR

with the sum in the denominator being over k
is nt

c
AR since only when k
(t−1) deﬁned; this completes the proof.

∈ S

1

−

∈ S

c
AR

∈ S

4.2 The M-step
Next, the maximisers, θn+1 = arg max

θ∈Θ

Q (θ, θn), are needed. The maximisers

for the parameters of each regime are generally problem speciﬁc, but the max-
imisers for the parameters pij, i, j
, can be derived in general. By the work
of [11],

∈ S

p(n+1)
ij

=

T

t=1
P

Pθn (Rt = j, Rt−1 = i

x0:T )
.

|

Pθn (Rt−1 = i

x0:T )

|

T

t=1
P

However, note that to get this analytic update for the p(n+1)
parameters, terms
involving πj in Equation (24) have been treated as if they are unrelated to
. However, this is not true when π is speciﬁed as the stationary
pij, i, j
, but holds for other cases, such as when π is
distribution of the process
some predetermined distribution, or when π is speciﬁed as a parameter to be
inferred. Nonetheless, this simpliﬁcation is appropriate if we assume that, as the
sample size grows, the contribution of terms involving R0 become insigniﬁcant.

∈ S

Rt

{

}

ij

24

4.3 Model-speciﬁc M-step updates

In electricity price models, it is common to specify spike or drop regimes as
either shifted-Gamma, shifted-log-normal, or occasionally a Gaussian distribu-
tion. Here we derive M-step updates for these regimes.

Corollary 12. Suppose Regime i is i.i.d. N

µi, σ2
i

. The M-step updates µ(n+1)

i

and

σ(n+1)
i

(cid:16)

2

(cid:17)

, for n

0, are

≥

µ(n+1)
i

=

T

t=0
P
T

(cid:0)

(cid:1)

Pθn (Rt = i

x0:T ) xt

|

,

Pθn (Rt = i

x0:T )

|

Pθn (Rt = i

x0:T )

xt

t=0
P
T

σ(n+1)
i
(cid:16)

2

=

(cid:17)

t=0
P

µ(n+1)
i

−

2

.

(cid:17)

|

(cid:16)
Pθn (Rt = i

x0:T )

|

T

t=0
P

2

, θn) and solving for zeros.
is a maximiser is shown by the second derivative test. Furthermore,
can be shown to be a maximiser by comparing the value of Q (θ, θn)

Proof. The results holds after diﬀerentiating the Q (
·
That µ(n+1)
i
σ(n+1)
i
(cid:17)
(cid:16)
σ(n+1)
when σ2
i =
i
σ2
i , and utilising the inequality 1

to the value of Q (θ, θn) evaluated at any other value of

log y.

1/y

(cid:16)

(cid:17)

2

−

≤

At time t, if Rt = i is a shifted-Gamma regime so that Xt

,
where qi is known, the M-step is not completely analytic. However, the di-
(cid:1)
mension of the maximisation problem can be reduced from 2-dimensional to
1-dimensional via the following corollary.

Gam

∼

−

qi

(cid:0)

µi, σ2
i

Corollary 13. Suppose Regime i follows an i.i.d. shifted-Gamma distribution,
, and suppose the
that is, if Xt is from Regime i, then (Xt
2

µi, σ2
i

Gam

qi)

∼

−

parameter qi is known. The M-step update for the scale parameter,
as a function of µ(n+1)

, is

(cid:1)

(cid:0)

i

σ(n+1)
i
(cid:16)

(cid:17)

,

Pθn (Rt = i

x0:T ) (xt

|

−

qi)
.

Pθn (Rt = i

x0:T )

|

T

t=0
P

σ(n+1)
i

µ(n+1)
i

2

= µ(n+1)
i

(cid:16)

(cid:16)

(cid:17)(cid:17)

T

t=0
P

25

The update for µi is then found by ﬁnding

µ(n+1)
i

= arg max

µi∈(0,∞)( −

µi log

Pθn (Rt = i

x0:T )

|

σ(n+1)
i
(cid:16)

T

T

2

(µi)

(cid:17)

t=0
X
Pθn (Rt = i

x0:T )

|

Pθn (Rt = i

x0:T ) log (xt

|

qi)

−

log Γ (µi)

−

t=0
X
T

+ (µi

1)

−

T

t=0
X

µi

−

t=0
X

Pθn (Rt = i

x0:T )

|

,

)

where Γ (
·

) is the Gamma function.

Proof. The result follows after diﬀerentiating Q with respect to σ2
for the stationary point, which is a maximum by the second derivative test.

i , and solving

Corollary 14. Suppose Regime i follows i.i.d. shifted-log-normal dynamics,
, and suppose the
that is, if Xt is from Regime i, then log (Xt
parameter qi is known. The M-step updates are

µi, σ2
i

qi)

N

−

∼

(cid:0)

(cid:1)

T

t=0
P

T

t=0
P

µ(n+1)
i

=

2

=

σ(n+1)
i
(cid:16)

(cid:17)

Pθn (Rt = i

x0:T ) log (xt

|

qi)

−

,

Pθn (Rt = i

x0:T )

|

T

t=0
P

Pθn (Rt = i

x0:T )

log (xt

qi)

−

−

µ(n+1)
i

2

.

(cid:17)

|

T

t=0
P

(cid:16)
Pθn (Rt = i

x0:T )

|

Proof. The proof is similar to the proof of Corollary 12.

Note that Corollaries 13 and 14 assume the parameter qi is known. This
is necessary for the shifted-log-normal distribution [12] and the shifted-Gamma
distribution when the shape parameter µi is less than 1 [19]. Furthermore, for
the shifted-Gamma distribution, [19] observe that related issues arise when µ is
near 1, and advise against maximum likelihood estimation of qi when µ < 2.5.
Simulations suggest this is also good advice when ﬁtting MRS models with
shifted-Gamma regimes [22].

In electricity price modelling literature it is common to specify a ‘base
regime’ as an AR(1) process. In existing literature the AR(1) regimes are as-
sumed to evolve at every time t but are only observed when in that regime.
Another possibility is that the AR(1) processes evolve only when they are ob-
I (Rℓ = i), and the AR(1) process in Regime i

served, that is, deﬁne τi (t) =

t

ℓ=0
P

26

{

Bτi(t)}τi(t)∈N. Our algorithms are applicable to both speciﬁcations; how-
as
ever, for simplicity, here we treat the former only. For more details on the latter
speciﬁcation, see [22].

Corollary 15. If Regime i is an AR(1) regime of an MRS model, the M-step of
2
the EM algorithm can be executed as follows. The updates α(n+1)
as functions of φ(n+1)

σ(n+1)
i

and

are

(cid:17)

(cid:16)

i

i

α(n+1)
i

φ(n+1)
i
(cid:16)

(cid:17)

σ(n+1)
i

φ(n+1)
i
(cid:16)

2

=

(cid:17)

T

t+1

=

t=0
P
T

m=1
P
t+1

t=0
P
T

m=1
P
t+1

t=0
P

m=1
P

Pθn (Rt = i, Nt,i = m

x0:T ) B(i)
t,m

|

,

Pθn (Rt = i, Nt,i = m

x0:T ) A(i)
t,m

|

Pθn (Rt = i, Nt,i = m

x0:T ) C(i)
t,m

|

,

Pθn (Rt = i

x0:T )

|

T

t=0
P

where

A(i)

t,m =

m

1

−
1

φ(n+1)
i
(cid:17)
φ(n+1)
i

(cid:16)
−













B(i)

t,m =

xt

−

(cid:16)

(cid:16)

φ(n+1)
i

m

xt−m

(cid:17)

C(i)

t,m =

xt

−





α(n+1)
i

φ(n+1)
i

(cid:16)

(cid:17)




1

1 +

1 +

φ(n+1)
i
(cid:16)
φ(n+1)
i

,

m
(cid:17)

(cid:16)
1 +

1 +

(cid:17)

1

m,
(cid:17)


(cid:17)
φ(n+1)
i
(cid:16)
φ(n+1)
i
(cid:16)

(cid:17)
φ(n+1)
i
(cid:16)
(cid:17)
φ(n+1)
i
−
φ(n+1)
i

−
1

2m

m



−

1



is given by

(cid:16)
−

(cid:17)
φ(n+1)
i
(cid:16)

(cid:17)

2 




The M-step update for φ(n+1)

i

φ(n+1)
i

= arg max
φi∈(−1,1)

g (φi)

φ(n+1)
i
(cid:16)



−



m

xt−m

(cid:17)

2





.

= arg max
φi∈(−1,1)

T

t+1

t=0
X

m=1
X

Pθn (Rt = i, Nt,i = m

|

x0:T ) Lt,m

φi, σ(n+1)
i

,

(cid:16)

(cid:17)

where

Lt,m

φi, σ(n+1)
i

=

(cid:16)

(cid:17)

1
2

log

(

1
1

−
−

φ2
i
φ2m
i ) −

log

σ(n+1)
i

(φi)

.

n

o

27

Proof. Diﬀerentiate Q with respect to αi and σ2
i and solve for when the deriva-
tive is zero. The parameter αi can be shown to be a maximiser by the second
derivative test, and σ2
i can be seen to be a maximiser using the same argu-
ment as used in the proof of Corollary 12. Next, substitute the maximisers,
α(n+1)
, into (24) and collect all terms involving
i
φi, to give the function g. That we need to search for the global maximiser of g
(cid:17)
(cid:16)
1, 1) only comes from the fact that we have assumed Regime
on the interval (
i is a stationary or mean-reverting process, in which case
< 1 is a necessary
condition.

φ(n+1)
i
(cid:16)

and σ(n+1)
i

φ(n+1)
i

φi

−

(cid:17)

2

|

|

|

e

(nt, i) , x0:t−1) =

−
fXt|Rt,X 0:t−1 (xt
|

AR, we let fXt|Ht,X0:t−1 (xt
fXt|Rt,X0:t−1 (xt

Remark 4.1: Truncation The forward and backward algorithms can be
computationally costly when T and/or k are large, so it may be preferential (or
necessary) to truncate the problem. We suggest that the memory of each of the
AR(1) processes be truncated. That is, for all nt such that nt,i > D
1 for some
i, x0:t−1) ,
i
∈ S
i, x0:t−1) is a density that does not depend on nt. An
where
|
f will be problem-speciﬁc. This truncation is equivalent
appropriate choice of
to truncating the state space of H t so that Nt,i
, and adjusting
the transitions of H t so that the counters remain at D, rather than contin-
uing to increase as they would in the original process. It can be shown that
(M 2DkT kk). For processes
the complexities of the truncated algorithms are
that decay to stationary between times at which they are observed (such as the
AR(1) processes in independent-regime MRS models used in this paper, see Sec-
i, x0:t−1) as the stationary distribution in
tion 4.3), then taking
Regime i is a logical choice. Furthermore, D should be chosen large enough so
that there is a low probability that
ever visits any speciﬁc state for more
Rt
than D consecutive transitions and of course D > k.

fXt|Rt,X0:t−1 (xt

1, . . . , D

∈ {

O

e

e

e

}

{

}

|

5 Simulation studies

We perform a simulation study to examine the properties of the algorithms.
The models used in the simulations are the following:

Xt =

Bt
St

(

for Rt = 1,
for Rt = 2,

(Model 1)

{

}

Bt

i.i.d. N (0, 1), St
1, 2

is an AR(1) process deﬁned by Bt = 0 + 0.75Bt−1 + εt,
is a Markov chain with state
Rt
}
, transition matrix entries p11 = p22 = 0.9, and initial distribu-

i.i.d N(0, 1) and

∼

=

{

where
εt
∼
space
S
tion (0.5, 0.5);

{

}

Xt =

B(1)
t
B(2)
t

(

for Rt = 1,
for Rt = 2,

(Model 2)

28

B(i)
where
t }
{
, ε(i)
and B(2)
t = 0 + 0.4B(2)
t ∼
chain with state space
1, 2
}
and initial distribution (0.5, 0.5).

t−1 + ε(2)
t
=
S

{

are independent AR(1) process deﬁned by B(1)

t−1+ε(1)
is a Markov
, transition matrix entries p11 = p22 = 0.6,

t = 0+0.9B(1)
Rt

i.i.d. N (0, 1) i = 1, 2, and

}

{

t

|

×

O

θn

T 1.70

θn+1 −

|∞ was less that 1.5

To investigate the bias and consistency of the MLE, 20 independent real-
isations of Models 1 and 2 were simulated for T = 50, 100, 200 and 400, and
the EM algorithm used to ﬁnd the MLE. The terminating criteria for the algo-
rithms was to stop when either the increase in the likelihood, or the step-size,
10−8. To attempt to avoid local
as measure by
maxima, the EM algorithm was initialised from 100 randomised values centred
around the true parameters. Of the corresponding 100 terminating points of
the EM algorithm, the parameters that achieved the highest loglikelihood value
were kept. Figure 3 shows box plots of the 20 terminating points of the EM
algorithm, one for each simulation, as well as the log-runtime and value of the
loglikelihood. For both models the MLE appears to be converging to the true
parameter value as sample sizes increase. Generally, there appears to be a much
larger variation in the MLEs for Model 2 than for Model 1. This could be be-
cause the inference problem for Model 2 is harder, as the regimes in Model 2
are more similar than they are in Model 1, or because of the nature of the hid-
den Markov chain is such that there is a lower probability of remaining in each
regime (p11 = p22 = 0.6 for Model 2, compared to p11 = p22 = 0.9 for Model 1),
or both. Regarding the run time, for data sets of length 100 and greater, the
empirical results suggest the complexity is approximately
for Model 1
and
for Model 2, which agrees with, and is signiﬁcantly better than,
our theoretical upper bound.
(cid:1)

To investigate the truncation method, we used the same data sets simulated
above for T = 400 and applied our truncated algorithm to this data with var-
ious truncation levels, D = 5, 10, 20, 40. The termination criteria for the EM
algorithm was to stop when either the increase in the likelihood, or the step-size,
10−8. Figure 4 plots the log10
as measure by
of the absolute value of the diﬀerence between the parameters recovered by the
truncated algorithm and the full algorithm. Figure 4 also shows log10 of the
absolute value of the diﬀerence in the loglikelihoods achieved by the truncated
algorithm and the full algorithm. Figure 4 also plots of the relative runtime
of the truncated algorithm compared to the full algorithm, that is, the ratio
of the time taken for the truncated algorithm compared to the full algorithm.
The run times for the truncated algorithms are signiﬁcantly lower than the full
algorithm. For these data sets the truncated algorithm performs reasonably
well, even when limiting the memory of the counters to just 5 time steps. For a
truncation level of 40, the errors are of the order 10−6 to 10−10 for both mod-
els, which is similar to the stopping criteria for the EM algorithm which is of
the order 10−8. For Model 1 the truncation appears to have a more signiﬁcant
negative eﬀect on the parameter estimates compared to Model 2. This could
be because the probability of staying in each regime is higher in Model 1 than
in Model 2, and therefore the probability of remaining in one regime for more

|∞ was less that 1.5

θn+1 −

T 1.02

θn

O

×

(cid:1)

(cid:0)

(cid:0)

|

29

3

2

1

0

-1

4

3

2

1

0

6

4

2

0

-2

-4

0.8

0.6

0.4

0.2

2.5

2

1.5

1

0.5

50 100 200 400
T

50 100 200 400
T

50 100 200 400
T

50 100 200 400
T

4

2

0

0.8

0.6

0.4

0.2

0

3

2

1

4

3

2

1

0

50 100 200 400
T

50 100 200 400
T

50 100 200 400
T

0.5

0

-0.5

4

3

2

1

0

0.95

0.9

0.85

0.8

0.75

0.9

0.8

0.7

0.6

3

2

1

0

50 100 200 400
T

50 100 200 400
T

-200

-400

-600

-800

8

6

4

2

50 100 200 400
T

50 100 200 400
T

-200

-400

-600

1

0.8

0.6

0.4

1

0.8

0.6

0.4

0.2

50 100 200 400
T

50 100 200 400
T

50 100 200 400
T

50 100 200 400
T

50 100 200 400
T

50 100 200 400
T

50 100 200 400
T

50 100 200 400
T

Figure 3: Box plots of MLEs, runtime (in log-seconds) and loglikelihood for
Models 1 (top) and 2 (bottom) found via the EM algorithm. Each boxplot
contains 20 independently simulated data sets. The x-axis is the length of the
simulated data set, T = 50, 100, 200 and 400. The true parameter values are
marked as horizontal lines. For both models the MLE appears to be consistent.
For Model 2 (bottom), the two regimes have similar behaviour with only the
parameters φi diﬀering between them by 0.25. With a data set of length T = 400
the algorithm appears to be able to diﬀerentiate the two regimes.

30

5

10 20 40

Truncation

5

10 20 40

Truncation

0

-2

-4

-6

-8

0

-2

-4

-6

-8

-10

0

-2

-4

-6

-8

0

-2

-4

-6

-8

-10

0

-2

-4

-6

-8

-10

0

-2

-4

-6

-8

0

-2

-4

-6

-8

5

10 20 40

Truncation

0

-2

-4

-6

-8

0

-2

-4

-6

-8

5

10 20 40

Truncation

0

-2

-4

-6

-8

-10

0

-2

-4

-6

-8

12

10

8

6

r
e
t
s
a
f

s
e
m
T

i

5

10 20 40

Truncation

5

10 20 40

Truncation

0

-2

-4

-6

-8

5

10 20 40

Truncation

5

10 20 40

Truncation

5

10

20

40

Truncation

5

10 20 40

Truncation

0

-2

-4

-6

-8

5

10 20 40

Truncation

5

10 20 40

Truncation

0

-2

-4

-6

-8

0

-2

-4

-6

-8

0

-2

-4

-6

-8

150

100

50

r
e
t
s
a
f

s
e
m
T

i

5

10 20 40

Truncation

5

10 20 40

Truncation

0

-2

-4

-6

-8

5

10 20 40

Truncation

5

10 20 40

Truncation

5

10

20

40

Truncation

5

10 20 40

Truncation

5

10 20 40

Truncation

Figure 4: Box plots of the log10 of the absolute value of the diﬀerence between
the MLEs and loglikelihood values found by the full algorithm and the trun-
cated algorithm, and plots of the ratio of the runtime of the truncated algorithm
compared to the full algorithm, for Model 1 (top) and Model 2 (bottom). Each
boxplot contains 20 independently simulated data sets. The x-axis is the max-
imum memory of the counters, D = 5, 10, 20, 40. For plots corresponding to
parameter estimates or loglikelihood values, the y-axis is the log10 error be-
tween the MLE found by the full algorithm and the truncated algorithm, for
the runtime plots the y-axis is multiples of the runtime of the full algorithm.
For both models, a truncation level of D = 40 gives a relative error of the or-
der approximately 10−8 which is the order of the stopping criterion of the EM
algorithm.

31

 
 
Table 1: Distributions used to sample initial parameter values for the EM algo-
rithm.

Parameter
Distribution U (

αi

φi

σ2
i

µ

p11

p22

1, 1) U (

−

1, 1) U (0, 4) U (0, 8) U (0, 1) U (0, 1)

−

that D consecutive transitions is lower in Model 2. It is likely that the error
due to truncation of the algorithm is insigniﬁcant compared to statistical error
of parameter estimates.

To investigate the convergence properties of the EM algorithm we used ﬁve
of the simulated data from above with T = 400 and used the truncated EM
algorithm, with a truncation level of D = 40 to search for maxima. For each of
the ﬁve simulated datasets we ran the EM algorithm 50 times, sampling initial
values for the algorithm independently each time and used the same terminating
criteria as before. The sampling distributions for the initial parameter values
of the algorithm are summarised in Table 1. For Model 2 we have ordered the
terminating values of the EM algorithm such that φ1 > φ2 for identiﬁability.

Observing Figure 5 we see that, for most of the simulations and most of
the starting values, the EM algorithm ﬁnds a single maxima. In Figure 5 the
black cross represents the point which achieved the highest maximum. We refer
to this point as the optimal parameter value. The proportion of times that
the algorithm converged to the optimal parameter value is reported in Table
2. Simulation 4 of Model 1 is outstanding since only 14% of initial values
resulted in the algorithm ﬁnding the optimal value. In Figure 5 there are some
instances when the algorithm terminates at a suboptimal point; in particular
simulated datasets 3 and 4 for Model 1. As shown in. Figure 5, for Model 1 the
optimal parameter set appears to reasonably estimate the true parameters for
all simulated dataset.

Estimating Model 2 is much harder since the regimes are both very similar.
Figure 5 shows that the optimal parameter set estimates the true parameters
reasonably for simulated datasets 2-5, but not for simulated dataset 1. The
behaviour of the algorithm for simulated dataset 1 for Model 2 is particularly
interesting. For this dataset, the algorithm terminates at one of two distinct
locations. One of these terminating points is at p11 = 1 and hence Regime 1
is absorbing. This means the algorithm has converged to a point where all but
the ﬁrst observation are captured by Regime 1. As p11 tends to 1 the algorithm
is able to send σ2
0 to achieve arbitrarily large values of the likelihood and
the model is unidentiﬁable. To prevent this behaviour, we suggest that the
parameters σ2
i and pij are restricted so that they are away from the boundary.
Indeed, for simulated dataset 1 of Model 2, if we take the terminating values
of the algorithm which lie away from the boundary, then we get reasonable
estimates of the true parameters. For more discussion see [22].

2 →

32

3

2

1

0

0

-0.05

-0.1

-0.15

-0.2

0.8

0.6

0.4

0.2

0

1

1

1

2

3
Simulation #

4

2

3
Simulation #

4

2

3
Simulation #

4

5

5

1

0.8

0.6

0.4

0.2

0

4

3

2

1

0

1

0.8

0.6

0.4

0.2

0

0.6

0.4

0.2

1

1

1

5

0

1

2

3
Simulation #

4

2

3
Simulation #

4

2

3
Simulation #

4

2

3
Simulation #

4

5

5

1

1

3

2

1

0

2

1.5

1

0.5

0

1.5

1

0.5

5

0

1

1

0.8

0.6

0.4

0.2

0

1

5

1

0.8

0.6

0.4

0.2

0

1

0.8

0.6

0.4

0.2

0

1

0.8

0.6

0.4

0.2

0

1

0.8

0.6

0.4

0.2

0

1

1

1

1

5

5

5

5

4.5

4

3.5

5

1

-740

-760

-780

-800

5

1

9.5

9

8.5

8

5

1

-650

-655

-660

-665

5

1

2

3
Simulation #

4

2

3
Simulation #

4

2

3
Simulation #

4

2

3
Simulation #

4

2

3
Simulation #

4

2

3
Simulation #

4

2

3
Simulation #

4

2

3
Simulation #

4

2

3
Simulation #

4

2

3
Simulation #

4

2

3
Simulation #

4

2

3
Simulation #

4

5

5

5

5

Figure 5: Scatter plots of terminating values of the truncated version of the
EM algorithm with truncation parameter D = 40 for Model 1 (top) and Model
2 (bottom), given random initial values sampled according to the distributions
in Table 1. For each model 5 datasets of length T = 400 were simulated,
as indicated on the x-axis. For each simulation the truncated EM algorithm
was run 50 times, initialising the algorithm at independently sampled random
initial values each time. The algorithm was run until either the increase in the
loglikelihood, or the diﬀerence between successive parameter values was less than
10−8 and the terminating values recorded. The black cross corresponds to
1.5
values associated with the highest loglikelihood value. Each circle corresponds
to a terminating value of the EM algorithm. The points have been ‘jittered’ so
they do not all lie on top of each other.

×

33

Table 2: Proportion of terminating values which lie within 0.0001 of the optimal
parameter value, as measured by the sup norm,
3
0.86
1

Simulation
Model 1
Model 2

| · |∞.
4
0.14
1

1
1
0.96

5
1
1

2
1
1

6 An application to South Australian wholesale

electricity market

The dataset consists of 81,792 half-hourly spot prices from the South Australian
electricity market (available at the AEMO website [3]) for the period 00:00
hours, 1st of January 2013, to 23:30 hours, 31st of September 2017. Note that
this dataset contains a period of 14 days over which the market was suspended
from 4:00pm, on the 28th of September until 10:30pm on the 11th of Octo-
ber. During this period prices were set by AEMO. We ignore this fact in our
modelling and include them in the data set anyway.

Following a common practice in the literature we model daily average prices
and thus we have a dataset of 1,704 daily average price observations to which
we ﬁt our model. The data that we model is plotted in Figure 6. To model
the South Australian wholesale electricity market, we break the price process
up in to two components, Pt = St + Xt, where Pt is the price on day t, St is
a deterministic trend component, and Xt is a stochastic component which is to
be modelled by an independent regime MRS model.

Electricity spot prices exhibit seasonality on daily, weekly, and longer scales.
To capture this multi-scale seasonality, the trend component consists of two
parts: a short-term component, gt, and a long-term component, ht, so St =
gt + ht. We model the long-term component, ht, using wavelet ﬁltering since it
has been shown to perform well for this application [18], and use Daubechies 24
wavelets and a level 6 approximation [18]. We use the short-term component, gt,
to capture the mean price for diﬀerent days of the week and indicator functions
to model this:

gt = βMonI (t

Mon) + βTueI (t

Tue) +

+ βSunI (t

Sun) ,

∈
where βMon, βTue, . . . , βSun are the mean deviations from the long-term trend
price on Monday, Tuesday, . . . , Sunday, respectively.

· · ·

∈

∈

Following [18] we use the RFP (recursive ﬁlter on prices) method to estimate
the seasonal component in the presence of extreme observations. The method
ﬁrst uses the raw price series to estimate the trend model, then removes this
from the data. Next, the standard deviation of these altered prices is estimated,
and any prices that are more than three standard deviations from the current
estimate of the trend are replaced with the value of the estimated trend at that
point. The procedure then re-estimates the trend component on the original
data set with spikes removed.

34

Table 3: BIC values of models M1-LN, M1-Gamma, M2-LN, and M2-Gamma,
ﬁtted to the SA electricity market data.

Model M1-LN M1-Gamma M2-LN M2-Gamma
BIC

15706

15572

15575

15681

Table 4: Parameter estimates of model M1-LN, which has the lowest BIC, ﬁtted
to the SA electricity market data.

Bt

St

α
-3.257
q3
7.106

φ
0.6830
µ2
3.751

σ2
1
213.26
σ2
2
1.268

p11
0.9140
p22
0.3945

We consider the following four models for the stochastic component:

Xt =

Bt
St

(

if Rt = 1,
if Rt = 2,

(M 1)

i.i.d. N(0, 1), and
q3, where q3 is a shifting parameter, follows either a Gamma distribution
2 (M1-LN).

where Bt = α + φBt−1 + σ1εt is an AR(1) process with εt
St
(M1-Gamma), or a log-normal distribution with parameters µ2 and σ2
The last two models introduce a ‘drop’ regime as well:

−

∼

Bt
St
Dt

if Rt = 1,
if Rt = 2,
if Rt = 3,

Xt = 


(M 2)


where Bt and St are as above, and
Dt + q1, where q1 is a shifting parameter,
−
follows a log-normal distribution with parameters µ3 and σ2
3. The use of the
shifting parameters q1 and q3 was proposed by [15]. As previosuly mentioned,
estimating the shifting parameters of these distribution is known to be a diﬃcult
task [19], so we ﬁx q1 and q3 as the ﬁrst and third quantiles of the detrended
data, as suggested by [17].

The models were ﬁtted to the detrended data using the truncated EM al-
gorithm with truncation level D = 56 (8 weeks). The Bayesian Information
Criterion (BIC) values of these models are reported in Table 3. From the BIC
values we choose model M1-LN, with an AR(1) base regime and a log-normal
spike regime. The parameters for this model are reported in Table 4. Of course,
for a rigorous treatment of this modelling problem, model assumptions should be
checked and we should not rely solely on the BIC. Using the smoothed probabil-
ities obtained while ﬁtting model M1-LN, the prices can be classiﬁed into which
regime is most likely. This is shown in Figure 6, where prices are highlighted in
red if Pbθ (Rt = 2

x0:T ) > 0.5, where

θ is the MLE.

|

b

35

SA Daily Average Whlesale Elecricity Prices, Jan 2013 - Sept 2017

Ave Price
Spike

Apr

Jul

Oct

Jan

Apr

Jul

Oct

Jan

Apr

Jul

Oct

Jan

Apr

Jul

Oct

Jan

Apr

Jul

1400

1200

1000

800

600

400

200

h
W
M

r
e
p
D
U
A
s
e
c
i
r

P
e
v
A
y

l
i

a
D

0
Jan

Figure 6: SA daily average wholesale electricity price for the period 1-Jan-2013
until 30-Sept-2017. Prices that have greater probability of being from Regime
2 in Model M1-LN are highlighted in red.

7 Conclusions

In this paper we have developed novel techniques for independent-regime MRS
models. Speciﬁcally, we consider models that are a collection of independent
AR(1) processes, where only one process is observed at each time t, and which
regime is observed is determined by a hidden Markov chain. We develop forward,
backward and EM algorithms for these models, and show that the methods
we develop here can outperform the existing method of inference used in the
electricity price modelling literature, the EM-like algorithm [17].

The construction of these methods relies on the idea of augmenting the hid-
den Markov chain with a set of counters, which keep track of the number of
transitions since the last visit to AR(1) regimes. The forward algorithm can be
used to evaluate the likelihood and ﬁltered and prediction probabilities, which
are used as inputs to the backward algorithm. The backward algorithm is used
to evaluate the smoothed probabilities. Together, the forward-backward proce-
dure executes the E-step of the EM algorithm. We showed that the complexity
, where M is the num-
of the forward and backward algorithms is
ber of regimes in the model, T the length of the observed sequence, and k the
number of AR(1) regimes in the model. This complexity may be impractically
large if T or k are large, so we introduce an approximation where the memory
of AR(1) processes in the model is truncated. These truncated methods are

M 2T k+1kk

O

(cid:0)

(cid:1)

(cid:1)

(cid:0)

O

, where D is the memory of the counters.

M 2DkT kk
Simulation suggests that the MLE found via the EM algorithm is consis-
tent, and that, even for models with regimes with similar characteristics (such
as Model 2), the MLE is still a reasonable estimator when the sample size is
large enough (400 observations in this case). Simulations also suggest that the
truncation method is a reasonable approximation to the full likelihood, while
improving runtime and memory requirements signiﬁcantly. As is typical for

36

 
 
 
 
 
hill-climbing algorithms, there is the possibility of the algorithm terminating
at sub-optimal values depending on the initial values of the algorithm. We
explored some of this behaviour via a simulation study and found, at most,
two local maxima of the likelihood function. Of course, the behaviour of the
algorithm is going to be model- and data-speciﬁc. This simulation study also
highlighted possible identiﬁability issues which can arise. However, these can
be rectiﬁed by restricting parameters away from the boundary.

Lastly, we apply our methods to estimate four models for the South Aus-
tralian wholesale electricity market and ﬁnd that a 2-regime model with an
AR(1) base regime and shifted log-normal spike regime is best as measured by
the BIC. We also demonstrate how prices can be classiﬁed into regimes using
the smoothed probabilities.

References

[1] P. Albert. A two-state Markov mixture model for a time series of epileptic
seizure counts. Biometrics, 47(4):13711381, December 1991. ISSN 0006-
341X.

[2] E. Alvaro, I. P. J., and V. Pablo. Modelling electricity prices: International
evidence. Oxford Bulletin of Economics and Statistics, 73(5):622–650, 2002.

[3] Australian Energy Market Operator. Data dashboard, 2018. Accessed:

2018-02-17.

[4] L. E. Baum and J. A. Eagon. An inequality with applications to statistical
estimation for probabilistic functions of Markov processes and to a model
for ecology. Bulletin of the American Mathematical Society, 73(3):360–363,
05 1967.

[5] L. E. Baum and T. Petrie. Statistical inference for probabilistic functions
of ﬁnite state Markov chains. The Annals of Mathematical Statistics, 37
(6):1554–1563, 12 1966. doi: 10.1214/aoms/1177699147.

[6] L. E. Baum, T. Petrie, G. Soules, and N. Weiss. A maximization technique
occurring in the statistical analysis of probabilistic functions of Markov
chains. The Annals of Mathematical Statistics, 41(1):164–171, 02 1970.
doi: 10.1214/aoms/1177697196.

[7] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from
incomplete data via the EM algorithm. Journal of the Royal Statistical
Society. Series B (Methodological), 39(1):1–38, 1977. ISSN 00359246.

[8] S. Deng. Stochastic models of energy commodity prices and their applica-
tions: Mean-reversion with jumps and spikes. Working Paper PWP-073,
University of California Energy Institute, 2000.

37

[9] R. G. Ethier and T. D. Mount. Estimating the volatility of spot prices
in restructured electricity markets and the implications for option values.
PSerc Working Paper, Cornell University, 1998.

[10] J. D. Hamilton. A new approach to the economic analysis of nonstationary
time series and the business cycle. Econometrica, 57(2):357–384, 1989.
ISSN 00129682, 14680262.

[11] J. D. Hamilton. Analysis of time series subject to changes in regime. Journal

of Econometrics, 45(1):39–70, 1990. ISSN 0304-4076.

[12] B. M. Hill. The three-parameter lognormal distribution and Bayesian anal-
ysis of a point-source epidemic. Journal of the American Statistical Asso-
ciation, 58(301):72–84, 1963.

[13] R. Huisman and C. de Jong. Option pricing for power prices with spikes.

Energy Power Risk Management, 7(11):12–16, 2003.

[14] R. Huisman and R. Mahieu. Regime jumps in electricity prices. Energy

Economics, 25(5):425–434, 2003.

[15] J. Janczura and R. Weron. Regime-switching models for electricity spot
prices: Introducing heteroskedastic base regime dynamics and shifted spike
distributions. In 2009 6th International Conference on the European Energy
Market, pages 1–6, May 2009.

[16] J. Janczura and R. Weron. An empirical comparison of alternate regime-
switching models for electricity spot prices. Energy Economics, 32(5):1059–
1073, 2010. ISSN 0140-9883.

[17] J. Janczura and R. Weron. Eﬃcient estimation of Markov regime-switching
models: An application to electricity spot prices. Advances in Statistical
Analysis, 96(3):385–407, 2012.

[18] J. Janczura, S. Tr¨uck, R. Weron, and R. C. Wolﬀ. Identifying spikes and
seasonal components in electricity spot price data: A guide to robust mod-
eling. Energy Economics, 38:96–110, 2013.

[19] N. L. Johnson, S. Kotz, and N. Balakrishnan. Continuous univariate dis-

tributions. New York Wiley, 2nd edition, 1994. ISBN 0471584959.

[20] C.-J. Kim. Dynamic linear models with Markov-switching. Journal of

Econometrics, 60(1-2):1–22, January-February 1994.

[21] S. E. Levinson, L. R. Rabiner, and M. M. Sondhi. An introduction to the
application of the theory of probabilistic functions of a Markov process to
automatic speech recognition. The Bell System Technical Journal, 62(4):
1035–1074, April 1983.

38

[22] A. Lewis. Inference of Markovian-regime-switching models with application
to South Australian electricity prices. Master’s thesis. The University of
Adelaide, 2018.

[23] M. Thyer and G. Kuczera. Modeling long-term persistence in hydroclimatic
time series using a hidden state Markov model. Water Resources Research,
36(11):3301–3310, 2000.

[24] R. Weron. Electricity price forecasting: A review of the state-of-the-art
with a look into the future. International Journal of Forecasting, 30(4):
1030–1081, 2014.

[25] R. Weron, M. Bierbrauer, and S. Tr¨uck. Modeling electricity prices: jump
diﬀusion and regime switching. HSC Research Reports HSC/03/01, Hugo
Steinhaus Center, Wroclaw University of Technology, 2003.

[26] S.-Z. Yu. Hidden Semi-Markov Models. Elsevier, Boston, 1st edition, 2016.

[27] W. Zucchini and P. Guttorp. A hidden Markov model for space-time pre-

cipitation. Water Resources Research, 27(8):1917–1923, 1991.

39

