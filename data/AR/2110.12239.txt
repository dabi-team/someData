1
2
0
2

t
c
O
3
2

]

G
L
.
s
c
[

1
v
9
3
2
2
1
.
0
1
1
2
:
v
i
X
r
a

The IISc Thesis Template

A PROJECT REPORT

SUBMITTED IN PARTIAL FULFILMENT OF THE

REQUIREMENTS FOR THE DEGREE OF
Master of Technology

IN
Faculty of Engineering

BY

Samineni Soumya Rani

Computer Science and Automation

Indian Institute of Science

Bangalore – 560 012 (INDIA)

October, 2021

 
 
 
 
 
 
Declaration of Originality

I, Samineni Soumya Rani, with SR No. 04-04-00-10-42-19-1-17066 hereby declare that
the material presented in the thesis titled

Policy Search using Dynamic Mirror Descent MPC for Model Free RL

represents original work carried out by me in the Department of Computer Science and
Automation at Indian Institute of Science during the years 2019-2021.
With my signature, I certify that:

• I have not manipulated any of the data or results.

• I have not committed any plagiarism of intellectual property. I have clearly indicated and

referenced the contributions of others.

• I have explicitly acknowledged all collaborative research and discussions.

• I have understood that any false claim will result in severe disciplinary action.

• I have understood that the work may be screened for any form of academic misconduct.

Date:

Student Signature

In my capacity as supervisor of the above-mentioned work, I certify that the above statements
are true to the best of my knowledge, and I have carried out due diligence to ensure the
originality of the report.

Advisor Name:

Dr.Shishir Nadubettu Yadukumar Kolathaya

Dr.Shalabh Bhatnagar

1

Advisor Signature

© Samineni Soumya Rani
October, 2021

All rights reserved

DEDICATED TO

My beloved father

who has always been my inspiration.

Acknowledgements

I would like to express my gratitude to Prof.Shishir Nadubettu Yadukumar Kolathaya and
Prof.Shalabh Bhatnagar for the opportunity to work under their guidance and for helping me
throughout my M.Tech, early from second semester to writing my ﬁrst paper based on this
Project. You both have always been available whenever I needed help. While I was juggling
with many research directions you helped me to stay focused. The freedom you oﬀered me in
exploring areas of Reinforcement Learning and Robotics had a great impact on my graduate
research experience. This work wouldn’t have been possible without the constant support and
help from both of you.

I would also like to thank Prof.Aditya Gopalan for sharing his valuable feedback in evalu-
ating my thesis and for his course on Online Learning.

A special thank you goes out to Utkarsh Aashu Mishra for helping me especially for being
a good company.

I would like to thank my mother for always motivating and encouraging me to acheive what
I beleive in and I would also like to thank my brother and my sister in law for their support
during my stay in Bengluru during Covid.

Further, I thank all the members of Stoch Lab and Stochastic Systems Lab for making the
experience a pleasant one. The exposure I got through both the labs was invaluable.

I would like to thank all the wonderful friends I made here who made my Journey at IISc
the most memorable one and I cherish all the moments we had together.

i

Abstract

Recent works in Reinforcement Learning (RL) combine model-free (Mf)-RL algorithms with
model-based (Mb)-RL approaches to get the best from both: asymptotic performance of Mf-RL
and high sample-eﬃciency of Mb-RL. Inspired by these works, we propose a hierarchical frame-
work that integrates online learning for the Mb-trajectory optimization with oﬀ-policy methods
for the Mf-RL. In particular, two loops are proposed, where the Dynamic Mirror Descent based
Model Predictive Control (DMD-MPC) is used as the inner loop to obtain an optimal sequence
of actions. These actions are in turn used to signiﬁcantly accelerate the outer loop Mf-RL. We
show that our formulation is generic for a broad class of MPC based policies and objectives, and
includes some of the well-known Mb-Mf approaches. Based on the frame work we deﬁne two
algorithms to increase sample eﬃciency of Oﬀ Policy RL and to guide end to end RL algorithms
for online adaption respectively. Thus we ﬁnally introduce two novel algorithms: Dynamic-
Mirror Descent Model Predictive RL (DeMoRL), which uses the method of elite fractions
for the inner loop and Soft Actor-Critic (SAC) as the oﬀ-policy RL for the outer loop and
Dynamic-Mirror Descent Model Predictive Layer (DeMo Layer), a special case of hierar-
chical framework which guides linear policies trained using Augmented Random Search(ARS).
Our experiments show faster convergence of the proposed DeMo RL, and better or equal per-
formance compared to other Mf-Mb approaches on benchmark MuJoCo control tasks. The
DeMo Layer was tested on classical Cartpole and custom built Quadruped trained using Linear
Policy Approach. The results shows that DeMo Layer signiﬁcantly increases performance of
the Linear Policy in both the settings.

ii

Acronyms

Table 1: Key Acronyms used in the Report

Acronyms Expansion
DMD MPC Dynamic Mirror Descent Model Predictive Control
ARS
SAC
DeMo RL
MoPAC
TOPDM

Augmented Random Search
Soft Actor Critic
Dynamic Mirror Descent Model Predictive RL
Model Predictive Actor Critic
Trajectory Optimisation for Precise Dexterous Manipulation

iii

Publications based on this Thesis

Accelerating Actor-Critic with Dynamic Mirror Descent based Model Predictive Control (Under
Review) submitted to CoRL 2021

iv

Contents

Acknowledgements

Abstract

Acronyms

Publications based on this Thesis

Contents

List of Figures

List of Tables

1 Introduction

1.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3 Contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.4 Outline of the Report . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 Preliminaries

2.1 Optimal Control- MPC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 Reinforcement Learning Framework . . . . . . . . . . . . . . . . . . . . . . . . .
2.3 Online Learning Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.4 Description of Algorithms
2.4.1 Augmented Random Search . . . . . . . . . . . . . . . . . . . . . . . . .
2.4.2
Soft Actor Critic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.4.3 Online Learning for MPC . . . . . . . . . . . . . . . . . . . . . . . . . .

v

i

ii

iii

iv

v

vii

viii

1
1
2
3
4

5
5
6
6
7
7
8
9

CONTENTS

3 Methodology: Novel Framework & Algorithms

3.1 Generalised Framework: DMD MPC & RL . . . . . . . . . . . . . . . . . . . . .
3.2 DeMo RL Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.3 DeMo Layer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4 Experimental Results

4.1 DeMo RL: Results and Comparison . . . . . . . . . . . . . . . . . . . . . . . . .
4.2 DeMo Layer: Results for Cartpole and Stoch . . . . . . . . . . . . . . . . . . . .

5 Conclusion and Future Work

Bibliography

11
11
16
18

20
20
23

24

25

vi

List of Figures

3.1 The Model Free Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . .
3.2 Model Based Reinforcement Learning.
. . . . . . . . . . . . . . . . . . . . . . .
3.3 The proposed hierarchical structure of Dynamic-Mirror Descent Model-Predictive
Reinforcement Learning (DeMoRL) with an inner loop DMD-MPC update and
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
an outer loop RL update.
3.4 The proposed DeMo Layer with an inner loop DMD-MPC update to guide outer
loop RL. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

MoPAC and MBPO.

4.1 Reward Performance of DeMoRL algorithm over other model based algorithms:
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2 Ablation study for elite percentage: Reward performance curve (left) and Accel-
eration analysis as epochs to reach 10000 rewards (right) . . . . . . . . . . . . .

12
13

14

19

21

22

vii

List of Tables

1

Key Acronyms used in the Report . . . . . . . . . . . . . . . . . . . . . . . . . .

iii

3.1 Mb-Mf algorithms as special cases of our generalised framework . . . . . . . . .

16

4.1 Mean Reward Performance Analysis of DeMo RL . . . . . . . . . . . . . . . . .
4.2 Reward Performance Analysis of DeMo Layer
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
4.3 Hyper parameters used for DeMo Layer

22
23
23

viii

Chapter 1

Introduction

Deep Reinforcement Learning (DRL) algorithms are shown to be highly successful in challeng-
ing control tasks like dexterous manipulation [22], agile locomotion [20, 10] and recent works
demonstrate applications to safety critical systems as well [3, 30, 23]. But the PROBLEMS
with Deep RL techniques are:

1. Policies trained in simulation often fail to transfer to real hardware.

2. Any modiﬁcation/update in the model renders the previously trained policy invalid. In

other words, the policies are sensitive to changes in the model.

3. Training in hardware is very expensive and time consuming.Further, any undesirable

behavior may lead to damages in the hardware.

Additional problems involve, the lack of model information in training Model Free RL algo-
rithms, which results in prolonged training. Hence, DRL algorithms need to be made Data ef-
ﬁcient, Generalisable and further have to adapt to unseen environments.Adapting as eﬃciently
as possible requires perceiving the environment and an investment in learning the Model is
highly sample eﬃcient, results in Model Based RL techniques. But Models fail to capture the
Global Dynamics and Model Based RL Techniques fail to achieve asymptotic performance of
Model Free RL Techniques. Several works combined both the Model Based and Model Free
Approaches and we propose a generalised framework combining the Model Based MPC with
Model Free Oﬀ Policy RL algorithms.

1.1 Motivation

Model-Free Reinforcement Learning (Mf-RL) algorithms are widely applied to solve challeng-
ing control tasks as they eliminate the need to model the complex dynamics of the system.

1

However, these techniques are signiﬁcantly data hungry and require millions of transitions.
Furthermore, these characteristics highly limit successful training on hardware as undergoing
such high number of transitions in hardware environments is infeasible. Thus, in order to over-
come this hurdle, various works have settled for a two loop model-based approach, typically
referred to as Model-based Reinforcement Learning (Mb-RL) algorithms. Such strategies take
the beneﬁt of the explored dynamics of the system by learning the dynamics model, and then
determining an optimal policy in this model. Hence this “inner-loop” optimization allows for a
better choice of actions before interacting with the original environment.

The inclusion of model-learning in RL has signiﬁcantly improved sampling eﬃciency [11, 16],
and there are numerous works in this direction. DRL algorithms, while exploring, collect sig-
niﬁcant amount of state transitions, which can be used to generate an approximate dynamics
model of the system. In the context of robotics, this model has proven to be very beneﬁcial in
developing robust control strategies based on predictive simulations [8]. They have successfully
handled minor disturbances and demonstrated sim2real feasibility. Moreover, the process of
planning with the learnt model is mainly motivated by the Model Predictive Control (MPC),
which is a well known strategy used in classical real-time control. Given the model and the
cost formulation, a typical MPC structure can be formulated in the form of a ﬁnite horizon tra-
jectory optimization problem. Thus our work is motivated to propose a generalised framework
combining Model Free and Model Based methods.

1.2 Related Work

The work with such a view of Mb-Mf approach and exploiting the approximated dynamics with
random shooting, [16] demonstrated its eﬃcacy in leveraging the overall learning performance.
Further, the work also showed how model-based (Mb) additions to typical model-free (Mf)
algorithms can accelerate signiﬁcantly the latter ones. Additionally, in this context of Mb-Mf
RL algorithms, [13] also introduced the use of value functions with an MPC formulation and [5]
shows a similar formulation with high-dimensional image observations. Recent works presented
in [29] showed adaptation to Dynamic changes using MPC with world models and [15] proposes
an actor critic framework using model predictive rollouts and demonstrated applicability on
real hardware. The TOPDM [1], a close approach to DeMo RL demonstrates spinning a pen
between the ﬁngers, the most challenging examples in dexterous hand manipulation.

Further, prior works [11], [24], [26] have explored guiding RL Policies using Mirror Descent
Approaches with KL Constraint on the policy update. As far as our knowledge, we are the ﬁrst

2

to generalise the Mb Mf Framework in the literature with the view of Dynamic Mirror Descent
MPC to RL polices

1.3 Contribution

With a view toward strengthening existing Mb-Mf approaches for learning, we propose a generic
framework that integrates a model-based optimization scheme with model-free oﬀ-policy learn-
ing. Motivated by the success of online learning algorithms [27] in RC buggy models, we
combine them with oﬀ-policy Mf learning, thereby leading to a two-loop Mb-Mf approach. In
particular, we implement dynamic mirror descent (DMD) algorithms on a model-estimate of
the system, and then the outer loop Mf-RL is used on the real system. The main advantage
with this setting is that the inner loop is computationally light; the number of iterations can be
large without eﬀecting the overall performance. Since this is a hierarchical approach, the inner
loop policy helps improve the outer loop policy, by eﬀectively utilizing the control choices made
on the approximate dynamics. This approach, in fact, provides a more generic framework for
some of the Mb-Mf approaches (e.g., [15], [17]).

In addition to the proposed framework, we introduce two new algorithms DeMo RL and
DeMo Layer. The Dynamic Mirror-Descent Model Predictive RL (DeMoRL), uses Soft actor-
critic (SAC) [4] in the outer loop as oﬀ-policy RL, and Cross-Entropy Method (CEM) in the
inner loop as DMD-MPC [27]. In particular, we use the exponential family of control distri-
butions with CEM on the objective. In each iteration, the optimal control sequence obtained
is then applied on the model-estimate to collect additional data. This is appended to the
buﬀer, which is then used by the outer-loop for learning the optimal policy. We show that the
DMD-MPC accelerates the learning of the outer-loop by simply enriching the data with better
choices of state-control transitions. We ﬁnally demonstrate this method on custom robotic en-
vironments and MuJoCo benchmark control tasks. Simulation results show that the proposed
methodology is better than or at least as good as MoPAC [15] and MBPO [8] in terms of
sample-eﬃciency. Furthermore, as our formulation is closer to that of [15], it is worth mention-
ing that even though we do not show results in hardware, the proposed algorithms can be used
to train in hardware more eﬀectively, which will be a part of future work.

The DeMo Layer, a special instance of hierarchical framework guides linear policies trained
using Augmented Random Search(ARS). The experiments are conducted Cartpole swing up
and quadrupedal walking. Our experimental results show that proposed DeMo Layer could
improve the policy and could be used end to end with any RL algorithm during deployment.

3

1.4 Outline of the Report

The report is structured as follows:

• Chapter 2. Problem Formulation

In this chapter, we provide the preliminaries for Reinforcement Learning and Online
Learning as followed in the report. We further describe the RL algorithms in speciﬁc the
Augmented Random Search, Soft Actor Critic and Online Learning approach to MPC.

• Chapter 3. Methodology: Novel Framework & Algorithms

We will describe the hierarchical framework for the proposed strategy, followed by the
description of the DMD-MPC. With the proposed generalised framework, we formulate
the two novel algorithms associated with the strategy DeMo RL and DeMO Layer in this
chapter.

• Chapter 4. Experimental Results

In this chapter we run experiments of DeMo RL on benchmark Mujoco Control Tasks
and we compare the results with existing and state of the art algorithms MOPAC and
MBPO. The experiments of DeMO Layer was conducted on swinging up Cartpole and
custom built quadruped Stoch2. Further, we discuss our experimental results and show
signiﬁcance of proposed algorithms.

• Chapter 5. Conclusion & Future Work

Finally, we end the report by summarizing the work done and proposing some interesting
future directions.

4

Chapter 2

Preliminaries

2.1 Optimal Control- MPC

The Model Predictive Control is a widely applied control strategy and gives practical and robust
controllers. It considers a stochastic dynamics model ˆf an approximation to real system f and
solves an H step optimisation problem at every time step and applies ﬁrst control to the real
dynamical system f to go to the next state xt+1. A popular MPC objective is the expected H
-step future costs

J (xt) = E [C (xt, ut)] ,

C (xt, ut) =

H−1
(cid:88)

h=0

γhc(xt,h, ut,h) + γHcH(xt,H)

(2.1)

(2.2)

where, c(xt,h, ut,h) is the cost incurred (for the control problem) and cH(xt,H) is the terminal
cost.
Since optimal control is obtained from J (xt) , which is based on xt, thus MPC is eﬀectively
state-feedback as desired for a stochastic system and is an eﬀective tool for control tasks in-
volving dynamic environments or non stationary setup.

Though MPC sounds intuitively promising, the optimization is approximated in practice and
the control command ut needs to be computed in real time at high frequency. Hence, a common
practice is to heuristically bootstrap the previous approximate solution as the initialization to
the current problem.

5

2.2 Reinforcement Learning Framework
We consider an inﬁnite horizon Markov Decision Process (MDP) given by {X, U, r, P, γ, ρ0}
where X ⊂ Rn refers to set of states of the robot and U ⊂ Rm refers to the set of control or
actions. r : X × U → R is the reward function, P : X × U × X → [0, 1] refers to the function
that gives transition probabilities between two states for a given action, and γ ∈ (0, 1) is the
discount factor of the MDP. The distribution over initial states is given by ρ0 : X → [0, 1]
and the policy is represented by πθ : X → U parameterized by θ ∈ Θ, a potentially feasible
high-dimensional space. If a stochastic policy is used, then πθ : X × U → [0, 1]. For ease of
notations, we will use a deterministic policy to formulate the problem. Wherever a stochastic
policy is used, we will show the extensions explicitly. In this formulation, the optimal policy is
the policy that maximizes the expected return (R):

R = E[rt + γrt+1 + γ2rt+2 + . . . ]

where the subscript for rt denotes the step index. Note that the system model dynamics can
be expressed in the form of an equation:

xt+1 ∼ f (xt, ut),

θ∗ := arg max

θ

Eρ0,πθ

(cid:34) ∞
(cid:88)

(cid:35)
γtr(xt, ut)

,

t=0

x0 ∼ ρ0,

xt+1 ∼ f (xt, πθ(xt)).

(2.3)

The oﬀpolicy techniques like TD3, SAC have shown better sample complexity compared to
TRPO, PPO. A simple random search based a Model Free Technique, Augmented Random
Search [14], proposed a Linear deterministic policy highly competitive to other Model Free
RL Techniques like TRPO, PPO and SAC. In the subsequent sections we describe the ARS
algorithm in detail along with the improvement in its implementation and we also describe
SAC.

2.3 Online Learning Framework

Another sequential decision making technique, Online learning is a framework for analyzing on-
line decision making, essentially with three components: the decision set, the learner’s strategy
for updating decisions, and the environment’s strategy for updating per-round losses.

At round t, the learner makes a decision ˜θt,along with a side information ut−1, then environment

6

chooses a loss function (cid:96)t and the learner suﬀers a cost (cid:96)t
the gradient of loss to aid in choosing the next decision.

(cid:17)

(cid:16)˜θt

. along with side information like

Here, the learner’s goal is to minimize the accumulated costs (cid:80)T
, i.e., by minimizing
the regret. We describe, in detail the Online Learning Approach to Model Predictive Control
[27] in subsequent sections.

t=1 (cid:96)t

(cid:17)

(cid:16)˜θt

2.4 Description of Algorithms

We describe the RL and Online Learning algorthms that are used in this work - Augmented
Random Search(ARS), Soft Actor Critic(SAC) and Online Learning Approach to MPC.

2.4.1 Augmented Random Search

Random Search is a Derivative Free Optimisation where the gradient is estimated through ﬁnite
diﬀerence Method [18].Objective is to maximize Expected return of a policy π parameterised
by θ under noise ξ

Eξ [r (πθ, ξ)]

max
θ

The gradient is found from the gradient estimate obtained from gradient of smoothened ver-
sion of above objective with Gaussian noise unlike from policy gradient theorem. Gradient of
smoothened objective is

r (πθ+νδ, ξ1) − r (πθ, ξ2)
ν
where δ is zero mean Gaussian. If ν is suﬃciently small, the Gradient estimate would be close
to the gradient of original objective. Further bias could be reduced with a two point estimate,

δ

r (πθ+νδ, ξ1) − r (πθ−νδ, ξ2)
ν

δ.

A Basic Random Search would involve the update of policy parameters according to

θj+1 = θj +

α
N

N
(cid:88)

k=1

[r (πj,k,+) − r (πj,k,−)] δk

(2.4)

Augmented Random Search, deﬁnes an update rule,

θj+1 = θj +

α
bσR

b
(cid:88)

k=1

(cid:2)r (cid:0)πj,(k),+

(cid:1) − r (cid:0)πj,(k),−

(cid:1)(cid:3) δ(k)

(2.5)

7

Policy is linear state feedback law,

pij(x) = (θj) (x)

where x is the state and It proposes three Augmentations to Basic Random Search.

i) Using top best b performing directions, They order the perturbation directions +δ(k), in
(cid:1) and uses only the top b directions.

decreasing order according to max r (cid:0)πj,(k),+
ii)Scaling by the standard deviation, helps in an adjusting the step size.
iii) Normalization of the states

(cid:1) and r (cid:0)πj,(k),−

πj(x) = (θj) diag (Σj)−1/2 (x − µj)

Accelerating ARS

Most optimisers use Adam to accelerate Stochastic Gradient Descent [7] in practical implemen-
tations. Hence with ARS we estimate the gradient, an acceleration technique is not used. So,
we deﬁne an acceleration based Gradient Estimate to ARS for faster convergence. Future Work
would involve validating this approach. The Modiﬁed ARS Algorithm with α and β are the
small and large step sizes respectively.

Algorithm 1 Accelerated ARS
1: Runaveragej = (cid:80)
(cid:80)b
2: θj+1 = θj + α
bσR
3: θaccj+1 = γθj+1 + (1 − γ) Runaveragej

i<j (1 − β)i θ(i−τ )
(cid:2)r (cid:0)πj,(k),+

k=1

(cid:1) − r (cid:0)πj,(k),−

(cid:1)(cid:3) δ(k)

2.4.2 Soft Actor Critic

Soft Actor-Critic (SAC) [4] is an oﬀpolicy model-free RL algorithm based on principle of entropy
maximization, with entropy of policy in addition to reward. It uses soft policy iteration for
policy evaluation and improvement. It uses two Q Value functions to mitigate positive bias
of value based methods and a minimum of the Q-functions is used for the value gradient and
policy gradient. Further, two Q Functions Speeds up training process. It also uses a target
network with weights updated by exponentially moving average, with a smoothing constant
tau, to increase stability.
The SAC policy πθ is updated using the loss function

Jθ = E(x,u,r,x(cid:48))∼D[DKL(π|| exp (Qξ − Vζ))]

8

where D, Vζ and Qξ represent the replay buﬀer, value function and Q-function associated with
πθ. The exploration by SAC helps in learning the underlying dynamics. In each gradient step
we update SAC parameters using data

ζ ← ζ − λψ∇ζJVζ

ξ ← ξ − λξ∇ξJQξ

θ− ← θ − λθ∇θJπθ

¯ζ ← τ ζ + (1 − τ )ζ

¯ξ ← τ ξ + (1 − τ )ξ

¯ξ and ¯ζ represent target networks.

2.4.3 Online Learning for MPC

The Online Learning (OL) makes a decision at time t to optimise for the regret over time while
MPC also optimizes for a ﬁnite H-step horizon cost at every time instant, thus having a close
similarity to OL [27].

The proposed work is motivated by such an OL approach to MPC, which considers a generic
algorithm Dynamic Mirror Descent (DMD) MPC, a framework that represents diﬀerent MPC
algorithms. DMD is reminiscent of the proximal update with a Bregman divergence that acts
as a regularization to keep the current control distribution parameterized by ηt at time t, close
to the previous one. The second step of DMD uses the shift model Φt to anticipate the optimal
decision for the next instant.

The DMD-MPC proposes to use the shifted previous solution for shift model as approximation
to the current problem. The proposed methodology also aims to obtain an optimal policy for
a ﬁnite horizon problem considering H-steps into the future using DMD MPC.
Denote the sequence of H states and controls as xt = (xt,0, xt,1, . . . , xt,H), and ut = (ut,0, ut,1, . . . , ut,H−1),
with xt,0 = xt. The cost for H steps is given by

C (xt, ut) =

H−1
(cid:88)

h=0

γhc(xt,h, ut,h) + γHcH(xt,H)

(2.6)

9

where, c(xt,h, ut,h) = −r(xt,h, ut,h) is the cost incurred (for the control problem) and cH(xt,H) is
the terminal cost. Each of the xt,h, ut,h are related by

xt,h+1 ∼ fφ(xt,h, ut,h),

h = 0, 1, . . . , H − 1,

(2.7)

with fφ being the estimate of f . We will use the short notation xt ∼ fφ to represent (2.7).
It will be shown later that in a two-loop scheme, the terminal cost can be the value function
obtained from the outer loop.
Now, by following the principle of DMD-MPC, for a rollout time of H, we sample the tuple ut
from a control distribution (πη) parameterized by η. To be more precise, ηt is also a sequence
of parameters:

ηt = (ηt,0, ηt,1, . . . , ηt,H−1)

which yield the control tuple ut. Therefore, given the control distribution paramater ηt−1 at
round t − 1, we obtain ηt at round t from the following update rule:

˜ηt := Φt(ηt−1)
J(xt, ˜ηt) := Eut∼π˜ηt ,xt∼fφ [C(xt, ut)]
ηt = arg min

[α(cid:104)∇˜ηtJ(xt, ˜ηt), η(cid:105) + Dψ(η(cid:107)˜ηt)] ,

η

(2.8)

where J is the MPC objective/cost expressed in terms of xt and π˜ηt, Φt is the shift model,
α > 0 is the step size for the DMD, and Dψ is the Bregman divergence for a strictly convex
function ψ.
Note that the shift parameter Φt is critical for convergence of this iterative procedure. Typically,
this is ensured by making it dependent on the state xt. In particular, for the proposed two-
loop scheme, we make Φt dependent on the outer loop policy πθt(xt). Also note that resulting
parameter ηt is still state-dependent, as the MPC objective J is dependent on xt.

With the two policies, πθt and πηt at time t, we aim to develop a synergy in order to leverage
the learning capabilities of both of them. In particular, the ultimate goal is to learn them in
“parallel”, i.e., in the form of two loops. The outer loop optimizes πθt and the inner loop
optimizes πηt for the MPC Objective. We discuss this in more detail in Section 3.

10

Chapter 3

Methodology: Novel Framework &
Algorithms

In this chapter, we discuss a generic approach for combining model-free (Mf) and model-based
(Mb) reinforcement learning (RL) algorithms through DMD-MPC. We deﬁne two new algo-
rithms with DMD MPC and RL: DeMo RL and DeMo Layer, one to improve sample eﬃciency
of Oﬀ policy RL techniques while training and the other technique guides RL algorithms online
for better policies.

3.1 Generalised Framework: DMD MPC & RL

In classical Mf-RL, data from the interactions with the original environment are used to obtain
the optimal policy parameterized by θ. While the interactions of the policy are stored in
memory buﬀer, DEN V , for oﬄine batch updates, they are used to optimize the parameters φ
for the approximated dynamics of the model, fφ. Such an optimized policy can then be used
in the DMD-MPC strategy to update the control distribution, πη. The controls sampled from
this distribution are rolled out with the model, fφ, to collect new transitions and store these
in a separate buﬀer DM P C. Finally, we update θ using both the data i.e., from the buﬀer
DEN V ∪ DM P C via one of the oﬀ-policy approaches (e.g. DDPG [12], SAC [4]). In this work, we
will demonstrate this using Soft Actor-Critic (SAC) [4]. This gives a generalised hierarchical
framework with two loops: Dynamic Mirror Descent (DMD) based Model Predictive Control
(MPC) forming an inner loop and model-free RL in the outer loop. A graphical representation
of Model Free RL, Model Based RL and the described framework are given in Figure 3.1,
Figure 3.2 and Figure 3.3.

There are two salient features in the two-loop approach:

11

Figure 3.1: The Model Free Reinforcement Learning

1. At round t, we obtain the shifting operator Φt by using the outer loop parameter θt. This
is in stark contrast to the classical DMD-MPC method shown in [27], wherein the shifting
operator is only dependent on the control parameter of the previous round ηt−1.

2. Inspired by [13, 15], the terminal cost cH(xt,H) = − Vζ(xt,H) is the value of the terminal
state for the ﬁnite horizon problem as estimated by the value function (Vζ, parameterized
by ζ) associated with the outer loop policy, πθt. This will eﬃciently utilise the model
learned via the RL interactions and will in turn optimize πθt with the updated setup.

Since there is limited literature on theoretical guarantees of DRL algorithms, it is diﬃcult
to show convergences and regret bounds for the proposed two-loop approach. However, there
are guarantees on regret bounds for dynamic mirror descent algorithms in the context of online
learning [6]. We restate them here using our notations for ease of understanding. We reuse
their following deﬁnitions:

GJ (cid:44) max
ηt∈P

(cid:107)∇J(ηt)(cid:107), M (cid:44) 1
2

max
ηt∈P

(cid:107)∇ψ(ηt)(cid:107)

Dmax (cid:44) max
t∈P

ηt,η(cid:48)

D(ηt(cid:107)η(cid:48)

t), and ∆Φt

(cid:44) max
t∈P
ηt,η(cid:48)

12

D(˜ηt(cid:107)˜η(cid:48)

t) − D(ηt(cid:107)η(cid:48)

t).

Figure 3.2: Model Based Reinforcement Learning.

By a slight abuse of notations, we have omitted xt in the arguments for J. We have the
following:

Lemma 3.1 Let the sequence ˜ηt be as in (2.8), and let ηt be any feasible arbitrary sequence;
then for the class of convex MPC objectives J, we have

J (˜ηt) − J (ηt) ≤

1
αt

[D (ηt(cid:107)˜ηt) − D (ηt+1(cid:107)˜ηt+1)] +

∆Φt
αt

+

4 M
αt

(cid:107)ηt+1 − Φt (ηt)(cid:107) +

αt
2σ

G2
J

Theorem 3.1 Given the shift operator Φt that is dependent on the outer-loop policy param-
eterised by θ at state xt, the Dynamic Mirror Descent (DMD) algorithm using a diminishing
step sequences αt gives the overall regret with the comparator sequence ηt as,

R (ηT ) =

T
(cid:88)

t=0

J (˜ηt) − J (ηt) ≤

Dmax
αT +1

+

4M
αT

WΦt (ηT ) +

G2
J
2σ

T
(cid:88)

t=0

αt

(3.1)

with

WΦt (ηT ) (cid:44)

T
(cid:88)

t=0

(cid:107)ηt+1 − Φt(ηt)(cid:107) .

Based on such a formulation, the regret bound is R (ηT ) = O

(cid:16)√

T [1 + WΦt (ηT )]

(cid:17)

.

13

Figure 3.3: The proposed hierarchical structure of Dynamic-Mirror Descent Model-Predictive
Reinforcement Learning (DeMoRL) with an inner loop DMD-MPC update and an outer loop
RL update.

Proofs of both Lemma 3.1 and Theorem 3.1 are given in [6]. Theorem 3.1 shows that the regret
is bounded by (cid:107)ηt+1 − Φt(ηt)(cid:107), where the shifting operator Φt is dependent on the outer-loop
policy. However, this result is not guaranteed for non-convex objectives, which will be a subject
of future work.

Having described the main methodology, we will now study a widely used family of control

distributions that can be used in the inner loop, the exponential family.

Exponential family of control distributions

We consider a parametric set of probability distributions for our control distributions in the
exponential family, given by natural parameters η, suﬃcient statistics δ and expectation pa-
rameters µ [27]. Further, we set Bregman divergence in (2.8) to the KL divergence, i.e.,

Dψ (η(cid:107)ηt) (cid:44) KL (πηt(cid:107)πη)

After employing KL divergence, our ηt update rule becomes:

ηt = arg min
η∈P

[α(cid:104)∇J(xt, ˜ηt), η(cid:105) + KL (π˜ηt(cid:107)πη)]

(3.2)

14

The natural parameter of control distribution, ˜ηt, is obtained with the proposed shift model
Φt from the outer loop RL policy πθt by setting the expectation parameter of ˜ηt: ˜µt = πθt(xt).
Note that we have overloaded the notation πθt to map the sequence xt to ˜µt, which is the
sequence of ˜µt,h = πθt (xt,h)1. Then, we have the following gradient of the cost:

∇˜ηtJ(xt, ˜ηt) = Eut∼π˜ηt ,xt∼fφ [C(xt, ut)(δ(ut) − ˜µt)] ,

(3.3)

where δ is the suﬃcient statistic, and for our experiments we choose Gaussian distribution
for control and δ(ut) := ut. We ﬁnally have the following update rule for the expectation
parameter [27]:

µt = (1 − α) ˜µt + αEπ˜ηt ,fφ [C (xt, ut) ut] .

(3.4)

Based on the data collected in the outer loop, the inner loop is executed via DMD-MPC as

follows:

Step 1. The shifting parameter Φt is obtained by using the outer loop parameter θt. Now, con-

sidering H-step horizon, for h = 0, 1, 2, . . . , H − 1, obtain

˜ηt,h = Σ−1 ˜µt,h,

˜µt,h = πθt(xt,h)

ut,h ∼ π˜ηt,h
xt,h+1 ∼ fφ(xt,h, ut,h).

(3.5)

(3.6)

(3.7)

where Σ represents the covariance for control distribution.

Step 2. Collect ˜ηt = (˜ηt,0, ˜ηt,1, . . . , ˜ηt,H−1), and apply DMD-MPC (3.12) to obtain ηt.

MPC objective formulations

Similar to the exponential family, we can use diﬀerent types of MPC objectives. Speciﬁcally,
we will be using the method of elite fractions that allows us to select only the best transitions.
This is given by the following:

J(xt, ˜ηt) := − log Eπ˜ηt ,fφ [1 {C (xt, ut) ≤ Ct,max}]

(3.8)

where we choose Ct,max as the top elite fraction from the estimates of rollouts. Alternative
formulations are also possible, and, speciﬁcally, the objective used by the MPPI method in [28]

1Note that if the policy is stochastic, then ˜µt,h ∼ πθt (xt,h). This is similar to the control choices made in

[15, Algorithm 2, Line 4].

15

is obtained by setting the following objective and α = 1 in (3.4) and for some λ > 0:

J(xt, ˜ηt) = − log Eπ˜ηt ,fφ

(cid:20)

(cid:18)

exp

−

1
λ

(cid:19)(cid:21)

C (xt, ut)

.

(3.9)

This shows that our formulation is more generic and some of the existing approaches could be
derived with suitable choice: [15, 1] and [8]. Table 4.3 shows the speciﬁc DMD-MPC algorithm
and the corresponding shift operator used for each case.

Table 3.1: Mb-Mf algorithms as special cases of our generalised framework

Mb-Mf Algorithm RL
SAC
TD3 MPPI with CEM Left shift (obtained from the previous iterate)
SAC

Shift Operator
Obtained from Mf-RL Policy

Obtained from Mf-RL Policy

MoPAC
TOPDM
DeMoRL

DMD-MPC
MPPI

CEM

3.2 DeMo RL Algorithm

DeMoRL algorithm derives from other Mb-Mf methods in terms of learning dynamics and fol-
lows a similar ensemble dynamics model approach. We have shown it in Algorithm 2. There are
three parts in this algorithm: Model learning, Soft Actor-Critic and DMD-MPC. We describe
them below.

Model learning. The functions to approximate the dynamics of the system are K-
probabilistic deep neural networks [9] cumulatively represented as {fφ1, fφ2, . . . , fφK }. Such
a conﬁguration is believed to account for the epistemic uncertainty of complex dynamics and
overcomes the problem of over-ﬁtting generally encountered by using single models [2].

SAC. Our implementation of the proposed algorithm uses Soft Actor-Critic (SAC) [4] as
the model-free RL counterpart. Based on principle of entropy maximization, the choice of
SAC ensures suﬃcient exploration motivated by the soft-policy updates, resulting in a good
approximation of the underlying dynamics.

DMD-MPC. Here, we solve for Eπ˜ηt ,fφ [C (xt, ut) ut] using a Monte-Carlo estimation ap-
proach. For a horizon length of H, we collect M trajectories using the current policy πθt and the
more accurate dynamic models from the ensemble having lesser validation losses. For all tra-
jectories, the complete cost is calculated using a deterministic reward estimate and the value
function through (2). After getting the complete state-action-reward H-step trajectories we
execute the following based on the CEM [21] strategy:

Step 1 Choose the p% elite trajectories according to the total H-step cost incurred. We set

16

p = 10% for our experiments, and denote the chosen respective action trajectories and
costs as Uelites and Celites respectively. Note that we have also tested for other values of
p, and the ablations are shown in the Appendix attached as supplementary.

Step 2 Using Uelites and Celites we calculate Eπ˜ηt ,fφ [C (xt, ut) ut] as the reward weighted mean of

the actions i.e.

gt =

(cid:80)

i∈elites Ci Ui
(cid:80)
i∈elites Ci

(3.10)

Step 3 Finally, we update the current policy actions, ˜µt = πθt(xt) according to (3.4) as

µt = (1 − α) ˜µt + αgt.

(3.11)

Algorithm 2 DeMoRL Algorithm

1: Initialize SAC and Model: φ, SAC and Environment Parameters
2: Initialize memory buﬀers: DEN V , DM P C
3: for each iteration do
4: DEN V ← DEN V ∪ {x, u, r, x(cid:48)} , u ∼ πθ (x)
for each model learning epoch do
5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

17:

Train model fφ on DEN V with loss : Jφ = (cid:107)x(cid:48) − fφ(x, u)(cid:107)2

end for
for each DMD-MPC iteration do

Sample xt,0 uniformly from DEN V
Simulate M trajectories of H steps horizon:
Perform CEM to get optimal action sequence: µt (3.10) and (3.11)
Collect complete trajectory: xt ∼ fφ(xt,0, µt)
Add all transitions to DM P C : DM P C ← DM P C ∪ {xt,h, ut,h, ˆrt,h, xt,h+1}

(3.5), (3.6) and (3.7)

end for
for each gradient update step do

Update SAC parameters using data from DEN V ∪ DM P C

end for

18: end for

17

3.3 DeMo Layer

We consider the special case of above generalised framework where the outer loop RL is not
updated and is already trained till convergence. At a state, the RL policy gives distribution
over actions. With the shift model obtained from the trained RL the equation 3.12 now has
ﬁxed shift model.

ηt = arg min

η

[α(cid:104)∇J(xt, ˜ηt), η(cid:105) + KL (π˜ηt(cid:107)πη)]

(3.12)

The ˜ηt,is obtained from RL policy πθ by setting

µt = πθ(xt)

The updated policy πη is optimal both in terms of long term expected reward and short term
horizon based cost.Following derivations from previous section we have the closed form expres-
sion for action with Gaussian distribution for control policy,

ut = (1 − γt) ut,RL + γtEπ ˜θt

,f [Ct (ˆxt, ˆut) ˆut,]

This gives an action which is a convex combination of RL action and the action that are good
according to the current cost J We sample an action from the updated policy πη and apply it to
the real environment unlike previous case, thus guiding the RL Policy End to End.A graphical
representation of the DemoLayer framework is given in Figure 3.4.We describe the three parts
of DeMo Layer, here: Model learning, ARS and DMD-MPC.

Model Learning:

Stoch:Model Dynamics is learnt using Feed forward Neural Networks [16]
Cartpole:We used the Model given in Open AI gym with biased length for the MPC
ARS: It is a linear deterministic policy and we have implemented this with modiﬁcation as

in Algorithm 1 for faster convergence.

DMD-MPC.: We use the similar strategy as described in DeMo RL.

18

Figure 3.4: The proposed DeMo Layer with an inner loop DMD-MPC update to guide outer
loop RL.

19

Chapter 4

Experimental Results

In this section, we implement the two-loop hierarchical framework as explained in the previous
sections for the DeMo RL and DeMo Layer Setting. We will compare the DeMo RL with the ex-
isting approaches MoPAC [15] and MBPO [8] on the benchmark MuJoCo control environments.
We test the DeMo Layer for cartpole and custom build Quadruped Stoch.

4.1 DeMo RL: Results and Comparison

Several experiments were conducted on the MuJoCo [25] continuous control tasks with the
OpenAI-Gym benchmark and the performance was compared with recent related works MoPAC
[15] and MBPO [8]. First, we discuss the hyperparameters used for all our experiments and
then the performance achieved in the conducted experiments

As the baseline of our framework is built upon MBPO implementation, we use the same
hyperparameters for our experiments and both the algorithms. We compare the results of three
diﬀerent seeds and the reward performance plots are shown in Figure 4.1. For the inner DMD-
MPC loop we choose a constant horizon length of 15 and perform 100 trajectory rollouts. With
our elite fraction as 10%, the updated model-based transitions are added to the MPC-buﬀer.
This process is iterated with a batch-size of 10, 000 thus completing the DMD-MPC block in
Inspired from MoPAC and MBPO, the number of interactions with the true
Algorithm 2.
environment for SAC were kept constant to 1, 000 for each epoch.

For HalfCheetah-v2, Hopper-v2 and InvertedDoublePendulum-v2, we clearly note an accel-
erated progress with approximately 30% faster rate in the reward performance curve. Whereas
in Ant-v2, our rewards were comparable with MoPAC but still signiﬁcantly better than MBPO.
Our ﬁnal rewards are eventually the same as achieved by MoPAC and MBPO, however the

20

(a) HalfCheetah-v2

(b) Ant-v2

(c) Hopper-v2

(d) InvertedDoublePendulum-v2

Figure 4.1: Reward Performance of DeMoRL algorithm over other model based algorithms:
MoPAC and MBPO.

progress rate is faster for all our experiments. and requires lesser number of true environment
interactions. Furthermore, all the experiments were conducted with the same set of hyper-
parameters, thus tuning them individually might give better insights. Table 4.3 shows the
empirical analysis of the acceleration achieved by DeMoRL.

Here, we not only show a generic formulation of the DMD-MPC, but also demonstrate how
new types of objectives can be obtained and further improvements can be made. As shown
in the table, we perform better than or at least as good as MoPAC, which uses information
theoretic model predictive path integrals (i-MPPI) [28], a special case of our setup as shown
in (3.9). The MPPI formulation uses all the rollouts to calculate the action sequence while the
CEM uses elite rollouts, which contributed to the accelerated progress.

Here we show a detailed study on the elite percentage referring to the previous steps, after

getting complete state-action-reward H-step trajectories, we execute Steps 1 to 3 in page 16.

21

Table 4.1: Mean Reward Performance Analysis of DeMo RL

Environment

HalfCheetah-v2

Ant-v2

Hopper-v2

20
7333
4978
7265
984.0
593.6
907.5

40
Epochs
10691
DeMoRL
8912
MoPAC
9461
MBPO
2278.4
DeMoRL
2337.3
MoPAC
MBPO
1275.6
DeMoRL 3077.3 3077.5
3137.9
MoPAC
2683.5
MBPO

789.9
813.9

60
11037
10212
10578
3845.5
3649.5
1891.9
3352.4
3270.2
3229.9

Figure 4.2: Ablation study for elite percentage: Reward performance curve (left) and Acceler-
ation analysis as epochs to reach 10000 rewards (right)

Given the sequence of controls µt, we collect the resulting trajectory and add them to our
buﬀer. Therefore, the quality of µt is a signiﬁcant factor aﬀecting the quality of data used for
the outer loop RL-policy. The selection strategy being CEM, a quality metric is dependent on
the choice of elite fractions p. We perform an ablation study for 6 values of p = 1, 5, 10, 20, 50
and 100% on HalfCheetah-v2 OpenAI gym environment. The analysis was performed based on
the reward performance curves as shown in Fig. 4.2 (left). Additionally, we realize the number
of the epochs required to reach a certain level of performance as a good metric to measure
acceleration achieved. Such an analysis is provided in Fig. 4.2 (right). We make the following
observations:

• Having a lesser value of p might ensure that learned dynamics is exploited the most, but

decreases the exploration performed in the approximated environment.

• Similarly, having higher value of p on the other hand will do more exploration using a

“not-so-perfect” policy and dynamics.

22

Thus, the elite fraction balances between exploration and exploitation.

4.2 DeMo Layer: Results for Cartpole and Stoch

We have conducted experiments on two diﬀerent environments.
Cartpole: A linear policy is trained on Cartpole using ARS and there exists no linear policy
that could acheive swing up and balance. Showed that DeMo Layer could guide the linear
policy to acheive swing up and balance on cartpole.
Stoch: Stoch2, a quadruped Robot is trained using the linear approach given in [19]. With
neural network approximation to the Model Dynamics, DeMo Layer is implemented on Stoch
to learn robust walking for episode length of 500.

Table 4.2: Reward Performance Analysis of DeMo Layer

Environment Linear Policy Linear Policy with DeMo Layer

Cartpole
Stoch2

1400
1500

1700
1850

Table 4.3: Hyper parameters used for DeMo Layer

Environment Horizon Sampled Trajectories

Cartpole
Stoch2

120
20

90
200

The simulation results for cartpole and stoch could be found here 1

1https://github.com/soumyarani/End-to-End-Guided-RL-using-Online-Learning

23

Chapter 5

Conclusion and Future Work

We proposed a generic framework using a novel combination of DMD MPC with Model Free
RL. Diﬀerent choices gives existing Mb-Mf Approaches with added ﬂexibility to use oﬀ the shelf
RL algorithms and ease to deﬁne new DeMo RL Algorithms with diﬀerent MPC techniques.
We have investigated the role of leveraging the model-based optimisation with online learning
to accelerate model-free RL algorithms using DeMo RL and we have given an end to end algo-
rithm DeMo Layer that could be used with RL algorithms for online adaption. While several
methods use model predictive rollouts in Mb-Mf approaches, we give a generalization using this
framework for diﬀerent MPC Controllers with Mf-RL algorithms.

Further, mirror-descent ensures that the MPC policy and the RL policy are proximal enough
so that transitions from MPC do not suﬀer from distributional shift. Our analysis shows that
our formulation is generic and achieves asymptotic performance of Mf-RL algorithms as the
underlying online learning tracks for the best policy and ultimately converges to the target
Mf-RL policy. Further the empirical study shows, the policy is learnt substantially faster than
prior Mf-Mb methods.

Future work would involve testing on the Real Robot and regret analysis of proposed hi-
erarchical approach. We would like to apply DeMo RL on other complex environments like
dexterous hand manipulation. We would also like to extend theDeMo Layer to Safe RL Set-
tings.

24

Bibliography

[1] Henry Charlesworth and G. Montana. Solving challenging dexterous manipulation tasks
with trajectory optimisation and reinforcement learning. ArXiv, abs/2009.05104, 2020. 2,
16

[2] Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep re-
inforcement learning in a handful of trials using probabilistic dynamics models.
In
S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Gar-
nett, editors, Advances in Neural Information Processing Systems, volume 31. Cur-
ran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/
3de568f8597b94bda53149c7d7f5958c-Paper.pdf. 16

[3] Gal Dalal, Krishnamurthy Dvijotham, Matej Vecerik, Todd Hester, Cosmin Padu-
raru, and Yuval Tassa. Safe exploration in continuous action spaces. arXiv preprint
arXiv:1801.08757, 2018. 1

[4] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Oﬀ-
policy maximum entropy deep reinforcement learning with a stochastic actor. In Interna-
tional Conference on Machine Learning, pages 1861–1870. PMLR, 2018. 3, 8, 11, 16

[5] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee,
and James Davidson. Learning latent dynamics for planning from pixels. In Kamalika
Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Con-
ference on Machine Learning, volume 97 of Proceedings of Machine Learning Research,
pages 2555–2565. PMLR, 09–15 Jun 2019. URL http://proceedings.mlr.press/v97/
hafner19a.html. 2

[6] Eric Hall and Rebecca Willett. Dynamical models and tracking regret in online convex
programming. In International Conference on Machine Learning, pages 579–587. PMLR,
2013. 12, 14

25

BIBLIOGRAPHY

[7] Prateek Jain, Sham M Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford.
Accelerating stochastic gradient descent. arXiv preprint arXiv:1704.08227, 3, 2017. 8

[8] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model:
Model-based policy optimization. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-
Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Sys-
tems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.
cc/paper/2019/file/5faf461eff3099671ad63c6f3f094f7f-Paper.pdf. 2, 3, 16, 20

[9] J. Zico Kolter and Gaurav Manek.

Learning stable deep dynamics models.

In
H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Gar-
nett, editors, Advances in Neural Information Processing Systems, volume 32. Cur-
ran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
0a4bbceda17a6253386bc9eb45240e25-Paper.pdf. 16

[10] Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, and Marco Hutter.
Learning quadrupedal locomotion over challenging terrain. Science robotics, 5(47), 2020.
1

[11] Sergey Levine and Vladlen Koltun. Guided policy search. In Sanjoy Dasgupta and David
McAllester, editors, Proceedings of the 30th International Conference on Machine Learning,
volume 28 of Proceedings of Machine Learning Research, pages 1–9, Atlanta, Georgia, USA,
17–19 Jun 2013. PMLR. URL http://proceedings.mlr.press/v28/levine13.html. 2

[12] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval
Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement
learning. In ICLR (Poster), 2016. URL http://arxiv.org/abs/1509.02971. 11

[13] Kendall Lowrey, Aravind Rajeswaran, Sham Kakade, Emanuel Todorov, and Igor Mor-
learn oﬄine: Eﬃcient learning and exploration via model-based
In International Conference on Learning Representations, 2019. URL https:

datch. Plan online,
control.
//openreview.net/forum?id=Byey7n05FQ. 2, 12

[14] Horia Mania, Aurelia Guy, and Benjamin Recht. Simple random search provides a com-
petitive approach to reinforcement learning. arXiv preprint arXiv:1803.07055, 2018. 6

[15] Andrew S Morgan, Daljeet Nandha, Georgia Chalvatzaki, Carlo D’Eramo, Aaron M Dollar,
and Jan Peters. Model predictive actor-critic: Accelerating robot skill acquisition with deep
reinforcement learning. arXiv preprint arXiv:2103.13842, 2021. 2, 3, 12, 15, 16, 20

26

BIBLIOGRAPHY

[16] Anusha Nagabandi, Gregory Kahn, Ronald S Fearing, and Sergey Levine. Neural network
dynamics for model-based deep reinforcement learning with model-free ﬁne-tuning.
In
2018 IEEE International Conference on Robotics and Automation (ICRA), pages 7559–
7566. IEEE, 2018. 2, 18

[17] Anusha Nagabandi, Kurt Konolige, Sergey Levine, and Vikash Kumar. Deep dynamics
models for learning dexterous manipulation. In Conference on Robot Learning, pages 1101–
1112. PMLR, 2020. 3

[18] Yurii Nesterov and Vladimir Spokoiny. Random gradient-free minimization of convex

functions. Foundations of Computational Mathematics, 17(2):527–566, 2017. 7

[19] Kartik Paigwar, Lokesh Krishna, Sashank Tirumala, Naman Khetan, Aditya Sagi, Ashish
Joglekar, Shalabh Bhatnagar, Ashitava Ghosal, Bharadwaj Amrutur, and Shishir Ko-
lathaya. Robust quadrupedal locomotion on sloped terrains: A linear policy approach,
2020. 23

[20] Xue Bin Peng, Erwin Coumans, Tingnan Zhang, Tsang-Wei Edward Lee, Jie Tan, and
Sergey Levine. Learning agile robotic locomotion skills by imitating animals. In Robotics:
Science and Systems, 07 2020. doi: 10.15607/RSS.2020.XVI.064. 1

[21] Pourchot and Sigaud. CEM-RL: Combining evolutionary and gradient-based methods
for policy search. In International Conference on Learning Representations, 2019. URL
https://openreview.net/forum?id=BkeU5j0ctQ. 16

[22] Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman,
Emanuel Todorov, and Sergey Levine. Learning complex dexterous manipulation with
deep reinforcement learning and demonstrations. In Proceedings of Robotics: Science and
Systems, Pittsburgh, Pennsylvania, June 2018. doi: 10.15607/RSS.2018.XIV.049. 1

[23] Alexander Robey, Lars Lindemann, Stephen Tu, and Nikolai Matni. Learning robust
hybrid control barrier functions for uncertain systems. arXiv preprint arXiv:2101.06492,
2021. 1

[24] John Schulman, Sergey Levine, Pieter Abbeel, Michael I. Jordan, and Philipp Moritz. Trust
region policy optimization. In ICML, pages 1889–1897, 2015. URL http://proceedings.
mlr.press/v37/schulman15.html. 2

27

BIBLIOGRAPHY

[25] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based
control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems,
pages 5026–5033. IEEE, 2012. 20

[26] Manan Tomar, Lior Shani, Yonathan Efroni, and Mohammad Ghavamzadeh. Mirror de-
scent policy optimization. CoRR, abs/2005.09814, 2020. URL https://arxiv.org/abs/
2005.09814. 2

[27] Nolan Wagener, Ching an Cheng, Jacob Sacks, and Byron Boots. An online learning
approach to model predictive control. In Proceedings of Robotics: Science and Systems,
FreiburgimBreisgau, Germany, June 2019. doi: 10.15607/RSS.2019.XV.033. 3, 7, 9, 12,
14, 15

[28] Grady Williams, Nolan Wagener, Brian Goldfain, Paul Drews, James M. Rehg, Byron
Boots, and Evangelos A. Theodorou.
Information theoretic mpc for model-based rein-
forcement learning. In 2017 IEEE International Conference on Robotics and Automation
(ICRA), pages 1714–1721, 2017. doi: 10.1109/ICRA.2017.7989202. 15, 21

[29] Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine,
Chelsea Finn, and Tengyu Ma. Mopo: Model-based oﬄine policy optimization.
In
H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Ad-
vances in Neural Information Processing Systems, volume 33, pages 14129–14142. Cur-
ran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
a322852ce0df73e204b7e67cbbef0d0a-Paper.pdf. 2

[30] Jesse Zhang, Brian Cheung, Chelsea Finn, Sergey Levine, and Dinesh Jayaraman. Cau-
In International

tious adaptation for reinforcement learning in safety-critical settings.
Conference on Machine Learning, pages 11055–11065. PMLR, 2020. 1

28

