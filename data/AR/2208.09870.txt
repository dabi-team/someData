2
2
0
2

g
u
A
1
2

]

V
C
.
s
c
[

1
v
0
7
8
9
0
.
8
0
2
2
:
v
i
X
r
a

Objects Can Move: 3D Change Detection by
Geometric Transformation Consistency

Aikaterini Adam1,2, Torsten Sattler1, Konstantinos Karantzalos2, and Tomas
Pajdla1

1 Czech Institute of Informatics, Robotics and Cybernetics, CTU in Prague
{Aikaterini.Adam,Torsten.Sattler,pajdla}@cvut.cz
2 National Technical University of Athens, Greece
karank@scentral.ntua.gr

Abstract. AR/VR applications and robots need to know when the
scene has changed. An example is when objects are moved, added, or
removed from the scene. We propose a 3D object discovery method
that is based only on scene changes. Our method does not need to en-
code any assumptions about what is an object, but rather discovers ob-
jects by exploiting their coherent move. Changes are initially detected
as diﬀerences in the depth maps and segmented as objects if they un-
dergo rigid motions. A graph cut optimization propagates the changing
labels to geometrically consistent regions. Experiments show that our
method achieves state-of-the-art performance on the 3RScan dataset
against competitive baselines. The source code of our method can be
found at https://github.com/katadam/ObjectsCanMove.

Keywords: 3D change detection, object discovery, graph optimization

1

Introduction

The ability to detect and interact with objects is critical to AR/VR applications
and for multiple robotics tasks, such as surveillance, robotic manipulation, and
maintaining order. All these tasks are operated in the same setting. Thus, the
robot, or the AR/VR device stores a reference map and builds a new map upon
each revisit. However, in-between the revisits, certain objects may have changed.
Checking for scene consistency and detecting changes on an object-level can thus
lead to 3D object discovery, without the need of labeled data.

Motivated by the above, we explore an object discovery approach, based on
examining scene consistency on an object-level and without using annotated
data. We are aiming at discovering entities (objects) that have changed when
revisiting a place. We show that it is possible to detect 3D objects purely geo-
metrically, without a predeﬁned notion of objects. The underlying idea is that
objects, unlike the static background of a scene, can be moved. This is an intu-
itive deﬁnition of “objectness” that does not need any annonated data.

Segmenting dynamic objects in temporal observations is a long-standing chal-
lenge. There are two ways to apply this idea: (1) segment objects from the

 
 
 
 
 
 
2

A. Adam et al.

background by actively observing their motion, e.g., by reconstructing dynamic
objects during SLAM [39], or (2) revisit the same scene after a (longer) period
and detect potential objects as changes between two maps [13]. We follow the
latter approach, i.e., we model the problem as a change detection task.

Detecting potential scene changes based on direct data analytics is a task
attracting much attention since aﬀordable 3D scanning technology [12, 37, 2]
makes such data widely available. However, a straightforward approach to de-
tecting changes between two scans based on voxel occupancy or inconsistency
maps [26] would often miss changes, e.g., when an object rotates around an axis
passing through the object or when it is “slid along itself”. An alternative ap-
proach employs the comparison of visual features and relies on photoconsistency
constraints [33]. Yet, this approach does not perform well in our setting since
there can be signiﬁcant illumination changes between the two maps.

To tackle the aforementioned shortcomings, we introduce a novel change
detection framework, depicted in Figure 1, that uses geometric transformation
consistency towards object discovery (i.e., change detection on an object-level).
3D objects are thus discovered without the need to encode what an object is. We
consider a scenario where we have two 3D maps, i.e., a reference scan (recorded
at time t0) and a rescan (recorded at time t1), of a scene, as well as the associated
camera poses. Initial change detections are computed as diﬀerences in the depth

Fig. 1. Workﬂow of the proposed method: given two scans recording changes and
the associated camera poses, we discover all objects that have been added, moved, or
removed from the scene. Initial geometric changes are detected as diﬀerences in depth
maps (Step 1). The dominant transformations are then computed (Step 2). The initial
set of detections is incomplete and thus reﬁned, using a graph cut-based optimization
on a supervoxel representation, propagating change to all regions undergoing the same
transformation (Step 3). Discovered objects are presented as the extracted connected
components of the reﬁned detections

Method OutputT1T2T3 Depth image 1Depth image 2Subtracted imageaddedReference scan (t0)Input Data: Aligned reference scan and rescan, camera poses. The changes are highlighted as recorded in the ground-truth.(1) Initial change detection: for all poses render a pair of depth maps, subtract them and threshold the result. Backproject and fuze the results  for each pair. (2) Establish correspondences between the reference scan and the rescan. Compute rigid transformations using a RANSAC-iterative scheme.Rescan (t1)Method InputStep 1Step 2Depth image pairAfter optimization(3) Propagate change to neighboring supervoxels, undergoing the same rigid transformation, using a graph cut optimization. A connected component analysis is then applied to form the discovered objects. The formed connected components are the output of the method.Before optimizationStep 3Fuzed backprojected detections for all pairs (set of camera poses)Reference scan (t0)Rescan (t1)Corresponding 3D points of thresholded imageThresholded imageOur connected componentsGround-truth connected components Objects Can Move

3

maps. As shown in Figures 1 and 2, the initial detected points mainly delineate
the boundaries of the moved objects. To recover all parts, we propagate changes
from regions where we can detect them to parts where no changes were seen,
but which belong to the same object. Our local robust feature matching between
parts of the two scans generates motion hypotheses for the scene parts, induced
by the moved objects. These motions can measure consistency as scene parts
that undergo the same rigid transformation.

Contributions. We introduce a novel 3D change detection framework via
geometric transformation consistency. As change detection is performed on an
object-level, this novel framework serves as an object discovery method in 3D
scenes, without needing any strong priors or deﬁnition of what objects are. We
showcase that even though we target rigid objects/changes, our method can also
handle non-rigid changes, as shown in Figure 4. The proposed method achieves
state-of-the-art performance on the 3RScan dataset [37], against competitive
baselines.

We evaluate our framework on the 3RScan dataset [37], initially designed for
benchmarking object instance relocalization. Our evaluation shows the potential
of the dataset to assess 3D change detection. We provide code to generate the
ground truth annotations.

2 Related Work

Change Detection. 3D Change detection is directly related to our method
since the presented workﬂow is modeled in this concept. Change detection has
been traditionally treated mostly by geometric approaches [34, 33, 25, 26, 40, 36].
Similar to our initial detection step, [33, 34, 25] detect changes based on inconsis-
tency maps from RGB or depth projections. Many change detection algorithms
[33, 27] are based on the concept of initial change detection (e.g., though color
consistency, comparing depth values, etc.), followed by propagating these detec-
tions to identify all regions that have changed. [33, 27] propagate change using
spatial and photoconsistency constraints. Our approach follows the same out-
line, but diﬀers in the key step of change propagation, through a novel geometric
twist. Thus, our method is illumination invariant and can be applied to complex,
open-set environments under varying illumination conditions.

SLAM Methods for Dynamic Object Segmentation. When addressing
dynamic scenes, tracking dynamic objects can be part of SLAM-based tech-
niques. In [1], dynamic parts of the scenes are recovered and a classiﬁer is trained
on them to distinguish between static and non-static parts. Semantic SLAM for
dynamic environments is presented in [7, 8]. In [30], the authors ﬁrst segment
objects and track them separately. In a similar vein to our research, [10, 15, 24]
aim at discovering objects through change observation on an object-level. How-
ever, these works build their methods upon a SLAM-based basis. Our method
is complementary to SLAM-based techniques since these methods demand the
recording of the object’s actual movement in front of the camera. On the other

4

A. Adam et al.

hand, our method needs two 3D models (reference scan and rescan), and the
associated camera poses, which are acquired over long time intervals. Thus, ob-
jects might have moved, appeared, or disappeared without their movement being
explicitly recorded.

3D Object Discovery. Our problem can be conceived as a 3D object discov-
ery technique when declaring as an object everything that can be moved, since
movement is an inherent property of objects. Concerning unsupervised object
discovery, the authors of [17] focus on identifying parts of the input mesh as
candidate objects. They then classify them as an object or clutter. More simi-
larly to our work, [22, 21] extract as objects all the novel additions to the scene.
Indeed, by scene comparison, they discover and label as an object anything that
has been added between two scans. In contrast, our proposed method does not
restrict itself only to added objects, but rather discovers all the objects that have
changed (added, moved or removed).

3 Detection via Geometric Consistency

Our method aims at detecting changes on an object-level, thus leading to ob-
ject discovery, without relying on annotated data. Given two 3D scans, i.e., a
reference scan and a rescan, and the associated camera poses, we propose three
discrete steps, as illustrated in Figure 1: (1) initial change detection, i.e., com-
pute the locations where a change might have occured, (2) compute dominant
transformations, and (3) graph optimization to ensure geometrical transforma-
tion consistency. Diﬀerences in depth maps provide an initial but incomplete
set of detections later reﬁned using a graph cut-based optimization. The central
insight is that scene parts that belong to the same object should undergo the
same physical transformation, which we model through a novel geometric trans-
formation consistency measure. A connected component analysis is then applied
to form the discovered objects.

Initial changes are calculated by depth map comparison. Given a reference
scan S of the scene and a rescan R aligned to each other, we render and sub-
tract depth maps. Their subtraction records changing depth values and thus
indicates changed regions. However, due to the way the objects move, it is dif-
ﬁcult to retrieve the whole object via this single step (as illustrated in Figure
2). To tackle this limitation, we integrate graph optimization [20], performed on
supervoxels [28]. Instead of using a simple voxel representation of the scene, we
ﬁrstly compute supervoxels, i.e., irregular clusters of 3D points sharing common
geometrical and color characteristics. Optimizing this representation leads to
more accurate results, since supervoxels separate the 3D space into elements, by
clustering points with same properties. This is not the case for voxels that are
created solely on spatial relations of the 3D points. Moreover, as supervoxels are
irregular patches of 3D points, they can preserve objects’ boundaries, contrary
to the simple voxel representation.

Graph optimization aims to enforce consistency for all the regions undergo-
ing the same rigid transformation. This will help us discover parts of the moving

Objects Can Move

5

object that may have been missed during the initial detection step. The change
is propagated to all the supervoxels undergoing the same movement. From the
above, it is clear that two steps are needed before the optimization: (1) initial
change detection, and (2) computation of all the dominant transformations in-
duced by moved objects. Towards the latter goal, learned descriptors [29] are
extracted for each point in the scans. We use a pre-trained model, trained on a
completely diﬀerent task (i.e., semantic segmentation). Matches are then com-
puted using nearest neighbor search [16]. The resulting correspondences are used
to calculate the 3D transformations.

Scan Alignment. Works and datasets [37, 13] exploring changing indoor scenes
demand the two scans to be registered. These datasets provide information for
the alignment since registering the scans is outside the scope of their research.
Similarly to these works, we use the initial alignment provided by the dataset,
which was obtained via manual annotations and is imperfect. In practical appli-
cations, the alignment could be provided by re-localization to the previous scan,
or by estimating the overall transformation via feature matching [3].

3.1 Initial Change Detection

The ﬁrst step of our method is identifying changing regions, which we will re-
ﬁne via a graph optimization. Initial change detection is based on depth map
comparison. We render depth maps DS,1,..,N , DR,1,..,N from the reference scan
S and the rescan R respectively, for all the viewpoints i = 1, 2, .., N , using the
P1,..,N projection matrices. Multiple poses cover the whole 3D scene. We use the
same poses to render both the reference scan S and the rescan R, as we assume
that both scans are already aligned, even if captured from diﬀerent viewpoints.
We render the depth images rather than simply use raw depth measurements
captured by a device to ensure the best possible quality. Moreover, we use depth
maps instead of directly working on the mesh, allowing us to handle occlusions
and partial observations more naturally than in the 3D space. Working on depth
images provides information about free space, which is not directly included in
the mesh. Indeed, rendering depth allows us to know if the corresponding 3D
region has been scanned or not. If one of the depth images does not contain infor-
mation, we exclude this region from the initial change detection. This procedure
is not straightforward in the mesh, as an intermediate step such as calculating
the bounding box of the scan or computing overlapping regions would be needed
to ensure that partial observations and free space is taken into consideration.
Rendered maps from the reference scan and the rescan are shown in Figure 1.
Lighter regions correspond to regions that lie closer to the camera. The paired
depth maps are subtracted, and the result is thresholded using [4].

The result is a binary mask, encoding information about changing regions,

which are back-projected to the 3D space:

[X, Y, Z]T = RT · (K−1 · [x, y, 1]T · D(x, y) − Tr) ,

(1)

6

A. Adam et al.

where [X, Y, Z] stands for the world-coordinates of the 3D point, R for the rota-
tion matrix, K for the calibration matrix and Tr for the translation vector, all
forming the projection matrix P = K[R|Tr]. Vector [x, y, 1] represents the pixel
coordinates and D1,..,N (x, y) the depth value stemming from the combination of
the depth of the reference scan and the depth of the rescan:

D(x, y)1,..,N =

(cid:40)

D(x, y)S,1,..,N
D(x, y)R,1,..,N

if D(x, y)S,1,..,N < D(x, y)R,1,..,N
if D(x, y)S,1,..,N > D(x, y)R,1,..,N

.

(2)

D(x, y)1,..,N represents the depth value at position (x, y) for the combined masks
1, .., N , D(x, y)S,1,..,N the depth value at position (x, y) for the 1, .., N rendered
depth maps of the reference scan S and D(x, y)R,1,..,N the depth value of the
rendered 1, .., N depth maps of the rescan R. This formulation always selects the
closest objects to the camera. After experiments, we concluded that in 97% of
examined cases, the smaller depth value corresponds to an object. In contrast,
the larger depth corresponds to the static background. Figures 1 and 2 depict
all the initial points labeled as changing for example scenes. Finally, as the
graph optimization is applied to the supervoxel representation, the supervoxels
for the scan and the rescan are extracted [28] and the number of changing points
belonging to each supervoxel is computed.

3.2 Computing Dominant Transformations

As seen in Figures 1-Before optimization, 2-Initial detected points, our initial
detection step may miss changes due to occluded parts of the objects, objects
partially captured in one of the scans or due to the way objects have moved.
Indeed, when an object is only slightly moved or rotated, there can be regions
where the depth values do not change, e.g., when a couch is only slightly shifted.
As a result, the initial detections might only cover part of the object. To this
end, we use a graph optimization to propagate change detection to the rest
of the objects based on consistency under geometric transformations T. We
thus compute the diﬀerent 3D rigid transformations T, induced by the moving
objects. Towards that, we match feature descriptors between scans.

Descriptors can be computed using hand-crafted features, such as FPFH [31]
and SHOT [35], or learned features. In our case, we use features extracted from
the encoder part of a pre-trained deep network. A forward-pass was deployed
to densely extract descriptors from the scans, using the weights of pre-trained
models on a semantic segmentation task. The models we are using are trained
for a completely diﬀerent task and dataset. Yet, using learned features does not
aﬀect our assertion of presenting an unsupervised method.

Correspondences were then computed over the entire scene, using the near-
est neighbor search. Visualizing correspondences for the diﬀerent features showed
that the pre-trained Dynamic Graph CNN (DGCNN) [38] had the best prelim-
inary results. To remove outliers from the matches, and given that we want to
establish correspondences only between moving objects, we eliminate correspon-
dences lying within a predeﬁned distance of each other in 3D, as these points

Objects Can Move

7

are considered part of the static background. All the valid correspondences are
then employed to compute the potential transformations using RANSAC [5]. We
iteratively apply RANSAC on the remaining set of matches after removing the
inliers of the previous estimate and stop once less than three matches remain.
Since this method will generate more transformations T than the real ones due
to limitations in establishing correspondences, we will continue selecting the top
k transformations T, with the most inliers, to propagate change during graph
optimization.

3.3 Supervoxel Graph Optimization

From the initial change detection (Section 3.1), we obtain an initial soft labeling
L, based on the fraction of changing 3D points belonging to each supervoxel.
Supervoxels with more points labeled as changing, during the initial change
detection, are more likely to belong to an object. Thus, the initial labeling L
determines the probability ρ of each supervoxel vi to be labeled as changing
ρ(vi, li = 1), or non changing ρ(vi, li = 0) as:

ρ(vi, li = 1) =

ρ(vi, li = 0) =

(cid:40)

0.8
0.5

(cid:40)

0.2
0.5

if changing points ∈ vi
if changing points /∈ vi

,

if changing points ∈ vi
if changing points /∈ vi

.

(3)

(4)

The weights used in Equations 3 and 4 were chosen based on experiments
with diﬀerent values on a set of scans used for tuning hyperparameters (cf.
Section 4). From the above, it is clear that we treat supervoxels with no detected
changing points as equally likely of having changed. Indeed, supervoxels without
any detected changing points do not necessarily correspond to static scene parts.
We thus decide whether a supervoxel belongs to a moving object or not by solving
a graph optimization problem [20] that allows us to propagate changes between
supervoxels, conditioned on a rigid transformation T.

To deploy the optimization, we create an undirected graph G = (V, E). Each
node v ∈ V corresponds to a supervoxel in the scene. Two nodes vi and vj are
connected through an edge {vi, vj} ∈ E if the corresponding supervoxels are
adjacent to each other. Given a rigid transformation T (cf. Section 3.2) between
the rescan and the reference, our goal is to compute an optimised binary labeling
L∗ = {l∗
i }i. This labeling indicates for each supervoxel whether it belongs to a
i = 1) or not (l∗
changing object consistent with T (and thus is labeled as l∗
i = 0).
We compute this labeling by solving the graph optimization problem [20]:

L∗ ∈ arg min
Q∈Ωv

{Φ(L, Q) + λΨ (Q)} .

(5)

Φ stands for the ﬁdelity term (here, we use the Kullback-Leibler ﬁdelity
function [19]), Ψ for the regularizer, λ for the regularization strength, and Ω for

8

A. Adam et al.

the search space. The ﬁdelity term Φ(L, Q) enforces the inﬂuence of the initial
labeling L, i.e., it decreases when Q lies closer to L. The regularizer Ψ (Q) favors
geometrically smooth solutions, i.e., it enforces smoothness to all neighboring
supervoxels vi and vj, undergoing the same transformation T. Ψ (Q)is based on
a Potts penalty function:

ΨP otts(Q) =

(cid:40)
1
0

if vi, vj consistent under T
otherwise

.

(6)

It is important to note here that the energy function from Equation 5 is
conditioned on each computed transformation T. Thus, we iterate through the
top k computed dominant transformations and we solve a series of graph cuts
problems. Each iteration segments out the object undergoing the speciﬁc trans-
formation T. Objects added or removed, for which a transformation T is not
established, are solely retrieved, based on the unary potentials of the optimiza-
tion. The results of the iterative procedure, i.e., the set of points labeled as
changing, are ﬁnally fused. A connected component analysis is ﬁnally applied to
the fused results, to discover the ﬁnal 3D objects. Connected component analysis
is crucial to form the 3D added or removed objects that are not conditioned on
a transformation, but also to overcome the problem of over-segmentation when
slightly diﬀerent transformations are computed for the same object. Optimiza-
tion results are illustrated in Figures 1, 2 and 3.

Fig. 2. Our approach: given two scans depicting a scene that has potentially changed,
we discover all changes on an object-level. We initially detect potentially changed scene
parts by comparing depth maps. We then propagate changes and segment out changed
regions based on the principle of geometric transformation consistency. (a) Reference
scan (b) Rescan (c) Initially detected areas, with false detections on the wall due to
misalignments between the scans. (d) Ground truth connected components and (e)
connected components detected by our approach

(a) Reference scan(b) Re-scan(e) Our  connected components(c) Initial detected pointsT1T2T3T4(d) Ground-truth  connected componentsObjects Can Move

9

4 Experimental Evaluation

Datasets. To assess the performance of the proposed approach, we have con-
ducted experiments using the 3RScan dataset [37]. The dataset comprises in-
dividual rooms, capturing natural changing indoor environments. It provides,
apart from the 3D meshes of the reference scans and the rescans, a series of
RGB-D images captured by a Google Tango mobile phone and information con-
cerning objects that have changed between the scenes, along with corresponding
transformations. The experiments have been conducted on the validation subset
of the dataset comprising 47 diﬀerent reference scans and 110 rescans. It is im-
portant to note that the 3RScan dataset was built initially for object instance
relocalization tasks. Therefore, we had to generate the ground truth data for the
changing objects based on the dataset’s supplementary information. The code is
publicly available at https://github.com/katadam/ObjectsCanMove, to enable
the usage of this dataset for benchmarking indoor 3D change detection.

To the best of our knowledge, there is no other appropriatbe benchmark
to assess 3D indoor change detection and 3D object discovery. Relevant works
evaluate their methods on their own datasets, which are either not publicly
available [10, 22, 18] or are very small and require manual labeling, as they do
not provide appropriate annotations [9, 24, 1]. Please refer to the SM for speciﬁc
information on discarded datasets.

Hyperparameter Tuning. Ten randomly selected scans from the training
split of 3RScan were used for parameter tuning, while the validation split was
used for evaluation. The validation split covers many diﬀerent scene types (i.e.,
oﬃces, restaurants, living rooms, kitchens, etc.) to assess the generalization per-
formance and robustness to challenging conditions and unseen environments. In
our method, the main hyperparameters that need to be tuned are: the RANSAC
inlier threshold t for computing transformations T, the number k of transforma-
tions T to compute, and the weights for the graph optimization (as described
in Section 3.3). The threshold t can be set intuitively by the desired resolution
of the transformations. The number k of transformations should be set to the
number of objects that change in a scene. Overestimating k is not an issue as
beyond the actual number of objects, RANSAC will be applied to outliers. Al-
ternatively, one could also just stop once only a few matches are left or once the
best model found by RANSAC only has a few inliers.

Baseline Methods. To compare the performance of our novel framework
against a competitive set of other methods, we have searched for appropriate
baselines. However, we had to discard some works treating indoor change detec-
tion and unsupervised object discovery, since they are not directly comparable
to our method. More particularly, the input to our method are two scans, with
changes between them but no recorded actual motion in front of the camera. As
such, it is complementary to SLAM methodologies and technologies built upon
SLAM systems [10, 15, 1, 9]. Moreover, even though [13, 37] deploy their meth-
ods on changing indoor scenes, they focus on instance segmentation and object
instance re-localization, respectively. Thus, they cannot be evaluated against our

10

A. Adam et al.

Fig. 3. Qualitative evaluation of the proposed method. Given two scans (a reference
scan (a) and a rescan (b)), we perform change detection on an object-level basis, to
discover 3D objects. We visualise the ﬁnal results after applying connected component
analysis to the ground truth (c) and to our detected changes (d)

change detection task. Approaches like [21, 24] integrate semantics, contrary to
our approach that discovers object-level changes without having a predeﬁned
notion of what an object is. The input of [18] is data from range sensors and
a highly precise 3D map created by a 3D laser scanner, which is not the case
for the 3RScan dataset. [22] aims only at discovering novel objects in the scene,
while our approach retrieves all the changed objects. Towards that, we have a
created a sub-task of discovering only added objects and compare against [22].
Results are available in the SM. Finally, since we aim at change detection on
an object-level towards unsupervised object discovery, we compared against an
unsupervised 3D object discovery method [17]. [17] ﬁrst discovers segments and
then classiﬁes them into objects and non-objects. However, the segments ob-
tained via the authors’ code after tuning parameter were not meaningful and we
were not able to avoid a severe oversegmentation. Thus, we did not include the
metrics in our experimental results. For visualizations please refer to the SM.

Our approach is mainly inspired by the change detection approach from [33,
25, 26]. To the best of our knowledge, these are the most closely related baseline
and one of our motivations to redeﬁne this problem in a new framework by taking
advantage of modern representations (i.e., supervoxels) and more recent graph
optimization algorithms [20]. Similar to our work, [26, 25, 33] are also focusing
on unsupervised change detection. Taking all the above into consideration, we
have decided to create two main baselines inspired by these works.

In [33], change detection is based on inconsistency maps, formed by subtract-
ing pairs of images taken at diﬀerent points in time. The newly acquired image
is warped into the old one, using the known 3D scene geometry and the known
poses of both images with respect to the scene. Assuming similar illumination
conditions, the two images should be identical if no change in the geometry has
happened. In turn, changes in scene geometry will lead to inconsistent projec-
tions from one image into the other. Change detection is then optimized via a

(a) Reference scan(b) RescanT1T2T3non-rigidnon-rigid(c) Ground-truth connected comp.(d) Our connected comp.(a) Reference scan(b) RescanT1T2T3T4T5added(c) Ground-truth connected comp.(d) Our connected comp.Objects Can Move

11

graph cut on the voxelized representation. The inconsistency maps are used to
calculate the unary term of the graph, while the binary term accounts for color
smoothing. Similar to the ﬁrst step of [33], [26, 25] are discovering changes by for-
mulating inconsistency maps. These works augment the number of inconsistency
maps to achieve better results without any further optimization.

Since two 3D models are available in our case, we use the initial change
detection step from Section 3.1 to create the inconsistency maps for the two
baselines inspired by [33, 25, 26]. We go one step further and resort to depth
images instead of RGB images to ensure robustness to illumination conditions.
This initial change detection step (i.e., our method before optimization) serves
as the 1st baseline, namely Papazzolo et al., as it is equivalent to the work
presented in [26, 25]. In these works, estimation of 3D change detection results
from back-projecting inconsistencies from multiple 2D maps.

A 2nd baseline (Taneja et al.) is formed, following [33], where the initial
change detection is optimized ensuring color consistency on a voxelized represen-
tation of the scene via a graph cut optimization (solved by max-ﬂow algorithm
[6]). The binary term of the graph is computed as described in Equation 7:

ψij(li, lj) = [li (cid:54)= lj] · γ/(

(cid:88)

It

||vi

t − vj

t ||2 + 1),

(7)

t − vj

t ||2 accounts for the L2-norm between RGB values of voxels vi

where ||vi
t and
vj
t and γ is a regularization factor. Comparing against this baseline shows the
impact of using geometric consistency for propagating change, which is the main
technical contribution of this work.

Ablation Study. Three more baselines are formed in the form of an ablation
study, for a better insight into the proposed method. Ablation baselines are
reporting intermediate results of our framework. They also calculate the metrics
when the method has access to more information, in order to test its robustness
with respect to diﬀerent parameters. Removing the optimization part of our
method and relying only on initial change detection is equivalent to [26] and thus
reported in Table 1. The ﬁrst ablation baseline (ground truth transforms.)
ensures geometric consistency using the ground truth transformations provided
by the dataset instead of our computed ones. This gives an upper bound to
the performance we can achieve and helps measure the impact of estimated
transformation’s accuracy on the overall system’s eﬃciency.

As the 2nd ablation baseline (RANSAC inliers), closely related to [32],
we present the metrics of the non-static points used to form the matches and
compute the rigid transformations T. This is equivalent to only the second step
of our method (Section 3.2), without the initial change detection and the graph
optimization. Ideally, each set of inliers consistent with each RANSAC execution
would form the corresponding object moved under this transformation.

Finally, as the 3rd ablation baseline (Mask-RCNN), we add a semantic
component to the formulated algorithm, as we would like to get an idea of
how well our approach performs with respect to a supervised method. Thus,
we replace our novel geometric consistency-based term with a term based on

12

A. Adam et al.

Fig. 4. A non-rigid change (curtain) is not recorded in the ground truth. The curtain
is diﬀerent between the reference (a) and rescan (b), as shown when the two scans are
overlaid in (c). The detected changes are shown with red colour in (d), overlaid on the
reference scan in blue

Table 1. Mean IoU and mean recall for the proposed method and the published
baselines

Method

IoU(%) Recall(%)

Palazzolo et al.[26] / Ours bf optim. 54.23% 31.48%
48.10% 44.50%
Taneja et al. [33]
68.40% 76.05%
Our method

Table 2. Mean IoU and mean recall for the proposed method and the ablation study’s
baselines

Method

IoU(%) Recall(%)

68.40% 76.05%
Our method
Ground truth transforms. 72.40% 93.89%
52.96% 89.22%
Mask-RCNN
10.82% 29.50%
RANSAC inliers

the instance labels of Mask-RCNN [14], propagating the change to all regions
sharing the same semantic label. Mask-RCNN is a powerful method for 2D object
detection. We deploy the 2D object detector, trained on the COCO dataset [23],
on the RGB images of each rescan.

Experimental Results.
In addition to the qualitative results presented in
Figures 1, 2, 3, we rigorously evaluate our method by using metrics that capture
the success of 3D change detection and 3D object discovery. Since we are aiming
at object discovery through change detection on an object-level basis, we should
ﬁrst evaluate the eﬃciency of our change detection results. Thus, we calculate
the metric of recall, on a voxel basis. Recall aims to calculate how many of the
ground truth changed voxels have been correctly retrieved.

Moving on to 3D object discovery, a 3D connected components analysis is
applied to the change detection results. To assess the eﬃciency of the proposed
method as an object discovery pipeline, we deploy the metric of Interestion
Over Union (IoU) per discovered object, as it encapsulates both the metrics of
precision and recall. To calculate this metric, the connected component analysis
is also applied to the ground truth changes. For our scenario, this analysis was
performed on a voxel grid of 10 cm, which could sometimes merge objects that

(a) Reference scan(d) Detected changes   (b)  Re-scan(c) Overlaid scansObjects Can Move

13

lie together into a single component. This does not aﬀect our metrics since the
same connected component analysis is applied both to the ground truth and our
solution. However, a smaller step size would lead to a more reﬁned and detailed
object discovery. The parameter can be tuned based on the size of the objects
we want to discover. We consider an object as successfully discovered when the
metric of IoU is more than 20%. The metrics are calculated at a voxel-level since
we are interested in measuring how two objects (volumes) intersect.

Tables 1 and 2 show the mean recall over all the scans and the mean IoU
of discovered objects. After close examination, it is clear that our method out-
performs the most competitive baseline based on [33] by roughly 30% in terms
of recall. It also improves the mean IoU by almost 20%. This shows that not
only supervoxels constitute a more eﬃcient representation compared to single
voxels, when it comes to graph optimization, but also that the novel geomet-
ric transformation consistency is much more successful for propagating change,
compared to photoconsistency. Moreover, evaluation metrics before and after
graph optimization, demonstrate the importance of the optimization, as it im-
proves the mean IoU by 14.17% and the mean recall by 44.57%. As explained
above, the method presented in [25, 26] is equivalent to the ﬁrst step of our
method, thus showing improved performance of our presented framework over
all published baselines. Integrating a voxel graph cut optimization, propagating
change to color-consistent regions [33], leads to better recall rates, but lower IoU,
as change is in some cases overpropagated, resulting in low precision, an thus
failure of discovering the objects, in terms of IoU.

Concerning the ablation, as denoted by the results of the MASK-RCNN
baseline, adding a semantic component is not improving the overall performance.
The MASK-RCNN baseline is capable of achieving a mean IoU of 52.96% and a
mean recall of 89.22%. This can be attributed to noisy RGB-D images, leading
to inaccurate segmentations. Indeed, background patches are falsely detected as
foreground objects. Thus, change is propagated into a large percentage of the
scene’s background, leading to a higher recall rate, compared with a relatively
low precision, and thus IoU. The solution using the ground truth transformation
(baseline ground truth transforms.) is in close proximity with our method in
terms of recall. Even a coarser estimation of the rigid transformation of the
scene is capable of achieving close to the best possible results. However, there is
still space for improvement, regarding the computation of transformations. The
mean IoU of 72.40% in this baseline, is explained due to initial false detections,
caused by occlusions and misalignments between scans. The experimental results
indicate that the two scans need to be correctly registered to avoid false initial
detections. False initial detections are merged with correctly estimated regions,
reducing the IoU score. Finally, it is worth mentioning that using only non-
static parts discovered by RANSAC iterations leads to results worse than our
solution before graph cuts optimization. This explicitly demonstrates that the
straightforward approach of feature matching and computing sets of motion-
consistent points is insuﬃcient.

14

A. Adam et al.

Fig. 5. A moved couch between the reference scan (a) and the rescan (b) that is not
part of the ground truth annotations. Overlaid scans in (c)

Finally, 3RScan is a dataset built towards assessing object instance relocal-
ization and not exhaustive change detection. Thus, our method uncovers changes
between the scans not recorded in the ground truth. Such cases would aﬀect the
evaluation metrics, and we wanted to check their extent. We randomly selected a
subset of 10 rescans and visually inspected them. In 60.00% of cases, we discov-
ered an unrecorded change (see, for example, Figure 5). The proposed approach
has correctly detected 66,67% of these cases. Moreover, an example of a non-rigid
and not recorded change is depicted in Figure 4.

Limitations. By deﬁnition of discovering objects via change detection, we will
miss objects that do not undergo a substantial enough change. Using a stricter
threshold for distinguishing between inliers and outliers in the RANSAC scheme
could help recover even small motions. Moreover, depth map subtraction could
lead to false initial change detection when the two scans are not entirely aligned.
A typical example is illustrated in the second row of Figure 3. Parts of the ﬂoor
are labeled as changing, forming a 3D object due to the scan’s misalignment.

5 Conclusion

The presented method achieves state-of-the-art performance on the object dis-
covery task, via change detection on an object-level basis, for the 3RScan dataset
against a competitive set of baselines. The method shows the surprising eﬀective-
ness of using scene change for high-recall object discovery and of using motion
constraints to achieve precise detections. The very general assumption that ob-
jects are connected and move in a coherent way is used to propagate initial
detections. Importantly, these geometric cues can be discovered directly from
unannotated data, so they do not introduce strong priors or any memorization
of what objects are.

Acknowledgment. This research was supported by projects EU RDF IM-
PACT No. CZ.02.1.01/0.0/0.0/15 003/0000468, EU H2020 ARtwin No. 856994
and the EU Horizon 2020 project RICAIP (grant agreement No 857306).

(a)  Re-scan(b)  Reference scan(c)  Overlaid scansAppendix

This is the appendix for our paper “Objects can move: 3D Change Detection by
Geometric Transformation Consistency”. In Section A, we discuss more quanti-
tative results for the task of 3D object discovery, and provide a more thorough
investigation of the calculated transformations. In Section B, more visual results
of our method and the baselines are shown. We showcase corner cases where our
output is inconsistent with the dataset’s ground-truth annotation. We discuss
in Section C why 3RScan [37] is the most appropriate dataset for evaluating 3D
change detection and 3D object discovery, and we also evaluate our results on
the sub-task of discovering added objects (Section D).

A Quantitative Results

Accuracy of the Computed Transformations. As explained in the main
paper, the computation of the transformations induced by moving objects con-
stitutes an essential component of the proposed method. We extract DGCNN
features [38] and establish correspondences on the whole scene based on nearest
neighbor search. Transformations are then computed using an iterative RANSAC
procedure [5]. We further evaluate the accuracy of the rigid transformations
with respect to the ground-truth ones. Results shown in Table 1, are evaluated
in terms of recall, capturing the percentage of correctly calculated transforma-
tions. We also provide the Mean Translation (MTE) and Mean Rotation (MRE)
Error, for all the correctly retrieved 3D rigid transforms. As in [37], we consider
a transformation as successfully calculated, if the alignment errors for the trans-
lation t∆ and rotation R∆ are lower than 10cm, 10◦ and 20cm, 20◦ respectively.
We compare our approach with handcrafted and learned descriptors for es-
tablishing correspondences. The baseline methods follow the object instance re-
localization protocol of the 3RScan dataset [37]: having access to an instance
segmentation of the reference scan, they only need to ﬁnd correspondences for
3D parts belonging to each instance. In contrast, our method does not use this
supervisory signal and performs full matching between the scenes. Thus, the
included baselines have access to more information compared to our approach.
As expected, the baselines estimate more precise transformations. However, our
results show that our approach is still competitive with such strong baselines.

Ablation of the k Transformations Used in the Optimization. One of the
tunable hyperparameters of our method is the number of top k transformations
T used to propagate changes during the optimization. Indeed, sometimes wrong
transformations with few inliers that are caused by imperfect correspondences
are established. We thus ablate the top k transformations with the most inliers
to propagate the change to neighboring regions. Table 2 shows the results for
the proposed method when k = 5, 10, 15 transformations are used. As expected,

16

Table 1. Transformation evaluation via the percentage of poses within given error
bounds on the position and orientation error (Recall), the Median Translation Error
(MTE) (in meters), and the Median Rotation Error (MRE) (in degrees). MRE and
MTE are provided for the correctly retrieved transformations, i.e., when the alignment
errors for the translation t∆ and rotation R∆ are lower than 10cm, 10◦ and 20cm, 20◦
respectively

Method

FPFH [31]
SHOT [35]
3D-match
dynamic [41]
RIO static [37]
RIO
dynamic [37]
Our method

Recall
(<0.10m,
10◦)
2.61
6.79
5.48

9.92
15.14

3.58

MRE(deg) MTE(m)

Recall
(<0.20m,
20◦)
8.36
12.27
13.05

17.75
23.76

MRE(deg) MTE(m)

10.57
8.18
7.30

6.39
6.08

4.25

0.0776
0.0393
0.0708

0.0545
0.0547

0.1381

0.0645
0.0268
0.0542

0.0425
0.0437

7.25
5.35
5.81

4.33
4.75

3.00

0.0799

18.21

using a larger k increases the mean recall (i.e., the percentage of correctly re-
trieved objects) and decreases the mean IoU, as in some cases, the change leads
to oversegmentation. However, there is no substantial diﬀerence in the overall
performance of the proposed method correlated with k. This validates the ro-
bustness of our approach.

Table 2. Mean IoU and mean recall for the proposed method using diﬀerent number
of computed transformations k to propagate geometrical consistency

Number of trans.

5
10
15

IoU(%)

68.40%
64.89%
65.50%

Recall(%)

76.05%
77.43%
79.09%

Table 3. Components of each presented method

Method

Palazzolo et al. [26] /Ours bf optim.
Taneja et al. [33]
Our method

Init.
Detect.
(cid:88)
(cid:88)
(cid:88)

Comp.
Transf.

Optim.

(cid:88)

(cid:88)
(cid:88)

Accuracy and Completeness of the 3D Discovered Objects. Towards
assessing 3D object discovery, we also deploy the metrics of accuracy and com-

Table 4. Mean accuracy and mean completeness for the proposed method and the
published baselines

Appendix

17

Method

Palazzolo et al. [26] /Ours bf optim.
Taneja et al. [33]
Our method

Acc.(%)

67.76 %
65.97%
54.60%

Compl.(%)

33.39%
30.53%
59.20%

Table 5. Mean accuracy and mean completeness for the proposed method and the
ablation study’s baselines

Method

Acc.(%)

Compl.(%)

Our method
Ground-truth transforms.
Mask-RCNN
RANSAC inliers

54.60%
48.79%
37.43%
52.47%

59.20%
74.54%
71.39%
14.73%

pleteness (on the point level). Per object accuracy refers to the percentage of
correctly predicted 3D points out of all the points forming our discovered ob-
ject. On the other hand, completeness captures how many of the ground-truth
object’s points were correctly retrieved by our solution. Tables 4 and 5 show the
mean accuracy and mean completeness for all objects. After close examination,
it is clear that our proposed method balances the most between the two metrics
when compared with the two published baselines (Palazzolo et al., Taneja et al.).
Table 3 summarizes the diﬀerent components of each published baselines. Con-
cerning the ablation baseline, having access to the ground-truth transformations
would slightly improve the overall performance. On the other hand, assigning
semantics masks to many parts of the scene (Mask-RCNN), i.e., labeling most
of the scene as foreground objects, results in a high completeness rate. However,
it also leads to poor accuracy as change is wrongly propagated to static regions
with the same label, starting from wrong initial change detections.

B Qualitative Results

In the following, initial detections of changing regions are depicted along with
the graph cut optimization results and ground-truth annotations. Corner cases
are also discussed.

Initial Detection Results. Results of the initially discovered changing regions
are depicted in Figures 1, 2, and 3. It is important to note here that in most cases,
the rescans constitute only partial observations of the reference scans; thus, the
marked changing regions refer only to the parts visible in the rescan. After close
inspection of Figures 1, 2, and 3, it becomes clear that in most cases, our initial
detection stage eﬃciently retrieves the changing regions. Wrong detections are
primarily attributed to slight misalignments between the reference scan and the
rescan.

18

Results after graph cut Optimization. After graph cut optimization, a 3D
connected component analysis is applied both to the ground-truth annotations
and to the results of our approach. This step aims to turn our detected changing
regions into the ﬁnal discovered objects. Figures 4, 5, and 6 present the ﬁnal
output of the proposed method.

Corner Cases. We also present cases where our results conﬂict with the anno-
tations provided by the dataset. This could be partially attributed to unrecorded
changes in the ground-truth. 3RScan [37] is a dataset built towards assessing ob-
ject instance re-localization and thus does not exhaustively record every change.
Typical examples are shown in Figures 6 and 9, where the non-rigid change of
the curtain is not recorded. However, there are also cases where rigid transfor-
mations are not included in the ground-truth annotations (cf. the refrigerator in
Figure 9). In all the above-mentioned cases, the proposed method successfully
detected these changes.

The main limitation of our method is handling misalignments between scans.
Indeed, diﬀerences in voxel occupancy and depth values occur when the reference
scan and the rescan are not correctly registered. Thus, regions are falsely labeled
as changing during our method’s initial detection step. A typical example is
Figure 8, where parts of the ﬂoor are wrongly retrieved as changing due to
the misalignment. Post-processing steps such as matching against a 3D object
database, can eliminate these false detections.

Baseline Comparisons. As stated in the Section 4 of the main paper, we com-
pare our method against two published baselines, Palazzolo et al. [26] and Taneja
et al. [33]. We also compare against three baselines in the form of an ablation
study (Ground-truth transforms, Mask-RCNN, RANSAC Inliers). Qualitative
results for the two published baselines and the ablation baselines (Ground-truth
transforms and Mask-RCNN) are depicted in Figure 7.

The main paper explains that Palazzolo et al. [26] is equivalent to our method
before the optimization step, i.e., it predicts change through depth comparison.
The depicted visual results prove the need for a more sophisticated solution
rather than simply relying on inconsistencies between 2D projections. On the
other hand, Taneja et al. [33] performs a graph cut optimization, where the
binary term ensures photoconsistency. This constraint seems to perform well on
objects with homogeneous texture, such as the chair in Figure 7, but fails when
the same object has multiple textures (cf. the cabinet in Figure 7).

Moving on to the ablation study, the baseline using the ground-truth trans-
formations provided by the dataset seems to work better than the presented
method. This is expected since our method ﬁrstly computes transformations
and then discovers objects. Thus, for the proposed method, the uncertainty of
the calculated transformations is propagated to the results of the graph opti-
mization step. Finally, it is evident that when Mask-RCNN is used, a lot of
background regions are labelled as changing, since they were wrongly extracted
as foreground in the RGB-D images (i.e., they were incorrectly assigned a se-
mantic mask). Moreover, it completely fails in regions that were not detected as
a foreground object (cf. cabinet in Figure 7 ), due to the lack of relevant training

Appendix

19

Table 6. Applicable datasets for change detection/ object discovery

Method

Scans

Rescans

Instance
Seg.

Annotation Available

Finman et al.[10]
Langer et al.[22]
Katsura et al.[18]
Herbst et al.[15]
Mason et al.[24]
Ambrus et al.[1]
Fehr et al.[9]
Wald et al.[37] -
3RScan
Halber et al.[13]
Langer et al.[21]

2
1
2
4
1
1
3
478

13
5

67
4
10+?
24
67
88
23
1482

45
31

?
—
?
(cid:88)
—
—
—
(cid:88)

(cid:88)
—

?
—
?
(cid:88)
—
—3
—
(cid:88)

—
(cid:88)

—
—
—
—1
(cid:88)2
(cid:88)
(cid:88)
(cid:88)4

(cid:88)
(cid:88)

data for these concepts. In contrast, our method does not rely on any predeﬁned
notion of what an object should look like.

C Datasets

The 3RScan dataset [37] is a dataset built towards assessing object instance re-
localization under rigid transformations. Thus, the dataset provides information
about the transformations induced by moving objects, along with an instance
segmentation of each scene. It also provides information about non-rigid changes
in some instances. Hence, we generate the ground-truth annotations by (i) ex-
tracting all the information about rigid and non-rigid movements as provided
by the dataset, and (ii) by comparing the instance segmentation of the reference
scan and rescan to discover objects that have been added or removed.

As stated above, even though 3RScan is not directly designed for 3D change
detection/ 3D object discovery, ground-truth information can be generated with-
out manual labeling. It is also a very large and diverse dataset, and thus, to the
best of our knowledge, the most appropriate dataset for our needs. Table 6
summarizes the characteristics of the adopted dataset, against other applicable
datasets that were not suitable for our scenario.

As shown in Table 6, there is no widely available dataset appropriate for
evaluating 3D indoor change detection and 3D object discovery. Each work eval-
uates its eﬃciency on tailor-made datasets, some of which [10, 22, 18, 15] are not
publicly available. Concerning the public datasets, [24] provides a relative large
amount of rescans. However, only a single environment is considered. Moreover,
its objects are not annotated so it cannot be used for quantitative evaluation.

1 The provided URL is not valid
2 Data available upon request
3 Inconsistent and incomplete annotation
4 With our provided code

20

Similarly, [1] consists of data captured by a robot in an oﬃce setting. The diver-
sity of the provided scenes is thus limited. The annotation is mostly inconsistent,
since some selected objects are annotated as new while other objects that are
physically new in a scene are not. In a similar vein to our used dataset [37], [9]
uses a hand-held Google Tango device to capture three rooms (reference scans)
and 23 rescans. Taking into account that the dataset is much smaller and less
diverse and its complete lack of annotations, we have decided not to use it. The
authors of [21] have created their own dataset, for evaluating added small objects
(from the YCB dataset [11]) in the scene. [22] provides ground-truth annotation
only for novel objects and not for moved ones, which makes the quantitative
evaluation of our task hard as not all the cases we are interested in can be
directly tested. Finally, [13] aims at tracking instance segmentation across tem-
poral changes. Thus, a ground-truth instance annotation is provided for every
rescan, but no annotations concerning moving/static objects.

D Novel Objects

Novel objects are also of broad interest to multiple robotic applications. Since
existing works in the research community [22, 21] focus on novel added objects,
we have decided to create a subtask of discovering all added objects in the scene.
To this end, we have prepossessed 3RScan to create a new ground truth including
only the novel objects, and we also decided to compare our algorithm against [22].
Originally, [22] detects only small objects on the ﬂoor. We modiﬁed this work to
discover objects regardless of their size and position for a fair comparison. Table 7
shows the results in terms of IoU and recall at the voxel level, to capture the
volumetric overlap between ground truth and predictions. After close inspection
of Tab. 7, it is clear that our method outperforms the most competitive baseline
in the sub-task of discovering added objects.

Table 7. Metrics of the proposed method and [22] on novel objects of the 3RScan
dataset

Method

[22]
Our method

IoU(%)

73.85%
75.08%

Recall(%)

64.25%
76.70%

Appendix

21

Fig. 1. Reference scans (a) and re-scans (b) of multiple scenes with highlighted occurred
changes (T1, T2, etc. refer to the ground-truth transformations between moving ob-
jects). Overlaid meshes of reference scan and rescan in red and blue (c). Ground-truth
changed regions of the point clouds (in red) overlaid on the reference scan (in blue) (d).
Initial change detection results of the point clouds (in red) overlaid on the reference
scan (in blue) (e)

T1T2(a) Reference scan(b) RescanT3non-rigid(c) Overlaid scans(d) Ground-truth annotations             (e) Initial detections22

Fig. 2. Reference scans (a) and re-scans (b) of multiple scenes with highlighted oc-
curred changes (T1, T2, etc. refer to the ground-truth transformations between moved
objects). Overlaid meshes of reference scan and rescan in red and blue (c). Ground-
truth changed regions of the point clouds (in red) overlaid on the reference scan (in
blue) (d). Initial change detection results of the point clouds (in red) overlaid on the
reference scan (in blue) (e)

(a) Reference scan(b) RescanT1T2T3T4T5added(c) Overlaid scans(d) Ground-truth annotations             (e) Initial detectionsAppendix

23

Fig. 3. Reference scans (a) and re-scans (b) of multiple scenes with highlighted occurred
changes (T1, T2, etc.. refer to the ground-truth transformations between moved ob-
jects). Overlaid meshes of reference scan and rescan in red and blue (c). Ground-truth
changed regions of the point clouds (in red) overlaid on the reference scan (in blue) (d).
Initial change detection results of the point clouds (in red) overlaid on the reference
scan (in blue) (e)

(a) Reference scan(b) RescanT2T3addedaddedT4non-rigidT1(c) Overlaid scans(d) Ground-truth annotations             (e) Initial detections24

Fig. 4. The meshes of the reference scan (a) and the rescan (b). Ground-truth instance
segmentation of the point cloud of the rescan (c). Ground-truth connected components
of the point cloud (d), compared with the connected components of our solution in (e)

(a) Reference scan(b) RescanT1T2T3non-rigidnon-rigid(d) Ground-truth connected components(e) Our connected components(c) Ground-truth instance annotationAppendix

25

Fig. 5. The meshes of the reference scan (a) and the rescan (b). Ground-truth instance
segmentation of the point cloud of the rescan (c). Ground-truth connected components
of the point cloud (d), compared with the connected components of our solution in (e)

(a) Reference scan(b) RescanT1T2T3T4T5added(d) Ground-truth connected components(e) Our connected components(c) Ground-truth instance annotation26

Fig. 6. The meshes of the reference scan (a) and the rescan (b). Ground-truth instance
segmentation of the point cloud of the rescan (c). Ground-truth connected components
of the point cloud (d), compared with the connected components of our solution in (e)

(a) Reference scan(b) RescanT2T3addedaddedT4non-rigidT1(d) Ground-truth connected components(e) Our connected components(c) Ground-truth instance annotationAppendix

27

Fig. 7. The meshes of the reference scan (a) and the rescan (b). Ground-truth solution
(c). Results of the published baselines in (d) and (e). Results of the proposed method
in (f). Results of the ablation study’s baselines in (g) and (h)

(a) Reference scan(b) RescanT1T2T3non-rigidnon-rigid(d) Palazzolo et al.(c) Ground-truth solution(e) Taneja et al.(f) Our Methodnon-rigid(h) Mask-RCNN(g) Ground-truth transf.28

Fig. 8. Slight misalignment between reference scan (a) and rescan (b). Overlaid refer-
ence scan and rescan (depicted in blue and red respectively) in (c). Initial detection
results in (d)

Fig. 9. Changes not recorded in the ground-truth. The mesh of the reference scan is
depicted in (a), and the green bounding boxes underline unrecorded changes. The mesh
of the rescan in (b). Recorded changes in the ground-truth are underlined in red color.
The meshes of the two scans overlaid in red and blue color, respectively (c). Ground-
truth annotations (in red) overlaid on the point cloud of the reference scan in blue
(d), and our graph cut optimization results (in red) overlaid on the point cloud of the
reference scan in blue (e). The rigid change of the refrigerator and the non-rigid change
of the curtain that were not recorded in the ground-truth are successfully detected by
our method

(a) Reference scan(d) Initial detection(b) Rescan(c) Overlaid scans(a) Reference scan(b) Re-scanaddedaddedT1(c) Reference scan and Re-scan(d) Ground-truth(e) Final solutionAppendix

29

References

1. Ambrus, R., Folkesson, J., Jensfelt, P.: Unsupervised object segmentation through
change detection in a long term autonomy scenario. In: 2016 IEEE-RAS 16th
International Conference on Humanoid Robots (Humanoids). pp. 1181–1187. IEEE
(2016)

2. Armeni, I., Sener, O., Zamir, A.R., Jiang, H., Brilakis, I., Fischer, M., Savarese,
S.: 3d semantic parsing of large-scale indoor spaces. In: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition. pp. 1534–1543 (2016)
3. Bai, X., Luo, Z., Zhou, L., Chen, H., Li, L., Hu, Z., Fu, H., Tai, C.L.: Pointdsc:
Robust point cloud registration using deep spatial consistency. In: Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.
15859–15869 (2021)

4. Barron, J.T.: A generalization of otsu’s method and minimum error thresholding.

In: European Conference on Computer Vision. pp. 455–470. Springer (2020)

5. Bolles, R.C., Fischler, M.A.: A ransac-based approach to model ﬁtting and its
application to ﬁnding cylinders in range data. In: IJCAI. vol. 1981, pp. 637–643.
Citeseer (1981)

6. Boykov, Y., Kolmogorov, V.: An experimental comparison of min-cut/max-ﬂow
algorithms for energy minimization in vision. IEEE transactions on pattern analysis
and machine intelligence 26(9), 1124–1137 (2004)

7. Brasch, N., Bozic, A., Lallemand, J., Tombari, F.: Semantic monocular slam for
highly dynamic environments. In: 2018 IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS). pp. 393–400. IEEE (2018)

8. Cui, L., Ma, C.: Sof-slam: A semantic visual slam for dynamic environments. IEEE

Access 7, 166528–166539 (2019)

9. Fehr, M., Furrer, F., Dryanovski, I., Sturm, J., Gilitschenski, I., Siegwart, R., Ca-
dena, C.: Tsdf-based change detection for consistent long-term dense reconstruc-
tion and dynamic object discovery. In: 2017 IEEE International Conference on
Robotics and automation (ICRA). pp. 5237–5244. IEEE (2017)

10. Finman, R., Whelan, T., Kaess, M., Leonard, J.J.: Toward lifelong object segmen-
tation from change detection in dense rgb-d maps. In: 2013 European Conference
on Mobile Robots. pp. 178–185. IEEE (2013)

11. Gadde, R., Jampani, V., Marlet, R., Gehler, P.V.: Eﬃcient 2d and 3d facade seg-
mentation using auto-context. IEEE transactions on pattern analysis and machine
intelligence 40(5), 1273–1280 (2017)

12. Glocker, B., Izadi, S., Shotton, J., Criminisi, A.: Real-time rgb-d camera relocal-
ization. In: 2013 IEEE International Symposium on Mixed and Augmented Reality
(ISMAR). pp. 173–179. IEEE (2013)

13. Halber, M., Shi, Y., Xu, K., Funkhouser, T.: Rescan: Inductive instance segmenta-
tion for indoor rgbd scans. In: Proceedings of the IEEE/CVF International Con-
ference on Computer Vision. pp. 2541–2550 (2019)

14. He, K., Gkioxari, G., Doll´ar, P., Girshick, R.: Mask r-cnn. In: Proceedings of the

IEEE international conference on computer vision. pp. 2961–2969 (2017)

15. Herbst, E., Henry, P., Ren, X., Fox, D.: Toward object discovery and modeling via
3-d scene comparison. In: 2011 IEEE International Conference on Robotics and
Automation. pp. 2623–2629. IEEE (2011)

16. Johnson, J., Douze, M., J´egou, H.: Billion-scale similarity search with gpus. arXiv

preprint arXiv:1702.08734 (2017)

30

17. Karpathy, A., Miller, S., Fei-Fei, L.: Object discovery in 3d scenes via shape anal-
ysis. In: 2013 IEEE International Conference on Robotics and Automation. pp.
2088–2095. IEEE (2013)

18. Katsura, U., Matsumoto, K., Kawamura, A., Ishigami, T., Okada, T., Kurazume,
R.: Spatial change detection using normal distributions transform. ROBOMECH
Journal 6(1), 1–13 (2019)

19. Kullback, S., Leibler, R.A.: On information and suﬃciency. The annals of mathe-

matical statistics 22(1), 79–86 (1951)

20. Landrieu, L., Obozinski, G.: Cut pursuit: Fast algorithms to learn piecewise con-
stant functions on general weighted graphs. SIAM Journal on Imaging Sciences
10(4), 1724–1766 (2017)

21. Langer, E., Patten, T., Vincze, M.: Robust and eﬃcient object change detection by
combining global semantic information and local geometric veriﬁcation. In: 2020
IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS).
pp. 8453–8460. IEEE (2020)

22. Langer, E., Ridder, B., Cashmore, M., Magazzeni, D., Zillich, M., Vincze, M.:
On-the-ﬂy detection of novel objects in indoor environments. In: 2017 IEEE Inter-
national Conference on Robotics and Biomimetics (ROBIO). pp. 900–907. IEEE
(2017)

23. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll´ar, P.,
Zitnick, C.L.: Microsoft coco: Common objects in context. In: European conference
on computer vision. pp. 740–755. Springer (2014)

24. Mason, J., Marthi, B.: An object-based semantic world model for long-term change
detection and semantic querying. In: 2012 IEEE/RSJ International Conference on
Intelligent Robots and Systems. pp. 3851–3858. IEEE (2012)

25. Palazzolo, E., Stachniss, C.: Change detection in 3d models based on camera im-
ages. In: 9th Workshop on Planning, Perception and Navigation for Intelligent
Vehicles at the IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS)
(2017)

26. Palazzolo, E., Stachniss, C.: Fast image-based geometric change detection given a
3d model. In: 2018 IEEE International Conference on Robotics and Automation
(ICRA). pp. 6308–6315. IEEE (2018)

27. Palma, G., Cignoni, P., Boubekeur, T., Scopigno, R.: Detection of geometric tem-
poral changes in point clouds. In: Computer Graphics Forum. vol. 35, pp. 33–45.
Wiley Online Library (2016)

28. Papon, J., Abramov, A., Schoeler, M., Worgotter, F.: Voxel cloud connectivity
segmentation-supervoxels for point clouds. In: Proceedings of the IEEE conference
on computer vision and pattern recognition. pp. 2027–2034 (2013)

29. Phan, A.V., Le Nguyen, M., Nguyen, Y.L.H., Bui, L.T.: Dgcnn: A convolutional
neural network over large-scale labeled graphs. Neural Networks 108, 533–543
(2018)

30. Runz, M., Buﬃer, M., Agapito, L.: Maskfusion: Real-time recognition, tracking and
reconstruction of multiple moving objects. In: 2018 IEEE International Symposium
on Mixed and Augmented Reality (ISMAR). pp. 10–20. IEEE (2018)

31. Rusu, R.B., Blodow, N., Beetz, M.: Fast point feature histograms (fpfh) for 3d
registration. In: 2009 IEEE international conference on robotics and automation.
pp. 3212–3217. IEEE (2009)

32. Steinhauser, D., Ruepp, O., Burschka, D.: Motion segmentation and scene clas-
siﬁcation from 3d lidar data. In: 2008 IEEE Intelligent Vehicles Symposium. pp.
398–403. IEEE (2008)

Appendix

31

33. Taneja, A., Ballan, L., Pollefeys, M.: Image based detection of geometric changes
in urban environments. In: 2011 International Conference on Computer Vision. pp.
2336–2343. IEEE (2011)

34. Taneja, A., Ballan, L., Pollefeys, M.: City-scale change detection in cadastral 3d
models using images. In: Proceedings of the IEEE Conference on computer Vision
and Pattern Recognition. pp. 113–120 (2013)

35. Tombari, F., Salti, S., Di Stefano, L.: Unique signatures of histograms for local
surface description. In: European conference on computer vision. pp. 356–369.
Springer (2010)

36. Ulusoy, A.O., Mundy, J.L.: Image-based 4-d reconstruction using 3-d change de-
tection. In: European Conference on Computer Vision. pp. 31–45. Springer (2014)
37. Wald, J., Avetisyan, A., Navab, N., Tombari, F., Nießner, M.: Rio: 3d object
instance re-localization in changing indoor environments. In: Proceedings of the
IEEE/CVF International Conference on Computer Vision. pp. 7658–7667 (2019)
38. Wang, Y., Sun, Y., Liu, Z., Sarma, S.E., Bronstein, M.M., Solomon, J.M.: Dynamic
graph cnn for learning on point clouds. Acm Transactions On Graphics (tog) 38(5),
1–12 (2019)

39. Wong, Y.S., Li, C., Niessner, M., Mitra, N.J.: Rigidfusion: Rgb-d scene reconstruc-

tion with rigidly-moving objects. Computer Graphics Forum 40(2) (2021)

40. Xiao, W., Vallet, B., Paparoditis, N.: Change detection in 3d point clouds acquired
by a mobile mapping system. ISPRS Annals of Photogrammetry, Remote Sensing
and Spatial Information Sciences 1(2), 331–336 (2013)

41. Zeng, A., Song, S., Nießner, M., Fisher, M., Xiao, J., Funkhouser, T.: 3dmatch:
Learning local geometric descriptors from rgb-d reconstructions. In: Proceedings
of the IEEE conference on computer vision and pattern recognition. pp. 1802–1811
(2017)

