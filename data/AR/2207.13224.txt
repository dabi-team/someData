2
2
0
2

l
u
J

7
2

]

O
R
.
s
c
[

1
v
4
2
2
3
1
.
7
0
2
2
:
v
i
X
r
a

PI-ARS: Accelerating Evolution-Learned Visual-Locomotion with
Predictive Information Representations

Kuang-Huei Lee‚àó1 OÔ¨År Nachum‚àó1

Tingnan Zhang1

Sergio Guadarrama1

Jie Tan1 Wenhao Yu1

Abstract‚Äî Evolution Strategy (ES) algorithms have shown
promising results in training complex robotic control policies
due to their massive parallelism capability, simple implemen-
tation, effective parameter-space exploration, and fast train-
ing time. However, a key limitation of ES is its scalability
to large capacity models, including modern neural network
architectures. In this work, we develop Predictive Information
Augmented Random Search (PI-ARS) to mitigate this limitation
by leveraging recent advancements in representation learning
to reduce the parameter search space for ES. Namely, PI-ARS
combines a gradient-based representation learning technique,
Predictive Information (PI), with a gradient-free ES algorithm,
to train policies that
Augmented Random Search (ARS),
can process complex robot sensory inputs and handle highly
nonlinear robot dynamics. We evaluate PI-ARS on a set of
challenging visual-locomotion tasks where a quadruped robot
needs to walk on uneven stepping stones, quincuncial piles, and
moving platforms, as well as to complete an indoor navigation
task. Across all tasks, PI-ARS demonstrates signiÔ¨Åcantly better
learning efÔ¨Åciency and performance compared to the ARS
baseline. We further validate our algorithm by demonstrating
that the learned policies can successfully transfer to a real
quadruped robot, for example, achieving a 100% success rate
on the real-world stepping stone environment, dramatically
improving prior results achieving 40% success.

I. INTRODUCTION

Evolution Strategy (ES) optimization techniques have re-
ceived increasing interest in recent years within the robotics
and deep reinforcement learning (DRL) communities [1],
[2], [3], [4], [5], [6], [7]. ES algorithms have been shown
to be competitive alternatives to commonly used gradient-
based DRL algorithms such as PPO [8] and SAC [9], while
also enjoying the beneÔ¨Åts of massive parallelism, simple
implementation, effective parameter-space exploration, and
faster training time [1].

Despite the promising progress of ES algorithms, they nev-
ertheless exhibit key limitations when compared to gradient-
based DRL algorithms. Namely, unlike gradient-based meth-
ods, ES methods scale poorly to high-dimensional search
spaces, commonly encountered when using high-capacity
modern neural network architectures [10], [1]. An important
and exemplary task is visual-locomotion [3], in which a
legged robot relies on its vision input to decide where to
precisely place its feet to navigate uneven terrains. Due to
the rich and diverse sensor observations as well as complex

*Equal contribution
1All authors are with Google Research,

1600 Amphitheatre Parkway Mountain View, CA 94043, United States
{leekh, ofirnachum, tingnan, sguada, jietan,
magicmelon}@google.com

The supplementary video is available at kuanghuei.github.io/piars

robot dynamics, learning such a task requires the use of deep
convolutional neural networks (CNNs) with a large num-
ber of learnable parameters, thus exacerbating the sample-
complexity of ES methods.

In this paper, we develop Predictive Information Aug-
mented Random Search (PI-ARS) to relieve this key bot-
tleneck of ES algorithms. Our key insight is to leverage the
power of gradient-based and gradient-free learning together
by modularizing the learning agent into two components: (1)
an encoder network mapping high-dimensional and diverse
observation inputs to a concise Ô¨Åxed-length vector represen-
tation, and (2) a smaller policy network that maps the com-
pressed representations to actions. For (1), we leverage the
power of gradient-based learning, and use a self-supervised
objective based on maximizing the predictive information
(PI) of the output representation, inspired by previous work
in representation learning [11], [12], [13], [14], [15], [16].
Meanwhile for (2), we leverage the simplicity and paralleliz-
ability of the ES optimization method Augmented Random
Search (ARS) [2]. By decoupling representation learning
from policy optimization in this way, we avoid scalability
issues while fully leveraging the advantages of ES methods.
We evaluate our proposed PI-ARS algorithm on a variety
of visual-locomotion tasks, both in simulation and on a
quadruped robot. Among these tasks, the robot is evaluated
on its ability to walk on uneven stepping stones, quincuncial
piles, and moving platforms, as well as to complete an
indoor navigation task (Figure 2). Through extensive exper-
imentation in simulation, we Ô¨Ånd that PI-ARS signiÔ¨Åcantly
outperforms the baselines (ARS [3], SAC [9], PI-SAC [13]),
both in training speed and Ô¨Ånal performance. We further
validate the results by deploying the learned policies on
a real quadruped robot. Using the same physical setup as
prior work [3], PI-ARS learns more robust policies that
can consistently Ô¨Ånish the entire course of stepping stones,
achieving 100% success over 10 real-robot trials, compared
to 40% success rate of prior work. We observe similarly
successful robustness to real-world transfer for the indoor
navigation policy.

In summary, the contributions of this paper are the fol-

lowing:

1) We propose a new PI-ARS algorithm that combines
the advantages of gradient-based self-supervised rep-
resentation learning and gradient-free policy learning,
thus solving a key bottleneck of ES algorithms.

2) We apply PI-ARS in visual-locomotion tasks, which
signiÔ¨Åcantly improve the state-of-the-art [3] both in
simulation and in the real world.

 
 
 
 
 
 
In PI-ARS, an observation encoder œÜ is used to provide a compressed observation representation. This way, ARS learning is focused on a much
Fig. 1.
smaller neural network than if the whole policy (mapping robot observations to actions) were learned end-to-end. As ARS uses sampled trajectories from
the policy to update its parameters, PI-ARS uses the same trajectories to maintain a replay buffer for training œÜ via predictive information (PI), which
includes auxiliary-learned networks as described in Section III-B.

II. RELATED WORK

A. Evolution Strategy for RL

There have been numerous works that demonstrate the
effectiveness of applying Evolution Strategy (ES) algorithms
to continuous control problems [17], [1], [2]. For example,
Tan et al. applied Neural Evolution of Augmenting Topolo-
gies (NEAT) to optimize a character to perform bicycle
stunts [17]. Within the Ô¨Åeld of deep reinforcement learning,
Salimans et al. Ô¨Årst demonstrated that ES algorithm can
be applied to train successful neural-network policies for
the OpenAI Gym control tasks [1]. Mania et al. introduced
Augmented Random Search (ARS), a simple yet effective
ES algorithm that further improves the learning efÔ¨Åciency
for robotic control tasks [2].

Compared to gradient-based RL algorithms, ES algorithms
can handle non-differentiable dynamics and objective func-
tions, explore effectively with sparse or delayed rewards, and
are naturally parallelizable. As such, researchers have applied
ES algorithms in a variety of applications such as legged
locomotion [3], [18], power grid control [7], and mixed
autonomy trafÔ¨Åc [19]. However, as ES algorithms do not
leverage backpropagation, they suffer from low sample efÔ¨Å-
ciency and may not scale well to complex high-dimensional
problems [1]. As a result, applying ES to learn vision-based
robotic control policies is rarely explored [3].

B. Predictive Representations for RL

Our work relies on learning representations that are pre-
dictive of future events. Prior work has shown beneÔ¨Åt from
having good models of the past and future states [20], [21],
[22]. More recently, using these principles to guide state
representation learning methods has been demonstrated to
yield favorable performance both in practice [13], [15] and
in theory [23], [24]. A natural approach to learning such
representations is using generative models to explicitly pre-
dict observations [12], [25], which could be challenging and
expensive for high-dimensional tasks. Alternatively, using
variational forms [26] of the predictive information [27],
commonly leading to contrastive objectives, makes learning

such representations more tractable [13], [15], [11]. In this
work, we take the contrastive approach to learn predictive
representations of the observed states, upon which we learn
an optimal policy with augmented random search (ARS) [2],
an ES method. To the best of our knowledge, the proposed
learning system is the Ô¨Årst to take such a combined approach,
and the Ô¨Årst to apply predictive information representations
on visual locomotion tasks with legged robots.

C. Visual-Locomotion for Legged Robots

Visual-locomotion is an important research direction that
has received much attention in the robotics research commu-
nity [28], [29], [30], [31], [32], [33], [34], [35], [36]. Directly
training a visual-locomotion controller is challenging due to
the high dimensional visual input and the highly nonlinear
dynamics of legged robots [3], [37]. Many existing methods
manually and explicitly decouple the problem into more
manageable components including perception [31], [36],
motion planning [32], [38], and whole body control [39],
[40], [41]. In this work, we consider direct learning of visual-
locomotion controllers for quadruped robots as the test-bed
for our proposed learning algorithm and demonstrate that by
combining gradient-free ES and gradient-based representa-
tion learning techniques, we can enable more effective and
efÔ¨Åcient learning of visual-locomotion controllers.

III. METHOD

In this section we describe our method, PI-ARS, which
combines representation learning based on predictive infor-
mation (PI), and Augmented Random Search (ARS) [2]. See
Figure 1 for a diagram of the algorithm and Algorithm 1 for
a pseudocode.

A. Problem Formulation and Notations

PI-ARS solves a sequential decision-making environment
in which at each timestep t the agent is presented with
an observation st. In visual-locomotion,
this observation
(e.g., depth camera
typically includes a visual
images) as well as proprioceptive states sp
t . The agent‚Äôs
policy determines its behavior, by providing a mapping from

input sv
t

ARSPredictive InformationVision obs (sv)Proprio obs (sp)ActionsARS paramsPolicyReplay bufferEnvironment trajectoriesùúôvùúôobservations st to actions at. After application of action at,
the agent receives a reward rt and a new observation st+1.
This process is repeated until the agent is terminated, either
due to a timeout or encountering a terminal condition (e.g.,
the robot falls). The agent‚Äôs return is computed as the sum
of rewards over an entire episode, and the agent‚Äôs goal is to
maximize the expected value of the return.

B. Predictive Information

A good observation encoder for policy learning must
provide representations that are both compressive ‚Äì so that
ARS learning is focused on much fewer parameters than
learning from raw observations would entail ‚Äì and task-
relevant ‚Äì so that ARS has access to all features neces-
sary for learning optimal behavior. To this end, we pro-
pose to learn an encoder œÜ to maximize predictive in-
formation (PI). In general, PI refers to the mutual infor-
mation between past and future I(past; future) [27]. In
our setting involving environment-produced sub-trajectories
œÑ = (st, at, rt, at+1, rt+1, . . . , at+k‚àí1, rt+k‚àí1, st+k), past
corresponds to (st, at, . . . , at+k‚àí1) and future refers to the
both the per-step rewards (rt, . . . , rt+k‚àí1) and the ultimate
visual observation sv

t+j).
t+k to a lower dimensional
representation. Namely, as shown in Figure 1, the observation
encoder œÜ contains a vision encoder œÜv that maps visual
observations sv
t to a 128-d representation. This representation
is subsequently concatenated with proprioceptive states, and
the concatenation is projected to be the output of œÜ, which
is also 128-d. We thus use the entire encoder œÜ to encode st,
while use only the vision encoder œÜv to encode the future
sv
t+k. By learning œÜ to maximize the mutual information
between these past and future, we ensure that œÜ encodes
the necessary information in st to predict the future. Notably,
previous work has shown that representations that are pre-
dictive of the future are also provably beneÔ¨Åcial for solving
the task [23].

We use œÜ to map both st and sv

1; i.e., st+j = (sv

t+j, sp

t+k

To learn œÜ, we use a combination of two objectives,
the Ô¨Årst corresponding to reward prediction and the sec-
ond to sv
t+k prediction.2 For the former, we simply pre-
dict rewards ÀÜrt, . . . , ÀÜrt+k‚àí1 using an RNN over inputs
(œÜ(st), at, . . . , at+k‚àí1). At time t + j, the RNN cell takes
a latent state and an action and outputs a reward prediction
ÀÜrt+j and the next latent state. The reward loss is deÔ¨Åned as

Lr = EœÑ

(cid:20)k‚àí1
(cid:88)

(ÀÜrt+j ‚àí rt+j)2

(cid:21)

.

(1)

j=0

This corresponds to maximizing I(past; future rewards)
with a generative model [42].
For the prediction of sv

t+k, rather than a generative model,
as used for ÀÜrt+j, which would present challenges for high-
dimensional image observations, we leverage InfoNCE, a
contrastive variational bound on mutual information [11],

1This empirical choice of future works well in our setting.
2We note that in our early experiments, we found learning œÜ using reward
prediction alone leads to insufÔ¨Åcient representations for solving the tasks.

[26], [13]. We use an auxiliary learned, scalar-valued func-
tion f to estimate the mutual information using access to
samples of sub-trajectories œÑ = (st, at, . . . , at+k‚àí1, st+k).
SpeciÔ¨Åcally, we conveniently exclude the course of actions
and choose the following form:

I(past; future obs) ‚â• ÀúI(past; future obs) =
(cid:20)

EœÑ

f (œÜ(st), œÜv(sv

t+k))

‚àí log EÀúst+k [exp{f (œÜ(st), œÜv(Àúsv

t+k))}]

(cid:21)

,

(2)

where Àúsv
t+k is from an observation randomly sampled in-
dependent of œÑ . Our objective is then to maximize this
variational form with respect to both œÜ and f .

To parameterize f , we use an MLP to map œÜ(st) to zpast
,
t
a 128-d vector. Meanwhile, we map œÜv(sv
t+k) to zfuture
t+k , a
128-d vector representation of the future, using another MLP.
The function f is then computed as a scalar dot-product of
the two vectors.

We train both the reward objective and the variational
objective using batch samples from a replay buffer of sub-
trajectories collected by ARS. To approximate EÀúst+k in
Equation (2), we use samples of Àúsv
t+k within the same
batch of sub-trajectories. The full objective is optimized
using the Adam stochastic gradient descent optimizer [43]
and the gradient is calculated using back-propagation. Full
implementation details are included in the Appendix.

Algorithm 1 Pseudocode for PI-ARS.

Initialize encoder œÜ, auxiliary networks (RNN, f ), and
optimizer OptœÜ,aux.
Initialize ARS parameters Œ∏ and optimizer OptŒ∏.
Initialize replay buffer B.
for T = 1, . . . do

#### ARS ####
Sample {œÉi}N
i=1 from normal with scale Œ¥.
Collect environment trajectories for policies (œÜ, Œ∏¬±œÉi).
Compute returns Ri,¬± for each policy.
Compute gradient of M best-performing directions:

ÀÜg = Œ¥
M

(cid:80)M

i=1(Ri,+ ‚àí Ri,‚àí)œÉi.

Update Œ∏ w.r.t. gradient ÀÜg and OptŒ∏.
Add trajectories to B.
#### PI ####
Sample batch of sub-trajectories {œÑi}B
for i = 1, . . . , B do

ÀÜLi = ‚àí ÀúI(œÑi) + Lr(œÑi)

i=1 from B.

Equations (1), (2)

end for
Compute total loss ÀÜL = (cid:80)B
Update œÜ and aux. networks w.r.t. loss ÀÜL and OptœÜ,aux.

ÀÜLi.

i=1

end for

C. PI-ARS

The encoder œÜ learned by PI maps a high-dimensional
observation to a concise 128-d representation, upon which
we use ARS to train a more compact policy network as
follows. At each iteration of ARS, the algorithm samples

Fig. 2. The environments that we used to benchmark the PI-ARS learning system: (a) uneven stepping stones, (b) quincuncial piles, (c) moving platforms
(the afterimage is for indicating that these platforms are moving), and (d) indoor navigation.

Fig. 3.
consistently and signiÔ¨Åcantly outperforms ARS.

Simulation results. We compare the performance of PI-ARS to ARS during training on four challenging simulation environments. PI-ARS

N perturbations [œÉ1, . . . , œÉN ] of the policy weights Œ∏ using
a standard normal distribution with scale œÉ. The algorithm
evaluates the policy returns at Œ∏ + œÉi and Œ∏ ‚àí œÉi. ARS then
computes an estimation of the policy gradient by aggregating
the returns from the best-performing perturbation directions:

ÀÜg =

Œ¥
M

M
(cid:88)

(Ri,+ ‚àí Ri,‚àí)œÉi,

i=1

(3)

where Œ¥ is the update coefÔ¨Åcient, M is the number of top-
performing perturbations to be considered, and Ri,+, Ri,‚àí
denote the total return of the policy at perturbations Œ∏ ¬± œÉi.
We refer the readers to [2] for additional details.

We iterate between updating the representation network œÜ
with the PI objective and updating the policy with ARS. To
maximize data re-use, we store the sampled trajectories from
perturbed policies evaluated by ARS in a replay buffer used
for the PI learning pipeline.

IV. EXPERIMENTS

of environments that we evaluate on. More details of each
environment can be found in Section B.

a) Uneven stepping stones: In this task, the robot is
tasked to walk over a series of randomly placed stepping
stones separated by gaps, and elevation of the stepping stones
changes dramatically (Figure 2 (a)).

b) Quincuncial piles: This is an extension to uneven
stepping stones, where we reduce the contact surface area
and arrange stones in both forward and lateral directions
(Figure 2(b)).

c) Moving platforms: We construct a set of stepping
stones and allow each piece to periodically move either
horizontally and vertically at a random speed (Figure 2(c)).
d) Indoor navigation with obstacles: In this task, we
evaluate the performance of PI-ARS controlling the robot
to navigate in a cluttered indoor environment (Figure 2 (d).
SpeciÔ¨Åcally, we randomly place boxes on the Ô¨Çoor of a
scanned indoor environment and command the robot to walk
to a target position.

We aim to answer the following questions in our experi-

ments:

B. Experiment Setup

‚Ä¢ Is our proposed algorithm, PI-ARS, able to learn vision-
based policies that solve challenging visual-locomotion
tasks?

‚Ä¢ Does PI-ARS achieve better performance than alterna-
tive methods that do not apply representation learning?

‚Ä¢ Are our learned policies applicable to real robots?

A. Visual-Locomotion Tasks

To answer the above questions, we design a variety of
challenging visual locomotion tasks. Figure 2 shows the suite

We use the Unitree Laikago quadruped robot [44], which
weighs 24kg and has 12 actuated joints, with two depth cam-
eras installed: one Intel D435 in the front for a wider Ô¨Åeld of
view and one Intel L515 on the belly for better close-range
depth quality. We create a corresponding simulated Laikago
robot in the PyBullet physics simulator [45] with physical
properties from hardware spec and simulated cameras that
matches the camera intrinsics and extrinsics from the real
cameras. The observation, action, and reward designs are
detailed as follows.

1) Observation Space: We design the observation space
in our visual-locomotion tasks following prior work by Yu
et al. [3]. In particular, our observation space consists of
two parts: s = (sv, sp), where sv are the two 32 √ó 24
images from depth sensors, and sp include all the propri-
oceptive states (and controller states). In our experiments,
sp = (qs, Àôp, ÀôŒ¶, ÀôŒò, r1...4, c1...4, œÜ1...4, aprev) includes the
CoM height, roll, and pitch qs = (pz, Œ¶, Œò), the estimated
CoM velocity Àôp, the gyroscope readings ÀôŒ¶, ÀôŒò, the robot‚Äôs
feet positions r1...4 in the base frame, the feet contact states
c1...4, the phase of each leg in its respective gait cycle œÜ,
and the previous action.

For the indoor navigation task, we additionally include the
relative goal vector n = ¬Øo ‚àí p as part of the observation,
where ¬Øo is the target location and p is the robot‚Äôs position.
2) Action Space: We follow the prior work [3] to use
a hierarchical design for the visual-locomotion controller
with a trainable high-level vision policy œÄŒ∏ that maps visual
and proprioceptive input to a high-level motion command,
and an MPC-based low-level motion controller that executes
the high-level motion command with trajectory optimization.
The high-level motion command, i.e. the action space for
the RL problem, is deÔ¨Åned as: (qd
1...4, h1...4), where
s and Àôqd are the desired CoM pose, velocity, rd
qd
is ith
i
foot‚Äôs target landing position (rxi, ryi, rzi), and hi is the
peak height of ith foot‚Äôs swing trajectory.

s, Àôqd, rd

3) Reward Function: For training a policy to walk on

different terrains, we use the following reward function:

R(s, a) = clip( Àôpx, ‚àí Àôpmax

x

, Àôpmax
x

) ‚àí w|Œ®|,

(4)

where Àôpx is the CoM velocity in the forward direction, and
Œ® the base yaw angle. The Ô¨Årst term rewards the robot to
move forward with a maximum speed controlled by Àôpmax
,
x
the second term encourages the robot to walk straightly.

For the indoor navigation task, we use the delta geodesic

(path) distance to the goal as our reward:

R(s, a, ¬Øo) = dt

g ‚àí dt‚àí1
g

,

(5)

where dt
target location at time t.

g is the geodesic distance between the robot and the

4) Early termination.: A training episode is terminated
if: 1) the robot loses balance (CoM height pz below 0.15 m,
pitch |Œò| > 1 rad, or roll |Œ¶| > 0.3 rad in our experiments),
or 2) the robot reaches an invalid joint conÔ¨Åguration, e.g.
knee bending backwards.

C. Learning in Simulation

In this subsection, we discuss the results of PI-ARS
learned on simulated visual-locomotion tasks and compare to
a state-of-the-art ARS approach to robotic visual-locomotion
[2], [3] (Figure 3). Among other baseline approaches that we
have tried include SAC [9] and PI-SAC [13] but both algo-
rithms failed to make any non-negligible learning progress
for the tasks we consider despite extensive hyperparameter
tuning, and so we omit these algorithms from the results. For
fair comparison, all algorithms utilize the same MPC-based

locomotion controller and learn policies in the high-level
command space described in Section IV-B.2. All policies
utilize the same network architecture; i.e., the policy learned
by the baseline ARS method is composed of the same set of
convolution and feed-forward layers used for œÜ in PI-ARS.
We train PI-ARS and ARS policies using a distributed
implementation. For all PI-ARS and ARS experiments, we
perform N = 1024 perturbations per ARS iteration and use
the top 50% performers (M = 512) to update the policy
network head. This choice, determined through a grid search,
empirically works the best for both PI-ARS and ARS in our
implementation. Further increase of N (and thus computation
cost) does not signiÔ¨Åcantly improve the performance. The
algorithm is run until convergence with a maximum of 4000
training iterations, resulting in a maximum of 2, 048, 000
simulation episodes per trial. We perform 30 trials of training
PI-ARS/ARS with uniformly sampled œÉ and Œ¥ values (œÉ ‚àº
[0.005, 0.05], Œ¥ ‚àº [0.005, 0.05]) and report the mean and
standard error of returns against number of training episodes
for each task.

As we demonstrate in the supplementary video, PI-ARS
is able to learn vision-based policies that successfully solve
these challenging visual-locomotion tasks. Figure 3 shows
that on all tasks, PI-ARS gives signiÔ¨Åcantly better returns
and sample-efÔ¨Åciency than the ARS baseline. For example,
on uneven stepping stones, the mean return after 2,000,000
episodes of training improves by 48.01%, from 2.93 to
4.34. This empirically demonstrates the effectiveness of
learning ARS policies upon compressed, gradient-learned
representations instead of end-to-end. On the other hand,
observing that SAC fails to learn, we hypothesize that the
advantages of ARS such as parameter-space exploration
and stability are critical to these complex visual-locomotion
tasks. Furthermore, adding predictive information to SAC,
i.e. PI-SAC, does not improve learning, suggesting that even
with an effective representation learner, without a powerful
policy solver, a learning algorithm is not able to sufÔ¨Åciently
tackle these visual-locomotion tasks.

D. Validation on Real Robot

We deploy the visual-locomotion policy trained in simu-
lation on a Laikago robot to perform two visual-locomotion
tasks: 1) walking over real-world stepping stones (Figure 4),
and 2) navigating in an indoor environment with obstacles
(Figure 5).

To overcome the sim-to-real gap, we adopt

the same
procedure as done by Yu et al. [3]. For the visual gap,
during training, we Ô¨Årst apply random noise to the simulated
depth images to mimic the real-world depth noises. Then
we apply a Navier-Stokes-based in-painting operation [46]
with radius of 1 to Ô¨Åll
the missing pixels, followed by
down-sampling to 32x24 (with OpenCV‚Äôs INTER AREA
interpolation method for resizing [47]). On the real hardware,
we obtain 640x480 raw depth images from both L-515 and
D435 cameras and perform the same in-painting and down-
sampling. To mitigate the dynamics gap, we apply dynamics
randomization during training.

Fig. 4. PI-ARS policy solving a challenging real-world visual-locomotion task involving a series of four step stones separated by gaps. PI-ARS successfully
completes this terrain, avoiding all gaps, 100% of the time measured over 10 trials.

of learning policies that can transfer to real robots.

V. CONCLUSION

We present a new learning method, PI-ARS, and apply it to
the visual-locomotion problem. PI-ARS combines gradient-
based representation learning with gradient-free policy opti-
mization to leverage the advantages of both. PI-ARS enjoys
the simplicity and scalability of gradient-free methods, and
it relieves a key bottleneck of ES algorithms on high-
dimensional problems by simultaneously learning a low-
dimensional representation that reduces the search space.
We evaluate our method on a set of challenging visual-
locomotion tasks, including navigating through uneven step-
ping stones, quincuncial piles, moving platforms, and clut-
tered indoor environments, among which PI-ARS signif-
icantly outperforms the state-of-the-art. Furthermore, we
validate the policy learned by PI-ARS on a real quadruped
robot. It enables the robot to walk over randomly-placed
stepping stones and navigating in an indoor space with
obstacles. In the future, we plan to test PI-ARS in outdoor
visual-locomotion tasks, which presents more diverse and
interesting terrains for the robot to overcome.

ACKNOWLEDGMENTS

We thank Noah Brown, Gus Kouretas, and Thinh Nguyen
for helping set up the real-world stepping stones and address
robot hardware issues.

REFERENCES

[1] T. Salimans, J. Ho, X. Chen, S. Sidor, and I. Sutskever, ‚ÄúEvolution
strategies as a scalable alternative to reinforcement learning,‚Äù arXiv
preprint arXiv:1703.03864, 2017.

[2] H. Mania, A. Guy, and B. Recht, ‚ÄúSimple random search provides
a competitive approach to reinforcement learning,‚Äù arXiv preprint
arXiv:1803.07055, 2018.

[3] W. Yu, D. Jain, A. Escontrela, A. Iscen, P. Xu, E. Coumans, S. Ha,
J. Tan, and T. Zhang, ‚ÄúVisual-locomotion: Learning to walk on
complex terrains with vision,‚Äù in 5th Annual Conference on Robot
Learning, 2021.

[4] X. Song, Y. Yang, K. Choromanski, K. Caluwaerts, W. Gao, C. Finn,
and J. Tan, ‚ÄúRapidly adaptable legged robots via evolutionary meta-
learning,‚Äù in 2020 IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS).

IEEE, 2020, pp. 3769‚Äì3776.

[5] W. Yu, J. Tan, Y. Bai, E. Coumans, and S. Ha, ‚ÄúLearning fast adapta-
tion with meta strategy optimization,‚Äù IEEE Robotics and Automation
Letters, vol. 5, no. 2, pp. 2950‚Äì2957, 2020.

[6] A. Cully, J. Clune, D. Tarapore, and J.-B. Mouret, ‚ÄúRobots that can

adapt like animals,‚Äù Nature, vol. 521, no. 7553, pp. 503‚Äì507, 2015.

[7] R. Huang, Y. Chen, T. Yin, X. Li, A. Li, J. Tan, W. Yu, Y. Liu, and
Q. Huang, ‚ÄúAccelerated deep reinforcement learning based load shed-
ding for emergency voltage control,‚Äù arXiv preprint arXiv:2006.12667,
2020.

Fig. 5. PI-ARS policy learns to navigate in a cluttered real-world indoor
environment.

Videos of our real-world experiments can be found in the

supplementary material.

a) Stepping Stones: For the stepping stones task, we
created a physical setup consisting of four stones separated
by three gaps between [0.12, 0.18]m (Figure 4). The PI-
ARS policy is learned in simulation with an easier version
of uneven stepping stones where stone heights change less
signiÔ¨Åcantly. Our PI-ARS policy was able to achieve 100%
success rate on the stepping stone environment with 10 trials.
In contrast, the ARS baseline [3] with the same training and
evaluation setting achieved 40% success rate for reaching the
last stone with all four legs and often failed at the last gap.
b) Indoor Navigation: For evaluating the navigation
task in the real world, we design a route in an indoor
environment with obstacles (Figure 5). The robot needs to
navigate to the target location while avoiding the obstacles.
To enable the robot to better avoid the obstacles, we rotate
the front camera of the robot such that it can see ‚àº3 meters
ahead of the robot. We also track the robot base position
using a motion capture system, which is needed to compute
the relative goal vector n. As shown in the supplementary
video, our PI-ARS policy is able to successfully navigate to
the designated target location. For the setting shown in Figure
5, our policy successfully discovered a ‚Äòshortcut‚Äô in between
two obstacles and was able to go through. We do note there
is collision with the obstacle‚Äôs arm. This is because the robot
was trained with simulated obstacles with box shapes only;
further training with more diverse obstacles would likely
mitigate this problem.

Overall, these experiments validate that PI-ARS is capable

[8] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,
preprint

optimization

algorithms,‚Äù

arXiv

‚ÄúProximal
arXiv:1707.06347, 2017.

policy

[9] T. Haarnoja, A. Zhou, K. Hartikainen, G. Tucker, S. Ha, J. Tan,
V. Kumar, H. Zhu, A. Gupta, P. Abbeel, et al., ‚ÄúSoft actor-critic
algorithms and applications,‚Äù arXiv preprint arXiv:1812.05905, 2018.
[10] Y. Nesterov and V. Spokoiny, ‚ÄúRandom gradient-free minimization
of convex functions,‚Äù Foundations of Computational Mathematics,
vol. 17, no. 2, pp. 527‚Äì566, 2017.

[11] A. v. d. Oord, Y. Li, and O. Vinyals, ‚ÄúRepresentation learning with
contrastive predictive coding,‚Äù arXiv preprint arXiv:1807.03748, 2018.
[12] D. Ha and J. Schmidhuber, ‚ÄúWorld models,‚Äù arXiv preprint

arXiv:1803.10122, 2018.

[13] K.-H. Lee, I. Fischer, A. Liu, Y. Guo, H. Lee, J. Canny, and S. Guadar-
rama, ‚ÄúPredictive information accelerates learning in rl,‚Äù Advances in
Neural Information Processing Systems, vol. 33, pp. 11 890‚Äì11 901,
2020.

[14] A. Srinivas, M. Laskin, and P. Abbeel, ‚ÄúCURL: Contrastive unsu-
pervised representations for reinforcement learning,‚Äù arXiv preprint
arXiv:2004.04136, 2020.

[15] M. Yang and O. Nachum, ‚ÄúRepresentation matters: OfÔ¨Çine pretrain-
ing for sequential decision making,‚Äù in International Conference on
Machine Learning. PMLR, 2021, pp. 11 784‚Äì11 794.

[16] X. Chen, S. Toyer, C. Wild, S. Emmons, I. Fischer, K.-H. Lee, N. Alex,
S. H. Wang, P. Luo, S. Russell, et al., ‚ÄúAn empirical investigation
of representation learning for imitation,‚Äù in Thirty-Ô¨Åfth Conference
on Neural Information Processing Systems Datasets and Benchmarks
Track (Round 2), 2021.

[17] J. Tan, Y. Gu, C. K. Liu, and G. Turk, ‚ÄúLearning bicycle stunts,‚Äù
ACM Trans. Graph., vol. 33, no. 4, pp. 50:1‚Äì50:12, 2014. [Online].
Available: http://doi.acm.org/10.1145/2601097.2601121

[18] D. Jain, A. Iscen, and K. Caluwaerts, ‚ÄúHierarchical reinforcement
learning for quadruped locomotion,‚Äù in 2019 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS).
IEEE, 2019,
pp. 7551‚Äì7557.

[19] E. Vinitsky, A. Kreidieh, L. Le Flem, N. Kheterpal, K. Jang, C. Wu,
F. Wu, R. Liaw, E. Liang, and A. M. Bayen, ‚ÄúBenchmarks for
reinforcement learning in mixed-autonomy trafÔ¨Åc,‚Äù in Conference on
robot learning. PMLR, 2018, pp. 399‚Äì409.

[20] J. Schmidhuber, ‚ÄúMaking the world differentiable: On using self-
supervised fully recurrent neural networks for dynamic reinforcement
learning and planning in non-stationary environments,‚Äù Institut f¬®ur
Informatik, Technische Universit¬®at M¬®unchen, Tech. Rep., 1990.
[21] ‚Äî‚Äî, ‚ÄúReinforcement learning in markovian and non-markovian en-
vironments,‚Äù in Advances in Neural Information Processing Systems,
1991, pp. 500‚Äì506.

[22] ‚Äî‚Äî, ‚ÄúOn learning to think: Algorithmic information theory for
novel combinations of reinforcement learning controllers and recurrent
neural world models,‚Äù arXiv preprint arXiv:1511.09249, 2015.
[23] O. Nachum and M. Yang, ‚ÄúProvable representation learning for imita-
tion with contrastive fourier features,‚Äù Advances in Neural Information
Processing Systems, vol. 34, 2021.

[24] M. Yang, S. Levine, and O. Nachum, ‚ÄúTrail: Near-optimal imitation
learning with suboptimal data,‚Äù arXiv preprint arXiv:2110.14770,
2021.

[25] D. Hafner, T. Lillicrap, J. Ba, and M. Norouzi, ‚ÄúDream to con-
imagination,‚Äù arXiv preprint

trol: Learning behaviors by latent
arXiv:1912.01603, 2019.

[26] B. Poole, S. Ozair, A. Van Den Oord, A. Alemi, and G. Tucker, ‚ÄúOn
variational bounds of mutual information,‚Äù in International Conference
on Machine Learning. PMLR, 2019, pp. 5171‚Äì5180.

[27] W. Bialek and N. Tishby, ‚ÄúPredictive information,‚Äù arXiv preprint

cond-mat/9902341, 1999.

[28] C. Mastalli, M. Focchi, I. Havoutis, A. Radulescu, S. Calinon,
J. Buchli, D. G. Caldwell, and C. Semini, ‚ÄúTrajectory and foothold
optimization using low-dimensional models for rough terrain loco-
motion,‚Äù in 2017 IEEE International Conference on Robotics and
Automation (ICRA).

IEEE, 2017, pp. 1096‚Äì1103.

[29] O. A. V. Magana, V. Barasuol, M. Camurri, L. Franceschi, M. Focchi,
M. Pontil, D. G. Caldwell, and C. Semini, ‚ÄúFast and continuous
foothold adaptation for dynamic locomotion through CNNs,‚Äù IEEE
Robotics and Automation Letters, vol. 4, no. 2, pp. 2140‚Äì2147, 2019.
[30] O. Villarreal, V. Barasuol, P. M. Wensing, D. G. Caldwell, and C. Sem-
ini, ‚ÄúMPC-based controller with terrain insight for dynamic legged

locomotion,‚Äù in 2020 IEEE International Conference on Robotics and
Automation (ICRA).

IEEE, 2020, pp. 2436‚Äì2442.

[31] P. Fankhauser, M. Bjelonic, C. D. Bellicoso, T. Miki, and M. Hutter,
‚ÄúRobust rough-terrain locomotion with a quadrupedal robot,‚Äù in 2018
IEEE International Conference on Robotics and Automation (ICRA).
IEEE, 2018, pp. 5761‚Äì5768.

[32] F. Jenelten, T. Miki, A. E. Vijayan, M. Bjelonic, and M. Hutter,
‚ÄúPerceptive locomotion in rough terrain‚Äìonline foothold optimization,‚Äù
IEEE Robotics and Automation Letters, vol. 5, no. 4, pp. 5370‚Äì5376,
2020.

[33] R. Grandia, A. J. Taylor, A. D. Ames, and M. Hutter, ‚ÄúMulti-layered
safety for legged robots via control barrier functions and model
predictive control,‚Äù arXiv preprint arXiv:2011.00032, 2020.

[34] S. Gangapurwala, M. Geisert, R. Orsolino, M. Fallon, and I. Havoutis,
‚ÄúRLOC: Terrain-aware legged locomotion using reinforcement learn-
ing and optimal control,‚Äù arXiv preprint arXiv:2012.03094, 2020.
[35] T. Miki, J. Lee, J. Hwangbo, L. Wellhausen, V. Koltun, and M. Hutter,
‚ÄúLearning robust perceptive locomotion for quadrupedal robots in the
wild,‚Äù Science Robotics, vol. 7, no. 62, p. eabk2822, 2022.

[36] D. Kim, D. Carballo, J. Di Carlo, B. Katz, G. Bledt, B. Lim, and
S. Kim, ‚ÄúVision aided dynamic exploration of unstructured terrain
with a small-scale quadruped robot,‚Äù in 2020 IEEE International
Conference on Robotics and Automation (ICRA).
IEEE, 2020, pp.
2464‚Äì2470.

[37] G. B. Margolis, T. Chen, K. Paigwar, X. Fu, D. Kim, S. Kim,
and P. Agrawal, ‚ÄúLearning to jump from pixels,‚Äù arXiv preprint
arXiv:2110.15344, 2021.

[38] H. W. Park, P. M. Wensing, and S. Kim, ‚ÄúOnline planning for
autonomous running jumps over obstacles in high-speed quadrupeds,‚Äù
in 2015 Robotics: Science and Systems Conference, RSS 2015. MIT
Press Journals, 2015.

[39] J. Di Carlo, P. M. Wensing, B. Katz, G. Bledt, and S. Kim, ‚ÄúDynamic
locomotion in the mit cheetah 3 through convex model-predictive
control,‚Äù in 2018 IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS), 2018, pp. 1‚Äì9.

[40] D. Kim, J. Di Carlo, B. Katz, G. Bledt, and S. Kim, ‚ÄúHighly dynamic
quadruped locomotion via whole-body impulse control and model
predictive control,‚Äù arXiv preprint arXiv:1909.06586, 2019.

[41] G. Bledt, P. M. Wensing, and S. Kim, ‚ÄúPolicy-regularized model
predictive control to stabilize diverse quadrupedal gaits for the mit
cheetah,‚Äù in 2017 IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS).

IEEE, 2017, pp. 4102‚Äì4109.

[42] I. Fischer, ‚ÄúThe conditional entropy bottleneck,‚Äù Entropy, vol. 22,

no. 9, p. 999, 2020.

[43] D. P. Kingma and J. Ba, ‚ÄúAdam: A method for stochastic optimiza-

tion,‚Äù arXiv preprint arXiv:1412.6980, 2014.

[44] ‚ÄúUnitree Robotics.‚Äù [Online]. Available: http://www.unitree.cc/
[45] E. Coumans and Y. Bai, ‚ÄúPybullet, a python module for physics

simulation in robotics, games and machine learning,‚Äù 2017.

[46] M. Bertalmio, A. L. Bertozzi, and G. Sapiro, ‚ÄúNavier-stokes, Ô¨Çuid
dynamics, and image and video inpainting,‚Äù in Proceedings of the
2001 IEEE Computer Society Conference on Computer Vision and
Pattern Recognition. CVPR 2001, vol. 1.

IEEE, 2001, pp. I‚ÄìI.

[47] G. Bradski, ‚ÄúThe OpenCV Library,‚Äù Dr. Dobb‚Äôs Journal of Software

Tools, 2000.

A. PI-ARS Implementation

APPENDIX

t and proprioceptive states sp

We describe implementation details of PI-ARS as follows.
1) Network Architecture for œÜ: In the visual-locomotion
tasks we consider, an observation st contains a visual obser-
vation sv
t . The visual observation
sv
t contains one depth image from the front camera and
one from the rear (described in Section IV-B). Accordingly,
our vision encoder œÜv contains two identical CNNs that
independently map the front and rear images to two 64-d
vectors and concatenates them into a 128-d representation
output zv
t . Each CNN consists of 2 convolution layers with
3 √ó 3 kernels, 8 channels, stride size 1, followed by a 64-
d linear projection, where each convolution and projection

is followed by a relu activation. zv
is then concatenated
t
with proprioceptive states sp
t and linearly projected with
tanh activation to yield a 128-d representation output zt.
These together give us the observation encoder œÜ, in which
zt = œÜ(sv

t ) = œÜ(st).

t , sp

2) Network Architecture for Auxiliary Functions: The
auxiliary-learned function f maps zt to a unit-length 128-d
vector zpast
, which corresponds to the past, via a 2-layer
MLP (64 units, 128 units) followed by l-2 normalization.

t

For the future observation, f only considers the visual

observation sv

t+k and ignores sp
t+k = h(stopgrad(œÜv(sv
zfuture

t+k:

t+k)))

(6)

where h is another 2-layer MLP (64 units, 128 units), and
stopgrad refers to the stop gradient operation. The output of
f is a dot-product of zpast

and zf uture
t+k

.

t

For predicting the future rewards, we recurrently apply an
auxiliary RNN cell g to encode a latent state and an action
(i.e. the past) and output a reward prediction and the next
latent state at each time step:

ÀÜrt, z(cid:48)

t+1 = g(zt, at), ÀÜrt+1, z(cid:48)
ÀÜrt+k‚àí1, z(cid:48)

t+2 = g(z(cid:48)
t+k = g(z(cid:48)

t+1, at+1), . . . ,
t+k‚àí1, at+k‚àí1).

g is a 3-layer MLP (128 units each with tanh activations),
and a 128-to-1 linear layer branch is attached to the second
layer of g to output reward predictions ÀÜrt, . . . , ÀÜrt+k‚àí1, one
at each step. The initial latent state is zt.

3) Policy Network Head: The ARS-learned policy net-
work head is a simple 3-layer MLP (64, 32, and dim(A)
units, where A is the action space) with tanh activation that
takes zt and proprioceptive states sp
t as input, and outputs an
action. The output of the policy network is tanh-squashed to
[‚àí1, 1] and subsequently re-scaled to the environment action
bounds (Table I).

4) PI-ARS Training:

In PI-ARS, we alternate between
1 step for ARS and 2 steps for PI. Each PI step uses a
batch of 512 k-step trajectories from the replay buffer and
performs gradient steps with a learning rate of 10‚àí4. We
set k = 5 for all tasks but indoor navigation. For indoor
navigation, we use k = 30 as the task nature requires a longer
planning horizon. We also apply observation normalization,
using running means and standard deviations.

5) ARS, SAC, PI-SAC Implementations: To ensure fair
comparison for non-representation learning methods (ARS,
SAC), we use the an identical policy network as used for PI-
ARS, which combines a base encoder œÜ and a policy network
head. Thus, all algorithms have access to the same capacity
policy network. The critic in SAC and PI-SAC share the same
base encoder œÜ with the policy (actor) network and we stop
gradients from the policy head to the base encoder following
[13]. We also apply the same observation normalization used
for PI-ARS to these baseline methods.

B. Visual-Locomotion Task details

Here we describe additional details regarding the visual-
locomotion tasks we used in our work. For the uneven

TABLE I
ACTION SPACE RANGES

action
Target local foothold
Target peak swing height
Desired CoM height
Desired base roll
Desired base pitch
Desired base twist speed

lower bound

upper bound

(-0.05, -0.05, -0.03)m (0.1, 0.05, 0.03)m

0.05m
0.42m
-0.1
-0.2
-0.2

0.1m
0.47m
0.1
0.2
0.2

stepping stone, quincuncial piles, and moving platform tasks
we follow the design in prior work [3].

a) Uneven stepping stones: In this task we evaluate
the ability for the policy to traverse stepping stones with
varied heights. The widths, and lengths of stepping stones
are sampled from [0.55, 0.7], and [0.5, 0.8] meters. The
height offsets of neighboring stones are uniformly sampled in
[0.13, 0.2]m, and a gap of [0.05, 0.1]m is added between the
stones. To successfully traverse this environment the agent
needs to identify and avoid the gaps between stones and land
the swing leg to the appropriate height for the next stone.

b) Quincuncial piles: In this task, we create a two-
dimensional stepping stones to evaluate the robot‚Äôs behavior
in avoiding gaps in both forward and lateral direction.
SpeciÔ¨Åcally, each stone has an area of 0.15 √ó 0.15m2 with a
standard deviation of 0.015m in height, and is separated by
[0.13, 0.17] m from each other in both x and y directions.
At the beginning of each episode, we also randomly rotate
entire stone grid in [0.1, 0.1] rad.

c) Moving platforms: Our proposed framework can
also be applied to handle dynamic objects. In this exam-
ple, we construct an environment with random stepping
stones and allow each piece to move dynamically. Each
platform follows a periodic movement whose magnitude
and frequency are randomly sampled in [0.10, 0.15]m and
[0.4, 1.0]Hz, respectively. Also, we randomly pick half of the
platforms to move horizontally and the rest vertically. This
task requires the control policy to infer both the position
and velocity of the platforms and thus presents a more
challenging representation learning problem. To enable the
model to infer velocity information, we stack a history of
three recent images as input to the policy for this task, which
slightly changes the observation spec.

d) Indoor navigation: For the indoor navigation task,
we use a scan of the building interior with size about
25 √ó 15m. We randomly place 50 boxes of different sizes
in the scanned scene to model obstacles during navigation.
Among the 50 obstacles, 20% are sampled with length in
[1.6, 2]m, width in [0.7, 1]m, and height in [0.4, 1.2]m, which
are to mimic bigger obstacles like sofa. 40% are sampled
with length and width in [0.5, 0.8]m and height in [0.4, 1.4]m,
which represent smaller obstacles like chairs. The rest 40%
are sampled with length and width in [0.1, 0.4]m and height
in [0.8, 2.0]m, which correspond to taller objects like pillars.

