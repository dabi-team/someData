Accurate and Lightweight Image Super-Resolution
with Model-Guided Deep Unfolding Network

Qian Ning, Student Member, IEEE, Weisheng Dong, Member, IEEE, Guangming Shi, Senior Member, IEEE,
Leida Li, Member, IEEE and Xin Li, Fellow, IEEE

1

0
2
0
2

v
o
N
1
2

]

V
C
.
s
c
[

2
v
4
5
2
6
0
.
9
0
0
2
:
v
i
X
r
a

Abstract—Deep neural networks (DNNs) based methods have
achieved great success in single image super-resolution (SISR).
However, existing state-of-the-art SISR techniques are designed
like black boxes lacking transparency and interpretability. More-
over, the improvement in visual quality is often at the price
of increased model complexity due to black-box design. In this
paper, we present and advocate an explainable approach toward
SISR named model-guided deep unfolding network (MoG-DUN).
Targeting at breaking the coherence barrier, we opt to work with
a well-established image prior named nonlocal auto-regressive
model and use it to guide our DNN design. By integrating
deep denoising and nonlocal regularization as trainable modules
within a deep learning framework, we can unfold the iterative
process of model-based SISR into a multi-stage concatenation
of building blocks with three interconnected modules (denoising,
nonlocal-AR, and reconstruction). The design of all three modules
leverages the latest advances including dense/skip connections as
well as fast nonlocal implementation. In addition to explainabil-
ity, MoG-DUN is accurate (producing fewer aliasing artifacts),
computationally efﬁcient (with reduced model parameters), and
versatile (capable of handling multiple degradations). The superi-
ority of the proposed MoG-DUN method to existing state-of-the-
art image SR methods including RCAN, SRMDNF, and SRFBN
is substantiated by extensive experiments on several popular
datasets and various degradation scenarios.

I. INTRODUCTION

The ﬁeld of deep learning for single image super-resolution
(SISR) has advanced rapidly in recent years. Super-resolution
by convolutional neural network (SRCNN) [1] represented one
of the pioneering works in this ﬁeld. Since then, many follow-
up works have been developed including Super-resolution via
Generative Adversarial Network (SRGAN) [2], SR via very
deep convolutional networks (VDSR) [3], Trainable Nonlin-
ear Reaction Diffusion Network(TNRD) [4], Deeply-recursive
convolutional network (DRCN) [5], Enhanced Deep Residual
Networks (EDSR) [6], Laplacian Pyramid Super-Resolution
Network (LapSRN) [7], and Deep Back-Projection Networks
(DBPN) [8]. Most recently, SISR has beneﬁted from the
advances in novel design of network architectures such as
densely-connected networks (e.g., RDN [9]), attention mecha-
nism (e.g., RCAN [10] ans SAN [11]), multiple degradations

Qian Ning, Weisheng Dong, Guangming Shi and Leida Li are with School

of Artiﬁcial Intelligence, Xidian University, Xi’an, 710071, China.

Xin Li is with the Lane Department of Computer Science and Electrical

Engineering, West Virginia University, Morgantown WV 26506-6109.

This work was supported in part by the National Key R&D Program of
China under Grant 2018AAA0101400 and the Natural Science Foundation of
China under Grant 61991451, Grant 61632019, Grant 61621005, and Grant
61836008. Xin Li’s work is partially supported by the DoJ/NIJ under grant
NIJ 2018-75-CX-0032, NSF under grant OAC-1839909 and the WV Higher
Education Policy Commission Grant (HEPC.dsr.18.5).

(e.g., SRMDNF [12]), feedback connections (e.g., SRFBN
[13] and feature aggregation [14].

Despite the rapid progress, one of the long-standing open
issues is the lack of interpretability. Most existing networks
for SISR are designed based on the black-box principle -
i.e., little is known about their internal workings regardless
of the desirable input-output mapping results. The difﬁculty
with understanding the internal mechanism of deep learning-
based SISR has become even more striking when the network
gets deeper and more sophisticated (e.g., due to attention and
feedback). For instance, the total number of parameters of
EDSR [6] has reached over 40M, which makes it a less feasible
for practical applications. By contrast, it is more desirable
to seek alternative glass-box design (a.k.a. clear-box) where
the inner components are readily available for inspection. A
hidden beneﬁt of such a transparent approach is that it might
lead to more efﬁcient design because any potential redundancy
(in terms of model parameters) can be cautiously avoided. Can
we solve SISR under an emerging framework of interpretable
machine learning [15]? Does a transparent design lead to
computationally more efﬁcient solution to SISR facilitating
practical applications (e.g., lightweight architecture [16])?

We provide afﬁrmative answers to the above questions in
this paper. An important new insight brought to the ﬁeld of
SISR by this work is model-guided (MoG) design for deep
neural networks. The basic idea behind MoG design is to
seek a mathematically equivalent implementation of existing
model-based solution by deep neural networks. Similar ideas
have scattered in the literature of so-called deep unfolding net-
works (e.g., [17], [18], [19]). It has been well-established that
classic model-based tools including sparse coding, Markov
Random Field, belief propagation, and non-negative matrix
factorization can be unfolded to a network implementation
[19], [17], [20], [21]. Note that in the ﬁeld of SISR, there are
plenty of model-based methods [22], [23], [24], [25], which
could provide a rich source of inspiration for MoG design.
Most recent work USRNet [21] serves as an exemplar of such
MoG design for handling classical degradation of SISR via a
single uniﬁed model. Based on our previous [26] and recent
[20] works, we propose a new MoG Deep Unfolding Network
(MoG-DUN) for SISR that is not only explainable and efﬁcient
but also accurate and versatile.

We will show that most model-based image priors or
regularization functions, including both convex and nonconvex
formulation, can be leveraged as the guidance for the design
the class of nonconvex
of deep neural networks. In fact,
regularization models do not pose additional difﬁculty to

 
 
 
 
 
 
2

model into the corresponding network implementation.
The unfolding result consists of the multi-stage concate-
nation of Unet-like deep denoising module along with a
nonlocal-AR module and a reconstruction module. Both
long and short skip connections are introduced within and
across different stages to facilitate the information ﬂow.
The model complexity of our MoG-DUN is shown to be
comparable to that of [20].

• We report extensive experimental results for the devel-
oped MoG-DUN, which justify its achieving an improved
trade-off between the modeling complexity and the SR
reconstruction performance. Our MoG-DUN has achieved
better performance than existing state-of-the-art SISR
such as RDN [9]) and RCAN [10] with a lower cost
as measured by the number of model parameters. In
particular, visual quality improvement in terms of sharper
edges and more faithful reconstruction of texture patterns
is mostly striking.

II. MODEL-BASED IMAGE INTERPOLATION AND
RESTORATION

We ﬁrst brieﬂy review previous works on model-based
image interpolation (e.g., NARM [26]) and image restoration
(e.g., DPDNN [20]), which sets up the stage for model-guided
network design. The NARM model has achieved state-of-the-
art performance in model-based image interpolation (a special
case of SISR involving down-sampling only and without any
anti-aliasing low-pass ﬁlter). Denoising Prior driven DNN
(DPDNN) [20] represents the most recent MoG design of deep
unfolding network whose variation has been adopted as the
baseline method in our research.

Fig. 2: The illustration of NARM [26] - a nonlocal extension
of classic auto-regressive (AR) model for image signals.

A. Nonlocal Auto-regressive Model (NARM)

The basic idea behind nonlocal auto-regressive modeling
(refer to Fig. 2) is to extend the traditional auto-regressive
(AR) models by redeﬁning the neighborhood. For a given
patch xi, NARM seeks its sparse linear decomposition over a
set of nonlocal (instead of local) neighborhood. Following the
notation in [26], we have

xi ≈

(cid:88)

i xj
ωj

i

j

(1)

Fig. 1: This work advances the state-of-the-art in SISR by
striking an improved tradeoff between the cost (as measured by
the number of model parameters) and performance (reﬂected
by the PSNR metric on Set5 with ×4 bicubic down-sampling).
Our methods are highlighted by the red color.

learning-based approaches despite their intractability from an
analytical perspective. This is because that the training of deep
neural networks (DNN) has a natural and intrinsic connection
with the optimization of cost functions by numerical methods
(e.g., gradient descent methods [27]). Aiming at breaking
the coherence barrier [28] (a long-standing open problem in
SISR), we have chosen a reference model, nonlocal autore-
gressive model (NARM) with improved incoherence properties
[26], to showcase the process of Model-Guided (MoG) design.
We will unfold this model with a nonconvex cost function
into network implementations consisting of multistage U-net
modules [20]. This work further extends our previous work
[20] by incorporating a nonlocal module (for AR modeling
computation) and a reconstruction module (for AR modeling
correction). Moreover, long connections are introduced across
different stages to copy the hidden states from previous stages
to the current, which facilitates the information ﬂow. Even
though MoG-DUN has achieved outstanding performance (in
terms of both subjective and objective qualities), its model
complexity has been kept much lower than that of DBPN
[8], RDN [9], and RCAN [10] and only slightly higher than
that of SRMDNF [12] and SRFBN [13], as shown in Fig. 1.
Meantime, MoG-DUN is explainable and versatile thanks for
its transparent design, which has the potential of leveraging to
other image restoration applications.

The key contributions of this paper are summarized below.

• We present a model-guided explainable approach toward
SISR. Our approach is capable of leveraging existing
NARM-based SISR into a network implementation in a
transparent manner. Thanks to the improved incoherence
property of NARM, the corresponding MoG-DUN has
better capability of alleviating long-standing problem in
SISR such as aliasing artifacts. The framework of MoG
design is applicable to all existing models including
nonconvex and nonlocal regularization.

• We demonstrate how to unfold an existing NARM [26]

RDN, 32.47EDSR, 32.46SRFBN, 32.47DBPN, 32.47SRMDNF, 31.96RCAN, 32.63DPDNN, 31.72DSRN, 31.4USRNet, 32.45MoG-DUN, 32.6031.231.431.631.83232.232.432.632.8010000200003000040000PSNR(dB)Number of Parameters(K)3

(a) The overview of our proposed MoG-DUN network for SISR.

(b) Deep denoising module

(c) Fast Nonlocal-AR module

(d) DS-net

(e) US-net

(f) The illustration of relationship between the Iterative algorithm of NARM model and MoG-DUN Network

Fig. 3: The architecture of our proposed MoG-DUN network for SISR. (a) The overall architecture of the proposed
network(ht−1, ht, hT −1 denote the hidden states of corresponding stages), (b)The inner structure of the Encoding and Decoding
block of the U-net Denoising Module, (c) The architecture of the Nonlocal-AR Module, (d) The inner structure of Down-
Sampling-network(DS-net), (e) The inner structure of Up-Sampling-network(US-net), (f) The illustration of corresponding
relationship between the Iterative algorithm of NARM model and MoG-DUN Network.

where xj
i denotes the j-th similar patch found in the nonlocal
neighborhood. A natural way of extending the classic AR
modeling is to formulate the following regularized Least-
Square problem:

B. Model-based Image Restoration

The objective of model-based image restoration (IR) is to
estimate an unknown image x from its degraded observation
y. The degradation process can be formulated by:

wi = argmin

||xi − Xwi||2

2 + γ||wi||2
2

(2)

y = Ax + n

(7)

wi
i , ..., xJ

i , x2

where X = [x1
i ]T , and γ is
i ], wi = [ω1
the regularization parameter. The closed-form solution to Eq.
(2) is given by

i , ..., ωJ

i , ω2

wi = (X T X + γI)−1(X T xi)

(3)

where I is an identity matrix with the same size as X T x.
Based on newly determined AR coefﬁcients wi, we can
represent the nonlocal autoregressive model (NARM) of image
x by

x = Sx + ex

(4)

where ex is the modeling error, and the NARM matrix S is

Si,j =

(cid:26) ωj
i ,
0,

i is a nonlocal neighbor of xi

if xj
otherwise

(5)

where A denotes the degradation operators (e.g., blurring
kernels, down-sampling operations) and n denotes the additive
noise. Accordingly, model-based IR can be formulated into the
following optimization problem:

x = argmin

x

||y − Ax||2

2 + λΩ(x)

(8)

where λ is the Lagrangian multiplier and Ω(x) the regulariza-
tion function. The choice of various regularization functions
Ω(x) reﬂects different ways of incorporating a priori knowl-
edge about the unknown HR image x.

To solve model-based SR problem in Eq. (8), half-quadratic
splitting method [29] converts an equally-constrained opti-
mization problem into an equivalent non-constrained optimiza-
tion problem, which can be written as

The NARM matrix S can be embedded into a standard image
degradation model by modifying Eq. (7) into

(x, v) = argmin

x,v

||y − Ax||2

2 + η||x − v||2

2 + λΩ(v)

(9)

y = ˜Ax + n
(6)
where ˜A = AS is the new degradation operator. It has
been shown in [26] that such nonlocal extension is beneﬁcial
to improving the incoherence between sampling matrix and
sparse dictionaries under the framework of model-based image
restoration. In this work, we will show how to unfold this
NARM into a DNN-based implementation.

where v is an auxiliary splitting variable. It follows that Eq.
(9) boils down to alternatively solving two sub-problems asso-
ciated with the ﬁdelity and regularization terms respectively,

x(t+1) = argmin

x

||y − Ax||2

2 + η||x − v(t)||2
2

(10a)

v(t+1) = argmin

v

η||x(t+1) − v||2

2 + λΩ(v)

(10b)

The main idea behind deep unfolding network is that
conventional iterative soft-thresholding algorithm (ISTA) in
sparse coding can be implemented equivalently by a stack
of recurrent neural networks [17]. Such correspondence has
inspired a class of convolutional neural network (CNN)-based
image denoising techniques [30]. CNN-based denoising prior
was later extended into other model-based image restoration
problems in Image Restoration via CNN (IRCNN) [31]. In
IRCNN, a CNN module is adopted to solve Eq.(10b) while
Eq.(10a) is solved in close-form by

x(t+1) = W −1b,

(11)

where W represents the matrix related to the degradation
matrix A and b is referring to ηv(t) + AT . Note that it is
often time-consuming to calculate an inverse matrix. Based on
such observation, a computationally more efﬁcient unfolding
strategy was developed in DPDNN [20]. DPDNN addressed
the problem of matrix inverse by solving Eq.(10a) in a different
way - i.e., they propose to compute x(t+1) with a single step
of gradient descent by

x(t+1) = xt − δ[AT (Ax(t) − y) + η(x(t) − v(t))]

= ¯Ax(t) + δAT y + δηv(t)

(12)

where ¯A = [(1 − δη)I − δAT A] can be pre-computed (so
the updating of x(t) can be done efﬁciently). As shown in
[20], we only need to obtain an approximated solution to
the x-subproblem, which can be done in a computationally
efﬁcient manner. Although DPDNN [20] has found several
image restoration applications including SISR, its network
architecture remains primitive (e.g., lacking dense and skip
connections) and there is still room for further optimization.

III. MODEL-GUIDED DEEP UNFOLDING NETWORK
In this paper, we take another step forward by showing a
generalized unfolding strategy applicable to almost any model-
based image restoration including both nonlocal and noncon-
vex regularization. It is well known that the class of nonconvex
optimization problems do not admit computationally efﬁcient
solutions, which calls for convex relaxation or approximation
[32]. However, we note that nonconvex cost functions have
become almost the default option in the ﬁeld of machine
learning [33] especially deep learning [34]. This is likely due
to the fact that the training of deep neural networks (DNN)
has a natural and intrinsic connection with the optimization of
cost functions by numerical methods such as gradient descent
methods [27] (e.g., the popular Adam optimization represents
a stochastic gradient descent method). Here, we have chosen
a previous reference model (NARM originally designed for
image interpolation [26]) to showcase the process of Model-
Guided (MoG) design. Thanks to the improved coherence
property of NARM, unfolding this model has the potential of
alleviating some long-standing open problems in SISR such
as the suppression of aliasing artifacts.

A. Deep Unfolding Network for NARM Model

When compared with the image prior model in Eq. (8),
NARM is more sophisticated because it involves two regular-
ization terms: one speciﬁed by the denoising prior (a variation

4

of DPDNN [20]) and the other related to the nonlocal AR
model [26] (a brand-new design). Formally, we consider the
following nonconvex optimization problem

x = argmin

x

||y − Ax||2

2 + µ||y − ASx||2

2 + λΩ(x)

(13)

where Ω(x) and Sx denote denoising-based prior and
NARM-deﬁned prior respectively. Using Half-Quadratic Split-
ting [29], we can rewrite the above optimization problem into

(x, v) = argmin

x,v

||y − Ax||2

2 + µ||y − ASx||2

2 + η||x − v||2

2 + λΩ(v)

where v is the auxiliary variable. It is well known that the
above optimization problem can be solved by alternatively
solving two subproblems related to x and v as shown in Eqs.
(10a) and (10b) [20].

(14)

Based on the NARM model in Eq. (4), we have Sx = x+e
where e = −ex is the modeling error (regardless of sign ﬂip).
It follows that the NARM optimization problem in Eq. (13)
can be translated into
(x, v, e) = argmin

2 + µ||y − A(x + e)||2
2

||y − Ax||2

x,v,e

(15)

+ γ||Sx − (x + e)||2

2 + η||x − v||2

2 + λΩ(v)

As an extension of previous result [20], we note that the newly-
formulated NARM-based optimization problem can be solved
by alternatively solving the following three sub-problems
2 + µ||y − A(x + e(t))||2
2

x(t+1) = argmin

||y − Ax||2

x

+ γ||Sx − (x + e(t))||2

2 + η||x − v(t)||2
2

(16a)

e(t+1) = argmin

e

µ||y −A(x(t) +e)||2

2 +γ||x(t) +e−Sx(t)||2
2
(16b)

v(t+1) = argmin

v

η||x(t) − v||2

2 + λΩ(v)

(16c)

In traditional model-based approaches [35], [36], alter-
natively solving the above three equations requires many
iterations to converge leading to prohibitive computational
cost. Meantime, the regularization functions and the hyper-
parameters cannot be jointly optimized in an end-to-end
manner. To address those issues, we propose to unfold the
NARM-based optimization in Eq. (16) into a concatenation of
repeating network modules as shown in Fig. 3 (a). Different
with previous work DPDNN [20],
the proposed network
architecture of our MoG-DUN incorporates two new modules
(nonlocal-AR and reconstruction) in addition to the denoising
module. Note that both short and long skip connections [37]
are incorporated to facilitate the information ﬂow across dif-
ferent stages. Furthermore, we have shown the corresponding
relationship between the Iterative algorithm implemented by
Eq. (16) and our proposed MoG-DUN network in Fig. 3 (f),
where the solutions (16c,20,21) of iterative updating variables
v(t+1), e(t+1), x(t+1) exactly match the corresponding mod-
ules of proposed deep unfolding network. To facilitate the
visual inspection, we have highlighted the core modules in
our MoG-DUN by red color.

5

HR (PSNR)

EDSR [6]

(23.78dB)

DPDNN [20]

(23.49dB)

RDN [9]

(23.81dB)

Img 046 from Urban100 ×4

RCAN [10]

(24.00dB)

D-DBPN [8]

(23.52dB)

SRFBN [13]

(23.85dB)

Ours (24.15dB)

Fig. 4: Visual quality and PSNR comparisons of different SISR methods for a sample image in the U rban100 dataset (bicubic-
downsampling, ×4).

The T repeating stages in MoG-DUN exactly executes
T iterations of Eq. (16). In our current implementation, a
total of T = 4 stages was adopted. Each stage of MoG-
DUN consists of three basic building blocks: U-net based
deep denoising module, a fast nonlocal-AR module, and a
versatile reconstruction module. The deep denoising module
is responsible for the updating of auxiliary variable v(t+1)
as described in Eq. (16); the fast nonlocal-AR module cal-
culates the NARM matrix S as deﬁned by Eq. (5) and the
corresponding Sx(t) in a computationally efﬁcient manner.
The versatile reconstruction module takes auxiliary variable
v(t+1) and nonlocal AR model Sx(t) as inputs and output
reconstructed image x(t+1). Then, the updated x(t+1) is fed
into the next stage to reﬁne the estimate of v and S again. The
denoising module, nonlocal-AR module and reconstruction
module are alternatively updated T times until reaching the
ﬁnal reconstruction. We will elaborate on each module next.

B. Deep Denoising Module via Dense RNN

In general, any existing image denoising network can be
used as the denoising module here. In this paper, we have
adopted a variant of U-net as the backbone of our denoising
module. The reasons are summarized as follows. First, the U-
net has been demonstrated to have great performance in the
domain of image restoration [20] including the task of image
denoising. Second, the U-net structure gradually decreases the
spatial resolution of feature maps along the encoding process
leading to reduced computation. Last, the U-net structure can
exploit the multi-scale redundancies of natural images, which
facilitates the recovery of spatially high-frequency information
such as edges and textures.

Alternatively, our design of denoising module can be viewed
as recurrent neural network (RNN) [38] as shown in Fig. 3.
Let h1, ..., ht denote the hidden states of t stages, which
will be used in the next stages of dense RNN. To better
illustrate this particular structure, we have shown the dense
RNN structure with more details in Fig. 5. It can be seen that
multiple hidden states are reused by the denoising module,
which is equivalent to the implementation by a RNN structure.

Fig. 5: The illustrations of the Dense RNN in the proposed
network. The blue block represents deep denoising module
called Dense RNN (h1, ht−1, ht denote the hidden states of
corresponding stages).

However, different from exiting RNN methods [5], [39], [13]
for SISR receiving only one state ht−1 of former stage, we
propose to leverage multiple states h1, ..., ht−1 of former
stages through long connections. As shown in Fig. 3 (a), the
processed information can be leveraged to reﬁne the current
the (t + 2)-th stage by receiving
image reconstruction at
former states h1, ..., ht−1, ht, ht+1 at previous t + 1 stages
[40]. Exploiting the hidden states of previous stages allows
us to more faithfully reconstruct the missing high-frequency
information for SISR (please refer to Figs. 4-7 for concrete
image examples).

As shown in Denoising Module of Fig. 3(a), the encoder
part consists of four encoding blocks (EB) and four decoding
blocks (DB). Except the last EB, each EB is followed by a
downsampling layer that sub-samples the feature maps with
scaling factor of two along both axes to increase the receipt
ﬁeld of neurons. As shown in the left part of Fig. 3(b), each
EB consists of three convolutional layers with 3 × 3 kernels, a
residual layer and ReLU nonlinearity to generate 64 channel
feature maps. The decoder reconstructs the image with four
DBs, each of which contains three convolutional layers and
a residual layer as shown in the right part of Fig. 3 (b).
Except the last DB, each DB is followed by a deconvolution
layer to increase the spatial size of feature maps by a scaling
factor of two. To compensate the lost spatial information,

upsampled feature maps are concatenated with the feature
maps of the same spatial dimension from the encoder. Thanks
to the transparency of our design, all dense-RNN module in
the T stages share the same network parameters.

C. Fast Nonlocal-AR Module

The nonlocal-AR module corresponds to the unfolding
of NARM matrix S into a network implementation. Based
on the observation that natural
images often contain rich
repetitive structures, nonlocal similarity has shown effective
for recovering missing high-frequency information in SISR.
In model-based implementation, ﬁnding similar patches is
often the computational bottleneck because nearest-neighbor
search is an NP-hard problem [41]. By contrast, calculating
the nonlocal relationship among image patches can be imple-
mented efﬁciently in parallel by nonlocal neural networks [42].
Inspired by the design of nonlocal operation in [42] and its
application into image restoration [43], we have designed a fast
nonlocal operation module for computing NARM matrix S
here. Fig. 3 (c) illustrates the block diagram of implementing
a non-local operation (highlighted by orange color) designed
for computing the similarity for a given image. Following
the formulation in nonlocal-mean ﬁltering [22] and bilateral
ﬁltering [44], a non-local operation can be deﬁned as

(cid:88)

yi = (

∀j

f (xi, xj)g(xj))/

(cid:88)

∀j

f (xi, xj)

(17)

where i is the index of an output position (e.g., in space or
time), the j is the enumeration of all possible positions, and
the pairwise similarity function f calculates the relationship
between i and all j. Similar to [43], we only calculate the q×q
block centered at position i instead of the whole image. For
a balanced trade-off between cost and performance, we have
chosen q = 15.

The design of similarity function f has been considered
in [42]. For example, an embedded Gaussian function can be
used to calculate similarity

f (xi, xj) = e(θ(xi)T φ(xj ))

(18)

where θ(xi) = Wθxi, φ(xj) = Wφxj, and Wθ, Wφ are the
weight matrices. For simplicity, we opt to employ a linear
embedding for g(xj) = Wgxj, where Wg is a learnable
weight matrix. Then the output of nonlocal-AR block Sxi
is calculated by

Sxi = Wωyi + xi = Wωσ[θ(xi)T φ(xj)]g(xj) + xi

(19)

where σ denotes the sof tmax operator and Wω is the embed-
ding weight matrix. Note that the pairwise computation of a
non-local block enjoys the beneﬁt of being lightweight because
its computational cost implemented by matrix multiplication
is comparable to a typical convolutional layer in standard net-
works. Moreover, pairwise computation of nonlocal blocks can
be used in high-level, sub-sampled feature maps (e.g., using
the subsampling trick as described in [42]). As shown in [45],
nonlocal module admits even more efﬁcient implementations
by considering a compact representation for multiple kernel
functions with Taylor expansion.

6

D. Versatile Reconstruction Module

With the output of denoising module v(t+1) and nonlocal-
AR module Sx(t), we can reconstruct the updated image
x(t+1) in two steps. First, we need to calculate e(t+1) by
solving Eq. (16b) using a single step of gradient descent

e(t+1) = e(t) − δ[µAT (A(x(t) + e(t)) − y) + γ(x(t) + e(t) − Sx(t))]
(20)
where δ is a parameter controlling the step size of convergence.
Then, we can reconstruct new x(t+1) with updated e(t+1)
and v(t+1) by solving Eq. (16a) using another single step of
gradient descent

x(t+1) =x(t) − δ

(cid:48)

[AT (Ax(t) − y) + µAT (A(x(t) − e(t+1)) − y)

+ γ(x(t) + e(t+1) − Sx(t)) + η(x(t) − v(t+1))]

(21)

where δ

(cid:48)

is another relaxation parameter.

Note that Eqs. (20),(21) still involve sophisticated degra-
dation matrix A that is expensive to calculate. In DPDNN
the pair of operators A and AT were replaced by
[20],
downsampling and upsampling operators but at
the price
of limited modeling capability (e.g.,
they can not handle
multiple degradation kernels [12]). Different from DPDNN,
we propose a more versatile design here - i.e., to simulate
the forward and inverse process of degradation by a shallow
four-layer convolutional network. Speciﬁcally, the degradation
process A is simulated by a network called Down-Sampling-
network (DS-net) consisting of three convolutional layers with
3 × 3 kernels and 64 channels and one convolutional layer
to decrease the spatial resolution with corresponding scale as
shown in Fig. 3 (d). In a similar way, the network called Up-
Sampling-network (US-net) representing AT consists of three
convolutional layers with 3 × 3 kernels and 64 channels and
one deconvolution layer to increase the spatial resolution with
corresponding scale as shown in Fig. 3 (e) .

The versatility of our newly-design reconstruction module
is further demonstrated by its capability of handling multiple
blur kernels [12]. Dealing with multiple degradations is highly
desirable in practical SISR scenarios where image degradation
is complex and even spatially-varying. In order to handle
multiple blur kernels, one can expand the blur kernel to the
same spatial dimension as the input images using a strategy
called “dimensionality stretching” [12]. Speciﬁcally, assuming
the blur kernel is sized by k × k, the blur kernel is ﬁrst
stretched into a vector of size k2 × 1 and then projected
onto a d−dimensional(d < k2) linear space by the principal
component analysis (PCA) to reduce the computation. This
way, the blur kernel map consisting of size d × H × W is
stretched from the original blur kernel of size k × k, where H
and W denote the height and width of corresponding variable.
Along the channel dimension, we concatenate the stretched
blur kernel maps with the corresponding intermediate result
which needs to be upsampled by a US-net or downsampled
by a DS-net. Then, the concatenated feature maps are fed into
the corresponding US-net or DS-net. With the Reconstruction
Module, our model can achieve great performance in terms
of handling multiple different degradations caused by varying
blur kernels (please refer to Table VI).

7

HR (PSNR)

EDSR [6] 27.58dB)

DPDNN [20]

(26.95dB)

RDN [9]

(27.82dB)

Img 078 from Urban100 ×4

RCAN [10]

(27.92dB)

D-DBPN [8]

(26.95dB)

SRFBN [13]

(27.73dB)

Ours (28.29dB)

Fig. 6: Visual quality and PSNR comparisons of different SISR methods for a sample image in the U rban100 dataset (bicubic-
downsampling, ×4).

HR (PSNR)

EDSR [6]

(28.38dB)

DPDNN [20]

(28.21dB)

RDN [9]

(28.38dB)

Img 002 from Set14 ×2

RCAN [10]

(28.44dB)

D-DBPN [8]

(28.40dB)

SRFBN [13]

(29.27dB)

Ours (30.16dB)

Fig. 7: Visual quality and PSNR comparisons of different SISR methods for a sample image in the Set14 dataset (bicubic-
downsampling, ×2).

IV. EXPERIMENTAL RESULTS

A. Experimental Settings

Benchmark Datasets and Performance Metrics. We
have used 800 high-quality (2K resolution) images from the
DIV2K dataset [46] for training. Following [1], [13], [6],
[9], [12], ﬁve standard benchmark datasets: Set5 [47], Set14
[48], BSD100[49], Urban100 [50], Manga109 [51]are used for
testing. Performance evaluation in terms of PSNR and SSIM
[52] metrics is conducted on the luminance (Y) channel only.
Degradation Models. In order to demonstrate the robust-
ness of our model in varying degradation scenarios, we have
designed the following experiments with different parameter
settings with the degradation model.

• Default setting. This is the scenario considered in most
previous SISR studies - i.e.,
the low-resolution (LR)
image is obtained by bicubic downsampling of the high-
resolution (HR) image. The downsampling ratio is usually
a small positive integer (×2,×3,×4).

• Interpolation setting. This is the scenario consistent with
the NARM study [26] in which a LR image is directly
down-sampled from the HR image without any anti-

aliasing ﬁltering involved. Due to the presence of aliasing,
this scenario is generally believed to be more difﬁcult
than the default setting.

• Realistic setting. To more faithfully characterize the
degradation in the real world, this setting aims at simu-
lating multiple degradation situations caused by different
Gaussian kernels [12]. Similar to SRMDNF [12], we have
obtained a single trained network through reconstruction
module taking multiple degradation kernels. The set
of degradation kernels include isotropic Gaussian blur
kernels maps whose width ranges are set to [0.2, 3],
[0.2, 3] and [0.2, 4] with scale factor ×2, ×3 and ×4
respectively. The low-resolution (LR) image y is obtained
by y = (x ⊗ k) ↓, where x represents HR image, ⊗
represents the convolution operator, k the blur kernel
and ↓ the downsampling operator. The kernel width is
uniformly sampled in the above ranges and the kernel
size is ﬁxed to 21 × 21 and the projected d−dimensional
linear space is set to 6, 8, 10 with scale factor ×2, ×3,
×4 respectively. The mean values of kernel width are
0.5, 1.3, 2.6 as shown in Table VI.

Training Setting. Thanks to the parameter sharing across T
stages, the overall MoG-DUN can be trained in an end-to-end
manner. In order to further reduce the number of parameters
and avoid over-ﬁtting, we enforce deep denoising module
(dense-RNN) and reconstruction module to share the same
parameters. Unlike DPDNN [20] adopting MSE loss, we have
found L1 loss function works better for training the proposed
MoG-DUN (e.g., it can facilitate the recovery of more high-
frequency information). The L1-based loss function can be
expressed as:

Θ = argmin

θ

N
(cid:88)

i=1

(cid:107)F(yi, k; Θ) − xi(cid:107)1,

(22)

where yi and xi denote the i-th pair of degraded and original
image patches respectively, k denotes the expanded blur kernel
maps (as explained in Section III-D) when dealing with mul-
tiple degradations, and F(yi, k; Θ) denotes the reconstructed
image patch by the network with the parameter set Θ. We
randomly select 16 RGB LR patches sized by 48 × 48 as the
inputs and stretch the blur kernels when dealing with multiple
degradations (called “dimensionality stretching” in [12]). The
image patches are randomly rotated by 90◦, 180◦, 270◦and
ﬂipped horizontally as standard data augmentation techniques
do. The ADAM algorithm [53] with β1 = 0.9, β2 = 0.999, (cid:15) =
10−8 is adopted to optimize the network. The initial learning
rate is 10−4 and decreases by half for every 300 epochs. Our
network is implemented under the Pytorch framework and the
training time takes less than 2days using 4 NVIDIA 1080Ti
GPUs.

B. Ablation Study

To further verify the effectiveness of nonlocal-AR module,
we have conducted an ablation study to compare the PSNR
performance of MoG-DUN with and without nonlocal-AR
module. In our ablation study, we have used directly down-
sampling degradation with different Gaussian kernel size of
0.5, 1.0 and ×3 directly downsampling on four frequently-used
benchmark datasets. As shown in Tab. II, the nonlocal-AR
module does make a contribution to the overall performance
of MoG-DUN.

To investigate the inﬂuence of those dense connections as
shown in Fig. 3 (a), we have conducted an ablation study to
compare the PSNR performance of MoG-DUN with and with-
out dense connections for directly downsampling degradation
at the scaling factor of ×3. As shown in Tab. III, we see
that dense connections contribute to the PSNR gain of about
0.16 dB on the average.

To explore the impact of the number of unfolded stages on
the SISR performance, we have conducted another experiment
with varying the parameter T . Fig. 8 shows the average PSNR
results of different stages T from two to six with ×2,×3
and ×4 bicubic-downsampling. It can be seen that the PSNR
increases as the number of stages increases. However, the
PSNR improvement rapidly saturates when T (cid:62) 4, which
justiﬁes the choice of T = 4 in our implementation to balance
the performance and computational complexity.

8

Fig. 8: The average PSNR performance as a function of
parameter T (the total number of Unet stages) of proposed
MoG-DUN with ×2, ×3, ×4 bicubic-downsampling on Set5
[47].

We have conducted experiments with the case where the
parameters are not shared in three (×2, ×3, ×4) bicubic-
downsampling settings when T = 4. As shown in Table IV,
disabling parameter sharing does not lead to any noticeable
performance gain at the price of quadrupled number of pa-
rameters. Based on such experimental ﬁnding, we conclude
that parameter sharing is a good strategy for MoG-DUN. It is
also worth mentioning that parameter sharing has also been
widely considered as an effective strategy of shrinking the
optimization gap in neural architecture search [55].

C. Experimental Results for the Default Setting

For bicubic downsampling, we have compared MoG-DUN
with eight state-of-the-art
image SR methods: EDSR [6],
DPDNN [20], DSRN [54], RDN [9], RCAN [10], D-DBPN
[8], SRMDNF [12], SRFBN [13], USRNet [21]. The average
PSNR and SSIM results of eight benchmark methods in Tab.I
are cited from corresponding papers. It is easy to see that our
method is superior to most of competing methods in terms of
PSNR and SSIM values. When compared with a much deeper
network RCAN [10] involving over 400 convolutional layers,
we can achieve highly comparable and sometimes even better
results.

The image comparison results for a scale factor of ×4 are
reported in Fig. 4. For this speciﬁc example, our SR-resolved
result of ‘Img 046’ from U rban100 are recovered with fewer
visible artifacts (e.g., the glassy surface on the right side of the
building) than other competing methods. Note that our PSNR
result is also noticeably higher than the previous state-of-the-
art RCAN at a lower cost. In another challenging example
(‘Img 078’ from U rban100 dataset), our method can recover
much more faithful textured details as shown in Fig.6; while all
other competing methods suffer from severe aliasing artifacts
(i.e., distorted tile patterns). The visual quality improvement
achieved by MoG-DUN is mainly due to the fact that our

TABLE I: Average PSNR and SSIM results for bicubic downsampling degradation on ﬁve benchmark datasets. The best
performance is shown in bold and the second best performance is shown in underline.

9

Set5 [47]

Set14 [48]

BSD100 [49]

Urban100 [50]

Method

Scale

EDSR[6]
DPDNN[20]
DSRN [54]
RDN[9]
RCAN [10]
D-DBPN[8]
SRMDNF[12]
SRFBN[13]
USRNet[21]
MoG-DUN(ours)
EDSR[6]
DPDNN[20]
DSRN [54]
RDN[9]
RCAN [10]
SRMDNF[12]
SRFBN[13]
USRNet[21]
MoG-DUN(ours)
EDSR[6]
DPDNN[20]
DSRN [54]
RDN[9]
RCAN [10]
D-DBPN[8]
SRMDNF[12]
SRFBN[13]
USRNet[21]
MoG-DUN(ours)

×2
×2
×2
×2
×2
×2
×2
×2
×2
×2
×3
×3
×3
×3
×3
×3
×3
×3
×3
×4
×4
×4
×4
×4
×4
×4
×4
×4
×4

PSNR
38.11
37.75
37.66
38.24
38.27
38.09
37.79
38.11
37.72
38.25
34.65
33.93
33.88
34.71
34.74
34.12
34.70
34.45
34.76
32.46
31.72
31.40
32.47
32.63
32.47
31.96
32.47
32.45
32.60

SSIM PSNR
33.92
0.9602
33.30
0.9600
33.15
0.9590
0.9614
34.01
34.12
0.9614
33.85
0.9600
33.32
0.9601
33.82
0.9609
33.49
-
0.9614
34.00
30.52
0.9280
30.02
0.9240
30.26
0.9220
30.57
0.9296
30.65
0.9299
30.04
0.9254
30.51
0.9292
30.51
-
0.9300
30.63
28.80
0.8968
28.28
0.8890
28.07
0.8830
28.81
0.8990
28.87
0.9002
28.82
0.8990
28.35
0.8925
28.81
0.8983
28.83
-
28.84
0.8998

SSIM PSNR
32.32
0.9195
32.09
0.9150
32.10
0.9130
32.34
0.9212
32.41
0.9216
32.27
0.9190
32.05
0.9159
32.29
0.9196
32.10
-
32.37
0.9205
29.25
0.8462
29.00
0.8360
28.81
0.8370
29.26
0.8468
29.32
0.8482
28.97
0.8382
28.81
0.8461
29.18
-
29.24
0.8479
27.71
0.7876
27.44
0.7730
27.25
0.7700
27.72
0.7871
27.77
0.7889
27.72
0.7860
27.49
0.7787
27.72
0.7868
27.69
-
0.7873
27.70

SSIM PSNR
32.93
0.9013
31.50
0.8990
30.97
0.8970
32.89
0.9017
33.34
0.9027
32.55
0.9000
31.33
0.8985
32.62
0.9010
31.79
-
32.75
0.9020
28.80
0.8093
27.61
0.8010
27.16
0.7970
28.80
0.8093
29.09
0.8111
27.57
0.8025
28.73
0.7868
28.38
-
28.82
0.8094
26.64
0.7420
25.53
0.7290
25.08
0.7240
26.61
0.7419
26.82
0.7436
26.38
0.7400
25.68
0.7337
26.60
0.7409
26.44
-
26.63
0.7403

SSIM PSNR
39.10
0.9351
-
0.9220
-
0.9160
39.18
0.9353
39.44
0.9384
38.89
0.9324
38.07
0.9204
39.09
0.9328
-
-
0.9421
39.37
34.17
0.8653
-
0.8420
-
0.8280
34.13
0.8653
34.44
0.8702
33.00
0.8398
34.18
0.8641
-
-
34.34
0.8651
31.02
0.8033
-
0.7680
-
0.7470
31.00
0.8028
0.8087
31.22
30.91
0.7946
30.09
0.7731
31.15
0.8015
-
-
31.26
0.8016

Manga109 [51]
SSIM
0.9773
-
-
0.9780
0.9786
0.9775
0.9761
0.9779
-
0.9783
0.9476
-
-
0.9484
0.9499
0.9403
0.9481
-
0.9490
0.9148
-
-
0.9151
0.9173
0.9137
0.9024
0.9160
-
0.9169

TABLE II: Average PSNR results with and without nonlocal-
AR module for directly downsampling degradation ×3 with
frequently-used
different Gaussian kernels width on four
benchmark datasets.

Methods
w/o NL-AR
w NL-AR
w/o NL-AR
w NL-AR

Kernel Width
0.5
0.5
1.0
1.0

Set5
32.86
32.98
34.15
34.34

Set14
28.99
29.11
30.15
30.23

BSD68
28.00
28.12
28.90
28.99

Urban100
27.27
27.51
28.37
28.63

TABLE III: Average PSNR results with and without dense
connections of denosing module for directly downsampling
degradation ×3.

Methods
w/o dense connections
w dense connections

Set5
32.19
32.36

Set14
28.10
28.21

BSD68
27.53
27.64

Urban100
26.92
27.15

proposed model makes full use of the feature maps from
former stages to reﬁne the ﬁnal result. Taking one more classic
example known for its notorious aliasing distortion, Fig. 7
shows the results of bicubic ×2 degradation of ‘Img 002’
from the Set14 dataset. Additionally, we have shown the
intermediate image comparison results of different stages in
Fig. 9, from which we can see that more high-frequency
information has been recovered along with the increasing
number of stages.

TABLE IV: Average PSNR results with and without param-
eters sharing of denosing module for bicubic downsampling
degradation with ×2, ×3, ×4 on Set5 datasets.

Methods
w/o params sharing
w params sharing

×2
38.27
38.25

×3
34.79
34.76

×4
32.63
32.60

D. Experimental Results for the Interpolation Setting

For directly downsampling degradation, we have compared
our model with ﬁve state-of-the-art image SR methods: EDSR
[6], DPDNN [20], RDN [9], RCAN [10], SRFBN [13]. The
average PSNR and SSIM results for three scaling factors of
×2, ×3, ×4 are shown in Tab.V. The PSNR/SSIM results of
ﬁve benchmark methods are retrained from the original source
codes released by their authors. From Tab.V, we can observe
that the proposed method is consistently superior to all ﬁve
benchmark methods in terms of both PSNR and SSIM values.
Subjective quality comparison results for a cartoon image at
the scale factor of ×4 are shown in Fig.10. Apparently, our SR
result of ‘Img 109’ is the closest to that of the ground-truth
both subjectively and objectively; the PSNR gain over other
competing methods is over 1.4dB for this speciﬁc case.

E. Experimental Results for the Realistic Setting

This scenario is arguable more challenging than the previous
two due to the uncertainty with multiple degradation kernels.
Five state-of-the-arts methods: EDSR [6], SRMDNF [12],

10

HR

T=1

T=2

T=3

T=4

Fig. 9: SISR intermediate visual results of different stages on ‘Img 003’ from Set5 [50] (bicubic-downsampling, ×3).

TABLE V: Average PSNR and SSIM results for directly downsampling degradation on ﬁve benchmark datasets. The best
performance is shown in bold.

Set5 [47]

Set14 [48]

BSD100 [49]

Urban100 [50]

Method

Scale

EDSR[6]
DPDNN[20]
RDN[9]
RCAN [10]
SRFBN[13]
MoG-DUN(ours)
EDSR[6]
DPDNN[20]
RDN[9]
RCAN [10]
SRFBN[13]
MoG-DUN(ours)
EDSR[6]
DPDNN[20]
RDN[9]
RCAN [10]
SRFBN[13]
MoG-DUN(ours)

×2
×2
×2
×2
×2
×2
×3
×3
×3
×3
×3
×3
×4
×4
×4
×4
×4
×4

PSNR
35.57
35.51
35.77
35.63
35.66
35.99
31.50
31.56
31.91
31.81
31.89
32.36
28.94
28.94
29.24
29.13
29.09
29.65

SSIM PSNR
31.30
0.9444
31.26
0.9445
31.38
0.9458
31.34
0.9449
31.35
0.9450
31.77
0.9474
27.79
0.8992
27.82
0.9004
28.00
0.9039
27.93
0.9026
27.96
0.9033
28.21
0.9095
25.98
0.8528
25.97
0.8539
26.16
0.8608
26.18
0.8580
26.07
0.8583
26.35
0.8706

SSIM PSNR
30.21
0.8864
30.13
0.8865
30.36
0.8870
30.30
0.8867
30.28
0.8866
30.53
0.9333
27.19
0.7976
27.13
0.7991
27.38
0.8017
27.33
0.7994
27.32
0.8012
27.64
0.8059
25.66
0.7326
25.57
0.7341
25.86
0.7383
25.82
0.7374
25.72
0.7354
25.98
0.7448

SSIM PSNR
28.82
0.8666
28.79
0.8648
29.54
0.8692
29.19
0.8677
29.04
0.8670
30.44
0.8724
25.41
0.7651
25.47
0.7638
26.02
0.7716
25.98
0.7677
25.85
0.7686
27.15
0.7806
23.47
0.6961
23.44
0.6937
24.05
0.7053
23.95
0.7002
23.71
0.6996
24.70
0.7107

SSIM PSNR
35.25
0.8971
35.15
0.8972
35.67
0.9071
35.59
0.9024
35.42
0.9002
36.33
0.9200
29.13
0.8038
29.23
0.8057
29.91
0.8204
29.68
0.8188
29.66
0.8174
30.65
0.8483
26.04
0.7303
25.96
0.7309
26.59
0.7527
26.35
0.7486
26.32
0.7402
26.99
0.7744

Manga109 [51]
SSIM
0.9446
0.9641
0.9658
0.9651
0.9648
0.9689
0.9109
0.9121
0.9189
0.9165
0.9172
0.9279
0.8567
0.8585
0.8688
0.8643
0.8648
0.8776

Img 109 from Manga109

HR (PSNR)

EDSR [6]

(24.55dB)

DPDNN [20]

(24.49dB)

RDN [9]

(24.65dB)

RCAN [10]

(24.76dB)

SRFBN [13]

(24.46dB)

Ours (26.17dB)

Fig. 10: SISR visual quality comparisons of different methods on ‘Img 109’ from Manga109 [51]. The degradation is directly
downsamping with scale factor ×4.

RDN[9], RCAN [10], SRFBN [13] are used to demonstrate
the effectiveness of our model. For this new degradation
assumption, we have to retrained their models by either
released source codes or published papers to generate the
ﬁnal results. The benchmark methods EDSR,RDN,RCAN and
SRFBN only take the LR images as inputs and lack the
ability of handling multiple degradation. For a fair comparison,
we have transformed those method into handling multiple
degradations by taking LR images concatenated with expanded
blur kernels maps as the inputs, following SRMDNF[12].

Tab. VI shows the average PSNR results of three differ-
ent Gaussian kernel widths 0.5, 1.3, 2.6 and three different
scaling factors ×2, ×3, ×4 respectively. Four commonly used
benchmark datasets are adopted to verify the effectiveness
of our method. Tab. VI demonstrates that our method can

achieve better performance than other competing methods.
One can be seen that our method handles multiple degradations
better than others. It is worth mentioning that the performance
for the kernel width of 0.5 is not as good as that for the
kernel width of 1.3. One possible explanation is that more
information might get lost by down-sampling when Gaussian
kernel width is 0.5. The visual quality comparisons are given
in Fig. 11 and Fig. 12. Fig. 11 shows that SRFBN [13] and
our method can recover sharper edges than other methods.
However, SRFBN [13] generates more twisty artifacts than
ours. From Fig. 12, we can observe that our method can
achieve a clearer image with sharper edges than other bench-
mark methods, which justiﬁes the superiority of our approach.
Tab. V and Tab. VI show the results of directly downsampling
degradation and multiple degradation which degradation ma-

trix lacks structural constraint [26] causing coherence. The
reference model, nonlocal autoregressive model(NARM) with
improved incoherence properties by connecting a pixel with
its nonlocal neighbors has been integrated into our model as
trainable Nonlocal-AR module. Obviously, our proposed model
can achieve better performance than other methods with the
beneﬁts of relieving coherence by Nonlocal-AR module aiming
to break coherence as well as effective model-guild designed
network.

F. Cost-Performance Tradeoff

To demonstrate the trade-off between the cost (in terms of
the number of parameters) and the performance (as measured
by PSNR values), we have compared this work against nine
existing SISR methods in Fig.1. We can see that our model
can achieve better PSNR performance than those models with
comparable model size (e.g., SRFBN [13] and DPDNN [20])
or signiﬁcant cost savings over those models with comparable
PSNR performance (e.g., RCAN [10] and RDN [9]). In
addition, we have compared the actual running time as well
as ﬂops against other competing methods in Table. VII. The
actual running time is the total
time of whole Urban100
dataset during the testing and the ﬂops is evaluated on the
LR input size 1 × 3 × 64 × 64. It can be observed that
our model has a similar ﬂops-running time performance to
RDN [9]. Although our proposed model does not achieve the
best performance in terms of ﬂops and running time, it still
have notable advantages over four other competing methods
including EDSR[6], DPDNN[20], SRFBN[13], USRNet[21].

V. CONCLUSION

In this paper, we have demonstrated how to unfold the
existing NARM into a multi-stage network implementation
that is both explainable and efﬁcient. The unfolded network
consists of a concatenation of multi-stage building blocks
each of which is decomposed of a deep denoising module,
a fast nonlocal-AR module, and a versatile reconstruction
module. This work extends the previous work DPDNN [20]
in the following aspects. First, the new regularization term
characterized by NARM leads to a three-way (instead of
double-headed) alternating optimization, which in principle is
applicable to other forms of regularization functions. Mean-
time, the improved incoherence property of NARM makes
it suitable for SISR applications particularly on suppressing
aliasing artifacts. Second, the unfolded network allows the
hidden states of previous stages to be exploited by the later
stage. Such densely connected recurrent network architecture
is shown important to the recovery of missing high-frequency
information in SISR. Extensive experimental results have
been reported to show that our MoG-DUN is capable of
achieving an improved trade-off between the cost (in terms of
network parameter size) and the performance (in terms of both
subjective and objective qualities of reconstructed SR images).
Currently, we are exploring further improvement based on
recently developed SISR method focusing on learning high-
level features via densely residue Laplacian network [56].

11

REFERENCES

[1] C. Dong, C. C. Loy, K. He, and X. Tang, “Learning a deep convolu-
tional network for image super-resolution,” in European Conference on
Computer Vision, 2014, pp. 184–199.

[2] C. Ledig, L. Theis, F. Husz´ar, J. Caballero, A. Cunningham, A. Acosta,
A. Aitken, A. Tejani, J. Totz, Z. Wang et al., “Photo-realistic single
image super-resolution using a generative adversarial network,” in
Proceedings of the IEEE conference on computer vision and pattern
recognition, 2017, pp. 4681–4690.

[3] J. Kim, J. Kwon Lee, and K. Mu Lee, “Accurate image super-resolution
using very deep convolutional networks,” in Proceedings of the IEEE
conference on computer vision and pattern recognition, 2016, pp. 1646–
1654.

[4] Y. Chen and T. Pock, “Trainable nonlinear reaction diffusion: A ﬂexible
framework for fast and effective image restoration,” IEEE Transactions
on Pattern Analysis Machine Intelligence, vol. 39, no. 6, pp. 1256–1272,
2017.

[5] J. Kim, J. K. Lee, and K. M. Lee, “Deeply-recursive convolutional
network for image super-resolution,” in 2016 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2016, pp. 1637–
1645.

[6] B. Lim, S. Son, H. Kim, S. Nah, and K. M. Lee, “Enhanced deep
residual networks for single image super-resolution,” in 2017 IEEE
Conference on Computer Vision and Pattern Recognition Workshops
(CVPRW), 2017, pp. 1132–1140.

[7] W.-S. Lai, J.-B. Huang, N. Ahuja, and M.-H. Yang, “Deep laplacian
pyramid networks for fast and accurate super-resolution,” in Proceedings
of the IEEE conference on computer vision and pattern recognition,
2017, pp. 624–632.

[8] M. Haris, G. Shakhnarovich, and N. Ukita, “Deep back-projection
networks for super-resolution,” in Proceedings of the IEEE conference
on computer vision and pattern recognition, 2018, pp. 1664–1673.
[9] Y. Zhang, Y. Tian, Y. Kong, B. Zhong, and Y. Fu, “Residual dense
network for image super-resolution,” in Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, 2018, pp. 2472–
2481.

[10] Y. Zhang, K. Li, K. Li, L. Wang, B. Zhong, and Y. Fu, “Image super-
resolution using very deep residual channel attention networks,” in
Proceedings of the European Conference on Computer Vision (ECCV),
2018, pp. 286–301.

[11] T. Dai, J. Cai, Y. Zhang, S.-T. Xia, and L. Zhang, “Second-order
attention network for single image super-resolution,” in Proceedings of
the IEEE conference on computer vision and pattern recognition, 2019,
pp. 11 065–11 074.

[12] K. Zhang, W. Zuo, and L. Zhang, “Learning a single convolutional
super-resolution network for multiple degradations,” in 2018 IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2018, pp.
3262–3271.

[13] Z. Li, J. Yang, Z. Liu, X. Yang, G. Jeon, and W. Wu, “Feedback network
for image super-resolution,” in Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 2019, pp. 3867–3876.
[14] J. Liu, W. Zhang, Y. Tang, J. Tang, and G. Wu, “Residual feature
aggregation network for image super-resolution,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2020, pp. 2359–2368.

[15] C. Molnar, Interpretable machine learning. Lulu. com, 2020.
[16] N. Ahn, B. Kang, and K.-A. Sohn, “Fast, accurate, and lightweight
super-resolution with cascading residual network,” in Proceedings of the
European Conference on Computer Vision (ECCV), 2018, pp. 252–268.
[17] S. Wisdom, T. Powers, J. Pitton, and L. Atlas, “Building recurrent net-
works by unfolding iterative thresholding for sequential sparse recovery,”
in 2017 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP).

IEEE, 2017, pp. 4346–4350.

[18] C. Bertocchi, E. Chouzenoux, M.-C. Corbineau, J.-C. Pesquet, and
M. Prato, “Deep unfolding of a proximal interior point method for image
restoration,” Inverse Problems, 2019.

[19] J. R. Hershey, J. L. Roux, and F. Weninger, “Deep unfolding:
Model-based inspiration of novel deep architectures,” arXiv preprint
arXiv:1409.2574, 2014.

[20] W. Dong, P. Wang, W. Yin, G. Shi, F. Wu, and X. Lu, “Denoising prior
driven deep neural network for image restoration,” IEEE transactions
on pattern analysis and machine intelligence, vol. 41, no. 10, pp. 2305–
2318, 2018.

[21] K. Zhang, L. Van Gool, and R. Timofte, “Deep unfolding network for
image super-resolution,” in IEEE Conference on Computer Vision and
Pattern Recognition, 2020, pp. 3217–3226.

TABLE VI: Average PSNR results for multiple degradation on four benchmark datasets. The best performance is shown in
bold.

12

Method

EDSR[6]
SRMDNF[12]
RDN[9]
RCAN[10]
SRFBN[13]
MoG-DUN(ours)
EDSR[6]
SRMDNF[12]
RDN[9]
RCAN[10]
SRFBN[13]
MoG-DUN(ours)
EDSR[6]
SRMDNF[12]
RDN[9]
RCAN[10]
SRFBN[13]
MoG-DUN(ours)

Kernel
Width
0.5
0.5
0.5
0.5
0.5
0.5
1.3
1.3
1.3
1.3
1.3
1.3
2.6
2.6
2.6
2.6
2.6
2.6

Set5 [47]
×3
32.37
31.30
32.72
32.62
32.56
32.94
33.87
32.58
34.26
34.06
34.09
34.51
33.07
31.83
33.50
33.28
33.15
34.64

×4
29.67
28.83
30.00
29.99
29.92
30.13
31.29
30.17
31.65
31.59
31.55
32.00
31.47
30.32
32.04
31.99
31.74
32.48

×2
35.52
35.01
36.46
36.37
36.31
36.60
37.51
35.67
37.45
37.44
37.39
38.11
33.79
32.28
33.61
33.70
33.64
37.91

Set14 [48]
×3
28.63
27.98
28.87
28.78
28.72
29.01
29.97
29.24
30.27
30.12
30.08
30.40
29.46
28.72
29.73
29.55
29.47
30.52

×4
26.61
26.04
26.94
26.79
26.88
26.97
28.01
27.31
28.36
28.27
28.26
28.46
28.18
27.45
28.56
28.50
28.36
28.85

×2
32.09
31.11
32.07
31.94
31.86
32.42
33.26
31.89
32.31
32.18
33.21
33.87
30.09
29.09
30.02
29.91
29.94
33.61

BSD100 [49]
×3
27.69
27.14
27.82
27.73
27.75
28.01
28.84
28.32
29.05
28.92
28.87
29.15
28.48
27.94
28.65
28.51
28.37
29.26

×2
30.94
30.12
30.95
30.87
30.78
31.09
31.98
31.00
32.05
31.96
31.89
32.19
29.02
28.34
29.04
28.98
28.89
32.30

×4
26.04
25.55
26.31
26.22
26.16
26.41
27.12
26.71
27.35
27.31
27.26
27.38
27.27
26.87
27.57
27.53
27.38
27.71

Urban100 [50]
×3
26.03
24.76
26.90
26.46
25.67
27.04
27.45
26.20
28.24
27.64
27.84
28.42
26.69
25.69
27.23
26.85
26.81
28.06

×2
29.77
27.66
29.89
29.49
28.57
30.53
30.69
28.60
30.94
30.57
30.66
31.73
26.89
25.62
26.93
26.80
26.78
30.85

×4
23.93
22.93
24.49
24.40
24.21
24.75
25.30
24.30
25.97
25.98
25.71
26.22
25.22
24.44
25.82
25.79
25.66
26.22

Img 039 from Urban100

HR (PSNR)

EDSR [6]

(23.97dB)

SRMDNF [12]

(22.96dB)

RDN [9]

(25.32dB)

RCAN [10]

(24.75dB)

SRFBN [13]

(25.03dB)

Ours (25.54dB)

Fig. 11: SISR visual quality comparisons of different methods on ‘Img 039’ from Urban100 [50]. The degradation involves
Gaussian kernel with kernel width 1.3 and direct downsamping with scale factor ×3.

Img 092 from Urban100

HR (PSNR)

EDSR [6]

(20.85dB)

SRMDNF [12]

(20.51dB)

RDN [9]

(21.90dB)

RCAN [10]

(21.82dB)

SRFBN [13]

(21.61dB)

Ours (25.62dB)

Fig. 12: SISR visual quality comparisons of different methods on ‘Img 092’ from Urban100 [50]. The degradation involves
Gaussian kernel with kernel width 2.6 and direct downsamping with scale factor ×2.

TABLE VII: The ﬂops and test running time results of ×2 bicubic downsampling.

Methods
ﬂops(G)
testing time(s)

EDSR[6]
166.9
27.25

DPDNN[20]
113.1
28.70

RDN[9]
90.6
20.76

RCAN[10]
62.9
15.71

D-DPBN[8]
61.8
17.03

SRFBN[13]
89.7
31.15

USRNet[21] MoG-DUN(ours)

151.9
82.59

96.4
21.46

[22] A. Buades, B. Coll, and J.-M. Morel, “A non-local algorithm for image
denoising,” in 2005 IEEE Computer Society Conference on Computer
Vision and Pattern Recognition (CVPR’05), vol. 2, no. 2, 2005, pp. 60–
65.

[23] J. Yang, J. Wright, T. S. Huang, and Y. Ma, “Image super-resolution via
sparse representation,” IEEE Transactions on Image Processing, vol. 19,
no. 11, pp. 2861–2873, 2010.

[24] W. Dong, L. Zhang, G. Shi, and X. Li, “Nonlocally centralized sparse
representation for image restoration,” IEEE Transactions on Image
Processing, vol. 22, no. 4, pp. 1620–1630, 2013.

[25] K. I. Kim and Y. Kwon, “Single-image super-resolution using sparse
image prior,” IEEE Transactions on Pattern
regression and natural
Analysis and Machine Intelligence, vol. 32, no. 6, pp. 1127–1133, 2010.
[26] W. Dong, L. Zhang, R. Lukac, and G. Shi, “Sparse representation
based image interpolation with nonlocal autoregressive modeling,” IEEE
Transactions on Image Processing, vol. 22, no. 4, pp. 1382–1394, 2013.
[27] Q. V. Le, J. Ngiam, A. Coates, A. Lahiri, B. Prochnow, and A. Y. Ng,

“On optimization methods for deep learning,” in ICML, 2011.

[28] B. Adcock, A. C. Hansen, C. Poon, B. Roman et al., “Breaking the
coherence barrier: asymptotic incoherence and asymptotic sparsity in
compressed sensing,” arXiv preprint arXiv:1302.0561, 2013.

[29] R. He, W.-S. Zheng, T. Tan, and Z. Sun, “Half-quadratic-based iterative
minimization for robust sparse representation,” IEEE transactions on
pattern analysis and machine intelligence, vol. 36, no. 2, pp. 261–275,
2013.

[30] K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang, “Beyond a gaussian
denoiser: Residual learning of deep cnn for image denoising,” IEEE
Transactions on Image Processing, vol. 26, no. 7, pp. 3142–3155, 2017.
[31] K. Zhang, W. Zuo, S. Gu, and L. Zhang, “Learning deep cnn denoiser

prior for image restoration,” arXiv preprint arXiv:1704.03264, 2017.

[32] R. Saab, R. Chartrand, and O. Yilmaz, “Stable sparse approximations via
nonconvex optimization,” in IEEE international conference on acoustics,
speech and signal processing, 2008, pp. 3885–3888.

[33] P. Jain, P. Kar et al., “Non-convex optimization for machine learning,”

13

Foundations and Trends® in Machine Learning, vol. 10, no. 3-4, pp.
142–363, 2017.

[34] I. Goodfellow, Y. Bengio, and A. Courville, Deep learning. MIT press,

2016.

[35] S. Boyd, N. Parikh, and E. Chu, Distributed optimization and statistical
Now

learning via the alternating direction method of multipliers.
Publishers Inc, 2011.

[36] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in 2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2016, pp. 770–778.

[37] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely
connected convolutional networks,” in Proceedings of the IEEE confer-
ence on computer vision and pattern recognition, 2017, pp. 4700–4708.
[38] K. Gregor, I. Danihelka, A. Graves, D. J. Rezende, and D. Wierstra,
“Draw: A recurrent neural network for image generation,” arXiv preprint
arXiv:1502.04623, 2015.

[39] Y. Tai, J. Yang, and X. Liu, “Image super-resolution via deep recursive
residual network,” in Proceedings of the IEEE conference on computer
vision and pattern recognition, 2017, pp. 3147–3155.

[40] Y. Zhao, Y. Shen, and J. Yao, “Recurrent neural network for text
classiﬁcation with hierarchical multiscale dense connections.” in IJCAI,
2019, pp. 5450–5456.

[41] P. Indyk and R. Motwani, “Approximate nearest neighbors: towards
removing the curse of dimensionality,” in Proceedings of the thirtieth
annual ACM symposium on Theory of computing, 1998, pp. 604–613.
[42] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local neural net-
works,” in Proceedings of the IEEE conference on computer vision and
pattern recognition, 2018, pp. 7794–7803.

[43] D. Liu, B. Wen, Y. Fan, C. C. Loy, and T. S. Huang, “Non-local recurrent
network for image restoration,” in Advances in Neural Information
Processing Systems, 2018, pp. 1673–1682.

[44] C. Tomasi and R. Manduchi, “Bilateral ﬁltering for gray and color
images,” in Sixth international conference on computer vision (IEEE
Cat. No. 98CH36271).

IEEE, 1998, pp. 839–846.

[45] K. Yue, M. Sun, Y. Yuan, F. Zhou, E. Ding, and F. Xu, “Compact
generalized non-local network,” in Advances in Neural Information
Processing Systems, 2018, pp. 6510–6519.

[46] R. Timofte, E. Agustsson, L. Van Gool, M.-H. Yang, and L. Zhang,
“Ntire 2017 challenge on single image super-resolution: Methods and
results,” in Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition Workshops, 2017, pp. 114–125.

[47] M. Bevilacqua, A. Roumy, C. Guillemot, and M. L. Alberi-Morel, “Low-
complexity single-image super-resolution based on nonnegative neighbor
embedding,” in British Machine Vision Conference 2012, 2012, pp. 1–
10.

[48] R. Zeyde, M. Elad, and M. Protter, “On single image scale-up using
sparse-representations,” in Proceedings of the 7th international confer-
ence on Curves and Surfaces, 2010, pp. 711–730.

[49] D. Martin, C. Fowlkes, D. Tal, and J. Malik, “A database of human
segmented natural images and its application to evaluating segmentation
algorithms and measuring ecological statistics,” in Proceedings Eighth
IEEE International Conference on Computer Vision. ICCV 2001, vol. 2,
2001, pp. 416–423.

[50] J.-B. Huang, A. Singh, and N. Ahuja, “Single image super-resolution
from transformed self-exemplars,” in 2015 IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2015, pp. 5197–5206.

[51] Y. Matsui, K. Ito, Y. Aramaki, A. Fujimoto, T. Ogawa, T. Yamasaki,
and K. Aizawa, “Sketch-based manga retrieval using manga109 dataset,”
Multimedia Tools and Applications, vol. 76, no. 20, pp. 21 811–21 838,
2017.

[52] Z. Wang, A. Bovik, H. Sheikh, and E. Simoncelli, “Image quality assess-
ment: from error visibility to structural similarity,” IEEE Transactions
on Image Processing, vol. 13, no. 4, pp. 600–612, 2004.

[53] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”

arXiv preprint arXiv:1412.6980, 2014.

[54] W. Han, S. Chang, D. Liu, M. Yu, M. Witbrock, and T. S. Huang, “Image
super-resolution via dual-state recurrent networks,” in 2018 IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2018, pp.
1654–1663.

[55] L. Xie, X. Chen, K. Bi, L. Wei, Y. Xu, Z. Chen, L. Wang, A. Xiao,
J. Chang, X. Zhang et al., “Weight-sharing neural architecture search: A
battle to shrink the optimization gap,” arXiv preprint arXiv:2008.01475,
2020.

[56] S. Anwar and N. Barnes, “Densely residual laplacian super-resolution,”
IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.

