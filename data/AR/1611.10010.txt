Deep Cuboid Detection: Beyond 2D Bounding Boxes

Debidatta Dwibedi∗
Carnegie Mellon University
debidatta@cmu.edu

Tomasz Malisiewicz Vijay Badrinarayanan Andrew Rabinovich
Magic Leap, Inc.
{tmalisiewicz,vbadrinarayanan,arabinovich}@magicleap.com

6
1
0
2

v
o
N
0
3

]

V
C
.
s
c
[

1
v
0
1
0
0
1
.
1
1
6
1
:
v
i
X
r
a

Abstract

We present a Deep Cuboid Detector which takes a
consumer-quality RGB image of a cluttered scene and lo-
calizes all 3D cuboids (box-like objects). Contrary to clas-
sical approaches which ﬁt a 3D model from low-level cues
like corners, edges, and vanishing points, we propose an
end-to-end deep learning system to detect cuboids across
many semantic categories (e.g., ovens, shipping boxes, and
furniture). We localize cuboids with a 2D bounding box,
and simultaneously localize the cuboid’s corners, effec-
tively producing a 3D interpretation of box-like objects.
We reﬁne keypoints by pooling convolutional features iter-
atively, improving the baseline method signiﬁcantly. Our
deep learning cuboid detector is trained in an end-to-end
fashion and is suitable for real-time applications in aug-
mented reality (AR) and robotics.

1. Introduction

Building a 3D representation of the world from a sin-
gle monocular image is an important problem in computer
vision. In some applications, we have the advantage of hav-
ing explicit 3D models and try to localize these objects in
the world while estimating their pose. But without such 3D
models, we must reason about the world in terms of sim-
ple combinations of geometric shapes like cuboids, cylin-
ders, and spheres. Such primitives, referred to as geons by
Biederman [5], are easy for humans to reason about. Hu-
mans can effortlessly make coarse estimates about the pose
of these simple geometric primitives and even compare ge-
ometric parameters like length, radius or area across dis-
parate instances. While many objects are composed of mul-
tiple geometric primitives, a large number of real objects
can be well approximated by as little as one primitive.

For example, let us consider a common shape that we
see everyday: the box. Many everyday objects can geomet-
rically be classiﬁed as a box (e.g., shipping boxes, cabinets,
washing machines, dice, microwaves, desktop computers).

∗Work done as an intern at Magic Leap, Inc. while a student at Carnegie

Mellon University

Figure 1. 2D Object detection vs. 3D Cuboid detection. Our
task is to ﬁnd all cuboids inside a monocular image and localize
their vertices. Our deep learning system is trained in an end-to-end
fashion, runs in real-time, and works on RGB images of cluttered
scenes captured using a consumer-grade camera.

Boxes (or cuboids, as we call them in this paper) span a
diverse set of everyday object instances, yet it is very easy
for humans to ﬁt an imaginary cuboid to these objects and
in doing so they localize its vertices and faces. People can
also compare the dimensions of different box-like objects
even though they are not aware of the exact dimensions or
even if the object is not a perfect cuboid. In this work, we
speciﬁcally deal with the problem of detecting class agnos-
tic geometric entities like cuboids. By class agnostic, we
mean that we do not differentiate between a shipping box, a
microwave oven, or a cabinet. All of these boxy objects are
represented with the same simpliﬁed concept called cuboid
in the rest of the paper.

Detecting boxy objects in images and extracting 3D
information like pose helps overall scene understanding.
Many high-level semantic problems have been tackled by
ﬁrst detecting boxes in a scene (e.g., extracting the free
space in a room by reducing the objects in a scene to
boxes [22], estimating the support surfaces in the scene [34,
21] and estimating the scene layout [17]).

A perfect cuboid detector opens up a plethora of possi-
bilities for augmented reality (AR), human-computer inter-
action (HCI), autonomous vehicles, drones, and robotics in
general. The cuboid detector can be used as follows:

• For Augmented Reality, cuboid vertex localization fol-
lowed by 6-dof pose estimation allows a content cre-

1

 
 
 
 
 
 
Figure 2. Deep Cuboid Detection Pipeline. The ﬁrst step is to ﬁnd Regions of Interest (RoIs) in the image where a cuboid might be
present. We train a Region Proposal Network (RPN) to output such regions. Then, features corresponding to each RoI are pooled from
a convolutional feature map (e.g., conv5 in VGG-M). These pooled features are passed though two fully connected layers just like the
region-based classiﬁcation network in Faster R-CNN (see model details in Section 3.1). However, instead of just producing a 2D bounding
box, we also output the normalized offsets of the vertices from the center of the region. Finally, we reﬁne our predictions by performing
iterative feature pooling (see Section 5.2). The dashed lines show the regions from which features will be pooled.

ator to use the cuboid-centric coordinate system de-
ﬁned by a stationary box to drive character animation.
Because we also know the volume of space occupied
by the stationary cuboid, animated characters can jump
on the box, hide behind it, and even start drawing on
one of the box’s faces.

• For Human-Computer Interaction, a hand-held cuboid
can be used as a lightweight game controller. A sta-
tionary camera observing the hand-held cube is able to
estimate the cube’s pose, effectively tracking the cube
in 3D space.
In essence, the cuboid can serve as a
means to improve interaction in AR systems(e.g., the
tabletop AR demo using cuboids [57]).

• For autonomous vehicles, 3D cuboid detection allows
the vehicle to reason about the spatial extent of rare
objects that might be missing in supervised training
set. By reasoning about the pose of objects in a class-
agnostic manner, autonomous vehicles can be safer
drivers.

• For drones, man-made structures such as buildings and
houses can be well-approximated with cuboids, assist-
ing navigation during unmanned ﬂights.

• For robotics in general, detecting boxy objects in im-
ages and extracting their 3D information like pose
helps overall scene understanding. Placing a handful

of cuboids in a scene (instead of Aruco markers) can
making pose tracking more robust for SLAM applica-
tions.

In general, one can formulate the 3D object detection
problem as follows: ﬁt a 3D bounding box to objects in an
RGB-D image [48, 49, 34, 46], detect 3D keypoints in an
RGB image [51, 53], or perform 3D model to 2D image
alignment [50, 24, 2]. In this paper, we follow the keypoint-
based formulation. Because an image might contain multi-
ple cuboids as well as lots of clutter, we must ﬁrst procure a
shortlist of regions of interest that correspond to cuboids. In
addition to the 2D bounding box enclosing the cuboid, we
estimate the location of all 8 vertices.

1.1. From Object Detection to Cuboid Detection

Deep learning [37] has revolutionized image recognition
in the past few years. Many state-of-the-art methods in ob-
ject detection today are built on top of deep networks that
were trained for the task for image classiﬁcation. Not only
has the accuracy of object detection increased, there are
many approaches that also run in real-time [20, 43, 40]. We
take advantage of these advances to detect boxy objects in
a scene. Unlike the usual task of object detection, we want
more than the bounding box of the object. Instead, we want
to localize the vertices of the cuboids (see Figure 1). An-
other aspect where we differ from the object detection task
is that we do not care about the class of the cuboids that

we are detecting. For our purpose, we deal with only two
classes: cuboid and not-a-cuboid.

Even if a cuboid is a geometric object that can be eas-
ily parameterized, we advocate the use of deep learning to
detect them in scenes. Classically, one approach to detect
cuboids is to detect the edges and try to ﬁt the model of
a cuboid to these edges. Hence, robust edge selection is
an important step. However, this becomes difﬁcult when
there are misleading textures on cuboidal surfaces, if edges
and corners are occluded or the scene contains considerable
background clutter. It is a difﬁcult task to classify whether
a given line belongs to a given cuboid with purely local fea-
tures. Hence, we propose to detect cuboids the same way
we detect classes like cars and aeroplanes in images, us-
ing a data-driven approach. However, the task of ﬁtting a
single label ’cuboid’ to boxy objects in the scene is not triv-
ial as the label is spread over many categories like houses,
washing machines, ballot boxes, etc. In our experiments,
we show how a CNN is successful to learn features that
help us identify cuboids in different scenes.

1.2. Our Contributions

Our main contribution is a deep learning model that
jointly performs cuboid detection and vertex localization.
We explore various ways to improve the detection and lo-
calization accuracy produced by a baseline CNN. The key
idea is to ﬁrst detect the object of interest and then make
coarse predictions regarding the location of its vertices.
The next stage acts as an attention mechanism, perform-
ing reﬁnement of vertices by only looking at regions with
high probability of being a cuboid. We improve localiza-
tion accuracy with an iterative feature pooling mechanism
(see Section 5.2), study the effect of combining cuboid-
related losses( see Section 5.1), experiment with alternate
parametrizations (see Section 6), and study the effect of dif-
ferent size base networks (see Section 5.4).

2. Related Work

Classical

ideas on 3D scene and object recognition
originate in Robert’s Blocks-World [44] and Biederman’s
Recognition-by-Components [5]. These early works were
overly reliant on bottom-up image processing, thus never
worked satisfactorily on real images. Many modern ap-
proaches utilize a large training database of 3D models and
some kind of learning for 2d-to-3d alignment [1, 2, 53, 9].
Cuboid detection has also been approached with
geometry-based methods [52, 28, 38, 22]. Shortly after the
success of Deformable Parts Model, researchers extended
HOG-based models to cuboids [55, 15]. RGB-D based ap-
proaches to cuboid detection are also common [35].

Our work revisits the problem of cuboid detection that
Xiao et al. introduced in [55]. They use a Deformable Parts-
based Model that uses HOG classiﬁers to detect cuboid

Figure 3. RoI-Normalized Coordinates Vertices are represented
as offsets from the center of the RoI and normalized by the region’s
width/height. (xv, yv) is a vertex and (xc, yc) is the center.

vertices in different views. Their model has four compo-
nents for scoring a ﬁnal cuboid conﬁguration: score from
the HOG classiﬁer, 2D vertex displacement, edge alignment
score and a 3D shape score that takes into account how close
the predicted vertices are to a cuboid in 3D. Their approach
jointly optimizes over visual evidence (corners and edges)
found in the image while penalizing the predictions that
stray too far from an actual 3D cuboid. A major limitation
of their approach is the computationally expensive test-time
iterative optimization step. Not only is their HOG-based
model inferior to its modern CNN-based counterpart (as we
demonstrate in our experiments), their approach takes more
than a minute to process a single image.

Today the best methods in object detection owe their per-
formance to convolutional neural networks. Krizhevsky et
al. [37] showed that CNNs were superior to existing meth-
ods for the task of image classiﬁcation. To localize an ob-
ject in an image, the image is broken down into regions and
these regions are classiﬁed instead. This approach was ﬁrst
shown to work in [20]. This was followed by attempts to
make object detection work almost real-time [19, 43]. More
recently, researchers have been able to make object detec-
tion faster by performing detection in a single step [40, 42].
These approaches run at about 50-60 frames per second
which opens up the possibility for adapting such network
architectures to real-time cuboid localization.

3D object localization using keypoints is commonly
studied [45, 30, 31, 10, 39, 11]. 3D keypoint detection us-
ing deep networks is also gaining popularity [53, 51]. There
has been a resurgence in work that aims to align 3D models
of objects in single view images [23, 36, 54].

The iterative vertex reﬁnement component of our ap-
proach is similar to the iterative error feedback approach
of [7], the network cascades in [12], the iterative bound-
ing box regression of Multi-region CNN [18] and Inside-
Outside Networks [4]. Such iterative models have been
reinterpreted as Recurrent Neural Networks in [3, 6], and
while most applications focus on human pose estimation,
the ideas can easily be extended to cuboid detection.

Figure 4. Deep Cuboid Detections using VGG16 + Iterative. Our system is able to localize the vertices of cuboids in consumer-grade
RGB images. We can handle both objects like boxes (that are perfectly modeled by a cuboid) as well as objects like sinks (that are only
approximate cuboids). Last row: failures of our system (see 6 for more details).

3. Deep Cuboid Detection

We deﬁne any geometric shape as a tuple of N param-
eters. These parameters might be geometric in nature like
the radius of a sphere or the length, width and height of a
cuboid. However, a more general way to parameterize any
geometric primitive is to represent it as a collection of points
on the surface of the primitive. If we choose a random point
on the surface of the primitive, it might not necessarily be
localizable from a computer-vision point of view. Ideally,
we want the set of parametrization points to be geometri-

cally informative and visually discriminative [16]. Speciﬁ-
cally, in the case of cuboids, these special keypoints are the
same as the cuboid’s corners.1

We deﬁne a cuboid as a tuple of 8 vertices where each
point is denoted by its coordinates (x, y) in the image. In
our default parametrization, we use the 16 coordinate pa-
rameters to represent a cuboid in an image. This, however,
is an overparameterization as 16 parameters are not required
to represent a cuboid (see Section 6 for a discussion on al-

1When discussing cuboids, we use the terms vertex, corner, and key-

point, interchangeably.

ternate cuboid parametrizations).

3.1. Network Architecture and Loss Functions

Our network is made up of several key components: the
CNN Tower, the Region Proposal Network (RPN), R-CNN
classiﬁer and regressor, and a novel iterative feature pool-
ing procedure which reﬁnes cuboid vertex estimates. Our
architecture is visually depicted in Figure 2.

1. The CNN Tower is the pre-trained fully convolutional
part of popular ConvNets like VGG and ResNets. We
refer to the last layer of the tower as the convolutional
feature map (e.g., conv5 in VGG16 is of size M ×N ×
512).

2. The RPN is a fully convolutional network that maps
every cell in the feature map to a distribution over K
multi-scale anchor-boxes, bounding box offsets, and
objectness scores. The RPN has two associated losses:
log loss for objectness and smooth L1 for bounding
box regression. Our RPN uses 512 3×3 ﬁlters, then 18
1×1 ﬁlters for objectness and 36 1×1 ﬁlters for bound-
ing box offsets. (See Faster R-CNN [43] for more de-
tails.)

3. The RoI pooling layer uses max pooling to convert the
features inside any valid region of interest into a small
ﬁxed-size feature map. E.g., for conv5 of size M ×N ×
512, the pooling layer produces an output of size 7 ×
7 × 512, independent of the input regions aspect ratio
and scale. RoI pooling is the non-hierarchical version
of SPPNet [27].

4. The RCNN regressor is then applied to each ﬁxed-size
feature vector, outputting a cuboidness score, bound-
ing box offsets (4 numbers), and 8 cuboid vertex loca-
tions (16 numbers). The bounding box regression val-
ues (∆x, ∆y, ∆w, ∆h) [19] are used to ﬁt the initial
object proposal tightly around the object. The vertex
locations are encoded as offsets from the center of the
RoI and are normalized by the proposal width/height
as shown in Figure 3. The ground truth targets for each
vertex are:

tx =

xv − xc
w

, ty =

yv − yc
h

The RCNN uses two fully connected layers (4096 neu-
rons each), and has three associated losses: log loss for
cuboidness and smooth L1 for both bounding box and
vertex regression.

5. When viewed in unison, the RoI pooling and RCNN
layers act as a reﬁnement mechanism, mapping an in-
put box to an improved one, given the feature map.

This allows us to apply the last part of our network
multiple times — a step we call iterative feature pool-
ing(See section 5.2).

The loss

function used in the RPN consists of
Lanchor−cls, the log loss over two classes (cuboid vs. not
cuboid) and Lanchor−reg, the Smooth L1 loss [19] of the
bounding box regression values for each anchor box. The
loss function for the R-CNN is made up of LROI−cls,
the log loss over two classes (cuboid vs.
not cuboid),
LROI−reg, the Smooth L1 loss of the bounding box regres-
sion values for the RoI and LROI−corner, the Smooth L1
loss over the RoI’s predicted vertex locations. We also refer
to the last term as the corner regression loss. In the experi-
ments section, we report how adding these additional tasks
affects the detector performance (see Table 1).

The complete loss function is a weighted sum of the

above mentioned losses and can be written as follows:

L =λ1Lanchor−cls + λ2Lanchor−reg+

λ3LROI−cls + λ4LROI−reg + λ5LROI−corner

We use Caffe [33] for our experiments and build on
top of the implementation of Faster R-CNN by Girshick et
al. [19]. For all experiments we start with either the VGG-
M [8] or VGG16 [47] networks that have been pre-trained
for the task of image classiﬁcation on ImageNet. VGG-M
is a smaller model with 7 layers while VGG16 contains 16
layers. We ﬁne-tune all models for 50K iterations using
SGD with a learning rate of 0.001 and reduce the learning
rate by a factor of 10 after 30K iterations. We also use a
momentum of 0.9, weight decay of 0.0005 and dropout of
0.5. We do not perform any stage-wise training. Instead,
we jointly optimize over all tasks and have kept the value of
all the loss weights as one in our experiments (i.e., λi = 1).

4. Data

We use the SUN Primitive dataset [55] to train our deep
cuboid detector. This is the largest publicly available cuboid
dataset — it consists of 3516 images and is a mix of in-
door scenes with lots of clutter, internet images contain-
ing only a single cuboid and outdoor images of buildings
that also look like cuboids. Both cuboid bounding boxes
and cuboid vertices have ground-truth annotations. In this
dataset, we have 785 images with 1269 annotated cuboids.
The rest of the images are negatives — they do not contain
any cuboids. We make a split to create a training set of 3000
images and 516 test images. We augment the 3000 images
with their horizontally ﬂipped versions while training, but
cuboid-speciﬁc data augmentation [29] strategies are likely
to further improve performance.

Additional loss function
Bounding Box Loss
Corner Loss
Bounding Box + Corner Loss

AP
66.33
58.39
67.11

APK
-
28.68
34.62

PCK
-
27.64
29.38

Table 1. Multi-task learning Results We ﬁrst train a network us-
ing only the bounding box loss, then use the cuboid corner loss.

5. Experiments

We evaluate the system on two tasks: cuboid bounding
box detection and cuboid keypoint localization. For detec-
tion, a bounding box is correct if the Intersection over Union
(IoU) overlap is greater than 0.5.2. Detections are sorted by
conﬁdence (the network’s classiﬁer softmax output) and we
report the mean Average Precision (AP) as well as the en-
tire Precision-Recall curve. For keypoint localization we
use the Probability of Correct Keypoint (PCK) and Aver-
age Precision of Keypoint (APK) metrics [56]. PCK and
APK are commonly used in the human pose estimation lit-
erature to measure the performance of systems predicting
the location of human body parts like head, wrist, etc. PCK
measures the fraction of annotated instances that are cor-
rect when all the ground truth boxes are given as input to
the system. A predicted keypoint is correct if its normal-
ized distance from the annotation is less than a threshold
(α). APK, on the other hand, takes both detection conﬁ-
dence and keypoint localization into consideration. We use
a normalized distance, α, of 0.1 which means that a pre-
dicted keypoint is considered to be correct if it lies within
0.1 × max(height, width) pixels of the ground truth anno-
tation of the keypoint. See Figure 7 to see these metrics re-
ported on the SUN Primitive test set and samples of cuboid
detections in Figure 4.

For bounding box detection, our best network architec-
ture achieves a mAP of 75.47, which is signiﬁcantly better
than the HOG-based system of Xiao et al. [55] which re-
ports a mAP of 24.0.

5.1. Multi-task learning

We train multiple networks each of which performs dif-
ferent multiple tasks. We start off with a base network that
just outputs bounding boxes around cuboids — that is we
train a network to perform general object detection using
just the rectangle enclosing the cuboids. The above network
outputs the class of the box and the bounding box regression
values. Next, we train a different model with additional su-
pervision about the location of the corners but this model
does not output bounding box regression coordinates. Fi-
nally, we train a network that outputs both the bounding box
regression values and the coordinates of the vertex. A cor-

2This is the standard way to evaluate a 2D object detector as was pop-

ularized by the PASCAL VOC object detection challenge [14]

Method
Corner Loss
Corner Loss + Iterative
BB+Corner Losses
BB+Corner Loss + Iterative

AP
58.39
62.89
67.11
71.72

APK
28.68
33.98
34.62
37.61

PCK
27.64
35.56
29.38
36.53

Table 2. Results for Iterative Feature Pooling. Our iterative al-
gorithm improves the box detection AP by over 4% and PCK over
7%.

responding term is added to the loss function for each addi-
tional task. From our experiments, we can see how adding
more tasks affects the performance of the cuboid detector
(see Table 1).

5.2. Iterative Feature Pooling

In R-CNN, the ﬁnal output is a classiﬁcation score and
the bounding box regression values for every region pro-
posal. The bounding box regression allows us to move
the region proposal around and scale it such that the ﬁnal
bounding box localizes just the object. This implies that our
initial region from which we pooled features to make this
prediction was not entirely correct. Hence, it makes sense
to go back and pool features from the correct bounding box.
We implement this in the network itself which means that
we perform iterative bounding box regression while training
and testing in exactly the same way. The input to the fully-
connected layers is a ﬁxed-size feature map that consists
of the pooled features from different region proposals from
conv5 layer. The R-CNN outputs are used for bounding box
regression on the input object proposals to produce new pro-
posals. Then features are pooled from these new proposals
and passed through the fully-connected layers again. This
model is now an “any-time prediction system” where for ap-
plications which are not bound by latency, bounding box re-
gression can be performed more than once. Our results (see
Table 2) show that iterative feature pooling can greatly im-
prove both bounding box detection and vertex localization
(see Figure 5). There isn’t a signiﬁcant change in perfor-
mance when we iteratively pool features more than twice.

5.3. Depth of Network

We experiment with two base models: VGG16 and
VGG-M. While VGG16 has a very deep architecture with
16 layers, VGG-M is a smaller model with 7 layers. We
report results of these experiments in Table 3. Interestingly,
for this dataset and task, two iterations through the shal-
lower network outperforms one iteration through the deeper
network. Coupled with the fact the shallower network with
iteration runs twice as fast, it can be a good choice for de-
ployment.

Figure 5. Vertex Reﬁnement via Iterative Feature Pooling. We reﬁne cuboid detection regions by re-pooling features from conv5 using
the predicted bounding boxes (see Section 5.2).

Method
VGG-M
VGG-M + I
VGG16
VGG16 + I

AP
67.11
71.72
70.50
75.47

APK
34.62
37.61
33.65
41.21

PCK
29.38
36.53
35.83
38.27

Size

Speed
334MB 14fps
334MB 10fps
5fps
522MB
4fps
522MB

Table 3. VGG-M (7 layer) vs. VGG16 (16 layer) base network.
I implies iterative feature pooling was performed. The deeper
cuboid detector outperforms the shallower one.

Number of Images
1000
2000
3000

AP
40.47
52.17
71.72

APK
20.83
27.51
37.61

PCK
26.60
29.31
36.53

Figure 6. Cuboid Vanishing Points. Vanishing points produced
by extrapolating the edges of a cube can be used to reduce the
number of parameters. We can drop the Front-Top-Left (FTL) and
Back-Bottom-Right (BBR) vertices from our parametrization, and
infer them using estimated VPs.

Table 4. Performance vs. number of training images. Deep
cuboid detection beneﬁts from more training images.

5.4. Effect of Training Set Size

We wanted to measure the impact of increasing the size
of training data. We create three datasets of varying sizes:
1K, 2K and 3K images and train a common network (VGG-
M + Iterative). Our results (see Table 4) show signiﬁcantly
improved performance when using larger training set sizes.

al. [55] takes minutes to process a single image. The real-
time nature of the system is due to the fact that our imple-
mentation is based on top of Faster R-CNN. We anticipate
that progress in object detection like SSD [40] can be lever-
aged to have a cuboid detector that is even faster. We also
report the model size in MB (see Table 3). Further opti-
mization can be done to reduce the size of the model such
that it ﬁts on mobile devices [41, 32, 25].

5.5. Memory and Runtime Complexity

6. Discussion

Our cuboid detector is able to run at interactive rates on
a Titan Z GPU while the HOG-based approach of Xiao et

So far, all of our networks output the cuboid’s 8 vertices
directly. However, a closer look at typical failures sug-

Iteration 1Iteration 2Iteration 1Iteration 2Iteration 1Iteration 2BBRFTLVP1VP2Vanishing LineBBRFTLVP1VP2Vanishing Line(a) Precision-Recall Curve

(b) APK vs. Recall

(c) PCK vs. α

(d) Keypoint-wise APK

(e) Keypoint-wise PCK

(f) Face-wise PCK

Figure 7. Deep Cuboid Detector Evaluation Metrics. APK: Average Precision of Keypoint, PCK: Probability of Correct Keypoint, α:
Normalized distance from GT corners, Order of keypoints: front-top-left, back-top-left, front-bottom-left, front-top-right, back-bottom-left,
front-bottom-right, back-top-right, back-bottom-right. B = Bounding box loss, C = Corner loss, I = Iterative.

0.00.20.40.60.81.0Recall0.00.20.40.60.81.0PrecisionVGGM_CVGGM_BCVGGM_BCIVGG16_CVGG16_BCVGG16_BCI0.00.20.40.60.81.0Recall0.00.20.40.60.81.0Average Precision of KeypointVGGM_CVGGM_BCVGGM_BCIVGG16_CVGG16_BCVGG16_BCI0.00.10.20.30.40.5Alpha0.00.20.40.60.81.0Probability of Correct KeypointVGGM_CVGGM_BCVGGM_BCIVGG16_CVGG16_BCVGG16_BCI12345678Keypoint ID0.00.20.40.60.81.0Average Precision of Keypoint (α=0.1)VGGM_CVGGM_BCVGGM_BCIVGG16_CVGG16_BCVGG16_BCI12345678Keypoint ID0.00.20.40.60.81.0Probability of Correct Keypoint (α=0.1)VGGM_CVGGM_BCVGGM_BCIVGG16_CVGG16_BCVGG16_BCIFrontBackFront and back Keypoints0.00.20.40.60.81.0Probability of Correct Keypoint (α=0.1)VGGM_CVGGM_BCVGGM_BCIVGG16_CVGG16_BCVGG16_BCIMethod
6 corners
8 corners

AP
65.26
67.11

APK
29.64
34.62

PCK PCK of BBR Corner
27.36
29.38

24.44
27.22

PCK of FTL Corner
21.11
29.44

PCK of Remaining Corners
28.89
29.73

Table 5. 8 corner vs. 6 corner parameterization The 8 corner parameterization uses all of the cuboid’s corners, whereas in the 6 corner
version, the BBR and FTL corners are dropped (see Figure 6) and inferred from the vanishing points. This shows how the network is able
to do geometric reasoning and the over-parameterization adds robustness to the system (see Section 6). BBR: Back-Bottom-Right and
FTL: Front-Top-Left

gests that our parametrization might be to blame. Certain
viewpoints have an inherent ambiguity (e.g., which face of
the Rubiks cube in Figure 4 should be labelled the front?).
Since our detector has trouble dealing with such conﬁgura-
tions, in this section we explore and discuss alternate cuboid
parametrizations. If we consider the world origin to coin-
cide with camera center coordinates, the minimal parame-
terization of the cuboid can be done with 12 numbers:

• (X, Y , Z) - Centre of cuboid in 3D

• (L, W , H) - Dimensions of the cuboid

• (θ, ψ, φ) - 3 angles of rotation of the cuboid

• (f , cx, cy) - Intrinsic camera parameters

For modern cameras, we can assume there is no
skew in the camera and equal focal lengths. The over-
parameterization we have in our network (as we predict 16
numbers) allows the network to produce outputs that do not
represent cuboids. We experimented with several differ-
ent re-parameterizations of a cuboid in an attempt to better
utilize the geometric constraints. In general, our observa-
tion is that the network is able to learn features for tasks
that have more visual evidence in the image and predict
parameters which can be scaled properly for stable opti-
mization. When dealing with 3D geometry and deep learn-
ing, proper parametrization is important. Even image-to-
image transformations like homographies beneﬁt from re-
parametrization (e.g., the 4-point parametrization [13]).

6-corner parametrization: We explore an alternate pa-
rameterization in which we predict only 6 coordinates and
infer the locations of the remaining two coordinates using
the fact that there are parallel edges in cuboids. The edges
that are parallel in 3D meet at the vanishing point in the
image. There are two pairs of parallel lines on the top and
bottom face of the cuboid. These lines should meet at the
same vanishing point [26] as shown in Figure 6. Using this
fact, we can infer the position of the remaining two points.
This allows us to parameterize our output in 12 numbers.

We carry out an experiment to be able to compare the
8 corner parameterization with the 6 corner parameteriza-
tion. To do so, we decide to not use the ground truth data
for two vertices while training. We leave one vertex each
from the back and front faces (those whose detection rates

(PCK) were the worst). We train a network to predict the
location of the remaining 6 corners. We infer the location
of the two dropped vertices using these 6 corners. We do
so by ﬁrst ﬁnding out the vanishing points corresponding
to the 6 points we predicted. This reparameterization, how-
ever, leads to a reduction in performance (see Table 5). This
degradation is due to the fact that we discard any visual ev-
idence corresponding to the two inferred corners present in
the image. Also, any error in prediction of one vertex due to
occlusion or any other reason is directly propagated to the
inferred corners. However, left to the CNN it learns multi-
ple models to detect a cuboid. The network is free to use
all visual evidence to localize the corners of the cuboid. It
is interesting to note that a CNN is capable of doing pure
geometric reasoning because in many cases the corner on
the back does not have visual evidence in the image due to
self-occlusion.

Another

Vanishing point parametrization:

re-
parameterization3 uses locations of the two vanishing
points and the slopes of six lines which will form the edges
of the cuboid (see Figure 6). Note that these vanishing
points correspond to a particular cuboid and might be
different from the vanishing point of the entire image.
The intersection points of these 6 lines would gives us
the vertices of the cuboid. However, the network fails to
learn the location of the vanishing points as many a time
they lie outside the region of interest and have little or
confounding visual evidence in the region of interest or the
entire image itself. It also becomes difﬁcult to normalize
the targets to predict the vanishing points directly. The
slopes of the 6 lines can vary between −∞ to +∞. Instead
of predicting the slope directly, we tried to regress to value
of sin(tan−1(θ)). We found it difﬁcult to train the network
with this parameterization but there might exist a set of
hyperparameters (loss weights, learning rates, solver, etc)
for which we can train the above mentioned network.

7. Conclusion

In this paper we have presented an end-to-end deep
learning system capable of detecting cuboids and localizing
their vertices in RGB images of cluttered scenes. By casting

3The standard parameterization involves predicting the 8 vertices (16

parameters) of the cuboid directly.

the problem in a modern end-to-end deep learning frame-
work, there is no need to design custom low-level detectors
for line segments, vanishing points, junctions, etc. With
Augmented Reality and Robotics applications in mind, we
hope that the design and architecture of our system will in-
spire researchers to improve upon our results. Future work
will focus on: training from much larger datasets and syn-
thetic data, network optimization, and various regulariza-
tion techniques to improve generalization.

References

[1] M. Aubry, D. Maturana, A. A. Efros, B. C. Russell, and
J. Sivic. Seeing 3d chairs: exemplar part-based 2d-3d align-
In Proceedings
ment using a large dataset of cad models.
of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 3762–3769, 2014. 3

[2] A. Bansal, B. Russell, and A. Gupta. Marr revisited: 2d-3d
alignment via surface normal prediction. In CVPR, 2016. 2,
3

[3] V. Belagiannis and A. Zisserman. Recurrent human pose
estimation. arXiv preprint arXiv:1605.02914, 2016. 3

[4] S. Bell, C. L. Zitnick, K. Bala, and R. Girshick.

Inside-
outside net: Detecting objects in context with skip pooling
and recurrent neural networks. In CVPR, 2016. 3

[5] I. Biederman. Recognition-by-components: a theory of hu-
man image understanding. Psychological review, 94(2):115,
1987. 1, 3

[6] A. Bulat and G. Tzimiropoulos. Human pose estimation via
convolutional part heatmap regression. In ECCV, pages 717–
732. Springer, 2016. 3

[7] J. Carreira, P. Agrawal, K. Fragkiadaki, and J. Malik. Hu-
man pose estimation with iterative error feedback. In CVPR,
2016. 3

[8] K. Chatﬁeld, K. Simonyan, A. Vedaldi, and A. Zisserman.
Return of the devil in the details: Delving deep into convo-
lutional nets. arXiv preprint arXiv:1405.3531, 2014. 5
[9] C. B. Choy, D. Xu, J. Gwak, K. Chen, and S. Savarese. 3d-
r2n2: A uniﬁed approach for single and multi-view 3d object
reconstruction. In ECCV, 2016. 3

[10] A. Collet, M. Martinez, and S. S. Srinivasa. The moped
framework: Object recognition and pose estimation for ma-
nipulation. The International Journal of Robotics Research,
30(10):1284–1306, 2011. 3

[11] A. Crivellaro, M. Rad, Y. Verdie, K. Moo Yi, P. Fua, and
V. Lepetit. A novel representation of parts for accurate 3d
object detection and tracking in monocular images. In Pro-
ceedings of the IEEE International Conference on Computer
Vision, pages 4391–4399, 2015. 3

[12] J. Dai, K. He, and J. Sun. Instance-aware semantic segmen-

tation via multi-task network cascades. In CVPR, 2016. 3

[13] D. DeTone, T. Malisiewicz, and A. Rabinovich. Deep image
homography estimation. arXiv preprint arXiv:1606.03798,
2016. 9

[14] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and
A. Zisserman. The pascal visual object classes (voc) chal-
lenge. International journal of computer vision, 88(2):303–
338, 2010. 6

[15] S. Fidler, S. Dickinson, and R. Urtasun. 3d object detec-
tion and viewpoint estimation with a deformable 3d cuboid
In Advances in neural information processing sys-
model.
tems, pages 611–619, 2012. 3

[16] D. F. Fouhey, A. Gupta, and M. Hebert. Data-driven 3d prim-
itives for single image understanding. In Proceedings of the
IEEE International Conference on Computer Vision, pages
3392–3399, 2013. 4

[17] A. Geiger, C. Wojek, and R. Urtasun. Joint 3d estimation of
objects and scene layout. In Advances in Neural Information
Processing Systems, pages 1467–1475, 2011. 1

[18] S. Gidaris and N. Komodakis. Object detection via a multi-
region and semantic segmentation-aware cnn model. In Pro-
ceedings of the IEEE International Conference on Computer
Vision, pages 1134–1142, 2015. 3

[19] R. Girshick. Fast r-cnn. In ICCV, pages 1440–1448, 2015.

3, 5

[20] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-
ture hierarchies for accurate object detection and semantic
In Proceedings of the IEEE conference on
segmentation.
computer vision and pattern recognition, pages 580–587,
2014. 2, 3

[21] A. Gupta, A. A. Efros, and M. Hebert. Blocks world re-
visited: Image understanding using qualitative geometry and
In European Conference on Computer Vision,
mechanics.
pages 482–496. Springer, 2010. 1

[22] A. Gupta, S. Satkin, A. A. Efros, and M. Hebert. From 3d
In Computer Vision
scene geometry to human workspace.
and Pattern Recognition (CVPR), 2011 IEEE Conference on,
pages 1961–1968. IEEE, 2011. 1, 3

[23] S. Gupta, P. Arbel´aez, R. Girshick, and J. Malik. Aligning
3d models to rgb-d images of cluttered scenes. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 4731–4740, 2015. 3

[24] S. Gupta, P. Arbel´aez, R. Girshick, and J. Malik.

In-
arXiv preprint

ferring 3d object pose in rgb-d images.
arXiv:1502.04652, 2015. 2

[25] S. Han, H. Mao, and W. J. Dally. Deep compression: Com-
pressing deep neural network with pruning, trained quanti-
zation and huffman coding. In ICLR, 2016. 7

[26] R. Hartley and A. Zisserman. Multiple view geometry in
computer vision. Cambridge university press, 2003. 9
[27] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling
in deep convolutional networks for visual recognition.
In
European Conference on Computer Vision, pages 346–361.
Springer, 2014. 5

[28] V. Hedau, D. Hoiem, and D. Forsyth. Recovering free space
of indoor scenes from a single image. In Computer Vision
and Pattern Recognition (CVPR), 2012 IEEE Conference on,
pages 2807–2814. IEEE, 2012. 3

[29] M. Hejrati and D. Ramanan. Categorizing cubes: Revisit-
ing pose normalization. In 2016 IEEE Winter Conference on
Applications of Computer Vision (WACV), pages 1–9. IEEE,
2016. 5

[30] D. Hoiem and S. Savarese. Representations and techniques
for 3d object recognition and scene interpretation. Synthe-
sis Lectures on Artiﬁcial Intelligence and Machine Learning,
5(5):1–169, 2011. 3

[47] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 5

[48] S. Song and J. Xiao. Sliding shapes for 3d object detection
in depth images. In ECCV, pages 634–651. Springer, 2014.
2

[49] S. Song and J. Xiao. Deep sliding shapes for amodal 3d

object detection in rgb-d images. In CVPR, 2016. 2

[50] H. Su, C. R. Qi, Y. Li, and L. J. Guibas. Render for cnn:
Viewpoint estimation in images using cnns trained with ren-
In Proceedings of the IEEE Inter-
dered 3d model views.
national Conference on Computer Vision, pages 2686–2694,
2015. 2

[51] S. Tulsiani and J. Malik. Viewpoints and keypoints.

In
IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), pages 1510–1519. IEEE, 2015. 2, 3

[52] M. Wilczkowiak, P. Sturm, and E. Boyer. Using geomet-
ric constraints through parallelepipeds for calibration and 3d
IEEE transactions on pattern analysis and ma-
modeling.
chine intelligence, 27(2):194–207, 2005. 3

[53] J. Wu, T. Xue, J. J. Lim, Y. Tian, J. B. Tenenbaum, A. Tor-
ralba, and W. T. Freeman. Single image 3d interpreter net-
work. In ECCV, 2016. 2, 3

[54] Y. Xiang, W. Choi, Y. Lin, and S. Savarese. Data-driven
In 2015
3d voxel patterns for object category recognition.
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 1903–1911. IEEE, 2015. 3

[55] J. Xiao, B. Russell, and A. Torralba. Localizing 3d cuboids
In Advances in neural information

in single-view images.
processing systems, pages 746–754, 2012. 3, 5, 6, 7

[56] Y. Yang and D. Ramanan. Articulated human detection with
ﬂexible mixtures of parts. PAMI, 35(12):2878–2890, 2013.
6

[57] Y. Zheng, X. Chen, M.-M. Cheng, K. Zhou, S.-M. Hu, and
N. J. Mitra.
Interactive images: cuboid proxies for smart
image manipulation. ACM Trans. Graph., 31(4):99–1, 2012.
2

[31] E. Hsiao, A. Collet, and M. Hebert. Making speciﬁc features
less discriminative to improve point-based 3d object recog-
nition. 3

[32] F. N. Iandola, M. W. Moskewicz, K. Ashraf, S. Han, W. J.
Dally, and K. Keutzer. Squeezenet: Alexnet-level accuracy
with 50x fewer parameters and¡ 1mb model size. arXiv
preprint arXiv:1602.07360, 2016. 7

[33] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolu-
tional architecture for fast feature embedding. In Proceed-
ings of the 22nd ACM international conference on Multime-
dia, pages 675–678. ACM, 2014. 5

[34] Z. Jia, A. Gallagher, A. Saxena, and T. Chen. 3d-based rea-
In Proceedings
soning with blocks, support, and stability.
of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 1–8, 2013. 1, 2

[35] H. Jiang and J. Xiao. A linear approach to matching cuboids
In Proceedings of the IEEE Conference
in rgbd images.
on Computer Vision and Pattern Recognition, pages 2171–
2178, 2013. 3

[36] A. Kar, S. Tulsiani, J. Carreira, and J. Malik. Category-
speciﬁc object reconstruction from a single image. In 2015
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 1966–1974. IEEE, 2015. 3

[37] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
classiﬁcation with deep convolutional neural networks.
In
Advances in neural information processing systems, pages
1097–1105, 2012. 2, 3

[38] D. C. Lee, M. Hebert, and T. Kanade. Geometric reasoning
for single image structure recovery. In Computer Vision and
Pattern Recognition, 2009. CVPR 2009. IEEE Conference
on, pages 2136–2143. IEEE, 2009. 3

[39] J. J. Lim, A. Khosla, and A. Torralba. Fpm: Fine pose parts-
based model with 3d cad models. In European Conference
on Computer Vision, pages 478–493. Springer, 2014. 3
[40] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, and S. Reed.
Ssd: Single shot multibox detector. In ECCV, 2016. 2, 3, 7
[41] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. Xnor-
net: Imagenet classiﬁcation using binary convolutional neu-
ral networks. In ECCV, 2016. 7

[42] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi. You
In
only look once: Uniﬁed, real-time object detection.
CVPR, 2016. 3

[43] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards
real-time object detection with region proposal networks. In
Advances in neural information processing systems, pages
91–99, 2015. 2, 3, 5

[44] L. G. Roberts. Machine perception of three-dimensional
soups. PhD thesis, Massachusetts Institute of Technology,
1963. 3

[45] S. Savarese and L. Fei-Fei. 3d generic object categorization,
localization and pose estimation. In 2007 IEEE 11th Inter-
national Conference on Computer Vision, pages 1–8. IEEE,
2007. 3

[46] T. Shao, A. Monszpart, Y. Zheng, B. Koo, W. Xu, K. Zhou,
Imagining the unseen: Stability-based
and N. J. Mitra.
cuboid arrangements for scene understanding. ACM Trans-
actions on Graphics, 33(6), 2014. 2

