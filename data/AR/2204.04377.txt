2
2
0
2

r
p
A
9

]

V
C
.
s
c
[

1
v
7
7
3
4
0
.
4
0
2
2
:
v
i
X
r
a

Robotic Surgery Remote Mentoring via AR with
3D Scene Streaming and Hand Interaction

Yonghao Long*, Chengkun Li*, and Qi Dou

Dept. of Computer Science and Engineering, The Chinese University of Hong Kong

Abstract. With the growing popularity of robotic surgery, education
becomes increasingly important and urgently needed for the sake of pa-
tient safety. However, experienced surgeons have limited accessibility due
to their busy clinical schedule or working in a distant city, thus can hardly
provide suﬃcient education resources for novices. Remote mentoring, as
an eﬀective way, can help solve this problem, but traditional methods
are limited to plain text, audio, or 2D video, which are not intuitive nor
vivid. Augmented reality (AR), a thriving technique being widely used
for various education scenarios, is promising to oﬀer new possibilities
of visual experience and interactive teaching. In this paper, we propose
a novel AR-based robotic surgery remote mentoring system with eﬃ-
cient 3D scene visualization and natural 3D hand interaction. Using a
head-mounted display (i.e., HoloLens), the mentor can remotely monitor
the procedure streamed from the trainee’s operation side. The mentor
can also provide feedback directly with hand gestures, which is in-turn
transmitted to the trainee and viewed in surgical console as guidance. We
comprehensively validate the system on both real surgery stereo videos
and ex-vivo scenarios of common robotic training tasks (i.e., peg-transfer
and suturing). Promising results are demonstrated regarding the ﬁdelity
of streamed scene visualization, the accuracy of feedback with hand in-
teraction, and the low-latency of each component in the entire remote
mentoring system. This work showcases the feasibility of leveraging AR
technology for reliable, ﬂexible and low-cost solutions to robotic surgical
education, and holds great potential for clinical applications.
Keywords: Robotic Surgery · Augmented Reality · Remote Mentoring.

Introduction

1
Robotic surgery has been revolutionized in past decades [14,4], greatly enhancing
the surgeon’s perception and operational ability in minimally invasive interven-
tions. With dramatic rise of adoption of robotic surgery, education is important
and requires urgent enhancement [2] for teaching the next generation of surgeons
how to use robots to conduct a surgery step-by-step, instead of using traditional
laparoscopic paradigms. Unfortunately, the novices always have limited access
to experienced surgeons who have intensive clinical schedule or work in a distant
city, making the surgical education currently challenging and expensive [3,5,7].
Remote mentoring, i.e., experts monitor surgical progress and provide guid-
ance for trainees from a diﬀerent geographical location, is envisaged as a promis-
ing way to reduce cost and facilitate surgical education. Hinata et al. [6] employ

*Authors contributed equally to this work.

 
 
 
 
 
 
2

Yonghao Long*, Chengkun Li*, and Qi Dou

video-audio communication and transfer handwritten notes with a touch tablet
for remote mentoring. Their results present similar surgical outcomes (e.g., oper-
ating time, complication, early continence status, positive margin rate) between
the remote and onsite mentoring groups. Mendez et al. [13] propose to transmit
video, audio communication and surgical instructions with electronic stylus and
pad, for a 400km long-distance mentoring in neurosurgery. Experiments validate
its safety and trustworthiness. However, common shortcomings of these methods
are that, the mentoring and monitoring information are conveyed through text,
voice, image and 2D video, which are not intuitive nor vivid for perceiving rich
circumstances of surgery and provide comprehensive feedback by mentors.

Recently, augmented reality (AR) brings new possibility to remote mentoring
along with many beneﬁts [20]. AR can provide an immersive way of displaying
the 3D information (e.g., surgical scene and tool movement) and allow easier and
natural interactions [1,10,16]. Some initial investigations exist with encouraging
progress. For examples, Jarc et al. [8] overlay ghost tools for guidance within the
view of trainee’s console; the mentor sees the surgical scene with a 3D display
and glass, and controls the ghost tools through an extra handle device. Shabir
et al. [18] propose a robotic surgery remote mentoring system, which uses haptic
devices to control virtual tools, but with a high latency observed even in a local
area network. Though much beneﬁts have been demonstrated, existing solutions
still cannot achieve high-ﬁdelity streaming of 3D scenes via AR, which constrains
both eﬃciency and quality of remote mentoring. In addition, the requirement of
extra interaction devices would unavoidably lead to inconvenience of education.
In this paper, to address current limitations, we design a novel robotic surgery
remote mentoring system relying on AR with 3D surgical scene streaming and
hand interaction. Speciﬁcally, we ﬁrst design an eﬃcient streaming scheme to
transmit the 3D surgical scene from the operation side to mentor side, using color
and disparity images to reduce transmission cost. Then, we allow the mentor to
use a portable head-mounted display to perceive 3D surgical scene in point cloud
representation and give feedback guidance with hand gestures conveniently. Last,
the instructions are returned to the trainee side and can be viewed in the console
for straight-forward guidance. The system is validated on both real surgery video
data and common ex-vivo scenarios of robotic training tasks, demonstrating
promising results in terms of ﬁdelity of streamed scene visualization, accuracy of
hand interaction, and low-latency of transmission. To the best of our knowledge,
this is the ﬁrst completely AR-based remote mentoring system with only a head-
mounted display for robotic surgical education. This new type of reliable, ﬂexible
and low-cost solution has great potential to be used in clinical practice.

2 Methods and Materials

The overview of our AR-based robotic surgery remote mentoring system is shown
in Fig. 1. It consists of four components: operation side, surgical scene streaming,
mentor side with hand interaction, guidance feedback for closed-loop education.
Brieﬂy speaking, in the operation side, a trainee conducts training tasks or a
surgery at the console, which shows stereo surgical video, as well as the mentor’s

AR-based Remote Mentoring for Robotic Surgery

3

Fig. 1. The overview of our proposed novel system for robotic surgery remote mentoring
via augmented reality (AR) with 3D scene streaming and hand interaction in HoloLens.

guidance information displayed in AR via TilePro toolkit (Intuitive Surgical Inc).
For surgical scene streaming, the RGB-D information of 3D scene is eﬃciently
transmitted via high-speed internet from the operation side to the remote mentor
side. Then, the mentor (i.e., an expert surgeon) uses a head-mounted AR glass
to see the surgical scene in real-time and accordingly provide guidance with hand
gestures. Finally, the mentor’s inputs with virtual objects are streamed back and
visualized in trainee’s console as a closed-loop education system.

2.1 Eﬃcient Surgical Scene Representation and Streaming
To represent the surgical scene, we opt for RGB-D as 3D representation in op-
eration side, owing to the advantages that it can accurately describe relatively
complex objects with ﬁnite points. It can also be easily obtained without heavy
reconstruction computation compared to mesh, as well as can be straightfor-
wardly and eﬃciently projected to point clouds for 3D visualization. Here, we
ﬁrst apply the HITNet [19], a real-time deep learning method which can extract
the high-level multi-scale context features from a stereo image pair (IL, IR) to
calculate the correspondence for each pixel, and yield the disparity map D of the
left image. These RGB and disparity images are leveraged for transmission, in
order to save memory and keep up the streaming speed for fast system responses.
Diﬀerent from previous work, we design a new compression operation to re-
duce the burden of wireless communication in-between mentor-operation sides.
Qian et al. [17] suggests to transmit ﬂattened RGB-D with three-bytes RGB color
and one-byte disparity value. However, it would sacriﬁce quality due to compact
representation which compresses a ﬂoat type disparity value to one-byte type.
Apart from this, the ﬂatten array representation will also cause heavy network
communication pressure as the frame resolution increasing. For instance, in the
case of 640 × 512 resolution, it would cost 640 × 512 × (3 + 1) × 8 bits = 10 Mb for
every frame. Instead, our calculated disparity map (which is a ﬂoat single chan-
nel image) is separated to two channels, with one denoting the integer part (I
channel) of the disparity map, and the other for the fractional part (F channel).
These two channels are then combined with an all-zero placeholder P channel
to generate a 3-channel image to fulﬁll the format requirement of the eﬃcient
“JPEG” compression algorithm, whose information loss and computational time

Operation SideStereo MatchingSurgicalRobotMentor SideStereoEndoscopeEncodeDecodeCoordinateTransformationPositionRotationFlagsOverlaid FrameVirtual ObjectSurgeonConsole3D ConsoleViewSurgicalSceneStreamingTileProInputOriginalviewCompressedData FlowNetworkTransmissionGuidanceFeedbackShader BufferVertex ShaderzRGBxyPRGBIFCamera ParametersOtherGuidanceHand GestureHMD (HoloLens)Camera viewNetworkTransmissionVirtual ObjectRendering and VisualizationMatchingLeftRightDisparity ImageIFPColor ImageRGBPoint Cloud4

Yonghao Long*, Chengkun Li*, and Qi Dou

will not exhibit exponential growth with larger data. Finally, the IFP image and
corresponding RGB frame are both encoded as “JPEG” format and then com-
bined to be our surgical scene streaming data ﬂow. In this way, we can reduce
the transmission data size to around 8 times smaller than previous work [17], i.e.
1.25 Mb for the resolution of 640×512. The data ﬂow is then sent to mentor side
using the internet socket TCP/IP. Similarly, we do “JPEG” decoding in remote
side to obtain RGB and IFP images for further processing.

2.2 HMD Visualization and Hand-based Natural Interaction
As mentor side is an expert who requires to setup and use the system as conve-
nient as possible, we leverage optical see-through HMD to fulﬁll the requirement.
Speciﬁcally, we ﬁst convert the IFP channels to disparity image and transfer it
to a point cloud with the following projection equation:

z = f · b ·

1
D(u, v)

,

x = z · (u − cx) ·

1
f

,

y = z · (u − cy) ·

1
f

,

(1)

where cx, cy are the camera centroid, f is focal length, and b is the baseline length
of the stereo camera, which can all be obtained by camera calibration. With
Eq.(1), the disparity vector q ∈ R3 = (u, v, D(u, v))T is projected to a spacial
point p ∈ R3 = (x, y, z)T . Given the color information from its corresponding
RGB image IL, we can obtain the point cloud representation of the surgical
scene as p ∈ R6 = (x, y, z, r, g, b)T . Then, we leverage renderer’s shader to
render and visualize the point cloud of the surgical scene. For correctly rendering
the point cloud frame in the HMD view, it is important to perform coordinate
transformations. In this regard, we propose the following scheme as:

˙pH = KH

W · KW

O · KO

C · ˙p, K =

(cid:18) R t
0T 1

(cid:19)

,

˙p = (pT |1)T ,

(2)

where pH and p are respectively the points in the HMD and camera coordi-
nate frame, the dot notation means their corresponding homogenous vectors. K
is transformation matrix where R and t denote rotation and translation shift
3, t ∈ R3(cid:9). For K, We use dif-
within the euclidean group SE3 := (cid:8)R, t|R ∈ SO
ferent subscripts to indicate diﬀerent coordinate system, {O} denotes the ren-
dering centroid which is deﬁned by box boundary of the ﬁrst point cloud frame,
{C} is the camera coordinate frame, {W } is the real world coordinate system,
{H} is the HMD coordinate frame. For instance, KH
W indicates the transforma-
tion matrix from the real world to HMD coordinate system. In our case, KO
C
is ﬁxed and KH
W can be automatically obtained from HMD. Users can ﬂexibly
change the location and scale of the point cloud scene by KW

O for interaction.

Importantly, we design all the mentor’s interactions to be conducted purely
by human hand which is highly user-friendly. Speciﬁcally, the cameras on HMD
will capture multi-view images, compute the depth information, and reconstruct
the environment. Then we employ built-in computer vision algorithms to analyze
the spatial and pose information to recognize hand gestures, and locate the hand
position w.r.t. the visualized virtual scene to achieve a hand-based interaction.
For example, we design to raise the hand for waking up the setting menu, move
ﬁnger in the air for pointing to certain critical anatomy in the scene. Moreover,

AR-based Remote Mentoring for Robotic Surgery

5

we create assisting virtual objects (e.g., needle) with AR to facilitate necessary
demonstration provided by the mentor. We design CAD models of the virtual ob-
jects and import them into HMD. With our system, the mentor can manipulate
the virtual object purely using a hand, just like naturally moving an object in
the real-world. For example, the mentor can manipulate a virtual needle with a
pinch to rotate and move, and draw lines as instructions of suturing trajectories.
These interactions can be recorded in real-time as the mentor’s feedback.

2.3 Closed-loop Feedback for Guidance in Surgeon Console
To feedback the guidance information of the mentor, we record the hand interac-
tions and virtual objects, and transmit them back to the operation side. Instead
of directly sending rendered image with overlaid guidance information, we again
design a light-weight data transfer approach to boost the eﬃciency. Speciﬁcally,
we only send well-formatted location information to operation side, which can
be the position of the virtual surgical tool, position of the draw trajectory, etc.
The format of the feedback data is as {m, i, αC, βC, γC, xC, yC, zC}, where m is
a ﬂag to denote whether this information is available, i is used to decide which
kinds of information is contained (such as diﬀerent kinds of virtual object),
the (αC, βC, γC) means the yaw, pitch, and roll angle in the camera coordinate
frame respectively, the (xC, yC, zC) represents the position of speciﬁed object.
The feedback message is also sent to operating side using the TCP/IP Socket.
After receiving the feedback information, operation side will decode the message
and overlay related guidance information on the stereo video, and ﬁnally input
to TilePro in the surgical robot console as augmented view to be perceived.

In this way, we build our system as an eﬀective close-loop education paradigm.
The mentor of expert surgeon can observe remote surgical procedures by simply
wearing an AR glass. By manipulating virtual objects or indicating instructions
with hands, the mentor can easily provide supportive advice to trainees in real-
time. This new AR-based remote mentoring solution is distinct from traditional
ones that use 2D video or haptic device. We have advantages of immersive visual
experience, natural interaction, high-speed and low-cost, as well as great poten-
tial for richer information to be added on top of the augmented environment.

3 Experiments

3.1 System Setup and Experiment Design
For system setup, we use an in-house daVinci Si surgical robot as the operation
side, which is retired but generally similar to the robot used in clinic. Its stereo
video is streamed to a computer equipping NVIDIA TITAN RTX GPU for com-
puting 3D representation of the surgical scene. The mentor side uses a Microsoft
HoloLens 2, the prevalent portable AR device, to visualize the 3D surgical scene
and provide hand interaction with Unity toolkit. We set up the wireless network
using a low-cost WIFI router (TP-LINK AX3000) to connect the computer and
HoloLens for remote surgical scene streaming and guidance feedback.

For experiment design, we extensively validate our proposed system on both
real surgery stereo videos and ex-vivo scenarios of common robotic training tasks.

6

Yonghao Long*, Chengkun Li*, and Qi Dou

Fig. 2. Case studies on real surgery data of prostatectomy. (a) Mentor uses the hand
to point key anatomies in surgical scene which can be shown in the operation console
for the trainee. (b) Mentor manipulates a virtual needle and draw suturing trajectories
(green line) which can be demonstrated to the trainee. More details are in Sec. 3.2.

For real surgery, we use two typical cases of DaVinci robotic surgery of prostate-
ctomy procedure, with stereo videos captured in the resolution of 1280×1024 at
60f ps. To reduce computation cost of disparity map with HITNet [19], we ﬁrst
downsample the resolution to 640×512. We play the video in its original speed
for simulation of the real clinical scenario with our system setup. The HoloLens
can render the scene at the range of 25 ∼ 30f ps for the mentor side. For ex-vivo
scenarios with daVinci Si, we capture stereo videos with the same resolution
and frame rate as the real surgery setting. We implement two scenarios, i.e.,
peg-transfer and suturing, which are common tasks for surgical skill training [9].
In peg-transfer, the trainee picks up the block from one peg and moves it to
another target peg. In suturing, the trainee manipulates the needle to suture the
phantom wound. We use these ex-vivo experiments for quantitative evaluations
of the interaction accuracy and remote mentoring latency of our system.

3.2 Feasibility Study on Real Surgical Scenes
To demonstrate the application scenario of our remote mentoring system in the
real clinical environment, we show two case studies with the prostatectomy pro-
cedure. As depicted in Fig. 2(a), the surgeon is conducting the step of “dissection
of the seminal vesicles and rectum” in prostatectomy. Inexperienced surgeons can
meet diﬃculties to precisely identify the positions of key anatomies for dissec-
tion, due to fuzzy blood occlusion and poor lighting condition. With our system,
the expert can monitor the procedure for a minute-long duration, and point
to the key anatomies in the air with hand from time to time. These hints are
feedback to the operator’s console for guidance. Similarly, in Fig. 2(b), the sur-
gical step is “van velthoven anastomosis and rocco stitch”, in which suturing is
demanding. The surgeon needs to incorporate as much of the muscular struc-
tural support behind the urethra as possible, therefore has to handle multiple

Mentor SideOperation Sidet=1st=5st=10st=15sMentor SideOperation Sidet=1mint=5mint=10mint=15mint=20st=20min(a)(b)AR-based Remote Mentoring for Robotic Surgery

7

Table 1. The accuracy results for two ex-vivo scenarios.

Scenario
Re-projection error (pixel)

Peg-transfer
7.35 ± 2.1

Suturing
5.12 ± 1.47

Fig. 3. Two scenarios for testing the accuracy, with peg-transfer evaluating pointing
accuracy and suturing evaluating the virtual needle interaction accuracy.

suturing targets and complex needle trajectories which always require expertise.
Using our system, the expert can provide help with a hand holding a virtual
needle to indicate the next suturing target and demonstrate suturing trajectory.
Such feedback together with the virtual needle can again be transmitted back to
operation console for helping trainees. These results on real surgical data present
the ﬁdelity of streamed surgical scenes and ﬂexibility of hand interactions, thus
demonstrating promising feasibility for application in clinical practice.

3.3 Quantitative Evaluation on Ex-vivo Scenarios

Accuracy evaluation. To evaluate how accurate the hand-based interactions
can achieve in this system, we conduct following experiments. Speciﬁcally, we
evaluate the pointing accuracy using peg-transfer scenario, where mentor points
to the position of starting peg and ending peg with hand in HoloLens, and
then feedback the position to surgeon’s console in operation side, as shown in
Fig. 3(a). We ﬁrst manually locate 2D location of the starting and ending peg tips
as ground truth. Then we get the hand-indicated 3D position from HoloLens and
re-project it to 2D location on console view. We then calculate the error between
re-projected 2D location and ground truth. In this way, we can obtain the close-
loop pointing error in operation side. We conduct 30 trials and average the ﬁnal
results. As shown in Tab. 1, the position error is 7.35 pixel in average, which is
hard to be perceived by human eyes and also within acceptable range compared
with HoloLens calibration error [15], thus precise enough for pointing.

Moreover, we evaluate the interaction accuracy of hand manipulation of
virtual-object using suturing scenario, where the mentor side is asked to move
a virtual needle with hand and align it to the real needle seen in the HoloLens,
as shown in Fig. 3(b). Similarly, we manually locate 2D location of needle as
ground truth, then record the “aligned” pose of virtual needle and re-project
it to 2D console view as “aligned” 2D location. We calculate the error between
’aligned’ 2D location and ground truth. In this way, we can obtain the close-loop
virtual-object based interaction error in operation side. We also conduct 30 tri-

Peg-transfer TaskSuturing TaskLocate Start PegLocate End PegMentor Side ViewOperate Virtual NeedleVirtual-Real AlignmentMentor Side View(a)(b)Operation Side ViewOperation Side View8

Yonghao Long*, Chengkun Li*, and Qi Dou

Table 2. The update frame rate in HoloLens under diﬀerent transmission resolutions.

Resolution 1280×720 960×540
10.43
FPS (Hz)

21.35

640×480
32.68

480×360
41.13

320×240
53.42

Fig. 4. The latency analysis of each component in our proposed system.

als and average the results. As shown in Tab. 1, 2D re-projection error is 5.12
pixel in average, also consistent to HoloLens calibration error [15], thus precise
enough for virtual-object based interaction of surgical mentoring.

Latency evaluation. To thoroughly evaluate the overall latency, we test our
system under diﬀerent common-used video resolutions from 320×240 to 1280×720.
Speciﬁcally, Tab. 2 reports the HoloLens update frame rate in mentor side, which
can achieve satisfying real-time update speed (32.68f ps) with standard deﬁnition
resolution (640×480). Even when given high deﬁnition resolution (1280×720),
the HoloLens can still achieve acceptable update rate (10.43f ps) to fulﬁll the low
requirement of mentor side for basic visual-hand interaction. We also evaluate
the end-to-end closed-loop latency for operation side and distinguish diﬀerent
components’ latency, as shown in Fig 4. The total latency ranges from 65.9 ±
5.0 ms to 322.6 ± 15.1 ms with diﬀerent resolution with even the highest latency
is within the acceptable latency (330 ms) in terms of surgeon’s perception of
safety according to prior studies [11,12], thus feasible and eﬃcient for remote
mentoring. We further discuss the latency of each component. Relative constant
time for console overlaying is observed, as GPU can overlay guidance parallelly.
And the disparity computing and HoloLens rendering time decrease dramatically
when given lower resolution, showing the high eﬃciency of our system.

4 Conclusion and Future Work

In conclusion, we present the ﬁrst completely AR-based remote mentoring sys-
tem with a head-mounted display for robotic surgical education. Experiments
and the achieved promising results have demonstrated its potential as a new re-
liable, ﬂexible and low-cost solution for next-generation remote mentoring. For
future work, we will extend the system to allow cooperative two-hands interac-
tion, in order to facilitate mentoring for more complex procedures. Moreover,
we will conduct systematic user study with our surgeons, and they have already
expressed interest and potential usefulness for their clinical practice.

050100150200250300350320*240480*360640*480960*5401280*720Time (ms)ResolutionSystemLatencyAnalysisDisparty ComputingData Flow EncodingNetwork TransmissionData Flow DecodingHoloLens RenderingConsole OverlayingAR-based Remote Mentoring for Robotic Surgery

9

References

1. Barmaki, R., Yu, K., Pearlman, R., Shingles, R., Bork, F., Osgood, G.M., Navab,
N.: Enhancement of anatomical education using augmented reality: An empirical
study of body painting. Anatomical sciences education 12(6), 599–609 (2019) 2
2. Chen, I., Alan, H., Ghazi, A., Sridhar, A., Stoyanov, D., Slack, M., Kelly, J.D.,
Collins, J.W.: Evolving robotic surgery training and improving patient safety, with
the integration of novel technologies. World Journal of Urology 39(8), 2883–2893
(2021) 1

3. Collins, J.W., Ma, R., Beaulieu, Y., Hung, A.J.: Telementoring for minimally in-

vasive surgery. In: Digital Surgery, pp. 361–378. Springer (2021) 1

4. D’Ettorre, C., Mariani, A., Stilli, A., Valdastri, P., Deguet, A., Kazanzides, P.,
Taylor, R.H., Fischer, G.S., DiMaio, S.P., Menciassi, A., et al.: Accelerating surgical
robotics research: Reviewing 10 years of research with the dvrk. arXiv preprint
arXiv:2104.09869 (2021) 1

5. Erridge, S., Yeung, D.K., Patel, H.R., Purkayastha, S.: Telementoring of surgeons:

a systematic review. Surgical innovation 26(1), 95–111 (2019) 1

6. Hinata, N., Miyake, H., Kurahashi, T., Ando, M., Furukawa, J., Ishimura, T.,
Tanaka, K., Fujisawa, M.: Novel telementoring system for robot-assisted radical
prostatectomy: impact on the learning curve. Urology 83(5), 1088–1092 (2014) 1
7. Huang, E.Y., Knight, S., Guetter, C.R., Davis, C.H., Moller, M., Slama, E., Cran-
dall, M.: Telemedicine and telementoring in the surgical specialties: a narrative
review. The American Journal of Surgery 218(4), 760–766 (2019) 1

8. Jarc, A.M., Stanley, A.A., Cliﬀord, T., Gill, I.S., Hung, A.J.: Proctors exploit
three-dimensional ghost tools during clinical-like training scenarios: a preliminary
study. World journal of urology 35(6), 957–965 (2017) 2

9. Joseph, R.A., Goh, A.C., Cuevas, S.P., Donovan, M.A., Kauﬀman, M.G., Salas,
N.A., Miles, B., Bass, B.L., Dunkin, B.J.: “chopstick” surgery: a novel tech-
nique improves surgeon performance and eliminates arm collision in robotic single-
incision laparoscopic surgery. Surgical endoscopy 24(6), 1331–1335 (2010) 6
10. Kovoor, J.G., Gupta, A.K., Gladman, M.A.: Validity and eﬀectiveness of aug-
mented reality in surgical education: A systematic review. Surgery (2021) 2
11. Lacy, A., Bravo, R., Otero-Pi˜neiro, A., Pena, R., De Lacy, F., Menchaca, R., Bal-
ibrea, J.: 5g-assisted telementored surgery. British Journal of Surgery 106(12),
1576–1579 (2019) 8

12. Marescaux, J., Leroy, J., Gagner, M., Rubino, F., Mutter, D., Vix, M., Butner, S.E.,
Smith, M.K.: Transatlantic robot-assisted telesurgery. Nature 413(6854), 379–380
(2001) 8

13. Mendez, I., Hill, R., Clarke, D., Kolyvas, G., Walling, S.: Robotic long-distance

telementoring in neurosurgery. Neurosurgery 56(3), 434–440 (2005) 2

14. Navab, N.: Robotic imaging, machine learning and augmented reality for computer
assisted interventions. In: Medical Imaging 2021: Imaging Informatics for Health-
care, Research, and Applications. vol. 11601, p. 1160103. International Society for
Optics and Photonics (2021) 1

15. Qian, L., Azimi, E., Kazanzides, P., Navab, N.: Comprehensive tracker based dis-
play calibration for holographic optical see-through head-mounted display. arXiv
preprint arXiv:1703.05834 (2017) 7, 8

16. Qian, L., Wu, J.Y., DiMaio, S.P., Navab, N., Kazanzides, P.: A review of augmented
reality in robotic-assisted surgery. IEEE Transactions on Medical Robotics and
Bionics 2(1), 1–16 (2019) 2

10

Yonghao Long*, Chengkun Li*, and Qi Dou

17. Qian, L., Zhang, X., Deguet, A., Kazanzides, P.: Aramis: Augmented reality as-
sistance for minimally invasive surgery using a head-mounted display. In: Interna-
tional Conference on Medical Image Computing and Computer-Assisted Interven-
tion. pp. 74–82. Springer (2019) 3, 4

18. Shabir, D., Abdurahiman, N., Padhan, J., Trinh, M., Balakrishnan, S., Kurer, M.,
Ali, O., Al-Ansari, A., Yaacoub, E., Deng, Z., et al.: Towards development of a tele-
mentoring framework for minimally invasive surgeries. The International Journal
of Medical Robotics and Computer Assisted Surgery 17(5), e2305 (2021) 2

19. Tankovich, V., Hane, C., Zhang, Y., Kowdle, A., Fanello, S., Bouaziz, S.: Hit-
net: Hierarchical iterative tile reﬁnement network for real-time stereo matching.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 14362–14372 (2021) 3, 6

20. Yoon, J.W., Chen, R.E., Kim, E.J., Akinduro, O.O., Kerezoudis, P., Han, P.K., Si,
P., Freeman, W.D., Diaz, R.J., Komotar, R.J., et al.: Augmented reality for the
surgeon: systematic review. The International Journal of Medical Robotics and
Computer Assisted Surgery 14(4), e1914 (2018) 2

