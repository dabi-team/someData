0
2
0
2

t
c
O
4
2

]

G
L
.
s
c
[

1
v
0
1
8
2
1
.
0
1
0
2
:
v
i
X
r
a

Autoregressive Score Matching

Chenlin Meng
Stanford University
chenlin@stanford.edu

Lantao Yu
Stanford University
lantaoyu@cs.stanford.edu

Yang Song
Stanford University
yangsong@cs.stanford.edu

Jiaming Song
Stanford University
tsong@cs.stanford.edu

Stefano Ermon
Stanford University
ermon@cs.stanford.edu

Abstract

Autoregressive models use chain rule to deﬁne a joint probability distribution as
a product of conditionals. These conditionals need to be normalized, imposing
constraints on the functional families that can be used. To increase ﬂexibility, we
propose autoregressive conditional score models (AR-CSM) where we parameterize
the joint distribution in terms of the derivatives of univariate log-conditionals
(scores), which need not be normalized. To train AR-CSM, we introduce a new
divergence between distributions named Composite Score Matching (CSM). For
AR-CSM models, this divergence between data and model distributions can be
computed and optimized efﬁciently, requiring no expensive sampling or adversarial
training. Compared to previous score matching algorithms, our method is more
scalable to high dimensional data and more stable to optimize. We show with
extensive experimental results that it can be applied to density estimation on
synthetic data, image generation, image denoising, and training latent variable
models with implicit encoders.

1

Introduction

Autoregressive models play a crucial role in modeling high-dimensional probability distributions.
They have been successfully used to generate realistic images [18, 21], high-quality speech [17], and
complex decisions in games [29]. An autoregressive model deﬁnes a probability density as a product
of conditionals using the chain rule. Although this factorization is fully general, autoregressive
models typically rely on simple probability density functions for the conditionals (e.g. a Gaussian or
a mixture of logistics) [21] in the continuous case, which limits the expressiveness of the model.

To improve ﬂexibility, energy-based models (EBM) represent a density in terms of an energy function,
which does not need to be normalized. This enables more ﬂexible neural network architectures, but
requires new training strategies, since maximum likelihood estimation (MLE) is intractable due to
the normalization constant (partition function). Score matching (SM) [9] trains EBMs by minimizing
the Fisher divergence (instead of KL divergence as in MLE) between model and data distributions. It
compares distributions in terms of their log-likelihood gradients (scores) and completely circumvents
the intractable partition function. However, score matching requires computing the trace of the
Hessian matrix of the model’s log-density, which is expensive for high-dimensional data [14].

To avoid calculating the partition function without losing scalability in high dimensional settings,
we leverage the chain rule to decompose a high dimensional distribution matching problem into
simpler univariate sub-problems. Speciﬁcally, we propose a new divergence between distributions,
named Composite Score Matching (CSM), which depends only on the derivatives of univariate log-
conditionals (scores) of the model, instead of the full gradient as in score matching. CSM training is

34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.

 
 
 
 
 
 
particularly efﬁcient when the model is represented directly in terms of these univariate conditional
scores. This is similar to a traditional autoregressive model, but with the advantage that conditional
scores, unlike conditional distributions, do not need to be normalized. Similar to EBMs, removing
the normalization constraint increases the ﬂexibility of model families that can be used.

Leveraging existing and well-established autoregressive models, we design architectures where we
can evaluate all dimensions in parallel for efﬁcient training. During training, our CSM divergence can
be optimized directly without the need of approximations [15, 25], surrogate losses [11], adversarial
training [5] or extra sampling [3]. We show with extensive experimental results that our method
can be used for density estimation, data generation, image denoising and anomaly detection. We
also illustrate that CSM can provide accurate score estimation required for variational inference with
implicit distributions [8, 25] by providing better likelihoods and FID [7] scores compared to other
training methods on image datasets.

2 Background

Given i.i.d. samples {x(1), ..., x(N )} ⊂ RD from some unknown data distribution p(x), we want to
learn an unnormalized density ˜qθ(x) as a parametric approximation to p(x). The unnormalized ˜qθ(x)
uniquely deﬁnes the following normalized probability density:

qθ(x) =

(cid:90)

, Z(θ) =

˜qθ(x)
Z(θ)

˜qθ(x)dx,

(1)

where Z(θ), the partition function, is generally intractable.

2.1 Autoregressive Energy Machine

To learn an unnormalized probabilistic model, [15] proposes to approximate the normalizing constant
using one dimensional importance sampling. Speciﬁcally, let x = (x1, ..., xD) ∈ RD. They ﬁrst
learn a set of one dimensional conditional energies Eθ(xd|x<d) (cid:44) − log ˜qθ(xd|x<d), and then
approximate the normalizing constants using importance sampling, which introduces an additional
network to parameterize the proposal distribution. Once the partition function is approximated, they
normalize the density to enable maximum likelihood training. However, approximating the partition
function not only introduces bias into optimization but also requires extra computation and memory
usage, lowering the training efﬁciency.

2.2 Score Matching

To avoid computing Z(θ), we can take the logarithm on both sides of Eq. (1) and obtain log qθ(x) =
log ˜qθ(x) − log Z(θ). Since Z(θ) does not depend on x, we can ignore the intractable partition
function Z(θ) when optimizing ∇x log qθ(x). In general, ∇x log qθ(x) and ∇x log p(x) are called
the score of qθ(x) and p(x) respectively. Score matching (SM) [9] learns qθ(x) by matching the
scores between qθ(x) and p(x) using the Fisher divergence:
L(qθ; p) (cid:44) 1
2

Ep[(cid:107)∇x log p(x) − ∇x log qθ(x)(cid:107)2
2].

(2)

Ref. [9] shows that under certain regularity conditions L(θ; p) = J(θ; p) + C, where C is a constant
that does not depend on θ and J(θ; p) is deﬁned as below:

J(θ; p) (cid:44) Ep

(cid:20) 1
2

(cid:107)∇x log qθ(x)(cid:107)2

2 + tr(∇2

x log qθ(x))

(cid:21)
,

where tr(·) denotes the trace of a matrix. The above objective does not involve the intractable term
∇x log p(x). However, computing tr(∇2
x log qθ(x)) is in general expensive for high dimensional
data. Given x ∈ RD, a naive approach requires D times more backward passes than computing
the gradient ∇x log qθ(x) [25] in order to compute tr(∇2
x log qθ(x)), which is inefﬁcient when D
is large. In fact, ref. [14] shows that within a constant number of forward and backward passes, it
is unlikely for an algorithm to be able to compute the diagonal of a Hessian matrix deﬁned by any
arbitrary computation graph.

2

3 Composite Score Matching

To make SM more scalable, we introduce Composite Score Matching (CSM), a new divergence
suitable for learning unnormalized statistical models. We can factorize any given data distribution
p(x) and model distribution qθ(x) using the chain rule according to a common variable ordering:

p(x) =

D
(cid:89)

d=1

p(xd|x<d),

qθ(x) =

D
(cid:89)

d=1

qθ(xd|x<d)

where xd ∈ R stands for the d-th component of x, and x<d refers to all the entries with indices
smaller than d in x. Our key insight is that instead of directly matching the joint distributions, we can
match the conditionals of the model qθ(xd|x<d) to the conditionals of the data p(xd|x<d) using the
Fisher divergence. This decomposition results in simpler problems, which can be optimized efﬁciently
using one-dimensional score matching. For convenience, we denote the conditional scores of qθ and
p as sθ,d(x<d, xd) (cid:44) ∂
log p(xd|x<d) :
∂xd
Rd−1 × R → R respectively. This gives us a new divergence termed Composite Score Matching
(CSM):

log qθ(xd|x<d) : Rd−1 × R → R and sd(x<d, xd) (cid:44) ∂
∂xd

LCSM (qθ; p) =

1
2

D
(cid:88)

d=1

Ep(x<d)Ep(xd|x<d)

(cid:20)
(sd(x<d, xd) − sθ,d(x<d, xd))2

(cid:21)
.

(3)

This divergence is inspired by composite scoring rules [1], a general technique to decompose
distribution-matching problems into lower-dimensional ones. As such, it bears some similarity with
pseudo-likelihood, a composite scoring rule based on KL-divergence. As shown in the following
theorem, it can be used as a learning objective to compare probability distributions:
Theorem 1 (CSM Divergence). LCSM (qθ, p) vanishes if and only if qθ(x) = p(x) a.e.

Proof Sketch. If the distributions match, their derivatives (conditional scores) must be the same,
hence LCSM is zero. If LCSM is zero, the conditional scores must be the same, and that uniquely
determines the joints. See Appendix for a formal proof.

Eq. (3) involves sd(x), the unknown score function of the data distribution. Similar to score matching,
we can apply integration by parts to obtain an equivalent but tractable expression:

JCSM (θ; p) =

D
(cid:88)

d=1

Ep(x<d)Ep(xd|x<d)

(cid:20) 1
2

sθ,d(x<d, xd)2 +

(cid:21)
,
sθ,d(x<d, xd)

∂
∂xd

(4)

The equivalence can be summarized using the following results:
Theorem 2 (Informal). Under some regularity conditions, LCSM (θ; p) = JCSM (θ; p) + C where
C is a constant that does not depend on θ.

Proof Sketch. Integrate by parts the one-dimensional SM objectives. See Appendix for a proof.

Corollary 1. Under some regularity conditions, JCSM (θ, p) is minimized when qθ(x) = p(x) a.e.

In practice, the expectation in JCSM (θ; p) can be approximated by a sample average using the
following unbiased estimator
ˆJCSM (θ; p) (cid:44) 1
N

<d, x(i)
d )

sθ,d(x(i)

sθ,d(x(i)

<d, x(i)

d )2 +

(cid:20) 1
2

N
(cid:88)

D
(cid:88)

(cid:21)
,

(5)

∂
∂x(i)
d

i=1

d=1

where {x(1), ..., x(N )} are i.i.d samples from p(x). It is clear from Eq. (5) that evaluating ˆJCSM (θ; p)
is efﬁcient as long as it is efﬁcient to evaluate sθ,d(x<d, xd) (cid:44) ∂
log qθ(xd|x<d) and its derivative
∂xd
∂
sθ,d(x<d, xd). This in turn depends on how the model qθ is represented. For example, if qθ is
∂xd
an energy-based model deﬁned in terms of an energy ˜qθ as in Eq. (1), computing qθ(xd|x<d) (and
hence its derivative, sθ,d(x<d, xd)) is generally intractable. On the other hand, if qθ is a traditional
autoregressive model represented as a product of normalized conditionals, then ˆJCSM (θ; p) will be
efﬁcient to optimize, but the normalization constraint may limit expressivity. In the following, we
propose a parameterization tailored for CSM training, where we represent a joint distribution directly
in terms of sθ,d(x<d, xd), d = 1, · · · D without normalization constraints.

3

4 Autoregressive conditional score models

We introduce a new class of probabilistic models, named autoregressive conditional score models
(AR-CSM), deﬁned as follows:
Deﬁnition 1. An autoregressive conditional score model over RD is a collection of D functions
ˆsd(x<d, xd) : Rd−1 × R → R, such that for all d = 1, · · · , D:

1. For all x<d ∈ Rd−1, there exists a function Ed(x<d, xd) : Rd−1 × R → R such that

∂
∂xd

Ed(x<d, xd) exists, and ∂
∂xd

Ed(x<d, xd) = ˆsd(x<d, xd).

2. For all x<d ∈ Rd−1, Zd(x<d) (cid:44) (cid:82) eEd(x<d,xd)dxd exists and is ﬁnite (i.e., the improper

integral w.r.t. xd is convergent).

Autoregressive conditional score models are an expressive family of probabilistic models for continu-
ous data. In fact, there is a one-to-one mapping between the set of autoregressive conditional score
models and a large set of probability densities over RD:
Theorem 3. There is a one-to-one mapping between the set of autoregressive conditional score
models over RD and the set of probability density functions q(x) fully supported over RD such that
log q(xd|x<d) exists for all d and x<d ∈ Rd−1. The mapping pairs conditional scores and
∂
∂xd
densities such that

ˆsd(x<d, xd) =

log q(xd|x<d)

∂
∂xd

The key advantage of this representation is that the functions in Deﬁnition 1 are easy to parameterize
(e.g., using neural networks) as the requirements 1 and 2 are typically easy to enforce. In contrast
with typical autoregressive models, we do not require the functions in Deﬁnition 1 to be normalized.
Importantly, Theorem 3 does not hold for previous approaches that learn a single score function for
the joint distribution [25, 24], since the score model sθ : RD → RD in their case is not necessarily
the gradient of any underlying joint density. In contrast, AR-CSM always deﬁne a valid density
through the mapping given by Theorem 3.

In the following, we discuss how to use deep neural networks to parameterize autoregressive condi-
tional score models (AR-CSM) deﬁned in Deﬁnition 1. To simplify notations, we hereafter use x to
denote the arguments for sθ,d and sd even when these functions depend on a subset of its dimensions.

4.1 Neural AR-CSM models

We propose to parameterize an AR-CSM based on existing autoregressive architectures for traditional
(normalized) density models (e.g., PixelCNN++ [21], MADE [4]). One important difference is that
the output of standard autoregressive models at dimension d depend only on x<d, yet we want the
conditional score sθ,d to also depend on xd.
To ﬁll this gap, we use standard autoregressive models to parameterize a "context vector" cd ∈ Rc
(c is ﬁxed among all dimensions) that depends only on x<d, and then incorporate the dependency
on xd by concatenating cd and xd to get a c + 1 dimensional vector hd = [cd, xd]. Next, we
feed hd into another neural network which outputs the scalar sθ,d ∈ R to model the conditional
score. The network’s parameters are shared across all dimensions similar to [15]. Finally, we can
compute ∂
sθ,d(x) using automatic differentiation, and optimize the model directly with the CSM
∂xd
divergence.

Standard autoregressive models, such as PixelCNN++ and MADE, model the density with a prescribed
probability density function (e.g., a Gaussian density) parameterized by functions of hd. In contrast,
we remove the normalizing constraints of these density functions and therefore able to capture
stronger correlations among dimensions with more expressive architectures.

4.2

Inference and learning

To sample from an AR-CSM model, we use one dimensional Langevin dynamics to sample from
each dimension in turn. Crucially, Langevin dynamics only need the score function to sample from
a density [19, 6]. In our case, scores are simply the univariate derivatives given by the AR-CSM.

4

Speciﬁcally, we use sθ,1(x1) to obtain a sample x1 ∼ qθ(x1), then use sθ,2(x1, x2) to sample from
x2 ∼ qθ(x2 | x1) and so forth. Compared to Langevin dynamics performed directly on a high
dimensional space, one dimensional Langevin dynamics can converge faster under certain regularity
conditions [20]. See Appendix C.3 for more details.

During training, we use the CSM divergence (see Eq. (5)) to train the model. To deal with data
distributions supported on low-dimensional manifolds and the difﬁculty of score estimation in low
data density regions, we use noise annealing similar to [24] with slight modiﬁcations: Instead of
performing noise annealing as a whole, we perform noise annealing on each dimension individually.
More details can be found in Appendix C.

5 Density estimation with AR-CSM

In this section, we ﬁrst compare the optimization performance of CSM with two other variants of
score matching: Denoising Score Matching (DSM) [28] and Sliced Score Matching (SSM) [25], and
compare the training efﬁciency of CSM with Score Matching (SM) [9]. Our results show that CSM
is more stable to optimize and more scalable to high dimensional data compared to the previous score
matching methods. We then perform density estimation on 2-d synthetic datasets (see Appendix B)
and three commonly used image datasets: MNIST, CIFAR-10 [12] and CelebA [13]. We further
show that our method can also be applied to image denoising and anomaly detection, illustrating
broad applicability of our method.

5.1 Comparison with other score matching methods

Setup To illustrate the scalability of CSM, we consider a simple setup of learning Gaussian
distributions. We train an AR-CSM model with CSM and the other score matching methods on a
fully connected network with 3 hidden layers. We use comparable number of parameters for all the
methods to ensure fair comparison.

(a) Training time per iteration.

(b) Variance comparison.

Figure 1: Comparison with SSM and SM in terms of loss variance and computational efﬁciency.

CSM vs. SM In Figure 1a, we show the time per iteration of CSM versus the original score
matching (SM) method [9] on multivariate Gaussians with different data dimensionality. We ﬁnd
that the training speed of SM degrades linearly as a function of the data dimensionality. Moreover,
the memory required grows rapidly w.r.t the data dimension, which triggers memory error on 12 GB
TITAN Xp GPU when the data dimension is approximately 200. On the other hand, for CSM, the
time required stays stable as the data dimension increases due to parallelism, and no memory errors
occurred throughout the experiments. As expected, traditional score matching (SM) does not scale as
well as CSM for high dimensional data. Similar results on SM were also reported in [25].

CSM vs. SSM We compare CSM with Sliced Score Matching (SSM) [25], a recently proposed
score matching variant, on learning a representative Gaussian N (0, 0.12I) of dimension 100 in
Figure 2 (2 rightmost panels). While CSM converges rapidly, SSM does not converge even after
20k iterations due to the large variance of random projections. We compare the variance of the two
objectives in Figure 1b. In such a high-dimensional setting, SSM would require a large number of
projection vectors for variance reduction, which requires extra computation and could be prohibitively
expensive in practice. By contrast, CSM is a deterministic objective function that is more stable to
optimize. This again suggests that CSM might be more suitable to be used in high-dimensional data
settings compared to SSM.

5

DSM (σ = 0.01)

DSM (σ = 0.05)

DSM (σ = 0.1)

SSM (σ = 0)

CSM (σ = 0)

Figure 2: Training losses for DSM, SSM and CSM on 100-d Gaussian distribution N (0, 0.12I). Note
the vertical axes are different across methods as they optimize different losses.

(a) MNIST negative log-
likelihoods

(b) 2-d synthetic dataset samples from MADE MLE baselines with n mixture of
logistics and an AR-CSM model trained by CSM.

Figure 3: Negative log-likelihoods on MNIST and samples on a 2-d synthetic dataset.

CSM vs. DSM Denoising score matching (DSM) [28] is perhaps the most scalable score matching
alternative available, and has been applied to high dimensional score matching problems [24]. How-
ever, DSM estimates the score of the data distribution after it has been convolved with Gaussian noise
with variance σ2I. In Figure 2, we use various noise levels σ for DSM, and compare the performance
of CSM with that of DSM. We observe that although DSM shows reasonable performance when σ is
sufﬁciently large, the training can fail to converge for small σ. In other words, for DSM, there exists
a tradeoff between optimization performance and the bias introduced due to noise perturbation for
the data. CSM on the other hand does not suffer from this problem, and converges faster than DSM.

Likelihood comparison To better compare density estimation performance of DSM, SSM and
CSM, we train a MADE [4] model with tractable likelihoods on MNIST, a more challenging data
distribution, using the three variants of score matching objectives. We report the negative log-
likelihoods in Figure 3a. The loss curves in Figure 3a align well with our previous discussion. For
DSM, a smaller σ introduces less bias, but also makes training slower to converge. For SSM, training
convergence can be handicapped by the large variance due to random projections. In contrast, CSM
can converge quickly without these difﬁculties. This clearly demonstrates the efﬁcacy of CSM over
the other score matching methods for density estimation.

5.2 Learning 2-d synthetic data distributions with AR-CSM

In this section, we focus on a 2-d synthetic data distribution (see Figure 3b). We compare the
sample quality of an autoregressive model trained by maximum likelihood estimation (MLE) and an
AR-CSM model trained by CSM. We use a MADE model with n mixture of logistic components for
the MLE baseline experiments. We also use a MADE model as the autoregressive architecture for the
AR-CSM model. To show the effectiveness of our approach, we use strictly fewer parameters for the
AR-CSM model than the baseline MLE model. Even with fewer parameters, the AR-CSM model
trained with CSM is still able to generate better samples than the MLE baseline (see Figure 3b).

5.3 Learning high dimensional distributions over images with AR-CSM

In this section, we show that our method is also capable of modeling natural images. We focus on
three image datasets, namely MNIST, CIFAR-10, and CelebA.

Setup We select two existing autoregressive models — MADE [4] and PixelCNN++ [21], as the
autoregressive architectures for AR-CSM. For all the experiments, we use a shallow fully connected
network to transform the context vectors to the conditional scores for AR-CSM. Additional details
can be found in Appendix C.

6

MADE MLE

MADE CSM

PixelCNN++ MLE

PixelCNN++ CSM

Figure 4: Samples from MADE and PixelCNN++ using MLE and CSM.

Results We compare the samples from AR-CSM with the ones from MADE and PixelCNN++ with
similar autoregressive architectures but trained via maximum likelihood estimation. Our AR-CSM
models have comparable number of parameters as the maximum-likelihood counterparts. We observe
that the MADE model trained by CSM is able to generate sharper and higher quality samples than
its maximum-likelihood counterpart using Gaussian densities (see Figure 4). For PixelCNN++, we
observe more digit-like samples on MNIST, and less shifted colors on CIFAR-10 and CelebA than
its maximum-likelihood counterpart using mixtures of logistics (see Figure 4). We provide more
samples in Appendix C.

5.4

Image denoising with AR-CSM

S&P Noise

Denoised

Gaussian Noise

Denoised

Figure 5: Salt and pepper denoising on CIFAR-10. Autoregressive single-step denoising on MNIST.

Besides image generation, AR-CSM can also be used for image denoising. In Figure 5, we apply 10%
"Salt and Pepper" noise to the images in CIFAR-10 test set and apply Langevin dynamics sampling
to restore the images. We also show that AR-CSM can be used for single-step denoising [22, 28] and
report the denoising results for MNIST, with noise level σ = 0.6 in the rescaled space in Figure 5.
These results qualitatively demonstrate the effectiveness of AR-CSM for image denoising, showing
that our models are sufﬁciently expressive to capture complex distributions and solve difﬁcult tasks.

5.5 Out-of-distribution detection with AR-CSM

Model

PixelCNN++ GLOW EBM AR-CSM(Ours)

We show that the AR-CSM model can also
be used for out-of-distribution (OOD) detec-
tion. In this task, the generative model is re-
quired to produce a statistic (e.g., likelihood,
energy) such that the outputs of in-distribution
examples can be distinguished from those of
the out-of-distribution examples. We ﬁnd that
hθ(x) (cid:44) (cid:80)D
d=1 sθ,d(x) is an effective statistic
for OOD. In Tab. 1, we compare the Area Under
the Receiver-Operating Curve (AUROC) scores obtained by AR-CSM using hθ(x) with the ones
obtained by PixelCNN++ [21], Glow [10] and EBM [3] using relative log likelihoods. We use SVHN,

Table 1: AUROC scores for models trained on
CIFAR-10.

SVHN
Const Uniform
Uniform
Average

0.63
0.30
1.0
0.64

0.32
0.0
1.0
0.44

0.68
0.57
0.95
0.73

0.24
0.0
1.0
0.41

7

MNIST (AIS)

CelebA (FID)

Latent Dim

8

ELBO
Stein
Spectral
SSM
SSM-AR
CSM (Ours)

96.74
96.90
96.85
95.61
95.85
95.02

16

91.82
88.86
88.76
88.44
88.98
88.42

32

66.31
108.84
121.51
62.50
66.88
62.20

Table 2: VAE results on MNIST and
CelebA.

Figure 6: CSM VAE MNIST and CelebA samples.

constant uniform and uniform as OOD distributions following [3]. We observe that our method can
perform comparably or better than existing generative models.

6 VAE training with implicit encoders and CSM

In this section, we show that CSM can also be used to improve variational inference with implicit
distributions [8]. Given a latent variable model pθ(x, z), where x is the observed variable and z ∈ RD
is the latent variable, a Variational Auto-Encoder (VAE) [11] contains an encoder qφ(z|x) and a
decoder pθ(x|z) that are jointly trained by maximizing the evidence lower bound (ELBO)

Epdata(x)[Eqφ(z|x) log pθ(x|z)p(z) − Eqφ(z|x) log qφ(z|x)],

(6)

Typically, qφ(z|x) is chosen to be a simple explicit distribution such that the entropy term in Equa-
tion (6), H(qφ(·|x)) (cid:44) −Eqφ(z|x)[log qφ(z|x)], is tractable. To increase model ﬂexibility, we can
parameterize the encoder using implicit distributions—distributions that can be sampled tractably but
do not have tractable densities (e.g., the generator of a GAN [5]). The challenge is that evaluating
H(qφ(·|x)) and its gradient ∇φH(qφ(·|x))) becomes intractable.

Suppose zd ∼ qφ(zd|z<d, x) can be reparameterized as gφ,d((cid:15)≤d, x), where gφ,d is a deterministic
mapping and (cid:15) is a D dimensional random variable. We can write the gradient of the entropy with
respect to φ as

∇φH(qφ(·|x))) = −

D
(cid:88)

d=1

Ep((cid:15)<d)Ep((cid:15)d)

(cid:20) ∂
∂zd

(cid:21)
,
log qφ(zd|z<d, x)|zd=gφ,d((cid:15)≤d,x)∇φgφ,d((cid:15)≤d, x)

where ∇φgφ,d((cid:15)≤d, x) is usually easy to compute and ∂
∂zd
by score estimation using CSM. We provide more details in Appendix D.

log qφ(zd|z<d, x) can be approximated

Setup We train VAEs using the proposed method on two image datasets – MNIST and CelebA. We
follow the setup in [25] (see Appendix D.4) and compare our method with ELBO, and three other
methods, namely SSM [25], Stein [26], and Spectral [23], that can be used to train implicit encoders
[25]. Since SSM can also be used to train an AR-CSM model, we denote the AR-CSM model
trained with SSM as SSM-AR. Following the settings in [25], we report the likelihoods estimated by
AIS [16] for MNIST, and FID scores [7] for CelebA. We use the same decoder for all the methods,
and encoders sharing similar architectures with slight yet necessary modiﬁcations. We provide more
details in Appendix D.

Results We provide negative log-likelihoods (estimated by AIS) on MNIST and the FID scores on
CelebA in Tab. 2. We observe that CSM is able to marginally outperform other methods in terms of
the metrics we considered. We provide VAE samples for our method in Figure 6. Samples for the
other methods can be found in Appendix E.

7 Related work

Likelihood-based deep generative models (e.g., ﬂow models, autoregressive models) have been
widely used for modeling high dimensional data distributions. Although such models have achieved

8

promising results, they tend to have extra constraints which could limit the model performance.
For instance, ﬂow [2, 10] and autoregressive [27, 17] models require normalized densities, while
variational auto-encoders (VAE) [11] need to use surrogate losses.

Unnormalized statistical models allow one to use more ﬂexible networks, but require new training
strategies. Several approaches have been proposed to train unnormalized statistical models, all with
certain types of limitations. Ref. [3] proposes to use Langevin dynamics together with a sample replay
buffer to train an energy based model, which requires more iterations over a deep neural network for
sampling during training. Ref. [31] proposes a variational framework to train energy-based models
by minimizing general f -divergences, which also requires expensive Langevin dynamics to obtain
samples during training. Ref. [15] approximates the unnormalized density using importance sampling,
which introduces bias during optimization and requires extra computation during training. There are
other approaches that focus on modeling the log-likelihood gradients (scores) of the distributions. For
instance, score matching (SM) [9] trains an unnormalized model by minimizing Fisher divergence,
which introduces a new term that is expensive to compute for high dimensional data. Denoising
score matching [28] is a variant of score matching that is fast to train. However, the performance
of denoising score matching can be very sensitive to the perturbed noise distribution and heuristics
have to be used to select the noise level in practice. Sliced score matching [25] approximates SM
by projecting the scores onto random vectors. Although it can be used to train high dimensional
data much more efﬁciently than SM, it provides a trade-off between computational complexity and
variance introduced while approximating the SM objective. By contrast, CSM is a deterministic
objective function that is efﬁcient and stable to optimize.

8 Conclusion

We propose a divergence between distributions, named Composite Score Matching (CSM), which
depends only on the derivatives of univariate log-conditionals (scores) of the model. Based on
CSM divergence, we introduce a family of models dubbed AR-CSM, which allows us to expand the
capacity of existing autoregressive likelihood-based models by removing the normalizing constraints
of conditional distributions. Our experimental results demonstrate good performance on density
estimation, data generation, image denoising, anomaly detection and training VAEs with implicit
encoders. Despite the empirical success of AR-CSM, sampling from the model is relatively slow
since each variable has to be sampled sequentially according to some order. It would be interesting to
investigate methods that accelerate the sampling procedure in AR-CSMs, or consider more efﬁcient
variable orders that could be learned from data.

Broader Impact

The main contribution of this paper is theoretical—a new divergence between distributions and a
related class of generative models. We do not expect any direct impact on society. The models we
trained using our approach and used in the experiments have been learned using classic dataset and
have capabilities substantially similar to existing models (GANs, autoregressive models, ﬂow models):
generating images, anomaly detection, denoising. As with other technologies, these capabilities can
have both positive and negative impact, depending on their use. For example, anomaly detection can
be used to increase safety, but also possibly for surveillance. Similarly, generating images can be
used to enable new art but also in malicious ways.

Acknowledgments and Disclosure of Funding

This research was supported by TRI, Amazon AWS, NSF (#1651565, #1522054, #1733686), ONR
(N00014-19-1-2145), AFOSR (FA9550-19-1-0024), and FLI.

References

[1] A. P. Dawid and M. Musio. Theory and applications of proper scoring rules. Metron, 72(2):169–

183, 2014.

9

[2] L. Dinh, J. Sohl-Dickstein, and S. Bengio. Density estimation using real nvp. arXiv preprint

arXiv:1605.08803, 2016.

[3] Y. Du and I. Mordatch. Implicit generation and generalization in energy-based models. arXiv

preprint arXiv:1903.08689, 2019.

[4] M. Germain, K. Gregor, I. Murray, and H. Larochelle. Made: Masked autoencoder for distribu-
tion estimation. In International Conference on Machine Learning, pages 881–889, 2015.
[5] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and
Y. Bengio. Generative adversarial nets. In Advances in neural information processing systems,
pages 2672–2680, 2014.

[6] U. Grenander and M. I. Miller. Representations of knowledge in complex systems. Journal of

the Royal Statistical Society: Series B (Methodological), 56(4):549–581, 1994.

[7] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by a two
time-scale update rule converge to a local nash equilibrium. In Advances in neural information
processing systems, pages 6626–6637, 2017.

[8] F. Huszár. Variational inference using implicit distributions. arXiv preprint arXiv:1702.08235,

2017.

[9] A. Hyvärinen. Estimation of non-normalized statistical models by score matching. Journal of

Machine Learning Research, 6(Apr):695–709, 2005.

[10] D. P. Kingma and P. Dhariwal. Glow: Generative ﬂow with invertible 1x1 convolutions. In

Advances in Neural Information Processing Systems, pages 10215–10224, 2018.

[11] D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114,

2013.

[12] A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009.
[13] Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face attributes in the wild. In Proceedings

of the IEEE international conference on computer vision, pages 3730–3738, 2015.

[14] J. Martens, I. Sutskever, and K. Swersky. Estimating the hessian by back-propagating curvature.

arXiv preprint arXiv:1206.6464, 2012.

[15] C. Nash and C. Durkan. Autoregressive energy machines. arXiv preprint arXiv:1904.05626,

2019.

[16] R. M. Neal. Annealed importance sampling. Statistics and computing, 11(2):125–139, 2001.
[17] A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner,
A. Senior, and K. Kavukcuoglu. Wavenet: A generative model for raw audio. arXiv preprint
arXiv:1609.03499, 2016.

[18] A. v. d. Oord, N. Kalchbrenner, and K. Kavukcuoglu. Pixel recurrent neural networks. arXiv

preprint arXiv:1601.06759, 2016.

[19] G. Parisi. Correlation functions and computer simulations. Nuclear Physics B, 180(3):378–384,

1981.

[20] G. O. Roberts, R. L. Tweedie, et al. Exponential convergence of langevin distributions and their

discrete approximations. Bernoulli, 2(4):341–363, 1996.

[21] T. Salimans, A. Karpathy, X. Chen, and D. P. Kingma. Pixelcnn++: Improving the pixelcnn with
discretized logistic mixture likelihood and other modiﬁcations. arXiv preprint arXiv:1701.05517,
2017.

[22] S. Saremi, A. Mehrjou, B. Schölkopf, and A. Hyvärinen. Deep energy estimator networks.

arXiv preprint arXiv:1805.08306, 2018.

[23] J. Shi, S. Sun, and J. Zhu. A spectral approach to gradient estimation for implicit distributions.

arXiv preprint arXiv:1806.02925, 2018.

[24] Y. Song and S. Ermon. Generative modeling by estimating gradients of the data distribution. In

Advances in Neural Information Processing Systems, pages 11895–11907, 2019.

[25] Y. Song, S. Garg, J. Shi, and S. Ermon. Sliced score matching: A scalable approach to density
and score estimation. In Proceedings of the Thirty-Fifth Conference on Uncertainty in Artiﬁcial
Intelligence, UAI 2019, Tel Aviv, Israel, July 22-25, 2019, page 204, 2019.

10

[26] C. M. Stein. Estimation of the mean of a multivariate normal distribution. The annals of

Statistics, pages 1135–1151, 1981.

[27] A. Van den Oord, N. Kalchbrenner, L. Espeholt, O. Vinyals, A. Graves, et al. Conditional image
generation with pixelcnn decoders. In Advances in neural information processing systems,
pages 4790–4798, 2016.

[28] P. Vincent. A connection between score matching and denoising autoencoders. Neural compu-

tation, 23(7):1661–1674, 2011.

[29] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, D. H. Choi,
R. Powell, T. Ewalds, P. Georgiev, et al. Grandmaster level in starcraft ii using multi-agent
reinforcement learning. Nature, 575(7782):350–354, 2019.

[30] M. Welling and Y. W. Teh. Bayesian learning via stochastic gradient langevin dynamics.
In Proceedings of the 28th international conference on machine learning (ICML-11), pages
681–688, 2011.

[31] L. Yu, Y. Song, J. Song, and S. Ermon. Training deep energy-based models with f-divergence

minimization. arXiv preprint arXiv:2003.03463, 2020.

11

A Proofs

A.1 Regularity conditions

The following regularity conditions are needed for identiﬁability and integration by parts.

We assume that for every x<d and for any θ

1.

∂
∂xd

log p(xd|x<d) and ∂
∂xd

log qθ(xd|x<d) are continuously differentiable over R.

(cid:16) ∂ log p(xd|x<d)
∂xd

2. Exd∼p(xd|x<d)[
3. lim|xd|→∞ p(xd|x<d) ∂ log qθ(xd|x<d)

(cid:17)2

∂xd

= 0.

] and Exd∼p(xd|x<d)[

(cid:16) ∂ log qθ(xd|x<d)
∂xd

(cid:17)2

] are ﬁnite.

A.2 Proof of Theorem 1 (See page 3)

Theorem 1 (CSM Divergence). LCSM (qθ, p) vanishes if and only if qθ(x) = p(x) a.e.

Proof. It is known that the Fisher divergence

L(qθ; p) =

1
2

(cid:20)
(cid:107)∇x log p(x) − ∇x log qθ(x)(cid:107)2
2

(cid:21)

Ep

(7)

is a strictly proper scoring rule, and L(qθ; p) ≥ 0 and vanishes if and only if qθ = p almost
everywhere [9].

Recall

LCSM (qθ; p) =

1
2

D
(cid:88)

d=1

Ep(x<d)Ep(xd|x<d)

(cid:20)
(sd(x) − sθ,d(x))2

(cid:21)
.

which we can rewrite as

LCSM (qθ; p) =

D
(cid:88)

d=1

Ep(x<d)

(cid:20)
L (qθ(xd | x<d); p(xd | x<d))

(cid:21)
.

(8)

(9)

When qθ = p almost everywhere, we have

qθ(x≤d) = qθ(x<d)qθ(xd | x<d) = p(x≤d) = p(x<d)p(xd | x<d) = qθ(x<d)p(xd | x<d) a.e.
(10)

Let A (cid:44) {x<d | qθ(x<d) > 0}. We ﬁrst observe that when x<d ∈ A, Eq. equation 10 implies that
qθ(xd | x<d) = p(xd | x<d) a.e, and subsequently, (sd(x) − sθ,d(x))2 = 0 a.e. Therefore

LCSM (qθ; p) =

=

1
2

1
2

D
(cid:88)

d=1

D
(cid:88)

d=1

= 0

Ep(x<d)Ep(xd|x<d)

(cid:20)
(sd(x) − sθ,d(x))2

(cid:21)

Ep(x<d)

(cid:20)
I[x<d ∈ A]Ep(xd|x<d)

(cid:20)

(sd(x) − sθ,d(x))2

(cid:21)(cid:21)

Now assume LCSM (qθ; p) = 0. Because L ≥ 0, Ep(x<d)
which means every term in the sum must be zero

(cid:20)
L (qθ(xd | x<d); p(xd | x<d))

(cid:21)

≥ 0

Ep(x<d)

(cid:20)
L (qθ(xd | x<d); p(xd | x<d))

(cid:21)

= 0 ∀d

and L (qθ(xd | x<d); p(xd | x<d)) = 0 p(x<d)-almost everywhere. Let’s show that qθ(x≤d) =
p(x≤d) almost everywhere using induction. When d = 1, L (qθ(x1); p(x1)) = 0 al-
most everywhere implies qθ(x1) = p(x1) almost everywhere. Assume the hypothesis

12

holds when d = k, that is qθ(x≤k) = p(x≤k)) almost everywhere. Using the fact that
L (qθ(xk+1 | x<k+1); p(xk+1 | x<k+1)) = 0 p(x<k+1)-almost everywhere (i.e., p(x≤k)-almost
everywhere), we have

qθ(x≤k+1) = qθ(xk+1 | x<k+1)qθ(x≤k) = p(xk+1 | x<k+1)p(x≤k) = p(x≤k+1) a.e.
Thus, the hypothesis holds when d = k + 1. By induction hypothesis, we have qθ(x≤d) =
p(x≤d) a.e. for any d. In particular,

qθ(x) = qθ(x≤D) = p(x≤D) = p(x) a.e.

A.3 Proof of Theorem 2 (See page 3)

Theorem 2 (Formal Statement). LCSM (θ; p) = JCSM (θ; p) + C where C is a constant does not
depend on θ.

Proof. Recall

LCSM (qθ; p) =

1
2

D
(cid:88)

d=1

Ep(x<d)Ep(xd|x<d)

(cid:20)
(sd(x) − sθ,d(x))2

(cid:21)
.

which we can rewrite as

LCSM (qθ; p) =

D
(cid:88)

Ep(x<d)

(cid:20)
L (qθ(xd | x<d); p(xd | x<d))

(cid:21)
.

(11)

(12)

d=1
where L(qθ; p) is the Fisher divergence.

Under the assumptions, we can use Theorem 1 from [9] which shows that L(qθ; p) = J(θ; p) + C,
where C is a constant independent of θ and J(qθ; p) is deﬁned as below:
(cid:20) 1
2

(cid:107)∇x log qθ(x)(cid:107)2

J(qθ; p) = Ep

x log qθ(x))

2 + tr(∇2

(cid:21)
,

Substituting into equation 12 we get

LCSM (qθ; p) =

D
(cid:88)

Ep(x<d)

(cid:21)
(cid:20)
J (qθ(xd | x<d); p(xd | x<d)) + C(x<d)

d=1
(cid:20)
Ep(xd|x<d)

Ep(x<d)

(cid:20) 1
2

D
(cid:88)

d=1

sθ,d(x)2 +

∂
∂xd

(cid:21)
sθ,d(x)

(cid:21)
+ C(x<d)

D
(cid:88)

d=1

Ep(x<d)

(cid:20)
Ep(xd|x<d)

(cid:20) 1
2

sθ,d(x)2 +

∂
∂xd

(cid:21)(cid:21)

sθ,d(x)

+ C

(15)

=

=

(13)

(14)

A.4 Proof of Theorem 3

Proof. Let Q be the set of joint distributions that satisﬁes the condition in Theorem 3. Let f be
deﬁned as:

f : AR-CSM → Q

f : ˆs(x) = (ˆs1(x1), ..., ˆsD(x<D, xD)) (cid:55)→ q(x) :=

D
(cid:89)

d=1

eEd(x<d,xd)
Zd(x<d)

Surjectivity
Given q ∈ Q, from the chain rule, we have

q(x) =

D
(cid:89)

d=1

q(xd|x<d).

13

(16)

By assumption, we have ∂
log q(xd|x<d)
∂xd
for each d. We can check that Ed(x<d, xd) = log q(xd|x<d) + C, where C is a constant. We also
have Zd(x<d) = eC (cid:82) q(xd|x<d)dxd = eC exists. Thus, s(x) ∈ AR-CSM. On the other hand, we
have

log q(xd|x<d) exists for all d. Deﬁne ˆsd(x<d, xd) = ∂
∂xd

f (s(x)) =

=

=

D
(cid:89)

d=1

D
(cid:89)

d=1

D
(cid:89)

eEd(x<d,xd)
Zd(x<d)

eC log q(xd|x<d)
eC

log q(xd|x<d)

d=1
= q(x)

(17)

(18)

(19)

(20)

Thus s(x) ∈ AR-CSM is a pre-image of q(x) and f is surjective.

Injectivity
Given q ∈ Q, assume there exist ˆs1(x), ˆs2(x) ∈ AR-CSM such that f (ˆs1(x)) = f (ˆs2(x)) = q(x),
we have

q(x) = f (ˆsi(x))
D
(cid:89)

eEi,d(x<d,xd)
Zi,d(x<d)

,

d=1

q(xd|x<d) =

D
(cid:89)

d=1

where i = 1, 2.

Lemma 1. For any d = 1, ..., D, we have q(xd|x<d) = eEi,d (x<d,xd )
Zi,d(x<d)

.

Proof. Let’s prove this argument using induction on d.
i) When d = 1, integrate equation 20 w.r.t. xD, ..., x2 sequentially, we have

(cid:90)

...

(cid:90) D
(cid:89)

d=1

q(xd|x<d)dxD...dx2 =

(cid:90)

...

(cid:90) D
(cid:89)

d=1

eEi,d(x<d,xd)
Zi,d(x<d)

dxD...dx2

q(x1) =

eEi,d(x1)
Zi,d(x1)

Thus, the condition holds when d = 1.
ii) Assume the condition holds for any d ≤ k, that is q(xd|x<d) = eEi,d (x<d,xd )
Zi,d(x<d)
implies

for any d ≤ k. This

q(x1, ..., xk) =

k
(cid:89)

d=1

q(xd|x<d) =

k
(cid:89)

d=1

eEi,d(x<d,xd)
Zi,d(x<d)

Similarly, integrating equation 20 w.r.t. xD, ..., xk+2 sequentially will give us

q(x1, ..., xk+1) =

k+1
(cid:89)

d=1

eEi,d(x<d,xd)
Zi,d(x<d)

.

Plugging in q(x1, ..., xk) = (cid:81)k
and use the fact that q(x1, ..., xk) (cid:54)= 0 (since q has
support equals to the entire space by assumption), we obtain q(xk+1|x<k+1) = eEi,k+1(x<k+1,xk+1)
Thus, the hypothesis holds when d = k + 1.
iii) By induction hypothesis, the condition holds for all d.

eEi,d (x<d ,xd )
Zi,d(x<d)

Zi,k+1(x<k+1)

d=1

.

14

From Lemma 1, we have

q(xd|x<d) =

eEi,d(x<d)
Zi,d(x<d)

log q(xd|x<d) = Ei,d(x<d, xd) − log Zi,d(x<d).

Taking the derivative w.r.t xd on both sides, since log Zi,d(x<d, xd) does not depend on xd, we
conclude that

∂
∂xd

log q(xd|x<d) =

∂
∂xd

Ei,d(x<d, xd) = ˆsi,d(x<d, xd),

where i = 1, 2. This implies ˆs1(x) = ˆs2(x), and f is injective.

B Data generation on 2D toy datasets

We perform density estimation on a couple two-dimensional synthetic distributions with various
shapes and number of modes using our method. In Figure 7, we visualize the samples drawn from
our model. We notice that the trained AR-CSM model can ﬁt multi-modal distributions well.

Figure 7: Samples from 2D synthetic datasets. The ﬁrst row: data distribution. The second row:
samples from AR-CSM.

C Additional details of AR-CSM experiments

C.1 More Samples

MADE MLE

15

(a) MNIST samples.

(b) CIFAR-10 samples.

Figure 8: MADE MLE samples.

MADE CSM

(a) MNIST samples.

(b) CIFAR-10 samples.

Figure 9: MADE CSM samples.

PixelCNN++ MLE

16

(a) MNIST samples.

(b) CIFAR-10 samples.

(c) CelebA samples.

Figure 10: PixelCNN++ MLE samples.

PixelCNN++ CSM

(a) MNIST samples.

(b) CIFAR-10 samples.

(c) CelebA samples.

Figure 11: PixelCNN++ CSM samples.

C.2 Noise annealing

Figure 12: Conditional noise annealing at dimension d. The context ˆcd only depends on the pixels in
front of ˆxd in ˆx (i.e. the green ones in ˆx).

Training score-based generative modeling has been a challenging problem due to the manifold
hypothesis and the existence of low data density regions in the data distribution. [24] shows the
efﬁcacy of noise annealing while addressing the above challenges. Based on their arguments, we
adopt a noise annealing scheme for training one dimensional score matching. More speciﬁcally,
we choose a positive geometric sequence {σi}L
> 1 to be our
noise levels. Since at each dimension, we perform score matching w.r.t. xd given x<d, the previous

i=1 that satisﬁes σ1
σ2

= ... = σL−1
σL

17

challenges apply to the one dimensional distribution p(xd|x<d). We thus propose to perform noise
annealing only on the scalar xd. This process requires us to deal with xd and x<d separately. For
convenience, let us denote the input for the autoregressive model (context network) as ˆx, and the
scalar pixel that will be concatenated with the context vector as ˜xd. We decompose the training
process into L stages. At stage i, we choose σi to be the noise level for ˜xd and use the perturbed
noise distribution pσi(˜xd|xd) = N (˜xd|xd, σ2
i ) to obtain ˜xd. We use a shared noise level ˆσ among
all stages for ˆx and use a perturbed noise distribution pˆσ(ˆx|x) = N (ˆx, |x, ˆσ2ID). We feed ˆx to the
context network to obtain the context vector ˆcd and concatenate it with ˜xd to obtain hd = [ˆcd, ˜xd],
which is then fed into the score network to obtain conditional scores sθ,d(x) (see Figure 12). At each
stage, we train the network until convergence before moving on to the next stage. We denote the
learned data distribution and the model distribution at stage i for the d-th dimension as pσi(˜xd|ˆx<d)
and qθ,σi(˜xd|ˆx<d) respectively. As the perturbed noise for ˜xd gradually decreases w.r.t. the stages,
we call this process conditional noise annealing. For consistency, we want the distribution of ˆx
to match the ﬁnal state distribution of ˜x = (˜x1, ..., ˜xD), we thus choose ˆσ = σD as the perturbed
distribution for ˆx among all the stages.

C.3 Inference with annealed autoregressive Langevin dynamics

To sample from the model, we can sample each dimension sequentially using one dimensional
Langevin dynamics. For the d-th dimension, given a ﬁxed step size (cid:15) > 0 and an initial value
˜x[0]
d drawn from a prior distribution π(x) (e.g., a standard normal distribution N (0, 1)), the one
dimensional Langevin method recursively computes the following based on the already sampled
previous pixels x[T ]
<d

d = x[t−1]
x[t]

d

+

(cid:15)
2

∂
∂x[t−1]
d

log p(x[t−1]

d

|x[T ]

<d) +

√

(cid:15)zt, t = 1, ..., T,

(21)

where zt ∼ N (0, 1). When (cid:15) → 0 and T → ∞, the distribution of x[T ]
d matches p(xd|x<d), in which
case x[T ]
is an exact sample from p(xd|x<d) under some regularity conditions [30]. Similar as [24],
d
we can use annealed Langevin dynamics to speed up the mixing speed of one dimensional Langevin
dynamics. Let (cid:15)0 > 0 be a prespeciﬁed constant scalar, we decompose the sampling process into L
stages for each d. At stage i, we run autoregressive Langavin dynamics to sample from pσi(˜xd|ˆx<d)
using the model qθ,σi(˜xd|ˆx<d) learned at the i-th stage of the training process. We deﬁne the anneal
Langevin dynamics update rule as

d = ˜x[t−1]
˜x[t]

d

+

(cid:15)
2

∂
∂ ˜x[t−1]
d

log qθ,σi(˜x[t−1]

d

|ˆx[T ]

<d) +

√

(cid:15)zt, t = 1, ..., T,

(22)

for the same reasoning as discussed in

where zt ∼ N (0, 1). We choose the step size (cid:15) = (cid:15)0 · σ2
i
σ2
L
[24]. At stage i > 1, we set the initial state ˜x[0]
d to be the ﬁnal samples of the previous simulation
at stage i − 1; and at stage one, we set the initial value ˜x[0]
to be random samples drawn from
1
the prior distribution N (0, 1). For each dimension d, we start from stage one, repeat the anneal
Langevin sampling process for ˜xd until we reach stage L, in which case we have sampled the d-th
component from our model. Compared to Langevin dynamics performed on a high dimensional
space, one dimensional Langevin dynamics is shown to be able to converge faster under certain
regularity conditions [20].

C.4 Setup

For CelebA, we follow a similar setup as [24]: we ﬁrst center-crop the images to 140 × 140 and then
resize them to 32 × 32. All images are rescaled so that pixel values are located between −1 and 1.
We choose L = 10 different noise levels for {σi}L
i=1. For MNIST, we use σ1 = 1.0 and σL = 0.04,
and σ1 = 0.2 and σL = 0.04 are used for CIFAR-10 and CelebA. We notice that for the used image
data, due to the rescaling, a Gaussian noise with σ = 0.04 is almost indistinguishable to human
eyes. During sampling, we ﬁnd T = 20 for MNIST and T = 10 for CIFAR-10 and CelebA work
reasonably well for anneal autoregressive Langevin dynamics in practice. We select two existing
autoregressive models, MADE [4] and PixelCNN++ [21], as the architectures for our autoregressive
context network (AR-CN). For all the experiments, we use a shallow fully connected network as the

18

architecture for the conditional score network (CSN). The amount of parameters for this shallow fully
connected network is almost negligible compared to the autoregressive context network. We train the
models for 200 epochs in total, using Adam optimizer with learning rate 0.0002.

D Additional details of VAE experiments

D.1 Background

Given a latent variable model p(x, z) where x is the observed variable and z is the latent variable, a
VAE contains the following two parts: i) an encoder qφ(z|x) that models the conditional distribution
of the latent variable given the observed data; and ii) a decoder pθ(x|z) that models the posterior
distribution of the latent variable. In general, a VAE is trained by maximizing the evidence lower
bound (ELBO):

Epdata(x)[Eqφ(z|x) log pθ(x|z)p(z) − Eqφ(z|x) log qφ(z|x)].

(23)

We refer to this traditional training method as "ELBO" throughout the discussion. In ELBO, qφ(z|x)
is often chosen to be a simple distribution such that H(qφ(·|x)) (cid:44) −Eqφ(z|x)[log qφ(z|x)] is tractable,
which constraints the ﬂexibility of an encoder.

D.2 Training VAEs with implicit encoders

Instead of parameterizing qφ(z|x) directly as a normalized density function, we can param-
eterize the encoder using an implicit distribution, which removes the above constraints im-
posed on ELBO. We call such encoder an implicit encoder. Denote Hd(qφ(·|z<d, x)) (cid:44)
−Eqφ(zd|z<d,x)[log qφ(zd|z<d, x)], using the chain rule of entropy, we have

H(qφ(·|x))) = −Eqφ(z|x)[log qφ(z|x)] =

D
(cid:88)

d=1

Hd(qφ(·|z<d, x)).

(24)

Suppose zd ∼ qφ(zd|z<d, x) can be parameterized as zd = hφ,d((cid:15)d, z<d, x), where (cid:15)d is a sim-
ple one dimensional random variable independent of φ (i.e. a standard normal) and hφ,d is a
deterministic mapping depending on φ at dimension d. By plugging in z<d into hφ,d and using
zd = hφ,d((cid:15)d, z<d, x) recursively, we can show that zd can be reparametrized as gφ,d((cid:15)≤d, x), which
is a deterministic mapping depending on φ. This provides the following equality for the gradient of
Hd(qφ(·|x)) w.r.t. φ

∇φHd(qφ(·|z<d, x)) (cid:44) −∇φEqφ(zd|z<d,x)[log qφ(zd|z<d, x)]

= −Ep((cid:15)d)[

∂
∂zd

log qφ(zd|z<d, x)|zd=gφ,d((cid:15)≤d,x)∇φgφ,d((cid:15)≤d, x)]

(25)

(26)

(See Appendix D.3). This implies

∇φH(qφ(·|x))) = −

D
(cid:88)

d=1

Ep((cid:15)<d)Ep((cid:15)d)

(cid:20) ∂
∂zd

(cid:21)
,
log qφ(zd|z<d, x)|zd=gφ,d((cid:15)≤d,x)∇φgφ,d((cid:15)≤d, x)

Besides the aforementioned encoder and decoder, to train a VAE model using an implicit encoder,
we introduce a third model: an AR-CSM model with parameter ˜φ denoted as s ˜φ(z|x) that is used
to approximate the conditional score sφ,d(z|x) (cid:44) ∂
log qφ(zd|z<d, x) of the implicit encoder.
∂zd
During training, we draw i.i.d. samples {z(1), ..., z(N )} from the implicit encoder qφ(z|x) and
use these samples to train s ˜φ(z|x) to approximate the conditional scores of the encoder using
CSM. After s ˜φ(z|x) is updated, we use the d-th component of s ˜φ(z|x), the approximation of
∂
∂
log qφ(zd|z<d, x) in equation 26. To compute
∂zd
∂zd
∇φH(qφ(·|x)), we can detach the approximated conditional score s ˜φ(z|x) so that the gradient of
H(qφ(·|x)) could be approximated properly using PyTorch backpropagation. This provides us with
a way to evaluate the gradient of Eq. equation 6 w.r.t. φ, which can be used to update the implicit
encoder qφ(z|x).

log qφ(zd|z<d, x), as the substitution for

19

D.3 Reparameterization

∇φHd(qφ(·|z<d, x)) (cid:44) −∇φEqφ(zd|z<d,x)[log qφ(zd|z<d, x)]

= −∇φEp((cid:15)d)[log qφ(gφ,d((cid:15)≤d, x))]
= −Ep((cid:15)d)[∇φ log qφ(gφ,d((cid:15)≤d, x))]

= −Ep((cid:15)d)[

∂
∂zd

log qφ(zd|z<d, x)|zd=gφ,d((cid:15)≤d,x)∇φgφ,d((cid:15)≤d, x)].

(27)

(28)

(29)

(30)

D.4 Setup

For CelebA, we follow the setup in [25]. We ﬁrst center-crop all images to a patch of 140 × 140, and
then resize the image size to 64 × 64. For MNIST experiments, we use RMSProp optimizer with a
learning rate of 0.001 for all methods except for the CSM experiments where we use learning rate of
0.0002 for the score estimator. On CelebA, we use RMSProp optimizer with a learning rate of 0.0001
for all methods except for the CSM experiments where we use a learning rate of 0.0002 for the score
estimator.

20

E VAE with implicit encoders

CSM

Figure 13: From left to right: VAE CSM MNIST samples with latent dimension 8, VAE CSM MNIST
samples with latent dimension 16, VAE CSM CelebA samples with latent dimension 32.

ELBO

Figure 14: From left to right: VAE ELBO MNIST samples with latent dimension 8, VAE ELBO
MNIST samples with latent dimension 16, VAE ELBO CelebA samples with latent dimension 32.

Stein

Figure 15: From left to right: VAE Stein MNIST samples with latent dimension 8, VAE Stein MNIST
samples with latent dimension 16, VAE Stein CelebA samples with latent dimension 32.

21

Spectral

Figure 16: From left to right: VAE Spectral MNIST samples with latent dimension 8, VAE Spectral
MNIST samples with latent dimension 16, VAE Spectral CelebA samples with latent dimension 32.

SSM-AR

Figure 17: From left to right: VAE SSM-AR MNIST samples with latent dimension 8, VAE SSM-AR
MNIST samples with latent dimension 16, VAE SSM-AR CelebA samples with latent dimension 32.

SSM

Figure 18: From left to right: VAE SSM MNIST samples with latent dimension 8, VAE SSM MNIST
samples with latent dimension 16, VAE SSM CelebA samples with latent dimension 32.

22

