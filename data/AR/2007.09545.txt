0
2
0
2

l
u
J

9
1

]

V
C
.
s
c
[

1
v
5
4
5
9
0
.
7
0
0
2
:
v
i
X
r
a

ContactPose: A Dataset of Grasps with Object
Contact and Hand Pose

Samarth Brahmbhatt1[0000−0002−3732−8865], Chengcheng Tang3, Christopher D.
Twigg3, Charles C. Kemp1, and James Hays1,2

1 Georgia Tech, Atlanta GA, USA {samarth.robo,hays}@gatech.edu,
charlie.kemp@bme.gatech.edu
2 Argo AI
3 Facebook Reality Labs {chengcheng.tang,cdtwigg}@fb.com

Abstract. Grasping is natural for humans. However, it involves com-
plex hand conﬁgurations and soft tissue deformation that can result in
complicated regions of contact between the hand and the object. Under-
standing and modeling this contact can potentially improve hand mod-
els, AR/VR experiences, and robotic grasping. Yet, we currently lack
datasets of hand-object contact paired with other data modalities, which
is crucial for developing and evaluating contact modeling techniques. We
introduce ContactPose, the ﬁrst dataset of hand-object contact paired
with hand pose, object pose, and RGB-D images. ContactPose has 2306
unique grasps of 25 household objects grasped with 2 functional in-
tents by 50 participants, and more than 2.9 M RGB-D grasp images.
Analysis of ContactPose data reveals interesting relationships between
hand pose and contact. We use this data to rigorously evaluate various
data representations, heuristics from the literature, and learning meth-
ods for contact modeling. Data, code, and trained models are available
at https://contactpose.cc.gatech.edu.

Keywords: contact modeling, hand-object contact, functional grasping

1

Introduction

A person’s daily experience includes numerous and varied hand-object interac-
tions. Understanding and reconstructing hand-object interaction has received
growing attention from the computer vision, computer graphics, and robotics
communities. Most research has focused on hand pose estimation [17, 53, 57, 59],
realistic hand and body reconstruction [24, 25, 61, 65], and robotic grasp pre-
diction for anthropomorphic hands [7, 36]. In this paper, we address the under-
explored problem of hand-object contact modeling i.e. predicting object contact
with the hand, based on other information about the grasp, such as the 3D hand
pose and grasp images. Accurate contact models have numerous applications
in computer interfaces, understanding social interaction, object manipulation,
and safety. For example, a hand contact model could interpret computer com-
mands from physical interactions with a 3D printed replica object, or estimate if

 
 
 
 
 
 
2

S. Brahmbhatt et al.

Fig. 1: Examples from ContactPose, a dataset capturing grasps of household
objects. ContactPose includes high-resolution contact maps (object meshes tex-
tured with contact), 3D joints, and multi-view RGB-D videos of grasps. Left
hand joints are green, right hand joints are red.

pathogens from a contaminated surface were transmitted through contact. More
broadly, accurate contact modeling can improve estimation of grasp dynam-
ics [14, 37, 40, 46], which can lead to better VR simulations of grasping scenarios
and grasping with soft robotic hands [11, 28].

Lack of ground-truth data has likely played a role in the under-exploration
of this problem. Typically, the contacting surfaces of a grasp are occluded from
direct observation with visible light imaging. Approaches that instrument the
hand with gloves [55, 62] can subtly inﬂuence natural grasping behavior, and
do not measure contact on the object surface. Approaches that intersect hand
models with object models require careful selection of proximity thresholds or
speciﬁc contact hand points [25, 61]. They also cannot account for soft hand
tissue deformation, since existing state-of-the-art hand models [50] are rigid.

Brahmbhatt et al. [6] recently introduced thermal cameras as sensors for
capturing detailed ground-truth contact. Their method observes the heat trans-
ferred from the (warm) hand to the object through a thermal camera after the
grasp. We adopt their method because it avoids the pitfalls mentioned above
and allows for evaluation of contact modeling approaches with ground-truth
data. However, it also imposes some constraints. 1) Objects have a plain visual
texture since they are 3D printed to ensure consistent thermal properties. This
does not aﬀect 3D hand pose-based contact modeling methods and VR/robotic
grasping simulators, since they rely on 3D shape and not texture. It does limit the
generalization ability of RGB-based methods, which can potentially be mitigated
by using depth images and synthetic textures. 2) The grasps are static, because
in-hand manipulation results in multiple overlapping thermal hand-prints that
depend on timing and other factors. Contact modeling for static grasps is still an
unsolved problem, and forms the basis for future work on dynamic grasps. The
methods we present here could be applied to dynamic scenarios frame-by-frame.
In addition, we develop a data collection protocol that captures multi-view
RGB-D videos of the grasp, and an algorithm for 3D reconstruction of hand
joints (§ 3.1). To summarize, we make the following contributions:

camera - useflashlight - useeyeglasses - usetoothpaste - handoffknife - handoffwine glass - handoffbinoculars - usebanana - usecamera - useContactPose: A Dataset of Grasps with Object Contact and Hand Pose

3

– Data: Our dataset (ContactPose) captures 50 participants grasping 25 ob-
jects with 2 functional intents. It includes high-quality contact maps for each
grasp, over 2.9 M RGB-D images from 3 viewpoints, and object pose and 3D
hand joints for each frame. We will make it publicly available to encourage
research in hand-object interaction and pose estimation.

– Analysis: We dissect this data in various ways to explore the interesting
relationship between contact and hand pose. This reveals some surprising
patterns, and conﬁrms some common intuitions.

– Algorithms: We explore various representations of object shape, hand pose,
contact, and network architectures for learning-based contact modeling. Im-
portantly, we rigorously evaluate these methods (and heuristic methods from
the literature) against ground-truth unique to ContactPose.

2 Related Work

Fig. 2: Comparison to ContactDB [6]. It includes contact maps and turntable
RGB-D images (a), which are often not enough to fully interpret the grasp e.g.
it is not clear which ﬁngers generated the contact. In contrast, ContactPose
includes 3D joint locations (b), which allows association of contacted areas to
hand parts (c), and multi-view RGB-D grasp images (d). These data enable a
more comprehensive interpretation of the grasp.

Capturing and modeling contact: Previous works have instrumented hands
and/or objects to capture contact. Bernardin et al. [5] and Sundaram et al. [55]
used a tactile glove to capture hand contact during grasping. Brahmbhatt et
al. [6] used a thermal camera after the grasp to observe the heat residue left
by the warm hand on the object surface. However, these datasets lacked either
hand pose or grasp images, which are necessary for developing applicable contact
models (Figure 2). Pham et al. [44, 45] and Ehsani et al. [12] tracked hands and
objects in videos, and trained models to predict contact forces and locations at
ﬁngertips that explain observed object motion. In contrast, we focus on detailed
contact modeling for complex objects and grasps, evaluated against contact maps
over the entire object surface.
Contact heuristics: Heuristic methods to detect hand-object contact are of-
ten aimed at improving hand pose estimation. Hamer et al. [21] performed joint

(a)(b)(c)(d)ContactDBContactPose (ours)4

S. Brahmbhatt et al.

Feature
3D joints
Object pose
Grasp RGB images
Grasp Depth images
Natural hand appearance
Natural object appearance
Naturally situated
Multi-view images
Functional intent
Hand-object contact
# Participants
# Objects

FPHA [17] HO-3D [23] FreiHand [69] STAG [55] ContactDB [6] Ours

(cid:88)
(cid:88)
(cid:88)
(cid:88)
×
×
(cid:88)
×
(cid:88)
×
6
4

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
×
×
×
×
8
8

(cid:88)
×
(cid:88)
×
(cid:88)
(cid:88)
×
(cid:88)
×
×
32
35

×
×
(cid:88)
×
×
(cid:88)
×
×
×
(cid:88)
1
26

×
(cid:88)
×
×
×
×
×
×
(cid:88)
(cid:88)
50
50

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
×
×
(cid:88)
(cid:88)
(cid:88)
50
25

Table 1: Comparison with existing hand-object datasets. ContactPose stands out
for its size, and paired hand-object contact, hand pose and object pose.

hand tracking and object reconstruction [22], and inferred contact only at ﬁnger-
tips using proximity threshold. In simulation [63] and robotic grasping [38, 40],
contact is often determined similarly, or through collision detection [33,58]. Bal-
lan et al. [4] deﬁned a cone circumscribing object mesh triangles, and penalized
penetrating hand points (and vice versa). This formulation has also been used to
penalize self-penetration and environment collision [43, 61]. While such methods
were evaluated only through proxy tasks (e.g. hand pose estimation), Contact-
Pose enables evaluation against ground-truth contact (§ 6).
Grasp Datasets: Focusing on datasets involving hand-object interaction, hand
pose has been captured in 3D with magnetic trackers [17], gloves [5, 19], op-
timization [23], multi-view boot-strapping [53], semi-automated human-in-the-
loop [69], manually [54], synthetically [25], or as instances of a taxonomy [8,13,49]
along with RGB-D images depicting the grasps. However, none of these have
contact annotations (see Table 1), and suﬀer additional drawbacks like lack of
object information [53,69] and simplistic objects [17,54] and interactions [25,54],
which make them unsuitable for our task. In contrast, ContactPose has a large
amount of ground-truth contact, and real RGB-D images of complex (including
bi-manual) functional grasps for complex objects. The plain object texture is a
drawback of ContactPose. Tradeoﬀs for this in the context of contact modeling
are discussed in § 1.

3 The ContactPose Dataset

In ContactPose, hand-object contact is represented as a contact map on the
object mesh surface, and observed through a thermal camera. Hand pose is rep-
resented as 3D hand(s) joint locations in the object frame, and observed through
multi-view RGB-D video clips. The cameras are calibrated and object pose is
known, so that the 3D joints can be projected into images (examples shown in
supplementary material). Importantly, we avoid instrumenting the hands with
data gloves, magnetic trackers or other sensors. This has the dual advantage of

ContactPose: A Dataset of Grasps with Object Contact and Hand Pose

5

not interfering with natural grasping behavior and allowing us to use the ther-
mal camera-based contact capture method from [6]. We develop a computational
approach (Section 3.2) that optimizes for the 3D joint locations by leveraging
accurate object tracking and aggregating over multi-view and temporal informa-
tion. Our data collection protocol, described below, facilitates this approach.

3.1 Data Capture Protocol and Equipment

(a)

(b)

Fig. 3: (a) Our setup consists of 7 Optitrack Prime 13W tracking cameras, 3
Kinect v2 RGB-D cameras, a FLIR Boson 640 thermal camera, 3D printed ob-
jects, and a turntable. (b) Left: Diﬀerent object tracking marker conﬁgurations
we investigate. Right: 3D printed object with recessed 3 mm hemispherical
markers (highlighted by red arrows) oﬀer a good compromise between unobtru-
siveness and tracking performance.

We invite able-bodied participants to our laboratory and collect data through
the following IRB-approved protocol. Objects are placed at random locations
on a table in orientation normally encountered in practice. Participants are in-
structed to grasp an object with one of two functional intents (either using the
object, or handing it oﬀ). Next, they stand in the data collection area (Figure 3a)
and move the object for 10-15 s in the cubical space. They are instructed to hold
their hand joints steady, but are free to arbitrarily rotate the wrist and elbow,
and to grasp objects with both hands or their dominant hand. This motion is
recorded by 3 Kinect v2 RGB-D cameras (used for hand pose) and an Optitrack
motion capture (mocap) system (used for object pose). Next, they hand the ob-
ject to a researcher, who places it on a turntable, handling it with gloved hands.
The object is recorded with the mocap system, Kinect v2, and a FLIR Boson
640 thermal camera as the turntable rotates a circle.

Optitrack Prime 13WFLIRBoson 640Kinect v2Kinect v2TurntableObject4 mm recessed planar square3 mm recessed planar square4 mm recessed hemisphere3 mm recessed hemisphere3 mm protruding hemisphere6

S. Brahmbhatt et al.

Contact Capture: Thermal images are texture-mapped to the object mesh
using Open3D [66, 67]. As shown in [6] and the supp. mat., the resulting mesh
textures (called contact maps) accurately capture hand-object contact.
Object Selection and Fabrication: We capture grasps on a subset of 25
objects from [6] that are applicable for both ‘use’ and ‘hand-oﬀ’ grasping (see
supp. mat. for a list). The objects are 3D printed in blue for good contrast with
hands and the green background of our capture area. 3D printing the objects
ensures consistent thermal properties and ensures geometric consistency between
real world objects in capture sessions and the 3D models in our dataset.

Mocap recovers the object pose using retro-reﬂective markers, whose the
placement on the object requires some care. Attaching a large ‘marker tree’ would
block interactions with a signiﬁcant area of the surface. Placing hemispherical
markers on the surface is more promising, but a suﬃcient number (8+) are
needed to ensure visibility during hand occlusion and the resulting ‘bumps’ can
be uncomfortable to touch, which might inﬂuence natural grasping behavior. We
investigate a few alternative marker conﬁgurations (Figure 3b). Flat pieces of
tape were more comfortable but only tracked well when the marker was directly
facing the camera. A good compromise is to use 3 mm hemispherical markers
but to recess them into the surface by adding small cut-outs during 3D printing.
These are visible from a wide range of angles but do not signiﬁcantly aﬀect the
user’s grip. Fixing the marker locations also allows for simple calibration between
the Optitrack rigid body and the object’s frame.

3.2 Grasp Capture without Hand Markers

Each grasp is observed through N frames of RGB-D images from C cameras. We
assume that the hand is ﬁxed relative to the object, and the 6-DOF object pose
for each frame is given. So instead of estimating 3D joints separately in each
frame, we can aggregate the noisy per-frame 2D joint detections into a single set
of high-quality 3D joints, which can be transformed by the frame’s object pose.
For each RGB frame, we use Detectron [26] to locate the wrist, and run the
OpenPose hand keypoint detector [53] on a 200×200 crop around the wrist. This
produces 2D joint detections {x(i)}N
i=1, following
the 21-joint format from [53]. One option is to lift these 2D joint locations to 3D
using the depth image [59], but that biases the location toward the camera and
the hand surface (our goal is to estimate joint locations internal to the hand).
Furthermore, the joint detections at any given frame are unreliable. Instead, we
use our hand-object rigidity assumption to estimate the 3D joint locations oX
in the object frame that are consistent with all N C images. This is done by
minimizing the average re-projection error:

i=1 and conﬁdence values {w(i)}N

min
oX

N
(cid:88)

C
(cid:88)

i=1

c=1

(cid:16)

D

x(i)

c , π

(cid:16)oX; Kc,c Tw

wT (i)
o

(cid:17)

(cid:17)

; w(i)
c

(1)

where D is a distance function, and π(·) is the camera projection function using
wT (i)
camera intrinsics Kc and object pose w.r.t. camera at frame i, cT (i)
o .

o = cTw

ContactPose: A Dataset of Grasps with Object Contact and Hand Pose

7

Our approach requires the object pose w.r.t. world at each frame wT (i)
i.e.
object tracking. This is done using an Optitrack motion capture system tracking
markers embedded in the object surface.

o

In practice, the 2D joint detections are noisy and object tracking fails in
some frames. We mitigate this by using the robust Huber function [29] over
Mahalanobis distance (w(i) acting as variance) as D, and wrapping Eq. 1 in
a RANSAC [16] loop. A second pass targets frames that fail the RANSAC in-
lier test due to inaccurate object pose. Their object pose is estimated through
the correspondence between their 2D detections and the RANSAC-ﬁt 3D joint
locations, and they are included in the inlier set if they pass the inlier test
(re-projection error less than a threshold). It is straightforward to extend the
optimization described above to bi-manual grasps. We manually curated the
dataset, including clicking 2D joint locations to aid the 3D reconstruction in
some cases, and discarding some obviously noisy data.
Hand Mesh Models: In addition to capturing grasps, hand shape information
is collected through palm contact maps on a ﬂat plate, and multi-view RGB-
D videos of the participant performing 7 known hand gestures (shown in the
supplementary material). Along with 3D joints, this data can potentially enable
ﬁtting of the MANO hand mesh model [50] to each grasp [41]. In this paper, we
use meshes ﬁt to 3D joints (Figure 4, see supp. mat. for details) for some of the
analysis and learning experiments discussed below.

Fig. 4: MANO hand meshes [50] ﬁt to ContactPose data. Both hand pose and
shape parameters are optimized to minimize the distance of MANO joints from
ContactPose 3D joint annotations.

4 Data Analysis

Contact maps are [0, 1] normalized following the sigmoid ﬁtting procedure from [6].
Association of Contact to Hand Parts: It has been observed that certain
ﬁngers and parts (e.g. ﬁngertips) are contacted more frequently than others [8,9].
ContactPose allows us to quantify this. This can potentially inform anthropo-
morphic robotic hand design and tactile sensor (e.g. BioTac [56]) placement in
robotic hands. For each grasp, we threshold the contact map at 0.4 and associate
each contacted object point with its nearest hand point from the ﬁtted MANO
hand mesh. A hand point is considered to be contacted if one or more contacted
object points are associated with it. A coarser analysis at the phalange level is

eyeglasses - usewine glasshandoffflashlightusebinocularsusecamera - handoff8

S. Brahmbhatt et al.

possible by modeling phalanges as line segments connecting joints. In this case,
the distance from an object point to a phalange is the distance to the closest
point on the line segment.

(a)

(b)

Fig. 5: (a) Hand contact probabilities estimated from the entire dataset. (b) As-
sociation of contacted binoculars points with ﬁngers (top) and sets of phalanges
at the same level of wrist proximity (bottom), indicated by diﬀerent colors.

Figure 5a shows the contact probabilities averaged over ‘use’ and ‘hand-oﬀ’
grasps. Not surprisingly, the thumb, index, and middle ﬁnger are the most con-
tacted ﬁngers, and tips are the most contacted phalanges. Even though ﬁngertips
receive much attention in grasping literature, the contact probability for all three
phalanges of the index ﬁnger is higher than that of the pinky ﬁngertip. Proximal
phalanges and palm also have signiﬁcant contact probabilities. This is consistent
with observations made by Brahmbhatt et al [6]. Interestingly, contact is more
concentrated at the thumb and index ﬁnger for ‘hand-oﬀ’ than ‘use’. ‘Use’ grasps
have an average contact area of 35.87 cm2 compared to 30.58 cm2 for ‘hand-oﬀ’.
This analysis is similar to that in Fig. 3 of Hasson et al. [25], but supported by
ground-truth contact rather than synthetic grasps.

Comparison of the average ﬁngertip vs. whole-hand contact areas (Figure 6)
shows that non-ﬁngertip areas play a signiﬁcant role in grasp contact, conﬁrming
the approximate analysis in [6].

(a) ‘use’ grasps

(b) ‘handoﬀ’ grasps

Fig. 6: Comparing average ﬁngertip (red) vs. whole-hand (blue) contact areas.

usehandoffContactPose: A Dataset of Grasps with Object Contact and Hand Pose

9

Fig. 7: Automatic ‘active area’ discovery: Contact probability for various hand
parts on the object surface.

Automatic Active Area Discovery: Brahmbhatt et al [6] deﬁne active areas
as regions on the object highly likely to be contacted. While they manually se-
lected active areas and measured their probability of being contacted by any part
of the hand, ContactPose allows us to ‘discover’ active areas automatically and
for speciﬁc hand parts. We use the object point-phalange association described
above (e.g. Fig. 5b) to estimate the probability of each object point being con-
tacted by a given hand part (e.g. index ﬁnger tip), which can be thresholded to
segment the active areas. Figure 7 shows this probability for the index ﬁnger-
tip and thumb, for ‘use’ grasps of some objects. This could potentially inform
locations for placing contact sensors (real [45] or virtual for VR) on objects.

(a)

(b)

Fig. 8: (a) Per-object standard deviation in 3D joint locations, for ‘use’ and
‘hand-oﬀ’. ‘Hand-oﬀ’ grasps consistently exhibit more diversity than ‘use’ grasps.
(b) A pair of grasps with similar hand pose but diﬀerent contact characteristics.
Hand contact feature color-coding is similar to Figure 5a.

Grasp Diversity: We further quantify the eﬀect of intent on grasping behavior
by measuring the standard deviation of 3D joint locations over the dataset. The
mean of all 21 joint standard deviations is shown in Figure 8a. It shows that
‘hand-oﬀ’ grasps are more diverse than ‘use’ grasps in terms of hand pose. We

thumbthumbthumbthumbindexfingerindexfingerindexfingerapplebananabinocularsbowlcameracell_phonecupeyeglassesflashlighthammerheadphonesknifelight_bulbmousemugpanps_controllerscissorsstaplertoothbrushtoothpasteutah_teapotwater_bottlewine_glass02040Joint Location Std. Dev. (mm)usehandoff10

S. Brahmbhatt et al.

Fig. 9: Examples from hand pose clusters for ‘use’ and ‘hand-oﬀ’ grasps. Grasps
from diﬀerent clusters are shown with diﬀerent colors (some grasps are bi-
manual). Left hand joints are green, right hand joints are red.

accounted for symmetrical objects (e.g. wine glass) by aligning the 6 palm joints
(wrist + 5 knuckles) of all hand poses for that object to a single set of palm
joints, where the only degree of freedom for alignment is rotation around the
symmetry axis. Hand size is normalized by scaling all joint location such that
the distance from wrist to middle knuckle is constant.

Organizing the grasps by clustering these aligned 3D joints (using L2 distance
and HDBSCAN [10]) reveals the diversity of grasps captured in ContactPose
(Figure 9). ‘Hand-oﬀ’ grasps exhibit a more continuous variation than ‘use’
grasps, which are tied more closely to the function of the object. The average
intra-cluster distance for ‘use’ grasps is 32.5% less than that for ‘handoﬀ’ grasps.
Figure 8b shows pair of grasps found by minimizing hand pose distance and
maximizing hand contact distance. We use the phalange-level contact associa-
tion described above. Summing the areas of all object mesh triangles incident
to all vertices associated with a phalange creates a 20-dimensional vector. We
use L2 distance over this vector as contact distance. It shows that grasps with
similar hand pose can contact diﬀerent parts of the object and/or hand, inducing
diﬀerent forces and manipulation possibilities [17] and emphasizing that hand
pose alone provides an inadequate representation of grasping.

5 Contact Modeling Experiments

This section describes our experiments on contact modeling given the hand pose
or RGB grasp image(s), assuming known object geometry and pose. Our exper-
iments focus on ﬁnding good data representations and learning algorithms, and
evaluating techniques against ground-truth. By providing high-quality contact
output from readily available input modalities, such models can enable better
hand-object dynamics simulation in AR/VR and soft robotic grasping.
Object Shape Representation: We represent the object shape through either
a pointcloud densely sampled from the surface (1K-30K points based on size),
or a 643 voxel occupancy grid. Features encoding the input hand pose are asso-
ciated with individual points (voxels). The entire pointcloud (voxel grid) is then
processed to predict contact values for points (surface voxels).

handoffuseContactPose: A Dataset of Grasps with Object Contact and Hand Pose

11

Hand Pose Representation: Features relating object shape to hand pose are
computed for each point or voxel. These features have varying levels of richness
of hand shape encoding. To simulate occlusion and noisy pose perception for the
ﬁrst 4 features, we sample a random camera pose and drop (set to 0) all features
associated with the farthest 15% of the joints from the camera.

– simple-joints: We start by simply using the 21 3D joint locations w.r.t.
the object coordinate system as 63-dimensional features for every point. For
bi-manual grasps, points use the hand with the closest joint.

– relative-joints: Since contact at an object surface point depends on the
relative position of the ﬁnger, we next calculate relative vectors from an
object point to every joint of the hand closest to it. Contact also depends
on the surface geometry: a ﬁnger is more likely to contact an object point if
the vector to it is parallel to the surface normal at that point. Hence we use
unit-norm surface normals and the relative joint vectors to form 63 + 3 = 66-
dimensional features for every point.

– skeleton: To better capture hand joint connectivity, we compute relative
vectors from an object point to the nearest point on phalanges, modeled as
line segments. 40-dimensional features for each object point are constructed
by concatenating the lengths of 20 such vectors (one for each phalange), and
their dot product with the surface normal at that object point.

– mesh: These features leverage the rich MANO hand model geometry. A rel-
ative vector is constructed from the object point to its closest hand mesh
point. 23-dimensional features are constructed from the length of this vector,
its dot product with the surface normal, and distances to 21 hand joints.
– Grasp Image(s): To investigate if CNNs can extract relevant information
directly from images, we extract dense 40-dimensional features from 256×256
crops of RGB grasp images using a CNN encoder-decoder inspired by U-
Net [51] (see supplementary material for architecture). These images come
from the same time instant. We investigate both 3-view and 1-view settings,
with feature extractor being shared across views for the former. Features
are transferred to corresponding 3D object points using the known object
pose and camera intrinsics, averaging the features if multiple images observe
the same 3D point (Figure 11a). Points not visible from any image have all
features set to 0. Image backgrounds are segmented by depth thresholding at
the 20th percentile, and the foreground pixels are composited onto a random
COCO [35] image. This investigation is complementary to recent work on
image-based estimation of object geometry [20, 68], object pose [18, 60], and
hand pose [23, 53, 57, 65, 69].

Contact Representation: We observed in early experiments that the mean
squared error loss resulted in blurred and saturated contact predictions. This
might be due to contact value occurrence imbalance and discontinuous contact
boundaries for smooth input features. Hence, we discretize the [0, 1] normalized
values into 10 equal bins and treat contact prediction as a classiﬁcation problem,
inspired by Zhang et al [64]. We use the weighted cross entropy loss, where
the weight for each bin is proportional to a linear combination of the inverse

12

S. Brahmbhatt et al.

occurrence frequency of that bin and a uniform distribution (Eq. 4 from [64]
with λ = 0.4). Following [64], we derive a point estimate for contact in [0, 1]
from classiﬁcation outputs using the annealed mean (T = 0.1).
Learning Algorithms: Given the hand pose features associated with points or
voxels, the entire pointcloud or voxel grid is processed by a neural network to
predict the contact map. We use the PointNet++ [48] architecture implemented
in pytorch-geometric [15, 42] (modiﬁed to reduce the number of learnable pa-
rameters) for pointclouds, and the VoxNet [39]-inspired 3D CNN architecture
from [6] for voxel grids (see the supplementary material for architectures). For
voxel grids, a binary feature indicating voxel occupancy is appended to hand
pose features. Following [6], hand pose features are set to 0 for voxels inside
the object. Because the features are rich and provide fairly direct evidence of
contact, we include a simple learner baseline of a multi-layer perceptron (MLP)
with 90 hidden nodes, parametric ReLU [27] and batchnorm [30].
Contact Modeling Heuristics: We also investigate the eﬀectiveness of heuris-
tic techniques, given detailed hand geometry through the MANO hand mesh.
Speciﬁcally, we use the conic distance ﬁeld Ψ from [4, 61] as a proxy for contact
intensity. To account for imperfections in hand modelling (due to rigidity of the
MANO mesh) and ﬁtting, we compute Ψ not only for collisions, but also when
the hand and object meshes are closer than 1 cm. Finally, we calibrate Ψ to our
ground truth contact through least-squares linear regression on 4700 randomly
sampled contact points. Both these steps improve the technique’s performance.

6 Results

Learner

Features

Participant Split Object Split
AuC (%) Rank AuC (%) Rank

MLP

None
VoxNet [6, 39]

Heuristic [4, 61]
skeleton
simple-joints
relative-joints
skeleton
mesh
simple-joints
relative-joints
skeleton
mesh
images (1-view)
PointNet++ images (3-view)

PointNet++

Image enc-dec,

5

3
4

2
1

78.31
77.94
75.11
75.39
80.78
79.89
71.61
74.51
81.15
81.29
72.89
78.06

81.11
79.99
77.83
78.83
80.07
84.74
73.67
77.10
81.49
84.18
77.09
80.80

4

1

3
2

5

Table 2: Contact prediction re-balanced AuC (%) (higher is better) for various
combinations of features and learning methods.

ContactPose: A Dataset of Grasps with Object Contact and Hand Pose

13

In this section, we evaluate various combinations of features and learning
algorithms described in § 5. The metric for quantitative evaluation is the area
under the curve formed by calculating accuracy at increasing contact diﬀerence
thresholds. Following [64], this value is re-balanced to account for varying occur-
rence frequencies of values in the 10 contact bins. We create two data splits: the
object split holds out mug, pan and wine glass following [6], and the participant
split holds out participants 5, 15, 25, 35, and 45. The held out data is used for
evaluation, and models are trained on the rest.

Table 2 shows the re-balanced AuC values averaged over held out data for
the two splits. We observe that features capturing richer hand shape information
perform better (e.g. simple-joints vs. skeleton and mesh). Learning-based
techniques with mesh features that operate on pointclouds are able to outperform
heuristics, even though the latter has access to the full high-resolution object
mesh, while the former makes predictions on a pointcloud. Learning also enables
skeleton features, which have access to only the 3D joint locations, to perform
competitively against mesh-based heuristics and features. While image-based
techniques are not yet as accurate as the hand pose-based ones, a signiﬁcant
boost is achieved with multi-view inputs.

Figure 10 shows contact prediction results from hand pose for mug, an un-
seen object. Predictions are transferred from the pointcloud to high-resolution
meshes for better visualization. The skeleton-PointNet++ combination is able
to predict plausible contact patterns for dropped-out parts of the hand, and
capture some of the nuances of palm contact. The mesh-PointNet++ combina-
tion captures more nuances, especially at the thumb and bottom of the palm.
In contrast, relative-joints features-based predictions are diﬀused, lack ﬁner
details, and have high contact probability in the gaps between ﬁngers, possibly
due to lack of access to information about joint connectivity and hand shape.

Figure 11b shows contact prediction results from RGB images for mug, an
unseen object. These predictions have less high-frequency details compared to
hand pose based predictions. They also suﬀer from depth ambiguity – the prox-
imal part of the index ﬁnger appears to be in contact from the mug images, but
is actually not. This can potentially be mitigated by use of depth images.

7 Conclusion and Future Work

We introduced ContactPose, the ﬁrst dataset of paired hand-object contact,
hand pose, object pose, and RGB-D images for functional grasping. Data analy-
sis revealed some surprising patterns, like higher concentration of hand contact
at the ﬁrst three ﬁngers for ‘hand-oﬀ’ vs. ‘use’ grasps. We also showed how
learning-based techniques for geometry-based contact modeling can capture nu-
anced details missed by heuristic methods.

Using this contact ground-truth to develop more realistic, deformable hand
mesh models could be an interesting research direction. State-of-the-art mod-
els (e.g. [31, 50]) are rigid, while the human hand is covered with soft tissue.
As the Future Work section of [50] notes, they are trained with meshes from

14

S. Brahmbhatt et al.

which objects are manually removed, and do not explicitly reason about hand-
object contact. ContactPose data can potentially help in the development and
evaluation of hand mesh deformation algorithms.

Fig. 10: Contact prediction for mug (an unseen object) from hand pose. All input
features related to black line segments and joints were dropped (set to 0). Notice
how the mesh- and skeleton-PointNet++ predictors is able to capture nuances
of palm contact, thumb and ﬁnger shapes.

(a)

(b)

Fig. 11: (a) Image-based contact prediction architecture. (b) Contact prediction
for mug (an unseen object) from RGB images, using networks trained with 3
views. Hand poses shown only for reference.

Acknowledgements: We are thankful to the anonymous reviewers for helping
improve this paper. We would also like to thank Elise Campbell, Braden Copple,
David Dimond, Vivian Lo, Jeremy Schichtel, Steve Olsen, Lingling Tao, Sue
Tunstall, Robert Wang, Ed Wei, and Yuting Ye for discussions and logistics
help.

skeleton : PointNet++relative-joints : PointNet++Ground Truthmesh : PointNet++3D Aggregationobject poseencdec PointNet++encdecencdec256256256404040cameraintrinsicspointcloudcontactprediction256256256PredictionGround TruthInput ImagesContactPose: A Dataset of Grasps with Object Contact and Hand Pose

15

Supplementary Material

Abstract. The supplementary material includes a discussion on contact
capture, accuracy evaluation of the hand pose and contact ground truth,
MANO hand mesh [50] ﬁtting details, network architectures, and imple-
mentation details for the learning algorithms. Finally, we present the list
of objects and their ‘use’ instructions, and describe the participants’ hand
information that is included in ContactPose. Please see the extended sup-
plementary material at https://contactpose.cc.gatech.edu for exam-
ple RGB-D imagery and slices through the data in the form of 1) object-
and intent-speciﬁc hand contact probabilities, and 2) ‘use’ vs. ‘hand-oﬀ’
contact maps and hand poses for all grasps of an object.

A Contact Capture Discussion

The process to convert thermal image pixels to contact values follows [6]. Raw
thermal readings were converted to continuous contact values in [0, 1] using a
sigmoid that maps the warmest point to 0.95 and the coldest point to 0.05.
These values non-linearly encode the temperature of the object, where [0, 1] ap-
proximately corresponds to [room temperature, body temperature]. While most
experiments used this continuous value, if a hard decision about the contact sta-
tus of a point was desired, this was done by thresholding these processed values
at 0.4.

B MANO Fitting

This section provides details for the ﬁtting procedure of the MANO [50] hand
model to ContactPose data. Borrowing notation from [50], the MANO model is
a mesh with vertices M (β, θ) parameterized by shape parameters β and pose
parameters θ. The 3D joint locations of the posed mesh, denoted here by J (β, θ),
are also a function of the shape and pose parameters. We modify the original
model by adding one joint at each ﬁngertip, thus matching the format of joints
J ∗ in ContactPose annotations.

MANO ﬁtting is performed by optimizing the following objective function,

which combines L2 distance of 3D joints and shape parameter regularization:

β∗, θ∗ = arg minβ,θ||J (β, θ) − J ∗|| +

1
σ

||β||

(2)

where σ is set to 10. It is optimized using the Dogleg [47] optimizer imple-
mented in chumpy [2]. We initialized β and θ to 0 (mean shape and pose) after
6-DOF alignment of the wrist and 5 palm joints. Finally, the MANO model in-
cludes a PCA decomposition of 45 pose parameters to 6 parameters by default.
We provide MANO ﬁtting data with 10 and 15 pose components in the Con-
tactPose dataset, but use the MANO models with 10 pose components in all our
contact modeling experiments.

16

S. Brahmbhatt et al.

C Dataset Accuracy

In this section, we cross-evaluate the accuracy of the hand pose and contact data
included in ContactPose.

C.1 Contact Accuracy

We note that conduction is the principal mode of heat transfer in solid-to-solid
contact [34]. Combined with the observation by Brahmbhatt et al. [6] that heat
dissipation within the 3D printed objects is low over the time scales we employ
to scan them, this indicates that conducted heat can accurately encode contact.
Following [6], we measure the conducted heat with a thermal camera.
Agreement with MANO Hand Mesh: The average distance of contacted
object points from their nearest hand point is 4.17 mm (10 MANO hand pose
parameters) and 4.06 mm (15 MANO hand pose parameters).
Agreement with Pressure-based Contact: We also veriﬁed thermal contact
maps against pressure images from a Sensel Morph planar pressure sensor [3,52].
After registering the thermal and pressure images, we thresholded the processed
thermal image at values in [0, 1] with an interval of 0.1. Any nonzero pixel in the
pressure image is considered to be contacted. Binary contact agreement peaks
at 95.4% at the threshold of 0.4 (Figure 12).

Fig. 12: Relation of contact value threshold to the binary contact agreement
with pressure data from the Sensel Morph sensor. Agreement maximizes at the
threshold value of 0.4.

contact value threshold02550751000.00.10.20.30.40.50.60.70.80.91.0Binary Contact Agreement with Pressure Data (%)ContactPose: A Dataset of Grasps with Object Contact and Hand Pose

17

C.2 3D Hand Pose Accuracy

Following [23], this is measured through the discrepancy between 3D joints of
the ﬁtted MANO model and the ground truth 3D joints. Low-quality physically
implausible ground truth can yield higher discrepancy, since the MANO model
is not able to ﬁt to physically implausible joint locations. Table 3 shows that
ContactPose has signiﬁcantly lower discrepancy than other recent datasets, even
though it uses less than one-third MANO hand pose parameters. Table 4 shows
statistics for hand-object penetration.

Dataset
HO-3D [23]
FreiHand [69]
HANDS 2019 [1]
ContactPose (ours) – 10 pose params
ContactPose (ours) – 15 pose params

Avg. 3D Joint Error (mm) AUC (%)

7.7
-
11.39
7.65
6.68

79.0
79.1
-
84.54
86.49

Table 3: Discrepancy between 3D joints of the ﬁtted MANO model and the
ground truth 3D joints. 3D joint error (lower is better) is averaged over all 21
joints. AUC (higher is better) is the area under the error threshold vs. percentage
of correct keypoints (PCK) curve, where the maximum error threshold is set to
5 cm.

Dataset
FPHA [17] (reported in [25])
ContactPose – 15 pose params

Mean Penetration (mm) Median Penetration (mm) Penetration freq (%)

11.0
2.02

-
1.53

-
4.75

Table 4: Statistics for hand-object penetration showing the accuracy of Contact-
Pose. Note that [25] report joint penetration for [17], while we report surface
penetration.

D Participants’ Hand Information

We captured information about each ContactPose participant’s hands in two
ways: 1) contact map on a ﬂat plate (example shown in Figure 13), and 2) RGB-
D videos of the participants performing 7 hand gestures (shown in Figure 14).
This can potentially be used to estimate the hand shape by ﬁt embodied hand
models (e.g. [50]).

18

S. Brahmbhatt et al.

Fig. 13: Contact map of a participant’s palm on a ﬂat plate. Such palm contact
maps for each participant are included in ContactPose.

Fig. 14: Pre-deﬁned hand gestures performed by each participant. RGB-D videos
from 3 Kinects of each participant performing these gestures are included in
ContactPose.

ContactPose: A Dataset of Grasps with Object Contact and Hand Pose

19

E Network Architectures

E.1 PointNet++

The PointNet++ architecture we use is similar to the pointcloud segmentation
network from Qi et al [48], with modiﬁcations aiming to reduce the number of
learnable parameters. Similarly to [48], we use SA (s, r, [l1, . . . , ld]) to indicate a
Set Abstraction layer with a farthest point sampling ratio s, ball radius r (the
pointcloud is normalized to lie in the [−0.5, 0.5] cube) and d fully connected
layers of size li(i = 1 . . . d). The global Set Abstraction layer is denoted without
farthest point sampling ratio and ball radius. F P (K, [l1, . . . , ld]) indicates a
Feature Propagation layer with K nearest neighbors and d fully connected layers
of size li(i = 1 . . . d). F C (Sin, Sout) indicates a fully connected layer of output
size Sout applied separately to each point (which has Sin-dimensional features).
Each fully connected layer in the Set Abstraction and Feature Propagation layers
is followed by ReLU and batch-norm layers. Our network architecture is:

SA (0.2, 0.1, [F, 64, 128]) − SA (0.25, 0.2, [128, 128, 256]) −

SA ([256, 512, 1024]) − F P (1, [1024 + 256, 256, 256])−

F P (3, [256 + 128, 256, 128]) − F P (3, [128 + F, 128, 128])−

F C(128, 128) − F C(128, 10)

where F is the number of input features and the ﬁnal layer outputs scores for
the 10 contact value classes.

E.2

Image Encoder-Decoder

We take inspiration from U-Net [51] and design the light-weight network shown
in 15 that extracts dense features from RGB images. The global average pooling
layer is intended to capture information about the entire hand and object.

F Training and Evaluation Details

All models are trained using PyTorch [42] and the Adam optimizer [32] (base
learning rate ∈ {5 × 10−4, 1 × 10−3, 5 × 10−3}, momentum of 0.9, weight decay
of 5e − 4, and a batch size of 25). Both point-clouds and voxel-grids are rotated
around their ‘up’-axis at regularly spaced 30◦ intervals. These rotations are con-
sidered separate data points during training, and their predictions are averaged
during evaluation.

For image-based contact prediction, ContactPose has approximately 300 RGB-
D frames (× 3 Kinects) for each grasp, but temporally nearby frames are highly
correlated because of the high frame rate. Hence, we include equally spaced 50
frames from each grasp in the training set. Evaluation is performed over equally
spaced 12 frames from this set of 50 frames.

20

S. Brahmbhatt et al.

Fig. 15: Architecture for the image encoder-decoder (Figure 10 in main paper).
Horizontal numbers indicate number of channels, and vertical numbers indicate
spatial dimensions.

256212821231616163264232326464643232161640output3x3 conv-ReLU-BN2x2 max poolglobal avg. pool2x bilinear upsampleconvContactPose: A Dataset of Grasps with Object Contact and Hand Pose

21

G List of Objects

Table 5 shows a list of all 25 objects in ContactPose, along with information
about the which of these objects are included in the two functional grasping
categories, and the speciﬁc ‘use’ instructions.

Object
apple
banana
binoculars
bowl
camera
cell phone
cup
door knob
eyeglasses
ﬂashlight
hammer
headphones
knife
light bulb
mouse
mug
pan
PS controller
scissors
stapler
toothbrush
toothpaste
Utah teapot
water bottle
wine glass
Total

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

use instruction
eat
peel
see through
drink from
take picture
talk on
drink from
twist to open door
wear
turn on
hit a nail
wear
cut
screw in a socket

handoﬀ use
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88) use to point and click
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88) squeeze out toothpaste
(cid:88)
(cid:88)
(cid:88)
25

drink from
cook in
play a game with
cut with
staple
brush teeth

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
24

pour tea from
open
drink wine from

Table 5: List of objects in ContactPose and speciﬁc ‘use’ instructions

References

1. 5th International Workshop on Observing and Understanding Hands in Action.
https://sites.google.com/view/hands2019/challenge#h.p_adfpp7VAhgAL, ac-
cessed: 2020-03-12 17

2. chumpy: Autodiﬀerentiation tool for Python. https://github.com/mattloper/

chumpy, accessed: 2020-03-12 15

3. Sensel morph. https://sensel.com/pages/the-sensel-morph, accessed 2020-07-

07 16

22

S. Brahmbhatt et al.

4. Ballan, L., Taneja, A., Gall, J., Van Gool, L., Pollefeys, M.: Motion capture of
hands in action using discriminative salient points. In: European Conference on
Computer Vision. pp. 640–653. Springer (2012) 4, 12

5. Bernardin, K., Ogawara, K., Ikeuchi, K., Dillmann, R.: A sensor fusion approach
for recognizing continuous human grasping sequences using hidden markov models.
IEEE Transactions on Robotics 21(1), 47–57 (2005) 3, 4

6. Brahmbhatt, S., Ham, C., Kemp, C.C., Hays, J.: ContactDB: Analyzing and pre-
dicting grasp contact via thermal imaging. In: The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) (June 2019) 2, 3, 4, 5, 6, 7, 8, 9, 12, 13,
15, 16

7. Brahmbhatt, S., Handa, A., Hays, J., Fox, D.: ContactGrasp: Functional Multi-
ﬁnger Grasp Synthesis from Contact. In: IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS) (2019) 1

8. Bullock, I.M., Feix, T., Dollar, A.M.: The yale human grasping dataset: Grasp, ob-
ject, and task data in household and machine shop environments. The International
Journal of Robotics Research 34(3), 251–255 (2015) 4, 7

9. Bullock, I.M., Zheng, J.Z., De La Rosa, S., Guertler, C., Dollar, A.M.: Grasp
frequency and usage in daily household and machine shop tasks. IEEE transactions
on haptics 6(3), 296–308 (2013) 7

10. Campello, R.J.G.B., Moulavi, D., Zimek, A., Sander, J.: Hierarchical density
estimates for data clustering, visualization, and outlier detection. ACM Trans.
Knowl. Discov. Data 10(1), 5:1–5:51 (Jul 2015). https://doi.org/10.1145/2733381,
http://doi.acm.org/10.1145/2733381 10

11. Deimel, R., Brock, O.: A novel type of compliant and underactuated robotic hand
for dexterous grasping. The International Journal of Robotics Research 35(1-3),
161–185 (2016) 2

12. Ehsani, K., Tulsiani, S., Gupta, S., Farhadi, A., Gupta, A.: Use the force, luke!
learning to predict physical forces by simulating eﬀects. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
(June 2020) 3

13. Feix, T., Romero, J., Schmiedmayer, H.B., Dollar, A.M., Kragic, D.: The grasp
taxonomy of human grasp types. IEEE Transactions on Human-Machine Systems
46(1), 66–77 (2015) 4

14. Ferrari, C., Canny, J.: Planning optimal grasps. In: Proceedings IEEE International

Conference on Robotics and Automation. pp. 2290–2295. IEEE (1992) 2

15. Fey, M., Lenssen, J.E.: Fast graph representation learning with PyTorch Geometric.
In: ICLR Workshop on Representation Learning on Graphs and Manifolds (2019)
12

16. Fischler, M.A., Bolles, R.C.: Random sample consensus: a paradigm for model
ﬁtting with applications to image analysis and automated cartography. Communi-
cations of the ACM 24(6), 381–395 (1981) 7

17. Garcia-Hernando, G., Yuan, S., Baek, S., Kim, T.K.: First-person hand action
benchmark with rgb-d videos and 3d hand pose annotations. In: Proceedings of
Computer Vision and Pattern Recognition (CVPR) (2018) 1, 4, 10, 17

18. Garon, M., Lalonde, J.F.: Deep 6-dof tracking. IEEE transactions on visualization

and computer graphics 23(11), 2410–2418 (2017) 11

19. Glauser, O., Wu, S., Panozzo, D., Hilliges, O., Sorkine-Hornung, O.: Interactive
hand pose estimation using a stretch-sensing soft glove. ACM Transactions on
Graphics (TOG) 38(4), 1–15 (2019) 4

ContactPose: A Dataset of Grasps with Object Contact and Hand Pose

23

20. Groueix, T., Fisher, M., Kim, V.G., Russell, B.C., Aubry, M.: A papier-mˆach´e
approach to learning 3d surface generation. In: Proceedings of the IEEE conference
on computer vision and pattern recognition. pp. 216–224 (2018) 11

21. Hamer, H., Gall, J., Weise, T., Van Gool, L.: An object-dependent hand pose prior
from sparse training data. In: IEEE Computer Society Conference on Computer
Vision and Pattern Recognition. pp. 671–678. IEEE (2010) 3

22. Hamer, H., Schindler, K., Koller-Meier, E., Van Gool, L.: Tracking a hand ma-
nipulating an object. In: 2009 IEEE 12th International Conference on Computer
Vision. pp. 1475–1482. IEEE 4

23. Hampali, S., Rad, M., Oberweger, M., Lepetit, V.: Honnotate: A method for 3d
annotation of hand and object poses. In: IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) (June 2020) 4, 11, 17

24. Hassan, M., Choutas, V., Tzionas, D., Black, M.J.: Resolving 3d human pose am-
biguities with 3d scene constraints. In: The IEEE International Conference on
Computer Vision (ICCV) (October 2019) 1

25. Hasson, Y., Varol, G., Tzionas, D., Kalevatykh, I., Black, M.J., Laptev, I., Schmid,
C.: Learning joint reconstruction of hands and manipulated objects. In: Proceed-
ings of the IEEE Conference on Computer Vision and Pattern Recognition. pp.
11807–11816 (2019) 1, 2, 4, 8, 17

26. He, K., Gkioxari, G., Dollr, P., Girshick, R.: Mask R-CNN. In: IEEE International

Conference on Computer Vision (ICCV). pp. 2980–2988 (Oct 2017) 6

27. He, K., Zhang, X., Ren, S., Sun, J.: Delving deep into rectiﬁers: Surpassing human-
level performance on imagenet classiﬁcation. In: Proceedings of the IEEE interna-
tional conference on computer vision. pp. 1026–1034 (2015) 12

28. Homberg, B.S., Katzschmann, R.K., Dogar, M.R., Rus, D.: Haptic identiﬁcation
of objects using a modular soft robotic gripper. In: IEEE/RSJ International Con-
ference on Intelligent Robots and Systems (IROS). pp. 1698–1705. IEEE (2015)
2

29. Huber, P.J.: Robust estimation of a location parameter. In: Breakthroughs in statis-

tics, pp. 492–518. Springer (1992) 7

30. Ioﬀe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In: International Conference on Machine Learning.
pp. 448–456 (2015) 12

31. Joo, H., Simon, T., Sheikh, Y.: Total capture: A 3d deformation model for tracking
faces, hands, and bodies. In: Proceedings of the IEEE conference on computer
vision and pattern recognition. pp. 8320–8329 (2018) 13

32. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980 (2014) 19

33. Larsen, E., Gottschalk, S., Lin, M.C., Manocha, D.: Fast distance queries with
rectangular swept sphere volumes. In: IEEE International Conference on Robotics
and Automation. Symposia Proceedings (Cat. No. 00CH37065). vol. 4, pp. 3719–
3726. IEEE (2000) 4

34. Lienhard, IV, J.H., Lienhard, V, J.H.: A Heat Transfer Textbook. Phlogiston Press,
Cambridge, MA, 5th edn. (Aug 2019), http://ahtt.mit.edu, version 5.00 16
35. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll´ar, P.,
Zitnick, C.L.: Microsoft coco: Common objects in context. In: European conference
on computer vision. pp. 740–755. Springer (2014) 11

36. Lu, Q., Chenna, K., Sundaralingam, B., Hermans, T.: Planning multi-ﬁngered
grasps as probabilistic inference in a learned deep network. In: International Sym-
posium on Robotics Research (2017) 1

24

S. Brahmbhatt et al.

37. Mahler, J., Matl, M., Satish, V., Danielczuk, M., DeRose, B., McKinley, S., Gold-
berg, K.: Learning ambidextrous robot grasping policies. Science Robotics 4(26),
eaau4984 (2019) 2

38. Mahler, J., Pokorny, F.T., Hou, B., Roderick, M., Laskey, M., Aubry, M., Kohlhoﬀ,
K., Kr¨oger, T., Kuﬀner, J., Goldberg, K.: Dex-net 1.0: A cloud-based network of 3d
objects for robust grasp planning using a multi-armed bandit model with correlated
rewards. In: IEEE international conference on robotics and automation (ICRA).
pp. 1957–1964. IEEE (2016) 4

39. Maturana, D., Scherer, S.: Voxnet: A 3d convolutional neural network for real-time
object recognition. In: IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS). pp. 922–928. IEEE (2015) 12

40. Miller, A.T., Allen, P.K.: Graspit! a versatile simulator for robotic grasping. IEEE

Robotics & Automation Magazine 11(4), 110–122 (2004) 2, 4

41. Moon, G., Yong Chang, J., Mu Lee, K.: V2V-posenet: Voxel-to-voxel prediction
network for accurate 3d hand and human pose estimation from a single depth
map. In: Proceedings of the IEEE conference on computer vision and pattern
Recognition. pp. 5079–5088 (2018) 7

42. Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,
Desmaison, A., Antiga, L., Lerer, A.: Automatic diﬀerentiation in PyTorch. In:
NIPS Autodiﬀ Workshop (2017) 12, 19

43. Pavlakos, G., Choutas, V., Ghorbani, N., Bolkart, T., Osman, A.A.A., Tzionas,
D., Black, M.J.: Expressive body capture: 3d hands, face, and body from a single
image. In: Proceedings IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR) (Jun 2019), http://smpl-x.is.tue.mpg.de 4

44. Pham, T.H., Kheddar, A., Qammaz, A., Argyros, A.A.: Towards force sensing
from vision: Observing hand-object interactions to infer manipulation forces. In:
Proceedings of the IEEE conference on computer vision and pattern recognition.
pp. 2810–2819 (2015) 3

45. Pham, T.H., Kyriazis, N., Argyros, A.A., Kheddar, A.: Hand-object contact force
estimation from markerless visual tracking. IEEE Transactions on Pattern Analysis
and Machine Intelligence 40, 2883–2896 (2018) 3, 9

46. Pollard, N.S.: Parallel methods for synthesizing whole-hand grasps from general-
ized prototypes. Tech. rep., MASSACHUSETTS INST OF TECH CAMBRIDGE
ARTIFICIAL INTELLIGENCE LAB (1994) 2

47. Powell, M.J.: A new algorithm for unconstrained optimization. In: Nonlinear pro-

gramming, pp. 31–65. Elsevier (1970) 15

48. Qi, C.R., Yi, L., Su, H., Guibas, L.J.: Pointnet++: Deep hierarchical feature learn-
ing on point sets in a metric space. In: Advances in neural information processing
systems. pp. 5099–5108 (2017) 12, 19

49. Rogez, G., Supancic, J.S., Ramanan, D.: Understanding everyday hands in ac-
tion from rgb-d images. In: Proceedings of the IEEE international conference on
computer vision. pp. 3889–3897 (2015) 4

50. Romero, J., Tzionas, D., Black, M.J.: Embodied hands: Modeling and capturing
hands and bodies together. ACM Transactions on Graphics (TOG) 36(6), 245
(2017) 2, 7, 13, 15, 17

51. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedi-
cal image segmentation. In: International Conference on Medical image computing
and computer-assisted intervention. pp. 234–241. Springer (2015) 11, 19

52. Rosenberg, I.D., Zarraga, J.A.: System for detecting and conﬁrming a touch input.

US Patent US20170336891A1 (2017) 16

ContactPose: A Dataset of Grasps with Object Contact and Hand Pose

25

53. Simon, T., Joo, H., Matthews, I., Sheikh, Y.: Hand keypoint detection in single

images using multiview bootstrapping. In: CVPR (2017) 1, 4, 6, 11

54. Sridhar, S., Mueller, F., Zollh¨ofer, M., Casas, D., Oulasvirta, A., Theobalt, C.:
Real-time joint tracking of a hand manipulating an object from rgb-d input. In:
European Conference on Computer Vision. pp. 294–310. Springer (2016) 4

55. Sundaram, S., Kellnhofer, P., Li, Y., Zhu, J.Y., Torralba, A., Matusik, W.: Learning
the signatures of the human grasp using a scalable tactile glove. Nature 569(7758),
698 (2019) 2, 3, 4

56. SynTouch LLC: BioTac. https://www.syntouchinc.com/robotics/, accessed:

2020-03-05 7

57. Tekin, B., Bogo, F., Pollefeys, M.: H+ o: Uniﬁed egocentric recognition of 3d
hand-object poses and interactions. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition. pp. 4511–4520 (2019) 1, 11

58. Teschner, M., Kimmerle, S., Heidelberger, B., Zachmann, G., Raghupathi, L.,
Fuhrmann, A., Cani, M.P., Faure, F., Magnenat-Thalmann, N., Strasser, W., et al.:
Collision detection for deformable objects. In: Computer graphics forum. vol. 24,
pp. 61–81. Wiley Online Library (2005) 4

59. Tompson, J., Stein, M., Lecun, Y., Perlin, K.: Real-time continuous pose recovery
of human hands using convolutional networks. ACM Transactions on Graphics
(ToG) 33(5), 169 (2014) 1, 6

60. Tremblay, J., To, T., Sundaralingam, B., Xiang, Y., Fox, D., Birchﬁeld, S.: Deep
object pose estimation for semantic robotic grasping of household objects. In: Con-
ference on Robot Learning (CoRL) (2018), https://arxiv.org/abs/1809.10790
11

61. Tzionas, D., Ballan, L., Srikantha, A., Aponte, P., Pollefeys, M., Gall, J.: Cap-
turing hands in action using discriminative salient points and physics simulation.
International Journal of Computer Vision 118(2), 172–193 (2016) 1, 2, 4, 12
62. Wade, J., Bhattacharjee, T., Williams, R.D., Kemp, C.C.: A force and thermal
sensing skin for robots in human environments. Robotics and Autonomous Systems
96, 1–14 (2017) 2

63. Ye, Y., Liu, C.K.: Synthesis of detailed hand manipulations using contact sampling.

ACM Transactions on Graphics (TOG) 31(4), 41 (2012) 4

64. Zhang, R., Isola, P., Efros, A.A.: Colorful image colorization. In: European confer-

ence on computer vision. pp. 649–666. Springer (2016) 11, 12, 13

65. Zhang, X., Li, Q., Mo, H., Zhang, W., Zheng, W.: End-to-end hand mesh recovery
from a monocular rgb image. In: The IEEE International Conference on Computer
Vision (ICCV) (October 2019) 1, 11

66. Zhou, Q.Y., Koltun, V.: Color map optimization for 3d reconstruction with con-
sumer depth cameras. ACM Transactions on Graphics (TOG) 33(4), 1–10 (2014)
6

67. Zhou, Q.Y., Park, J., Koltun, V.: Open3D: A modern library for 3D data process-

ing. arXiv:1801.09847 (2018) 6

68. Zhou, X., Leonardos, S., Hu, X., Daniilidis, K.: 3d shape estimation from 2d land-
marks: A convex relaxation approach. In: proceedings of the IEEE conference on
computer vision and pattern recognition. pp. 4447–4455 (2015) 11

69. Zimmermann, C., Ceylan, D., Yang, J., Russell, B., Argus, M., Brox, T.: Freihand:
A dataset for markerless capture of hand pose and shape from single rgb images. In:
The IEEE International Conference on Computer Vision (ICCV) (October 2019)
4, 11, 17

