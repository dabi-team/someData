8
1
0
2

y
a
M
8
2

]

R
P
.
h
t
a
m

[

1
v
8
4
9
0
1
.
5
0
8
1
:
v
i
X
r
a

Note on AR(1)-characterisation of stationary
processes and model ﬁtting

Marko Voutilainen∗and Lauri Viitasaari† and Pauliina Ilmonen∗

March 22, 2022

Abstract

It was recently proved that any strictly stationary stochastic process can be viewed
as an autoregressive process of order one with coloured noise. Furthermore, it
was proved that, using this characterisation, one can deﬁne closed form estimators
for the model parameter based on autocovariance estimators for several different
lags. However, this estimation procedure may fail in some special cases. In this
article we provide a detailed analysis of these special cases. In particular, we
prove that these cases correspond to degenerate processes.

AMS 2010 Mathematics Subject Classiﬁcation: (Primary) 60G10, (Secondary) 62M10

Keywords: stationary processes, covariance functions

1

Introduction

Stationary processes are important tool in many practical applications of time series
analysis, and the topic is extensively studied in the literature. Traditionally, stationary
processes are modelled by using autoregressive moving average processes or linear
processes (see monographs [2, 4] for details).

One of the most simple example of an autoregressive moving average process is an

autoregressive process of order one. That is, a process (Xt)t∈Z deﬁned by

Xt = φXt−1 + εt,

t ∈ Z,

(1)

where φ ∈ (−1, 1) and (εt)t∈Z is a sequence of independent and identically distributed
square integrable random variables. The continuous time analogue of (1) is called the
Ornstein-Uhlenbeck process, which can be deﬁned as the stationary solution of the
Langevin-type stochastic differential equation

dUt = −φUtdt + dWt,

(2)

∗Department of Mathematics and Systems Analysis, Aalto University School of Science, Finland
†Department of Mathematics and Statistics, University of Helsinki, Finland

1

 
 
 
 
 
 
where φ > 0 and (Wt)t∈R is a two-sided Brownian motion. Such equations have also
applications in mathematical physics.

Statistical inference for AR(1)-process or Ornstein-Uhlenbeck process is well-
established in the literature. Furthermore, recently a generalised continuous time
Langevin equation, where the Brownian motion W in (2) is replaced with a more
general driving force G, have been a subject of active study. Especially, the so-called
fractional Ornstein-Uhlenbeck processes introduced by [3] have been studied exten-
sively. For parameter estimation in such models, we mention a recent monograph [5]
dedicated to the subject, and the references there in.

When the model becomes more complicated, the number of parameters increases
and the estimation may become a challenging task. For example, it may happen that
standard maximum likelihood estimators cannot be expressed in closed form [2]. Even
worse, it may happen that classical estimators such as maximum likelihood or least
squares estimators are biased and not consistent (cf. [1] for discussions on the gener-
alised ARCH-model with fractional Brownian motion driven liquidity). One way to
tackle such problems is to consider one parameter model, and to replace white noise
in (3) with some other stationary noise. It was proved in [7] that each discrete time
strictly stationary process can be characterised by

Xt = φXt−1 + Zt,

(3)

where φ ∈ (0, 1). This representation can be viewed as a discrete time analogue
of the fact that Langevin-type equation characterises strictly stationary processes in
continuous time [6].

The authors in [7] applied characterisation (3) to model ﬁtting and parameter esti-
mation. The presented estimation procedure is straightforward to apply with the excep-
tion of certain special cases. The purpose of this paper is to provide a comprehensive
analysis of these special cases. In particular, we show that such cases do not provide
very useful models. This highlights the wide applicability of characterization (3) and
the corresponding estimation procedure.

The rest of the paper is organised as follows. In Section 2 we brieﬂy discuss the
motivating estimation procedure of [7]. We also present and discuss our main results
together with some illustrative ﬁgures. All the proofs and technical lemmas are post-
poned to Section 3.

2 Motivation and formulation of the main results

Let X = (Xt)t∈Z be a stationary process. It was shown in [7] that equation

Xt = φXt−1 + Zt,

(4)

where φ ∈ (0, 1) and Zt is another stationary process, characterises all discrete time
(strictly) stationary processes. Throughout this paper we suppose that X and Z are

2

square integrable processes with autocovariance functions γ(·) and r(·), respectively.
Using Equation (4), one can derive Yule-Walker type equations for the parameter φ,
which can be solved in an explicit form. Namely, for any m ∈ Z such that γ(m) (cid:54)= 0
we have

φ =

γ(m + 1) + γ(m − 1) ± (cid:112)(γ(m + 1) + γ(m − 1))2 − 4γ(m)(γ(m) − r(m))
2γ(m)

.

(5)
The estimation of the parameter φ is obvious from (5) provided that one can determine
which sign, plus or minus, one should choose. In practice, this can be done by choosing
different lags m for which to estimate the covariance function γ(m). Then one can
determine the correct value φ by comparing different signs in (5) for different lags m
(We refer to [7, p. 387] for detailed discussion). However, this approach fails, i.e. one
cannot ﬁnd suitably chosen lags leading to the correct choice of the sign and only one
value φ, if, for m ∈ Z such that γ(m) = 0 we also have r(m) = 0, and for any m ∈ Z
such that γ(m) (cid:54)= 0, the ratio

am =

r(m)
γ(m)

= a

for some constant a ∈ (0, 1). The latter is equivalent [7, p. 387] to the fact that

γ(m + 1) + γ(m − 1)
γ(m)

= b

for some constant b with φ < b < φ + φ−1. This leads to

γ(m + 1) = bγ(m) − γ(m − 1).

(6)

(7)

(8)

Moreover, if γ(m) = r(m) = 0 for some m, it is straightforward to verify that (8)
holds in this case as well. Thus (8) holds for all m ∈ Z. Since covariance functions
are necessarily symmetric, we obtain an ”initial” condition γ(1) = b
2γ(0). Thus (8)
admits a unique symmetric solution.

From γ(1) = b

2γ(0) it is clear that (8) does not deﬁne covariance function for
b > 2. Furthermore, since φ > 0, it sufﬁces to study the regime b ∈ [0, 2] (we
include the trivial case b = 0). For b = 2 this corresponds to the case Xt = X0 for
all t ∈ Z which is hardly interesting. Similarly, the case b = 0 leads to a process
(. . . , X0, X1, −X0, −X1, X0, X1, . . .) which again does not provide a practical model.
On the other hand, it is not clear whether for some other values b ∈ (0, 2) Equation
(8) can lead to some non-trivial model in which estimation procedure explained above
cannot be applied. It turns out that, for any b ∈ [0, 2], Equation (8) deﬁnes a covariance
function. On the other hand, the resulting covariance function, denoted by γb, leads to
a model that is either not very interesting.

Theorem 2.1. Let b ∈ (0, 2) and γb be the (unique) symmetric function satisfying (8).
Then

3

1. Let b = 2 sin (cid:0) k

(cid:1), where k and l are strictly positive integers such that k

l ∈

l
(0, 1). Then γb(m) is periodic.

2. Let b = 2 sin (cid:0)r π

(cid:1), where r ∈ (0, 1) \ Q. Then for any M ≥ 0, the set {γb(M +

π
2

2

m) : m ≥ 0} is dense in [−γ(0), γ(0)].

3. For any b ∈ [0, 2], γb(·) is a covariance function.

In many applications of stationary processes, it is assumed that the covariance func-
tion γ(·) vanishes at inﬁnity, or that γ(·) is periodic. Note that the latter case corre-
sponds simply to the analysis of ﬁnite-dimensional random vectors with identically
distributed components. Indeed, γ(m) = γ(0) implies Xn = X0 almost surely, so
periodicity of γ(·) with period N implies that there exists at most N random variables
as the source of randomness. By items (2) and (3) of Theorem 2.1, we observe that, for
suitable values of b, (8) can be used to construct covariance functions that are neither
periodic nor vanishing at inﬁnity. On the other hand, in this case there are arbitrary
large lags m such that γb(m) is arbitrary close to γb(0). Consequently, it is expected
that different estimation procedures fail. Indeed, even the standard covariance estima-
tors are not consistent. A consequence of Theorem 2.1 is that only a little structure in
the noise Z is needed in order to apply the estimation procedure of the parameter φ
introduced in [7], provided that one has consistent estimators for the covariances of X.
The following is a precise mathematical formulation of this observation.

Theorem 2.2. Let X be given by (4) for some φ ∈ (0, 1) and noise Z. Assume that
there exists (cid:15) > 0 and M ∈ N such that r(m) ≤ r(0)(1 − (cid:15)) or r(m) ≥ −r(0)(1 − (cid:15))
for all m ≥ M . Then the covariance function γ of X does not satisfy (8) for any
b ∈ [0, 2].

We end this section by visual illustrations of the covariance functions deﬁned by
(8). In these examples we have set γb(0) = 1. In Figures 1 and 2 we have illustrated the
(cid:1) = 1.
case of item (1) of Theorem 2.1. Note that in Figure 1a we have b = 2 sin (cid:0) 1
Figure 2 demonstrates how k can affect the shape of the covariance function. Finally,
Figure 3b illustrates the case of item (2) of Theorem 2.1.

π
2

3

3 Proofs

Throughout this section, without loss of generality, we assume γb(0) = 1. We also drop
the sub-index and simply denote γ. The following ﬁrst result gives explicit formula for
the solution to (8).

Proposition 3.1. The unique symmetric solution to (8) is given by

γ(m) =

(cid:26) (−1) m
(−1)

2 cos (cid:0)m arcsin (cid:0) b

(cid:1)(cid:1),
sin (cid:0)m arcsin (cid:0) b

(m−1)
2

2

(cid:1)(cid:1),

2

for m is even
for m is odd.

(9)

4

(a) k = 1 and l = 3.

(b) k = 5 and l = 7.

Figure 1: Examples of covariance functions corresponding to b = 2 sin (cid:0) k

l

(cid:1).

π
2

Proof. Clearly, γ(m) given by (9) is symmetric, and thus it sufﬁces to consider m ≥ 0.
(cid:1) so that
Moreover γ(0) = 1 and γ(1) = b
sin A = b

2. We use the short notation A = arcsin (cid:0) b

2. Assume ﬁrst m + 2 ≡ 2 (mod 4). Then

2

γ(m + 2) = − cos ((m + 2)A)

= − cos (mA) cos (2A) + sin (mA) sin (2A)
= − cos (mA) (cid:0)1 − 2 sin2 A(cid:1) + 2 sin (mA) sin A cos A
= − cos (mA) (1 − b sin A) + b sin (mA) cos A
= b (cos (mA) sin A + sin (mA) cos A) − cos (mA)
= b sin ((m + 1)A) − cos (mA)
= bγ(m + 1) − γ(m).

Similarly, for m + 2 ≡ 3 (mod 4) we observe

5

020406080100−1.0−0.50.00.51.0LagAutocovariance02004006008001000−1.0−0.50.00.51.0LagAutocovariance(a) k = 1 and l = 3371.

(b) k = 3367 and l = 3371.
Figure 2: Examples of covariance functions corresponding to b = 2 sin (cid:0) k

l

(cid:1).

π
2

γ(m + 2) = − sin ((m + 2)A)

= − sin (mA) cos (2A) − sin (2A) cos (mA)
= − sin (mA)(1 − 2 sin2 A) − 2 sin A cos A cos (mA)
= −b(cos A cos (mA) − sin (mA) sin A) − sin (mA)
= −b cos ((m + 1)A) − sin (mA)
= bγ(m + 1) − γ(m).

Treating cases m + 2 ≡ 0 (mod 4) and m + 2 ≡ 1 (mod 4) similarly, we deduce that
(9) satisﬁes (8).

Remark 3.2. Using (8) directly, we observe, for even m ≥ 1, that

γ(m) = bm +

m−1
(cid:88)

n= m
2

(−1)m−n

(cid:18)(cid:18) n

m − n

(cid:19)

b2n−m +

(cid:18)

n
m − n − 1

(cid:19) b2n−m+2
2

(cid:19)

.

(10)

Similarly, for odd m ≥ 1, we obtain

γ(m) =

m
(cid:88)

n= m+1

2

(−1)m−n

(cid:18) n

(cid:19)

m − n

b2n−m +

m−1
(cid:88)

n= m−1

2

6

(−1)m−n

(cid:18)

n
m − n − 1

(cid:19) b2n−m+2
2

.

(11)

0200040006000800010000−1.0−0.50.00.51.0LagAutocovariance0200040006000800010000−1.0−0.50.00.51.0LagAutocovariance(a) b = 0.6.

(b) b = 1.7.

Figure 3: Examples of covariance functions corresponding to b = 2 sin (cid:0)r π

2

(cid:1).

These formulas are ﬁnite polynomial expansions, in variable b, of the functions pre-
sented in (9) which could have been deduced also by using some well-known trigono-
metric identities.

Before proving our main theorems we need several technical lemmas.

Deﬁnition 3.3. We denote with Q a subset of rationals deﬁned by

Q :=

(cid:26) k
l

: k, l ∈ N,

k
l

∈ (0, 1), k − l ≡ 1

(mod 2)

(cid:27)

Remark 3.4. The modulo condition above means only that either k is even and l is
odd, or vice versa.

Lemma 3.5. Let x = k
l

Proof. We write

π

2 , where k

l ∈ Q. Then

2l−1
(cid:88)

j=1

cos2 (jx)(−1)j = −1.

2l−1
(cid:88)

j=1

cos2 (jx)(−1)j = cos2 (lx)(−1)l +

cos2 (jx)(−1)j +

2l−1
(cid:88)

j=l+1

cos2 (jx)(−1)j.

l−1
(cid:88)

j=1

7

02004006008001000−1.0−0.50.00.51.0LagAutocovariance0200040006000800010000−1.0−0.50.00.51.0LagAutocovarianceChange of variable t = j − l gives

cos2 (jx)(−1)j =

l−1
(cid:88)

cos2 ((t + l)x)(−1)t+l

2l−1
(cid:88)

j=l+1

=

l−1
(cid:88)

t=1

cos2 (tx + k

π
2

)(−1)t+l =

t=1
(cid:40) (cid:80)l−1
(cid:80)l−1

t=1 cos2 (tx)(−1)t+l,
t=1 sin2 (tx)(−1)t+l,

k even

k odd.

Consequently, for even k and odd l we have

2l−1
(cid:88)

j=1

cos2 (jx)(−1)j = − cos2 (cid:16)

k

(cid:17)

π
2

= −1.

Similarly, for odd k and even l,

2l−1
(cid:88)

j=1

cos2 (jx)(−1)j = cos2 (cid:16)

k

(cid:17)

π
2

+

l−1
(cid:88)

j=1

(−1)j = −1.

Lemma 3.6. Let γ(·) be given by (9) with b = 2 sin (cid:0) k
non-zero eigenvalues of the matrix

l

π
2

(cid:1) for some k

l ∈ Q. Then the












C :=

γ(0)
γ(1)
γ(2)
...

γ(1)
γ(0)
γ(1)
...
γ(4l − 2) γ(4l − 3) γ(4l − 4)
γ(4l − 1) γ(4l − 2) γ(4l − 3)

γ(2)
γ(1)
γ(0)
...

· · · γ(4l − 1)
· · · γ(4l − 2)
· · · γ(4l − 3)
. . .
· · ·
· · ·

...
γ(1)
γ(0)












(12)

are either 2l with multiplicity of two or 4l with multiplicity of one.

Proof. Let ci denote the ith column of C. Then, by the deﬁning equation (8), ci =
bci−1 − ci−2 for any i ≥ 3. Consequently, there exists at most two linearly independent
columns. Thus rank(C) ≤ 2, which in turn implies that there exists at most two non-
zero eigenvalues λ1 and λ2. In order to compute λ1 and λ2, we recall the following
identities:

tr(C) = λ1 + λ2 = 4l
tr(C 2) = λ2

1 + λ2

2 = ||C||2
F ,

(13)
(14)

where || · ||F is the Frobenius norm. If rank(C) = 1, then λ2 = 0 implying the second
part of the claim. Suppose then rank(C) = 2. Observing that the squared sum of the

8

diagonals is 4l and, for j = 1, 2, . . . , 4l − 1, a term γ(j) appears in C exactly 2(4l − j)
times, we obtain

||C||2

F = 4l + 2

4l−1
(cid:88)

(4l − j)γ(j)2.

j=1

Dividing the sum into two parts and using sin2(x) = 1 − cos2(x) we have

||C||2

F = 4l + 2

2l−1
(cid:88)

(4l − (2j + 1))γ(2j + 1)2 + 2

j=0

2l−1
(cid:88)

(4l − 2j)γ(2j)2

j=1

= 4l + 2

2l−1
(cid:88)

(4l − (2j + 1)) sin2 ((2j + 1)x) + 2

j=0

2l−1
(cid:88)

(4l − 2j) cos2 (2jx)

j=1

= 4l + 2

2l−1
(cid:88)

(4l − (2j + 1)) + 2

4l−1
(cid:88)

(4l − j) cos2 (jx)(−1)j

j=0

j=1

= 8l2 + 4l + 2

4l−1
(cid:88)

(4l − j) cos2 (jx)(−1)j,

j=1

where in the last equality we have used

2l−1
(cid:88)

(4l − (2j + 1)) =

j=0

Now

2l−1
(cid:88)

(4l − 1) − 2

j=0

2l−1
(cid:88)

j=0

j = 2l(4l − 1) + 2l(2l − 1) = 4l2.

4l−1
(cid:88)

(4l − j) cos2 (jx)(−1)j = 2l +

2l−1
(cid:88)

(4l − j) cos2(jx)(−1)j

j=1

j=1

4l−1
(cid:88)

+

(4l − j) cos2(jx)(−1)j,

j=2l+1

where substitution j = 4l − t yields

4l−1
(cid:88)

(4l − j) cos2(jx)(−1)j =

j=2l+1

=

2l−1
(cid:88)

t=1

t cos2(2kπ − tx)(−1)t =

Now (15), (16), and Lemma 3.5 imply
(cid:32)

||C||2

F = 8l2 + 4l + 2

2l + 4l

2l−1
(cid:88)

t=1

2l−1
(cid:88)

t=1

2l−1
(cid:88)

j=1

t cos2((4l − t)x)(−1)4l−t

t cos2(tx)(−1)t.

(cid:33)

cos2(jx)(−1)j

= 8l2.

9

(15)

(16)

Finally, using (13) and (14) together with ||C||2

F = 8l2, we obtain
√

√

1 + (4l − λ1)2 − 8l2 = 2λ2
λ2

1 − 8lλ1 + 8l2 = (

2λ1 −

8l)2 = 0.

Hence λ1 = λ2 = 2l.

We are now ready to prove Theorem 2.1 and Theorem 2.2.

Proof the Theorem 2.1. Throughout the proof we denote a2 ≡ a1 (mod 2π) if a2 =
a1 + 2kπ for some k ∈ Z. That is, a1 and a2 are identiﬁable when regarding them as
points on the unit circle. By a3 ∈ (a1, a2) (mod 2π) we mean that a3 ≡ a (mod 2π)
for some a ∈ (a1, a2).
1. Since arcsin (cid:0) b

π
2 , the ﬁrst claim follows from Proposition 3.1 together
with the fact that functions sin(·) and cos(·) are periodic. In particular, we have
γ(4l + m) = γ(m) for every m ∈ Z.

(cid:1) = k

2

l

2. Denote A = arcsin (cid:0) b

(cid:1) = r π

2

2 . By Proposition 3.1, mA is the corresponding
angle for γ(m) on the unit circle. Note ﬁrst that, due the periodic nature of cos
and sin functions, it sufﬁces to prove the claim only in the case M = 0. In
what follows, we assume that m ≥ 0. We show that the function γ(m), m ≡ 0
(mod 4) is dense in [−1, 1], while a similar argument could be used for other
equivalence classes as well. That is, we show that the function cos(mA), m ≡ 0
(mod 4) is dense in [−1, 1]. Essentially this follows from the observation that, as
r /∈ Q, the function m (cid:55)→ cos(mA) is injective. Indeed, if cos( ˜mA) = cos(mA)
for some ˜m, m ≥ 0, ˜m (cid:54)= m, it follows that

˜mA = ˜mr

π
2

= ±mr

π
2

+ k2π = ±mA + k2π

for some k ∈ Z.

This implies

r =

4k
˜m ± m

,

which contradicts r /∈ Q. Since cos(mA) is injective, it is intuitively clear
that cos(mA), m ≡ 0 (mod 4) is dense in [−1, 1]. For a precise argument, we
argue by contradiction and assume there exists an interval (c1, d1) ⊂ [−1, 1]
such that cos(mA) /∈ (c1, d1) for any m ≡ 0 (mod 4). This implies that
there exists an interval (c2, d2) ⊂ [0, 2π] such that for every m ≡ 0 (mod 4)
it holds that mA /∈ (c2, d2) (mod 2π). Without loss of generality, we can as-
sume c2 = 0 and that for some m0 ≡ 0 (mod 4) we have m0A ≡ 0 (mod 2π).
Let mn = m0 + 4n with n ∈ N and denote by (cid:98)·(cid:99) the standard ﬂoor function.
Suppose that for some n ∈ N and pn ∈ (−d2, 0) we have mnA ≡ pn (mod 2π).
Since by injectivity 2π
|pn| (cid:99)A ∈ (0, d2) (mod 2π) leading to
a contradiction. This implies that for every n ∈ N we have mnA /∈ (−d2, d2)
(mod 2π) (for a visual illustration, see Figure 4). Similarly, assume next that

|pn| /∈ N, we get mn(cid:98) 2π

10

mn1A ≡ pn1 (mod 2π) and mn1+n2A − mn1A ∈ (−d2, d2) (mod 2π). Then
mn2A ∈ (−d2, d2) (mod 2π) which again leads to a contradiction (see Fig-
ure 5). This means that for an arbitrary point pn on the unit circle such that
mnA ≡ pn (mod 2π), we get an interval (pn − d2, pn + d2) (understood as an
angle on the unit circle) such that this interval cannot be visited later. As the
whole unit circle is covered eventually, we obtain the expected contradiction.

Figure 4: Example of the excluded interval (−d2, d2) around zero. Here n∗ = (cid:98) 2π
|pn| (cid:99),
and we have visualized the points on the unit circle corresponding to the steps
0, n, 2n, (n∗ − 1)n and n∗n.

π
2

3. Consider ﬁrst the case b = 2 sin (cid:0) k

(cid:1), where k

l

l ∈ Q. By Lemma 3.6, the
symmetric matrix C deﬁned by (12) has non-negative eigenvalues, and thus C is
a covariance matrix of some random vector (X0, X1, . . . , X4l−1). Now it sufﬁces
to extend this vector to a process X = (Xt)t∈Z by the relation X4l+t = Xt.
Indeed, it is straightforward to verify that X has the covariance function γ.
Assume next b = 2 sin (cid:0)r π
(cid:1), where r ∈ (0, 1) \ Q. We argue by contradiction
and assume that there exists k ∈ N, and vectors t = (t1, t2, ..., tk)T ∈ Zk and
a = (a1, a2, ..., ak)T ∈ Rk such that

2

11

d2−d2m0AmnAmn∗nAm(n∗−1)nAm2nAFigure 5: Example of two excluded intervals and an angle mn1A.

k
(cid:88)

i,j=1

aiγ(ti − tj)aj = −(cid:15)

for some (cid:15) > 0,

where γ(·) is the covariance function corresponding to the value b. Since Q
is dense in [0, 1], it follows that there exists (qn)n∈N ⊂ Q such that qn → r.
Denote the corresponding sequence of covariance functions with (γn(·))n∈N. By
deﬁnition,

k
(cid:88)

i,j=1

aiγn(ti − tj)aj ≥ 0

for every n.

On the other hand, continuity implies γn(m) → γ(m) for every m. This leads
to

k
(cid:88)

lim
n→∞

aiγn(ti − tj)aj =

aiγ(ti − tj)aj = −(cid:15)

k
(cid:88)

i,j=1

i,j=1

12

d2−d2m0Ad2−d2mn1Amn1+n2Amn2Agiving the expected contradiction.

Remark 3.7. Note that in the periodic case the covariance matrix C deﬁned by (12)
satisﬁes rank(C) ≤ 2. Thus, in this case, the process (Xt)t∈Z is driven linearly by
only two random variables Y1 and Y2. In other words, we have

Xt = a1(t)Y1 + a2(t)Y2,

t ∈ Z

for some deterministic coefﬁcients a1(t) and a2(t).

Proof of Theorem 2.2. Suppose γ satisﬁes (8) and r(m) ≤ r(0)(1 − (cid:15)) for all m ≥ M .
By Theorem 2.1, there exists m∗ ≥ M such that

γ(m∗) ≥ γ(0)

(cid:16)

1 −

(cid:17)

.

(cid:15)
2

Furthermore, (8) implies (6) for every m such that γ(m) (cid:54)= 0. Now

am∗ =

r(m∗)
γ(m∗)

≤

r(0)(1 − (cid:15))
γ(0) (cid:0)1 − (cid:15)

2

(cid:1) <

r(0)
γ(0)

= a0

leading to a contradiction. Treating the case r(m) ≥ −r(0)(1 − (cid:15)) for all m ≥ M
similarly concludes the proof.

References

[1] M. Bahamonde, S. Torres, and C.A. Tudor. ARCH model with fractional Brownian

motion. Statistics and Probability Letters, 134:70–78, 2018.

[2] P.J. Brockwell and R.A. Davis. Time Series: Theory and Methods. Springer

Science & Business Media, 2013.

[3] P. Cheridito, H. Kawaguchi, and M. Maejima. Fractional Ornstein-Uhlenbeck
processes. Electronic Journal of Probability, 8:no. 3, 14 pp. (electronic), 2003.

[4] J.D. Hamilton. Time Series Analysis, volume 2. Princeton university press Prince-

ton, 1994.

[5] K. Kubilius, Y. Mishura, and K. Ralchenko. Parameter Estimation in Fractional

Diffusion Models. Springer, 2018.

[6] L. Viitasaari. Representation of stationary and stationary increment processes via
Langevin equation and self-similar processes. Statistics and Probability Letters,
115:45–53, 2016.

[7] M. Voutilainen, L. Viitasaari, and P. Ilmonen. On model ﬁtting and estimation
of strictly stationary processes. Modern Stochastics: Theory and Applications,
4(4):381–406, 2017.

13

