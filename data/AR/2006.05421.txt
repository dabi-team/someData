0
2
0
2

n
u
J

9

]

G
L
.
s
c
[

1
v
1
2
4
5
0
.
6
0
0
2
:
v
i
X
r
a

Conditional Sig-Wasserstein GANs for Time Series
Generation

Hao Ni
Department of Mathematics
University College London
London, UK
h.ni@ucl.ac.uk

Lukasz Szpruch
Department of Mathematics
University of Edinburgh
Edinburgh, UK
lszpruch@turing.ac.uk

Magnus Wiese
Department of Mathematics
University of Kaiserslautern
Kaiserslautern, Germany
wiese@rhrk.uni-kl.de

Shujian Liao
Department of Mathematics
University College London
London, UK
shujian.liao.18@ucl.ac.uk

Baoren Xiao
Department of Mathematics
University College London
London, UK
baoren.xiao.18@ucl.ac.uk

Abstract

Generative adversarial networks (GANs) have been extremely successful in gener-
ating samples, from seemingly high dimensional probability measures. However,
these methods struggle to capture the temporal dependence of joint probability dis-
tributions induced by time-series data. Furthermore, long time-series data streams
hugely increase the dimension of the target space, which may render generative
modelling infeasible. To overcome these challenges, we integrate GANs with
mathematically principled and efﬁcient path feature extraction called the signature
of a path. The signature of a path is a graded sequence of statistics that provides a
universal description for a stream of data, and its expected value characterises the
law of the time-series model. In particular, we a develop new metric, (conditional)
Sig-W1, that captures the (conditional) joint law of time series models, and use it
as a discriminator. The signature feature space enables the explicit representation
of the proposed discriminators which alleviates the need for expensive training.
Furthermore, we develop a novel generator, called the conditional AR-FNN, which
is designed to capture the temporal dependence of time series and can be efﬁciently
trained. We validate our method on both synthetic and empirical dataset and ob-
serve that our method consistently and signiﬁcantly outperforms state-of-the-art
benchmarks with respect to measures of similarity and predictive ability.

1

Introduction

Time-series data streams are often not stationary, which means that one may only have a relative
small sample sets of data with the same joint probability distribution. In addition there are only a
few data points far out in the tails of the probability distribution (e.g ﬁnancial crash, pandemic),
which makes robust training of data hungry algorithms infeasible. It is therefore critical to research
methods for synthesising time-series datasets that exhibit the same properties as the real data. Such
synthetic datasets can facilitate testing and validation of data-driven products and enable data sharing
by respecting the demand for privacy constrains. See [1, 2] for the overview of the applications and
challenges for synthetic data generation.

Preprint. Under review.

 
 
 
 
 
 
1.1 Generative model for time series

Let ν be an unknown target distribution. Consider a sequence of N data points x1:N in X ⊆ Rd from
ν. The aim of the generative model is to map samples from some basic distribution µz supported
on Z ⊆ Rdz into samples from ν. More precisely, given latent (Z, B(Z)) and target (X , B(X ))
measure spaces one considers a map G : Θ(g) × Z → X , with Θ(g) being a parameter space that,
given parameters θ ∈ Θ(g), transports µz into Gθ
#µz = µz((Gθ)−1(B)), B ∈ B(X ). The aim is to
ﬁnd θ such that Gθ
#µz is a good approximation of ν with respect to a suitable metric. The optimal
transport metrics, such as the Wasserstein distance, are a popular choice due to the ability to capture
meaningful geometric features between measures even when their supports do not overlap but are
expensive to compute. Maximum Mean Discrepancy (MMD) metrics are much cheaper to compute
but their utility hinges on the choice of the kernel [3].

For time-series generation, learning the conditional distribution is often more desirable than learning
joint law. Indeed for predictive modelling, one is interested in conditional distribution ν(·|xpast)
of the future time series xfuture := xt+1:t+q given the past time series xpast := xt−p+1:t. Learning
such conditional distributions is particularly important in the case when a series x1:N comes from
a non-stationary distribution and learning conditional laws is more efﬁcient and requires less data
[4]. It has been observed in [5, 6] that unconditional GANs are not ﬁt to generate the conditional
distribution of the future time series based on the past time-series information. However, the challenge
in developing conditional integrated probability metrics is that conditional expectations are difﬁcult
to compute and standard statistical approximation methods do not scale well with the dimension.
Additional difﬁculty in generating time-series data is the dimension of the problem. Indeed joint
distributions are supported on RN ×d. Hence, for long time-series data streams this hugely increases
the dimension of the problem at hand making synthetic time-series data generator highly non-trivial.

1.2 Our contributions

In this work, we combine classical GANs with an efﬁcient path feature extraction method called the
signature of a path. The signature of a path is a mathematical object that emerges from rough-path
theory and provides a highly abstract and universal description of complex multimodal data streams
that has recently demonstrated great success in several machine learning tasks [7, 8, 9]. Furthermore,
by working with the signature of a path one can design algorithms that are robust with respect to
missing data points or datasets that are not uniformly distributed across the time dimension [10].
Importantly, the signature preserves the order of data and captures relevant structural/geometric
properties. In this paper, we propose a generic SigCWGAN framework for training conditional
generators. More precisely:

• Discriminator: We develop a novel (conditional) Sig-W1 metric for distributions on the
path space. By taking advantage of the signature feature space, Sig-W1 has an explicit
solution and does not require costly computation needed for the classical Wasserstein metric
[11]. The ability of Sig-W1 metric to capture the (conditional) distribution over the path
space means that the generator trained with respect to this metric is generic in a sense that it
will generalise well with respect to other metrics used in time-series analysis, such as the
auto-correlation function (ACF), which we conﬁrm empirically in section 5. This stands in
contrast to the majority of the results in literature that evaluate the generators with respect to
a speciﬁc criteria e.g. classiﬁcation. For the conditional variant of C-Sig-W1, we leverage
the universality of the signature representation and reduce the computation of conditional
expectations to a simple linear regression problem yielding an explicit solution to C-Sig-W1.
• Generator: We build an AR-FNN that maps the past time-series and noise vectors into
the future time-series. Our generators by construction capture the auto-regressive nature of
time-series.

1.3 Literature Review

The literature on the generative models is in abundance and here we focus on works who speciﬁcally
tackled time-series data generation. Inspired by the recent success of generative adversarial networks
(GANs), [12] present Quant GANs, consisting of a generator and discriminator which utilise temporal
convolutional networks (TCNs). Quant GANs can well capture the temporal dependence of time

2

series, such as volatility clustering; Kondratyev et al.[13] used Restricted Boltzmann Machine (RBM)
to build a generator of synthetic market data and reported good performance in particular due to
high regularisation. A variant of Conditional GANs has been proposed in [14] to generate order
ﬂow in the limit order book. Conditional GANs have also been considered in [15, 16, 17, 18] for
univariate and in [19, 20, 21] for multivariate time-series generation. Henry-Labordere in [22, 23]
proposed several algorithms for data generation using dual and primal-dual representations for optimal
transport metrics. His algorithms achieved good performance for 6-dimensional time-series data. The
combination of signatures, genetic algorithms and autoencoders has been developed in [24]. The
importance of including the correct prior when training GANs for time series data has been suggested
in [5]. There authors develop a novel Time-series GAN algorithm that signiﬁcantly outperforms
classical GAN.

1.4 Outline of the paper.

In section 2, we introduce the signature of a path and review its properties. Then we proceed to
propose the Sig-W1 metric on measures on the path space by combining W1 metric and the signature
feature map in section 3. In section 4, we propose a novel signature-based framework for time-series
generation, which uses the AR-FNN network as a conditional generator and the conditional Sig-W1
metric as a discriminator. Finally, we present our numerical results in section 5 and outline the
direction of our future work in section 6.

2 The Signature and Expected Signature of Data Streams

In this section, we give a brief introduction to the signature feature of a path and expected signature
of a stochastic process, which can characterise the law on the stochastic process and can be thought
of a moment generating function on the path space. The rigorous and thorough introduction of the
signature feature set for un-parameterized paths can be found in [25, 26, 27]. For simplicity, we
restrict the discussion to the space of continuous functions mapping from a compact time interval J
to Rd with ﬁnite p-variation and starting from the origin, denoted by Cp

0 (J, Rd).

2.1 The Signature of a Path

k=0(Rd)⊗k be a tensor algebra space (c.f supplementary material A), where the

Let T ((Rd)) := ⊕∞
signature of a Rd-valued path takes value.
Deﬁnition 2.1 (Signature). Let X ∈ Cp
signature of the path X is deﬁned as S(XJ ) = (1, X 1

0 (J, Rd) such that the following integration makes sense. The

J , · · · , X k

J , · · · ) ∈ T ((Rd)), where

X k

J =

(cid:90)

t1<t2<···<tk,t1,··· ,tk∈J

dXt1 ⊗ · · · dXtk .

(1)

Let SM (XJ ) denote the truncated signature of X of degree M , i.e. SM (XJ ) = (1, X 1

J , ..., X M

J ).

For p ∈ [1, 2), the integration is deﬁned as the Young’s integral. When X is a piecewise linear path
of ﬁnite pieces, the signature of X can be computed explicitly and we use the open source python
package signatory [28], which supports PyTorch [29], to do it.

The signature of a path provides a top-down description of the path; low order terms of the signature
capture the global description of the path and higher order term gives more information on the local
structure of the path. Let J = [s, t]. The ﬁrst level of the signature X 1
J is the increment of the path
Xt − Xs, and the second level of the signature X 1
J includes the information of the area enclosed by
X and the chord connecting the end and start points of X. X k
J has the dimension dk, and thus the
dimension of the truncated signature ofSM (XJ ) is dM +1−1

.

d−1

0(J, Rd) denote the space of time augmented paths, i.e. Ω1

Let Ω1
0 (J, Rd)}. For any discrete time series X := (Xti)L
C1
time dimension and embed it to ¯X ∈ Ω1

0(J, Rd) = {t (cid:55)→ (t, xt)|x ∈
i=1, we follow Deﬁnition 4.3 [27] to augment

0(J, Rd).

The following properties make the signature an excellent candidate for the feature of data streams:

3

• Universality. Non-linear continuous functions of the un-parameterized data streams are
universally approximated by linear functionals in the signature space [27]. More precisely,
we have the following result:
0(J, Rd). Denote by S the function that
Theorem 2.1. Consider a compact set K ⊂ Ω1
maps a path X from K to its signature S(X). Let f : K → R be any continuous function.
Then, for any (cid:15) > 0, there exists M > 0, and a linear functional L acting on the truncated
signature of degree M such that

sup
X∈K

|f (X) − (cid:104)L, SM (X)(cid:105)| < (cid:15)

(2)

The theorem tells us that any continuous functional on the paths can be arbitrarily well
approximated by a linear combination of coordinate signatures.

• Uniqueness. The signature of a path determines the path up to time parameterization[30, 31].
0(J, Rd), the signature map is bijective.

More speciﬁcally, when restricted the path space to Ω1
In other words, the signature of a path in Ω1

0(J, Rd) determines the path completely.

Moreover, the signature of a path takes the functional view on the time-series data by lifting discrete
time series to a continuous path by interpolation. Therefore, it allows the uniﬁed treatment on
irregular time series (e.g. variable length, missing data, asynchronous multi-dimensional data) to the
path space [26].

2.2 Expected Signature of a Stochastic Process

Let X denote a stochastic process deﬁned on a probability space. Suppose that Equation (1) is well
deﬁned for X a.s., and thus the signature of X is a random variable in T ((Rd)). Assume that the
expectation of S(X) is ﬁnite, we deﬁne the expected signature of the stochastic process X. By
Proposition 6.1 in [32], we have the following result:
Theorem 2.2. Let X and Y be two Ω1
and E[S(X)] has inﬁnite radius of convergence, then X = Y in the distribution sense.

0(J, Rd)-valued random variables. If E[S(X)] = E[S(Y )],

In other words, under the regularity condition, the distribution µ on the path space is characterized by
EX∼µ[S(X)]. Intuitively, the signature of a path plays a role of a non-communicate polynomial on
the path space. Therefore the expected signature of a random process can be viewed as an analogy
of the moment generating function of a d-dimensional random variable. For example, the expected
Stratonovich signature of Brownian motion determines the law of the Brownian motion [33].

3 The Signature Wasserstein-1 metric (Sig-W1) and Sig-MMD

We propose a new Signature Wasserstein-1 (Sig-W1) metric on the measures on the path space
0(J, Rd)1 by combining the signature feature and the Wasserstein-1 (W1) metric. Let µ and ν
X = Ω1
be two measures on the path space X with a compact support K. The Kantorovich and Rubinstein
dual representation of Wasserstein-1 metric is given by

W1(µ, ν) = sup

(cid:26)(cid:90)

f (x)d(µ − ν)(x)}| continuous f : X → R, Lip(f ) ≤ 1

(cid:27)

,

(3)

where Lip(f ) denotes the Lipschitz constant of f . From this deﬁnition and the universality of the
signature map stated in Theorem 2.1, it is natural to embed the path to the signature space and
consider the distance between µ and ν by

Sig-W1(µ, ν) :=

sup
|L|≤1,L is a linear functional

L (Eµ[S(X)] − Eν[S(X)]) ,

(4)

where Eµ and Eν are the expectation taken under µ and ν respectively. Hence, we see that by working
with the signature feature space one can reduce the nonlinear optimisation of computing W1 distance
over the class of Lipschitz functionals to the linear one over the linear functionals on the signature

1We focus on the path space Ω1

0(J, Rd), as any discrete time series can be embedded to this path space.

4

space. Due to the factorial decay of the signature (Lemma A.1), we can approximate Equation (4) by
using the truncated signature up to a ﬁnite degree M , i.e.

Sig-W (M )

1

(µ, ν) :=

sup
|L|≤1,L is a linear functional

L (Eµ[SM (X)] − Eν[SM (X)]) ,

(5)

When the norm of L is chosen as the l2 norm of the linear coefﬁcients of L, this reduced optimization
problem admits the analytic solution

Sig-W (M )

(6)
where |.| is l2 norm (see Lemma A.3). We call (6) the truncated Sig-W1 metric of degree M . In [34],
if one chooses the truncated signature up to degree M as the feature map, then the corresponding
Maximum Mean Discrepancy (Sig-MMD) is the square of Sig-W (M )

(µ, ν) = |Eµ[SM (X)] − Eν[SM (X)]|,

(µ, ν).

1

1

4 The Signature-based Conditional Generator (SigCWGAN)

Motivated by the autoregressive type models in time series literature, we assume that a Rd-valued time
t=1 satisﬁes Xt+1 = f (Xt−p+1:t) + εt, where E[εt+1|Ft] = 0, Ft is the information up
series (Xt)T
to time t and f : Rp×d → Rd is a continuous but unknown function. The objective of the Signature-
based Conditional Generator for time series (SigCWGAN) is to generate the joint distribution of
the future time series xfuture = Xt:t+q given the past time series xpast = Xt−p+1:t. The following
analysis holds for any method of embedding a discrete time series to a path as long as its signature
can uniquely determine the path; for example, we can embed any discrete time series x as a time
augmented path in Ω1

0(J, Rd) deﬁned before.

4.1 The Conditional AR-FNN Generator

The conditional generator Gθ : Rd×p × Z → Rd, which aims to take the past path (Xt−p+1:t = x)
and the noise vector Zt+1 to generate a random variable in Rd whose conditional distribution of the
next step forecast is as close as possible to P(Xt+1|Xt−p+1:t = x). Here Z = Rd and (Zt)t are iid
with the standard normal distribution. Here the AR-FNN generator Gθ is chosen to be a feedforward
neural network, residual connections [35] and parametric ReLUs as activation functions [36] (see
subsection A.4 for a detailed description). Given Xt−p+1:t, we estimate the next step estimator
ˆX (t)
t+1 to generate the step-2
estimator by ˆX (t)
t+1, Zt+2). To predict the i-step estimator at each time t, we
update the conditioning variable by inserting ˆX (t)
t+i−1 to the end of the previous conditioning variable
in a sliding window fashion. By repeating this procedure for q steps, we obtain the step-q estimator
ˆX (t)

t+1 = Gθ(Xt−p+1:t, Zt+1). Then we use the next step estimator ˆX (t)

t+1:t+q (See Algorithm 1 in Supplementary Material).

t+2 = Gθ(Xt−p+2:t, ˆX (t)

The proposed AR-FNN generator is designed to capture the autoregressive structure of the target
time series by construction. We use the lagged p values as the conditioning variable for the AR-FNN
generator and the simple forward neural network to achieve impressive resemblance of temporal
dependence to real time series.

4.2 The Conditional Sig-W1 Discriminator

Given two conditional distributions of µ(Xfuture|xpast) and ν(Xfuture|xpast), we aim to test whether
they are the same in the sense that when xpast = x is ﬁxed and quantify the distance between them.
Similarly to the unconditional case, we deﬁne the truncated conditional Signature Wasserstein-1
metric of degree M denoted by C-Sig-W (M )
on µ and ν as the l2 norm of the difference between
two conditional expected signatures up to degree M , i.e.

1

C-Sig-W (M )

1

(µ, ν) := |Eµ[SM (Xfuture)|xpast = x] − Eν[SM (Xfuture)|xpast = x]|.

Therefore, we deﬁne the loss function as the summation of the l2 norm of the error between the
conditional expected signature of future true path and future path generated by the generator given
the past path over each time t, i.e.

L(θ) =

(cid:88)

t

|Eµ[SM (Xt+1:t+q)|Xt−p+1:t] − Eν[SM ( ˆX (t)

t+1:t+q)|Xt−p+1:t]|,

(7)

5

where ν and µ denote the conditional distribution induced by the real data and synthetic generator
respectively, Gθ is the generator, ˆX (t)

t+1:t+q is the q-step forecast generated by Gθ.

4.3 SigCWGAN Algorithm

We are ready to present the SigCWGAN algorithm based on the proposed conditional genera-
tor/discriminator. The ﬂowchart of SigCWGAN algorithm is given in Figure 1.

Figure 1: The illustration of the ﬂowchart of SigCWGAN.

First, we estimate Eν[SM (Xt:t+q)|Xt−p+1:t] accurately from the true data. Based on the autoregres-
sive assumption of X, the conditional expected signature of Xt+1:t+q given xt−p+1:t = x does not
depend on t, which allows us to use the supervised learning algorithm to learn from true data.

By embedding the past path in the signature space, [27] shows that this non-linear supervised learning
problem can be reduced to a linear regression on the truncated signature of the past path. Thus we
apply the linear regression on (SN (Xt−p+1:t), SM (Xt+1:t+q))t and obtain the estimator of the linear
functional ˆL. We use ˆL(SN (Xt−p+1:t)) as an estimator for Eν[SM (Xt+1:t+q)|Xt−p+1:t].
Given Xt−p+1:t, we sample the noise from the distribution of the latent process Zt+1:t+q and
generate a trajectory ˆX (t)
t+1:t+q by Gθ. By Monte-Carlo method we can get the reliable estimator
for Eµ[S( ˆX (t)
t+1:t+q)|Xt−p+1:t]. We compute the loss function based on Equation (7) and update the
model parameters of the generator Gθ by the stochastic gradient descent algorithm. The pseudocode
of SigCWGAN is listed in Algorithm 2 in Supplementary Material.

5 Numerical Experiments

To benchmark with SigCWGAN, we choose three representative generative models for the time-series
generation, i.e. (1) TimeGAN [5], (2) RCGAN [37] - a conditional GAN and (3) GMMN [38] -
an unconditional MMD with Gaussian kernel. To assess the goodness of the ﬁtting of a generative
model, we consider three main criteria (a) the marginal distribution of time series; (b) the temporal
and feature dependence; (c) the usefulness[5] - synthetic data should be as useful as the real data
when used for the same predictive purposes (i.e. train-on-synthetic, test-on-real). The test metrics are
deﬁned below.

Metric on marginal distribution: We compute the empirical probability density function (epdf) of
the real time series and synthetic data using the histogram. We take the absolute difference of those
two epdfs as the metric on marginal distribution averaged over feature dimension.

Metric on dependency : We use the absolute error of the auto-correlation estimator by real data and
synthetic data as the metric to assess the temporal dependency. For d > 1, we use the l1 norm of the
difference between cross correlation matrices.
R2 comparison: Following [17] and [5], we consider the problem of predicting next-step temporal
vectors using the lagged values of time series using the real data and synthetic data. First we train a
supervised learning model on real data and evaluate the trained model on the real data in terms of
R2(TRTR). Then we train the same supervised learning model on synthetic data and compute R2 of
the trained model on the true data (TSTR). The closer two R2 are, the better generative model it is.

The Supplementary Material contains the precise deﬁnitions of test metrics, additional information on
implementation details of SigCWGAN and numerical results of VAR(1) data, ARCH(1) data and

6

empirical data. Implementation of SigCWGAN can be found in https://github.com/SigCGANs/
Conditional-Sig-Wasserstein-GANs.

5.1 Synthetic data generated by Vector Autoregressive Model

To demonstrate the model’s ability to generate realistic multi-dimensional time series in a controlled
environment, we consider synthetic data generated by the Vector Autoregressive (VAR) model, which
is a key illustrative example in TimeGAN [5]. In the d-dimensional VAR(1) model time series
are deﬁned recursively for t ∈ {1, . . . , T − 1} through Xt+1 = φXt + (cid:15)t+1, where ((cid:15)t)T
t=1 are iid
Gaussian-distributed random variables with co-variance matrix σ1 + (1 − σ)I; I is a d × d identity
matrix. Here, the coefﬁcient φ ∈ [−1, 1] controls the auto-correlation of the time series and σ ∈ [0, 1]
the correlation of the d features.

In our benchmark, we investigate the dimensions d = 1, 2, 3 and various (σ, φ). Across all dimensions
we observe that the SigCWGAN has a comparable performance or outperforms the baseline models
in terms of the metrics deﬁned above. Furthermore, we ﬁnd that as the dimension increases the
performance of SigCWGANs exceeds baselines. We illustrate this ﬁnding in Figure 2(Right) which
shows the relative error of TSTR R2 when varying the dimensionality of VAR(1). Observe that the
SigCWGAN remains a very low relative error, but the performance of the other models deteriorate
signiﬁcantly, especially the GMMN.

Figure 2: (Left) The distributional metric (abs_metrics) comparison; (Right) the R2 (TSTR) compari-
son. VAR(1) data is generated for φ = 0.8 and σ = 0.8.

Figure 3: (Upper panel) Evolution of the training loss functions. (Lower panel) Evolution of the ACF
scores. Each colour represents the ACF score of one dimension. Results are for the 3-dimensional
VAR(1) model for φ = 0.8 and σ = 0.8.

Figure 3 shows the development of the ACF scores through the course of training for the 3-dimensional
VAR(1) model. While the ACF scores of the baseline models oscillate heavily, the SigCWGAN

7

02004006008001000# of generator steps0.250.500.751.001.251.501.75ACF scoreSigCWGANSig-W1 loss02004006008001000# of generator steps0.60.81.01.21.41.61.82.0ACF scoreTimeGANG lossD loss02004006008001000# of generator steps0.60.81.01.21.41.61.8ACF scoreRCGANG lossD loss02004006008001000# of generator steps0.020.030.040.050.060.070.080.090.10ACF scoreGMMNMMD02004006008001000# of generator steps0.000.050.100.150.200.25ACF scoreSigCWGAN02004006008001000# of generator steps0.000.050.100.150.200.25ACF scoreTimeGAN02004006008001000# of generator steps0.000.050.100.150.200.25ACF scoreRCGAN02004006008001000# of generator steps0.000.050.100.150.200.25ACF scoreGMMNACF score and Sig-W1 distance converge nicely towards zero. Also, the MMD loss converges nicely
towards zero. However, in contrast the ACF scores do not converge. This highlights the stability and
usefulness of the Sig-W1 distance as a loss function.

Furthermore, the SigCWGAN has the advantage of generating the realistic long time series over the
other models, which is reﬂected by that the marginal density function of a synthetic sampled path of
80,000 steps is much closer to that of real data than baselines in Figure 4.

Figure 4: Comparison of the marginal distributions of one long sampled path (80,000 steps) with the
real distribution.

5.2 Empirical Data

To assess the performance of our method on the empirical data, we choose the dataset of the S&P
500 index (SPX) and Dow Jones index (DJI) and their realized volatility, which is retrieved from the
Oxford-Man Institute’s "realised library"[39]. We aim to generate a time series of both the log return
of the close prices and the log of median realised volatility of (a) the SPX only; (b) the SPX and
DJI. Table 1 shows that SigCWGAN achieves the superior or comparable performance to the other
baselines. The SigCWGAN generates the realistic synthetic data of the SPX and DJI data shown by
the marginal distribution comparison with that of real data in Figure 5. For the SPX only data, GMMN
performs slightly better than our model in terms of the ﬁtting of lag-1 auto-correlation and marginal
distribution (≤ 0.0013), but it suffers from the poor predictive performance and feature correlation in
Table 1. When the SigCWGAN is outperformed, the difference is negligible. Furthermore, the test
metrics, i.e. the ACF loss and density metric, of our model are evolving much smoother than the test
metrics of the other baseline models shown in Figure 13.

Figure 5: Comparison of the marginal distributions of the generated SigCWGAN paths and the SPX
and DJI data.

Metrics

marginal distribution

auto-correlation

correlation

SigCWGAN

0.01730, 0.01674

0.01342, 0.01192

0.01079, 0.07435

TimeGAN

0.02155, 0.02127

0.05792, 0.03035

0.12363, 0.61488

RCGAN

0.02094, 0.01655

0.03362, 0.04075

0.04606, 0.15353

R2(%)

2.996, 7.948

5.955, 8.586

2.788, 7.190

Sig-W1
0.18448, 4.36744

0.58541, 5.99482

0.47107, 5.43254

0.01608, 0.02387

GMMN
Table 1: Numerical results of the stock datasets. In each cell, the left/right number are the result for
the SPX data/ the SPX and DJI data respectively. We use the relative error of TSTR R2 against TRTR
R2 as the R2 metric.

0.59073, 6.23777

0.04651, 0.22380

0,01283, 0.02676

9.049, 7.384

8

420240.000.050.100.150.200.250.300.350.40pdfSigCWGANHistoricalGenerated420240.00.10.20.30.40.50.60.70.8pdfTimeGANHistoricalGenerated420240.00.20.40.60.8pdfRCGANHistoricalGenerated420240.00.10.20.30.40.50.6pdfGMMNHistoricalGenerated420240.00.10.20.30.40.50.60.7pdfSPX log-returnHistoricalGenerated20240.00.10.20.30.40.5pdfSPX log-volHistoricalGenerated64202460.00.10.20.30.40.50.60.7pdfDJI log-returnHistoricalGenerated20240.00.10.20.30.4pdfDJI log-volHistoricalGenerated6 Conclusion

In this paper, we developed the conditional Sig-Wasserstein GAN for time series generation based
on the explicit approximation of W1 metric using the signature features space. This eliminates the
problem of having to approximate a costly critic / discriminator and, as a consequence, dramatically
simpliﬁes training. Our method achieves state-of-the-art results on both synthetic and empirical
dataset.

Broader Impact

We have introduced a new tool for generating time series. As with any tool, it may be used in
both positive and negative ways. The authors have a particular interest in applying the developed
methodology for training, testing and validating machine learning algorithms and also for enabling
data sharing of priority data with a wider research community.

Acknowledgments and Disclosure of Funding

HN is supported by the EPSRC under the program grant EP/S026347/1. HN and LS are supported by
the Alan Turing Institute under the EPSRC grant EP/N510129/1.

References

[1] Samuel Assefa, Danial Dervovic, Mahmoud Mahfouz, Tucker Balch, Prashant Reddy, and

Manuela Veloso. Generating synthetic data in ﬁnance: opportunities, challenges and pitfalls.

[2] Steven M Bellovin, Preetam K Dutta, and Nathan Reitinger. Privacy and synthetic datasets.

Stan. Tech. L. Rev., 22:1, 2019.

[3] Aude Genevay, Lénaic Chizat, Francis Bach, Marco Cuturi, and Gabriel Peyré. Sample

complexity of sinkhorn divergences. arXiv preprint arXiv:1810.02733, 2018.

[4] Andrew Y Ng and Michael I Jordan. On discriminative vs. generative classiﬁers: A comparison
of logistic regression and naive bayes. In Advances in neural information processing systems,
pages 841–848, 2002.

[5] Jinsung Yoon, Daniel Jarrett, and Mihaela van der Schaar. Time-series generative adversarial
networks. In Advances in Neural Information Processing Systems, pages 5509–5519, 2019.

[6] Yong Ren, Jun Zhu, Jialian Li, and Yucen Luo. Conditional generative moment-matching
networks. In Advances in Neural Information Processing Systems, pages 2928–2936, 2016.

[7] Zecheng Xie, Zenghui Sun, Lianwen Jin, Hao Ni, and Terry Lyons. Learning spatial-semantic
context with fully convolutional recurrent network for online handwritten chinese text recog-
nition. IEEE transactions on pattern analysis and machine intelligence, 40(8):1903–1917,
2017.

[8] Weixin Yang, Terry Lyons, Hao Ni, Cordelia Schmid, Lianwen Jin, and Jiawei Chang.
Leveraging the path signature for skeleton-based human action recognition. arXiv preprint
arXiv:1707.03993, 2017.

[9] Patrick Kidger, Patric Bonnier, Imanol Perez Arribas, Cristopher Salvi, and Terry Lyons. Deep
signature transforms. In Advances in Neural Information Processing Systems, pages 3099–3109,
2019.

[10] Patrick Kidger, James Morrill, James Foster, and Terry Lyons. Neural controlled differential

equations for irregular time series. arXiv preprint arXiv:2005.08926, 2020.

[11] Gabriel Peyré, Marco Cuturi, et al. Computational optimal transport. Foundations and Trends R(cid:13)

in Machine Learning, 11(5-6):355–607, 2019.

[12] Magnus Wiese, Robert Knobloch, Ralf Korn, and Peter Kretschmer. Quant gans: deep genera-

tion of ﬁnancial time series. Quantitative Finance, pages 1–22, 2020.

[13] Alexei Kondratyev and Christian Schwarz. The market generator. Available at SSRN, 2019.

9

[14] Junyi Li, Xintong Wang, Yaoyang Lin, Arunesh Sinha, and Michael P Wellman. Generating

realistic stock market order streams. 2018.

[15] Chris Donahue, Julian J. McAuley, and Miller Puckette. Adversarial audio synthesis. In ICLR,

2019.

[16] Jesse Engel, Kumar Krishna Agrawal, Shuo Chen, Ishaan Gulrajani, Chris Donahue, and Adam
Roberts. Gansynth: Adversarial neural audio synthesis. ArXiv, abs/1902.08710, 2019.

[17] Cristóbal Esteban, Stephanie L. Hyland, and Gunnar Rätsch. Real-valued (medical) time series

generation with recurrent conditional gans, 2017.

[18] Adriano Koshiyama, Nick Firoozye, and Philip Treleaven. Generative adversarial networks
for ﬁnancial trading strategies ﬁne-tuning and combination. arXiv preprint arXiv:1901.01751,
2019.

[19] Rao Fu, Jie Chen, Shutian Zeng, Yiping Zhuang, and Agus Sudjianto. Time series simulation

by conditional generative adversarial net. arXiv preprint arXiv:1904.11419, 2019.

[20] Alireza Koochali, Andreas Dengel, and Sheraz Ahmed. If you like it, gan it. probabilistic

multivariate times series forecast with gan. arXiv preprint arXiv:2005.01181, 2020.

[21] Magnus Wiese, Lianjun Bai, Ben Wood, and Hans Buehler. Deep hedging: Learning to simulate

equity option markets. SSRN Electronic Journal, 2019.

[22] Pierre Henry-Labordere. (martingale) optimal transport and anomaly detection with neural

networks: A primal-dual algorithm. Available at SSRN 3370910, 2019.

[23] Pierre Henry-Labordere. Generative models for ﬁnancial data. Available at SSRN 3408007,

2019.

[24] I. Perez Arribaz T. Lyons . Buehler, B. Hovarth and B. Wood. A data-driven market simulator

for small data environments. in preperation, 2020.

[25] Terry Lyons. Rough paths, signatures and the modelling of functions on streams. arXiv preprint

arXiv:1405.4537, 2014.

[26] Ilya Chevyrev and Andrey Kormilitzin. A primer on the signature method in machine learning.

arXiv preprint arXiv:1603.03788, 2016.

[27] Daniel Levin, Terry Lyons, and Hao Ni. Learning from the past, predicting the statistics for the

future, learning an evolving system. arXiv preprint arXiv:1309.0260, 2013.

[28] Patrick Kidger and Terry Lyons. Signatory: differentiable computations of the signature and

logsignature transforms, on both CPU and GPU. arXiv:2001.00706, 2020.

[29] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative
style, high-performance deep learning library. In Advances in Neural Information Processing
Systems, pages 8024–8035, 2019.

[30] B.M. Hambly and Terry Lyons. Uniqueness for the signature of a path of bounded variation and

the reduced path group. Annals of Mathematics,, 171(1):109–167, 2010.

[31] Horatio Boedihardjo and Xi Geng. The uniqueness of signature problem in the non-markov

setting. arXiv preprint arXiv:1401.6165, 2014.

[32] Ilya Chevyrev, Terry Lyons, et al. Characteristic functions of measures on geometric rough

paths. The Annals of Probability, 44(6):4049–4082, 2016.

[33] Terry Lyons, Hao Ni, et al. Expected signature of brownian motion up to the ﬁrst exit time from

a bounded domain. The Annals of Probability, 43(5):2729–2762, 2015.

[34] Ilya Chevyrev and Harald Oberhauser. Signature moments to characterize laws of stochastic

processes. arXiv preprint arXiv:1810.10971, 2018.

[35] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 770–778, 2016.

[36] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers:
Surpassing human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE
international conference on computer vision, pages 1026–1034, 2015.

10

[37] Stephanie Hyland, Cristóbal Esteban, and Gunnar Rätsch. Real-valued (medical) time series

generation with recurrent conditional gans. 2018.

[38] Yujia Li, Kevin Swersky, and Rich Zemel. Generative moment matching networks. In Interna-

tional Conference on Machine Learning, pages 1718–1727, 2015.

[39] Gerd Heber, Asger Lunde, Neil Shephard, and Kevin Sheppard. Oxford-man institute’s realized

library, version 0.3, 2009.

[40] Terry J Lyons, Michael Caruana, and Thierry Lévy. Differential equations driven by rough

paths. Springer, 2007.

[41] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980, 2014.

[42] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances
in neural information processing systems, pages 6626–6637, 2017.

11

A Supplementary Material

A.1 Preliminary

For the sake of precision, we start by introducing basic concepts around the signature of a path,
which lays the foundation for our analysis on the signature approximation for Wasserstein-1 Distance.
Besides, we give a brief introduction to Maximum Mean Discrepancy (MMD) in this subsection.

A.1.1 Tensor algebra space

The tensor algebra space of E is where the signature of a E-valued path take values. For simplicity,
ﬁx E = Rd. Then E has the basis {e1, . . . , ed}. Consider the successive tensor powers E⊗n of E
(equipped with some tensor norm). If one thinks of the elements ei as letters, then E⊗n is spanned
by the words of length n in the letters {e1, . . . , ed}, and can be identiﬁed with the space of real
homogeneous non-commuting polynomials of degree n in d variables. We note that E⊗0 = R.
Deﬁnition A.1. The space T ((E)) is deﬁned to be the vector space of all formal E-tensors series.
Deﬁnition A.2. Let n ≥ 1 be an integer. Let Bn = {a = (a0, a1, ...)|a0 = ... = an = 0}.The
truncated tensor algebra T (n)(E) of order n over E is deﬁned as the quotient algebra

T (n)(E) = T ((E)) /Bn.

(8)

The canonical homomorphism T ((E)) −→ T (n)(E) is denoted by πn.

In order for our analysis to work, we introduce the admissible norm of tensor powers deﬁned as
follows (the injective and projective norms, e.g. lp-norm, satisfy our constraints).
Deﬁnition A.3. We say that the tensor powers of E are endowed with an admissible norm |.|, if the
following conditions hold:

1. For each n ≥ 1, the symmetric group Sn acts by isometry on E⊗n, i.e.

|σv| = |v|, ∀v ∈ E⊗n, ∀σ ∈ Sn

2. The tensor product has norm 1, i.e. ∀n, m ≥ 1,

|v ⊗ w| ≤ |v||w|, ∀v ∈ E⊗n, w ∈ E⊗m.

We introduce the p-variation as a measure of the roughness of the path.
Deﬁnition A.4 (p-Variation). Let p ≥ 1 be a real number. Let X : J → E be a continuous path.
The p-variation of X on the interval J is deﬁned by

||X||p,J =



 sup
D⊂J

r−1
(cid:88)

j=0



1
p

(cid:12)
(cid:12)Xtj+1 − Xtj

(cid:12)
(cid:12)

p



,

(9)

where the supremum is taken over any time partition of J, i.e. D = (t1, t2, · · · , tr). 2

Let Cp(J, E) denote the range of any continuous path mapping from J to E of ﬁnite p-variation. The
larger p-variation is, the rougher a path is. The compactness of the time interval J can’t ensure the
ﬁnite 1-variation of a continuous path in general. For example, Brownian motion has (2 + ε)-variation
a.s ∀ε > 0, but it has inﬁnite p-variation a.s.∀p ∈ [1, 2].

For each p ≥ 1, the p-variation norm of a path X ∈ Cp(J, E) is denoted by ||X||p−var and deﬁned
as follows:

||X||p−var = ||X||p,J + sup
t∈J

||Xt||.

For concreteness, we state the decay rate of the signature for the path is of ﬁnite 1-variation. However,
there is a similar statement of the factorial decay for the case of paths of ﬁnite p-variation [40].

2 Let J = [s, t] be a closed bounded interval. A time partition of J is an increasing sequence of real numbers
D = (t0, t1, · · · , tr) such that s = t0 < t1 < · · · < tr = t. Let |D| denote the number of time points in D, i.e.
|D| = r + 1. ∆D denotes the time mesh of D, i.e. ∆D :=

(ti+1 − ti).

r−1
max
i=0

12

Lemma A.1 (Factorial Decay of the Signature). Let X ∈ C1(J, E). Then there exists a constant
C > 0, such that for all m ≥ 0,

|πm(S(X))| ≤

|X|m
1−var
m!

.

A.1.2 Maximum Mean Discrepancy (MMD)

Maximum Mean Discrepancy (MMD) is a popular choice of distance between two distributions µ
and ν, which can be used as the deterministic loss function in the context of the generative model.
MMD is deﬁned as

MMD(µ, ν) = sup
f ∈H

EX∼µ[f (X)] − EX∼ν[f (X)],

where H denotes a Reproducing kernel Hilbert space (RKHS) with kernel k. RKHS is a Hilbert space
of functionals X → R endowed with the inner product and reproducing property and is rich enough
to distinguish two measures. By the reproducing property of the kernel, MMD can be rewritten as

MMD(µ, ν) = E

XX (cid:48) iid∼ µ

[k(X, X (cid:48))] − 2E

X∼µ,Y ∼ν,X id∼Y

[k(X, Y )] + E

Y Y (cid:48) iid∼ ν

[k(Y, Y (cid:48))].

We implicitly assumed that (µ, ν) have required integrability properties for the right hand side to be
ﬁnite. Due to this explicit representation MMD is very efﬁcient to compute, but its performance in
generative models depends on the choice of kernel k [3].

A.2 The Signature Wasserstein-1 metric (Sig-W1)

In this subsection, we explain why the truncated Sig-W1 metric can be used to approximate the
conventional W1 metric on the measures on the path space. Then we give the detailed derivation on
the explicit formula on the truncated Sig-W1 metric (Lemma A.3).
Let us consider the space of time augmented paths X = Ω1
0(J, E) deﬁned in Section 23, in which
any discrete d-dimensional time series can be embedded using time augmentation and inserting the
zero initial point (Deﬁnition 4.3, [27]).
Let K be a compact subset of Ω1
0(J, E) and µ is a measure deﬁned on K. For any X is sampled from
µ, the expected signature of X under the measure µ is well deﬁned and determines the measure on K
uniquely [40]. By the deﬁnition of W1(µ, ν), there exists a sequence of fn : K → R with bounded
Lipschitz norm to attain the supremum W1(µ, ν). By the universality of the signature, it implies that
∀(cid:15) > 0, for each fn, there exists a linear functional Ln : T ((E)) → R to approximate fn uniformly,
i.e.

(cid:90)

|

K

fn(x)µ(dx) − fnν(dx) −

Ln(S(x))µ(dx) − Ln(S(x)ν(dx))

| ≤ 2(cid:15)

(cid:19)

(cid:18)(cid:90)

K

Therefore we propose the Sig-W1 metric on µ and ν by

Sig-W1(µ, ν) =

sup
|L|≤1, is a linear functional

L (Eµ[S(x)] − Eν[S(x)]) ,

where the Lipschitz L can be computed explicitly as L is a linear functional.

Next we give the error bound when restricting the linear functional L on the truncated signature in
Lemma A.2 using the factorial decay of the signatures.
Lemma A.2. Let L : T ((E)) → R be a bounded linear functional, and K be a compact set of the
range of the signature of a path in C1

0 (J, E). For any (cid:15) > 0, there exists an integer M > 0,

|L(x) − L(πM (x))| ≤ (cid:15).

sup
x∈K

(10)

3To avoid the technical difﬁculties and highlight the main idea of our approach, we consider Ω1
However, this assumption of the path space is strong. For example, Wiener measure is not deﬁned on Ω1
The condition may be relaxed if we embed the path of ﬁnite p-variation to the p-geometric rough path.

0(J, E).
0(J, E).

13

Proof. By Lemma A.1, for any x ∈ S, there exists l ∈ C1(J, E),

|x − πM (x)| ≤

(cid:88)

m≥M

|πm(x)| ≤

|l|m

1−var
m!

≤

|l|M +1
1−var
(M + 1)!

.

(cid:88)

m≥M

(11)

As S is a compact set, therefore L := supS(l)∈S |l|1−var is bounded. It follows that

lim
M→∞

LM +1
(M + 1)!

= 0,

which concludes the proof.

Recall that the truncated Sig-W1 norm of degree M is deﬁned as for a = EµSM (X) − EνSM (X) ∈
TM (E),

sup{L(a)|L is a linear functional on TM (E), and |L| ≤ 1}.

(12)

When |L| is deﬁned as the l2 norm of the linear coefﬁcient vector induced by L, Equation (12) admits
the solution |a| where |.| is the l2 norm. The derivation boils down to the optimization problem in
Lemma A.3.
Lemma A.3. Fix a ∈ Rd. Let l∗ denote the optimal l ∈ Rd such as to maximize (cid:104)l, a(cid:105) subject to
|l| ≤ 1. Then l∗ = a

|a| , and (cid:104)l∗, a(cid:105) = sup|l|≤1(cid:104)l, a(cid:105) = |a|.

Proof. We apply the method of Lagrange multipliers to solve this problem. Let us consider the
optimization problem U (l, λ) = (cid:104)l, a(cid:105) + λ(|l|2 − 1). Then (l∗, λ∗) satisﬁes the following equation:

∂lU (l∗, λ) = a − l∗λ = 0
∂λU (l∗, λ) = |l∗|2 − 1 = 0.

It follows that λ = |a| and l∗ = a/|a|, which implies that (cid:104)l∗, a(cid:105) = |a|.

A.3 SigCWGAN

The core idea of SigCWGAN is to lift the time series to the signature feature as a principled and
more effective feature extraction. In practice, the signature feature may often be accompanied with
several following path transformations:

• Time jointed transformation ( Deﬁnition 4.3, [27] );

• Cumulative sum transformation:

it

is deﬁned to map every (Xt)T

t=1 to CSt

:=

(cid:80)t

i=1 Xi, ∀t ∈ {1, · · · , T } and CS0 = 0 ( Equation (2.20) in [26] ).

• Lead-Lag transformation ( Equation (2.8) in [26] ).
• Lag added transformation: The m-lag added transformation of (Xt)T

t=1 is deﬁned as follows:

Lagm(X) = (Yt)T −m

t=1 , such that

Yt = (Xt, · · · , Xt+m).

Although in our analysis on the Sig-W1 metric, we use the time augmented path to embed the discrete
time series X to a continuous path for the ease of the discussion. However, to use Sig-W1 metric to
differentiate two measures on the path space, the only requirement of the way of embedding a discrete
time series to a continuous path is that this embedding needs to ensure the bijection between the
time series and its signature. Therefore, in practice we can choose other embedding to achieve that;
for example, by applying the lead-lag transformation to time series, one can ensure the one-to-one
correspondence between the time series and the signature.
The pseudocode of generating the next q-step forecast using Gθ is given in Algorithm 1.

Pseudocode of SigCWGAN is listed in Algorithm 2.

14

Algorithm 1 Pseudocode of Generating the next q-step forecast using Gθ

Input: xt−p+1:t, Gθ
Output: ˆxt+1:t+q

1: ˆxfuture ← a matrix of zeros of dimension d × q.
2: ˆx ←the concatenation of xt−p+1:t and ˆxfuture.
3: for i = 1 : q do
4:
5:

We sample Zi from the iid standard normal distribution.
ˆxt+i = G(ˆxt+i−p:t+i−1, Zi).

return ˆxt+1:t+q.

Algorithm 2 Pseudocode of SigCWGAN

Input: (xt)T

t=1, the signature degree of future path n, the signature degree of past path m,

the length of future path q, the length of past path p, learning rate l, batch size B

Output: θ - the optimal parameter of the AR-FNN generator G

1: We apply the path transformations to the past path and future path respectively and compute the

corresponding the truncated signature (Sm(xt−p+1:t), Sn(xt+1:t+q))t.

2: Apply the linear regression on (Sm(xt−p+1:t), Sn(xt+1:t+q))t and obtain the estimator of the

linear function ˆL.

3: Initialise the parameters θ of the generator.
4: for i = 1 : N do
5:
6:
7:

We randomly select the set of time index of batch size B, denoted by T .
for t ∈ T do

We estimate the conditional signature under the measure ν, i.e.

ˆEν[Sn(xt+1:t+q))t|xt−p+1:t] = ˆL(Sm(xt−p+1:t)).

8:

9:

Simulate nMC samples of the simulated future path segments (ˆx(j))nMC

j=1 by the generator

Gθ given the past path xt−p+1:t using Algorithm 1.

We estimate the conditional expected signature under the measure µ induced by the

generator as follows:

ˆEµ[Sn(xt+1:t+q))t|xt−p+1:t] =

1
nMC

nMC(cid:88)

j=1

Sm(ˆx(j)).

10:

Compute the loss function

|ˆEν[Sn(xt+1:t+q))t|xt−p+1:t] − ˆEµ[Sn(xt+1:t+q))t|xt−p+1:t]|

L(θ) =

1
T

(cid:88)

t∈T

11:

θ ← θ − l dL(θ)
dθ .

return θ.

A.4 AR-FNN Architecture

We give a detailed description of the AR-FNN architecture below. For this purpose let us begin by
deﬁning the employed transformations, namely the parametric rectiﬁer linear unit and the residual
layer.
Deﬁnition A.5 (Parametric rectiﬁer linear unit). The parametrised function φα ∈ C(R, R), α ≥ 0
deﬁned as

φα(x) = max(0, x) + α min(0, x)

is called parametric rectiﬁer linear unit (PReLU).
Deﬁnition A.6 (Residual layer). Let F : Rn → Rn be an afﬁne transformation and φα, α ≥ 0 a
PReLU. The function R : Rn → Rn deﬁned as

R(x) = x + φα ◦ F (x)
where φα is applied component-wise, is called residual layer.

15

The AR-FNN is deﬁned as a composition of PReLUs, residual layers and afﬁne transformations.
Its inputs are the past p-lags of the d-dimensional process we want to generate as well as the
d-dimensional noise vector. A formal deﬁnition is given below.
Deﬁnition A.7 (AR-FNN). Let d, p ∈ N, A1 : Rd(p+1) → R50, A4 : R50 → Rd be afﬁne
transformations, φα, α ≥ 0 a PReLU and R2, R3 : R50 → R50 two residual layers. Then the
function ArFNN : Rdp × Rd → Rd deﬁned as

ArFNN(x, z) = A4 ◦ R3 ◦ R2 ◦ φα ◦ A1(xz)

where xz denotes the concatenated vectors x and z, is called autoregressive feedforward neural
network (AR-FNN).

A.5 Numerical Results

We use the following public codes for implementing the baselines:

• RCGAN: https://github.com/ratschlab/RGAN
• Time-GAN: https://github.com/jsyoon0823/TimeGAN
• GMMN: https://github.com/yujiali/gmmn

For a fair comparison, we use the same neural network generator architecture, namely the 3-layer
AR-FNN described in subsection A.4, for the SigCWGAN, TimeGAN, RCGAN and GMMN. The
TimeGAN and RCGAN discriminators take as inputs the conditioning time series X1:p concatenated
with the synthetic time series Xp+1:p+q. Both discriminators use the AR-FNN as the underlying
architecture. However, the ﬁrst afﬁne layer is adjusted such that the AR-FNN is deﬁned as a function
of the concatenated time series, i.e. p + q lags and not p-lags as for the generator. Similarly, the
MMD is computed by concatenating the conditioning and synthetic time series. In order to obtain the
bandwidth parameter for computing the MMD of the GMMN we benchmarked the median heuristic
against using a mixture of bandwidths spanning multiple ranges as proposed in [38] and found latter
to work best. In our experiments we used three kernels with bandwidths 0.1, 1, 5.

All algorithms were optimised for a total of 1,000 generator weight updates. The neural network
weights were optimised by using the Adam optimiser [41] and learning rates for the generators were
set to 0.001. For the RCGAN and TimeGAN we applied two time-scale updates (TTUR) [42] and
set the learning rate to 0.003. Furthermore, we updated the discriminator’s weights two times per
generator weight update in order to improve convergence of the GAN.

In our numerical experiments, to compute the signature for the SigCWGAN method, we choose to
apply the following path transformations on the time series before computing the signatures: (1)
we combine the path xpast with its cumulative sum transformed path, denoted by ypast, which is
a 2d-dimensional path; (2) we apply 1-lag added transformation on ypast; (3) it follows with the
Lead-Lag transformation. The signature of such transformed path can well capture the marginal
distributions, auto-correlations and other temporal characteristics of the time-series data.
In the following, we describe the calculation of the test metrics precisely. Let (Xt)T
t=1 denote a
d-dimensional time series sampled from the real target distribution. We ﬁrst extract the input-out pairs
(Xt−p+1:t, Xt+1:t+q)t∈T , where T is the set of time indexes. Given the generator G, for each input
sample (Xt−p+1:t), we generate one sample of the q-step forecast ˆX (t)
t+1,t+q (if G is not conditional
generator, we generate a sample of q-step forecast ˆX (t)
t+1,t+q without any conditioning variable.). The
synthetic data generated by G is given by ( ˆX (t)

t+1,t+q)t, which we use to compute the test metrics.

Metric on marginal distribution Following [21], we use (Xt+1:t+q)t∈T and ( ˆX (t)
t+1:t+q)t∈T as
the samples of the marginal distribution of the real data and synthetic data per each time step. For
each feature dimension i ∈ {1, · · · , d}, we compute two empirical density functions based on the
i
histograms of the real data and synthetic data resp. denoted by ˆdf
G. Then the metric on
marginal distribution of the true and synthetic data is given by

r and ˆdf

i

1
d

d
(cid:88)

i=1

| ˆdf

i

r − ˆdf

i
G|1.

16

Absolute difference of lag-1 auto-correlation The auto-covariance of ith feature of the real data
with lag value k is computed by

ρi
r(k) :=

1
T − k

T −k
(cid:88)

t=1

(X i

t − ¯X i)(X i

t+k − ¯X i),

where ¯X i is the average of (X i
For the synthetic data, we estimate the auto-covariance of ith feature with lag value k is computed by

t=1.

t )T

ρi
G(k) :=

1
|T |

|T |
(cid:88)

t=1

ˆX (t),i
t+1

ˆX (t),i

t+k −





1
|T |

|T |
(cid:88)

t=1





ˆX (t),i
t+1





1
|T |

|T |
(cid:88)

t=1



ˆX (t),i
t+k

 .

(13)

r(0) / ρi
The estimator of the lag-1 auto-correlation of the real/synthetic data is given by ρi
r(1)
ρi
ACF score is deﬁned to be the absolute difference of lag-1 auto-correlation given as follows:

G(1)
G(0) . The
ρi

1
d

d
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

ρi
r(1)
ρi
r(0)

−

ρi
G(1)
ρi
G(0)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

Metric on the correlation We estimate the covariance of the ith and jth feature of time series
from the true data as follows:

covi,j

r =

1
T

T
(cid:88)

t=1

X i

t X i

t −

(cid:33) (cid:32)

(cid:32)

1
T

T
(cid:88)

t=1

X i
t

(cid:33)

X j
t

.

1
T

T
(cid:88)

t=1

Similarly, we estimate the covariance of the ith and jth feature of time series from the synthetic data
by

covi,j

G =

1
|T |

1
q

|T |
(cid:88)

q
(cid:88)

t=1

s=1

ˆX (t),i

t+s X (t),i

t+s −

(cid:32)

1
|T |

(cid:88)

t∈T

(cid:33) 


ˆX (t),i
t+s

1
|T |

(cid:88)

t∈|T |



X (t),j
t+s

 .

Thus the estimator of the correlation of the ith and jth feature of time series from the real/synthetic
data are given by τ i,j
. Then the metric on the correlation

and τ i,j

:=

r

covi,j
r√
covi,i

r covj,j

r

G :=

covi,j
G√
G covj,j
covi,i

G

between the real data and synthetic data is given by l1 norm of the difference of two correlation
matrices (cid:0)τ i,j
.

(cid:17)

(cid:16)

(cid:1)

r

i,j∈{1,··· ,d} and

τ i,j
G

i,j∈{1,··· ,d}

TRTR/TSTR R2 We split the input-output pairs (Xt−p+1:t, Xt+1) from the real data into the train
set and test set. We apply the linear signature model on real training data (Xt−p+1:t, Xt+1), validate
it and compute the corresponding R2 on the on the real test data (TRTR R2). Then we apply the
same linear signature model on the synthetic data (Xt−p+1:t, ˆXt+1), where ˆXt+1) is simulated by
the generator conditioning on the Xt−p+1:t. We evaluate the trained model on the real test data and
corresponding R2 is called (TSTR R2).

A.5.1 VAR(1)

We conduct the extensive experiments on VAR(1) with different hyper-parameter settings, i.e. d ∈
{1, 2, 3}, σ, φ ∈ {0.2, 0.5, 0.8}. The numerical results for VAR(1) synthetic data are summarized in
Table 2, 3 and 4. We highlight the best results with respect to each test metric in bold.

17

Table 2: Numerical results of VAR(1) for d = 1

Temporal Correlations

Settings

φ = 0.2

φ = 0.5

φ = 0.8

Metric on marginal distribution

SigCWGAN
TimeGAN
RCGAN
GMMN

0,0118
0,0244
0,0133
0,0064

0,0101
0,0259
0,0065
0,0065

0,0070
0,0127
0,0049
0,0055

Absolute difference of lag-1 autocorrelation

SigCWGAN
TimeGAN
RCGAN
GMMN

0,0067
0,0027
0,0408
0,0173

0,0019
0,0499
0,0378
0,0048

0,0029
0,0086
0,0106
0,0140

R2 obtained from TSTR. (TRTR ﬁrst row.)

TRTR

SigCWGAN
TimeGAN
RCGAN
GMMN

0,0510

0,0502
0,0479
0,0506
0,0501

0,2625

0,2614
0,2541
0,2622
0,2603

Sig-W1 distance

SigCWGAN
TimeGAN
RCGAN
GMMN

0,1029
0,3845
0,2268
0,1441

0,0846
0,2851
0,1695
0,1337

0,6418

0,6413
0,6391
0,6404
0,6307

0,0552
0,1766
0,1201
0,2306

18

Table 3: Numerical results of VAR(1) for d = 2

Temporal Correlations (ﬁxing σ = 0.8)

Feature Correlations (ﬁxing φ = 0.8)

Settings

φ = 0.2

φ = 0.5

φ = 0.8

σ = 0.2

σ = 0.5

σ = 0.8

SigCWGAN
TimeGAN
RCGAN
GMMN

SigCWGAN
TimeGAN
RCGAN
GMMN

SigCWGAN
TimeGAN
RCGAN
GMMN

TRTR

SigCWGAN
TimeGAN
RCGAN
GMMN

SigCWGAN
TimeGAN
RCGAN
GMMN

0,0059
0,0092
0,0820
0,0053

0,0027
0,0366
0,0527
0,0336

0,0087
0,0501
0,0321
0,0187

0,6480

0,6475
0,5823
0,6359
0,6093

0,1346
0,5495
0,6922
0,5271

Metric on marginal distribution

0,0083
0,0211
0,0120
0,0082

0,0059
0,0092
0,0080
0,0053

0,0063
0,0159
0,0071
0,0053

Absolute difference of lag-1 autocorrelation

0,0063
0,0978
0,0209
0,0500

0,0027
0,0366
0,0527
0,0336

0,0010
0,1115
0,0722
0,0766

0,0078
0,0209
0,0061
0,0067

0,0123
0,0536
0,0169
0,0190

0,0054
0,0163
0,0122
0,0054

0,0016
0,0916
0,0271
0,0526

0,0073
0,0234
0,0962
0,0096

L1-norm of real and generated cross correlation matrices
0,0087
0,0501
0,0321
0,0187
R2 obtained from TSTR. (TRTR ﬁrst row.)

0,0121
0,0557
0,2091
0,0135

0,0136
0,0851
0,0225
0,0415

0,0131
0,1965
0,0753
0,0275

0,0393

0,0383
0,0263
0,0167
0,0228

0,2500
0,6853
0,4253
0,3879

0,2498

0,2482
0,2333
0,1987
0,2138

0,2013
0,5377
0,6706
0,4081

0,6480

0,6475
0,5823
0,5763
0,6093

Sig-W1 distance
0,1346
0,5495
0,6922
0,527

0,6450

0,6443
0,6163
0,6304
0,5754

0,1358
0,6392
0,7832
0,7057

0,6465

0,6459
0,6031
0,6203
0,5846

0,1363
0,6321
0,6038
0,6507

19

Table 4: Numerical results of VAR(1) for d = 3

Temporal Correlations (ﬁxing σ = 0.8)

Feature Correlations (ﬁxing φ = 0.8)

Settings

φ = 0.2

φ = 0.5

φ = 0.8

σ = 0.2

σ = 0.5

σ = 0.8

0,0053
0,0100
0,0078
0,0072

0,0024
0,0796
0,0355
0,0977

0,0059
0,2475
0,2118
0,1820

0,6519

0,6511
0,6091
0,5951
0,5342

0,2688
0,8851
0,9599
0,9873

SigCWGAN
TimeGAN
RCGAN
GMMN

SigCWGAN
TimeGAN
RCGAN
GMMN

SigCWGAN
TimeGAN
RCGAN
GMMN

TRTR

SigCWGAN
TimeGAN
RCGAN
GMMN

SigCWGAN
TimeGAN
RCGAN
GMMN

Metric on marginal disribution

0,0069
0,0220
0,0080
0,0081

0,0053
0,0100
0,0078
0,0072

0,0043
0,0131
0,0093
0,0108

Absolute difference of lag-1 autocorrelation

0,0050
0,0559
0,0741
0,0582

0,0024
0,0796
0,0355
0,0977

0,0024
0,0319
0,0888
0,0947

0,0065
0,0269
0,0077
0,0081

0,0122
0,0578
0,0298
0,0340

0,0050
0,0105
0,0082
0,0109

0,0037
0,0245
0,1069
0,1200

0,0065
0,2301
0,1489
0,0707

L1-norm of real and generated cross correlation matrices
0,0059
0,2475
0,2118
0,1820
R2 obtained from TSTR. (TRTR ﬁrst row.)

0,0026
0,5181
0,2610
0,1107

0,0165
0,1201
0,0981
0,2246

0,0110
0,2235
0,2917
0,3934

0,441

0,0401
0,0100
0,0266
-0,0099

0,5011
1,2627
0,7411
0,8352

0,2601

0,2580
0,2064
0,2043
0,1711

0,3970
1,0881
0,9273
0,8492

0,6519

0,6511
0,6091
0,5951
0,5342

Sig-W1 distance
0,2688
0,8851
0,9599
0,9873

0,6515

0,6503
0,6305
0,5915
0,4955

0,2664
0,8467
1,1650
1,2482

0,6517

0,6508
0,6140
0,5855
0,5017

0,2681
0,9126
1,1130
1,2047

20

(a) SigCWGAN

(b) TimeGAN

(c) RCGAN

(d) GMMN

Figure 6: Exemplary development of the considered distances and score functions during training for
the 1-dimensional VAR(1) model with autocorrelation coefﬁcient φ = 0.8.

21

(a) SigCWGAN

(b) TimeGAN

(c) RCGAN

(d) GMMN

Figure 7: Exemplary development of the considered distances and score functions during training for
the 2-dimensional VAR(1) model with autocorrelation coefﬁcient φ = 0.8 and co-variance parameter
σ = 0.8. The colours blue and orange indicate the relevant distance / score for each dimension.

22

(a) SigCWGAN

(b) TimeGAN

(c) RCGAN

(d) GMMN

Figure 8: Exemplary development of the considered distances and score functions during training for
the 3-dimensional VAR(1) model with autocorrelation coefﬁcient φ = 0.8 and co-variance parameter
σ = 0.8. The colours blue, orange and green indicate the relevant distance / score for each dimension.

23

For d = 3, φ = 0.8, σ = 0.8, the comparison results of the test metrics on the marginal distribution
and auto-correlation are given in Table 9. Figure 10 proves that the SigCWGAN can produce the
synthetic data to capture the feature correlation of time series better than the other baselines.

(a) SigCWGAN

(b) TimeGAN

(c) RCGAN

(d) GMMN

Figure 9: The plot of the marginal distribution on the linear scale (1st column), log-scale (2nd
column) and the auto-correlation ﬁt of the ﬁrst lag (3rd column) for the d = 3-dimensional VAR(1)
model with autocorrelation coefﬁcient φ = 0.8 and co-variance parameter σ = 0.8. The orange
bars in the ﬁrst two columns display the generated PDF, whereas the PDF of the real distribution
is displayed in blue. Similarly, the orange lines in the plot of the auto-correlation ﬁt display the
generated autocorrelation, whereas the real auto-correlation is displayed in blue. The synthetic data
from which marginals and auto-correlations were computed was obtained by generating recurrently
q = 3 steps ahead.

Figure 10: The plot of correlation ﬁtting for VAR(1) model with autocorrelation coefﬁcient φ = 0.8
and co-variance parameter σ = 0.8.

24

420240.000.050.100.150.200.250.300.350.40pdfHistoricals=0.01=0.01Generateds=0.01=0.04HistoricalGenerated42024104103102101log-pdfHistoricalGenerated01Lags0.8000.8250.8500.8750.9000.9250.9500.9751.000ACFHistoricalGenerated4321012340.000.050.100.150.200.250.300.350.40pdfHistoricals=0.03=0.03Generateds=0.07=0.03HistoricalGenerated432101234103102101log-pdfHistoricalGenerated01Lags0.8000.8250.8500.8750.9000.9250.9500.9751.000ACFHistoricalGenerated420240.000.050.100.150.200.250.300.350.40pdfHistoricals=0.01=0.01Generateds=0.00=0.00HistoricalGenerated42024104103102101log-pdfHistoricalGenerated01Lags0.8000.8250.8500.8750.9000.9250.9500.9751.000ACFHistoricalGenerated420240.000.050.100.150.200.250.300.350.40pdfHistoricals=0.01=0.01Generateds=0.16=0.04HistoricalGenerated42024104103102101log-pdfHistoricalGenerated01Lags0.700.750.800.850.900.951.00ACFHistoricalGenerated4321012340.000.050.100.150.200.250.300.350.40pdfHistoricals=0.03=0.03Generateds=0.03=0.29HistoricalGenerated432101234103102101log-pdfHistoricalGenerated01Lags0.8000.8250.8500.8750.9000.9250.9500.9751.000ACFHistoricalGenerated420240.00.10.20.30.4pdfHistoricals=0.01=0.01Generateds=0.16=0.07HistoricalGenerated42024104103102101log-pdfHistoricalGenerated01Lags0.8000.8250.8500.8750.9000.9250.9500.9751.000ACFHistoricalGenerated420240.000.050.100.150.200.250.300.350.40pdfHistoricals=0.01=0.01Generateds=0.28=0.10HistoricalGenerated42024104103102101log-pdfHistoricalGenerated01Lags0.800.850.900.951.00ACFHistoricalGenerated4321012340.000.050.100.150.200.250.300.350.40pdfHistoricals=0.03=0.03Generateds=0.13=0.22HistoricalGenerated432101234103102101log-pdfHistoricalGenerated01Lags0.8000.8250.8500.8750.9000.9250.9500.9751.000ACFHistoricalGenerated420240.000.050.100.150.200.250.300.350.40pdfHistoricals=0.01=0.01Generateds=0.00=0.00HistoricalGenerated42024104103102101log-pdfHistoricalGenerated01Lags0.8000.8250.8500.8750.9000.9250.9500.9751.000ACFHistoricalGenerated420240.000.050.100.150.200.250.300.350.40pdfHistoricals=0.01=0.01Generateds=0.07=0.07HistoricalGenerated42024104103102101log-pdfHistoricalGenerated01Lags0.700.750.800.850.900.951.00ACFHistoricalGenerated4321012340.000.050.100.150.200.250.300.350.40pdfHistoricals=0.03=0.03Generateds=0.02=0.14HistoricalGenerated432101234103102101log-pdfHistoricalGenerated01Lags0.750.800.850.900.951.00ACFHistoricalGenerated420240.000.050.100.150.200.250.300.350.40pdfHistoricals=0.01=0.01Generateds=0.16=0.09HistoricalGenerated42024104103102101log-pdfHistoricalGenerated01Lags0.700.750.800.850.900.951.00ACFHistoricalGenerated(a) SigCWGAN

(b) TimeGAN

(c) RCGAN

(d) GMMN

Figure 11: The plot of the marginal distribution and the auto-correlation ﬁtting for VAR(1) model
with autocorrelation coefﬁcient φ = 0.8 and co-variance parameter σ = 0.8. The synthetic data
from which marginals and auto-correlations were computed was obtained by generating recurrently a
single path for 80.000 steps. Visually, the marginals and auto-correlations of the baseline models do
not ﬁt the real distribution well, whereas the SigCWGAN gives a good ﬁt.

25

420240.000.050.100.150.200.250.300.350.40pdfHistoricals=0.01=0.01Generateds=0.08=0.17HistoricalGenerated42024104103102101log-pdfHistoricalGenerated01Lags0.8000.8250.8500.8750.9000.9250.9500.9751.000ACFHistoricalGenerated4321012340.00.10.20.30.4pdfHistoricals=0.03=0.03Generateds=0.06=0.15HistoricalGenerated432101234104103102101log-pdfHistoricalGenerated01Lags0.8000.8250.8500.8750.9000.9250.9500.9751.000ACFHistoricalGenerated420240.000.050.100.150.200.250.300.350.40pdfHistoricals=0.01=0.01Generateds=0.06=0.11HistoricalGenerated42024104103102101log-pdfHistoricalGenerated01Lags0.8000.8250.8500.8750.9000.9250.9500.9751.000ACFHistoricalGenerated420240.00.10.20.30.40.5pdfHistoricals=0.01=0.01Generateds=0.10=0.06HistoricalGenerated42024104103102101log-pdfHistoricalGenerated01Lags0.40.50.60.70.80.91.0ACFHistoricalGenerated4321012340.00.10.20.30.40.50.60.7pdfHistoricals=0.03=0.03Generateds=0.23=0.02HistoricalGenerated432101234104103102101100log-pdfHistoricalGenerated01Lags0.700.750.800.850.900.951.00ACFHistoricalGenerated420240.00.10.20.30.40.50.60.70.8pdfHistoricals=0.01=0.01Generateds=0.20=0.20HistoricalGenerated42024104103102101100log-pdfHistoricalGenerated01Lags0.750.800.850.900.951.00ACFHistoricalGenerated420240.00.10.20.30.40.50.60.70.8pdfHistoricals=0.01=0.01Generateds=0.30=0.47HistoricalGenerated42024104103102101100log-pdfHistoricalGenerated01Lags0.60.70.80.91.0ACFHistoricalGenerated4321012340.00.10.20.30.40.50.60.7pdfHistoricals=0.03=0.03Generateds=0.05=0.23HistoricalGenerated432101234104103102101100log-pdfHistoricalGenerated01Lags0.60.70.80.91.0ACFHistoricalGenerated420240.00.20.40.60.8pdfHistoricals=0.01=0.01Generateds=0.46=0.38HistoricalGenerated42024104103102101100log-pdfHistoricalGenerated01Lags0.600.650.700.750.800.850.900.951.00ACFHistoricalGenerated420240.00.10.20.30.40.5pdfHistoricals=0.01=0.01Generateds=0.05=0.02HistoricalGenerated42024104103102101log-pdfHistoricalGenerated01Lags0.50.60.70.80.91.0ACFHistoricalGenerated4321012340.00.10.20.30.40.50.60.70.8pdfHistoricals=0.03=0.03Generateds=0.10=0.47HistoricalGenerated432101234104103102101100log-pdfHistoricalGenerated01Lags0.50.60.70.80.91.0ACFHistoricalGenerated420240.00.10.20.30.40.50.6pdfHistoricals=0.01=0.01Generateds=0.19=0.30HistoricalGenerated42024104103102101100log-pdfHistoricalGenerated01Lags0.50.60.70.80.91.0ACFHistoricalGeneratedA.5.2 ARCH(p)

We implement extensive experiments on ARCH(p) with different p−lag values, i.e. p ∈ {2, 3, 4}. The
numerical results are summarized in Table 5. The best results among all the models are highlighted
in bold.

Table 5: Numerical results of the ARCH(p) datasets.

Settings

p = 2

p = 3

p = 4

Metric on marginal distribution

SigCWGAN
TimeGAN
RCGAN
GMMN

0,00918
0,02569
0,01069
0,00744

0,00880
0,02119
0,01612
0,00783

Absolute difference of lag-1 autocorrelation

SigCWGAN
TimeGAN
RCGAN
GMMN

0,00542
0,01714
0,05372
0,02056

0,00852
0,02401
0,01685
0,00859

0.01142
0.2191
0.01182
0.01259

0.01106
0.03267
0.04879
0.01441

L1-norm of real and generated cross correlation matrices

SigCWGAN
TimeGAN
RCGAN
GMMN

0,00462
0,00315
0,01604
0,04326

0,00546
0,06551
0,08823
0,03930

R2 obtained from TSTR. (TRTR ﬁrst row.)

TRTR

SigCWGAN
TimeGAN
RCGAN
GMMN

0,32168

0,31623
0,30835
0,31146
0,27982

0,32615

0,31913
0,30556
0,30727
0,28072

Sig-W1 distance

SigCWGAN
TimeGAN
RCGAN
GMMN

0,12210
0,20228
0,18781
0,26797

0,14682
0,22761
0,20943
0,26853

0.00489
0.04408
0.00235
0.01603

0.33305

0.31642
0.30240
0.30924
0.30742

0.14098
0.23398
0.21876
0.25811

26

(a) SigCGAN

(b) TimeGAN

(c) RCGAN

(d) GMMN

Figure 12: The plot displays a comparison of the marginal distribution on the linear- and log-scale, as
well as the ﬁt of the auto-correlation for ARCH(3) data. Histograms and the auto-correlation of the
data is indicated in blue and the ﬁts from the generator are coloured in orange.

27

A.5.3 Empirical Data

Table 6: Numerical results of the stocks datasets.

Data type

SPX

SPX + DJI

Metric on marginal distribution

SigCWGAN
TimeGAN
RCGAN
GMMN

0,01730
0,02155
0,02094
0,01608

0,01674
0,02127
0,01655
0,02387

Absolute difference of lag-1 autocorrelation

SigCWGAN
TimeGAN
RCGAN
GMMN

0,01342
0,05792
0,03362
0,01283

0,01192
0,03035
0,04075
0,02676

L1-norm of real and generated cross correlation matrices

SigCWGAN
TimeGAN
RCGAN
GMMN

0,01079
0,12363
0,04606
0,04651

0,07435
0,61488
0,15353
0,22380

R2 obtained from TSTR. (TRTR ﬁrst row.)

TRTR

SigCWGAN
TimeGAN
RCGAN
GMMN

SigCWGAN
TimeGAN
RCGAN
GMMN

0,31706

0,30797
0,29818
0,30822
0,28837

Sig-W1 distance

0,31775
0,58541
0,47107
0,59073

0,31934

0,29396
0,29192
0,29638
0,29576

4,36744
5,99482
5,43254
6,23777

28

(a) SigCWGAN

(b) TimeGAN

(c) RCGAN

(d) GMMN

Figure 13: Exemplary development of the considered distances and score functions during training
for SPX data.

29

(a) SigCWGAN

(b) TimeGAN

(c) RCGAN

(d) GMMN

Figure 14: Exemplary development of the considered distances and score functions during training
for SPX and DJI data.

30

(a) SigCWGAN

(b) TimeGAN

(c) RCGAN

(d) GMMN

Figure 15: The plot displays a comparison of the marginal distribution on the linear- and log-scale, as
well as the ﬁt of the auto-correlation for SPX data. Histograms and the auto-correlation of the data is
indicated in blue and the ﬁts from the generator are coloured in orange.

Figure 16: Comparison of real and synthetic cross-correlation matrices for SPX data. On the far
left the real cross-correlation matrix from SPX log-return and log-volatility data is shown. The
colorbar on the right indicates the range of values taken. Observe that the historical correlation
between log-returns and log-volatility is negative, indicating the presence of leverage effects, i.e.
when log-returns are negative, log-volatility is high.

31

420240.00.10.20.30.40.50.60.7pdfHistoricals=0.36=3.27Generateds=0.31=2.40HistoricalGenerated42024103102101100log-pdfHistoricalGenerated01Lags0.00.20.40.60.81.0ACFHistoricalGenerated321012340.00.10.20.30.40.5pdfHistoricals=0.43=0.09Generateds=0.33=0.22HistoricalGenerated32101234102101log-pdfHistoricalGenerated01Lags0.800.850.900.951.00ACFHistoricalGenerated420240.00.10.20.30.40.50.60.70.8pdfHistoricals=0.36=3.27Generateds=0.04=1.54HistoricalGenerated42024103102101100log-pdfHistoricalGenerated01Lags0.00.20.40.60.81.0ACFHistoricalGenerated321012340.00.10.20.30.40.5pdfHistoricals=0.43=0.09Generateds=0.38=0.09HistoricalGenerated32101234103102101log-pdfHistoricalGenerated01Lags0.800.850.900.951.00ACFHistoricalGenerated420240.00.20.40.60.8pdfHistoricals=0.36=3.27Generateds=0.14=2.62HistoricalGenerated42024103102101100log-pdfHistoricalGenerated01Lags0.00.20.40.60.81.0ACFHistoricalGenerated321012340.00.10.20.30.40.5pdfHistoricals=0.43=0.09Generateds=0.54=0.34HistoricalGenerated32101234103102101log-pdfHistoricalGenerated01Lags0.800.850.900.951.00ACFHistoricalGenerated420240.00.10.20.30.40.50.60.7pdfHistoricals=0.36=3.27Generateds=0.08=0.89HistoricalGenerated42024103102101100log-pdfHistoricalGenerated01Lags0.00.20.40.60.81.0ACFHistoricalGenerated321012340.00.10.20.30.40.5pdfHistoricals=0.43=0.09Generateds=0.67=0.61HistoricalGenerated32101234103102101log-pdfHistoricalGenerated01Lags0.750.800.850.900.951.00ACFHistoricalGenerated(a) SigCWGAN

(b) TimeGAN

(c) RCGAN

(d) GMMN

Figure 17: The plot displays a comparison of the marginal distribution on the linear- and log-scale, as
well as the ﬁt of the auto-correlation for SPX and DJI data. Histograms and the auto-correlation of
the data is indicated in blue and the ﬁts from the generator are colored in orange.

Figure 18: Comparison of real and synthetic cross-correlation matrices for SPX and DJI log-return
and log-volatility data. On the far left the real cross-correlation matrix from SPX and DJI data is
shown. The colorbar on the far right indicates the range of values taken.

32

420240.00.10.20.30.40.50.60.7pdfHistoricals=0.36=3.25Generateds=0.29=2.85HistoricalGenerated42024103102101100log-pdfHistoricalGenerated01Lags0.00.20.40.60.81.0ACFHistoricalGenerated321012340.00.10.20.30.40.5pdfHistoricals=0.43=0.09Generateds=0.42=0.44HistoricalGenerated32101234103102101log-pdfHistoricalGenerated01Lags0.800.850.900.951.00ACFHistoricalGenerated64202460.00.10.20.30.40.50.60.7pdfHistoricals=0.37=3.25Generateds=0.07=2.73HistoricalGenerated6420246103102101log-pdfHistoricalGenerated01Lags0.00.20.40.60.81.0ACFHistoricalGenerated321012340.00.10.20.30.4pdfHistoricals=0.48=0.18Generateds=0.31=0.44HistoricalGenerated32101234103102101log-pdfHistoricalGenerated01Lags0.800.850.900.951.00ACFHistoricalGenerated420240.00.10.20.30.40.50.60.70.8pdfHistoricals=0.36=3.25Generateds=0.16=1.20HistoricalGenerated42024103102101100log-pdfHistoricalGenerated01Lags0.00.20.40.60.81.0ACFHistoricalGenerated321012340.00.10.20.30.40.5pdfHistoricals=0.43=0.09Generateds=0.53=0.29HistoricalGenerated32101234103102101log-pdfHistoricalGenerated01Lags0.800.850.900.951.00ACFHistoricalGenerated64202460.00.10.20.30.40.50.60.7pdfHistoricals=0.37=3.25Generateds=0.06=0.93HistoricalGenerated6420246103102101100log-pdfHistoricalGenerated01Lags0.00.20.40.60.81.0ACFHistoricalGenerated321012340.00.10.20.30.40.5pdfHistoricals=0.48=0.18Generateds=0.39=0.19HistoricalGenerated32101234103102101log-pdfHistoricalGenerated01Lags0.750.800.850.900.951.00ACFHistoricalGenerated420240.00.10.20.30.40.50.60.7pdfHistoricals=0.36=3.25Generateds=0.22=1.43HistoricalGenerated42024103102101100log-pdfHistoricalGenerated01Lags0.00.20.40.60.81.0ACFHistoricalGenerated321012340.00.10.20.30.40.5pdfHistoricals=0.43=0.09Generateds=0.48=0.26HistoricalGenerated32101234103102101log-pdfHistoricalGenerated01Lags0.800.850.900.951.00ACFHistoricalGenerated64202460.00.10.20.30.40.50.6pdfHistoricals=0.37=3.25Generateds=0.32=1.19HistoricalGenerated6420246103102101log-pdfHistoricalGenerated01Lags0.00.20.40.60.81.0ACFHistoricalGenerated321012340.00.10.20.30.4pdfHistoricals=0.48=0.18Generateds=0.37=0.21HistoricalGenerated32101234103102101log-pdfHistoricalGenerated01Lags0.800.850.900.951.00ACFHistoricalGenerated420240.00.10.20.30.40.50.60.7pdfHistoricals=0.36=3.25Generateds=0.04=0.14HistoricalGenerated42024103102101100log-pdfHistoricalGenerated01Lags0.00.20.40.60.81.0ACFHistoricalGenerated321012340.00.10.20.30.40.5pdfHistoricals=0.43=0.09Generateds=0.84=0.76HistoricalGenerated32101234103102101log-pdfHistoricalGenerated01Lags0.750.800.850.900.951.00ACFHistoricalGenerated64202460.00.10.20.30.40.50.6pdfHistoricals=0.37=3.25Generateds=0.00=0.03HistoricalGenerated6420246103102101log-pdfHistoricalGenerated01Lags0.00.20.40.60.81.0ACFHistoricalGenerated321012340.00.10.20.30.40.5pdfHistoricals=0.48=0.18Generateds=0.56=0.36HistoricalGenerated32101234103102101log-pdfHistoricalGenerated01Lags0.800.850.900.951.00ACFHistoricalGenerated