Energy Drain of the Object Detection Processing Pipeline for Mobile
Devices: Analysis and Implications

Haoxin Wang∗, BaekGyu Kim†, Jiang Xie∗, and Zhu Han‡
∗University of North Carolina at Charlotte, Charlotte, NC 28223, U.S.A.
†Toyota Motor North America (TMNA) R&D InfoTech Labs, U.S.A.
‡University of Houston, Houston, TX 77004, U.S.A.

0
2
0
2

v
o
N
6
2

]
F
P
.
s
c
[

1
v
5
7
0
3
1
.
1
1
0
2
:
v
i
X
r
a

Abstract—Applying deep learning to object detection provides
the capability to accurately detect and classify complex objects
in the real world. However, currently, few mobile applications
use deep learning because such technology is computation-
intensive and energy-consuming. This paper, to the best of our
knowledge, presents the ﬁrst detailed experimental study of a
mobile augmented reality (AR) client’s energy consumption and
the detection latency of executing Convolutional Neural Networks
(CNN) based object detection, either locally on the smartphone
or remotely on an edge server. In order to accurately measure the
energy consumption on the smartphone and obtain the break-
down of energy consumed by each phase of the object detection
processing pipeline, we propose a new measurement strategy. Our
detailed measurements reﬁne the energy analysis of mobile AR
clients and reveal several interesting perspectives regarding the
energy consumption of executing CNN-based object detection.
Furthermore, several
insights and research opportunities are
proposed based on our experimental results. These ﬁndings from
our experimental study will guide the design of energy-efﬁcient
processing pipeline of CNN-based object detection.

I. INTRODUCTION

With the advancement in Deep Learning in the past few
years, we are able to create complex machine learning models
for detecting objects in real-time video frames. This advance-
ment has the potential to make Augmented Reality (AR) de-
vices highly intelligent and enable industries to favor machine
learning models with superior performance. For example,
AR automotive applications (e.g., deep learning-based AR
head-up-displays (HUDs)) are promised to help increase road
safety, bring intuitive activities to driving, and enhance driving
experience in the future. Meanwhile, as people nowadays are
using their smartphones to a larger extent and also expect
increasingly advanced performance from their mobile applica-
tions, the industry needs to adopt more advanced technologies
to meet such expectations. One such adoption can be the use
of deep learning-based AR applications.

However, few mobile AR applications use deep learning to-
day because of inadequate infrastructure support (e.g., limited
computation capacity and battery resource of smartphones).
Deep learning algorithms are computation-intensive, and ex-
ecuted locally in ill-equipped smartphones may not provide
acceptable latency for end users. For instance, in Deepmon
[1], it takes approximately 600 ms for small and medium

convolutional neural network (CNN1) models and almost 3
seconds for large CNN models to process one frame, which
is obviously not acceptable for real-time processing [2].

Two research directions have emerged to address this chal-
lenge. The ﬁrst direction is to tailor the computation-intensive
deep learning algorithms to be executed on smartphones. For
instance, Tiny-YOLO [3] that has only 9 convolutional layers
(24 convolutional layers in a full YOLO network) is developed
and optimised for use on embedded and mobile devices.
TensorFlow Lite [4] is TensorFlow’s lightweight solution for
embedded and mobile devices. It enables low-latency inference
of on-device machine learning models with a small binary
size and fast performance supporting hardware acceleration.
However,
the
cost of the precision degradation of the detection. The other
research direction that is widely used in running deep learning
the computation data to
in smartphones is to transfer all
more powerful
the remote cloud and
edge servers) and execute deep learning algorithms there [5]–
[7]. Such ofﬂoading-based solutions can reduce the inference
latency and extend smartphones’ battery life only when the
network access is reliable and sufﬁciently fast.

the reduction of the inference latency is at

infrastructures (e.g.,

Our Motivation. Although the complexity and capabili-
ties of smartphones continue to grow at an amazing pace,
smartphones are expected to continually become lighter and
slimmer. When combined with energy-hungry deep learning-
based applications, the limited battery capacity allowed by
these expectations now motivates signiﬁcant investment into
smartphone power management research. In order to better
investigate and understand the relationship between the energy
consumption and the performance of deep learning-based
applications such as CNN-based object detection, we propose
the following questions:

RQ 1. How is energy consumed when a CNN-based object
detection application is executed locally on a mobile AR
client? In order to help a mobile AR device to extend its
battery life, conducting a comprehensive measurement study
is signiﬁcantly important.

RQ 2. Does ofﬂoading the object detection tasks to a
powerful infrastructure signiﬁcantly decrease both the energy
consumption and latency? When a CNN-based object detec-

This is a personal copy of the authors. Not for redistribution. The ﬁnal
version of this paper was accepted by IEEE Transactions on Green Commu-
nications and Networking.

1A CNN is a deep learning algorithm which has demonstrated great success

on image recognition, image classiﬁcations, object detection, etc.

 
 
 
 
 
 
tion application is executed remotely, communication latency
is non-negligible and unstable, especially in wireless networks.
Previous work [8] shows that smartphone’s radio interfaces
account for up to 50% of the total power budget. In addition,
improved communication speeds generally come at the cost of
higher power consumption [9].

RQ 3. Besides the network condition, what else impacts the
energy consumption and latency when executed remotely, and
how? Executing object detection on a remote edge server is
one of the most commonly used approaches to assist resource-
constrained smartphones in improving their energy efﬁciency
and performance [10]. Therefore, to further improve the efﬁ-
ciency for executing object detection remotely, understanding
the factors that may impact
the detection performance is
critical.

Our Contributions. In this paper, we conduct, to the best
of our knowledge, the ﬁrst comprehensive experimental study
that investigates how a mobile AR client’s energy efﬁciency,
latency, and detection accuracy are inﬂuenced by diverse
factors (e.g., CPU governor, CNN model size, and image post
processing algorithm) in both local and remote executions. We
make the following contributions:

1) Developing two Android benchmark applications that
perform real-time object detections: one is running a light
CNN model locally on the smartphone and the other is
running a large CNN model remotely on an edge server.
2) Measuring and evaluating the energy consumption and
latency of each phase in the implemented end-to-end
CNN-based object detection processing pipeline. Both
local and remote executions are investigated.

3) Comparing the local execution and the remote execution
in terms of energy efﬁciency, latency, detection accuracy,
etc.

4) Proposing several insights which can potentially guide
the future design of energy-efﬁcient mobile AR systems
based on our experimental study.

The rest of this paper is organized as follows. Section II
discusses related work. Section III describes our proposed
methodology and key performance metrics that we consider
in this study. Experimental results of the local execution and
remote execution are presented in Section IV and Section V,
respectively. Finally, Sections VI and VII discusses threats to
validity and concludes the paper, respectively.

II. RELATED WORK
Energy Measurement. With the popularity of energy con-
strained mobile devices (e.g., smartphone, AR glass, and
smartwatch), a number of research has investigated how the
energy is consumed in mobile devices when executing appli-
cations through measurement studies [11]–[13]. [14] proposes
and implements a measurement framework that can physi-
cally measure the energy consumption of mobile devices and
automate the reporting of measurement back to researchers.
[15], [16] study the energy consumption of GUI colors on
OLED displays. In addition, the energy efﬁciency of network
protocols such as HTTP on mobile devices has been discussed

in [17], [18]. However, very few energy measurement studies
focus on running deep learning-based applications on mobile
devices, especially mobile AR applications. Although [19] dis-
cusses and compares the energy efﬁciency of different machine
learning applications in terms of algorithm, implementation,
and operating system (OS), our work focuses on a speciﬁc
application, object detection, and conducts a comprehensive
study on (i) energy efﬁciency comparison between local and
remote executions as well as (ii) how hardware and software
conﬁgurations impact the energy efﬁciency of executing object
detections on smartphones.

Energy Modeling. Energy modeling has been widely used
for investigating the factors that inﬂuence the energy consump-
tion of mobile devices. [20]–[24] propose energy models of
WiFi and LTE data transmission with respect to the network
performance metrics, such as data and retransmission rates.
[25]–[29] propose multiple power consumption models to
estimate the energy consumption of mobile CPUs. Tail energy
caused by different components, such as disk, Wi-Fi, 3G,
and GPS in smartphones has been investigated in [11], [29].
However, none of them can be directly applied to estimate the
energy consumed by mobile AR applications. This is because
mobile AR applications introduce a variety of (i) energy
consuming components (e.g., camera sampling and image
conversion) that are not considered in the previous models
and (ii) conﬁguration variables (e.g., computation model size
and camera sample rate) that also signiﬁcantly inﬂuence the
energy consumption of mobile devices.

CNN. In recent years, applying CNNs to object detection
has been proven to achieve excellent performance [1], [3],
[30]–[33]. In [34], [35], the speed and accuracy trade-offs of
various modern CNN models are compared. However, none
of these works considered the performance of running CNNs
on smartphones. In addition, although existing papers have
extensively investigated how to run CNN models on mobile
devices, including model compression of CNNs [36], GPU
acceleration [1], and only processing important frames [5],
none of these works considered the energy consumption of
executing CNNs on smartphones. In [37], a small number
of measurements on the battery drain of running a CNN on
a powerful smartphone are conducted. However, its battery
drain results are reported by the Android OS that can only
provide coarse-grained results. For example, it only shows
the total battery usage of running a CNN on a smartphone
for 30 minutes. In addition, it only studies running CNNs
on smartphones with high computation capabilities and the
experimental results are not comparable to smartphones with
poor computation capabilities.

Computation Ofﬂoading. Most existing research on com-
putation ofﬂoading focuses on how to make ofﬂoading de-
cisions. [38]–[41] coordinate the scheduling of ofﬂoading
requests for multiple applications to further reduce the wireless
energy cost caused by the long tail problem. [42] proposes
an energy-efﬁcient ofﬂoading approach for multicore-based
mobile devices. [43] discusses the energy efﬁciency of com-
putation ofﬂoading for mobile clients in cloud computing.

Edge Server. The edge server is developed to process
received image frames sent from a smartphone and send
the detection results back to the smartphone. We implement
an edge server on an Nvidia Jetson AGX Xavier which is
connected to a WiFi AP through a 1Gbps Ethernet cable (the
length of the cable is less than 1 meter). The transmission
latency between the server and AP can be ignored. Two major
modules are implemented on the edge server. The ﬁrst one
is the communication service handler module which performs
authentication and establishes a TCP socket connection with
the mobile AR client. This module is also responsible for dis-
patching the detection results to the corresponding smartphone.
The second one is the object detection module that is designed
based on a custom framework called Darknet [49] with GPU
acceleration and runs YOLOv3 [3], a large neural network
model with 24 convolutional layers. The YOLOv3 model used
in our experiments is trained on COCO dataset [50] and can
detect 80 classes.

Power Monitor. To measure the power consumption, we
use an external power monitor, a Monsoon Power Moni-
tor [51], to provide power supply for the test smartphone.
Different from old smartphone models, modern smartphones
like Nexus 6 have very tiny battery connectors, making it
very challenging to connect the power monitor to them. To
solve this problem, we modify the battery connection of
Nexus 6 by designing a customized circuit and soldering it
to the smartphone’s power input interface. In addition, the
power measurements are taken with the screen on, with the
Bluetooth/LTE radios disabled, and with minimal background
application activity, ensuring that the smartphone’s base power
is low and does not vary unpredictably over time. For the
measurements of the power consumption in local execution,
the base power is deﬁned as the power consumed by the
smartphone when its WiFi interface is turned off. For the
measurements of the power consumption in remote execution,
the base power is deﬁned as the power consumed when
the smartphone is connected to the AP without any data
transmission activity.

B. Benchmark Applications

Three benchmark applications are implemented in this pa-
per. The ﬁrst application is executing CNN-based object de-
tection on tested smartphones, deﬁned as local execution. The
second application is executing CNN-based object detection on
our equipped edge server, deﬁned as remote execution. Figs.
2 and 3 provide an overview of the processing pipeline of
these two benchmark applications implemented in this paper,
composed of ﬁve pipelined operations: image generation, pre-
view, image conversion, local/remote execution, and detection
result rendering. These two benchmark applications share the
same pipelined operations except Phase 4 (i.e., local execution
(yellow box) and remote execution (green box)). The third
application only executes the image generation and preview
(i.e., Phase 1 and 2).

Image Generation (Phase 1). The input to this phase is
continuous light signal and the output is an image frame.

(a) Mobile AR client and power mon-
itor

(b) Edge server and WiFi AP

Fig. 1. Overview of the developed testbed.

However, these solutions cannot be applied to improving the
energy efﬁciency of mobile devices in mobile AR ofﬂoading
cases. This is because (i) a variety of pre-processing tasks
in mobile AR executions, such as camera sampling, screen
rendering, and image conversion, are not taken into account
and (ii) besides the latency constraint that is considered in
most existing computation ofﬂoading approaches, detection
accuracy is also a key performance metric, which must be
considered while designing a mobile AR ofﬂoading solution.
In addition, although some existing work proposes to study the
tradeoffs between the mobile AR service latency and detection
accuracy [35], [44], [45], none of them considered (i) the
energy consumption of the mobile AR device and (ii) the
whole processing pipeline of mobile AR (i.e., starting from
camera sampling to obtaining detection results).

CPU Frequency Scaling. Our work is also related to
CPU frequency scaling. For modern mobile devices, such as
smartphones, CPU frequency and the voltage provided to the
CPU can be adjusted at run-time, which is called Dynamic
Voltage and Frequency Scaling (DVFS). Prior work [38], [46]–
[48] proposes various DVFS strategies to reduce the mobile
device energy consumption under various applications, such
as video streaming [38] and delay-tolerant applications [47].
However,
there have been
no efforts factoring in the energy efﬁciency of mobile AR
applications in the context of mobile device DVFS.

to the best of our knowledge,

III. PROPOSED METHODOLOGY

This section describes the overview of our developed testbed
for experimental studies,
implemented benchmark applica-
tions, our proposed energy measurement process, along with
the key performance metrics deﬁned to evaluate the perfor-
mance of the CNN-based object detection processing pipeline.

A. Overview of the Testbed

As shown in Fig. 1, our testbed consists of three major
components: mobile AR client (e.g., smartphone), edge server
attached to a WiFi access point (AP), and power monitor.

Mobile AR Client. We implement a mobile AR client on
a rooted Nexus 6 smartphone running Android 5.1.1 OS. It
is equipped with Qualcomm Snapdragon 805 SoC (System-
on-Chip). The CPU frequency ranges from 0.3 GHz to 2.649
GHz.

SmartphonePower MonitorEdge ServerWi-Fi Access PointFig. 2. Processing pipeline of the CNN-based object detection application implemented in this paper.

(a) Pipeline for the local execution

(b) Pipeline for the remote execution

Fig. 3. The diagrams of the pipelines for benchmark applications.

In this phase, the image sensor ﬁrst senses the intensity of
light and converts it into an electronic signal. A Bayer ﬁlter
is responsible for determining the color information. Then, an
image signal processor (ISP) takes the raw data from the image
senor and converts it into a high-quality image frame. The
ISP performs a series of image signal processing operations
to deliver a high-quality image, such as noise reduction,
color correction, and edge enhancement. In addition, the ISP
conducts automated selection of key camera control values
according to the environment (e.g., auto-focus (AF), auto-
exposure (AE), and auto-white-balance (AWB)). The whole
image generation pipeline in our benchmark applications
is constructed based on android.hardware.camera2
which is a package that provides an interface to individual
camera devices connected to an Android device. Captur-
eRequest is a class in android.hardware.camera2
that constructs the conﬁgurations for the capture hardware
the processing pipeline, and the
(sensor,
control algorithms. Therefore,
in our implemented bench-
mark applications, we use CaptureRequest to set up
image generation conﬁgurations. For example, Captur-
eRequest.CONTROL_AE_MODE_OFF disables AE and
CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE
sets the camera FPS (i.e., the number of frames that the camera
samples per second). In this paper, all default image processing
operations are enabled and the camera FPS is set to 15 fps.

lens, and ﬂash),

Preview (Phase 2). The input to this phase is a latest
generated image frame with YUV_420_888 format2 (i.e.,
the output of Phase 1) and the output is a camera preview

2For android.hardware.camera2, YUV_420_888 format is recom-

mended for YUV output [52].

rendered on a smartphone’s screen with a pre-deﬁned pre-
view resolution. In this phase,
the latest generated image
frame is ﬁrst resized to the desired preview resolution and
then buffered in a SurfaceTexture which is a class
capturing frames from an image stream (e.g., camera pre-
view or video decode) as an OpenGL ES texture. Finally,
the camera preview frame in SurfaceTexture is copied
and sent to a dedicated drawing surface, SurfaceView,
and rendered on the screen. In our benchmark applications,
the preview resolution is set via method SurfaceTex-
ture.setDefaultBufferSize(). In this paper, the pre-
view resolution is set to 800 × 600 pixels (different Android
devices may have different supported preview resolution sets).

is

(i.e.,

In this phase,

latest generated image

the output of Phase 1) and the output

Image Conversion (Phase 3). The input

an ImageReader class
the

to this phase
is a latest generated image frame with YUV_420_888
is
format
a cropped RGB image frame.
in order
to further process camera captured images (i.e., object
implemented
detection),
frame, where
to acquire
ImageReader.OnImageAvailableListener
provides a callback interface for being notiﬁed that a
new generated image
and method
ImageReader.acquireLatestImage() acquires the
latest image frame from the ImageReader’s queue while
dropping an older image. Additionally, the desired size and
format of acquired image frames are conﬁgured once an
ImageReader is created. In our benchmark applications,
the desired size and the preview resolution are the same
(800 × 600 pixels) and the image format in ImageReader
is set to YUV_420_888. Furthermore, an image converter is

available

frame

is

LightCamera LensImage sensorBayer filterImage signal processingScale & cropImage bufferYUV_420_888Surface TextureImage ReaderSensor parameters controlConvert YUVto RGB& crop5. Return Detection ResultsScreen rendering1. Image Generation3. Image ConversionMAR ClientEdge ServerImage transmissionWireless connectionCNNNoise reductionColor correctionEdge enhancement…Raw Bayer imageDarknet (YOLOv3)4. Local ExecutionTensorFlow Lite(MobileNetv1)2. Preview4. Remote Execution5. Detection Result RenderingBTimeIPIPIPIPIPIPIPIPCLERCIn parallel...Image Generation PreviewConversionLocal ExecutionBTimeIPIPIPIPIPIPIPIPCRERCIn parallel...Remote ExecutionResult RenderingBaseimplemented to convert the YUV_420_888 image to an RGB
image, because the input to a CNN-based object detection
model must be an RGB image. Two image conversion
methods are implemented in our benchmark applications: one
is Java-based and the other is C-based (we compare these two
methods in Section V-D). Finally, the converted RGB image is
cropped to the size of the CNN model for object detections.
Local/Remote Execution (Phase 4). The input to this phase
is a converted and cropped image frame (i.e., the output of
Phase 3) and the output is an object detection result. In our
benchmark applications, the object detection result contains
one or multiple bounding boxes with labels that identify the
locations and classiﬁcations of the objects in an image frame.
Each bounding box consists of 5 predictions: (x, y, w, h)
and a conﬁdence score [3]. The (x, y) coordinates represent
the center of the box relative to the bounds of the grid cell.
The (h, w) coordinates represent the height and width of the
bounding box relative to (x, y). The conﬁdence score reﬂects
how conﬁdent the CNN-based object detection model is on
the box containing an object and also how accurate it thinks
the box is what it predicts. (i) In the local execution, the
benchmark application is implemented with a light framework
called TensorFlow Lite [4] which is TensorFlow’s lightweight
solution for embedded and mobile devices. It runs a small
CNN model, called MobileNetv1 [36]. In order to run Mo-
bileNetv1 with different frame resolutions in TensorFlow Lite
on smartphones, we convert pre-trained MobileNetv1 SSD
models to TensorFlow Lite models (i.e., optimized FlatBuffer
format identiﬁed by the .tflite ﬁle extension). (ii) In the
remote execution, the benchmark application transmits the
converted and cropped image frame to the edge server through
a wireless TCP socket connection in real time. To avoid having
the server process stale frames, the application always sends
the latest generated frame to the server and waits to receive the
detection result before sending the next frame for processing.
Detection Result Rendering (Phase 5). The input to this
phase is the object detection result of an image frame (i.e.,
the output of Phase 4) and the output is a view with overlaid
augmented objects (speciﬁcally, overlaid bounding boxes and
labels in this paper) on top of the physical objects (e.g., a cup).

C. Energy Measurement Strategy

In order to measure the energy consumption of running
those two benchmark applications on a smartphone and obtain
the breakdown of energy consumed by each phase presented
in Fig. 2, we design a measurement strategy. The key idea
of the proposed measurement strategy is synchronizing the
recorded time in log ﬁles (saved by benchmark applications in
the tested Android smartphone) and power measurement data
(exported by the Monsoon power monitor). However, this is
very challenging, because the tested smartphone and the power
monitor do not share the same global clock. For example,
in Android smartphones, the recorded time of an event can

be counted by a system clock, uptimeMillis3, where the
clock is counted in milliseconds since the system is booted
(e.g., if an event happens 100 milliseconds after the system is
booted, the exported timestamp of the event in the log ﬁle is
100). On the other hand, in the power monitor, the timestamp
is counted in milliseconds since the power measurement is
launched.

Local Clock Synchronization & Event Localization. To
synchronize the exported timestamps of the Android smart-
phone and the power monitor, we propose to set up a ﬂag
event that can be tracked easily and accurately in both of them.
The touch event that launches the benchmark application is
selected as the ﬂag event to synchronize the timestamps. For
example, Fig. 4(a) illustrates the power consumption of the
tested smartphone recorded by a Monsoon power monitor. The
power measurement is launched at time 0 and the smartphone
only consumes the base power, described in Section III-A.
Then, we touch the icon of the benchmark application at time
0.192s (i.e., the moment that the benchmark application is
launched). On the other hand, Fig. 4(b) depicts the timestamps
recorded by the Android kernel4 when the touch event is
triggered, which denotes that
the touch event happens at
time 4716.264801s. After the timestamp of the ﬂag event is
acquired, the local clocks in the tested smartphone and the
power monitor can be synchronized easily and accurately. For
example, if the start and end time for executing an image
conversion are recorded as 4726.136s and 4726.612s in the
log ﬁle generated by the benchmark application, the power
consumption of the image conversion can be localized between
10.063s and 10.539s in the power measurement data recorded
by the power monitor. Fig. 5 presents an example of event
localization in the collected power measurement data through
our proposed strategy.

Script for Capturing the Touch Event. Since the touch
event happens before the benchmark application launching,
the function of capturing the touch event cannot be directly
added into the benchmark application. In addition, the tested
smartphone’s USB interface is automatically disabled when
the power measurement starts. Thus, the smartphone cannot be
instructed to start or terminate a touch event listener through
transmitting adb shell commands by a computer. Consid-
ering the above mentioned limitations, we design a lightweight
application and implement
to record touch events. The
it
function of this application is to run a touch event listener in
the background using an adb shell command getevent
-lt /dev/input/event0. We evaluate whether running
this background touch event listener will impact the power
consumption of benchmark applications. The measurement
results show that the average power consumption of the smart-

3This clock stops when the system enters deep sleep (CPU off, display dark,
and device waiting for external input), but is not affected by clock scaling,
idle, or other power saving mechanisms. Additionally, it is guaranteed to be
monotonic, and is suitable for interval timing when the interval does not span
device sleep.

4These timestamps are also generated by clock uptimeMillis but with

microsecond precision.

(a) Power consumption recorded by power monitor

(b) Timestamp of touch event recorded by smartphone

Fig. 4. An example of local clock synchronization.

Fig. 5. An example of event localization in the collected power measurement data (CPU governor: Interactive, remote execution with C-based image conversion
method).

Fig. 6. Power measurement and valid data collection process.

Fig. 7. A fragment of the power consumption of image generation and preview and base (CPU governor: Interactive).

phone is 3.721W (running the remote execution benchmark
application with the touch event listener) and 3.704W (running
the remote execution benchmark application without the touch
event listener), where the two measurements are under the
same conditions and each measurement runs for 5 minutes.
Therefore, the result demonstrates that our background touch
event listener has little impact on the power consumption of
benchmark applications.

Cooling-off Period. Furthermore, in order to mitigate the
interference from screen touching, application launching, cam-
era initializing, and CNN model loading in the collected power
measurement data, a cooling-off period is set up, as shown
in Fig. 6. In the cooling-off period, benchmark applications
only executes Phases 1 and 2 for generating a ﬁxed number
of image frames (e.g., 150 frames in this paper). After the
cooling-off period, benchmark applications start executing
the whole processing pipeline and generating valid power
consumption data.

Power/Energy Consumption Dissection. Fig. 3 illustrate
that image generation and preview (Phases 1 and 2), image
conversion and local/remote execution (Phases 3 and 4), and
base (e.g., OS and screen) are executed in parallel. The
workload of running our benchmark applications on the tested
smartphone is composed of these three parallel executions. In
addition, the power consumption is increased by the workload
increment. Therefore, our strategy for dissecting the power
consumption of each phase is:

1) Measuring the power consumption of the whole process-
ing pipeline, image generation and preview (Phases 1
and 2)5, and base separately with the same conﬁgurations
(e.g., CPU governor, camera sampling rate, and preview

5In order

to measure the power consumption of phases 1 and 2,
we implement an application that only executes image generation and
preview, where it uses the same Android camera package (i.e., an-
droid.hardware.camera2) and camera conﬁgurations (e.g., preview
resolution) with our benchmark applications.

00.511.5Time (s)02468Power (W)Touch the iconLaunch app, setup camera ... 0.192timestampstimestampsTouchLaunch...Cooling-off periodValid dateAn image generation and previewresolution) and conditions (e.g., background activity and
screen brightness). Figs. 5 and 7 present examples of how
the power consumption of these three parallel executions
look like.

2) Isolating the power consumption of (i) image generation
and preview + image conversion + base, (ii) image
generation and preview + local/remote execution + base,
and (iii) image generation and preview + others + base
through the proposed clock synchronization and event
localization strategy.

3) Obtaining the power consumption of image conversion,
local/remote execution, and others by subtracting the
average power consumption of image generation and pre-
view and base from cases (i), (ii), and (iii), respectively.
4) Obtaining the energy consumption of each phase via
calculating the integral of the power consumption over
the latency. For example, the energy consumption of an
image conversion is the sum of its power consumption
within an image conversion latency.

Validation. Paper [13] observed that a single energy mea-
surement could be misleading due to the variability in energy
consumption. Therefore, in this paper, all of our measurement
experiments are repeated multiple times, and each energy
consumption and latency result shown in Sections IV and
V is the mean value of completing 200 object detections.
Since we observe that the variance of the mean value of
the measured data, such as power consumption, per frame
latency, and per frame energy consumption, is negligible after
the number of the collected object detections is over 200 in
all of our measurement experiments, collecting measurement
results based on 200 object detection executions is good
enough for achieving stable and accurate results. Furthermore,
in order to ensure that each measurement is launched with a
clean environment, the benchmark application is re-installed
on the tested smartphone through Android Studio and the
data generated during the execution such as log ﬁles are
transferred to a workstation and removed from the smartphone
after each measurement, even though the conﬁguration of the
benchmark application does not require to be changed in the
next measurement.

D. Key Performance Metrics

We deﬁne three performance metrics to evaluate the perfor-
mance of the CNN-based object detection processing pipeline
implemented in this paper:

Per Frame Latency. The per frame latency is the total time
needed to obtain the detection results on one image frame
(i.e., usually shown as one or multiple bounding boxes that
identify the locations and classiﬁcations of the objects in a
frame). In this paper, it is deﬁned as the time period from
the moment the Image Reader acquires one camera captured
image frame to the moment the bounding boxes are drawn on
the mobile AR client’s screen, as depicted in Fig. 2. In the
local execution, the per frame latency includes the time used
for converting the YUV frame to the RGB frame, cropping the
frame to the ﬁtted resolution k ×k pixels, and executing CNN,

deﬁned as inference latency, on the smartphone. In the remote
execution, the per frame latency includes, besides the image
conversion and crop latency that are both executed locally on
the smartphone, the communication latency (i.e., transmitting
the frame and receiving the results) and the inference latency
on the edge server.

Per Frame Energy Consumption. The per frame energy
consumption is the total amount of energy consumed in
a mobile AR client by successfully performing the object
detection on one image frame. In the local execution, the
per frame energy consumption includes the energy consumed
by camera sampling (i.e., image generation), screen rendering
(i.e., preview),
inference, and operating
system (i.e., base). In the remote execution, it includes the
energy consumed by camera sampling, screen rendering, im-
age conversion, communication, and operating system. In a per
frame energy consumption, the image generation and preview
are usually executed multiple times (depends on the length
of the per frame latency), while the image conversion and
local/remote execution are executed only once.

image conversion,

Detection Accuracy. The mean average precision (mAP)
is a commonly used performance metric in object detection.
Better performance is indicated by a higher mAP value.
Speciﬁcally, the average precision [53] is computed as the area
under the precision/recall curve through numerical integration.
The mAP is the mean of the average precision across all
classes.

IV. EXPERIMENTAL RESULTS OF LOCAL EXECUTION

RQ 1. How is energy consumed when a CNN-based object
detection application is executed locally on a mobile AR
client? To answer this question, in this section, we describe
our efforts towards measuring and understanding the energy
consumption and the performance of running CNN models on
smartphones locally. We begin by measuring the per frame
latency and the per frame energy consumption of executing
CNN-based object detection under different smartphone’s CPU
governors in Section IV-A. In addition, we explore the impact
of the CNN model size on the per frame latency and the per
frame energy consumption in Section IV-B. Lastly, in Section
IV-C, we summarize the insights from our measurement stud-
ies and discuss potential research opportunities for improving
the energy efﬁciency of locally executing CNN-based object
detection on smartphones.

A. The Impact of CPU Governor

CPU Governor6 Dynamic voltage and frequency scaling
(DVFS) is a technique commonly used for dynamically ad-
justing the voltage and frequency of a mobile device’s CPU in
order to balance the trade-off between the power consumption
of the device and the required performance. In order to offer
DVFS, the CPU provides a set of valid voltages and frequen-
cies that can be dynamically selected by a power management

6We change the Android smartphone’s CPU governor manually by writ-
ing ﬁles in /sys/devices/system/cpu/[cpu#]/cpufreq/ scal-
ing_governor virtual ﬁle system with root privilege.

(a) Conservative

(b) Ondemand

(c) Interactive

(d) Userspace

(e) Powersave

(f) Performance

(g) Conservative

(h) Ondemand

(i) Interactive

(j) Userspace

(k) Powersave

(l) Performance

Fig. 8. CPU governor vs. per frame latency (CNN model size: 300 × 300 pixels).

(a) Conservative

(b) Ondemand

(c) Interactive

(d) Userspace

(e) Powersave

(f) Performance

(g) Conservative

(h) Ondemand

(i) Interactive

(j) Userspace

(k) Powersave

(l) Performance

(m) Conservative

(n) Ondemand

(o) Interactive

(p) Userspace

(q) Powersave

(r) Performance

Fig. 9. CPU governor vs. power and average per frame energy consumption (CNN model size: 300 × 300 pixels).

policy which is usually called a CPU governor. Different CPU
governors adjust the CPU voltage and frequency based on
variant criteria such as CPU usage. The six most popular CPU
governors are described as follows:

• Conservative governor: It adjusts the CPU frequency
based on the current usage and it biases the mobile device
to prefer the lowest possible CPU frequency as often as
possible. In other words, a large and persistent load can
be placed on the CPU only before the CPU frequency is
raised. Thus, the conservative governor is good for the
mobile device’s battery life.

• Ondemand governor: It adjusts the CPU frequency based
on the current usage, which is similar to the conservative

governor. However, the ondemand governor immediately
boosts the CPU to the highest possible frequency when
there is a load on the CPU and switches back to the lowest
possible frequency when the CPU is idle rather than
gradually increases and decreases the CPU frequency.
Thus, it offers excellent interface ﬂuidity due to its high-
frequency bias.

• Interactive governor: It is the default CPU governor for
most android mobile devices. Similar to conservative and
ondemand governors, it sets the CPU frequency based on
the current usage. However, the interactive governor is
designed for latency-sensitive and interactive workloads,
so it is more aggressive about scaling the CPU speed up

01020Video frame index00.511.500.511.52Conversion latencyInference latencyFPSOthersLatency (s)Latency (s)Frames per secondFrames per second01020Video frame index00.511.500.511.52Frames per secondFrames per secondLatency (s)Latency (s)01020Video frame index00.511.500.511.52Frames per secondFrames per secondLatency (s)Latency (s)051015Video frame index00.511.500.511.52Frames per secondFrames per secondLatency (s)Latency (s)123Video frame index01234500.51Frames per secondFrames per secondLatency (s)Latency (s)01020Video frame index00.511.500.511.52Frames per secondFrames per secondLatency (s)Latency (s)InferenceConversionBaseOthersPer frame total energy consumptionImage generation & previewTABLE I
LATENCY RESULTS OF THE LOCAL EXECUTION WITH DIFFERENT CPU GOVERNORS.

CPU Governor
Per Frame Latency (second)
Image Conversion Latency (second)
Inference Latency (Second)
Others (Second)

Conservative
0.915
0.436
0.424
0.055

Ondemand
0.904
0.425
0.431
0.047

Interactive
1.013
0.466
0.501
0.047

Userspace
1.444
0.693
0.699
0.052

Powersave
7.766
3.713
3.971
0.082

Performance
0.823
0.391
0.386
0.046

TABLE II
PER FRAME ENERGY CONSUMPTION RESULTS OF THE LOCAL EXECUTION WITH DIFFERENT CPU GOVERNORS.

CPU Governor
Power Consumption (watt)
Per Frame Energy Consumption (Joule)
Image Generation & Preview Energy Consumption (Joule)
Inference Energy Consumption (Joule)
Image Conversion Energy Consumption (Joule)
Base Energy Consumption (Joule)
Others (Joule)

Conservative
5.357
5.284
2.639
1.009
0.977
0.534
0.125

Ondemand
5.725
5.179
2.385
1.084
1.004
0.591
0.115

Interactive
5.415
5.487
2.622
1.153
0.992
0.621
0.099

Userspace
3.814
5.508
3.349
0.593
0.553
0.971
0.042

Powersave
2.308
17.926
12.552
0.432
0.380
4.555
0.007

Performance
6.115
5.037
2.281
1.028
0.983
0.622
0.123

in response to CPU-intensive activities.

• Userspace governor: It allows the user or any userspace
program to set the CPU to a speciﬁc frequency (i.e., the
CPU frequency is set to 1.497 GHz in this work), whereas
it only allows the CPU frequency to be set to predeﬁned
ﬁxed values.

• Powersave governor: It sets the CPU statically to the
lowest possible frequency to minimize the energy con-
sumption of the mobile device’s CPU.

• Performance governor: It sets the CPU statically to the
highest possible frequency to maximize the performance
of the mobile device’s CPU.

Per Frame Latency. We ﬁrst seek to investigate how
the CPU governor impacts the per frame latency of object
detection in the local execution scenario, where a CNN model
is executed on a smartphone and the model size is 300 × 300
pixels. The experimental results are shown in Fig. 8, where
Figs. 8(a)-8(f) depict the frequency variations of the tested
smartphone’s CPUs and Figs. 8(g)-8(l) illustrate the latency
of each phase in the object detection processing pipeline.
We show the latency of the two highest
time-consuming
phases, image conversion and inference latency, which comes
up to 95% of the per frame latency. We observe that (1)
as described above,
the conservative governor provides a
graceful CPU frequency increase, which causes temporarily
high per frame latency and low frame per second (FPS)
when the object detection application is launched, as shown
in Figs. 8(a) and 8(g). This observation demonstrates that
the conservative governor may not be suitable for CNN-
based object detection applications because object detection
requires a high ﬂuidity to interact with the user. (2) Although
both ondemand and interactive governors provide aggressive
responses to the execution of object detection, as depicted in
Figs. 8(b) and 8(c), the ondemand governor offers a relatively
steadier latency performance than the interactive governor due
to its high-frequency bias. In Figs. 8(d), 8(e), and 8(f), the CPU
is set to a user-deﬁned, the lowest, and the highest possible

frequencies, respectively. (3) It is not surprising to ﬁnd that
the powersave governor is the worst-performing governor in
terms of the latency, where its per frame latency is almost
eight times higher than that of the performance governor. (4)
The performance governor outperforms other presented CPU
governors in terms of latency, and the measured average per
frame latency is shown in Table I.

Per Frame Energy Consumption. We next examine how
the CPU governor impacts the per frame energy consumption
of executing object detection on the smartphone. The experi-
mental results are shown in Fig. 9, where Figs. 9(a)-9(f) depict
the power consumption; Figs. 9(g)-9(l) illustrate the average
per frame energy consumption; and Figs. 9(m)-9(r) depict
the average percentage breakdown of energy consumed by
each phase in the processing pipeline. We make the following
observations. (5) The performance governor consumes the
highest power consumption, as shown in Fig. 9(f), because
the processors always run with the highest possible CPU
frequency. Although it is capable of providing the best la-
tency performance, continuously run with the highest CPU
frequency may cause the smartphone overheating and trigger
CPU throttling mechanisms to avoid thermal emergencies by
sacriﬁcing the performance. (6) Interestingly, the performance
governor provides the lowest per frame energy consumption,
while the powersave governor offers the highest per frame
energy consumption, as shown in Table II. This observation
indicates a critical trade-off between the battery life (i.e.,
power consumption) and per frame energy consumption in
CNN-based object detection applications.

In order to dissect

the energy drain through different
processing pipeline phases, we break down the per frame
energy consumption as follows: image generation and preview,
inference, image conversion, base, and others. We ﬁnd that (7)
the image generation and preview phase always contributes
the highest energy consumption (i.e., approximately 45.3% -
70.0%). The reason it consumes considerably high energy is
executing the 3A (i.e., AF, AE, and AWB) and multiple ﬁne-

(a) 1002 pixels

(b) 2002 pixels

(c) 3002 pixels

(d) 4002 pixels

(e) 5002 pixels

(f) 6002 pixels

(g) Per frame latency

(h) Image conversion latency

(i) Inference latency

Fig. 10. CNN model size vs. CPU frequency & latency (CPU governor: interactive).

(a) Per frame latency

(b) Image conversion latency

(c) Inference latency

Fig. 11. CNN model size vs. latency (CPU governor: performance).

grained image post processing algorithms (e.g., noise reduction
(NR), color correction (CC), and edge enhancement (EE)) on
ISP. These sophisticated algorithms are designed to make an
image that is captured by the smartphone camera look perfect.
However,
is it always necessary for the camera captured
frame to be processed by all of those energy-hungry image
processing algorithms in order to achieve a successful object
detection result? In addition, the number of frames captured
by the camera per second is a ﬁxed value (e.g., 24 or 30
[7, 30] frames/second),
frames/second) or in a range (e.g.,
which is controlled by the AE algorithm. However, due to
the limited computation capacity of smartphones, usually the
detection FPS is far slower than the camera capture frame rate.
On the other hand, the CNN always extracts the latest camera
captured frame, which indicates that, from the perspective of
the energy efﬁciency of the object detection pipeline, capturing
frames with a fast rate is unnecessary and energy-inefﬁcient.
Therefore, both raising CPU frequency and decreasing camera
capture frame rate are efﬁcient approaches to reduce the
energy consumption of image generation and preview.

Besides the energy consumption of image generation and
preview, inference and image conversion phases consume a
large amount of energy, as depicted in Fig. 9 and Table II.
(8) Although a low CPU frequency incurs high per frame
energy consumption, it decreases the energy consumption of

both inference and image conversion phases. This observation
indicates that there is a trade-off between the energy consump-
tion reduction of image generation and preview phases and in-
ference and image conversion phases. For example, raising the
CPU frequency can decrease the energy consumption of image
generation and preview phases but concurrently increases the
energy consumption of inference and image conversion phases.
Furthermore, (9) the conservative governor provides the lowest
base energy consumption, which demonstrates that existing
CPU governors are capable of scaling CPU frequency for the
workload of the smartphone’s operating system.

B. The Impact of CNN Model Size

CNN Model Size. Recently, CNN-based methods have
become the leading approach for achieving high quality object
detection. The CNN model size determines the detection
accuracy (i.e., mAP). Increasing the CNN model size always
results in a gain of mAP [54], [55]. In this section, we
seek to investigate how the CNN model size impacts the per
frame latency and energy consumption of executing the object
detection on the smartphone.

Per Frame Latency. In this experiment, we implement the
MobileNets [36] with six different CNN model sizes (i.e.,
from 100 × 100 to 600 × 600 pixels). Figs. 10 and 11 depict
the latency results of running CNN-based object detection
with different model sizes, where the smartphone works on

mAP = 19.3mAP = 19.3(a) 1002 pixels

(b) 2002 pixels

(c) 3002 pixels

(d) 4002 pixels

(e) 5002 pixels

(f) 6002 pixels

(g) 1002 pixels

(h) 2002 pixels

(i) 3002 pixels

(j) 4002 pixels

(k) 5002 pixels

(l) 6002 pixels

Fig. 12. CNN model size vs. per frame energy consumption (CPU governor: interactive).

(a) 1002 pixels

(b) 2002 pixels

(c) 3002 pixels

(d) 4002 pixels

(e) 5002 pixels

(f) 6002 pixels

(g) 1002 pixels

(h) 2002 pixels

(i) 3002 pixels

(j) 4002 pixels

(k) 5002 pixels

(l) 6002 pixels

Fig. 13. CNN model size vs. per frame energy consumption (CPU governor: performance).

interactive and performance governors, respectively. We make
the following observations. (10) Running a large CNN model
increases the average CPU frequency under the interactive
governor, as shown in Figs. 10(a)-10(f). This observation
demonstrates that a larger CNN model will generate more
workload on the smartphone’s CPU. (11) A larger CNN model
always results in a higher per frame latency for both interactive
and performance governors, as depicted in Figs. 10(g) and
11(a), where the per frame latency of the interactive and
performance boosts 220% and 247%, respectively, when the
CNN model size increases from 100×100 to 600×600 pixels.
(12) The per frame latency increment is mainly from the raise
of the inference latency, while the image conversion latency
does not vary much when the CNN model size increases, as
illustrated in Figs. 10(h), 10(i), 11(b), and 11(c). This is be-
cause no matter what the CNN model size k ×k is conﬁgured,
every YUV frame is converted to an RGB frame with the
preview resolution k1 × k2 ﬁrst. After the image conversion is
completed, the RGB frame is resized to k × k pixels. (13) For
each CNN model size, the performance governor provides a
lower per frame latency (i.e., 14%-21%) than the interactive
governor, which demonstrates that our observation (4) can be
applied to diverse CNN model sizes.

Per Frame Energy Consumption. We next examine how
the CNN model size impacts the per frame energy consump-
tion of executing object detection on the smartphone. Figs.
12 and 13 depict the measured per frame energy consumption
results of running CNN-based object detection with different
model sizes, where the smartphone works on interactive and
performance governors, respectively. Figs. 12(a)-12(f) and
13(a)-13(f) illustrate the average per frame energy consump-
tion; and Figs. 12(g)-12(l) and Figs. 13(g)-13(l) depict the
average percentage breakdown of energy consumed by each
phase in the processing pipeline. We observe that (14) the per
frame energy consumption grows dramatically as the CNN
model size increases, which is mainly contributed by the
inference energy consumption increment. For example, the
inference energy consumption accounts for 4.7% and 38.0%
of the per frame energy consumption when the CNN model
size is 100 × 100 and 600 × 600 pixels, respectively, as
shown in Fig. 12. In addition, although increasing the CNN
model size always results in a gain of mAP, the gain of mAP
becomes smaller as the increase of the model size [3]. This
observation inspires us to trade mAP for the per frame energy
consumption reduction when the CNN model size is large.
(15) There is a reduction in the proportion of the energy

InferenceConversionBaseOthersPer frame total energy consumptionImage generation & previewInferenceConversionBaseOthersPer frame total energy consumptionImage generation & previewconsumption of both the image generation and preview phase
and base phase when the CNN model size grows. As we
discussed in Section IV-A, a large proportion of the image
generation and preview energy consumption to the per frame
energy consumption may result in the smartphone expending
signiﬁcant reactive energy for sampling non-detectable image
frames. These two observations indicate that there is a trade-
off between reducing the per frame energy consumption and
decreasing the proportion of the reactive energy. Therefore,
a comprehensive approach for improving the energy efﬁciency
of executing CNN-based object detection on smartphones must
take into account the reduction of both the per frame energy
consumption and the proportion of the reactive energy.

C. Insights and Research Opportunities

Insights.
• Ondemand and performance CPU governors achieve
lower per frame energy consumption and latency than the
other four popular CPU governors when the smartphone
locally executes the CNN-based object detection applica-
tion. However, as the smartphone’s CPUs keep running
at the highest frequency in the performance governor,
it may cause the smartphone overheating and trigger
CPU throttling mechanism to avoid thermal emergencies
by sacriﬁcing the performance. Therefore, the ondemand
governor is recommended as the default CPU governor
of the local execution, which supports sustainable and
low per frame energy consumption and latency object
detections.

• Both the CPU governor (i.e., CPU frequency) and the
CNN model size signiﬁcantly impact the per frame la-
tency and energy consumption. However, simply increas-
ing the CPU frequency or decreasing the CNN model
size is inadequate to minimize the per frame energy
consumption because different phases may have opposite
reactions.

• Increasing the CNN model size always results in a gain
of mAP and an increment of the per frame energy
consumption. However, the amount of the increment of
mAP becomes smaller as the increase of the model size,
while the increment of the per frame energy consumption
becomes larger as the increase of the model size. There-
fore, this observation inspires us to trade mAP for the
per frame energy consumption reduction when the CNN
model size is large.

• In order to improve the energy efﬁciency of smartphones
that locally execute the CNN-based object detection, we
must jointly consider the per frame energy consumption,
the proportion of the reactive energy, and the battery life.

Research Opportunities.
• Current CPU governors cannot achieve energy-efﬁcient
object detection on smartphones (i.e., jointly considering
the per frame energy consumption, the proportion of the
reactive energy, and the battery life). A CPU governor
speciﬁcally designed for CNN-based object detection
applications is critical and desirable.

• An intelligent conﬁguration adaption algorithm that is
capable of selecting the best combination of the CPU
governor, CNN model size, and camera sample rate
according to the smartphone’s battery life, processor’s
temperature, and detection accuracy requirement might
be a potential solution for achieving energy-efﬁcient and
high-performance object detection.

V. EXPERIMENTAL RESULTS OF REMOTE EXECUTION

RQ 2. Does ofﬂoading the object detection tasks to a
powerful infrastructure signiﬁcantly decrease both the energy
consumption and latency? To answer this question, in this
section, we describe the experimental results on evaluating
the impact of various factors on the energy consumption of a
mobile AR client, latency, and detection accuracy of remotely
executing CNN-based object detection on smartphones. We
begin by measuring the per frame latency and the per frame
energy consumption of executing CNN-based object detection
under different smartphone’s CPU governors in Section V-A.
In addition, we explore the impact of the CNN model size
on the per frame latency and the per frame energy consump-
tion in Section V-B. Furthermore, the image generation and
preview phase and image conversion phase are discussed in
Sections V-C and V-D, respectively. Lastly, in Section V-E,
we summarize the insights from our measurement studies
and discuss potential research opportunities for improving the
energy efﬁciency of remotely executing CNN-based object
detection on smartphones.

A. The Impact of CPU Governor

Per Frame Latency. We ﬁrst seek to investigate how
the CPU governor impacts the per frame latency of object
detection in the remote execution scenario, where a CNN
model
is executed on the implemented edge server with
a 5GHz WiFi link to the smartphone. The executed CNN
model size is 320 × 320 pixels. The experimental results are
shown in Fig. 14, where Figs. 14(a)-14(f) depict the frequency
variations of the tested smartphone’s CPUs and Figs. 14(g)-
14(l) illustrate the latency of each phase in the object detection
processing pipeline. Compared to the local execution, a new
time-consuming phase named communication is introduced
into the processing pipeline of the remote execution besides
image conversion and inference phases. We obtain similar
observations to (3) and (4). In addition, (16) Fig. 14(a) shows
that only one core of the smartphone’s processor reaches to the
highest possible frequency under the conservative governor,
which indicates that running CNN models on the edge server is
capable of reducing the workload on the smartphone’s CPUs.
(17) However, because of the workload reduction and the con-
servative governor’s low-frequency bias, the per frame latency
of the remote execution under the conservative governor is
approximately 10.3% larger than that of the local execution,
as shown in Table III. This observation demonstrates that the
conservative governor is not suitable for the remote execution
either and performs worse in the remote execution.

(a) Conservative

(b) Ondemand

(c) Interactive

(d) Userspace

(e) Powersave

(f) Performance

(g) Conservative

(h) Ondemand

(i) Interactive

(j) Userspace

(k) Powersave

(l) Performance

Fig. 14. CPU governor vs. per frame latency (CNN model size: 320 × 320 pixels).

TABLE III
LATENCY RESULTS OF THE REMOTE EXECUTION WITH DIFFERENT CPU GOVERNORS.

CPU Governor
Per Frame Latency (second)
Image Conversion Latency (second)
Inference Latency (Second)
Communication Latency (Second)
Others (Second)
Per Frame Latency Reduction (%)

Conservative
1.009
0.571
0.189
0.205
0.044
-10.3

Ondemand
0.853
0.409
0.189
0.210
0.045
5.6

Interactive
0.905
0.477
0.179
0.200
0.049
10.7

Userspace
1.075
0.666
0.180
0.174
0.055
25.6

Powersave
4.214
3.622
0.185
0.293
0.114
45.7

Performance
0.819
0.376
0.192
0.207
0.044
0.5

TABLE IV
SMARTPHONES AND THE EDGE SERVER USED IN THIS EXPERIMENT.

Manufacturer

Samsung

Google

Asus

Nvidia

Model
OS
SoC
CPU

GPU

Galaxy S5
Android 6.0.1
Snapdragon 801 (28 nm)
32-bit 4-core 2.5GHz Krait 400

Nexus 6
Android 5.1.1
Snapdragon 805 (28 nm)
32-bit 4-core 2.7GHz Krait 450

ZenFone AR
Android 7.0
Snapdragon 821 (14 nm)
64-bit 4-core 2.4GHz Kryo

578MHz Adreno 330

600MHz Adreno 420

653MHz Adreno 530

RAM
WiFi
Release date

2GB
802.11n/ac, MIMO 2 × 2
April 2014

3GB
802.11n/ac, MIMO 2 × 2
November 2014

6GB
802.11n/ac/ad, MIMO 2 × 2
July 2017

Jetson AGX Xavier
Ubuntu 18.04 LTS aarch64
Xavier
64-bit 8-core 2.26GHz Carmel
512-core 1377MHz Volta
with 64-TensorCores
16GB
—
September 2018

(18)

Interestingly, we ﬁnd that

the remote execution
achieves signiﬁcantly distinct per frame latency reduction
when the smartphone works on different CPU governors. For
example, as shown in Table III, the remote execution achieves
a per frame latency reduction of 45.7% in the powersave
governor compared to the local execution, while it only obtains
a per frame latency reduction of 0.5% in the performance
governor. This observation may infer that locally executing
CNN-based object detection on the smartphone with advanced
processors and working on a high CPU frequency is capable
of achieving a comparable latency performance as the remote
execution. This inference is important for guiding whether a
smartphone has to ofﬂoad its object detection tasks to the edge
server for reducing the service latency.

In order to verify this inference, we conduct a measurement

TABLE V
CLASSIFICATION & LATENCY RESULTS OF DIFFERENT SMARTPHONES.

Smartphone
CPU Score
GPU Score
Image Processing Score
Total Score
Class
Local
Per Frame
Remote
Latency (s)
Latency Reduction (%)

S5
36871
6678
3103
66414
Low-end
1.098
0.956
12.9

Nexus 6
37521
18063
6862
80047
Low-end
1.013
0.905
10.7

ZenFone AR
58531
67286
11321
173472
High-end
0.225
0.312
-38.7

study using three smartphones with different computation
capacities, where their characteristics are summarized in Table
IV. We classify them into two classes, low-end and high-end

05101520Time (s)0123CPU frequency (GHz)CPU0CPU1CPU2CPU305101520Time (s)0123CPU frequency (GHz)CPU0CPU1CPU2CPU3(a) Conservative

(b) Ondemand

(c) Interactive

(d) Userspace

(e) Powersave

(f) Performance

(g) Conservative

(h) Ondemand

(i) Interactive

(j) Userspace

(k) Powersave

(l) Performance

(m) Conservative

(n) Ondemand

(o) Interactive

(p) Userspace

(q) Powersave

(r) Performance

Fig. 15. CPU governor vs. power and average per frame energy consumption (CNN model size: 320 × 320 pixels).

smartphones, according to their general hardware performance
tested by using an Antutu benchmark [56]. The testing results
are shown in Table V. All these three smartphones work on the
interactive governor. The results verify our inference above,
where the per frame latency of the low-end smartphones is
decreased around 12%, whereas the per frame latency of the
high-end smartphone is increased approximately 38.7% when
ofﬂoading the object detection tasks to the edge server (note
that the value of the latency reduction may differ depending
on how powerful the edge server’s GPU is). This observation
supports the fact that lots of recently released smartphones
with high computation power possess the capability of running
a light CNN model with low latency. However, the detection
accuracy of the large CNN model on the edge server is
better than that of the light CNN model on the smartphone
(e.g., mAP = 51.5 on the server and mAP = 19.3 on the
smartphone when the frame resolution is around 300 × 300
pixels). Furthermore, in general, different use cases may have
variant latency/accuracy requirements. For example, the AR
cognitive assistance case where a high-end wearable device
helps visually impaired people to navigate on a street may
need a low latency but can tolerate a relatively high number
of false positives (i.e., false alarms are ﬁne but missing any
potential threats on the street is costly) [37]. In contrast, an
AR used for recommending products in shopping malls or
supermarkets may tolerate a relatively long latency but require
high detection accuracy. Therefore, both the smartphone’s
computation capacity and the use case should be considered
when determining the appropriate execution approach (i.e.,
local or remote).

Per Frame Energy Consumption. We next explore how

the CPU governor impacts the per frame energy consumption
of object detection in the remote execution scenario, where
the smartphone works on the interactive CPU governor. The
experimental results are shown in Fig. 15, where Figs. 15(a)-
15(f) depict the power consumption; Figs. 15(g)-15(l) illustrate
the average per frame energy consumption; and Figs. 15(m)-
15(r) show the average percentage breakdown of energy con-
sumed by each phase in the processing pipeline. We ﬁnd that
(19) the remote execution decreases the power consumption
compared to the local execution when the smartphone works
on conservative, ondemand, interactive, and performance CPU
governors. However, when the smartphone works on userspace
and powersave CPU governors, the remote execution con-
sumes more power than the local execution, as shown in
Table VI. This observation is a supplement to observation (16),
which indicates that (i) ofﬂoading the object detection tasks
to the edge server may not be able to reduce the workload on
the smartphone when the smartphone’s CPUs run at a low fre-
quency; (ii) the communication phase (i.e., remote execution)
is more power-consuming than the inference phase (i.e., local
execution) when the CPU frequency is low. (20) As depicted in
Figs. 15(g)-15(l) and Table VI, the remote execution is capable
of reducing the per frame energy consumption compared to
the local execution when the smartphone works on these six
tested CPU governors. In addition, observation (18) and its
corresponding inference are also applicable for the per frame
energy consumption.

Interestingly, (21) the userspace governor (i.e., the CPU
frequency is set to 1.49GHz) achieves the lowest per frame
energy consumption in the remote execution, as illustrated in
Figs. 15(g)-15(l) and Table VI. This observation is different

ConversionCommunicationBaseOthersPer frame total energy consumptionImage generation & previewTABLE VI
PER FRAME ENERGY CONSUMPTION RESULTS OF THE REMOTE EXECUTION WITH DIFFERENT CPU GOVERNORS.

CPU Governor
Power Consumption (watt)
Per Frame Energy Consumption (Joule)
Power Consumption Reduction (%)
Per Frame Energy Consumption Reduction (%)

Conservative
4.579
4.620
14.5
12.6

Ondemand
5.074
4.328
11.4
16.4

Interactive
4.728
4.278
12.7
22.0

Userspace
3.867
4.157
-1.4
24.5

Powersave
2.410
10.156
-4.4
43.4

Performance
5.341
4.375
12.7
13.1

(a) CPU frequency vs. per frame latency

(b) CPU frequency vs. power consumption

(c) CPU frequency vs. per frame energy con-
sumption

Fig. 16. Performance variations with increasing the CPU frequency in remote execution (CNN model size: 320 × 320 pixels).

from the local execution, where the CPU with the highest
frequency achieves the lowest per frame energy consumption.
We conduct an experiment study to explore the reason, where
we set the test smartphone to the userspace governor and
gradually raise its CPU frequency from the lowest to the
highest. The experimental results are shown in Fig. 16. We
ﬁnd that (22) the higher the CPU frequency, the lower per
frame latency the smartphone derives and the higher power
it consumes. However, the reduction of the per frame latency
and the increase of the power consumption are disproportional,
as depicted in Figs. 16(a) and 16(b). For example, as com-
pared to 2.26GHz, 2.64GHz only reduces about 5% latency
but increases about 14% power consumption. As compared
to 0.3GHz, 0.72GHz reduces about 55% latency but only
increases about 24% power consumption. This observation
advocates adapting the smartphone’s CPU frequency for the
per frame latency reduction by trading as little increase of
the per frame energy consumption as possible. For example,
Fig. 16(c) illustrates that selecting the CPU frequency around
2.26GHz achieves the lowest per frame energy consumption, a
comparable per frame latency, and a lower power consumption
compared to 2.64GHz.

B. The Impact of CNN Model Size

Per Frame Latency. In this experiment, we implement six
object detection models based on the YOLOv3 framework
[3] with different CNN model sizes (i.e., from 128 × 128 to
608 × 608 pixels). The test smartphone works on the default
CPU governor,
interactive. Fig. 17 depicts the per frame
latency of running CNN-based object detection with different
model sizes in the remote execution. We make the following
observations. (23) In contrary to the local execution, raising the
CNN model size in the remote execution decreases the average
CPU frequency, as shown in Figs. 17(a)-17(f). This is because

the smartphone experiences a relatively long idle period (i.e.,
waiting for the detection results from the edge server) when
the CNN model size is large (i.e., a long inference latency
at the edge server side). In WiFi networks, when transmitting
a single image frame, the smartphone’s wireless interface ex-
periences four phases: promotion, data transmission, tail, and
idle. When an image transmission request comes, the wireless
interface enters the promotion phase. Then, it enters the data
transmission phase to send the image frame to the edge server.
After completing the transmission, the wireless interface is
forced to stay in the tail phase for a ﬁxed duration and waits
for other data transmission requests and the detection results.
If the smartphone does not receive the detection result in the
tail phase, it enters the idle phase and waits for the feedback
from its associated edge server. Therefore, in contrary to the
local execution, using a large CNN model size in the remote
execution can extend the battery life and improve the detection
accuracy. (24) Similar to the local execution, a larger CNN
model size always results in a higher per frame latency in the
remote execution, where the per frame latency increment is
mainly from the raise of the communication and the inference
latency, as shown in Figs. 17(g)-17(j). In addition, Fig. 17(k)
depicts the detection accuracy of the YOLO under different
CNN model sizes, where the detection accuracy is deﬁned
as the ratio of the number of correctly recognized objects to
that of the total objects in an image frame (on calculating the
accuracy, we assume that the YOLO is capable of detecting
all objects in an image frame when the CNN model size is
608 × 608 pixels). We ﬁnd that (25) although a higher CNN
model size enables a better detection accuracy, the accuracy
gain narrows down at a high CNN model size. However, (26)
the speed of the per frame latency and the inference latency
increases becomes faster at a higher CNN model size, as
illustrated in Figs. 17(g) and 17(j). These two observations

(a) 1282 pixels

(b) 2242 pixels

(c) 3202 pixels

(d) 4162 pixels

(e) 5122 pixels

(f) 6082 pixels

(g) Per frame latency

(h) Image conversion latency

(i) Communication latency

(j) Inference latency

(k) Detection accuracy

Fig. 17. CNN model size vs. CPU frequency, latency, and detection accuracy (CPU governor: interactive).

(a) 1282 pixels

(b) 2242 pixels

(c) 3202 pixels

(d) 4162 pixels

(e) 5122 pixels

(f) 6082 pixels

(g) 1282 pixels

(h) 2242 pixels

(i) 3202 pixels

(j) 4162 pixels

(k) 5122 pixels

(l) 6082 pixels

Fig. 18. CNN model size vs. per frame energy consumption (CPU governor: interactive).

inspire us to trade detection accuracy (i.e., mAP) for the per
frame latency reduction when the CNN model size is large.

Per Frame Energy Consumption. We next investigate
how the CNN model size impacts the per frame energy
consumption in the remote execution. Fig. 18 shows the
measured energy consumption results, where the smartphone
works on the interactive CPU governor. We observe that (27)
the remote execution saves approximately 52.5% per frame
energy on average when the frame resolution is 608 × 608
pixels, as shown in Table VII. However, it consumes slightly
more per frame energy than the local execution when the frame
resolution is 128 × 128 pixels. This observation is rather sig-
niﬁcant, which demonstrates that running CNN-based object
detection remotely does not always consume less energy than
the local execution. In addition, (28) the larger the CNN model
size, the more per frame energy reduction the remote execution
derives compared to the local execution. For example, running
a 224 × 224 pixels model only reduces about 14.1% per frame
energy, while executing a 608 × 608 pixels model decreases
about 52.5% per frame energy consumption. Therefore, in
order to take the best advantage of the remote execution,
executing a CNN with a larger model size is recommended.

C. The Impact of Image Generation and Preview

RQ 3. Besides the network condition, what else impacts the
energy consumption and latency when executed remotely, and
how? As we presented in the aforementioned observations, the
image generation and preview is the most energy-consuming
phase in both local and remote execution scenarios. Thus,
to improve the energy efﬁciency of the object detection pro-
cessing pipeline, we must reduce the energy consumption of
image generation and preview phases. We seek to understand
the interactions between the power consumption and various
factors (e.g., the preview resolution, 3A, and several image
post processing algorithms) as follows.

Preview Resolution vs. Power Consumption. We ﬁrst
examine how the preview resolution inﬂuences the power con-
sumption of image generation and preview phases, as shown in
Fig. 19(a). We ﬁnd that (29) as the preview resolution grows,
the power consumption increases dramatically. Therefore, a
preview with a higher frame resolution on the smartphone
provides a better quality preview for users, but at the expense
of battery drain, which is applicable for both local and remote
execution cases.

Camera FPS vs. Power Consumption. We next vary

mAP = 51.5InferenceConversionBaseOthersPer frame total energy consumptionImage generation & previewTABLE VII
PER FRAME ENERGY CONSUMPTION RESULTS OF THE REMOTE EXECUTION WITH DIFFERENT CNN MODEL SIZES.

CNN Model Size (pixels)

Per Frame Energy Consumption (J)

Local
Remote
Per Frame Energy Consumption Reduction (%)

128 × 128
3.584
3.586
0

224 × 224
4.210
3.616
14.1

320 × 320
5.923
4.242
28.4

416 × 416
7.961
4.736
40.5

512 × 512
11.169
6.111
45.3

608 × 608
13.699
6.508
52.5

(a) Preview resolution vs. power consumption.

(b) Camera FPS vs. power consumption.

(c) Camera FPS vs. sampling efﬁciency.

(d) 3A and image post processing algorithms
vs. power consumption.

(e) Comparison of the per frame energy con-
sumption.

Fig. 19. Power consumption analyses of image generation and preview phases (remote execution).

the smartphone’s camera FPS to explore how it
impacts
the device’s power consumption, where the camera FPS is
deﬁned as the number of frames that the camera samples
per second. Fig. 19(b) shows that (30) a large camera FPS
leads to a high power consumption. However, as shown in
Fig. 2, not every camera captured image frame is sent to
the edge server for detection. Because of the need (i) to
avoid the processing of stale frames and (ii) to decrease
the transmission energy consumption, only the latest camera
sampled image frame is transmitted to the server. This may
result in the smartphone expending signiﬁcant reactive power
for sampling non-detectable image frames. In Fig. 19(c), we
quantify the sampling efﬁciency with the variation of the
camera FPS. As we expected, (31) a large camera FPS leads
to a lower sampling efﬁciency (e.g., less than 2% of the power
is consumed for sampling the detectable image frames when
the camera FPS is set to 30). However, in most mobile AR
applications, users usually request a high camera FPS for a
smoother preview experience, which is critical for tracking
targets in physical environments. Interestingly, (32) increasing
CPU frequency can reduce the reactive power for sampling, as
shown in Fig. 19(c). These observations demonstrate that when
a high camera FPS is requested, increasing CPU frequency
can promote the sampling efﬁciency but may also boost the
power consumption. Therefore, ﬁnding a CPU frequency that
can balance this tradeoff is critical.

Image Post Processing and 3A Algorithms vs. Power

Consumption. Lastly, we examine the effect of multiple image
post processing and 3A algorithms on the power consumption
of image generation and preview phases, as shown in Figs.
19(d) and 19(e). Note that when the AE is disabled, we
manually set the camera ISO and exposure time to 400 and
20 ms, respectively. We observe that (33) disabling the 3A,
NR, CC, and EE algorithms decreases the power consumption
by 14.8%. We conduct another experiment to understand if
disabling these algorithms would impact the object detection
performance. As shown in Fig. 20, (34) the detection perfor-
mance does not degrade. Furthermore, we compare the per
frame energy consumption among three cases, as depicted
in Fig. 19(e): all enabled with camera capture frame rate
30, all disabled with camera capture frame rate 30, and all
disabled with camera capture frame rate 5. We ﬁnd that (35)
the per frame energy consumption of the second and the third
cases decreases by approximately 10% and 27%, respectively,
compared to the ﬁrst case. Therefore, these three observations
may answer the question that we presented in Section IV-A:
these energy-hungry image post processing algorithms may
not be necessary for camera captured image frames to achieve
successful object detection results.

D. The Impact of the Image Conversion Method

As depicted in Tables I and III,

the image conversion
phase is one of the most time-consuming phases in both local
and remote execution scenarios. This is because the image

Preview resolution (pixels)33.544.555.5Power consumption (W)Power consumption of the image generationand preview phasePreview resolution (pixels)33.544.555.5Power consumption (W)Power consumption of the image generationand preview phase2510152430Camera FPS (frames/second)11.522.53Power consumption (W)Power consumption of the image generationand preview phase0123Power consumption (W)01234All on, 30 FPSAll off, 30 FPSAll off, 5 FPSTABLE VIII
IMAGE CONVERSION LATENCY RESULTS WITH DIFFERENT CONVERSION METHODS.

Preview Resolution (pixels)

Conversion Latency (ms)

Latency Reduction (%)

Java
C

320 × 240
130.1
14.7
88.7

352 × 288
175.9
16.4
90.7

640 × 480
484.9
40.3
91.7

720 × 480
539.0
48.3
91.0

800 × 480
596.6
48.4
91.9

800 × 600
711.9
54.4
92.4

1024 × 768
1157.1
82.6
92.9

• The energy consumption of the communication phase
becomes the second largest portion of the per frame
energy consumption when the frame resolution of the
ofﬂoaded image is large (determined by the CNN model
size). Thus, improving the image transmission energy
efﬁciency is a potential research issue for the remote exe-
cution. For example, as we presented, when transmitting
an image frame, the mobile AR client’s wireless interface
experiences four phases: promotion, data transmission,
tail, and idle. After completing the transmission,
the
wireless interface is forced to stay in the tail phase for
a ﬁxed duration and waits for other data transmission
requests and the detection results. Therefore, developing
a mechanism that can adaptively adjust the duration of
the tail phase based on the predicted inference latency at
the edge server and background activities of the mobile
AR client may possibly improve the energy efﬁciency of
the mobile AR client by allowing it to enter the idle phase
faster.

• Although our experimental results indicate that some
energy-consuming image post processing algorithms may
not be necessary for mobile AR clients to achieve
successful object detection results, more comprehensive
studies are required to investigate this issue. For example,
is this result inﬂuenced by other factors, such as the
object category, frame resolution, and object detection
algorithm?

VI. THREATS TO VALIDITY

External validity. External validity can be criticized by us-
ing a single version of Android OS on the tested Google Nexus
6 smartphone. The threat is mitigated by running our bench-
mark applications on multiple smartphones with signiﬁcantly
different computation capacity (i.e., high-end and low-end), as
described in Table IV. Furthermore, although the numerical
values of our measurement with a speciﬁc experiment con-
ﬁguration (e.g., the per frame energy consumption of locally
executing a 320 × 320 CNN model with Interactive CPU gov-
ernor) cannot be generalized to all possible smartphones, such
as iPhone 12 and Samsung Galaxy Note20, this paper focuses
on investigating the trend of how the smartphone’s energy
consumption may vary when our benchmark applications are
executed with different conﬁgurations, which will help predict
variations on the energy consumption of other smartphones. In
addition, an architecture-level comparison is not conducted in
this paper (e.g., comparing the energy efﬁciency of running the
benchmark applications on an iPhone with an Apple’s bionic
chip and an Android phone with a Samsung’s Exynos chip).

(a) All enabled.

(b) All disabled.

Fig. 20. Comparison of the object detection results (remote execution).

conversion method that we implemented in the testbed is
developed based on Java, which is inefﬁcient and slow. Thus,
in order to improve the efﬁciency of the image conversion, we
implement image conversion based on C in Android Native
Development Kit (NDK). We compare these two methods
by measuring their conversion latency with different preview
resolutions. The measurement results are presented in Table
VIII. We ﬁnd that the image conversion method developed
based on C decreases the image conversion latency by over
90%.

E. Insights and Research Opportunities

Insights.

• Ofﬂoading the object detection to the edge server does
not always reduce the per frame latency and energy
consumption of the mobile AR client compared to the
local execution. For example, as we observed in our
experiments, locally running a detection model with a size
of 100 × 100 pixels achieves lower per frame latency and
energy consumption than the remote execution that runs
a CNN with a similar model size. In addition, the speciﬁc
CNN model size when the local execution has better
performance than the remote execution may vary with
the computation capacities of the edge server and mobile
AR clients, and even the wireless network bandwidth.
• In the remote execution, the mobile AR client does not
achieve the lowest per frame energy consumption when
its CPU is set to the highest frequency, which is different
from the local execution. For example, in our experiment,
the lowest per frame energy consumption is obtained
when the CPU frequency is around 2.26GHz. Although
this value may be different for diverse smartphones or
wearable AR devices, this knowledge is important for
designing the CPU scaling mechanism.

Research Opportunities.

Because comparing different architectures is more challenging
and needs more efforts on the hardware setup (e.g., selecting
appropriate smartphones and using different ways to connect
the power supply to each smartphone based on their different
circuit designs) as well as the experiment design, we leave it
to our future work. External validity may also be threatened by
using a custom-designed object detection benchmark instead
of real-world applications. However, our benchmark applica-
tions exercise most of the main functionalities of existing and
potential mobile object detection applications, such as image
generation, camera preview, image conversion, inference, data
transmission, and virtual content rendering, which means that
our custom-designed object detection benchmark applications
are representative.

Internal validity. The collected power consumption and
CPU frequency data might be inﬂuenced by the background
activities. We mitigate this threat by terminating all other
optional applications and services that can impact the smart-
phone’s workload. One of the main contributions of this paper
is comparing the energy consumption and latency of executing
CNN-based object detections locally and remotely. To have
a fair comparison between the local and remote executions,
each comparison is conducted with the same conﬁgurations
(e.g., CPU governor, CNN model size, preview resolution,
and camera sampling rate) and under the same conditions.
For instance, all the power measurements are conducted in a
constant temperature laboratory. In addition, the temperature
of the tested smartphone’s CPU may increase when running
the benchmark application, which may impact
the power
consumption of the smartphone. To mitigate this threat, a new
measurement is launched only if the temperature of the CPU
cools down to around 42°C after the previous measurement.
Construct validity. Dissecting the energy consumption
for each phase in an application is difﬁcult. The accuracy
of our evaluation is guaranteed by the energy measurement
strategy we proposed in Section III-C, including the local
clock synchronization and cooling-off period. Speciﬁcally, the
precision of the local clock synchronization is in millisecond.
In addition, we hypothesize that the power consumption is
inﬂuenced by the workload accumulation, which is the as-
sumption for breaking down the power consumption of the
three parallel executions in our benchmark applications.

VII. CONCLUSION

In this paper, we presented the ﬁrst detailed experimental
study of the energy consumption and the performance of a
CNN-based object detection application. We examined both
local and remote execution cases. We found that the per-
formance of object detection is heavily affected by various
factors, such as CPU governor, CPU frequency, and CNN
model size. Although executing object detection on remote
edge servers is one of the most commonly used approaches
low-end smartphones in improving their energy
to assist
efﬁciency and performance, contrary to our expectation, local
execution may consume less energy and obtain lower latency,
as compared to remote execution. Overall, we believe that our

ﬁndings provide great insights and guidelines to the future
design of energy-efﬁcient processing pipeline of CNN-based
object detection.

REFERENCES

[1] L. N. Huynh, Y. Lee, and R. K. Balan, “Deepmon: Mobile GPU-based
deep learning framework for continuous vision applications,” in Proc.
ACM Mobisys, 2017, pp. 82–95.

[2] D. Chatzopoulos, C. Bermejo, Z. Huang, and P. Hui, “Mobile augmented
reality survey: From where we are to where we go,” IEEE Access,
vol. 82, pp. 6917–6950, 2017.

[3] J. Redmon and A. Farhadi, “Yolov3: An incremental improvement,”

arXiv, 2018.

[4] “Tensorﬂow lite,” https://www.tensorﬂow.org/lite/.
[5] T. Y.-H. Chen, L. Ravindranath, S. Deng, P. Bahl, and H. Balakrishnan,
“Glimpse: Continuous, real-time object recognition on mobile devices,”
in Proc. ACM Sensys, 2015, pp. 155–168.

[6] P. Jain, J. Manweiler, and R. R. Choudhury, “Low bandwidth ofﬂoad

for mobile AR,” in Proc. ACM CoNEXT, 2016, pp. 237–251.

[7] H. Wang, T. Liu, B. G. Kim, C.-W. Lin, S. Shiraishi, J. Xie, and Z. Han,
“Architectural design alternatives based on cloud/edge/fog computing for
connected vehicles,” IEEE Communications Surveys & Tutorials, 2020.
[8] A. Carroll, G. Heiser et al., “An analysis of power consumption in a
smartphone,” in Proc. USENIX Annual Technical Conference, 2010.
[9] S. K. Saha, P. Deshpande, P. P. Inamdar, R. K. Sheshadri, and D. Kout-
sonikolas, “Power-throughput tradeoffs of 802.11 n/ac in smartphones,”
in Proc. IEEE INFOCOM, 2015, pp. 100–108.

[10] L. Liu, H. Li, and M. Gruteser, “Edge assisted real-time object detection
for mobile augmented reality,” in Proc. ACM 25th Annual International
Conference on Mobile Computing and Networking (MobiCom), 2019,
pp. 1–16.

[11] A. Pathak, Y. C. Hu, and M. Zhang, “Where is the energy spent inside
my app? ﬁne grained energy accounting on smartphones with eprof,” in
Proc. the 7th ACM european conference on Computer Systems, 2012,
pp. 29–42.

[12] W. de Oliveira J´unior, R. O. dos Santos, F. J. C. de Lima Filho, B. F.
de Ara´ujo Neto, and G. H. L. Pinto, “Recommending energy-efﬁcient
Java collections,” in Proc. IEEE/ACM 16th International Conference on
Mining Software Repositories (MSR), 2019, pp. 160–170.

[13] S. Chowdhury, S. Borle, S. Romansky, and A. Hindle, “GreenScaler:
training software energy models with automatic test generation,” Em-
pirical Software Engineering, vol. 24, no. 4, pp. 1649–1692, 2019.
[14] A. Hindle, A. Wilson, K. Rasmussen, E. J. Barlow, J. C. Campbell,
and S. Romansky, “Greenminer: A hardware based mining software
repositories software energy consumption framework,” in Proc. ACM
the 11th Working Conference on Mining Software Repositories, 2014,
pp. 12–21.

[15] T. Agolli, L. Pollock, and J. Clause, “Investigating decreasing energy
usage in mobile apps via indistinguishable color changes,” in Proc.
2017 IEEE/ACM 4th International Conference on Mobile Software
Engineering and Systems (MOBILESoft), 2017, pp. 30–34.

[16] M. Wan, Y. Jin, D. Li, and W. G. Halfond, “Detecting display energy
hotspots in android apps,” in Proc. IEEE 8th International Conference
on Software Testing, Veriﬁcation and Validation (ICST), 2015, pp. 1–10.
[17] D. Li, Y. Lyu, J. Gui, and W. G. Halfond, “Automated energy optimiza-
tion of HTTP requests for mobile applications,” in Proc. IEEE/ACM
38th International Conference on Software Engineering (ICSE), 2016,
pp. 249–260.

[18] S. A. Chowdhury, V. Sapra, and A. Hindle, “Client-side energy efﬁciency
of HTTP/2 for web and mobile app developers,” in Proc. IEEE 23rd
international conference on software analysis, evolution, and reengi-
neering (SANER), vol. 1, 2016, pp. 529–540.

[19] A. McIntosh, S. Hassan, and A. Hindle, “What can android mobile
app developers do about the energy consumption of machine learning?”
Empirical Software Engineering, vol. 24, no. 2, pp. 562–601, 2019.
[20] Y. Xiao, Y. Cui, P. Savolainen, M. Siekkinen, A. Wang, L. Yang,
A. Yl¨a-J¨a¨aski, and S. Tarkoma, “Modeling energy consumption of data
transmission over Wi-Fi,” IEEE Transactions on Mobile Computing,
vol. 13, no. 8, pp. 1760–1773, 2013.

[21] H. Wang, J. Xie, and X. Liu, “Rethinking mobile devices’ energy
efﬁciency in WLAN management services,” in Proc. IEEE SECON,
2018, pp. 1–9.

[44] X. Ran, H. Chen, X. Zhu, Z. Liu, and J. Chen, “Deepdecision: A
mobile deep learning framework for edge video analytics,” in Proc. IEEE
INFOCOM, 2018, pp. 1421–1429.

[45] J. Hanhirova, T. K¨am¨ar¨ainen, S. Sepp¨al¨a, M. Siekkinen, V. Hirvisalo,
and A. Yl¨a-J¨a¨aski, “Latency and throughput characterization of convo-
lutional neural networks for mobile computer vision,” in Proc. 9th ACM
Multimedia Systems Conference, 2018, pp. 204–215.

[46] J.-J. Chen, C.-Y. Yang, T.-W. Kuo, and C.-S. Shih, “Energy-efﬁcient real-
time task scheduling in multiprocessor DVS systems,” in Proc. IEEE
Asia and South Paciﬁc Design Automation Conference, 2007, pp. 342–
349.

[47] J. Kwak, O. Choi, S. Chong, and P. Mohapatra, “Dynamic speed scaling
for energy minimization in delay-tolerant smartphone applications,” in
Proc. IEEE INFOCOM, 2014, pp. 2292–2300.

[48] W. Y. Lee, “Energy-saving DVFS scheduling of multiple periodic real-
time tasks on multi-core processors,” in Proc. 13th IEEE/ACM Interna-
tional Symposium on Distributed Simulation and Real Time Applications,
2009, pp. 216–223.

[49] J. Redmon, “Darknet: Open source neural networks in C,” http://pjreddie.

com/darknet/, 2013–2016.

[50] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Doll´ar, and C. L. Zitnick, “Microsoft COCO: Common objects in
context,” in Proc. European Conference on Computer Vision, 2014.

[51] “Monsoon power monitor,” https://www.msoon.com/.
[52] Android. ImageFormat. Accessed on Oct. 2020. [Online]. Available:

https://developer.android.com/reference/android/graphics/ImageFormat

[53] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisser-
man, “The pascal visual object classes (voc) challenge,” International
Journal of Computer Vision, vol. 88, no. 2, pp. 303–338, 2010.
[54] H. Wang and J. Xie, “User preference based energy-aware mobile AR
system with edge computing,” in Proc. IEEE INFOCOM, 2020, pp. 1–
10.

[55] H. Wang, B. Kim, J. Xie, and Z. Han, “How is energy consumed in
smartphone deep learning apps? Executing locally vs. remotely,” in Proc.
IEEE Globecom, 2019, pp. 1–6.

[56] “Antutu benchmark,” https://www.antutu.com/en/.

[22] J. Huang, F. Qian, A. Gerber, Z. M. Mao, S. Sen, and O. Spatscheck,
“A close examination of performance and power characteristics of 4G
LTE networks,” in Proc. ACM MobiSys, 2012, pp. 225–238.

[23] H. Wang, J. Xie, and T. Han, “A smart service rebuilding scheme across
cloudlets via mobile AR frame feature mapping,” in Proc. IEEE ICC,
2018, pp. 1–6.

[24] ——, “V-handoff: A practical energy efﬁcient handoff for 802.11

infrastructure networks,” in Proc. IEEE ICC, 2017, pp. 1–6.

[25] A. Shye, B. Scholbrock, and G. Memik, “Into the wild: studying real user
activity patterns to guide power optimizations for mobile architectures,”
in Proc. IEEE/ACM International Symposium on Microarchitecture,
2009, pp. 168–178.

[26] M. J. Walker, S. Diestelhorst, A. Hansson, A. K. Das, S. Yang, B. M.
Al-Hashimi, and G. V. Merrett, “Accurate and stable run-time power
modeling for mobile and embedded CPUs,” IEEE Transactions on
Computer-Aided Design of Integrated Circuits and Systems, vol. 36,
no. 1, pp. 106–119, 2016.

[27] K. DeVogeleer, G. Memmi, P. Jouvelot, and F. Coelho, “Modeling
the temperature bias of power consumption for nanometer-scale CPUs
in application processors,” in Proc. IEEE International Conference on
Embedded Computer Systems: Architectures, Modeling, and Simulation
(SAMOS XIV), 2014, pp. 172–180.

[28] F. Xu, Y. Liu, Q. Li, and Y. Zhang, “V-edge: Fast self-constructive power
modeling of smartphones based on battery voltage dynamics,” in Proc.
USENIX Symposium on Network Systems Design and Implementation
(NSDI), 2013, pp. 43–55.

[29] A. Pathak, Y. C. Hu, M. Zhang, P. Bahl, and Y.-M. Wang, “Fine-grained
power modeling for smartphones using system call tracing,” in Proc. the
sixth conference on Computer systems, 2011, pp. 153–168.

[30] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards real-
time object detection with region proposal networks,” in Proc. Advances
in Neural Information Processing Systems, 2015, pp. 91–99.

[31] R. J. Wang, X. Li, and C. X. Ling, “Pelee: A real-time object detection
system on mobile devices,” in Proc. Advances in Neural Information
Processing Systems, 2018, pp. 1963–1972.

[32] X. Zhang, X. Zhou, M. Lin, and J. Sun, “Shufﬂenet: An extremely
efﬁcient convolutional neural network for mobile devices,” in Proc. IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), 2018,
pp. 6848–6856.

[33] D. Chen, L. J. Xie, B. Kim, L. Wang, C. S. Hong, L.-C. Wang,
and Z. Han, “Federated learning based mobile edge computing for
augmented reality applications,” in Proc. IEEE ICNC, 2020, pp. 767–
773.

[34] J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara, A. Fathi, I. Fischer,
Z. Wojna, Y. Song, S. Guadarrama et al., “Speed/accuracy trade-offs for
modern convolutional object detectors,” in Proc. IEEE CVPR, 2017.

[35] Q. Liu, S. Huang, J. Opadere, and T. Han, “An edge network orchestrator
for mobile augmented reality,” in Proc. IEEE INFOCOM, 2018, pp. 756–
764.

[36] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,
T. Weyand, M. Andreetto, and H. Adam, “MobileNets: Efﬁcient convo-
lutional neural networks for mobile vision applications,” arXiv preprint
arXiv:1704.04861, 2017.

[37] X. Ran, H. Chen, Z. Liu, and J. Chen, “Delivering deep learning to
mobile devices via ofﬂoading,” in Proc. ACM Workshop on Virtual
Reality and Augmented Reality Network, 2017, pp. 42–47.

[38] W. Hu and G. Cao, “Energy-aware CPU frequency scaling for mobile
video streaming.” in Proc. IEEE ICDCS, 2017, pp. 2314–2321.
[39] ——, “Energy optimization through trafﬁc aggregation in wireless

networks,” in Proc. IEEE INFOCOM, 2014, pp. 916–924.

[40] N. C. Luong, P. Wang, D. Niyato, Y. Wen, and Z. Han, “Resource
management in cloud networking using economic analysis and pricing
models: A survey,” IEEE Communications Surveys & Tutorials, vol. 19,
no. 2, pp. 954–1001, 2017.

[41] H. Wang, B. Kim, J. Xie, and Z. Han, “E-auto: A communication scheme
for connected vehicles with edge-assisted autonomous driving,” in Proc.
IEEE ICC, 2019, pp. 1–6.

[42] Y. Geng, Y. Yang, and G. Cao, “Energy-efﬁcient computation ofﬂoading
for multicore-based mobile devices,” in Proc. IEEE INFOCOM, 2018,
pp. 46–54.

[43] A. P. Miettinen and J. K. Nurminen, “Energy efﬁciency of mobile clients

in cloud computing.” HotCloud, vol. 10, no. 4, p. 19, 2010.

