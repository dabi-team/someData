1
2
0
2

p
e
S
6
2

]

D
S
.
s
c
[

2
v
2
0
7
4
0
.
4
0
1
2
:
v
i
X
r
a

BOUNDARY AND CONTEXT AWARE TRAINING FOR CIF-BASED
NON-AUTOREGRESSIVE END-TO-END ASR

Fan Yu1, Haoneng Luo1, Pengcheng Guo1, Yuhao Liang1, Zhuoyuan Yao1,
Lei Xie1∗, Yingying Gao2, Leijing Hou2, Shilei Zhang2

1Audio, Speech and Language Processing Group (ASLP@NPU), School of Computer Science,
Northwestern Polytechnical University, Xi’an, China
2China Mobile Research Institute

ABSTRACT

Continuous integrate-and-ﬁre (CIF) based models, which use
a soft and monotonic alignment mechanism, have been well
applied in non-autoregressive (NAR) speech recognition with
competitive performance compared with other NAR meth-
ods. However, such an alignment learning strategy may suf-
fer from an erroneous acoustic boundary estimation, severely
hindering the convergence speed as well as the system per-
formance. In this paper, we propose a boundary and context
aware training approach for CIF based NAR models. Firstly,
the connectionist temporal classiﬁcation (CTC) spike infor-
mation is utilized to guide the learning of acoustic boundaries
in the CIF. Besides, an additional contextual decoder is in-
troduced behind the CIF decoder, aiming to capture the lin-
guistic dependencies within a sentence. Finally, we adopt a
recently proposed Conformer architecture to improve the ca-
pacity of acoustic modeling. Experiments on the open-source
Mandarin AISHELL-1 corpus show that the proposed method
achieves a comparable character error rates (CERs) of 4.9%
with only 1/24 latency compared with a state-of-the-art au-
toregressive (AR) Conformer model. Futhermore, when eval-
uating on an internal 7500 hours Mandarin corpus, our model
still outperforms other NAR methods and even reaches the
AR Conformer model on a challenging real-world noisy test
set.

Index Terms— Non-autoregressive, end-to-end speech

recognition, continuous integrate-and-ﬁre

1. INTRODUCTION

End-to-end (E2E) models have achieved great success in au-
tomatic speech recognition (ASR) due to their effectiveness
in sequence-to-sequence modeling [1–6]. Compared with the
traditional hybrid systems [7, 8], E2E models can not only
simplify the model training process but also achieve competi-
tive or even better recognition accuracy. However, most state-
of-the-art E2E models follow an autoregressive (AR) fash-

*Lei Xie is the corresponding author, lxie@nwpu.edu.cn

ion and recursively generate an output token based on pre-
viously generated tokens and the input sequence. Thus, it
will take at least L iterations to generate an L-length output
sequence, resulting in a complex computation and a large la-
tency with the increment of sequence length. In contrast, the
non-autoregressive (NAR) models make a conditional inde-
pendence assumption among the output tokens and no longer
rely on the temporal relationship from left to right, which
can generate an L-length sequence in parallel with a constant
K(<< L) iterations.

NAR models are originally proposed in neural machine
translation (NMT) tasks and have been well applied in ASR
recently. The major difﬁculties of NAR models consist of the
following two aspects: the accurate length prediction of a tar-
get sequence and the parallel inference of decoder. Introduc-
ing a length prediction network behind the encoder [9–11] or
estimating an empirical target length according to the source
sequence [12] are two typical length prediction approaches.
However, both methods will bring redundant computations
and can not guarantee the accuracy of the predicted lengths,
especially in intricate real-world scenarios. Since connec-
tionist temporal classiﬁcation (CTC) [13] is good at learning
frame-wise latent alignments between the input speech and
output tokens, more and more studies are trying to get rid of
the cumbersome length estimation and focus on incorporat-
ing the CTC into NAR models. In [14, 15], conditional chain
based methods were proposed for NAR multi-speaker ASR,
which inferred the output of each speaker one-by-one using a
NAR CTC model. With this design, the total inference steps
will be restricted to the number of mixed speakers. In [16],
Tian et al. proposed a spike-triggered based NAR method,
which used the encoded states corresponding to the CTC
spikes as the decoder input. Although the decoder can eas-
ily perform parallel computation with this design, the length
mismatch between the CTC spikes and target sequence may
lead to a computation problem of cross-entropy (CE) loss. In-
spired by the masked language modeling, Mask-CTC [17,18]
initialized the input target sequence with masked ground-
truth during the training and masked token-level CTC outputs

 
 
 
 
 
 
during the inference, respectively. The idea of Mask-CTC
was to infer the masked tokens by the decoder and iteratively
reﬁne them. In addition to Mask-CTC, Imputer [19] effec-
tively modeled context dependencies and CASS-NAT [20]
generated token-level acoustic embeddings through CTC
alignments, which were also confronting the redundant com-
putation problem caused by a longer sequence.

Recently, a novel soft and monotonic alignment mecha-
nism was proposed for sequence transduction, named con-
tinuous integrate-and-ﬁre (CIF) [21]. By accumulating the
weight of the vector representation in each encoder step, CIF
effectively locates the acoustic boundaries and extracts the
acoustic information corresponding to each target token. Al-
though it is more accurate in estimating the length of the target
sequence, there still exists a deviation in the prediction of the
acoustics boundaries. In this paper, in order to improve the ac-
curacy of acoustic boundary prediction (e.g, character bound-
aries in Mandarin) for the CIF based models, we propose a
novel auxiliary objective function to constrain the acoustic
boundary of each label by using the spike information and
alignment states of CTC. Morever, a recently proposed Con-
former architecture [22] is adopted to enhance the speech rep-
resentation learning, which has better local and global mod-
eling capabilities than Transformer. Finally, we integrate a
new contextual decoder to model the linguistic and contex-
tual dependencies within the target sequences. Evaluating on
the open-source Mandarin corpora AISHELL-1, our proposed
method achieves a comparable character error rates (CERs)
of 4.9% with only 1/24 latency compared with a state-of-the-
art AR Conformer model. We also conduct experiments on
a large scale 7500 hours internal Mandarin corpus and our
model shows similar trend on in-domain test set.

The rest of this paper is organized as follows. Section
2 introduces non-autoregressive continuous integrate-and-ﬁre
(CIF) model. Section 3 describes our proposed method. Sec-
tion 4 presents our experimental setup and results. The con-
clusions and future work will be given in Section 5.

2. NON-AUTOREGRESSIVE CONTINUOUS
INTEGRATE-AND-FIRE (CIF)

Give an input speech sequence x = [x1, ..., xT ], where T
means the number of frames, conventional AR models itera-
tively produce output tokens y = [y1, ..., yL] as:

PAR(y|x) =

L
(cid:89)

l=1

P (yl|y<l, x),

(1)

where L refers to the length of target sequence. With the pre-
viously generated tokens y<l, AR models estimate the output
token yl one-by-one, which makes it hard to compute in par-
allel and results in a large latency during the inference. By
contrast, the NAR models aim to get rid of such temporal de-
pendency and perform parallel computation. During the infer-

ence, NAR models will generate the probability distribution
of y within a constant number of iterations that is not con-
strict to the sequence length. Mathematically, a NAR model
can be deﬁned as:

PNAR(y|x) =

L
(cid:89)

l=1

P (yl|x).

(2)

As a NAR model, CIF [21] utilizes a soft and monotonic
alignment mechanism between the encoder and decoder, and
the encoder outputs, replacing traditional token embeddings,
are directly used as the input of decoder to achieve paral-
lel NAR computation. The left part of Fig. 1 shows a de-
tailed structure of the CIF model. At each encoder step, CIF
receives the current encoder output hu and a corresponding
weight αu, where u ∈ U means the index of encoder step.
The weight αu scales the amount of acoustic information and
is accumulated until reaching a threshold β = 1, which means
an acoustic boundary of a speciﬁc target token. At this bound-
ary point, the hu will be divided into two parts: one for com-
pleting the integration of current token, and the other is used
for the accumulation of acoustic information for the next to-
ken. Then, the weighted summation of encoder outputs is
ﬁred as the input of decoder. Therefore, CIF is able to provide
a soft alignment between acoustic frames and target labels.

In order to improve the accuracy of sequence length pre-
diction by CIF, a quantity loss is also presented, forcing the
model to predict the quantity of integrated embeddings closer
to the quantity of targeted label sequence. Quantity loss LQua
can be deﬁned as:

LQua = |

U
(cid:88)

αu − ˜S|,

(3)

u=1
where ˜S is the ground-truth length of the target sequence y.

3. PROPOSED METHOD

Our proposed approach is based on the CIF model with
substantial improvements to obtain more accurate acous-
tic boundaries and explicitly capture the linguistic contexts.
Speciﬁcally, we use a CTC module to guide the learning of
acoustic boundaries and introduce a novel context decoder
behind the CIF decoder to model the token relationship, as
shown in Fig. 1. Besides, since Conformer encoder is pro-
posed to learn both local and global acoustic correlations
synchronously, we also adopt it to extract better acoustic hid-
den representations. Moreover, unlike the original CIF which
only uses acoustic embedding of CIF module as in Eq. (4),
we ﬁre both the acoustic embedding c and encoder output
h to the decoder to predict the probability of output tokens
y = [y1, ..., yi, ...] in parallel as in Eq. (5). We notice that this
can lead to a further improvement of performance.

P (y|c) = Decoder(c),
P (y|c, h) = Decoder(c, h).

(4)

(5)

Fig. 2.
The spike-like trigger probability curve. The red
curve represents the non-blank label probability, and the solid
black line represents the trigger threshold θ. The probability
of a non-blank token greater than θ is a spike.

utive CTC spikes. The CTC spike module is shown in Fig. 2.
Given a CTC spike sequence Ps = [0, 0, 1, 0, 0, 1, 0, 1, 0, ...],
where 1 means the probability of non-blank token is greater
than a speciﬁc threshold θ, we constrain the CIF weights α
by the label boundary sequence Pb = [0, 2, 5, 7, ...]1, where
each number is the index of non-blank token. The additional
alignment loss LAli can be formulated as:

LAli =

L
(cid:88)

|

Pb(i+1)
(cid:88)

i=0

j=Pb(i)

α(j) − 1|.

(6)

where L is the ground-truth length of the targets sequence
y. By providing the alignment label boundary constraints, we
combine the boundary information from both CTC spikes and
CIF to promote the model to locate boundaries in a parameter-
efﬁcient way.

3.3. Contextual decoder

To further help the CIF decoder to capture the token relation-
ship, we propose a contextual decoder after CIF decoder to
capture the contextual relationship within a sequence. Al-
though the outputs of the CIF decoder can be regarded as
an integrated representation of acoustic and linguistic infor-
mation corresponding to the tokens, the context correlation
between tokens is weak. The contextual decoder leverages a
stack of self-attention blocks, and its input of the query, key,
and value is the high-level representations from the CIF de-
coder. Meanwhile, contextual decoder does not require the
acoustic output representation of encoder since it already con-
tains enough acoustic information of each token. Moreover,
considering the deep layers of the model, we calculate the
cross-entropy (CE) loss of between the outputs of CIF de-
coder and contextual-decoder to assist the backward update
of the gradient and speed up the convergence. The cross-
entropy loss can be formulated as LCE = LCE(y
),
where y
indicate CIF decoder output and contextual
decoder output respectively. Note that our model with contex-
tual decoder needs to train more epochs or use a pre-trained
encoder to speed up the model convergence.

)+LCE(y

and y

(cid:48)(cid:48)

(cid:48)(cid:48)

(cid:48)

(cid:48)

1The ﬁrst element of Pb is set to 0 for identifying the beginning of the

spike sequence.

Fig. 1.
The architecture of our proposed CIF-based non-
autoregressive model for the ASR task. The left part illus-
trates the structure of CIF module, and the right part shows
the contextual decoder and CTC alignment loss.

3.1. Conformer Encoder

Our encoder block follows the same Conformer architec-
ture as in [22, 23], which includes a multihead self-attention
(MHSA) module, a convolution (Conv) module, and a pair of
positionwise feed-forward (FFN) module in the Macaron-Net
style. Meanwhile the MHSA module can learn long-range
global context, the Conv module is good at extracting ﬁne-
grained local features. By adopting Conformer, we expect to
improve the prediction performance of CTC and CIF modules
as well.
In addition, incorporating the relatively positional
embeddings to MHSA modules further improves the general-
ization of the model on variable lengths of input sequences.

3.2. CTC alignment loss

Due to the speaker’s speaking rate, accent, silence and noise,
which often happen in real-world applications, the alignment
learning strategy of CIF may result in an inaccurate acoustic
boundary estimation, which severely inﬂuences the conver-
gence speed. To ﬁgure out these problems and improve the
performance, we present an additional CTC alignment loss
function (as shown in Fig 1) to guide the CIF-based model to
predict the acoustic boundary closer to the actual boundary by
making full use of the CTC spike information. Since the CTC
spike is usually within the acoustic boundary of the label, we
can roughly determine the boundary of a label by two consec-

      Encoder Conv & FCSigmoidScalingCIFQuantityLossCIF Decoder  CE LosstraininferSContextualDecoderCE LossFCCTC LossCTC SpikeCTCAlignmentLossSumFC&Softmax  FC&Softmax0.400.20.60.81213456789101112131415161718θblanknon blank3.4. Training strategy

As described in Section 2, considering the accuracy of se-
quence length prediction for CIF module and the lack of left-
to-right constraints for the attention model, our model adopts
In addition, we introduce
the quantity loss and CTC loss.
a CTC alignment loss Lali to further promote the learning
of acoustic boundary as described in Section 3.2. Finally,
we also compute the CE loss LCE for the CIF decoder and
the contextual decoder to boost the model learning. There-
fore, our model is trained with a combination of four different
losses:

L = LCE + λ1LAli + λ2LCTC + λ3LQua,

(7)

where λ1, λ2 and λ3 are interpolation factors. In this study,
these hyper-parameters are set to 1 and the effects of these
auxiliary functions will be investigated in the following ex-
periments.

4. EXPERIMENTS

4.1. Dataset

In this paper, we evaluate our Conformer-CIF based model
on two Mandarin speech recognition corpora: an open-source
AISHELL-1 corpus [24] and a large-scale internal 7500 hours
corpus [25]. The AISHELL-1 corpus contains 150 hours of
speech recorded by 340 speakers as training set, 18 hours of
speech recorded by 40 speakers as development set, and about
10 hours speech as test set. The speakers of different sets are
not overlapped and each set is recorded in a relatively quiet
environment. The 7500 hours internal corpus contains read-
ing speech data in various domains, such as entertainment,
journalism, literature, technology, free conversation, etc. For
experiments on the 7500 hours corpus, we report character er-
ror rate results (CERs) on four test sets, namely AISHELL-1
test set (TA1) [24], AISHELL-2 test set (TA2) [26], an inter-
nal voice input test set (VI), and a voice assistant (VA) test
set. The VI test set consists of about 3.4 hours data with 3063
sentences covering lots of proper nouns and named entities,
which is used to verify the language generalization ability of
the model. The VA test set is collected under various noisy
scenarios, containing about 3.9 hours data with 5000 voice
assistant commands.

4.2. Experimental Setup

In our work, the 80-dimensional log Mel-ﬁlter bank feature
(Fbank) plus 3-dimensional pith feature are used as the in-
put feature. The window size is 25 ms with a shift of 10 ms.
We use 4231 characters extracted from the training transcrip-
tions as the modeling units. Our CIF-based NAR model is
comprised of 12 encoder layers, 6 CIF decoder layers, and 6
contextual decoder layers. Particularly, when performing ex-
periments on the 7500 hours corpus, the number of encoder

Table 1. The character error rates (CERs) of the systems on
AISHELL-1. Real-time factor (RTF) is computed as the ratio
of the total inference time to the total duration of the test set.

Model

Autoregressive

Kaldi chain [24]
Conformer

+ Contextual decoder

Non-autoregressive

Transformer-A-FMLM [2]
Transformer-Insertion [31]
Transformer-LASO [12]
Transformer-CASS-NAT [20]
Conformer-CTC
Conformer-Spike-Triggered [16]†

Non-autoregressive (proposed)

Conformer-CIF

+ Contextual decoder

+ CTC alignment loss

CER (%)
Dev Test

RTF

-
4.8
4.4

6.2
6.1
5.8
5.3
5.1
5.0

4.8
4.6
4.5

7.6
5.1
4.8

6.7
6.7
6.4
5.8
5.7
5.6

5.3
5.0
4.9

-
0.2768
0.4306

-
-
-
-
0.0125
0.0152

0.0166
0.0181
0.0182

†: This models is re-implemented by ourselves with the same parameter
structure as our model.

layers and contextual decoder layers is set to 20 and 4, respec-
tively. The common parameters of the encoder and decoder
layers are: dhead = 4, dattn = 256, df f = 2048 for the
number of attention heads, dimension of attention module,
and dimension of feedforward layer, respectively. We use the
Adam optimizer [27] and the warmup scheduler [5] to train
the model for 80 epochs. The warmup step is set to 25k iter-
ations. SpecAugment [28] and speed perturbation [29] with
the factor of 0.0, 1.0, and 1.1 are also applied to avoid over-
ﬁtting. For a more fair comparison, we re-implement the CTC
and Spike-Triggered [16] models with the same parameters
and Conformer structure as used in our model. All the ex-
periments are conducted using the open-source ESPnet [30]
toolkit on a single NVIDIA RTX 2080Ti GPU.

4.3. Results on AISHELL-1

To explore the efﬁciency of our proposed Conformer-CIF
based model, we ﬁrst evaluate it on the open-source AISHELL-
1 corpus. As show in Table 1, we can ﬁnd that our model
outperforms the hybrid Kaldi chain system and even reaches
the state-of-the-art AR Conformer model, achieving CERs of
4.8% and 5.3% on dev and test sets, respectively. Comparing
with other NAR models, our Conformer-CIF based model
is still better than other methods with a slight increment of
RTF. When incorporating with the contextual decoder and
CTC alignment loss, we can obtain further improvement,
achieving CERs of 4.5% and 4.9% with about 24× faster in
inference speech.

Fig. 3. Visualization on the prediction of weight α for each encoded state for the original CIF model (upper) and our CIF model
(lower) with contextual decoder and CTC alignment loss. These ﬁgures are drawn for the utterance index BAC009S0764W0356
in the AISHELL-1 test set. We ﬁnd that the original CIF model (upper), as mentioned in [21], is more prone to be located ahead
of time, but our CIF model (lower) predicts the boundary of each token more accurately. Red dotted vertical line: acoustic
boundary of the token.

4.4. Results on 7500 hours

We further evaluate the proposed model on a large-scale inter-
nal 7500 hours corpus, as shown in Table 2. In addition to the
clean test sets TA1 and TA2, we also involve two more chal-
lenging test sets VI and VA. From the table, we can see a quite
similar trend as experiments on the AISHLLE-1 corpus. The
proposed Conformer-CIF based model outperforms the cas-
cade RNN-T model on 3 test sets and even shows good poten-
tial on the most challenging sets VI and VA, where VA is cor-
rupted by real-world noise or reverberation. Comparing with
the state-of-the-art AR Conformer model, our model achieves
competitive results on TA1 and TA2 tests with only 1/18 la-
tency. Since VI and VA are collected under more compli-
cated acoustic conditions from a mismatched domain, com-
paring with AR model, the performance of the NAR model
is reduced due to lack of context information within the se-
quences. Morever, by comparing the results with Conformer-
CTC NAR models, our model yields up to 9.5%, 8.0%, 5.8%,
and 1.5% relative CERs reductions on four test sets, respec-
tively.

4.5. Result Analysis

The results on Section 4.3 and Section 4.4 have demonstrated
that the proposed method signiﬁcantly improves the perfor-
mance with a negligible RTF increment. In this Section, we
will investigate the effectiveness of the CTC alignment loss
and the contextual decoder.

Fig. 3 shows a visualization of the encoded state weight
α for the original CIF model and our proposed Conformer-
CIF model with contextual decoder and CTC alignment loss.
For the upper and lower parts, the red dash lines are acoustic

Table 2. The character error rates (CERs) of the systems on
the the 7500-hour corpus. Real-time factor (RTF) is computed
as the ratio of the total inference time to the total duration of
the test set.

Model

Autoregressive

Cascade RNN-T [25]
Conformer

Non-autoregressive
Conformer-CTC
Conformer-CIF

CER(%)
TA1 TA2 VI

4.6
5.1

6.1
5.5

9.2
8.4

9.7
8.9

8.7
6.7

8.6
8.1

VA

28.1
25.7

27.6
27.2

Param.(M)/RTF

95.5M /
71.2M / 0.4487

-

55.8M / 0.0201
73.6M / 0.0253

boundaries estimated by the models, which means the accu-
mulated weight reaches 1.0 at this point. The middle part is
the spectrogram of the audio and the acoustic boundaries are
marked manually. We can see that the original CIF model is
prone to be located ahead of time, as mentioned in [21], while
our model gives a more accurate estimation of the acoustic
boundaries. By using the CTC spike information to guide
the learning of acoustic boundaries, our model can not only
converge faster, but also predict the length of the target se-
quence correctly, which will dramatically reduce the insertion
and deletion errors.

Moreover, we also notice that most of the inference errors
are substitution errors with similar pronunciation, as shown in
Fig 4. The upper part shows the ground truth sequence, while
the middle and lower parts are the output hypotheses obtained
by different models. Considering the lack of context depen-
dencies within a sentence, we adopt an additional contextual
decoder to improve the linguistic and contextual relationship
and aim to recover such substitution errors. From ﬁgure 4, we

十多岁到五十九岁岁九十五到岁多十十多岁到五十九岁Original CIFGround TruthProposedFig. 4. Decoding example for BAC009S0903W0239 in the
AISHELL-1 test set. Red indicates characters with errors and
blue indicates ones recovered with a contextual decoder.

can see that the proposed contextual decoder can successfully
eliminate substitution errors.

5. CONCLUSIONS

In this paper, we propose a boundary and context aware train-
ing approach for CIF based non-autoregressive models to im-
prove the accuracy of the acoustic boundary prediction. Our
Conformer-CIF based model utilizes the CTC spike informa-
tion to guide the learning of acoustic boundaries in the CIF,
and integrates a new contextual decoder to model the linguis-
tic and contextual dependencies within the target sequence.
Meanwhile, we also adopt a recently proposed Conformer
architecture to improve the capacity of acoustic modeling.
Evaluating on the open-source Mandarin corpora AISHELL-
1 and an internal 7500 hours Mandarin corpus, our proposed
approach achieves comparable character error rates with only
1/24 latency compared with a state-of-the-art autoregressive
Conformer model. In the future, we will integrate the exter-
nal language model into our proposed model to improve the
language generalization ability.

6. ACKNOWLEDGEMENT

This work was supported by MoE-CMCC “Artiﬁcial Intelli-
gence” Project (MCM20190701).

7. REFERENCES

[1] Jan Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk,
Kyunghyun Cho, and Yoshua Bengio, “Attention-based
models for speech recognition,” in Proc. NIPS, 2015,
pp. 577–585.

[2] William Chan, Navdeep Jaitly, Quoc Le, and Oriol
Vinyals, “Listen, attend and spell: A neural network for
large vocabulary conversational speech recognition,” in
Proc. ICASSP. IEEE, 2016, pp. 4960–4964.

[3] Suyoun Kim, Takaaki Hori, and Shinji Watanabe, “Joint
CTC-attention based end-to-end speech recognition us-
ing multi-task learning,” in Proc. ICASSP. IEEE, 2017,
pp. 4835–4839.

[4] Chung-Cheng Chiu, Tara N Sainath, Yonghui Wu, Ro-
hit Prabhavalkar, Patrick Nguyen, Zhifeng Chen, An-
juli Kannan, Ron J Weiss, Kanishka Rao, Ekaterina Go-
nina, et al., “State-of-the-art speech recognition with
sequence-to-sequence models,” in Proc. ICASSP. IEEE,
2018, pp. 4774–4778.

[5] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
“Attention is all you

Uszkoreit, Llion Jones, et al.,
need,” in Proc. NeurIPS, 2017, pp. 5998–6008.

[6] Linhao Dong, Shuang Xu, and Bo Xu,

“Speech-
Transformer: A no-recurrence sequence-to-sequence
model for speech recognition,” in Proc. ICASSP. IEEE,
2018, pp. 5884–5888.

[7] Vijayaditya Peddinti, Daniel Povey, and Sanjeev Khu-
danpur, “A time delay neural network architecture for
efﬁcient modeling of long temporal contexts,” in Proc.
INTERSPEECH. ISCA, 2015, pp. 3214–3218.

[8] Daniel Povey, Vijayaditya Peddinti, Daniel Galvez, Pe-
gah Ghahremani, Vimal Manohar, Xingyu Na, Yim-
ing Wang, and Sanjeev Khudanpur, “Purely sequence-
trained neural networks for asr based on lattice-free
in Proc, INTERSPEECH. ISCA, 2016, pp.
mmi.,”
2751–2755.

[9] Jason Lee, Elman Mansimov, and Kyunghyun Cho,
sequence
in Proc. EMNLP.

“Deterministic non-autoregressive neural
modeling by iterative reﬁnement,”
ACL, 2018, pp. 1173–1182.

[10] Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK
Li, and Richard Socher, “Non-autoregressive neural ma-
chine translation,” in Proc. ICLR, 2018.

[11] Xuezhe Ma, Chunting Zhou, Xian Li, Graham Neubig,
and Eduard Hovy, “FlowSeq: Non-autoregressive con-
ditional sequence generation with generative ﬂow,” in
Proc. EMNLP-IJCNLP, 2019, pp. 4273–4283.

[12] Ye Bai, Jiangyan Yi, Jianhua Tao, Zhengkun Tian,
Zhengqi Wen, and Shuai Zhang,
“Listen attentively,
and spell once: Whole sentence generation via a
non-autoregressive architecture for low-latency speech
recognition,” in Proc. INTERSPEECH. ISCA, 2020, pp.
3381–3385.

[13] Alex Graves, Santiago Fern´andez, Faustino Gomez, and
J¨urgen Schmidhuber, “Connectionist temporal classiﬁ-
cation: Labelling unsegmented sequence data with re-
current neural networks,” in Proc. ICML. PMLR, 2006,
pp. 369–376.

[14] Jing Shi, Xuankai Chang, Pengcheng Guo, Shinji
Watanabe, Yusuke Fujita, Jiaming Xu, Bo Xu, and Lei

Ground truth重 点 突 破 棉 花 油 菜 甘 蔗 收 获 机 械 化 瓶 颈CIF-based inference重 点 突 破 棉 花 油 菜 干 着 收 获 机 械 化 瓶 静重 点 突 破 棉 花 油 菜 甘 蔗 收 获 机 械 化 瓶 颈CIF-based with contextual decoder“Sequence to multi-sequence learning via con-
Xie,
ditional chain mapping for mixture signals,” in Proc.
NeurIPS, 2020, pp. 3735–3747.

on-device Mandarin speech recognition with a syllable-
to-character converter,” in Proc. SLT. IEEE, 2021, pp.
15–21.

[15] Pengcheng Guo, Xuankai Chang, Shinji Watanabe,
“Multi-speaker ASR combining non-
and Lei Xie,
autoregressive Conformer CTC and conditional speaker
chain,” in Proc. INTERSPEECH. ISCA, 2021.

[26] Jiayu Du, Xingyu Na, Xuechen Liu, and Hui Bu,
“Aishell-2: Transforming Mandarin ASR research into
arXiv preprint arXiv:1808.10583,
industrial scale,”
2018.

[27] Diederik P Kingma and Jimmy Ba, “Adam: A method
for stochastic optimization,” in Proc. ICLR, 2015.

[28] Daniel S Park, William Chan, Yu Zhang, Chung-Cheng
Chiu, Barret Zoph, Ekin D Cubuk, and Quoc V Le,
“Specaugment: A simple data augmentation method for
in Proc. Interspeech,
automatic speech recognition,”
2019, pp. 2613–2617.

[29] Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-
jeev Khudanpur, “Audio augmentation for speech recog-
nition,” in Proc. Interspeech, 2015, pp. 3586–3589.

[30] Shinji Watanabe, Takaaki Hori, Shigeki Karita,
Tomoki Hayashi, Jiro Nishitoba, Yuya Unno, Nelson-
Enrique Yalta Soplin, Jahn Heymann, Matthew Wiesner,
Nanxin Chen, et al., “ESPnet: End-to-End speech pro-
cessing toolkit,” in Proc. Interspeech, 2018, pp. 2207–
2211.

[31] Yuya Fujita, Shinji Watanabe, Motoi Omachi, and Xu-
ankai Chan, “Insertion-based modeling for end-to-end
in Proc. Interspeech,
automatic speech recognition,”
2020, pp. 3660–3664.

[16] Zhengkun Tian, Jiangyan Yi, Jianhua Tao, Ye Bai,
Shuai Zhang, and Zhengqi Wen,
“Spike-triggered
non-autoregressive Transformer for end-to-end speech
recognition,” in Proc. INTERSPEECH. ISCA, 2020, pp.
5026–5030.

[17] Yosuke Higuchi, Shinji Watanabe, Nanxin Chen, Tetsuji
Ogawa, and Tetsunori Kobayashi, “Mask CTC: Non-
autoregressive end-to-end ASR with CTC and mask pre-
dict,” in Proc. Interspeech. ISCA, 2020, pp. 3655–3659.

[18] Yosuke Higuchi, Hirofumi Inaguma, Shinji Watanabe,
Tetsuji Ogawa, and Tetsunori Kobayashi,
“Improved
Mask-CTC for non-autoregressive end-to-end ASR,” in
Proc. ICASSP. IEEE, 2021, pp. 8363–8367.

[19] William Chan, Chitwan Saharia, Geoffrey Hinton, Mo-
hammad Norouzi, and Navdeep Jaitly, “Imputer: Se-
quence modelling via imputation and dynamic program-
ming,” in Proc. ICML. PMLR, 2020, pp. 1403–1413.

[20] Ruchao Fan, Wei Chu, Peng Chang, and Jing Xiao,
“CASS-NAT: CTC alignment-based single step non-
autoregressive Transformer for speech recognition,” in
Proc. ICASSP. ISCA, 2021, pp. 5874–5878.

[21] Linhao Dong and Bo Xu, “CIF: Continuous integrate-
and-ﬁre for end-to-end speech recognition,” in Proc.
ICASSP. IEEE, 2020, pp. 6079–6083.

[22] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Par-
mar, et al., “Conformer: Convolution-augmented trans-
former for speech recognition,” in Proc. Interspeech.
ISCA, 2020, pp. 5036–5040.

[23] Pengcheng Guo, Florian Boyer, Xuankai Chang,
Tomoki Hayashi, Yosuke Higuchi, Hirofumi Inaguma,
Naoyuki Kamo, Chenda Li, Daniel Garcia-Romero, Jia-
tong Shi, et al., “Recent developments on ESPnet toolkit
boosted by Conformer,” in Proc. ICASSP. IEEE, 2021,
pp. 5874–5878.

[24] Hui Bu, Jiayu Du, Xingyu Na, Bengu Wu, and Hao
Zheng, “Aishell-1: An open-source mandarin speech
corpus and a speech recognition baseline,” in Proc. O-
COCOSDA. IEEE, 2017, pp. 1–5.

[25] Xiong Wang, Zhuoyuan Yao, Xian Shi, and Lei Xie,
“Cascade RNN-Transducer: Syllable based streaming

