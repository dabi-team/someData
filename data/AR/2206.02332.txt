Effects of Augmented-Reality-Based Assisting Interfaces on Drivers’
Object-wise Situational Awareness in Highly Autonomous Vehicles

Xiaofeng Gao∗

Xingwei Wu†‡

Samson Ho†

Teruhisa Misu†

Kumar Akash†

2
2
0
2

n
u
J

6

]

C
H
.
s
c
[

1
v
2
3
3
2
0
.
6
0
2
2
:
v
i
X
r
a

Abstract— Although partially autonomous driving (AD) sys-
tems are already available in production vehicles, drivers
are still required to maintain a sufﬁcient level of situational
awareness (SA) during driving. Previous studies have shown
that providing information about the AD’s capability using user
interfaces can improve the driver’s SA. However, displaying
too much information increases the driver’s workload and
can distract or overwhelm the driver. Therefore, to design an
efﬁcient user interface (UI), it is necessary to understand its
effect under different circumstances. In this paper, we focus on
a UI based on augmented reality (AR), which can highlight
potential hazards on the road. To understand the effect of
highlighting on drivers’ SA for objects with different types and
locations under various trafﬁc densities, we conducted an in-
person experiment with 20 participants on a driving simulator.
Our study results show that the effects of highlighting on
drivers’ SA varied by trafﬁc densities, object locations and
object types. We believe our study can provide guidance in
selecting which object to highlight for the AR-based driver-
assistance interface to optimize SA for drivers driving and
monitoring partially autonomous vehicles.

I. INTRODUCTION

Autonomous vehicles (AVs) have the potential to revolution-
ize the transportation industry. Despite the rapid development
of the autonomous driving (AD) system, fully automated
cars are still not available on public roads. Currently, some
vehicles on the market are equipped with advanced driver
assistance systems (ADAS) that allow partially automated
driving, or SAE Level 2 (L2) automation [1]. While drivers
can brieﬂy enjoy feet-free and hands-free driving under certain
driving situations at this level of automation, they are still
required to monitor the trafﬁc conditions and prepare for
sudden maneuvers and possible takeover requests. As a result,
it is crucial to maintain the driver’s situational awareness
(SA) when interacting with the AD system and avoid the
out-of-the-loop problem [2].

With the goal of improving drivers’ SA and trust, re-
searchers investigated various ways of communication to
convey internal information to the drivers. The challenge is
that showing additional information to the drivers can increase
their cognitive load and cause distractions [3]. Showing too
much information not only prevents drivers from paying
attention to the most critical information during driving [4],
but is also against the main motivation of developing the AD
system, i.e. reducing driver workload. Therefore, we believe

∗Xiaofeng Gao is with the University of California, Los Angeles. e-mail:

xfgao@ucla.edu. The work was done at Honda Research Institute USA.

†Xingwei Wu, Samson Ho, Teruhisa Misu and Kumar Akash are with the
Honda Research Institute USA. e-mail:{sho, tmisu, kakash}@honda-ri.com
‡Xingwei Wu is currently with Zoox. e-mail: xingweiwu19@gmail.com.

Fig. 1: Our driving simulator is composed of a steering wheel
and two pedals mounted on a cockpit, and three 55-inch
displays showing the front and side views of the virtual
environment.

that a smart user interface (UI) should be able to strike a
balance between the amount of information provided and the
driver’s limited attention and cognitive load.

Results from previous studies showed that highlighting
hazardous objects via augmented reality (AR) based UI is a
promising way to increase drivers’ SA [5], [6]. Nevertheless,
those works mainly focused on evaluating the effects of
highlighting on SA across all objects. We believe a smart UI
should optimize the highlighting for each object to maintain a
proper workload and help the driver be aware of the potential
hazards that are prone to be ignored. Therefore, we need
to understand the effects of highlighting on each speciﬁc
object, depending on the object characteristics.

Speciﬁcally, in this work, we distinguish objects by three
properties 1) locations (relative to the driver), 2) types (i.e.
pedestrian or vehicle), and 3) trafﬁc densities and evaluate
the effect of highlighting considering those factors.

We implemented object highlighting via a UI on a driving
simulator based on Unreal Engine 4 (UE4), and conducted
an in-person study (N=20) on the simulator to investigate the
effect of highlighting on drivers’ attention allocation and SA
for each object in an urban environment.

The main contributions of this paper are:

• We implemented an AR-based UI in a driving simulator
to inform drivers of the AD’s perception capabilities
by highlighting hazardous objects. We focused on urban
intersections because of their complex trafﬁc conditions,
which can be demanding for the drivers to monitor.

• We designed and conducted a simulator experiment to
evaluate the impact of highlighting on drivers’ object-wise
SA and attention allocation. Speciﬁcally, we designed a

 
 
 
 
 
 
Fig. 2: This is a forward event intersection with high trafﬁc density, corresponding to the event in Figure 6a. We highlight
objects using bounding boxes on the user interface: red for pedestrians and blue for cars. In addition, we also display the ego
vehicle’s current speed and heading direction with yellow texts and arrows in the middle. During the study, this concatenated
screenshot is separately shown on three displays to simulate the ﬁeld-of-view of a driver in the real world (see Figure 1).

Fig. 3: To evaluate drivers’ SA, we pause the simulation and hide all road users. On top of the background road scene of the
intersection, we display several regions and ask users to choose which regions were occupied by pedestrians or vehicles.

novel Situational Awareness Global Assessment Technique
(SAGAT) protocol with temporal variations to measure the
same driver’s SA changes before and after the highlighting
in two identical intersections to better understand the effect
of highlighting on SA. Objects’ locations and movements
at intersections are discretized based on spatial distance
and eccentricity.

• We carefully analyzed the effect of highlighting with the
AR interface compared to non-highlighting in different
conditions, including a combination of object types, object
locations and trafﬁc densities.

The results of our study suggest that the effects of highlighting
on perception-level SA highly depend on object properties
and trafﬁc densities. We believe the results pave the way for
a smart UI that can selectively highlight objects to improve
SA for drivers of AVs, leading to more safety in driving and
monitoring partially autonomous vehicles.

II. RELATED WORK
A. Situational Awareness Measurements

SA can be generally understood as knowing what is going
on around you [2]. Over the years, various methods have
been proposed to measure SA. They can be categorized
into objective measurements (e.g. SPAM [7] and DAZE [8])
and subjective measurements (e.g. SART [9] and SARS
[10]). SAGAT is a widely-known technique to measure
SA objectively [11]. During a SAGAT session, the display
is frozen at selected times and participants are asked to
answer questions to measure SA. An advantage of SAGAT
is that participants are unable to prepare for the questions
in advance, thus minimizing the possibility of attention
bias. Studies suggested that SAGAT is a technique with

a high degree of reliability [12] and validity [13]. Apart from
direct measurements, SA can also be inferred with indirect
measurements, including eye gaze behaviors and takeovers
[14]–[16]. For a more comprehensive review of situational
awareness measurements, we refer readers to this survey [17].

B. Ways to Improve Driving Situational Awareness

Driver’s SA at SAE L2 or L3 automated driving has been
widely studied for years. Previous works have shown that
driver’s SA can be inﬂuenced by a wide range of factors, such
as age [18], driving experience [19] and working memory [20],
[21]. Since driver’s SA plays an important role in driving
safety, different methods have been proposed to enhance
SA during driving. Recently, the idea of assisting human-
machine interface (HMI) has generated much interest [22].
Studies that examined the effects of AR windshield display
(WSD) interface found signiﬁcant effects on driver’s SA,
when highlighting potential driving threats [5] or common
trafﬁc objects (e.g. cars, pedestrians, and trafﬁc signs) [6],
[23]. The SA improvement was observed at both perception
level [24] and comprehension and projection levels [25]. In
these works, the focus is to evaluate the effect of HMI on
the driver’s average SA across all trafﬁc objects between
experimental groups. Thus the driver’s SA is measured within
each treatment group without distinguishing between objects
[5], [6], [25]. Our work takes one further step in this direction:
1) we focus on evaluating the effect of HMI on driver’s SA for
each object, distinguished by their locations and types, and
2) propose a novel SAGAT protocol with temporal variations
by measuring the same participant’s SA before and after the
treatment to better analyze its effects toward a speciﬁc object.

(a) Area Discretizations based on spatial
distance and eccentricity relative to the ego
vehicle (green).

(b) Pedestrian movements. Yellows arrows
show target pedestrians’ all possible move-
ments across the intersection.

(c) Vehicles movements. Gray rectangles are
target vehicles’ initial locations and arrows
show vechicles’ possible moving directions.

Fig. 4: We ﬁrst discretize an intersection into 4 areas based on spatial distance and eccentricity relative to the green ego
vehicle. According to area discretizations, we then discretize pedestrians’ and vehicles’ all possible movements near the
intersection. For example, pedestrian A crossing the top of the intersection is considered moving in area 1, while car F going
straight on the left will be moving in areas 2 and 3.

III. METHOD

In this section, we introduce our simulation study. We start
with participant and apparatus in Section III-A and talk about
the AD system and AR cues in Section III-B. We discuss how
we discretize the object locations in Section III-C and the
details of the scenarios in Section III-D. Then we describe how
we measure attention allocation and situational awareness of
the driver in Section III-E. Finally, we go through the whole
procedure of our study in Section III-F.

A. Participants and Apparatus

A total of 20 participants (12 males, and 8 females) from
the San Francisco Bay area completed the study. Their age
ranged from 20 to 49 years old. To be eligible for the study,
each participant was required to have had a valid license for
more than two years and drive more than 5,000 miles (8,047
km) per year.

As shown in Figure 1, we used a medium-ﬁdelity driving
simulator built with AirSim [26], a plug-in for UE4, to
conduct all driving sessions. Also, Tobii Pro Glasses 31 were
used to collect the participant’s eye-tracking data.

B. SAE L2 AD System and AR Assisting Cues

The Wizard of Oz method is used to simulate realistic
AD driving. To more realistically emulate a functioning AD
system, we had an expert driver drive the ego-vehicle through
the premeditated route in the simulated environment. The
drive was recorded by saving all pedal and steering inputs
to an AD ﬁle, which then provided realistic autonomous
car behavior to participants. To ensure consistency of the
AD driving behavior, all driving data were recorded from
a single expert driver. The driving behaviors were further
reviewed by two researchers, and routes were practiced to
ensure consistency. During the driving, the participant was
requested to indicate their take-over intention by pressing the
brake pedal.

1https://www.tobiipro.com/glasses3

The AR cues were developed and assigned to highlight
speciﬁc road users within intersections. As shown in Figure 2,
AR graphics used for highlighting were consistent in shape,
i.e. a series of 3D bounding boxes that formed a cubic region
surrounding a speciﬁc type of road users (blue for vehicles
and red for pedestrians).

C. Object Location Discretization

The human visual ﬁeld can be commonly divided into
three major regions: foveal, parafoveal, and peripheral. The
foveal region extends out to an angle of 1 degree and the
parafoveal region from 1 to 5 degrees [27], [28]. Those two
together are commonly referred to as the central vision, and
the peripheral region encompasses the remainder of the visual
ﬁeld. Many researchers have noted that, as a result of the
inhomogeneity of the visual system, attention allocation and
awareness are strongly affected by the target eccentricity and
spatial distance. Detecting a target far away in the peripheral
as opposed to nearby and central vision requires longer search
times and more eye movements [29].

Considering both an object’s spatial distance (i.e. near or
far) and eccentricity (i.e. center or marginal) relative to the
driver in the ego vehicle, we categorized the object positions
in an intersection into four types of areas: the top center area
(area 1), the bottom center area (area 2), the bottom left and
bottom right areas (area 3) and the top left and top right
areas (area 4), as shown in Figure 4a. Since the pedestrians
and cars move across different areas, their movements were
also discretized (Figure 4b, Figure 4c).

D. Driving Scenario and Events

The driving scenario is an urban environment in daylight
conditions with a posted speed limit of 25 mph. Events are
triggered when the ego vehicle comes near event intersections.
During an event,
the ego vehicle ﬁrst stops before the
intersection due to a stop sign or a ﬂashing red trafﬁc
light. The vehicle then waits until the other road users
have passed the intersection following the trafﬁc rule. While

Fig. 5: Drives and intersections. Light trafﬁc route 1 (LT1) and light trafﬁc route 2 (LT2) are of low trafﬁc density, while
dense trafﬁc route 1 (DT1) and dense trafﬁc route 2 (DT2) are of high trafﬁc density. Blue dots represent event intersections
where the target objects are highlighted. Yellow dots represent event intersections where the target objects are not highlighted.
Green dots are non-event intersections where we ask dummy SAGAT questions to reduce the learning effect of SA in
event intersections. In intersections a1, b1 and c1, SAGAT questions are asked before the treatment (highlighting or not
highlighting). In intersections a2, b2 and c2, SAGAT questions are asked after the treatment.

the vehicle is waiting, the driver is asked to continuously
monitor the surroundings and take over the control if the
AD system has made an unexpected or dangerous move. The
cars and pedestrians across intersections are consistent in
appearance. Intersections are of similar sizes (L = 15.3 ± 2.0
m, W = 14.4 ± 1.2 m).

We inserted a SAGAT pause while the ego-car is waiting
for other trafﬁc at an intersection. The participant is asked to
answer the positions of objects, including cars and pedestrians
(Level 1 SA). We are particularly interested to study the SA
of some of the objects that can potentially collide with the ego
vehicle (the future trajectory of the object will intersect with
the ego-car’s future trajectory). We carefully design the event
timing so that only these objects are located in certain regions
at the SAGAT pauses. We refer to them as target objects
and other objects as distractor objects. During the SAGAT
pause, the simulation is frozen and other situational objects
(i.e. pedestrians, vehicles and trafﬁc lights) are hidden in the
simulator. Meanwhile, several regions would be displayed
on the blank scene, as shown in Figure 3. The driver is
asked to speak about all the regions where he/she believes
there were pedestrians and/or vehicles. In particular, we have
designed three types of events that correspond to three heading
directions (i.e. forward, turning left and turning right) of the
ego vehicle.

Two driving routes with opposite directions and different
trafﬁc densities were designed in this experiment (See
Figure 5). We change the trafﬁc density of routes, by adding
or removing distractor objects from event intersections. The
average number of total objects, including both distractors

and target objects, is 10 for dense trafﬁc (DT) route and 5
for light trafﬁc (LT) route drives in event intersections. Based
on each route, two drives with different event designs were
developed (LT1 & LT2 and DT1 & DT2). Figure 6 illustrates
the design of each type of event and target objects in the
event:
• Forward intersections (a1 and a2 in Figure 5). For intersec-
tions where the ego vehicle is heading straight, the target
objects are pedestrian A, vehicles F and G (Figure 6a). The
SAGAT pause occurs while pedestrian A is crossing the
intersection on the top (at area 1) and vehicles F and G
are going through the intersection in the middle (at area 1
and area 2 respectively).

• Left intersections (b1 and b2 in Figure 5). For intersections
where the ego vehicle is heading left, the target objects are
pedestrians B, C and vehicle G (Figure 6b). The SAGAT
pause occurs while pedestrian B is crossing the intersection
on the top left (at area 4), C is on the bottom center (at
area 2) and vehicle G is waiting on the top right (at area
4).

• Right intersections (c1 and c2 in Figure 5). For intersections
where the ego vehicle is heading right, the target objects
include the pedestrian D and vehicle F (Figure 6c). The
SAGAT pause occurs while vehicle F is waiting on the
bottom left (at area 3) and pedestrian D is crossing the
intersection on the bottom right (at area 3).
Each participant was randomly assigned to one of the
four experimental groups and experienced one of the four
combinations of two drives (routes) in sequential order: group
(i) LT1 and then DT2; group (ii) DT2 and then LT1; group

(a) Forward event intersection

(b) Left event intersection

(c) Right event intersection

Fig. 6: We display the locations and heading directions of target objects in three types of event intersections. For clarity,
distractor objects are not shown here. The green rectangle is the ego vehicle. Gray rectangles are other vehicles’ locations
and gray arrows show their moving directions. Yellows arrows show pedestrians’ movements across the intersection.

(iii) LT2 and then DT1; and group (iv) DT1 and then LT2.
Figure 5 illustrates the event/non-event intersections for all
four drives. For each drive, there are six event intersections,
including two left intersections, two right intersections and
two forward intersections. In some event intersections (blue
dots in Figure 5), the target objects are highlighted. In order
to reduce the learning effect between the two intersections
of the same type within a drive, we also design a non-
event intersection (green dots) between them: in these non-
event intersections, the driver also needs to answer a dummy
SAGAT question, such as the heading direction of the vehicle
and the color of the trafﬁc light. The average duration of a
drive is 15 minutes.

Our goal is to understand how highlighting would change
the SA and attention for different objects. Thus we measure
SA from the same driver at two different timings in one
type of intersection: 1) before treatment (i.e. highlighting
or not highlighting the target object) and 2) 1 second after
the treatment. To reduce the order effect, we implemented
two separate intersections with the same type of events, but
with different timing of the SAGAT pauses. For example, in
drive LT1, the SAGAT pause in one of the forward event
intersections (a2) is delayed by 1 second, while the SAGAT
pause in the other forward event intersection (a1) is not
delayed. The purpose of this delay is to study how highlighting
or not highlighting the target object within this delayed period
would change drivers’ SA. Since the delayed and undelayed
intersections have exactly the same event, we can compare
the driver’s SA responses to better understand the effect of
highlighting.

E. Dependent Variables

Attention allocation. Attention allocation is strongly associ-
ated with situational awareness. To form situational awareness,
one needs to perceive and process the environment [2].
However, the limited capacity of human attentional resources
in combination with the excessive attentional demands in a
dynamic driving environment can result in a loss of situational
awareness. To study attention allocation, one well-established
measurement is to track human ﬁxation behavior. We collect

drivers’ eye-tracking data with Tobii Pro Glasses 3. We also
annotate the target objects in each event intersection using
Vatic [30]. Based on the ﬁnding that humans can recognize
information in the fovea (2.5 deg [27], [28]) within 120 ms
[31], we deﬁne that the driver has ﬁxated on an object if the
gaze has stayed within 2.5 degrees from the center of the
object for more than 120 milliseconds.
Situational awareness. To measure drivers’ situational aware-
ness, we adopt
the SAGAT technique and ask drivers
questions about the location of pedestrians and vehicles
during the pauses in the event intersections (Figure 3). We
focus on the Level 1 SA (perception) since it is the most
fundamental one. Participants were asked to select all the
regions that they think had a pedestrian or a vehicle the
moment before the SAGAT pause. Two images with region
highlightings, each corresponding to pedestrians and vehicles
(e.g., Figure 3 is for pedestrians), are presented in sequence
with a corresponding SAGAT question. They were also asked
to provide a conﬁdence level (from 0 to 100) for each region
they speciﬁed, which is further discretized to low conﬁdence
(0-50) and high conﬁdence (51-100). Regions that are not
selected by the participant are treated as low conﬁdence.
The order of SAGAT questions (i.e. pedestrian or vehicle) is
randomized to reduce the order effect. During the analysis,
we study the SA response on the discretized regions that are
occupied by the target objects.

F. Procedure

Participants ﬁrst completed a pre-study survey to provide
demographics and driving experience. They also ﬁlled in a
questionnaire designed to evaluate their trust in automation
[32]. The moderator then gave each participant a brief
introduction to the study and set up the Tobii glasses and
the driving simulator. The study began with a practice drive
where the participant was asked a sample question during
a SAGAT pause. During the practice drive, the participant
was asked to take over control of the vehicle using the brake
pedal whenever they felt uncomfortable with the AD system.
The participant was also given a chance to practice answering
the SAGAT question at an intersection as well as indicating

(a) Fixation time on cars

Fig. 7: Drivers’ ﬁxation time (in second) on each car and pedestrian given trafﬁc density. “N” represents non-highlighting
results and “H” represents highlighting results. We report the p-value between highlighting conditions for each object.

(b) Fixation time on pedestrians

their intention to take over. After the practice drive, the
participant was randomly assigned to an experimental group,
and went through two standard drives of different trafﬁc
densities. During a drive, each participant experienced six
event intersections and two non-event intersections (Figure 5).
In each event intersection, the participant was asked about
the locations of vehicles and pedestrians during the SAGAT
pause (Figure 3).

IV. RESULTS

In this section, we present the results of our study, analyzing
how highlighting objects changes drivers’ attention allocation
and situational awareness during their interaction with an AD
system based on the data collected from the driving simulator
experiment.

A. Attention Allocation

For attention allocation, we analyze the driver’s ﬁxation
time on target objects. Since our goal is to study how
highlighting would change the driver’s attention, we focus on

the driver’s ﬁxation during the delayed period (Section III-
D). We show the results for speciﬁc cars and pedestrians
in Figure 7 at different trafﬁc densities. Running a pairwise
t-test, we found a signiﬁcant effect (p = .02) for highlighting
the top center pedestrian, i.e. the pedestrian A, when the
trafﬁc density is low. We don’t ﬁnd the same trend for top
center pedestrians at high trafﬁc density.

B. Situational Awareness

SA response accuracy. We analyze drivers’ responses to the
SAGAT questions at delayed intersections, when different
highlighting conditions have been applied to the target objects
(Figure 8). Across all objects, driver’s SA on highlighted
objects (M = 0.60, SD = 0.49) are higher than the unhigh-
lighted ones (M = 0.52, SD = 0.50), but the difference is
not statistically signiﬁcant (p = .14). For cars (Figure 8a),
we observed a signiﬁcant difference between highlighting
conditions for the top right car (car G) in a low trafﬁc
density environment(p = .02) and for the top center car and
bottom center car (cars F and G) in high trafﬁc density

(a) Response accuracy on cars

Fig. 8: Drivers’ SAGAT question response accuracy in delayed intersections. “N” represents non-highlighting results and “H”
represents highlighting results. We report the p-value between highlighting conditions for each object.

(b) Response accuracy on pedestrians

environment(p = .02). For pedestrians (Figure 8b), we only
found a signiﬁcant change in SA for top center pedestrians
during light trafﬁc routes (p < .0001). The results indicate
that highlighting can improve the SA for the top right car
and the top center pedestrian at low trafﬁc density, while
decreasing the SA for the bottom center car and top center
car at high trafﬁc density. The Pearson correlation coefﬁcient
between ﬁxation time and SA response accuracy is r = .12
(p = .03), indicating a weak correlation between attention
and SA.

SA transition. We ﬁrst analyze the transition of drivers’
SA from undelayed pauses to delayed pauses across all
objects, when we apply different highlighting conditions to
the target objects during time t and time t + 1 (Figure 9).
Given low trafﬁc density, for drivers with an initial low
SA on target objects, highlighting leads to SA improvement
(from low to high) for 55.3% of the drivers, compared to
36.8% in the non-highlighting conditions. For drivers with
an initial high SA on target objects, we found that those in
the highlighting conditions are more likely to maintain their

high SA (78.6%) compared to drivers in the non-highlighting
conditions (69.0%) for low density. Similarly, when the trafﬁc
density is high, highlighting also helps more drivers maintain
high SA (66.7%) compared to the no highlighting (59.5%).
Running a two-sample proportion test, however, we didn’t
ﬁnd any signiﬁcant effect of highlighting on SA transition
for either trafﬁc density across all objects.

Looking at the SA transition for speciﬁc objects, we found
from a proportion test that for the top center pedestrian,
highlighting can signiﬁcantly increase the proportion of
improve low SA (p = .0007) and maintain
drivers that
high SA (p = .03) compared to the control condition when
the trafﬁc density is low (Figure 10). On the contrary, for
the bottom center car, we found that highlighting actually
decreases the proportion of drivers that maintain high SA
(p = .02) when the trafﬁc density is high (Figure 11). We
didn’t ﬁnd any signiﬁcant difference in SA transition between
highlighting conditions for other objects.

(a) low density w/o highlighting (b) low density w/ highlighting

(a) low density w/o highlighting (b) low density w/ highlighting

(c) high density w/o highlighting (d) high density w highlighting

(c) high density w/0 highlighting (d) high density w/ highlighting

Fig. 9: SA transition conditioned on trafﬁc density and
highlighting across all objects. ”SA at time t” represents
drivers’ SA response before the treatment, while ”SA at time
t+1” is for SA response after the treatment. The shade of each
region represents the proportion of the samples falling into
each category. Darker color represents a higher proportion.

V. DISCUSSION

The results indicate that the effect of highlighting varies a
lot depending on the situation. Highlighting can signiﬁcantly
improve SA on certain objects at low trafﬁc density. However,
it can also decrease drivers’ SA of some objects at high trafﬁc
density. These ﬁndings can provide guidance in selecting
which object to highlight for the UI to improve the driver’s
SA while driving and monitoring SAE L2 or L3 AVs.

A. Attention Allocation, Workload and SA

Driving is a visual and motor control process. Thus,
drivers’ attention allocation and workload play important
roles in establishing their SA. Previous works have proposed
quantitative methods to model the interplay between attention
allocation, workload and SA [33]–[35]than on unhighlighted
ones. Speciﬁcally, the attention allocation process can be
largely inﬂuenced by the salience of an object and workload
[36]. A high workload can result in attention tunneling and
negatively impact SA. In our study results, highlighting the
cars in the center of the driver’s ﬁeld of view signiﬁcantly
decreases SA when the trafﬁc density is high, while the
difference is not signiﬁcant when the trafﬁc density is lower.
This can probably be explained by (i) the driver’s high
workload given the dense trafﬁc ii) the highlighting AR cues
induce additional workload (iii) the fact that cars in the center
are already very salient even without highlighting. These
reasons can also explain why signiﬁcant SA improvement
was found for the top center pedestrian (which is not visually
salient and easy to be ignored by the driver) at low trafﬁc
density and why improvement is not signiﬁcant at higher
trafﬁc density (due to the driver being overwhelmed by the

Fig. 10: SA transition for the top center pedestrian (pedestrian
A in Figure 6a). The shade of each region represents the
proportion of the samples falling into each category. Darker
color represents a higher proportion.

dense trafﬁc). We believe these results shed light on designing
object-speciﬁc AR cues on human-machine interfaces.

B. Comparison with Previous Studies

Previous works focus on evaluating the driver’s average
SA across all trafﬁc objects in different experimental groups.
By controlling a speciﬁc object’s spatial characteristic in the
driving simulator, we are able to further study the transition of
the user’s SA on the object before and after the highlighting.
Results from previous works [5], [6], [25] showed that using
an AR interface could improve drivers’ average SA across all
objects. Thanks to the unique study design covering objects
properties and trafﬁc conditions in common intersections
as well as the proposed SAGAT protocol with temporal
variations, we are able to see signiﬁcant positive effects of
AR cues on some objects and negative effects on some other
objects. These results extend knowledge of the community on
the effects of AR cues beyond speciﬁcally-designed scenarios
and hand-picked objects, showing how different objects can
beneﬁt from the AR cues in more general driving scenarios.

C. Limitations and Future Work

Our UI is implemented in a driving simulator, which
enables us to control the timing of events accurately. Driving
scenarios in the real world are more complex and have more
variety than our examined scenarios. In reality, the AR cues
can be implemented by detecting vehicles and pedestrians
from sensors and highlighting them using bounding boxes
on the AR-HUD. In addition, every participant experienced
two similar event intersections - one before and one after
the highlighting. We ask dummy SAGAT questions in non-
event intersections between the two events intersections to
reduce the learning effect, but the effect may not be canceled
off completely. Additionally, we measure SA using SAGAT,

[3] T. Helldin, “Transparency for future semi-automated systems: Effects
of transparency on operator performance, workload and trust,” Ph.D.
dissertation, ¨Orebro Universitet, 2014.

[4] M. Ananny and K. Crawford, “Seeing without knowing: Limitations of
the transparency ideal and its application to algorithmic accountability,”
new media & society, vol. 20, no. 3, pp. 973–989, 2018.

[5] P. Lindemann, T.-Y. Lee, and G. Rigoll, “Catch my drift: Elevating
situation awareness for highly automated driving with an explanatory
windshield display user interface,” Multimodal Technologies and
Interaction, vol. 2, no. 4, p. 71, 2018.

[6] M. Colley, B. Eder, J. O. Rixen, and E. Rukzio, “Effects of semantic
segmentation visualization on trust, situation awareness, and cognitive
load in highly automated vehicles,” in Proceedings of the 2021 CHI
Conference on Human Factors in Computing Systems, 2021, pp. 1–11.
[7] F. T. Durso, C. A. Hackworth, T. R. Truitt, J. Crutchﬁeld, D. Nikolic,
and C. A. Manning, “Situation awareness as a predictor of performance
for en route air trafﬁc controllers,” Air Trafﬁc Control Quarterly, vol. 6,
no. 1, pp. 1–20, 1998.

[8] D. Sirkin, N. Martelaro, M. Johns, and W. Ju, “Toward measurement
of situation awareness in autonomous vehicles,” in Proceedings of the
2017 CHI Conference on Human Factors in Computing Systems, 2017,
pp. 405–415.

[9] R. Taylor, “Situational awareness rating technique (sart): The devel-
opment of a tool for aircrew systems design. situational awareness
in aerospace operations (agard-cp-478),” Neuilly Sur Seine, France:
NATO-AGARD, 1990.

[10] W. L. Waag and M. R. Houck, “Tools for assessing situational
awareness in an operational ﬁghter environment.” Aviation, space,
and environmental medicine, 1994.

[11] M. R. Endsley, “Situation awareness global assessment technique
(sagat),” in Proceedings of the IEEE 1988 national aerospace and
electronics conference.

IEEE, 1988, pp. 789–795.

[12] M. R. Endsley and C. A. Bolstad, “Individual differences in pilot
situation awareness,” The International Journal of Aviation Psychology,
vol. 4, no. 3, pp. 241–264, 1994.

[13] M. R. Endsley, “Predictive utility of an objective measure of situation
awareness,” in Proceedings of the Human Factors Society annual
meeting, vol. 34, no. 1. SAGE Publications Sage CA: Los Angeles,
CA, 1990, pp. 41–45.

[14] Y. Barnard and F. Lai, “Spotting sheep in yorkshire: Using eye-tracking
for studying situation awareness in a driving simulator,” in Human
Factors: A System View of Human, Technology and Organisation.
Annual Conference of the Europe Chapter of the Human Factors and
Ergonomics Society 2009, 2010.

[15] Y. Yang, B. Karakaya, G. C. Dominioni, K. Kawabe, and K. Bengler,
“An hmi concept to improve driver’s visual behavior and situation
awareness in automated vehicle,” in 2018 21st International Conference
on Intelligent Transportation Systems (ITSC).
IEEE, 2018, pp. 650–
655.

[16] H. Zhu, T. Misu, S. Martin, X. Wu, and K. Akash, “Improving driver
situation awareness prediction using human visual sensory and memory
mechanism,” arXiv preprint arXiv:2111.00087, 2021.

[17] P. Salmon, N. Stanton, G. Walker, and D. Green, “Situation awareness
measurement: A review of applicability for c4i environments,” Applied
ergonomics, vol. 37, no. 2, pp. 225–238, 2006.

[18] C. A. Bolstad, “Situation awareness: does it change with age?” in
Proceedings of the human factors and ergonomics society annual
meeting, vol. 45, no. 4. SAGE Publications Sage CA: Los Angeles,
CA, 2001, pp. 272–276.

[19] T. J. Wright, S. Samuel, A. Borowsky, S. Zilberstein, and D. L. Fisher,
“Experienced drivers are quicker to achieve situation awareness than
inexperienced drivers in situations of transfer of control within a level
3 autonomous environment,” in Proceedings of the Human Factors and
Ergonomics Society Annual Meeting, vol. 60, no. 1. Sage Publications
Sage CA: Los Angeles, CA, 2016, pp. 270–273.

[20] K. R. Johannsdottir and C. M. Herdman, “The role of working memory
in supporting drivers’ situation awareness for surrounding trafﬁc,”
Human factors, vol. 52, no. 6, pp. 663–673, 2010.

[21] A. Heenan, C. M. Herdman, M. S. Brown, and N. Robert, “Effects of
conversation on situation awareness and working memory in simulated
driving,” Human factors, vol. 56, no. 6, pp. 1077–1092, 2014.
[22] C. Merenda, H. Kim, K. Tanous, J. L. Gabbard, B. Feichtl, T. Misu,
and C. Suga, “Augmented reality interface design approaches for goal-
directed and stimulus-driven driving tasks,” IEEE transactions on

(a) low density w/o highlighting (b) low density w/ highlighting

(c) high density w/o highlighting (d) high density w/ highlighting

Fig. 11: SA transition for the bottom center car (car F in
Figure 6a). The shade of each region represents the proportion
of the samples falling into each category. Darker color
represents a higher proportion.

which is known to be highly reliable [12]. The drawback is
that SAGAT requires the participant to memorize the objects
and thus can also increase the workload [37]. Non-intrusive
SA measures can be considered in a future study to ensure an
accurate measure of drivers’ workload when interacting with
an AD system. Finally, in the future work, we plan to consider
other object features (e.g. object colors and speed) and the
differences in the intersections’ background environment,
which are also likely to affect SA.

VI. CONCLUSION

This work aims to investigate the effects of highlighting
objects with an AR interface on drivers’ perception-level
SA for SAE L2 or L3 AVs under different circumstances,
including object types, locations and trafﬁc densities in
urban environments. We conducted a user study in a driving
simulator (N = 20). The results show that highlighting has
a positive impact on SA when the trafﬁc density is low and
the highlighted object has originally low visual saliency, and
sometimes causes a reduction in SA when the object is already
very salient even without highlighting during dense trafﬁc.
This work extends the knowledge on methods to improve
driver’s situational awareness for autonomous vehicles, and
enables the development of a smart driver-assistance interface
that can selectively highlight objects to improve SA for drivers
monitoring partially autonomous vehicles.

REFERENCES

[1] S. international, “Taxonomy and deﬁnitions for terms related to driving

automation systems for on-road motor vehicles,” SAE, 2018.

[2] M. R. Endsley, “Design and evaluation for situation awareness
enhancement,” in Proceedings of the Human Factors Society annual
meeting, vol. 32, no. 2. Sage Publications Sage CA: Los Angeles,
CA, 1988, pp. 97–101.

visualization and computer graphics, vol. 24, no. 11, pp. 2875–2885,
2018.

[23] J. Wang, W. Wang, P. Hansen, Y. Li, and F. You, “The situation
awareness and usability research of different hud hmi design in driving
while using adaptive cruise control,” in International Conference on
Human-Computer Interaction. Springer, 2020, pp. 236–248.
[24] Y. Tong and B. Jia, “An augmented-reality-based warning interface for
pedestrians: User interface design and evaluation,” in Proceedings of
the Human Factors and Ergonomics Society Annual Meeting, vol. 63,
no. 1.
SAGE Publications Sage CA: Los Angeles, CA, 2019, pp.
1834–1838.

[25] M. T. Phan, I. Thouvenin, and V. Fr´emont, “Enhancing the driver
awareness of pedestrian using augmented reality cues,” in 2016 IEEE
19th International Conference on Intelligent Transportation Systems
(ITSC).

IEEE, 2016, pp. 1298–1304.

[26] S. Shah, D. Dey, C. Lovett, and A. Kapoor, “Airsim: High-ﬁdelity
visual and physical simulation for autonomous vehicles,” in Field and
service robotics. Springer, 2018, pp. 621–635.

[27] W. W. Nelson and G. R. Loftus, “The functional visual ﬁeld during pic-
ture viewing.” Journal of Experimental Psychology: Human Learning
and Memory, vol. 6, no. 4, p. 391, 1980.

[28] N. Quinn, L. Csincsik, E. Flynn, C. A. Curcio, S. Kiss, S. R. Sadda,
R. Hogg, T. Peto, and I. Lengyel, “The clinical relevance of visualising
the peripheral retina,” Progress in retinal and eye research, vol. 68,
pp. 83–109, 2019.

[29] M. Carrasco, D. L. Evert, I. Chang, and S. M. Katz, “The eccentricity
effect: Target eccentricity affects performance on conjunction searches,”
Perception & psychophysics, vol. 57, no. 8, pp. 1241–1261, 1995.
[30] C. Vondrick, D. Patterson, and D. Ramanan, “Efﬁciently scaling up
crowdsourced video annotation,” International journal of computer
vision, vol. 101, no. 1, pp. 184–204, 2013.

[31] K. Rayner and M. Castelhano, “Eye movements,” Scholarpedia, vol. 2,

no. 10, p. 3649, 2007.

[32] J.-Y. Jian, A. M. Bisantz, and C. G. Drury, “Foundations for an empir-
ically determined scale of trust in automated systems,” International
journal of cognitive ergonomics, vol. 4, no. 1, pp. 53–71, 2000.
[33] C. D. Wickens, J. S. McCarley, A. L. Alexander, L. C. Thomas,
M. Ambinder, and S. Zheng, “Attention-situation awareness (a-sa)
model of pilot error,” Human performance modeling in aviation, pp.
213–239, 2008.

[34] L. Shuang, W. Xiaoru, and Z. Damin, “A quantitative situational
awareness model of pilot,” in Proceedings of
the International
Symposium on Human Factors and Ergonomics in Health Care, vol. 3,
no. 1.
SAGE Publications Sage CA: Los Angeles, CA, 2014, pp.
117–122.

[35] S. Liu, X. Wanyan, and D. Zhuang, “Modeling the situation awareness
by the analysis of cognitive process,” Bio-medical materials and
engineering, vol. 24, no. 6, pp. 2311–2318, 2014.

[36] C. D. Wickens, “Situation awareness and workload in aviation,” Current
directions in psychological science, vol. 11, no. 4, pp. 128–133, 2002.
[37] M. Fujino, J. Lee, T. Hirano, Y. Saito, and M. Itoh, “Comparison
of sagat and spam for seeking effective way to evaluate situation
awareness and workload during air trafﬁc control task,” in Proceedings
of the Human Factors and Ergonomics Society Annual Meeting, vol. 64,
no. 1.
SAGE Publications Sage CA: Los Angeles, CA, 2020, pp.
1836–1840.

