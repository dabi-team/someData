GrabAR: Occlusion-aware Grabbing Virtual Objects in AR
Xiao Tang, Xiaowei Hu, Chi-Wing Fu, and Daniel Cohen-Or

0
2
0
2

n
u
J

3
2

]

R
G
.
s
c
[

3
v
7
3
6
0
1
.
2
1
9
1
:
v
i
X
r
a

Figure 1. Most AR applications today ignore the occlusion between real (a) and virtual (b) objects when incorporating virtual objects in user’s view
(c). Using depth from 3D sensors relieves the issue, but the depth is not accurate enough and may not match the rendered depth for virtual objects, so
undesirable artifacts often appear; see green arrows in (d). Our GrabAR learns to compose real and virtual objects with natural partial occlusion (e).

ABSTRACT
Existing augmented reality (AR) applications often ignore
the occlusion between real hands and virtual objects when
incorporating virtual objects in user’s views. The challenges
come from the lack of accurate depth and mismatch between
real and virtual depth. This paper presents GrabAR, a new
approach that directly predicts the real-and-virtual occlusion
and bypasses the depth acquisition and inference. Our goal is
to enhance AR applications with interactions between hand
(real) and grabbable objects (virtual). With paired images of
hand and object as inputs, we formulate a compact deep neural
network that learns to generate the occlusion mask. To train
the network, we compile a large dataset, including synthetic
data and real data. We then embed the trained network in
a prototyping AR system to support real-time grabbing of
virtual objects. Further, we demonstrate the performance of
our method on various virtual objects, compare our method
with others through two user studies, and showcase a rich
variety of interaction scenarios, in which we can use bare hand
to grab virtual objects and directly manipulate them.

Author Keywords
Augmented reality, occlusion, interaction, neural network

CCS Concepts
•Computing methodologies → Mixed / augmented reality;
Neural networks;

INTRODUCTION
Augmented Reality (AR) [23, 8] has become more popular
nowadays with many phone applications supporting it. How-

Submitted to UIST’20, October 20–23, 2020, Minneapolis, MN, USA

ACM ISBN 978-1-4503-6708-0/20/04. . . $15.00
DOI: https://doi.org/10.1145/3313831.XXXXXXX

ever, most AR applications today simply put virtual objects as
an overlay above the real hands and cannot support free-hand
interactions with the virtual objects. It remains challenging
to incorporate virtual objects into the real world, such that
the real hands and virtual objects are perceived to naturally
co-exist in the views, and further, we can directly interact with
the virtual objects, as if they are in our physical world.

To achieve the goal, one main challenge is handling the occlu-
sion between the real and virtual objects, since they may fully
or partially occlude one another. Figure 1 shows an example:
(a) user’s hand in the AR view and (b) a virtual can. If we
simply draw the can over the hand (c), the result is unrealistic,
since parts of the ﬁngers should go above the can when the
hand grabs the can. Occlusion (or interposition) is a crucial
visual cue [35, 1], allowing us to rank the relative proximity
among objects in views. It comes naturally in the real world,
but is typically ignored in existing AR applications.

One common approach to handle the occlusion is to acquire
the depth of our hand using 3D sensors, then determine the
occlusion by comparing the depth of the hand and the ren-
dered depth of the virtual objects [3, 15]. Figure 1(d) shows a
typical result. Certainly, this depth-based approach alleviates
the occlusion problem. However, depth acquisition is often
noisy and imprecise, so the hand-object boundary is usually er-
roneous with obvious ﬂickering during the interactions. Also,
user’s hand may easily penetrate the virtual object (see arrows
in Figure 1(d)), since the real depth of the hand may not pre-
cisely match the virtual depth from renderings. Particularly,
without haptic feedback, it is hard for one to avoid penetration.
Furthermore, this approach requires an additional depth sensor
attached and registered with the RGB camera.

Recently, RGB-based hand shape estimation has drawn atten-
tion [19, 5] with the potential to solve the occlusion problem.
However, the approach has several drawbacks. First, the train-
ing data for hand shape estimation is hard and tedious to
collect, since annotations are difﬁcult and time-consuming.
Second, existing datasets, e.g., [52], mostly use a general hand

 
 
 
 
 
 
Figure 2. Interaction scenarios empowered by GrabAR—user can grab these “virtual” objects and manipulate them interactively with functionality.

model to represent the hand shape, so the real hand and hand
model often misalign, leading to obvious artifacts in our views.
Third, hand shape estimation is a regression problem, which
is sensitive to outliers; hence, the robustness of the existing
methods is still weak. We will provide visual comparisons
with the state-of-the-art method later in the paper.

In this paper, we show that

by learning from natural grabbing poses in the image
space, we are able to obtain plausible occlusion between
the real hand and virtual objects, and further enable
direct hand interactions with virtual objects.

In this work, we present GrabAR, a new approach to resolve
the occlusion between hand (real) and grabbable objects (vir-
tual) by learning to determine the occlusion from the hand
grabbing poses. By using GrabAR, one can use his/her hand in
physical space to grab the virtual objects in AR view, in which
the hand can be naturally composed with the virtual object, as
if the hand directly grabs a real object; see Figure 1(e).

Technically, GrabAR is a purely image-based approach that
computes the occlusion entirely in the image space, without
acquiring or inferring depth. To design and train a deep neural
network to determine the occlusion, the main challenges come
from the time performance requirement and the dataset. To this
end, we design a compact deep neural network by adopting
a fast global context module to aggregate global information
of grabbing postures and formulating a lightweight detail en-
hancement module to enhance the boundary details between
the real and virtual objects, with two novel loss terms to pro-
gressively focus on the regions of interest, while penalizing
non-smooth boundaries. Also, we compiled a large synthetic
dataset and a small real dataset to enable effective network
training. Further, we implemented a prototyping AR system
with the trained network and showcase a rich variety of interac-
tion scenarios, e.g., a scrollable virtual phone and a clickable
virtual lightsaber, in which bare hand can directly grab and
manipulate the virtual object, expanding the design space of
AR applications; see Figure 2. Lastly, we performed various

experiments both quantitatively and qualitatively, and two user
studies, to evaluate our method among different methods.

RELATED WORK
Hand-based AR interactions were ﬁrst realized using data
gloves [12, 47, 28], e.g., Benko et al. [4]. However, the de-
vice is expensive, tedious to set up, fragile, and also, hinders
the user actions. Later, many works explore depth sensors
and hand tracking for hand-object interactions in AR. Rad-
kowski et al. [37] use the Kinect sensor to enable users to
move virtual objects with ﬁst/open hand gestures. Moser et
al. [29] implement an optical see-through AR system using
LeapMotion [26]. However, the depth sensor is used only to
track the hand, without resolving the hand-object occlusion.

Some recent works begin to explore hand-object occlusions in
AR. Feng et al. [15] use LeapMotion and combine a tracking-
based method and a model-based method to estimate the oc-
clusion mask. Though some results are shown for few objects,
their approach requires a cumbersome set up to customize
user’s hand shape on paper, then track and estimate the hand
geometry using a depth sensor for producing the occlusion
mask. Battisti et al. [3] leverage stereo images from LeapMo-
tion to obtain depth of hands, then compare it with rendered
depth from virtual objects to determine the hand-object occlu-
sion. As discussed in the introduction, this approach, however,
suffers from various issues due to depth inaccuracy and mis-
match between the acquired depth and rendered depth.

Vision-based methods typically require a monocular camera to
track user’s hands, which are more generic for use, e.g., with
smartphones. However, hand-pose and depth estimation in
monocular views are still challenging, so most existing works
focus on gesture recognition instead of occlusion resolution.
Chun et al. [11] develop a simple hand recognition method for
AR interactions. Choi et al. [10] estimate the six-DoF pose of
a jazz hand for simple virtual object overlay. Song et al. [42]
use an RGB camera to recognize hand shapes, estimate the
mean hand-camera distance, and use the distance for assorted
interactions, e.g., selection. However, the result is still not
sufﬁciently precise to resolve the hand-object occlusion.

Figure 3. GrabAR is a new approach to resolve the occlusion between real (hand) and virtual (grabbable) objects in the AR views. Given an image pair
of hand and virtual objects (a), we ﬁrst preprocess them (b) and feed them to our neural network (c). The network predicts an occlusion mask that
indicates the hand regions that should overlay above the virtual object. Hence, we can readily compose the two input images using the network-predicted
occlusion mask, and produce a more natural composition of the real and virtual objects in the AR view (d).

Hand pose/shape estimation aims to employ a 3D hand mesh
to estimate an approximate 3D hand shape/pose from RGBD
images [39, 31, 50, 49], single RGB images [51, 6, 36, 30, 2,
19, 5], or single depth images [27]. Among them, Ge et al. [19]
and Boukhayma et al. [5] predict both the hand pose and 3D
hand model. However, their focus is on the hand shape/pose
instead of precise depth for hand and ﬁngers. Hence, if we
take their results to determine the occlusion relations, we still
have the hand-object penetration problem, which is nontrivial
on its own. In this work, our new approach bypasses the depth
estimation and avoid the hand-object penetration altogether by
directly predicting the occlusion in the image space.

Occlusion estimation [38, 20, 43, 46] is a long-standing prob-
lem, aiming to estimate the object boundaries in images and to
identify the occlusion relations between objects per boundary
segment. Early works extract edge features [38] and geometric
grouping cues [43] to identify the foreground and background.
Recently, deep neural networks are explored to detect ob-
ject boundaries and estimate the occlusion [46, 45], but so
far, works in this area focus on foreground (ﬁgure) and back-
ground (ground). We are not aware of any method that directly
reasons the occlusion between real and virtual objects.

Single-view depth estimation and occlusion estimation are
inter-related problems [46]. If we obtain accurate depth for an
image, we can easily infer the occlusions. Conversely, if we
ﬁnd the occlusion boundaries, we can take them to improve
the depth estimation, as shown in works such as [38, 20].

Nowadays, single-view depth estimation is usually powered by
SLAM [14, 32], which takes a video stream from a single cam-
era as input and predicts depth by means of an optimization.
Valentin et al. [44] use 6-DoF-pose trackers on smartphones to
estimate a sparse depth map via stereo matching. Holynski et
al. [21] leverage sparse SLAM points extracted from a video
stream and depth edges predicted from optical ﬂow. However,
SLAM-based methods typically assume static scenes, which
may not hold in scenarios with hand interactions.

Another stream of work explores depth estimation for sin-
gle image in monocular views, which beneﬁts scenarios with
dynamic interactions. Early works learn to predict depth
in images using hand-crafted features, e.g., Markov random
ﬁeld [40, 41], light ﬂow [17], and perspective geometry [24].

Recently, convolutional neural networks (CNN) are explored
for predicting depth. Eigen et al. [13] adopt a CNN to predict a
coarse depth map then apply another CNN to reﬁne the result.
Laina et al. [25] develop a deep residual network and leverage
the reverse Huber loss to predict depth. Recently, Fu et al. [16]
formulate network learning as an ordinal regression problem
and develop a spacing-increasing discretization strategy to
discretize depth, whereas Nicodemou et al. [34] adopt the
hourglass network [33] to infer depth of human hand.

The occlusion relationship in our task can be inferred from
depth estimation. However, taking a depth estimation ap-
proach to the problem requires high-quality depth for hands,
which is hard to acquire using commodity depth sensors. More
importantly, the estimated depth is often not accurate enough
for resolving the hand-object penetration for grabbing gestures
in AR. Our GrabAR is a new approach that directly predicts
the occlusion between real hand and virtual grabbable objects,
and bypasses the need to predict or acquire depth.

OVERVIEW
GrabAR enables not only a natural composition of real hand
(camera view) and virtual object (rendering) with partial oc-
clusion, but also direct use of bare hands in physical space to
grab the virtual objects in AR view and manipulate them, to a
certain extent. Figure 3 presents an overview of our approach.
The input is a pair of hand (real) and object (virtual) images (a).
To compose them with partial occlusion, we ﬁrst preprocess
them (b) and feed the results into our deep neural network (c).
The network then predicts an occlusion mask that marks the
portions of the hand over the virtual object. Using this mask,
we can then correctly compose the hand and virtual object in
AR. Note the partial occlusion achieved in our result (d).

The main challenges come from the dataset and the time per-
formance requirement. First, the real hand and virtual object
images must be compatible in the view space, so the hand pose
in training data should appear to grab the virtual object in the
view. Second, we need ground-truth occlusion masks to super-
vise the network training. However, to obtain accurate ground
truths is typically very tedious, due to the manual labeling
work; yet, training a network often requires a large amount of
data. Lastly, the network inference should run in real-time, so
the network architecture cannot be too complex.

Figure 4. Synthetic data preparation. (a) Real hand in front of LeapMo-
tion. (b) Rigged 3D hand model (virtual) corresponding to the real hand.
(c) Target virtual object added next to the virtual hand. (d) Hand pose
that appears to grab the virtual object. (e) Visible portions (blue) of the
virtual object in the view. (f) Automatically-generated occlusion mask.

Figure 5. Real data preparation.
(a) Captured hand in real physical
space. (b) Rendered hand in virtual space. (c) Semi-transparent virtual
(d) Adjusted hand pose to grab the virtual
object overlaid on hand.
object.
(f) After we
grab-and-rotate the virtual object with our real hand.

(e) Manually-labeled occlusion mask (on hand).

To meet these challenges, we ﬁrst employ 3D renderings to
produce a large synthetic data with automatically-generated
labels. Then, we compile a rather small set of real images
with manual labels to overcome the domain gap of synthetic
images. So, we can leverage a large amount of training data
and signiﬁcantly reduce the burden of manual labeling. Next,
rather than directly taking the hand and virtual object images
as the network inputs, we preprocess them by overlaying the
virtual object on hand and coloring it in blue; see Figure 3 (b).
Hence, the virtual object can look more distinctive from the
hand, for the network to better learn from the inputs. Then,
we formulate a compact neural network and train it to locate
hand portions above virtual object (see “Method” Section).
Equipped with the network to generate occlusion masks, we
further create a prototyping AR system and deliver various
interaction scenarios. Please watch the supplementary video
for real-time capture of the various demonstrations.

DATASETS
Our dataset contains image tuples of (i) hand; (ii) virtual ob-
ject; and (iii) associated occlusion mask (as ground truth),
which marks the visible portions of hand and object. To pre-
pare compatible images (i)-(iii) in the same view, a naive
approach is to manually label the visible regions in hand and
virtual object images. However, such a process is very tedious
and the hand pose may not match the virtual object in the AR
view. Another approach is to use an RGBD sensor, but the
acquired depth is often noisy and may not match the rendered
depth, so we still need to manually edit the occlusion mask.

To address these challenges and obtain a large amount of
data, we prepare a large synthetic data and a small real data,
and design a two-stage procedure to train our neural network:
ﬁrst, we use the synthetic data to pre-train the network, then
we use the real data to ﬁne-tune it. The key advantage be-
hind is that we can efﬁciently obtain large synthetic data with
automatically-generated occlusion masks without the burden

Figure 6. Virtual objects employed in preparing the datasets.

of manual labeling, and the real data can further help to address
the domain gap due to the synthetic renderings.

Synthetic dataset. Figures 4 (a)-(f) illustrate how we prepare
the synthetic data. First, we use LeapMotion [26] to capture
the 6-DoF skeletal joints of hand in physical space (a) and
deform a rigged 3D hand model to ﬁt these skeletal joints (b).
Then, we render the virtual object around the 3D hand model
and display them together (c). By adjusting the hand pose to
appear to grab the virtual object, we can obtain a plausible
hand-object occlusion in the virtual space (d). We can then
locate the foreground pixels of hand and virtual object (e) and
automatically generate the occlusion mask (f). By rendering
the results in varying camera views and repeating the process
for different target objects, we can efﬁciently produce a large
volume of synthetic data with occlusion masks.

Real dataset. Next, Figures 5 (a)-(f) illustrate the procedure
to prepare the real data. First, we use a webcam to capture
the hand (a) and a display to continuously show it in the
virtual space (b). We then render the target virtual object with
transparency over the hand (c) to allow us to interactively
adjust the hand pose until it appears to grab the object (d).
So, we can obtain a real image of the hand and a compatible
rendered image of the virtual object in the same view, and
label the visible portion of hand as the occlusion mask (e).
Further, to efﬁciently capture more data of different poses and
views, we attach an ArUco marker [18] below the hand to
track its pose in the camera view. Once the hand appears to
grab the virtual object, we use the tracked pose information to
move the virtual object with hand. Therefore, we can capture
more images of hand and object in different views (f).

We employed ten virtual objects of various types when com-
piling the datasets; see Figure 6. Note the different object
grabbing poses, and some objects can have multiple (common)
grabbing poses, e.g., see the ping-pong paddle in Figure 6.
Altogether, we collected 24,539 image tuples in the synthetic
dataset and 1,232 image tuples in the real dataset. We will
release the two datasets upon the publication of this work.

METHOD
In this section, we present the architecture of our deep neural
network and the implementation details and network losses
we designed for improving the performance.

Neural Network Architecture
Figures 7 (a)-(e) show the whole neural network architecture.
We ﬁrst extract the hand portion (b) in the input image (a)
using an encoder-decoder network, and design another deep

Figure 7. (a) Input real image; (b) hand extracted from real image; (c) virtual object; (d) our deep neural network architecture for occlusion prediction,
in which Conv, GN, GC, and DE denote the convolutional operation, group normalization, global context module, and detail enhancement module,
respectively; and (e) ground-truth occlusion mask from the prepared dataset for supervising the network training. Note that we employ the cross-
entropy losses Lce as the supervisions for the intermediate layers in decoder, and the progressively-focusing cross-entropy loss L f and local smoothness
loss Ls to supervise the ﬁnal network output. (f) The structure of the detail enhancement (DE) module.

neural network architecture (d) to predict the occlusion mask.
The network (d) has a ﬁve-block encoder and a ﬁve-block
decoder to extract convolutional features, where each block
has a convolutional operation (Conv), a group normalization
(GN), and a leaky ReLU operation. Also, we introduce the
global context modules (GC) in the ﬁrst three blocks of the
decoder to aggregate global information, and design the detail
enhancement (DE) modules to harvest detail information in
low-level features with skip connections. Further, we leverage
another convolutional operation and softmax to generate the
network output (occlusion mask), and use deep supervision to
compute the loss and predict an occlusion mask per layer in
the decoder. In the end, we take the occlusion mask predicted
from the last layer as the ﬁnal network output.

Global context module. Convolution operations tend to miss
the global context, since they focus on local regions [48].
However, to better infer the hand-object occlusions, we should
treat the whole hand gesture as the global context and ignore
the background. Thus, we remove the background from the
input hand image (see Figure 7 (b)) using a simple encoder-
decoder network, so the occlusion prediction network can
focus on the hand (foreground). Further, we adopt the fast
global context module (GC) in [7] and compute the correlation
among pixels in the feature map. Please see [7] for details.

Detail enhancement module. Low-level features in deep neu-
ral networks contain rich ﬁne details. Hence, to enhance the
boundaries between hand and virtual objects, we formulate the
detail enhancement (DE) module to harvest detail features Fl at
shallow layers of the network when the prediction conﬁdence
is low. As shown in Figure 7(e), we extract the prediction
conﬁdence M at deep layers Fh via a convolutional layer and
softmax, and obtain the enhanced low-level feature F (cid:48)

l by

F (cid:48)
l = (2 − Up(M) × 1.5) × Fl ,
where Up denotes the upsampling operation, which enlarges
mask M to the size of the feature maps at the corresponding
shallow layer (see Figure 7(e)), and (2 − Up(M) × 1.5) aims
to rescale the range of pixel value from 0.33 ∼ 1 (three classes
in total) to 1.5 ∼ 0.5, which are then taken as weight to indi-
cate the importance of the detailed features at shallow layers.

(1)

Figure 8. User grabs a virtual pistol (a) and the ground-truth occlusion
relation is shown in (b). Compared to our network without DE (c), our
network with DE (d) better preserves the details around the ﬁngers.

Lastly, we enlarge the high-level feature map Fh at deep layer,
add it to the reﬁned low-level feature F (cid:48)
l , and merge them
through a 3 × 3 convolutional operation. Note that our detail
enhancement module introduces only the parameters of one
convolutional operation, so the computational time is negligi-
ble. Figure 8 shows the prediction results of the network with
and without DE modules, where our method with DE better
preserves the detailed structures in the prediction mask.

Loss Functions
Progressively-focusing cross-entropy loss. To predict the
hand-object occlusion, the key is to look at the overlap region
between the virtual object and real hand. Hence, we ﬁrst
formulate the focusing cross-entropy loss L f to emphasize
more on the overlap regions in the network training:

L f = −

H×W
∑
i

(αΩi + β ) × yi log(pi) ,

(2)

where Ω is a mask that indicates the overlap region between
the virtual object and real hand; Ωi equals one, if pixel i is
in the overlap region, and it equals zero, otherwise; α and β
are for softening the hard-coded mask Ω; H ×W is the size of
the target domain; and yi and pi denote the ground truth and
prediction result of pixel i, respectively.

However, the overlap region is usually a small part of the
whole image. If the loss focuses too much on it, the network is
hard to converge, due to the lack of global information. Hence,
we formulate a learning scheme to adaptively adjust the weight
over time. The ﬁnal progressively-focusing cross-entropy loss

Figure 9. Visual comparison of using (c) progressively-focusing cross-
entropy loss L( j)
in overall loss L, replacing L( j)
in L by (d) the focusing
p
p
cross-entropy loss L f . or (e) the standard cross-entropy loss Lce.

L( j)
p at the j-th training iteration is deﬁned as

L( j)
p = −

H×W
∑
i

(cid:2) (

j
N

)τ [αΩi − β ] + 1.0 (cid:3) × yi log(pi) ,

(3)

where N is the total number of training iterations; j is the
current iteration ( j ∈ {1, ..., N}), so j/N increases from zero
to one following a polynomial function; and parameters α,
β , and τ are empirically set as 0.4, 0.2, and 0.8, respectively.
Hence, the network can gradually pay more attention to the
regions of interest (Ωi), yet having a global perception of
the whole image. Figure 9 shows the visual comparisons of
using different loss terms, where the proposed progressively-
focusing cross-entropy loss achieves the best performance.

Local smoothness loss. We present a local smoothness loss
to improve the smoothness of the boundaries in the occlusion
predictions by minimizing the gradient orientation between
the prediction and ground truth. First, we apply a soft-argmax
function [9] on the network prediction p, and obtain a label
map b, such that bi indicates the class of pixel i. Here, we
have three classes: background, object, and hand.

bi =

3
∑
k=1

eγ pi,k
eγ pi,1 + eγ pi,2 + eγ pi,3

k ,

(4)

where pi,k is the raw network output (before softmax) that
indicates the probability of pixel i being in class k; and γ is
a parameter (which is set as 100 in practice) to enlarge the
weight of the larger values. Next, we obtain the gradient
orientations (cid:126)o at label bi and at ground truth yi by 3 × 3 Sobel
operators, and compute the mean squared loss in the angular
domain as the local smoothness loss:

Ls =

H×W
∑
i

|| cos−1(cid:104)(cid:126)o(bi) , (cid:126)o(yi)(cid:105)||2 .

(5)

Overall loss function. Finally, we combine the cross-entropy
loss Lce deﬁned on the prediction results produced from the
four network layers (see Lce’s in Figure 7), the progressively-
focusing cross-entropy loss L( j)
p , and the local smoothness loss
Ls, to formulate the overall loss function

L =

4
∑
s=1

α1Ls

ce + α2L( j)

p + α3Ls ,

(6)

where we empirically set parameters α1, α2, and α3 as 0.2,
0.2, and 1

30 , respectively.

Figure 10. Our prototyping AR System.

Training and Testing Strategies
Training parameters. First, we initialize the network param-
eters by random noise, which follows a zero-mean Gaussian
distribution with a standard deviation of 0.1. Then, we train
the network by using 1 : 8 as the ratio of real to synthetic data
in each mini-batch. we empirically set the batch size as 16
and optimize the network by a stochastic gradient descent op-
timizer with a momentum value of 0.9 and a weight decay of
5 × 10−4. The learning rate is adjusted by a polynomial strat-
egy with a decay rate of 0.9. We terminate the whole training
process after 50 epochs, and employ horizontal ﬂip, random
rotation, random sharpness, random brightness, and random
contrast as the data augmentation. Finally, we adopt the above
strategies to ﬁrst train the encoder-decoder network for real
hand segmentation, then adopt the same strategies to jointly
train this segmentation sub-network and the whole network
for occlusion prediction. A median ﬁlter and a morphological
close ﬁlter are then applied to the network output.

Discussion. To meet the real-time processing requirement,
the network model has to be simple and lightweight. In the
course of this work, we follow the design principle of “fewer
channels with more layers” [22] to build the network, which
consumes less processing time and produces higher accuracy.
On the other hand, in the early development of this work, we
trained a network model individually for each virtual object
in the dataset. The results were, however, not satisfactory.
Interestingly, we later trained the network using the whole
dataset of all virtual objects together. The results improved,
since more training data and object categorizes enhance the
generalization capability of the deep neural network.

PROTOTYPING AR SYSTEM AND APPLICATIONS
With plausible occlusions between the virtual object and real
hand, the virtual object can become grabbable and movable, as
well as functional. We develop a prototyping AR system to ex-
plore the design space of functional virtual objects. Figure 10
shows the prototyping AR system with (i) a webcam that live
captures the hand in front of the camera; (ii) an ArUco marker
attached below the hand for tracking; and (iii) a screen that
shows the composed AR view. To track user’s hand, initially
we utilized several RGB-based 3D hand pose estimation net-
works [19, 51]. However, these methods did not ﬁt our task

for several reasons, including not being able to meet the real-
time requirement and weak robustness for smooth interactions.
Thus, we use an ArUco marker in our prototyping system to
track the hand pose in front of the camera. We will explore
other methods for hand pose tracking in the future.

In detail, we render the virtual object, as if it is at a distance
of ∼40cm from the camera. When the user’s hand enters the
camera view, our system will start to use the neural network
to determine the occlusion mask and take it to compose the
hand and virtual object in the view. Then, when the user feels
that the object has been grabbed, he can use the other hand
(the non-dominant one) to press a button to tell the system that
the object has been grabbed. We leave it as a future work to
automatically tell the moment at which the real hand grabs
the virtual object in the AR view. After the grab, the pose
of the virtual object will be locked with the hand, so we can
use bare hand to directly move it and further perform various
interesting interaction scenarios, as presented below:

Scenario #1: Virtual but clickable lightsaber. The ﬁrst sce-
nario features the virtual lightsaber shown in Figure 2 (a). It
is a virtual 3D object in the AR view with a clickable button.
After one grabs the object with bare hand and presses the (red)
button on it, the light blade can show up over the lightsaber.
This is an unseen virtual object, not in the training data.

Scenario #2: Virtual but zoom-able loupe. Figure 2 (b)
shows a virtual magnifying glass (loupe) that allows us to
magnify the background in the glass. When one grabs this vir-
tual loupe, he/she can switch between normal and magnifying
modes by touching the glass surface with his/her thumb.

Scenario #3: Virtual but shoot-able camera. Figure 2 (c)
shows our virtual camera, with which one can grab, aim at a
target, and press the shuttle to take pictures; here, a ﬂash can
also be triggered. Besides, we can use this camera to reveal
hidden contents on screen; see again Figure 2 (c), e.g., the
computer screen is grey but colors are shown on the camera.

Scenario #4: Virtual but scrollable phone. The fourth sce-
nario is a virtual phone in AR; see Figure 2 (d). After one
grabs the phone in AR, our system can present the hand (real)
and phone (virtual) with natural partial occlusions. Further,
when one uses his/her thumb to scroll up/down, the virtual
phone can respond to the action like a real one on our hand.
We develop a simple UI on the virtual phone, with which we
can fade between real and virtual backgrounds using the red
slider and change the camera ﬁlters using the blue slider. With
see-through AR glasses in the future, this interaction scenario
suggests that one may manipulate a smartphone in the AR
view, even without physically holding a real phone.

Scenario #5: Virtual but manipulatable walkie-talkie. The
ﬁfth scenario features a virtual walkie-talkie (see Figure 2 (e)),
with which one can press the red button with thumb to turn
it on and scroll on its side with his/her index ﬁnger to switch
channels. This virtual object is not in our training data.

Scenario #6: Virtual but rotatable knob. The sixth scenario
features a virtual but grabbable and rotatable knob (see Fig-
ure 2 (f)). After one grabs it in the AR view, he/she can rotate
it to the left or right. Also, one may release the knob, grab it

Table 1. Time performance for each computation stage.

Stages
(i) Inputs (capture hands & render)
(ii) Network for real-hand extraction
(iii) Network for occlusion prediction
(iv) Image composition
Total

Time
10ms
7ms
8ms
6ms
31ms

again, and make a larger turn until he/she opens the safe. This
virtual object is also not in our training data.

The above scenarios show that empowered by our prototyping
system, we can design AR objects that are not only grabbable
but also functional (clickable, scrollable, etc.) and greatly im-
prove the user experience when interacting with virtual objects
in AR, beyond what the existing AR applications can offer.
Please refer to the supplementary video for demonstrations.

EVALUATIONS
This section presents a series of experiments to evaluate
GrabAR, including time performance, qualitative and quanti-
tative comparison with other approaches, and two user studies
to explore the user interaction experience with occlusions.

Time Performance
GrabAR is built on a desktop computer with a 3.7GHz CPU
and a single NVidia Titan Xp GPU. Also, it makes use of
the Logitech C920 camera to capture RGB images of the
real world. The system runs at 32.26 frames per second for
320 × 320 images, including the image capture. Table 1 shows
the detailed time performance for each computation stage.

Visual Comparison on Seen and Unseen Objects
Existing methods [3, 15] mostly adopt depth sensors to esti-
mate the occlusion mask between the real hand and virtual
objects. Here, we use the Intel RealSense D435 depth camera
to set up an AR system, in which we render the virtual objects
over the real RGB background from a color camera, calibrate
the color camera with the virtual camera, then use the depth
information of the virtual objects (directly from rendering
the object) and real hand (solely from the depth sensor) to
determine their occlusion relationship.

Figure 11 shows some of the visual comparison results. The
hand models from Ge et al. [19] (see column (b)) do not align
precisely with the hand, the “naïve overlay” approach (see
column (c)) fails to give the perception that the real hand and
virtual objects co-exist in the same space, whereas the “depth
sensor” approach (see column (d)) introduces artifacts on the
hand-object boundaries. Note, for better comparison, we ap-
plied the same post-processing step (see the “Method” section)
to the results produced by the “depth sensor” approach (see
column (e)), since the raw results contain heavy noise. In con-
trast, our method is able to generate more realistic composition
of hand and virtual objects (see column (f)). Further, note that
the result in Figure 11 (bottom) is on an unseen virtual object,
which is not in our synthetic and real datasets; yet, our method
can still produce plausible occlusions. This result demon-
strates the generalization capability of our method (see also
the experiment in next subsection). Since we trained GrabAR-
Net using all the virtual objects in the training data together, it
should be able to better learn the hand grabbing gestures rather

Figure 11. Visual comparisons between different methods on typical seen (above) and unseen (below) test data.

than simply remembering the virtual objects. Furthermore,
our system is ﬂexible to set up, comparing with those that rely
on depth sensors. Note that more visual comparison results
can be found in the supplementary material.

Table 2. Quantitative comparison between different methods (DS: depth-
sensor-based approach; DSP: depth-sensor-based approach with post-
processing; and GN: GrabAR-Net) in terms of the ODSC metric. Un-
seen virtual objects are marked with “*” and virtual objects with differ-
ent grabbing poses are marked with “◦”.

Quantitative Comparison on Seen and Unseen Objects
Evaluation metric. Since the challenges of handling occlu-
sions in AR happen near the hand-object boundaries, we thus
design a new evaluation metric, namely ODSC, speciﬁcally to
look at the overlap regions between hand and virtual object,
while ignoring remaining regions in the image space. To do so,
we ﬁrst extract overlap region (Ω) between the real hand and
virtual object, then compute the dice score coefﬁcient (DSC)
between the predicted occlusion mask (P) and ground truth
image (G) within Ω. In detail, we deﬁne ODSC as

ODSC =

2|P ∩ G|Ω
|P|Ω + |G|Ω

,

(7)

where subscript Ω counts only pixels in Ω = H ∩ O, while H
and O denote the hand and virtual object regions, respectively.

Evaluation results. We recruited ten volunteers to help us
annotate 180 pairs of real hand and virtual object images as
the ground-truth. This additional evaluation dataset covers
ten different virtual objects: two have two different grabbing
poses and three are unseen. Then, we quantitatively com-
pare the results produced by GrabAR-Net (GN) with results
produced from the depth-sensor-based approach (DS), depth-
sensor-based approach with post-processing (DSP) and Ge et
al. [19]. Table 2 reports the ODSC values for different seen
and unseen virtual objects, showing that GrabAR-Net consis-
tently produces far better results than the other approaches by
a large margin. We also conducted an ablation study to evalu-
ate the effectiveness of the major components in our method.
Please refer to the supplementary material for more details.

Study 1: The Effect of Occlusion
First, we explore how occlusions affect user interaction expe-
rience. We compare GrabAR-Net (GN) with (i) rendering the

Object
can◦
cup
ﬂashlight
gun
loupe
paddle◦
phone
knob*
lightsaber*
pistol*
average

DS
63.35%
84.74%
76.09%
80.78%
91.13%
84.78%
85.16%
76.89%
84.84%
93.91%
81.57%

DSP
59.64%
63.24%
72.72%
79.57%
80.61%
80.68%
84.48%
75.72%
83.23%
89.51%
75.81%

Ge et al. [19] GN (our)
90.44%
87.14%
95.30%
96.42%
94.41%
95.02%
95.78%
81.79%
94.46%
96.73%
92.76%

73.56%
83.17%
84.97%
84.67%
86.25%
70.56%
86.61%
66.64%
78.98%
84.90%
78.69%

Figure 12. Different ways of handling the occlusion between virtual ob-
ject and real hand: (a) our GrabAR-Net (GN); (b) render virtual object
above user’s hand (OA); (c) make the hand-object overlap region semi-
transparent (ST); and (d) render virtual object behind user’s hand (OB).

virtual object above user’s hand (OA) and (ii) making the hand-
object overlap region semi-transparent (ST); see Figure 12.
Before this study, we recruited three participants in a pilot
study, in which we considered also rendering the virtual object
behind user’s hand (OB). All participants reported that their
hands occluded most of the virtual object in OB and hindered
their interactions, so we ignored OB in Study 1.

Participants. We recruited nine participants in campus: four
females and ﬁve males, aged between 22 and 28. Among them,
eight had experience with AR, and all are right-handed.

Tasks. Task 1 is on rotating the virtual knob shown in Figure 2.
In each trial, the participant presses a button to initiate the trial,

then adjusts his/her hand to grab the virtual knob. When the
participant feels like successfully grabbing the knob in the AR
view, he/she can press another button and start rotating the
knob for +90, −90, and +70 degrees to open the safe. We
record the time taken by each participant to adjust hand for
grabbing and also the time to complete the task.

Task 2 is on operating the virtual walkie-talkie shown in Fig-
ure 2. In each trial, the participant opens the walkie-talkie
and uses his/her index ﬁnger to freely switch channels with
no time limit. To open the walkie-talkie, the participant can
use his/her thumb to touch the red button via ST/GN, or press
a button on keyboard via OA. Like (i), we record the object
grabbing time. Note that, in both tasks, we counterbalance the
order of the compared methods across the participants.

Procedure. When a participant came to our lab, we ﬁrst
introduced the system to him/her and started a ten-minute
tutorial for him/her to try out these tasks for all methods.
Then, the participant performed the tasks. After that, each
participant was asked to ﬁll a questionnaire with two Likert-
scale questions (1: disagree and 5: agree) on GN, ST, and
OA: (Q1) the AR object is plausible and realistic (“AR object”
for short); and (Q2) the interaction experience is intuitive and
immersive (“Interaction” for short). Lastly, we interviewed
the participant to obtain his/her comments.

Quantitative comparison results. Figure 13 summarizes
the results, in which we further evaluate the signiﬁcance using
paired t-tests on the ﬁrst two time-related objective quantities
and the Wilcoxon signed rank tests on the user ratings:

(i) Time to grab (sec.). Time to grab the virtual objects under
GN (M=3.766, SD=0.455) is signiﬁcantly shorter than those
of ST (M=5.419, SD=0.398, t-test: t(8)=6.337, p=0.0002)
and OA (M=6.514, SD=0.641, t-test: t(8)=8.059, p<0.0001),
where M and SD are mean and standard deviation, respectively.

(ii) Time to complete Task 1 (sec.). Completion time under
OA (M=57.52, SD=4.776) is signiﬁcantly longer than those
under GN (M=47.08, SD=4.787, t-test: t(8)=5.107, p=0.0009)
and ST (M=49.87, SD=6.049, t-test: t(8)=3.690, p=0.0061),
whereas GN and ST have no signiﬁcant differences.

(iii) AR object (Q1).
User ratings under GN (M=4.8,
SD=0.416) are signiﬁcantly higher than those under OA
(M=1.2, SD=0.416, Z=-2.739, p=0.006) and ST (M=2.4,
SD=0.685, Z=-2.716, p=0.007). The result implies that the
participants found GN producing better AR composition.

(iv) Interaction (Q2).
User ratings under GN (M=4.6,
SD=0.497) are signiﬁcantly higher than those under OA
(M=1.2, SD=0.416, Z=-2.724, p=0.006) and ST (M=3.0,
SD=0.667, Z=-2.714, p=0.007). The result implies that partic-
ipants preferred the interaction experience under GN.

Participant comments. Almost all participants stated that
OA is not ideal. Speaking of ST, P3 said that “ST made her
feel that she was moving ﬁngers behind the virtual object. P7
explained why it costed him more time to grab the virtual
object under OA and ST: “Without occlusion, I was not sure if
my grabbing pose was correct, so I hesitated for a while before
grabbing.” The participants highly praised GN: Plausible

Figure 13. Study 1 results. Comparing our GrabAR-Net (GN) with
rendering virtual object above user’s hand (OA) and making the hand-
object overlap region semi-transparent (ST). Note that the boxes show
the mean values (M) and error bars show the standard deviations (SD),
whereas the user ratings range 1 (worst) to 5 (best).

occlusion “made me feel that I was actually grabbing the
virtual object” (P3) and “brought fancy AR scenes” (P1 & P4).

Study 2: The Quality of Occlusion
Next, we compare GN with depth-sensor-based methods on
the quality of resolving hand-object occlusion. Since Ge et
al. [19] is not real-time (10.75 FPS), we excluded it in Study
2. Also, in a pilot study, we found that the participants always
preferred the depth-sensor-based method with post-processing
(DSP), which has less noise, than the vanilla depth-sensor-
based method, so we only compared GN with DSP here.

Participants. We recruited ten participants in campus: four
females and six males, aged between 22 and 28. Among them,
eight had experience with AR, and all are right-handed.

Tasks. Task 1 is on grabbing virtual objects. Each participant
was asked to grab six virtual objects of various kinds and
poses (paddle with handshake and penhold grips, can, pistol,
ﬂashlight, and lightsaber). The virtual objects were rendered
using GN or DSP in different trials (two trials per permutation).
Overall, we have 240 trials: ten participants × six objects ×
two modes (GN/DSP) × two blocks of trials. In each trial, the
participant presses a button on the keyboard to initiate a trial.
Then, the system renders the next virtual object in the AR view
using GN or DSP for the participant to stretch hand and make
a hand grabbing pose to fetch the object. Immediately after the
participant feels that the object has been grabbed, he/she can
use the other hand to press a button. Our system then records
the time taken by the participant in the trial.

Task 2 is on scrolling on the virtual phone with the same set
up as Task 1, but focuses on letting the participants try and
experience hand interactions with AR objects under GN and
DSP. With our system, the participant can grab the virtual
phone, scroll on it like a real one, and look at different parts
of the image on the phone screen; see Figure 2. Further, the
participant can ﬁnd a button and click on it. In Task 2, we do
not impose any time limit but require them to interact with
GN and DSP for roughly the same amount of time. Also, we
count the number of times that their hand penetrates the virtual
objects for GN and for DSP. In both tasks, we counterbalance
the order of GN and DSP across participants.

Procedure. When a participant came to our lab, we ﬁrst in-
troduced the AR system and gave a tutorial session to him/her
on how to grab a virtual object with naïve overlay, GN, and
DSP. Particularly, when we showed the naïve overlay mode

Figure 14. Study 2 results. Comparing our GrabAR-Net (GN) with the
depth-sensor-based method with post-processing (DSP). Note that the
boxes show the mean values (M), error bars show the standard devia-
tions (SD), and user ratings range 1 (worst) to 5 (best).

to the participants, we asked them to point out the occlusion
issue to ensure that they understood the occlusion relationship
between the real hand and virtual objects. After that, each
participant performed Tasks 1 and 2 as presented above.

Then, each participant was asked to ﬁll a questionnaire with
three Likert-scale questions (1: disagree and 5: agree) on GN
and DSP: (Q1) the presented occlusion is handled correctly
(“Correctness” for short); (Q2) the image quality is good, i.e.,
noise-free without ﬂickering (“Image Quality” for short); and
(Q3) the hand-object interaction is plausible and immersive
(“Interaction” for short). Lastly, we further interviewed the
participant to obtain their comments.

Quantitative comparison results. Figures 14 summarizes
the results. Again, we evaluate the signiﬁcance using paired
t-tests on the ﬁrst two time-related objective quantities and the
Wilcoxon signed rank tests on the user ratings.

(i) Time to grab the object (in sec.)
in Task 1 under GN
is (M=4.135, SD=0.894), which is signiﬁcantly shorter than
those of DSP (M=6.820, SD=1.314, t-test: t(9)=13, p<0.0001).

(ii) Penetration count under GN is (M = 0.28, SD = 0.270),
which is signiﬁcantly lower than those of DSP (M=2.12,
SD=0.953, t-test: t(9)=6.241, p=0.0002). The result shows
that GN can better avoid object-hand penetration in the AR
view, i.e., less interference during the hand-object interactions.

(iii) Correctness (Q1) ratings under GN (M=4.3, SD=0.483)
are signiﬁcantly higher than those under DSP (M=1.9,
SD=0.567, Z=-2.848, p=0.004), implying that the participants
found the occlusions under GN to be more plausible.

(iv) Image Quality (Q2) ratings under GN (M=4.6, SD=0.516)
are signiﬁcantly higher than those under DSP (M=1.7,
SD=0.8233, Z=-2.836, p=0.005). The result implies that GN
produced better image quality as observed by the participants.

(v) Interaction (Q3) ratings under GN (M=3.9, SD=0.568) are
signiﬁcantly higher than those under DSP (M=2.2, SD=0.633
Z=-2.701, p=0.007). The result implies that participants had
better interaction experience with GN.

Participant comments.
In Task 1, all participants found
naïve overlay not ideal for supporting virtual object grabbing
in AR and preferred GN, because GN offers sharper and more
accurate results at edges, while the results of DSP have more
noise and ﬂickering. As for Task 2, we observed that the
participants often needed more time to adjust their hand poses
to grab the virtual phone under DSP. P2 gave a representative
comment: “It was hard for me to sense the distance between

the camera and virtual object, so I often reached out my hand to
the front or to the back of the virtual object instead of directly
grabbing it between my ﬁngers under DSP. But this situation
happened less under GN.” When scrolling on the virtual phone,
seven participants pointed out that object-hand penetrations
happened too often under DSP, due to unstable hand motions.
It is “caused by the unintentional scrolling operations on the
virtual phone (P1 & P3).” and “interfered me and made me
feel unrealistic (P6).” Object-hand penetration also happens
under GN, due to mis-predictions by GrabAR-Net, but “it
occurred at an acceptable frequency (P9).”

However, GN is not perfect all the time. Two participants
reported that attaching the ArUco marker to their hands was
bothering, which is a limitation of our existing prototyping
system. Three participants mentioned that although the system
was occlusion-aware, they still felt unrealistic due to the lack
of tactile feedback, which is a common drawback in AR inter-
actions. However, P5 said that “I preferred this comparing to
carrying an extra physical device for tactile feedback.”

DISCUSSION AND FUTURE WORK
This paper presents GrabAR, a new approach that makes use
of an embedded neural network to directly predict real-and-
virtual occlusion in AR views, while bypassing inaccurate
depth estimation and inference. Using our approach, we can
enable users to grab and manipulate various virtual objects
with accurate occlusion, and increase the sense of immersion
for hand interactions with virtual objects. Our key idea is to
learn to determine the occlusion from hand grabbing poses
that describe natural virtual object grabs with user’s hand in
physical space. Particularly, we design a GrabAR-Net to pre-
dict the occlusion mask of the virtual object and real hand by
formulating the global context module and detail enhancement
module to aggregate the global and local information simul-
taneously and presenting two novel loss functions to provide
better supervision for the network training. Further, we com-
pile synthetic and real datasets and use them to train the neural
network to leverage the automatically-generated labels in syn-
thetic data and reduce the burden of manually labeling the
real data. A prototyping AR system is presented with various
interaction scenarios, showing that virtual objects in AR can
be grabbable, as well as interactable. We compared GrabAR
with other methods through various experiments, and show
that GrabAR provides signiﬁcantly more plausible results than
other existing methods, both visually and quantitatively.

As the ﬁrst work, we are aware of, that explores the deep
neural network to compose virtual objects with real hands,
there are still several problems that we plan to work on in the
future. (i) Single hand interactions. Currently, our system
supports interaction using a single hand only. We plan to
extend it to allow bimanual interactions by including more
training samples. (ii) ArUco marker. Our system relies on
an ArUco marker to detect the six-DoF poses of user’s hand,
which is cumbersome to set up and limits the motion range
of user’s hand. In the future, we will explore the use of only
a single RGB image by training a deep neural network to
learn the six-DoF hand poses automatically. (iii) Capturing
user’s intention. Without depth reference, it is hard to capture

user’s intention of grabbing or releasing the virtual object. In
our prototype system we require users to press a button to
indicate the moment of grabbing and releasing. It somehow
breaks the sense of immersion.
In the future, we plan to
explore methods, e.g., time-series analysis, to simplify the
interactions. Moreover, we consider further improving our
method by taking the temporal information of video into the
network for smoothing the results.

REFERENCES
[1] Barton L Anderson. 2003. The role of occlusion in the

perception of depth, lightness, and opacity.
Psychological Review 110, 4 (2003), 785.

[2] Seungryul Baek, Kwang In Kim, and Tae-Kyun Kim.
2019. Pushing the Envelope for RGB-based Dense 3D
Hand Pose Estimation via Neural Rendering. In CVPR.
1067–1076.

[3] Caterina Battisti, Stefano Messelodi, and Fabio Poiesi.

2018. Seamless Bare-Hand Interaction in Mixed Reality.
In ISMAR. 198–203.

[4] Hrvoje Benko and Steven Feiner. 2007. Balloon
selection: A multi-ﬁnger technique for accurate
low-fatigue 3D selection. In 2007 IEEE Symposium on
3D User Interfaces.

[5] Adnane Boukhayma, Rodrigo de Bem, and Philip HS

Torr. 2019. 3D hand shape and pose from images in the
wild. In CVPR. 10843–10852.

[6] Yujun Cai, Liuhao Ge, Jianfei Cai, and Junsong Yuan.

2018. Weakly-supervised 3D hand pose estimation from
monocular RGB images. In ECCV. 666–682.

[7] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han

Hu. 2019. GCNet: Non-Local Networks Meet
Squeeze-Excitation Networks and Beyond. In ICCV
Workshops.

[8] Thomas P Caudell and David W Mizell. 1992.

Augmented reality: An application of heads-up display
technology to manual manufacturing processes. In
Hawaii Iternational Conference on System Sciences,
Vol. 2. 659–669.

[9] Olivier Chapelle and Mingrui Wu. 2010. Gradient

descent optimization of smoothed information retrieval
metrics. Information Retrieval 13, 3 (2010), 216–235.

[10] Junyeong Choi, Jungsik Park, Hanhoon Park, and

Jong-II Park. 2013. iHand: an interactive
bare-hand-based augmented reality interface on
commercial mobile phones. Optical Engineering 52, 2
(2013), 1–11.

[11] Wendy H Chun and Tobias Höllerer. 2013. Real-time

hand interaction for augmented reality on mobile phones.
In Proceedings of the 2013 international conference on
Intelligent user interfaces. 307–314.

[12] Klaus Dorfmuller-Ulhaas and Dieter Schmalstieg. 2001.

Finger tracking for interaction in augmented
environments. In ISMAR. 55–64.

[13] David Eigen, Christian Puhrsch, and Rob Fergus. 2014.
Depth map prediction from a single image using a
multi-scale deep network. In NIPS. 2366–2374.

[14] Jakob Engel, Thomas Schöps, and Daniel Cremers.

2014. LSD-SLAM: Large-scale direct monocular
SLAM. In ECCV. 834–849.

[15] Qi Feng, Hubert PH Shum, and Shigeo Morishima.

2018. Resolving occlusion for 3D object manipulation
with hands in mixed reality. In VRST. 119.

[16] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan

Batmanghelich, and Dacheng Tao. 2018. Deep ordinal
regression network for monocular depth estimation. In
CVPR. 2002–2011.

[17] Ryo Furukawa, Ryusuke Sagawa, and Hiroshi Kawasaki.

2017. Depth Estimation Using Structured Light
Flow–Analysis of Projected Pattern Flow on an Object’s
Surface. In ICCV. 4640–4648.

[18] Sergio Garrido-Jurado, Rafael Muñoz-Salinas,

Francisco José Madrid-Cuevas, and Manuel Jesús
Marín-Jiménez. 2014. Automatic generation and
detection of highly reliable ﬁducial markers under
occlusion. Pattern Recognition 47, 6 (2014), 2280–2292.

[19] Liuhao Ge, Zhou Ren, Yuncheng Li, Zehao Xue,

Yingying Wang, Jianfei Cai, and Junsong Yuan. 2019.
3D Hand Shape and Pose Estimation from a Single RGB
Image. In CVPR. 10833–10842.

[20] Derek Hoiem, Andrew N Stein, Alexei A Efros, and

Martial Hebert. 2007. Recovering occlusion boundaries
from a single image. In ICCV. 1–8.

[21] Aleksander Holynski and Johannes Kopf. 2018. Fast
Depth Densiﬁcation for Occlusion-aware Augmented
Reality. ACM Transactions on Graphics (SIGGRAPH
Asia) 37, 6 (Dec. 2018), 194:1–194:11.

[22] Kye-Hyeon Kim, Sanghoon Hong, Byungseok Roh,

Yeongjae Cheon, and Minje Park. 2016. PVAnet: Deep
but lightweight neural networks for real-time object
detection. arXiv preprint arXiv:1608.08021 (2016).

[23] Myron W Krueger, Thomas Gionfriddo, and Katrin

Hinrichsen. 1985. VIDEOPLACE—an artiﬁcial reality.
In ACM SIGCHI Bulletin, Vol. 16. 35–40.

[24] Lubor Ladicky, Jianbo Shi, and Marc Pollefeys. 2014.
Pulling things out of perspective. In CVPR. 89–96.

[25] Iro Laina, Christian Rupprecht, Vasileios Belagiannis,

Federico Tombari, and Nassir Navab. 2016. Deeper
depth prediction with fully convolutional residual
networks. In 3DV. 239–248.

[26] LeapMotion. [Online; accessed on 13-August-2019].

https://www.leapmotion.com/. ([Online; accessed on
13-August-2019]).

[27] Jameel Malik, Ahmed Elhayek, Fabrizio Nunnari, Kiran
Varanasi, Kiarash Tamaddon, Alexis Heloir, and Didier
Stricker. 2018. DeepHPS: End-to-end estimation of 3D
hand pose and shape by learning from synthetic depth.
In 3DV. 110–119.

[42] Jie Song, Fabrizio Pece, Gábor Sörös, Marion Koelle,
and Otmar Hilliges. 2015. Joint estimation of 3D hand
position and gestures from monocular video for mobile
interaction. In CHI. 3657–3660.

[43] Ching Teo, Cornelia Fermuller, and Yiannis Aloimonos.

2015. Fast 2D border ownership assignment. In ICCV.
5117–5125.

[44] Julien Valentin, Adarsh Kowdle, Jonathan T. Barron,

Neal Wadhwa, Max Dzitsiuk, Michael John Schoenberg,
Vivek Verma, Ambrus Csaszar, Eric Lee Turner, Ivan
Dryanovski, Joao Afonso, Jose Pascoal, Konstantine
Nicholas John Tsotsos, Mira Angela Leung, Mirko
Schmidt, Onur Gonen Guleryuz, Sameh Khamis,
Vladimir Tankovich, Sean Fanello, Shahram Izadi, and
Christoph Rhemann. 2018. Depth from motion for
smartphone AR. ACM Transactions on Graphics
(SIGGRAPH Asia) 37, 6 (2018), 1–19.

[45] Guoxia Wang, Xiaochuan Wang, Frederick WB Li, and
Xiaohui Liang. 2018b. DOOBNet: Deep Object
Occlusion Boundary Detection from an Image. In ACCV.
686–702.

[46] Peng Wang and Alan Yuille. 2016. DOC: Deep

occlusion estimation from a single image. In ECCV.
545–561.

[47] Robert Y Wang and Jovan Popovi´c. 2009. Real-time

hand-tracking with a color glove. ACM Transactions on
Graphics (SIGGRAPH) 28, 3 (2009), 63.

[48] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and

Kaiming He. 2018a. Non-local neural networks. In
CVPR. 7794–7803.

[49] Qi Ye and Tae-Kyun Kim. 2018. Occlusion-aware hand
pose estimation using hierarchical mixture density
network. In ECCV. 801–817.

[50] Shanxin Yuan, Guillermo Garcia-Hernando, Björn

Stenger, Gyeongsik Moon, Ju Yong Chang, Kyoung
Mu Lee, Pavlo Molchanov, Jan Kautz, Sina Honari,
Liuhao Ge, and others. 2018. Depth-based 3D hand pose
estimation: From current achievements to future goals.
In CVPR. 2636–2645.

[51] Christian Zimmermann and Thomas Brox. 2017.

Learning to estimate 3D hand pose from single RGB
images. In ICCV. 4903–4911.

[52] Christian Zimmermann, Duygu Ceylan, Jimei Yang,
Bryan Russell, Max Argus, and Thomas Brox. 2019.
FreiHAND: A Dataset for Markerless Capture of Hand
Pose and Shape From Single RGB Images. In ICCV.

[28] Nicolai Marquardt, Ricardo Jota, Saul Greenberg, and
Joaquim A Jorge. 2011. The continuous interaction
space: interaction techniques unifying touch and gesture
on and above a digital surface. In IFIP Conference on
Human-Computer Interaction. 461–476.

[29] Kenneth R Moser, Sujan Anreddy, and J Edward Swan.
2016. Calibration and interaction in optical see-through
augmented reality using leap motion. In IEEE VR.
332–332.

[30] Franziska Mueller, Florian Bernard, Oleksandr

Sotnychenko, Dushyant Mehta, Srinath Sridhar, Dan
Casas, and Christian Theobalt. 2018. Ganerated hands
for real-time 3D hand tracking from monocular RGB. In
CVPR. 49–59.

[31] Franziska Mueller, Dushyant Mehta, Oleksandr

Sotnychenko, Srinath Sridhar, Dan Casas, and Christian
Theobalt. 2017. Real-time hand tracking under occlusion
from an egocentric RGB-D sensor. In ICCV. 1284–1293.

[32] Raul Mur-Artal, Jose Maria Martinez Montiel, and
Juan D Tardos. 2015. ORB-SLAM: a versatile and
accurate monocular SLAM system. IEEE Transactions
on Robotics 31, 5 (2015), 1147–1163.

[33] Alejandro Newell, Kaiyu Yang, and Jia Deng. 2016.

Stacked hourglass networks for human pose estimation.
In ECCV. 483–499.

[34] Vassilis C Nicodemou, Iason Oikonomidis, Georgios

Tzimiropoulos, and Antonis Argyros. 2018. Learning to
Infer the Depth Map of a Hand from its Color Image.
arXiv preprint arXiv:1812.02486 (2018).

[35] Hiroshi Ono, Brian J Rogers, Masao Ohmi, and Mika E

Ono. 1988. Dynamic occlusion and motion parallax in
depth perception. Perception 17, 2 (1988), 255–266.

[36] Paschalis Panteleris, Iason Oikonomidis, and Antonis

Argyros. 2018. Using a single RGB frame for real time
3D hand pose estimation in the wild. In WACV.
436–445.

[37] Rafael Radkowski and Christian Stritzke. 2012.

Interactive hand gesture-based assembly for augmented
reality applications. In Proceedings of the 2012
International Conference on Advances in
Computer-Human Interactions. 303–308.

[38] Xiaofeng Ren, Charless C Fowlkes, and Jitendra Malik.

2006. Figure/ground assignment in natural images. In
ECCV. 614–627.

[39] Grégory Rogez, James S Supancic, and Deva Ramanan.
2015. First-person pose recognition using egocentric
workspaces. In CVPR. 4325–4333.

[40] Ashutosh Saxena, Sung H Chung, and Andrew Y Ng.

2006. Learning depth from single monocular images. In
NIPS. 1161–1168.

[41] Ashutosh Saxena, Min Sun, and Andrew Y Ng. 2008.

Make3D: Learning 3D scene structure from a single still
image. IEEE Transactions on Pattern Analysis and
Machine Intelligence 31, 5 (2008), 824–840.

