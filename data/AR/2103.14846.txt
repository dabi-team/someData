AR Mapping: Accurate and EfÔ¨Åcient Mapping for Augmented Reality

Rui Huang1, Chuan Fang, Kejie Qiu, Le Cui, Zilong Dong, Siyu Zhu, Ping Tan

1
2
0
2

r
a

M
7
2

]

V
C
.
s
c
[

1
v
6
4
8
4
1
.
3
0
1
2
:
v
i
X
r
a

Abstract‚Äî Augmented reality (AR) has gained increasingly
attention from both research and industry communities. By
overlaying digital information and content onto the physical
world, AR enables users to experience the world in a more
informative and efÔ¨Åcient manner. As a major building block
for AR systems, localization aims at determining the device‚Äôs
pose from a pre-built ‚Äúmap‚Äù consisting of visual and depth
information in a known environment. While the localization
problem has been widely studied in the literature, the ‚Äúmap‚Äù
for AR systems is rarely discussed. In this paper, we introduce
the AR Map for a speciÔ¨Åc scene to be composed of 1) color
images with 6-DOF poses; 2) dense depth maps for each image
and 3) a complete point cloud map. We then propose an efÔ¨Åcient
end-to-end solution to generating and evaluating AR Maps.
Firstly, for efÔ¨Åcient data capture, a backpack scanning device
is presented with a uniÔ¨Åed calibration pipeline. Secondly, we
propose an AR mapping pipeline which takes the input from
the scanning device and produces accurate AR Maps. Finally,
we present an approach to evaluating the accuracy of AR Maps
with the help of the highly accurate reconstruction result from
a high-end laser scanner. To the best of our knowledge, it is
the Ô¨Årst time to present an end-to-end solution to efÔ¨Åcient and
accurate mapping for AR applications.

I. INTRODUCTION

Augmented reality (AR) has gained increasingly atten-
tion from both research and industry communities. Many
AR applications have emerged in areas of entertainment,
construction and indoor navigation. By overlaying digital
information and content onto the physical world, AR enables
users to experience the world in a more informative and
efÔ¨Åcient manner. According to Bimber et.al, [1], AR systems
are built upon three major blocks including tracking and
registration, display technology and real time rendering. In
order to render the virtual object correctly, the system needs
to constantly determine the pose within the environment of
the user with respect to the virtual object. Those applications
such as Google Map AR [2] and Pokemon Go [3] start
with re-localizing the device‚Äôs absolute pose outdoor with
the help of GPS, and then track the device‚Äôs subsequent
motion continuously using motion tracking systems such as
ARCore [4].

It has been shown that localization is a major building
block for AR systems [1], [5]. Currently, the state-of-the-art
approaches to localization in arbitrary environments can be
divided into two categories: 1) sparse feature matching [6]; 2)
scene coordinate map regression [7]. Both categories needs
a pre-built ‚Äúmap‚Äù including color images and corresponding
sparse or dense depth information. However, while the local-
ization problem has been widely studied in the literature, the

1 All

the authors are with Alibaba Group, Hangzhou, China.

rui.hr@alibaba-inc.com

Fig. 1: The AR Maps of two large scenes: WestCity (1st
row) and EFC Mall (2nd row). From left to right, it shows
the captured color image, corresponding depth map and
reconstructed 3D model from point cloud map.

‚Äúmap‚Äù for AR systems has not been well deÔ¨Åned so that we
propose a framework for building and evaluating AR Maps.
We introduce AR Maps which can be used in any AR
systems. For a speciÔ¨Åc scene, an AR Map is composed of 1)
color images with 6-DOF poses; 2) dense depth map for
each image and 3) a complete point cloud map. An AR
Map is the set of information which enables basic functions
such as localization and occlusion detection in AR systems.
To ensure that the AR Map meets the requirements of AR
applications, the following issues need to be considered.

‚Ä¢ How to capture the visual and depth information of a
scene efÔ¨Åciently so that the AR Map can be updated
frequently with low cost.

‚Ä¢ How a mapping system can be designed to process
the raw data and generate an accurate AR Map. The
accuracy of depth and geometry in local areas is critical
for some tasks such as occlusion detection and realistic
rendering in AR systems.

‚Ä¢ How to evaluate an AR Map to ensure that its accuracy

meets the requirements of AR applications.

To address these issues, we propose an end-to-end solution
to generating and evaluating AR Maps. Firstly, we present
a scanning device in the form of backpack equipped with
2 multi-beam lidars, a panoramic camera with 4 Ô¨Åsheye
lens and an inertial measurement unit (IMU). Lidars are
active sensors which directly measure distance by emitting
laser light to the target. In constrast to RGB cameras, they
are not affected by the illumination condition or texture
richness of the environment. We capture dense depth by
multi-beam lidars and obtain color images by the panoramic
camera. To calibrate a device with multi-model sensors,

 
 
 
 
 
 
it usually requires overlap between all sensors and time-
consuming manual placement of calibration board [8]. We
further propose an uniÔ¨Åed extrinsic calibration approach
which requires only one-time installment and data capture.
The subsequent calibration process is fully automated.

Secondly, we propose a lidar-based AR mapping system to
construct accurate AR Maps. Traditional lidar odometry and
mapping systems [9], [10] focus on motion estimation. The
mapping process is usually done by transforming the point
cloud to a global frame. To ensure efÔ¨Åcient and accurate
mapping, we Ô¨Årstly introduce a lidar odometry module with
some improvements based on the classic LOAM [9] system.
These improvements include an enhanced feature selection
model and Ô¨Åltering strategy to remove outliers. To correct
the accumulated error in odometry for large-scale scenes, we
further propose a submap-based global optimization module
which optimizes the global trajectory using constraints of
loop closing and consistency between neighboring submaps.
This module eliminates the drifting error while maintaining
the local map consistency. Moreover, the Ô¨Ånal point cloud
map is generated from a stable map fusion module. By
enforcing the map points with sufÔ¨Åcient observations, this
module guarantees a clean map even in highly dynamic
environments. Once obtaining the optimized global trajectory
and point cloud map, the camera poses for color images can
be interpolated from the lidar poses and the corresponding
depth maps are rendered from the reconstructed 3D mesh.
The content of resulting AR Maps are demonstrated in Fig. 1.
Finally, we give an approach to evaluating the accuracy
of AR Maps based on the highly accurate point cloud
reconstruction from a high-end laser scanner. The accuracy
of dense depth maps and 6-DOF poses of color images
are also evaluated according to their local consistency. The
experiments show that our system is able to generate accurate
AR Maps efÔ¨Åciently.

II. RELATED WORKS
A. 3D Scanning Devices and UniÔ¨Åed Multi-Sensor Calibra-
tion

3D scanning devices are used to capture data for 3D
reconstruction and mapping. They are different
in terms
of the sensor combination and scanning method. Here we
only review those devices which can generate the content
of an AR Map. Matterport Pro2 3D camera [11] uses a
structured light (infrared) sensor to obtain the 3D point cloud.
At each scan, the 3D camera is mounted at a tripod and
rotates by 360 degrees. It takes 20 seconds for each scan
and the maximum operating distance is 4.5 meters. Another
3D scanner, Leica BLK360 [12], uses a rotating laser scanner
to capture the point cloud map. Utilizing the same rotating
and scanning manner, It takes minutes for each scan and
requires manual editing to register the point cloud from all
scans into a Ô¨Ånal 3D map. Both devices are able to generate
panoramic images by stitching images captured at different
angles during rotation.

Since those 3D scanners can be Ô¨Åxed on a tripod and rotat-
ing is controlled precisely by a motor, they are usually able to

produce high-precision point cloud. However, the capturing
process is time-consuming and thus infeasible for AR Map
generation in large-scale scenes. Instead of capturing at a
Ô¨Åxed spot per scan, mobile scanners such as handheld or
backpack scanners capture the point cloud data with lidars
by moving around in the environment. Leica Pegasus [13] is
a high-cost backpack mobile scanner equipped with 5 color
cameras, 2 VLP-lidars and GNSS. It is designed for outdoor
large scale mapping. Kaarta Contour is a handheld scanner
equipped with a continuously rotating laser scanner and an
onboard HD color camera which has too limited FOV to
ensure the visual coverage in the scene. We therefore design
a backpack scanning system with multi-beam lidars and a
panoramic camera for efÔ¨Åcient data capture of AR maps.

To calibrate a system with multi-modal sensors, Lee et
al. [8] propose an uniÔ¨Åed multi-camera multi-lidar calibra-
tion approach by using only one calibration board. Since
the overlap between at least two sensors are required, the
checkerboard needs to be manually placed at multiple spots.
Jeong et al. [14] present a framework for target-less extrinsic
calibration of stereo cameras and lidars. They make use of
the static road markings as features. However, this approach
is speciÔ¨Åcally designed for system on a car in city environ-
ment, which limits its extension to other general systems.
We therefore propose an uniÔ¨Åed extrinsic calibration method
for multi-camera multi-lidar calibration which needs easy
deployment of calibration environment and only one-time
data capture.

B. Lidar-based Odometry and Mapping

Lidar Odometry generally estimates the motion by com-
puting the relative pose of consecutive scans. Zhang and
Singh proposes the classic lidar odometry and mapping
(LOAM) system [9]. Based on the idea of LOAM, many
works have been proposed to improve the accuracy and
robustness of lidar odometry and mapping. Lego-LOAM [15]
is a light-weight and ground-optimized LOAM system de-
signed for ground vehicle mapping in outdoor. However it
assums that the vehicle is always moving on a plane. A
loosely-coupled laser-visual-inertial odometry and mapping
system [10] adopts a sequential pipeline to estimate motion in
a coarse to Ô¨Åne manner. A visual-inertial motion estimation
system is followed by a scan matching module to further
reÔ¨Åne the pose.

Some other works propose tightly-coupled sensor fusion
for lidar odometry. LIO-SAM [16] solves a factor graph
including lidar odometry factor, IMU preintegration fator,
GPS factor and loop closure factor. The lidar odometry
factor is formed by the relative pose constraint which is
computed from scan-matching. Since there is no scan-to-
map registration, the drifting error will be accumulated fast
and the mapping result relies on the loop closure factor and
the GPS factor in outdoor. LIO-mapping [17] is a tightly-
coupled lidar inertial odometry and mapping framework. The
measurements from lidar and IMU are jointly optimized to
achieve comparable or higher accuracy than LOAM system.
However, the tightly-coupled lidar odometry methods re-

Fig. 2: Our end-to-end solution to generating AR Maps with a backpack scanning system and the AR mapping system.

quires suffcient motion excitation to IMU for bias estimation
and the optimization framework is computationally heavy.

Considering the balance between efÔ¨Åciency and accuracy,
we adopt the idea of loosely-coupled approach as in LOAM
for lidar odometry and mapping. We then apply a few effec-
tive improvements to enhance the robustness and accuracy. In
addition, those works above mainly focus on pose estimation.
Since the map accuracy is crucial for occlusion detection
and realistic rendering in AR systems, we design a global
optimization and stable mapping module to ensure both the
global and local consistency of AR Maps.

III. BACKPACK SCANNING SYSTEM

A. Hardware Design

Our backpack scanning system is designed for AR map-
it consists of two 16-beams
ping. As shown in Fig. 3,
RoboSense lidars, an MTi-3 AHRS IMU and a Teche 360
Anywhere panoramic camera with 4 Ô¨Åsheye lens. All the
sensors are synchronized by a time server. The angle between
the scanning planes of two lidars is about 25 degrees. As
the lidar spins continuously, it rotates 360 degrees within
0.1 seconds and generate 75 data packets. The timestamp
of each packet is also synchronized with the time server.
The time server sends pulse signal in 100 Hz to trigger the
measurement of IMU.

While scanning the environment, the operator carries the
backpack system and a touch pad for controlling. When the
color images need to be captured, the operator stops walking
and presses a ‚Äúcapture‚Äù button on the pad to take 4 images
from lens of the panoramic camera. The data from lidars are
fused with IMU measurements to generate the point cloud
map and 6-DOF poses at each scan. The dense depth map
for each color image will be produced from the the point
cloud map, lidar poses and the extrinsic parameters between
lidar and camera lens.

B. UniÔ¨Åed and EfÔ¨Åcient Multi-Sensor Calibration

We use the method proposed by Furgale et al, [18] to
calibrate the intrinsic parameters of the panoramic camera
and the IMU. Then we present an approach to efÔ¨Åcient ex-
trinsic calibration of our backpack scanning system. Firstly,

Fig. 3: The hardware design and components of our backpack
system (left) and an operator carrying the backpack and a
touch pad for data capture (right).

as shown in Fig 4, we setup a calibration environment (a 4.8
m √ó 3.4 m √ó 3 m room in our case) by placing CCTag mark-
ers [19] densely on the wall. There is no strict requirement on
the markers‚Äô positions but the distance between neighboring
markers is generally less than 20 cm. We then use a high-
accuracy laser scanner Leica BLK360 [12] to reconstruct
the colored dense point cloud D of the calibration room. All
marker positions can be determined by detecting CCtags in
rendered images of the room or manual labeling. In this way,
we have a sparse reconstruction S of the room including all
markers‚Äô 3D positions. Note that the calibration environment
needs to be set up only once.

Using the calibration room as a common reference R,
we estimate the transformation from each lidar and camera
lens to R and then the relative poses between different
sensors can be easily obtained. Firstly, we use 3D Delaunay
triangulation to connect neighboring 3D markers in S. For
a speciÔ¨Åc camera Ci, we detect the CCTags‚Äô positions in its
captured image Ii and apply 2D Delaunay triangulation [20]
to connect nearby markers in Ii. Then we select a 2D
triangle Œ¥ and greedily search for its corresponding 3D
triangle ‚àÜ in S by solving a Perspective-n-Point problem
with RANSAC algorithm [21]. After an optimal 2D-3D

BackpackScanningLidar OdometryPoint CloudIMUPanoramicimagesStable Lidar MappingGlobal OptimizationCompletePointCloudMapDense Depth MapsUndistorted Panoramic ImagesImage Pose Interpolation DepthMap Rendering6DOFPoses Raw DataAR Mapping SystemAR MapPanoramic camera16-beam lidarsTime Server Board & IMUBatteryOnboardProcessorFor the unskewed point cloud of each scan, we extract the
edge and planar features. As described in ZhangJi‚Äôs work [9],
the features are selected based on a ‚Äúcurvature‚Äù score which
measures the smoothness of local surface as following,

c1 =

1
|Pn| ¬∑ (cid:107)pi(cid:107)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

j‚ààS

(pj ‚àí pi)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

,

(1)

(a)

(b)

Fig. 4: (a) The high-accuracy rotating laser scanner Leica
BLK 360 mounted on a tripod. (b) The colored dense point
cloud generated by capturing at one spot with Leica BLK
360. Notice that the empty area on the ground is caused by
the occlusion of the tripod.

triangle match is determined, the transformation T Ci
R from
the camera frame to R can be estimated using all inliers
from the RANSAC process. Finally, for a speciÔ¨Åc lidar Li,
we estimate the transformation T Li
R from lidar frame to R by
using Generalized-ICP [22] to register Li‚Äôs scan and dense
point cloud D. The initial pose ÀÜT Li
R can be obtained by
multiplying T Ci
R with the manually measured transformation
ÀÜT Li
Ci

from Li to Ci.

IV. AR MAPPING SYSTEM

A. System Overview

The raw data generated from our backpack scanning de-
vice includes the point cloud per scan from lidars, IMU mea-
surements and images from 4 Ô¨Åsheye lens of the panoramic
camera. The raw data is then fed into an AR mapping
system to generate AR Maps. Firstly, a lidar odometry system
produces unskewed point cloud and 6-DOF pose for each
scan. In order to correct the drifting error accumulated during
the odometry, the results of the lidar odometry are then
optimized in a global optimization module. This module is
designed to ensure both global and local consistency of point
cloud map. After that, a stable lidar mapping is carried out
to construct the Ô¨Ånal point map with an map fusion strategy
for outlier Ô¨Åltering. Finally, the pose of color images is
interpolated from lidar poses and the dense depth maps are
rendered from the point cloud map. We then elaborate the
individual modules in the following. The pipeline of our AR
Mapping system is illustrated in Fig. 2.

B. Lidar Odometry System with Improved Feature Selection

To be self-contained, we give a brief review on the LOAM
system [9] and then propose our improvements. For each
scan, the edge and planar features are extracted based on
a ‚Äúcurvature‚Äù score computed from the distance between
neighboring points. Then scan-to-scan motion is estimated by
solving a point-to-line or point-to-plane iterative closest point
(ICP) problem. A map consisting of features is built online
and each scan is further registered to the feature map for
pose reÔ¨Ånement. The measurements from an IMU are used
to make motion prediction and remove the scan‚Äôs distortion
introduced by lidar‚Äôs movement.

where pi is ith point in the scan, pj belongs to a point set Pn
which consists of the neighboring points on the same scan
line of pi. c1 is the curvature score. As shown in Equation 1,
the surface smoothness is measured by the sum of distance
from one point‚Äôs neighbors to itself. The intuition behind is
that the distance summation will be small if the neighbors
lie on the same plane with pi. Otherwise, the score is large
if pi is on an edge, However, if there is occlusion between
neighboring points or the points‚Äô distribution is noisy, the
curvature score will fail to convey the surface smoothness
information. To address this problem, we add another score
to improve the curvature model as following,

c2 = (cid:104)

1
|Pl|

(cid:88)

pl‚ààPl

(pl ‚àí pi),

1
|Pr|

(cid:88)

pr‚ààPr

(pr ‚àí pi)(cid:105),

(2)

where Pl and Pr are the point sets on the left and right to
pi in the same scan line respectively. (cid:104)v1, v2(cid:105) represents the
angle between the vector v1 and v2. Intuitively, c2 represents
the curve angle of the scan line at pi. As shown in Fig 5,
c2 will be large for planar features and small for the edge
features. Therefore, a point pi can be classiÔ¨Åed to edge or
planar features based on the criteria as follows,

(cid:40)

pi ‚àà

Fe,
Fp,

if c1 > Œ±e, c2 < Œ≤e
if c1 < Œ±p, c2 > Œ≤p

(3)

where Fe and Fp are the sets of edge and planar features
respectively. Œ±e(= 0.005), Œ≤e(= 2.09rad), Œ±p(= 0.0002)
and Œ≤p(= 2.62rad) are the thresholds.

However, the curvature score and curve angle may both
fail to describe a reliable feature in the following cases:
1) outlier point(Fig. 6a); 2) depth discontinuity(Fig. 6b); 3)
occluded regions(Fig. 6c). As shown in Fig. 6, the point in
red will be falsely classiÔ¨Åed as edge feature based on c1 and
c2. Therefore, we apply feature Ô¨Åltering strategy for each
case and remove all outlier points in above cases:

pi ‚àà

Ô£±
Ô£¥Ô£≤

Ô£¥Ô£≥

outlier points,
depth discontinuity,
occluded regions,

if ‚àÜd1/df > r1, ‚àÜd2/df > r1
if |d1 ‚àí d2| > Œ≥1, Œ∏ < Œ∏1
if Œ∏ > Œ∏2

(4)
where ‚àÜd is the distance between neighboring points on
the same scan line. df is the distance from lidar to the
scan points. Œ∏ is the angle between vectors pointing from
lidar to neighboring points. r1(= 0.014), Œ≥1(= 0.33), Œ∏1(=
5.72rad) and Œ∏2(= 0.57rad) are the thresholds. As shown
in Fig. 7, a large number of false positive edge features are
removed by the feature Ô¨Åltering.

Fig. 5: Feature selection based on the curvature scores and
curve angle.

(a)

(b)

(c)

Fig. 6: Outlier features should be removed in cases: (a)
outlier point, (b) depth discontinuity, (c) occluded regions.

(a) Without feature Ô¨Ålter

(b) With feature Ô¨Ålter

Fig. 7: A demonstration of edge features (coloered in red) in
one scan with and without feature Ô¨Ålter. It can be seen that
a large number of false positive edge features are removed
and only those points at real edges are kept.

(a) Pose Graph only

(b) Global Optimization

Fig. 8: Top view of a detail in point cloud map. It shows
the comparison between with (left) and without (right) map
consistency constraints in global optimization. Notice the
blurry edges in the left and the sharp structure in the right.

C. Global Optimization

The point cloud map generated from lidar odometry usu-
ally suffers from accumulated drifting error especially in
large scale scenes. This results in inconsistent map at loop
points where the backpack scans twice. We adopt a submap-
based loop closing and global optimization strategy to reÔ¨Åne
the trajectory while enforcing the map consistency. Tradi-
tional lidar odometry system [15], [16] utilizes a pose graph
to optimize the lidar‚Äôs trajectory. However, in this way it only
enforces the pose consistency at loop points while ignores the
map consistency globally. To resolve this issue, we follow
an approach similar to sparse surface adjustment [23] and

optimize the Ô¨Ånal trajectory with constraints of poses and
map points.

After processing lidar odometry, we save the undistorted
point cloud and estimated pose for each scan. Assuming the
drifting error is small in a short period of time, we construct
submaps consisting of a small number of consecutive scans.
The pose of the submap is the scan pose in the middle of
the trajectory. For a speciÔ¨Åc submap si, we search for the
spatially nearby submaps Sn. We then remove the ones close
to si temporally in Sn. Finally, we get a set Sc consisting of
the submap candidates for loop closing. For each candidate
sc, we use generalized-ICP [22] to register sc and si and
obtain the relative pose T sc
si . For all point correspondences
within 0.5 m, we compute the Ô¨Åtness score which is the
sum of squared distance from source to target. The registered
submaps with Ô¨Åtness score larger than 0.1 will be rejected.
A pose graph can be constructed using the pose constraints
between the consecutive submaps and submap pairs from
the loop detection. In this way, we construct a pose graph
consisting of relative pose constraints as follows,
(cid:88)

Epose =

T ‚àí1
i+1 ¬∑ Ti ¬∑ Œ¥T odom
i,i+1

Ti,Ti+1‚ààSp
(cid:88)

+

(Tj ,Tk)‚ààSl

T ‚àí1
k

¬∑ Tj ¬∑ Œ¥T loop
j,k

(5)

where Ti stands for the transformation from world frame to
submap i‚Äôs frame. Sp is the set of all submaps and Sl is
the set of all submap pairs generated from loop detection.
Œ¥T odom
i,i+1 is the relative pose between consecutive submaps
from lidar odometry and Œ¥T loop
is the relative pose between
j,k
submap pairs computed from loop closing.

To obtain a consistent point cloud model, Ruhnke et
al, [23] propose to do a joint optimization over sensor pose
and the positions of surface points, which increases variables
signiÔ¨Åcantly and is computationally heavy. Inspired by this
idea, we then add the point-to-plane constraints between
submaps to enforce the map consistency. For each point pi
in submap si, the closest 5 neighboring points are found
using a k-d tree and used to Ô¨Åt plane parameters (n, d). If
the distance from any point to the plane is larger than 0.1 m,
we reject the Ô¨Åtted plane. Finally, we use all the constraints
between point to valid planes. The cost function is presented
below,

Eloop =

(cid:88)

(Tj ,Tk)‚ààSl

nj ¬∑ (T ‚àí1

j

¬∑ Tk ¬∑ pk) + dj,

(6)

where pk is a point in submap k, (nj, dj) is the correspond-
ing plane parameters Ô¨Åtted in submap j. We then optimize
the submap poses by minimizing the total cost function,

Etotal = argmin
Ti‚ààSp

(w1Epose + Eloop),

(7)

where w1 is a balancing weight. Finally, all the poses for scan
will be adjusted according to the optimized submap poses.
Fig 8 shows the comparison between pose graph optimization
and our global optimization. The map is more consistent after
the global optimization.

Lidarùëù"Planar featureùêè"$ùêè"%ùëù&ùêè&$ùêè&%Edge featureLidar‚àÜùëë#‚àÜùëë$ùëë%Lidar‚àÜùëë#ùúÉùëë#ùëë%LidarùúÉ(a)

(b)

Fig. 9: We show the point cloud map constructed in a busy
ofÔ¨Åce with people moving around often. The left shows the
result of simply stitching all point clouds and the right shows
the stable mapping result.

D. Stable Map Construction

Once the global optimization is Ô¨Ånished, the complete
point cloud map is then constructed by stitching all scans‚Äô
point clouds using their poses. In the original LOAM system,
the map consisting of only sparse feature points is maintained
online. The feature map is divided into cubes with size of
dc √ó dc √ó dc (dc = 50m in implementation). After a scan‚Äôs
points is added into the feature map, the point cloud in the
corresponding cubes are downsampled by a voxel grid Ô¨Ålter.
This Ô¨Ålter constructs a voxel grid over the input point cloud
and keeps the centroid of points in each voxel. This process
is illustrated in Fig 9. SpeciÔ¨Åcally, for the map point pm
k in
voxel Vk, the update process with input scan points Ps
k in
Vk can be written as the following,

pm

k =

1
1 + |Ps
k|

(pm

k +

(cid:88)

ps),

ps‚ààPs
k

(8)

Intuitively, as can be seen from Equation 8, the updated map
point pm
k is mainly determined by the centroid position of the
input scan points Ps
k. However, as the drifting error in pose
estimation is accumulated during the odometry, the position
of scan points Ps
k will drift as well. If we give a higher
weight to the input scan points, the map point will drift away
from the orginal position. We then re-design the map fusion
strategy. First, we maintain a map consisting of voxels with
Ô¨Åner resolution (= 5cm in our implementation). For the map
point in each voxel, we use the following update policy to
prevent drifting error.

pm

k =

1
k + |Ps
k|

N m

(N m

k ¬∑ pm

k +

N m

k = N m

k + |Ps
k|,

(cid:88)

ps),

ps‚ààPs
k

(9)

(10)

where the N m
is the set of historical scan points input to
k
the voxel k. In addition, to Ô¨Ålter out the points on dynamic
objects and noisy points, we keep the map points in voxels
with sufÔ¨Åcient observations by checking N m
k > œÑ . Fig. 9
shows the stable mapping module is able to produce cleaner
and more accurate map in highly dynamic environments.

E. Image Pose Interpolation and Depth Map Rendering

So far we have optimized poses for each scan and a
complete point cloud map. We then interpolate the camera

pose of captured color images according to the timestamps.
A 3D model is generated from the point cloud map using
Poisson surface reconstruction [24] and used to render dense
depth maps.

V. EXPERIMENTS

A. Evaluation on Point Cloud Map Accuracy

To evaluate the mapping accuracy of our backpack system,
we use a high-accuracy laser scanner Leica BLK360 to
capture the point maps in different environments as ground
truth. We compare the mapping results from our backpack
with the ground truth maps in 3 different scenes: (a) a
45m √ó 23m ofÔ¨Åce Ô¨Çoor; (b) a 100m √ó 50m campus building
Ô¨Çoor; (c) a 100m √ó 60m outdoor open area. Fig 10 shows
the point cloud maps from Leica BLK360 and AR mapping.
We align the two maps by selecting corresponding points
manually and conducting ICP registration. Table I shows the
mapping accuracy is below 0.05m for indoor environments
and below 0.10m for the outdoor environment. Table II
shows the time of map capturing. Compared to the high-
cost industrial laser scanner, the backpack scanning system
produces map with comparable accuracy and much higher
efÔ¨Åciency.

We further compare our results with the state-of-the-art
lidar mapping system LIO-SAM [16]. Since the implemen-
tation of LIO-SAM only works with single lidar, we run the
lidar odometry using only the horizontal lidar for LIO-SAM
and AR Mapping. Fig. 11a shows the ground truth point
cloud map of campus outdoor scene reconstructed by Leica
BLK360. Fig. 11b shows the point cloud map reconstructed
using AR mapping system. The points are colored based on
the error with respect to the Leica Point cloud. The green
means the smaller error while the red are large errors. We
evaluate the map accuracy with respect to the Leica point
cloud. Since some regions may not be covered by the Leica
point cloud, we ignore the points with errors larger than
0.2m. The mean, median and RMS errors of AR Mapping
is 0.077m, 0.069m and 0.091m which is smaller than map
from LIO-SAM with corresponding error of 0.087m, 0.080m
and 0.103m. Fig. 11c and 11d shows a visual comparison
between maps from AR mapping and LIO-SAM respectively.
AR map has sharper structures while the LIO-SAM map has
relatively blurry details.

Dataset
OfÔ¨Åce
Campus Building
Campus Outdoor

Mean Err. Median Err. RMS Err.
0.051 m
0.042 m
0.046 m
0.062 m
0.037 m
0.048 m
0.082 m
0.059 m
0.068 m

TABLE I: Map accuracy compared with ground truth.

B. Evaluation on Pose and Depth Map Accuracy

We further evaluate the accuracy of 6-DOF poses and
depth maps on AR Maps captured in two large scale shop-
ping mall, WestCity and WestLake. Each shopping mall has
5 Ô¨Çoors and the area of each Ô¨Çoor is around 2000 m2.

Dataset
OfÔ¨Åce
Campus Building
Campus Outdoor

Area
300 m2
5000 m2
6000 m2

Backpack
7 min
41 min
33 min

Leica
66 min
120 min
60 min

TABLE II: Mapping efÔ¨Åciency compared with Leica BLK
360 Stationary laser scanner.

(a) OfÔ¨Åce

(b) Campus building

Fig. 10: The comparison of point map generated from Leica
BLK360 (the Ô¨Årst row) and our AR mapping system. (the
second row).

Fig. 12: Example color image pairs with feature matches
captured in WestCity.

(a) Leica Point Cloud

(b) AR Map Point Cloud

(c) AR Map Detail

(d) LIO-SAM Detail

Fig. 11: (a) The high-accuracy map from Leica BLK360,
(b) Map from AR Mapping, (c) and (d) show a visual
comparison between map quality of AR Mapping system
and LIO-SAM.

This evaluation is based on the image pairs captured in
sequence. As illistrated in Fig. 12, for a speciÔ¨Åc image pair
(Ii, Ij), we Ô¨Årst use guided-matching to Ô¨Ånd high-accuracy
SIFT [25] feature matches between them. Since (Ii, Ii+1)
has corresponding depth maps (Di, Dj) and absolute poses
(Ti, Tj) from the AR map, we can project a feature f i in Ii
to Ij and compute the reprojection error and epipolar error
as follows,

ÀÜf j = Œò(Œò‚àí1(f i, Di, Ti), Tj)

e1 = ÀÜf j ‚àí f j, e2 = Dist( ÀÜf j, lj)

(11)

(12)

where Œò is denoted as the projection function from a 3D
point to a 2D coordinates in a image plane with the camera‚Äô
s extrinsic and intrinsic parameters. Œò‚àí1 is the inverse
projection from 2D to 3D. ÀÜf j is the reprojected feature
coordinates on Ij. lj is the epipolar line corresponding to
fi in Ij. We denote e1 the reprojection error and e2 the

(a)

(c)

(b)

(d)

Fig. 13: The histogram of reprojection error and epipolar
error in WestCity and WestLake Datasets.

epiploar error. The error histograms of AR Map in WestCity
and WestLake are presented in Fig. 13. The quantitative
evaluation results in terms of the mean and media of e1
and e2 are presented in Table III. We also divide the errors
by the camera‚Äôs focal length. This normalized error basically
means an angular error regardless of the image resolution.
It shows that the e1 and e2 are mostly distributed around 10
pixels with the color image resolution 2304 x 3840 and the
normalized error is below 1.5‚ó¶.

C. Application of ARMap in Localization

We further verify the effectiveness of AR Map by applying
it to a localization task as in real AR applications. We capture
a set of query images with resolution 3024 √ó 4032 pixels by
a iPhone 11 mobile phone. For each query image, we Ô¨Årst
use deep image retrieval [26] to search for the scene images
in AR Map and then manually label feature correspondences
between the query and scene images. Finally, with the known

Dataset Mean ¬Øe1 Med. Àúe1 Mean ¬Øe2 Med. Àúe2
12.76px.
5.98px.
WestLake
0.80‚ó¶
0.37‚ó¶
16.86px.
8.99px.
1.05‚ó¶
0.56‚ó¶

7.77px.
0.48‚ó¶
11.13px.
0.69‚ó¶

23.79px.
1.48‚ó¶
24.14px.
1.50‚ó¶

WestCity

TABLE III: Quantitative evaluation on e1 and e2 in datasets
WestCity and WestLake. In each cell, we show the euclidean
pixel error in the Ô¨Årst row and the normalized angular error
in the second row.

(a)

(b)

Fig. 14: The example and camera poses of (a) scene images
and (b) query images in AR Map of WestCity mall.

depth information, the 6-DOF pose of query image can be
easily computed by solving a 3D-2D PnP problem [21]. As
shown in Fig. 14, the example query and retrieved scene
images are presented along with their poses in the map. We
treat those query images successfully localized if the number
of inliers is larger than 8 from solving PnP. There are 97 out
of 110 query images (88.2%) which can be localized and it
shows the AR Map is able to support localization task in the
large scene.

VI. CONCLUSION

In this paper, we propose an end-to-end framework to
build and evaluate AR Maps. A backpack scanning system
is designed with a uniÔ¨Åed calibration approach for efÔ¨Åcient
data capture and the raw data is further processed by a
AR Mapping system to generate accurate AR Maps. The
feature Ô¨Åltering strategy and submap-based global optimiza-
tion module ensure accurate trajectory estimation. The stable
mapping component is able to fuse the lidar scans to produce
high quality AR map even in highly dynamic environment.
Finally, we present an approach for systematic evaluation on
AR Maps.

REFERENCES

[1] O. Bimber and R. Raskar, Spatial augmented reality: merging real

and virtual worlds. CRC press, 2005.

[2] ‚ÄúGoogle map ar,‚Äù https://arvr.google.com/ar/.
[3] ‚ÄúPokemon go,‚Äù https://pokemongolive.com.
[4] ‚ÄúAr core,‚Äù https://developers.google.com/ar.
[5] M. Mekni and A. Lemieux, ‚ÄúAugmented reality: Applications, chal-
lenges and future trends,‚Äù Applied Computational Science, vol. 20, pp.
205‚Äì214, 2014.

[6] P.-E. Sarlin, D. DeTone, T. Malisiewicz, and A. Rabinovich, ‚ÄúSu-
perglue: Learning feature matching with graph neural networks,‚Äù in
Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 2020, pp. 4938‚Äì4947.

[7] E. Brachmann, A. Krull, S. Nowozin, J. Shotton, F. Michel,
S. Gumhold, and C. Rother, ‚ÄúDsac-differentiable ransac for camera
localization,‚Äù in Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 2017, pp. 6684‚Äì6692.

[8] C. L. J. Lee, Wonmyung;Won, ‚ÄúUniÔ¨Åed calibration for multi-camera
multi-lidar systems using a single checkerboard,‚Äù in 2020 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS).
IEEE, 2020, pp. 9033‚Äì9039.

[9] J. Zhang and S. Singh, ‚ÄúLoam: Lidar odometry and mapping in real-

time.‚Äù in Robotics: Science and Systems, vol. 2, no. 9, 2014.

[10] ‚Äî‚Äî, ‚ÄúLaser-visual-inertial odometry and mapping with high robust-
ness and low drift,‚Äù Journal of Field Robotics, vol. 35, no. 8, pp.
1242‚Äì1264, 2018.
pro2

https://matterport.com/cameras/

[11] ‚ÄúMatterport

camera,‚Äù

3d

pro2-3D-camera.

[12] ‚ÄúLeica blk360 imaging laser scanner,‚Äù https://leica-geosystems.com/

products/laser-scanners/scanners/blk360.

[13] ‚ÄúLeica pegasus:backpack wearable mobile mapping solution,‚Äù
https://leica-geosystems.com/en-us/products/mobile-sensor-platforms/
capture-platforms/leica-pegasus-backpack.

[14] J. Jeong, Y. Cho, and A. Kim, ‚ÄúThe road is enough! extrinsic
calibration of non-overlapping stereo camera and lidar using road
information,‚Äù IEEE Robotics and Automation Letters, vol. 4, no. 3,
pp. 2831‚Äì2838, 2019.

[15] T. Shan and B. Englot, ‚ÄúLego-loam: Lightweight and ground-
optimized lidar odometry and mapping on variable terrain,‚Äù in 2018
IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS).

IEEE, 2018, pp. 4758‚Äì4765.

[16] T. Shan, B. Englot, D. Meyers, W. Wang, C. Ratti, and D. Rus,
‚ÄúLio-sam: Tightly-coupled lidar inertial odometry via smoothing and
mapping,‚Äù arXiv preprint arXiv:2007.00258, 2020.

[17] H. Ye, Y. Chen, and M. Liu, ‚ÄúTightly coupled 3d lidar inertial
odometry and mapping,‚Äù in 2019 International Conference on Robotics
and Automation (ICRA).

IEEE, 2019, pp. 3144‚Äì3150.

[18] P. Furgale, J. Rehder, and R. Siegwart, ‚ÄúUniÔ¨Åed temporal and spatial
calibration for multi-sensor systems,‚Äù in 2013 IEEE/RSJ International
Conference on Intelligent Robots and Systems. IEEE, 2013, pp. 1280‚Äì
1286.

[19] L. Calvet, P. Gurdjos, C. Griwodz, and S. Gasparini, ‚ÄúDetection and
accurate localization of circular Ô¨Åducials under highly challenging
conditions,‚Äù in Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 2016, pp. 562‚Äì570.

[20] D.-T. Lee and B. J. Schachter, ‚ÄúTwo algorithms for constructing a
delaunay triangulation,‚Äù International Journal of Computer & Infor-
mation Sciences, vol. 9, no. 3, pp. 219‚Äì242, 1980.

[21] S. Li, C. Xu, and M. Xie, ‚ÄúA robust o (n) solution to the perspective-
n-point problem,‚Äù IEEE transactions on pattern analysis and machine
intelligence, vol. 34, no. 7, pp. 1444‚Äì1450, 2012.

[22] A. Segal, D. Haehnel, and S. Thrun, ‚ÄúGeneralized-icp.‚Äù in Robotics:
science and systems, vol. 2, no. 4. Seattle, WA, 2009, p. 435.
[23] M. Ruhnke, R. K¬®ummerle, G. Grisetti, and W. Burgard, ‚ÄúHighly
accurate 3d surface models by sparse surface adjustment,‚Äù in 2012
IEEE International Conference on Robotics and Automation.
IEEE,
2012, pp. 751‚Äì757.

[24] M. Kazhdan, M. Bolitho, and H. Hoppe, ‚ÄúPoisson surface recon-
struction,‚Äù in Proceedings of the fourth Eurographics symposium on
Geometry processing, vol. 7, 2006.

[25] D. G. Lowe, ‚ÄúObject recognition from local scale-invariant features,‚Äù
the seventh IEEE international conference on

in Proceedings of
computer vision, vol. 2.

Ieee, 1999, pp. 1150‚Äì1157.

[26] A. Gordo, J. Almazan, J. Revaud, and D. Larlus, ‚ÄúEnd-to-end learning
of deep visual representations for image retrieval,‚Äù International
Journal of Computer Vision, vol. 124, no. 2, pp. 237‚Äì254, 2017.

