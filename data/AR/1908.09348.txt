Color Blending in Outdoor Optical See-through AR:  
The Effect of Real-world Backgrounds on User Interface Color 

Joseph L. Gabbard, Member IEEE, J. Edward Swan II, Senior Member, IEEE, and Adam Zarger 

Abstract— It has been noted anecdotally and through a small number of formal studies that ambient lighting conditions and dynamic real-
world backgrounds affect the usability of optical see-through augmented reality (AR) displays; especially so in outdoor environments. Our 
previous work examined these effects using painted posters as representative real-world backgrounds [1]. In this paper, we present a study 
that employs an experimental testbed that allows AR graphics to be overlaid onto real-world backgrounds as well as painted posters. Our 
results  indicate  that  color  blending  effects  of  physical  materials  as  backgrounds  are  nearly  the  same  as  their  corresponding  poster  back-
grounds, even though the colors of each pair are only a metameric match. More importantly, our results suggest that given the current capa-
bilities of optical see-through head-mounted displays (oHMDs), the implications are, at a minimum, a reduced color gamut available to user 
interface (UI) designers. In worse cases, there are unknown or unexpected color interactions that no UI or system designers can plan for; 
significantly crippling the usability of the UI or altering the semantic interpretation of graphical elements. Further, our results support the 
concept of an adaptive AR system which can dynamically alter the color of UI elements based on predicted background color interactions. 
These interactions can be studied and predicted through methods such as those presented in this work.  

Index Terms: Outdoor Augmented Reality, Optical See-through Display, User Interface Design, Color Perception. 

1 

INTRODUCTION 

During the past 20 or so years, AR technologies have slowly made 
their way into several application domains (e.g., Okur et al. [2]; 
Debenham, Thomas & Trout [3]; Livingston et al. [4], Gleue and 
Dähne [5], Caudell and Mizell [6]).  More recently, AR concepts 
and applications have begun to gain renewed momentum in the 
public  eye;  be  it  through  the  proliferation  of  location-aware 
handheld AR apps [7], growing numbers of AR demonstrations 
on YouTube, television advertisements and live sporting events 
that  continue  to  leverage  AR  concepts,  penetration  of  more  so-
phisticated video-AR systems into the mainstream automobile in-
dustry  (e.g.,  overlaid  graphics  onto  rear-  and  side-facing  cam-
eras), and most recently, Google's announcement of Project Glass 
(an  optical-see  through  head-mounted  display  with  voice-acti-
vated  smartphone,  GPS  and  internet  capabilities).    As  has  been 
seen in earlier AR system and with other technologies (e.g., vir-
tual reality and artificial intelligence), there are well-founded con-
cerns that AR may be over-promised or at least, over-perceived.  
And while significant progress has been made on technically-lim-
iting hurdles such as outdoor tracking and registration, we must 
be careful not to ignore other equally challenging and potentially 
crippling AR research areas as AR attempts to is affirm its posi-
tion as a viable consumer technology platform. 

With significant improvements in mobile phone and tablet dis-
plays, user expectations of display performance is at an all-time 
high. Classical display characteristics such as resolution, clarity, 
color,  contrast  and  (for  head-worn  displays)  field  of  view,  etc., 
likely  play  an  important  roles  in  meeting  these  expectations. 
Many of these technical features are largely controlled by display 
design and specification (resolution, fov, etc.).  Others, such as 
color,  luminance  and  thus  contrast,  manifest  themselves  differ-
ently depending on the context (absolute dark vs. daylight) and 
significant interactions with the real world. Currently, we do not 
fully understand how even the most advanced optical see-through 

• Joseph L. Gabbard is with Virginia Tech.  

E-mail: jgabbard@vt.edu. 

• J. Edward Swan II is with Mississippi State University.  

E-mail: swan@acm.org 

• Adam Zarger is with Excella Consulting  and formally with Virginia Tech.  

E-mail: adam.zarger@excella.com 

display's color and luminance systems interact with environmen-
tal lighting and backgrounds that effectively act as the AR sys-
tems canvas or "desktop".   

Traditional UI designers, are afforded the luxury of being able 
to carefully control the color each pixel atop a black (blank) can-
vas in order to carefully craft elegant designs and effective UIs 
(Figure 1).  In video see-through AR, UI designers must take into 
account dynamic backgrounds and lighting, but in the end are able 
to completely "overwrite" any given camera-captured real-world 
pixel and/or augmented overlaid graphics pixel color and trans-
parency to mitigate problems with color perception and/or con-
trast to ultimately achieve a desired effect. Moreover, in both tra-
ditional UIs and video-based AR systems, designers are privy to 
a rich color-palette, ranging from black (RGB: 0, 0, 0) to white 
(RGB: 255, 255, 255), and including over 65 million colors.  In 
optical see-through displays, UI designers are at a relative disad-
vantage. Since their canvas is not black (it is the optical light from 
real-world), nor is it completely "overwrite-able", even the most 
saturated pixel color and/or most illuminated pixel rendered via 
today's oHMD displays result in some level of graphics transpar-
ency onto the real-world.  Indeed, it is this capability of transpar-
ency that makes optical see-through AR applications appealing in 
many contexts.  Aside from the dynamic and, to some extent, un-
controllable canvas (i.e., real-world background), oHMD UI de-
signers also have a much more limited color palette in which to 
work since the color black is essentially completely transparent 
using additive color oHMD displays. As such, there is a large set 
of non-black colors (to date, the actual number has not been quan-
tified) with low luminance (i.e., "dark colors") that in other dis-
play paradigms afford good contrast against bright backgrounds, 
yet are simply unavailable or un-useable.  To exacerbate the chal-
lenge, the colors that are available to the oHMD UI designer may 
be perceived differently from moment to moment depending upon 
the  constantly  changing  canvas  (real-world  background).    How 
can oHMD UI designers create effective and semantically mean-
ingful UI designs without understanding how these designs may 
be perceived by different users under different conditions?  Cur-
rently, one strategy is to design for specific usage contexts where 
the backgrounds are likely known, and may not change that often 
either  in  chrominance  or  luminance,  e.g.,  in  an  desert  environ-
ment.  Another  popular  strategy  (for  oHMD  text  color,  contrast 
and legibility) is to surround the graphical element in a large white 

 
 
 
 
 
 
 
Figure 1.  The designer's canvas in outdoor optical see-through AR settings is much more challenging that that of traditional or even video-see 
through  AR.    Specifically,  the  color  palette  is  greatly  reduced,  and  there  is  no  certainty  that  the  color  used  for  an  interface  element  will  be 
preserved given dynamic changes in real-world backgrounds and lighting conditions. 

billboard; however such strategies occlude more of the real world 
than may be necessary. Longitudinally, our work aims to better 
understand these color and luminance interactions, and ultimately 
create a model capable of taking a UI designers intent (e.g., I want 
this pixel to be perceived as blue by users with normal vision) and 
dynamically  rendering  it  (e.g.,  using  real-time  adaptive  algo-
rithms) to an oHMD in order to best preserve and maintain that 
intent.  

In this paper, we examine and quantify, how environmental and 
display-generated light combine in an oHMD, given a set of rep-
resentative (proxy and physical/actual) real-world backgrounds. 
Section 2 presents related work in AR and other fields.  In section 
3, we discuss our optical testbed that emulates outdoor lighting 
conditions and allows us to carefully measure the combined color 
of virtual colors and real-world backgrounds as projected through 
an optical see-through display. Section 4 describes our study, in-
cluding the experimental design, how materials were assembled 
to  create  real-world  backgrounds,  and  data  collection  methods. 
We present a rich set of results and discussion in Section 5, in-
cluding our approach to qualitative and quantitative data analysis, 
followed by ideas for Future Work in Section 6. 

2  RELATED WORK 

While not fully understood, recent work in AR has undoubtedly 
shown that the blending of real-world lighting, backgrounds, and 
virtual graphics has an effect on display usability and user perfor-
mance (Kerr et al. [8], Kalkofen [9], Gabbard et al. [1, 10, 11], 
Livingston  et  al.  [12],  Peterson,  Axholt  &  Ellis  [13],  Pingel  & 
Clarke [14], Thomas et al. [15]).  In many cases, outdoor environ-
mental  conditions  can,  for  example,  dramatically  alter  users’ 
color perception of user interface elements.  A common observa-
tion  in  a  number  of  studies  using  outdoor  AR  using  oHMDs  is 
that text or icon colors become washed out in outdoor conditions. 
Kerr et al. [8] present a study a employing a monocular oHMD 
in  an  outdoor  urban  environment,  where  participants  complete  
wayfinding tasks while relying on augmented imagery for orien-
tation. The authors emphatically state that the most important is-
sue they encountered was the inability of participants to see the 
display clearly outdoors, adding that all participants encountered 
difficulty using the oHMD in bright outdoor environments with 
some graphical elements more visible than others. 

Pingel  &  Clarke  [14]  noted  that  participants  had  difficulty, 
measured both quantitatively and qualitatively, locating waypoint 
markers due to issues with the colors on their  AR map display.  
They further consider mitigating the problem by choosing colors 
that  "stand  out"  all  the  time,  or  by  using  adaptive  cartographic 
(color)  representations,  but  note  users'  difficulty  to  cognitively 
follow constantly changing symbol colors.  

Thomas et al. [15] conducted an informal study to determine a 
set of initial colors to graphically encode an outdoor AR user in-
terface. Their goal was to assess the opaqueness and visibility of 
augmenting  graphics,  under  a  variety  of  viewing  conditions 
(shade-to-sun, sun-to-shade, etc.). After examining 36 AR colors, 
they listed a small number of usable colors for specific viewing 
conditions, but do not give details on the real-world background 
on which their results are based (although some rough assump-
tions can be made by examining their figures and usage content). 
Kalkofenet al. [9] addresses similar color issues in mixed-real-
ity, and warns readers that "careless generation of visualization in 
AR environments may lead to misleading interactions of colors 
and shades representing the real and the virtual objects."  They 
further assert that renderings of virtual objects that do not take the 
prospective real world surroundings into account may result in a 
misinterpretation  of  intention  of  the  composed  virtual  and  real 
scene. 

These insights speak directly to the issue of color blending with 
respect to UI color encoding and color perception; specifically the 
fact  that  semantic  interpretation  of  interface  elements  can,  and 
quite often will, be compromised – resulting in severe usability 
problems or critical incidents in application domains where color 
encoding  is  critical  (e.g.,  military,  medical  visualization,  etc.).  
While numerous researchers have seen this anecdotally, there is 
still much work to be done to fully understand this phenomenon. 
Some work has been done to quantify the effects of real-world 
background  and  lighting  on  virtual  graphics.  Gabbard  et  al.  [1] 
present an engineering study that uses a colorimeter to measure 
the light that enters a users eye given an AR display color and a 
representative real-world background.  They measured over two 
dozen  AR  display  colors  against  several  poster-based  back-
grounds.  Their results show a strong interaction between back-
ground and AR color, and more specifically show how the color 

 
 
 
gamut for oHMDs in outdoor settings can be significantly com-
pressed around the whitepoint.  

Livingston et al. [12] also examines the combined effect dis-
play contrast and resolution on visual acuity using a varied set of 
different AR displays. They describe a color matching study that 
suggests  AR  users  perceptually  distort  some  colors  (e.g.,  blue) 
more than others  (e.g., the physical color swatch a user specifies 
to  match  an  AR  color  is  much  different  than  the  target  color).  
Their user studies described therein further suggest that modest 
adjustments  to  levels  of  contrast  can  increase  acuity  in  visual 
tasks.  

Peterson, Axholt & Ellis [13] present a novel technique for seg-
regating  overlapping  labels  for  far-field  objects  in  outdoor  AR.  
In this work, they note the difficulty in maintaining legible text 
due to the dynamic outdoor conditions (e.g., passing clouds and 
movement  of  the  sun  during  the  day),  and  ultimately  vary  the 
number of neutral density filters used to ensure visibility both of 
the virtual images and the background markers. For their study, it 
was necessary to keep the virtual display elements at least 10-15% 
brighter than the background.  

Gabbard, Swan, and Hix [11] present an oHMD-based outdoor 
study that examined the effect of text color and backgrounds on a 
text legibility task. The study was extended by Gabbard, Swan, et 
al.  [10]  to  include  additional  independent  variables  and  actual 
real-world backgrounds.  Results from these studies indicate that 
backgrounds affect perceived text color and thus legibility in out-
door AR reading tasks.  

3  A  TESTBED  FOR  CONDUCTING  COLOR  BLENDING  

ENGINEERING STUDIES  

Our goals for creating the testbed were to (1) simulate natural out-
door lighting quality and brightness in a controlled indoor envi-
ronment, (2) systematically vary real-world background objects, 
and thus systematically vary reflected, colored light, (3) integrate 
an oHMD display, driven by software capable of systematically 
displaying virtual stimuli, (4) determine a data collection method 
that is accurate, reliable, and automated as much as possible, and 
(5) establish a research testbed flexible enough to conduct both 
engineering studies, as well as, planned user-based studies. 

The testbed is mounted on an optical bench so that we can pre-
cisely  adjust  and  control  the  position  and  orientation  of  each 
testbed  component  (Figure  2).  The  testbed  supports  systematic 
manipulation of AR light and light reflected off actual physical 
materials (e.g., real bricks, pavement cut from a road, etc.) as real-
world backgrounds, as well as painted posters used as representa-
tive or proxy real-world backgrounds. 

We used specialized lights that reproduce the daylight standard 
white point of D65, and accurately project as much of the visible 
color  spectrum  as  naturally  exists  outdoors.    Light  sources  are 
rated using the color rendering index (CRI), which ranges from 
1–100, with 100 denoting a light that can perfectly reproduce the 
entire daylight color spectrum. For this study, we used fluorescent 
lights rated at 95 CRI. We sealed the lights in an enclosure con-
structed  of  black  foam  core  board.    We  cut  a  large  rectangular 
hole in the back of the enclosure to allow physical materials to be 
placed flush up against the back. For a more thorough discussion 
of the enclosure’s lighting and general construction see [1].   

For our testbed, we disassembled an NVIS nVisor SX oHMD 
and custom mounted its monocle inline with other testbed com-
ponents.  The  oHMD  monocle  is  designed  to  be  23  millimeters 
from a user’s eye, where the resulting display projection creates a 
footprint smaller than what is required by our colorimeter sensor. 
As such, we placed a 50 mm camera SLR lens at a location ap-
proximate to a user’s eye when wearing the display to increase 
the area in which to sample. We mounted an OTC1000 ColorPro 
Optical  Color  Analyzer  (colorimeter)  downstream  of  the  SLR 

lens to measure the blended light. Both the SLR lens and the col-
orimeter were mounted on precision optical bench hardware al-
lowing for fine tuning along 5 degrees of freedom (both compo-
nents were fixed in-line along one positional axis as shown in Fig-
ure 2). 

Lastly,  we  developed  a  calibration  process  for  the  testbed  to 
ensure our components were positioned correctly, and that data 
collection was as accurate and reproducible as possible. We based 
the  calibration  process  on  the  D65  white  point,  verifying  white 
point  measurements  (and  slightly  adjusting  testbed  components 
as needed) under three different white point measurement condi-
tions: oHMD projecting all white graphics with enclosure lights 
off; oHMD off with enclosure lights on; and oHMD projecting all 
white graphics with enclosure lights on. By examining all three 
conditions, we were assured that data collected under any exper-
imental lighting condition were accurate and consistent with re-
spect to the D65 white point. We further verified the calibration 
by separately projecting red, green and blue oHMD light under 
the no-lights (enclosure lights off) condition, checking the three 

Figure 2.  (Above) A top view schematic of our experimental testbed 
depicting an enclosure (green) containing two 15-watt lights reflect-
ing off of real-world backgrounds (orange).  Indirectly reflected light 
exited the enclosure through the frustum (red) and into the AR oHMD 
monocle (blue).  The AR oHMD image, designed to be focused on a 
user’s retina, was directed towards the colorimeter (grey) using  an 
SLR  camera  lens  (yellow).    A  desktop  computer  (not  shown)  was 
used to drive the AR display, as well as, to capture/log data from the 
colorimeter.  (Below) An annotated photograph of our optical exper-
imental testbed shown with  pavement  material as real-world back-
ground. 

 
Figure  3.    We  created  five  real-world  backgrounds  using  physical 
materials (top row): brick, green foliage, pavement, sand and brown 
foliage. These backgrounds were mounted in modular frames so we 
could easily  place  them at the  back of the  enclosure.   We also  in-
cluded four painted posters (bottom row): brick, green foliage, pave-
ment  and  sidewalk  as  additional  backgrounds.  We  used  a  blank 
poster for the white background condition. The eleventh background 
was a no-lights condition. 

extreme  colorimeter  measurements  against  the  respective  red, 
green, blue corners of the standard sRGB color gamut (i.e., per 
oHMD color specification). 

4  ENGINEERING STUDY: MATERIALS AND APPROACH 

We  overlaid  27  virtual  colors  presented  via  the  oHMD  onto 
eleven representative real-world background conditions (illumi-
nated using 95CRI lights as described in section 3). We measured 
light  exiting  the  oHMD  monocle  under  the  three  experimental 
lighting  conditions:  backgrounds  illuminated  (no  AR  oHMD 
light), oHMD illuminated (no illuminated backgrounds), and both 
background and oHMD illuminated. 

Five of the eleven backgrounds were actual physical materials 
(Figure  3)  representative  of  outdoor  real-world  backgrounds: 
brick, brown foliage, green foliage, pavement, and sand. We set 
these real backgrounds in 16" x 24" frames constructed using 2x4 
lumber.    This  allowed  us  to  easily  handle  the  backgrounds  and 
swap them in and out of the testbed without compromising back-
ground surfaces.  The specific way in which we assembled and 
framed  each  background  was  slightly  different  depending  upon 
the  nature  of  the  physical  material.    The  brick  background  was 
assembled  by  stacking  several  ten-hole  perforated  bricks  into  a 
frame in a fashion consistent with a brick building facade.  Both 
brown  foliage  and  green  foliage  backgrounds  were  created  by 
scavenging leaves from ground litter (mostly fallen oak leaves) 
and  a  living  plant  (Spathiphyllum  cochlearispathum,  or  peace 
lily)  respectively;  then  affixed  and  pressed  to  a  1/2"  plywood 
panel  using  industrial  strength  spray  adhesive.    We  obtained  a 
slab of pavement from a road construction site, and then cut the 
slab to fit tightly in a frame.  As with the foliage backgrounds, we 
created the sand background by adhering new playground sand to 
a 1/2" plywood panel.  We mounted each of the three panels into 
a frame using four set screws.  

Four of the backgrounds were painted posters (Figure 3) also 
representing  the  color  of  typical  real-world  backgrounds:  brick 
poster,  green  foliage  poster,  pavement  poster  and  sidewalk 
poster. By using these posters, we were able to make interesting 

Figure 4.  We tested 27 colors, defined by every combination of red 
(R), green (G), and blue (B), at the levels of 0, 128, and 255.  For the 
color  black,  the  oHMD  was  turned  off.  Note  that  the  colors  shown 
here, as well as elsewhere in this paper, are just representative  of 
the real-world experience of seeing that color. 

comparisons  between  poster-based  backgrounds  and  back-
grounds made of physical materials.  Note that there was no at-
tempt to color-match poster backgrounds with their physical ma-
terial (real) counterparts; although in two cases the matching was 
very close (brick and pavement).  

We  used  two  additional  backgrounds:  a  no-lights  condition 
where the fluorescent lights were turned off, and a white condition 
where we placed a white poster in our apparatus.   

We derived 27 colors (presented via the oHMD) to overlay onto 
backgrounds using combinations of fully saturated (256), half sat-
urated (128) and desaturated (0) values for red, green, and blue 
(Figure 4). We turned the oHMD off for the black (0,0,0) color 
condition,  in  essence  measuring  the  background  color  as  seen 
through the oHMD’s display optics. To automate the projection 
of  color  via  the  oHMD,  we  created  a  PowerPoint  presentation 
containing 26 full-screen colored slides. Each colored slide was 
rendered to the oHMD for two minutes as we continuously rec-
orded data from the colorimeter.  On average, we collected ap-
proximately two readings per second, although the rate varied ac-
cording to the color being measured. As a result, we collected be-
tween 1459 and 7595 readings per background, for a total of N = 
31,195 data points across eleven backgrounds. 

We next investigated the colorimeter’s behavior over time by 
studying time-series graphs of each background by color combi-
nation.  Based on these graphs, we calculated each background by 
color cell by taking the median xyY value (the colorimeter reports 
color using the CIE 1931 xyY format, see [16] for details). These 
medians  reduced  our  collected  dataset  to  one  value  per  back-
ground by color combination, giving N = 296 data points1.   

Lastly,  we  transformed  the  data  into  the  CIE  1976  u’v’  and 
L*u*v* color spaces, using the formulas found in Foley et al. [17].  
There is a direct linear relationship between xyY and u’v’, but con-
verting from xyY to L*u*v* requires that the xyY colors be nor-
malized relative to a white point.  For our experimental setup, this 
white point represents the brightest possible color (the color with 
the largest energy density).  Our experimental setup yielded three 
different  white  points:  (1)  only  the  oHMD  illuminated  (the  no-
lights  condition),  and  the  oHMD  presenting  the  color  white 
(255,255,255), (2) the oHMD off (the color black) against the il-
luminated white background, and (3) the oHMD presenting the 
color white against the illuminated white background.  Our col-
lected data contained xyY values for each of these conditions; we 

1  There  were  10  (background:  brick  poster,  real  brick,  real 
brown foliage, green foliage poster, real green foliage, pavement 
poster, real pavement, real sand, sidewalk poster, white poster) × 

27  (color)  +  1  (no  lights)  ×  26  (all  colors  except  black)  =  296 
experimental cells; the no-lights, black condition yields no light 
energy and was not included. 

 
                                                                    
We next examined the distribution of these values over the back-
ground pairs, and determined that the average was a reasonable 
measure of central tendency.   

In  Figure  5,  the  stacked  bar  graphs  depict  these  calculations.  
Together, the pink and blue bars depict the total average length of 
L*u*v*  for  each  background  pair.    The  pink  bar  represents  the 
average projection of L* on L*u*v*. Our interpretation is that the 
pink bar measures luminance change, the blue bar measures chro-
maticity change, and both bars together measure total perceptual 
color change. 

The  upper-left  graph,  which  compares  the  no-lights  and  the 
white  poster  backgrounds,  has  the  largest  perceptual  color 
change;  the  left-hand  edge  of  this  bar  has  the  value  0,  and  the 
right-hand edge has the value 116.6, in L*u*v* units.  All of the 
other bar graphs are scaled relative to this maximum value.   

After printing and cutting out these 55 small multiples, we con-
sidered the information contained in both the u’v’ scatterplot and 
the L*u*v* stacked bar chart.  Figure 5 shows our final arrange-
ment: rows are ordered according to the first background in each 
pair,  while  columns  are  ordered  according  to  the  second  back-
ground in each pair.  The colored borders indicate the following 
five categories: 
Washout Due to Chromaticity (a): This category consists of the 
10  background  pairs  in  the  top  row  of  Figure  5,  where  the  no-
lights  background  is  paired  with  all  of  the  other  backgrounds.  
Figure 6a shows a larger version of the u’v’ scatterplot of one of 
these pairs, the white poster / no-lights combination; these larger 
scatterplots allow an analysis of how each oHMD color changes 
when measured against these two backgrounds.  In this category, 
in the no-lights condition, the colors are widely distributed in u’v’ 
space.  This is equivalent to using an AR system on a dark night, 
where we would expect a large hue separation between the dis-
play colors.  When measured against an illuminated background, 
the colors collapse into a much smaller point, clustering around 
the  background  color,  and  creating  a  large  star-shaped  pattern.  
This causes the hue differences to appear washed out — a com-
mon  AR  experience  outdoors.    For  example,  in  Figure  6a  the 
rightmost data point (a black diamond ((cid:1) )) represents the color 
red in the no-lights condition; under the white poster background 
this color has shifted leftwards and is part of the central grouping 
of colors.  This indicates that the color red is quite vibrant and 
saturated  in  the  no-lights  condition,  but  becomes  desaturated 
against white, and may even no longer appear to be red (although 
verifying this would require a human color judgment).   

The unexpected element of this analysis comes from studying 
the bar graphs for this group: these indicate that the great majority 
of the perceptual change is due to a large chromaticity shift for 
each color.  Surprisingly, the bar graphs indicate that the change 
in  luminance  is  a  relatively  minor  perceptual  component  when 
going from complete darkness to an illuminated background.   

used these values for our three white points.  We used white point 
(1) when only the oHMD was illuminated, white point (2) when 
only the background was illuminated, and white point (3) when 
both were illuminated; we only used these white points to covert 
xyY values into L*u*v* values. Our calculations used the follow-
ing formulas (Foley et al. [18]): 

where (eq 1) transforms xyY into 1931 XYZ CIE primaries, (eq 2) 
transforms  XYZ  into  1976  CIE  u’v’  values,  (eq  3)  transforms 
XnYnZn, the color of the relevant white point, into corre-sponding 
un’vn’  values,  and  (eq  4)  transforms  Y,  Yn,  u’v’,  and  un’vn’  into 
L*u*v* values.  

5  RESULTS AND DISCUSSION 

To analyze the data, we created a small-multiples graph that de-
picted all 55 pairwise combinations of our 11 backgrounds.  Our 
motivation for looking at pairwise combinations is the idea that 
an AR user would be facing one background, and then turn to face 
another  background;  we  wanted  to  see  the  effect  of  this  on  the 
displayed  colors.    We  then  printed  this  large  graph  on  a  color 
printer, cut out each small graph, and then hand-sorted the graphs 
into various groups and orderings, looking for patterns in how the 
colors changed against different backgrounds.   

Figure  5  shows  the  result  of  this  analysis.   Each  small  graph 
depicts a scatterplot in u’v’ chromaticity space, and lists a pair of 
backgrounds in the title strip.  On the scatterplot we drew a vector 
for each color; for a given vector one endpoint (colored white) is 
the u’v’ value of the color measured against the first background, 
and the other endpoint (colored gray) is the u’v’ value measured 
against the second background.  We did not label each color, but 
instead  examined  the  general  pattern  formed  by  the  lines.    For 
example, the upper-left small graph of Figure 5 compares the no-
lights and the white poster backgrounds.  In this graph, we see a 
star-shaped pattern: against the white poster, the colors are clus-
tered in a small region around the white point, and against the no-
lights background the colors are spread throughout u’v’ space. 

Each small graph also contains a stacked bar graph, which de-
picts the average change in CIE 1976 L*u*v* space when going 
from one background to the other.  Because L*u*v* space is an 
approximately  perceptually  linear  3D  space,  we  calculated  3D 
Euclidean norm distances in this space.  For each of our 27 colors, 
we  calculated  the  3D  Euclidean  distance  for  each  pair  of  back-
grounds (for backgrounds paired with the no-lights condition we 
calculated 26 distances).  Each of these distances is the length of 
a vector in L*u*v* space, and represents a total perceptual color 
change.    We  can  consider  this  vector  to  consist  of  two  compo-
nents: u*v*, which represents a change in chromaticity, and L*, 
which  represents  a  change  in  luminance.    In  order  to  consider 
chromaticity and luminance separately, we calculated the length 
of L* projected onto the vector L*u*v*; this length gives the pro-
portion of the total perceptual color change that is due to lumi-
nance.  This gave us 27 (26) pairs of values per background pair.  

 
 
Figure 5. Pairwise visual analysis of eleven backgrounds.  Small multiples of scatterplots are outlined into four categories (a–d), depending 
upon the nature of color shifts: (a) washout due to chromaticity, (b) washout mostly due to luminance, (c) washout due to both chromaticity 
and luminance, and (d) linear shift in chromaticity.  Category (e) compares the three poster-real conditions.  See the text for a detailed descrip-
tion of the small multiple scatterplots.  From top to bottom, scatterplots are grouped according to the first background denoted in the pair.  
From left to right, scatterplots are ordered alphabetically according to the second background in the pair.  Each scatterplot is labeled to denote 
the pair of backgrounds depicted using NL (no light), White (white), Brk (brick), BrnFg (brown foliage), GrnFg (green foliage), Pvmt (pave-
ment), Sand (sand), and Sdwlk (sidewalk).  Labels ending in an ‘R’ denote a real physical material background; labels ending in a ‘P’ denote a 
poster background.   

 
 
 
 
This analysis raises two potential concerns: (1) will UI compo-
nents become less legible against the bright white background?  
And, (2) will the hues in the center become less perceptually dis-
criminable, meaning that fewer hues are available as a UI design 
component?  Figures 5 and 6 suggest that both effects may occur, 
but human judgments are required to validate both concerns. 
Washout Due to Luminance (b): This category consists of the 10 
background pairs in the second row of Figure 5, where the white 
poster is paired with every other background, as well as the sand 
/  real  green  foliage  combination  from  the  third  row.    Here  the 
washout  effect  is  broadly  similar  to  (a),  but  the  chromaticity 
change  is  greatly  reduced,  appearing  as  a  much  smaller  star-
shaped pattern.  Figure 6b shows a representative example from 
this group, the white poster / real green foliage combination.  For 
this group both backgrounds are relatively bright, and so the col-
ors remain washed out; the bar graphs indicate that the majority 
of the perceptual color change is due to a change in luminance. 
Washout Due to Luminance + Chromaticity (c):  This category 
consists of 12 background pairs in the third and forth rows of Fig-
ure 5, where the sand and sidewalk poster backgrounds are paired 
with other backgrounds. Figure 6c shows a representative exam-
ple  (real  sand  /  real  pavement).    The  pairs  in  this  category  are 
broadly similar in appearance to (b), in that they all show a simi-
lar-sized washout effect and star-shaped pattern.  However, here 
the bar graphs indicate that luminance and chromaticity contrib-
ute relatively evenly to the overall perceptual color change.  We 
note that there is not a clear separation between this category and 
(b), and several background pairs placed here could also be placed 
in (b).  
Linear Chromaticity Shift (d): This category consists of the re-
maining 23 background pairs, where the less bright backgrounds 
are compared to each other; Figure 6d shows a representative ex-
ample (real brown foliage / real brick).  Here the shape in the u’v’ 
scatterplots indicates a linear shift in all of the colors; for exam-
ple, in Figure 6d, when the user moves from a brown foliage back-
ground to red bricks, all of the oHMD colors are shifted from left 

to right, towards the red side of the chromaticity diagram.  The 
bar graphs indicate that luminance contributes very little; almost 
all of the perceptual color change is due to this linear shift in chro-
maticity.  The primary usability concern from such a linear shift 
is situations where hues change; for example in Figure 6d yellows 
shift into oranges, greens shift into yellows, and blues shift into 
purples.  While Figure 6d suggests that there are a variety of hues 
available for UI components against both backgrounds, the shift-
ing of hues could be problematic in domains where color encod-
ing is critically important.   
Poser vs. Real Comparisons (e): This category is a selection of 
three background pairs from category (d), but here the interest is 
that the real backgrounds of brick, green foliage, and pavement 
are each compared with their respective poster versions. In Fig-
ure 5(e) we compare the posters and the real backgrounds; Figure 
7 shows a larger version of these scatterplots.  As with the rest of 
category (d), there was a linear color shift for each pair.  However, 
for the real / poster pavement condition (Figure 7c) we measured 
the  smallest  L*u*v*  color  change  in  the  entire  experiment  (5.6 
L*u*v* units). Figure 7a, comparing the brick poster with a back-
ground  of  real  bricks,  shows  a  small  star-shaped  pattern,  while 
Figure 7b, comparing the green foliage poster with a background 
of actual green leaves, shows a small linear shift. 

A finding of this study is that the patterns and trends described 
above related to physical materials as real-world backgrounds are 
broadly similar to those patterns reported in other related studies 
that employed color-matched, painted posters as real-world back-
grounds (e.g., [1]).  We were somewhat surprised that there were 
not larger color changes between the real and poster versions of 
these backgrounds, especially considering that the painted posters 
were  only  a  metameric  color  match  against  real-world  back-
grounds. 

Figure  6.   Pairwise visual  analysis of four background  pairs.   Each panel  is an example from the first four categories depicted in Figure 5: 
(a) washout due to chromaticity (white poster à no lights); (b) washout mostly due to luminance (white poster à real green foliage); (c) washout 
due to both chromaticity and luminance (real sand à real pavement); and (d) linear shift in chromaticity (real brown foliage à real brick).  Each 
panel shows how every color changes when the background changes; for each color, the arrow points from the first background towards the 
second background as indicated in the panel title.  The cyan square is for the color black, when the HMD is turned off; this is the color of each 
background as measured through the HMD’s optics.  Note that in (a) there is only one cyan square; this is the poster + lights white point. 

 
 
This  finding  is  one  argument  for  the  utility  of  color-matched 
posters for this type of analysis.  Indeed, since posters can be cre-
ated more readily and are easier to handle, these types of engi-
neering studies can be used to examine a large set of independent 
variables (e.g., backgrounds, AR colors, source lighting) in a rel-
atively short amount of time.  However, the texture, depth, and 
reflectance properties of painted posters may not be comparable 
to their respective physical material. 

Lastly, since this work is an engineering study that measured 
quantifiable elements, we need to conduct follow-on user studies 
to verify the perceptual consequences we conjecture above.  From 
our own experiences (and others), we know that washout is com-
mon and is obvious to most causal outdoor oHMD users.  How-
ever, we have not seen a discussion in the literature related to lin-
ear chromaticity shift.  One explanation may be color constancy; 
a feature of the human perception system that ensures an object’s 
perceived color remains constant under illumination changes that 
alter the object’s measured color [19]. 

6  FUTURE WORK 

In a future study, we will consider independently varying the lu-
minance of both the lighting source and oHMD. Using this study 
as a rich set of baseline data, we will then be able examine how 
differing levels of luminance contrast (between light source and 
oHMD) affects color shifts in AR graphics against various real-
world backgrounds. Of specific interest would be integrating an 
oHMD  with  much  higher  luminance  to  examine  the  degree  to 
which  simply  building  brighter  displays  mitigates  these  color 
blending effects.  Based on our work to date, we are confident that 
there  are  important  chromaticity  interactions  to  consider  inde-
pendent of luminance. 

We  plan  to  conduct  a  user-based  perceptual  color-matching 
study using these same independent variables to compare how us-
ers’  perception  matches  these  data.  If  the  data  matches  under 
some conditions, then we can subsequently collect volumes of en-
gineering  data  with  some  assurance  that  resulting  analyses  will 
provide insight into actual user experiences. 

We  are  planning  a  mobile  version  of  the  testbed  to  use  both 
indoors and outdoors in arbitrary locations where varied, dynamic 
and/or  combined  white  points  are  present  to  better  understand 
how colors perception is effected when users move about. 

This planned work, in conjunction with other work in the field, 
can be used to inform a predictive model of color blending.  Lon-
gitudinally,  we  hope  to  apply  this  predictive  model  to  adaptive 
AR systems that make real-time adjustments (based on the envi-
ronment) to the UI color presentation layer to preserve color en-
coding and designer intent.  

REFERENCES 

[1] 

J.  L.  Gabbard,  J.  Zedlitz,  J.  E.  Swan  II,  and  W.  W.  Winchester, 

"More Than Meets the Eye: An Engineering Study to Empirically 

Examine the Blending of Real and Virtual Color Spaces," in IEEE 

conference on Virtual Reality, Waltham, MA, 2010, pp. 79-86. 

[2]  A. Okur, S.-A. Ahmadi, A. Bigdelou, T. Wendler, and N. Navab, 

"MR in OR: First analysis of AR/VR visualization in 100 intra-op-

erative  Freehand  SPECT  acquisitions,"  in  Mixed  and  Augmented 

Reality  (ISMAR),  2011  10th  IEEE  International  Symposium  on, 

2011, pp. 211-218. 

[3]  P. Debenham, G. Thomas, and J. Trout, "Evolutionary augmented 

reality at the Natural History Museum," in Mixed and Augmented 

Reality  (ISMAR),  2011  10th  IEEE  International  Symposium  on, 

2011, pp. 249-250. 

[4]  M. A. Livingston, L. J. Rosenblum, S. J. Julier, D. Brown, Y. Bail-

lot, J. E. Swan II, et al., "An Augmented Reality System for Military 

Operations  in  Urban  Terrain,"  in Interservice  /  Industry  Training, 

Simulation, & Education Conference (I/ITSEC ’02), Orlando, Flor-

ida, USA, 2002. 

 
 
 
 
[5]  T. Gleue and P. Dähne, "Design and Implementation of a Mobile 

Device  for  Outdoor  Augmented  Reality  in  the  Archeoguide  Pro-

ject," presented at the Conference on Virtual Reality, Archeology, 

and Cultural Heritage, Glyfada, Greece, 2001. 

[6]  T. P. Caudell and D. W. Mizell, "Augmented Reality: An Applica-

tion  of  Heads-Up  Display  Technology  to  Manual  Manufacturing 

Processes," in 25th Hawaii International Conference on System Sci-

ences (HICSS), 1992, pp. 659–669. 

[7]  B. MacIntyre, A. Hill, H. Rouzati, M. Gandy, and B. Davidson, "The 

Argon AR Web Browser and standards-based AR application envi-

ronment,"  in  Mixed  and  Augmented  Reality  (ISMAR),  2011  10th 

IEEE International Symposium on, 2011, pp. 65-74. 

[8]  S. J. Kerr, M. D. Rice, Y. Teo, M. Wan, Y. L. Cheong, J. Ng, et al., 

"Wearable mobile augmented reality: evaluating outdoor user expe-

rience," presented at the Proceedings of the 10th International Con-

ference on Virtual Reality Continuum and Its Applications in Indus-

try, Hong Kong, China, 2011. 

[9]  D. Kalkofen, C. Sandor, S. White, and D. Schmalstieg, "Visualiza-

tion  Techniques  for  Augmented  Reality  Handbook  of  Augmented 

Reality," B. Furht, Ed., ed: Springer New York, 2011, pp. 65-98. 

[10]  J. L. Gabbard, J. E. Swan II, D. Hix, S. J. Kim, and G. Fitch, "Active 

Text Drawing Styles for Outdoor Augmented Reality: A User-Based 

Study and Design Implications," in IEEE conference on Virtual Re-

ality, Charlotte, North Carolina, USA, 2007, pp. 35-42. 

[11]  J. L. Gabbard, J. E. Swan II, and D. Hix, "The Effects of Text Draw-

ing Styles, Background Textures, and Natural Lighting on Text Leg-

ibility  in  Outdoor  Augmented  Reality,"  Presence:  Teleoperators 

and Virtual Environments, vol. 15, pp. 16-32, 2006. 

[12]  M. A. Livingston, J. H. Barrow, and C. M. Sibley, "Quantification 

of Contrast Sensitivity and Color Perception using Head-worn Aug-

mented  Reality  Displays,"  presented  at  the  IEEE  Virtual  Reality 

2009, Lafayette, LA, USA, 2009. 

[13]  S. D. Peterson, M. Axholt, and S. R. Ellis, "Label Segregation by 

Remapping  Stereoscopic  Depth  in  Far-field  Augmented  Reality," 

presented  at  the  Proceedings  of  the  7th  IEEE/ACM  International 

Symposium on Mixed and Augmented Reality, 2008. 

[14]  T. Pingel, and Clarke, K.C., "Assessing the Usability of a Wearable 

Computer for Outdoor Pedestrian Navigation," presented at the Au-

toCarto 2005, Las Vegas, NV, 2005. 

[15]  B. Thomas, B. Close, J. Donoghue, J. Squires, P. D. Bondi, and W. 

Piekarski, "First Person Indoor/Outdoor Augmented Reality Appli-

cation: ARQuake," Personal and Ubiquitous Computing, vol. 6, pp. 

75-86, 2002. 

[16]  T. Smith and J. Guild, "The C.I.E. colorimetric standards and their 

use," Transactions of the Optical Society, vol. 33, pp. 73–134, 1931. 

[17]  J. D. Foley, A. van Dam, S. K. Feiner, and J. F. Hughes, Computer 

Graphics: Principles and Practice in C, 2nd ed.: Addison-Wesley 

Professional, 1995. 

[18]  J. Foley, R. Phillips, J. Hughes, A. van Dam, and S. Feiner, Intro-

duction to Computer Graphics: Addison-Wesley Longman Publish-

ing Co., Inc., 1994. 

[19]  M. C. Stone, A Field Guide to Digital Color. Natick, Massachusetts: 

A K Peters, 2003. 

 
 
