2
2
0
2

g
u
A
7
1

]

V
C
.
s
c
[

3
v
3
0
1
0
1
.
2
1
1
2
:
v
i
X
r
a

SAGA: Stochastic Whole-Body
Grasping With Contact

Yan Wu∗1, Jiahao Wang∗2, Yan Zhang1, Siwei Zhang1, Otmar Hilliges1, Fisher
Yu1, Siyu Tang1

1 ETH Z¨urich, Switzerland
2 Max Planck Institute for Informatics, Germany
yan.wu@vision.ee.ethz.ch, jiwang@mpi-inf.mpg.de,
{yan.zhang,siwei.zhang,otmar.hilliges,siyu.tang}@inf.ethz.ch, i@yf.io

Abstract. The synthesis of human grasping has numerous applications
including AR/VR, video games and robotics. While methods have been
proposed to generate realistic hand–object interaction for object grasping
and manipulation, these typically only consider interacting hand alone.
Our goal is to synthesize whole-body grasping motions. Starting
from an arbitrary initial pose, we aim to generate diverse and natural
whole-body human motions to approach and grasp a target object in
3D space. This task is challenging as it requires modeling both whole-
body dynamics and dexterous finger movements. To this end, we propose
SAGA (StochAstic whole-body Grasping with contAct), a framework
which consists of two key components: (a) Static whole-body grasping
pose generation. Specifically, we propose a multi-task generative model,
to jointly learn static whole-body grasping poses and human-object con-
tacts. (b) Grasping motion infilling. Given an initial pose and the gener-
ated whole-body grasping pose as the start and end of the motion respec-
tively, we design a novel contact-aware generative motion infilling module
to generate a diverse set of grasp-oriented motions. We demonstrate the
effectiveness of our method, which is a novel generative framework to syn-
thesize realistic and expressive whole-body motions that approach and
grasp randomly placed unseen objects. Code and models are available at
https://jiahaoplus.github.io/SAGA/saga.html.

Keywords: motion generation, whole-body grasping synthesis, human-
object interaction.

1

Introduction

A fully automated system that synthesizes realistic 3D human bodies approach-
ing and grasping a target object in 3D space will be valuable in various fields,
from robotics and animation to computer vision. Although remarkable progress
has been made towards synthesizing realistic hand–object interactions, most ex-
isting works only focus on hand pose synthesis without considering whole-body

* Equal contribution.

 
 
 
 
 
 
2

Y. Wu∗, J. Wang∗ et al.

Fig. 1. Generated whole-body grasping motion sequences (in beige) starting from a
given pose (in white) to approach and grasp randomly placed unseen objects. For each
sample, we present hand motion details in the last few frames on the left column.

movements [23, 25, 55]. Meanwhile, whole-body motion synthesis [18, 20, 40, 65]
largely ignores the presence of objects in the scene.

Modeling and synthesizing realistic whole-body grasping motions are chal-
lenging and remain unsolved due to a number of reasons. Firstly, whole-body
grasping motions involve both full-body dynamics and dexterous finger move-
ments [55], while the high dimensional degrees of freedom make the synthesis
of grasping motions complicated. Secondly, a whole-body grasping sequence ex-
hibits complex and frequent body–scene and hand–object contacts which are
challenging to synthesize in a perceptually realistic way. For example, the hand’s
surface should conform naturally to the object and there should be no foot-
skating artifacts in the whole-body motion. Thirdly, given only a starting pose
and a target object in 3D space, there could be an infinite number of ways for
the person to approach and grasp the object. The diversity of plausible grasping
motions is further amplified by the large potential variation in object shape and
pose. How to build an effective generative model that can capture this diver-
sity and synthesize diverse realistic motions to grasp various 3D objects is an
unsolved question.

To address these challenges, we propose SAGA (StochAstic whole-body
Grasping with contAct), a novel whole-body grasping generation framework that
can synthesize stochastic motions of a 3D human body approaching and grasp-
ing 3D objects. Our solution consists of two components: (1) a novel 3D body
generator that synthesizes diverse static whole-body grasping end poses, and
(2) a novel human motion generator that creates diverse and plausible motions
between given start and end poses. We present two key insights on both compo-
nents. First, instead of directly using parametric body models (e.g. SMPL [42])
to represent 3D bodies, we employ the markers-based representation [65] which
captures 3D human shape and pose information with a set of sparse markers
on the human body surface. As demonstrated in [65], the markers-based rep-
resentation is easier for neural networks to learn than the latent parameters of
the parametric body models, yielding more realistic motion. We show that the
markers-based representation is especially advantageous to the latent body pa-

SAGA

3

rameters for learning whole-body grasping, as the accumulation of errors along
the kinematic chain has a significant impact on the physical plausibility of gen-
erated hand grasps, resulting in severe hand–object interpenetration. Second,
contact plays a central role in our pipeline. As the human moves in 3D space
and grasps a 3D object, physical contact is key for modeling realistic motions
and interactions. For both components of our method, we learn contact represen-
tations from data and use them to guide interaction synthesis, greatly improving
the realism of the generated motion.

For the static grasping pose generation, we built a multi-task conditional
variational autoencoder (CVAE) to jointly learn whole-body marker positions
and fine-grained marker–object contact labels. During inference, given a target
object in 3D space, our model jointly generates a diverse set of consistent full-
body marker locations and the contact labels on both the body markers and
the object surface. A contact-aware pose optimization module further recovers
a parametric body mesh from the predicted markers, while explicitly enforcing
the hand–object contact by leveraging the predicted contact labels. Next, given
the generated static whole-body grasping pose as the end pose, and an initial
pose as a start pose, we propose a novel generative motion infilling network
to capture motion uncertainty and generate diverse motions in between. We
design a CVAE-based architecture to generate both the diverse in-between mo-
tion trajectories and the diverse in-between local pose articulations. In addition,
contacts between feet and the ground are also predicted as a multi-task learning
objective to enforce a better foot-ground interaction. Furthermore, leveraging
the predicted human–object contacts, we design a contact-aware motion opti-
mization module to produce realistic grasp-oriented whole-body motions from
the generated marker sequences. By leveraging the GRAB [55] and AMASS [37]
datasets to learn our generative models, our method can successfully generate
realistic and diverse whole-body grasping motion sequences for approaching and
grasping a variety of 3D objects.
Contributions. In summary, we provide (1) a novel generative framework to
synthesize diverse and realistic whole-body motions approaching and grasping
various unseen objects for 3D humans that exhibit various body shapes, (2) a
novel multi-task learning model to jointly learn the static whole-body grasping
poses and the body–object interactions, (3) a novel generative motion infilling
model that can stochastically infill both the global trajectories and the local
pose articulations, yielding diverse and realistic full-body motions between a
start pose and end pose. We perform extensive experiments to validate technical
contributions. Experimental results demonstrate both the efficacy of our full
pipeline and the superiority of each component to existing solutions.

2 Related Work

Human Grasp Synthesis is a challenging task and has been studied in com-
puter graphics [24, 29, 32, 43, 47, 63] and robotics [10, 21, 28, 32, 35, 51]. With the
advancement in deep learning, recent works also approach the realistic 3D human

4

Y. Wu∗, J. Wang∗ et al.

grasp synthesis task by leveraging large-scale datasets [4, 23, 25, 55, 63], however
they only focus on hand grasp synthesis.

Grasping Field [25] proposes an implicit representation of hand–object in-
teraction and builds a model to generate plausible human grasps. GrabNet [55]
proposes a CVAE to directly sample the MANO [48] hand parameters, and
additionally train a neural network to refine the hand pose for a more plausi-
ble hand–object contact. GraspTTA [23] suggests using consistent hand–object
interactions to synthesize realistic hand grasping poses. It sequentially gener-
ates coarse hand grasping poses and estimates consistent object contact maps,
and using the estimated object contact maps, the produced hand pose is fur-
ther optimized for realistic hand–object interactions. Similarly, ContactOpt [12]
proposes an object contact map estimation network and a contact-based hand
pose optimization module to produce realistic hand–object interaction. Differ-
ent from GraspTTA and ContactOpt, which predict consistent hand pose and
hand–object contacts sequentially in two stages, we build a multi-task generative
model that generates consistent whole-body pose and the mutual human-object
contacts jointly to address a more complicated whole-body grasping pose learn-
ing problem. Going beyond the static grasp pose generation, provided the wrist
trajectory and object trajectory, ManipNet [63] generates dexterous finger mo-
tions to manipulate objects using an autoregressive model. Nonetheless, to our
best knowledge, none of the previous works studied 3D human whole-body grasp
learning and synthesis.

3D Human Motion Synthesis. In recent years, human motion prediction has
received a lot of attention in computer vision and computer graphics [5,9,11,19,
22, 33, 34, 39, 53, 58, 65, 66]. Existing motion prediction models can also be split
into two categories: deterministic [18, 20, 26, 40, 60] and stochastic [2, 6, 30, 61].
For deterministic motion prediction, [26] adopt convolutional models to provide
spatial or temporal consistent motions, and [40] propose an RNN with residual
connections and sampling-based loss to model human motion represented by
joints. For stochastic motion prediction, recently, Li et al. [30] and Cai et al. [6]
use VAE-based models to address general motion synthesis problems. While
these methods make great contributions to human motion understanding, they
do not study the interaction with the 3D environment.

There are several works predict human motion paths or poses in scene con-
text [1,7,13,14,16,17,31,38,46,49,50,52,56,57,59,64]. Cao et al. [7] estimate goals,
3D human paths, and pose sequences given 2D pose histories and an image of the
scene. However, the human is represented in skeletons, thus it is hard to accu-
rately model body–scene contacts, which limits its application. Recently, Wang
et al. [59] propose a pipeline to infill human motions in 3D scenes, which first syn-
thesizes sub-goal bodies, then fills in the motion between these sub-goals, then
refines the bodies. However, the generated motion appears unnatural especially
in the foot–ground contact. [16] presents an RNN-based network with contact
loss and adversarial losses to handle motion in-betweening problems. They use
the humanoid skeleton as the body representation and require 10 start frames
and one end frame as input. [46] adopts a conditional variational autoencoder

SAGA

5

to correct the pose at each timestamp to address noise and occlusions. They
also use a test-time optimization to get more plausible motions and human–
ground contacts. [64] propose a contact-aware motion infiller to generate the
motion of unobserved body parts. They predict motion with better foot–ground
contact, but their deterministic model does not capture the nature of human
motion diversity. Unlike the methods mentioned above, our generative motion
infilling model, when given the first and the last frame, captures both the global
trajectory diversity in between and the diversity of local body articulations.
Concurrent work. GOAL [54] builds a similar two-stage pipeline to approach
the whole-body grasping motion generation, producing end pose first and then
infilling the in-between motion. Unlike our work which captures both the diver-
sity of grasping ending poses and in-between motions, however, GOAL builds
a deterministic auto-regressive model to in-paint the in-between motion, which
does not fully explore the uncertainty of grasping motions.

3 Method

Preliminaries. (a) 3D human body representation. (1) SMPL-X [42] is a
parametric human body model which models body mesh with hand details. In
this work, the SMPL-X body parameters Θ include the shape parameters β ∈
R10, the body global translation t ∈ R3, the 6D continuous representation [67]
of the body rotation R ∈ R6, and full-body pose parameters θ = [θb, θh, θe],
where θb ∈ R32, θh ∈ R48, θe ∈ R6 are the body pose in the Vposer latent
space [42], the hands pose in the MANO [48] PCA space and the eyes pose,
respectively; (2) Markers-based representation [65] captures the body shape and
pose information with the 3D locations M ∈ RN ×3 of a set of sparse markers
on the body surface, where N is the number of markers. We learn the markers
representation in our neural networks, from which we further recover SMPL-X
body mesh. (b) 3D objects are represented with centered point cloud data O
and the objects height tO ∈ R1. We sample 2048 points on the object surface
and each point has 6 features (3 XYZ positions + 3 normal features).
Notations. For clarity, in the following text, ˜X and ˆX denote the CVAE recon-
struction result of X, and random samples of X from CVAE, respectively.

3.1 Overview

Given an initial human pose and a 3D object randomly placed in front of the
human within a reasonable range, our goal is to generate realistic and diverse
whole-body motions, starting from the given initial pose and approaching to
grasp the object. As presented in Fig. 2, we propose a two-stage grasping motion
generation pipeline to approach this task.
Stage 1: Stochastic whole-body grasping ending pose generation (§ 3.2).
We first build an object-conditioned multi-task CVAE which synthesizes whole-
body grasping ending poses in markers and the explicit human–object contacts.

6

Y. Wu∗, J. Wang∗ et al.

Fig. 2. Illustration of our two-stage pipeline. Given an object in 3D space and a start
pose, our method produces diverse human whole-body grasping motions. In stage 1, (a)
given 3D object information, our WholeGrasp-VAE (§ 3.2) decoder generates whole-
body grasping poses represented by markers and mutual marker–object contact proba-
bilities (green markers and red areas on the object surface indicate high contact proba-
bilities); (b) GraspPose-Opt (§ 3.2) further recovers body mesh from predicted markers.
We use the generated grasping pose as the targeted end pose. Then in stage 2, (c) we
feed in the start pose and the end pose into the MotionFill-VAE decoder (§ 3.3) to gen-
erate the in-between motions in marker representation, and (d) and GraspMotion-Opt
(§ 3.4) further recovers smooth and realistic whole-body grasping motions.

We further perform contact-aware pose optimization to produce 3D body meshes
with realistic interactions with objects by leveraging the contacts information.
Stage 2: Stochastic grasp-oriented motion infilling. We build a novel gen-
erative motion infilling model (§ 3.3) which takes the provided initial pose and
the generated end pose in stage 1 as inputs, and outputs diverse intermediate
motions. We further process the generated motions via a contact-aware opti-
mization step (§ 3.4) to produce realistic human whole-body grasping motions.

3.2 Whole-Body Grasping Pose Generation

To synthesize diverse whole-body poses to grasp a given object, we propose a
novel multi-task WholeGrasp-VAE to learn diverse yet consistent grasping poses
and mutual contacts between human and object. The explicit human–object
contacts provide fine-grained human–object interaction information which helps
to produce realistic body meshes with high-fidelity interactions with the object.
Model Architecture. We visualize the multi-task WholeGrasp-VAE design in
Fig. 3. The encoder takes the body markers’ positions M ∈ RN ×3, body markers
contacts CM ∈ {0, 1}N and object contacts CO ∈ {0, 1}2048 as inputs, where N
is the number of markers, and learns a joint Gaussian latent space zj. We use
PointNet++ [45] to encode the object feature.
Training. The overall training objective is given by Ltrain = Lrec + λKLLKL +
λcLc, where λKL, λc are hyper-parameters.

Reconstruction loss includes the L1 reconstruction loss of body markers’

postions and the binary cross-entropy (BCE) loss of contact probabilities:

Lrec = |M − ˜M | + λM Lbce(CM , ˜CM ) + λOLbce(CO, ˜CO).

(1)

KL-divergence loss. We employ the robust KL-divergence term [65] to

avoid the VAE posterior collapse:

LKL = Ψ (DKL(q(zj|M , CM , CO, tO, O)||N (0, I))),

(2)

Object in 3D spaceInitial body pose WholeGrasp-VAEDecoderStage1: Stochastic Grasping Pose GenerationStage2: Stochastic Grasping Motion InﬁllingMotionFill-VAE        DecoderPipeline Input (a)(c)  (d)Stage2 Input (b)  GraspPose-OptGraspMotion-Opt   Start & End posesSAGA

7

Fig. 3. The WholeGrasp-VAE design. WholeGrasp-VAE jointly learns the (1) body
marker locations; (2) body marker contacts (markers with high contact probability are
shown in green); (3) object contact map (the area with high contact probability is
shown in red). The red arrow indicates sampling from the latent space. At inference
time, activated modules are shown in orange.

√

where Ψ (s) =
to update the above KLD term, when the KL-divergence is small.

s2 + 1−1 [8]. This function automatically penalizes the gradient

Consistency loss. We use a consistency loss to implicitly encourage consis-

tent predictions of marker positions and mutual marker–object contacts:

Lc =

(cid:88)

˜Cm|d( ˜m, O) − d(m, O)| +

m∈M , ˜m∈ ˜M

˜Co|d(o, ˜M ) − d(o, M )|,

(3)

(cid:88)

o∈O

||x − y||2

2 is the minimum distance from point x to point cloud Y.

d(x, Y) = min
y∈Y
Inference. During inference, we feed the provided target object information
into the WholeGrasp-VAE decoder to generate plausible body markers ˆM and
marker–object contact labels ˆCM , ˆCO. We design a contact-aware pose optimiza-
tion algorithm, GraspPose-Opt, to generate a realistic body mesh from markers
and refine body pose for high-fidelity human–object interaction by leveraging the
fine-grained human–object contacts. Specifically, by optimizing SMPL-X param-
eters Θ, the overall optimization objective is given by:

Eopt(Θ) = Ef it + Eo

colli + Eo

cont + Eg

cont.

(4)

Marker fitting loss. To project the predicted markers to a valid body mesh,
we minimize the L1 distance between the sampled markers ˆM and the queried
markers M (Θ) on the SMPL-X body mesh:

Ef it(Θ) = | ˆM − M (Θ)| + αθ|θ|2,

(5)

where αθ is the pose parameters regularization weight.

Object contact loss. By leveraging sampled contact maps, we propose
a mutual contact loss to encourage body markers and object points with high
contact probabilities to contact the object surface and body surface, respectively.

Eo

cont(Θ) = αo

cont

(cid:88)

o∈O

ˆCod(o, VB(Θ)) + αm

cont

(cid:88)

ˆCmd(m, O).

(6)

m∈M (Θ)

PointNet++EncResBlockDecPointNet++SigmoidSigmoidObject point cloud Object contact mapObject translationBody markers with contact mapPointNet++Generated object contact mapGenerated body markers with contact mapResBlockResBlockcccc concatenation8

Y. Wu∗, J. Wang∗ et al.

Fig. 4. MotionFill-VAE consists of two concatenated CVAEs: (1) TrajFill outputs the
infilled global root trajectory when the start root and the end root are given; (2)
LocalMotionFill takes the global trajectory information from TrajFill as one of the
inputs, and it outputs the infilled local motion when the start pose, the end pose, and
the global trajectory are given. We reconstruct the global motion from the generated
global trajectory and the local motion. The red arrow indicates sampling from the
latent space. The dash arrow indicates the input processing step for building the four-
channel motion image (one local motion channel with contact states and three root
velocity channels). At inference time, activated modules are shown in orange.

where VB(Θ) denotes the SMPL-X body vertices.

Collision loss. We employ a signed-distance based collision loss to penalize

the body–object interpenetration:

Ecolli(Θ) = αB

colli

(cid:88)

b∈V h

B (Θ)

max(−S(b, O), σb) + αO

colli

(cid:88)

o∈O

max(−S(o, V h

B(Θ)), σo) (7)

where S(x, Y) is the signed-distance from point x to point cloud Y, V h
denotes the hand vertices, and σb, σo are small interpenetration thresholds.

B(Θ)

Ground contact loss is given by Eg

penalize the heights of feet vertices V f

cont(Θ) = αcont

|h(v)|, where we
B to enforce a plausible foot–ground contact.

v∈V f
B

(cid:80)

3.3 Generative Motion Infilling

Given body markers on the start and end poses produced by GraspPose-Opt, i.e.,
M0 and MT , many in-between motions are plausible. To model such uncertainty,
we build a novel generative motion infilling model, namely MotionFill-VAE, to
capture both the uncertainties of intermediate global root (pelvis joint) trajec-
tories and intermediate root-related local body poses. Specifically, given motion
M0:T represented in a sequence of markers positions, following [20, 26, 64], we
represent the global motion M0:T with a hierarchical combination of global root
velocity v0:T (where vt = Γt+1 − Γt, t ∈ [0, T ], Γ and v denote the root trajec-
tory and root velocity respectively) and the trajectory-conditioned local motion
M l
0:T . Accordingly, we build the MotionFill-VAE to capture both the conditional

.........γγγγγγSAGA

9

T ).

0, M l

0:T |v0:T , M l

global trajectory distribution P (Γ0:T +1|Γ0, ΓT ) and the conditional local motion
distribution P (M l
Model Architecture. As shown in Fig. 4, the MotionFill-VAE consists of two
concatenated CVAEs: (1) TrajFill learns the conditional intermediate global
root trajectory latent space zt. Taking the root states Γ0 and ΓT as inputs,
which are derived from the given start and end pose, our goal is to get the tra-
jectory Γ0:T +1. Instead of directly learning Γ0:T +1, we build TrajFill to learn
the trajectory deviation ∆Γ0:T +1 = Γ0:T +1 − Γ 0:T +1, where Γ 0:T +1 is a straight
trajectory which is a linear interpolation and one-step extrapolation of the given
Γ0 and ΓT . We further compute the velocity v0:T from the predicted trajectory
Γ0:T +1. (2) LocalMotionFill learns the conditional intermediate local motion
latent space zm. Taking the TrajFill output v0:T and the given M0, MT as
inputs, LocalMotionFill generates the trajectory-conditioned local motion se-
quence. Specifically, following [64], we build a four-channel image I, which is a
concatenation of local motion information with foot–ground contact labels and
root velocity, and we use it as the input to our CNN-based LocalMotionFill ar-
chitecture. Similarly, we build the four-channel conditional image Ic with the
unknown motion in between filled with all 0.
Training. The training loss is LM = Lrec + λKLLKL, and λKL is hyper-parameter.
Reconstruction loss Lrec contains the global trajectory reconstruction,
local motion reconstruction and foot–ground contact label reconstruction losses:

Lrec =

T +1
(cid:88)

t=0

|Γt − ˜Γt| + λ1

T
(cid:88)

t=0

|vΓ

t − ˜vΓ

t | + λ2Lbce(CF , ˜CF )

+ λ3

T
(cid:88)

t=0

|M l

t − ˜M l

t | + λ4

T −1
(cid:88)

t=0

|vM l

t − ˜vM l

t

|,

(8)

where v(∗)

t = (∗)t+1 − (∗)t denotes the velocity, and λ1−λ4 are hyper-parameters.
KL-divergence loss. We use the robust KL-divergence loss for both TrajFill

and LocalMotionFill:

LKL = Ψ (DKL(q(zt|Γ0:T +1, Γ 0:T +1)||N (0, I))) + Ψ (DKL(q(zm|I, Ic)||N (0, I))). (9)

Inference. At inference time, given the start and end body markers M0, MT
with known root states Γ0, ΓT , by first feeding the initial interpolated trajectory
Γ 0:T +1 into the decoder of TrajFill, we generate stochastic in-between global mo-
tion trajectory ˆΓ0:T +1. Next, with the given M0, MT and the generated ˆΓ0:T +1,
we further build the condition input image Ic as the input to the LocalMotion-
Fill decoder, from which we can generate infilled local motion sequences ˆM l
and also the foot–ground contact probabilities ˆCF0:T . Finally, we reconstruct the
global motion sequences ˆM0:T from the generated ˆΓ0:T and ˆM l

0:T

0:T .

3.4 Contact-aware Grasping Motion Optimization
With the generated marker sequences ˆM0:T , foot–ground contacts ˆCF from
MotionFill-VAE, and the human–object contacts ˆCM , ˆCO from WholeGrasp-

10

Y. Wu∗, J. Wang∗ et al.

VAE, we design GraspMotion-Opt, a contact-aware motion optimization algo-
rithm, to recover smooth motions B0:T with natural interactions with the scene.
Similar to GraspPose-opt, we propose the contact-aware marker fitting loss:

Ebasic(Θ0:T ) =

T
(cid:88)

t=0

(Ef it(Θt) + Eo

colli(Θt)) +

T
(cid:88)

t=T −4

Eo

cont(Θt),

(10)

where Ef it, Eo
contact loss Eo

cont, Eo
colli on the last 5 frames.

colli are formulated in Eq. 5-7, and we only apply object

We design the following loss to encourage a natural hand grasping motion

by encouraging the palm to face the object’s surface on approach.

Eg(Θ0:T ) =

T
(cid:88)

t=0

αt

(cid:88)

m∈V p

B (Θt)

1(d(m, O) < σ)(cos γm − 1)2,

(11)

T )2, V p

where αt = 1 − ( t
B(Θ) denotes the selected vertices on palm, and γm is
the angle between the palm normal vector and the vector from palm vertices
to the closest object surface points. We only apply this constraint when palm
vertices are close to the object’s surface (within radius σ = 1cm).

Inspired by [64], we enforce smoothness on the motion latent space to yield
smoother motion, and we also reduce the foot skating artifacts by leveraging the
foot–ground contact labels ˆCF . For more details, please refer to the Appendix.

4 Experiments

Datasets. (1) We use GRAB [55] dataset to train and evaluate our WholeGrasp-
VAE and also finetune the MotionFill-VAE. For WholeGrasp-VAE training and
evaluation, following [55], we take all frames with right-hand grasps and have
the same train/valid/test set split. For MotionFill-VAE training, we downsample
the motion sequences to 30fps and clip 62 frames per sequence, with last frames
being in stable grasping poses. (2) We use the AMASS [37] dataset to pretrain
our LocalMotionFill-CVAE. We down-sample the sequences to 30 fps and cut
them into clips with 61 frames. (3) We take unseen objects from HO3D [15]
dataset to test the generalization ability of our method.

We conduct extensive experiments to study the effectiveness of each stage in
our pipeline. In Sec. 4.1 and Sec. 4.2, we study our static grasping pose generator
and the stochastic motion infilling model respectively. In Sec. 4.3, we evaluate
the entire pipeline performance for synthesizing stochastic grasping motions. We
encourage readers to watch the video of generated grasping poses and motions.

4.1 Stochastic Whole-body Grasp Pose Synthesis

We evaluate our proposed stochastic whole-body grasp pose generation module
on GRAB dataset. We also conduct ablation studies to study the effectiveness

SAGA

11

Table 1. Comparisons with the extended GrabNet baseline and ablation study re-
sult on the multi-task WholeGrasp-VAE design. Numbers in bold/blue indicates the
best/second-best respectively.

Method

GrabNet [55]-SMPLX

WholeGrasp-single w/o opt.∗
WholeGrasp-single w/ heuristic opt.

WholeGrasp w/o opt.∗
WholeGrasp w/ opt. (Ours)

APD Contact Ratio Inter. Vol. Inter. Depth

(↑)

0.33

2.94

2.92

(↑)

0.65

0.90
0.81

0.96
0.94

[cm3] (↓)

[cm] (↓)

14.15

11.44
0.21

12.20
0.48

0.78

0.78
0.12

0.85
0.16

∗ Body meshes are recovered from sampled markers with only Ef it in Eq. 5.

of several proposed components, including the multi-task CVAE design and the
contact-aware optimization module design.

Baseline. GrabNet [55] builds a CVAE to generate the MANO hand parameters
for grasping a given object, and we extend GrabNet to whole-body grasp synthe-
sis by learning the whole-body SMPL-X parameters. We compare our method
against the extended GrabNet (named as GrabNet-SMPLX)*.
Evaluation Metrics.(1) Contact Ratio. To evaluate the grasp stability, we
measure the ratio of body meshes being in minimal contact with object meshes.
(2) Interpenetration Volume and Depth. We measure the interpenetration vol-
umes and depths between the body and object mesh. Low interpenetration vol-
ume and depth with a high contact ratio are desirable for perceptually realistic
body–object interactions. (3) Diversity. We follow [62] to employ the Average
L2 Pairwise Distance (APD) to evaluate the diversity within random samples.

Results. In Table 1, we compare our method against the extended GrabNet
baseline. Because the extended GrabNet baseline does not include an additional
body mesh refinement step, we compare it to our results without GraspPose-Opt
optimization (WholeGrasp w/o. opt. in Table 1). Our method w/o optimization
outperforms the extended GrabNet baseline in the sample diversity (APD) and
achieves higher contact ratio and smaller intersection. The extended GrabNet
experiment demonstrates the challenges in learning the whole-body pose param-
eters for a plausible human–object interaction, with marker representation ap-
pearing to be more favorable for learning human grasping pose. Nevertheless, the
derived body meshes from markers without pose optimization still have human–
object interpenetration, and our contact-aware pose optimization (WholeGrasp
w/ opt. in Table 1) drastically reduces the human–object collision issue while
maintaining a high contact ratio.

Fig. 5 presents 5 random samples together with the generated object contact
maps and hand grasping details for an unseen object. We can see that our models
generate natural grasping poses with diverse body shapes and whole-body poses.

* Please refer to the Appendix for experimental setup and implementation details.

12

Y. Wu∗, J. Wang∗ et al.

Table 2. Ablation studies on different optimization losses (Ef it, Ecolli, Eo
cont in Eq. 5-
Eq. 6). We fit ground truth markers (GT columns) and sampled markers (Samples
columns), and numbers in bold/blue indicate the best/second-best respectively.

Contact Ratio(↑) Inter. Vol.(↓) Inter. Depth(↓)
GT Samples GT Samples
GT Samples

GT Mesh

0.99

-

2.04

-

Ef it + Eg
Ef it + Eg
Ef it + Eg

cont
cont + Ecolli
cont + Ecolli + Eo

0.99
0.25
cont 0.94

0.96
0.24
0.94

2.21
12.20
0.12 0.12
0.48
0.52

0.45

0.46
0.04
0.17

-

0.85
0.07
0.16

Fig. 5. Five random samples for an unseen object placed at the same positions. Left
side: top view and front view of generated whole-body poses. Right side: hand grasping
details and generated object contact maps (red areas indicate high contact probability).

Ablation Study. (1) Multi-task WholeGrasp design: To study the effect of
learning human–object contact labels, we build a single-task WholeGrasp-VAE
architecture which only learns the markers’ positions (WholeGrasp-single in Ta-
ble 1). A similar pose optimization step as our GraspPose-opt further refines the
grasping pose (WholeGrasp-single w/ heuristic opt. in Table 1), but we replace
the mutual contact loss Econt in Eq. 6 with a heuristic contact loss which is
based on a pre-defined hand contact pattern. Both the single-task and multi-
task WholeGrasp experiments demonstrate the benefit of using contact to re-
fine the human–object interaction, and our multi-task WholeGrasp with explicit
mutual contact learning outperforms the single-task setup with the pre-defined
hand contact pattern. (2) Study of GraspPose-Opt (see Table 2): We evaluate
recovered body meshes from both the ground truth markers and the randomly
sampled markers, and also the ground truth body mesh. By fitting the body
mesh to ground truth markers, our proposed GraspPose-Opt with only Ef it can
recover an accurate body mesh with the human–object interaction metrics com-
parable to the ground truth mesh. The proposed Ecolli and Econt help to recover
realistic human–object interaction significantly.

4.2 Stochastic Motion Infilling

We evaluate our motion infilling model on AMASS and GRAB datasets. To
our best knowledge, we are the first generative model to learn both the global

HandContactHand + ContactSAGA

13

Table 3. Comparisons with motion infilling baselines. Best results are in boldface.

Local motion
infilling∗

Methods

CNN-AE [26]
LEMO [64]
PoseNet [59]
Ours-Local†

Traj + local

Route+PoseNet [59]

motion infilling Ours†

ADE
(↓)

0.091
0.083
0.090
0.079

0.219
0.083

Skat
(↓)

0.245
0.152
0.236
0.137

0.575
0.394

PSKL-J (↓)

(P, GT)

(GT, P)

0.804
0.507
0.611
0.377

0.955
0.772

0.739
0.447
0.668
0.327

0.884
0.609

∗ Ground truth trajectories are used in the local motion infilling experiments.
† Generative models. And all the other methods are deterministic models.

trajectory and the local motion infilling given only one start pose and end pose.
We compare our method with several representative motion infilling models.
Baselines. Wang et al. [59] proposed two sequential yet separate LSTM-based
deterministic networks to first predict global trajectory (RouteNet) and then
the local pose articulations (PoseNet) to approach the motion infilling task, and
we take this sequential network (named as Route+PoseNet) as a baseline to
our end-to-end generative global motion infilling model. There are some existing
works which take the ground truth trajectory, start pose and end poses as inputs
to predict the intermediate local poses, and following the same task setup, we
also compare the generative local motion infilling component in our network
against these baselines, including the convolution autoencoder network (CNN-
AE) in [26], LEMO [64] and PoseNet [59]. We have chosen these baselines as they
are the closest ones compared with our setting. For fair comparisons, we use the
same body markers and the trajectory representation in all experiments*.
Evaluation Metrics. (1) 3D marker accuracy. For deterministic models, we
measure the marker prediction accuracy by computing the Average L2 Distance
Error (ADE) between the predicted markers and ground truth. For our gener-
ative model, we follow [62] to measure the sample accuracy by computing the
minimal error between the ground truth and 10 random samples.

(2) Motion smoothness. We follow [64] to use PSKL-J to measure the Power
Spectrum KL divergence between the acceleration distribution of synthesized
and ground truth joint motion sequences. PSKL-J being non-symmetric, we
show the results of both direction, i.e., (Predicted, Ground Truth) and (Ground
Truth, Predicted). (3) Foot skating. Following [65], we measure the foot skating
artifacts during motion and define skating as when the heel is within 5cm of
the ground and the heel speed of both feet exceeds 75mm/s. (4) foot–ground
collision. We also use a non-collision score, defined as the number of body mesh
vertices above the ground divided by the total number of vertices.
Results. In Table 3, we compare our generative motion infilling model with
the deterministic Route+PoseNet baseline [59], and both methods can infill the
global trajectory and local pose motion. The results show that our generative
model can yield much lower average 3D marker distance error (ADE). Also, our

14

Y. Wu∗, J. Wang∗ et al.

method has less foot skating and lower PSKL-J scores in both directions, which
demonstrates that our method can generate more natural motions. We also com-
pare our stochastic local motion infilling component (Ours-Local) against other
deterministic local motion infilling baselines in Table 3. Our method outperforms
all the other baselines in ADE, foot skating and PSKL-J, demonstrating that the
our generative model can better capture human motion patterns and generate
more natural motions. The motion sequences from the GRAB dataset and our
generated motions have non-collision score of 0.9771 and 0.9743, respectively,
showing that our method can effectively prevent foot–ground interpenetration.

4.3 Whole-body Grasp Motion Synthesis

Experiment setup. We test our grasping motion generation pipeline on 14
unseen objects from GRAB and HO3D dataset, and we generate 2s motions to
grasp the object. Given different initial human poses, we place objects in front
of the human at different heights (0.5m–1.7m) with various orientations (0–360◦
around the gravity axis) and different distances from start point to objects (5cm–
1.1m). We conduct user studies on Amazon Mechanical Turk (AMT) for both
ground truth grasping motion sequences from GRAB and our generated samples.
On a five-point scale, three users are asked to rate the realism of presented
motions, ranging from strongly disagree (score 0) to strongly agree (score 5)*.
Results. The perceptual scores for ground truth sequences and our synthesized
sequences are 4.04 (around agree) and 3.15 (above slightly agree) respectively,
showing that our proposed pipeline can synthesize high-fidelity grasping motions.

5 Conclusion and Discussion

In this work, we address an important task on how to synthesize realistic whole-
body grasping motion. We propose a new approach consisting of two stages:
(a) a WholeGrasp-VAE to generate static whole-body grasping poses; (b) a
MotionFill-VAE to infill the grasp-oriented motion, given an initial pose and
the predicted end pose. Our method, SAGA, is able to generate diverse motion
sequences that have realistic interactions with the ground and random objects.
We believe SAGA makes progress towards synthesizing human–object interac-
tion, and provides a useful tool for computer graphics and robotics applications.
However, in this work, we focus on the human motion synthesis task where a
virtual human approaches to grasp an object without further hand–object ma-
nipulation. A future work is to synthesize the hand–object manipulation, while
taking the object affordance, physics and the goal of the interaction into account.
Acknowledgement. This work was supported by the SNF grant 200021 204840
and Microsoft Mixed Reality & AI Zurich Lab PhD scholarship. We also thank
Omid Taheri and Dimitrios Tzionas for helpful discussions.

SAGA

15

References

1. Alahi, A., Ramanathan, V., Fei-Fei, L.: Socially-aware large-scale crowd forecast-
ing. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition. pp. 2203–2210 (2014) 4

2. Barsoum, E., Kender, J., Liu, Z.: Hp-gan: Probabilistic 3d human motion pre-
diction via gan. In: Proceedings of the IEEE conference on computer vision and
pattern recognition workshops. pp. 1418–1427 (2018) 4

3. Brahmbhatt, S., Ham, C., Kemp, C.C., Hays, J.: ContactDB: Analyzing and pre-
dicting grasp contact via thermal imaging. In: The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) (2019) 22

4. Brahmbhatt, S., Handa, A., Hays, J., Fox, D.: ContactGrasp: Functional Multi-
finger Grasp Synthesis from Contact. In: 2019 IEEE/RSJ International Conference
on Intelligent Robots and Systems (IROS) (2019) 4

5. Cai, Y., Huang, L., Wang, Y., Cham, T.J., Cai, J., Yuan, J., Liu, J., Yang, X.,
Zhu, Y., Shen, X., et al.: Learning progressive joint propagation for human motion
prediction. In: European Conference on Computer Vision. pp. 226–242. Springer
(2020) 4

6. Cai, Y., Wang, Y., Zhu, Y., Cham, T.J., Cai, J., Yuan, J., Liu, J., Zheng, C.,
Yan, S., Ding, H., et al.: A unified 3d human motion synthesis model via condi-
tional variational auto-encoder. In: Proceedings of the IEEE/CVF International
Conference on Computer Vision. pp. 11645–11655 (2021) 4

7. Cao, Z., Gao, H., Mangalam, K., Cai, Q.Z., Vo, M., Malik, J.: Long-term human
motion prediction with scene context. In: European Conference on Computer Vi-
sion. pp. 387–404. Springer (2020) 4

8. Charbonnier, P., Blanc-Feraud, L., Aubert, G., Barlaud, M.: Two deterministic
half-quadratic regularization algorithms for computed imaging. In: Proceedings of
1st International Conference on Image Processing. vol. 2, pp. 168–172 vol.2 (1994)
7

9. Chiu, H.k., Adeli, E., Wang, B., Huang, D.A., Niebles, J.C.: Action-agnostic human
pose forecasting. In: 2019 IEEE Winter Conference on Applications of Computer
Vision (WACV). pp. 1423–1432. IEEE (2019) 4

10. Detry, R., Kraft, D., Buch, A.G., Kr¨uger, N., Piater, J.: Refining grasp affordance
models by experience. In: 2010 IEEE International Conference on Robotics and
Automation. pp. 2287–2293 (2010) 3

11. Fragkiadaki, K., Levine, S., Felsen, P., Malik, J.: Recurrent network models for hu-
man dynamics. In: Proceedings of the IEEE International Conference on Computer
Vision. pp. 4346–4354 (2015) 4

12. Grady, P., Tang, C., Twigg, C.D., Vo, M., Brahmbhatt, S., Kemp, C.C.: Contac-
tOpt: Optimizing contact to improve grasps. In: Conference on Computer Vision
and Pattern Recognition (CVPR) (2021) 4

13. Gupta, A., Satkin, S., Efros, A.A., Hebert, M.: From 3d scene geometry to human

workspace. In: CVPR 2011. pp. 1961–1968. IEEE (2011) 4

14. Gupta, A., Johnson, J., Fei-Fei, L., Savarese, S., Alahi, A.: Social gan: Socially
acceptable trajectories with generative adversarial networks. In: Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition. pp. 2255–
2264 (2018) 4

15. Hampali, S., Rad, M., Oberweger, M., Lepetit, V.: Honnotate: A method for 3d
annotation of hand and object poses. In: Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition. pp. 3196–3206 (2020) 10, 25

16

Y. Wu∗, J. Wang∗ et al.

16. Harvey, F.G., Yurick, M., Nowrouzezahrai, D., Pal, C.: Robust motion in-

betweening. ACM Transactions on Graphics (TOG) 39(4), 60–1 (2020) 4

17. Helbing, D., Molnar, P.: Social force model for pedestrian dynamics. Physical re-

view E 51(5), 4282 (1995) 4

18. Hernandez, A., Gall, J., Moreno-Noguer, F.: Human motion prediction via spatio-
temporal inpainting. In: Proceedings of the IEEE/CVF International Conference
on Computer Vision. pp. 7134–7143 (2019) 2, 4

19. Holden, D., Komura, T., Saito, J.: Phase-functioned neural networks for character

control. ACM Transactions on Graphics (TOG) 36(4), 1–13 (2017) 4

20. Holden, D., Saito, J., Komura, T.: A deep learning framework for character motion
synthesis and editing. ACM Transactions on Graphics (TOG) 35(4), 1–11 (2016)
2, 4, 8

21. Hsiao, K., Lozano-Perez, T.: Imitation learning of whole-body grasps. In: 2006
IEEE/RSJ international conference on intelligent robots and systems. pp. 5657–
5662. IEEE (2006) 3

22. Jain, A., Zamir, A.R., Savarese, S., Saxena, A.: Structural-rnn: Deep learning on
spatio-temporal graphs. In: Proceedings of the ieee conference on computer vision
and pattern recognition. pp. 5308–5317 (2016) 4

23. Jiang, H., Liu, S., Wang, J., Wang, X.: Hand-object contact consistency reasoning
for human grasps generation. In: Proceedings of the International Conference on
Computer Vision (2021) 2, 4

24. Kalisiak, M., Van de Panne, M.: A grasp-based motion planning algorithm for
character animation. The Journal of Visualization and Computer Animation 12(3),
117–129 (2001) 3

25. Karunratanakul, K., Yang, J., Zhang, Y., Black, M., Muandet, K., Tang, S.: Grasp-
ing field: Learning implicit representations for human grasps. In: 8th International
Conference on 3D Vision. pp. 333–344. IEEE (Nov 2020) 2, 4

26. Kaufmann, M., Aksan, E., Song, J., Pece, F., Ziegler, R., Hilliges, O.: Convolutional
autoencoders for human motion infilling. In: 2020 International Conference on 3D
Vision (3DV). pp. 918–927. IEEE (2020) 4, 8, 13, 27

27. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980 (2014) 24

28. Krug, R., Dimitrov, D., Charusta, K., Iliev, B.: On the efficient computation of
independent contact regions for force closure grasps. In: 2010 IEEE/RSJ Interna-
tional Conference on Intelligent Robots and Systems. pp. 586–591 (2010) 3
29. Kry, P.G., Pai, D.K.: Interaction capture and synthesis. ACM Trans. Graph. 25(3),

872–880 (Jul 2006) 3

30. Li, J., Villegas, R., Ceylan, D., Yang, J., Kuang, Z., Li, H., Zhao, Y.: Task-generic
hierarchical human motion prior using vaes. In: 2021 International Conference on
3D Vision (3DV). pp. 771–781. IEEE (2021) 4

31. Li, X., Liu, S., Kim, K., Wang, X., Yang, M.H., Kautz, J.: Putting humans in a
scene: Learning affordance in 3d indoor environments. In: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition. pp. 12368–12376 (2019)
4

32. Li, Y., Fu, J.L., Pollard, N.S.: Data-driven grasp synthesis using shape match-
ing and task-based pruning. IEEE Transactions on Visualization and Computer
Graphics 13(4), 732–747 (2007) 3

33. Ling, H.Y., Zinno, F., Cheng, G., Van De Panne, M.: Character controllers using

motion vaes. ACM Transactions on Graphics (TOG) 39(4), 40–1 (2020) 4

SAGA

17

34. Liu, L., Hodgins, J.: Learning basketball dribbling skills using trajectory opti-
mization and deep reinforcement learning. ACM Transactions on Graphics (TOG)
37(4), 1–14 (2018) 4

35. Liu, M., Pan, Z., Xu, K., Ganguly, K., Manocha, D.: Generating grasp poses for
a high-dof gripper using neural networks. In: 2019 IEEE/RSJ International Con-
ference on Intelligent Robots and Systems (IROS). pp. 1518–1525. IEEE (2019)
3

36. Lucas, J., Tucker, G., Grosse, R.B., Norouzi, M.: Understanding posterior collapse

in generative latent variable models. In: DGS@ICLR (2019) 23

37. Mahmood, N., Ghorbani, N., Troje, N.F., Pons-Moll, G., Black, M.J.: AMASS:
Archive of motion capture as surface shapes. In: International Conference on Com-
puter Vision. pp. 5442–5451 (Oct 2019) 3, 10, 30

38. Makansi, O., Ilg, E., Cicek, O., Brox, T.: Overcoming limitations of mixture density
networks: A sampling and fitting framework for multimodal future prediction. In:
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.
pp. 7144–7153 (2019) 4

39. Mao, W., Liu, M., Salzmann, M., Li, H.: Learning trajectory dependencies for
human motion prediction. In: Proceedings of the IEEE International Conference
on Computer Vision. pp. 9489–9497 (2019) 4

40. Martinez, J., Black, M.J., Romero, J.: On human motion prediction using recurrent
neural networks. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition. pp. 2891–2900 (2017) 2, 4

41. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T.,
Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z.,
Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., Chintala, S.:
Pytorch: An imperative style, high-performance deep learning library. In: Wallach,
H., Larochelle, H., Beygelzimer, A., d'Alch´e-Buc, F., Fox, E., Garnett, R. (eds.)
Advances in Neural Information Processing Systems 32, pp. 8024–8035. Curran
Associates, Inc. (2019) 23

42. Pavlakos, G., Choutas, V., Ghorbani, N., Bolkart, T., Osman, A.A.A., Tzionas,
D., Black, M.J.: Expressive body capture: 3D hands, face, and body from a single
image. In: Proceedings IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR). pp. 10975–10985 (2019) 2, 5

43. Pollard, N.S., Zordan, V.B.: Physically based grasping control from example. In:
Proceedings of the 2005 ACM SIGGRAPH/Eurographics symposium on Computer
animation. pp. 311–318 (2005) 3

44. Prokudin, S., Lassner, C., Romero, J.: Efficient learning on point clouds with basis
point sets. In: Proceedings of the IEEE/CVF International Conference on Com-
puter Vision (ICCV) (October 2019) 26

45. Qi, C.R., Yi, L., Su, H., Guibas, L.J.: Pointnet++: Deep hierarchical feature learn-
ing on point sets in a metric space. Advances in Neural Information Processing
Systems 30 (2017) 6, 20, 21, 26

46. Rempe, D., Birdal, T., Hertzmann, A., Yang, J., Sridhar, S., Guibas, L.J.: Hu-
mor: 3d human motion model for robust pose estimation. In: Proceedings of the
IEEE/CVF International Conference on Computer Vision. pp. 11488–11499 (2021)
4

47. Rijpkema, H., Girard, M.: Computer animation of knowledge-based human grasp-

ing. ACM Siggraph Computer Graphics 25(4), 339–348 (1991) 3

48. Romero, J., Tzionas, D., Black, M.J.: Embodied hands: Modeling and capturing
hands and bodies together. ACM Transactions on Graphics, (Proc. SIGGRAPH
Asia) 36(6) (Nov 2017) 4, 5

18

Y. Wu∗, J. Wang∗ et al.

49. Sadeghian, A., Kosaraju, V., Sadeghian, A., Hirose, N., Rezatofighi, H., Savarese,
S.: Sophie: An attentive gan for predicting paths compliant to social and physi-
cal constraints. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition. pp. 1349–1358 (2019) 4

50. Savva, M., Chang, A.X., Hanrahan, P., Fisher, M., Nießner, M.: Pigraphs: learning
interaction snapshots from observations. ACM Transactions on Graphics (TOG)
35(4), 1–12 (2016) 4

51. Seo, J., Kim, S., Kumar, V.: Planar, bimanual, whole-arm grasping. In: 2012 IEEE
International Conference on Robotics and Automation. pp. 3271–3277 (2012) 3
52. Starke, S., Zhang, H., Komura, T., Saito, J.: Neural state machine for character-

scene interactions. ACM Trans. Graph. 38(6), 209–1 (2019) 4

53. Starke, S., Zhao, Y., Komura, T., Zaman, K.: Local motion phases for learning
multi-contact character movements. ACM Transactions on Graphics (TOG) 39(4),
54–1 (2020) 4

54. Taheri, O., Choutas, V., Black, M.J., Tzionas, D.: Goal: Generating 4d whole-body
motion for hand-object grasping. arXiv preprint arXiv:2112.11454 (2021) 5
55. Taheri, O., Ghorbani, N., Black, M.J., Tzionas, D.: GRAB: A dataset of whole-
body human grasping of objects. In: European Conference on Computer Vision
(ECCV) (2020) 2, 3, 4, 10, 11, 20, 22, 26, 28, 29

56. Tai, L., Zhang, J., Liu, M., Burgard, W.: Socially compliant navigation through
raw depth inputs with generative adversarial imitation learning. In: 2018 IEEE
International Conference on Robotics and Automation (ICRA). pp. 1111–1117.
IEEE (2018) 4

57. Tan, F., Bernier, C., Cohen, B., Ordonez, V., Barnes, C.: Where and who? auto-
matic semantic-aware person composition. In: 2018 IEEE Winter Conference on
Applications of Computer Vision (WACV). pp. 1519–1528. IEEE (2018) 4

58. Wang, B., Adeli, E., Chiu, H.k., Huang, D.A., Niebles, J.C.: Imitation learning for
human pose prediction. In: Proceedings of the IEEE International Conference on
Computer Vision. pp. 7124–7133 (2019) 4

59. Wang, J., Xu, H., Xu, J., Liu, S., Wang, X.: Synthesizing long-term 3d human
motion and interaction in 3d scenes. In: Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition. pp. 9401–9411 (2021) 4, 13, 27
60. Yan, S., Li, Z., Xiong, Y., Yan, H., Lin, D.: Convolutional sequence generation for
skeleton-based action synthesis. In: Proceedings of the IEEE/CVF International
Conference on Computer Vision. pp. 4394–4402 (2019) 4

61. Yan, X., Rastogi, A., Villegas, R., Sunkavalli, K., Shechtman, E., Hadap, S., Yumer,
E., Lee, H.: Mt-vae: Learning motion transformations to generate multimodal hu-
man dynamics. In: Proceedings of the European Conference on Computer Vision
(ECCV). pp. 265–281 (2018) 4

62. Yuan, Y., Kitani, K.: Dlow: Diversifying latent flows for diverse human motion pre-
diction. In: Proceedings of the European Conference on Computer Vision (ECCV)
(2020) 11, 13

63. Zhang, H., Ye, Y., Shiratori, T., Komura, T.: Manipnet: neural manipulation syn-
thesis with a hand-object spatial representation. ACM Trans. Graph. 40, 121:1–
121:14 (2021) 3, 4

64. Zhang, S., Zhang, Y., Bogo, F., Pollefeys, M., Tang, S.: Learning motion priors for
4d human body capture in 3d scenes. In: IEEE/CVF International Conference on
Computer Vision (ICCV 2021) (2021) 4, 5, 8, 9, 10, 13, 21, 22, 24, 27

65. Zhang, Y., Black, M.J., Tang, S.: We are more than our joints: Predicting how 3d
bodies move. In: Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition. pp. 3372–3382 (2021) 2, 4, 5, 6, 13, 22, 24

SAGA

19

66. Zhang, Y., Yu, W., Liu, C.K., Kemp, C., Turk, G.: Learning to manipulate amor-
phous materials. ACM Transactions on Graphics (TOG) 39(6), 1–11 (2020) 4
67. Zhou, Y., Barnes, C., Lu, J., Yang, J., Li, H.: On the continuity of rotation rep-
resentations in neural networks. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) (June 2019) 5

SAGA: Stochastic Whole-Body Grasping with
Contact
** Appendix **

Yan Wu∗1, Jiahao Wang∗2, Yan Zhang1, Siwei Zhang1, Otmar Hilliges1, Fisher
Yu1, Siyu Tang1

1 ETH Z¨urich, Switzerland
2 Max Planck Institute for Informatics, Germany
yan.wu@vision.ee.ethz.ch, jiwang@mpi-inf.mpg.de,
{yan.zhang,siwei.zhang,otmar.hilliges,siyu.tang}@inf.ethz.ch, i@yf.io

In the appendix, we first provide the body markers representation, architec-
ture details, training and optimization setup, dataset pre-processing details, and
the AMT user study evaluation details in Appendix A. In Appendix B, we il-
lustrate the detailed baseline experiments and ablation study setup. We further
provide additional experimental results and visualization results in Appendix C,
and we discuss the existing limitations in Appendix D. Please see the video at
our project page for more random samples of synthesized grasping poses and
grasping motions.

A Method and Implementation Details

A.1 Body Markers Placement

To have informative yet compact markers setup, as illustrated in Fig. S1, we
follow the placement of the markers in GRAB [55] MoCap system, having 49
markers for the body, 14 for the face, 6 for hands and 30 for fingers (see Fig. S1
(a)-(c)) on SMPL-X body surface. As hand poses are subtle and the palm is
frequently in contact with the object, we additionally have 44 markers on two
palms (see Fig S1 (d-1)) to further enrich the markers information for a better
grasp. For the grasping ending pose generation in stage1, we use all these 143
markers for training and optimization. For the motion infilling network training
in stage2, we only use a sparse set of palm markers with 10 markers on fingertips
(see Fig. S1(d-2)).

A.2 Architecture Details

WholeGrasp-VAE We have visualized the WholeGrasp-VAE architecture in
Fig. 3. This CVAE is conditioned on the object height information and the
object geometry feature extracted with PointNet++ [45] encoder. In the en-
coder, taking the body markers’ postions M ∈ RN ×3 and body markers contacts

* Equal contribution.

SAGA-Appendix

21

Fig. S1. Visualization of our body markers placement. We have 69 markers on the
body surface, which are visualized as red spheres on SMPL-X body surface, in which
49 for the body, 14 for the face and 6 for hands. For the WholeGrasp-VAE training,
we additionally have 30 markers on fingers and 44 markers on palms (d-1). For the
MotionFill-VAE training, we only have sparse hand markers (d-2) with 10 markers on
fingertips.

CM ∈ {0, 1}N as inputs, where N is the number of markers, the body branch
encodes the body feature FB; Taking the object contacts CO ∈ {0, 1}2048 as
an additional feature of the object point cloud data, the object branch uses the
PointNet++ to encode the object feature. Further, we fuse them into a joint
16-dimensional latent space zs. In the decoder, we individually decode the body
markers’ positions, markers’ contacts, and object contacts. Note that we model
the contacts learning as a two-class (in-contact or out-of-contact) classification
task, and the decoder outputs the in-contact probability of each points. And
the PointNet++ encoder architecture is given by: SA(256, 0.2, [64, 128]) →
SA(128, 0.25, [128, 256]) → SA([256, 512]), where SA denotes the set abstrac-
tion level [45].

MotionFill-VAE In TrajFill, the root state at time t is given by Γt = (xt, yt,
cos γt, sin γt), where xt, yt are the position of pelvis joint in the x-y (ground)
plane, and γt is the body rotation around z-axis. Given Γ0 and ΓT , the TrajFill is
built to learn the deviation ∆Γ0:T +1 = Γ0:T +1−Γ 0:T +1 from an initial trajectory
Γ 0:T +1 which is a linear interpolation and one-step extrapolation of the given
Γ0 and ΓT , and we use Γ 0:T +1 as the condition. Inside TrajFill, we use MLP
structures for the encoder and the decoder. For the encoder, input trajectory
features are passed through two residual blocks, which has the same hidden size
as the input dimension (8T , where T is the time length of the input). After that,
two linear branches project the features into the 512-dimensional latent space.
The decoder includes two residual blocks with hidden sizes equal to 8T and 4T ,
respectively. We get the final output of TrajFill by adding the last residual block
output and the initial rough trajectory Γ 0:T +1.

In LocalMotionFill, following the same input processing step as in [64], we
build a 4-channel local motion image I ∈ R4×(3N +n)×T , where N, n are the

(a) Front view(b) Back view(c) Top view(d-1) Dense hand markers(d-2) Sparse hand markers22

Y. Wu∗, J. Wang∗ et al.

number of markers and the dimension of foot-ground contact labels. The first
channel of I is a concatenation of foot-ground contacts CF0:T ∈ {0, 1}n×T and the
0:T ∈ R3N ×T .The other three channels of Il are the
normalized local markers M l
normalized root local velocities vl
0:T . To incorporate the condition information
(M0, MT , v0:T ) into the LocalMotionFill, similarly, we build a condition image
Ic which essentially is the masked input image I. We use the same CNN-based
encoder and decoder network as in [64] to learn the infilled motion image.

A.3 Dataset Processing

GRAB. We use GRAB (https://grab.is.tue.mpg.de/license) dataset to train
both the WholeGrasp-VAE and MotionFill-VAE for grasping ending pose gen-
eration and motion infilling, respectively.

For WholeGrasp-VAE training, considering the different body shape pattern
of male and female, we suggest training the male model and the female model
separately. Following GrabNet [55], for training, we take all frames with right-
hand grasps. And out of the 51 different objects in GRAB dataset, following
the same split of object class in GrabNet [3, 55] we take out 4 validation objects
(apple, toothbrush, elephant, hand ) and 6 test objects (mug, camera, toothpaste,
wineglass, frying pan, binoculars), and the remaining 41 objects are used for
training. We center the object point cloud and the body markers at the geometry
center of the object.

For MotionFill-VAE training, we only utilize sequences where humans are
approaching to grasp the object. GRAB dataset captures the motion sequences
where the human starts with T-pose, approaches and grasp the object, and then
interacts with the object. For MotionFill-VAE training, we clip those approach-
ing and grasping sequences. Since most of these approaching motion sequences
only last for about 2s in the GRAB dataset, we clip 2-second videos from each
sequence by ensuring that the last frames are at stable grasping poses. If the
sequence is shorter than 2s, we pad the first frame to have the two-second clip.

AMASS. We pre-train our motion infilling model MotionFill-VAE on the AMASS
(https://amass.is.tue.mpg.de/license.html). We down-sample the sequences to
30 fps and cut them into clips with same duration. To be consistent with GRAB
dataset, we clip 2-second sequences from the AMASS for the grasping motion
infilling task. We also evaluate our motion infilling network by conducting ex-
periments on the general motion infilling task with different time lengths (see
Appendix C.2), and for that, we clip the AMASS dataset into 4-second and 6-
second sequences. Similar to [65], we reset the world coordinate for each clip.
The origin of the world coordinate is set to the pelvis joint in the first frame. The
x-axis is the horizontal component of the direction from the left shoulder to right
shoulder, the y-axis faces forward, and the z-axis points upward. For training, we
use all the mocap datasets except EKUT, KIT, SFU, SSM synced, TCD hand-
Mocap, and TotalCapture. For testing, we use TCD handMocap, TotalCapture,
and SFU.

SAGA-Appendix

23

Algorithm 1 WholeGrasp-Opt: grasping pose optimization
Input: Sampled markers ˆM , markers-object contacts ˆCM , ˆCO.
Output: Body mesh BT (ΘT ) and the queried markers MT .
Require: Optimization steps (N1, N2, N3) = (300, 400, 500).

for i = 1 : N1 do

Optimize t, R to minimize Ef it in Eq. 5.

for i = N1 : N1 + N2 do

Optimize t, R, β, θb to minimize Ef it in Eq. 5.

for i = N1 + N2 : N1 + N2 + N3 do

Optimize θb, θh, θe to minimize Eopt in Eq. 4-6.

return ΘT = [β, t, R, θb, θh, θe]

A.4 Implementation Details

We implement our experiments using PyTorch v1.6.0 [41]. In the following, we
introduce the training details of WholeGrasp-VAE and MotionFill-VAE, and
optimization details of GraspPose-Opt and GraspMotion-Opt respectively.
WholeGrasp-VAE training. In Sec. 3.2, we have introduced the WholeGrasp-
VAE training losses. Note that for the object and markers contact map recon-
struction, due to the class in-balance, we employ the weighted binary cross-
entropy loss, and we empirically set the weights for in-contact class for objects
and markers as 3 and 5 respectively. And we set the object and markers con-
tact map reconstruction weight λM , λO in Eq. 1 as 1. For the VAE training, we
adopt the linear KL weight annealing strategy [36] to avoid posterior collapse
issue in VAE training. And we empirically set λc = 1 and λKL = 0.005e in
Ltrain = Lrec + λKLLKL + λcLc, where e is the epoch number, and we train the
WholeGrasp-VAE for 40 epochs.
MotionFill-VAE training. In the experiments for Table. 3, for local motion
infilling (given the starting pose, ending pose and trajectory), we train our Local-
MotionFill model on AMASS training set; for the entire MotionFill-VAE (“Traj
+ local motion infilling”), we first pretrain our TrajFill module and LocalMo-
tionFill module on the GRAB and AMASS training set respectively, and we
further finetune the entire MotionFill-VAE on the GRAB training set. We em-
pirically set the hyper-parameters in LM = Lrec +λKLLKL and Eq. 8 as follows:
{λKL, λ1, λ2, λ3, λ4} = {1, 0.05, 1, 1, 0.5}.
GraspPose-Opt optimization. In Sec. 3.2, we have illustrated the GraspPose-
Opt optimization losses design to recover SMPL-X body mesh from sparse mark-
ers and refine the body pose for more perceptually realistic human-object inter-
actions. We empirically set the hyper-parameters in Eq. 5-7 {αo
colli,
αB
colli, αθ} = {15, 15, 100, 200, 0.0005}. As it can be difficult for the optimization
process to converge by jointly optimizing the high-dimensional SMPL-X param-
eters, which include the body global configuration t and R, shape parameters
β, body pose parameters θb, and the more local hand pose θh and eye pose

cont, αm

cont, αO

24

Y. Wu∗, J. Wang∗ et al.

θe parameters, similar as in MOJO [65], we suggest a multi-stage optimization
mechanism by optimizing in a global-to-local fashion to facilitate a gradual con-
vergence. And the detailed multi-stage training process can be found in Alg. 1.
We use Adam [27] optimizer, and the initial learning rates for these three stages
are set as 0.016, 0.012, 0.008 respectively.

GraspMotion-Opt optimization. In Sec. 3.4, we have shown the GraspMotion-
Opt optimization losses design to recover smooth SMPL-X body motions from
sparse markers sequence. Additionally, we introduce more details about our mo-
tion smoothness loss and foot skating loss design.

– Cross-frame smoothness loss. To encourage a temporarily smooth whole-
body motion, following [64], we enforce smoothness on the smooth motion
latent space S1:T −1 = AE(M1:T − M0:T −1) encoded by a pretrained autoen-
coder. Also, we explicitly enforce smoothness on the hand vertices, and the
overall smoothness loss is given by:

Esmooth = αB
s

T −2
(cid:88)

t=1

|St+1 − St|2 + αh
s

T −1
(cid:88)

t=0

|V h

Bt+1

− V h
Bt

|2

(S.1)

– Foot skating loss. Following [64], we reduce the foot skating artifacts by

optimization based on the foot-ground contact labels ˆCF .

Eskat = αskat

(cid:88)

(cid:88)

t∈Tc

|vf oot
t

|≥σ

||vf oot
t

| − σ|

(S.2)

where Tc means the timestamps with foot-ground contact, vf oot
represents
the velocity (location difference between adjacent timestamps t and t + 1) of
vertices on the left toe, left heel, right toe, and right heel, at time t. σ is a
threshold and we use σ = 0.1 in our experiments.

t

The overall optimization loss is given by Ebasic + Eg + Esmooth + Eskat, where
Ebasic and Eg are formulated in Eq. 10 - 11. We optimize the overall loss in two
stages, where we first fit SMPL-X body mesh to the predicted markers sequences
by minimizing the markers fitting loss (Ef it in Eq. 10), and then we refine the
recovered body mesh sequences by minimizing the overall loss. We present the
detailed optimization procedure in Alg. 2. We also use Adam [27] optimizer. For
the first frame, the initial learning rate is 0.1, and for the other frames the initial
learning rate is 0.01. Stage 1 optimization takes 100 steps and the learning rate
becomes 0.01 after step 60 and decreases to 0.003 after step 80. The second
stage optimization takes 300 steps, and the initial learning rate is set to 0.01
and decays to 0.005 after 150 steps.

SAGA-Appendix

25

Algorithm 2 GraspMotion-Opt: grasping motion optimization
Input: Sampled body markers sequences ˆM0:T , mutual markers-object contacts ˆCM ,
ˆCO and dynamic foot-ground contact ˆCF0:T ; Body shape parameters β
Output: Smoothed whole-body grasping motion B0:T (Θ0:T ).
Require: Optimization steps (N1, N2) = (100, 300).

for i = 1 : N1 do

Optimize [t, R, θ]0:T to minimize

T
(cid:80)
t=0

Ef it in Eq. 10.

for i = N1 : N1 + N2 do

Minimize Ebasic + Eg + Esmooth + Eskat in § 3.4

return Θ0:T = {β, [t, R, θ]0:T }

A.5 Amazon Mechanical Turk (AMT) User Study

We perform user study via AMT, and the user study interface is presented
in Fig. S2. We perform user study on both ground truth motion sequences
from GRAB dataset and our randomly generated sample sequences. We test
our pipeline with 14 unseen objects from both GRAB test set and HO3D [15]
dataset, and we generate 50 random grasping motion sequences for each object,
where objects are randomly placed. Each sequence is scored by 3 users and we
take the average score, and the score range from 0 to 5 (from strongly disagree to
strongly agree). The average perceptual score for each object class is presented
in Table S1.

Table S1. Perceptual score results of both ground truth (GT) grasping motion se-
quences and our synthesized sequences for grasping various unseen objects. Due to the
lack of ground truth whole-body grasping motions for objects in HO3D dataset, we
only evaluate ground truth sequences for objects from GRAB dataset.

GRAB

Object

Score
GT Ours

HO3D

Average Score

Object

Score (Ours) GT Ours

Binoculars 3.83 2.98

Cracker box
Sugar box

Camera

3.92 3.60 Mustard bottle

Toothpaste 4.22 3.47
4.38 2.82

Mug

Meat can
Pitcher base

Wine glass 3.85 2.87 Bleach cleanser
Frying pan 4.16 1.7

Mug
Power drill

3.15
3.45
3.49
3.43
2.68
3.56
2.61
2.86

4.04

3.15

26

Y. Wu∗, J. Wang∗ et al.

Fig. S2. AMT user study interface. We present both the first-view video (on the left
side) and third-view video (on the right side) to the user for the grasping motion quality
evaluation.

B Baselines Implementation Details

In Sec. 4.1 and Sec. 4.2, we conduct several baseline experiments as comparisons
with our WholeGrasp-VAE and MotionFill-VAE and also some ablation studies.
In this section, we illustrate more implementation details about our baselines
and ablation studies.

B.1 Baselines to WholeGrasp-VAE

In Sec. 4.1, we extend the GrabNet [55] to the whole-body grasping pose gen-
eration task (GrabNet-SMPLX) as a comparison with our WholeGrasp-VAE
design, and we also study the effectiveness of the multi-task WholeGrasp-VAE
by comparing with the single-task design (WholeGrasp-single). In the following,
we provide detailed experimental setup of these two experiments.

– GrabNet-SMPLX. GrabNet proposed to synthesize diverse hand grasps
by directly learning the hand model parameters. And a similar idea can be
extended to the full-body grasp synthesis by learning the compact SMPL-X
body model parameters, which can include the body global configurations
t and R, the shape parameters β, and the full-body pose θ = [θb, θh, θe].
Fig. S3 (a) shows the schematic architecture of the GrabNet-SMPLX base-
line. Different from the original GrabNet, instead of encoding the object
shape using basic point set features [44], we employ the PointNet++ [45]

SAGA-Appendix

27

in the GrabNet-SMPLX baseline, which is consistent with our WholeGrasp-
VAE.

– WholeGrasp-single. As visualized in Fig. S3 (b), we build a single-task
WholeGrasp-VAE, namely WholeGrasp-single, where we only learns the
body markers positions. We employ the same multi-stage optimization algo-
rithm as in GraspPose-Opt to fit SMPL-X body mesh to sampled markers.
Recall that in our GraspPose-Opt, with the predicted body and object con-
tact map, we design a contact loss accordingly (see Eq. 6) to refine the
mutual contacts between the human body and object. However, due to the
lack of contact map prediction in the single-task WholeGrasp-single, we can-
not directly leverage this contact loss term to refine the hand pose. Instead of
simply ignoring the contact refinement loss term in this test-time optimiza-
tion step, we build a strong baseline by pre-defining a fixed hand contact
pattern and designing a heuristic contact loss accordingly. Concretely, we
firstly compute the average contact probability for each hand vertices over
all the GRAB dataset, and we denote this hand contact prior probability
as CH. Heuristically, we encourage those hand vertices that have a high
prior contact probability (greater than 0.7) and also are closed enough to
the object (less than 2cm) to contact with the object, and we formulate this
heuristic contact loss baseline Eh

cont as:

Eh

cont = αh

cont

1(Ch > 0.7)1(d(h, O) < 0.02) ∗ Chd(h, O)

(S.3)

(cid:88)

h∈V h
B

where V h
tively, and d(x, Y) = miny∈Y ||x − y||2
loss for the single-task WholeGrasp-single experiment is given by:

B and O denote the hand vertices and object point cloud respec-
2. Therefore, the overall optimization

Esingle
opt

(Θ) = Ef it + Eo

colli + Eh

cont + Eg

cont.

(S.4)

where Ef it, Eo
Opt.

colli, Eg

cont have the same formulations as in our GraspPose-

B.2 Baselines to MotionFill-VAE

In Sec. 4.2, we compare our method with the convolution autoencoder network
(CNN-AE) in [26], LEMO [64], and RouteNet and PoseNet from Wang et al. [59].
For RouteNet and PoseNet, we remove the scene encoding branch from [59] and
adopt the same route encoding branch and pose encoding branch architecture
design. We use the same body representation as ours in all these experiments.

C Additional Results

C.1 Ablation Study on GraspPose-Opt optimization losses

In Sec. 4.1 and Table 2, we have studied the effectiveness of our proposed
GraspPose-Opt optimization loss design in Eq. 4 for optimizing human-object

28

Y. Wu∗, J. Wang∗ et al.

Fig. S3. (a) The schematic architecture of GrabNet-SMPLX baseline. Similar as in
GrabNet, we build a CVAE model to directly generate SMPL-X parameters; (b) The
Schematic pipeline of the WholeGrasp-single baseline, which merely learns positions
information of body markers.

interactions. In Fig. S4, we also present the visualization results of optimized
hand poses using different loss designs to show the effects of our proposed loss
terms. Since the hand pose can be highly sensitive to even tiny noises in mark-
ers positions, using only the basic markers fitting loss and foot ground contact
loss, the recovered hand pose from markers can hardly interact with the object
in a perceptually realistic way (see visualization result in Fig. S4 (a)). While
the object collision loss Ecolli helps to mitigate the hand-object interpenetration
issue (Fig. S4 (b)), the optimized hand does not grasp the object steadily. Us-
ing our mutual human-object contact loss Eo
cont, the object surface areas with
higher contact probability attract hand vertices, and we can yield realistic and
plausible hand-object interaction (see visualization result in Fig. S4 (c)).

Fig. S4. Visualization results of ablation study on the GraspPose-Opt optimization loss
design in Table 2. We present the optimized hand poses using different loss designs,
and the red areas on the object surface indicate higher contact probability.

C.2 Additional Visualizations and Results on MotionFill-VAE

In Table 3, we have shown the quantitative results of our method compared with
other state-of-the-art methods. In Fig. S5, we qualitatively present the diversity
of the motions generated by our model which is finetuned on GRAB dataset [55].
The figure shows that our method can generate diverse trajectories as well as
diverse local motions.

Limited by the short sequence length in the GRAB dataset, we only conduct
the two-second motion infilling experiments with our MotionFill-VAE. Beyond

(a) (b) (c) SAGA-Appendix

29

Fig. S5. Visualization on generated diverse motion sequences. (a) Three motion se-
quences on three different trajectories generated by TrajFill-CVAE, respectively. (b)
Three motion sequences generated on the same ground truth trajectory. Different colors
(red, green, blue) represent different motion sequences in each sub-figure. The diverse
intermediate frames show the stochasticity of our TrajFill-CVAE and LocalMotionFill-
CVAE.

generating two-second motion sequences given the starting pose and the ending
pose, we show that our motion infilling model can be easily generalized to longer
time lengths. Given the starting pose, ending pose, we train our MotionFill-VAE
on AMASS dataset with 2s, 4s, 6s clips, respectively. In Fig. S6, we present the
infilled motion sequences of 2 seconds, 4 seconds, and 6 seconds. The visualization
results show that our motion infilling model is able to generate motions with
different time lengths.

D Limitations

Although our method can generate realistic grasping poses and grasping motions
for most of the unseen objects in our test set, we observe some failure cases
where the synthesized human fails to grasp the object in a realistic way. We
have the similar observation as mentioned in GrabNet [55], the frying pan is the
most challenging object to grasp. As visualized in Fig. S7, though the generated
humans are in contact with the pan, they typically fail to grasp the pan handle,
resulting in perceptually unrealistic results and low perceptual score in Table S1.

30

Y. Wu∗, J. Wang∗ et al.

Fig. S6. Visualization on generated motion sequences with different time lengths (2s,
4s, 6s). (a) 2-second motion sequences. (b) 4-second motion sequences. (c) 6-second
motion sequences. We train these three MotionFill-VAE models using training data
with different time lengths on AMASS dataset [37]. The visualization results show
that our motion infilling model can be easily generalized to different time horizons.

Fig. S7. Grasping pose random samples for grasping the frying pan. Generating real-
istic grasping poses for frying pan pan is challenging. Although the generated humans
are still in contact with the pan, they typically fails to grasp the handle of the pan,
resulting in perceptually unrealistic results.

