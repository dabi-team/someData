[POSTER] Semantic Augmented Reality Environment with Material-Aware
Physical Interactions

Long Chen*
Bournemouth University

Karl Francis†
Bournemouth University

Wen Tang‡§
Bournemouth University

8
1
0
2

r
a

M
6
1

]

V
C
.
s
c
[

3
v
8
0
2
1
0
.
8
0
7
1
:
v
i
X
r
a

ABSTRACT
In Augmented Reality (AR) environment, realistic interactions be-
tween the virtual and real objects play a crucial role in user experi-
ence. Much of recent advances in AR has been largely focused on
developing geometry-aware environment, but little has been done in
dealing with interactions at the semantic level. High-level scene un-
derstanding and semantic descriptions in AR would allow effective
design of complex applications and enhanced user experience. In
this paper, we present a novel approach and a prototype system that
enables the deeper understanding of semantic properties of the real
world environment, so that realistic physical interactions between the
real and the virtual objects can be generated. A material-aware AR
environment has been created based on the deep material learning us-
ing a fully convolutional network (FCN). The state-of-the-art dense
Simultaneous Localisation and Mapping (SLAM) has been used for
the semantic mapping. Together with efﬁcient accelerated 3D ray
casting, natural and realistic physical interactions are generated for
interactive AR games. Our approach has signiﬁcant impact on the
future development of advanced AR systems and applications.

Index Terms: H.5.1 [Information Interfaces And Presentation]:
Multimedia Information Systems—Artiﬁcial, Augmented, and Vir-
tual Realities; I.2.10 [Artiﬁcial Intelligence]: Vision and Scene
Understanding—3D/Stereo Scene Analysis

1 INTRODUCTION
The latest research on Simultaneous Localisation and Mapping
(SLAM) has opened up a new world for AR technology devel-
opment. SLAM has changed the ways that how traditional AR
systems worked in terms of camera pose estimations and tracking.
Furthermore, advanced SLAM systems such as KinectFusion [8] and
Dynamicfusion [7] are able to reconstruct dense surfaces to generate
geometric information of the real scene, providing structural-aware
AR environment, which is a step changer in AR technology. How-
ever, although through SLAM the geometric surfaces of the scene
can be efﬁciently recovered, individual semantic properties of vari-
ous different objects of the real world remain unknown in the AR
environment.

Realistic interactions in augmented reality (AR) applications re-
quire not only structural information, but also semantic understand-
ings of the scene. While geometric or structural information allows
accurate information augmentation and placement in AR, at the in-
teraction level, semantic knowledge will enable realistic interactions
between the virtual and the real objects, for example realistic physi-
cal interactions (e.g. a virtual glass can be shunted on a real concrete
ﬂoor). More importantly, using semantic scene descriptions, we can
develop high-level tools for efﬁcient design and constructions of
complex AR applications, such as large scale complex AR games.

*e-mail: chenl@bournemouth.ac.uk
†e-mail:i7241745@bournemouth.ac.uk
‡e-mail:wtang@bournemouth.ac.uk
§Corresponding Author

In virtual reality (VR) applications, recent research attempts have
been made towards virtual object classiﬁcations using semantic
associations to describe virtual object behaviours [2]. Troyer et. al.
has discussed the opportunities and challenges of using the notion
of conceptual modelling for virtual reality, and pointed out that
the gap between the conceptual level and the implementation level
for virtual reality is too large to be bridged in one single step [3].
Therefore, three phases: conceptual speciﬁcation phase, the mapping
phase and the generation phase have been suggested. In contrast
to the research in VR, little work has been presented in conceptual
modelling for AR. We argue that deep understanding of the AR
environment through semantic segmentation techniques is the ﬁrst
step towards the high-level design concept for AR. More recently,
real-time 3D semantic reconstruction has been a challenging topic in
robotics with many recent works being focused on object semantic
labeling [6].

In this paper, we propose a novel material-aware semantic AR
framework by constructing 3D volumetric semantics and an occu-
pancy map of the real objects. We focus on the labeling of material
properties of the real environment and the realistic physical interac-
tions between the virtual and the real objects in AR. Staring with
KinectFusion for camera pose recovery and the 3D model recon-
struction, we trained a fully convolutional network (FCN) based on
VGG-16 model [10] to obtain 2D semantic material segmentation
maps [11] for detecting material properties of each object in the
scene. The 3D point cloud of the scene is labeled with the semantic
materials, so that realistic physics can be applied during interactions
through real-time inference. Our proposed framework is presented
through a semantic material-aware game example to demonstrate
realistic physical interactions between the virtual and real objects.

2 METHODS

2.1 Camera tracking and model reconstruction

We have adapted KinectFusion as the core camera tracking system
with dense 3D model reconstructions. A Kinect depth sensor has
been used to fuse the data into a single global surface model while
simultaneously obtaining the camera pose by using a coarse-to-ﬁne
iterative closest point (ICP) algorithm. The tracking and modeling
processes consist of four steps: (i) Each pixel acquired by the depth
camera is ﬁrstly transformed into the 3D space by the camera´s
intrinsic parameters and the corresponding depth value acquired by
the camera; (ii) A ICP alignment algorithm is performed to estimate
the camera pose between the current frame and the reconstructed
model; (iii) With the available camera poses, each consecutive depth
frame can be fused incrementally into one single 3D reconstruction
by a volumetric truncated signed distance function (TSDF); (iv)
Finally, a surface model is predicted via a ray-casting process.

2.2 Deep learning for material recognition

To train a neural network for material recognition, we follow the
method in [11], the VGG-16 pre-trained model for ImageNet Large-
Scale Visual Recognition Challenge (ILSVRC) is used as the initial
weights of our neural network [10]. We then ﬁne-tuned the network
from 1000 different classes of materials into 23 class labels as the
output based on the Materials in Context Database (MINC) that

 
 
 
 
 
 
(a)

(b)

Figure 1: The result of 3D model reconstruction, 3D semantic la-
belling and semantic 3D model fusing.

contains 3 million material samples across 23 categories. However,
the Convolutional Neural Network (CNN) is speciﬁcally designed
for classiﬁcation tasks and only produces a single classiﬁcation
result for a single image. We manually cast the CNN into a Fully
Convolutional Network (FCN) for pixel-wise dense outputs [9]. By
transforming the last three inner product layers into convolutional
layers, the network can learn to make dense predictions efﬁciently
at pixel level for tasks like semantic segmentation. Finally, we have
trained the FCN-32s, FCN-16s and FCN-8s consecutively using
images with material labels provided in the MINC database.

2.3 Semantic label fusion using CRF
KinectFusion builds a 3D model, but our material recognition net-
work only provides 2D outputs. Therefore, following [1] [4] we em-
ployed a graphical model of Conditional Random Fields (CRF) [5]
to guide the fusion process of mapping the 2D semantic labels onto
the 3D reconstruction model. CRF is to ensure the contextual con-
sistency, and the ﬁnal fusion result is shown in Figure 1.

3 RESULT AND DISCUSSION
We have developed a small shooting game demo 1 (see Figure 2)
in Unity to demonstrate our proposed concept of semantic material-
aware AR. Our framework is built as a drop-and-play plugin in
Unity, which processes the AR camera pose tracking and feeds the
3D semantic-aware model. The game contains two layers, in which
the top layer displays the current video stream from a RGBD camera,
whilst the semantic 3D model serves for physical interaction layer
by correctly mapping the video stream with synchronised camera
poses for semantic inference. An oct-tree acceleration data structure
has been implemented for efﬁcient ray-casting to query the mate-
rial properties and corresponding physical interactions are applied
through physic simulations. As can be seen from Figure 2, realistic
interactions between the real and virtual objects (e.g. bullet holes,
ﬂying chips and sound) are simulated at real-time with various differ-
ent material responses i.e. (a)wood, (b)glass and (c)fabric, creating
a real-time interactive semantic driven AR shooting game. Our
work demonstrates the ﬁrst step towards the high-level conceptual
interaction modelling for enhanced user experience in complex AR
environment.

REFERENCES

[1] I. Armeni, O. Sener, A. R. Zamir, H. Jiang, I. Brilakis, M. Fischer,
and S. Savarese. 3d semantic parsing of large-scale indoor spaces. In

1https://www.youtube.com/watch?v=02ZAqXH2FGU

(c)

(d)

Figure 2: A prototype shooting game. Different material responses:
(a) wood, (b) glass, (c) fabric. (d) shows the hidden material-aware
layer that handles the physical interaction.

Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 1534–1543, 2016.

[2] P. Chevaillier, T.-H. Trinh, M. Barange, P. De Loor, F. Devillers,
J. Soler, and R. Querrec. Semantic modeling of virtual environments
using mascaret. In Software Engineering and Architectures for Real-
time Interactive Systems (SEARIS), 2012 5th Workshop on, pp. 1–8.
IEEE, 2012.

[3] O. De Troyer, F. Kleinermann, B. Pellens, and W. Bille. Concep-
tual modeling for virtual reality. In the 26th international conference
on Conceptual modeling-Volume 83, pp. 3–18. Australian Computer
Society, Inc., 2007.

[4] A. Hermans, G. Floros, and B. Leibe. Dense 3d semantic mapping of
indoor scenes from rgb-d images. In Robotics and Automation (ICRA),
2014 IEEE International Conference on, pp. 2631–2638. IEEE, 2014.
[5] P. Kr¨ahenb¨uhl and V. Koltun. Efﬁcient inference in fully connected
crfs with gaussian edge potentials. In Advances in neural information
processing systems, pp. 109–117, 2011.

[6] J. McCormac, A. Handa, A. Davison, and S. Leutenegger. Semanticfu-
sion: Dense 3d semantic mapping with convolutional neural networks.
arXiv preprint arXiv:1609.05130, 2016.

[7] R. A. Newcombe, D. Fox, and S. M. Seitz. Dynamicfusion: Recon-
struction and tracking of non-rigid scenes in real-time. In Proceedings
of the IEEE CVPR, pp. 343–352, 2015.

[8] R. A. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux, D. Kim, A. J.
Davison, P. Kohi, J. Shotton, S. Hodges, and A. Fitzgibbon. Kinect-
fusion: Real-time dense surface mapping and tracking. In Mixed and
augmented reality (ISMAR), 2011 10th IEEE international symposium
on, pp. 127–136. IEEE, 2011.

[9] E. Shelhamer, J. Long, and T. Darrell. Fully convolutional networks
for semantic segmentation. IEEE transactions on pattern analysis and
machine intelligence, 39(4):640–651, 2017.

[10] K. Simonyan and A. Zisserman. Very deep convolutional networks
In International Conference on

for large-scale image recognition.
Learning Representations, 2015.

[11] C. Zhao, L. Sun, and R. Stolkin. A fully end-to-end deep learning
approach for real-time simultaneous 3d reconstruction and material
In 2017 18th International Conference on Advanced
recognition.
Robotics (ICAR), pp. 75–82, July 2017. doi: 10.1109/ICAR.2017.
8023499

