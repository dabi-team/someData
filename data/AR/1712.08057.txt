On Long Memory Origins and Forecast Horizons

J. Eduardo Vera-Vald´es∗

Department of Mathematical Sciences, Aalborg University, and CREATES.

Abstract

Most long memory forecasting studies assume that the memory is generated by the fractional dif-
ference operator. We argue that the most cited theoretical arguments for the presence of long
memory do not imply the fractional diﬀerence operator, and assess the performance of the autore-
gressive fractionally integrated moving average (ARF IM A) model when forecasting series with
long memory generated by nonfractional processes. We ﬁnd that high-order autoregressive (AR)
models produce similar or superior forecast performance than ARF IM A models at short horizons.
Nonetheless, as the forecast horizon increases, the ARF IM A models tend to dominate in forecast
performance. Hence, ARF IM A models are well suited for forecasts of long memory processes
regardless of the long memory generating mechanism, particularly for medium and long forecast
horizons. Additionally, we analyse the forecasting performance of the heterogeneous autoregressive
(HAR) model which imposes restrictions on high-order AR models. We ﬁnd that the structure
imposed by the HAR model produces better long horizon forecasts than AR models of the same
order, at the price of inferior short horizon forecasts in some cases. Our results have implications
for, among others, Climate Econometrics and Financial Econometrics models dealing with long
memory series at diﬀerent forecast horizons. We show in an example that while a short memory
autoregressive moving average (ARM A) model gives the best performance when forecasting the
Realized Variance of the S&P 500 up to a month ahead, the ARF IM A model gives the best
performance for longer forecast horizons.

JEL classiﬁcation: C53, C22.

Keywords:

forecasting, ARF IM A, long memory, model conﬁdence set, HAR model.

7
1
0
2

c
e
D
1
2

]

M
E
.
n
o
c
e
[

1
v
7
5
0
8
0
.
2
1
7
1
:
v
i
X
r
a

∗E-mail: eduardo@math.aau.dk Webpage: https://sites.google.com/view/veravaldes/

Address: Skjernvej 4A, 9220 Aalborg East, Denmark.

Preprint submitted to arXiv

December 22, 2017

 
 
 
 
 
 
1. Introduction

Long memory analysis deals with the notion of series with strong persistence in the sense of
long lasting correlations. One of the ﬁrst works on strong persistence is due to Hurst (1956). He
studied the long-term capacity of reservoirs for the Nile and recommended to increase the height
of a dam to be built given his observations on cycles of highs at the river. As found by Hurst,
failing to account for the presence of long memory can lead to inaccurate forecasts. If the data
is best modelled by a long memory process, then the predictions computed with standard models
would be too optimistic, in the sense that they would predict a return to normal events faster than
what we would observe in reality. A dam built based on a short memory forecast would be more
prone to overﬂow that one built based on a long memory forecast, hence increasing the risk of a
catastrophic event. Hurst’s work highlights the importance of developing appropriate forecasting
tools to deal with the presence of long memory.

In the time series literature, the ARF IM A class of models remains to be the most popular
given its appeal of bridging the gap between the stationary ARM A models, and the nonstation-
ary ARIM A model. Moreover, some eﬀort has been directed to assess the performance of the
ARF IM A type of models when forecasting long memory processes.

Ray (1993) calculates the percentage increase in mean-squared error (M SE) from forecasting
fractionally integrated (F I) series with AR models. She argues that the M SE may not increase
signiﬁcantly, particularly when we do not know the true long memory parameter. Crato and Ray
(1996) compare the forecasting performance of ARF IM A models against ARM A alternatives and
ﬁnd that ARF IM A models are in general outperformed by ARM A alternatives for short forecast
horizons. Looking at real data, Martens et al. (2009) show that for daily realized volatility for
forecast horizons of up to twenty days, it seems to be beneﬁcial to use a ﬂexible high-order AR
model instead of a parsimonious but stringent fractionally integrated model. On the other hand,
Barkoulas and Baum (1997) ﬁnd improvements in forecasting accuracy when ﬁtting ARF IM A
models to Eurocurrency returns series, particularly for longer horizons. By allowing for larger data
sets of both ﬁnancial and macro variables, and considering larger forecast horizons, Bhardwaj and
Swanson (2006) ﬁnd that ARF IM A processes generally outperform ARM A alternatives in terms
of forecasting performance. Thus, there does not seem to be a consensus regarding the forecast
performance of the ARF IM A model.

One thing that most forecasting comparison studies have in common is the underlying assump-
tion that long memory is generated by an ARF IM A process. There are two predominant theo-
retical explanations for the presence of long memory in the time series literature: cross-sectional
aggregation of dynamic persistent micro units (Granger, 1980), and that shocks may be of random
duration (Parke, 1999). As argued in Section 2, neither of these sources of long memory imply
an ARF IM A speciﬁcation. The question addressed in this paper is if an ARF IM A speciﬁca-
tion serves as a good approximation for forecasting purposes when the long memory generating
mechanism is diﬀerent from the ARF IM A model.

Moreover, as argued by Baillie et al. (2012), a practitioner’s goals will generally include making
forecasts over both short and long horizons. As an example, the surge of Climate Econometrics
as a way to address Climate Change relies on the construction of long horizon forecasts while
addressing medium term policy goals. Thus, we analyse the forecasting performance of short and
long memory models at several forecast horizons.
In particular, we extend previous studies to
larger forecast horizons relevant to Climate Change analysis.

This paper proceeds as follows. In Section 2, we present the long memory generating processes

2

considered, and show that the most cited theoretical explanations for the presence of long memory
do not imply an ARF IM A speciﬁcation. Section 3 describes the design of the Monte Carlo analysis
used for the forecasting study. Section 4 presents the results from the forecasting analysis, while
Section 5 discusses them in a bias-variance trade-oﬀ context. Moreover, Section 6 shows that the
insights gained from the Monte Carlo simulations hold on real data. Finally, Section 7 presents
the conclusions.

2. Long Memory Generating Processes

In this section, we present the selected processes used to generate long memory. All processes
considered are long memory in the covariance sense, see Haldrup and Vera Vald´es (2017) for other
deﬁnitions.
In contrast to the alternatives, the covariance sense relates to the rate of decay of
the autocorrelations. In this sense, the ﬁtted models try to mimic the rate of decay of the weight
that past observations have on future realizations. In this context, the models this information
to produce better forecasts; thus, the covariance sense is a sensible deﬁnition of long memory for
forecasting purposes.

2.1. The ARFIMA Model

As a benchmark, we include the ARF IM A process due to Granger and Joyeux (1980), and
Hosking (1981) in the analysis. The authors extended the ARM A model to include fractional
dynamics by considering the process

φ(L)(1 − L)dxt = θ(L)(cid:15)t,

(1)

where (cid:15)t is a white noise process, d ∈ (−1/2, 1/2), and φ(L) and θ(L) are polynomials in the lag
operator with no common roots, all outside the unit circle. The authors used the standard binomial
expansion to decompose the fractional diﬀerence operator (1 − L)d in a series with coeﬃcients
πj = Γ(j + d)/(Γ(d)Γ(j + 1)) for j ∈ N. Using Stirling’s approximation, it can be shown that these
coeﬃcients decay at a hyperbolic rate, which in turn translates to slowly decaying autocorrelations.
It is well known that ARF IM A processes are long memory by all deﬁnitions typically considered
in the literature, and are relatively easy to estimate by Maximum Likelihood, see Sowell (1992).
Thus, the ARF IM A model has become the canonical construction for modelling and forecasting
long memory in the time series literature; see Beran (1994), and Baillie (1996) for a review.

For the Monte Carlo analysis, we consider ARF IM A(1, d, 0) processes as a way to incorporate

both long and short term dynamics.

2.2. Cross-Sectional Aggregation

Granger (1980), in line with the work of Robinson (1978) on autoregressive processes with
random coeﬃcients, showed that aggregating AR(1) processes with coeﬃcients sampled from a
Beta distribution can produce long memory. He considered N series generated as

xi,t = αixi,t−1 + εi,t

i = 1, 2, · · · , N ;

where εi,t is a white noise process with E[ε2
α2

i ∼ B(α; p, q) with p, q > 1, and where B(α; p, q) is the Beta distribution with density given by

i,t] = σ2

ε ∀i ∈ {1, 2, · · · , N }, ∀t ∈ Z. Moreover,

B(α; p, q) =

1
B(p, q)

αp−1(1 − α)q−1

for α ∈ (0, 1),

3

with B(·, ·) the Beta function. Furthermore, deﬁne the cross-sectional aggregated series as

xt =

1
√
N

N
(cid:88)

i=1

xi,t.

Granger showed that as N → ∞, the autocorrelations of xt decay at a hyperbolic rate with

parameter d = 1 − q/2; thus, xt has long memory in the covariance sense.

The cross-sectional aggregation result has been extended in diﬀerent ways, including to allow
for general ARM A processes, and to other distributions; see Oppenheim and Viano (2004), Linden
(1999), and Zaﬀaroni (2004).

Haldrup and Vera Vald´es (2017) showed that the long memory generated by cross-sectional
aggregation does not correspond to the one associated to the ARF IM A model.
In particular,
they showed that although the long memory by cross-sectional aggregation can be removed by
fractional diﬀerencing, the resulting series does not belong to the class of linear ARM A processes.
The question addressed in this paper is whether an ARF IM A speciﬁcation remains useful for
forecasting purposes.

2.3. Error Duration Model

The error duration model was introduced by Parke (1999). He showed that if the series is the
result of the sum of shocks of stochastic duration, then it would exhibit long memory in the form
of hyperbolic decaying autocorrelations.

Let εs be a series of i.i.d. shocks with mean zero and ﬁnite variance σ2. Assume that the shock
εi has a stochastic duration of ni ≥ 0 time periods, and thus surviving from period i until period
i + ni. Let pk be the probability that event εi survives until period i + k, and take gi,t to be the
indicator function for the event that the error εi survives until period t. Furthermore, deﬁne xt as

xt =

t
(cid:88)

s=−∞

gs,tεs.

Then, if1 pk ∼ k−2+2d as k → ∞, xt will have long memory in the covariance sense.

By properly choosing the error survival probabilities, Parke showed that the autocorrelation
function will decay at a rate similar to F I(d) processes. However, the resulting series has dichoto-
mous coeﬃcients that do not correspond to the fractional diﬀerence operator.

We follow Parke’s speciﬁcation in the Monte Carlo analysis and consider error survival proba-

bilities that mimic those of the F I(d) model.

Table 1 summarizes the long memory generating mechanisms to be analysed.

Table 1: Long Memory Generating Processes

ARF IM A(p, d, q)
(DGP 1)

φ(L)(1 − L)dxt = θ(L)εt

φ(z) = 1 − φ1z − · · · − φpzp
θ(z) = 1 + θ1z + · · · + θqzq

(1 − L)d =

∞
(cid:88)

s=0

Γ(s − d)
Γ(−d)Γ(s + 1)

Ls

1For two series at, bt, with bt (cid:54)= 0 ∀t, we write at ∼ bt if limt→∞ at/bt = 1.

4

Cross-Sectional Aggregation
(DGP 2)

Error Duration Model
(DGP 3)

xt =

1
√
N

N
(cid:88)

i=1

xi,t

xi,t = αixi,t−1 + εi,t
αi ∼ B(α; p, q); p, q > 1

t
(cid:88)

xt =

gs,tεs

gs,s+k =

s=−∞

(cid:26) 0 w.p. 1 − pk
1 w.p. pk

pk = k2d−2

3. Monte Carlo Design

In this section, we describe the Monte Carlo analysis designed to compare the forecasting
performance of ARF IM A models against ARM A and high-order AR models on long memory
series generated by the processes described in Section 2.

3.1. Forecast Evaluation

We use the Model Conﬁdence Set (M CS) approach of Hansen et al. (2011) to assess the
forecasting performance of the selected models. From an initial set of models, the methodology
allows us to obtain the superior set at a given conﬁdence level. In this sense, the M CS is better
suited to compare the forecast performance of a large set of competing models.

The M CS algorithm proceeds as follows. From a starting set of competing model, M0, we

search for the set of superior models at forecast horizon h, M∗, deﬁned by

M∗ = {i ∈ M0 | E(dh

i,j) ≤ 0 ∀j ∈ M0},

where dh

i,j is the loss diﬀerential between models i and j.

We obtain M∗ by sequential elimination. For each long memory generating process, we ﬁt all
the competing models in the starting set for a sample size T . The models are indexed by i ∈
{1, 2, . . . , m}, and the out of sample forecast from model i is denoted by ˆyi
T +k, ∀k ∈ {1, . . . , h}. We
rank the models according to their expected loss using one of two loss functions: the mean square
(cid:1)2, and the mean absolute deviation (M AD),
error (M SE), LSQ
(cid:0)yT +k, ˆyi
LAD
Deﬁne the loss diﬀerential between models i and j by
(cid:16)
(cid:1) − LM

(cid:0)yT +k, ˆyi
T +k
(cid:12)yT +k − ˆyi

(cid:1) = (cid:0)yT +k − ˆyi

(cid:0)yT +k, ˆyi

yT +k, ˆyj

(cid:1) = (cid:12)

dk
i,j = LM

T +k

T +k

T +k

(cid:12)
(cid:12).

T +k

(cid:17)

,

T +k

for M = SQ, AD; i, j ∈ {1, 2, . . . , m}. We eliminate the worst performing model at each step, and
we continue with the process until we can not reject the null hypothesis of equal loss diﬀerentials
for all models in the set; that is,

H0 : E(dk

i,j) ≤ 0 ∀i, j ∈ M.

The null is tested by using either the range statistic, TR, or the semiquadratic statistic, TSQ,

deﬁned by

TR = max
i,j∈M

(cid:0)

| ¯di,j|
(cid:100)var( ¯di,j)(cid:1)1/2

TSQ =

(cid:88)

i(cid:54)=j

( ¯di,j)2
(cid:100)var( ¯di,j)(cid:1)1/2

(cid:0)

.

5

In the Monte Carlo analysis, we present the percentage number of times each model is contained

in M∗ for each forecast horizon.

Additionally, as another measure of forecast performance, we compute both the out of sample
root mean square error (RM SE), and the out of sample root mean absolute deviation (RM AD)
given by

RM SEi

h =

(cid:32)

1
h

h
(cid:88)

k=1

(cid:33)1/2

(cid:0)yT +k − ˆyi

T +k

(cid:1)2

RM ADi

h =

(cid:32)

1
h

h
(cid:88)

k=1

(cid:12)
(cid:12)yT +k − ˆyi

T +k

(cid:33)1/2
(cid:12)
(cid:12)

,

where h and ˆyi
replications.

s are deﬁned as above. We report the mean of both RM SE and RM AD across all

Note that the M CS, and RM SE or RM AD evaluation criteria are complementary by con-
struction. The M CS measure computed in this way, the percentage number of times each model
is contained in the set of superior models, tells us about the success rate of the models; that is,
how often do we expect each model to perform well. Meanwhile, the RM SE and RM AD criteria
measure the average performance of each model. We will see in Section 4 how this distinction
becomes relevant when selecting a forecasting model.

3.2. Model Selection

This section presents the models considered for the forecasting analysis. Table 2 presents the

starting set, M0, for the M CS approach explained in Section 3.1.

Table 2: Starting Set M0

F I(d)

ARM A(1, 1) HAR(3)
ARF IM A(1, d, 0) ARM A(2, 1) AR(22)
ARF IM A(0, d, 1) ARM A(1, 2) AR(30)
ARF IM A(1, d, 1) ARM A(3, 3) AR(50)
ARF IM A(2, d, 1) ARM A(4, 4)

I(1)

Model selection was based on two criteria.
As a ﬁrst criterion, we use the Bayesian Information Criterion (BIC) to select the number
of lags to include in both the ARF IM A and ARM A models in an independent Monte Carlo
analysis. The validity of the BIC for the class of processes with fractional diﬀerencing was proven
by Beran et al. (1998). The authors show that for this class of processes the penalty term must
tend to inﬁnity simultaneously with the sample size; thus, the Akaike Information Criterion is not
consistent while the BIC is. Note that we made the lag selection exercise independent from the
forecasting analysis to avoid the multiple testing problem.

We allow for a maximum of two lags at both components of the ARF IM A model, while the
maximum was set to four for the ARM A model. We use Maximum Likelihood for the estimation
of both classes of models with parameter speciﬁcations as reported in Appendix A.

Results from the lag selection exercise, presented in Appendix B, show that not many lags
are selected for the ARF IM A speciﬁcation for either component. This suggests that the short
term component is not that persistent once we control for the long memory behaviour. For the
ARM A speciﬁcation, perhaps not surprisingly, more lags are selected due to the fact that we are
not controlling for the long memory behaviour by way of the estimation of the fractional memory
d. Nonetheless, the maximum number of lags selected by the BIC is two.

6

As a second criterion, in addition to the preferred models from the lag selection exercise, we
follow previous works on long memory forecasting and consider high-order AR processes, AR(30)
and AR(50). Moreover, given the success of the HAR(3) model of Corsi (2009) on mimicking long
memory behaviour, see for instance Andersen et al. (2007) and Chiriac and Voev (2011), we include
both the unconstrained AR(22), and the HAR(3) models. The HAR(3) model is a constrained
AR(22) given by

where x(f )

t−1 = xt−1, x(w)

xt = a0 + a1x(f )
(cid:80)5

t−1 + a2x(w)
t−1 = 1
22
The HAR speciﬁcation has been used to model ﬁnancial data, it reﬂects the fact that diﬀerent
agents respond to uncertainty at distinct horizons. In this context, the three components of the
model seek to capture the daily (x(f )

t−1 + a3x(m)
(cid:80)22

), and monthly (x(m)

i=1 xt−i and, x(m)

) levels of uncertainty.

), weekly (x(w)

t−1 = 1
5

t−1 + (cid:15)t,

i=1 xt−i.

t

t

t

Note that including the HAR(3) model allows us to extend Corsi’s (2009) results in several
directions. We make comparisons against a larger set of models, and we use the M CS approach,
which is better suited for comparisons between multiple alternatives. Also, we include larger
forecast horizons, and we remove the uncertainty regarding the presence of long memory in the
data by comparing the performance of the HAR model in simulated long memory series, whereas
Corsi used real data.

3.3. Monte Carlo Design

All models were estimated by Maximum Likelihood (M LE) following the work of Baillie et al.
(2012) on long memory estimators for forecasting purposes. The authors ﬁnd the forecasts based on
M LE to be superior than the ones obtained from local Whittle estimators. Moreover, throughout,
we use a large sample size of T = 1, 000 to reduce the estimation error, and we consider values
of the long memory parameter in the stationary range, d ∈ (0, 1/2). Furthermore, given the rise
of Climate Econometrics studies keen on producing long horizon forecasts, we consider it relevant
to evaluate forecast performances to horizons as far as h = 300, which correspond to twenty-ﬁve
years of monthly forecasts.

Table 3 presents the Monte Carlo design for the forecasting analysis.

Table 3: Monte Carlo Design

• Generate series of size T + h using the long memory generating processes considered,
Section 2, Table 1. The model calibrations are reported in Table 9 in Appendix A.
• Fit by Maximum Likelihood the competing models in the starting set M0, Table 2,
for a sample size T .
• Construct forecasts from each model for horizons h ∈ {5, 10, 30, 50, 100, 300}.
• Determine the M CS and compute the RM SE and RM AD.
• Repeat the steps above R times, the number of replications.
• After the R replications, report the percentage number of times each model is contained in
the M CS, and the mean values of RM SE and RM AD, for each forecast horizon.

7

4. Monte Carlo Results

In this section, we present the results from the Monte Carlo simulations. The parameters for
the simulations are presented in Appendix A. Throughout, for reasons of space, we focus on the
M AD loss function given that it is less sensitive to large misspredictions, see Hansen et al. (2003).
Nonetheless, tables using the M SE loss function, reported in an Online Appendix, show similar
results.

4.1. DGP 1: ARFIMA

As a benchmark, we present in Table 4 and Figure 1 the results from the Monte Carlo analysis

for an ARF IM A(1, d, 0) process, DGP 1, for d = 0.3.

h=5

10

30

50

100

300

DGP 1
d = 0.3
F I(d)
0.937
ARF IM A(1, d, 0) 0.933
ARF IM A(0, d, 1) 0.933
0.935
ARF IM A(1, d, 1)
0.935
ARF IM A(2, d, 1)
0.944
ARM A(1, 1)
ARM A(2, 1)
0.937
0.938
ARM A(1, 2)
0.937
ARM A(3, 3)
0.938
ARM A(4, 4)
0.936
HAR(3)
AR(22)
0.939
0.941
AR(30)
0.948
AR(50)
1.036
I(1)

RM AD M CS RM AD M CS RM AD M CS RM AD M CS RM AD M CS RM AD M CS
0.315
0.055
0.078
0.053
0.061
0.092
0.045
0.043
0.048
0.063
0.159
0.047
0.071
0.116
0.126

0.964
0.962
0.961
0.963
0.963
0.975
0.966
0.968
0.967
0.967
0.965
0.967
0.970
0.976
1.076

0.109
0.029
0.031
0.009
0.020
0.130
0.023
0.036
0.032
0.040
0.039
0.075
0.073
0.143
0.212

0.984
0.983
0.982
0.983
0.984
0.995
0.988
0.990
0.988
0.988
0.986
0.988
0.989
0.994
1.113

0.118
0.028
0.030
0.011
0.019
0.137
0.027
0.032
0.034
0.042
0.045
0.070
0.072
0.127
0.208

0.259
0.035
0.039
0.025
0.033
0.107
0.024
0.034
0.025
0.030
0.084
0.037
0.060
0.099
0.162

1.004
1.003
1.003
1.003
1.004
1.007
1.006
1.006
1.007
1.007
1.006
1.005
1.005
1.006
1.200

0.995
0.994
0.994
0.994
0.995
1.002
0.999
1.000
0.999
0.999
0.998
0.998
0.998
1.000
1.151

0.193
0.034
0.028
0.028
0.026
0.128
0.025
0.037
0.019
0.037
0.049
0.038
0.067
0.111
0.183

0.988
0.988
0.987
0.988
0.989
0.999
0.993
0.995
0.993
0.993
0.992
0.992
0.993
0.997
1.125

0.171
0.026
0.025
0.018
0.033
0.138
0.013
0.039
0.026
0.035
0.043
0.042
0.076
0.123
0.193

Table 4: Mean of the RM AD and proportion of times the model is in the M CS using the M AD loss function and
the TR statistic at a 95% conﬁdence level.

Table 4 shows that ARF IM A models are the preferred speciﬁcation for all forecast horizons
measured by the RM AD criterion, which is not surprising given that DGP 1 is indeed an ARF IM A
process. Turning to the M CS criterion, note that the no-change I(1) model gives the best forecast
performance for short horizons, while the F I(d) model is the preferred one for medium and large
forecast horizons, and its relative performance increases with the forecast horizon.

The results for the I(1) model are of particular interest. Note that it is the preferred model by
the M CS criterion for short forecast horizons, while it gives the worst performance by the RM AD
criterion for all horizons. As discussed in Section 3.1, these apparent conﬂicting results can be
explained given the complimentary nature of the forecast evaluation measures. Recalling that the
RM AD is a measure across all replications, the results suggest that when the I(1) is not in the
model conﬁdence set, its forecasts perform badly. Nonetheless, its success rate for short forecast
horizons could make it a reasonable alternative in some cases, which will probably relate to the
distance from the last observation before the forecast to the overall mean. If the last observation
is already near the mean, and given that all models will forecast a return to the mean given the
stationarity assumption, the no-change model could provide a good forecast alternative, the mean.
Looking at both criteria, we ﬁnd that high-order AR and ARM A models perform quite well
when forecasting a true ARF IM A process for short forecast horizons. In particular, the AR(50)
and ARM A(1, 1) fall in the superior set of models more than times than ARF IM A speciﬁcations

8

for h = 5, and h = 10; while the increase in the RM AD criterion relative to ARF IM A speciﬁca-
tions is not too large. A practitioner could in principle construct a weighted measure between the
two criteria depending on the problem at hand and choose to use this class of models.

Nonetheless, for medium and large forecast horizons, the F I(d) model is the one contained in
the M CS the most, and with a RM AD close to the minimum. In this sense, the F I(d) appears
to be a good overall model for forecasts at medium and large horizons. The superior performance
of the F I(d) model compared to the correct ARF IM A(1, d, 0) speciﬁcation may be explained
given the small value of the autoregressive coeﬃcient and the estimation error in the long memory
parameter. The table suggests that the F I(d) model seems to capture enough information for
forecasting purposes in the long horizon once the short memory component fades away.2

Figure 1: Proportion of times the top performing models are in the M CS at a 95% conﬁdence level when forecasting
DGP 1 with diﬀerent degrees of memory at several horizons.

Furthermore, Figure 1 allows us to contrast the performance of high-order AR models and
ARF IM A models to diﬀerent degrees of memory.3 The ﬁgure shows that for h = 5 and h = 10, and
for all degrees of memory, the AR(50) produces better or similar forecast performance according
to the M CS criterion than the F I(d) model. Yet, the F I(d) models tend to lead in forecast
performance as the horizon increases.

Finally, the ﬁgure allows us to compare the HAR(3) model against the AR(22) model. Note
the crossing in preferred model according to the M CS criterion between the AR(22) and HAR(3)
models as both the forecast horizon, and degree of memory increase. The ﬁgure shows that for
h = 5, the AR(22) model is always on top of the HAR(3) model. Nonetheless, the preferred
model between the two switches from the AR(22) to the HAR(3) model as the forecast horizon
increases. Furthermore, the crossing happens sooner for higher degrees of memory. This suggests

2Results allowing more short-term dynamics are presented in the Online Appendix.
3For ease of exposition, we present a subset of the top performing models in the ﬁgures; nonetheless, we present

plots with all competing models in the Online Appendix.

9

that the structure imposed by the HAR(3) speciﬁcation helps to improve forecasting performance
for higher degrees of memory, and for larger forecast horizons, at the cost of lower performance at
small horizons.

Overall, Table 4, and Figure 1 extend the ﬁndings of previous studies on forecasting long
memory when the long memory is generated by ARF IM A processes. They show that high-order
AR models are good alternatives for short forecast horizons, while extending the analysis to show
that ARF IM A models are better suited for medium and large forecast horizons. Moreover, we
ﬁnd that the constraints imposed by the HAR model improve forecasting performance over the
unconstrained same-order AR model for higher degrees of memory, and longer forecast horizons.

4.2. DGP 2: Cross-Sectional Aggregation

Results from the Monte Carlo analysis for the cross-sectional aggregated processes, DGP 2,

are presented in Table 5, and Figure 2.

h=5

10

30

50

100

300

DGP 2
d = 0.3
F I(d)
1.027
ARF IM A(1, d, 0) 1.019
ARF IM A(0, d, 1)
1.020
ARF IM A(1, d, 1) 1.019
ARF IM A(2, d, 1) 1.019
1.029
ARM A(1, 1)
1.022
ARM A(2, 1)
1.026
ARM A(1, 2)
1.026
ARM A(3, 3)
1.024
ARM A(4, 4)
1.021
HAR(3)
AR(22)
1.023
1.025
AR(30)
1.032
AR(50)
1.075
I(1)

RM AD M CS RM AD M CS RM AD M CS RM AD M CS RM AD M CS RM AD M CS
0.206
0.062
0.105
0.072
0.071
0.084
0.049
0.037
0.054
0.060
0.173
0.040
0.035
0.071
0.191

1.088
1.084
1.085
1.084
1.086
1.097
1.089
1.093
1.092
1.090
1.087
1.089
1.092
1.097
1.164

0.172
0.036
0.046
0.020
0.007
0.095
0.032
0.024
0.024
0.025
0.017
0.060
0.070
0.145
0.227

1.267
1.266
1.266
1.266
1.268
1.275
1.272
1.274
1.275
1.273
1.275
1.271
1.271
1.273
1.479

0.135
0.037
0.047
0.045
0.040
0.132
0.016
0.035
0.026
0.023
0.059
0.038
0.042
0.105
0.220

0.134
0.034
0.039
0.028
0.019
0.121
0.034
0.026
0.028
0.028
0.019
0.050
0.063
0.142
0.235

1.161
1.159
1.159
1.161
1.164
1.184
1.172
1.178
1.173
1.171
1.168
1.168
1.171
1.177
1.275

1.192
1.191
1.191
1.192
1.196
1.216
1.205
1.212
1.205
1.204
1.203
1.201
1.203
1.208
1.326

0.149
0.038
0.059
0.041
0.060
0.107
0.028
0.033
0.030
0.029
0.077
0.033
0.037
0.112
0.208

0.136
0.037
0.032
0.048
0.024
0.142
0.026
0.036
0.022
0.023
0.052
0.033
0.047
0.120
0.222

1.228
1.227
1.227
1.227
1.230
1.243
1.235
1.241
1.237
1.235
1.236
1.233
1.234
1.237
1.387

Table 5: Mean of the RM AD and proportion of times the model is in the M CS using the M AD loss function and
the TR statistic at a 95% conﬁdence level.

Note that the I(1) model is the one contained in the superior set of models the most for forecasts
horizons up to 100, while performing last according to the RM AD criterion. Once again, this seems
to suggest a higher success rate for the I(1) model at short and medium forecast horizons, but
at the price of high variability on its performance, which may relate to the distance from the last
observed value to the mean.

Looking for a better balance between the criteria, note that the ARF IM A class of models is
the preferred one according to the RM AD criterion for all forecast horizons, with the F I(d) in
particular remaining among the top performing according to the M CS criterion. In this sense,
a weighted average between the criteria would presumably result in selecting the F I(d) model
as a well suited model for forecasting purposes, specially for medium and large forecast horizons.
Among the short memory models, the ARM A(1, 1) and AR(50) models could provide good forecast
alternatives for short forecast horizons, as seen by their relatively high value for the M CS criterion,
and RM AD relatively close to the minimum.

We can see the eﬀect that the degree of long memory has on the results in Figure 2. We plot
the percentage of number of times the models are contained in the M CS for diﬀerent degrees of
memory, for all forecast horizons.

10

Figure 2: Proportion of times the top performing models are in the M CS at a 95% conﬁdence level when forecasting
DGP 2 with diﬀerent degrees of memory at several horizons.

The ﬁgure extends the ﬁndings in Table 5, it shows the good performance of the ARM A(1, 1)
and AR(50) models at short and medium forecast horizons, providing similar results to the F I(d)
speciﬁcation. Nonetheless, the ﬁgure shows the increase in relative forecast performance of the
F I(d) model when the forecast horizon increases. Also, the plot shows the increase in forecast
performance of the HAR(3) model for large forecast horizons. In particular, while the performance
in small forecast horizons is inferior in comparison to the unconstrained AR(22), the constrains
seem to introduce the additional structure needed for good medium and large horizon forecasts.
Section 5 will analyse this feature further.

Overall, Table 5 and Figure 2 indicate that the ARF IM A class of models are a good speciﬁca-
tion for forecast construction when working with long memory series generated by cross-sectional
aggregation, DGP 2, particularly for medium and large forecast horizons. This in the sense that
they obtain a good balance performance among both evaluation criteria. Among the short memory
models, the AR(50) and ARM A(1, 1) could provide sensible alternatives for smaller forecasting
periods. Finally, the HAR(3) model starts to show good performance at larger horizons, with
slightly inferior performance at short forecast horizons.

4.3. DGP 3: Error Duration Model

Table 6 presents the results from the Monte Carlo analysis for processes generated using the

error duration model, DGP 3, for long memory parameter d = 0.3.

We can see from the table that the ARF IM A class of models provide the best performance
measured by the RM AD criterion for all forecast horizons, while showing the best performance by
the M CS criterion for h = 30 and larger. Thus, the ARF IM A class of models seem to provide
the best forecast performance for series generated by the error duration model, DGP 3.

Turning to short memory alternatives, note the relatively good performance of the AR(50)
model for all forecast horizons. Thus, even though the F I(d) model appears as the best performing

11

h=5

10

30

50

100

300

DGP 3
d = 0.3
1.097
F I(d)
1.075
ARF IM A(1, d, 0)
1.074
ARF IM A(0, d, 1)
ARF IM A(1, d, 1) 1.067
ARF IM A(2, d, 1) 1.067
1.077
ARM A(1, 1)
1.074
ARM A(2, 1)
1.070
ARM A(1, 2)
1.069
ARM A(3, 3)
1.070
ARM A(4, 4)
1.076
HAR(3)
1.077
AR(22)
1.078
AR(30)
1.085
AR(50)
1.212
I(1)

RM AD M CS RM AD M CS RM AD M CS RM AD M CS RM AD M CS RM AD M CS
0.277
0.063
0.059
0.037
0.041
0.071
0.050
0.044
0.065
0.079
0.145
0.065
0.096
0.143
0.068

1.123
1.105
1.109
1.100
1.100
1.109
1.108
1.103
1.102
1.103
1.106
1.108
1.109
1.116
1.261

0.130
0.038
0.028
0.012
0.012
0.129
0.031
0.018
0.023
0.043
0.100
0.052
0.074
0.146
0.164

1.140
1.136
1.156
1.135
1.135
1.137
1.137
1.136
1.136
1.135
1.137
1.136
1.136
1.137
1.371

0.217
0.042
0.029
0.021
0.031
0.073
0.043
0.030
0.040
0.047
0.099
0.057
0.085
0.134
0.089

1.131
1.120
1.130
1.118
1.118
1.125
1.125
1.120
1.119
1.119
1.121
1.122
1.124
1.131
1.298

1.133
1.124
1.137
1.123
1.123
1.129
1.128
1.125
1.124
1.124
1.126
1.126
1.126
1.133
1.309

0.193
0.035
0.023
0.021
0.021
0.082
0.042
0.030
0.033
0.050
0.095
0.066
0.089
0.125
0.095

1.136
1.130
1.145
1.129
1.129
1.133
1.133
1.130
1.130
1.130
1.131
1.131
1.131
1.134
1.324

0.126
0.037
0.028
0.012
0.016
0.110
0.040
0.020
0.025
0.033
0.104
0.062
0.077
0.149
0.161

0.162
0.041
0.027
0.009
0.025
0.089
0.043
0.024
0.033
0.039
0.086
0.074
0.094
0.138
0.116

Table 6: Mean of the RM AD and proportion of times the model is in the M CS using the M AD loss function and
the TR statistic at a 95% conﬁdence level.

model, high-order AR models seem to produce good short and medium horizon forecast alternatives
for DGP 3. Moreover, contrasting the performance of the HAR(3) model against the AR(22)
model, the table shows the gains in performance of imposing some structure into the higher-order
AR models when forecasting DGP 3, with similar or better performance on both criteria.

Figure 3: Proportion of times the top performing models are in the M CS at a 95% conﬁdence level when forecasting
DGP 3 with diﬀerent degrees of memory at several horizons.

Figure 3 presents the proportion of times the models are contained in the M CS when forecasting
DGP 3 for diﬀerent degrees of memory. The ﬁgure shows the relative performance increase of the
F I(d) model over high-order AR models as both the degree of memory, and the forecast horizon
increase; thus, it extends the insights gained from the table to all degrees of memory considered.
Also, note that the HAR(3) model is always on top of the unconstrained AR(22) model, if slightly

12

for medium horizons.

Overall, Table 6 and Figure 3 convey one of the main insights from this paper; that is, short
memory models are good alternatives for short horizon forecasts, while long memory models are
preferred for medium and long horizon forecasts of long memory processes, regardless of the memory
generating mechanism.

5. Discussion

Looking at the relative performance between an unconstrained high-order AR model and a
constrained HAR, the results from the Monte Carlo simulation can be further analysed in the
context of the bias-variance trade-oﬀ typically studied in regression analysis.

All processes considered in this paper are long memory in the covariance sense; hence, the
models are ﬁtted to capture the information contained in the autocorrelation function and use it
for forecasting purposes. In other words, the models select {ai}T
i=1 aixt−i
representation, with the aim of replicating the autocorrelation function.

i=0 in the xt = a0 + (cid:80)T

ARF IM A and ARM A models diﬀer in terms of the way to select the coeﬃcients ai. ARF IM A
models impose a hyperbolic rate by the fractional diﬀerencing operator (1 − L)d, see Equation 1,
while high-order AR models are more ﬂexible by selecting each coeﬃcient individually. In this sense,
the fractional models need just one parameter to establish the inﬁnite list of coeﬃcients, and are
hence of low variance. Nonetheless, the uncertainty surrounding the estimation of the long memory
parameter may introduce some bias. As an alternative, high-order AR models are more ﬂexible.
Hence, they can reduce the bias of the coeﬃcients, but suﬀer from increased variance given the
number of estimated parameters. This distinction can be particularly important in the scenario of
having small estimation samples, something we abstract from in this study. Given the uncertainty
associated to estimating an increasing number of parameters, we would expect the performance
of high-order AR models to deteriorate in short series. Yet, as the Monte Carlo analysis showed,
this ﬂexibility can produce good forecast performance at short horizons, particularly when the
degree of memory is small. Nonetheless, AR models lose forecasting power as the forecast horizon
gets larger. We could increase the order of the autoregressive process to incresae the forecasting
performance at long horizons, but the estimation becomes unstable.

In this context, HAR models are a compromise between the rigid ARF IM A and ﬂexible
high-order AR model speciﬁcations. They incorporate high-order autoregressive speciﬁcations
while greatly restricting the number of parameters to be estimated. This arrangement allows the
HAR model to provide similar forecast performance at medium forecast horizons as same-order
unrestricted AR models, while providing better long horizon forecasts. Yet, as shown in the Monte
Carlo analysis, HAR models may suﬀer a forecast performance loss at short horizons.

To further illustrate this point, Figure 4 compares the forecasting performance of constrained
AR models in the spirit of the HAR model against their unconstrained speciﬁcations. In particular,
in addition to the HAR(3) and AR(22) models, we show an unrestricted AR(50) model and a
HAR(4) given by

where x(f )
the HAR(4) model just described is a constrained AR(50).

t−1 = xt−1, x(w)

i=1 xt−i, x(m)

t−1 = 1
5

xt = a0 + a1x(f )
(cid:80)5

t−1 + a2x(w)
t−1 = 1
22

t−1 + a3x(m)
(cid:80)22

t−1 + a4x(b)
i=1 xt−i, and x(b)

t−1 + (cid:15)t,

t−1 = 1
50

(cid:80)50

i=1 xt−i. Note that

Figure 4 shows the average number of times two high-order AR processes and their comparable
HAR speciﬁcations are contained in the M CS when forecasting DGP 1. The ﬁgure displays the

13

Figure 4: Proportion of times the models are in the M CS at a 95% conﬁdence level when forecasting DGP 1 with
diﬀerent degrees of memory at several horizons. For the plots, the starting set contains only the six models shown.

superior performance of unconstrained autoregressive processes against constrained ones of the
same order for short forecast horizons. Nonetheless, it shows the increase in relative performance
for the constrained versions at large forecast horizons. In particular, for h = 5 and h = 10, the
unconstrained AR models give better performance than equivalent order HAR alternatives for all
degrees of memory; while at h = 300, the HAR4 is always on top of the unconstrained AR(50).4
The bias-variance trade-oﬀ has been a topic of great interest in the literature of regressions with
a high number of covariates, it thus would be compelling to adapt shrinkage and sparse methods to
lag selection in the context of long memory forecasting. This line of inquiry is left open for future
research.

6. Illustrative Example

In this section, we evaluate the forecasting performance of the competing models on real data.
We select the Realized Variance (RV ) of the S&P 500 for illustration. As it is well known, the
Realized Variance is a measure of volatility. We obtain the RV series from the Oxford-Man
Institute’s “Realised Library” computed on the basis of intradaily observations spaced into 5-
minute intervals and subsampled at a 1-minute frequency. The sample runs from January 3, 2000
until December 30, 2015.

The RV has been proven to have long memory by, among others, Martens et al. (2009) and
Andersen et al. (2003). In particular, notice that by construction the S&P 500 series is an aggre-
gated measure; thus, it is in line with the cross-sectional argument for long memory. Furthermore,
Parke (1999) argues that given the diﬀerence between information quality among agents, the error
duration model is capable of explaining the long memory in volatility. Hence, realized variance can

4The Online Appendix shows that this result extends to the other DGP s considered.

14

be argued to have long memory by the theoretical explanations considered in this work, making it
a good ﬁt for the exercise.

Figure 5: Realized Variance and its autocorrelation function.

Figure 5 presents the RV series and its autocorrelation function. The autocorrelation function
shows behaviour similar to those of long memory processes, remaining signiﬁcant at large lags. The
estimates for the long memory parameter are 0.4501, 0.4675, and 0.3756 by the semi-parametric
estimator of Geweke and Porter-Hudak (1983), the local Whittle estimator of Robinson (1995) and
K¨unsch (1986), and the Maximum Likelihood Estimator of Sowell (1992), respectively.

We use as estimation sample an increasing window starting from the period between January
3, 2000 to December 30, 2014. The last estimation window runs from January 3, 2000 to December
30, 2015. For each estimation window we construct forecasts for 5, 10, 22, 66, 120 and 254 periods
ahead, and compute the Model Conﬁdence Set for each estimation window and forecast horizon.
Table 7 presents the results from the forecasting exercise on the ﬁrst estimation window. As
the table shows,5 short memory models give the best forecast performance for horizons up to 22
periods ahead, in line with the results of Martens et al. (2009). In particular, for short forecast
horizons either the ARM A(1, 2) or the ARM A(1, 2) is the one with minimum RM AD, and the
only one contained in the M CS. Nonetheless, the ARF IM A type of models tends to dominate in
forecast performance as the forecast horizon increases, in line with our results from the Monte Carlo
analysis. For horizons h ∈ {66, 120, 254}, representing 3, 6 and 12 months, the ARF IM A(0, d, 1)
is the only one contained in the M CS and with lowest RM AD.

The results from Table 7 can be better understood by looking at Figure 6. The ﬁgure shows
the true RV and the forecasts from the best competing models. The top plot shows the forecasts
up to a month ahead, while the bottom one extends it to a year ahead.

5The ARF IM A(2, d, 1) showed convergence problems and thus was excluded from this exercise.

15

Realized Variance

h=5

10

22

66

120

254

F I(d)
ARF IM A(1, d, 0)
ARF IM A(0, d, 1)
ARF IM A(1, d, 1)
ARM A(1, 1)
ARM A(2, 1)
ARM A(1, 2)
ARM A(3, 3)
ARM A(4, 4)
HAR(3)
AR(22)
AR(30)
AR(50)
I(1)

RM AD M CS RM AD M CS RM AD M CS RM AD M CS RM AD M CS RM AD M CS
0.0049
0.0051
0.0054
0.0054
0.0057
0.0043
0.0041
0.0064
0.0056
0.0055
0.0052
0.0057
0.0056
0.0074

0.0056
0.0057
0.0060
0.0060
0.0064
0.0044
0.0043
0.0065
0.0058
0.0057
0.0057
0.0064
0.0062
0.0080

0.0054
0.0056
0.0054
0.0054
0.0060
0.0077
0.0078
0.0074
0.0075
0.0068
0.0069
0.0064
0.0070
0.0061

0.0053
0.0057
0.0052
0.0053
0.0066
0.0083
0.0084
0.0081
0.0082
0.0077
0.0077
0.0071
0.0079
0.0053

0.0063
0.0065
0.0062
0.0063
0.0076
0.0086
0.0086
0.0085
0.0085
0.0083
0.0083
0.0079
0.0084
0.0070

0.0060
0.0060
0.0063
0.0063
0.0065
0.0048
0.0050
0.0060
0.0053
0.0056
0.0057
0.0063
0.0059
0.0083

0
0
0
0
0
0
1
0
0
0
0
0
0
0

0
0
1
0
0
0
0
0
0
0
0
0
0
0

0
0
1
0
0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
1
0
0
0
0
0
0
0

0
0
0
0
0
1
0
0
0
0
0
0
0
0

0
0
1
0
0
0
0
0
0
0
0
0
0
0

Table 7: RM AD and indicator function for the model being contained in the M CS using the M AD loss function
and the TR statistic at a 95% conﬁdence level.

Figure 6: Realized Variance and forecasts.

As the ﬁgure shows, all models determine that the RV is away from its mean by the end of
the estimation period. The main diﬀerence between models being the speed of convergence to the
mean. The fast convergence of the ARM A speciﬁcations allows them to capture the quick bounce
at the beginning of the forecasting period much better than the ARF IM A alternatives, which
results in better short horizon forecasts. Nonetheless, the long memory alternatives are better
suited to capture the long horizon dynamics of the RV as the forecast horizon increases.

For robustness, we extend the analysis to the exercise with an increasing estimation window.
Table 8 presents the average number of times the competing models are in the superior set of
models. For ease of exposition, we have pooled the results between long memory alternatives

16

(ARF IM A models) and short memory ones (ARM A models).

h = 5

Realized Variance
10
ARF IM A alternatives 0.268 0.342 0.342 0.425 0.720 0.705
ARM A alternatives
I(1) no-change model

0.409 0.370 0.335 0.264
0.323 0.287 0.323 0.311

0.008
0.311

0
0.335

120

254

66

22

Table 8: Average number of times the type of model is in the M CS at a 95% conﬁdence level for the increasing
estimation window exercise. In total, 254 estimation windows were considered.

The table conﬁrms our simulations and ﬁrst estimation window results. It shows that for the
short horizon, the ARM A models are the ones contained the most in the M CS. As we increase the
forecasting horizon, the ARF IM A alternatives are the ones contained the most in the M CS. In
particular, no short memory alternative is contained in the M CS for forecast up to a year ahead,
for all estimation windows.

7. Conclusions

This paper argues that the most cited theoretical arguments behind the presence of long memory
in the data do not correspond to the fractional diﬀerence operator. In this context, it evaluates the
forecasting performance of ARF IM A models when the memory is generated from nonfractional
sources.

We ﬁnd that high-order AR models produce comparable forecasts as ARF IM A models at short
horizons. Nonetheless, as the forecast horizon increases, the ARF IM A models tend to dominate
in terms of forecast performance. Hence, ARF IM A models are well suited for medium and long
horizon forecasts of long memory regardless of the generating mechanism, while high-order AR
models may be good alternatives for forecasts at short horizons. In particular, we ﬁnd that if the
long memory is generated by the error duration model, the fractionally integrated model produces
the best forecast performance at medium and large horizons for all degrees of memory, while
remaining competitive at short horizons.

Additionally, by making a compromise between ﬂexibility and complexity, we ﬁnd that the
structure imposed by the HAR model induces a trade-oﬀ in forecast performance at diﬀerent
forecast horizons. In other words, the HAR model produces better long horizon forecasts, similar
medium horizon forecasts, and similar or inferior short horizon forecasts, than same-order AR
model speciﬁcations.

Our results have implications for Climate Econometrics and Financial Econometrics models
dealing with forecasts at diﬀerent horizons. As an illustrative example, we show for the Realized
Variance of the S&P 500 that while short memory models are well suited for forecasts up to a
month ahead, the ARF IM A class of models dominate in forecast performance for longer horizons.

Acknowledgements

The author would like to thank Niels Haldrup for all the insightful comments, the paper greatly
improved because of him. The paper was written in part while on a visiting stay at Erasmus
University Rotterdam. The author would like to thank the Faculty and Administrative Staﬀ at
in particular to Michel van der Wel and
the Erasmus School of Economics for all their help;

17

Dick van Dijk for the interesting conversations. This work was supported by CREATES - Center
for Research in Econometric Analysis of Time Series (DNRF78), funded by the Danish National
Research Foundation.

Appendix

A. Parameters

All

DGP 1
DGP 2
DGP 3

εt ∼ i.i.d.N (0, 1) ∀t
T = 1, 000; R = 1, 000
φ1 = 0.2
N = 10, 000; p = 1.4
pk = (Γ(k + d)Γ(2 − d))/(Γ(k + 2 − d)Γ(d))

Table 9: Parameter conﬁguration for the Monte Carlo analysis

B. Lag Selection Exercise

Model
DGP 2

DGP 3

d
0.2
0.4
0.2
0.4

ARF IM A ARM A

(1,0)
(1,0)
(0,1)
(0,1)

(2,1)
(2,1)
(1,1)
(1,2)

Table 10: Results from the lag selection for the AR and M A components. We show the preferred model for each
criteria from R = 1, 000 replications using a sample size of T = 1, 000. Results based on the Bayesian Information
Criteria following the work of Beran et al. (1998) on consistency of information criteria for long memory processes.

References

Andersen, T. G., Bollerslev, T., Diebold, F. X., 2007. Roughing It Up: Including Jump Components in the Mea-
surement, Modeling, and Forecasting of Return Volatility. Review of Economics and Statistics 89 (November),
701–720.

Andersen, T. G., Bollerslev, T., Diebold, F. X., Labys, P., 2003. Modeling and forecasting realized volatility. Econo-

metrica 71 (2), 579–625.

Baillie, R. T., 1996. Long memory processes and fractional integration in econometrics. Journal of Econometrics

73 (1), 5–59.

Baillie, R. T., Kongcharoen, C., Kapetanios, G., 2012. Prediction from ARFIMA models: Comparisons between

MLE and semiparametric estimation procedures. International Journal of Forecasting 28 (1), 46–53.

Barkoulas, J. T., Baum, C. F., 1997. Fractional Diﬀerencing Modeling and Forecasting of Eurocurrency Deposit

Rates. The Journal of Financial Resarch XX (3), 355–372.

Beran, J., 1994. Statistics for long-memory processes. Chapman & Hall.
Beran, J., Bhansali, R. J., Ocker, D., 1998. On uniﬁed model selection for stationary and non stationary short and

long memory autoregressive processes. Biometrika 85 (4), 921–934.

Bhardwaj, G., Swanson, N. R., 2006. An empirical investigation of the usefulness of ARFIMA models for predicting

macroeconomic and ﬁnancial time series. Journal of Econometrics 131 (1-2), 539–578.

Chiriac, R., Voev, V., 2011. Modelling and Forecasting Multivariate Realized Volatility. Journal of Applied Econo-

metrics 26 (6), 922–947.

18

Corsi, F., 2009. A simple approximate long-memory model of realized volatility. Journal of Financial Econometrics

7 (2), 174–196.

Crato, N., Ray, B. K., 1996. Model selection and forecasting for long-range dependent processes. Journal of Fore-

casting 15 (2), 107–125.

Geweke, J., Porter-Hudak, S., 1983. The estimation and application of long memory time series models. Journal of

Time Series Analysis 4 (4), 221–238.

Granger, C. W., 1980. Long memory relationships and the aggregation of dynamic models. Journal of Econometrics

14 (2), 227–238.

Granger, C. W., Joyeux, R., 1980. An Introduction to Long Memory Time Series Models and Fractional Diﬀerencing.

Journal of Time Series Analysis 1 (1), 15–29.

Haldrup, N., Vera Vald´es, J. E., 2017. Long memory, fractional integration, and cross-sectional aggregation. Journal

of Econometrics 199 (1), 1–11.

Hansen, P. R., Lunde, A., Nason, J. M., 2003. Choosing the Best Volatility Models: The Model Conﬁdence Set

Approach. Oxford Bulletin of Economics and Statistics 65 (401), 839–861.

Hansen, P. R., Lunde, A., Nason, J. M., 2011. The Model Conﬁdence Set. Econometrica 79 (2), 453–497.
Hosking, J. R. M., 1981. Fractional diﬀerencing. Biometrika 68 (1), 165–176.
Hurst, H. E., 1956. The Problem of Long-Term Storage in Reservoirs. International Association of Scientiﬁc Hydrol-

ogy. Bulletin 1 (3), 13–27.

K¨unsch, H., 1986. Discrimination between monotonic trends and long range dependence. Journal of Applied Proba-

bility 23 (4), 1025–1030.

Linden, M., 1999. Time series properties of aggregated AR(1) processes with uniformly distributed coeﬃcients.

Economics Letters 64 (1), 31–36.

Martens, M., van Dijk, D., de Pooter, M., 2009. Forecasting S&P 500 volatility: Long memory, level shifts, lever-
age eﬀects, day-of-the-week seasonality, and macroeconomic announcements. International Journal of Forecasting
25 (2), 282–303.

Oppenheim, G., Viano, M. C., 2004. Aggregation of random parameters ornstein-uhlenbeck or ar processes: Some

convergence results. Journal of Time Series Analysis 25 (3), 335–350.

Parke, W., 1999. What is fractional integration? Review of Economics and Statistics 81 (4), 632–638.
Ray, B. K., 1993. Modeling Long Memory Processes for Optimal Long Range Prediction. Journal of Time Series

Analysis 14 (5), 511–525.

Robinson, P. M., 1978. Statistical Inference for a Random Coeﬃcient Autoregressive Model. Scandinavian Journal

of Statistics 5 (3), 163–168.

Robinson, P. M., 1995. Gaussian Semiparametric Estimation of Long Range Dependence. The Annals of Statistics

23 (5), 1630–1661.

Sowell, F., 1992. Maximum likelihood estimation of stationary univariate fractionally integrated time series models.

Journal of Econometrics 53 (1-3), 165–188.

Zaﬀaroni, P., 2004. Contemporaneous aggregation of linear dynamic models in large economies. Journal of Econo-

metrics 120 (1), 75–102.

19

