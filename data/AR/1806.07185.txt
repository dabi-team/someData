8
1
0
2

n
u
J

9
1

]

G
L
.
s
c
[

1
v
5
8
1
7
0
.
6
0
8
1
:
v
i
X
r
a

1

MixedbatchesandsymmetricdiscriminatorsforGANtrainingThomasLucas*1CorentinTallec*2JakobVerbeek1YannOllivier3AbstractGenerativeadversarialnetworks(GANs)arepow-erfulgenerativemodelsbasedonprovidingfeed-backtoagenerativenetworkviaadiscriminatornetwork.However,thediscriminatorusuallyas-sessesindividualsamples.Thispreventsthedis-criminatorfromaccessingglobaldistributionalstatisticsofgeneratedsamples,andoftenleadstomodedropping:thegeneratormodelsonlypartofthetargetdistribution.Weproposetofeedthediscriminatorwithmixedbatchesoftrueandfakesamples,andtrainittopredicttheratiooftruesamplesinthebatch.Thelatterscoredoesnotdependontheorderofsamplesinabatch.Ratherthanlearningthisinvariance,weintroduceagenericpermutation-invariantdiscriminatorar-chitecture.Thisarchitectureisprovablyauni-versalapproximatorofallsymmetricfunctions.Experimentally,ourapproachreducesmodecol-lapseinGANsontwosyntheticdatasets,andobtainsgoodresultsontheCIFAR10andCelebAdatasets,bothqualitativelyandquantitatively.1.IntroductionEstimatinggenerativemodelsfromunlabeleddataisoneofthechallengesinunsupervisedlearning.Recently,sev-erallatentvariableapproacheshavebeenproposedtolearnﬂexibledensityestimatorstogetherwithefﬁcientsampling,suchasgenerativeadversarialnetworks(GANs)(Good-fellowetal.,2014),variationalautoencoders(Kingma&Welling,2014;Rezendeetal.,2014),iterativetransforma-tionofnoise(Sohl-Dicksteinetal.,2015),ornon-volumepreservingtransformations(Dinhetal.,2017).InthisworkwefocusonGANs,currentlythemostcon-*Equalcontribution1Universit´eGrenobleAlpes,Inria,CNRS,GrenobleINP,LJK,38000Grenoble,France.2Universit´eParisSud,INRIA,´equipeTAU,Gif-sur-Yvette,91190,France.3FacebookArtiﬁcialIntelligenceResearchParis,France.Cor-respondenceto:CorentinTallec<corentin.tallec@inria.fr>,ThomasLucas<thomas.lucas@inria.fr>.Proceedingsofthe35thInternationalConferenceonMachineLearning,Stockholm,Sweden,PMLR80,2018.Copyright2018bytheauthor(s).vincingsourceofsamplesofnaturalimages(Karrasetal.,2018).GANsconsistofageneratorandadiscriminatornetwork.Thegeneratormapssamplesfromalatentrandomvariablewithabasicprior,suchasamulti-variateGaussian,totheobservationspace.Thisdeﬁnesaprobabilitydistribu-tionovertheobservationspace.Adiscriminatornetworkistrainedtodistinguishbetweengeneratedsamplesandtruesamplesintheobservationspace.Thegenerator,ontheotherhand,istrainedtofoolthediscriminator.Inanideal-izedsettingwithunboundedcapacityofbothnetworksandinﬁnitetrainingdata,thegeneratorshouldconvergetothedistributionfromwhichthetrainingdatahasbeensampled.Inmostadversarialsetups,thediscriminatorclassiﬁesindi-vidualdatasamples.Consequently,itcannotdirectlydetectdiscrepanciesbetweenthedistributionofgeneratedsamplesandglobalstatisticsofthetrainingdistribution,suchasitsmomentsorquantiles.Forinstance,ifthegeneratormodelsarestrictedpartofthesupportofthetargetdistributionverywell,thiscanfoolthediscriminatoratthelevelofindividualsamples,aphenomenonknownasmodedropping.Insuchacasethereislittleincentiveforthegeneratortomodelotherpartsofthesupportofthetargetdistribution.Amorethor-oughexplanationofthiseffectcanbefoundin(Salimansetal.,2016).Inordertoaccessglobaldistributionalstatistics,imagineadiscriminatorthatcouldsomehowtakefullprobabilitydis-tributionsasitsinput.Thisisimpossibleinpractice.Still,itispossibletofeedlargebatchesoftrainingorgeneratedsamplestothediscriminator,asanapproximationofthecorrespondingdistributions.Thediscriminatorcancomputestatisticsonthosebatchesanddetectdiscrepanciesbetweenthetwodistributions.Forinstance,ifalargebatchexhibitsonlyonemodefromamultimodaldistribution,thediscrimi-natorwouldnoticethediscrepancyrightaway.Eventhoughasinglebatchmaynotencompassallmodesofthedistri-bution,itwillstillconveymoreinformationaboutmissingmodesthananindividualexample.Trainingthediscriminatortodiscriminate“pure”batcheswithonlyrealoronlysyntheticsamplesmakesitstasktooeasy,asasinglebadsamplerevealsthewholebatchassyn-thetic.Instead,weintroducea“mixed”batchdiscriminationtaskinwhichthediscriminatorneedstopredicttheratioofrealsamplesinabatch. 
 
 
 
 
 
2

MixedbatchesandsymmetricdiscriminatorsforGANtrainingThisuseofbatchesdiffersfromtraditionalminibatchlearn-ing.Thebatchisnotusedasacomputationaltricktoin-creaseparallelism,butasanapproximatedistribution,onwhichtocomputeglobalstatistics.Anaivewayofdoingsowouldbetoconcatenatethesam-plesinthebatch,feedingthediscriminatorasingletensorcontainingallthesamples.However,thisisparameter-hungry,andthecomputedstatisticsarenotautomaticallyinvarianttotheorderofsamplesinthebatch.Tocomputefunctionsthatdependonthesamplesonlythroughtheirdis-tribution,itisnecessarytorestricttheclassofdiscriminatornetworkstopermutation-invariantfunctionsofthebatch.Forthis,weadaptandextendanarchitecturefromMcGre-gor(2007)tocomputesymmetricfunctionsoftheinput.Weshowthiscanbedonewithminimalmodiﬁcationtoexistingarchitectures,atanegligiblecomputationaloverheadw.r.t.ordinarybatchprocessing.Insummary,ourcontributionsarethefollowing:•Naivelytrainingthediscriminatortodiscriminate“pure”batcheswithonlyrealoronlysyntheticsam-plesmakesitstaskwaytooeasy.Weintroduceadis-criminationlossbasedonmixedbatchesoftrueandfakesamples,thatavoidsthispitfall.Wederivetheassociatedoptimaldiscriminator.•Weprovideaprincipledwayofdeﬁningneuralnet-worksthatarepermutation-invariantoverabatchofsamples.Weformallyprovethattheresultingclassoffunctionscomprisesallsymmetriccontinuousfunc-tions,andonlysymmetricfunctions.•WeapplytheseinsightstoGANs,withgoodexperi-mentalresults,bothqualitativelyandquantitatively.WebelievethatdiscriminatingbetweendistributionsatthebatchlevelprovidesanequallyprincipledalternativetoapproachestoGANsbasedondualityformulas(Nowozinetal.,2016;Gulrajanietal.,2017;Arjovskyetal.,2017).2.RelatedworkThetrainingofgenerativemodelsviadistributionalratherthanpointwiseinformationhasbeenexploredinseveralrecentcontributions.Batchdiscrimination(Salimansetal.,2016)usesahandmadelayertocomputebatchstatisticswhicharethencombinedwithsample-speciﬁcfeaturestoenhanceindividualsamplediscrimination.Karrasetal.(2018)directlycomputethestandarddeviationoffeaturesandfeeditasanadditionalfeaturetothelastlayerofthenet-work.Bothmethodsuseasinglelayerofhandcraftedbatchstatistics,insteadoflettingthediscriminatorlearnarbitrarybatchstatisticsusefulfordiscriminationasinourapproach.Moreover,inbothmethodsthediscriminatorstillassessessinglesamples,ratherthanentirebatches.Radfordetal.(2015)reportedimprovedresultswithbatchnormalization+. . . . . . . . . Figure1.Graphicalrepresentationofourdiscriminatorarchitec-ture.EachconvolutionallayerofanotherwiseclassicalCNNarchitectureismodiﬁedtoincludepermutationinvariantbatchstatistics,denotedρ(x).Thisisrepeatedateverylayersothatthenetworkgraduallybuildsupmorecomplexstatistics.inthediscriminator,whichmayalsobeduetorelianceonbatchstatistics.Otherworks,suchas(Lietal.,2015)and(Dziugaiteetal.,2015),replacethediscriminatorwithaﬁxeddistributionallossbetweentrueandgeneratedsamples,themaximummeandiscrepancy,asthecriteriontotrainthegenerativemodel.ThishastheadvantageofrelievingtheinherentinstabilityofGANs,butlackstheﬂexibilityofanadaptivediscriminator.Thediscriminatorweintroducetreatsbatchesassetsofsamples.Processingsetsprescribestheuseofpermuta-tioninvariantnetworks.Therehasbeenalargebodyofworkaroundpermutationinvariantnetworks,e.g(McGre-gor,2007;2008;Qietal.,2016;Zaheeretal.,2017;Vaswanietal.,2017).Ourprocessingisinspiredby(McGregor,2007;2008)whichdesignsaspecialkindoflayerthatpro-videsthedesiredinvarianceproperty.ThenetworkfromMcGregor(2007)isamulti-layerperceptroninwhichthesinglehiddenlayerperformsabatchwisecomputationthatmakestheresultequivariantbypermutation.Hereweshowthatstackingsuchhiddenlayersandreducingtheﬁnallayerwithapermutationinvariantreduction,coversthewholespaceofcontinuouspermutationinvariantfunctions.Zaheeretal.(2017)ﬁrstprocesseachelementofthesetindependently,thenaggregatetheresultingrepresentationusingapermutationinvariantoperation,andﬁnallypro-cessthepermutationinvariantquantity.Qietal.(2016)process3Dpointclouddata,andinterleavelayersthatpro-cesspointsindependently,andlayersthatapplyequivarianttransformations.Theoutputoftheirnetworksareeitherpermutationequivariantforpointcloudsegmentation,orper-mutationinvariantforshaperecognition.Inourapproachwestackpermutationequivariantlayersthatcombinebatch3

MixedbatchesandsymmetricdiscriminatorsforGANtraining07001400210028003500Iteration0.000.030.060.090.120.15Discriminator loss(£100)°=0:2°=0:5°=0:307001400210028003500Iteration1.82.12.42.73.03.3Generator loss(£100)°=0:2°=0:5°=0:3Figure2.Effectofbatchsmoothingwithdifferentγ’sonthegen-eratoranddiscriminatorlosses.informationandsampleinformationateverylevel,andag-gregatetheseintheﬁnallayerusingapermutationinvariantoperation.Morecomplexapproachestopermutationinvarianceorequivarianceappearin(Guttenbergetal.,2016).Weprove,however,thatoursimplerarchitecturealreadycoversthefullspaceofpermutationinvariantfunctions.ImprovingthetrainingofGANshasreceivedalotofrecentattention.Forinstance,Arjovskyetal.(2017),Gulrajanietal.(2017)andMiyatoetal.(2018)constraintheLipschitzconstantofthenetworkandshowthatthisstabilizestrainingandimprovesperformance.Karrasetal.(2018)achievedimpressiveresultsbygraduallyincreasingtheresolutionofthegeneratedimagesastrainingprogresses.3.Adversariallearningwithpermutation-invariantbatchfeaturesUsingabatchofsamplesratherthanindividualsamplesasinputtothediscriminatorcanprovideglobalstatisticsaboutthedistributionsofinterest.Suchstatisticscouldbeusefultoavoidmodedropping.Adversariallearning(Goodfellowetal.,2014)caneasilybeextendedtothebatchdiscrimi-nationcase.ForaﬁxedbatchsizeB,thecorrespondingtwo-playeroptimizationprocedurebecomesminGmaxDEx1,...,xB∼D[logD(x1,...,xB)]+(1)Ez1,...,zB∼Z[log(1−D(G(z1),...,G(zB)))]withDtheempiricaldistributionoverdata,Zadistributionoverthelatentvariablethatistheinputofthegenerator,GapointwisegeneratorandDabatchdiscriminator.1ThisleadstoalearningproceduresimilartotheusualGANalgorithm,exceptthatthelossencouragesthediscriminatortooutput1whenfacedwithanentirebatchofrealdata,and0whenfacedwithanentirebatchofgenerateddata.Unfortunately,thisbasicproceduremakestheworkofthediscriminatortooeasy.Asthediscriminatorisonlyfacedwithbatchesthatconsistofeitheronlytrainingsamplesoronlygeneratedsamples,itcanbaseitspredictiononanysubsetofthesesamples.Forexample,asinglepoorgeneratedsamplewouldbeenoughtorejectabatch.Tocopewiththisdeﬁciency,weproposetosamplebatchesthatmixbothtrainingandgenerateddata.Thediscriminator’staskistopredicttheproportionofrealimagesinthebatch,whichisclearlyapermutationinvariantquantity.3.1.BatchsmoothingasaregularizerAnaiveapproachtosamplingmixedbatcheswouldbe,foreachbatchindex,topickadatapointfromeitherrealorgeneratedimageswithprobability12.Thisisnecessarilyillbehaved:asthebatchsizeincreases,theratiooftrainingdatatogenerateddatainthebatchtendsto12bythelawoflargenumbers.Consequently,adiscriminatoralwayspredicting12wouldachieveverylowerrorwithlargebatchsizes,andprovidenotrainingsignaltothegenerator.Instead,foreachbatchwesamplearatiopfromadis-tributionPon[0,1],andconstructabatchbypickingrealsampleswithprobabilitypandgeneratedsampleswithprob-ability1−p.Thisforcesthediscriminatortopredictacrossanentirerangeofpossiblevaluesofp.Formally,supposewearegivenabatchoftrainingdatax∈RB×nandabatchofgenerateddata˜x∈RB×n.Tomixxand˜x,abinaryvectorβissampledfromB(p)B,aB-dimensionalBernoullidistributionwithparameterp.Themixedbatchwithmixingvectorβisdenotedmβ(x,˜x):=x(cid:12)β+˜x(cid:12)(1−β).(2)1ThegeneratorGcouldalsobemodiﬁedtoproducebatchesofdata,whichcanhelptocovermoremodesperbatch,butthisdeviatesfromtheobjectiveoflearningadensityestimatorfromwhichwecandrawi.i.d.samples.4

MixedbatchesandsymmetricdiscriminatorsforGANtrainingSquaresCirclesGanmixupGanBGan(γ=0.3)GanmixupGanBGan(γ=0.3)Figure3.Comparisonbetweenstandard,mixupandbatchsmoothingGANsona2Dexperiment.Trainingatiterations10,100,1000,10000and20000.Thisapparentlywastessomesamples,butwecanreusethediscardedsamplesbyusing1−βinthenextbatch.Thediscriminatorhastopredicttheratioofrealimages,#βBwhere#βisthesumofthecomponentsofβ.Asalossonthepredictedratio,weusetheKullback–LeiblerdivergencebetweenaBernoullidistributionwiththeactualratioofrealimages,andaBernoullidistributionwiththepredictedratio.ThedivergencebetweenBernoullidistributionswithparametersuandvisKL(B(u)||B(v))=uloguv+(1−u)log1−u1−v.(3)Formally,thediscriminatorDwillminimizetheobjectiveEp∼P,β∼B(p)BKL(cid:18)B(cid:18)#βB(cid:19)||B(D(mβ(x,˜x)))(cid:19),(4)wheretheexpectationisoversamplingpfromadistributionP,typicallyuniformon[0,1],thensamplingamixedmini-batch.Forclarity,wehaveomittedtheexpectationoverthesamplingoftrainingandgeneratedsamplesThegeneratoristrainedwiththelossEp∼P,β∼B(p)Blog(D(mβ(x,˜x))).(5)Thisloss,whichisnotthegeneratorlossassociatedtothemin-maxoptimizationproblem,isknowntosaturateless(Goodfellowetal.,2014).Insomeexperimentalcases,usingthediscriminatorloss(4)withP=U([0,1])madediscriminatortrainingtoodifﬁcult.Toalleviatesomeofthedifﬁculty,wesampledthemixingvariablepfromareducedsymmetricunionofintervals[0,γ]∪[1−γ,1].Withlowγ,allgeneratedbatchesarenearlypurelytakenfromeitherrealorfakedata.Werefertothistrainingmethodasbatchsmoothing-γ.Batchsmoothing-0correspondstonomixing,whilebatchsmoothing-0.5correspondstoequation(4).3.2.TheoptimaldiscriminatorforbatchsmoothingTheoptimaldiscriminatorforbatchsmoothingcanbecom-putedexplicitly,forp∼U([0,1]),andextendstheusualGANdiscriminatorwhenB=1.Proposition1.Theoptimaldiscriminatorfortheloss(4),5

MixedbatchesandsymmetricdiscriminatorsforGANtraininggivenabatchy∈RB×N,isD∗(y)=12punbalanced(y)pbalanced(y)(6)wherethedistributionpbalancedandpunbalancedonbatchesaredeﬁnedaspbalanced(y)=1B+1Xβ∈{0,1}Bp1(y)βp2(y)1−β(cid:0)B#β(cid:1)punbalanced(y)=2B+1Xβ∈{0,1}Bp1(y)βp2(y)1−β(cid:0)B#β(cid:1)#βB.(7)inwhichp1isthedatadistributionandp2thedistributionofgeneratedsamples,andwherep1(y)βisshorthandforp1(y1)β1...p1(yB)βB.Theproofistechnicalandisdeferredtothesupplementarymaterial.Fornon-uniformbetadistributionsonp,asimilarresultholds,withdifferentcoefﬁcientsdependingon#βandBinthesum.Theseheavyexpressionscanbeinterpretedeasily.First,inthecaseB=1,theoptimaldiscriminatorreducestotheop-timaldiscriminatorforastandardGAN,D∗=p1(y)p1(y)+p2(y).Actuallypbalanced(y)issimplythedistributionofbatchesyunderourprocedureofsamplingpuniformly,thensamplingβ∼B(p)B.Thebinomialcoefﬁcientsputonequalfootingcontributionswithdifferenttrue/fakeratios.Thegeneratorloss(5),whenfacedwiththeoptimaldis-criminator,istheKullback–Leiblerdivergencebetweenpbalancedandpunbalanced(uptosignandaconstantlog(2)).Sincepunbalancedputsmoreweightonbatcheswithhigher#β(moretruesamples),thisbringsfakesamplesclosertotrueones.Sincepbalancedandpunbalanceddifferbyafactor2#β/B,theratioD∗=12punbalanced(y)pbalanced(y)issimplytheexpectationof#β/Bunderaprobabilitydistributiononβthatisproportionaltop1(y)βp2(y)1−β(cid:0)B#β(cid:1).Butthisistheposteriordistributiononβgiventhebatchyandtheuniformpriorontheratiop.Thus,theoptimaldiscriminatorisjusttheposteriormeanoftheratiooftruesamples,D∗(y)=IEβ|yh#βBi.ThisisstandardwhenminimizingtheexpecteddivergencebetweenBernoullidistributionsandtheapproachcanthereforebeextendedtonon-uniformpriorsonpasshowninsection9.4.PermutationinvariantnetworksComputingstatisticsofprobabilitydistributionsfrombatchesofi.i.d.samplesrequirestocomputequantitiesthatareinvarianttopermutingtheorderofsampleswithinthebatch.Inthissectionweproposeapermutationequivariantlayerthatcanbeusedtogetherwithapermutationinvariantaggregationoperationtobuildnetworksthatarepermutationinvariant.Wealsoprovideasketchofproof(fullydevel-opedinthesupplementarymaterial)thatthisarchitectureisabletoreachallsymmetriccontinuousfunctions,andonlyrepresentssuchfunctions.4.1.BuildingapermutationinvariantarchitectureAnaivewayofachievinginvariancetobatchpermutationsistoconsiderthebatchdimensionasaregularfeaturedi-mension,andtorandomlyreorderthebatchesateachstep.Thismultipliestheinputdimensionbythebatchsize,andthusgreatlyincreasesthenumberoftrainableparameters.Moreover,thisonlyprovidesapproximateinvariancetobatchpermutation,asthenetworkhastoinfertheinvariancebasedonthetrainingdata.Instead,weproposetodirectlybuildinvarianceintothearchitecture.Thismethoddrasticallyreducesthenumberofparameterscomparedtothenaiveapproach,bringingitbackinlinewithordinarynetworks,andensuresstrictinvariancetobatchpermutation.Letusﬁrstformalizethenotionofbatchpermutationinvari-anceandequivariance.AfunctionffromRB×ltoRB×Lisbatchpermutationequivariantifpermutingsamplesinthebatchresultsinthesamepermutationoftheoutputs:foranypermutationσoftheinputs,f(xσ(1),...,xσ(B))=f(x)σ(1),...,f(x)σ(B).(8)Forinstance,anyregularneuralnetworkorotherfunctiontreatingtheinputsx1,...,xBindependentlyinparallel,isbatchpermutationequivariant.AfunctionffromRB×ltoRLisbatchpermutationinvari-antifpermutingtheinputsinthebatchdoesnotchangetheoutput:foranypermutationonbatchindicesσ,f(xσ(1),...,xσ(B))=f(x1,...,xB).(9)Themean,themaxorthestandarddeviationalongthebatchaxisareallbatchpermutationinvariant.Permutationequivariantandpermutationinvariantfunctionscanbeobtainedbycombiningordinary,paralleltreatmentofbatchsampleswithanadditionalbatch-averagingoperationthatperformsanaverageoftheactivationsacrossthebatchdirection.Inourarchitecture,thisaveragingistheonlyformofinteractionbetweendifferentelementsofthebatch.Itisoneofourmainresultsthatsuchoperationsaresufﬁcienttorecoverallinvariantfunctions.Formally,onabatchofdatax∈RB×n,ourproposedbatch6

MixedbatchesandsymmetricdiscriminatorsforGANtrainingFigure4.SampleimagesgeneratedbyourbestmodeltrainedonCIFAR10.permutationinvariantnetworkfθisdeﬁnedasfθ(x)=1BBXb=1(φθp◦φθp−1◦...◦φθ0(x))b(10)whereeachφθiisabatchpermutationequivariantfunctionfromRB×li−1toRB×li,wheretheli’sarethelayersizes.TheequivariantlayeroperationφθwithlinputfeaturesandLoutputfeaturescomprisesanordinaryweightmatrixΛ∈Rl×Lthattreatseachdatapointofthebatchindepen-dently(“non-batch-mixing”),abatch-mixingweightmatrixΓ∈Rl×L,andabiasvectorβ∈RL.Asinregularneuralnetworks,Λprocesseseachdatapointinthebatchindepen-dently.Ontheotherhand,theweightmatrixΓoperatesaftercomputinganaverageacrossthewholebatch.Deﬁningρasthebatchaverageforeachfeature,ρ(x1,...,xB):=1BBXb=1xb(11)thepermutation-equivariantlayerφisformallydeﬁnedasφθ(x)b:=µ(cid:16)β+xbΛ+ρ(x)Γ(cid:17)(12)whereµisanonlinearity,bisabatchindex,andtheparam-eterofthelayerisθ=(β,Λ,Γ).4.2.NetworksofequivariantlayersprovideuniversalapproximationofpermutationinvariantfunctionsThenetworksconstructedabovearepermutationinvariantbyconstruction.However,itisunclearapriorithatallpermutationinvariantfunctionscanberepresentedthisway:thefunctionsthatcanbeapproximatedtoarbitraryprecisionbythosenetworkscouldbeastrictsubsetofthesetofpermutationinvariantfunctions.Theoptimalsolutionforthediscriminatorcouldlieoutsidethissubset,makingourconstructiontoorestrictive.Wenowshowthisisnotthecase:ourarchitecturesatisﬁesauniversalapproximationtheoremforpermutation-invariantfunctions.Theorem1.ThesetofnetworksthatcanbeconstructedbystackingasinEq.(10)thelayersφdeﬁnedinEq.(12),withsigmoidnonlinearitiesexceptontheoutputlayer,isdenseinthesetofpermutation-invariantfunctions(forthetopologyofuniformconvergenceoncompactsets).Whilethecaseofone-dimensionalfeaturesisrelativelysimple,themultidimensionalcaseismoreintricate,andthedetailedproofisgiveninthesupplementarymaterial.Letusdescribethekeyideasunderlyingtheproof.Thestandarduniversalapproximationtheoremforneu-ralnetworksprovesthefollowing:foranycontinuousfunctionf,wecanﬁndanetworkthatgivenabatchx=(x1,...,xB),computes(f(x1),...,f(xB)).Thisisinsufﬁcientforourpurposeasitprovidesnowayofmixinginformationbetweensamplesinthebatch.First,weprovethatthesetoffunctionsthatcanbeapproxi-matedtoarbitraryprecisionbyournetworksisanalgebra,i.e.,avectorspacestableunderproducts.Fromthispointon,itremainstobeshownthatthisalgebracontainsagenerativefamilyofthecontinuoussymmetricfunctions.Toprovethatwecancomputethesumoftwofunctionsf1andf2,computef1andf2ondifferentchannels(thisispossibleeveniff1andf2requiredifferentnumbersoflayers,byﬁllinginwiththeidentityifnecessary).Thensumacrosschannels,whichispossiblein(12).Tocomputeproducts,ﬁrstcomputef1andf2ondifferentchannels,thenapplytheuniversalapproximationtheoremtoturnthisintologf1andlogf2,thenadd,thentaketheexponentialthankstotheuniversalapproximationtheorem.Thekeypointisthenthefollowing:thealgebraofallpermutation-invariantpolynomialsoverthecomponentsof(x1,...,xB)isgeneratedasanalgebrabytheaverages1B(f(x1)+...+f(xB))whenfrangesoverallfunctionsofsinglebatchelements.Thisnon-trivialalgebraicstate-mentisprovedinthesupplementarymaterial.7

MixedbatchesandsymmetricdiscriminatorsforGANtrainingFigure5.Samplesobtainedafter66000iterationsonthecelebAdataset.Fromlefttoright:(a)StandardGAN(b)Singlebatchdiscriminator,nobatchsmoothing.(c)Singlebatchdiscriminator,batchsmoothingγ=0.5.(d)Multiplebatchdiscriminators,batchsmoothingγ=0.5Byconstruction,suchfunctions1B(f(x1)+...+f(xB))arereadilyavailableinourarchitecture,bycomputingfasinanordinarynetworkandthenapplyingthebatch-averagingoperationρinthenextlayer.Furtherlayersprovidesumsandproductsofthosethankstothealgebraproperty.WecanconcludewithasymmetricversionoftheStone–Weierstrasstheorem(polynomialsaredenseincontinuousfunctions).4.3.PracticalarchitectureInourexperiments,weapplytheconstructionsabovetostan-dard,deepconvolutionalneuralnetworks.Inpractice,forthelinearoperationsΛandΓin(12)weuseconvolutionalkernels(ofsize3×3)actingoverxbandρ(x)respectively.WeighttensorsΛandΓarealsoreweightedlikesothatatthestartoftrainingρ(x)doesnotcontributedispropor-tionatelycomparedwithotherfeatures:˜Λ=|B||B|+1Λand˜Γ=1|B|+1Γwhere|B|denotesthesizeofbatchB.Whilethesecoefﬁcientscouldbelearned,wehavefoundthisex-plicitinitializationtoimprovetraining.Figure1showshowtomodifystandardCNNarchitecturestoadapteachlayertoourmethod.Intheﬁrstsetup,whichwerefertoasBGAN,apermutationinvariantreductionisdoneattheendofthediscriminator,yieldingasinglepredictionperbatch,whichisevaluatedwiththelossin(4).Wealsointroduceasetup,M-BGAN,whereweswaptheorderofaveragingandapplyingtheloss.2Namely,lettingybethesingletargetforthebatch(inourcase,theproportionofrealsamples),theBGANcasetranslatesintoL((o1,...,oB),y)=‘ 1BBXi=1oi,y!(13)2Thiswasinitiallyabugthatworked.whileM-BGANtranslatestoL((o1,...,oB),y)=1BBXi=1‘(oi,y)(14)whereListheﬁnallossfunction,‘istheKLlossfunctionusedin(4),(o1,...,ob)istheoutputofthelastequivariantlayer,andyisthetargetforthewholebatch.Boththeselossesarepermutationinvariant.Amorede-tailledexplanationofM-BGANisgiveninSection11.5.Experiments5.1.Synthetic2DdistributionsThesyntheticdatasetfromZhangetal.(2017)isexplicitlydesignedtotestmodedropping.ThedataaresampledfromamixtureofconcentratedGaussiansinthe2Dplane.WecomparestandardGANtraining,“mixup”training(Zhangetal.,2017),andbatchsmoothingusingtheBGANfromSection4.3.Inallcases,thegeneratorsanddiscriminatorsarethree-layerReLUnetworkswith512unitsperlayer.Thelatentvari-ablesofthegeneratorare2-dimensionalstandardGaussians.ThemodelsaretrainedontheirrespectivelossesusingtheAdam(Kingma&Ba,2015)optimizer,withdefaultparam-eters.Thediscriminatoristrainedforﬁvestepsforeachgeneratorstep.TheresultsaresummarizedinFigure3.Batchsmoothingandmixuphavesimilareffects.ResultsforBGANandM-BGANarequalitativelysimilaronthisdatasetandweonlydisplayresultsforBGAN.ThestandardGANsettingquicklydiverges,duetoitsinabilitytoﬁtseveralmodessimultaneously,whilebothbatchsmoothingandmixupsuc-cessfullyﬁtthemajorityofmodesofthedistribution.8

MixedbatchesandsymmetricdiscriminatorsforGANtraining01234Iteration4.55.05.56.06.57.07.5Inception score(£1e5)BGAN, mixed batchesBGAN, pure batchesBatch discriminationM-BGAN, mixed batchesFigure6.InceptionscoreforvariousversionsofBGANandforbatchdiscrimination(Salimansetal.,2016).5.2.ExperimentalresultsonCIFAR10Next,weconsiderimagegenerationontheCIFAR10dataset.Weusethesimplearchitecturefrom(Miyatoetal.,2018),minimallymodiﬁedtoobtainpermutationinvariancethanksto(12).Allotherarchitecturalchoicesareunchanged.ThesameAdamhyperparametersfrom(Miyatoetal.,2018)areusedforallmodels:α=2e−4,β1=0.5,β2=0.999,andnolearningratedecay.Weperformedhyperparametersearchforthenumberofdiscriminationstepsbetweeneachgenerationstep,ndisc,overtherange{1,...,5},andforthebatchsmoothingparameterγover[0.2,0.5].Allmodelsaretrainedfor400,000iterations,countingbothgenerationanddiscriminationsteps.WecomparesmoothedBGANandM-BGAN,andthesamenetworktrainedwithspectralnormalization(Miyatoetal.,2018)(SN),andgradientpenalty(Gulrajanietal.,2017)onboththeWasserstein(Arjovskyetal.,2017)(WGP)andthestandardloss(GP).Wealsocomparetoamodelusingthebatch-discriminationlayerfrom(Salimansetal.,2016),addingaﬁnalbatchdiscriminationlayertothearchitectureof(Miyatoetal.,2018).AllmodelsareevaluatedbyreportingtheInceptionScoreandtheFr´echetInceptionDistance(Heuseletal.,2017)andresultsaresummarizedinTable2.Figure4displayssampleimagesgeneratedwithourbestmodel.Figure5.2highlightsthetrainingdynamicsofeachmodel3.Onthisarchitecture,M-BGANheavilyoutperformsbothbatchdiscriminationandourothervariants,andyieldsre-sultssimilarto,orslightlybetterthan(Miyatoetal.,2018).Modeltrainedwithbatchsmoothingdisplayresultsonparwithbatchdiscrimination,andmuchbetterthanwithoutbatchsmoothing.3Forreadability,aslightsmoothingisperformedonthecurves.Table1.Comparisontothestateoftheartintermsofinceptionscore(IS)andFr´echetinceptiondistance(FID).ModelISFIDWGP(Miyatoetal.,2018)6.68±.0640.2GP(Miyatoetal.,2018)6.93±.0837.7SN(Miyatoetal.,2018)7.42±.0829.3Salimansetal.7.09±.0835.0BGAN7.05±.0636.47M-BGAN7.49±.0623.715.3.EffectofbatchsmoothingonthegeneratoranddiscriminatorlossesTochecktheeffectofthebatchsmoothingparameterγontheloss,weplotthediscriminatorandgeneratorlossesofthenetworkfordifferentγ’s.Thesmallertheγ,thepurerthebatches.Wewouldexpectdiscriminatortrainingtobemoredifﬁcultwithlargerγ.Theresultscorroboratethisinsight(Fig.2).BGANandM-BGANbehavesimilarlyandweonlyreportonBGANintheﬁgure.Thediscriminatorlossisnotdirectlyaffectedbyanincreaseinγ,butthegeneratorlossislowerforlargerγ,revealingtherelativeadvantageofthegeneratoronthediscriminator.Thissuggeststoincreaseγifthediscriminatordominateslearning,andtodecreaseγifthediscriminatorisstuckatahighvalueinspiteofpoorgeneratedsamples.5.4.QualitativeresultsoncelebAFinally,onthecelebAfacedataset,weadaptthesimplear-chitectureof(Miyatoetal.,2018)totheincreasedresolutionbyaddingalayertobothnetworks.ForoptimizationweuseAdamwithβ1=0,β2=0.9,α=1e−4,andndisc=1.Fig.5dislaysBGANsampleswithpurebatches,andBGANandM-BGANsampleswithγ=.5.Thevisualqualityofthesamplesisreasonable;webelievethatanimprovementisvisiblefrompurebatchestoM-BGAN.6.ConclusionWeintroducedamethodtofeedbatchesofsamplestothediscriminatorofaGANinanprincipledway,basedontwoobservations:feedingall-fakeorall-genuinebatchestoadiscriminatormakesitstasktooeasy;second,asimplear-chitecturaltrickmakesitpossibletoprovablyrecoverallfunctionsofthebatchasanunorderedset.Experimentally,thisprovidesanew,alternativemethodtoreducemodedrop-pingandreachgoodquantitativescoresinGANtraining.9

MixedbatchesandsymmetricdiscriminatorsforGANtrainingReferencesArjovsky,M.,Chintala,S.,andBottou,L.Wassersteingen-erativeadversarialnetworks.InProceedingsofthe34thInternationalConferenceonMachineLearning,ICML2017,Sydney,NSW,Australia,6-11August2017,pp.214–223,2017.URLhttp://proceedings.mlr.press/v70/arjovsky17a.html.Cybenko,G.Approximationbysuperpositionsofasig-moidalfunction.MathematicsofControl,SignalsandSystems,2(4):303–314,Dec1989.ISSN1435-568X.doi:10.1007/BF02551274.URLhttps://doi.org/10.1007/BF02551274.Dinh,L.,Sohl-Dickstein,J.,andBengio,S.Densityestima-tionusingrealNVP.InICLR,2017.Dziugaite,G.K.,Roy,D.M.,andGhahramani,Z.Traininggenerativeneuralnetworksviamaximummeandiscrep-ancyoptimization.arXivpreprintarXiv:1505.03906,2015.Goodfellow,I.,Pouget-Abadie,J.,Mirza,M.,Xu,B.,Warde-Farley,D.,Ozair,S.,Courville,A.,andBengio,Y.Generativeadversarialnets.InAdvancesinneuralinformationprocessingsystems,pp.2672–2680,2014.Gulrajani,I.,Ahmed,F.,Arjovsky,M.,Dumoulin,V.,andCourville,A.C.Improvedtrainingofwassersteingans.CoRR,abs/1704.00028,2017.URLhttp://arxiv.org/abs/1704.00028.Guttenberg,N.,Virgo,N.,Witkowski,O.,Aoki,H.,andKanai,R.Permutation-equivariantneuralnetworksappliedtodynamicsprediction.CoRR,abs/1612.04530,2016.URLhttp://arxiv.org/abs/1612.04530.Heusel,M.,Ramsauer,H.,Unterthiner,T.,Nessler,B.,Klambauer,G.,andHochreiter,S.Ganstrainedbyatwotime-scaleupdateruleconvergetoanashequi-librium.CoRR,abs/1706.08500,2017.URLhttp://arxiv.org/abs/1706.08500.Karras,T.,Aila,T.,andabdJ.Lehtinen,S.L.ProgressivegrowingofGANsforimprovedquality,stability,andvariation.InICLR,2018.Kingma,D.andBa,J.Adam:Amethodforstochasticoptimization.Iniclr,2015.Kingma,D.andWelling,M.Auto-encodingvariationalBayes.InICLR,2014.Li,Y.,Swersky,K.,andZemel,R.S.Generativemomentmatchingnetworks.CoRR,abs/1502.02761,2015.URLhttp://arxiv.org/abs/1502.02761.McGregor,S.Neuralnetworkprocessingformultisetdata.InProceedingsofthe17thInternationalConferenceonArtiﬁcialNeuralNetworks,ICANN’07,pp.460–470,Berlin,Heidelberg,2007.Springer-Verlag.ISBN3-540-74689-7,978-3-540-74689-8.URLhttp://dl.acm.org/citation.cfm?id=1776814.1776866.McGregor,S.Furtherresultsinmultisetprocessingwithneuralnetworks.NeuralNetworks,21(6):830–837,2008.doi:10.1016/j.neunet.2008.06.020.URLhttps://doi.org/10.1016/j.neunet.2008.06.020.Miyato,T.,Kataoka,T.,Koyama,M.,andYoshida,Y.Spec-tralnormalizationforgenerativeadversarialnetworks.InternationalConferenceonLearningRepresentations,2018.URLhttps://openreview.net/forum?id=B1QRgziT-.acceptedasoralpresentation.Nowozin,S.,Cseke,B.,andTomioka,R.f-gan:Traininggenerativeneuralsamplersusingvariationaldivergenceminimization.InAdvancesinNeuralInformationPro-cessingSystems,pp.271–279,2016.Qi,C.R.,Su,H.,Mo,K.,andGuibas,L.J.Point-net:Deeplearningonpointsetsfor3dclassiﬁcationandsegmentation.CoRR,abs/1612.00593,2016.URLhttp://arxiv.org/abs/1612.00593.Radford,A.,Metz,L.,andChintala,S.Unsupervisedrep-resentationlearningwithdeepconvolutionalgenerativeadversarialnetworks.arXivpreprintarXiv:1511.06434,2015.Rezende,D.,Mohamed,S.,andWierstra,D.Stochasticbackpropagationandapproximateinferenceindeepgen-erativemodels.InICML,2014.Salimans,T.,Goodfellow,I.,Zaremba,W.,Cheung,V.,Radford,A.,andChen,X.ImprovedtechniquesfortrainingGANs.InNIPS,2016.Sohl-Dickstein,J.,Weiss,E.A.,Maheswaranathan,N.,andGanguli,S.Deepunsupervisedlearningusingnonequilib-riumthermodynamics.arXivpreprintarXiv:1503.03585,2015.Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,Kaiser,L.u.,andPolosukhin,I.Atten-tionisallyouneed.InGuyon,I.,Luxburg,U.V.,Bengio,S.,Wallach,H.,Fergus,R.,Vishwanathan,S.,andGar-nett,R.(eds.),AdvancesinNeuralInformationProcess-ingSystems30,pp.6000–6010.CurranAssociates,Inc.,2017.URLhttp://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf.Zaheer,M.,Kottur,S.,Ravanbakhsh,S.,P´oczos,B.,Salakhutdinov,R.,andSmola,A.J.Deepsets.CoRR,abs/1703.06114,2017.URLhttp://arxiv.org/abs/1703.06114.10

MixedbatchesandsymmetricdiscriminatorsforGANtrainingZhang,H.,Ciss´e,M.,Dauphin,Y.N.,andLopez-Paz,D.mixup:Beyondempiricalriskminimization.CoRR,abs/1710.09412,2017.URLhttp://arxiv.org/abs/1710.09412.11

MixedbatchesandsymmetricdiscriminatorsforGANtraining7.SupplementarymaterialInwhatfollows,weaimatprovingauniversalapproximationtheoremfortheclassofpermutationinvariantneuralnetworkswehavedeﬁned.Toeasereadings,products,sumsandrealfunctionapplicationsareassumedtobebroadcastedwhenneedbe.Throughoutthepaperthebatchdimensionnisconstantandommitedfromsetindices.Deﬁnition1.Afunctionf:Rn×k7→Rlissymmetricifforanypermutationofindexesσandforallx∈Rn×k,f(xσ(1),...,xσ(n))=f(x1,...,xn).ThesetofcontinuoussymmetricfunctionsfromRn×ktoRlisdenotedbyIlkDeﬁnition2.Afunctionf:Rn×k7→Rn×lispermutationequivariantifforanypermutationofindexesσandforalx∈Rn,f(xσ(1),...,xσ(n))=f(x)σ(1),...,f(x)σ(n).Whensymmetricfunctionsandpermutationequivariantfunctionsarerestrictedtoacompact,weassumethatthecompactitselfissymmetric.Inwhatfollows,weuseρasareducingoperatoronvectorsdeﬁnedforx∈Rn×kbyρ(x)j=1nnXi=1xi,j.(15)Deﬁnition3.LetthesetsElkbesetsthatcontainpermutationequivariantneuralnetworksfromRn×ktoRn×l,recursivelydeﬁnedthus:•Forallk∈N,theidentityfunctiononRn×kbelongstoEkk.•Forallf∈Ekr,Γ∈Rl×k,Λ∈Rl×kandβ∈Rl,andforact,asigmoidactivationfunction,gdeﬁnedasg(x)i,j=kXp=1Γj,pact(f(x))i,p+kXp=1Λj,pρ(act◦f(x))p+βj)(16)isinElr.Thenumberoflayersofthenetworkisdeﬁnedastheinductiondepthofthepreviousconstruction.ThesetofthusconstructedpermutationequivariantneuralnetworkswithnumberoflayersLisdenotedbyE(L)lk.Notethatthisclassoffunctionistriviallystablebycomposition,i.e.ifg1∈El2l1andg2∈El3l2,theg2◦g1∈El3l1.Deﬁnition4.LetIlkbeasetcontainingsymmetricneuralnetworksfromRn×ktoRldeﬁnedasIlk=ρ(Elk).(17)WehaveconstructedsetsIlk,containingpermutationinvarientnetworks.Wenowshowthatthewaywetheyareconstructedisnottoorestictive,i.e.thatanyanalyticalsymmetricfunctioncanbeapproximatedwitharbitraryprecisionbyasufﬁcientlyexpressivenetworkofourconstruct.InotherwordsweaimatprovingTheorem12.Theorem2.Foralln,k,landforallcompactK,Ilk(cid:12)(cid:12)KisdenseinIlk(cid:12)(cid:12)K.TheﬁrststepoftheproofistoshowthattheclosureofIlk(cid:12)(cid:12)Kisaring,i.e.thatitisstablebysum,productandthateachelementhasaninversefor+,aswellasavectorialspace,makingitanalgebra.Thesecondstepistoprovethatthisclosurecontainsagenerativefamillyofthesetofallpolynomialsthatoperatesymmetricallyonthebatchdimensionandbecausesymmetricpolynomialsaredenseinthesetofallsymmetricfunctions,thisprovesthetheorem.Lemma1.Iff1∈El2l1(cid:12)(cid:12)(cid:12)Kandf2∈El3l2(cid:12)(cid:12)(cid:12)f1(K)thenf2◦f1∈El3l1(cid:12)(cid:12)(cid:12)K.Proof.Letε>0,f2iscontinuousonacompactset,thusuniformlycontinuous,andthereexistsanη>0suchthatkx−x0k<ηimplieskf2(x)−f2(x0)k<ε2.Nowletg1∈El2l1(cid:12)(cid:12)(cid:12)Kbesuchthatkg1−f1k∞≤ηandg2∈El3l2(cid:12)(cid:12)(cid:12)Ksuchthatkg2−f2k∞≤ε2,then,forxinKkf2◦f1(x)−g2◦g1(x)k≤kf2◦f1(x)−g2◦f1(x)k+kg2◦f1(x)−g2◦g1(x)k≤ε12

MixedbatchesandsymmetricdiscriminatorsforGANtrainingLemma2.Foranycontinuousfunctionsg:Rk7→Rl,therestrictionofthefunctionG:Rn×k7→Rn×k,deﬁnedasG(x)=(g(x1),...,g(xn)),toacompactKisinElk(cid:12)(cid:12)K.Moreprecisely,forallL≥2,therestrictionofGtoKisinE(L)lk(cid:12)(cid:12)K.Proof.Thisisaconsequenceoftheneuralnetworkuniversalapproximationtheorem,asstatede.g.in(Cybenko,1989).Lemma3.Iff1∈El1k(cid:12)(cid:12)(cid:12)K,f2∈El2k(cid:12)(cid:12)(cid:12)Kandf1andf2havethesamenumberoflayers(i.e.theyhavethesameinductiondepth),thenconcat1(f1,f2)∈El1,l2k(cid:12)(cid:12)(cid:12)K,withconcat1(x,y)i,j=(xi,jifj≤l1yi,j−l1otherwise(18)Proof.ByinductiononthenumberoflayersL,•ifL=0,theresultisclear.•ifL>0,letg1,Γ1,Λ1andβ1aswellasg2,Γ2,Λ2andβ2betheparametersassociatedtof1andf2,then,byinduction,concat1(g1,g2)isapermutationequivariantnetwork,andconcat1(f1,f2)isobtainedbysettingΓtobetheblockdiagonalmatrixobtainedwithΓ1andΓ2,Λ,theblockdiagonalmatrixobtainedwithΛ1andΛ2,andβtheconcatenationofbothβ’s.Lemma4.Iff1∈El1k(cid:12)(cid:12)(cid:12)K,f2∈El2k(cid:12)(cid:12)(cid:12)K,thenconcat1(f1,f2)∈El1+l2k(cid:12)(cid:12)(cid:12)K.Proof.Letε>0,letg1∈El1k(cid:12)(cid:12)(cid:12)Kandg2∈El2k(cid:12)(cid:12)(cid:12)Kbesuchthatkg1−f1k∞≤ε4andkg2−f2k∞≤ε4.DenotebyL1andL2thenumbersoflayersofg1andg2.WeassumeL1≥L2withoutlossofgenerality.Bylemma2,thereexisth1∈El1l1(cid:12)(cid:12)(cid:12)Kandh2∈El2l2(cid:12)(cid:12)(cid:12)Kwithh1ofdepth2andh2ofdepthL1−L2+2suchthatkh1−Idk∞≤ε4ong1(K)andkh2−Idk∞≤ε4ong2(K).Thenetworksh1◦g1andh2◦g2havethesamenumberoflayers,consequently,concat1(h1◦g1,h2◦g2)∈El1,l2k(cid:12)(cid:12)(cid:12)K.Besides,kconcat1(f1,f2)−concat1(h1◦g1,h2◦g2)k∞(19)≤kf1−g1k∞+kh1◦g1−g1k∞+kf2−g2k∞+kh2◦g2−g2k∞(20)≤ε(21)yieldingtheresult.Lemma5.Iff1andf2areinElk(cid:12)(cid:12)K,thenf1+f2istoo.Proof.Bylemma3,concat1(f1,f2)isinE2lk(cid:12)(cid:12)K.Considerthelayerg,withkernelsΓi,j=(1ifj=iorj=k+i0otherwise,1≤i≤l,1≤j≤2l,Λ=0,β=0.Bylemma1,asbothconcat1(f1,f2)andgareinclosuresofpermutationequivariantnetworks,theircompositionistoo.Thiscompositionisact(f1+f2).Bytheuniversalapproximationtheoremact−1isalsointheclosuresof1+f2isintheclosure.Moregenerally,followingsimilarreasonings,closuresofpermutationequivariantnetworksarevectorialspaces.Itfollowsthatclosuresofpermutationinvariantnetworksarevectorialspacestoo.13

MixedbatchesandsymmetricdiscriminatorsforGANtrainingLemma6.Iff∈Ilk(cid:12)(cid:12)K,thenFdeﬁnedbyF(x)i,j=f(x)j(22)foralli,j,isinElk(cid:12)(cid:12)KProof.Bydeﬁnition,foranyε>0,thereexistsaGinElk(cid:12)(cid:12)Ksuchthatfandρ(G)areatdistanceatmostε2.Letαbeanonzerorealnumbersuchthatact−1(αG(x))iswelldeﬁnedforanyx∈K.Considertheequivariantlayerm(x)i,j=α−1ρ(act(x))j.(23)Letη1beapositiverealnumber,andLη1beacompactsetthatcontainsbothact−1(αG(K))andanyballofradiusη1containedinthisset.misuniformlycontinuousonLη1,andconsequentlythereexistsanη2suchthatifxandyareatdistanceatmostη2,m(x)andm(y)areatdistanceatmostε2.Now,bycompositionandtheuniversalapproximationtheorem,leth∈Elkbesuchthathandact−1(αG)areatdistanceatmostmin(η1,η2).Thenm◦act−1(αG)andm◦hareatdistanceatmostε2,andbytriangularinequality,Fandm◦hareatdistanceatmostε.Lemma7.Iff1andf2areinIlk(cid:12)(cid:12)K,thenf1f2istoo.Proof.LetF1andF2betheextensionsoff1,f2asdeﬁnedinlemma6.ThereexistsaC∈Rsuchthatforalli,j,x∈K,F1(x)i,j+C>0,andsimilarilyforF2.Consequently,bylemma1,lemma2andlemma5,exp(log(F1+C)+log(F2+C))=F1F2+F1C+F2C+C2∈Elk(cid:12)(cid:12)K.Asthisclosureisavectorialspace,F1F2∈Elk(cid:12)(cid:12)K.Consequently,f1f2=ρ(F1F2)∈Ilk(cid:12)(cid:12)K.WehavenowshownthatIlk(cid:12)(cid:12)Kisaring.Wearelefttoprovethatitcontainsagenerativefamillyofthecontinuoussymmetricfunctions.Letusﬁrstexhibitafamillyofcontinuoussymmetricfunctionsthatiscontainedinthesetofinterest,andthatwewilllatershowgenerateallcontinuoussymmetricfunction.Lemma8.Forallf,restrictionofafunctionfromRltoRktoacompactsetK,thesymmetricfunctionF,deﬁnedonKn×lbyF(x)=nXi=1f(xi)(24)isinIlk.Proof.Bytheuniversalapproximationtheorem,fisinIlk(cid:12)(cid:12)K.Bylemma6,thereexistsaGinElk(cid:12)(cid:12)Kthatreplicatesfalongthebatchaxisofanequivariantnetwork.Consequently,ρ(G)=FisinIlk(cid:12)(cid:12)K.Wearegoingtoprovethatthisfamillyoffunctionsgeneratesthesetofallsymmetricpolynomials.DerivingageneralizationofStoneWeierstrasstheoremtosymmetricfunctions,weobtaintheﬁnalresult.Tokeepthingsgeneral,inwhatfollows,Xdenotesanarbitraryset,FanalgebraoffunctionsonX,andSisthesymmetrizationoperatoronfunctionsofXn,i.e.forall(x1,...,xn)∈Xn,(Sf)(x1,...,xn)=Xσf(xσ(1),...,xσ(n))(25)wherethesumisoverallpermutationsof[1,n].LetPbethealgebraoffunctionsofXngeneratedbythefunctionsf(xk):x→f(xk)forfinF,withaslightabuseofnotations.Pislinearlygeneratedbythemonomialsf1(x1)...fn(xn)forfkarbitraryfunctionsofF.WeareinterestedinthesymmetrizationofP,SP.BylinearityofS,SPisgeneratedbythesymmetrizedmonomials,Sf1(x1)...fn(xn)=XσnYk=1fk(xσ(k)).(26)14

MixedbatchesandsymmetricdiscriminatorsforGANtrainingLemma9.SPisgeneratedasanalgebrabySf(x1)forf∈F.Notably,Sf(x1)takesthespecialformSf(x1)=Xσf(xσ(1))=(n−1)!nXk=1f(xk).(27)Typically,forourcase,X=Rlforlthenumberofinputfeatures,FisanalgebraoffunctionscontainingthemultivariatepolynomialsonRl,andSPthuscontainsthesetofallpolynomialswhicharesymmetricalongthebatchdimension.Proof.Callrankofamonomialf1(x1)...fn(xn),thenumberoffunctionsfksuchthatfk6=1.Letk1,...,krbetheseindices.Uptorenamingfk1tof1,etc.,themonomialcanbewrittenasf1(xk1)...fr(xkr).Wewillworkbyinductiononr.Forr=1theclaimistrivial.SinceSdoesnotcareaboutpermutingthevariables,wehaveSf1(xk1)...fr(xkr)=Sf1(x1)...fr(xr)=Xσ∈SKrYi=1fi(xσ(i))(28)andnowthevaluesσ(r+1),...,σ(n)havenoinﬂuencesothatSf1(x1)...fn(xn)=(n−r)!Xσ∈InjnrrYi=1fi(xσ(i))(29)whereInjnristhesetofinjectivefunctionsfromrton.Assumewecangenerateallsymmetricmonomialsuptorankr.BydeﬁnitionwecangenerateSfr+1(x1)foranyfr+1∈F.Thenwecangeneratetheproduct1(n−r−1)!(Sfr+1(x1))Xσ∈InjnrrYi=1fi(xσ(i))=(Xk∈nfr+1(xk))Xσ∈InjnrrYi=1fi(xσ(i))=Xσ∈InjnrXk∈nfr+1(xk)rYi=1fi(xσ(i))Now,foreachσ,wecandecomposeaccordingtowhetherk∈Imσork∈n\Imσ,whereImσ={σ(1),...,σ(r)}istheimageofσ.Weobtaintwoterms...=Xσ∈InjnrXk∈Imσfr+1(xk)rYi=1fi(xσ(i))+Xσ∈InjnrXk∈n\Imσfr+1(xk)rYi=1fi(xσ(i))ButifkisnotinImσ,then(σ(1),...,σ(r),k)isaninjectivefunctionfromr+1ton.Sosummingoverσthenonk∈n\Imσisexactlyequivalenttosummingoverσ∈Injnr+1.SothesecondtermaboveisXσ∈Injnr+1 rYi=1fi(xσ(i))!fr+1(σ(r+1))=Xσ∈Injnr+1r+1Yi=1fi(xσ(i))=Sf1(xk1)...fr+1(xkr+1)whichistheoneweareinterestedin.Soifweprovethatwecangeneratetheﬁrstterm,wearedone.Letusconsidertheﬁrstterm,withk∈Imσ.Now,sincek∈Imσ,wecandecomposeoverthecasesk=σ(1),...,k=σ(r),namely,Xσ∈InjnrXk∈Imσfr+1(xk)rYi=1fi(xσ(i))=Xσ∈InjnrrXj=1fr+1(xσ(j))rYi=1fi(xσ(i))(30)=rXj=1Xσ∈InjnrrYi=1˜fij(xσ(i))(31)15

MixedbatchesandsymmetricdiscriminatorsforGANtrainingwhere˜fij:=(fii6=jfifr+1i=j(32)NowsinceFisaring,fifr+1∈F.ForeachjthetermXσ∈InjnrrYi=1˜fij(xσ(i))(33)isequaltoS˜f1j...˜frjuptoafactor(n−(r+1))!.Byourinductionhypothesis,eachtermcanbegenerated.Thisendstheproof.Lemma10.ForanycompactK,anyl∈N,theintersectionofI1lwiththesetofmultivariatepolynomialsisdenseinI1lfortheinﬁnitynorm.Proof.Letε>0,andfbeinI1l.ThereexistsamultivariatepolynomialsPsuchthatkP−fk∞≤ε.Letusconsiderthesymmetrizedpolynomial˜P(x1,...,xn)=1n!XσP(xσ(1),...,xσ(n)).(34)Then˜Pisintheintersection,and,forx∈K,k˜P(x)−f(x)k=k1n!Xσ(P(xσ(1),...,xσ(n))−f(xσ(1),...,xσ(n)))k(35)≤1n!XσkP(xσ(1),...,xσ(n))−f(xσ(1),...,xσ(n))k(36)≤ε.(37)Wenowhavealltheingredientstoendtheproof.ForagivencompactKofRl,foranymultivariatepolynomialPofRl,anyε>0,theretriviallyexistsanelementfofI1katdistanceatmostεofx→nPi=1P(xi).Thismeansthattheclosureoftheconsideredsetcontainsallsuchfunctions.Asthisclosureisanalgebra(itisbotharingandavectorialspace),bylemma8,itcontainstheintersectionofI2lwiththesetofmultivariatepolynomials.Bylemma10,itcontainsI1l,whichendstheproof.8.Otherdetails8.1.pbalancedandpunbalancedarewellnormalized:Wenowshowthatpunbalancediswelldeﬁned.Thecomputationforpbalancedisalmostidenticalandlefttothereader.Zypunbalanced(y)dy=2B+1Xβ∈{0,1}B#βBC#βBZypx(y)βp˜x(y)1−βdy=2B+1BX#β=1C#βB#βBC#βB=2(B+1)BBX#β=1#β=2(B+1)BB(B+1)2=116

MixedbatchesandsymmetricdiscriminatorsforGANtraining9.OptimaldiscriminatorforgeneralbetapriorWeherebygiveaderivationoftheoptimaldiscriminatorexpression,whenmixingparameters,p’saredrawnfromBeta(a,b).ThisextendsEq.(7),asBeta(1,1)=U([0,1]).Betaprioronbatchmixingproportion.ConsidermixedbatchesofsamplesofsizeB.Thei-thsampleofthebatchisarealsampleifβi=1andafalsesampleifβi=0.Givenacertainmixingproportionp,assumingthatsampleoriginearesampledindependantlyaccordingtoaBernoulliofparameterp,theprobabilityofacertainβisP(β|p)=Yipβi(1−p)1−βi,(38)ConsideringabetapriordistributionBeta(a,b)onthemixingparameterp∈[0,1],theposteriordistributiononthenumberofrealsampleinthebatch#β=Piβiisgivenbythebeta-binomialcompounddistributionP(#β)=ZpBeta(p|a,b)P(#β|p)(39)=(cid:18)B#β(cid:19)B(#β+a,B−#β+b)B(a,b)(40)whereB(·,·)isthebetafunction.Fora=1,b=1,i.e.auniformdistributiononmixingparameters,thebeta-binomialcompounddistributionreducestoauniformdistributionon#β.FromtheexpressionofP(#β)itfollowsthatP(β)=B(#β+a,B−#β+b)B(a,b).(41)Optimaldiscriminator.Lety=mβ(x,˜x)denoteamixedbatchofsamples.ThediscriminatorminimizestheKLdivergencebetweenD(y)andβ,averagedoverbatchesandmixingvectorsβ,seeEq.(4)inthemainpaper.Thisreducestominimizingtheexpectedcross-entropy.Foragivenbatchandmixingvectorβ,L(D(y),#β)=−#βBlnD(y)−B−#βBln(1−D(y)).(42)Averagingoverbatchesandmixingvectors,IEβ,y[L(D(y),#β)]=ZyP(y)XβP(β|y)L(D(y),#β)(43)=−ZyP(y)(cid:20)IEβ|y(cid:20)#βB(cid:21)lnD(y)+IEβ|y(cid:20)B−#βB(cid:21)ln(1−D(y))(cid:21)(44)Fromthelatterityieldsthatforanyy,theoptimaldiscriminatorvalueD∗(y)isD∗(y)=IEβ|y(cid:20)#βB(cid:21),(45)i.e.theposteriorexpectationofthefractionoftrainingsamplesinthebatch.Posterioranalysis.ThroughBayesrule,theposteriorexpectationyieldsD∗(y)=IEβ|y(cid:20)#βB(cid:21)=Pβ#βBP(y|β)P(β)P(y).(46)ThemarginalonthebatchyisP(y)=XβP(y|β)P(β)(47)=XβP(y|β)B(#β+a,B−#β+b)B(a,b).(48)17

MixedbatchesandsymmetricdiscriminatorsforGANtrainingThenumeratorinEq.(46)canbewrittenasadistributionony,Q(y)=XβP(y|β)Q(β)(49)Q(β)=a+baP(β)#βB.(50)ThedistributionQonβsumsto1,asIEP(#β)[#β]=Baa+b.ThisﬁnallyyieldsD∗(y)=aa+bQ(y)P(y),(51)whichfortheuniformprioronpsimpliﬁestoD∗(y)=12Q(y)P(y).(52)ExpressingP(y|β).Noticethatmβ(x,˜x)=yisequivalenttoforalliin{1,...,B},xi=yiandβi=1or˜xi=yiandβi=0.Denotebyp1(resp.p2)thedistributionofrealsamples(resp.generatedsamples).Fromthepreviousobservation,ityieldsthatP(y|β)=BYi=1p1(yi)βip2(yi)1−βi.(53)FromthelatterandEq.(52)weobtaintheoptimaldiscriminatorexpression.18

MixedbatchesandsymmetricdiscriminatorsforGANtraining10.AdditionalexperimentsFigure7.SampleimagesgeneratedbyourbestmodeltrainedonSTL10.WeadditionallyprovideresultsontheSTL-10dataset,whereM-BGANyieldsnumericalresultsslightlybelowSpectralNormalization.Exceptfortheadaptationofthenetworkto48×48images,asdonein(Miyatoetal.,2018),theexperimentalsetupoftheexperimentalsectionisleftunchanged.Table2.Comparisontothestateoftheartintermsofinceptionscore(IS)andFr´echetinceptiondistance(FID)ontheSTL-10dataset.ModelISFIDWGP(Miyatoetal.,2018)8.455M-BGAN8.751SN(Miyatoetal.,2018)8.747.5SN(Hingeloss)(Miyatoetal.,2018)8.843.211.M-BGANasanensemblingmethodIntuitively,theM-BGANlossperformsasimpleensemblingofmanystronglydependantpermutationinvariantdiscrimina-tors,atnoadditionalcost.Inthegeneralcase,ensemblingofNindependantdiscriminatorsD1,...,DNamountstotrainingeachdiscriminatorindependently,andusingtheaveragedgradientsignaltotrainthegenerator.EnsemblingisexpectedtoalleviatesomeofthedifﬁcultiesofGANtraining:aslongasoneofthediscriminatorsstillprovidesasigniﬁcantgradientsignal,trainingofthegeneratorispossible.Withequation(14),M-BGANisanensembleofBpermutationinvariantdiscriminators,withrespectiveoutputs1-th(o1,...,oB),...,B-th(o1,...,oB),wherei-thisthefunctionthatreturnsthei-thgreatestelementofaBdimensionalvector.Indeed,1NNXi=1l(i-th(o1,...,oB),y)=1NNXi=1l(oi,y).(54)whichistheM-BGANloss.TheensembleddiscriminatorsoftheM-BGANallsharethesameweights.WebelievethisensemblingeffectatleastpartiallyexplainstheimprovedperformanceofM-BGAN.