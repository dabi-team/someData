Noname manuscript No.
(will be inserted by the editor)

An EM Based Probabilistic Two-Dimensional CCA
with Application to Face Recognition

Mehran Safayani · Seyed Hashem
Ahmadi · Homayun Afrabandpey ·
Abdolreza Mirzaei

Received: date / Accepted: date

Abstract Recently, two-dimensional canonical correlation analysis (2DCCA)
has been successfully applied for image feature extraction. The method instead
of concatenating the columns of the images to the one-dimensional vectors, di-
rectly works with two-dimensional image matrices. Although 2DCCA works
well in diﬀerent recognition tasks, it lacks a probabilistic interpretation. In
this paper, we present a probabilistic framework for 2DCCA called probabilis-
tic 2DCCA (P2DCCA) and an iterative EM based algorithm for optimizing
the parameters. Experimental results on synthetic and real data demonstrate
superior performance in loading factor estimation for P2DCCA compared to
2DCCA. For real data, three subsets of AR face database and also the UMIST
face database conﬁrm the robustness of the proposed algorithm in face recog-
nition tasks with diﬀerent illumination conditions, facial expressions, poses
and occlusions.

Keywords Canonical Correlation Analysis (CCA) · Two-dimensional CCA ·
Probabilistic Feature extraction · Dimension Reduction · Face recognition

1 Introduction

Although many real-world applications encounter high dimensional data, the
most informative part of the data can be modeled in a low dimensional space.
Moreover, processing high-dimensional data is a time consuming process and
requires lots of resources. To tackle these problems, feature extraction has been
used as a tool for ﬁnding a compact and meaningful data representation.

Department of Electrical and Computer Engineering, Isfahan University of Technology, Is-
fahan 84156-83111, IRAN
Tel.: +98 31 33919063
Fax: +98 31 33912451
E-mail:
h.afraei@ec.iut.ac.ir · mirzaei@cc.iut.ac.ir

safayani@cc.iut.ac.ir(Corresponding Author)

· hashem.ahmadi@ec.iut.ac.ir

·

7
1
0
2

b
e
F
5
2

]

V
C
.
s
c
[

1
v
4
8
8
7
0
.
2
0
7
1
:
v
i
X
r
a

 
 
 
 
 
 
2

Mehran Safayani et al.

For single-mode source data, some subspace learning methods are conducted
to learn more semantic description subspaces. Examples of these methods
are principal component analysis (PCA) [1] and linear discriminant analysis
(LDA). However, for observations from two sources that share some mutual in-
formation, canonical correlation analysis (CCA) [2] is a very popular approach
for dimensionality reduction. CCA seeks a lower-dimensional space where two
sets of variables are maximally correlated after projecting on it. This technique
is widely used in diﬀerent ﬁelds of pattern recognition, computer vision, bioin-
formatics, etc. [3,4,5]. In the CCA-based methods, it is necessary to vectorize
2D image matrices. Vectorization has three main drawbacks: (I) breaking the
spatial structure of image data which may cause losing potentially useful struc-
tural information among column/rows [6], (II) leading to a high-dimensional
vector space and small sample size problem which in turn makes it diﬃcult to
calculate the covariance matrices [7] and (III) causing the covariance matrices
to be very large which in turn makes the eigen-decomposition of such large
matrices very time-consuming.
To overcome these drawbacks, in 2007 two-dimensional CCA (2DCCA) was
introduced by Lee and Choi [8] which computes CCA directions based on 2D
image matrices. The proposed 2DCCA overcomes the curse of dimensional-
ity and signiﬁcantly reduces the computational cost, by directly working with
2D images instead of reshaping them into 1D vectors. In [8], higher recogni-
tion accuracies were reported using 2DCCA compared to CCA using two face
databases and the time complexity has been improved.
However, an associated probabilistic model for observed data was notably ab-
sent form these feature extraction methods. A probabilistic feature extraction
algorithm could be intuitively appealing for so many reasons [9]. To bridge
the gap, in 1999, Tipping and Bishop proposed probabilistic PCA [9] based on
a latent variable model known as factor analysis (FA) [10, 11]. The proposed
PPCA was then used as a framework for many other new formulations for
PCA [12,13,14,15]. Also, there have been some probabilistic models proposed
for LDA [16,17]. In 2005, Bach and Jordan [18] also proposed a probabilistic
interpretation of CCA and estimate the parameters of their proposed model us-
ing both maximum likelihood and expectation maximization. Recently, many
inspiring research proceeded in the 1D CCA domain, including kernel based,
semiparametric and nonparametric methods [19,20,21], but in the 2D CCA
domain, we feel that more work is required. To bridge the gap, a probabilistic
model of 2DCCA was introduced by Safayani et al. in 2011 [22]. They showed
that the maximum likelihood estimation of parameters, leads to the two di-
mensional canonical correlation directions. However, they didn't propose an
EM based solution for their model. EM does not require the explicit eigen-
decomposition of covariance matrices. Moreover, using EM it is possible to
handle models with incomplete data such as mixture models where the cluster
labels are the missing values [23].
In this paper, we present a probabilistic interpretation of 2DCCA, referred to
as P2DCCA, together with an EM based solution to estimate the parameters
of the model. The proposed model can handle the small sample size problem

Probabilistic 2DCCA with EM

3

eﬀectively.
The rest of the paper is organized as follows: Section 2 brieﬂy reviews some
related algorithms such CCA, PCCA and 2DCCA which are necessary to un-
derstand how the proposed algorithms work. The proposed P2DCCA model
is introduced in Section 3. In Section 4, some experiments on synthetic data
and several face databases are given to evaluate performance of the proposed
algorithm; ﬁnally, the paper is concluded in Section 5.

2 Background

2.1 Canonical Correlation Analysis (CCA)

Imagine that we are given two sets of random vectors t1 and t2 where t1,n ∈
RD1 and t2,n ∈ RD2 for n ∈ 1, 2, ..., N are realizations of the corresponding
random vectors, respectively. CCA seeks transformation vectors w1 ∈ RD1 and
w2 ∈ RD2 such that correlation between wT
2 t2 are maximized. The
correlation between wT

2 t2 can be formulated as

1 t1 and wT

1 t1 and wT

ρ =

(cid:113)
2

cov(wT

var(wT

1 t1, wT
2 t2)
1 t1)var(wT

2 t2)

=

(cid:113)
2

wT

1 Σ12w2
1 Σ11w1)(wT

(wT

2 Σ22w2)

(1)

(cid:80)N

where Σij = 1
N
matrix of t1 and t2 and µi = 1
N
of ti.
Then, the objective function for CCA can be written as:

n=1(ti,n −µi)(tj,n −µj)T for i, j ∈ 1, 2 is the cross-covariance
n=1 ti,n for i ∈ {1, 2} denotes the mean vector

(cid:80)N

wT

1 Σ12w2

(2)

argmax
w1,w2
s.t. wT
wT

1 Σ11w2 = 1
2 Σ22w2 = 1

Optimizing such a constrained maximization problem with respect to w1 and
w2 leads to the following generalized eigenvalue problem:
(cid:21) (cid:20)w1
(cid:21)
w2

(cid:20) 0 Σ12
Σ21 0

(cid:20)Σ11 0
0 Σ22

(cid:21) (cid:20)w1
w2

= λ

(3)

(cid:21)

By solving equation (3), w1 and w2 that maximize the correlation between the
projected data can be found.

2.2 Probabilistic CCA

The generative model introduced by Bach and Jordan for CCA is as follows:

ti = Wiz + µi + (cid:15)i

i ∈ {1, 2}

(4)

4

Mehran Safayani et al.

In this model, Wi ∈ RDi×d
i ∈ {1, 2} are linear projections that map two
sets of high dimensional observed random vectors ti ∈ RDi, i ∈ {1, 2} to a
set of lower dimensional latent vectors z ∈ Rd. Here µi is the mean vector
for xi and (cid:15)i is the error term which is assumed to follow a multivariate
Gaussian distributions with zero mean and inverse covariance matrix Ψi. Bach
and Jordan proved that the maximum likelihood estimation for the parameters
of this model would lead to the canonical directions. The maximum likelihood
estimates of the projection matrices are given by:

(cid:99)Wi = ΣiiUidMi,

i ∈ {1, 2}

(5)

where Σii is the sample covariance matrix, Uid are the ﬁrst d canonical di-
rections and Mi ∈ Rd×d where i ∈ {1, 2} are arbitrary matrices such that
M1M2 = Pd, where Pd is the diagonal matrix of the ﬁrst d canonical correla-
tions. Figure 1 shows a graphical representation of the model.

Fig. 1: Graphical model for probabilistic CCA

2.3 Two-Dimensional CCA

Two-dimensional CCA (2DCCA) was proposed to tackle the problem of vector-
izing data in CCA. For each random matrix T1 and T2, 2DCCA introduces left
transforms ui and right transforms vi where i ∈ {1, 2}. After the projection,
data would have the form uT
i Tivi. 2DCCA ﬁnds these left and right transforms
in a way to maximize the correlation between projected data. Therefore, the
objective function of 2DCCA can be formulated as:

cov(uT

1 T1v1, uT

2 T2v2)

argmax
u1,u2,v1,v2
s.t. var(uT
var(uT

1 T1v1) = 1
2 T2v2) = 1,

(6)

Probabilistic 2DCCA with EM

5

u1 and u2 can be obtained by solving the generalized eigenvalue problem (7)
with ﬁxed v1 and v2:

(cid:20) 0 Σr
12
Σr
21 0

(cid:21) (cid:20)u1
u2

(cid:21)

= λ

(cid:20)Σr

11 0
0 Σr
22

(cid:21)

(cid:21) (cid:20)u1
u2

(7)

In a similar way, given u1 and u2, right transforms v1 and v2 can be found by
solving

(cid:20) 0 Σl
12
Σl
21 0

(cid:21) (cid:20)v1
v2

(cid:21)

= λ

(cid:20)Σl

11 0
0 Σl
22

(cid:21)

(cid:21) (cid:20)v1
v2

(8)

where Σr
and Σr
respectively:

i,j, i, j ∈ {1, 2} is the cross-covariance matrix between Ti and Tj
ii, i ∈ {1, 2} is the auto-covariance matrix of Ti deﬁned as follows,

Σr

ij =

Σl

ij =

1
N

1
N

N
(cid:88)

(Ti,n − µi)vivT

j (Tj,n − µj)T ,

i, j ∈ {1, 2}

(9)

n=1

N
(cid:88)

(Ti,n − µi)uiuT

j (Tj,n − µj)T ,

i, j ∈ {1, 2}

(10)

n=1

(cid:80)N

n=1 Ti,n is the corresponding matrix of mean values.

where Ti,n for n ∈ {1, ..., N } is the realization of the random matrix Ti and
µi = 1
N
Left transforms (lx and ly) and right transforms (rx and ry) are obtained by
iteratively solving equations (7) and (8), until convergence. The eigenvectors
associated with d1 largest eigenvalues in (7) determine left transform matrices
U1 and U2 and the eigenvectors associated with d2 largest eigenvalues in (8)
determine right transform matrices V1 and V2. Using these transform matrices
it is possible to project data from a high dimensional space to a new lower
dimensional feature space.

3 Probabilistic Two Dimensional CCA (P2DCCA)

In this section, we propose probabilistic two-dimensional CCA and an EM-
based solution for ﬁnding the parameters of the model. In our model, observed
data are modeled as two-dimensional matrices as follows:

Ti = UiZV T

i ∈ {1, 2}

i + µi + Ξi
where Ti ∈ Rmi×ni for i ∈ {1, 2} are observed matrices and Z ∈ Rm(cid:48)×n(cid:48)
is
the latent matrix. Ui ∈ Rmi×m(cid:48)
, Vi ∈ Rni×n(cid:48)
are projection matrices, µi is the
mean matrix of the observed data and Ξi is the residual matrix. Based on this
deﬁnition, parameters of the model are {Ui, Vi}2
i=1 and the parameters of the
distribution of Ξi

(11)

(cid:48).

6

Mehran Safayani et al.

Let Di = {Ti;n}N
n=1 where i ∈ {1, 2} be a set containing N observed data ma-
trices and {Zn}N
n=1 be the corresponding latent variable set. Then the complete
data would be (T1,n, T2,n, Zn) and the log likelihood of the complete data can
be written as

Lc =

N
(cid:88)

n=1

log p(T1,n, T2,n, Zn)

(12)

To estimate the parameters, ﬁrst we must calculate expectation of the log-
likelihood and then take the derivative of the expected log-likelihood with
respect to each parameter. Unfortunately there is no closed-form solution for
computing the projection matrices {Ui, Vi}2
i=1 simultaneously. Inspired by [24],
a decoupled probabilistic model is employed to obtain projection matrices sep-
arately using an alternating optimization procedure. In such a model, we ﬁrst
assume that the value of one set of projection matrices, e.g. the right projec-
tions {Vi}2
i=1, is known. Then observations are projected to the corresponding
latent spaces. The projection procedure is a probabilistic one that introduced
in section (3.3). By doing this, the left probabilistic model is deﬁned as

i = UiZ l + Ξl
T l
i,

i = 1, 2

(13)

where Z l is the left model latent matrix, µl
i is the mean matrix of the left
projected observations and Ξl
i is the noise source for left probabilistic model
where columns of the noise matrix follow a normal distribution with zero
mean and covariance matrix Ψl
i. By such deﬁnition, parameter set for the left
probabilistic model would be Θl = {Ui, Ψl
i=1 which can be estimated using
expectation maximization procedure. The estimation procedure is explained
later in this section.
In a similar procedure and parallel to the left probabilistic model, for the right
probabilistic model, we assume that the left projection matrices, i.e. {Ui}2
i=1,
are known. Then observations are projected over the corresponding latent
spaces,hence the right probabilistic model is deﬁned as
i = ViZ r + Ξr
T r
i ,

(14)

i}2

Similar to the left probabilistic model, Z r, µr
i are deﬁned for the right
model where in this model the noise source have N (0, Ψr
i ) distribution. The
parameter set for the right probabilistic model would be Θr = {Vi, Ψr
i=1.
Using these deﬁnitions, the decoupled predictive density p(T1, T2, Z) could be
deﬁned as

i }2

i = 1, 2
i and Ξr

2 , Z r)
Now we can rewrite the log likelihood of equation (15) as

p(T1, T2, Z) ∝ p(T l

2, Z l)p(T r

1 , T r

1, T l

Lc =

=

N
(cid:88)

n=1

N
(cid:88)

n=1

log(p(T l

1,n, T l

2,n, Z l

n)p(T r

1,n, T r

2,n, Z r

n))

log p(T l

1,n, T l

2,n, Z l

n) +

N
(cid:88)

n=1

log p(T r

1,n, T r

2,n, Z r

n).

(15)

(16)

Probabilistic 2DCCA with EM

7

To apply the EM algorithm to the decoupled probabilistic model, in E-step
expectation of log likelihood for the left probabilistic model and the right prob-
abilistic model is computed, separately. Then each of the expected log likeli-
hood is maximized with respect to its parameters. In the following subsections
we describe how to optimize left and right probabilistic model respectively.

3.1 Optimizing the left probabilistic model

i,j be the jth column vector of T l

Let tl
to be independent of each other, the distribution of T l

i ∈ Rmi×n(cid:48)

. By assuming columns of T l

i is deﬁned as

p(T l

i ) =

n(cid:48)
(cid:89)

j=1

p(tl

i,j),

i ∈ {1, 2}

(17)

j ∈ Rm(cid:48)×1 as the jth column vector of Z l which has normal
i,j is the jth column vector
j. Based on equation (13) and the distribution considered for Z l and Ξl
i,

We also consider zl
distribution of N (0, I) and also in the same way µl
of µl
it can be concluded that

p(tl

i,j) ∼ N (µl

i,j, UiU T

i + Ψl

i),

i = 1, 2

(18)

ml

j = [(µl

Suppose τ l

2,n,j)T ]T ∈ R(m1+m2)×1, V = [U T
1 U T
(cid:19)

1,n,j)T (tl
2,j)T ]T ∈ R(m1+m2)×1 and Ψl =

n,j = [(tl
1,j)T (µl
abilistic model, where tl
ith observation set where i ∈ {1, 2}. Therefore, distributions of p(τ l
obtained as follows:

i,n,j refers to the jth column vector of nth image in the
j) can be

2 ]T ∈ R(m1+m2)×m(cid:48)
for the left prob-

1 0
0 Ψl
2

(cid:18) Ψl

,

p(τ l

j) ∼ N (ml

j, Σl),

j ∈ [1, n(cid:48)]

(19)

where Σl = U U T + Ψl and we assume Σl > 0.
Based on (17) we can write:

p(T l

1,n, T l

2,n, Z l

n) =

n(cid:48)
(cid:89)

j=1

p(τ l

n,j, zl

n,j) =

n(cid:48)
(cid:89)

j=1

p(τ l

n,j|zl

n,j)p(zl

n,j)

(20)

To apply the EM algorithm to the decoupled probabilistic model, for each of
the probabilistic models expectation of the log likelihood function is calcu-
lated in the E-step E(Ll
c) where the detail is given in the appendix, and then
maximization step (M-step) is done by maximizing E(Ll
c) with respect to V
and Ψl. By doing so, the values of the parameters are estimated as

Ut+1 =

(M l

t )−1 + (M l

(cid:101)Σl(Ψl
t )−1U l

t)−1Ut(M l
t (Ψl

t )−1
t)−1 (cid:101)Σl(Ψl

t)−1Ut(M l

t )−1

(21)

8

(cid:32)

Ψl

t+1 =

((cid:101)Σl − (cid:101)Σl(Ψl

t)−1Ut(M l
0

t )−1U T

t+1)11

Mehran Safayani et al.

((cid:101)Σl − (cid:101)Σl(Ψl

0
t)−1Ut(M l

t )−1U T

t+1)22

(cid:33)

(22)

where M l = I + U T (Ψl)−1U and At shows the value of parameter A in iter-
ation t and (cid:101)Σl is the sample covariance matrix of observed data for the left
probabilistic model , i.e.

(cid:101)Σl =

1
N

N
(cid:88)

[(T l

1,n − µl

1)T (T l

2,n − µl

2)T ]T [(T l

1,n − µl

1)T (T l

2,n − µl

2)T ]

(23)

n=1

3.2 Optimizing the right probabilistic model

In the manner similar to optimization of the left probabilistic model we have:

p(T r

i ) =

m(cid:48)
(cid:89)

j=1

p(tr

i,j),

i ∈ {1, 2}

(24)

i,j is the jth column vector of T r

i ∈ Rni×m(cid:48)

. Then p(tr

i,j) is computed

where tr
as:

p(tr

i,j) ∼ N (µr

i,j, ViV T

i + Ψr

i ),

Let τ r

n,j = [(tr

1,n,j)T (tr

(25)
2 ]T ∈ R(n1+n2)×n(cid:48)
1 V T
(cid:19)
, where tr
i,n,j refers
to the jth column vector of nth image in the ith observation set. Then p(τ r
j )
and p(T r

2,n,j)T ]T ∈ R(n1+n2)×1, V = [V T
1 0
0 Ψr
2

2,j)T ]T ∈ R(n1+n2)×1 and Ψr =

1,j)T (µr

j = [(µr

i ∈ {1, 2}

(cid:18) Ψr

mr

,

1,n, T r

2,n, Z r

n) are obtained as follows:
j ) ∼ N (mr

j , Σr),

p(τ r

p(T r

1,n, T r

2,n, Z r

n) =

m(cid:48)
(cid:89)

j=1

p(τ r

n,j, zr

n,j) =

j ∈ [1, m(cid:48)]

m(cid:48)
(cid:89)

j=1

p(τ r

n,j|zr

n,j)p(zr

n,j)

(26)

(27)

where Σr = V V T +Ψr > 0. Given the details in appendix (A), after computing
c) in the E-step, the parameters V and Ψr are computed by maximizing
E(Lr
the likelihood in the M-step. So, we have:

Vt+1 =

(M r

t )−1 + (M r

(cid:101)Σr(Ψr
t )−1V r

t )−1Vt(M r
t (Ψr

t )−1
t )−1 (cid:101)Σr(Ψr

t )−1Vt(M r

t )−1

(cid:32)

Ψr

t+1 =

((cid:101)Σr − (cid:101)Σr(Ψr

t )−1Vt(M r
0

t )−1V T

t+1)11

((cid:101)Σr − (cid:101)Σr(Ψr

0
t )−1Vt(M r

t )−1V T

t+1)22

where M r = I + V T (Ψr)−1V and (cid:101)Σr is computed as follows:

(cid:101)Σr =

1
N

N
(cid:88)

[(T r

1,n − µr

1)T (T r

2,n − µr

2)T ]T [(T r

1,n − µr

1)T (T r

2,n − µr

2)T ]

n=1

(28)

(cid:33)

(29)

(30)

Probabilistic 2DCCA with EM

9

3.3 Probabilistic projection and dimension reduction

We can project the observation matrices into the latent space using the stan-
dard projection matrices ,i.e., {U1, U2, V1, V2}. However, as described in [9], it
is more natural to use probabilistic projections. In this regard, we represent
each projected observation matrix, Ti, by mean of distribution of correspond-
ing latent space, i.e., E(Z|Ti). For the left model, it can be shown that

E(Z l|T1, T2) = (M r)−1 (cid:2) V T

1 V T
2

(cid:3)

(cid:20) Ψ1 0
0 Ψ2

(cid:21)−1 (cid:20) T1 − µ1
T2 − µ2

(cid:21)

(31)

E(Z l|T1) and E(Z l|T2) are obtained by marginalizing (31) over T2 and T1
respectively. So we have

E(Z l|Ti) = (M r)−1V T

i (Ψr

i )−1(Ti − µi),

Similarly for the right model we have

E(Z r|Ti) = (M l)−1U T

i (Ψl

i)−1(Ti − µi),

i ∈ {1, 2}

(32)

i ∈ {1, 2}

(33)

The procedure for dimension reduction is given by sequential projection in left
and right models as

E(Z|Ti) = (M l)−1U T

i (Ψl

i)−1(Ti − µi)((M r)−1V T

i (Ψr

i )−1)T

(34)

The P2DCCA algorithm is summarized in Figure 2. The proposed P2DCCA
model beneﬁts the ability to extend to other methods such as Mixtures of
P2DCCA, Bayesian P2DCCA and also to robust P2DCCA.

4 Experimental Results

We evaluated our algorithm on both synthetic and real data. In synthetic
data part we veriﬁed our implementation of P2DCCA algorithm by simple
synthetic data and randomly generated projected matrices. we also compared
our algorithm with 2DCCA method in projection matrices estimation . For real
part evaluation, the proposed P2DCCA method was used for face recognition
on two well-known face image databases (AR [25] and UMIST [26]). The AR
database is divided into three subsets for evaluating the performance of the
system in regard to diﬀerent illumination, expression and occlusion conditions.
The UMIST database is used to obtain the performance in dealing with pose
variation.

10

Mehran Safayani et al.

(cid:48)

1,i and U l

Initialize: V l
for t=1,...,Tmax
M r = I + V T
t (Ψr
T l
i,n = (M r)−1V T
Initialize Ψl
while Ll

Input: the matrices Ti,n|N
1
2 Output: Find Ui ∈ (cid:60)mi×m
3
4
5
6
7
8
9
10
11
12
13

E-step:
Compute M l = I + U T
Compute E{zl
Compute E{(zl
M-step:

t )−1Vt
t,i(Ψr
t randomly
c not converge

n=1∈ (cid:60)mi×ni , i ∈ {1, 2}

(cid:48)

, Vi ∈ (cid:60)ni×n

,Ψl and Ψr
1,i using 2DCCA and initialize Ψl

1 and Ψr

1 with I

t,i)−1(Ti,n − µi), i ∈ {1, 2}, n = 1, ..., N

t)−1Ut
t (Ψl
n,j } = (M l)−1U T
n,j )T zl

t (Ψl
n,j } = (M l)−1 + E{zl

t)−1(τ l

n,j − ml
j )
n,j }E{zl

n,j }T

14

15

16

17
18
19
20
21
22
23
24
25
26

27

28

29

30
31

n,j − ml

j )T + Ut+1E{xl

n,j }(τ l

n,j − ml
j )

Ut+1 =

Al = 1

ΣN

(cid:48)
n=1Σn
j=1(τ l
(cid:48)
j=1E{zl
ΣN
n=1Σn
(cid:48)
j=1(τ l
n=1Σn
n(cid:48) ΣN
(cid:19)
(cid:18) Al
11 0
0 Al
22

n,j −ml

j )E{zl

n,j }

n,j (zl
n,j − ml

n,j )T }
j )(τ l

Ψl

t+1 =
End while
M l = I + U T
T r
i,n = (M l
Initialize Ψr
while Lr

t)−1Ut
t (Ψl
t,i(Ψl
t )−1U T
t randomly
c not converge

t,i)−1(Ti,n − µi), i ∈ {1, 2}, n = 1, ..., N

E-step:
Compute M r = I + V T
Compute E{zr
Compute E{(zr
M-step:

t (Ψr)−1Vt
n,j } = (M r)−1V T
n,j )T zr

t (Ψr)−1(τ r
n,j } = (M r)−1 + E{zr

n,j − mr
j )
n,j }E{zr

n,j }T

Vt+1 =

Ar = 1

n,j −mr

j )E{zr

n,j }

n,j (zr
n,j − mr

n,j )T }
j )(τ r

ΣN

(cid:48)
n=1Σm
j=1(τ r
(cid:48)
ΣN
n=1Σm
n=1Σm
m(cid:48) ΣN
(cid:18) Ar
11 0
0 Ar
22

j=1E{zr
j=1(τ r
(cid:19)

(cid:48)

Ψr

t+1 =
End while

n,j − mr

j )T + Vt+1E{zr

n,j }(τ r

n,j − mr
j )

end;

Fig. 2: Procedure of P2DCCA.

4.1 Experiments on synthetic data

In this section, we aim to verify our implementation of the proposed method
with the simplest possible scenario. So we generate some synthetic data and
projection matrices. Then estimate the projection matrices using our method
and compare them with the true ones. We know that P2DCCA estimations
are up to rotation, hence to simplify comparison we assume Z to be 1 × 1
dimension. We set dimensions of T1 and T2 to 5 × 5. Then we generate 1000
samples of Z from a normal distribution with zero mean and unit variance,
also we randomly generate the elements of Ui ∈ (cid:60)5, Vi ∈ (cid:60)5, i ∈ {1, 2} using

Probabilistic 2DCCA with EM

11

a uniform distribution in [0, 1] interval and consider them as the ground truth
projection matrices, then Ti will obtained using (11). In this equation, each
element of the residual matrices sampled from a gaussian distribution with
0 mean and σ2
i variance. Having the synthetic data we run P2DCCA algo-
rithm as discussed in Figure 2 and calculate Ui and Vi. We also run 2DCCA
and obtain the corresponding projection matrices. Then we compare obtained
matrices by these two algorithms with the ground truth projection matrices.
To cancel the scale factors we divide each transform by its norm before com-
parison. Euclidean distance is utilized to compare the normalized transforms.
Figure 3 shows the results. It is obvious that in the worst-case the distance
value becomes one and in the best-case it is zero. As it is depicted in this
ﬁgure, P2DCCA estimation of Ui and Vi for i ∈ {1, 2} are much closer to the
ground truth compared to the those obtained by 2DCCA. In this experiment
we used 1000 generated samples and set σi = 0.1. To examine the eﬀect of
these selections, we repeat our experiment with diﬀerent values of sample num-
bers and also noise variances. Figure 4 demonstrates the results. As it can be
observed from this ﬁgure the P2DCCA estimation in all cases is much closer
to the ground truth compared to the 2DCCA method.

Fig. 3: Comparison of 2DCCA and P2DCCA in loading factors estimation on the synthetic
data with 1000 generated samples and σ = 0.1

4.2 Experiments on the AR database

The AR face database contains over 4,000 color face images including frontal
views of faces with diﬀerent facial expressions, illumination conditions and

12

Mehran Safayani et al.

(a)

(b)

Fig. 4: Comparison of 2DCCA and P2DCCA in U1 loading factor estimation on the synthetic
data

(a)

(b)

(c)

Fig. 5: Exemplary face images in (a). AR-1, (b). AR-2 and (c). AR-3

occlusions. For most individuals, there are two sessions of images which were
taken in two diﬀerent time periods. Each session contains 13 images. In our
experiments, we used the ﬁrst session, because some individuals do not have
the second session of images. We collected 1310 face images of 131 people (72
male and 59 female). For each person, there are 10 diﬀerent face images in
our collected images: three with diﬀerent illumination conditions; three with
diﬀerent expressions; three with occlusions and the remaining images are those
with neutral expression and no occlusion which are known as reference images
in our experiments. To examine the performance of the proposed methods
in diﬀerent conditions, we partitioned the collected images into three subsets
known as AR-1, AR-2 and AR-3. For each individual, AR-1 contains four
images, three of which are images with diﬀerent lighting conditions and the
remaining one is the reference image. AR-2 is used to test performance of the
algorithms when there exist expression variation. AR-2 involves four images
per individuals: three images have diﬀerent expressions and the last one is
the reference image. AR-3 is prepared to test performance in the presence of
occlusion. Again, this subset contains four images per individuals where three
images were taken with glasses and the last one is the reference image. Fig-
ure 5 shows exemplary face images of a man and a woman in AR-1, AR-2
and AR-3, respectively. Image are gray scaled, resized and then normalized to
50 × 50 pixels. We compared the performance of the proposed method with
a range of diﬀerent supervised and unsupervised dimensionality reduction al-

Number of samples01002003004005006007008009001000Distance to ground truth10-310-210-1100101P2DCCA2DCCANoise variance00.10.20.30.40.50.60.70.80.91Distance to ground truth00.050.10.150.20.25P2DCCA2DCCAProbabilistic 2DCCA with EM

13

gorithms and diﬀerent versions of them including PCA, LDA, CCA, PPCA,
PCCA, 2DPCA, 2DLDA and 2DCCA. Both PCA and LDA based methods
work with one set of data. Also LDA based methods are supervised while PCA
and CCA based algorithms are unsupervised. The task here is to investigate
how well diﬀerent algorithms can relate face images with varying illumina-
tion conditions, expressions and occlusion, in correspondence to the reference
face images. The CCA, PCA, LDA, PPCA, PCCA, 2DCCA, 2DPCA, 2DLDA
and P2DCCA are used to extract features from facial images and then a 1-
NN classiﬁer is employed for classiﬁcation. Note that based on output type of
each of the algorithms (vector or matrix), for 2DCCA, 2DPCA, 2DLDA and
P2DCCA, Frobenius distance is used to calculate the distance between two
feature matrices, while for CCA, PCA, LDA, PPCA and PCCA the common
Euclidean distance measure is adopted. Furthermore, it should be noted here
that since PCCA suﬀers the small sample size problem, implementing it using
formulas introduced in [18] caused the covariance matrices to be singular. To
solve the problem, we did dimension reduction using PCA before implement-
ing the algorithm.

To evaluate the recognition accuracy, we used “three-fold cross-validation”.
As it is evident, CCA based algorithms need two sets of images for training,
where in this paper the training sets are called left training set and Right train-
ing set. To form the training sets, e.g., for AR-1, neutral images (images with
no illumination) are considered as Left training set, while to form the Right
training set, one of the three images of each individual with diﬀerent illumi-
nation conditions is selected randomly. The other two images are considered
as test images. This procedure is repeated for three times, where each time a
diﬀerent image among the three images is selected for the Right training set,
while neutral images are always used to form the Left training set. To test the
performance of each algorithm, all images of both right and left training sets
are projected on the new feature spaces using their corresponding transforms.
Also, each of the test images is projected on both feature spaces, so we have
two projection for every test image. Then we calculate the distance between
each of the two projected test images and projected training images. The label
of the training image with the nearest projection to any of the two test image
projection determine the ﬁnal class of the test image. This procedure iterates
until we ﬁnd the ﬁnal class for all images in the test set. These ﬁnal classes
are compared to the real classes of the images and the recognition accuracy of
each algorithm is calculated. Finally, the average recognition rate of the three
round experiments is recorded as the ﬁnal recognition accuracy.

Since PCA and LDA based algorithms work with one set of data, to have
a fair comparison we used two images as training and the other two images as
the test data, where neutral images are always in the training data together
with one of the other images with diﬀerent illuminations in each iterations.
Again the process is repeated three times and the ﬁnal accuracy is the average
of the three runs. Figure 7 shows the test process for P2DCCA.

14

Mehran Safayani et al.

Train and test procedure for AR2 and AR3 subsets are similar to AR1 and Ta-

Fig. 6: A representation of training and testing images used in each fold for AR-1

ble 1 through Table 3 demonstrate the recognition accuracy of evaluated algo-
rithms for the experiments conducted on AR-1, AR-2 and AR-3, respectively.
In these tables, d is the dimension of the reduced feature space. Note that
output for two dimensional algorithms (2DCCA, 2DPCA, 2DLDA, P2DCCA
and MP2DCCA) is of matrix type with dimension d × d, while for CCA, PCA,
LDA, PPCA and PCCA methods output is a vector of dimension d. In these
results we see that P2DCCA get the best performance among all tested meth-
ods. We see about 10% improvement of recognition rate for P2DCCA over
2DCCA in AR-1 and AR-3, and about 3% improvement in AR-2.

Table 1: Comparison of the average recognition accuracy rates of the nine evaluated algo-
rithms on AR-1 (%)

Figure 8 shows how the log-likelihoods of the left probabilistic model and the
right probabilistic model of P2DCCA improve with each iteration. As it can
be seen in the ﬁgure, both left and right models converge.

It should be noted that it is very common that in the algorithms for opti-
mizing row and column projections only one iteration with the iterative algo-

Probabilistic 2DCCA with EM

15

Table 2: Comparison of the average recognition accuracy rates of the nine evaluated algo-
rithms on AR-2 (%)

Table 3: Comparison of the average recognition accuracy rates of the nine evaluated algo-
rithms on AR-3 (%)

Fig. 7: Graphical representation of recognition process in P2DCCA

rithm is performed[27]. Therefore in all the experiments we use one iteration
of the algorithm, i.e., Tmax = 1. This signiﬁcantly reduces computational cost
of the algorithm. We tried more iterations and got no signiﬁcant improvement
in the recognition rate. Figure 9 shows the results for 1 to 5 iterations for AR1,
AR2 and AR3. This results support the idea of choosing Tmax = 1.

Left image setRight image setDimension reduction eq(34)i = 1m1m2comparem1m2Test image----Dimension reduction eq(34)i = 2Dimension reduction eq(34)i = 1Dimension reduction eq(34)i = 216

Mehran Safayani et al.

(a)

(b)

Fig. 8: Log-Likelihood values versus the number of iterations for (a) left probabilistic model
and (b) Right probabilistic model of P2DCCA for d = 20

Fig. 9: Recognition rate of AR1, AR2 and AR3 for T max > 1

4.3 Experiments on the UMIST database

The UMIST face database also known as Sheﬃeld face database [28] consists
of 564 images of 20 subjects. Subjects have diﬀerent races, sexes, and appear-
ances. For each subject, there are images with diﬀerent poses from proﬁle to
frontal view. Images have 256 grey levels with resolution of 220×220 pixels. In
our experiment, 360 images with 18 samples per subject are used to examine
performance of diﬀerent algorithms when face orientation varies signiﬁcantly.
Figure 10 shows 18 images of one subject.
We select frontal image as well as seven other randomly selected images for
training set and the remaining images for the test set. In the training phase
of CCA based methods, frontal image always is selected as the left training

Iterations12345Recognition accuracy(%)707580859095100AR1AR2AR3Probabilistic 2DCCA with EM

17

Fig. 10: Eighteen face examples of one subject from the UMIST database

image and one of the seven other images as the right training image. 1-NN
classiﬁer is used for classiﬁcation. This procedure is repeated for twenty times,
and the average recognition rates of algorithms are reported. Table 5 shows the
recognition accuracy of evaluated algorithms for the experiments conducted
on UMIST. In this test while P2DCCA achieved slightly better performance
compared to 2DCCA, it is not the best. In fact LDA achieved the best per-
formance. In the AR test, LDA had 2 images per class for training, but here
it has 8 images per class which leaded to best performance for this supervised
method. Ignoring LDA, we see that P2DCCA performance is higher than other
methods.

Table 4: Comparison of the average recognition accuracy rates of the nine evaluated algo-
rithms on UMIST (%)

Since the UMIST face dataset contains 20 subjects to be discriminated, the
LDA features is limited to 19. To be able to compare the results of LDA with
that of the other algorithms, we showed the results for d = 5, d = 10 and
d = 15 for LDA and larger values of d for other algorithms.

4.4 Evaluation of the Experimental Results

The above experiments showed that the accuracy of P2DCCA is consistently
better than other CCA based methods, i.e. CCA, PCCA and 2DCCA. But, a
question sill remains: “Are these diﬀerences statistically signiﬁcant?”. In this
section we answered the question by evaluating the experimental results using
independent-samples T-test (or independent t-test, for short). In this section
and also the next section, we only considered CCA based algorithms including

18

Mehran Safayani et al.

CCA, PCCA, 2DCCA and P2DCCA since the goal of this paper is to compare
the functionality of the newly proposed CCA based method with that of the
other CCA based algorithms. The desired signiﬁcance level is 0.05 and the
null hypothesis is that there is no signiﬁcant diﬀerence between recognition
rates of P2DCCA and CCA, PCCA and 2DCCA, respectively. We reject the
null hypothesis whenever the resulted ρ-value becomes lower than 0.05 and in
this case the result can be considered statistically signiﬁcant. It is necessary to
note that to run the t-test in each dataset, for each algorithm we considered
the highest recognition rate. Table 5 shows the ρ-value of the test. As can be
seen from this table, P2DCCA signiﬁcantly outperforms other algorithms and
the null-hypothesis has been rejected in all cases.

Table 5: The ρ-value associated with the null hypothesis: “no signiﬁcant diﬀerence between
recognition rates of P2DCCA and the corresponding algorithm”

Data Sets \Algorithms

CCA PCCA 2DCCA

AR-1
AR-2
AR-3
UMIST

1.9e-06
6.1e-12
2.5e-05
6.1e-05

9.5e-07
5.7e-12
4.8e-05
3.5e-05

8.6e-10
7.5e-05
5.2e-06
5.7e-03

4.5 Computational Complexity

This section compares the computational cost of the algorithms. To compare
the time complexity of the algorithms, we consider input images of size m × m
where we want to reduce their dimension to d × d in case of two-dimensional
algorithms. For CCA and PCCA, vectorization caused the input data to have
dimension m2 × 1 and the output to have dimension d × 1. However, for sim-
plicity, d is considered to be equal to m, i.e. d = m in our analysis. Table
6 shows the computational complexity of the algorithms. In this table, N is
the number of random samples in the dataset. It should be noted that there
are two types of iteration in the corresponding methods; one is the iteration
necessary for the convergence of the EM part of the algorithm and the other is
the iteration for alternating the optimization procedure between left and right
model. We show the former by t and the latter by r in the table. However,
r = 1 in our experiments.

Table 6: Time complexity of algorithms

Probabilistic 2DCCA with EM

19

5 Conclusion

This paper proposed a probabilistic model for two dimensional CCA termed
as P2DCCA together with an EM-based solution to estimate the parameters
of the model. Experimental results demonstrated the functionality of the pro-
posed method. The proposed P2DCCA has many advantages over 2DCCA
where the most signiﬁcant advantage is its ability to extend to a mixture of
P2DCCA model. It may also be possible to develop a probabilistic Bayesian
model for P2DCCA and gaining the beneﬁts of a Bayesian model. These are
our future works.

References

1. I. Jolliﬀe, Principal component analysis, Wiley Online Library, 2002.
2. H. Hotelling, Relations between two sets of variates, Biometrika (1936) 321–377.
3. C.-C. Jia, S.-J. Wang, X.-J. Peng, W. Pang, C.-Y. Zhang, C.-G. Zhou, Z.-Z. Yu, Incre-
mental multi-linear discriminant analysis using canonical correlations for action recog-
nition, Neurocomputing 83 (2012) 56–63.

4. Y. R. Wang, K. Jiang, L. J. Feldman, P. J. Bickel, H. Huang, Inferring gene association
networks using sparse canonical correlation analysis, arXiv preprint arXiv:1401.6504.
5. S. Huang, J. Chen, Z. Luo, Sparse tensor cca for color face recognition, Neural Com-

puting and Applications 24 (7-8) (2014) 1647–1658.

6. J. Ye, R. Janardan, Q. Li, Gpca: an eﬃcient dimension reduction scheme for image
compression and retrieval, in: Proceedings of the tenth ACM SIGKDD international
conference on Knowledge discovery and data mining, ACM, 2004, pp. 354–363.

7. D. Zhang, Z.-H. Zhou, (2d) 2pca: Two-directional two-dimensional pca for eﬃcient face

representation and recognition, Neurocomputing 69 (1) (2005) 224–231.

8. S. H. Lee, S. Choi, Two-dimensional canonical correlation analysis, Signal Processing

Letters, IEEE 14 (10) (2007) 735–738.

9. M. E. Tipping, C. M. Bishop, Probabilistic principal component analysis, Journal of
the Royal Statistical Society: Series B (Statistical Methodology) 61 (3) (1999) 611–622.
10. B. Thompson, Exploratory and conﬁrmatory factor analysis: Understanding concepts

and applications., American Psychological Association, 2004.

11. M. W. Browne, The maximum-likelihood solution in inter-battery factor analysis,
British Journal of Mathematical and Statistical Psychology 32 (1) (1979) 75–86.
12. A. Klami, S. Kaski, Probabilistic approach to detecting dependencies between data sets,

Neurocomputing 72 (1) (2008) 39–46.

13. C. Archambeau, N. Delannay, M. Verleysen, Robust probabilistic projections, in: Pro-
ceedings of the 23rd International conference on machine learning, ACM, 2006, pp.
33–40.

14. C. Archambeau, F. R. Bach, Sparse probabilistic projections, in: Advances in neural

information processing systems, 2009, pp. 73–80.

15. J. Zhao, P. L. Yu, J. T. Kwok, Bilinear probabilistic principal component analysis,
Neural Networks and Learning Systems, IEEE Transactions on 23 (3) (2012) 492–503.
16. S. Ioﬀe, Probabilistic linear discriminant analysis, in: Computer Vision–ECCV 2006,

Springer, 2006, pp. 531–542.

17. S. Kaski, J. Peltonen, Informative discriminant analysis, in: ICML, 2003, pp. 329–336.
18. F. R. Bach, M. I. Jordan, A probabilistic interpretation of canonical correlation analysis.
19. R. R. Sarvestani, R. Boostani, Ff-skpcca: Kernel probabilistic canonical correlation

analysis, Applied Intelligence (2016) 1–17.

20. A. Podosinnikova, F. Bach, S. Lacoste-Julien, Beyond cca: Moment matching for multi-
view models, in: Proc. 33rd International Conference on Machine Learning, 2016.
21. T. Michaeli, W. Wang, K. Livescu, Nonparametric canonical correlation analysis, in:

http://arxiv.org/abs/1511.04839, 2016.

20

Mehran Safayani et al.

22. M. Safayani, M. T. M. Shalmani, Matrix-variate probabilistic model for canonical cor-
relation analysis, EURASIP Journal on Advances in Signal Processing 2011 (1) (2011)
748430.

23. H. Wang, S. Chen, Z. Hu, B. Luo, Probabilistic two-dimensional principal component
analysis and its mixture model for face recognition, Neural Computing and Applications
17 (5-6) (2008) 541–547.

24. D. Tao, M. Song, X. Li, J. Shen, J. Sun, X. Wu, C. Faloutsos, S. J. Maybank, Bayesian
tensor approach for 3-d face modeling, Circuits and Systems for Video Technology, IEEE
Transactions on 18 (10) (2008) 1397–1410.

25. A. Martinez, R. Benavente, The ar face database, CVC Technical Report 24.
26. D. B. Graham, N. M. Allinson, Characterising virtual eigensignatures for general pur-

pose face recognition, in: Face Recognition, Springer, 1998, pp. 446–456.

27. J. Ye, R. Janardan, Q. Li, Two-dimensional linear discriminant analysis., in: NIPS,

2004, pp. 1569–1576.

28. http://www.sheffield.ac.uk/eee/research/iel/research/face.

Probabilistic 2DCCA with EM

21

A

As it is mentioned in Section 3, each column of the latent matrix Z has the distribution
N (0, I). Furthermore, based on (13) and (14) we can write:

p(τ l

n,j |zl

n,j ) ∼ N (U zl

n,j + ml

j , Ψl)

p(τ r

n,j |zr

n,j ) ∼ N (V zr

n,j + mr

j , Ψr).

Now from (20) and (27) and the distribution of columns of Z, we have

p(T l

1,n, T l

2,n, Zl

n) =

n(cid:48)
(cid:89)

[(2π)−

m1+m2
2

|Ψl|− 1

2

j=1

n,j − U zl

n,j − ml

j )T (Ψl)−1(τ l

n,j − U zl

n,j − ml

j ))

(τ l

exp(−

1
2
(cid:48)
(2π)− m
2 exp(−

1
2

(zl

n,j )T (zl

n,j ))],

p(T r

1,n, T r

2,n, Zr

n) =

m(cid:48)
(cid:89)

[(2π)−

n1+n2
2

|Ψr|− 1

2

j=1

n,j − V zr

n,j − mr

j )T (Ψr)−1(τ r

n,j − V zr

n,j − mr

j ))

(τ r

exp(−

1
2
(cid:48)
(2π)− n
2 exp(−

1
2

(zr

n,j )T (zr

n,j ))],

where |A| denotes the determinant of matrix A.
In the E-step, expectation of log likelihood for each of the probabilistic models is calculated
as:

E(Ll

c) =

N
(cid:88)

n(cid:48)
(cid:88)

[−

n=1

j=1

m1 + m2
2

log(2π) −

log|Ψl|

1
2

tr{(Ψl)−1(τ l

n,j )(τ l

n,j )T } + tr{(E[(zl

n,j )] − ml

tr{(E[zl

n,j ] − ml

j )T U T (Ψl)−1U (E[zl

n,j ] − ml

n,j }

j )T U T (Ψl)−1τ l
m(cid:48)
2

log(2π)

j )} −

tr{E[(zl

n,j )(zl

n,j )T ]}]

E(Lr

c ) =

N
(cid:88)

m(cid:48)
(cid:88)

[−

n=1

j=1

n1 + n2
2

log(2π) −

log|Ψr|

1
2

tr{(Ψr)−1(τ r

n,j )(τ r

n,j )T } + tr{(E[(zr

n,j )] − mr

tr{(E[zr

n,j ] − mr

j )T V T (Ψr)−1V (E[zr

n,j ] − mr

n,j }

j )T V T (Ψr)−1τ r
n(cid:48)
2

log(2π)

j )} −

tr{E[(zr

n,j )(zr

n,j )T ]}]

−

−

−

1
2
1
2
1
2

−

−

−

1
2
1
2
1
2

(35)

(36)

(37)

(38)

(39)

(40)

22

Mehran Safayani et al.

E[zl

n,j ] = (M l)−1U T (Ψl)−1(τ l

n,j − ml
j )

E[(zl

n,j )(zl

n,j )T ] = (M l)−1 + E[(zl

n,j )]E[(zl

n,j )]T

E[zr

n,j ] = (M r)−1V T (Ψr)−1(τ r

n,j − mr
j )

E[(zr

n,j )(zr

n,j )T ] = (M r)−1 + E[(zr

n,j )]E[(zr

n,j )]T .

(41)

(42)

(43)

(44)

By obtaining formulas of E(Ll
log-likelihoods.

c) and E(Lr

c ), the M-step is done by derivation of each expected

