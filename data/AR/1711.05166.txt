Robust Keyframe-based Dense SLAM with an
RGB-D Camera
Haomin Liu1†, Chen Li1†, Guojun Chen1, Guofeng Zhang1∗, Michael Kaess2, and Hujun Bao1

1State Key Lab of CAD&CG, Zhejiang University

2Robotics Institute, Carnegie Mellon University

1

7
1
0
2

v
o
N
4
1

]

V
C
.
s
c
[

1
v
6
6
1
5
0
.
1
1
7
1
:
v
i
X
r
a

Abstract—In this paper, we present RKD-SLAM, a robust
keyframe-based dense SLAM approach for an RGB-D camera
that can robustly handle fast motion and dense loop closure,
and run without time limitation in a moderate size scene. It
not only can be used to scan high-quality 3D models, but
also can satisfy the demand of VR and AR applications. First,
we combine color and depth information to construct a very
fast keyframe-based tracking method on a CPU, which can
work robustly in challenging cases (e.g. fast camera motion
and complex loops). For reducing accumulation error, we also
introduce a very efﬁcient incremental bundle adjustment (BA)
algorithm, which can greatly save unnecessary computation and
perform local and global BA in a uniﬁed optimization framework.
An efﬁcient keyframe-based depth representation and fusion
method is proposed to generate and timely update the dense 3D
surface with online correction according to the reﬁned camera
poses of keyframes through BA. The experimental results and
comparisons on a variety of challenging datasets and TUM
RGB-D benchmark demonstrate the effectiveness of the proposed
system.

Index Terms—RGB-D SLAM, bundle adjustment, mapping,

depth fusion

I. INTRODUCTION

Simultaneous localization and mapping (SLAM) is a fun-
damental problem in both, the robotics and computer vision
communities. Over the past decade, real-time structure-from-
motion or visual SLAM has seen many successes [1]–[3].
However, visual SLAM has inherent difﬁculty in handling
textureless scenes and in reconstructing dense 3D information
in real-time. Using depth sensors can help address these two
problems. Along with the popularity of depth sensors (e.g.
Microsoft Kinect and Intel RealSense 3D Camera), more and
more SLAM approaches [4]–[9] with depth or RGB-D sensors
have been proposed.

Most dense SLAM methods use a frame-to-frame or frame-
to-model alignment strategy, which easily results in accumula-
tion of drift and fails eventually in challenging environments.
Some methods [6], [7], [9] proposed to use non-rigid mesh
deformation techniques with loop closure constraints to opti-
mize the map and limit drift. However, the model error caused
by inaccurate alignment cannot be fully corrected by these
methods, which may lead to increasing tracking error and
eventual failure.

Recently, BundleFusion [10] was proposed, an end-to-end
real-time 3D reconstruction system that uses all RGB-D input

† Joint ﬁrst authors.
Corresponding
*

guofeng@cad.zju.edu.cn

author:

Guofeng

Zhang,

Email:

zhang-

and globally optimizes the camera poses and 3D structure in an
efﬁcient hierarchical way. Different from previous methods us-
ing a frame-to-model strategy, BundleFusion performs brute-
force matching for each input frame with all other frames,
and then aligns the 3D points for fusion. However, it requires
two powerful GPUs (a NVIDIA GeForce GTX Titan X and a
GTX Titan Black) to achieve real-time performance. Another
major limitation is that it saves all RGB-D input data and
only can run for about 10 minutes even with a very powerful
PC, making it inappropriate for virtual reality and augmented
reality applications which typically require much longer run
time. Most recently, Maier et al. [11] proposed to improve this
work by using a keyframe fusion and re-integration method
based on DVO-SLAM [12], which can perform real-time dense
SLAM with online surface correction using a single GPU.

In this paper, we present RKD-SLAM, a robust keyframe-
based dense SLAM system for an RGB-D camera that is able
to perform in real-time on a laptop without time limitation in a
moderate size scene. RKD-SLAM also can handle fast camera
motion and low-frame-rate live RGB-D sequences. The main
contributions of our paper are as follows:

1) We propose a robust keyframe-based RGB-D tracking
method which combines visual and depth information
to achieve robust and very fast camera tracking (about
70 ∼ 200 fps) on a single CPU.

2) We propose an efﬁcient incremental bundle adjustment
algorithm which makes maximum use of intermediate
computation for efﬁciency, while adaptively updating
affected keyframes for map reﬁnement.

3) We propose an efﬁcient keyframe-based depth represen-
tation and fusion method which can generate and timely
update the dense 3D surface with online correction
without delay.

II. RELATED WORK

In the past few years, many methods have been proposed
to use RGB-D camera data for dense 3D reconstruction and
real-time SLAM. Huang et al. [13] proposed to use RGB-
D data for real-time odometry, while dense mapping is done
ofﬂine using sparse bundle adjustment (BA). Endres et al. [14]
presented a 3D mapping system using various visual features
in combination with depth to estimate camera motion, while
using 3D occupancy grid maps to represent the environment.
Kerl et al. [12] proposed a dense direct RGB-D odometry by
minimizing photometric error and depth error that leads to a
higher pose accuracy when compared to sparse feature based

 
 
 
 
 
 
methods. Newcombe et al. [4] proposed an impressive dense
SLAM system called KinectFusion, which used an iterative
closest point (ICP) algorithm [15] to align each frame to the
global model with volumetric fusion. KinectFusion works well
in a small scene, but could not handle a large-scale scene
because in large-scale scenes the memory requirements for the
volumetric space representation quickly exceeds any available
memory. In addition, it suffers from drift problems and cannot
handle loop closure. Following KinectFusion, many methods
have been proposed to address these two problems. Most of
them [5], [16], [17] focused on exploiting more effective data
structures for real-time volumetric fusion in a larger scale
scene. For example, Kintinuous [16] extends the KinectFusion
with volume shift. Niener et al. [5] proposed to use a sparse
volumetric grid to store the volumetric information with spatial
hashing. However, the drift problem of pose estimation and
online dense surface adjustment are not addressed in these
methods.

Drift-free pose estimation and sparse mapping have been
extensively studied in visual SLAM. Many monocular SLAM
methods have been proposed which can perform real-time
tracking and sparse mapping in a small workspace [1] or even
a street-scale scene [2], [3]. Relocalization and loop closure
also can be handled online by some methods [2], [3], [18].
However, these methods do not generate dense 3D models.
Although some methods [19]–[22] have been proposed to
reconstruct dense 3D models online, it is still either limited
to a small scene, or drift-free dense 3D reconstruction is not
considered.

Although some ofﬂine methods [23]–[25] can close loops
to obtain a drift-free dense 3D reconstruction, the computation
speed is still far from real-time. Recently, Whelan et al.
[9] proposed a novel real-time dense RGB-D SLAM system
with volumetric fusion, which can detect loop closure in a
large-scale scene and correct drift through as-rigid-as-possible
surface deformation. Instead of volumetric fusion, ElasticFu-
sion [7] employed a surfel-based fusion method and also used
the non-rigid surface deformation technique for loop closure
and model reﬁnement. Both of these methods use frame-to-
model alignment, where the alignment error will affect the
model accuracy, and the erroneous model will signiﬁcantly
harm the camera tracking. Using surface deformation with
loop closure constraints cannot correct
this error, so the
tracking will eventually fail in complex environments. Bundle-
Fusion [10] uses brute-force matching to register frames, and
re-integrates the depth maps of adjusted frames to obtain
a globally consistent reconstruction. However, BundleFusion
saves all input data, which is intractable for processing long
sequences. In addition, the computation of brute-force match-
ing also will become too time consuming for processing a very
long sequence even with very powerful GPUs. Most recently,
Maier et al. [11] proposed to use a keyframe fusion and
re-integration strategy which can efﬁciently perform surface
correction on-the-ﬂy. In our system, we adopt this strategy
with further improvement to perform depth fusion more timely.
Bundle Adjustment [26] or pose graph optimization [27]–
[29] is frequently used in SLAM or SfM systems to reduce
accumulation error or close loops to eliminate reconstruction

2

Fig. 1. Framework of our system

drift. Several works exploited the incremental nature of SLAM
to speed up BA or smoothing [30]–[33]. Instead of construct-
ing and factorizing the information matrix from scratch for
each incoming frame, Kaess et al. [30] proposed to incremen-
tally update the QR factorization of the information matrix.
Later, Kaess et al. [31] further improved this method by using
the Bayes tree to efﬁciently identify the subset of variables
and the part of factorization that need to be updated. In [32], a
similar method was proposed to update the mean of variables.
In addition, it proposes an efﬁcient method to update the
covariance of variables. Most recently, Ila et al. [33] proposed
to incrementally update Schur complement for achieving fast
incremental bundle adjustment. The covariance matrix is also
efﬁciently recovered in their method.

III. FRAMEWORK OVERVIEW

Figure 1 illustrates the framework of our system, which
performs tracking and mapping in parallel threads. For each
input frame, which contains an RGB image and a depth image,
our system combines RGB-D alignment and a homography-
based feature tracking method with depth ﬁltering to make
camera tracking as robust as possible. We also extract ORB
features [34] for keyframes and match them with bags-of-
words place recognition method [35] to detect loop closure
and build loop constraints. Periodically, an incremental BA
is invoked in the background to reﬁne the camera poses of
keyframes and sparse 3D points. The mapping component will
fuse the depth map of each input frame if its camera pose is
accurately estimated. In order to allow adjusting of the dense
surface online, the depth map is ﬁrst fused to the keyframe
with largest overlapping, which is followed by de-integration
and integration in keyframes. This strategy allows our system
to only re-integrate the depth maps and associated 3D point
cloud of keyframes whose camera poses are reﬁned by BA. By
controlling the number of keyframes in a moderate size scene,
our system can run in real-time without time limitation, even
on a laptop.

IV. KEYFRAME-BASED TRACKING

Our keyframe-based tracking leverages both intensity and
depth information to track camera motion for each frame. We

RGB-D InputKeyframe-basedTrackingIncremental BAKeyframe-based DenseMappingCamera Poses & Depth MapsYesUpdated Posesof KeyframesNewKeyframe?3

where H(i−1)→i is obtained by direct image alignment using
the small blurry image (SBI) as used in [36]:

H(i−1)→i = arg min

H

(cid:213)

||

˜Ii−1(x) − ˜Ii( ˜Hx)
σc

||δ,

(2)

x∈ ˜Ii−1
where ˜Ii−1 and ˜Ii are the SBIs of last frame and the current
frame respectively. The tilde above the homography ˜H con-
verts H from the original image space to that of SBI. After
obtaining a set of feature matches Mk→i = {(xk, xi)} between
keyframe k and current frame i, we reﬁne Hk→i by:

Hk→i = arg min

(cid:213)

||

˜Ik(x) − ˜Ii( ˜Hx)
σc

||δ

H

x∈ ˜Ik
+ (cid:213)

(xk,xi )∈Mk→i

1
σ2
x

||Hxk − xi ||2
2 .

(3)

Fig. 2. Framework of our keyframe-based tracking.

combine both dense RGB-D based and sparse feature based
methods to make the odometry more robust. The framework
is illustrated in Figure 2.

A. Feature Tracking with Low-resolution RGB-D Alignment

For each current frame Fi (its camera pose is denoted as Ci),
we ﬁrst use a fast RGB-D alignment algorithm to estimate the
relative pose T(i−1)→i from last frame Fi−1. Here we only need
a coarse estimate of T(i−1)→i, so we use “small blurry image”
(SBI) as used in [36] to achieve strong real-time performance
without GPU acceleration.

Similar to [12], we project the previous frame Fi−1 to current
frame Fi and estimate the relative pose T(i−1)→i by solving the
following energy function combining photometric error and
inverse depth error

||δ+

Incorporating Mk→i into direct image alignment can prevent
the solution being biased towards a major plane.

T(i−1)→i = arg min

(cid:213)

(||

˜Ii(π(KT( ˜Zi−1(x)K−1 ˆx))) − ˜Ii−1(x)
σc

x∈ ˜Ii−1

T
(π(KT( ˜Zi−1(x)K−1 ˆx))) − z−1(T( ˜Zi−1(x)K−1 ˆx))

˜Z −1
i

||

σz

||δ),
(1)
where ˜Ii−1 and ˜Ii are the SBIs of Fi−1 and Fi respectively.
˜Zi−1 and ˜Zi are the downsampled depth maps of Fi−1 and Fi
respectively. ˆx denotes the homogenous coordinate of x, and
z(X) extracts the z-component of X. π(·) is the projective func-
tion π([x, y, z](cid:62)) = [x/z, y/z](cid:62). σc and σz are the parameters
controlling corresponding weights. ||(cid:15)||δ is the Huber norm

||(cid:15)||δ =

(cid:26) ||(cid:15)||2

δ(2||(cid:15)|| − δ)

if ||(cid:15)|| ≤ δ,
otherwise.

In our experiments, we generally set the Huber threshold δ =
1.345.

An accurate pose estimate is obtained by feature correspon-
dences. We track the map features in keyframes to the current
frame by the homography-based feature tracking proposed in
[18] to handle strong rotation and fast motion. For reducing
computation, we only use global homography for tracking.
During strong rotation and fast motion, the perspective dis-
tortion between frames may be too large for robust feature
matching. Homography helps to rectify the patch distortion,
so that the simple zero-mean SSD [1] is able to work.

As in [18], we propagate the global homography from the
last frame to the current frame. For a keyframe k, the global
homography is propagated as Hk→i = H(i−1)→iHk→(i−1),

Note that we only need to track a small set of keyframes
to the current frame. Similar to [3], we ﬁrst select the set
K1 containing keyframes sharing common points with the last
frame, then select the second set K2 sharing common points
with keyframes in K1.

i

Homography is also used to determine a small search region
around xhomo
instead of searching along the whole epipolar
line. In this work, since we have depth measurement zk and
the estimated camera pose obtained by RGB-D alignment
Ci = T(i−1)→iCi−1, we deﬁne the search region as the union
=
of the one around xhomo
k (zkK−1 ˆxk))). Here ˆxk denotes the homogenous
π(K(CiC−1
coordinate of xk, and K is the intrinsic matrix which is
assumed to be known and constant.

and the one around xRGB-D

i

i

With the relative pose T(i−1)→i estimated by low-resolution
RGB-D alignment and the set of 3D-2D feature correspon-
dences X = {(Xj, xj)} obtained by homography-based feature
tracking, we estimate the camera pose Ci by minimizing both
the relative pose error and re-projection error

C∗
i

= arg min
Ci

||log(CiC−1

i−1T−1

(i−1)→i

)||2
ΣT

+

(cid:213)

(cid:32)

||

(X j,x j )∈X

π(K(CiXj )) − xj
σx

||δ + ||

z−1(CiXj ) − z−1
j
σz

(cid:33)

(4)

||δ

,

where log(T) maps the 3D rigid transform T ∈ SE(3) to se(3),
= (cid:15)(cid:62)Σ−1(cid:15)
and returns the minimal vector in R6. Here, ||(cid:15)||2
Σ

is the squared Mahalanobis distance, and zj is the measured
depth value at xj in the current frame. σx and σz are normal-
ization parameters that control the corresponding weights. In
our experiments, they are generally set to 1 pixel and 0.05,
respectively.

B. Depth Filtering

If tracking fails, we invoke the relocalization procedure as
in [36] to track features again. Otherwise, we use the new
feature measurements to ﬁlter depths of those features. In the
pose estimation of (4), the 3D positions X are assumed to be
known and kept ﬁxed during optimization. X can be obtained
directly from the depth measurement at the keyframe k when
the feature was ﬁrst extracted, i.e. X = π−1(Ck, xk, zk), where
π−1(C, x, z) = C−1(zK−1 ˆx). The depth value can be further
reﬁned by the following frames with new depth measurements.
The depth ﬁlter must be robust to outliers since many features
are extracted at object boundaries where depth measurements
are unreliable.

We use the Bayesian depth ﬁlter proposed in [37], which has
been successfully used in other SLAM systems like SVO [38]
and REMODE [39]. The ﬁlter continuously updates the joint
distribution of the depth estimate ρ and the inlier probability
γ by each incoming depth measurement ˆρ with variance ˆσ2.
The distribution of the measurement ˆρ given the correct ρ and
γ is modeled as a Gaussian + uniform mixture distribution:
P( ˆρ| ρ, γ) = γN ( ˆρ| ρ, ˆσ2) + (1 − γ)U( ˆρ| ρmin, ρmax).

(5)

Given the set of measurements ˆρ1, · · · , ˆρn, the Bayesian pos-
terior distribution of ρ and γ is estimated by

P(ρ, γ| ˆρ1, · · · , ˆρn) ∝ P(ρ, γ)

n
(cid:214)

i

P( ˆρi | ρ, γ)

(6)

∝ P(ρ, γ| ˆρ1, · · · , ˆρn−1)P( ˆρn| ρ, γ),

where P(ρ, γ) is the prior on ρ and γ. Simply evaluating
all probabilities of (6) and choosing the best ρ and γ is
computationally too expensive. The authors of [37] propose
a parametric Gaussian × beta approximation to (6):
P(ρ, γ| ˆρ1, · · · , ˆρn) ≈ N (ρ| µn, σ2

n)B(γ|an, bn),

(7)

where µn, σ2
n are parameters controlling the Gaussian dis-
tribution, and an, bn are parameters controlling the beta
distribution. These parameters are incrementally updated by
each depth measurements. Substituting (7) to (6), we obtain

N (ρ| µn, σ2
∝N (ρ| µn−1, σ2

n)B(γ|an, bn)

n−1)B(γ|an−1, bn−1)P( ˆρn| ρ, γ).

(8)

By matching the ﬁrst and second order moments for ρ and γ
in (8), the updated parameters (µn, σ2
n, an, bn) can be derived.
In our implementation, we use inverse depth [40], which is
better approximated by a Gaussian distribution, i.e. ρ = 1/z.
For each feature correspondence (xk, xi) between keyframe Fk
and current frame Fi, we obtain a new measurement ρi→k for
the inverse depth ρk of xk as:

ρi→k = arg min

ρ

1
σ2
x
+ 1
σ2
z

||π(K(Ciπ−1(Ck, xk, ρ−1))) − xi ||2
2

(z−1(Ciπ−1(Ck, xk, ρ−1)) − z−1
i

)2,

(9)

4

Fig. 3. Comparison with/without
low-resolution RGB-D alignment. (a)
Warping frame 27 to frame 28 with the camera pose estimated by combining
feature tracking and low resolution RGB-D alignment. (b) Warping frame
27 to frame 28 with the camera pose estimated by feature tracking only. (a)
has much better alignment result than (b), which indicates that the estimated
camera pose is more accurate.

where π−1(C, x, z) = C−1(zK−1 ˆx). The ﬁlter continuously
updates the joint distribution of ρk and its inlier probability
by each incoming ρi→k. Please see [37] for more details. At
last, we decide whether Fi is selected as a new keyframe. The
keyframe which has maximal number of feature matches with
Fi is denoted as FKi . If the difference of view angle between
FKi and Fi exceeds 45◦, or the distance between them exceeds
0.5zKi (zKi is the mean depth of FKi ), then we select Fi as a
new keyframe.

Figure 3 shows a comparison with and without

low-
resolution RGB-D alignment. For simulating fast motion, we
extract one frame for every 10 frames from “fr3_long_ofﬁce”
sequence in TUM RGB-D dataset [41] to constitute a new se-
quence and then perform tracking. With low-resolution RGB-
D alignment, the tracking robustness is signiﬁcantly improved
but the computation does not increase much.

V. INCREMENTAL BUNDLE ADJUSTMENT

BA is performed when a new keyframe is inserted, or a
new loop is found. In the former case, performing global
BA seems to be unnecessary because only the local map will
actually change. However, only performing local BA tends to
be suboptimal especially when the local map contains large
error. In that case it is better to involve more variables to
BA, or else the error cannot be completely eliminated. We
propose an efﬁcient incremental BA (called EIBA) that is able
to provide nearly the same solution as global BA, but with
signiﬁcantly less computation time, which is proportional to
how many variables are actually changed.

Before diving into our EIBA, we ﬁrst revisit the algorithm
of standard BA [26]. For easier illustration, we ﬁrst introduce
a regular BA function as follows:

(cid:32)

(cid:213)

(cid:213)

j

i ∈Vj

||

π(K(CiXj)) − xji
σx

||δ + ||

z−1(CiXj) − z−1
ji
σz

(cid:33)

||δ

,

which contains re-projection error term and inverse depth prior
term. Vj is the set of cameras in which point j is visible. The
Huber norms can be converted to the form of squared sum
using re-weighting scheme [42]:

f = (cid:213)

(cid:213)

j

i ∈Vj

||fi j(Ci, Xj)||2
2,

(10)

(a)(b)where fi j ∈ R3 (ﬁrst two components for image re-projection
and the third for depth prior). At each iteration, fi j is linearized
at the current estimate as

fi j(Ci, Xj) ≈ JCi j δCi

+ JXi j δX j − ei j,

(11)

is the Jacobian of fi j with respect to Ci, JXi j

where JCi j
is
the Jacobian of fi j with respect to Xj, and ei j is the residual
error of fi j. So we have

f ≈ ||Jδ − e||2
2,

(12)

where J is the 3nx × (6nc + 3np) Jacobian matrix, e is the
error vector, nx is the number of re-projection functions, nc
and np is the number of cameras and points respectively. δ
is the variable for the current iteration, δ = [δ(cid:62)
X](cid:62), δC =
C, δ(cid:62)
](cid:62). The update δ is
[δ(cid:62)
Xn p
C1
obtained by solving the normal equations

](cid:62) and δX = [δ(cid:62)
X1

, · · · , δ(cid:62)

, · · · , δ(cid:62)

Cnc

J(cid:62)Jδ = J(cid:62)e.

(13)

Since each fi j relates only one camera and one point, the
normal equations are sparse and have the following form:

(cid:20) U W
W(cid:62) V

(cid:21) (cid:20)δC
δX

(cid:21)

=

(cid:21)

(cid:20)u
v

,

(14)

where U and V are nc × nc and np × np block matrices
respectively, and only the diagonal block matrices Uii and
Vj j are non-zero. W is a nc × np block matrix with non-zero
block matrices Wi j if and only if point j is visible in camera
i. For efﬁcient indexing and computation, we actually do not
construct the whole matrices for U, V and W. Similar to [43],
we compute and store the small non-zero block matrices Uii,
Vii, Wii. Compared to using general sparse matrix format, this
data structure is more efﬁcient and requires less memory space.
Especially, when new keyframes or 3D points are added during
incremental reconstruction, we do not need to reconstruct J(cid:62)J
from scratch and only need to add new block matrices.

We ﬁrst introduce the standard BA procedure as described
in Algorithm 1. Equation (14) can be efﬁciently constructed as
in step 1. A common strategy to solve (14) is to marginalize
all points to construct the Schur complement and solve δC ﬁrst

SδC = g,
S = (U − WV−1W(cid:62)),
g = u − WV−1v.

(15)

Note that S is also sparse, with non-zero block matrix Si1i2 if
and only if camera i1 and i2 share common points, thus can
be efﬁciently constructed as in step 2 of Algorithm 1. The
sparseness of S can also be exploited to solve δC. We use the
preconditioned conjugate gradient (PCG) algorithm [43] which
naturally leverages the sparseness of S. With solved δC, each
δX j can be solved separately by back substitution:

δX j

= V−1
j j

(cid:213)

i ∈Vj

(cid:169)
vj −
(cid:173)
(cid:171)

W(cid:62)

i j δCi

(cid:170)
(cid:174)
(cid:172)

.

(16)

Because of the criterion of keyframe selection, only a small
number of keyframe pairs share common points. We assume
the number of 3D points is N, the number of keyframes is

5

TABLE I
COMPUTATIONAL COMPLEXITY FOR EACH STEP IN STANDARD BA
(ALGORITHM 1).

Step
1
2
3
4

Complexity
O(Nl) or O(K m)
O(Nl2) or O(K ml)
O(Kt)
O(N ) or O(K m/l)

K, and the average observation number of each 3D point in
keyframes is l. So the average number of the observations in
each keyframe is m = Nl
K . We assume the average number of
PCG iterations is t. Table I lists the computational complexity
for each step, showing that most computation is required for
steps 1 and 2. In most cases, m is much larger than the number
of PCG iterations t, typically hundreds for m and dozens for
t. Then the computation consumed in steps 1 and 2 would be
hundreds of times larger than for step 3.

During incremental reconstruction, most variables are nearly
unchanged after global BA, thus most computation in steps
1, 2, and 4 are actually unnecessary. Speciﬁcally, in step 1,
the contribution of most fi js to (14) nearly remains the same
between successive iterations. Here, we propose an efﬁcient
incremental BA (EIBA) algorithm which can make maximum
use of intermediate computation to save computation. As
shown in Algorithm 2,
instead of constructing (14) from
scratch at each iteration, we update (14) from the last iteration.
i j, AV
i j and bv
i j, bu
We store the effect of fi j
i j.
= 0
= 0 and bv
We initialize AU
i j
i j
in the beginning. They are re-computed if and only if the
linearization point of fi j is changed. In this case, we remove
their contribution to (14) from the last iteration, refresh them,
and update (14) for the current iteration. If and only if Vj j
is updated, point j must be re-marginalized. Then we update
point marginalization and Schur complement (15) in a similar
way, see Algorithm 2 for details. In step 3, we solve (15) by
PCG, and change Ci only if ||δCi || exceeds a threshold (cid:15)c. In
step 4, we perform back substitution only for points visible in
the changed cameras, and change Xj only if ||δX j || exceeds a
threshold (cid:15)p.

to (14) in AU
= 0, bu
i j

= 0, AV
i j

The above paragraphs introduce the incremental optimiza-
tion with a regular BA function. Actually, our EIBA is quite
general and can be naturally extended to solve the following
energy function:

(cid:32)

(cid:213)

(cid:213)

j

i ∈Vj

||

π(K(CiXj)) − xji
σx

||δ + ||

z−1(CiXj) − z−1
ji
σz

(cid:33)

||δ

+ (cid:213)

(i1,i2)∈L

||log(Ci1 ◦ Ci2 ◦ T−1
i1i2

)||2

Σi1 i2

,

(17)
where L is the set of loop constraint. Each loop constraint
is represented as relative pose Ti1i2 with covariance Σi1i2. It
does not harm the sparseness of normal equation or Schur
complement. When the state of Ci1 or Ci2
is changed, the
error function is re-linearized as

f(Ci1, Ci2) ≈ Ji1 δCi1

+ Ji2 δCi2

− e.

(18)

Algorithm 1 One iteration in standard BA

Algorithm 2 One iteration in our incremental BA

1) Construct normal equations (14)

1) Update normal equations (14) and Schur complement (15)

U = 0; V = 0; W = 0; u = 0; v = 0
for each point j and each camera i ∈ Vj do

for each point j and each camera i ∈ Vj that Ci or Xj is
changed do

6

JCi j
JXi j

Construct linearized equation (11)
Uii+ = J(cid:62)
Ci j
Vj j + = J(cid:62)
Xi j
ui+ = J(cid:62)
Ci j
vj + = J(cid:62)
Xi j
Wi j = J(cid:62)
Ci j

ei j
ei j
JXi j

end for

JCi j ; Sii+ = AU
i j
JXi j ; Vj j + = AV
i j

Construct linearized equation (11)
Sii− = AU
i j ; AU
i j
Vj j − = AV
i j ; AV
i j
= J(cid:62)
gi− = bu
i j ; bu
i j
Ci j
= J(cid:62)
vj − = bv
i j ; bv
i j
Xi j
Wi j = J(cid:62)
JXi j
Ci j
Mark Vj j updated

= J(cid:62)
Ci j
= J(cid:62)
Xi j
ei j ; gi+ = bu
i j
ei j ; vj + = bv
i j

2) Marginalize points to construct Schur complement (15)

end for

S = U
for each point j and each camera pair (i1, i2) ∈ Vj × Vj
do

Si1i2 − = Wi1 j V−1

j j W(cid:62)
i2 j

end for
g = u
for each point j and each camera i ∈ Vj do

gi− = Wi j V−1

j j vj

end for
3) Update cameras

Solve δCi in (15) using PCG [43]
for each keyframe i do
Ci = exp(δCi )Ci

end for

4) Update points

for each point j do

Solve δX j by (16)
Xj + = δX j

end for

Ji2 and J(cid:62)
Ji1, J(cid:62)
Then J(cid:62)
Ji2 are updated to Si1i1, Si1i2 and Si2i2
i2
i1
i1
respectively. Similarly, J(cid:62)
e and J(cid:62)
e are updated to gi1 and gi2
i2
i1
respectively. In addition, we use inverse depth to parameterize
Xj. Assuming the ﬁrst keyframe that Xj is observed is frame k,
we have Xj = C−1
k (zjkK−1 ˆxjk). So each re-projection equation
fi j actually relates two camera poses (i.e. Ci and Ck) and one
3D point Xj. So at each iteration, fi j is linearized as

fi j(Ci, Ck, Xj) ≈ JCi j δCi

+ JCk j δCk

+ JXi j δX j − ei j,

(19)

where JCk j
is the Jacobian of fi j with respect to Ck. So in
Step 1 of Algorithm 2, we also need to update Skk, Sik, Wk j
and gk for each observation.

Although our incremental Schur complement is essentially
similar to [33], our computation is more efﬁcient with block-
wise matrix computation and storing the updated matrix blocks
for reducing calculation. In addition, we use preconditioned
conjugate gradient (PCG) algorithm to solve the linear system,
which is more efﬁcient than using factorization methods due
to the following reasons: 1) block-based PCG can better
leverage the sparseness to efﬁciently solve Schur complement
as veriﬁed in [43], and 2) PCG can make the most of the
incremental nature because good initial values of the variables

2) Update point marginalization and Schur complement (15)

for each point j that Vj j is updated and each camera pair
(i1, i2) ∈ Vj × Vj do

+ = AS

Si1i2
AS
i1i2 j
Si1i2 − = AS

i1i2 j
= Wi1 j V−1

i1i2 j

j j W(cid:62)
i2 j

end for
for each point j that Vj j is updated and each camera i ∈ Vj
do

gi+ = bg

i j ; bg

i j

= Wi j V−1

j j vj ; gi− = bg

i j

end for
3) Update cameras

Solve δCi in (15) using PCG [43]
for each keyframe i that ||δCi || > (cid:15)c do

Ci = exp(δCi )Ci
Mark Ci changed

end for

4) Update points

for each point j that any Ci with i ∈ Vj is changed do

Solve δX j by (16)
if ||δX j || > (cid:15)p then
Xj + = δX j
Mark Xj changed

end if

end for

can be easily obtained in incremental BA so that a few
iterations are generally enough to converge.

Kaess et al. [44] also performs incremental BA (called
iSAM2) by updating a matrix factorization. In iSAM2, the
square root information matrix of J is encoded in a Bayes tree,
in which each node contains a set of frontal variables F and
represents a conditional density P(F |S), where S is contained
in the frontal variables of its parent node. Inserting a new
keyframe will only affect nodes containing its visible points
as frontal variables and their ancestor nodes. All the affected
variables will be re-eliminated. For efﬁciency, it is better to
push the visible points of the new keyframe to the root, i.e.
marginalizing these points in the end. However, if their visible
cameras are marginalized before them, correlations among
these points will occur, which signiﬁcantly degrades efﬁciency.
In addition,
if the camera is moving to and fro, a large
number of invisible points may also be affected by iSAM2.
By comparison, in our EIBA, points are always marginalized

TABLE II
TIMING COMPARISON FOR INCREMENTAL BA.

Sequence

Num. of Camera / Points

Num. of Observations

EIBA

fr3_long_ofﬁce
fr2_desk

92 / 4322
63 / 2780

12027
6897

88.9ms
34.8ms

No relinearization
983.9ms
507.8ms

iSAM2
relinearizeSkip = 10
1968.2ms
850.4ms

relinearizeSkip = 5
2670.9ms
1152.0ms

7

ﬁrst to minimize ﬁll-in, and only the points visible in the new
keyframe will be affected. Although all the cameras will also
be affected in step 3, since we are dealing with moderate
size scenes, the number of cameras is much smaller than the
number of points potentially affected by iSAM2. Besides, the
Schur complement is very sparse, so that PCG is able to solve
it very efﬁciently.

We use “fr3_long_ofﬁce” and “fr2_desk” sequences from
TUM RGB-D benchmark [41] to make comparisons with
iSAM2. On “fr3_long_ofﬁce”, there are 92 keyframes, 4, 322
3D points and 12, 027 observations. On “fr2_desk” sequence,
there are 63 keyframes, 2, 780 3D points and 6, 897 obser-
vations. We perform incremental BA for (17) while adding
each new keyframe. For fair comparison, both two mehods
perform only one iteration. Since our EIBA applies SSE
instructions for code optimization, we also enable SSE op-
timization while compiling Eigen library used in iSAM2. We
use Gauss-Newton optimization method for both EIBA and
iSAM2. In addition, the linearization parameter will signif-
icantly inﬂuence the speed and accuracy of iSAM2. In our
experiments, we test three conﬁgurations for iSAM2, i.e. no
linearization, linearization every 10 steps, and linearization
every 5 steps. The running time is tested on a desktop PC
with i7 3.6GHz CPU and 16G memory. Figure 4 shows the
computation time for EIBA and iSAM2 while adding each new
keyframe (the time of loading data is not included). Table 4
shows the total running time. As can be seen, for our EIBA,
the computation almost keeps constant (about 1ms) when the
number of keyframes increases, except when loop closure is
detected, in which case the poses of more keyframes need to
be updated. Although the computation of iSAM2 also almost
keeps constant, it is generally slower than ours by an order
of magnitude even without using linearization. The optimized
reprojection error by EIBA is lower than iSAM2 without
linearization and comparable with iSAM2 with relinearizeSkip
= 5, as shown in Figure 5.

Especially, we found that the computation time of iSAM2
signiﬁcantly increases when loop clousre is detected. In con-
trast, the computation time of EIBA does not increase so much
(only increases to about 10ms). The reason is that when a large
loop detected, the information matrix will become very dense
and almost all varaibles need to be updated. In EBA, although
the Schur complement matrix S will become very dense in
this case, we use block-based PCG algorithm to solve the
linear system which is more efﬁcient than using a factoriation
method.

VI. KEYFRAME-BASED DENSE MAPPING

Fig. 4. The computation time of our EIBA and iSAM2 while incrementally
adding each new keyframe on “fr3_long_ofﬁce” sequence.

Fig. 5. The optimized reprojection error (RMSE) for our EIBA and iSAM2
while incrementally adding each new keyframe on “fr3_long_ofﬁce” se-
quence.

ing [5], to fuse the depth maps to construct the complete 3D
model. Kahler et al. [8] proposed a very efﬁcient volumetric
integration method based on voxel block hashing. We adapt
this method for fast volumetric fusion. When the camera pose
of the current frame is estimated with good quality, we need
to fuse the depth map into the global model. However, if we
directly integrate the depth map of each frame and discard
the frame, we could not correct the model again when a loop
closure is detected or the poses of frames are reﬁned by BA. A
simple solution is to store all the depth maps and re-integrate
them once the camera poses are reﬁned. However, this will
be intractable for real-time application since the number of
frames always increases and may become very large. So we
proposed to use keyframes to represent the depth data and
operate integration and de-integration in keyframes.

Similar to [11], we also do integration and de-integration
in keyframes. We use a volumetric method with spatial hash-

If the current frame Fi

is selected as keyframe, we can
directly integrate the depth map Di into the global model. For

102030405060708090Key Frames10-210-1100101102103Time (ms)iSAM2 Relinearize Skip 10iSAM2 Relinearize Skip 5iSAM2 No RelinearizationEIBA102030405060708090Key Frames11.522.533.544.55Reprojection Error (pixel)iSAM2 Relinearize Skip 10iSAM2 Relinearize Skip 5iSAM2 No RelinearizationEIBAeach voxel v, its truncated signed distance is denoted as D(v),
and the weight is denoted as W(v). For pixel x in Fi, its SDF
is deﬁned as φ(x) = Di(x) − zi(v), where zi(v) denotes the
projected depth in Fi for voxel v. If φ(x) ≥ −µ where µ is a
pre-deﬁned truncated value, we can update the corresponding
TSDF of v as

8

D(cid:48)(v) = D(v)W(v) + wi (x) min(µ, φ(x))

W(v) + wi (x)
where wi(x) is the integration weight for x.

, W(cid:48)(v) = W(v) + wi (x),

(20)

If Fi is not selected as keyframe, we ﬁrst ﬁnd the keyframe
which has maximal number of feature matches with Fi,
denoted as FKi . We de-integrate the depth map of FKi from
the global model. Inspired by [10], the de-integration operation
is similar to integration. If φ(x) ≥ −µ, each voxel v can be
updated as

D(cid:48)(v) = D(v)W(v) − wi (x) min(µ, φ(x))

W(v) − wi (x)

, W(cid:48)(v) = W(v) − wi (x).

(21)

After de-integration, we fuse depth map Di

into FKi by
projecting it to FKi , which is similar to that in [11]. The major
difference is that we take into account the occlusion and store
the unfused depths instead of simply discarding. For pixel x
in Fi, its projection in FKi is denoted as y. If the difference of
the inverse depth of pixel y (i.e. 1/DFKi
(y)) and the projected
inverse depth of x (denoted as 1/zi→Ki
) is less than a threshold
τd, we ﬁlter the depth of y as

x

D(cid:48)(y) =

wKi (y)DKi (y) + wi (x)z i→Ki
wKi (y) + wi (x)

x

, wKi

= wKi (y) + wi (x).

(22)

About wi(x) in (20), (21) and (22), we set it as follows: if x
is in a key frame, wi(x) is set to the ﬁltering number of x,
otherwise it is set to 1.

We count the fusion number NKi for each keyframe FKi
to control the maximum number of depth fusion since too
many fusions may be unnecessary and even degrade the
reconstruction quality. Since the overlap of Fi and FKi
is
generally large, most depths of the current frame can be fused
into FKi except some pixels that are occluded or out of view.
This strategy can signiﬁcantly reduce the depth redundancy. If
the number of unfused depths is less than a threshold τ, we
simply discard these unfused 3D points. Otherwise, we create
a point cloud set Vi to store these unfused 3D points, and link
it to FKi (we store the relative pose between Fi and FKi for 3D
points projection during integration and de-integration). Then
we integrate the updated depth map DKi . If Vi is not empty,
the 3D points in Vi are also projected into Fi, and then we
perform integration on Fi. So for each incoming frame that is
not selected as keyframe, we perform two integrations and one
de-integration. Since the number of unfused 3D points in Fi is
small, the integration time is also small. So the computation
time of our keyframe-based fusion is generally slightly larger
than two times that of the traditional volumetric fusion method.
In [11], they ﬁrst fuse a constant number of non-keyframes
to a nearest keyframe, and then integrate the depth map of
this keyframe to the 3D model. The 3D model will be not
updated until the keyframe fusion is ﬁnished. In our method,
since we ﬁrst de-integrate the old depth map of keyframe FKi
from 3D model and then re-integrate the updated depth map

Fig. 6. Comparison with/without re-integration. (a) The reconstructed 3D
model without re-integration. (b) The reconstructed 3D model with our
keyframe-based re-integration, which is more accurate and globally consistent
than (a), as highlighted with the red rectangle.

immediately while fusing each non-keyframe Fi to FKi , the 3D
model can be timely updated without delay. If NKi is large and
the number of keyframes is not increased for a long time, it
means that not sufﬁcient new content has been scanned. In
this case, we simply discard Di without fusion.

To further reduce redundancy, if the current frame Fi
is
selected as keyframe, we ﬁnd the point cloud sets linked to
nearby keyframes and fuse them to Fi. If the remaining number
of points of Vj is too small after fusion (less than 2, 000 in our
experiments), we will simply discard it and de-integrate it from
the global map. Otherwise, we only de-integrate the 3D points
which have been fused into Fi. For real-time computation, we
only fuse 5 ∼ 10 point cloud sets in our experiments.

If the poses of keyframes are modiﬁed by BA, we need
to re-integrate the depth maps of all the updated keyframes
and their linked point cloud sets. However, if the number of
adjusted keyframes is large, the re-integration time will be too
large to satisfy real-time applications. Therefore, we propose
to limit the number of re-integration operations for each time
instance. We maintain a update queue for the keyframes which
poses have been updated. The keyframes with largely changed
poses will be re-integrated with higher priority. This strategy
can guarantee that the mapping can always run with almost
constant speed even when BA is invoked. In [11], they perform
depth re-integration only when receiving a pose update, and
the uncorrected depth maps of the adjusted poses need to
wait for re-integration in a ﬁnal pass. In contrast, we perform
surface update for each time instance so that the surface can
be corrected more timely. The uncorrected depth maps of the
updated poses still have chance to be re-integrated in the next
time instance, and do not need to wait for the re-integration
in a ﬁnal pass. Figure 6 shows a comparison with and without
re-integration. As can be seen, due to accumulation error, if
we do not re-integrate the depth maps, the reconstructed 3D
surface has obvious artifacts. In contrast, with our keyframe-
based re-integration, the reconstructed 3D surface becomes
more accurate and globally consistent.

For further acceleration, we can fuse only one frame out

(a)(b)9

Fig. 7. Our reconstructed 3D model for “Cubes” sequence.

Fig. 9. Our reconstruction result of “Ofﬁce” dataset with 20, 862 frames in
total.

A. Qualitative Evaluation

We ﬁrst evaluate our system with some challenging datasets
captured by ourselves, which may contain complex loops with
fast motion and are very long.

Loop Closure and Low-frame-Rate Sequences. Figure 7
shows an indoor example “Cubes” where the scale is large and
there are complex loops. The number of frames is 14,817.
As can be seen, our method faithfully detect and close
the loops, achieving a drift-free 3D reconstruction result, as
shown in Figure 7. Our system also can handle low-frame-
rate sequences. We extract every third frame from “Cubes”
sequence to constitute a new sequence. Figure 8 (a) shows
the reconstruction result by our system, which is comparable
to the original one. The reconstructions of Kintinuous and
ElasticFusion are shown in Figures 8 (b) and (c), both of which
have serious drift. Please refer to our supplementary video for
more examples and comparison results.

Time Limitation. Our system can produce drift-free 3D
reconstruction without time limitation in a moderate scale
scene since the number of keyframes is not always increased
in this case. Figure 9 shows another indoor example where the
camera capture 20,862 frames in total. Our system can process
all data and produce drift-free reconstruction.

Relocalization. For some extremely challenging cases, the
tracking may be lost. In our system, if the tracking is poor or
even lost, the depth map will be not integrated. The camera can
be relocalized when the camera moves back to a previously
visited position. Please refer to our supplementary video to
watch the result and comparison to other systems.

B. Quantitative Evaluation of Trajectory Accuracy

We use the RGB-D benchmark of Sturm et al. [41] to
evaluate our system and make comparisons with other state-
of-the-art systems, i.e. DVO-SLAM [12], RGBD-SLAM [45],

Fig. 8. Comparison in a low-frame-rate example. (a) The reconstruction
of ours. (b) The reconstruction of Kintinuous. (c) The reconstruction of
ElasticFusion.

of every two or more frames, which does not degrade much
the reconstruction quality but can signiﬁcantly accelerate the
volumetric fusion.

VII. EXPERIMENTAL RESULTS

We have conducted experiments with both TUM RGB-D
benchmark [41] and indoor sequences captured by ourselves.
i5 3.3GHz CPU, 20GB
On a desktop PC with an Intel
memory and GTX 1070 graphics card (8GB video memory),
the tracking component without GPU acceleration takes about
5 ∼ 14ms per frame, and the dense mapping component in
the foreground thread takes about 1.2 ∼ 6ms per frame. The
whole system enabling both tracking and dense mapping runs
above 50fps. For a laptop with an Intel i7 2.6GHz CPU, 16GB
memory and GTX 960M graphics card (4GB video memory),
the system runs around 30fps. If we fuse only one out of three
frames, the running time could be even faster.

(a)(b)(c)TABLE III
COMPARISON OF ATE RMSE ON ALL OF THE SCENES IN THE TUM RGB-D BENCHMARK.

10

fr1_360
fr1_desk
fr1_desk2
fr1_ﬂoor
fr1_plant
fr1_room
fr1_rpy
fr1_teddy
fr1_xyz
fr2_360_hemisphere
fr2_360_kidnap
fr2_coke
fr2_desk
fr2_dishes
fr2_large_no_loop
fr2_large_with_loop
fr2_metallic_sphere
fr2_metallic_sphere2
fr2_pioneer360
fr2_pioneer_slam
fr2_pioneer_slam2
fr2_pioneer_slam3
fr2_rpy
fr2_xyz
fr2_ﬂowerbouquet
fr2_ﬂowerbouquet_brownbackground
fr2_desk_with_person
fr3_cabinet
fr3_large_cabinet
fr3_long_ofﬁce_household
fr3_nostructure_notexture_far
fr3_nostructure_notexture_near_with_loop
fr3_nostructure_texture_far
fr3_nostructure_texture_near_withloop
fr3_structure_notexture_far
fr3_structure_notexture_near
fr3_structure_texture_far
fr3_structure_texture_near
fr3_nostructure_notexture_near
fr3_teddy
fr3_sitting_xyz
fr3_walking_xyz
fr3_sitting_halfsphere
fr3_sitting_static
fr3_walking_static
fr3_walking_rpy
fr3_sitting_rpy
fr3_walking_halfsphere

Ours
(all frames)
13.0cm
2.5cm
2.8cm
325.3cm
5.0cm
14.8cm
2.2cm
18.7cm
1.0cm
37.6cm
132.6cm
17.2cm
7.2cm
8.4cm
-
198.3cm
34.1cm
11.1cm
40.5cm
91.2cm
169.7cm
28.1cm
0.8cm
1.2cm
7.0cm
53.3cm
4.7cm
39.9cm
20.9cm
3.2cm
-
-
10.8cm
2.9cm
-
-
1.8cm
1.6cm
-
-
2.1cm
2.8cm
1.7cm
1.3cm
5.2cm
42.0cm
2.7cm
25.6cm

Ours
(key frames)
10.9cm
2.1cm
2.4cm
26.2cm
3.8cm
13.4cm
3.7cm
15.7cm
0.7cm
31.1cm
6.1cm
20.2cm
7.1cm
7.9cm
-
196.7cm
44.3cm
8.4cm
35.8cm
85.5cm
3.3cm
19.1cm
0.6cm
1.2cm
5.0cm
51.7cm
4.5cm
7.9cm
14.8cm
2.8cm
-
-
5.3cm
2.7cm
-
-
1.6cm
1.8cm
-
-
1.7cm
2.4cm
1.9cm
0.9cm
3.9cm
33.7cm
4.1cm
18.2cm

Kintinuous ElasticFusion DVO-SLAM RGB-D SLAM MRSMap BundleFusion

1.6cm

8.3cm
2.1cm
4.6cm

2.8cm
5.3cm
2.0cm
3.4cm
1.1cm

2.3cm
4.3cm

9.1cm
8.4cm
2.6cm

1.4cm

1.7cm

5.7cm

4.3cm
4.9cm

2.6cm
6.9cm
2.7cm

1.3cm

5.2cm

8.6cm

1.8cm

0.8cm

2.0cm

1.1cm

3.5cm

3.2cm

4.2cm

2.2cm

1.8cm

1.7cm

201.8cm

1.2cm

3.7cm
7.1cm

4.7cm
7.5cm
2.8cm

1.7cm

3.4cm

2.9cm

3.0cm

3.1cm

10.8cm
2.0cm
4.8cm
-
2.2cm
6.8cm
2.5cm
8.3cm
1.1cm
-
-
-
7.1cm
-
-
-
-
-
-
-
-
-
1.5cm
1.1cm

-
9.9cm
1.7cm
-
-
7.4cm
1.6cm
3.0cm
2.1cm
1.3cm
1.5cm

4.9cm

MRSMap [46], Kintinuous [16], ElasticFusion [7], Bundle-
Fusion [10]. We test all scenes in the RGB-D benchmark
of Sturm et al. [41]. Table III shows the measured absolute
trajectory error (ATE). For other methods, we directly use
the reported ATE from their papers: “-” indicates tracking
failure, and the blank indicates not reported. Since our system
uses keyframes, we compute the RMSE of keyframes for our
method. For more fair comparison, we also compute ATE for
all frames. Speciﬁcally, we output the camera pose when a
frame is processed, which will not be further reﬁned by BA.
In contrast, the camera poses of keyframes are reﬁned by BA,
so their error is further minimized. As can be seen, our system
achieves quite comparable results with the state-of-the-art

methods. Compared to ElasticFusion [7] which tested on all of
the static scenes in the RGB-D benchmark of Sturm et al. [41],
our method can track successfully in more scenes, which
demonstrate the robustness of our keyframe-based tracking.

C. Quantitative Evaluation of Surface Accuracy

We perform surface reconstruction accuracy on the syn-
thetic ICL-NUIM dataset [47], which provides the synthetic
3D model with ground truth camera poses. We select three
living room scenes (including synthetic noise) to evaluate our

TABLE IV
COMPARISON OF ATE RMSE ON THE SYNTHETIC ICL-NUIM DATASET.

Seq.
Ours (all frames)
Ours (key frames)
Kintinuous
ElasticFusion
DVO-SLAM
RGB-D SLAM
MRSMap
BundleFusion

kt2
kt1
kt0
4.2cm
2.4cm
11.9cm
3.2cm
1.6cm
1.8cm
1.0cm
0.5cm
7.2cm
1.4cm
0.9cm
0.9cm
19.1cm
2.9cm
10.4cm
2.6cm
1.8cm
0.8cm
20.4cm 22.8cm 18.9cm
0.6cm
0.4cm
0.6cm

TABLE V
COMPARISON OF SURFACE ACCURACY ON THE SYNTHETIC ICL-NUIM
DATASET.

Seq.
Our keyframe-based fusion
Our every frame fusion
Kintinuous
ElasticFusion
DVO-SLAM
RGB-D SLAM
MRSMap
BundleFusion

kt0
kt1
0.9cm
1.1cm
0.8cm
1.3cm
1.1cm
0.8cm
0.7cm
0.7cm
3.2cm
6.1cm
3.2cm
4.4cm
6.1cm 14.0cm
0.6cm
0.5cm

kt2
1.6cm
1.8cm
0.9cm
0.8cm
11.9cm
3.1cm
9.8cm
0.7cm

approach. The measured ATE RMSE are listed in Table IV 1,
and the surfce reconstruction accuracy results are shown in Ta-
ble V. For these three scenes, some frames are quite textureless
which make tracking very challenging. Especially, our method
only perform low-resolution RGB-D alignment and may have
problem if the scene is extremely textureless and there are
not sufﬁcient features can be matched. Especially, in “kt0”
sequence, the camera poses of 30.5% frames are not recovered
due to this reason. Due to imperfect camera parameters in
this dataset, the surface accuracy of our method is slightly
worse than ElasticFusion and BundleFusion. Nevertheless, the
surface accuracy of reconstruction results by using keyframe-
based fusion and every frame fusion are quite comparable,
which demonstrate the effectiveness of our keyframe-based
fusion method.

VIII. DISCUSSIONS AND CONCLUSIONS

In this paper, we have presented a novel keyframe-based
dense SLAM approach which is not only robust to fast motion,
but also can recover from tracking failure, handle loop closure
and adjust the dense surface online to achieve drift-free 3D
reconstruction even on a laptop. In order to achieve this
goal, we ﬁrst contributed a keyframe-based tracking approach
which combines color and geometry information to make
the tracking as robust as possible. Secondly, we proposed
a novel incremental BA which makes maximal use of in-
termediate computation to save computation and adaptively
update necessary keyframes for map reﬁnement. Finally, we
proposed a keyframe-based dense mapping method which can
adjust the dense surface online by a few de-integration and
integration operations. The depths of non-keyframes are fused

1For “kt0” sequence, because 30.5% frames are lost for our method, we
only use the successfully recovered cameras poses to compute ATE RMSE.

11

into keyframes as much as possible to reduce redundancy. With
this representation, our system not only can adjust the dense
surface online but also can operate for extended periods of
time in a moderate size scene.

Our system still has some limitations. If the scene is fully
planar and extremely textureless, our system may fail. In
addition, our system still has difﬁculties in handling serious
motion blur. In future work, we would like to include inertial
measurements to further increase robustness.

ACKNOWLEDGMENT

The authors would like to thank Bangbang Yang, Weijian
Xie and Shangjin Zhai for their kind help in making experi-
mental results and comparisons.

REFERENCES

[1] G. Klein and D. W. Murray, “Parallel tracking and mapping for small
AR workspaces,” in 6th IEEE/ACM International Symposium on Mixed
and Augmented Reality, 2007, pp. 225–234.

[2] J. Engel, T. Schöps, and D. Cremers, “LSD-SLAM: Large-scale direct
monocular SLAM,” in 13th European Conference on Computer Vision,
Part II. Springer, 2014, pp. 834–849.

[3] R. Mur-Artal, J. Montiel, and J. D. Tardos, “ORB-SLAM: a versatile
and accurate monocular SLAM system,” IEEE Transactions on Robotics,
vol. 31, no. 5, pp. 1147–1163, 2015.

[4] R. A. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux, D. Kim, A. J.
Davison, P. Kohli, J. Shotton, S. Hodges, and A. W. Fitzgibbon,
“Kinectfusion: Real-time dense surface mapping and tracking,” in 10th
IEEE International Symposium on Mixed and Augmented Reality, 2011,
pp. 127–136.

[5] M. Nießner, M. Zollhöfer, S. Izadi, and M. Stamminger, “Real-time
3D reconstruction at scale using voxel hashing,” ACM Trans. Graph.,
vol. 32, no. 6, pp. 169:1–169:11, 2013.

[6] T. Whelan, S. Leutenegger, R. F. Salas-Moreno, B. Glocker, and
A. J. Davison, “Elasticfusion: Dense SLAM without A pose graph,” in
Robotics: Science and Systems XI, Sapienza University of Rome, Rome,
Italy, July 13-17, 2015, 2015.

[7] T. Whelan, R. F. Salas-Moreno, B. Glocker, A. J. Davison, and
S. Leutenegger, “Elasticfusion: Real-time dense SLAM and light source
estimation,” I. J. Robotics Res., vol. 35, no. 14, pp. 1697–1716, 2016.
[8] O. Kahler, V. A. Prisacariu, C. Y. Ren, X. Sun, P. H. S. Torr, and D. W.
Murray, “Very High Frame Rate Volumetric Integration of Depth Images
on Mobile Device,” IEEE Transactions on Visualization and Computer
Graphics (Proceedings of ISMAR 2015), vol. 22, no. 11, 2015.

[9] T. Whelan, M. Kaess, H. Johannsson, M. F. Fallon, J. J. Leonard,
and J. McDonald, “Real-time large-scale dense RGB-D SLAM with
volumetric fusion,” I. J. Robotics Res., vol. 34, no. 4-5, pp. 598–626,
2015.

[10] A. Dai, M. Nießner, M. Zollhöfer, S. Izadi, and C. Theobalt, “Bundle-
fusion: Real-time globally consistent 3D reconstruction using on-the-ﬂy
surface reintegration,” ACM Trans. Graph., vol. 36, no. 3, pp. 24:1–
24:18, 2017.

[11] R. Maier, R. Schaller, and D. Cremers, “Efﬁcient online surface correc-
tion for real-time large-scale 3D reconstruction,” in BMVC, 2017, pp.
1–12.

[12] C. Kerl, J. Sturm, and D. Cremers, “Dense visual SLAM for RGB-D
cameras,” in IEEE/RSJ International Conference on Intelligent Robots
and Systems, 2013, pp. 2100–2106.

[13] A. S. Huang, A. Bachrach, P. Henry, M. Krainin, D. Maturana, D. Fox,
and N. Roy, “Visual odometry and mapping for autonomous ﬂight using
an RGB-D camera,” in The 15th International Symposium on Robotics
Research, 2011, pp. 235–252.

[14] F. Endres, J. Hess, J. Sturm, D. Cremers, and W. Burgard, “3-D mapping
with an RGB-D camera,” IEEE Transactions on Robotics, vol. 30, no. 1,
pp. 177–187, 2014.

[15] P. J. Besl and N. D. McKay, “A method for registration of 3-D shapes,”
IEEE Trans. Pattern Anal. Mach. Intell., vol. 14, no. 2, pp. 239–256,
1992.

12

tional Conference on Robotics and Automation, ICRA 2014, Hong Kong,
China, May 31 - June 7, 2014, 2014, pp. 2609–2616.

[40] J. Civera, A. J. Davison, and J. M. Montiel, “Inverse depth parametriza-
tion for monocular SLAM,” IEEE Transactions on Robotics, vol. 24,
no. 5, pp. 932–945, 2008.

[41] J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cremers, “A
benchmark for the evaluation of RGB-D SLAM systems,” in IEEE/RSJ
International Conference on Intelligent Robot Systems, Oct. 2012, pp.
573–580.

[42] R. Hartley and A. Zisserman, Multiple view geometry in computer vision.

Cambridge university press, 2004.

[43] Y. Jeong, D. Nister, D. Steedly, R. Szeliski, and I.-S. Kweon, “Pushing
the envelope of modern methods for bundle adjustment,” IEEE transac-
tions on pattern analysis and machine intelligence, vol. 34, no. 8, pp.
1605–1617, 2012.

[44] M. Kaess, H. Johannsson, R. Roberts, V. Ila, J. J. Leonard, and
F. Dellaert, “iSAM2: Incremental smoothing and mapping using the
bayes tree,” International Journal of Robotics Research, vol. 31, no. 2,
pp. 216–235, 2012.

[45] F. Endres, J. Hess, N. Engelhard, J. Sturm, D. Cremers, and W. Burgard,
“An evaluation of the RGB-D SLAM system,” in IEEE International
Conference on Robotics and Automation, ICRA 2012, 14-18 May, 2012,
St. Paul, Minnesota, USA, 2012, pp. 1691–1696.

[46] J. Stückler and S. Behnke, “Multi-resolution surfel maps for efﬁcient
dense 3D modeling and tracking,” J. Visual Communication and Image
Representation, vol. 25, no. 1, pp. 137–147, 2014.

[47] A. Handa, T. Whelan, J. McDonald, and A. J. Davison, “A benchmark
for RGB-D visual odometry, 3D reconstruction and SLAM,” in 2014
IEEE International Conference on Robotics and Automation, ICRA
2014, Hong Kong, China, May 31 - June 7, 2014, 2014, pp. 1524–1531.

[16] T. Whelan, J. McDonald, M. Kaess, M. Fallon, H. Johannsson, and
J. Leonard, “Kintinuous: Spatially extended KinectFusion,” in RSS Work-
shop on RGB-D: Advanced Reasoning with Depth Cameras, Sydney,
Australia, Jul 2012.

[17] M. Zeng, F. Zhao, J. Zheng, and X. Liu, “Octree-based fusion for
realtime 3D reconstruction,” Graphical Models, vol. 75, no. 3, pp. 126–
136, 2013.

[18] H. Liu, G. Zhang, and H. Bao, “Robust keyframe-based monocular
SLAM for augmented reality,” in IEEE International Symposium on
Mixed and Augmented Reality, 2016.

[19] R. A. Newcombe, S. Lovegrove, and A. J. Davison, “DTAM: dense
tracking and mapping in real-time,” in IEEE International Conference
on Computer Vision, 2011, pp. 2320–2327.

[20] V. Pradeep, C. Rhemann, S. Izadi, C. Zach, M. Bleyer, and S. Bathiche,
“Monofusion: Real-time 3D reconstruction of small scenes with a
single web camera,” in IEEE International Symposium on Mixed and
Augmented Reality, 2013, pp. 83–88.

[21] T. Schöps, T. Sattler, C. Häne, and M. Pollefeys, “3D modeling on
the go: Interactive 3D reconstruction of large-scale scenes on mobile
devices,” in 2015 International Conference on 3D Vision, 2015, pp. 291–
299.

[22] P. Ondruska, P. Kohli, and S. Izadi, “Mobilefusion: Real-time volumetric
surface reconstruction and dense tracking on mobile phones,” IEEE
Trans. Vis. Comput. Graph., vol. 21, no. 11, pp. 1251–1258, 2015.
[23] Q. Zhou, S. Miller, and V. Koltun, “Elastic fragments for dense scene
reconstruction,” in IEEE International Conference on Computer Vision,
ICCV 2013, Sydney, Australia, December 1-8, 2013, 2013, pp. 473–480.
[24] K. Wang, G. Zhang, and H. Bao, “Robust 3D reconstruction with an
RGB-D camera,” IEEE Trans. Image Processing, vol. 23, no. 11, pp.
4893–4906, 2014.

[25] S. Choi, Q. Zhou, and V. Koltun, “Robust reconstruction of indoor
scenes,” in IEEE Conference on Computer Vision and Pattern Recogni-
tion, 2015, pp. 5556–5565.

[26] B. Triggs, P. F. McLauchlan, R. I. Hartley, and A. W. Fitzgibbon,
“Bundle adjustment - a modern synthesis.” in Workshop on Vision
Algorithms, 1999, pp. 298–372.

[27] E. Olson, J. J. Leonard, and S. J. Teller, “Fast iterative alignment of pose
graphs with poor initial estimates,” in Proceedings of IEEE International
Conference on Robotics and Automation, 2006, pp. 2262–2269.
[28] H. Strasdat, J. M. M. Montiel, and A. J. Davison, “Scale drift-aware
large scale monocular SLAM,” in Robotics: Science and Systems VI,
2010.

[29] R. Kümmerle, G. Grisetti, H. Strasdat, K. Konolige, and W. Burgard,
“g2o: A general framework for graph optimization,” in IEEE Interna-
tional Conference on Robotics and Automation, 2011, pp. 3607–3613.
[30] M. Kaess, A. Ranganathan, and F. Dellaert, “iSAM: Incremental smooth-
ing and mapping,” IEEE Transactions on Robotics, vol. 24, no. 6, pp.
1365–1378, 2008.

[31] M. Kaess, H. Johannsson, R. Roberts, V. Ila, J. J. Leonard, and
F. Dellaert, “iSAM2: Incremental smoothing and mapping using the
bayes tree,” The International Journal of Robotics Research, vol. 31,
no. 2, pp. 216–235, 2012.

[32] V. Ila, L. Polok, M. Solony, and P. Svoboda, “SLAM++ -a highly
efﬁcient and temporally scalable incremental slam framework,” The
International Journal of Robotics Research, vol. 36, no. 2, pp. 210–
230, 2017.

[33] V. Ila, L. Polok, M. Solony, and K. Istenic, “Fast incremental bundle
adjustment with covariance recovery,” in International Conference on
3D Vision, 2017, pp. 4321–4330.

[34] E. Rublee, V. Rabaud, K. Konolige, and G. Bradski, “ORB: an efﬁcient
alternative to SIFT or SURF,” in IEEE International Conference on
Computer Vision.

IEEE, 2011, pp. 2564–2571.
[35] D. Gálvez-López and J. D. Tardós, “Bags of binary words for fast
place recognition in image sequences,” IEEE Transactions on Robotics,
vol. 28, no. 5, pp. 1188–1197, 2012.

[36] G. Klein and D. W. Murray, “Improving the agility of keyframe-based
SLAM,” in 10th European Conference on Computer Vision, Part II.
Springer, 2008, pp. 802–815.

[37] G. Vogiatzis and C. Hernández, “Video-based, real-time multi-view
stereo,” Image Vision Comput., vol. 29, no. 7, pp. 434–441, 2011.
[38] C. Forster, M. Pizzoli, and D. Scaramuzza, “SVO: Fast semi-direct
monocular visual odometry,” in IEEE International Conference on
Robotics and Automation.

IEEE, 2014, pp. 15–22.

[39] M. Pizzoli, C. Forster, and D. Scaramuzza, “REMODE: probabilistic,
monocular dense reconstruction in real time,” in 2014 IEEE Interna-

