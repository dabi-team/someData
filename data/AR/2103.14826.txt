Compact 3D Map-Based Monocular Localization Using Semantic
Edge Alignment

Kejie Qiu, Shenzhou Chen, Jiahui Zhang, Rui Huang, Le Cui, Siyu Zhu, and Ping Tan

1
2
0
2

r
a

M
7
2

]

O
R
.
s
c
[

1
v
6
2
8
4
1
.
3
0
1
2
:
v
i
X
r
a

Abstract— Accurate localization is fundamental to a variety
of applications, such as navigation, robotics, autonomous driv-
ing, and Augmented Reality (AR). Different from incremental
localization, global localization has no drift caused by error
accumulation, which is desired in many application scenarios.
In addition to GPS used in the open air, 3D maps are also
widely used as alternative global localization references. In this
paper, we propose a compact 3D map-based global localization
system using a low-cost monocular camera and an IMU (Inertial
Measurement Unit). The proposed compact map consists of two
types of simpliﬁed elements with multiple semantic labels, which
is well adaptive to various man-made environments like urban
environments. Also, semantic edge features are used for the
key image-map registration, which is robust against occlusion
and long-term appearance changes in the environments. To
further improve the localization performance, the key semantic
edge alignment is formulated as an optimization problem based
on initial poses predicted by an independent VIO (Visual-
Inertial Odometry) module. The localization system is realized
with modular design in real time. We evaluate the localization
accuracy through real-world experimental results compared
with ground truth, long-term localization performance is also
demonstrated.

I. INTRODUCTION
With the fast development of large-scale 3D reconstruction
and high-level autonomous driving, various 3D map produc-
tions have become widely available nowadays. Given a 3D
map as the global localization reference, 6-DoF sensor pose
(position and orientation) can be derived using speciﬁc pose
estimation algorithms. Map-based localization is essentially
a registration problem to match the sensor observations with
a prior map. Usually high-quality sensors such as GPS-RTK
(real-time kinematic), Lidar, and SINS (Strapdown Inertial
Navigation System) are used for High Deﬁnition (HD) map
construction, and low-cost sensors are used for localization.
For example, a point cloud map constructed by a Lidar can
not only be used for Lidar-based localization, but also a
monocular camera-based localization approach [1]. A more
general vision-based localization method is to utilize a map
consists of a mass of visual features and their descriptors
[2]. However, this map is difﬁcult to maintain and the map
size is too large for large-scale environments.

Besides point cloud maps and visual feature maps, there
are many other map formats such as textured mesh model
constructed by visual reconstruction, and even vector maps.
Recently, the growing popularity of vector map standards
such as OpenDrive 1 and NDS 2 indicates that a compressed

All

the authors are with Alibaba A.I. Labs, Hangzhou, China.

kejie.qkj@alibaba-inc.com.

1http://opendrive.org
2http://nds-association.org/

Fig. 1: Map compaction using the proposed compact map format in
an urban environment. The map size is signiﬁcantly reduced while
the key landmarks are reserved using two types of line segments
(a line segment & a wireframe). Each type can be labeled with
multiple semantic categories.

map is a tendency for a light-weight and scalable map repre-
sentation. To make full use of available 3D maps, we propose
to use a more compact map that only consists of semantic
line segments and wireframes. Because plenty of structural
elements exist in man-made environments and many of them
are comprised of straight line segments, such as lane lines,
lamp poles, trafﬁc signs in outdoor environments (Fig. 1), as
well as windows, ceilings, and walls in indoor environments.
Thanks to the compact representation, the proposed compact
map can be generated from various HD maps of man-made
environments. Also, the map size can be further reduced for
on-board and large-scale deployment.

Visual localization, as a low-cost and light-weight solu-
tion, has attracted signiﬁcant study these years, especially
the monocular solutions. However, a monocular camera
lacks direct range measurement, and the traditional point
features are easily affected by perspective and illumination
changes. To overcome this problem, edge features are used
in a model-based localization system [3]. Because of the
rapid development of learning-based perception, real-time
object detection [4] and semantic segmentation [5], [6], [7]
can help improve the robustness of visual feature-based
localization. In fact, semantic features have been used in
multiple incremental and global localization systems [8], [9].
Compared with the widely used semantic point cloud or
semantic objects, the proposed semantic edges achieve to
balance both compactness and accuracy.

 
 
 
 
 
 
In this paper, we propose a visual localization solution
based on a compact map and a low-cost sensor set consists
of a monocular camera and an IMU, which can be mounted
on vehicles, robots, and hand-held devices to realize real-time
global localization. With semantic perception, the extracted
edge features are marked with speciﬁc semantic labels for
robust feature association and accurate pose estimation.

We identify our contributions as follows:
• We propose a light-weight map format consisting of
semantic line segments and wireframes, which can be
generated from various 3D maps and take very little
storage space.

• We adopt semantic edges to achieve robust feature
association and potentially higher accuracy compared
with other semantic elements.

• We implement the localization system in real-time to
validate the localization performance using real-world
experiments.

II. RELATED WORK

Map-based visual localization is a state estimation problem
about how to register visual observations to a pre-constructed
3D map, a great number of works about this topic have been
studied in the past few years. According to whether or not
a prior camera pose is needed, the localization algorithms
can be divided into independent localization and prior-based
localization.

Independent localization is also known as relocalization
or loop closure detection of simultaneous localization and
mapping (SLAM). An inquiry image is the only input and
the absolute camera pose is estimated. For example, it can
be realized by establishing 2D-3D correspondences between
the image pixels and the 3D points of the map, followed by
a Perspective-n-Point (PnP) solver [10], [11], [2]. However,
these methods require feature descriptors to be stored in the
map, resulting in large map sizes. In practice, an image
retrieval algorithm or a rough position range can also be
utilized to speed up the feature match process. Recently,
end-to-end regression networks are used to solve visual
localization in small-size environments [12], [13], but they
cannot generalize to new scenarios.

Prior-based localization, on the other hand, depends on
an initial camera pose for accurate camera pose estimation.
Landmarks are selected from the map according to the initial
pose for feature alignment, it is then formulated as an op-
timization problem by iteratively minimizing the alignment
cost [14], [15], [16]. Thus, in addition to an inquiry image,
a prior camera pose is needed to make the optimization
successfully converged. This method does not need image
descriptors for image-level match because of the use of
the prior camera pose, and has various feature alignment
approaches in addition to descriptor-based alignment. For
instance, Zuo et.al propose to align the semi-dense point
cloud provided by a stereo camera with a prior Lidar map
[17], and Caselitz et.al utilize the sparse points generated
by monocular visual bundle adjustment instead of a stereo
camera [18]. The map size can be signiﬁcantly reduced

compared with the map used by independent localization.
Recently, several researchers propose to use a semantic map
[9] or a vector map [15] as the key localization reference,
and the map size can be further reduced.

The map used for localization and the visual features
used for data association are highly related. Most traditional
independent localization methods opt to utilize point features
such as SIFT [19] and ORB [20], so the corresponding maps
are represented with 3D feature points and their descriptors
[2]. The prior-based localization methods prefer to use other
alignment approaches instead of descriptor-based feature
alignment. For instance, only a point cloud map is needed
by using ICP (Iterative Closest Point) alignment [17], [18].
Besides point features, line features or edge features are
also widely used. They are regarded to be more robust
against perspective and illumination changes. [3] uses a
textured mesh model as the reference map, and edge features
could be extracted from the rendered images. [14] organizes
the map as a set of road markings consists of solid and
dashed lines for urban environment localization, in which
the line features are directly used for feature association.
Recently, semantic features have played an important role
in robust data association,
they are proved to be more
stable and robust features because of the speciﬁc semantic
information attached to the features. For urban environments,
semantic elements like lane lines, lamp poles, trafﬁc signs are
represented in a self-deﬁned format [16] or standard vector
map format [15] for outdoor localization, while [9] builds a
semantic point cloud map to represent guide signs, parking
lines, speed bumps in parking lots for indoor localization.
However, most map formats mentioned above are dedicatedly
designed for a certain environment.

The proposed method belongs to the prior-based localiza-
tion, and semantic edge features are used for observation-
map registration. We propose to use a line segments-based
the semantic edge features. Only the
map to represent
semantic edges can be represented by line segments and
wireframes are used for map construction and localization
landmarks, and this concise representation way is applicable
to all man-made environments.

III. SYSTEM OVERVIEW

The proposed modular localization system includes a
semantic segmentation module, a VIO module, a landmark
selection module, a feature extraction module, and a semantic
edge alignment module. We will focus on the last three
modules in this paper. For system completeness, the compact
map generation process is also brieﬂy introduced in Section
IV-A. Take the urban environment localization, for example,
the localization pipeline is shown in Fig. 2. The localization
system is initialized by a global reference such as GPS
or other visual relocalization methods. Given a captured
image, the semantic segmentation module ﬁrst labels the
image pixels semantically, without loss of generality, we
segment the image into two kinds of semantic areas (road
and non-road). According to the segmentation results, the
potential dynamic image areas are masked out before further

Fig. 2: The overall pipeline of the proposed map-based localization system. The whole localization system is initialized by a global
reference shown in the dashed box, all the solid boxes denote modules running in real time. In the feature extraction module, semantic
edge features are extracted from the input image considering the segmentation results, semantic energy maps (black: low energy; white:
high energy) represented with distance transform are generated. In the landmark selection module, the landmarks for feature alignment
are selected according to the prior camera pose provided by an independent VIO module. In the edge alignment module, the reprojections
of the landmarks (color points) are illustrated before and after optimization.

processing, and the semantic edge features are extracted from
the captured image using edge detection algorithms and the
semantic segmentation results, the separate semantic edge
images are turned into corresponding distance transforms for
dense edge alignment. Meanwhile, the current camera pose is
predicted according to the last camera pose and the odometry
input from the VIO module. With the predicted camera
pose, the corresponding landmarks are selected from the
pre-constructed compact map for feature alignment. Finally,
the global camera pose is derived within an optimization
framework.

For the notations used in this paper, (·)w is the world
frame, (·)r the prior camera frame for landmark selection,
(·)c the camera frame. tx
y and Rx
y are the 3D translation
and rotation of frame (·)y with respect to frame (·)x. The
skew-symmetric matrix operator is denoted as (cid:98)·(cid:99)×. The
focal lengths of the undistorted images are denoted as fx
and fy, respectively. The corresponding image gradients
of the distance transform are represented by Gu and Gv.
X(p), Y(p), Z(p) are the x, y, z values of a 3D point
p. Furthermore, we deﬁne the camera projection function
π : R3 (cid:55)−→ R2 projects the 3D points onto 2D images.

IV. METHODOLOGY

A. Map deﬁnition & generation

We deﬁne two types of landmarks, namely, a line segment
and a wireframe, each type can be attached with multiple
semantic labels. A semantic line segment is represented by a

semantic label and two 3D points, and a semantic wireframe
is represented by a semantic label and more than two 3D
points (four points for a rectangle wireframe):

Map := {Lm|m=0,1,···, Wn|n=0,1,···},

(1)

in which

(2)

Lm = {s, p0, p1},
Wn = {s, p0, p1, p2, p3},
where s denotes a semantic label and p ∈ R3 denotes a
3D point in the global coordinate. The used semantic labels
depend on the speciﬁc application scenarios, for example,
they will be road marks (lane lines, crosswalks, and other
marks) on a road, lamp poles on roadsides, trafﬁc signs
above a road, and skylines generated by building edges in
urban environments. Also, the map is compatible with indoor
environments with different semantic labels.

As for map generation, the compact landmarks can be
converted from standard map formats or generated from the
results of multiple mapping algorithms using various sensors.
For instance, based on an HD map constructed by visual
dense mapping, the road marks can be efﬁciently detected
in the inverse perspective mapping (IPM) images using a
segmentation algorithm [21], as shown in Fig. 3(a). And
the non-road elements can be ﬁrstly detected in the images
using a dedicated neural network and secondly projected
to the global coordinate with the help of corresponding
depth information rendered from the mapping result, as
shown in Fig. 3(b). In practice, both detection results can

(a) road mark detection

(b) non-road mark detection

Fig. 3: The road landmarks are detected in the IPM (bird’s eye view)
image while the non-road landmarks are detected in the normal
image view.

Fig. 4: Map generation of a sample urban environment.

be reﬁned through manual annotation for better accuracy,
and the landmarks shielded by vegetation are not kept. One
sample compact map generation result is shown in Fig. 4.

B. Landmark selection

r tw

The landmarks used for prior-based localization are pre-
selected from the map according to the prior camera pose
Rw
r . In fact, the predicted camera pose is computed by
fusing the lastest reliable localization results tw
cl , and
the relative camera pose provided by the VIO module tcl
r
Rcl
r :

cl, Rw

tw
r = tw
cl
r = Rw
Rw
cl

+ Rw
cl
Rcl
r ,

tcl
r ,

(3)

where cl is the latest frame with reliable localization result.
In fact, only the landmarks within the prior camera ﬁeld-
of-view (FOV) are selected for feature alignment. Since
the landmarks may be occluded by each other, we have to
simulate the correct occlusion by considering the relative

position of the landmarks. For example, a trafﬁc sign may
cover the lamp poles or building edges behind it, and
although the trafﬁc sign is denoted by a wireframe consists
of four separate line segments, the closed rectangle area
is considered for depth buffering check. For the occlusion
generated by the objects that appeared in the real sensing
stage, we design a robust feature alignment method based
on the semantic segmentation results, which will be detailed
in the following content.

In fact, the landmark selection process is quite similar
to the image rendering process in graphics: given a camera
pose for image rendering, a virtual RGB image and its depth
map are generated with correct occlusion simulation by using
depth buffering, as shown in the landmark selection module
of Fig. 2. Thus, we also make use of depth buffering to
generate landmarks, because some of them may partially or
completely be occluded by the landmarks in front of them. In
particular, for a lamp pole, we ﬁrst expand the line segment
into a cylinder with an averaging pole radius for further
landmark rendering. Finally, the visible landmark parts are
sampled with sparse points on the corresponding edges in
the rendered images for the following edge alignment.

C. Feature extraction & semantic edge alignment

The key feature alignment algorithm is based on matching
semantic edge features with the corresponding semantic
landmarks selected before. Given an image for localization,
the edge features are detected by multiple edge detection
algorithms, which are further segmented with the semantic
segmentation results. The edges with the same semantic
label are collected to formulate an edge image, then the
corresponding semantic landmark points are reprojected onto
the edge image, the residual term of the ith edge point of
semantic label s (psi in the camera frame) is deﬁned as the
distance between the reprojected pixel and the nearest edge
pixel:

T (Rw

c , tw

r − tw

rsi(Rw

r psi + tw

c ) = minjD(π[Rw
c

c )], usj ),
(4)
where D : (R2, R2) (cid:55)−→ R denotes the Euclidean distance
between those points. If the 3D edge points are preselected
then the minimization object function is exactly the deﬁnition
of the distance transform [22]. It
is generated from the
corresponding edge image, as shown in the feature extraction
module of Fig. 2. We denote the distance transform of the
edge image as V : R2 (cid:55)−→ R. The residual term of one
speciﬁc edge point becomes

rsi(Rw

c , tw

c ) = V(π[Rw
c

T (Rw

r psi + tw

r − tw

c )]).

(5)

The overall energy term is formulated by accumulating all

the edge point residuals with all the semantic labels:
(cid:88)

(cid:88)

e(Rw

c , tw

c ) =

(rsi(Rw

c , tw

c ))2.

(6)

s
Finally, the optimal camera pose estimation Rw

c and tw
c
can be derived by minimizing the overall energy term. To
be speciﬁc, we use the minimal representation, namely ξ ∈

i

Fig. 5: The sensor set used for experiments. Only one camera and
the inside IMU are used for the map-based monocular localization.

TABLE I: Timing statistics.

module

time

thread

semantic segmentation
VIO
landmark selection
feature extraction & alignment

49 ms
34 ms
8 ms
37 ms

1
2
3
4

se(3) instead of Rw
c for optimization implementation,
and we apply the Gaussian Newton method to solve this
optimization problem:

c and tw

JT Jδξ = −JT r(0),

(7)

where J is the stacked matrix of all Jsi pixel-wise Jacobians
with a speciﬁc semantic label.

The key Jacobian with regard to the error state δξ is

computed as follows:

Jsi =

∂rsi
∂psi

∂psi
∂δξ

,

(8)

where

∂rsi
∂psi

=





Gufx/Z(psi)
Gvfy/Z(psi)
−GufxX(psi )/Z(psi)2 − GvfyY(psi)/Z(psi)2



 ,

∂psi
∂δξ

= [ −Rw
c

T | (cid:98)Rw
c

T (Rw

r psi + tw

r − tw

c )(cid:99)×].

The camera pose ξ is updated by ξ = log(exp(ξ)exp(δξ)),
c and tw
then the camera pose Rw
c can be recovered from it
if the optimization converges.

To further evaluate the localization results, we check the
consistency of the estimated camera pose and the predicted
camera pose, then drop the ones with large pose incon-
sistency. Also, we check the ﬁnal averaging reprojection
error, namely, the energy function value at the ﬁnal iteration
over the number of edge pixels, the estimations with large
reprojection errors are also dropped. With the VIO module
providing relative camera pose, the latest camera pose can be
correctly predicted by skipping the dropped frames (Equation
(3)). The proposed localization method is drift-free almost
all the time, as long as the detected edge features and the
selected landmarks are enough for pose estimation.

V. EXPERIMENTAL RESULTS

A. Implementation details

The compact maps used for experiments are generated
from a dense point cloud map constructed by a high-end
device including ﬁve industry cameras, one high-precision
SINS and one wheel odometry mounted on a data acquisition

Fig. 6: Reprojections of selected 3D landmarks onto the captured
images after optimization (the road and non-road distance trans-
forms are combined, all the non-road landmarks are shown in
green). First row: the reprojected landmarks match well with the
detected edges, which means the optimization converges. Second
row: the reprojected landmarks poorly match the detected edges,
which means the optimization fails.

car. On the other hand, the sensor set used for localization
includes the left monocular camera of an MYNT EYE
camera which captures 640 by 400 images at 20 Hz and the
internal IMU runs at 200Hz, as shown in Fig. 5. The intrinsic
parameters of the camera and the extrinsic parameters be-
tween the camera and the IMU are calibrated in advance. The
localization system is initialized with a visual relocalization
method based on SuperPoint [10]. VINS-Mono [23] is used
for monocular VIO implementation. DeepLabv3+ [5] with
xception [24] model is used for semantic segmentation in
urban environments, resulting road and non-road areas for
further feature extraction. The whole localization system runs
real-time on a desktop computer with an i7-8700K CPU and
a GeForce GTX 1080 Ti graphics card, the detailed timing
statistics are shown in Table I.

B. Localization evaluation

To vividly present the edge alignment results, we reproject
the landmarks onto the composed distance transform that
combines all the semantic layers for visualization, as shown
in the semantic edge alignment module of Fig. 2. Normally,
the reprojection landmarks (color points) are located in the
low energy areas (black areas of the distance transform) after
optimization. In other words, we can intuitively judge the
localization results through the reprojection images.

TABLE II: Map statistics.

trial 1

trial 2

trial 3

landmarks number

line segments

wireframes

lane line
lamp pole
building edge

rectangle mark
trafﬁc sign

419

315
5
13

81
5

144

85
3
7

46
3

74

40
5
0

26
3

road length
original map size
compact map size
compression factor

303 m
220MB
25.2KB
8.9K

171 m
184MB
9.2KB
20.5K

146 m
69.4MB
4.9KB
14.5K

The edge-based feature alignment is robust against illu-
mination or appearance changes, and the usage of separated
semantic feature layers further improves the convergence and
accuracy. But there still exist several failure cases when the
selected landmarks from the map are too few, or when severe
occlusion occurs. For example, two reprojection images after
optimization are shown in Fig. 6. If the landmarks are par-
tially occluded, the rest landmarks still lead to correct pose
estimation. However, if the landmarks are mostly occluded
by other vehicles,
to converge
because the camera pose is not fully observable.

the localization may fail

To evaluate the localization accuracy of the proposed
method, we collect the localization data together with the
high-end device which are all rigidly mounted on a data
acquisition vehicle (the extrinsic parameters are calibrated
in advance). The corresponding localization results are used
as the ground truth for accuracy evaluation. There is nothing
particular about the roads we select for experiments, pedes-
trians, riders, and many other vehicles are involved in the
data acquisition process. Due to the semantic segmentation
module, all potential dynamic objects are masked out from
the camera view,
the edge features are also marked for
robust feature association. In the meantime, the VIO module
keeps providing accurate relative pose prediction. As a result,
the map-based localization survives from several occlusion
situations.

Three trials of experimental data with a total length of
620m are collected together with the ground truth. The
statistics of the compact maps used for localization are shown
in Table II, the map of trial 1 has a relatively larger landmark
density. Importantly, the map sizes are signiﬁcantly reduced
(compression factor = original map size/compact map size)
with the compact map representation, which is beneﬁcial to
onboard systems and large-scale deployment.

The coordinate of the localization results has been uniﬁed
to the world coordinate of the ground truth since the compact
map is generated from the dense mapping result of the high-
end device. We use Z-Y-X Euler angles for rotation repre-
sentation. By considering the pre-calibrated extrinsic param-
eters, the pose estimation errors compared with the ground
truth are shown in Fig. 7. The detailed numerical results
evaluated with RMSE are shown in Table III. Due to the use
of semantic edges instead of semantic objects, the proposed
method achieves the position accuracy within 0.29m and the
rotation accuracy within 0.52◦, which satisfy the lane-level
accuracy requirement of autonomous driving. Moreover, as
shown in Table IV, we compare the proposed algorithm with
other map-based algorithms using their reported localization
accuracy, which shows the proposed method achieves the

TABLE III: RMSE of pose estimations.

trial

1
2
3

x
(m)

0.15
0.23
0.10

y
(m)

0.21
0.12
0.22

z
(m)

0.10
0.12
0.09

norm yaw
(◦)
(m)

0.29
0.29
0.26

0.19
0.20
0.12

pitch
(◦)

0.31
0.41
0.17

roll
(◦)

0.15
0.25
0.20

angle
(◦)

0.46
0.52
0.29

Fig. 7: Boxplot of the pose estimation error.

best performance level among the relevant works. More
localization details are demonstrated in the attached video.

C. Long-term localization

In order to evaluate the long-term localization performance
using the same compact map, we also collect the sensor data
for localization four months later on the same road. The high-
end device is not used this time, but the localization results
can still be judged by checking the reprojection landmarks
in the captured images with the estimated camera poses. For
example, the day for mapping is a cloudy day and the road
surface is dry, while the day for localization evaluation is
a sunny day and the road surface is wet. Because of the
robustness of the semantic edge features we use, the captured
images can still be localized by only using the compact map
constructed four months ago, more details are demonstrated
in the attached video.

VI. CONCLUSION AND FUTURE WORK

In this paper, we propose a global localization system
using monocular vision and inertial measurement based on
a self-deﬁned 3D compact map. The map consists of two
types of landmarks: a line segment and a wireframe, each
type with multiple semantic labels. An urban environment
is used to illustrate methodology and evaluation. Moreover,
thanks to the generalization of the deﬁned map format, it
can easily adapt to indoor environments or any man-made
environments using speciﬁc semantic labels. In addition, the
semantic edge features used for feature alignment are robust
against perspective and illumination changes, resulting in
long-term localization under the condition of complicated
appearance changes. In the future, we will generate the

TABLE IV: Comparison of other methods.

methods

sensors

maps

position
accuracy

rotation
accuracy

[18]
[25]
[16]
[15]
ours

camera
stereo camera
camera
camera
camera-IMU

point cloud
point cloud
line+point
vector map
line segment

0.30m
0.60m
0.35m
0.29m
0.29m

-
-
1.09◦
-
0.52◦

compact map from other map formats and implement the
localization system in other man-made environments.

REFERENCES

[1] R. W. Wolcott and R. M. Eustice, “Visual localization within lidar
maps for automated urban driving,” in 2014 IEEE/RSJ International
Conference on Intelligent Robots and Systems.
IEEE, 2014, pp. 176–
183.

[2] M. B¨urki, I. Gilitschenski, E. Stumm, R. Siegwart, and J. Nieto,
“Appearance-based landmark selection for efﬁcient long-term visual
localization,” in 2016 IEEE/RSJ International Conference on Intelli-
gent Robots and Systems (IROS).

IEEE, 2016, pp. 4137–4143.

[3] K. Qiu, T. Liu, and S. Shen, “Model-based global localization for
aerial robots using edge alignment,” IEEE Robotics and Automation
Letters, vol. 2, no. 3, pp. 1256–1263, 2017.

[4] J. Redmon and A. Farhadi, “Yolov3: An incremental improvement,”

arXiv preprint arXiv:1804.02767, 2018.

[5] L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam,
“Encoder-decoder with atrous separable convolution for semantic
image segmentation,” in Proceedings of the European conference on
computer vision (ECCV), 2018, pp. 801–818.

[6] H. Zhao, X. Qi, X. Shen, J. Shi, and J. Jia, “Icnet for real-time
semantic segmentation on high-resolution images,” in Proceedings of
the European Conference on Computer Vision (ECCV), 2018, pp. 405–
420.

[7] C. Yu, C. Gao, J. Wang, G. Yu, C. Shen, and N. Sang, “Bisenet
v2: Bilateral network with guided aggregation for real-time semantic
segmentation,” arXiv preprint arXiv:2004.02147, 2020.

[8] S. L. Bowman, N. Atanasov, K. Daniilidis, and G. J. Pappas, “Proba-
bilistic data association for semantic slam,” in 2017 IEEE international
conference on robotics and automation (ICRA).
IEEE, 2017, pp.
1722–1729.

[9] T. Qin, T. Chen, Y. Chen, and Q. Su, “Avp-slam: Semantic visual
mapping and localization for autonomous vehicles in the parking lot,”
2020.

[10] D. DeTone, T. Malisiewicz, and A. Rabinovich, “Superpoint: Self-
supervised interest point detection and description,” in Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition
Workshops, 2018, pp. 224–236.

[11] B. Zeisl, T. Sattler, and M. Pollefeys, “Camera pose voting for
large-scale image-based localization,” in Proceedings of the IEEE
International Conference on Computer Vision, 2015, pp. 2704–2712.
[12] E. Brachmann, A. Krull, S. Nowozin, J. Shotton, F. Michel,
S. Gumhold, and C. Rother, “Dsac-differentiable ransac for camera

localization,” in Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 2017, pp. 6684–6692.

[13] E. Brachmann and C. Rother, “Learning less is more-6d camera
localization via 3d surface regression,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, 2018, pp.
4654–4662.

[14] Y. Lu, J. Huang, Y.-T. Chen, and B. Heisele, “Monocular localization
in urban environments using road markings,” in 2017 IEEE Intelligent
Vehicles Symposium (IV).

IEEE, 2017, pp. 468–474.

[15] Z. Xiao, D. Yang, T. Wen, K. Jiang, and R. Yan, “Monocular
localization with vector hd map (mlvhm): A low-cost method for
commercial ivs,” Sensors, vol. 20, no. 7, p. 1870, 2020.

[16] Z. Xiao, K. Jiang, S. Xie, T. Wen, C. Yu, and D. Yang, “Monocular
vehicle self-localization method based on compact semantic map,”
in 2018 21st International Conference on Intelligent Transportation
Systems (ITSC).

IEEE, 2018, pp. 3083–3090.
[17] X. Zuo, P. Geneva, Y. Yang, W. Ye, Y. Liu, and G. Huang, “Visual-
inertial localization with prior lidar map constraints,” IEEE Robotics
and Automation Letters, vol. 4, no. 4, pp. 3394–3401, 2019.

[18] T. Caselitz, B. Steder, M. Ruhnke, and W. Burgard, “Monocular
camera localization in 3d lidar maps,” in 2016 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS).
IEEE, 2016,
pp. 1926–1931.

[19] D. G. Lowe, “Distinctive image features from scale-invariant key-
points,” International journal of computer vision, vol. 60, no. 2, pp.
91–110, 2004.

[20] R. Mur-Artal and J. D. Tard´os, “Orb-slam2: An open-source slam
system for monocular, stereo, and rgb-d cameras,” IEEE Transactions
on Robotics, vol. 33, no. 5, pp. 1255–1262, 2017.

[21] K. He, G. Gkioxari, P. Doll´ar, and R. Girshick, “Mask r-cnn,” in
Proceedings of the IEEE international conference on computer vision,
2017, pp. 2961–2969.

[22] P. F. Felzenszwalb and D. P. Huttenlocher, “Distance transforms of
sampled functions,” Theory of computing, vol. 8, no. 1, pp. 415–428,
2012.

[23] T. Qin, P. Li, and S. Shen, “Vins-mono: A robust and versatile monoc-
ular visual-inertial state estimator,” IEEE Transactions on Robotics,
no. 99, pp. 1–17, 2018.

[24] F. Chollet, “Xception: Deep learning with depthwise separable convo-
lutions,” in Proceedings of the IEEE conference on computer vision
and pattern recognition, 2017, pp. 1251–1258.

[25] E. Stenborg, C. Toft, and L. Hammarstrand, “Long-term visual
localization using semantically segmented images,” in 2018 IEEE
International Conference on Robotics and Automation (ICRA).
IEEE,
2018, pp. 6484–6490.

