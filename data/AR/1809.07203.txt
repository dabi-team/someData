Parameter Estimation of Heavy-Tailed AR Model
with Missing Data via Stochastic EM

Junyan Liu, Sandeep Kumar, and Daniel P. Palomar, Fellow, IEEE

1

9
1
0
2

b
e
F
9

]
P
A

.
t
a
t
s
[

2
v
3
0
2
7
0
.
9
0
8
1
:
v
i
X
r
a

Abstract—The autoregressive (AR) model

is a widely used
model to understand time series data. Traditionally, the innova-
tion noise of the AR is modeled as Gaussian. However, many time
series applications, for example, ﬁnancial time series data, are
non-Gaussian, therefore, the AR model with more general heavy-
tailed innovations is preferred. Another issue that frequently
occurs in time series is missing values, due to system data record
failure or unexpected data loss. Although there are numerous
works about Gaussian AR time series with missing values, as
far as we know, there does not exist any work addressing the
issue of missing data for the heavy-tailed AR model. In this
paper, we consider this issue for the ﬁrst time, and propose an
efﬁcient framework for parameter estimation from incomplete
heavy-tailed time series based on a stochastic approximation
expectation maximization (SAEM) coupled with a Markov Chain
Monte Carlo (MCMC) procedure. The proposed algorithm is
computationally cheap and easy to implement. The convergence
of the proposed algorithm to a stationary point of the observed
data likelihood is rigorously proved. Extensive simulations and
real datasets analyses demonstrate the efﬁcacy of the proposed
framework.

Index Terms—AR model, heavy-tail, missing values, SAEM,

Markov chain Monte Carlo, convergence analysis

I. INTRODUCTION

I N the recent era of data deluge, many applications collect

and process time series data for inference, learning, pa-
rameter estimation, and decision making. The autoregressive
(AR) model is a commonly used model to analyze time series
data, where observations taken closely in time are statistically
dependent on others. In an AR time series, each sample is
a linear combination of some previous observations with a
stochastic innovation. An AR model of order p, AR(p), is
deﬁned as

p

yt = ϕ0 +

ϕiyt−i + εt,

(1)

i=1
X
where yt is the t-th observation, ϕ0 is a constant, ϕi’s are
autoregressive coefﬁcients, and εt is the innovation associated
with the t-th observation. The AR model has been successfully
used in many real-world applications such as DNA microarray
data analysis [1], EEG signal modeling [2], ﬁnancial time
series analysis [3], and animal population study [4], to name
but a few.

Traditionally, the innovation εt of the AR model is assumed
to be Gaussian distributed, which, as a result of the linear-
ity of the AR model, means that the observations are also

This work was supported by the Hong Kong RGC 16208917 research grant.
The authors are with the Hong Kong University of Science and Tech-
nology, Hong Kong (e-mail: jliubl@connect.ust.hk; eesandeep@ust.hk; palo-
mar@ust.hk).

Gaussian distributed. However, there are situations arising in
applications of signal processing and ﬁnancial markets where
the time series are non-Gaussian and heavy-tailed, either due to
intrinsic data generation mechanism or existence of outliers.
Some examples are, stock returns [3], [5], brain fMRI [6],
[7], and black-swan events in animal population [4]. For these
cases, one may seek an AR model with innovations following
a heavy-tailed distribution such as the Student’s t-distribution.
The Student’s t-distribution is one of the most commonly used
heavy-tailed distributions [8]. The authors of [9] and [10]
have considered an AR model with innovations following a
Student’s t-distribution with a known number of degrees of
freedom, whereas [11] and [12] investigated the case with an
unknown number of degrees of freedom. The Student’s t AR
model performs well for heavy-tailed AR time series and can
provide robust reliable estimates of the regressive coefﬁcients
when outliers occur.

Another issue that frequently occurs in practice is missing
values during data observation or recording process. There
are various reasons that can lead to missing values: values
may not be measured, values may be measured but get lost,
or values may be measured but are considered unusable
[13]. Some real-world cases are: some stocks may suffer a
lack of liquidity resulting in no transaction and hence no
price recorded, observation devices like sensors may break
down during measurement, and weather or other conditions
disturb sample taking schemes. Therefore, investigation of
AR time series with missing values is signiﬁcant. Although
there are numerous works considering Gaussian AR time
series with missing values [14]–[17], less attention has been
paid to heavy-tailed AR time series with missing values,
since parameter estimation in such a case is complicated due
to the intractable problem formulation. The frameworks for
parameter estimation for heavy-tailed AR time series in [9]–
[12] require complete data, and thereby, are not suited for
scenarios with missing data. The objective of the current
paper is to deal with this challenge and develop an efﬁcient
framework for parameter estimation from incomplete data
under the heavy-tailed time series model via the expectation-
maximization (EM) type algorithm.

The EM algorithm is a widely used iterative method to
obtain the maximum likelihood (ML) estimates of parameters
when there are missing values or unobserved latent variables.
In each iteration, the EM algorithm maximizes the conditional
expectation of the complete data likelihood to update the
estimates. Many variants of the EM algorithm have been
proposed to deal with speciﬁc challenges in different missing
value problems. For example, to tackle the problem posed by

 
 
 
 
 
 
the intractability of the conditional expectation of the complete
data log-likelihood, a stochastic variant of the EM algorithm,
which approximates the expectation by drawing samples of
the latent variables from the conditional distribution, has been
proposed in [18], [19]. The stochastic EM has also been quite
popular to curb the curse of dimensionality [14], [20], since its
computation complexity is lower than the EM algorithm. The
expectation conditional maximization (ECM) algorithm has
been suggested to deal with the unavailability of the closed-
form maximizer of the expected complete data log-likelihood
[21]. The regularized EM algorithm has been used to enforce
certain structures in parameter estimates like sparsity, low-
rank, and network structure [22].

In this paper, we develop a provably convergent low cost al-
gorithmic framework for parameter estimation of the AR time
series model with heavy-tailed innovations from incomplete
time series. As far as we know, there does not exist any conver-
gent algorithmic framework for such problem. Following [9]–
[11], here we consider the AR model with the Student’s t dis-
tributed innovations. We formulate an ML estimation problem
and develop an efﬁcient algorithm to obtain the ML estimates
of the parameters based on the stochastic EM framework. To
tackle the complexity of the conditional distribution of latent
variables, we propose a Gibbs sampling scheme to generate
samples. Instead of directly sampling from the complicated
conditional distribution, the proposed algorithm just need to
sample from Gaussian distributions and gamma distributions
alternatively. The convergence of the proposed algorithm to a
stationary point is established. Simulations on real data and
synthetic data show that the proposed framework can provide
accurate estimation of parameters for incomplete time series,
and is also robust against possible outliers. Although here we
only focus on the Student’s t distributed innovation, the idea of
the proposed approach and the algorithm can also be extended
to the AR model with other heavy-tailed distributions.

This paper is organized as follows. The problem formulation
is provided in Section II. The review of the EM and its
stochastic variants is presented in Section III. The proposed
algorithm is derived in Section IV. The convergence analysis
is carried out in Section V. Finally, Simulation results for the
proposed algorithm applied to both real and synthetic data are
provided in Section VI, and Section VII concludes the paper.

II. PROBLEM FORMULATION

For simplicity of notations, we ﬁrst introduce the AR(1)
model. Suppose a univariate time series y1, y2,. . ., yT follows
an AR(1) model

yt = ϕ0 + ϕ1yt−1 + εt,

(2)

where the innovations εt’s follow a zero-mean heavy-tailed
. The Student’s t-
Student’s t-distribution εt
distribution is more heavy-tailed as the number of degrees of
freedom ν decreases. Note that the Gaussian distribution is a
special case of the Student’s t-distribution with ν = +

i.i.d.
∼

0, σ2, ν

(cid:0)

(cid:1)

t

Given all the parameters ϕ0, ϕ1, σ2 and ν, the distribution
t−1, which
. . .,yt−1, only depends on the previous

the preceding data

of yt conditional on all
consists of y1,y2,
sample yt−1:

F

.
∞

2

(3)

− ν+1
2

,

p

|

yt
= p
(cid:0)
= ft
(cid:0)

t−1

ϕ0, ϕ1, σ2, ν,
ϕ0, ϕ1, σ2, ν, yt−1
yt
(cid:1)
|
yt; ϕ0 + ϕ1yt−1, σ2, ν
(cid:1)

F

(yt

−

1 +

ϕ1yt−1)2

(cid:1)
ϕ0 −
νσ2

!

=

ν+1
(cid:0)
Γ
2
√νπσΓ
(cid:0)

ν
(cid:1)
2
(cid:0)

where ft (
·
a Student’s t-distribution.

(cid:1)

) denotes the probability density function (pdf) of

In practice, a certain sample yt may be missing due to
various reasons, and it is denoted by yt = NA (not available).
Here we assume that the missing-data mechanism is ignorable,
i.e., the missing does not depend on the value [13]. Suppose
we have an observation of this time series with D missing
blocks as follows:

y1, . . . , yt1, NA, . . . , NA, yt1+n1+1, . . . ytd , NA, . . . , NA,
ytd+nd+1, . . . , ytD , NA, . . . , NA, ytD+nD +1, . . . , yT ,

where, in the d-th missing block, there are nd missing samples
ytd+1,. . .,ytd+nd , which are surrounded from the left and the
right by the two observed data ytd and ytd+nd+1. We set for
convenience t0 = 0 and n0 = 0. Let us denote the set of
the indexes of the observed values by Co, and the set of
the indexes of the missing values by Cm. Also denote y =
Cm) .
(yt, 1
∈
Θ with Θ =
. Ignoring the marginal distribution of y1,
(cid:1)
the log-likelihood of the observed data is
(cid:8)

Let us assume θ =
σ2 > 0, ν > 0
θ

Co), and ym = (yt, t

∈
ϕ0, ϕ1, σ2, ν

T ), yo = (yt, t

≤

≤

∈

(cid:9)

(cid:0)

t

|

(4)

.

!

(cid:1)

l (θ; yo) = log

p (y; θ) dym

= log

(cid:18)Z

T

 Z

t=2
Y
T

(cid:19)
t−1) dym

F

!

p (yt

θ,

|

ft

yt; ϕ0 + ϕ1yt−1, σ2, ν

dym

= log

 Z

t=2
Y

(cid:0)

Then the maximum likelihood (ML) estimation problem for θ
can be formulated as

maximize
θ∈Θ

l (θ; yo) .

(5)

The integral in (4) has no closed-form expression, thus, the
objective function is very complicated, and we cannot solve
the optimization problem directly. In order to deal with this, we
resort to the EM framework, which circumvents such difﬁculty
by optimizing a sequence of simpler approximations of the
original objective function instead.

III. EM AND ITS STOCHASTIC VARIANTS

The EM algorithm is a general iterative algorithm to solve
ML estimation problems with missing data or latent data. More
speciﬁcally, given the observed data X generated from a sta-
tistical model with unknown parameter θ, the ML estimator of
the parameter θ is deﬁned as the maximizer of the likelihood
of the observed data

l (X; θ) = log p(X
|

θ).

(6)

 
3

In practice, it often occurs that l (X; θ) does not have manage-
able expression due to the missing data or latent data Z, while
the likelihood of complete data p(X, Z
θ) has a manageable
|
expression. This is when the EM algorithm can help. The
EM algorithm seeks to ﬁnd the ML estimates by iteratively
applying these two steps [23]:

(E)

Expectation: calculate the expected log-likelihood of
the complete data set (X, Z) with respect to the
current conditional distribution of Z given X and
the current estimate of the parameter θ(k):

The SAEM requires a smaller amount of samples per iteration
due to the recycling of the previous simulations. A small value
of L is enough to ensure satisfying results [27].

When the conditional distribution is very complicated, and
the simulation step (E-S1) of the SAEM cannot be directly
performed, Kuhn and Lavielle proposed to combine the SAEM
algorithm with a Markov Chain Monte Carlo (MCMC) proce-
dure, which yields the SAEM-MCMC algorithm [19]. Assume
the conditional distribution p (Z
X, θ) is the unique stationary
|
distribution of the transition probability density function Πθ,
the simulation step of the SAEM is replaced with

Q

θ

θ(k)

=

|

log p (X, Z
|

θ) p

(cid:16)

Z
where k is the iteration number.
(M) Maximization: ﬁnd the new estimate

(cid:17)

(cid:16)

Z
|

X, θ(k)

dZ,
(7)

(cid:17)

θ(k+1) = arg max

θ

Q

θ

(cid:16)

|

θ(k)

.

(cid:17)

(8)

The sequence

l

X; θ(k)

generated by the EM algorithm

(cid:16)

n

(cid:17)o
is non-decreasing, and the limit points of the sequence
are proven to be the stationary points of the observed data
o
log-likelihood under mild regularity conditions [24]. In fact,
the EM algorithm is a particular choice of the more general
majorization-minimization algorithm [25].

θ(k)

n

However, in some applications of the EM algorithm, the
expectation in the E step cannot be obtained in closed-form.
To deal with this, Wei and Tanner proposed the Monte Carlo
EM (MCEM) algorithm, in which the expectation is computed
by a Monte Carlo approximation based on a large number of
independent simulations of the missing data [26]. The MCEM
algorithm is computationally very intensive.

In order to reduce the amount of simulations required by the
MCEM algorithm, the stochastic approximation EM (SAEM)
algorithm replaces the E step of the EM algorithm by a
stochastic approximation procedure, which approximates the
expectation by combining new simulations with the previous
ones [18]. At iteration k, the SAEM proceeds as follows:
generate

Z(k,l)
(l = 1, 2 . . . , L) from the conditional distribution
p

(E-S1) Simulation:

realizations

X, θ(k)

L

Z(k−1,l),
·
the sequence
(cid:1)

(cid:0)

(E-S2) Simulation: draw realizations Z(k,l) (l = 1, 2 . . . , L)
based on the transition probability density function
Πθ(k)

.

Z(k,l)

is a Markov chain
For each l,
. The
with the transition probability density function
Πθ(k)
Markov Chain generation mechanism needs to be well de-
signed so that the sampling is efﬁcient and the computational
cost is not too high.

k≥0

(cid:8)

(cid:9)

{

}

IV. SAEM-MCMC FOR STUDENT’S t AR MODEL

For the ML problem (5), if we only regard ym as missing
data and apply the EM type algorithm, the resulting condi-
tional distribution of the missing data is still complicated, and
it is difﬁcult to maximize the expectation or the approximated
expectation of the complete data log-likelihood. Interestingly,
the Student’s t-distribution can be regarded as a Gaussian
mixture [28]. Since εt
, we can present it as
a Gaussian mixture

0, σ2, ν

∼

t

(cid:0)
σ2, τt ∼

εt

|

(cid:1)
σ2
τt (cid:19)

,

N

0,

(cid:18)

τt ∼ Gamma (ν/2, ν/2) ,

(11)

(12)

where τt is the mixture weight. Denote τ =
.
}
We can use the EM type algorithm to solve the above
optimization problem by regarding both ym and τ as latent
data, and yo as observed data.

τt, 1 < t

≤

T

{

The resulting complete data likelihood is

L (θ; y, τ )
= p (y, τ ; θ)

(E-A) Stochastic approximation: update ˆQ

(cid:16)

(cid:17)

ac-

T

θ(k)

|

θ

(cid:16)

(cid:17)

(cid:17)
θ(k−1)

+ γ(k)

(cid:17)

1
L

(cid:18)

θ

|

(cid:16)

L

log p

X, Z(k,l)
(cid:16)
θ(k−1)

l=1
X
ˆQ

θ

−

|

,
(cid:17)(cid:19)

(9)
is a decreasing sequence of positive

(cid:16)

γ(k)

where
step sizes.

(cid:8)

(cid:9)

(M) Maximization: ﬁnd the new estimate

θ(k+1) = arg max

θ

ˆQ

θ

|

(cid:16)

θ(k)

.

(cid:17)

(10)

=

=

θ

|

(cid:17)

fN

yt; ϕ0 + ϕ1yt−1,

t=2 (cid:26)
Y
T

t=2(
Y

(cid:18)
1
2πσ2/τt

exp

−

(cid:18)

σ2
τt (cid:19)
1
2σ2/τt

ν
2

,

fg

τt;

(cid:16)

ν
2

(yt

ϕ0 −

−

(cid:17)(cid:27)
ϕ1yt−1)2

(cid:19)

ν
2

ν
p
2
ν
Γ
(cid:1)
(cid:0)
2
ν
(cid:0)
2
ν
(cid:1)
2

ν
2 −1
τ
t

exp

τt

ν
2

−

(cid:16)

)

(cid:17)
τt
2σ2 (yt

T

ν−1
2

ν
(cid:1)
2 τ
t
√2πσ2

=

exp

ν
2
Γ
t=2 (cid:0)
Y
(13)
(cid:0)
(cid:1)
where fN (
) denote the pdf’s of the Normal
) and fg (
·
·
(Gaussian) and gamma distributions, respectively. Through
some simple derivation, it is observed that the likelihood of

ϕ1yt−1)2

ϕ0 −

−

−

−

(cid:16)

τt

,

(cid:17)

Z
|

cording to

θ(k)

|

ˆQ

θ

(cid:16)
= ˆQ

complete data belongs to the curved exponential family [29],
i.e., the pdf can be written as

L (θ; y, τ ) = h (y, τ ) exp (

−

ψ (θ) +

s (yo, ym, τ ) , φ (θ)
) ,
i
h
(14)

where

,

h·

·i

is the inner product,

h (y, τ ) =

T

− 1
τ
2
t

,

(15)

ψ (θ) =

(T

1)

−

−

ν
2

(

log

ν
2

log

Γ

(cid:16)

(cid:16)

(cid:17)(cid:17)

(16)

−

1
2

−

log

1
2

−

log (2π)

,
)

∝

t=2
Y

t=2
Y
ν
2

(cid:16)

(cid:17)
σ2

4

τt
2σ2 (yt

ϕ0 −

−

ϕ1yt−1)2

ν
2

τt

−

(cid:17)

ϕ0 −

−

ϕ1yt−1)2

ν
2

−

τt

. (21)

(cid:19)

p (ym, τ

yo; θ)
|
p (y, τ ; θ)
p (yo; θ)

=

=

∝

=

p (y, τ ; θ)
p (y, τ ; θ) dymdτ

p (y, τ ; θ)
RR
T

ν

Γ
t=2 (cid:0)
Y
T

ν−1
2

2 τ
t
√2πσ2

ν
2
ν
(cid:1)
2

(cid:0)
ν−1
2

τ
t

(cid:1)
exp

(cid:18)

−

exp

−

(cid:16)
τt
2σ2 (yt

φ (θ) =

(cid:20)

ν
2

,

1
2σ2 ,

−

ϕ2
0
2σ2 ,

−

(cid:1)
(cid:0)
ϕ2
1
2σ2 ,

−

and the minimal sufﬁcient statistics

ϕ0
σ2 ,

ϕ1
σ2 ,

ϕ0ϕ1
σ2

−

,

(cid:21)
(17)

s (yo, ym, τ ) =

T

"

t=2
X
T

(log (τt)

τt) ,

−

T

T

T

T

τty2
t ,

τt,

τty2

t−1,

t=2
X
T

t=2
X

t=2
X

τtyt,

τtytyt−1,

τtyt−1

t=2
X

t=2
X

t=2
X

.

#

(18)

Then the expectation of the complete data log-likelihood

can be expressed as

θ(k)

|

(cid:17)

Q

θ

(cid:16)

=

log (L (θ; y, τ )) p

ym, τ

yo; θ(k)

dymdτ

Z Z

Z Z

=

=

log

h (y, τ ) exp

ψ (θ) +

−
(cid:16)
yo; θ(k)

|

(cid:18)
p

ym, τ

×

(cid:16)

log (h (y, τ )) p

(cid:17)
ym, τ

|

(cid:16)

s (yo, ym, τ ) p

Z Z

ψ (θ) +

−

φ (θ)

DZ Z

|

(cid:16)

(cid:17)

s (yo, ym, τ ) , φ (θ)

D

dymdτ

E(cid:17)(cid:19)

yo; θ(k)

dymdτ

(cid:17)
ym, τ

|

(cid:16)

yo; θ(k)

dymdτ ,

(cid:17)

|

¯s

θ(k)

=

s (yo, ym, τ ) p

ym, τ

yo; θ(k)

dymdτ .

Z Z

(cid:17)

(cid:16)

(cid:16)

(20)
The EM algorithm is conveniently simpliﬁed by utilizing the
properties of the exponential family. The E step of the EM
algorithm is reduced to the calculation of the expected minimal
sufﬁcient statistics ¯s
, and the M step is reduced to the
maximization of the function (19).

θ(k)

(cid:17)

(cid:16)

(cid:17)

A. E step

RR

p (y, τ ; θ) dymdτ does not have a
Since the integral
closed-from expression, we only know p (ym, τ
yo; θ) up to
a scalar. In addition, the proportional term is complicated,
and we cannot get closed-form expression for the conditional
expectations ¯s
. Therefore, we resort to
|
the SAEM-MCMC algorithm, which generates samples from
the conditional distribution using a Markov chain process,
and approximates the expectation ¯s
by
a stochastic approximation.

and Q

or Q

θ(k)

θ(k)

θ(k)

θ(k)

θ

θ

(cid:0)

(cid:1)

(cid:0)

(cid:1)

|

|

(cid:0)

(cid:1)

(cid:0)

(cid:1)

We propose to use the Gibbs sampling method to generate
the Markov chains. The Gibbs sampler divides the latent vari-
ables (ym, τ ) into two blocks τ and ym, and then generates a
Markov chain of samples from the distribution p (ym, τ
yo; θ)
by drawing realizations from its conditional distributions
τ , yo; θ) alternatively. More specif-
p (τ
ically, at iteration k, given the current estimate θ(k), the Gibbs
τ (k−1,l), y(k−1,l)
(l = 1, 2 . . . , L) and
sampler starts with
(cid:16)

ym, yo; θ) and p (ym

via the following

τ (k,l), y(k,l)
(cid:17)

m

|

|

|

m

generate the next sample
scheme:

• sample τ (k,l) from p
• sample y(k,l)

from p

m

(cid:16)
τ

|
ym

(cid:17)
y(k−1,l)
, yo; θ(k)
m
τ (k,l), yo; θ(k)

,

(cid:17)
.

(cid:16)

(cid:16)

|

Then the expected minimal sufﬁcient statistics ¯s
the expected complete data likelihood Q
imated by

θ

|

(cid:17)
θ(k)

θ(k)
and
are approx-
(cid:0)
(cid:1)

(cid:0)

(cid:1)

1
L

L

l=1
X

s

m , τ (k,l)

yo, y(k,l)
(cid:16)

−

(cid:17)

ˆs(k−1)

,

!
(22)

ˆQ

θ, ˆs(k)

=

(cid:16)

(cid:17)

ψ (θ) +

−

ˆs(k), φ (θ)
D
E

+ const.

(23)

|

ym, yo; θ) and p (ym

Lemmas 1 and 2 give the two conditional distributions
τ , yo; θ). Basically, to sample from
p (τ
them, we just need to draw realizations from certain Gaussian
distributions and gamma distributions, which is simple. Based
on the above sampling scheme, we can get the transition
probability density function of the Markov chain as follows:

|

E
ψ (θ) +

=

−
where

¯s

θ(k)

, φ (θ)

+ const.,

(19)

D

(cid:16)

(cid:17)

E

ˆs(k) = ˆs(k−1) + γ(k)

The conditional distribution of ym and τ given yo and θ

is:

Πθ (ym, τ , y′

m, τ ′) = p (τ ′

ym, yo; θ) p (y′

m|

|

τ ′, yo; θ) .

(24)

 
Lemma 1. Given ym, yo, and θ, the mixture weights
independent from each other, i.e.,

τt
{

}

ym, yo; θ) =

p (τ

|

T

t=2
Y

ym, yo; θ) .

p (τt

|

(25)

In addition, τt follows a gamma distribution:

ym, yo; θ

Gamma

τt

|

∼

ν + 1
2

,

(yt

ϕ0 −

−

ϕ1yt−1)2 /σ2 + ν

2

Proof: See Appendix A-A.

Lemma 2. Given τ , yo, and θ, the missing blocks yd =
[ytd+1, ytd+2, . . . , ytd+nd ]T , where d = 1, 2, . . . ,D, are inde-
pendent from each other, i.e.,

D

τ , yo; θ) =

p (yd

p (ym

|

τ , yo; θ) .

(27)

|

d=1
Y
In addition, the conditional distribution of yd only depends
on the two nearest observed samples ytd and ytd+nd+1 with

are

B. M step

5

After obtaining the approximation ˆQ

in (23), we
need to maximize it to update the estimates. The function
ˆQ

can be rewritten as

θ, ˆs(k)

θ, ˆs(k)

(cid:0)

(cid:1)

(cid:0)
ˆQ

=

(cid:1)
θ, ˆs(k)
(cid:16)
−

(cid:17)

ψ (θ) +

= (T

+

.

!

(26)

1)

−

(cid:26)
ν
ˆs(k)
1 −
2
ϕ0ϕ1ˆs(k)
7
σ2

−
where ˆs(k)

i

+ const.

ˆs(k), φ (θ)
D
ν
2

log

ν
2

E

log

Γ

(cid:16)

−
(cid:17)
0ˆs(k)
ϕ2
3
2σ2 −

−

ν
2
(cid:17)(cid:17)
(cid:16)
1ˆs(k)
ϕ2
4
2σ2 +

(cid:16)
ˆs(k)
2
2σ2 −

+ const,

σ2

log

1
2
(cid:0)
ϕ0ˆs(k)
5
σ2 +

(cid:27)
(cid:1)
ϕ1ˆs(k)
6
σ2

(33)

(i = 1, 2, . . . , 7) is the i-th component of ˆs(k).

The optimization of ϕ0, ϕ1, and σ2 is decoupled from the
with

optimization of ν. Setting the derivatives of ˆQ
respect to to ϕ0, ϕ1, and σ2 to 0 gives

θ, ˆs(k)

ϕ(k+1)
0

=

ˆs(k)
5 −

ˆs(k)
7

,

ϕ(k+1)
1
ˆs(k)
3

ϕ(k+1)
1

=

ˆs(k)
3 ˆs(k)
3 ˆs(k)
ˆs(k)

6 −

4 −

5 ˆs(k)
ˆs(k)
7
2 ,
ˆs(k)
7
(cid:16)

(cid:17)

τ , yo; θ

yd

|

∼ N

(µd, Σd) ,

where the i-th component of µd

µd(i) =

i−1

q=0
X

ϕq
1ϕ0 + ϕi

1ytd +

i
q=1

ϕ

nd+1
P
q=1

ϕi−2q
1
τtd +q
nd+1−2q
1
τtd +q

ytd+nd+1 −

×  

nd

q=0
X

P
ϕq
1ϕ0 −

ϕnd+1
1

ytd

,

!

(28)

and

σ(k+1)
(cid:16)

(cid:17)

2

=

T

1

−

1

ˆs(k)
2 +

ϕ(k+1)
0

2

ˆs(k)
3 +

(cid:18)

(cid:16)
2ϕ(k+1)
0
−
+ 2ϕ(k+1)
0

(cid:17)
ˆs(k)
5 −
ϕ(k+1)
1

2ϕ(k+1)
1
ˆs(k)
7

.
(cid:19)

The ν(k+1) can be found by:

(29)

and the component in the i-th column and the j-th row of Σd

ν(k+1) = arg max

f

ν>0

ν, ˆs(k)
1
(cid:16)

(cid:17)

Σd(i,j)

min(i,j)

=



q=1
X

ϕi+j−2q
1
τtd+q − (cid:16)P

i
q=1

ϕi−2q
1
τtd +q

j
q=1

ϕj−2q
1
τtd +q

nd+1
q=1

−2q
(cid:17) (cid:16)P
ϕ
1
τtd +q

σ2,

(cid:17)





(30)

where the sums of geometric progressions in µd(i) can be
simpliﬁed as

P

ν, ˆs(k)
1

ν
2 log

ν
2

(cid:16)

=

log

with f
2(T −1) . Ac-
−
cording to Proposition 1 in [30], ν(k+1) always exists and
(cid:1)
is unique. As suggested in [30], the maximizer ν(k+1) can
be obtained by one-dimensional search, such as half interval
method [31].

(cid:1)(cid:1)(cid:9)

Γ

(cid:17)

(cid:8)

(cid:0)

(cid:0)

(cid:0)

1

ν
2

+ ν ˆs(k)

The resulting SAEM-MCMC algorithm is summarized in

Algorithm 1.

i−1

q=0
X

ϕq

1ϕ0 =

iϕ0,
ϕ0(ϕi

1−1)

ϕ1−1

(

ϕ1 = 1,

, ϕ1 6

= 1,

and

ϕq

1ϕ0 =

nd

q=0
X

(nd + 1) ϕ0,
nd+1
−1(cid:17)
ϕ0(cid:16)ϕ
1
ϕ1−1




ϕ1 = 1,

, ϕ1 6

= 1.

(32)

Proof: See Appendix A-B.



(31)

C. Particular Cases

In cases where some parameters in θ are known, we just
need to change the updates in M-step accordingly, and the
simulation and approximation steps remain the same. For
example, if we know that the time series is zero mean [1],
and ϕ(k+1)
[12], i.e., ϕ0 = 0, then the update for ϕ(k+1)
1
should be replaced with

0

ϕ(k+1)
0

= 0,

(38)

(cid:0)

(cid:1)

(34)

(35)

2

ˆs(k)
4

ϕ(k+1)
1
(cid:16)
ˆs(k)
6

(cid:17)

(36)

(37)

 
(39)

(M1) For any θ

Θ,

∈

Algorithm 1 SAEM-MCMC Algorithm for Student’s t AR(1)
1: Initialize θ(0)
1, 2 . . . , L..

Θ, ˆs(0) = 0, k = 0, and y(0,l)

for l =

∈

m

4:

5:

2: for k = 1, 2, . . . do
Simulation:
3:
for l = 1, 2 . . . , L do
sample τ (k,l)
Lemma 1,
sample y(k,l)
Lemma 2.

6:

m

from p

τ

y(k−1,l)
m

, yo; θ(k)

using

for p

ym

τ (k,l), yo; θ(k)

|

(cid:17)

using

(cid:17)

|

(cid:16)

(cid:16)

7:
8:

end for
Stochastic approximation: evaluate ˆs(k) and ˆQ
as in (22) and (23) respectively.

(cid:1)
9: Maximization: update θ(k+1) as in (34), (35), (36) and

(cid:0)

θ, ˆs(k)

(37).
if stopping criteria is met then

10:

terminate loop

11:
end if
12:
13: end for

and

ϕ(k+1)
1

=

ˆs(k)
6
ˆs(k)
4

,

If the time series is known to follow the random walk model
[14], which is a special case of AR(1) model with ϕ1 = 1,
and ϕ(k+1)
then the update for ϕ(k+1)
should be replaced with
1
ˆs(k)
5 −
ˆs(k)
3

ϕ(k+1)
0

ˆs(k)
7

(40)

=

0

,

and

ϕ(k+1)
1

= 1.

(41)

D. Generalization to AR(p)

The above ML estimation method can be immediately

generalized to the Student’s t AR(p) model:

p

yt = ϕ0 +

ϕiyt−i + εt,

(42)

i=1
X

|
θ(k)

i.i.d.
∼

t

0, σ2, ν

. Similarly, we can apply the SAEM-
where εt
MCMC algorithm to obtain the estimates by considering τ and
ym as latent data, and yo as observed data. At each iteration,
we draw some realizations of τ and ym from the conditional
to approximate the expecta-
distribution p

yo; θ(k)

ym, τ

(cid:0)

(cid:1)

|

|

θ

θ

(cid:17)

(cid:17)

(cid:16)

(cid:17)

(cid:16)

, and maximize the approximation

(cid:16)
tion function Q
ˆQ
θ(k)
to update the estimates. The main difference is
that the conditional distribution of the AR(p) will become
more complicated than that of the AR(1), since each sample of
the AR(p) has more dependence on the previous samples. To
deal with this challenge, when applying the Gibbs sampling,
we can divide the the latent data (ym, τ ) into more blocks, τ
as a block and each yi∈Cm as a block, so that the distribution
of each block of latent variables conditional on other latent
variables will be easy to obtain and sample from. For limit of

6

space, we do not go into details here, and we will consider
this in our future work.

V. CONVERGENCE

In this section, we provide theoretical guarantee for the
convergence of the proposed algorithm. The convergence of
the simple deterministic EM algorithm has been addressed
by many different authors, starting from the seminal work
in [23], to a more general consideration in [24]. However,
the convergence analysis of stochastic variants of the EM
algorithm, like the MCEM, SAEM and SAEM-MCMC algo-
rithms, is challenging due to the randomness of sampling. See
[18], [19], [32]–[35] for a more general overview of these
stochastic EM algorithms and their convergence analysis. Of
speciﬁc interest, the authors in [18] introduced the SAEM
algorithm, and established the almost sure convergence to the
stationary points of the observed data likelihood under mild
additional conditions. The authors in [19] coupled the SAEM
framework with an MCMC procedure, and they have given the
convergence conditions for the SAEM-MCMC algorithm when
the complete data likelihood belongs to the curved exponential
family. The given set of conditions in our case is as follows.

s (yo, ym, τ )

.
∞
(43)
(M2) ψ (θ) and φ (θ) are twice continuously differentiable

yo; θ) dymdτ <

p (ym, τ

Z Z

k

k

|

on Θ.
(M3) The function

¯s (θ) =

s (yo, ym, τ ) p (ym, τ

Z Z

yo; θ) dymdτ

(44)

|

is continuously differentiable on Θ.

(M4) The objective function

l (θ; yo) = log

p (y, τ ; θ) dymdτ

(45)

(cid:18)Z Z
is continuously differentiable on Θ, and

(cid:19)

∂θ

p (y, τ ; θ) dymdτ =

∂θp (y, τ ; θ) dymdτ .

Z Z

(M5) For Q (θ, ¯s) =

Z Z

ψ (θ) +

(46)
+ const., there
i
¯s and
θ
Θ,
∀
Q (θ, ¯s) . In addition, the function

¯s, φ (θ)
h

∈

∀

˜θ (¯s) , ¯s
(cid:16)
(cid:17)

−
exists a function ˜θ (¯s) such that
Q
˜θ (¯s) is continuously differentiable.
∞
k=1 γ(k) =
γ(k)

[0, 1],
∈
1 such that
P

(SAEM1) For all k, γ(k)
there exists 1
2 < λ

∞
k=1

≥

≤

.
∞
(SAEM2) l (θ; yo) is d times differentiable on Θ, where
d = 7 is the dimension of s (yo, ym, τ ), and ˜θ (s) is
d times differentiable.

P

(cid:0)

(cid:1)

∞
1+λ

and
<

(SAEM3)

1) The chain takes its values in a compact set Ω.
2) The s (yo, ym, τ ) is bounded on Ω, and the
takes its values in a compact

ˆs(k)

sequence
subset.

(cid:8)

(cid:9)

3) For any compact subset V of Θ, there exists a
in V 2

real constant L such that for any

θ, θ′

sup
m,τ ′)∈Ω2

(ym,τ ,y′

Πθ (ym, τ , y′

(cid:0)
m, τ ′)

(cid:1)

(cid:12)
(cid:12)
(cid:12)
−

Πθ′ (ym, τ , y′

m, τ ′)

(47)

L

θ

|

−

θ′

.

|

≤

(cid:12)
(cid:12)
(cid:12)

4) The transition probability Πθ generates a
chain whose
invariant
conditional distribution

uniformly ergodic
the
probability is
yo; θ).
p (ym, τ

|

In summary, the conditions (M1)-(M5) are all about the model,
and are conditions for the convergence of the deterministic
EM algorithm. The conditions (M1) and (M3) require the
boundedness and continuous differentiability of the expecta-
tion of the sufﬁcient statistics. The conditions (M2) and (M4)
guarantee the continuous differentiability of the complete data
log-likelihood l (θ; y, τ ), the expectation of the complete data
likelihood Q
, and the observed data log-likelihood
l (θ; yo). The condition (M5) indicates the existence of a
(cid:16)
global maximizer for Q (θ, ¯s).

θ(k)

(cid:17)

θ

|

k

n

o

≥

(cid:9)
≤

γ(k)

θ(k)

K and γ(k) = 1

The conditions (SAEM1)-(SAEM3) are additional require-
ments for the SAEM-MCMC convergence. The condition
(SAEM1) is about the step sizes
. This condition can
be easily satisﬁed by choosing the step sizes properly. It is
(cid:8)
recommended to set γ(k) = 1 for 1
k−K
≤
for k
K + 1, where K is a positive integer, since the
initial guess θ(0) may be far from the ML estimates we are
looking for, and choosing the ﬁrst K step sizes equal to 1
to have a large variation and then
allows the sequence
converge to a neighborhood of the maximum likelihood [27].
The condition (SAEM2) requires d = 7 times differentiability
of l (θ; yo) and ˆθ
. The condition (SAEM3) imposes
some constraints on the generated Markov chains.
(cid:1)

In [19], the authors have established the convergence of the
SAEM-MCMC algorithm to the stationary points. However,
their analysis assumes that complete data likelihood belongs to
the curved exponential family, and all these conditions (M1)-
(M5) and (SAEM1)-(SAEM3) are satisﬁed. These assumptions
are very problem speciﬁc, and do not hold trivially for our
case, since our conditional distribution of the latent variable is
extremely complicated. To comment on the convergence of our
proposed algorithm, we need to establish the conditions (M1)-
(M5) and (SAEM1)-(SAEM3) one by one. Finally, we have the
convergence result about our proposed algorithm summarized
in the following theorem.

ˆs(k)

(cid:0)

Theorem 1. Suppose that the parameter space Θ is set to be a
sufﬁciently large bounded set1 with the parameter ν > 2, and
the Markov chain generated from (25) and (27) takes values in
a compact set2, the sequence
generated by Algorithm

θ(k)

n
1This means that the unconstrained maximizer of (33) (given by (34), (35),

o

(36), and (37)) lies in this bounded set.

2Theoretically, the Markov chain generated from (25) and (27) takes its
values in an unbounded set. However, in practice, the chain will not take
very large values, and we can consider the chain takes values in a very large
compact set [19], [27].

7

1 has the following asymptotic property: with probability 1,
denotes the
= 0, where d
limk→+∞ d
distance from θ(k) to the set of stationary points of observed
data log-likelihood

(cid:16)
Θ, ∂l(θ;yo)

θ(k),

θ(k),

(cid:17)
.

=

L

L

(cid:17)

(cid:16)

θ

∂θ = 0

o

Proof: Please refer to Appendix B for the proof of the
conditions (M1)-(M5) and (SAEM2)-(SAEM3). The condition
(SAEM1) can be be easily satisﬁed by choosing the step
sizes properly as mentioned before. Upon establishing these
conditions, the proof of this theorem follows straightforward
from the analysis of the work in [19].

L

∈

n

VI. SIMULATIONS

In this section, we conduct a simulation study of the per-
formance of the proposed ML estimator and the convergence
of the proposed algorithm. First, we show that the proposed
estimator is able to make good estimates of parameters from
the incomplete time series which have been synthesized to
ﬁt the model. Second, we show its robustness to innovation
outliers. Finally, we test it on a real ﬁnancial time series, the
Hang Seng index.

A. Parameter Estimation

In this subsection, we show the convergence of the proposed
SAEM-MCMC algorithm and the performance of the proposed
estimator on incomplete Student’s t AR(1) time series with
different numbers of samples and missing percentages. The
estimation error is measured by the mean square error (MSE):

2

,

(cid:21)

(cid:17)

−

θtrue

MSE (θ) := E

ˆθ
(cid:20)(cid:16)
where ˆθ is the estimate for the parameter θ, and θtrue is
its true value. The parameter θ can be ϕ0, ϕ1, σ2, and ν.
The expectation is approximated via Monte Carlo simulations
using 100 independent incomplete time series.

We set ϕtrue

0 = 1, ϕtrue

1 = 0.5, (σtrue)2 = 0.01, and
νtrue = 2.5. For each incomplete data set yo, we ﬁrst
with T samples
randomly generate a complete time series
based on the Student’s t AR(1) model. Then nmis number of
samples are randomly deleted to obtain an incomplete time
series. The missing percentage of the incomplete time series
is ρ := nmis

100%.

yt
{

}

In Section V, we have established the convergence of the
proposed SAEM-MCMC algorithm to the stationary points of
the observed data likelihood. However, it is observed that the
estimation result obtained by the algorithm can be sensitive
to initializations due to the existence of multiple stationary
points. This is an inevitable problem since it is a non-convex
optimization problem. Interestingly, it is also observed that
when we initialize our algorithm using the ML estimates
assuming the Gaussian AR(1) model, the ﬁnal estimates are
signiﬁcantly improved, in comparison to random initializa-
tions. The ML estimation of the Gaussian AR model from
incomplete data has been introduced in [13], and the estimates
can be easily obtained via the deterministic EM algorithm.
We initialize ϕ(0)
use the estimates from

σ(0)

0 , ϕ(0)

1 , and

2

T ×

(cid:0)

(cid:1)

0

1

2

2
1
.
1

8
0
.
1

4
0
.
1

0
0
.
1

0
5
.
0

8
4
.
0

6
4
.
0

4
4
.
0

0
3
0
.
0

0
2
0
.
0

0
1
0
.
0

0
1

8

6

4

2

0

20

40

60

80

100

Iteration k

Fig. 1. Estimates versus iterations.

TABLE I
ESTIMATION RESULTS FOR INCOMPLETE STUDENT’S t AR(1).

True value
Gaussian AR(1)
Student’s t AR(1)

ˆϕ0
1.000
1.119
0.989

ˆϕ1
0.500
0.442
0.501

2
(ˆσ)
0.010
0.033
0.009

ˆν
2.5
+∞
2.234

σ2

m

(cid:0)

(cid:1)

(cid:0)

≤

≥

≤

σ2

30 and γ(k) = 1

ym; yo, (ϕ0)g , (ϕ1)g ,
(cid:16)

g , and
the Gaussian AR(1) model (ϕ0)g, (ϕ1)g, and
initialize y(0,l)
using the mean of the conditional distribution
(cid:1)
, which is a Gaussian distri-
p
g
bution. The parameter ν(0) is initialized as a random positive
(cid:17)
number. In each iteration, we draw L = 10 samples. For the
step sizes, we set γ(k) = 1 for 1
k
k−30
for k
31. Figure 1 gives an example of applying the
proposed SAEM-MCMC algorithm to estimate the parameters
on a synthetic AR(1) data set with T = 300 and a missing
percentage ρ = 10%. We can see that the algorithm converges
in less than 100 iterations, where each iteration just needs
L = 10 runs of Gibbs sampling, and also the ﬁnal estimation
error is small. Table I compares the estimation results of
the Student’s t AR model and the Gaussian AR model. This
testiﬁes our argument that, for incomplete heavy-tailed data,
the traditional method for incomplete Gaussian AR time series
is too inefﬁcient, and signiﬁcant performance gain can be
achieved by designing algorithms under heavy-tailed model.
Figure 2 shows the estimation results with the numbers

8

r = 40%     

r = 30%     

r = 20%     

r = 10%     

r = 0

0

 f
f

o
E
S
M

1

 f
f

o
E
S
M

2

 s
f
o
E
S
M

 n
f
o
E
S
M

4
0
0

.

3
0

.

0

2
0

.

0

1
0

.

0

0
0
0

.

8
0
0
0

.

4
0
0
0

.

.

0
0
0
0
5
0
−
e
2
.
1

6
0
−
e
0
.
8

6
0
−
e
0
.
4

0
0
+
e
0
.
0

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

100

200

300

400

500

Number of samples T

Fig. 2. MSEs for the incomplete time series with different number of samples
and missing percentages.

of samples T = 100, 200, 300, 400, 500 and the missing
percentages ρ =10%, 20%, 30%, 40%. For reference, we have
also given the ML estimation result from the complete data
sets (ρ =0), which is obtained using the algorithm in [11].
We can observe that our method performs satisfactorily well
even for high percentage of missing data, and, with increasing
sample sizes, the estimates with missing values match with
the estimates of the complete data.

B. Robustness to Outliers

A useful characteristic of the Student’s t is its resilience to
outliers, which is not shared by the Gaussian distribution. Here
we illustrate that the Student’s t AR model can provide ro-
bust estimation of autoregressive coefﬁcients under innovation
outliers.

An innovation outlier is an outlier in the εt process, and it
is a typical kind of outlier in AR time series [36], [37]. Due to

f
f
s
n
 
 
 
 
TABLE II
ESTIMATION AND PREDICTION RESULTS FOR INCOMPLETE GAUSSIAN
AR(1) TIME SERIES WITH OUTLIERS.

Gaussian AR(1)
Student’s t AR(1)
Kharin’s method

ˆϕ1 (ϕtrue

1 = 0.5)
0.5337
0.4947
0.4210

Averaged prediction error
0.0121
0.0110
0.0212

the temporal dependence of AR time series data, an innovation
outlier will affect not only the current observation yt, but
also subsequent observations. Figure 3 gives an example of a
Gaussian AR(1) time series contaminated by four innovation
outliers.

is unknown.

i.i.d.
∼ N

1 = 0.5 and εt

is known and ϕtrue

When an AR time series is contaminated by outliers, the
traditional ML estimation of autoregressive coefﬁcients based
on the Gaussian AR model, which is equivalent
to least
squares ﬁtting, will provide unreliable estimates. Although,
for complete time series, there are numerous works about the
robust estimation of autoregressive coefﬁcients under outliers,
unfortunately, less attention was paid to robust estimation
from incomplete time series. As far as we know, only Kharin
and Voloshko have considered robust estimation with missing
values [16]. In their paper, they assume that φ0 is known and
equal to 0. To be consistent with Kharin’s method, in this
simulation, we also assume ϕtrue
0 = 0,
0
although our method can also be applied to the case where
ϕtrue
0
We let ϕtrue

(0, 0.01). Note here
the innovations follow a Gaussian distribution. We randomly
generate an incomplete Gaussian AR(1) time series with
T = 100 samples and a missing percentage ρ = 0.1, and
it is contaminated by four innovation outliers. The values
of the innovation outliers are set to be 5, -5, 5,
5, and
the positions are selected randomly. See Figure 3 for this
incomplete contaminated time series. The Gaussian AR(1)
model, the Student’s t AR(1) model, and Kharin’s method
are applied to estimate the autoregressive coefﬁcient ϕ1. After
obtaining the estimate ˆϕ1, we compute the one-step-ahead
yt)2 for
predictions ˆyt = ˆϕ1yt−1 and the prediction error (ˆyt
Co. It is not surprising that the outliers are
t
poorly predicted, so we omit it when computing the averaged
prediction error. Table II shows the estimation results and
the one-step-ahead prediction errors. It is clear that the ML
estimator based on the Gaussian AR(1) has been signiﬁcantly
affected by the presence of the outliers, while the Student’s
t AR(1) model is robust to them, since the outliers cause
the innovations to have a heavy-tailed distribution, which can
be modeled by the Student’s t distribution. Kharin’s method
does not perform well, either, as this method is designed
for addictive outliers and replacement outliers, rather than
innovation outliers.

Co and t

−

−

−

∈

∈

1

C. Real Data

Here we consider the returns of the Hang Seng index over
260 working days from Jan. 2017 to Nov. 2017 (excluding
weekends and public holidays). Figure 4 shows the quantile-
quantile (QQ) plot of these returns. The deviation from the

9

outlier
missing values

4

2

t

y

0

2
−

4
−

0

20

40

60

80

100

t

Fig. 3.

Incomplete AR(1) time series with four innovation outliers.

s
e

l
i
t
n
a
u
Q
e
p
m
a
S

l

5
1
0
.
0

1
0
.
0

5
0
0
.
0

0

5
0
0
.
0
−

1
0
.
0
−

5
1
0
.
0
−

−3

−2

−1

0

1

2

3

Standard Normal Quantiles

Fig. 4. Quantile-quantile plot of the the Hang Seng index returns showing
that they are heavy-tailed.

straight red line indicates that the returns are signiﬁcantly non-
Gaussian and heavy-tailed.

We divide the 260 returns into two parts: the estimation data,
which involves the ﬁrst 250 samples, and the test data, which
involves the remaining 10 samples. First, we ﬁt the estimation
data to the Gaussian AR(1) model and the Student’s t AR(1)
model, and estimate the parameters. Then we predict the test
data using the one-step-ahead predication method based on the
estimates, and compute the averaged prediction errors. Next,
we randomly delete 10 of the estimation data, and estimate the
parameters of the Gaussian AR(1) model and the Student’s
t AR(1) model from this incomplete data set. Finally, we
also make predictions and compute the averaged prediction
errors based on these estimates of the parameters. The result is
summarized in Table III. We have the following conclusions: i)

 
TABLE III
ESTIMATION AND PREDICTION RESULTS FOR THE HANG SENG INDEX RETURNS.

Complete data assuming Gaussian innovations
Incomplete data assuming Gaussian innovations
Complete data assuming Student’s t innovations
Incomplete data assuming Student’s t innovations

ˆϕ0

ˆϕ1

7.548 × 10−4 −1.058 × 10−1
8.618 × 10−4 −1.253 × 10−1
5.440 × 10−4 −9.580 × 10−2
5.538 × 10−4 −9.459 × 10−2

2
(ˆσ)
1.702 × 10−5
1.665 × 10−5
6.524 × 10−6
6.331 × 10−6

ˆν
+∞
+∞
2.622
2.671

Averaged prediction error
9.141 × 10−6
9.455 × 10−6
8.836 × 10−6
8.831 × 10−6

10

the Student’s t AR(1) model performs better than the Gaussian
AR(1) model for this heavy-tailed time series, ii) the proposed
parameter estimation method for incomplete Student’s t AR(1)
time series can provide similar estimates to the result of
complete data.

VII. CONCLUSIONS

In this paper, we have considered parameter estimation of
the heavy-tailed AR model with missing values. We have for-
mulated an ML estimation problem and developed an efﬁcient
approach to obtain the estimates based on the stochastic EM.
Since the conditional distribution of the latent data in our case
is complicated, we proposed a Gibbs sampling scheme to draw
realizations from it. The convergence of the proposed algo-
rithm to the stationary points has been established. Simulations
show that the proposed approach can provide reliable estimates
from incomplete time series with different percentages of
missing values, and is robust to outliers. Although in this paper
we only focus on the univariate AR model with the Student’s
t distributed innovations due to the limit of the space, our
method can be extended to multivariate AR model and also
other heavy-tailed distributed innovations.

APPENDIX A
PROOF FOR LEMMAS 1 AND 2

A. Proof for Lemma 1

The conditional distribution of τ

ym, yo; θ is

|

p (τ

ym, yo; θ)
|
p (y, τ ; θ)
p (y; θ)
p (y, τ ; θ)

=

∝

=

ν

ν−1
2

2 τ
t
√2πσ2

ν
2
ν
(cid:1)
2

T

Γ
t=2 (cid:0)
Y
T

exp

(cid:16)
(yt

τt
2σ2 (yt

−

−

ϕ0 −
ϕ1yt−1)2

ϕ1yt−1)2

ν
2

τt

−

(cid:17)

+

ν
2 !

τt

,

!

(48)
are independent from each other with

(cid:0)
ν−1
2

τ
t

(cid:1)
exp

 −  

−

ϕ0 −
2σ2

∝

t=2
Y

which implies that

τt
{

}

p (τt

ym, yo; θ)

|
ν−1
2

τ
t

∝

exp

 −  

(yt

−

ϕ1yt−1)2

ϕ0 −
2σ2

+

ν
2 !

τt

.

!

(49)

Comparing this expression with the pdf of the gamma distri-
ym, yo; θ follows a gamma distribution:
bution, we get that τt

|

ym, yo; θ

Gamma

τt

|

∼

ν + 1
2

,

(yt

ϕ0 −

−

ϕ1yt−1)2 /σ2 + ν

2

!

.

(50)

B. Proof for Lemma 2

(12), given τ and θ, εt follows a Gaussian distribution: εt

According to the Gaussian mixture representation (11) and
i.i.d.
∼
. From equation (2), we can see that, given τ and
N
θ, the distribution of yt conditional on all the preceding data

(cid:16)
t−1, only depends on the previous sample yt−1:

µ, σ2
τt

(cid:17)

F

p (yt

τ ,

t−1; θ) =p (yt

τ , yt−1; θ) .

|

F

|
the distribution of yt conditional on all

the
o
t−1, τ , and θ, only depends on

(51)

In addition,
preceding observed data
F
the nearest observed sample:

p

yt

τ ,

o
t−1; θ
τ , yt−1; θ)
(cid:1)

|
F
p (yt

(cid:0)
= 


|

|

p (yt

τ , yt−nd−1; θ)

t = td + nd + 2, . . . , td+1,
f or d = 0, 1, . . . , D,
t = td + nd + 1, f or d = 1, 2, . . . , D.



(52)
The ﬁrst case refers to the situation where the previous sample
yt−1 is observed, while the second case is when yt−1 is
missing.

Based on the above properties, we have

p (ym

|

τ , yo; θ) =

=

=

=

=

τ , yt−1; θ)

Q
Q

t∈Co

τ ,
τ ,

T
t−1; θ)
t=2 p (yt
F
|
o
t−1; θ
yt
p
F
|
T
τ , yt−1; θ)
t=2 p (yt
(cid:0)
(cid:1)
|
td+1
t=td+nd+2 p (yt
1

Q

|

D
d=0

Q
×

Q
D

Q
D
τ , ytd; θ)
d=1 p (ytd+nd+1|
D
td+nd+1
τ , yt−1; θ)
t=td+1 p (yt
Q
d=1
|
D
τ , ytd ; θ)
d=1 p (ytd+nd+1|
Q
p (yd, ytd+nd+1|
Q
p (ytd+nd+1|

τ , ytd ; θ)

τ , ytd ; θ)

(53a)

(53b)

(53c)

(53d)

p (yd

|

τ , ytd , ytd+nd+1; θ) ,

(53e)

d=1
Y
D

d=1
Y

where the equations (53a) and (53e) are from the deﬁnition
of conditional pdf, the equation (53b) is from (51) and (52).
The equation (53e) implies that the different missing blocks
yd
are independent from each other, and the conditional
{
distribution of yd only depends on the two nearest observed
samples ytd and ytd+nd+1.

}

|

To
p (yd
pdf of
ycd =
p (ycd

of

pdf

obtain

the
τ , ytd , ytd+nd+1; θ), we

block
joint
ﬁrst
the missing block and next observed sample
yT
= [ytd+1, ytd+2, . . . , ytd+nd+1]:

the missing
analyze

d , ytd+nd+1

the

T

τ , ytd ; θ). Given τ , ytd , and θ, from (2), we have

|

(cid:2)

(cid:3)

 
ytd+i =ϕ0 + ϕ1ytd+i−1 + εtd+i

=ϕ0 + ϕ1 (ϕ0 + ϕ1ytd+i−2 + εtd+i−1) + εtd+i
=ϕ0 + ϕ1ϕ0 + ϕ2
1ytd+i−2 + ϕ1εtd+i−1 + εtd+i

(54)

=

i−1

q=0
X

ϕq
1ϕ0 + ϕi

1ytd +

ϕ(i−q)
1

εtd+q,

i

q=1
X

for i = 1, 2, . . .,nd + 1, which means that ytd+i can be
expressed as the sum of the constant
1ytd
and a linear combination of the independent Gaussian random
variables εtd+1, εtd+2, . . ., εtd+i. Therefore, we can obtain
that ycd follows a Gaussian distribution as follows:

1ϕ0 + ϕi

q=0 ϕq

P

i−1

ycd

τ , ytd ; θ

∼ N
where the i-th component of µcd

|

µcd(i) =E [ytd+i]

(µcd, Σcd) ,

(55)

=E

i−1

ϕq
1ϕ0 + ϕi

1ytd +

ϕ(i−q)
1

εtd+q

i

#

E [εtd+q]

(56)

i

q=1
X
ϕ(i−q)
1

q=1
X

"
i−1

q=0
X
ϕq
1ϕ0 + ϕi

1ytd +

q=0
X
i−1

q=0
X

ϕq
1ϕ0 + ϕi

1ytd ,

=

=

and the component in the i-th column and the j-th row of Σcd
Σcd(i,j) =E

µcd(i)

ytd+j

ytd+i
i

q1=1
X
j

=E

(cid:2)(cid:0)

" 

i

=

−
ϕ(i−q1)
1

(cid:1) (cid:0)
εtd+q1

ϕ(i+j−q1−q2)
1

−

µcd(j)
j

!  

q2=1
X
E [εtd+q1 εtd+q2 ]

(cid:1)(cid:3)
ϕ(j−q2)
1

εtd+q2

!#

q1=1
X

q2=1
X
min(i,j)

=σ2

q=1
X

ϕ(i+j−2q)
1
τtd+q

.

with the last equation following from

E [εtd+q1 εtd+q2 ] =

,

σ2
τtd +q1
0,

(

q1 = q2;
q1 6

= q2.

Recall that p (yd
τ , ytd , ytd+nd+1; θ) is a conditional pdf
|
of p (yd, ytd+nd+1|
τ , ytd ; θ). Since conditional distributions
of a Gaussian distribution is Gaussian, we can get
that
yd
τ , ytd, ytd+nd+1; θ follows a Gaussian distribution as (28).
The parameters of this conditional distribution can be com-
puted based on

|

µd = µcd(1:nd) +

Σcd(1:nd,nd+1)
Σcd(nd+1,nd+1)

and

ytd+nd+1 −
(cid:0)

µcd(nd+1)

,

(58)
(cid:1)

11

where µcd(a1:a2) denotes the subvector consisting of the a1-th
to a2-th component of µcd, and the Σcd(a1:a2,b1:b2) means the
submatrix consisting of the components in the a1-th to a2-
th rows and the b1-th to b2-th columns of Σcd. Plugging the
equations (56) and (57) into the equations (58) and (59) gives
the equations (29) and (30), respectively.

APPENDIX B
PROOF FOR CONDITIONS (M1)-(M5) AND
(SAEM2)-(SAEM3)

In this section, we will establish the listed conditions one
by one. The observed data yo is known. We assume that yo is
ﬁnite. Since the parameter space Θ is a large bounded set with
< ϕ+
1 , σ > σ−,
ν > 2, we can assume that
ϕ0|
and ν− < ν < ν+, where ϕ+
0 , ϕ+
1 , and ν+ are very large
positive numbers, σ− is a very small positive number, and
ν− is a very small positive number satisfying ν−
2. We
ﬁrst prove the conditions (M1)-(M5), then prove the conditions
(SAEM2) and (SAEM3).

< ϕ+
0 ,

ϕ1|

≥

|

|

A. Proof of (M1)-(M5)

The proof begins by establishing the following two inter-

mediary lemmas.
Lemma 3. For any yo and θ

p (y, τ ; θ) dymdτ =

∈
p (y; θ) dym <

Θ, p (yo; θ) =

.
∞

Lemma 4. For any yo , θ
R R
R

Θ and 1 < t

T

≤
g (y, τ ) p (y, τ ; θ) dymdτ <

∈

(60)

t , y2

log (τt)

Z Z
where g (y, τ ) can be τt, τ 2

t−1, τty2

Lemma 3 indicates that

t ,τty2
the observed data likelihood
p (yo; θ) is bounded, and Lemma 4 shows that the expecta-
tion of g (y, τ ) is bounded. These lemmas provide the key
ingredients required for establishing (M1)-(M5), and their
usage for subsequent analysis is self-explanatory. Due to space
limitations, we do not include their proofs here. Interested
readers may refer to the supplementary material.

−

(M1) For condition (M1), based on (18), we can get

,
∞
t , or

(57)

k

Z Z
=

R R

k

s (yo, ym, τ )

k
s (yo, ym, τ )

p (ym, τ

|

yo; θ) dymdτ

p (yo, ym, τ ; θ) dymdτ

k

p (yo; θ)

1
p (yo; θ)

≤

T

log (τt)

t=2 Z Z (cid:18)
X
(cid:12)
+
(cid:12)

τty2

t−1

τt

+

τty2
t

+

τt

(cid:12)
τtyt
(cid:12)

(cid:12)
+
(cid:12)

(cid:12)
(cid:12)
τtytyt−1
(cid:12)
(cid:12)

(cid:12)
(cid:12)

−

+

1
p (yo; θ)

≤

T

+

(cid:12)
(cid:12)

(cid:12)
(cid:12)
τt

t=2 Z Z (cid:18)
X

−

τtyt−1

(cid:12)
(cid:12)

(cid:12)
p (yo, ym, τ ; θ) dymdτ
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:19)

(cid:12)
(cid:12)

log (τt) + τty2

t + τt

+ τty2
t−1 +
t + y2
τ 2
2

+

t−1

τt

+

t + y2
τ 2
t
2
(cid:1)
(cid:0)
p (yo, ym, τ ; θ) dymdτ

t + y2
y2
2

t−1

(cid:19)

(61)

Σd = Σcd(1:nd,1:nd) −

Σcd(1:nd,nd+1)Σcd(nd+1,1:nd)
Σcd(nd+1,nd+1)

,

(59)

<

,
∞

where the three inequalities follow from the triangular inequal-
, and Lemma 4,
ity, the property of squares x1x2 ≤
respectively.

1+x2
x2
2
2

(M2) From the deﬁnition of ψ (θ) and φ (θ) in (16) and
(17), their continuous differentiability can be easily veriﬁed.

(M3) For condition (M3),

≤

≤

¯s (θ) =

s (yo, ym, τ ) p (ym, τ

yo; θ) dymdτ

|

T

|
j=2(cid:18)
X
T

p (y, τ ; θ)
σ2

p (y, τ ; θ∗)
(σ−)2

τjyjyj−1|

+

|

τj

j + y2
y2
2

j−1

ϕ0τjyj−1|
ϕ+
0

+

j=2(cid:18)
X

(cid:0)

+ ϕ+

1 τj y2

j−1

(cid:1)

(cid:19)

s (yo, ym, τ )

p (y, τ ; θ)
p (yo; θ)

dymdτ

(62)

= gϕ1 (y, τ ) ,

12

+

|

ϕ1τjy2

j−1|

(cid:19)

j + y2
τ 2
2

j−1

(cid:0)

(cid:1)

(64)

Z Z

Z Z

=

=

s (yo, ym, τ ) p (y, τ ; θ) dymdτ
p (y, τ ; θ) dymdτ

.

R R

R R

R R
p (y, τ ; θ) dymdτ = p (yo; θ) > 0 and p (y, τ ; θ)
Since
is continuously differentiable, which can be easily checked
from its deﬁnition (19), we can get that ¯s (θ) is continuously
differentiable.
(M4) Since

p (y, τ ; θ) dymdτ > 0, and p (y, τ ; θ) is
p (y, τ ; θ) dymdτ
7 times differentiable, l (θ; yo) = log
is 7 times differentiable. For the veriﬁcation of the equation
(cid:1)
(46), according to Leibniz integral rule, the equation (46) holds
under the following three conditions:

(cid:0)R R

R R

p (y, τ ; θ) dymdτ <

,
∞
exists for all the θ

1)
2) ∂p(y,τ ;θ)
R R
3) there is an integrable function g (y, τ ) such that
Θ and almost every y

g (y, τ ) for all θ

Θ,

∂θ

∈

∈

Since the ﬁrst condition has been proved in Lemma 3, and
the second condition can be easily veriﬁed from its deﬁnition,
here we focus on the third condition.

From the equation (13), the derivative of p (y, τ ; θ) with

respect to ϕ0 is

∂p(y,τ ;θ)
∂θ
and τ .
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

≤

T

τj (yj

j=2
X
T

−

ϕ0 −
σ2

ϕ1yj−1)

τjyj

(

|

|

+

|

ϕ0τj

+

|

|

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
ϕ1τj yj−1|
ϕ+
1

)

j=2
X
T

j=2(cid:26)(cid:18)
X

j + y2
τ 2
j
2

+ ϕ+

0 τj +

j−1 + τ 2
y2
j
2

(cid:0)

(cid:1)

(cid:19)(cid:27)

where θ∗ = arg max

θ∈Θ

(63)
p (y, τ ; θ) . The ﬁrst inequality follows

from the triangle inequality, and the second inequality follows
from p (y, τ ; θ∗)
p (y, τ ; θ),
1 , σ >
|
σ−, and the property of squares.

ϕ0|
The derivative with respect to ϕ1 is

< ϕ+
0 ,

< ϕ+

ϕ1|

≥

|

(cid:12)
(cid:12)
(cid:12)
(cid:12)
=

≤

≤

∂p (y, τ ; θ)
∂ϕ0

(cid:12)
(cid:12)
(cid:12)
p (y, τ ; θ)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
p (y, τ ; θ)
(cid:12)
σ2

p (y, τ ; θ∗)
(σ−)2

= gϕ0 (y, τ ) ,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
=

∂p (y, τ ; θ)
∂ϕ1

(cid:12)
(cid:12)
(cid:12)
p (y, τ ; θ)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

where the ﬁrst inequality follows from the triangle inequality,
< ϕ+
and the second inequality follows from
1 ,
σ > σ−, and the property of squares.
The derivative with respect to σ2 is

< ϕ+
0 ,

ϕ0|

ϕ1|

|

|

∂p (y, τ ; θ)
∂σ2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
= p (y, τ ; θ)
(cid:12)
(cid:12)

T

p (y, τ ; θ)

p (y, τ ; θ)

p (y, τ ; θ)

j=2 (cid:26)
X
T

j=2 (cid:26)
X
T

j=2 (cid:26)
X
T

j=2 (cid:26)
X
T

p (y, τ ; θ∗)

≤

≤

≤

≤

τj
2σ4 (yj

ϕ0 −

−

ϕ1yj−1)2

−

τj
2σ4 (yj

ϕ0 −

−

ϕ1yj−1)2 +

1
2σ2

1
2σ2

(cid:27)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:27)

τj
2σ4

τj
2σ4

ϕ0)2 + 2ϕ2

1y2

j−1

2 (yj

(cid:16)

−

4y2

j + 4ϕ2

0 + 2ϕ2

1y2

j−1

+

1
2σ2

(cid:27)

1
2σ2

(cid:27)

(cid:17)

+

(cid:1)

(cid:0)
τj
2 (σ−)2
1
2 (σ−)2

j=2(cid:26)
X
+

4y2

j + 4

ϕ+
0

2

+ 2

ϕ+
1

2

y2
j−1

(cid:0)

(cid:1)

(cid:0)

(cid:1)

(cid:17)

(cid:16)

(cid:27)

= gσ2 (y, τ ) ,

(65)
where the ﬁrst inequality follows from the triangle inequality,
the second and third inequalities follow from the property of
1 + x2
x2
, and the last inequality fol-
squares (x1 −
2
2
≤
lows from p (y, τ ; θ∗)
< ϕ+
< ϕ+
p (y, τ ; θ),
0 ,
1 ,
and σ > σ−.

x2)2

ϕ1|

ϕ0|

(cid:0)
≥

(cid:1)

|

|

The derivative with respect to ν is
∂p (y, τ ; θ)
∂ν

(cid:12)
(cid:12)
(cid:12)
(cid:12)
=

(cid:12)
(cid:12)
(cid:12)
p (y, τ ; θ)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

p (y, τ ; θ)

1
2

T

j=2
X
T

1
2

(cid:16)

≤

≤

p (y, τ ; θ∗)

j=2 (cid:26)(cid:12)
X
(cid:12)
(cid:12)
T
1
(cid:12)
2

j=2(cid:18)
X
1
+
2

1 + log

(cid:16)

1 + log

ν
2

(cid:17)
ν
2

(cid:16)

+

1
2

log

(cid:18)

+ log (τj )

−

τj

ν
2

Ψ

−

(cid:16)

(cid:17)
ν
2

Ψ

−

+

log (τj)

−

(cid:17)
ν−
2

(cid:17)(cid:12)
(cid:12)
(cid:12)
(cid:12)

Ψ

(cid:16)
1
2

(cid:18)

(cid:12)
(cid:12)
(cid:12)
ν−
(cid:12)
2

(cid:19)

−

(cid:19)

(cid:17)(cid:12)
(cid:12)
(cid:12)
(cid:12)

τj

(cid:27)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

τj

−

1
2

log (τj)

(cid:19)

(66)

1
σ2 τjyj−1 (yj

ϕ0 −

−

ϕ1yj−1)

T

j=2
X

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= gν (y, τ ) ,

·

where Ψ (
inequality
) is the digamma function. The ﬁrst
follows from the triangle inequality, and the second inequality
is due to that log
is positive and strictly decreasing
ν− [30].
for ν
(cid:0)

−

ν
2

ν
2

Ψ

(cid:1)

≥

(cid:0)
Based on Lemmas 3 and 4, we
gϕ0 (y, τ , ) dymdτ <
gσ2 (y, τ ) dymdτ <

,
∞
, and

(cid:1)

gϕ1 (y, τ ) dymdτ <
gν (y, τ ) dymdτ <

can obtain that
,
∞
.
∞

RR

∞

RR
The condition (M4) is veriﬁed.
RR
(M5) This condition requires the existence of the global
maximizer ˜θ (¯s) for Q (θ, ¯s) and its continuous differentiabil-
ity. Since Q (θ, ¯s) takes the same form with ˆQ
, the
maximizer will also take the same form. From (34)-(37), we
have

θ, ˆs(k)

RR

(cid:1)

(cid:0)

˜ϕ0 (¯s) =

˜ϕ1 (¯s) =

,

¯s5 −

˜ϕ1 (¯s) ¯s7
¯s3
¯s3¯s6 −
¯s3¯s4 −

¯s5¯s7
¯s2
7

,

¯s2 + ( ˜ϕ0 (¯s))2 ¯s3 + ( ˜ϕ1 (¯s))2 ¯s4 −
(cid:18)
2 ˜ϕ0 (¯s) ¯s6 + 2 ˜ϕ0 (¯s) ˜ϕ1 (¯s) ¯s7

−

˜ν (¯s) = arg max
ν−<ν<ν+

f (ν, ¯s1) ,

(67)

(68)

2 ˜ϕ0 (¯s) ¯s5

,
(cid:19)

(69)

(70)

(˜σ (¯s))2 =

1

−

1

T

and

where ¯si (i = 1, . . . 7) is the i-th component of ¯s. It can be
easily veriﬁed that ˜ϕ0 (¯s), ˜ϕ1 (¯s) and (˜σ (¯s))2 are continuous
functions of ¯s, and are 7 times differentiable with respect to
¯s. For ˜ν (¯s), the gradient of f (ν, ¯s1) at ˜ν

g (˜ν, ¯s1) =

∂f (ν, ¯s1)
∂ν

(cid:12)
(cid:12)
(cid:12)
˜ν
(cid:12)
(cid:12)
2

=

1
2
=0.

log

(cid:18)

(cid:18)

ν=˜ν

Ψ

˜ν
2

(cid:19)

(cid:18)

−

(cid:19)

+ 1 +

¯s1

T

1

(cid:19)

−

(71)

According to the implicit
g (˜ν, ¯s1) is 7 times continuously differentiable and ∂g(˜ν,¯s1)
1
2
continuously differentiable with respect to ¯s.

function theorem [38], since
∂ ˜ν =
= 0 for any ˜ν and ¯s1 [30], ˜ν (s) is 7 times

1
˜ν −

2 Ψ′

˜ν
2

1

(cid:0)

(cid:0)

(cid:1)(cid:1)

B. Proof of (SAEM2) and (SAEM3)

(cid:9)

(cid:8)

ˆs(k)

The condition (SAEM2) has been veriﬁed in the proof of the
conditions (M4) and (M5). The condition (SAEM3.1) holds
due to the compactness assumption of the chain in the theorem.
The functions s (yo, ym, τ ) and
are continuous func-
tion of the chain, therefore, they also take values in a compact
set according to the boundness theorem, which implies the
condition (SAEM3.2) hold. Now we focus on the proof of the
conditions (SAEM3.3) and (SAEM3.4).
deﬁnition

From the
probability
Πθ (ym, τ , y′
in (24), we can easily verify that
the transition probability Πθ (ym, τ , y′
m, τ ′) is continuously
differentiable with respect
In addition, since the
derivative is a continuous
V and
(ym, τ , y′
Ω2, where V and Ω2 are compact set,
according to the boundness theorem, the derivative is bounded.

function of θ

transition

m, τ ′)

m, τ ′)

to θ.

the

of

∈

∈

13

Therefore, Πθ (ym, τ , y′
for any (ym, τ , y′
m, τ ′)
K (ym, τ , y′

m, τ ′) is Lipschitz continuous, i.e.,
Ω2, there exists a real constant

m, τ ′) such that for any

θ, θ′

∈

V 2,

∈
m, τ ′)
Πθ′ (ym, τ , y′

(cid:0)

(cid:1)

Πθ (ym, τ , y′

K (ym, τ , y′

m, τ ′)
−
m, τ ′)

(cid:12)
(cid:12)
≤
(cid:12)

θ

θ′

.

|

−

|

(72)

(cid:12)
(cid:12)
(cid:12)

It follows that

sup
(ym,τ ,y′
m,τ ′)∈Ω2
θ′
θ
L

−

|

Πθ (ym, τ , y′

m, τ ′)

(cid:12)
(cid:12)
(cid:12)

−

|

≤
with L =

K (ym, τ , y′
the condition (SAEM3.3) is veriﬁed.

m,τ ′)∈Ω2

(ym,τ ,y′

max

Πθ′ (ym, τ , y′

m, τ ′)

(cid:12)
(cid:12)
(cid:12)
(73)
m, τ ′), which implies that

The condition (SAEM3.4) is about the uniform ergodicity
of the Markov chain generated by the transition probabil-
ity Πθ (ym, τ , y′
m, τ ′). According to Theorem 8 in [39], a
Markov chain is uniformly ergodic, if the transition proba-
bility satisﬁes some minorization condition, i.e., there exists
) such that
α
m, τ ′)
ǫδ (y′
Πα
∈
m, τ ′) is a
Ω2. Recall our transition probability Πθ (ym, τ , y′
continuous function for (ym, τ )
Ω, according to the extreme
m, τ ′, θ) =
value theorem, there must exist an inﬁmum g (y′

N + and some probability measure δ (
·

m, τ ′) for any (ym, τ , y′

θ (ym, τ , y′

m, τ ′)

≥

∈

∈
m, τ ′) . It follows that

inf
(ym,τ )∈Ω

Πθ (ym, τ , y′

≥

Πθ (ym, τ , y′
g (y′

m, τ ′)
m, τ ′, θ) dτ ′dy′

m, τ ′)
ǫδ (y′
m, and δ (y′

m, τ ′) =
m, τ ′, θ). Therefore, the minorization condition holds
the Markov chain generated by
is uniformly ergodic. The condition

with ǫ =
ǫ−1g (y′
in our case, and thus,
Πθ (ym, τ , y′
(SAEM3.4) is veriﬁed.

RR
m, τ ′)

(74)

REFERENCES

[1] M. K. Choong, M. Charbit, and H. Yan, “Autoregressive-model-based
missing value estimation for DNA microarray time series data,” IEEE
Trans. Inf. Technol. Biomed., vol. 13, no. 1, pp. 131–137, 2009.

[2] A. Schlögl and G. Supp, “Analyzing event-related EEG data with
multivariate autoregressive parameters,” Prog. Brain Res., vol. 159, pp.
135–147, 2006.

[3] R. S. Tsay, Analysis of Financial Time Series, 2nd ed. Hoboken, NJ:

John Wiley & Sons, 2005.

[4] S. C. Anderson, T. A. Branch, A. B. Cooper, and N. K. Dulvy, “Black-
swan events in animal populations,” Proc. Natl. Acad. Sci., vol. 114,
no. 12, pp. 3252–3257, 2017.

[5] S. T. Rachev, Handbook of Heavy Tailed Distributions in Finance:
Handbooks in Finance. Amsterdam, Netherlands: Elsevier, 2003.
[6] D. Alexander, G. Barker, and S. Arridge, “Detection and modeling of
non-gaussian apparent diffusion coefﬁcient proﬁles in human brain data,”
Magn. Reson. Med., vol. 48, no. 2, pp. 331–340, 2002.

[7] F. Han and H. Liu, “Eca: High-dimensional elliptical component analysis
in non-gaussian distributions,” J. Am. Stat. Assoc., vol. 113, no. 521, pp.
252–268, 2018.

[8] K. L. Lange, R. J. Little, and J. M. Taylor, “Robust statistical modeling
using the t distribution,” J. Am. Stat. Assoc., vol. 84, no. 408, pp. 881–
896, 1989.

[9] M. L. Tiku, W.-K. Wong, D. C. Vaughan, and G. Bian, “Time series
models in non-normal situations: Symmetric innovations,” J. Time Ser.
Anal., vol. 21, no. 5, pp. 571–596, 2000.

[10] B. Tarami and M. Pourahmadi, “Multi-variate t autoregressions: Innova-
tions, prediction variances and exact likelihood equations,” J. Time Ser.
Anal., vol. 24, no. 6, pp. 739–754, 2003.

[11] U. C. Nduka, “EM-based algorithms for autoregressive models with t-
distributed innovations,” Commun. Stat. Simul. and Comput., vol. 47,
no. 1, pp. 206–228, 2018.

6
14

[12] J. Christmas and R. Everson, “Robust autoregression: Student-t inno-
vations using variational Bayes,” IEEE Trans. Signal Process., vol. 59,
no. 1, pp. 48–57, 2011.

[13] R. J. Little and D. B. Rubin, Statistical Analysis with Missing Data,

2nd ed. Hoboken, N.J.: John Wiley & Sons, 2002.

[14] G. DiCesare, “Imputation, estimation and missing data in ﬁnance,” PhD

thesis, University of Waterloo, Canada, 2006.

[15] J. Ding, L. Han, and X. Chen, “Time series AR modeling with missing
observations based on the polynomial transformation,” Math. Comput.
Modelling, vol. 51, no. 5-6, pp. 527–536, 2010.

[16] V. A. V. Yuriy S. Kharin, “Robust estimation of AR coefﬁcients under
simultaneously inﬂuencing outliers and missing values,” J. Stat. Plan.
Inference, vol. 141, no. 9, pp. 3276–3288, 2011.

[17] J. Sargan and E. Drettakis, “Missing data in an autoregressive model,”

Int. Econ. Rev., vol. 15, no. 1, pp. 39–58, 1974.

[18] B. Delyon, M. Lavielle, and E. Moulines, “Convergence of a stochastic
approximation version of the EM algorithm,” Ann. Stat., vol. 27, no. 1,
pp. 94–128, 1999.

[19] E. Kuhn and M. Lavielle, “Coupling a stochastic approximation version
of EM with an MCMC procedure,” ESAIM Probab. and Statist., vol. 8,
pp. 115–131, 2004.

[20] S. F. Nielsen et al., “The stochastic EM algorithm: estimation and
asymptotic results,” Bernoulli, vol. 6, no. 3, pp. 457–489, 2000.
[21] X.-L. Meng and D. B. Rubin, “Maximum likelihood estimation via the
ECM algorithm: A general framework,” Biometrika, vol. 80, no. 2, pp.
267–278, 1993.

[22] X. Yi and C. Caramanis, “Regularized EM algorithms: A uniﬁed
framework and statistical guarantees,” in Proc. of Adv. Neural Inf.
Process. Syst., 2015, pp. 1567–1575.

[23] A. P. Dempster, N. M. Laird, and D. B. Rubin, “Maximum likelihood
from incomplete data via the EM algorithm,” J. R. Stat. Soc. Series B
Stat. Methodol., vol. 39, no. 1, pp. 1–38, 1977.

[24] C. J. Wu, “On the convergence properties of the EM algorithm,” Ann.

Stat., vol. 11, no. 1, pp. 95–103, 1983.

[25] Y. Sun, P. Babu, and D. P. Palomar, “Majorization-minimization algo-
rithms in signal processing, communications, and machine learning,”
IEEE Trans. Signal Process., vol. 65, no. 3, pp. 794–816, 2017.
[26] G. C. Wei and M. A. Tanner, “A Monte Carlo implementation of the
EM algorithm and the poor man’s data augmentation algorithms,” J. Am.
Stat. Assoc., vol. 85, no. 411, pp. 699–704, 1990.

[27] E. Kuhn and M. Lavielle, “Maximum likelihood estimation in nonlinear
mixed effects models,” Comput. Stat. Data Anal., vol. 49, no. 4, pp.
1020–1038, 2005.

[28] C. Liu, “ML estimation of the multivariate t distribution and the EM
algorithm,” J. Multivar. Anal., vol. 63, no. 2, pp. 296–312, 1997.
[29] A. DasGupta, The Exponential Family and Statistical Applications. New

York, NY: Springer New York, 2011, pp. 583–612.

[30] C. Liu and D. B. Rubin, “ML estimation of the t distribution using EM
and its extensions, ECM and ECME,” Stat. Sin., vol. 5, no. 1, pp. 19–39,
1995.

[31] B. Carnahan and H. A. Luther, Applied Numerical Methods. New York:

John Wiley, 1969.

[32] K. Chan and J. Ledolter, “Monte Carlo EM estimation for time series
models involving counts,” J. Am. Stat. Assoc., vol. 90, no. 429, pp. 242–
252, 1995.

[33] M. G. Gu and F. H. Kong, “A stochastic approximation algorithm
with Markov chain Monte-Carlo method for incomplete data estimation
problems,” Proc. .Natl. Acad. Sci., vol. 95, no. 13, pp. 7270–7274, 1998.
[34] G. Fort, E. Moulines et al., “Convergence of the Monte Carlo expectation
maximization for curved exponential families,” Ann. Stat., vol. 31, no. 4,
pp. 1220–1259, 2003.

[35] R. C. Neath et al., “On convergence properties of the Monte Carlo EM
Inst. Math. Stat.,

algorithm,” in Proc. Adv. Modern Stat. Theory Appl.
2013, pp. 43–62.

[36] R. Maronna, R. D. Martin, and V. Yohai, Robust Statistics: Theory and
Methods. New York: Wiley, 2006, ch. Time Series, pp. 247–323.
[37] C. Caroni and V. Karioti, “Detecting an innovative outlier in a set of
time series,” Comput. Stat. Data Anal., vol. 46, no. 3, pp. 561–570,
2004.

[38] S. G. Krantz and H. R. Parks, Introduction to the Implicit Function
Theorem. New York, NY: Springer New York, 2013, pp. 1–12.
[39] G. O. Roberts, J. S. Rosenthal et al., “General state space Markov chains
and MCMC algorithms,” Probab. Surv., vol. 1, pp. 20–71, 2004.

9
1
0
2

b
e
F
9

]
P
A

.
t
a
t
s
[

2
v
3
0
2
7
0
.
9
0
8
1
:
v
i
X
r
a

Supplementary Material

Junyan Liu, Sandeep Kumar, and Daniel P. Palomar

February 12, 2019

In this supplementary material, we give detailed proof for the Lemmas 3 and 4:

Lemma 3 For any yo and θ

∈

Θ, p (yo; θ) =

p (y, τ ; θ) dymdτ =

p (y; θ) dym <

.
∞

Lemma 4 For any yo , θ

Θ and 1 < t

∈

R R

T

≤
g (y, τ ) p (y, τ ; θ) dymdτ <

where g (y, τ ) can be τt, τ 2

t , y2

Z Z
t ,τty2
t
−

1, τty2

t , or

log (τt).

−

R

,
∞

(1)

To establish these lemmas, we ﬁrst introduce some equations and inequalities in the ﬁrst section. They

are the key ingredients for the proof. Then we establish Lemma 3 and Lemma 4 in the second and third

sections, respectively. For simplicity of notations, we use ft (yj; yj
fN (yj; yj

1, τj) to denote fN

yj; ϕ0 + ϕ1yj

−
, and fg (τj) to denote fg

1) to denote ft

yj; ϕ0 + ϕ1yj
τj; ν
2 , ν
.
(cid:0)

2

1, σ2, ν

,

−

(cid:1)

1, σ2
τj

−

−

1 Ingredients

(cid:16)

(cid:17)

(cid:0)

(cid:1)

Recall that, given ϕ0, ϕ1, σ2, ν, and yj
t

ϕ0 + ϕ1yj

1, σ2, ν

∼
. Based on properties of the Student’s t-distribution, we can get the following

1, the variable yj follows a Student’s t-distribution: yj

−

−

equations and inequality about the variable yj [1]:

(cid:0)

(cid:1)

1. The integral of the pdf should be 1:

ft (yj; yj

−

1) dyj = 1.

Z

2. The ﬁrst raw moment can be expressed as

3. The second raw moment can be expressed as

yjft (yj; yj

−

1) dyj = ϕ0 + ϕ1yj

1.

−

Z

y2
j ft (yj; yj

1) dyj =

−

Z

νσ2
2
ν

−

+ (ϕ0 + ϕ1yj

1)2 .

−

(2)

(3)

(4)

4. Since the Student’s t-distribution can be represented as a Gaussian mixture [2], the pdf can be

rewritten as

ft (yj; yj

1) =

−

Z

fg (τj) fN (yj; yj

1, τj) dτj.

−

(5)

5. For ν > ν−

≥

2, the pdf of yj can be bounded as

ft (yj; yj

1) =

−

ν+1
Γ
2
√νπσΓ
(cid:0)

ν
(cid:1)
2
(cid:0)

(cid:1)

(yt

−

1 +

1

ϕ1yt

−

ϕ0 −
νσ2

ν+1
2

−

1)2

!

≤

ν+1
Γ
2
√νπσΓ
(cid:0)

ν
(cid:1)
2
(cid:0)

(cid:1)

<

.
∞

(6)

 
 
 
 
 
 
 
Then we introduce two important inequalities about τj, which we will use later.

1. The ﬁrst is about the expectation of τ b

j with b = 1, 2:

τ b
j fg (τj) fN (yj; yj

1, τj) dτj

−

ν
2

ν
2
√2πσ2
(cid:1)

Γ

ν
(cid:0)
2

ν+2b−1
2

τ
t

Z

=

=

Z

Γ

ν
ν
(cid:1)
(cid:0)
2
2
√2πσ2
(cid:1)

ν
(cid:0)
2

(cid:1)

(cid:0)
= ft (yj; yj

1)

−

(yj

−

(cid:16)

Γ

ν+1
2

ft (yj; yj

1)

−

≤

2bΓ
(cid:0)
νbΓ
(cid:0)

(cid:1) (cid:16)
ν+2b+1
2
ν+1
2

,

(cid:1)

(yt

−

ϕ1yt

−

ϕ0 −
2σ2

exp

 −  
ν+2b+1
2

Γ

ϕ1yj−1)2

(cid:0)

ϕ0−
2σ2

ν+2b+1
2

(cid:1)
+ ν
2

(cid:17)

Γ

(yj

ν+2b+1
2
ϕ0−
2σ2

(cid:0)
−

ϕ1yj−1)2
(cid:1)

+ ν
2

b

(cid:17)

1)2

+

ν
2 !

τt

!

dτj

(7a)

(7b)

(7c)

(7d)

where the equations (7a) and (7c) follow from the deﬁnition of these pdf’s, the equation (7b) follows
βx) dx = 1(the integral of the pdf of the gamma distribution is 1), the last

1 exp (

from

βα
Γ(α) xα

−

(cid:0)

(cid:1)

inequality (7d) follows from

R

(yj

−

ϕ1yj−1)2

ϕ0−
2σ2

+ ν
2

−

(cid:1)
2. The second inequality is about the expectation of log (τj),

(cid:0)

(cid:16)

b

≥

b

.

ν
2

(cid:17)

log (τj ) fg (τj ) fN (yj; yj

1, τj) dτj

−

Z

=

=

Z

Γ

ν
2

ν
2
√2πσ2
(cid:1)

Γ

ν
(cid:0)
2

log (τj) τ
t

ν−1
2

exp

(yt

−

 −  

ϕ1yt

ϕ0 −
2σ2

−

1)2

+

ν
2 !

ν
ν
(cid:1)
(cid:0)
2
2
√2πσ2
(cid:1)

ν
(cid:0)
2

(cid:0)

(cid:1)

(yj

−

(cid:16)

Γ

ν+1
2

ϕ1yj−1)2
(cid:1)

(cid:0)

ϕ0−
2σ2

Ψ

ν+1
2  

(cid:18)

ν + 1
2

−

(cid:19)

log

(yj

−

+ ν
2

(cid:17)

τt

dτj

!
ϕ0 −
2σ2

ϕ1yj

=

Ψ

Ψ

Ψ

Ψ

≥  

≥  

≥  

ν + 1
2

ν + 1
2

ν + 1
2

ν + 1
2

(cid:18)

(cid:18)

(cid:18)

(cid:18)

−

(cid:19)

−

(cid:19)

−

(cid:19)

−

(cid:19)

log

(yj

(yj

(yj

−

ϕ1yj

ϕ0 −
2σ2

1)2

−

+

ν
2 !!

ft (yj; yj

1)

−

−

−

ϕ1yj

ϕ0 −
2σ2
ϕ0)2 + ϕ2
σ2

1)2

−

1y2
j
−

1

ν
2 !

ν
2 !

−

−

ft (yj; yj

1)

−

ft (yj; yj

1)

−

2y2

j + 2ϕ2
0 + ϕ2
σ2

1y2
j
−

1

−

ν
2 !

ft (yj; yj

1) ,

−

(8a)

1)2

−

+

ν
2 !!

(8b)

(8c)

(8d)

(8e)

(8f)

where the equations (8a) and (8c) follow from the deﬁnition of these pdf’s, the equation (8b)

follows from

log (x) xα

−

1 exp (

βx) dx = Γ(α)

βα (Ψ (α)

−

x, the inequalities (8e) and (8f) follow from (x1 + x2)2

−

log (β)) [3], the inequality (8d) follows
2x2

1 + 2x2
2.

≤

from

log (x)
R
≥ −

−

2

 
 
 
2 Proof for Lemma 3

Lemma 3 is about the boundedness of the marginal pdf p (yo; θ). The pdf can be written as

p (yo; θ) =

p (y; θ) dym

Z

Z

T

j=2
Y
D

ft (yj; yj

1) dym

−

td+1

D

td+nd+1

ft (yj; yj

1)



−



d=0
Y

j=td+nd+2
Y

d=1 Z
Y

j=td+1
Y





=

=





where we move the term that does not involve ym outside the integral.

(9)

ft (yj; yj

1) dyd

−

,





Since the pdf of the Student’s t-distribution ft (yj; yj

term c1 (yo, θ) =
∞
to establish the boundedness of the second term, i.e.,

td+1
j=td+nd+2 ft (yj; yj

D
d=0

1) <

−

Q

Q

1) is positive and bounded, we can get the ﬁrst
. Thus, in order to establish Lemma 3, it is suﬃcient

−

td+nd+1

Z

j=td+1
Y

ft

yj; yj

−

1, σ2, ν

dyd <

.
∞

(cid:0)

(cid:1)

(10)

Before carrying out a general proof for the above inequality (10), we demonstrate the schematic and
intuition with a simple example. We consider a time series as follows: y1, y2, NA, NA, NA, y6, y7. The
corresponding second term (10) in this example can be expressed as

ft (y6; y5) ft (y5; y4) dy5

ft (y4; y3) dy4

ft (y3; y2) dy3

(11a)

Z (cid:26)Z (cid:18)Z

(cid:19)

(cid:27)

ft (y5; y4) dy5

ft (y4; y3) dy4

)

!

ft (y3; y2) dy3

(11b)

6

Z

j=3
Y

ft (yj; yj

−

1) dy1 =

≤

=

=

=

ν
(cid:1)
2
(cid:0)
ft (y4; y3) dy4

(cid:1)

ν
(cid:1)
2
(cid:0)
ft (y3; y2) dy3

(cid:1)

ν+1
Γ
2
√νπσΓ
Z (Z  Z
(cid:0)
ν+1
Γ
2
√νπσΓ
Z (Z
(cid:0)
ν+1
Γ
2
√νπσΓ
(cid:0)
ν+1
Γ
2
√νπσΓ
(cid:0)

ν
(cid:1)
2
(cid:0)

Z

(cid:1)

ν
(cid:1)
2
(cid:0)

(cid:1)

<

,
∞

ft (y3; y2) dy3

)

(11c)

(11d)

(11e)

(11f)

Γ( ν+1
2 )
√νπσΓ( ν

2 )
Γ( ν+1
2 )
√νπσΓ( ν

where the inequality (11b) follows from (6), the equations (11c)-(11e) hold from (2), and the inequality

(11f) follows from the boundness theorem. By applying (6), we ﬁnd a upper bound

Γ( ν+1
2 )
√νπσΓ( ν
ft (y5; y4) dy5, is easy to compute since

for ft (y6; y5),

2 )

which does not involve y5. Then the integral about y5,

R

ft (y5; y4) dy5 = 1 from (2), and the result is a function
Γ( ν+1
2 )
√νπσΓ( ν

R
the integral about y4,

2 )
ft (y4; y3) dy4, also becomes simple, and the result does not involve y3.

, which does not involve y4. Next,

2 )

Finally, we can get that the integral about y3 equals to a continuous function

R

Γ( ν+1
2 )
√νπσΓ( ν

2 )

. According to the

boundness theorem, a continuous function on a closed bounded set is bounded [4], therefore,

is bounded.

Γ( ν+1
2 )
√νπσΓ( ν

2 )

3

Now we come to the general proof for the inequality (10). The idea is the same as in the example:

td+nd+1

Z

j=td+1
Y

ft (yj; yj

−

1) dyd =

· · ·

Z Z

Z

ft (ytd+nd+1; ytd+nd ) ft (ytd+nd ; ytd+nd

1) dytd+nd

−

(ytd+nd

1; ytd+nd

−
ν+1
Γ
2
√νπσΓ
(cid:0)

ν
(cid:1)
2
(cid:0)

≤

Z

· · ·

Z (Z
(ytd+nd

−
ν+1
Γ
2
√νπσΓ
(cid:0)

ν
(cid:1)
2
(cid:0)

=

=

· · ·

Z

Z

Γ

ν+1
2
√νπσΓ
(cid:0)

<

,
∞

ν
(cid:1)
2
(cid:0)

(cid:1)

2) dytd+nd

−

−

1 . . . ft (ytd+1; ytd ) dytd+1

(12a)

ft (ytd+nd ; ytd+nd

1) dytd+nd

−

)

(cid:1)
1; ytd+nd
−

2) dytd+nd

−

1 . . . ft (ytd+1; ytd ) dytd+1

(12b)

(ytd+nd

−

1; ytd+nd

−

2) dytd+nd

−

1 . . . ft (ytd+1; ytd) dytd+1

(cid:1)

(12c)

(12d)

(12e)

where the inequality (12b) follows from (6), the equations (12b) and (12c) hold from (2), and the inequality
(12e) follows from the boundness theorem. Therefore, p (yo; θ) <

. Lemma 3 is proved.

∞

3 Proof for Lemma 4

Lemma 4 is about the boundedness of the expectation of g (y, τ ). For convenience of proof, we divide the
diﬀerent cases of g (y, τ ) into four groups:(1) g (y, τ ) = τt or τ 2
1 or
τty2

t , (3) g (y, τ )=τty2
t
−
log (τt). We will prove that the inequality (1) is satisﬁed for these groups one by one.

t , (2) g (y, τ ) = y2

t , and (4)

−

3.1

g (y, τ ) = τt or τ 2
t

For g (y, τ ) = τ b

t with b = 1, 2, we have

g (y, τ ) p (y, τ ; θ) dymdτ

τ b
t p (τ ; θ) p (y

|

τ ; θ) dymdτ

fg (τj) fN (yj; yj
{

1, τj)

−

}

dymdτ

j=2
Y
τ b
t fg (τt) fN (yt; yt

−

1, τj) dτt

fg (τj) fN (yj; yj

=t (cid:18)Z
j
Y

ft (yt; yt

−

1) dym

ft (yt; yt

−

1)

(cid:1)

=t
j
Y

p (yo; θ)

Z Z
=

=

=

≤

=

Z Z

Z Z

T

τ b
t

Z (Z
2Γ

ν+2b+1
2
ν+1
2

νΓ
(cid:0)
ν+2b+1
(cid:0)
2
ν+1
2

(cid:1)

(cid:1)

Z
2bΓ

νbΓ
(cid:0)

<

.
∞

(cid:0)

(cid:1)

1, τj) dτj

−

dym

(cid:19))

(13a)

(13b)

(13c)

(13d)

(13e)

(13f)

In (13c), we split the integral of

into two parts: the ﬁrst part involves τt, while the second does not.
In (13d), we apply the inequality (7d) to the ﬁrst part of (13c). The inequality (13f) follows from Lemma

τj

}

{

3 and the boundness theorem.

4

6
6
(14)

(15a)

(15b)

(15c)

(15d)

3.2

g (y, τ ) = y2
t

For g (y, τ ) = y2
observed, then we can easily get

t , we need to consider two diﬀerent cases: yt is observed, and yt is missing.

If yt is

g (y, τ ) p (y, τ ; θ) dymdτ

Z Z
=

t p (y, τ ; θ) dymdτ
y2

Z Z

y2
t

=

Z
= y2
t

p (y, τ ; θ) dτ

dym

(cid:26)Z
p (y; θ) dym

(cid:27)

Z

= y2

t p (yo; θ)

<

.
∞

where the last inequality holds from Lemma 3 and the fact that yo is ﬁnite.

If yt is missing, assume that yt is in the d1-th missing block with t = td1 + i, we have

g (y, τ ) p (y, τ ; θ) dymdτ

Z Z
=

t p (y, τ ; θ) dymdτ
y2

=

=

=

=

Z Z

y2
t

p (y, τ ; θ) dτ

dym

Z

Z
D

(cid:26)Z
t p (y; θ) dym
y2

(cid:27)

td+1

D

td+nd+1

ft (yj; yj

1)

−

ft (yj; yj

1)

−

d=0
Y
D

j=td+nd+2
Y
td+1

d=0
Y

j=td+nd+2
Y

y2
t

d=1
Y

j=td+1
Y
td+nd+1

=d1 Z
d
Y

j=td+1
Y

Z






ft (yj; yj

1) dyd

−

ft (yj; yj

−

1) dyd





Z

y2
td1 +i

td1 +nd1 +1

j=td1 +1
Y

ft (yj; yj

−





.

1) dyd1

(15e)


In (15d), we move the term that does not involve ym =

integral of

yd
{

}
Since the pdf ft (yj; yj

yd
{
into two parts: the ﬁrst part does not involve ytd1 +i, while the second part does.
1) is bounded from (6), the ﬁrst term of (15e) is bounded:

outside the integral. In (15e), we split the

}

−

D

td+1

d=0
Y

j=td+nd+2
Y

ft (yj; yj

1) <

−

.
∞

In addition, from (12d), the second term is also bounded:

td+nd+1

=d1 Z
d
Y

j=td+1
Y

ft (yj; yj

−

1) dyd <

.
∞

(16)

(17)

Thus, in order to prove that (15e) is bounded, it is suﬃcient to establish the boundedness of the third

term, i.e.,

y2
td1 +i

Z

td1 +nd1 +1

j=td1 +1
Y

ft (yj; yj

−

1) dyd1 <

.
∞

(18)

Before carrying out a general proof for the inequality (18), we demonstrate the schematic and intuition
with a simple example. Again, we consider the time series as follows: y1, y2, NA, NA, NA, y6, y7. Then we

5

6
6
have

Z

=

≤

=

=

=

=

=

<

,
∞

6

y2
4

j=3
Y

ft (yj; yj

1) dy1

−

ft (y6; y5) ft (y5; y4) dy5

4ft (y4; y3) dy4
y2

ft (y3; y2) dy3

(cid:27)
ft (y5; y4) dy5

(cid:27)
4ft (y4; y3) dy4
y2

!

ft (y3; y2) dy3

)

ν
(cid:1)
2
(cid:0)
4ft (y4; y3) dy4
y2

(cid:1)

ft (y3; y2) dy3

)

+ (ϕ0 + ϕ1y3)2

ft (y3; y2) dy3

(cid:19)

+ ϕ2

0 + 2ϕ0ϕ1y3 + ϕ2

1y2
3

ft (y3; y2) dy3

(cid:19)

Z (cid:26)Z (cid:26)Z

ν
(cid:1)
2
(cid:0)

ν+1
Γ
2
√νπσΓ
Z (Z  Z
(cid:0)
ν+1
Γ
2
√νπσΓ
Z (Z
(cid:0)
ν+1
Γ
2
ν
√νπσΓ
(cid:0)
(cid:1)
2
ν+1
Γ
(cid:0)
2
√νπσΓ
(cid:0)
ν+1
Γ
2
√νπσΓ
(cid:0)

(cid:18)
νσ2
(cid:1)
2
ν

ν
(cid:1)
2
(cid:0)

(cid:18)

Z

Z

(cid:1)

(cid:18)

−

νσ2
(cid:1)
2
ν

−
νσ2
2
ν

−

ν
(cid:1)
2
(cid:0)
ν+1
Γ
2
√νπσΓ
(cid:0)

ν
(cid:1)
2
(cid:0)

+ ϕ2
0

+

(cid:19)

(cid:1)

(cid:1)

νσ2
2
ν

−

(cid:18)

+ ϕ2
0

+

(cid:19)

ν+1
Γ
2
√νπσΓ
(cid:0)

ν
(cid:1)
2
(cid:0)
ν+1
Γ
2
√νπσΓ
(cid:0)

ν
(cid:1)
2
(cid:0)

(cid:1)

(cid:1)

2ϕ0ϕ1

y3ft (y3; y2) dy3 +

Z

ν+1
Γ
2
√νπσΓ
(cid:0)

ν
(cid:1)
2
(cid:0)

2ϕ0ϕ1 (ϕ0 + ϕ1y2) +

ν+1
Γ
2
√νπσΓ
(cid:0)

ν
(cid:1)
2
(cid:0)

ϕ2
1

(cid:18)

(cid:1)

(19a)

(19b)

(19c)

(19d)

(19e)

ϕ2
1

3ft (y3; y2) dy3
y2

Z

(cid:1)
νσ2
2
ν

−

(19f)

+ (ϕ0 + ϕ1y2)2

(cid:19)

(19g)

(19h)

where the inequality (19b) follows from (6), the equations (19c)-(19g) follow from (2)-(4), and the in-

equality (19h) holds due to the boundness theorem. By applying (6), we ﬁnd an upper bound

for ft (y6; y5), which does not involve y5, so that the integral about y5,

Γ( ν+1
2 )
√νπσΓ( ν

2 )

and equals to

Γ( ν+1
2 )
√νπσΓ( ν
simple and the ﬁnal result is bounded.

2 )

, which does not involve y4. Then the following integrals about y4 and y3 are also

R

Now we come to the general proof for the inequality (18). The idea is the same as in the above

example:

Γ( ν+1
2 )
√νπσΓ( ν

2 )
ft (y5; y4) dy5, is simple

y2
td1 +i

td1 +nd1 +1

j=td1 +1
Y

ft (yj; yj

1) dyd1

−

Z

=

· · ·

Z

Z

· · ·

Z (cid:26)Z
ft

; ytd1 +nd1 −

1

dytd1 +nd1

ft

ytd1 +nd1
(cid:0)
(cid:1)
dytd1 +nd1 −
dytd1 +i . . . ft
(cid:1)

1 . . .

ytd1 +1; ytd1
(cid:0)
; ytd1 +nd1 −
1
dytd1 +nd1 −
dytd1 +i . . . ft
(cid:1)

dytd1 +nd1

(cid:1)
1 . . .

(cid:1)

dytd1 +1

(cid:1)

(cid:27)

ytd1 +1; ytd1
(cid:0)

dytd1 +nd1 −

dytd1 +1

(cid:1)
1 . . .

1; ytd1 +nd1 −

2

2

1

ft

ft

1; ytd1 +nd1 −

ytd1 +nd1 +1; ytd1 +nd1
(cid:0)
ytd1 +nd1 −
y2
ytd1 +i; ytd1 +i
td1 +ift
(cid:0)
−
ν+1
Γ
(cid:1)
(cid:0)
2
ytd1 +nd1
ν
√νπσΓ
(cid:0)
(cid:1)
2
(cid:0)
1; ytd1 +nd1 −
ytd1 +nd1 −
(cid:0)
y2
td1 +ift
(cid:0)
Γ

ytd1 +i; ytd1 +i
−
ν+1
(cid:0)
2
√νπσΓ
(cid:0)

(cid:1)
ytd1 +nd1 −
ν
(cid:1)
2
(cid:0)
ytd1 +i; ytd1 +i
(cid:0)
−
(cid:0)

ft

(cid:1)

(cid:1)

(cid:1)

1

2

1

dytd1 +i . . . ft

(cid:1)
ytd1 +1; ytd1
(cid:0)

(cid:1)

dytd1 +1

6

(cid:27)

(20a)

(20b)

(20c)

≤

Z

· · ·

Z

· · ·

Z (cid:26)Z
ft

=

· · ·

Z

Z

· · ·

Z
y2
td1 +ift

=

=

· · ·

· · ·

Z

Z

Z

Z

Γ

ν+1
2
ν
√νπσΓ
(cid:0)
(cid:1)
2
ν+1
Γ
(cid:0)
2
√νπσΓ
(cid:0)

ν
(cid:1)
2
(cid:0)

(cid:1)

y2
td1 +ift

(cid:1)

νσ2
2
ν

−

(cid:18)

1

ytd1 +i; ytd1 +i
−
(cid:0)
+

(cid:1)
ϕ0 + ϕ1ytd1 +i
−
(cid:0)

2

1

(cid:19)

(cid:1)

dytd1 +i . . . ft

ytd1 +1; ytd1
(cid:0)
dytd1 +i

1 . . . ft

−

dytd1 +1

(cid:1)

ytd1 +1; ytd1
(cid:0)

(cid:1)

(20d)

dytd1 +1

(20e)

where the inequality (20b) follows from (6), the equations (20c) and (20d) hold from (2), and the equation

follows from (4). Similar to the example, this integral of (20e) will ﬁnally reduce to a quadratic function
of ytd1

. Then, according to the boundness theorem, we can obtain

y2
td1 +i

Z

td1 +nd1 +1

j=td1 +1
Y

ft (yj; yj

−

1) dyd1 <

,
∞

(21)

and thus,

t p (y, τ ; θ) dymdτ <
y2

.
∞

3.3

RR

g (y, τ )=τty2

t−1

or τty2

t

In this subsection, we consider the cases of g (y, τ )=τty2
t , the case τty2
case of g (y, τ ) = τty2
t
−

t or τty2
t
−
1 can be veriﬁed similarly. For g (y, τ ) = τty2

t , we have

1. Here we only present proof for the

g (y, τ ) p (y, τ ; θ) dymdτ

Z Z
=

τty2

t p (y, τ ; θ) dymdτ

Z Z

Z Z

τty2
t

T

{

j=2
Y

fN (yj; yj

1, τj) fg (τj)
}

−

dymdτ

=

=

≤

=

Z

y2
t

y2
t

(Z
2Γ
νΓ

Z
2Γ
νΓ

(cid:0)
(cid:0)

(cid:1)
(cid:1)

ν+3
2
ν+1
2

(cid:0)
(cid:0)

ν+3
2
ν+1
2

Z

<

.
∞

τtfN (yt; yt

−

1, τt) fg (τt) dτt

fN (yj; yj

−

1, τj) fg (τj) dτj

dym

(cid:19))

=t (cid:18)Z
j
Y

ft (yt; yt

1)

ft (yj; yj

1) dym

−

−

(cid:1)
(cid:1)
t p (yo, ym; θ) dym
y2

=t
j
Y

In (22c), we split the integral of τ =

into two parts: the ﬁrst part involves τt, while the second does
not. The inequality (22d) holds from (7d). The inequality (22f) follows from the result of last subsection

τj
{

}

and the boundness theorem.

3.4

g (y, τ ) =

log (τt)

−

Finally, we consider the case of g (y, τ ) =

log (τt):

−

g (y, τ ) p (y, τ ; θ) dymdτ

log (τt) p (y, τ ; θ) dymdτ

log (τt)

−

Z Z

T

j=2
Y

fN (yj; yj
{

1, τj) fg (τj)
}

−

dymdτ

Z Z
=

−

Z Z

=

=

log (τt) fN (yt; yt

−

1, τt) fg (τt) dτt

−

Z (Z

fN (yj; yj

−

1, τj ) fg (τj) dτj

dym

(cid:19))

=t (cid:18)Z
j
Y

7

(22a)

(22b)

(22c)

(22d)

(22e)

(22f)

(23a)

(23b)

(23c)

6
6
6
Ψ

ν + 1
2

2y2

t + 2ϕ2
0 + ϕ2
σ2

1y2
t
−

1

ν
2

−

−

Z (cid:18)

2y2

(cid:18)
(cid:19)
t + 2ϕ2
0 + ϕ2
σ2

1y2
t
−

1

+

ν + 1
2

(cid:19)(cid:19)

Ψ

ν
2 −
ϕ2
1
σ2

Z

(cid:18)
y2
t
−

t p (yo, ym; θ) dym +
y2

1p (yo, ym; θ) dym +

≤ −

=

=

Z (cid:18)
2
σ2

Z

<

.
∞

ft (yt; yt

1)

ft (yj; yj

−

(cid:19)

=t
j
Y
p (yo, ym; θ) dym

1) dym

−

(23d)

(23e)

2ϕ2
0
σ2 +

ν
2 −

Ψ

ν + 1
2

(cid:18)

(cid:19)(cid:19)

(cid:18)

p (yo; θ)

(23f)

(23g)

In (23c), we split the integral of τ =

into two parts: the ﬁrst part involves τt, while the second does
not. The inequality (23d) holds from (8f). The inequality (23g) is from the result of the last subsection,

τj
{

}

Lemma 3 and the boundness theorem.

References

[1] M. Ahsanullah, B. G. Kibria, and M. Shakil, Normal and Student’s t Distributions and Their Appli-

cations. Paris: Atlantis Press, 2014.

[2] C. Liu, “ML estimation of the multivariate t distribution and the EM algorithm,” J. Multivar. Anal.,

vol. 63, no. 2, pp. 296–312, 1997.

[3] https://math.stackexchange.com/questions/138252/expected-value-of-ln-x-if-x-is-gammaa-b-

distributed.

[4] H. L. Royden and P. Fitzpatrick, Real Analysis. New York: Macmillan, 1988.

8

6
