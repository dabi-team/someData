FastS2S-VC: Streaming Non-Autoregressive
Sequence-to-Sequence Voice Conversion

Hirokazu Kameoka, Kou Tanaka, and Takuhiro Kaneko

1

1
2
0
2

r
p
A
4
1

]

D
S
.
s
c
[

1
v
0
0
9
6
0
.
4
0
1
2
:
v
i
X
r
a

Abstract—This paper proposes a non-autoregressive extension
of our previously proposed sequence-to-sequence (S2S) model-
based voice conversion (VC) methods. S2S model-based VC
methods have attracted particular attention in recent years for
their ﬂexibility in converting not only the voice identity but also
the pitch contour and local duration of input speech, thanks to
the ability of the encoder-decoder architecture with the attention
mechanism. However, one of the obstacles to making these
methods work in real-time is the autoregressive (AR) structure.
To overcome this obstacle, we develop a method to obtain a
model that is free from an AR structure and behaves similarly
to the original S2S models, based on a teacher-student learning
framework. In our method, called “FastS2S-VC”, the student
model consists of encoder, decoder, and attention predictor.
The attention predictor learns to predict attention distributions
solely from source speech along with a target class index with
the guidance of those predicted by the teacher model from
both source and target speech. Speciﬁcally, it is designed as
a network that takes the mel-spectrogram of source speech
as input and generates in parallel each row of an attention
weight matrix represented by a Gaussian function. Thanks to the
model structure that does not take the mel-spectrogram of target
speech as input, the model is freed from an AR structure and
allows for parallelization. Furthermore, we show that FastS2S-
VC is suitable for real-time implementation based on a sliding-
window approach, and describe how to make it run in real-time.
Through speaker-identity and emotional-expression conversion
experiments, we conﬁrmed that FastS2S-VC was able to speed up
the conversion process by 70 to 100 times compared to the original
AR-type S2S-VC methods, without signiﬁcantly degrading the
audio quality and similarity to target speech. We also conﬁrmed
that the real-time version of FastS2S-VC can be run with a
latency of 32 ms when run on a GPU.

Index Terms—Voice conversion (VC), sequence-to-sequence
learning, attention, many-to-many VC, non-autoregressive model.

I. INTRODUCTION

The technique to modify some characteristics of speech,
such as voice identity, emotional expression, and accents,
without changing the linguistic content is called voice conver-
sion (VC). Early studies of VC focused on learning spectral
feature mapping rules using time-aligned parallel utterances of
source and target speech. To name just a few, examples include
methods based on Gaussian mixture models (GMMs) [1], [2],
partial least square regression [3], frequency warping [4], non-
negative matrix factorization [5], group sparse representation
[6], fully connected deep neural networks (DNNs) [7], [8],
and long-short term memory networks (LSTMs) [9], [10].
Along with these studies, there has also been a lot of work

done on non-parallel VC methods, which require no parallel
utterances for training. These include methods based on an
i-vector representation [11], restricted Boltzmann machines
[12], phonetic posteriograms [13], regular/variational autoen-
coders [14]–[20], generative adversarial networks [21], [22],
ﬂow-based models [23], and score-based generative models
[24]. More details on the trends and challenges in VC research
can be found in a recent review article [25].

Among many VC studies, methods based on the sequence-
to-sequence (S2S) learning framework [26], [27] have received
particular attention in recent years [28]–[37]. The S2S learning
framework offers a general and powerful solution to a broad
class of sequential mapping problems,
including machine
translation (MT), automatic speech recognition (ASR) [27],
and text-to-speech (TTS) [38]–[43]. While most conventional
VC methods have focused on converting only the voice
identity, S2S-based VC (hereafter referred to as S2S-VC)
methods are particularly attractive in that they can ﬂexibly
change not only the voice identity but also the speaking style,
thanks to the ability of the encoder-decoder architecture with
the attention mechanisim. However, one challenge in S2S-VC
is how to make the model work well even with a limited
amount of training data given the high cost of collecting
parallel utterances. One idea involves using auxiliary text
transcriptions for training, as in [28]–[31]. As another way,
we proposed methods with recurrent neural network (RNN)
[33], convolutional neural network (CNN) [34], [35], and
Transformer [36], [37] architectures, and several ideas to allow
these architectures to work stably under limited resources.

When building real-time S2S-VC systems, another impor-
tant challenge is how to keep the latency of the conversion
process as low as possible. One obstacle to this is the autore-
gressive (AR) structure common to S2S models. Hence, one
solution would be to develop a non-autoregressive (NAR) S2S
model tailored to VC. Recently, several attempts have already
been made to develop NAR-S2S models designed for MT [44],
ASR [45], and TTS [46]–[48]. Inspired by these studies, in
this paper we propose “FastS2S-VC”, a VC method based on
a VC-tailored NAR-S2S model built upon our convolutional
S2S (ConvS2S)-based [34], [35] and Transformer-based [36],
[37] models, and describe its real-time implementation.

II. SEQUENCE-TO-SEQUENCE VOICE CONVERSION

A. General Structure

H. Kameoka, K. Tanaka, and T. Kaneko are with NTT Communication
Science Laboratories, Nippon Telegraph and Telephone Corporation, Atsugi,
Kanagawa, 243-0198 Japan (e-mail: hirokazu.kameoka.uh@hco.ntt.co.jp).

In this section, we ﬁrst introduce a general form of our S2S-
VC models, which reduces to the architectures we proposed
previously [33]–[37] under certain designs.

 
 
 
 
 
 
′

′

′

)

)

) = [x(k
1

1 , . . . , x(k)

We hereafter use X(k) = [x(k)
, . . . , x(k

N ] ∈ RD×N and
M ] ∈ RD×M to denote the feature
X(k
vector sequences of source and target speech reading the same
sentence, where the source and target speech are assumed to
belong to classes k and k′. N and M denote the lengths of
the two sequences and D denotes the feature dimension. Here,
a class represents any non-linguistic attribute of speech, for
example, speaker identity in a speaker conversion task and
emotional state in an emotional expression conversion task. In
the following, we consider a many-to-many S2S model that
learns to map X(k) to X(k

) where k, k′ ∈ {1, . . . , K}.

′

While in our previous studies, we chose to use a set of the
mel-cepstral vocoder parameters (the mel-cepstral coefﬁcients,
log fundamental frequency, and aperiodicity) extracted at each
short-term frame as the feature vector [33]–[35], [37], this was
only a tentative choice in anticipation of the future use of one
of neural vocoders for waveform generation. In this paper, we
use the 80-dimensional mel-spectrum instead as the feature
vector and choose to use Parallel WaveGAN [49] for waveform
generation. In the following, for the sake of distinction, we
refer to these versions of RNNS2S-VC [33], ConvS2S-VC
[34], [35], and Transformer-VC (also called Voice Trans-
former Network; VTN) [36], [37] as RNNS2S-VC2, ConvS2S-
VC2, and Transformer-VC2, respectively. For the purpose
of speeding up and stabilizing the training and inference of
S2S-VC models, we split the mel-spectral sequence (mel-
spectrogram) of each utterance into non-overlapping segments
of equal length r and use the stack of the mel-spectra within
each segment as a new feature vector, as in [35]–[37]. This
reshaping makes the new feature sequence r times shorter than
the original mel-spectrogram. Thus, D = 80 × r.

′

′

′

The overall structure of the general form of our S2S-
VC model is illustrated in Fig. 1. As Fig. 1 shows, our
S2S-VC model consists of seven modules: source and target
prenets, encoder, predecoder, attention module, postdecoder,
and postnet. The roles of these modules are as follows. The
two prenets are responsible for capturing the local dynamics
in X(k) and X(k
), the mel-spectrograms of source and target
speech. The encoder and predecoder are expected to extract
the linguistic contents of source and target speech in the form
of context vector sequences Z(k), Z(k
) from the sequences
˜X(k), ˜X(k
) produced by the two prenets. The attention module
computes a similarity (attention) matrix A(k,k
) between the
source and target context vector sequences, and uses it to
warp the source context vector sequence so that the warped
time points contextually correspond to the time points of the
target context vector sequence. The postdecoder and postnet
are responsible for converting the warped source context vector
sequence R(k,k
) into a time-shifted version of the target
mel-spectrogram so that the ﬁnal output from the postnet at
a certain step can be used recursively as an input into the
target prenet at the next step at test inference time. For this
reason, it is important to note that the target prenet, predecoder,
postdecoder, and postnet must be designed in such a manner
that they do not use future information of the input sequence
when producing the output at each time step.

′

′

Note that in most literatures related to S2S models, the

2

Fig. 1. General structure of our S2S-VC model for the case of r = 1.
Here, ⊕ is used to denote either a sum of arrays with compatible sizes or a
concatenation of arrays along the channel axis. The notation “×L” indicates
L repetitions of the block enclosed by the solid frame.

sub-modules that we term the predecoder, attention module,
and postdecoder in the above model are often collectively
called the “decoder”. However, we will stick to this sub-
module representation for the sake of brevity in the subsequent
explanations. In fact, it allows us to view the architectures
of RNNS2S-VC2, ConvS2S-VC2, and Transformer-VC2 in
a uniﬁed manner as special cases of the above model. Fig.
2 shows the designs of the seven modules where the above
model reduces to the RNNS2S-VC2, ConvS2S-VC2, and
Transformer-VC2 models.

1) RNNS2S-VC2: The architecture of RNNS2S-VC [33]
was inspired by “Tacotron” [38], an S2S model-based end-
to-end TTS system. The same applies to RNNS2S-VC2,
except for some slight modiﬁcations. The source prenet is
composed of four 1D convolution layers with the kernel size
of 5, each followed by a leaky rectiﬁed linear unit (ReLU)
activation function. The target prenet is composed of three
fully-connected linear layers (applied independently to each
vector in an input sequence), each followed by a leaky ReLU
activation function. The encoder is a two-layer birectional
LSTM whereas the predecoder is a one-layer unidirectional
LSTM. The attention module computes Bahdanau’s additive
attention [51]. The postdecoder is again a one-layer unidirec-
tional LSTM and the postnet is a single fully-connected linear
layer that projects the vectors produced by the postdecoder
onto mel-spectra. Note that the LSTMs in the predecoder
and postdecoder are unidirectional so as not to allow them to
look at future information in computing the outputs. Weight
normalization [52] is applied to all the learnable weights. L is
1. The source and target class labels are fed into each module
in the form of continuous vectors stored in a learnable lookup

3

Fig. 2. Architecture details of (1) RNNS2S-VC2, (2) ConvS2S-VC2, and (3) Transformer-VC2. The blue boxes indicate layers with learnable parameters
and the green boxes indicate operations with no learnable parameters. The attention module internally generates one attention weight matrix A(k,k
) for
(1) and (2), and H attention weight matrices A(k,k
}1≤h≤H for (3). In each layer that takes a class index as an input, an embedding vector
is ﬁrst retrieved from a learnable lookup table according to that class index, then the embedding vector is repeated along the time axis, and the resulting
vector sequence is ﬁnally appended to the input sequence along the channel direction before being fed into that layer. P and P′ denote sinusoidal position
encodings [50].

) = {A(k,k

h

)

′

′

′

table.

2) ConvS2S-VC2: The architecture of ConvS2S-VC2 is
also slightly modiﬁed from the original architecture [34],
[35]. The source and target prenets and postnet are each
composed of a single fully-connected linear layer. The encoder
is composed of eight dilated convolution layers with the kernel
size of 5 and the dilation factors of 1, 3, 9, 27, 1, 3, 9, and
27, respectively, each followed by a gated linear unit (GLU)
activation function [53]. The same design is used for the
predecoder and postdecoder except that all convolutions are
causal so as not to allow them to look at future information
in computing the outputs. The attention module computes the
scaled dot-product attention introduced in [50]. L is 1. Weight
normalization [52] is applied to all the learnable weights.

3) Transformer-VC2: The architecture of Transformer-VC2
is exactly the same as the one we proposed in [37], except that
it uses the mel-spectrograms of source and target speech as the
input and output. See Fig. 2 for the architecture details.

B. Autoregressive Structure

In all the above models, the target prenet is assumed to
take the output vector generated by the postnet at the previous
recursive step as the input at each step during test time. This
implies that the process of generating the mel-spectrogram of
target speech cannot be parallelized. This AR structure, which

is common in all regular S2S models, is a major bottleneck
for achieving real-time VC. Hence, the challenge is how to
achieve a model that is free from an AR structure without
compromising the capabilities of S2S models.

C. Real-Time System Design

As already described in [35], [37], it is actually possible to
implement real-time systems with several slight modiﬁcations
to the above models: First, we make the source prenet and
encoder depend only on past and present
inputs as with
the target prenet, predecoder, and postdecoder. This can be
done by replacing all
the convolutions and LSTMs with
causal and unidirectional versions, respectively, and applying
appropriately designed masks to all the self-attention matrices
so that position n will depend only on the elements at positions
less than n. Second, we design the postdecoder so that it does
not directly receive the output from the predecoder (i.e., Q(k
)
in Fig. 2). Namely, we let the postdecoder receive only the
ouput of the attention module and the target class index (i.e.,
) and k′ in Fig. 2). Third, at test time, we force the
R(k,k
attention weight matrices to be identity matrices. This frees
the models from the AR structure and makes inference faster
and easier.

′

′

Note, however, that in this way,

the speaking rate and
rhythm of the converted speech will be exactly the same as

4

(a) Teacher model

(b) Student model

Fig. 3. Structures of the teacher and student models in FastS2S-VC. The teacher model is slightly modiﬁed from the one shown in Fig. 1 so that the
postdecoder does not receive direct input from the predecoder. The attention predictor in the student model is the only network that is trained during student
training; the remaining networks are copied from the pretrained teacher model.

that of input speech. Here, we are concerned with a more chal-
lenging problem: allowing even the speaking rate and rhythm
of input speech to be converted, under the constraint that the
timings of the input and converted speech are synchronized at
a certain interval. If we choose not to skip performing attention
matrix prediction at test time, we cannot avoid AR recursion as
far as we use the above models as they are. Nevertheless, fast
inference is still possible up to a point if we use sufﬁciently
shallow architectures. However, making the network deeper to
achieve better conversion quality comes at the cost of longer
latency. To achieve both good quality and short latency, we
believe an NAR extension of S2S models is one of the keys.

III. FASTS2S-VC

A. Overall Structure

Motivated by the above, the main idea we propose in this
paper is an NAR extension of S2S models tailored to real-time
VC. We call the VC method based on the proposed model
“FastS2S-VC”.

One major factor causing the AR structure in all the above
models is the attention mechanism, which is designed to use
both source and target mel-spectrograms to compute attention
weight matrices. Therefore, at test time, the attention module
will not be able to compute attention weights for a particular
frame until the postnet has ﬁnished producing the target mel-
spectrum in that frame. To remove this restriction, the attention
module must have a structure such that attention weight
matrices are computable without target mel-spectrograms. One
possible solution to this would be to introduce an additional
network that learns to predict the attention weight matrix only

from a source mel-spectrogram along with source and target
class labels. We call this network “attention predictor”. To
facilitate training, we can adopt a teacher-student learning
framework, in which one of the above AR-type models is
used as the teacher model, as detailed below. Another factor
causing the AR structure lies in the design such that the
postdecoder takes the output of the predecoder as input.
This restriction can be avoided simply by not allowing the
postdecoder to use the predecoder output in its process. The
teacher model must also be redesigned accordingly. To put the
above together, the teacher and student models we study in this
paper is structured as in Fig. 3. In the following, we discuss
how the attention predictor should be designed and trained.
Note that since architectures containing recurrent units are not
parallelizable, the architecture of the student model considered
below assumes the use of the ConvS2S-VC2 or Transformer-
VC2 architecture as the teacher model.

As detailed later, our real-time S2S-VC system operates
in a sliding-window fashion designed so that the window
length corresponds to the latency of the entire VC process. A
straightforward implemention of this can cause problems when
the speaking rate of the converted speech is very different
from that of input speech. For example, when the converted
speech becomes faster, a silent segment must be inserted
somewhere within each window, and when it becomes slower,
some segment of the converted speech must be skipped. This
may result in choppy-sounding or unintelligible speech even
though the conversion process has been successful. To avoid
this, the attention weight matrix should be represented in a
form that is ﬂexibly expandable and contractible to ﬁt into a

given window length. Furthermore, the peak of the attention
weight distribution in each frame must be guaranteed to move
only forward, so that some segments of the converted speech
will not be skipped or repeated.

B. Attention Predictor

5

Given the above requirements and inspired by Graves’
Gaussian attention [54], we propose using (unnormalized)
Gaussian distribution functions to model attention weight
matrices and designing the attention predictor to produce
their parameters. The idea of using parametric functions such
as Gaussian and logistic distribution functions to represent
attention distributions for end-to-end TTS has already been
proposed [55], [56]. However, the only point that our idea
has in common with these previous studies is the use of
parametric functions to represent attention distributions, but
the way they are used is very different: While the previous
studies use parametric functions to represent each column
of an attention weight matrix with the aforementioned AR
structure in mind, we use them to represent each row instead
so that the prediction of each row of an attention weight matrix
can be executed in parallel. Another advantage is that this
representation allows the length of the converted speech in
each sliding window to be ﬂexibly expandable or contractible
to ﬁt into a given window length without sacriﬁcing the audio
quality, thus facilitating smooth chunk-by-chunk conversion.
The attention module in the teacher model computes one
attention weight matrix for the ConvS2S architecture and
multiple attention weight matrices for the Transformer archi-
tecture, and uses it (them) to produce a time-warped version
) denote
of the source context vector sequence Z(k). Let A(k,k
the attention weight matrix obtained by the attention module
in the ConvS2S model and A(k,k
denote the
attention weight matrices obtained by the Lth (ﬁnal) attention
module in the Transformer model, where H is the number of
heads in multi-head attention. For the ConvS2S architecture,
the output of the attention module is given by

, . . . , A(k,k

H

1

)

)

′

′

′

′

′

),

R(k,k

) = V(k)A(k,k

(1)
where V(k) ∈ Rd/2×N is one of the half splits of Z(k) ∈
Rd×N in the channel direction and d is the dimension of
each context vector. For the Transformer architecture, the
ﬁnal attention module generates a time-warped context vector
sequence R(k,k

), given by

′

′

′

′

)

)

1

R(k,k

1 A(k,k

H A(k,k

) = W[V(k)

1 , . . . , V(k)

; . . . ; V(k)
(2)
H ∈ Rd/H×N are linear projections of
where V(k)
Z(k) ∈ Rd×N obtained using different
learnable weight
matrices, and W ∈ Rd×d is another learnable weight matrix.
To design the student model, we now consider modeling
′
)
using a parametric function

],

H

each element ah,n,m of A(k,k
αh,n(m) of the form

h

αh,n(m) = φh,n exp

−

(cid:18)

(m − µh,n)2
2σ2

h,n

,
(cid:19)

(3)

Fig. 4. Attention distribution function αn(m).

attention for the Transformer architecture. Obviously, the peak
of αh,n(m) is centered at µh,n. This representation can thus be
interpreted to mean that time n in the source speech is likely
to correspond to time µh,n in the target speech. Since the time-
ordering of µh,1, . . . , µh,N should be non-decreasing from left
to right, we would like them to satisfy µh,1 ≤ µh,2 ≤ · · · ≤
µh,N −1 ≤ µh,N . A convenient way to handle this constraint
would be to reparametrize them using non-negative variables
∆h,1, . . . , ∆h,N as

µh,1 = ∆h,1,
µi,n = µi,n−1 + ∆h,n (n ≥ 2).

(4)

(5)

Hence, the relationship between µh = [µh,1, . . . , µh,N ] and
∆h = [∆h,1, . . . , ∆h,N ] can be written in vector notation as

[µh,1, . . . , µh,N ]

= [∆h,1, . . . , ∆h,N ]

µh
{z

|

}

|

∆h
{z

}






1 · · · 1
...
. . .
1





0

.

(6)

U
{z

}

|

′

′

We design the attention predictor in the student model as a
network that takes the source context vector sequence Z(k)
and source and target class indices (k, k′), and produces an
array θ(k,k
) consisting of the parameters of the Gaussians,
) = [∆; Σ; Φ] ∈ R3H×N , where ∆ = (∆h,n)H×N ,
θ(k,k
Σ = (σh,n)H×N , and Φ = (φh,n)H×N (H = 1 for
the ConvS2S architecture and H ≥ 1 for the Transformer
architecture), as an intermediate output. Note that here we have
used [; ] to denote vertical concatenation of arrays, matrices or
vectors with compatible sizes. Once θ(k,k
) is determined, the
attention predictor computes attention distribution functions
α(k,k
= (αh,n(m))N ×M (h = 1, . . . , H) using Eqs. (6) and
h
(3). Since attention distributions are assumed to sum to 1 along
the n-axis, each element of α(k,k

is further normalized as

)

)

′

′

′

h

αh,n(m) ←

αh,n(m)
n′ αh,n′ (m)

.

(7)

P
The attention module in the student model ﬁnally produces
a time-warped version of Z(k) in the same manner as in the
)
teacher model, by using α(k,k

instead of A(k,k

.

)

′

′

h

h

Since all the parameters of the Gaussians must be non-

negative, we include a layer that performs

where h is an index that can be dropped for the ConvS2S
architecture and corresponds to one of the heads in multi-head

∆ ← absolute(∆),
Σ ← min(max(absolute(Σ), 0.001), 1.0),

(8)
(9)

D. Student Training Objective

6

′

In the student training phase, the source prenet, encoder,
postdecoder, and postnet are assumed to be ﬁxed at the ones
copied from a pretrained teacher model, and the only network
to be trained is the attention predictor. For details about the
teacher training objectives, please refer to [35], [37].

Let X(k) ∈ RD×N and X(k

) ∈ RD×M be the mel-
spectrograms of a pair of parallel utterances, where k and
k′ denote the source and target classes. As a start-of-sequence
token, an all-zero vector is appended to the left end of the
)]. This results
target sequence only, namely, X(k
in a length of M + 1. Given this single pair of utterances, the
main loss to be minimized is the mean absolute error between
) and a time-shifted version of the
the model output Y(k,k
), as in the teacher training:
target mel-spectrogram X(k

) ← [0, X(k

′

′

′

′

L0 = 1

M kY(k,k

1:M − X(k

)

2:M+1k1,

)

′

′

(11)

′

′

)

)

h

where the notation Xi:j refers to the submatrix consisting
of the ith to jth columns of the matrix X. In addition to
this, we can use the attention weight matrix A(k,k
computed
by the teacher model for each source-target utterance pair
as a regression target to guide the attention predictor out-
put α(k,k
. However, it transpired that using the difference
h
between A(k,k
alone as the loss did not work
very well. This was because both the regression target and
approximator were likely to be very sparse: Even when the
peaks of the regression target and approximator were slightly
apart, the gradients tended to vanish easily. For faster and more
stable training, we have found it useful to use

and α(k,k

h

h

)

)

′

′

L1 = 1
HN

h,n(|µh,n − ˆµh,n| + |σh,n − ˆσh,n|),

(12)

P

as an auxiliary loss, where ˆµh,n and ˆσh,n are the
mean and standard deviation of time m when each row
a(k,k
h,n,1 , . . . , a(k,k
is seen as a histogram of m =
1, . . . , M , given by

h,n,M of A(k,k

h

)

)

)

′

′

′

ˆµh,n =

ˆσ2
h,n =

′

P

M

M

m=1 a(k,k
)
h,n,mm
m=1 a(k,k′)
P
m=1 a(k,k
M

P

h,n,m
′
)
h,n,m(m − ˆµh,n)2
m=1 a(k,k′)
M

h,n,m

,

(13)

.

(14)

P
To further enforce α(k,k
to be as diagonally dominant and
orthogonal as possible, we introduce the diagonal [42] and
orthogonal [35] attention losses:

h

)

′

HN M

L2 = 1
L3 = 1

HN 2

hkGN ×M (ν) ⊙ α(k,k
P
hkGN ×N (ρ) ⊙ (α(k,k

h

h

′

′

)

)

k1,
α(k,k
h

′

T

)

)k1,

(15)

(16)

P

where ⊙ denotes an elementwise product, and GN ×M (ν) ∈
RN ×M is a non-negative weight matrix whose (n, m)th ele-
ment gn,m is deﬁned as gn,m = 1 − e−(n/N −m/M)2/2ν2

.

Given all examples of parallel utterances, the total training

loss for the student model to be minimized is given as

L =

Xk,k′

EX(k),X(k′

) {L0 + λ1L1 + λ2L2 + λ3L3} ,

(17)

(a) ConvS2S

(b) Transformer

Fig. 5. Architectures of the student attention module assuming the use of the
(a) ConvS2S and (b) Transformer architectures as the teacher model. In (b),
α(k,k
h }h,
respectively.

) and V(k) are abbreviations of the sets {α(k,k

}h and {V(k)

h

)

′

′

Φ ← 0.2sigmoid(Φ) + 0.8,

(10)

′

before producing θ(k,k
), where absolute(·) and sigmoid(·)
denote elementwise absolute and sigmoid functions, and
min(a, b) and max(a, b) denote functions that return the
elementwise minimum and maximum of a and b. Note that
Eqs. (9) and (10) are to prevent each Gaussian from becoming
too wide and vanishing, respectively.

The operation in the attention predictor network must
be parallelizable and causal. We thus choose to design the
network with a fully convolutional architecture, consisting
of two fully-connected linear layers and eight dilated causal
convolution layers with the kernel sizes of 5 and the dilation
factors of 1, 3, 9, 27, 1, 3, 9, and 27, respectively, each
followed by a GLU.

C. Random Noise Input

Speech ﬂuctuates in time from utterance to utterance. There-
fore, the timing differences between source and target speech
also ﬂuctuate, and the rules governing these ﬂuctuations are
neither unique nor deterministic. In other words, the distri-
bution of the time-warping functions the attention module
needs to predict is likely to be multimodal. Thus, the problem
the attention module must handle is a one-to-many mapping
problem. Although AR models are generally known to be
reasonably good at handling one-to-many mapping problems,
since the attention predictor does not rely on an AR structure,
it must have some mechanism that can deal with this problem
nicely. Although very simple, we propose including randomly
drawn samples in the input to the attention predictor in the
hope that these samples will be used to explain the randomness
related to the temporal ﬂuctuations in the timing differences
between source and target speech that cannot be explained by
deterministic rules. This simple idea actually worked out well.
To put the above together, the architecture of the student

attention module can be conﬁgured as in Fig. 5.

7

nately, causal convolutions can be computed in a sliding-
window fashion without any approximation, as explained in
Fig. 6. Namely, consider dividing an input sequence into non-
overlapping chunks and performing convolution on each chunk
in a way that a portion of the previous chunk is padded to
the left end of the current chunk. When the padding size is
δ(κ − 1), where δ and κ are the dilation factor and kernel
size of the convolution, respectively, the concatenation of
all the outputs becomes exactly the same as the output of
that layer where the entire sequence is given as input. Note
that since the architecture of Parallel WaveGAN consists of
non-causal convolution layers, we have redesigned all the
convolution layers to be causal so that the same technique
can be used. As for the self-attention layers, the distance
between elements in a sequence that can interact with each
other must be limited, since storing all
the elements and
keeping track of their interdependencies are not possible in
real-time streaming scenarios. To avoid having to compute
the self-attention between two elements that are more than
a certain distance J apart, we mask each self-attention matrix
so that position n can depend only on the elements at positions
n − 1, . . . , n − J. This is equivalent to multiplying a binary
matrix like the one in Fig. 7 elementwise by an unconstrained
self-attention matrix. This constraint makes the training and
runtime conditions consistent, and allows us to compute the
process of a self-attention layer in a sliding-window fashion
without approximation by padding a portion of length J of
the previous chunk of an input sequence to the left end of the
current chunk, and passing a sequence consisting of only the
last S elements of the ouput sequence to the next layer.

One of the key advantages of representing each row of an
attention matrix as a Gaussian distribution function in FastS2S-
VC is the ﬂexibility to expand or contract the length of the
converted version of each chunk of input speech to match the
window length S. If we use ¯µ1 and ¯µS to denote the means
of µ1,1, . . . , µH,1 and µ1,S, . . . , µH,S, respectively, one simple
way would be to transform the centers µh,1, . . . , µh,S of the
Gaussians linearly:

µh,n ←

(S − 1)(µh,n − ¯µ1)
¯µS − ¯µ1

+ 1 (n = 1, . . . , S),

(18)

′

so that ¯µ1 = 1 and ¯µS = S, and compute α(k,k
R(k,k
resulting α(k,k

and
) accordingly. By keeping σh,1, . . . , σh,S unchanged, the
can be made neither too blurry nor too sharp.

h

)

)

′

h

′

IV. EXPERIMENTS

A. Experimental Settings

To evaluate the conversion quality of the proposed methods,
we conducted objective and subjective evaluation experiments
involving speaker-identity and emotional-expression conver-
sion tasks. For the speaker-identity conversion task, we used
the CMU Arctic database [57], which consists of recordings
of 1,132 phonetically balanced English utterances spoken
by four US English speakers, clb (female), bdl (male), slt
(female), and rms (male). Therefore, there were a total of 12
combinations of source and target speakers. For each speaker,
the ﬁrst 1,000 utterances were used as the training set, and

Fig. 6.

Implementation of sliding-window causal convolutions.

Fig. 7.
Image of a mask by which a self-attention matrix is to
be multiplied elementwise. The black and white areas represent
the 0 and 1 elements, respectively. Only the submatrix of a self-
attention matrix corresponding to the area surrounded by the blue
line is used to compute the output of a self-attention layer.

where EX(k),X(k′
) {·} is the sample mean over all the training
examples of parallel utterances of classes k and k′, and λ1 ≥ 0,
λ2 ≥ 0 and λ3 ≥ 0 are regularization parameters.

E. Conversion process

Once the attention predictor has been trained, the conver-
sion process simply becomes a cascade of the ﬁve trained
networks, namely the source prenet, encoder, attention module,
postdecoder, and postnet. It is worth noting that this process
can be executed in parallel without recursion. After converting
the mel-spectrogram of input speech through this process, we
ﬁnally generate a waveform using Parallel WaveGAN [49].

F. Real-Time Implementation Details

Here, we desribe some of the techniques to implement real-
time systems of FastS2S-VC. First, all the convolution and
self-attention layers must be causal. To make the best use
of the parallelizable structures of the convolution and self-
attention layers, we take a sliding-window approach. Fortu-

8

TABLE I
METHODS FOR COMPARISON

Base model

Method
Processing

Architecture

Explanation

S2S-VC

BAT

RNN/Conv/Trans

Batch processing (BAT) version of S2S-VC with different architectures (RNNS2S-VC2,
ConvS2S-VC2, and Transformer-VC2). Conversion is performed using AR recursion.

FastS2S-VC

BAT

Conv/Trans

Batch processing (BAT) version of FastS2S-VC. All the networks are designed to be causal.
Conversion is done in parallel without AR recursion.

RT

Conv/Trans

Real-time processing (RT) version of FastS2S-VC described in Subsection III-F. All
the
networks are designed to be causal. Conversion is done in a sliding-window fashion without AR
recursion. The function to convert the local speaking rate (rhythm) of input speech is enabled.

the remaining 132 utterances were used as the evaluation set.
For the emotional-expression conversion task, we used audio
signals of 503 phonetically balanced sentences from the ATR
Japanese Speech Database [58], read by one voice actress with
four different emotional expressions (neutral, angry, sad, and
happy). For each expression, the utterances corresponding to
the ﬁrst 450 sentences were used as the training set, and those
corresponding to the remaining 53 sentences were used as
the evaluation set. All the speech signals were sampled at 16
kHz. The mel-spectrogram with 80 frequency bands of each
utterance was computed with a frame length of 64 ms and a
hop size of 8 ms. The reduction factor r was set to 4. Hence,
the dimension of the acoustic feature was D = 80 × 4 = 320.
As described in Subsection II-A, we used Parallel Wave-
GAN [49] for waveform generation from mel-spectrograms.
For its implementation, we used the unofﬁcial source code
available on github1. Speciﬁcally, we trained the speaker-
independent version on the same training set used to train
the core feature mapping models. All algorithms were imple-
mented in PyTorch and run on a single Tesla V100 SXM2
GPU with a 32.0 GB memory and an Intel(R) Xeon(R) Gold
5218 16-core CPU @ 2.30GHz.

B. Methods for Comparison

The methods used for comparison in the current experiments
are listed in Table I. These methods are categorized by base
model, processing type, and architecture type, where the base
model
includes the baseline S2S-VC model and the pro-
posed FastS2S-VC model, the processing type includes batch
(BAT) and real-time (RT) processing, and the architecture
type includes recurrent (RNN), convolutional, and Transformer
architectures. Note that existing S2S model-based VC methods
[29], [31], including our RNNS2S-VC2, fall into the category
of “S2S-VC–BAT–RNN”, albeit with some differences in
architectural details and training objectives.

We also chose the open-source VC system called sprocket
[59] as a baseline in the subjective listening tests. To run this
method, we used the source code provided by its author2. Note
that this system was used as a baseline system in the Voice
Conversion Challenge (VCC) 2018 [60].

1https://github.com/kan-bayashi/ParallelWaveGAN
2https://github.com/k2kobayashi/sprocket

(a)

(b)

(c)

head1

head2

head3

head4

(d)

head1

head2

head3

head4

(e)

head1

head2

head3

head4

(f)

Fig. 8. Attention matrices predicted by (a) S2S-VC–BAT–Conv, the (b)
teacher and (c) student models in FastS2S-VC–BAT–Conv, (d) S2S-VC–BAT–
Trans, and the (e) teacher and (f) student models in FastS2S-VC–BAT–Trans
from the same utterance of clb when the target speaker was bdl. In (d) and (e),
only the attention matrices produced from the last decoder layer are shown.

C. Hyperparameter Settings in Model Training

λ1, λ2, and λ3 were set at 1, 2000, and 2000, respectively.

Both ν and ρ were set at 0.3.

Adam optimization [61] was used for model training where
the mini-batch size was 16 for all the models. 70,000 iterations

TABLE II
MCDS AND LFCS OBTAINED WITH S2S-VC (BAT) AND FASTS2S-VC (BAT/RT).

(a) MCD

9

source

Speakers

target
bdl
slt
rms
clb
slt
rms
clb
bdl
rms
clb
bdl
slt
All pairs

clb

bdl

slt

rms

source

Speakers

target
bdl
slt
rms
clb
slt
rms
clb
bdl
rms
clb
bdl
slt
All pairs

clb

bdl

slt

rms

S2S-VC (BAT)
Conv
6.44
5.80
6.14
5.89
5.87
6.19
5.86
6.47
6.14
6.00
6.51
5.90
6.10

Trans
6.70
6.11
6.45
6.24
6.25
6.50
6.13
6.70
6.48
6.42
6.74
6.28
6.41

RNN
6.54
6.19
6.59
6.30
6.34
6.78
6.27
6.66
6.66
6.31
6.59
6.40
6.47

FastS2S-VC (BAT)
Conv
6.45
5.95
6.25
6.09
6.08
6.44
6.03
6.56
6.35
6.14
6.56
6.10
6.25

Trans
6.63
6.22
6.52
6.47
6.37
6.60
6.32
6.79
6.58
6.44
6.82
6.43
6.52

FastS2S-VC (RT)
Conv
6.44
5.84
6.26
6.17
6.14
6.84
6.11
6.56
6.52
6.23
6.54
5.98
6.28

Trans
6.55
6.18
6.53
6.48
6.32
7.15
6.30
6.63
6.80
6.41
6.65
6.28
6.50

(b) LFC

S2S-VC (BAT)
Conv
0.773
0.841
0.738
0.834
0.840
0.655
0.832
0.757
0.731
0.731
0.757
0.816
0.772

Trans
0.798
0.817
0.529
0.732
0.795
0.493
0.805
0.752
0.640
0.782
0.690
0.778
0.732

RNN
0.621
0.657
0.515
0.676
0.729
0.489
0.692
0.636
0.497
0.692
0.597
0.741
0.626

FastS2S-VC (BAT)
Conv
0.803
0.883
0.699
0.793
0.824
0.666
0.811
0.805
0.669
0.785
0.774
0.831
0.780

Trans
0.731
0.890
0.642
0.751
0.849
0.600
0.823
0.717
0.705
0.764
0.717
0.787
0.747

FastS2S-VC (RT)
Conv
0.772
0.851
0.663
0.751
0.825
0.683
0.799
0.726
0.714
0.704
0.737
0.764
0.750

Trans
0.688
0.886
0.610
0.771
0.827
0.517
0.853
0.687
0.554
0.721
0.582
0.781
0.732

were run for the baseline S2S-VC models and the teacher
models in FastS2S-VC, and 300,000 iterations for the student
models. The learning rate and the exponential decay rate for
the ﬁrst moment for Adam were set at 5 × 10−5 and 0.9.

TABLE III
REAL-TIME FACTORS FOR (1) FEATURE EXTRACTION, (2) FEATURE
MAPPING, AND (3) WAVEFORM GENERATION IN S2S-VC (BAT) AND
FASTS2S-VC (BAT).

D. Objective Performance Measures

The evaluation set for the speaker-identity conversion task
consists of utterances of the same set of sentences read by
different speakers. Therefore, the quality of each converted
utterance can be objectively evaluated by treating the cor-
responding target utterance as the ground truth. To evaluate
the similarity between the converted and target utterances, we
used the mel-cepstral distortion (MCD) and log F0 correlation
coefﬁcient (LFC).

To evaluate the speed of conversion by the BAT version
of each method and the feasible latency of the RT version
of FastS2S-VC, we evaluated the real-time factor (RTF) and
average execution time taken within each sliding window.

1) Mel-cepstral distortion: Given the pair of a converted
speech signal and the corresponding reference speech signal,
we used the average of the MCDs taken along the dynamic
time warping path between the mel-cepstrum sequences of the
two signals. The smaller the MCD, the better the performance.
2) Log F0 Correlation Coefﬁcient: To evaluate the log F0
contour of converted speech, we used the LFC [62] between
the converted and target speech as the objective performance
measure. In the experiment, we used the average of LFCs

stage

(1)
(2)
(3)
total

S2S-VC (BAT)
Conv
0.0037
0.3575
0.0066
0.3678

Trans
0.0039
0.7585
0.0066
0.7689

RNN
0.0039
0.1173
0.0064
0.1276

FastS2S-VC (BAT)
Conv
0.0038
0.0048
0.0064
0.0150

Trans
0.0039
0.0072
0.0065
0.0177

taken over all the test utterances. The closer the LFC is to 1,
the better the performance.

3) Real-Time Factor: To evaluate the speed of conversion
by the BAT version of each method, we measured RTF, i.e.,
the execution time divided by the length of the input speech.
4) Execution Time: To determine the feasible latency for
the RT version of FastS2S-VC, we measured the absolute
execution time in milliseconds within each sliding window
under different window lengths, when run on the GPU and
CPU, respectively.

E. Objective Evaluations

Fig. 8 shows examples of attention matrices predicted by
S2S-VC–BAT–Conv (namely, ConvS2S-VC2), the teacher and
student models in FastS2S-VC–BAT–Conv, S2S-VC–BAT–
Trans (namely, Transformer-VC2), and the teacher and student
models in FastS2S-VC–BAT–Trans. From these examples, we

TABLE IV
AVERAGE EXECUTION TIME IN MILLISECONDS ON GPU AND CPU FOR
(1) FEATURE EXTRACTION, (2) FEATURE MAPPING, AND (3) WAVEFORM
GENERATION PERFORMED AT EACH SLIDING WINDOW WITH THE RT
VERSION OF FASTS2S-VC.

S [ms]

stage

32

64

128

256

(1)
(2)
(3)
total
(1)
(2)
(3)
total
(1)
(2)
(3)
total
(1)
(2)
(3)
total

GPU

CPU

Conv
2.5
9.7
13.4
25.5
2.5
10.1
13.9
26.6
2.7
10.8
14.9
28.4
3.3
11.1
17.0
31.4

Trans
2.4
16.1
13.7
32.2
2.5
16.2
14.5
33.1
2.6
16.6
16.1
35.4
3.3
17.2
17.6
38.1

Conv
2.4
88.6
56.6
147.6
2.5
94.8
68.9
166.2
2.7
95.3
91.8
189.7
3.4
97.6
132.1
233.1

Trans
2.4
85.5
56.3
144.2
2.6
90.6
67.7
160.9
2.7
92.4
90.7
185.8
3.4
93.6
130.1
227.1

can see that the teacher and student models in FastS2S-VC
have successfully been able to produce attention matrices
similar to those produced by the original S2S-VC in both
architectures.

The MCDs and LFCs obtained with all the methods in the
speaker-identiy conversion task are shown in Table II. For
results obtained by other methods under the same conditions,
please refer to [35], [37], [63]. Comparing between architec-
tures, the result showed that the convolutional architecture
performed best for all the methods, followed by the Trans-
former architecture. This result is actually consistent with the
result reported in [37], indicating that the size of the present
training set may not have fully exploited the potential of the
Transformer architecture, and that the convolutional architec-
ture can be a reasonable choice when the amount of training
data is limited, as in the present dataset. Under the same
architectural conditions, the baseline S2S-VC performed the
best. This result is reasonable, since the baseline S2S-VC can
take advantage of the ability of the AR structure (but with the
disadvantage of slow conversion speed). What is noteworthy
here is that both the BAT and RT versions of FastS2S-VC
showed comparable or only slightly worse performance than
the baseline S2S-VC, despite the NAR structure. It is also
important to note that the BAT and RT versions of FastS2S-
VC showed comparable performance, which means that the
sliding-window type conversion process had little negative
impact on the overall conversion quality.

The RTF comparison of S2S-VC and FastS2S-VC is shown
in Table III. As the result shows, FastS2S-VC was able to
perform conversion signiﬁcantly faster than S2S-VC (more
than 70 times faster for the convolutional architecture and
100 times faster for the Transformer architecture) thanks to
its NAR structure. This explains why we named the proposed
method FastS2S-VC. It is worth noting that for both architec-
tures, the feature mapping process took only as much time as
the feature extraction and waveform generation processes.

The average execution time within each sliding window

10

taken by the RT version of FastS2S-VC is shown in Table
IV. The execution time that is shorter than the window length
S is shown in bold. When run on the GPU, both architectures
were able to complete the feature extraction, feature mapping,
and waveform generation processes within most of the tested
window lengths (i.e., 32, 64, 128, and 256 ms). For example,
the shortest feasible latency for the convolutional architecture
was 32 ms. However, when run on the CPU, both architectures
could not complete all the processes in the time corresponding
to some window lengths. Speciﬁcally, both architectures took
longer than S to complete all the processes when S was 32,
64, or 128 ms, and the shortest feasible latency was 256 ms.
However, as can be seen from the breakdown of the execution
time listed in Table IV,
the waveform generation process
accounts for much of the processing time. This suggests that
the latency of the system can be further reduced by using a
faster waveform generation method.

F. Subjective Listening Tests

We conducted subjective listening tests to evaluate the
audio quality and speaker similarity in the speaker-identity
conversion task and the emotional expression similarity in the
emotional-expression conversion task. In each test, twenty-four
listeners (including 21 native Japanese speakers) participated.
All
the tests were conducted online using Amazon Web
Services, and each participant was asked to use a headphone
in a quiet environment. Since the objective evaluation showed
that the convolutional version of FastS2S-VC performed better
than the Transformer version, and since we are particularly
interested in the performance of the real-time version, we
used only the audio samples of FastS2S-VC–RT–Conv as
representative of FastS2S-VC.

With the audio quality test, the mean opinion score (MOS)
was evaluated for each speech sample. In this test, we included
the audio samples generated by Parallel WaveGAN from the
mel-spectrograms extracted from all the test samples. We refer
to this method of sample generation as “Ana/Syn (PWG)”.
Since all the methods compared used Parallel WaveGAN for
waveform generation, the scores for these samples can be seen
as the upper bound of the performance. The speech samples
were presented in a random order to avoid bias in the order
of the stimuli. Each listener was asked to rate the naturalness
of each utterance by selecting 5: Excellent, 4: Good, 3: Fair,
2: Poor, or 1: Bad. The obtained scores along with 95%
conﬁdence intervals are shown in Fig. 9. As the results show,
the one that produced the best-sounding speech was S2S-
VC–BAT–Conv, followed by S2S-VC–BAT–Trans. FastS2S-
VC–RT–Conv was the next best to these two methods. It is
worth noting that it performed better than S2S-VC–BAT–RNN,
which represents existing S2S model-based VC methods.

For the speaker similarity and emotional-expression similar-
ity tests, each listener was asked to rate the subjective scores
on a ﬁve-point scale, similar to the audio quality test. In the
speaker similarity test, each listener was given a converted
speech sample and a real speech sample of the corresponding
target speaker, and asked to rate how likely they were to have
been uttered by the same speaker on a scale of 5: Deﬁnitely, 4:

11

Fig. 9. Audio quality score in speaker-identity conversion task

Fig. 10. Similarity score in speaker-identity conversion task

another, and consists of prenet, encoder, attention predictor,
predecoder, postdecoder, and postnet. The attention predictor
is designed as a fully convolutional network that produces
as intermediate output an attention weight matrix represented
using constrained Gaussian functions, and does not require
the mel-spectrogram of target speech as input, which is the
key to free the entire model from an AR structure and
allow for parallelization. The model is trained to learn to
behave similarly to the S2S-VC model based on a teacher-
student learning framework. We also discussed the idea of
implementing FastS2S-VC as a real-time system based on a
sliding-window approach.

The speaker-identity and emotional-expression conversion
experiments showed that FastS2S-VC was able to speed up the
conversion process by 70 to 100 times, while performing com-
parably or only slightly worse compared to the original S2S-
VC. We also showed that the real-time version of FastS2S-
VC can be run with a latency of 32 ms when run on the
GPU. When run on the CPU, however, the feasible latency
was 256 ms. Even though Parallel WaveGAN is known to be
a relatively fast method, the waveform generation process took
up much of the processing time within each sliding window. In
the future, we would like to pursue faster waveform generation
methods as well as more lightweight network architectures
for the feature mapping model to further reduce the feasible
latency.

ACKNOWLEDGMENTS

This work was supported by JST CREST Grant Number

JPMJCR19A3, Japan.

REFERENCES

Fig. 11. Similarity score in emotional-expression conversion task

Likely, 3: Maybe, 2: Not very likely, and 1: Unlikely. In the
emotional-expression similarity test, each listener was given
a converted speech sample and a real speech sample spoken
with the corresponding target emotional expression, and asked
to rate the similarity of their emotional expressions on a scale
of 5: Very similar, 4: Similar, 3: Fair, 2: Not very similar, and
1: Not alike. The speaker and emotional-expression similarity
scores obtained with these tests along with 95% conﬁdence
intervals are shown in Figs. 10 and 11. The results showed that
the order of superiority of the tested methods was the same
as in the audio quality test, but the performance difference of
each method was smaller in the speaker similarity task and
larger in the emotional-expression conversion task. In terms
of both similarity scores, the proposed FastS2S-VC performed
better than or comparably to S2S-VC–BAT–RNN. This is very
promising, considering the advantage of FastS2S-VC being
able to operate as a real-time system.

Audio examples of the tested methods can be found here3.

V. CONCLUSIONS

In this paper, we proposed FastS2S-VC, an NAR extesnion
of S2S-VC tailored to real-time VC. Our FastS2S-VC is
based on a model that converts one mel-spectrogram into

3http://www.kecl.ntt.co.jp/people/kameoka.hirokazu/Demos/convs2s-vc2/
http://www.kecl.ntt.co.jp/people/kameoka.hirokazu/Demos/transformer-vc2/
http://www.kecl.ntt.co.jp/people/kameoka.hirokazu/Demos/fasts2s-vc/

[1] Y. Stylianou, O. Capp´e, and E. Moulines, “Continuous probabilistic
transform for voice conversion,” IEEE Trans. SAP, vol. 6, no. 2, pp.
131–142, 1998.

[2] T. Toda, A. W. Black, and K. Tokuda, “Voice conversion based on
maximum-likelihood estimation of spectral parameter trajectory,” IEEE
Transactions on Audio, Speech, and Language Processing, vol. 15, no. 8,
pp. 2222–2235, 2007.

[3] E. Helander, T. Virtanen, J. Nurminen, and M. Gabbouj, “Voice conver-
sion using partial least squares regression,” IEEE Transactions on Audio,
Speech, and Language Processing, vol. 18, no. 5, pp. 912–921, 2010.

[4] X. Tian, S. W. Lee, Z. Wu, E. S. Chng, and H. Li, “An exemplar-
based approach to frequency warping for voice conversion,” IEEE/ACM
Transactions on Audio, Speech, and Language Processing, vol. 25,
no. 10, pp. 1863–1876, 2017.

[5] R. Takashima, T. Takiguchi, and Y. Ariki, “Exemplar-based voice
conversion in noisy environment,” in Proc. IEEE Spoken Language
Technology Workshop (SLT), 2012, pp. 313–317.

[6] B. Sisman, H. Li, and K. C. Tan, “Sparse representation of phonetic fea-
tures for voice conversion with and without parallel data,” in Proc. IEEE
Automatic Speech Recognition and Understanding Workshop (ASRU),
2017, pp. 677–684.

[7] S. Desai, A. W. Black, B. Yegnanarayana, and K. Prahallad, “Spectral
mapping using artiﬁcial neural networks for voice conversion,” IEEE
Transactions on Audio, Speech, and Language Processing, vol. 18, no. 5,
pp. 954–964, 2010.

[8] S. H. Mohammadi and A. Kain, “Voice conversion using deep neural
networks with speaker-independent pre-training,” in Proc. IEEE Spoken
Language Technology Workshop (SLT), 2014, pp. 19–23.

[9] L. Sun, S. Kang, K. Li, and H. Meng, “Voice conversion using deep
bidirectional long short-term memory based recurrent neural networks,”
in Proc. International Conference on Acoustics, Speech and Signal
Processing (ICASSP), 2015, pp. 4869–4873.

[10] H. Ming, D. Huang, L. Xie, J. Wu, M. Dong, and H. Li, “Deep
bidirectional LSTM modeling of timbre and prosody for emotional voice
conversion,” in Proc. Annual Conference of the International Speech
Communication Association (Interspeech), 2016, pp. 2453–2457.
[11] T. Kinnunen, L. Juvela, P. Alku, and J. Yamagishi, “Non-parallel voice
conversion using i-vector plda: Towards unifying speaker veriﬁcation
and transformation,” in Proc. IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP), 2017, pp. 5535–5539.

[12] T. Nakashika, T. Takiguchi, and Y. Ariki, “High-order sequence mod-
eling using speaker-dependent recurrent temporal restricted Boltzmann
machines for voice conversion,” in Proc. Annual Conference of the
International Speech Communication Association (Interspeech), 2014,
pp. 2278–2282.

[13] L. Sun, K. Li, H. Wang, S. Kang, and H. Meng, “Phonetic posterior-
grams for many-to-one voice conversion without parallel data training,”
in Proc. International Conference on Multimedia and Expo (ICME),
2016, pp. 1–6.

[14] C.-C. Hsu, H.-T. Hwang, Y.-C. Wu, Y. Tsao, and H.-M. Wang, “Voice
conversion from non-parallel corpora using variational auto-encoder,”
in Proc. Asia-Paciﬁc Signal and Information Processing Association
Annual Summit and Conference (APSIPA ASC), 2016, pp. 1–6.

[15] C.-C. Hsu, H.-T. Hwang, Y.-C. Wu, Y. Tsao, and H.-M. Wang,
“Voice conversion from unaligned corpora using variational autoen-
coding Wasserstein generative adversarial networks,” in Proc. Annual
the International Speech Communication Association
Conference of
(Interspeech), 2017, pp. 3364–3368.

[16] A. van den Oord, O. Vinyals, and K. Kavukcuoglu, “Neural discrete
representation learning,” in Adv. Neural Information Processing Systems
(NIPS), 2017.

[17] Y. Saito, Y. Ijima, K. Nishida, and S. Takamichi, “Non-parallel voice
conversion using variational autoencoders conditioned by phonetic
posteriorgrams and d-vectors,” in Proc. International Conference on
Acoustics, Speech and Signal Processing (ICASSP), 2018, pp. 5274–
5278.

[18] H. Kameoka, T. Kaneko, K. Tanaka, and N. Hojo, “ACVAE-VC:
Non-parallel voice conversion with auxiliary classiﬁer variational au-
toencoder,” IEEE/ACM Transactions on Audio, Speech, and Language
Processing, vol. 27, no. 9, pp. 1432–1443, 2019.

[19] P. L. Tobing, Y.-C. Wu, T. Hayashi, K. Kobayashi, and T. Toda,
“Non-parallel voice conversion with cyclic variational autoencoder,” in
Proc. Annual Conference of the International Speech Communication
Association (Interspeech), 2019, pp. 674–678.

[20] K. Qian, Y. Zhang, S. Chang, X. Yang, and M. Hasegawa-Johnson,
“AutoVC: Zero-shot voice style transfer with only autoencoder loss,”
in Proc. International Conference on Machine Learning (ICML), 2019,
pp. 5210–5219.

[21] T. Kaneko and H. Kameoka, “CycleGAN-VC: Non-parallel voice con-
version using cycle-consistent adversarial networks,” in Proc. European
Signal Processing Conference (EUSIPCO), 2018, pp. 2100–2104.
[22] H. Kameoka, T. Kaneko, K. Tanaka, and N. Hojo, “StarGAN-VC: Non-
parallel many-to-many voice conversion using star generative adversarial
networks,” in Proc. IEEE Spoken Language Technology Workshop (SLT),
2018, pp. 266–273.

12

[23] J. Serr`a, S. Pascual, and C. Segura, “Blow: A single-scale hy-
raw-audio voice conversion,”

perconditioned ﬂow for non-parallel
arXiv:1906.00794 [cs.LG], Jun. 2019.

[24] H. Kameoka, T. Kaneko, K. Tanaka, N. Hojo, and S. Seki, “VoiceGrad:
Non-parallel any-to-many voice conversion with annealed langevin
dynamics,” arXiv:2010.02977 [cs.SD], 2020.

[25] B. Sisman, J. Yamagishi, S. King, and H. Li, “An overview of voice con-
version and its challenges: From statistical modeling to deep learning,”
arXiv:2008.03648 [eess.AS], 2020.

[26] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning
with neural networks,” in Adv. Neural Information Processing Systems
(NIPS), 2014, pp. 3104–3112.

[27] J. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio,
“Attention-based models for speech recognition,” in Adv. Neural Infor-
mation Processing Systems (NIPS), 2015, pp. 577–585.

[28] H. Miyoshi, Y. Saito, S. Takamichi, and H. Saruwatari, “Voice con-
version using sequence-to-sequence learning of context posterior prob-
abilities,” in Proc. Annual Conference of
the International Speech
Communication Association (Interspeech), 2017, pp. 1268–1272.
[29] J.-X. Zhang, Z.-H. Ling, L.-J. Liu, Y. Jiang, and L.-R. Dai, “Sequence-
to-sequence acoustic modeling for voice conversion,” arXiv:1810.06865
[cs.SD], Oct. 2018.

[30] M. Zhang, X. Wang, F. Fang, H. Li, and J. Yamagishi, “Joint training
framework for text-to-speech and voice conversion using multi-source
Tacotron and WaveNet,” in Proc. Annual Conference of the International
Speech Communication Association (Interspeech), 2019, pp. 1298–1302.
[31] F. Biadsy, R. J. Weiss, P. J. Moreno, D. Kanevsky, and Y. Jia, “Parrotron:
An end-to-end speech-to-speech conversion model and its applications
to hearing-impaired speech and speech separation,” in Proc. Annual
Conference of
the International Speech Communication Association
(Interspeech), 2019, pp. 4115–4119.

[32] S. Liu, Y. Cao, D. Wang, X. Wu, X. Liu, and H. Meng, “Any-to-many
voice conversion with location-relative sequence-to-sequence modeling,”
arXiv:2009.02725 [eess.AS], 2020.

[33] K. Tanaka, H. Kameoka, T. Kaneko, and N. Hojo, “AttS2S-VC:
Sequence-to-sequence voice conversion with attention and context
preservation mechanisms,” in Proc. International Conference on Acous-
tics, Speech and Signal Processing (ICASSP), 2019, pp. 6805–6809.

[34] H. Kameoka, K. Tanaka, T. Kaneko, and N. Hojo, “ConvS2S-
sequence-to-sequence voice conversion,”

VC: Fully convolutional
arXiv:1811.01609 [cs.SD], Nov. 2018.

[35] H. Kameoka, K. Tanaka, D. Kwa´sny, T. Kaneko, and N. Hojo,
“ConvS2S-VC: Fully convolutional sequence-to-sequence voice con-
version,” IEEE/ACM Transactions on Audio, Speech, and Language
Processing, vol. 28, pp. 1849–1863, 2020.

[36] W.-C. Huang, T. Hayashi, Y.-C. Wu, H. Kameoka, and T. Toda,
“Voice transformer network: Sequence-to-sequence voice conversion
using transformer with text-to-speech pretraining,” in Proc. Annual
Conference of
the International Speech Communication Association
(Interspeech), 2020.

[37] H. Kameoka, W.-C. Huang, K. Tanaka, T. Kaneko, N. Hojo, and T. Toda,
“Many-to-many voice transformer network,” IEEE/ACM Transactions on
Audio, Speech, and Language Processing, vol. 29, pp. 656–670, 2020.
[38] Y. Wang, R. Skerry-Ryan, D. Stanton, Y. Wu, R. J. Weiss, N. Jaitly,
Z. Yang, Y. Xiao, Z. Chen, S. Bengio, Q. Le, Y. Agiomyrgiannakis,
R. Clark, and R. A. Saurous, “Tacotron: Towards end-to-end speech
synthesis,” in Proc. Annual Conference of the International Speech
Communication Association (Interspeech), 2017, pp. 4006–4010.
[39] S. O. Arık, M. Chrzanowski, A. Coates, G. Diamos, A. Gibiansky,
Y. Kang, X. Li, J. Miller, A. Ng, J. Raiman, S. Sengupta, and
M. Shoeybi, “Deep Voice: Real-time neural text-to-speech,” in Proc.
International Conference on Machine Learning (ICML), 2017.

[40] S. O. Arık, G. Diamos, A. Gibiansky, J. Miller, K. Peng, W. Ping,
J. Raiman, and Y. Zhou, “Deep Voice 2: Multi-speaker neural text-to-
speech,” in Adv. Neural Information Processing Systems (NIPS), 2017.
[41] J. Sotelo, S. Mehri, K. Kumar, J. F. Santos, K. Kastner, A. Courville,
and Y. Bengio, “Char2Wav: End-to-end speech synthesis,” in Proc.
International Conference on Learning Representations (ICLR), 2017.

[42] H. Tachibana, K. Uenoyama, and S. Aihara, “Efﬁciently trainable text-
to-speech system based on deep convolutional networks with guided
attention,” in Proc. International Conference on Acoustics, Speech and
Signal Processing (ICASSP), 2018, pp. 4784–4788.

[43] W. Ping, K. Peng, A. Gibiansky, S. O. Arık, A. Kannan, S. Narang,
J. Raiman, and J. Miller, “Deep Voice 3: Scaling text-to-speech with
convolutional sequence learning,” in Proc. International Conference on
Learning Representations (ICLR), 2018.

13

[44] J. Gu, J. Bradbury, C. Xiong, V. O. Li, and R. Socher, “Non-
autoregressive neural machine translation,” in Proc. International Con-
ference on Learning Representations (ICLR), 2018.

[45] N. Chen, S. Watanabe, J. Villalba, and N. Dehak, “Listen and ﬁll in the
missing letters: Non-autoregressive transformer for speech recognition,”
arXiv:1911.04908 [eess.AS], 2020.

[46] Y. Ren, Y. Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y. Liu,
“FastSpeech: Fast, robust and controllable text to speech,” in Adv. Neural
Information Processing Systems (NeurIPS), 2019.

[47] Y. Ren, C. Hu, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y. Liu,
to speech,”

“FastSpeech 2: Fast and high-quality end-to-end text
arXiv:2006.04558 [eess.AS], 2020.

[48] I. Elias, H. Zen, J. Shen, Y. Zhang, Y. Jia, R. Weiss, and Y. Wu, “Parallel
Tacotron: Non-autoregressive and controllable TTS,” arXiv:2010.11439
[cs.SD], 2020.

[49] R. Yamamoto, E. Song, and J.-M. Kim, “Parallel WaveGAN: A fast
waveform generation model based on generative adversarial networks
with multi-resolution spectrogram,” in Proc. International Conference
on Acoustics, Speech and Signal Processing (ICASSP), 2020, pp. 6199–
6203.

[50] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
L. Kaiser, and I. Polosukhin, “Attention is all you need,” in Adv. Neural
Information Processing Systems (NIPS), 2017.

[51] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by
jointly learning to align and translate,” in Proc. International Conference
on Learning Representations (ICLR), 2015.

[52] T. Salimans and D. P. Kingma, “Weight normalization: A simple
reparameterization to accelerate training of deep neural networks,” in
Adv. Neural Information Processing Systems (NIPS), 2016, pp. 901–909.
[53] Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier, “Language modeling
with gated convolutional networks,” in Proc. International Conference
on Machine Learning (ICML), 2017, pp. 933–941.

[54] A. Graves, “Generating sequences with recurrent neural networks,”

arXiv:1308.0850 [cs.NE], 2013.

[55] S. Vasquez and M. Lewis, “Melnet: A generative model for audio in the

frequency domain,” arXiv:1906.01083 [eess.AS], 2019.

[56] E. Battenberg, R. Skerry-Ryan, S. Mariooryad, D. Stanton, D. Kao, and
T. B. Matt Shannon, “Location-relative attention mechanisms for robust
long-form speech synthesis,” in Proc. IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP), 2020, pp. 6194–
6198.

[57] J. Kominek and A. W. Black, “The CMU Arctic speech databases,” in
Proc. ISCA Speech Synthesis Workshop (SSW), 2004, pp. 223–224.
[58] A. Kurematsu, K. Takeda, Y. Sagisaka, S. Katagiri, H. Kuwabara,
and K. Shikano, “ATR Japanese speech database as a tool of speech
recognition and synthesis,” Speech Communication, vol. 9, no. 4, pp.
357–363, Aug. 1990.

[59] K. Kobayashi and T. Toda, “sprocket: Open-source voice conversion

software,” in Proc. Odyssey, 2018, pp. 203–210.

[60] J. Lorenzo-Trueba, J. Yamagishi, T. Toda, D. Saito, F. Villavicen-
cio, T. Kinnunen, and Z. Ling, “The voice conversion challenge
2018: Promoting development of parallel and nonparallel methods,”
arXiv:1804.04262 [eess.AS], Apr. 2018.

[61] D. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
in Proc. International Conference on Learning Representations (ICLR),
2015.

[62] D. J. Hermes, “Measuring the perceptual similarity of pitch contours,”

J. Speech Lang. Hear. Res., vol. 41, no. 1, pp. 73–82, 1998.

[63] H. Kameoka, T. Kaneko, K. Tanaka, and N. Hojo, “Nonparallel voice
conversion with augmented classiﬁer star generative adversarial net-
works,” IEEE/ACM Transactions on Audio, Speech, and Language
Processing, vol. 28, pp. 2982–2995, 2020.

