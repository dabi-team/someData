8
1
0
2

n
u
J

2
2

]

R
P
.
h
t
a
m

[

1
v
8
0
6
8
0
.
6
0
8
1
:
v
i
X
r
a

On generalized ARCH model with stationary
liquidity

Pauliina Ilmonen∗ and Soledad Torres† and Ciprian Tudor‡
and Lauri Viitasaari§ and Marko Voutilainen∗

June 25, 2018

Abstract

We study a generalized ARCH model with liquidity given by a general stationary
process. We provide minimal assumptions that ensure the existence and unique-
ness of the stationary solution. In addition, we provide consistent estimators for
the model parameters by using AR(1) type characterisation. We illustrate our
results with several examples and simulation studies.

AMS 2010 Mathematics Subject Classiﬁcation: (Primary) 60G10, (Secondary) 62M10,
62G05

Keywords: ARCH model, stationarity, estimation, consistency

1

Introduction

The ARCH and GARCH models have become important tools in time series analysis.
The ARCH model has been introduced by Engle in [5] and then it has been generalized
by Bollerslev to the GARCH model in [2]. Since, a large collection of variants and
extensions of these models has been produced by many authors. See for example [3]
for a glossary of models derived from ARCH and GARCH.

In this work, we also focus on a generalized ARCH model, namely the model (1).
Our contribution proposes to include in the expression of the squared volatility σ2
t a
factor Lt−1, which we will call liquidity. The motivation to consider such a model
comes from mathematical ﬁnance, where the factor Lt, which constitutes a proxi for
the trading volume at day t, has been included in order to capture the ﬂuctuations of the
intra-day price in ﬁnancial markets. A more detailed explanation can be found in [1] or

∗Department of Mathematics and Systems Analysis, Aalto University School of Science, Finland
†CIMFAV, Facultad de Ingeniería, Universidad de Valparaíso, Valparaiso, Chile
‡UFR Mathématiques, Université de Lille 1, France
§Department of Mathematics and Statistics, University of Helsinki, Finland

1

 
 
 
 
 
 
[9]. In the work [1] we considered the particular case when Lt is the squared increment
t )2, where
of the fractional Brownian motion (fBm in the sequel), i.e. Lt = (BH
BH is a fBm with Hurst parameter H ∈ (0, 1).

t+1 − BH

In this work, our purpose is twofold. Firstly, we enlarge the ARCH with fBm
liquidity in [1] by considering, as a proxi for the liquidity, a general positive (strictly)
stationary process (Lt)t∈Z. This includes, besides the above mentioned case of the
squared increment of the fBm, many other examples.

The second purpose is to provide a method to estimate the parameters of the model.
As mentioned in [1], in the case when L is a process without independent increments,
the usual approaches for the parameter estimation in ARCH models (such as least
squares method and maximum likelihood method) do not work, in the sense that the
estimators obtained by these classical methods are biased and not consistent. Here we
adopt a different technique, based on the AR(1) characterization of the ARCH process,
which has also been used in [11]. The AR(1) characterization leads to Yule-Walker
type equations for the parameters of the model. These equations are of quadratic form
and then we are able to ﬁnd explicit formulas for the estimators. We prove that the
estimators are consistent by using extended version of the law of large numbers and by
assuming enough regularity for the correlation structure of the liquidity process. We
also provide a numerical analysis of the estimators.

The rest of the paper is organised as follows. In Section 2 we introduce our model
and prove the existence and uniqueness of the stationary solution. We also provide
necessary and sufﬁcient conditions for the existence of the autocovariance function.
We derive the AR(1) characterization and Yule-Walker type equations for the param-
eters of the model. Section 3 is devoted to the estimation of the model parameters.
We construct estimators in a closed form and we prove their consistency via extended
versions of the law of large numbers and a control of the behaviour of the covariance
of the liquidity process. Several examples are discussed in details. In particular, we
study squared increments of the fBm, squared increments of the compensated Poisson
process, and the squared increments of the Rosenblatt process. We end the paper with
a numerical analysis of our estimators.

2 The model

The generalized ARCH model is deﬁned for every t ∈ Z as

Xt = σt(cid:15)t,

t = α0 + α1X 2
σ2

t−1 + l1Lt−1,

(1)

where α0 ≥ 0, α1, l1 > 0, and ((cid:15)t)t∈Z is an i.i.d. process with E((cid:15)0) = 0 and E((cid:15)2
0) =
1. Moreover, (Lt)t∈Z is a strictly stationary positive process with E(L0) = 1 and
independent of ((cid:15)t)t∈Z. We ﬁrst give sufﬁcient conditions to ensure the existence of a
stationary solution. Note that we have a recursion

t = α0 + α1(cid:15)2
σ2

t−1σ2

t−1 + l1Lt−1.

(2)

2

Let us denote

At = α1(cid:15)2
t

and Bt = α0 + l1Lt

for every t ∈ Z.

Using (2) k + 1 times we get

t−1 + AtBt−1 + Bt

t+1 = Atσ2
σ2

t + Bt
= AtAt−1σ2
= . . .
(cid:32) k
(cid:89)

At−i

=

(cid:33)

σ2
t−k +

k
(cid:88)

(cid:32)i−1
(cid:89)

i=0

j=0

(cid:33)

At−j

Bt−i,

(3)

with the convention (cid:81)−1

0 = 1.

i=0

The following lemma ensures that we are able to continue the recursion inﬁnitely

many times.
Lemma 2.1. Suppose α1 < 1 and supt∈Z E(σ2
have

t ) ≤ M1 < ∞. Then, as k → ∞, we

(cid:33)

At−i

σ2
t−k → 0

(cid:32) k
(cid:89)

i=0

in L1. Furthermore, if α1 < 1√
gence holds also almost surely.

E((cid:15)4
0)

and supt∈Z E(σ4

t ) ≤ M2 < ∞, then the conver-

Proof. By independence of (cid:15), we have
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:32) k
(cid:89)

σ2
t−k

At−i

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:33)

E

i=0

= αk+1
1

E(σ2

t−k) ≤ αk+1

1 M1 → 0

proving the ﬁrst part of the claim. For the second part, Chebysev’s inequality implies

P

(cid:32)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:32) k
(cid:89)

i=0

(cid:33)

At−i

t−k − αk+1
σ2

1

E(σ2

(cid:12)
(cid:12)
(cid:12)
t−k)
(cid:12)
(cid:12)

(cid:33)

Var

> ε

≤

(cid:16)(cid:16)(cid:81)k

(cid:17)

(cid:17)

σ2

t−k

i=0 At−i
ε2
(cid:16)(cid:16)(cid:81)k

α2k+2
1

E

i=0 (cid:15)4

t−i

(cid:17)

(cid:17)

σ4

t−k

− α2k+2
1

E(σ2

t−k)2

=

≤

(cid:0)α2

1

E((cid:15)4

0)(cid:1)k+1

ε2
1 M 2
1

M2 − α2k+2
ε2

,

which is summable by assumptions. Borel-Cantelli then implies

(cid:33)

At−i

t−k − αk+1
σ2

1

E(σ2

t−k) → 0

(cid:32) k
(cid:89)

i=0

almost surely proving the claim.

3

2.1 Existence of a stationary solution

The following theorem gives the existence of a stationary solution under relatively
weak assumptions (we only assume the existence of the second moment of L and the
usual condition α1 < 1 (see e.g. [6])).

Theorem 2.2. Assume that E(L2
strictly stationary solution

0) < ∞ and α1 < 1. Then (1) has the following

σ2
t+1 =

∞
(cid:88)

(cid:32)i−1
(cid:89)

i=0

j=0

(cid:33)

At−j

Bt−i.

(4)

Proof. We begin by showing that (4) is well-deﬁned. That is, we prove that

k
(cid:88)

(cid:32)i−1
(cid:89)

i=0

j=0

lim
k→∞

(cid:33)

At−j

Bt−i

deﬁnes an almost surely ﬁnite random variable. First we observe that the summands
above are non-negative and hence, the pathwise limits exist in [0, ∞]. Write

∞
(cid:88)

(cid:32)i−1
(cid:89)

i=0

j=0

(cid:33)

At−j

Bt−i = α0

∞
(cid:88)

(cid:32)i−1
(cid:89)

(cid:33)

At−j

+ l1

∞
(cid:88)

(cid:32)i−1
(cid:89)

i=0

j=0

i=0

j=0

(cid:33)

At−j

Lt−i

(5)

and denote

an =

(cid:32)n−1
(cid:89)

j=0

(cid:33)

At−j

Lt−n,

bn =

(cid:33)

At−j

.

(cid:32)n−1
(cid:89)

j=0

By the root test it sufﬁces to prove that

1
n

n < 1
a

lim sup
n→∞

lim sup
n→∞

b

1
n

n < 1.

(6)

(7)

1
n

n = e

a

1

n log an = L

1
n

t−ne

1
n

(cid:80)n−1

j=0 log At−j ,

and

Here

where

1
n

e

(cid:80)n−1

j=0 log At−j a.s−→ eE log A0 = α1eE log (cid:15)2

0

by the law of large numbers and continuous mapping theorem. By Jensen’s inequality
we obtain that

4

α1eE log (cid:15)2

0 ≤ α1elog E((cid:15)2

0) = α1 < 1.

That is

1
n

e

(cid:80)n−1

j=0 log At−j < 1

lim
n→∞

almost surely. This proves (7) which implies that the ﬁrst series in (5) is almost surely
t−n ≤ 1 almost surely. We
convergent. To obtain (6), it remains to show lim supn→∞ L
have

1
n

1
n

t−n = 1Lt−n<1L
L

1
n

t−n + 1Lt−n≥1L

where we have used

1
n

t−n ≤ 1 + 1Lt−n≥1L

t−n − 1Lt−n≥1

1
n

(8)

1Lt−n<1L

1
n

t−n ≤ 1Lt−n<1 = 1 − 1Lt−n≥1.

Now

P

(cid:16)(cid:12)
(cid:12)
(cid:12)

1Lt−n≥1L

t−n − 1Lt−n≥1

1
n

E

(cid:12)
(cid:12)
(cid:12)

(cid:17)

(cid:12)
(cid:12)
(cid:12) ≥ ε

≤

1Lt−n≥1L

(cid:18)

E

1Lt−n≥1

=

2

(cid:12)
(cid:12)
(cid:12)

1
n

t−n − 1Lt−n≥1
ε2
(cid:16)

(cid:17)2(cid:19)

L

t−n − 1

1
n

ε2

.

Consider now the function fx(a) := xa for x ≥ 1 and a ≥ 0. Since f (cid:48)
we obtain by the mean value theorem that

x(a) = xa log x

|fx(a) − fx(0)| ≤ max
0≤b≤a

|f (cid:48)

x(b)| a = axa log x.

Hence

1
n2 L
On the other hand, for n ≥ 2 and Lt−n ≥ 1 it holds that

≤ 1Lt−n≥1

1Lt−n≥1

t−n − 1

L

2
n

1
n

(cid:17)2

(cid:16)

t−n (log Lt−n)2 .

2
n

t−n (log Lt−n)2
L

L2

t−n

≤

(log Lt−n)2
Lt−n

< 1,

since for x ≥ 1, the function g(x) := (log x)2 x−1 has the maximum g(e2) = 4e−2.
Consequently,

(cid:18)

E

(cid:16)

1Lt−n≥1

(cid:17)2(cid:19)

1
n

L

t−n − 1

ε2

E (cid:0)1Lt−n≥1L2
ε2n2

t−n

(cid:1)

<

(cid:1)

E (cid:0)L2
t−n
ε2n2

.

≤

5

Hence Borel-Cantelli implies

1Lt−n≥1L

1
n

t−n − 1Lt−n≥1

a.s.→ 0

which by (8) shows (6). Let us next show that (4) satisﬁes (2).

Atσ2

t + Bt =

=

=

∞
(cid:88)

(cid:32) i

(cid:89)

i=0

∞
(cid:88)

j=0
(cid:32)i−1
(cid:89)

i=1

∞
(cid:88)

j=0
(cid:32)i−1
(cid:89)

i=0

j=0

(cid:33)

At−j

Bt−i−1 + Bt

(cid:33)

At−j

Bt−i + Bt

(cid:33)

At−j

Bt−i = σ2

t+1.

It remains to prove that (4) is stationary. However, since (At, Bt) is stationary, we have

k
(cid:88)

(cid:32)i−1
(cid:89)

i=0

j=0

(cid:33)

At−j

Bt−i

law=

k
(cid:88)

(cid:32)i−1
(cid:89)

i=0

j=0

(cid:33)

A−j

B−i

for every t and k. Since the limits of the both sides exist as k → ∞ we have

σ2
t+1 =

∞
(cid:88)

(cid:32)i−1
(cid:89)

i=0

j=0

(cid:33)

At−j

Bt−i

law=

∞
(cid:88)

(cid:32)i−1
(cid:89)

i=0

j=0

(cid:33)

A−j

B−i = σ2
1.

Treating multidimensional distributions similarly concludes the proof.

We show below that the stationary solution is unique in some class of processes.

Corollary 2.3. Suppose α1 < 1 and E(L2
by (4) in the class of processes satisfying supt∈Z E(σ2

t ) < ∞.

0) < ∞. Then (1) has a unique solution given

Proof. By Theorem 2.2 (4) provides a stationary solution. Hence it remains to prove
the uniqueness. By (3) we have for every t ∈ Z and k ∈ {0, 1, . . .} that

σ2
t+1 =

(cid:33)

At−i

σ2
t−k +

(cid:32) k
(cid:89)

i=0

k
(cid:88)

(cid:32)i−1
(cid:89)

i=0

j=0

(cid:33)

At−j

Bt−i.

Suppose now that there exists two solutions σ2
and supt∈Z E(˜σ2
t ) < ∞. Then

t and ˜σ2

t satisfying supt∈Z E(σ2

t ) < ∞

|σ2

t+1 − ˜σ2

t+1| ≤

(cid:33)

At−i

σ2
t−k +

(cid:32) k
(cid:89)

i=0

(cid:33)

At−i

˜σ2
t−k.

(cid:32) k
(cid:89)

i=0

6

As both terms on the right-side converges in L1 to zero by Lemma 2.1, we observe that

E|σ2

t+1 − ˜σ2

t+1| = 0

for all t ∈ Z which implies the result.

Remark 2.4. We assumed that the liquidity (Lt)t∈Z is a strictly stationary sequence.
Nevertheless, the results in this section can be obtained by assuming that (Lt)t∈Z is
weakly stationary (i.e., we have the shift-invariance in time of the ﬁrst and second
moments of the process). That is, by assuming weak stationarity of the noise, we
obtain weak stationarity of the volatility (σ2
t )t∈Z in Theorem 2.2. We prefer to keep
the assumption of strict stationarity because it is needed later to simplify the third
and fourth order assumptions of Lemma 3.5 and also because our main examples of
liquidities are strictly stationary processes (see Section 3.3)

In the sequel, we consider the stationary solution (σ2

t )t∈Z given by Theorem 2.2.

Therefore, we will always implicitly assume that

0) < ∞ and α1 < 1.
In order to study covariance function of the solution (4), we need that the moments
t ) exists. Necessary and sufﬁcient conditions for this are given in the following

E(L2

E(σ4
lemma.

Lemma 2.5. Suppose E((cid:15)4

0) < ∞. Then E(σ4

0) < ∞ if and only if α1 < 1√

E((cid:15)4
0)

.

Proof. Denote E((cid:15)4
stationary solution

0) = C(cid:15) and E(L2

0) = CL. By the deﬁnition (4) of the strictly

E(σ4

t+1) = E

(cid:32) ∞
(cid:88)

(cid:32)i−1
(cid:89)

(cid:33)

(cid:33)2

At−j

Bt−i

,

i=0

j=0

and since all the terms above are positive, both sides are simultaneously ﬁnite or in-
ﬁnite. Note also that, as the terms all positive, we may apply Tonelli’s theorem to
change the order of summation and integration obtaining

E(σ4

t+1) =

+

(cid:32)i−1
(cid:89)





j=0
(cid:32)(cid:32)i−1
(cid:89)

j=0

∞
(cid:88)

E

i=0

∞
(cid:88)

E

i,k=0
i(cid:54)=k

(cid:33)2



At−j

B2

t−i



(cid:33)

At−j

Bt−i

(cid:32)k−1
(cid:89)

j=0

(cid:33)

(cid:33)

At−j

Bt−k

.

(9)

Let us begin with the ﬁrst term above. By independence, we obtain

7

∞
(cid:88)

E

(cid:32)(cid:32)i−1
(cid:89)

i=0

j=0

(cid:33)

(cid:33)

A2

t−j

B2

t−i

=

∞
(cid:88)

(cid:32)i−1
(cid:89)

(cid:33)

α2

1C(cid:15)

E(B2

t−i)

i=0

j=0
∞
(cid:88)

= E(B2
0)

(α2

1C(cid:15))i.

(10)

Consequently, E(σ4
, since it is the radius of convergence of
the series above. For the converse, consider the latter term in (9). By Cauchy-Schwarz
inequality we obtain

0) < ∞ implies α1 < 1√
C(cid:15)

i=0

∞
(cid:88)

E

(cid:32)(cid:32)i−1
(cid:89)

(cid:33)

At−j

Bt−i

(cid:33)

(cid:33)

At−j

Bt−k

(cid:32)k−1
(cid:89)

j=0

i,k=0
i(cid:54)=k

j=0

∞
(cid:88)

≤

(cid:118)
(cid:117)
(cid:117)
(cid:116)E

(cid:32)(cid:32)i−1
(cid:89)

i,k=0
i(cid:54)=k

j=0

(cid:33)

A2

t−j

B2

t−i

(cid:33)(cid:118)
(cid:117)
(cid:117)
(cid:116)E

(cid:32)(cid:32)k−1
(cid:89)

(cid:33)

(cid:33)

A2

t−j

B2

t−k

j=0

= E(B2
0)

∞
(cid:88)

(α2

1C(cid:15))

i

2 (α2

1C(cid:15))

k
2 ,

where

i,k=0
i(cid:54)=k

∞
(cid:88)

(α2

1C(cid:15))

i,k=0
i(cid:54)=k

i

2 (α2

1C(cid:15))

k
2 <

∞
(cid:88)

i=0

(α1C

1
2

(cid:15) )i

∞
(cid:88)

(α1C

1
2

(cid:15) )k.

k=0

Together with (10) this shows that if α1 < 1√
C(cid:15)
E(σ4

0) < ∞.

, all the series are convergent and thus

Remark 2.6. As expected, in order to have ﬁnite moments of higher order we needed
to pose more restrictive assumption α1 < 1√
0) = 1. For example, in
the case of Gaussian innovations we obtain the well-known condition α1 < 1√
(see
3
e.g. [6] or [7]). An explicit expression of the fourth moment can be obtained when L
is the squared increment of fBm (see Lemma 4 in [1]).

≤ 1 as E((cid:15)2

E((cid:15)4
0)

2.2 Computation of the model parameters

In this section we compute the parameters α0, α1, l1 in (1) by using the aucovariance
functions of X 2 and L. To this end, we use an AR(1) characterization of the ARCH

8

process. From this characterization, we derive, using an idea from [11], a Yule -Walker
equation of quadratic form for the parameters, that we can solve explicitly. This con-
stitutes the basis of the construction of the estimators in the next section. From (1) it
follows that if (σ2

t )t∈Z is stationary, then so is (X 2

t )t∈Z. In addition

X 2

t = σ2

t (cid:15)2
t − σ2
= α0 + α1X 2

t + α0 + α1X 2
t−1 + σ2

t−1 + l1Lt−1
t − 1) + l1Lt−1.

t ((cid:15)2

E(X 2

t ) = α0 + α1E(X 2

t−1) + l1

Now

and hence

Let us deﬁne an auxiliary process (Yt)t∈Z by

E(X 2

t ) =

α0 + l1
1 − α1

.

α0 + l1
1 − α1
Now Y is a zero-mean stationary process satisfying

Yt = X 2

t −

.

(11)

(12)

Yt = α1Yt−1 + α0 + σ2

t ((cid:15)2

t − 1) + l1Lt−1 −

α0 + l1
1 − α1

+ α1

α0 + l1
1 − α1

(13)

= α1Yt−1 + σ2

t ((cid:15)2

t − 1) + l1(Lt−1 − 1).

By denoting

we may write

Zt = σ2

t ((cid:15)2

t − 1) + l1(Lt−1 − 1)

Yt = α1Yt−1 + Zt

corresponding to the AR(1) characterization ([11]) of Yt for 0 < α1 < 1.
In what follows, we denote the autocovariance functions of X 2 and L with γ(n) =
)2 and s(n) = E(LnLt+n) − 1 respectively.
E(X 2

t X 2

t+n) − ( α0+l1
1−α1

Lemma 2.7. Suppose E((cid:15)4

0) < ∞ and α1 < 1√

E((cid:15)4
0)

. Then for any n (cid:54)= 0 we have

1γ(n) − α1(γ(n + 1) + γ(n − 1)) + γ(n) − l2
α2

1s(n) = 0

(14)

and for n = 0 it holds that

α2

1γ(0) − 2α1γ(1) + γ(0) −

E(X 4

0 )V ar((cid:15)2
0)
E((cid:15)4
0)

− l2

1s(0) = 0.

(15)

9

Proof. First we notice that

E(X 4

0 ) = E(σ4

0(cid:15)4

0) = E(σ4

0)E((cid:15)4

0) < ∞

(16)

by Lemma 2.5. Hence, the stationary processes Y and Z have ﬁnite second moments.
Furthermore, the covariance of Y coincides with the one of X 2. Applying Lemma 1
of [11] we get

α2

1γ(n) − α1(γ(n + 1) + γ(n − 1)) + γ(n) − r(n) = 0
for every n ∈ Z, where r(·) is the autocovariance function of Z. For r(n) with n ≥ 1
we obtain

r(n) = E(Z1Zn+1)
= E[(σ2
= l2
1

1((cid:15)2

1 − 1) + l1(L0 − 1))(σ2
1s(n),

E[(L0 − 1)(Ln − 1)] = l2

n+1((cid:15)2

n+1 − 1) + l1(Ln − 1))]

(17)

since the sequences ((cid:15)t)t∈Z and (Lt)t∈Z are independent of each other, and (cid:15)t is inde-
pendent of σs for s ≤ t. By the same arguments, for n = 0 we have

r(0) = E

1 − 1) + l1(L0 − 1)(cid:1)2(cid:105)
(cid:104)(cid:0)σ2
1((cid:15)2
1 − 1)2(cid:3) + l2
= E (cid:2)σ4
E (cid:2)(l0 − 1)2(cid:3)
1((cid:15)2
= E(σ4
0) + l2
1)V ar((cid:15)2

1
1s(0).

(18)

Now using (16) and γ(−1) = γ(1) completes the proof.

Now, let ﬁrst n ∈ Z with n (cid:54)= 0. Then

α2

1γ(0) − 2α1γ(1) + γ(0) −

E(X 4

0 )V ar((cid:15)2
0)
E((cid:15)4
0)

− l2

1s(0) = 0

1γ(n) − α1(γ(n + 1) + γ(n − 1)) + γ(n) − l2
α2

1s(n) = 0.

(19)

From the ﬁrst equation we get

l2
1 =

1
s(0)

(cid:18)

α2

1γ(0) − 2α1γ(1) + γ(0) −

E(X 4

0 )V ar((cid:15)2
0)
E((cid:15)4
0)

(cid:19)

.

Substitution to (19) yields

(cid:18)

α2
1

γ(n) −

s(n)
s(0)

(cid:19)

γ(0)

+ α1

(cid:18)

2

s(n)
s(0)

γ(1) − (γ(n + 1) + γ(n − 1))

(cid:19)

+ γ(n) +

s(n)
s(0)

(cid:18) E(X 4

0 )V ar((cid:15)2
0)
E((cid:15)4
0)

(cid:19)

− γ(0)

= 0

10

Let us denote γγγ0 = [γ(n + 1), γ(n), γ(n − 1), γ(1), γ(0), E(X 4

0 )] and

a0(γγγ0) = γ(n) −

s(n)
s(0)

γ(0)

b0(γγγ0) = 2

s(n)
s(0)

γ(1) − (γ(n + 1) + γ(n − 1))

(20)

c0(γγγ0) = γ(n) +

s(n)
s(0)

(cid:18) E(X 4

0 )V ar((cid:15)2
0)
E((cid:15)4
0)

(cid:19)

− γ(0)

.

Assuming that a0(γγγ0) (cid:54)= 0 we have the following solutions for the model parameters
α1 and l1:

α1(γγγ0) =

−b0(γγγ0) ± (cid:112)b0(γγγ0)2 − 4a0(γγγ0)c0(γγγ0)
2a0(γγγ0)

(21)

and

l1(γγγ0) =

(cid:115)

1
s(0)

(cid:18)

α1(γγγ0)2γ(0) − 2α1(γγγ0)γ(1) + γ(0) −

E(X 4

0 )V ar((cid:15)2
0)
E((cid:15)4
0)

(cid:19)
.

(22)

Finally, denoting µ = E(X 2

0 ) and using (12) we may write

Now, let n1, n2 ∈ Z with n1 (cid:54)= n2 and n1, n2 (cid:54)= 0. Then

α0(γγγ0, µ) = µ(1 − α1(γγγ0)) − l1(γγγ0).

α2
1γ(n1) − α1(γ(n1 + 1) + γ(n1 − 1)) + γ(n1) − l2
α2
1γ(n2) − α1(γ(n2 + 1) + γ(n2 − 1)) + γ(n2) − l2

1s(n1) = 0
1s(n2) = 0.

Assuming that n2 is chosen in such a way that s(n2) (cid:54)= 0 we have

l2
1 =

α2

1γ(n2) − α1(γ(n2 + 1) + γ(n2 − 1)) + γ(n2)
s(n2)

.

Substitution to (24) yields

(23)

(24)

(25)

(cid:32)

α2
1

γ(n1) −

s(n1)
s(n2)

(cid:33)

(cid:32)

γ(n2)

− α1

γ(n1 + 1) + γ(n1 − 1) −

s(n1)
s(n2)

(γ(n2 + 1) + γ(n2 − 1))

+ γ(n1) −

(cid:33)

s(n1)
s(n2)

γ(n2) = 0.

Let us denote γγγ = [γ(n1 + 1), γ(n2 + 1), γ(n1), γ(n2), γ(n1 − 1), γ(n2 − 1)] and

a(γγγ) = γ(n1) −

s(n1)
s(n2)

γ(n2)

b(γγγ) =

s(n1)
s(n2)

(γ(n2 + 1) + γ(n2 − 1)) − (γ(n1 + 1) + γ(n1 − 1)).

(26)

11

Assuming a(γγγ) (cid:54)= 0 we obtain the following solutions for the model parameters α1
and l1:

α1(γγγ) =

−b(γγγ) ± (cid:112)b(γγγ)2 − 4a(γγγ)2
2a(γγγ)

,

and

(cid:115)

l1(γγγ) =

α2

1(γγγ)γ(n2) − α1(γγγ)(γ(n2 + 1) + γ(n2 − 1)) + γ(n2)
s(n2)

.

Again, α0 is given by

α0(γγγ, µ) = µ(1 − α1(γγγ)) − l1(γγγ).

(27)

(28)

(29)

Remark 2.8. Note that here we assumed s(n2) (cid:54)= 0 and a(γγγ) (cid:54)= 0 which means that
we choose n1, n2 in a suitable way. Notice however, that these assumptions are not a
restriction. Firstly, the case where s(n2) = 0 for all n2 (cid:54)= 0 corresponds to the more
simple case where L is a sequence of uncorrelated random variables. Secondly, if
s(n2) (cid:54)= 0 and a(γγγ) = 0, the second order term vanishes and we get a linear equation
for α1. For detailed discussion on this phenomena, we refer to [11].

Remark 2.9. At ﬁrst glimpse Equations (21) and (27) may seem useless as one needs
to choose between signs. However, it usually sufﬁces to know additional values of the
covariance of the noise (see [11]). In particular, it sufﬁces that s(n) → 0 (see [12]).

3 Parameter estimation

In this section we discuss how to estimate the model parameters consistently from
the observations provided that the covariance of the liquidity L is known. Based on
formulas for the parameters provided in Subsection 2.2, it sufﬁces that the covariances
of X 2 can be estimated consistently.

3.1 Consistency of autocovariance estimators

Throughout this section we denote

f (t − s) = E(LtLs) = Cov(Lt, Ls) + 1 = s(t − s) + 1.

Lemma 3.1. Let t, s ∈ Z. Then

E(σ2

t Ls) =

α0
1 − α1

+ l1

∞
(cid:88)

i=0

αi

1f (t − s − i − 1)

12

Proof. By (4) and Fubini-Tonelli

E(σ2

t Ls) =

∞
(cid:88)

(cid:32)i−1
(cid:89)

i=0

j=0

(cid:33)

α1E((cid:15)2

t−1−j)

E ((α0 + l1Lt−1−i)Ls)

= α0

= α0

∞
(cid:88)

i=0
∞
(cid:88)

i=0

αi

1 + l1

αi

1 + l1

∞
(cid:88)

i=0
∞
(cid:88)

i=0

αi
1

E(Lt−1−iLs)

αi

1f (t − s − i − 1),

where the series converges since α1 < 1 and E(L2

0) < ∞.

The following variant of the law of large number is needed for the proof of the

consistency of the estimators.

Lemma 3.2. Let (U1, U2, ...) be a sequence of random variables with a mutual expec-
tation. In addition, assume that Var(Uj) ≤ C and |Cov(Uj, Uk)| ≤ g(|k − j|), where
g(i) → 0 as i → ∞. Then

1
n

n
(cid:88)

k=1

Uk → E(U1)

in probability.

Proof. By Chebyshev’s inequality

P

(cid:32)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

k=1

Uk − E(U1)

(cid:33)

> ε

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Var ((cid:80)n
ε2n2

k=1 Uk)

,

where

(cid:32) n

(cid:88)

(cid:33)

Uk

=

n
(cid:88)

Var

k=1

k,j=1

Cov (Uk, Uj)

=

n
(cid:88)

k=1

Var(Uk) + 2

n
(cid:88)

k−1
(cid:88)

k=1

j=1

Cov(Uk, Uj)

≤ nC + 2

n
(cid:88)

k−1
(cid:88)

k=1

j=1

|Cov(Uk, Uj)| .

Fix δ > 0. Then, there exists Nδ ∈ N such that g(|k − j|) < δ whenever |k − j| ≥ Nδ.
Note also that by Cauchy-Schwarz it holds that |Cov(Uk, Uj)| ≤ C. Assume that
n > Nδ. Now

13

n
(cid:88)

k−1
(cid:88)

k=1

j=1

|Cov(Uk, Uj)| ≤

n
(cid:88)

k−Nδ(cid:88)

g(|k − j|) +

k=1

j=1
≤ n2δ + nNδC.

n
(cid:88)

k=1

k−1
(cid:88)

C
j=k−Nδ+1

Hence

P

(cid:32)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

k=1

(cid:12)
(cid:12)
Uk − E(U1)
(cid:12)
(cid:12)
(cid:12)

(cid:33)

> ε

≤

nC + 2n2δ + 2nNδC
ε2n2

=

2δ
ε2 + O

(cid:19)

(cid:18) 1
n

concluding the proof, since δ was arbitrary small.

Remark 3.3. Note that the convergence in Lemma 3.2 actually takes place also in
L2. However, to obtain consistency of our estimators, the convergence in probability
sufﬁces.

Assume that (X 2

N ) is an observed series from an generalized ARCH
process (Xt)t∈Z. We use the following estimator of the autocovariance function of X 2
t

2 , . . . , X 2

1 , X 2

ˆγN (n) =

1
N

N −n
(cid:88)

t=1

(cid:0)X 2

t − ¯X 2(cid:1) (cid:0)X 2

t+n − ¯X 2(cid:1)

for n ≥ 0,

where ¯X 2 is the sample mean of the observations. We show that the estimator above
is consistent in two steps. Namely, we consider the sample mean and the term

1
N

N −n
(cid:88)

t=1

X 2

t X 2

t+n

separately. If the both terms are consistent, consistency of the autocovariance estimator
follows.

Lemma 3.4. Suppose E((cid:15)4
1√

, then the sample mean

E((cid:15)4
0)

0) < ∞ and s(t) = cov(L0Lt) → 0 as t → ∞. If α1 <

ˆµN =

1
N

N
(cid:88)

t=1

X 2
t

converges in probability to E(X 2

0 ).

Proof. By Lemma 3.2 it sufﬁces to show that cov(X 2
t+1) converges to zero as t
tends to inﬁnity. For simplicity, let us assume that t ≥ 2. Now by ﬁxing k = t − 1 in
(3) we have

1 , X 2

14

X 2

t+1 = (cid:15)2

t+1

= (cid:15)2

t+1

Hence

(cid:32)(cid:32)t−1
(cid:89)

i=0
(cid:32)(cid:32)t−2
(cid:89)

i=0

(cid:33)

At−i

σ2
1 +

t−1
(cid:88)

(cid:32)i−1
(cid:89)

i=0

j=0

(cid:33)

(cid:33)

At−j

Bt−i

(cid:33)

At−i

α1X 2

1 +

t−1
(cid:88)

(cid:32)i−1
(cid:89)

i=0

j=0

(cid:33)

(cid:33)

At−j

Bt−i

.

X 2

t+1X 2

1 =

(cid:33)

α1(cid:15)2

t−i

(cid:32)t−2
(cid:89)

i=0

α1X 4

1 (cid:15)2

t+1 + (cid:15)2

t+1X 2
1

t−1
(cid:88)

(cid:32)i−1
(cid:89)

i=0

j=0

(cid:33)

α1(cid:15)2

t−j

(α0 + l1Lt−i).

Taking expectations yields

E(X 2

t+1X 2

1 ) = αt
1

E(X 4

1 ) + α0E(X 2
1 )

t−1
(cid:88)

i=0

αi

1 + l1

t−1
(cid:88)

i=0

αi
1

E(X 2

1 Lt−i).

By Lemma 3.1, and since α1 < 1 we obtain that

E(X 2

t+1X 2

1 ) = αt
1

E(X 4

0 ) + α0E(X 2
0 )

t−1
(cid:88)

i=0

αi

1 + l1



αi
1



t−1
(cid:88)

i=0

α0
1 − α1

+ l1

∞
(cid:88)

j=0



αj

1f (i − t − j)



= αt
1

E(X 4

0 ) +

(cid:18)

α0E(X 2

0 ) +

l1α0
1 − α1

(cid:19) t−1
(cid:88)

1 + l2
αi
1

t−1
(cid:88)

∞
(cid:88)

i=0

i=0

j=0

αi+j

1 f (i − t − j).

As t tends to inﬁnity

lim
t→∞

E(X 2

t+1X 2

1 ) =

α0E(X 2
0 )
1 − α1

+

l1α0

(1 − α1)2 + l2

1 lim
t→∞

t−1
(cid:88)

∞
(cid:88)

i=0

j=0

αi+j

1 f (i − t − j)

=

α2
0 + 2α0l1
(1 − α1)2 + l2

1 lim
t→∞

∞
(cid:88)

∞
(cid:88)

i=0

j=0

αi+j

1 f (i − t − j),

where we have used (12) for expectation of X 2
there exists M > 0 such that for the terms in the double sum it holds that

0 . Note that f (t) = s(t) + 1. Hence,

(cid:12)
(cid:12)αi+j

1 f (i − t − j)(cid:12)

(cid:12) ≤ M αi+j
Thus we have a uniform integrable upper bound and consequently, dominated conver-
gence theorem yields

for every i, j, t.

1

15

lim
t→∞

∞
(cid:88)

∞
(cid:88)

i=0

j=0

αi+j

1 f (i − t − j) =

∞
(cid:88)

∞
(cid:88)

i=0

j=0

αi+j

1 =

1
(1 − α1)2 .

Finally, we may conclude that

lim
t→∞

E(X 2

t+1X 2

1 ) =

(cid:19)2

(cid:18) α0 + l1
1 − α1

= E(X 2

1 )2.

Lemma 3.5. Suppose E(L4
In addition, assume that for
every ﬁxed n, n1 and n2 it holds that cov(L0, Lt) → 0, cov(L0Ln, L±t) → 0 and
cov(L0Ln1, LtLt+n2) → 0 as t → ∞. If α1 < 1
E((cid:15)8
0)

0) < ∞ and E((cid:15)8

0) < ∞.

, then

1
4

1
N − n

N −n
(cid:88)

t=1

X 2

t X 2

t+n

converges in probability to E(X 2

0 X 2

n) for every n ∈ Z.

Proof. Again, by Lemma 3.2 it sufﬁces to show that cov(X 2
0 X 2
to zero as t tends to inﬁnity. Hence we assume that t > n. By (4)

n, X 2

t X 2

t+n) converges

E(X 2

0 X 2

nX 2

t X 2

t+n) = E

∞
(cid:88)

∞
(cid:88)

∞
(cid:88)

∞
(cid:88)





i1−1
(cid:89)


 B−1−i1(cid:15)2
0

A−1−j





i2−1
(cid:89)


 Bn−1−i2 (cid:15)2
n

An−1−j

i1=0

i2=0

j=0

i4=0

i3=0

 Bt−1−i3(cid:15)2
t

j=0


 Bt+n−1−i4 (cid:15)2

t+n.

At+n−1−j





i4−1
(cid:89)

At−1−j





i3−1
(cid:89)

j=0

j=0

(30)

Since the summands are non-negative, we can take the expectation inside. Further-
more, by independence of the sequences (cid:15)t and Lt we observe

E(X 2

0 X 2

nX 2

t X 2

t+n) =

∞
(cid:88)

∞
(cid:88)

∞
(cid:88)

∞
(cid:88)

E (B−1−i1Bn−1−i2Bt−1−i3Bt+n−1−i4)

i2=0

i3=0

i4=0

i1=0
(cid:32)

E

0(cid:15)2
(cid:15)2

n(cid:15)2

t (cid:15)2

t+n

i1−1
(cid:89)

j=0

A−1−j

i2−1
(cid:89)

j=0

An−1−j

i3−1
(cid:89)

j=0

At−1−j

i4−1
(cid:89)

j=0

(cid:33)

At+n−1−j

.

(31)

Next we justify the use of the dominated convergence theorem in order to change the
order of the summations and taking the limit. Consequently, it sufﬁces to study the
limits of the terms

(cid:32)

E

0(cid:15)2
(cid:15)2

n(cid:15)2

t (cid:15)2

t+n

i1−1
(cid:89)

j=0

A−1−j

i2−1
(cid:89)

j=0

An−1−j

i3−1
(cid:89)

j=0

At−1−j

i4−1
(cid:89)

j=0

(cid:33)

At+n−1−j

·

(32)

E (B−1−i1Bn−1−i2Bt−1−i3Bt+n−1−i4) .

16

Step 1: ﬁnding summable upper bound.
First note that the latter term is bounded by a constant.
(Bt)t∈Z we can write
E (B−i1Bn−i2Bt−i3Bt+n−i4) = α4

0l1 + α2

0 + 4α3

0l2
1

Indeed, by stationarity of

(cid:0)E(L−i1Ln−i2) + E(L−i1Lt−i3)

+ E(L−i1Lt+n−i4) + E(Ln−i2Lt−i3) + E(Ln−i2Lt+n−i4)
+ E(Lt−i3Lt+n−i4)(cid:1) + α0l3
(cid:0)E(L−i1Ln−i2Lt−i3)
+ E(L−i1Lt−i3Lt+n−i4) + E(L−i1Ln−i2Lt+n−i4)
+ E(Ln−i2Lt−i3Lt+n−i4)(cid:1) + l4

1

1

E(L−i1Ln−i2Lt−i3Lt+n−i4),
(33)

which is bounded by a repeated application of Cauchy-Schwarz inequality and the fact
that the fourth moment of L0 is ﬁnite.
Consider now the ﬁrst term in (32). First we recall the elementary fact

1 = E((cid:15)2

0) ≤

(cid:113)

E((cid:15)4

0) ≤ E((cid:15)6
0)

1

3 ≤ E((cid:15)8
0)

1
4 < ∞.

(34)

Next note that the ﬁrst term in (32) is bounded for every set of indices. Indeed, this
follows from the independence of (cid:15) and the observation that we obtain terms up to
power 8 at most. That is, terms of form (cid:15)8
t ) < ∞. Let now
n > 0. Then

t and by assumption, E((cid:15)8

(cid:32)

E

0(cid:15)2
(cid:15)2

n(cid:15)2

t (cid:15)2

t+n

= E (cid:0)(cid:15)2

t+n

(cid:1) E

i1−1
(cid:89)

j=0
(cid:32)

A−1−j

i2−1
(cid:89)

j=0

An−1−j

i3−1
(cid:89)

j=0

At−1−j

i4−1
(cid:89)

j=0

(cid:33)

At+n−1−j

0(cid:15)2
(cid:15)2

n(cid:15)2
t

i1−1
(cid:89)

j=0

A−1−j

i2−1
(cid:89)

j=0

An−1−j

i3−1
(cid:89)

j=0

At−1−j

i4−1
(cid:89)

j=0

(cid:33)

At+n−1−j

≤ E (cid:0)(cid:15)2

t+n

(cid:1) E (cid:0)(cid:15)4

t

(cid:1) E



0(cid:15)2
(cid:15)2
n





i1−1
(cid:89)

j=0

A−1−j

i2−1
(cid:89)

j=0

An−1−j

i3−1
(cid:89)

j=0

i4−1
(cid:89)

At−1−j

At+n−1−j

j=0
j(cid:54)=n−1







≤ E (cid:0)(cid:15)2

t+n

(cid:1) E (cid:0)(cid:15)4

t

(cid:1) E (cid:0)(cid:15)6

n

(cid:1) E









(cid:15)2
0

i1−1
(cid:89)

j=0

A−1−j

i2−1
(cid:89)

j=0

An−1−j

i3−1
(cid:89)

At−1−j

i4−1
(cid:89)

At+n−1−j

j=0
j(cid:54)=t−1−n

j=0
j(cid:54)=n−1
j(cid:54)=t−1









≤ E (cid:0)(cid:15)2

t+n

(cid:1) E (cid:0)(cid:15)4

t

(cid:1) E (cid:0)(cid:15)6

n

(cid:1) E (cid:0)(cid:15)8

0

(cid:1) E











i1−1
(cid:89)

j=0











.

i2−1
(cid:89)

A−1−j

An−1−j

i3−1
(cid:89)

At−1−j

i4−1
(cid:89)

At+n−1−j

j=0
j(cid:54)=n−1

j=0
j(cid:54)=t−1−n
j(cid:54)=t−1

j=0
j(cid:54)=n−1
j(cid:54)=t−1
j(cid:54)=t+n−1

17

Computing similarly for n = 0, using stationarity of A, and observing that

1 = E((cid:15)2

0) ≤ E((cid:15)4

0) ≤ E((cid:15)6

0) ≤ E((cid:15)8
0)

we hence deduce

(cid:32)

E

0(cid:15)2
(cid:15)2

n(cid:15)2

t (cid:15)2

t+n

i1−1
(cid:89)

j=0

A−1−j

i2−1
(cid:89)

j=0

An−1−j

i3−1
(cid:89)

At−1−j

i4−1
(cid:89)

At+n−1−j

(cid:33)

(cid:32)i1−1
(cid:89)

i2−1
(cid:89)

A−j

An−j

≤ CE

j=0

j=0

i3−1
(cid:89)

j=0

At−j

j=0
(cid:33)

At+n−j

,

j=0

i4−1
(cid:89)

j=0

(35)

where C is a constant. Moreover, by using similar arguments we observe





i1−1
(cid:89)

E

i2−1
(cid:89)

A−j

An−j

j=0

j=0

i3−1
(cid:89)

j=0

At−j

i4−1
(cid:89)

j=0


 ≤ E

At+n−j





i1−1
(cid:89)

i2−1
(cid:89)

A−j

i3−1
(cid:89)

A−j

i4−1
(cid:89)

A−j



A−j

 .

j=0

j=0

j=0

j=0

Combining all the estimates above, it thus sufﬁces to prove that

∞
(cid:88)

∞
(cid:88)

∞
(cid:88)

∞
(cid:88)

(cid:32)i1−1
(cid:89)

E

i2−1
(cid:89)

i3−1
(cid:89)

i4−1
(cid:89)

A−j

A−j

A−j

A−j

(cid:33)

i1=0

i2=0

i3=0
i4(cid:88)

i4=0
i3(cid:88)

∞
(cid:88)

≤ 4!

j=0

j=0

j=0

j=0

i2(cid:88)

E

(cid:32)i1−1
(cid:89)

i2−1
(cid:89)

i3−1
(cid:89)

A−j

i4−1
(cid:89)

A−j

A−j

(cid:33)

A−j

< ∞.

i4=0

i3=0

i2=0

i1=0

j=0

j=0

j=0

j=0

Now for i1 ≤ i2 ≤ i3 ≤ i4 we have

(cid:32)i1−1
(cid:89)

E

i2−1
(cid:89)

A−j

i3−1
(cid:89)

A−j

i4−1
(cid:89)

A−j

j=0

j=0

j=0

j=0

(cid:33)

A−j

= αi1+i2+i3+i4

1

which yields

E((cid:15)8

0)i1E((cid:15)6

0)i2−i1E((cid:15)4

0)i3−i2

∞
(cid:88)

i4(cid:88)

i3(cid:88)

i2(cid:88)

4!

(cid:32)i1−1
(cid:89)

E

i2−1
(cid:89)

A−j

i3−1
(cid:89)

A−j

i4−1
(cid:89)

A−j

A−j

(cid:33)

i2=0
i4(cid:88)

i1=0
i3(cid:88)

i2(cid:88)

j=0

j=0

j=0

j=0

αi1+i2+i3+i4

1

E((cid:15)8

0)i1E((cid:15)6

0)i2−i1E((cid:15)4

0)i3−i2

i4=0

i3=0

∞
(cid:88)

i4=0
∞
(cid:88)

= 4!

= 4!

i3=0

i1=0

i2=0
i4(cid:88)

αi4
1

(cid:0)α1E((cid:15)4

0)(cid:1)i3

i3(cid:88)

i2=0

(cid:18)

α1

E((cid:15)6
0)
E((cid:15)4
0)

(cid:19)i2

i2(cid:88)

(cid:18)

i1=0

α1

E((cid:15)8
0)
E((cid:15)6
0)

(cid:19)i1

.

18

i4=0

i3=0

Denote

a1 = α1

E((cid:15)8
0)
E((cid:15)6
0)

,

a2 = α1

E((cid:15)6
0)
E((cid:15)4
0)

and a3 = α1E((cid:15)4

0).

Then we need to show that

∞
(cid:88)

i4(cid:88)

i3(cid:88)

ai2
2

ai3
3

αi4
1

i2(cid:88)

i4=0

i3=0

i2=0

i1=0

ai1
1 < ∞.

(36)

For this suppose ﬁrst that 1 /∈ S := {a1, a2, a3, a1a2, a2a3, a1a2a3}. Then we are able
to use geometric sums to obtain

i2(cid:88)

i1=0

ai1
1 =

1 − ai2+1
1
1 − a1

for a1 (cid:54)= 1.

Continuing like this in the iterated sums in (36) we deduce

i3(cid:88)

i2=0

2 (1 − ai2+1
ai2

1

) =

i3(cid:88)

i2=0

i3(cid:88)

ai2
2 − a1

(a1a2)i2 =

i2=0

1 − ai3+1
2
1 − a2

− a1

1 − (a1a2)i3+1
1 − a1a2

,

i4(cid:88)

i3=0

3 (1 − ai3+1
ai3

2

) =

1 − ai4+1
3
1 − a3

− a2

1 − (a2a3)i4+1
1 − a2a3

,

and

i4(cid:88)

i3=0

ai3
3 (1 − (a1a2)i3+1) =

1 − ai4+1
3
1 − a3

− a1a2

1 − (a1a2a3)i4+1
1 − a1a2a3

.

Consequently, it sufﬁces that the following three series converge

∞
(cid:88)

i4=0

1 ai4+1
αi4

3

,

∞
(cid:88)

i4=0

αi4
1 (a2a3)i4+1

and

∞
(cid:88)

i4=0

αi4
1 (a1a2a3)i4+1

yielding constraints

α1 <

1
(cid:112)E((cid:15)4
0)
However, these follow from the assumption α1 < 1
E((cid:15)8
0)
sufﬁces to replace a1, a2, a3 with

1
0) 1
E((cid:15)6

, α1 <

3

and α1 <

1
0) 1
E((cid:15)8

4

.

. Finally, if 1 ∈ S it simply

1
4

˜a1 = α1

(cid:18) E((cid:15)8
0)
E((cid:15)6
0)

(cid:19)

+ δ

,

˜a2 = α1

(cid:18) E((cid:15)6
0)
E((cid:15)4
0)

(cid:19)

+ δ

and

˜a3 = α1

(cid:0)E((cid:15)4

0) + δ(cid:1)

19

such that

1 /∈ {˜a1, ˜a2, ˜a3, ˜a1˜a2, ˜a2˜a3, ˜a1˜a2˜a3}.

1
4

is strict.

Choosing δ < 0 small enough the claim follows from the fact that the inequality
α1 < 1
E((cid:15)8
0)
Step 2: computing the limit of (30).
By step 1 we can apply dominated convergence theorem in (30). For this let us analyze
the limit behaviour of (32). For the latter term we use (33). By assumptions, we have
e.g. the following identities:

lim
t→∞

E(Lt−i3Lt+n−i4) = 1

lim
t→∞

E(L−i1Lt−i3Lt+n−i4) = f (n + i3 − i4)

lim
t→∞

E(L−i1Ln−i2Lt−i3Lt+n−i4) = f (n + i1 − i2)f (n + i3 − i4).

Therefore the limit of the latter term of (32) is given by

lim
t→∞

E (B−i1 Bn−i2 Bt−i3 Bt+n−i4 ) = α4

0 + 4α3

0l1 + α2

0l2
1

(cid:0)4 + f (n + i1 − i2) + f (n + i3 − i4)(cid:1)

(cid:0)f (n + i1 − i2) + f (n + i3 − i4) + f (n + i1 − i2)

+ α0l3
1
+ f (n + i3 − i4)(cid:1) + l4
0 + 2α0l1 + l2
= (α2

1f (n + i1 − i2)f (n + i3 − i4)
0 + 2α0l1 + l2

1f (n + i1 − i2))(α2

1f (n + i3 − i4))

The ﬁrst term of (32) can be divided into two independent parts whenever t is large
enough. More precisely, for t > max{n + i3, i4}, we have

0(cid:15)2
(cid:15)2

n(cid:15)2

t (cid:15)2

t+n

i1−1
(cid:89)

j=0

A−1−j

(cid:32)

E

(cid:32)

=E

0(cid:15)2
(cid:15)2
n

(cid:32)

=E

0(cid:15)2
(cid:15)2
n

i1−1
(cid:89)

j=0

i1−1
(cid:89)

j=0

A−1−j

A−1−j

i2−1
(cid:89)

j=0

i2−1
(cid:89)

j=0

i2−1
(cid:89)

j=0

i3−1
(cid:89)

j=0

An−1−j

(cid:33)

(cid:32)

An−1−j

E

t (cid:15)2
(cid:15)2

t+n

At−1−j

(cid:33)

At+n−1−j

i4−1
(cid:89)

j=0

i3−1
(cid:89)

j=0

At−1−j

i4−1
(cid:89)

j=0

(cid:33)

At+n−1−j

(cid:33)

(cid:32)

An−1−j

E

0(cid:15)2
(cid:15)2
n

i3−1
(cid:89)

j=0

A−1−j

(cid:33)

An−1−j

,

i4−1
(cid:89)

j=0

where the last equality follows from stationarity of At. Hence

t X 2
t+n)

(cid:15)2

E

0(cid:15)2
n

lim
t→∞

E(X 2

0 X 2

nX 2

∞
(cid:88)

∞
(cid:88)

∞
(cid:88)

=

∞
(cid:88)

i2=0

i3=0

i1=0
(α2

i4=0
0 + 2α0l1 + l2

i1−1
(cid:89)

j=0

i2−1
(cid:89)

A−1−j

An−1−j


 E


(cid:15)2

0(cid:15)2
n

i3−1
(cid:89)

j=0

A−1−j

i4−1
(cid:89)

j=0



An−1−j

 ·

1f (n + i1 − i2))(α2

1f (n + i3 − i4)).

j=0
0 + 2α0l1 + l2

20

On the other hand, by (4)

E(X 2

0 X 2

n) =

=

∞
(cid:88)

∞
(cid:88)

i1=0

i2=0

∞
(cid:88)

∞
(cid:88)

i1=0

i2=0


(cid:15)2

0(cid:15)2
n

E


(cid:15)2

0(cid:15)2
n

E

i1−1
(cid:89)

j=0

i1−1
(cid:89)

j=0

A−1−j

A−1−j

i2−1
(cid:89)

j=0

i2−1
(cid:89)

j=0


 E ((α0 + l1L−1−i1)(α0 + l1Ln−1−i2 ))

An−1−j


 (α2

0 + 2α0l1 + l2

1f (n + i1 − i2)).

An−1−j

Consequently, we conclude that

lim
t→∞

proving the claim.

E(X 2

0 X 2

nX 2

t X 2

t+n) = E(X 2

0 X 2

n)2

Remark 3.6. The assumptions of Lemma 3.5 cohere with the assumptions of Lemma
3.4. Moreover, the assumptions made related to convergence of covariances are very
natural. Indeed, we only assume that the (linear) dependencies within the process Lt
vanish over time. Examples of L satisfying the required assumptions can be found in
Section 3.3.

3.2 Estimation of the model parameters

Set, for N ≥ 1,

ˆµ2,N =

1
N

N
(cid:88)

t=1

X 4
t

and

g0(γγγ0) = b0(γγγ0)2 − 4a0(γγγ0)c0(γγγ0),
where a0(γγγ0), b0(γγγ0) and c0(γγγ0) are as in (20). In addition, let

ˆγγγ0,N = [ˆγN (n + 1), ˆγN (n), ˆγN (n − 1), ˆγN (1), ˆγN (0), ˆµ2,N ]

and ˆξξξ0,N = [ˆγγγ0,N , ˆµN ] for some ﬁxed n (cid:54)= 0. The following estimators are motivated
by (21), (22) and (23).

Deﬁnition 3.7. We deﬁne estimators ˆα1, ˆl1 and ˆα0 for the model parameters α1, l1 and
α0 respectively through

ˆα1 = α1(ˆγγγ0,N ) =

−b0(ˆγγγ0,N ) ±

(cid:113)

g0(ˆγγγ0,N )

2a0(ˆγγγ0,N )

,

(37)

21

ˆl1 = l1(ˆγγγ0,N ) =

(cid:115)

(cid:18)

1
s(0)

and

α1(ˆγγγ0,N )2ˆγN (0) − 2α1(ˆγγγ0,N )ˆγN (1) + ˆγN (0) −

(cid:19)

ˆµ2,N V ar((cid:15)2
0)
E((cid:15)4
0)

(38)

ˆα0 = α0(ˆξξξ0,N ) = ˆµN (1 − α1(ˆγγγ0,N )) − l1(ˆγγγ0,N ),

(39)

where n (cid:54)= 0.

Theorem 3.8. Assume that a0(γγγ0) (cid:54)= 0 and g0(γγγ0) > 0. Let the assumptions of Lemma
3.5 prevail. Then ˆα1, ˆl1 and ˆα0 given by (37), (38) and (39) are consistent.

Proof. Since the assumptions of Lemma 3.5 are satisﬁed, so are the assumptions of
Lemma 3.4 implying that the autocovariance estimators, the mean and the second mo-
ment estimator of X 2
t are consistent. The claim follows from the continuous mapping
theorem.

Let us denote

where a(γγγ) and b(γγγ) are as in (26). In addition, let

g(γγγ) = b(γγγ)2 − 4a(γγγ)2,

ˆγγγN = [ˆγN (n1 + 1), ˆγN (n2 + 1), ˆγN (n1), ˆγN (n2), ˆγN (n1 − 1), ˆγN (n2 − 1)]

and ˆξξξN = [ˆγγγN , ˆµN ] for some ﬁxed n1, n2 (cid:54)= 0 with n1 (cid:54)= n2. The following estimators
are motivated by (27), (28) and (29).

Deﬁnition 3.9. We deﬁne estimators ˆα1, ˆl1 and ˆα0 for the model parameters α1, l1 and
α0 respectively through

ˆα1 = α1(ˆγγγN ) =

−b(ˆγγγN ) ± (cid:112)g(ˆγγγN )
2a(ˆγγγN )

,

(40)

(cid:115)

ˆl1 = l1(ˆγγγN ) =

α2

1(ˆγγγN )ˆγN (n2) − α1(ˆγγγN )(ˆγN (n2 + 1) + ˆγN (n2 − 1)) + ˆγN (n2)
s(n2)

(41)

and

ˆα0 = α0(ˆξξξN ) = ˆµN (1 − α1(ˆγγγN )) − l1(ˆγγγN ),

(42)

where n1, n2 (cid:54)= 0 and n1 (cid:54)= n2.

Theorem 3.10. Assume that s(n2) (cid:54)= 0, a(γγγ) (cid:54)= 0 and g(γγγ) > 0. Let the assumptions
of Lemma 3.5 prevail. Then ˆα1, ˆl1 and ˆα0 given by (40), (41) and (42) are consistent.

22

Proof. The proof is basically the same as with Theorem 3.8.

Remark 3.11.

• Statements of Theorems 3.8 and 3.10 hold true also when g0(γγγ0) =
0 and g(γγγ) = 0, but in these cases the estimators do not necessarily become real
valued as the sample size grows. In comparison, in [11] the estimators were
forced to be real by using indicator functions.

• The estimators from Deﬁnitions 3.7 and 3.9 are of course related. In practice
(see the next section) we use those from Deﬁnition 3.7 while those from Deﬁ-
nition 3.9 are needed just in case when we need more information in order to
choose the correct sign for ˆα1, see Remark 2.9.

• Note that here we implicitly assumed that the correct sign can be chosen in ˆα1.

However, this is not a restriction as discussed.

3.3 Examples

We will present several examples of stationary processes for which our main result
stated in Theorem 3.8 apply. Our examples are constructed as

Lt := (Xt+1 − Xt)2 , for every t ∈ Z

where (Xt)t∈R is a stochastic process with stationary increments. We discuss below
the case when X is a continuous Gaussian process (the fractional Brownian motion),
a continuous non-Gaussian process (the Rosenblatt process), or a jump process (the
compensated Poisson process).

3.3.1 The fractional Brownian motion

for every t ∈ R where (BH

Let Xt := BH
t )t∈R is a two-sided fractional Brownian
t
motion with Hurst parameter H ∈ (0, 1). Recall that BH is a centered Gaussian
process with covariance

E(BtBs) =

1
2

(|t|2H + |s|2H − |t − s|2H), s, t ∈ R.

Let us verify that the conditions from Lemma 3.5 and Theorem 3.8 are satisﬁed by
Lt = (BH

t )2. First, notice that (see Lemma 2 in [1]) that for t ≥ 1
t )2(cid:1) − 1 = 2(rH(t))2

Cov(L0, Lt) = E (cid:0)(BH

t+1 − BH

t+1 − BH

1 )2(BH

with

rH(t) =

1
2

(cid:2)(t + 1)2H + (t − 1)2H − 2t2H(cid:3) →t→∞ 0

(43)

since rH(t) behaves as t2H−2 for t large.

23

Let us now turn to the third-order condition, i.e. Cov(L0Ln, Lt) = E(L0LnLt) −

E(L0Ln) → 0 as t → ∞. We can suppose n ≥ 1 is ﬁxed and t > n.

For any three centered Gaussian random variables X1, X2, X3 with unit variance

we have E(X 2
E(X 2

1 X 2
1 X 2

2 ) = 1 + 2(E(X1X2))2 and
2 X 2

3 ) = 2 (cid:0)(E(X1X2))2 + (E(X1X3))2 + (E(X2X3))2(cid:1)

1 X 2

2 ) + 2 (cid:0)(E(X1X3))2 + (E(X2X3))2(cid:1)

+ 4E(X1X2))E(X1X3))E(X2X3)) + 1
= E(X 2
+ 4E(X1X2))E(X1X3))E(X2X3)).
t+1 − BH
1 , X2 = BH
Cov(L0Ln, Lt) = 2rH(t)2 + 2rH(t − n)2 + 4rH(n)rH(t)rH(t − n)

n , X3 = BH

n+1 − BH

By applying this formula to X1 = BH

t , we ﬁnd

where rH is given by (43). By (43), the above expression converges to zero as t → ∞.
Similarly for the fourth-order condition, the formulas are more complex but we
can verify by standard calculations that, for every n1, n2 ≥ 1 and for every t >
max(n1, n2), the quantity

E(L0Ln1LtLt+n2) − E(L0Ln1)E(LtLt+n2)
can be expressed as a polynomial (without term of degree zero) in rH(t), rH(t −
n1), rH(t + n2), rH(t + n2 − n1) with coefﬁcients depending on n1, n2. The conclusion
is obtained by (43).

3.3.2 The compensated Poisson process

Let (Nt)t∈R be a Poisson process with intensity λ = 1. Recall that N is a cadlag
adapted stochastic process, with independent increments, such that for every s < t, the
random variable Nt − Ns follows a Poisson distribution with parameter t − s. Deﬁne
the compensated Poisson process ( ˜Nt)t∈R by ˜Nt = Nt − t for every t ∈ R and let
Lt = (Nt+1 − Nt)2. Clearly ELt = 1 for every t and, by the independence of the
increments of ˜N , we have that for t large enough

Cov(L0, Lt) = Cov(L0Ln, Lt) = Cov(L0Ln1, LtLt+n2) = 0,

so the conditions in Theorem 3.8 are fulﬁlled.

3.3.3 The Rosenblatt process

The (one-sided) Rosenblatt process (Z H
t )t≥0 is a self-similar stochastic process with
stationary increments and long memory in the second Wiener chaos, i.e.
it can be
expressed as a multiple stochastic integral of order two with respect to the Wiener pro-
cess. The Hurst parameter H belongs to ( 1
2, 1) and it characterizes the main properties
of the process. Its representation is
(cid:90)

(cid:90)

Z H

t =

R

R

fH(y1, y2)dW (y1)dW (y2)

24

(cid:82)

where (W (y))y∈R is Wiener process and fH is deterministic function such that
(cid:82)
R fH(y1, y2)2dy1dy2 < ∞. See e.g. [10] for a more complete exposition on the
R
Rosenblatt process. The two-sided Rosenblatt process has been introduced in [4]. In
particular, it has the same covariance as the fractional Brownian motion, so E(Lt) =
E(Z H
t )2 = 1 for every t. The use of the Rosenblatt process can be motivated by
the presence of the long-memory in the emprical data for liquidity in ﬁnancial markets,
see [8].

t+1 − Z H

The computation of the quantities Cov(L0, Lt), Cov(L0Ln, Lt) and Cov(L0Ln1, LtLt+n2)

requires rather technical tools from stochastic analysis including properties of multiple
integrals and product formula which we prefer to avoid here. We only mention that the
term Cov(L0, Lt) can be written as P (rH(t), rH,1(t)) where P is a polynomial without
term of degree zero, rH is given by (43), while

rH,1(t) =

Note that

rH,1(t) =

(cid:90) 1

(cid:90) 1

(cid:90) t+1

(cid:90) t+1

0

0

t

t

du1du2du3du4|u1−u2|H−1|u2−u3|H−1|u3−u4|H−1|u4−u1|H−1.

(cid:90)

[0,1]4

du1du2du3du4|u1−u2|H−1|u2−u3+t|H−1|u3−u4|H−1|u4−u1+t|H−1.

Since |u1 − u2|H−1|u2 − u3 + t|H−1|u3 − u4|H−1|u4 − u1 + t|H−1 converges to zero as
t → ∞ for every ui and since this integrand is bounded for t large by |u1 −u2|H−1|u2 −
u3|H−1|u3 − u4|H−1|u4 − u1|H−1, which is integrable over [0, 1]4, we obtain, via the
dominated convergence theorem, that Cov(L0, Lt) →t→∞ 0. Similarly, the quanti-
ties Cov(L0Ln, Lt) and Cov(L0Ln1, LtLt+n2) can be also expressed as polynomials
(without constant terms) of rH, rH,k, k = 1, 2, 3, 4 where

(cid:90)

rH,k(t) =

A1×...A2k

du1...du2k|u1 − u2|H−1...|u2k−1 − u2k|H−1|u2k − u1|H−1,

where at least one set Ai is (t, t + 1). Thus we may apply a similar argument as above.

4 Simulations

This section provides some visual illustrations of convergence of the estimators (37),
(38) and (39) with respect to different liquidities (Lt)t∈Z.
The general setting throughout the simulations is the following. The IID process
((cid:15)t)t∈Z is assumed to be a sequence of standard normals. In this case the restriction
given by Lemma 3.5 reads α1 < 1
≈ 0.31. The lag used is n = 1 and the true
105
values of the model parameters are α0 = 1, α1 = 0.1 and l1 = 0.5. The used sample
sizes are N = 100, N = 1000, N = 10000 and N = 100000. The initial X 2
0 is set to a
value 1.7. After the processes Lt with t = 0, 1, ...N − 2 and (cid:15)t with t = 1, 2, ...N − 1
are simulated, the initial is used to generate σ2
1 using (1). Together with (cid:15)1 this gives

1
4

25

1 , after which (1) yields the sample {X 2

X 2
0 , X 2
In the ﬁrst three subsections simulation results of the generalized ARCH process with
t )2 are presented. The used Hurst indices are
liquidity given by Lt = (BH
H = 1
5. In the fourth subsection the liquidity process is given by
Lt = ( ˜Nt+1 − ˜Nt)2, where Nt is a compensated Poisson process with λ = 1.

3 and H = 4

3, H = 2

t+1 − BH

1 , ..., X 2

N −1}.

In all subsections the sample size N is varied, and each setting is repeated 1000
times to provide histograms of the estimates. Our simulations show that the behaviour
of the limit distributions is close to Gaussian one, as N increases. We also note that,
since the estimators involve square roots, they may produce complex valued estimates.
However, asymptotically the estimates become real. Throughout the simulations the
complex valued estimates have been simply removed, although the percentage of com-
plex values is computed in each setting. Finally, some illustrative tables are given in
Appendix A.

4.1 Fractional Brownian motion with H = 1
3.

t )2 with H = 1

Histograms of the estimates of the model parameters corresponding to Lt = (BH
t+1 −
BH
3 are provided in Figures 1, 2, 3 and 4. The used sample sizes were
N = 100, N = 1000, N = 10000 and N = 100000. The sample sizes N = 100 and
N = 1000 resulted complex valued estimates in 44.2% and 3.5% of the simulations
respectively, whereas with the larger sample sizes all the estimates were real.

(a) Estimates of α0.

(b) Estimates of α1.

(c) Estimates of l1.

Figure 1: Fractional Brownian motion liquidity with H = 1

3 and N = 100.

26

Estimate valueFrequency−1.0−0.50.00.51.01.52.00102030405060Estimate valueFrequency−0.20.00.20.40.60204060Estimate valueFrequency0.00.51.01.52.02.5010203040(a) Estimates of α0.

(b) Estimates of α1.

(c) Estimates of l1.

Figure 2: Fractional Brownian motion liquidity with H = 1

3 and N = 1000.

(a) Estimates of α0.

(b) Estimates of α1.

(c) Estimates of l1.

Figure 3: Fractional Brownian motion liquidity with H = 1

3 and N = 10000.

27

Estimate valueFrequency0.60.81.01.21.41.601020304050Estimate valueFrequency0.00.10.20.3020406080Estimate valueFrequency0.00.20.40.60.81.00102030405060Estimate valueFrequency0.80.91.01.1020406080Estimate valueFrequency0.050.100.150.20020406080100120Estimate valueFrequency0.30.40.50.60.7010203040506070(a) Estimates of α0.

(b) Estimates of α1.

(c) Estimates of l1.

Figure 4: Fractional Brownian motion liquidity with H = 1

3 and N = 100000.

4.2 Fractional Brownian motion with H = 2
3.

t )2 with H = 2

Histograms of the estimates of the model parameters corresponding to Lt = (BH
t+1 −
BH
3 are provided in Figures 5, 6, 7 and 8. The used sample sizes were
N = 100, N = 1000, N = 10000 and N = 100000. The sample sizes N = 100 and
N = 1000 resulted complex valued estimates in 45.5% and 2.9% of the simulations
respectively, whereas with the larger sample sizes all the estimates were real.

(a) Estimates of α0.

(b) Estimates of α1.

(c) Estimates of l1.

Figure 5: Fractional Brownian motion liquidity with H = 2

3 and N = 100.

28

Estimate valueFrequency0.940.960.981.001.021.041.06020406080100Estimate valueFrequency0.0850.0900.0950.1000.1050.1100.115020406080Estimate valueFrequency0.450.500.55020406080100Estimate valueFrequency−1.0−0.50.00.51.01.52.0010203040506070Estimate valueFrequency−0.20.00.20.40.6020406080Estimate valueFrequency0.00.51.01.52.02.5010203040(a) Estimates of α0.

(b) Estimates of α1.

(c) Estimates of l1.

Figure 6: Fractional Brownian motion liquidity with H = 2

3 and N = 1000.

(a) Estimates of α0.

(b) Estimates of α1.

(c) Estimates of l1.

Figure 7: Fractional Brownian motion liquidity with H = 2

3 and N = 10000.

29

Estimate valueFrequency0.00.51.01.5020406080100120Estimate valueFrequency0.00.10.20.3020406080Estimate valueFrequency0.00.51.01.5020406080100120Estimate valueFrequency0.80.91.01.1020406080Estimate valueFrequency0.040.060.080.100.120.140.160.18020406080100120Estimate valueFrequency0.30.40.50.60.70204060(a) Estimates of α0.

(b) Estimates of α1.

(c) Estimates of l1.

Figure 8: Fractional Brownian motion liquidity with H = 2

3 and N = 100000.

4.3 Fractional Brownian motion with H = 4
5.

t )2 with H = 4

Histograms of the estimates of the model parameters corresponding to Lt = (BH
t+1 −
BH
5 are provided in Figures 9, 10, 11 and 12. The used sample sizes were
N = 100, N = 1000, N = 10000 and N = 100000. The sample sizes N = 100 and
N = 1000 resulted complex valued estimates in 47.9% and 4.3% of the simulations
respectively, whereas with the larger sample sizes all the estimates were real.

(a) Estimates of α0.

(b) Estimates of α1.

(c) Estimates of l1.

Figure 9: Fractional Brownian motion liquidity with H = 4

5 and N = 100.

30

Estimate valueFrequency0.940.960.981.001.021.04020406080100Estimate valueFrequency0.090.100.110.12020406080Estimate valueFrequency0.440.460.480.500.520.540.560.58020406080100120Estimate valueFrequency0.00.51.01.52.0010203040Estimate valueFrequency−0.20.00.20.40.6010203040506070Estimate valueFrequency0.00.51.01.52.0010203040(a) Estimates of α0.

(b) Estimates of α1.

(c) Estimates of l1.

Figure 10: Fractional Brownian motion liquidity with H = 4

5 and N = 1000.

(a) Estimates of α0.

(b) Estimates of α1.

(c) Estimates of l1.

Figure 11: Fractional Brownian motion liquidity with H = 4

5 and N = 10000.

31

Estimate valueFrequency0.00.10.20.3020406080Estimate valueFrequency0.00.10.20.3020406080Estimate valueFrequency0.00.20.40.60.81.01.21.4020406080100120Estimate valueFrequency0.80.91.01.1020406080Estimate valueFrequency0.040.060.080.100.120.140.160.18020406080100120Estimate valueFrequency0.30.40.50.60.70.8020406080(a) Estimates of α0.

(b) Estimates of α1.

(c) Estimates of l1.

Figure 12: Fractional Brownian motion liquidity with H = 4

5 and N = 100000.

4.4 Compensated Poisson with λ = 1.
Histograms of the estimates of the model parameters corresponding to Lt = ( ˜Nt+1 −
˜Nt)2 with λ = 1 are provided in Figures 13, 14, 15 and 16. The used sample sizes were
N = 100, N = 1000, N = 10000 and N = 100000. The sample sizes N = 100 and
N = 1000 resulted complex valued estimates in 45.1% and 3.0% of the simulations
respectively, whereas with the larger sample sizes all the estimates were real.

(a) Estimates of α0.

(b) Estimates of α1.

(c) Estimates of l1.

Figure 13: Compensated Poisson liquidity with λ = 1 and N = 100.

32

Estimate valueFrequency0.920.940.960.981.001.021.041.06020406080100Estimate valueFrequency0.090.100.110.120204060Estimate valueFrequency0.450.500.55020406080Estimate valueFrequency−0.50.00.51.01.52.0010203040Estimate valueFrequency−0.20.00.20.40204060Estimate valueFrequency0.00.51.01.52.02.53.0020406080(a) Estimates of α0.

(b) Estimates of α1.

(c) Estimates of l1.

Figure 14: Compensated Poisson liquidity with λ = 1 and N = 1000.

(a) Estimates of α0.

(b) Estimates of α1.

(c) Estimates of l1.

Figure 15: Compensated Poisson liquidity with λ = 1 and N = 10000.

33

Estimate valueFrequency0.00.51.01.5020406080100120Estimate valueFrequency0.000.050.100.150.200.250.30020406080100Estimate valueFrequency0.00.51.01.52.0020406080100120Estimate valueFrequency0.60.70.80.91.01.11.2020406080100120Estimate valueFrequency0.050.100.150.200.25020406080100Estimate valueFrequency0.30.40.50.60.70.80.91.0020406080100120(a) Estimates of α0.

(b) Estimates of α1.

(c) Estimates of l1.

Figure 16: Compensated Poisson liquidity with λ = 1 and N = 100000.

A Tables

In the following tables we have presented means and standard deviations of the esti-
mates in different cases. In addition, we have provided tables demonstrating how the
estimates match their theoretical intervals 0 ≤ α0, 0 < α1 < 1
and 0 < l1. We
105
can see that multiplying the mean squared error (RMSE) provided by Tables 1-4 with
N H, the power H of the sample size, gives us evidence of the convergence rates of the
estimators.

1
4

α0
N
100
1.063 (0.374)
1000 1.037 (0.169)
10000 1.007 (0.061)
1.001 (0.019)

100000

α1
0.074 (0.124)
0.100 (0.051)
0.100 (0.018)
0.100 (0.005)

l1
0.541 (0.346)
0.465 (0.167)
0.494 (0.060)
0.499 (0.019)

Table 1: Table of means and standard deviations corresponding to fractional Brownian
motion liquidity with H = 1
3.

α0
N
100
1.086 (0.359)
1000 1.029 (0.181)
10000 1.005 (0.057)
1.000 (0.019)

100000

α1
0.080 (0.127)
0.097 (0.052)
0.099 (0.017)
0.100 (0.005)

l1
0.538 (0.358)
0.479 (0.184)
0.497 (0.059)
0.500 (0.019)

Table 2: Table of means and standard deviations corresponding to fractional Brownian
motion liquidity with H = 2
3.

34

Estimate valueFrequency0.900.951.001.05020406080Estimate valueFrequency0.080.090.100.110.120.130.140.15050100150Estimate valueFrequency0.450.500.550.60020406080N
α0
1.068 (0.322)
100
1000 1.042 (0.163)
10000 1.009 (0.059)
1.001 (0.020)

100000

α1
0.090 (0.139)
0.098 (0.052)
0.099 (0.018)
0.100 (0.006)

l1
0.535 (0.336)
0.467 (0.180)
0.491 (0.064)
0.499 (0.022)

Table 3: Table of means and standard deviations corresponding to fractional Brownian
motion liquidity with H = 4
5.

α0
N
100
1.110 (0.346)
1000 1.057 (0.190)
10000 1.011 (0.075)
1.001 (0.025)

100000

α1
0.071 (0.128)
0.095 (0.050)
0.098 (0.020)
0.100 (0.007)

l1
0.508 (0.384)
0.452 (0.207)
0.493 (0.081)
0.498 (0.026)

Table 4: Table of means and standard deviations corresponding to compensated Pois-
son liquidity with λ = 1.

N α0
100 55.1
1000 96.5
100
100

10000
100000

α1
65.4
98.8
100
100

l1
55.8
96.5
100
100

Table 5: Table of percentages of the estimates lying on their theoretical intervals cor-
responding to fBm liquidity with H = 1
3.

N α0
100 54.0
1000 97.1
100
100

10000
100000

α1
66.6
99.0
100
100

l1
54.5
97.1
100
100

Table 6: Table of percentages of the estimates lying on their theoretical intervals cor-
responding to fBm liquidity with H = 2
3.

35

N α0
100 52.0
1000 95.7
100
100

10000
100000

α1
65.2
98.5
100
100

l1
52.1
95.7
100
100

Table 7: Table of percentages of the estimates lying on their theoretical intervals cor-
responding to fBm liquidity with H = 4
5.

N α0
100 54.9
1000 96.6
100
100

10000
100000

α1
61.8
98.7
100
100

l1
55.3
96.9
100
100

Table 8: Table of percentages of the estimates lying on their theoretical intervals cor-
responding to compensated Poisson liquidity with λ = 1.

References

[1] M. Bahamonde, S. Torres and C. A. Tudor (2018): ARCH model and fractional

Brownian motion. Statistics and Probability Letters, 134, 70-78.

[2] T. Bollerslev (1986): Generalized autoregressive conditional heteroskedasticity.

Journal of Econometrics, 31(3), 307-327.

[3] T. Bollerslev (2008): Glossary to ARCH (GARCH). CREATES Research Papers

2008-49.

[4] P. Coupek (2018): Limiting measure and stationarity of solutions to stochastic
evolution equations with Volterra noise. Stoch. Anal. Appl. 36(3), 393-412.

[5] R. F. Engle (1982): Autoregressive conditional heteroskedasticity with estimates

of the variance of the U.K. inﬂation. Econometrica, 987-1108.

[6] C. Francq and J-M. Zakoian (2010): Garch Models. Wiley.

[7] A. Lindner (2008):

Stationarity, Distributional properties and Moments of

GARCH (p,q) -processes. Handbook on ﬁnancial time series, 43-69. Springer.

[8] C. Tsuji (2002): Long-Term Memory and Applying the Multi-Factor ARFIMA

Models in Financial Markets. Asia-Paciﬁc Markets, 9, 283-304.

[9] C.A. Tudor and C. Tudor (2014): EGARCH model with weighted liquidity. Com-
munications in Statistics: Simulation and Computation, 43(5), 1133-1142.

36

[10] C.A. Tudor (2013): Analysis of variations for self-similar processes. A Stochastic

Calculus Approach. Springer, Cham.

[11] M. Voutilainen, L. Viitasaari, P. Ilmonen (2017): On model ﬁtting and estimation
of strictly stationary processes. Modern Stochastics: Theory and Applications,
4(4), 381-406.

[12] M. Voutilainen, L. Viitasaari, P. Ilmonen (2018): Note on AR(1)-characterisation

of stationary processes and model ﬁtting. arXiv:1805.10948.

37

