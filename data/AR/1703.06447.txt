Submitted to the Annales de l’Institut Henri Poincar´e - Probabilit´es et Statistiques

0
2
0
2
c
e
D
7

]

R
P
.
h
t
a
m

[

4
v
7
4
4
6
0
.
3
0
7
1
:
v
i
X
r
a

Persistence exponents in Markov chains

Frank Aurzada∗, Sumit Mukherjee† and Ofer Zeitouni‡

Fachbereich Mathematik
Technische Universit¨at Darmstadt,
Darmstadt, Germany
E-mail: aurzada@mathematik.tu-darmstadt.de

Department of Statistics,
Columbia University,
New York, USA
E-mail: sm3949@columbia.edu

Department of Mathematics,
Weizmann Institute of Science,
Rehovot, Israel
E-mail: ofer.zeitouni@weizmann.ac.il

Abstract. We prove the existence of the persistence exponent

log λ := lim
n→∞

1
n

log Pµ(X0 ∈ S, . . . , Xn ∈ S)

for a class of time homogeneous Markov chains {Xi}i≥0 taking values in a Polish space, where S is a Borel measurable
set and µ is an initial distribution. Focusing on the case of AR(p) and MA(q) processes with p, q ∈ N and continuous
innovation distribution, we study the existence of λ and its continuity in the parameters of the AR and MA processes,
respectively, for S = R≥0. For AR processes with log-concave innovation distribution, we prove the strict monotonicity
of λ. Finally, we compute new explicit exponents in several concrete examples.

MSC 2010 subject classiﬁcations: Primary 60J05, 60F10; secondary 45C05, 47A75.
Keywords: ARMA, eigenvalue problem, integral equation, large deviations, Markov chain, persistence, quasi-stationary distri-
bution.

1. Introduction

Let
given Borel measurable set S, we are interested in the asymptotics of the persistence probability

Xi}i≥0 be a time homogenous Markov chain on a Polish space with transition kernel P (x, dy). For a

{

pn(P, S, µ) := Pµ(Xi ∈

S, 0

i

≤

≤

n) =

P (xi, dxi+1)µ(dx0),

ZSn+1

∗Research partially supported by DFG grant AU370/4-1.
†Research partially supported by NSF grant DMS-1712037.
‡Research partially supported by grant 147/17 from the Israel Science Foundation and by a US-Israel BSF grant.

1

 
 
 
 
 
 
2

Aurzada, Mukherjee and Zeitouni

where µ is the initial distribution, i.e. the law of X0. We stress that we shall be particularly interested in
non-compact S. We will be interested in the existence of the persistence exponent λ = λ(P, S, µ), deﬁned as

log λ(P, S, µ) := lim
n→∞

1
n

log pn(P, S, µ)

(1.1)

and its continuity and monotonicity properties in parameters of the kernel.

The asymptotics of persistence probabilities for not necessarily Markov processes has received both clas-
sical and recent interest in probability theory and theoretical physics. For recent surveys on persistence
probabilities we refer the reader to [7] for a theoretical physics point of view and to [4] for a review of the
mathematical literature. We note in passing that outside the literature on Markov processes, a big compo-
nent of the literature on persistence is that for Gaussian processes, with links in the stationary case to their
spectral properties (see [12, 13, 15–17] and references therein for some recent developments in this area).

Our approach exploits the Markovian structure and relates the persistence exponent to an eigenvalue
of an appropriate operator, via the Krein-Rutman theorem. Such ideas have been extensively employed to
study general versions of the persistence problem for Markov processes, under the name of quasi-stationary
distributions (see Tweedie [26, 27], and for more recent work, see e.g. [8, 9, 24]). We work under somewhat
diﬀerent assumptions than is typical in that literature, for the sake of the applications that we have in
mind, which are MA (moving average) processes and AR (auto regressive) processes of ﬁnite order (to be
deﬁned later). In particular, we do not assume that the operator is irreducible; and much of our eﬀort lies
in deriving the existence of the persistence exponent and its properties directly in terms of the kernel. The
quasi-stationary approach developed in [26, 27] shows, under assumptions that are not always satisﬁed in
the examples that we consider, the equivalence of the exponent’s existence and properties of the eigenvalue
equation determined by PS (c.f. (1.3) for the deﬁnition of PS). One of our key observations is that, even in
very natural examples as in Section 5, we often need to work not with PS but rather with a modiﬁcation
of it. In addition, the existing literature is focused on persistence exponents for Dirac initial conditions,
whereas we in general require the initial distribution µ to charge all open sets. If the operator PS is assumed
to be irreducible, then all our results apply to degenerate initial distributions. Even in the irreducible case,
the persistence exponent need not exist for general initial distributions; see Proposition 2.5 for an example
where the persistence exponent exists and is universal if the initial distribution is an atom, but does not
need to not exist for general initial distributions.

1, 2

A more detailed study of persistence for AR processes of order 1 is provided by the very recent paper
[18]. See also the works [5, 6], where the author studies persistence of AR processes for general innovation
distributions with orders
. Of relevance is the recent work [8], that gives general criteria for convergence
in total variation norm to a quasi-stationary distribution, for continuous time Markov processes. We do note
that in our general setup, we cannot verify their conditions. In general, it is not clear that the existence of
the persistence exponent, even under our conditions, would imply the existence of a Yaglom limit, i.e. of
a quasi-limiting distributions. Also related to our persistence problem is the study of the population size
in critical branching processes, see e.g. [2], also see [21] for a recent work concerning Yaglom limits in this
context.

{

}

One upshot of our approach is a study of monotonicity and continuity properties of the persistence
exponent in parameters of the kernel P . We illustrate this in the case of MA processes and AR processes,
where the kernel (and thus the persistence exponent) depends on the coeﬃcient vector. In the setting of
AR processes, we derive a monotonicity lemma (Lemma 5.3) that might be of independent interest. As an
application, we prove strict monotonicity of the persistence exponent for AR(p) processes with log concave
innovation distributions. Finally, we demonstrate the strength of our approach by computing a number of
new persistence exponents in concrete examples by solving the corresponding eigenvalue equation.

The outline of the paper is as follows: Section 1.1 contains our main abstract existence result. The short
and technical Section 1.2 contains an abstract monotonicity lemma and a continuity lemma. The abstract
framework is then applied in Section 2 to moving-average (MA) processes and auto-regressive (AR), where
existence of the exponent, continuity of the exponent, (strict) monotonicity results, and the question whether
the exponent is degenerate are discussed. Finally, Section 3 contains a number of concrete cases where we are

Persistence exponents in Markov chains

3

able to solve the eigenvalue equation, i.e. to ﬁnd the leading eigenvalue explicitly. Sections 4–6 are devoted
to the proofs corresponding to the former three topics, respectively.

1.1. Existence of the exponent

We begin with a deﬁnition. Throughout,

k∞ denotes the sup norm of a function g on S.
(S) denote the set of all bounded measurable functions on S, and let

Deﬁnition 1.1. Let
(S)
denote the space of continuous bounded functions on S equipped with the sup norm. For a bounded linear
operator K mapping

(S) to itself, deﬁne the operator norm

Cb(S)

⊂ B

B

k

g

B

(1.2)

K

k

≤ k

. Note also that pn(P, S, µ) =

K

k

k

:=

sup
g∈B(S):kgk∞≤1 k

Kg

k∞,

and the spectral radius

λ(K) := lim

n→∞ k

1/n.

K n

k

Note that the limit in (1.2) exists by sub-additivity, and that λ(K)
S P n
(S) deﬁned by
R

S 1(x)µ(dx), where PS is the linear operator on

B

[PS(g)](x) :=

g(y)P (x, dy),

x

S,

∈

ZS

(1.3)

Cb(S)

while, for comparison, the spectral radius satisﬁes

λ(P, S) := λ(PS) = lim
n→∞

P n

S 1(x)

1/n

.

(cid:19)

sup
x∈S

(cid:18)

We recall that an operator K from

with

gnk∞ ≤

k

1 one ﬁnds a subsequence

Cb(S) to itself is called compact if for any sequence

gn}n≥1 in

{

nk}k≥1 such that

{

Kgnk}k≥1 converges in sup norm.

{

Theorem 1.2. Assume the following conditions:

(i) K is a non-negative linear operator which maps
(ii) µ is a probability measure such that µ(U ) > 0 for any non empty open set U

Cb(S) into itself, and K k is compact for some k

S.

⊆

Then,

lim
n→∞

1
n

log

(cid:18)ZS

K n1(x)µ(dx)

= log λ(K).

(cid:19)

1.

≥

(1.4)

Further, if λ(K) > 0, then λ(K) is the largest eigenvalue of the operator K, the corresponding eigenfunction
∈ Cb(S) is non-negative, and there exists a bounded, non-negative, ﬁnitely additive regular measure m on
ψ
S which is a left eigenvector of K corresponding to the eigenvalue λ(K), i.e.

m(dx)

K(x, dy)f (y) = λ(K)m(f ),

ZS

ZS

f

∈ Cb(S).

Remark 1.3. a) Replacing K by PS in Theorem 1.2 yields a suﬃcient condition for the existence of a
universal persistence exponent for all initial conditions µ satisfying condition (ii). As we will see in Section
2, this is not always the best choice.
b) The assumption of compactness of K k for some k (rather than the compactness of K itself ) is, on the one
hand, suﬃcient for the proof to go through and, on the other hand, necessary for dealing with some concrete
examples. For example, the operator PS related to MA(q) processes is typically not compact, whereas P q+1
is compact.
c) The left eigenvector m in Theorem 1.2 is only ﬁnitely additive. This is a consequence of the fact that S can
be (and typically is, in our applications) non-compact. This complicates some of the following arguments.
For example, the proof of Proposition 3.4 would be immediate if m were a measure.

S

4

Aurzada, Mukherjee and Zeitouni

1.2. Properties of exponents

We begin with a deﬁnition.

Deﬁnition 1.4. Suppose S is equipped with a partial order
non-negative, non-decreasing (in the sense of this partial order) measurable functions on S.

B+,>(S) denote the class of bounded,
(S) is said to be non-decreasing with respect to the partial

≤S. Let

A non-negative bounded linear operator K on

order

≤S, if K maps

B+,>(S) to itself.

B

The following lemma gives a suﬃcient condition for comparing λ(K1) and λ(K2) for two bounded non-

negative linear operators K1, K2.

Lemma 1.5. Let K1 and K2 be two bounded non-negative linear operators on
conditions hold:

B

(S), such that the following

(i) There exists a non-negative measurable function h on S such that [K1(g)](x)

x

S, g

∈

∈ B+,>(S).

(ii) K1 is non-decreasing on S.

h(x)[K2(g)](x) for any

≥

Then for any g

∈ B+,>(S) we have K n

1 (g)

≥

K n

2,h(g), where [K2,h(g)](x) := h(x)[K2(g)](x).

The next lemma, relating the continuity of exponents to continuity in operator norm, is useful when

studying the continuity of exponents.

Lemma 1.6. For every 1
ℓ
then limℓ→∞ λ(Kℓ) = λ(K∞).

≤

≤ ∞

let Kℓ be a bounded linear operator on

(S). If limℓ→∞ k

Kℓ −

K∞k

B

= 0,

2. Results for MA and AR processes

In this section we consider our two main examples, moving-average processes and auto regressive processes.

2.1. Moving Average processes

Let
coeﬃcient vector a := (a1, . . . , aq)

ξi}i≥−q be a sequence of i.i.d. random variables from a continuous distribution function F . For a

Rq deﬁne the moving average (MA(q)) process

{

Zi}i≥0 by setting

{

∈

q

Zi := ξi +

ajξi−j ,

i = 0, 1, 2, . . . .

Deﬁne the operator K mapping

j=1
X

Cb(Rq) to itself by

g(x2,

, xq, y)F (dy).

Kg(x1, . . . , xq) =

j=1 aj xq+1−j >0

Zy+Pq
Theorem 2.1. For all MA(q) processes with P(Z0 ≥
Zi ≥

log P( min
0≤i≤n

0) = log βF (a).

lim
n→∞

1
n

· · ·

0) < 1, there is a βF (a)

[0, 1) so that

∈

(2.1)

(2.2)

Further, if βF (a) > 0 then βF (a) is the largest eigenvalue of the operator K deﬁned in (2.1), and the
corresponding eigenfunction ψ(
·

) is non-negative and continuous.

The next theorem establishes the continuity of the MA(q) persistence exponent.

Theorem 2.2. In the setting of Theorem 2.1, the function a

βF (a) is continuous on Rq.

7→

Theorem 2.1 shows that βF (a)

∈

and any innovation distribution with a continuous density, we have

Persistence exponents in Markov chains

[0, 1). As noted in [20, 22], for the particular case q = 1 and a1 =

5

1

−

P( min
0≤i≤n

Zi ≥

0) = P(ξ−1 < ξ0 < . . . < ξn) =

1
(n + 2)!

,

(2.3)

and so βF (

1) = 0. The next proposition gives a necessary and suﬃcient condition for βF (a) > 0.

−

Zi}i≥0 is a MA(q) process such that P(ξ1 > 0) > 0, P(ξ1 < 0) > 0. Then
Proposition 2.3. Suppose that
{
βF (a) > 0 if and only if
=
j=1 aj 6
−

1.

q

2.2. Auto-regressive processes

P

{

Let
to the Lebesgue measure. Let a := (a1, . . . , ap)
Rp independent of the sequence
(Z0, . . . , Zp−1)

ξi}i≥p be a sequence of i.i.d. random variables of law F possessing a density function φ(
) with respect
·
Rp be a vector, called coeﬃcient vector. Given Z0 :=
∈
Zi}i≥p by setting
ξi}
{

, we deﬁne an AR(p) process

∈

{

p

Zi :=

ajZi−j + ξi,

i

p.

≥

j=1
X
The law of Z0 is denoted by µ and called the initial distribution. Furthermore, let K :

([0,

B

)p)

∞

→

([0,

B

∞

)p) be deﬁned by

Kψ(x1,

· · ·

, xp) :=

Zy+Pp

j=1 aj xp+1−j>0

ψ(x2,

· · ·

p

, xp, y +

ajxp+1−j )φ(y)dy.

(2.4)

j=1
X

Under the above assumptions, K maps

Cb

[0,

)p

∞

to itself.

The behavior of the persistence probabilities of AR processes is surprisingly rich. The recent work [11]
studies AR processes with a Gaussian innovation density, uncovering a rich structure of varying rates of
persistence decay from exponential to stretched exponential, polynomial and constant. On the contrary, our
approach is mostly equipped to handle the case when the persistence decay is exponential, so we concentrate
on a part of this spectrum. One advantage of our approach is that it also gives the existence of a persistence
exponent, which, to the best of our understanding, does not follow from [11].

(cid:17)

(cid:16)

In this paper we treat two particular sub parameter regimes while studying persistence exponents for the
. The behavior of the operator is somewhat
ai|

AR(p) process, namely a
diﬀerent in these two regimes, so we have to treat them separately.

p
i=1 |

0 and

Rp :

< 1

≤

∈

a

{

{

}

), initial distribution µ satisfying µ(U ) > 0 for

Theorem 2.4. Fix p
all open U

∈
+, and let

∈
(a) There is a θF (a)

Rp

≤

0, an innovation density φ(
·
Zi}i≥0 be the associated AR(p) process.
{
[0, 1], independent of µ, such that

N, a

∈

P

lim
n→∞

1
n

log Pµ( min
0≤i≤n

Zi ≥

0) = log θF (a).

Further, if θF (a) > 0, then θF (a) is the largest eigenvalue of the operator K from (2.4), viewed
to itself. The corresponding eigenvector ψ is non-negative and
as an operator mapping
continuous.

Cb

[0,

∞

)p

(cid:16)

(cid:17)

(b) If PF (ξ1 > 0) > 0 then θF (a) > 0, and if PF (ξ1 > 0) < 1 then θF (a) < 1.
(c) If a(k) is a sequence of vectors in (

, 0]p converging to a and ap < 0, then limk→∞ θF (a(k)) = θF (a).

−∞

As the next proposition shows, the persistence exponent may not exist for some initial distributions, if

the coeﬃcient vector a in Theorem 2.4 is allowed to have positive entries.

6

Aurzada, Mukherjee and Zeitouni

Proposition 2.5. Suppose
[0, 1).

Zi}

{

is an AR(1) process with innovation distribution F =

(0, 1), and a1 ∈

N

(a) If the initial distribution is

:=

N

N

(0, 1/(1

−

a2
1)), then the exponent θF (a1,

) deﬁned by

N

log θF (a1,

) := lim
n→∞

N

1
n

log P( min
0≤i≤n

Zi ≥

0)

exists, belongs to (0, 1), and is continuous as a function of a1.

(b) If the initial distribution µ satisﬁes

lim sup
M→∞

1
log M
1
log M

lim inf
M→∞

log Pµ(Z0 > M ) =0,

log Pµ(Z0 > M ) =

,
−∞

then there exist sequences

mk}k≥1 and

{

nk}k≥1 such that

{

lim
k→∞

lim
k→∞

1
mk
1
nk

log P( min

0≤i≤mk

Zi ≥

0) =0,

log P( min
0≤i≤nk

Zi ≥

0) = log θF (a1,

),

N

where θF (a1,

N

) is as in part (a). In particular, the limit

(2.5)

(2.6)

(2.7)

(2.8)

lim
n→∞

1
n

log P( min
0≤i≤n

Zi ≥

0)

(0, 1).

does not exist for any a1 ∈

(c) If µ = δx0 for some x0 > 0, then the exponent θF (a1, δx0) deﬁned by

log θF (a, δx0) := lim
n→∞

1
n

log P( min
0≤i≤n

Zi ≥

0)

exists, and equals θF (a1,

) of part (a).

(cid:16)

(cid:17)

)
∞

(0, 1) and F =

N
It follows that the AR(1) operator K = PS with a1 ∈
[0,

(0, 1) is no longer compact on
, as otherwise Theorem 1.2 would be applicable with K = PS, giving the existence of an exponent
Cb
in part (b). On the other hand, there does exist a universal exponent for initial distributions consisting of
a single atom. This motivates our focus in this paper on studying nonatomic initial distributions. It may
be possible to weaken the assumption that the initial distribution puts mass on all open sets. In particular,
one may be able to adapt the technique used in the recent work [8], and work with a weighted sup norm
as opposed to the usual sup norm considered in this paper. It is however not clear whether the Harnack
condition used in [8] holds for AR processes, particularly without extra assumptions on the innovation
density.

N

In order to derive an existence result for situations where the operator PS is not compact, one needs
to make a judicious choice of the operator K in Theorem 1.2. This requires additional assumptions on the
initial measure and innovation. We focus below on the contractive case

< 1.

Theorem 2.6. Fix p
∈
distribution µ satisfying µ(U ) > 0 for all open U
Further assume that there exists δ > 0 such that

N, parameters a satisfying

∈

p
aj|
j=1 |
+, and let

Rp
P

p
j=1 |

aj|
< 1, an innovation density φ(
·

), an initial
Zi}i≥0 be the associated AR(p) process.

P

{

p−1

Eµ

exp

δ

Zj

1{min0≤i≤p−1 Zi≥0}

<

h

n

j=0
X

o

i

∞

(2.9)

Persistence exponents in Markov chains

log φ(t) < 0.

7

(2.10)

and

lim sup
|t|→∞

1
t

|

|

(a) There is a θF (a)

∈

[0, 1], independent of µ, such that

lim
n→∞

1
n

log Pµ( min
0≤i≤n

Zi ≥

0) = log θF (a).

Further, if θF (a) > 0, then θF (a) is an eigenvalue of the operator K on
The corresponding eigenfunction ψ is non-negative and continuous.
(b) If PF (ξ1 > 0) > 0 then θF (a) > 0, and if PF (ξ1 > 0) < 1 then θF (a) < 1.
θF (a) is continuous on the set
(c) The function a

< 1.

p
j=1 |

aj|

7→

Cb

[0,

)p

∞

(cid:16)

(cid:17)

deﬁned by (2.4).

As mentioned before, the proof of Theorem 2.6 employs a modiﬁed version of the operator PS, which now
< 1. The motivation behind the modiﬁcation of the operator borrows
turns out to be compact if
from [3, 5, 23], where a similar strategy was used to deal with AR(1) processes with Gaussian innovations
starting at stationarity. An equivalent proof of Theorem 2.6 might be obtained by replacing the sup norm
)p) by a weighted sup norm with geometrically growing weights, which ensures that PS
topology on
is compact with respect to this new topology.

Cb([0,

p
j=1 |

aj|

P

P

∞

We also note that the assumption of existence of a ﬁnite exponential moment for the initial distribution

is only needed for the upper bound, in the sense that even if (2.9) is removed, one can still show that

lim inf
n→∞

1
n

log P( min
0≤i≤n

Zi > 0)

≥

log θF (a),

where θF (a) is the universal exponent from Theorem 2.6. Only the upper bound may fail here without (2.9),
as shown in Proposition 2.5.

7→

∞

2.2.1. Strict monotonicity of the exponent
)p, a simple coupling argument shows that for any initial
If we restrict a to the non negative orthant [0,
distribution µ the function a
0) is monotonically non-decreasing in a. This implies
)p, provided it exists.
that the exponent θF (a, µ) with initial distribution µ is non-decreasing in a
Note however that if in Proposition 2.5, the limit in (2.6) equals 0, then the same proof shows that the
θF (a1, µ)
corresponding exponent θF (a1, µ) equals 0 for all a1 ∈
is not strictly monotone.

(0, 1), and consequently the function a1 7→

Pa(min0≤i≤n Zi ≥

Our next theorem shows that if F has a log concave density on R and the initial distribution µ has ﬁnite
θF (a) is strictly increasing
exponential moment, then the exponent θF (a, µ) is free of µ, and the map a
. The exponential decay of log concave densities ensures that (2.10) holds,
on the set
and so Theorem 2.6 guarantees the existence of a non-trivial exponent which is free of the initial distribution
µ.
Theorem 2.7. Assume that φ is a strictly positive log concave density over R, that a
and that µ satisﬁes (2.9). Then b

= a implies θF (b) > θF (a).

p
j=1 aj < 1,

p
j=1 aj < 1

a with b

0 :

[0,

P

∞

7→

0,

≥

≥

∈

a

}

{

≥

P

We complete the picture on the positive orthant through the next proposition, which states that the

persistence exponent θF (a) = 1 for all a

0 such that

≥
Proposition 2.8. Assume that a
0 and
and the innovation density satisﬁes P(ξ1 > 0) > 0, then θF (a, µ) = θF (a) = 1.

p
j=1 aj > 1. If the initial distribution satisﬁes µ((0,

P

≥

p
j=1 aj > 1, for any innovation distribution F .

)p) > 0,

∞

p
j=1 aj < 1

Proposition 2.8, together with Theorems 2.6 and 2.7, gives an almost complete picture in terms of
θF (a, µ) is continuous and non-decreasing on
monotonicity on the positive orthant. The function a
a :
. If further the innovation den-
, and identically equal to 1 on the set
{
sity is log concave and the initial distribution has ﬁnite exponential moment, then the exponent is strictly
P
. In the critical case, the exponent is usually one, as shown in some speciﬁc
increasing on
examples in [6, 11].

p
j=1 aj > 1

p
j=1 aj < 1

7→
{

a :

a :

P

}

}

{

}

P

P

6
8

Aurzada, Mukherjee and Zeitouni

2.2.2. Positivity of the exponent
Part (b) of Theorem 2.4 and Theorem 2.6 give conditions ensuring that the exponent is non-trivial, i.e. the
persistence probability decays at an exponential rate. The next proposition generalizes this to show that no
matter what the coeﬃcient vector a may be, the exponent can never be 0, i.e. the persistence probability
can never decay at a super exponential rate.

Proposition 2.9. Fix p
its support, and µ satisfying µ((0, δ)p) > 0 for every δ > 0. Let
Then,

N, parameters a, an innovation distribution such that 0 is an interior point of
Zi}i≥0 be the associated AR(p) process.

∈

{

log P( min
0≤i≤n
In particular, if θF (a, µ) exists then it must be positive.

lim inf
n→∞

1
n

Zi ≥

0) >

.
−∞

2 forces Z1 =

Remark 2.10. One cannot dispense completely of the assumptions in Proposition 2.9. Indeed, concerning
2 , µ((2, 4))) = 1 and PF ((0, 1)) = 1, one sees that
the condition on initial distribution, when p = 1, a1 =
1
2 Z0 + ξ1 < 0, and so θF (a1, µ) = 0. On the other hand, concerning the condition on
Z0 ≥
the innovation distribution, if p = 1, a1 = 1, PF ((
for all x > 0, one
−
n) = e−n2
obtains that P(min0≤i≤n Zi ≥

1,
, and so again θF (a1, µ) = 0.

2)) = 1 and µ((x,

P(Z0 ≥

)) = e−x2

∞

0)

−

−

≤

−

1

3. Exponents for concrete cases

Using our operator approach, we can compute the persistence exponent in a number of concrete examples.

3.1. MA(1) processes

We ﬁrst consider MA(1) processes, starting with uniform innovation density.

Zi}i≥0 be a MA(1) process with a1 = 1 and innovation density φ = (a + b)−11(−a,b),

{

Proposition 3.1. Let
where a, b > 0.

• If a

b then

≥

• If a < b then

P( min
0≤i≤n

Zi ≥

0) =

4b
π(a + b)

n+o(n)

.

(cid:17)

(cid:16)
0) = λn+o(n).

Zi ≥
where λ is the largest real solution to the equation

P( min
0≤i≤n

tan

(cid:18)

a
(a + b)λ

=

1
(1
−
1 + (1

−
−

(cid:19)

2a/(a + b))/λ
2a/(a + b))/λ

.

(3.1)

For a = b in Proposition 3.1, one obtains βF (1) = 2/π. The next theorem shows that for continuous

symmetric innovation distributions this value is universal.

Theorem 3.2. Let

Zi}i≥0 be a MA(1) process with a1 = 1 and symmetric innovation density. Then

{

P( min
1≤i≤n

Zi ≥

0) = P( min
0≤i≤n

ξi + ξi−1 ≥

0) =

2
(π/2 + 2πk)n+2 =

2
π

(cid:18)

n+o(n)
.

(cid:19)

Xk∈Z

Theorem 3.2 ﬁrst appears in [22], where the proof technique is diﬀerent.

Proposition 3.3 below shows that the universality in Theorem 3.2 does not extend to discrete distributions.
In fact, for discrete innovation distributions F , there can be non-trivial diﬀerences between the two quantities

Indeed, we have the following proposition, whose proof is elementary and left to the reader.

P( min
0≤i≤n

Zi > 0)

and

P( min
0≤i≤n

Zi ≥

0).

Persistence exponents in Markov chains

9

Proposition 3.3. Let
equal

1 with probability 1/2. Then

Zi}i≥0 denote an MA(1) process with a1 = 1 and Rademacher innovations, i.e. ξi

{

±

P( min
0≤i≤n

Zi > 0) = (1/2)n+2,

while

P( min
0≤i≤n

Zi ≥

0) =

1
2

+

1
√5

(cid:18)

(cid:19)  

1 + √5

4 !

n+1

1
2 −

1
√5

+

(cid:18)

(cid:19)  

1

√5
−
4 !

n+1

.

Our ﬁnal MA example considers MA(1) processes with exponential innovation distribution.

Proposition 3.4. Let
vations. Then

{

3.2. AR(1) processes

Zi}i≥0 denote an MA(1) process with a1 ∈
Zi ≥

(
−
0) = (1 + a1)n+o(n).

P( min
0≤i≤n

1, 0) and standard exponential inno-

We now consider persistence exponent for AR(1) processes with uniformly distributed innovations.

Proposition 3.5. Let
Zi}i≥0 be an AR(1) process with a1 =
innovation density φ = (a + b)−11(−a,b), where a, b > 0. Then

{

−

1, arbitrary initial distribution µ, and with

(cid:18)
Our ﬁnal example concerns exponential innovations.

P( min
0≤i≤n

Zi ≥

0) =

2b
π(a + b)

n+o(n)

.

(cid:19)

Proposition 3.6. Let
exponential innovations. Then

{

Zi}i≥0 be an AR(1) process with a1 < 0, arbitrary initial distribution µ, and standard

P( min
0≤i≤n

Zi ≥

0) =

n−1

1

1

(cid:18)

−

a1 (cid:19)

Eea1Z0 1{Z0≥0}.

4. Proof of the results of Section 1

Proof of Theorem 1.2. The upper bound is simple: using that 1(
·

)

∈ Cb(S), we obtain from (1.2) that

[K n(1)](x)µ(dx)

[K n1](x) =

sup
x∈S

K n(1)

k∞ ≤

k

≤

λ(K)n+o(n).

ZS

We turn to the lower bound. We may and will assume that λ := λ(K) > 0 since otherwise there is nothing
is a Banach space (even
g(x)
|
7→ Cb(S) the k-fold composition of K
Cb(S)), by assumption (i), K k is a compact operator.

left to prove. Note that
g
k
if S is not compact, see [14], p. 257). Thus denoting by K k :
(note that we consider K k acting on the smaller space
Further,

Cb(S) equipped with the sup norm

k∞ := supx∈S |
Cb(S)

lim
n→∞

k

(K k)n

k

1/n

=

lim
n→∞

(cid:18)

k

(cid:16)

k

1
kn

= λ(K)k > 0,

(cid:19)

(cid:0)
and so an application of the Krein-Rutman theorem (see [10, Theorem 19.2] and [1, Problem 7.1.9]) yields
the existence of a non-negative continuous function ˜ψ

= 0, such that

(cid:1)

K nk(1)

k∞

(cid:17)
∈ Cb(S), ˜ψ
x

S.

∀

∈

K k ˜ψ(x) = λk ˜ψ(x),

6
10

Setting

Aurzada, Mukherjee and Zeitouni

k−1

ψ(x) :=

λa[K k−1−a( ˜ψ)](x),

we note that ψ
telescopic cancellation gives

∈ Cb(S). Also note that ψ(x)

a=0
X
λk−1 ˜ψ(x), and so ψ is non-zero and non-negative. Finally, a

≥

Kψ

−

λψ =

k−1

a=0
X

λaK k−a( ˜ψ)

k−1

−

a=0
X

λa+1K k−1−a( ˜ψ) = K k ˜ψ

λk ˜ψ = 0,

−

and so Kψ = λψ. Thus, setting c :=

ψ

k∞ > 0, we obtain (using that K preserves the order)

k

[K n(1)](x)

1
c

≥

[K n(ψ)](x) =

1
c

λnψ(x).

Integrating the last inequality with respect to µ gives

[K n(1)](x)µ(dx)

ZS

S ψ(x)µ(dx)
c

≥ R

λn.

Since

S ψ(x)µ(dx) > 0 by assumption (ii) on µ, the lower bound in (1.4) follows at once.
R

Finally, the fact that λ = λ(K) is the largest eigenvalue of K follows from the fact that λk is the largest
eigenvalue of K k, another consequence of the Krein-Rutman theorem. Also, existence of the left eigenvector
m follows from [10, Exercise 12, p. 236], along with the observation that the dual of
Cb(S) is the space of
bounded, ﬁnitely additive regular measures on S, see [14, Theorem IV.6.2.2].

Proof of Lemma 1.5. Since g
n
1. Using condition (i) K1(g)
the statement for general i, we proceed by induction:

∈ B+,>(S), using assumption (ii) we have K i
≥

≤
h(x)K2(g) = K2,h(g), which is the desired conclusion for i = 1. To verify

∈ B+,>(S) for all 1

1(g)

≤

−

i

K i

1(g) = K1(K i−1

1

(g))

≥

h(x)K2(K i−1

1

(g)) = K2,h(K i−1

1

(g))

K i

2,h(g).

≥

In the last display, we use the fact that K i−1
∈ B+,>(S) along with condition (i) for the ﬁrst inequality,
and the induction hypothesis along with the fact that K2,h preserves the ordering in the second inequality,
which is true of any non negative operator.

(g)

1

Proof of Lemma 1.6. Since
k
scaling all operators involved if necessary, we can assume that
(0, 1/2) arbitrary we have

converges to 0, w.l.o.g. assume
K∞k ≤

K∞k

Kℓ −

1 and δ

k

f

K∞k ≤
Kℓ −
k
1. Thus, for any f

1. Also w.l.o.g. by
(S) with

∈ B

k

k∞ ≤

K n
ℓ f

k

∈
(K∞ + Kℓ −
k∞ =
k
n
nδ
nδ

≤⌊

⌋

(cid:18)
⌊

k
⌋(cid:19)

K∞)nf

k∞

K n−⌊nδ⌋
∞

+ 2n

k

Kℓ −

k

K∞k

⌊nδ⌋.

On taking sup over f , invoking (1.2) along with Stirling’s approximation gives

λ(Kℓ)

≤

max

δ−δ(1

(cid:16)

Letting ℓ

→ ∞

followed by δ

0 gives

→

δ)1−δλ(K∞)1−δ, 2

−

Kℓ −

k

K∞k

δ

.

(cid:17)

which is the upper bound. The lower bound follows by a symmetric argument, reversing the roles of Kℓ and
K∞.

lim sup
ℓ→∞

λ(Kℓ)

≤

λ(K∞),

Persistence exponents in Markov chains

11

5. Proofs of the results of Section 2

5.1. Proof of results in Subsection 2.1

n
q+1 ⌋

⌊
0)m+1,

Proof of theorem 2.1. The MA(q) process is q-dependent, and so with m =

we have

P( min
0≤i≤n

Zi ≥

0)

≤

P( min
0≤i≤m

Zi(q+1) ≥

0) = P(Z0 ≥

from which βF (a) < 1 follows.

The sequence

{
Theorem 1.2 with k = q + 1. Setting X(i) := (ξi−q, . . . , ξi) we have that
Markov chain on Rq+1. Thus, with

Zi}i≥0 is well deﬁned and stationary. We now show existence of the exponent using
}i≥0 is a time homogenous

X(i)

{

S :=

{

q

x : xq+1 +

ajxq+1−j > 0

j=1
X

,

}

the q + 1 fold operator P q+1

S

is given by

[P q+1
S

(g)](x1, . . . , xq+1)

g(xq+2, . . . , x2q+2)

=

Rq+1

Z

2q+2

Yℓ=q+2

1xℓ+Pq

j=1 aj xℓ−j>0dF (xℓ),

where F is the distribution function of the innovation distribution. Thus for any sequence
gnk∞ ≤

1 we have

k

P q+1
S

(gn)

P q+1
S

(gm)

−

k∞ ≤ k

Hn −

Hmk∞,

where

Hn(sq+2, . . . , s2q+1)

k

gn}n≥1 such that

{

:=

Rq+1

Z

gn(xq+2,

, x2q+2)

· · ·

2q+2

Yℓ=q+2

1xℓ+sℓ+Pℓ−q−2

j=1

aj xℓ−j >0dF (xℓ),

q
j=ℓ−q−1 ajxℓ−j for ℓ

with sℓ :=
[q + 2, 2q + 1], and s2q+2 := 0. It thus suﬃces to show that Hn is
Cauchy in sup norm along a subsequence. To this end, we consider three sub cases depending on the value
P
of s := (sq+2,
(a) If sℓ <

, s2q+1).
L for some ℓ

[q + 2, 2q + 1] then

∈

· · ·
−

∈

Hn(sq+2, . . . , s2q+1)

P(ξℓ +

≤

ℓ−q−2

j=1
X

ajξℓ−j > L),

and so given ε > 0 there exists L = L(ε) <

such that

∞

sup
s∈Rq:minq+2≤ℓ≤2q+1 sℓ<−L

Hn(s)

ε.

≤

(5.1)

(b) If s

Rq is such that for some r

1, . . . , q

the coordinates sℓ1, . . . , sℓr are in [

L, L], and the other

−
coordinates sℓr+1, . . . , sℓq are larger than L, then setting Hn,ℓ1,...,ℓr (sℓ1 , . . . , sℓr ) to equal

∈ {

∈

}

gn(xq+2,

· · ·

Rq+1

Z

r

, x2q+2)

1

Yk=0

xℓk +sℓk +P

ℓk −q−2
j=1

aj xℓk −j >0

2q+2

Yℓ=q+2

dF (xℓ)

12

we have

Aurzada, Mukherjee and Zeitouni

Hn(s0, . . . , sq)

|

−

Hn,ℓ0,...,ℓr (sℓ0, . . . , sℓr )

| ≤

P(

r
k=0{
∪

ξℓk +

ℓk−q−2

ajξℓk−j <

L

),

}

−

j=1
X

and so again by choosing L large enough we can ensure that

Hn(s)

−

Hn,ℓ0,...,ℓr (sℓ0, . . . , sℓr )

| ≤

ε.

(5.2)

sup
s∈Rq:sℓk ∈[−L,L],1≤k≤r,sℓk >L,r+1≤k≤q |
Rq is such that sℓ ∈

−

∈

[

(c) If s

L, L] for all ℓ

[q + 2, 2q + 1], then we have

∈
ℓ−q−2

Hn(s)

|

−

Hn(t)

| ≤

P(
{

min
q+2≤ℓ≤2q+1

sℓ + ξℓ +

ajξℓ−j > 0

j=1
X

∆
{

}

min
q+2≤ℓ≤2q+1

tℓ + ξℓ +

ℓ−q−2

j=1
X

ajξℓ−j > 0

),

}

t
k∞ < δ, we have
L, L]q. By the Arzel`a-Ascoli theorem, we have that
−

where ∆ is the symmetric diﬀerence between two sets. Since the right-hand side in the last display is
L, L]2q (due to the assumption of continuity of
continuous in the arguments s, t over the compact set [
F ), and vanishes on the set s = t, it follows that given η > 0 there exists δ = δ(η, L) such that whenever
s
< η. In particular, this means that Hn(.) is uniformly equicontinuous
k
−
Hn}n≥1 is compact with respect to sup norm
on [
L, L]q, and so there exists a subsequence which is Cauchy in sup norm. A similar argument
topology on [
applies to each of the functions Hn,ℓ0,...,ℓr for all choices of r
which are subsets
ℓ1, . . . , ℓr}
of size r. Thus by going through subsequences, we may assume all the functions
of
Hn,ℓ1,...,ℓr are Cauchy in sup norm on [

q + 2, . . . , 2q + 1

Hn(t)
|

L, L]q.

1, . . . , q

Hn(s)

and

∈ {

−

−

−

}

{

{

{

}

|

Taking limits along the subsequence from step (c) gives and using (5.1) and (5.2) gives

−

lim sup
m,n→∞

sup
s∈Rq |

Hn(s)

Hm(s)

2ε,

−
Hn}n≥1 is Cauchy in sup norm on Rq. Thus it follows by an application of Theorem 1.2 that the
and so
operator PS has largest eigenvalue βF (a). Finally note that [PS](g)(x0,
, xq) is by deﬁnition independent
of x0, and so w.l.o.g. the eigenfunction ψ can be taken to be a function of q variables giving the eigenvalue
equation

| ≤

· · ·

{

βF (a)ψ(x1,

, xq) =

· · ·

ψ(x2,
j=1 aj xq+1−j >0

Zy+Pq

· · ·

, xq, y)dF (y) = [K(ψ)](x1,

, xq),

· · ·

where K is as deﬁned in the theorem. Thus, K satisﬁes the desired eigenvalue equation.

Finally it remains to check condition (ii) in Theorem 1.2. To this end, setting A to be the support of F ,

X is a Markov chain on Aq+1. Since sets of the form

with

Uj, 0

{

j

q

}

≤

≤

open sets in R form a base of the topology on Aq+1 and since

x

{

∈

Aq+1 : xj ∈

Uj ∩

A, 0

j

q

}

≤

≤

P(ξj ∈

Uj ∩

A, 0

j

≤

≤

q) =

q

j=0
Y

P(ξj ∈

Uj ∩

A),

it suﬃces to show that P(ξ0 ∈
U
once from the assumption of a continuous distribution function.

∩

A) > 0 for every open (in R) set U which intersects A; this follows at

We have veriﬁed that the conditions of Theorem 1.2 hold; an application of the latter yields the existence

of βF (a), and hence completes the proof of Theorem 2.1.

Proof of Theorem 2.2. We recall the notation X(i) := (ξi−q, . . . , ξi) and set S = S(a) :=

q
i=1 ajxq−j > 0

.

}

(x0, . . . , xq)
|

{

xq+

P

Persistence exponents in Markov chains

Let

and

a(k)

{

}k≥1 be a sequence of vectors in Rq converging to a. Then for any 1
for j
1)(m + q) + 1, (j

1)(m + q) + m

(j

Ij :=

−

−

we have

(cid:2)

(cid:3)

n setting Mn :=

13

n
m+q ⌋

⌊

m

≤

≤

1,

≥

Pa(k)(Zi > 0, 0

i

n)

≤

≤
Pa(k) (Zi > 0, i

Pa(k) (Zi > 0, i

≤
∈
Ij ) = Pa(k) (Zi > 0, 1

Ij, 1

i

≤

Mn)

j

≤

m)Mn ,

≤

≤

∈

=

Mn

j=1
Y

which upon taking log, dividing by n, and letting n

we obtain that

→ ∞

log λ(PS(a(k) ))

1
m + q

≤

log P

a(k) (Zi > 0, 1

i

≤

≤

m).

Letting k
(Z1,

· · ·

→ ∞

, Zm) under a gives

and noting that the distribution of (Z1,

, Zm) under a(k) converges to the distribution of

· · ·

lim sup
k→∞

log λ(PS(a(k) ))

1
m + q

≤

log Pa(Zi > 0, 1

i

≤

≤

m),

which upon letting m

gives lim supk→∞ λ(PS(a(k)))

log λ(PS(a)), thus giving the upper bound.

We now turn to the lower bound. Fix M > 0, set SM = SM (a) := S(a)

→ ∞

≤

M, M ]q+1, and invoke

[

−

∩

Theorem 2.1 to obtain

P

a(k)(X(i)

S, 0

i

≤

≤

n)

≥

∈

P

a(k)(X(i)

SM , 0

i

≤

≤

∈

n) = λ(PSM (a(k)))n+o(n),

where PSM (·) is viewed as an operator on
([
λ(PSM (ak)). From this the lower bound will follow via Lemma 1.6 if we can show the following:

M, M ]q+1) (and not

(Rq+1)). This gives λ(PS(ak))

−

B

B

≥

lim
k→∞ k
lim sup
M→∞

PSM (a(k)) −
λ(PSM (a))

PSM (a)k

= 0,

λ(PS(a)).

≥

To show (5.3), for any f

([

−

∈ B

M, M ]q+1) such that

(PSM (a(k))f

|

−

PSM (a)f )(x1,

· · ·

, xq+1)

| ≤

ξℓ = xℓ, 1

ℓ

≤

≤

q + 1),

f

1 we have

k∞ ≤

k
P(A(a(k))∆A(a)
|

(5.3)

(5.4)

where ∆ denotes symmetric set diﬀerence, and

q

ξℓ +

ajξℓ−j > 0, q + 2

ℓ

≤

≤

2q + 2

.

}

A(a) :=

{

j=1
X
q
j=ℓ−q−1 ajxℓ−j for q + 2

Setting sℓ(a, x) :=

2q + 2 we have

ℓ

≤

≤

P
P(A(a(k))∆A(a)
|
ℓ−q−2

= P(
{

ξℓ +

j=1
X

ξℓ = xℓ, 1

ℓ

≤

≤

q + 1)

a(k)
j ξℓ−j + sℓ(a(k), x) > 0

}

∆
{

ℓ−q−2

ξℓ +

ajξℓ−j + sℓ(a, x) > 0

).

}

j=1
X
sℓ(a(k), x)

Since a(k) converges to a we have maxx∈[−M,M]q+1
sℓ(a, x)
|
of distribution functions gives that the RHS above converges to 0 as k
[

M, M ]q+1, and so we have veriﬁed (5.3).

−

|

= 0, which along with the continuity

, uniformly in (x1,

→ ∞

, xq+1)

∈

· · ·

−

14

Aurzada, Mukherjee and Zeitouni

Proceeding to verify (5.4), ﬁxing M, ε > 0 and invoking Theorem 2.1 there exists N := N (ε, M ) <

such that for all n

N we have

≥

∞

Pa(X(i)

SM , 0

i

≤

≤

n)

≤

∈

(λ(PSM (a)) + ε)n.

Thus with δ > 0 and ℓ :=

nδ

we have

Pa(X(i)
Pa(
(is)
∃
≤
+Pa(
(is)
∃

∈
∈

∈

⌊

⌋
S, 0
n)
i
≤
[0, n]ℓ : X(i)
[0, n]n−ℓ : X(is) /
∈

≤

∈

SM , i /

∈ {
SM , 1

i1,

≤

)

· · ·
s

, iℓ}
ℓ),
≤

(5.5)

where [0, n]ℓ is the set of all distinct integer tuples of size ℓ with entries in [0, n]. Fixing an increasing
sequence (i1,

ℓ with i0 := 0 we have

is−1 for 1

, iℓ)

s

· · ·

Pa(X(i)

∈

SM , i /

∈

[0, n]ℓ let js := is −
, is}

· · ·

i1,

)

∈ {

≤
≤
Pa(X(i)

≤
Y1≤s≤ℓ:js−q≥N
(λ(PSM (a)) + ε)Ps:js−q>N (js−q).

≤

∈

≤

SM , 1

i

q)

js −

≤

To estimate the exponent in the RHS above, ﬁrst note that

s:js−q≤N js ≤

(q + N )ℓ

≤

(q + N )nδ. Combining these two estimates gives

P

ℓ
s=1 js = n + 1

ℓ

−

≥

n

−

nδ, whereas

P

(js −

q)

≥

Xs:js−q>N

Xs:js−q>N

qℓ

js −

≥

n

−

nδ

−

(2q + N )nδ,

which on using (5.5) gives

Pa(X(i)

nδ

⌋
(cid:18)
(is)

≤ ⌊
+Pa(
∃

S, 0

i

n)

≤

∈
n + 1
nδ
[0, n]n−ℓ : X(is) /
∈

⌊
∈

⌋ (cid:19)

≤
(λ(PSM (a)) + ε)n−nδ−(2q+N )nδ

SM , 1

s

≤

≤

ℓ).

To estimate the second term in the right hand side of (5.6) note that X(i) /
∈
1)

M , and so

i
j=i−q |

ξi| ≥

P

P(X(is) /
∈

SM , 1

s

≤

≤

ℓ)

≤

n

P(

1

> M

ξi|

{|

} ≥

M
q + 1

)

P

≤

i=−q
X
Bin(n + q + 1, P(
|
(cid:16)

> M )) >

ξ1|

n
q + 1

δ

,

(cid:17)

(5.6)

M, M ]q+1 implies (q +

[

−

which on taking log, dividing by n and letting n
and so does not contribute. Thus, taking log, dividing by n and letting n

followed by M

→ ∞

→ ∞

gives

for every ﬁxed δ > 0,
on both sides of (5.6) gives

−∞

→ ∞

lim
n→∞

1
n

log Pa(X(i)

S, 1

i

≤

≤

n)

≤

(1

−

∈

δ(1 + 2q + N )) log(λ(PSM (a)) + ε).

On letting M

→ ∞

followed by δ

→

0 in the above display gives

which verﬁes (5.4) as ε > 0 is arbitrary, and hence completes the proof of the theorem.

log λ(PS,a)

lim inf
M→∞

≤

log

λ(PSM (a)) + ε

,

(cid:16)

(cid:17)

Persistence exponents in Markov chains

15

Proof of Proposition 2.3. We start with the “if” part. With a0 := 1, we have

q
j=0 aj > 0 (the case < 0 is analogous), and set

q

j=0 aj 6

= 0. Assume

P

P

A :=

aj > B :=

Xj∈{0,...,q}:aj >0

Xj∈{0,...,q}:aj <0

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

aj(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Since the distribution of ξ is continuous and P(ξ0 > 0) > 0, there exist x > 0, δ
P(ξj ∈

(x, x + δ)) > 0. This gives

0, x(A−B)
B

such that

(cid:17)

∈

(cid:16)

Ax

−

B(x + δ) = x(A

B)

−

−

Bδ > 0,

and so

P( min

i∈{0,...,n}

Zi > 0)

P(ξi ∈

≥

(x, x + δ),

q

−

≤

i

≤

n).

Indeed, to see this note that

q

Zi =ξi +

ajξi−j > x

j=1
X

Xj∈{0,...,q}:aj >0

aj| −

|

(x + δ)

> 0.

aj|

|

Xj∈{0,...,q}:aj <0

The desired conclusion then follows on noting that P(ξi ∈

(x, x + δ)) > 0.

We continue with the “only if” part of the proposition. Deﬁne the numbers bi :=

q
j=i+1 aj and
1. A short computation using the assumption

−

P

the MA(q
q
j=1 aj =

−
−

P

P( min
0≤i≤n

1)-process ˜Zi :=
1 shows that Zi = ˜Zi −
Zi > 0) = P( ˜Z0 < ˜Z1 < . . . < ˜Zn)

q−1
j=1 bjξi−j + ξi for i
˜Zi−1 for all i

P

≥

≥ −
0. Thus,

P( ˜Zkq < ˜Z(k+1)q, k

≤

0, . . . ,

n/q

).

∈ {

⌊
˜Zkq}k≥1 are independent (since

⌋}

Now note that the random variables
identically distributed (by the stationarity of ˜Z). Thus, (2.3) shows that the last probability equals 1/(
⌊
1)! giving βF (a) = 0.

1)-dependent) and
+

n/q

−

{

{

⌋

˜Zi}i≥1 is (q

5.2. Proof of the results of Section 2.2

Proof of Theorem 2.4. The proof is based on Theorem 1.2 with S := [0,
and consists in checking the assumptions there, and in particular the compactness of K p.

∞

⊂

)p

Rp, K = PS and k = p,

(a) If a = 0 then the process is i.i.d. for which all conclusions are trivial. Thus assume w.l.o.g. that a
= 0,
and that ap < 0 (otherwise we can reduce the value of p). Note that X(i) := (Zi−p+1, . . . , Zi) is a Markov
chain on Rp. Note that

[P p

S (g)](x1, . . . , xp) =

g(xp+1, . . . , x2p)

Z(0,∞)p

2p

p

φ(xℓ −

j=1
X

Yℓ=p+1

ajxℓ−j)dxℓ

=

g(xp+1,

Z(0,∞)p

· · ·

, x2p)H

(x1,

(cid:16)

, xp), (xp+1,

· · ·

· · ·

2p

, x2p)

dxℓ,

(cid:17)

Yℓ=p+1

where

H

(x1,

(cid:16)

· · ·

, xp), (xp+1,

· · ·

, x2p)

:=

(cid:17)

Yℓ=p+1

ajxℓ−j)

φ(xℓ −

j=1
X

2p

p

6
16

Aurzada, Mukherjee and Zeitouni

for (x1,
[0,

· · ·

)2p we have

, xp, xp+1,

∞

, x2p)

∈

· · ·

R2p. If xℓ > L for some 1

ℓ

≤

≤

p, then for (x1,

, xp, xp+1,

, x2p)

∈

· · ·

· · ·

xp+ℓ −

p

j=1
X

ajxp+ℓ−j ≥

xp+ℓ −

apxℓ ≥

xp+ℓ −

apL,

and so given a sequence of functions

gn}n≥1 such that
{
k
[P p
S (gn)](x)
sup
x∈S:kxk∞>L |

gnk∞ ≤
F (
1

| ≤

−

1 we have

apL),

−

where we recall that F is the distribution function of the innovation density φ. Therefore, given ε > 0 there
exists L = L(ε) <

such that

(5.7)

(5.8)

R such that

7→

φ(x)

−

R |
R

∞
[P p

sup
x∈S:kxk∞>L |

S (gn)](x)

| ≤

On the other hand, for x(1), x(2)

ε.

∈

[0, L]p we have

[P p

S (gn)](x(1))

[P p

S (gn)](x(2))

−

| ≤

Rp

H(x(1), y)

Z
Now given η > 0 there exists a non negative continuous integrable function ˜φ : R
˜φ(x)
|
˜H
and using a telescopic argument we have

dx < η, which in particular implies

(cid:12)
(cid:12)
(cid:12)
˜φ(x)dx

1 + η. Using this, setting

≤
˜φ(xℓ −

p
j=1 ajxℓ−j)

R
2p
R
ℓ=p+1

, xp), (xp+1,

, x2p)

(x1,

· · ·

· · ·

:=

(cid:17)

Q

P

dy.

−

H(x(2), y)
(cid:12)
(cid:12)
(cid:12)

|

(cid:16)

Rp |

Z

H(x, y)

2p

dy

−

˜H(x, y)
|
p

2p

p

2p

=

≤

Rp

Z
p

(cid:12)
(cid:12)
(cid:12)

r=1 Z
X

φ(xℓ −

Yℓ=p+1

p+r−1

ajxℓ−j)

j=1
X

p

−

Yℓ=p+1
2p

˜φ(xℓ −

φ(xℓ −

ajxℓ−j )

j=1
X

Yℓ=p+r+1

Rp "

Yℓ=p+1
˜φ(xp+r −

(cid:12)
(cid:12)
(cid:12)
(1 + η)p−rη

×

p

j=1
X

Rr−1

Z

Yℓ=p+1
ηp(1 + η)p.

(1 + η)p−rη

≤

ajxp+r−j)

φ(xp+r −

−

p+r−1

p

j=1
X

φ(xℓ −

ajxℓ−j )

ajxp+r−j)
(cid:12)
(cid:12)
(cid:12)
dxℓ

p+r−1

dxℓ

ajxℓ−j)
(cid:12)
(cid:12)
(cid:12)
ajxℓ−j)

p

Yℓ=p+1

j=1
X
˜φ(xℓ −

p

j=1
X

2p

#

Yℓ=p+1

dxℓ

≤

=

p

r=1
X
p

r=1
X

∈

(cid:12)
(cid:12)
(cid:12)

j=1
X

Yℓ=p+1

(5.9)

Finally, using the continuity of ˜φ along with Scheﬀe’s Lemma, there exists a δ = δ(η, L) such that for all
x(1), x(2)

[0, L]p with

x(2)

x(1)

k∞ < δ we have

dy < η.

(5.10)

˜H(x(1), y)

−

Rp

Z

k

−

˜H(x(2), y)
(cid:12)
(cid:12)
(cid:12)

Combining (5.8), (5.9), (5.10), we conclude that for any x(1), x(2)
have

[0, L]p such that

x(1)

k

−

x(2)

k∞ < δ, we

∈

[P p

S (gn)](x(1))

[P p

−

η + 2ηp(1 + η)p.

≤

S (gn)](x(2))
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

Persistence exponents in Markov chains

17

Since η > 0 is arbitrary, it follows that the sequence
by the Arzel`a-Ascoli theorem, there exists a subsequence along which
on [0, L]p. Taking limits along this subsequence and using (5.7) gives

S (gn)

}n≥1 is uniformly equicontinuous on [0, L]p.Thus
}n≥1 is Cauchy in sup norm

S (gn)

P p

{

{

P p

lim sup
m,n→∞ k

P p
S gn −

P p
S gmk∞ ≤

2ε,

and so we have proved the existence of a convergent subsequence in sup norm on [0,
compactness of K p. An application of Theorem 1.2 then yields part (a).

), and thus the

∞

(b) The fact that θF (a) > 0 follows from Proposition 2.9 along with the assumption that µ has full
)p) such

support, and PF (ξ1 > 0) > 0. For the other inequality, for any non-negative function g
that

1 we have

∈ B

([0,

∞

g

k

k∞ ≤
PS(g)(x1,

, xp)

· · ·

j=1 aj xp+1−j>0

=

≤

Zy+Pp
P(ξ1 > 0)

g(x2,

· · ·

p

, xp, y +

ajxp+1−j )φ(y)dy

j=1
X

and so θF (a) = λ(PS)

≤ k
(c) By assumption we have limk→∞ a(k)

PSk∞ ≤

P(ξ1 > 0) < 1.

p = ap < 0, and so there exists δ > 0 such that a(k)
p

1. Along with (5.7), this gives

k

≥

P p
a(k),Sf (x)

|

−

P p

a,Sf (x)

| ≤

2(1

−

F (δL)) + 2 sup

x∈[0,L]pk

P p
a(k) (x, .)

−

P p

a (x, .)

kT V ,

δ for all

≤ −

which on taking a sup over f such that
2(1
the desired conclusion.

F (δL)). Upon letting L

→ ∞

−

f

1 and letting k

k

k ≤
we have limk→∞ k

P p

a(k),S −

a,Sk

gives lim supk→∞ k

P p
P p
a,Sk∞ ≤
= 0, which, using Lemma 1.6, gives

a(k),S −

→ ∞
P p

The proof of Proposition 2.5 uses the next lemma, which is an adaptation of [12, Theorem 1.6] and
[13, Lemma 3.1] for discrete time Gaussian processes. Because of the discreteness of the involved processes,
we are able to verify continuity of the persistence exponent of any stationary Gaussian process (with non
negative correlations) in its levels (c.f. (5.13)), thereby removing one of the conditions of [12, Theorem 1.6].
}i≥0 be a discrete time centered Gaussian sequence with non negative

1 let
Zk(i)
{
), such that

Lemma 5.1. For all k
≥
,
covariance function Ak(
·
·
Ak(i, i + τ )

lim
k→∞

sup
i≥0 |

A(τ )
|

−

= 0,

for all τ

0,

≥

for some function A(
·

). Suppose further that

lim sup
k,τ →∞

sup
i≥0

log Ak(i, i + τ )
log τ

<

1.

−

Then for every r

R we have

∈

lim
k,n→∞

1
n

log P( min
0≤i≤n

Zk(i) > r) = lim
n→∞

1
n

log P( min
0≤i≤n

Z(i) > r),

where

Z(i)

}i≥0 is a centered stationary Gaussian sequence with covariance A(i

{

−

j).

(5.11)

(5.12)

18

Aurzada, Mukherjee and Zeitouni

Proof. To begin note that the proof of the continuous case [12, Theorem 1.6] goes through verbatim in the
R we
discrete case under (5.12) and the extra assumptions of [12, Theorem 1.6], namely that for every r
have

∈

lim
ε↓0

lim
n→0

1
n

log P( min
0≤i≤n

Z(i) > r

ǫ) = lim
n→0

−

1
n

log P( min
0≤i≤n

Z(i) > r);

and that for every z

∈

R and positive integer M , we have

P( sup

0≤τ ≤M

Z(τ ) < z)

≤

≤

≤

lim inf
k→∞

inf
i≥0

0≤τ ≤M
P( sup

0≤τ ≤M

sup
i≥0

Z(τ )

z).

≤

lim sup
k→∞
P( sup

0≤τ ≤M

P( sup

Zk(i + τ ) < z)

Zk(i + τ ) < z)

(5.13)

(5.14)

R. It thus remains
[12, Theorem 1.6] considers the case r = 0, but a similar argument applies for any r
to verify these two extra conditions, of which (5.14) follows from (5.11). Proceeding to verify (5.13), ﬁxing
and its
ε, δ > 0 and setting ℓ :=
complement, we have

and intersecting with the set

[0, n] : Z(i)

> nδ

ε, r]

nδ

(r

{|

−

∈

∈

∈

}

⌈

⌉

i

|

Z(i) > r

ε)

−

[0, n]n−ℓ : min
1≤s≤ℓ

Z(is) > r)

(5.15)

(5.16)

P( min
0≤i≤n
P(
∃
+P(
∃

(is)

≤

∈
(is)

≤

X(is)∈[0,n]n−ℓ

[0, n]ℓ : Z(is)
∈
P(Z(is) > r) +

(r

∈

−

s

ε, r], 1
ℓ)
≤
≤
P(Z(is) > (r

X(is)∈[0,n]ℓ

ε, r], 1

s

≤

≤

ℓ),

−

where [0, n]ℓ is the set of all integer tuples in [0, n]ℓ+1 with all entries distinct. For estimating the ﬁrst
there must be at least ℓ′ indices
≤
1. Thus, if B denotes the covariance matrix

{
N , where ℓ′

ε, r], 0

Z(is)

(r

≤

}

s

ℓ

∈
−
⌊nδ⌋
N , for any N

term in the RHS of (5.15), on the set
j1,
{
of

js−1 ≥
, we have

, jℓ′
· · ·
Z(j1),

with js −
, Z(j′
ℓ)
}

}
· · ·

{

≥

≥

ℓ′
max
i=1

Xj:j6=i

B(i, j)

2

≤

g(i),

∞

Xi=N

where g is a summable function satisfying

sup
i≥0,k≥1

Ak(i, i + τ )

g(τ )

≤

for all τ

≥

0, the existence of which is guaranteed by (5.12). By choosing N large enough we can ensure that

B(i, j)

B(i, i)
2

=

A(0)
2

.

≤

max
1≤i≤ℓ′

Xj:j6=i

Consequently, by the Gershgorin Circle Theorem all eigenvalues of B lie within [A(0)/2, 3A(0)/2]. This gives

ℓ′) =

(r−ε,r]ℓ′ e−z′B−1z/2dz
Rℓ′ e−z′B−1z/2dz

R

P(Z(js)

(r

ε, r], 1

s

≤

≤

−

∈
(r−ε,r]ℓ′ e−z′z/3A(0)dz
Rℓ′ e−z′z/A(0)dz

≤ R
3

≤

n+1
2 P(
R

N

(0, 3A(0)/2)

ε, r])

nδ
N .

(r

∈

−

R
3ℓ′/2P(

N

≤

(0, 3A(0)/2)

ε, r])ℓ′

(r

∈

−

Plugging this in the ﬁrst term of the RHS of (5.15) we get the bound

Persistence exponents in Markov chains

19

(n + 1)(2√3)n+1P(

N

(0, 3A(0)/2)

(r

ε, r])

nδ

N ,

∈

−
followed by ε

, for every δ > 0 ﬁxed.
which on taking log, dividing by n and letting n
Thus this term does not contribute to the limit. For estimating the second term in the RHS of (5.15), using
the non negativity of the correlation function along with Slepian’s inequality, we get

0 gives

→ ∞

−∞

→

P( min

0≤s≤n−ℓ

Z(is) > 0)

P(min0≤i≤n Z(i) > 0)
P(Z(0) > 0)nδ

.

≤

This gives the bound

(n + 1)

n + 1
nδ

(cid:18)

⌈

⌉ (cid:19)

P(min0≤i≤n Z(i) > 0)
P(Z(0) > 0)nδ

for the second term in the RHS of (5.15). Taking log, dividing by n and letting n
we conclude that

→ ∞

followed by δ

0,

→

lim
ε↓0

lim
n→∞

1
n

log P( min
0≤i≤n

Z(i) > r

ε)

lim
n→∞

≤

−

1
n

log P( min
0≤i≤n

Z(i) > r),

verifying (5.13), and hence completing the proof of the lemma.

{

Proof of Proposition 2.5. (a) In this case
summable correlations, and the conclusions follow from Slepian’s Lemma along with sub-additivity.
(b) Note that (2.5) implies the existence of a sequence of positive reals
Pµ(Z0 > xk)
≥
) if necessary, we can also assume that √εk log xk diverges to +
max(εk,
and ﬁxing a1 ∈
P( min

Zi}i≥0 is a stationary Gaussian sequence with non-negative
such that
xk}
∞
is a positive sequence converging to 0. W.l.o.g., by replacing εk by
√εk log xk⌋

εk}
(0, 1), for any M <

, for all k large enough we have

∞
P(Z0 > xk,

. Setting mk :=

diverging to +

, where

1
log xk

x−εk
k

mk).

M, 1

∞

0)

{

{

⌊

i

0≤i≤mk

Zi ≥

≥

ξi| ≤

|

≤

≤

Indeed, this is because on this set we have

Zi = ai

1Z0 +

i−1

j=0
X

aj
1ξi−j > amk

1 xk −

M

a1

1

−

k → ∞
.
−→ ∞

Therefore, for k large enough (depending on a1 and M )

P( min

0≤i≤mk

Zi ≥

0)

P(Z0 > xk)P(
|

≥

ξ1| ≤

M )mk

x−εk
k

P(
|

ξ1| ≤

≥

M )mk ,

implying that

lim inf
k→∞

1
mk

log P( min

0≤i≤mk

Zi ≥

0)

≥

log P(
|

ξ1| ≤

M ).

Proceeding to verifying (2.8), use (2.6) to get the existence of a sequence of positive reals

, and for any δ > 0 set n′

y−Nk
k

≤

, where
k :=

Nk}
log yk−log δ
log(1/a1) ⌉

to note that

is a sequence of positive reals diverging to +

{

yk}k≥1 diverging
. Set

∞

Upon letting M

, (2.7) follows.

to +
nk :=

→ ∞
, such that Pµ(Z0 > yk)
∞
√Nk log yk⌉
⌈
P( min
Zi ≥
0≤i≤nk

0)

≤

{
⌈
Z0 ≤
Yi+n′

P(Z0 > yk) + P(0

≤

yk, min

1≤i≤nk

0)

Zi≥

y−Nk
k

≤

+ P( min

0≤i≤nk−n′
k

k ≥ −

δ),

(5.17)

20

Aurzada, Mukherjee and Zeitouni

i

ℓ=1 ai−ℓ

ξℓ for 1

where Yi :=
ﬁrst term in the RHS of (5.17) on taking log, dividing by nk and letting k
consider the second term. To this eﬀect, note that the sequence
negative covariance

n, and we use the fact that n′

k < nk for all k large enough. Since the
, it suﬃces to
gives
k }i≥0 is a Gaussian sequence with non

Yi+n′

→ ∞

−∞

P

≤

≤

i

1

which satisﬁes (5.11) of Lemma 5.1 with A(τ ) := aτ
an application of Lemma 5.1, for any r

Ak(i, i + τ ) := aτ

1(1 + a2
1(1

−

1 +
· · ·
1)−1. Also, (5.12) is immediate as well, and so by
a2

),

1

R we have the second term in the RHS of (5.17) satisﬁes

{
+ a2(i+n′

k−1)

∈

lim
k→∞

1
nk

log P( min
n′

k≤i≤nk

Yi≥

r) = lim
n→∞

1
n

log PN ( min
0≤i≤n

Z(i)

≥

r).

(5.18)

Using this with r =
0 and noting
that (5.13) holds for any centered stationary Gaussian process with non negative summable correlations, as
shown in the proof of Lemma 5.1.

δ along with (5.17) gives the upper bound in (2.8), upon letting δ

→

−

To get the lower bound of (2.8), note that

P( min
0≤i≤nk

Zi ≥

0)

P(Z0 ≥
P(Z0 ≥
P(Z0 ≥

≥

≥

≥

0, min
1≤i≤n′
k

Yi > 0, min

0≤i≤nk−n′
k

Yi+n′

k

> 0)

0, min
1≤i≤n′
k
0)P(ξ1 > 0)n′

ξi > 0, min

0≤i≤nk−n′
k

k P( min

0≤i≤nk−n′
k

Yi+n′

k

> 0)

Yi+n′

k

> 0),

(5.19)

where the last inequality uses positive association. The lower bound of (2.8) follows from (5.19), on noting
that the the ﬁrst two terms in (5.19) do not contribute to the limit, and the third term can be analyzed
using (5.18) with r = 0.
(c) The proof of part (c) follows by a similar argument as that of part (b). For the upper bound, ﬁxing
x0 ∈

R and δ > 0, setting N := max

log |x0|−log δ
log 1
a1

we have

1,

P( min
1≤i≤n

Zi ≥

0

|

Z0 = x0)

≤

n
(cid:6)
P( min
N ≤i≤n

Yi ≥ −

δ)

(cid:7)o

where Yi =
gives

P

i

ℓ=1 ai−ℓ

1

ξℓ. An argument similar to the proof of part (b), using Lemma 5.1 with r =

δ, then

−

Yi ≥ −
For the lower bound, using Slepian’s Lemma gives

log P( min
N ≤i≤n

lim
n→∞

lim
δ↓0

1
n

δ) = lim
n→∞

1
n

log PN ( min
0≤i≤n

Z(i)

≥

0).

P( min
1≤i≤n

Zi ≥

0

|

Z0 = x0)

P( min

1≤i≤N −1

≥

Zi ≥

0

|

Z0 = x0)P( min
N ≤i≤n

Zi ≥

0

|

Z0 = x0).

Invoking Lemma 5.1 with r = 0 controls the second factor in the RHS of the last display, whereas the ﬁrst
factor remains bounded away from 0 and therefore does not contribute to the limit.

Proof of Theorem 2.6. (a) By assumption there exists δ1 > 0 such that Eeδ1 Pp−1
such that φ(t)

. Invoking (2.10) gives the existence of δ2 > 0 and C2 <
1, deﬁne

∞
min(δ1, δ2/p), and set h(x) := eδ Pp−1

j=0 xj . For any k

∞

≤

j=0 Zj 1{min0≤j≤p−1 Zj >0} <
C2e−δ2|t|. Fix δ <

≥

F (x1,

· · ·

, x(k+1)p) :=

h(xkp+1,
h(x1,

· · ·
· · ·

, x(k+1)p)
, xp)

(k+1)p

p

φ(xℓ −

j=1
X

Yℓ=p+1

ajxℓ−j).

We have the following lemma, whose proof is deferred.

Persistence exponents in Markov chains

21

Lemma 5.2. There exist k
have

≥

1 and C, γ > 0 such that for any vector (x1,

, x(k+1)p)

[0,

∈

· · ·

)(k+1)p we

∞

F (x1,

· · ·

, x(k+1)p)

≤

Ce−γ P(k+1)p

j=1

xj .

Further, the constants C and γ can be chosen uniformly over the parameter space
η > 0.

(5.20)

p

j=1 aj ≤

1

−

η for any

Continuing with the proof of Theorem 2.6, deﬁne the operator Qδ,k on

Cb([0,

P
)p) by

∞
(k+1)p

Qδ,kg(x1, . . . , xp) =

g(xkp+1,··· ,(k+1)p)F (x1,

, x(k+1)p)

dxℓ.

Yℓ=p+1
Lemma 5.2 ensures that Qδ,k1 is a bounded function, and hence Qδ,k is well deﬁned on
gh is bounded, and Qδ,k(g) = 1
1. Thus we have
for i

B
S (gh) by deﬁnition, which on using induction gives Qi

h P kp

Z[0,∞)kp

· · ·

([0,
∞
δ,k(g) = 1

)p). Note that
h P kpi
S (gh)

≥

pn =

P n

S 1dµ

Z

≤

Z

hQ

⌊ n
kp ⌋
δ,k (

1
h

)dµ

Q

⌊ n
kp ⌋
δ,k k

≤ k

µ(h1[0,∞)p),

which, because of the assumption µ(h1{[0,∞)p}) <

, results in

∞

lim sup
n→∞

1
n

log pn ≤

1
kp

log λ(Qδ,k).

For the lower bound on the persistence probability, ﬁxing M > 0 we have

P n

S 1

⌈ n
kp ⌉
δ,k (

Q

1
h

)

≥

≥

e−pδM Q

⌈ n
kp ⌉
δ,k,M (1[0,M]p ),

where Qδ,k,M is the operator from

([0,

B

)p) to itself given by

∞

Qδ,k,M g(x1, . . . , xp) =

1{max1≤ℓ≤p xℓ≤M}

g(xkp+1,··· ,(k+1)p)F (x1,

· · ·

Z[0,M]kp

(k+1)p

, x(k+1)p)

dxℓ.

Yℓ=p+1

(5.21)

(5.22)

For any f

([0,

)p) with

1, and max1≤ℓ≤p xℓ > M we have that

∈ B

∞
Qδ,k,M f (x1,

f

k

k∞ ≤
Qδ,kf (x1,

, xp)

−

· · ·

F (x1,

· · ·

, x(k+1)p)

Z[0,∞)kp

|

=

, xp)

| ≤

· · ·

Qδ,k1(x1,

, xp)

· · ·

dxℓ ≤

Ce−γM γ−kp,

(k+1)p

Yℓ=p+1

where the last inequality is due to (5.20). Similarly, if max1≤ℓ≤p xℓ ≤
Qδ,kf (x1,

Qδ,k,M f (x1,

, xp)

M , then

|

· · ·

−

· · ·

, xp)
|
(k+1)p

≤

Zmaxp+1≤ℓ≤(k+1)p xℓ>M

F (x1,

· · ·

, x(k+1)p)

dxℓ ≤

Ckpγ−kpe−γM .

Combining these two estimates gives limM→∞ k
λ(Qδ,k,M ) = λ(Qδ,k).

lim
M→∞

Qδ,kk

= 0, and consequently Lemma 1.6 gives

(5.23)

Yℓ=p+1
Qδ,k,M −

22

Aurzada, Mukherjee and Zeitouni

Denoting by ˜Qδ,k,M the restriction of the operator Qδ,k,M to
such that

1 we have

f

([0, M ]p), for any function f

B

∈ Cb([0, M ]p)

k∞ ≤
k
˜Qδ,k,M f (x1,

, xp)

|

· · ·
sup
zℓ∈[0,M],p+1≤ℓ≤(k+1)p |

−

≤

˜Qδ,k,M f (y1,
F (x1,

, yp)
|
, xp, zp+1,

· · ·

M −kp

· · ·

, z(k+1)p)

−

· · ·

F (y1,

· · ·

, yp, zp+1,

, z(k+1)p|

,

· · ·

∈ Cb([0, M ]p),
and so the functions
quently, Arzel`a-Ascoli theorem gives that ˜Qδ,k,M is compact as an operator on
of Theorem 1.2 then gives

are uniformly equicontinuous on [0, M ]p. Conse-
Cb([0, M ]p). An application

k∞ ≤

˜Qδ,k,M f : f

{

k

1

}

f

⌈ n
kp ⌉
δ,k,M (1[0,M]p )dµ = λ( ˜Qδ,k,M )

Q

n

kp +o(n),

which along with (5.22) gives

Z

lim inf
n→∞

1
n

log pn ≥

1
kp

log λ( ˜Qk,δ,M ).

(5.24)

≥

1 we have Qi

δ,k,M (1[0,M]p ), which immediately gives λ( ˜Qδ,k,M )
Also note that for any i
= λ(Qδ,k,M ). The lower bound follows from this, on invoking (5.23) and (5.24). Thus we have veriﬁed that
the log persistence exponent exists and equals λ(Qδ,k)1/kp, which a priori can depend on δ > 0. However
the above argument works for any δ < min(δ1, δ2/p), and so the persistence exponent does not depend on
δ1, and in particular does not depend on the initial distribution as long as the latter has ﬁnite exponential
moment.

δ,k,M (1) = Qi

For relating λ(Qδ,k) to the eigenvalue equation, we will invoke Theorem 1.2, for which we need to show
, it suﬃces to show

converges to 0 as M

)p). Since

)p). Also as shown for the derivation of (5.23) we have

→ ∞

Qδ,k,M −

k

Qδ,kk

that Qδ,k is compact on
that Qδ,k,M is compact on

Cb([0,

∞
Cb([0,

∞

sup
max1≤ℓ≤p xℓ>M |

Qδ,k1(x1,

, xp)

| ≤

· · ·

Ce−γM γ−kp,

which converges to 0 as M
Qδ,k is compact on
Cb([0,
that

∞

˜ψ
k∞ = 1, which is non-negative and satisﬁes

k

. This along with the compactness of ˜Qδ,k,M on
→ ∞
)p). If λ(Qδ,k) > 0, Theorem 1.2 implies that there exists ˜ψ

Cb([0, M ]p) implies that
)p) such

∈ Cb([0,

∞

˜ψ(xkp+1,

· · ·

, x(k+1)p)F (x1,

, x(k+1)p)

· · ·

Z[0,∞)kp

(k+1)p

Yℓ=p+1

dxℓ = λkp ˜ψ(x1,

, xp),

· · ·

(5.25)

where λ := λ(Qδ,k)1/kp. Using Lemma 5.2 along with (5.25) gives

Plugging this bound back in (5.25), another application of Lemma 5.2 gives

˜ψ(x1,

, xp)

≤

· · ·

C

λkpγkp e−γ Pp

i=1 xi.

λkp ˜ψ(x1,

, xp) =

· · ·

˜ψ(xkp+1,

Z(0,∞)kp

, x(k+1)p)F (x1,

· · ·

· · ·

e−γ Pp

i=1 xkp+iF (x1,

· · ·

C
λkpγkp

≤

≤

Z(0,∞)kp
λkpγ2kp e−2γ Pp

C2

i=1 xi,

(k+1)p

, x(k+1)p)

dxℓ

Yℓ=p+1
(k+1)p

, x(k+1)p)

dxℓ

Yℓ=p+1

Persistence exponents in Markov chains

23

which gives

C2

˜ψ(x1,

λ2kpγ2kp e−2γ Pp
≤
This, via an inductive argument gives that ˜ψ(x1,
, xp) has super exponential decay, and so the function
h ˜ψ
S with
eigenvalue λkp. It then follows by a telescopic argument similar to Theorem 1.2 that PS has an eigenvalue
λ, and the corresponding eigenfunction ψ

S (h ˜ψ) is well deﬁned, and (5.25) shows that h ˜ψ is an eigenfunction of P kp

)p) is non-negative.

)p). Thus P kp

∈ Cb([0,

i=1 xi.

, xp)

· · ·

· · ·

∞

(b) As before, λ > 0 follows from Proposition 2.9. To show that λ < 1, assume by way of contradiction
k∞ = 1
S ψ. By the proof of part (a), it follows that ψ has super exponential tails,

that λ = 1. Invoking part (a) there exists a non zero non-negative function ψ on [0,
and ψ = PSψ. This implies ψ = P p
and so ψ vanishes at

)p such that

. Letting

∞

ψ

k

∈ Cb([0,

∞

∞

A :=

(x1,

, xp)

[0,

)p : ψ(x1,

∞
we thus have that A is compact, and so max(x1,
then (x1,

· · ·

∈

{

, xp) = 0 is a global maximum, and so plugging in (x1,

· · ·

, xp) =

ψ

k∞ = 1

}

,

k

· · ·

, xp) has a minimum on A. If the minimum equals 0,
, xp) = 0 we have

· · ·

· · ·

1 =

Z(0,∞)

ψ(0,

· · ·

, 0, y)φ(y)dy

P(ξ1 > 0),

≤

which is a contradiction. Thus w.l.o.g. we may assume that the minimum of max(x1,
m > 0, and ﬁx (x1,

A such that max1≤i≤p xi = m. Since

, xp)

· · ·

∈

, xp) over A is

· · ·

1 = ψ(x1,

· · ·

, xp) = E
(cid:16)

ψ(Xp+1,

, X2p)
|

· · ·

X1 = x1,

· · ·

, Xp = xp

,

(cid:17)

we must have

P(Xp+1 > 0,
P(ψ(Xp+1,

· · ·

, X2p > 0
, X2p) = 1

|

X1 = x1,
X1 = x1,

· · ·

, Xp = xp) = 1,
, Xp = xp) = 1.

· · ·

· · ·
Since P(ξ1 < 0) > 0, there exists c < 0 such that for every ε > 0 we have P(ξ1 ∈
along with (5.26) gives

|

P(

2p
ℓ=p+1{

∩

Xℓ > 0,

ξℓ −

|

c

| ≤

ε

}|

X1 = x1,

· · ·

, Xp = xp) > 0.

(5.26)

(5.27)

ε, c + ε]) > 0, which

[c

−

(5.28)

Deﬁne the tuple (xp+1,
· · ·
2p
that on the set
c
ξℓ −
ℓ=p+1|
which on using induction gives

{∩

ε

}

| ≤

we have

, x2p) by inductively setting xℓ := c +
xℓ+1| ≤
max
xℓ| ≤
p+1≤ℓ≤2p |
)p. Indeed, if xℓ < 0 for some p + 1

ε + maxp+1≤j≤ℓ |
P
pε.

Xℓ+1 −
Xℓ −

p
j=1 ajxℓ−j for p + 1
Xℓ −

, x2p)

ℓ

|

∞
xℓ + pε < 0, which is a contradiction to (5.28).

≤

≤

ℓ
≤
, p + 1

2p, and note
2p,
ℓ

≤

≤

≤
xℓ|

We now claim that (xp+1.
[0,
small we have xℓ + pε < 0, and so Xℓ ≤
ε > 0 such that ψ(yp+1,
have

We ﬁnally claim that ψ(xp+1,

· · ·

· · ·

· · ·

∈

P(ψ(Xp+1,

, X2p) < 1

, y2p) < 1 for all (y1,

, x2p) = 1. By way of contradiction, if ψ(xp+1,
, yp) such that maxp+1≤ℓ≤2p |
P(
ξℓ −
, Xp = xp)

2p
ℓ=p+1|

X1 = x1,

{∩

· · ·

≥

· · ·
yℓ −
c

| ≤

, x2p) < 1, there exists
pε. But then we
xℓ| ≤
ε

) > 0,

}

· · ·
· · ·
which is a contradiction to (5.27), verifying (xp+1,

|

A. Finally, using c < 0 gives xp+1 = c +
· · ·
p
j=1 ajxp+1−j < m. Continuing this by induction it is easy to check that max(xp+1, . . . , x2p) < m, which

, x2p)

∈

2p, then by choosing ε

(c) Let

is a contradiction. Thus we have veriﬁed that λ < 1.
P

a(∞)
j
Fr accordingly, where k is as in Lemma 5.2. Then there exists η > 0 such that

r=1 be a sequence in Rp converging to a(∞) such that

p
j=1 |

a(r)

∞

}

{

P

|

< 1, and deﬁne Q(r)
p
j=1 |

a(r)
j

δ,k and
η for all

| ≤

−

1

P

24

Aurzada, Mukherjee and Zeitouni

r large enough. By Lemma 5.2 the constants C, γ depend only on η, and hence we can choose C, γ which
works for all r

1, and so we have the bound

≥

Fr(x1,

sup
r≥1

· · ·

, x(k+1)p)

≤

Ce−γ P(k+1)p

i=1

xi.

(5.29)

Now ﬁx f
gives

([0,

∈ B

)p) such that

∞

f

k

k∞ ≤

1, and M > 0. If we have max1≤ℓ≤p xℓ > M , then invoking (5.29)

Q(r)

δ,kf (x1,

|

· · ·
If max1≤ℓ≤p xℓ ≤
not gives

, xp)

−

Q(∞)

δ,k f (x1,

, xp)

| ≤

· · ·

2Ce−γM γ−kp

(5.30)

M , then splitting the integral depending on whether the integration is over [0, M ]kp or

Q(r)

δ,kf (x1,

|

, xp)

−

· · ·

Q(∞)

δ,k f (x1,

, xp)
|

· · ·

Fr(x1,

, x(k+1)p)

· · ·

Z(0,∞)kp |
2Ckp
γkp e−γM + εr,M M kp,

≤

≤

where

F∞(x1,

, x(k+1)p)
|

· · ·

−

(k+1)p

Yℓ=p+1

dxℓ

(5.31)

εr,M :=

sup
(x1,··· ,x(k+1)p)∈[0,M](k+1)p |

Fr(x1,

· · ·

, x(k+1)p)

F∞(x1,

, x(k+1)p)
|

· · ·

−

converges to 0 as r

, with M ﬁxed. Combining (5.30) and (5.31) gives

→ ∞

Q(r)

δ,k −

k

Q(∞)

δ,k k ≤

2Ckp
γkp e−γM + εr,M M kp.

On letting r
converges to λ(Q(∞)

→ ∞

followed by M

gives that
δ,k ) by Lemma 1.6. This completes the proof of part (c).

δ,k −

→ ∞

k

Q(r)

Q(∞)
δ,k k

converges to 0 as r

, and so λ(Q(r)
δ,k)

→ ∞

Proof of Lemma 5.2. Let δ > 0 be ﬁxed as in the proof of Theorem 2.6. Recall that we have φ(t)
C2e−δ2|t|, where δ2 > pδ. Choose ρ
p
and ε < pδ(1
aj|
j=1 |
deﬁne the variables (A1,

≤
ρ−j = 1. Fix ε > 0 such that pδ(1 + ε) < δ2
). For any positive integer k (the exact choice of which is speciﬁed below in (5.34)),

P
, Akp) inductively by setting

(0, 1) such that

p
j=1 |

aj|

−

∈

P

· · ·

Aℓ −

Aℓ −

ℓ−1

j=1
X
p

j=1
X

aj|

|

aj|

|

Aℓ−j = δ(1 + ε) if 1

ℓ

≤

≤

p,

Aℓ−j = min(ρℓ, ε) if p + 1

ℓ

≤

≤

kp.

Note that Aℓ > 0 for all 1

ℓ

kp by deﬁnition. We now claim that

≤

≤
max
1≤ℓ≤kp

Aℓ <δ2 for all k

1,

≥

max
kp+1≤ℓ≤(k+1)p

p

Xj=ℓ−kp

aj|

Aℓ−j ≤

|

(K + kp + p)ρkp+1, for all k

1,

≥

(5.32)

(5.33)

where K := pδ(1 + ε)ρ−p. Note that (5.33) implies in particular that there exists k such that

Persistence exponents in Markov chains

max
kp+1≤ℓ≤(k+1)p

p

Xj=ℓ−kp

Aℓ−j < δ.

aj|

|

25

(5.34)

Deferring the proof of (5.32) and (5.33) we will ﬁx such a k, and ﬁnish the proof of the lemma. To this end,
C2e−Aℓ|t| for all t
invoking (5.32) and using the bound φ(t)

kp we have

R and 1

ℓ

≤

∈

≤

≤

≤

≤

F (x1,

· · ·

, x(k+1)p)

where

2 eδ Pp
Ckp

j=1(xkp+j −xj)

2 eδ Pp
Ckp

j=1(xkp+j −xj)

(k+1)p

Yℓ=p+1
(k+1)p

Yℓ=p+1

e−A(k+1)p+1−ℓ|xℓ−Pp

j=1 aj xℓ−j |

e

−A(k+1)p+1−ℓ

xℓ−Pp

j=1 |aj |xℓ−j

(cid:16)

(cid:17)

(k+1)p

=Ckp
2

e−α(k+1)p+1−ℓxℓ,

Yℓ=1

ℓ−1

αℓ := 


p

δ

Aℓ −
Aℓ −
δ
−

j=1 Aℓ−j|
j=1 Aℓ−j|
P
p
aj|
P
j=ℓ−kp |
, Akp) we have αℓ > 0 for 1

aj| −
aj|
Aℓ−j

≤

if 1
ℓ
if p + 1
if kp + 1

≤
≤

p,
ℓ

≤
ℓ

≤

≤

kp,

(k + 1)p.

By the choice of (A1,

2 and
γ := min1≤ℓ≤(k+1)p αℓ > 0 gives the desired conclusion. The uniformity of the choice of C is immediate, and
for the uniformity of γ, note that

(k + 1)p, which on setting C := Ckp

· · ·



P

≤

≤

ℓ

and these parameters are uniform over the parameter space

min

δε, ρℓ, δ

(K + kp + p)ρkp+1

,

γ

≥

−

(cid:16)

p

j=1 aj ≤

(cid:17)
1

−

η for any η > 0.

It thus remains to prove (5.32) and (5.33), which we break down into a few steps.
First we show by induction on ℓ that

P

Aℓ ≤

ℓδ(1 + ε)

for 1

ℓ

≤

≤

p.

(5.35)

Indeed, for ℓ = 1 we have A1 = δ(1 + ε) by deﬁnition. If (5.35) holds for 1
formula for Aℓ gives

j

ℓ

−

≤

≤

1, then using the

ℓ−1

Aℓ = δ(1 + ε) +

j=1
X

Aℓ−j|

aj| ≤

δ(1 + ε) + max

1≤j≤ℓ−1

Aℓ−j

δ(1 + ε) + (ℓ

1)δ(1 + ε)

ℓδ(1 + ε),

≤

−

≤

which completes the induction.

Next we show that

Aℓ ≤

δp(1 + ε)

for all 1

ℓ

≤

≤

kp.

Indeeed, note that (5.36) follows from (5.35) for ℓ
induction to note that, by the choice of ε,

≤

p. For larger ℓ, use the deﬁnition of Aℓ along with

(5.36)

Aℓ ≤

p

ε + (

j=1
X

aj|

|

) max
1≤j≤p

Aℓ−j ≤

ε + pδ(1 + ε)

p

j=1
X

aj| ≤

|

pδ(1 + ε).

26

Aurzada, Mukherjee and Zeitouni

Note that (5.35) and (5.36) together yield (5.32), since pδ(1 + ε) < δ2.

We turn to the proof of (5.33) for which we ﬁrst show that

(K + ℓ)ρℓ

for all 1

Aℓ ≤

ℓ

≤

≤

kp,

(5.37)

where K = pδ(1 + ε)ρ−p as before. The choice of K and the observation that Aℓ ≤
ℓ′
for 1

p. For ℓ > p, we proceed by induction. Assume (5.37) holds for all 1

ℓ

ℓδ(1 + ε) yields (5.37)
1, and note that

ℓ

≤

−

≤

≤

≤

ρℓ +

Aℓ ≤

p

j=1
X

aj|

|

Aℓ−j ≤

ρℓ +

p

j=1
X

(K + ℓ

j)
|

aj|

−

ρℓ−j

ρℓ

≤

1 + (K + ℓ

h

1)

−

p

j=1
X

aj|

|

ρ−j

= (K + ℓ)ρℓ.

i

This yields (5.37), which for ℓ

[kp + 1, (k + 1)p] gives

∈
p

Xj=ℓ−kp
from which (5.33) follows trivially.

|

aj|

Aℓ−j ≤

(K + ℓ)ρℓ

p

j=1
X

aj|

|

ρ−j = (K + ℓ)ρℓ,

For the proof of Theorem 2.7, we need the following lemma.

Lemma 5.3. Suppose ξ has a strictly positive log-concave density φ with respect to the Lebesgue measure
on R. Then, for any δ

0 and any bounded non-decreasing function g on R ,

≥

E[g(ξ + δ)
|

ξ + δ > 0] =

E[g(ξ + δ)1{ξ+δ>0}]
P(ξ + δ > 0)

E[g(ξ)1{ξ>0}]
P(ξ > 0)

≥

= E[g(ξ)
|

ξ > 0].

We note that one can construct even unimodal densities for which the conclusion of Lemma 5.3 is false.

Proof. The lemma follows from the inequality in [25, Theorem 3]: one uses

∞

f1(x) := (

0

Z

∞

f2(x) := (

−δ

Z

φ(t)dt)−1φ(x)1[0,∞)(x),

φ(t)dt)−1φ(x

δ)1[0,∞)(x).

−

One can check easily that the log concavity of φ implies the assumptions for f1, f2 required by [25].

We further remark that for ﬁnite subsets of R, Lemma 5.3 follows from Holley’s inequality for ﬁnite

lattices [19].

Proof of Theorem 2.7. For proving strict monotonicity we will invoke Lemma 1.5 with P = P b and
Q = P a and

h(x) :=

Pξ(ξ +
Pξ(ξ +

p

j=1 bjxp+1−j ≥
j=1 ajxp+1−j ≥

p
P

0)

0)

.

For any g

∈ B+,>(S), we have by Lemma 5.3,

P

p
j=1 bjxp+1−j)1{ξ+Pp
p
j=1 ajxp+1−j )1{ξ+Pp
P

j=1 bj xp+1−j ≥0}

j=1 aj xp+1−j≥0}

PS(g)
QS(g)

=

≥

Eξg(x2,
Eξg(x2,
Pξ(ξ +
Pξ(ξ +

, xp, ξ +

· · ·

, xp, ξ +
· · ·
p
j=1 bjxp+1−j ≥
j=1 ajxp+1−j ≥

p
P

P

P

0)

0)

= h(x),

Persistence exponents in Markov chains

27

showing that condition (i) holds. Proceeding to checking condition (ii), for any g

∈ B+,>(S) we have

bjxp+1−j > 0)[

P (g)](x),

[PS(g)](x) = Pξ(ξ +

p

j=1
X

where

y+Pp

[

P (g)](x) :=

R

≥

e
0 we have that x

Since b
P is non-decreasing. To this end, for any g
and ξy = ξ +
e

p
j=1 bjyp+1−j. Then,

P(ξ+

P

7→

e
j=1 bj xp+1−j≥0 g(x2,
y+Pp
j=1 bjxp+1−j ≥

R

p

· · ·

p
j=1 bjxp+1−j)dF (y)

, xp, y +
j=1 bj xp+1−j ≥0 dF (y)
0) is non-decreasing in x, and so it suﬃces to show that
p
j=1 bjxp+1−j

y, write ξx = ξ+

S with x

P

.

∈ B+,>(S) and x, y

∈

≤

P

[

[

P
P (g)](y)
P (g)](x)
e

e

Eξ
Eξ
Eξ
Eξ

=

≥

g(y2,
g(x2,
(cid:0)
g(x2,
(cid:0)
g(x2,

(cid:0)

· · ·

· · ·

· · ·

· · ·

, yp, ξy)1{ξy≥0} |
, xp, ξx)1{ξx≥0} |
, xp, ξy)1{ξy≥0} |
, xp, ξx)1{ξx≥0} |

ξy
ξx

ξy
ξx

0

0
(cid:1)
0
(cid:1)
0

(cid:1)

≥

≥

≥

≥

1,

≥

(cid:1)
where the ﬁrst inequality uses the fact that g is coordinate-wise increasing, and the second inequality uses
Lemma 5.3 and the positivity of b.

(cid:0)

Having veriﬁed that its conditions are satisﬁed, we apply Lemma 1.5 and get [P n−p

S

(1)](x)

≥

[Qn−p

S,h (1)](x),

which, setting Ax := {(Z0, . . . , Zp−1) = x}, is the same as

Pb( min
p≤i≤n

Zi > 0

Ax)

|

≥

Ea

n

h

i=p
Y

1{Zi>0}h(Zi−p,

, Zi−1)
|

· · ·

Ax

.

i

Let
i
expectations with respect to µ and rearranging gives

Z+ =

Zi ≥

0, 0

≤

≤

n

}

{

. Multiplying both sides of the last display by 1{min0≤j≤p−1 Zj >0}, taking

Pb(

Z+)

Pa(

Z+)Ea

≥

n

h

i=p
Y

h(Zi−p,

, Zi−1)

|Z+

· · ·

.

i

By Proposition 2.9 we have θF (a) > 0, and so, given the above inequality, it suﬃces to show that

lim inf
n→∞

1
n

log Ea

n

h

i=p
Y

h(Zi−p,

, Zi−1)

|Z+

· · ·

> 0.

i

For showing (5.38), we claim the existence of k > 1 such that

(cid:16)

(cid:17)
i=p δZi is an empirical measure of total mass n−p+1

n

n

. Indeed, given (5.39) and (5.40) we

lim sup
n→∞

lim sup
n→∞

1
n
1
n

log Pa

Ln[k,

)
∞

≥

log Pa

(cid:16)
Ln[0, 1/k]

≥

1
4p |Z+
1
4p |Z+

< 0,

< 0,

(cid:17)

where LZ
have

n := 1
n

P

n

Ea

h

i=p
Y

h(Zi−p,

, Zi−1)

|Z+

· · ·

i

≥

(1 + η)

n
2p −o(1)Pa

Ln[1/k, k]

(cid:16)

n

−

p + 1
n

1
2p |Z+

−

≥

(cid:17)

(5.38)

(5.39)

(5.40)

28

Aurzada, Mukherjee and Zeitouni

−

1 + inf x∈[1/k,k]ph(x). Also we have η > 0, since ξ has a strictly positive density on the whole
with η :=
of R, which implies that the continuous function h is strictly greater than 1 point wise on the compact set
[1/k, k]p. (5.38) follows after applying log, normalizing by n and taking limits.
It thus remains to prove (5.39) and (5.40). For this, recall that the initial distribution µ satisﬁes, for any
λ′

(0, δ),

∈

1
n

log Eµ

(cid:16)

1 Pp−1

eλ′

j=0 Zj 1{min0≤i≤p−1 Zi>0}

<

.
∞

(cid:17)

Proceeding to showing (5.39), by the log concavity of φ there exist λ0 > 0, λ1 ∈
λ0 −

R, and so with

for all x

λ1|

x
|

∈

we have

L :=

x = (xp,

n

e

, xn)

[0,

∈

· · ·

)n−p+1 :

∞

i

|

∈

[p, n] : xi ≥

k

| ≥

n
4p

o

(5.41)

(0, δ) such that log φ(x)

≤

Pa

Ln[k,

(cid:16)

)
∞

≥

1
4p

,

Z+

Z0,

· · ·

, Zp−1

(cid:17)

(cid:12)
(cid:12)
(cid:12)

≤

≤

≤

≤

Z
eL

i=p
Y

n

enλ0

enλ0

Z
eL

i=p
Y
n

n

p

φ(xi −

ajxi−j )dxi

j=0
X
e−λ1|xi−Pp

j=0 aj xi−j |dxi

e−λ1xi+λ1 Pp

j=0 aj xi−j dxi

i=p
Y

Z
eL
enλ0+λ′

1 Pp−1

j=0 xj

e−eλ1xidxi,

n

Z
eL

i=p
Y

where λ′
1 := λ1(
dµ(x0,
1
p
i

≤

−

}

· · ·
P

p
j=1 aj) < δ, and
, xp−1) gives

λ′
1 > 0. Integrating both sides with respect to 1

xi ≥

{

0, 0

≤

λ1 := λ1 −
e

Ln[k,

1
)
4p
≥
∞
1 Pp−1
enλ0 Eeλ′

,

Z+

(cid:17)

Pa

(cid:16)

≤

j=0 Zj 1{min0≤j≤p−1 Zj >0}

−nP

λ1

n [k,

LY
(cid:16)

)
∞

≥

1
4p

,

(cid:17)

where (Yi, p
Since Eeλ′

i
≤
1 Pp−1
j=0 Zj 1{min0≤j≤p−1 Zj >0} <

≤

n) are i.i.d. exponential random variables with parameter

f

λ1, and LY

n := 1
n

n
i=p δYi.

by (5.41), it suﬃces to show that

∞

e

P

lim sup
k→∞

lim sup
n→∞

1
n

log P

n [k,

LY
(cid:16)

)
∞

≥

1
4p

=

.
−∞

(cid:17)

But this follows on invoking Sanov’s theorem to note that LY
good rate function on M1(0,
Thus (5.39) holds, and a similar proof shows (5.40).

), the set of probability measures on (0,

∞

∞

n satisﬁes a large deviation principle with a
) with respect to the weak topology.

Proof of Proposition 2.8. Fix ε > 0 such that P((Z0, . . . , Zp−1)
of the initial distribution. Deﬁne an associated AR process

(ε,

)p) > 0. This exists by the choice
Z ′
i}i≥1 on the same probability space by setting

∞

∈

0
ε

Z ′

i := 




P

p
j=1 ajZ ′

i−j + ξi

{
i
∈ {
i = p
p.
i

≥

2

}

−

0, . . . , p
1

−

Persistence exponents in Markov chains

29

Since ai ≥

0, on the set

{

min0≤i≤p−1 Zi > ε

}

Z ′

n for all n

p. Thus,

≥

P( min
0≤i≤n

Zi ≥

0)

≥

P( min

0≤i≤p−1

P( min

0≤i≤p−1

≥
= P( min

0≤i≤p−1

we have Zn ≥
Zi > 0)

Zi > ε, min
p≤i≤n
Zi > ε, min
p≤i≤n

Z ′

i > 0)

Zi > ε)

P( min
p≤i≤n

·

Z ′

i > 0)

and so it suﬃces to show that

lim sup
n→∞

1
n

log P( min
p≤i≤n

Z ′

i > 0) = 0.

To this eﬀect, use induction to note that

i

Z ′

i+p−1 =

bi−jξj

j=0
X

where ξ0 = ε, b0 := 1 and bi :=
that

p
j=1 aj > 1 there exists α, β > 0 and a (unique) ρ > 1 such that

p
j=1 bi−jaj and bi = 0 for i < 0. Also, we claim that for any a

P

P
αρi

bi ≤

≤

βρi,

1.

i

≥

(5.42)

(5.43)

0 such

≥

(5.44)

To see this, consider the function f (ρ) :=
satisﬁes f (1) =
the deﬁnition of
to verify (5.42), set

p
j=1 ajρ−j. This function is strictly decreasing on [1,
) and
p
j=1 aj > 1 and f (
) = 0. Therefore, there must be a (unique) ρ > 1 with f (ρ) = 1. Using
bj}j∈Z, it is easy to see that for this ρ one can ﬁnd α, β > 0 satisfying (5.44). Proceeding

{
P

P

∞

∞

n

Sn :=

ξi >

{

min(κρii−2, M )
}

,

−

i=1
\

with some ﬁxed 0 < κ < εα(β
min0≤i≤n+p−1 Z ′
follows.

P

i > 0 for any ﬁxed M . Given this, since P(

P(ξ1 >

∞
k=1 k−2)−1, and M > 0. We will show that on the set

Sn we have
M )n and M is arbitrary, (5.42)

Sn)

≥

−
Sn, for which use (5.43) and (5.44) to note that

It remains to show that persistence happens on

Z ′

i =

i+p−1

j=0
X

αρi+p−1ε +

≥

bi−j+p−1ξj = bi+p−1ξ0 +

bi+p−1−jξj

i+p−1

j=1
X

βρi+p−1−jκρjj−2

ρiρp−1(εα

βκ

−

≥

i+p−1

−

j=1
X

j−2) > 0,

∞

j=1
X

by the choice of κ, and so the proof is complete.

Proof of Proposition 2.9. Since 0 is in the interior of the support of F , there exists α < 0, β > 0 such
(a, b)) > 0. Indeed,
that [α, β] is contained in the support. Then for any (a, b)
otherwise the complement of (a, b) is a closed set of probability 1, which implies (a, b) is not in the support
of F , a contradiction.
Let N := max(1,

(α, β) we have PF (ξ1 ∈

) be a positive integer, and set

∈

p
j=1 |

aj|⌉

⌈

P

L := min(β/N,

α/N ) > 0.

−

30

Aurzada, Mukherjee and Zeitouni

Then we claim that P(Zi ∈
Yn :=
k

i
p
j=1 ajZn−j on this set, we have
note that if Yn ∈

1, . . . , 4N

(0, L), 0

∈ {

≤

}

P

≤
Yn|
|
Ik, then on the set

n) decays slower than some exponential rate. Indeed, setting
2 ) for

LN + kL

< N L. Now setting Ik := (
n

[0, L), 0

LN + (k−1)L
we have
1

−

2

,

P(Yn + ξn ∈

[0, L)
|

Z0,

· · ·

{

Zi ∈
, Zn−1)

−
−
Jk),

i
≤
≤
P(ξn ∈
≥
(α, β) for any k

}

where Jk := (LN
of L, as β

(k−1)L
2
−
LN and α

, LN

−

(k−2)L
2

LN . An inductive argument then gives

). Also note that Jk ⊂

1, . . . , 4N

}

∈ {

by the choice

≥

≤ −
n
i=0{
∩

P(

Zi ∈

[0, L)
}

)

≥

from which the desired conclusion follows.

P(

p−1
i=0 {

∩

Zi ∈

[0, L)
}

)

min
1≤k≤N

(cid:16)

P(ξ1 ∈

n−p

,

Jk)

(cid:17)

6. Proofs for the exponents in the concrete examples

We only give hints on how to solve the concrete eigenvalue equations.

Proof of Proposition 3.1. Recall from Theorem 2.1 that the eigenvalue equation in this case is λg(x) =
E[g(ξ)1{ξ>−x}], x

R.

Let us start with some facts for distributions with bounded support: Assume ξ

Then one can show that g(x) = 0 for x
even on [b,

). If a

b one ﬁnds that g(x) = λ−1g(a)P(ξ >

≤ −

b. Further, g is constant on [a,

Now assume that ξ is uniformly distributed in (

∞

≤

∞
b,
−
a, b). In this case, the eigenvalue equation becomes

x) for x

(
−

−

∈

∈

(
−

a, b) almost surely.
). If a > b then g is constant
a].

∈

−

b

λg(x) =

g(y)dy

−x

Z

We split in cases.

1
a + b

,

x

(
−

∈

a, b).

(6.1)

∈

≤

(a) a
(
−

b: In this case, we already know g on (

a, a)c from the above observations. For the range
a, a), the functions g(x) = κ(cos(αx) + sin(αx)) can be seen to be the only solutions (e.g. by
x
(a+b)λ . The restrictions of the integral equation are
diﬀerentiating (6.1) twice), where necessarily α =
equivalent to (3.1). Since α = 1
(a+b)λ and a, b are known, this is a non-linear equation for λ. It has several
solutions, but we are interested in the smallest possible value for λ−1, which corresponds to the unique
non-negative eigenfunction, as one can check.

−

1

≥

(b) a

b: This case is actually simpler. We already know from the observations on distributions with
bounded support that g is zero left of
b, b).
One can check that the functions g(x) = κ(cos(αx) + sin(αx)) are the only solutions to the integral equation
b, b) with α = 1/(λ(a + b)) (e.g. by diﬀerentiating (6.1) twice). The restrictions from the integral
for x
Z. Since we are interested
equation are equivalent to cos(αb) = sin(αb), which holds for αkb = π/4 + kπ, k
in the largest possible value for λ (which corresponds to the unique non-negative eigenfunction), the solution
in this case is α0b = π/4.

b and constant right of b. Thus, it only remains to consider x

(
−

(
−

−

∈

∈

∈

Proof of Theorem 3.2. The ﬁrst observation is that the persistence probability is in fact distribution-free
(for symmetric densities). This can be seen by representing the i.i.d. (ξi) with the help of i.i.d. (Ui) that
are uniform on [0, 1]: ξi = F −1(Ui), where F is the distribution function of ξ1. The fact that the (ξi) has a
symmetric density is equivalent to

F −1(u) = F −1(1

[0, 1]. This shows

u) for all u

P( min
1≤i≤n

Zi ≥

0) = P(
i
∀
= P(
i
∀
= P(
i
∀
= P(
i
∀
= P(
i
∀

−
ξi−1)

−
1, . . . , n

∈ {

1, . . . , n

1, . . . , n

1, . . . , n
1, . . . , n

∈ {

∈ {

∈ {
∈ {

: ξi ≥ −
: F −1(Ui)
: F −1(Ui)
: Ui ≥
1
: 2(Ui −

}

}

}

}
}

F −1(Ui−1))

≥ −

F −1(1

≥
Ui−1)

Ui−1))

−

−
1/2) + 2(Ui−1 −

1/2)

0).

≥

∈

Persistence exponents in Markov chains

31

This shows that the persistence probability is distribution free and the problem can be reduced to the density
φ = 1

2 1[−1,1].

In this case, the eigenvalue equation reads

λg(x) = [Kg](x) :=

g(z)

1

−x

Z

dz
2

,

x

[

−

∈

1, 1].

Diﬀerentiating twice yields g′′(x) =
(2λ)−2g(x), whose only solutions (up to constant multiples) are of
the form g(x) = sin(x/(2λ)) + cos(x/(2λ)). In order to satisfy not only the diﬀerential equation but also
the original eigenvalue equation, one needs to demand that g(
1) = 0, which gives the set of solutions
−
Z. The positive eigenfunction (or equivalently the largest eigenvalue) is obtained for
λ−1
k = π/2 + 2πk, k
k = 0. This identiﬁes the persistence exponent.

−

∈

To see the explicit formula in the statement one easily sees that (gk) is an orthonormal basis of L2[

1, 1],
1)k√2λk, and that the persistence probability equals E[K n1(ξ0)],

−

1)k√2λkgk, E[gk(ξ0)] = (
1 =
the latter of which can then be readily computed.

k∈Z(

−

−

P

Proof of Proposition 3.4. By Theorem 2.1, the eigenvalue equation reads:

∞

λg(x) =

max(0,−a1x)

Z

g(y)e−ydy,

R.

x

∈

One checks easily that g(x) := ea1x/(1+a1)1x≥0 + 1x≤0 is a non-negative eigenfunction for the eigenvalue
λ = 1 + a1. Note however that one needs to verify that λ = 1 + a1 is the largest eigenvalue of this operator.
λ > 0 denote the largest eigenvalue of K, and use Theorem 1.2 to get the existence
To this eﬀect, let β
∈ Cb(R) we have
of a non-negative, ﬁnitely additive measure m on R such that m(R) = 1 and for every ψ

≥

∞

−∞

Z

m(dx)

Zmax(0,−a1x)

e−yψ(y)dy = βm(ψ).

Thus, setting ψ = g in (6.2) gives

(6.2)

∞

βm(g) =

m(dx)

−∞

Z

Zmax(0,−a1x)

e−yg(y)dy =

∞

−∞

Z

Kg(x)m(dx) = λm(g),

where the last equality uses Kg = λg. Thus to conclude β = λ, it suﬃces to show that m(g) is not zero. If
m(g) = 0, using the fact that g is lower bounded by a positive constant on (
, L] = 0 for
all L

−∞
) = 1. But invoking (6.2) with ψ(x) = 1{x>L} gives

R. Consequently by ﬁnite additivity m(L,

, L] gives m(

−∞

∈

∞

βm(L,

) =

∞

∞

−∞

Z

m(dx)

Zy+a1x>0,y>L

e−ydy

∞

≤

−∞

Z

m(dx)

e−ydy = e−L,

Zy>L

which on taking limits as L

→ ∞

gives β = 0, which is a contradiction.

Proof of Proposition 3.5. Considering the eigenvalue equation in Theorem 2.4 gives the existence of a
continuous non-negative function g : [0,

) satisfying (λ = θF (

1) for short)

[0,

(a + b)λg(x) =

g(y

b

x
Z

−

x)dy =

g(y)dy,

0
Z

(0, b],

x

∈

)
∞
b−x

7→

∞

−

and g(x) = 0 for x > b. It is easy to check that the only solutions to this are given by multiples of g(t) =
cos(αkt) with αkb = π
1)k(αk(a+b))−1
, the largest one of which is λ0.

Z. This gives the corresponding eigenvalues λk = (

2 +πk for some k

−

∈

32

Aurzada, Mukherjee and Zeitouni

Proof of Proposition 3.6. One can check that PS1 = g with g(x) = ea1x. Therefore, the persistence
probability can be computed as

EP n

S 1(Z0)1{Z0>0} = EP n−1

S

g(Z0)1{Z0>0} =

n−1

1

1

(cid:18)

−

a1 (cid:19)

Eg(Z0)1{Z0>0}.

Acknowledgement. We thank the American Institute of Mathematics (AIM) for hosting SQuaREs
which brought the authors together and at which occasions the initial questions for this project were dis-
cussed. We thank Ohad Feldheim for help with the proof of Proposition 2.3, and Amir Dembo, Naomi
Feldheim, and Fuchang Gao for their helpful comments and suggestions. We would also like to thank the
AE and an anonymous referee, whose comments have greatly improved the presentation of this paper.

References

[1] Y.A. Abramovich and C.D. Aliprantis, Problems in Operator Theorey American Mathematical Society, vol. 51 in Graduate

Studies in Mathematics, 2002.

[2] V.I. Afanasyev, On the maximum of a critical branching process in a random environment. Discrete Math. Appl. 9,

267–284, 1999.

[3] F. Aurzada and C. Baumgarten, Survival probabilities of weighted random walks. ALEA Lat. Am. J. Probab. Math. Stat.

8 (2011), 235–258.

[4] F. Aurzada and T. Simon, Persistence Probabilities and Exponents, L´evy matters V:183-221, Lecture Notes in Math 2149,

Springer, 2015.

[5] C. Baumgarten, Persistence of sums of independent random variables, iterated processes and fractional Brownian motion,
PhD dissertation, 2013, available from https://depositonce.tu-berlin.de/bitstream/11303/3917/1/Dokument 39.pdf.

[6] C. Baumgarten, Survival probabilities of autoregressive processes, ESAIM Probab. Stat. 18, 145–170, 2014.
[7] A.J. Bray, S.N. Majumdar, and G. Schehr, Persistence and ﬁrst-passage properties in nonequilibrium systems, Advances

in Physics 62(3):225–361, 2013.

[8] N. Champagnat and D. Villemonais, General criteria for the study of quasi-stationarity, arXiv:1712.08092, 2017.
[9] P. Collet, S. Mart´ınez, and J. San Mart´ın, Quasi-stationary distributions. Markov chains, diﬀusions and dynamical

systems, Probability and its Applications (New York), Springer, Heidelberg, 2013.

[10] K. Deimling, Nonlinear Functional Analysis, Springer, 1985.
[11] A. Dembo, J. Ding and J. Yan, Persistence versus Stability for Auto-Regressive Processes, arXiv:1906.00473, 2019.
[12] A. Dembo and S. Mukherjee, No zero-crossings for random polynomials and the heat equation, Ann. Probab. 43, 85–118,

2015.

[13] A. Dembo and S. Mukherjee, Persistence of Gaussian Processes: Non-summable correlations, Probability Theory and

Related Fields, 169, 1007–1039, 2017.

[14] N. Dunford and J.T. Schwartz, Linear operators. Part I. General theory. John Wiley & Sons, Inc., New York, 1988.
[15] N. Feldheim and O. Feldheim, Long gaps between sign-changes of Gaussian Stationary Processes, Inter. Math. Res.

Notices, 11, 3012–3034, 2015.

[16] N. Feldheim, O. Feldheim and S. Nitzan, Persistence of Gaussian stationary processes: a spectral perspective,

arXiv:1709:00204, 2017.

[17] N. Feldheim, O. Feldheim, F. Nazarov, S. Nitzan and M. Sodin, On the probability that a stationary Gaussian process

with spectral gap remains non-negative on a long interval, arXiv:1807:01132, 2018.

[18] G. Hinrichs, M. Kolb, and V. Wachtel, Persistence of one-dimensional AR(1)-sequences, J. Theoret. Probab. 33, 65-102,

2020.

[19] R. Holley, Remarks on the FKG inequalities, Comm. Math. Phys. 36, 227–231, 1974.
[20] M. Krishnapur and M. Krishna, Persistence probabilities in centered, stationary, Gaussian processes in discrete time.

Indian J. Pure Appl. Math. 47, 183–194, 2016.

[21] P. Maillard. The λ-invariant measures of subcritical Bienaym´e-Galton Watson processes. Bernoulli 24, 297–315, 2018.
[22] S.N. Majumdar and D. Dhar, Persistence in a stationary time series. Physical Review E 64: 046123, 2001.
[23] S.N. Majumdar, A.J. Bray and G.C.M.A. Ehrhardt, Persistence of a continuous stochastic process with discrete-time

sampling, Physical Review E64: 015101(R), 2001.

[24] S. M´el´eard and D. Villemonais, Quasi-stationary distributions and population processes, Probab. Surv. 9, 340–410, 2012.
[25] C.J. Preston, A Generalization of the FKG Inequalities, Comm. Math. Phys. 36, 233–241, 1974.
[26] R.L. Tweedie, R-Theory for Markov Chains on a general state space I: Solidarity properties and R-Recurrent Chains.

Ann. Probab. 2(5):840-864, 1974.

[27] R.L. Tweedie, Quasi-Stationary Distributions for Markov Chains on a general state. Journal of Applied Probability.

11(4):726-741, 1974.

