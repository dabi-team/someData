Bayesian Analysis of AR (1) model 

Hossein Masoumi Karakani, University of Pretoria, South Africa 

Janet van Niekerk, University of Pretoria, South Africa 

Paul van Staden, University of Pretoria, South Africa 

Abstract: 

The first â€“ order autoregressive process, AR (1), has been widely used and implemented in 
time series analysis. Different estimation methods have been employed in order to estimate 
the autoregressive parameter. This article focuses on subjective Bayesian estimation as 
opposed to objective Bayesian estimation and frequentist procedures. The truncated normal 
distribution is considered as a prior, to impose stationarity. The posterior distribution as well 
as the Bayes estimator are derived. A comparative study between the newly derived estimator 
and other existing estimation methods (frequentist) is employed in terms of simulation and 
real data. Furthermore, a posterior sensitivity analysis is performed based on four different 
priors; g prior, natural conjugate prior, Jeffreysâ€™ prior and truncated normal prior and the 
performance is compared in terms of Highest Posterior Density Region criterion. 

Keywords: Autoregressive process, Bayesian estimation, G prior, Highest posterior density 
region, Jeffreysâ€™ prior, Natural conjugate prior, Stationarity, Subjective prior, Truncated 
normal distribution. 

1.  Introduction 

With the exponential growth of time-stamped data from social media, e-commerce and sensor 
systems, time series data that arise in many areas of scientific endeavor is of growing interest 
for extracting useful insights. There exist various models used for time series, however, the 
first-order autoregressive process, denoted as AR (1), is the most well â€“ known and widely 
used model in time series analysis. 

Different type of estimation methods for the AR (1) process have been developed and 
proposed in literature. Frequentist estimation methods, including method of moments 
estimation (MME), conditional least squares estimation (CLS), exact maximum likelihood 
estimation (MLE), and conditional maximum likelihood estimation (CMLE) are commonly 
used. To improve the current estimation procedures, specifically for small samples, a 
Bayesian method of estimation is considered for the AR (1) model. In general, reasons 
involving Bayesian approach in time series analysis are; firstly, this approach can 
successfully provide logical interpretation for statistical inferences in time series analysis, and 
secondly, results can always be updated based on assimilation of new information. 

There is an extensive literature on autoregressive processes using Bayesian methods. 
Bayesian analysis of AR models began with the work of Zellner and Tiao (1964) who 
considered the AR (1) process. Zellner (1971), Box et al. (1976), Monahan (1984) and 
Marriott and Smith (1992), discuss the Bayesian approach to analyze the AR models. Lahiff 
(1980) developed a numerical algorithm to produce posterior and predictive analysis for AR 
(1) process. Diaz and Farah (1981) devoted a Bayesian technique for computing posterior 
analysis of AR process with an arbitrary order. Phillips (1991) discussed the implementation 
of different prior distributions to develop the posterior analysis of AR models with no 

stationarity assumption assumed. Broemeling and Cook (1993) implement a Bayesian 
approach to determine the posterior probability density function for the mean of a pth order 
AR model. Ghosh and Heo (2000) introduced a comparative study to some selected 
noninformative (objective) priors for the AR (1) model. Ibazizen and Fellag (2003), assumed 
a noninformative prior for the autoregressive parameter without considering the stationarity 
assumption for the AR (1) model. However, most literature considers a noninformative 
(objective) prior for the Bayesian analysis of AR (1) model without considering the 
stationarity assumption. See for example, DeJong and Whiteman (1991), Schotman and Van 
Dijk (1991), Sims and Uhlig (1991). 

The intention of this paper is to impose the stationarity assumption by assuming a truncated 
normal distribution as an informative (subjective) prior and derive the closed form 
expressions for the posterior density function as well as Bayes estimator (BE). 

It is worth noting that, the current article can be considered as the first pioneer to use the 
truncated normal distribution as subjective prior for the autoregressive parameter in AR (1) 
model. The article inspects the numerical effectiveness of using the aforementioned prior in 
finding the posterior distribution as well as BE and compare the performance of this newly 
derived estimator with other frequentist methods via comprehensive simulation studies and 
bias plot. Furthermore, the efficiency of this subjective prior (truncated normal) with three 
well â€“ known priors are considered. They are g prior, asserted by Zellner (1983 and 1986), 
natural conjugate (NC) prior, asserted by Raiffa and Schlaifer (1961), and Jeffreysâ€™ prior, 
asserted by Jeffreysâ€™ (1961). In Section 2, the methodology from Bayesian perspective will 
be explained. Furthermore, the newly derived posterior density function as well the Bayes 
estimator will be discussed. Section 3 contains the comparative study in terms of simulation 
algorithm, comparison between newly derived BE and other well â€“ known frequentist 
estimation methods in terms of different sample sizes and bias plot, posterior distribution 
efficiency of the truncated normal distribution with g, natural conjugate and Jeffreysâ€™ priors 
in terms of HDP. Finally, Section 4 conclude the implementation of newly derived results 
into a real dataset. 

2.  Methodology 

In this section, the posterior distribution as well as the BE for the AR (1) autoregressive 
parameter are derived. Derivations of posterior densities depend on the likelihood function 
and the form of the prior distribution which are both defined hereafter. 

A sequence of random variables {Yt: t â‰¥ 1} defined by: 

ğ²ğ­ = ğœ + âˆ…ğ²ğ­âˆ’ğŸ + ğ›†ğ­               âˆ’ğŸ < âˆ… < ğŸ                  (1) 

is a stationary AR (1) process (Box et al., 1970), where yt is the tth time series observation, 
t = 1,2, â€¦ , n, {Îµt} is the white noise process, c is a constant term and âˆ… is the autoregressive 
parameter. 

Throughout this paper we assume that {Îµt} is a sequence of mutually independent identically 
(i.i.d) distributed with a Gaussian (normal) distribution that has a mean of 0 and a common 
variance ÏƒÎµ

2 , which will be considered a known parameter. 

The conditional likelihood function of the complete sample for the AR (1) model is given by 
(Hamilton, 1994): 

ğ‹(âˆ…|ğ²ğ“, â€¦ , ğ²ğŸ, ğ›”ğ›†

ğ“
ğŸ) = âˆ
ğ­=ğŸ

ğŸ

ğŸ
âˆšğŸğ›‘ğ›”ğ›†

ğğ±ğ© [

âˆ’(ğ²ğ­âˆ’âˆ…ğ²ğ­âˆ’ğŸ)ğŸ
ğŸ 
ğŸğ›”ğ›†

       (2) 
]

Since the AR (1) model is only stationary when |âˆ…| < 1, the truncated normal distribution is 
considered as a subjective prior distribution for âˆ…. See Johnson, Kotz and Balakrishnan 
(1970) for distributional details of the truncated normal distribution. The probability density 
function (pdf) for the prior distribution with parameters d and Ïƒâˆ…
parameter, âˆ…, is: 

2 of the autoregressive 

ğ›‘(âˆ…|ğ›”ğ›†

ğŸ) =

ğŸ

ğŸ
âˆšğŸğ›‘ğ›”âˆ…

ğğ±ğ©[

âˆ’(âˆ…âˆ’ğ)ğŸ
ğŸ 
ğŸğ›”âˆ…

ğ›—(

ğ›âˆ’ğ
ğ›”âˆ…

)âˆ’ğ›—(

ğšâˆ’ğ
ğ›”âˆ…

)

]

                    (3) 

where a and b are the lower and the upper truncation points and Ï†(. ) denotes the standard 
normal distribution function (CDF). 

Since âˆ’1 < âˆ… < 1, the truncation points (a and b) can be replaced with â€“ 1 and 1 repectively. 
Hence, equation (3) can be rewritten as: 

ğ›‘(âˆ…|ğ›”ğ›†

ğŸ) =

ğŸ

ğğ±ğ©[

âˆ’(âˆ…âˆ’ğ)ğŸ
ğŸ 
ğŸğ›”âˆ…

]

ğŸ
âˆšğŸğ›‘ğ›”âˆ…
ğŸâˆ’ğ
ğ›”âˆ…

ğ›—(

)âˆ’ğ›—(

âˆ’ğŸâˆ’ğ
ğ›”âˆ…

)

                    (4) 

The expected value and the variance of the truncated normal distribution are (Johnson, Kotz 
and Balakrishnan, 1970): 

ğ„(âˆ…) = ğ + ğ›”âˆ…

âˆ…(ğ›‚)âˆ’âˆ…(ğ›ƒ)
ğ›—(ğ›ƒ)âˆ’ğ›—(ğ›‚)

                       (5) 

and 

ğ•ğ€ğ‘(âˆ…) = ğ›”âˆ…

ğŸ [ğŸ +

ğ›‚âˆ…(ğ›‚)âˆ’ğ›ƒâˆ…(ğ›ƒ)
ğ›—(ğ›ƒ)âˆ’ğ›—(ğ›‚)

âˆ’ (

âˆ…(ğ›‚)âˆ’âˆ…(ğ›ƒ)
)
ğ›—(ğ›ƒ)âˆ’ğ›—(ğ›‚)

ğŸ

 ]               (6) 

respectively, where Î± =

âˆ’1âˆ’d
Ïƒâˆ…

, Î² =

1âˆ’d
Ïƒâˆ…

 and Ï†(. ) is the standard normal CDF. 

Theorem 1:  

The posterior distribution for the autoregressive parameter, âˆ…, of the first-order 
autoregressive model, AR (1), with a truncated normal prior, has a truncated normal 
distribution with the following pdf: 

ğŸ(âˆ…|ğ²ğ“, â€¦ , ğ²ğŸ, ğ›”ğ›†

ğŸ, ğ›”âˆ…

ğŸ, ğ) =

ğ
âˆšğŸğğ±ğ©(âˆ’
ğŸ

ğ
(âˆ…âˆ’
ğŸ

ğŸ
)

 )

      (7) 

ğ
ğ
)))
))âˆ’ğ›—(âˆšğŸ(âˆ’ğŸâˆ’
âˆšğŸğ›‘(ğ›—(âˆšğŸ(ğŸâˆ’
ğŸ
ğŸ

where e =

âˆ‘

T
t=2

ytytâˆ’1
2
ÏƒÎµ

+

d
2 and f =
Ïƒâˆ…

âˆ‘

T
2
ytâˆ’1
t=2
2 +
ÏƒÎµ

1
2.  
Ïƒâˆ…

Proof: 

By replacing the conditional likelihood function (2) and the prior pdf (4) into Bayesâ€™ 
theorem, the posterior pdf is as follows: 

ğŸ(âˆ…|ğ²ğ“, â€¦ , ğ²ğŸ, ğ›”ğ›†

ğŸ, ğ›”âˆ…

ğŸ, ğ) âˆ âˆ
ğ“
ğ­=ğŸ

ğŸ

ğŸ
âˆšğŸğ›‘ğ›”ğ›†

ğğ±ğ© [

âˆ’(ğ²ğ­âˆ’âˆ…ğ²ğ­âˆ’ğŸ)ğŸ
ğŸ 
ğŸğ›”ğ›†

] Ã—

ğŸ

ğğ±ğ©[

âˆ’(âˆ…âˆ’ğ)ğŸ
ğŸ 
ğŸğ›”âˆ…

]

ğŸ
âˆšğŸğ›‘ğ›”âˆ…
ğŸâˆ’ğ
ğ›”âˆ…

ğ›—(

)âˆ’ğ›—(

âˆ’ğŸâˆ’ğ
ğ›”âˆ…

)

      (8) 

Equation (8), can be rewritten as: 

ğŸ

ğŸ(âˆ…|ğ²ğ“, â€¦ , ğ²ğŸ, ğ›”ğ›†

ğŸ, ğ›”âˆ…

ğŸ, ğ) âˆ (

ğŸ

ğŸ
âˆšğŸğ›‘ğ›”ğ›†

ğ“âˆ’ğŸ
ğŸ  ğğ±ğ© (âˆ‘ [

ğ“
ğ­=ğŸ

)

âˆ’(ğ²ğ­âˆ’âˆ…ğ²ğ­âˆ’ğŸ)ğŸ
ğŸ 
ğŸğ›”ğ›†

]

) Ã—

Hence: 

ğğ±ğ©[

âˆ’(âˆ…âˆ’ğ)ğŸ
ğŸ 
ğŸğ›”âˆ…

]

)âˆ’ğ›—(

âˆ’ğŸâˆ’ğ
)
ğ›”âˆ…

ğŸ
âˆšğŸğ›‘ğ›”âˆ…
ğŸâˆ’ğ
ğ›”âˆ…

ğ›—(

     (9) 

ğŸ(âˆ…|ğ²ğ“, â€¦ , ğ²ğŸ, ğ›”ğ›†

ğŸ, ğ›”âˆ…

ğŸ, ğ) âˆ ğğ±ğ© (âˆ’

ğ

ğŸ

(âˆ… âˆ’

ğŸ

ğ

ğŸ

)

 )       (10) 

where e =

âˆ‘

T
t=2

ytytâˆ’1
2
ÏƒÎµ

+

d
2 and f =
Ïƒâˆ…

âˆ‘

2
T
ytâˆ’1
t=2
2 +
ÏƒÎµ

1
2. 
Ïƒâˆ…

Equation (10) is identified as the kernel of a truncated normal pdf (see equation (4)) and 
hence the posterior pdf is: 

ğŸ(âˆ…|ğ²ğ“, â€¦ , ğ²ğŸ, ğ›”ğ›†

ğŸ, ğ›”âˆ…

ğŸ, ğ) =

ğ
âˆšğŸğğ±ğ©(âˆ’
ğŸ

ğ
(âˆ…âˆ’
ğŸ

ğŸ
)

 )

ğ
ğ
)))
))âˆ’ğ›—(âˆšğŸ(âˆ’ğŸâˆ’
âˆšğŸğ›‘(ğ›—(âˆšğŸ(ğŸâˆ’
ğŸ
ğŸ

âˆ 

Under the squared error loss function, the Bayes estimator of the autoregressive parameter, âˆ…, 
is given by the posterior mean (5) and its accuracy can be described by the posterior variance 
(6). 

Hence, the Bayes estimator of the autoregressive parameter, âˆ…, will be presented in the 
following theorem. 

Theorem 2: 

Under the squared error loss function, the Bayes estimator for the autoregressive parameter, 
âˆ…, of the first-order autoregressive model, AR (1), under the truncated normal distribution as 
a prior and the squared error loss function with reference to Theorem 1 and equation (5) is: 

âˆ…Ì‚ = ğ›ğŸ + ğ›”ğŸ

âˆ…(

ğ›—(

âˆ’ğŸâˆ’ğ›ğŸ
ğ›”ğŸ
ğŸâˆ’ğ›ğŸ
ğ›”ğŸ

)âˆ’âˆ…(

)âˆ’ğ›—(

)

ğŸâˆ’ğ›ğŸ
ğ›”ğŸ
âˆ’ğŸâˆ’ğ›ğŸ
ğ›”ğŸ

         (12) 
)

e

where Î¼1 =
f
normal CDF. 

, Ïƒ1 = âˆšf âˆ’1, e =

âˆ‘

T
t=2

ytytâˆ’1
2
ÏƒÎµ

+

d
2 , f =
Ïƒâˆ…

âˆ‘

2
T
ytâˆ’1
t=2
2 +
ÏƒÎµ

1
2 and Ï†(. ) is the standard 
Ïƒâˆ…

3.  Comparative Studies 

 
  
 
This section is devoted to investigate and compare the performance of the newly derived 
estimator (BE) with other well â€“ known frequentist methods of estimation. This comparison 
will be implemented in terms of comprehensive simulation studies and bias plot. 
Furthermore, the studies compare the performance of the three different priors (g, natural-
conjugate and Jeffreysâ€™) with truncated normal prior using different AR (1) models and 
different time series lengths. The sensitivity of the posterior distribution to the change in the 
prior used is studied. The comparative study is implemented via Highest Posterior Density 
Region (HPDR). All computations are performed using MATLAB 2015a. 

3.1  Simulation Algorithm 

The current simulation study deals with data generated from the AR (1) model. For 
estimating the hyperparameters, training sample method (Shaarawy et al., 2010) is 
considered. For each time series, a training sample of size either 10 or 10% of the considered 
time series observations is used in this consequence. Thereafter, the estimated 
hyperparameters are used to conduct the simulation studies. In all simulation studies the 
white noise variance, ÏƒÎµ

2, is set to be 1. 

The simulation algorithm for obtaining the frequentist and Bayesian estimate can be 
described as follows: 

ï‚·  Arbitrary values for the autoregressive parameter, âˆ…, are chosen as â€“ 0.9, - 0.5, 0, 0.5 

and 0.9; 

ï‚·  Time series lengths are T = 30 and T = 100; 
ï‚·  The white noise variance ÏƒÎµ
ï‚·  The hyperparameters for the truncated normal distribution are chosen by 

2, is set to be 1; and 

implementing training sample method. 

Comparison for the sensitivity and efficiency of different priors for the autoregressive parameter, 
starts by generating a set of 500 normal variates Îµt samples for each model of length 700. For each 
sample, the first 200 observations are ignored to overcome the effect of the initial values. Five 
different time series lengths have been chosen to study the influence of the series length on the 
performance of different prior distributions. These lengths are 30, 50, 100, 200 and 500. Six cases of 
AR (1) models are considered, for which, the values of the autoregressive parameter, âˆ…, were 
Â±0.2, Â±0.5 and Â±0.8, respectively. These values were chosen inside the stationarity domain 
to fulfil this crucial assumption for the autoregressive parameter of the AR (1) model. The 
comparative study depends on some criteria (HPDR) as will be discussed in detail later. 

3.2  Frequentist vs Bayesian estimate 

In this section, the newly derived BE is illustrated and evaluated against the MME, CLS, 
MLE and CMLE. The evaluation and the comparison will be performed in terms of 
numerical estimation as well as bias plot provided. 

3.2.1  Numerical estimation 

The frequentist and Bayes estimates for the autoregressive parameter, âˆ…, are shown in Tables 
1 and 2 based on different time series lengths (T): 

âˆ… 
âˆ’ğŸ. ğŸ— 
âˆ’ğŸ. ğŸ“ 

MME 
âˆ’ğŸ. ğŸ—ğŸğŸ”ğŸ– 
âˆ’ğŸ. ğŸ“ğŸğŸğŸ— 

CLS 
âˆ’ğŸ. ğŸ—ğŸ’ğŸ‘ğŸ” 
âˆ’ğŸ. ğŸ”ğŸğŸ—ğŸ– 

MLE 
âˆ’ğŸ. ğŸ—ğŸğŸ•ğŸ– 
âˆ’ğŸ. ğŸ“ğŸ—ğŸ•ğŸ“ 

CMLE 
âˆ’ğŸ. ğŸ—ğŸ’ğŸ‘ğŸ• 
âˆ’ğŸ. ğŸ”ğŸğŸğŸ” 

BE 
âˆ’ğŸ. ğŸ—ğŸğŸ—ğŸ” 
âˆ’ğŸ. ğŸ“ğŸğŸğŸ 

 
ğŸ 
ğŸ. ğŸ“ 
ğŸ. ğŸ— 

ğŸ. ğŸğŸğŸ—ğŸ 
ğŸ. ğŸ“ğŸğŸğŸ— 
ğŸ. ğŸ–ğŸ–ğŸ“ğŸ 

ğŸ. ğŸğŸğŸ—ğŸ 
ğŸ. ğŸ“ğŸğŸ•ğŸ— 
ğŸ. ğŸ—ğŸğŸğŸ 

ğŸ. ğŸğŸğŸ–ğŸ” 
ğŸ. ğŸ“ğŸğŸğŸ‘ 
ğŸ. ğŸ–ğŸ•ğŸğŸ 

ğŸ. ğŸğŸğŸ—ğŸ 
ğŸ. ğŸ“ğŸğŸ”ğŸ• 
ğŸ. ğŸ–ğŸ–ğŸ”ğŸ 

ğŸ. ğŸğŸğŸ—ğŸ— 
ğŸ. ğŸ“ğŸğŸ’ğŸ 
ğŸ. ğŸ—ğŸğŸğŸ 

Table 1: Estimates based on different values for âˆ…, with time series lengths, T = 30, 
white noise variance, ğ›”ğ›†

ğŸ = ğŸ. 

âˆ… 
âˆ’ğŸ. ğŸ— 
âˆ’ğŸ. ğŸ“ 
ğŸ 
ğŸ. ğŸ“ 
ğŸ. ğŸ— 

MME 
âˆ’ğŸ. ğŸ–ğŸ—ğŸ•ğŸ– 
âˆ’ğŸ. ğŸ“ğŸğŸğŸ 
âˆ’ğŸ. ğŸğŸğŸ•ğŸ’ 
ğŸ. ğŸ“ğŸğŸ”ğŸ’ 
ğŸ. ğŸ–ğŸ—ğŸğŸ“ 

BE 
âˆ’ğŸ. ğŸ–ğŸ—ğŸ—ğŸ 
âˆ’ğŸ. ğŸ“ğŸğŸğŸ• 
âˆ’ğŸ. ğŸğŸğŸ•ğŸ’ 
ğŸ. ğŸ“ğŸğŸ•ğŸ” 
ğŸ. ğŸ–ğŸ—ğŸ“ğŸ‘ 
Table 2: Estimates based on different values for âˆ…, with time series lengths, T = 100, 
white noise variance, ğ›”ğ›†

MLE 
âˆ’ğŸ. ğŸ–ğŸ—ğŸ–ğŸ 
âˆ’ğŸ. ğŸ’ğŸ—ğŸ—ğŸ• 
âˆ’ğŸ. ğŸğŸğŸ•ğŸ’ 
ğŸ. ğŸ“ğŸğŸ”ğŸ’ 
ğŸ. ğŸ–ğŸ—ğŸğŸ• 

CLS 
âˆ’ğŸ. ğŸ–ğŸ—ğŸ–ğŸ— 
âˆ’ğŸ. ğŸ“ğŸğŸğŸ 
âˆ’ğŸ. ğŸğŸğŸ•ğŸ’ 
ğŸ. ğŸ“ğŸğŸ”ğŸ– 
ğŸ. ğŸ–ğŸ—ğŸ‘ğŸ‘ 

CMLE 
âˆ’ğŸ. ğŸ–ğŸ—ğŸ–ğŸ— 
âˆ’ğŸ. ğŸ“ğŸğŸğŸ 
ğŸ. ğŸğŸğŸ•ğŸ’ 
ğŸ. ğŸ“ğŸğŸ”ğŸ– 
ğŸ. ğŸ–ğŸ—ğŸğŸ“ 

ğŸ = ğŸ. 

In terms of bias, according to Tables 1 and 2, the BE performs the best for small samples for 
most of the cases and is comparative for large samples.  

3.2.2  Bias Plot 

For further investigation, the simulation study is repeated 10 times. For each estimate in each 
replication the absolute bias is calculated and displayed graphically in Figure 1 for âˆ… = 0.5. 

Figure 1: Bias plot for the autoregressive parameter, âˆ…, estimates. 

From Figure 1, one can make a comparison between estimates based on the values of the 
bias. The BE (blue line) is competitive compared to other frequentist estimates. Frequentist 
estimates (MME, CLS, MLE and CMLE) bias levels are approximately the same. 

3.3  Posterior Distribution Sensitivity 

Various frequentist criteria are helpful to compare among prior distributions. The basic idea 
is to use the prior distribution to generate a posterior distribution, and investigate the 
frequentist properties of such resulted distribution (Yang, 1994). 

 
 
An interesting tool will be used to determine the reasonable prior distribution. It is a 
percentage measure for the number of samples that satisfy some condition. The criterion can 
be explained as follows: 

95% Highest Posterior Density Region (HPDR) that is defined as the region under the 
posterior density over the interval centered at the posterior mean with probability 95%. For 
each simulation, nâˆ— is defined to be the number of samples where the 95% HPDR contains 
the true value of the autoregressive parameter. Then, the percentage P is evaluated such that: 

P =

nâˆ—
500

Ã— 100                    (13) 

The performance of a prior is evaluated according to the value of P. In other words, for a 
given prior, the greater percentage indicates a higher performance of the prior to guide to a 
posterior that presents powerfully the autoregressive parameter. 

For the posterior distribution sensitivity of AR (1) models, the stationarity assumption for the 
autoregressive parameter will be considered. The posterior outputs of all the proposed six AR 
(1) models will be studied using truncated normal (TN) prior, g prior, natural conjugate (NC) 
prior and Jeffreysâ€™ prior. The algorithm of the comparative analysis was implemented 
according to the following outlines. For each of the 500 samples; the first 30 observations 
used to evaluate the posterior distribution of the autoregressive parameter via four candidate 
priors. The posterior mean and the posterior variance of the autoregressive parameter were 
computed given each prior. Tracing the HPDR method, an interval centered at the posterior 
mean with probability 0.95 is evaluated. For each model, the percentage of samples for which 
the actual autoregressive parameter exists within the indicated interval was computed. 
Similarly, the process is repeated for the first 100, 200 and 500 observations. 

The results for each of the six models are summarized throughout six tables. These tables 
represent the percentage (P) defined by (13) for each nâˆ—. The first column represents the time 
series length, while each column matches the used prior distribution. The values in the cells 
of the table denote the percentages P. 

n 
30 
50 
100 
200 
500 

Jeff. Prior 
93.4 
94.5 
94 
95.6 
96 

g Prior 
94.4 
94.5 
94.2 
95.4 
94.8 

NC Prior 
95 
95.2 
94.5 
95.4 
94.6 

TN Prior 
97.2 
96.4 
94.6 
95.8 
94.4 

Table 3: Highest Posterior Density Region values based on different priors when the 
autoregressive parameter is â€“ 0.2. 

n 
30 
50 
100 
200 
500 

Jeff. Prior 
94.4 
95 
94 
96 
95.4 

g Prior 
94 
94.6 
94.6 
95.8 
95.4 

NC Prior 
95.6 
95.2 
94.8 
95.8 
95.2 

TN Prior 
97.6 
97.2 
96.2 
95.8 
95.6 

Table 4: Highest Posterior Density Region values based on different priors when the 
autoregressive parameter is 0.2. 

 
n 
30 
50 
100 
200 
500 

Jeff. Prior 
95.4 
96.4 
95.4 
95.8 
96 

g Prior 
94.4 
96.2 
94.2 
96.8 
95.4 

NC Prior 
96 
96.6 
94.4 
96.4 
95.2 

TN Prior 
97.4 
97 
95.8 
96.4 
95 

Table 5: Highest Posterior Density Region values based on different priors when the 
autoregressive parameter is â€“ 0.5. 

n 
30 
50 
100 
200 
500 

Jeff. Prior 
94.4 
94.6 
94.2 
95.4 
93.8 

g Prior 
94.8 
94.4 
94.6 
96.2 
94.6 

NC Prior 
95.8 
95.2 
94.6 
96 
94 

TN Prior 
97.6 
96.4 
94.4 
96 
94.2 

Table 6: Highest Posterior Density Region values based on different priors when the 
autoregressive parameter is 0.5. 

n 
30 
50 
100 
200 
500 

Jeff. Prior 
95.4 
94.4 
94.2 
95.2 
96.4 

g Prior 
95.6 
95.8 
94.4 
96.2 
98.2 

NC Prior 
97.6 
96.4 
94.2 
95.6 
96.6 

TN Prior 
98.4 
97.2 
94.2 
95.4 
96.4 

Table 7: Highest Posterior Density Region values based on different priors when the 
autoregressive parameter is â€“ 0.8. 

n 
30 
50 
100 
200 
500 

Jeff. Prior 
95.4 
94 
94 
92.8 
93.6 

g Prior 
94.6 
93.8 
95.4 
94.4 
95.2 

NC Prior 
98.2 
95.4 
95 
93.6 
95.4 

TN Prior 
98.8 
96.6 
95.2 
94.6 
95 

Table 8: Highest Posterior Density Region values based on different priors when the 
autoregressive parameter is 0.8. 

Regarding the above tables, we achieve the following conclusion: 

1.  All priors lead to consistent posterior, in the sense that the HPDR includes the 
autoregressive parameter value in more than 90% of the cases at all time series 
lengths. However, for the small sample sizes (30 and 50) the subjective prior 
distribution (TN) considered in this article provides better coverage of the 
autoregressive parameter comparing to other 3 priors and supports the findings. Since, 
TN assumes the stationarity assumptions for the autoregressive parameter, therefore it 
is of extreme importance to consider this assumption in the prior distribution and 
Bayesian analysis of AR (1) model. 

 
 
2.  For the small sample sizes (30 and 50) truncated normal prior is highly better than 

other priors. In terms of moderate to large sample sizes (100, 200 and 300), truncated 
normal prior is competitive with other priors and all of the priors are highly better 
than Jeffreysâ€™ prior which appears to be less consistent at all time series lengths. 
3.  The goodness of each prior is insensitive to the increase of the time series length. 

Hence, the above results support the use of truncated normal prior, since it assumes the 
stationarity assumption of the AR (1) model and it avoids the problem of estimating the 
hyperparameters as well. Furthermore, the truncated normal prior appears to be a good choice 
for small time series length (30 and 50). However, for longer time series lengths, truncated 
normal prior is competitive with other priors. 

4.  Blowfly data 

To illustrate the achieve results of the simulation study in Section 3, the blowfly dataset from 
(Wei, 1990) is considered. It consists of the number of adults blowflies with balanced sex 
ratios kept inside a cage and given a fixed amount of food daily. The sample size is 82. A 
graphical representation using SAS is enclosed to describe this dataset through a time plot. 
Phillips â€“ Perron test is conducted to test for stationarity assumption. Moreover, the 
estimation of the autoregressive parameter will be considered in terms of Bayesian and 
frequentist estimation methods. Posterior sensitivity is also implemented by using different 
priors and calculate the 95% HPDR. 

4.1  Time series analysis 

Time series analysis consists of different steps and all of these steps need to be considered in 
order to find a reliable time series model. In Table 9, step by step approach on time series 
analysis is shown and will be considered thereafter: 

Steps 
1 
2 
3 
4 
5 

Explanation 
Time series visualization 
Testing for stationarity 
Model identification 
Estimation 
Model diagnostics and residual analysis 

Table 9: Time series analysis steps. 

Note that all of the steps and calculations is performed using SAS 9.4. 

The graphical representation of blowfly dataset which is known as time series plot, is 
illustrated in the following figure: 

Figure 2: Time series analysis steps. 

From Figure 2, the blowfly data has no cycle, trend or seasonal pattern and only irregular 
variation can be identified from the time series plot. Therefore, one can make a conclusion 
that this time series is stationary. However, more analysis is required to test for stationarity of 
this dataset. 

The Phillips - Perron test (Phillips and Perron, 1988) is utilized to check for the stationarity 
assumption. The results for this test are shown in the following table: 

Lags 
0 
1 
2 
3 

Rho 
âˆ’ğŸğŸ. ğŸ’ğŸ•ğŸ•ğŸ 
âˆ’ğŸğŸ‘. ğŸğŸ•ğŸ‘ğŸ• 
âˆ’ğŸğŸ’. ğŸğŸ“ğŸ‘ğŸ’ 
âˆ’ğŸğŸ‘. ğŸ”ğŸ”ğŸ•ğŸ” 

Pr < Rho 
ğŸ. ğŸğŸğŸ“ğŸ 
ğŸ. ğŸğŸğŸ‘ğŸ 
ğŸ. ğŸğŸğŸğŸ’ 
ğŸ. ğŸğŸğŸğŸ– 

Tau 
âˆ’ğŸ‘. ğŸ”ğŸ’ 
âˆ’ğŸ‘. ğŸ•ğŸ“ 
âˆ’ğŸ‘. ğŸ–ğŸ 
âˆ’ğŸ‘. ğŸ•ğŸ– 

Pr < Tau 
ğŸ. ğŸğŸğŸ”ğŸ– 
ğŸ. ğŸğŸğŸ’ğŸ— 
ğŸ. ğŸğŸğŸ’ğŸ 
ğŸ. ğŸğŸğŸ’ğŸ“ 

Table 10: Phillips â€“ Perron Unit Root Tests. 

The null hypothesis of the unit root test is rejected, since all of the p â€“ values for different 
lags are less than 0.01. Therefore, the null hypothesis can be rejected at 1% level of 
significance. As a conclusion it can be concluded that the blowfly data is stationary. 

The next step is to identify the model as well as the order of the model. The minimum 
information criterion (MINIC) method (Hannan and Rissanen, 1982) results are shown in 
Table 11. 

Lags 
AR 0 
AR 1 
AR 2 

MA 1 
ğŸğŸ‘. ğŸ—ğŸğŸ—ğŸ“ 
ğŸğŸ‘. ğŸ’ğŸğŸğŸ” 
ğŸğŸ‘. ğŸ’ğŸ•ğŸğŸ• 
Table 11: Minimum information Criterion (MINIC) results. 

MA 0 
ğŸğŸ’. ğŸ‘ğŸ‘ğŸğŸ 
ğŸğŸ‘. ğŸ‘ğŸ—ğŸ–ğŸ‘ 
ğŸğŸ‘. ğŸ’ğŸğŸ—ğŸ• 

MA 2 
ğŸğŸ‘. ğŸ”ğŸğŸ–ğŸ 
ğŸğŸ‘. ğŸ’ğŸ’ğŸ—ğŸ” 
ğŸğŸ‘. ğŸ“ğŸğŸğŸ“ 

From Table 11, since the minimum value is 13.3983, the chosen model is AR (1). 

The next step is to estimate the autoregressive parameter for the blowfly data. The values for 
the hyperparameters are chosen as 0.75 and 1. Furthermore, the value for the common 
variance, ÏƒÎµ
on the frequentist and Bayesian estimation methods are given in Table 12. 

2, is chosen as 1. The estimated values for the autoregressive parameter, âˆ… based 

Parameter 
âˆ… 

MME 
ğŸ. ğŸ•ğŸ‘ğŸ’ğŸ– 

CLS 
ğŸ. ğŸ•ğŸ“ğŸ•ğŸ 

MLE 
ğŸ. ğŸ•ğŸ“ğŸ”ğŸ’ 

CMLE 
ğŸ. ğŸ•ğŸ‘ğŸ’ğŸ– 

BE 
ğŸ. ğŸ•ğŸ“ğŸ”ğŸ 

 
Table 12: Blowfly data estimates for âˆ…. 

The final step for time series analysis is to determine whether the fitted model is an adequate 
representation of the process that generated the observed time series. Residual analysis is 
useful in order to identify the validity of a model. 

Figure 3: Residual Normality Diagnostics for blowfly data. 

The histogram for the residuals in Figure 3 is approximately symmetric around zero. The 
normal density curve (kernel) added to the histogram suggest that the residuals may be 
normal. This is verified with the quantile â€“ quantile (Q â€“ Q) plot on the right hand side of 
Figure 3. 

Various tests for normality are given in Table 13. P â€“ values for all of these test is greater 
than 0.05. Therefore, the null hypothesis of normality cannot be rejected at a 5% level of 
significance. Hence, the residuals are normally distributed. 

Test 
Kolmogorov â€“ Smirnov 
Cramer â€“ von Mises 
Anderson â€“ Darling 

Statistic 
0.0610 
0.0676 
0.5814 

P â€“ value 
> 0.150 
> 0.250 
0.131 

Table 13: Goodness of fit Tests for Normal Distribution for blowfly data. 

4.2  Posterior analysis 

The next step after considering time series analysis, is to demonstrate the posterior analysis 
that was accomplished in the previous section and compared over the four candidate priors; g 
prior, NC prior, Jeffreysâ€™ prior and truncated normal prior. For the blowfly dataset, the 
posterior mean and the 95% HPDR centered at the posterior mean are evaluated with respect 
to the four proposed prior distributions. The results of posterior analysis are summarized 
through the following table (Table 14): 

Prior 
Jeffreysâ€™ prior 
g prior 
NC prior 
TN prior 

Posterior Mean 
ğŸ. ğŸ•ğŸ’ğŸ—ğŸ 
ğŸ. ğŸ•ğŸ“ğŸğŸ 
ğŸ. ğŸ•ğŸ“ğŸ’ğŸ 
ğŸ. ğŸ•ğŸ“ğŸ”ğŸ 

95% HPDR 
[ğŸ. ğŸ•ğŸ‘ğŸ“ğŸ, ğŸ. ğŸ•ğŸ–ğŸ“ğŸ] 
[ğŸ. ğŸ•ğŸ’ğŸ‘ğŸ, ğŸ. ğŸ•ğŸ—ğŸ‘ğŸ] 
[ğŸ. ğŸ•ğŸ’ğŸ—ğŸ, ğŸ. ğŸ•ğŸ–ğŸ—ğŸ] 
[ğŸ. ğŸ•ğŸ’ğŸ—ğŸ, ğŸ. ğŸ•ğŸ“ğŸ—ğŸ]  

Table 14: Posterior Mean of âˆ… and the 95% HPDR centered at the posterior mean by 
Prior Distribution for the blowfly data (ğ§ = ğŸ–ğŸ). 

 
Examining the above results shows similar conclusions for the posterior analysis. The 
performance of the four priors based on the posterior mean values are almost the same. The 
unique difference is shown through the 95% HPDR that supposed to give a probability 0.95 
with shortest interval. Therefore, the length of the computed interval is taken as a powerful 
tool to compare the performance of the priors. 

Table 14 shows that the TN prior gave the shortest interval with length 0.02 comparing to 
(Broemeling & Cook, 1993) other priors. 

5.  Conclusion 

In this paper, a subjective Bayesian analysis of the AR (1) model was done. Furthermore, the 
posterior sensitivity analysis was performed based on four different priors; Jeffreysâ€™ prior, g 
prior, natural conjugate prior and truncated normal prior. The main contributions can be 
summarized as follows: 

ï‚·  Closed form expressions for the posterior pdf and Bayes estimator (BE) are derived 

under the truncated normal prior. 

ï‚·  The newly derived BE was compared to the well â€“ known frequentist estimation 

ï‚· 

methods. It showed to be a competitive choice for all sample sizes and superior for 
small samples. 
In terms of posterior sensitivity based on four different priors, truncated normal 
distribution that was considered in this paper to assume the stationarity assumption, 
outperforms other priors in terms of Highest Posterior Density Region (HPDR) 
criterion. 

ï‚·  The study considered real time series example (blowfly data) to illustrate the time 

series analysis steps and the process of prior selection in the posterior analysis in real 
life. The blowfly dataset follows the AR (1) process. Posterior analysis of the blowfly 
data showed similar results comparing to the simulation study. 

References 
Box, G., Jenkins, G. & Reinsel, G., 1976. Time Series Analysis: Forecasting and Control.. New Jersey, 
United States of America: John Wiley and Sons. 

Broemeling, L. & Cook, P., 1993. Bayesian estimation of the means of the AR process. Journal of 
Applied Statistics, Volume 20, pp. 25-39. 

DeJong, D. & Whiteman, C., 1991. REconsidering Trends and Random Walks in Macroeconomic Time 
Series. Journal of Monetary Economics, Volume 28, pp. 221-254. 

Diaz, J. & Farah, J., 1981. Bayesian identification of Autoregressive Processes. s.l., 22nd NBER-NSF 
Seminar on Bayesian Inference in Econometrics. 

Ghosh, M. & Heo, J., 2000. Default Bayesian Priors for Regression Models with First-Order 
Autoregressive Residuals. Journal of Time Series Analysis, 24(3), pp. 269-282. 

Hamilton, J., 1994. Time Series Analysis. Princeton, New Jersey: Princeton University Press. 

 
Hannan, E. & Rissanen, J., 1982. Recursive estimation of mixed autoregressive moving average order. 
Biometrika, pp. 81-94. 

Ibazizen, M. & Fellag, H., 2003. Bayesian estimation of an AR(1) process with exponential white 
noise. Journal of Theoretical and Applied Statistics, 37(5), pp. 365-372. 

Jeffreys, H., 1961. Theory of Probability. 3 ed. London: Oxford University Press. 

Johnson, N., Kotz, S. & Balakrishnan, N., 1970. Continuous Univariate Distributions. New Jersey, 
United States of America: John Wiley and Sons. 

Lahiff, M., 1980. Time series forecasting with informative prior distribution, University of Chicago: 
Technical Report, No. 111, Department of Statistics. 

Marriot, J. & Smith, A., 1992. Reparametrization aspects of numerical Bayesian methods for 
autoregressive moving average models. Journal of Time Series Analysis, Volume 13, pp. 327-343. 

Monahan, J., 1984. Fully Bayesian analysis of ARIMA time series models. Journal of Econometrics, 
Volume 21, pp. 307-331. 

Phillips, P., 1991. To Criticize the Critics: An Objective Bayesian Analysis of Stochastic Trends. Journal 
of Applied Econometrics, Volume 6, pp. 333-364. 

Phillips, P. & Perron, P., 1988. Testing for a unit root in time series regression. Biometrika, 75(2), pp. 
335-346. 

Raiffa, H. & Schlaifer, R., 1961. Applied Statistical Decision Theory, Boston: Division of Research, 
Graduate School of Business Administration, Harvard University. 

Schotman, P. & Van Dijk, H., 1991. On Bayesian routes to unit roots. Journal of Applied Econometrics, 
6(4), pp. 387-401. 

Shaarawy, S., Soliman, E. & Shahin, H., 2010. Bayesian Prediction of Autoregressive Models Using 
Different Types of Priors. The Egyptian Statistical, 54(2), pp. 108-126. 

Sims, C. & Uhlig, H., 1991. Understanding Unit Rooters: A Helicopter Tour. Econometrics, 59(6), pp. 
1591-1599. 

Wei, W., 1990. Time Series Analysis: Univariate and Multivariate Methods. Reading: Addison-Wesley. 

Yang, R., 1994. Development of Noninformative Priors for Bayesian Analysis, USA: Purdue University. 

Zellner, A., 1983. Application of Bayesian Analysis and Econometrics. The Statistician, Volume 32, pp. 
23-34. 

Zellner, A., 1986. On Assessing Prior Distributions and Bayesian Regression Analysis with g-Prior 
distributions, North Holland: Essays in Honor of Bruno de Finetti, P.d.f. Goel and A. Zellner, eds.. 

Zellner, A. & Tiao, G., 1964. Bayesian Analysis of the Regression Model with Autocorrelated Errors. 
Journal of the American Statistical Association, Volume 59, pp. 763-778. 

 
 
 
 
 
 
