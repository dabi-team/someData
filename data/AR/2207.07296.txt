Direction-Aware Adaptive Online Neural Speech Enhancement with an
Augmented Reality Headset in Real Noisy Conversational Environments

Kouhei Sekiguchi1, Aditya Arie Nugraha1, Yicheng Du2,
Yoshiaki Bando3, Mathieu Fontaine4, and Kazuyoshi Yoshii5

2
2
0
2

l
u
J

5
1

]
S
A
.
s
s
e
e
[

1
v
6
9
2
7
0
.
7
0
2
2
:
v
i
X
r
a

Abstract— This paper describes the practical response- and
performance-aware development of online speech enhancement
for an augmented reality (AR) headset that helps a user un-
derstand conversations made in real noisy echoic environments
(e.g., cocktail party). One may use a state-of-the-art blind source
separation method called fast multichannel nonnegative matrix
factorization (FastMNMF) that works well in various environ-
ments thanks to its unsupervised nature. Its heavy computa-
tional cost, however, prevents its application to real-time pro-
cessing. In contrast, a supervised beamforming method that
uses a deep neural network (DNN) for estimating spatial in-
formation of speech and noise readily ﬁts real-time process-
ing, but suffers from drastic performance degradation in mis-
matched conditions. Given such complementary characteristics,
we propose a dual-process robust online speech enhancement
method based on DNN-based beamforming with FastMNMF-
guided adaptation. FastMNMF (back end) is performed in a
mini-batch style and the noisy and enhanced speech pairs are
used together with the original parallel training data for updat-
ing the direction-aware DNN (front end) with backpropagation
at a computationally-allowable interval. This method is used
with a blind dereverberation method called weighted predic-
tion error (WPE) for transcribing the noisy reverberant speech
of a speaker, which can be detected from video or selected by
a user’s hand gesture or eye gaze, in a streaming manner and
spatially showing the transcriptions with an AR technique. Our
experiment showed that the word error rate was improved by
more than 10 points with the run-time adaptation using only
twelve minutes observation.

I. INTRODUCTION

Sound source separation and enhancement form the basis of
robot audition and computational auditory scene analysis [1],
[2]. To comprehend and respond to the surrounding environ-
ment around robots or intelligent systems in real time, it is
important to maintain both the low computational time and
high separation performance. Many real-time robot audition
systems including HARK [1], ODAS [2], and ManyEars [3]

This work was supported by JSPS KAKENHI Nos. 19H04137, 20K19833,

20H01159, and 20K21813.

1 Kouhei Sekiguchi and Aditya Arie Nugraha are with the Center
for Advanced Intelligence Project (AIP), RIKEN, Tokyo, Japan (e-mail:
adityaarie.nugraha@riken.jp, kouhei.sekiguchi@riken.jp). These authors are
co-ﬁrst authors contributing equally to this work.

2 Yicheng Du is with the Graduate School of Informatics, Kyoto Univer-

sity, Kyoto, Japan (e-mail: du.yicheng.64z@st.kyoto-u.ac.jp).

3 Yoshiaki Bando is with the National Institute of Advanced Industrial
Science and Technology (AIST), Tokyo, Japan and also with the Center
for Advanced Intelligence Project (AIP), RIKEN, Tokyo, Japan (e-mail:
y.bando@aist.go.jp).

4 Mathieu Fontaine is with LTCI, T´el´ecom Paris, Institut Polytechnique

de Paris, Paris, France (e-mail: mathieu.fontaine@telecom-paris.fr).

5 Kazuyoshi Yoshii is with the Graduate School of Informatics, Kyoto
University, Kyoto, Japan and also with the Center for Advanced Intelligence
Project (AIP), RIKEN, Tokyo, Japan (e-mail: yoshii@i.kyoto-u.ac.jp).

have been developed by combining multichannel source lo-
calization, separation, and recognition techniques.

Beamforming is an efﬁcient multichannel source separa-
tion technique that can extract the signal coming from a tar-
get direction [4], [5]. There are various kinds of beamforming
methods such as the delay-and-sum (DS) beamforming and
the minimum power distortionless response (MPDR) beam-
forming [5], which require frequency-wise steering vectors
representing the sound propagation from the source to the
microphones. When pre-recorded steering vectors are used,
the separation performance is limited because the actual envi-
ronment may differ from the environment in which the steer-
ing vectors are recorded. Other beamforming methods such
as the minimum variance distortionless response (MVDR)
beamforming and the generalized eigenvalue (GEV) beam-
forming [5]–[7] require the estimates of the spatial covari-
ance matrices of the noise and target sources.

To estimate such spatial covariance matrices in beamform-
ing, deep neural networks (DNNs) have been used for esti-
mating time-frequency (TF) masks of speech and noise from
the observed mixture spectrogram, sometimes given other fea-
tures including directional information [7]–[11]. Since DNNs
are typically trained in a supervised manner, they are vulnera-
ble to the mismatch of environmental conditions. Conversely,
multichannel blind source separation (BSS) methods such
as multichannel non-negative matrix factorization (MNMF)
[12], independent low-rank matrix analysis (ILRMA) [13],
and FastMNMF [14], [15], can work well in a variety of en-
vironments. In particular, FastMNMF has been demonstrated
to outperform MNMF and ILRMA [15]. Their computational
cost, however, is relatively high, and thus they are not suit-
able for real-time processing.

To fuse the strengths of the supervised and unsupervised
approaches in a practical real-time application, we propose a
dual-process adaptive online speech enhancement framework
consisting of front-end low-latency online DNN-based MVDR
beamforming that provides enhanced speech for downstream
tasks (e.g., speech recognition and translation) and back-end
high-latency semi-online FastMNMF whose outputs are used
for adapting the DNN to the actual environment. To estimate
the spectrogram of target speech, the demixing ﬁlters of
MVDR beamforming are computed from speech and noise
masks estimated by a DNN that takes as input the observed
mixture spectrogram and directional features. The enhanced
speech spectrogram given by FastMNMF, together with the
mixture spectrogram, are used to update the DNN parameters
in a supervised manner.

 
 
 
 
 
 
Fig. 1. The overview of the proposed system for online speech enhancement and recognition in real noisy echoic environments.

Those two components can be executed in parallel with sep-
arate computational resources. Since the back-end FastMNMF
and the DNN adaptation, which are based on iterative opti-
mization, are computationally demanding, they are assumed
to run with the support of graphic processing units (GPUs).
The front-end DNN-based beamforming can run even on an
edge device because of its relatively light-weight computation.
The proposed framework can be interpreted in terms of
teacher-student learning [16], where the front-end DNN acts
as a student model that tries to mimic the back-end Fast-
MNMF acting as a teacher model. Whereas BSS methods
have conventionally been used for training a separation DNN
from scratch in an ofﬂine manner [17]–[19], our adaptation
framework uses the input and output (noisy and enhanced
speeches) of FastMNMF together with the original training
data consisting of noisy and clean speech pairs for ﬁne-tuning
a pretrained separation DNN at a computationally-allowable
interval in a semi-online manner.

The downstream task we consider in this paper is a stream-
ing automatic speech recognition (ASR) [20], [21] for cocktail-
party conversation assistance with an augmented reality (AR)
application with a head-mounted display (HMD). This ap-
plication can also be used for older adults and people with
hearing impairment. The proposed adaptive online speech en-
hancement framework is expected to provide the accurate
speech estimate with low latency for timely transcription,
while being robust against acoustic conditions varying ac-
cording to physical environments and source types and activ-
ities. We thus integrate a blind dereverberation method called
weighted prediction error (WPE) [22], [23], which has been
shown to be effective for improving the ASR performance
in diverse reverberant environments [24], into both the front
and back ends of the proposed framework.

In general, AR applications can help a user perceive the
surrounding environment by superimposing virtual contents
on the user’s view of the real world. Adaptive systems are
required to provide the currently relevant contents based on
the signals captured by various sensors attached to the HMD
(AR headset). AR applications have been studied in many
different contexts, e.g., health care [25], [26], medical care

[27], [28], manufacturing [29], [30], data analytics [31], and
leisure [32]. In addition to visual information and user poses
that have been exploited in conventional studies, the ASR
application considered also relies on audio information, i.e.,
noisy reverberant audio signals. The AR headset that we use
is the Microsoft HoloLens 2 (HL2)1, which is widely avail-
able in the market. Using the data from the sensors attached
on HL2, face tracking, eye tracking, and hand gesture recogni-
tion are performed to estimate the target speaker direction for
guiding the speech enhancement. To our knowledge, speech
enhancement for an AR headset has been dealt with only in
[33], where beamforming is performed based on the target
directions given by a precise motion tracking system Opti-
Track2 whose markers are attached to AR headset prototype.
We experimentally evaluated the proposed system using real
recordings obtained by setting the HL2 on a dummy head.
For evaluation purpose only, we use an ofﬂine ASR system.
We conﬁrmed that with the online adaptation of the DNN
using only twelve minutes of noisy observation, the average
word error rate (WER) was improved by more than 10 points.
The rest of this paper is organized as follows. Section II
overviews our system for an AR headset with multi-modal
sensors. Section III describes the proposed direction-aware
adaptive online speech enhancement framework. Section IV
presents the evaluation. Section V concludes this paper.

II. AUTOMATIC SPEECH RECOGNITION SYSTEM
FOR AUGMENTED REALITY HEADSET

This section provides an overview of our ASR system for
AR headset that incorporates the proposed adaptive online
speech enhancement framework (Figure 1). At an abstract
level, the system uses the visual information from the camera
images to provide possible target speakers from which the
user can select using eye gaze or hand gesture. The speech
enhancement front-end, which is occasionally ﬁne-tuned by
the back-end, extracts possibly multiple speech signals coming
from the target directions corresponding to the user selection

1https://www.microsoft.com/hololens/
2https://optitrack.com/

and input the extracted speech signals into an ASR system.
The generated transcriptions are then displayed as virtual
contents on the AR headset.

The rest of this section describes the equipment used to re-
alize the system, the data communication, the visual informa-
tion processing, the ASR system, and the feedback to and from
the AR headset user. The proposed adaptive online speech en-
hancement framework is presented in the following section.

A. Equipment

The AR headset that we use is the Microsoft HoloLens 2
(HL2). HL2 is an optical see-through HMD that allows the
user to see the real-world surrounding with his/her own eyes,
not through cameras as in a video see-through HMD. It has
inertial measurement unit (IMU), different types of cameras
that are used for different purposes, including head tracking
and eye tracking, and a 5-channel microphone array. Among
several AR headsets available in the market nowadays, we
choose HL2 not only because it has the sensors we need, but
also because it is supported by the Mixed Reality Toolkit
(MRTK)3 that enables us to have rapid AR application de-
velopment using Unity4.

As the server, we use a desktop or laptop computer with
sufﬁcient GPU computing power. We describe the relevant
speciﬁcations of the machine used for our experiments in
Section IV.

Fig. 2. An example of what the AR headset user sees.

transformation matrices that are also sent by HL2 and the
ratio of the detected face width to a pre-deﬁned face width
reference such that virtual face indicators can be generated
on top of the detected face (see Figure 2).

D. Automatic Speech Recognition

The speech transcription is supposed to be obtained using
a streaming ASR system [20], [21], such as one based on Mi-
crosoft Azure5 or Google Cloud6 service. Using a streaming
ASR, the user can continuously receive transcriptions. The
latency of the ASR system is beyond the scope of this paper.

B. Data Communication

E. Human-Machine Interaction

Data is exchanged via Robot Operating System (ROS) [34].
HL2 and the server are connected on the same wireless local
area network so the HL2 user has freedom to move. Wired
connection can also be used for having a more reliable con-
nection while charging the HL2, e.g., in the case of record-
ing large amount of data for the experiments.

HL2 sends RGB (red, green, blue) images with a size of
960 x 540 pixels and 5 frames per second, 5-channel audio
signals sampled at 48 kHz, and information whether face
indicators are selected by the user. Different modules running
on the server receive different parts of the data published by
HL2. The face tracking module takes the images to obtain
the face positions such that the program running on HL2 can
generate selectable virtual indicators on top of the detected
faces. The speech enhancement module uses the selected face
indicator information and the multichannel audio signals from
HL2 to obtain the target speech signals for the ASR module
inputs. The ASR module sends the transcriptions to HL2
for which virtual contents are generated by the application
running on HL2.

C. Visual Information Processing

The AR headset user’s eyes and hands are tracked on the
HL2 using MRTK. The face tracking based on FaceNet [35]
is done on the server given the RGB images from HL2. The
resulting bounding boxes on 2-dimensional images are then
projected to the 3-dimensional virtual world using the camera

Figure 2 illustrates what the AR headset user sees. The face
indicators and the text panel showing the ASR transcriptions
are contents generated in the virtual world. The face indicators
are virtually placed on top of the real-world detected face. The
indicators are also served as pressable buttons for selecting
target speakers. The user can focus on a face indicator using
his/her eye gaze, and then select the indicator using a pinching
hand gesture. The text panel is always in front of the user
following his/her movement to make sure that the text is
legible.

III. DIRECTION-AWARE ADAPTIVE ONLINE
SPEECH ENHANCEMENT FRAMEWORK

This section explains in detail the proposed adaptive online
speech enhancement framework that composed of a front-end
online processing based on beamforming with a separation
DNN and a back-end semi-online processing based on multi-
channel BSS for ﬁne-tuning the front-end DNN. Both front-
end and back-end systems use WPE for dereverberation [22],
[23].

Let X (cid:44) {xf t}F,T

f,t=1 ∈ CF ×T ×M be the observed multi-
channel mixture short-time Fourier transform (STFT) spectro-
gram, where F , T , and M represent the numbers of frequency
bins, time frames, and microphones, respectively. Both front-
end and back-end parts apply the block-online processing,
where one block is composed of a sequence of multiple time
frames. To have outputs with a low latency, the front-end

3https://docs.microsoft.com/windows/mixed-reality/
4https://unity.com/

5https://azure.microsoft.com/services/cognitive-services/speech-to-text/
6https://cloud.google.com/speech-to-text/

part can use a smaller block size to reduce the computational
time and a smaller block shift size to perform computations
more often. Conversely, the back-end part can use a larger
block size to obtain more data useful for BSS.

A. Front-end DNN-Based Beamforming

xf t1

Given multichannel mixture spectrogram and spatial infor-
mation about a target direction as input, the separation net-
work estimates the time-frequency (TF) masks of the signal
coming from the target direction (Figure 3). Speciﬁcally, the
inputs are the log-magnitude spectrograms of ﬁrst channel of
the mixture signals, the log-magnitude spectrograms of the
delay-and-sum beamforming (DSBF) [5] results, inter-channel
phase differences (IPDs) expressed as the cosine and sine of
{∠ xf t,m(≥2)
}F,T,M
f =1,t=1,m=2, and an angle of the target direc-
tion. For each block composed of T frames, a matrix of size
T × (2M F ) consisting of the log-magnitude spectrograms
and IPDs is fed into a preprocessing network that outputs a
matrix B = [b1, . . . , bT ]T ∈ RT ×L, where L is the input di-
mension of the following network. The cosine and sine of the
angle of the target direction are fed into a direction attractor
network (DAN) [11] to obtain a vector c ∈ RL. {bt}T
t=1 and
c are then multiplied in an element-wise manner and fed into
a bidirectional long short-term memory (BLSTM) network
with a ﬁnal fully-connected layer resulting in TF masks.

The estimated TF masks {mf t}F,T

f,t=1 are used for calcu-
lating a minimum variance distortionless response (MVDR)
beamformer [6] wMVDR

given as follows:

f

wMVDR
f

=

tr

Σ−1
(cid:16)

N,f ΣS,f
Σ−1

N,f ΣS,f

(cid:17) un0 ,

(1)

where un0 is a one-hot vector whose n0-th entry is 1 and 0
elsewhere and n0 is the index of the reference microphone
(e.g., the top center microphone of HL2). ΣS,f and ΣN,f
are speech and noise spatial covariance matrices (SCMs),
respectively, and calculated as follows:

ΣS,f =

ΣN,f =

T
(cid:88)

t=1
T
(cid:88)

t=1

mf txf txH
f t,

(1 − mf t)xf txH
f t.

(2)

(3)

A source SCM encodes the spatial characteristics (including
direction and spread) of a source as seen by the microphone
array [36]. The enhanced time-frequency domain signal given
)Hxf t is transformed into a time-domain signal
by (wMVDR
f
senh by using inverse STFT. In the pre-training step, negative
SI-SDR [37] between the enhanced signal senh and the clean
signal of the target direction sref is used as a loss function:

SI-SDR(senh, sref) = 10 log10

(cid:18)

||αsref||2
||αsref − senh||2

(cid:19)

,

(4)

where α = sT
enhsref/||sref||2 is a scaling factor. In the ﬁne-
tuning step, instead of the clean reference signals, the outputs
of the back-end BSS are used as reference.

B. Back-end Blind Source Separation

A widely-used approach to BSS is to formulate and opti-
mize a probabilistic model of the observed multichannel mix-
ture spectrogram X consisting of a spatial model that repre-
sents the SCMs and a source model that represents the power
spectral densities (PSDs). Our system adopts the state-of-the-
art BSS method called FastMNMF [15] shown in Figure 4 that
consists of a jointly-diagonalizable (JD) spatial model and a
nonnegative matrix factorization (NMF)-based source model.
Suppose that a mixture of N sources is recorded by M mi-
crophones. Let Xn (cid:44) {xnf t}F,T
f,t=1 be the multichannel spec-
trogram of source n called image. FastMNMF assumes that
each source image xnf t follows an M -variate complex-valued
circularly-symmetric Gaussian distribution, whose covariance
matrix is jointly-diagonalizable by a time-invariant diagonal-
ization matrix shared among all sources Qf ∈ CM×M :

xnf t ∼ N M
C

(cid:16)
0, λnf tQ−1

f Diag(˜gn)Q−H

f

(cid:17)

,

(5)

where λnf t ∈ R+ is the PSD, and Diag(˜gn) is a diagonal
matrix whose diagonal elements ˜gn (cid:44) [˜g1n, . . . , ˜gM n]T ∈ RM
+
is a frequency-invariant nonnegative vector [15]. This joint-
diagonalization leads to the mixture decorrelation:

yf t (cid:44) Qf xf t ∼ N M
C

0,

(cid:32)

(cid:33)

λnf tDiag(˜gn)

,

(6)

N
(cid:88)

n=1

(cid:0)0, σ2

(cid:1) are independent
so {yf t}m ∼ NC
of each other. We optimize the parameters Φ (cid:44) {Qf , uncf ,
vnct, ˜gmn|∀m, ∀n, ∀f, ∀t, ∀c} by maximizing

n=1 λnf t˜gmn

(cid:44) (cid:80)N

mf t

ln p(X) =

F,T,M
(cid:88)

f,t,m=1

ln p({yf t}m) + T

F
(cid:88)

f =1

ln |Qf |2 ,

(7)

|{yf t}m|2
σ2

ln p({yf t}m) = −

− ln σ2

mf t + const.

(8)

mf t
The estimated source image (cid:98)xnf t can then be computed given
xf t and Φ by Wiener ﬁltering:

(cid:98)xnf t = Q−1

f Diag

(cid:32)

λnf t˜gn
n(cid:48)=1 λn(cid:48)f t˜gn(cid:48)

(cid:80)N

(cid:33)

Qf xf t.

(9)

The parameter optimization is performed in an iterative
manner, and sophisticated initialization often improves the
performance and makes convergence speed faster. In the be-
ginning of the iterations, a frequency-invariant source model
given by λnf t = λnt is used to alleviate the frequency per-
mutation problem. Then, the source model is switched to an
NMF-based model given by λnf t = (cid:80)C
c=1 uncf vnct to pre-
cisely represent the PSDs, where uncf ∈ R+ is a basis, vnct ∈
R+ is an activation, and C is the number of NMF components.
In our system, the direction of the target sound source
obtained from the visual information is used for initialization.
Because the column vectors of Q−1
in the JD spatial model
are known to play a similar role as steering vectors [15], the
ﬁrst column vector of Q−1
is initialized to the steering vector
af of the target direction, and ˜g1 is initialized to a one-hot-
like vector whose ﬁrst element is 1 and other elements are (cid:15)

f

f

Fig. 3. The architecture of the front-end DNN-based beamforming.

Fig. 4. The probabilistic generative model of a multichannel mixture using a jointly-diagonalizable full-rank spatial model [15].

(e.g., 0.01). This means that the SCM of source 1 is set to
a matrix that is close to the rank-1 matrix af aH
f . Note that
the target sound source is not always active, and if it is not
active, the SCM will move away from af aH
f .

For storing ﬁne-tuning data, we need to estimate which
estimated source image corresponds to the target direction.
Thanks to the initialization explained above, source 1 often
corresponds to the target direction. However, this does not
always hold, especially when the steering vector used for
initialization signiﬁcantly differs from the actual steering
vector and when the target source is inactive. This calls for
the estimation of the direction for each source image.

f

f Diag(˜gn)Q−H

Inspired by the source localization method called multiple
signal classiﬁcation (MUSIC) [38], we estimate the corre-
spondences between source images and directions using the
eigenvectors {vnf m}M
m=1 computed from the source SCM
Gnf (cid:44) Q−1
. Assuming that source n cor-
responds to the target direction, the principal eigenvector
vnf 1 is close to the steering vector of the target direction
af , and the other eigenvectors {vnf m}M
m=2 are orthogonal
to the steering vector. Thus, the degree of response given by
ln (cid:44) (cid:80)F
f vnf m|2 becomes small for the target
direction and large for other directions. If ln is smaller than
a threshold, source n is regarded as the target source, and the
corresponding estimated source image is stored in a buffer
for ﬁne-tuning the separation DNN.

m=2 |aH

(cid:80)M

f =1

C. Online Fine-tuning Strategy

To collect data for ﬁne-tuning the front-end separation
network, the BSS method is always working in the back-end.
If the estimated source image corresponding to the target
direction is obtained, the source image, the target direction,
and the observed multichannel signals are stored. When the
amount of the data exceeds a threshold, ﬁne-tuning of the
separation network is executed. To reduce the computational
cost and to deal with the change of a surrounding environment,
only recent data is used for ﬁne-tuning. The ﬁrst ﬁne-tuning
step updates the pre-trained network parameters, while the
subsequent ﬁne-tuning steps update the network parameters
obtained from the previous ﬁne-tuning step.

IV. EVALUATION AND DISCUSSION

In this section, we evaluate the effectiveness of the online
ﬁne-tuning of the separation DNN by using data recorded
with Microsoft HoloLens 2 (HL2).

A. Experimental Conditions

For ﬁne-tuning and evaluation, we separately recorded 5-
channel (M = 5) reverberant speech signals and diffuse noise
in a room with an RT60 of about 800 ms by using a HL2.
The HL2 was set on a dummy head, and a loudspeaker was
placed at each of eight directions with 45 degrees interval.
The distances between the HL2 and the loudspeakers were set

TABLE I
AVERAGE WERS [%] AND COMPUTATION TIMES [S] OF THE BASELINE

METHODS. THE TOTAL TIME LATENCY [S] IS THE SUM OF THE BLOCK
SHIFT SIZE AND THE AVERAGE COMPUTATION TIME FOR EACH BLOCK.

Block

Comp.

Total

Size [s] Shift [s] Time [s] Latency [s]

Method
Clean
Observation (Noisy)
DSBF
MPDR
FastMNMF
FastMNMF
FastMNMF
Pre-trained DNN
+ MVDR

–
–
3.07
3.07
3.07
3.07
9.22

3.07

–
–
0.51
0.51
0.51
3.07
9.22

0.50

–
–
0.04
0.04
0.66
0.68
1.89

0.25

WER
[%]
6.1
92.1
70.5
53.0
29.2
24.2
15.0

–
–
0.55
0.55
1.17
3.75
11.11

0.75

35.6

TABLE II
AVERAGE WERS [%] OF THE OFFLINE FINE-TUNING

Fine-tune
Data [min]
3
6
12
18
24
30
36
42
48

5

0

3

30

20

40

The number of epochs for ﬁne-tuning
50
10
1
35.6 31.6 30.8 29.2 31.6 34.0 32.8 34.9 35.3
35.6 27.3 29.4 28.0 28.4 26.8 30.8 31.7 31.3
35.6 26.4 25.3 23.6 27.0 26.9 29.2 27.9 28.1
35.6 25.7 26.9 24.1 25.6 26.9 24.2 26.5 26.4
35.6 26.8 22.9 26.8 28.2 25.8 24.9 24.2 24.9
35.6 25.6 24.0 21.3 23.7 23.0 22.5 22.9 21.9
35.6 25.1 23.0 23.7 21.0 22.0 23.9 22.3 23.4
35.6 26.1 27.7 20.8 22.5 20.4 22.2 25.5 25.6
35.6 23.7 21.3 25.2 23.3 21.7 22.9 24.1 28.6

to 1.5 meters except for 135 degrees, and that at 135 degrees
was 3 meters. The loudspeaker at 0 degree was regarded as the
target, and that of the other directions except for 135 degrees
were regarded as interference speakers. The loudspeaker at
135 degrees were used for noise. To make the noise signals
diffuse, a room partition was placed between a loudspeaker
and the device, and the loudspeaker was directed to the
opposite direction to the device. The dry speech signals were
randomly selected from the test-clean subset of Librispeech
dataset [39], and the noise signals were randomly selected
from the backgrounds folder in CHiME3 dataset [40]. We
then synthesized 66 minutes noisy mixture signals consisting
of two reverberant speech and one diffuse noise signals. The
signals were split into 48 minutes ﬁne-tuning data and 18
minutes evaluation data.

For pre-training the separation network, we synthesized
noisy mixture signals of about 15 hours consisting of two
speech and diffuse noise signals by using recorded impulse re-
sponses. The dry speech signals were randomly selected from
WSJ0 SI-84 training set, and the noise signals were randomly
selected from DEMAND dataset. The impulse responses were
recorded at 72 directions with 5 degrees interval in an ane-
choic room. To simulate diffuse noise signals, we summed
up 36 different noise signals each of which was convolved
with one of 36 impulse responses with 10 degrees interval.
Audio signals were sampled at 16 kHz and processed by
STFT with a Hann window of 1024 points (F = 513) and a
shifting interval of 256 points. The block size of the back-end
FastMNMF for making the ﬁne-tuning data was 561 frames
(about 9 seconds), and the number of NMF components C
was 8. The number of iterations for updating FastMNMF
with the frequency-invariant source model was 50, and that
for updating vanilla FastMNMF was 50. For both front-end
and back-end, WPE with the tap length of 5, the delay of 3,
and the number of iterations of 3 was used.

of the hidden layers was 512. In the pre-training step, the
separation network was trained with a learning rate of 0.001,
batch size of 128, and the maximum number of epochs of
500. In the inference step, it was used with a shifting interval
of 8000 points (0.5 seconds), and except for the ﬁrst iteration,
only the last 8000 points were used as output. In the ﬁne-
tuning step, the network was trained with a learning rate
of 0.001 and batch size of 16. To stabilize the training, in
addition to the ﬁne-tuning data, we also used the data used
in the pre-training step. The ratio between the ﬁne-tuning
and pre-training data was set to 1:1.

To conﬁrm the upper bound of the ﬁne-tuning, we tested off-
line ﬁne-tuning that used a part of the ﬁne-tuning data at one
time. The amount of data was {3, 6, 12, 18, 24, 30, 36, 42, 48}
minutes, and the maximum number of epochs was 50. Then,
we tested online ﬁne-tuning discussed in Section III-C. The
ﬁne-tuning was executed every 3 minutes, and the amount of
data used in one ﬁne-tuning was 12 minutes at most. Until
12 minutes of data was obtained, all of the accumulated data
was used. After that, only the latest 12 minutes of data was
used. As for the number of epochs per one ﬁne-tuning, we
tested {1, 3, 5, 10}.

As a baseline, we tested delay-and-sum beamforming
(DSBF), minimum power distortionless response (MPDR)
beamforming, and FastMNMF with block size of 192 frames
(about 3 seconds) and shift size of 32 frames (about 0.5 sec-
onds), which was almost the same as the front-end DNN-
based beamforming.

The word error rate (WER) was used as a criterion for
evaluating the performance. For evaluation purpose only, we
used an ofﬂine ASR system using the transformer-based
acoustic and language models of the SpeechBrain toolkit
[41]. The computation time for processing one block was
measured on Intel Xeon W-2145 (3.70 GHz) with NVIDIA
GeForce Titan RTX.

B. Experimental Results

The block size of the separation network was 189 frames
(about 3 seconds). As preprocessing network and direction
attractor network described in Section III-A, we used three
fully-connected layers, and the dimension L was 1024. The
number of layers of the BLSTM was three and the dimension

Table I shows the average WERs and computation times of
the baseline methods, and Table II shows the average WERs
of the front-end DNN-based beamforming with the ofﬂine
ﬁne-tuning, where the base performance using the pre-trained
DNN is shown when the number of epochs equals zero.

V. CONCLUSION

In this article, we proposed the streaming ASR system
with the front-end DNN-based beamforming and the back-
end semi-online BSS for ﬁne-tuning the front-end DNN.
Using visual information and eye gaze and hand gesture
recognition, the device can select a target speaker, and the
transcription of the target speaker given from the front-end
system is continuously shown on the device. To compensate
for the sensitivity to a surrounding environment of the front-
end DNN, the back-end system accumulates the signals from
the target speaker by using the state-of-the-art BSS method,
and periodically ﬁne-tunes the DNN. In the experiment, we
conﬁrmed that the WER improves by 12 points with only
twelve minutes of ﬁne-tuning data.

REFERENCES

[1] K. Nakadai, H. G. Okuno, and T. Mizumoto, “Development, deployment
and applications of robot audition open source software HARK,” J.
Robot. Mechatronics, vol. 29, no. 1, pp. 16–25, 2017.

[2] F. Grondin, D. L´etourneau, C. Godin, J.-S. Lauzon, J. Vincent,
S. Michaud, S. Faucher, and F. Michaud, “ODAS: Open embedded
audition system,” pp. 1–6, 2021, arXiv:2103.03954v1.

[3] F. Grondin, D. L´etourneau, F. Ferland, V. Rousseau, and F. Michaud,
“The ManyEars open framework,” Autonomous Robots, vol. 34, no. 3,
pp. 217–232, 2013.

[4] K. Kumatani, J. McDonough, and B. Raj, “Microphone array processing
for distant speech recognition: From close-talking microphones to far-
ﬁeld sensors,” IEEE Signal Process. Mag., vol. 29, no. 6, pp. 127–140,
2012.

[5] E. Vincent, T. Virtanen, and S. Gannot, Eds., Audio Source Separation
and Speech Enhancement. Hoboken, USA: John Wiley & Sons, 2018.
[6] M. Souden, J. Benesty, and S. Affes, “On optimal frequency-domain
multichannel linear ﬁltering for noise reduction,” IEEE Audio, Speech,
Language Process., vol. 18, no. 2, pp. 260–276, Feb. 2010.

[7] J. Heymann, L. Drude, and R. Haeb-Umbach, “A generic neural acoustic
beamforming architecture for robust multi-channel speech processing,”
Computer Speech & Language, vol. 46, pp. 374–385, 2017.

[8] Z. Chen, X. Xiao, T. Yoshioka, H. Erdogan, J. Li, and Y. Gong,
“Multi-channel overlapped speech recognition with location guided
speech extraction network,” in Proc. IEEE Spoken Language Technology
Workshop, 2018, pp. 558–565.

[9] J. M. Mart´ın-Do˜nas, J. Heitkaemper, R. Haeb-Umbach, A. M. Gomez,
and A. M. Peinado, “Multi-channel block-online source extraction based
on utterance adaptation,” in Proc. INTERSPEECH, 2019, pp. 96–100.
[10] A. S. Subramanian, C. Weng, M. Yu, S.-X. Zhang, Y. Xu, S. Watanabe,
and D. Yu, “Far-ﬁeld location guided target speech extraction using
end-to-end speech recognition objectives,” in Proc. IEEE Int. Conf.
Acoust., Speech, Signal Process., 2020, pp. 7299–7303.

[11] Y. Nakagome, M. Togami, T. Ogawa, and T. Kobayashi, “Deep speech
extraction with time-varying spatial ﬁltering guided by desired direction
attractor,” in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process.,
2020, pp. 671–675.

[12] A. Ozerov and C. F´evotte, “Multichannel nonnegative matrix factoriza-
tion in convolutive mixtures for audio source separation,” IEEE/ACM
Trans. Audio, Speech, Language Process., vol. 18, no. 3, pp. 550–563,
2009.

[13] D. Kitamura, N. Ono, H. Sawada, H. Kameoka, and H. Saruwatari,
“Determined blind source separation unifying independent vector anal-
ysis and nonnegative matrix factorization,” IEEE/ACM Trans. Audio,
Speech, Language Process., vol. 24, no. 9, pp. 1626–1641, 2016.
[14] N. Ito and T. Nakatani, “FastMNMF: Joint diagonalization based ac-
celerated algorithms for multichannel nonnegative matrix factorization,”
in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., Brighton,
UK, May 2019, pp. 371–375.

[15] K. Sekiguchi, Y. Bando, A. A. Nugraha, K. Yoshii, and T. Kawahara,
“Fast multichannel nonnegative matrix factorization with directivity-
aware jointly-diagonalizable spatial covariance matrices for blind source
separation,” IEEE/ACM Trans. Audio, Speech, Language Process.,
vol. 28, pp. 2610–2625, 2020.

Fig. 5. Average WERs [%] of the online ﬁne-tuning. The horizontal lines
indicate the average WERs over all time.

TABLE III
COMPUTATION TIME [S] OF FINE-TUNING PER ONE EPOCH

Fine-tune Data [min]
Comp. Time [s]

6

3

48
24
11.4 21.7 42.5 63.3 84.4 105 126 146 169

12

42

18

30

36

In terms of the ASR performance, FastMNMF outper-
formed the other baseline methods and the DNN-based beam-
forming using a pre-trained DNN. With a larger block size, the
performance of FastMNMF was improved, but the computa-
tion time was also increased. Even with a block size of about 3
s and a block shift size of about 0.5 s, the computation time of
0.66 s was more than the block shift, making real-time process-
ing impossible to achieve the same ASR performance level.
Conversely, using the front-end DNN-based beamforming,
the average computation time for processing one block with a
similar size was only 0.25 s. These experimental results vali-
date that FastMNMF is not suitable for front-end processing.
Table II further demonstrates that FastMNMF is useful as
a back-end for ﬁne-tuning the front-end DNN. After ﬁne-
tuning for 10 epochs or less, the best performance can be
achieved in almost all conditions. When the amount of data
is small, too many epochs caused overﬁtting and decreased
the performance. With only 12 minutes of data, the WER
improved 12 points from that without ﬁne-tuning, and it is
better than that of FastMNMF with almost the same block size
(3 seconds). With 42 minutes of data, the WER improvement
was the highest (15.2 points).

Figure 5 shows the average WERs of the online ﬁne-tuning.
The average WERs over all time were 25.8, 25.4, 25.2, and
27.3% when the number of epochs was 1, 3, 5, and 10,
respectively. Although the performance sometimes ﬂuctuated,
the average WERs improved by more than 10 points from
that without the ﬁne-tuning. Table III shows that performing
ﬁne-tuning with 12 minutes of data for 1 or 3 epochs is
a reasonable choice because the total computation time is
less than the ﬁne-tuning interval duration, i.e., 3 minutes.
The computation time can be further reduced by changing
the ratio between the ﬁne-tuning and original training data,
although it sometimes makes the training unstable.

036912151821242730333639424548The elapsed time [min]20.022.525.027.530.032.535.037.5WER [%]epoch=  1epoch=  3epoch=  5epoch=10[16] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge
in a neural network,” in Proc. NIPS Deep Learning and
Representation Learning Workshop, 2015, pp. 1–9. [Online]. Available:
http://arxiv.org/abs/1503.02531

[17] L. Drude, D. Hasenklever, and R. Haeb-Umbach, “Unsupervised train-
ing of a deep clustering model for multichannel blind source separa-
tion,” in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2019,
pp. 695–699.

[18] Y. Bando, Y. Sasaki, and K. Yoshii, “Deep bayesian unsupervised
source separation based on a complex gaussian mixture model,” in
Proc. IEEE Int. Workshop Machine Learning Signal Process., 2019.

[19] Y. Nakagome, M. Togami, T. Ogawa, and T. Kobayashi, “Mentoring-
reverse mentoring for unsupervised multi-channel speech source sepa-
ration,” in Proc. INTERSPEECH, 2020, pp. 86–90.

[20] J. Yu, W. Han, A. Gulati, C.-C. Chiu, B. Li, T. N. Sainath, Y. Wu,
and R. Pang, “Dual-mode ASR: Unify and improve streaming ASR
with full-context modeling,” in Proc. Int. Conf. Learn. Representations,
2021, pp. 1–13.

[21] L. Lu, N. Kanda, J. Li, and Y. Gong, “Streaming end-to-end multi-
talker speech recognition,” IEEE Signal Process. Lett., vol. 28, pp.
803–807, 2021.

[22] T. Yoshioka and T. Nakatani, “Generalization of multi-channel linear
prediction methods for blind MIMO impulse response shortening,”
IEEE Audio, Speech, Language Process., vol. 20, no. 10, pp. 2707–
2720, 2012.

[23] L. Drude, J. Heymann, C. Boeddeker, and R. Haeb-Umbach, “NARA-
WPE: A python package for weighted prediction error dereverberation
in numpy and tensorﬂow for online and ofﬂine processing,” in Speech
Communication; 13th ITG-Symposium, 2018, pp. 1–5.

[24] K. Kinoshita, M. Delcroix, S. Gannot, E. A. P. Habets, R. Haeb-
Umbach, W. Kellermann, V. Leutnant, R. Maas, T. Nakatani, B. Raj,
A. Sehr, and T. Yoshioka, “A summary of the reverb challenge: State-
of-the-art and remaining challenges in reverberant speech processing
research,” EURASIP J. Adv. Signal Process., vol. 2016, no. 7, pp. 1–
19, 2016.

[25] D. J. Geerse, B. Coolen, and M. Roerdink, “Quantifying spatiotemporal
gait parameters with HoloLens in healthy adults and people with
parkinson’s disease: Test-retest reliability, concurrent validity, and face
validity,” Sensors, vol. 20, no. 11, pp. 1–12, 2020.

[26] A.-L. Guinet, G. Bouyer, S. Otmane, and E. Desailly, “Validity of
hololens augmented reality head mounted display for measuring gait
parameters in healthy adults and children with cerebral palsy,” Sensors,
vol. 21, no. 8, pp. 1–16, 2021.

[27] G. Badiali, L. Cercenelli, S. Battaglia, E. Marcelli, C. Marchetti, V. Fer-
rari, and F. Cutolo, “Review on augmented reality in oral and cranio-
maxillofacial surgery: Toward “surgery-speciﬁc” head-up displays,”
IEEE Access, vol. 8, pp. 59 015–59 028, 2020.

[28] C. M. Andrews, A. B. Henry, I. M. Soriano, M. K. Southworth, and
J. R. Silva, “Registration techniques for clinical applications of three-
dimensional augmented reality devices,” IEEE J. Transl. Eng. Health
Med., vol. 9, pp. 1–14, 2021.

[29] P. Fraga-Lamas, T. M. Fern´andez-Caram´es, ´O. Blanco-Novoa, and
M. A. Vilar-Montesinos, “A review on industrial augmented reality
systems for the industry 4.0 shipyard,” IEEE Access, vol. 6, pp. 13 358–
13 375, 2018.

[30] G. Avalle, F. De Pace, C. Fornaro, F. Manuri, and A. Sanna, “An
augmented reality system to support fault visualization in industrial
robotic tasks,” IEEE Access, vol. 7, pp. 132 343–132 359, 2019.
[31] B. Hoppenstedt, T. Probst, M. Reichert, W. Schlee, K. Kammerer,
M. Spiliopoulou, J. Schobel, M. Winter, A. Felnhofer, O. D. Koth-
gassner, and R. Pryss, “Applicability of immersive analytics in mixed
reality: Usability study,” IEEE Access, vol. 7, pp. 71 921–71 932, 2019.
[32] I. Picallo, A. Vidal-Balea, O. Blanco-Novoa, P. Lopez-Iturri, P. Fraga-
Lamas, H. Klaina, T. M. Fern´andez-Caram´es, L. Azpilicueta, and
F. Falcone, “Design and experimental validation of an augmented reality
system with wireless integration for context aware enhanced show
experience in auditoriums,” IEEE Access, vol. 9, pp. 5466–5484, 2021.
[33] J. Donley, V. Tourbabin, J.-S. Lee, M. Broyles, H. Jiang, J. Shen,
M. Pantic, V. K. Ithapu, and R. Mehra, “EasyCom: An augmented
reality dataset to support algorithms for easy communication in noisy
environments,” pp. 1–9, 2021, arXiv:2107.04174v2.

[34] M. Quigley, K. Conley, B. Gerkey, J. Faust, T. Foote, J. Leibs,
R. Wheeler, and A. Ng, “ROS: an open-source robot operating sys-
tem,” in Proc. of the IEEE Intl. Conf. on Robotics and Automation
(ICRA) Workshop on Open Source Robotics, 2009, pp. 1–6.

[35] F. Schroff, D. Kalenichenko, and J. Philbin, “FaceNet: A uniﬁed
embedding for face recognition and clustering,” in Proc. IEEE Conf.
Comput. Vision Pattern Recognition, 2015, pp. 815–823.

[36] N. Q. K. Duong, E. Vincent, and R. Gribonval, “Under-determined
reverberant audio source separation using a full-rank spatial covariance
model,” IEEE Audio, Speech, Language Process., vol. 18, no. 7, pp.
1830–1840, 2010.

[37] J. L. Roux, S. Wisdom, H. Erdogan, and J. R. Hershey, “SDR – half-
baked or well done?” in Proc. IEEE Int. Conf. Acoust., Speech, Signal
Process., Brighton, UK, May 2019, pp. 626–630.

[38] R. Schmidt, “Multiple emitter location and signal parameter estimation,”
IEEE Trans. Antennas Propag., vol. 34, no. 3, pp. 276–280, 1986.

[39] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech: An
asr corpus based on public domain audio books,” in Proc. IEEE Int.
Conf. Acoust., Speech, Signal Process., 2015, pp. 5206–5210.
[40] E. Vincent, S. Watanabe, A. A. Nugraha, J. Barker, and R. Marxer, “An
analysis of environment, microphone and data simulation mismatches
in robust speech recognition,” Computer Speech & Language, vol. 46,
pp. 535–557, 2017.

[41] M. Ravanelli, T. Parcollet, P. Plantinga, A. Rouhe, S. Cornell, L. Lu-
gosch, C. Subakan, N. Dawalatabad, A. Heba, J. Zhong, J.-C. Chou,
S.-L. Yeh, S.-W. Fu, C.-F. Liao, E. Rastorgueva, F. Grondin, W. Aris,
H. Na, Y. Gao, R. D. Mori, and Y. Bengio, “SpeechBrain: A general-
purpose speech toolkit,” pp. 1–34, 2021, arXiv:2106.04624v1.

