Streaming Align-Reﬁne for Non-autoregressive Deliberation

Weiran Wang

Ke Hu

Tara N. Sainath

Google, Inc.
{weiranwang, huk, tsainath}@google.com

2
2
0
2

r
p
A
5
1

]
L
C
.
s
c
[

1
v
6
5
5
7
0
.
4
0
2
2
:
v
i
X
r
a

Abstract
We propose a streaming non-autoregressive (non-AR) decoding
algorithm to deliberate the hypothesis alignment of a stream-
ing RNN-T model. Our algorithm facilitates a simple greedy
decoding procedure, and at the same time is capable of produc-
ing the decoding result at each frame with limited right context,
thus enjoying both high efﬁciency and low latency. These ad-
vantages are achieved by converting the ofﬂine Align-Reﬁne al-
gorithm to be streaming-compatible, with a novel transformer
decoder architecture that performs local self-attentions for both
text and audio, and a time-aligned cross-attention at each layer.
Furthermore, we perform discriminative training of our model
with the minimum word error rate (MWER) criterion, which
has not been done in the non-AR decoding literature. Experi-
ments on voice search datasets and Librispeech show that with
reasonable right context, our streaming model performs as well
as the ofﬂine counterpart, and discriminative training leads to
further WER gain when the ﬁrst-pass model has small capacity.
Index Terms: streaming ASR, non-autoregressive decoding,
discriminative training

1. Introduction
There has been a surge of interest in non-autoregressive (non-
AR) automatic speech recognition (ASR) models that are not
constrained to decode in a left-to-right manner [1, 2, 3, 4, 5, 6, 7,
8]. These models make parallel update steps during inference,
i.e., each decoding step can modify multiple or all positions of
previous step results simultaneously. And they are attractive due
to the simplicity and efﬁciency of their inference procedure.

Non-AR models can be largely categorized into two classes.
The ﬁrst class of models iteratively reﬁne the label sequences [2,
3, 4, 6], following the general framework of mask-predict [1]:
in each reﬁnement step, certain positions of the input label se-
quence are replaced by a special [mask] token, and the model
learns to predicts all masked tokens simultaneously given the
partially observed token sequence. A challenge to this approach
is to estimate the length of the ground truth label sequence, for
which heuristics based on CTC decoding results [3, 5] and the
dynamic length prediction task [4] have been developed.

The second class of non-AR methods perform iterative re-
ﬁnement instead on alignments, which are sequences containing
underlying tokens (including blanks for non-emission) at each
frame. Representative methods in this class are Imputer [7] and
Align-Reﬁne [8, 9]. The former gradually reveals positions of a
fully masked alignment in a ﬁxed number of steps, and its train-
ing procedure suffers from exposure bias. The latter updates
complete alignment in each reﬁnement step, effective allowing
complex edits to the label sequences.

Previous work [9] has proposed a practical use case of
Align-Reﬁne for deliberation [10, 11]. The authors use a small
causal RNN-T [12, 13], which runs fast and has reasonable ac-
curacy, to generate the initial hypothesis alignment, as opposed
to using CTC [14] as ﬁrst-pass in the original algorithm of [8].

The authors then apply Align-Reﬁne to the initial alignment for
a few steps to generate new hypotheses of improved accuracy.

We note however, an important aspect of ASR—latency—
has not been addressed by existing alignment-based non-AR
methods. In this work, we develop a streaming version of Align-
Reﬁne, which is capable of producing results through greedy
decoding as data comes in, with controllable delay at each
frame; this allows us to extend the use of Align-Reﬁne into ap-
plication scenarios with more stringent requirements on latency.
To achieve this goal, we propose a novel transformer decoder
architecture that performs local self-attentions for both text and
audio separately, and a time-aligned cross-attention at each
layer; this architecture incorporates audio right context with-
out incurring unnecessary model delay. While there have been
use of other non-AR methods in the streaming mode [6, 15], the
inference procedures of these methods are not as simpleas ours.
Furthermore, given our clean formulation and inference
procedure, we perform discriminative training of streaming
Align-Reﬁne, with the minimum word error rate (MWER) cri-
terion [16]. We found discriminative training to provide further
WER gain when the ﬁrst-pass model has small capacity. To the
best of our knowledge, this is the ﬁrst time sequence training
is used in the non-AR decoding literature. Experimental results
on voice search datasets and Librispeech show that with reason-
able right context, streaming Align-Reﬁne performs similarly
well as the ofﬂine counterpart, and compares favorably against
existing non-AR methods and deliberation methods.

2. Streaming Non-AR deliberation

2.1. Review of ofﬂine Align-Reﬁne

We generally follow the framework of [9] to apply Align-
Reﬁne [8] for deliberation. We use a streaming RNN-T [12, 13]
to generate ﬁrst-pass hypothesis by autoregressive beam search,
which achieves both good accuracy (by modeling label depen-
dency) and low latency (without using right context). Beam
search returns the alignment of the most probable hypothesis for
each input utterance. The hypothesis alignment is a sequence
of discrete tokens corresponding to the inferred labels of each
frame; each label can be <blank> to indicate non-emission.

The ﬁrst-pass hypothesis alignment is fed to the Align-
Reﬁne decoder for S steps of iterative reﬁnement. Each reﬁne-
ment step takes in an initial alignment and outputs a complete
updated alignment; all steps share the same model parameters.
Similarly to the decoder module in attention-based model [17],
the Align-Reﬁne decoder consists of a series of transformer lay-
ers to integrate the text-side information from alignment and
the audio-side information from encoder output. A schematic
diagram of the transformer architecture of [9] is provided in
Figure 1. On the text side (”Alignment self-atten”), each trans-
former layer performs a self-attention for the text features, and
use the result as query to perform cross-attention on audio fea-
tures, whose result is in turn used as text-side features for the

 
 
 
 
 
 
cates label emission without advancing in time. By counting the
number of previous blanks, we determine the ”time”, or equiv-
alently the audio frame index, when each token is emitted. For
the above example, tokens are output at frame indices (0-based)

0

1

1

2

3

4

4

4

where wor and ld are both emitted at audio frame 4 as allowed
by RNN-T. We use these timestamp information to construct
”local” cross-attention, so that a token emitted at audio frame
index t attends to a window of audio frames around t.

Third, we propose a novel decoder architecture, shown in
Figure 2, to utilize additional audio self-attention like cascaded
encoder but without incurring further delay. For each layer,
besides the alignment self-attention and time-aligned cross-
attention, we compute in parallel an audio self-attention with
the same amount of right context in ”time” (audio frame index).
While the cross-attention output is passed to the next layer as
alignment features, the audio self-attention output is passed to
the next layer as audio features. In such a way, attention opera-
tions are synchronized according to the audio time, and delays
from alignment side and audio side do not add up. Speciﬁcally,
in the schematic diagram of Figure 2, we have L = 3 self-
attention operations for both alignments and audio, but the ef-
fective depth of the architecture L + 1 = 4 for determining
model delay,1 versus 2L if they are stacked as in Figure 1.

2.3. Discriminative training

Figure 1: Transformer architecture of ofﬂine Align-Reﬁne [9].

next layer. A ﬁnal softmax layer is used on top of the last trans-
former layer output to predict labels and <blank>’s. On the
audio side (”Audio self-atten”), the authors propose to use ad-
ditional non-causal cascaded encoder on top of the causal en-
coder of ﬁrst-pass RNN-T, to extract audio features of rich right
context; this architectural design indeed signiﬁcantly boost the
deliberation accuracy. Since the model of [9] works in the of-
ﬂine mode, the alignment self-attentions use full context, and
each transformer layer attends to the cascaded encoder output
with full context. All layers are trained jointly using the CTC
loss [14], which marginalizes alignments between CTC predic-
tions and ground truth label sequences.

The CTC greedy alignment, obtained by picking the most
probably token for each frame in parallel, is used as the out-
put alignment and fed to the next reﬁnement step. The greedy
alignment after all S steps is collapsed (by removing repeti-
tions and then <blank>’s) into a label sequence as the ﬁnal
decoding result. The overall maximum likelihood estimation
(MLE) loss is the averaged CTC losses at all steps: for input ut-
terance X with label sequence y, we minimize (cid:96)S
MLE(X, y) :=
ctc(y|X) where log P i
− 1
ctc(y|X) is the full-sum
S
conditional log-probability under the CTC model at step i.

i=1 log P i

(cid:80)S

2.2. Streaming Align-Reﬁne
We make architectural changes to convert Align-Reﬁne to be
streaming compatible. First, we ensure the alignment self-
attention is ”local”, so that the attention context vector of each
frame only depends on a local window around itself, i.e., repre-
sentation of each token is computed by attending to a number of
previous tokens and a small number of future tokens. The total
amount of right context accumulates with the depth of the ar-
chitecture and becomes its model delay, e.g., if we have L = 6
layers with per-layer right context C = 2, the model waits for
L × C = 12 future frames to output for the current frame.

Second, we ensure the cross-attention between alignment
audio features is ”local”, so that the context vector of alignment
frame (acting as the ”query”) only depends a window of au-
dio feature frames (acting as the ”key” and ”value”). Note that
owing to the RNN-T alignment topology [12], the alignment
sequence and audio sequence are of different lengths. As an
example, assume the input utterance has transcription ”hello
world” which is decomposed into 3 wordpieces { hello,
wor, ld}, and assume the encoder output consists of 5 audio
frames. Then a plausible RNN-T alignment is

<b> hello <b> <b> <b> wor ld <b>

It is known in the literature that, after initial training of an ASR
model with the MLE criterion, ﬁnetuning it with a loss closer
to the ﬁnal evaluation criterion (i.e., WER) often yields further
accuracy gain [18, 19, 16]. In this work, we perform discrimi-
native training of Align-Reﬁne using the minimum word error
rate (MWER) criterion. For an input utterance X, we forward
the Align-Reﬁne model to perform S(cid:48) reﬁnement steps, with S(cid:48)
potentially different from the S used in initial MLE training.
And at the end of S(cid:48) reﬁnement steps, we perform CTC beam
search instead of greedy decoding, to output K > 1 hypotheses
denoted as ˆy1, . . . , ˆyK . We then compute the log-probabilities
of the hypotheses under our model, deﬁned as
(cid:88)S(cid:48)
i=1

log P (ˆyk|X) =

k = 1, . . . , K.

ctc(ˆyk|X),

log P i

1
S(cid:48)

We make the approximation that probability over possible label
sequences concentrate in the top-K space, to compute

Pk =

P (ˆyk|X)
y(cid:48) P (y(cid:48)|X)

(cid:80)

≈

P (ˆyk|X)
k=1 P (ˆyk|X)

(cid:80)K

,

and subsequently the MWER loss

(cid:96)S(cid:48)
MWER (X, y, {ˆy1, · · · , ˆyK }) :=

(cid:88)K

k=1

Pk · NWE(ˆyi, y)

where NWE(ˆyi, y) measures the number of word errors be-
tween hypothesis ˆyi and ground truth y. We use superscript
S(cid:48) to signify that hypotheses are obtained after S(cid:48) reﬁnement
steps. As is common in the literature, for better learning stabil-
ity, we minimize a composite loss

MWER (X, y, {ˆy1, · · · , ˆyK }) + γ · (cid:96)S(cid:48)
(cid:96)S(cid:48)

MLE(X, y)

over the training set in the discriminative training phase, and we
ﬁx γ = 0.005 in this work. Note that while we use beam search
in generating multiple hypotheses for MWER training, we still
perform greedy decoding for ﬁnal inference.

where <b> denotes <blank>. Each <blank> token advances
the audio frame index by 1 whereas each non-blank token indi-

1We could remove audio self-attention at the bottom to make the

effective depth L instead of L + 1, though we have not in this paper.

Alignment self-attenAudio self-attenCross-atten to cascade encoder outputCausal enc. outputSoftmax predictionFigure 2: Left: Transformer architecture of streaming Align-Reﬁne where audio self-attention is synchronized with alignment self-
attention. Right: The attention mask with shape 8 × 7 shared by all time-aligned cross-attention operations in the left ﬁgure. Black
indicates non-zero attention weights. Highlighted row indicates that frame 4 of the alignment feature sequence attends to a window
around frame 3 of the audio feature sequence, with a left context of 2 frames and a right context of 1 frame.

3. Experiments on voice search datasets

In this section, we follow the experimental setup of [9] on
voice search datasets so that our results are directly com-
parable. The training set includes anonymized and hand-
transcribed multi-domain audio data [20] and has gone through
multi-condition training (MTR [21]) and random 8kHz down-
sampling [22] as augmentation. We use a development set of
12K anonymized and hand-transcribed utterances that are rep-
resentative of Google’s Voice Search trafﬁc, with an average
duration of 5.5 seconds; this set is denoted by VS. Final eval-
uation is performed on ﬁve test sets. The ﬁrst set is the side-
by-side losses test (SXS), containing 1K utterances where the
quality of the E2E model transcription has more errors than a
state-of-the-art conventional model [23]. The rest four are TTS
generated test sets containing rare proper nouns (RPN) which
appear less than 5 times in the training set. These sets cover
the Maps, News, Play, and QSession domains and are denoted
RPN-M, RPN-N, RPN-P, and RPN-Q respectively, each con-
taining 10K utterances.

3.1. Cascaded RNN-T as ﬁrst-pass model

The ﬁrst-pass model for generating the initial hypothesis is a
cascaded model designed for on-device usage [24], consisting
of two conformer encoders and a embedding-based decoder.
The ﬁrst encoder is causal and is used by the decoder to gener-
ate partial results in streaming mode, while the second encoder
sits on top of the causal encoder with a right-context of 0.9s
and is used for generating more accurate ﬁnal hypotheses. Note
that [9] used the causal pass of this model, with a total of 56M
weight parameters, to generate initial hypotheses. While we
mostly focus on the same setup as it better simulates the stream-
ing usage of Align-Reﬁne, we also provide results for deliber-
ating the non-causal pass which has 155M weight parameters.
We freeze the ﬁrst-pass model for deliberation training. The
MLE training phase takes 800K steps, with a global batch size
of 4096 utterances. In the MWER training phase, batch size is
reduced to 2048 and we evaluate models at 25K steps.

3.2. Streaming Align-Reﬁne for causal RNN-T

We use a model architecture similar to that of [9] for Align-
Reﬁne: the transformer decoder consists of L = 6 transformer
layers with attention dimension 640 and 8 attention heads, and
we perform S = 3 reﬁnement steps during MLE training. We

Table 1: VS WERs (%) of Align-Reﬁne for different amounts
of model delays, with up to three reﬁnement steps during in-
ference. The ﬁrst pass model is causal RNN-T of 56M weight
parameters, and has a WER of 7.8% on VS.

per-step model
delay (secs)

Inference reﬁnement step
2

3

7.5
7.1
6.6
6.1

0.00
0.36
0.72
1.80

1
Alignment self-attention only
7.5
7.6
7.2
7.3
6.6
6.9
6.2
6.5
+ Audio self-attention
7.3
7.5
6.4
6.6
6.0
6.3
5.7
6.1
+ Continue with discriminative training
7.3
6.5
6.2
5.9

0.00
0.42
0.84
2.10

0.00
0.42
0.84
2.10

7.1
6.2
5.9
5.5

7.3
6.3
6.0
5.7

7.1
6.2
5.9
5.5

achieve different amount of model delay by varying the number
of right-context frames C at each layer, and thus the model de-
lay per reﬁnement step is L × C × f where f is the decoder
frame size (60ms in this section) without audio self-attention,
and (L + 1) × C × f with audio self-attention.

We set the beam size to 4 for the ﬁrst-pass causal RNN-
T model during both training and inference. We provide
the WERs of Align-Reﬁne using only alignment self-attention
(with time-aligned cross-attention performed on causal encoder
outputs) in the top section of Table 1. While Align-Reﬁne
barely improves over the ﬁrst pass when not using any right
context (i.e., 0.0 sec of model delay), the accuracy improves
sharply as the amount of right context increases. We then in-
clude audio self-attention in the model architecture, which leads
to larger model size but not much increase in delay, and results
of these models are given in the middle section of Table 1. Ob-
serve that audio self-attention consistently reduces WERs for all
model delays, implying that right context in the audio modality
is complementary to right context in the text modality.

We then ﬁnetune the models with audio self-attention, us-

Audio self-attenTime-aligned cross-attenTime-aligned cross-attenTime-aligned cross-attenAlignment self-attenCausal enc. outputAlignment time axisAudio time axisTable 2: Test WERs (%) on voice search datasets for causal ﬁrst-pass (top section) and non-causal ﬁrst-pass (bottom section).

Additional Total model

WERs (%)

Method

Causal RNN-T
Attention seq2seq Delib. [25]
Ofﬂine Align-Reﬁne [9]
Streaming Align-Reﬁne

weights
0
48M
55M
70M

Non-causal RNN-T
Attention seq2seq Delib. [25]
Stream Align-Reﬁne

99M
149M
169M

delay (secs) VS
7.8
6.0
5.7
7.1
6.2
5.9
5.5
5.2
4.9
4.9

0.00
∞
∞
0.00
0.84
1.68
4.20
0.90
∞
5.10

SXS RPNM RPNN RPNP RPNQ
25.6
11.4
37.5
22.2
10.2
34.3
23.5
10.0
32.0
24.3
11.6
35.0
22.7
10.5
32.6
22.3
10.2
31.1
21.8
9.9
30.1
20.2
9.0
27.6
18.8
7.4
25.2
19.9
8.8
27.3

16.6
13.8
14.6
15.8
15.0
14.5
14.1
12.9
12.4
12.8

40.9
36.2
38.3
39.7
38.5
38.3
37.4
37.8
34.1
37.2

ing the discriminative training procedure discussed in Sec 2.3.
Empirically, we observe no beneﬁt by using more than S(cid:48) = 1
reﬁnement steps in (cid:96)MWER, and we stick to this conﬁguration
here. Results of discriminatively trained models are presented
in the bottom section of Table 1. These models consistently
outperform their MLE-trained counterparts, across all delays.

As noted by [9] for ofﬂine Align-Reﬁne, signiﬁcant WER
gains are achieved in the ﬁrst two reﬁnement steps during infer-
ence. Interestingly, we observe a new type of trade-off here: it
may be as accurate to run a model of smaller delay for two steps,
than to run a model of larger delay for a single step. For exam-
ple, in the bottom section of Table 1, running the discrimina-
tively trained model with 0.42s per-step delay for 2 steps leads
to a WER of 6.2%, the same as running the model with 0.84s
per-step delay for 1 step. Similarly, running the model with
0.84s per-step delay for 2 steps leads to a WER of 5.9%, which
is the same as running the model with 2.1s per-step delay, but
the former as a smaller total delay of 1.68s.

3.3. Align-Reﬁne for non-causal RNN-T
Given the large WER improvements for deliberating a small
ﬁrst-pass, a natural question is whether our method is still use-
ful when the initial hypothesis is generated by a stronger model.
To answer this question, we apply streaming Align-Reﬁne with
2.1s per-step delay to the non-causal RNN-T of 155M param-
eters (causal encoder + non-causal encoder + RNN-T decoder)
and 0.9s model delay. We feed the causal encoder output to
Align-Reﬁne, which has its own audio self-attention. This
model improves the ﬁrst-pass WER of 5.2% to 5.0% in one
step, and to 4.9% in another step, without discriminative train-
ing (which did not help further).

3.4. Comparisons with other methods
We provide test sets WERs of our models with audio self-
attention in Table 2, along with comparisons with a few models.
We take the best streaming from Sec 3.2 and Sec 3.3 and evalu-
ate them with 2 reﬁnement steps. For each method, we provide
the amount of additional weight parameters on top of the 56M
causal RNN-T, as well as the total model delay. In the case of
causal ﬁrst-pass (Table 2 top section), we compare with the best
ofﬂine model from [9], as well as an attention-based seq2seq
deliberation model similar to that of [25]. Observe that with
sufﬁcient right context (between 1.68s to 4.2s delay), stream-
ing Align-Reﬁne performs as well as the ofﬂine counterpart and
outperforms prior methods on most test sets with 4.2s delay.

For the non-causal ﬁrst-pass (Table 2 bottom section), we
compare again with [25], where the attention-based decoder
performs beam search with beam size 8. Streaming Align-

Table 3: Librispeech WERs (%) of non-AR methods.

test other
9.1

test clean
4.0

dev clean
3.6

dev other
Method
First-pass
9.5
Streaming Align-Reﬁne per-step delay and WERs @step 1/2
8.8/8.6
9.1/8.9
8.5/8.2
8.6/8.3
8.3/8.0
8.4/8.0
7.8/7.6
8.0/7.7
11.1
9.0

0.84s
2.10s
4.20s
21.0s
Imputer [7]
Ofﬂine CTC + Align-Reﬁne [8]

3.7/3.6
3.4/3.3
3.3/3.2
3.2/3.0
4.0
3.6

3.5/3.4
3.2/3.1
3.1/3.0
2.9/2.8

Reﬁne performs similarly to this model on VS (mostly contain-
ing frequent words) with a simpler inference procedure, while
attention-based deliberation is more accurate on rare words.

4. Experiments on Librispeech

We perform experiments on Librispeech [26] to compare with
existing non-AR methods. The ﬁrst-pass is a 122M causal
RNN-T with conformer encoder and embedding decoder, and
uses a beam size of 8 for inference. The token set contains 1024
wordpieces. We use the streaming Align-Reﬁne architecture
found on voice search (70M parameters, including audio self-
attention) without further tuning. We have trained four models
of different per-step model delays: 0.84s, 2.1s, 4.2s, and 21.0s,
with the last setup simulating ofﬂine Align-Reﬁne. We do not
ﬁnd discriminative training to be helpful in this setup, and report
WERs from MLE training. We compare our results with those
of ofﬂine non-AR methods [7, 8], as shown in Table 3. Note
that our ﬁrst-pass RNN-T already has WERs similar to the ﬁ-
nal WERs of prior work. Streaming Align-Reﬁne consistently
improves over the strong ﬁrst-pass, and its accuracy steadily in-
creases with model delay. We measure the inference speed of
methods with a single Intel Xeon CPU (@2.20GHz). The real-
time-factor (RFT) of the ﬁrst-pass is 0.123, while the RTF of
streaming Align-Reﬁne is 0.045 per reﬁnement step.

5. Conclusions

We have proposed a streaming non-autoregressive decoding
method for second-pass deliberation. Our method improves
WERs of both small and large streaming RNN-T models with
controllable model delay, and beneﬁts from discriminative
training when the ﬁrst-pass has small capacity. As a future di-
rection, we would like to incorporate large amount of unpaired
data into our model training [5, 27], to better capture label de-
pendency and improve on rare word recognition.

6. References
[1] M. Ghazvininejad, O. Levy, Y. Liu, and L. Zettlemoyer, “Mask-
predict: Parallel decoding of conditional masked language mod-
els,” in Empirical Methods in Natural Language Processing,
2019.

[2] N. Chen, S. Watanabe, J. Villalba, and N. Dehak, “Listen and ﬁll
in the missing letters: Non-autoregressive transformer for speech
recognition,” arXiv preprint arXiv:1911.04908, 2019.

[3] Y. Higuchi, S. Watanabe, N. Chen, T. Ogawa, and T. Kobayashi,
“Mask CTC: Non-autoregressive end-to-end ASR with CTC and
mask predict,” arXiv preprint arXiv:2005.08700, 2020.

[4] Y. Higuchi, H.

and
T. Kobayashi, “Improved Mask-CTC for non-autoregressive end-
to-end ASR,” in ICASSP, 2021.

Inaguma, S. Watanabe, T. Ogawa,

[5] K. Deng, Z. Yang, S. Watanabe, Y. Higuchi, G. Cheng, and
P. Zhang, “Improving non-autoregressive end-to-end speech
recognition with pre-trained acoustic and language models,” in
ICASSP, 2022.

[20] A. Narayanan, R. Prabhavalkar, C.-C. Chiu, D. Rybach,
T. Sainath, and T. Strohman, “Recognizing long-form speech us-
ing streaming end-to-end models,” in ASRU, 2019.

[21] C. Kim, A. Misra, K. Chin, T. Hughes, A. Narayanan, T. Sainath,
and M. Bacchiani, “Generation of large-scale simulated utterances
in virtual rooms to train deep-neural networks for far-ﬁeld speech
recognition in google home,” in Interspeech, 2017.

[22] J. Li, D. Yu, J.-T. Huang, and Y. Gong, “Improving wideband
speech recognition using mixed-bandwidth training data in CD-
DNN-HMM,” in IEEE Spoken Language Technology Workshop
(SLT), 2012.

[23] G. Pundak and T. Sainath, “Lower frame rate neural network

acoustic models,” in Interspeech, 2016.

[24] T. Sainath, Y. He, A. Narayanan, R. Botros, W. Wang, D. Qiu,
C. cheng Chiu, R. Prabhavalkar, A. Gruenstein, A. Gulati, B. Li,
D. Rybach, E. Guzman, I. McGraw, J. Qin, K. Choromanski,
Q. Liang, R. David, R. Pang, S. Chang, T. Strohman, W. R.
Huang, W. Han, Y. Wu, and Y. Zhang, “Improving the latency
and quality of cascaded encoders,” in ICASSP, 2022.

[6] T. Wang, Y. Fujita, X. Chang, and S. Watanabe, “Streaming end-
to-end ASR based on blockwise non-autoregressive models,” In-
terspeech, 2021.

[25] K. Hu, R. Pang, T. Sainath, and T. Strohman, “Transformer based
deliberation for two-pass speech recognition,” in IEEE Spoken
Language Technology Workshop (SLT), 2021.

[26] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Lib-
rispeech: An ASR corpus based on public domain audio books,”
in ICASSP, 2015.

[27] K. Hu, T. Sainath, Y. He, R. Prabhavalkar, T. Strohman, S. Ma-
vandadi, and W. Wang, “Improving deliberation by text-only and
semi-supervised training,” In submission, 2022.

[7] W. Chan, C. Saharia, G. Hinton, M. Norouzi,

Jaitly,

N.
“Imputer:
tion and dynamic programming,”
ference on Machine Learning, 2020.
https://proceedings.mlr.press/v119/chan20b.html

Sequence modelling via

and
imputa-
in International Con-
[Online]. Available:

[8] E. Chi, J. Salazar, and K. Kirchhoff, “Align-reﬁne: Non-
autoregressive speech recognition via iterative realignment,” in
Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies
(NAACL), 2020.

[9] W. Wang, K. Hu, and T. Sainath, “Deliberation of streaming rnn-
transducer by non-autoregressive decoding,” in ICASSP, 2022.

[10] Y. Xia, F. Tian, L. Wu, J. Lin, T. Qin, N. Yu, and T.-Y. Liu, “De-
liberation networks: Sequence generation beyond one-pass de-
coding,” in Advances in Neural Information Processing Systems,
2017.

[11] K. Hu, T. Sainath, R. Pang, and R. Prabhavalkar, “Deliberation
model based two-pass end-to-end speech recognition,” in ICASSP,
2020.

[12] A. Graves, “Sequence transduction with recurrent neural net-

works,” in ICML Workshop on Representation Learning, 2012.

[13] Y. He, T. Sainath, R. Prabhavalkar, I. McGraw, and et al.,
“Streaming end-to-end speech recognition for mobile devices,” in
ICASSP, 2019.

[14] A. Graves, S. Fern´andez, F. Gomez, and J. Schmidhuber, “Con-
nectionist temporal classiﬁcation: Labelling unsegmented se-
quence data with recurrent neural networks,” in International
Conference on Machine learning, 2006.

[15] Y. Fujita, T. Wang, S. Watanabe, and M. Omachi, “Toward stream-
ing asr with non-autoregressive insertion-based model,” in Inter-
speech, 2021.

[16] R. Prabhavalkar, T. Sainath, Y. Wu, P. Nguyen, Z. Chen, C.-C.
Chiu, and A. Kannan, “Minimum word error rate training for
attention-based sequence-to-sequence models,” in ICASSP, 2018.

[17] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, L. u. Kaiser, and I. Polosukhin, “Attention is all you
need,” in Advances in Neural Information Processing Systems,
2017.

[18] K. Vesel`y, A. Ghoshal, L. Burget, and D. Povey, “Sequence-
discriminative training of deep neural networks.” in Interspeech,
2013.

[19] M. Shannon, “Optimizing expected word error rate via sampling
for speech recognition,” arXiv preprint arXiv:1706.02776, 2017.

