Occlusion Guided Self-supervised Scene Flow Estimation on 3D Point Clouds

Bojun Ouyang
Tel Aviv University
bojungouyang@mail.tau.ac.il

Dan Raviv
Tel Aviv University
darav@tauex.tau.ac.il

1
2
0
2

t
c
O
7
1

]

V
C
.
s
c
[

2
v
4
2
7
4
0
.
4
0
1
2
:
v
i
X
r
a

Abstract

Understanding the ﬂow in 3D space of sparsely sam-
pled points between two consecutive time frames is the core
stone of modern geometric-driven systems such as VR/AR,
Robotics, and Autonomous driving. The lack of real, non-
simulated, labeled data for this task emphasizes the impor-
tance of self- or un-supervised deep architectures. This
work presents a new self-supervised training method and
an architecture for the 3D scene ﬂow estimation under oc-
clusions. Here we show that smart multi-layer fusion be-
tween ﬂow prediction and occlusion detection outperforms
traditional architectures by a large margin for occluded and
non-occluded scenarios. We report state-of-the-art results
on Flyingthings3D and KITTI datasets for both the super-
vised and self-supervised training. 1 2

1. Introduction

Due to the development of autonomous driving, robotic
manufacturing, and virtual and augmented technologies,
understanding the motion in the dynamic scene becomes
important and critical in many backbones [28, 6]. Unlike
Optical Flow [10], where we search for the projected 2D
motion in the image, in Scene Flow [40], we wish to ﬁnd
also the ﬂow along the depth dimension. Traditionally, the
scene ﬂow estimation was performed on the stereo [3, 14]
or from RGB-D [11, 13] sensors for indoor environments
and used Light Detecting and Ranging (LiDAR) sensors in
the outdoor scenes.

Switching from 2D to 3D introduces interesting chal-
lenges. While RGB images contain color information of
the scene and are provided as a dense regular grid, the point
clouds carry the geometric information and are presented as
a sparse unordered set of points in space, which forces us
to switch from traditional image-based algorithms to graph
models. Axiomatic methods, such as [5, 8], found the rigid
alignment between the point clouds by solving an energy

1Our code will be publicly available upon publication.
2https://github.com/BillOuyang/3D-OGFlow.git

minimization problem. Later, [2] relaxed the rigid assump-
tions, but their optimization problem is hard to solve.

Moving from Axiomatic models towards Learnable ar-
chitectures [24, 12, 32, 45], we have seen a large boost in
performance and running time in supervised architectures.
Due to the lack of the annotated labels, there is a demand for
self-supervised training methods for the scene ﬂow estima-
tion on point clouds. Among popular methods one can ﬁnd,
[45, 30] suggested minimizing the nearest neighbor dis-
tances between the target and the warped source according
to the estimated ﬂow, [48] proposed self-supervised learn-
ing based on the adversarial metric learning techniques.

When we estimate the ﬂow, we always encounter occlu-
sion, where some regions in one scene might not exist in
the other. This is mainly caused by the motion in the scene,
so that some objects may enter or leave the visible zone of
the camera. The main difﬁculty in ﬂow estimation under
occlusion relates to the connection between the ﬂow cor-
relation and the magnitude of the occlusion. On the one
hand, given the occluded parts, we can optimize for the best
ﬂow, and on the other hand, given the best ﬂow, we can con-
clude which part is occluded. In practice, optimizing those
two unknowns in parallel is non-trivial in a self-supervised
scheme due to possible collapse towards an all-occluded so-
lution.

Although we already have extensive studies of occlusion
in ﬂow estimation in 2D images [16, 47, 36], it is still an
open challenge for 3D point clouds that merely no one has
studied. Due to the difference in the information carried
by these two data structures, directly utilizing those image-
based occlusion handling techniques to the point cloud data
does not provide the boost we need. [31] was the ﬁrst to esti-
mate the occlusion in point clouds, but their training method
requires the ground truth occlusion label, which is almost
impossible to acquire in the real scenario.

In this paper, we focus on the scene ﬂow estimation prob-
lem on point clouds with occlusion. We present a self-
supervised architecture called 3D-OGFlow that merges two
networks across all layers, where one learns the ﬂow and
the other learns the occlusions. We further present a novel
Cost Volume layer that can encode the similarity between

 
 
 
 
 
 
the point clouds with occlusion handling. We show state-
of-the-art performance on Flyingthings3D and KITTI scene
ﬂow 2015 benchmark for both occluded and non-occluded
versions.

2. Related Work

Scene Flow Estimation on Point Clouds. Due to the in-
creasing popularity of range data and the development of
the 3D deep learning [25, 1, 33, 46, 7, 34, 41, 44], many
works such as [38, 9, 4, 35, 39, 42, 12, 20] suggested di-
rectly estimating the scene ﬂow on the point clouds obtained
from LiDAR scans. Based on the hierarchical architecture
of [34], FlowNet3D [24] was the ﬁrst to propose the ﬂow
embedding layer which can aggregate the features across
consecutive frames. Inspired by the feature pyramid struc-
ture of [37], PointPWC [45] suggested estimating the scene
ﬂow on multiple levels. They also introduced a novel Cost
Volume that can aggregate the Matching Cost between the
point clouds in a patch-to-patch manner using a learnable
weighted sum. Later, FLOT [32] proposed a network that
learns the correlation in an all-to-all manner based on the
graph matching and optimal transport.
Occlusion in Flow Estimation.
In the optical ﬂow or
scene ﬂow, handling the occlusion in images is impor-
tant as it can highly inﬂuence the estimation accuracy.
Many works in optical ﬂow [47, 16, 19], scene ﬂow [36],
or both [18], suggested using a CNN to learn the occlu-
sion, which is further used to reﬁne the predicted ﬂow.
Other works [15, 43, 27, 22, 23, 17] suggested estimating
the occlusion using forward-backward consistency check.
In [47, 36], they excluded the occluded regions before the
correlation/Cost Volume construction, which signiﬁcantly
improved the performance. When it comes to the point
cloud data, [31] recently suggested excluding the computed
Cost Volume for the occluded points, but this can harm the
ﬂow estimation accuracy for the occluded regions.

Unlike [47, 36, 31], our method does not exclude the oc-
cluded regions during the correlation construction as they
contain useful geometric information.
Instead, We use a
separate construction of the Cost Volume for the occluded
and non-occluded regions according to their properties, and
then we aggregate the two in an occlusion-weighted man-
ner.
Self-supervised Learning.
In the case of 2D images,
minimizing the photometric loss between reference and
warped target images is common in unsupervised learn-
ing. [27, 22, 23, 21, 17] suggested excluding the occluded
pixels in the photometric loss, which makes a lot more
sense. [22, 23] learned the optical ﬂow for the occluded re-
gions using data augmentation, while [21] suggested mak-
ing another forward inference on the augmented data as
a regularization. When it comes to scene ﬂow estima-
tion on point clouds, [30] take the supervised pretrained

FlowNet3D [24] as a backbone and ﬁne-tune it on the unan-
notated KITTI using the self-supervised nearest neighbor
loss and cyclic consistency. [45] use the Chamfer distance
together with the smoothness and Laplacian regulariza-
tion as their self-supervised training losses. Recently, [48]
proposed a self-supervised training scheme based on met-
ric learning. They use triplet loss and cyclic consistency
and showed a remarkable results on the occluded Flyingth-
ings3D [26] and KITTI [28, 29].

In our work, we suggest a novel self-supervised train-
ing scheme that can learn the scene ﬂow for the occluded
scene. Our strategy shows a signiﬁcant improvement in the
occluded datasets compared to the previous state-of-the-art.

3. Problem Deﬁnition

Consider the sampling of a 3D scene at two different
time frames, denote S = {si ∈ R3}n1
i=1 the source sam-
pling with n1 points, and T = {tj ∈ R3}n2
j=1 the target
sampling with n2 points. In addition to the si, tj that de-
scribe the spatial coordinate, each source and target point
can also have an associated feature such as surface normal,
which is denoted by ci ∈ Rd and gj ∈ Rd respectively.

For the scene ﬂow estimation on point clouds, the goal is
to ﬁnd a 3D non-rigid ﬂow f (si) ∈ R3 for every source
point si towards T , such that the warped source Sw =
{si + f (si)}n1
i=1 has the best alignment with T . Due to the
possible occlusion and the difference in the sampling, some
points in S might not exit in T . For this reason, we learn a
ﬂow representation for each si ∈ S towards the T instead
of the correspondence between S and T .

We also want to ﬁnd the occlusion label for every source
point si with respect to T , denoted by O(si). O(si) = 1
when si is non-occluded. When O(si) = 0, it means si
is occluded, in other words it does not exist in the target
frame.

4. Architecture

The architecture of 3D-OGFlow is shown in Fig.1. The
inputs of the model are the source and the target point
clouds sampled at different time frames. The outputs are
the predicted scene ﬂow f (si) and occlusion label O(si)
for si ∈ S with respect to T . We adopt the 4-level feature
pyramid network in [45], where we ﬁrst generate the down-
sampled source (Sl) and target (T l) point clouds for each
pyramid level l using Farthest Point Sampling (FPS) [34].
Then, we use PointConv [44] to perform the convolution on
the point clouds, which generates and increases the depth
of the encoded features for the downsampled point clouds
along the pyramid. At each pyramid level l, we ﬁrst per-
form a backward warping from the target point cloud to-
wards the source by using the upsampled ﬂow from pyra-
mid level l + 1. The warping brings the two point clouds

Figure 1: 3D-OGFlow.On the left, we show our model’s general structure. We use a feature pyramid structure to perform the downsampling
of the point clouds and the feature encoding. On the right, we show the structure at each pyramid level. We ﬁrst warp the target point cloud
towards the source using the upsampled ﬂow from the previous level. Then we construct our Occlusion-weighted Cost Volume using the
predicted occlusion. Finally, we estimate the residual ﬂow and add it to the upsampled ﬂow to generate the ﬁner scene ﬂow.

closer to each other such that the tracking of large motion
can be more accurate. Second, we estimate the occlusion
label for each source point using the occlusion predictor.
Third, we construct our occlusion-weighted Cost Volume,
which encodes the ﬂow information for both the occluded
and non-occluded points in the source. Finally, we use a
similar predictor layer as in [45] to predict the residual ﬂow
for each source point, and the ﬁner scene ﬂow is the addi-
tion of the residual and the upsampled ﬂow. The predicted
scene ﬂow and occlusion mask at pyramid level l are further
used in pyramid level (l − 1) above it.

In this section, we mainly discuss the novel components
in our model: Occlusion predictor and Occlusion-weighted
Cost Volume.
Implementation details and the schematic
plots for all the components can be found in the supple-
mentary.

4.1. Occlusion Predictor

Since the occluded regions usually produce misleading
information for the ﬂow estimation, they need special treat-
ment in the architecture and training loss.
In our work,
We use a small neural network to estimate the occlusion
label for each point in the source. The occlusion predictor’s
inputs are the source point cloud, the warped target point
cloud, and the upsampled occlusion label from the previous
pyramid level. We use several 1×1 convolutions to encode
the similarity between each source point and its neighboring
target points. Then we use a Max-pooling followed by MLP
to generate the ﬁnal occlusion label based on the encoded
similarity between the point clouds. We also use a Sigmoid
activation layer at the end to ensure the output O(si) to be
an occlusion probability with a value in the range [0,1] for
each point in the source. Details and schematic plots of this
layer can be found in the supplementary.

4.2. Occlusion-weighted Cost Volume

Cost Volume is a standard concept in stereo matching,
it encodes the similarity and correlation between the con-
secutive time frames. [45] was the ﬁrst to introduce Cost
Volume’s concept for the scene ﬂow estimation on point
clouds. However, their design does not consider the occlu-
sion issues, and their model’s performance on the occluded
scene can signiﬁcantly decrease. [31] proposed an occlu-
sion masking operation such that their Cost Volume for the
occluded points becomes 0, but this can be harmful for the
ﬂow prediction for the occluded regions.

In our Occlusion-weighted Cost Volume layer, we ﬁrst
construct the Matching Cost, it encodes the point-wise cor-
relation between a source point si and a target point tj. By
using the source point feature ci and target point feature gj,
we calculate the Matching Cost between them by the fol-
lowing:

cost(si, tj) = h(ci, gj, tj − si)

(1)

Where in h(·) we ﬁrst concatenate all the inputs along the
feature dimension, then we use several 1×1 convolutions to
process the data.

After the Matching Cost construction, we construct the
Cost Volume for si by aggregating their Matching Cost with
tj using Max-pooling. In order to avoid the expensive com-
putation and high memory consumption, we only apply the
aggregation among the K nearest neighbor (k-NN) in the
warped target Tw around si (NT w(si)):

CVcross(si) = M AX

{cost(si, tj)}

tj ∈NT w(si)

(2)

For the occluded points in the source, which do not exist
in the target frame, the CV construction in Eq.2 based on
the pair-wise cross-correlation might not be accurate. For

FinerFlowFinerMaskCoarserFlowCoarserMaskTargetScene FlowOcclusion MaskSourceZoomeddetailWarpingOcclusionPredictorOcc-weightedCost VolumeResidual flowPredictorUpsampleUpsampleCoarser FlowCoarser MaskFiner FlowFiner MaskFigure 2: Self-supervised Learning. To learn the occlusion label, we create ˜T from S by applying a random translation ˜fgt and remove
the k-NN of randomly selected points. The removed regions can be considered as occlusions and we obtain the occlusion label ˜Ogt(si) for
si ∈ S with respect to ˜T . We make a forward and backward inference on (S, T ) using our model to construct the non-occluded Chamfer
loss and regularization. We make a third inference on (S, ˜T ) and use ˜fgt/ ˜Ogt as the ground truth supervision for the occlusion learning.

such a point, its scene ﬂow should be guided by its clos-
est non-occluded points and consistent with its local nearest
neighbors. By this motivation, we propose to construct a
self Cost Volume by applying another self-aggregation:

CVself (si) = M AX

{CVcross(sk)}

sk∈NS (si)

(3)

Our ﬁnal Cost Volume for each source point is the sum of
the CVcross and CVself weighted by the predicted occlu-
sion label:

CV (si) = O(si)CVcross(si)+(1−O(si))CVself (si) (4)

where O(si) is the predicted occlusion label for the si. The
schematic plot can be found in the supplementary.

Notice that during the supervised training for the scene
ﬂow, to make the accurate ﬂow prediction, the model needs
to force the CVcross(·) term in Eq. 4 to have a higher con-
tribution for the non-occluded point si. While for the oc-
cluded point, the model needs to force the CVself (·) term
to have a higher contribution. Since the predicted occlu-
sion O(si) controls this weighting, it means our occlusion
label is learned in a self-supervised manner without explicit
occlusion supervision during the supervised training of the
scene ﬂow.

5. Self-supervised Training

Since the acquisition of the ground truth annotation is
difﬁcult or even impossible in many real-world scenarios,
we present a self-supervised training method that does not
require any ground truth scene ﬂow or occlusion label.
Most of the previous self-supervised methods [45, 20] use
Chamfer distance loss with some regularization to move the

source smoothly towards the target. Although these losses
work perfectly on the non-occluded data, they often lead to
an incorrect prediction of the ﬂow when the scene contains
occluded regions. This is because those methods do not
exclude the occluded regions in the Chamfer distance cal-
culation, so the occluded regions in one point cloud might
map to the non-occluded regions in the other. To discard
the occluded points in the loss function, we need to know
each source point’s occlusion label, but this often requires
an accurate scene ﬂow prediction, which leads to a paradox.
In our work, we suggest using another synthetic target point
cloud to train the occlusion predictor.

For each source point cloud S in the dataset, we gener-
ate a synthetic target point cloud ˜T from it by ﬁrst apply-
ing a randomly generated translation to every point in the
source. Then, we randomly choose several center points
in the translated point cloud and remove their k-NN points
from the translated point cloud, so that these removed re-
gions can be considered as occluded. Since we generate the
˜T from S by ourselves, we know the ground truth scene
ﬂow ˜fgt(si) and the occlusion mask ˜Ogt(si) from S to ˜T .
By training with this pair of point clouds (S, ˜T ) using the
supervised scene ﬂow loss and occlusion loss, our model
can learn to estimate the occlusion. Since we can never
generate a real scene ﬂow, the main goal of using (S, ˜T )
is to learn the occlusion but not the scene ﬂow. Due to the
consideration of the expense in computation, we only use a
simple rigid translation as our ˜fgt when constructing the ˜T
in our work.

We also need to train our model with the regular pair of
the point clouds (S, T ) using the Non-occluded Chamfer
distance with regularization to learn the scene ﬂow. We for-
mulate the overall self-supervised loss in Sec. 6.1, and the
general idea of this approach is shown in Fig. 2.

TranslationandOcclusion creationSyntheticOcclusion3D-OGFlowandScene FlowOcclusion3D-OGFlowNon-occluded Chamfer+RegularizationFlow loss+Occlusion lossOcclusion3D-OGFlowScene FlowOcclusionSharedWeightlearning for Scene flow learning for Occlusion Dataset

Method

Sup.

EPEf ull↓

EPE↓

ACC05↑ ACC10↑ Outliers↓

Flyingthings3D

KITTI

FLOT(K=1) [32]
HPLFlowNet [12]
FlowNet3D [24]
OGSFNet [31]
PointPWC-Net [45]
Ours

ICP [5, 8]
PointPWC-Net [45]
Ours

FLOT(K=1) [32]
HPLFlowNet [12]
FlowNet3D [24]
OGSFNet [31]
PointPWC-Net [45]
Ours

ICP [5, 8]
PointPWC-Net [45]
Ours

Full
Full
Full
Full
Full
Full

Self
Self
Self

Full
Full
Full
Full
Full
Full

Self
Self
Self

PointPWC-Net [45]
Ours

Full+Full ft
Full+Full ft

PointPWC-Net [45]
Ours

Self +Self ft
Self +Self ft

0.2502
0.2012
0.2119
0.1634
0.1953
0.1383

0.5048
0.6579
0.3373

0.1303
0.3430
0.1834
0.0751
0.1180
0.0595

0.3801
0.3373
0.2091

0.0650
0.0249

0.1632
0.0857

0.1530
0.1689
0.1577
0.1217
0.1552
0.1031

0.4848
0.3821
0.2796

-
-
-
-
-
-

-
-
-

-
-

-
-

0.3965
0.2629
0.2286
0.5518
0.4160
0.6376

0.1215
0.0489
0.1232

0.2788
0.1035
0.0980
0.7060
0.4031
0.7755

0.1038
0.0529
0.2107

0.6618
0.9335

0.2117
0.5146

0.6608
0.5745
0.5821
0.7767
0.6990
0.8240

0.2558
0.1936
0.3593

0.6672
0.3867
0.3945
0.8693
0.7573
0.9069

0.2913
0.2125
0.4904

0.8894
0.9721

0.5409
0.8108

0.6625
0.8123
0.8040
0.5180
0.6389
0.4251

0.9441
0.9741
0.9104

0.5299
0.8142
0.7993
0.3277
0.4966
0.2732

0.8307
0.9352
0.7241

0.3569
0.1498

0.6934
0.4724

Table 1: Evaluation on occluded Flyingthings3D and KITTI. For Flyingthings3D, Full / Self means the training is done using
supervised/self-supervised schemes. For KITTI, Full / Self means we evaluate the corresponding trained models from Flyingthings3D
directly on the KITTI without ﬁne-tuning. We also perform 2 kinds of ﬁne-tuning experiments on the bottom: Full+Full ft means we
apply the supervised ﬁne-tuning on the supervised pretrained weight on Flyingthings3D, and similarly for the Self +Self ft. Our model
outperforms the previous methods by a large margin on all kinds of supervision and evaluation metrics.

6. Loss functions

6.1. Self-supervised Loss

Flow/Occlusion loss (synthetic). To train the occlusion
predictor in a self-supervised manner, we create a synthetic
target ˜T as explained in the previous section. Since the oc-
clusion prediction at each pyramid level depends on the up-
sampled scene ﬂow from its previous level, we also need
the ﬂow loss in addition to the occlusion loss. By using the
synthetic ground truth scene ﬂow ˜fgt and occlusion mask
˜Ogt from S to ˜T , we construct the following multi-level
“supervised” losses:

˜Lf (S, ˜T , Θ) =

3
(cid:88)

αl

(cid:88)

l=0

si∈Sl

(cid:107) ˜fgt(si) − ˜f (si)(cid:107)2

(5)

˜Loc(S, ˜T , Θ) =

3
(cid:88)

αl

(cid:88)

(cid:107) ˜Ogt(si) − ˜O(si)(cid:107)

(6)

l=0

si∈Sl
Where S and ˜T are the inputs to the model, Sl is the down-
sampled source point cloud at pyramid level l, Θ is all the

learnable parameters of the model, and ˜f (si) and ˜O(si) are
the predicted ﬂow and occlusion from si to ˜T .
Non-occluded Chamfer Distance. As explained in the pre-
vious section, it is crucial to exclude the occluded region in
the chamfer distance calculation. In order to minimize the
distances between the non-occluded source points and the
non-occluded target points, we use the following loss:

Dl(Sl, T l, Θ) = |Sl|

min
tj ∈T l

(cid:88)

si∈Sl
w

min
si∈Sl
w

+|T l|

(cid:88)

tj ∈T l

(cid:107)si − tj(cid:107)2 · Of (si)

(cid:80)

Of (si)

si∈Sl
w
(cid:107)tj − si(cid:107)2 · Ob(tj)

(cid:80)
tj ∈T

Ob(tj)

Lnch(S, T, Θ) =

3
(cid:88)

l=0

αlDl(Sl, T l, Θ)

(7)

Where Of and Ob are the predicted forward (S→T ) and
backward (T →S) occlusion label, |Sl| and |T l| are the
numbers of points in Sl and T l, Sl
w={si + f (si)|si ∈
Sl}|Sl|
i=1 is the warped source point cloud at each pyramid

Figure 3: Visualization on KITTI. We plot the source (red) and the target (blue) frames from the KITTI on the same 3D space on the left.
On the right, we show the warped source point cloud according to the estimated ﬂow (source+ﬂow) learned from the supervised training
and the target point cloud. We also provide the zoomed view for the circled region to show the alignment better.

level l according to the predicted ﬂow. The backward occlu-
sion Ob(tj) from the target towards the source is obtained
by simply swapping the order of inputs (S, T ). In order to
avoid a degenerated solution where all the points are pre-
dicted as occluded (Of (si) = Ob(tj) = 0), we remove
Of (si) and Ob(tj) from the computational graph during
the backpropagation with SGD. In other words, when we
update the model’s parameters according to the gradient of
non-occluded Chamfer distance, we exclude the parameters
of the occlusion predictor and only update the rest of it.
Smoothness regularization.
In addition to the non-
occluded Chamfer distance, we also need a regularization
to ensure that the predicted scene ﬂow is smooth among
the local neighbourhoods. Since the ﬂow of the occluded
points should be consistent with and guided by the ﬂow of
its nearest neighbors, we do not exclude the occluded re-
gions in this smoothness regularization. Following [45],
our regularization term is deﬁned as:

Rl(Sl, T l, Θ) =

(cid:88)

(cid:88)

si∈Sl

sk∈NS (si)

(cid:107)f (si) − f (sk)(cid:107)1
|NS(si)|

Lreg(S, T, Θ) =

3
(cid:88)

l=0

αlRl(Sl, T l, Θ)

(8)

Where |NS(si)| is the number of points in the neighbour-
hood NS(si).

The overall self-supervised loss is the weighted sum of

these four losses with the scale factor λreg, λf , λoc:

Lself = Lnch + λregLreg + λf ˜Lf + λoc ˜Loc

(9)

6.2. Fully-supervised Loss

We use similar multi-level ﬂow loss in Eq.5 with the
ground truth scene ﬂow for the supervised training. Due to
our occlusion-weighted design in Sec 4.2, the model does

not require explicit occlusion loss for the supervised learn-
ing since it can extract the occlusion information from the
ground truth scene ﬂow. Our supervised scene ﬂow loss is
formulated below:

Lsup(S, T, Θ) =

3
(cid:88)

αl

(cid:88)

l=0

si∈Sl

(cid:107)fgt(si) − f (si)(cid:107)2

(10)

where fgt(si) and f (si) are the ground truth and predicted
scene ﬂow from si ∈ S to T .

7. Experiments

By following the experimental procedure as in [24,
32, 45], we ﬁrst train our model on the occluded Fly-
ingthings3D [26] dataset using the supervised and self-
supervised training schemes (Sec. 7.1). Then, we evalu-
ate the trained models from the two training schemes on
the real LiDAR scans from the occluded KITTI scene ﬂow
2015 [28, 29] with and without ﬁne-tuning (Sec. 7.2). This
kind of evaluation procedure is a common practice in the
previous works since it is difﬁcult to acquire scene ﬂow
from real data and the KITTI dataset is too small for the
training. Notice that the two datasets we are using are pre-
processed by [24], where the scene in both datasets contains
occlusion up to some degree. The Flyingthings3D datasets
contain 20000 pairs of point clouds in the training set and
2000 in the validation set. Each pair in the dataset con-
tains two point clouds representing the sampled 3D syn-
thetic scene at two different time frames, and the scene in
this dataset is highly occluded. The KITTI dataset contains
the LiDAR scans of the real scene with some occlusions,
and it contains 150 pairs of point clouds. We provide more
visualization results and explanations on the two datasets in
the supplementary. In Sec. 7.3, we present several abla-
tion experiments to validate our novel design in the model
and our self-supervised losses. Finally, in Sec. 7.4, to show

our model’s expressiveness, we compare our model with the
previous work on the non-occluded datasets.
Implementation details. Our model uses the Feature Pyra-
mid Network and Upsample layer proposed by [45] and
Implementation de-
uses the Warping layer as in [31].
tails of our architecture can be found in the supplemen-
tary. We use n1 = n2 = 8192 points for each point
cloud in the two datasets when performing all kinds of ex-
periments. The weights {α0, ..., α3} in the loss functions
are {0.02, 0.04, 0.08, 0.16}. For the supervised training on
Flyingthings3D, we use 2 GTX2080Ti GPU with a batch
size of 8. We train it for 120 epochs with an initial learning
rate of 0.001, and we reduce it with a decay rate of 0.85 af-
ter every 10 epochs. We further reduce the decay rate from
0.85 to 0.8 after 75 epochs. For the self-supervised training
on Flyingthings3D, we use 8 GTX2080Ti GPU with a batch
size of 24. In Eq. 9, we choose λf = 0.6 and λoc = 1.0.
We set λreg = 3.0 in the ﬁrst 50 epochs, and then we re-
duce it to 1.0 gradually from 50 to 70 epochs. We train the
model for 150 epochs with an initial learning rate of 0.001
and a decay rate of 0.83 for every 10 epochs. In the ﬁrst 30
epochs, we use Eq. 9 as the loss function. After 30 epochs,
we exclude the synthetic ﬂow loss and only use the rest of
the 3 terms in Eq. 9 for the self-supervised training. The
magnitude of the randomly generated translation ( ˜fgt(si))
is 2 meters.
Metrics. We follow the same evaluation metrics as in [32,
24, 31] to evaluate the performance of the different models:
(cid:13)fgt(si) − f (si)(cid:13)

• EP Ef ull(m): (cid:13)

(cid:13)2 averaged over all

pi ∈ S.
• EP E(m): (cid:13)

occluded pi ∈ S.

(cid:13)fgt(si) − f (si)(cid:13)

(cid:13)2 averaged over all non

• ACC05: percentage of points whose EP E < 0.05m

or relative error< 5%

• ACC10: percentage of points whose EP E < 0.1m or

relative error< 10%

• Outlier: percentage of points whose EP E > 0.3m or

relative error> 10%

7.1. Evaluation on Flyingthings3D

We ﬁrst train our model using the supervised (Sec. 6.2)
and self-supervised (Sec. 6.1) frameworks on the training
set of Flyingthings3D, then we test the corresponding mod-
els on the validation set. The results are reported in Ta-
ble 1. We can see that our method has the best performance
on all metrics. For fully-supervised training, we outper-
form [31] by 15.3% on EPEf ull. Our self-supervised sys-
tem outperforms [45] by 48% on EPEf ull, which clearly
shows the power of our self-supervised training schemes.

Figure 4: Visualization on Flyingthings3D. In a), we plot the
source (red) and the target (blue) of a test sample on the same
3D space. In b) and c), we plot the warped source according to
predicted/ground truth ﬂow (source+ﬂow) and the target on the
top, we can see that warped source aligned to the target. On the
bottom, we show the predicted/ground truth occlusion map, where
black points mean occluded and red points mean non-occluded.

We want to emphasize that the reported numbers for the
PointPWC-Net [45] and HPLFlowNet [12] in Table 1 are
different from the one reported by their own paper, this
is because we evaluate all the models in Table 1 on the
occluded Flyingthings3D and occluded KITTI proposed
by [24], while [45, 12] only evaluate on non-occluded ver-
sion proposed by [12]. The evaluation results on the non-
occluded version of datasets can be found in Table. 3

For the occlusion estimation, our model achieves a
92.3% accuracy on the validation set of Flyingthings3D un-
der the fully-supervised training scheme. When we train
the model using our self-supervised losses, where the oc-
clusion predictor is purely trained from the synthetic oc-
clusion, we can still achieve a 90.9% accuracy. These re-
sults demonstrate the generalization ability of our model to
unseen data with real occlusion under both the supervised
and self-supervised frameworks. Visualizations are shown
in Fig. 4.

7.2. Evaluation on KITTI

In this section, we ﬁrst evaluate the supervised and self-
supervised pretrained models from Flyingthings3D directly
on all the 150 samples from KITTI without ﬁne-tuning.
Then, we perform the ﬁne-tuning experiments on KITTI by
using the pretrained weight from Flyingthings3D. We split
the KITTI into 100 samples of the training set for the ﬁne-
tuning, 50 samples of the test set. The numbers are shown
in Table 1. Since the KITTI does not provide the ground
truth occlusion mask, we cannot evaluate the EPE on this
dataset. Some visualizations are shown in Fig.3.
Generalization results. The direct evaluation results are
shown in the upper part of the KITTI section in Table 1.
For both the supervised and self-supervised frameworks,

a)b) predc) gtCost Volume

EPE↓

CVcross(·)
CVself (·)
O(·)CVcross(·)
Occ weighted

0.1352
0.1324
0.1242
0.1031

(a) Model Design

Chamfer

Reg. Occ (Syn.)

Flow (Syn.)

EPEf ull↓

Regular
Regular

(cid:37)
(cid:33)
Non-occluded (cid:33)
Non-occluded (cid:33)

(cid:37)
(cid:37)
(cid:33)
(cid:33)

(cid:37)
(cid:37)
(cid:37)
(cid:33)

0.9071
0.4696
0.4085
0.3373

(b) Self-supervised losses

Table 2: Ablation Study. In (a), we test different design choices of the Cost Volume, and our occlusion-weighted design gives the best
performance. In (b), we compare the regular and non-occluded Chamfer loss and show each term’s usefulness in our self-supervised losses.

our model has the best generalization ability.
Fine-tuned results. The ﬁne-tuning results are shown in
the lower part of the KITTI section in Table 1. We ﬁrst
perform the supervised ﬁne-tuning (Full ft) using Eq. 10
on the training set of KITTI using the supervised (Full)
pretrained weight on Flyingthings3D. Then, we perform
the self-supervised ﬁne-tuning (Self ft) using Eq. 9 on the
training set of KITTI using the self-supervised (Self ) pre-
trained weight on Flyingthings3D without using any kinds
of ground truth. We compare our results with [45], and we
can see that our supervised/self-supervised ﬁne-tuning can
give much larger improvements in the performance.

7.3. Ablation Study

In Table 2 (a), we perform several ablation experiments
to validate the novel design of our Cost Volume layer. We
train the model with the corresponding design on the Fly-
ingthings3D, and we report their EPE on the validation set.
When we use the cross-correlation term CVcross as our Cost
Volume, we achieve a 0.1352 EPE. If we exclude the oc-
cluded regions in the CVcross by masking with the pre-
dicted occlusion (O(si)·CVcross(si)) as in [31], we achieve
a 8% improvement in EPE. In the last row, our occlusion
weighted design in Eq.4 gives the best results.

In Table 2 (b), we test the performance of our model
trained by different combinations of self-supervised losses
and present the EPEf ull on the validation set of Flyingth-
ings3D. Using the regular Chamfer loss and smoothness
regularization, we get an EPEf ull of 0.4696. When we ex-
clude the occluded regions in the Chamfer loss by using the
predicted occlusion label learned from synthetic occlusion
loss, the performance improves. When we add the synthetic
ﬂow loss, our occlusion learning can be more accurate, and
so is the occlusion elimination in the non-occluded Chamfer
loss. This design gives the best performance.

7.4. Evaluation on non-occluded datasets

Many of the previous works [12, 45] evaluated their
methods only on the non-occluded version of Flyingth-
ings3D and KITTI proposed by [12], where the oc-
cluded regions in the point clouds are removed during pre-
processing, accordingly, we also evaluate 3D-OGFlow on

Datasets

Method

EPEf ull↓ ACC05↑ Outliers↓

Flyingthing3D

KITTI

FlowNet3D [24]
HPLFlowNet [12]
PointPWC-Net [45]
Ours

FlowNet3D [24]
HPLFlowNet [12]
PointPWC-Net [45]
Ours

0.1136
0.0804
0.0588
0.0360

0.1767
0.1169
0.0694
0.0385

0.4125
0.6144
0.7379
0.8790

0.3738
0.4783
0.7281
0.8817

0.6016
0.4287
0.3424
0.1969

0.5271
0.4103
0.2648
0.1754

Table 3: Evaluation on non-occluded datasets. We evaluate
our methods on the non-occluded version of Flyingthings3D and
KITTI used by [12, 45]. Both models are trained using the super-
vised loss.

these non-occluded datasets to show the robustness of our
model. By following the same procedure as in [12, 45], we
train all the models on the non-occluded Flyingthings3D in
a fully-supervised manner and then directly evaluate them
on non-occluded KITTI without any ﬁne-tuning. The re-
sults are shown in Table 3, and we can see that we are also
winning on these non-occluded datasets.

8. Conclusion

In this work, we present a neural network with a novel
occlusion-aware correlation layer for the scene ﬂow estima-
tion on the point clouds. Our occlusion weighted Cost Vol-
ume layer can compute the correlation between the point
clouds for both the occluded and non-occluded regions ac-
cording to their properties. We conduct several ablations to
validate our occlusion weighted design of the Cost Volume.
As a side beneﬁt, by only using the ground truth scene ﬂow
as the supervision, this design can also let us learn the occlu-
sion in a self-supervised manner. We also propose a novel
self-supervised training scheme to learn the scene ﬂow for
the occluded scene efﬁciently. We achieve state-of-the-art
performance on multiple datasets for both the supervised
and self-supervised training schemes.

References

[1] Michael Allen. Voxnet: Reducing latency in high data
rate applications. Wireless Sensor Networks, page 115–158,

2010. 2

[2] B. Amberg, S. Romdhani, and T. Vetter. Optimal step non-
rigid icp algorithms for surface registration. In 2007 IEEE
Conference on Computer Vision and Pattern Recognition,
pages 1–8, 2007. 1

[3] T. Basha, Y. Moses, and N. Kiryati. Multi-view scene ﬂow
estimation: A view centered variational approach. In 2010
IEEE Computer Society Conference on Computer Vision and
Pattern Recognition, pages 1506–1513, 2010. 1

[4] Aseem Behl, Despoina Paschalidou, Simon Donne, and An-
dreas Geiger. Pointﬂownet: Learning representations for
rigid motion estimation from point clouds. 2019 IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR), 2019. 2

[5] Paul J. Besl and Neil D. McKay. Method for registration of
3-D shapes. In Paul S. Schenker, editor, Sensor Fusion IV:
Control Paradigms and Data Structures, volume 1611, pages
586 – 606. International Society for Optics and Photonics,
SPIE, 1992. 1, 5

[6] Nicolas Bonneel, James Tompkin, Kalyan Sunkavalli, De-
qing Sun, Sylvain Paris, and Hanspeter Pﬁster. Blind video
temporal consistency. ACM Transactions on Graphics (Pro-
ceedings of SIGGRAPH Asia 2015), 34(6), 2015. 1

[7] R. Qi Charles, Hao Su, Mo Kaichun, and Leonidas J. Guibas.
Pointnet: Deep learning on point sets for 3d classiﬁcation
and segmentation. 2017 IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR), 2017. 2

[8] Y. Chen and G. Medioni. Object modeling by registration
of multiple range images. Proceedings. 1991 IEEE Interna-
tional Conference on Robotics and Automation. 1, 5

[9] A. Dewan, T. Caselitz, G. D. Tipaldi, and W. Burgard. Rigid
In 2016 IEEE/RSJ Interna-
scene ﬂow for 3d lidar scans.
tional Conference on Intelligent Robots and Systems (IROS),
pages 1765–1770, 2016. 2

[10] A. Dosovitskiy, P. Fischer, E. Ilg, P. H¨ausser, C. Hazirbas, V.
Golkov, P. v. d. Smagt, D. Cremers, and T. Brox. Flownet:
Learning optical ﬂow with convolutional networks. In 2015
IEEE International Conference on Computer Vision (ICCV),
pages 2758–2766, 2015. 1

[11] V. Golyanik, K. Kim, R. Maier, M. Nießner, D. Stricker, and
J. Kautz. Multiframe scene ﬂow with piecewise rigid motion.
In 2017 International Conference on 3D Vision (3DV), pages
273–281, 2017. 1

[12] Xiuye Gu, Yijie Wang, Chongruo Wu, Yong Jae Lee, and
Panqu Wang. Hplﬂownet: Hierarchical permutohedral lat-
tice ﬂownet for scene ﬂow estimation on large-scale point
clouds. 2019 IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), 2019. 1, 2, 5, 7, 8

[13] S. Hadﬁeld and R. Bowden. Kinecting the dots: Particle
based scene ﬂow from depth sensors. In 2011 International
Conference on Computer Vision, pages 2290–2295, 2011. 1
[14] F. Huguet and F. Devernay. A variational method for scene
ﬂow estimation from stereo sequences. In 2007 IEEE 11th
International Conference on Computer Vision, pages 1–7,
2007. 1

[15] Junhwa Hur and Stefan Roth. Mirrorﬂow: Exploiting sym-
metries in joint optical ﬂow and occlusion estimation. 2017

IEEE International Conference on Computer Vision (ICCV),
2017. 2

[16] Junhwa Hur and Stefan Roth. Iterative residual reﬁnement
In CVPR,

for joint optical ﬂow and occlusion estimation.
2019. 1, 2

[17] Junhwa Hur and Stefan Roth. Self-supervised monocular

scene ﬂow estimation. In CVPR, 2020. 2

[18] Eddy Ilg, Tonmoy Saikia, Margret Keuper, and Thomas
Brox. Occlusions, motion and depth boundaries with a
generic network for disparity, optical ﬂow or scene ﬂow es-
timation. Computer Vision – ECCV 2018 Lecture Notes in
Computer Science, page 626–643, 2018. 2

[19] Joel Janai, Fatma G”uney, Anurag Ranjan, Michael J. Black,
and Andreas Geiger. Unsupervised learning of multi-frame
In European Conference on
optical ﬂow with occlusions.
Computer Vision (ECCV), volume Lecture Notes in Com-
puter Science, vol 11220, pages 713–731. Springer, Cham,
Sept. 2018. 2

[20] Yair Kittenplon, Yonina C. Eldar, and Dan Raviv. Flow-
step3d: Model unrolling for self-supervised scene ﬂow es-
In Proceedings of the IEEE/CVF Conference on
timation.
Computer Vision and Pattern Recognition (CVPR), pages
4114–4123, June 2021. 2, 4

[21] Liang Liu, Jiangning Zhang, Ruifei He, Yong Liu, Yabiao
Wang, Ying Tai, Donghao Luo, Chengjie Wang, Jilin Li, and
Feiyue Huang. Learning by analogy: Reliable supervision
from transformations for unsupervised optical ﬂow estima-
tion. In IEEE Conference on Computer Vision and Pattern
Recognition(CVPR), 2020. 2

[22] Pengpeng Liu, Irwin King, Michael R. Lyu, and Jia Xu.
Ddﬂow: Learning optical ﬂow with unlabeled data distilla-
tion. In AAAI, 2019. 2

[23] Pengpeng Liu, Michael R. Lyu, Irwin King, and Jia Xu. Self-
In CVPR,

low: Self-supervised learning of optical ﬂow.
2019. 2

[24] Xingyu Liu, Charles R Qi, and Leonidas J Guibas.
Flownet3d: Learning scene ﬂow in 3d point clouds. CVPR,
2019. 1, 2, 5, 6, 7, 8

[25] Zhijian Liu, Haotian Tang, Yujun Lin, and Song Han. Point-
voxel cnn for efﬁcient 3d deep learning. In Advances in Neu-
ral Information Processing Systems, 2019. 2

[26] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer,
Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A
large dataset to train convolutional networks for disparity,
optical ﬂow, and scene ﬂow estimation. 2016 IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
2016. 2, 6

[27] Simon Meister, Junhwa Hur, and Stefan Roth. UnFlow: Un-
supervised learning of optical ﬂow with a bidirectional cen-
sus loss. In AAAI, New Orleans, Louisiana, Feb. 2018. 2
[28] Moritz Menze and Andreas Geiger. Object scene ﬂow for
autonomous vehicles. 2015 IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2015. 1, 2, 6
[29] M. Menze, C. Heipke, and A. Geiger. Joint 3d estimation
ISPRS Annals of Photogram-
of vehicles and scene ﬂow.
metry, Remote Sensing and Spatial Information Sciences, II-
3/W5:427–434, 2015. 2, 6

[30] Himangi Mittal, Brian Okorn, and David Held. Just go with
the ﬂow: Self-supervised scene ﬂow estimation. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), June 2020. 1, 2

[31] Bojun Ouyang and Dan Raviv. Occlusion guided scene
In Proceedings of
ﬂow estimation on 3d point clouds.
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) Workshops, pages 2805–2814, June
2021. 1, 2, 3, 5, 7, 8

[32] Gilles Puy, Alexandre Boulch, and Renaud Marlet. FLOT:
Scene Flow on Point Clouds Guided by Optimal Transport.
In European Conference on Computer Vision, 2020. 1, 2, 5,
6, 7

[33] Charles R. Qi, Hao Su, Matthias Niebner, Angela Dai,
Mengyuan Yan, and Leonidas J. Guibas. Volumetric and
multi-view cnns for object classiﬁcation on 3d data. 2016
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2016. 2

[34] Charles R Qi, Li Yi, Hao Su, and Leonidas J Guibas. Point-
net++: Deep hierarchical feature learning on point sets in a
metric space. arXiv preprint arXiv:1706.02413, 2017. 2
[35] Rishav, Ramy Battrawy, Ren´e Schuster, Oliver Wasenm¨uller,
and Didier Stricker. Deeplidarﬂow: A deep learning archi-
tecture for scene ﬂow estimation using monocular camera
and sparse lidar, 2020. 2

[36] Rohan Saxena, Ren´e Schuster, Oliver Wasenm¨uller, and Di-
dier Stricker. PWOC-3D: Deep occlusion-aware end-to-end
scene ﬂow estimation. In IEEE Intelligent Vehicles Sympo-
sium (IV), 2019. 1, 2

[37] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz.
Models matter, so does training: An empirical study of cnns
IEEE Transactions on Pattern
for optical ﬂow estimation.
Analysis and Machine Intelligence (TPAMI). to appear. 2

[38] I. Tishchenko, S. Lombardi, M. R. Oswald, and M. Polle-
feys. Self-supervised learning of non-rigid residual ﬂow and
ego-motion. In 2020 International Conference on 3D Vision
(3DV), pages 150–159, 2020. 2

[39] A. K. Ushani, R. W. Wolcott, J. M. Walls, and R. M. Eustice.
A learning approach for real-time temporal scene ﬂow esti-
mation from lidar data. In 2017 IEEE International Confer-
ence on Robotics and Automation (ICRA), pages 5666–5673,
2017. 2

[40] S. Vedula, P. Rander, R. Collins, and T. Kanade. Three-
dimensional scene ﬂow. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, 27(3):475–480, 2005. 1
[41] Lei Wang, Yuchun Huang, Yaolin Hou, Shenman Zhang, and
Jie Shan. Graph attention convolution for point cloud se-
mantic segmentation. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019. 2
[42] Shenlong Wang, Simon Suo, Wei-Chiu Ma, Andrei
Pokrovsky, and Raquel Urtasun. Deep parametric continuous
convolutional neural networks. 2018 IEEE/CVF Conference
on Computer Vision and Pattern Recognition, 2018. 2
[43] Yang Wang, Yi Yang, Zhenheng Yang, Liang Zhao, Peng
Wang, and Wei Xu. Occlusion aware unsupervised learn-
ing of optical ﬂow. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June
2018. 2

[44] Wenxuan Wu, Zhongang Qi, and Li Fuxin. Pointconv: Deep
convolutional networks on 3d point clouds. 2019 IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR), 2019. 2

[45] Wenxuan Wu, Zhi Yuan Wang, Zhuwen Li, Wei Liu, and Li
Fuxin. Pointpwc-net: Cost volume on point clouds for (self-)
supervised scene ﬂow estimation. In European Conference
on Computer Vision, pages 88–107. Springer, 2020. 1, 2, 3,
4, 5, 6, 7, 8

[46] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Lin-
guang Zhang, Xiaoou Tang, and Jianxiong Xiao.
3d
shapenets: A deep representation for volumetric shapes.
2015 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2015. 2

[47] Shengyu Zhao, Yilun Sheng, Yue Dong, Eric I-Chao Chang,
and Yan Xu. Maskﬂownet: Asymmetric feature match-
In Proceedings of
ing with learnable occlusion mask.
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), June 2020. 1, 2

[48] Victor Zuanazzi, Joris van Vugt, Olaf Booij, and Pascal
Mettes. Adversarial self-supervised scene ﬂow estimation.
In Vitomir Struc and Francisco G´omez Fern´andez, editors,
8th International Conference on 3D Vision, 3DV 2020, Vir-
tual Event, Japan, November 25-28, 2020, pages 1049–1058.
IEEE, 2020. 1, 2

9. Supplementary

9.1. Details of the Architecture

The details of the Occlusion predictor are shown in the
ﬁgure 6. The details of the Occlusion-weighted cost volume
are shown in the ﬁgure 7. We set dcv in the cost volume
layer to be [32, 64, 128, 256] at pyramid level l = 0, 1, 2, 3.
We set doc in the occlusion predictor to be 64 at all pyra-
mid levels. The numbers of the nearest neighbors we are
using are k1 = 32 and k2 = 64. Notice that the beginning
part (relative displacement/feature grouping) of these two
layers have the same structure, so they are being shared in
our implementation to reduce the running time. The details
of the Residual ﬂow predictor are shown in the ﬁgure 5.
The predicted ﬁner ﬂow at each pyramid level l is simply
the element-wise addition of the residual and the upsam-
pled ﬂow from level l + 1. In our implementation, we set
the length of the point cloud features d to be [64, 96, 192,
320] at pyramid level l = 0, 1, 2, 3.

9.2. More Visualization

In ﬁgure 8, we provide more visualization of the super-
vised training on the Flyingthings3D dataset. We ﬁrst train
our model using the supervised loss on the training set of the
Flyingthings3D, and then we show the visualization of the
predictions on the validation set. For better visualization of
the predicted occlusion mask, we consider a point si ∈ S to
be occluded if its predicted occlusion probability is less than

Figure 5: Residual ﬂow predictor. We ﬁrst concatenate the
source feature, cost volume, upsampled ﬂow from the previous
pyramid level, and predicted occlusion mask at the current level
along the feature dimension. Then, we use the PointConv to per-
form the feature encoding. Finally, we use MLP to produce the
ﬁner residual scene ﬂow.

0.5. A point is considered to be non-occluded if its occlu-
sion probability is greater than or equal to 0.5. In ﬁgure 9,
we provide more visualization of the self-supervised train-
ing on the Flyingthings3D dataset. As we can see, both the
scene ﬂow and occlusion label for the source point cloud
can be learned accurately without any ground truth label
by using our novel self-supervised learning scheme. In ﬁg-
ure 10 and ﬁgure 11, we provide more visualization on the
KITTI dataset. In ﬁgure 10, we ﬁrst train our model on the
Flyingthings3D using the supervised loss, and then we show
the predictions on the KITTI. In ﬁgure 11, we ﬁrst train our
model on the Flyingthings3D using the self-supervised loss,
and then we perform the self-supervised ﬁne-tuning on the
training set of KITTI, a sample from the test set is shown.

ConcatSourcePointSourcefeatureCostVolumeUpsampledflowFiner Occlusion maskMLPFinerResidual Scene flowFigure 6: Occlusion predictor. The modules in purple represent the operation without any learnable parameters. The modules in yellow
represent the operation with learnable parameters. The shape of the intermediate tensors is also provided. We ﬁrst ﬁnd the k-NN index
(NT w(si)) in the warped target Tw for each si ∈ S. Second, we gather the relative displacement (tj − si) and the target features (gj)
using the k-NN index. We concatenate the upsampled occlusion mask, source features ci, relative displacement tj − si, and target features
gj along the feature dimension. By using the 1×1 convolutions, Max-pooling, and multilayer perceptron (MLP), we obtain the predicted
ﬁner occlusion mask. We also connect a Sigmoid activation layer at the end to ensure the output has values within the range [0,1].

SourcePointTargetPointdisplacementgroupingfeaturegroupingSourcefeatureTargetfeatureReplicate    timesConcatk-NNSourcePointTargetPointUpsampledocclusion maskReplicate    timesMaxpoolingMLPFinerOcclusion maskSigmoidFigure 7: Occlusion-weighted cost volume layer. The modules in purple represent the operation without any learnable parameters. The
modules in yellow represent the operation with learnable parameters. The shape of the intermediate tensors is also provided. We ﬁrst ﬁnd
the k-NN index (NT w(si)) in the warped target Tw for each si ∈ S. Then, we ﬁnd the nearest local neighbors (NS(si)) in the source for
each si. We gather the relative displacement (tj − si) and the target features (gj) using the k-NN index NT w(si), and then we concatenate
these intermediate tensors along the feature dimension. We construct the matching cost between (si,tj) using the 1×1 convolutions. By
using the Max-pooling as aggregation function, we obtain CVcross. By further applying a self-aggregation on the CVcross, we obtained
the CVself . The ﬁnal cost volume is simply the weighted sum of the two terms using the predicted occlusion as explained in the paper.

SourcePointTargetPointdisplacementgroupingfeaturegroupingSourcefeatureTargetfeatureReplicate   timesConcatk-NNSourcePointTargetPointFinerOcclusion maskMaxpoolingMaxpoolingk-NNSourcePointSourcePointSelfgrouping  Replicate       timesFigure 8: Supervised visualization on Flyingthings3D. We ﬁrst train our model using the supervised loss on the training set of Flyingth-
ings3D and then show the prediction on 3 samples from the validation set. In a), we plot the source (red) and target (blue) point clouds from
the dataset on the same 3D space. In b) and c), we show the warped source point cloud (source+ﬂow) according to the predicted/ground
truth scene and the target point cloud. The red isolated regions in these two plots represent the occlusion in the source since they do not
have corresponding blue target regions. In d) and e), we show the predicted and ground truth occlusion map of the source, where the
non-occluded regions are colored by red and the occluded regions are colored by black.

Figure 9: Self-supervised visualization on Flyingthings3D. We train our model using the Self-supervised losses on the training set of
Flyingthings3D and show the prediction on 2 samples from the validation set. In a), we plot the source (red) and target (blue) point
clouds from the dataset on the same 3D space. In b) and c), we show the warped source point cloud (source+ﬂow) according to the
predicted/ground truth scene and the target point cloud. In d) and e), we show the predicted and ground truth occlusion map of the source,
where the non-occluded regions are colored by red and the occluded regions are colored by black. As we can see, by using our novel
self-supervised training scheme, both the scene ﬂow and occlusion can be learned without any ground truth label.

a) beforeb) predc) gtd) prede) gtb) preda) beforec) gtd) prede) gta) beforeb) predc) gtd) prede) gta) beforeb) predc) gtd) prede) gtb) preda) beforec) gtd) prede) gtFigure 10: Supervised visualization on KITTI. On the left, we plot the source (red) and target (blue) point clouds from the KITTI dataset
on the same 3D space. On the right, we plot the warped source according to the predicted scene ﬂow and the target point cloud. The
predicted scene ﬂow is obtained from the supervised pretrained model on Flyingthings3D without ﬁne-tuning. The zoomed view for the
marked regions is also provided. The better the alignment between the warped source and the target, the more accurate the estimated ﬂow.

Figure 11: Self-supervised visualization on KITTI. On the left, we plot the source (red) and target (blue) point clouds of a sample from
the test set of KITTI on the same 3D space. On the right, we plot the warped source according to the predicted scene ﬂow and the target
point cloud. The predicted scene ﬂow is obtained from the self-supervised pretrained + self-supervised ﬁne-tuned model as explained in
the paper. The zoomed view for the marked regions is also provided. The better the alignment between the warped source and the target,
the more accurate the estimated ﬂow.

