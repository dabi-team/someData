AR Point&Click: An Interface for Setting Robot Navigation Goals

Morris Gu, Elizabeth Croft and Akansel Cosgun
Monash University, Australia

2
2
0
2

r
a

M
9
2

]

O
R
.
s
c
[

1
v
9
1
2
5
1
.
3
0
2
2
:
v
i
X
r
a

Abstract—This paper considers the problem of designating
locations for interactive mobile robots. We
navigation goal
propose a point-and-click interface, implemented with an Aug-
mented Reality (AR) headset. The cameras on the AR headset
are used to detect natural pointing gestures performed by the
user. The selected goal is visualized through the AR headset,
allowing the users to adjust the goal location if desired. We
conduct a user study in which participants set consecutive
navigation goals for the robot using three different interfaces:
AR Point & Click, Person Following and Tablet (birdeye map
view). Results show that the proposed AR Point&Click interface
improved the perceived accuracy, efﬁciency and reduced mental
load compared to the baseline tablet interface, and it performed
on-par to the Person Following method. These results show that
the AR Point&Click is a feasible interaction model for setting
navigation goals.

I. INTRODUCTION

The service robot sector is continuing to grow at a strong
pace, with an expected 22.6% cumulative average growth rate
from 2020 to 2025 [1]. As these robots move from industrial
applications to adoption in public facing and residential
use, they are increasingly being run by non-expert users.
Interactive robots are typically designed to take commands
from users in response to changing demands and workspace
situations, as opposed to pre-programmed industrial robots
operating in separated assembly lines. As such, there is a
demand for user-friendly methods that allow users to set
goals for the robots in intuitive ways. For interactive mobile
robots, allowing users to set navigation goals is a problem
of broad interest.

Several methods of setting navigation goals exist in the
literature. One of the most common methods is setting the
goal from a birdseye map of the environment. A prominent
example of this is the RViz tool within Robot Operating
System (ROS) which is popular among the robotics developer
community. Another common method is sending the robot
to semantically meaningful locations, such as to rooms or
labeled areas [2]. Other, more interactive methods for setting
goals can involve co-located direct driving [3], or letting the
robot follow the user [4, 5]. In human-to-human interaction,
a common method is using deictic (pointing) gestures.

Deictic gestures are also commonly used for human-robot
interaction. Some approaches use pointing devices to provide
goals to robots. In implementations by Kemp et al. [6]
and Gualtieri et al. [7], a point and click and gesture is
implemented using a laser pointer to select the goal objects
for fetch-and-carry tasks. Kemp et al. [6] suggest that point
and click interfaces are simple and have a diverse set of
applications. This concept is extended by Nguyen et al. [8]

Fig. 1: We propose the AR Point&Click interface which allows
users to perform a pointing gesture with their arm, followed by a
pinching gesture with the ﬁngers to assign a navigation goal to a
mobile robot. The chosen goal is visualized through the AR headset
as a virtual golf ﬂag pole, which allows the user to view and adjust
the goal on the ﬂy.

to driving tasks, although users can only select from a set
of pre-deﬁned discrete navigation targets. Sprute et al. [9]
proposes a system in which laser pointers can be used to
deﬁne a virtual barrier for navigation. Chen et al. [10] uses
the head orientation of the Augmented Reality headset to
choose between various target control devices. Alternatively,
some designs use natural hand and/or arm gestures for setting
robot goals, for instance, for selecting object goals [11] or
to point
into space for specifying region goal [12]. Our
work also employs natural deictic gestures for selecting robot
goals.

Augmented Reality (AR) technology has been growing in
popularity for human robot interaction [13], with applications
ranging from visualizing robots behind walls [14], visualizing
the robot’s intent for object handovers [15] or for drones
[16], and human-robot cooperative search [17]. Our previous
works from Hoang et al. [18] and Waymouth et al. [19]
demonstrate choosing goal points for interactive manipu-
lation tasks using natural hand gestures. Kousi et al. [20]
demonstrates an AR application to set navigation goals and
teleoperate a virtual robot. To the authors’ best knowledge,
only the work from Kousi et al. [20] and our previous
work [21] introduce a method in AR which uses point and
click gestures to set navigation goals.

In this work, we investigate the usability of an AR point
and click interface that was initially demonstrated as a proof-
of-concept in our previous work [21] for providing navigation
goals to mobile robots. This method was explored as it

 
 
 
 
 
 
allows the chosen goal to be communicated to the user via
visualization and AR headsets allow natural gesture detection
without additional external setup. The aim of this research is
to evaluate this proposed interface against a Tablet interface,
which acts similarly to RViz, and person following, which is
introduced as a method which is simple to understand and
use. In this research, we envision a scenario where a user is
directed to navigate a mobile robot to and from a set number
of ﬁxed goals.

The contribution of this work is an evaluation of the
proposed AR Point and Click interface in a user study that
aims to investigate the usability of the proposed method
against more traditional methods of setting navigation goals
to robots: person following and tablet-based interface.

II. AR POINT&CLICK INTERFACE

The proposed system consists of two main modules, a
Robot module which employs Robot Operating System
(ROS) and a Interface module which is either an AR headset
which uses Unity Engine or an android tablet which employs
ROS-Mobile [22]. The Interface module communicates the
robot’s goal to the Robot module, which then uses this
through
information to navigate to a user-speciﬁed goal
velocity commands. The Robot module communicates the
visualization information to the Interface module. Commu-
nication between the modules is achieved through a Wi-Fi
router over a local network. On the Robot side and tablet
side we use the ROS inter-node communication. To allow
the Unity and HoloLens side to communicate with the ROS
side, we employ the ROS# software package.

A. Robot Implementation

For the Robot module we employ the Fetch Robotics
Mobile Manipulator (Fetch robot). The Fetch robot has a
maximum speed of 1 m/s and employs a ankle-height SICK
2D LIDAR sensor with a range of 25m and 220 degrees
FOV. For localization, we use the standard ROS Navigation
Stack packages which are modiﬁed by Fetch Robotics for
mapping and localization of the robot: OpenSLAM gmapping
and Monte Carlo localization. In this study, the robot is to be
navigated between points in the real-world where there are
no static obstacles in between them. Therefore, we chose
to adopt a simple navigation behavior from our previous
works [14, 23]. Collision avoidance is implemented using
the LIDAR sensor to detect objects within a certain distance
and detects for a roughly 100◦ cone 1 meter in front of the
robot.

B. Augmented Reality Interface

We introduce an AR Point&Click (AR) interface. In
this interface the goal
is speciﬁed with a hand gesture,
as implemented by Hoang [21]. The hand gesture involves
pointing a ray that points from the user’s arm at the ground
and pinching their thumb and index ﬁnger. This method is
chosen to be an AR implementation of the more standard
RViz interface. It employs the Microsoft HoloLens 2 as the
AR headset. This headset can display virtual objects on a

set of see-through holographic lenses and provides head pose
tracking and localizes itself to a ﬁxed reference frame. Using
this, a ﬂag pole is visualized at the goal, as shown in Figure 1.
We localize the AR headset with off-the-shelf packages
provided by Vuforia [24]. We localize the origin of the
world frame using a printed-out AR marker provided by
Microsoft. The marker does not need to be within the FOV
of the headset but it is possible that the localization drifts.
If the localization is off, the user can globally re-localize the
headset by looking at the physical AR marker and uttering
the word “calibrate”. To localize the robot and AR headset
frames in the same coordinate frame, the AR marker is placed
at the corresponding pose of the origin in the Robot’s map
frame. In this case the AR marker is placed on the ﬂoor. Due
to errors in localization and the placement of the marker,
which are additive, there may be slight mismatches in the
frame of the AR headset and Robot.

III. USER STUDY

In this study,

the user is given the task to command
the robot to navigate 6 goal positions with a ﬁxed, pre-
determined order. The user starts at ﬁxed initial position but
is allowed to move around the area to navigate the robot.
The experimental setup is shown in Figure 2. After reaching
each goal, the user is asked to wait the robot be become
stationary, and then utter the word “done” to indicate to
the experimenter that the goal has been reached. After this
occurs, the experimenter speciﬁes the next goal and the user
sets the goal position for the robot to navigate. The task is
completed when the ﬁnal goal is reached by the robot.

Fig. 2: The experimental setup for the robot navigation task with
in their starting positions. The goals are
robot and participant
highlighted and the numbers for each goal represent the navigation
order of the goals.

In addition to the proposed AR Point&Click interface, we
also adopt two baseline navigation interfaces, a Person Fol-
lowing interface and a Tablet interface, which are elaborated
on in the next section.

With these three interfaces, our user study has 3 conditions
represented by the 3 interfaces. Each user was tested on
each condition separately. The order of the conditions was
counterbalanced between users to reduce ordering effects.
For each condition, the user completed a single trial of the
navigation task.

A. Baseline Interfaces

Fig. 3: An example of the Tablet interface which uses ROS mobile
[22] that implements the popular RViz interface on mobile devices.
Note that the visualized ﬂag is not seen in by the user and is used
to symbolize the user’s goal in this scenario

We implement two baseline methods, to compare with our

AR-based point and click method.

Person Following (PF) interface: The robot follows the
user from behind by moving to a certain distance from the
user. This behavior is activated with a “follow me” speech
command which then uses the pose tracking provided by
the HoloLens 2 Headset to obtain the user’s position. This
interface is designed to be easy to use that allows the users
to continually control the robot’s position by simply walking.
Tablet interface: A goal pose is chosen in a top-down 2D
map, through the android tablet app ROS Mobile [22] by
double tapping the screen. The implementation is designed
to be similar to the standard RViz interface built into ROS.
It visualizes a top-down 2D occupancy map, the robot’s
position and orientation (represented as an arrow) as well
the current observation from the laser scanner.

B. User Study Procedure

The user study was conducted in an open area so that line
of sight between the robot and each of the goals could be
maintained. Before the user study commenced, the participant
read an explanatory statement and ﬁlled in a consent form.
As well, the participants watched a video which explained
the task and its rules. The experimenter read from a script
throughout the user study to ensure uniformity of procedure.
Each user was given two minutes to practice on each inter-
face before starting the task. This practice time was recorded.
Afterwards, the user began the task on the experimenters
signal. After each condition, the user was given a survey to
rate the system with questions from Table I. After completing
the task with all 3 interfaces, the participants were given the
option to engage in a post-experiment interview. The mean
duration for each participant was around 30 minutes.

C. Metrics

We log both objective and subjective metrics.

Objective metrics:
• Positional Error (m): The Euclidean distance between
the desired goal position and the robot’s position when
the user is deemed to complete the navigation task by
uttering the word “done”. from the goal for the robot
and user’s goal respectively.

• Goal-to-goal time (s): The time spent between goals.
• Practice time (s): The time from when the participants
starts trialing the interface until they decide to stop, or
up to 2 minutes maximum.

Subjective metrics: After each task is completed, the
participant was asked to respond to a set of questions, shown
in Table I. The questions were all measured on a 7-point
Likert scale and were designed to focus on the perceived
usability and efﬁcacy of the system.

TABLE I: User Study Survey Questions

Q1: I felt safe during the task.
Q2: I was satisﬁed with the robot’s navigation accuracy.
Q3: The user interface aided my ability to complete the task.
Q4 The interface was easy to use.
Q5 The task was mentally demanding.

After the participant completed the task on all

three
interfaces, they are asked for their preferred user interface
for this task.

D. Hypotheses

that

We expect

the AR based interface will result

in
improved positional accuracy and perceived safety. We also
expect reduced mental load and improved perception of task
efﬁciency and accuracy for both the person following and
AR based interfaces. As well, based on its design, we predict
that the Person Following interface will be most intuitive.
As such we formulate the following hypotheses.
H1 The AR Point&Click method will have the highest

Perceived Safety.

H2 The AR Point&Click and Person Following interfaces
will be rated signiﬁcantly higher in Perceived Accuracy
than the Tablet interface.

H3 The AR Point&Click and Person Following interfaces
will be rated signiﬁcantly higher in Perceived Task
Efﬁciency than the Tablet interface.

H4 The Person Following interface will be the Easiest to

Use.

H5 The Tablet interface will be the most Mentally De-

manding to use.

H6 The AR Point&Click interface will have the lowest

Positional Error compared to other methods

IV. QUANTITATIVE RESULTS

For this study we recruited 18 participants from within
the university, including 16 male and 2 female participants,
between the ages of 21 and 33 (µ = 24.4, σ = 3.6). All
but one of the participants had prior experience in robotics.
6 out of the 54 trials were repeated due to AR application
crashes. Prior to any task employing AR applications, user’s

Perceived Safety

Perceived Accuracy

Perceived Efﬁciency

AR

7

11

1

1

5

11

1

1

9

7

1

2

1

1

7

9

8

7

PF

5

5

T

4

2

5

8

7

3

2

4

3

3

3

2

4

2

6

3

1

0% 25% 50% 75% 100%

0% 25% 50% 75% 100%

0% 25% 50% 75% 100%

Ease of Use

Mental Load (Reverse Scale)

AR

2

1

PF

2

4

6

7

10

T

4

3

2

2

3

4

4

6

6

4

5

2

1

7

1

1

1

2

3

4

1

5

4

1

0% 25% 50% 75% 100%

0% 25% 50% 75% 100%

Strongly Disagree

Disagree

Slightly Disagree

Neutral

Slightly Agree

Agree

Strongly Agree

Fig. 4: Summary of the raw data obtained from the survey ﬁlled out by 18 participants, comparing the AR Point&Click (AR), Person
Following (PF) and Tablet (T) interfaces, for each one of the 5 survey questions shown in Table I.

TABLE II: Post-hoc Nemenyi test results for the survey questions
used to test H1-H5. Signiﬁcant results are indicated in bold and
was determined for p-values < 0.05.

p (AR (cid:54)= PF)

p (AR > T)

p (PF > T)

Perceived Safety
Perceived Accuracy
Perceived Efﬁciency
Ease of Use
Mental Load

0.29
0.83
0.99
0.077
0.99

0.06
<0.001
0.002
0.63
0.001

0.73
0.005
0.001
0.005
<0.001

were informed that they would repeat the task in the case
of an AR application crash. They were informed that this
error should not be reﬂected in their responses to the survey
questions.

A. Subjective Metrics

The raw distribution of responses for each survey question

is shown in Fig. 4.

To test H1-H5, we use a non-parametric Friedman test to
analyze the likert scale variables. Due to the ordinal nature of
likert scale variables requiring a non-parametric test, this test
was chosen as it analyzes a single variable with more than
2 categories. The test found signiﬁcance for all 5 questions,
hence a post-hoc Nemenyi test was then conducted between
pairs of interfaces with p-values shown in Table II.

Regarding Perceived Safety (Q1), no statistical signiﬁ-
cance between any two methods were found, therefore H1
cannot be afﬁrmed. We do, however, observe a trend that the
AR Point&Click is rated highest by the participants, with
11 people strongly agreeing that they felt safe. We believe
that a larger participant pool might afﬁrm this hypothesis,
especially for the hypothesis that AR Point&Click would
be found signiﬁcantly safer than the Tablet method (p=0.06,

which is close to the signiﬁcance boundary). We also observe
that the Perceived Safety question was rated the highest
overall among all 5 questions, signalling that the participants
felt safe interacting with the robot, which is an important
consideration for any human-robot interaction scenario.

Participants

rated both Person Following and AR
Point&Click interfaces signiﬁcantly higher than the Tablet
interface in terms of Perceived Accuracy (Q2) and Per-
ceived Task Efﬁciency (Q3), afﬁrming H2 and H3, re-
spectively. Moreover, both of these methods were rated
to require signiﬁcantly less Mental Load (Q5) than the
Tablet interface, afﬁrming H5. No difference between AR
Point&Click and Person Following interfaces was found
for these three metrics.

The Person Following method was found to be the most
Easy to Use (Q4) with the difference to the Tablet interface
signiﬁcant (p=0.005). However a signiﬁcant difference was
not found to the AR Point&Click interface (p=0.077).
Therefore, H4 can only be partially afﬁrmed and further user
studies and user feedback is warranted.

When asked for which interface they would prefer for this
task after being exposed to all three interfaces, 12 users out
of 18 indicated that they prefer AR Point&Click interface,
including 3 who indicated that they equally preferred another
method. The distribution of user preferences post-study are
shown in Figure 5.

B. Objective Metrics

The mean and standard deviation for each of the three
objective metrics and for each interface is shown in Table III.
While Person Following had the lowest Practice Time and
Goal Time, AR Point&Click had the lowest Positional Error.
We conducted a one-way ANOVA test in which we found
that there was no statistical signiﬁcance of the Practice Time

s
t
n
a
p
i
c
i
t
r
a
P

#

15

10

5

0

10.5

6

AR

PF

1.5

T

Fig. 5: Distribution of user preferences. Where users preferred more
than one interface, fractions were used. The AR Point&Click was
the most preferred interface for the goal setting task.

TABLE III: Mean and standard deviation for each objective metric
is shown for each interface. Lower numbers are better - and the best
performing methods are indicated in bold.

AR

PF

T

Practice time (s) 84.1 (σ = 40.1)
19.3 (σ = 7.3)
Goal Time (s)
Pos. Error (m)

61.4 (σ = 26.2)
74.7 (σ = 30.8)
24.5 (σ = 4.9)
16.8 (σ = 3.7)
0.137 (σ = 0.03) 0.141 (σ = 0.04) 0.188 (σ = 0.05)

metric, while there were statistically signiﬁcant differences
for the Goal Time and Positional Error metrics. Hence, we
ran post-hoc t-tests on those two metrics. The Bonferroni
method [25] was employed to account for the repeated tests,
which reduced the threshold for statistical signiﬁcance by
a factor of 3, hence we used p=0.0167 as the statistical
signiﬁcance threshold. The t-test results for each objective
metric pair are shown in Table IV.

TABLE IV: p-values for post-hoc t-test results for the objective
metrics. Signiﬁcant results and preferred method are indicated in
bold.

p (AR (cid:54)= PF)

p (AR > T)

Goal Time
Positional Error

0.25
0.74

0.030
0.001

p (PF > T)
<0.001
<0.001

While we found that AR Point&Click interface resulted in
the lowest Positional Error, and it was signiﬁcantly lower than
Tablet interface, we could not ﬁnd a statistical difference
between AR Point&Click and Person Following. Hence,
H6 can not be afﬁrmed.

Overall, the user study results supports the hypotheses H2,
H3, and H5 and partially supports H4, but does not have
statistically signiﬁcant results to afﬁrm H1 and H6.

V. QUALITATIVE ANALYSIS

During the user studies, we noticed that the participants’
perception of accuracy and mental load appeared to be linked
to how easy it is to make ﬁne adjustments in position rather
than the actual accuracy of the robot’s position. When using
the Tablet interface, one participant (R16) found that
it
was ”hard at times to pinpoint where the robot was and
[that] several times [they] had to make many adjustments”.
Meanwhile another participant (R17) stated that ”it was nice

to be able to see where you were going to send the robot
and ... [that] you had the location to conﬁrm” in reference
to the AR Point&Click interface.

Unexpectedly, we observed that users made ﬁne adjust-
ments with the Person Following interface by making small
movements for the robot to follow. This, along with com-
ments about the AR Point&Click interface, suggests that
positional accuracy and perceived efﬁciency improve with
the ability to localize goals within the physical space. A
participant (R12) suggested that ”the best interface would be
a combination of the [AR Point&Click and Person Following
interfaces]”.

It can be noted that the AR Point&Click had similar
improvements over the Tablet interface when compared to
the Person Following interface with the exception of Q4,
where users did not ﬁnd the AR interface easy to use relative
to the other options. One participant (R5) notes that the AR
Point&Click interface ”did take longer to ﬁgure out how to
use but once [they] ﬁgured it out, it was quite good to use”.
This suggests users may need to be given more time and that
the short calibration time of the user study may not be fully
representative of the interface.

Though H1 was not afﬁrmed, the perceived safety of the
AR Point&Click may be a key factor in its adoption as
the AR headset allowed users to look at the robot instead
of looking down at a Tablet interface or being in close
proximity to the robot in the Person Following interface.
A participant (R11) found that ”you can observe the motion
of the robot” with the AR Point&Click interface while also
stating that ”It was a little bit scary” to have the robot follow
them with the Person Following interface.

An interesting observation was that the majority of the par-
ticipants expressed their preference for the AR Point&Click
interface, despite the fact that it did not perform signiﬁcantly
better than Person Following in any of the metrics. This
result might be a novelty effect of the AR technology, which
motivates long-term studies.

VI. CONCLUSIONS AND FUTURE WORK

We propose a point and click interface using an Aug-
mented Reality headset which allows users to use a natu-
ral point-and-click gesture for setting navigation goals for
mobile robots. The interface also allows users to visualize
the goal after it was set which lets them to adjust the goal
location if desired. We validate the efﬁcacy of the proposed
interface through a user study,
in which participants set
navigation goals for a robot. We compare the proposed AR
Point&Click interface to a Tablet interface which is similar
to the popular developer tool RViz, and a Person Following
behavior, designed to be an intuitive interface.

User study results show that the users were able to set
the goals with highest accuracy using the proposed AR
Point&Click interface, with Person Following being a
close second. Moreover, AR Point&Click was subjectively
assessed to have higher perceived accuracy and efﬁciency
and a lower mental load compared to the Tablet interface.
Though not conclusive, there was some indication that the

AR Point&Click interface also improved the perceived
safety. The proposed interface was rated similarly to Person
Following by the participants, however, the majority of the
participants indicated that AR Point&Click is their interface
of choice, likely due to novelty effect. On the other hand,
there was some indication that Person Following method
was found to be easier to use than AR Point&Click, which
might be alleviated with higher levels of adoption of the AR
technology in the future. Given these results, the proposed
AR Point&Click is a feasible method for setting navigation
goals for mobile robots and there is room for improvement
in the user experience.

Some of

found for

the main drawbacks

the AR
Point&Click were that sometimes (R11) ”[you] couldn’t
the point” due to the AR headset’s low
exactly look at
ﬁeld-of-view which is a limitation of the technology itself.
This limitation also found its way into the gestures where
participants often found that the click gesture recognition
wouldn’t register or would falsely register. It is important
to note that the improvements to the AR technology in the
future can alleviate some of the issues experienced by users.
It should be noted that this work is a proof-of-concept
for using an Augmented Reality headset to control mobile
robots and there are several ways to improve the current
implementation. First, the feasibility of the navigation goals
set by the users are not checked, hence collision checks can
be incorporated. Second, the user study can be extended
to test how well people can set goal poses (position and
orientation) instead of just position goals. Third, the current
navigation behavior is very simple and therefore implement-
ing a more complex navigation algorithm, which may be
human-aware [26], may be another avenue to investigate.
Finally, the proposed system can be extended to select other
entities, such as people or objects.

VII. ACKNOWLEDGEMENT

This project was supported by the Australian Research

Council (ARC) Discovery Project Grant DP200102858.

[6] C. C. Kemp, C. D. Anderson, H. Nguyen, A. J. Trevor, and Z. Xu, “A
point-and-click interface for the real world: laser designation of objects
for mobile manipulation,” in ACM/IEEE International Conference on
Human-Robot Interaction (HRI), 2008, pp. 241–248.

[7] M. Gualtieri, J. Kuczynski, A. M. Shultz, A. Ten Pas, R. Platt, and
H. Yanco, “Open world assistive grasping using laser selection,” in
IEEE International Conference on Robotics and Automation (ICRA),
2017.

[8] H. Nguyen, A. Jain, C. Anderson, and C. C. Kemp, “A clickable
world: Behavior selection through pointing and context for mobile
manipulation,” in IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS), 2008, pp. 787–793.

[9] D. Sprute, K. T¨onnies, and M. K¨onig, “This far, no further: Introducing
virtual borders to mobile robots using a laser pointer,” in IEEE
International Conference on Robotic Computing (IRC), 2019, pp. 403–
408.

[10] Y.-H. Chen, B. Zhang, C. Tuna, Y. Li, E. A. Lee, and B. Hartmann,
“A context menu for the real world: Controlling physical appliances
through head-worn infrared targeting,” UC Berkeley EECS, Tech. Rep.,
2013.

[11] A. Cosgun, A. J. Trevor, and H. I. Christensen, “Did you mean this
object?: detecting ambiguity in pointing gesture targets,” in HRI’15
Towards a Framework for Joint Action Workshop, 2015.

[12] Y. Hato, S. Satake, T. Kanda, M. Imai, and N. Hagita, “Pointing
to space: modeling of deictic interaction referring to regions,” in
ACM/IEEE International Conference on Human-Robot Interaction
(HRI), 2010.

[13] Z. Makhataeva and H. A. Varol, “Augmented reality for robotics: A

review,” Robotics, vol. 9, no. 2, p. 21, 2020.

[14] M. Gu, A. Cosgun, W. P. Chan, T. Drummond, and E. Croft, “Seeing
thru walls: Visualizing mobile robots in augmented reality,” in IEEE
International Conference on Robot Human Interactive Communication
(RO-MAN), 2021, pp. 406–411.

[15] R. Newbury, A. Cosgun, T. Crowley-Davis, W. P. Chan, T. Drummond,
and E. Croft, “Visualizing Robot Intent for Object Handovers with
Augmented Reality,” arXiv:2103.04055 [cs], 2021.

[16] M. Walker, H. Hedayati, J. Lee, and D. Szaﬁr, “Communicating robot
motion intent with augmented reality,” in ACM/IEEE International
Conference on Human-Robot Interaction (HRI), 2018.

[17] C. Reardon, K. Lee, and J. Fink, “Come See This! Augmented
Reality to Enable Human-Robot Cooperative Search,” in 2018 IEEE
International Symposium on Safety, Security, and Rescue Robotics
(SSRR), 2018.

[18] K. C. Hoang, W. P. Chan, S. Lay, A. Cosgun, and E. Croft, “Virtual
Barriers in Augmented Reality for Safe and Effective Human-Robot
Cooperation in Manufacturing,” arXiv:2104.05211 [cs], 2021.
[19] B. Waymouth, A. Cosgun, R. Newbury, T. Tran, W. P. Chan, T. Drum-
mond, and E. Croft, “Demonstrating cloth folding to robots: Design
and evaluation of a 2d and a 3d user interface,” in IEEE International
Conference on Robot Human Interactive Communication (RO-MAN),
2021.

REFERENCES
[1] “USD 35.27 Billion Growth in Service Robotics Market: By
Application (professional robots and personal robots) and Geography -
Global Forecast to 2025,” https://www.prnewswire.com/news-releases/
usd-35-27-billion-growth-in-service-robotics-market-by-application-professional-robots-and-personal-robots-and-geography---global-forecast-to-2025--301454915.
html, 2022-03-24.

[21] K. C. Hoang, W. P. Chan, S. Lay, A. Cosgun, and E. Croft, “Arviz: An
augmented reality-enabled visualization platform for ros applications,”
IEEE Robotics Automation Magazine, pp. 2–11, 2022.

[20] N. Kousi, C. Stoubos, C. Gkournelos, G. Michalos, and S. Makris,
“Enabling Human Robot Interaction in ﬂexible robotic assembly lines:
an Augmented Reality based software suite,” Procedia CIRP, 2019.

[2] A. Cosgun and H. I. Christensen, “Context-aware robot navigation us-
ing interactively built semantic maps,” Paladyn, Journal of Behavioral
Robotics, vol. 9, no. 1, 2018.

[3] A. Cosgun, A. Maliki, K. Demir, and H. Christensen, “Human-centric
assistive remote control for co-located mobile robots,” in ACM/IEEE
International Conference on Human-Robot Interaction (HRI) Extended
Abstracts, 2015, pp. 27–28.

[4] A. Cosgun, D. A. Florencio, and H. I. Christensen, “Autonomous
person following for telepresence robots,” in IEEE International Con-
ference on Robotics and Automation (ICRA), 2013.

[5] P. Scales, O. Aycard, and V. Auberg´e, “Studying Navigation as a Form
of Interaction: a Design Approach for Social Robot Navigation Meth-
ods,” in IEEE International Conference on Robotics and Automation
(ICRA), 2020.

[22] N. Rottmann, N. Studt, F. Ernst, and E. Rueckert, “ROS-Mobile: An
Android application for the Robot Operating System,” arXiv preprint
arXiv:2011.02781, 2020.

[23] R. Newbury, A. Cosgun, M. Koseoglu, and T. Drummond, “Learning
to Take Good Pictures of People with a Robot Photographer,” in
IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS), 2020.

[24] “Vuforia Developer Portal |.” [Online]. Available: https://developer.

vuforia.com/

[25] M. Jafari and N. Ansari-Pour, “Why, when and how to adjust your p

values?” Cell journal, vol. 20, pp. 604–607, 2019.

[26] A. Cosgun, E. A. Sisbot, and H. I. Christensen, “Anticipatory robot
path planning in human environments,” in IEEE international sym-
posium on robot and human interactive communication (RO-MAN),
2016.

