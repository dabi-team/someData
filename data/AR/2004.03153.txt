Adaptive Multiscale Illumination-Invariant Feature Representation for Undersampled Face 
Recognition 

Yang Zhang a,b, Changhui Hu a,b,d, Xiaobo Lua,b,* 

a School of Automation, Southeast University, Nanjing 210096, China 

b Key Laboratory of Measurement and Control of Complex Systems of Engineering, Ministry of Education, Southeast University, 

Nanjing 210096, China 

d School of Automation, Nanjing University of Posts and Telecommunications, Nanjing 210023, China 

*Corresponding author, E-mail: xblu2013@126.com (X.B.Lu) 

Abstract- This paper presents an novel illumination-invariant feature representation approach used 

to  eliminate  the  varying  illumination  affection  in  undersampled  face  recognition.  Firstly,  a  new 

illumination  level  classification  technique  based  on  Singular  Value  Decomposition  (SVD)  is 

proposed  to  judge  the  illumination  level  of  input  image.  Secondly,  we  construct  the  logarithm 

edgemaps  feature  (LEF)  based  on  lambertian  model  and  local  near  neighbor  feature  of  the  face 

image, applying to local region within multiple scales. Then, the illumination level is referenced to 

construct the high performance LEF as well realize adaptive fusion for multiple scales LEFs for the 

face image,  performing  JLEF-feature.  In  addition,  the  constrain  operation  is used  to  remove  the 

useless  high-frequency  interference,  disentangling  useful  facial  feature  edges  and  constructing 

AJLEF-face. Finally, the effects of the our methods and other state-of-the-art algorithms including 

deep learning methods are tested on Extended Yale B, CMU PIE, AR as well as our Self-build Driver 

database  (SDB).  The  experimental  results  demonstrate  that  the  JLEF-feature  and  AJLEF-face 

outperform other related approaches for undersampled face recognition under varying illumination. 

Keywords:  Undersampled  face  recognition,  Singular  value  decomposition,  Multiple  scales 

edgemaps, Illumination-invariant feature extraction 

1. INTRODUCTION 

With the development of image processing technique, face recognition is commonly used in our 

daily  life,  such  as  in  airport,  bank  and  Intelligent  Transportation  System(ITS)  [1-4].  In  these 

applications, frontal face image on ID card or e-passport is served as gallery, meanwhile, the real-

time taken face image affected by varying lighting is served as probe [5]. Just like the examples in 

Fig. 1, the driver images taken by surveillance cameras on highroads are always influenced by serve 

illumination. 

In these situations, undersampled face recognition under severe illumination is a difficult issue to 

 
be solved [6]. 

Aiming  at  processing  the  illumination  factor  in  face  recognition,  a  large  amount  of  related 

approaches appeared. After conclusion, these methods are separated into illumination preprocessing 

and  invariant  feature  extraction  methods.  Illumination  preprocessing  techniques  [7-9]  take  the 

whole image as process object to reduce the illumination affection. Unfortunately, these methods' 

satisfied  performances  are  based  on  strict  alignment  face  images.  In  addition,  they  cannot  take 

advantage  of  the  high  frequency  features,  which  contain  most  facial  inherent  information.  Via 

contrast,    illumination  invariant  feature  extraction  methods  [10-15]  are  equipped  with  more 

efficient illumination processing ability. LOG-DCT [11] aims at realizing the domain changing for 

face  image  from  spatial  domain  to  frequency  domain,  extracting  the  low-frequency  DCT 

coefficients for eliminating. MSLDE [15] extracts the edgemaps of pixels in face image, utilizing 

Lambertian  reflectance  model  to  eliminate  illumination  effect.  However,  these  methods  lack 

adaptability, only performing well under particular illumination condition.   

Based on previous research, generic learning is the conventional solution for undersampled face 

recognition [1, 2, 16]. Generic learning aims at forming the generic set, consisted of face images 

which are not belong to the training set, to learn the intra-class variations among the various faces 

in training set. The adaptive linear regression classification (ALRC) [18] takes use of the k nearest 

neighbors (kNNs) principle to form intra-class variations. The stacking supervised auto-encoders 

(SSAE) [19] utilizes the deep supervised auto-encoder to formulate the generic set. However, these 

formulated generic sets cannot realize effective performances under serious illumination conditions. 

Fig. 1. The driver images taken by surveillance cameras on highroads. (Provided by Public Security Department of 

China) 

         
 
 
With the increasingly widespread use of deep learning approaches, deep neural networks have 

been  applied  in  representation  extraction  based  face  recognition  [20-26,  47-55].  However,  the 

shortcoming of undersampled face recognition is lack of training samples, which is the fatal flaw 

for  adopting  deep  learning  approach.  In  addition,  to  the  best  of  our  knowledge,  until  now,  deep 

learning approaches possess no serious illumination processing ability. The existing deep learning 

methods, such as multi-view perceptron (MVP) [27], multi-task deep neural network (DNN) [28] 

as well as disentangled representation learning-generative adversarial network (DR-GAN) [29] are 

utilized to processing slight illumination and varying pose. In this way, they cannot handle serious 

illumination changes on face images, such as samples in Extended Yale B [30] and CMU PIE [31] 

database. 

This research proposes a new disentangled illumination-invariant feature representation approach. 

Firstly,  we  propose  a  new  illumination  level  classification  approach  based  on  SVD.  Then,  the 

illumination  intensity  is  referenced  to  realize  adaptive  illumination-invariant  feature  extraction, 

constructing JLEF-feature. In addition, the constrain operation is used to remove the useless high-

frequency interference from illumination shadows, aiming at retaining useful facial edges, forming 

AJLEF-face.     

Different from the previous researches such as [15, 32, 33], the contributions of this research are 

as follows: 

1)  We  propose  a  new  illumination  level  classification  approach,  judging  the  corresponding 

illumination level of input image. 

2)  We  extract  JLEF-feature  which  utilizes  the  adaptive  fusion  structure  on  LEFs  based  on  the 

corresponding illumination level.   

3)  The  constrain  operation  is  introduced  to  JLEF-feature  to  disentangle  useful  high-frequency 

facial feature from useless high-frequency interference from illumination shadows, forming the 

AJLEF-face. 

4)  This  method  has  been  verified  on  our  SDB  database,  which  is  constructed  in  real  scene 

accompanied with varying illumination. Moreover, we take our methods together with other 

similar state-of-the-art approaches including deep learning researches into comparison on other 

common datasets, such as Yale B, CMU-PIE and AR databases. 

 
2. RELATED WORK 

2.1. SVD 

According to previous study, SVD has been widely used in digital image processing, including 

feature  extraction  and  noise  filter. Andrews  et  al.  [34]  utilizes  SVD  to  perform  the  functions  of   

‚Äúlow- and high- pass‚Äù filter. Later, the fractional order SVD representation (FSVDR) [35] is served 

as ‚Äúhigh-pass‚Äù nonlinear filter to eliminate the disturbing noise in face recognition. Demirel et al. 

[36]  and W.  Kim  et  al.  [37]  held  the  similar  opinion  that  the  corresponding  singular  matrix  can 

represent the illumination invariant factor of face image. Thus, this research aims at exploring the 

potential relationship between singular value and illumination level among the face image. 

2.2. Illumination-Invariant Feature based Illumination Processing 

According to the previous study, illumination-invariant feature extraction is known as an effective 

illumination  processing  method.  Gradient-face[38]  and  Weber-face[39]  were  proposed  firstly  to 

calculate the illumination insensitive features. The former one is to calculate the ratio between y-

gradient with x-gradient, the latter one is to get the ratio between local intensity difference and the 

constant background. Later, MSLDE is proposed by Lai et al. [34], which utilizes the illumination 

invariant edges of local area to calculate the illumination invariant feature. However, the common 

shortage is existing in the above methods, which are all under the assumption that the illumination 

conditions are similar in local area. Once the illumination shadow in face image varies dramatically, 

these methods show poor performances. In this paper, we try to construct an adaptive illumination-

invariant feature based on illumination intensity and local near neighbor feature, realizing adaptive 

illumination robust feature extraction.   

3. THE PROPOSED METHOD 

3.1. Illumination level classification   

This  research  proposes  a  new  illumination  level  classification  method  based  on  SVD.  The 

classification principle is introduced below. 

First of all, the input image is processed by logarithm transformation, showing in Eq.(1). 

                                                          f(x,  y)=ln  I(x,  y)                                                                (1)                           

After logarithm transformation, SVD is introduced to make decomposition on the logarithm face 

 
 
f(x, y). The processing progress is as following:   

      f(x, y)=UDVùëá, D=

.

.

ùëë1

(

                                                (2) 

.

ùëëùëõ)

                                                                (3) 

                                                                  (4) 

Where  matrix  U  and  matrix V  perform  orthogonal  essentially.  In  Eq.(2),  singular  value  d1  to 

singular value dn ranks from large to small, in turn. The length and width of input pixel matrix are 

m and n, respectively. 

ùëá
                        ùëì(ùë•, ùë¶) = ‚àë ùëëùëñùë¢ùëñùë£ùëñ

ùëõ
ùëñ=1

                                                                  (5) 

In Eq.(5) 

  represents ith feature of input face. 

      ùëêùëñ = ùëÜigmoid(ùõΩùëëùëñ) =

1
1+ùëí‚àíùõΩùëëùëñ

                                                        (6)                                                       

Aiming at getting energy coefficient for illumination level (ECIL), ci, to realize classification, we 

make Sigmoid calculation on all the singular values, di. Under the affection of different Œ≤ in Sigmoid 

function, the corresponding ECILs show varying distribution in Fig. 2. We utilize the three images 

of the same individual from different subset in YaleB to perform the three regular ECIL curves. The 

intensive degree of ci curves is increasing with the value of Œ≤. Based on the experiment result, Œ≤=1 

leads to the uniform ci curve, classifying the illumination level effectively. Thus, this paper presents 

a  new  approach  to  get  the  illumination  coefficient  of  corresponding  face  image,  ILcoefficient, 

performing in Eq.(7). 

                                              ùêºùêøùëêùëúùëíùëìùëìùëñùëêùëñùëíùëõùë° = ‚Äñùëê1, ùëê2, ‚Ä¶ , ùëêùëõ‚Äñ2                                                      (7) 

This research takes Yale B face database [30], constructed on various illumination conditions, to 

divide  the  ILcoefficient.  Thus,  the  ILcoefficients  of  all  the  face  images  in  Yale  B  are  calculated.  Not 

surprisingly, the ILcoefficient of the face image taken under serious illumination condition is larger 

than that under normal illumination condition.     

Thus, the maximum ILcoefficient in Yale B database is defined as Max- ILcoefficient and minimum 

ILcoefficient  is  defined  as  Max-  ILcoefficient.  This  research  proposes  Eq.(8)  to  calculate  the  dividing 

boundary ‚àá, realizing illumination level classification. 

                                              ‚àá=(Max-ILcoefficient- Min-ILcoefficient)/3                                          (8) 

ÔÅõÔÅùnmnRuuuU*21,...,,ÔÉé=ÔÅõÔÅùnnnRvvvV*21,...,,ÔÉé=TiivuB=i 
 
 
 
                   
                             
In this way, 5 boundary ILlevels are calculated, as Table 1 shows. 

Table 1   

The classified illumination level 

ùêºùêøùëôùëíùë£ùëíùëô 

ùêºùêøùëôùëíùë£ùëíùëô0 

ùêºùêøùëôùëíùë£ùëíùëô1 

ùêºùêøùëôùëíùë£ùëíùëô2 

ùêºùêøùëôùëíùë£ùëíùëô3 

ùêºùêøùëôùëíùë£ùëíùëô ùëÖùëéùëõùëîùëí 

                                                        < Min-IL coefficient 
Min-IL coefficient +‚àá 
Min-IL coefficient +2‚àá 

Min-IL coefficient 
Min-IL coefficient +‚àá 
Min-ILl coefficient +2‚àá 

Max-IL coefficient 

                    ùêºùêøùëôùëíùë£ùëíùëô4 

> Max-IL coefficient 

Based on Table 1, all the images in Yale B can be classified into three illumination conditions. 

Table 2 performs the processing steps. By convention, Yale B can be used as a benchmark database. 

Thus,  ILlevel0  and  ILlevel4  are  generated  to  act  as  the  references  for  other  database‚Äôs  illumination 

conditions. Fig. 3 shows some example images in various ILlevel subsets within Yale B database. 

Fig. 2. Distributions of ECILs based on different Œ≤. 

Table 2   

The illumination level (ILlevel) classification steps 

Step1 An input image I;   

Step2 Logarithmic transformation, performing in Eq.(1); 

Step3 SVD decomposition, performing in Eq.(2); 

Step4 Calculate the singular value di;   

Step5 Calculate the ECIL ci of corresponding di, performing in Eq.(6);   

Step6 Calculate the ILcoefficient, performing in Eq.(7); 

Step7 Classify ILlevel by Table 1. 

 
                                                 
 
 
 
 
(a)ILlevel1                                (b) ILlevel 2                                  (c) ILlevel3 

Fig. 3. Example images in various ILlevel subsets. 

3.2. JLEF feature 

According  to  previous  study,  the  Lambertian  reflectance  model  [28]  is  the  basic  theory  for 

illumination research in face recognition, showing in Eq.(9). 

      ùêº(ùë•, ùë¶) = ùëÖ(ùë•, ùë¶) ‚ãÖ ùêø(ùë•, ùë¶), 1 ‚â§ ùë• ‚â§ ùëù, 1 ‚â§ ùë¶ ‚â§ ùëû                                (9) 

Where I is a face image with the gray-scale of p*q. I(x, y) represents the pixel intensity at each 

pixel point (x, y) among the face image. Based on the previous researches [32, 33], R(x, y) and L(x, 

y)  are  the  corresponding  facial  intrinsic  feature  and  illumination  component,  respectively. Thus, 

separating L(x, y) from R(x, y) becomes an hot but ill-posed issue.   

In  order  to  solve  this  tricky  problem,  logarithm  transformation  is  introduced  to  transform  the 

multiplication model into concise addition model, forming Eq. (10). 

  ùëì(ùë•, ùë¶) = ùëôùëõ ùêº(ùë•, ùë¶) = ùëôùëõ ùëÖ(ùë•, ùë¶) + ùëôùëõ ùêø(ùë•, ùë¶)                                  (10) 

In  Eq(10),  f  performs  logarithm  transformation  on  the  original  face  image  I. What  should  be 

noticed is that, logarithm transformation can realize enhancement on original image and hold the 

corresponding intrinsic characteristics.   

3.2.1 LEF feature 

Weber-face [33] holds the opinion that the illumination component L(x, y) keeps consistent in 

local pixel area while the facial intrinsic feature R(x, y) changes abruptly. Considering two pixel 

points (x1, y1) and (x2, y2) in local area among the face image. The opinion can be expressed as: 

    L(ùë•1, y1) ‚âà L(ùë•2, y2)                                                    (11) 

Combining Eq.(10) and Eq. (11), we can get 

 ùëì(ùë•1, y1) ‚àí ùëì(ùë•2, y2) = ùëôùëõ ùëÖ(ùë•1, y1) + ùëôùëõ ùêø(ùë•1, y1) ‚àí (ùëôùëõùëÖ(ùë•2, y2) + ùëôùëõ ùêø(ùë•2, y2)) 

                                   =  ùëôùëõ ùëÖ(ùë•1, y1) ‚àí ùëôùëõùëÖ(ùë•2, y2)                                                        (12) 

In this research, we formulate the kth LEF feature, which contains all the logarithm difference 

edgemaps, belonging to various local areas among Œ†k, as Eq.(13). 

          LEFùëò(x, y) = ‚àë (ùëì(ùë•, ùë¶) ‚àí ùëì(ùë•ùëñ, ùë¶ùëñ))
Œ†ùëò

= ‚àë (ùëôùëõùëÖ(ùë•, ùë¶) ‚àí ùëôùëõùëÖ(ùë•ùëñ, ùë¶ùëñ))

Œ†ùëò

, 

 
 
 
 
                                            (ùë•ùëñ , ùë¶ùëñ) ‚àà ùëÅùëíùëñùëî‚Ñéùëèùëúùë¢ùëü‚Ñéùëúùëúùëë ((ùë•, ùë¶); ùë†ùëñùëßùëí = ùëò)                              (13) 

where local areas Œ†1 ‚äÇ ‚ãØ ‚äÇ Œ†k-1 ‚äÇ Œ†k.   

Commonly,  the  illumination  robust  feature  extraction  performances,  which  we  defined  as 

discriminative power in this research, of multiple scale (Œ†k (k=1,2,3,4, ‚ãØ)), named as the kth LEF 

features, are different.   

3.2.2 Joint LEF feature 

Then, we define the joint LEF feature (JLEF feature) for the local area. JLEF feature can refine 

the performance of LEF feature by effective fusing all the LEFks among the local area.   

JLEF(x, y) = ‚àë (ùë§ùëî(ùëò) ‚àó LEFùëò(ùë•, ùë¶)

ùõº
ùëò=1

)  , Œ±=1,2,3,4,5‚Ä¶                    (14)   

where Œ± is the maximal region size of the local area, which is determined by the corresponding 

illumination level of the face image in this research. œâg (k) is the adaptive fusion weight of LEFk, 

which  is  used  to  adjust  the  importance  and  influence  of  the  corresponding  LEFk  based  on  its 

performance.   

3.2.3 Adaptive parameter determination 

The Yale B face database [30] with covering a wide range of illumination variations is selected 

to  conduct  the  parameter  determination  experiments.  According  to  the  illumination  dividing 

technique in Section 2.1, the 2432 face images in Extended Yale B database are redivided into ILlevel 

1-3 subset, accompanying by varying illumination levels from slight to severe, which contain 196, 

949, 1203. In ILlevel1 subset, all the face images are under normal illumination condition. For the 

ILlevel2 subset, the illumination conditions are with minor scale cast shadows. For the ILlevel3 subset, 

the illumination conditions are with major scale cast shadows.   

We exploit the redivided Yale B to estimate the adaptive parameter of JLEF feature and AJLEF-

face associated with each ILlevel. Our experiments are composed of two parts. 1)The single training 

set consists of the first image of each person in subset 1 (i.e. clean training images), and the rest 

images of Yale B construct the testing set. 2) The first image of each person in subset 5 in the original 

Yale B database (i.e. unclean training images) forms the single training set, and the Yale B images 

excluding training ones are designated to test. The nearest neighbor classifier based on Euclidean 

distance  is  adopted  for  the  final  classification.  In  this  research,  all  parameter  estimations  of  the 

proposed methods employ the unclean single training set. 

The discriminative power of the kth LEF feature in the redivided subset can be estimated via the 

experiments. Here, the single training set in the experiment is consisted of unclean training images 

(the first image of each person in subset 5 in original Yale B database). Fig. 4 shows the recognition 

results of the multiple scale (Œ†k (k=1,2,3, ‚ãØ,10)) LEFks in each subset. The most important is, Fig. 

4 indicates that LEFks show the best performances when k=5, 4 and 3 for ILlevel1, ILlevel2 and ILlevel3 

subsets, respectively.   

Fig. 4. The recognition rates of LEFk under various k. 

From Fig.4, the recognition rates of all the LEFks in one subset can form the LEF-performance 

set which reflects the performances of all the LEFks. Take the ILlevel3 subset in Yale B as example, 

LEF-performance =  [92.5  93.4  95.9  95.1  93.7  92.2  91.8  91.1  90.3  89.2], which just  reflects  the   

importance  and  influence  of  the  corresponding  LEFks  for  face  recognition  in  ILlevel3  subset. 

However, due to the small gaps between the parameters among LEF-performance, the performances 

of effective LNN-features cannot make significant achievement in JLEF feature. Thus, we perform 

Softmax operation on the parameters in LEF-performance to realize normalization and highlight 

maximum probability, forming the corresponding œânormal set.   

In addition, according to Fig.4, the LEFks' performance curves are similar to Gaussian distribution. 

Aiming at further increasing the gaps between œânormals, the Gaussian function is utilized to forming 

Gaussian Weights set  ùúîùëî, performing effective feature extraction, showing in Eq.(16). 

                                     œânormal(ùëò) =

ùëíLEF‚àíperformance(k)
ùõº
ùëò=1

ùëíLEF‚àíperformance(k)

‚àë

      Œ±=1,2,3,4,5‚Ä¶                              (15) 

                                    ùúîùëî(ùëò) = exp (‚àíùúîùëõùëúùëüùëöùëéùëô(ùëò)2/2ùúé2)    Œ±=1,2,3,4,5‚Ä¶                            (16) 

where œâg (k) is the kth element in w. Through Eq.(15), the more power the LEFk is, the larger the 

corresponding weight (œâg (k)) of LEFk is. Eq.(16) enlarges the differences between the parameters 

among  LEF-performance,  forming  the  final  weight  set  ùúîùëî.  The  same  as  Fig.4,  Fig.5  shows  the 

performance for the JLEF feature under different numbers of LEFk s and various œÉ2 in each subset. 

Not surprisingly, the values of ks for the best performances of JLEF features in ILlevel1, ILlevel2 and 

 
ILlevel3  subsets  are  the  same  to  LEFks‚Äô.  In  addition,  for  Subset  ILlevel1,  the  best  performance  is 

appeared when œÉ2=2. When it comes to ILlevel2 and ILlevel3, œÉ2=1 lead to the best performances.       

(a)  Subset ILlevel1                        (b) Subset ILlevel2                        (c) Subset ILlevel3 

Fig. 5. The recognition rates of JLEF feature under various k and œÉ2. 

3.3. AJLEF-face 

However, the JLEF-feature is proposed under the assumption that the illumination components 

in  local  area are  approximately  the same.  In  fact,  noise points and  illumination  shadows always 

attack the local area. Thus, the illumination component in Eq.(9) cannot be ignored. Thus, we define 

the new illumination component in Eq.(17). 

ùêø(ùë•, ùë¶) = (1 + ùúÄùëñ,ùëó)ùêø(ùë•ùëñ, ùë¶ùëó), 

(ùë•ùëñ , ùë¶ùëñ) ‚àà ùëÅùëíùëñùëî‚Ñéùëèùëúùë¢ùëü‚Ñéùëúùëúùëë ((ùë•, ùë¶); ùë†ùëñùëßùëí = ùëò)                                (17) 

In  Eq.(17),  Œµi,j  reflects  the  illumination  variation  between  the  central  pixel  point  (x,  y)  with 

corresponding neighbor point (xi, yj). L(x, y) represents the illumination component of pixel point 

(x,  y). k  is the  maximal  region  size of  the local  area, which  is determined  by  the corresponding 

illumination level of the face image in this research. 

Then, the new calculating model is formed (Eq.(18)).   

ùõº

JLEF(x, y) = ‚àë (ùúîùëî(ùëò) ‚àë (ùëôùëõùëÖ(ùë•, ùë¶) ‚àí ùëôùëõùëÖ(ùë•ùëñ, ùë¶ùëñ))

) 

ùëò=1

Œ†ùëò

 ln (1 + ùúÄùëñ,ùëó) ‚Üí 0), Œ± = 1,2,3,4,5 ‚Ä¶                                                      (18) 

Compared to Gradient-face [32] and Weber-face [33], JLEF-feature is just determined by ln (1 + 

Œµi,  j),  resulting  in  better  processing  capacity  for  illumination  changes. The  derivation  process  for 

Gradient-face [32] and Weber-face [33] and JLEF-feature are shown in Appendix A. 

The effect of ln (1 + Œµi, j) to JLEF-feature is shown in Fig.6, which performs the distribution of 

gray values of the JLEF-features for the two face images belonging to one individual under various 

illumination  conditions.  Based  on  the assumption  of  JLEF-feature,  these  distributions  should  be 

 
 
 
consistent, resulting in Œµi, j ‚âà 0. However, Fig.6 shows two different distributions, indicates that the 

JLEF-feature is inevitably influenced by ln (1 + Œµi, j), which cannot treated as zero in the model. 

  In addition, it also validates that the influences of varying lighting can be divided into two parts, 

one is the high frequency noises including polluted points as well as edges of cast shadows, the 

other one is the distortions on the high frequency features of face, just like edges of eyes, mouth and 

nose. The latter influence can also explain the poor performance of deep learning methods on the 

face  dataset  influenced  by  severe  illumination  variations,  due  to  its  requirement  for  the  similar 

variations of training and validation images. 

Fig. 6. The JLEF-feature (Œ±=1) distributions of the face images belonging to the same individual in subset ILlevel1 

and ILlevel3. 

Above all, the influences of ln (1 + Œµi,j) to JLEF-feature are the useless high-frequency features 
produced by varying illumination. However, it is uneasy to distinguish the influence of ln (1 + Œµi,j)   
from useful high-frequency facial feature such as eyes, mouth and nose. What is more, the useful 
facial features are easily polluted by severe illumination changes. 

In this research, we adopt the saturation function to constrain the useless high-frequency feature 
in  JLEF-feature.  We  introduce  the  sigmoid  function  to  address  the  high-frequency  influence  in 
JLEF-feature, getting the adaptive JLEF face (AJLEF-face). 

  AJLEF ‚àí face(x, y) =

1

1+ùëí‚àíùõøùêΩùêøùê∏ùêπ(ùë•,ùë¶)                                            (19) 

    In Eq.(19), Œ¥ represents the gain and proportion for the slope of unsaturated part in sigmoid 
function. 

Fig.7  performs  the  recognition  performances  of AJLEF-face  in  different  subset  under  varying 
values of Œ¥. We can see that AJLEF-face gets the best performance (100%) when Œ¥=3 and 4 in subset 
ILlevel1, Œ¥=3 in subset ILlevel2 and Œ¥=2 in subset ILlevel1. However, the more larger the value of Œª is, 
the more distorted information of JLEF-feature appeared in saturated part. Thus, we choose Œª=3 for 
AJLEF-face in subset ILlevel3.   

In all, according to the above research on Yale B benchmark database, the adaptive values for œÉ2 
and Œª for different illumination level are performed in Table3. The clean images in Yale B may not 
be as bright as the clean ones in other face databases such as AR [45] or LFW [46], however, the 

 
 
 
 
 
adaptive values are generic for all the clean images, resulting in the same adaptive values in ILlevel0 
and ILlevel1. Moreover, Yale B may contain the darkest face image. Hence, ILlevel4 is assigned the 
same adaptive values as ILlevel3. 

Fig. 7. The recognition rates of AJLEF-face under different values of Œ¥. 

Table 3 

The corresponding values for œÉ2 and Œª in various IL-level. 

Subset 

ILlevel0 
ILlevel1 
ILlevel2 
ILlevel3 
ILlevel4 

k 

5 
5 
4 
3 
3 

œÉ2 
2 
2 
1 
1 
1 

Œ¥ 

3 
3 
3 
2 
        2 

3.4. Algorithm framework 

Above all, the whole processing procedure is performed in Fig.8. In addition, Fig.9 shows some 
JLEF  features  and  their  corresponding AJLEF-faces.  We  can  see  that  JLEF  features  own  better 
visual quantities, but AJLEF-faces can get better recognition rates under varying illumination.   

Fig. 8. The whole algorithm framework 

f(x,y)2ed LEF feature SVDclassifierECILIllumination level judgementJLEF-feature1st LEF featurekth LEF featureadaptive fusionmultiple scalesprocessing...constrain operationAJLEF-faceInput face imageI (x,y)logarithm transformation  
 
 
 
 
Fig.  9.  The  JLEF  features  and  their  corresponding AJLEF-faces.  1st  column  (left):  pixel  images;  2nd  column: 

logarithm images; 3rd to 5th columns: JLEF features corresponding to k: 3, 4, 5 in Eq. (14); 6th to 8th column: 

AJLEF-faces corresponding to Œ¥: 1, 2, 3 in Eq. (19). 

4 EXPERIMENTS 

This research conducts JLEF-feature and AJLEF-face methods on Extended Yale B [30], CMU-

PIE [31], AR [45], along with our Self-build Driver database to extract the illumination-invariant 

 
 
 
 
 
 
 
 
 
 
feature  used  in  face  recognition.  The  state-of-the-art  methods  including  deep  learning  based 

methods are taken into comparison. 

4.1. Database Description 

Yale  B  is  a  famous  public  database  which  is  constructed  and  released  by  Yale  University, 

containing  38  individuals'  face  images  taken  in  9  different  poses  and  64  varying  illumination 

conditions.  This  research  chooses  all  the  frontal  images  of  the  38  individuals,  focusing  on  the 

images' illumination issue.     

CMU  PIE  is  also  a  commonly  used  face  database,  which  includes  41368  face  images  of  68 

individuals  based  on  illumination,  expression  and  varying  pose.  The  P27  subset  in  CMU  PIE, 

consisted of 1428 images, is taken in 21 different illumination backgrounds, respectively. Thus, it 

is chose as our experiment sample. 

The AR database [33] includes two subsets, containing no less than 4000 face images for 126 

individuals. This research utilizes subset 1 as test bed. 

To simulate the validity of this research in real traffic scenarios, we construct the SDB database 

for research. Fig. 10 performs all the cropped face images of one driver. SBD database is consisted 

of  28  individuals.  Each  individual  has  22  face  images  taken  indoor  (12  images)  and  in  car  (10 

images).   

Fig. 10. The cropped face images of one individual in SDB database 

(a) incar(b) indoor(c) varying illumination condition 
 
 
 
4.2. Experimental Setting 

Baseline.  Average  recognition  rate  (ARR)  is  proposed  in  our  experiment  to  evaluate  the 

performances of the compared methods. The gallery image varies from the first image to the last 

one for one individual, in turn. Then, the other images of the same individual are used as probes. In 

this way, the test time is the same as the total images' amount for one individual in the dataset. Then, 

ARR  is the average  level  for  all  the test  results.  AAE  is  a  relative fair  result  compared to  other 

related researches [5-19], which use limited gallery images. Due to the deep learning methods, deep 

lambertian network (DLN) [42] and SSAE [40], need a large amount of training samples, we choose 

1792 face images of the corresponding 28 individuals in Yale B database as training set. Then, the 

rest images for other 10 persons form the testing set.   

The proposed approach. JLEF-feature and AJLEF-face. 

Original  and  LOG.  Original  image  is  the  face  image  with  raw  pixels.  When  the  logarithm 

transformation is made on original image, the LOG image is produced. These two style of images 

undergo no processing, remaining the original facial features used in face recognition. Thus, the 

extended sparse representation classifier (ESRC) [44] is utilized as classification approach here.   

Deep learning method. The related deep learning approaches include MADE [40], SSAE [19] as 

well 

as  VGG 

[41].  The 

basic 

architecture 

of  SSAE 

is 

employed 

from 

http://www.cs.toronto.edu/_hinton/MatlabForSciencePaper.html. We perform the same experiment 

set with the original setting in DLN.   

Illumination-invariant feature extraction approaches. This research takes LOG-DCT [11], LTV 

[12],  MSF  [14],  Gradient-face  [32],  MSLDE  [15]  as  well  as  Weber-face  [33]  into  comparison. 

According  to  previous  study,  Œª  =0.4  is  set  for  LTV  and  Œª  =  0.1  is  set  for  MFS.  The  common 

parameter  Œª  in  gaussian  kernel  filter  for  Weber-face  and  MSLDE  method  is  set  to  1.  The  basic 

architecture  codes  for  Log-DCT, Gradient-face,  MSF  together with  Weber-face were  introduced 

from  http://luks.fe.uni-lj.si/sl/osebje/vitomir/  face  tools/INFace/index.html.  The  original  code  for 

LTV was referred from http://www.caam.rice.edu/_wy1/ParaMaxFlow/2007/06/binarb-code.html.   

4.3. Performance Analyzation 

The experiment performances of the above metheds on Extended Yale B, CMU-PIE, AR and 

SBD  are  performed  in  Table  4  and  Table  5.  Moreover,  Fig.  9  performs  part  JLEF-features  and 

AJLEF-faces on Yale B and CMU-PIE databases. Based on the above, we can get the conclusion 

that the performances of JLEF-feature and AJLEF-face are excellent compared to other state-of-

the-art methods for extract illumination-invariant representations utilized for face recognition.   

(a)  Performance on Extended Yale B database. 

This research chooses subset 3, 4 and 5 in YaleB dataset, which are constructed under small scale 

cast  shadow,  moderate  scale  cast  shadow  and  large  scale  cast  shadow,  respectively,  to  carry 

experiment. Even though the face images in Yale B database are influenced by varying illumination 

conditions, the corresponding facial inherent features still can be extracted. Just like Fig. 9 shows, 

after logarithm changing, the inherent edges and facial features are revealed. However, previous 

related  methods  [5-19]  show  unsatisfactory  extraction  performances,  especially  on  the  subsets 

which are influenced by serious illumination. 

According to Fig. 11 and Table 4, the JLEF-feature and AJLEF-face proposed by our research 

perform more efficient feature extraction under varying illumination condition, especially in serious 

illumination  condition.  Some  previous  approaches  perform  excellent  performances  under  the 

experiment  method  that  the  gallery  set  contains  only  one  frontal  face  image  under  normal 

illumination  condition.  However,  in  our  experiment  setting,  which  takes  AAE  (mentioned  in 

baseline)  to  judge  the  whole  performance,  their  performances  are  lower  than  JLEF-feature  and 

AJLEF-face.  Just  like  the  performances  of  MSF  and  MSLDE,  which  are  high  in  their  original 

researches.  When  tested  under  ARR,  which  shows  the average  extraction  level  and  relative  fair 

result,  their  recognition  performances  cannot  keep  excellent.  On  subset  3  and  4  in  YaleB,  their 

recognition rates are lower than JLEF-feature and AJLEF-face by gaps of 10% more. 

Among all the compared methods, the recognition accuracies of LOG-DCT, LTV, Gradient-face, 

Weber-face and MSLDE on subset 3 and 4 fall behind subset 5. Thus, we can get the conclusion 

that they are more suitable for the dataset influenced by large scale cast shadow. Yet, due to lack of 

training samples, the deep learning techniques, including MADE, SSAE and VGG, cannot achieve 

good performances, especially on subset 5, which owns relative less samples. Fig. 11 and Table 4 

show the recognition performances of all the compared methods in the whole Yale B database. 

Table 4   

The average recognition rates of all the compared techniques in Yale B database 

Approach 

Original 

LOG 

Subset3 

33.06% 

    40.63% 

Subset4 

19.52% 

  31.24% 

Subset5 

15.42% 

32.16% 

Total 

20.32% 

22.42% 

LOG-DCT [11] 

  77.44% 

68.24% 

LTV [12] 

MSF [14] 

Gradient-face [32] 

Weber-face [33] 

MSLDE [15] 

MADE [40] 

SSAE [19] 

VGG [41] 

JLEF-feature 

AJLEF-face 

76.62% 

84.82% 

78.22% 

79.42% 

71.42% 

43.41% 

44.23% 

36.64% 

98.01% 

98.66% 

66.43% 

69.46% 

73.51% 

76.36% 

73.98% 

29.26% 

28.18% 

23.64% 

86.39% 

87.54% 

93.22% 

81.97% 

83.99% 

91.02% 

95.34% 

95.44% 

20.31% 

21.76% 

17.08% 

96.04% 

97.22% 

77.95% 

67.63% 

69.85% 

80.75% 

83.95% 

82.08% 

29.12% 

30.17% 

21.73% 

86.29% 

88.61% 

Fig. 11. Average recognition accuracy of the compared techniques in the whole Yale B database. 

(b)  Results on CMU PIE, AR and SDB databases. 

1) Results on CMU PIE. From Table 5, the JLEF-feature and AJLEF-face still outperform other 

comparative methods on CMU PIE database, ranking 1st and 2ed. Due to the fact that the illumination 

condition  in  CMU  PIE  database  is  not  so  various  as  it  in  Yale  B.  Thus,  the  differences  of  the 

compared methods' performances on CMU PIE database are not so remarkable. Not surprisingly, 

deep  learning  methods,  including  MADE,  SSAE  and  VGG,  still  cannot  achieve  satisfied 

performances,  owning  to  less  training  samples  under  similar  illumination  condition  as  well  as 

unsupervised  learning  setting.  Compared  to  other  methods,  the  JLEF-feature  and  AJLEF-face 

contain  the  restriction  factor  for  illumination  component  and  enhancement  factor  for  facial 

component to ensure the identity invariance. Thus, the superiority of JLEF-feature and AJLEF-face 

for undersampled face recognition on CMU PIE database is significant. Fig. 12 indicates the average 

performances of the related methods in the whole CMU PIE database. 

 
 
 
2) Results on AR and SBD. According to Table 5, AJLEF-face shows best recognition accuracy on 

AR, owning the excellent illumination robust feature extraction ability. According to Fig. 10, the 

illumination  condition  in  SBD  are  moderate.  Not  surprisingly,  JLEF-feature  and  AJLEF-face 

outperform other compared methods on SBD, whereas the margins are not as large as on other face 

datasets.   

Table 5   

The average recognition rates (%) of the above methods in CMU PIE, AR and SBD databases 

Approach 

CMU PIE 

AR 

  Original 

LOG 

29.81% 

30.36% 

LOG-DCT[11] 

70.65% 

LTV [12] 

71.18% 

Gradient-face [32] 

87.66% 

Weber-face [33] 

88.90% 

MSLDE [15]   

90.11% 

MFS [14] 

SSAE [19] 

VGG [41] 

JLEF-feature 

AJLEF-face 

59.12% 

68.29% 

50.03% 

90.64% 

91.33% 

53.51% 

67.12% 

56.11% 

60.12% 

57.39% 

60.42% 

63.28% 

43.71% 

50.94% 

47.07% 

68.88% 

71.02% 

SBD 

32.38% 

35.30% 

51.79% 

53.75% 

60.42% 

63.38% 

68.94% 

50.91% 

42.26% 

49.80% 

72.56% 

74.72% 

Fig. 12. The average performances of the related methods in the whole CMU PIE database. 

5. Conclusion 

In  this  paper,  we  propose  JLEF-feature  and  AJLEF-face  methods  based  on  illumination  level 

classification,  extracting  the  illumination-invariant  feature  as  well  preserving  inherent  facial 

information. Comparative trials on Yale B, CMU-PIE, AR as well as SBD databases show that the 

proposed methods own better performances when compared to other feature extraction techniques, 

 
 
 
including LOG-DCT, MSF, Gradient-face, LTV, Weber-face and MSLDE, as well as related deep 

learning approaches, including MADE, VGG and SSAE. It indicates that JLEF-feature and AJLEF-

face outperform other state-of-the-art methods used in undersampled face recognition affected by 

varying illumination. In the next research, we will improve this work to make it not only equipped 

with  illumination  robustness  but  also  pose  and  occlusion  robustness  by  adjusting  the  algorithm 

structure. 

Acknowledgments   

This  work  was  supported  by  the  National  Natural  Science  Foundation  of  China  (No.61871123 

&No.61802203), Key Research and Development Program in Jiangsu Province (No.BE2016739), 

Natural Science Foundation of Jiangsu Province (No.BK20180311) and a Project Funded by the 

Priority Academic Program Development of Jiangsu Higher Education Institutions. 

Declarations of interest   

None. 

Appendix A 

The influences of illumination difference Œµi,j to Weber-face, Gradient-face and JLEF-feature are as 

below. 

Weber ‚àí face = arctan (‚àë

ùêº(ùë•, ùë¶) ‚àí ùêº(ùë•ùëñ, ùë¶ùëó)
ùêº(ùë•, ùë¶)

) 

Œ†ùëò

= arctan (‚àë

= arctan (‚àë

= arctan (‚àë

= arctan (‚àë

Œ†ùëò

Œ†ùëò

Œ†ùëò

Œ†ùëò

ùëÖ(ùë•, ùë¶)ùêø(ùë•, ùë¶) ‚àí ùëÖ(ùë•ùëñ, ùë¶ùëó)ùêø(ùë•ùëñ, ùë¶ùëó)
ùëÖ(ùë•, ùë¶)ùêø(ùë•, ùë¶)

) 

ùëÖ(ùë•, ùë¶)ùêø(ùë•, ùë¶) ‚àí ùëÖ(ùë•ùëñ, ùë¶ùëó)(1 + ùúÄùëñ,ùëó)ùêø(ùë•, ùë¶)
ùëÖ(ùë•, ùë¶)ùêø(ùë•, ùë¶)

) 

ùëÖ(ùë•, ùë¶) ‚àí ùëÖ(ùë•ùëñ, ùë¶ùëó)(1 + ùúÄùëñ,ùëó)
ùëÖ(ùë•, ùë¶)

) 

ùëÖ(ùë•, ùë¶) ‚àí ùëÖ(ùë•ùëñ, ùë¶ùëó)
ùëÖ(ùë•, ùë¶)

‚àí

ùëÖ(ùë•ùëñ, ùë¶ùëó)
ùëÖ(ùë•, ùë¶)

ùúÄùëñ,ùëó) 

=  arctan (‚àë

Œ†ùëò

ùëÖ(ùë•, ùë¶) ‚àí ùëÖ(ùë•ùëñ, ùë¶ùëó)
ùëÖ(ùë•, ùë¶)

) ,

ùëÖ(ùë•ùëñ, ùë¶ùëó)
ùëÖ(ùë•, ùë¶)

ùúÄùëñ,ùëó ‚Üí 0  

 
 
 
Gradient ‚àí face = arctan (

ùúïùë¶ùêº(ùë•, ùë¶)
ùúïùë•ùêº(ùë•, ùë¶)

) 

= arctan (

ùúïùë¶ùëÖ(ùë•, ùë¶)ùêø(ùë•, ùë¶)
ùúïùë•ùëÖ(ùë•, ùë¶)ùêø(ùë•, ùë¶)

) 

= arctan (

ùëÖ(ùë•, ùë¶ + ‚àÜùë¶ùëó)ùêø(ùë•, ùë¶ + ‚àÜùë¶ùëó) ‚àí ùëÖ(ùë•, ùë¶)ùêø(ùë•, ùë¶)
‚àÜùë¶ùëó
ùëÖ(ùë• + ‚àÜùë•ùëñ, ùë¶)ùêø(ùë• + ‚àÜùë•ùëñ, ùë¶) ‚àí ùëÖ(ùë•, ùë¶)ùêø(ùë•, ùë¶)
‚àÜùë•ùëñ

) 

ùëÖ(ùë•, ùë¶ + ‚àÜùë¶ùëó)(1 + ùúÄùëñ,ùëó)ùêø(ùë•, ùë¶) ‚àí ùëÖ(ùë•, ùë¶)ùêø(ùë•, ùë¶)
‚àÜùë¶ùëó
ùëÖ(ùë• + ‚àÜùë•ùëñ, ùë¶)(1 + ùúÄùëñ,ùëó)ùêø(ùë•, ùë¶) ‚àí ùëÖ(ùë•, ùë¶)ùêø(ùë•, ùë¶)
‚àÜùë•ùëñ

)

= arctan

(

= arctan (

ùëÖ(ùë•, ùë¶ + ‚àÜùë¶ùëó) ‚àí ùëÖ(ùë•, ùë¶) + ùëÖ(ùë•, ùë¶ + ‚àÜùë¶ùëó)ùúÄùëñ,ùëó
‚àÜùë¶ùëó
ùëÖ(ùë• + ‚àÜùë•ùëñ, ùë¶) ‚àí ùëÖ(ùë•, ùë¶) + ùëÖ(ùë• + ‚àÜùë•ùëñ, ùë¶)ùúÄùëñ,ùëó
‚àÜùë•ùëñ

) 

= arctan (

ùëÖ(ùë•, ùë¶ + ‚àÜùë¶ùëó) ‚àí ùëÖ(ùë•, ùë¶)
‚àÜùë¶ùëó
ùëÖ(ùë• + ‚àÜùë•ùëñ, ùë¶) ‚àí ùëÖ(ùë•, ùë¶)
‚àÜùë•ùëñ

+

+

ùëÖ(ùë•, ùë¶ + ‚àÜùë¶ùëó)ùúÄùëñ,ùëó
‚àÜùë¶ùëó
ùëÖ(ùë• + ‚àÜùë•ùëñ, ùë¶)ùúÄùëñ,ùëó
‚àÜùë•ùëñ

) 

= arctan (

ùúïùë¶ùëÖ(ùë•, ùë¶) +

ùúïùë•ùëÖ(ùë•, ùë¶) +

ùëÖ(ùë•, ùë¶ + ‚àÜùë¶ùëó)ùúÄùëñ,ùëó
‚àÜùë¶ùëó
ùëÖ(ùë• + ‚àÜùë•ùëñ, ùë¶)ùúÄùëñ,ùëó
‚àÜùë•ùëñ

) 

= arctan (

ùúïùë¶ùëÖ(ùë•,ùë¶)
ùúïùë•ùëÖ(ùë•,ùë¶)

), 

ùëÖ(ùë•,ùë¶+‚àÜùë¶ùëó)ùúÄùëñ,ùëó
‚àÜùë¶ùëó

‚Üí 0,

ùëÖ(ùë•+‚àÜùë•ùëñ,ùë¶)ùúÄùëñ,ùëó
‚àÜùë•ùëñ

‚Üí 0   

JLEF ‚àí feature = ‚àë

ùõº
ùëò=1

(ùúîùëî(ùëò) ‚àó LEFùëò(ùë•, ùë¶))

ùõº

 = ‚àë (ùúîùëî(ùëò) ‚àë (ùëì(ùë•, ùë¶) ‚àí ùëì(ùë•ùëñ, ùë¶ùëñ))
)
Œ†ùëò

ùëò=1

ùõº

= ‚àë (ùúîùëî(ùëò) ‚àë ((ùëôùëõùëÖ(ùë•, ùë¶) + ùëôùëõùêø(ùë•, ùë¶)) ‚àí (ùëôùëõùëÖ(ùë•ùëñ, ùë¶ùëñ) + ùëôùëõùêø(ùë•ùëñ, ùë¶ùëñ))
)

= ‚àë (ùúîùëî(ùëò) ‚àë ((ùëôùëõùëÖ(ùë•, ùë¶) + ùëôùëõùêø(ùë•, ùë¶)) ‚àí (ùëôùëõùëÖ(ùë•ùëñ, ùë¶ùëñ) + ln (1 + ùúÄùëñ,ùëó) + ùëôùëõùêø(ùë•, ùë¶))

)

= ‚àë (ùúîùëî(ùëò) ‚àë (ùëôùëõùëÖ(ùë•, ùë¶) ‚àí ùëôùëõùëÖ(ùë•ùëñ, ùë¶ùëñ) ‚àí ln(1 + ùúÄùëñ,ùëó))
)

 = ‚àë (ùúîùëî(ùëò) ‚àë (ùëôùëõùëÖ(ùë•, ùë¶) ‚àí ùëôùëõùëÖ(ùë•ùëñ, ùë¶ùëñ))

), ln (1 + ùúÄùëñ,ùëó) ‚Üí 0),

Œ± = 1,2,3,4,5 ‚Ä¶     

Œ†ùëò

ùëò=1

ùõº

ùõº

ùëò=1

ùëò=1

ùõº
ùëò=1

Œ†ùëò

Œ†ùëò

Œ†ùëò

 
 
 
 
 
 
 
                                           
       
 
                                                                                               
 
 
 
Appendix B 
Some processed results of the compared methods: original, logarithm processing, SQI, weber-face 
, gradient-face, MSLDE, JLEF-feature and AJLEF-face. 

 
 
 
 
 
 
 
 
 
 
 
References 

[1] W. Deng, J. Wu and J. Guo, ‚ÄúExtended SRC: Undersampled face recognition via intraclass variant dictionary‚Äù, IEEE Trans. Pattern 

Anal. Mach. Intell., vol. 34, no. 9, pp. 1864-1870, Jan. 2012. 

[2] L. Wei, and Y. F. Wang, ‚ÄúUndersampled face recognition via robust auxiliary dictionary learning‚Äù, IEEE Trans. Image Process., 

vol. 24, no. 6, pp. 1722-1734, Mar. 2015. 

[3] Chiang H H, Chen Y L, Wu B F, et al. Embedded Driver-Assistance System Using Multiple Sensors for Safe Overtaking Maneuver[J]. 

IEEE Systems Journal, 2017, 8(3):681-698. 

[4] Clara Marina Martinez, Mira Heucke, Fei-Yue Wang, et al. Driving Style Recognition for Intelligent Vehicle Control and Advanced 

Driver Assistance: A Survey[J]. IEEE Transactions on Intelligent Transportation Systems, 2018, PP(99):1-11. 

[5] Y. Su, S. Shan, X. Chen, and W. Gao, ‚ÄúAdaptive generic learning for face recognition from a single sample per person,‚Äù in Proc. IEEE 

Conf. Comput. Vis. Pattern Recognit., Jun. 2010, pp. 2699‚Äì2706. 

[6] X. Tan, S. Chen, Z.-H. Zhou, and F. Zhang, ‚ÄúFace recognition from a single image per person: A survey,‚Äù Pattern Recognit., vol. 39, 

no. 9, pp. 1725‚Äì1745, 2006.   

[7] Liu H D, Yang M, Gao Y, et al (2014). Local histogram specification for face recognition under varying lighting conditions [J]. Image 

& Vision Computing, 32(5):335-347. 

[8] R.C. Gonzales and R.E. Woods (2002). Digital image processing. Prentice-Hall, NJ: Upper Saddle River. 

[9] W. Zhao, R. Chellappa (2001). Symmetric shape-from-shading using self-ratio image, Int. J. Comput. Vis. 45:55-75. 

[10]  Liu  C,  Wechsler  H  (2002).  Gabor  feature  based  classification  using  the  enhanced  fisher  linear  discriminant  model  for  face 

recognition[J]. IEEE Transactions on Image Processing A Publication of the IEEE Signal Processing Society, 11(4): 467.   

[11]  W.  Chen,  M.J.  Er,  S.  Wu  (2006).  Illumination  compensation  and  normalization  for  robust  face  recognition  using  discrete  cosine 

transform in logarithm domain, IEEE Trans.Syst. Man Cybern. Part B: Cybern. 36: 458-466. 

[12] Chen T, Yin W, Zhou X S, et al (2006). Total Variation Models for Variable Lighting Face Recognition[J]. IEEE Transactions on 

Pattern Analysis & Machine Intelligence, 28(9): 1519. 

[13] A. Shashua, T. Riklin-Raviv (2001). The quotient image: class-based re-rendering and recognition with varying illuminations, IEEE 

Trans. Pattern Anal. Mach. Intell. 23: 129-139.   

[14] T. Zhang, B. Gang, Y. Yuan, Y. Y. Tang, Z. Shang, D. Li and F. Lang, ‚ÄúMultiscale facial structure representation for face recognition 

under varying illumination‚Äù, Pattern Recognit., vol. 42, no. 2, pp. 251-258, Feb. 2009. 

[15] Z. R. Lai, D. Q. Dai, C. X. Ren and K. K. Huang, ‚ÄúMultiscale Logarithm Difference Edgemaps for Face Recognition Against Varying 

Lighting Conditions‚Äù, IEEE Trans. Image Process., vol. 24, no. 6, pp. 1735-1747, Mar. 2015. 

[16] C. Hu, M. Ye, S. Ji, W. Zeng and X. Lu, ‚ÄúA new face recognition method based on image decomposition for single sample per person 

problem‚Äù, Neurocomputing, vol. 160, pp. 287-299, Jul. 2015. 

[17] M. Kan, S. G. Chan, Y. Zhou, D. Xu and X. L. Chen, ‚ÄúAdaptive discriminant learning for face recognition‚Äù, Pattern Recognition, 

vol. 46, no. 9, pp. 2497-2509, Sep. 2013. 

[18] B. Wan, W. F. Li, Z. M. Li and Q. M. Liao, ‚ÄúAdaptive linear regression for single-sample face recognition‚Äù, Neurocomputing, vol. 

115, pp. 186-191, Sep. 2013. 

[19] S. Gao, Y. Zhang, K. Jia, J. Lu, and Y. Zhang (2015). Single sample face recognition via learning deep supervised autoencoders, IEEE 

Trans. Inform. Forensics Security, 10(10), p 2108-2118. 

[20] R. Marc‚ÄôAurelio, F. J. Huang, Y.-L. Boureau, and Y. LeCun, ‚ÄúUnsupervised learning of invariant feature hierarchies with applications 

to object recognition,‚Äù in CVPR, 2007. 

[21] T. D. Kulkarni, W. F. Whitney, P. Kohli, and J. Tenenbaum, ‚ÄúDeep convolutional inverse graphics network,‚Äù in NIPS, 2015. 

[22] Q. V. Le et al., ‚ÄúBuilding high-level features using large scale unsupervised learning,‚Äù in Proc. 29th Int. Conf. Mach. Learn., 2012, 

pp. 81‚Äì88. 

[23] A. Krizhevsky, I. Sutskever, and  G. E. Hinton, ‚ÄúImageNet classification with deep convolutional neural networks,‚Äù in  Proc. Adv. 

Neural Inf.Process. Syst., 2012, pp. 1106‚Äì1114. 

[24] G. E. Hinton, S. Osindero, and Y.-W. Teh, ‚ÄúA fast learning algorithm for deep belief nets,‚Äù Neural Comput., vol. 18, no. 7, pp. 1527‚Äì

1554, 2006. 

[25] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol, ‚ÄúStacked denoising autoencoders: Learning useful representations 

in a deep network with a local denoising criterion,‚Äù J. Mach. Learn. Res., vol. 11, no. 5, pp. 3371‚Äì3408, 2010. 

[26]  D.  Erhan,  Y.  Bengio,  A.  Courville,  P.-A.  Manzagol,  P.  Vincent,  and  S.  Bengio,  ‚ÄúWhy  does  unsupervised  pre-training  help  deep 

learning?‚ÄùJ. Mach. Learn. Res., vol. 11, pp. 625‚Äì660, Feb. 2010. 

[27] Z. Zhu, P. Luo, X. Wang, and X. Tang, ‚ÄúMulti-view perceptron: a deep model for learning face identity and view representations,‚Äù in 

NIPS, 2014. 

[28] J. Yim, H. Jung, B. Yoo, C. Choi, D. Park, and J. Kim, ‚ÄúRotating your face using multi-task deep neural network,‚Äù in CVPR, 2015. 

[29] Luan Quoc Tran, Xi Yin, Xiaoming Liu. Representation Learning by Rotating Your Faces[J]. IEEE Transactions on Pattern Analysis 

and Machine Intelligence, 2017, PP(99):1-1. 

[30] Georghiades A S, Belhumeur P N, Kriegman D J (2001). From Few to Many: Illumination Cone Models for Face Recognition under 

Variable Lighting and Pose[J]. IEEE Transactions on Pattern Analysis & Machine Intelligence, 23(6): 643-660. 

[31] T. Sim, S. Baker, M. Bsat (2003). The CMU pose, illumination, and expression database, IEEE Trans. Pattern Anal. Mach. Intell. 25: 

1615‚Äì1618. 

[32] T. Zhang, Y.Y. Tang, B. Fang, Z. Shang, X. Liu, Face recognition under varying illumination using gradient faces, IEEE Trans. Image 

Process. 18 (2009)2599‚Äì2606. 

[33] B. Wang, W. Li, W. Yang, Q. Liao, Illumination normalization based on weber's law  with application to face recognition, IEEE Signal 

Process. Lett. 18 (2011) 462‚Äì465. 

[34] H. Andrews and C. Patterson, ‚ÄúSingular value decompositions and digital image processing‚Äù, IEEE Trans. Acoust., Speech, Signal 

Process., vol. 24, no. 1, pp. 26-53, Feb. 1976. 

[35] J. Liu, S. Chen and X. Tan, ‚ÄúFractional order singular value decomposition representation for face recognition‚Äù, Pattern Recognit., vol. 

41, no. 1, pp. 378-395, Jan. 2008. 

[36] J. W. Wang, J. S. Lee and W. Y. Chen, ‚ÄúFace recognition based on projected color space with lighting compensation‚Äù, IEEE Signal 

Process. Lett., vol. 18, no. 10, pp. 567-570, Oct. 2011. 

[37] W. Kim, S. Suh, W. Hwang and J. J. Han, ‚ÄúSVD face: illumination-invariant face representation‚Äù, IEEE Signal Process.Lett., vol. 21, 

no. 11, pp. 1336-1340, Nov. 2014. 

[38] T. Zhang, Y.Y. Tang, B. Fang, Z. Shang, X. Liu, Face recognition under varying illumination using gradient faces, IEEE Trans. Image 

Process. 18 (2009)2599‚Äì2606. 

[39] B. Wang, W. Li, W. Yang, Q. Liao, Illumination normalization based on weber's law  with application to face recognition, IEEE 

Signal Process. Lett. 18 (2011) 462‚Äì465. 

[40] P. J. S. Vega, R. Q. Feitosa, V. H. A Quirita and P. N. Happ, ‚ÄúSingle Sample Face Recognition from Video via Stacked Supervised 

Auto-Encoder‚Äù, 2016 29th SIBGRAPI Conference on Graphics, Patterns and Images, pp. 96-103, Oct. 2016. 

[41] O. M. Parkhi, A. Vedaldi and A. Zisserman, ‚ÄúDeep face recognition‚Äù, in Proc. Brit. Mach. Vis. Conf., pp. 1-12, Sep. 2015. 

[42] Y. Tang, R. Salakhutdinov, and G. E. Hinton, ‚ÄúDeep Lambertian networks,‚Äù in Proc. 29th Int. Conf. Mach. Learn., 2012, pp. 1623‚Äì

1630. 

[43] G. E. Hinton and R. R. Salakhutdinov, ‚ÄúReducing the dimensionality of data with neural networks‚Äù, science, vol. 313, no. 5786, pp. 

504-507, Jul. 2006. 

[44] W. Deng, J. Hu, J. Guo, Extended SRC: undersampled face recognition via intraclass variant dictionary, IEEE Trans. Pattern Anal. 

Mach. Intell. 34,1864-1870 (2012). 

[45] A. M. Martinez and R. Benavente, ‚ÄúThe AR face database‚Äù,CVC Tech. Rep. #24, Jun. 1998. 

[46] G. B. Huang, M. Ramesh, T. Berg and E. Learned-Miller, ‚ÄúLabeled faces in the wild: A database for studying face recognition in 

unconstrained environments‚Äù, Technical Report,Dept. Comput. Sci., Univ. Massachusetts, Amherst, MA, USA, Tech. Rep. 07-49, Oct. 

2007. 

[47] Xin Yu, Basura Fernando, Bernard Ghanem, Fatih Porikli, and Richard Hartley. Face super-resolution guided by facial component 

heatmaps. In ECCV, pages 217‚Äì233, 2018. 

[48] Xin Yu, Basura Fernando, Richard Hartley, and Fatih Porikli. Super-resolving very low-resolution face images with supplementary 

attributes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 908‚Äì917, 2018. 

[49] Xin Yu and Fatih Porikli. Ultra-resolving face images by discriminative generative networks. In ECCV, pages 318‚Äì333, 2016. 

[50] Xin Yu and Fatih Porikli. Face hallucination with tiny unaligned images by transformative discriminative neural networks. In AAAI, 

2017. 

[51]  Xin  Yu  and  Fatih  Porikli.  Hallucinating  very  lowresolution  unaligned  and  noisy  face  images  by  transformative  discriminative 

autoencoders. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3760‚Äì3768, 2017. 

[52] Xin Yu and Fatih Porikli. Imagining the unimaginable faces by deconvolutional networks. IEEE Transactions on Image Processing, 

27(6):2747‚Äì2761, 2018. 

[53]  Xin  Yu,  Fatih  Porikli,  Basura  Fernando,  and  Richard  Hartley.  Hallucinating  unaligned  face  images  by  multiscale  transformative 

discriminative networks. International Journal of Computer Vision, 128(2):500‚Äì526, 2020. 

[54] Xin Yu, Fatemeh Shiri, Bernard Ghanem, and Fatih Porikli. Can we see more? joint frontalization and hallucination of unaligned tiny 

faces. IEEE transactions on pattern analysis and machine intelligence, 2019. 

[55] Xin Yu, Basura Fernando, Richard Hartley, and Fatih Porikli. Semantic face hallucination: Super-resolving very low-resolution face 

images with supplementary attributes. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2019. 

 
