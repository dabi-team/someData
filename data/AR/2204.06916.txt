2
2
0
2

r
p
A
4
1

]

C
H
.
s
c
[

1
v
6
1
9
6
0
.
4
0
2
2
:
v
i
X
r
a

Should I Follow AI-based Advice? Measuring Appropriate Reliance in
Human-AI Decision-Making

MAX SCHEMMER, Karlsruhe Institute of Technology, Germany
PATRICK HEMMER, Karlsruhe Institute of Technology, Germany
NIKLAS KÃœHL, Karlsruhe Institute of Technology, Germany
CARINA BENZ, Karlsruhe Institute of Technology, Germany
GERHARD SATZGER, Karlsruhe Institute of Technology, Germany

Many important decisions in daily life are made with the help of advisors, e.g., decisions about medical treatments or financial

investments. Whereas in the past, advice has often been received from human experts, friends, or family, advisors based on artificial

intelligence (AI) have become more and more present nowadays. Typically, the advice generated by AI is judged by a human and

either deemed reliable or rejected. However, recent work has shown that AI advice is not always beneficial, as humans have shown to

be unable to ignore incorrect AI advice, essentially representing an over-reliance on AI. Therefore, the aspired goal should be to enable

humans not to rely on AI advice blindly but rather to distinguish its quality and act upon it to make better decisions. Specifically,

that means that humans should rely on the AI in the presence of correct advice and self-rely when confronted with incorrect advice,

i.e., establish appropriate reliance (AR) on AI advice on a case-by-case basis. Current research lacks a metric for AR. This prevents a

rigorous evaluation of factors impacting AR and hinders further development of human-AI decision-making. Therefore, based on the

literature, we derive a measurement concept of AR. We propose to view AR as a two-dimensional construct that measures the ability

to discriminate advice quality and behave accordingly. In this article, we derive the measurement concept, illustrate its application and

outline potential future research.

CCS Concepts: â€¢ Human-centered computing â†’ HCI theory, concepts and models; Empirical studies in HCI; â€¢ Computing
methodologies â†’ Artificial intelligence.

Additional Key Words and Phrases: Human-AI Decision-Making, Appropriate Reliance, Human-AI teams

ACM Reference Format:

Max Schemmer, Patrick Hemmer, Niklas KÃ¼hl, Carina Benz, and Gerhard Satzger. 2022. Should I Follow AI-based Advice? Measuring

Appropriate Reliance in Human-AI Decision-Making. In CHI Conference on Human Factors in Computing Systems (CHI â€™22), Workshop

on Trust and Reliance in AI-Human Teams (trAIt), April 30, 2022, New Orleans, LA, USA. 10 pages.

1 INTRODUCTION

For many important decisions in life, we seek the opinion of advisors. While in the past, advice was typically obtained

from human experts, nowadays, advisors based on artificial intelligence (AI) are becoming more and more present in

research and practice [11]. For example, AI now advises medical professionals with regard to breast cancer screening

[19], or in detecting COVID-19 pneumonia [5]. Past research has often focused on maximizing the utilization of

advice [13, 27, 30], i.e., increasing the amount of accepted advice leading to increased compliance. Even though this

might be valid for human advice seeking, a different paradigm might be more suitable in the scenario of human-AI

decision-making due to a change in objectives. Research on human advice is often based on the perspective of the

advisor, who is most likely interested in high advice utilization [30]. For example, when considering a bank that offers

advice for investment decisions, the investments will have different potential outcomes for the bank itself. Therefore, the

bank is interested in the client following its advice as it has â€œstakesâ€ in the decision. However, the critical difference in

the human-AI decision-making setting is that the advice seekers can design and develop the advisor based on personal

1

 
 
 
 
 
 
CHI â€™22, TRAIT, April 30, 2022, New Orleans, LA, USA

Schemmer et al.

goals. If those goals are to maximize decision-making performance, blindly following AI advice does not necessarily

lead to the best possible outcome [1]. For this reason, researchers argue that in order to benefit the most from AI advice,

humans need to be able to appropriately rely on it [1, 2, 38]. We define appropriate reliance (AR) in AI advice as the

humanâ€™s ability to differentiate between correct and incorrect AI advice and to act upon that discrimination.

As AI becomes more important in our daily lives, both professionally and privately, humans need to be able to

discriminate between correct and incorrect advice. If their discrimination capabilities are sufficient, they could even

benefit from an AI that performs worse on average compared to them. However, the discriminating ability is not the

only factor of appropriate reliance as humans need not only to discriminate but also to adapt their decisions accordingly.

For example, in theory, they could be capable of detecting errors but do not dare to contradict AI advice due to a

disproportionately high level of trust in or perceived authority/skill of the AI.

To enable humans to rely on AI advice appropriately, we need to have a precise understanding of AR and measure it

coherently. Currently, there is no specific measurement for AR with regard to AI advice, and researchers are using

many different measurement concepts. Therefore, we derive a measurement concept for AR in the context of AI advice

based on literature in automation on AR [16, 31] and organizational psychology [30]. Subsequently, we illustrate our

measurement concept through a behavioral experiment. Lastly, we discuss possible avenues for future research.

The remainder of this article is structured as follows: In Section 2, we first outline related work on AR in the context of

human-AI decision-making. In Section 3, we propose a measurement concept capturing AR, followed by an illustration

drawn from a user study in Section 4. In Section 5 we discuss AR and provide ideas for future work. Section 6 concludes

our work.

2 RELATED WORK

Historically, many researchers have worked on AR with regard to automation [16] and robotics [31]. In the following,

we will provide an overview of the most common definitions. Fundamental work in the context of AR in automation

has been laid by Lee and See [16]. The authors outline the relationship between â€œappropriate trustâ€ and AR in their

work. However, they do not define AR explicitly but provide examples of inappropriate reliance, such as â€œmisuse and

disuse are two examples of inappropriate reliance on automation that can compromise safety and profitabilityâ€ [16, p.

50]. Other researchers go one step further and define inappropriate reliance as under- or over-reliance [18, 32, 37].

Wang et al. [33] define appropriate reliance as the impact of reliance on performance. For example, they discuss the

situation in which automation reaches a reliability of 99%, and the human performance is 50%. In their opinion, it would

be appropriate to always rely on AI as this would increase performance. Talone [31] follows the work by Wang et al.

[33] and defines AR as â€œthe pattern of reliance behavior(s) that is most likely to result in the best human-automation

team performanceâ€ [31, p. 13]. Both see appropriate reliance as a function of team performance.

Recent work in human-AI decision-making has started to discuss AR in the context of AI advice. Lai et al. [14] gives

an overview of empirical studies that analyze AI advice considering AR. For example, Chandrasekaran et al. [4] analyze

whether humans can learn to predict the model behavior. This ability is associated with an improved ability to rely on

the modelâ€™s predictions for the right cases. Moreover, Gonzalez et al. [8] evaluate the impact of explainable AI (XAI) on

the discrimination of incorrect and correct AI advice. Similarly, Poursabzi-Sangdeh et al. [23, p. 1] point out the idea of

AR in the form of â€œmaking people more closely follow a modelâ€™s predictions when it is beneficial for them to do so or

enabling them to detect when a model has made a mistakeâ€. However, the authors do not explicitly relate this idea to

the concept of AR. In this context, additional work uses the term â€œappropriate trustâ€ with a similar interpretation as the

behavior to follow â€œthe fraction of tasks where participants used the modelâ€™s prediction when the model was correct

2

Measuring Appropriate Reliance in Human-AI Decision-Making

CHI â€™22, TRAIT, April 30, 2022, New Orleans, LA, USA

and did not use the modelâ€™s prediction when the model was wrongâ€ [34, p. 323]. Finally, also Yang et al. [35, p. 190]

define â€œappropriate trust is to [not] follow an [in]correct recommendationâ€. All these articles have in common that

they consider AR or appropriate trust on a case-by-case basis. Similar to our work, BuÃ§inca et al. [2] analyze whether

cognitive forcing functions can reduce over-reliance on AI advice, which is measured as the percentage of agreement

with the AI when the AI makes incorrect predictions. Bussone et al. [3] assess how explanations impact trust and

reliance on clinical decision support systems. The authors partition reliance into over- and self-reliance as part of their

study. However, they use a qualitative approach to answer their research questions. To summarize, previous research

does not provide a unified measurement concept that allows measuring AR on AI advice.

3 DERIVING A MEASUREMENT CONCEPT OF APPROPRIATE RELIANCE ON AI ADVICE

Despite several studies having examined human-AI interaction with regard to reliance, an agreed-upon definition of

AR is still missing. We, therefore, initiate our research by deriving a definition of AR. To provide an accurate definition,

we first analyze the two terms â€œappropriateâ€ and â€œrelianceâ€ individually.

Reliance. Reliance itself is defined as a behavior [7, 16]. This means it is neither a feeling nor an attitude but the actual

action conducted. Defining reliance as behavior also clarifies the role of trust, which is defined as â€œthe attitude that an

agent will help achieve an individualâ€™s goals in a situation characterized by uncertainty and vulnerabilityâ€ [16, p. 51]. In

general, research has shown that trust increases reliance, but it can also take place without trust being present [16]. For

example, we might not trust the banking advisor but consciously decide that the advice is still the best possible decision.

The final reliance is beyond trust and is also influenced by other attitudes such as perceived risk or self-confidence [25].

Appropriateness. After establishing a common understanding of reliance, we proceed by defining â€œappropriatenessâ€.

The appropriateness of reliance stems from the fact that current AI is imperfect, i.e., it may provide erroneous advice.

This erroneous advice can be divided into systematic errors and random errors [31]. While humans can identify

systematic errors, random errors have no identifiable patterns and can not be distinguished. These different errors allow

differentiation between two cases of AR. If all errors are random and cannot be detected, then humans should always

rely on AI if, on average, AI performs better and never rely if AI performs worse on average [31]. However, suppose

there are some systematic errors, depending on the discrimination capabilities. In that case, humans might be able to

differentiate between correct and incorrect advice, which may even result in superior performance compared to the

scenario of AI and humans conducting the task alone [9]. This changes the overall discrimination to a case-by-case

discrimination. In the presence of systematic errors, humans should evaluate each case individually. Since the solution

approach in the presence of just random errors is relatively simple, as pointed out above, in this article, we focus on the

more complicated setting when a significant proportion of task instances inhibit systematic errors.

For AR in the presence of systematic errors, we see two main aspects. First, humans need to be able to differentiate

between correct and incorrect advice. Second, people need to act upon their discrimination accordingly due to its

behavioral nature. For instance, a human decision-maker might be able to differentiate between correct and incorrect AI

advice but has a too high level of trust to reject the advice of the AI. Thus, AR consists of the capability to discriminate

and execute the consequent behavior.

Definition 1. Appropriate reliance on AI advice is

a) the human capability to differentiate between correct and incorrect AI advice and

b) to act upon that discrimination.

3

CHI â€™22, TRAIT, April 30, 2022, New Orleans, LA, USA

Schemmer et al.

Now that we defined AR for our study, we derive a corresponding measurement. Current metrics in literature do not

capture the ability to discriminate AI advice, including the final decision by the human. For example, the weight on

advice (WOA) metric measures advice utilization [30]. This means the metric does not differentiate between correct or

incorrect advice but instead measures the share of taken advice. Furthermore, performance metrics blur the effect of

AR. For example, if AI has higher performance on a task than a human, blindly relying on AI without differentiating

between correct and incorrect advice might increase the team performance. However, it will not result in the desired

outcome that team performance exceeds the one of humans or AI conducting the task alone. For this reason, one cannot

assume AR after a performance increase. This results in the need for a measurement concept that reflects how well

humans are able to discriminate between correct and incorrect advice.

Following judge-advisor literature [30], we propose to study AR in a sequential human-AI decision-making setup with

two steps of human decision-making. Table 1 gives an overview of the different combinations based on a classification

task. Note that for simplicity, we refer to classification problems. However, the measurement concept can be extended

to regression problems as well. We consider a sequential decision process which can be described as follows: First,

the human makes a decision, then receives AI advice. Second, the human is asked to update the initial decision, i.e.,

either adopt or overwrite the AI advice. This allows measuring AR in a fine-granular way. For example, the initial

human decision can either be correct or incorrect in the classification setting. The follow-up AI advice can then either

confirm the humanâ€™s initial decisionâ€”or contradict it. We call the two contradictory cases positive and negative AI

advice. For any further analysis, we propose to focus on the two contradictory cases as the confirmation cases do

not allow to measure the humanâ€™s discrimination ability and blur the actual discrimination capability. In general, if

we do not consider the initial human decision, information about the human discrimination ability, including the

consequent action, gets lostâ€”it is not traceable how the human would have decided without the AI advice. Nevertheless,

especially this interaction needs to be documented to research AR holistically. To give an example, imagine a case

where the AI gives 10 times correct advice and 10 times incorrect advice. The human is initially 10 times correct. Now

the question is, how are these 10 correct initial decisions distributed over the AI advice. If, for example, 8 correct initial

decisions are followed by correct AI advice, this leads to a high accuracy after receiving correct AI advice. However, it

is not distinguishable whether the resulting accuracy results from a good discrimination ability or other factors, e.g.,

uncertainty in oneâ€™s own decision followed by confirmation by the AI prediction. By considering the initial human

decision and focusing on the contradiction cases, the influence of confounding factors that can lead to misinterpretations

can be minimized.

Initial human
decision
Correct
Correct
Incorrect
Incorrect
Incorrect
Incorrect
Correct
Correct

AI
advice
Correct
Correct
Incorrect
Incorrect
Correct
Correct
Incorrect
Incorrect

Relationship between initial human Human decision after
receiving AI advice
Correct
Incorrect
Correct
Incorrect
Correct
Incorrect
Correct
Incorrect

decision and AI advice
Confirmation
Confirmation
Confirmation
Confirmation
Positive advice (PA)
Positive advice (PA)
Negative advice (NA)
Negative advice (NA)

Reliance

n/a
n/a
n/a
n/a
Positive AI reliance
Negative self-reliance
Positive self-reliance
Negative AI reliance

Table 1. Effect of AI advice on reliance.

4

Measuring Appropriate Reliance in Human-AI Decision-Making

CHI â€™22, TRAIT, April 30, 2022, New Orleans, LA, USA

After receiving AI advice, humans need to decide whether to rely on AI or self-rely. We can now classify four types

of reliance. First, positive AI-reliance, which describes the case when the human is initially incorrect, receives correct

advice and relies on that advice. Second, the case in which the human relies on the initial incorrect decision and neglects

positive AI advice. This is denoted as negative self-reliance. Third, if the human is initially correct and receives incorrect

advice, this can either result in positive self-reliance, i.e., neglecting the incorrect AI advice, or relying on it, which is

denoted as negative AI-reliance. We can now observe that the positive effect of AI-reliance and self-reliance depends on

the quality of the AI advice. Therefore, we propose to measure AR on two dimensions.

On the first dimension, we calculate the ratio of cases where the human relies on correct AI advice under the condition

that the decision was initially not correct, i.e., in which the human rightfully changes his mind to follow the AI advice.

ğ‘…ğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’ ğ´ğ¼ ğ‘Ÿğ‘’ğ‘™ğ‘–ğ‘ğ‘›ğ‘ğ‘’ (ğ‘…ğ´ğ¼ğ‘…) =

ğ‘ƒğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’ ğ´ğ¼ ğ‘Ÿğ‘’ğ‘™ğ‘–ğ‘ğ‘›ğ‘ğ‘’
ğ‘ƒğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’ ğ´ğ¼ ğ‘Ÿğ‘’ğ‘™ğ‘–ğ‘ğ‘›ğ‘ğ‘’ + ğ‘ ğ‘’ğ‘”ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ ğ‘ ğ‘’ğ‘™ ğ‘“ -ğ‘Ÿğ‘’ğ‘™ğ‘–ğ‘ğ‘›ğ‘ğ‘’

(1)

On the second dimension, we propose to measure the relative amount of positive self-reliance in the presence of

negative advice.

ğ‘…ğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’ ğ‘ ğ‘’ğ‘™ ğ‘“ -ğ‘Ÿğ‘’ğ‘™ğ‘–ğ‘ğ‘›ğ‘ğ‘’ (ğ‘…ğ‘†ğ‘…) =

ğ‘ƒğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’ ğ‘ ğ‘’ğ‘™ ğ‘“ -ğ‘Ÿğ‘’ğ‘™ğ‘–ğ‘ğ‘›ğ‘ğ‘’
ğ‘ ğ‘’ğ‘”ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ ğ´ğ¼ ğ‘Ÿğ‘’ğ‘™ğ‘–ğ‘ğ‘›ğ‘ğ‘’ + ğ‘ƒğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’ ğ‘ ğ‘’ğ‘™ ğ‘“ -ğ‘Ÿğ‘’ğ‘™ğ‘–ğ‘ğ‘›ğ‘ğ‘’

(2)

Fig. 1. Two-dimensional depiction of appropriate reliance.

Figure 1 highlights both dimensions. On the x-axis, we depict the relative positive AI-reliance (ğ‘…ğ´ğ¼ğ‘…), and on the
y-axis, the relative positive self-reliance (ğ‘…ğ‘†ğ‘…). The figure highlights the properties of the measurement concept. It
ranges on both dimensions between 0 and 1. As a baseline, we can consider a random decision. The random baseline
allows us to detect under- and over-reliance in a static case. If ğ‘…ğ‘†ğ‘… and ğ‘…ğ´ğ¼ğ‘… are below this threshold, we observe over-
5

Relative positive AI reliance (RAIR)Relative positive selft-reliance (RSR)1.01.00.00.50.5Optimal appropriate relianceLegend:Over-relianceUnder-relianceAppropriate relianceIllustrative upper boundary of appropriate reliance0.0CHI â€™22, TRAIT, April 30, 2022, New Orleans, LA, USA

Schemmer et al.

and under-reliance. More specifically, a ğ‘…ğ‘†ğ‘… below the threshold means that a human performs worse than by chance
in detecting incorrect AI advice, essentially representing an over-reliance on AI. Similarly, a ğ‘…ğ´ğ¼ğ‘… below the random
threshold means that a human differentiates correct AI advice worse than a random guess, essentially representing an

under-reliance on AI. We depict the decision-making threshold in Figure 1 to illustrate this reasoning. Furthermore, we
can use the space to analyze the effect of experimental treatments. For example, a treatment that increases ğ‘…ğ´ğ¼ğ‘… and
decreases ğ‘…ğ‘†ğ‘… actually just increases over-reliance on AI. Similarly, a condition that increases ğ‘…ğ‘†ğ‘… while decreasing
ğ‘…ğ´ğ¼ğ‘… points towards under-reliance.

We refer to the theoretical goal of having a ğ‘…ğ‘†ğ‘… and a ğ‘…ğ´ğ¼ğ‘… metric of â€œ1â€ as optimal AR. Most likely, this theoretical
goal will not be reached in any practical context as humans will not always be able to perfectly discriminate on a

case-by-case basis whether they should rely on AI advice. Furthermore, random errors will reduce AR as they cannot be
discriminated against. Therefore, optimal AR will most likely be a theoretical goal. Lastly, the area with ğ‘…ğ´ğ¼ğ‘… and ğ‘…ğ‘†ğ‘…
larger than the random threshold encompasses the proportion of final decisions that did not occur by chance. Therefore,

AR is defined as every combination larger than the random threshold. In Figure 1 it refers to the right top quadrant. This
means AR is not binary but a tuple of ğ‘…ğ´ğ¼ğ‘… and ğ‘…ğ‘†ğ‘… above the random threshold with the theoretical optimum of â€œ1â€.
To illustrate our measurement concept, in the following Section 4, we describe the results of an experimental study.

4 ILLUSTRATION OF APPROPRIATE RELIANCE ON AI ADVICE

To illustrate the proposed measurement concept, we conducted a user study. The studyâ€™s goal is to highlight how

our measurement concept can be used to evaluate human-AI decision-making experiments with regard to AR. For

illustration, we focus on the explainability of AI advice as a design decision. XAI is intensively discussed in research

with regards to its impact on human-AI decision-making in general and AR in specific [1, 2, 14, 29].

To discriminate advice, humans need information that can approximate the quality of advice, e.g., the uncertainty

of the advisor or explanations [30]. The emerging research stream of XAI [10] aims to equip human users with such

insight into AI advice. Explanations might help the human decision-maker better judge the quality of an AI-based

decision. An analogy would be the interaction between a consultant, who provides advice, and his client. To assess

the quality of the advice, the client will ask the consultant to describe the reasoning. Based on the explanations, the

decision-maker should be able to determine whether the advice can be relied upon or not. The same logic should hold

for an AI advisor.

On the other hand, experimental studies indicate that explanations in human-AI decision-making can lead to over-

reliance [1, 38]. Research shows that explanations are sometimes interpreted more as a general sign of competence

[2] and have a persuasive character [3]. This is supported by psychology literature which has shown that human

explanations cause humans to agree even when the explanation is wrong [12]. The same effect could occur when

the explanations are generated by AI. Therefore, there exists an ambiguous trade-off between enabling the human

to discriminate the AIâ€™s advice and, on the other hand, the tendency that the sole existence of an explanation could

increase AI reliance [1]. In this illustrative study, we use this often discussed ambiguity to highlight the advantages of

our measurement concept.

As an experimental task, we have chosen a deceptive hotel review classification. Humans have to differentiate

whether a given hotel review is deceptive or genuine. Ott et al. [20, 21] provide the research community with a data

set of 400 deceptive and 400 genuine hotel reviews. The deceptive ones were created by crowd-workers, resulting in

corresponding ground truth labels.

6

Measuring Appropriate Reliance in Human-AI Decision-Making

CHI â€™22, TRAIT, April 30, 2022, New Orleans, LA, USA

The implemented AI is based on a Support Vector Machine with an accuracy of 86%, which is a performance that

is similar to the performance in related literature [15]. For the XAI condition, we use a state-of-the-art explanation

technique, namely LIME feature importance explanations [24]. Feature importance aims to explain the influence of an

independent variable on the AIâ€™s decision in the form of a numerical value. Since we deal with text data, a common

technique to display the values is to highlight the respective words according to their computed influence on the AIâ€™s

decision [15]. We additionally provide information on the direction of the effect and differentiate the values into three

effect sizes following the implementation of Lai et al. [15] (see step 2 in Figure 2).

Fig. 2. Online experiment graphical user interface for the XAI treatment. The ground truth of the exemplarily shown hotel review is
â€œfakeâ€. The design of the interface is adapted from Lai et al. [15].

For the AR measurement concept, a sequential task processing is essential. In our study, this means the human

first receives a review without any AI advice, i.e., just the plain text, and classifies whether the review is deceptive or

genuine (see step 1 in Figure 2). Following that, the human either receives a simple AI advice statement, e.g. â€œthe AI

predicts that the review is fakeâ€ or the AI advice and additional explanations (see step 2 in Figure 2). This sequential

two-step decision-making allows us to measure AR.

The participants were recruited using the platform Prolific.co. In total, we conducted the experiment with 200

participants. In each treatment, participants were provided with 16 reviewsâ€”8 correct and 8 incorrect ones.

We depict the results of the experiment in Figure 3. They highlight in the AI condition a high ğ‘…ğ‘†ğ‘… of 0.72 (Â±0.03)
and a relatively low ğ‘…ğ´ğ¼ğ‘… of 0.3 (Â±0.03). This indicates that humans in the setting were able to differentiate wrong AI
advice and self-rely to a high degree. The ğ‘…ğ´ğ¼ğ‘… of 0.3 shows that we can observe under-reliance on AI as the ğ‘…ğ´ğ¼ğ‘… is
below the random guess of our binary classification task. Our experiment thereby highlights the general tendency of

humans to ignore AI advice that is in literature usually discussed as algorithm aversion [6].

In the XAI condition, we can observe a significant increase (ğ‘¡ = âˆ’1.95, ğ‘ = 0.05) in ğ‘…ğ´ğ¼ğ‘… from 0.3 (Â±0.03) to 0.39
(Â±0.03) while the ğ‘…ğ‘†ğ‘… does not change significantly (0.7 Â± 0.03 for AI and 0.72 Â± 0.03 for XAI, ğ‘¡ = 0.61, ğ‘ = 0.54). This
means explanations of AI decisions can reduce under-reliance. It is important to highlight that under-reliance is not
reduced simply by relying more often on AI advice, as this would have also reduced the ğ‘…ğ‘†ğ‘… significantly. Thus, our
experiment indicates that explanations may have a positive effect on human-AI decision-making and do not necessarily

result in over-reliance. Our user study further illustrates the usage of our measurement concept and the potential to

apply it in experimental studies.

7

CHI â€™22, TRAIT, April 30, 2022, New Orleans, LA, USA

Schemmer et al.

Fig. 3. Illustration of appropriate reliance on AI advice including standard errors.

5 DISCUSSION

In this paper, we conducted a review on AR and proposed a measurement concept for its measurement in human-AI

decision-making. Subsequently, we illustrated this concept in the scope of a user study to highlight its capability. With

our approach, AR can be measured on a more fine-granular level. Thus, it can be leveraged in future experimental

studies to address questions on how to design for AR.

In this context, research needs to investigate the capability of AR to discriminate between incorrect and correct AI
advice and evaluate possible impact factors. For example, the potential to discriminate might be different between ğ‘…ğ‘†ğ‘…
and ğ‘…ğ´ğ¼ğ‘…. One would assume that it might be easier to discriminate negative advice than positive advice, as in the
negative advice condition, the human is initially, per definition, able to solve the task. In contrast, it might be challenging

to discriminate positive AI advice after failing to solve a task alone correctly. Our initial illustrative experiment also
showed these differences. The experiment further highlighted that while XAI might address ğ‘…ğ´ğ¼ğ‘…, it does not seem to
influence ğ‘…ğ‘†ğ‘…. Therefore, research needs to investigate the differences in detail and find proper ways to address them.
Furthermore, researchers need to investigate factors beyond the discrimination capability that influence the behavioral

part of AR. For instance, research models could incorporate attitudes as well as human biases that might influence

AR. Among others, important constructs that have been evaluated in the judge-advisor literature concerning advice

utilization are human confidence [17], perception [26] and trust [30]. Other research has shown the influence of

human bias on AR. For example, so-called egocentric discountingâ€”humans systematically overweighting their own

decisionsâ€”increases under-reliance on AI [36]. Some of these attitudes might enable enhanced discrimination, such

as engagement in AI advice [10]. Others could potentially harm AR. For example, maximizing trust in AI could lead

to â€œblind trustâ€ and consequently lead to a situation where humans accept all advice [1]. Similar phenomena could

happen in terms of cognitive constraints. Research in automation has shown that humans tend always to follow the

path of least cognitive effort [28]. Simply accepting could therefore be a preferred human decision. Research needs to

investigate these factors in future work.

8

0.00.20.40.60.81.0Relative positive AI reliance (RAIR)0.00.20.40.60.81.0Relative positive self-reliance (RSR)AI adviceXAI adviceMeasuring Appropriate Reliance in Human-AI Decision-Making

CHI â€™22, TRAIT, April 30, 2022, New Orleans, LA, USA

Lastly, we want to emphasize several limitations of the proposed measurement concept. First, the concept is limited

to classification tasks but will be extended in future work. First approaches can be found in the work of Petropoulos

et al. [22]. Furthermore, the sequential task setup necessary for our measurement concept has some disadvantages as it

changes the task itself. Since conducting the same task initially alone before receiving AI advice, the human is already

mentally prepared and might react differently than after directly receiving AI advice. Moreover, sequentially conducted

tasks with AI advice might not always be possible or desired in real-world cases. Therefore, the measurement should be

seen as an approximation of real human behavior. Instead of having a sequential task setup, one alternative option

could be to simulate a human model based on a data set of task instances solved by humans without AI advice. This

simulation model could approximate the initial human decision within a non-sequential task setting. However, also this

approach is an approximation of real human behavior. Future work should compare both approaches.

6 CONCLUSION

Many researchers highlighted the need for humans to rely on AI advice appropriately, i.e., being able to discriminate

AI advice quality and acting upon it for the best possible human-AI decision-making [1, 2, 18, 38]. However, current

research is missing a measurement concept for AR that allows the evaluation of human-AI decision-making experiments.

Therefore, in this article, we develop a new measurement concept for quantifying AR in the human-AI decision-

making context. Specifically, we propose to view AR as a two-dimensional construct that measures the capability to

discriminate the quality of advice and behave accordingly. The first dimension considers the relative positive effect

of relying on AI advice, whereas the second dimension assesses the relative positive self-reliance in the presence of

incorrect AI advice. Subsequently, we illustrate our measurement concept and provide an outlook on future research.

Our research provides a basis for future studies to evaluate the impact factors of AR and develop designs possibilities to

improve human-AI decision-making.

REFERENCES

[1] Gagan Bansal, Tongshuang Wu, Joyce Zhou, Raymond Fok, Besmira Nushi, Ece Kamar, Marco Tulio Ribeiro, and Daniel Weld. 2021. Does the whole
exceed its parts? the effect of ai explanations on complementary team performance. In Proceedings of the 2021 CHI Conference on Human Factors in
Computing Systems. 1â€“16.

[2] Zana BuÃ§inca, Maja Barbara Malaya, and Krzysztof Z Gajos. 2021. To trust or to think: cognitive forcing functions can reduce overreliance on AI in

AI-assisted decision-making. Proceedings of the ACM on Human-Computer Interaction 5, CSCW1 (2021), 1â€“21.

[3] Adrian Bussone, Simone Stumpf, and Dympna Oâ€™Sullivan. 2015. The role of explanations on trust and reliance in clinical decision support systems.

In 2015 international conference on healthcare informatics. 160â€“169.

[4] Arjun Chandrasekaran, Viraj Prabhu, Deshraj Yadav, Prithvijit Chattopadhyay, and Devi Parikh. 2018. Do explanations make VQA models more

predictable to a human?. In EMNLP.

[5] Muhammad EH Chowdhury, Tawsifur Rahman, Amith Khandakar, Rashid Mazhar, Muhammad Abdul Kadir, Zaid Bin Mahbub, Khandakar Reajul
Islam, Muhammad Salman Khan, Atif Iqbal, Nasser Al Emadi, et al. 2020. Can AI help in screening viral and COVID-19 pneumonia? IEEE Access 8
(2020), 132665â€“132676.

[6] Berkeley J Dietvorst, Joseph P Simmons, and Cade Massey. 2015. Algorithm aversion: people erroneously avoid algorithms after seeing them err.

Journal of Experimental Psychology: General 144, 1 (2015), 114.

[7] Mary T Dzindolet, Scott A Peterson, Regina A Pomranky, Linda G Pierce, and Hall P Beck. 2003. The role of trust in automation reliance. International

journal of human-computer studies 58, 6 (2003), 697â€“718.

[8] Ana Valeria Gonzalez, Gagan Bansal, Angela Fan, Robin Jia, Yashar Mehdad, and Srinivasan Iyer. 2020. Human evaluation of spoken vs. visual

explanations for open-domain qa. arXiv preprint arXiv:2012.15075 (2020).

[9] Patrick Hemmer, Max Schemmer, Michael VÃ¶ssing, and Niklas KÃ¼hl. 2021. Human-AI Complementarity in Hybrid Intelligence Systems: A Structured

Literature Review. PACIS 2021 Proceedings (2021).

[10] Robert R Hoffman, Shane T Mueller, Gary Klein, and Jordan Litman. 2018. Metrics for explainable AI: Challenges and prospects. arXiv preprint

arXiv:1812.04608 (2018).

9

CHI â€™22, TRAIT, April 30, 2022, New Orleans, LA, USA

Schemmer et al.

[11] Dominik Jung, Verena Dorner, Florian Glaser, and Stefan Morana. 2018. Robo-advisory. Business & Information Systems Engineering 60, 1 (2018),

81â€“86.

[12] Derek J Koehler. 1991. Explanation, imagination, and confidence in judgment. Psychological bulletin 110, 3 (1991), 499.
[13] Niklas KÃ¼hl, Jodie Lobana, and Christian Meske. 2019. Do you comply with AI?â€”Personalized explanations of learning algorithms and their impact

on employeesâ€™ compliance behavior. International Conference on Information Systems (ICIS) (2019).

[14] Vivian Lai, Chacha Chen, Q Vera Liao, Alison Smith-Renner, and Chenhao Tan. 2021. Towards a Science of Human-AI Decision Making: A Survey

of Empirical Studies. arXiv preprint arXiv:2112.11471 (2021).

[15] Vivian Lai, Han Liu, and Chenhao Tan. 2020. " Why isâ€™ Chicagoâ€™deceptive?" Towards Building Model-Driven Tutorials for Humans. In Proceedings of

the 2020 CHI Conference on Human Factors in Computing Systems. 1â€“13.

[16] John D Lee and Katrina A See. 2004. Trust in automation: Designing for appropriate reliance. Human factors 46, 1 (2004), 50â€“80.
[17] Duri Long and Brian Magerko. 2020. What is AI literacy? Competencies and design considerations. In Proceedings of the 2020 CHI conference on

human factors in computing systems. 1â€“16.

[18] Zhuoran Lu and Ming Yin. 2021. Human Reliance on Machine Learning Models When Performance Feedback is Limited: Heuristics and Risks. In

Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 1â€“16.

[19] Scott Mayer McKinney, Marcin Sieniek, Varun Godbole, Jonathan Godwin, Natasha Antropova, Hutan Ashrafian, Trevor Back, Mary Chesus, Greg S

Corrado, Ara Darzi, et al. 2020. International evaluation of an AI system for breast cancer screening. Nature 577, 7788 (2020), 89â€“94.

[20] Myle Ott, Claire Cardie, and Jeffrey T Hancock. 2013. Negative deceptive opinion spam. In Proceedings of the 2013 conference of the north american

chapter of the association for computational linguistics: human language technologies. 497â€“501.

[21] Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T Hancock. 2011. Finding deceptive opinion spam by any stretch of the imagination. arXiv preprint

arXiv:1107.4557 (2011).

[22] Fotios Petropoulos, Robert Fildes, and Paul Goodwin. 2016. Do â€˜big lossesâ€™ in judgmental adjustments to statistical forecasts affect expertsâ€™ behaviour?

European Journal of Operational Research 249, 3 (2016), 842â€“852.

[23] Forough Poursabzi-Sangdeh, Daniel G Goldstein, Jake M Hofman, Jennifer Wortman Wortman Vaughan, and Hanna Wallach. 2021. Manipulating

and measuring model interpretability. In Proceedings of the 2021 CHI conference on human factors in computing systems. 1â€“52.

[24] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. Model-agnostic interpretability of machine learning. arXiv preprint arXiv:1606.05386

(2016).

[25] Victor Riley. 2018. Operator reliance on automation: Theory and data. In Automation and human performance: Theory and applications. 19â€“35.
[26] Jakob Schoeffer, Yvette Machowski, and Niklas KÃ¼hl. 2022. Perceptions of Fairness and Trustworthiness Based on Explanations in Human vs.

Automated Decision-Making. HICSS (2022).

[27] Thomas Schultze, Anne-Fernandine Rakotoarisoa, and Stefan Schulz-Hardt. 2015. Effects of distance between initial estimates and advice on advice

utilization. Judgment & Decision Making 10, 2 (2015).

[28] Linda J Skitka, Kathleen L Mosier, and Mark Burdick. 1999. Does automation bias decision-making? International Journal of Human-Computer

Studies 51, 5 (1999), 991â€“1006.

[29] Alison Smith-Renner, Ron Fan, Melissa Birchfield, Tongshuang Wu, Jordan Boyd-Graber, Daniel S Weld, and Leah Findlater. 2020. No explainability
without accountability: An empirical study of explanations and feedback in interactive ml. In Proceedings of the 2020 chi conference on human factors
in computing systems. 1â€“13.

[30] Janet A Sniezek and Lyn M Van Swol. 2001. Trust, confidence, and expertise in a judge-advisor system. Organizational behavior and human decision

processes 84, 2 (2001), 288â€“307.

[31] Andrew Talone. 2019. The effect of reliability information and risk on appropriate reliance in an autonomous robot teammate. (2019).
[32] Peter-Paul Van Maanen, Tomas Klos, and Kees Van Dongen. 2007. Aiding human reliance decision making using computational models of trust. In

2007 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology-Workshops. 372â€“376.

[33] Lu Wang, Greg A Jamieson, and Justin G Hollands. 2008. Selecting methods for the analysis of reliance on automation. In Proceedings of the Human

Factors and Ergonomics Society Annual Meeting, Vol. 52. 287â€“291.

[34] Xinru Wang and Ming Yin. 2021. Are explanations helpful? a comparative study of the effects of explanations in ai-assisted decision-making. In 26th

International Conference on Intelligent User Interfaces. 318â€“328.

[35] Fumeng Yang, Zhuanyi Huang, Jean Scholtz, and Dustin L Arendt. 2020. How do visual explanations foster end usersâ€™ appropriate trust in machine

learning?. In Proceedings of the 25th International Conference on Intelligent User Interfaces. 189â€“201.

[36] Ilan Yaniv and Eli Kleinberger. 2000. Advice taking in decision making: Egocentric discounting and reputation formation. Organizational behavior

and human decision processes 83, 2 (2000), 260â€“281.

[37] Nirit Yuviler-Gavish and Daniel Gopher. 2011. Effect of descriptive information and experience on automation reliance. Human factors 53, 3 (2011),

230â€“244.

[38] Yunfeng Zhang, Q Vera Liao, and Rachel KE Bellamy. 2020. Effect of confidence and explanation on accuracy and trust calibration in AI-assisted

decision making. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. 295â€“305.

10

