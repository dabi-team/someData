6
1
0
2

l
u
J

1

]

G
L
.
s
c
[

1
v
6
4
1
0
0
.
7
0
6
1
:
v
i
X
r
a

Eﬃcient and Consistent Robust Time Series Analysis

Kush Bhatia∗

Prateek Jain∗

Parameswaran Kamalaruban#

Purushottam Kar†
∗Microsoft Research, Bangalore, India
@microsoft.com
t-kushb, prajain
}

{

#Australian National University, Canberra, Australia
kamalaruban.parameswaran@nicta.com.au
†Indian Institute of Technology Kanpur, India
purushot@cse.iitk.ac.in

Abstract

We study the problem of robust time series analysis under the standard auto-regressive (AR) time
series model in the presence of arbitrary outliers. We devise an eﬃcient hard thresholding based algorithm
which can obtain a consistent estimate of the optimal AR model despite a large fraction of the time series
points being corrupted. Our algorithm alternately estimates the corrupted set of points and the model
parameters, and is inspired by recent advances in robust regression and hard-thresholding methods.
However, a direct application of existing techniques is hindered by a critical diﬀerence in the time-series
domain: each point is correlated with all previous points rendering existing tools inapplicable directly.
We show how to overcome this hurdle using novel proof techniques. Using our techniques, we are also able
to provide the ﬁrst eﬃcient and provably consistent estimator for the robust regression problem where
a standard linear observation model with white additive noise is corrupted arbitrarily. We illustrate our
methods on synthetic datasets and show that our methods indeed are able to consistently recover the
optimal parameters despite a large fraction of points being corrupted.

1

Introduction

Several real world prediction problems, for instance, the temperature of a city, stock prices, traﬃc patterns,
the GPS location of a car etc are naturally modeled as time series. One of the most popular and simple
model for time series is the auto-regressive (AR (d)) model which models a given observation as a sample
from a distribution with mean given by a ﬁxed linear combination of previous d time series values. That is,
xt =

i + ǫi where ǫi is unbiased noise.

P

Unfortunately, in real life scenarios, time series tend to have several outliers. For example, traﬃc patterns
may get disrupted due to accidents and stock prices may get aﬀected by unforseen political or social inﬂuences.
The estimation of model parameters in the presence of such outliers is a classical problem in time-series
literature and is given a detailed treatment in several texts [11, 12].

d
i=1 w∗i xt

−

Existing time-series texts deﬁne two major outlier models: a) innovative outliers, b) additive outliers. In
innovative outliers, corrupted values become a part of the time series and inﬂuence future iterates i.e. if xt
is corrupted and we observe ˜xt = xt + bt then subsequent values xt′ (t′ > t) are obtained by using ˜xt rather
than xt. In the additive outlier model, on the other hand, although the observation of ˜xt is corrupted, the
time series itself continues using the clean value xt. Conventional wisdom in time series literature considers
innovative outliers to be “good” and helpful in spurring a shift in the time series [11]. Additive outliers, on
the other hand, are considered more challenging due to this latent behaviour in the model and can cause
standard estimators for the AR model to diverge.

1

 
 
 
 
 
 
Due to importance of the problem, several estimators have been proposed for the AR model under
corruption, e.g. the generalized M-estimator by [13]. However, most existing estimators are computationally
intractable (operate in exponential time) and do not oﬀer non-asymptotic guarantees.

Our goal in this work is to devise an eﬃcient and consistent estimator for the Robust Time Series
Estimation (RTSE) problem in the AR(d) model with non-asymptotic convergence guarantees in the presence
of a large number of outliers. To this end, we cast the model estimation problem as a sparse estimation
problem and use techniques from the sparse regression literature [10] to devise our hard-thresholding based
algorithm. At a high level, our algorithm locates the corrupted indices by using a projected gradient method
where the projection is onto the set of sparse vectors.

However, analyzing this technique proves especially challenging. While hard threshodling methods have
been extensively studied for sparse linear regression [10, 6, 23], similar techniques do not apply directly to
our problem because of two key challenges: a) in the time series domain, data points xt’s are dependent on
each other while sparse linear regression techniques typically assume independence of the data points, and
b) even for robust linear regression (where each row of data matrix is assumed to be independent), existing
analyses [5] are unable to guarantee consistent estimates.

Using a novel two-stage proof technique, we show that our method provides a consistent estimator for
the true model w∗ so long as the number of outliers k satisﬁes k = O( n
d log n ), where n is the total number
of points in the time series and d is the order of the model. Whenever k satisﬁes the above assumption, our
method in time ˜O(nd) outputs an estimate
w
. We direct
the reader to Theorem 9 for precise rates.

f (n) where f

k2 ≤

0 as n

w s.t.

→ ∞

w∗

→

−

k

In fact, using our techniques, we are also able to give a consistent estimator for the robust least squares
b
regression (RLSR) problem [22, 16, 5] even when a constant fraction of the responses are corrupted. Here
again, our algorithm runs in time ˜O(nd), where d is the dimensionality of the data. To the best of our
knowledge, our method is the ﬁrst eﬃcient and consistent estimator for the RLSR problem in the challenging
setting where a constant fraction of the responses can be corrupted.

b

We then study our methods empirically for both the robust time series analysis, as well as the standard
robust regression problems. Our methods demonstrate consistency for both problem settings. Moreover, our
results for robust time series show that the ordinary least squares estimate, that ignores outliers, provides
very poor estimators and hence, is signiﬁcantly less accurate. In contrast, our proposed method and a few
variants of it indeed recover the underlying AR(d) model accurately.

Paper Organization: Section 3 considers the “warm-up” problem of robust regression and presents our
algorithm and theoretical guarantees. We then, introduce the robust time series problem and our algorithm
and analysis in Section 4. Section 5 presents simulations on synthetic datasets.

2 Related Works

Time Series: Analysing time series with corruptions is a classical and widely studied problem in statistics
literature. In an early work, [13] proposed a generalized M-estimator for the RTSE problem in the additive
[11] detail a robust variant of the Durbin-Levinson
outlier (AO) model with a positive breakdown point.
algorithm for RTSE and demonstrate the eﬃcacy of the model empirically.
[20] provide an analysis of M-
estimators for RTSE with innovative outliers (IO), but show that the standard M-estimator has a break
down point of zero in the presence of AO. This shows that standard M-estimators cannot handle even a
non-zero fraction of corruptions. Recently, [8] proposed a method based on Least Trimmed Squares, which is
closely related to our method, and used Monte Carlo simulations to validate the eﬀectiveness of their method.
[15] present a method based on robust ﬁlters in the more powerful ARMA model. Most of the estimators
mentioned above are either not eﬃcient (i.e. exponential time complexity) or do not provide non-asymptotic
error rates. In contrast, we provide a consistent and nearly linear time algorithm that allows a large fraction
of points to be corrupted. Recently, [3] studied time series with missing values but their results do not extend
to cases with latent corruptions. Moreover, they consider the online setting as compared to the stochastic
setting considered by our method.

2

Robust Regression: The goal in RLSR is to recover a parameter using noisy linear observations that
are corrupted sparsely. RLSR is a classical problem in statistics, but computationally eﬃcient, provable
algorithms have been proposed only in recent years. The Least Trimmed Squares (LTS) method guarantees
consistency but in general requires exponential running time [17, 1, 2]. Recently [22, 16] proposed L1 norm
minimization based methods for RLSR but their analyses do not guarantee consistent estimates in presence
of dense unbiased i.i.d. noise. Recently, [5] proposed a hard thresholding style algorithm for RLSR but are
unable to guarantee better than O(σ) error in the estimation of w∗ where σ is the standard deviation of
noise. However, as detailed in section 3, their results holds in a weaker adversarial model than ours. In
contrast, we provide nearly optimal σ √d
√n error rates for our algorithm. [7] considers a stronger model where
along with the response variables, the covariates can also be corrupted. However, their result also do not
provide consistency guarantees and they can only tolerate k

n/√d corruptions.

≤

3 Robust Least Squares Regression

We use robust least squares regression (RLSR) as a warm up problem to introduce the tools, as well as
establish notation that will be used for our time-series analysis. We present the problem formulation,
propose our CRR algorithm, and then prove its consistency and robustness guarantees.

Problem Formulation and Notation: We are given a set of n data points X = [x1, x2, . . . , xn]

Rd are the covariates, y

Rn is the vector of responses generated as

where xi ∈

∈

y = X ⊤w∗ + b∗ + ǫ,

Rd

∈

n,

×

(1)

∈

Rd. The responses suﬀer two kinds of perturbations – dense white
for some true underlying model w∗
(0, σ2) that is chosen in an i.i.d. fashion independently of the data X and the model w∗, and
noise ǫi ∼ N
sparse adversarial corruptions in the form of b whose support is chosen independently of X, w∗ and ǫ. We
assume that b∗ is a k∗-sparse vector albeit one with potentially unbounded entries. The constant k∗ will be
called the corruption index of the problem. The above model is stronger than that of [5] which considers a
fully adaptive adversary. However, whereas [5] is unable to give a consistent estimate, we give an algorithm
CRR that does provide a consistent estimate. We also note that [5] is unable to give consistent estimates
even in our model. As noted in the next section, our result requires signiﬁcantly more ﬁne analysis; standard
ℓ2-norm style anlaysis by [5] seems unlikely to lead to a consistency result in the robust regression setting.
We will require the notions of Subset Strong Convexity and Subset Strong Smoothness similar to [5] and
| denote the matrix with columns
⊂
Rn similarly. λmin(X) and λmax(X) will denote, respectively, the

reproduce the same below. For any set S
in that set. We deﬁne vS for a vector v
smallest and largest eigenvalues of a square symmetric matrix X.

[n], let XS := [xi]i
∈

S ∈

Rp

∈

×|

S

n is said to satisfy the Subset Strong Convexity
Deﬁnition 1 (SSC and SSS Properties). A matrix X
∈
Property (resp. Subset Strong Smoothness Property) at level k with strong convexity constant λk (resp.
strong smoothness constant Λk) if the following holds:

×

Rp

λk ≤

min
=k
S
|
|

λmin(XSX ⊤S )

≤

max
=k
S
|
|

λmax(XSX ⊤S )

Λk.

≤

We refer the reader to the appendix for SSC/SSS bounds for Gaussian ensembles.

3.1 CRR: A Hard Thresholding Approach to Consistent Robust Regression
We now present our consistent method CRR for the RLSR problem. CRR takes a signiﬁcantly diﬀerent
approach to the problem than previous works. Instead of attempting to exclude data points deemed unclean,
CRR concentrates on correcting the errors instead. This allows CRR to work with the entire data set at all
times, as opposed Torrent [5] that work with a fraction of the data.

Starting with the RLSR formulation minw

2
2, we realize that given any
k∗
b of the corruption vector, the optimal model with respect to this estimate is given by the expression
(cid:13)
(cid:13)

X ⊤w

k0≤

b)

(y

Rp,

(cid:13)
(cid:13)

−

−

1
2

∈

b

k

estimate

b

3

Algorithm 1 CRR: Consistent Robust Re-
gression

Algorithm 2 CRTSE: Consistent Robust Time Series Estimation
Input: Time-series data yi, i =
d + 1, . . . , n, corruption index k,

Input: Covariates X = [x1, . . . , xn], responses
y = [y1, . . . , yn]⊤, corruption index k, tol-
erance ǫ

←

0, t
0,
X ⊤(XX ⊤)−1X
bt

bt−1(cid:13)

(cid:13)2 > ǫ do
−
HTk(PX bt + (I
−

←
←

1: b0
PX
2: while (cid:13)
(cid:13)
bt+1
3:
←
t + 1
t
4:
←
5: end while
6: return wt

(XX ⊤)−1X(y

bt)

−

←

PX )y)

−

,

{

}

σ
b

min

tolerance ǫ, time series order d, error trimming level
σ
yi,
}
−b
{
(yi−1, . . . , yi−d)⊤, X
X ⊤(XX ⊤)−1X, t
←
bt−1(cid:13)
bt
(cid:13)2 > ǫ do
k (cid:0)PX bt + (I
−

[x1, . . . , xn], y
0

←
0, b0

PX ) y(cid:1)

−
HTG

←

←

σ
b

←
←

1: yi = max
2: xi
PX
3: while (cid:13)
(cid:13)
bt+1
4:
←
t + 1
t
5:
←
6: end while
7: return wt

(XX ⊤)−1X(y

bt)

−

←

(y1, . . . , yn)⊤,

w = (XX ⊤)−
RLSR problem.

1X(y

−

b). Plugging this expression for

w into the formulation allows us to reformulate the

b

b

f (b) =

min
b
k0≤

k∗

k

1
2 k

(I

b
−

PX )(y

b)

2
2
k

−

(2)

1X. This greatly simpliﬁes the problem by casting it as a sparse parameter
where PX = X ⊤(XX ⊤)−
estimation problem instead of a data subset selection problem. CRR directly optimizes (2) by using a form
of iterative hard thresholding. At each step, CRR performs the following update: bt+1 = HTk(bt
f (bt)),
where k is a parameter for CRR. Any value k
k∗ suﬃces to ensure convergence and consistency. The hard
thresholding operator is deﬁned below.

−∇

≥

Deﬁnition 2 (Hard Thresholding). For any v
in descending order of their magnitudes. Then for any k
v = HTk(v) where
k and 0 otherwise.

vi = vi if σ−

1
v (i)

∈

≤

≤

Rn, let the permutation σv

Sn order elements of v
n, we deﬁne the hard thresholding operator as

∈

b

We note that CRR functions with a ﬁxed, unit step length, which is convenient in practice as it avoids
b
step length tuning, something most IHT algorithms [9, 10] require. For the RLSR problem, we will consider
(0, Σ). Since CRR interacts with the data only using the
data sets that are Gaussian ensembles i.e. xi ∼ N
projection matrix PX , one can assume , without loss of generality, that the data points are generated from
a standard Gaussian i.e. xi ∼ N
d). Our analysis will take care of the condition number of the data
ensemble whenever it is apparent.

(0, Id

×

3.2 Convergence and Consistency Guarantees

Rd, 1

Theorem 3. Let xi ∈
from a Gaussian distribution and let yi’s be
generated using (1) for a ﬁxed w∗ and let σ2 be the noise variance. Let the number of corruptions k∗ be s.t.
k∗
d))) steps,
k
ensures that

n/10000. Then, with probability at least 1
(σ

k2 /n) + log(n/(σ

n be generated i.i.d.

δ, CRR, after

d/n log(nd/δ)).

(log(
k

b∗

O

≤

≤

≤

≤

−

i

·

w∗

wt
k

−

k2 ≤ O

(σ

p

The above result establishes consistency of the CRR method with ˜
O

d/n) error rates that are known
to be statistically optimal, notably in the presence of gross and unbounded outliers. We reiterate that to
the best of our knowledge, this is the ﬁrst instance of a poly-time algorithm being shown to be consistent
for the RLSR problem. It is also notable that the result allows the corruption index to be k∗ = Ω(n), i.e.
allows upto a constant factor of the total number of data points to be arbitrarily corrupted, while ensuring
consistency, which existing results [5, 16] do not ensure.

For our analysis, we will divide CRR’s execution into two phases – a coarse convergence phase and
a ﬁne convergence phase. CRR will enjoy a linear rate of convergence in both phases. However, the
w∗
(σ). The ﬁne convergence phase will then
coarse convergence analysis will only ensure
(log n) more iterations, CRR
use a much more careful analysis of the algorithm to show that in at most

k2 =

wt

p

O

−

k

O

4

w∗

wt
k

k2 = ˜
ensures
O
Torrent, are able to reach an error level
p
1X(bt
b∗), g := (I

−
Let λt := (XX ⊤)−

(σ

d/n), thus establishing consistency of the method. Existing methods, including

(σ), but no further.
PX )ǫ, and vt = X ⊤λt +g. Let S∗ := supp(b∗) true locations of
supp(b∗),
supp(b∗) respectively denote the coordinates that were missed detections, false alarms,

supp(b∗). Let MDt = supp(b∗)
\

supp(bt), FAt = supp(bt)
\

O
−

−

∪

the corruptions and I t := supp(bt)
and CIt = supp(bt)
and correctly identiﬁcations.

∩

Coarse convergence: Here we establish a result that guarantees that after a certain number of steps

T0, CRR identiﬁes the corruption vector with a relatively high accuracy i.e.

2 ≤ O
Lemma 4. For any data matrix X that satisﬁes the SSC and SSS properties such that 2Λk+k∗

wT0

w∗

−

(σ).

< 1, CRR,

(cid:13)
(cid:13)
k2

b∗
√n

steps,

(cid:17)

(cid:13)
(cid:13)
λn
bT0

b∗

3e0,

2 ≤

−

(cid:13)
(cid:13)

(cid:13)
(cid:13)

when executed with a parameter k

≥

k∗, ensures that after T0 =

log k

O

σ

(k + k∗) log

n
δ(k+k∗)

(cid:16)
for standard Gaussian designs.

where e0 =

O

(cid:16)

q

(cid:17)

assuming k = k∗

Using Lemma 17 (see the appendix), we can translate the above result to show that

wT0
n
150 . However, Lemma 4 will be more useful in the following analysis.
Fine convergence: We now show that CRR progresses further at a linear rate to achieve a consistent
2 can be bounded, apart from diminishing or negligible terms,
solution. First Lemma 5 will show that
by the amount of mass that is present in the false alarm coordinates MDt. Lemma 6 will next bound this
(cid:13)
(cid:13)
quantity. For all analyses hereon, we will assume t > T0.

0.95σ,

2 ≤

w∗

λt

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

≤

−

Lemma 5. Suppose k∗

ensures that

λt+1

δ + 2.001
λn k
We note that in the RHS above, the ﬁrst term diminishes at a linear rate and the second term is a

XFAt+1 (X ⊤FAt+1λt + gFAt+1 )

k2 ≤

k2.

q

2d

k

k
n/10000. Then with probability 1
≤
λt
k2 + 2σ

n log d

≤
1
100 k

−

δ, at every time instant t > T0, CRR

negligible quantity since it is ˜
O

(

d/n). In the following we bound the third term.

Lemma 6. For k∗

n/10000, with probability at least 1

p

0.98

λt

σ

2 + C

(cid:13)
(cid:13)
(cid:13)
Putting all these results together establishes Theorem 3. See Appendix A for a detailed proof.
(cid:13)

for some constant C.

q

(cid:13)
(cid:13)

·

k
≤
≤
d
n log nd

δ

δ, CRR ensures at all t > T0, 2.001
λn

XFAt+1(X ⊤FAt+1 λt + gFAt+1)

−

4 Robust Time Series Estimation

Similar to RLSR, we formulate the Robust Time Series Estimation (RTSE) with additive outliers (AO)
problem, propose our CRTSE algorithm, and prove its consistency and robustness guarantees.

Problem Formulation and Notation: Let (x
d+1, . . . , xn) be the “clean” time series which is a sta-
−
dw∗d + ǫt where ǫt ∼ N
(0, σ2) are i.i.d.
1w∗1 +
tionary and stable AR (d) process deﬁned as xt = xt
−
noise values chosen independently of the data and the model. We compactly represent this AR (d) process
as,

+ xt

· · ·

−

y∗ = X ⊤w∗ + ǫ,

Rn, xi = (xi
where y∗ = (x1, . . . , xn)⊤
−
observe the “clean” time series. Instead, we observe the time series (y
corruptions. Deﬁning y
using (x

Rd
d+1, . . . , xn), we have the resulting AO model as follows:

∈
Rn, X

d)⊤, and X = [x1, . . . , xn]

n. However, we do not
d+1, . . . , yn) which contains additive
d+1, . . . , yn) in similar manner as y∗ and X are deﬁned

n using (y

1, . . . , xi

Rd

∈

∈

∈

−

−

×

−

×

−

y = y∗ + e∗ = X ⊤w∗ + ǫ + b∗,

(3)

where e∗ is the actual corruption vector (k∗-sparse), and b∗ is the resulting model corruption vector (with
at most k∗-blocks of size d being non-zero). See (19) (see Appendix B.2) for a clearer characterization of the
y, X.

5

Now, given y, X, our goal will be to recover a consistent estimate of the parameter w∗. For our results
the following simple observation would be crucial: since supp(b∗) is a union of k∗ groups (intervals) of size
G0 is the Group-ℓ0 pseudo-norm of b that we deﬁne below. For a set of
d, we have
groups S, supp(S;

b∗
k

b
k
.

k
S

k

G0 ≤
G

2k∗, where
Gi, i
) =
{

∈

}

We now deﬁne certain quantities that are crucial in understanding the AR (d) process. The spectral

density of the “clean” AR (d) process y∗ is given by:

ρw∗ (ω) =

1

(cid:16)

d
k=1 w∗keikω

−

d
k=1 w∗ke−

ikω

−

(cid:17)

σ2

1

(cid:17) (cid:16)

, for ω

∈

[0, 2π] .

(4)

P
[0,2π] ρw∗ (ω) and mw∗ := inf ω

P

M

w∗ := supω

We deﬁne
appear in our results (see Appendix B.2 for a brief primer on AR (d) process).

MW will also
For our analysis, we will also require notions of Sub-group Strong Convexity and Sub-group Strong Smooth-
= k

[0,2π] ρw∗ (ω). Another constant

supp(S;

) : S

s.t.

S

∈

∈

ness for the time series which we deﬁne below. For any k
denote the set of all collections of k groups from

.

≤

G
n satisﬁes the Subgroup Strong Convexity Property (resp.
Deﬁnition 7 (SGSC/SGSS). A matrix X
Subgroup Strong Smoothness Property) at level k with strong convexity constant λk (resp. strong smoothness
constant Λk) if the following holds:

Rd

∈

×

(cid:8)

(cid:2)

(cid:3)

n
d , we let

Gk =
S

|

|

⊆

n
d

G

(cid:9)

λk ≤

min
G
S
k

∈S

λmin

XSX ⊤S

(cid:0)

(cid:1)

≤

max
G
S
k

∈S

λmax

XSX ⊤S

Λk.

≤

(cid:0)

(cid:1)

4.1 CRTSE: A Block Sparse Hard Thresholding Approach to Consistent Robust

Time Series Estimation

We now present our CRTSE method for obtaining consistent estimates in the RTSE problem. By following
2
the similar approach as CRR, we begin with the RTSE formulation minw
2,
∈
b of the corruption vector, the optimal model with respect to that
and observe that for any given estimate
(cid:13)
(cid:13)
w into the formulation, we
w = (XX ⊤)−
estimate is
reformulate the RTSE problem as follows
b

b). Then by plugging this expression for

1X(y

X ⊤w

G
0 ≤

b)

(y

Rd,

(cid:13)
(cid:13)

−

−

−

k∗

1
2

b

k

k

b

b

min
G
b
0 ≤

k

k∗

k

f (b) =

1
2 k

(I

−

PX )(y

2
2

b)
k

−

b

(5)

1X. CRTSE uses a variant of iterative hard thresholding to optimize the above
where PX = X ⊤(XX ⊤)−
formulation. At every iteration, CRTSE takes a step along the negative gradient of the function f and then
performs group hard thresholding to select the top k aligned groups (i.e. groups in
) of the resulting vector
and setting the rest to zero.

G

where k

≥

2k∗ and the group hard thresholding operator is deﬁned below.

bt+1 = HTGk (bt

f (bt)),

− ∇

Deﬁnition 8 (Group Hard Thresholding). For any vector g
gj|

Gσg (2) |

gj|

Gσg ( n

d ) |

. . .

2

2

j

∈

j

Gσg (1) |

gj|
∈
thresholding operator as
P

P

≥

∈

j

≥
≥
g = HTGk (g) where

P

Rn, let σg
2. Then for any k

∈

d be the permutation s.t.
S n
n
d , we deﬁne the group hard

∈
≤

b

gi =

gi
0

(

1
g (

if σ−
else

i
d

)

k

≤

(cid:6)

(cid:7)

We note that this step can be done in quasi linear time. Due to the delicate correlations between data
points in the time series, in order to keep the problem well conditioned (see Theorem 22 and Remark 23),

b

6

yi,

d + 1, . . . , n as follows:
we will perform a pre-processing step on the corrupted time series instances yi, i =
. Note that since the clean underlying time series is a
yi = max
min
σ
{
−
Gaussian process ǫi ≤ O
σ. Thus we
will not clip any clean point because of the above step but ensure that we can, from now on, assume that
b
(cid:0)
b∗
σ.

σ =
and all its entries are, with high probability, bounded by
b
(cid:1)

, where
}
σ√log n

√log nσ

O

−

σ

b

}

{

(cid:0)

(cid:1)

,

b

k

k∞ ≤

b

4.2 Convergence and Consistency Guarantees
We now present the estimation error bound for our CRTSE algorithm.

Theorem 9. Let y be generated using AR (d) process with k∗ additive outliers (see (3)). Also, let k∗
d log n (for some universal constant C > 0). Then, with probability at least 1
C

≤
δ, CRTSE, after

≤

k

n

W

k2 /n) + log(n/(σ

·

d))) steps, ensures that

wt
k

−

w∗

k2 ≤ O

σ

M

mw∗
Mw∗ +
(log(
k

M
b∗

O

The result does establish consistency of the CRTSE method as it oﬀers convergence to

error levels. Also note that in typical time series data, d lies in the range 5
this is the ﬁrst instance of a poly-time algorithm being shown to be consistent for the RTSE problem.
w∗

e
Following the similar approach of the consistency analysis for CRR, we will ﬁrst ensure that
(σ). Then in the ﬁne analysis phase, we will show that after additional
O
wt
k

−
Coarse convergence: Here we establish a result that after a certain number of iterations, CRTSE
identiﬁes the corruption vector with a relatively high accuracy. Our analysis relies on a novel Theorem 22,
which is a key result that shows that the AR (d) process with AO indeed satisﬁes SGSC and SGSS properties
(see Deﬁnition 8), as long as the number of corruptions k∗ is small.

k2 =
(log n) iterations, CRTSE ensures

d log n/n

k2 =

wt
k

w∗

p

p

O

O

−

−

(cid:17)

(cid:16)

σ

e

.

Theorem 10. For any data matrix X that satisﬁes the SGSC and SGSS properties such that 4Λk+k∗ <
k2 /√n)) steps,
λ n
5e0. Additionally, if X is generated using our AR (d) process with AO (see (3)), then

d , CRTSE, when executed with a parameter k
bT0

k∗, ensures that after T0 =

b∗
k

(log (

b∗

O

≥

−

w∗ /mw∗

d log n/n log (d/δ)

.

(cid:16)

p

(cid:17)
d log n/n
σ
10. As in the case of CRR,
(cid:17)
(cid:16)

O

−

2 ≤
(k + k∗)d log

σ

(cid:16)

O

e0 =
(cid:13)
(cid:13)

(cid:13)
(cid:13)
q
mw∗
k
Mw∗ +
probability at least 1

≤

C

M

n

W

−

n
δ(k+k∗)d

.

(cid:17)

Note that if X is given by AR (d) process with AO model and if k is suﬃciently small i.e. k∗

≤
d log n (for some universal constant C > 0) and n is suﬃciently large enough, then with

δ, we have 4Λk+k∗ < λ n

d . See Remark 23 for more details.

Fine Convergence: As was the case in least squares regression, we will now sketch a proof that the
CRTSE algorithm indeed moves beyond the convergence level achieved in the coarse analysis and proceeds
towards a consistent solution at a linear rate. We begin by noting that by applying Lemma 24, we can derive
a result similar to Lemma 17. With high probability, we have for all t > 1

wt

w∗

−

C

·

2 ≤

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Λn
λn  

σ

r

d log n
n

log

d
δ

+

λt

,

2

!

(6)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

for a universal constant C. We note that for large enough n, Lemma 19 shows that Λn
(1). Since
λn
the ﬁrst term in the bracket is a negligible term, one that does not hinder consistency, save log factors, we
are just left to establish the convergence of the iterates λt. We next note that Lemma 24, along with the
fact that the locations of the corruptions were decided obliviously and independently of the noise values
, allows us to also prove the following equivalent of Lemma 5 for the time series case as well: with high
ǫi}
{
probability, for every time instant t > T0, we have

O

=

λt+1

1
100

2 ≤

λt

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

2 + C
(cid:13)
(cid:13)

σ

·  

r

d log n
n

log

d
δ

+

1
λn (cid:18)

1 +

Λn
λn (cid:19)

XFAt+1 (X ⊤FAt+1λt + gFAt+1 )
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

,

!

(7)

7

5

2
k
∗
w
−
w
k

0

0

d = 500 sigma = 2 k = 0.3*n

OLS
TORRENT-FC
CRR

n = 2000 sigma = 1 k = 600

OLS
TORRENT-FC
CRR

5

2
k
∗
w
−
w
k

n = 2000 d = 500 k = 600

OLS
TORRENT-FC
CRR

4

2

2
k
∗
w
−
w
k

10000

0

0

5000
Number of Datapoints
(a)

800

0

0

600
400
200
Dimensionality (d)
(b)

1

1.5

0.5
White Noise (sigma)
(c)

)
c
e
s
n
i
(
e
m
T

i

2

d = 500 sigma = 7.5 k = 0.3*n

TORRENT-FC
CRR

20

10

0

0

5000
Number of Datapoints
(d)

10000

Figure 1: (a), (b) and (c) show variation of recovery error with varying n, d and σ. CRR and TORRENT show
better recovery properties than the non-robust OLS. These plots also ascertain the √n-consistency of CRR as is
shown in the theoretical analysis. (d) shows the average CPU run time of TORRENT and CRR with increasing
sample size. CRR can be upto 2x faster than TORRENT while ensuring similar recovery properties.

by

for some universal constant C. Noticing yet again that Λn
λn
the quantity

XFAt+1 (X ⊤FAt+1 λt + gFAt+1)
λt + gSt
XFAt+1 (X ⊤St
(cid:13)
(cid:13)
λt + gS∗ . This allows us to establish the following result.
(cid:13)
X ⊤S∗
(cid:13)
(cid:13)
Lemma 11. Suppose k∗
at least 1

δ, CRR ensures at every time instant t > T0

by selecting the set St

2
(cid:13)
(cid:13)
(cid:13)
k

≤

≤

)

k

k

=

(1) leaves us to prove a bound on
2. We now notice that one can upper bound this quantity
k of the top k elements by magnitude in the vector
(cid:13)
(cid:13)

O

n/(C′ρ(w∗)d log n) for some large enough constant C′. Then with probability

−

C
λn (cid:18)

1 +

Λn
λn (cid:19)

XFAt+1(X ⊤FAt+1 λt + gFAt+1 )

0.5

λt

2 ≤

σ

O  

r

d log n
n

log

1
δ !

2 +
(cid:13)
(cid:13)

(cid:13)
(cid:13)
Above lemma with (7) suﬃces to establish Theorem 9. See Appendix B for details of all the steps sketched

(cid:13)
(cid:13)

(cid:13)
(cid:13)

above.

5 Experiments

Several numerical simulations were carried out on synthetically generated linear regression and AR (d) time-
series data with outliers. The experiments show that in the robust linear regression setting, CRR gives
a consistent estimator and is 2x times faster as compared with TORRENT [5] while in the robust AR (d)
time-series setting, CRTSE gives a consistent estimator and oﬀers statistically better recovery properties as
compared with baseline algorithms.

h

∈

5.1 Robust Linear Regression
Rd was chosen to be a random unit norm vector. The
Data: For the RLSR problem, the regressor w∗
(0, Id). The k∗ non-zero locations of the corruption vector b∗
data matrix was generated as each xi ∼ N
U (10, 20).
were chosen uniformly at random from [n] and the value of the corruptions were set to b∗i ∼
(0, σ2). All plots for
The response variables y were then generated as yi =
the RLSR problem have been generated by averaging the results over 20 random instances of the data and
regressor.

+ ηi + b∗i where ηi ∼ N

xi, w∗

Baseline Algorithms: We compare CRR with two baseline algorithms: Ordinary Least Squares (OLS)
and TORRENT ([5]). All the three algorithms were implemented in Matlab and were run on a single core
2.4GHz machine with 8GB RAM.

Recovery Properties & Timing: As can be observed from Figure(1), CRR performs as well as
TORRENT in terms of the residual error
k2 and both their performances are better as compared with
the non-robust OLS method. Further, ﬁgures 1(a), 1(b) and 1(c) explain our near optimal recovery bound of
d
n by showing the corresponding variation of the recovery error with variations in n, d and σ, respectively.
σ
Figure 1(d) shows a comparison of variation of average CPU time (in secs) with increasing number of data
q
samples and shows that CRR can be upto 2x faster than TORRENT while provably guaranteeing consistent
estimates for the regressor.

w∗

w

−

k

i

8

 
 
d = 3 sigma = 0.05 k = 0.05*n

n = 300 sigma = 0.5 k = 15

n = 300 d = 3 sigma = 0.5

n = 300 d = 3 k = 15

2
k
∗
w
−
w
k

0.15

0.1

200

OLS
TORRENT-FC
CRR
CRTSE

2
k
∗
w
−
w
k

0.4

0.3

1000

0.2

1

800
600
400
Number of Points

(a)

OLS
TORRENT-FC
CRR
CRTSE

4

2

3
Order of Time Series (d)
(b)

2
k
∗
w
−
w
k

0.32
0.3
0.28
0.26
0.24

5

OLS
TORRENT-FC
CRR
CRTSE

10

20
Number of Corruptions (k)

30

(c)

0.3

0.25

2
k
∗
w
−
w
k

0.2

0

OLS
TORRENT-FC
CRR
CRTSE

0.5

1
White Noise (sigma)
(d)

Figure 2: (a), (b), (c), and (d) show variation of recovery error with varying n, d, k and σ, respectively. CRTSE
outperforms OLS, and both the point-wise thresholding algorithms, TORRENT and CRR. Also, the decreasing error
with increasing n shows the consistency of our estimator in this regime.

5.2 Robust Time Series with Additive Corruptions
Data: For the RTSE problem, the regressor w∗
) norm
∈
(0, 1)
(to avoid the time-series from diverging). The initial d points of the time-series are chosen as xi ∼ N
for i = 1 . . . d. The time-series, generated according AR (d) model with regressor w∗, was then allowed to
stabilize for the next 100 time-steps. We consider the points generated in the next n time steps as xi for
i = 1 . . . n. The k∗ non-zero locations of the corruption vector b∗ were chosen uniformly at random from
[n] and the value of the corruptions were set to b∗i ∼
U (10, 20). The observed time series is then generated
as yi = xi + b∗i . All plots for the RTSE problem have been generated by averaging the outcomes over 200
random runs of the above procedure.

Rd was chosen to be a random vector with O( 1
√d

Baseline Algorithms: We compare CRTSE with three baseline algorithms: Ordinary Least Squares
(OLS) , TORRENT ([5]) and CRR. For TORRENT and CRR, we set the thresholding parameter k = 2k∗d
and compare results with CRTSE. All simulations were done on a single core 2.4GHz machine with 8GB
RAM.

Recovery Properties: Figure 2 shows the variation of recovery error

k2 for the AR (d) time-
series with Additive Corruptions. CRTSE outperforms all three competitor baselines: OLS, TORRENT
and CRR. Since CRTSE uses a group thresholding based algorithm as compared with TORRENT and
CRR which use point-wise thresholding, CRTSE is able to identify blocks which contain both response and
data corruptions and give better estimates for the regressor. Also, ﬁgure 2(a) shows that the recovery error
goes down with increasing number of points in the time-series, as is evident from our consistency analysis of
CRTSE.

w∗

w

−

k

9

References

[1] Jan ´Amos Vi˜sek. The least trimmed squares. Part I: Consistency. Kybernetika, 42:1–36, 2006.
[2] Jan ´Amos Vi˜sek. The least trimmed squares. Part II: √n-consistency. Kybernetika, 42:181–202, 2006.

[3] Oren Anava, Elad Hazan, and Assaf Zeevi. Online time series prediction with missing data. In Proceedings of

the 32nd International Conference on Machine Learning (ICML-15), pages 2191–2199, 2015.

[4] Sumanta Basu and George Michailidis. Regularized Estimation in Sparse High-dimensional Time Series Models.

The Annals of Statistics, 43(4):1535–1567, 2015.

[5] Kush Bhatia, Prateek Jain, and Purushottam Kar. Robust Regression via Hard Thresholding. In 29th Annual

Conference on Neural Information Processing Systems (NIPS), 2015.

[6] Thomas Blumensath and Mike E. Davies. Iterative Hard Thresholding for Compressed Sensing. Applied and

Computational Harmonic Analysis, 27(3):265–274, 2009.

[7] Yudong Chen, Constantine Caramanis, and Shie Mannor. Robust Sparse Regression under Adversarial Corrup-

tion. In 30th International Conference on Machine Learning (ICML), 2013.

[8] Christophe Croux and Kristel Joossens. Robust estimation of the vector autoregressive model by a least trimmed

squares procedure. In COMPSTAT 2008, pages 489–501. Springer, 2008.

[9] Rahul Garg and Rohit Khandekar. Gradient descent with sparsiﬁcation: an iterative algorithm for sparse recovery

with restricted isometry property. In 26th International Conference on Machine Learning (ICML), 2009.

[10] Prateek Jain, Ambuj Tewari, and Purushottam Kar. On Iterative Hard Thresholding Methods for High-
dimensional M-estimation. In 28th Annual Conference on Neural Information Processing Systems (NIPS), 2014.

[11] Ricardo A. Maronna, R. Douglas Martin, and Victor J. Yohai. Robust Statistics: Theory and Methods. J. Wiley,

2006.

[12] R. Douglas Martin. Robust estimation for time series autoregressions. In ROBERT L. LAUNER and GRA-

HAM N. WILKINSON, editors, Robustness in Statistics, pages 147 – 176. Academic Press, 1979.

[13] R. Douglas Martin and Judy Zeh. Robust generalized m-estimates for autoregressive parameters: smallsample

behavior and applications. Technical Report 214, University of Washington, Seattle, 1978.

[14] Igor Melnyk and Arindam Banerjee. Estimating structured vector autoregressive model. arXiv:1602.06606

(math.ST), 2016.

[15] Nora Muler, Daniel Pena, and Victor J. Yohai. Robust estimation for arma models. The Annals of Statistics,

37(2):816–840, 2009.

[16] Nam H. Nguyen and Trac D. Tran. Exact recoverability from dense corrupted observations via L1 minimization.

IEEE Transaction on Information Theory, 59(4):2036–2058, 2013.

[17] Peter J. Rousseeuw. Least Median of Squares Regression. Journal of the American Statistical Association,

79(388):871–880, 1984.

[18] Mark Rudelson and Roman Vershynin. Hanson-Wright Inequality and Sub-gaussian Concentration. Electronic

Communications in Probability, 18(82):1–9, 2013.

[19] Ohad Shamir. A variant of azuma’s inequality for martingales with subgaussian tails. arXiv:1110.2392 (cs.LG),

2011.

[20] Norbert Stockinger and Rudolf Dutter. Robust time series analysis: A survey. Kybernetika, 23(7):1–3, 1987.

[21] Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. In Y. Eldar and G. Kutyniok,
editors, Compressed Sensing, Theory and Applications, chapter 5, pages 210–268. Cambridge University Press,
2012.

[22] John Wright and Yi Ma. Dense Error Correction via ℓ1 Minimization. IEEE Transaction on Information Theory,

56(7):3540–3560, 2010.

[23] Tong Zhang. Adaptive Forward-Backward Greedy Algorithm for Learning Sparse Representations. IEEE Trans.

Inf. Theory, 57:4689–4708, 2011.

10

A Supplementary Material for Consistent Robust Regression

A.1 SSC/SSS guarantees

In this section we restate some results from [5] which are required for the convergence analysis of the RLSR
problem.

Deﬁnition 12. A random variable x

∈

R is called sub-Gaussian if the following quantity is ﬁnite

p−

1/2 (E [

p])1/p .

x
|

|

sup
1
p

≥

Moreover, the smallest upper bound on this quantity is referred to as the sub-Gaussian norm of x and denoted
as

.

x
kψ2

k

Deﬁnition 13. A vector-valued random variable x
marginals
lows

are sub-Gaussian for all v

x, v
h

Sp

∈

−

i

Rp is called sub-Gaussian if its unidimensional
1. Moreover, its sub-Gaussian norm is deﬁned as fol-

∈

X
k

kψ2

:= sup
v

Sp−1 kh

x, v

∈

ikψ2

Rp

×

n be a matrix whose columns are sampled i.i.d from a standard Gaussian distri-

(0, I). Then for any ǫ > 0, with probability at least 1

δ, X satisﬁes

−

Lemma 14. Let X
bution i.e. xi ∼ N

∈

λmax(XX ⊤)

n + (1

−

≤

1

2ǫ)−

cnp + c′n log

r

λmin(XX ⊤)

(1

n

−

−

≥

1

2ǫ)−

cnp + c′n log

r

2
δ
2
δ

,

where c = 24e2 log 3

ǫ and c′ = 24e2.
Rp

Theorem 15. Let X
distribution i.e. xi ∼ N
SSC and SSS properties with constants

×

∈
(0, I). Then for any γ > 0, with probability at least 1

n be a matrix whose columns are sampled i.i.d from a standard Gaussian
δ, the matrix X satisﬁes the

−

Λγ ≤

γn

1 + 3e

6 log

(cid:18)

r

+

e
γ

(cid:19)

np + n log

1
δ !

O  r
e

λγ ≥

n

−

(1

−

γ)n

1 + 3e

6 log

(cid:18)

r

1

−

γ

(cid:19)

−

Ω

np + n log

 r

1
δ !

.

Lemma 16. Let X
sub-Gaussian norm K and covariance Σ. Then, for any δ > 0, with probability at least 1
following statements holds true:

n be a matrix with columns sampled from some sub-Gaussian distribution with
δ, each of the

−

∈

×

Rp

λmax(XX ⊤)

λmin(XX ⊤)

λmax(Σ)

λmin(Σ)

n + CK ·
CK ·
n
−

·

·

≤

≥

√pn + t√n

√pn

−

t√n,

1
where t =
cK
the distribution.

q

log 2

δ , and cK, CK are absolute constants that depend only on the sub-Gaussian norm K of

11

A.2 Convergence Proofs for CRR

Theorem 3. For k∗
b∗
n + log n
k2

log k

d

≤

≤
steps, ensures that

O

(cid:16)

(cid:17)

Proof. Putting Lemmata 5 and 6 establishes that

k

n/10000 and Gaussian designs, with probability at least 1

δ, CRR, after

−

wt

k

−

w∗

k2 ≤ O

σ
λmin(Σ)

(cid:18)

q

d

n log nd

δ

.
(cid:19)

which ensures a linear convergence of the terms

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)
λt

then ﬁnishes oﬀ the result.

λt+1

0.99

2 ≤

λt

d
n

log

nd
δ

,

r

2 + Cσ
(cid:13)
(cid:13)
2 to a value
(cid:13)
(cid:13)

(cid:13)
(cid:13)

d

n log nd

δ

O

σ

(cid:18)

q

. Applying Lemma 17

(cid:19)

Lemma 4. For any data matrix X that satisﬁes the SSC and SSS properties such that 2Λk+k∗

when executed with a parameter k

≥

where e0 =

O

σ

(k + k∗) log

n
δ(k+k∗)

(cid:16)

q

(cid:17)

k∗, ensures that after T0 =

log k

b∗
√n

k2

steps,

O

(cid:16)
for standard Gaussian designs.

(cid:17)

Proof. We start with the update step in CRR, and use the fact that y = X ⊤w∗ + b∗ + ǫ to rewrite the
update as

Since X ⊤ = PX X ⊤, we get, using the notation set up before,

bt+1

←

HTk(PX bt + (I

−

PX )(X ⊤w∗ + b∗ + ǫ)).

λn
bT0

−

< 1, CRR,

b∗

3e0,

2 ≤

(cid:13)
(cid:13)

(cid:13)
(cid:13)

k∗, using the properties of the hard thresholding step gives us

bt+1

←

HTk(b∗ + X ⊤λt + g).

Since k

≥
bt+1

(b∗I t+1 + X ⊤I t+1 λt + gI t+1)

I t+1 −

−
This, upon applying the triangle inequality, gives us

2 ≤

b∗I t+1

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(b∗I t+1 + X ⊤I t+1 λt + gI t+1)

2 =
(cid:13)
(cid:13)

X ⊤I t+1λt + gI t+1
(cid:13)
(cid:13)

2 .
(cid:13)
(cid:13)

bt+1

b∗

−

2

2 ≤

X ⊤I t+1 λt + gI t+1
(cid:13)
(cid:13)

2 .
(cid:13)
X ⊤I t+1λt
(cid:13)
(cid:13)
(cid:13)

2 =
(cid:13)
(cid:13)

Now, using the SSC and SSS properties of X, we can show that
Λk+k∗
λn

b∗

bt

(cid:13)
(cid:13)

(cid:13)
(cid:13)

k2.

k

−

(cid:13)
(cid:13)
Since ǫ is a Gaussian vector, using tail bounds for Chi-squared random variables (for example, see [5,
σ2(k + k∗) +
gives us, with

Lemma 20]), for any set S of size k + k∗, we have with probability at least 1
6(k + k∗) log 1
2eσ2
δ . Taking a union bound over all sets of size (k + k∗) and
probability at least 1

δ, for all sets S of size at most (k + k∗),

ǫSk
≤

2
2 ≤
k
en
k

δ ,
−
n
k

k

q

(cid:0)

(cid:1)

(cid:0)

(cid:1)

X ⊤I t+1 (XX ⊤)−
(cid:13)
(cid:13)

1X ⊤I t(bt

b∗)

−

2 ≤

−

ǫSk2 ≤

k

σ

(k + k∗)

1 + 2e

s

6 log

r

p

en
δ(k + k∗)

at least 1

Using tail bounds on Gaussian random variables1, we can also show that for every i, with probability
2 log 1
δ . Taking a union bound gives us, with the same
gI t+1
k

(X ⊤)i
2σ2dΛn log d
q
(cid:13)
(cid:13)
(cid:13)
(cid:13)

δ . This allows us to bound

δ ≤

k2

−

σ

k

2

conﬁdence,

(Xǫ)ik2 ≤
δ, we have
k
2
2
Xǫ
F log d
2σ2
X
2 ≤
k
k
k
ǫ
gI t+1
k2 =
I t+1
k
e−t2/2dt

−

∞
(cid:13)
1
(cid:13)
√2π Z
x

t
x

≤

e−t2/2dt =

1 1

∞

√2π Z
x

X ⊤I t+1 (XX ⊤)−

1Xǫ

2

1
x√2π

e−x2/2

(cid:13)
(cid:13)

12

(k + k∗)

1 + 2e

s

(k + k∗)

1 + 2e

s

6 log

r

6 log

r

en
δ(k + k∗)

+ σ

Λk+k∗ Λn
λn

p

2d log

r

d
δ

en
δ(k + k∗)

1 +

2d
n

log

d
δ !

r

σ

σ

≤

≤

p

p

= 1.0003e0,

|

e0

{z

}

where the second last step is true for Gaussian designs and suﬃciently large enough n. Note that e0 does
note depend on the iterates and is thus, a constant. This gives us

For data matrices sampled from Gaussian ensembles, whose SSC and SSS properties will be established
later, assuming n
< 1, then in

. Thus, if 2Λk+k∗

d log d, we have e0 =

σ

λn

bt+1

b∗

−

2 ≤

2Λk+k∗
λn

(cid:13)
(cid:13)

(cid:13)
(cid:13)

bt

b∗

−

2 + 2.0006e0.
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(k + k∗) log

n
δ(k+k∗ )

log k

b∗
e0

k2

T0 =

O

(cid:16)

≥
=

O

(cid:16)

(cid:17)

log k

b∗
√n

k2

(cid:17)

O

(cid:16)
steps, CRR ensures that

q

Lemma 17. Let λmin(Σ) be the smallest eigenvalue of the covariance matrix of the distribution

(0, Σ) that

bT0

b∗

(cid:17)
2 ≤

−

2.0009e0.

(cid:13)
(cid:13)

(cid:13)
(cid:13)
wt

generates the data points. Then at any time instant t, we have

k

w∗

−

k2 ≤

2
λmin(Σ)

2σ

Proof. As described in Algorithm 1, wt = (XX ⊤)−
get

1X(y

−

bt) = w∗ + (XX ⊤)−

wt

w∗

−

2 ≤

1
λmin(XX ⊤)

X ⊤(wt

w∗)

2

−

N
n log d
δ +

d

λt

.
(cid:19)
(cid:13)
bt). Thus, we
(cid:13)

(cid:13)
(cid:13)

2

−

(cid:18)

q
1X(ǫ + b∗

(cid:13)
(cid:13)

(cid:13)
(cid:13)

≤

≤

1
nλmin(Σ)
1
nλmin(Σ)

(cid:13)
(cid:13)
CΣ√n

−

CΣ√n

X ⊤(wt

(cid:13)
(cid:13)
−

w∗)

2

(cid:13)
(cid:13)

X ⊤(XX ⊤)−

(cid:13)
(cid:13)

1X(ǫ + b∗

−
Λn

≤

nλmin(Σ)

−

CΣ√n

2
λmin(Σ)  

2σ

≤

d
n

r

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
log

(XX ⊤)−

1X(ǫ + b∗

−

d
δ

+

λt

2

,

!

(cid:13)
(cid:13)

(cid:13)
(cid:13)

−

bt)
2
(cid:13)
(cid:13)
(cid:13)

bt)
2
(cid:13)
(cid:13)
(cid:13)

where the second step follows from results on eigenvalue bounds for data matrices drawn from non-spherical
Gaussians, where CΣ is a constant dependent on the subGaussian norm of the distribution, and the last step
assumes n

λmin(Σ) and uses the proof technique used in Lemma 4 to get

2CΣ

≥

(XX ⊤)−

1Xǫ

√Λn
λn r

2d log

d
δ ≤

2σ

d
n

r

log

d
δ

.

σ

2 ≤

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
k

≤
λt

≤
1
100

Lemma 5. Suppose k∗
ensures that

λt+1

2 ≤

q
Proof. We have bt+1 = HTk(b∗ + X ⊤λt + g). To analyze λt+1 = (XX ⊤)−
b∗) = XMDt+1(bt+1
looking at X(bt+1
We then have

(cid:13)
(cid:13)
b∗MDt+1 ) + XFAt+1 (bt+1

MDt+1 −

FAt+1 −

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

−

2d

n/10000. Then with probability 1
2 + 2σ
(cid:13)
(cid:13)

δ + 2.001
λn

n log d

−

XFAt+1(X ⊤FAt+1 λt + gFAt+1 )

δ, at every time instant t > T0, CRR
2.
(cid:13)
1X(bt+1
(cid:13)
b∗FAt+1) + XCIt+1 (bt+1

b∗), we start by
b∗CIt+1 ).

−

CIt+1 −

XMDt+1(bt+1

MDt+1 −

b∗MDt+1 ) = XMDt+1 (

−

b∗MDt+1 )

13

 
XCIt+1 (bt+1
XFAt+1(bt+1

CIt+1 −
FAt+1 −

b∗CIt+1 ) = XCIt+1(X ⊤CIt+1 λt + gCIt+1 )
b∗FAt+1 ) = XFAt+1 (X ⊤FAt+1λt + gFAt+1 ).

This gives us upon completing the terms, and using CIt+1

MDt+1 = S∗,

⊎

X(bt+1

−

b∗) = XFAt+1(X ⊤FAt+1 λt + gFAt+1 ) + XS∗ (X ⊤S∗ λt + gS∗)

−

Now due to the hard thresholding operation, we have
This gives us

b∗MDt+1 + X ⊤MDt+1λt + gMDt+1
(cid:13)
(cid:13)
XMDt+1(b∗MDt+1 + X ⊤MDt+1 λt + gMDt+1 )
2 =
(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤

XMDt+1 (b∗MDt+1 + X ⊤MDt+1 λt + gMDt+1 ).
X ⊤FAt+1 λt + gFAt+1
(cid:13)
(cid:13)

2 ≤

(cid:13)
(cid:13)

2.

(cid:13)
(cid:13)

2

2

X(b∗MDt+1 + X ⊤MDt+1 λt + gMDt+1 )
b∗MDt+1 + X ⊤MDt+1λt + gMDt+1
Λn
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
X ⊤FAt+1λt + gFAt+1
Λn
(cid:13)
(cid:13)
(cid:13)
(cid:13)
Λn
(cid:13)
X(X ⊤FAt+1 λt + gFAt+1)
(cid:13)
λn
(cid:13)
Λn
XFAt+1(X ⊤FAt+1 λt + gFAt+1 )
(cid:13)
λn
(cid:13)
(cid:13)
XFAt+1 (X ⊤FAt+1 λt + gFAt+1)
1.001
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

2

2

2 ,
(cid:13)
(cid:13)

≤

≤

=

≤

where the last step uses a large enough n so that the data matrix X is well conditioned. Thus,

λt+1

(cid:13)
(cid:13)

2 =
(cid:13)
(cid:13)

≤

≤

−

b∗)

1X(bt+1

(XX ⊤)−
1
(cid:13)
XS∗(X ⊤S∗ λt + gS∗)
(cid:13)
λn
(cid:13)
(cid:13)
1
d
(cid:13)
(cid:13)
log
δ
100

2d
n

λt

2

(cid:13)
2 +
(cid:13)

+

2.001
λn
2.001
λn

2 + 2σ
(cid:13)
(cid:13)

r

(cid:13)
(cid:13)

XFAt+1(X ⊤FAt+1 λt + gFAt+1)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
XFAt+1 (X ⊤FAt+1 λt + gFAt+1)
(cid:13)
(cid:13)

2 ,
(cid:13)
(cid:13)

2

where the third step follows by observing that the columns of X are (statistically equivalent to) i.i.d. samples
from a standard Gaussian, the fact that the support of the corruptions S∗ is chosen independently of the
data and the noise, and requiring that k∗

n
100 .

≤

Lemma 6. Suppose k∗
instant t > T0, for some constant C

≤

≤

k

n/10000. Then with probability at least 1

δ, CRR ensures at every time

−

2.001
λn

XFAt+1 (X ⊤FAt+1 λt + gFAt+1)

0.98

λt

2 ≤

d
n

log

nd
δ

r

2 + Cσ
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Proof. For this we ﬁrst observe that, since entries in the set FAt+1 survived the hard thresholding step, they
must have been the largest elements by magnitude in the set S∗ i.e.

X ⊤FAt+1 λt + gFAt+1 = HT

FAt+1

(X ⊤S∗ λt + gS∗)

|

|
k∗ with respect to the data points and the Gaussian

Note that
noise. Thus, if we denote by St

≤

FAt+1

k and S∗ is a ﬁxed set of size n

−

(cid:12)
(cid:12)

(cid:12)
(cid:12)

k, the set of top k coordinates by magnitude in S∗ i.e.
= HTk(X ⊤S∗ λt + gS∗ ),

+ gSt

X ⊤St

k

k

then

XFAt+1 (X ⊤FAt+1λt + gFAt+1 )
In the following, we will, for sake of simplicity, omit the subscript S∗.
(cid:13)
(cid:13)
(cid:13)
(cid:13)

λt + gSt

(X ⊤St

XSt

2 ≤

)

2

k

k

k

(cid:13)
(cid:13)
(cid:13)

Before we move ahead, we make a small change to notation for convenience. At the moment, we are
1X)ǫ and analyzing the vector X ⊤λt + g.

deﬁning λt = (XX ⊤)−

X ⊤(XX ⊤)−

1(bt

. Thus, all we need to do is bound this term.

(cid:13)
(cid:13)
(cid:13)
b∗) and g = (I

−

−

14

However, this is a bit cumbersome since g is not distributed as a spherical Gaussian, something we would
like to be able to use in the subsequent proofs. To remedy this, we simply change notation to denote
1Xǫ and g = ǫ. This will not aﬀect the results in the least since we
λt = (XX ⊤)−
−
have, as shown in the proof of Lemma 4,
σ because of which we can set n large

n log d

(XX ⊤)−

(XX ⊤)−

1Xǫ
σ
100 still holds. Given this, we prove the following result:

1(bt

b∗)

2 ≤

q

−

σ

2d

enough so that

λt

2 ≤

×

Lemma 18. Let X = [x1, x2, . . . , xn] be a data matrix consisting of i.i.d.
xi ∼ N
In
λ
∈
vi|
|
we have 1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
d), and g
(0, Id
Rd such that
λ
> τ and zi = 0 otherwise. Then, with probability at least 1
λ
k2 + 2.02σ

standard normal vectors i.e
N (0, σ2
n) be standard normal vector drawn independently of X. For any
∼
100 , deﬁne v = X ⊤λ + g. For any τ > 0, deﬁne the vector z such that zi = vi if
σ
100 ,

δ, for all λ
∈
τ + 1
τ

δ , where M (τ ) < 0.808

Rd with norm at most σ

τ 2
2.001σ2

.

n log nd

k2 ≤

M (τ )

k2 ≤

λn k

exp

Xz

−

−

k

k

×

σ

d

·

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Proof. We will ﬁrst prove this result by ﬁrst assuming that λ is a ﬁxed d-dimensional vector with small
norm and X and ǫ are chosen independently of λ. We will then generalize to all small norm vectors in Rd
by taking a suitably ﬁne ǫ-net over them. Let us denote the ith row of X as X i, and the entry at the jth
n
λ + gj and hence vj and
j=1 X i
column in this row as X i
vj′ are independent for j
We also note that vj |

j. Then (Xz)i = z⊤X i =
= j′. Because of this, X i
jλi, σ2 +
X i
vi. Using a simpler notation temporarily x := X i

jzj. Note that vj = x⊤j
jzj is also independent from X i
P
=i λ2
i′ ). Let ˜σ2 := σ2 +
j, z := zj and v := vj lets us write

i′ . Note that zi = I

j ∼ N

=i λ2

j′ zj′ .

vi|

(X i

> τ

} ·

{|

i′

i′

(cid:0)

(cid:1)

(cid:16)

(cid:17)

q

P

P

E [xz] =

R

Z

[

\

−

R
τ,τ ] Z

xv p(x, v) dx dv.

Let Di :=

I + λ2
i
σ2
(cid:16)

1/2

(cid:17)

. Then for any ﬁxed v, we have

xv p(x, v) dx =

R

Z

=

=

=

=

≤

xv p(x)p(v

x) dx

|

R

Z

R

Z

1
˜σ(√2π)2
vD−
i
˜σ(√2π)2

2

2

vD−
i

R

Z
exp

(cid:19)

xv exp

(cid:18)

x2
2
−
u2
2
−
(cid:18)
2˜σ2 + v2D−2
i λ2
2˜σ4

v2

i

+

u exp

v2D−

i λi exp

3

−
(cid:16)
˜σ(√2π)2

(cid:17)
Z
2˜σ2 + v2D−2
i λ2
2˜σ4

v2

i

R

−
(cid:16)
˜σ3√2π
v2
2.001σ2

−

(cid:17)

,

(cid:17)

v2D−

i λi exp

3

1.001σ3√2π

(cid:16)

(v

xλi)2

exp

−

(cid:18)

v2
˜σ2 −

2vuD−
˜σ2

−
2˜σ2
1
i λi

(cid:19)

dx

(cid:19)

du

u exp

1
2

 −

u

(cid:18)

−

1
i λi
vD−
˜σ2

2

!

(cid:19)

du

where in the third step, we perform a change of variables u = Dix and in the last step, we use the fact that
2. Plugging this into the expression for E [xz]
˜σ2
and using elementary manipulations such as integration by parts gives us

σ2 + σ2/10000 since

σ/100, as well as λ2

i ≤ k

k2 ≤

λ
k

λ
k

≤

2

E

X i

jzj

= M (τ )λi, i.e., E

λT xjzj

= M (τ )
k

λ

2
2,

k

h

i

where M (τ ) < 0.8
for any j, λT xj is a
(cid:0)

τ

τ

σ + σ
2
2. Moreover,
λ
hP
k2-subGaussian random variable and zj is a 2σ-subGaussian random variable as

= nM (τ )
k

. This gives us E

λT xjzj

n
j=1

−

λ

(cid:16)

(cid:17)

k

k

i

(cid:1)

(cid:2)
exp

(cid:3)
τ 2
2.001σ2

15

6
6
6
λ
k2 ≤

σ/100. Hence, λT xjzj is a sub-exponential random variable with sub-exponential norm 2σ
λ
k2.
k
Using the Bernstein inequality for subexponential variables [21], then allows us to arrive at the following
result, with probability at least 1

δ.

k

−

n

j=1
X

λT xjzj ≤

nM (τ )
k

λ
k

2
2 + 2

λ

σ

k

k2

p

2
δ

.

n log

r

Taking a union bound over an ǫ-net over all possible values of λ (i.e. which satisfy the norm bound), for
δ, for all λ
ǫ = 1/100 gives us, with probability at least 1

Rd satisfying

λ
k

k2 ≤

σ
100 ,

−

1
λn

λT Xz

≤

1.01M (τ )

λ
k

k

2
2 + 2.02

σ

λ
k2

k

d
n

r

log

200
δ

.

(8)

Rd s.t. λT v = 0. In this case, vT xj is independent
Now, again consider a ﬁxed λ and a ﬁxed unit vector v
of zj. Hence, E [[] vT xjzj] = 0. Moreover, vT xjzj is a 2σ-subexponential random variable. Moreover,
number of ﬁxed λ and v in their ǫ-net is 1
. Hence, using the subexponential Bernstein inequality
ǫ
and using union bound over all v and λ, we get (w.p.

δ):

∈

1
ǫ

1

−

d

d

1

·

∈

p

≥

−

max
v,λ

1
λn

vT Xz

≤

2.02

σ

r

d
n

log

200
δ

.

(9)

Lemma now follows by using

This establishes the claimed result.

Xz
k

k

2 = 1
2
2
λ
2
k
k

(λT Xz)2 + maxv,

k2=1,vT λ=0(vT Xz)2 with (8) and (9).

v

k

Although Lemma 18 seems to close the issue of convergence of the iterates λt, and hence the convergence
of wt and consistency, it is not so. The reason is twofold – ﬁrstly Lemma 18 works with a value based
thresholding whereas CRR uses a cardinality based thresholding. Secondly, in order to establish a linear
convergence rate for λt, we need to show that the constant M (τ ) is smaller than 98/100 so that we can

O

2 +
(cid:13)
(cid:13)

(cid:18)q

(cid:19)

ensure that

λt+1

1
100 + 0.98

λt

0.99

λt

2 ≤

2 ≤

d
n

, thus ensuring a linear convergence for

(cid:0)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

λt, save negligible terms. We do both of these in the subsequent discussion.
(cid:1) (cid:13)
(cid:13)

We address both the above issues by showing that while thresholding the vector X ⊤λt + g (recall that
for sake of notational convenience we are still omitting the subscript S∗), the kth top element in terms of
magnitude will be large enough. Thus, thresholding at that value will recover the top k elements. If we are
able to get a sample independent bound on the magnitude of this element then we can set τ to this in the
analysis of Lemma 18 and be done. Of course, it will still have to be ensured that for this value of τ , we
have M (τ ) < 1.

(cid:13)
(cid:13)

e

To simplify the discussion and calculations henceforth, we shall assume that σ = 1, δ = 1, and k = k∗.
We stress that all our analyses go through even for non-unit variance noise, projection parameters that diﬀer
from the true corruption sparsity (i.e. k
= k∗), as well as can be readily modiﬁed to give high conﬁdence
bound. However, these assumptions greatly simplify our analyses.

We notice that the vector being thresholded has two components X ⊤λt and g. Whereas g has a nice
characterization, being a standard Gaussian vector, there is very little we can say about the vector X ⊤λt
other than that the norm of the vector λt is small. This is because the vector λt is dependent on previous
iterations and hence, dependent on X as well as g. The way out of this is to show that the kth largest
element in g is reasonably large and X ⊤λt, on account of its small norm, cannot diminish it.
To proceed in this direction, we ﬁrst recall the coarse convergence analysis. Letting α := k∗

n and making

the assumptions stated above we know that

λT0

k

k2 ≤ C

(α) where

(α) = 2.001√2α

1 + 2e

s

C

e
2α

.

6 log

r

16

6
→

0 C

(α) = 0, as well as that

√n. This bound gives us an idea about
Note that limα
how much weight lies in the vector X ⊤λt in the iterations t > T0. Next we look at the other component g.
(cid:13)
(cid:13)
(cid:13)
For any value η > 0, the probability of a Gaussian variable exceeding that value in magnitude is given by
erfc(η/√2), where erfc is the complimentary error function. By an application of Chernoﬀ bounds, we
√2
Ω(n))
can then conclude that in any ensemble of n such Gaussian variables, with probability at least 1
fraction) of points will exceed the
at least a 0.99
value η.

fraction (as well as at most a 1.01

X ⊤λT0
(cid:13)
(cid:13)
(cid:13)

2 ≤ C

exp(

η
√2

η
√2

erfc

erfc

(α)

−

−

·

·

·

·

(cid:16)

(cid:17)

(cid:16)

(cid:17)

We also recall the quantity

M (ζ) < 0.8

ζ +

(cid:18)

exp

1
ζ

(cid:19)

ζ2
2.001

,

(cid:19)

−

(cid:18)

and notice that, in order for M (ζ) to get less than 98/100, ζ must be greater than 0.99. Now the previous
Ω(n)), at least a
estimate for bounds on Gaussian variables tells us that with probability at least 1
β = 1/25 fraction of values in the vector g, which is a standard Gaussian (since we have assumed σ = 1 for
sake of simplicity) will exceed the value 1.98.

exp(

−

−

Let Sβ denote the set of coordinates of g which exceed the value 1.98. Let us call a coordinate i

corrupted if

(X ⊤λT0 )i

0.98. Now we notice that if this happens for (β

α)

≥
α)n. Thus, we set

(cid:12)
(cid:12)
2 ≥
(cid:12)

(β

0.98

X ⊤λT0
from happening. We note that for all values of α < 1
(cid:13)
n
(cid:13)
(cid:13)
points in the set S are of magnitude at least 1 and thus we can set τ = 1 in Lemma 18 which then ﬁnishes
the proof since M (1) < 0.98.

10000 this is true. This ensures that at least k∗ = α
p

α)n to prevent this

α)n = 0.98

√n < 0.98

(0.04

(α)

(cid:13)
(cid:13)
(cid:13)

p

p

(β

−

−

C

·

·

(cid:12)
(cid:12)
−
(cid:12)

Sβ
n points in the set Sβ, then

∈

−

·

B Supplementary Material for Consistent Robust Time Series Es-

timation

B.1 Main Result

mw∗
Mw∗ +
(log(
k

M
b∗

O

Theorem 9. Let y be generated using AR (d) process with k∗ additive outliers (see (3)). Also, let k∗
C
d log n (for some universal constant C > 0). Then, with probability at least 1

≤
δ, CRTSE, after

≤

k

n

W

k2 /n) + log(n/(σ

·

d))) steps, ensures that

wt
k

−

w∗

k2 ≤ O

σ

w∗ /mw∗

M

Proof. Putting together the Lemma 11 and the equation (7) establishes that

−

d log n/n log (d/δ)

.

p

(cid:17)

(cid:16)

λt+1

0.51

λt

2 ≤

2 +

σ
O  

r

d log n
n

log

d
δ !

,

2 to a value
(cid:13)
(cid:13)

O

σ

d log n
n

log d
δ

(cid:18)

q

(cid:19)

. Applying the equation

(cid:13)
(cid:13)
which ensures a linear convergence of the terms

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)
λt

(6), then ﬁnishes oﬀ the result.

(cid:13)
(cid:13)

B.2 Back ground on Time Series

AR (d) process is deﬁned as

xt = xt

−

1w∗1 +

+ xt

−

dw∗d + ǫt where ǫt ∼ N

· · ·

(0, σ2).

(10)

17

Note that xt ∼ N
we have

(0, Γ(0)), where Γ (h) = E [xtxt+h] is the auto-covariance function of the time series. Then

x1
...
xn



· · ·

= 

x0
...
xn


· · ·


y∗ = X ⊤w∗ + ǫ.

−

1






x

d+1

−

...
xn






d

−

w∗1
...
w∗d

· 



+ 









ǫ1
...
ǫn






The spectral density of this AR (d) process can be given as

ρw∗ (ω) =

1

d
k=1 w∗keikω

−

d
k=1 w∗ke−

ikω

−

(cid:16)

P

P

(cid:17)

σ2

1

(cid:17) (cid:16)

, for ω

∈

[0, 2π] .

Observe that any column vector of the matrix X is distributed as X i ∼ N

(0, CX ), where

(11)

(12)

Γ (0)
Γ (1)
...

Γ (1)
Γ (0)
...

Γ (d

1) Γ (d

2)

−

−

· · ·
· · ·
. . .

· · ·

1)
2)

Γ (d
Γ (d

−
−
...
Γ (0)



.






CX = 





Since CX is a block-Toeplitz matrix, we have

mw∗ := inf

ω

[0,2π]

∈

ρw∗ (ω)

Λmin [CX ]

≤

≤

Λmax [CX ]

≤

ω

sup
[0,2π]

∈

ρw∗ (ω) =:

w∗.

M

(13)

The columns of X can be viewed as a d-variate of VAR (1) process as follows

xi
xi
...

−

1

xi

(d

−

−

1)












=











Xi = W

w∗1 w∗2
0
1
1
0
...
...
0
0

Xi

−

1 +

b



· · ·
· · ·
· · ·
. . .

1 w∗d
0
0
...
0

w∗d
−
0
0
...



1



Ei, for i = 1, . . . , n.

· · ·

·

1

2

−

−

xi
xi
...
xi

d

−








ǫi
0
...
0

+ 



















(14)

By letting

u∗ = 

b
X1
...
b
Xn









the above VAR (1) process can be compactly written as follows
b

Rnd,

∈

U

= 

Rnd, and



∈

X0
...
b
Xn




1

−




b
u∗ = W

+

.

E

U

Rnd

E

= 

E1
...
En



∈






Then the spectral density of the above VAR (1) process is given by

ρW (ω) =

I

W e−

iω

1

−

−

(cid:0)

(cid:1)

where

Σǫ

I
h(cid:0)
σ2
0
...
0

−

0
0
...
0

∗ , for ω
i

∈

[0, 2π] ,

W e−

iω

1

−

(cid:1)
0
0
...
0








.

· · ·
· · ·
. . .

· · ·

Σǫ = 





18

The covariance matrix of vector

= E

C
U

⊤

UU

(cid:2)

(cid:3)

U

=

is given by

E

X0

X ⊤0

Since C
U

is a block-Toeplitz matrix, we have

E

E

h

i

X0

X ⊤1

X ⊤1
b

i

i

b
X ⊤1

1

X1
b
...

h

b
Xn
h

b

−

i

b

E



E

h

X ⊤0
b

i

b
X ⊤0

1

X1
b
...

h

b
Xn
h

b

−

i

b









E

· · ·

· · ·
. . .

· · ·

E

E

E

X0
h
X1
b
h

b
Xn

1

1

X ⊤n
−
X ⊤n
b
−
...
b
X ⊤n
−

1

i

i

1

−

h

b

b

.










i

Λmax [C
U

]

≤

ω

sup
[0,2π]

∈

ρW (ω) =

inf ω

∈

[0,2π] Λmin [(I

σ2
W ⊤eiω) (I

−

W e−

iω)]

−

=:

MW .

(15)

Consider a vector q = X ⊤a
(0, Qa) where Qa =

that q

∼ N

(cid:1)
trace (Qa) = na⊤CX a

Qa

∈

Rn for any a
a⊤
In ⊗
(cid:0)

C
U

≤
2
a
2 Λmax [C
k
U
trace (QaQa)

Sd
∈
(In ⊗
nΛmax [CX ]
]

1. Since each element X ⊤
−
a). From this we can note that

i a

∼ N

n

w∗

M

≤

0, a⊤CX a

, it follows

(cid:0)

(cid:1)

(16)

(17)

k
Qa

≤ MW
Qa
k
q
Additive Corruptions: Now consider the following additive corruption mechanism (at most k∗ data
points):

k2 ≤ k
kF =

k2 trace (Qa)

MW M

(18)

w∗ .

p

p

≤

≤

n

k

Since we observe the corrupted time series data (y

d+1, . . . , y0, y1, . . . , yn), we have

−

yi = y∗i + e∗i = xi + e∗i for i = 1, . . . , n.

y0
y1
y2
...
yn

−










y
1
−
y0
y1
...
yn

−

2

· · ·
· · ·
· · ·

· · ·

1

y
y
y

d+1

d+2

−

−

d+3





=

−

...
yn

−














X ⊤ = X ⊤ + E⊤

−

d

x0
x1
x2
...
xn

x
1
−
x0
x1
...
1 xn

2

−

x
x
x

−

−

d+1

d+2

d+3

−

...
xn

d

−

· · ·
· · ·
· · ·

· · ·

Thus the observed time series can be modeled as follows

y = y∗ + e∗

= X ⊤w∗ + ǫ + e∗

E⊤)w∗ + ǫ + e∗

= (X ⊤
= X ⊤w∗ + ǫ + b∗e∗,w∗ ,

−

0
e∗1
e∗2
...
e∗n

−

0
0
e∗1
...
1 e∗n

−










+










· · ·
· · ·
· · ·

2

· · ·

0
0
0
...
e∗n
−










d

(19)

(20)

where e∗ = (e∗1, . . . , e∗n)⊤ is k∗-sparse, and b∗e∗,w∗ = e∗
(since Ew∗ is k∗-block-sparse with block size of d).

−

E⊤w∗ is k∗-block-sparse with block size of d + 1

B.3 Singular values of X

Lemma 19. Let X be a matrix whose columns are sampled from a stationary and stable VAR (1) process
(0, CX ). Then for any ǫ > 0, with probability at least 1
given by (14) i.e. X i ∼ N
XX ⊤

δ, X satisﬁes

w∗ + α1(d, δ, ǫ)

nα1(d, δ, ǫ)

w∗ + (1

λmax

2ǫ)−

−

n

1

≤

M

−

(cid:16)

(cid:17)

MW M

MW

o

np
19

λmin

XX ⊤

(cid:16)

≥
(cid:17)
δ + cd log 3

where α1(d, δ, ǫ) = c log 2

nmw∗

(1

−

−

1

2ǫ)−

ǫ for some universal constant c.

nα1(d, δ, ǫ)

MW M

w∗ + α1(d, δ, ǫ)

MW

,

o

np

Proof. Using the results from [4, 14], we ﬁrst show that with high probability,

XX ⊤

nCX

−

2 ≤

ǫ1

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

(cid:16)

for some ǫ > 0. Doing so will automatically establish the following result

nΛmin [CX ]

ǫ1 ≤

−

λmin

XX ⊤

λmax

XX ⊤

≤

(cid:17)

(cid:16)

(cid:17)

nΛmax [CX ] + ǫ1.

≤

1 (ǫ)

Let Cd
5.2) guarantee such a cover of size at most

1 be an ǫ-cover of Sd

Sd

⊂

−

−

−

1 ([21], see Deﬁnition 5.1). Standard constructions ([21], see Lemma

d

1 + 2
ǫ

≤

d

3
ǫ

. Further by Lemma 5.4 from [21], we have

XX ⊤

nCX

−

2 ≤

(cid:0)
(1

−

(cid:1)
2ǫ)−

1

(cid:0)

(cid:1)
sup
C d−1(ǫ)

u

∈

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

(cid:16)

(cid:12)
(cid:12)
(cid:12)

u⊤

XX ⊤

nCX

u

.

−

By following the analysis given in [4, 14], we can provide a high probability bound on

1, let q = X ⊤u

Sd

−

For any u
∈
q⊤q = z⊤Quz, where z
zikψ2 ≤
[18], with
k
P

∼ N
1 since zi ∼ N
u⊤

XX ⊤

(0, Qu) where Qu =

∼ N

(0, In). Also, u⊤nCX u = E

In ⊗
z⊤Quz
(cid:0)

(0, 1), we get

(cid:1)

nCX

−

u

> λ

= P

z⊤Quz

(cid:2)

(cid:3)

E

−

z⊤Quz

> λ

(cid:17)

(cid:12)
(cid:12)
(cid:12)
u⊤

XX ⊤

nCX

u

.

−

(cid:12)
u⊤
u). Note that u⊤XX ⊤u =
(cid:12)
(cid:12)
. So, by the Hanson-Wright inequality of

(In ⊗

C
U

(cid:12)
(cid:12)
(cid:12)

(cid:17)

(cid:16)

(cid:16)

h(cid:12)
(cid:12)
(cid:12)

(cid:17)

(cid:12)
(cid:12)
(cid:12)

i

(cid:2)(cid:12)
(cid:12)
2 exp

(cid:2)

min

1
c

λ2
Qu
k

(cid:3)(cid:12)
(cid:12)
,
2
F
k

(cid:3)
λ
Qu

≤

 −

k2 )!
k2, and taking a union bound over all Cd

(

k

.

1 (ǫ), we get

−

Setting λ =

α1(d, δ, ǫ)

Qu

kF + α1(d, δ, ǫ)

Qu
k

k

u⊤

XX ⊤

nCX

u

>

α1(d, δ, ǫ)

Qu
k

kF + α1(d, δ, ǫ)

Qu
k

k2

#

p

≤

P

2

sup
C d−1(ǫ)

d

exp

"
u

∈
3
ǫ

(cid:18)

(cid:19)

(cid:12)
(cid:12)
(cid:12)
 −

(cid:16)
1
c

min

(

−

λ2
Qu
k
δ,

2
F
k

−

(cid:12)
(cid:12)
λ
(cid:12)
Qu

(cid:17)

,

k

p

δ.

k2 )! ≤

This implies that probability at least 1

XX ⊤

nCX

−

2 ≤

1

2ǫ)−

(1

−

α1(d, δ, ǫ)

Qu

kF + α1(d, δ, ǫ)

Qu
k

k2

k

,

which (along with the bounds given in (16),(17), and (18)) gives us the claimed bounds on the singular
values of XX ⊤.

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

np

o

B.4 Restricted Singular values of X

Lemma 20. Let X be a matrix whose columns are sampled from a stationary and stable VAR (1) process
n
δ, the matrix X satisﬁes
given by (14) i.e. X i ∼ N
d , with probability at least 1
the SGSC and SGSS properties with constants

(0, CX ). Then for any k

≤

−

Λk ≤

k

d

M

(

w∗ +

d
MW M

r

w∗

1
c

log

en
kd

+

MW log

en
kd )

20

λk ≥

kdα2(d, δ)

MW M

(α2(d, δ)

w∗

+

(cid:17)

O

w∗ +

d

M

(

d
MW M

r

w∗

MW )
1
c

log

n

+

O

nmw∗

(cid:16)p

−

(cid:16)

n
d −

k

Ω

−

1 +

r

(cid:17)
n

kd
−
n !

nα2(d, δ)

MW M

p

w∗

! −

Ω (α2(d, δ)

MW ) ,

en

kd

−

+ log

en

kdMW

)

n

−

where α2(d, δ) = log 1

δ + d and c is some universal constant.

Proof. One can easily observe that considering the columns-restricted matrix X S wouldn’t impact the anal-
ysis of Lemma 19. Thus for any ﬁxed S

Gk , Lemma 19 guarantees the following bound (since

= kd)

S

∈ S

|

|

λmax

X SX ⊤
S

(cid:16)

(cid:17)

kd

M

≤

w∗ + (1

−

1

2ǫ)−

kdα1(d, δ, ǫ)

MW M

w∗ + α1(d, δ, ǫ)

MW

.

o

Taking a union bound over

Gk and noting that
S

, gives us with probability at least 1

δ

−

np

≤

k

en
kd

Gk

S
(cid:12)
(cid:12)
M

kd

(cid:12)
w∗ + (1
(cid:12)

(cid:0)

(cid:1)
−

2ǫ)−

1M,

where

Λk ≤

M =

α1(d, δ, ǫ) + ck log

r(cid:16)

≤

α1(d, δ, ǫ)kd

MW M

en
kd

(cid:17)
w∗ + k

p

kd

MW M

w∗ +

α1(d, δ, ǫ) + ck log

(cid:16)
en
kdMW M

cd log

r

en
kd

(cid:17)

MW

w∗ +

α1(d, δ, ǫ) + ck log

(cid:16)

c) and noting that Θ

−

(cid:17)
1
c α1

en
kd

MW .

d, δ, 1

−
2

c

=

(cid:0)

(cid:0)

(cid:1)(cid:1)

If c < 1 (which can be ensured by scaling), by setting ǫ = 1
Θ

, we get

log 1

2 (1

δ + d
(cid:1)

(cid:0)

Λk ≤

k

d

M

(

w∗ +

d
MW M

r

w∗

1
c

log

en
kd

+

MW log

en
kd )

+

kd

O  s

1
δ

log

(cid:18)

+ d

(cid:19)

MW M

w∗

+

O

!

log

(cid:18)(cid:18)

1
δ

+ d

(cid:19)

MW

.

(cid:19)

For the second bound, we use the equality

which provides the following bound for λk,

X SX ⊤

S = XX ⊤

X SX ⊤
S ,

−

λk ≥

λmin

XX ⊤

(cid:16)

−

T

(cid:17)

max
G
n
d

∈S

−k

λmax

X T X ⊤
T

= λmin

XX ⊤

(cid:16)

(cid:17)

(cid:16)

(cid:17)

Λ n

d −

k.

−

Using Lemma 19 to bound the ﬁrst quantity and the ﬁrst part of this theorem to bound the second quantity
gives us, with probability at least 1

δ,

−

λk ≥

nmw∗

n
d −

k

−

(cid:16)

(1

−

−

1

2ǫ)−

( 

d
(cid:17) (cid:26)
1 +

w∗ + (1

1

2ǫ)−

−

cd log

(cid:18)r

en

kdMW M

n

−

w∗ + c log

M

n

kd
−
n !

r

nα1(d, δ, ǫ)

MW M

w∗ + 2α1(d, δ, ǫ)

MW

p

21

en

kdMW

(cid:19)(cid:27)

n

)

−

.

  
By setting ǫ = 1

2 (1

−

c) we get the following bound

λk ≥

nmw∗

n
d −

−

(cid:16)

w∗ +

d

M

(

d
MW M

r

w∗

1
c

log

en

kd

n

−

+ log

n

kdMW

)

Ω

−

1 +

r

kd

n
−
n ! s

1
δ

log

(cid:18)

+ d

(cid:19)

MW M

w∗

! −

Ω

(cid:18)(cid:18)

log

+ d

(cid:19)

MW

.

(cid:19)

en

−
1
δ

k

(cid:17)
n

M

w∗ ,

Remark 21. Note that
and σ (not on the realized data). Moreover
time-series with 0 < w∗1 < 1, we have
large enough n so that √n
as follows

≪

MW and mw∗ will depend only on the actual model parameter vector w∗
MW are closely related. For example, for AR(1)
1 )2 and mw∗ =
1 )2 . Then for suﬃciently
n, the restricted singular value bounds of X from Lemma 20 can be simpliﬁed

w∗ and
MW =

σ2
(1+w∗

w∗ =

σ2
w∗

M

M

(1

−

Λk ≤ O
λ n

d ≥

k

d

(cid:18)

M
(cid:26)
Ω (nmw∗) .

w∗ +

r

d

M

w∗

MW log

en
δkd

+

MW log

en
δkd

and

(cid:27)(cid:19)

B.5 Restricted Singular values of X
Theorem 22 (SGSS/SGSC in AR (d) with AO model). Let X be the matrix given in (3) (additive corrupted
AR (d) model setting). Then for any k
δ,
the matrix X satisﬁes the SGSC and SGSS properties with constants

n
d and suﬃciently large enough n, with probability at least 1

−

≤

k

d log n

Λk ≤ O
λ n

d ≥

(cid:18)

(cid:26)
Ω (nmw∗) .

w∗ +

M

d

M

r

w∗

MW log

en
δkd

+

MW log

en
δkd

(cid:27)(cid:19)

Proof. Recall that the matrix X can be decomposed as follows

X = X + E.

2

−

k

k

∈

1,

e∗

d
i=1

Ev
k

(E⊤)i, v

Since for any v
√d

2
Sn
2 =
k2. By using the inequality
Λmin
E

XS −
P
(cid:13)
(cid:13)
k2 ≤
σ =
and Remark 21, for suﬃciently large enough n (with probability at least 1

Since e∗ is k∗-sparse and e∗i ≤

(E⊤)i
k2 we get
(cid:13)
(cid:13)
(cid:13)
(cid:13)
Λmax
Λmax [XS]

≤
ESk2 ≤ k
(cid:11)
P

(√log nσ), we have

(cid:13)
Λmin [XS]
(cid:13)

k2 ≤ O

d
i=1
E

e∗
k

2 ≤ k

X S
(cid:10)

− k

X S

O

≤

≤

(cid:3)

(cid:2)

2
2 k

v

2
2 ≤

k

d

e∗

2
2, we get
k

k

E
k

k2 ≤

E

+

X S

k2 .
(√k∗ log nσ). Thus from Lemma 20

k

(cid:3)

(cid:2)

δ) we get

−

b

Λk ≤ O 


p

k

s

(cid:26)

w∗ +

d

M

d

M

r

w∗

MW log

en
δkd

+

MW log

en
δkd



(cid:27)

+

(

O

k∗d log nσ)

p

≤ O 

k

s

(cid:26)

d log n

w∗ +

M

d

M

r

w∗

MW log

en
δkd

+

MW log

λ n

d ≥



Ω (√nmw∗)

Ω(

k∗d log nσ)

−

≥

Ω (√nmw∗ ) ,


en
δkd

(cid:27)





q

which completes the proof.

p

22

  
Remark 23. Using Theorem 22, we can bound
of CRTSE) as follows (with probability at least 1

√Λk+k∗
λ n
d

−

(which is required for the coarse convergence analysis

δ, and suﬃciently large enough n)

Λk+k∗
λ n

d

p

≤ O 

1
k
nmw∗ s

(cid:26)

d log n

w∗ +

M

d

M

r

w∗

MW log

en
δkd

+

MW log

en
δkd



f (w∗, σ)√log n
n

=

(k + k∗)

s

d + 2e

6d log

(cid:18)

r

en
δ(k + k∗)d

,
(cid:19)



(cid:27)



for some positive function f (w∗, σ) (suppressing

w∗ ,
From Theorem 22, it can also be observed that, if k
δ, we get Λk+k∗
λ n

C > 0), then with probability at least 1

M

−

MW and mw∗ ).
mw∗
Mw∗ +
≤
M
1
4 .
d ≤

C
Λ2k
λ n

W

d ≤

n

d log n (for some universal constant

B.6 Bound on

Xǫ

k

k2

Lemma 24. Let X be the matrix given in (3) (additive corrupted AR (d) model setting). Then with proba-
bility at least 1

δ,

−

for some constant c′ > 0.

Xǫ
k

k2 ≤

2σ

n

r

log nc′d log

2d
δ

.

p

Proof. We ﬁrst bound the absolute value of (Xǫ)i =
E [zj|
ǫ1, . . . , ǫj
zj : j
{
1)
for any j, (zj|
−
have

1] = 0,
−
ǫ1, . . . , ǫj

[n]
}
(0, x2
j

∈
∼ N

n
j=1
is a martingale diﬀerence sequence w.r.t

i. Since
−
. Also note that
iσ2). Then using the tail bounds on Gaussian random variables we

i for i = 1, . . . , d. Let zj := ǫjxj
ǫj : j

ǫjxj

[n]

P

∈

{

}

−

−

P [

zj|

|

> t

|

ǫ1, . . . , ǫj

1]

−

2
π

≤ r

1

xj

|

i|

−

σ

exp

t2
iσ2

−
2x2
j

−

2
π

1
c√log nσ2

exp

t2
−
2c2 log nσ4

,

(cid:19)

(cid:18)

! ≤ r

since supi
∈

[n] |

xi| ≤ O

√log nσ

with high probability. Then by using Theorem 2 from [19], we get

(cid:0)
P

(cid:1)

n

zj > nǫ





j=1
X

exp

−
28







≤



for some constant c′. Similarly we also have

1

2c2 log nσ4 nǫ2

2
π

1
c√log nσ2

q





= exp

nǫ2
c′√log nσ2

−

,

(cid:19)

(cid:18)

P

n





j=1
X

zj <

nǫ



−

≤

exp

nǫ2
c′√log nσ2

−

.

(cid:19)

(cid:18)



Then by using the union bound we get (for any δ > 0)

n

P

(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
That is with probability at least 1
(cid:12)
−

j=1
X

> nǫ

zj(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
δ we have
(cid:12)





2 exp

≤

(cid:18)

nǫ2
c′√log nσ2

−

= δ.

(cid:19)

(Xǫ)i

nǫ

≤

≤

(cid:12)
(cid:12)

(cid:12)
(cid:12)

σ

n

log nc′ log

p

r

23

2
δ

.

 
Taking a union bound gives us, with the same conﬁdence,

Xǫ

2
2 ≤

σ2n

log nc′d log

2d
δ

.

Now we bound the absolute value of (Eǫ)i =
1)

for any j, (zj|
∼ N
similar analysis as above, we have with probability at least 1

(cid:13)
(cid:13)
i,jσ2) and supi,j
P

ǫ1, . . . , ǫj

(0, E2

[n] |

p
n
j=1

(cid:13)
(cid:13)

−

∈

ǫjEi,j for i = 1, . . . , d. Let zj := ǫjEi,j . Note that
. Then by following the

√log nσ

σ =

Ei,j | ≤
δ,
−

O

(cid:0)

(cid:1)

b
2d
δ

.

2
Eǫ
2 ≤
k

k

σ2n

log nc′d log

Then using the triangular inequality, with probability at least 1

p

δ,

−

Xǫ

k2 ≤

k

Xǫ

2 +
(cid:13)
(cid:13)

(cid:13)
(cid:13)

Eǫ
k

k2 ≤

2σ

n

r

log nc′d log

2d
δ

.

p

B.7 Coarse Convergence Analysis

Theorem 10. For any data matrix X that satisﬁes the SGSC and SGSS properties such that 4Λk+k∗

< 1,

CRTSE, when executed with a parameter k

k∗, ensures that after T0 =

5e0, where e0 =

σ

small i.e. k∗

O
k

≤

C
q

(cid:16)
≤

(k + k∗)d log
n

mw∗
Mw∗ +

W

M

enough, then with probability at least 1

≥

n
δ(k+k∗)d

2 ≤
for standard Gaussian AR (d) process. If k is suﬃciently
d log n (for some universal constant C > 0) and n is suﬃciently large

(cid:13)
(cid:13)

(cid:13)
(cid:13)

O

−

(cid:16)

(cid:17)

(cid:17)

log k

b∗
√n

k2

steps,

bT0

b∗

λ n
d

δ, we have 4Λk+k∗

λ n
d

−

< 1.

Proof. We start with the update step in CRTSE, and use the fact that y = X ⊤w∗ + ǫ + b∗ to rewrite the
update as

where PX = X ⊤(XX ⊤)−

1X. Since X ⊤ = PX X ⊤, we get

bt+1

HTGk (PX bt + (I

PX )(X ⊤w∗ + ǫ + b∗)),

←

−

bt+1

←

HTGk (b∗ + PX (bt

supp(b∗), λt := (XX ⊤)−
Let I t := supp(bt)
properties of the hard thresholding step gives us

∪

1X(bt

−

PX )ǫ).

b∗) + (I

−
b∗), and g := (I

−

PX )ǫ. Since k

−

k∗, using the

≥

(cid:13)
(cid:13)
bt+1

I t+1 −

(cid:13)
(cid:13)
b∗I t+1

bt+1

(b∗I t+1 + X ⊤I t+1 λt + gI t+1)

I t+1 −

−
This, upon applying the triangle inequality, gives us

2 ≤

b∗I t+1

(cid:13)
(cid:13)

(b∗I t+1 + X ⊤I t+1 λt + gI t+1)

2 =
(cid:13)
(cid:13)

X ⊤I t+1λt + gI t+1
(cid:13)
(cid:13)

2 .
(cid:13)
(cid:13)

2 ≤

2

X ⊤I t+1λt + gI t+1
(cid:13)
(cid:13)
Λk+k∗
λ n
d

2 .
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)
1X ⊤I t (bt

2 =

X ⊤I t+1(XX ⊤)−

Now, using the SGSC and SGSS properties of X (since G-supp(I t+1)
X ⊤I t+1λt
(cid:13)
(cid:13)
Lemma 20]), for any set S of size (k + k∗)d, we have with probability at least 1
6(k + k∗)d log 1
2eσ2
us, with probability at least 1

k
Since ǫ is a Gaussian vector, using tail bounds for Chi-squared random variables (for example, see [5,
σ2(k + k∗)d +
k
gives

k
δ . Taking a union bound over all sets of group size (k + k∗) and

δ, for all sets S of group size at most (k + k∗),

k + k∗), we can show that

2
2 ≤
n/d
k

ǫSk

k2.

2 ≤

b∗)

en
kd

b∗

δ ,

bt

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

≤

−

−

≤

−

q

(cid:0)

(cid:1)

(cid:0)

(cid:1)

−

ǫSk2 ≤
k

σ

p

(k + k∗)

d + 2e

s

6d log

r

en
δ(k + k∗)d

24

From Lemma 24, with probability at least 1

us to bound

k
gI t+1
k

gI t+1
k2 =

k2

ǫI t+1

−

X ⊤I t+1 (XX ⊤)−

1Xǫ

2

δ, we have

−

Xǫ
k

k2 ≤

2σ

n√log nc′d log 2d

δ . This allows

q

n

(cid:13)
(cid:13)
σ

≤

σ

≤

p

p

= 1.0003e0,

|

(k + k∗)

d + 2e

s

(cid:13)
en
(cid:13)
6d log
δ(k + k∗)d

r

+ 2σ

Λk+k∗
λ n

p

d r

log nc′d log

2d
δ

(k + k∗)

d + 2e

6d log

s

en
δ(k + k∗)d

1 + 2f (w∗, σ)

r

e0

{z





}

p
c′d (log n)3/2
n

s

log

2d
δ 



where the second last step is due to Remark 23 for suﬃciently large enough n so that √n
e0 does note depend on the iterates and is thus, a constant. This gives us

≪

n. Note that

bt+1

b∗

−

2 ≤

2Λk+k∗
λ n

d

bt

−

b∗

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

2 + 2.0006e0.
(cid:13)
(cid:13)
(k + k∗)d log
δ for k∗
bT0

−

2 ≤

(cid:13)
(cid:13)

(cid:13)
(cid:13)

in Theorem 22, assuming n

For data matrices sampled from AO-AR(d) ensembles, whose SGSC and SGSS properties are established
. Thus, if Λk+k∗
< 1
λ n
4
d
mw∗
(cid:17)
d log n ), then in
Mw∗ +
M
4.0015e0.

≥
(which is guaranteed by Remark 23 with probability at least 1
q
b∗
√n

−
steps, CRTSE ensures that

d log d, we have e0 =

n
δ(k+k∗)d
k

≤
b∗

T0 =

log k

log k

b∗
e0

O

≤

=

C

k2

k2

(cid:16)

σ

n

W

O

O

(cid:16)

(cid:17)

(cid:16)

(cid:17)

B.8 Fine Convergence Analysis

Lemma 11. Suppose k∗
least 1

≤
δ, CRR ensures at every time instant t > T0

≤

k

−

n/(C′d log n) for some large enough constant C′. Then with probability at

C
λn

(1 +

Λn
λn

)

XFAt+1 (X ⊤FAt+1λt + gFAt+1 )

0.5

λt

2 ≤

σ

r

O  

d log n
n

log

1
δ !

2 +
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Proof. As before, we change the problem so that instead of thresholding the top k elements of the vector
X ⊤λt + g by magnitude, we threshold all elements which exceed a certain value τ in magnitude. Again as
before, we show that with high probability, for suﬃciently small k, the kth largest element of the vector will
have a large magnitude.

Proving the second part of the result is relatively simple in the time series setting because of the error
tolerance bound k∗
n/(d log n) that we assume in this setting. For sake of simplicity, as well as without
loss of generality, assume as before that σ = 1. Then using the tail bounds for martingales with sub-Gaussian
entries from [19], we can yet again show that with probability at least 1
exp (Ω(n)), at least a 1/50 fraction
of points in the vector g will exceed the value 1.75 in magnitude.

≤

−

Xik2 ≤

Now, using the subset smoothness of the data matrix X from Theorem 22 on subsets of size 1 tells us that
(d log n), where Xi is the ith column of the data matrix X. Note that this also includes
k <

maxi k
the inﬂuence of the error vector e∗. Thus, if we assume k∗
k2
1
4 . This assures us that for any k < n/50, the k largest elements by magnitude

Λ1 ≤ O

1
4 maxi

X ⊤λt

1
d log n

k2 ≤

then

λT0

O

≤

Xi

(cid:17)

(cid:16)

k

k

which gives us
in the vector X ⊤λt + g will be larger than 1.5.
(cid:13)
(cid:13)

∞ ≤

(cid:13)
(cid:13)

Having assured ourselves of this, we move on to the analysis assuming that thresholding is done by value

and not by cardinality. Let z = [z1, z2, . . . , zn] where zi = (X ⊤i
n

n

λ + gi)

I

·

X ⊤i

λ + gi

> τ

. We have

Xz =

Xjzj =

j=1
X

i=1
X

Xj(X ⊤j

λ + gj)

I

·

25

(cid:8)(cid:12)
λ + gj
(cid:12)

X ⊤j

> τ

,

(cid:12)
(cid:12)

(cid:9)

(cid:8)(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:9)

where the previous result ensures that we can set τ
the following, we analyze the ith coordinate of the vector i.e.

≥

1.5, as well as safely assume that

0.25. In

λ

X ⊤j

≤

(cid:12)
(cid:12)

(cid:12)
(cid:12)

n

(Xz)i =

X i

j(X ⊤j

λ + gj)

X ⊤j

λ + gj

> τ

=:

ζi.

n

I

·

i=1
X

(cid:12)
(cid:12)
(0, σ2) which allows us to construct the following martingale diﬀerence sequence

(cid:8)(cid:12)
(cid:12)

(cid:9)

i=1
X

We notice that gi|

Xi ∼ N

n

i=1
X

ζi −

E [ζi|

g1, g2, . . . , gi

−

1]

We also note that the elements of the above sequence are conditionally sub-Gaussian with the sub-Gaussian
(log n). Then using the Azuma style inequality for martingales with sub-Gaussian tails from
norm at most
[19] gives us, with high probability

O

n

ζi −

E [ζi|

g1, g2, . . . , gi

−

1]

≤ r

112 log n
n

log

1
δ

i=1
X
1] = X i
j ·

−

Note that E [ζi|
(X ⊤j
Gaussian variable Y

g1, g2, . . . , gi

λ + gj) is conditionally distributed as

E

(X ⊤j

(cid:2)

N

(µ, 1), we have

∼ N

λ + gj)
(X ⊤j

I

X ⊤j

. Also note that
λ, 1) as we have assumed σ = 1 for simplicity. For a

g1, g2, . . . , gi

> τ

−

1

·

|

λ + gj

(cid:3)

(cid:8)(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:9)

E [Y

I

·

Y

{|

|

> τ

] = µ
}

−

E [Y

I

Y

{|

·

] =

τ

}

| ≤

φ(τ
Φ(

−

−
τ

µ)

φ(

τ
−
Φ(τ

−
µ)

−

−

µ)
µ)

,

−
−

where φ(
·
variable. Now, applying the mean value theorem gives us

) and Φ(
·

) are respectively, the density and cumulative distribution functions of the standard normal

φ(τ

|

−

µ)

−

φ(

−

τ

µ)
|

−

=

|

φ(τ + µ)

φ(τ

−

−

= 2

ηφ(η)µ
µ)
|
|
0.25, we have

,

|
φ(τ

for some η
0.25. For the same values we have Φ(

µ, τ +µ]. For the ensured values of τ = 1.25 and
τ

φ(
0.68. Putting these together, we get

Φ(τ

| ≤

µ)

µ)

µ)

−

−

−

[τ

∈

µ

|

|

τ

µ)

|

−

<

−

−

−

−

≥

−

n

(Xz)i| ≤

|

Cτ

X i

jX ⊤j

λ

+ D

· (cid:12)
(cid:12)
j=1
X
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

|

where
0.4 and D
Cτ | ≤
simply requiring that k∗
that Cτ ≤

0.9 Λn
λn

k <
q
. This gives us

≤
≤

C ′

·

112 log n
n

log 1

δ . We note that this value of Cτ can be made arbitrarily small by
n
d log n for a large enough constant C′ > 0. In particular, we set k, k∗ such

1
λn k

Xz

k2 ≤

Cτ
λn

XX ⊤λ

which concludes the proof.

(cid:13)
(cid:13)

d
λn

D

0.5

λ
k

k2 +

≤

d log n
n

log

1
δ !

,

O  r

2 +
(cid:13)
(cid:13)

26

