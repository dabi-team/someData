7
1
0
2

p
e
S
2
1

]

R
P
.
h
t
a
m

[

1
v
3
5
7
3
0
.
9
0
7
1
:
v
i
X
r
a

AR(1) sequence with random coefﬁcients: Regenerative properties and

its application

KRISHNA B. ATHREYA ∗

KOUSHIK SAHA †

RADHENDUSHKA SRIVASTAVA ‡

July 17, 2021

Abstract

Let {Xn}n≥0 be a sequence of real valued random variables such that Xn = ρnXn−1 + ǫn, n = 1, 2, . . .,

where {(ρn, ǫn)}n≥1 are i.i.d. and independent of initial value (possibly random) X0. In this paper it is shown

that, under some natural conditions on the distribution of (ρ1, ǫ1), the sequence {Xn}n≥0 is regenerative in

the sense that it could be broken up into i.i.d. components. Further, when ρ1 and ǫ1 are independent, we

construct a non-parametric strongly consistent estimator of the characteristic functions of ρ1 and ǫ1.

1 Introduction

Let {Xn}n≥0 be a sequence of real valued random variables satisfying the stochastic recurrence equation

Xn = ρnXn−1 + ǫn, n = 1, 2, . . .

(1)

where {(ρn, ǫn)}n≥1 are i.i.d. R2-valued random vectors and independent of the initial random variable X0. If
E(|X0|) < ∞ and E(ǫn) = 0, for each n ≥ 1 and then E(Xn|X0, . . . , Xn−1) = ρnXn−1. For this reason

the sequence {Xn} satisfying (1) is often referred to in the time series literature as Random Coefﬁcient Auto

Regressive sequence of order one (RCAR(1)) (see Andˇel [1976], Robinson [1978], Nicholls and Quinn [1980],

Brandt [1986]). Aue et al. [2006] studied a parametric model for (ρ1, ǫ1) under the assumption that ρ1 and ǫ1

are independent and provided a consistent estimator of the model parameters.

In the current paper, we ﬁnd

conditions on the distribution function of (ρ1, ǫ1) to ensure that {Xn} is a Harris recurrent Markov chain and

hence regenerative, i.e., it can be broken up into i.i.d. excursions. We exploit the regenerative property of

∗Departments of Mathematics and Statistics, Iowa State University, Iowa 50011, USA and Distinguished Visiting Profrssor, Depart-

ment of Mathematics, IIT Bombay, Mumbai 400076, India. kba@iastate.edu.

†Department of Mathematics, Indian Institute of Technology, Bombay, Mumbai 400076, India. ksaha@math.iitb.ac.in.
‡Department of Mathematics, Indian Institute of Technology, Bombay, Mumbai 400076, India. radhe@math.iitb.ac.in.

1

 
 
 
 
 
 
{Xn} to construct a non-parametric consistent estimator of the characteristic functions of ρ1 and ǫ1 under the

independence assumption of ρ1 and ǫ1.

A sequence {Xn}n≥0 is said to be delayed regenerative if there exists a sequence {Tj }j≥1 of positive integer
valued random variables such that P(0 < Tj+1 − Tj < ∞) = 1 for all j ≥ 1 and the random cycles ηj ≡ ({Xi :
Tj ≤ i < Tj+1}, Tj+1 − Tj) for j = 1, 2, . . . are i.i.d. and independent of η0 ≡ ({Xi : 0 ≤ i < T1}, T1). If

{ηj}j≥0 are i.i.d. then {Xn} is called non-delayed regenerative sequence. If, in addition, E(T2 − T1) < ∞ then

{Xn} is called regenerative and positive recurrent.

If {Xn} is a Markov chain with a general state space (S, S), that is Harris irreducible and recurrent (see

Deﬁnition 1) then it can be shown that {Xn} is regenerative (Athreya and Ney [1978]). Further if {Xn} admits

a stationary probability measure (necessarily unique because of irreducibility), then {Xn} is positive recurrent

regenerative as well.

In Sections 2 and 3, under some condition on the distribution of (ρ1, ǫ1) we show that the sequence {Xn}

satisfying (1) is positive recurrent and regenerative by establishing that {Xn} admits a stationary distribution and

is Harris irreducible, respectively. In Section 4, we show that the distribution of (ρ1, ǫ1) can be determined by

transition probability function of {Xn}. We subsequently provide a consistent estimator of transition probability

function of {Xn} by using the regenerative property. Finally, if ρ1 and ǫ1 are independent then we provide a

non-parametric consistent estimator of characteristic function of ρ1 and ǫ1, based on {Xn}n≥0.

2 Limit distribution of Xn

We begin with existence of the limiting distribution of Xn in (1).

Theorem 1. Let −∞ ≤ E(log |ρ1|) < 0 and E(log |ǫ1|)+ < ∞. Then {Xn} in (1) converges in distribution to
X∞ as n → ∞ where

X∞ ≡ ǫ1 + ρ1ǫ2 + ρ1ρ2ǫ3 + . . . + ρ1 . . . ρnǫn+1 + . . . .

(2)

The inﬁnite series on the right hand side of (2) is absolutely convergent with probability 1.

The above result can be deduced from Brandt [1986]. A proof of Theorem 1 is given in the appendix.

Theorem 1 does not indicate nature of limiting distribution of Xn. We show that the distribution of X∞ is

non-atomic when the distribution of (ρ1, ǫ1) is non-degenerate.

Theorem 2. Let −∞ ≤ E(log |ρ1|) < 0, E(log |ǫ1|)+ < ∞, P(ρ1 = 0) = 0 and (ρ1, ǫ1) has a non-degenerate
distribution. Then X∞ has a non atomic distribution, i.e., P(X∞ = a) = 0 for all a ∈ R.

Proof. Since (ρ1, ǫ1) has a nondegenerate distribution, the random variable X∞ as in (2) does not have a degen-
erate distribution and hence sup{P(X∞ = a) : a ∈ R} ≡ p < 1. Let a0 be such that P(X∞ = a0) = p. Then,

2

by Doob’s martingale convergence theorem (see page 211 of Athreya and Lahiri [2006]), we have

E(I(X∞ = a0)|Fn) → E(I(X∞ = a0)|F∞) w. p. 1

(3)

where, Fn ≡ σ{(ρi, ǫi) : i = 1, 2, . . . , n, X0}, the σ-algebra generated by (ρi, ǫi) for i = 1, . . . , n and X0, and
F∞ ≡ σ{(ρi, ǫi) : i ∈ N, X0}. Since X∞ is measurable with respect to F∞, E(I(X∞ = a0)|F∞) = I(X∞ =
a0). Next

E(I(X∞ = a0)|Fn)

= P(ǫ1 + ρ1ǫ2 + · · · + ρ1 · · · ρn−1ǫn + ρ1 · · · ρn(ǫn+1 + ρn+1ǫn+2 + · · · ) = a0|Fn)

= P

Yn =

(cid:18)

a0 − ǫ1 − ρ1ǫ2 − · · · − ρ1ρ2 · · · ρn−1ǫn
ρ1ρ2 · · · ρn

|Fn

(cid:19)

(since P(ρ1 = 0) = 0, |ρ1 · · · ρn| 6= 0 ∀ n ≥ 1)

where Yn = ǫn+1 + ρn+1ǫn+2 + ρn+1ρn+2ǫn+3 + · · · . But Yn and X∞ have the same distribution, and Yn is
independent of Fn and a0−ǫ1−ρ1ǫ2−···−ρ1ρ2···ρn−1ǫn

is Fn measurable. So

ρ1ρ2···ρn

E(I(X∞ = a0)|Fn) ≤ p < 1 for all n ≥ 1.

From (3), it follows that I(X∞ = a0) ≤ p < 1 with probability 1. Since I(X∞ = a0) is a {0, 1} valued random
variable, I(X∞ = a0) = 0 with probability 1 and hence P(X∞ = a0) = 0. Hence, X∞ has a non atomic
distribution.

A natural question is under what additional conditions on the distribution of (ρ1, ǫ1), the sequence {Xn} is

regenerative. When a Markov sequence is Harris recurrent and σ-algebra is countably generated then it can be

established that the sequence exhibits regenerative property (see Athreya and Ney [1978]). We now explore the

Harris recurrence property of {Xn}.

3 Harris recurrence of Xn

Deﬁnition 1. A Markov chain {Xn}n≥0 is called Harris or φ-recurrent if there exists a σ-ﬁnite measure φ on

the state space (S, S) such that

φ(A) > 0 =⇒ P(τA < ∞|X0 = x) = 1 ∀x ∈ S,

(4)

where τA = min{n : n ≥ 1, Xn ∈ A}.

Note that any irreducible and recurrent Markov chain with a countable state space is Harris recurrent as one

can take φ to be the δ measure at some i0 ∈ S. A deﬁnition related to Deﬁnition 1 is given by Athreya and Ney

[1978].

3

Deﬁnition 2. A Markov chain {Xn} is called (A, ǫ, φ, n0) recurrent if there exists a set A ∈ S, a probability

measure φ on S, a real number ǫ > 0, and an integer n0 > 0 such that

P(τA < ∞|X0 = x) ≡ Px(τA < ∞) = Px(Xn ∈ A for some n ≥ 1) = 1 ∀ x ∈ S and

P(Xn0 ∈ E|X0 = x) ≡ Px(Xn0 ∈ E) = P(n0)(x, E) ≥ ǫφ(E) ∀ x ∈ A and ∀ E ⊂ S.

(5)

(6)

It can be shown by using the C-set lemma of Doob (see Orey [1971]) that when S is countably generated, then

Deﬁnition 1 implies Deﬁnition 2. That Deﬁnition 2 implies Deﬁnition 1 is not difﬁcult to prove.

The following theorem provides a sufﬁcient condition for {Xn} in (1) to be a Harris recurrent Markov chain.

Theorem 3. Let −∞ ≤ E(log |ρ1|) < 0, E(log |ǫ1|)+ < ∞, P(ρ1 = 0) = 0 and −∞ < c < d < ∞ be such
that P(c ≤ X∞ ≤ d) > 0. Then, for all x ∈ R,

P(Xn ∈ [c, d] for some n ≥ 1|X0 = x) = 1.

In addition, let there exists a ﬁnite measure φ on R such that φ([c, d]) > 0 and 0 < α < 1 such that

inf
c≤x≤d

P(ρ1x + ǫ1 ∈ ·) ≥ αφ(·).

(7)

(8)

Then, the Markov chain {Xn}n≥0 as described in (1) is Harris recurrent and hence regenerative.

Note that since Xn converges in distribution to X∞ which is a proper real valued random variable, {Xn}n≥0

is positive recurrent as well. Thus under the hypothesis of Theorem 3, {Xn}n≥0 is regenerative and positive

recurrent. The proof of Theorem 3 is based on the following results.

Lemma 1. Let −∞ ≤ E(log |ρ1|) < 0, E(log |ǫ1|)+ < ∞, P(ρ1 = 0) = 0 and −∞ < c < d < ∞ be such that
P(c ≤ X∞ ≤ d) > 0. Then, there exist θ > 0 and for all x ∈ R, an integer nx ≥ 1 such that

P(Xn ∈ [c, d]|X0 = x) ≥ θ

for all n ≥ nx.

(9)

Proof. Iterating (1) yields,

Xn = ρnρn−1 · · · ρ1X0 + ρnρn−1 · · · ρ2ǫ1 + · · · + ρnǫn−1 + ǫn ≡ ZnX0 + Yn, say.

So, if X0 = x w. p. 1, then

Px(Xn ∈ [c, d]) = P(Yn + Znx ∈ [c, d])

≥ P(Yn ∈ [c + η, d − η], |Znx| < η)

≥ P(Yn ∈ [c + η, d − η]) − P(|Znx| ≥ η),

where η > 0 such that c + η < d − η. Now, deﬁne

Y ′
n ≡ ǫ1 + ρ1ǫ2 + ρ1ρ2ǫ3 + · · · + ρ1 . . . ρn−1ǫn.

(10)

4

Note that the distribution of Yn and Y ′

n are same and from Theorem 1, Y ′

n → X∞ with probability 1. Thus, we

have

Px(Xn ∈ [c, d]) ≥ P(Y ′

n ∈ [c + η, d − η]) − P(|Znx| ≥ η)
n ∈ [c + η, d − η], |Y ′

n − X∞| ≤ η

′

≥ P(Y ′

) − P(|Znx| ≥ η)

≥ P(X∞ ∈ [c + η + η

′

, d − η − η

′

]) − P(|Y ′

n − X∞| ≥ η

′

) − P(|Znx| ≥ η),

where η

′

> 0 such that c + η + η
Now choose n1 large such that P(|Y ′

′

< d − η − η

′

.

n − X∞| ≥ η

′

) ≤ δ

2 and n2 large such that P(|Zn2x| ≥ η) ≤ δ

2 . Note

that choice of n2 depends on x. Let nx = max(n1, n2). Then, for all n ≥ nx,

Px(Xnx ∈ [c, d]) ≥ P(X∞ ∈ [c + η + η

′

, d − η − η

′

]) − δ.

Since X∞ has a continuous distribution by Theorem 2 and P(c ≤ X∞ ≤ d) > 0, ﬁrst choose η and η
δ small enough such that

′

and then

θ ≡ P(X∞ ∈ [c + η + η

′

, d − η − η

′

]) − δ > 0.

Thus (9) is established.

Lemma 2. Let {Xn} be a time homogeneous Markov chain with state space (S, S) and transition function

P (·, ·). Let there exists A ∈ S and 0 < θ ≤ 1 such that for all x ∈ S, there exists an integer nx ≥ 1 such that

Then for all x ∈ S,

where τA = min{n : n ≥ 1, Xn ∈ A)}.

P(Xnx ∈ A|X0 = x) ≥ θ.

P(τA < ∞|X0 = x) = 1

(11)

(12)

Proof. Fix x ∈ S. Let B0 ≡ {Xnx /∈ A} and τ0 = nx. Then B0 ≡ {Xτ0 /∈ A}. Let us deﬁne

B1 ≡ {Xτ0 /∈ A, Xτ0+nXτ0
τ1 = τ0 + nXτ0

/∈ A}

B2 ≡ {Xτ0 /∈ A, Xτ1 /∈ A, Xτ1+nXτ1
τ2 = τ1 + nXτ1 ,

/∈ A}

and so on. Note B1 = {Xτ0 /∈ A, Xτ1 /∈ A}, B2 = {Xτ0 /∈ A, Xτ1 /∈ A, Xτ2 /∈ A} and for any integer k ≥ 3,

Bk ≡ {Xτ0 /∈ A, Xτ1 /∈ A, . . . , Xτk /∈ A},

5

. By hypothesis (11), P((B0) ≤ (1 − θ). By the strong Markov property of {Xn},
P(Bk) < ∞ since θ > 0.
IBk (·) < ∞ with probability 1. This implies that with probability 1, IBk = 0 for all large k > 1. That

with τk = τk−1 + nXτk−1
P(B1) ≤ (1 − θ)2 and P(Bk) ≤ (1 − θ)k+1 for all integer k ≥ 3. This implies
So
is, for all x ∈ S, Px(Xτk ∈ A for some k < ∞) = 1. Hence, for all x ∈ S, Px(τA < ∞) = 1.

∞
k=0

∞
k=0

P

P

Proof of Theorem 3. In view of Deﬁnition 2, it is enough to prove (7) to show {Xn} is Harris recurrent. The

proof of (7) follows from Lemma 1 and 2. Now from Lemma 2.2.5 of Athreya and Atuncar [1998], it follows

that Xn is regenerative.

Theorem 3 provides sufﬁcient conditions on (ρ1, ǫ1) so that the sequence Xn becomes Harris recurrent and

hence regenerative. These sufﬁcient conditions are fairly general and hold for large class of distribution of

(ρ1, ǫ1). Here are some examples where (7) and (8) hold.
Example 1: ǫ1 is a standard normal, N (0, 1) random variable, ρ1 has bounded support with E log |ρ1| < 0 and
ǫ1, ρ1 are independent.
Example 2: ǫ1 is a Uniform (−1, 1) random variable, ρ1 has bounded support with E log |ρ1| < 0 and ǫ1, ρ1 are
independent.

In both the cases hypothesis of Theorem 1 hold and X∞ is of the form (ǫ1 + ρ1 ˜X∞) where ˜X∞ has the same
distribution as X∞ and independent of X∞. One can show in both above cases that for some c < 0 < d, |c| and

d sufﬁciently small, conditions (7) and (8) hold.

In Theorem 3, growth sequence {ρn} has no mass at zero and the regeneration property of Xn is established
by showing Harris recurrence of the sequence. When P(ρ1 = 0) > 0, then the regenerative property of Xn can
be shown more easily.

Theorem 4. Let {Xn}n≥0 be a RCAR(1) sequence as in (1). If P(ρ1 = 0) ≡ α > 0, then {Xn}n≥0 is a positive
recurrent regenerative sequence.

Proof. Let τ0 = 0 and τj+1 = min{n : n ≥ τj + 1, ρn = 0} for j ≥ 0. We need to show that

P(τj+1 − τj = kj, Xτj +l ∈ Al,j, 0 ≤ l < kj, 1 ≤ j ≤ r) =

r

Yj=1

P(τ2 − τ1 = kj, Xτ1+l ∈ Al,j, 0 ≤ l < kj) (13)

for all k1, k2, . . . , kr ∈ N and Al,j ∈ B(R), 0 ≤ l < kj, j = 1, 2, . . . , r, r = 1, 2, . . ..

Since {(ρn, ǫn)}n≥1 are i.i.d. and P(ρ1 = 0) = α > 0, it follows that {τj+1 − τj, j ≥ 0} are i.i.d. with jump

distribution

P(τj+1 − τj = k) = (1 − α)k−1α,

for k = 1, 2, . . . ,

that is, geometric with “success” parameter α. Next, since {(ρn, ǫn)}n≥1 are i.i.d. (13) follows. Further since
E(τ2 − τ1) < ∞, the sequence {Xn} is positive recurrent regenerative.

6

Remark 1. When P(ρ1 = 0) = α > 0 and the joint distribution of (ρ1, ǫ1) is discrete, then the limiting
distribution π of X∞ is a discrete probability distribution, that is, there exists a countable set A0 in R2 such that
π(A0) = 1. This is in contrast to Theorem 2 which provides a sufﬁcient condition for X∞ to have a non atomic

distribution.

4 Estimation of transition function and characteristic functions of ρ1 and ǫ1

The transition function P(x, A) of the Markov chain {Xn}n≥0, deﬁned by (1), is precisely equal to P(ρ1x + ǫ1 ∈
A). The following result determines the joint distribution of (ρ1, ǫ1) in terms of the transition function, P(·, ·).

Theorem 5. If the distribution of ρ1x + ǫ1 is known for all x of the form t1
t2
R2 then the distribution of (ρ1, ǫ1) is determined.

where t2 6= 0 and (t1, t2) is dense in

Proof. For any (t1, t2) ∈ R2, the characteristic function of (ρ1, ǫ1) is

ψ(ρ1,ǫ1)(t1, t2) = E(ei(t1ρ1+t2ǫ1)) = E(e

it2(ρ1

t1
t2

+ǫ1)) = φ t1

t2

(t2)

where φx(t) = E(eit(ρ1x+ǫ1)) for all x, t ∈ R. If φx(·) is known for all x of the form t1
where (t1, t2) is dense
t2
in R2, then ψ(ρ1,ǫ1)(t1, t2) is determined for all such (t1, t2) and hence by continuty for all (t1, t2) ∈ R2. Hence
the distribution of (ρ1, ǫ1) is determined completely.

Theorem 5 implies that if the transition function P(x, A) of {Xn}n≥0 can be determined from observing the
sequence sequence {Xn}, then the distribution of (ρ1, ǫ1) can also be determined. We now estimate the transition
probability function P(x, (−∞, y]), for x, y ∈ R2 from the data {Xi}n
i=0. In the following theorems, we show
that the estimator Fn,h(x, y), given in (14) below, is a strongly consistent estimator for P(X1 ≤ y|X0 = x).

Theorem 6. Let {Xn} satisﬁes the hypothesis of Theorem 3. For n ≥ 1, h > 0, x, y ∈ R, let

1

nh Pn−1

i=0
1
nh Pn

I(x≤Xi≤x+h,Xi+1≤y)
I(x≤Xi≤x+h)

i=0

Fn,h(x, y) = 


0

if I(x ≤ Xi ≤ x + h) 6= 0 for some 0 ≤ i ≤ n − 1

otherwise,

where I(A) denotes the indicator function of the event A.



(a) Then with probability 1, for each x, y ∈ R

lim
n→∞

Fn,h(x, y) ≡ ψ(x, y, h) =

x+h
x G(u, y)P(X∞ ∈ du)
P(X∞ ∈ (x, x + h])

,

R

where G(u, y) = P(x, (−∞, y]) = P(X1 ≤ y|X0 = x).

7

(14)

(15)

(b) In addition, let G(x, y) and the random variable X∞ satisfy

x+h
x G(u, y)P(X∞ ∈ du)
P(x < X∞ ≤ x + h)

lim
h→0 R

= G(x, y),

for x, y ∈ R2.

Then for x, y ∈ R

lim
h→0

lim
n→0

Fn,h(x, y) = P(X1 ≤ y|X0 = x), with probability 1.

(16)

(17)

erative and positive recurrent Markov chain. The numerator in (14) converges to

Proof. Since {Xi}i≥0 is regenerative and positive recurrent, the vector sequence {(Xi, Xi+1)}i≥0 is also regen-
x+h
x G(u, y)P(X∞ ∈ du)
with probability 1 by using Theorem 9.2.10 of Athreya and Lahiri [2006]. Similarly denominator converges to
P(X∞ ∈ (x, x + h]) with probability 1. This completes the proof of part (a).

R

The proof of part (b) follows from (15) and (16).

Remark 2. A sufﬁcient condition for (16) to hold is that the distribution of X∞ is absolutely continuous with

strictly positive and continuous density function and the function G(x, y) is continuous in x for ﬁxed y.

The following result is similar to that of Theorem 6.

Theorem 7. Fix x, t, h ∈ R. Let

1

nh Pn−1

j=0 eitXj+1 I(x<Xj≤x+h)
nh Pn−1

I(x<Xj≤x+h)

j=0

1

if I(x ≤ Xi ≤ x + h) 6= 0 for some 0 ≤ i ≤ n − 1,

otherwise.

lim
h→0

lim
n→∞

φn,h,x(t) = E(eit(ρ1x+ǫ1)) with probability 1,

x+h
x
R
Proof. Proof of this theorem is similar to the proof of Theorem 6 and hence omitted.

R

lim
h→0

1
h

x+h
x

E(eit(ρ1x+ǫ1))P(X∞ ∈ du)
P(X∞ ∈ du)
1
h

= E(eit(ρ1x+ǫ1)).

(18)

Remark 3. A sufﬁcient condition for (18) to hold is that the distribution of X∞ is absolutely continuous with
strictly positive and continuous density function on (−∞, ∞) and the function E(eit(ρ1x+ǫ1)) is continuous in x

for ﬁxed t.

Let {ρ1} and {ǫ1} are independent random variables.Then

φx(t) ≡ Ex(eitX1 ) = E(eit(ρ1x+ǫ1)) = ψρ(tx)ψǫ(t),

where ψρ(t) = E(eitρ) and ψǫ(t) = E(eitǫ). Also, note that

ψǫ(t) = φ0(t) and ψρ(tx) =

φx(t)
φ0(t)

, when ψǫ(t) 6= 0.

This yields the following corollary of Theorem 7.

8

φn,h,x(t) = 

0


Then

provided

Corollary 1. Let ρ1 and ǫ1 be independent and conditions of Theorem 7 holds. Then

(a) limh→0 limn→∞ φn,h,0(t) = ψǫ(t) for all t ∈ R with probability 1.

(b) Let ψǫ(t) 6= 0 for all t ∈ R, then for all x 6= 0

lim
h→0

lim
n→∞

ψn,h,x(t/x)
φn,h,0(t/x)

= ψρ(t) for all t ∈ R with probability 1.

Acknowledgement. K. B. Athreya would like to thank the Department of Mathematics, IIT Bombay and in

particular, Prof. Sudhir Ghorpade for offering him visiting professorship. The work of Radhendushka Srivastava

is partially supported by INSPIRE research grant of DST, Govt. of India and a seed grant from IIT Bombay.

5 Appendix

Proof of Theorem 1: Choose ǫ > 0 such that E(log |ρ1|) + ǫ < 0. Now, by the strong law of large number,

E(log |ρ1|) < 0 ⇒

1
n

n

Xi=1

log |ρi| ≤ E(log |ρ1|) + ǫ,

for sufﬁciently large n, with probability 1. Hence

|ρ1ρ2 . . . ρn| ≤ e−nλ,

(19)

where 0 < λ ≡ −(E(log |ρ1| + ǫ) < ∞, for all large n, with probability 1.

Also E(log |ǫ1|)+ < ∞ implies that for any µ > 0,

∞
n=1

P(log |ǫ1| > nµ) < ∞ and hence

P(log |ǫn| >

n

nµ) < ∞. By Borel Cantelli lemma, |ǫn| ≤ enµ for all n large enough, with probability 1.

P

P

Now choose 0 < µ < λ. Then, for sufﬁciently large n, with probability 1,

|ǫn+1ρ1ρ2 . . . ρn| ≤ e−nλe(n+1)µ.

Therefore

n |ǫn+1|ρ1ρ2 . . . ρn| < ∞ with probability 1. Hence ˜X∞ = ǫ1+ρ1ǫ2+ρ1ρ2ǫ3+. . .+ρ1 . . . ρnǫn+1+

. . . is well deﬁned.

P

Observe that

Xn = ρn(ρn−1Xn−2 + ǫn−1) + ǫn

= ρnρn−1 · · · ρ1X0 + ρnρn−1 · · · ρ2ǫ1 + · · · + ρnǫn−1 + ǫn

and which has the same distribution as

ǫ1 + ρ1ǫ2 + · · · + ρ1ρ2 · · · ρn−1ǫn + ρ1ρ2 · · · ρnX0.

(20)

9

Now by using (19) and above, we have |ρ1ρ2 · · · ρnX0| converges to zero with probability 1. Thus, from (20), as

n → ∞, we have

Xn

d
→ ˜X∞,

where d

→ stands for convergence in distribution.

References

J. Andˇel. Autoregressive series with random parameters. Math. Operationsforsch. Statist., 7(5):735–741, 1976.

K. B. Athreya and G. S. Atuncar. Kernel estimation for real-valued Markov chains. Sankhy¯a Ser. A, 60(1):1–17,

1998.

K. B. Athreya and S. N. Lahiri. Probaility theory (TRIM Series 41). Hindustan Book agency, New Delhi, 2006.

K. B. Athreya and P. Ney. A new approach to the limit theory of recurrent Markov chains. Trans. Amer. Math.

Soc., 245:493–501, 1978.

A. Aue, L. Horv´ath, and J. Steinebach. Estimation in random coefﬁcient autoregressive models. J. Time Ser.

Anal., 27(1):61–76, 2006.

A. Brandt. The stochastic equation Yn+1 = AnYn + Bn with stationary coefﬁcients. Adv. in Appl. Probab., 18

(1):211–220, 1986.

D. F. Nicholls and B. G. Quinn. The estimation of random coefﬁcient autoregressive models. I. J. Time Ser.

Anal., 1(1):37–46, 1980.

S. Orey. Lecture notes on limit theorems for Markov chain transition probabilities. Van Nostrand Reinhold Co.,

London-New York-Toronto, Ont., 1971. Van Nostrand Reinhold Mathematical Studies, No. 34.

P. M. Robinson. Statistical inference for a random coefﬁcient autoregressive model. Scand. J. Statist., 5(3):

163–168, 1978.

10

