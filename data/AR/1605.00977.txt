6
1
0
2

y
a
M
3

]

C
O
.
h
t
a
m

[

1
v
7
7
9
0
0
.
5
0
6
1
:
v
i
X
r
a

Blackwell-Nash Equilibrium for Discrete and
Continuous Time Stochastic Games

Vikas Vikram Singh

Laboratoire de Recherche en Informatique, Universit´e Paris Sud, Orsay
91405, France
Email: vikas.singh@lri.fr, vikasstar@gmail.com

N. Hemachandra

Industrial Engineering and Operations Research, Indian Institute of
Technology Bombay, Mumbai 400076, India
Email: nh@iitb.ac.in

September 5, 2018

Abstract

We consider both discrete and continuous time ﬁnite state-action stochastic
games. In discrete time stochastic games, it is known that a stationary Blackwell-
Nash equilibrium (BNE) exists for a single controller additive reward (SC-AR)
stochastic game which is a special case of a general stochastic game. We show
that, in general, the additive reward condition is needed for the existence of a BNE.
We give an example of a single controller stochastic game which does not satisfy
additive reward condition. We show that this example does not have a stationary
BNE. For a general discrete time discounted stochastic game we give two different
sets of conditions and show that a stationary Nash equilibrium that satisﬁes any
set of conditions is a BNE. One of these sets of conditions weakens a set of con-
ditions available in the literature. For continuous time stochastic games, we give
an example that does not have a stationary BNE. In fact, this example is a single
controller continuous time stochastic game. Then, we introduce a continuous time
SC-AR stochastic game. We show that there always exists a stationary determin-
istic BNE for continuous time SC-AR stochastic game. For a general continuous
time discounted stochastic game we give two different sets of conditions and show
that a Nash equilibrium that satisﬁes any set of conditions is a BNE.

Index terms— Stochastic game, Markov decision process, Blackwell-Nash equi-

librium.

1

 
 
 
 
 
 
1 Introduction

Blackwell optimality is a very desirable property of discrete time discounted Markov
decision processes. It ensures the existence of an optimal policy for every discount
factor close enough to one. For ﬁnite state-action discrete time Markov decision pro-
cess (DTMDP), Blackwell [2] showed that a stationary deterministic Blackwell optimal
policy always exists (see also [11]). It is natural to extend the concept of Blackwell op-
timality in discounted MDPs to discounted stochastic games. Singh et al. [13] call
a strategy pair Blackwell-Nash equilibrium (BNE) if it is a Nash equilibrium for ev-
ery discount factor close enough to one. In discrete time discounted stochastic games
there always exists a stationary Nash equilibrium for a ﬁxed discount factor [5], [14].
However, the existence of a BNE is not always guaranteed. This can be seen from
“Big Match” stochastic game example [6]. There are some special classes of discrete
time stochastic games admitting a stationary BNE. Gimbert and Zielonka [7] showed
that a zero sum perfect information stochastic game with state dependent discount
factors always possesses a stationary deterministic Blackwell optimal strategy pair.
Avrachenkov et al. [1] proposed two algorithms to compute a Blackwell optimal strat-
egy pair for two player zero sum perfect information stochastic games. Singh et al. [13]
proposed a single controller additive reward (SC-AR) stochastic game and showed the
existence of a stationary deterministic BNE for a general sum SC-AR stochastic game.
For general sum discounted stochastic games they proposed a set of three conditions
which together are sufﬁcient for any of its stationary Nash equilibrium to be a BNE.
There are not much results known for continuous time stochastic games. Recently,
Neyman [10] showed that there always exists a stationary Nash equilibrium for a ﬁnite
state-action continuous time discounted stochastic game. He showed the existence of
a stationary Blackwell optimal strategy pair for a two player continuous time zero sum
perfect information stochastic game.

In this paper, we consider both discrete and continuous time 2-player ﬁnite state-
action stochastic games with discounted payoff criterion. For discrete time stochastic
game, we strengthen the BNE results given in [13]. We ﬁrst show that the additive
reward condition in SC-AR stochastic game considered in [13] is needed for the exis-
tence of BNE. We give an example of a single controller stochastic game which does
not satisfy the additive reward condition. We show that this example does not ad-
mit a stationary BNE. For general sum discounted stochastic games we weaken the
conditions, given in [13], which together are sufﬁcient for any of its stationary Nash
equilibrium to be a BNE. In particular, in [13] the Markov chain induced by the Nash
equilibrium is required to have only one absorbing state with all other states being tran-
sient; this seems to be a very strong condition. We now propose a weaker condition
where the Markov chain induced by the Nash equilibrium satisﬁes state independent
transition (SIT) property. The condition on one period rewards is suitably modiﬁed.
We also propose another different set of conditions which together are sufﬁcient for a
stationary Nash equilibrium to be a BNE. We now have two disjoint sets of conditions.
Hence, it is clear that none of these sets of conditions are necessary. Along similar
lines of discrete time stochastic games we give the BNE results for continuous time
stochastic games. We ﬁrst give an example that shows that a stationary BNE need not
always exist for a general continuous time stochastic game. In fact, the example be-

2

long to the class of single controller games. Hence, in general even a single controller
continuous time stochastic game need not have a BNE. Then, similar to discrete time
stochastic game we introduce continuous time SC-AR stochastic game. We show that
there always exists a stationary deterministic BNE for continuous time SC-AR stochas-
tic game. For a general continuous time stochastic game we give two disjoint sets of
conditions such that a stationary Nash equilibrium satisfying any set of conditions is a
BNE.

We now describe the structure of the rest of our paper. Section 2 contains the BNE
results for 2-player discrete time stochastic games. Section 3 contains the BNE results
for 2-player continuous time stochastic games. We conclude the paper in Section 4.

2 Discrete time stochastic games

We ﬁrst consider discrete time stochastic games. We recall the details of the model
like dynamics and notations from [13]. A 2-player stochastic game is described by the
tuple (S, A1, A2, r1, r2, p), where

(i) S is a ﬁnite state space. Generic element of S is denoted by s.

(ii) Ai is a ﬁnite action set of player i, i = 1, 2, let Ai(s) denotes the set of actions

available to player i at state s, where Ai =

s∈S Ai(s).

(iii) For player i, i = 1, 2, ri

K =

(s, a1, a2)| s ∈ S, a1 ∈ A1(s), a2 ∈ A2(s)

: K → R is immediate reward function, where
.

S

(iv) For a given set M , let ℘(M ) be the set of all probability measures on M . The

(cid:8)

(cid:9)

transition law of the game is the function p : K → ℘(S).

(cid:1)

A stochastic game proceeds through stages t = 0, 1, 2, · · · . At stage t the game is in
state st ∈ S, player 1 chooses an action a1
t ∈ A1(st) and player 2 chooses an action
a2
t ∈ A2(st), then player 1 (resp. player 2) receives immediate reward r1(st, a1
t , a2
t )
t , a2
resp. r2(st, a1
. At time t + 1 game moves to state st+1 with probability
t )
t , a2
p(st+1|st, a1
t ). The same thing repeats at st+1 and game continues for the inﬁnite
(cid:0)
time horizon. Both the players are interested in maximizing their expected discounted
reward collected during the play over inﬁnite time horizon.
1, a2

0, a2
1, · · · , st−1, a1
Deﬁne a history at time t as ht = (s0, a1
t−1, st),
where sm ∈ S for m = 0, 1, · · · , t, and ai
m ∈ Ai(sm) for m = 0, 1, · · · , t − 1, for
all i = 1, 2. Let Ht denote the set of all possible histories of length t. At time t a
decision rule ft of player 1 (resp., gt of player 2) assigns to each ht ∈ Ht with ﬁnal
state st a probability measure ft(ht) ∈ ℘(A1(st))
. A
sequence of such decision rules is called history dependent strategy of the game. A
history dependent strategy is called Markovian strategy if decision rule at time t de-
pends only on the state at time t. A stationary strategy is a Markovian strategy which
does not depend on the time, i.e., for a stationary strategy of player 1 (resp., player
2) there exists an f (resp., g) such that ft = f (resp., gt = g) for all t. We de-
note, with some abuse of notations, f and g as stationary strategies of player 1 and
player 2 respectively. Let FS and GS denote the sets of all stationary strategies of

resp., gt(ht) ∈ ℘(A2(st))

0, s1, a1

t−1, a2

(cid:0)

(cid:1)

3

(cid:0)

(f (1))T , (f (2))T , · · · , (f (|S|))T

player 1 and player 2 respectively. A stationary strategy f ∈ FS is identiﬁed with
T
f =
;
|M | denotes the cardinality of a given set M and T denotes the transposition. Simi-
(cid:1)
larly, a stationary strategy g ∈ GS of player 2 is deﬁned. It is well known that for a
discrete time stochastic game with discounted payoff criterion there always exists a sta-
tionary Nash equilibrium (see [5], [14]). Therefore, we restrict ourselves to stationary
strategies.

, where for each s ∈ S, f (s) ∈ ℘

A1(s)

(cid:0)

(cid:1)

For an initial state s ∈ S and a strategy pair (f, g) the expected discounted reward

of player i, i = 1, 2, is deﬁned as

vi
β(s, f, g) =

∞

t=0
X

βt[P t(f, g)]sri(f, g),

(1)

where β ∈ [0, 1) is a ﬁxed discount factor, and P 0(f, g) is an identity matrix, and
P t(f, g) is a t-step stochastic matrix induced by a strategy pair (f, g), and ri(f, g) is a
|S| × 1 vector of the expected immediate rewards of player i whose sth component is
a2∈A2(s) f (s, a1)ri(s, a1, a2)g(s, a2). For a given matrix
ri(s, f, g) =
B, [B]k denotes its kth row. The expected discounted reward deﬁned by (1) can be
written as

a1∈A1(s)

P

P

vi
β(s, f, g) = [I − βP (f, g)]−1

s ri(f, g), ∀ i = 1, 2,

where I denotes an identity matrix. A strategy pair (f ∗, g∗) ∈ FS × GS is said to
be a Nash equilibrium of a discounted stochastic game if for all s ∈ S the following
inequalities hold simultaneously:

β(s, f ∗, g∗) ≥ v1
v1
β(s, f ∗, g∗) ≥ v2
v2

β(s, f, g∗), ∀ f ∈ FS,
β(s, f ∗, g), ∀ g ∈ GS, .

It is possible to give the Nash equilibrium deﬁnition, in our setting, by restricting to
f ∈ FS and g ∈ GS because when one player’s strategy is ﬁxed to a stationary strategy,
then other player’s problem is a MDP where an optimal strategy exists in the space of
stationary strategies. We now introduce the notations which we use throughout this
paper. For i = 1, 2 and s, s′ ∈ S,

• Ri(s) =

ri(s, a1, a2)

i at state s.
(cid:2)

(cid:3)

• vi =

vi(1), vi(2), · · · , vi(|S|)

T

.

|A1
(s)|,|A2
a1=1,a2=1

(s)|

, where Ri(s) is the reward matrix of player

• P (s′|s) =

(cid:0)

p(s′|s, a1, a2)

(s)|,|A2
|A1
(cid:1)
a1=1,a2=1

(s)|

.

(cid:3)
(cid:2)
• 1n = (1, 1, · · · , 1)T ∈ Rn.

2.1 Blackwell-Nash equilibrium in discrete time stochastic games

A strategy pair is said to be a BNE if it is Nash equilibrium for all the discount factors
close enough to one. We present some new results that strengthens the results given in

4

[13]. We show that a stationary BNE may not always exist if we relax additive reward
assumption in SC-AR stochastic games considered in [13]. We give an example of sin-
gle controller stochastic game that fails to satisfy the additive reward assumption. We
show that there does not exist a stationary BNE in this game. For general stochastic
games, we propose two disjoint sets of conditions that are sufﬁcient for a Nash equi-
librium to be a BNE. One set of conditions are more general than the set of conditions
given in [13]. We recall the deﬁnition of BNE as given in [13].

Deﬁnition 1 ([13]). A strategy pair (f ∗, g∗) is said to be a BNE of a discrete time
stochastic game if there exists a β0 ∈ [0, 1) such that (f ∗, g∗) is a β-discounted Nash
equilibrium for every β ∈ [β0, 1).

2.1.1 Single controller stochastic games

In these games the transition probabilities are controlled by only one player. We assume
that player 2 controls the transition probabilities, i.e., p(s′|s, a1, a2) = p(s′|s, a2) for
all s ∈ S, a1 ∈ A1(s), a2 ∈ A2(s). Singh et al. [13] further assume that the immediate
rewards of player 1 satisfy additive condition (2) given below.

r1(s, a1, a2) = r1

1(s, a1) + r1

2(s, a2), ∀ s ∈ S, a1 ∈ A1(s), a2 ∈ A2(s).

(2)

They call these games single controller additive reward (SC-AR) stochastic games.
Singh et al. [13] showed that there always exists a stationary deterministic Blackwell-
Nash equilibrium for a SC-AR stochastic game. We give an example which is a single
controller game but does not satisfy the additive reward assumption. We show that this
game does not have any stationary Blackwell-Nash equilibrium. From this example it
is clear that the stationary Blackwell-Nash equilibrium may not always exist in single
controller stochastic games.

Example 2. We consider a 2 states stochastic game where both the players have
two actions at state 1 and only one action at state 2, i.e., S = {1, 2}, A1(1) =
A2(1) = {1, 2}, A1(2) = A2(2) = {1}. The immediate rewards of both the play-
ers and the transition probabilities for different combinations of states and actions are
summarized in the Table 1.

Table 1: Immediate rewards and Transition Probabilities

(a) s = 1

(4, 9)

(1, 0)

PPPPPPPP
PPPPPPPP

(1, 0)

(5, 4)

(0, 1)

(6, 3)

PPPPPPPP
PPPPPPPP

(4, 5)

(0, 1)

(b) s = 2

PPPPPPPP

(6, 7)

(1, 0)

.

The rows and columns of the tables represent actions of player 1 and player 2 respec-
tively. The upper half of each box of these tables represents transition probabilities
and lower half represents immediate rewards. For example, if at state 1 both players
choose their ﬁrst action, player 1 gets 4 and player 2 gets 9, and with probability 1

5

game remains in state 1. From the above tables it is clear that the game is controlled
only by player 2. The additive reward condition (2) for Example 2 can be written as,

r1
1(1, 1) + r1
r1
1(1, 1) + r1
r1
1(1, 2) + r1
r1
1(1, 2) + r1

2(1, 1) = 4
2(1, 2) = 6
2(1, 1) = 5
2(1, 2) = 4.

(3)

(4)

(5)

(6)

It follows from the subtraction of (3) with (4) and the subtraction of (5) with (6) that the
above system of equations are inconsistent. That is, the immediate rewards of player 1
are not additive.

Theorem 3. The discrete time single controller stochastic game given in Example 2
does not have a stationary Blackwell-Nash equilibrium.

Proof. We represent any stationary strategy pair (f, g) = ((p, 1 − p), (q, 1 − q)) for
some 0 ≤ p, q ≤ 1 because at state 2 both the players have only one action. For a
ﬁxed stationary strategy of one player, the best response strategy of other player can be
obtained by solving a DTMDP. It is well known that in DTMDPs there always exists a
stationary deterministic optimal strategy. For a ﬁxed stationary strategy g = (q, 1 − q)
of player 2, f ∗ is a best response of player 1 if and only if for each s ∈ S

v1
β(s, f ∗, g) = max
f ∈FS

v1
β(s, f, g).

As the game is controlled only by player 2, so f ∗ will be best response of player 1 if
and only if for each s ∈ S

r1(s, f ∗, g) = max
f ∈FS

r1(s, f, g) = max

a1∈A1(s)

[R1(s)g(s)]a1 .

(7)

We need to determine f ∗ only at state s = 1 because at s = 2 there is only one action.
We have

R1(1)g(1) = [6 − 2q, 4 + q]T .

Let f1 = (1, 0) and f2 = (0, 1) be two stationary deterministic strategies of player 1.
From (7), we have

f1
f2

(p, 1 − p) : 0 ≤ p ≤ 1

f ∗ = 


if q < 2
3
if q > 2
3
if q = 2
3 .

(8)

(cid:9)



(cid:8)

Equation (8) gives the best response of player 1, when player 2 ﬁxes his strategy as
g = (q, 1 − q), for all β ∈ [0, 1).

For a ﬁxed stationary strategy f = (p, 1 − p) of player 1, player 2 faces a DTMDP
with immediate rewards ˜r(1, 1) = r2(1, f, 1) = 4 + 5p, ˜r(1, 2) = r2(1, f, 2) = 5 − 2p,
˜r(2, 1) = r2(2, f, 1) = 7 and the same transition probabilities as given in Example 2.

6

Let g1 = (1, 0) and g2 = (0, 1) be two stationary deterministic strategies of player 2.
By using the data given in Example 2 we have

v2
β(g1) = [I − βP (g1)]−1˜r(g1) =

4 + 5p
1 − β

,

(cid:20)

(4 + 5p)β
1 − β

β(g2) = [I − βP (g2)]−1˜r(g2) =
v2

By using (9) and (10) we have

5 − 2p + 7β
1 − β2

,

(cid:20)

T

+ 7

.

(cid:21)
(5 − 2p)β + 7
1 − β2

T

.

(cid:21)

(9)

(10)

v2
β(g1) − v2

β(g2) =

(cid:20)

p(7 + 5β) − (3β + 1)
1 − β2

,

β(p(7 + 5β) − (3β + 1))
1 − β2

T

(cid:21)

.

(11)

From (11) the best response g∗ of player 2 against a ﬁxed strategy f = (p, 1 − p) of
player 1 for a given discount factor β is given by (12)

g1
g2

(q, 1 − q) : 0 ≤ q ≤ 1

g∗ = 


if p > 3β+1
7+5β
if p < 3β+1
7+5β
if p = 3β+1
7+5β .

(12)



(cid:8)

(cid:9)

,

2

3

(cid:0)

(cid:17)

7+5β

3β+1

3 , 1

β , g∗

is such that f ∗

7+5β , 6+2β

β). Now, we consider two cases.

From (8) and (12) it is easy to see that for a discount factor β, a strategy pair (f ∗

β) =
β and g∗
β are best responses of each other, i.e.,
β , g∗
it is a Nash equilibrium. Next, we show that (f ∗
β) is the unique Nash equilibrium.
(cid:1)(cid:17)
(cid:16)(cid:16)
Let ( ˜f , ˜g) = ((˜p, 1 − ˜p), (˜q, 1 − ˜q)) for some 0 ≤ ˜p, ˜q ≤ 1 be another Nash equilibrium
β , g∗
different from (f ∗
Case I: Let ˜p 6= 3β+1
7+5β . Then we have two sub cases. If ˜p > 3β+1
7+5β then from (12)
˜q = 1. But, from (8) the best response of player 1 corresponding to ˜q = 1 is f2. This
gives the contradiction because f2 does not correspond to ˜p > 3β+1
7+5β then
from (12) ˜q = 0. But, from (8) the best response of player 1 corresponding to ˜q = 0
is f1. This again gives the contradiction because f1 does not correspond to ˜p < 3β+1
7+5β .
Hence ˜p 6= 3β+1
Case II: If ˜q 6= 2
3 then from (8) ˜p = 0. But, from (12) the best response
of player 2 corresponding to ˜p = 0 is g2. This gives the contradiction from the similar
argument given in Case I. If ˜q < 2
3 the from (8) ˜p = 1. But, from (12) the best response
of player 2 corresponding to ˜p = 1 is g1 which again gives the contradiction. Hence
˜q 6= 2

7+5β is not possible.
3 . Again if ˜q > 2

7+5β . If ˜p < 3β+1

3 is not possible.
From Case I and Case II it is clear that (f ∗

β) is an unique Nash equilibrium
for each β. As f ∗
β , g∗
β)
varies with discount factor β. This implies that Example 2 will not have a stationary
Blackwell-Nash equilibrium.

β , g∗
β is an invertible function of β, then the Nash equilibrium (f ∗

7

Average Nash equilibrium

Here we show that lim
β↑1

3 , 1
stochastic game. For a ﬁxed stationary strategy g = (q, 1 − q) of player 2, f ∗ is a best
(cid:0)
(cid:1)(cid:1)
response of player 1 if and only if for each s ∈ S

is a Nash equilibrium for an average

3 , 2

β , g∗

β)=

(f ∗

(cid:0)(cid:0)

(cid:1)

3

1

3

2

,

v1
ea(s, f ∗, g) = max
f ∈FS

v1
ea(s, f, g) = max
f ∈FS

[P ∗(g)]sr1(f, g),

where P ∗(g) is a Cesaro limit matrix of P (g). So, f ∗ will be best response of player 1
if and only if for each s ∈ S

r1(s, f ∗, g) = max
f ∈FS

r1(s, f, g) = max

a1∈A1(s)

[R1(s)g(s)]a1 .

(13)

As similar to Theorem 3 the best response f ∗ is given by (14)

f1
f2

(p, 1 − p) : 0 ≤ p ≤ 1

f ∗ = 


if q < 2
3
if q > 2
3
if q = 2
3 .

(14)



(cid:8)

Equation (14) gives the best response of player 1, when player 2 ﬁxes his strategy as
g = (q, 1−q). For a ﬁxed stationary strategy f = (p, 1−p) of player 1, player 2 faces a
MDP with immediate rewards ˜r(1, 1) = r2(1, f, 1) = 4 + 5p, ˜r(1, 2) = r2(1, f, 2) =
5 − 2p, ˜r(2, 1) = r2(2, f, 1) = 7 and the same transition probabilities as given in
Example 2. By using the data given in Example 2 we have

(cid:9)

ea(g1) = P ∗(g1)˜r(g1) = [4 + 5p, 4 + 5p]T .
v2

β(g2) = P ∗(g2)˜r(g2) = [6 − p, 6 − p]T .
v2

By using (15) and (16) we have

ea(g1) − v2
v2

ea(g2) = [6p − 2, 6p − 2]T .

(15)

(16)

(17)

From (17) the best response g∗ of player 2 against a ﬁxed strategy f = (p, 1 − p) of
player 1 is given by (18)

g1
g2

g∗ = 


(q, 1 − q) : 0 ≤ q ≤ 1

if p > 1
3
if p < 1
3
if p = 1
3 .


(cid:8)
As similar to Theorem 3 we can show that (f ∗
unique Nash equilibrium for an undiscounted game.

(cid:9)
avg, g∗

avg) =

8

(18)

is an

,

2

3 , 1

3

1

3 , 2

3

(cid:0)(cid:0)

(cid:1)

(cid:0)

(cid:1)(cid:1)

Alternative method

.

1

2

3

3 , 1
={a2|g∗
(cid:1)(cid:1)

We can also show the same result

3 , 2
,
3
g∗
β(s)
(cid:0)
(cid:1)
(cid:1)

in a different way.
β (s, a1)
f ∗
β (s)
β(s, a2) > 0}. It is easy to see that the Car(f ∗

Deﬁne Car

= {a1|f ∗

avg, g∗
Let (f ∗
avg) =
>
and
0}
β (s)) and Car(g∗
β(s))
avg for all β and the game is controlled
avg) will be

Car
(cid:0)(cid:0)
are constant for all β and for each s. As g∗
only by player 2, then the Markov chain structure induced by P (f ∗
β , g∗
same as the one induced by P (f ∗

avg, g∗
β). Hence, from Corollary 5.3.9 of [3].

(cid:1)
β = g∗

(cid:0)

(cid:0)

vi
ea(f ∗

avg, g∗

avg) = lim
β↑1

(1 − β)vi

β(f ∗

β , g∗

β), ∀ i = 1, 2.

For all f ∈ FS, we have

v1
ea(f, g∗

avg) = lim
β↑1

(1 − β)v1

β (f, g∗
β)

(1 − β)v1

β (f ∗

β , g∗
β)

≤ lim
β↑1
= v1
ea(f ∗

avg, g∗

avg).

(19)

(20)

First equality above comes from the fact that g∗
comes from the fact that (f ∗
due to (19). For all g ∈ GS, we have

β , g∗

avg for all β and then inequality
β) is a Nash equilibrium for all β. The last equality is

β = g∗

v2
ea(f ∗

avg, g) = lim
β↑1

(1 − β)v2

β(f ∗

β , g)

(1 − β)v2

β(f ∗

β , g∗
β)

≤ lim
β↑1
= v2
ea(f ∗

avg, g∗

avg).

(21)

First equality above comes from Corollary 5.3.9 of [3] and then inequality comes from
the fact that (f ∗
β) is a Nash equilibrium for all β. The last equality is due to (19).
From (20) and (21) (f ∗
avg) is a Nash equilibrium of an undiscounted stochastic
game.

avg, g∗

β , g∗

2.1.2 Sufﬁcient conditions for Blackwell-Nash equilibrium

We give two different sets of conditions, i.e., a stationary Nash equilibrium that
satisﬁes one set of conditions does not satisfy other set of conditions. As similar
in [13] we show that a stationary Nash equilibrium of a discounted stochastic game
satisfying these sets of conditions is a Blackwell-Nash equilibrium. The set of
conditions given in [13] come under a special case of the ﬁrst set of conditions given
here. Thus, these results are more general than given in [13]. Since, there are two
different sets of conditions for the existence of Blackwell-Nash equilibrium, then it
is clear that both sets of conditions are only sufﬁcient but not necessary. For a Nash
s ∈ A2(s)) as action of player 1
s ∈ A1(s) (resp., a2
equilibrium (f ∗, g∗), we denote a1
(resp., player 2) such that f ∗(s, a1
s) = 1 (resp., g∗(s, a2
s) = 1) for all s ∈ S. We use
these notations throughout the paper.

9

First set of sufﬁcient conditions

The ﬁrst set of conditions are as follows:
C1. (f ∗, g∗) is a pure strategy Nash equilibrium of a discounted stochastic game.
C2.

· · ·
· · ·

p|S|
p|S|
...
p|S|
s∈S ps = 1. The Markov chain induced by (f ∗, g∗) satisﬁes

P (f ∗, g∗) = 





p1
p1
...
p1

p2
p2
...
p2






· · ·



where ps ≥ 0, ∀ s ∈ S,
the state independent transition (SIT) property.

P

C3.






ps′ r1(s′, a1

s′ , a2

s′) ≥

ps′ r2(s′, a1

s′ , a2

s′) ≥

p(s′|s, a1, a2

s)r1(s′, a1

s′, a2

s′ ), ∀ s ∈ S, a1 ∈ A1(s),

p(s′|s, a1

s, a2)r2(s′, a1

s′, a2

s′ ), ∀ s ∈ S, a2 ∈ A2(s).

s′∈S
X

s′∈S
X

s′∈S
X

s′∈S
X

Remark 4. If p¯s = 1 for some ¯s ∈ S and ps = 0, ∀ s ∈ S, s 6= ¯s then C2 and C3
correspond to the conditions given in [13].

Remark 5. A pure Nash equilibrium of a SIT stochastic game will always satisfy the
conditions C1 and C2.

It is known that there is a one to one correspondence between the stationary Nash
equilibria of a discounted stochastic game and the global minimizers, with objective
function value zero, of a non-convex constrained optimization problem [OP] given
below (see [4], [3]). We denote the decision variables and the objective function of
[OP] by x =

(v1)T , (v2)T , f T , gT

and ψ(x) respectively.

T

(cid:0)

2

(cid:1)

vk − rk(f, g) − βP (f, g)vk

1T
|S|

k=1
X

(cid:2)

(cid:3)

[OP] min

x

s.t.

(i) R1(s)g(s) + β

P (s′|s)g(s)v1(s′) ≤ v1(s)1|A1(s)|, ∀ s ∈ S

s′∈S
X
(ii) (f (s))T R2(s) + β

(f (s))T P (s′|s)v2(s′) ≤ v2(s)1T

|A2(s)|, ∀ s ∈ S

(iii)

(iv)

Xa1∈A1(s)

s′∈S
X
f (s, a1) = 1, ∀ s ∈ S

g(s, a2) = 1, ∀ s ∈ S

Xa2∈A2(s)

(v) f (s, a1) ≥ 0, ∀ s ∈ S, a1 ∈ A1(s)
(vi) g(s, a2) ≥ 0, ∀ s ∈ S, a2 ∈ A2(s).

10

Theorem 6. If (f ∗, g∗) is a stationary Nash equilibrium of a discrete time discounted
stochastic game at some discount factor ˆβ ∈ [0, 1) and satisﬁes the conditions C1, C2
and C3, then it will be a Blackwell-Nash equilibrium.

Proof. We prove this by using the similar argument given in [13]. Let (f ∗, g∗) be a
stationary Nash equilibrium of a discounted stochastic game at some discount factor
ˆβ and satisﬁes the conditions C1, C2 and C3. Deﬁne, the vector vi∗
β(f ∗, g∗) =
[I − βP (f ∗, g∗)]−1 ri(f ∗, g∗), i = 1, 2, where

β = vi

[I − βP (f ∗, g∗)]−1 =

1
1 − β

1 − β + βp1
βp1
...
βp1

βp2
1 − β + βp2
...
βp2

· · ·
· · ·

· · ·

βp|S|
βp|S|
...
1 − β + βp|S|










.






The value vector vi∗

β , i = 1, 2, can be written as a function of β as,

vi∗
β (1)
vi∗
β (2)
...
vi∗
β (|S|)

vi∗
β = 





ri(1, a1
ri(2, a1

1, a2
2, a2

1) + β
1−β
2) + β
1−β

s∈S psri(s, a1
s∈S psri(s, a1

s, a2
s)
s, a2
s)



.



= 





β )T , f ∗T , g∗T






P
...
P
|S|) + β

1−β

T

P

ri(|S|, a1

|S|, a2

s∈S psri(s, a1

s, a2
s)







(v1∗

β )T , (v2∗

, then ψ(x∗) = 0 for all β ∈ [0, 1). To show
Let x∗ =
that (f ∗, g∗) is a Blackwell-Nash equilibrium, it is sufﬁcient to show that there exists
a β0 ∈ [0, 1) such that x∗ is a feasible point of the optimization problem [OP] for all
β ∈ [β0, 1). At x∗ the constraints (i) and (ii) of [OP] can be written as

(cid:17)

(cid:16)

r1(s, a1, a2

s) + β

s′∈S
X

r2(s, a1

s, a2) + β

p(s′|s, a1, a2

s)v1∗

β (s′) ≤ v1∗

β (s), ∀ s ∈ S, a1 ∈ A1(s).

p(s′|s, a1

s, a2)v2∗

β (s′) ≤ v2∗

β (s), ∀ s ∈ S, a2 ∈ A2(s).

For all s ∈ S, a1 ∈ A1(s), deﬁne

s′∈S
X

s,a1 = r1(s, a1, a2
θ1

s) + β

For all s ∈ S, a2 ∈ A2(s), deﬁne

θ2
s,a2 = r2(s, a1

s, a2) + β

s′∈S
X

s′∈S
X

p(s′|s, a1, a2

s)v1∗

β (s′) − v1∗

β (s).

p(s′|s, a1

s, a2)v2∗

β (s′) − v2∗

β (s).

Now, we consider two cases
Case I: For each s ∈ S we have two sub cases as given below.

11

If a1 ∈ A1(s) is such that a1 = a1

s, then

θ1
s,a1

s = r1(s, a1

s, a2

s) + β

ps′ v1∗

β (s′) − v1∗

β (s)

= r1(s, a1

s, a2

s) + β

s′∈S
X

ps′

r1(s′, a1

s′ , a2

s′) +

s′∈S
X
− r1(s, a1

s, a2

s) −

= 0, ∀ β.

If a1 ∈ A1(s) is such that a1 6= a1

s, then,

s,a1 = r1(s, a1, a2
θ1

s) + β

p(s′|s, a1, a2
s)

s′∈S
X

− r1(s, a1

s, a2

s) −

That is

θ1
s,a1 =

r1(s, a1, a2

s) − r1(s, a1

s, a2
s)

β
1 − β

p˜s r1(˜s, a1

˜s, a2
˜s)

!

X˜s∈S
p˜s r1(˜s, a1

˜s, a2
˜s)

β
1 − β

X˜s∈S

r1(s′, a1

s′ , a2

s′ ) +

β
1 − β

p˜s r1(˜s, a1

˜s, a2
˜s)

!

p˜s r1(˜s, a1

˜s, a2

X˜s∈S
˜s).

β
1 − β

X˜s∈S

(cid:0)

− β

s′∈S
X

(cid:1)

ps′ r1(s′, a1

s′, a2

s′ ) −

p(s′|s, a1, a2

s)r1(s′, a1

s′ , a2

s′ )

s′∈S
X

(22)

!

s′∈S ps′ r1(s′, a1

When
independent of β. Hence, θ1
P
C3 we have, θ1

s,a1 ≤ 0, ∀ β ≥ β1

P
s,a1, where

s′ , a2

s′ ) =

s′∈S p(s′|s, a1, a2

s′ ), then, (22) is
s,a1 ≤ 0, ∀ β because it holds for ˆβ. In other cases from

s)r1(s′, a1

s′ , a2

β1
s,a1 =

s′∈S ps′ r1(s′, a1
(cid:2)

s′∈S p(s′|s, a1, a2

s)r1(s′, a1

s′, a2

s′ )

(cid:3)

s) − r1(s, a1

s, a2
s)

r1(s, a1, a2
s′ , a2
s′ ) −

.

(23)

(cid:2)P

s,a1 ≤ ˆβ < 1 because (f ∗, g∗) is a Nash equilibrium at ˆβ ∈ [0, 1) and

It is clear that β1
hence each constraint of [OP] is satisﬁed by (f ∗, g∗) at ˆβ.
Case II: As similar to Case I, for each s ∈ S we have two sub cases.

P

(cid:3)

If a2 ∈ A2(s) is such that a2 = a2
a2 6= a2

s, then,

s, then, θ2

s,a2
s

θ2
s,a2 =

r2(s, a1

s, a2) − r2(s, a1

s, a2
s)

= 0 ∀ β. If a2 ∈ A2(s) is such that

(cid:0)

− β

s′∈S
X

(cid:1)

ps′ r2(s′, a1

s′, a2

s′ ) −

p(s′|s, a1

s, a2)r2(s′, a1

s′ , a2

s′ )

s′∈S
X

(24)

12

!

.

.

 
 
 
 
s′∈S ps′ r2(s′, a1

s′ , a2
When
independent of β and hence θ2
0, ∀ β ≥ β2

s,a2 , where

P

s′∈S p(s′|s, a1

s′ ) =
s,a2 ≤ 0, ∀ β. In other cases from C3 we have, θ2

s′ ), then, (24) is
s,a2 ≤

s, a2)r2(s′, a1

s′ , a2

P

β2
s,a2 =

s′∈S ps′ r2(s′, a1
(cid:2)

r2(s, a1
s′ , a2

s, a2) − r2(s, a1

s, a2
s)
s, a2)r2(s′, a1
s′∈S p(s′|s, a1

s′ ) −

(cid:3)

s′, a2

s′ )

.

(25)

From the same argument as used in Case I, we have β2

P

(cid:2)P

s,a2 < 1. Now, deﬁne

β0 = max
s∈S

max
a1∈A1(s);a16=a1
s

max
a2∈A2(s);a26=a2
s

{0, β1

s,a1, β2

s,a2 }

(cid:3)

(26)

s,a1, β2

s,a1 and β2

whenever β1
s,a2 are well deﬁned. We include “0” in (26) because lower
bounds β1
s,a2 deﬁned in (23), (25) respectively can be negative also. It is clear
that β0 ∈ [0, 1). It is easy to see that the constraints (i) and (ii) of the optimization
problem [OP] are feasible at x∗ =
for all β ∈ [β0, 1).
The other constraints (iii)-(vi) of [OP] does not depend on β and are feasible at
(f ∗, g∗). At x∗ the objective function value of [OP] is zero and all the constraints
are feasible for all β ∈ [β0, 1) which means that (f ∗, g∗) is a Blackwell-Nash equilib-
rium.

β )T , f ∗T , g∗T

β )T , (v2∗

(v1∗

(cid:16)

(cid:17)

T

Second set of sufﬁcient conditions

The second set of conditions are as follows:
D1. (f ∗, g∗) is a pure strategy Nash equilibrium of a discounted stochastic game.
D2. The Markov chain induced by (f ∗, g∗) reduces into |S| ergodic classes where each
class contains only one state, i.e.,

P (f ∗, g∗) = 





1
0
...
0

0 · · ·
1 · · ·
...
0 · · ·

.

0
0

...

1




D3.

r1(s, a1, a2

s) ≥

r2(s, a1

s, a2) ≥

s′∈S
X

s′∈S
X

p(s′|s, a1, a2

s)r1(s′, a1

s′, a2

s′ ), ∀ s ∈ S, a1 ∈ A1(s),

p(s′|s, a1

s, a2)r2(s′, a1

s′, a2

s′ ), ∀ s ∈ S, a2 ∈ A2(s).

Theorem 7. If (f ∗, g∗) is a stationary Nash equilibrium of a discounted stochastic
game at some discount factor ˆβ ∈ [0, 1) and satisﬁes the conditions D1, D2 and D3,
then it will be a Blackwell-Nash equilibrium.






13

Proof. We prove this by using the similar argument as in the proof of Theorem
6. Let (f ∗, g∗) be a stationary Nash equilibrium of a discounted stochastic game
at some discount factor ˆβ and satisﬁes the conditions D1, D2 and D3. Let x∗ =
(v1∗

where

β )T , f ∗T , g∗T

β )T , (v2∗

T

(cid:0)
vi∗
β = [I−βP (f ∗, g∗)]−1ri(f ∗, g∗) =

(cid:1)

1, a2
1)

ri(1, a1
1 − β

,

2, a2
2)

ri(2, a1
1 − β

, · · · ,

ri

|S|, a1

|S|, a2
|S|

1 − β

(cid:0)

,

(cid:1)

!

for i = 1, 2. From the construction of the objective function of [OP], ψ(x∗) = 0 for
all β ∈ [0, 1). To show that (f ∗, g∗) is a Blackwell-Nash equilibrium, it is sufﬁcient to
show that there exists a β0 ∈ [0, 1) such that x∗ is a feasible point of the optimization
problem [OP] for all β ∈ [β0, 1). We discuss two cases.
Case I: For each s ∈ S we have two sub cases as given below.

If a1 ∈ A1(s) is such that a1 = a1
a1 6= a1

s, then,

s, then, θ1

s,a1
s

= 0, ∀ β. If a1 ∈ A1(s) is such that

s,a1 = r1(s, a1, a2
θ1

s) + β

p(s′|s, a1, a2
s)

That is,

θ1
s,a1 =

1
1 − β

s′∈S
X

r1(s, a1, a2

s) − r1(s, a1

s, a2
s)

r1(s′, a1

s′ , a2

s′)

1 − β

−

s, a2
s)

r1(s, a1
1 − β

.

(cid:0)

−

(cid:1)
r1(s, a1, a2

s) −

β
1 − β  

p(s′|s, a1, a2

s)r1(s′, a1

s′, a2

s′ )

s′∈S
X

(27)

!

When r1(s, a1, a2
of β and hence θ1
β1
s,a1, where

P

s′∈S p(s′|s, a1, a2

s) =
s,a1 ≤ 0, ∀ β. In other cases from D3 we have, θ1

s′ ), then, (27) is independent
s,a1 ≤ 0, ∀ β ≥

s)r1(s′, a1

s′ , a2

β1
s,a1 =

r1(s, a1, a2
r1(s, a1, a2
s) −
(cid:2)

s) − r1(s, a1
s′∈S p(s′|s, a1, a2

s, a2
s)
s)r1(s′, a1
(cid:3)

s′ , a2

s′ )

.

(28)

s,a1 ≤ ˆβ < 1 because (f ∗, g∗) is a Nash equilibrium at ˆβ ∈ [0, 1) and

It is clear that β1
hence each constraint of [OP] is satisﬁed by (f ∗, g∗) at ˆβ.
Case II: As similar to Case I, for each s ∈ S we have two sub cases.

P

(cid:3)

(cid:2)

If a2 ∈ A2(s) is such that a2 = a2
a2 6= a2

s, then,

s, then, θ2

s,a2
s

= 0 ∀ β. If a2 ∈ A2(s) is such that

θ2
s,a2 =

1
1 − β

r2(s, a1

s, a2) − r2(s, a1

s, a2
s)

(cid:0)

−

(cid:1)
s, a2) −
r2(s, a1

β
1 − β  

s′∈S
X

14

p(s′|s, a1

s, a2)r2(s′, a1

s′, a2

s′ )

!

(29)

.

.

 
s, a2) =

When r2(s, a1
of β and hence θ2
β2
s,a2, where

P

s′∈S p(s′|s, a1

s, a2)r2(s′, a1

s′ , a2

s,a2 ≤ 0, ∀ β. In other cases from D3 we have, θ2

s′ ), then, (29) is independent
s,a2 ≤ 0, ∀ β ≥

β2
s,a2 =

r2(s, a1

s, a2) −
(cid:2)

r2(s, a1

s, a2) − r2(s, a1
s′∈S p(s′|s, a1

s, a2
s)
s, a2)r2(s′, a1

(cid:3)

s′ , a2

s′ )

.

(30)

From the same argument as used in Case I, we have β2

P

(cid:2)

s,a2 < 1. Now, deﬁne

(cid:3)

β0 = max
s∈S

max
a1∈A1(s);a16=a1
s

max
a2∈A2(s);a26=a2
s

{0, β1

s,a1, β2

s,a2 }

(31)

s,a1 and β2

whenever β1
s,a2 are well deﬁned. Now, at x∗ the objective function value
of [OP] is zero and the constraints are feasible for all β ∈ [β0, 1) which means that
(f ∗, g∗) is a Blackwell-Nash equilibrium.

Next, we give an example where a stationary Nash equilibrium of the discounted game
at β = 0.6 satisﬁes the conditions D1, D2 and D3 and hence it is a Blackwell-Nash
equilibrium from Theorem 7.

Example 8. We consider a stochastic game where there are 2 states and both the
players have two actions at state 1 and only one action at state 2 , i.e., S = {1, 2},
A1(1) = A2(1) = {1, 2}, A1(2) = A2(2) = {1}. The rewards of both the players
and the transition probabilities for different combinations of states and actions are
summarized in the Table 2.

Table 2: Immediate rewards and Transition Probabilities

(a) s = 1

(1,0)

(4,4.4)

PPPPPPPP
PPPPPPPP

(5,6)

(0,1)

(4,5)

(0,1)

PPPPPPPP
PPPPPPPP

(3,2)

(1,0)

(b) s = 2

PPPPPPPP

(0, 1)

(3,4)

.

In the above game there are two stationary deterministic strategies for each player. We
denote the stationary deterministic strategies of player 1 by f1 = (1, 0) and f2 = (0, 1)
and the stationary deterministic strategies of player 2 by g1 = (1, 0) and g2 = (0, 1).
We prove that (f1, g1) = ((1, 0), (1, 0)) is a Blackwell-Nash equilibrium of the game.
We ﬁrst show that (f1, g1) is a Nash equilibrium at β = 0.6. Using the data given
in Table 2, we have v1
0.6(f2, g1) =
(9.5, 7.5), v2

0.6(f1, g1) = (10, 7.5), v2

0.6(f1, g1) = (11, 10), v1

0.6(f1, g2) = (11, 10). That is

0.6(f1, g1) ≥ v1
v1

0.6(f2, g1).

0.6(f1, g1) ≥ v2
v2

0.6(f1, g2).

(32)

(33)

From (32) and (33) (f1, g1) is a Nash equilibrium because for a ﬁxed stationary strategy
of one player, other player faces a MDP where optimal strategy exists in the space of

15

stationary deterministic strategies (see [11]). It is easy to verify that D1, D2 and D3
hold at (f1, g1), i.e., it is a Blackwell-Nash equilibrium. From (31), β0 = 0.6, so
(f1, g1) is a Nash equilibrium for all β ∈ [0.6, 1).

3 Continuous time stochastic game

We recall the deﬁnition of a continuous time stochastic game from [10]. Similar to
a discrete time stochastic game, S denote a ﬁnite set of states, and Ai(s) denote a
ﬁnite set of actions of player i available at state s ∈ S, and ri is an immediate payoff
6= s, and a1 ∈ A1(s), a2 ∈
function of player i. For all s, s′ ∈ S such that s′
A2(s), let µ(s′, s, a1, a2) ≥ 0 be a rate of transition from state s to state s′, when
player 1 and player 2 choose actions a1 and a2 respectively. Denote µ(s, s, a1, a2) =
s′6=s µ(s′, s, a1, a2). At time t ∈ [0, ∞), if state is s, and player 1 plays an action
−
a1, and player 2 plays an action a2 during inﬁnitesimal time dt, the payoff of player 1
is r1(s, a1, a2)dt, and the payoff of player 2 is r2(s, a1, a2)dt. A transition from s
to s′ occurs with probability µ(s′, s, a1, a2)dt. It stays in state s with probability 1 +
µ(s, s, a1, a2)dt. In the former case, the sojourn time at state s follows an exponential
distribution with parameter −µ(s, s, a1, a2) ≥ 0.

P

t ), with xi

A play of a continuous time stochastic game is a measurable function h : [0, ∞) →
t ∈ ℘(Ai(st)), i = 1, 2. Given
S × ℘(A1) × ℘(A2), t 7→ h(t) = (st, x1
t , x2
a play h, we deﬁne ht as history up to time t as the restriction of the ﬁrst coordinate
of h to the time interval [0, t] and the restriction of the second and third coordinate to
[0, t). The above deﬁnitions of play and history in continuous time stochastic game
are due to Neyman [10] where players observe their past mixed actions unlike the
pure actions in discrete games. The decision of choosing action at any time t might
depend on various factors and it leads to different class of strategies. The case where
decision of choosing an action at any time t depends on the entire history up to time
t deﬁnes the history dependent strategies while for Markov strategies decision making
depend only on time t and the state at time t. The stationary strategies are deﬁned by
the decision making rules that depend only on the states. The deﬁnition of stationary
strategy f (resp., g) of player 1 (resp., player 2) is same as in discrete time stochastic
game. Unlike in discrete time stochastic games, a strategy pair (π1, π2) and an initial
state s0 need not deﬁne unambiguously a probability distribution Ps0
π1,π2 over plays of
continuous time stochastic game. A strategy proﬁle (π1, π2) is an admissible strategy
proﬁle, if for a given initial state s0, probability distributions Ps0
1,π2 and Ps0
over
π′
π1,π′
2
plays of continuous time stochastic game are unambiguously deﬁned for all π′
1 and π′
2.
The class of Markov strategies and stationary strategies are contained in the class of
admissible strategies. For a stationary strategy pair (f, g) ∈ FS × GS and initial state
s0, a unique probability distribution P s0

f,g satisﬁes the equality,

P s0

f,g(st+δ = s|ht) = δµ(s0, st, f (st), g(st)) + o(δ),

where, µ(s0, st, f (st), g(st)) =
For the details about all the deﬁnitions given above see [10]. The existence of a sta-
tionary Nash equilibrium in the discounted continuous time stochastic game restricted

a2∈A2(st) µ(s0, st, a1, a2)f (st, a1)g(st, a2).

a1∈A1(st)

P

P

16

to Markov strategies appears in [8]. Neyman [10] showed the existence of stationary
Nash equilibrium by allowing history dependent strategies. Therefore, from now
onwards we restrict ourselves to the class of stationary strategies.

For a given strategy pair (f, g) and an initial state s, the expected discounted reward

of player i, i = 1, 2, is given by

vi
α(s, f, g) = Es
f,g

∞

0
Z

e−αtri(st, x1

t , x2

t )dt,

(34)

where α > 0 is a discount rate. A strategy pair (f ∗, g∗) is said to be an α-discounted
Nash equilibrium if for all s ∈ S the following inequalities hold,

v1
α(s, f ∗, g∗) ≥ v1
α(s, f ∗, g∗) ≥ v2
v2

α(s, f, g∗), ∀ f ∈ FS,
α(s, f ∗, g), ∀ g ∈ GS.

We call strategy pair (f ∗, g∗) Nash equilibrium despite restricting f and g as stationary
strategies. This is possible because for a ﬁxed stationary strategy of one player, other
player’s problem is a continuous time Markov decision process (CTMDP) where an
optimal strategy exists in the space of stationary strategies [11], [9].

3.1 Some preliminary results and notations

We give some preliminary results which are useful in the subsequent analysis. Deﬁne,

||µ|| =

sup
s∈S,a1∈A1(s),a2∈A2(s) 

µ(s′, s, a1, a2)



.

s′∈S;s′6=s
X


For a ﬁxed stationary strategy g of player 2, player 1 faces a CTMDP(g).
The immediate rewards and transition rates of CTMDP(g) are respectively
given by r1(s, a1, g) =
a2∈A2(s) r1(s, a1, a2)g(s, a2) and µ(s′, s, a1, g) =
It is well known
that using uniformization technique a CTMDP can be solved by an equivalent DT-
P
MDP [12] [9]. The rewards, transition probabilities, and discount factor of DTMDP(g)
equivalent to CTMDP(g) are given by,

a2∈A2(s) µ(s′, s, a1, a2)g(s, a2) for all s, s′ ∈ S, a1 ∈ A1(s).

P



¯r1(s, a1) =

p1(s′|s, a1) =

r1(s, a1, g)
||µ|| + α
µ(s′, s, a1, g)
||µ||

β =

||µ||
α + ||µ||

,

, ∀ s ∈ S, a1 ∈ A1(s),

+ δ(s, s′), ∀ s, s′ ∈ S, a1 ∈ A1(s),

(35)






where δ(·) is a Kronecker delta. Similarly, for a ﬁxed stationary strategy f of player
1, player 2 faces a CTMDP(f ). The immediate rewards and transition rates of
a1∈A1(s) r2(s, a1, a2)f (s, a1)
CTMDP(f ) are respectively given by r2(s, f, a2) =
a1∈A1(s) µ(s′, s, a1, a2)f (s, a1) for all s, s′ ∈ S, a2 ∈ A2(s).
and µ(s′, s, f, a2) =
P

P

17

The rewards, transition probabilities, and discount factor of DTMDP(f ) equivalent to
CTMDP(f ) are given by,

¯r2(s, a2) =

p2(s′|s, a2) =

r2(s, f, a2)
||µ|| + α
µ(s′, s, f, a2)
||µ||

β =

||µ||
α + ||µ||

.

, ∀ s ∈ S, a1 ∈ A1(s),

+ δ(s, s′), ∀ s, s′ ∈ S, a2 ∈ A2(s),

(36)






The continuous time stochastic game is deﬁned using the transition rates. Let Q(f, g)
denote the transition rate matrix induced by a stationary strategy pair (f, g), where
Q(f, g) = [µ(s′, s, f, g)]ss′.

3.2 Blackwell-Nash equilibrium in continuous time stochastic

games

A BNE for continuous time stochastic games can be deﬁned similar to discrete time
stochastic games as follows:

Deﬁnition 9. A strategy pair (f ∗, g∗) is said to be a BNE of a continuous time stochas-
tic game if there exists an α0 > 0 such that (f ∗, g∗) is an α-discounted Nash equilib-
rium for every α ∈ (0, α0].

We provide the results on BNE for continuous time stochastic games along sim-
ilar lines. We ﬁrst show that a stationary BNE in general continuous time stochastic
games need not alway exist. We give an example of a single controller continuous
time stochastic game where stationary BNE does not exist. This example shows that
BNE need not always exist even for single controller games which is a special class
of general stochastic games. Then, we show the existence of a stationary determinis-
tic BNE for continuous time SC-AR stochastic games. Finally, for general continuous
time stochastic games we give two different sets of conditions and show that each set
of conditions together are sufﬁcient for a Nash equilibrium to be a BNE.

3.2.1 A counter example

We give an example which does not have any stationary BNE.

Example 10. We consider a continuous time stochastic game with 2 states and both
players having two actions at state 1 and only one action at state 2, i.e., S = {1, 2},
A1(1) = A2(1) = {1, 2}, A1(2) = A2(2) = {1}. The rewards of both the players and
transition rates for different combination of states and actions are summarized in the
Table 3. The upper half of each box of table represents transition rates and lower half
represents immediate rewards.

The Example 10 can be viewed as a continuous time version of Example 2.
the Example 10 does not possess a stationary BNE. Let

We show that

18

Table 3: Immediate rewards and Transition Rates

(a) s = 1

(4, 9)

(0, 0)

PPPPPPPP
PPPPPPPP

(0, 0)

(5, 4)

(6, 3)

(-1, 1)

PPPPPPPP
PPPPPPPP

(-1, 1)

(4, 5)

(b) s = 2

PPPPPPPP

(1, -1)

(6, 7)

.

(f, g) = ((p, 1 − p), (q, 1 − q)), for some 0 ≤ p, q ≤ 1, be an arbitrary stationary
strategy pair. For ﬁxed g, player 1 faces a CTMDP(g). From the data of the game
||µ|| = 1. The CTMDP(g) is equivalent to the DTMDP(g) deﬁned by (35). The transi-
tion probabilities of DTMDP(g) do not depend on the actions of player 1 because the
transition rates do not depend on the actions of player 1. So, f ∗ is an optimal policy of
player 1 for DTMDP(g) if and only if for each s ∈ S,

¯r1(s, f ∗) = max
f ∈FS

¯r1(s, f ) =

1
1 + α

max
a1∈A1(s)

[R1(s)g(s)]a1 .

(37)

We need to determine f ∗ only at state s = 1. We have,

R1(1)g(1) = [6 − 2q, 4 + q]T .

Let f1 = (1, 0) and f2 = (0, 1) be two stationary deterministic strategies of player 1.
From (37),

f1
f2

(p, 1 − p) : 0 ≤ p ≤ 1

f ∗ = 


if q < 2
3
if q > 2
3
if q = 2
3 .

(38)

(cid:9)



(cid:8)

Equation (38) gives the optimal policy of player 1 for DTMDP(g) for all β ∈ [0, 1).
Therefore, f ∗ gives an optimal policy of CTMDP(g) for all α > 0. That is, f ∗ gives
the best response of player 1 for all α > 0 for a ﬁxed strategy g = (q, 1−q) of player 2.
Similarly, for a ﬁxed f = (p, 1 − p), player 2 faces a CTMDP(f ). The equivalent
DTMDP(f ) is deﬁned by (36). Let g1 = (1, 0) and g2 = (0, 1) be two stationary
deterministic strategies of player 2. By using the data given in Example 10, the value
vector of player 2 for DTMDP(f ) is given below:

β(f, g1) = [I − βP (g1)]−1 ¯r2(g1) = β
u2

4 + 5p
1 − β

,

(cid:20)

(4 + 5p)β
1 − β

β(f, g2) = [I − βP (g2)]−1¯r2(g2) = β
u2

By using (39) and (40) we have,

5 − 2p + 7β
1 − β2

,

(cid:20)

T

+ 7

.

(39)

(cid:21)
(5 − 2p)β + 7
1 − β2

T

(cid:21)

.

(40)

β(f, g1) − u2
u2

β(f, g2) = β

(cid:20)

p(7 + 5β) − (3β + 1)
1 − β2

,

β(p(7 + 5β) − (3β + 1))
1 − β2

T

.

(cid:21)

(41)

19

By substituting β = 1
given by

1+α in (41), the difference in the value vector of CTMDP(f ) is

α(f, g1) − v2
v2

α(f, g2) =

(cid:20)

p(12 + 7α) − (4 + α)
α(α + 2)

,

p(12 + 7α) − (4 + α)
α(α + 2)(1 + α)

T

(cid:21)

.

(42)

From (42) the best response g∗ of player 2 against a ﬁxed strategy f = (p, 1 − p) of
player 1 for a given discount rate α is given by (43)

g1
g2

g∗ = 


(q, 1 − q) : 0 ≤ q ≤ 1

if p > 4+α
12+7α
if p < 4+α
12+7α
if p = 4+α
12+7α .

(43)

(cid:9)


(cid:8)
3 , 1

3

,

2

4+α

12+7α

12+7α , 8+6α

From (38) and (43) it is easy to see that for a discount rate α, a strategy pair (f ∗

α) =
α are best responses of each other,
α) follows from the similar
α is an invertible function of α, then the Nash
α) varies with discount rate α. This implies that Example 10 will not

i.e., it is a Nash equilibrium. The uniqueness of (f ∗
(cid:1)(cid:17)
(cid:16)(cid:16)
(cid:0)
arguments used in Example 2. Since, f ∗
equilibrium (f ∗
have a stationary BNE.

is such that f ∗

α and g∗

α, g∗

α, g∗

α, g∗

(cid:17)

From Example 10 it is clear that in general a continuous time stochastic game need
not admit a stationary BNE. In fact Example 10 belongs to the class of single controller
games. So, even for the class of single controller games there is no guarantee that
a stationary BNE will exist. Next, we describe SC-AR stochastic games which is a
special class of single controller games. Similar to discrete case we show that there
always exists a stationary deterministic BNE.

3.2.2 Single Controller Additive Reward Games

A continuous time SC-AR stochastic game is characterized by the following assump-
tions:

(a) µ(s′, s, a1, a2) = µ(s′, s, a2) for all s′, s ∈ S, a1 ∈ A1(s), a2 ∈ A2(s), i.e., the

transition rates only depend on the actions of player 2.

(b) r1(s, a1, a2) = r1

1(s, a1) + r1

2(s, a2), for all s ∈ S, a1 ∈ A1(s), a2 ∈ A2(s).

Theorem 11. Every continuous time SC-AR stochastic game possesses a stationary
deterministic BNE.

Proof. For each s ∈ S select an action a1∗
s
1(s, a1)}. Deﬁne f ∗ ∈ FS by
argmax
a1∈A1(s)

{r1

∈ A1(s) such that a1∗
s

∈

f ∗(s, a1) =

1
0
(

if a1 = a1∗
s ,
otherwise

(44)

20

1−β0
β0
v2
α(s, f ∗, g∗) ≥ v2

(cid:0)

for each s ∈ S. For above strategy f ∗ of player 1, player 2 faces a CTMDP(f ∗).
The equivalent DTMDP(f ∗) is deﬁned by (36). For DTMDP(f ∗) there always exists a
stationary deterministic strategy g∗ which is Blackwell optimal [2]. Then, there exists
a discount factor β0 such that g∗ is an optimal strategy for all β ∈ [β0, 1), i.e., for all
s ∈ S,

u2
β(s, f ∗, g∗) ≥ u2

β(s, f ∗, g), ∀ g ∈ GS, β ∈ [β0, 1),

(45)
where u2
β(s, f ∗, g) is the expected discounted reward of the DTMDP(f ∗) for a given
initial state s and strategy g. From [12], v2
β(s, f ∗, g), for all s ∈ S,
where relationship between α and β is given by (36). Then, for discount factor β0 we
have discount rate α0 = ||µ||

. Therefore, from (45) we have for all s ∈ S

α(s, f ∗, g) = u2

α(s, f ∗, g), ∀ g ∈ GS, α ∈ (0, α0].
(cid:1)

(46)

From (44), we have

r1(f ∗, g∗) ≥ r1(f, g∗), ∀ f ∈ FS.
Because the transitions rates do not depend on the strategies f ∈ FS, therefore, we
have for all s ∈ S,

(47)

v1
α(s, f ∗, g∗) ≥ v1
From (46) and (48), (f ∗, g∗) is a BNE.

α(s, f, g∗), ∀ f ∈ FS, α > 0.

(48)

3.2.3 Sufﬁcient conditions for BNE in general stochastic games

We consider a two player general continuous time stochastic game with discounted
payoff criterion. We give two disjoint sets of conditions where each set of conditions
together are sufﬁcient for a stationary Nash equilibrium to be a BNE.

First set of sufﬁcient conditions:
M1. (f ∗, g∗) is a pure strategy Nash equilibrium.

M2.

p1 − 1
p1
Q(f ∗, g∗) = ||µ|| 
...

p1



s∈S ps = 1.

p2
p2 − 1
...
p2

· · ·
· · ·

· · ·

p|S|
p|S|
...
p|S| − 1


,






where ps ≥ 0, ∀ s ∈ S,

ps′ r1(s′, a1

P
s′, a2

s′ ) ≥

M3.

s′∈S
X

s′∈S
X






ps′ r2(s′, a1

s′, a2

s′ ) ≥

+ δ(s, s′)

r1(s′, a1

s′, a2

s′ ),

(cid:19)

∀ s ∈ S, a1 ∈ A1(s),

+ δ(s, s′)

r2(s′, a1

s′, a2

s′ ),

(cid:19)

∀ s ∈ S, a2 ∈ A2(s).

µ(s′, s, a1, a2
s)
||µ||

s, a2)

µ(s′, s, a1
||µ||

s′∈S (cid:18)
X

s′∈S (cid:18)
X

21

Theorem 12. If (f ∗, g∗) is a stationary Nash equilibrium of a discounted continuous
time stochastic game at some discount rate ˆα > 0 and satisﬁes the conditions M1, M2
and M3, then it will be a BNE.

Proof. Let (f ∗, g∗) be a stationary Nash equilibrium of a continuous time discounted
stochastic game at some discount rate ˆα > 0. Then, f ∗ is an optimal policy of
CTMDP(g∗) at discount rate ˆα. Therefore, f ∗ is an optimal policy of the equivalent
DTMDP(g∗), deﬁned by (35), at ˆβ = ||µ||
α+||µ|| [12]. We are interested in the range of β
for which f ∗ is an optimal policy of DTMDP(g∗). That is, the range of β for which the
optimality equations for the DTMDP(g∗) given below are satisﬁed by f ∗,

u1∗(s) = ¯r1(s, a1

s) + β

p1(s′|s, a1

s)u1∗(s′), ∀ s ∈ S,

(49)

and

s′∈S
X

u1∗(s) ≥ ¯r1(s, a1) + β

p1(s′|s, a1)u1∗(s′), ∀ s ∈ S, a1 ∈ A1(s), a1 6= a1

s, (50)

s′∈S
X
where u1∗ is the value vector of player 1 for f ∗. That is,

u1∗ = u1

β(f ∗) = (I − βP 1(f ∗))−1 ¯r1(f ∗),

where transitions probability matrix induced by f ∗ for DTMDP(g∗) is given by,

P 1(f ∗) =

Q(f ∗, g∗)
||µ||

From direct calculation we have,

p1
p1
...
p1

p2
p2
...
p2

· · ·
· · ·

· · ·

+ I = 





p|S|
p|S|
...
p|S|


.






u1(f ∗) = ¯r1(f ∗) +

β
1 − β

s∈S
X

ps¯r1(s, a1

s)1|S|.

It is easy to see that (49) holds. Denote,

s,a1 = ¯r1(s, a1) + β
θ1

p1(s′|s, a1)u1∗(s′) − u1∗

β (s).

(51)

s′∈S
X

for all s ∈ S, a1 ∈ A1(s), a1 6= a1

s. By substituting the value of u1∗ in (51), we have

θ1
s,a1 =

¯r1(s, a1)−¯r1(s, a1
s)

−β

ps′ ¯r1(s′, a1

s′) −

(cid:0)

(cid:1)

s′∈S
X

s′∈S
X

for all s ∈ S, a1 ∈ A1(s), a1

6= a1
s.
s′ ) for some s ∈ S, a1 ∈ A1(s), a1 6= a1

s′∈S ps′ ¯r1(s′, a1

If

s′∈S p(s′|s, a1)¯r1(s′, a1

P

p1(s′|s, a1)¯r1(s′, a1

s′ )

,

!
(52)
s′ ) =
s, then, (52) is

P

22

 
independent of β. Therefore, θ1
M3, we have, θ1

s,a1 ≤ 0, ∀ β ≥ β1

s,a1, where

s,a1 ≤ 0, ∀ β because it holds for ˆβ. In other cases from

β1
s,a1 =

=

P

¯r1(s, a1) − ¯r1(s, a1
s)

s′∈S ps′ ¯r1(s′, a1

s′ ) −

s′∈S p1(s′|s, a1)¯r1(s′, a1
s′ )
s) − r1(s, a1
s, a2
s)
µ(s′,s,a1,a2
s)
+ δ(s, s′)
||µ||

r1(s, a1, a2
P
s′ ) −

s′∈S

s′∈S ps′ r1(s′, a1

s′ , a2

P

P

(cid:16)

(cid:17)

It is clear that β1

s,a1 ≤ ˆβ < 1, because f ∗ is an optimal policy at ˆβ. Deﬁne,

r1(s′, a1

s′ , a2

.

s′ )
(53)

β1
0 =

max
s∈S,a1∈A1(s),a16=a1
s

{0, β1

s,a1},

(54)

whenever β1
s,a1 is well deﬁned. We include “0” in (54) because β1
can be negative. Now, f ∗ is an optimal policy of the DTMDP(g∗) for all β ∈ [β1
From [12], f ∗ is an optimal policy of the CTMDP(g∗) for all α ∈ (0, α1
0], where,

s,a1 deﬁned by (53)
0 , 1).

α1

0 =

(1 − β1
0 )||µ||
β1
0

.

Therefore, f ∗ is a best response of g∗ for all α ∈ (0, α1
0].

For ﬁxed f ∗, player 2 faces a CTMDP(f ∗) whose optimal policy is g∗ at discount
rate ˆα. Therefore, g∗ is an optimal policy of player 2 for the equivalent DTMDP(f ∗),
deﬁned by (36), at ˆβ = ||µ||
ˆα+||µ|| . We are interested in ﬁnding the range of β for which
the optimality equations for DTMDP(f ∗) given below are satisﬁed at g∗.

u2∗(s) = ¯r2(s, a2

s) + β

p2(s′|s, a2

s)u2∗(s′), ∀ s ∈ S.

(55)

s′∈S
X

u2∗(s) ≥ ¯r2(s, a2) + β

p2(s′|s, a2)u2∗(s′), ∀ s ∈ S, a2 ∈ A2(s), a2 6= a2

s, (56)

s′∈S
X

where u2∗ is the value vector of player 2 at g∗. The transition probability matrix in-
duced by g∗ for DTMDP(f ∗) is given by,

P 2(g∗) =

Q(f ∗, g∗)
||µ||

+ I.

As similar to previous case,

u2∗ = [I − βP 2(g∗)]−1¯r2(g∗) = ¯r2(g∗) +

It is clear that (55) holds. Denote,

β
1 − β

s∈S
X

ps¯r2(s, a2

s)1

|S|.

s,a2 = ¯r2(s, a2)+β
θ2

p2(s′|s, a2)u2∗(s′)−u2∗(s), ∀ s ∈ S, a2 ∈ A2(s), a2 6= a2
s.

s′∈S
X

23

(57)

By substituting the value of u2∗ in (57) we have

θ2
s,a2 =

¯r2(s, a2)−¯r2(s, a2
s)

−β

ps′ ¯r2(s′, a2

s′) −

(cid:0)

(cid:1)

s′∈S
X

s′∈S
X

for all s ∈ S, a2 ∈ A2(s), a2

6= a2
s.
s′ ) for some s ∈ S, a2 ∈ A2(s), a2 6= a2

s′∈S ps′ ¯r2(s′, a2

If

s,a2 ≤ 0 for all β. In other cases from M3 we have, θ2

P

s′∈S p(s′|s, a2)¯r2(s′, a2

independent of β and θ2
P
for all β ≥ β2
s,a2 , where

p2(s′|s, a2)¯r2(s′, a2

s′ )

,

!
(58)
s′ ) =
s, then (58) is
s,a2 ≤ 0

β2
s,a2 =

=

P

P

¯r2(s, a2) − ¯r2(s, a2
s)

s′∈S ps′ ¯r2(s′, a2

s′ ) −

s′∈S p2(s′|s, a2)¯r2(s′, a2
s′ )
s, a2
s, a2) − r2(s, a1
s)
s,a2)
+ δ(s, s′)

µ(s′,s,a1
||µ||

r2(s, a1
P
s′ ) −

s′∈S

.

r2(s′, a1

s′ , a2

s′ )

s′∈S ps′ r2(s′, a1

s′ , a2

It is clear that β2

(cid:16)
s,a2 ≤ ˆβ < 1, because g∗ is an optimal policy at ˆβ. Deﬁne

P

(cid:17)

β2
0 =

max
s∈S,a2∈A2(s),a26=a2
s

{0, β2

s,a2},

whenever β2
DTMDP(f ∗) for all β ∈ [β2
for all α ∈ (0, α2

0], where

s,a2 is well deﬁned. This implies that g∗ is an optimal policy of the
0 , 1). From [12], g∗ is an optimal policy of the CTMDP(f ∗)

α2

0 =

(1 − β2
0 )||µ||
β2
0

.

That is, g∗ is a best response of f ∗ for all α ∈ (0, α2

0]. Deﬁne,

α0 = min{α1

0, α2

0}.

(59)

We can say that f ∗ and g∗ are best response of each other for all α ∈ (0, α0]. So,
(f ∗, g∗) is a Nash equilibrium of a continuous time α-discounted stochastic game for
all α ∈ (0, α0], i.e., it is a BNE.

Now, we give an example of a continuous time stochastic game that possess a Nash

equilibrium which satisﬁes M1, M2 and M3.

Example 13. We consider a 2 states continuous time stochastic game where both the
players have two actions at state 1 and only one action at state 2 , i.e., S = {1, 2},
A1(1) = A2(1) = {1, 2}, A1(2) = A2(2) = {1}. The rewards of both the players and
the transition rates for different combinations of states and actions are summarized in
the Table 4.

We show that (f ∗, g∗) = ((1, 0), (0, 1)) is a Blackwell Nash equilibrium of the con-
tinuous time stochastic game given in above example. We ﬁrst show that (f ∗, g∗) is a
Nash equilibrium at α = 0.5. From the data of the game ||µ|| = 1. Fix g∗ = (0, 1),

24

 
Table 4: Immediate rewards and Transition Rates

(a) s = 1

(0,0)

(5,3)

PPPPPPPP
PPPPPPPP

(-1,1)

(3,4)

(2,3)

(-1,1)

PPPPPPPP
PPPPPPPP

(0,0)

(4,2)

(b) s = 2

PPPPPPPP

(0, 0)

(5,4)

.

then player 1 faces a CTMDP(g∗). The optimal policy of CTMDP(g∗) at discount
rate α = 0.5 can be computed by solving an equivalent DTMDP(g∗), deﬁned by (35),
at discount factor β = 1
1+α = 0.67. It is known that the optimal policy of a DT-
MDP exists among the class of stationary deterministic policies. Let f1 = (1, 0) and
f2 = (0, 1) be two stationary deterministic policies for player 1. From the above data,
the transition probability matrices induced by f1 and f2 for DTMDP(g∗) are given by,

P 1(f1) =

0
0
(cid:18)

, P 1(f2) =

1
1
(cid:19)

1
0
(cid:18)

.

0
1
(cid:19)

We have,

u1
0.67(f1) = [I − 0.67P 1(f1)]−1¯r1(f1) = (8, 10).
u1
0.67(f2) = [I − 0.67P 1(f2)]−1¯r1(f2) = (8, 10).
From (60) and (61), f1 = (1, 0) = f ∗ and f2 both are optimal policy of DTMDP(g∗)
at β = 0.67. This implies f ∗ is an optimal policy of CTMDP(g∗) at α = 0.5, i.e., f ∗ is
a best response of g∗. Fix f ∗ = (1, 0), then player 2 faces a CTMDP(f ∗). The optimal
policy of CTMDP(f ∗) can be computed by solving an equivalent DTMDP(f ∗) deﬁned
by (36). Let g1 = (1, 0) and g2 = (0, 1) be two stationary deterministic policies of
player 2. The transition probability matrices induced by g1 and g2 are given by,

(60)

(61)

P 2(g1) =

1
0
(cid:18)

, P 2(g2) =

0
1
(cid:19)

0
0
(cid:18)

.

1
1
(cid:19)

We have,

u2
0.67(g1) = [I − 0.67P 2(g1)]−1¯r2(g1) = (6, 8).
0.67(g2) = [I − 0.67P 2(g2)]−1¯r2(g2) = (7.33, 8).
u2
From (62) and (63) g2 = (0, 1) = g∗ is an optimal policy of DTMDP(f ∗) at β = 0.67.
This implies g∗ is an optimal policy of CTMDP(f ∗) at α = 0.5, i.e., g∗ is a best
response of f ∗. Hence (f ∗, g∗) is a Nash equilibrium at α = 0.5. It easy to check that
(f ∗, g∗) satisﬁes conditions M1, M2 and M3. Hence, from Theorem 12 it is a BNE.
From (59), α0 = 0.5, so (f ∗, g∗) is a Nash equilibrium for all α ∈ (0, 0.5].

(63)

(62)

Second set of sufﬁcient conditions:
N1. (f ∗, g∗) is a pure strategy Nash equilibrium

25

N2.

N3.






Q(f ∗, g∗) = 





0
0
...
0

0
0
...
0

· · ·
· · ·

· · ·

0
0
...
0



,






i.e., all the states of Markov chain induced by (f ∗, g∗) are absorbing.

r1(s, a1, a2

s) ≥

µ(s′, s, a1, a2
s)
||µ||

+ δ(s, s′)

(cid:19)

s′∈S (cid:18)
X

r2(s, a1

s, a2) ≥

µ(s′, s, a1
||µ||

s, a2)

+ δ(s, s′)

(cid:19)

s′∈S (cid:18)
X

r1(s′, a1

s′ , a2

s′ ),

∀ s ∈ S, a1 ∈ A1(s),

r2(s′, a1

s′ , a2

s′ ),

∀ s ∈ S, a2 ∈ A2(s).

Theorem 14. If (f ∗, g∗) is a stationary Nash equilibrium of a discounted continuous
time stochastic game at some discount rate ˆα > 0 and satisﬁes the conditions N1, N2
and N3, then it will be a BNE.

Proof. The proof follows using the similar arguments as in Theorem 12. The required
discount rate α0 is given by

α0 = min{α1

0, α2

0},

(64)

where αi

0 = (1−βi
0)||µ||
βi
0

, i = 1, 2. The bounds βi

0, i = 1, 2, can be calculated from (65)

βi
0 =

max
s∈S,ai∈Ai(s),ai6=ai
s

{0, βi

s,ai}, i = 1, 2,

where the bounds β1

s,a1 and β2

s,a2 , whenever well deﬁned, are given by

β1
s,a1 =

r1(s, a1, a2

s) −

s′∈S

r1(s, a1, a2

s) − r1(s, a1
µ(s′,s,a1,a2
s)
||µ||

s, a2
s)
+ δ(s, s′)

β2
s,a2 =

r2(s, a1

s, a2) −

P

(cid:16)
s, a2) − r2(s, a1
r2(s, a1
µ(s′,s,a1
s,a2)
||µ||

s′∈S

s, a2
s)
+ δ(s, s′)

(cid:17)

for all s ∈ S, a1 ∈ A1(s), a1 6= a1

s, a2 ∈ A2(s), a2 6= a2
s.

P

(cid:16)

(cid:17)

r1(s′, a1

s′ , a2

s′)

r2(s′, a1

s′ , a2

s′)

(65)

(66)

(67)

,

,

Now, we give an example and show that there exists a stationary Nash equilibrium

which satisﬁes conditions N1, N2 and N3.

Example 15. We consider a 2 states continuous time stochastic game where both the
players have two actions at state 1 and only one action at state 2 , i.e., S = {1, 2},
A1(1) = A2(1) = {1, 2}, A1(2) = A2(2) = {1}. The rewards of both the players and
the transition rates for different combinations of states and actions are summarized in
the Table 5.

26

Table 5: Immediate rewards and Transition Rates

(a) s = 1

(0,0)

(4,4.4)

PPPPPPPP
PPPPPPPP

(-1,1)

(5,6)

(4,5)

(-1,1)

PPPPPPPP
PPPPPPPP

(3,2)

(0,0)

(b) s = 2

PPPPPPPP

(0, 0)

(3,4)

.

The Example 15 can be viewed as a continuous time version of Example 8. We show
that (f ∗, g∗) = ((1, 0), (1, 0)) is a BNE. We ﬁrst show that (f ∗, g∗) is a Nash equilib-
rium at α = 2
3 . From data of the game ||µ|| = 1. Fix g∗ = (1, 0), then ﬁrst player faces
a CTMDP(g∗). The optimal policy of CTMDP(g∗) at α = 2
3 can be computed by solv-
ing the equivalent DTMDP(g∗), deﬁned by (35), at β = 1
1+α = 0.6. Let f1 = (1, 0)
and f2 = (0, 1) be two stationary deterministic policies. Using the above data, the
transition probability matrices induced by f1 and f2 for DTMDP(g∗) are given by,

P 1(f1) =

1
0
(cid:18)

, P 1(f2) =

0
1
(cid:19)

0
0
(cid:18)

.

1
1
(cid:19)

We have

0.6(f1) = [I − 0.6P (f1)]−1¯r1(f1) = (6, 4.5).
u1

(68)

0.6(f2) = [I − 0.6P (f2)]−1¯r1(f2) = (5.7, 4.5).
u1
From (68) and (69) f 1 = (1, 0) = f ∗ is the optimal policy of DTMDP(g∗). Therefore,
f ∗ is the optimal policy of CTMDP(g∗), i.e., f ∗ is best response of g∗. Now, ﬁx
f ∗, then player 2 faces CTMDP(f ∗). To compute the optimal policy of CTMDP(f ∗)
at α = 2
3 , we solve the equivalent DTMDP(f ∗) deﬁned by (36) at β = 0.6. Let
g1 = (1, 0) and g2 = (0, 1) be two stationary deterministic policies for player 2.
Using the above data, the transition probability matrices induced by g1 and g2 for
DTMDP(f ∗) are given by,

(69)

P 1(g1) =

1
0
(cid:18)

, P 1(g2) =

0
1
(cid:19)

0
0
(cid:18)

.

1
1
(cid:19)

We have

0.6(g1) = [I − 0.6P (g1)]−1¯r2(g1) = (6.6, 6).
u2

(70)

0.6(g2) = [I − 0.6P (g2)]−1¯r2(g2) = (6.6, 6).
u2
From (70) and (71), g1 and g2 both are the optimal policies of DTMDP(f ∗) at β = 0.6.
This implies g∗ = g1 is the best response of f ∗ at α = 2
3 . Hence (f ∗, g∗) is a Nash
equilibrium at α = 2
It is easy to check that all the conditions N1, N2, N3 are
3 .
satisﬁed at (f ∗, g∗). Hence, from Theorem 14 (f ∗, g∗) is a BNE. From (64), α0 = 2
3 ,
i.e., (f ∗, g∗) is a Nash equilibrium for all α ∈

(71)

.

0, 2
3

(cid:0)

(cid:3)

27

4 Conclusions

We study BNE in both discrete and continuous time stochastic games. We give counter
examples to show that in general discrete as well as continuous time stochastic games
need not possess a stationary BNE. We show the existence of a BNE for SC-AR
stochastic games. For general stochastic games we give two different sets of condi-
tions that together are sufﬁcient for a Nash equilibrium to be a BNE. We give few
examples which show that the Nash equilibria satisfying the proposed sufﬁcient condi-
tions indeed exist.

References

[1] K. Avrachenkov, L. Cottatellucci, and L. Maggi. Algorithms for uniform opti-
mal strategies in two-player zero sum stochastic games with perfect information.
Operations Research Letters, 40(1):56–60, 2012.

[2] D. Blackwell. Discrete dynamic programming. The Annals of Mathematical

Statistics, 33(2):719–726, 1962.

[3] J. Filar and K. Vrieze. Competitive Markov Decision Processes. Springer, New

York, 1997.

[4] J. A. Filar, T. A. Schultz, F. Thuijsman, and O. J. Vrieze. Nonlinear program-
ming and stationary equilibria in stochastic games. Mathematical Programming,
50(1):227–237, 1991.

[5] A.M. Fink. Equilibrium in a stochastic n-person game. Journal of Science of

Hiroshima University Series A-I Math, 28(1):89–93, 1964.

[6] D. Gillette.

In W. Tucker
Stochastic games with zero stop probabilities.
M. Dresher and P. Wolfe, editors, Contributions to the theory of games, Annals of
Mathematics Studies 39, pages 179–187. Princeton University Press, Princeton,
NJ, 1957.

[7] H. Gimbert and W. Zielonka. Blackwell-optimal strategies in priority mean-
payoff games. In M. Napoli A. Montanari and M. Parente, editors, Proceedings
of GandALF, EPTCS 25, pages 7–21, 2010.

[8] X. Guo and O. Hern´andez-Lerma. Nonzero-sum games for continuous-time
markov chains with unbounded discounted payoffs. Journal of Applied Proba-
bility, 42(2):303–320, 2005.

[9] X. Guo and O. Hern´andez-Lerma. Continuous-Time Markov decision processes

theory and applications. Springer, 2009.

[10] A. Neyman. Continuous time stochastic games. Center for the Study of Rational-

ity, DP-616:1–68, 2012.

28

[11] M.L. Puterman. Markov Decision Processes: Discrete stochastic dynamic pro-

gramming. Wiley, New York, 1994.

[12] R.F. Serfozo. An equivalence between continuous and discrete time Markov de-

cision processes. Operations Research, 27(3):616–620, 1979.

[13] Vikas Vikram Singh, N. Hemachandra, and K. S. Mallikarjuna Rao. Blackwell
optimality in stochastic games. International Game Theory Review, 15(4), 2013.

[14] M. Takahashi. Equilibrium points of stochastic non-cooperative n-person games.
Journal of Science of Hiroshima University Series A-I Math, 28(1):95–99, 1964.

29

