Published at 2020 International Symposium on Mixed and Augmented Reality (ISMAR2020)

Collaborative Augmented Reality on Smartphones
via Life-long City-scale Maps

Lukas Platinsky

Michal Szabados

Filip Hlasek

Ross Hemsley

Luca Del Pero

Andrej Pancik

Bryan Baum

Hugo Grimmett

Peter Ondruska*

0
2
0
2

v
o
N
0
1

]

V
C
.
s
c
[

1
v
0
7
3
5
0
.
1
1
0
2
:
v
i
X
r
a

Figure 1: Shared and collaborative augmented reality experience on mobile devices, powered by high-precision localisation in
city-scale 3D maps.

ABSTRACT

In this paper we present the ﬁrst published end-to-end production
computer-vision system for powering city-scale shared augmented
reality experiences on mobile devices. In doing so we propose a
new formulation for an experience-based mapping framework as an
effective solution to the key issues of city-scale SLAM scalability,
robustness, map updates and all-time all-weather performance re-
quired by a production system. Furthermore, we propose an effective
way of synchronising SLAM systems to deliver seamless real-time
localisation of multiple edge devices at the same time. All this in
the presence of network latency and bandwidth limitations. The
resulting system is deployed and tested at scale in San Francisco
where it delivers AR experiences in a mapped area of several hun-
dred kilometers. To foster further development of this area we offer
the data set to the public, constituting the largest of this kind to date.

Keywords: Computer vision, Augmented reality, Structure from
motion, Large-scale SLAM

Index Terms: Computing methodologies—Artiﬁcial intelligence—
Computer vision—Tracking and Reconstruction; Computing
methodologies—Mixed / augmented Reality

1 INTRODUCTION

Most of the currently available production Augmented Reality (AR)
today are single-user experiences running at a small scale. For
example, in games such as Pokemon Go, a user can see AR content
on their edge device (e.g. smartphone or AR glasses), but a second
user cannot see the same content in the same place on their own
device. This fundamental limitation is due to the each device’s

*This work was conducted at Blue Vision Labs (now part of Lyft Level

5). To contact authors please email: peter@ondruska.com

pose being estimated in its own coordinate systems different to
each other, by the simultaneous localisation and mapping (SLAM)
system running on-board. Additionally, these are often limited to
small scale (i.e. one room), constant visual conditions, and do not
support large-scale applications such as outdoor AR navigation.

The properties of SLAM systems are well known [11, 32, 34], but
building production-grade, city-size, and multi-agent solutions poses
unique challenges due to the high requirements for system robust-
ness, scalability, multi-weather and time-of-the day performance,
and map updates combination of which was never explored together.
In this work we present the ﬁrst published end-to-end production
system that allows city-scale robust localisation of multiple devices
in the same shared coordinate system for building shared and collab-
orative augmented reality experiences (as shown in Figure 1, and the
accompanying video 1). In doing so we propose and exploit a new
experience-based mapping framework for constructing large-scale
all-time all-weather maps, keeping them perpetually updated and
supporting localisation of edge devices over a mobile network. In
particular, the contributions of this paper are ﬁve-fold:

1. The end-to-end design of a production-grade system for aug-

mented reality at scale.

2. An experience-based mapping framework, which is an effec-
tive solution to the key issues of all-time all-weather city-scale
SLAM scalability.

3. A novel way of localising multiple edge devices in the same
coordinate space, in real time over a mobile network with
latency and limited bandwidth.

4. A comprehensive evaluation of map-building and AR localisa-

tion performance on a city-scale dataset.

5. A new city-scale dataset 2.

1Video of the deployed system is at https://youtu.be/tXwVg2S9wuY.
2To request access to the dataset please visit www.bluevisionlabs.org

 
 
 
 
 
 
Published at 2020 International Symposium on Mixed and Augmented Reality (ISMAR2020)

2 RELATED WORK

Some previous work tackles global localisation without using a map
of the environment, for example by combining GPS and inertial sen-
sors (e.g. [30]), or GPS and visual odometry (e.g., in [15]). However,
in our tests neither of these options met the accuracy requirements
for providing immersive real-time AR experiences due to large GPS
location error, particularly in urban settings with tall buildings (ur-
ban canyons). To obtain the required accuracy, we have used a
visual mapping and localisation approach providing a more accurate
localisation signal than GPS. This approach requires constructing a
large-scale global map of the environment before the AR experiences
can happen.

Much work has been done on building mapping and localisa-
tion algorithms. Many of the classical systems [7, 8, 25] have been
achieving impressive results with single-agent real-time systems per-
forming mapping and localisation at the same time (SLAM). There
are also several impressive solutions that speciﬁcally provide the
ability to build a map ﬁrst and then use it to localise allowing also for
multi-agent solutions [10, 20, 31]. These systems, however, typically
operate in a limited geographic area due to the memory constraints
and algorithmic complexity of the underlying structure-from-motion
(SfM) problem. Their map-building part is often limited to a sin-
gle machine and cannot be directly used with the amounts of data
processed by the solution proposed in this paper.

Building visual maps at a scale needed for the described use cases
has been addressed in [2,14,39]. Compared to these approaches, our
method is designed to optimally exploit the multi-experience nature
of the data collection leading to approximately linear complexity
of map-building process, fast map updates, and extreme robustness
to outliers and SfM failures arising from difﬁcult visual conditions.
Unlike the previous solutions, the proposed system is signiﬁcantly
more robust to the variances in quality of the input data, allowing for
crowd-sourced approach for data collection. Moreover, we do not
stop at the map-building stage and provide a large-scale multi-agent
localization framework as well.

Image-based location retrieval is a topic with a lot of interest
and it was explored, for example, in [6, 9, 12, 13, 16, 18, 27, 28, 33,
37]. Many of the approaches focus on limited geographic areas or
on a limited set of lighting conditions. Several impressive works
focusing on providing localisation of mobile devices presented in
[21,35,36] suffer from similar vices, allowing for experiences only in
signiﬁcantly limited areas and we show several orders of magnitude
improvement in the size of the area supported in our experiments.
We present a multi-agent system where many devices continuously
interact and localise within a single, unbounded map under varying
visual conditions.

Vision systems degrade in changing visual conditions due to
increasing perceptual difference (i.e. day vs night) between the
map and localisation images. Robust, all time-time, all-weather

performance is key for production systems and was considered
in [3, 17, 29] by building robust localisation feature representations.
In contrast, we take inspiration from experience-based navigation
framework [5] where robust multi-weather performance is achieved
by repeatedly collecting data along a route in varied conditions
and building one multi-weather map. By doing so the map covers
all possible visual conditions resulting into robust performance.
This setup is uniquely beneﬁcial for building productions systems,
where more data can be collected as required to achieve the desired
performance of the system. In particular, in this work we focus on
the ability to combine these experiences, collected at scale by a ﬂeet
of vehicles, robustly fuse them to form a single map, and efﬁciently
update the map when more data are available.

Finally, unlike all previous works that addressed only parts of the
technical problem, this is the ﬁrst work that ties all the components
together and at scale into one production-grade AR system. We ac-
knowledge imperfections in the current algorithms and our collected
data, and rather than attempting to ﬁnd a silver bullet for each part of
the system, we acknowledge the imperfections and build a redundant
and robust solution around it.

We are also releasing a new large-scale mapping and localisation
dataset from San Francisco, comprising millions of frames and
videos. It is the largest publicly-available dataset of this kind, 5x
larger than the next largest similar dataset [4].

Main difference - many orders of magnitude more. Focus is
not on solving each individual part perfectly. We accept imperfect
nature of algorithms and design the whole end-to-end approach with
redundancy and solution focused on manipulating several orders of
magnitude larger data than the previous known approaches.

3 SYSTEM OVERVIEW

We propose a computer vision system that enables collaborative
augmented reality experiences. At its core lies a map-based localisa-
tion system outlined in Figure 2 that can track the global pose of an
edge device (e.g. smartphone or AR glasses) pM
over time in the
t
shared coordinate space of the 3D map M of the environment. This
leverages the approximate location pgps
given by the device GPS,
and the image history of what it sees through its camera I1...t .

t

The map is constructed from a large set of individual data col-
lection logs called experiences {I 1 . . . I N }. Each experience com-
prises a set of images I i = {Ii
1 . . . Ii
O} and GPS points collected
during a single session at particular time of day / weather condition
by a vehicle equipped with multiple cameras and an INS system. It
can range in length from several meters to up to a few kilometers.
The ability to build maps from very large number of experiences is
the key to achieving high performance and robustness (see Sec. 4).
We use a ﬂeet of cars to collect experiences that are used in map-
building to efﬁciently control and improve the performance of the
resulting system.

Figure 2: The system diagram for the proposed system. A large-scale 3D map is computed from a vehicle-based image collection and used to
localise camera-equipped edge devices for AR experiences over a mobile network.

Large-scale 3D mapVehicle-basedimage collectionDistributedstructure-from-motionCloud localisationserviceOver-the-air SLAMsynchronisationCollaborativeAR experiencePublished at 2020 International Symposium on Mixed and Augmented Reality (ISMAR2020)

Figure 3: The city-scale map built for supporting AR experiences. The subset of the map we evaluated AR performance (middle) consists of 4249
submaps and was computed on a cluster of 500 CPUs over period of 91 hours.

The map contains billions of landmarks, and due to its size cannot
be stored on the edge device. Instead, the device runs a visual-
inertial odometry (VIO) system locally. This system is aligned with
the map through the local-to-global coordinate transform Tvo→M ,
recovered by estimating the pose with respect to the map of a subset
of the individual images captured by the device. Due to map size
and the need to keep it perpetual updates, this takes place in the
cloud. In Section 5 we describe how we dealt with network latency
and limited network bandwidth.

This localisation system is then augmented with persistent content
storage and real-time pose sharing, enabling powerful collaborative
AR experiences. We describe details of these in Section 6. In what
follows we present the core architecture of the system, while we
discuss smaller implementations details in 6.

4 LARGE SCALE EXPERIENCE-BASED MAPPING
In this section we present our system for building a large scale map
M from the set of collected experiences I1 . . . IN . It is designed
to achieve three key goals:

1. Localisation performance: reliable all-time, all-weather locali-

sation

2. Map robustness: ensure map quality over a large geographic

area in the face of SfM failures

3. Map freshness: efﬁcient map updates by adding more data

when needed.

The algorithm consists of four components: (1) data splitting, (2)
submap building, (3) submap veriﬁcation, and (4) submap fusion
with map update. We now provide an overview before presenting
each component in detail.

The algorithm starts with an empty map M and then iteratively
adds new experiences to it. For each experience, we group all its
images into smaller overlapping subsets (data splitting), and then
run SfM on each subset independently, producing submaps (submap
building). This is the most expensive part of the algorithm, but we
parallelise it on a cluster of CPUs. Therefore, there is no limit to
the size of input experiences. Next, SfM failures are identiﬁed, and
these ‘broken’ submaps are discarded. The remaining submaps are
fused into a single, global map. The entire map can be built from
scratch in approximately linear time complexity with the amount
of data, or alternatively, experiences can be added one-by-one. The

resulting map M comprises K geo-aligned submaps M1...K, each
containing the camera poses p of the input images, and a set of 3D
localisation landmarks x such that Mi = {p1...M, x1...L}.

1. Data splitting. We bound the computational complexity of
submap building (step 2) by grouping the images in an experience
(which can be potentially numerous) into consistently-sized subsets
of images. The ﬁrst image in the experience is assigned to a subset,
which is grown in size until it contains 1,000 images (we enforce
subsets to have a circular shape, with a minimum radius of 20m).
This is done using a greedy line-sweep algorithm over the 2D GPS
positions of each image (see Figure 4). In areas with large image
density, we randomly sub-sample images, to ﬁt into the 1,000 limit.
These values were chosen empirically to allow submaps to cover
enough geographical area (making the grouping robust to GPS errors,
which are severe in urban canyons), yet be small enough for efﬁcient
submap building. Lastly, for each submap in an experience we add
images randomly chosen from neighbouring submaps, which is key
for submap fusion (step 3). To enable the fusion of submaps across
experiences, we also add randomly-selected images from the same
geographic location but different experiences. While this increases
the computation required in step 2 (most images are in multiple
submaps), this greatly simpliﬁes the fusion step.

Figure 4: The line-sweep algorithm that splits data collected during
one mapping session into submaps of size up to 1,000 frames.

Published at 2020 International Symposium on Mixed and Augmented Reality (ISMAR2020)

2. Submap building. Next, incremental SfM [23] is run on
each subset independently, relying on a standard pipeline of feature
extraction, matching, track creation and bundle adjustment. This
recovers the poses of the images and the 3D positions of landmarks,
by minimising the re-projection error:

C (x, p) = ∑
i, j

(zi, j − π(xi, p j))2 + λ ∑

(pi − pgps

i

)2

(1)

i

where zi, j is the position of the i-th feature in the j-th image, and
π(xi, p j) is re-projection of landmark xi using pose p j. The last
term is a GPS prior, weighted by λ . Thanks to the small size of the
subsets, each submap can be built on a single CPU. Examples of
submaps are shown in Figure 5.

3. Submap veriﬁcation. Due to challenging conditions such
as occlusions, moving objects, or camera failures, SfM can output
incorrect or warped 3D maps as shown in Figure 5. To detect this
we check whether the reconstructed poses agree with the relative
orientations of consecutive images as measured by the INS. Submaps
are also discarded if the reconstructed gravity vectors deviate too
much from the INS deviate measurement, or if the positions of
consecutive images violate the kinematic constraints of a simple
vehicle motion model. Discarded submaps are not used for map
fusion or update.

4. Submap fusion & map update. Lastly, new submaps and
existing submaps in M are fused together rigidly. For each of K
submaps, we compute a 7DoF transformation Tk that minimises the
displacement between images that occur in multiple submaps (i.e.,
in the coordinate system of the fused map, the positions of the same
image in different submaps must coincide):

Figure 6: Detail of the geo-aligned map shown on top of satellite
picture. Blue: triangulated landmarks, green: camera poses, red:
GPS priors.

C (T ) = ∑
i

∑
k,l

(Tk pk

i − Tl pl

i)2 + λ ∑

(Tk pk

i − pgps
i

)2

(2)

i,k

where pk
pairs of submaps containing Ii.

i is the pose of image Ii in submap Mk and k, l iterate over

The result is a geo-aligned map (Fig. 3). Figure 6 shows the
map’s alignment with geo-coordinates due to the combination of
geometric and GPS terms.

Despite its simplicity, the overall mapping framework is remark-
ably robust. As further discussed in Section 6, it robustly builds
highly accurate 3D maps even if the SfM has only a 89% success rate
at the individual submap level (step 2). Moreover, both map com-
pleteness and localisation performance improve as new experiences
are fused with the map, ﬁlling any low-density regions.

Removing data from the map, which might be desirable in the
case of environmental change or manually detected map errors, is
equally simple. The affected submaps are simply removed and we

Figure 5: Examples of submaps the map is composed of. On the
left a submap where SfM succeeded, and on the right one where
structure-from-motion failed due to difﬁcult visual conditions. Failed
submaps are discarded prior to submap fusion.

Figure 7: The process of continuously localising edge device in the
map over the network. Recently captured images are transmitted over
the network to determine their global position and compute optimal
offset of the local visual-odometry frame of reference.

rerun the fusion steps on the remaining submaps. Note that we
do not perform any global bundle-adjustment to reﬁne positions of
landmark after step 4, unlike [40]. We found that this only marginally
improves map accuracy, and thus does not warrant the additional
computational cost for the AR use case.

5 REAL-TIME SLAM SYNCHRONISATION

In this section we describe how to continuously localise an edge
device in the large-scale multi-experience map we described in the
previous section.

Given the map is stored in the cloud direct image-based localisa-
tion impossible. Instead, each device runs a visual-inertial odometry
system that continuously tracks its own pose pvo
t over time in a
local coordinate frame and simultaneously estimates a global-to-
local coordinate transform Tvo→M . This is achieved by (1) sending
recently captured keyframes over the mobile network to the cloud
service containing the map, (2) the cloud service determining the
edge device’s global pose by performing image-based localisation,
and (3) sending the global localisation result back to the edge device,
as shown in Figure 7. The optimal transform T ∗
vo→M is given by

t1t2t3voMI1I2I3Tvo   M→Published at 2020 International Symposium on Mixed and Augmented Reality (ISMAR2020)

minimising

C (T ) = ∑
It

τ −t (cid:107)pM (It ) − T pvo

t (cid:107)δ ,

(3)

over the history of transmitted images It where pM (It ) is the result
of the localisation of image It in map M , pvo
is the local pose of the
t
image and τ is a time decay factor that favours recent measurements.
We use robust Huber loss to mitigate outlier localisation. The current
real-time pose is then given as

pM
t = T ∗

vo→M pvo
t

.

(4)

Recall that the map is composed of submaps M1...K. In order
to handle a very large map, the submaps are stored together based
on GPS proximity, and the image is queried against all submaps
within a GPS radius. The query result with the largest number of
inlier feature matches is then used. This process typically takes
approximately one second per image, and is independent of the size
of the map.

The accuracy of visual localisation performance depends on a
number of factors and the localisation error is approximately given
by

ε = εmap + εloc + εvo

(5)

∆t
rloc

where εmap, εloc, εvo is respectively the error of the map, localisation
and odometry drift over localisation request time interval ∆t and rloc
is the localisation success rate %. As discussed in Section 6 this is
usually in order of few centimeters.

Figure 8: The dataset we collected in San Francisco and used to
evaluate the system. Blue: data collected by vehicles to build the map,
Red: data collected by pedestrians to test localisation performance.
The area with both vehicle and pedestrian coverage is released to the
public.

Figure 9: Samples from the dataset. Top: a view from 8 cameras
mounted on the vehicle. Bottom: data seen by a mobile phone held
by pedestrian during localisation.

6 EXPERIMENTS
In this section we evaluate the proposed system and outline how
it can be used to AR usecases. In particular, we are interested in
(a) the experience-based mapping system’s ability to produce large-
scale maps, and (b) the edge-device localisation performance for
multi-user, production AR.

6.1 Dataset
We have collected two datasets to build a map and evaluate the
system. We report the performance of the system on the subset
of the data where these two datasets overlap, and release it to the
public as a part of this work. The used dataset is several orders
of magnitude larger than data used for experimentation by similar
end-to-end multi-agent mapping and localisation systems (e.g., [31]),
rendering any state-of-the-art comparisons infeasible.

Map-building dataset. To build the map of the environment we
collected 27M images in San Francisco using a ﬂeet of vehicles
carrying camera and GPS-equipped devices over a period of around
3 months. Subset of 7M (equivalent of roughly 1,000 miles driven)
is being released and used for evaluation.

Each vehicle used for the 7M dataset was equipped by 8 rolling-
shutter cameras in rosette conﬁguration as shown in Figure 9. Each
camera has a 70◦ Field of View (FOV) providing a 360◦ view around
the vehicle with approximately 25◦ overlap between neighbouring
cameras. The vehicles traversed the city, covering each street multi-
ple times and thus creating several experiences per street. Experi-

Stage
Data splitting
Feature detection
Feature matching
SfM
Submap fusion
Total

# CPUs
1
300
300
500
1

Per submap
-
1 hour
1.5 hours
4 hours
-

Total
1 hour
18 hours
27 hours
44 hours
1 hour
91 hours

Table 1: The map-building compute time broken down by each stage,
for a map consisting of 7 million frames split into 4249 submaps.

Published at 2020 International Symposium on Mixed and Augmented Reality (ISMAR2020)

Area
1+ passes
4+ passes

Attempted
3000 miles
1517 miles

Reconstructed
1950 miles
1384 miles

Yield
66%
91%

Table 2: Map yield by number of passes. Map yield is deﬁned as a
percentage of the streets where we have successfully reconstructed
batches against where we attempted to construct the map.

Figure 10: Map coverage as a function of more experiences from the
area. Left: top-down view of a challenging area of the city after a
single data collection, right: the same area after adding more data to
the map.

ences come from different times of the day to cover different lighting
conditions and improve the robustness of the resulting system. Most
of the dataset was collected under sunny or overcast conditions. It
contains images, IMU data at 100Hz, and GPS at 1Hz. The cam-
eras are rigidly mounted but slightly different for different collect
vehicles as this simpliﬁes data collection operations. Exact camera
intrinsics and extrinsics are determined as part of the structure-from-
motion process and the cameras are not synchronised to further
decrease the operational complexity.

AR localisation dataset.

In order to evaluate the map’s per-
formance for pedestrian AR use cases, we collected 1009 short
video snippets of handheld data together with device’s visual-inertial
odometry log modelling user using the system. A human operator
would stand close to the road (within 2m of the curb) and slowly
looks around, to cover roughly a 180◦ FOV, and then walks roughly
20m along the sidewalk. We have chosen this evaluation because for
any AR experience to give a good user experience, the edge device
must quickly re-localize against the map, and with minimal user
interaction.

6.2 Map building performance

We processed data collected by the mapping vehicles and build
the map shown in Figure 3. We split the data in the evaluated
area into 4249 submaps, with an average diameter of 50m, and
20m overlap between neighbouring submaps. The success rate
of building submaps was 89%. Failures were caused by difﬁcult
visual lighting conditions (strong sun), large numbers of moving
objects, or operational issues with data collection hardware. We
also deliberately set a high threshold for the automated and manual
checks on the submaps, resulting in many being discarded.

Implementation details. For our mapping pipeline, we use SIFT
features [19] and OpenMVG’s incremental SfM implementation
[23] when building submaps. We do not perform any form of map
simpliﬁcation, since our submaps comfortably ﬁt into our servers’
memory, though this would be an interesting optimisation. We
separately experimented with masking out dynamic agents in the
images, but we do not use it for these experiment as it did not bring
any improvement (parked cars can provide good features in map

building and thanks to combining map localization and odometry
proved enough to reliably ﬁlter outliers in localization). We do not
perform further bundle adjustment for fusing the submaps as it would
be infeasible given the overall map size, and this choice proved to
acheive enough localisation accuracy to support AR applications
(our goal). In our production system, before deploying a new map we
perform a light-weight manual inspection in a web viewer to detect
obvious errors (typically larger geometric disagreements between
submaps in the same region which are easy to spot). After removing
these submaps we rerun the submap fusion step, and deploy.

Computation time. The computation took more than 35,000
hours distributed over 500 CPUs and produced a 700 GB map. For
run-time and parallelisation of the different steps, see Table 1. Most
of the time is spent in the feature matching and bundle adjustment
steps, which run in quadratic and cubic time in the number of images
in a submap respectively. A map update on this map consisting of
building one or multiple additional submaps and fusing them in
takes approximately 6 hours.

The impact of experiences on map coverage. Figure 10 quali-
tatively shows improvement of map coverage as we add experiences.
With an average of only one mapping experience per location, the
map displays missing areas and would be unﬁt for localisation on
those streets. As seen in Table 2, this signiﬁcantly improves as
more experiences are added, despite high failure rate of individual
submaps.

6.3 Localisation performance
We measured the performance of edge device localisation in the map.
Figure 13 shows typical localisation performance for a pedestrian’s
smartphone as they walk down the road, as compared with GPS. The
visual localisation system is signiﬁcantly more accurate than GPS
localisation. We used 1,009 recordings from the AR localisation
dataset described above. The key localisation metric used is local-
isation rate, deﬁned as the fraction of images able to localise. We
complement it with two accuracy metrics: localisation consistency
and localisation accuracy.

Implementation details. For the server-side localization of sin-
gle images we used SIFT features [19] and the localization tool from
OpenMVG [24]. We modiﬁed the approximate nearest neighbour
lookup implementation to reduce the size of the data and speed of
loading it into memory to ﬁt our submap size. This was necessary
to guarantee server responses within around 1 second at a stable
4G connection. For odometry, we experimented with VINS [26]
and ORBSLAM [25], but we ended up using a commercial version
(ARKit [1]), as it performed best for our handheld AR use case and
provided the most robust experience on the tested smartphones.

Figure 11: A histogram of metric error of localised pose, compared
with a ground-truth from a high-accuracy sensor, over a 20m distance.

012345error [m]051015104Published at 2020 International Symposium on Mixed and Augmented Reality (ISMAR2020)

Figure 12: Examples of different augmented reality experiences we built powered by the proposed localisation of edge devices held by pedestrians
or equipped on vehicles: Shared and persistent AR content visible by all users at the same location, avatar-augmented device pose, AR navigation
and collaborative AR games.

Localisation rate. We evaluated 37,727 localisation requests that
originated from the challenging AR dataset recordings simulating a
real use of the system across the city. Out of those, 73% were suc-
cessfully localised. As these poses can be computed from erroneous
matches we have also calculated that after further ﬁltering of outlier
localizations in the system, 61% of the requests produce a coherent
result when combined with the visual-inertial odometry running on
the client. This rate is enough to provide a smooth user experience.

Localisation consistency. We measured the positional discrep-
ancy of the server-side localisation service against the visual odome-
try running on the device before fusing them to be 30cm. We have
tuned the system to bias heavily towards false positives in server-side
localizations. The synchronisation algorithm running on the device
can pick out the correct relocalisations and discard the high-variance
noise.

Localisation accuracy. To evaluate positional accuracy against
a ground truth we collected a separate dataset comprising 2h of data.
It was obtained by attaching an edge device to a self-driving-capable
vehicle with high-performance odometry, and driving it around the
city. We repeatedly computed the difference between raw localised
poses after the device traveled distance of 20m and compared it
against the groundtruth odometry. Figure 11, shows result of this
relative error [38] comparison. The mean error is 42cm (2%) and
median error is 18cm (1%). This accuracy is sufﬁcient for most AR
usecases.

The impact of experiences on localisation performance. Sim-
ilarly to [5], in Table 3 we observe increased localisation perfor-
mance as data collected from varying times of day and weather are
combined. To measure the impact on localisation performance we
collected mapping data and corresponding edge device localisation
recordings in one area of the map at different times of day. We
evaluate the localisation rate as a function of the amount of data used
for map-building.

Computation time in Figure 4 summarises timing of individual
stages of a single localisation request. On average the localisation
requests take 970ms on 4G network and 1700ms on 3G network.

Figure 13: A comparison of typical GPS localisation accuracy of (red)
and the proposed visual localisation (blue) for a pedestrian holding
a smartphone while walking down the street. Visual localisation is
signiﬁcantly more accurate.

6.4 AR system

Our system can be run in parallel on multiple edge devices jointly
localising in the same coordinate space of the map M . In order to

Published at 2020 International Symposium on Mixed and Augmented Reality (ISMAR2020)

Mapping collections in an area
1 day-time experience
4 day-time experiences
6 day and night experiences
7 day and night experiences

Localization rate
69.5%
81.5%
96.6%
97.3%

Table 3: Localisation performance as a function of more experiences
from the area. With more observations the localisation rate across
different lighting conditions increases.

Stage
Image transfer
Feature detection
Feature matching
Triangulation

Timing (ms)
200 (4G) – 1,000 (3G)
250
500
20

Table 4: Timing of different stages of localising one image from the
cloud device in the map stored in the cloud.

allow users to create and interact with shared persistent content and
with each other, we augmented the base localisation system with
two services:

Persistent content storage allows users to create and store the
coordinates of AR content in a shared database, making it immedi-
ately visible to other users.

Real-time pose sharing exchanges current poses between ac-
tive devices. This is important for interaction and allows you to,
say, show an avatar on top of players’ current locations, or play
collaborative games.

Figure 12 and the accompanied video showcase example AR
experiences we built using these methods. Thanks to the shared
coordinate system, all users see content at exactly the same place
and can interact with each other.

6.5 Limitations and failure cases

We observed the system works robustly under usual lightning condi-
tions. Here we summarise the most signiﬁcant causes we observed
to be limiting its performance:

Multiple passes required for mapping. Our system achieve
robustness by exploiting redundancies in the data - this was a delib-
erate decision. We embraced the stochastic nature of the individual
methods in the overall system (each step can fail on subsets of the
data), but redundancy in data allows us to handle local algorithmic
failures (e.g. while building a submap from an experience with
SfM can fail due poor illumination, occlusions, etc., a successful
submap from another experience in the same area can ﬁll the gap).
The drawback is that we typically require a minimum amount of
data in every area: note how mapping performance degrades as we
go from 4 passes to 1 pass (this is the same as an experience) in
Table 2, with localisation rate showing a similar trend in Table 3.
However, this is not a problem in our setup due to the scalability
of our algorithms, and the simplicity of our sensor installation and
data collection processes which allows us to collect large amounts
of data efﬁciently.

Mobile network connection. One of the biggest limitations
we observed in practice is the requirement of a stable mobile data
connection during its use. We observed common sudden drops in
mobile bandwidth when testing in crowded areas of the city. This
has a large noticeable impact on initalisation of the system and also
on the positional error accumulated due to dead-reckoning drift of
odometry. This can be solved by doing the map relocalisation on
the edge device. While possible, it would require optimising the
algorithm’s runtime, using quicker (most likely binary rather than
ﬂoat) feature descriptors, and simpliﬁcation of the map to reduce

Figure 14: Typical localisation failure cases. These include obstructed
sidewalks, user pointing the phone upwards, repetitive structures and
reﬂections.

its size to serve it to the device, which will also need to cache parts
of the map. This optimisation is left as future work for the next
versions of the system.

Viewpoint change too large. While the system is resilient to
localization from some novel viewpoints, the second most common
issue comes from inability of the system to match images due to
occlusion, or when there is a large difference in viewpoint. The
latter is mostly noticeable in areas where there is a larger gap be-
tween the sidewalk (where the edge device is held) and the street
(where the mapping collections happens). Thanks to the submap
implementation and focus on data redundancy, one way to tackle
this is to also use data collected from the sidewalks with a smaller
hand-held camera rig for mapping (additionally to our current car
mapping collect). This works well, however, pedestrian data col-
lection can become a bottleneck when scaling to larger areas. An
alternative is to explore methods like ASIFT [22] known to increase
the robustness to viewpoint changes.

Other failure cases. Figure 14 displays some of the failure cases
mentioned above, such as obstructed sidewalks, and a few more
failures due to excessive vertical tilting, reﬂections, and repetitive
textures. These can prevent or slow down an initial localisation in
some areas (e.g. where recent changes happened due to construc-
tions, or the user is pointing their phone towards the sky), but we
observed that their impact on the overall experience is mitigated by
the VIO system.

7 CONCLUSIONS

We have presented the ﬁrst published end-to-end production system
for building collaborative augmented reality experiences at city-scale.
The presented system can be extended in many ways, for example,
by automatically updating the map when detecting changes in the
environment, potentially with data transmitted by the edge devices.
We believe that , together with the released dataset, this work will
enable the community to address these and other challenges, and
accelerate the development of AR systems towards seamless and
ubiquitous experiences.

ACKNOWLEDGMENTS

This work was done thanks to the hard work and support of the Blue
Vision Labs team: Ben Haines, Bryon Shannon, David Evans, Ed
Dingley, Fady Kalo, Filippo Brizzi, Gabriel Nica, Gabriele Angeletti,
George Thomas, Giacomo Dabisias, Guido Zuidhof, Haoyue Zhu,
Ivan Katanic, James Close, Jesse DaSilva, Karim Shaban, Long
Chen, Lorenzo Peppoloni, Lucas Chatham, Matej Hamas, Mimosa
Nguyen, Miranda Aperghis, Owen Parker, Robert Kesten, Sina
Nickdel, Stepan Simsa, Suraj Mannakunnel Surendran, Yerzhan
Utkelbayev.

Published at 2020 International Symposium on Mixed and Augmented Reality (ISMAR2020)

REFERENCES

[1] ARKit. https://developer.apple.com/augmented-reality/

arkit/.

[2] S. Agarwal, Y. Furukawa, N. Snavely, I. Simon, B. Curless, S. M. Seitz,
and R. Szeliski. Building rome in a day. Commun. ACM, 54(10):105–
112, Oct. 2011. doi: 10.1145/2001269.2001293

[3] R. Arandjelovic, P. Gronat, A. Torii, T. Pajdla, and J. Sivic. Netvlad:
Cnn architecture for weakly supervised place recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern recognition,
pp. 5297–5307, 2016.

[4] D. Chen, G. Baatz, K. Koeser, S. Tsai, R. Vedantham, T. Pyl-
vanainen, K. Roimela, X. Chen, J. Bach, M. Pollefeys, B. Girod, and
R. Grzeszczuk. City-scale landmark identiﬁcation on mobile devices.
In IEEE International Conference on Computer Vision and Pattern
Recognition, 2011.

[5] W. Churchill and P. Newman. Experience-based Navigation for Long-
term Localisation. The International Journal of Robotics Research
(IJRR), 2013.

[6] M. J. Cummins and P. M. Newman. Fab-map: Appearance-based place
recognition and mapping using a learned visual vocabulary model. In
Proceedings of the 27th International Conference on Machine Learning
(ICML-10), pp. 3–10, 2010.

[7] A. J. Davison, I. D. Reid, N. D. Molton, and O. Stasse. Monoslam:
Real-time single camera slam. IEEE transactions on pattern analysis
and machine intelligence, 29(6):1052–1067, 2007.

[8] J. Engel, T. Sch¨ops, and D. Cremers. Lsd-slam: Large-scale direct
In European conference on computer vision, pp.

monocular slam.
834–849. Springer, 2014.

[9] Y. Feng, L. Fan, and Y. Wu. Fast localization in large-scale environ-
ments using supervised indexing of binary features. IEEE Transactions
on Image Processing, 25(1):343–358, 2015.

[10] C. Forster, S. Lynen, L. Kneip, and D. Scaramuzza. Collaborative
monocular slam with multiple micro aerial vehicles. In 2013 IEEE/RSJ
International Conference on Intelligent Robots and Systems, pp. 3962–
3970, 2013.

[11] J. Fuentes-Pacheco, J. Ruiz-Ascencio, and J. M. Rend´on-Mancha. Vi-
sual simultaneous localization and mapping: a survey. Artiﬁcial Intelli-
gence Review, 43(1):55–81, 2015.

[12] A. Irschara, C. Zach, J.-M. Frahm, and H. Bischof. From structure-
from-motion point clouds to fast location recognition. In 2009 IEEE
Conference on Computer Vision and Pattern Recognition, pp. 2599–
2606. IEEE, 2009.

[13] A. Kendall, M. Grimes, and R. Cipolla. Posenet: A convolutional
network for real-time 6-dof camera relocalization. In Proceedings of
the IEEE international conference on computer vision, pp. 2938–2946,
2015.

[14] B. Klingner, D. Martin, and J. Roseborough. Street view motion-
from-structure-from-motion. In Proceedings of the IEEE International
Conference on Computer Vision, pp. 953–960, 2013.

[15] D. Larnaout, V. Gay-Bellile, S. Bourgeois, and M. Dhome. Vehicle 6-
dof localization based on slam constrained by gps and digital elevation
model information. In 2013 IEEE International Conference on Image
Processing, pp. 2504–2508, 2013.

[16] Y. Li, N. Snavely, D. Huttenlocher, and P. Fua. Worldwide pose
estimation using 3d point clouds. In European conference on computer
vision, pp. 15–29. Springer, 2012.

[17] C. Linegar, W. Churchill, and P. Newman. Made to measure: Bespoke
landmarks for 24-hour, all-weather localisation with a camera. In 2016
IEEE International Conference on Robotics and Automation (ICRA),
pp. 787–794. IEEE, 2016.

[18] L. Liu, H. Li, and Y. Dai. Efﬁcient global 2d-3d matching for camera
In Proceedings of the IEEE
localization in a large-scale 3d map.
International Conference on Computer Vision, pp. 2372–2381, 2017.
[19] D. G. Lowe. Distinctive image features from scale-invariant keypoints.
Int. J. Comput. Vision, 60(2):91–110, Nov. 2004. doi: 10.1023/B:VISI.
0000029664.99615.94

[20] S. Lynen, T. Sattler, M. Bosse, J. A. Hesch, M. Pollefeys, and R. Sieg-
wart. Get out of my lab: Large-scale, real-time visual-inertial localiza-
tion. In Robotics: Science and Systems, vol. 1, 2015.

[21] S. Middelberg, T. Sattler, O. Untzelmann, and L. Kobbelt. Scalable
6-dof localization on mobile devices. In D. Fleet, T. Pajdla, B. Schiele,
and T. Tuytelaars, eds., Computer Vision – ECCV 2014, pp. 268–283.
Springer International Publishing, Cham, 2014.

[22] J.-M. Morel and G. Yu. Asift: A new framework for fully afﬁne
invariant image comparison. SIAM J. Img. Sci., 2(2):438–469, Apr.
2009. doi: 10.1137/080732730

[23] P. Moulon, P. Monasse, and R. Marlet. Adaptive structure from motion
with a contrario model estimation. In Asian Conference on Computer
Vision, pp. 257–270. Springer, 2012.

[24] P. Moulon, P. Monasse, R. Perrot, and R. Marlet. Openmvg: Open
multiple view geometry. In International Workshop on Reproducible
Research in Pattern Recognition, pp. 60–74. Springer, 2016.

[25] R. Mur-Artal, J. M. M. Montiel, and J. D. Tard´os. ORB-SLAM: a
versatile and accurate monocular SLAM system. IEEE Transactions on
Robotics, 31(5):1147–1163, 2015. doi: 10.1109/TRO.2015.2463671

[26] T. Qin, P. Li, and S. Shen. Vins-mono: A robust and versatile monoc-
ular visual-inertial state estimator. IEEE Transactions on Robotics,
34(4):1004–1020, 2018.

[27] T. Sattler, B. Leibe, and L. Kobbelt. Fast image-based localization
using direct 2d-to-3d matching. In 2011 International Conference on
Computer Vision, pp. 667–674. IEEE, 2011.

[28] T. Sattler, B. Leibe, and L. Kobbelt. Improving image-based local-
ization by active correspondence search. In European conference on
computer vision, pp. 752–765. Springer, 2012.

[29] T. Sattler, W. Maddern, C. Toft, A. Torii, L. Hammarstrand, E. Stenborg,
D. Safari, M. Okutomi, M. Pollefeys, J. Sivic, et al. Benchmarking 6dof
outdoor visual localization in changing conditions. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition, pp.
8601–8610, 2018.

[30] G. Schall, D. Wagner, G. Reitmayr, E. Taichmann, M. Wieser,
D. Schmalstieg, and B. Hofmann-Wellenhof. Global pose estima-
tion using multi-sensor fusion for outdoor augmented reality. In 2009
8th IEEE International Symposium on Mixed and Augmented Reality,
pp. 153–162, 2009.

[31] T. Schneider, M. T. Dymczyk, M. Fehr, K. Egger, S. Lynen,
I. Gilitschenski, and R. Siegwart. maplab: An open framework for
research in visual-inertial mapping and localization. IEEE Robotics
and Automation Letters, 2018. doi: 10.1109/LRA.2018.2800113
[32] H. Strasdat, J. M. Montiel, and A. J. Davison. Visual slam: why ﬁlter?

Image and Vision Computing, 30(2):65–77, 2012.

[33] L. Sv¨arm, O. Enqvist, F. Kahl, and M. Oskarsson. City-scale localiza-
tion for cameras with known vertical direction. IEEE transactions on
pattern analysis and machine intelligence, 39(7):1455–1461, 2016.

[34] T. Taketomi, H. Uchiyama, and S. Ikeda. Visual slam algorithms: a
survey from 2010 to 2016. IPSJ Transactions on Computer Vision and
Applications, 9(1):16, 2017.

[35] J. Ventura, C. Arth, G. Reitmayr, and D. Schmalstieg. Global localiza-
tion from monocular SLAM on a mobile phone. IEEE Transactions on
Visualization and Computer Graphics, 20(4):531–539, apr 2014. doi:
10.1109/tvcg.2014.27

[36] J. Ventura and T. H¨ollerer. Wide-area scene mapping for mobile
visual tracking. In 2012 IEEE International Symposium on Mixed and
Augmented Reality (ISMAR), pp. 3–12, 2012.

[37] B. Zeisl, T. Sattler, and M. Pollefeys. Camera pose voting for large-
scale image-based localization. In The IEEE International Conference
on Computer Vision (ICCV), December 2015.

[38] Z. Zhang and D. Scaramuzza. A tutorial on quantitative trajectory eval-
uation for visual (-inertial) odometry. In 2018 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS), pp. 7244–7251.
IEEE, 2018.

[39] S. Zhu, T. Shen, L. Zhou, R. Zhang, J. Wang, and T. Fang. Parallel
structure from motion from local increment to global averaging. 2017.
[40] S. Zhu, R. Zhang, L. Zhou, T. Shen, T. Fang, P. Tan, and L. Quan. Very
large-scale global sfm by distributed motion averaging. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 4568–4577, 2018.

