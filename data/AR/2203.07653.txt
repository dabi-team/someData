Generalized but not Robust? Comparing the Effects of Data Modiﬁcation
Methods on Out-of-Domain Generalization and Adversarial Robustness

Tejas Gokhale∗

Swaroop Mishra∗ Man Luo∗

Bhavdeep Singh Sachdeva

Chitta Baral

Arizona State University
{tgokhale, srmishr1, mluo26, bssachde, chitta}@asu.edu

Abstract

Data modiﬁcation, either via additional train-
ing datasets, data augmentation, debiasing,
and dataset ﬁltering, has been proposed as
an effective solution for generalizing to out-
of-domain (OOD) inputs, in both natural lan-
guage processing and computer vision litera-
ture. However, the effect of data modiﬁcation
on adversarial robustness remains unclear. In
this work, we conduct a comprehensive study
of common data modiﬁcation strategies and
evaluate not only their in-domain and OOD
performance, but also their adversarial robust-
ness (AR). We also present results on a two-
dimensional synthetic dataset to visualize the
effect of each method on the training distribu-
tion. This work serves as an empirical study to-
wards understanding the relationship between
generalizing to unseen domains and defending
against adversarial perturbations. Our ﬁndings
suggest that more data (either via additional
datasets or data augmentation) beneﬁts both
OOD accuracy and AR. However, data ﬁlter-
ing (previously shown to improve OOD accu-
racy on natural language inference) hurts OOD
accuracy on other tasks such as question an-
swering and image classiﬁcation. We provide
insights from our experiments to inform future
work in this direction.

1

Introduction

Deep neural networks have emerged as a widely
popular architectural choice for modeling tasks in
multiple domains such as (but not limited to) com-
puter vision (Yuille and Liu, 2021), natural lan-
guage processing (Hochreiter and Schmidhuber,
1997; Vaswani et al., 2017), and audio (Hannun
et al., 2014). While these models are highly capa-
ble of learning from training data, recent studies
show that they are quite prone to failure on new
test sets or under distribution shift (Taori et al.,
2020), natural corruptions (Hendrycks and Diet-
terich, 2019), adversarial attacks (Goodfellow et al.,

2015), spurious correlations (Beery et al., 2018),
and many other types of “unseen” changes that
may be encountered after training. This shortcom-
ing stems from the i.i.d. assumption in statistical
machine learning which guarantees good perfor-
mance only on test samples that are drawn from an
underlying distribution that is identical to the train-
ing dataset. For instance, digit recognition models
trained on the black-and-white MNIST training
images are almost perfect (> 99% accuracy) on
the corresponding test set, yet their performance
on colored digits and real-world digits from street
number plates is less than 75%. Similarly, state-of-
the-art NLP models have been shown to fail when
negation is introduced in the input (Kassner and
Schütze, 2020). These ﬁndings pose a signiﬁcant
challenge to the practical adoption of these models
and their reliability in the real-world.

To test model performance beyond the tradi-
tional notion of in-domain (ID) generalization, two
prominent ideas have emerged: out-of-domain
(OOD generalization) a.k.a. domain generaliza-
tion1, and adversarial robustness. The OOD gener-
alization objective expects a model which is trained
on distribution D to perform reliably on unseen dis-
tributions De, e ∈ {1, . . . , n}, that differ from D.
For a trained classiﬁer f ∗, OOD accuracy on previ-
ously unseen distribution De is deﬁned as:

acce

OOD =

E
(x,y)∼De

[I(f ∗(x) = y)]

(1)

To deﬁne adversarial robustness, consider an input
x and a true label y. For a classiﬁer loss function
(cid:96), a loss-maximizing perturbation δ∗ within ∆(cid:15) (an
(cid:15)-bounded neighborhood of x) is deﬁned as:

δ∗
x = max
δ∈∆(cid:15)

(cid:96)(f ∗(x +δ), y).

(2)

The second idea is that of adversarial robustness.
Recent work on adversarial examples has revealed

∗Equal Contribution

1In this paper we use these two terms interchangeably.

2
2
0
2

r
a

M
5
1

]
L
C
.
s
c
[

1
v
3
5
6
7
0
.
3
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
the vulnerability of deep neural networks against
small perturbations of the original data. Adversar-
ial robustness in such under this setting is deﬁned
as the accuracy of the classiﬁer on adversarial sam-
ples x +δx, where the perturbation lies within an
(cid:96)p norm bound: ||δx||p < (cid:15).

accrob = E

(x,y)∼D

I(f ∗(x +δx) = y).

(3)

In the context of text classiﬁcation, the norm-bound
can also be in the form of small character-level or
word-level perturbations such as swapping, insert-
ing, or deleting characters or words. In essence,
adversarial robustness measures the invariance of
the classiﬁer to small perturbations of the input.

Various methods have been developed that either
improve OOD generalization or improve adversar-
ial robustness. Notable among these are techniques
that modify the distribution of the training dataset.
In this paper, we focus on three major data modiﬁ-
cation techniques – the use of additional datasets
(also known as multi-source training), data aug-
mentation, and data ﬁltering; in addition we also
consider model-based debiasing techniques which
do not alter the data distribution explicitly. We
study the performance of these methods on three
representative tasks – natural language inference
(NLI), extractive question answering (QA), and
image classiﬁcation (IC).

Our ﬁrst aim in this paper is to understand
whether the increase or decrease in OOD gener-
alization by each method over the naive baseline
(standard training on the source dataset) is consis-
tent across tasks. To further conduct ﬁne-grained
analysis, we also analyze the effect of these meth-
ods on in-domain (ID) accuracy on the test set for
each task, since in the ideal case improvement in
OOD performance should not come at the cost of
in-domain accuracy.

Recent work seeks to understand the relation-
ships between in-domain and out-of-domain perfor-
mance: for instance, Miller et al. (2021) empirically
show that ID and OOD performance are strongly
correlated, Raghunathan et al. (2020); Yang et al.
(2020) show a trade-off between robustness and ac-
curacy for adversarially trained models. However it
is not clear how methods designed for OOD gener-
alization affect robustness. This is largely because
work on domain generalization reports only IID
and OOD metrics, and work on robustness reports
only ID and robustness metrics. Our second aim
is to understand the effect of these generalization

methods on adversarial robustness.

In addition to our experiments on NLP and
vision tasks, we also provide an experiment on
a synthetic binary classiﬁcation dataset where
points lie in a 2-dimensional feature space and are
separated by concentric circles into class labels.
This setting allows us to visualize the effect of data
modiﬁcation techniques on the training distribution
and the resulting performance.

Our ﬁndings can be summarized as follows:

• More data beneﬁts OOD generalization,
• Data ﬁltering hurts OOD generalization, and
• Data ﬁltering signiﬁcantly hurts adversarial

robustness on all benchmarks.

These ﬁndings and our additional analysis raise
new questions for robustness and domain general-
ization research. Signiﬁcant among these are the
importance of both diversity and number of train-
ing samples for inductive bias and generalization
guarantees, the problems associated with data ﬁl-
tering in terms of robustness, and the importance
of a comprehensive set of evaluation metrics that
could be adopted for future work.

2 Categorization of Domain
Generalization Methods

In this section, we provide a categorization of meth-
ods that are typically used as baselines for domain
generalization. We brieﬂy explain the method and
provide relevant related work in which these ideas
are used as methods for domain generalization.
Throughout this paper, we will refer to the orig-
inal training distribution as the “source” and the
out-of-distribution datasets as the “targets”.

refers

Single-Source Training (SS)
to the
“vanilla” baseline which is trained only on the
source dataset, without any dataset modiﬁcation.
SS utilizes no other information apart from the
single source dataset D and updates parameters θ
of classiﬁer f to minimize the risk on the source
using approaches such as ERM (Vapnik and Cher-
vonenkis, 1991).

minimize
θ

E
(x,y)∼D

(cid:96)(f (x; θ), y).

(4)

Multi-Source Training (MS). This method is
identical to SS except that additional training
datasets D(cid:48) are used for risk minimization.

minimize
θ

E
(x,y)∼D∪D(cid:48)

(cid:96)(f (x; θ), y).

(5)

Usually D(cid:48) are designed for the same task as D
but may have different styles, characteristics, or
sources of collection. For instance, while both
SNLI (Bowman et al., 2015) and MNLI (Williams
et al., 2018) are datasets for natural language infer-
ence with identical class labels, SNLI was collected
from image captions, while MNLI was collected
from Open American National Corpus2.

Gulrajani and Lopez-Paz (2020) provide an ex-
tensive comparitive study of models trained for
multi-source domain generalization for image clas-
siﬁcation and surprisingly ﬁnd that if multiple
source domains are available, ERM is empirically
the best approach as compared to specially de-
signed DG methods such as meta-learning (Li et al.,
2018a), learning domain-invariant features (Ganin
et al., 2016), invariant risk minimization (Arjovsky
et al., 2019), etc. These ﬁndings have also been
observed on text classiﬁcation experiments in (Koh
et al., 2021). Hendrycks et al. (2020a) show that
pre-training transformer architectures on diverse
data leads to higher OOD accuracies on multiple
tasks such as semantic textual similarity, sentiment
classiﬁcation, reading comprehension and natural
language inference.

Data Augmentation (DA). When additional
training distributions are not directly available,
transformations of samples in D using pre-deﬁned
augmentation functions can be used to create D(cid:48)
and train the model. Such data augmentation func-
tions are typically derived from existing knowledge
about the invariance of the task w.r.t. certain trans-
formations. For instance, for image classiﬁcation,
addition of small noise, small translations, scal-
ing, etc. are common data augmentation functions,
since they do not change the true label for the im-
age. Similarly, for text inputs, synonyms of words
are commonly used since they do not change the
semantics of the sentence. NLP data augmenta-
tion techniques include UDA (Xie et al., 2020),
EDA (Wei and Zou, 2019), and back-translation
for question answering (Longpre et al., 2019).

(DF). Dataset ﬁltering has been
Data Filtering
previously explored for quality control, such as,
removing noise and artifacts to curate and improve
publicly sourced datasets. However, there has been
recent interest in considering DF as a method for
bias reduction and generalization. This idea can be
traced back to work by Zellers et al. (2018, 2019),

2https://www.anc.org/

that proposed DF as an algorithmic method to avoid
annotation artifacts and spurious correlations dur-
ing dataset construction. AFLite (Bras et al., 2020)
extended this idea to a generic ﬁltering methodol-
ogy that can work without any pre-deﬁned rules
or strategies. Instead, AFLite operates by utiliz-
ing several weak learners (such as support-vector
machines) trained over small subsets to identify
samples that are easy to classify. It is argued that
such samples are more likely to carry biases, and
as such, could be removed. AFLite suggests that
reduction of a dataset to even 10% of the original
size can boost OOD accuracy on NLI. In the vi-
sion domain, similar ideas have been proposed con-
currently, including REPAIR (Li and Vasconcelos,
2019) and RESOUND (Li et al., 2018b), in which
instead of completely removing samples, biased
samples are assigned smaller weights. However
these methods require a prior knowledge of the
bias variable. Liu et al. (2021) have recently pro-
posed a simple approach which upweights samples
which have higher loss – this is shown to improve
worst-group accuracy without having access to the
bias variable.

(DB). Methods under this cat-
Model De-biasing
egory do not directly alter the training dataset, but
instead resort to changes in the modeling technique
– these changes can be in terms of the optimization
function, regularization, additional auxiliary costs,
etc. The main idea in DB is to utilize known biases
(or identify unknown biases) in the data distribu-
tion, model these biases in the training pipeline, and
use this knowledge to train robust classiﬁers (Clark
et al., 2019; Wu et al., 2020; Bhargava et al., 2021).
In the image classiﬁcation literature, there is grow-
ing consensus on enforcing a consistency on differ-
ent views (or augmentations) of an image in order
to achieve debiasing (Hendrycks et al., 2020c; Xu
et al., 2020; Chai et al., 2021; Nam et al., 2021).
Unlike DF, model de-biasing does not directly al-
ter the training distribution, but instead allows the
model to learn which biases to ignore.

3 Toy Example: Concentric Circles

We begin with a simple two-dimensional example
to illustrate our experimental setting and to show
how each method affects the distribution of the
training set. Consider the set of points shown in
Figure 1 where the points belong to two class la-
bels (either 0 or 1) and are seen to lie on concentric
circles. Points with label 0 are closer to the origin,

each category is provided in Table 1 and the abbre-
viations SS, MS, DA, DB, DF are used henceforth.

4.1 Natural Language Inference (NLI)

NLI is the task of determining whether a hypoth-
esis is true (entailment), false (contradiction), or
undetermined (neutral) given a premise.

Methods. We use RoBERTa as the backbone
model for each method and SNLI (Bowman et al.,
2015) as our source training corpus. A model
trained with expected risk minimization (ERM)
on SNLI alone, forms our single-source (SS) base-
line. A model trained with a combination of SNLI
and MNLI (Williams et al., 2018) forms our multi-
source (MS) baseline. We apply EDA (Wei and
Zou, 2019) to augment our training dataset with
100% of additional data to train a DA model. The
LMH debiasing method from Clark et al. (2019)
represents our DB model. For data ﬁltering, we
use AFlite (Bras et al., 2020) to ﬁlter out 90% of
the SNLI training data, and use the remaining 10%
data to train our DF model – this setting is based
on the experiments from (Bras et al., 2020).

Evaluation Protocol. We report accuracy on the
SNLI test set (IID), and to evaluate generalization,
(Wang
we report accuracy on NLI diagnostics
et al., 2018), Stress test evaluation (Naik et al.,
2018a) and HANS (McCoy et al., 2019a). We use
two metrics for evaluating robustness:
• model-based robustness uses BAE adversarial
attack (Garg and Ramakrishnan, 2020), imple-
mented using TextAttack (Morris et al., 2020),
and reports robustness as number of queries (se-
quential perturbations) needed to fool the model.
• model-free robustness uses six pre-deﬁned op-
erations to transform SNLI test inputs into ad-
versarial examples. These six methods are:
CLARE (Li et al., 2021a), character-swap (Pruthi
et al., 2019), Checklist (Ribeiro et al., 2020),
EDA (Wei and Zou, 2019), counter-ﬁtted em-
beddings (Emb) (Alzantot et al., 2018a).

Results. Table 2 shows the performance of each
method in terms of in-domain and out-of-domain
accuracy. We observe that four methods all im-
prove the generalization performance on average
but decrease the in-domain performance. Espe-
cially, DF method is the best in terms of OOD
accuracy, but is the worst in terms of in-domain
performance. We also see a trend that four meth-
ods improve the generalization in all sets of NLI-

Figure 1: Our toy experimental setting consists of
points in R2 belonging to two classes (0/1). This il-
lustration shows the discrepancy between the source
dataset (SS) and the out-of-domain dataset (OOD).

while points with label 1 are closer to a distance of
1 from the origin. Our aim is to start with the single
source dataset and train the model to generalize on
the out-of-domain (OOD) dataset. An important
thing to note here is that the source dataset contains
a subset of points with label 0 (orange) clustered
around (0.4, 0.0) and a subset with label 1 clus-
tered around (−1, 0.0). This implies that class-0
is biased towards x > 0, while class-1 is biased
towards x < 0. In total, our SS dataset consists of
10000 samples, of which 20% are biased.

We apply three data modiﬁcations: additional
source (MS), gaussian data augmentation (DA)
∼N (0, 0.1), and data ﬁltering (AFLite) which re-
duces the dataset size to 10%. Note that we do not
show model debiasing (DB) here, since it does not
alter the data distribution. Figure 2 shows the effect
on the data distribution. The most striking is the ef-
fect of DF which removes all samples previously in
the biased clusters near (0.4, 0.0) and (−1.0, 0.0).
Equipped with these resulting datasets, we train
a linear SGD classiﬁer with log-loss and evaluate
the robustness of each model in terms of in-domain
and OOD accuracies. We also evaluate adversarial
robustness by using standard PGD attacks. Results
are shown in the textboxes in Figure 2.
It can
be seen that data ﬁltering signiﬁcantly hurts both
OOD generalization and robustness. This ﬁnding
motivates our experiments to understand the effect
of each method for NLP and vision tasks.

4 Experiments

In this section, we present three tasks and their cor-
responding experimental setup, evaluation protocol
and our ﬁndings. A summary of methods belong to

Figure 2: This ﬁgure illustrates the effect of data modiﬁcation techniques on the training distribution. The leftmost
ﬁgure shows the training distribution in the single-source setting. The introduction of a second dataset or Data-
augmentation (done using small perturbations of source samples with Gaussian noise) makes the distribution more
diverse in the multi-source (MS) and data augmentation (DA) setting respectively. On the other hand, data ﬁltering,
in order to remove spurious correlations from the dataset, removes points from certain sectors of the distribution.
The effect of each strategy on OOD generalization and robustness is shown below each plot.

Method Category

Tasks

Natural Language Inference Question Answering

Image Classiﬁcation

SS (Single-Source ERM)
MS (Multi-Source ERM)
DA (Data Augmentation)
DB (Model De-biasing)
DF (Data Filtering)

SNLI
SNLI + MNLI
EDA (Wei and Zou, 2019)
LMH (Clark et al., 2019)
AFLite (Bras et al., 2020)

NQ (Kwiatkowski et al., 2019)
NQ + SQuAD+NQA+HQA+SQA+TQA MNIST + USPS
QG (Chan and Fan, 2019)
Mb-CR(Wu et al., 2020)
AFLite (adapted for QA)

M-ADA (Qiao et al., 2020)
RandConv (Xu et al., 2020)
AFLite

MNIST

Table 1: List of method categories and speciﬁc methods that we use under each task setting in nour experiments.
Details for each can be found in Section 4 for the corresponding task.

Diagnostics and HANS, while all four methods do
not show improvement on generalization on Dis-
traction and Noise sets of Stress dataset.

Table 3 shows the robustness evaluation. We
see that except for DF, all methods improve the
robustness under both model-based and model-free
evaluation. MS improves the robustness in all trans-
formations except for EDA. DA achieves the best
robustness by model-based evaluation but is not
consistent in terms of different transformations of
model-free evaluation. DB improves the robustness
in terms of every transformation and achieves the
best robustness in terms of average of model-free
evaluation. DF signiﬁcantly hampers the model-
free robustness with a drop in all transformations.

4.2 Question Answering (QA)

We focus on extractive QA. Given a passage (or
“context”) and a question, the task is to extract the
answer span from the passage.

Methods. We use BERT (Devlin et al., 2019)
as the backbone model for each method. We use
MRQA (Fisch et al., 2019) which is a collection of
12 publicly available multi-domain QA datasets –

with Natural Questions (NQ) (Kwiatkowski et al.,
2019) as the source dataset. SQuAD, NewsQA,
HotpotQA, SearchQA, and TriviaQA are used as
additional datasets for multi-source training. Simi-
lar to NLI, we use EDA for DA by applying EDA
on the question. We apply the augmentation to all
samples in the training set and combine them with
the original set to train a DA model. For model de-
biasing (DB), we use Mb-CR approach (Wu et al.,
2020), where a teacher and bias models are trained
a priori, and are used for debiasing.

We modify AFLite for our QA task of span pre-
diction, since AFLite was originally designed for
classiﬁcation tasks. To do so, we ﬁrst randomly
divide the training set into 10 subsets (or folds)
S1:10. For k∈{1, . . ., 10}, we pick Sk as the held-
out test set, and train models on the rest, and obtain
10 such models. At test time, models are used for
predicting an answer by only looking at the context
(without access to the question) – this allows us to
identify strong spurious correlations in the dataset.
Based on the predictions, samples are sorted on the
basis of their F1 score. A higher F1 score implies
that the model is more likely to answer the question

Method

In-Domain
Acc. (%)

SS
MS
DA
DB
DF

89.6
87.8
87.2
81.8
62.6

NLI-Diagnostics

Kno.

Lex.

Log.

51.8
52.1
52.1
52.4
53.9

65.7
66.8
66.0
66.0
66.5

57.8
57.8
58.1
58.4
58.7

PAS

72.6
72.8
72.6
72.8
68.9

OOD Acc. (%)
Stress Test

HANS

Comp. Distr. Noise

Lex.

Subs.

Consti.

77.9
79.6
79.6
79.3
79.1

73.5
72.4
71.8
71.8
72.0

79.8
79.2
79.2
79.5
79.5

88.4
92.0
92.8
92.2
94.1

28.2
33.6
32.8
33.8
46.3

21.7
26.7
26.4
27.5
38.5

Avg

61.74
63.30
63.14
63.37
65.75

Table 2: NLI Result: In-domain (IID) accuracy and out-of-domain generalization (OOD) on the NLI benchmark
using SNLI as source dataset. 3 See Table 1 for method abbreviations.

Method

Model Based
#Num Queries

CharSwap

EasyData

Embedding WordNet CheckList CLARE

Avg

Model Free Accuracy (%)

SS
MS
DA
DB
DF

53.56
54.44
55.06
54.82
51.13

81.3
81.5
77.7
81.5
65.2

72.0
71.6
74.1
72.4
56.8

81.9
82.0
80.7
82.3
66.2

77.0
78.2
80.2
78.0
62.5

89.4
89.2
86.6
89.2
72.3

76.3
77.5
80.5
77.0
62.5

79.65
80.00
79.97
80.07
64.25

Table 3: NLI Result: Comparison of robustness in terms of model-based evaluation (number of queries needed to
fool the model) and model-free (accuracy on adversarial transformations). 2 See Table 1 for method abbreviations.

without even knowing the question. We retain 10%
samples with the lowest F1 scores – these represent
the task since the model is not likely to predict the
correct answer without knowing the question.

mance in model-based and model-free evaluation.
DF signiﬁcantly hampers the model-free robustness
with drop in all transformations, meanwhile, the
model-based robustness also drops.

Evaluation Protocol. We report exact-match
(EM) accuracy for MRQA. To evaluate the gen-
eralization performance, we use six OOD develop-
ment sets from MRQA: DROP, RACE, BioASQ,
TextbookQA, RelationExtraction, and DuoRC. For
robustness, we use the “Morphues” attack (Tan
et al., 2020) on the question as the model-based
evaluation, the attack method is similar to NLI.
Model-free methods are the same as NLI.

Results. Table 4 shows the performance of each
method in terms of in-domain and out-of-domain
accuracy. We observe that two methods, MS and
DB, improve the generalization performance on
each out-of-domain dataset and also improve the
in-domain performance. The improvement of MS
is larger than DB. DA improves on some out-of-
domain datasets but not all, and it also improves the
in-domain performance. DF dramatically reduces
both out-of-domain and in-domain datasets.

Table 5 shows that except for DF, all meth-
ods improve over SS for both model-based and
model-free robustness evaluation. MS, DA, and DB
improve the robustness in all transformations of
model-free evaluation as well as the model-based
evaluation, where MS achieves the best perfor-

4.3

Image Classiﬁcation

We conduct our experiments on the standard do-
main generalization benchmark “Digits”, which
is a collection of handwritten digit classiﬁcation
datasets belonging to 10 classes (digits 0–9). Fol-
lowing standard practice(Volpi et al., 2018), we
train models on 10000 images from MNIST (Le-
Cun et al., 1998) as the source, and use SVHN (Net-
zer et al., 2011), SYN and MNIST-M (Ganin and
Lempitsky, 2015) as the OOD datasets.

Methods. We use DigitNet (Volpi et al., 2018) as
our backbone image classiﬁer architecture. Our SS
baseline uses MNIST for training; MS uses MNIST
and USPS (Denker et al., 1988). For data aug-
mentation we rely on M-ADA (Qiao et al., 2020)
which is a perturbation-based min-max algorithm
to create augmented data. Our debiasing method
is RandConv (Xu et al., 2020) which utilizes a ran-
dom convolutional layer to generate novel views
of each input image, and a KL-divergence based
loss function that encourages the classiﬁer to pre-
dict consistent predictions for each version of the
image. This leads to the model being debiased on
spurious features like background, texture, or color
of digits. We use AFLite as our DF method.

Method

In-Domain
EM. (%)

OOD EM. (%)

DROP RACE BioASQ TBQA

R.E.

DuoRC

Avg

SS
MS
DA
DB
DF

63.76
65.07
63.84
64.58
49.56

20.09
26.88
19.23
20.83
9.25

19.29
27.45
19.73
19.73
11.72

33.91
45.01
32.31
34.64
20.94

28.61
40.52
28.54
31.20
19.63

62.82
72.86
61.97
63.64
45.28

32.71
43.44
32.31
35.98
21.45

32.91
42.69
32.35
34.34
21.38

Table 4: QA Result: Source (IID) accuracy and domain generalization (OOD) on the Question Answering bench-
mark with NaturalQuestions as source dataset. EM: Exact-Match. See Table 1 for method abbreviations.

Method

Model Based
#Queries

CharSwap

EasyData

Embedding WordNet CheckList CLARE

Avg

Model Free EM. (%)

SS
MS
DA
DB
DF

19.55
21.97
21.91
20.40
19.19

60.29
62.22
60.88
61.62
47.97

52.17
52.65
54.52
53.16
42.48

61.21
63.22
62.02
62.35
48.55

58.41
59.84
59.82
59.32
47.19

63.22
64.42
63.42
64.03
49.34

61.92
63.55
62.36
63.01
48.72

59.54
60.98
60.5
60.58
47.38

Table 5: QA Result: Comparison of robustness in terms of model-based evaluation (number of queries needed to
fool the model) and model-free (accuracy on adversarial transformations). 2 See Table 1 for method abbreviations.

Method

In-Domain
Acc. (%)

SS
MS
DA
DB
DF

98.40
98.54
99.30
98.86
95.27

OOD Acc. (%)

MNIST-M SVHN SYNTH Avg

58.09
59.79
67.94
87.67
51.04

33.85
33.87
42.55
54.95
22.07

45.94
48.42
48.95
63.37
27.83

45.96
47.36
53.15
68.66
33.65

Table 6: Source (in-domain) accuracy and domain gen-
eralization (OOD accuracy) on the Digits benchmark
with MNIST-10k as source dataset.2

Evaluation Protocol. We report IID accuracy on
the MNIST test set and generalization as the accu-
racy on our OOD datasets. For evaluating adver-
sarial robustness we use Foolbox (Rauber et al.,
2017) and use 10 attack methods (both (cid:96)2 and (cid:96)∞
versions of FGSM, PGD, BIM, AUN, and Deep-
Fool). Robustness is calculated as the accuracy
for 20 values of (cid:15) between [0, 2], and is plotted as
robustness curves for visualization, along with the
average values for area under the curve (AUC).

and area under the curve (AUC) for each plot. It
can be observed that DF is worse than SS for all
10 attack variants. We observe that DA and DB are
better than SS, and the drop for DF is the largest.

5 Analysis

Based on the results of three tasks, we have the
following observations about the performance of
each method compared to the SS baseline:

• MS increases OOD accuracy on all three tasks
and robustness on two tasks (NLI and QA).
• DA increases OOD on two tasks (NLI and IC)

and robustness on all three tasks.

• DB increases OOD on three tasks and robust-

ness on two tasks (NLI and QA).

• DF decreases OOD on two tasks (QA and IC)

and robustness on all three tasks.

Decrease in NLI in-domain accuracy
is seen
for all methods, even though these lead to increase
in OOD accuracy. This suggests that the training
dataset (SNLI) has a large shift w.r.t. OOD datasets.

Results. Table 6 shows the performance of each
method in terms of in-domain and OOD accuracy.
MS, DA and DB, improve the generalization perfor-
mance on each OOD dataset and also improve the
in-domain performance, where DB displays best
generalization capacity. DF dramatically reduces
the OOD performance with signiﬁcant reduction
across all datasets; the in-domain accuracy also
decreases. Figure 3 shows robustness (accuracy)

More data implies more OOD generalization:
While this trend is observed for both MS and DA,
there is one anomaly – DA for the QA task leads to
marginal decrease compared to SS (a difference of
0.56%). This ﬁnding is aligned with Longpre et al.
(2019), who report no signiﬁcant effect of data aug-
mentation (back translation) on OOD performance
for question answering. This points to the need
for improving data augmentation techniques in QA.

Figure 3: Evaluation of adversarial robustness (using 10 attack methods) for MNIST10k.

Marginal Improvement on Robustness: From
the results, it is easy to see that the improvement
on OOD is more noticeable than robustness, for ex-
ample, MS improves OOD performance by ∼10%,
but improves only by ∼1% under model-free eval-
uation. While this observation is reasonable since
each method is designed to improve the generaliza-
tion, new methods that improve both generalization
and robustness should be encouraged.

5.1 Correlation between Adversarial

Robustness and OOD Generalization

Our experiments reveal the alarming ﬁnding that
across the board, DF reduces adversarial robust-
ness. To investigate further, we conduct an anal-
ysis on the Digits benchmark and compare SS
and DF when trained with equal amounts of data
({10%, 20%, . . . , 100%}). Note that for SS the
data are sampled randomly, while for DF the data
are obtained via AFLite data ﬁltering. Results are
shown in Figure 4.
It can be observed that the
OOD accuracy increases as the size of the dataset
increases, and is greater for SS than DF. To un-
derstand how an increase in OOD accuracy affects
robustness, we also compute the robustness values
at each size of training data, and compute the Pear-
son correlation coefﬁcient for each attack method
– positive correlation implies that as OOD accu-
racy increases, robustness also increases. Figure 5
shows clear evidence in favor of positive correla-
tion; interestingly, SS has higher correlation for (cid:96)2
attacks, while DF is higher for (cid:96)∞ attacks. The evi-
dence is clear: OOD generalization increases with
the size of the dataset and adversarial robustness is

Figure 4: Comparison between SS and DF models
trained with different percentages of MNIST10k.

Figure 5: Pearson Correlation between OOD accuracy
and robustness for SS and DF models on MNIST10k.

On the other hand, the performance drop due to DF
is signiﬁcantly large for QA (11.53%).

Decrease in MNIST robustness: For MNIST,
the DA method (M-ADA (Qiao et al., 2020)) is the
best in terms of robustness and also improves OOD
accuracy. M-ADA is an “adversarial data augmen-
tation” method, i.e., it uses a min-max objective to
ﬁnd loss-maximizing perturbations and uses these
perturbations as augmented data. It is therefore
intuitive that such a method would do well on the
adversarial robustness metric (although robustness
evaluation was not reported by Qiao et al. (2020)).

0.00.51.01.52.00.00.20.40.60.81.02 FGSMSS  AUC= 1.8226MS  AUC= 1.6728DA  AUC= 1.8915DB  AUC= 1.8788DF  AUC= 1.24880.00.51.01.52.02 PGDSS  AUC= 1.8097MS  AUC= 1.6477DA  AUC= 1.8886DB  AUC= 1.7681DF  AUC= 1.25490.00.51.01.52.02 BIMSS  AUC= 1.7688MS  AUC= 1.5834DA  AUC= 1.8472DB  AUC= 1.7059DF  AUC= 1.18070.00.51.01.52.02 AUNSS  AUC= 1.9860MS  AUC= 1.8400DA  AUC= 2.0000DB  AUC= 1.9800DF  AUC= 1.65710.00.51.01.52.02 DFSS  AUC= 1.7718MS  AUC= 1.6013DA  AUC= 1.8610DB  AUC= 1.7402DF  AUC= 1.20070.00.51.01.52.00.00.20.40.60.81.0 FGSMSS  AUC= 0.1883MS  AUC= 0.1900DA  AUC= 0.4142DB  AUC= 0.3880DF  AUC= 0.07530.00.51.01.52.0 PGDSS  AUC= 0.1143MS  AUC= 0.0901DA  AUC= 0.1342DB  AUC= 0.0946DF  AUC= 0.06720.00.51.01.52.0 BIMSS  AUC= 0.1068MS  AUC= 0.0825DA  AUC= 0.1215DB  AUC= 0.0891DF  AUC= 0.06220.00.51.01.52.0 AUNSS  AUC= 1.0551MS  AUC= 0.8531DA  AUC= 1.5526DB  AUC= 1.1399DF  AUC= 0.60700.00.51.01.52.0 DFSS  AUC= 0.1191MS  AUC= 0.0929DA  AUC= 0.1335DB  AUC= 0.1012DF  AUC= 0.0663EpsilonAccuracyAvg. AUC. SS: 1.074MS: 0.965DA: 1.184DB: 1.089DF: 0.742Avg. AUC. SS: 1.074MS: 0.965DA: 1.184DB: 1.089DF: 0.742Avg. AUC. SS: 1.074MS: 0.965DA: 1.184DB: 1.089DF: 0.742Avg. AUC. SS: 1.074MS: 0.965DA: 1.184DB: 1.089DF: 0.742Avg. AUC. SS: 1.074MS: 0.965DA: 1.184DB: 1.089DF: 0.742Avg. AUC. SS: 1.074MS: 0.965DA: 1.184DB: 1.089DF: 0.742Avg. AUC. SS: 1.074MS: 0.965DA: 1.184DB: 1.089DF: 0.742Avg. AUC. SS: 1.074MS: 0.965DA: 1.184DB: 1.089DF: 0.742Avg. AUC. SS: 1.074MS: 0.965DA: 1.184DB: 1.089DF: 0.742Avg. AUC. SS: 1.074MS: 0.965DA: 1.184DB: 1.089DF: 0.74220406080100Size of Training Dataset (%)35.037.540.042.545.047.5OOD Accuracy (%)SS.  AUC=0.3875DF.  AUC=0.37692 FGSM2 PGD2 BIM2 AUN2 DF FGSM PGD BIM AUN DFAttack Method0.750.800.850.900.951.00Pearson Coefficient Pearson Correlation between OOD Accuracy and RobustnessSSDFpositively correlated with OOD generalization.

Our experiments show that the size of the train-
ing set directly affects both robustness and general-
ization. While removing 90% data increased OOD
accuracy in NLI, the effect was the exact opposite
for QA and MNIST. The key idea in domain gener-
alization is that the test distributions are unknown
and little information about them is available apart
from the fact that there is no task shift. Without this
prior knowledge, deciding whether (or how much)
to ﬁlter a dataset is a challenging task.

6 Related Work

In Section 2 we have provided relevant work that
falls into one of our ﬁve modeling categories. Here,
we discuss additional literature on robustness and
generalization and new efforts towards dataset cre-
ation, benchmarks, and evaluation.

Generalization Benchmarks. Hendrycks et al.
(2020b) have constructed a robustness benchmark
for multiple language understanding tasks by split-
ting training sets from existing benchmarks accord-
ing to topics, styles, and vocabulary; this has been
subsequently used to study robustness of model
rankings (Mishra and Arunkumar, 2021). Bench-
marks have also been constructed to study dataset
artifacts and generalization capabilities of mod-
els (Mishra et al., 2020a,b; Mishra and Sachdeva,
2020). MRQA (Fisch et al., 2019) is a bench-
mark for evaluating domain generalization of ques-
tion answering (reading comprehensive) models.
MRQA contains 6 datasets each for training, devel-
opment, and evaluation. For image classiﬁcation,
many benchmarks have been proposed to evalu-
ate domain generalization, such as PACS (Li et al.,
2017), OfﬁceHome (Venkateswara et al., 2017),
Digits (Volpi et al., 2018), and WILDS (Koh et al.,
2021) which is a compendium of domain general-
ization bechmarks for tasks such as image classiﬁ-
cation, text sentiment and toxicity prediction.

Corruption Robustness. Hendrycks and Diet-
terich (2019) introduced ImageNet-C and CIFAR-
C to test robustness along corruptions such as
weather, noise, blur, and digital artifacts, and
ImageNet-P which tests robustness against small
tilts and changes in brightness. MNIST-C was in-
troduced by Mu and Gilmer (2019) for similar cor-
ruptions of handwritten digit images.

Ribeiro et al., 2018; Iyyer et al., 2018; Alzantot
et al., 2018b) and approaches to defend against
word substitution (Jia et al., 2019) have been ex-
plored. Contrastive examples have been introduced
as a means for evaluation, for example, manually
crafted contrast sets for textual entailment (Gard-
ner et al., 2020) or template-based (McCoy et al.,
2019b; Glockner et al., 2018; Naik et al., 2018b).
Model-in-the-loop dataset creation methods have
also been proposed for various NLP tasks (Nie
et al., 2020; Arunkumar et al., 2020; Kiela et al.,
2021) and visual question answering (Sheng et al.,
2021; Li et al., 2021b).

7 Discussion

Recently, Miller et al. (2021) have empirically
shown linear trends between in-distribution and
out-of-distribution performance on multiple image
classiﬁcation tasks, across various model architec-
tures, hyper-parameters, training set size, and du-
ration of training. They also show that there are
certain settings of domain shift under which the
linear trend does not hold. Our work empirically
shows that while data ﬁltering may beneﬁt OOD
generalization on the NLI benchmark, this does not
hold for other tasks such as image classiﬁcation
and question answering. This suggests that data
ﬁltering may beneﬁt generalization in certain types
of domain shift, but not on others. Concurrently,
Yi et al. (2021) have theoretically shown that mod-
els robust to input perturbations generalize well
on OOD distribution within a Wasserstein radius
around the training distribution. Our empirical ob-
servations in this paper in both vision and language
domains, agree with the theory of Yi et al. (2021).
In this work, we conduct a comprehensive study
of methods which are designed for OOD generaliza-
tion on three tasks: NLI, QA, and IC. We evaluate
each method on in-domain, OOD, and adversarial
robustness. 4 Our ﬁndings suggest that more data
typically beneﬁts both OOD and robustness. Data
ﬁltering hurts OOD accuracy on two out of three
tasks, and also hurts robustness on all three tasks.
In context of our ﬁndings and work by Miller et al.
(2021); Yi et al. (2021), we recommend that meth-
ods designed either for robustness or generalization
should be evaluated on multiple aspects and not on
the single metric that they are optimized for.

Adversarial and Contrastive Sets. Generation
of adversarial examples (Jia and Liang, 2017;

4Code for our experiments will be released at https:

//github.com/tejas-gokhale/gen-vs-rob.

Acknowledgements

This work was funded in part by DARPA SAIL-ON
program (W911NF2020006) and DARPA CHESS
program (FA875019C0003). The views and opin-
ions of the authors expressed herein do not neces-
sarily state or reﬂect those of the funding agencies
and employers.

Broader Impact

One underlying assumption behind using large
datasets for training (or pre-training) vision and
language models is that larger datasets increase the
likelihood of obtaining a diverse set of samples to
reduce overﬁtting. However, recent studies (Ben-
der et al., 2021; Stanovsky et al., 2019) serve as
cautionary tales when employing uncurated inter-
net data to train large language models, and discuss
how large data does not necessarily imply that mod-
els will learn the dievrse distribution. At the same
time, the inverse (small data aids diversity) is also
not true (as shown by this paper) and comes with its
own problems – for instance, Figure 2 shows that
dataset ﬁltering can lead to much larger changes
in the data distribution beyond notions of propor-
tionality and fairness. As such, the decision on
how many and what samples to remove can also
introduce its own set of biases. Data curation is a
challenging problem and needs further task-speciﬁc
study since the concepts of bias and fairness often
depend on the task deﬁnition and speciﬁcations of
ideal outcomes. Insights from this paper could help
researchers and practitioners in choosing appropri-
ate approaches for improving generalization and
robustness.

References

Moustafa Alzantot, Yash Sharma, Ahmed Elgohary,
Bo-Jhang Ho, Mani Srivastava, and Kai-Wei Chang.
2018a. Generating natural language adversarial ex-
amples. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing,
pages 2890–2896, Brussels, Belgium. Association
for Computational Linguistics.

Moustafa Alzantot, Yash Sharma, Ahmed Elgohary,
Bo-Jhang Ho, Mani Srivastava, and Kai-Wei Chang.
2018b. Generating natural language adversarial ex-
amples. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing,
pages 2890–2896, Brussels, Belgium. Association
for Computational Linguistics.

Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, and

David Lopez-Paz. 2019. Invariant risk minimization.
arXiv preprint arXiv:1907.02893.

Anjana Arunkumar, Swaroop Mishra, Bhavdeep
Sachdeva, Chitta Baral, and Chris Bryan. 2020.
Real-time visual feedback for educative benchmark
creation: A human-and-metric-in-the-loop work-
ﬂow.

Sara Beery, Grant Van Horn, and Pietro Perona. 2018.
Recognition in terra incognita. In Proceedings of the
European conference on computer vision (ECCV),
pages 456–473.

Emily M Bender, Timnit Gebru, Angelina McMillan-
Major, and Shmargaret Shmitchell. 2021. On the
dangers of stochastic parrots: Can language models
be too big? In Proceedings of the 2021 ACM Confer-
ence on Fairness, Accountability, and Transparency,
pages 610–623.

Prajjwal Bhargava, Aleksandr Drozd, and Anna Rogers.
2021. Generalization in NLI: Ways (not) to go be-
In Proceedings of the Sec-
yond simple heuristics.
ond Workshop on Insights from Negative Results in
NLP, pages 125–135, Online and Punta Cana, Do-
minican Republic. Association for Computational
Linguistics.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
632–642, Lisbon, Portugal. Association for Compu-
tational Linguistics.

Ronan Le Bras, Swabha Swayamdipta, Chandra Bha-
gavatula, Rowan Zellers, Matthew E. Peters, Ashish
Sabharwal, and Yejin Choi. 2020. Adversarial ﬁl-
ters of dataset biases. In Proceedings of the 37th In-
ternational Conference on Machine Learning, ICML
2020, 13-18 July 2020, Virtual Event, volume 119 of
Proceedings of Machine Learning Research, pages
1078–1088. PMLR.

Lucy Chai, Jun-Yan Zhu, Eli Shechtman, Phillip Isola,
and Richard Zhang. 2021. Ensembling with deep
generative views. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recog-
nition, pages 14997–15007.

Ying-Hong Chan and Yao-Chung Fan. 2019. A recur-
rent bert-based model for question generation.
In
Proceedings of the 2nd Workshop on Machine Read-
ing for Question Answering, pages 154–162.

Christopher Clark, Mark Yatskar, and Luke Zettle-
moyer. 2019. Don’t take the easy way out: En-
semble based methods for avoiding known dataset
In Proceedings of the 2019 Conference on
biases.
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP), pages
4069–4082, Hong Kong, China. Association for
Computational Linguistics.

JS Denker, WR Gardner, HP Graf, D Henderson,
RE Howard, W Hubbard, LD Jackel, HS Baird, and
I Guyon. 1988. Neural network recognizer for hand-
written zip code digits. In Proceedings of the 1st In-
ternational Conference on Neural Information Pro-
cessing Systems, pages 323–331.

Ian J. Goodfellow, Jonathon Shlens, and Christian
Szegedy. 2015. Explaining and harnessing adversar-
In 3rd International Conference on
ial examples.
Learning Representations, ICLR 2015, San Diego,
CA, USA, May 7-9, 2015, Conference Track Proceed-
ings.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
In Proceedings of the 2019 Conference
standing.
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers),
pages 4171–4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.

Adam Fisch, Alon Talmor, Robin Jia, Minjoon Seo,
Eunsol Choi, and Danqi Chen. 2019. Mrqa 2019
shared task: Evaluating generalization in reading
In Proceedings of the 2nd Work-
comprehension.
shop on Machine Reading for Question Answering,
pages 1–13.

Yaroslav Ganin and Victor Lempitsky. 2015. Unsuper-
vised domain adaptation by backpropagation. In In-
ternational conference on machine learning, pages
1180–1189. PMLR.

Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan,
Pascal Germain, Hugo Larochelle, François Lavi-
olette, Mario Marchand, and Victor Lempitsky.
2016. Domain-adversarial training of neural net-
works. The journal of machine learning research,
17(1):2096–2030.

Matt Gardner, Yoav Artzi, Victoria Basmov, Jonathan
Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi,
Dheeru Dua, Yanai Elazar, Ananth Gottumukkala,
Nitish Gupta, Hannaneh Hajishirzi, Gabriel Ilharco,
Daniel Khashabi, Kevin Lin, Jiangming Liu, Nel-
son F. Liu, Phoebe Mulcaire, Qiang Ning, Sameer
Singh, Noah A. Smith, Sanjay Subramanian, Reut
Tsarfaty, Eric Wallace, Ally Zhang, and Ben Zhou.
2020. Evaluating models’ local decision boundaries
In Findings of the Association
via contrast sets.
for Computational Linguistics: EMNLP 2020, pages
1307–1323, Online. Association for Computational
Linguistics.

Siddhant Garg and Goutham Ramakrishnan. 2020.
Bae: Bert-based adversarial examples for text classi-
ﬁcation. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 6174–6181.

Max Glockner, Vered Shwartz, and Yoav Goldberg.
2018. Breaking NLI systems with sentences that re-
In Proceedings of
quire simple lexical inferences.
the 56th Annual Meeting of the Association for Com-
putational Linguistics (Volume 2: Short Papers),
pages 650–655, Melbourne, Australia. Association
for Computational Linguistics.

Ishaan Gulrajani and David Lopez-Paz. 2020.

search of lost domain generalization.
tional Conference on Learning Representations.

In
In Interna-

Awni Hannun, Carl Case, Jared Casper, Bryan Catan-
zaro, Greg Diamos, Erich Elsen, Ryan Prenger, San-
jeev Satheesh, Shubho Sengupta, Adam Coates, et al.
2014. Deep speech: Scaling up end-to-end speech
recognition. arXiv preprint arXiv:1412.5567.

Dan Hendrycks and Thomas G. Dietterich. 2019.
Benchmarking neural network robustness to com-
In 7th Inter-
mon corruptions and perturbations.
national Conference on Learning Representations,
ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.
OpenReview.net.

Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam
Dziedzic, Rishabh Krishnan, and Dawn Song. 2020a.
Pretrained transformers improve out-of-distribution
robustness. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics,
pages 2744–2751.

Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam
Dziedzic, Rishabh Krishnan, and Dawn Song.
Pretrained transformers improve out-of-
2020b.
distribution robustness. In Proceedings of the 58th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 2744–2751, Online. Asso-
ciation for Computational Linguistics.

Dan Hendrycks, Norman Mu, Ekin Dogus Cubuk,
Barret Zoph, Justin Gilmer, and Balaji Lakshmi-
narayanan. 2020c. Augmix: A simple data process-
ing method to improve robustness and uncertainty.
In 8th International Conference on Learning Repre-
sentations, ICLR 2020, Addis Ababa, Ethiopia, April
26-30, 2020. OpenReview.net.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Neural computation,

Long short-term memory.
9(8):1735–1780.

Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke
Zettlemoyer. 2018. Adversarial example generation
with syntactically controlled paraphrase networks.
In Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long Papers), pages 1875–1885, New
Orleans, Louisiana. Association for Computational
Linguistics.

Robin Jia and Percy Liang. 2017. Adversarial exam-
ples for evaluating reading comprehension systems.
In Proceedings of the 2017 Conference on Empiri-
cal Methods in Natural Language Processing, pages

2021–2031, Copenhagen, Denmark. Association for
Computational Linguistics.

Robin Jia, Aditi Raghunathan, Kerem Göksel, and
Percy Liang. 2019. Certiﬁed robustness to adver-
In Proceedings of the
sarial word substitutions.
2019 Conference on Empirical Methods in Natu-
ral Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), pages 4129–4142, Hong Kong,
China. Association for Computational Linguistics.

Nora Kassner and Hinrich Schütze. 2020. Negated and
misprimed probes for pretrained language models:
Birds can talk, but cannot ﬂy. In Proceedings of the
58th Annual Meeting of the Association for Compu-
tational Linguistics, pages 7811–7818, Online. As-
sociation for Computational Linguistics.

Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh
Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vid-
gen, Grusha Prasad, Amanpreet Singh, Pratik Ring-
shia, Zhiyi Ma, Tristan Thrush, Sebastian Riedel,
Zeerak Waseem, Pontus Stenetorp, Robin Jia, Mo-
hit Bansal, Christopher Potts, and Adina Williams.
2021. Dynabench: Rethinking benchmarking in
In Proceedings of the 2021 Conference of
NLP.
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 4110–4124, Online. Association for
Computational Linguistics.

Pang Wei Koh, Shiori Sagawa, Sang Michael Xie,
Marvin Zhang, Akshay Balsubramani, Weihua Hu,
Michihiro Yasunaga, Richard Lanas Phillips, Irena
Gao, Tony Lee, et al. 2021. Wilds: A benchmark
In International
of in-the-wild distribution shifts.
Conference on Machine Learning, pages 5637–5664.
PMLR.

Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
ﬁeld, Michael Collins, Ankur Parikh, Chris Al-
berti, Danielle Epstein, Illia Polosukhin, Jacob De-
vlin, Kenton Lee, Kristina Toutanova, Llion Jones,
Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai,
Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.
Natural questions: A benchmark for question an-
swering research. Transactions of the Association
for Computational Linguistics, 7:452–466.

Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick
Haffner. 1998. Gradient-based learning applied to
document recognition. Proceedings of the IEEE,
86(11):2278–2324.

Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M
Hospedales. 2017. Deeper, broader and artier do-
In Proceedings of the IEEE
main generalization.
international conference on computer vision, pages
5542–5550.

Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M
Hospedales. 2018a. Learning to generalize: Meta-
In Thirty-
learning for domain generalization.
Second AAAI Conference on Artiﬁcial Intelligence.

Dianqi Li, Yizhe Zhang, Hao Peng, Liqun Chen, Chris
Brockett, Ming-Ting Sun, and William B Dolan.
2021a. Contextualized perturbation for textual ad-
In Proceedings of the 2021 Con-
versarial attack.
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 5053–5069.

Linjie Li, Jie Lei, Zhe Gan, and Jingjing Liu. 2021b.
Adversarial vqa: A new benchmark for evaluating
the robustness of vqa models. In International Con-
ference on Computer Vision (ICCV).

Yi Li and Nuno Vasconcelos. 2019. Repair: Removing
In Pro-
representation bias by dataset resampling.
ceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 9572–9581.

Yingwei Li, Yi Li, and Nuno Vasconcelos. 2018b. Re-
sound: Towards action recognition without represen-
tation bias. In Proceedings of the European Confer-
ence on Computer Vision (ECCV), pages 513–528.

Evan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi
Raghunathan, Pang Wei Koh, Shiori Sagawa, Percy
Liang, and Chelsea Finn. 2021. Just train twice: Im-
proving group robustness without training group in-
formation. In International Conference on Machine
Learning, pages 6781–6792. PMLR.

Shayne Longpre, Yi Lu, Zhucheng Tu, and Chris
DuBois. 2019. An exploration of data augmentation
and sampling techniques for domain-agnostic ques-
tion answering. In Proceedings of the 2nd Workshop
on Machine Reading for Question Answering, pages
220–227, Hong Kong, China. Association for Com-
putational Linguistics.

Tom McCoy, Ellie Pavlick, and Tal Linzen. 2019a.
Right for the wrong reasons: Diagnosing syntactic
heuristics in natural language inference. In Proceed-
ings of the 57th Annual Meeting of the Association
for Computational Linguistics, pages 3428–3448,
Florence, Italy. Association for Computational Lin-
guistics.

Tom McCoy, Ellie Pavlick, and Tal Linzen. 2019b.
Right for the wrong reasons: Diagnosing syntactic
heuristics in natural language inference. In Proceed-
ings of the 57th Annual Meeting of the Association
for Computational Linguistics, pages 3428–3448,
Florence, Italy. Association for Computational Lin-
guistics.

John P Miller, Rohan Taori, Aditi Raghunathan, Sh-
iori Sagawa, Pang Wei Koh, Vaishaal Shankar, Percy
Liang, Yair Carmon, and Ludwig Schmidt. 2021.
Accuracy on the line: On the strong correlation be-
tween out-of-distribution and in-distribution gener-
alization. In International Conference on Machine
Learning, pages 7721–7735. PMLR.

Swaroop Mishra and Anjana Arunkumar. 2021. How
robust are model rankings: A leaderboard cus-
In
tomization approach for equitable evaluation.

Proceedings of the AAAI Conference on Artiﬁcial In-
telligence, volume 35, pages 13561–13569.

Swaroop Mishra, Anjana Arunkumar, Chris Bryan, and
Chitta Baral. 2020a. Our evaluation metric needs an
update to encourage generalization. arXiv preprint
arXiv:2007.06898.

Swaroop Mishra, Anjana Arunkumar, Bhavdeep
Sachdeva, Chris Bryan, and Chitta Baral. 2020b.
arXiv
Dqi: A guide to benchmark evaluation.
preprint arXiv:2008.03964.

Swaroop Mishra and Bhavdeep Singh Sachdeva. 2020.
Do we need to create big datasets to learn a task?
In Proceedings of SustaiNLP: Workshop on Simple
and Efﬁcient Natural Language Processing, pages
169–173.

John Morris, Eli Liﬂand, Jin Yong Yoo, Jake Grigsby,
Di Jin, and Yanjun Qi. 2020. TextAttack: A frame-
work for adversarial attacks, data augmentation, and
adversarial training in NLP. In Proceedings of the
2020 Conference on Empirical Methods in Natu-
ral Language Processing: System Demonstrations,
pages 119–126, Online. Association for Computa-
tional Linguistics.

Norman Mu and Justin Gilmer. 2019. Mnist-c: A
robustness benchmark for computer vision. arXiv
preprint arXiv:1906.02337.

Danish Pruthi, Bhuwan Dhingra, and Zachary C Lip-
ton. 2019. Combating adversarial misspellings with
In Proceedings of the
robust word recognition.
57th Annual Meeting of the Association for Compu-
tational Linguistics, pages 5582–5591.

Fengchun Qiao, Long Zhao, and Xi Peng. 2020. Learn-
ing to learn single domain generalization. In 2020
IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition, CVPR 2020, Seattle, WA, USA,
June 13-19, 2020, pages 12553–12562. IEEE.

Aditi Raghunathan, Sang Michael Xie, Fanny Yang,
John Duchi, and Percy Liang. 2020. Understand-
ing and mitigating the tradeoff between robustness
and accuracy. Proceedings of Machine Learning Re-
search.

Jonas Rauber, Wieland Brendel, and Matthias Bethge.
2017. Foolbox: A python toolbox to benchmark the
robustness of machine learning models. In Reliable
Machine Learning in the Wild Workshop, 34th Inter-
national Conference on Machine Learning.

Marco Tulio Ribeiro, Sameer Singh, and Carlos
Guestrin. 2018. Semantically equivalent adversar-
ial rules for debugging NLP models. In Proceedings
of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 856–865, Melbourne, Australia. Association
for Computational Linguistics.

Aakanksha Naik, Abhilasha Ravichander, Norman
Sadeh, Carolyn Rose, and Graham Neubig. 2018a.
Stress test evaluation for natural language inference.
In Proceedings of the 27th International Conference
on Computational Linguistics, pages 2340–2353,
Santa Fe, New Mexico, USA. Association for Com-
putational Linguistics.

Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin,
and Sameer Singh. 2020. Beyond accuracy: Be-
havioral testing of NLP models with CheckList. In
Proceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 4902–
4912, Online. Association for Computational Lin-
guistics.

Aakanksha Naik, Abhilasha Ravichander, Norman
Sadeh, Carolyn Rose, and Graham Neubig. 2018b.
Stress test evaluation for natural language inference.
In Proceedings of the 27th International Conference
on Computational Linguistics, pages 2340–2353,
Santa Fe, New Mexico, USA. Association for Com-
putational Linguistics.

Hyeonseob Nam, HyunJae Lee, Jongchan Park, Won-
jun Yoon, and Donggeun Yoo. 2021. Reducing do-
main gap by reducing style bias. In Proceedings of
the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 8690–8699.

Yuval Netzer, Tao Wang, Adam Coates, Alessandro
Bissacco, Bo Wu, and Andrew Y Ng. 2011. Read-
ing digits in natural images with unsupervised fea-
ture learning.

Yixin Nie, Adina Williams, Emily Dinan, Mohit
Bansal, Jason Weston, and Douwe Kiela. 2020. Ad-
versarial NLI: A new benchmark for natural lan-
guage understanding. In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics, pages 4885–4901, Online. Association
for Computational Linguistics.

Sasha Sheng, Amanpreet Singh, Vedanuj Goswami,
Jose Alberto Lopez Magana, Wojciech Galuba,
Devi Parikh, and Douwe Kiela. 2021. Human-
arXiv
adversarial visual question answering.
preprint arXiv:2106.02280.

Gabriel Stanovsky, Noah A. Smith, and Luke Zettle-
moyer. 2019. Evaluating gender bias in machine
translation. In Proceedings of the 57th Annual Meet-
ing of the Association for Computational Linguistics,
pages 1679–1684, Florence, Italy. Association for
Computational Linguistics.

Samson Tan, Shaﬁq Joty, Min-Yen Kan, and Richard
Socher. 2020. It’s morphin’time! combating linguis-
tic discrimination with inﬂectional perturbations. In
Proceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 2920–
2935.

Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas
Carlini, Benjamin Recht, and Ludwig Schmidt.
2020. Measuring robustness to natural distribution
shifts in image classiﬁcation. In Advances in Neural
Information Processing Systems, volume 33, pages
18583–18599.

tion for consistency training. Advances in Neural
Information Processing Systems, 33.

Zhenlin Xu, Deyi Liu, Junlin Yang, Colin Raffel, and
Marc Niethammer. 2020. Robust and generalizable
visual representation learning via random convolu-
tions. In International Conference on Learning Rep-
resentations.

Yao-Yuan Yang, Cyrus Rashtchian, Hongyang Zhang,
Russ R Salakhutdinov, and Kamalika Chaudhuri.
2020. A closer look at accuracy vs. robustness. In
NeurIPS.

Mingyang Yi, Lu Hou, Jiacheng Sun, Lifeng Shang,
Xin Jiang, Qun Liu, and Zhiming Ma. 2021.
Im-
proved ood generalization via adversarial training
and pretraing. In International Conference on Ma-
chine Learning, pages 11987–11997. PMLR.

Alan L Yuille and Chenxi Liu. 2021. Deep nets: What
have they ever done for vision? International Jour-
nal of Computer Vision, 129(3):781–802.

Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin
Choi. 2019. From recognition to cognition: Visual
In IEEE Conference on
commonsense reasoning.
Computer Vision and Pattern Recognition, CVPR
2019, Long Beach, CA, USA, June 16-20, 2019,
pages 6720–6731. Computer Vision Foundation /
IEEE.

Rowan Zellers, Yonatan Bisk, Roy Schwartz, and
Yejin Choi. 2018. SWAG: A large-scale adversar-
ial dataset for grounded commonsense inference. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, pages 93–
104, Brussels, Belgium. Association for Computa-
tional Linguistics.

Vladimir N Vapnik and A Chervonenkis. 1991. The
necessary and sufﬁcient conditions for consistency
of the method of empirical risk minimization. Pat-
tern Recognition and Image Analysis, 1(3):284–305.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, December 4-
9, 2017, Long Beach, CA, USA, pages 5998–6008.

Hemanth Venkateswara,

Jose Eusebio,

Shayok
Chakraborty, and Sethuraman Panchanathan. 2017.
Deep hashing network for unsupervised domain
adaptation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages
5018–5027.

Riccardo Volpi, Hongseok Namkoong, Ozan Sener,
John C. Duchi, Vittorio Murino, and Silvio Savarese.
2018. Generalizing to unseen domains via adver-
In Advances in Neural
sarial data augmentation.
Information Processing Systems 31: Annual Con-
ference on Neural Information Processing Systems
2018, NeurIPS 2018, December 3-8, 2018, Mon-
tréal, Canada, pages 5339–5349.

Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel Bowman. 2018. Glue:
A multi-task benchmark and analysis platform for
In Proceedings
natural language understanding.
of the 2018 EMNLP Workshop BlackboxNLP: An-
alyzing and Interpreting Neural Networks for NLP,
pages 353–355.

Jason Wei and Kai Zou. 2019. EDA: Easy data aug-
mentation techniques for boosting performance on
In Proceedings of the
text classiﬁcation tasks.
2019 Conference on Empirical Methods in Natu-
ral Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), pages 6382–6388, Hong Kong,
China. Association for Computational Linguistics.

Adina Williams, Nikita Nangia, and Samuel Bowman.
2018. A broad-coverage challenge corpus for sen-
tence understanding through inference. In Proceed-
ings of the 2018 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume
1 (Long Papers), pages 1112–1122, New Orleans,
Louisiana. Association for Computational Linguis-
tics.

Mingzhu Wu, Naﬁse Sadat Moosavi, Andreas Rücklé,
and Iryna Gurevych. 2020. Improving qa generaliza-
tion by concurrent modeling of multiple biases. In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing: Findings,
pages 839–853.

Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong,
and Quoc Le. 2020. Unsupervised data augmenta-

