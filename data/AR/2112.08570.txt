Can Multilinguality beneﬁt Non-autoregressive Machine Translation?

Sweta Agrawal1 and Julia Kreutzer2 and Colin Cherry2
1Department of Computer Science, University of Maryland
2Google Research
sweagraw@umd.edu, {jkreutzer, colincherry}@google.com

Abstract

Non-autoregressive (NAR) machine transla-
tion has recently achieved signiﬁcant improve-
ments, and now outperforms autoregressive
(AR) models on some benchmarks, provid-
ing an efﬁcient alternative to AR inference.
However, while AR translation is often imple-
mented using multilingual models that beneﬁt
from transfer between languages and from im-
proved serving efﬁciency, multilingual NAR
models remain relatively unexplored. Taking
Connectionist Temporal Classiﬁcation (CTC)
as an example NAR model and Imputer as a
semi-NAR model (Saharia et al., 2020), we
present a comprehensive empirical study of
multilingual NAR. We test its capabilities with
respect to positive transfer between related lan-
guages and negative transfer under capacity
constraints. As NAR models require distilled
training sets, we carefully study the impact of
bilingual versus multilingual teachers. Finally,
we ﬁt a scaling law for multilingual NAR,
which quantiﬁes its performance relative to the
AR model as model scale increases.

1

Introduction

Non-autoregressive (NAR) models generate output
tokens in parallel instead of sequentially, achieving
signiﬁcantly faster inference speed that no longer
depends on sequence length. They depend heavily
on sequence-level knowledge distillation to reach
the quality of AR models (Gu et al., 2018). As
the notion of NAR has expanded to include semi-
NAR models that generate their outputs in multi-
ple steps, each time generating several tokens non-
autoregressively (Lee et al., 2018; Ghazvininejad
et al., 2019), we have begun to see cases where
NAR matches the quality of AR. Most prior works
have tested the performance of NAR models on a
handful of benchmarks of selected language pairs
like German (De), Chinese (Zh), Romanian (Ro).
To efﬁciently expand this set of languages, it makes

sense to begin exploring multilingual NAR transla-
tion models.

Multilingual machine translation models (Dong
et al., 2015; Johnson et al., 2017) translate between
multiple languages. They have better parameter-
efﬁciency than building one bilingual model per
language pair, and they are able to transfer knowl-
edge from high-resource languages to low-resource
ones. They have become an attractive solution for
expanding the language coverage of AR models
(Aharoni et al., 2019; Fan et al., 2021). The capa-
bility of doing multilingual modeling is a major
feature of the AR regime, and it is one that we
should seek to maintain in NAR models.

It is unclear to what extent the properties of mul-
tilingual AR models apply to NAR models. Do
related languages help each other (positive trans-
fer) as easily? Do unrelated languages interfere
with one another (negative transfer) to the same ex-
tent? Since NAR models tend to trade target-side
modeling for improved modeling of the source, the
answer to both questions is unclear. Furthermore,
NAR modeling raises a new issue of multilingual
distillation. To retain the training-time efﬁciency
of multilingual modeling, it is crucial that NAR
works well with multilingual teachers; otherwise,
the prospect of training many bilingual teachers
would greatly increase the effective training cost.
It may actually be the case that multilingual teach-
ers are better suited than bilingual ones, as the
effective capacity reduction may result in less com-
plex (Zhou et al., 2019) and less multimodal out-
puts (Gu et al., 2018).

We present an empirical study of multilingual
NAR modeling. Taking CTC (Libovický and Helcl,
2018) as our canonical NAR method, and Im-
puter (Saharia et al., 2020) as our canonical semi-
NAR model, we study how they respond to multi-
linguality in a 6-language scenario designed to em-
phasize negative transfer, as well as two-language
scenarios designed to emphasize positive transfer.

1
2
0
2
c
e
D
6
1

]
L
C
.
s
c
[

1
v
0
7
5
8
0
.
2
1
1
2
:
v
i
X
r
a

 
 
 
 
 
 
In doing so, we make the following contributions:

1. We show that multilingual NAR models suffer
more from negative transfer and beneﬁt less
from postive transfer than AR models.

2. We ﬁt a scaling law for our 6-language NAR
scenario, showing that this trend continues as
model size increases.

3. We demonstrate that multilingual NAR per-
forms equally well with multilingual and bilin-
gual teachers, even in scenarios where the mul-
tilingual teacher has lower BLEU.

Unfortunately, our results indicate that the time is
not quite right for multilingual NAR, as least for
the models studied here, but our analysis should
help future efforts in this space.

2 Non-Autoregressive Multilingual NMT

Let, Dl = (x, y) ∈ X × Y denote the bilingual cor-
pus of a language pair, l. Given an input sequence
x of length T ′, an AR model (Bahdanau et al.,
2015; Vaswani et al., 2017) predicts the target y
with length T sequentially based on the conditional
distribution p(yt ∣ y<t, x1∶T ′; θ). NAR models as-
sume conditional independence in the output token
space; that is, they model p(y1∶T ∣ x1∶T ′; φ). Due
to this conditional independence assumption, train-
ing NAR models directly on the true target distri-
bution leads to degraded performance (Gu et al.,
2018). Hence, NAR models are typically trained
with sequence-level knowledge distillation (Kim
and Rush, 2016) to reduce the modeling difﬁculty.

2.1 Non-Autoregressive NMT with CTC

In this work, we focus on NAR modelling via
CTC (Graves et al., 2006) due to its superior per-
formance on NAR generation and the ﬂexibility
of variable length prediction (Libovický and Helcl,
2018; Saharia et al., 2020; Gu and Kong, 2021).

CTC models an alignment a that provides a map-
ping between a sequence of predicted and target
tokens. Alignments can be constructed by inserting
special blank tokens ("_") and token repetitions into
the target sequence. The alignment is monotonic
with respect to the target sequence and is always
the same length as the source sequence x. How-
ever, in MT, the target sequence y can be longer
than the source sequence x. This is handled via
upsampling the source sequence x, to s times its
original length. An alignment is valid only if when

collapsed, i.e., merging repeated tokens and remov-
ing blank tokens, it results in the original target
sequence. The CTC loss marginalizes over all pos-
sible valid alignments Γ(y) compatible with the
target y and is deﬁned as:

p(y ∣ x) = ∑

a∈Γ(y)

p(a1∶T ′ ∣ x1∶T ′; φ).

Note that each alignment token at′ is modeled inde-
pendently. This conditional independence allows
CTC to predict the single most likely alignment
non-autoregressively at inference time, which can
then be efﬁciently collapsed to an output sequence.
This same independence assumption enables efﬁ-
cient minimization of the CTC loss via dynamic
programming (Graves et al., 2006). While CTC
enforces monotonicity between the alignment and
the target, it does not require any cross- or self-
attention layers inside the model to be monotonic.
Hence, CTC should still be able to model language
pairs with different word orders between the source
and the target sequence. Following Saharia et al.
(2020), we train encoder-only CTC models, using
a stack of self-attention layers to map the source
sequence directly to the alignments.

2.2

Iterative Decoding with Imputer

IMPUTER (Saharia et al., 2020) extends NAR
CTC modeling by iterative reﬁnement (Lee et al.,
2018). At each inference step, it conditions on
a previous partially generated alignment to emit
a new alignment. While IMPUTER, like CTC,
generates all tokens at each inference step, only
a subset of these tokens are selected to generate
a partial alignment, similar to iterative masking
approaches (Ghazvininejad et al., 2019). This is
achieved by training with marginalization over par-
tial alignments:

p(y ∣ x) = ∑

p(a ∣ aMask, x; φ),

a∈Γ(a)

where aMask is a partially masked input-alignment.
At training time, the aMask alignment is generated
using a CTC model trained on the same dataset,
and its masked positions are selected randomly.
This training procedure enables IMPUTER to it-
eratively reﬁne a partial alignment over multiple
decoding steps at inference time - consuming its
own alignments as input to the next iteration. With
k > 1 decoding steps, the IMPUTER becomes semi-
autoregressive, requiring k times more inference
passes than pure CTC models.

TGT WORD ORDER

SIZE

SCRIPT DIFFERENCE WHITE SPACE

SRC LENGTH TGT LENGTH

EN-KK

EN-DE
EN-PL
EN-HI

EN-JA
EN-RU
EN-FR

SOV

SVO/SOV
SVO
SOV

SOV
Free
SVO

150K

4.6M
5M
8.6M

17.9M
33.5M
38.1M

(cid:51)

(cid:55)
(cid:55)
(cid:51)

(cid:51)
(cid:51)
(cid:55)

(cid:51)

(cid:51)
(cid:51)
(cid:51)

(cid:55)
(cid:51)
(cid:51)

26.7

25.7
16.2
18.3

21.4
23.2
29.2

20.0

24.3
14.6
19.8

25.9
21.5
32.8

Table 1: Details on training data used. Target word orders are the ones that are dominating within the language
according to (Dryer and Haspelmath, 2013), but there may be sentence-speciﬁc variations. English follows pre-
dominantly SVO (Subject-Verb-Object) order. Size is measured as the number of parallel sentences in the training
data. Source (Src) and Target (Tgt) length are averaged across sentences after word-based tokenization.

IMPUTER differs from Conditional Masked Lan-
guage Modeling (CMLM) (Ghazvininejad et al.,
2019) in that it utilizes the CTC loss instead of
the standard cross-entropy loss, removing the need
for explicit output length prediction. Also, IM-
PUTER is an encoder-only model that makes one
prediction per source token, just like CTC. The
cross-attention component from encoder-decoder is
replaced by a simple sum between the embeddings
of the source sequence and the input alignment
(aMask) before the ﬁrst self-attention layer.1

2.3 Multilingual Modeling

l=1.

Multilingual MT (Dong et al., 2015; Johnson et al.,
2017) extends bilingual MT by training a sin-
gle model with datasets from multiple language
pairs, {Dl}L
To enable multilingual mod-
elling in both AR and NAR models, we prepend
each source sequence with the desired target lan-
guage tag (<2tgt>) and generate a shared vocab-
ulary across all languages (Johnson et al., 2017).
The model encodes this tag as any other vocabulary
token, and can use this to guide the generation of
the output sequence in the desired target language.

2.4 Efﬁciency

Inference We refrain from wallclock inference
time measurements since these are dependent on
implementation, low-level optimization and ma-
chines (Dehghani et al., 2021), and instead com-
pare generation speed in terms of the number of
tokens that get generated per iteration (Kreutzer
et al., 2020), which is < 1 for AR models,2 N for

1We experimented with an encoder-decoder variant of IM-
PUTER but it did not change the overall output quality in
multilingual scenarios or otherwise.

21 for greedy search, < 1 to account for multiple hypothe-
ses that have to be considered and expanded in every step for
beam search.

fully non-autoregressive models like CTC and k
for iterative semi-autoregressive models like IM-
PUTER. We acknowledge that other factors like
model-depth play a role for inference time, but
we assume that both NAR and AR models can be
optimized for this aspect (Kasai et al., 2020).

Training At training time, NAR models are less
efﬁcient than AR models because their quality de-
pends on distillation (Gu and Kong, 2021). Extra
cost is incurred to train a teacher model (usually
AR) and to use it to decode the training set.

Multilinguality As discussed above, multilin-
gual models have the advantage of multi-tasking
over language pairs, so that a single multilin-
gual model can replace several bilingual models.
Thanks to transfer across languages, model size
usually needs to be increased less than m-fold for
modeling m languages instead of a single one.

Considering all of the above factors, an ideal
model requires only a few iterations (decoder
passes or steps), requires no teacher, and covers sev-
eral languages, while incurring the smallest drop
in quality compared to less efﬁcient models. CTC
is desirable as it uses only one pass, while Imputer
gives up some efﬁciency to improve quality. Both
require a teacher, but we can try to reduce teacher
training costs through distillation.

3 Experimental Setup

Data We perform our experiments on six lan-
guage pairs, translating from English into WMT-14
German (de)3, WMT-15 French (fr)4, WMT-19

3http://www.statmt.org/wmt14/

translation-task.html

4http://www.statmt.org/wmt15/

translation-task.html

TEACHER

Ngen EN-FR EN-DE EN-PL EN-RU EN-HI EN-JA AVG.

MODEL

AR-big
multi-AR-big

Bilingual Models

AR-base

CTC

AR-big
multi-AR-big

IMPUTER

AR-big

Multilingual Models

multi-AR-base

< 1 38.8
38.5

< 1 38.2

N

8

35.7
35.1

38.5

< 1 35.2

CTC

IMPUTER

AR-big
multi-AR-big

AR-big
multi-AR-big

N

8

31.6
31.2

34.4
34.1

29.0
27.0

27.6

25.2
24.0

27.2

24.8

20.5
20.5

22.8
21.2

21.4
21.6

27.2
25.3

34.6
32.6

35.4
33.6

21.2

18.0
17.7

21.2

19.7

13.0
13.7

14.9
16.4

26.2

21.4
20.8

25.6

23.2

17.7
18.0

21.3
21.7

33.8

31.6
30.8

32.0

30.8

28.2
27.8

29.9
29.9

34.8

31.6
28.9

32.0

31.2

28.1
27.5

29.6
27.9

31.1
29.3

30.3

27.3
26.2

29.4

27.5

23.2
23.1

25.5
25.2

Table 2: Multilingual and Bilingual AR and NAR models trained on English → X direction.

Russian (ru)5, WMT-20 Japanese (ja), WMT-20
Polish (pl)6 and Samanantar Hindi (hi) (Ramesh
et al., 2021). We also use WMT-19 English-Kazakh
(kk)7 in Section 5. The sizes and properties of the
datasets are listed in Table 1. Target word order
and the writing script differ across these language
pairs. We consider translating from English as this
is more interesting and difﬁcult direction.

We use SentencePiece (Kudo and Richardson,
2018) to generate a shared subword vocabulary for
the source and target language pairs. The propor-
tion of sub-words allocated for each language de-
pends on the size of the language in the combined
training data.

Evaluation Metrics We evaluate translation
quality via BLEU (Papineni et al., 2002) as cal-
culated by Sacrebleu (Post, 2018). For En-Ja we
measure Character-level BLEU to be independent
of speciﬁc tokenizers.

Architecture We train the IMPUTER model us-
ing the same setup as described in Saharia et al.
(2020): We follow their base model with dmodel =
512, dhidden = 2048, nheads = 8, nlayers = 12, and
pdropout = 0.1. AR models follow Transformer-
base (Vaswani et al., 2017) and have similar pa-

5http://www.statmt.org/wmt19/

translation-task.html

6https://www.statmt.org/wmt20/

translation-task.html

7https://www.statmt.org/wmt19/

translation-task.html

rameter counts. We train both models using Adam
with learning rate of 0.0001. We train CTC mod-
els with a batch size of 2048 and 8192 sentences
for 300K steps for the bilingual and multilingual
models respectively. We train the IMPUTER us-
ing CTC loss using a Bernoulli masking policy for
next 300K steps with a batch size of 1024 and 2048
sentences for the bilingual and multilingual models
respectively. We upsample the source sequence
by a factor of 2 for all our experiments.8 We pick
the best checkpoint based on validation BLEU for
bilingual models and use the last checkpoint for
multilingual models.

Distillation We apply sequence-level knowledge
distillation (Kim and Rush, 2016) from AR teacher
models as widely used in NAR generation (Gu
et al., 2018). Speciﬁcally, when training the NAR
models, we replace the reference sequences during
training with translation outputs from Transformer-
Big AR teacher model with beam = 4. We also
report the quality of the AR teacher models. We
experiment with two types of teachers, bilingual
and multilingual.

4 Negative Transfer Scenario

Our main experiment compares English-to-X mod-
els for the six high-resource languages in Table 1.

8While increasing the upsampling ratio can provide a larger
alignment space, we do not vary the upsampling ratio due to
small difference in the performance of the resulting NAR
models (See Table 6, Gu and Kong (2021)).

These languages are typologically diverse, and each
have enough data so that we do not expect them
to beneﬁt substantially from positive transfer. We
use this scenario to test the impact of multilingual
teachers, and to measure each paradigm’s ability
to model several unrelated languages. Results are
shown in Table 2.

4.1 Multilingual Teacher Comparison

The top two rows of Table 2 show that in this neg-
ative transfer scenario, multilingual teachers have
substantially reduced BLEU compared to bilingual
teachers. However, as we look at the impact on
bilingual students, we see that CTC models trained
from the multilingual teacher, multi-AR-big,
do not reﬂect the entirety of this drop in teacher
quality when compared to training with the bilin-
gual AR-big. An average teacher gap of −1.8
BLEU is mapped to −1.1 in the corresponding
students. The comparison becomes more inter-
esting as we shift to multilingual students: mul-
tilingual CTC does not suffer at all from having a
multilingual teacher (average BLEU gap of −0.1),
and multilingual Imputer likewise suffers very lit-
tle (−0.3). These three results taken together sug-
gest that datasets distilled from multilingual mod-
els are likely simpler and easier to model non-
autoregressively, which makes up for their lower
BLEU. We explore this hypothesis further in Sec-
tion 4.3. We hope that highly multilingual models,
trained with similar target language pairs to exhibit
positive transfer (Tan et al., 2019), might be yet
better suited to serve as teachers for multilingual
NAR models, which we leave to future work.

4.2 Multilingual Model Comparison

Returning to the “Bilingual Models” section of Ta-
ble 2 with AR-big teachers, we can see that we
have reproduced the expected results of Saharia
et al. (2020). Bilingual CTC does well for a fully
NAR method, but does not come close to AR qual-
ity. IMPUTER ably closes the gap with AR, sur-
passing or coming within 0.2 BLEU of the AR-
base models on 3 out of 6 language pairs, with the
largest gap in performance for the distant En-Ja.
Does this story hold as we move to multilingual
NAR students?

To understand each model’s multilingual capa-
bilities, we can compare its bilingual performance
to its multilingual performance. Comparing AR-
base to multilingual AR-base gives us a baseline
average drop of −2.8 BLEU, conﬁrming that this

is indeed a difﬁcult multilingual scenario that leads
to negative transfer. Comparing bilingual CTC to
multilingual CTC, both with AR-big teachers, we
see an average drop of −4.1. This larger drop indi-
cates that CTC suffers more from negative transfer
than its AR counterpart. We hypothesize that CTC
needs more capacity compared to the AR model
to achieve similar multilingual performance, mo-
tivating our scaling law experiments in Section 6.
Performing the same bilingual-to-multilingual com-
parison for IMPUTER shows a similar −3.9 average
drop due to negative transfer. So although IM-
PUTER is indeed substantially better than CTC, it
does not seem to be necessarily better suited for
multilingual modeling in this difﬁcult scenario.

4.3 How do the distilled datasets differ?

Table 3 summarizes different statistics for the orig-
inal (R) and distilled datasets from both multilin-
gual (M ) and bilingual (B) AR teacher models.
We report the number of types and average se-
quence length (in tokens) for the target side of
the dataset. We compute the complexity of the
dataset based on probabilities from a statistical
word aligner (Zhou et al., 2019). The FRS (Talbot
et al., 2011) score represents the average fuzzy re-
ordering score over all the sentence pairs for the
respective language pair as measured in Xu et al.
(2021), with higher values suggesting that the target
is more monotonic with the source sequence. We
also report BLEU for the distilled datasets relative
to the original training corpora.

The datasets distilled from the bilingual AR
models (B) are shorter, less complex, have reduced
lexical diversity (in number of types) and are more
monotonic compared to the original corpora (R),
which is aligned with prior work (Zhou et al., 2019;
Xu et al., 2021). Interestingly, for En-Ja, we ob-
serve that the distilled datasets are less monotonic
than the original corpora.

The multilingual distilled datasets (M ) have fur-
ther reduced types, are shorter and less complex
than the distilled datasets from bilingual teach-
ers. The resulting distilled datasets from the multi-
lingual teacher model speciﬁcally have increased
monotonicity (FRS) for the more distant language
pairs, Japanese and Hindi. As shown in Xu et al.
(2021), the reduced lexical diversity and reordering
complexity both help NAR learn better alignment
between source and target, improving the transla-
tion quality of the outputs.

PROPERTY

R

B

M

4.4 Error Analysis

# TYPES

AVG. LENGTH

COMPLEXITY
FRS

BLEU (Train)

# TYPES

AVG. LENGTH

COMPLEXITY
FRS

BLEU (Train)

# TYPES

AVG. LENGTH

COMPLEXITY
FRS

BLEU (Train)

# TYPES

AVG. LENGTH

COMPLEXITY
FRS

BLEU (Train)

# TYPES

AVG. LENGTH

COMPLEXITY
FRS

BLEU (Train)

EN-FR

522K 430K 396K
29.2
31.2
32.8

1.529
0.463

1.167
0.541

0.944
0.536

-

40.8

37.8

EN-DE

812K 616K 573K
22.2
23.4
24.3

1.243
0.490

0.819
0.606

0.709
0.605

-

35.0

26.4

EN-PL

636K 516K 503K
12.7
13.4
14.6

1.435
0.590

0.942
0.678

0.591
0.695

-

26.3

22.0

EN-RU

636K 516K 503K
19.5
20.5
21.5

1.083
0.640

0.882
0.719

0.819
0.716

-

43.2

40.0

EN-HI

346K 200K 185K
17.8
18.8
19.8

1.438
0.347

1.256
0.363

1.138
0.366

-

34.6

28.0

EN-JA

# TYPES

547K 440K 402K

AVG. LENGTH

25.9

23.5

22.2

COMPLEXITY
FRS

BLEU (Train)

1.541
0.344

1.369
0.337

1.338
0.340

-

35.9

30.6

Table 3: Comparison of datasets distilled from Bilin-
gual (B) or Multilingual (M) AR models on a subset of
1M samples: Multilingual distilled datasets have fewer
types, are less complex and more monotonic than bilin-
gual distilled datasets, despite having lower BLEU.

In this section, we present a qualitative analysis to
provide some insights on how NAR models differ
in output quality across different language pairs
when trained in isolation (bilingual) or with other
language pairs (multilingual).

Figure 1: Brevity penalty (BP) scores for all models
for all the language pairs. “-B” and “-M” and bilingual
and multilingual models respectively.

Effect of length We show the brevity penalty
scores from all the languages in Figure 1. Among
all the language pairs, both en-pl and en-ja have
lowest brevity penalty scores. This could be at-
tributed to the subject pronouns being dropped in
both of these target languages. Multilingual model-
ing of most of the language pairs results in shorter
outputs relative to bilingual models for both AR
and NAR models. While IMPUTER is generally
able to improve the low brevity penalty values com-
pared to CTC models, they still lag behind AR
models, suggesting that length of the output might
need to be controlled explicitly for these language
pairs (Gu and Kong, 2021).

Invalid Words Our manual inspection suggested
that CTC frequently generates invalid words —
tokens that are not present in the target side of the
training set or in the test set references. In the Hindi
example below, the invalid (or made-up) word in
the sentence is marked in red.

We compute the percentage of sequences that
include at least one invalid word and report the
statistics in Figure 2. CTC generates many invalid
words compared to both AR (Average: 0.09-0.14)
and IMPUTER (Average: 0.14-0.37), with multi-
lingual modeling leading to an average increase
in invalid words by 37%. We attribute this to the
limited vocabulary of the model resulting in longer
subword segmentation and the conditional inde-
pendence assumption leading to unrelated adjacent

subwords, which merge to create invalid words.

MODEL

EN-DE

EN-FR

Bilingual Models

AR
CTC
IMPUTER

22.8
21.5
22.8

27.7
26.5
28.1

Multilingual Models
AR
CTC
IMPUTER

24.3 +1.5 29.0 +1.3
22.1 +0.6 26.9 +0.4
23.7 +1.3 28.5 +0.4

Table 4: Results on subsampled (1M) training dataset
for German and French.

ru = 0.75, p1/T

ru = 0.56, p1/T

ru = 0.995, p1/T

where, pl = Dl
. We experiment with multiple
∑k Dk
temperature values: 1, 3, 5, 10, 20, where T = 1
implies p1/T
kk = 0.005 and T = 20
results in approximately, p1/T
kk = 0.45.
The best performance on validation set was using
T = 5 (p1/T

kk = 0.25).
As can be seen in Table 5, both AR and CTC
beneﬁt from positive transfer when translating into
Kazakh when trained with Russian. The CTC-M
model is able to improve (BLEU: +1.6) over the
CTC-B model but the overall quality of the outputs
is very low compared to the teacher model (BLEU:
-5.3). It highlights that current NAR models do not
perform well on very low-resource language pairs
and might beneﬁt from additional data augmenta-
tion strategies in addition to transfer from other
similar language pairs.9

MODEL

PRISM

TEACHER EN-KK EN-RU

-

8.9

27.0

Bilingual Models
AR
CTC
Multilingual Models
AR
CTC

PRISM

PRISM

4.4
1.2

7.1
2.8

-
-

26.0
20.4

Table 5: Results on English → Kazakh.

6

Impact of Model Scale

Prior work has studied scaling laws for MT to un-
derstand the relationship between the output quality

9We do not train IMPUTER for Kazakh as the quality of
both the distilled datasets and generated alignments from CTC
is very low.

Figure 2: % Invalid words observed in the outputs from
all languages.“-B” and “-M” and bilingual and multilin-
gual models respectively.

5 Positive Transfer Scenario

In this section we present two experimental setups
designed to study positive transfer for similar lan-
guage pairs, where one or both languages have less
than the ideal amount of data.

English→{German, French} To better isolate
the effect of transfer via multilingual modelling,
we simulate a resource-limited scenario by subsam-
pling 1M samples each from German and French
and train bilingual and multilingual models for both
AR and NAR paradigms. By pairing two related
languages and subsampling down to smaller dataset
sizes, we relax the capacity bottleneck and compe-
tition amongst the languages for parameters.

Table 4 shows that NAR models beneﬁt from
training with multilingual language pairs in the
resource-constrained scenario — all models exhibit
positive transfer. IMPUTER achieves higher posi-
tive transfer than CTC across both the language
pairs, but lags behind the AR multilingual model
in en-fr. Note however that, for en-fr, the bilin-
gual IMPUTER is already ahead of the bilingual
AR model by 0.4 BLEU.

English→{Russian, Kazakh} We test the perfor-
mance of the multilingual NAR model on the low-
resource scenario of En→Kk, where there is not
sufﬁcient clean training data to train an AR model
in the ﬁrst place. We instead distill datasets from
the publicly available multilingual autoregressive
model, PRISM (Thompson and Post, 2020). We
then pair with the high-resource Russian to encour-
age positive transfer to Kazakh. Given the huge
difference in dataset sizes for Russian and Kazakh
(see Table 1), we sample training data from the two
languages based on the dataset size scaled by a tem-
perature value (T), p1/T
(Arivazhagan et al., 2019),

l

(BLEU), the cross-entropy loss and the number of
parameters used for training the model (Ghorbani
et al., 2021; Gordon et al., 2021). Based on our
hypothesis that CTC might require more capacity
than AR models, we study the relationship between
the number of parameters used for training the mod-
els and the development BLEU averaged across all
the language pairs.

We derive the relationship between BLEU and
the number of parameters (N ) directly from the
scaling laws proposed in Gordon et al. (2021) and
Ghorbani et al. (2021) as follows:
L(N ) ≈ L0 + αn(1/N )αk (Ghorbani et al., 2021)
BLEU(L) ≈ Ce−kL (Gordon et al., 2021)

BLEU(N ) ≈ ae−b(1/N )

c

(this work)

where L is the test loss, {αn, αk, L0, C, k} are
ﬁtted parameters from previous power laws, and
{a, b, c} are the collapsed ﬁtted parameters of our
power law. Ghorbani et al. (2021)’s L0 corresponds
to the irreducible loss of the data, which becomes
a in our formulation.

Setup We train seven different models with vary-
ing capacity using uniform scaling for both AR
and CTC models (see below). We use the same
number of layers and model dimension and train
both AR and CTC models on distilled datasets
from bilingual teacher, AR-big, to make a fair
comparison.10

# of Layers
6
6
12
12
24
12
24

Model Size
128
256
256
512
512
1024
1024

Table 6: The feed-forward size is 4 times the size of the
model. All AR models have equal number of encoder
and decoder layers. The number of attention heads is
given by (8/(512/M odelSize)).

Results Figure 3 shows the ﬁtted parameters us-
ing the model derived using scaling laws respec-
tively — the scaling-law based model is almost able

10We acknowledge that this setup might still be unfair to
both CTC and AR models as the former has architectural dif-
ferences (e.g. lack of cross-attention and distinction between
encoding and decoding layers), whereas AR models might be
limited by the quality of their teacher models.

to perfectly describe the relationship between the
number of parameters and the development BLEU
(R2 AR: 0.99). When the number of parameters
are less than 10M, both AR and CTC model result
in approximately similar quality outputs. However,
as the number of parameters increases, the gap in
BLEU also increases, suggesting that with sufﬁ-
cient number of parameters AR models are able to
generate higher quality outputs due to the condi-
tional dependence between tokens. We can also see
that CTC needs many more parameters to achieve
comparable BLEU to AR models and plateaus
early at a BLEU of 26.7, while AR models plateau
at 30.8. By projecting the curves out to 1 billion
parameters, we can show that increasing the capac-
ity of NAR is insufﬁcient to reach the quality of
AR models.

(a) Scaling law ﬁt (R2 AR: 0.99, R2 AR: 0.99).

Figure 3: Development BLEU versus number of param-
eters in millions, with ﬁtted power-law curves. CTC re-
quires relatively more parameters to achieve the same
performance as AR models.

7 Related Work

Non-Autoregressive MT Multiple approaches
with varying architectures (Gu et al., 2018, 2019;
Chan et al., 2020; Xu and Carpuat, 2021), custom
loss functions (Ghazvininejad et al., 2020; Du et al.,
2021) and training strategies (Ghazvininejad et al.,
2019; Qian et al., 2021) have been used to enable
parallel generation of output tokens for MT. While
most of the prior work focuses on better utiliza-
tion of distilled datasets, we focus on evaluating
and understanding the impact of using multilingual
distilled datasets for NAR training.

Multilingual MT There has been a lot of in-
terest in the AR literature on understanding and
proposing models that enable translation between

more than two language pairs (Dabre et al., 2020).
Both supervised and unsupervised (Sun et al., 2020)
learning in MT have beneﬁtted from training with
multiple languages, especially those that have very
litte (Siddhant et al., 2020) to no training data
(Zhang et al., 2020). However, multilingual mod-
elling has not yet received any attention in the NAR
literature. Concurrent to our work, Anonymous
(2022) investigate a non-autoregressive multilin-
gual machine translation model with a code-switch
decoder. They show that adding code-switched
back-translation data to the training of multilingual
models improves performance. Our work instead
focuses on understanding multilinguality for both
the student and the teacher model in the context
of NAR training without using any additional data
augmentation strategies.

Distillation Sequence-level knowledge distilla-
tion is one of the key ingredient in the training of
NAR models. Recent works have focused on un-
derstanding the success of knowledge distillation
in NAR training. Zhou et al. (2019) show that dis-
tilled datasets have reduced complexity compared
to original bitext which is suitable for NAR train-
ing. Xu et al. (2021) further show that different
types of complexity, i.e. reducing lexical diversity
and reordering degree have different impacts on
the training. Voita et al. (2021) argue that the com-
plexity of the dataset increases as the training of
the AR model progresses and use this to improve
the performance of NAR model by distilling from
an earlier checkpoint. In this work, we focus on un-
derstanding the impact on quality and complexity
of distilled datasets from multilingual and bilingual
AR teacher models.

Scaling Laws While large scale models improve
performance, it is practically impossible, time con-
suming and expensive to train the different vari-
ants of the model given different architectures and
dataset sizes based on the amount of compute avail-
able. Recent works have derived empirical scaling
laws that govern the relationship between the per-
formance of the model and these factors (Kaplan
et al., 2020; Hernandez et al., 2021; Bahri et al.,
2021; Gordon et al., 2021; Ghorbani et al., 2021).
However, these scaling laws have not yet been stud-
ied for multilingual MT which we explore in our
work.

8 Conclusion

While the community has achieved signiﬁcant
progress in improving the performance of NAR
and semi-NAR models, the focus on benchmark-
ing on a few language pairs has impeded our un-
derstanding of how these models might general-
ize to multilingual settings. The need for distilled
datasets from AR models limits the ceiling on trans-
lation quality while adding the expense of train-
ing additional models to translate the training set.
We have shown that generating distilled datasets
from a multilingual model is a good alternative,
and have provided insights on how multilingual
teachers might differ from bilingual ones in qual-
ity and complexity. But as we show in our analy-
sis, generating sequences of the right length and
with valid tokens remains a challenge, and one that
only grows as we move to either multilingual mod-
eling or to more distant language pairs. We are
far from using off-the-shelf models to enable non-
autoregressive generation in resource constrained
scenario even with positive transfer from similar
languages. Moving forward, we need to improve
the coverage of language pairs studied in order
to set better expectations from multilingual NAR
models, and to better understand why our multilin-
gual models are unable to reach the quality of their
teachers, even with increased capacity.

References

Roee Aharoni, Melvin Johnson, and Orhan Firat. 2019.
Massively multilingual neural machine translation.
In Proceedings of NAACL-HLT, pages 3874–3884.

Anonymous. 2022. Non-autoregressive models are bet-
In Submitted to The
ter multilingual translators.
Tenth International Conference on Learning Repre-
sentations. Under review.

Naveen Arivazhagan, Ankur Bapna, Orhan Firat,
Dmitry Lepikhin, Melvin Johnson, Maxim Krikun,
Mia Xu Chen, Yuan Cao, George Foster, Colin
Cherry, et al. 2019. Massively multilingual neural
machine translation in the wild: Findings and chal-
lenges. arXiv preprint arXiv:1907.05019.

Dzmitry Bahdanau, Kyung Hyun Cho, and Yoshua
Bengio. 2015.
Neural machine translation by
jointly learning to align and translate. In 3rd Inter-
national Conference on Learning Representations,
ICLR 2015.

Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon
Lee, and Utkarsh Sharma. 2021. Explaining neural
scaling laws. arXiv preprint arXiv:2102.06701.

William Chan, Chitwan Saharia, Geoffrey Hinton, Mo-
hammad Norouzi, and Navdeep Jaitly. 2020.
Im-
puter: Sequence modelling via imputation and dy-
In International Conference
namic programming.
on Machine Learning, pages 1403–1413. PMLR.

Raj Dabre, Chenhui Chu, and Anoop Kunchukut-
tan. 2020. A survey of multilingual neural ma-
chine translation. ACM Computing Surveys (CSUR),
53(5):1–38.

Mostafa Dehghani, Anurag Arnab, Lucas Beyer,
Ashish Vaswani, and Yi Tay. 2021. The efﬁciency
misnomer. arXiv preprint arXiv:2110.12894.

Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, and
Haifeng Wang. 2015. Multi-task learning for mul-
In Proceedings of the
tiple language translation.
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), pages 1723–1732, Beijing,
China. Association for Computational Linguistics.

Matthew S. Dryer and Martin Haspelmath, editors.
2013. WALS Online. Max Planck Institute for Evo-
lutionary Anthropology, Leipzig.

Cunxiao Du, Zhaopeng Tu,

Jiang.
2021.
Order-agnostic cross entropy for non-
autoregressive machine translation. arXiv preprint
arXiv:2106.05093.

Jing

and

Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi
Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep
Baines, Onur Celebi, Guillaume Wenzek, Vishrav
Chaudhary, et al. 2021. Beyond english-centric mul-
tilingual machine translation. Journal of Machine
Learning Research, 22(107):1–48.

Marjan Ghazvininejad, Vladimir Karpukhin, Luke
Zettlemoyer, and Omer Levy. 2020. Aligned cross
entropy for non-autoregressive machine translation.
CoRR, abs/2004.01655.

Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and
Luke Zettlemoyer. 2019. Mask-predict: Parallel de-
coding of conditional masked language models. In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP), pages 6112–
6121.

Behrooz Ghorbani, Orhan Firat, Markus Freitag, Ankur
Bapna, Maxim Krikun, Xavier Garcia, Ciprian
Scaling laws
Chelba, and Colin Cherry. 2021.
arXiv preprint
for neural machine translation.
arXiv:2109.07740.

Mitchell A Gordon, Kevin Duh, and Jared Kaplan.
2021. Data and parameter scaling laws for neural
In Proceedings of the 2021
machine translation.
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 5915–5922.

Alex Graves, Santiago Fernández, Faustino Gomez,
Connectionist
and Jürgen Schmidhuber. 2006.
temporal classiﬁcation:
labelling unsegmented se-
quence data with recurrent neural networks. In Pro-
ceedings of the 23rd international conference on Ma-
chine learning, pages 369–376.

Jiatao Gu, James Bradbury, Caiming Xiong, Vic-
tor O.K. Li, and Richard Socher. 2018. Non-
autoregressive neural machine translation. In Inter-
national Conference on Learning Representations.

Jiatao Gu and Xiang Kong. 2021.

Fully non-
autoregressive neural machine translation: Tricks of
the trade. In Findings of the Association for Compu-
tational Linguistics: ACL-IJCNLP 2021, pages 120–
133, Online. Association for Computational Linguis-
tics.

Jiatao Gu, Changhan Wang,

and Junbo Zhao.
2019. Levenshtein transformer.
In H. Wallach,
H. Larochelle, A. Beygelzimer, F. d'Alché-Buc,
E. Fox, and R. Garnett, editors, Advances in Neural
Information Processing Systems 32, pages 11179–
11189. Curran Associates, Inc.

Danny Hernandez, Jared Kaplan, Tom Henighan, and
Sam McCandlish. 2021. Scaling laws for transfer.
arXiv preprint arXiv:2102.01293.

Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim
Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,
Fernanda Viégas, Martin Wattenberg, Greg Corrado,
Macduff Hughes, and Jeffrey Dean. 2017. Google’s
multilingual neural machine translation system: En-
abling zero-shot translation. Transactions of the As-
sociation for Computational Linguistics, 5:339–351.

Jared Kaplan, Sam McCandlish, T. J. Henighan, Tom B.
Brown, Benjamin Chess, Rewon Child, Scott Gray,
Alec Radford, Jeff Wu, and Dario Amodei. 2020.
Scaling laws for neural language models. ArXiv,
abs/2001.08361.

Jungo Kasai, Nikolaos Pappas, Hao Peng, James Cross,
and Noah Smith. 2020. Deep encoder, shallow
decoder: Reevaluating non-autoregressive machine
In International Conference on Learn-
translation.
ing Representations.

Yoon Kim and Alexander M. Rush. 2016. Sequence-
level knowledge distillation. In Proceedings of the
2016 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1317–1327, Austin,
Texas. Association for Computational Linguistics.

Julia Kreutzer, George Foster, and Colin Cherry. 2020.
Inference strategies for machine translation with
In Proceedings of the 2020
conditional masking.
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 5774–5782, On-
line. Association for Computational Linguistics.

Taku Kudo and John Richardson. 2018. Sentencepiece:
A simple and language independent subword tok-
enizer and detokenizer for neural text processing. In
EMNLP (Demonstration).

Jason Lee, Elman Mansimov, and Kyunghyun Cho.
2018. Deterministic non-autoregressive neural se-
In Pro-
quence modeling by iterative reﬁnement.
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1173–
1182, Brussels, Belgium. Association for Computa-
tional Linguistics.

Jindˇrich Libovický and Jindˇrich Helcl. 2018. End-to-
end non-autoregressive neural machine translation
In Pro-
with connectionist temporal classiﬁcation.
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing, pages 3016–
3021, Brussels, Belgium. Association for Computa-
tional Linguistics.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th annual meeting of the Association for Compu-
tational Linguistics, pages 311–318.

Matt Post. 2018. A call for clarity in reporting BLEU
scores. In Proceedings of the Third Conference on
Machine Translation: Research Papers, pages 186–
191, Brussels, Belgium. Association for Computa-
tional Linguistics.

Lihua Qian, Hao Zhou, Yu Bao, Mingxuan Wang, Lin
Qiu, Weinan Zhang, Yong Yu, and Lei Li. 2021.
Glancing transformer for non-autoregressive neural
machine translation. In Proceedings of the 59th An-
nual Meeting of the Association for Computational
Linguistics and the 11th International Joint Confer-
ence on Natural Language Processing (Volume 1:
Long Papers), pages 1993–2003, Online. Associa-
tion for Computational Linguistics.

Gowtham Ramesh, Sumanth Doddapaneni, Aravinth
Bheemaraj, Mayank Jobanputra, Raghavan AK,
Ajitesh Sharma, Sujit Sahoo, Harshita Diddee, Di-
vyanshu Kakwani, Navneet Kumar, et al. 2021.
Samanantar: The largest publicly available parallel
corpora collection for 11 indic languages. arXiv
preprint arXiv:2104.05596.

Chitwan Saharia, William Chan, Saurabh Saxena, and
Mohammad Norouzi. 2020. Non-autoregressive ma-
chine translation with latent alignments. In Proceed-
ings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
1098–1108.

Aditya Siddhant, Ankur Bapna, Yuan Cao, Orhan Fi-
rat, Mia Chen, Sneha Kudugunta, Naveen Arivazha-
gan, and Yonghui Wu. 2020. Leveraging mono-
lingual data with self-supervision for multilingual
In Proceedings of the
neural machine translation.
58th Annual Meeting of the Association for Compu-
tational Linguistics, pages 2827–2835, Online. As-
sociation for Computational Linguistics.

Meeting of the Association for Computational Lin-
guistics, pages 3525–3535, Online. Association for
Computational Linguistics.

David Talbot, Hideto Kazawa, Hiroshi Ichikawa, Jason
Katz-Brown, Masakazu Seno, and Franz Josef Och.
2011. A lightweight evaluation framework for ma-
chine translation reordering. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 12–21.

Xu Tan, Jiale Chen, Di He, Yingce Xia, Tao Qin, and
Tie-Yan Liu. 2019. Multilingual neural machine
translation with language clustering. In Proceedings
of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th Interna-
tional Joint Conference on Natural Language Pro-
cessing (EMNLP-IJCNLP), pages 963–973.

Brian Thompson and Matt Post. 2020. Paraphrase gen-
eration as zero-shot multilingual translation: Disen-
tangling semantic similarity from lexical and syntac-
tic diversity. In Proceedings of the Fifth Conference
on Machine Translation, pages 561–570, Online. As-
sociation for Computational Linguistics.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
In Advances in neural information pro-
you need.
cessing systems, pages 5998–6008.

Elena Voita, Rico Sennrich, and Ivan Titov. 2021. Lan-
guage modeling, lexical translation, reordering: The
training process of NMT through the lens of classi-
cal SMT. In Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing,
pages 8478–8491, Online and Punta Cana, Domini-
can Republic. Association for Computational Lin-
guistics.

Weijia Xu and Marine Carpuat. 2021. Editor: An edit-
based transformer with repositioning for neural ma-
chine translation with soft lexical constraints. Trans-
actions of the Association for Computational Lin-
guistics, 9:311–328.

Weijia Xu, Shuming Ma, Dongdong Zhang, and Ma-
rine Carpuat. 2021. How does distilled data com-
plexity impact the quality and conﬁdence of non-
autoregressive machine translation? In Findings of
the Association for Computational Linguistics: ACL-
IJCNLP 2021, pages 4392–4400, Online. Associa-
tion for Computational Linguistics.

Biao Zhang, Philip Williams, Ivan Titov, and Rico Sen-
nrich. 2020. Improving massively multilingual neu-
ral machine translation and zero-shot translation. In
Proceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1628–
1639, Online. Association for Computational Lin-
guistics.

Haipeng Sun, Rui Wang, Kehai Chen, Masao Utiyama,
Eiichiro Sumita, and Tiejun Zhao. 2020. Knowledge
distillation for multilingual unsupervised neural ma-
chine translation. In Proceedings of the 58th Annual

Chunting Zhou,

Jiatao Gu, and Graham Neubig.
2019. Understanding knowledge distillation in non-
autoregressive machine translation. In International
Conference on Learning Representations.

