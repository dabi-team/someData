0
2
0
2
c
e
D
2
2

]

R
P
.
h
t
a
m

[

2
v
6
2
3
9
0
.
2
1
0
2
:
v
i
X
r
a

Mathematical modelling of the performance of a student in
non-collaborative and non-presential learning

A.R. Sagaceta-Mej´ıa(1), J. A. Fres´an-Figueroa(2) and E.M. Mart´ın-Gonz´alez.(3)
(1) Departamento de F´ısica y Matem´aticas, Universidad Iberoamericana, M´exico
e-mail: alma.sagaceta@ibero.mx,
(2) Departamento de Matem´aticas Aplicadas y Sistemas,
Universidad Aut´onoma Metropolitana, Unidad Cuajimalpa, M´exico,
e-mail: jfresan@cua.uam.mx,
(3) Departamento de Matem´aticas, Universidad de Guanajuato, M´exico
e-mail: ehyter.martin@ugto.mx

November 16, 2021

Abstract

In this paper we propose a model to study the appropriation of knowledge of one student in a non-
collaborative online class. We formulate a stochastic model based on the quality of the teacher’s class
and the aﬃnity of the student to understand the sessions, under the assumption that previous sessions
have some inﬂuence in the understanding of the next sessions. This assumption implies that the process
is not even a Markov process. This kind of situation appears in seminars and classes with many diﬀerent
sessions. We derive some recursive expressions for the distribution of the number of sessions that the
student comprehends. Furthermore, we study the convergence of this distribution and study the speed
of this convergence through some numerical examples.

1 Introduction

Simple methods from physics and mathematics have been recently adopted to model, as mathematical
metaphors, a wide range of social phenomena and social systems [1, 4, 6, 8, 9, 11]. However, to the best of
our knowledge, this kind of models have not been comprehensively used to study the way students learn in
a certain class. How the behaviour of students aﬀects how well they learn is an old question and has been
widely studied in several contexts since people started to concern about teaching and pedagogy [2, 5, 7, 10].
Currently, in the context of the forced transition to online education due to the worldwide health alert,
providing an answer to this important issue and other closely related problems could legitimize or give birth
to new learning techniques. After this epidemic it can be expected that more capital will be invested to
online teaching and blended learning. Even though moving to a total online or blended learning model will
be a long process, it is important to study the intrinsic behaviour of students to provide better answers to
the challenges ahead.

In this work we proﬀer to answer the following question:

• How does a student, non-interacting with others, understands according to their aﬃnity to the sessions,

during a course?

Although general behaviors could be modeled as particles or agents (students) that interact and exchange
”something” (knowledge), in the current environment of social distancing; it is a known fact that many
students have resented this way of living in ”solitude” particularly in the classroom. This way of being in
solitude presents new opportunities and challenges when trying to measure how the student understand a
class session. In the present work we wish to model the possible appropriation of knowledge, whose easiness
varies across the student’s understanding throughout the course and the complexity of each session if they
do not interact with their classmates.

We assume that the course is presented in a number of sessions such that session j helps the student’s
understanding of session j + 1. This assumption implies that the given model does not rely on a Markov
process [3], because session j depends strongly on all the j − 1 previous sessions.

This work is organized as follows: A general description of the model is given in Section 2. The mathe-
matical manipulation of the model and some results are presented in Section 3. Afterwards, we study some
particular cases in Section 4 and present some numerical examples of the mode. Finally, conclusions and
future work appear in Section 5.

1

 
 
 
 
 
 
2 Description of the Model

We consider the situation in which the course consists of n sessions and the learning process of each student
in the same course is independent of the other students.

Under this situation, we assume that the lecturer teaches each session according to a measurable pa-
rameter q, which represents the quality of the sessions. Therefore we refer to q as the quality parameter of
each session. In this work we only consider the case when this parameter remains constant along the whole
course.

The event in which the student understands the ﬁrst session has a probability given by F (1 − q), for a

certain probability distribution F , where F := 1 − F .

From the second session until the end of the course, if the student understood session j, they understand
the next one with probability F j+1(1 − q) := F j(1 − q − ε). Here F1 refers to some initial distribution F
and each Fj for j ≥ 2 is constructed conditioned on the result of all the previous j − 1 sessions.

Similarly if the student did not understand session j, they understand the next one with probability

F j+1(1 − q) := F j(1 − q + ε).

The parameter ε is assumed to be positive and ﬁxed during the entire course and it reﬂects how the
comprehension of the content of a session inﬂuences the next session. Hence we call ε the dependence
parameter. In some courses this dependence parameter will be relatively big in comparison with n, like in
a Calculus class. But in many other cases, ε will be relatively small compared to n, like in seminaries, or
panoramic courses.

We wish to avoid the situation when the student understands the last sessions of the course with prob-
ability 1, due to the cumulative eﬀect of ε after some point, therefore we assume that ε is o(f (n)), for a
properly chosen function f depending on the total number of sessions n.

In the following sections we manipulate this model to obtain some results related to the distribution of

the number of sessions that the student understands along the course.

3 Main results

We let Y (j) be Bernoulli random variables deﬁned as follows: For session 1, Y (1) is Bernoulli with parameter
F (1 − q) for the given initial continuous distribution F .

For session 2, by the Law of Total Probability we have:

P [Y (2) = 1] = P [Y (2) = 1|Y (1) = 1] F (1 − q) + P [Y (2) = 1|Y (1) = 0] F (1 − q)

(1)

Given that the student understood session 1, the probability that they understand session 2 is given by

Similarly, if the student did not understand session 1, the probability that they understand session 2 is

P [Y (2) = 1|Y (1) = 1] = F (1 − q − ε).

It follows that (1) is equivalent to

P [Y (2) = 1|Y (1) = 0] = F (1 − q − ε).

P [Y (2) = 1] = F (1 − q − ε)F (1 − q) + F (1 − q + ε)F (1 − q).

(2)

For the general setting, we denote by Fj the probability distribution such that

F j(x) = F j−1(x − ε)pj−1 + F j−1(x + ε) (1 − pj−1) ,

provided that F j−1(x − ε) and F j−1(x + ε) do not equal zero or one.

P [Y (k) = 1] for k > 1 with p1 = F (1 − q) and F1 := F .

In the equation above, pk :=

Using this notation we obtain

pj = F j−1

(cid:0)1 − q − ε(cid:1)pj−1 + F j−1

(cid:0)1 − q + ε(cid:1)(1 − pj−1).

(3)

We derive a recursive expression for the probabilities {pm, 1 < m ≤ n}, which is given in the following

result.

Theorem 3.1 Let n and ε be such that (n − 1) ε < min {1 − q, q}, then the general expression for {pm, 1 <
m ≤ n} reads

pm = F 1 (1 − q)

m−1
(cid:89)

j=1

(cid:2)F j (1 − q − ε) − F j (1 − q + ε)(cid:3)+

m−1
(cid:88)

i=1

F i (1 − q + ε)

m−1
(cid:89)

j=i+1

(cid:2)F j (1 − q − ε) − F j (1 − q + ε)(cid:3) .

2

Proof. The result holds for m = 1 by the deﬁnition of p1. We proceed by induction, assuming that for an
integer k ≥ 1,

pk = F 1 (1 − q)

k−1
(cid:89)

j=1

By equation (3)

(cid:2)F j (1 − q − ε) − F j (1 − q + ε)(cid:3)+

k−1
(cid:88)

i=1

F i (1 − q + ε)

k−1
(cid:89)

j=i+1

(cid:2)F j (1 − q − ε) − F j (1 − q + ε)(cid:3) .

pk+1 = F k (1 − q − ε) pk + F k (1 − q + ε) (1 − pk) = pk

(cid:2)F k (1 − q − ε) − F k (1 − q + ε)(cid:3) + F k (1 − q + ε) ,

hence we obtain from the induction hypothesis

pk+1 = (cid:2)F k (1 − q − ε) − F k (1 − q + ε)(cid:3) F 1 (1 − q)

k−1
(cid:89)

j=1

(cid:2)F j (1 − q − ε) − F j (1 − q + ε)(cid:3)

+ (cid:2)F k (1 − q − ε) − F k (1 − q + ε)(cid:3)

k−1
(cid:88)

i=1

F i (1 − q + ε)

k−1
(cid:89)

j=i+1

(cid:2)F j (1 − q − ε) − F j (1 − q + ε)(cid:3)

+ F k (1 − q + ε)

= F 1 (1 − q)

k
(cid:89)

j=1

(cid:2)F j (1 − q − ε) − F j (1 − q + ε)(cid:3)

+

k
(cid:88)

i=1

F i (1 − q + ε)

k
(cid:89)

j=i+1

(cid:2)F j (1 − q − ε) − F j (1 − q + ε)(cid:3) .

The result now follows.

From this point on, we drop the notation F1 and write F instead.

We let Bn denote the number of sessions, not necessarily consecutive, from a total of n that the student
understood. We are interested in the probability function of Bn, {P [Bn = k] , 0 ≤ k ≤ n}, for which we
consider the following particular cases.

• Case 1: n = 3 and k = 0.

This is the case when the student understands 0 sessions out of 3. The probability reads:

P [B3 = 0] = F (1 − q) F (1 − q + ε) F (1 − q + 2ε)

• Case 2: n = 3 and k = 1.

If student understand only one session (i.e. k = 1), the probability is the sum of the following cases.

2.1 The student understands only the ﬁrst session

F (1 − q) F (1 − q − ε) F (1 − q) .

2.2 Student i understands only the second session

F (1 − q) F (1 − q + ε) F (1 − q) .

2.3 And Student i understands only the third session

F (1 − q) F (1 − q + ε) F (1 − q + 2ε) .

Note that the probabilities in cases 2.1 and 2.2 equal P [B2 = 0] F (1 − q) and the probability of case
2.3 corresponds to P [B2 = 1] F (1 − q + 2ε). Hence

P [B3 = 1] = P [B2 = 0] F (1 − q) + P [B2 = 0] F (1 − q) .

• Case 3: n = 3 and k = 2. If student understand two sessions (i.e. k = 2), the probability is the sum

of the following cases:

3.1 Student i understands the ﬁrst and second sessions but not the third

F (1 − q) F (1 − q − ε) F (1 − q − 2ε) .

3

3.2 Student i understands the second and third sessions but not the ﬁrst

F (1 − q) F (1 − q + ε) F (1 − q) .

3.3 Student i understands the ﬁrst and third sessions but not the second

F (1 − q) F (1 − q − ε) F (1 − q) .

Note that cases 2 and 3 correspond to P [B2 = 1] F (1 − q) and the ﬁrst case equals P [B2 = 2] F (1 − q − 2ε).
Hence

P [B3 = 2] = P [B2 = 1] F (1 − q) + P [B2 = 2] F (1 − q − 2ε) .

• Case 4: n = 3 and k = 3.

This is the case when student i understands all sessions. This probability is given by

P [B3 = 3] = F (1 − q) F (1 − q − ε) F (1 − q − 2ε) .

The recursive behaviour observed in the probability function of B3 is generalized in the following theorem.

Theorem 3.2 Let n ≥ 3 be an integer. The probability function of the random variable Bn satisﬁes the
following relations.

1. P [Bn = 0] = P [Bn−1 = 0] F (1 − q + (n − 1) ε) ,

2. P [Bn = n] = P [Bn−1 = n − 1] F (1 − q − (n − 1) ε) ,

3. P [Bn = k] = P [Bn−1 = k] F (1 − q + (n − 1 − 2k) ε)+P [Bn−1 = k − 1] F (1 − q + (n − 1 − 2 (k − 1)) ε) .

Proof.

1. If the student has not understand the ﬁrst n−1 sessions from a total of n, it follows from the construction
of the model that the student’s parameter for understanding the nth-session becomes 1 − q + (n − 2)ε.
Hence

P [Bn = 0] = P [Bn−1 = 0, Y (n) = 0] = P [Y (n) = 0 | Bn−1 = 0] P [Bn−1 = 0]

= F (1 − q + (n − 1)ε)P [Bn−1 = 0] .

2. Let An denote the number of sessions that the student has not understood from a total of n. Then
the event {Bn = n} is the same as {An = 0} and hence P [Bn = n] = P [An = 0]. Now the result in 1
yields

P [Bn = n] = P [An−1 = 0] [1 − F (1 − q + (n − 1) ε)] = P [Bn−1 = n − 1] F (1 − q + (n − 1)ε).

3. Let Uk(n) := {(x1, . . . , xn) ∈ {0, 1}n : x1 + · · · + xn = k}, then

P [Bn = k] = P [Bn−1 = k, Y (n) = 0] + P [Bn−1 = k − 1, Y (n) = 1]

=

+

=

+

(cid:88)

P [Y (1) = x1, . . . , Y (n − 1) = xn−1, Y (n) = 0]

(x1,...,xn−1)∈Uk(n−1)
(cid:88)

P [Y (1) = x1, . . . , Y (n − 1) = xn−1, Y (n) = 1]

(x1,...,xn−1)∈Uk−1(n−1)
(cid:88)

P [Y (n) = 0 | Y (1) = x1, . . . , Y (n − 1) = xn−1] P [Y (1) = x1, . . . , Y (n − 1) = xn−1]

(x1,...,xn−1)∈Uk(n−1)
(cid:88)

(x1,...,xn−1)∈Uk−1(n−1)

P [Y (n) = 1 | Y (1) = x1, . . . , Y (n − 1) = xn−1] P [Y (1) = x1, . . . , Y (n − 1) = xn−1] .

(4)

Note that, given the conﬁguration Y (1) = x1, . . . , Y (n − 1) = xn−1 in which the student understood
exactly k sessions, we have added k times ε to the quality parameter q. Moreover, the maximum
number of times we may add or subtract ε in a total of n sessions equals n − 1, since we do not
add or subtract anything in session 1. Hence if the student understood k of n sessions, they did not
understand n − k and we have subtracted n − 1 − k times ε. This means that understanding session n
depends on the parameter

q + kε − (n − 1 − k)ε = q − (n − 1 − 2k)ε.

4

It follows that the student does not understand session n with probability F (1 − q − (n − 1 − 2k)ε) .
Since this holds for any given conﬁguration Y (1) = x1, . . . , Y (n − 1) = xn−1, in which the student has
understood exactly k sessions, we obtain

(cid:88)

P [Y (n) = 0 | Y (1) = x1, . . . , Y (n − 1) = xn−1] P [Y (1) = x1, . . . , Y (n − 1) = xn−1]

(x1,...,xn−1)∈Uk(n−1)

= F (1 − q − (n − 1 − 2k)ε)

(cid:88)

= F (1 − q − (n − 1 − 2k)ε) P [Bn−1 = k] .

(x1,...,xn−1)∈Uk(n−1)

P [Y (1) = x1, . . . , Y (n − 1) = xn−1]

(5)

Analogously

(cid:88)

P [Y (n) = 1 | Y (1) = x1, . . . , Y (n − 1) = xn−1] P [Y (1) = x1, . . . , Y (n − 1) = xn−1]

(x1,...,xn−1)∈Uk−1(n−1)
= F (1 − q − (n − 1 − 2(k − 1))ε) P [Bn−1 = k − 1] .

The result follows by substituting equations (5) and (6) in equation (4).

Equations in Theorem 3.2 can be written as a single matrix equation. Let (cid:126)Bn ∈ M1,n+1 be given by

(cid:126)Bn = [P [Bn = 0] , P [Bn = 1] , P [Bn = 2] , . . . , P [Bn = n]] ,

and denote by Mn ∈ Mn+1,n the matrix such that

(Mn)a,b =






F (1 − q − (n − 1 − 2a) ε)
F (1 − q − (n − 1 − 2a) ε)
0

for a = b
for a = b − 1
otherwise .

Using the notation above we note that (cid:126)Bn = Mn · (cid:126)Bn−1, therefore

(6)

(7)

(cid:126)Bn =

n
(cid:89)

k=1

Mk (cid:126)B0,i,

where (cid:126)B0 = (cid:2)F (1 − q) , F (1 − q)(cid:3). This representation is used for some numerical examples in Section 5.

The explicit distribution of Bn is not easy to obtain even in simple cases (such as the case when F is a
uniform distribution). Nevertheless, in the following result we provide a simple asymptotic expression for
this distribution.

Theorem 3.3 Let {pn(k), k = 0, 1, . . . , n} denote the probability function of a Binomial(n, p) distribution,
with p := F (1 − q). Suppose n, ε are such that n2ε → 0 as n → ∞ and F is absolutely continuous with
density f such that f is continuous at 1 − q, then,

lim
n→∞

P [Bn = k]
pn(k)

= 1,

∀k ∈ {0, 1, . . . , n}.

Proof. First we prove the case when k /∈ {0, n}. Following the arguments in the proof of Theorem 3.2, we
have

P [Bn = k] = F (1 − q − (n − 1 − 2k)ε)

(cid:88)

P [Y (1) = x1, . . . , Y (n − 1) = xn−1]

+ F (1 − q − (n − 1 − 2(k − 1))ε)

(cid:88)

P [Y (1) = x1, . . . , Y (n − 1) = xn−1] , (8)

(x1,...,xn−1)∈Uk(n−1)

(x1,...,xn−1)∈Uk−1(n−1)

where

Uk(n) = {(x1, . . . , xn) ∈ {0, 1}n : x1 + · · · + xn = k}.

From the hypothesis n2ε → 0 it follows that ε → 0. Hence, by the assumption of continuity of F , the

following convergences as n → ∞ hold:

F (1 − q − (n − 1 − 2k)ε) → F (1 − q) ,

F (1 − q − (n − 1 − 2(k − 1))ε) → F (1 − q).

Note from the matrix representation given in (7) that P [Y (1) = x1, . . . , Y (n − 1) = xn−1] can be ex-

pressed as

5

pn−1,k :=

n−1
(cid:89)

j=1

F

where

v1(j)(cid:16)

1 − q − u1(j)ε + u0(j)ε

(cid:17)

F 1−v1(j)(cid:16)

1 − q − u1(j)ε + u0(j)ε

(cid:17)

,

j
(cid:88)

u1(j) =

(−1)xa ,

u0(j) =

j
(cid:88)

(−1)xa ,

a=1,xa=1

a=1,xa=0

and v1(j) = 1 if student understood session j.
We have the following bounds for pn−1,k:

k(cid:16)

F

1 − q + u0(j)ε

(cid:17)

F n−1−k(cid:16)

1 − q − u1(j)ε

(cid:17)

≤ pn−1,k ≤ F

k(cid:16)

1 − q − u1(j)ε

(cid:17)

F n−1−k(cid:16)

1 − q + u0(j)ε

(cid:17)

(9)

Since the terms with the tail F converge to F (1 − q) and their exponents do not depend on n, we only

need to prove that

or equivalently

F n−1−k(cid:16)

1 − q − u1(j)ε

(cid:17)

F n−1−k(1 − q)

→ 1, n → ∞,

(n − 1 − k) log

(cid:16)

F





1 − q − u1(j)ε

F (1 − q)

(cid:17)



 → 0, n → ∞.

Using the hypothesis n2ε → 0 we may write ε = cn−2−η for some c, η > 0. Applying L’Hˆopital’s rule we

obtain

(cid:16)

F





1 − q − u1(j)ε

F (1 − q)

lim
n→∞

n log

(cid:17)



 = cu(j)(2 + η) lim
n→∞

n−3−ηF (1 − q)f (1 − q − u1(j)cn−2−η)
(−n−2)F (1 − q − u1(j)cn−2−η)

= 0

From the limit above and (9) it follows that

pn−1,k

k

F

(1 − q)F n−1−k(1 − q)

→ 1, n → ∞.

(10)

Now let us consider the term

(cid:88)

P [Y (1) = x1, . . . , Y (n − 1) = xn−1] .

(x1,...,xn−1)∈Uk(n−1)

Since |Uk(n − 1)| = (cid:0)n−1

k

n we have

(cid:1), using the result in equation (10), for an arbitrary β > 0 and suﬃciently large

1 − β ≤

(cid:88)

P [Y (1) = x1, . . . , Y (n − 1) = xn−1]

(x1,...,xn−1)∈Uk(n−1)

(cid:0)n−1
k

(cid:1)F

k

(1 − q)F n−1−k(1 − q)

≤ 1 + β.

The result follows by letting n → ∞ and β → 0. The result for the second term in equation (8) is
obtained analogously. Now we proceed in a similar way to prove that P [Bn = 0] is asymptotically equivalent
to pn(0). It might be easily checked that P [Bn = 0] = F (1 − q)

F (1 − q + jε), hence

n−1
(cid:81)
j=1

1 =

(cid:19)n

(cid:18) F (1 − q)
F (1 − q)

≤

F (1 − q)

F (1 − q + jε)

n−1
(cid:81)
j=1
F n(1 − q)

≤

(cid:18) F (1 − q + nε)
F (1 − q)

(cid:19)n−1

.

Using the representation ε = cn−η−2 and L’Hˆopital’s rule again, we obtain

lim
n→∞

(n − 1) log

F (1 − q + nε)
F (1 − q)

= −c(η + 2) lim
n→∞

(n − 1)2 n−η−2F (1 − q + cn−η−1)

F (1 − q)

f (cid:0)1 − q + cn−η−1(cid:1) = 0.

The proof for P [Bn = n] is analogous.

Theorem 3.3 says that for a suﬃciently large number of sessions, the dependence becomes less relevant,
hence the number of sessions that each student understands behaves like a binomial distribution in which
the occurrence of successes is independent.

6

4 The case of the uniform distribution

In this section we present some important quantities in the particular case when the students initial dis-
tribution for understanding is uniform in [0, 1]. Throughout this section we assume that ε is such that
[1 − q − (n − 1)ε, 1 − q + (n − 1)ε] ⊆ [0, 1], which we name as Hypothesis 1.

Proposition 4.1 Let F be the uniform distribution over (0, 1) and assume Hypothesis 1 holds. Then for
i ≤ n we have

F i (1 − q − mε) = (1 + 2ε)i−1

(cid:27)

(cid:26)

q −

1
2

+

1
2

+ mε

Proof. Recall that F1 = F . The result holds for i = 1, since

We proceed by induction, assuming that for an integer j ≥ 1

F 1 (1 − q − mε) = q + mε

F j (1 − q − mε) = (1 + 2ε)j−1

(cid:26)

q −

(cid:27)

1
2

+

1
2

+ mε

By the Law of Total Probability and induction hypothesis

(cid:26)

(cid:27)

+

=

q −

(cid:20)
(1 + 2ε)j−1

F j+1 (1 − q − mε) = F j (1 − q − (m + 1) ε) F j (1 − q) + F j (1 − q − (m − 1) ε) (cid:0)1 − F j (1 − q)(cid:1)
1
2
1
2
1
2
1
2

(cid:27)(cid:21) (cid:20) 1
2
(cid:27)(cid:21) (cid:20) 1
2

(cid:20)
(1 + 2ε)j−1

(cid:20)
(1 + 2ε)j−1

(cid:20)
(1 + 2ε)j−1

− (1 + 2ε)j−1

(cid:20) 1
2
(cid:20) 1
2

(1 + 2ε)j−1

(1 + 2ε)j−1

(1 + 2ε)j−1

+ (m + 1) ε

+ (m − 1) ε

+ (m + 1) ε

+ (m − 1) ε

(cid:21) (cid:20) 1
2

1
2
(cid:27)(cid:21)

1
2
(cid:26)

1
2
1
2

1
2
1
2

q −

q −

q −

q −

q −

q −

q −

(cid:21) (cid:20)

1
2

+

−

+

+

+

+

+

=

+

(cid:27)

(cid:26)

(cid:27)

(cid:26)

(cid:26)

(cid:26)

(cid:27)

(cid:26)

(cid:26)

(cid:27)

(cid:21)

(cid:21)

(cid:21)

1
2
1
2
1
2
1
2
(cid:27)

= (1 + 2ε)j−1

(cid:26)

q −

= (1 + 2ε)j

(cid:26)

q −

1
2

Hence the result follows.

+ mε + 2ε

(cid:20)
(1 + 2ε)j−1

(cid:26)

q −

(cid:27)(cid:21)

1
2

1
2

+ mε

1
2
(cid:27)

+

1
2

+

(cid:21)

(cid:21)

+ (m + 1) ε

+ (m − 1) ε

Theorem 4.1 Let F be the uniform distribution over (0, 1) and assume Hypothesis 1 holds. Then for m ≤ n
we have

pm =

+ (1 + 2ε)m−1

q −

(cid:18)

(cid:19)

1
2

1
2

Proof. For 1 ≤ j ≤ m − 1, it follows from Proposition 4.1 that

F j (1 − q − ε) − F j (1 − q + ε) = 2ε

Since F (1 − q) = q, by Theorem 3.1 we have

pm = F 1 (1 − q)

m−1
(cid:89)

j=1

(cid:2)F j (1 − q − ε) − F j (1 − q + ε)(cid:3) +

m−1
(cid:88)

i=1

F i (1 − q + ε)

m−1
(cid:89)

j=i+1

(cid:2)F j (1 − q − ε) − F j (1 − q + ε)(cid:3)

= q [2ε]m−1 +

(cid:26) 1
2

(cid:27) m−1
(cid:88)

− ε

(2ε)m−1 (2ε)−i +

(cid:26)

q −

1
2

(cid:27) m−1
(cid:88)

i=1

(1 + 2ε)i−1 [2ε]m−1−i

(2ε)−i +

q − 1
2
1 + 2ε

m−1
(cid:88)

i=1

(cid:18) 1 + 2ε
2ε

(cid:19)i(cid:35)

.

(11)

(cid:34)

= [2ε]m−1

q +

(cid:26) 1
2

− ε

Using the identities

i=1
(cid:27) m−1
(cid:88)

i=1

n−1
(cid:88)

i=1

(cid:19)i

(cid:18) 1 + 2ε
2ε

= [1 + 2ε]

(cid:34)(cid:18) 1 + 2ε
2ε

(cid:19)n−1

(cid:35)

− 1

and

n−1
(cid:88)

i=1

(2ε)−i =

(cid:20)

1
1 − 2ε

(cid:21) (cid:34)(cid:18) 1
2ε

(cid:19)n−1

(cid:35)

− 1

,

the right hand of equation (11) becomes

7

(a)

(b)

Figure 1: In this ﬁgure we plot some simulations of Bn (dark line) and its approximating binomial distribution
(light line).In all the plots we consider ε = 1/n2 and n = 10 in blue; n = 30 in red; n = 60 in green and
n = 100 in purple. Plot (a) is made for q = 0.2; Plot (b) is made for q = 0.5 and Plot (c) for q = 0.8.

(c)

(cid:34)

[2ε]m−1

q +

(cid:27) (cid:20)

− ε

(cid:26) 1
2

1
1 − 2ε

(cid:21) (cid:34)(cid:18) 1
2ε

(cid:19)m−1

(cid:35)

− 1

+

q − 1
2
1 + 2ε

[1 + 2ε]

(cid:34)(cid:18) 1 + 2ε
2ε

(cid:19)m−1

(cid:35)(cid:35)

− 1

= [2ε]m−1

(cid:34)

(cid:18) 1
2ε

1
2

(cid:19)m−1

+ q

(cid:18) 1 + 2ε
2ε

(cid:19)m−1

−

(cid:18) 1 + 2ε
2ε

1
2

(cid:19)m−1(cid:35)

=

(cid:20) 1
2

+ q (1 + 2ε)m−1 −

(1 + 2ε)m−1

(cid:21)

.

1
2

Hence we obtain

And the result follows.

pm =

1
2

+ (1 + 2ε)m−1

(cid:18)

q −

(cid:19)

.

1
2

As we previously mentioned, an analytic expression for the exact distribution of Bn is not easy to obtain
even in this simple case when F is the uniform distribution. However, in Theorem 3.3 we have seen that
Bn behaves asymptotically like a binomial random variable. We present some numerical examples of the
performance of this approximation, using diﬀerent values of n, q and ε. These examples show that the two
distributions are very close even for values of n which are not so large.

In Figure 1 we ﬁx the value of q, we simulate the exact distribution and compare it to the approximating
binomial distribution. This is made for n = 10, 30, 60, 100 (plots in blue, red, green and purple respectively).
The light line corresponds to the approximating binomial distribution while the dark line represents the
simulated exact distribution of Bn.

As it can be seen in the distinct plots, the convergence to the binomial distribution is quite fast and it

grows faster when q = .5. Moreover, it is seen numerically that for any q, when n ≥ 40,

P[Bn=k]
pn(k) ≥ 0.95.

Our numerical examples show that the speed of convergence depends of the value of q, as it can be seen
in Figure 2, where we use diﬀerent values of q with ﬁxed n and compare the exact distribution of Bn to the
approximating binomial distribution.

Another point worth mentioning is that the distributions behave symmetrically with respect to q = 0.5.
In this case, the convergence to the binomial distribution is faster than in the other cases. In fact, the cases
when q is nearly 0 or 1, present a slower convergence to the binomial distribution. This may imply that the
dependence is stronger when the quality of the class is low or high.

8

51015202530350.050.100.150.200.250.30102030405060700.050.100.150.200.25204060801000.050.100.150.200.250.30(a)

(c)

(b)

(d)

Figure 2: In this ﬁgure we plot some simulations of Bn (dark line) and its approximating binomial distribution
(light line).In all the plots we consider ε = 1/n2 and q = 0.1 in blue; q = 0.3 in red; q = 0.5 in green; q = 0.7
in purple and q = 0.9 in yellow. Plot (a) is made for n = 10; Plot (b) is made for n = 30; Plot (c) is made
for n = 60 and Plot (d) for n = 100.

5 Conclusions and Future work

In this work we present a model to study the behaviour of the understanding of a student along several sessions
of a course taught totally online and with no interaction between other students. In particular we study the
case where the dependence between sessions is relatively small compared to the total number of sessions, as
in seminars or panoramic courses. We obtained a recursive expression for the distribution of the number
of sessions that the student understands along the course and showed that when the dependence parameter
is small, this distribution has a binomial approximation. The speed of convergence of the approximation
depends on the number of sessions and the quality of them. Even though this is a simple model, it can be
fruitfully extended to consider many more situations, some of which we list below:

• The environment we considered assumes the student has no interaction with their classmates. However
several studies have shown that collaborative learning provides better results for students. It would be
very interesting to modify the model to consider this situation.

• We studied the case when ε is constant and relatively small compared to n, but this may not always be
the case, as in some science classes. Therefore it would be useful to consider the cases when ε changes
according to the sessions themselves or according to the number of sessions previously understood.

• All the results obtained in this work were made for q constant, but the value of q can vary along the

course due to exhaustion and motivation of the student and the teacher.

• Even though the distribution of Bn can be approximated by a binomial distribution, we were not able

to provide similar results for the corresponding mean and variance.

• It is interesting to test the model with real world data and study or develop some statistical procedures

in order for the model to be ﬁtted and validated.

References

[1] D. Cajueiro. Enforcing social behavior in an ising model with complex neighborhoods. Physica A:

Statistical Mechanics and its Applications, 390(9):1695 – 1703, 2011.

[2] R. J. Dufresne, W. J. Gerace, W. J. Leonard, J. P. Mestre, and L. Wenk. Classtalk: A classroom
communication system for active learning. Journal of Computing in Higher Education, 7(2):3–47, Mar
1996.

9

2468100.10.20.30.40.5510152025300.050.100.150.200.251020304050600.050.100.15204060801000.020.040.060.080.100.12[3] Y.-M. Huang, T.-C. Huang, K.-T. Wang, and W.-Y. Hwang. A markov-based recommendation model
for exploring the transfer of learning on the web. Journal of Educational Technology and Society,
12(2):144–162, 2009.

[4] K. Klemm. Nonequilibrium transitions in complex networks: A model of social interaction. Physical

Review E, 67(2), 2003.

[5] Y.-A. Lee. Third turn position in teacher talk: Contingency and the work of teaching. Journal of

Pragmatics, 39(6):1204 – 1230, 2007.

[6] C. Li, F. Liu, and P. Li. Ising model of user behavior decision in network rumor propagation. Discrete

Dynamics in Nature and Society, 2018:1–10, Aug 2018.

[7] J. C. McCroskey. Classroom consequences of communication apprehension. Communication Education,

26(1):27–33, Jan 1977.

[8] S. Nakayama and Y. Nakamura. A fashion model with social interaction. Physica A: Statistical Me-

chanics and its Applications, 337(3):625 – 634, 2004.

[9] M. Ostilli, E. Yoneki, I. X. Y. Leung, J. F. F. Mendes, P. Li´o, and J. Crowcroft. Ising model of rumour
spreading in interacting communities. Technical Report UCAM-CL-TR-767, University of Cambridge,
Computer Laboratory, Jan. 2010.

[10] V. P. Richmond. Communication in the classroom: Power and motivation. Communication Education,

39(3):181–195, Jul 1990.

[11] D. Stauﬀer.

Social applications of two-dimensional

ising models. American Journal of Physics,

76(4):470–473, Apr 2008.

10

