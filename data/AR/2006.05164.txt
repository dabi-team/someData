AR-DAE: Towards Unbiased Neural Entropy Gradient Estimation

Jae Hyun Lim 1 2 Aaron Courville 1 2 3 4 Christopher Pal 1 5 4 Chin-Wei Huang 1 2

0
2
0
2

n
u
J

9

]

G
L
.
s
c
[

1
v
4
6
1
5
0
.
6
0
0
2
:
v
i
X
r
a

Abstract

Entropy is ubiquitous in machine learning, but it
is in general intractable to compute the entropy of
the distribution of an arbitrary continuous random
variable. In this paper, we propose the amortized
residual denoising autoencoder (AR-DAE) to ap-
proximate the gradient of the log density function,
which can be used to estimate the gradient of
entropy. Amortization allows us to signiﬁcantly
reduce the error of the gradient approximator by
approaching asymptotic optimality of a regular
DAE, in which case the estimation is in theory
unbiased. We conduct theoretical and experimen-
tal analyses on the approximation error of the
proposed method, as well as extensive studies on
heuristics to ensure its robustness. Finally, using
the proposed gradient approximator to estimate
the gradient of entropy, we demonstrate state-of-
the-art performance on density estimation with
variational autoencoders and continuous control
with soft actor-critic.

1. Introduction

Entropy is an information theoretic measurement of uncer-
tainty that has found many applications in machine learning.
For example, it can be used to incentivize exploration in
reinforcement learning (RL) (Haarnoja et al., 2017; 2018);
prevent mode-collapse of generative adversarial networks
(GANs) (Balaji et al., 2019; Dieng et al., 2019); and cal-
ibrate the uncertainty of the variational distribution in ap-
proximate Bayesian inference. However, it is in general
intractable to compute the entropy of an arbitrary random
variable.

In most applications, one actually does not care about the
quantity of entropy itself, but rather how to manipulate and

1Mila 2Université de Montréal 3CIFAR fellow 4Canada CI-
FAR AI Chair 5Polytechnique Montréal. Correspondence to: Jae
Hyun Lim <jae.hyun.lim@umontreal.ca>, Chin-Wei Huang <chin-
wei.huang@umontreal.ca>.

Proceedings of the 37 th International Conference on Machine
Learning, Vienna, Austria, PMLR 108, 2020. Copyright 2020 by
the author(s).

control this quantity as part of the optimization objective. In
light of this, we propose to approximately estimate the gra-
dient of entropy so as to maximize or minimize the entropy
of a data sampler. More concretely, we approximate the
gradient of the log probability density function of the data
sampler. This is sufﬁcient since the gradient of its entropy
can be shown to be the expected value of the path deriva-
tive (Roeder et al., 2017). We can then plug in a gradient
approximator to enable stochastic backpropagation.

We propose to use the denoising autoencoder (DAE, Vin-
cent et al. (2008)) to approximate the gradient of the log
density function, which is also known as denoising score
matching (Vincent, 2011). It has been shown that the op-
timal reconstruction function of the DAE converges to the
gradient of the log density as the noise level σ approaches
zero (Alain & Bengio, 2014). In fact, such an approach
has been successfully applied to recover the gradient ﬁeld
of the density function of high-dimensional data such as
natural images (Song & Ermon, 2019), which convincingly
shows DAEs can accurately approximate the gradient. How-
ever, in the case of entropy maximization (or minimization),
the non-stationarity of the sampler’s distribution poses a
problem for optimization. On the one hand, the log density
gradient is recovered only asymptotically as σ → 0. On the
other hand, the training signal vanishes while a smaller noise
perturbation is applied, which makes it hard to reduce the
approximation error due to suboptimal optimization. The
fact that the sampler’s distribution is changing makes it even
harder to select a noise level that is sufﬁciently small. Our
work aims at resolving this no-win situation.

In this work, we propose the amortized residual denoising
autoencoder (AR-DAE), which is a conditional DAE of a
residual form that takes in σ as input. We condition the
DAE on σ = 0 at inference time to approximate the log den-
sity gradient while sampling non-zero σ at training, which
allows us to train with σ sampled from a distribution that
covers a wide range of values. If AR-DAE is optimal, we
expect to continuously generalize to σ = 0 to recover the
log density gradient, which can be used as an unbiased esti-
mate of the entropy gradient. We perform ablation studies
on the approximation error using a DAE, and show that
our method provides signiﬁcantly more accurate approxi-
mation than the baselines. Finally, we apply our method to
improve distribution-free inference for variational autoen-

 
 
 
 
 
 
AR-DAE: Towards Unbiased Neural Entropy Gradient Estimation

Ex[− log pθ(x)] =

(cid:104)

Ez

log det|Jzgθ(z)| − log p(z)

(cid:105)

iterative optimization algorithm such as (stochastic) gradient
descent, which is commonly employed in machine learning,
it is sufﬁcient to compute the gradient of the entropy rather
than the entropy itself.

Following Roeder et al. (2017), we can rewrite the entropy
of x by changing the variable and neglecting the score func-
tion which is 0 in expectation to get

x

AR-DAE
far(x) ≈ ∇x log pθ(x)

∇θH(pg(x)) = −Ez

(cid:2)[∇x log pg(x)|x=gθ(z)](cid:124)Jθgθ(z)(cid:3) ,
(1)

z ∼ p(z)

x = gθ(z)
log det|Jzgθ(z)|

forward

θ

backward
Ez[∇θ log det|Jzgθ(z)|]

(a)

z ∼ p(z)

x = gθ(z)

forward

θ

backward
- Ez[far(gθ(z))(cid:124)Jθgθ(z)|]

(b)

Figure 1. (a) Entropy gradient wrt parameters of an invertible gen-
erator function. (b) Approximate entropy gradient using the pro-
posed method.

coders (Kingma & Welling, 2014; Rezende et al., 2014b)
and soft actor-critic (Haarnoja et al., 2018) for continuous
control problems in reinforcement learning. As these tasks
are non-stationary, amortized (conditional), and highly struc-
tured, it demonstrates AR-DAE can robustly and accurately
approximate log density gradient of non-trivial distributions
given limited computational budgets.

2. Approximate entropy gradient estimation

2.1. Background on tractability of entropy

An implicit density model is characterized by a data gen-
eration process (Mohamed & Lakshminarayanan, 2016).
The simplest form of an implicit density model contains a
prior random variable z ∼ p(z), and a generator function
g : z (cid:55)→ x. The likelihood of a particular realization of x is
implied by the pushforward of p(z) through the mapping g.

Unlike an explicit density model, an implicit density model
does not require a carefully designed parameterization for
the density to be explicitly deﬁned, allowing it to approx-
imate arbitrary data generation process more easily. This
comes at a price, though, since the density function of the
implicit model cannot be easily computed, which makes it
hard to approximate its entropy using Monte Carlo methods.

2.2. Denoising entropy gradient estimator

Let z and g be deﬁned as above, and let θ be the parameters
of the mapping g (denoted gθ). Most of the time, we are
interested in maximizing (or minimizing) the entropy of the
implicit distribution of x = gθ(z). For example, when the
mapping g is a bijection, the density of x = gθ(z) can be de-
composed using the change-of-variable density formula, so
controlling the entropy of x amounts to controlling the log-
determinant of the Jacobian of gθ (Rezende & Mohamed,
2015), as illustrated in Figure 1-(a). This allows us to es-
timate both the entropy and its gradient. However, for an

where Jθgθ(z) is the Jacobian matrix of the random sample
x = gθ(z) wrt to the sampler’s parameters θ. See Appendix
A for the detailed derivation. We emphasize that this formu-
lation is more general as it does not require g to be bijective.

Equation (1) tells us that we can obtain an unbiased estimate
of the entropy by drawing a sample of the integrand, which
is the path derivative of z. The integrand requires evaluating
the sample x = gθ(z) under the gradient of its log density
∇x log pg(x). As log pg(x) is usually intractable or simply
not available, we directly approximate its gradient using
a black box function. As long as we can provide a good
enough approximation to the gradient of the log density and
treat it as the incoming unit in the backward differentiation
(see Figure 1-(b)), the resulting estimation of the entropy
gradient is approximately unbiased.

In this work, we propose to approximate the gradient of
the log density using a denoising autoencoder (DAE, Vin-
cent et al. (2008)). A DAE is trained by minimizing the
reconstruction loss d of an autoencoder r with a randomly
perturbed input

LDAE(r) = E[d(x, r(x + (cid:15)))],

where the expectation is taken over the random perturbation
(cid:15) and data x. Alain & Bengio (2014) showed that if d is
the L2 loss and (cid:15) is a centered isotropic Gaussian random
variable with variance σ2, then under some mild regular-
ity condition on log pg the optimal reconstruction function
satisﬁes

r∗(x) = x + σ2∇x log pg(x) + o(σ2),

as σ2 → 0. That is, for sufﬁciently small σ, we can approx-
imate the gradient of the log density using the black box
function fr(x) := r(x)−x

assuming r ≈ r∗.

σ2

3. Error analysis of ∇x log pg(x) ≈ fr(x)

Naively using fr(x) to estimate the gradient of the entropy
is problematic. First of all, the division form of fr can lead
to numerical instability and magnify the error of approxima-
tion. This is because when the noise perturbation σ is small,
r(x) will be very close to x and thus both the numerator and
the denominator of fr are close to zero.

AR-DAE: Towards Unbiased Neural Entropy Gradient Estimation

Second, using the triangle inequality, we can decompose the
error of the approximation ∇x log pg(x) ≈ fr(x) into

||∇x log pg(x) − fr(x)|| ≤

||∇x log pg(x) − fr∗ (x)||
(cid:123)(cid:122)
(cid:125)
(cid:124)
asymp error

+ ||fr∗ (x) − fr(x)||.

The ﬁrst error is incurred by using the optimal DAE to
approximate ∇x log pg(x), which vanishes when σ → 0.
We refer to it as the asymptotic error. The second term is
the difference between the optimal DAE and the “current”
reconstruction function. Since we use a parametric family
of functions (denoted by F) to approximate fr∗ , it can be
further bounded by

||fr∗ (x) − fr(x)|| ≤

||fr∗ (x) − fr∗
(cid:124)

(cid:123)(cid:122)
param error

F

(x)||
(cid:125)

+ ||fr∗
(cid:124)

F

,
(x) − fr(x)||
(cid:125)

(cid:123)(cid:122)
optim error

where r∗
F := arg minr∈F LDAE(r) is the optimal reconstruc-
tion function within the family F. The ﬁrst term measures
how closely the family of functions F approximates the op-
timal DAE, and is referred to as the parameterization error.
The second term reﬂects the suboptimality in optimizing
r. It can be signiﬁcant especially when the distribution of
x is non-stationary, in which case r needs to be constantly
adapted. We refer to this last error term as the optimiza-
tion error. As we use a neural network to parameterize
r, the parameterization error can be reduced by increas-
ing the capacity of the network. The optimization error is
subject to the variance of the noise σ2 (relative to the dis-
tribution of x), as it affects the magnitude of the gradient
signal E[∇||r(x + (cid:15)) − x||2]. This will make it hard to de-
sign a ﬁxed training procedure for r as different values of σ
requires different optimization speciﬁcations to tackle the
optimization error.

4. Achieving asymptotic optimality

In this section, we propose the amortized residual
DAE (AR-DAE), an improved method to approximate
∇x log pg(x) that is designed to resolve the numerical insta-
bility issue and reduce the error of approximation.

4.1. Amortized residual DAE

AR-DAE (denoted far) is a DAE of residual form condi-
tioned on the magnitude of the injected noise, minimizing
the following optimization objective.

Lar (far) =

E
x∼p(x)
u∼N (0,I)
σ∼N (0,δ2)

(cid:107)u + σfar(x + σu; σ)(cid:107)2(cid:105)
(cid:104)

.

(2)

This objective involves three modiﬁcations to the regular
training and parameterization of a DAE: residual connection,
loss rescaling, and scale conditioning for amortization.

Residual form First, we consider a residual form of DAE
(up to a scaling factor): let r(x) = σ2far(x) + x, then
∇x log pg(x) is approximately equal to

r(x) − x
σ2

=

σ2far(x) + x − x
σ2

= far .

That is, this reparameterization allows far to directly ap-
proximate the gradient, avoiding the division that can cause
numerical instability. The residual form also has an obvious
beneﬁt of a higher capacity, as it allows the network to rep-
resent an identity mapping more easily, which is especially
important when the reconstruction function is close to an
identity map for small values of σ (He et al., 2016).

Loss rescaling To prevent the gradient signal from vanish-
ing to 0 too fast when σ is arbitrarily small, we rescale the
loss LDAE by a factor of 1/σ, and since we can decouple the
noise level from the isotropic Gaussian noise into (cid:15) = σu
for standard Gaussian u, the rescaled loss can be written as
E[||σfar(x + σu) + u||2].

We summarize the properties of the optimal DAE of the
rescaled residual form in the following propositions:

Proposition 1. Let x and u be distributed by p(x) and
N (0, I). For σ (cid:54)= 0, the minimizer of the functional
Ex,u[||u + σf (x + σu)||2] is almost everywhere determined
by

f ∗(x; σ) =

− Eu[p(x − σu)u]
σ Eu[p(x − σu)]

.

Furthermore, if p(x) and its gradient are both bounded, f ∗
is continuous wrt σ for all σ ∈ R\0 and limσ→0 f ∗(x; σ) =
∇x log pg(x).

The above proposition studies the asymptotic behaviour of
the optimal f ∗
ar as σ → 0. Below, we show that under
the same condition, f ∗
ar approaches the gradient of the log
density function of a Gaussian distribution centered at the
expected value of x ∼ p(x) as σ is arbitrarily large.

Proposition 2. limσ→∞

f ∗(x;σ)

∇x log N (x;Ep[X],σ2I) → 1.

Scale conditioning Intuitively, with larger σ values, the
perturbed data x + σu will more likely be “off-manifold”,
which makes it easy for the reconstruction function to point
back to where most of the probability mass of the distribu-
tion of x resides. Indeed, as Proposition 2 predicts, with
larger σ the optimal f ∗
ar tends to point to the expected value
Ep[X], which is shown in Figure 2-left. With smaller values
of σ, training far becomes harder, as one has to predict the
vector −u from x + σu (i.e. treating x as noise and trying to
recover u). Formally, the training signal (∆) has a decaying

AR-DAE: Towards Unbiased Neural Entropy Gradient Estimation

Figure 2. Residual DAE trained with a large (left) vs small (right)
σ value. Red cross indicates the mean of the swissroll. The arrows
indicate the approximate gradient directions.

rate of O(σ2) for small σ values, because

Eu[∆] := Eu[∇||u + σf (x + σu)||2]

(cid:18)

= 2σ2∇

tr(∇xf (x)) +

(cid:19)

||f (x)||2

1
2

+ o(σ2),

where the ﬁrst term is proportional to the stochastic gradient
of the implicit score matching (Hyvärinen, 2005). That is,
with smaller σ values, minimizing the rescaled loss is equiv-
alent to score matching, up to a diminishing scaling factor.
Moreover, the variance of the gradient signal Var(∆) also
has a quadratic rate O(σ2), giving rise to a decreasing signal-
to-noise ratio (SNR) E[∆]/(cid:112)Var(∆) = O(σ), which is an
obstacle for stochastic optimization (Shalev-Shwartz et al.,
2017). See Appendix C for the SNR analysis.

In order to leverage the asymptotic optimality of the gradi-
ent approximation as σ → 0 (Figure 2-right), we propose
to train multiple (essentially inﬁnitely many) models with
different σ’s at the same time, hoping to leverage the beneﬁt
of training a large-σ model while training a model with a
smaller σ.

More concretely, we condition far on the scaling factor σ, so
that far can "generalize" to the limiting behaviour of f ∗
ar as
σ → 0 to reduce the asymptotic error. Note that we cannot
simply take σ to be zero, since setting σ = 0 would result
in either learning an identity function for a regular DAE or
learning an arbitrary function for the rescaled residual DAE
(as the square loss would be independent of the gradient
approximator).

The scale-conditional gradient approximator far(x; σ) will
be used to approximate ∇x log pg(x) by setting σ = 0
during inference, while σ is never zero at training. This can
be done by considering a distribution of σ, which places
zero probability to the event {σ = 0}; e.g. a uniform density
between [0, δ] for some δ > 0. The issue of having a non-
negative support for the distribution of σ is that we need to
rely on far to extrapolate to 0, but neural networks usually
perform poorly at extrapolation. This can be resolved by
having a symmetric distribution such as centered Gaussian

Figure 3. Approximating log density gradient of 1-D MoG. AR-
DAE: the approximation error of the proposed method. f ∗
ar: the
optimal DAE. resDAE: a DAE of residual form (as well as loss
rescaling). regDAE: a regular DAE.

with variance δ2 or uniform density between [−δ, δ]; owing
to the the symmetry of the noise distribution N (u; 0, I), we
can mirror the scale across zero without changing the loss:

Eu

(cid:2)(cid:107)u + σf (x + σu)(cid:107)2(cid:3) = E (cid:2)(cid:107)(−u) + σf (x + σ(−u))(cid:107)2(cid:3)
= E (cid:2)(cid:107)u + (−σ)f (x + (−σ)u)(cid:107)2(cid:3) .

ar(x, σ(cid:48)) would be close to f ∗

Furthermore, Proposition 1 implies a good approximation
to f ∗
ar(x, σ) if σ(cid:48) is sufﬁciently
close to σ. We suspect this might help to reduce the opti-
mization error of AR-DAE, since the continuity of both
far and f ∗
ar implies that far(x, σ) only needs to reﬁne
far(x, σ(cid:48)) slightly if the latter already approximates the cur-
vature of f ∗
ar(x, σ(cid:48)) well enough. Then by varying different
σ values, the conditional DAE is essentially interpolating
between the gradient ﬁeld of the log density function of in-
terest and that of a Gaussian with the same expected value.

4.2. Approximation error

To study the approximation error with different variants of
the proposed method, we consider a 1-dimensional mixture
of Gaussians (MoG) with two equally weighted Gaussians
centered at 2 and -2, and with a standard deviation of 0.5, as
this simple distribution has a non-linear gradient function
and an analytical form of the optimal gradient approximator
f ∗. See Appendix D.1 for the formula and an illustration of
approximation with f ∗ with different σ values.

We let p be the density function of the MoG just described.
For a given gradient approximator f , we estimate the ex-
pected error Ep[|∇x log p(x) − f |] using 1000 i.i.d. samples
of x ∼ p. The results are presented in Figure 3. The curve
of the expected error of the optimal f ∗
ar shows the asymp-
totic error indeed shrinks to 0 as σ → 0, and it serves as a
theoretical lower bound on the overall approximation error.

Our ablation includes two steps of increments. First, modi-
fying the regular DAE (regDAE) to be of the residual form
(with loss rescaling, resDAE) largely reduces the param-

AR-DAE: Towards Unbiased Neural Entropy Gradient Estimation

eterization error and optimization error combined, as we
use the same architecture for the reconstruction function of
regDAE and for the residual function of resDAE. We also
experiment with annealing the σ values (as opposed to train-
ing each model individually): we take the model trained
with a larger σ to initialize the network that will be trained
with a slightly smaller σ. Annealing signiﬁcantly reduces
the error and thus validates the continuity of the optimal
f ∗
ar. All four curves have a jump in the error when σ gets
sufﬁciently small, indicating the difﬁculty of optimization
when the training signal diminishes. This leads us to our
second increment: amortization of training (i.e. AR-DAE).
We see that not only does the error of AR-DAE decrease
and transition more smoothly as σ gets closer to 0, but it
also signiﬁcantly outperforms the optimal f ∗
ar for large σ’s.
We hypothesize this is due to the choice of the distribution
over σ; N (0, δ2) concentrates around 0, which biases the
training of far to focus more on smaller values of σ.

5. Related Works

Denoising autoencoders were originally introduced to learn
useful representations for deep networks by Vincent et al.
(2008; 2010). It was later on noticed by Vincent (2011) that
the loss function of the residual form of DAE is equal to the
expected quadratic error ||f −∇x log pσ||2, where pσ(x(cid:48)) =
(cid:82) p(x)N (x(cid:48); x, σ2I)dx is the marginal distribution of the
perturbed data, to which the author refers as denoising score
matching. Minimizing expected quadratic error of this form
is in general known as score matching (Hyvärinen, 2005),
where ∇x log p is referred to as the score 1 of the density p.
And it is clear now when we convolve the data distribution
with a smaller amount of noise, the residual function f tends
to approximate ∇x log p(x) better. This is formalized by
Alain & Bengio (2014) as the limiting case of the optimal
DAE. Saremi et al. (2018); Saremi & Hyvarinen (2019)
propose to use the residual and gradient parameterizations
to train a deep energy model with denoising score matching.

As a reformulation of score matching, instead of explicitly
minimizing the expected square error of the score, the origi-
nal work of Hyvärinen (2005) proposes the Implicit score
matching and minimizes

Ep

(cid:20) 1
2

||f (x)||2 + tr(∇xf (x))

(cid:21)

.

(3)

Song et al. (2019) proposed a stochastic algorithm called the
sliced score matching to estimate the trace of the Jacobian,
which reduces the computational cost from O(d2
x) to O(dx)
(where dx is the dimensionality of x). It was later noted by
the same author that the computational cost of the sliced

1This is not to be confused with the score (or informant) in
statistics, which is the gradient of the log likelihood function wrt
the parameters.

1

2

3

4

1

Z e−U (x)

Aux
hierarchical

AR-DAE
hierarchical

AR-DAE
implicit

Figure 4. Fitting energy functions. First column: target energy
functions. Second column: auxiliary variational method for hier-
archical model. Third column: hierarchical model trained with
AR-DAE. Last column: implicit model trained with AR-DAE.

score matching is still much higher than that of the denoising
score matching (Song & Ermon, 2019).

Most similar to our work are Song & Ermon (2019) and
Bigdeli et al. (2020). Song & Ermon (2019) propose to
learn the score function of a data distribution, and propose
to sample from the corresponding distribution of the learned
score function using Langevin dynamics. They also propose
a conditional DAE trained with a sequence of σ’s in decreas-
ing order, and anneal the potential energy for the Langevin
dynamics accordingly to tackle the mixing problem of the
Markov chain. Bigdeli et al. (2020) propose to match the
score function of the data distribution and that of an implicit
sampler. As the resulting algorithm amounts to minimizing
the reverse KL divergence, their proposal can be seen as a
combination of Song & Ermon (2019) and our work.

Implicit density models are commonly seen in the context
of likelihood-free inference (Mescheder et al., 2017; Tran
et al., 2017; Li et al., 2017; Huszár, 2017). Statistics of an
implicit distribution are usually intractable, but there has
been an increasing interest in approximately estimating the
gradient of the statistics, such as the entropy (Li & Turner,
2018; Shi et al., 2018) and the mutual information (Wen
et al., 2020).

6. More Analyses and Experiments

6.1. Energy function ﬁtting

In Section 4.2, we have analyzed the error of approximat-
ing the gradient of the log-density function in the context
of a ﬁxed distribution. In reality, we usually optimize the
distribution iteratively, and once the distribution is updated,

AR-DAE: Towards Unbiased Neural Entropy Gradient Estimation

Figure 5. Density estimation with VAE on 5 × 5 grid MoG. 1st column: data sampled from the MoG. 2nd column: VAE+DAE in residual
form trained with a large σ value. 3rd column: VAE+DAE in residual form trained with a small σ value. 4th column: VAE+AR-DAE.
Last column: averaged log variance of z throughout training.

the gradient approximator also needs to be updated accord-
ingly to constantly provide accurate gradient signal. In this
section, we use the proposed entropy gradient estimator
to train an implicit sampler to match the density of some
unnormalized energy functions.

Concretely, we would like to approximately sample from
a target density which can be written as ptarget(x) ∝
exp(−U (x)), where U (x) is an energy function. We train
a neural sampler g by minimizing the reverse Kullback-
Leibler (KL) divergence

DKL(pg(x)||ptarget(x))
= −H(pg(x)) + E

x∼pg(x)

(cid:2)− log ptarget(x)(cid:3) .

(4)

where pg is the density induced by g. We use the target en-
ergy functions U proposed in Rezende & Mohamed (2015)
(see Table 4 for the formulas). The corresponding density
functions are illustrated at the ﬁrst column of Figure 4.

We consider two sampling procedures for g. The ﬁrst
let z be distributed by
one has a hierarchical structure:
N (0, I), and x be sampled from the conditional pg(x|z) :=
N (µg(z), σ2
g(z)) where µg and log σg are parameterized
by neural networks. The resulting marginal density has the
form pg(x) = (cid:82) pg(x|z)pg(z)dz, which is computationally
intractable due to the marginalization over z. We compare
against the variational method proposed by Agakov & Bar-
ber (2004), which lower-bounds the entropy by

H(pg(x)) ≥ −

E
x,z∼pg(x|z)p(z)

(cid:20)

log

(cid:21)

pg(x|z)p(z)
h(z|x)

.

(5)

Plugging (5) into (4) gives us an upper bound on the KL
divergence. We train pg and h jointly by minimizing this
upper bound 2 as a baseline.

The second sampling procedure has an implicit density: we
ﬁrst sample z ∼ N (0, I) and pass it through the generator
x = g(z). We estimate the gradient of the negentropy of
both the hierarchical and implicit models by following the
approximate gradient of the log density far ≈ log pg. The
experimental details can be found in Appendix E.

As shown in Figure 4, the density learned by the auxiliary
method sometimes fails to fully capture the target density.
As in this experiment, we anneal the weighting of the cross-
entropy term from 0.01 to 1, which is supposed to bias
the sampler to be rich in noise during the early stage of
training, the well-known mode seeking behavior of reverse
KL-minimization should be largely mitigated. This suggests
the imperfection of the density trained with the auxiliary
method is a result of the looseness of the variational lower
bound on entropy, which leads to an inaccurate estimate
of the gradient. On the other hand, the same hierarchical
model and the implicit model trained with AR-DAE both
exhibit much higher ﬁdelity. This suggests our method can
provide accurate gradient signal even when the sampler’s
distribution pg is being constantly updated. 3

6.2. Variational autoencoder

In the previous section, we have demonstrated that AR-
DAE can robustly approximate the gradient of the log den-
sity function that is constantly changing and getting closer
to some target distribution. In this section, we move on
to a more challenging application: likelihood-free infer-
ence for variational autoencoders (VAE, Kingma & Welling
(2014); Rezende et al. (2014a)). Let p(z) be the standard
normal. We assume the data is generated by x ∼ p(x|z)
which is parameterized by a deep neural network. To esti-
mate the parameters, we maximize the marginal likelihood
(cid:82) p(x|z)p(z)dz of the data x, sampled from some data dis-

2The normalizing constant of the target density will not affect

the gradient ∇x log ptarget(x) = −∇xU (x).

3We update far 5 times per update of pg to generate this ﬁgure;
we also include the results with less updates of far in Appendix E.

AR-DAE: Towards Unbiased Neural Entropy Gradient Estimation

log p(x)

Gaussians. See Appendix F.3 for the experimental details.

Gaussian†
HVI aux†
AVB†

Gaussian
HVI aux
HVI AR-DAE (ours)
IVI AR-DAE (ours)

MLP
-85.0
-83.8
-83.7

-84.40
-84.63
-83.42
-83.62

Conv
-81.9
-81.6
-81.7

-81.82
-81.87
-81.46
-81.26

ResConv
-
-
-

-80.75
-80.80
-80.45
-79.18

Table 1. Dynamically binarized MNIST. †Results taken from
Mescheder et al. (2017).

Model
(Models with a trained prior)
VLAE (Chen et al., 2016)
PixelHVAE + VampPrior (Tomczak & Welling, 2018)

(Models without a trained prior)
VAE IAF (Kingma et al., 2016)
VAE NAF (Huang et al., 2018)
Diagonal Gaussian
IVI AR-DAE (ours)

log p(x)

-79.03
-79.78

-79.88
-79.86
-81.43
-79.61

Table 2. Statically binarized MNIST.

Figure 6. Generated samples (the mean value of the decoder) of
the IVI AR-DAE trained on statically binarized MNIST.

tribution pdata(x). Since the marginal likelihood is usually
intractable, the standard approach is to maximize the evi-
dence lower bound (ELBO):

log p(x) ≥ E

z∼q(z|x)

[log p(x, z) − log q(z|x)] ,

(6)

where q(z|x) is an amortized variational posterior distribu-
tion. The ELBO allows us to jointly optimize p(x|z) and
q(z|x) with a uniﬁed objective.

Note that the equality holds iff q(z|x) = p(z|x), which mo-
tivates using more ﬂexible families of variational posterior.
Note that this is more challenging for two reasons: the target
distribution p(z|x) is constantly changing and is conditional.
Similar to Mescheder et al. (2017), we parameterize a con-
ditional sampler z = g((cid:15), x), (cid:15) ∼ N (0, I) with an implicit
q(z|x). We use AR-DAE to approximate ∇z log q(z|x) and
estimate the entropy gradient to update the encoder while
maximizing the ELBO. To train AR-DAE, instead of ﬁx-
ing the prior variance δ in the Lar we adaptively choose
δ for different data points. See Appendix F for a detailed
description of the algorithm and heuristics we use.

Toy dataset To demonstrate the difﬁculties in inference,
we train a VAE with a 2-D latent space on a mixture of 25

In Figure 5, we see that if a ﬁxed σ is chosen to be too
large for the residual DAE, the DAE tends to underestimate
the gradient of the entropy, so the variational posteriors col-
lapse to point masses. If σ is too small, the DAE manages
to maintain a non-degenerate variational posterior, but the
inaccurate gradient approximation results in a non-smooth
encoder and poor generation quality. On the contrary, the
same model trained with AR-DAE has a very smooth en-
coder that maps the data into a Gaussian-shaped, aggregated
posterior and approximates the data distribution accurately.

MNIST We ﬁrst demonstrate the robustness of our
method on different choices of architectures for VAE: (1)
a one-hidden-layer fully-connected network (denoted by
MLP), (2) a convolutional network (denoted by Conv), and
(3) a larger convolutional network with residual connections
(denoted by ResConv) from (Huang et al., 2018). The ﬁrst
two architectures are taken from Mescheder et al. (2017) for
a direct comparison with the adversarially trained implicit
variational posteriors (AVB). We also implement a diago-
nal Gaussian baseline and the auxiliary hierarchical method
(HVI aux, (Maaløe et al., 2016)). We apply AR-DAE to
estimate the entropy gradient of the hierarchical posterior
and the implicit posterior (denoted by HVI AR-DAE and
IVI AR-DAE, respectively). As shown in Table 1, AR-DAE
consistently improves the quality of inference in compari-
son to the auxiliary variational method and AVB, which is
reﬂected by the better likelihood estimates.

We then compare our method with state-of-the-art VAEs
evaluated on the statically binarized MNIST dataset
(Larochelle & Murray, 2011). We use the implicit distri-
bution with the ResConv architecture following the previ-
ous ablation. As shown in Table 2, the VAE trained with
AR-DAE demonstrates state-of-the-art performance among
models with a ﬁxed prior. Generated samples are presented
in Figure 6.

6.3. Entropy-regularized reinforcement learning

We now apply AR-DAE to approximate entropy gradient in
the context of reinforcement learning (RL). We use the soft
actor-critic (SAC, Haarnoja et al. (2018)), a state-of-the-art
off-policy algorithm for continuous control that is designed
to encourage exploration by regularizing the entropy of the
policy. We train the policy π(a|s) to minimize the following
objective:

L(π) = E
s∼D

(cid:18)

(cid:20)
DKL

π(a|s)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

exp (Q(s, a))
Z(s)

(cid:19)(cid:21)

,

where D is a replay buffer of the past experience of the agent,
Q is a “soft” state-action value function that approximates
the entropy-regularized expected return of the policy, and

AR-DAE: Towards Unbiased Neural Entropy Gradient Estimation

Figure 7. Continuous control in reinforcement learning. SAC: soft actor-critic with diagonal Gaussian. SAC-NF: soft actor-critic with
normalizing ﬂows. SAC-AR-DAE: soft actor-critic with implicit distribution trained with AR-DAE. The shaded area indicates the standard
error with 5 runs.

Z(s) = (cid:82)
a exp(Q(s, a))da is the normalizing constant of
the Gibbs distribution. A complete description of the SAC
algorithm can be found in Appendix G.1. We compare with
the original SAC that uses a diagonal Gaussian distribution
as policy and a normalizing ﬂow-based policy proposed by
Mazoure et al. (2019). We parameterize an implicit policy
and use AR-DAE to approximate ∇a log π(a|s) to estimate

∇φL(π)
= E
s∼D
a∼π

(cid:124)
[[∇a log πφ(a|s) − ∇aQ(s, a)]

Jφgφ((cid:15), s)] , (7)

where π(a|s) is implicitly induced by a = gφ((cid:15), s) with
(cid:15) ∼ N (0, I). We parameterize far as the gradient of a
scalar function Far, so that Far can be interpreted as the
unnormalized log-density of the policy which will be used
to update the soft Q-network. We run our experiments on
ﬁx continuous control environments from the OpenAI gym
benchmark suite (Brockman et al., 2016) and Rllab (Duan
et al., 2016). The experimental details can be found in
Appendix G.2.

The results are presented in Table 3 and Figure 7. We see
that SAC-AR-DAE using an implicit policy improves the
performance over SAC-NF. This also shows the approxi-
mate gradient signal of AR-DAE is stable and accurate even
for reinforcement learning. The extended results for a full
comparison of the methods are provided in Table 5 and 6.

6.4. Maximum entropy modeling

As a last application, we apply AR-DAE to solve the con-
strained optimization problem of the maximum entropy prin-
ciple. Let m ∈ R10 be a random vector and B ∈ R10×10 be

SAC

SAC-NF

SAC-AR-DAE

HalfCheetah-v2
Ant-v2
Hopper-v2
Walker-v2
Humanoid-v2
Humanoid (rllab)

9695 ± 879
5345 ± 553
3563 ± 119
4612 ± 249
5965 ± 179
6099 ± 8071

9325 ± 775
4861 ± 1091
3521 ± 129
4760 ± 624
5467 ± 44
3442 ± 3736

10907 ± 664
6190 ± 128
3556 ± 127
4793 ± 395
6275 ± 202
10739 ± 10335

Table 3. Maximum average return. ± corresponds to one standard
deviation over ﬁve random seeds.

Figure 8. Maximum entropy principle experiment. Left: estimated
EMD of (red) the implicit distribution trained with AR-DAE and
(green) the IAF. Right: the estimated EMD of the implicit distribu-
tion minus that of the IAF.

a random matrix with mi and Bij drawn i.i.d. from N (0, 1).
It is a standard result that among the class of real-valued ran-
dom vectors x ∈ R10 satisfying the constraints E[x] = m
and Var(x) = B(cid:62)B, x ∼ N (m, B(cid:62)B) has the maximal
entropy. Similar to Loaiza-Ganem et al. (2017), we solve
this constrained optimization problem but with an implicit
distribution. We use the penalty method and increasingly
penalize the model to satisfy the constraints. Concretely, let
˜m and ˜C be the sample mean and sample covariance matrix,
respectively, estimated with a batch size of 128. We mini-
mize the modiﬁed objective −H(pθ(x)) + λ (cid:80)
j∈{1,2} c2
j ,
where c1 = || ˜m − m||2 and c2 = || ˜C − B(cid:62)B||F , with

AR-DAE: Towards Unbiased Neural Entropy Gradient Estimation

increasing weighting λ on the penalty. We estimate the
entropy gradient using AR-DAE, and compare against the
inverse autoregressive ﬂows (IAF, Kingma et al. (2016)). At
the end of training, we estimate the earth mover’s distance
(EMD) from N (m, B(cid:62)B).

We repeat the experiment 256 times and report the histogram
of EMD in Figure 8. We see that most of the time the im-
plicit model trained with AR-DAE has a smaller EMD,
indicating the extra ﬂexibility of arbitrary parameterization
allows it to satisfy the geometry of the constraints more eas-
ily. We leave some more interesting applications suggested
in Loaiza-Ganem et al. (2017) for future work.

7. Conclusion

We propose AR-DAE to estimate the entropy gradient of
an arbitrarily parameterized data generator. We identify the
difﬁculties in approximating the log density gradient with
a DAE, and demonstrate the proposed method signiﬁcantly
reduces the approximation error. In theory, AR-DAE ap-
proximates the zero-noise limit of the optimal DAE, which
is an unbiased estimator of the entropy gradient. We apply
our method to a suite of tasks and empirically validate that
AR-DAE provides accurate and reliable gradient signal to
maximize entropy.

Acknowledgments

We would like to thank Guillaume Alain for an insightful
discussion on denoising autoencoders. Special thanks to
people who have provided their feedback and advice during
discussion, including Bogdan Mazoure and Thang Doan
for sharing the code on the RL experiment; to Joseph Paul
Cohen for helping optimize the allocation of computational
resources. We thank CIFAR, NSERC and PROMPT for their
support of this work.

References

Agakov, F. V. and Barber, D. An auxiliary variational

method. In ICONIP, 2004.

Alain, G. and Bengio, Y. What regularized auto-encoders
learn from the data-generating distribution. J. Mach.
Learn. Res., 2014.

Balaji, Y., Hassani, H., Chellappa, R., and Feizi, S. Entropic
gans meet vaes: A statistical approach to compute sample
likelihoods in gans. In ICML, 2019.

Bertsekas, D. Nonlinear programming. 3rd edn. massachus-

sets: Athena scientiﬁc, 2016.

density estimators. arXiv preprint arXiv:2001.02728,
2020.

Brockman, G., Cheung, V., Pettersson, L., Schneider, J.,
Schulman, J., Tang, J., and Zaremba, W. Openai gym.
arXiv preprint arXiv:1606.01540, 2016.

Burda, Y., Grosse, R. B., and Salakhutdinov, R. Importance

weighted autoencoders. In ICLR, 2016.

Chen, X., Kingma, D. P., Salimans, T., Duan, Y., Dhariwal,
P., Schulman, J., Sutskever, I., and Abbeel, P. Variational
lossy autoencoder. In ICLR, 2016.

Dieng, A. B., Ruiz, F. J. R., Blei, D. M., and Titsias,
M. K. Prescribed generative adversarial networks. arXiv
preprint arXiv:1910.04302, 2019.

Duan, Y., Chen, X., Houthooft, R., Schulman, J., and
Abbeel, P. Benchmarking deep reinforcement learning
for continuous control. In ICML, 2016.

Durrett, R. Probability: theory and examples, volume 49.

Cambridge university press, 2019.

Fujimoto, S., van Hoof, H., and Meger, D. Addressing
function approximation error in actor-critic methods. In
ICML, 2018.

Geyer, C. J. Reweighting monte carlo mixtures. Technical

report, University of Minnesota, 1991.

Ha, D., Dai, A. M., and Le, Q. V. Hypernetworks. In ICLR,

2017.

Haarnoja, T., Tang, H., Abbeel, P., and Levine, S. Rein-
forcement learning with deep energy-based policies. In
ICML, 2017.

Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft actor-
critic: Off-policy maximum entropy deep reinforcement
learning with a stochastic actor. In ICML, 2018.

Hasselt, H. V. Double q-learning. In NIPS, 2010.

He, K., Zhang, X., Ren, S., and Sun, J. Deep residual

learning for image recognition. In CVPR, 2016.

Huang, C., Krueger, D., Lacoste, A., and Courville, A. C.

Neural autoregressive ﬂows. In ICML, 2018.

Huszár, F. Variational inference using implicit distributions.

arXiv preprint arXiv:1702.08235, 2017.

Hyvärinen, A. Estimation of non-normalized statistical
models by score matching. Journal of Machine Learning
Research, 6(Apr):695–709, 2005.

Bigdeli, S. A., Lin, G., Portenier, T., Dunbar, L. A., and
Zwicker, M. Learning generative models using denoising

Kingma, D. P. and Welling, M. Auto-encoding variational

bayes. ICLR, 2014.

AR-DAE: Towards Unbiased Neural Entropy Gradient Estimation

Kingma, D. P., Salimans, T., Józefowicz, R., Chen, X.,
Improving variational
Sutskever, I., and Welling, M.
autoencoders with inverse autoregressive ﬂow. In NIPS,
2016.

Kumar, A., Poole, B., and Murphy, K. Regularized au-
toencoders via relaxed injective probability ﬂow. arXiv
preprint arXiv:2002.08927, 2020.

Larochelle, H. and Murray, I. The neural autoregressive

distribution estimator. In AISTATS, 2011.

Li, Y. and Turner, R. E. Gradient estimators for implicit

models. In ICLR, 2018.

Li, Y., Turner, R. E., and Liu, Q. Approximate inference
with amortised mcmc. arXiv preprint arXiv:1702.08343,
2017.

Loaiza-Ganem, G., Gao, Y., and Cunningham, J. P.
arXiv preprint

Maximum entropy ﬂow networks.
arXiv:1701.03504, 2017.

Maaløe, L., Sønderby, C. K., Sønderby, S. K., and Winther,
O. Auxiliary deep generative models. arXiv preprint
arXiv:1602.05473, 2016.

Mazoure, B., Doan, T., Durand, A., Hjelm, R. D., and
Pineau, J. Leveraging exploration in off-policy algorithms
via normalizing ﬂows. arXiv preprint arXiv:1905.06893,
2019.

Mescheder, L. M., Nowozin, S., and Geiger, A. Adversarial
variational bayes: Unifying variational autoencoders and
generative adversarial networks. In ICML, 2017.

Minka, T. et al. Divergence measures and message passing.
Technical report, Technical report, Microsoft Research,
2005.

Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Ve-
ness, J., Bellemare, M. G., Graves, A., Riedmiller, M. A.,
Fidjeland, A., Ostrovski, G., Petersen, S., Beattie, C.,
Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wier-
stra, D., Legg, S., and Hassabis, D. Human-level control
through deep reinforcement learning. Nature, 518(7540):
529–533, 2015.

Mohamed, S. and Lakshminarayanan, B.

ing in implicit generative models.
arXiv:1610.03483, 2016.

Learn-
arXiv preprint

Odena, A., Buckman, J., Olsson, C., Brown, T. B., Olah, C.,
Raffel, C., and Goodfellow, I. Is generator conditioning
arXiv preprint
causally related to gan performance?
arXiv:1802.08768, 2018.

Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E.,
DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer,
A. Automatic differentiation in pytorch. 2017.

Polyak, B. T. and Juditsky, A. B. Acceleration of stochastic
approximation by averaging. SIAM Journal on Control
and Optimization, 30(4):838–855, 1992.

Ranganath, R., Tran, D., and Blei, D. Hierarchical varia-

tional models. In ICML, 2016.

Rezende, D. J. and Mohamed, S. Variational inference with

normalizing ﬂows. In ICML, 2015.

Rezende, D. J., Mohamed, S., and Wierstra, D. Stochas-
tic backpropagation and approximate inference in deep
generative models. In ICML, 2014a.

Rezende, D. J., Mohamed, S., and Wierstra, D. Stochastic
backpropagation and approximate inference in deep gen-
erative models. arXiv preprint arXiv:1401.4082, 2014b.

Roeder, G., Wu, Y., and Duvenaud, D. K. Sticking the
landing: Simple, lower-variance gradient estimators for
variational inference. In NIPS, 2017.

Saremi, S. and Hyvarinen, A. Neural empirical bayes. Jour-

nal of Machine Learning Research, 20:1–23, 2019.

Saremi, S., Mehrjou, A., Schölkopf, B., and Hyvärinen,
A. Deep energy estimator networks. arXiv preprint
arXiv:1805.08306, 2018.

Savitzky, A. and Golay, M. J. Smoothing and differentiation
of data by simpliﬁed least squares procedures. Analytical
chemistry, 36(8):1627–1639, 1964.

Shalev-Shwartz, S., Shamir, O., and Shammah, S. Failures

of gradient-based deep learning. In ICML, 2017.

Shi, J., Sun, S., and Zhu, J. A spectral approach to gradient
estimation for implicit distributions. In ICML, 2018.

Song, Y. and Ermon, S. Generative modeling by estimating
gradients of the data distribution. In NeurIPS, 2019.

Song, Y., Garg, S., Shi, J., and Ermon, S. Sliced score match-
ing: A scalable approach to density and score estimation.
In UAI, 2019.

Sutton, R. S., Barto, A. G., et al. Introduction to reinforce-
ment learning, volume 2. MIT press Cambridge, 1998.

Tomczak, J. and Welling, M. Vae with a vampprior. In

AISTATS, 2018.

Tran, D., Ranganath, R., and Blei, D. Hierarchical implicit
models and likelihood-free variational inference. In NIPS,
2017.

AR-DAE: Towards Unbiased Neural Entropy Gradient Estimation

Vincent, P. A connection between score matching and de-
noising autoencoders. Neural computation, 23(7):1661–
1674, 2011.

Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.
Extracting and composing robust features with denoising
autoencoders. In ICML, 2008.

Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., and Man-
zagol, P.-A. Stacked denoising autoencoders: Learning
useful representations in a deep network with a local de-
noising criterion. Journal of machine learning research,
11(Dec):3371–3408, 2010.

Wen, L., Zhou, Y., He, L., Zhou, M., and Xu, Z. Mutual in-
formation gradient estimation for representation learning.
In ICLR, 2020.

Ziebart, B. D. Modeling purposeful adaptive behavior with

the principle of maximum causal entropy. 2010.

AR-DAE: Towards Unbiased Neural Entropy Gradient Estimation

A. Gradient of the entropy with respect to density functions

Consider a probability density function pg(x). We assume pg is the pushforward of some prior distribution p(z) by a
mapping gθ : z (cid:55)→ x. Our goal is to compute the gradient of the entropy of pg wrt the parameter θ. Following Roeder et al.
(2017), we show that the entropy gradient can be rewritten as Equation (1).

Proof. By the law of the unconscious statistician∗ (LOTUS, Theorem 1.6.9 of Durrett (2019)), we have

∇θH(pg(x)) = ∇θ

E
x∼pg(x)

[− log pg(x)]

∗= ∇θ E
z∼p(z)
(cid:90)

[− log pg(gθ(z))]

= −∇θ

p(z) log pg(gθ(z))dz

(cid:90)

(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)

p(z)∇θ log pg(x)|x=gθ(z)dz −

−

=

(cid:90)

p(z)[∇x log pg(x)|x=gθ(z)](cid:124)Jθgθ(z)dz

= − E

z∼p(z)

(cid:2)[∇x log pg(x)|x=gθ(z)](cid:124)Jθgθ(z)(cid:3) .

where the crossed-out term is due to the following identity

E
z∼p(z)

(cid:2)∇θ log pg(x)(cid:12)

(cid:12)x=gθ(z)

(cid:3) = E

[∇θ log pg(x)] =

(cid:90)

pg(x)∇θ log pg(x)dx

x∼pg(x)
(cid:90)

(cid:8)(cid:8)(cid:8)
pg(x)

=

∇θpg(x)dx = ∇θ

(cid:90)

1
(cid:8)(cid:8)
pg(x)
(cid:8)

pg(x)dx = ∇θ1 = 0.

B. Properties of residual DAE

Proposition 1. Let x and u be distributed by p(x) and N (0, I). For σ (cid:54)= 0, the minimizer of the functional Ex,u[||u +
σf (x + σu)||2] is almost everywhere determined by

f ∗(x; σ) =

− Eu[p(x − σu)u]
σ Eu[p(x − σu)]

.

Furthermore, if p(x) and its gradient are both bounded, f ∗ is continuous wrt σ for all σ ∈ R \ 0 and limσ→0 f ∗(x; σ) =
∇x log pg(x).

Proof. For simplicy, when the absolute value and power are both applied to a vector-valued variable, they are applied
elementwise. The characterization of the optimal function f ∗ can be derived by following Alain & Bengio (2014). For the
second part, the symmetry of the distribution of u implies

f ∗(x; σ) =

=

− Eu[p(x − σu)u]
σ Eu[p(x − σu)]
Eu[p(x + σu)u]
σ Eu[p(x + σu)]

= f ∗(x; −σ),

so we only need to show f ∗ is continuous for σ > 0. Since p is bounded, by the dominated convergence theorem (DOM),
both Eu[p(x − σu)u] and Eu[p(x − σu)] are continuous for σ > 0, and so is f ∗(x, σ).

Lastly, an application of L’Hôpital’s rule gives

lim
σ→0

f ∗(x; σ) = lim
σ→0

d
dσ
d

Eu[p(x + σu)u]
dσ σ Eu[p(x + σu)]

,

AR-DAE: Towards Unbiased Neural Entropy Gradient Estimation

which by another application of DOM (since gradient of p is bounded) is equal to

Applying DOM a ﬁnal time gives

E[∇p(x + σu)(cid:62)uu]
E[p(x + σu)] + σ E[∇p(x + σu)(cid:62)u]

.

lim
σ→0

f ∗(x; σ) =

lim
σ→0

∇p(x) (cid:12) E[u2]
p(x)

= ∇ log p(x).

Proposition 2. limσ→∞

f ∗(x;σ)

∇x log N (x;Ep[X],σ2I) → 1.

Proof. We rewrite the optimal gradient approximator as

f ∗(x; σ) =

(cid:90)

1
σ2

N (u; 0, I)p(x − σu)

(cid:82) N (u(cid:48); 0, I)p(x − σu(cid:48))du(cid:48) · σu du.

Changing the variables (cid:15) = σu and (cid:15)(cid:48) = σu(cid:48) gives

(cid:90)

1
σ2

N ((cid:15)/σ; 0, I)p(x − (cid:15))

(cid:82) N ((cid:15)(cid:48)/σ; 0, I)p(x − (cid:15)(cid:48))d(cid:15)(cid:48) · (cid:15) d(cid:15),

which can be written as 1
By DOM (applied to the numerator and denominator separately, since the standard Gaussian density is bounded), Eq[(cid:15)] →
(cid:82) p(x − (cid:15))(cid:15) d(cid:15) as σ → ∞. The latter integral is equal to Ep[X] − x (which can be seen by substituting y = x − (cid:15)).

σ2 Eq((cid:15))[(cid:15)] where q((cid:15)) ∝ N ((cid:15)/σ; 0, I)p(x − (cid:15)) is the change-of-variable density.

C. Signal-to-noise ratio analysis on DAE’s gradient

Fixing x and u, the gradient of the L2 loss can be written as

∆ := ∇||u + σf (x + σu)||2 = ∇

(cid:32)

(cid:88)

(ui + σfi(x + σu))2

i

(cid:33)

(cid:88)

=

i

∇(ui + σfi(x + σu))2,

where i iterates over the entries of the vectors u and f , and ∇ denotes the gradient wrt the parameters of f . We further
expand the gradient of the summand via chain rule, which yields

∇(ui + σfi(x + σu))2 = 2σ(ui + σfi(x + σu))∇fi(x + σu)


= 2σ

ui∇ fi(x + σu)
(cid:125)

(cid:124)

(cid:123)(cid:122)
A

+σ fi(x + σu)
(cid:125)

(cid:124)

(cid:123)(cid:122)
B



 .
∇ fi(x + σu)
(cid:125)
(cid:123)(cid:122)
C

(cid:124)

Taylor theorem with the mean-value form of the remainder allows us to approximate fi(x + σu) by fi(x) as σ is small:

fi(x + σu) = fi(x) + σ∇xfi(ˆx)(cid:62)u

= fi(x) + σ∇xfi(x)(cid:62)u +

σ2
2

u(cid:62)∇2

xfi(˜x)u,

(8)

(9)

where ∇x denotes the gradient wrt the input of f , and ˆx and ˜x are points lying on the line interval connecting x and x + σu.
Plugging (9) into A and (8) into B and C gives

(cid:18)

(cid:18)

2σ

ui∇

fi(x) + σ∇xfi(x)(cid:62)u +

(cid:19)

u(cid:62)∇2

xfi(˜x)u

σ2
2

+ σ (cid:0)fi(x) + σ∇xfi(ˆx)(cid:62)u(cid:1) ∇ (cid:0)fi(x) + σ∇xfi(ˆx)(cid:62)u(cid:1)

(cid:19)

= 2σui∇fi(x) + 2σ2ui∇∇xfi(x)(cid:62)u + σ3ui∇u(cid:62)∇2

xfi(˜x)u

+ 2σ2fi(x)∇fi(x) + 2σ3fi(x)∇∇xfi(ˆx)(cid:62)u + 2σ3∇x

(cid:0)fi(ˆx)(cid:62)u(cid:1) ∇fi(x) + 2σ4∇x

(cid:0)fi(ˆx)(cid:62)u(cid:1) ∇∇xfi(ˆx)(cid:62)u.

AR-DAE: Towards Unbiased Neural Entropy Gradient Estimation

With some regularity conditions (DOM-style assumptions), marginalizing out u and taking σ to be arbitrarily small yield

Eu[∆] =

(cid:88)

2σ2∇

∂
∂xi

fi(x) + 2σ2fi(x)∇fi(x) + o(σ2)

i

= 2σ2∇

(cid:18)

tr(∇xf (x)) +

(cid:19)

||f (x)||2

1
2

+ o(σ2).

In fact, we note that the ﬁrst term is the stochastic gradient of the implicit score matching objective (Theorem 1, Hyvärinen
(2005)), but it vanishes at a rate O(σ2) as σ2 → 0.

For the second moment, similarly,

Eu[∆∆(cid:62)] = 4σ2 (cid:88)

∇fi(x)∇fi(x)(cid:62) + o(σ2).

i

As a result,

E[∆]
(cid:112)Var(∆)

=

E[∆]
(cid:112)E(∆∆(cid:62)) − E(∆)E(∆)(cid:62)

=

O(σ2)
(cid:112)O(σ2) − O(σ4)

= O(σ).

D. Experiment: Error analysis

D.1. Main experiments

Figure S1. Left: Density function of mixture of Gaussians. Right: gradient of the log density function (dotdash line) and gradient
approximations using optimal DAE with different σ values (solid lines).

Dataset and optimal gradient approximator As we have described in Section 4.2, we use the mixture of two Gaussians
to analyze the approximation error (see Figure S1 (left)). Formally, we deﬁne p(x) = 0.5N (x; 2, 0.25)+0.5N (x; −2, 0.25).
For notational convenience, we let p1 and p2 be the density functions of these two Gaussians, respectively. We obtain
∇x log p(x) by differentiating log p(x) wrt x using auto-differentiation library such as PyTorch (Paszke et al., 2017). With
some elementary calculation, we can expand the formula of the optimal gradient approximator f ∗ as,

f ∗(x; σ) =

− Eu[p(x − σu)u]
σ Eu[p(x − σu)]

=

− (cid:80)2
σ (cid:80)2

i=1 S(cid:48)
iµ(cid:48)
i
i=1 S(cid:48)
i

,

2π(0.52+12) exp (cid:0)−(µi+x/σ)2/2(0.52+12)(cid:1) for i ∈ 1, 2, µ1 = −2, and µ1 = 2.

where S(cid:48)

i = 1/

√

Proof. The numerator Eu[Eu[p(x − σu)u](x − σu)u] can be rewritten as follows:

[p(x − σu)u] =

E
u

(cid:90)

(0.5p1(x − σu) + 0.5p2(x − σu)) p(u)u du =

0.5
σ

2
(cid:88)

i=1

(cid:90)

S(cid:48)
i

N (u; µ(cid:48)

i, σ(cid:48)

i)u du =

0.5
σ

2
(cid:88)

i=1

iµ(cid:48)
S(cid:48)
i,

where S(cid:48)

i = 1/

√

2π(0.52+12) exp (cid:0)−(µi+x/σ)2/2(0.52+12)(cid:1) for i ∈ 1, 2, µ1 = −2, and µ1 = 2.

AR-DAE: Towards Unbiased Neural Entropy Gradient Estimation

The second equality comes from the fact that all p1, p2, and p(u) are normal distributions, and thus we have

pi(x − σu)p(u) =

1
σ

pi (u − x/σ) p(u) =

1
σ

S(cid:48)
iN (u; µ(cid:48)

i, σ(cid:48)

i).

Similarly, we can rewrite the denominator as Eu[p(x − σu)] = 0.5
σ

(cid:80)2

i=1 S(cid:48)
i.

Experiments For AR-DAE, we indirectly parameterize it as the gradient of some scalar-function (which can be thought
of as an unnormalized log-density function); i.e. we deﬁne a scalar function and use its gradient wrt the input vector. The
same trick has also been employed in recent work by Saremi et al. (2018); Saremi & Hyvarinen (2019). We use the network
architecture with the following conﬁguration4: [2 + 1, 256] + [256, 256] × 2 + [256, 1], with the Softplus activation
function. We use the same network architecture for resDAE except it doesn’t condition on σ. For regDAE, the network is set
to reconstruct input.

All models are trained for 10k iterations with a minibatch size of 256. We use the Adam optimizer for both AR-DAE and
the generator, with the default β1 = 0.9 and β2 = 0.999. For all models, the learning rate is initially set to 0.001 and is
reduced by half every 1k iterations during training.

For regDAE and resDAE, we train models individually for every σ value in Figure 3. For regDAEannealed and resDAEannealed,
we anneal σ from 1 to the target value.For AR-DAE, δ is set to 0.05 and we sample 10 σ’s from N (0, δ2) for each iteration.
We train all models ﬁve times and present the mean and its standard error in the ﬁgures.

D.2. Symmetrizing the distribution of σ

In Section 4.1, we argue that neural networks are not suitable for extrapolation (vs. interpolation), to motivate the use
of a symmetric prior over σ. To contrast the difference, we sample σ ∼ N (0, δ2) and compare two different types of
σ-conditioning: (1) conditioning on σ, and (2) conditioning on |σ|. We use the same experiment settings in the previous
section, but we use a hypernetwork (Ha et al., 2017) that takes σ (resp. |σ|) as input and outputs the parameters of AR-DAE,
to force AR-DAE to be more dependent on the value of σ (resp. |σ|). The results are shown in Figure S2.

We see that the two conditioning methods result in two distinct approximation behaviors. First, when AR-DAE only observes
positive values, it fails to extrapolate to the σ values close to 0. When a symmetric σ distribution is used, the approximation
error of AR-DAE is more smooth. Second, we notice that the symmetric σ distribution bias far to focus more on small
σ values. Finally, the asymmetric distribution helps AR-DAE reduce the approximation error for some σ. We speculate
that AR-DAE with the asymmetric σ distribution has two times higher to observe small σ-values during training, and thus
improves the approximation. In general, we observe that the stability of the approximation is important for our applications,
in which case AR-DAE need to adapt constantly in the face of non-stationary distributions.

Figure S2. Comparison of two σ-conditioning methods to approximate log density gradient of 1D-MOG. AR-DAE: conditioning on σ.
AR-DAE (|σ|): conditioning on |σ|. σ is sampled from N (0, δ) for all experiments.

4[dinput, doutput] denotes a fully-connected layer whose input and output feature sizes are dinput and doutput, respectively.

AR-DAE: Towards Unbiased Neural Entropy Gradient Estimation

E. Experiment: Energy Fitting

(cid:16)

2 [ z1−2

0.6 ]2

e− 1

+ e− 1

2 [ z1+2

0.6 ]2(cid:17)

Potential U (z)
(cid:17)2
(cid:16) (cid:107)z(cid:107)−2
0.4
(cid:16) z2−w1(z)
0.4
(cid:18)

1: 1
2
2: 1
2

3: − ln

− ln
(cid:17)2

e− 1

2

(cid:104) z2−w1(z)
0.35

(cid:105)2

+ e− 1

2

(cid:104) z2−w1(z)+w2(z)
0.35

(cid:18)

e− 1

2

(cid:104) z2−w1(z)
0.4

(cid:105)2

+ e− 1

2

4: − ln

(cid:104) z2−w1(z)+w3(z)
0.35

(cid:105)2(cid:19)

(cid:105)2(cid:19)

where w1(z) = sin (cid:0) 2πz1

4

(cid:1), w2(z) = 3e− 1

2 [ z1−1

0.6 ]2

, w3(z) = 3σ (cid:0) z1−1

0.3

(cid:1), σ(x) = 1

1+e−x .

Table 4. The target energy functions introduced in Rezende & Mohamed (2015).

E.1. Main experiments

Parametric densities trained by minimizing the reverse KL divergence tend to avoid “false positive”, a well known problem
known as the zero-forcing property (Minka et al., 2005). To deal with this issue, we minimize a modiﬁed objective:

DKLα(pg(x)||ptarget(x)) = −H(pg(x)) − α E

x∼pg(x)

(cid:2)log ptarget(x)(cid:3) ,

(10)

where α is annealed from a small value to 1.0 throughout training. This slight modiﬁcation of the objective function
“convexiﬁes” the loss landscape and makes it easier for the parametric densities to search for the lower energy regions. For
AR-DAE training, we use Equation (2) with a ﬁxed prior variance δ = 0.1.

For all experiments, we use a three-hidden-layer MLP for both hierarchical distribution as well as implicit distribution. More
speciﬁcally, the generator network for the hierarchical distribution has the following conﬁguration: [dz, 256] + [256, 256] × 2
+ [256, 2] × 2. dz indicates the dimension of the prior distribution p(z) and is set to 2. The last two layers are for mean and
log-variance5 of the conditional distribution pg(x|z). For the auxiliary variational method, the same network architecture
is used for h(z|x) in Equation (5). When we train the hierarchical distribution with AR-DAE, we additionally clamp the
log-variance to be higher than -4. Similar to the hierarchical distribution, the generator of the implicit distribution is deﬁned
as, [dz, 256] + [256, 256] × 2 + [256, 2]. Unlike the hierarchical distribution, dz is set to 10. ReLU activation function is
used for all but the ﬁnal output layer.

For AR-DAE, we directly parameterize the residual function far. We use the following network architecture: [2, 256] +
[256, 256] × 2 + [256, 2]. Softplus activation function is used.

Each model is trained for 100,000 iterations with a minibatch size of 1024. We update AR-DAE Nd times per generator
update. For the main results, we set Nd = 5. We use the Adam optimizer for both the generator and AR-DAE, where
β1 = 0.5 and β2 = 0.999. The learning rate for the generator is initially set to 0.001 and is reduced by 0.5 for every 5000
iterations during training. AR-DAE’s learning rate is set to 0.001. To generate the ﬁgure, we draw 1M samples from each
model to ﬁll up 256 equal-width bins of the 2D histogram.

E.2. Effect of the number of updates (Nd) of the gradient approximator

In addition to the main results, we also analyze how the number of updates of AR-DAE per generator update affects the
quality of the generator. We use the same implicit generator and AR-DAE described in the main paper, but vary Nd from 1
to 5. The result is illustrated in Figure S3. In principle, the more often we update AR-DAE, the more accurate (or up-to-date)
the gradient approximation will be. This is corroborated by the improved quality of the trained generator.

5diagonal elements of the covariance matrix in log-scale

AR-DAE: Towards Unbiased Neural Entropy Gradient Estimation

1

2

3

4

1

Z e−U (x)
(a)

Nd = 1

Nd = 2

Nd = 5

dz = 2

dz = 3

dz = 10

(b)

(c)

Figure S3. Fitting energy functions with implicit model using AR-DAE. (a) Target energy functions. (b) Varying number of AR-DAE
updates per model update. (c) Varying the dimensionality of the noise source dz.

E.3. Effect of the noise dimension of implicit model

In this section, we study the effect of varying the dimensionality of the noise source of the implicit distribution. We use the
same experiment settings in the previous section. In Figure S3 (right panel), we see that the generator has a degenerate
distribution when dz = 2, and the degeneracy can be remedied by increasing dz.

F. Experiment: variational autoencoders

F.1. VAE with the entropy gradient approximator

Let pω(x|z) be the conditional likelihood function parameterized by ω and p(z) be the the prior distribution. We let p(z) be
the standard normal. As described in Section 6.2, we would like to maximize the ELBO (denoted as LELBO) by jointly
training pω and the amortized variational posterior qφ(z|x). Similar to Appendix A, the posterior qφ(z|x) can be induced
by a mapping gφ : (cid:15), x (cid:55)→ z with a prior q((cid:15)) that does not depend on the parameter φ. The gradient of LELBO wrt the
parameters of the posterior can be written as,

∇φLELBO(q) =

E
z∼qφ(z|x)
x∼pdata(x)

(cid:124)
[[∇z log pω(x, z) − ∇z log qφ(z|x)]

Jφgφ((cid:15), x)] .

(11)

We plug in AR-DAE to approximate the gradient of the log-density, and draw a Monte-Carlo sample of the following
quantity to estimate the gradient of the ELBO

ˆ∇φLELBO(q)

.
=

E
z∼qφ(z|x)
x∼pdata(x)

[[∇z log pω(x, z) − far,θ(z; x, σ)|σ=0]

(cid:124)

Jφgφ((cid:15), x)] ,

(12)

F.2. AR-DAE

To approximate ∇z log qφ(z|x), we condition AR-DAE on both the input x as well as the noise scale σ. We also adaptively
choose the prior variance δ2 for different data points instead of ﬁxing it to be a single value.

In addition, we make the following observations. (1) The posteriors qφ are usually not centered, but the entropy gradient
approximator only needs to model the dispersion of the distribution. (2) The variance of the approximate posterior can
be very small during training, which might pose a challenge for optimization. To remedy these, we modify the input of
.
= s(z − b(x)), where s is a scaling factor and b(x) is a pseudo mean. Ideally, we would like to set b(x) to
AR-DAE to be ˜z
.
be Eq(z|x)[z]. Instead, we let b(x)
= g(0, x), as 0 is the mode/mean of the noise source. The induced distribution of ˜z will

AR-DAE: Towards Unbiased Neural Entropy Gradient Estimation

be denoted by qφ(˜z|x). By the change-of-variable density formula, we have ∇z log q(z|x) = s∇˜z log q(˜z|x). This allows
us to train AR-DAE with a better-conditioned distribution and the original gradient can be recovered by rescaling.

In summary, we optimize the following objective

Lar (far) =

E
x∼p(x)
˜z∼q(˜z|x)
u∼N (0,I)
σ|x∼N (0,δ(x)2)

(cid:104)

(cid:107)u + σfar(˜z + σu; x, σ)(cid:107)2(cid:105)

.

(13)

.
= δscaleSz|x and Sz|x is sample standard deviation of z given x. We use nz samples per data to estimate Sz|x.

where δ(x)
δscale is chosen as hyperparameter.

In the experiments, we either directly parameterize the residual function of AR-DAE or indirectly parameterize it as the
gradient of some scalar-function. We parameterize far(˜z; x, σ) as a multi-layer perceptron (MLP). Latent z and input x
are encoded separately and then concatenated with σ (denoted by "mlp-concat" in Table 8). The MLP encoders have menc
hidden layers. The concatenated representation is fed into a fully-connected neural network with mfc hidden layers. Instead
of encoding the input x directly, we either use a hidden representation of the variational posterior q or b(x). We use dh
hidden units for all MLPs. We stress that the learning signal from Lar (far) is not backpropagated to the posterior.

Algorithm 1 VAE AR-DAE

Input: Dataset D; mini-batch size ndata; sample size nz;prior variance δ2; learning rates αθ and αφ,ω
Initialize encoder and decoder pω(x|z) and qφ(z|x)
Initialize AR-DAE far,θ(z|x)
repeat

Draw ndata datapoints from D
for k = 0 . . . Nd do

Draw nz latents per datapoint from z ∼ qφ(z|x)
δi ← δscaleSz|xi for i = 1, . . . , ndata
Draw nσ number of σis per z from σi ∼ N (0, δ2
i )
Draw ndatanznσ number of us from u ∼ N (0, I)
Update θ using gradient ∇θLfar with learning rate αθ

end for
z ∼ qφ(z|x)
Update ω using gradient ∇ωLELBO with learning rate αφ,ω
Update φ using gradient ˆ∇φLELBO with learning rate αφ,ω, whose entropy gradient is approximated using far,θ(z|x).

until Until some stopping criteria

F.3. Experiments

We summarize the architecture details and hyperparameters in Table 7 and 8, respectively.

Mixture of Gaussian experiment For the MoG experiment, we use 25 Gaussians centered on an evenly spaced 5 by 5
grid on [−4, 4] × [−4, 4] with a variance of 0.1. Each model is trained for 16 epochs: approximately 4000 updates with a
minibatch size of 512.

For all experiments, we use a two-hidden-layer MLP to parameterize the conditional diagonal Gaussian p(x|z). For the
implicit posterior q, the input x and the d(cid:15)-dimensional noise are separately encoded with one fully-connected layer, and
then the concatenation of their features will be fed into a two-hidden-layer MLP to generate the 2-dimensional latent z. The
size of the noise source (cid:15) in the implicit posterior, i.e. d(cid:15), is set to 10.

MNIST We ﬁrst describe the details of the network architectures and then continue to explain training settings. For the
MLP experiments, we use a one-hidden-layer MLP for the diagonal Gaussian decoder p(x|z). For the diagonal Gaussian
posterior q(z|x). aka vanilla VAE, input x is fed into a fully-connected layer and then the feature is later used to predict
the mean and diagonal component of the covariance matrix of the multivariate Gaussian distribution. For the hierarchical

AR-DAE: Towards Unbiased Neural Entropy Gradient Estimation

posterior, both q(z0|x) and q(z|z0, x) are one-hidden-layer MLPs with diagonal Gaussian similar to the vanilla VAE. For
the implicit posterior, the input is ﬁrst encoded and then concatenated with noise before being fed into another MLP to
generate z.

For Conv, the decoder starts with a one-fully connected layer followed by three deconvolutional layers. The encoder has
three convolutional layers and is modiﬁed depending on the types of the variational posteriors, similar to MLP. For ResConv,
ﬁve convolutional or deconvolutional layers with residual connection are used for the encoder and the decoder respectively.

Following Maaløe et al. (2016); Ranganath et al. (2016), when the auxiliary variational method (HVI aux) is used to train
the hierarchical posterior, the variational lower bound is deﬁned as, we maximize the following lower bound to train the
hierarchical variational posterior with auxiliary variable (HVI aux)

log p(x) ≥ E

z∼q(z|x)

[log p(x, z) − log q(z|x)] ≥

E
z0∼q(z0|x)
z∼q(z|z0,x)

[log p(x, z) − log q(z0|x) − log q(z|z0, x) + log h(z0|z, x)] .

For the dynamically binarized MNIST dataset, we adopt the experiment settings of Mescheder et al. (2017). The MNIST
data consists of 50k train, 10k validation, and 10k test images. In addition to the original training images, randomly selected
5k validation images are added to the training set. Early stopping is performed based on the evaluation on the remaining 5k
validation data points. The maximum number of iterations for the training is set to 4M.

For the statically binarized MNIST dataset, we use the original data split. Early stopping as well as hyperparameter
search are performed based on the estimated log marginal probability on the validation set. We retrain the model with the
selected hyperparameters with the same number of updates on the combination of the train+valid sets, and report the test set
likelihood. We also apply polyak averaging (Polyak & Juditsky, 1992).

We evaluate log p(x) of the learned models using importance sampling (Burda et al., 2016) (with neval samples). For the
baseline methods, we use the learned posteriors as proposal distributions to estimate the log probability. When a posterior
is trained with AR-DAE, we ﬁrst draw neval z’s from the posterior given the input x, and then use the sample mean and
covariance matrix to construct a multivariate Gaussian distribution. We then use this Gaussian distribution as the proposal.

G. Experiment: entropy-regularized reinforcement learning

G.1. Soft actor-critic

Notation We consider an inﬁnite-horizon Markov decision process (MDP) deﬁned as a tuple (S, A, R, penv, γ) (Sutton
et al., 1998), where S, A, R are the spaces of state, action and reward, respectively, penv(st+1|st, at) and penv(s0) represent
the transition probability and the initial state distribution, r(st, at) is a bounded reward function, and γ is a discount factor.
We write τ as a trajectory resulting from interacting with the environment under some policy π(at|st).

The entropy-regularized reinforcement learning (Ziebart, 2010) is to learn a policy π(at|st) that maximizes the following
objective;

L(π) = E

τ ∼π,penv

(cid:34) ∞
(cid:88)

(cid:35)
γt (r(st, at) + αH(π(·|st)))

,

t=0

(14)

where α is an entropy regularization coefﬁcient. We deﬁne a soft state value function V π and s soft Q-function Qπ as
follows,

V π(s) = E

τ ∼π,penv
(cid:34)

Qπ(s, a) = E

τ ∼π,penv

r(st, at) +

(cid:34) ∞
(cid:88)

t=0

(cid:12)
(cid:12)
γt (r(st, at) + αH(π(·|st)))
(cid:12)
(cid:12)
(cid:12)

(cid:35)

s0 = s

∞
(cid:88)

t=1

(cid:12)
(cid:12)
γt (r(st, at) + αH(π(·|st)))
(cid:12)
(cid:12)
(cid:12)

(cid:35)

s0 = s, a0 = a

.

By using these deﬁnitions, we can rewrite V π and Qπ as V π(s) = Ea∼π [Qπ(s, a)] + αH(π(·|s)) and Qπ(s) =
[r(s, a) + Es(cid:48)∼penv γV π(s(cid:48))].

AR-DAE: Towards Unbiased Neural Entropy Gradient Estimation

Soft actor-critic One way to maximize (14) is to minimize the following KL divergence,

πnew = arg min

π

DKL

π(·|st)

(cid:18)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

exp (Qπold(st, ·))
Z πold (st)

(cid:19)

,

where Z πold(st) is the normalizing constant (cid:82) exp (Qπold (st, a)) da. Haarnoja et al. (2018) show that for ﬁnite state space
the entropy-regularized expected return will be non-decreasing if the policy is updated by the above update rule. In practice,
however, we do not have access to the value functions, so Haarnoja et al. (2018) propose to update the policy by ﬁrst
approximating Qπold and V πold by some parametric functions Qω and Vν, and training the policy by minimizing

L(π) = E

st∼D

(cid:18)

(cid:20)
DKL

π(at|st)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

exp (Qω(st, ·))
Zω(st)

(cid:19)(cid:21)

,

where D is a replay buffer that stores all the past experience. The soft Q-function and soft state value function will be trained
by minimizing the following objectives,

L(Vν) = E
st∼D

(cid:18)

(cid:34)

1
2

Vν(st) − E
at∼π

[Qω(st, at) − α log π(at|st)]

(cid:19)2(cid:35)

L(Qω) = E

st,at∼D

(cid:16)

(cid:20) 1
2

Qω(st, at) − ˆQ(st, at)

(cid:17)2(cid:21)

,

.
where ˆQ(st, at)
= r(st, at) + γ Est+1∼penv[V¯ν(st+1)] and V¯ν is a target value network. For the target value network, SAC
follows Mnih et al. (2015): V¯ν is deﬁned as a polyak-averaged model (Polyak & Juditsky, 1992) of Vν. Note that Vν is
.
inferred from Qω via Monte Carlo, i.e. Vν(st)
= Qω(st, at) − α log π(at|st) where at ∼ π(at|st). Moreover, we follow
the common practice to use the clipped double Q-functions (Hasselt, 2010; Fujimoto et al., 2018) in our implementations.

G.2. SAC-AR-DAE and its implementations

Main algorithm Our goal is to train an arbitrarily parameterized policy within the SAC framework. We apply AR-DAE
to approximate the training signal for policy. Similar to the implicit posterior distributions in the VAE experiments, the
policy consists of a simple tractable noise distribution π((cid:15)) and a mapping gφ : (cid:15), s (cid:55)→ a. The gradient of L(π) wrt the
policy parameters can be written as

∇φL(π) = E
st∼D
(cid:15)∼π

(cid:2)(cid:2)∇a log πφ(a|st)|a=gφ((cid:15),st) − ∇aQω(st, a)|a=gφ((cid:15),st)

(cid:3)(cid:124)

Jφgφ((cid:15), st)(cid:3) .

Let far,θ be AR-DAE which approximates ∇a log πφ(a|s) trained using Equation (13). Speciﬁcally for the SAC experiment,
AR-DAE is indirectly parameterized as the gradient of an unnormalized log-density function ψar,θ : a, s, σ (cid:55)→ R as in,
.
= ∇aψar,θ(a; s, σ).

far,θ(a; s, σ)

As a result, log π(a|s) can also be approximated by using ψar,θ: log π(a|s) ≈ ψar,θ(a; s, σ)|σ=0 − log Zθ(s), where
Zθ(s) = (cid:82) exp (ψar,θ(a; s, σ)|σ=0) da.
Using AR-DAE, we can modify the objective function L(Vν) to be

ˆL(Vν) = E
st∼D

(cid:34)

1
2

(cid:18)

Vν(st) − E
at∼π

[Qω(st, at) − ψar,θ(at; st, σ)|σ=0] − log Zθ(st)

.

(cid:19)2(cid:35)

The same applies to L(Qω). We also use the polyak-averaged target value network and one-sample Monte-Carlo estimate as
done in SAC. Finally, the gradient signal for the policy can be approximated using AR-DAE:

ˆ∇φL(π)

.
= E
st∼D
(cid:15)∼π

(cid:2)(cid:2)far,θ(gφ((cid:15), st); st, σ)|σ=0 − ∇aQω(st, a)|a=gφ((cid:15),st)

(cid:3)(cid:124)

Jφgφ((cid:15), st)(cid:3) .

We summarize all the details in Algorithm 2.

AR-DAE: Towards Unbiased Neural Entropy Gradient Estimation

Algorithm 2 SAC-AR-DAE

Input: Mini-batch size ndata; replay buffer D; number of epoch T ; learning rates αθ, αφ, αω, αν
Initialize value function Vν(s), critic Qω(s, a), policy πφ(a|s), and AR-DAE far,θ(a|s)
Initialize replay buffer D ← ∅
for epoch = 1, ..., T do

Initialize a state from s0 ∼ penv(s0)
for t = 0 . . . do
a ∼ πφ(.|st)
(rt, st+1) ∼ penv(·|st, at)
D ← D ∪ {(st, at, rt, st+1)}
for each learning step do

Draw ndata number of (st, at, rt, st+1)s from D
for k = 0 . . . Nd do

Draw na actions per state from a ∼ πφ(a|s)
δi ← δscaleSa|si for i = 1, . . . , ndata
Draw nσ number of σis per a from σi ∼ N (0, δ2
i )
Draw ndatananσ number of us from u ∼ N (0, I)
Update θ using gradient ∇θLfar with learning rate αθ

end for
Update ν using gradient ∇ν ˆLV with learning rate αν
Update ω using gradient ∇ω ˆLQ with learning rate αω
Update φ using gradient ˆ∇φLπ which is approximated with far,θ(a|s)
¯ν ← τ ν + (1 − τ )¯ν

end for

end for

end for

Bounded action space The action space of all of our environments is an open cube (−1, 1)da , where da is the dimen-
sionality of the action. To implement the policy, we apply the hyperbolic tangent function. That is, a := tanh(gφ((cid:15), st)),
where the output of gφ (denoted as ˜a) is in (−∞, ∞). Let ˜ai be the i-th element of ˜a. By the change of variable formula,
log π(a|s) = log π(˜a|s) − (cid:80)da

i=1 log(1 − tanh2(˜ai)).

In our experiments, we train AR-DAE on the pre-tanh action ˜a. This implies that AR-DAE approximate ∇˜a log π(˜a|s). We
correct the change of volume induced by the tanh using

∇˜a log π(a|s) = ∇˜a log π(˜a|s) + 2 tanh(˜a).

To sum up, the update of the policy follows the approximated gradient

ˆ∇φL(π)

.
= E
st∼D
(cid:15)∼π

(cid:2)(cid:2)far,θ(gφ((cid:15), st); st, σ)|σ=0 + 2 tanh(gφ((cid:15), st)) − ∇˜aQω(st, tanh(˜a))|˜a=gφ((cid:15),st)

(cid:3)(cid:124)

Jφgφ((cid:15), st)(cid:3) .

In order to train SAC-AR-DAE in practice, efﬁcient computation of log Zθ(s) is
Estimating normalizing constant
required. We propose to estimate the normalizing constant (Geyer, 1991) using importance sampling. Let h(a|s) be the
proposal distribution. We compute the following (using the log-sum-exp trick to ensure numerical stability)

log Zθ(s) = log

(cid:90)

exp (ψar,θ(a; s, σ)|σ=0) da

= log E
a∼h

[exp (ψar,θ(a; s, σ)|σ=0 − log h(a|s))]

≈ log

1
NZ

NZ(cid:88)

j

[exp (ψar,θ(aj; s, σ)|σ=0 − log h(aj|s) − A)] + A,

where aj is the j-th action sample from h and A := maxaj exp (ψar,θ(aj; s, σ)|σ=0 − log h(aj|s)). For the proposal
.
distribution, we use h(a|s)
= ψar,θ(gφ((cid:15), s); s, σ)|(cid:15)=0,σ=0 and c is some constant. We set c to
be log c = −1.

.
= N (µ(s), cI), where µ(s)

AR-DAE: Towards Unbiased Neural Entropy Gradient Estimation

Figure S4. Additional results on SAC-AR-DAE, ablating Jacobian clamping regularization on implicit policy distributions in comparison
with the rest.

Target value calibration In order to train the Q-function more efﬁciently, we calibrate its target values. Training the
policy only requires estimating the gradient of the Q-function wrt the action, not the value of the Q-function itself. This
means that while optimizing Qω (and Vν), we can subtract some constant from the true target to center it. In our experiment,
this calibration is applied when we use one-sample Monte-Carlo estimate and the polyak-averaged Q-network Q¯ω. That is,
L(Qω) can be rewritten as,

L(Qω) =

E
st,at,st+1∼D
at+1∼π

(cid:20) 1
2

(Qω(st, at) + B − r(st, at) − γ (Q¯ω(st+1, at+1) − α log π(at+1|st+1)))2

(cid:21)

.

where B is a running average of the expected value of γα log π(a|s) throughout training.

Jacobian clamping
In addition, we found that the implicit policies can potentially collapse to point masses. To mitigate
this, we regularize the implicit distributions by controlling the Jacobian matrix of the policy wrt the noise source as in Odena
et al. (2018); Kumar et al. (2020), aka Jacobian clamping. The goal is to ensure all singular values of Jacobian matrix of
pushforward mapping to be higher than some constant. In our experiments, we follow the implementation of Kumar et al.
(2020): (1) stochastic estimation of the singular values of Jacobian matrix at every noise, and the Jacobian is estimated by
ﬁnite difference approximation, and (2) use of the penalty method (Bertsekas, 2016) to enforce the constraint. The resulting
regularization term is

Lreg(π) =

E
st∼D
(cid:15)∼π
v∼N (0,I)

(cid:34)

min

(cid:18) (cid:107)gφ((cid:15) + ξv, st) − gφ((cid:15), st)(cid:107)2
ξ2(cid:107)v(cid:107)2

2

(cid:19)2(cid:35)

− η, 0

,

where η, ξ > 0, and nperturb number of the perturbation vector v is sampled. We then update policy π with ˆ∇φL(π) +
λ∇φLreg(π) where λ is increased throughout training. We set λ = 1 + iν/1000 at i-th iteration and ν ∈ [1.1, 1.3].

G.3. Experiments

For the SAC-AR-DAE experiments, aside from the common practice for SAC, we follow the experiment settings from
Mazoure et al. (2019) and sample from a uniform policy for a ﬁxed number of initial interactions (denoted as warm-up). We
also adopt the same network architecture for the Q-network, discounting factor γ, entropy regularization coefﬁcient α, and
target smoothing coefﬁcient τ . For AR-DAE, we use the same network architecture as VAE. We also rescale the unbounded
action ˜a by s for better conditioning. The details of hyperparameters are described in Table 9.

AR-DAE: Towards Unbiased Neural Entropy Gradient Estimation

We run ﬁve experiments for each environment without ﬁxing the random seed. For every 10k steps of environment interaction,
the average return of the policy is evaluated with 10 independent runs. For visual clarify, the learning curves are smoothed
by second-order polynomial ﬁlter with a window size of 7 (Savitzky & Golay, 1964). For each method, we evaluate the
maximum average return: we take the maximum of the average return for each experiment and the average of the maximums
over the ﬁve random seeds. We also report ‘normalized average return’, approximately area under the learning curves:
we obtain the numerical mean of the ‘average returns’ over iterates. We run SAC and SAC-NF with the hyperparameters
reported in Mazoure et al. (2019).

G.4. Additional Experiments

In addition to the main results in Figure 7 and Table 3, we also compare the effect of Jacobian clamping regularization on
implicit policy distribution in SAC-AR-DAE. In each environment, the same hyperparameters are used in SAC-AR-DAEs
except for the regularization. Our results are presented in Figure S4 and Table 5, 6.

The results shows that Jacobian clamping regularization improves the performance of SAC-AR-DAE in general, especially
for Humanoid-rllab. In Humanoid-rllab, we observe that implicit policy degenerates to point masses without the Jacobian
clamping, potentially due to the error of AR-DAE. However, the Jacobian clamping helps to avoid the degenerate distributions,
and the policy facilitates AR-DAE-based entropy gradients.

SAC

SAC-NF

SAC-AR-DAE

SAC-AR-DAE (w/o jc)

HalfCheetah-v2
Ant-v2
Hopper-v2
Walker-v2
Humanoid-v2
Humanoid (rllab)

9695 ± 879
5345 ± 553
3563 ± 119
4612 ± 249
5965 ± 179
6099 ± 8071

9325 ± 775
4861 ± 1091
3521 ± 129
4760 ± 624
5467 ± 44
3442 ± 3736

10907 ± 664
6190 ± 128
3556 ± 127
4793 ± 395
6275 ± 202
10739 ± 10335

10677 ± 374
6097 ± 140
3634 ± 45
4843 ± 521
6268 ± 77
761 ± 413

Table 5. Maximum average return. ± corresponds to one standard deviation over ﬁve random seeds.

SAC

SAC-NF

SAC-AR-DAE

SAC-AR-DAE (w/o jc)

HalfCheetah-v2
Ant-v2
Hopper-v2
Walker-v2
Humanoid-v2
Humanoid (rllab)

8089 ± 567
3280 ± 553
2442 ± 426
3023 ± 271
3471 ± 505
664 ± 321

7529 ± 596
3440 ± 656
2480 ± 587
3317 ± 455
3447 ± 260
814 ± 630

8493 ± 602
4335 ± 241
2631 ± 160
3036 ± 271
4215 ± 170
2021 ± 1710

8636 ± 307
4015 ± 363
2734 ± 194
3094 ± 209
3808 ± 137
332 ± 136

Table 6. Normalized average return. ± corresponds to one standard deviation over ﬁve random seeds.

H. Improved techniques for training AR-DAE and implicit models

In order to improve and stabilize the training of both the generator and AR-DAE, we explore multiple heuristics.

H.1. AR-DAE

Activity function During preliminary experiments, we observe that smooth activation functions are crucial in parameter-
izing AR-DAE as well as the residual form of regular DAE. We notice that ReLU gives less reliable log probability gradient
for low density regions.

Number of samples and updates
In the VAE and RL experiments, it is important to keep AR-DAE up-to-date with the
generator (i.e. posterior and policy). As discussed in Appendix E, we found that increasing the number of AR-DAE updates
helps a lot. Additionally, we notice that increasing nz is more helpful than increasing ndata given ndatanz is ﬁxed.

Scaling-up and zero-centering data To avoid using small learning rate for AR-DAE in the face of sharp distributions
with small variance, we choose to scale up the input of AR-DAE. As discussed in Appendix F.2, we also zero-center the

AR-DAE: Towards Unbiased Neural Entropy Gradient Estimation

latent samples (or action samples) to train AR-DAE. This allows AR-DAE to focus more on modeling the dispersion of the
distribution rather than where most of the probability mass resides.

H.2. Implicit distributions

Noise source dimensionality We note that the implicit density models can potentially be degenerate and do not admit a
density function. For example, in Appendix E we show that increasing the dimensionality of the noise source improves the
qualities of the implicit distributions.

Jacobian clamping Besides of increasing noise source dimensionality, we can consider Jacobian clamping distributions
to prevent implicit posteriors from collapsing to point masses. As pointed out in Appendix G.2, we observe that using this
regularization technique can prevent degenerate distributions in practice, as it at least regularizes the mapping locally if its
Jacobian is close to singular.

AR-DAE: Towards Unbiased Neural Entropy Gradient Estimation

MLP
toy

MLP
dbmnist

Conv
dbmnist

ResConv
dbmnist
(or sbmnist)

p(x|z)

[2, 256]
[256, 256] × 2
[256, dx] × 2

[dz, 300]
[300, dx]

[dz, 300]
[300, 512]
[32, 32, 5 × 5, 2, 2, deconv]
[32, 16, 5 × 5, 2, 2, deconv]
[16, 1, 5 × 5, 2, 2, deconv]

[dz, 450]
[450, 512]
[upscale by 2]
[32, 32, 3 × 3, 1, 1, res]
[32, 32, 3 × 3, 1, 1, res]
[upscale by 2]
[32, 16, 3 × 3, 1, 1, res]
[16, 16, 3 × 3, 1, 1, res]
[upscale by 2]
[16, 1, 3 × 3, 1, 1, res]

Common

Gaussian

q(z|x)

[dx, 256]

-

[dx, 300]

[300, dz] × 2

HVI

-

[300, dz] × 2

(or (cid:2)300, dz0

(cid:3) × 2)

[1, 16, 5 × 5, 2, 2]
[16, 32, 5 × 5, 2, 2]
[32, 32, 5 × 5, 2, 2]

[512, 800]
[800, dz] × 2

[512, 800]
[800, dz] × 2
(or (cid:2)800, d(z0)(cid:3) × 2)

[1, 16, 3 × 3, 2, 1, res]
[16, 16, 3 × 3, 1, 1, res]
[16, 32, 3 × 3, 2, 1, res]
[32, 32, 3 × 3, 1, 1, res]
[32, 32, 3 × 3, 2, 1, res]
[512, 450, res]

[450, dz] × 2

[450, 450]
[450, dz] × 2

(or (cid:2)450, dz0

(cid:3) × 2)

implicit

[256 + d(cid:15), 256]
[256, 256]
[256, dz]

[300 + d(cid:15), 300]
[300, dz]

[512 + d(cid:15), 800]
[800, dz]

[450 + d(cid:15), 450, res]
[450, dz, res]

Table 7. Network architectures for the VAE experiments. Fully-connected layers are characterized by [input size, output size], and
convolutional layers by [input channel size, output channel size, kernel size, stride, padding]. “res” indicates skip connection, aka residual
layer (He et al., 2016). Deconvolutional layer is marked as “deconv”.

model

learning

model

learning

parameterization
network
mfc
menc
activation
dh
s

nz
ndata
nσ
Nd
δscale
optimizer
learning rate αθ

network
dz
dz0 or d(cid:15)
ndata
optimizer
learning rate αφ,ω
β-annealing
e-train with train+val

polyak (decay)
polyak (start interation)
neval

toy

gradient
mlp-concat
3
3
softplus
256
10000

256
512
1
1
0.1
rmsprop, 0.5
0.0001

mlp
2
10

MLP

dbmnist

gradient
mlp-concat
5
5
softplus
256
10000

625
128
1
{1,2}
{0.1, 0.2, 0.3}
rmsprop, 0.5
0.0001

mlp
32
100

Conv
dbmnist

gradient
mlp-concat
5
5
softplus
256
10000

256
128
1
{1,2}
{0.1, 0.2, 0.3}
rmsprop, 0.9
0.0001

conv
32
100

dbmnist

residual
mlp-concat
5
5
softplus
512
100

625
128
1
2
{0.1, 0.2, 0.3}
rmsprop, 0.9
0.0001

rescov
32
100

512
adam, 0.5, 0.999
0.0001
no
no

128
adam, 0.5, 0.999
0.0001
no
no

128
adam, 0.5, 0.999
0.0001
no
no

128
adam ,0.9, 0.999
{0.001, 0.0001}
{no, 50000}
no

ResConv

sbmnist

residual
mlp-concat
5
5
softplus
512
100

625
128
1
2
{0.1, 0.2, 0.3}
rmsprop, 0.9
0.0001

rescov
32
100

128
adam 0.9, 0.999
{0.001, 0.0001}
{no, 50000}
yes

-
-
-

no
no
40000

no
no
40000

no
no
20000

0.998
{0, 1000, 5000, 10000}
20000

AR-DAE

Encoder/decoder

Evaluation

Table 8. Hyperparameters for the VAE experiments. toy is the 25 Gaussian dataset. dbmnist and sbmnist are dynamically and statically
binarized MNIST, respectively.

AR-DAE: Towards Unbiased Neural Entropy Gradient Estimation

model

learning

model

learning

model

learning

AR-DAE

policy

Q-network

general

parameterization
network
mfc
menc
activation
dh
s

na,dae
ndata
nσ
Nd
δscale
optimizer
learning rate αθ

network
mfc
menc
activation
dh
d(cid:15)

HalfCheetah-v2

gradient
mlp
5
5
elu
256
10000

Ant-v2

gradient
mlp
5
5
elu
256
10000

Hopper-v2

Walker-v2

Humanoid-v2

Humanoid (rllab)

gradient
mlp
5
0
elu
256
10000

gradient
mlp
5
1
elu
256
10000

gradient
mlp
5
1
elu
256
10000

gradient
mlp
5
1
elu
256
10000

128
256
1
1
0.1
adam, 0.9, 0.999
0.0003

64
256
1
1
0.1
adam, 0.9, 0.999
0.0003

128
256
1
1
0.1
adam, 0.9, 0.999
0.0003

128
256
1
1
0.1
adam, 0.9, 0.999
0.0003

64
256
4
1
0.1
adam, 0.9, 0.999
0.0003

64
256
4
1
0.1
adam, 0.9, 0.999
0.0003

mlp
1
1
elu
256
10

mlp
1
1
elu
256
10

mlp
1
2
elu
256
10

mlp
2
1
elu
256
10

mlp
2
3
elu
64
32

mlp
2
3
elu
64
100

nperturb
optimizer
ξ, η, ν
learning rate αφ

10
adam, 0.9, 0.999
0.01, 0.1, 1.1
0.0003

10
adam, 0.9, 0.999
0.01, 0.01, 1.1
0.0003

10
adam, 0.9, 0.999
0.01, 0.01, 1.1
0.0003

10
adam, 0.9, 0.999
0.01, 0.01, 1.1
0.0003

10
adam, 0.9, 0.999
0.01, 0.1, 1.3
0.0003

10
adam, 0.9, 0.999
0.01, 0.1, 1.3
0.0003

network
mfc
activation
dh

optimizer
learning rate αω

α
τ
γ
nZ
target calibration
warm-up

mlp
2
relu
256

mlp
2
relu
256

mlp
2
relu
256

mlp
2
relu
256

mlp
2
relu
256

mlp
2
relu
256

adam, 0.9, 0.999
0.0003

adam, 0.9, 0.999
0.0003

adam, 0.9, 0.999
0.0003

adam, 0.9, 0.999
0.0003

adam, 0.9, 0.999
0.0003

adam, 0.9, 0.999
0.0003

0.05
0.005
0.99
100
no
5000

0.05
0.005
0.99
10
no
10000

0.05
0.005
0.99
100
yes
10000

0.05
0.005
0.99
100
no
10000

0.05
0.005
0.99
10
no
10000

0.05
0.005
0.99
10
no
10000

Table 9. Hyperparameters for RL experiments.

