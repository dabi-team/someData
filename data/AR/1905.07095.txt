Draft version May 20, 2019
Typeset using LATEX default style in AASTeX62

9
1
0
2

y
a
M
7
1

]

R
S
.
h
p
-
o
r
t
s
a
[

1
v
5
9
0
7
0
.
5
0
9
1
:
v
i
X
r
a

Predicting Solar Flares Using a Long Short-Term Memory Network

Hao Liu,1, 2 Chang Liu,1, 3, 4 Jason T. L. Wang,1, 2 and Haimin Wang1, 3, 4

1Institute for Space Weather Sciences, New Jersey Institute of Technology, University Heights, Newark, NJ 07102-1982, USA
hl422@njit.edu, chang.liu@njit.edu, wangj@njit.edu, haimin.wang@njit.edu
2Department of Computer Science, New Jersey Institute of Technology, University Heights, Newark, NJ 07102-1982, USA
3Big Bear Solar Observatory, New Jersey Institute of Technology, 40386 North Shore Lane, Big Bear City, CA 92314-9672, USA
4Center for Solar-Terrestrial Research, New Jersey Institute of Technology, University Heights, Newark, NJ 07102-1982, USA

ABSTRACT

We present a long short-term memory (LSTM) network for predicting whether an active region (AR)
would produce a Υ-class ﬂare within the next 24 hours. We consider three Υ classes, namely ≥M5.0
class, ≥M class, and ≥C class, and build three LSTM models separately, each corresponding to a Υ
class. Each LSTM model is used to make predictions of its corresponding Υ-class ﬂares. The essence
of our approach is to model data samples in an AR as time series and use LSTMs to capture temporal
information of the data samples. Each data sample has 40 features including 25 magnetic parameters
obtained from the Space-weather HMI Active Region Patches (SHARP) and related data products
as well as 15 ﬂare history parameters. We survey the ﬂare events that occurred from 2010 May to
2018 May, using the GOES X-ray ﬂare catalogs provided by the National Centers for Environmental
Information (NCEI), and select ﬂares with identiﬁed ARs in the NCEI ﬂare catalogs. These ﬂare events
are used to build the labels (positive vs. negative) of the data samples. Experimental results show that
(i) using only 14-22 most important features including both ﬂare history and magnetic parameters can
achieve better performance than using all the 40 features together; (ii) our LSTM network outperforms
related machine learning methods in predicting the labels of the data samples. To our knowledge, this
is the ﬁrst time that LSTMs have been used for solar ﬂare prediction.

Keywords: magnetic ﬁelds − methods: deep learning − Sun: activity − Sun: ﬂares

1. INTRODUCTION

Solar ﬂares, the largest explosive events in our solar system, are intense bursts of radiation that occur in the Sun’s
atmosphere and release massive amounts of energy into space. They last from minutes to hours and are often seen
as bright chromospheric ribbons and hot coronal loops on the Sun. Some ﬂares are small and innocent while others
can be tremendous and violent. Powerful ﬂares and the often accompanied coronal mass ejections (CMEs) can cause
severe inﬂuences on the near-Earth environment, resulting in potentially life-threatening consequences (Daglis et al.
2004). Therefore, substantial eﬀorts are being invested on solar ﬂare research including forecast and mitigation plans.
The triggering mechanism of solar ﬂares is far from being fully understood. Many studies have shown that ﬂares
and CMEs could be powered by the free magnetic energy accumulated in the coronal ﬁeld, which can be impulsively
released by magnetic reconnection (Priest & Forbes 2002; Shibata & Magara 2011). Since the buildup process of
coronal free energy is driven by long-term evolution of the magnetic ﬁeld on the photosphere (Takasao et al. 2015),
the features of the photospheric magnetic ﬁeld, which can be directly observed and derived from photospheric vector
magnetograms, may be crucial indicators for the energy transportation and triggering processes of ﬂares/CMEs. These
features include the size and complexity of sunspots, unsigned magnetic ﬂux, gradient of the magnetic ﬁeld, magnetic
energy dissipation, vertical electric currents, integrated Lorentz forces, magnetic shear, magnetic helicity injection,
and so on (Park et al. 2008; Song et al. 2009; Yu et al. 2009; Steward et al. 2010). With the recent development of
instruments and techniques, it becomes easier to obtain extensive measurements of these features.

Many researchers have demonstrated that using photospheric vector magnetograms in combination with machine
learning can predict solar ﬂares eﬀectively. Bobra & Couvidat (2015) described 25 features, or predictive parameters,
derived from vector magnetograms provided by the Helioseismic and Magnetic Imager (HMI; Schou et al. 2012) on
board the Solar Dynamics Observatory (SDO; Pesnell et al. 2012). The authors considered ﬂares of M1.0 class or

 
 
 
 
 
 
2

Liu et al.

higher, as deﬁned by the peak 1–8 ˚A ﬂux measured by the Geostationary Operational Environmental Satellite system
(GOES ). Liu et al. (2017) took 13 parameters out of the 25 features and used them to perform multiclass predictions of
solar ﬂares. Nishizuka et al. (2017) employed both photospheric vector-magnetic ﬁeld data and chromospheric data to
predict prominent ﬂares. The authors observed that pre-ﬂare events such as ultraviolet brightening are associated with
trigger mechanisms of solar ﬂares. They counted the number of previous ﬂares in an active region (AR) and showed that
both the previous ﬂare activity information and ultraviolet brightening are crucial for ﬂare prediction. The authors
later extended their study to include more features such as the X-ray intensity to further improve ﬂare prediction
performance (Nishizuka et al. 2018). Jonas et al. (2018) carried out solar ﬂare prediction by utilizing photospheric
vector-magnetic ﬁeld data, ﬂaring history, as well as multiple wavelengths of image data from the chromosphere,
transition region, and corona. In contrast to the ﬂare history used by Nishizuka et al. (2017, 2018), Jonas et al. (2018)
constructed ﬂare time series for each AR by taking the list of associated ﬂares in the GOES solar-ﬂare catalogs. The
constructed time series are then convolved with exponentially decaying windows of varying length for ﬂare prediction.
Machine learning is a subﬁeld of artiﬁcial intelligence, which grants computers abilities to learn from the past data
and make predictions on unseen future data (Alpaydin 2009). Commonly used machine learning methods for ﬂare
prediction include decision trees (Yu et al. 2009, 2010), random forests (Barnes et al. 2016; Liu et al. 2017; Florios
et al. 2018; Breiman 2001), k-nearest neighbors (Li et al. 2008; Huang et al. 2013; Winter & Balasubramaniam 2015;
Nishizuka et al. 2017), support vector machines (Qahwaji & Colak 2007; Yuan et al. 2010; Bobra & Couvidat 2015;
Boucheron et al. 2015; Muranushi et al. 2015; Florios et al. 2018), ordinal logistic regression (Song et al. 2009), the least
absolute shrinkage and selection operator (LASSO) (Benvenuto et al. 2018; Jonas et al. 2018), extremely randomized
trees (Nishizuka et al. 2017), and neural networks (Qahwaji & Colak 2007; Wang et al. 2008; Colak & Qahwaji 2009;
Higgins et al. 2011; Ahmed et al. 2013). Recently, Nishizuka et al. (2018) adopted a deep neural network, named Deep
Flare Net, for ﬂare prediction.

In this paper, we attempt to use SDO/HMI vector magnetic ﬁeld data together with ﬂaring history to predict solar
ﬂares that would occur in an AR within 24 hours of a given time point, with a deep learning method, named long
short-term memory (LSTM) (Hochreiter & Schmidhuber 1997). An LSTM network is a special kind of recurrent
neural networks (RNNs) (Hopﬁeld 1982) that can learn the order dependence between samples in a sequence. LSTMs
have been widely used in a variety of applications such as speech recognition (Graves & Schmidhuber 2005; Fern´andez
et al. 2007; Graves et al. 2013), handwriting recognition (Graves et al. 2007; Graves & Schmidhuber 2008), time series
forecasting (Schmidhuber et al. 2005) among others. In a solar ﬂare prediction task, the observations in each AR form
time series data, and hence LSTMs are suitable for this prediction task. To our knowledge, this is the ﬁrst time that
LSTMs are used for solar ﬂare prediction.

The rest of this paper is organized as follows. Section 2 describes our data collection scheme and predictive param-
eters used in the study presented here. Section 3 details our LSTM architecture and algorithm. Section 4 reports
experimental results. Section 5 concludes the paper.

2. DATA AND PREDICTIVE PARAMETERS

In this study we adopt the data product, named Space-weather HMI Active Region Patches (SHARP; Bobra et al.
2014), produced by the SDO/HMI team. These data were released at the end of 2012 (Bobra et al. 2014) and can
be found as the hmi.sharp data series at the Joint Science Operations Center (JSOC).1 The SHARP data encompass
automatically identiﬁed and tracked ARs in map patches and provide many physical parameters suitable for ﬂare
prediction. Another useful data series, produced based on SHARP data, is cgem.Lorentz. This data series includes
estimations of integrated Lorentz forces (Sun et al. 2014) which help diagnose the dynamic process of each AR (Fisher
et al. 2012).

Our deep learning method requires training samples. We surveyed ﬂares that occurred in the period between
2010 May and 2018 May, using the GOES X-ray ﬂare catalogs provided by the National Centers for Environmental
Information (NCEI), and selected ﬂares with identiﬁed ARs in the NCEI ﬂare catalogs. This yielded a database of
4,203 B-class ﬂares, 6,768 C-class ﬂares, 704 M-class ﬂares, and 49 X-class ﬂares. We used both the hmi.sharp data
series and cgem.Lorentz data series, which were queried from the JSOC website by using SunPy (SunPy Community
et al. 2015). The data samples were collected at a cadence of 1 hour.

1 http://jsoc.stanford.edu/

Predicting Solar Flares Using a Long Short-Term Memory Network

3

We adopt two groups of predictive parameters for ﬂare prediction. The ﬁrst group contains the 25 physical parameters
described in Bobra & Couvidat (2015) that characterize AR magnetic ﬁeld properties. The second group contains 15
features related to ﬂaring history. Six of the 15 features are related to time decay values and are calculated based on
the formula described in Jonas et al. (2018). Speciﬁcally, for the data sample xt observed at time point t in an AR,
the time decay value of xt with respect to B-class (C-class, M-class, X-class, respectively) ﬂares, denoted Bdec(xt)
(Cdec(xt), Mdec(xt), Xdec(xt), respectively), is computed respectively as

Bdec(xt) =

(cid:88)

e− t−t(fi)

τ

,

Cdec(xt) =

Mdec(xt) =

Xdec(xt) =

fi∈FB
(cid:88)

fi∈FC
(cid:88)

fi∈FM
(cid:88)

fi∈FX

e− t−t(fi)

τ

,

e− t−t(fi)

τ

,

e− t−t(fi)

τ

,

(1)

(2)

(3)

(4)

where FB (FC, FM , FX , respectively) represents the set of B-class (C-class, M-class, X-class, respectively) ﬂares that
occurred in the same AR before the data observation time point t, t(fi) denotes the occurrence time of ﬂare fi, and τ
is a decay constant that is set to 12 as suggested by Jonas et al. (2018). Figure 1 illustrates how to calculate Mdec(xt)
for a data sample xt.

Figure 1. Calculation of the time decay value Mdec(xt) in an AR. The white rectangular box represents the data sample xt
observed and collected at time point t in the AR. There are p M-class ﬂares that occurred in the same AR prior to time point
t, so FM contains p ﬂares fi, 1 ≤ i ≤ p. Red vertical lines represent the occurrence times of the ﬂares in FM where the ith red
vertical line indicates the starting time t(fi) of the ith M-class ﬂare fi.

The other two time decay values for the data sample xt observed at time point t in an AR are computed by

considering all ﬂares, regardless of their classes, that occurred in the same AR before the time point t as follows:

Edec(xt) =

(cid:88)

fi∈F

E(fi) · e− t−t(fi)

τ

,

logEdec(xt) =

(cid:88)

fi∈F

log(E(fi)) · e− t−t(fi)

τ

,

(5)

(6)

where F = FB

(cid:83) FC

(cid:83) FM

(cid:83) FX and E(fi) is the magnitude of ﬂare fi.

In addition, the second group contains 9 ﬂare history features for the data sample xt in the AR as described in
Nishizuka et al. (2017). These nine features include Bhis (Chis, Mhis, Xhis, respectively) representing the total number
of B-class (C-class, M-class, X-class, respectively) ﬂares that occurred in the same AR before the data observation
time point t, Bhis1d (Chis1d, Mhis1d, Xhis1d, respectively) representing the total number of B-class (C-class, M-class,
X-class, respectively) ﬂares that occurred in the same AR during the 24 hours (i.e., 1 day) prior to the time point t,
and Xmax1d representing the maximum X-ray intensity in the same AR during the 24 hours prior to the time point
t. In total, we use 40 features, including 25 physical features and 15 ﬂare history features, which are summarized in
Table 1.

4

Liu et al.

Table 1. Overview of the 40 Features Including 25 SDO/HMI Magnetic Parameters and 15 Flare History Features

Description
Total unsigned current helicity
Total magnitude of Lorentz force
Total photospheric magnetic free energy density
Total unsigned vertical current
Absolute value of the net current helicity
Sum of the modulus of the net current per polarity
Total unsigned ﬂux

Keyword
TOTUSJH
TOTBSQ
TOTPOT
TOTUSJZ
ABSNJZH
SAVNCPP
USFLUX
AREA ACR Area of strong ﬁeld pixels in the active region
MEANPOT Mean photospheric magnetic free energy
R VALUE
SHRGT45
MEANSHR Mean shear angle

Sum of ﬂux near polarity inversion line
Fraction of area with shear > 45◦

MEANGAM Mean angle of ﬁeld from radial

MEANGBT Mean gradient of total ﬁeld

MEANGBZ Mean gradient of vertical ﬁeld

MEANGBH Mean gradient of horizontal ﬁeld

MEANJZH Mean current helicity
MEANJZD Mean vertical current density

MEANALP Mean characteristic twist parameter, α

TOTFX
TOTFY
TOTFZ
EPSX

EPSY

EPSZ

Bdec

Cdec

Mdec

Xdec

Edec

logEdec

Bhis
Chis
Mhis
Xhis
Bhis1d
Chis1d
Mhis1d
Xhis1d
Xmax1d

Sum of x -component of Lorentz force
Sum of y-component of Lorentz force
Sum of z -component of Lorentz force
Sum of x -component of normalized Lorentz force

Sum of y-component of normalized Lorentz force

Sum of z -component of normalized Lorentz force

Time decay value based on the previous B-class ﬂares only

Time decay value based on the previous C-class ﬂares only

Time decay value based on the previous M-class ﬂares only

Time decay value based on the previous X-class ﬂares only

Time decay value based on the magnitudes of all previous ﬂares

Time decay value based on the log-magnitudes of all previous ﬂares

Total history of B-class ﬂares in an AR
Total history of C-class ﬂares in an AR
Total history of M-class ﬂares in an AR
Total history of X-class ﬂares in an AR
1-day history of B-class ﬂares in an AR
1-day history of C-class ﬂares in an AR
1-day history of M-class ﬂares in an AR
1-day history of X-class ﬂares in an AR
Maximum X-ray intensity one day before

z JzdA|

z JzdA| + | (cid:80)B−

Formula
Hctotal ∝ (cid:80) |Bz · Jz|
F ∝ (cid:80) B2
ρtot ∝ (cid:80)(BBBObs − BBBPot)2dA
Jztotal = (cid:80) |Jz|dA
Hcabs ∝ | (cid:80) Bz · Jz|
Jzsum ∝ | (cid:80)B+
Φ = (cid:80) |Bz|dA
Area = (cid:80) Pixels
(cid:80)(BBBObs − BBBPot)2
ρ ∝ 1
N
Φ = (cid:80) |BLoS|dA within R mask
Area with shear > 45◦/ total area
(cid:80) arccos( BBBObs·BBBPot
Γ = 1
N
(cid:80) arctan( Bh
γ = 1
)
Bz
N
(cid:80) (cid:113)
∂x )2 + ( ∂B
( ∂B
|∇Btot| = 1
N
(cid:80) (cid:113)
∂x )2 + ( ∂Bz
(cid:80) (cid:113)
∂x )2 + ( ∂Bh

∂y )2
∂y )2
∂y )2

|BBBObs||BBBPot| )

( ∂Bh

( ∂Bz

∂x − ∂Bx
∂y )

y − B2

z )dA

y −B2
z )

(cid:80) B2

(cid:80) BzJz
(cid:80)( ∂By
(cid:80) Jz Bz
(cid:80) B2
z

|∇Bz| = 1
N
|∇Bh| = 1
N
Hc ∝ 1
N
Jz ∝ 1
N
αtotal ∝
Fx ∝ − (cid:80) BxBzdA
Fy ∝ (cid:80) ByBzdA
Fz ∝ (cid:80)(B2
x + B2
(cid:80) BxBz
δFx ∝
(cid:80) B2
δFy ∝ − (cid:80) By Bz
(cid:80)(B2
x+B2
δFz ∝
(cid:80) B2
Bdec(xt) = (cid:80)
Cdec(xt) = (cid:80)
Mdec(xt) = (cid:80)
Xdec(xt) = (cid:80)
Edec(xt) = (cid:80)
logEdec(xt) = (cid:80)
-
-
-
-
-
-
-
-
-

fi∈FB

fi∈FC

fi∈FM

t−t(fi)
τ

t−t(fi)
τ

t−t(fi)
τ

t−t(fi)
τ

e−
e−
e−
e−

fi∈FX
fi∈F Ei · e−

t−t(fi)
τ

fi∈F log(Ei) · e−

t−t(fi)
τ

Predicting Solar Flares Using a Long Short-Term Memory Network

5

Table 2. Numbers of Positive and Negative Samples for Each Flare Class

≥C class

≥M class
Positive Negative Positive Negative Positive Negative
18,266
7,055
8,732

66,311
19,418
35,957

83,944
26,181
44,509

81,867
25,126
43,411

2,710
1,347
1,278

633
292
180

≥M5.0 class

Training
Validation
Testing

Because the features have diﬀerent units and scales, we normalize the feature values as follows. For the 25 physical

features, let zk

i denote the normalized value of the ith feature of the kth data sample. Then

zk
i =

vk
i − µi
σi

,

(7)

where vk
i
standard deviation of the ith feature. For the 15 ﬂare history features, we have

is the original value of the ith feature of the kth data sample, µi is the mean of the ith feature, and σi is the

zk
i =

vk
i − mini
maxi − mini

,

(8)

where maxi and mini are the maximum and minimum value of the ith feature, respectively.

3. METHODOLOGY

3.1. Prediction Task

Following Bobra & Couvidat (2015), Jonas et al. (2018) and Nishizuka et al. (2018), we intend to use past observations
of an AR to predict its future ﬂaring activity. Speciﬁcally, we want to solve the following binary classiﬁcation problem:
will this AR produce a Υ-class ﬂare within the next 24 hours? We consider three Υ classes separately: ≥M5.0 class,
≥M class, and ≥C class. The importance of these classes has been discussed in recent works (Jonas et al. 2018;
Nishizuka et al. 2018). Both the ≥M class and ≥C class were studied in Nishizuka et al. (2018). Also, the ≥M class
was discussed in Liu et al. (2017) and the ≥C class was analyzed in Jonas et al. (2018). In addition, we consider the
≥M5.0 class due to the few X-class ﬂares in our dataset where a ≥M5.0-class ﬂare means the GOES X-ray ﬂux value
of the ﬂare is above 5 × 10−5Wm−2. A ﬂare in the ≥M5.0 class is generally considered a major ﬂare.

As in Bobra & Couvidat (2015), observation data whose AR is outside ± 70◦ of the center meridian or whose features
are incomplete are ignored. Data samples collected in years 2010–2013 are used for training, those in year 2014 are
used for validation, and those in years 2015–2018 are used for testing. The training set and testing set are disjoint, and
hence our algorithm will make predictions on ARs that it has never seen before. Figure 2 illustrates how we construct
positive samples and negative samples used by the proposed deep learning method. For the ≥C class, data samples
collected 24 hours prior to an X-class, M-class, or C-class ﬂare in an AR are positive and all other data samples in
the AR are negative. For the ≥M class, data samples collected 24 hours prior to an X-class or M-class ﬂare in an AR
are positive and all other data samples in the AR are negative. For the ≥M5.0 class, data samples collected 24 hours
prior to a ≥M5.0-class ﬂare in an AR are positive and all other data samples in the AR are negative. Notice that, if
a data sample is missing at some time point or if there are insuﬃcient data samples during the 24 hours prior to a
Υ-class ﬂare, we adopt a zero-padding strategy by adding synthetic data samples with zeros for all feature values to
yield a complete non-gapped time-series dataset. This zero-padding method is used after applying the normalization
procedures described in Equations (7) and (8). Therefore, the zero-padding method does not aﬀect the normalization
procedures. Table 2 shows the numbers of positive and negative samples for each ﬂare class where there are 1,269
ARs in total. Because most ARs do not produce ﬂares, our approach yields an imbalanced dataset in which negative
samples greatly outnumber positive samples.

For a given time point t and an AR, the proposed deep learning method predicts whether the AR will produce a
Υ-class ﬂare within the next 24 hours of t. There are three Υ classes, namely ≥M5.0 class, ≥M class, and ≥C class.
Therefore, we build three deep learning models separately, referred to as the ≥M5.0 model, ≥M model, and ≥C model
respectively, each corresponding to a Υ class of ﬂares.

6

Liu et al.

Figure 2. Construction of positive and negative samples used in the prediction task. Each rectangular box represents a data
sample, and corresponds to 1 hour in time. The red vertical line indicates the starting time of a Υ-class ﬂare. Data samples
collected 24 hours prior to the ﬂare, represented by blue rectangular boxes, belong to the positive class. The other data samples,
represented by green rectangular boxes, belong to the negative class. The white rectangular box, in which the ﬂare occurs, is
not included in our dataset.

Figure 3. Illustration of an LSTM unit. Here, ft is the forget gate, it is the input gate, ot is the output gate, Ct is the cell
state, xt is the input vector to the LSTM unit, and ht is the output vector of the LSTM unit.

3.2. Prediction Method

Our deep learning models employ a long short-term memory (LSTM) network. An LSTM unit contains four in-
teractive parts including a memory cell, an input gate, an output gate and a forget gate, as illustrated in Figure 3.
The key to LSTMs is the cell state, which is represented by the horizontal line at the top of the diagram in Figure 3.
Speciﬁcally, the new cell state Ct is updated by the old cell state Ct−1 and the candidate cell state ˜Ct as follows:

Ct = ft (cid:12) Ct−1 + it (cid:12) ˜Ct,

where the forget gate ft that controls the extent to which a value remains in the cell is calculated as:

ft = σ(Wf · [ht−1, xt] + Bf ),

and the input gate it that controls the extent to which a new value ﬂows into the cell is computed as:

it = σ(Wi · [ht−1, xt] + Bi).

(9)

(10)

(11)

Here xt represents the input vector at time step t and ht−1 represents the output vector at time step t − 1. The
candidate cell state ˜Ct is computed as:

˜Ct = tanh(Wc · [ht−1, xt] + Bc).

Finally, the output vector ht at time step t, which is based on the new cell state Ct, is computed as:

ht = ot (cid:12) tanh(Ct),

(12)

(13)

Predicting Solar Flares Using a Long Short-Term Memory Network

7

where

ot = σ(Wo · [ht−1, xt] + Bo).
In the above equations, W and B contain weights and biases respectively, which need to be learned during training;
[.] denotes the concatenation of two vectors; σ(·) is the sigmoid function, i.e., σ(z) = 1
1+e−z ; tanh(·) is the hyperbolic
tangent function, i.e., tanh(z) = ez−e−z

ez+e−z ; (cid:12) denotes the Hadamard product (element-wise multiplication).

Our deep learning architecture contains an LSTM layer with m LSTM units (in the study presented here, m is set
to 10). Motivated by the previous work in language translation (Bahdanau et al. 2014), where attention mechanism
was applied to allow a model to automatically search for parts of a source sentence that are related to the prediction
of a target word, we add an attention layer with m neurons above the LSTM layer to focus on information in relevant
time steps. The attention layer would take the states in all time steps into account and assign a weight to each state,
which indicates the importance of information that state has. The weight wi for state hi is derived by comparing the
target state with each source state, which is computed as:

(14)

wi =

escore(hi,ht)
j escore(hj ,ht)

(cid:80)

.

(15)

Here, ht is the state at the last time step and score(·) is a content-based function. We adopt the function used by
Luong et al. (2015), which is deﬁned as

where W contains learnable parameters. After wi is obtained, a context vector ct can be computed as:

score(hi, ht) = hT

t W hi,

(cid:88)

ct =

wihi.

(16)

(17)

i
The ﬁnal attention vector v of the input sequence is derived by concatenating the context vector ct and last hidden
state ht, and then being activated by a hyperbolic tangent layer as follows:

v = tanh(Wv[ct; ht]).

(18)

This activation vector v is then sent to two fully connected layers, the ﬁrst one having 200 neurons and the second one
having 500 neurons. Finally the output layer with 2 neurons, which is activated by the softmax function, produces
predicted values. Figure 4 shows the overall architecture of our LSTM network.

Let xt represent the data sample collected at time point t. During training, for each time point t, we take m
consecutive data samples xt−m+1, xt−m+2, . . . , xt−1, xt from the training set and use the m consecutive data samples
to train the LSTM network. The label of these m consecutive data samples is deﬁned to be the label of the last data
sample xt. Thus, if xt belongs to the positive class, then the input sequence xt−m+1, xt−m+2, . . . , xt−1, xt is deﬁned
as positive; otherwise the sequence is deﬁned as negative. Because the data samples are collected continuously at a
cadence of 1 hour and missing values are ﬁlled up by our zero-padding strategy, the input sequence spans m hours.

Because the dataset at hand is imbalanced where negative samples outnumber positive samples (see Table 2), we
use a weighted cross entropy cost function for optimizing model parameters during training. The cost function is
computed as:

J =

N
(cid:88)

K
(cid:88)

n=1

k=1

ωkynklog(ˆynk).

(19)

Here, N is the total number of sequences each having m consecutive data samples in the training set, K is the number
of classes, which is 2 in our case since we have only positive and negative classes, ωk is the weight of the kth class,
which is derived by the ratio of the sizes of the positive and negative classes with more weight given to the minority
(i.e., positive) class, ynk and ˆynk denote the observed probability (which is equal to 1 if the nth sequence belongs to
the kth class) and the estimated probability of being in the kth class of the nth sequence, respectively.2

2 We model data samples in ARs as time series. However, we do not bind the many time series to a single one. Instead, we process ARs
separately as follows. Our LSTM network accepts as input m=10 data samples at a time. Assuming there are P data samples on an active
region AR1, with our zero-padding strategy, we generate P sequences each having 10 data samples from AR1, and feed these P sequences,
one sequence at a time, to the LSTM network. Next, assuming there are Q data samples on another active region AR2, we generate Q
sequences each having 10 data samples from AR2, and feed these Q sequences, one sequence at a time, to the LSTM network. Although
AR1 and AR2 may have overlapping time points, we process the active regions separately, one at a time. N in Equation (19) represents
the total number of sequences we generate from the training set.

8

Liu et al.

Figure 4. Architecture of the proposed LSTM network. This network is mainly comprised of an LSTM layer, an attention
layer, two fully connected layers and an output layer. Each gray box in the LSTM layer is an LSTM unit as shown in Figure 3.
There are m LSTM units in the LSTM layer, m neurons in the attention layer, 200 neurons in the ﬁrst fully connected layer, 500
neurons in the second fully connected layer, and 2 neurons in the output layer activated by the softmax function. The LSTM
network takes as input the sequence xt−m+1, xt−m+2, . . . , xt−1, xt and produces as output a 2-dimension vector [y0, y1] with a
value of [1, 0] or [0, 1] which is determined by the probability calculated by the softmax function in the output layer.

The proposed LSTM network is implemented in Python, TensorFlow and Keras. A mini-batch strategy (Goodfellow
et al. 2016) is used to achieve faster convergence during backpropagation. The validation dataset is used for tuning
model hyperparameters. The optimizer used is Adam (Kingma & Ba 2014), which is a method for stochastic gradient
descent, where the learning rate is set to 0.001, β1 is set to 0.9, and β2 is set to 0.999. The batch size is set to 256
and the number of epochs is set to 7 . The length of each input sequence, m, is set to 10, meaning that every time 10
consecutive data samples are used as input to our LSTM network.

During testing, to predict whether an AR will produce a Υ-class ﬂare within the next 24 hours of a time
point t, we take xt and its preceding m − 1 data samples, and then feed the m consecutive testing data samples
xt−m+1, xt−m+2, . . . , xt−1, xt into the trained LSTM network. Here, the Υ class refers to the ≥M5.0 class, ≥M class,
and ≥C class, respectively. The output of the LSTM network, i.e., the predicted result, is a 2-dimension vector [y0, y1]
with a value of [1, 0] or [0, 1], indicating xt is positive (i.e., the AR will produce a Υ-class ﬂare within the next 24
hours of t) or xt is negative (i.e., the AR will not produce a Υ-class ﬂare within the next 24 hours of t). This value
is determined by comparing the probability calculated by the softmax function with a threshold. If the probability is
greater than or equal to the threshold, then xt is predicted to be positive; otherwise xt is predicted to be negative. It
should be pointed out that, the way we use the m consecutive testing data samples xt−m+1, xt−m+2, . . . , xt−1, xt to
predict whether there is a ≥M5.0-class (≥M-class, ≥C-class, respectively) ﬂare within the next 24 hours of the time
point t is totally diﬀerent from the previously published machine learning methods for solar ﬂare prediction (Bobra
& Couvidat 2015; Jonas et al. 2018; Nishizuka et al. 2018), which used only the testing data sample xt to make the
prediction.

Given an AR and a data sample xt observed at time point t, we deﬁne xt to be a true positive (TP) if the ≥M5.0
(≥M, ≥C, respectively) model predicts that xt is positive, i.e., the AR will produce a ≥M5.0- (≥M-, ≥C-, respectively)

4. RESULTS

4.1. Performance Metrics

Predicting Solar Flares Using a Long Short-Term Memory Network

9

class ﬂare within the next 24 hours of t, and xt is indeed positive. We deﬁne xt to be a false positive (FP) if the
≥M5.0 (≥M, ≥C, respectively) model predicts that xt is positive while xt is actually negative i.e., the AR will not
produce a ≥M5.0- (≥M-, ≥C-, respectively) class ﬂare within the next 24 hours of t. We say xt is a true negative
(TN) if the ≥M5.0 (≥M, ≥C, respectively) model predicts xt to be negative and xt is indeed negative; xt is a false
negative (FN) if the ≥M5.0 (≥M, ≥C, respectively) model predicts xt to be negative while xt is actually positive. We
also use TP (FP, TN, FN, respectively) to represent the total number of true positives (false positives, true negatives,
false negatives, respectively) produced by a model.

The performance metrics used in this study include the following:

Recall =

TP
TP + FN

,

Precision =

TP
TP + FP

,

Accuracy (ACC) =

TP + TN
TP + FP + TN + FN

,

Balanced Accuracy (BACC) =

1
2

(

TP
TP + FN

+

TN
TN + FP

),

Heidke Skill Score (HSS) =

2(TP × TN − FP × FN)
(TP+FN)(FN+TN) + (TP+FP)(FP+TN)

,

True Skill Statistics (TSS) =

TP
TP + FN

−

FP
TN + FP

.

(20)

(21)

(22)

(23)

(24)

(25)

ACC is not suitable for imbalanced classiﬁcation (He & Garcia 2009). The reason is that a naive classiﬁer predicting
all instances in the minority class to belong to the majority class would still get a high ACC value. Instead, BACC is
suggested for imbalanced classiﬁcation (He & Garcia 2009). Because of its unbiasedness over class-imbalance ratios,
we also follow the suggestion of Bloomﬁeld et al. (2012) to use the TSS score, which is the recall subtracted by the
false alarm rate. The Heidke Skill Score (HSS) (Heidke 1926) is used to measure the fractional improvement of our
prediction over the random prediction (Florios et al. 2018). The larger BACC, HSS, and TSS score a method has, the
better performance the method achieves.

4.2. Model Evaluation

We ﬁrst conduct an ablation study to analyze the proposed LSTM framework by considering three alternative
architectures, denoted by LSTM−a, LSTM−c and LSTM−ac, respectively. LSTM−a (LSTM−c, LSTM−ac, respectively)
is obtained by removing the attention layer (the two fully connected layers, both the attention layer and the two fully
connected layers, respectively) from the LSTM architecture in Figure 4. Table 3 shows the prediction results of the
four architectures for the three Υ classes, namely ≥M5.0 class, ≥M class, and ≥C class, of ﬂares. It can be seen from
Table 3 that the proposed LSTM architecture outperforms the ablations LSTM−a, LSTM−c and LSTM−ac in terms
of BACC, HSS and TSS scores. By including the attention layer and the two fully connected layers, the performance
of the LSTM framework improves. Our zero-padding strategy does not negatively aﬀect the prediction performance.
Although some normalized feature values of a data sample may become zeros due to the normalization procedures
described in Equations (7) and (8), the attention layer of our LSTM model is able to distinguish between this data
sample and those synthetic data samples added by our zero-padding method whose feature values are all zeros. The
attention layer pays little attention to the synthetic data samples whose feature values are all zeros.

We next compare our LSTM framework with ﬁve closely related machine learning methods including multilayer
perceptrons (MLP) (Haykin & Network 2004; Florios et al. 2018), Jordan network (JN) (Jordan 1997), support vector
machines (SVM) (Qahwaji & Colak 2007; Yuan et al. 2010; Bobra & Couvidat 2015; Boucheron et al. 2015; Muranushi
et al. 2015; Florios et al. 2018), random forests (RF) (Barnes et al. 2016; Liu et al. 2017; Florios et al. 2018), and
a recently published deep learning-based method, Deep Flare Net (DeFN; Nishizuka et al. 2018). All these methods
including ours (LSTM) can be used as a binary classiﬁcation model (Nishizuka et al. 2018; Jonas et al. 2018) or a
probabilistic forecasting model (Florios et al. 2018). A binary classiﬁcation model predicts whether or not an AR will

10

Liu et al.

Table 3. Flare Prediction Results (within 24 hours) of Four LSTM Architectures

≥M5.0 class ≥M class ≥C class

Recall

Precision

ACC

BACC

HSS

TSS

LSTM−ac
LSTM−a
LSTM−c
LSTM
LSTM−ac
LSTM−a
LSTM−c
LSTM
LSTM−ac
LSTM−a
LSTM−c
LSTM
LSTM−ac
LSTM−a
LSTM−c
LSTM
LSTM−ac
LSTM−a
LSTM−c
LSTM
LSTM−ac
LSTM−a
LSTM−c
LSTM

0.944
0.939
0.956
0.978
0.042
0.039
0.041
0.038
0.914
0.904
0.910
0.899
0.929
0.933
0.933
0.938
0.074
0.068
0.071
0.074
0.858
0.865
0.865
0.877

0.888
0.899
0.876
0.881
0.181
0.184
0.216
0.222
0.882
0.883
0.906
0.909
0.885
0.891
0.891
0.895
0.267
0.271
0.316
0.347
0.770
0.782
0.783
0.790

0.743
0.747
0.750
0.762
0.543
0.537
0.536
0.544
0.828
0.825
0.824
0.829
0.796
0.795
0.796
0.803
0.519
0.515
0.514
0.539
0.591
0.591
0.592
0.607

produce a ≥M5.0- (≥M-, ≥C-, respectively) class ﬂare within the next 24 hours. A probabilistic forecasting model
predicts the probability for an AR to produce a ≥M5.0- (≥M-, ≥C-, respectively) class ﬂare within the next 24 hours.
A probabilistic forecasting model can be converted into a binary classiﬁcation model by using a probability threshold
to make predictions as follows. If the predicted probability is greater than or equal to the threshold, then the AR will
produce a ﬂare within the next 24 hours; otherwise the AR will not produce a ﬂare within the next 24 hours.

The MLP method consists of an input layer, an output layer and two hidden layers with 200 neurons and 500 neurons
respectively. For the JN method, its output dimension is set to 10. SVM uses the radial basis function (RBF) kernel.
RF has two parameters: mtry (number of features randomly selected to split a node) and ntree (number of trees to
grow in the forest). We vary the values of ntree ∈ {300, 500, 1,000} and mtry ∈ [2, 8], and set ntree to 500 and mtry to
3 since these two parameter values yield the maximum TSS for RF. Table 4 compares our LSTM with the ﬁve related
methods. All the methods are treated as binary classiﬁcation models where the probability thresholds are chosen to
maximize their respective TSS values, and the same threshold is used to calculate all performance metrics including
BACC, HSS and TSS for each method with respect to each ﬂare class. It can be seen from Table 4 that LSTM and
RF are the two best methods. The probability thresholds used by LSTM in Table 4 are 50%, 60% and 50% for ≥M5.0,
≥M and ≥C models respectively (the same thresholds are used in Table 3). The probability thresholds used by RF in
Table 4 are 0.5%, 5% and 25% for ≥M5.0, ≥M and ≥C models respectively.

To further understand the behavior of the proposed LSTM method, we conduct a cross-validation study as follows.
Refer to Table 2. For each of the ≥M5.0, ≥M and ≥C models, we partition its training (testing, respectively) set into
10 equal-sized folds. For every two training (testing, respectively) folds i and j, i (cid:54)= j, fold i and fold j are disjoint;
furthermore, fold i and fold j contain approximately the same number of positive training (testing, respectively) data
samples and approximately the same number of negative training (testing, respectively) data samples. In run (i, j),
1 ≤ i ≤ 10, 1 ≤ j ≤ 10, all training samples except those in training fold i are used to train a model, and the trained
model is used to make predictions on all testing samples except those in testing fold j. We calculate the performance

Predicting Solar Flares Using a Long Short-Term Memory Network

11

Table 4. Flare Prediction Results (within 24 hours) of Our LSTM and Five Related Machine Learning Methods

≥M5.0 class ≥M class ≥C class

Recall

Precision

ACC

BACC

HSS

TSS

MLP
SVM
JN
DeFN
RF
LSTM
MLP
SVM
JN
DeFN
RF
LSTM
MLP
SVM
JN
DeFN
RF
LSTM
MLP
SVM
JN
DeFN
RF
LSTM
MLP
SVM
JN
DeFN
RF
LSTM
MLP
SVM
JN
DeFN
RF
LSTM

0.944
0.644
0.923
0.889
1.000
0.978
0.037
0.014
0.033
0.037
0.034
0.038
0.901
0.818
0.882
0.907
0.886
0.899
0.922
0.732
0.903
0.898
0.943
0.938
0.064
0.020
0.056
0.064
0.059
0.074
0.845
0.464
0.806
0.796
0.886
0.877

0.812
0.692
0.851
0.891
0.850
0.881
0.143
0.106
0.178
0.173
0.252
0.222
0.855
0.824
0.884
0.872
0.924
0.909
0.834
0.760
0.868
0.881
0.888
0.895
0.204
0.141
0.260
0.253
0.361
0.347
0.669
0.520
0.736
0.763
0.776
0.790

0.637
0.746
0.701
0.761
0.727
0.762
0.451
0.497
0.543
0.497
0.532
0.544
0.778
0.803
0.826
0.801
0.822
0.829
0.725
0.781
0.779
0.786
0.786
0.803
0.389
0.472
0.502
0.476
0.502
0.539
0.449
0.562
0.558
0.572
0.572
0.607

metric values based on the predictions made in run (i, j). There are 100 runs. The means and standard deviations
over the 100 runs are calculated and recorded.

4.3. Feature Assessment

Motivated by RF which uses only 3 features to split a node when constructing a tree, we wonder whether using
fewer features can also achieve better performance for our LSTM method. We thus analyze the importance of each
of the 40 features studied in the paper with respect to the ≥M5.0, ≥M, and ≥C models respectively based on the
LSTM architecture in Figure 4 using the cross-validation methodology described above. Each time only one feature
is used by the ≥M5.0 (≥M, ≥C, respectively) model to predict whether a given AR will produce a ≥M5.0- (≥M-,
≥C-, respectively) class ﬂare within the next 24 hours. The probability threshold is set to maximize the TSS score in
each test. The corresponding mean TSS score is recorded. There are 40 features, so 40 mean individual TSS scores

12

Liu et al.

are recorded. These 40 mean individual TSS scores are sorted in descending order, and the 40 corresponding features
are ranked from the most important to the least important accordingly. The sorted, mean individual TSS scores and
ranked features are plotted in a chart for each model. Then, according to the ranked features, mean cumulative TSS
scores are calculated and plotted in the chart. Speciﬁcally, the mean cumulative TSS score of the top k, 1 ≤ k ≤ 40,
most important features with respect to the ≥M5.0 (≥M, ≥C, respectively) model is equal to the mean TSS score of
the ≥M5.0 (≥M, ≥C, respectively) model that uses the top k most important features altogether for ﬂare prediction.
Figure 5 (6, 7, respectively) presents the feature importance chart for the ≥M5.0 (≥M, ≥C, respectively) model. In
each ﬁgure, blue bars represent mean individual TSS scores of the 40 features and the red polygonal line represents
mean cumulative TSS scores of the top k, 1 ≤ k ≤ 40, most important features. Error bars, representing standard
deviations, are also plotted. It can be seen from the ﬁgures that predictive parameters that are consistently ranked in
the top 20 list for all the three models include the following 16 features: TOTUSJH, TOTBSQ, TOTPOT, TOTUSJZ,
ABSNJZH, SAVNCPP, USFLUX, AREA ACR, MEANPOT, TOTFX, Cdec, Chis, Chis1d, Edec, Mhis and Xmax1d.
Among these 16 features, there are 10 physical parameters, or SDO/HMI magnetic parameters, including TOTUSJH,
TOTBSQ, TOTPOT, TOTUSJZ, ABSNJZH, SAVNCPP, USFLUX, AREA ACR, MEANPOT and TOTFX. All these
10 physical parameters except TOTFX are among the 13 magnetic parameters that are also considered important in
Bobra & Couvidat (2015) and Liu et al. (2017), which used diﬀerent methods for assessing the importance of features.
Thus, our ﬁndings are consistent with those reported in the literature. There are 4 magnetic parameters, TOTFZ,
R VALUE, SHRGT45 and EPSZ, which are considered important in Bobra & Couvidat (2015) and Liu et al. (2017),
but are not ranked high in our list; some ﬂare history features are ranked higher than these four magnetic parameters.
It is worth noting that TOTUSJH plays the most important role, i.e., is ranked the top one, for all the three models.
Moreover, some features including TOTUSJH, TOTUSJZ, TOTPOT, TOTBSQ, USFLUX, SAVNCPP, Cdec, Chis
and Chis1d show strong predictive power and are consistently ranked in the top 10 list for all the three models.

Figure 5. Assessment of feature importance for predicting ≥M5.0-class ﬂares. Blue bars represent mean individual TSS scores
and the red polygonal line represents mean cumulative TSS scores of the features.

We note that the history of C-class ﬂares has a high impact on ﬂare prediction. Chis and Chis1d from Nishizuka
et al. (2017) count the number of previous C-class ﬂares. Cdec from Jonas et al. (2018) is the time decay value based
on previous C-class ﬂares. These three ﬂare history features are ranked high in our list. The history of M-class ﬂares
plays a more important role for the ≥M5.0 and ≥M models than for the ≥C model. Other ﬂare history features such

Predicting Solar Flares Using a Long Short-Term Memory Network

13

Figure 6. Assessment of feature importance for predicting ≥M-class ﬂares. Blue bars represent mean individual TSS scores
and the red polygonal line represents mean cumulative TSS scores of the features.

Figure 7. Assessment of feature importance for predicting ≥C-class ﬂares. Blue bars represent mean individual TSS scores
and the red polygonal line represents mean cumulative TSS scores of the features.

14

Liu et al.

Table 5. Flare Prediction Results (within 24 hours) of Our LSTM and RF Obtained from Cross Validation

Recall

Precision

ACC

BACC

HSS

TSS

RF

RF

RF

≥M5.0 class
0.960 (0.027)
LSTM 0.960 (0.017)
0.026 (0.001)
LSTM 0.048 (0.008)
0.853 (0.003)
LSTM 0.921 (0.014)
0.906 (0.014)
LSTM 0.940 (0.007)
0.042 (0.002)
LSTM 0.084 (0.015)
0.812 (0.027)
LSTM 0.881 (0.014)

RF

RF

RF

≥M class
0.888 (0.006)
0.885 (0.017)
0.179 (0.003)
0.222 (0.023)
0.880 (0.002)
0.907 (0.013)
0.884 (0.003)
0.896 (0.004)
0.262 (0.004)
0.323 (0.030)
0.768 (0.005)
0.792 (0.008)

≥C class
0.730 (0.002)
0.773 (0.027)
0.499 (0.003)
0.541 (0.030)
0.804 (0.001)
0.826 (0.015)
0.776 (0.001)
0.806 (0.004)
0.469 (0.003)
0.526 (0.021)
0.552 (0.003)
0.612 (0.009)

as the histories of B-class and X-class ﬂares are relatively unimportant for predicting ≥M5.0- (≥M-, ≥C-, respectively)
class ﬂares.

By carefully examining Figures 5, 6 and 7, we ﬁnd that using all the 40 features together does not yield the highest
mean cumulative TSS scores. In fact, using roughly the top 14-22 most important features together yields the highest
mean cumulative TSS scores, achieving the best performance. Speciﬁcally, using the top 20 (22, 14, respectively) most
important features yields the highest mean cumulative TSS score for the ≥M5.0 (≥M, ≥C, respectively) model. This
happens probably because low ranked features are noisy features, and using them may deteriorate the performance of
the models. In subsequent experiments, we use the best features for each model.

4.4. Comparison between RF and LSTM

Table 4 shows that RF and LSTM are the two best methods.

In this subsection, we further compare RF and
LSTM using the cross-validation methodology described above. Table 5 shows their performance metric values where
standard deviations are enclosed in parentheses. The probability thresholds used by RF are set to 0.5%, 5%, 25%
for ≥M5.0, ≥M, ≥C class respectively where the thresholds are chosen to maximize TSS. The probability thresholds
used by LSTM are set to 75%, 60%, 50% for ≥M5.0, ≥M, ≥C class respectively. These optimal thresholds are slightly
diﬀerent from those used in Table 4. This happens because the performance metric values in Table 4 are obtained
from a single dataset whereas those in Table 5 are obtained from cross validation. Furthermore, LSTM in Table 4
uses all 40 features together whereas LSTM in Table 5 uses only the (14-22) best features. According to Table 5 and a
Wilcoxon signed rank test (Wilcoxon 1947), our LSTM method is signiﬁcantly better than RF (p < 0.05) in all three
ﬂare classes in terms of BACC, HSS and TSS.3 These results indicate that LSTM outperforms RF when the methods
are used as binary classiﬁcation models.

We next compare RF and LSTM using (i) skill scores proﬁles (SSP) of BACC, HSS and TSS as functions of the
probability threshold, (ii) Receiver Operating Characteristic (ROC) curves, and (iii) Reliability Diagrams (RD). ROC
curves describe the relationship between the true positive rate and false positive rate. The Area Under the Curve
(AUC) in the ROC represents the degree of separability, indicating how well a model is capable of distinguishing
between two classes with the ideal value of one (Marzban 2004). The RD describes the relationship between the actual
observed frequencies of ﬂares of interest and the probabilities predicted by a model. A bin diagram, presented as
an inset in the RD, is used to show the distribution of the predicted probabilities. The X-axis of the bin diagram
represents the predicted probabilities and the Y-axis represents the numbers of data samples. As in Florios et al.
(2018) we use 20 bins of length 0.05 each. Thus, for example, the Y value of the ﬁrst bin shows the number of data
samples whose predicted probabilities of having a ﬂare of interest in the next 24 hours are within 0.05. The ideal
situation is represented by the diagonal line in the RD; a point (x, x) on the diagonal indicates that among those data
samples whose predicted probability is x, the ratio of the data samples that actually have a ﬂare of interest within
the next 24 hours is also x. In addition, we use the Brier Score (BS) (Brier 1950) and Brier Skill Score (BSS) (Wilks

3 The source code of LSTM and dataset can be downloaded from https://web.njit.edu/∼wangj/LSTMpredict/.

Predicting Solar Flares Using a Long Short-Term Memory Network

15

(a) RF, SSP

(b) RF, ROC

(c) RF, RD

(d) LSTM, SSP

(e) LSTM, ROC

(f) LSTM, RD

Figure 8. Comparison between RF (top) and LSTM (bottom) for ≥M5.0-class ﬂare prediction where the corresponding SSP,
ROC and RD are displayed from left to right.

2010, 2011) to quantitatively assess the performance of probabilistic forecasting models. The values of BS range from
0 to 1 with the perfect score being 0. The values of BSS range from minus inﬁnity to 1 with the perfect score being 1.
Figure 8 presents SSP, ROC and RD plots for RF and LSTM respectively when the methods are used in ≥M5.0-class
ﬂare prediction. Refer to the SSP plots. For RF, the maximum TSS=0.812 is obtained with a probability threshold
of 0.5%. With this threshold, BACC=0.906±0.014 and HSS=0.042±0.002. For LSTM, the maximum TSS=0.881 is
obtained with a probability threshold of 75%. With this threshold, BACC=0.940±0.007 and HSS=0.084±0.015. The
ROC curve of LSTM is better than that of RF. LSTM has an AUC of 0.984±0.003, which is better than RF with an
AUC of 0.948±0.011. Refer to the RD plots. The curves of RF and LSTM are far away from the diagonal lines in the
RD plots. The BS and BSS achieved by RF are 0.004±0.001 and 0.053±0.008 respectively. The BS and BSS achieved
by LSTM are 0.090±0.011 and -21.576±2.956 respectively. In terms of BS and BSS, RF is better than LSTM.

Figure 9 presents SSP, ROC and RD plots for RF and LSTM respectively when the methods are used in ≥M-class
ﬂare prediction. Refer to the SSP plots. For RF, the maximum TSS=0.768 is obtained with a probability threshold
of 5%. With this threshold, BACC=0.884±0.003 and HSS=0.262±0.004. For LSTM, the maximum TSS=0.792 is
obtained with a probability threshold of 60%. With this threshold, BACC=0.896±0.004 and HSS=0.323±0.030. The
ROC curve of LSTM is slightly better than that of RF. LSTM has an AUC of 0.948±0.003, which is better than RF
with an AUC of 0.935±0.002. Refer to the RD plots. The curve of RF is closer to the diagonal than the curve of
LSTM. The BS and BSS achieved by RF are 0.021±0.001 and 0.260±0.006 respectively. The BS and BSS achieved

16

Liu et al.

(a) RF, SSP

(b) RF, ROC

(c) RF, RD

(d) LSTM, SSP

(e) LSTM, ROC

(f) LSTM, RD

Figure 9. Comparison between RF (top) and LSTM (bottom) for ≥M-class ﬂare prediction where the corresponding SSP,
ROC and RD are displayed from left to right.

by LSTM are 0.090±0.009 and -2.241±0.319 respectively. These results suggest that RF be a better probabilistic
forecasting model than LSTM.

Figure 10 presents SSP, ROC and RD plots for RF and LSTM respectively when the methods are used in ≥C-class
ﬂare prediction. Refer to the SSP plots. For RF, the maximum TSS=0.552 is obtained with a probability threshold
of 25%. With this threshold, BACC=0.776±0.001 and HSS=0.469±0.003. For LSTM, the maximum TSS=0.612 is
obtained with a probability threshold of 50%. With this threshold, BACC=0.806±0.004 and HSS=0.526±0.021. The
ROC curve of LSTM is better than that of RF. LSTM has an AUC of 0.871±0.002, which is better than RF with
an AUC of 0.851±0.001. Refer to the RD plots. The curve of RF almost overlaps the diagonal. The BS and BSS
achieved by RF are 0.103±0.001 and 0.344±0.002 respectively. The BS and BSS achieved by LSTM are 0.133± 0.007
and 0.152±0.047 respectively. These results indicate that RF is better than LSTM when the methods are used as
probabilistic forecasting models.

5. DISCUSSION AND CONCLUSIONS

We develop a long short-term memory (LSTM) network to predict whether an AR would produce a Υ-class ﬂare
within the next 24 hours. We consider three Υ classes, namely ≥M5.0 class, ≥M class, and ≥C class, and build
three LSTM models separately, each corresponding to a Υ class. Each LSTM model is used to make predictions of
its corresponding Υ-class ﬂares. We build a dataset containing the data in the period from 2010 May to 2018 May,
gathered from the JSOC website. Each sample in the dataset has 40 features, including 25 magnetic parameters

Predicting Solar Flares Using a Long Short-Term Memory Network

17

(a) RF, SSP

(b) RF, ROC

(c) RF, RD

(d) LSTM, SSP

(e) LSTM, ROC

(f) LSTM, RD

Figure 10. Comparison between RF (top) and LSTM (bottom) for ≥C-class ﬂare prediction where the corresponding SSP,
ROC and RD are displayed from left to right.

provided by SHARP and related data products as well as 15 ﬂare history parameters. We divide the dataset into three
subsets: the subset covering 2010–2013 for training, the subset covering 2014 for validation, and the subset covering
2015–2018 for testing. The training subset and testing subset are disjoint, and hence our proposed method will make
predictions on ARs that it has never seen before. With extensive experiments, we evaluate the performance of all three
LSTM models and compare them with closely related machine learning methods using diﬀerent performance metrics.
The main results are summarized as follows.

1. Solar data samples in an AR are considered as time series in this study. Although some researchers
(Nishizuka et al. 2017; Jonas et al. 2018) utilize information concerning ﬂaring history for solar ﬂare
forecasts, none of the previous studies model the data samples as time series and adopt LSTMs to capture
dependencies in the temporal domain of the data samples. To our knowledge, this is the ﬁrst attempt of
using LSTMs for solar ﬂare prediction.

2. We evaluate the importance of each of the 40 features used in this study. Our experimental results
show that, among these 40 features, 10 SDO/HMI magnetic parameters including TOTUSJH, TOTBSQ,
TOTPOT, TOTUSJZ, ABSNJZH, SAVNCPP, USFLUX, AREA ACR, MEANPOT, TOTFX, and 6 ﬂare
history parameters including Cdec, Chis, Chis1d, Edec, Mhis and Xmax1d are more important than the
other features for ﬂare prediction. Our ﬁndings on the SDO/HMI magnetic parameters are mostly consis-
tent with those reported in Bobra & Couvidat (2015). It was also observed that the history of C-class ﬂares

18

Liu et al.

contributes the most to ﬂare prediction among all the ﬂare history parameters. Although the rankings of
the features are not the same for the three LSTM models, some features such as TOTUSJH, TOTUSJZ,
TOTPOT, TOTBSQ, USFLUX, SAVNCPP, Cdec, Chis and Chis1d exhibit great predictive power for all
the three models. Furthermore, using only 14-22 most important features including both ﬂare history and
magnetic parameters can achieve better performance than using all the 40 features together.

3. Our LSTM-based approach achieves better performance than related machine learning methods such as
multilayer perceptrons (MLP) (Haykin & Network 2004; Florios et al. 2018), Jordan network (JN) (Jordan
1997), support vector machines (SVM) (Qahwaji & Colak 2007; Yuan et al. 2010; Bobra & Couvidat 2015;
Boucheron et al. 2015; Muranushi et al. 2015; Florios et al. 2018), and a recently published deep learning-
based method, Deep Flare Net (DeFN; Nishizuka et al. 2018). In addition, we conduct an ablation study by
considering three alternative architectures (ablations) LSTM−a, LSTM−c and LSTM−ac. Our experimental
results show that the proposed LSTM architecture achieves better performance than the three ablations,
demonstrating the eﬀectiveness of adding the attention layer and fully connected layers to LSTM units.

4. A related machine learning algorithm, namely random forests (RF) (Barnes et al. 2016; Liu et al. 2017;
Florios et al. 2018), is comparable to our LSTM method. If one is interested in getting a probabilistic
estimate of how likely an AR will produce a ≥M5.0- (≥M-, ≥C-, respectively) class ﬂare within the next 24
hours, then RF would be the best choice. On the other hand, if one is interested in getting a ﬁrm answer
regarding whether or not an AR will produce a ≥M5.0- (≥M-, ≥C-, respectively) class ﬂare within the
next 24 hours, then our LSTM method is signiﬁcantly better than RF and is recommended.

Based on our experimental results, we conclude that the proposed LSTM-based framework is a valid method for
solar ﬂare prediction. Considering ﬂare history parameters, besides SDO/HMI magnetic parameters, helps improve
prediction performance, a ﬁnding consistent with that reported in Nishizuka et al. (2017) and Jonas et al. (2018). As
solar data from various instruments are gathered at an unprecedented rate, some researchers attempt to employ addi-
tional features including solar images for ﬂare prediction (Jonas et al. 2018). In future work, we plan to incorporate the
image data into deep learning methods for predicting solar ﬂares and other events (e.g., ﬁlament eruptions and CMEs).

We thank the referee for very helpful and thoughtful comments. We also thank the team of SDO/HMI for produc-
ing vector magnetic ﬁeld data products. The X-ray ﬂare catalogs were prepared by and made available through
NOAA NCEI. The related machine learning methods studied in this work were implemented in Python. C.L.
and H.W. acknowledge the support of NASA under grants NNX16AF72G, 80NSSC17K0016, 80NSSC18K0673 and
80NSSC18K1705.

REFERENCES

Ahmed, O. W., Qahwaji, R., Colak, T., et al. 2013, SoPh,

Bobra, M. G., Sun, X., Hoeksema, J. T., et al. 2014, SoPh,

283, 157, doi: 10.1007/s11207-011-9896-1

289, 3549, doi: 10.1007/s11207-014-0529-3

Alpaydin, E. 2009, Introduction to machine learning (MIT

Boucheron, L. E., Al-Ghraibah, A., & McAteer, R. T. J.

press)

2015, ApJ, 812, 51, doi: 10.1088/0004-637X/812/1/51

Bahdanau, D., Cho, K., & Bengio, Y. 2014, CoRR,

Breiman, L. 2001, Machine learning, 45, 5

abs/1409.0473. https://arxiv.org/abs/1409.0473

Brier, G. W. 1950, Monthly Weather Review, 78, 1

Barnes, G., Leka, K. D., Schrijver, C. J., et al. 2016, ApJ,

Colak, T., & Qahwaji, R. 2009, Space Weather, 7, S06001,

829, 89, doi: 10.3847/0004-637X/829/2/89

doi: 10.1029/2008SW000401

Benvenuto, F., Piana, M., Campi, C., & Massone, A. M.

Daglis, I., Baker, D., Kappenman, J., Panasyuk, M., &

2018, ApJ, 853, 90, doi: 10.3847/1538-4357/aaa23c

Daly, E. 2004, Space Weather, 2, S02004,

Bloomﬁeld, D. S., Higgins, P. A., McAteer, R. T. J., &

doi: 10.1029/2003SW000044

Gallagher, P. T. 2012, ApJL, 747, L41,

doi: 10.1088/2041-8205/747/2/L41

Fern´andez, S., Graves, A., & Schmidhuber, J. 2007, in

Proceedings of the 17th International Conference on

Bobra, M. G., & Couvidat, S. 2015, ApJ, 798, 135,

Artiﬁcial Neural Networks, Part II, 220–229.

doi: 10.1088/0004-637X/798/2/135

https://doi.org/10.1007/978-3-540-74695-9 23

Predicting Solar Flares Using a Long Short-Term Memory Network

19

Fisher, G. H., Bercik, D. J., Welsch, B. T., & Hudson, H. S.

Muranushi, T., Shibayama, T., Muranushi, Y. H., et al.

2012, SoPh, 277, 59, doi: 10.1007/s11207-011-9907-2

Florios, K., Kontogiannis, I., Park, S.-H., et al. 2018, SoPh,

2015, Space Weather, 13, 778,
doi: 10.1002/2015SW001257

293, 28, doi: 10.1007/s11207-018-1250-4

Goodfellow, I. J., Bengio, Y., & Courville, A. C. 2016, Deep

Learning, Adaptive computation and machine learning

(MIT Press). http://www.deeplearningbook.org/

Graves, A., Fern´andez, S., Liwicki, M., Bunke, H., &

Schmidhuber, J. 2007, in Proceedings of the 21st Annual

Conference on Neural Information Processing Systems,

577–584

Graves, A., Mohamed, A., & Hinton, G. E. 2013, in

Proceedings of the IEEE International Conference on

Acoustics, Speech and Signal Processing, 6645–6649.

https://doi.org/10.1109/ICASSP.2013.6638947

Graves, A., & Schmidhuber, J. 2005, Neural Networks, 18,

602, doi: 10.1016/j.neunet.2005.06.042

Graves, A., & Schmidhuber, J. 2008, in Proceedings of the

22nd Annual Conference on Neural Information

Processing Systems, 545–552

Haykin, S., & Network, N. 2004, Neural networks, 2, 41

He, H., & Garcia, E. A. 2009, IEEE Trans. Knowl. Data

Eng., 21, 1263, doi: 10.1109/TKDE.2008.239

Heidke, P. 1926, Geograﬁska Annaler, 8, 301

Higgins, P. A., Gallagher, P. T., McAteer, R. T. J., &

Bloomﬁeld, D. S. 2011, Advances in Space Research, 47,

2105, doi: 10.1016/j.asr.2010.06.024

Hochreiter, S., & Schmidhuber, J. 1997, Neural

Computation, 9, 1735, doi: 10.1162/neco.1997.9.8.1735

Nishizuka, N., Sugiura, K., Kubo, Y., Den, M., & Ishii, M.
2018, ApJ, 858, 113, doi: 10.3847/1538-4357/aab9a7

Nishizuka, N., Sugiura, K., Kubo, Y., et al. 2017, ApJ, 835,

156, doi: 10.3847/1538-4357/835/2/156

Park, S.-H., Lee, J., Choe, G. S., et al. 2008, ApJ, 686,

1397, doi: 10.1086/591117

Pesnell, W. D., Thompson, B. J., & Chamberlin, P. C.
2012, SoPh, 275, 3, doi: 10.1007/s11207-011-9841-3
Priest, E. R., & Forbes, T. G. 2002, A&A Rv, 10, 313,

doi: 10.1007/s001590100013

Qahwaji, R., & Colak, T. 2007, SoPh, 241, 195,

doi: 10.1007/s11207-006-0272-5

Schmidhuber, J., Wierstra, D., & Gomez, F. J. 2005, in

Proceedings of the 19th International Joint Conference
on Artiﬁcial Intelligence, 853–858.
http://ijcai.org/Proceedings/05/Papers/1452.pdf

Schou, J., Scherrer, P. H., Bush, R. I., et al. 2012, SoPh,

275, 229, doi: 10.1007/s11207-011-9842-2

Shibata, K., & Magara, T. 2011, Living Reviews in Solar

Physics, 8, 6, doi: 10.12942/lrsp-2011-6

Song, H., Tan, C., Jing, J., et al. 2009, SoPh, 254, 101,

doi: 10.1007/s11207-008-9288-3

Steward, G., Lobzin, V., & Wilkinson, P. J. 2010, AGU Fall

Meeting Abstracts, SH11B

Sun, X., & for the CGEM Team 2014, arXiv:1405.7353
SunPy Community, Mumford, S. J., Christe, S., et al. 2015,

Computational Science and Discovery, 8, 014009,
doi: 10.1088/1749-4699/8/1/014009

Hopﬁeld, J. J. 1982, Proceedings of the National Academy

Takasao, S., Fan, Y., Cheung, M. C. M., & Shibata, K.

of Sciences, 79, 2554, doi: 10.1073/pnas.79.8.2554

Huang, X., Zhang, L., Wang, H., & Li, L. 2013, A&A, 549,

2015, ApJ, 813, 112, doi: 10.1088/0004-637X/813/2/112
Wang, H. N., Cui, Y. M., Li, R., Zhang, L. Y., & Han, H.

A127, doi: 10.1051/0004-6361/201219742

Jonas, E., Bobra, M., Shankar, V., Todd Hoeksema, J., &

Recht, B. 2018, SoPh, 293, 48,

doi: 10.1007/s11207-018-1258-9

Jordan, M. I. 1997, in Advances in psychology, Vol. 121

(Elsevier), 471–495

Kingma, D. P., & Ba, J. 2014, CoRR, abs/1412.6980.

https://arxiv.org/abs/1412.6980

Li, R., Cui, Y., He, H., & Wang, H. 2008, Advances in

2008, Advances in Space Research, 42, 1464,
doi: 10.1016/j.asr.2007.06.070

Wilcoxon, F. 1947, Biometrics, 3, 119
Wilks, D. S. 2010, Quarterly Journal of the Royal

Meteorological Society, 136, 2109

—. 2011, Statistical Methods in the Atmospheric Sciences,

Volume 100 (Academic Press)

Winter, L. M., & Balasubramaniam, K. 2015, Space
Weather, 13, 286, doi: 10.1002/2015SW001170

Yu, D., Huang, X., Hu, Q., et al. 2010, ApJ, 709, 321,

Space Research, 42, 1469, doi: 10.1016/j.asr.2007.12.015

doi: 10.1088/0004-637X/709/1/321

Liu, C., Deng, N., Wang, J. T. L., & Wang, H. 2017, ApJ,

Yu, D., Huang, X., Wang, H., & Cui, Y. 2009, SoPh, 255,

843, 104, doi: 10.3847/1538-4357/aa789b

91, doi: 10.1007/s11207-009-9318-9

Luong, M., Pham, H., & Manning, C. D. 2015, CoRR,

abs/1508.04025. https://arxiv.org/abs/1508.04025

Marzban, C. 2004, Weather and Forecasting, 19, 1106

Yuan, Y., Shih, F. Y., Jing, J., & Wang, H.-M. 2010,
Research in Astronomy and Astrophysics, 10, 785,
doi: 10.1088/1674-4527/10/8/008

