Developing a Lecture Video Recording System
Using Augmented Reality

Yuma Ito, Masato Kikuchi, Tadachika Ozono, Toramatsu Shintani
Department of Computer Science, Graduate School of Engineering
Nagoya Institute of Technology
Gokiso-cho, Showa-ku, Nagoya, Aichi, 466-8555 Japan
E-mail: {higashi,kikuchi,ozono,tora}@ozlab.org

1
2
0
2

t
c
O
1
1

]

C
H
.
s
c
[

1
v
5
5
9
5
0
.
0
1
1
2
:
v
i
X
r
a

Abstract—Assistive technology is a prerequisite for making a
high-quality lecture video. It is therefore imperative to edit the
lecture video after recording. In this study, we aim to reduce
the cumbersome task of lecture video editing by developing
a system that enables the addition of visual effects in the
video while recording. In particular, we use augmented reality
(AR) technology to digitize and display in real-time lecture
materials, assistant agents, and other recording contents used
by the lecturer. Our system realizes such a mechanism as a
lecture recording environment. In addition, our system based
on AR technology can support the work of the lecturer, which
is difﬁcult to do by oneself while conducting the lecture, using
the information of the lecturer’s position and the progress of the
lecture. We evaluated the system functionality and performance,
and veriﬁed the system’s correct behavior. If the burden of
making lecture videos can be reduced, the lecturer will be able
to devote more time to improving the quality of lecture contents,
which is expected to contribute to the improvement of lectures.
Index Terms—Augmented Reality, Semi-autonomous Agent,

Making Video Effect, Lecture Video Making Support

I. INTRODUCTION

In some cases, lecture videos are used as pre-distributed
materials in on-demand classes and ﬂipped classrooms [1].
Lecturers produce lecture videos in various ways. The lecture
video that this study aims to produce shows the lecturer’s
facial expressions and motions, is easy to watch, and has visual
effects to a certain extent. Lecture videos are preferred to be
in a format that shows the lecturer [2].

Making lecture videos consists of recording and editing.
After the recording, lecturers usually edit videos to add rich
effects, however, editing is time-consuming. We aim to realize
a system that can automatically add appropriate effects to a
video while recording it. This means that lecturers need not
edit after recording to create high-quality lecture videos. To
reduce editing work after recording, this system must operate
exactly as intended by the user while recording a long lecture
video.

Our approach provides support for video making during
lecture recording, thereby reducing the burden after recording.
Speciﬁcally, our system uses augmented reality (AR) tech-
nology to create a lecture video with added visual effects
in real-time in accordance with the progress of the lecture.
Our system digitizes and displays lecture materials, lecture
assistant agents, and other lecture recording contents. The

lecturer uses these tools to conduct lectures in augmented
reality (AR) space. Our system provides the lecturer such a
lecture video recording environment in the AR space. Our
system using digital lecture contents can adapt to various
lecture formats envisaged by lecturers, even when recording a
lecture without the need for video editing. Another advantage
of using AR technology, which is a feature of our system,
enables our system to handle three-dimensional information
based on depth data. For instance,
includes the plane
information and position of the lecturer in the lecture space.
Our system uses this information to conﬁgure the display
of AR Slide, to control lecture assistant agent, and system
functions for lecture support. Especially in designing such
a system, we need to reduce unwanted editing work after
recording due to system errors or delays.

it

The rest of the paper is organized as follows. Section II
presents related works. Section III describes how to support the
creation of lecture videos using AR technology. Sections IV
and V describe the implementation and experiments. Finally,
we discuss our system considering the results in Section VI
and conclude this paper in Section VII.

II. RELATED WORK

A. Video Effect Production Support

Some researchers have focused on adding visual effects
to lecture and presentation videos using AR and 2DCG
technology to have a high effect in terms of expression and
entertainment. In particular, the addition of visual effects using
AR technology is superior in that it enables three-dimensional
visual expression and reduces complicated video editing work.
In a previous study, we developed a video editing system
for ﬂipped classrooms using AR technologies [3]. We designed
the agents playing the role of a virtual student in AR space,
called AR puppets, which interact with the lecturer and help
students learn. We used AR puppets and created lecture videos
together to add visual effects to the lecture videos. Saquib et al.
[4] developed a system that displays materials such as ﬁgures
and graphs in videos during a live presentation, with which the
presenter can interact in real time with gestures and postures.
Relevant to our system is to display the digital materials and
the presenter as if they were in real space. It allows the
audience to watch it as a single video content without having
to pay attention to both materials and the presenter.

©2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including
reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or
reuse of any copyrighted component of this work in other works. DOI: 10.1109/IIAI-AAI53430.2021.00011

 
 
 
 
 
 
(a) one AR Slide

(b) real lecture material (whiteboard)

(c) multi AR Slides

Fig. 1. Example of displayed lecture material by our system.

Many lecturers create lecture videos using standard pre-
sentation tools such as Microsoft PowerPointiand Keynoteii.
These tools make it easy to create a narrated presentation
or presentation with an overlaid lecturer’s capture. Our study
focuses on features of AR technology, which realize added
graphical expressions of presentation using three-dimensional
spatial information to video during lectures without any editing
work. Our system uses digital lecture contents for lectures,
such as lecture materials and agents. This can ﬂexibly adapt
to various lecture formats by manipulating and adjusting the
lecture recording contents (Fig. 1).

B. Human-like Virtual Agent

Human-like virtual agents, which have a form and commu-
nication similar to humans, have been researched and used in
various situations, as language instructors [5] and a museum
guide [6]. Particularly in a presentation situation, the presenter
makes a collaborative presentation with a virtual agent [7], and
projects an avatar that reﬂects the presenter’s behavior on the
slides [8]. Mumm et al. [9] investigated how the presence of
virtual agents can affect the motivation of participants toward
given tasks. It is inferred that abstracted humanoid agents
may have a positive effect on human motivation. We notably
focus on visual effects of virtual agents without the need for
manpower.

In this study, we use a human-like virtual agent as the
lecture assistant agent, who provides the graphical effect as
video content and lecture assistance for making the lecture
videos more comprehensible. In a previous study, we explored
ways to control a virtual agent in an AR space [10]. We
considered whether the performer could control the virtual
agent himself or herself without disturbing the progress of
performance. We extend this work further and design three-
dimensional control of lecture assistant agent. Our system
controls lecture assistant agent semi-autonomously using the
information in the lecture space, such as the lecturer’s position,
lecture materials, and lecture progress.

ihttps://www.microsoft.com/en-us/microsoft-365/powerpoint
iihttps://www.apple.com/jp/keynote/

III. LECTURE VIDEO MAKING SUPPORT USING AR
TECHNOLOGY

Our system realizes a collaborative presentation by a lec-
turer and lecture assistant agent in the AR space. We assume
that the lecturers record lecture videos by themselves, and the
length of typical videos is longer than 30 minutes. This section
describes in detail how we support the creation of lecture
videos using AR technology.

A. Design Lecture Material Object

Lecturer can use two types of lecture materials in our
system: AR Slide and real lecture materials. AR Slide is a
lecture material displayed as a digital object in the AR space
using AR technology. AR Slide can be used in image formats
such as PowerPoint, textbooks, and reference images, as well
as in video format, taking into account the lecture format
assumed by each lecturer. The real lecture materials in this
paper refer to the materials that the lecturer writes directly on
real objects such as blackboards and whiteboards. The lecturer
usually gives a lecture using this AR Slide.

the video later, and it

By using AR Slide, there is no need for a monitor or
projector to display the lecture materials, nor is there a need
to edit
is possible to display the
materials following the progress of the lecture. In addition,
it can be displayed clearly. A digitally displayed AR slide
can be displayed stably without being affected by the actual
recording environment. Moreover, AR technology has people
occlusion, which grasps the front-back relationship between
AR objects and people and renders AR contents appropriately.
This technology makes it easy to realize representations that
look like chroma key compositing of AR Slide and the lecturer.
The type of lecture videos that different lecturers want to
make varies. This may also vary depending on the ﬁeld and
content of the lecture. It would be a burden for the lecturer
to prepare for the lecture video that the lecturer assumes in
advance or in the middle of the recording. Our system enables
the lecturer to freely manipulate and adjust the AR Slide, and
corresponds to various lecture formats that change the display
style of AR Slide. The layout of the materials is limited with
a monitor or projector screen.

AR technology makes it possible to place the lecture ma-
terials anywhere in AR space. The lecturer can freely adjust
the size and aspect ratio of AR Slide.

There are mainly three display styles of AR Slide. Fig. 1
denotes the actual display styles of AR Slide by our system.
(a) is the most basic display style, showing a single slide. (b)
is the case when the lecturer uses real lecture material such
as a blackboard and whiteboard. The AR slide is moved to
the upper left corner, and the lecturer gives a lecture using
a real lecture material. (c) is a style in which multiple slides
are displayed in the video. Fig. 1c shows the main slide and
previous and next slides are displayed.

In another case, the lecturer can proceed with the lecture
displaying the left slide ﬁxedly on a particular slide (this func-
tion is called a Pin Function). The lecturer can switch these
styles appropriately during recording, allowing the lecturer to
ﬂexibly change depending on the content of the lecture.

B. Lecture Assistant Agent

Our study realizes an agent provides supplementary expla-
nations for lectures. Lecture assistant agent plays two roles.
One is to provide a visual effect to relieve the monotony of the
lecture video. The other is the role of assisting the lecture. We
are familiar with the format in news and commercials, where
the main speaker and assistant speaker are separated and give
a presentation together.

Because the lecturer records a lecture video by himself or
herself, the lecture assistant agent should be controlled semi-
autonomously. Proceeding with the lecture itself becomes a
burden to the lecturer.

Therefore, it is necessary to avoid that the control of the
lecture assistant agent becomes obstructive to the progress
of the lecture. In our system, lecture assistant agent semi-
autonomously controls its movements using comprehensive
information about lecture progress, lecture materials, the po-
sition of the lecturer and pointer, and so on.

We assumed that there would be two types of supplemen-
tary explanations needed in a lecture: explanations about the
lecture content and about the lecture progress. Supplementary
explanations about
the lecture content require appropriate
content and timing for the progress of the lecture. This is
because meaningless explanations can distract students from
understanding lecture video content. Because it is difﬁcult for
the agent to provide such explanations with semi-autonomous
control, this paper mainly focuses on supplementary explana-
tions about the lecture progress. Fig. 2 denotes lecture assistant
agent that prompts the viewer to pay attention when the lecture
progress transitions, such as switching slides or using a pointer.

C. Functions using Lecturer’s Recording Information

Our system handles two types of information as Lecturer’s
Recording Information: the lecturer’s position and the progress
of the lecture. On TV news, we can see that the camera
crews switch between the material, the newscaster, and the
guest according to the situation. It is possible that adjusting
recording contents according to the progress of the lecture

Fig. 2. Visual Effects by lecture assistant agent. The agent pop-up AR objects
in a text format chronologically above its head.

Fig. 3. Recording Content Switching Function. It is automatically controlled
camera using the lecturer’s position information. Above: Close-up display of
lecture materials. Below: Close-up display of the lecturer.

will make it easier for viewers to see and understand the
contents. However, it is desirable to avoid direct control of
the camera, which requires human work, and to make it as
automatic as possible. An example of the recording contents
switching function of this system is shown in Fig. 3. This
function provides a close-up view of the lecture materials
when the lecturer exits the camera screen (The above Fig.
3). In addition, our system controls the camera based on the
lecturer’s position and displays a close-up of the lecturer (Fig.
3 below).

Our system creates lecture videos with added visual effects
in real time to reduce the burden of video editing. However,
editing video after recording is necessary to some extent, such
as cut editing for retake parts. Therefore, we propose a system

Fig. 4. Video cut editing assistance function after lecture recording. Two
methods to save the retake point are used. Left: Insert a visual effect into the
video. Right: Save as a text log along with the operation record.

Fig. 6. Adjusting function of AR Slide.

set up between the two devices to create a communication
environment. In second step, each device is assigned to a
recording device or a remote operation device.

The lecturer prepares for the lecture recording in third step.
Speciﬁcally, the lecturer sets up lecturer’s position tracking and
AR Slide. The lecturer always holds a remote operation device
while recording because of the speciﬁcations of this system.
Therefore, our system estimates the lecturer’s position based
on the position coordinates of the remote operation device.
Therefore, it is necessary to acquire the position coordinates
of the remote operation device in the AR space created by the
recording device. This mechanism is implemented by a remote
operation device sharing its coordinates with a recording
device through self-localization estimation in AR technology.
However, the coordinate information provided by the self-
localization estimation is expressed in relative coordinates
from the origin of the AR space constructed by oneself. Hence,
it is essential to match the origin and axis (direction) of the
AR space that each device constructs.

Our system uses the overall spatial mapping information,
which is referred to as the AR map, and shares it among
other devices to align the origin and axis of the AR space.
Subsequently, it estimates the position of the remote operation
device and lecturer by self-localization estimation.

When setting up the AR slide, the system displays the
lecture materials prepared in advance as AR Slide. In addition,
the lecturer can adjust the position and size of the AR slide
(Fig. 6). Our system uses the plane detection of AR technology
to render the lecture materials as if they were pasted on a
vertical surface in the recording space.

After ﬁnishing preparation for recording, the lecturer can
give a lecture by manipulating the AR slide. The lecturer will
use the system functions according to the content and progress
of the lecture. In the previous section, we introduced some
system functions.

The other function of our system is to display a pointer on

the AR slide (Fig. 7).

B. System Architecture

Our system consists of a recording device, a remote opera-

tion device, and a database (Fig. 8).

Fig. 5. Hardware setup.

to record the retake point of the video using the information of
the lecture’s progress and to assist in editing the video after
recording. Fig. 4 denotes two recording methods: one is to
record by inserting a visual effect into the video, and the other
is to record by saving as text log along with the operation log.

IV. LECTURE VIDEO RECORDING SYSTEM

Our system, developed as iOS application with Unity, uses
two iOS devices. Fig. 5 denotes the hardware conﬁguration
of our system. These devices are called recording device
and remote operation device, respectively. Recording Device
creates lecture videos with added visual effects. Because the
lecturer is approximately several meters away from the record-
ing device, it is impossible to directly control the recording
device during the lecture. Therefore, the recording device, that
is, the recording system, was remotely controlled via a remote
operation device. We used iPhone 8 running on iOS14.4 as the
remote operation device and iPad Pro 2nd generation running
on iOS14.4 as the recording device.

A. System Flow

We describe the procedure for making a lecture video with
our system. The procedure consists of four steps mentioned
as follows: 1) launch the application, 2) register the recording
device and remote operation device, 3) prepare for recording:
track lecturer’s position and put AR slide with recording
device, and 4) record a lecture: conduct a lecture with a remote
operation device.

In ﬁrst step, the lecturer starts this application on each of the
two iOS devices. After some time, the network is automatically

to track the lecturer’s position. Speciﬁcally, one device sends
the AR map to the other device via the database. The lecture
materials for AR slide are also stored in the database. The
material is saved in the database as image or video format.
The lecturer can freely change and use the materials from a
web browser by referring to them from the database without
the need to import them into the device in advance.

Recording device mainly comprises recording information
management subsystem, lecture assistant agent, and AR slide.
The recording information management subsystem handles
the lecture progress and lecturer’s position information as
recording information. Since the lecturer uses AR Slide and
system functions to conduct a lecture, the progress of the
lecture is directly related to the system’s operation. Therefore,
the system determines the lecture progress based on the
operation information.

Lecture assistant agent

is controlled semi-autonomously.
Recording information management subsystem commands
the agent to perform lecture assistance considering lecture
progress. The system controls the gaze and posture of the
lecture assistant agent in three dimensions, depending on the
position of the lecturer and AR Slide. AR Slide is displayed
by projecting material data onto a ﬂat AR image object. AR
Slide has a layered structure of multiple AR objects. The
system controls the AR Slide, such as page manipulation and
changing display style, by a combination of showing and
hiding these objects and adjusting their position and size.
Finally, the system makes AR video as a lecture video in real
time by combining digital data of AR contents and camera
images using AR technology.

V. EVALUATION

In this section, we evaluate the functionality and perfor-
mance of our system, as a recording system using AR technol-
ogy. We conducted experiments to demonstrate that our system
can add appropriate visual effects in real time in accordance
with the lecturer’s operation. Speciﬁcally, we measured their
accuracy and process speed. We evaluated that the system
can operate exactly as the user intended while recording a
lecture video. The system should perform appropriate editing
in real time for the lecturer’s operations. In other words, the
system can select appropriate commands for the lecturer’s
operations. Therefore, we measured the elapsed time as the
response speed of the system, which is from the issue of
command by the remote operation device to the completion
of that system function. We selected four functions, which
are required to execute in real time. We measured 100 times
for each function and calculated the average and maximum
values. Table I shows that the accuracy is 100% and response
time is low. The average and maximum response time for the
Pin Function is 74 ms and 302 ms, respectively, which is the
slowest among the measured functions. The results indicate
that our system has sufﬁcient real-time performance for adding
visual effects during lecture recording.

Our system allows the use of the information in the
recording space, such as the lecturer’s standing position. As

Fig. 7. Pointer function. The remote operation device displays a 2D image
of a material synchronized with an AR slide that exists in the AR space of the
recording device. A pointer is displayed in relation to the touched position of
the material displayed on the remote operation device.

Fig. 8. System Architecture.

The remote operation device has a communication control
module that is responsible for the communication function
between devices.

The lecturer remotely controls the recording device via the
communication control module based on the input from the
remote operation device. This module also updates the UI of
the remote operation device by receiving the system informa-
tion from the recording device, such as lecture information
and results of operation commands, so that the lecturer can
directly receive the system feedback.

The database holds two types of data: AR map and lecture

material.

The system needs to share an AR map with each device

explained in Section IV, the accuracy of the lecturer’s position
recognized by the system depends on the position tracking
accuracy of the remote operation device. Therefore, we mea-
sured the accuracy of the position tracking of remote operation
device. We describe the measurement procedure. First, the
system places a target object randomly in the AR space of the
recording device. Second, the user overlaps remote operation
device on the target object. Finally, the system measures the
distance between the position of the target object and the
tracked position of the remote operation device by the system.
Because there might be some hand tremors and misrecog-
nition,
the coordinates by position tracking are calculated
as a ﬁve-second moving average value. We conducted the
experiments in two cases. The results are shown in Table II.
Case 1 involves measuring in various spaces within a short
period after setting up tracking. Case 2 involved setting up
tracking only once and measuring it in the same space for 30
min.

system perspective, tracking accuracy is sufﬁcient, even to a
certain degree, rather than a strict accuracy. Therefore, the
performance of the system is practical enough.

We assume that a lecturer records long lecture videos
by using our system, so the system is required to execute
appropriate commands for the lecturer’s operations in real-
time in order to reduce time-consuming editing work after
long recording. Because of the speciﬁcation of AR technology,
the extraction of spatial feature points is always executed as
long as the system is running. Therefore, it is assumed that the
accuracy will decrease as the system is working for a long time
because of the speciﬁcation of position tracking. However, we
did not observe any loss in accuracy (Case 2). The variance
was smaller than that observed in Case 1. The results indicated
that the newly acquired spatial feature points were processed
appropriately without conﬂicting with the old. We conclude
that the tracking implementation of our system is practical for
long-term lecture recording.

TABLE I
RESPONSE TIME AND ACCURACY

Page Operation
Display Format Change
Pointer
Pin Function

max
[ms]
189
162
177
302

mean
[ms]
61
53
69
74

accuracy
[%]
100
100
100
100

VII. CONCLUSION

We proposed a lecture video recording system providing AR
space for giving lectures including AR Slide, Lecture Assistant
Agent, and so on. Our experimental results demonstrated that
the system can add visual effects to lecture videos on recording
without unwanted editing work caused by system errors or
delays. Our system can help lecturers create lecture videos
with visual effects to help students. However, we need to
conduct usability tests of our system in the near future.

TABLE II
POSITION ERROR OF RECOGNIZED REMOTE OPERATION DEVICE

ACKNOWLEDGMENT

count

100
54

min
[cm]
2.5
3

max
[cm]
35.5
35.6

mean
[cm]
16
12.7

variance

60.7
44.7

Case 1
Case 2

VI. DISCUSSION

Our system provides an environment for recording lecture
videos in AR space. As shown in Table I, to reduce the burden
of video editing, the system adds visual effects to the video
in real time. Our experimental results show that the system
can add appropriate visual effects for the lecturer’s operations;
therefore, the system does not cause unwanted editing work
after recording due to system errors or delays. Our system
can help lecturers create lecture videos because the system
can add visual effects to lecture videos on recording without
time-consuming editing work. Furthermore, the system is also
expected to reduce the additional work, such as retaking and
video editing after recording due to the addition of visual
effects beyond the lecturer’s expectations.

The feature of our system is the acquisition of spatial
information using AR technology, and the utilization of the
same to create visual effects and functions. The accuracy of
the acquired spatial information affects the functionality of
the entire system. In Case 1, the average error of the position
tracking of the remote operation device was 16cm. From a

This work was supported in part by JSPS KAKENHI Grant

Numbers 19K12097,19K12266.

REFERENCES

[1] H. Okumoto, M. Yoshida, K. Umemura, Y. Ichikawa, “Response Col-
lector: A Video Learning System for Flipped Classrooms” 2018 5th
International Conference on Advanced Informatics: Concept Theory and
Applications (ICAICTA), pp.176-181, 2018.

[2] P. J. Guo, J. Kim, R. Rubin, “How Video Production Affects Student
Engagement: An Empirical Study of MOOC Videos” ACM conference
on Learning at Scale, pp.41-50, 2014.

[3] H. Kataoka, T. Ozono, T. Shintani, “Realizing a Video Editing System
for Flipped Classrooms with AR Puppets” IEICE Technical Report,
Vol.119, No.317, AI2019-32, pp.13-18, 2019.

[4] N. Saquib, R. H. Kazi, L.-Y. Wei, W. Li, “Interactive Body-Driven
Graphics for Augmented Video Performance” CHI 2019, ACM, No.622,
12p, 2019.

[5] M. Macedonia, I. Groher, F. Roithmayr, “Intelligent virtual agents as
language trainers facilitate multilingualism” Frontiers in Psychology,
14;5:295, 5p, eCollection 2014.

[6] N. Tcchasarntikul, P. Ratsamee, J. Orlosky, T. Mashita, Y. Uranishi, K.
Kiyokawa, H. Takemura, “The Effect of the Presence of an Embodied
Agent in an AR Guiding System” Transactions of the Virtual Reality
Society of Japan, Vol.25, No.1, pp.68-77, 2020.

[7] H. Trinh, L. Ring, T. Bickmore, “DynamicDuo: Co-presenting with Vir-
tual Agents” CHI ’15: Proceedings of the 33rd Annual ACM Conference
on Human Factors in Computing Systems, pp.1739-1748, 2015.

[8] F. Matulic, L. Engein, C. Tr¨ager, R. Dachselt, “Embodied Interactions
for Novel Immersive Presentational Experiences” Conference: the 2016
CHI Conference Extended Abstracts, pp.1713-1720, 2016.

[9] J. Mumm, B. Mutlu, “Designing motivational agents: The role of praise,
social comparison, and embodiment in computer feedback!” Computers
in Human Behavior Volume 27, Issue 5, pp.1643-1650, 2011.

[10] H. Kataoka, T. Ozono, T. Shintani, “Developing an AR Pop-up Picture
Book and its Effect Editor Based on Teaching Motions” Information
Engineering Express, International Institute of Applied Informatics,
vol.7, no.1, pp.1-10, 2021.

