2
2
0
2

l
u
J

8
2

]

Y
S
.
s
s
e
e
[

4
v
7
6
8
3
1
.
6
0
1
2
:
v
i
X
r
a

POLAR: A Polynomial Arithmetic
Framework for Verifying
Neural-Network Controlled Systems

Chao Huang1, Jiameng Fan2, Xin Chen3, Wenchao Li2, and Qi Zhu4

1 University of Liverpool, chao.huang2@liverpool.ac.uk
2 Boston University, {jmfan, wenchao}@bu.edu
3 University of Dayton, xchen4@udayton.edu
4 Northwestern University, qzhu@northwestern.edu

Abstract. We present POLAR5, a polynomial arithmetic-based frame-
work for eﬃcient bounded-time reachability analysis of neural-network
controlled systems (NNCSs). Existing approaches that leverage the stan-
dard Taylor Model (TM) arithmetic for approximating the neural-network
controller cannot deal with non-diﬀerentiable activation functions and
suﬀer from rapid explosion of the remainder when propagating the TMs.
POLAR overcomes these shortcomings by integrating TM arithmetic
with Bernstein polynomial interpolation and symbolic remain-
ders. The former enables TM propagation across non-diﬀerentiable ac-
tivation functions and local reﬁnement of TMs, and the latter reduces
error accumulation in the TM remainder for linear mappings in the net-
work. Experimental results show that POLAR signiﬁcantly outperforms
the current state-of-the-art tools in terms of both eﬃciency and tightness
of the reachable set overapproximation.

1

Introduction

Neural networks have been increasingly used as the central decision makers in a
variety of control tasks [23,25,18]. However, the use of neural-network controllers
also gives rise to new challenges on verifying the correctness of the resulting
closed-loop control systems especially in safety-critical settings. In this paper,
we consider the reachability veriﬁcation problem of neural-network controlled
systems (NNCSs). The high-level architecture of a simple NNCS is shown in
Figure 1 in which the neural network senses the system state, i.e. the value
of (cid:126)x, at discrete time steps, and computes the corresponding control values (cid:126)u
for updating the system dynamics which is deﬁned by an ordinary diﬀerential
equation (ODE) over (cid:126)x and (cid:126)u. The bounded-time reachability analysis problem
of an NNCS is to compute an (overapproximated) reachable set that contains all
the trajectories starting from an initial set for a ﬁnite number of control steps.
The initial set can represent uncertainties in the starting state of the system
or error (e.g. localization error) bounds in estimating the current system state

5 The source code can be found at https://github.com/ChaoHuang2018/POLAR_Tool.

 
 
 
 
 
 
2

Chao Huang, Jiameng Fan, Xin Chen, Wenchao Li, and Qi Zhu

Fig. 1: A typical NNCS
model.

Fig. 2: Executions over 4 control steps.

during an execution of the system. Figure 2 shows an illustration of reachable
sets for 4 steps, where the orange region represents the reachable set, and the
two red, arrowed curves are two example trajectories starting from two diﬀerent
initial states in the initial set X0 (blue).

Reachability analysis of general NNCSs is notoriously diﬃcult due to non-
linearity in both the neural-network controller and the plant. The diﬃculty is
further exacerbated by the coupling of the controller and the plant over multi-
ple control steps. Since exact reachability of general nonlinear systems is unde-
cidable [2], current approaches for reachability analysis of nonlinear dynamical
systems largely focus on computing a tight overapproximation of the reachable
sets [20,26,10,6,1]. Verisig [15] leverages properties of the sigmoid activation func-
tion and converts an NNCS with these activation functions to an equivalent hy-
brid system. Thus, existing tools for hybrid system reachability analysis can be
directly applied to solve the NNCS reachability problem. However, this approach
inherits the eﬃciency problem of hybrid system reachability analysis and does
not scale beyond very small NNCSs. Another line of approach is to draw on tech-
niques for computing the output ranges of neural networks [12,17,30,31,32,28]
by directly integrating them with reachability analysis tools designed for dy-
namical systems. NNV [29], for instance, combines star set analysis on the neu-
ral network with zonotope-based analysis of the nonlinear plant dynamics from
CORA [1]. However, this type of approach has been shown to be ineﬀective for
NNCS veriﬁcation due to the lack of consideration on the interaction between
the neural-network controller and the plant dynamics [8,11,13]. In particular,
since the primary goal of these techniques is to bound the output range of the
neural network instead of approximating its input-output function, they cannot
track state dependencies across the closed-loop system and across multiple time
steps in reachability analysis.

More recent advances in NNCS reachability analysis are based on the idea of
function overapproximation of the neural network controller. A function overap-
proximation of a neural network κ has two components: an approximated func-
tion p and an error term I (e.g. an interval) that bounds the approximation error.
Such function overapproximation that produces a point-wise approximation of
κ with an interval error term (typically called a remainder) is also known as a
Taylor model (TM). Function-overapproximation approaches can be broadly cat-
egorized into two classes: direct end-to-end approximation such as Sherlock [8],

x(t)δc2δc3δc4δc0X0·x=f(x,u0),u0=κ(x(0))·x=f(x,u3),u3=κ(x(3δc))·x=f(x,u1),u1=κ(x(δc))·x=f(x,u2),u2=κ(x(2δc))POLAR

3

ReachNN [11] and ReachNN* [9], and layer-by-layer propagation such as Verisig
2.0 [14,13]. The former computes a function overapproximation of the neural net-
work end-to-end by sampling from the input space. The main drawback of this
approach is that it does not scale beyond systems with more than a few input
dimensions. The latter approach tries to exploit the neural network structure
and uses Taylor model arithmetic to more eﬃciently obtain a function overap-
proximation of κ by propagating the TMs layer by layer through the network
(details in Section 3). However, due to limitations of basic TM arithmetic, these
approaches cannot handle non-diﬀerentiable activation functions and suﬀer from
rapid growth of the remainder during propagation. For instance, explosion of the
interval remainder would degrade a TM propagation to an interval analysis.

In this paper, we propose a principled polynomial arithmetic framework
(POLAR) that enables precise layer-by-layer propagation of TMs for general
feed-forward neural networks. Basic Taylor model arithmetic cannot handle
ReLU that is nondiﬀerentiable (cannot produce the polynomial), and also suf-
fers from low approximation precision (large remainder). POLAR addresses the
key challenges of applying basic TM arithmetic through a novel use of uni-
variate Bernstein polynomial interpolation and symbolic remainders. Univariate
Bernstein polynomial interpolation enables the handling of non-diﬀerentiable ac-
tivation functions and local reﬁnement of Taylor models (details in Section 3.1).
Symbolic remainders can taper the growth of interval remainders by avoiding the
so-called wrapping eﬀect [16] in linear mappings. The paper has the following
novel contributions: (I) A polynomial arithmetic framework using both Taylor
and univariate Bernstein approximations for computing NNCS reachable sets
to handle general NN controllers; (II) An adaptation of the symbolic remainder
method for ODEs to the layer-by-layer propagation for neural networks; (III)
A comprehensive experimental evaluation of our approach on challenging case
studies that demonstrates signiﬁcant improvements of POLAR against SOTA.

2 Preliminaries

A Neural-Network Controlled System (NNCS) is a continuous plant governed by
a neural network controller. The plant dynamics is deﬁned by an ODE of the
form ˙(cid:126)x = f ((cid:126)x, (cid:126)u) wherein the state variables and control inputs are denoted by
the vectors (cid:126)x and (cid:126)u respectively. We assume that the function f is at least locally
Lipschitz continuous such that its solution w.r.t. an initial state and constant
control inputs is unique [22]. We denote the input-output mapping of the neural
network controller as κ. The controller is triggered every δc time which is also
called the control stepsize. A system execution (trajectory) is produced in the
following way: starting from an initial state (cid:126)x(0), the controller senses the system
state at the beginning of every control step t=jδc for j=0, 1, . . ., and updates
the control inputs to (cid:126)vj=κ((cid:126)x(jδc)). The system’s dynamics in that control step
is governed by the ODE ˙(cid:126)x=f ((cid:126)x, (cid:126)vj).

Given an initial state set X0 ⊂ Rn, all executions from a state in this set
can be formally deﬁned by a ﬂowmap function ϕN : X0 × R≥0 → Rn, such that

4

Chao Huang, Jiameng Fan, Xin Chen, Wenchao Li, and Qi Zhu

the system state at any time t ≥ 0 from any initial state (cid:126)x0 ∈ X0 is ϕN ((cid:126)x0, t).
We call a state (cid:126)x(cid:48) ∈ Rn reachable if there exists (cid:126)x0 ∈ X0 and t ≥ 0 such that
(cid:126)x(cid:48) = ϕN ((cid:126)x0, t). The reachability problem on NNCS is to decide whether a state is
reachable in a given NNCS, and it is undecidable since NNCS is more expressive
than two-counter machines for which the reachability problem is already unde-
cidable [2]. Many formal veriﬁcation problems can be reduced to the reachability
problem. For example, the safety veriﬁcation problem can be reduced to checking
reachability to an unsafe state. In the paper, we focus on computing the reach-
able set for an NNCS over a bounded number K of control steps. Since ﬂowmap
ϕN often does not have a closed form due to the nonlinear ODEs, we seek to
compute state-wise overapproximations for it over multiple time segments, that
is, in each control step [jδc, (j + 1)δc] for j = 0, . . . , K − 1, the reachable set is
overapproximated by a group of ﬂowpipes F1((cid:126)x0, τ ), . . . , FN ((cid:126)x0, τ ) over the N
uniformly subdivided time segments of the time interval, such that Fi((cid:126)x0, τ ) is a
state-wise overapproximation of ϕN ((cid:126)x0, jδc + (i − 1)δ + τ ) for τ ∈ [0, δc/N ], i.e.,
Fj((cid:126)x0, τ ) contains the exact reachable state from any initial state (cid:126)x0 in the i-th
time segment of the j-th control step. Here, τ is the local time variable which
is independent in each ﬂowpipe. A high-level ﬂowpipe construction algorithm is
presented as follows, in which ˆX0 = X0 and δ = δc/N is called the time step.
1: for j = 0 to K − 1 do
2:
3:

Computing an overapproximation ˆUj for the control input range κ( ˆXj);
Computing the ﬂowpipes F1((cid:126)x0, τ ), . . . , FN ((cid:126)x0, τ ) for the continuous dy-
namics ˙(cid:126)x = f ((cid:126)x, (cid:126)u), ˙(cid:126)u = 0 from the initial set (cid:126)x(0) ∈ ˆXj, (cid:126)u(0) ∈ ˆUj;

4: R ← R ∪ {(F1((cid:126)x0, τ ), . . . , FN ((cid:126)x0, τ )};
5:
6: end for

ˆXj+1 ← FN ((cid:126)z, δ);

Notice that (cid:126)x(0) denotes the local initial set for the ODE used in the current
control step, that is the system reachable set at the time jδc, while the variables
(cid:126)x0 in a ﬂowpipe are the symbolic representation of an initial state in X0. Intu-
itively, a ﬂowpipe overapproximates not only the reachable set in a time step,
but also the dependency from an initial state to its reachable state at a particular
time. For settings where the plant dynamics of an NNCS is given as a diﬀerence
equation in the form of (cid:126)xk+1 = f ((cid:126)xk, (cid:126)uk), we can obtain discrete ﬂowpipes which
are the reachable set overapproximations at discrete time points by repeatedly
computing the state set at the next step using TM arithmetic.
Dependencies on the initial set. As we mentioned previously, the reachable
state of an NNCS at a time t > 0 is uniquely determined by its initial state if there
is no noise or disturbance in the system dynamics or on the state measurements.
If we use Xj to denote the exact reachable set {ϕN ((cid:126)x0, jδc) | (cid:126)x0 ∈ X0} from
a given initial set X0, then the control input range is deﬁned by the set Uj =
{κ((cid:126)xj) | (cid:126)xj = ϕN ((cid:126)x0, jδc) and (cid:126)x0 ∈ X0}. More intuitively, the set Uj is the image
from the initial set X0 under the mapping κ(ϕN (·, jδc)). The main challenge
in computing NNCS reachable sets is to control the overapproximation, which
requires accurately tracking the dependency of a reachable set on the initial set

POLAR

5

across multiple control steps. In this paper, we present a polynomial arithmetic
framework for tracking such dependencies using Taylor models.
Taylor model arithmetic. Taylor models are originally proposed to compute
higher-order overapproximations for the ranges of continuous functions (see [4]).
They can be viewed as a higher-order extension of intervals [24], which are
sets of real numbers between lower and upper real bounds, e.g., the interval
[a, b] wherein a ≤ b represents the set of {x | a ≤ x ≤ b}. A Taylor model
(TM) is a pair (p, I) wherein p is a polynomial of degree k over a ﬁnite group
of variables x1, . . . , xn ranging in an interval domain D ⊂ Rn, and I is the
remainder interval. The range of a TM is the Minkowski sum of the range of
its polynomial and the remainder interval. Thereby we sometimes intuitively
denote a TM (p, I) by p + I in the paper. TMs are closed under operations
such as addition, multiplication, and integration (see [21]). Given functions f, g
that are overapproximated by TMs (pf , If ) and (pg, Ig), respectively, a TM for
f + g can be computed as (pf + pg, If + Ig), and an order k TM for f · g can be
computed as ( pf · pg − rk , If · B(pg) + B(pf ) · Ig + If ·Ig + B(rk) ), wherein B(p)
denotes an interval enclosure of the range of p, and the truncated part rk consists
of the terms in pf · pg of degrees > k. Similar to reals and intervals, TMs can also
be organized as vectors and matrices to overapproximate the functions whose
ranges are multidimensional. Notice that a TM is a function overapproximation
and not just a range overapproximation like intervals or polyhedra.

3 Framework of POLAR

In this section, we describe POLAR’s approach for computing a TM for the out-
put range of a neural network (NN) when the input range is deﬁned by a TM.
POLAR uses the layer-by-layer propagation strategy, and features the following
key novelties: (a) A method to compute univariate Bernstein Polynomial (BP)
overapproximations for activation functions, and selectively uses Taylor or Bern-
stein polynomials to limit the overestimation produced when overapproximating
the output ranges of individual neurons. (b) A technique to symbolically repre-
sent the intermediate linear transformations of TM interval remainders during
the layer-by-layer propagation. The purpose of using Symbolic Remainders (SR)
is to reduce the accumulation of overestimation in composing a sequence of TMs.

3.1 Main Framework

We begin by introducing POLAR’s propagation framework that incorporates
only (a), and then describe how to extend it by further integrating (b). Al-
though using TMs to represent sets in layer-by-layer propagation is already used
in [14,13], the method only computes Taylor approximations for activation func-
tions, and the TM output of one layer is propagated by the existing arithmetic
for TM composition to the next layer. Such a method has the following short-
comings: (1) the activation functions have to be diﬀerentiable, (2) standard TM
composition is often the source of overestimation even if preconditioning and

6

Chao Huang, Jiameng Fan, Xin Chen, Wenchao Li, and Qi Zhu

Algorithm 1 Layer-by-layer propagation using polynomial arithmetic and TMs

Input: Input TM (p1((cid:126)x0), I1) with (cid:126)x0 ∈ X0, the M + 1 matrices W1, . . . , WM +1 of
the weights on the incoming edges of the hidden and the output layers, the M + 1
vectors B1, . . . , BM +1 of the neurons’ bias in the hidden and the output layers, the
M + 1 activation functions σ1, . . . , σM +1 of hidden and output layers.

Output: a TM (pr((cid:126)x0), Ir) that contains the set κ((p1((cid:126)x0), I1)).
1: (pr, Ir) ← (p1, I1);
2: for i = 1 to M + 1 do
3:
4:
5:
6:
7: end for
8: return (pr, Ir).

(pt, It) ← Wi · (pr, Ir) + Bi;
Computing a polynomial approximation pσ,i for σ w.r.t. the domain (pt, It);
Evaluating a conservative remainder Iσ,i for pσ,i w.r.t. the domain (pt, It);
(pr, Ir) ← pσ,i(pt + It) + Iσ,i;

# Using TM arithmetic

# Using TM arithmetic

shrink wrapping are used. Here, we seek to improve the use of TMs in the above
two aspects.

Before presenting our layer-by-layer prop-
agation method, we describe how a TM out-
put is computed from a given TM input for a
single layer. The idea is illustrated in Fig. 3.
The circles in the right column denote the
neurons in the current layer which is the i-
th layer, and those in the left column de-
notes the neurons in the previous layer. The
weights on the incoming edges to the current
layer is organized as a matrix Wi, while we
use Bi to denote the vector organization of
the biases in the current layer. Given that
the output range of the neurons in the pre-
vious layer is represented as a TM (vector)
(pi((cid:126)x0), Ii) wherein (cid:126)x0 are the variables ranging in the NNCS initial set. Then,
the output TM (pi+1((cid:126)x0), Ii+1) of the current layer can be obtained as fol-
lows. First, we compute the polynomial approximations pσ1,i, . . . , pσl,i for the
activation functions σ1, . . . , σl of the neurons in the current layer. Second, in-
terval remainders Iσ1,i, . . . , Iσl,i are evaluated for those polynomials to ensure
that for each j = 1, . . . , l, (pσj ,i, Iσj ,i) is a TM of the activation function
σj w.r.t. zj ranging in the j-th dimension of the set Wi(pi((cid:126)x0) + Ii). Third,
(pi+1((cid:126)x0, Ii+1)) is computed as the TM composition pσ,i(Wi(pi((cid:126)x0) + Ii) + Iσ,i
wherein pσ,i((cid:126)z) = (pσ1,i(z1), . . . , pσl,i(zk))T and Iσ,i = (Iσ1,i, . . . , Iσl,i)T . Hence,
when there are multiple layers, starting from the ﬁrst layer, the output TM of
a layer is treated as the input TM of the next layer, and the ﬁnal output TM is
computed by composing TMs layer-by-layer.

Fig. 3: Single layer propagation

POLAR

7

We give the whole procedure by Algorithm 1. In our approach, the polynomial
approximation pσ,i and its remainder interval Iσ,i for the vector of activation
functions σ in the i-th layer can be computed in the following two ways.
Taylor approximation. When the activation function is diﬀerentiable in the
range deﬁned by (pt, It). The polynomial pσ,i can be computed as the order
k Taylor expansion of σ (in each of its dimension) at the center of (pt, It),
and the remainder is evaluated using interval arithmetic based on the Lagrange
remainder form. More details are described elsewhere [21].
Bernstein interpolation. The use of Bernstein approximation only requires
the activation function to be continuous in (pt, It), and can be used not only
in more general situations, but also to obtain better polynomial approximations
than Taylor expansions (see [19]). Intuitively, an order k Taylor approximation
can only guarantee to have the same value as the approximated function at the
expansion point, however, an order k Bernstein interpolation has the same value
as the approximated function at k+1 points. We give the details of our Bernstein
overapproximation method as follows.
Bernstein approximation for σ((cid:126)z) w.r.t. (cid:126)z ∈ (pt, It). Given (pt, It) computed in
Line 3, the j-th component of the polynomial vector pσ,i is the order k Bernstein
interpolation of the activation function σj of the j-th neuron. It can be computed
as pσj ,i(zj)= (cid:80)k
, such that ¯Zj and
s
Z j denote the upper and lower bounds respectively of the range in the j-th
dimension of (pt, It), and they can be obtained by interval evaluating TM.
Evaluating the remainder Iσ,i. The j-th component Iσj ,i of Iσ,i is computed as
a conservative remainder for the polynomial pσj ,i, and it can be obtained as a
symmetric interval [−(cid:15)j, (cid:15)j] such that

(cid:1) (Zj −Zj )s( ¯Zj −zj )k−s
( ¯Zj −Zj )k

s + Z j)(cid:0)k

¯Zj −Zj
k

σj(

s=0

(cid:18)

(cid:19)

(cid:15)j= max

s=1,···,m

(cid:32)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

pσj ,i(

Z j−Z j
m

(s−

1
2

)+Z j)−σj(

Z j − Z j
m

(s −

(cid:12)
(cid:12)
(cid:12)
)+Z j)
(cid:12)
(cid:12)

1
2

(cid:33)

+Lj·

Z j−Z j
m

wherein Lj is a Lipschitz constant of σj with the domain (pt, It), and m is the
number of samples that are uniformly selected to estimate the remainder. The
soundness of the error bound estimation above has been proven in [11] for mul-
tivariate Bernstein polynomials. Since univariate Bernstein polynomials, which
we use in this paper, is a special case of multivariate Bernstein polynomials, our
approach is also sound. A detailed proof is given in the appendix.

The following theorem states that a TM ﬂowpipe computed by our approach
is not only a range overapproximation of a reachable set segment, but also a
function overapproximation for the dependency of a reachable state on its initial
state. The proof is given in the appendix.

Theorem 1. If F((cid:126)x0, τ ) is the i-th TM ﬂowpipe computed in the j-st control
step, then for any initial state (cid:126)x0 ∈ X0, the box F((cid:126)x0, τ ) contains the actual
reachable state ϕN ((cid:126)x0, (j − 1)δc + (i − 1)δ + τ ) for all τ ∈ [0, δ].

8

Chao Huang, Jiameng Fan, Xin Chen, Wenchao Li, and Qi Zhu

3.2 Selection of Polynomial Approximations

Since an activation function is univariate, both of its Taylor and Bernstein ap-
proximations have a size which is linear in the order k. Then we investigate the
accuracy produced by both approximation forms. Since the main operation in
the TM layer-by-layer propagation framework is the composition of TMs, we
study the preservation of accuracy for both of the forms under the composition
with a given TM. We ﬁrst deﬁne the Accuracy Preservation Problem.

When a function f ((cid:126)x) is overapproximated by a TM (p((cid:126)x), I) w.r.t. a bounded
domain D, the approximation quality, i.e., size of the overestimation, is directly
reﬂected by the width of I, since f ((cid:126)x) = p((cid:126)x) for all (cid:126)x ∈ D when I is zero by
the TM deﬁnition. Given two order k TMs (p1((cid:126)x), I1) and (p2((cid:126)x), I2) which are
overapproximations of the same function f ((cid:126)x) w.r.t. a bounded domain D ⊂ Rn,
we use (p1((cid:126)x), I1) ≺k (p2((cid:126)x), I2) to denote that the width of I1 is smaller than
the width of I2 in all dimensions, i.e., (p1((cid:126)x), I1) is a more accurate overapprox-
imation of f ((cid:126)x) than (p2((cid:126)x), I2).
Accuracy Preservation Problem. If both (p1((cid:126)x), I1) and (p2((cid:126)x), I2) are over-
approximations of f ((cid:126)x) with (cid:126)x ∈ D, and (p1((cid:126)x), I1) ≺k (p2((cid:126)x), I2). Given another
function g((cid:126)y) which is already overapproximated by a TM (q((cid:126)y), J) whose range
is contained in D. Then, does p1(q((cid:126)y) + J) + I1 ≺k p2(q((cid:126)y) + J) + I2 still hold
using order k TM arithmetic?

We give the following counterexample to show that the answer is no, i.e.,
although (p1((cid:126)x), I1) is more accurate than (p2((cid:126)x), I2), the composition p1(q((cid:126)y) +
J) + I1 might not be a better order k overapproximation than p2(q((cid:126)y) + J) + I2
for the composite function f ◦ g. Given p1 = 0.5 + 0.25x − 0.02083x3, I1 = [-
7.93e-5, 1.92e-4], and p2 = 0.5 + 0.24855x − 0.004583x3, I2 = [-2.42e-4, 2.42e-4],
which are both TM overapproximations for the sigmoid function f (x) = 1
1+e−x
w.r.t. x ∈ q(y) + J such that q = 0.1y − 0.1y2, J = [−0.1, 0.1], and y ∈ [−1, 1].
We have that (p1, I1) ≺3 (p2, I2), however after the compositions using order 3
TM arithmetic, the remainder of p1(q(y) + J) + I1 is [−0.0466, 0.0477], while
the remainder of p2(q(y) + J) + I2 is [−0.0253, 0.0253], and we do not have
(p1(q(y) + J) + I1) ≺3 (p1(q(y) + J) + I1).

Since the accuracy is not preserved under composition, we do not decide
which approximation to choose directly based on the their remainders. Instead,
we integrate an additional step in Algorithm 1 to replace line 4-6: in each iter-
ation, both of Taylor and Bernstein overapproixmations are computed for each
of the activation functions, and we choose the one that produces the smaller
remainder interval Ir after composition.

3.3 Symbolic Remainders in Layer-by-Layer Propagation

We describe the use of symbolic remainders (SR) in the layer-by-layer propaga-
tion of computing an NN output TM. The method was originally proposed in [7]
for reducing the overestimation of TM ﬂowpipes in the reachability computation
for nonlinear ODEs, we adapt it particularly for reducing the error accumulation

Algorithm 2 TM output computation using symbolic remainders, input and
output are the same as those in Algorithm 1

POLAR

9

1: Setting Q as an empty array which can keep M + 1 matrices;
2: Setting J as an empty array which can keep M + 1 multidimensional intervals;
3: J ← 0;
4: for i = 1 to M + 1 do
5:

Computing the composite function pσ,i and the remainder interval Iσ,i using the
BP technique;
Evaluating qi((cid:126)x0) + Ji based on J and Q[1]I1;
J ← Ji;
Φi = QiWi;
for j = 1 to i − 1 do
Q[j] ← Φi · Q[j];

# Q[1]I1 = I1 when i = 1

end for
Adding Φi to Q as the last element;
for j = 2 to i do

6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17: end for
18: Computing an interval enclosure Ir for J + Q[1]I1;
19: return qM +1((cid:126)x0) + Ir.

end for
Adding Ji to J as the last element;

J ← J + Q[j] · J [j − 1];

# interval evaluation

in the TM remainders during the layer-by-layer propagation. Unlike the BP tech-
nique whose purpose is to obtain tighter TMs for activation functions, the use
of SR only aims at reducing the overestimation accumulation in the composition
of a sequence of TMs each of which represents the input range of a layer.

Consider the TM composition for computing the output TM of a single layer
in Fig. 3, the output TM pσ,i(Wi(pi((cid:126)x0) + Ii) + Bi) + Iσ,i equals to QiWipi((cid:126)x0) +
QiWiIi +QiBi +pR
σ,i(Wi(pi((cid:126)x0)+Ii)+Bi)+Iσ,i such that Qi is the matrix of the
linear coeﬃcients in pσ,i, and pR
σ,i consists of the terms in pσ,i of the degrees (cid:54)= 1.
Therefore, the remainder Ii in the second term can be kept symbolically such
that we do not compute QiWiIi out as an interval but keep its transformation
matrix QiWi to the subsequent layers. Given the image S of an interval under a
linear mapping, we use S to denote that it is kept symbolically, i.e., we keep the
interval along with the transformation matrix, and S to denote that the image
is evaluated as an interval.

Then we present the use of SR in layer-by-layer propagation. Starting from
the NN input TM (p1((cid:126)x0), I1), the output TM of the ﬁrst layer is computed as

Q1W1p1((cid:126)x0) + Q1B1 + pR
(cid:124)

σ,1(W1(p1((cid:126)x0) + I1) + B1) + Iσ,1
(cid:125)

(cid:123)(cid:122)
q1((cid:126)x0)+J1

+Q1W1I1

10

Chao Huang, Jiameng Fan, Xin Chen, Wenchao Li, and Qi Zhu

which can be kept in the form of q1((cid:126)x0) + J1 + Q1W1I1. Using it as the input
TM of the second layer, we have the following TM

pσ,2(W2(q1((cid:126)x0) + J1 + Q1W1I1) + B2) + Iσ,2

= Q2W2q1((cid:126)x0) + Q2B2 + pR

(cid:124)

σ,2(W2(q1((cid:126)x0) + J1 + Q1W1I1) + B2) + Iσ,2
(cid:125)

(cid:123)(cid:122)
q2((cid:126)x0)+J2

+ Q2W2J1 + Q2W2Q1W1I1

for the output range of the second layer. Therefore the output TM of the i-th
layer can be obtained as qi((cid:126)x0) + Ji + QiWi · · · Q1W1I1 such that Ji = Ji +
QiWiJi−1 + QiWiQi−1Wi−1Ji−2 + · · · + QiWi · · · Q2W2J1.

We present the SR method by Algorithm 2 in which we use two lists: Q[j]
for QiWi · · · · · QjWj and J [j] for Jj to keep the intervals and their linear trans-
formations. The symbolic remainder representation is replaced by its interval
enclosure Ir at the end of the algorithm.
Time and space complexity. Although Algorithm 2 produces TMs with
tighter remainders than Algorithm 1 because of the symbolic interval representa-
tions under linear mappings, it requires (1) two extra arrays to keep the interme-
diate matrices and remainder intervals, (2) two extra inner loops which perform
i−1 and i−2 iterations in the i-th outer iteration. The size of QiWi ·· · ··QjWj is
determined by the rows in Qi and the columns in Wj, and hence the maximum
number of neurons in a layer determines the maximum size of the matrices in
Q. Similarly, the maximum dimension of Ji is also bounded by the maximum
number of neurons in a layer. Because of the two inner loops, time complexity
of Algorithm 2 is quadratic in M , whereas Algorithm 1 is linear in M .

4 Experiments

In this section, we perform a comprehensive empirical study of POLAR against
state-of-the-art (SOTA) techniques. We ﬁrst demonstrate the performance of
POLAR on two examples with high dimensional states and multiple inputs,
which are far beyond the ability of current SOTA techniques (Section 4.1). A
comprehensive comparison with SOTA over the full benchmarks in [11,13] is then
given (Section 4.2). Finally, we present additional ablation studies, scalability
analysis, and the ability to handle discrete-time systems (Section 4.3).

All our experiments were run on a machine with 6-core 2.90 GHz Intel Core
i5 and 8GB of RAM. POLAR is implemented with C++. We present the results
for POLAR, Verisig 2.0 and Sherlock using a single core without parallelization.
The results of ReachNN* were computed on the same machine with the aid of
GPU acceleration on an Nvidia GeForce RTX 2060 GPU.
State-of-the-art tools. We compare with SOTA tools in the NNCS reachability
analysis literature, including Sherlock [8] (only works for ReLU), Verisig 2.0 [13]
(only works for sigmoid and tanh), NNV [29], and ReachNN*[9]6.

6 The results of ReachNN* are based on GPU acceleration.

POLAR

11

Fig. 4: Comparison between reachable sets of the 6-dimensional attitude con-
trol benchmark produced by POLAR (dark green), Verisig 2.0 (gray) and NNV
(yellow). The red curves are simulated trajectories.

(a) QUAD

(b) Mountain Car

Fig. 5: (a) Results of QUAD. POLAR for 50 steps (dark green sets), Verisig 2.0
for 3 steps (grey sets), and simulation traces for 50 steps (red curves). It took
POLAR 1533 seconds to compute the ﬂowpipes for 50 steps. On the other hand,
it took Verisig 2.0 more than 5 hours to compute the ﬂowpipes for the ﬁrst 3
steps, and at the 4th step, the remainders of the TM computed by Verisig 2.0
for the outputs of the neural-network controller already exploded to 1015. NNV
crashed with out-of-memory errors when computing the 1st step. (b) Results of
Mountain Car. POLAR for 150 steps (dark green sets), Verisig 2.0 for 150 steps
(grey sets), ReachNN* for 90 steps (light green sets), NNV for 65 steps, and
simulation traces for 150 steps (red curves).

4.1 High Dimensional Case Studies: Attitude Control & QUAD.

We consider an attitude control of a rigid body with 6 states and 3 control
inputs [27], and quadrotor (QUAD) with 12 states and 3 control inputs [3] to
evaluate the performance of POLAR on diﬃcult problems. The complexity of
these two example lies in the combination of the numbers of the state variables
and control inputs. For each example, we trained a sigmoid neural-network con-

-0.500.5-1.4-1.2-1-0.8-0.6Attitude ControlInitial Set-0.8-0.6-0.4-0.200.50.60.70.80.911.1Attitude ControlInitial Set-0.500.51-0.8-0.6-0.4-0.200.2Attitude ControlInitial Set012345-0.500.511.5QUADInitial Set-20246x0-0.2-0.100.10.2x1Discrete Mountain CarInitial Set12

Chao Huang, Jiameng Fan, Xin Chen, Wenchao Li, and Qi Zhu

Table 1: V : number of state variables, σ: activation functions, M : number of
hidden layers, n: number of neurons in each hidden layer. For each approach
(POLAR, ReachNN*, Sherlock, Verisig 2.0), we give the runtime in seconds if it
successfully veriﬁes the property. ‘Unknown’: the property could not be veriﬁed.
‘–’: the approach cannot be applied due to the type of σ.

1

2

2

2

3

2

V

#

POLAR

NN Controller

σ
ReLU
sigmoid
tanh
ReLU+tanh
ReLU
sigmoid
tanh
ReLU+tanh
ReLU
sigmoid
tanh

[13]
–
47
46
–
–
7
Unknown
–
–
44
38
–
–
11
10
–
–
190
179
–
–
83
70
–
3344
652
Unknown
Unknown
1 This example has multi-dimensional control inputs. ReachNN* only supports NN

ReachNN* Sherlock Verisig 2.0
[8]
42
–
–
–
3
–
–
–
143
–
–
–
21
–
–
–
15
–
–
–
35
–
–
–
–
–
–
–

M n
2 20
2 20
2 20
2 20
2 20
2 20
2 20
2 20
2 20
2 20
2 20
ReLU+sigmoid 2 20
2 20
2 20
2 20
2 20
3 100
3 100
3 100
3 100
3 20
3 20
3 20
3 20
3 20
2 20
3 64
3 64

[9]
26
75
Unknown
71
5
13
73
Unknown
94
146
137
150
8
22
21
12
103
27
Unknown
Unknown
1130
13350
2416
1413
Unknown
–1
–1
–1

ReLU
sigmoid
tanh
ReLU+tanh
ReLU
sigmoid
tanh
ReLU+tanh
ReLU
sigmoid
tanh
ReLU+tanh
tanh
tanh
sigmoid
sigmoid

12
17
20
13
2
9
3
2
16
36
26
15
2
3
3
2
13
76
76
10
16
21
19
15
343
61
201
1533

6
6
Attitude Control 6
12

ACC
QMPC

QUAD

6

3

5

3

4

4

controllers that produce single-dimensional control inputs.

troller and compare POLAR with Verisig 2.0 and NNV. The detailed setting of
these two examples can be found in the Appendix.

The result for the attitude control benchmark is shown in Figure 4, and the
result for the QUAD benchmark is shown in Figure 5a. In the attitude control
benchmark, POLAR computed the TM ﬂowpipes for 30 control steps in 201
seconds. From Figure 4, We can observe that the ﬂowpipes computed by POLAR
are tight w.r.t. the simulated traces. As a comparison, although Verisig 2.0 [13]
can handle this system in theory, its remainder exploded very quickly and the
tool crashed after only a few steps. NNV computed ﬂowpipes for 25 steps by
doing extensive splittings on the state space and crashed with out-of-memory
errors. In the QUAD benchmark, POLAR computed the TM ﬂowpipes for 50
control steps in 1533 seconds, while Verisig 2.0 and NNV took hours to compute
ﬂowpipes just for the ﬁrst few steps.

POLAR

13

4.2 Comparison over A Full Set of Benchmarks

We compare POLAR with the SOTA tools mentioned previously, including Sher-
lock, Verisig 2.0, NNV, and ReachNN* over the full benchmarks in [11,13]. We
refer to [11,13] for more details of these benchmarks. The results are presented
in Table 1 where NNV is not included since we were not able to successfully use
it to prove any of the benchmarks likely because it is designed for linear systems.
Similar results for NNV are also observed in [13]. We can see that POLAR suc-
cessfully veriﬁes all the cases and the runtime is on average 8x and up to 94x
faster7 compared with the tool with the second best eﬃciency. The ”Unknown”
veriﬁcation results either indicate the overapproximation of reachable set were
too large for verifying the safety property or the tool terminated early due to
an explosion of the overapproximation. POLAR achieves the best performance
among all the tools (visualizations and detailed comparisons of the reachable
sets can be found in the Appendix).

4.3 Discussion

POLAR demonstrates substantial performance improvement over existing tools.
In this section, we seek to further explore the capability of POLAR. We conduct
several experiments for the QUAD benchmark to better understand the limita-
tion and scalability of POLAR. We also include a mountain car example to show
that POLAR is able to handle discrete-time systems.
Ablation Studies. To explore the impact of the two proposed techniques,
namely Bernstein polynomial interpolation (BP) and symbolic remainder (SR)
on the overall performance, we conduct a series of experiments on the QUAD
benchmark with diﬀerent conﬁgurations. Table 2 shows the performance of PO-
LAR with and without the proposed techniques SR and BP in the NN propa-
gation: 1) TM: only TM arithmetic is used; 2) TM+SR: SR is used with TM
arithmetic; 3) BP is used with TM arithmetic; and 4) Both BP and SR are used
with TM arithmetic. Based on the results, we can observe that SR signiﬁcantly
improves the accuracy of the reachable set overapproximation. Finally, the com-
bination of basic TM with BP and SR not only achieves the best accuracy, but
also is the most eﬃcient. While the additional BP and SR operations can incur
runtime overhead compared with basic TM, they help to produce a tighter over-
estimation and thus reduce the state space being explored during reachability
analysis. As a result, the overall performance including runtime is better.

The following further observations can be obtained from Table 2. (i) Both
of the independent use of BP and SR techniques signiﬁcantly improves the per-
formance of reachable set overapproximations. (ii) When the BP technique is
used, Bernstein approximation is often not used on activation functions, but
the few times for which they are used signiﬁcantly improve the accuracy. The
reason of having this phenomenon is that Taylor and Bernstein approximations

7 These are lower bounds on the improvements since other tools terminated early for

certain settings due to explosion of their computed ﬂowpipes.

14

Chao Huang, Jiameng Fan, Xin Chen, Wenchao Li, and Qi Zhu

X0

k

Table 2: Ablation Studies for POLAR on the QUAD benchmark. We compare
the width of TM remainder on x3 at the 50th step under diﬀerent settings. For
settings with BP, we also list the percentage of times where BP is used among
9600 neurons. If a setting cannot compute ﬂowpipes for all 50 steps, it is marked
as Unknown. X0 is the radius of the initial set. k is the order of the TM.
TM+BP+SR

TM+BP

TM+SR

TM

Width
7.5e-04
2
5.2e-04
3
4
4.9.e-04
2 Unknown
1.8e-03
3
4
1.6e-03
2 Unknown
9.0e-03
3
4
5.0e-03
2 Unknown
3 Unknown
4 Unknown

Time (s) Width
1.3e-04
6.5e-05
6.2e-05
2.3e-03
2.2e-04
1.9e-04
Unknown
1.9e-03
9.2e-04
Unknown
Unknown
Unknown

229
273
332
–
352
431
–
721
761
–
–
–

Time (s) Width
6.8e-04
5.0e-04
4.7e-04
1.0e-02
1.7e-03
1.5e-03
Unknown
7.8e-03
4.7e-03
Unknown
Unknown
Unknown

233
251
270
319
287
304
–
412
403
–
–
–

Time (s) BP % Width
5.79% 1.2e-04
3.62% 6.5e-05
3.57% 6.2e-05
9.68% 1.1e-03
6.85% 2.2e-04
6.70% 1.9e-04
Unknown
4.03% 1.6e-03
4.38% 8.1e-04
Unknown
Unknown
3.7e-02

228
274
336
325
349
427
–
670
728
–
–
–

–
–
–

–

Time (s) BP %
1.34%
0%
0%
4.80%
0%
0%
–
0.77%
0.07%
–
–
3.25%

231
251
270
289
287
304
–
394
396
–
–
1533

0.05

0.1

0.2

0.4

are similarly accurate in approximating activation functions with small domain.
However, the Lagrange form-based remainder evaluation in Taylor polynomials
performs better than the sample-based remainder evaluation in Bernstein poly-
nomials in those cases. It can also be seen that for each X0, the use of Bernstein
approximation becomes more frequent when the TMs has larger remainders. (iii)
When both BP and SR techniques are used, the approach produces the tight-
est TMs compared with the other columns in the table even though the use
Bernstein approximation is less often. The reason is that the remainders of the
TMs are already well-limited and most of the activation functions handled in
the reachability computation are with a “small” TM domain.

Scalability Analysis. Table 1 shows that POLAR can handle much larger
NNCSs compared with the current SOTA. To better understand the scalability of
POLAR, we further conduct scalability analysis on the size of the NN controller
and the width of the initial set using the QUAD benchmark. The experiment
results in Figure 6 for the neural networks with diﬀerent widths and depths show
that POLAR scales well on the number of layers and the number of neurons in
each layer in the NN controller. On the other hand, the time cost grows rapidly
when the width of the initial set becomes larger. Such a phenomenon already
exists in the literature for reachability analysis of ODE systems [5]. The reason
for this is that when the initial set is larger, it is more diﬃcult to track the state
dependencies and requires keeping more terms in a TM ﬂowpipe.

Discrete-time NNCS. Finally, we use Mountain car, a common benchmark in
Reinforcement Learning literature, to show that POLAR also works on discrete-
time systems. The detailed setting can be found in the Appendix B. The com-
parison with Verisig 2.0, ReachNN* and NNV is shown in Figure 5b. POLAR
also outperforms these tools substantially for this example.

POLAR

15

Fig. 6: Scalability analysis for POLAR on the QUAD benchmark. We present the
runtime of QUAD for 50 steps reachability analysis. Under all settings, POLAR
can verify that the system reaches the target set at the 50th step. Left ﬁgure:
Runtime on diﬀerent neural network architectures with the input set radius as
0.05. We study neural-network controllers with diﬀerent number of layers (2, 3,
4, 5) and neurons (64, 100, 150, 200). Right ﬁgure: Runtime on the diﬀerent
input set radius of the QUAD benchmark. We use the same network in Figure 5
which has 3 hidden layers with 64 neurons in each layer.

5 Conclusion

In this paper, we propose POLAR, a polynomial arithmetic framework, which
integrates TM ﬂowpipe construction, Bernstein overapproximation, and sym-
bolic remainder method to eﬃciently compute reachable set overapproximations
for NNCS. Empirical comparison over a suite of benchmarks shows that PO-
LAR performs signiﬁcantly better than SOTAs in terms of both computation
eﬃciency and tightness of reachable set estimation.

References

1. Althoﬀ, M.: An introduction to CORA 2015. In: International Workshop on Ap-
plied veRiﬁcation for Continuous and Hybrid Systems (ARCH). EPiC Series in
Computing, vol. 34, pp. 120–151 (2015)

2. Alur, R., Dill, D.L.: A theory of timed automata. Theoretical computer science

126(2), 183–235 (1994)

3. Beard, R.: Quadrotor dynamics and control rev 0.1 (2008)
4. Berz, M., Makino, K.: Veriﬁed integration of ODEs and ﬂows using diﬀerential
algebraic methods on high-order Taylor models. Reliable computing 4, 361–369
(1998)

5. Chen, X.: Reachability Analysis of Non-Linear Hybrid Systems Using Taylor Mod-

els. Ph.D. thesis, RWTH Aachen University (2015)

6. Chen, X., ´Abrah´am, E., Sankaranarayanan, S.: Flow*: An analyzer for non-linear

hybrid systems. In: Proc. of CAV’13. LNCS, vol. 8044, pp. 258–263 (2013)

7. Chen, X., Sankaranarayanan, S.: Decomposed reachability analysis for nonlinear

systems. In: Proc. of RTSS’16. pp. 13–24 (2016)

8. Dutta, S., Chen, X., Sankaranarayanan, S.: Reachability analysis for neural feed-
back systems using regressive polynomial rule inference. In: Proc. of HSCC’19. pp.
157–168. ACM (2019)

2345Number of Hidden Layers5001000150020002500Runtime (s)641001502000.050.10.20.4Input Set Radius2565121024Runtime (s)(log-scale)16

Chao Huang, Jiameng Fan, Xin Chen, Wenchao Li, and Qi Zhu

9. Fan, J., Huang, C., Chen, X., Li, W., Zhu, Q.: ReachNN*: A tool for reachabil-
ity analysis of neural-network controlled systems. In: Proceedings of International
Symposium on Automated Technology for Veriﬁcation and Analysis (ATVA).
LNCS, vol. 12302, pp. 537–542. Springer (2020)

10. Frehse, G., Guernic, C.L., Donz´e, A., Cotton, S., Ray, R., Lebeltel, O., Ripado,
R., Girard, A., Dang, T., Maler, O.: Spaceex: Scalable veriﬁcation of hybrid sys-
tems. In: Proceedings of International Conference on Computer Aided Veriﬁcation
(CAV). Lecture Notes in Computer Science, vol. 6806, pp. 379–395 (2011)

11. Huang, C., Fan, J., Li, W., Chen, X., Zhu, Q.: ReachNN: Reachability analysis
of neural-network controlled systems. ACM Trans. Embed. Comput. Syst. 18(5s),
106:1–106:22 (2019)

12. Huang, X., Kwiatkowska, M., Wang, S., Wu, M.: Safety veriﬁcation of deep neural
networks. In: Proc. of CAV’17. LNCS, vol. 10426, pp. 3–29. Springer (2017)
13. Ivanov, R., Carpenter, T., Weimer, J., Alur, R., Pappas, G.J., Lee, I.: Verisig 2.0:
Veriﬁcation of neural network controllers using taylor model preconditioning. In:
Proc. of CAV’21. LNCS, vol. 12759, pp. 249–262. Springer (2021)

14. Ivanov, R., Carpenter, T.J., Weimer, J., Alur, R., Pappas, G.J., Lee, I.: Verifying
the safety of autonomous systems with neural network controllers. ACM Trans.
Embed. Comput. Syst. 20(1), 7:1–7:26 (2021)

15. Ivanov, R., Weimer, J., Alur, R., Pappas, G.J., Lee, I.: Verisig: verifying safety
properties of hybrid systems with neural network controllers. In: Proc. of HSCC’18.
pp. 169–178. ACM (2019)

16. Jaulin, L., Kieﬀer, M., Didrit, O., Walter, ´E.: Interval analysis. In: Applied Interval

Analysis. Springer (2001)

17. Katz, G., Barrett, C.W., Dill, D.L., Julian, K., Kochenderfer, M.J.: Reluplex: An
eﬃcient SMT solver for verifying deep neural networks. In: Proc. of CAV’17. LNCS,
vol. 10426, pp. 97–117. Springer (2017)

18. Levine, S., Finn, C., Darrell, T., Abbeel, P.: End-to-end training of deep visuomotor
policies. The Journal of Machine Learning Research 17(1), 1334–1373 (2016)

19. Lorentz, G.G.: Bernstein Polynomials. American Mathematical Society (2013)
20. Lygeros, J., Tomlin, C.J., Sastry, S.: Controllers for reachability speciﬁcations for

hybrid systems. Automatica 35(3), 349–370 (1999)

21. Makino, K., Berz, M.: Taylor models and other validated functional inclusion meth-
ods. International Journal of Pure and Applied Mathematics 4(4), 379–456 (2003)

22. Meiss, J.D.: Diﬀerential Dynamical Systems. SIAM publishers (2007)
23. Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G.,
Graves, A., Riedmiller, M., Fidjeland, A.K., Ostrovski, G., et al.: Human-level
control through deep reinforcement learning. Nature 518(7540), 529–533 (2015)

24. Moore, R.E., Kearfott, R.B., Cloud, M.J.: Introduction to Interval Analysis. SIAM

(2009)

25. Pan, Y., Cheng, C., Saigol, K., Lee, K., Yan, X., Theodorou, E.A., Boots, B.: Agile
autonomous driving using end-to-end deep imitation learning. In: Proc. of RSS’18
(2018)

26. Prajna, S., Jadbabaie, A.: Safety veriﬁcation of hybrid systems using barrier cer-

tiﬁcates. In: HSCC. pp. 477–492. Springer (2004)

27. Prajna, S., Parrilo, P.A., Rantzer, A.: Nonlinear control synthesis by convex opti-

mization. IEEE Transactions on Automatic Control 49(2), 310–314 (2004)

28. Singh, G., Ganvir, R., P¨uschel, M., Vechev, M.T.: Beyond the single neuron convex
barrier for neural network certiﬁcation. In: Proc. of NeurIPS’19. pp. 15072–15083
(2019)

POLAR

17

29. Tran, H., Yang, X., Lopez, D.M., Musau, P., Nguyen, L.V., Xiang, W., Bak, S.,
Johnson, T.T.: NNV: the neural network veriﬁcation tool for deep neural net-
works and learning-enabled cyber-physical systems. In: Proc. of CAV’20. LNCS,
vol. 12224, pp. 3–17. Springer (2020)

30. Wang, S., Pei, K., Whitehouse, J., Yang, J., Jana, S.: Formal security analysis of
neural networks using symbolic intervals. In: Proc. of USENIX Security (USENIX).
pp. 1599–1614 (2018)

31. Weng, T.W., Zhang, H., Chen, H., Song, Z., Hsieh, C.J., Daniel, L., Dhillon, I.: To-
wards fast computation of certiﬁed robustness for relu networks. In: International
Conference on Machine Learning (ICML) (2018)

32. Zhang, H., Weng, T.W., Chen, P.Y., Hsieh, C.J., Daniel, L.: Eﬃcient neural net-
work robustness certiﬁcation with general activation functions. In: Proceedings of
the 32nd International Conference on Neural Information Processing Systems. pp.
4944–4953 (2018)

18

Chao Huang, Jiameng Fan, Xin Chen, Wenchao Li, and Qi Zhu

A Additional Experimental Results

Here, we present additional plots of reachable sets computed by diﬀerent tech-
niques for the benchmarks in Section 4 of the main paper.

For each benchmark, the goal is to check whether the system will reach a
given target set. For each tool and in each test, if the computed reachable set
overapproximation for the last control step lies entirely in the target set, we
consider the tool to have successfully veriﬁed the reachability property. If the
overapproximation of the reachable set does not intersect with the target set,
the tool would have successfully disproved the reachability property. Otherwise,
we consider the veriﬁcation result to be unknown.

Results of Benchmark 1-6, the ACC benchmark, and the QMPC benchmark
are shown in Figure 7, 8, 9 respectively, while the results of the Attitude control
benchmark and the QUAD benchmark are shown previously in Figure 4 and
Figure 5a. The red trajectories are sample system executions and should be
contained entirely by the ﬂowpipes computed by each tool. The dark green sets
are the ﬂowpipes computed by POLAR. The light green sets are the ﬂowpipes
computed by ReachNN* [11,9]. The blue sets are the ﬂowpipes computed by
Sherlock [8]. The grey sets are the ﬂowpipes computed by Verisig 2.0 [14]. In some
benchmarks, the reachable sets computed by Verisig 2.0 are almost overlapping
with the reachable sets computed by POLAR. However, POLAR takes much less
time to compute the reachable sets compared to Verisig 2.0 as shown in Table 1
of the main paper. We also show results from NNV [29] in yellow for some of the
benchmarks. For the rest, NNV used up all of the system memory (8GB) and
could not ﬁnish the computation. Our observations are consistent with those in
[14] where NNV is not able to verify any of these benchmarks. The blue box
represents the target set in each test. POLAR produces the tightest reachable
set estimation and successfully proves or disproves the reachability property for
all the examples.

POLAR

19

(a) ex1-relu

(b) ex1-sigmoid

(c) ex1-tanh

(d) ex1-relu-tanh

(e) ex2-relu

(f) ex2-sigmoid

(g) ex2-tanh

(h) ex2-relu-tanh

(i) ex3-relu

(j) ex3-sigmoid

(k) ex3-tanh

(l) ex3-relu-sigmoid

(m) ex4-relu

(n) ex4-sigmoid

(o) ex4-tanh

(p) ex4-relu-tanh

(q) ex5-relu

(r) ex5-sigmoid

(s) ex5-tanh

(t) ex5-relu-tanh

(u) ex6-relu

(v) ex6-sigmoid

(w) ex6-tanh

(x) ex6-relu-tanh

Fig. 7: Results of Benchmarks. We can see that except for ex2-sigmoid, POLAR
produces the tightest reachable set estimation (dark green sets) and successfully
proves or disproves the reachability property for all the examples. This is in
comparison with other state-of-the-art tools including ReachNN* [11,9] (light
green sets), Sherlock [8] (blue sets), Verisig 2.0 [14] (grey sets), and NNV [29]
(yellow sets). Except for (f), (g), (v) and (w), NNV used up all of the system
memory and could not ﬁnish the computation.

-0.500.51x1-0.6-0.4-0.200.20.40.6x2Benchmark 1 (ReLU)-0.500.51x1-0.6-0.4-0.200.20.40.6x2Benchmark 1 (sigmoid)-0.500.51x1-0.6-0.4-0.200.20.40.6x2Benchmark 1 (tanh)00.20.40.60.81x1-0.6-0.4-0.200.20.40.6x2Benchmark 1 (ReLU_tanh)-0.500.51x1-1.5-1-0.500.51x2Benchmark 2 (ReLU)-1-0.500.51x0-3-2-1012x1Benchmark 2 (sigmoid)-1.5-1-0.500.51x1-3-2-10123x2Benchmark 2 (tanh)-0.500.51x1-1.5-1-0.500.51x2Benchmark 2 (ReLU_tanh)0.20.40.60.81x1-0.4-0.200.20.40.6x2Benchmark 3 (ReLU)0.20.40.60.81-0.4-0.200.20.40.60.20.40.60.81x1-0.4-0.200.20.40.6x2Benchmark 3 (tanh)00.20.40.60.81-0.4-0.200.20.40.6-0.2-0.100.10.20.3x100.050.1x2Benchmark 4 (ReLU)-0.100.10.20.3x1-0.0500.050.1x2Benchmark 4 (sigmoid)-0.100.10.20.3x1-0.0500.050.1x2Benchmark 4 (tanh)-0.2-0.100.10.20.3x100.050.1x2Benchmark 4 (ReLU_tanh)-0.4-0.200.20.4x100.10.20.30.40.50.6x2Benchmark 5 (ReLU)-0.500.5x100.10.20.30.40.50.6x2Benchmark 5 (sigmoid)-0.500.5x100.10.20.30.40.50.6x2Benchmark 5 (tanh)-0.500.5x100.10.20.30.40.50.6x2Benchmark 5 (ReLU_tanh)-1-0.500.51x1-1-0.500.51x2Benchmark 6 (ReLU)-1-0.500.51x1-1.5-1-0.500.51x2Benchmark 6 (sigmoid)-1-0.500.51x1-1.5-1-0.500.51x2Benchmark 6 (tanh)-1-0.500.51x1-1-0.500.51x2Benchmark 6 (ReLU_tanh)20

Chao Huang, Jiameng Fan, Xin Chen, Wenchao Li, and Qi Zhu

Fig. 8: Results of Adaptive Cruise Control (ACC). POLAR for 50 steps (dark
green sets), Verisig 2.0 for 50 steps (grey sets), ReachNN* for 3 steps (light
green sets), NNV for 50 steps (yellow sets), and simulation traces for 50 steps
(red curves).

(a)

(b)

Fig. 9: Results of QMPC. POLAR for 30 steps (dark green sets), Verisig 2.0 for
30 steps (grey sets), and simulation traces for 30 steps (red curves).

B Discrete-time Mountain Car Benchmark

Example 1 (MC). In this benchmark, an under-powered car targets to drive up
a steep hill. Since the car does not have enough power to Widthelerate up the
hill, it needs to drive up the opposite hill ﬁrst to gain enough momentum. The
car has the following discrete-time dynamics:

(cid:40)

x0[t + 1] = x0[t] + x1[t],
x1[t + 1] = x1[t] + 0.0015 · u[t] − 0.0025 · cos(3 · x0[t]).

242628303229.429.629.83030.230.430.6ACCInitial Set-0.100.10.2-0.3-0.2-0.100.1QMPCInitial Set-0.3-0.2-0.100.1-0.0200.020.040.060.080.1QMPCInitial SetFor this benchmark, the initial set is x0 ∈ [−0.53, −0.5] and x1 = 0. The target
is x0 ≥ 0.2 and x1 ≥ 0 where the car reaches the top of the hill and is moving
forward. The total control steps N is 150.

POLAR

21

C Benchmarks with High Dimensional States and

Multiple Outputs

Example 2 (Attitude Control). We consider the attitude control of a rigid body
with six states and three inputs as a physically illustrating example [27]. The
system dynamics is



˙ω1 = 0.25(u0 + ω2ω3),

˙ψ1=0.5 (cid:0)ω2(ψ2
2+ψ2
˙ψ2=0.5 (cid:0)ω1(ψ2
2+ψ2

˙ψ3=0.5 (cid:0)ω1(ψ2
2−ψ2+ψ2

˙ω2 = 0.5(u1 − 3ω1ω3),
2+ψ2+ψ2
1+ψ2
2+ψ2
1−ψ1+ψ2
2+ψ2
1+ψ1+ψ2

3−ψ3)+ω3(ψ2
3+ψ3)+ω3(ψ2
3)+ω2(ψ2

1+ψ2
1+ψ2
1+ψ2

˙ω3 = u2 + 2ω1ω2,
3+1)(cid:1) ,
2+ψ2
3+1)(cid:1) ,
2+ψ2
3+1)(cid:1) .
2+ψ2

1+ψ2
1+ψ2
1+ψ2

3)+ω1(ψ2
3)+ω2(ψ2
3)+ω3(ψ2

wherein the state (cid:126)x=(ω, ψ) consists of the angular velocity vector in a body-ﬁxed
frame ω∈R3, and the Rodrigues parameter vector ψ∈R3.

The control torque u∈R3 is updated every 0.1 second by a neural network
with 3 hidden layers, each of which has 64 neurons. The activations of the hid-
den layers are sigmoid and identity, respectively. We train the neural-network
controller using supervised learning methods to learn from a known nonlinear
controller [27]. The initial state set is:

ω1 ∈ [−0.45, −0.44], ω2 ∈ [−0.55, −0.54], ω3 ∈ [0.65, 0.66],
ψ1 ∈ [−0.75, −0.74], ψ2 ∈ [0.85, 0.86], ψ3 ∈ [−0.65, −0.64].

Example 3 (QUAD). We study a neural-network controlled quadrotor (QUAD)
with 12 states [3]. For the states, we have the inertial (north) position x1, the
inertial (east) position x2, the altitude x3, the longitudinal velocity x4, the lateral
velocity x5, the vertical velocity x6, the roll angle x7, the pitch angle x8, the yaw
angle x9, the roll rate x10, the pitch rate x11, and the yaw rate x12. The control
torque u ∈ R3 is updated every 0.1 second by a neural network with 3 hidden
layers, each of which has 64 neurons. The activations of the hidden layers and
the output layer are sigmoid and identity, respectively.

22

Chao Huang, Jiameng Fan, Xin Chen, Wenchao Li, and Qi Zhu






˙x1 = cos(x8) cos(x9)x4 + (sin(x7) sin(x8) cos(x9) − cos(x7) sin(x9)) x5

+ (cos(x7) sin(x8) cos(x9) + sin(x7) sin(x9)) x6

˙x2 = cos(x8) sin(x9)x4 + (sin(x7) sin(x8) sin(x9) + cos(x7) cos(x9)) x5

+ (cos(x7) sin(x8) sin(x9) − sin(x7) cos(x9)) x6
˙x3 = sin(x8)x4 − sin(x7) cos(x8)x5 − cos(x7) cos(x8)x6
˙x4 =x12x5 − x11x6 − g sin(x8)
˙x5 =x10x6 − x12x4 + g cos(x8) sin(x7)
˙x6 =x11x4 − x10x5 + g cos(x8) cos(x7) − g − u1/m
˙x7 =x10 + sin(x7) tan(x8)x11 + cos(x7) tan(x8)x12
˙x8 = cos(x7)x11 − sin(x7)x12

˙x9 =

˙x10 =

˙x11 =

˙x12 =

sin(x7)
cos(x8)
Jy − Jz
Jx
Jz − Jx
Jy
Jx − Jy
Jz

x11 − sin(x7)x12

x11x12 +

x10x12 +

x10x11 +

1
Jx
1
Jy
1
Jz

u2

u3

τψ

The initial set is:

x1∈[−0.4, 0.4], x2∈[−0.4, 0.4], x3∈[−0.4, 0.4], x4∈[−0.4, 0.4],
x5∈[−0.4, 0.4], x6∈[−0.4, 0.4], x7=0, x8=0, x9=0, x10=0, x11=0, x12=0

The control goal is to stabilize the attitude x3 to a goal region [0.94, 1.06].

Example 4 (Discrete-Time Mountain Car (MC)). We consider a common bench-
mark in Reinforcement Learning problems, namely Mountain Car. In this bench-
mark, an under-powered car targets to drive up a steep hill. Since the car does
not have enough power to accelerate up the hill, it needs to drive up the oppo-
site hill ﬁrst to gain enough momentum. The car has the following discrete-time
dynamics:

x0[t + 1] = x0[t] + x1[t],
x1[t + 1] = x1[t] + 0.0015 · u[t] − 0.0025 · cos(3 · x0[t]).

For this benchmark, the initial set is x0 ∈ [−0.53, −0.5] and x1 = 0. The
target is x0 ≥ 0.2 and x1 ≥ 0 where the car reaches the top of the hill and is
moving forward. The total control steps N is 150.

D Theorem Proof

D.1 Proof of Soundness of Sampling-based Error Analysis

POLAR

23

Zj −Zj

segments. Consider the i-th segment [

Proof. The input range of an activation function σj is subdivided into m line
Zj −Zj
m (i) + Z j], and let
m (i − 1) + Z j,
m (i − 1
2 ) + Z j be the center of the segment. The diﬀerence between the
c =
Bernstein polynomial pj
σ and the activation function at the center of the i-th
segment is computed as (cid:12)
(cid:12)pj
(cid:12) . Then, the value of (cid:15)j can be bounded
by this diﬀerence at the center, as well as the product between the Lipschitz
constant of the activation function with respect to this segment Lj and the size
m . The detailed deduction is given below.

σ(c) − σj(c)(cid:12)

m , i.e., Lj ·

of the segment

Zj −Zj

Zj −Zj

Zj −Zj

|pσj ,i(x) − σj(x)|

=|pσj ,i(x) − pσ+j,i(c) + pσj ,i(c) − σj(c) + σj(c) − σj(x)|
≤|pσj ,i(x)−pσj ,i(c)|+|pσj ,i(c)−σj(c)|+|σj(c)−σj(x)|

≤|pσj ,i(x)−pσj ,i(c)|+|pσj ,i(c)−σj(c)|+Lj ·

Z j−Z j
2m

≤Lj ·

Z j−Z j
2m

+|pσj ,i(c)−σj(c)|+Lj ·

Z j−Z j
2m

=|pσj ,i(c) − σj(c)| + Lj

Z j − Z j
m

Triangle inequality

Lipschitz continuity for σj

Lipschitz continuity for pσj ,i

Note that Bernstein polynomial pσj ,i has the same Lipschitz constant with σj.
Thus we also use Lj to bound |pσj ,i(x) − pσj ,i(c)| in the deduction. The error
bound over the whole range [Z j, Z j] should be the largest error bound among
(cid:3).
all the segments.

D.2 Proof of Theorem 1

Proof. First, due to the overapproximation property of our methods, any of
our Bernstein overapproximation pσ,I + Iσ,i satisﬁed that for (cid:126)z in the domain
on which Iσ,i is evaluated, we have that σ((cid:126)z) ∈ pσ,i((cid:126)z) + Iσ,i. Therefore, by
the overapproximation property of TM arithmetic, the returned (pr((cid:126)x0), Ir) of
Algorithm 1 or 2 is a state-wise overapproximation of the control input range
w.r.t. the TM variable (cid:126)x0 ∈ X0 wherein X0 is the NNCS initial set.

We prove Theorem 1 by an induction on the number of control steps j.
Assume that N = δc/δ is the number of ﬂowpipes computed in each control
step.
Base Case. When j = 1, the TM ﬂowpipes are computed for the reachable set
in the ﬁrst control step and the evolution is under the pure continuous dynamics
˙(cid:126)x = f ((cid:126)x, (cid:126)u0),
˙(cid:126)u = 0 with (cid:126)x(0) ∈ X0 and (cid:126)u(0) = κ((cid:126)x(0)). The image of the
mapping κ((cid:126)x0) from (cid:126)x0 ∈ X0 is overapproximated by a TM (pr((cid:126)x0), Ir) with
(cid:126)x0 ∈ X0. Hence, by performing TM ﬂowpipe construction for the ODE ˙(cid:126)x =
f ((cid:126)x, (cid:126)u), ˙(cid:126)u = 0 with the initial set (cid:126)u(0) ∈ pr((cid:126)x0) + Ir, (cid:126)x(0) = (cid:126)x0, we have that for

24

Chao Huang, Jiameng Fan, Xin Chen, Wenchao Li, and Qi Zhu

any i = 1, . . . , N , the i-th TM ﬂowpipe Fi((cid:126)x0, τ ) contains the exact reachable
state at the time (i − 1)δ + τ for τ ∈ [0, δ].
Induction. When j > 1, we assume that the local initial set ˆXj−1 = (p0((cid:126)x0), I0)
is a state-wise overapproximation of the reachable set at the time jδc from any
(cid:126)x0 ∈ X0. Then the TM (pr((cid:126)x0), Ir) is a state-wise overapproximation for the
control input set κ( ˆXj−1), i.e., the NN controller’s output produced based on
the jδc-time state in the execution from an initial state (cid:126)x0 ∈ X0 is contained
in the box pr((cid:126)x0) + Ir for any (cid:126)x0 ∈ X0. Hence, for any i = 1, . . . , N , the i-th
ﬂowpipe Fi((cid:126)x0, τ ) computed for the ODE ˙(cid:126)x = f ((cid:126)x, (cid:126)u), ˙(cid:126)u = 0 with the initial set
(cid:126)u(0) ∈ pr((cid:126)x0) + Ir, (cid:126)x(0) ∈ ˆXj−1 contains the actual reachable state ϕN ((cid:126)x0, (j −
(cid:3)
1)δc + (i − 1)δ + τ ) for any τ ∈ [0, δ].

