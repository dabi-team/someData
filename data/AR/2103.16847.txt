A NOVEL DEEP LEARNING ARCHITECTURE BY INTEGRATING VISUAL
SIMULTANEOUS LOCALIZATION AND MAPPING (VSLAM) INTO CNN FOR REAL-TIME
SURGICAL VIDEO ANALYSIS

Ella Lan

The Harker School
500 Saratoga Avenue, San Jose, CA 95129

2
2
0
2

r
p
A
8
1

]

V

I
.
s
s
e
e
[

3
v
7
4
8
6
1
.
3
0
1
2
:
v
i
X
r
a

ABSTRACT

Seven million people suffer surgical complications each year,
but with sufﬁcient surgical training and review, 50% of these
complications could be prevented. To improve surgical per-
formance, existing research uses various deep learning (DL)
technologies including convolutional neural networks (CNN)
and recurrent neural networks (RNN) to automate surgical
tool and workﬂow detection. However, there is room to im-
prove accuracy; real-time analysis is also minimal due to the
complexity of CNN. In this research, a novel DL architec-
ture is proposed to integrate visual simultaneous localization
and mapping (vSLAM) into Mask R-CNN. This architecture,
vSLAM-CNN (vCNN), for the ﬁrst time, integrates the best
of both worlds, inclusive of (1) vSLAM for object detection,
by focusing on geometric information for region proposals,
and (2) CNN for object recognition, by focusing on seman-
tic information for image classiﬁcation, combining them into
one joint end-to-end training process. This method, using
spatio-temporal information in addition to visual features, is
evaluated on M2CAI 2016 challenge datasets, achieving the
state-of-the-art results with 96.8 mAP for tool detection and
97.5 mean Jaccard score for workﬂow detection, surpassing
all previous works, and reaching a 50 FPS performance, 10x
faster than the region-based CNN. A region proposal module
(RPM) replaces the region proposal network (RPN) in Mask
R-CNN, accurately placing bounding boxes and lessening the
annotation requirement. Furthermore, a Microsoft HoloLens
2 application is developed to provide an augmented reality
(AR)-based solution for surgical training and assistance.

Index Terms— VSLAM, CNN, Deep Learning, Aug-

mented Reality (AR), Computer Vision

1. INTRODUCTION

Each year, roughly seven million people suffer complications
after surgery, but half of those complications could be pre-
vented [1]. Effective surgical video review has proved to
enhance surgical performance. Recognition of surgical tools
and workﬂows is essential to automate surgical review, mon-
itor surgical processes, and support surgeon decision-making

Fig. 1. The Visual Simultaneous Localization and Mapping
(vSLAM) Flow. The main beneﬁts of vSLAM are the key
points (the point clouds) that are input into the region proposal
module (RPM) for region calculation purpose.

during operations.

Multiple studies have used a variety of neural networks to
gather visual features from surgical videos and perform tool
and workﬂow detection, ranging from challenges such as the
M2CAI16 Challenge [2] to independent reviews [3] and stud-
ies. Twinanda et al. [2] extracted visual features via AlexNet
convolutional neural networks (CNN); Shvets et al. [4] per-
formed robotic instrument semantic segmentation via deep
learning; Bodenstedt et al. [5] calculated bounding boxes via
the random forest algorithm. These studies use visual fea-
tures to detect tool and workﬂow. However, spatio-temporal
information has not been fully used in existing works. Due to
the standardization of surgical videos and the U.S. Food and
Drug Administration (FDA) protocols regarding the tool us-
age and order sequence in each workﬂow [6], spatio-temporal
information can become valuable, potentially improving the
model performance.

This paper presents a novel architecture that integrates vi-
sual simultaneous localization and mapping (vSLAM) [7] [8]
with CNN, named vSLAM-CNN (vCNN). SLAM technol-
ogy [9] has been applied broadly in the ﬁeld of robots and

 
 
 
 
 
 
self-driving cars but rarely in analysis of surgical videos. The
visual SLAM framework, depicted in Figure 1, utilizes com-
puter vision to calculate the position and orientation of a de-
vice with respect to its surroundings. And at the same time,
vSLAM maps the environment using only visual inputs from
a camera for object detection and localization. Under vCNN,
a vSLAM-based region proposal module (RPM) is created to
replace region proposal network (RPN) [10]; it utilizes spatio-
temporal information to generate region set proposals from
maps created by vSLAM. This architecture improves upon
the accuracy in previous studies for both tool and workﬂow
detection by incorporating geometric information and seman-
tic information into the training process. The vCNN also sub-
stantially improves the model’s prediction speed through vS-
LAM’s ability to analyze surgical videos in real-time. Due
to the past successes of CNN in object recognition and im-
age classiﬁcation, the architecture is built atop Mask R-CNN
[11], utilizing its region of interest (ROI) Align on extracted
features and proposed regions.

Fig. 2. The Relationship Between Visual Features, Spatial In-
formation, and Temporal Information When Performing Sur-
gical Video Analysis. This ﬁgure summarizes the technolo-
gies used in previous research and depicts the framework that
was used to develop the concept of creating an architecture
that would use visual features and spatio-temporal informa-
tion by integrating vSLAM into CNN.

2. RELATED WORKS

applications, methods yielding accurate prediction and real-
time detection speed become critical.

As can be seen in Figure 2, most of the previous research [12]
[13] [14] [15] [16] [17] [18] [19] has focused on one genre of
information: visual features, spatial information, or temporal
information. Studies that use visual features rely on CNN via
the ﬁne-tuning approach. For instance, Twinanda et al. [2]
used ﬁne-tuning CNN to create their own EndoNet architec-
ture, which was the ﬁrst to use CNN for multiple recognition
tasks; in this way, they were able to design a CNN architecture
that jointly performs the workﬂow recognition and tool pres-
ence detection tasks using only visual features. Despite their
state-of-art results, Twinanda et al. [2] were unable to explore
localization tracking for tools due to a lack of datasets with
spatial bounds. Previous research indicated spatial informa-
tion has the potential to increase the accuracy of tool detec-
tion. Jin et al. [13] created a new dataset called m2cai16-
tool-locations, which extends the m2cai16-tool dataset (the
tool training dataset given by the M2CAI16 Challenge) by
adding manual spatial bound annotations around the tools.
Jo et al. [15] applied YOLO9000 to generate the bounding
boxes. Additionally, workﬂow detection accuracy can ben-
eﬁt from time sequence and temporal information as proved
by studies, ranging from hidden Markov model (HMM) [16]
to recurrent neural networks (RNNs). For instance, Yu et
al. [18] tracked the time sequence of the surgical video us-
ing long short-term memory (LSTM), and Nwoye et al. [19]
applied convolutional LSTM using temporal information in
addition to visual features. These studies suggest that time
sequence is a prominent feature in the analysis of phase de-
tection.

In summary, previous works have yielded a range of re-
sults by using a variety of technologies. However, to maxi-
mize the impact of tool and workﬂow detection on real-world

Fig. 3. The Overall Visual SLAM Convolutional Neural Net-
works (vCNN) Architecture.

Different from other studies, this paper presents a deep
learning architecture to perform an end-to-end training for
both visual features and spatio-temporal information by in-
tegrating vSLAM into Mask R-CNN. This approach would
not only improve the model’s performance speed through
vSLAM’s instantaneous localization and mapping, but also
achieve higher accuracy for both tool and workﬂow detection,
with a lessened requirement of the training data annotations.

3. METHODOLOGY

The complete pipeline of the proposed approach is depicted
in Figure 3. Mask R-CNN is chosen as a backbone due to
its success with object classiﬁcation and ROI Align architec-
ture. The key innovation is to create the visual SLAM-based
region proposal module (RPM) to generate region proposals
through vSLAM 3D mapping, replacing region proposal net-
work (RPN). From there, region proposals, ROI, are obtained

and trained alongside the feature maps through a ﬁne-tuning
process. After detecting the tools, the fully connected layers
are input into the recurrent neural networks LSTM for work-
ﬂow detection. The details of this approach are explained as
follows.

clouds on cholecystectomy videos.

3.2. Mask R-CNN

3.1. vSLAM-Based Region Proposal Module (RPM)

RPM is a vSLAM-based module. Through openvslam [20],
an open source framework for visual SLAM (ORB-based [8]),
3D maps are generated from the videos captured by monocu-
lar cameras, and key points are extracted from the key frames
on the maps. Then, region proposals are created through K-
means clustering among the key points, as seen in Figure 4.
In addition, since these frame images are standardized in size,
the typical nine anchors are also created from the centroid of
each cluster.

unique
complexity

The surgical videos
settings.
have
Their
in-
cludes dark and blurring
camera
surroundings,
lenses covered by blood,
objects overlapping or
partially blocked by hu-
man organs, small move-
ments in a narrow envi-
ronment, etc. The RPM
architecture is based on
the main assumption that
through
understanding
the constant localization
of the tools through 3D
the proposal re-
maps,
gions will accurately capture and present the bounding boxes
for the tools. A new set of region generation algorithms has
been developed to allow vSLAM to generate sufﬁcient point
clouds for surgical videos and to extract the tools location
information from 3D maps.

Fig. 4. An Illustration of Re-
gion Generation via K-means.

• Optimize key frame generation by expanding the time
window to utilize more spatio-temporal information to
enrich key points’ coverage on tools. Expand the key
frame selection from the current frame to include all the
key frames within the previous ﬁve-second range to gen-
erate more regions for the current targeted frame.

• Adjust and apply the different K values in K-Means, in-
cluding 2, 3, 4, 5, 6, for each frame, to create broader
region proposals.

• Experiment with additional nine anchors (standard boxes
in Faster-RCNN [10]) on each centroids from K-means
clustering with additional regions.
In addition, the different conﬁgurations in openvslam, in-
cluding maximum key points, scale factor, the number of lev-
els, et cetera, are explored to allow vSLAM to generate point

Fig. 5. The Mean Average Precision (mAP) Comparison of
vSLAM-CNN (vCNN) to Previous Studies for Surgical Tool
Detection. The vCNN performance achieves the state-of-the-
art results.

To allow RPM to replace RPN, a technology deeply in-
grained inside Faster R-CNN and carried along into the ar-
chitecture of Mask R-CNN, the ﬁrst step was to code directly
in the open source deep learning learning (DL) framework
detectron2 [21], and to override its layers forwarding logic
to rebuild data ﬂow through the entire pipeline. Although the
mask mechanism of Mask R-CNN is not used in this research,
the reason for choosing Mask R-CNN is to use its ROI Align
layer to retrieve regional proposals from RPM to align with
the feature maps from CNN.

3.3. Fine-Tuning

By leveraging transfer-learning, the ﬁne-tuning approach is
used with Mask R-CNN. After loading in the pretrained ar-
chitecture and weights of the ResNet101 and feature pyramid
networks (FPN) [21], the model was trained by using custom
tools/workﬂow COCO datasets generated from the original
M2CAI16 training data [2] [13].

3.4. Long Short-Term Memory (LSTM) for Workﬂow

After the fully connected layers classify the tools, the ﬁnal
tool classiﬁcation, the created bounding boxes, and the fully
connected layer itself, are fed into the LSTM, an extra recur-
rent neural network (RNN) layer that uses temporal informa-
tion to predict the workﬂows.

4. EXPERIMENTAL SETUP AND DATASETS

All datasets used in this study consist of cholecystectomy
surgical videos collected from the University Hospital of
Strasbourg in France. The m2cai16-tool is a dataset that
was released for the M2CAI 2016 Tool Presence Detection
Challenge [2]. The m2cai16-workﬂow is a dataset that was
released for the M2CAI 2016 Workﬂow Presence Detection

Challenge. Another dataset used in the development of the
V-CNN architecture is the m2cai16-tool-locations. This is a
dataset created by Jin et al. for their study [13] and extends
upon the original m2cai16-tool by manually adding spatial
bounds around the tools. Though it is not essential for spatial
bounds of locations to be known because of the replacement
of RPN with vSLAM-based RPM, it was utilized to train the
vCNN ﬁne-tuning process because of the presumption that
locating the tools could potentially increase the accuracy of
the overall model. Each surgical video frame is labeled with
the certain workﬂow phase the surgery is in at that time.

5. RESULTS AND DISCUSSIONS

and

by
constant

The vCNN model gen-
erates
propos-
region
understanding
als
the
localiza-
tion from vSLAM 3D
maps,
providing
DL models with more
information to capture
and present bounding
By comparing
boxes.
the model’s
and
tool
workﬂow detection on
the blind testing video
frames with the ground
truth,
vCNN’s perfor-
mance is evaluated. The
vCNN achieves the state-
of-the-art results of 96.8
mAP for tool detection
(Figure 5) and 97.5 Jac-
card score for workﬂow
detection. It uses feature
maps via CNN, regions
via RPM, and sequences
via LSTM in an end-to-end training process, in which visual
features, spatial information, and temporal information are
used in a respective manner.

Fig. 6. Samples of Correct and
Incorrect Tool Detection Cases.

As indicated in Figure 5, the vCNN generates accurate
tool detection, surpassing prior studies by (1) optimizing re-
gion generation through the RPM algorithms and (2) fully uti-
lizing spatio-temporal information. And, unlike other studies
that have a wide range of detection accuracies for different
tools, the mAP of all the tools detection by vCNN is in a more
compact range, reﬂecting the universality of this model and its
stable performance under the innovative vCNN architecture.
Furthermore, as depicted in Figure 6, an in-depth analysis is
performed for both correct and incorrect detection.

Most of R-CNN-based algorithms have a 5 FPS limit
due to the RPN complexity; meanwhile, random forests and
YOLO directly generates bounding boxes to aim real-time

speed (25+ FPS). The vCNN model in run-time takes a total
20 ms, on average, for the blind test prediction, beneﬁted
by the instantaneous localization and mapping capability of
vSLAM, reaching a real-time speed of 50 FPS.

By understanding real-time surgical videos, the proposed
architecture has numerous potential medical applications,
such as the automatic indexing of surgical video databases,
the monitoring of surgical processes, the alerting of an up-
coming complication, the optimization of real-time operating
room scheduling, etc.

The real-time top performance of vCNN, via augmented
reality (AR), could be utilized in providing surgical training
and even in assisting real-time decisions. To prove the con-
cept, A Microsoft HoloLens 2 application is created to pro-
vide real-time surgical (video) analysis via AR as a proto-
type, presenting a new capability in healthcare applications,
including educational training and clinical practices. This ap-
plication provides a training simulator to interact with surgery
procedures in real-time AR, which should help trainees to
develop decision-making abilities within weeks rather than
months to years from long clinical practice; it is applicable
to remote learning and coaching as well.

6. CONCLUSIONS

This paper proposed a deep learning architecture that utilizes
visual features and spatio-temporal
information gathered
from videos. This method is based on a novel architecture
called vCNN, which integrates a vSLAM-based RPM with
CNN. It demonstrates that connecting visual features, spatial
information, and temporal information for the deep learn-
ing training can increase the accuracy of tool detection to
96.8 mAP and workﬂow detection to 97.5 mean Jaccard on
the surgical video datasets of M2CAI 2016 challenge. The
vSLAM process is capable of generating 3D maps from the
surgery videos recorded by a monocular camera. RPM can
effectively generate region proposals from vSLAM 3D maps
through constant tracking and localization, offering a high
performance solution to capture and present bounding boxes
in real-time, replacing RPN with comparable high accuracy
and removing the ground truth requirements for bounding
box annotations in the training datasets.

Future work includes continuing the expansion of re-
search to higher-quality video recordings via stereo or RGB-
D cameras, for better vSLAM/RPM performance, while also
exploring general object detection in computer vision for
biomedical imaging.

7. REFERENCES

[1] Blake C Alkire, Nakul P Raykar, Mark G Shrime,
Thomas G Weiser, Stephen W Bickler, John A Rose,
Cameron T Nutt, Sarah LM Greenberg, Meera Kotagal,
Johanna N Riesel, et al., “Global access to surgical care:

a modelling study,” The Lancet Global Health, vol. 3,
no. 6, pp. e316–e323, 2015.

[2] Andru P Twinanda, Sherif Shehata, Didier Mutter,
Jacques Marescaux, Michel De Mathelin, and Nicolas
Padoy, “Endonet: a deep architecture for recognition
IEEE transactions on
tasks on laparoscopic videos,”
medical imaging, vol. 36, no. 1, pp. 86–97, 2016.

[3] David Bouget, Max Allan, Danail Stoyanov, and Pierre
Jannin, “Vision-based and marker-less surgical tool de-
tection and tracking: a review of the literature,” Medical
image analysis, vol. 35, pp. 633–654, 2017.

[4] Alexey A Shvets, Alexander Rakhlin, Alexandr A
Kalinin, and Vladimir I Iglovikov, “Automatic instru-
ment segmentation in robot-assisted surgery using deep
learning,” in 2018 17th IEEE International Conference
on Machine Learning and Applications (ICMLA). IEEE,
2018, pp. 624–628.

[5] Sebastian Bodenstedt, Antonia Ohnemus, Darko Katic,
Anna-Laura Wekerle, Martin Wagner, Hannes Ken-
ngott, Beat M¨uller-Stich, R¨udiger Dillmann, and Ste-
fanie Speidel, “Real-time image-based instrument clas-
arXiv preprint
siﬁcation for laparoscopic surgery,”
arXiv:1808.00178, 2018.

[6] Maryam Gholinejad, Arjo J. Loeve, and Jenny Dankel-
man,
“Surgical process modelling strategies: which
method to choose for determining workﬂow?,” Mini-
mally Invasive Therapy & Allied Technologies, vol. 28,
no. 2, pp. 91–104, 2019.

[7] Takafumi Taketomi, Hideaki Uchiyama, and Sei Ikeda,
“Visual slam algorithms: a survey from 2010 to 2016,”
IPSJ Transactions on Computer Vision and Applica-
tions, vol. 9, no. 1, pp. 1–11, 2017.

[8] Raul Mur-Artal, Jose Maria Martinez Montiel, and
“Orb-slam: a versatile and accu-
IEEE transactions on

Juan D Tardos,
rate monocular slam system,”
robotics, vol. 31, no. 5, pp. 1147–1163, 2015.

[9] Tim Bailey and Hugh Durrant-Whyte, “Simultaneous
localization and mapping (slam): Part ii,” IEEE robotics
& automation magazine, vol. 13, no. 3, pp. 108–117,
2006.

[10] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian
Sun, “Faster r-cnn: Towards real-time object detection
with region proposal networks,” Advances in neural in-
formation processing systems, vol. 28, pp. 91–99, 2015.

[11] Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross
Girshick, “Mask r-cnn,” in Proceedings of the IEEE
international conference on computer vision, 2017, pp.
2961–2969.

[12] Manish Sahu, Anirban Mukhopadhyay, Angelika Szen-
“Tool and phase recogni-
arXiv preprint

gel, and Stefan Zachow,
tion using contextual cnn features,”
arXiv:1610.08854, 2016.

[13] Amy Jin, Serena Yeung, Jeffrey Jopling, Jonathan
Krause, Dan Azagury, Arnold Milstein, and Li Fei-Fei,
“Tool detection and operative skill assessment in surgi-
cal videos using region-based convolutional neural net-
works,” in 2018 IEEE Winter Conference on Applica-
tions of Computer Vision (WACV). IEEE, 2018, pp. 691–
699.

[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun, “Deep residual learning for image recognition,” in
Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 770–778.

[15] Kyungmin Jo, Yuna Choi, Jaesoon Choi, and Jong Woo
Chung, “Robust real-time detection of laparoscopic in-
struments in robot surgery using convolutional neural
networks with motion vector prediction,” Applied Sci-
ences, vol. 9, no. 14, pp. 2865, 2019.

[16] Remi Cadene, Thomas Robert, Nicolas Thome, and
Matthieu Cord, “M2cai workﬂow challenge: convolu-
tional neural networks with time smoothing and hidden
markov model for video frames classiﬁcation,” arXiv
preprint arXiv:1610.05541, 2016.

[17] Hassan Al Hajj, Mathieu Lamard, Pierre-Henri Conze,
B´eatrice Cochener, and Gwenol´e Quellec, “Monitoring
tool usage in surgery videos using boosted convolutional
and recurrent neural networks,” Medical image analysis,
vol. 47, pp. 203–218, 2018.

[18] Tong Yu, Didier Mutter, Jacques Marescaux, and Nico-
las Padoy, “Learning from a tiny dataset of manual an-
notations: a teacher/student approach for surgical phase
recognition,” arXiv preprint arXiv:1812.00033, 2018.

[19] Chinedu Innocent Nwoye, Didier Mutter,

Jacques
Marescaux, and Nicolas Padoy,
“Weakly supervised
convolutional lstm approach for tool tracking in laparo-
scopic videos,” International journal of computer as-
sisted radiology and surgery, vol. 14, no. 6, pp. 1059–
1067, 2019.

[20] Shinya Sumikura, Mikiya Shibuya, and Ken Sakurada,
in
“Openvslam: a versatile visual slam framework,”
Proceedings of the 27th ACM International Conference
on Multimedia, 2019, pp. 2292–2295.

[21] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-
Yen Lo, and Ross Girshick, “Detectron2. 2019,” URL
https://github. com/facebookresearch/detectron2, vol. 2,
no. 3, 2019.

