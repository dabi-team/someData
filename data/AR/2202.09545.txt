Springer Nature 2021 LATEX template

2
2
0
2

b
e
F
6
2

]

V
C
.
s
c
[

2
v
5
4
5
9
0
.
2
0
2
2
:
v
i
X
r
a

Going Deeper into Recognizing Actions in
Dark Environments: A Comprehensive
Benchmark Study

Yuecong Xu1‚Ä†, Jianfei Yang2*‚Ä†, Haozhi Cao2‚Ä†, Jianxiong
Yin3, Zhenghua Chen1, Xiaoli Li1, Zhengguo Li1
and Qianwen Xu4

1Institute for Infocomm Research, A*STAR, 1 Fusionopolis Way,
138632, Singapore.
2*School of Electrical and Electronic Engineering, Nanyang
Technological University, 50 Nanyang Avenue, 639798, Singapore.
3NVIDIA AI Tech Centre, 3 International Business Park Rd,
609927, Singapore.
4Department of Electric Power and Energy Systems, KTH Royal
Institute of Technology, Teknikringen 33, Stockholm, 114 28,
Sweden.

*Corresponding author(s). E-mail(s): yang0478@e.ntu.edu.sg;
Contributing authors: xuyu0014@e.ntu.edu.sg;
haozhi001@e.ntu.edu.sg; jianxiongy@nvidia.com;
chen0832@e.ntu.edu.sg; xlli@i2r.a-star.edu.sg;
ezgli@i2r.a-star.edu.sg; qianwenx@kth.se;
‚Ä†These authors contributed equally to this work.

Abstract

While action recognition (AR) has gained large improvements with
the introduction of large-scale video datasets and the development of
deep neural networks, AR models robust to challenging environments
in real-world scenarios are still under-explored. We focus on the task of
action recognition in dark environments, which can be applied to Ô¨Åelds
such as surveillance and autonomous driving at night. Intuitively, cur-
rent deep networks along with visual enhancement techniques should
be able to handle AR in dark environments, however, it is observed

1

 
 
 
 
 
 
Springer Nature 2021 LATEX template

2

Going Deeper into Recognizing Actions in Dark Environments

that this is not always the case in practice. To dive deeper into explor-
ing solutions for AR in dark environments, we launched the UG2+
Challenge Track 2 (UG2-2) in IEEE CVPR 2021, with a goal of evalu-
ating and advancing the robustness of AR models in dark environments.
The challenge builds and expands on top of a novel ARID dataset,
the Ô¨Årst dataset for the task of dark video AR, and guides models to
tackle such a task in both fully and semi-supervised manners. Base-
line results utilizing current AR models and enhancement methods are
reported, justifying the challenging nature of this task with substan-
tial room for improvements. Thanks to the active participation from
the research community, notable advances have been made in partici-
pants‚Äô solutions, while analysis of these solutions helped better identify
possible directions to tackle the challenge of AR in dark environments.

Keywords: Action recognition, Dark environments, Visual enhancements,
Neural networks, Fully-supervised learning, Semi-supervised learning

1 Introduction

The emergence of various large-scale video datasets, along with the continuous
development of deep neural networks have vastly promoted the development
of video-based machine vision tasks, with action recognition (AR) being one of
the spotlights. Recently, there have been increasing applications of automatic
AR in diverse Ô¨Åelds, e.g., security surveillance (Y.-L. Chen, Wu, Huang, & Fan,
2010; Ullah et al., 2021; Zou et al., 2019), autonomous driving (D. Cao & Xu,
2020; L. Chen et al., 2020; Royer, Lhuillier, Dhome, & Lavest, 2007), and smart
home (Fahad & Rajarajan, 2015; Feng, Setoodeh, & Haykin, 2017; Yang, Zou,
Jiang, & Xie, 2018). As a result, eÔ¨Äective AR models that are robust to the
diÔ¨Äerent environments are required to cope with the diÔ¨Äerent real-world sce-
narios. There has indeed been a signiÔ¨Åcant improvement in the performance of
AR models, reaching superior accuracies across various datasets (Ghadiyaram,
Tran, & Mahajan, 2019; Gowda, Rohrbach, & Sevilla-Lara, 2021; L. Wang,
Koniusz, & Huynh, 2019).

Despite the rapid progress made by current AR research, most research
aims to improve the model performance on existing AR datasets that are
constrained by several factors, one of which concerns the fact that videos in
existing datasets are shot under a non-challenging environment, with ade-
quate illumination and contrast. The existence of such constraints could lead
to the observable fragility of proposed methods, which are not capable to gen-
eralize well to adverse environments, including dark environments with low
illumination. Take security surveillance as an example: automated AR mod-
els could play a vital role in anomaly detection. However, anomaly actions
are more common at night time and in dark environments, yet current AR
models are obscured by darkness, and are unable to recognize any actions
eÔ¨Äectively. Autonomous systems are another example, where darkness has

Springer Nature 2021 LATEX template

Going Deeper into Recognizing Actions in Dark Environments

3

hampered the eÔ¨Äectiveness of onboard cameras so severely that most vision-
based autonomous driving systems are strictly prohibited at night (Brown,
2019), while those who do allow night operation could cause severe accidents
(Boudette, 2021).

The degrading performance of AR models in dark environments is emi-
nently related to the construction and collection of current large-scale AR
datasets (e.g. HMDB51 (Kuehne, Jhuang, Garrote, Poggio, & Serre, 2011),
UCF101 (Soomro, Zamir, & Shah, 2012), and Kinetics (Kay et al., 2017)),
which heavily rely on online video platforms (e.g., YouTube and Flickr). While
online video platforms contain a large variety of videos and support the collec-
tion of large-scale video datasets, videos on these platforms are generally shot
under normal illumination. Whereas in reality, poor illumination (darkness) is
quite common in many scenarios, such as surveillance at night and dark sce-
narios. Yet it is not covered properly in any existing datasets for training AR
models. Meanwhile, current AR models are predominantly data-driven and
are sensitive to unseen conditions. With the lack of low illumination videos in
current AR datasets, the degradation in current AR models is not unexpected.
To mitigate performance degradation of AR models in dark environments,
one intuitive method is to perform pre-processing of dark videos which could
improve the visibility of the dark videos. Such a method is indeed eÔ¨Äective from
the human vision perspective. Over the past decade, various visual enhance-
ment techniques (C. Guo et al., 2020; X. Guo, Li, & Ling, 2016; Li, Guo,
& Chen, 2021; Ying, Li, Ren, Wang, & Wang, 2017; Y. Zhang, Zhang, &
Guo, 2019) have been proposed to improve the visibility of degraded images
and videos, ranging from dehazing, de-raining to illumination enhancements.
Given the eÔ¨Äectiveness of deep neural networks in related tasks such as image
reconstruction, deep-learning based illumination enhancement methods have
also been developed with the introduction of various illumination enhancement
datasets (e.g., SID (C. Chen, Chen, Xu, & Koltun, 2018), ReNOIR (Anaya
& Barbu, 2018) and LOL dataset (J. Liu, Xu, Yang, Fan, & Huang, 2021)).
The results are reportedly promising from a human vision viewpoint, given
their capability in improving the visual quality of low-illumination images and
videos.

In spite of their capability in generating visually enhanced images and
videos, prior research has shown that a majority of illumination enhancement
methods are incapable of improving AR performance in dark videos consis-
tently. This is caused by two aspects: Ô¨Årst, most illumination enhancement
methods are developed upon low-illumination images, which are static and do
not contain motion information. For the few illumination enhancement video
datasets (e.g, DRV (C. Chen, Chen, Do, & Koltun, 2019)), videos collected
are also mostly static, with the ‚Äúground truth‚Äù of the dark videos shot by long
exposures. In contrast, actions in videos are closely correlated with motion
information, which is not generally included in current datasets for illumina-
tion enhancement. Second, current illumination enhancement datasets target
predominantly on human vision, with the evaluation of method based not only

Springer Nature 2021 LATEX template

4

Going Deeper into Recognizing Actions in Dark Environments

on quantitative evaluation but also on rather subjective qualitative evaluation
(e.g., US (C. Guo et al., 2020) and PI scores (Blau & Michaeli, 2018; Ma,
Yang, Yang, & Yang, 2017; Mittal, Soundararajan, & Bovik, 2012)). Quantita-
tive evaluation of illumination enhancement methods is also based mostly on
the quality of the image/video (e.g., PSNR) instead of the understanding of
image/video (e.g., classiÔ¨Åcation and segmentation). The misalignment between
the target of applying illumination enhancements to dark videos for AR and
that of the illumination enhancement datasets would therefore be unable to
guide illumination enhancement methods to improve on AR accuracies in dark
videos.

To apply AR models in real-world practical applications, the model is
expected to be robust to videos shot in all environments, including the chal-
lenging dark environments. In view of the inability of current solutions in
addressing AR in dark environments, it is therefore highly desirable to conduct
comprehensive research on eÔ¨Äective methods to cope with such challenging
environments. Such research could enable models to handle real-world dark
scenarios, and beneÔ¨Åt in all Ô¨Åelds such as security and autonomous driving.

To bridge the gap between the lack of research in AR models robust to
dark environments and the wide application in real-world scenarios of such
research, we propose the UG2+ Challenge Track 2 in IEEE CVPR 2021. The
UG2+ Challenge Track 2 (UG2-2) aims to evaluate and advance the robustness
of AR models in poor visibility environments, focusing on dark environments.
SpeciÔ¨Åcally, UG2-2 is structured into two sub-challenges, featuring diÔ¨Äerent
actions and diverse training protocols. UG2-2 is built on top of a novel AR
dataset: ARID, which is a collection of realistic dark videos dedicated to AR.
UG2-2 further expands the original ARID, strengthening its capability of guid-
ing models in recognizing actions in dark environments. More speciÔ¨Åc dataset
details and evaluation protocols are illustrated in Section 3.1. Compare with
previous works and challenges, UG2-2 and its relevant datasets include the
following novelties:
‚Ä¢ Addressing Videos from Dark Environments: The dataset utilized in UG2-2
is the Ô¨Årst video dataset dedicated to action recognition in the dark. The
original dataset with its expansion is collected from real-world scenarios. It
provides much-needed resources to research actions captured in the challeng-
ing dark environments, and to design eÔ¨Äective recognition methods robust
towards dark environments.

‚Ä¢ Covering Fully and Semi-Supervised Learning: The two sub-challenges in
UG2-2 are structured to cover both fully supervised learning (UG2-2.1) and
semi-supervised learning (UG2-2.2). To the best of our knowledge, this is
the Ô¨Årst challenge that involves semi-supervised learning of dark videos.
While our dataset provides resources for AR in dark environments, more
feasible and eÔ¨Écient strategies to learn robust AR models is to adapt or
generalize models learnt in non-challenging environments (which usually are

Springer Nature 2021 LATEX template

Going Deeper into Recognizing Actions in Dark Environments

5

of larger scale) to the dark environments. In this sense, our challenge pro-
motes research into leveraging current datasets to boost performance on
dark videos.

‚Ä¢ Greatly Challenging: Compare with conventional AR datasets

(e.g.,
UCF101), the dataset utilized in the fully supervised sub-challenge is of small
scale. Yet the winning solution of this sub-challenge achieves a performance
inferior to that in UCF101. Meanwhile, even though the cross-domain video
dataset used in the semi-supervised sub-challenge is comparable to conven-
tional cross-domain video dataset (i.e., UCF-HMDB (Sultani & Saleemi,
2014)), the winning solution performance is also inferior to that achieved
in UCF-HMDB. Performances of second runner-up solutions of the semi-
supervised sub-challenge are of a large gap away from the winning solution.
The results prove that our datasets are greatly challenging with a large room
for further improvements.

The rest of this article is organized as follows: Section 2 reviews previous
action recognition and dark visual datasets, as well as various action recogni-
tion methods. Section 3 introduces the details of the UG2-2 challenge, with its
dataset, evaluation protocol and baseline results. Further, Section 4 illustrates
the results of the competition and related analysis, while brieÔ¨Çy discussing
the reÔ¨Çected insights as well as possible future developments. The article is
concluded in Section 5.

2 Related Works

2.1 Large-Scale Datasets

Various datasets have been proposed to advance the development of video
action recognition (AR). Earlier datasets (e.g. KTH (Schuldt, Laptev, &
Caputo, 2004), Weizmann (Gorelick, Blank, Shechtman, Irani, & Basri, 2007),
and IXMAS (Weinland, Boyer, & Ronfard, 2007)) comprise a relatively small
number of action classes. The videos in these datasets were recorded oÔ¨Ñine per-
formed by several actors under limited scenarios. For example, KTH (Schuldt
et al., 2004) includes six diÔ¨Äerent action classes performed by 25 actors under
4 diÔ¨Äerent scenarios. With the advancing performance of deep-learning-based
methods, there has been an urging demand for larger and more complicated
datasets. To address this issue, subsequent datasets, such as HMDB51 (Kuehne
et al., 2011) and UCF101 (Soomro et al., 2012), have been proposed by collect-
ing videos from more action classes and more diverse scenarios. SpeciÔ¨Åcally,
HMDB51 (Kuehne et al., 2011) is constructed with videos of 51 action classes
collected from a variety of sources from movies to online video platforms, while
UCF101 (Soomro et al., 2012) is a larger dataset, consisting of 101 diÔ¨Äerent
actions collected from user-uploaded videos.

Both HMDB51 (Kuehne et al., 2011) and UCF101 (Soomro et al., 2012)
have served as the standard benchmark of AR, while they possess insuÔ¨Écient
data variation to train deep models, mainly because they contain multiple

Springer Nature 2021 LATEX template

6

Going Deeper into Recognizing Actions in Dark Environments

clips sampled from the same video. To address this issue, larger datasets with
more variation have been proposed. One of the most representative examples
is the famous Kinetics-400 (Kay et al., 2017). The Kinetics-400 incorporates
306,245 clips from 306,245 videos (i.e. each clip is from a diÔ¨Äerent video) in 400
action classes. There are at least 400 clips within each class, which guarantees
more inner-class variety compared to other datasets. The following versions
of Kinetics dataset, including Kinetics-600 (Carreira, Noland, Banki-Horvath,
Hillier, & Zisserman, 2018) and Kinetics-700 (Carreira, Noland, Hillier, & Zis-
serman, 2019), have also been collected abiding a similar protocol. In addition
to Kinetics datasets, many large-scale datasets are presented to increase the
variety of samples from diÔ¨Äerent perspective, such as Something-Something
(Goyal et al., 2017) for human-object interactions, AVA (Gu et al., 2018) for
localized actions, Moments-in-Time (Monfort et al., 2019) for both visual and
auditory information. While the emerging large-scale datasets push the perfor-
mance limit of deep models, most of them are mainly collected from internet
or shot under normal illuminations.

2.2 Dark Visual Datasets

There have been emerging research interests towards high-level tasks in low-
illumination environments in the Ô¨Åeld of computer vision. This increasing
attention leads to a number of image-based datasets in dark environments.
The earlier datasets were mainly designed for image enhancement or restora-
tion, which include LOL (Wei, Wang, Yang, & Liu, 2018), SID (C. Chen et al.,
2018), ExDARK (Loh & Chan, 2019) and DVS-Dark (S. Zhang et al., 2020).
SpeciÔ¨Åcally, LOL (Wei et al., 2018) and SID (C. Chen et al., 2018) consist
of pairs of images shot under diÔ¨Äerent exposure time or ISO, while ExDARK
(Loh & Chan, 2019) contains images collected from various online platforms.
DVS-Dark consists of event images instead of RGB images, which can respond
to changes in brightness, and the recent work (Lv, Li, & Lu, 2021) proposed
to further extend the scale of the dataset by introducing synthetic low-light
images. These research interests have also expanded to the video domain. Sev-
eral video datasets, such as DRV (C. Chen et al., 2019) and SMOID (Jiang &
Zheng, 2019), have been proposed speciÔ¨Åcally for low-light video enhancement,
which include raw videos captured in dark environments and corresponding
noise-free videos obtained by using long-exposure. However, these datasets
mainly encompass static scenes with trivial dynamic motion and therefore are
not suitable for AR which signiÔ¨Åcantly relies on motion information. Further-
more, both datasets are of small scales (e.g. 179 samples for DRV (C. Chen et
al., 2019) and 202 samples for SMOID (Jiang & Zheng, 2019)). In this paper,
we introduce the ARID dataset containing more samples of various actions as
our evaluation benchmark.

Springer Nature 2021 LATEX template

Going Deeper into Recognizing Actions in Dark Environments

7

2.3 Action Recognition Methods

In the era of deep learning, early state-of-the-art AR methods are fully super-
vised methods mainly based on either 3D CNN (Ji, Xu, Yang, & Yu, 2012) or
2D CNN (Karpathy et al., 2014). 3D CNN (Ji et al., 2012) attempts to jointly
extract the spatio-temporal features by expanding the 2D convolution kernel to
the temporal dimension, while this expansion suÔ¨Äers from high computational
cost. To alleviate this side eÔ¨Äect, subsequent works, such as P3D (Qiu, Yao, &
Mei, 2017) and R(2+1)D (Tran et al., 2018), improve the eÔ¨Éciency by replacing
3D convolution kernels with pseudo 3D kernels. As for 2D CNN, due to the lack
of temporal features, early 2D-based methods (Simonyan & Zisserman, 2014)
usually require additional hand-crafted features as input (e.g. optical Ô¨Çow) to
represent the temporal information. More recent methods attempt to model
the temporal information in a learnable manner. For example, TSN (L. Wang
et al., 2016) proposed to extract more abundant temporal information by uti-
lizing a sparse temporal sampling strategy. SlowFast networks (Feichtenhofer,
Fan, Malik, & He, 2019) proposed to utilize dual pathways with slow or high
temporal resolutions to extract spatial or temporal features, respectively.

The outstanding performance of fully supervised methods mainly relies
on large-scale labeled datasets, whose annotations are resource-expensive.
Moreover, networks trained in the fully-supervised manner suÔ¨Äer from poor
transferability and generalization. To increase the eÔ¨Éciency and generaliza-
tion of extracted features, some works proposed to utilize semi-supervised
approaches, such as self-supervised learning (Fernando, Bilen, Gavves, &
Gould, 2017; J. Wang, Jiao, & Liu, 2020; D. Xu et al., 2019; Yao, Liu,
Luo, Zhou, & Ye, 2020) and Unsupervised Domain Adaptation (UDA) (Choi,
Sharma, Schulter, & Huang, 2020; Munro & Damen, 2020; B. Pan, Cao, Adeli,
& Niebles, 2020; Y. Xu, Yang, Cao, Chen, et al., 2021). Self-supervised learn-
ing is designed to extract eÔ¨Äective video representation from unlabeled data.
The core of self-supervised learning is to design a pretext task to generate
supervision signals through the characteristic of videos, such as frame orders
(Fernando et al., 2017; D. Xu et al., 2019) and play rates (J. Wang et al., 2020;
Yao et al., 2020). On the other hand, UDA aims to extract the transferable
representation across the labeled data in the source domain and the unlabeled
data in the target domain. Compared to image-based UDA methods (Busto,
Iqbal, & Gall, 2018; Ganin & Lempitsky, 2015; Ganin et al., 2016), there exists
fewer works in the Ô¨Åeld of video-based UDA (VUDA). (B. Pan et al., 2020) is
one of the primary works focusing on VUDA, which attempts to address the
temporal misalignment by introducing a co-attention module across the tem-
poral dimension. (Munro & Damen, 2020) further leverages the multi-modal
input of video to tackle VUDA problem. SAVA (Choi et al., 2020) proposed an
attention mechanism to attend to the discriminate clips of videos and PATAN
(Y. Xu, Yang, Cao, Chen, et al., 2021) further expanded the UDA problem
to a more general partial domain adaption problem. In this work, we struc-
tured sub-challenges by covering both fully-supervised and semi-supervised to
inspire novel AR methods in poor visibility environments.

Springer Nature 2021 LATEX template

8

Going Deeper into Recognizing Actions in Dark Environments

Fig. 1 (a) Bar charts of the RGB mean (left) and standard deviation (right) values for
ARID and the expanded dataset for UG2-2: ARID-plus. The statistics for the two sub-
challenges UG2-2.1 and UG2-2.2 are separated. All values are normalized to the range of
[0.0. 1.0]. (b) The distribution of clips among action classes in ARID-plus (UG2-2.1). (c) The
distribution of clips among action classes in ARID-plus (UG2-2.2). For (b) and (c), the blue
and red bars indicate the number of clips in the training/validation and testing partitions.
Best viewed in color.

3 Introduction of UG2+hallenge Track 2

The UG2+ Challenge Track 2 (UG2-2) aims to evaluate and advance the
robustness of AR methods in dark environments. In this section, we detail the
datasets and evaluation protocols used in UG2-2, as well as the baseline results
for either sub-challenges. The datasets of UG2-2 for either sub-challenges are
built based on the Action Recognition In the Dark (ARID) dataset. We begin
this section by a brief review of the ARID dataset.

3.1 The ARID Dataset

The ARID dataset (Y. Xu, Yang, Cao, Mao, et al., 2021) is the Ô¨Årst video
dataset dedicated to action recognition in dark environments. The dataset is
a collection of videos shot by commercial cameras in dark environments, with
actions performed by 11 volunteers. In total, it comprises 11 action classes,
including both Singular Person Actions (i.e., jumping, running, turning, walk-
ing, and waving) as well as Actions with Objects (i.e., drinking, picking,
pouring, pushing, sitting, and standing). The dark videos are shot in both

3552242424093733342091891592282562480100200300400500600700RunSitStandTurnWalkWaveTrain/ValTest(b)310392204345362159125931531920100200300400500600DrinkJumpPickPourPushTrain/ValTest(c)0.079610.073080.066930.073890.067280.060270.078420.065940.0612600.050.1R MeanG MeanB MeanARIDARID-plus(UG2-2.1)ARID-plus(UG2-2.2)0.100460.098410.090160.097050.093580.086610.08990.082180.0806900.050.10.15R StdG StdB StdARIDARID-plus(UG2-2.1)ARID-plus(UG2-2.2)(a)Springer Nature 2021 LATEX template

Going Deeper into Recognizing Actions in Dark Environments

9

Fig. 2 Sampled frames from (a) the train/validation set of ARID-plus (UG2-2.1), (b) the
hold-out test set of ARID-plus (UG2-2.1), (c) the train/validation set of ARID-plus (UG2-
2.2) and (d) the hold-out test set of ARID-plus (UG2-2.2). Note that all sampled frames
from dark videos have been tuned much brighter for visualization.

indoor and outdoor scenes with varied lighting conditions. The dataset con-
sists of a total of 3,784 video clips, with the minimum action class containing
205 video clips. The clips of every action class are divided into clip groups
according to the diÔ¨Äerent actors and scenes. Similar to previous action recog-
nition datasets (e.g., HMDB51 (Kuehne et al., 2011) and UCF101 (Soomro
et al., 2012)), three train/test splits are selected, with each split partitioned
according to the clip groups, with a ratio of 7 : 3. The splits are selected to
maximize the possibility that each clip group is presented in either the train-
ing or testing partition. All video clips in ARID are Ô¨Åxed to a 30 FPS frame
rate, and a uniÔ¨Åed resolution of 320 √ó 240. The overall duration of all video
clips combined is 8,721 seconds.

Though the ARID dataset pioneers the investigation of action recogni-
tion methods in dark environment, it has its own limitations. Compared with
current SOTA benchmarks such as Kinetics (Kay et al., 2017) and Moments-
in-Time (Monfort et al., 2019), the ARID is of limited scale, especially in
terms of the number of videos per class. The limited scale of ARID prohibits
complex deep learning methods to be trained, owing to a higher risk of over-
Ô¨Åtting. Increasing the dataset scale is an eÔ¨Äective solution for such constraint,
given that conventional action recognition dataset follows the same develop-
ment path. However, the collection and annotation of dark videos is of high
cost, given that there is limited public dark video on any public video plat-
forms. Therefore, the strategy of increasing the dataset scale could only bring
limited improvement to the dataset. Given the vast availability of videos shot
in non-challenging environments, such videos should be fully utilized to train
transferable models that could generalize to dark videos. To this end, we intro-
duce a comprehensive extension of the ARID dataset: ARID-plus, to address
the issues of the original ARID dataset, and serve as the datasets for the two
sub-challenges of UG2-2.

3.2 Fully Supervised Action Recognition in the Dark

To equip AR models the ability to cope with dark environments for applica-
tions such as night surveillance, the most intuitive method would be no other
than training action models in a fully supervised manner with videos shot

(a)(b)(c)(d)Springer Nature 2021 LATEX template

10

Going Deeper into Recognizing Actions in Dark Environments

Table 1 Baseline Results of AR models without enhancements for UG2-2.1.

Input

RGB

Optical
Flow

Two-
stream

Models
I3D-RGB
3D-ResNet-50
3D-ResNeXt-101
TSM
SlowOnly
X3D-M

I3D-OF
SlowOnly-OF

I3D-TS
SlowOnly-TS

Top-1
Top-5
21.64% 85.42%
34.14% 94.49%
34.45% 96.82%
26.37% 87.82%
27.08% 91.00%
20.25% 87.98%

20.56% 85.18%
57.25% 96.82%

21.41% 85.26%
50.61% 96.74%

in the dark, which motivates the construction of Sub-Challenge 1. The Ô¨Årst
component of ARID-plus serves as the dataset of Sub-Challenge 1 of UG2-2
(UG2-2.1), where participants are given the annotated dark videos for fully
supervised action recognition. A total of 1,937 real-world dark video clips cap-
turing actions by volunteers are adopted as the training and/or validation
sets, with the recommended train/validation split provided to participants.
The video clips contain six categories of actions, i.e., run, sit, stand, turn,
walk, and wave. For testing, a hold-out set with 1,289 real-world dark video
clips are provided, collected with similar methods as the training/validation
video clips, with the same classes. In total, there are a minimum of 456 clips
for each action. A detailed distribution of train(validation)/test video clips is
shown in Fig. 1(b). During training, participants can optionally use pre-trained
models (e.g., models pretrained on ImageNet (Deng et al., 2009) or Kinetics),
and/or external data, including self-synthesized or self-collected data. If any
pre-trained model or external data is used, participants must state explicitly
in their submissions. The participants are ranked by the top-1 accuracy of the
hold-out test set, while all the solutions of candidate winners are tested for
their reproducibility.

The video clips adopted for training and testing in UG2-2.1 include that
in the original ARID dataset, as well as new video clips. Several changes are
adopted during the collection of the new video clips. Firstly, the new video
clips are shot in a number of new scenes, whose visibility is even lower. This is
justiÔ¨Åed statistically by lower RGB mean values and standard deviation (std)
values as depicted in Fig. 1(a). Secondly, videos collected in the original ARID
dataset follow a 4 : 3 aspect ratio, which matches standard 320p or 480p videos.
Meanwhile, the currently more common High-DeÔ¨Ånition (HD) videos would
have a 16 : 9 aspect ratio, with larger view angles. Following the aspect ratio
of HD videos, the new video clips are Ô¨Åxed to a resolution of 426 √ó 240. We
have also extended the length of each video clip, from an average 2.3 seconds
per clip for clips in the original ARID to an average of 4 seconds per clip
for the new video clips. Sampled frames from the train/validation set and the
hold-out test set are displayed in Fig. 2(a) and Fig. 2(b).

Springer Nature 2021 LATEX template

Going Deeper into Recognizing Actions in Dark Environments

11

Table 2 Baseline Results of AR models with various oÔ¨Ä-the-shelf enhancements for
UG2-2.1.

Enhancements
GIC
LIME
Zero-DCE
StableLLVE
None

I3D-RGB 3D-ResNet-50

19.08%
18.39%
18.39%
19.86%
21.64%

39.48%
39.02%
48.80%
46.24%
26.37%

3D-ResNeXt-101
50.50%
33.75%
60.28%
34.45%
34.14%

TSM SlowOnly X3D-M
18.00%
37.47%
26.84%
18.00%
27.54%
27.54%
19.08%
27.60%
23.20%
17.84%
30.41%
29.64%
20.25%
27.08%
34.45%

Fig. 3 Comparison of (a) a sampled frame from ARID-plus with the results after applying
enhancements: (b) GIC, (c) LIME, (d) Zero-DCE, and (e) StableLLVE.

3.3 Semi-Supervised Action Recognition in the Dark

While fully supervised training in dark videos allow models to cope with
dark environments directly, publicly available dark videos are scarce com-
pared with the vast amount of normal illuminated videos, which could be
obtained with ease. Due to the high cost of both video collection and anno-
tation, simply increasing the scale of dark video datasets for improving the
eÔ¨Äectiveness of fully supervised learning would not be a feasible strategy.
Alternatively, the large amount of normal illuminated videos presented in
previous public datasets should be utilized to train transferable models that
could be generalized to dark videos. Such transfer may be further boosted
with certain frame enhancements. The above strategy could be regarded as
a semi-supervised learning strategy for action recognition in dark videos,
which motivates the design of Sub-Challenge 2. The Sub-Challenge 2 of UG2-
2 (UG2-2.2) is designed to guide participants to tackle action recognition
in dark environments in a semi-supervised manner, achieved by generalizing
models learnt in non-challenging environments to the challenging dark environ-
ments. To this end, the participants are provided with a subset of the labeled
HMDB51 (Kuehne et al., 2011) that includes 643 videos from Ô¨Åve action classes
(i.e., drink, jump, pick, pour, and push), for the training of models in non-
challenging environments. Meanwhile, to facilitate the transfer of models, the
second component of ARID-plus, with a total of 1,613 dark video clips, is pro-
vided to the participants in an unlabeled manner, which can be optionally
used at the participants‚Äô discretion for training and validation. The 1,613 clips

(a)(d)(e)(b)(c)Springer Nature 2021 LATEX template

12

Going Deeper into Recognizing Actions in Dark Environments

Table 3 Baseline Results of AR models with various domain adaptation methods for
UG2-2.2.

Domain Adaptation

I3D

3D-ResNet-50

3D-ResNeXt-101

Source-only

DANN
MK-MMD
MCD

Target-only

26.38%

35.18%
35.32%
27.15%

46.25%

24.10%

36.14%
26.90%
29.36%

75.07%

27.42%

44.04%
32.55%
28.25%

76.87%

contain the same Ô¨Åve categories of actions. Similar to UG2-2.1, a hold-out set
containing 722 real-world dark video clips with the same classes is provided for
testing. Overall, there are at least 297 clips for each action class. The detailed
distribution of train(validation)/test dark video clips is shown in Fig. 1(c).

During training, participants can also optionally use pre-trained models,
and/or external data, including self-synthesized or self-collected data. How-
ever, the 1,613 dark video clips provided during the training/validation phase
are not allowed to be manually labeled for training (i.e., they must remain
to be unlabeled). Participants are to state explicitly if any pre-trained model
or external data is used, and are ranked by the top-1 accuracy of the hold-
out test set with reproducibility subject to testing if the relevant solution‚Äôs
testing accuracy stands out. Changes in extra data that have been applied in
UG2-2.1 have also been employed in the extra dark video clips for UG2-2.2.
Such changes result in a similar degradation of clip visibility (as depicted in
Fig. 1(a)), and an increase in view angles and average clip length. We show
the sampled frames from the labeled HMDB51 train set, the unlabeled dark
train/validation set, as well as the hold-out test set in Fig. 2(c) and Fig. 2(d).

3.4 Baseline Results and Analysis

For both sub-challenges, we report baseline results utilizing oÔ¨Ä-the-shelf
enhancement methods with Ô¨Åne-tuning of several popular pre-trained action
recognition models and domain adaptation methods. It should be noted that
these enhancement methods, pre-trained models and domain adaptation meth-
ods are not designed speciÔ¨Åcally for dark videos, hence they are by no means
very competitive, and performance boosts are expected from participants.

3.4.1 Fully Supervised UG2-2.1 Baseline Results

For UG2-2.1, we report baseline results from a total of six AR models including:
I3D (Carreira & Zisserman, 2017), 3D-ResNet-50 (Hara, Kataoka, & Satoh,
2018), 3D-ResNeXt-101 (Hara et al., 2018), TSM (Lin, Gan, & Han, 2019),
SlowOnly (Feichtenhofer et al., 2019), and X3D-M (Feichtenhofer, 2020).
Among which, RGB frames are utilized as the input for all methods, while we
also report the results utilizing optical Ô¨Çow obtained through TV-L1 (Zach,
Pock, & Bischof, 2007) for I3D and SlowOnly methods, along with the results
by class score fusion (Simonyan & Zisserman, 2014) with both RGB frames

Springer Nature 2021 LATEX template

Going Deeper into Recognizing Actions in Dark Environments

13

and optical Ô¨Çow. Meanwhile, applying enhancement methods which improve
the visibility of dark videos is an intuitive method to improve AR accuracies.
Therefore, we also evaluate the above methods using RGB input with four
enhancement methods: Gamma Intensity Correction (GIC), LIME (X. Guo et
al., 2016), Zero-DCE (C. Guo et al., 2020) and StableLLVE (F. Zhang, Li,
You, & Fu, 2021). All AR models and enhancement methods adopt the oÔ¨É-
cially released versions when applicable, where all learning-based methods are
written with the PyTorch (Paszke et al., 2019) framework. All AR models are
Ô¨Åne-tuned from their models pre-trained on Kinetics-400 (Kay et al., 2017),
and trained for a total of 30 epochs. Due to the constraints in computation
power, the batch size is uniÔ¨Åed for all models and set to 8 per GPU. All exper-
iments are conducted with two NVIDIA RTX 2080Ti GPUs. The reported
results are an average of Ô¨Åve experiments. The detailed results are found in
Table 1 and Table 2.

Overall, with the training settings as introduced above, current AR models
performs poorly without any enhancements in UG2-2.1. The best performance
is achieved by using optical Ô¨Çow input with the SlowOnly model, i.e., an accu-
racy of 57.25%. In comparison, the evaluated models could achieve at least
70% accuracy on the large-scale Kinetics dataset, and over 80% accuracy on
the HMDB51 dataset. It is worth noting that newer models (e.g., X3D-M)
which produce SOTA results on large-scale datasets may perform inferior to
previous models (e.g., 3D-ResNeXt-101). Therefore novel AR models may not
be more generalizable than prior AR models.

Meanwhile, the results after applying enhancements show that the evalu-
ated enhancements may not bring consistent improvements in action recogni-
tion accuracy. The evaluated enhancements all produce visually clearer videos,
where actions are more recognizable by humans, as shown in Fig. 3. The
actor who is running can be seen visually in all sampled frames with enhance-
ments, while the actor is almost unrecognizable in the original dark video.
However, at least three AR models produce inferior performance when apply-
ing any enhancement. The best result is obtained with 3D-ResNeXt-101 while
applying Zero-DCE enhancement. In general, Zero-DCE results in the best
average improvement of 5.57%. Meanwhile, the susceptibility of each model
varies greatly. 3D-ResNet-50 gains the most positive eÔ¨Äect of 17.02% average
accuracy gain with enhancements applied, while TSM is most susceptible to
negative eÔ¨Äects with an average loss of 7.65% accuracy.

We argue that the negative eÔ¨Äect of applying enhancements results from
the noise brought by enhancements. Though enhanced videos are clearer from
human perspectives, some enhancements break the original data distribution,
and can therefore be regarded as artifacts or adversarial attacks for videos.
The change in data distribution and the addition of noise could result in a
notable decrease in performance for AR models. The deÔ¨Åciencies of the exam-
ined enhancements suggest that simple integration of frame enhancements may
not be suÔ¨Écient. Instead, other techniques such as domain adaptation or self-
supervision could be further employed to improve the eÔ¨Äectiveness of frame
enhancements.

Springer Nature 2021 LATEX template

14

Going Deeper into Recognizing Actions in Dark Environments

3.4.2 Semi-Supervised UG2-2.2 Baseline Results

For UG2-2.2, we report baseline results with three AR models: I3D, 3D-
ResNet-50, and 3D-ResNeXt-101. To transfer networks from the labeled
normal videos to unlabeled dark videos, we employ and evaluate three diÔ¨Äerent
domain adaptation methods: the adversarial-based DANN (Ganin & Lempit-
sky, 2015), and the discrepancy-based MK-MMD (Long, Cao, Wang, & Jordan,
2015) and MCD (Saito, Watanabe, Ushiku, & Harada, 2018). We also examine
both the source-only scenario (i.e., fully supervised learning) and target-only
scenario (i.e., without any domain adaptation method). Similar to the base-
line experiments in UG2-2.1, all models are pre-trained on Kinetics-400, with
the whole training process set to 30 epochs. For all AR models, we freeze the
Ô¨Årst three convolutional layers, and the batch size is set to 8 per GPU. The
experiments are conducted with the same hardware and framework as that of
UG2-2.1 baselines. No enhancement method is employed when conducting the
baseline experiments for UG2-2.2. The reported results are an average of Ô¨Åve
experiments. Detailed results are shown in Table 3.

The results in Table 3 imply that though all three adaptation methods
can improve the generability of the respective AR models, scoring higher than
the source-only scenarios, all have a large gap towards the target-only accu-
racies, which are the upper bounds of the networks‚Äô performances. The large
performance gap towards the upper bound also justiÔ¨Åes the fact that there
exists a large domain gap between videos shot in non-challenging environments
and videos shot in dark environments. Among the three adaptation methods,
DANN produces the best performance in general, resulting in an average per-
formance gain of 12.82% towards the models‚Äô source only performances. The
best baseline result is obtained with 3D-ResNeXt while applying DANN as
the domain adaptation method. It should be noted that no enhancements or
other training tricks are applied when obtaining the baseline results for UG2-
2.2. Therefore, it is expected that participants could score higher than the
target-only accuracies in Table 3.

4 Results and Analysis

A total of 34 teams registered in the UG2+ Challenge Track 2 (UG2-2) at
CVPR, among which 25 and 12 teams submitted their results to the fully
supervised sub-challenge (UG2-2.1) and the semi-supervised sub-challenge
(UG2-2.2), respectively. For each sub-challenge, the team with the highest per-
formance is selected as the winner. In this section, we summarize the technical
details of some outstanding performers and compare them with our baseline
results. The full leaderboards can be found in the website‚àó.

‚àóhttps://cvpr2021.ug2challenge.org/leaderboard21 t2.html

Springer Nature 2021 LATEX template

Going Deeper into Recognizing Actions in Dark Environments

15

(a) One-stream Structure

(b) Two-stream Structure i

(c) Two-stream Structure ii
Fig. 4 Structures utilized in UG2-2.1. Here vRGB, vf low are vector features extracted from
RGB and Ô¨Çow input, respectively. p is the prediction of the Ô¨Ånal classiÔ¨Åer.

4.1 UG2-2.1: Fully Supervised Learning

Among the 25 teams that successfully participated in this sub-challenge, 11
teams proposed novel models that outperform our baseline results. Among
them, 7 teams are included in our leaderboard, where the winner team
AstarTrek achieved the best performance of 93.72%. While all teams con-
structed their models based on complex backbones, some interesting obser-
vations are as follows: (i) besides RGB, 3 out of 6 teams in the leaderboard
utilized additional optical Ô¨Çow as input, while this extra modality did not
bring solid improvement compared to those using pure RGB input; (ii) teams
achieving top performance utilized low-light enhancement methods; (iii) except
for the winner team, all teams trained their model from scratch with large
epoch numbers (more than 200) rather than utilizing other pre-trained models,
surpassing our baseline results by at least 18.83%.

Enhance & SamplingRGB StreamRaw VideoInput Clipùë£ùëÖùê∫ùêµClassifierùëùEnhance & SamplingFlow StreamRaw VideoInput Clipùë£ùëìùëôùëúùë§Ensemble & ClassifierùëùRGB Streamùë£ùëÖùê∫ùêµRGBOptical ExtractionEnhance & SamplingFlow StreamRaw VideoInput Clipùë£ùëìùëôùëúùë§Ensemble & ClassifierùëùRGB Streamùë£ùëÖùê∫ùêµOptical ExtractionSpringer Nature 2021 LATEX template

16

Going Deeper into Recognizing Actions in Dark Environments

The winner team AstarTrek adopted a two-stream structure as shown in
Fig. 4b. The team Ô¨Årst utilized the Gamma Intensity Correction (GIC) with
Œ≥ = 3 to enhance the illumination level of videos. Subsequently, both RGB and
optical Ô¨Çow were generated as the input of the two-stream structure. SpeciÔ¨Å-
cally, the SlowFast Network (Feichtenhofer et al., 2019) (based on ResNet 50
(K. He, Zhang, Ren, & Sun, 2016)) pretrained on Kinetics-400 (K400) was
adopted as the RGB stream to extract spatial features from raw RGB input.
For the Ô¨Çow stream, the team utilized ResNet-50-based I3D (Carreira & Zisser-
man, 2017) pretrained on K400 to extract temporal information from optical
Ô¨Çow. During the training process, the team adopted a two-stage procedure,
where each stream was trained independently to ensure that each of them can
provide reliable predictions by itself. Each stream was trained with stochas-
tic gradient descent (SGD) with a momentum of 0.9 and a weight decay of
0.0001. The batch size was set to 32 and the initial learning rate was 0.001,
decayed by a factor of 0.1 at epochs 60 and 100 (with total epochs of 800).
Each input (RGB or optical Ô¨Çow) was Ô¨Årst resized to a square of the height
randomly sampled from [224, 288], then randomly cropped into a square of
size 224√ó224, followed by a horizontal Ô¨Çip with a probability of 0.5. During
inference, each input (RGB or optical Ô¨Çow) was resized to the size of 240√ó300.
The Ô¨Ånal prediction is the average of results from both streams.

On the other hand, the runner-up team ArtiÔ¨Åcially Inspired adopted diÔ¨Äer-
ent backbones and strategies, achieving a competitive performance of 92.32%.
As shown in Fig. 4a, taking pure RGB as input, the team utilized Zero-
DCE (C. Guo et al., 2020) as their enhancement method and R(2+1)D-Bert
(Kalfaoglu, Kalkan, & Alatan, 2020) as their single stream backbone. In
fact, they are the only participant in the leaderboards that utilized deep-
model-based method to improve the quality of dark videos. Moreover, noticing
samples in ARID containing a relatively small number of frames, the team
utilized Delta Sampling strategy that constructed the input sample by various
sample rates while avoiding loop sampling. The team utilized 4,500 diÔ¨Äer-
ent images to train the Zero-DCE model, where 2,500 images were randomly
sample from ARID dataset and the others are of diÔ¨Äerent illumination levels
collected from other datasets. During the training process, videos were Ô¨Årst
enhanced by the frozen Zero-DCE model to enhance their light levels and then
resized to 112√ó112. The team also included a random horizontal Ô¨Çip and rota-
tion to increase the variation of input samples. According to their ablation
studies, the utilization of Zero-DCE can bring an improvement of 2.98% and
the proposed sampling strategy surpassed other alternatives. More technical
details can be refer to their report (Hira, Das, Modi, & Pakhomov, 2021).

Besides AstarTrek, there are two teams in the leaderboard, Cogvengers and
MKZ5, which attempted to leverage the optical Ô¨Çow as the additional input
to improve the performance. However, their performance is surpassed by most
of teams taking pure RGB input by more than 2.5%, mainly because they uti-
lized inferior strategies for extracting and processing optical Ô¨Çow. SpeciÔ¨Åcally,
MKZ5 utilized a diÔ¨Äerent two-stream structure as shown in Fig. 4c, which

Springer Nature 2021 LATEX template

Going Deeper into Recognizing Actions in Dark Environments

17

directly extracted optical Ô¨Çow from dark videos, while AstarTrek extracted
optical Ô¨Çow from enhanced videos. The direct extraction may end up a worse
quality of optical Ô¨Çow since most of the optical Ô¨Çow estimation methods show
poor performance with low-light data (Zheng, Zhang, & Lu, 2020). As for
Cogvengers, while they adopted the structure in Fig. 4b similar to the winner
team, they follow a one-stage training strategy to jointly optimized the two-
stream model, which might be the reason for their performance gap compared
to others. In addition to the two-stream models based on RGB and optical
Ô¨Çow, the team White give proposed another interesting two-stream structure
based on pure RGB input (R. Chen, Chen, Liang, Gao, & Lin, 2021). Specif-
ically, adopting a similar structure in Fig. 4c, the team replaced the Ô¨Çow
stream with a shared-weight RGB stream taking original dark clips as input.
The features from enhanced clips and dark clips were then ensembled by a
self-attention module (X. Wang, Girshick, Gupta, & He, 2018) to extract the
eÔ¨Äective spatio-temporal features from dual streams.

4.2 UG2-2.2: Semi-Supervised Learning

A total of 12 teams submitted their results in the semi-supervised challenge.
Among the participants, the winner team ArtiÔ¨Åcially Inspired achieved the
best performance of 93.77%. Similar to UG2-2.1, there is a noticeable gap
between leaderboard performance and our baseline results. This is mainly
because our baseline evaluation simply adopts existing domain adaptation
methods without any other pre-processing or enhancement techniques to
further boost the performance. Also, in order to achieve state-of-the-art per-
formance, all teams utilized much larger epoch number (e.g. total 425 epochs
for the winner team) and more complicated networks.

For the winner team ArtiÔ¨Åcially Inspired, they adopted the same backbone
and enhancement method from the UG2-2.1. To fully leverage the unlabeled
data from ARID, the team adopted pseudo-label strategy to create pseudo-
labels for the unlabeled data as shown in Fig. 5a. SpeciÔ¨Åcally, in the Ô¨Årst
run, the team Ô¨Årst trained the model with labeled data from HMDB51 and
generated the pseudo-labels ÀÜpu of unlabeled data by inference. Samples with
conÔ¨Ådent pseudo-labels were subsequently Ô¨Åltered based on their conÔ¨Ådence
scores and subsequently joined the supervised training process together with
the data from HMDB51. The team initially chose a relatively high threshold
œÉ of 0.99 and further increased it up to 0.999999 from the fourth run to the
tenth run. At the end of each run, the checkpoint of the trained model was
saved. During the testing process, the Ô¨Ånal prediction was generated as the
average of predictions from the model saved at the end of each run.

As for the runner-up team, DeepBlueAI achieved a competitive result of
93.63% with only a minor gap of 93.77% compared with the best result. The
team utilized CSN (Tran, Wang, Torresani, & Feiszli, 2019) based on ResNet-
152, which is a more complex backbone compared to the winner team. While
they also adopted the pseudo-label strategy similar to ArtiÔ¨Åcially Inspired as
in Fig. 5a, they designed a diÔ¨Äerent set of Ô¨Åltering rules. SpeciÔ¨Åcally, they

Springer Nature 2021 LATEX template

18

Going Deeper into Recognizing Actions in Dark Environments

(a) Pseudo-label Based Structure

(b) Structure of the TCL-based method. Details can be found in (Singh et al., 2021).

(c) Structure of the DA-based method. Details can be found in (Ganin et al., 2016).
Fig. 5 Structures utilized in UG2-2.2. Here p and ÀÜp indicate the prediction and the ground-
truth or pseudo-label, with superscript l and u indicating whether it is from labeled or
unlabeled sources, respectively. lCE is the cross entropy loss. Modules connected with dotted
lines with dual arrows are identical or shared-weight. In Fig. 5b, subscripts M , N refer to
the frame numbers of the input clips. lIC and lGC refer to Instance Contrastive (IC) loss
and Group Contrastive (GC) loss, respectively. In Fig. 5c, the pd, ÀÜpd are the predictions and
ground-truth for the domain classiÔ¨Åcation, respectively.

Enhance & SamplingUnlabeled VideoInput ClipùëùùëôNetworkLabeled VideoInput ClipNetworkEnhance & Samplingùëùùë¢∆∏ùëù∆∏ùëùùë¢ùúéùëôùê∂ùê∏Enhance & SamplingUnlabeled VideoùëÄFramesùëùùëôNetworkLabeled VideoùëÄFramesNetworkEnhance & SamplingùëùùëÄùë¢∆∏ùëùùëôùëôùê∂ùê∏ùëÅFramesNetworkEnhance & SamplingùëùùëÅùë¢ùëôùêºùê∂ùëôùê∫ùê∂SamplingUnlabeled VideoInput ClipùëùùëôNetworkLabeled VideoInput ClipNetworkSamplingùëùùëë∆∏ùëùùëôùê∂ùê∏∆∏ùëùùëëùëôùê∂ùê∏Springer Nature 2021 LATEX template

Going Deeper into Recognizing Actions in Dark Environments

19

designed a four-run training process, where all samples with pseudo-labels
generated in the Ô¨Årst run were included in the supervised training process of
the second run. In the rest process, pseudo-labels of two classes, including
‚ÄúDrink‚Äù and ‚ÄúPick‚Äù, were changed to the class ‚ÄúPour‚Äù if satisfying one of the
two following rules: (i) if the conÔ¨Ådence score of ‚ÄúPour‚Äù is larger than 2.0, or
(ii) if the conÔ¨Ådence score of ‚ÄúPour‚Äù is larger than 1/3 of the highest score.
While the team does not reveal their rational of this design, it might be the
similarity between these three actions that motivates this speciÔ¨Åc design. Also
diÔ¨Äerent from the winner team, DeepBlueAI generated their prediction only
based on the model of the Ô¨Ånal run.

Other teams also provided interesting solutions to generate supervision
signal from the unlabeled ARID data. For example, team Cogvengers (rank No.
3, Top1 84.35%), which utilized R(2+1)D-Bert as their based model, adopted
Temporal Contrastive Learning (TCL) (Singh et al., 2021) for semi-supervised
learning as shown in Fig. 5b. SpeciÔ¨Åcally, after performing GIC enhancement,
the team adopted two diÔ¨Äerent instance-level contrastive loss lIC to maximize
the mutual information between clips from the same video under diÔ¨Äerent
frame rates. For unlabeled samples with the same pseudo-label, a group-level
contrastive loss lGC was utilized to minimize the feature distance within the
group with the same pseudo-label. As for AstarTrek as shown in Fig. 5c, they
adopted an adversarial-based unsupervised domain adaptation method DANN
(Ganin et al., 2016) to adapt the features learned from the labeled HMDB51
data to the unlabeled data. However, they adopted a shallow backbone ResNet-
18, which might be the reason for their inferior performance (Top1 79.22%)
compared with others.

4.3 Analysis and Discussion

As presented above, participants have provided various solutions to tackle
action recognition in dark video for the UG2-2 challenge. While all winning
solutions improved substantially from the baseline results, the best results are
still lower compared to results in datasets of comparable scale (e.g., IXMAS
(Weinland et al., 2007)). Moreover, there is a signiÔ¨Åcant gap among the winning
solutions. Both observations justify the diÔ¨Éculty of the challenge with much
room for improvement.

In summary, advancements have been made by the various challenge solu-
tions, all winning solutions utilize deep learning based methods with complex
backbones, trained from scratch with a long training process. Such strategy
possesses a high risk of overÔ¨Åtting given the scale of the ARID-plus, while
also suÔ¨Äers from the need for large computational resources. Therefore, though
achieving notable performances, such strategy may not be ultimate for AR
in dark videos. Meanwhile, though domain adaptation approaches have been
popular in coping with semi-supervised action recognition, where dark videos
are unlabeled, domain adaptation solutions are not the preeminent ones in the
challenge, due to unique characteristics of dark videos. Such observation sug-
gests that there are limitations in applying domain adaptation to dark videos

Springer Nature 2021 LATEX template

20

Going Deeper into Recognizing Actions in Dark Environments

directly. To further improve AR accuracy, an intuitive strategy is to apply
low-light enhancement methods. However, empirical results go against such
intuition.

Are image enhancement methods eÔ¨Äective? While some low-light
enhancement methods do bring improvements in accuracy, results show that
the improvements are erratic. Negative eÔ¨Äects due to enhancements could be
explained by its disruption over the original data distribution as well as the
introduction of noise. Interestingly, the few adopted enhancements in the win-
ning solutions may not produce the best visual enhancement results. Instead,
it could be observed that these methods would either preserve the character
of the original data distribution or introduce less noise. Therefore, it could
be argued that for any enhancement to bring substantial improvement in AR
accuracy, either condition should be met. Since less noise could contribute
towards AR accuracy, employing further denoising methods (Sheth et al., 2021;
Tassano, Delon, & Veit, 2020) could be examined along with the various low-
light enhancement methods to suppress noise, mitigating the possible negative
eÔ¨Äects. Meanwhile, current solutions only exploit one single enhancement. To
this end, enhancement-invariant methods may be developed to capture under-
lying distributions that are not inÔ¨Çuenced by enhancement methods, which
could be the key to understanding dark videos. This strategy could be imple-
mented with various enhancement methods applied simultaneously to the dark
videos, with the invariant features trained by contrastive learning (T. Pan,
Song, Yang, Jiang, & Liu, 2021; Qian et al., 2021) of the enhanced results. The
Ô¨Ånal classiÔ¨Åcation would be performed on the enhancement-invariant features
extracted.

How to reduce model complexity? To overcome the risk of overÔ¨Åt-
ting and the requirement for large computational resource due to the use
of large-scale deep learning methods, multiple alternative strategies could
be considered. One of which is few-shot learning (Bo, Lu, & He, 2020;
Kumar Dwivedi, Gupta, Mitra, Ahmed, & Jain, 2019), which has enabled mod-
els to be trained with limited data while generalizing to unseen test data, and
has been gaining research interest for action recognition. This conforms to the
task of AR in dark environments, and should therefore be considered as a feasi-
ble alternative to the fully supervised strategy. Further, due to the insuÔ¨Écient
number of classes in ARID-plus, winning solutions may not be capable of gen-
eralizing to videos in the wild, where most actions are considered to be unseen
by ARID-plus. To overcome such shortcoming, zero-shot learning (K. Liu, Liu,
Ma, Huang, & Dong, 2019; Mishra et al., 2018; X. Xu, Hospedales, & Gong,
2017) endows AR methods the capacity of predicting unseen actions, which
could better cope with real-world scenarios. Meanwhile, techniques such as
self-supervised learning would also boost model capacity by exploiting extra
information within videos, such as video speed (J. Wang et al., 2020) and video
coherence (H. Cao et al., 2021). Meanwhile, to apply models in areas such as
surveillance, models should be deployed on edge devices (e.g., embedding sys-
tems such as Jetson). These devices possess limited computation resources but

Springer Nature 2021 LATEX template

Going Deeper into Recognizing Actions in Dark Environments

21

are able to be mass deployed. These attributes prohibit large-scale models to
be applied directly. One possible solution would be model compression (Y. He
et al., 2018; Y. Pan et al., 2019), which aims to deploy models in low-power and
resource-limited devices without a signiÔ¨Åcant drop in accuracy. The ability of
the compressed model to be applied on edge devices could help to expand the
application of AR solutions in scenarios such as nighttime autonomous driv-
ing systems, where conventional hardware (i.e., GPUs and TPUs) could not
be installed.

Does domain adaptation help a lot? Applying domain adaptation
approaches directly to semi-supervised AR of dark videos is ineÔ¨Äective largely
due to the large domain gap between normal videos and videos in dark environ-
ments. Domain adaptation approaches would therefore be unable to minimize
the discrepancies between diÔ¨Äerent domains, or to extract domain-invariant
features for transferring. Currently, most domain adaptation approaches align
high-level features (Ganin & Lempitsky, 2015; Ganin et al., 2016; Long et al.,
2015; Saito et al., 2018), which is in accord with the fact that high-level features
are utilized for the Ô¨Ånal classiÔ¨Åcation task. However, large discrepancies would
exist between the low-level features of normal and dark videos, given the large
diÔ¨Äerences in mean and standard deviation values of video frames. The dis-
crepancies between low-level features would escalate the discrepancies between
high-level features, therefore undermining the eÔ¨Äort of current domain adapta-
tion approaches in obtaining transferable features from normal videos. In view
of such observation, low-level features should be aligned with high-level fea-
tures jointly when designing domain adaption approaches for semi-supervised
AR in dark videos.

How to leverage multi-modality information? Besides the techniques
mentioned above, it is observed that optical Ô¨Çow could bring performance
improvement. Optical Ô¨Çow can be viewed as an additional modality embed-
ded in videos, and could provide more eÔ¨Äective information thanks to the
fact that it is essentially computed as the correlation of spatiotemporal pix-
els between successive frames, which is highly related to motion. However,
in solutions utilizing optical Ô¨Çow, it is extracted with hand-crafted methods,
such as TVL1, which require a large computation cost. Hand-crafted optical
Ô¨Çow also prohibits end-to-end training due to the need for storing optical Ô¨Çow
before subsequent training. Advances have been made in optical Ô¨Çow estima-
tion with deep learning method (Hui, Tang, & Loy, 2018; Ranjan & Black,
2017; Sun, Yang, Liu, & Kautz, 2019) that allows optical Ô¨Çow estimation to
be performed along with the training of feature extractors and classiÔ¨Åers in
an end-to-end manner. However, these advances are made with normal illu-
minated videos, and it is worth exploring whether these models could also be
applied with videos shot in dark environments. Meanwhile, with the optical
Ô¨Çow as an additional modality of information, current solutions tend to uti-
lize optical Ô¨Çow independent from RGB features, with the results obtained in
a late fusion fashion. Since both modalities are obtained from the same set of
data, it would be worth exploring how to train with both modalities jointly

Springer Nature 2021 LATEX template

22

Going Deeper into Recognizing Actions in Dark Environments

through approaches such as cross modality self-supervision (Khowaja & Lee,
2020; Sayed, Brattoli, & Ommer, 2018), which can be applied in both super-
vised training and cross-domain semi-supervised training (Munro & Damen,
2020). Such approach enables network to learn features with high semantic
meaning, which could lead to further improvements in AR eÔ¨Äectiveness.

5 Conclusion

In this work, we dive deeper into the challenging yet under-explored task of
action recognition (AR) in dark videos, with the introduction of a novel UG2+
Challenge Track 2 (UG2-2). UG2-2 aims to promote the research of AR in
challenging dark environments from both fully supervised and semi-supervised
manners, improving the generability of AR models in dark environments. Our
baseline analysis justiÔ¨Åes the diÔ¨Éculties of the challenges, with poor results
obtained from current AR models, enhancement methods and domain adap-
tation methods. While solutions in UG2-2 has introduced promising progress,
there remain large room for improvements. We hope this challenge and the
current progress could draw more interest from the community to tackle AR
in dark environments.

6 Acknowledgment

We would like to thank the group members of the winner teams in UG2-2 who
have contributed to the winning methods, listed as follows:

Ruibing Jin, Keyu Wu, Yubo Hou, and Min Wu from the Institute for Info-
comm Research, A*STAR, Singapore. Zaifeng Yang from the Institute of High
Performance Computing, A*STAR, Singapore. Ritwik Das, Sanchit Hira, and
Abhinav Modi from Team ArtiÔ¨Åcially Inspired. Hyeokjae Choi, Hwehee Chung,
Dongheon Cho, Hoseong Lee, and Eun-gyo Suh from Cognex Korea. Zhipeng
Luo, Jianye He, and Zhiguang Zhang from Deep Blue AI. Shan Lin, Jun
Chen, Rui Chen, and Huaien Gao from the School of Automation, Guangdong
University of Technology, Guangdong, China.

Declarations

‚Ä¢ Funding: No funding was received to assist with the preparation of this

manuscript.

‚Ä¢ Competing interests: The authors have no competing interests to declare

that are relevant to the content of this article.

Springer Nature 2021 LATEX template

Going Deeper into Recognizing Actions in Dark Environments

23

References

Anaya, J., & Barbu, A. (2018). Renoir‚Äìa dataset for real low-light image noise
reduction. Journal of Visual Communication and Image Representation,
51 , 144‚Äì154.

Blau, Y., & Michaeli, T. (2018). The perception-distortion tradeoÔ¨Ä. Proceed-
ings of the ieee conference on computer vision and pattern recognition
(pp. 6228‚Äì6237).

Bo, Y., Lu, Y., He, W. (2020). Few-shot learning of video action recogni-
tion only based on video contents. Proceedings of the ieee/cvf winter
conference on applications of computer vision (pp. 595‚Äì604).

Boudette, N.E.

(2021, Aug).

‚Äôit happened so fast‚Äô: Inside a fatal
tesla autopilot accident. The New York Times. Retrieved from
https://www.nytimes.com/2021/08/17/business/tesla-autopilot-
accident.html

Brown, P.

(2019, Nov).
Vehicle

night.
from
Autonomous
https://www.autonomousvehicleinternational.com/opinion/autonomous-
vehicles-at-night.html

at
Retrieved

International.

Autonomous

vehicles

Busto, P.P., Iqbal, A., Gall, J.

(2018). Open set domain adaptation for
image and action recognition. IEEE transactions on pattern analysis and
machine intelligence, 42 (2), 413‚Äì429.

Cao, D., & Xu, L. (2020). Bypass enhancement rgb stream model for pedes-
trian action recognition of autonomous vehicles. Pattern recognition (pp.
12‚Äì19). Singapore: Springer Singapore.

Cao, H., Xu, Y., Yang, J., Mao, K., Xie, L., Yin, J., See, S. (2021). Self-
supervised video representation learning by video incoherence detection.
arXiv preprint arXiv:2109.12493 .

Carreira, J., Noland, E., Banki-Horvath, A., Hillier, C., Zisserman, A. (2018).
A short note about kinetics-600. arXiv preprint arXiv:1808.01340 .

Carreira, J., Noland, E., Hillier, C., Zisserman, A. (2019). A short note on the

kinetics-700 human action dataset. arXiv preprint arXiv:1907.06987 .

Springer Nature 2021 LATEX template

24

Going Deeper into Recognizing Actions in Dark Environments

Carreira, J., & Zisserman, A. (2017). Quo vadis, action recognition? a new
model and the kinetics dataset. proceedings of the ieee conference on
computer vision and pattern recognition (pp. 6299‚Äì6308).

Chen, C., Chen, Q., Do, M.N., Koltun, V. (2019). Seeing motion in the dark.
Proceedings of the ieee international conference on computer vision (pp.
3185‚Äì3194).

Chen, C., Chen, Q., Xu, J., Koltun, V. (2018). Learning to see in the dark. Pro-
ceedings of the ieee conference on computer vision and pattern recognition
(pp. 3291‚Äì3300).

Chen, L., Ma, N., Wang, P., Li, J., Wang, P., Pang, G., Shi, X. (2020). Sur-
vey of pedestrian action recognition techniques for autonomous driving.
Tsinghua Science and Technology, 25 (4), 458‚Äì470.

Chen, R., Chen, J., Liang, Z., Gao, H., Lin, S. (2021, June). Darklight networks
for action recognition in the dark. Proceedings of the ieee/cvf conference
on computer vision and pattern recognition (cvpr) workshops (p. 846-
852).

Chen, Y.-L., Wu, B.-F., Huang, H.-Y., Fan, C.-J. (2010). A real-time vision
system for nighttime vehicle detection and traÔ¨Éc surveillance.
IEEE
Transactions on Industrial Electronics, 58 (5), 2030‚Äì2044.

Choi, J., Sharma, G., Schulter, S., Huang, J.-B. (2020). ShuÔ¨Ñe and attend:
Video domain adaptation. European conference on computer vision (pp.
678‚Äì695).

Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., Fei-Fei, L. (2009).

Ima-
genet: A large-scale hierarchical image database. 2009 ieee conference
on computer vision and pattern recognition (pp. 248‚Äì255).

Fahad, L.G., & Rajarajan, M. (2015). Integration of discriminative and gen-
erative models for activity recognition in smart homes. Applied Soft
Computing, 37 , 992‚Äì1001.

Feichtenhofer, C.

(2020). X3d: Expanding architectures for eÔ¨Écient video
recognition. Proceedings of the ieee/cvf conference on computer vision
and pattern recognition (pp. 203‚Äì213).

Feichtenhofer, C., Fan, H., Malik, J., He, K. (2019). Slowfast networks for
video recognition. Proceedings of the ieee/cvf international conference
on computer vision (pp. 6202‚Äì6211).

Springer Nature 2021 LATEX template

Going Deeper into Recognizing Actions in Dark Environments

25

Feng, S., Setoodeh, P., Haykin, S. (2017). Smart home: Cognitive interac-
tive people-centric internet of things. IEEE Communications Magazine,
55 (2), 34‚Äì39.

Fernando, B., Bilen, H., Gavves, E., Gould, S. (2017). Self-supervised video
representation learning with odd-one-out networks. Proceedings of the
ieee conference on computer vision and pattern recognition (pp. 3636‚Äì
3645).

Ganin, Y., & Lempitsky, V.
backpropagation.
1180‚Äì1189).

(2015). Unsupervised domain adaptation by
International conference on machine learning (pp.

Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,
F., . . . Lempitsky, V.
(2016). Domain-adversarial training of neural
networks. The journal of machine learning research, 17 (1), 2096‚Äì2030.

Ghadiyaram, D., Tran, D., Mahajan, D.

Large-scale weakly-
supervised pre-training for video action recognition. Proceedings of
the ieee/cvf conference on computer vision and pattern recognition (pp.
12046‚Äì12055).

(2019).

Gorelick, L., Blank, M., Shechtman, E., Irani, M., Basri, R. (2007). Actions as
space-time shapes. IEEE transactions on pattern analysis and machine
intelligence, 29 (12), 2247‚Äì2253.

Gowda, S.N., Rohrbach, M., Sevilla-Lara, L. (2021). Smart frame selection
for action recognition. Proceedings of the aaai conference on artiÔ¨Åcial
intelligence (Vol. 35, pp. 1451‚Äì1459).

Goyal, R., Ebrahimi Kahou, S., Michalski, V., Materzynska, J., Westphal, S.,
Kim, H., . . . others (2017). The‚Äù something something‚Äù video database
for learning and evaluating visual common sense. Proceedings of the ieee
international conference on computer vision (pp. 5842‚Äì5850).

Gu, C., Sun, C., Ross, D.A., Vondrick, C., Pantofaru, C., Li, Y., . . . oth-
ers (2018). Ava: A video dataset of spatio-temporally localized atomic
visual actions. Proceedings of the ieee conference on computer vision and
pattern recognition (pp. 6047‚Äì6056).

Guo, C., Li, C., Guo, J., Loy, C.C., Hou, J., Kwong, S., Cong, R. (2020).
Zero-reference deep curve estimation for low-light image enhancement.
Proceedings of the ieee/cvf conference on computer vision and pattern
recognition (pp. 1780‚Äì1789).

Springer Nature 2021 LATEX template

26

Going Deeper into Recognizing Actions in Dark Environments

Guo, X., Li, Y., Ling, H. (2016). Lime: Low-light image enhancement via
illumination map estimation. IEEE Transactions on Image Processing,
26 (2), 982‚Äì993.

Hara, K., Kataoka, H., Satoh, Y. (2018). Can spatiotemporal 3d cnns retrace
the history of 2d cnns and imagenet? Proceedings of the ieee conference
on computer vision and pattern recognition (pp. 6546‚Äì6555).

He, K., Zhang, X., Ren, S., Sun, J. (2016). Deep residual learning for image
recognition. Proceedings of the ieee conference on computer vision and
pattern recognition (pp. 770‚Äì778).

He, Y., Lin, J., Liu, Z., Wang, H., Li, L.-J., Han, S. (2018). Amc: Automl for
model compression and acceleration on mobile devices. Proceedings of
the european conference on computer vision (eccv) (pp. 784‚Äì800).

Hira, S., Das, R., Modi, A., Pakhomov, D. (2021, June). Delta sampling r-
bert for limited data and low-light action recognition. Proceedings of the
ieee/cvf conference on computer vision and pattern recognition (cvpr)
workshops (p. 853-862).

Hui, T.-W., Tang, X., Loy, C.C. (2018). LiteÔ¨Çownet: A lightweight convolu-
tional neural network for optical Ô¨Çow estimation. Proceedings of the ieee
conference on computer vision and pattern recognition (pp. 8981‚Äì8989).

Ji, S., Xu, W., Yang, M., Yu, K. (2012). 3d convolutional neural networks for
human action recognition. IEEE transactions on pattern analysis and
machine intelligence, 35 (1), 221‚Äì231.

Jiang, H., & Zheng, Y. (2019). Learning to see moving objects in the dark.
Proceedings of the ieee/cvf international conference on computer vision
(pp. 7324‚Äì7333).

Kalfaoglu, M.E., Kalkan, S., Alatan, A.A.

(2020). Late temporal model-
ing in 3d cnn architectures with bert for action recognition. European
conference on computer vision (pp. 731‚Äì747).

Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei,
L.
(2014). Large-scale video classiÔ¨Åcation with convolutional neural
networks. Proceedings of the ieee conference on computer vision and
pattern recognition (pp. 1725‚Äì1732).

Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan,
S., . . . others (2017). The kinetics human action video dataset. arXiv
preprint arXiv:1705.06950 .

Springer Nature 2021 LATEX template

Going Deeper into Recognizing Actions in Dark Environments

27

Khowaja, S.A., & Lee, S.-L. (2020). Hybrid and hierarchical fusion networks:
a deep cross-modal learning architecture for action recognition. Neural
Computing and Applications, 32 (14), 10423‚Äì10434.

Kuehne, H., Jhuang, H., Garrote, E., Poggio, T., Serre, T. (2011). Hmdb: a
large video database for human motion recognition. 2011 international
conference on computer vision (pp. 2556‚Äì2563).

Kumar Dwivedi, S., Gupta, V., Mitra, R., Ahmed, S., Jain, A. (2019). Pro-
togan: Towards few shot learning for action recognition. Proceedings of
the ieee/cvf international conference on computer vision workshops (pp.
0‚Äì0).

Li, C., Guo, C., Chen, C.L.

(2021). Learning to enhance low-light image
via zero-reference deep curve estimation. IEEE Transactions on Pattern
Analysis and Machine Intelligence.

Lin, J., Gan, C., Han, S. (2019). Tsm: Temporal shift module for eÔ¨Écient
video understanding. Proceedings of the ieee/cvf international conference
on computer vision (pp. 7083‚Äì7093).

Liu, J., Xu, D., Yang, W., Fan, M., Huang, H. (2021). Benchmarking low-light
International Journal of Computer

image enhancement and beyond.
Vision, 129 (4), 1153‚Äì1184.

Liu, K., Liu, W., Ma, H., Huang, W., Dong, X. (2019). Generalized zero-shot
learning for action recognition with web-scale video data. World Wide
Web, 22 (2), 807‚Äì824.

Loh, Y.P., & Chan, C.S. (2019). Getting to know low-light images with the
exclusively dark dataset. Computer Vision and Image Understanding,
178 , 30‚Äì42.

Long, M., Cao, Y., Wang, J., Jordan, M. (2015). Learning transferable features
International conference on machine

with deep adaptation networks.
learning (pp. 97‚Äì105).

Lv, F., Li, Y., Lu, F. (2021). Attention guided low-light image enhancement
with a large scale low-light simulation dataset. International Journal of
Computer Vision, 129 (7), 2175‚Äì2193.

Springer Nature 2021 LATEX template

28

Going Deeper into Recognizing Actions in Dark Environments

Ma, C., Yang, C.-Y., Yang, X., Yang, M.-H. (2017). Learning a no-reference
quality metric for single-image super-resolution. Computer Vision and
Image Understanding, 158 , 1‚Äì16.

Mishra, A., Verma, V.K., Reddy, M.S.K., Arulkumar, S., Rai, P., Mittal, A.
(2018). A generative approach to zero-shot and few-shot action recog-
nition. 2018 ieee winter conference on applications of computer vision
(wacv) (pp. 372‚Äì380).

Mittal, A., Soundararajan, R., Bovik, A.C.
blind‚Äù image quality analyzer.
209‚Äì212.

(2012). Making a ‚Äúcompletely
IEEE Signal processing letters, 20 (3),

Monfort, M., Andonian, A., Zhou, B., Ramakrishnan, K., Bargal, S.A., Yan,
T., . . . others (2019). Moments in time dataset: one million videos for
event understanding. IEEE transactions on pattern analysis and machine
intelligence, 42 (2), 502‚Äì508.

Munro, J., & Damen, D. (2020). Multi-modal domain adaptation for Ô¨Åne-
grained action recognition. Proceedings of the ieee/cvf conference on
computer vision and pattern recognition (pp. 122‚Äì132).

Pan, B., Cao, Z., Adeli, E., Niebles, J.C. (2020). Adversarial cross-domain
action recognition with co-attention. Proceedings of the aaai conference
on artiÔ¨Åcial intelligence (Vol. 34, pp. 11815‚Äì11822).

Pan, T., Song, Y., Yang, T., Jiang, W., Liu, W.

(2021). Videomoco:
Contrastive video representation learning with temporally adversarial
examples. Proceedings of the ieee/cvf conference on computer vision and
pattern recognition (pp. 11205‚Äì11214).

Pan, Y., Xu, J., Wang, M., Ye, J., Wang, F., Bai, K., Xu, Z. (2019). Compress-
ing recurrent neural networks with tensor ring for action recognition.
Proceedings of the aaai conference on artiÔ¨Åcial intelligence (Vol. 33, pp.
4683‚Äì4690).

Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., . . .
others
(2019). Pytorch: An imperative style, high-performance deep
learning library. Advances in neural information processing systems, 32 ,
8026‚Äì8037.

Springer Nature 2021 LATEX template

Going Deeper into Recognizing Actions in Dark Environments

29

Qian, R., Meng, T., Gong, B., Yang, M.-H., Wang, H., Belongie, S., Cui,
Y.
(2021). Spatiotemporal contrastive video representation learning.
Proceedings of the ieee/cvf conference on computer vision and pattern
recognition (pp. 6964‚Äì6974).

Qiu, Z., Yao, T., Mei, T.

(2017). Learning spatio-temporal representation
with pseudo-3d residual networks. proceedings of the ieee international
conference on computer vision (pp. 5533‚Äì5541).
10.1109/ICCV.2017
.590

Ranjan, A., & Black, M.J. (2017). Optical Ô¨Çow estimation using a spatial
pyramid network. Proceedings of the ieee conference on computer vision
and pattern recognition (pp. 4161‚Äì4170).

Royer, E., Lhuillier, M., Dhome, M., Lavest, J.-M. (2007). Monocular vision
for mobile robot localization and autonomous navigation. International
Journal of Computer Vision, 74 (3), 237‚Äì260.

Saito, K., Watanabe, K., Ushiku, Y., Harada, T. (2018). Maximum classiÔ¨Åer
discrepancy for unsupervised domain adaptation. Proceedings of the ieee
conference on computer vision and pattern recognition (pp. 3723‚Äì3732).

Sayed, N., Brattoli, B., Ommer, B. (2018). Cross and learn: Cross-modal self-

supervision. German conference on pattern recognition (pp. 228‚Äì243).

Schuldt, C., Laptev, I., Caputo, B.

(2004). Recognizing human actions: a
local svm approach. Proceedings of the 17th international conference on
pattern recognition, 2004. icpr 2004. (Vol. 3, p. 32-36 Vol.3). 10.1109/
ICPR.2004.1334462

Sheth, D.Y., Mohan, S., Vincent, J.L., Manzorro, R., Crozier, P.A., Khapra,
(2021). Unsupervised deep video
international conference on

Proceedings of the ieee/cvf

M.M., . . . Fernandez-Granda, C.
denoising.
computer vision (pp. 1759‚Äì1768).

Simonyan, K., & Zisserman, A. (2014). Two-stream convolutional networks for
action recognition in videos. Advances in neural information processing
systems (pp. 568‚Äì576).

Singh, A., Chakraborty, O., Varshney, A., Panda, R., Feris, R., Saenko, K.,
Das, A. (2021). Semi-supervised action recognition with temporal con-
trastive learning. Proceedings of the ieee/cvf conference on computer
vision and pattern recognition (pp. 10389‚Äì10399).

Soomro, K., Zamir, A.R., Shah, M. (2012). Ucf101: A dataset of 101 human
actions classes from videos in the wild. arXiv preprint arXiv:1212.0402 .

Springer Nature 2021 LATEX template

30

Going Deeper into Recognizing Actions in Dark Environments

Sultani, W., & Saleemi, I. (2014). Human action recognition across datasets
by foreground-weighted histogram decomposition. Proceedings of the ieee
conference on computer vision and pattern recognition (pp. 764‚Äì771).

Sun, D., Yang, X., Liu, M.-Y., Kautz, J.

(2019). Models matter, so does
training: An empirical study of cnns for optical Ô¨Çow estimation. IEEE
transactions on pattern analysis and machine intelligence, 42 (6), 1408‚Äì
1423.

Tassano, M., Delon, J., Veit, T. (2020). Fastdvdnet: Towards real-time deep
video denoising without Ô¨Çow estimation. Proceedings of the ieee/cvf
conference on computer vision and pattern recognition (pp. 1354‚Äì1363).

Tran, D., Wang, H., Torresani, L., Feiszli, M. (2019). Video classiÔ¨Åcation with
channel-separated convolutional networks. Proceedings of the ieee/cvf
international conference on computer vision (pp. 5552‚Äì5561).

Tran, D., Wang, H., Torresani, L., Ray, J., LeCun, Y., Paluri, M. (2018). A
closer look at spatiotemporal convolutions for action recognition. Pro-
ceedings of the ieee conference on computer vision and pattern recognition
(pp. 6450‚Äì6459). 10.1109/CVPR.2018.00675

Ullah, A., Muhammad, K., Ding, W., Palade, V., Haq, I.U., Baik, S.W. (2021).
EÔ¨Écient activity recognition using lightweight cnn and ds-gru network
for surveillance applications. Applied Soft Computing, 103 , 107102.

Wang, J., Jiao, J., Liu, Y.-H.

(2020). Self-supervised video representation
learning by pace prediction. European conference on computer vision
(pp. 504‚Äì521).

Wang, L., Koniusz, P., Huynh, D.Q. (2019). Hallucinating idt descriptors and
i3d optical Ô¨Çow features for action recognition with cnns. Proceedings
of the ieee/cvf international conference on computer vision (pp. 8698‚Äì
8708).

Wang, L., Xiong, Y., Wang, Z., Qiao, Y., Lin, D., Tang, X., Van Gool, L.
(2016). Temporal segment networks: Towards good practices for deep
action recognition. European conference on computer vision (pp. 20‚Äì36).

Wang, X., Girshick, R., Gupta, A., He, K.

(2018). Non-local neural net-
works. Proceedings of the ieee conference on computer vision and pattern
recognition (pp. 7794‚Äì7803).

Springer Nature 2021 LATEX template

Going Deeper into Recognizing Actions in Dark Environments

31

Wei, C., Wang, W., Yang, W., Liu, J. (2018). Deep retinex decomposition for

low-light enhancement. arXiv preprint arXiv:1808.04560 .

Weinland, D., Boyer, E., Ronfard, R. (2007). Action recognition from arbitrary
views using 3d exemplars. 2007 ieee 11th international conference on
computer vision (pp. 1‚Äì7).

Xu, D., Xiao, J., Zhao, Z., Shao, J., Xie, D., Zhuang, Y. (2019). Self-supervised
spatiotemporal learning via video clip order prediction. Proceedings of
the ieee/cvf conference on computer vision and pattern recognition (pp.
10334‚Äì10343).

Xu, X., Hospedales, T., Gong, S. (2017). Transductive zero-shot action recog-
nition by word-vector embedding. International Journal of Computer
Vision, 123 (3), 309‚Äì333.

Xu, Y., Yang, J., Cao, H., Chen, Z., Li, Q., Mao, K. (2021). Partial video
domain adaptation with partial adversarial temporal attentive network.
Proceedings of the ieee/cvf international conference on computer vision
(pp. 9332‚Äì9341).

Xu, Y., Yang, J., Cao, H., Mao, K., Yin, J., See, S. (2021). Arid: A new
dataset for recognizing action in the dark. International workshop on
deep learning for human activity recognition (pp. 70‚Äì84).

Yang, J., Zou, H., Jiang, H., Xie, L. (2018). Device-free occupant activity
sensing using wiÔ¨Å-enabled iot devices for smart homes. IEEE Internet
of Things Journal , 5 (5), 3991‚Äì4002.

Yao, Y., Liu, C., Luo, D., Zhou, Y., Ye, Q. (2020, June). Video playback rate
perception for self-supervised spatio-temporal representation learning.
Proceedings of the ieee/cvf conference on computer vision and pattern
recognition (cvpr).

Ying, Z., Li, G., Ren, Y., Wang, R., Wang, W. (2017). A new image contrast
enhancement algorithm using exposure fusion framework. International
conference on computer analysis of images and patterns (pp. 36‚Äì46).

Zach, C., Pock, T., Bischof, H. (2007). A duality based approach for realtime
tv-l 1 optical Ô¨Çow. Joint pattern recognition symposium (pp. 214‚Äì223).

Zhang, F., Li, Y., You, S., Fu, Y. (2021). Learning temporal consistency for low
light video enhancement from single images. Proceedings of the ieee/cvf
conference on computer vision and pattern recognition (pp. 4967‚Äì4976).

Springer Nature 2021 LATEX template

32

Going Deeper into Recognizing Actions in Dark Environments

Zhang, S., Zhang, Y., Jiang, Z., Zou, D., Ren, J., Zhou, B. (2020). Learning to
see in the dark with events. Computer vision‚Äìeccv 2020: 16th european
conference, glasgow, uk, august 23‚Äì28, 2020, proceedings, part xviii 16
(pp. 666‚Äì682).

Zhang, Y., Zhang, J., Guo, X. (2019). Kindling the darkness: A practical
low-light image enhancer. Proceedings of the 27th acm international
conference on multimedia (pp. 1632‚Äì1640). New York, NY, USA: ACM.
10.1145/
Retrieved from http://doi.acm.org/10.1145/3343031.3350926
3343031.3350926

Zheng, Y., Zhang, M., Lu, F. (2020). Optical Ô¨Çow in the dark. Proceedings of
the ieee/cvf conference on computer vision and pattern recognition (pp.
6749‚Äì6757).

Zou, H., Yang, J., Prasanna Das, H., Liu, H., Zhou, Y., Spanos, C.J. (2019).
WiÔ¨Å and vision multimodal learning for accurate and robust device-
free human activity recognition. Proceedings of the ieee conference on
computer vision and pattern recognition workshops (pp. 0‚Äì0).

