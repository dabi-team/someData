Variable Selection in Regression Model with AR(p) Error Terms Based on Heavy Tailed 
Distributions 

Y. TuaÃ§1 and O. Arslan1 

1Ankara University, Faculty of Science, Department of Statistics, 06100 Ankara/Turkey 

ytuac@ankara.edu.tr oarslan@ankara.edu.tr 

Abstract 
Parameter  estimation  and  the  variable  selection  are  two  pioneer  issues  in  regression  analysis.  
While traditional variable selection methods require prior estimation of the model parameters, the 
penalized methods simultaneously carry on parameter estimation and variable select. Therefore, 
penalized variable  selection  methods are of  great interest and have been  extensively  studied in 
literature. However, most of the papers in literature are only limited to the regression models with 
uncorrelated  error  terms  and  normality  assumption.  In  this  study,  we  combine  the  parameter 
estimation and the variable selection in regression models with autoregressive error term by using 
different  penalty  functions  under  heavy  tailed  error  distribution  assumption.  We  conduct  a 
simulation study and a real data example to show the performance of the estimators. 

JEL Codes: C150, C510, C520. 
Keywords: Dependent errors, EM algorithm, LASSO, heavy tailed distributions, variable 
selection. 

1. Introduction  

The  high  dimensional  nature  of  the  existing  data  sets  is  required  new  variable  section  and 
parameter  estimation  methods  which  has  brought  great  attention  to  the  statisticians.  Variable 
selection focuses on searching for the best set of relevant variables to include in a model. In linear 
regression  models  there  are  two  popular  variable  selection  procedures:  subset  selection  and 
penalized methods. Although, subset selection methods are quite practically useful, these methods 
need  parameter  estimation  before  performing  the  variable  selection  and  they  often  show  high 
variability which cannot reduce the prediction error of the full model. On the other hand, penalized 
regression methods  which we consider in this study such as LASSO, bridge, ridge, SCAD and 
elastic-net  are  select  variables  and  estimate  model  parameters  simultaneously.  Hence,  these 
methods provide less prediction error and more stability than the subset selection methods. We 
can briefly summarize the basic properties of these methods as follows: Least Absolute Shrinkage 
Selection  Operator  (LASSO)  estimation  method  (Tibshirani,  1996)  the  OLS  loss  function 
  type 
penalized function. Unlike ridge regression, there is no analytic solution for the LASSO therefore 
(ğ‘¦ğ‘¦ âˆ’ ğ‘¿ğ‘¿ğ›½ğ›½)
numeric methods should be used. LASSO estimation method sets the unimportant coefficients to 
zero  to  achieve  the  model  selection  while  doing  the  parameter  estimation  simultaneously. 
Smoothly Clipped Absolute Deviation (SCAD) introduced by  Fan  and  Li (2001) possesses the 
oracle  properties  unlike  the  LASSO  such  as:  unbiasedness  for  large  true  coefficient  to  avoid 

  is  minimized  under  the  restriction 

.  LASSO  use 

(ğ‘¦ğ‘¦ âˆ’ ğ‘¿ğ‘¿ğ›½ğ›½)

 ï¿½ğ›½ğ›½ğ‘—ğ‘—ï¿½

ğ‘ğ‘
ğ‘—ğ‘—=1

â‰¤ ğ‘¡ğ‘¡

ğ¿ğ¿1

âˆ‘

ğ‘‡ğ‘‡

1 

 
 
 
 
 
 
 
 
 
ğ›¼ğ›¼ = 3.7

ğ›¼ğ›¼
and the LASSO 

excessive estimation bias, sparsity to reduce model complexity, continuity to avoid unnecessary 
 can be choose by using 
variation in model prediction. According to Fan and Li (2001) optimal 
 parameter. Fan and Li 
Cross Validation (CV) method, it does not bring any difference to select 
ğœ†ğœ†
 is a good choice for various problem, we also use the same value 
(2001) suggested taking 
in  simulation  study.  Bridge,  introduced  by  Frank  and  Friedman  (1993)  utilizes  the 
penalty and thus, it includes the ridge (Hoerl and Kennard, 1970) 
ğ¿ğ¿ğ›¾ğ›¾ (ğ›¾ğ›¾ > 0)
. In the literature there 
as special cases. Bridge estimators produce sparse models when 
(ğ›¾ğ›¾ = 2) 
(ğ›¾ğ›¾ = 1)
 (Park and Yoon, 2011, Liu et 
are some algorithms which proposed to choose optimal value of 
0 < ğ›¾ğ›¾ â‰¤ 1
al., 2007). In this study we choose 
 parameters by using a two-dimensional  grid search. If 
ğ›¾ğ›¾
some  of  the  covariates  are  highly  correlated,  then  the  elastic-net  proposed  by  Zou  and  Hastie 
(2005) which combines 
These methods are widely used in literature and most studies have been done under assumption 
that observations are independent. However, if the error terms are correlated in time and ignoring 
this correlation effect while estimating the model parameters may result in incorrectly estimate of 
the variance of the model parameters. This causes the invalid resulting of confidence intervals and 
hypothesis tests. To overcome this problem, we assumed p order autoregressive (AR(p)) (Box and 
Jenkins,  1976)  structure  to  model  the  error  terms.  This  type  of  regression  models  has  been 
considered  before  both  parameter  estimation  without  variable  selection  and  penalized  based 
variable selection cases.  

ğ›¾ğ›¾
 norms can be used.  

 and 

 and 

ğ¿ğ¿2

ğ¿ğ¿1

ğœ†ğœ†

Concerning the parameter estimation, some papers in literature which use normal distribution as 
error distribution assumption might be listed as follows. Alpuim and El-Shaarawi (2008) estimate 
parameters of the regression model with AR(p) error term using the ordinary least square (OLS) 
estimation  method.  They  also  use  the  maximum  likelihood  (ML)  and  conditional  maximum 
likelihood (CML) estimation methods under the assumption of normality and study the asymptotic 
properties of the resulting estimators. Beach and Mackinnon (1978) used ML method to estimate 
the parameters  of AR(1) error term  regression models. However, normality  assumption is very 
restrictive and sensitive to the existence of outliers in the data. The t distribution provides a useful 
alternative to the normal distribution for statistical modelling of data sets that have heavier tailed 
empirical  distribution.  Tiku  (2007)  estimates  the  parameters  by  using  the  modified  maximum 
likelihood (MML) method for the regression model with AR(1) error terms under the assumption 
that  the  error  term  has  heavy  tailed  distribution.  Recently,  TuaÃ§  et  al.  (2018)  proposed  to  use 
symmetric t distribution as the distribution for error terms and used CML method to carry on the 
estimation  of  the  model  parameters  in  autoregressive  error  term  regression  model  as  a  robust 
alternative  to  the  normal  distribution.  Further,  TuaÃ§  et  al.  (2020)  used  skew  alternatives  to  the 
normal and the 

 distributions as error distributions in AR(p) error term regression model.  

ğ‘¡ğ‘¡

Regarding  the  penalized  variable  selection  case  there  are  also  some  studies  are  exist  based  on 
AR(p)  error  term  regression  model.  Wang  et  al.  (2007)  consider  linear  regression  with 
autoregressive  errors.  They  employ  the  modified  LASSO-type  penalty  both  on  regression  and 
autoregressive  coefficients.  Yoon  et  al.  (2012)  consider  adaptive  LASSO,  SCAD  and  bridge 
method in AR(p) error term regression models under normality assumption. However, these papers 
are not considered any other distribution apart from the normal. In the regression model, the error 
terms may not always conform to the normal distribution in real life. The distribution of the data 
may be thicker-tailed than the normal distribution. Using normal distribution for the error terms 
might result lack of the performance of the selection of the important variables in the model besides 

2 

 
 
 
 
 
the higher MSE (mean squared error) of the parameter estimations. To solve this problem, as an 
alternative to the normal distribution, the symmetric t distribution, which exhibits a thicker-tailed 
behavior  than  the  normal  distribution,  is  considered  (TuaÃ§,  2020).  Using  t  distribution  as  error 
distribution improve the ability of the selection of the important variables and the resulting smaller 
MSE of the parameter estimations when there are outliers exist. In this paper, we construct five 
different  penalized  regression  methods;  LASSO,  bridge,  ridge,  SCAD  and  elastic-net  in  the 
autoregressive error term regression model with the distribution assumption of t distribution as a 
heavy tailed alternative to the normal distribution. To handle the complex maximization problem, 
we  combine  the  Expectation  Maximization  (EM)  (Dempster  et.al,  1977,  McLachilan  and 
Krishnan, 1997) algorithm with aforementioned variable selection methods. 

The remainder of  the paper  is organized as  follows,  Section  2  gives  concise  information about 
LASSO,  bridge,  SCAD  and  elastic-net  estimators.  In  Section  3,  we  discuss  penalizing  the 
autoregressive error term regression model with mentioned methods. In Section 4, we give some 
brief  description  of  EM  algorithm  and  we  propose  an  Expectation  Conditional  Maximization 
(ECM) (Meng, X. L. and Rubin, D. B., 1993) type of algorithm to calculate the ML estimate of 
the parameters and variable selection of penalized regression model with AR(p) error terms for 
each five methods under t distribution assumption. Section 5 presents a simulation study and a real 
data  example  for  comparison  of  the  five  methods  based  on  t  distribution.  Finally,  Section  6 
summarize the paper with a conclusion.  

2. Penalized variable selection methods 

Consider the following linear regression model 

ğ‘€ğ‘€

ğ‘¦ğ‘¦ğ‘¡ğ‘¡ = ï¿½ ğ‘¥ğ‘¥ğ‘¡ğ‘¡,ğ‘–ğ‘–
ğ‘–ğ‘–=1

ğ›½ğ›½ğ‘–ğ‘– + ğ‘’ğ‘’ğ‘¡ğ‘¡   ,

 ğ‘¡ğ‘¡ = 1,2, â€¦ , ğ‘ğ‘                                                                                (1) 

  is the response variable,  

where, 
are  independent  and  identically  distributed  random  variables  with  mean  0  and  variance 
Penalized regression methods can be done by maximizing the following objective function  

  explanatory  variables,  

 regression parameters and 

ğ‘¥ğ‘¥ğ‘¡ğ‘¡,ğ‘–ğ‘–

ğ‘¦ğ‘¦ğ‘¡ğ‘¡

ğ›½ğ›½ğ‘–ğ‘–

.  
ğ‘’ğ‘’ğ‘¡ğ‘¡
2
ğœğœ

ğ‘ğ‘

ğ¿ğ¿ğ‘ğ‘ = âˆ’ ln ğ¿ğ¿(ğœ·ğœ·, ğœğœ

2

 is the log-likelihood function and  

|ğ‘’ğ‘’ğ‘¡ğ‘¡) + ğ‘ğ‘ ï¿½  ğ‘ğ‘ğœ†ğœ†ğ‘–ğ‘–(ğ›½ğ›½ğ‘–ğ‘–),                                                                                    (2)
â€™s are penalty 

 is a penalty function and 

ğ‘–ğ‘–=1

In this paper, we consider five different forms of 

where  
parameters.  
ğ‘™ğ‘™ğ‘™ğ‘™ğ¿ğ¿(. )

(i) 

LASSO
pÎ» Î² Î» Î²
)
j
j

= âˆ‘ ,  

(

p

j

=
1

0 

ğœ†ğœ† >

3 

ğ‘ğ‘ğœ†ğœ†ğ‘–ğ‘–(. )
: 

ğ‘ğ‘ğœ†ğœ†ğ‘–ğ‘–(. )

ğœ†ğœ†ğ‘–ğ‘–

 
 
 
 
 
 
 
 
 
 
 
 
(ii) 

SCAD

pÎ»

Î²
(
j

)

ï£±
ï£´
ï£´
ï£´ ï£«
ï£´ ï£¬
= âˆ’
ï£²
ï£¬
ï£´ ï£­
ï£´
ï£´
ï£´
ï£³

Î»Î²
j

2

âˆ’

Î²
j

+

2
Î±Î»Î² Î»
2
j
1)

Î±

âˆ’

2(

(
)
2
Î± Î»
+
1

2

ï£¶
ï£·
ï£·
ï£¸

,

,

,

Î² Î»
â‰¤

j

Î» Î² Î±Î»
j

<

â‰¤

Î² Î±Î»
>

j

(iii) 

(iv) 

Bridge
Î» Î² Î» Î²
p
)
j

= âˆ‘  ,   

(

j

p

Î³

Ridge
pÎ» Î² Î» Î²
)
j
j

= âˆ‘ ,  

(

2

0 < ğ›¾ğ›¾ â‰¤ 1
0 

j

=
1

p

j

=
1

(v) 

Elastic Net

âˆ’

pÎ»

p

ğœ†ğœ† >
+âˆ‘
Î² Î» Î² Î» Î²
(
j
j

âˆ‘  

=

)

2

p

2

1

j

j

=
1

j

=
1

3. Estimation in penalized autoregressive regression model 

We consider model (1) with error terms have dependent structure as AR(p) model as follows  

 .                         

       (3) 

For simplicity, following backshift operator

ğ‘ğ‘ğ‘¡ğ‘¡ = ğ‘’ğ‘’ğ‘¡ğ‘¡ âˆ’ ğœ™ğœ™1ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’1 âˆ’ â‹¯ âˆ’ ğœ™ğœ™ğ‘ğ‘ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’ğ‘ğ‘,     ï¿½ğœ™ğœ™ğ‘—ğ‘—ï¿½ < 1   ,   ğ‘—ğ‘— = 1,2, â€¦ , ğ‘ğ‘

 can be used   

 (ğµğµ)

       (4) 

ğ‘ğ‘ğ‘¡ğ‘¡ = Î¦(ğµğµ)ğ‘’ğ‘’ğ‘¡ğ‘¡
 will be the new error term for regression model.  

 and they are 
where 
uncorrelated random variables with constant variance. Then, with the help of backshift operator, 
2
the regression model given in (1) with autoregressive error terms can be rewritten as  

ğ‘‰ğ‘‰ğ‘ğ‘ğ‘‰ğ‘‰(ğ‘ğ‘ğ‘¡ğ‘¡) = ğœğœ

ğ¸ğ¸(ğ‘ğ‘ğ‘¡ğ‘¡) = 0

ğ‘ğ‘ğ‘¡ğ‘¡

, 

                                                            (5) 

where,  

Î¦(ğµğµ)ğ‘¦ğ‘¦ğ‘¡ğ‘¡ = âˆ‘ ğ›½ğ›½ğ‘–ğ‘–

ğ‘€ğ‘€
ğ‘–ğ‘–=1 Î¦(ğµğµ)ğ‘¥ğ‘¥ğ‘¡ğ‘¡,ğ‘–ğ‘– + ğ‘ğ‘ğ‘¡ğ‘¡,    ğ‘¡ğ‘¡ = 1,2, â€¦ , ğ‘‡ğ‘‡
, 

,        

,  

Î¦(ğµğµ)ğ‘¦ğ‘¦ğ‘¡ğ‘¡ = ğ‘¦ğ‘¦ğ‘¡ğ‘¡ âˆ’ ğœ™ğœ™1ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 âˆ’ â‹¯ âˆ’ ğœ™ğœ™ğ‘ğ‘ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’ğ‘ğ‘
: number of regression coefficients,  
Î¦(ğµğµ)ğ‘¥ğ‘¥ğ‘¡ğ‘¡,ğ‘–ğ‘– = ğ‘¥ğ‘¥ğ‘¡ğ‘¡,ğ‘–ğ‘– âˆ’ ğœ™ğœ™1ğ‘¥ğ‘¥ğ‘¡ğ‘¡âˆ’1,ğ‘–ğ‘– âˆ’ â‹¯ âˆ’ ğœ™ğœ™ğ‘ğ‘ğ‘¥ğ‘¥ğ‘¡ğ‘¡âˆ’ğ‘ğ‘,ğ‘–ğ‘–
ğ‘€ğ‘€
ğ‘‡ğ‘‡ = ğ‘ğ‘ âˆ’ ğ‘ğ‘
ğ‘ğ‘:
ğ‘ğ‘:
process. 
ğœ™ğœ™ğ‘—ğ‘—

 autoregressive degree, 
 sample size,                                                  
  are  the  AR(p)  model  parameters  for 

  and 

  iid  error  terms  with  white  noise 

ğ‘ğ‘ğ‘¡ğ‘¡

ğ‘—ğ‘— = 1,2, â€¦ , ğ‘ğ‘

4 

 
 
 
 
 
 
 
 
 
 
 
                                        
         
 
 
 
 
 
 
 
 
 
      
                  
 
        
 
        
  
3.1 Normal distribution assumption 

The parameter  estimations and variable selection of regression model given in (4) can be done 
with penalized objective function as below. (Yoon et al., 2012) 

ğ‘ğ‘

ğ‘ğ‘

2

ğ‘€ğ‘€

ğ‘ğ‘

     (6) 

ğ‘‡ğ‘‡
ğ‘„ğ‘„ğ‘ğ‘(ğœ·ğœ·, ğ“ğ“) = ï¿½ ï¿½ğ‘¦ğ‘¦ğ‘¡ğ‘¡ âˆ’ ğ’™ğ’™ğ’•ğ’•

ğ‘¡ğ‘¡=ğ‘ğ‘+1

ğ‘—ğ‘—=1

ğœ·ğœ· âˆ’ ï¿½ ğœ™ğœ™ğ‘—ğ‘—ï¿½ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’ğ‘—ğ‘— âˆ’ ğ’™ğ’™ğ‘¡ğ‘¡âˆ’ğ‘—ğ‘—

ğœ·ğœ·ï¿½

ï¿½

+ ğ‘‡ğ‘‡ ï¿½  ğ‘ğ‘ğœ†ğœ†ğ‘–ğ‘–|ğ›½ğ›½ğ‘–ğ‘–|   + ğ‘‡ğ‘‡ ï¿½ ğ‘ğ‘ğ›¿ğ›¿ğ‘—ğ‘— ï¿½ğœ™ğœ™ğ‘—ğ‘—ï¿½

ğ‘–ğ‘–=1

ğ‘—ğ‘—=1

ğ‘‡ğ‘‡

Under the assumptions that 
log likelihood function with penalized term as follows. 

2

, this function also equivalently can be used as negative 

ğ‘ğ‘ğ‘¡ğ‘¡~ğ‘ğ‘(0, ğœğœ

)

ğ‘€ğ‘€

ğ‘ğ‘

     (7) 

|ğ‘ğ‘ğ‘¡ğ‘¡) + ğ‘‡ğ‘‡ ï¿½  ğ‘ğ‘ğœ†ğœ†ğ‘–ğ‘–|ğ›½ğ›½ğ‘–ğ‘–|   + ğ‘‡ğ‘‡ ï¿½ ğ‘ğ‘ğ›¿ğ›¿ğ‘—ğ‘— ï¿½ğœ™ğœ™ğ‘—ğ‘—ï¿½ 

ğ‘–ğ‘–=1

ğ‘—ğ‘—=1

2

ğ‘„ğ‘„(ğœ·ğœ·, ğ“ğ“, ğœğœ

) = âˆ’lnğ¿ğ¿ğ‘ğ‘(ğœ·ğœ·, ğ“ğ“, ğœğœ

2

where 

, 

 penalize function for 
 penalize function for 
ğ›½ğ›½
ğœ™ğœ™

ğ‘ğ‘ğœ†ğœ†ğ‘–ğ‘–|ğ›½ğ›½ğ‘–ğ‘–|:
ğ‘ğ‘ğ›¿ğ›¿ğ‘—ğ‘— ï¿½ğœ™ğœ™ğ‘—ğ‘—ï¿½:
and 

, 

           (8) 

2

2

1

ğ‘‡ğ‘‡
2 ln(ğœğœ

2

|ğ‘ğ‘ğ‘¡ğ‘¡) = ğ‘ğ‘ âˆ’

ğ‘ğ‘
ğ‘¡ğ‘¡=ğ‘ğ‘+1  ï¿½Î¦(ğµğµ)ğ‘¦ğ‘¦ğ‘¡ğ‘¡ âˆ’ âˆ‘ ğ›½ğ›½ğ‘–ğ‘–

ğ‘€ğ‘€
ğ‘–ğ‘–=1 Î¦(ğµğµ)ğ‘¥ğ‘¥ğ‘¡ğ‘¡,ğ‘–ğ‘–ï¿½

2 âˆ‘

2ğœğœ

lnğ¿ğ¿(ğœ·ğœ·, ğ“ğ“, ğœğœ

) âˆ’
is the log likelihood function of the normal distribution. Someone can replace 
  function 
with interested penalized methods penalize function then the parameter estimation and the variable 
selection  can  be  done  simultaneously  by  minimizing  equation  (6)  or  maximizing  equation  (7). 
Note  that  according  to  Yoon  et  al.  (2012),  sake  of  the  ease  of  the  computation,  penalizing  the 
autoregressive  parameters  are  ignored.  In  this  study,  we  also  calculate  both  regression  and 
autoregression parameter estimations with only penalizing the regression model parameters. 
The  penalized  functions  belong  to  LASSO,  SCAD,  bridge  and  elastic-net  methods  are  not 
differentiable at the origin for the minimization problem in equation (6) and because they are not 
concave, Local Quadratic Approximation (LQA) which is proposed by Fun and Li (2001) can be 
used to solve this problem. According to the LQA the penalized functions are locally approximated 
at 

 in the following quadratic function  

ğ‘ğ‘ğœ†ğœ†ğ‘–ğ‘–|ğ›½ğ›½ğ‘–ğ‘–|

(0)

ğœ·ğœ·

0
= (ğ›½ğ›½1

0
 , â€¦ , ğ›½ğ›½ğ‘€ğ‘€

 )

ğ‘ğ‘ğœ†ğœ†ğ‘–ğ‘–(|ğ›½ğ›½ğ‘–ğ‘–|) â‰ˆ ğ‘ğ‘ğœ†ğœ†ğ‘–ğ‘– ï¿½ï¿½ğ›½ğ›½ğ‘–ğ‘–

(0)

â€²
ğ‘ğ‘ğœ†ğœ†

(0)

ï¿½ï¿½ğ›½ğ›½ğ‘–ğ‘–
(0)

ï¿½ğ›½ğ›½ğ‘–ğ‘–

ï¿½

=

1
2

where, 

â€²
ğ‘ğ‘ğœ†ğœ†

1
2

(0)

ğ‘–ğ‘– ï¿½ï¿½ğ›½ğ›½ğ‘–ğ‘–
(0)

ï¿½ğ›½ğ›½ğ‘–ğ‘–

ï¿½

ï¿½ï¿½ +

ï¿½ï¿½

2
 ï¿½ğ›½ğ›½ğ‘–ğ‘–

(0)2

âˆ’ ğ›½ğ›½ğ‘–ğ‘–

ï¿½

ï¿½ï¿½

2
ğ›½ğ›½ğ‘–ğ‘–

(0)

+ ğ‘ğ‘ğœ†ğœ† ï¿½ï¿½ğ›½ğ›½ğ‘–ğ‘–

ï¿½ï¿½ âˆ’

â€²
ğ‘ğ‘ğœ†ğœ†

(0)

ï¿½ï¿½ğ›½ğ›½ğ‘–ğ‘–
(0)

ï¿½ğ›½ğ›½ğ‘–ğ‘–

ï¿½

1
2

ï¿½ï¿½

(0)2

ğ›½ğ›½ğ‘–ğ‘–

5 

                                                     (9)

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 : derivative of the interested penalized function 

ğ‘–ğ‘–
(0)

: initial values of 

â€²
ğ‘ğ‘ğœ†ğœ†
By using this LQA method the minimization problem in equation (9) will be reduced the following 
ğ›½ğ›½ğ‘–ğ‘–
objective function around 

ğ›½ğ›½ğ‘–ğ‘–

. 

(0)

ğ›½ğ›½

ğ‘ğ‘

ğ‘ğ‘

2

ğ‘€ğ‘€

2

ğ‘‡ğ‘‡
) = ï¿½ ï¿½ğ‘¦ğ‘¦ğ‘¡ğ‘¡ âˆ’ ğ’™ğ’™ğ’•ğ’•

ğ‘„ğ‘„(ğœ·ğœ·, ğ“ğ“, ğœğœ

      (10)
By minimizing the above objective function the parameter estimations and the variable selections 
for  autoregressive  error  term  regression  model  with  t  distribution  assumption  can  be  made 
simultaneously.  

ğœ·ğœ· âˆ’ ï¿½ ğœ™ğœ™ğ‘—ğ‘—ï¿½ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’ğ‘—ğ‘— âˆ’ ğ’™ğ’™ğ’•ğ’•âˆ’ğ’‹ğ’‹

ï¿½  
ğ‘–ğ‘–=1

ğ‘¡ğ‘¡=ğ‘ğ‘+1

ï¿½ğ›½ğ›½ğ‘–ğ‘–

ğœ·ğœ·ï¿½

ğ‘—ğ‘—=1

+

ï¿½

ï¿½

ğ‘‡ğ‘‡
2

ğ‘‡ğ‘‡

â€²
ğ‘ğ‘ğœ†ğœ†ğ‘–ğ‘–

(0)

ï¿½ï¿½ğ›½ğ›½ğ‘–ğ‘–
(0)

ï¿½ï¿½

2
ğ›½ğ›½ğ‘–ğ‘–

ğ‘»ğ‘»
ğœ·ğœ·ï¿½ = ï¿½Î¦ï¿½ (ğµğµ)ğ‘¿ğ‘¿

Î¦ï¿½ (ğµğµ)ğ‘¿ğ‘¿ + ğ‘‡ğ‘‡ğ‘ğ‘ğœ†ğœ†(ğ›€ğ›€)ï¿½

âˆ’1

ğ‘»ğ‘»
ï¿½Î¦ï¿½ (ğµğµ)ğ‘¿ğ‘¿

Î¦ï¿½ (ğµğµ)ğ’€ğ’€ï¿½

where 

                                                  (11) 

:  is  the  diagonal  matrix  derived  from  LQA  method  with  penalty  parameter  of  interested 

variable selection method. 
ğ‘ğ‘ğœ†ğœ†(ğ›€ğ›€)

where 

âˆ’ğŸğŸ

ğ“ğ“ï¿½ = ğ‘ğ‘ï¿½ğœ·ğœ·ï¿½ï¿½

ğ‘…ğ‘…0ï¿½ğœ·ğœ·ï¿½ï¿½                                                                                                                      (12)

ğ‘ğ‘

ğ‘ğ‘

ğ‘ğ‘

ğ‘ğ‘

ğ‘…ğ‘…0ï¿½ğœ·ğœ·ï¿½ï¿½ =

ï¿½ ğ‘’ğ‘’ğ‘¡ğ‘¡ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’1
ğ‘¡ğ‘¡=ğ‘ğ‘+1
ğ‘ğ‘

ï¿½ ğ‘’ğ‘’ğ‘¡ğ‘¡ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’2
ğ‘¡ğ‘¡=ğ‘ğ‘+1

ğ‘ğ‘

â‹®

ï¿½ ğ‘’ğ‘’ğ‘¡ğ‘¡ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’ğ‘ğ‘
ğ‘¡ğ‘¡=ğ‘ğ‘+1

â¡
â¢
â¢
â¢
â¢
â¢
â¢
â¢
â¢
â£

â¤
â¥
â¥
â¥
â¥
â¥
â¥
â¥
â¥
â¦

ğ‘ğ‘

, ğ‘ğ‘ï¿½ğœ·ğœ·ï¿½ï¿½   =

2

ï¿½ ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’2ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’1
ğ‘¡ğ‘¡=ğ‘ğ‘+1

ï¿½ ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’1
ğ‘¡ğ‘¡=ğ‘ğ‘+1
ğ‘ğ‘

â¡
â¢
â¢
â¢
â¢
â¢
â¢
â¢
ï¿½ ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’ğ‘ğ‘ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’1
â¢
ğ‘¡ğ‘¡=ğ‘ğ‘+1
â£

ğ‘ğ‘

â‹®

ï¿½ ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’1ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’2
ğ‘¡ğ‘¡=ğ‘ğ‘+1
ğ‘ğ‘

2

ï¿½ ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’2
ğ‘¡ğ‘¡=ğ‘ğ‘+1

ğ‘ğ‘

â‹®

ï¿½ ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’ğ‘ğ‘ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’2
ğ‘¡ğ‘¡=ğ‘ğ‘+1

ï¿½ ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’1ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’ğ‘ğ‘
ğ‘¡ğ‘¡=ğ‘ğ‘+1
ğ‘ğ‘

ï¿½ ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’2ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’ğ‘ğ‘
ğ‘¡ğ‘¡=ğ‘ğ‘+1

â‹®

ğ‘ğ‘

2

ï¿½ ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’ğ‘ğ‘
ğ‘¡ğ‘¡=ğ‘ğ‘+1

â€¦
â€¦

â‹±

â€¦

.

â¤
â¥
â¥
â¥
â¥
â¥
â¥
â¥
â¥
â¦

2

ğœğœï¿½

=

1
ğ‘‡ğ‘‡

ï¿½ ï¿½Î¦ï¿½ (ğµğµ)ğ‘¦ğ‘¦ğ‘¡ğ‘¡ âˆ’ Î¦ï¿½ (ğµğµ)ğ’™ğ’™ğ’•ğ’•ğœ·ğœ·ï¿½ï¿½
ğ‘¡ğ‘¡=ğ‘ğ‘+1

2

3.2 t distribution assumption 

                                                                                      (13)

In this paper, we use ECM algorithm to find the CML estimates. Note that when t distribution is 
considered  in  different  models,  the  ECM  algorithm  is  extensively  used  because  of  the  normal 

6 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
mixture representation of the t distribution. In the next section, we show how we employ the ECM 
algorithm to estimate the AR(p) regression model parameters under t distribution assumption. The 
probability density  function (pdf)  of Studentâ€™s t distribution which  we used in this article is as 
follow. 

where 

ğ‘“ğ‘“(ğ‘¥ğ‘¥) =

ğ‘ğ‘ğ‘£ğ‘£
ğœğœ
Î½+1

Î“ï¿½

2 ï¿½ğœˆğœˆ
ğ‘£ğ‘£
2ï¿½

âˆšğœ‹ğœ‹Î“ï¿½

ğ‘ğ‘ğ‘£ğ‘£ =

âˆ’

ğ‘£ğ‘£+1
2

2

2ï¿½

,                                       âˆ’ âˆ < ğ‘¥ğ‘¥ < âˆ                                         (14)

ï¿½ğœˆğœˆ +

ğ‘¥ğ‘¥
ğœğœ

ğœˆğœˆ/2

 , 

 degrees of freedom and 

 scale parameter.  

2

ğœˆğœˆ > 0

ğœğœ

> 0

The t distribution is a symmetric heavy tailed alternative to the normal distribution and it is more 
resistant  than  the  normal  distribution  to  the  outliers.    Consider  the  model  in  equation  (5)  and 
assume  that  error  terms  have  the  t  distribution
.  The  CML  estimators  of  the 
  can  be  obtained  by  using  the  following  conditional  log 
unknown  parameters 
likelihood function 

 ï¿½ğ‘ğ‘ğ‘¡ğ‘¡~ğ‘¡ğ‘¡ğœˆğœˆ(0, ğœğœ

, ğœˆğœˆ)ï¿½

2

2

ğœ½ğœ½ = (ğœ·ğœ·, ğ“ğ“, ğœğœ

, ğœˆğœˆ)

ğ‘ğ‘

ğ‘£ğ‘£ + 1
2

ï¿½Î¦(ğµğµ)ğ‘¦ğ‘¦ğ‘¡ğ‘¡ âˆ’ âˆ‘ ğ›½ğ›½ğ‘–ğ‘–

2
ğ‘€ğ‘€
ğ‘–ğ‘–=1 Î¦(ğµğµ)ğ‘¥ğ‘¥ğ‘¡ğ‘¡,ğ‘–ğ‘– ï¿½
ğœğœ

ğ‘¡ğ‘¡=ğ‘ğ‘+1 

ï¿½ğ‘£ğ‘£ +

ln ğ¿ğ¿(ğœ½ğœ½|ğ‘ğ‘ğ‘¡ğ‘¡) = ln ğ‘ğ‘ğœˆğœˆ âˆ’ ğ‘‡ğ‘‡ ln ğœğœ âˆ’
ï¿½.             (15)
ï¿½ ln
  can  be  obtained  by  maximizing  this  conditional  log 
The  estimators  of  the  parameter  vector 
likelihood function. However, it is not tractable to obtain the maximum of this function. Therefore, 
some numerical methods should be used to obtain the CML estimates of the parameters. Among 
these numerical methods, the ECM algorithm will be slightly easy to use because of the normal 
mixture  representation  of  the  t  distribution.  First,  hierarchical  formulation  in  terms  of  the 
conditional distributions can be expressed as follows. 

ğœ½ğœ½

2

ğ‘»ğ‘»
Î¦(ğµğµ)ğ‘¦ğ‘¦ğ‘¡ğ‘¡|ğ‘¢ğ‘¢ğ‘¡ğ‘¡~ğ‘ğ‘ ï¿½Î¦(ğµğµ)ğ’™ğ’™ğ’•ğ’•

ğœ·ğœ·,

2

ğœğœ
ğ‘¢ğ‘¢ğ‘¡ğ‘¡ï¿½ ,                                                                                                 (16)

ğ‘¢ğ‘¢ğ‘¡ğ‘¡~ğºğºğ‘ğ‘ğºğºğºğºğ‘ğ‘ ï¿½

ğœˆğœˆ
2

ğœˆğœˆ
 is missing and 
2

ï¿½,

,

  is the observed data, respectively. Then the conditional 

where   
pdf is  

ğ’–ğ’– = (ğ‘¢ğ‘¢1, â€¦ , ğ‘¢ğ‘¢ğ‘›ğ‘›)

ğ‘¦ğ‘¦

ğ‘¢ğ‘¢ğ‘¡ğ‘¡|Î¦(ğµğµ)ğ‘¦ğ‘¦ğ‘¡ğ‘¡ ~ ğºğºğ‘ğ‘ğºğºğºğºğ‘ğ‘ ï¿½

2
 (ğœˆğœˆ + ğœ…ğœ…ğ‘¡ğ‘¡

)ï¿½,                                                                        (17)

ğœˆğœˆ + 1
2

,

1
2

. Considering 

ğŸğŸ

 is missing data and the complete data log likelihood 

where 
2
function for 
ğœ…ğœ…ğ‘¡ğ‘¡
=

ğ‘‡ğ‘‡
ï¿½Î¦(ğµğµ)ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’Î¦(ğµğµ)ğ’™ğ’™ğ‘¡ğ‘¡
ğœ·ğœ·ï¿½
 can be written as 

2

ğœğœ

(ğ’šğ’š, ğ’–ğ’–)

ğ‘¢ğ‘¢

ğ‘ğ‘

ğ‘™ğ‘™ğ‘ğ‘(ğœ½ğœ½; ğ’šğ’š, ğ’–ğ’–) = âˆ’

2

ln(2ğœ‹ğœ‹ğœğœ

) +

ğ‘‡ğ‘‡
2

1
2

ï¿½ ln ğ‘¢ğ‘¢ğ‘¡ğ‘¡
ğ‘¡ğ‘¡=ğ‘ğ‘+1
7 

 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
 
 
 
ğ‘ğ‘

2
ğœ…ğœ…ğ‘¡ğ‘¡
2

ğœˆğœˆ
2

ğœˆğœˆ
2

ğ‘ğ‘

âˆ’ ï¿½ ï¿½
ğ‘¡ğ‘¡=ğ‘ğ‘+1

+ ğ‘‡ğ‘‡ ln ğ‘ğ‘ğœˆğœˆ                                 (18)
According to the procedure of the ECM algorithm, the conditional expectation of the complete 
data log likelihood function, given the observed data and the current parameter estimate should be 
calculated. Therefore, the following conditional expectation should be calculated. 

âˆ’ 1ï¿½  ï¿½ ln ğ‘¢ğ‘¢ğ‘¡ğ‘¡
ğ‘¡ğ‘¡=ğ‘ğ‘+1

ğ‘¢ğ‘¢ğ‘¡ğ‘¡ + ï¿½

+

ï¿½

ğ¸ğ¸(ğ‘™ğ‘™ğ‘ğ‘(ğœ½ğœ½; ğ’šğ’š, ğ’–ğ’–)|Î¦(ğµğµ)ğ‘¦ğ‘¦ğ‘¡ğ‘¡) = âˆ’

2

ğ‘‡ğ‘‡
ln(2ğœ‹ğœ‹ğœğœ
2
ğ‘ğ‘

) + ğ‘‡ğ‘‡ ln ğ‘ğ‘ğœˆğœˆ

+

1
2

ï¿½ ğ¸ğ¸(ln ğ‘¢ğ‘¢ğ‘¡ğ‘¡ |Î¦(ğµğµ)ğ‘¦ğ‘¦ğ‘¡ğ‘¡)
ğ‘¡ğ‘¡=ğ‘ğ‘+1

ğ‘ğ‘

âˆ’ ï¿½
ğ‘¡ğ‘¡=ğ‘ğ‘+1

ğ‘ğ‘

2
ğœ…ğœ…ğ‘¡ğ‘¡
2

ğ¸ğ¸(ğ‘¢ğ‘¢ğ‘¡ğ‘¡|Î¦(ğµğµ)ğ‘¦ğ‘¦ğ‘¡ğ‘¡)

âˆ’

ğœˆğœˆ
2

ï¿½ ğ¸ğ¸(ğ‘¢ğ‘¢ğ‘¡ğ‘¡|Î¦(ğµğµ)ğ‘¦ğ‘¦ğ‘¡ğ‘¡)
ğ‘¡ğ‘¡=ğ‘ğ‘+1

ğ‘µğ‘µ

The conditional expectations 

ğ‚ğ‚
ğŸğŸ

+ ï¿½
 and 

âˆ’ ğŸğŸï¿½ ï¿½ ğ‘¬ğ‘¬( ğ¥ğ¥ğ¥ğ¥ ğ’–ğ’–ğ’•ğ’• |ğš½ğš½(ğ‘©ğ‘©)ğ’šğ’šğ’•ğ’•)

                         (ğŸğŸğŸğŸ)
  can be computed as follows. 

ğ’•ğ’•=ğ’‘ğ’‘+ğŸğŸ

ğ¸ğ¸(ğ‘¢ğ‘¢ğ‘¡ğ‘¡|Î¦(ğµğµ)ğ‘¦ğ‘¦ğ‘¡ğ‘¡)

ğ¸ğ¸(ln ğ‘¢ğ‘¢ğ‘¡ğ‘¡ |Î¦(ğµğµ)ğ‘¦ğ‘¦ğ‘¡ğ‘¡)

ğ¸ğ¸(ğ‘¢ğ‘¢ğ‘¡ğ‘¡|Î¦(ğµğµ)ğ‘¦ğ‘¦ğ‘¡ğ‘¡) =

ğœˆğœˆ + 1
ğœˆğœˆ + ğœ…ğœ…ğ‘¡ğ‘¡

2 ,                                                                                                           (20)

ğ¸ğ¸(ln ğ‘¢ğ‘¢ğ‘¡ğ‘¡ |Î¦(ğµğµ)ğ‘¦ğ‘¦ğ‘¡ğ‘¡) = ğ·ğ·ğºğº ï¿½

2
 (ğœˆğœˆ + ğœ…ğœ…ğ‘¡ğ‘¡
 is digamma function. Then the steps of the ECM algorithm can be given as follows.  

)ï¿½,                                                         (21)

ğœˆğœˆ + 1
2

ï¿½ âˆ’ ln ï¿½

1
2

where 

ğ·ğ·ğºğº

The steps of the ECM algorithm: 
The steps of the ECM algorithm for this study can be briefly summarized as follows. 
E step Find the conditional expectations of the complete data log likelihood function given 
current  parameter  estimates 
 to be maximized. 
M step Maximize the objective function 
ğ‘„ğ‘„ï¿½ğœ½ğœ½; ğœ½ğœ½
(ğ‘˜ğ‘˜)
new estimates 
These steps are very general. For the details of the algorithm to complete the estimates to following 
IRA can be given for the t distribution.  

 and 
.  These  conditional  expectations  result  the  objective  function 

 with respect to the parameters 

 to obtain the 

ğ‘„ğ‘„ï¿½ğœ½ğœ½; ğœ½ğœ½

(ğ’Œğ’Œ+ğŸğŸ)

ğ‘¦ğ‘¦ğ‘¡ğ‘¡

(ğ‘˜ğ‘˜)

.  

(ğ’Œğ’Œ)

ğœ½ğœ½

ğœ½ğœ½

ğœ½ğœ½

ï¿½

ï¿½

8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
1.  Determine  the  initial  value 
stopping criteria 

.  

(ğŸğŸ)

(ğŸğŸ)

(ğŸğŸ)

2(1)

(1)

ğœ½ğœ½

= ï¿½ğœ·ğœ·

, ğ“ğ“

, ğœğœ

, ğœˆğœˆ

ï¿½

    of  the  parameter  vector 

    and 

ğœ½ğœ½

2.  (E-step)  Compute  the  following  conditional  expectations  using  the  current  estimates             

ğœ‚ğœ‚

(ğ’Œğ’Œ)

(ğ’Œğ’Œ)

(ğ’Œğ’Œ)

2(ğ‘˜ğ‘˜)

(ğ‘˜ğ‘˜)

ğœ½ğœ½

= ï¿½ğœ·ğœ·

, ğ“ğ“

, ğœğœ

, ğœˆğœˆ

ï¿½

ğ‘˜ğ‘˜ = 1,2,3 â€¦ 

 for 

(ğ‘˜ğ‘˜)
ğ‘†ğ‘†Ì‚1ğ‘¡ğ‘¡

= ğ¸ğ¸ï¿½ğ‘¢ğ‘¢ğ‘¡ğ‘¡ï¿½Î¦(ğµğµ)ğ‘¦ğ‘¦ğ‘¡ğ‘¡, ğœ½ğœ½ï¿½

(ğ’Œğ’Œ)

ï¿½ =

ğœˆğœˆ
(ğ‘˜ğ‘˜)

ğœˆğœˆ

(ğ’Œğ’Œ)

(ğ‘˜ğ‘˜)

+ 1

+ ğœ…ğœ…ğ‘¡ğ‘¡
ğœˆğœˆ

(ğ‘˜ğ‘˜)2                                                                            (22)
(ğ‘˜ğ‘˜)

(ğ‘˜ğ‘˜)
ğ‘†ğ‘†Ì‚2ğ‘¡ğ‘¡

= ğ¸ğ¸ï¿½ln ğ‘¢ğ‘¢ğ‘¡ğ‘¡ ï¿½Î¦(ğµğµ)ğ‘¦ğ‘¦ğ‘¡ğ‘¡, ğœ½ğœ½ï¿½

+ ğœ…ğœ…ğ‘¡ğ‘¡
3.  (E-step)  Use  these  conditional  expectations  in  the  objective  function 

ï¿½ï¿½                  (23)
    to  form  the 
. By writing these conditional expectations 
following function to be maximized with respect to 
in equation (21) the objective function to be maximized at M step can be obtained as follows:  

ğ‘„ğ‘„ï¿½ğœ½ğœ½; ğœ½ğœ½ï¿½ï¿½

ï¿½ = ğ·ğ·ğºğº ï¿½

ï¿½ âˆ’ ln ï¿½

 ï¿½ğœˆğœˆ

+ 1
2

1
2

(ğ‘˜ğ‘˜)

(ğ‘˜ğ‘˜)2

ğœ½ğœ½

ğ‘ğ‘

) + ğ‘‡ğ‘‡ ln(ğ‘ğ‘ğœˆğœˆ) +
ğ‘ğ‘

(ğ‘˜ğ‘˜)
ï¿½ ğ‘†ğ‘†Ì‚2ğ‘¡ğ‘¡
ğ‘¡ğ‘¡=ğ‘ğ‘+1

1
2

(ğ‘˜ğ‘˜)

ğ‘„ğ‘„ï¿½ğœ½ğœ½; ğœ½ğœ½ï¿½

ï¿½ = âˆ’

2

ln(2ğœ‹ğœ‹ğœğœ

ğ‘‡ğ‘‡
2

ğ‘ğ‘

ğœˆğœˆ
2
4. (CM step) Maximize 

âˆ’

(ğ‘˜ğ‘˜)
ï¿½ ğ‘†ğ‘†Ì‚1ğ‘¡ğ‘¡
ğ‘¡ğ‘¡=ğ‘ğ‘+1

+ ï¿½

ğœˆğœˆ
2

(ğ‘˜ğ‘˜)
âˆ’ 1ï¿½ ï¿½ ğ‘†ğ‘†Ì‚2ğ‘¡ğ‘¡
ğ‘¡ğ‘¡=ğ‘ğ‘+1
 with respect to 

âˆ’

ğ‘‡ğ‘‡ğœ†ğœ†
2

ğ‘ğ‘

âˆ’ ï¿½
ğ‘¡ğ‘¡=ğ‘ğ‘+1

2
ğœ…ğœ…ğ‘¡ğ‘¡
2

(ğ‘˜ğ‘˜)
ğ‘†ğ‘†Ì‚1ğ‘¡ğ‘¡

ğ‘€ğ‘€

ï¿½ï¿½ğ›½ğ›½ğ‘–ğ‘–
ğ‘–ğ‘–=1

âˆ’1

(0)

2
ğ›½ğ›½ğ‘–ğ‘–
ï¿½
 to obtain  

                              (24)

(ğ’Œğ’Œ+ğŸğŸ)

th iteration  
= ï¿½ğœ·ğœ·ï¿½
ğœ½ğœ½

(ğ’Œğ’Œ+ğŸğŸ)

, ğ“ğ“ï¿½

(ğ‘˜ğ‘˜)

(ğ‘˜ğ‘˜+1)

2(ğ‘˜ğ‘˜+1)

ğ‘„ğ‘„ï¿½ğœ½ğœ½; ğœ½ğœ½ï¿½
, ğœğœï¿½

. By doing this we obtain following estimates at (k+1) 

2

ğœ½ğœ½ = (ğœ·ğœ·, ğ“ğ“, ğœğœ

, ğœˆğœˆ)

(ğ‘˜ğ‘˜+1)

ï¿½

ï¿½ 
, ğœˆğœˆ

(ğ‘˜ğ‘˜)
ğœ·ğœ·ï¿½ = ï¿½ğ‘†ğ‘†Ì‚1ğ‘¡ğ‘¡

ğ‘»ğ‘»
Î¦ï¿½ (ğµğµ)ğ‘¿ğ‘¿

âˆ’1

Î¦ï¿½ (ğµğµ)ğ‘¿ğ‘¿ + ğ‘‡ğ‘‡ğ‘ğ‘ğœ†ğœ†(ğ›€ğ›€)ï¿½

                                  (25) 

(ğ‘˜ğ‘˜)
ï¿½ğ‘†ğ‘†Ì‚1ğ‘¡ğ‘¡

ğ‘»ğ‘»
Î¦ï¿½ (ğµğµ)ğ‘¿ğ‘¿

ğğÎ¦ï¿½ (ğµğµ)ğ’€ğ’€ï¿½

(ğ‘˜ğ‘˜+1)

ğ“ğ“ï¿½

âˆ’ğŸğŸ

= (ğ‘¹ğ‘¹ğ’”ğ’”)

(ğ‘¹ğ‘¹ğŸğŸğ’”ğ’”)                                                                                                                (26)

ğ‘ğ‘

2(ğ‘˜ğ‘˜+1)

(ğ‘˜ğ‘˜)
ï¿½ ï¿½ ğ‘†ğ‘†Ì‚1ğ‘¡ğ‘¡
ğ‘¡ğ‘¡=ğ‘ğ‘+1

1
ğ‘‡ğ‘‡

ğ‘‡ğ‘‡
ï¿½Î¦ï¿½ (ğµğµ)ğ‘¦ğ‘¦ğ‘¡ğ‘¡ âˆ’ Î¦ï¿½ (ğµğµ)ğ’™ğ’™ğ‘¡ğ‘¡

2

(ğ‘˜ğ‘˜)

ğœğœï¿½

=

ğœ·ğœ·ï¿½
:  is  the  diagonal  matrix  derived  from  LQA  method  with  penalty  parameter  of  interested 

ï¿½                                                        (27)

ï¿½

variable selection method. 
ğ‘ğ‘ğœ†ğœ†(ğ›€ğ›€)
Here 

9 

 
 
   
  
 
 
 
 
 
 
 
 
 
 
 
 
 
ğ‘ğ‘

ğ‘ğ‘

(ğ‘˜ğ‘˜)
ï¿½ ğ‘†ğ‘†Ì‚1ğ‘¡ğ‘¡
ğ‘¡ğ‘¡=ğ‘ğ‘+1

(ğ‘˜ğ‘˜)
ï¿½ ğ‘†ğ‘†Ì‚1ğ‘¡ğ‘¡
ğ‘¡ğ‘¡=ğ‘ğ‘+1
ğ‘ğ‘

â¡
â¢
â¢
â¢
â¢
â¢
â‹®
â¢
â¢
(ğ‘˜ğ‘˜)
ï¿½ ğ‘†ğ‘†Ì‚1ğ‘¡ğ‘¡
â¢
ğ‘¡ğ‘¡=ğ‘ğ‘+1
â£

ğ‘ğ‘

ğ‘’ğ‘’ğ‘¡ğ‘¡ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’1

ğ‘’ğ‘’ğ‘¡ğ‘¡ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’2

ğ‘’ğ‘’ğ‘¡ğ‘¡ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’ğ‘ğ‘

â¤
â¥
â¥
â¥
â¥
â¥
â¥
â¥
â¥
â¦

(ğ‘˜ğ‘˜)
ï¿½ ğ‘†ğ‘†Ì‚1ğ‘¡ğ‘¡
ğ‘¡ğ‘¡=ğ‘ğ‘+1
ğ‘ğ‘

(ğ‘˜ğ‘˜)
ï¿½ ğ‘†ğ‘†Ì‚1ğ‘¡ğ‘¡
ğ‘¡ğ‘¡=ğ‘ğ‘+1

â¡
â¢
â¢
â¢
â¢
â¢
â‹®
â¢
â¢
(ğ‘˜ğ‘˜)
ï¿½ ğ‘†ğ‘†Ì‚1ğ‘¡ğ‘¡
â¢
ğ‘¡ğ‘¡=ğ‘ğ‘+1
â£

ğ‘ğ‘

2
ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’1

ğ‘ğ‘

(ğ‘˜ğ‘˜)
ï¿½ ğ‘†ğ‘†Ì‚1ğ‘¡ğ‘¡
ğ‘¡ğ‘¡=ğ‘ğ‘+1
ğ‘ğ‘

ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’1ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’2

â€¦

ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’2ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’1

(ğ‘˜ğ‘˜)
ï¿½ ğ‘†ğ‘†Ì‚1ğ‘¡ğ‘¡
ğ‘¡ğ‘¡=ğ‘ğ‘+1

2
ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’2

      â€¦    

ğ‘ğ‘

(ğ‘˜ğ‘˜)
ï¿½ ğ‘†ğ‘†Ì‚1ğ‘¡ğ‘¡
ğ‘¡ğ‘¡=ğ‘ğ‘+1
ğ‘ğ‘

(ğ‘˜ğ‘˜)
ï¿½ ğ‘†ğ‘†Ì‚1ğ‘¡ğ‘¡
ğ‘¡ğ‘¡=ğ‘ğ‘+1

ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’1ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’ğ‘ğ‘

ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’2ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’ğ‘ğ‘

ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’ğ‘ğ‘ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’1

ğ‘ğ‘

â‹®
(ğ‘˜ğ‘˜)
ï¿½ ğ‘†ğ‘†Ì‚1ğ‘¡ğ‘¡
ğ‘¡ğ‘¡=ğ‘ğ‘+1

â‹±

â€¦

ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’ğ‘ğ‘ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’2

 estimate of 

ğ‘ğ‘

â‹®
(ğ‘˜ğ‘˜)
ï¿½ ğ‘†ğ‘†Ì‚1ğ‘¡ğ‘¡
ğ‘¡ğ‘¡=ğ‘ğ‘+1

2
ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’ğ‘ğ‘

â¤
â¥
â¥
â¥
â¥
â¥
â¥
â¥
â¥
â¦

5. (CM-step) Solve the following equation to obtain the 

ğ‘¹ğ‘¹ğŸğŸğ’”ğ’” =

, ğ‘¹ğ‘¹ğ’”ğ’” =

(ğ‘˜ğ‘˜)

ğœ•ğœ•ğ‘„ğ‘„ï¿½ğœ½ğœ½; ğœ½ğœ½ï¿½
ğœ•ğœ•ğœˆğœˆ

ğ‘ğ‘

ï¿½

1
2

= ï¿½ ï¿½
ğ‘¡ğ‘¡=ğ‘ğ‘+1
ğ‘ğ‘

ln ï¿½

ğœˆğœˆ
2

ï¿½ +

1
2

âˆ’

1
2

â€²

Î“

ï¿½

ğœˆğœˆ
ï¿½
2
ğœˆğœˆ
2ï¿½

Î“ ï¿½

(ğ‘˜ğ‘˜ + 1) ğ‘¡ğ‘¡â„

ğœˆğœˆ

+

1
2

(ğ‘˜ğ‘˜)
ğ‘†ğ‘†Ì‚2ğ‘¡ğ‘¡

âˆ’

(ğ‘˜ğ‘˜)
ğ‘†ğ‘†Ì‚1ğ‘¡ğ‘¡

1
2

ï¿½

                                      (28)

= ï¿½ ï¿½DG ï¿½

ï¿½ âˆ’ ln ï¿½

ğ‘¡ğ‘¡=ğ‘ğ‘+1

ğœˆğœˆ
2

(ğ‘˜ğ‘˜)
ğœˆğœˆ
ï¿½ âˆ’ 1 âˆ’ ğ‘†ğ‘†Ì‚2ğ‘¡ğ‘¡
2

(ğ‘˜ğ‘˜)
+ ğ‘†ğ‘†Ì‚1ğ‘¡ğ‘¡

ï¿½

= 0.                                        

7. These steps are calculated until the convergence criteria  

 is satisfied.  

(ğ’Œğ’Œ+ğŸğŸ)

ï¿½ğœ½ğœ½ï¿½

(ğ’Œğ’Œ)

âˆ’ ğœ½ğœ½ï¿½

ï¿½ < ğœ‚ğœ‚

4. Simulation study 

In  this  section,  an  extensive  simulation  study  is  given  to  compare  the  performances  of  model 
parameter estimation and variable selection obtained by using OLS, LASSO, SCAD, ridge, bridge 
and elastic net methods under autoregressive error term regression models based on t distributions 
for finite sample size. The calculations in the simulation study were made through the R 3.4.1 (R 
Core Team, 2020) program. 

4.1 Sampling designs 

â€²

and 

ğœ·ğœ·ğŸğŸ = (ğ›½ğ›½01, ğ›½ğ›½02, â€¦ , ğ›½ğ›½0ğ‘šğ‘š)

We consider three different sampling cases, representing commonly encountered problems in data 
denotes the true  parameter vectors. 
sets. 
  follows  the  AR(p)  process  as  in  equation  (2)  with  t  distribution.  Estimating  the  degrees  of 
ğ“ğ“ğŸğŸ = ï¿½ğœ™ğœ™01, ğœ™ğœ™02, â€¦ , ğœ™ğœ™0ğ‘ğ‘ï¿½
freedom  along  with  the  other  parameters  may  cause  the  influence  function  of  the  resulting 
ğ‘’ğ‘’ğ‘¡ğ‘¡
estimators unbounded  and  hence they  are not  going to  be robust  (Lucas,  1997).  Therefore, the 
degrees  of  freedom  is  usually  taken  as  fixed  and  treated  as  a  robustness  tuning  parameter  in 
robustness  studies  (for  example  see  Lange  et.al,  1989).  In  this  simulation  study  we  use  two 
different fixed degrees of freedom values for the t distribution. We choose the degrees of freedom 
).  First  value  of  this  distribution  provides  heavy-tailed  error 
as 
distributions and the second value of degrees of freedom provides a thin-tailed distribution.  

)  and 

  (

(

â€²

ğœˆğœˆ = 3

ğ‘¡ğ‘¡3

ğœˆğœˆ = 10 

ğ‘¡ğ‘¡10

While applying the LASSO method for model selection, the Î» parameter was constructed with 0.1 
increments  between  [0,  3]  and  BIC  values  were  calculated  for  each  result  obtained  from  this 
interval, and the model corresponding to the smallest BIC value was selected as the best model. 

10 

 
 
 
 
 
 
 
 
 
 
 
 
 
Similarly, the Î³ parameter for the bridge method was produced in 0.1 increments in the range of 
[0.1, 2], and the Î» parameter used in the bridge method was also included, and a two-dimensional 
grid  search  was  performed.  The  model  corresponding  to  the  smallest  obtained  BIC  value  was 
considered as the best model. For the SCAD method, the value suggested by Fun and Li (2001) 
was taken as  Î±=3.7 and  the Î»  parameter was set  up as 0.1 increments between [0, 3] as  in the 
LASSO method. For the elastic net method, Î»1 and Î»2 were scanned simultaneously and the model 
corresponding to the smallest of the BIC values for each result obtained was determined as the 
best model. Sample sizes for all cases were taken as 

. 

We consider three different simulation cases as explained below to show the performance of the 
mentioned methods. 

ğ‘ğ‘ = 50, 100, 300

Case 1: In this case we consider 8 covariates for the model (1) where 
. 
The  independent  variables  of  the  regression  model  are  independently  generated  from  the 
multivariate normal distribution 
. 
 and 
We use 

. In this case we use AR order 
 for 

ğœ·ğœ·ğŸğŸ = (3,1.5,0,0,2,0,0,0)

 and 

 for 

. 

â€²

ğœ™ğœ™ = 0.8

â€²
Case 2:  In this  case  we  would like to  show  the  performance of  the  variable  selection  methods 
under  the  higher  dimensional  case.  We  use  40  covariates  for  the  model  (1).  We  take  20 
insignificant and 20 significant independent variables with the following as  

ğ‘ğ‘ = 2

ğ‘ğ‘ = 1

ï¿½ğ‘ğ‘ğ‘¡ğ‘¡~ğ‘ğ‘ğ‘ğ‘(ğğ, ğšºğšº)ï¿½
ğ“ğ“ = (0.8, âˆ’0.2)

ğ‘ğ‘ = 1,2

ğœğœ = 1

 , 2 â€¦ , 2ï¿½ï¿½ï¿½ï¿½ï¿½
10
We generate the regression coefficients from multivariate normal distribution as in Case 1. 

ğœ·ğœ· = ï¿½0, â€¦ , 0,
ï¿½ï¿½ï¿½ï¿½ï¿½
10

,  0, â€¦ , 0,
ï¿½ï¿½ï¿½ï¿½ï¿½
10

 2, â€¦ ,2ï¿½ï¿½ï¿½
10

ï¿½

.

â€²

Case 3: In order to show the robustness of the estimators based on the t distribution against outliers, 
 indicate the contamination rate 
the model designed in Case I is reconsidered with outliers. Let 
  being  the 
with  a  value  of  0.1  and 
sample volume. 
 distribution, 
ğ‘™ğ‘™
a data set with outliers was created. Actual 
the remaining 
, Taken as 
in case of
values of 
explanatory variables were obtained as in Case I. 

 contaminated data with 
ğºğº = [ğœ–ğœ–ğ‘ğ‘] 
 and 

is  the  full  value  of  the  contaminated  data,  with 

ğ‘¡ğ‘¡ = 1,2, â€¦ , ğºğº
 model parameters 
ğ‘ğ‘(0,1) 

ğ‘¡ğ‘¡ = ğºğº, ğºğº + 1, ğºğº + 2 â€¦ , ğ‘ğ‘ 

, produced from 

ğ‘ğ‘(50,1)

data 

ğºğº

ğœ–ğœ–

â€²

ğ›½ğ›½ = (3,1.5,0,0,2,0,0)

ğ´ğ´ğ‘…ğ‘…(ğ‘ğ‘)

ğœ™ğœ™ = 0.8 

 ğ‘ğ‘ = 1

Variable selection performance. All simulations were repeated 100 times and the ratio of correctly 
predicted zero coefficients under each condition was counted to measure the performance of the 
model selection criteria (Correct). Similarly, the coefficients with zero, which should not be zero 
in the model, were also counted and their ratios were calculated (Incorrect). At the same time, the 
correct zero amounts given in the real model were counted simultaneously in each simulation run, 
and  the  correct  model  ratio  (Cor.fit.)  obtained  in  100  repetitions  was  calculated.  For  the 
autoregressive model, in each case, the number of correct autoregressive order (AR order) in 100 
replicates was counted. 

Parameter estimation performance. In order to measure the parameter estimation performances of 
the proposed methods, the results of the 95% confidence intervals calculated with the standard 

11 

 
 
 
 
 
 
 
 
 
 
errors of the  parameters  obtained with the help of  the  observed  fisher  information  matrix. The 
mean of the mean squared errors (MeanMSE) computed by using the following formula. 

We  give  the  average  of  the  performance  measures  based  on  100  simulated  data  sets  for  each 
simulation setting. 

ğ‘€ğ‘€ğ‘†ğ‘†ğ¸ğ¸ï¿½ğ›½ğ›½Ì‚ï¿½ = ï¿½ğ›½ğ›½Ì‚ âˆ’ ğ›½ğ›½ï¿½

â€²

â€²

ğ¸ğ¸(ğ‘¥ğ‘¥ğ‘¥ğ‘¥

)ï¿½ğ›½ğ›½Ì‚ âˆ’ ğ›½ğ›½ï¿½

4.3 Simulation results 

The simulation results for variable selection are given in Tables 1 - 10, and the results for mean of 
 and upper bounds 
model parameter estimations, MSE, standard errors

, lower bounds 

 are reported in Tables 11 - 15. 

 (ğ‘†ğ‘†ğ¸ğ¸ï¿½ï¿½ï¿½ï¿½)

(ğ¿ğ¿ğµğµï¿½ï¿½ï¿½ï¿½)

ğ‘¡ğ‘¡3

ğ‘¡ğ‘¡10

 and 

(ğ‘¡ğ‘¡3)

(ğ‘ˆğ‘ˆğµğµï¿½ï¿½ï¿½ï¿½)
In Tables 1 â€“ 4, the results for Case 1 are summarized. Table 1 and Table 2 show the results of the 
 assumptions, respectively. 
calculations with first-order autoregressive error terms, for 
It  can  be  observed  from  Table  1  that  all  methods  have  similar  performances  according  to  the 
parameter estimation. This situation is also supported by the standard error and confidence interval 
estimations of the parameters in Table 11 and Table 12. It is easily seen that if the error distribution 
assumption  is  normal  by  then,  LASSO,  SCAD,  bridge  and  elastic-net  methods  have  equally 
superior performances over other methods for variable selection and selection of the correct AR 
degree.  In Table 2, the  results are presented under the heavy-tailed 
 distribution. From this 
table, it can be said that OLS, LASSO, SCAD and ridge regression methods are affected by the 
heavy taildness both in parameter estimation and in variable selection cases. Tables 3 and 4 show 
the results for the second order autocorrelation error terms. From these tables, we can see that as 
the  autoregressive  order  increases,  all  the  results  obtained  from  the  aforementioned  methods 
deteriorate according to the MSE values. However, LASSO, SCAD, bridge and elastic net methods 
successfully maintain their variable selection performance. High dimensional simulation results 
mentioned in Case 2 are shown in Tables 5 - 8. From Table 5, it can be easily said that bridge and 
elastic net methods have quite good performance in terms of parameter estimation according to 
small MSE values. The results of the variable selection scenario in the high dimension, LASSO, 
SCAD,  bridge  and  elastic  net  methods  are  able  to  identify  the  significant  variables  quite  well. 
Table 6 shows the heavy-tailed error assumption and high dimension results together. From this 
table, the negative impact of parameter estimation performance on OLS, LASSO, SCAD and ridge 
regression methods can be seen under the heavy tail assumption. On the other hand; bridge and 
elastic net have better results than other methods on parameter estimation performance under both 
heavy tail and high dimension effects. From the parameter estimation perspective of view Table 
13 summarizes both heavy and thin tailed results. Bridge and elastic net methods have superior 
performances due to the better variable selection in high dimension case. Tables 7 and 8 show the 
results assuming second degree autoregressive error terms distributed as 
, respectively. 
Similar  to  Case  1,  it  can  be  said  that  the  parameter  estimation  performance  of  the  mentioned 
methods worsens as the autoregressive effect increases. However, bridge and elastic net methods 
perform better in parameter estimation. The most striking part about this situation is seen in the 

 and 

ğ‘¡ğ‘¡10

ğ‘¡ğ‘¡3

12 

 
 
 
 
 
 
ğ‘¡ğ‘¡3

ğ‘¡ğ‘¡10

 and 

correct determination of the AR degree among all the mentioned methods. Although LASSO and 
SCAD  methods  have  equal  performance  with  bridge  and  elastic  net  in  terms  of  determining 
significant  variables  in  the  regression  model,  bridge  and  elastic  net  have  a  much  superior 
performance than other methods. Table 9 and Table 10 contains the simulation results for Case 3. 
For this case 10% contamination is generated for the response variable. Results regarding the data 
obtained from 
 distribution assumptions respectively. From Table 9 when the default 
distribution is normal, it is clear that the parameter estimation and variable selection performances 
of all methods deteriorate if there  are some vertical outliers in the data. In Table 10, while the 
distribution  assumption  is  heavy  tailed,  it  can  be  said  that  bridge  and  elastic-net  methods 
outperform  other  methods  for  both  parameter  estimation  and  variable  selection  performances. 
Especially  when  the  sample  size  increased,  the  variable  selection  performances  of  bridge  and 
elastic net methods improved both for the selection of important variables for the regression model 
and for the correct degree of AR. This results are also supported by Table 14 and 15. The parameter 
and interval estimation performances are effected from the contamination. From these tables under 
the outlier case the heavy tailed distribution assumption have better standard error and confidence 
interval  estimation  performance.  Besides,  bridge  and  elastic  net  methods  cope  better  with  the 
contaminations than the other methods.  

13 

 
 
Table 1. Variable selection performance for 

N 

  Method 
50  OLS 

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

100  OLS 

(ğ›¾ğ›¾ = 0.7)

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

300  OLS 

(ğ›¾ğ›¾ = 0.7)

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

(ğ›¾ğ›¾ = 0.7)

MSE 
0.63022 
0.71577 
0.70708 
0.81457 
0.64991 
0.66223 
0.50160 
0.58037 
0.58469 
0.75763 
0.58469 
0.59624 
0.08640 
0.01501 
0.01446 
0.02276 
0.01936 
0.01994 

 error (Case 1 AR order=1) 
Regression Coefficient 
Incorrect 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 

ğ‘¡ğ‘¡10
Correct 
0.22 
5.00 
5.00 
0.22 
5.00 
5.00 
0.36 
5.00 
5.00 
0.41 
5.00 
5.00 
0.66 
5.00 
5.00 
1.75 
5.00 
5.00 

Cor. fit. 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 

Table 2. Variable selection performance for 

 error (Case 1 AR order=1) 

N 

  Method 
50  OLS 

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

100  OLS 

(ğ›¾ğ›¾ = 0.7)

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

300  OLS 

(ğ›¾ğ›¾ = 0.7)

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

(ğ›¾ğ›¾ = 0.7)

ğ‘¡ğ‘¡3

MSE 
6.14354 
5.25620 
4.24132 
9.02561 
1.19562 
1.29511 
4.58984 
3.23587 
2.52147 
5.66542 
1.10548 
1.16212 
1.41556 
2.25871 
1.95621 
3.25471 
0.75625 
0.88524 

Regression Coefficient 

Incorrect 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 

Cor. fit. 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 

Correct 
0.10 
5.00 
5.00 
0.06 
5.00 
5.00 
0.18 
5.00 
5.00 
0.23 
5.00 
5.00 
0.46 
5.00 
5.00 
0.55 
5.00 
5.00 

14 

AR order 
0 
100 
100 
0 
100 
100 
0 
100 
100 
0 
100 
100 
0 
100 
100 
0 
100 
100 

AR order 
1 
80 
81 
1 
100 
98 
3 
85 
87 
3 
100 
99 
5 
91 
95 
6 
100 
100 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Table 3. Variable selection performance for 

N 
  Method 
50  OLS 

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

100  OLS 

(ğ›¾ğ›¾ = 0.7)

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

300  OLS 

(ğ›¾ğ›¾ = 0.7)

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

(ğ›¾ğ›¾ = 0.7)

MSE 
1.20224 
2.55725 
1.50708 
2.71457 
1.17904 
1.03620 
0.81485 
1.39416 
1.25595 
1.99491 
1.00469 
0.70041 
0.52598 
1.26441 
0.81565 
1.20595 
0.57927 
0.61257 

ğ‘¡ğ‘¡10

 error (Case 1 AR order=2) 
Regression Coefficient 
Incorrect 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 

Correct 
0.12 
5.00 
5.00 
0.13 
5.00 
5.00 
0.25 
5.00 
5.00 
0.24 
5.00 
5.00 
0.60 
5.00 
5.00 
1.64 
5.00 
5.00 

Cor. fit. 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 

Table 4. Variable selection performance for 

 error (Case 1 AR order=2) 

N 
  Method 
50  OLS 

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

100  OLS 

(ğ›¾ğ›¾ = 0.7)

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

300  OLS 

(ğ›¾ğ›¾ = 0.7)

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

(ğ›¾ğ›¾ = 0.7)

ğ‘¡ğ‘¡3

MSE 
8.39124 
6.81415 
6.50981 
10.1145 
2.65604 
2.02209 
6.01922 
5.58925 
3.52912 
7.56630 
1.95292 
1.69129 
4.05256 
4.12623 
2.95562 
5.95624 
0.98527 
0.99814 

Regression Coefficient 
Incorrect 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 

Cor. fit. 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 

Correct 
0.10 
5.00 
5.00 
0.09 
5.00 
5.00 
0.25 
5.00 
5.00 
0.32 
5.00 
5.00 
0.31 
5.00 
5.00 
0.43 
5.00 
5.00 

15 

AR order 
0 
100 
100 
0 
100 
100 
0 
100 
100 
0 
100 
100 
0 
100 
100 
0 
100 
100 

AR order 
1 
77 
75 
1 
100 
95 
2 
83 
80 
3 
100 
99 
4 
85 
90 
5 
100 
100 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Table 5. Variable selection performance for 

 error (Case 2 AR order=1) 

N 
  Method 
50  OLS 

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

100  OLS 

(ğ›¾ğ›¾ = 0.8)

LASSO 
SCAD 
ridge 

elastic-net 

300  OLS 

bridge (ğ›¾ğ›¾ = 0.8)

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

MSE 
149.9554 
15.57302 
20.42574 
22.06755 
1.838428 
1.549361 
95.9561 
7.30260 
11.4591 
12.6466 
0.95664 
0.65483 
75.6543 
6.55169 
9.65418 
9.87563 
0.56978 
0.45929 

ğ‘¡ğ‘¡10

Regression Coefficient 
Incorrect 
0.02 
0.00 
0.00 
0.01 
0.00 
0.00 
0.01 
0.00 
0.00 
0.01 
0.00 
0.00 
0.01 
0.00 
0.00 
0.01 
0.00 
0.00 

Cor. fit. 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 

Correct 
0.10 
20.00 
20.00 
0.30 
20.00 
20.00 
0.21 
20.00 
20.00 
0.42 
20.00 
20.00 
0.40 
20.00 
20.00 
0.56 
20.00 
20.00 

(ğ›¾ğ›¾ = 0.7)
Table 6. Variable selection performance 

 error (Case 2 AR order=1) 

N 
  Method 
50  OLS 

LASSO 
SCAD 
ridge 

elastic-net 

100  OLS 

bridge (ğ›¾ğ›¾ = 0.8)

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

300  OLS 

(ğ›¾ğ›¾ = 0.7)

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

(ğ›¾ğ›¾ = 0.8)

ğ‘¡ğ‘¡3
MSE 
592.955 
59.4949 
40.2942 
69.5491 
2.98516 
2.78965 
254.281 
39.3218 
20.9265 
49.5232 
1.84567 
1.95148 
175.986 
26.5169 
16.5166 
39.9563 
1.55941 
1.46847 

Regression Coefficient 
Incorrect 
0.02 
0.00 
0.00 
0.01 
0.00 
0.00 
0.01 
0.00 
0.00 
0.01 
0.00 
0.00 
0.01 
0.00 
0.00 
0.01 
0.00 
0.00 

Cor. fit. 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 

Correct 
0.06 
20.00 
20.00 
0.12 
20.00 
20.00 
0.12 
20.00 
20.00 
0.25 
20.00 
20.00 
0.21 
20.00 
20.00 
0.33 
20.00 
20.00 

16 

AR order 
0 
100 
100 
0 
100 
100 
0 
100 
100 
0 
100 
100 
0 
100 
100 
0 
100 
100 

AR order 
0 
90 
95 
0 
98 
97 
0 
92 
96 
0 
100 
99 
0 
96 
99 
0 
100 
100 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Table 7. Variable selection performance for 

N 
  Method 
50  OLS 

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

100  OLS 

(ğ›¾ğ›¾ = 0.7)

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

300  OLS 

(ğ›¾ğ›¾ = 0.7)

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

(ğ›¾ğ›¾ = 0.8)

MSE 
198.112 
21.2455 
27.1567 
24.8754 
2.31128 
2.79868 
129.135 
18.3025 
18.2169 
19.6468 
1.64554 
1.98415 
91.6543 
10.1249 
11.6874 
10.1982 
0.82985 
0.95956 

ğ‘¡ğ‘¡10

 error (Case 2 AR order=2) 
Regression Coefficient 
Incorrect 
0.02 
0.00 
0.00 
0.01 
0.00 
0.00 
0.01 
0.00 
0.00 
0.01 
0.00 
0.00 
0.01 
0.00 
0.00 
0.01 
0.00 
0.00 

Correct 
0.08 
20.00 
20.00 
0.27 
20.00 
20.00 
0.11 
20.00 
20.00 
0.30 
20.00 
20.00 
0.40 
20.00 
20.00 
0.56 
20.00 
20.00 

Cor. fit. 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 

Table 8. Variable selection performance for 

 error (Case 2 AR order=2) 

N 
  Method 
50  OLS 

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

100  OLS 

(ğ›¾ğ›¾ = 0.8)

LASSO 
SCAD 
ridge 

elastic-net 

300  OLS 

bridge(ğ›¾ğ›¾ = 0.7)

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

(ğ›¾ğ›¾ = 0.8)

ğ‘¡ğ‘¡3

MSE 
854.541 
61.1156 
63.1565 
84.7545 
6.15694 
5.79868 
651.122 
52.8302 
54.2169 
71.2646 
4.64554 
3.98415 
501.250 
42.8921 
46.1258 
62.4518 
2.12355 
2.12585 

Regression Coefficient 
Incorrect 
0.02 
0.00 
0.00 
0.01 
0.00 
0.00 
0.01 
0.00 
0.00 
0.01 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 

Cor. fit. 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 

Correct 
0.02 
20.00 
20.00 
0.18 
20.00 
20.00 
0.08 
20.00 
20.00 
0.24 
20.00 
20.00 
0.16 
20.00 
20.00 
0.45 
20.00 
20.00 

17 

AR order 
0 
100 
100 
0 
100 
100 
0 
100 
100 
0 
100 
100 
0 
100 
100 
0 
100 
100 

AR order 
0 
51 
78 
0 
100 
99 
0 
59 
89 
0 
100 
100 
0 
65 
91 
0 
100 
100 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Table 9. Variable selection performance for 

 error (Case 3) 

N 

50  OLS 

Method 

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

100  OLS 

(ğ›¾ğ›¾ = 0.8)

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

300  OLS 

(ğ›¾ğ›¾ = 0.7)

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

(ğ›¾ğ›¾ = 0.8)

MSE 
8548.54 
2547.11 
125.156 
1587.875 
52.1569 
48.7986 
8548.541 
2547.115 
125.1565 
1587.875 
52.15694 
48.79868 
1058.25 
1012.25 
41.2541 
421.542 
10.1231 
11.2354 

ğ‘¡ğ‘¡10

Regression Coefficient 
Incorrect 
0.21 
0.02 
0.09 
0.20 
0.02 
0.03 
0.21 
0.02 
0.09 
0.20 
0.02 
0.03 
0.00 
0.08 
0.00 
0.02 
0.00 
0.00 

Cor. fit. 
0.00 
0.88 
0.75 
0.00 
0.99 
0.97 
0.00 
0.88 
0.75 
0.00 
0.99 
0.97 
0.00 
1.00 
0.98 
0.00 
1.00 
1.00 

ğœ–ğœ– = 0.1
Correct 
0.00 
3.88 
3.75 
0.00 
4.20 
4.03 
0.00 
3.88 
3.75 
0.00 
4.20 
4.03 
0.00 
4.55 
4.31 
0.00 
4.82 
4.81 

Table 10. Variable selection performance for 

N 
  Method 
50  OLS 

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

100  OLS 

(ğ›¾ğ›¾ = 0.8)

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

300  OLS 

(ğ›¾ğ›¾ = 0.7)

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

(ğ›¾ğ›¾ = 0.8)

MSE 
298.651 
259.842 
54.6590 
55.9861 
8.36152 
8.15713 
142.122 
122.830 
38.2169 
47.2646 
6.19841 
7.98415 
109.251 
18.4892 
32.1258 
42.4568 
5.01521 
5.12585 

 error (Case 3) 
Regression Coefficient 
Incorrect 
0.21 
0.00 
0.01 
0.15 
0.00 
0.00 
0.11 
0.00 
0.00 
0.10 
0.00 
0.00 
0.10 
0.00 
0.00 
0.04 
0.00 
0.00 

Cor. fit. 
0.00 
1.00 
0.97 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 

ğ‘¡ğ‘¡3
ğœ–ğœ– = 0.1
Correct 
0.07 
4.02 
4.55 
0.08 
4.95 
4.91 
0.11 
4.45 
4.72 
0.18 
5.00 
5.00 
0.20 
4.71 
4.85 
0.22 
5.00 
5.00 

18 

AR order 
0 
32 
46 
0 
89 
82 
0 
32 
46 
0 
89 
82 
0 
72 
79 
0 
100 
100 

AR order 
0 
51 
81 
0 
100 
95 
0 
89 
90 
0 
100 
99 
0 
95 
97 
0 
100 
100 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Table 11. Parameter estimation results  for 

 (Case 1 AR order=1) 

N 
50 

ğ‘¡ğ‘¡10

100 

300 

   OLS 
2.8669 
  0.2056 
0.8353 
1.8426 
4.5439 
1.4632 
  0.0127 
0.1874 
1.3229 
1.6793 
2.2774 
  0.6923 
1.0568 
0.9591 
3.1372 
2.8849 
  0.1470 
0.7016 
1.9461 
4.0616 
1.5091 
  0.0013 
0.1158 
1.3969 
1.5881 
1.9526 
  0.5104 
0.8996 
1.3219 
2.7985 
3.1849 
  0.0470 
0.0016 
2.2461 
3.8609 
1.5659 
  0.0181 
0.0990 
1.4537 
1.5313 
2.0094 

ğ›½ğ›½Ì‚1
ğ‘€ğ‘€ğ‘†ğ‘†ğ¸ğ¸
ğ‘†ğ‘†ğ¸ğ¸ï¿½ï¿½ï¿½ï¿½
ğ¿ğ¿ğµğµï¿½ï¿½ï¿½ï¿½
ğ‘ˆğ‘ˆğµğµï¿½ï¿½ï¿½ï¿½
ğ›½ğ›½Ì‚2
ğ‘€ğ‘€ğ‘†ğ‘†ğ¸ğ¸
ğ‘†ğ‘†ğ¸ğ¸ï¿½ï¿½ï¿½ï¿½
ğ¿ğ¿ğµğµï¿½ï¿½ï¿½ï¿½
ğ‘ˆğ‘ˆğµğµï¿½ï¿½ï¿½ï¿½
ğ›½ğ›½Ì‚5
ğ‘€ğ‘€ğ‘†ğ‘†ğ¸ğ¸
ğ‘†ğ‘†ğ¸ğ¸ï¿½ï¿½ï¿½ï¿½
ğ¿ğ¿ğµğµï¿½ï¿½ï¿½ï¿½
ğ‘ˆğ‘ˆğµğµï¿½ï¿½ï¿½ï¿½
ğ›½ğ›½Ì‚1
ğ‘€ğ‘€ğ‘†ğ‘†ğ¸ğ¸
ğ‘†ğ‘†ğ¸ğ¸ï¿½ï¿½ï¿½ï¿½
ğ¿ğ¿ğµğµï¿½ï¿½ï¿½ï¿½
ğ‘ˆğ‘ˆğµğµï¿½ï¿½ï¿½ï¿½
ğ›½ğ›½Ì‚2
ğ‘€ğ‘€ğ‘†ğ‘†ğ¸ğ¸
ğ‘†ğ‘†ğ¸ğ¸ï¿½ï¿½ï¿½ï¿½
ğ¿ğ¿ğµğµï¿½ï¿½ï¿½ï¿½
ğ‘ˆğ‘ˆğµğµï¿½ï¿½ï¿½ï¿½
ğ›½ğ›½Ì‚5
ğ‘€ğ‘€ğ‘†ğ‘†ğ¸ğ¸
ğ‘†ğ‘†ğ¸ğ¸ï¿½ï¿½ï¿½ï¿½
ğ¿ğ¿ğµğµï¿½ï¿½ï¿½ï¿½
ğ‘ˆğ‘ˆğµğµï¿½ï¿½ï¿½ï¿½
ğ›½ğ›½Ì‚1
ğ‘€ğ‘€ğ‘†ğ‘†ğ¸ğ¸
ğ‘†ğ‘†ğ¸ğ¸ï¿½ï¿½ï¿½ï¿½
ğ¿ğ¿ğµğµï¿½ï¿½ï¿½ï¿½
ğ‘ˆğ‘ˆğµğµï¿½ï¿½ï¿½ï¿½
ğ›½ğ›½Ì‚2
ğ‘€ğ‘€ğ‘†ğ‘†ğ¸ğ¸
ğ‘†ğ‘†ğ¸ğ¸ï¿½ï¿½ï¿½ï¿½
ğ¿ğ¿ğµğµï¿½ï¿½ï¿½ï¿½
ğ‘ˆğ‘ˆğµğµï¿½ï¿½ï¿½ï¿½
ğ›½ğ›½Ì‚5

ğ’•ğ’•ğŸğŸğŸğŸ

ridge 
3.4934 
0.4057 
1.2716 
1.5691 
4.9199 
1.2202 
0.0711 
0.2752 
1.1838 
1.8102 
2.5224 
0.8292 
1.3351 
0.8289 
3.6184 
2.7458 
0.3565 
1.1169 
1.6533 
4.7023 
1.2811 
0.0609 
0.1152 
1.2536 
1.7049 
2.5108 
0.6860 
0.9718 
1.1815 
3.1949 
3.1458 
0.0565 
0.0869 
1.7533 
4.3023 
1.3379 
0.0441 
0.0984 
1.3104 
1.6481 
2.4540 

LASSO 
3.4320 
0.6891 
1.8533 
1.2407 
4.8874 
1.2230 
0.0700 
0.2990 
1.1370 
1.8531 
2.6285 
0.9656 
1.2937 
0.7899 
3.6710 
3.2669 
0.6454 
1.6581 
1.5158 
4.7221 
1.3168 
0.0580 
0.1982 
1.2538 
1.7463 
2.6096 
0.8703 
0.9703 
1.2099 
3.1733 
3.1669 
0.0754 
0.1015 
1.9158 
4.3821 
1.3736 
0.0412 
0.1814 
1.3106 
1.6895 
2.5528 

19 

SCAD 
3.3545 
0.3830 
1.4886 
1.3597 
4.2134 
1.3386 
0.0605 
0.2685 
1.4002 
1.7207 
2.1193 
0.7865 
1.2685 
0.8695 
3.6330 
3.2796 
0.3373 
1.2231 
1.4332 
4.1899 
1.3598 
0.0424 
0.1790 
1.3352 
1.6837 
2.1134 
0.6943 
1.1136 
1.2931 
3.1377 
3.0796 
0.0573 
0.0231 
1.7332 
3.9899 
1.4166 
0.0256 
0.1622 
1.3920 
1.6269 
2.0566 

bridge 
3.3037 
0.2185 
0.7352 
1.8846 
4.1438 
1.3410 
0.0053 
0.1772 
1.3169 
1.6697 
2.0424 
0.7216 
1.1317 
0.9499 
3.1867 
3.1216 
0.1600 
0.5932 
1.9337 
3.9815 
1.3961 
0.0047 
0.1738 
1.3969 
1.6481 
1.9896 
0.6704 
0.8896 
1.4216 
2.8985 
3.0297 
0.0550 
0.0516 
2.1456 
3.9616 
1.4529 
0.0121 
0.1570 
1.4537 
1.5913 
1.9328 

elastic-net 
3.3934 
0.2357 
0.9720 
1.7693 
4.1169 
1.3825 
0.0161 
0.2796 
1.2838 
1.7103 
2.0662 
0.7288 
1.0986 
0.9337 
3.2182 
3.2455 
0.1577 
0.7774 
1.8276 
4.3065 
1.4011 
0.0109 
0.1952 
1.2796 
1.6049 
2.0308 
0.7076 
0.9718 
1.2818 
2.9649 
3.0458 
0.0565 
0.1169 
1.9533 
3.9813 
1.4579 
0.0059 
0.1784 
1.3364 
1.5481 
1.9740 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  0.4936 
0.8828 
1.3787 
2.7417 

0.6692 
0.9550 
1.2383 
3.1381 

0.8535 
0.9535 
1.2667 
3.1165 

0.6775 
1.0968 
1.3499 
3.0809 

0.6536 
0.8728 
1.4784 
2.8417 

0.6908 
0.9550 
1.3386 
2.9081 

ğ‘€ğ‘€ğ‘†ğ‘†ğ¸ğ¸
ğ‘†ğ‘†ğ¸ğ¸ï¿½ï¿½ï¿½ï¿½
ğ¿ğ¿ğµğµï¿½ï¿½ï¿½ï¿½
ğ‘ˆğ‘ˆğµğµï¿½ï¿½ï¿½ï¿½

Table 12. Parameter estimation results for 

N 
50 

ğ‘¡ğ‘¡3

100 

300 

   OLS 
2.7854 
  0.2301 
0.8598 
1.8101 
4.5754 
1.3817 
  0.0372 
0.2119 
1.2904 
1.7108 
2.1959 
  0.7168 
1.0813 
0.9266 
3.1687 
2.8034 
  0.1715 
0.7261 
1.9136 
4.0931 
1.4276 
  0.0232 
0.1403 
1.3644 
1.6196 
1.8711 
  0.5349 
0.9241 
1.2894 
2.8300 
3.1034 
  0.0715 
0.0261 
2.2136 
3.8924 
1.4844 

ğ›½ğ›½Ì‚1
ğ»ğ»ğ»ğ»ğ»ğ»
ğ‘†ğ‘†ğ¸ğ¸ï¿½ï¿½ï¿½ï¿½
ğ¿ğ¿ğµğµï¿½ï¿½ï¿½ï¿½
ğ‘ˆğ‘ˆğµğµï¿½ï¿½ï¿½ï¿½
ğ›½ğ›½Ì‚2
ğ»ğ»ğ»ğ»ğ»ğ»
ğ‘†ğ‘†ğ¸ğ¸ï¿½ï¿½ï¿½ï¿½
ğ¿ğ¿ğµğµï¿½ï¿½ï¿½ï¿½
ğ‘ˆğ‘ˆğµğµï¿½ï¿½ï¿½ï¿½
ğ›½ğ›½Ì‚5
ğ»ğ»ğ»ğ»ğ»ğ»
ğ‘†ğ‘†ğ¸ğ¸ï¿½ï¿½ï¿½ï¿½
ğ¿ğ¿ğµğµï¿½ï¿½ï¿½ï¿½
ğ‘ˆğ‘ˆğµğµï¿½ï¿½ï¿½ï¿½
ğ›½ğ›½Ì‚1
ğ»ğ»ğ»ğ»ğ»ğ»
ğ‘†ğ‘†ğ¸ğ¸ï¿½ï¿½ï¿½ï¿½
ğ¿ğ¿ğµğµï¿½ï¿½ï¿½ï¿½
ğ‘ˆğ‘ˆğµğµï¿½ï¿½ï¿½ï¿½
ğ›½ğ›½Ì‚2
ğ»ğ»ğ»ğ»ğ»ğ»
ğ‘†ğ‘†ğ¸ğ¸ï¿½ï¿½ï¿½ï¿½
ğ¿ğ¿ğµğµï¿½ï¿½ï¿½ï¿½
ğ‘ˆğ‘ˆğµğµï¿½ï¿½ï¿½ï¿½
ğ›½ğ›½Ì‚5
ğ»ğ»ğ»ğ»ğ»ğ»
ğ‘†ğ‘†ğ¸ğ¸ï¿½ï¿½ï¿½ï¿½
ğ¿ğ¿ğµğµï¿½ï¿½ï¿½ï¿½
ğ‘ˆğ‘ˆğµğµï¿½ï¿½ï¿½ï¿½
ğ›½ğ›½Ì‚1
ğ»ğ»ğ»ğ»ğ»ğ»
ğ‘†ğ‘†ğ¸ğ¸ï¿½ï¿½ï¿½ï¿½
ğ¿ğ¿ğµğµï¿½ï¿½ï¿½ï¿½
ğ‘ˆğ‘ˆğµğµï¿½ï¿½ï¿½ï¿½
ğ›½ğ›½Ì‚2

ğ‘¡ğ‘¡3

ridge 
3.5749 
0.4302 
1.2961 
1.5366 
4.9514 
1.3017 
0.0956 
0.2997 
1.1513 
1.8417 
2.6039 
0.8537 
1.3596 
0.7964 
3.6499 
2.8273 
0.3810 
1.1414 
1.6208 
4.7338 
1.3626 
0.0854 
0.1397 
1.2211 
1.7364 
2.5923 
0.7105 
0.9963 
1.1490 
3.2264 
3.2273 
0.0810 
0.1114 
1.7208 
4.3338 
1.4194 

 (Case 1 AR order=1) 
LASSO  SCAD 
3.4360 
3.5135 
0.4075 
0.7136 
1.5131 
1.8778 
1.3272 
1.2082 
4.2449 
4.9189 
1.4201 
1.3045 
0.0850 
0.0945 
0.2930 
0.3235 
1.3677 
1.1045 
1.7522 
1.8846 
2.2008 
2.7100 
0.8110 
0.9901 
1.2930 
1.3182 
0.8370 
0.7574 
3.6645 
3.7025 
3.3611 
3.3484 
0.3618 
0.6699 
1.2476 
1.6826 
1.4007 
1.4833 
4.2214 
4.7536 
1.4413 
1.3983 
0.0669 
0.0825 
0.2035 
0.2227 
1.3027 
1.2213 
1.7152 
1.7778 
2.1949 
2.6911 
0.7188 
0.8948 
1.1381 
0.9948 
1.2606 
1.1774 
3.1692 
3.2048 
3.1611 
3.2484 
0.0818 
0.0999 
0.0476 
0.1260 
1.7007 
1.8833 
4.0214 
4.4136 
1.4981 
1.4551 

20 

bridge 
3.3852 
0.2430 
0.7597 
1.8521 
4.1753 
1.4225 
0.0298 
0.2017 
1.2844 
1.7012 
2.1239 
0.7461 
1.1562 
0.9174 
3.2182 
3.2031 
0.1845 
0.6177 
1.9012 
4.0130 
1.4776 
0.0292 
0.1983 
1.3644 
1.6796 
2.0711 
0.6949 
0.9141 
1.3891 
2.9300 
3.1112 
0.0795 
0.0761 
2.1131 
3.9931 
1.5344 

elastic-net 
3.4749 
0.2602 
0.9965 
1.7368 
4.1484 
1.4640 
0.0406 
0.3041 
1.2513 
1.7418 
2.1477 
0.7533 
1.1231 
0.9012 
3.2497 
3.3270 
0.1822 
0.8019 
1.7951 
4.3380 
1.4826 
0.0354 
0.2197 
1.2471 
1.6364 
2.1123 
0.7321 
0.9963 
1.2493 
2.9964 
3.1273 
0.0810 
0.1414 
1.9208 
4.0128 
1.5394 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  0.0064 
0.1235 
1.4212 
1.5628 
1.9279 
  0.5181 
0.9073 
1.3462 
2.7732 

0.0686 
0.1229 
1.2779 
1.6796 
2.5355 
0.6937 
0.9795 
1.2058 
3.1696 

0.0657 
0.2059 
1.2781 
1.7210 
2.6343 
0.8780 
0.9780 
1.2342 
3.1480 

0.0501 
0.1867 
1.3595 
1.6584 
2.1381 
0.7020 
1.1213 
1.3174 
3.1124 

0.0124 
0.1815 
1.4212 
1.6228 
2.0143 
0.6781 
0.8973 
1.4459 
2.8732 

0.0186 
0.2029 
1.3039 
1.5796 
2.0555 
0.7153 
0.9795 
1.3061 
2.9396 

ğ»ğ»ğ»ğ»ğ»ğ»
ğ‘†ğ‘†ğ¸ğ¸ï¿½ï¿½ï¿½ï¿½
ğ¿ğ¿ğµğµï¿½ï¿½ï¿½ï¿½
ğ‘ˆğ‘ˆğµğµï¿½ï¿½ï¿½ï¿½
ğ›½ğ›½Ì‚5
ğ»ğ»ğ»ğ»ğ»ğ»
ğ‘†ğ‘†ğ¸ğ¸ï¿½ï¿½ï¿½ï¿½
ğ¿ğ¿ğµğµï¿½ï¿½ï¿½ï¿½
ğ‘ˆğ‘ˆğµğµï¿½ï¿½ï¿½ï¿½

N 

ğ‘¡ğ‘¡3

50 

100 

300 

Table 13. Parameter estimation results for Case 2 AR order =1 
   OLS 
0.7766 
  16.387 
0.1530 
0.1776 
1.7069 
0.8581 
  14.525 
0.1398 
0.2101 
1.6754 
0.9396 
  12.663 
0.1266 
0.2426 
1.6439 
0.8581 
  14.030 
0.0598 
0.2101 
1.6754 
0.9396 
  12.168 
0.0466 
0.2426 
1.6439 
1.0211 
  10.306 
0.0334 
0.2751 

LASSO 
0.6320 
17.493 
0.1610 
0.0757 
1.8504 
0.7135 
15.631 
0.1478 
0.1082 
1.8189 
0.7950 
13.769 
0.1346 
0.1407 
1.7874 
0.7135 
15.136 
0.0678 
0.1082 
1.8189 
0.7950 
13.274 
0.0546 
0.1407 
1.7874 
0.8765 
11.412 
0.0414 
0.1732 

ridge 
0.6934 
18.659 
0.1897 
0.0741 
1.8799 
0.7749 
16.797 
0.1765 
0.1066 
1.8484 
0.8564 
14.935 
0.1633 
0.1391 
1.8169 
0.7749 
16.302 
0.0965 
0.1066 
1.8484 
0.8564 
14.440 
0.0833 
0.1391 
1.8169 
0.9379 
12.578 
0.0701 
0.1716 

ğ›½ğ›½Ì‚Ì…
ğ‘€ğ‘€ğ‘†ğ‘†ğ¸ğ¸
ğ‘†ğ‘†ğ¸ğ¸ï¿½ï¿½ï¿½ï¿½
ğ¿ğ¿ğµğµï¿½ï¿½ï¿½ï¿½
ğ‘ˆğ‘ˆğµğµï¿½ï¿½ï¿½ï¿½
ğ›½ğ›½Ì‚Ì…
ğ‘€ğ‘€ğ‘†ğ‘†ğ¸ğ¸
ğ‘†ğ‘†ğ¸ğ¸ï¿½ï¿½ï¿½ï¿½
ğ¿ğ¿ğµğµï¿½ï¿½ï¿½ï¿½
ğ‘ˆğ‘ˆğµğµï¿½ï¿½ï¿½ï¿½
ğ›½ğ›½Ì‚Ì…
ğ‘€ğ‘€ğ‘†ğ‘†ğ¸ğ¸
ğ‘†ğ‘†ğ¸ğ¸ï¿½ï¿½ï¿½ï¿½
ğ¿ğ¿ğµğµï¿½ï¿½ï¿½ï¿½
ğ‘ˆğ‘ˆğµğµï¿½ï¿½ï¿½ï¿½
ğ›½ğ›½Ì‚Ì…
ğ‘€ğ‘€ğ‘†ğ‘†ğ¸ğ¸
ğ‘†ğ‘†ğ¸ğ¸ï¿½ï¿½ï¿½ï¿½
ğ¿ğ¿ğµğµï¿½ï¿½ï¿½ï¿½
ğ‘ˆğ‘ˆğµğµï¿½ï¿½ï¿½ï¿½
ğ›½ğ›½Ì‚Ì…
ğ‘€ğ‘€ğ‘†ğ‘†ğ¸ğ¸
ğ‘†ğ‘†ğ¸ğ¸ï¿½ï¿½ï¿½ï¿½
ğ¿ğ¿ğµğµï¿½ï¿½ï¿½ï¿½
ğ‘ˆğ‘ˆğµğµï¿½ï¿½ï¿½ï¿½
ğ›½ğ›½Ì‚Ì…
ğ‘€ğ‘€ğ‘†ğ‘†ğ¸ğ¸
ğ‘†ğ‘†ğ¸ğ¸ï¿½ï¿½ï¿½ï¿½
ğ¿ğ¿ğµğµï¿½ï¿½ï¿½ï¿½

100 

300 

ğ‘¡ğ‘¡10

50 

21 

SCAD 
0.7545 
10.464 
0.1363 
0.1947 
1.7764 
0.8360 
8.6025 
0.1231 
0.2272 
1.7449 
0.9175 
6.7405 
0.1099 
0.2597 
1.7134 
0.8360 
8.1075 
0.0431 
0.2272 
1.7449 
0.9175 
6.2455 
0.0299 
0.2597 
1.7134 
0.9990 
4.3835 
0.0167 
0.2922 

bridge 
0.8039 
6.3801 
0.1130 
0.2197 
1.7313 
0.8854 
4.5181 
0.0998 
0.2522 
1.6998 
0.9669 
2.6561 
0.0998 
0.2847 
1.6683 
0.8854 
4.0231 
0.0198 
0.2522 
1.6998 
0.9669 
2.1611 
0.0066 
0.2847 
1.6683 
1.0484 
0.2991 
0.0066 
0.3172 

elastic-net 
0.8034 
6.3842 
0.1197 
0.1436 
1.7437 
0.8849 
4.5222 
0.1065 
0.1761 
1.7122 
0.9664 
2.6602 
0.0933 
0.2086 
1.6807 
0.8849 
4.0272 
0.0265 
0.1761 
1.7122 
0.9664 
2.1652 
0.0133 
0.2086 
1.6807 
1.0479 
0.3032 
0.0001 
0.2411 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
1.6124 

1.7854 

1.7559 

1.6819 

1.6368 

1.6492 

ğ‘¡ğ‘¡10

100 

N 
50 

   OLS 
5.6484 

2.9285 
0.4101 
6.0754 
4.2447 

2.2806 
-0.1096 
3.2108 
5.0589 

ğ‘ˆğ‘ˆğµğµï¿½ï¿½ï¿½ï¿½
Table 14. Parameter estimation results for 
 ridge 
ğ‘¡ğ‘¡10
6.2749 
  11.5626  11.7627 
3.3648 
0.1366 
6.4514 
4.0017 
  10.3697  10.4281 
2.3684 
-0.2487 
3.3417 
5.3039 
  11.0493  11.1862 
3.4283 
-0.6036 
5.1499 
5.5273 
10.0135 
3.2101 
0.2208 
6.2338 
4.0626 
9.7179 
2.2084 
-0.1789 
3.2364 
5.2923 
  10.1674  10.3430 
3.0650 
-0.2510 
4.7264 
5.9273 
8.7135 
2.1801 
0.3208 
5.8338 
4.1194 
8.7011 
2.1916 
-0.1221 
3.1796 

3.1500 
-0.4734 
4.6687 
5.6664 
  9.8040 
2.7948 
0.5136 
5.5931 
4.2906 
  9.6583 
2.2090 
-0.0356 
3.1196 
4.7341 

2.9928 
-0.1106 
4.3300 
5.9664 
  8.7040 
2.0948 
0.8136 
5.3924 
4.3474 
  8.6751 
2.1922 
0.0212 
3.0628 

ğ›½ğ›½Ì‚1
ğ‘€ğ‘€ğ‘†ğ‘†ğ¸ğ¸
ğ‘†ğ‘†ğ¸ğ¸ï¿½ï¿½ï¿½ï¿½
ğ¿ğ¿ğµğµï¿½ï¿½ï¿½ï¿½
ğ‘ˆğ‘ˆğµğµï¿½ï¿½ï¿½ï¿½
ğ›½ğ›½Ì‚2
ğ‘€ğ‘€ğ‘†ğ‘†ğ¸ğ¸
ğ‘†ğ‘†ğ¸ğ¸ï¿½ï¿½ï¿½ï¿½
ğ¿ğ¿ğµğµï¿½ï¿½ï¿½ï¿½
ğ‘ˆğ‘ˆğµğµï¿½ï¿½ï¿½ï¿½
ğ›½ğ›½Ì‚5
ğ‘€ğ‘€ğ‘†ğ‘†ğ¸ğ¸
ğ‘†ğ‘†ğ¸ğ¸ï¿½ï¿½ï¿½ï¿½
ğ¿ğ¿ğµğµï¿½ï¿½ï¿½ï¿½
ğ‘ˆğ‘ˆğµğµï¿½ï¿½ï¿½ï¿½
ğ›½ğ›½Ì‚1
ğ‘€ğ‘€ğ‘†ğ‘†ğ¸ğ¸
ğ‘†ğ‘†ğ¸ğ¸ï¿½ï¿½ï¿½ï¿½
ğ¿ğ¿ğµğµï¿½ï¿½ï¿½ï¿½
ğ‘ˆğ‘ˆğµğµï¿½ï¿½ï¿½ï¿½
ğ›½ğ›½Ì‚2
ğ‘€ğ‘€ğ‘†ğ‘†ğ¸ğ¸
ğ‘†ğ‘†ğ¸ğ¸ï¿½ï¿½ï¿½ï¿½
ğ¿ğ¿ğµğµï¿½ï¿½ï¿½ï¿½
ğ‘ˆğ‘ˆğµğµï¿½ï¿½ï¿½ï¿½
ğ›½ğ›½Ì‚5
ğ‘€ğ‘€ğ‘†ğ‘†ğ¸ğ¸
ğ‘†ğ‘†ğ¸ğ¸ï¿½ï¿½ï¿½ï¿½
ğ¿ğ¿ğµğµï¿½ï¿½ï¿½ï¿½
ğ‘ˆğ‘ˆğµğµï¿½ï¿½ï¿½ï¿½
ğ›½ğ›½Ì‚1
ğ‘€ğ‘€ğ‘†ğ‘†ğ¸ğ¸
ğ‘†ğ‘†ğ¸ğ¸ï¿½ï¿½ï¿½ï¿½
ğ¿ğ¿ğµğµï¿½ï¿½ï¿½ï¿½
ğ‘ˆğ‘ˆğµğµï¿½ï¿½ï¿½ï¿½
ğ›½ğ›½Ì‚2
ğ‘€ğ‘€ğ‘†ğ‘†ğ¸ğ¸
ğ‘†ğ‘†ğ¸ğ¸ï¿½ï¿½ï¿½ï¿½
ğ¿ğ¿ğµğµï¿½ï¿½ï¿½ï¿½
ğ‘ˆğ‘ˆğµğµï¿½ï¿½ï¿½ï¿½

300 

 (Case 3 AR order=1) 
LASSO    SCAD    
6.2135 
12.0461 
3.9465 
-0.1918 
6.4189 
4.0045 
10.4270 
2.3922 
-0.2955 
3.3846 
5.4100 
11.3226 
3.3869 
-0.6426 
5.2025 
6.0484 
10.3024 
3.7513 
0.0833 
6.2536 
4.0983 
9.7150 
2.2914 
-0.1787 
3.2778 
5.3911 
10.5273 
3.0635 
-0.2226 
4.7048 
5.9484 
8.7324 
2.1947 
0.4833 
5.9136 
4.1551 
8.6982 
2.2746 
-0.1219 
3.2210 

6.1360 
11.7400 
3.5818 
-0.0728 
5.7449 
4.1201 
10.4175 
2.3617 
-0.0323 
3.2522 
4.9008 
11.1435 
3.3617 
-0.5630 
5.1645 
6.0611 
9.9943 
3.3163 
0.0007 
5.7214 
4.1413 
9.6994 
2.2722 
-0.0973 
3.2152 
4.8949 
10.3513 
3.2068 
-0.1394 
4.6692 
5.8611 
8.7143 
2.1163 
0.3007 
5.5214 
4.1981 
8.6826 
2.2554 
-0.0405 
3.1584 

22 

elastic-net 
bridge 
6.0852 
6.1749 
11.5755  11.5927 
3.0652 
2.8284 
0.3368 
0.4521 
5.6484 
5.6753 
4.1225 
4.1640 
10.3623  10.3731 
2.3728 
2.2704 
-0.1487 
-0.1156 
3.2012 
3.2418 
4.8477 
4.8239 
11.0786  11.0858 
3.1918 
3.2249 
-0.4988 
-0.4826 
4.7497 
4.7182 
6.0270 
5.9031 
9.8147 
9.8170 
2.8706 
2.6864 
0.3951 
0.5012 
5.8380 
5.5130 
4.1826 
4.1776 
9.6679 
9.6617 
2.2884 
2.2670 
-0.1529 
-0.0356 
3.1364 
3.1796 
4.7711 
4.8123 
10.3274  10.3646 
3.0650 
2.9828 
-0.1507 
-0.0109 
4.4964 
4.4300 
5.8273 
5.8112 
8.7135 
8.7120 
2.2101 
2.1448 
0.5208 
0.7131 
5.5128 
5.4931 
4.2394 
4.2344 
8.6629 
8.6691 
2.2716 
2.2502 
-0.0961 
0.0212 
3.0796 
3.1228 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
4.7909 
  9.1506 
2.9760 
-0.0538 
4.2732 

5.2355 
9.3262 
3.0482 
-0.1942 
4.6696 

5.3343 
9.5105 
3.0467 
-0.1658 
4.6480 

4.8381 
9.3345 
3.1900 
-0.0826 
4.6124 

4.7143 
9.3106 
2.9660 
0.0459 
4.3732 

4.7555 
9.3478 
3.0482 
-0.0939 
4.4396 

ğ›½ğ›½Ì‚5
ğ‘€ğ‘€ğ‘†ğ‘†ğ¸ğ¸
ğ‘†ğ‘†ğ¸ğ¸ï¿½ï¿½ï¿½ï¿½
ğ¿ğ¿ğµğµï¿½ï¿½ï¿½ï¿½
ğ‘ˆğ‘ˆğµğµï¿½ï¿½ï¿½ï¿½

ğ‘¡ğ‘¡3

100 

N 
50 

Table 15. Parameter estimation results for 
 ridge 
ğ‘¡ğ‘¡3
4.3564 
4.7872 
1.3893 
0.7041 
5.7829 
2.0832 
2.4526 
0.3929 
0.3188 
2.6732 
3.3854 
3.2107 
1.4528 
-0.0361 
4.4814 
3.6088 
2.7380 
1.2346 
0.7883 
5.5653 
2.1441 
2.4424 
0.2329 
0.3886 
2.5679 
3.3738 
3.0675 
1.0895 
0.3165 
4.0579 
4.0088 
2.4380 
0.2046 
0.8883 

   OLS 
4.5669 
  4.8871 
0.9530 
0.9776 
5.4069 
2.1632 
  2.3942 
0.3051 
0.4579 
2.5423 
2.9774 
  3.0738 
1.1745 
0.0941 
4.0002 
3.5849 
  2.9285 
0.8193 
1.0811 
4.9246 
2.2091 
  2.3802 
0.2335 
0.5319 
2.4511 
2.6526 
  2.8919 
1.0173 
0.4569 
3.6615 
3.8849 
  2.4285 
0.1193 
1.3811 

ğ›½ğ›½Ì‚1
ğ‘€ğ‘€ğ‘†ğ‘†ğ¸ğ¸
ğ‘†ğ‘†ğ¸ğ¸ï¿½ï¿½ï¿½ï¿½
ğ¿ğ¿ğµğµï¿½ï¿½ï¿½ï¿½
ğ‘ˆğ‘ˆğµğµï¿½ï¿½ï¿½ï¿½
ğ›½ğ›½Ì‚2
ğ‘€ğ‘€ğ‘†ğ‘†ğ¸ğ¸
ğ‘†ğ‘†ğ¸ğ¸ï¿½ï¿½ï¿½ï¿½
ğ¿ğ¿ğµğµï¿½ï¿½ï¿½ï¿½
ğ‘ˆğ‘ˆğµğµï¿½ï¿½ï¿½ï¿½
ğ›½ğ›½Ì‚5
ğ‘€ğ‘€ğ‘†ğ‘†ğ¸ğ¸
ğ‘†ğ‘†ğ¸ğ¸ï¿½ï¿½ï¿½ï¿½
ğ¿ğ¿ğµğµï¿½ï¿½ï¿½ï¿½
ğ‘ˆğ‘ˆğµğµï¿½ï¿½ï¿½ï¿½
ğ›½ğ›½Ì‚1
ğ‘€ğ‘€ğ‘†ğ‘†ğ¸ğ¸
ğ‘†ğ‘†ğ¸ğ¸ï¿½ï¿½ï¿½ï¿½
ğ¿ğ¿ğµğµï¿½ï¿½ï¿½ï¿½
ğ‘ˆğ‘ˆğµğµï¿½ï¿½ï¿½ï¿½
ğ›½ğ›½Ì‚2
ğ‘€ğ‘€ğ‘†ğ‘†ğ¸ğ¸
ğ‘†ğ‘†ğ¸ğ¸ï¿½ï¿½ï¿½ï¿½
ğ¿ğ¿ğµğµï¿½ï¿½ï¿½ï¿½
ğ‘ˆğ‘ˆğµğµï¿½ï¿½ï¿½ï¿½
ğ›½ğ›½Ì‚5
ğ‘€ğ‘€ğ‘†ğ‘†ğ¸ğ¸
ğ‘†ğ‘†ğ¸ğ¸ï¿½ï¿½ï¿½ï¿½
ğ¿ğ¿ğµğµï¿½ï¿½ï¿½ï¿½
ğ‘ˆğ‘ˆğµğµï¿½ï¿½ï¿½ï¿½
ğ›½ğ›½Ì‚1
ğ‘€ğ‘€ğ‘†ğ‘†ğ¸ğ¸
ğ‘†ğ‘†ğ¸ğ¸ï¿½ï¿½ï¿½ï¿½
ğ¿ğ¿ğµğµï¿½ï¿½ï¿½ï¿½

300 

 (Case 3 AR order=1) 

LASSO    SCAD    
4.2950 
5.0706 
1.9710 
0.3757 
5.7504 
2.0860 
2.4515 
0.4167 
0.2720 
2.7161 
3.4915 
3.3471 
1.4114 
-0.0751 
4.5340 
4.1299 
3.0269 
1.7758 
0.6508 
5.5851 
2.1798 
2.4395 
0.3159 
0.3888 
2.6093 
3.4726 
3.2518 
1.0880 
0.3449 
4.0363 
4.0299 
2.4569 
0.2192 
1.0508 

4.2175 
4.7645 
1.6063 
0.4947 
5.0764 
2.2016 
2.4420 
0.3862 
0.5352 
2.5837 
2.9823 
3.1680 
1.3862 
0.0045 
4.4960 
4.1426 
2.7188 
1.3408 
0.5682 
5.0529 
2.2228 
2.4239 
0.2967 
0.4702 
2.5467 
2.9764 
3.0758 
1.2313 
0.4281 
4.0007 
3.9426 
2.4388 
0.1408 
0.8682 

bridge 
4.1667 
4.6000 
0.8529 
1.0196 
5.0068 
2.2040 
2.3868 
0.2949 
0.4519 
2.5327 
2.9054 
3.1031 
1.2494 
0.0849 
4.0497 
3.9846 
2.5415 
0.7109 
1.0687 
4.8445 
2.2591 
2.3862 
0.2915 
0.5319 
2.5111 
2.8526 
3.0519 
1.0073 
0.5566 
3.7615 
3.8927 
2.4365 
0.1693 
1.2806 

elastic-net 
4.2564 
4.6172 
1.0897 
0.9043 
4.9799 
2.2455 
2.3976 
0.3973 
0.4188 
2.5733 
2.9292 
3.1103 
1.2163 
0.0687 
4.0812 
4.1085 
2.5392 
0.8951 
0.9626 
5.1695 
2.2641 
2.3924 
0.3129 
0.4146 
2.4679 
2.8938 
3.0891 
1.0895 
0.4168 
3.8279 
3.9088 
2.4380 
0.2346 
1.0883 

23 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
4.7239 
2.2659 
  2.3634 
0.2167 
0.5887 
2.3943 
2.7094 
  2.8751 
1.0005 
0.5137 
3.6047 

5.1653 
2.2009 
2.4256 
0.2161 
0.4454 
2.5111 
3.3170 
3.0507 
1.0727 
0.3733 
4.0011 

5.2451 
2.2366 
2.4227 
0.2991 
0.4456 
2.5525 
3.4158 
3.2350 
1.0712 
0.4017 
3.9795 

4.8529 
2.2796 
2.4071 
0.2799 
0.5270 
2.4899 
2.9196 
3.0590 
1.2145 
0.4849 
3.9439 

4.8246 
2.3159 
2.3694 
0.2747 
0.5887 
2.4543 
2.7958 
3.0351 
0.9905 
0.6134 
3.7047 

4.8443 
2.3209 
2.3756 
0.2961 
0.4714 
2.4111 
2.8370 
3.0723 
1.0727 
0.4736 
3.7711 

ğ‘ˆğ‘ˆğµğµï¿½ï¿½ï¿½ï¿½
ğ›½ğ›½Ì‚2
ğ‘€ğ‘€ğ‘†ğ‘†ğ¸ğ¸
ğ‘†ğ‘†ğ¸ğ¸ï¿½ï¿½ï¿½ï¿½
ğ¿ğ¿ğµğµï¿½ï¿½ï¿½ï¿½
ğ‘ˆğ‘ˆğµğµï¿½ï¿½ï¿½ï¿½
ğ›½ğ›½Ì‚5
ğ‘€ğ‘€ğ‘†ğ‘†ğ¸ğ¸
ğ‘†ğ‘†ğ¸ğ¸ï¿½ï¿½ï¿½ï¿½
ğ¿ğ¿ğµğµï¿½ï¿½ï¿½ï¿½
ğ‘ˆğ‘ˆğµğµï¿½ï¿½ï¿½ï¿½
5. Real data example 

In  this  section,  the  data  set  which  is  used  by  Ramanathan  (1998)  is  discussed  to  compare  the 
proposed  methods  under 
  distribution  assumption  with  normal  distribution  assumption.  We 
discussed the data set in terms of parameter estimation and variable selection cases. The data set 
considers the electricity consumption provided to the households by San Diago Electric and Gas 
Company.  Data  consisted  of  87  quarters  between  1972  and  1993.  The  response  variable  is  the 
logarithmic value of the electricity consumed by the houses in kilowatts (LKWH). The explanatory 
variables  are  incomes  of  householders  (LY),  electricity  kilowatt  price  (LPRICE),  the  energy 
required  to  cool  houses  (CDD)  and  the  energy  required  to  heat  houses  (HDD).  The  regression 
model obtained by Ramanathan (1998) and the expected parameter signs are as follows. 

ğ‘¡ğ‘¡

ğ›½ğ›½4 > 0.

ğ›½ğ›½1 > 0,

ğ›½ğ›½3 > 0,

ğ›½ğ›½2 < 0,

ğ¿ğ¿ğ»ğ»ğ¿ğ¿ğ»ğ» = ğ›½ğ›½0 + ğ›½ğ›½1ğ¿ğ¿ğ¿ğ¿ + ğ›½ğ›½2ğ¿ğ¿ğ¿ğ¿ğ‘…ğ‘…ğ¿ğ¿ğ¿ğ¿ğ¸ğ¸ + ğ›½ğ›½3ğ¿ğ¿ğ·ğ·ğ·ğ· + ğ›½ğ›½4ğ»ğ»ğ·ğ·ğ·ğ· + Îµt
In  case  of  omission  of  autoregressive  error  assumption,  the  sign  of  the  parameter  of  housing 
income (LY) variable was obtained as negative while the positive result was expected. However, 
when  the  error  terms  were  modeled  with  the  autoregressive  model,  a  4th degree  autoregressive 
model  was  obtained  according  to  the  ACF  (autocorrelation  function)  and  PACF  (partial 
autocorrelation  function)  graphs,  and  under  this  assumption,  as  a  result  of  the  re-parameter 
estimations,  the  signs  for  all  parameter  estimates  were  obtained  as  expected.  In  this  study, 
parameter estimations were re-made by using the proposed variable selection methods. The results 
of these estimates are given in Table 16. From this table we can see that if the autoregressive error 
term  assumption  is  not  considered  although  it  exists,  the  expected  sign  of  the  LY  variable  is 
appeared in opposite way. From this result we can understand that the using of autoregressive error 
term assumption is crucial for this type of data. By using AR(4) assumption and t distributed error 
term the performance of the variable selection methods are given in Table 17. According to this 
table OLS and ridge with AR(4) assumption select all the variables significant for the model. On 
the other hand, LASSO, SCAD and the bridge methods are finding the LY insignificant. When we 
model the data by using t distribution with 3 degrees of freedom and 4th degree of autoregression, 
t-ridge, t-LASSO and t-SCAD methods select LY and LPRICE variables. However, t-bridge and 
t-elastic-net methods finds only LPRICE variable significant. From these results we can say that 
only the electricity kilowatt price is affected the amount of electricity consumption in the houses.  

24 

 
 
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Table 16. Parameter estimation results for Electric and Gas Company data. 

OLS 
  -0.00234 
-0.01856 
0.06365 
0.08564 
- 

OLS(AR) 
0.18625 
-0.09354 
0.00029 
0.00022 
4 

LY 
LPRICE 
CDD 
HDD 
AR order 

ğœ·ğœ·ï¿½ğŸğŸ
ğ›½ğ›½Ì‚2
ğ›½ğ›½Ì‚3
ğ›½ğ›½Ì‚4

Table 17. Selected variables for Electric and Gas Company data. 

Method 

OLS 
ridge 
LASSO 
SCAD 
bridge 
elastic-net 
-ridge 
-LASSO 
-SCAD 
-bridge 
-elastic-net 

ğ‘¡ğ‘¡3
ğ‘¡ğ‘¡3
ğ‘¡ğ‘¡3
ğ‘¡ğ‘¡3
ğ‘¡ğ‘¡3

(ğ›¾ğ›¾ = 0.7)

(ğ›¾ğ›¾ = 0.7)

Selected variables 
(1,2,3,4) 
(1,2,3,4) 
(2,3,4) 
(2,3,4) 
(2,3,4) 
(2,3,4) 
(2,3,4) 
(1,2) 
(1,2) 
(2) 
(2) 

6. Conclusion 

We  have  proposed  the  t  distributed  variable  selection  methods  with  autoregressive  error  term 
regression models to improve the resistance of the mentioned methods against the outliers. The 
variable selection methods; LASSO, SCAD, bridge and elastic-net based on t distributed are robust 
alternatives to the normal ones. We have provided an ECM algorithm to compute the proposed 
methods and performed a simulation study and a real data example to show the performance of the 
proposed  methods.  The  simulation  study  shows  that  t  distributed  methods  have  superior 
performance  under  the  existence  of  the  outliers  among  the  other  methods  in  terms  of  both 
parameter estimation and variable selection. In terms of the determining the correct AR order for 
the  error  model,  LASSO,  SCAD,  bridge  and  the  elastic-net  methods  have  nearly  the  same 
performance. On the other hand, if the degree of autoregressive order is getting higher, than the 
bridge and the elastic-net methods perform better than the others. Using t distribution as an error 
term assumption can be extended to the case of heavy tailed skew distributions. This problem will 
be a further study.  

Acknowledgments 

This study is a part  of PhD dissertation in Ankara University, Graduate School  of Natural  and 
Applied Sciences. 

25 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
References 

Alpuim, T. and El-Shaarawi, A., 2008. On the efficiency of regression analysis with   AR(p) errors. 

Journal of Applied Statistics, 35:7, 717-737.  

Arslan, O. 2016 Penalized MM regression estimation with LÎ³ penalty: a robust version of bridge 

regression, Statistics, 50:6. 

Beach, C.M. and Mackinnon, J. G., 1978. A Maximum Likelihood Procedure for Regression with 

Autocorrelated Errors. Econometrica, vol. 46, no: 1,  pp. 51- 58. 

Box, E. P. G and Jenkins, M. G., 1976. Time Series Analysis: Forecasting and Control. Holden-

Day: San Fransisco. 

Dempster, A.P., Laird, N.M. and Rubin, D.B. 1977. Maximum likelihood from incomplete data 

via the EM algorithm. Journal of the Royal Statistical Society, Series B, 39,1-38. 

Fun, J. and Li,  R. 2001 Variable Selection via nonconcave penalized likelihood and its oracle 

properties. Journal of American Statistical Assoc. 96. 1348-1360. 

Frank, I.E. and Friedman, J. (1993). A statistical view of some chemometrics regression 

tools. Thechnometrics, Vol. 35, pp. 109-148. 

Hoerl,  A.E.,  and  Kennard,  R.W.,  1970  ridge  regression:  Biased  estimation  for  nonorthagonal 

problems. Technometrics.  

Lange,  K.L.,  Little,  R.J.A.  and  J.M.G.  Taylor,  1989.    Robust  statistical  modeling  using  the  t-

distribution, J. Am. Stat. Assoc. 84 pp. 881â€“896. 

Liu, Y., Zhang, H.H., Park, C., and Ahn, J. 2007. Support vector machines with adaptive 

 Lq penalty. Computational Statistics & Data Analysis, Vol. 51, pp.6380-6394. 

Lucas, A. 1997. Robustness of the student t-based M-estimator. Communications in Statistics â€“ 

Theory and Methods, 26(5), pp: 1165-1182. 

McLachilan,  G.  J.,  Krishnan,  T.,  (1997)  The  EM  Algorithm    and  Extensions,  Wiley  series  in 

probability and statistics, USA. 

Meng, X. L. and Rubin, D. B. (1993). Maximum likelihood estimation via the ECM algorithm: A 

general framework. Biometrika 80, 267-278 

Park, C., and Yoon, Y. J. 2011. bridge regression: Adaptivity and group selection. 

26 

 
 
Journal of Statistical Planning and Inference, Vol. 141, pp. 3506-3519. 

Tibshiran, R. (1996). Regression shrinkage and selection via the LASSO. Journal of the 

Royal Statistical Society B, Vol. 58, pp. 267-288. 

Tiku, M., Wong, W., & Bian, G., 2007, Estimating Parameters In Autoregressive Models In Non-

Normal  Situations:  Symmetric  Innovations  .  Communications  in  Statistics  -  Theory  and 

Methods, 28 (2), 315-341. 

TuaÃ§ Y., GÃ¼ney Y., ÅenoÄŸlu B. and Arslan O., 2018 Robust parameter estimation of regression 

model with AR(p) error terms, Communications in Statistics â€“ Simulation and Computation, 

47:8, 2343-2359. 

TuaÃ§, Y., GÃ¼ney, Y., & Arslan, O. (2020). Parameter estimation of regression model with AR (p) 

error terms based on skew distributions with EM algorithm. Soft Computing, 24(5), 3309-3330. 

TuaÃ§, Y. 2020, Robust Parameter EstÄ±matÄ±on and Model SelectÄ±on in AutoregressÄ±ve Error Term 

RegressÄ±on  Modelsi,  Ph.D  dissertion,  Ankara  University,  Graduate  School  of  Natural  and 

Applied Sciences, Ankara. 

Wang, H., Guodong L., and Chih-L,T., 2007. â€˜Regression Coefficient and Autoregressive Order 

Shrinkage and Selection via the Lassoâ€™. Journal of the Royal Statistical Society: Series B 

(Statistical Methodology) 69, no. 1  

Yoon, J.Y., Park, C. and Lee, T., 2012. Penalized Regression Models with Autoregressive Error 

Terms, Journal of Statistical Computation and Simulation, 83:9,1756-1772. 

Zou, H. and Hastie, T., 2005 Regularization and variable selection via the elastic net. J.R. Statist. 

Soc. B. 67, part 2, pp. 301-320. 

27 

 
 
