Variable Selection in Regression Model with AR(p) Error Terms Based on Heavy Tailed 
Distributions 

Y. Tuaç1 and O. Arslan1 

1Ankara University, Faculty of Science, Department of Statistics, 06100 Ankara/Turkey 

ytuac@ankara.edu.tr oarslan@ankara.edu.tr 

Abstract 
Parameter  estimation  and  the  variable  selection  are  two  pioneer  issues  in  regression  analysis.  
While traditional variable selection methods require prior estimation of the model parameters, the 
penalized methods simultaneously carry on parameter estimation and variable select. Therefore, 
penalized variable  selection  methods are of  great interest and have been  extensively  studied in 
literature. However, most of the papers in literature are only limited to the regression models with 
uncorrelated  error  terms  and  normality  assumption.  In  this  study,  we  combine  the  parameter 
estimation and the variable selection in regression models with autoregressive error term by using 
different  penalty  functions  under  heavy  tailed  error  distribution  assumption.  We  conduct  a 
simulation study and a real data example to show the performance of the estimators. 

JEL Codes: C150, C510, C520. 
Keywords: Dependent errors, EM algorithm, LASSO, heavy tailed distributions, variable 
selection. 

1. Introduction  

The  high  dimensional  nature  of  the  existing  data  sets  is  required  new  variable  section  and 
parameter  estimation  methods  which  has  brought  great  attention  to  the  statisticians.  Variable 
selection focuses on searching for the best set of relevant variables to include in a model. In linear 
regression  models  there  are  two  popular  variable  selection  procedures:  subset  selection  and 
penalized methods. Although, subset selection methods are quite practically useful, these methods 
need  parameter  estimation  before  performing  the  variable  selection  and  they  often  show  high 
variability which cannot reduce the prediction error of the full model. On the other hand, penalized 
regression methods  which we consider in this study such as LASSO, bridge, ridge, SCAD and 
elastic-net  are  select  variables  and  estimate  model  parameters  simultaneously.  Hence,  these 
methods provide less prediction error and more stability than the subset selection methods. We 
can briefly summarize the basic properties of these methods as follows: Least Absolute Shrinkage 
Selection  Operator  (LASSO)  estimation  method  (Tibshirani,  1996)  the  OLS  loss  function 
  type 
penalized function. Unlike ridge regression, there is no analytic solution for the LASSO therefore 
(𝑦𝑦 − 𝑿𝑿𝛽𝛽)
numeric methods should be used. LASSO estimation method sets the unimportant coefficients to 
zero  to  achieve  the  model  selection  while  doing  the  parameter  estimation  simultaneously. 
Smoothly Clipped Absolute Deviation (SCAD) introduced by  Fan  and  Li (2001) possesses the 
oracle  properties  unlike  the  LASSO  such  as:  unbiasedness  for  large  true  coefficient  to  avoid 

  is  minimized  under  the  restriction 

.  LASSO  use 

(𝑦𝑦 − 𝑿𝑿𝛽𝛽)

 �𝛽𝛽𝑗𝑗�

𝑝𝑝
𝑗𝑗=1

≤ 𝑡𝑡

𝐿𝐿1

∑

𝑇𝑇

1 

 
 
 
 
 
 
 
 
 
𝛼𝛼 = 3.7

𝛼𝛼
and the LASSO 

excessive estimation bias, sparsity to reduce model complexity, continuity to avoid unnecessary 
 can be choose by using 
variation in model prediction. According to Fan and Li (2001) optimal 
 parameter. Fan and Li 
Cross Validation (CV) method, it does not bring any difference to select 
𝜆𝜆
 is a good choice for various problem, we also use the same value 
(2001) suggested taking 
in  simulation  study.  Bridge,  introduced  by  Frank  and  Friedman  (1993)  utilizes  the 
penalty and thus, it includes the ridge (Hoerl and Kennard, 1970) 
𝐿𝐿𝛾𝛾 (𝛾𝛾 > 0)
. In the literature there 
as special cases. Bridge estimators produce sparse models when 
(𝛾𝛾 = 2) 
(𝛾𝛾 = 1)
 (Park and Yoon, 2011, Liu et 
are some algorithms which proposed to choose optimal value of 
0 < 𝛾𝛾 ≤ 1
al., 2007). In this study we choose 
 parameters by using a two-dimensional  grid search. If 
𝛾𝛾
some  of  the  covariates  are  highly  correlated,  then  the  elastic-net  proposed  by  Zou  and  Hastie 
(2005) which combines 
These methods are widely used in literature and most studies have been done under assumption 
that observations are independent. However, if the error terms are correlated in time and ignoring 
this correlation effect while estimating the model parameters may result in incorrectly estimate of 
the variance of the model parameters. This causes the invalid resulting of confidence intervals and 
hypothesis tests. To overcome this problem, we assumed p order autoregressive (AR(p)) (Box and 
Jenkins,  1976)  structure  to  model  the  error  terms.  This  type  of  regression  models  has  been 
considered  before  both  parameter  estimation  without  variable  selection  and  penalized  based 
variable selection cases.  

𝛾𝛾
 norms can be used.  

 and 

 and 

𝐿𝐿2

𝐿𝐿1

𝜆𝜆

Concerning the parameter estimation, some papers in literature which use normal distribution as 
error distribution assumption might be listed as follows. Alpuim and El-Shaarawi (2008) estimate 
parameters of the regression model with AR(p) error term using the ordinary least square (OLS) 
estimation  method.  They  also  use  the  maximum  likelihood  (ML)  and  conditional  maximum 
likelihood (CML) estimation methods under the assumption of normality and study the asymptotic 
properties of the resulting estimators. Beach and Mackinnon (1978) used ML method to estimate 
the parameters  of AR(1) error term  regression models. However, normality  assumption is very 
restrictive and sensitive to the existence of outliers in the data. The t distribution provides a useful 
alternative to the normal distribution for statistical modelling of data sets that have heavier tailed 
empirical  distribution.  Tiku  (2007)  estimates  the  parameters  by  using  the  modified  maximum 
likelihood (MML) method for the regression model with AR(1) error terms under the assumption 
that  the  error  term  has  heavy  tailed  distribution.  Recently,  Tuaç  et  al.  (2018)  proposed  to  use 
symmetric t distribution as the distribution for error terms and used CML method to carry on the 
estimation  of  the  model  parameters  in  autoregressive  error  term  regression  model  as  a  robust 
alternative  to  the  normal  distribution.  Further,  Tuaç  et  al.  (2020)  used  skew  alternatives  to  the 
normal and the 

 distributions as error distributions in AR(p) error term regression model.  

𝑡𝑡

Regarding  the  penalized  variable  selection  case  there  are  also  some  studies  are  exist  based  on 
AR(p)  error  term  regression  model.  Wang  et  al.  (2007)  consider  linear  regression  with 
autoregressive  errors.  They  employ  the  modified  LASSO-type  penalty  both  on  regression  and 
autoregressive  coefficients.  Yoon  et  al.  (2012)  consider  adaptive  LASSO,  SCAD  and  bridge 
method in AR(p) error term regression models under normality assumption. However, these papers 
are not considered any other distribution apart from the normal. In the regression model, the error 
terms may not always conform to the normal distribution in real life. The distribution of the data 
may be thicker-tailed than the normal distribution. Using normal distribution for the error terms 
might result lack of the performance of the selection of the important variables in the model besides 

2 

 
 
 
 
 
the higher MSE (mean squared error) of the parameter estimations. To solve this problem, as an 
alternative to the normal distribution, the symmetric t distribution, which exhibits a thicker-tailed 
behavior  than  the  normal  distribution,  is  considered  (Tuaç,  2020).  Using  t  distribution  as  error 
distribution improve the ability of the selection of the important variables and the resulting smaller 
MSE of the parameter estimations when there are outliers exist. In this paper, we construct five 
different  penalized  regression  methods;  LASSO,  bridge,  ridge,  SCAD  and  elastic-net  in  the 
autoregressive error term regression model with the distribution assumption of t distribution as a 
heavy tailed alternative to the normal distribution. To handle the complex maximization problem, 
we  combine  the  Expectation  Maximization  (EM)  (Dempster  et.al,  1977,  McLachilan  and 
Krishnan, 1997) algorithm with aforementioned variable selection methods. 

The remainder of  the paper  is organized as  follows,  Section  2  gives  concise  information about 
LASSO,  bridge,  SCAD  and  elastic-net  estimators.  In  Section  3,  we  discuss  penalizing  the 
autoregressive error term regression model with mentioned methods. In Section 4, we give some 
brief  description  of  EM  algorithm  and  we  propose  an  Expectation  Conditional  Maximization 
(ECM) (Meng, X. L. and Rubin, D. B., 1993) type of algorithm to calculate the ML estimate of 
the parameters and variable selection of penalized regression model with AR(p) error terms for 
each five methods under t distribution assumption. Section 5 presents a simulation study and a real 
data  example  for  comparison  of  the  five  methods  based  on  t  distribution.  Finally,  Section  6 
summarize the paper with a conclusion.  

2. Penalized variable selection methods 

Consider the following linear regression model 

𝑀𝑀

𝑦𝑦𝑡𝑡 = � 𝑥𝑥𝑡𝑡,𝑖𝑖
𝑖𝑖=1

𝛽𝛽𝑖𝑖 + 𝑒𝑒𝑡𝑡   ,

 𝑡𝑡 = 1,2, … , 𝑁𝑁                                                                                (1) 

  is the response variable,  

where, 
are  independent  and  identically  distributed  random  variables  with  mean  0  and  variance 
Penalized regression methods can be done by maximizing the following objective function  

  explanatory  variables,  

 regression parameters and 

𝑥𝑥𝑡𝑡,𝑖𝑖

𝑦𝑦𝑡𝑡

𝛽𝛽𝑖𝑖

.  
𝑒𝑒𝑡𝑡
2
𝜎𝜎

𝑝𝑝

𝐿𝐿𝑁𝑁 = − ln 𝐿𝐿(𝜷𝜷, 𝜎𝜎

2

 is the log-likelihood function and  

|𝑒𝑒𝑡𝑡) + 𝑁𝑁 �  𝑝𝑝𝜆𝜆𝑖𝑖(𝛽𝛽𝑖𝑖),                                                                                    (2)
’s are penalty 

 is a penalty function and 

𝑖𝑖=1

In this paper, we consider five different forms of 

where  
parameters.  
𝑙𝑙𝑙𝑙𝐿𝐿(. )

(i) 

LASSO
pλ β λ β
)
j
j

= ∑ ,  

(

p

j

=
1

0 

𝜆𝜆 >

3 

𝑝𝑝𝜆𝜆𝑖𝑖(. )
: 

𝑝𝑝𝜆𝜆𝑖𝑖(. )

𝜆𝜆𝑖𝑖

 
 
 
 
 
 
 
 
 
 
 
 
(ii) 

SCAD

pλ

β
(
j

)




 
 
= −


 





λβ
j

2

−

β
j

+

2
αλβ λ
2
j
1)

α

−

2(

(
)
2
α λ
+
1

2






,

,

,

β λ
≤

j

λ β αλ
j

<

≤

β αλ
>

j

(iii) 

(iv) 

Bridge
λ β λ β
p
)
j

= ∑  ,   

(

j

p

γ

Ridge
pλ β λ β
)
j
j

= ∑ ,  

(

2

0 < 𝛾𝛾 ≤ 1
0 

j

=
1

p

j

=
1

(v) 

Elastic Net

−

pλ

p

𝜆𝜆 >
+∑
β λ β λ β
(
j
j

∑  

=

)

2

p

2

1

j

j

=
1

j

=
1

3. Estimation in penalized autoregressive regression model 

We consider model (1) with error terms have dependent structure as AR(p) model as follows  

 .                         

       (3) 

For simplicity, following backshift operator

𝑎𝑎𝑡𝑡 = 𝑒𝑒𝑡𝑡 − 𝜙𝜙1𝑒𝑒𝑡𝑡−1 − ⋯ − 𝜙𝜙𝑝𝑝𝑒𝑒𝑡𝑡−𝑝𝑝,     �𝜙𝜙𝑗𝑗� < 1   ,   𝑗𝑗 = 1,2, … , 𝑝𝑝

 can be used   

 (𝐵𝐵)

       (4) 

𝑎𝑎𝑡𝑡 = Φ(𝐵𝐵)𝑒𝑒𝑡𝑡
 will be the new error term for regression model.  

 and they are 
where 
uncorrelated random variables with constant variance. Then, with the help of backshift operator, 
2
the regression model given in (1) with autoregressive error terms can be rewritten as  

𝑉𝑉𝑎𝑎𝑉𝑉(𝑎𝑎𝑡𝑡) = 𝜎𝜎

𝐸𝐸(𝑎𝑎𝑡𝑡) = 0

𝑎𝑎𝑡𝑡

, 

                                                            (5) 

where,  

Φ(𝐵𝐵)𝑦𝑦𝑡𝑡 = ∑ 𝛽𝛽𝑖𝑖

𝑀𝑀
𝑖𝑖=1 Φ(𝐵𝐵)𝑥𝑥𝑡𝑡,𝑖𝑖 + 𝑎𝑎𝑡𝑡,    𝑡𝑡 = 1,2, … , 𝑇𝑇
, 

,        

,  

Φ(𝐵𝐵)𝑦𝑦𝑡𝑡 = 𝑦𝑦𝑡𝑡 − 𝜙𝜙1𝑦𝑦𝑡𝑡−1 − ⋯ − 𝜙𝜙𝑝𝑝𝑦𝑦𝑡𝑡−𝑝𝑝
: number of regression coefficients,  
Φ(𝐵𝐵)𝑥𝑥𝑡𝑡,𝑖𝑖 = 𝑥𝑥𝑡𝑡,𝑖𝑖 − 𝜙𝜙1𝑥𝑥𝑡𝑡−1,𝑖𝑖 − ⋯ − 𝜙𝜙𝑝𝑝𝑥𝑥𝑡𝑡−𝑝𝑝,𝑖𝑖
𝑀𝑀
𝑇𝑇 = 𝑁𝑁 − 𝑝𝑝
𝑝𝑝:
𝑁𝑁:
process. 
𝜙𝜙𝑗𝑗

 autoregressive degree, 
 sample size,                                                  
  are  the  AR(p)  model  parameters  for 

  and 

  iid  error  terms  with  white  noise 

𝑎𝑎𝑡𝑡

𝑗𝑗 = 1,2, … , 𝑝𝑝

4 

 
 
 
 
 
 
 
 
 
 
 
                                        
         
 
 
 
 
 
 
 
 
 
      
                  
 
        
 
        
  
3.1 Normal distribution assumption 

The parameter  estimations and variable selection of regression model given in (4) can be done 
with penalized objective function as below. (Yoon et al., 2012) 

𝑁𝑁

𝑝𝑝

2

𝑀𝑀

𝑝𝑝

     (6) 

𝑇𝑇
𝑄𝑄𝑁𝑁(𝜷𝜷, 𝝓𝝓) = � �𝑦𝑦𝑡𝑡 − 𝒙𝒙𝒕𝒕

𝑡𝑡=𝑝𝑝+1

𝑗𝑗=1

𝜷𝜷 − � 𝜙𝜙𝑗𝑗�𝑦𝑦𝑡𝑡−𝑗𝑗 − 𝒙𝒙𝑡𝑡−𝑗𝑗

𝜷𝜷�

�

+ 𝑇𝑇 �  𝑝𝑝𝜆𝜆𝑖𝑖|𝛽𝛽𝑖𝑖|   + 𝑇𝑇 � 𝑝𝑝𝛿𝛿𝑗𝑗 �𝜙𝜙𝑗𝑗�

𝑖𝑖=1

𝑗𝑗=1

𝑇𝑇

Under the assumptions that 
log likelihood function with penalized term as follows. 

2

, this function also equivalently can be used as negative 

𝑎𝑎𝑡𝑡~𝑁𝑁(0, 𝜎𝜎

)

𝑀𝑀

𝑝𝑝

     (7) 

|𝑎𝑎𝑡𝑡) + 𝑇𝑇 �  𝑝𝑝𝜆𝜆𝑖𝑖|𝛽𝛽𝑖𝑖|   + 𝑇𝑇 � 𝑝𝑝𝛿𝛿𝑗𝑗 �𝜙𝜙𝑗𝑗� 

𝑖𝑖=1

𝑗𝑗=1

2

𝑄𝑄(𝜷𝜷, 𝝓𝝓, 𝜎𝜎

) = −ln𝐿𝐿𝑁𝑁(𝜷𝜷, 𝝓𝝓, 𝜎𝜎

2

where 

, 

 penalize function for 
 penalize function for 
𝛽𝛽
𝜙𝜙

𝑝𝑝𝜆𝜆𝑖𝑖|𝛽𝛽𝑖𝑖|:
𝑝𝑝𝛿𝛿𝑗𝑗 �𝜙𝜙𝑗𝑗�:
and 

, 

           (8) 

2

2

1

𝑇𝑇
2 ln(𝜎𝜎

2

|𝑎𝑎𝑡𝑡) = 𝑐𝑐 −

𝑁𝑁
𝑡𝑡=𝑝𝑝+1  �Φ(𝐵𝐵)𝑦𝑦𝑡𝑡 − ∑ 𝛽𝛽𝑖𝑖

𝑀𝑀
𝑖𝑖=1 Φ(𝐵𝐵)𝑥𝑥𝑡𝑡,𝑖𝑖�

2 ∑

2𝜎𝜎

ln𝐿𝐿(𝜷𝜷, 𝝓𝝓, 𝜎𝜎

) −
is the log likelihood function of the normal distribution. Someone can replace 
  function 
with interested penalized methods penalize function then the parameter estimation and the variable 
selection  can  be  done  simultaneously  by  minimizing  equation  (6)  or  maximizing  equation  (7). 
Note  that  according  to  Yoon  et  al.  (2012),  sake  of  the  ease  of  the  computation,  penalizing  the 
autoregressive  parameters  are  ignored.  In  this  study,  we  also  calculate  both  regression  and 
autoregression parameter estimations with only penalizing the regression model parameters. 
The  penalized  functions  belong  to  LASSO,  SCAD,  bridge  and  elastic-net  methods  are  not 
differentiable at the origin for the minimization problem in equation (6) and because they are not 
concave, Local Quadratic Approximation (LQA) which is proposed by Fun and Li (2001) can be 
used to solve this problem. According to the LQA the penalized functions are locally approximated 
at 

 in the following quadratic function  

𝑝𝑝𝜆𝜆𝑖𝑖|𝛽𝛽𝑖𝑖|

(0)

𝜷𝜷

0
= (𝛽𝛽1

0
 , … , 𝛽𝛽𝑀𝑀

 )

𝑝𝑝𝜆𝜆𝑖𝑖(|𝛽𝛽𝑖𝑖|) ≈ 𝑝𝑝𝜆𝜆𝑖𝑖 ��𝛽𝛽𝑖𝑖

(0)

′
𝑝𝑝𝜆𝜆

(0)

��𝛽𝛽𝑖𝑖
(0)

�𝛽𝛽𝑖𝑖

�

=

1
2

where, 

′
𝑝𝑝𝜆𝜆

1
2

(0)

𝑖𝑖 ��𝛽𝛽𝑖𝑖
(0)

�𝛽𝛽𝑖𝑖

�

�� +

��

2
 �𝛽𝛽𝑖𝑖

(0)2

− 𝛽𝛽𝑖𝑖

�

��

2
𝛽𝛽𝑖𝑖

(0)

+ 𝑝𝑝𝜆𝜆 ��𝛽𝛽𝑖𝑖

�� −

′
𝑝𝑝𝜆𝜆

(0)

��𝛽𝛽𝑖𝑖
(0)

�𝛽𝛽𝑖𝑖

�

1
2

��

(0)2

𝛽𝛽𝑖𝑖

5 

                                                     (9)

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 : derivative of the interested penalized function 

𝑖𝑖
(0)

: initial values of 

′
𝑝𝑝𝜆𝜆
By using this LQA method the minimization problem in equation (9) will be reduced the following 
𝛽𝛽𝑖𝑖
objective function around 

𝛽𝛽𝑖𝑖

. 

(0)

𝛽𝛽

𝑁𝑁

𝑝𝑝

2

𝑀𝑀

2

𝑇𝑇
) = � �𝑦𝑦𝑡𝑡 − 𝒙𝒙𝒕𝒕

𝑄𝑄(𝜷𝜷, 𝝓𝝓, 𝜎𝜎

      (10)
By minimizing the above objective function the parameter estimations and the variable selections 
for  autoregressive  error  term  regression  model  with  t  distribution  assumption  can  be  made 
simultaneously.  

𝜷𝜷 − � 𝜙𝜙𝑗𝑗�𝑦𝑦𝑡𝑡−𝑗𝑗 − 𝒙𝒙𝒕𝒕−𝒋𝒋

�  
𝑖𝑖=1

𝑡𝑡=𝑝𝑝+1

�𝛽𝛽𝑖𝑖

𝜷𝜷�

𝑗𝑗=1

+

�

�

𝑇𝑇
2

𝑇𝑇

′
𝑝𝑝𝜆𝜆𝑖𝑖

(0)

��𝛽𝛽𝑖𝑖
(0)

��

2
𝛽𝛽𝑖𝑖

𝑻𝑻
𝜷𝜷� = �Φ� (𝐵𝐵)𝑿𝑿

Φ� (𝐵𝐵)𝑿𝑿 + 𝑇𝑇𝑝𝑝𝜆𝜆(𝛀𝛀)�

−1

𝑻𝑻
�Φ� (𝐵𝐵)𝑿𝑿

Φ� (𝐵𝐵)𝒀𝒀�

where 

                                                  (11) 

:  is  the  diagonal  matrix  derived  from  LQA  method  with  penalty  parameter  of  interested 

variable selection method. 
𝑝𝑝𝜆𝜆(𝛀𝛀)

where 

−𝟏𝟏

𝝓𝝓� = 𝐑𝐑�𝜷𝜷��

𝑅𝑅0�𝜷𝜷��                                                                                                                      (12)

𝑁𝑁

𝑁𝑁

𝑁𝑁

𝑁𝑁

𝑅𝑅0�𝜷𝜷�� =

� 𝑒𝑒𝑡𝑡𝑒𝑒𝑡𝑡−1
𝑡𝑡=𝑝𝑝+1
𝑁𝑁

� 𝑒𝑒𝑡𝑡𝑒𝑒𝑡𝑡−2
𝑡𝑡=𝑝𝑝+1

𝑁𝑁

⋮

� 𝑒𝑒𝑡𝑡𝑒𝑒𝑡𝑡−𝑝𝑝
𝑡𝑡=𝑝𝑝+1

⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣

⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦

𝑁𝑁

, 𝐑𝐑�𝜷𝜷��   =

2

� 𝑒𝑒𝑡𝑡−2𝑒𝑒𝑡𝑡−1
𝑡𝑡=𝑝𝑝+1

� 𝑒𝑒𝑡𝑡−1
𝑡𝑡=𝑝𝑝+1
𝑁𝑁

⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎢
� 𝑒𝑒𝑡𝑡−𝑝𝑝𝑒𝑒𝑡𝑡−1
⎢
𝑡𝑡=𝑝𝑝+1
⎣

𝑁𝑁

⋮

� 𝑒𝑒𝑡𝑡−1𝑒𝑒𝑡𝑡−2
𝑡𝑡=𝑝𝑝+1
𝑁𝑁

2

� 𝑒𝑒𝑡𝑡−2
𝑡𝑡=𝑝𝑝+1

𝑁𝑁

⋮

� 𝑒𝑒𝑡𝑡−𝑝𝑝𝑒𝑒𝑡𝑡−2
𝑡𝑡=𝑝𝑝+1

� 𝑒𝑒𝑡𝑡−1𝑒𝑒𝑡𝑡−𝑝𝑝
𝑡𝑡=𝑝𝑝+1
𝑁𝑁

� 𝑒𝑒𝑡𝑡−2𝑒𝑒𝑡𝑡−𝑝𝑝
𝑡𝑡=𝑝𝑝+1

⋮

𝑁𝑁

2

� 𝑒𝑒𝑡𝑡−𝑝𝑝
𝑡𝑡=𝑝𝑝+1

…
…

⋱

…

.

⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦

2

𝜎𝜎�

=

1
𝑇𝑇

� �Φ� (𝐵𝐵)𝑦𝑦𝑡𝑡 − Φ� (𝐵𝐵)𝒙𝒙𝒕𝒕𝜷𝜷��
𝑡𝑡=𝑝𝑝+1

2

3.2 t distribution assumption 

                                                                                      (13)

In this paper, we use ECM algorithm to find the CML estimates. Note that when t distribution is 
considered  in  different  models,  the  ECM  algorithm  is  extensively  used  because  of  the  normal 

6 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
mixture representation of the t distribution. In the next section, we show how we employ the ECM 
algorithm to estimate the AR(p) regression model parameters under t distribution assumption. The 
probability density  function (pdf)  of Student’s t distribution which  we used in this article is as 
follow. 

where 

𝑓𝑓(𝑥𝑥) =

𝑐𝑐𝑣𝑣
𝜎𝜎
ν+1

Γ�

2 �𝜈𝜈
𝑣𝑣
2�

√𝜋𝜋Γ�

𝑐𝑐𝑣𝑣 =

−

𝑣𝑣+1
2

2

2�

,                                       − ∞ < 𝑥𝑥 < ∞                                         (14)

�𝜈𝜈 +

𝑥𝑥
𝜎𝜎

𝜈𝜈/2

 , 

 degrees of freedom and 

 scale parameter.  

2

𝜈𝜈 > 0

𝜎𝜎

> 0

The t distribution is a symmetric heavy tailed alternative to the normal distribution and it is more 
resistant  than  the  normal  distribution  to  the  outliers.    Consider  the  model  in  equation  (5)  and 
assume  that  error  terms  have  the  t  distribution
.  The  CML  estimators  of  the 
  can  be  obtained  by  using  the  following  conditional  log 
unknown  parameters 
likelihood function 

 �𝑎𝑎𝑡𝑡~𝑡𝑡𝜈𝜈(0, 𝜎𝜎

, 𝜈𝜈)�

2

2

𝜽𝜽 = (𝜷𝜷, 𝝓𝝓, 𝜎𝜎

, 𝜈𝜈)

𝑁𝑁

𝑣𝑣 + 1
2

�Φ(𝐵𝐵)𝑦𝑦𝑡𝑡 − ∑ 𝛽𝛽𝑖𝑖

2
𝑀𝑀
𝑖𝑖=1 Φ(𝐵𝐵)𝑥𝑥𝑡𝑡,𝑖𝑖 �
𝜎𝜎

𝑡𝑡=𝑝𝑝+1 

�𝑣𝑣 +

ln 𝐿𝐿(𝜽𝜽|𝑎𝑎𝑡𝑡) = ln 𝑐𝑐𝜈𝜈 − 𝑇𝑇 ln 𝜎𝜎 −
�.             (15)
� ln
  can  be  obtained  by  maximizing  this  conditional  log 
The  estimators  of  the  parameter  vector 
likelihood function. However, it is not tractable to obtain the maximum of this function. Therefore, 
some numerical methods should be used to obtain the CML estimates of the parameters. Among 
these numerical methods, the ECM algorithm will be slightly easy to use because of the normal 
mixture  representation  of  the  t  distribution.  First,  hierarchical  formulation  in  terms  of  the 
conditional distributions can be expressed as follows. 

𝜽𝜽

2

𝑻𝑻
Φ(𝐵𝐵)𝑦𝑦𝑡𝑡|𝑢𝑢𝑡𝑡~𝑁𝑁 �Φ(𝐵𝐵)𝒙𝒙𝒕𝒕

𝜷𝜷,

2

𝜎𝜎
𝑢𝑢𝑡𝑡� ,                                                                                                 (16)

𝑢𝑢𝑡𝑡~𝐺𝐺𝑎𝑎𝐺𝐺𝐺𝐺𝑎𝑎 �

𝜈𝜈
2

𝜈𝜈
 is missing and 
2

�,

,

  is the observed data, respectively. Then the conditional 

where   
pdf is  

𝒖𝒖 = (𝑢𝑢1, … , 𝑢𝑢𝑛𝑛)

𝑦𝑦

𝑢𝑢𝑡𝑡|Φ(𝐵𝐵)𝑦𝑦𝑡𝑡 ~ 𝐺𝐺𝑎𝑎𝐺𝐺𝐺𝐺𝑎𝑎 �

2
 (𝜈𝜈 + 𝜅𝜅𝑡𝑡

)�,                                                                        (17)

𝜈𝜈 + 1
2

,

1
2

. Considering 

𝟐𝟐

 is missing data and the complete data log likelihood 

where 
2
function for 
𝜅𝜅𝑡𝑡
=

𝑇𝑇
�Φ(𝐵𝐵)𝑦𝑦𝑡𝑡−Φ(𝐵𝐵)𝒙𝒙𝑡𝑡
𝜷𝜷�
 can be written as 

2

𝜎𝜎

(𝒚𝒚, 𝒖𝒖)

𝑢𝑢

𝑁𝑁

𝑙𝑙𝑐𝑐(𝜽𝜽; 𝒚𝒚, 𝒖𝒖) = −

2

ln(2𝜋𝜋𝜎𝜎

) +

𝑇𝑇
2

1
2

� ln 𝑢𝑢𝑡𝑡
𝑡𝑡=𝑝𝑝+1
7 

 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
 
 
 
𝑁𝑁

2
𝜅𝜅𝑡𝑡
2

𝜈𝜈
2

𝜈𝜈
2

𝑁𝑁

− � �
𝑡𝑡=𝑝𝑝+1

+ 𝑇𝑇 ln 𝑐𝑐𝜈𝜈                                 (18)
According to the procedure of the ECM algorithm, the conditional expectation of the complete 
data log likelihood function, given the observed data and the current parameter estimate should be 
calculated. Therefore, the following conditional expectation should be calculated. 

− 1�  � ln 𝑢𝑢𝑡𝑡
𝑡𝑡=𝑝𝑝+1

𝑢𝑢𝑡𝑡 + �

+

�

𝐸𝐸(𝑙𝑙𝑐𝑐(𝜽𝜽; 𝒚𝒚, 𝒖𝒖)|Φ(𝐵𝐵)𝑦𝑦𝑡𝑡) = −

2

𝑇𝑇
ln(2𝜋𝜋𝜎𝜎
2
𝑁𝑁

) + 𝑇𝑇 ln 𝑐𝑐𝜈𝜈

+

1
2

� 𝐸𝐸(ln 𝑢𝑢𝑡𝑡 |Φ(𝐵𝐵)𝑦𝑦𝑡𝑡)
𝑡𝑡=𝑝𝑝+1

𝑁𝑁

− �
𝑡𝑡=𝑝𝑝+1

𝑁𝑁

2
𝜅𝜅𝑡𝑡
2

𝐸𝐸(𝑢𝑢𝑡𝑡|Φ(𝐵𝐵)𝑦𝑦𝑡𝑡)

−

𝜈𝜈
2

� 𝐸𝐸(𝑢𝑢𝑡𝑡|Φ(𝐵𝐵)𝑦𝑦𝑡𝑡)
𝑡𝑡=𝑝𝑝+1

𝑵𝑵

The conditional expectations 

𝝂𝝂
𝟐𝟐

+ �
 and 

− 𝟏𝟏� � 𝑬𝑬( 𝐥𝐥𝐥𝐥 𝒖𝒖𝒕𝒕 |𝚽𝚽(𝑩𝑩)𝒚𝒚𝒕𝒕)

                         (𝟏𝟏𝟏𝟏)
  can be computed as follows. 

𝒕𝒕=𝒑𝒑+𝟏𝟏

𝐸𝐸(𝑢𝑢𝑡𝑡|Φ(𝐵𝐵)𝑦𝑦𝑡𝑡)

𝐸𝐸(ln 𝑢𝑢𝑡𝑡 |Φ(𝐵𝐵)𝑦𝑦𝑡𝑡)

𝐸𝐸(𝑢𝑢𝑡𝑡|Φ(𝐵𝐵)𝑦𝑦𝑡𝑡) =

𝜈𝜈 + 1
𝜈𝜈 + 𝜅𝜅𝑡𝑡

2 ,                                                                                                           (20)

𝐸𝐸(ln 𝑢𝑢𝑡𝑡 |Φ(𝐵𝐵)𝑦𝑦𝑡𝑡) = 𝐷𝐷𝐺𝐺 �

2
 (𝜈𝜈 + 𝜅𝜅𝑡𝑡
 is digamma function. Then the steps of the ECM algorithm can be given as follows.  

)�,                                                         (21)

𝜈𝜈 + 1
2

� − ln �

1
2

where 

𝐷𝐷𝐺𝐺

The steps of the ECM algorithm: 
The steps of the ECM algorithm for this study can be briefly summarized as follows. 
E step Find the conditional expectations of the complete data log likelihood function given 
current  parameter  estimates 
 to be maximized. 
M step Maximize the objective function 
𝑄𝑄�𝜽𝜽; 𝜽𝜽
(𝑘𝑘)
new estimates 
These steps are very general. For the details of the algorithm to complete the estimates to following 
IRA can be given for the t distribution.  

 and 
.  These  conditional  expectations  result  the  objective  function 

 with respect to the parameters 

 to obtain the 

𝑄𝑄�𝜽𝜽; 𝜽𝜽

(𝒌𝒌+𝟏𝟏)

𝑦𝑦𝑡𝑡

(𝑘𝑘)

.  

(𝒌𝒌)

𝜽𝜽

𝜽𝜽

𝜽𝜽

�

�

8 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
1.  Determine  the  initial  value 
stopping criteria 

.  

(𝟏𝟏)

(𝟏𝟏)

(𝟏𝟏)

2(1)

(1)

𝜽𝜽

= �𝜷𝜷

, 𝝓𝝓

, 𝜎𝜎

, 𝜈𝜈

�

    of  the  parameter  vector 

    and 

𝜽𝜽

2.  (E-step)  Compute  the  following  conditional  expectations  using  the  current  estimates             

𝜂𝜂

(𝒌𝒌)

(𝒌𝒌)

(𝒌𝒌)

2(𝑘𝑘)

(𝑘𝑘)

𝜽𝜽

= �𝜷𝜷

, 𝝓𝝓

, 𝜎𝜎

, 𝜈𝜈

�

𝑘𝑘 = 1,2,3 … 

 for 

(𝑘𝑘)
𝑆𝑆̂1𝑡𝑡

= 𝐸𝐸�𝑢𝑢𝑡𝑡�Φ(𝐵𝐵)𝑦𝑦𝑡𝑡, 𝜽𝜽�

(𝒌𝒌)

� =

𝜈𝜈
(𝑘𝑘)

𝜈𝜈

(𝒌𝒌)

(𝑘𝑘)

+ 1

+ 𝜅𝜅𝑡𝑡
𝜈𝜈

(𝑘𝑘)2                                                                            (22)
(𝑘𝑘)

(𝑘𝑘)
𝑆𝑆̂2𝑡𝑡

= 𝐸𝐸�ln 𝑢𝑢𝑡𝑡 �Φ(𝐵𝐵)𝑦𝑦𝑡𝑡, 𝜽𝜽�

+ 𝜅𝜅𝑡𝑡
3.  (E-step)  Use  these  conditional  expectations  in  the  objective  function 

��                  (23)
    to  form  the 
. By writing these conditional expectations 
following function to be maximized with respect to 
in equation (21) the objective function to be maximized at M step can be obtained as follows:  

𝑄𝑄�𝜽𝜽; 𝜽𝜽��

� = 𝐷𝐷𝐺𝐺 �

� − ln �

 �𝜈𝜈

+ 1
2

1
2

(𝑘𝑘)

(𝑘𝑘)2

𝜽𝜽

𝑁𝑁

) + 𝑇𝑇 ln(𝑐𝑐𝜈𝜈) +
𝑁𝑁

(𝑘𝑘)
� 𝑆𝑆̂2𝑡𝑡
𝑡𝑡=𝑝𝑝+1

1
2

(𝑘𝑘)

𝑄𝑄�𝜽𝜽; 𝜽𝜽�

� = −

2

ln(2𝜋𝜋𝜎𝜎

𝑇𝑇
2

𝑁𝑁

𝜈𝜈
2
4. (CM step) Maximize 

−

(𝑘𝑘)
� 𝑆𝑆̂1𝑡𝑡
𝑡𝑡=𝑝𝑝+1

+ �

𝜈𝜈
2

(𝑘𝑘)
− 1� � 𝑆𝑆̂2𝑡𝑡
𝑡𝑡=𝑝𝑝+1
 with respect to 

−

𝑇𝑇𝜆𝜆
2

𝑁𝑁

− �
𝑡𝑡=𝑝𝑝+1

2
𝜅𝜅𝑡𝑡
2

(𝑘𝑘)
𝑆𝑆̂1𝑡𝑡

𝑀𝑀

��𝛽𝛽𝑖𝑖
𝑖𝑖=1

−1

(0)

2
𝛽𝛽𝑖𝑖
�
 to obtain  

                              (24)

(𝒌𝒌+𝟏𝟏)

th iteration  
= �𝜷𝜷�
𝜽𝜽

(𝒌𝒌+𝟏𝟏)

, 𝝓𝝓�

(𝑘𝑘)

(𝑘𝑘+1)

2(𝑘𝑘+1)

𝑄𝑄�𝜽𝜽; 𝜽𝜽�
, 𝜎𝜎�

. By doing this we obtain following estimates at (k+1) 

2

𝜽𝜽 = (𝜷𝜷, 𝝓𝝓, 𝜎𝜎

, 𝜈𝜈)

(𝑘𝑘+1)

�

� 
, 𝜈𝜈

(𝑘𝑘)
𝜷𝜷� = �𝑆𝑆̂1𝑡𝑡

𝑻𝑻
Φ� (𝐵𝐵)𝑿𝑿

−1

Φ� (𝐵𝐵)𝑿𝑿 + 𝑇𝑇𝑝𝑝𝜆𝜆(𝛀𝛀)�

                                  (25) 

(𝑘𝑘)
�𝑆𝑆̂1𝑡𝑡

𝑻𝑻
Φ� (𝐵𝐵)𝑿𝑿

𝝎𝝎Φ� (𝐵𝐵)𝒀𝒀�

(𝑘𝑘+1)

𝝓𝝓�

−𝟏𝟏

= (𝑹𝑹𝒔𝒔)

(𝑹𝑹𝟎𝟎𝒔𝒔)                                                                                                                (26)

𝑁𝑁

2(𝑘𝑘+1)

(𝑘𝑘)
� � 𝑆𝑆̂1𝑡𝑡
𝑡𝑡=𝑝𝑝+1

1
𝑇𝑇

𝑇𝑇
�Φ� (𝐵𝐵)𝑦𝑦𝑡𝑡 − Φ� (𝐵𝐵)𝒙𝒙𝑡𝑡

2

(𝑘𝑘)

𝜎𝜎�

=

𝜷𝜷�
:  is  the  diagonal  matrix  derived  from  LQA  method  with  penalty  parameter  of  interested 

�                                                        (27)

�

variable selection method. 
𝑝𝑝𝜆𝜆(𝛀𝛀)
Here 

9 

 
 
   
  
 
 
 
 
 
 
 
 
 
 
 
 
 
𝑁𝑁

𝑁𝑁

(𝑘𝑘)
� 𝑆𝑆̂1𝑡𝑡
𝑡𝑡=𝑝𝑝+1

(𝑘𝑘)
� 𝑆𝑆̂1𝑡𝑡
𝑡𝑡=𝑝𝑝+1
𝑁𝑁

⎡
⎢
⎢
⎢
⎢
⎢
⋮
⎢
⎢
(𝑘𝑘)
� 𝑆𝑆̂1𝑡𝑡
⎢
𝑡𝑡=𝑝𝑝+1
⎣

𝑁𝑁

𝑒𝑒𝑡𝑡𝑒𝑒𝑡𝑡−1

𝑒𝑒𝑡𝑡𝑒𝑒𝑡𝑡−2

𝑒𝑒𝑡𝑡𝑒𝑒𝑡𝑡−𝑝𝑝

⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦

(𝑘𝑘)
� 𝑆𝑆̂1𝑡𝑡
𝑡𝑡=𝑝𝑝+1
𝑁𝑁

(𝑘𝑘)
� 𝑆𝑆̂1𝑡𝑡
𝑡𝑡=𝑝𝑝+1

⎡
⎢
⎢
⎢
⎢
⎢
⋮
⎢
⎢
(𝑘𝑘)
� 𝑆𝑆̂1𝑡𝑡
⎢
𝑡𝑡=𝑝𝑝+1
⎣

𝑁𝑁

2
𝑒𝑒𝑡𝑡−1

𝑁𝑁

(𝑘𝑘)
� 𝑆𝑆̂1𝑡𝑡
𝑡𝑡=𝑝𝑝+1
𝑁𝑁

𝑒𝑒𝑡𝑡−1𝑒𝑒𝑡𝑡−2

…

𝑒𝑒𝑡𝑡−2𝑒𝑒𝑡𝑡−1

(𝑘𝑘)
� 𝑆𝑆̂1𝑡𝑡
𝑡𝑡=𝑝𝑝+1

2
𝑒𝑒𝑡𝑡−2

      …    

𝑁𝑁

(𝑘𝑘)
� 𝑆𝑆̂1𝑡𝑡
𝑡𝑡=𝑝𝑝+1
𝑁𝑁

(𝑘𝑘)
� 𝑆𝑆̂1𝑡𝑡
𝑡𝑡=𝑝𝑝+1

𝑒𝑒𝑡𝑡−1𝑒𝑒𝑡𝑡−𝑝𝑝

𝑒𝑒𝑡𝑡−2𝑒𝑒𝑡𝑡−𝑝𝑝

𝑒𝑒𝑡𝑡−𝑝𝑝𝑒𝑒𝑡𝑡−1

𝑁𝑁

⋮
(𝑘𝑘)
� 𝑆𝑆̂1𝑡𝑡
𝑡𝑡=𝑝𝑝+1

⋱

…

𝑒𝑒𝑡𝑡−𝑝𝑝𝑒𝑒𝑡𝑡−2

 estimate of 

𝑁𝑁

⋮
(𝑘𝑘)
� 𝑆𝑆̂1𝑡𝑡
𝑡𝑡=𝑝𝑝+1

2
𝑒𝑒𝑡𝑡−𝑝𝑝

⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦

5. (CM-step) Solve the following equation to obtain the 

𝑹𝑹𝟎𝟎𝒔𝒔 =

, 𝑹𝑹𝒔𝒔 =

(𝑘𝑘)

𝜕𝜕𝑄𝑄�𝜽𝜽; 𝜽𝜽�
𝜕𝜕𝜈𝜈

𝑁𝑁

�

1
2

= � �
𝑡𝑡=𝑝𝑝+1
𝑁𝑁

ln �

𝜈𝜈
2

� +

1
2

−

1
2

′

Γ

�

𝜈𝜈
�
2
𝜈𝜈
2�

Γ �

(𝑘𝑘 + 1) 𝑡𝑡ℎ

𝜈𝜈

+

1
2

(𝑘𝑘)
𝑆𝑆̂2𝑡𝑡

−

(𝑘𝑘)
𝑆𝑆̂1𝑡𝑡

1
2

�

                                      (28)

= � �DG �

� − ln �

𝑡𝑡=𝑝𝑝+1

𝜈𝜈
2

(𝑘𝑘)
𝜈𝜈
� − 1 − 𝑆𝑆̂2𝑡𝑡
2

(𝑘𝑘)
+ 𝑆𝑆̂1𝑡𝑡

�

= 0.                                        

7. These steps are calculated until the convergence criteria  

 is satisfied.  

(𝒌𝒌+𝟏𝟏)

�𝜽𝜽�

(𝒌𝒌)

− 𝜽𝜽�

� < 𝜂𝜂

4. Simulation study 

In  this  section,  an  extensive  simulation  study  is  given  to  compare  the  performances  of  model 
parameter estimation and variable selection obtained by using OLS, LASSO, SCAD, ridge, bridge 
and elastic net methods under autoregressive error term regression models based on t distributions 
for finite sample size. The calculations in the simulation study were made through the R 3.4.1 (R 
Core Team, 2020) program. 

4.1 Sampling designs 

′

and 

𝜷𝜷𝟎𝟎 = (𝛽𝛽01, 𝛽𝛽02, … , 𝛽𝛽0𝑚𝑚)

We consider three different sampling cases, representing commonly encountered problems in data 
denotes the true  parameter vectors. 
sets. 
  follows  the  AR(p)  process  as  in  equation  (2)  with  t  distribution.  Estimating  the  degrees  of 
𝝓𝝓𝟎𝟎 = �𝜙𝜙01, 𝜙𝜙02, … , 𝜙𝜙0𝑝𝑝�
freedom  along  with  the  other  parameters  may  cause  the  influence  function  of  the  resulting 
𝑒𝑒𝑡𝑡
estimators unbounded  and  hence they  are not  going to  be robust  (Lucas,  1997).  Therefore, the 
degrees  of  freedom  is  usually  taken  as  fixed  and  treated  as  a  robustness  tuning  parameter  in 
robustness  studies  (for  example  see  Lange  et.al,  1989).  In  this  simulation  study  we  use  two 
different fixed degrees of freedom values for the t distribution. We choose the degrees of freedom 
).  First  value  of  this  distribution  provides  heavy-tailed  error 
as 
distributions and the second value of degrees of freedom provides a thin-tailed distribution.  

)  and 

  (

(

′

𝜈𝜈 = 3

𝑡𝑡3

𝜈𝜈 = 10 

𝑡𝑡10

While applying the LASSO method for model selection, the λ parameter was constructed with 0.1 
increments  between  [0,  3]  and  BIC  values  were  calculated  for  each  result  obtained  from  this 
interval, and the model corresponding to the smallest BIC value was selected as the best model. 

10 

 
 
 
 
 
 
 
 
 
 
 
 
 
Similarly, the γ parameter for the bridge method was produced in 0.1 increments in the range of 
[0.1, 2], and the λ parameter used in the bridge method was also included, and a two-dimensional 
grid  search  was  performed.  The  model  corresponding  to  the  smallest  obtained  BIC  value  was 
considered as the best model. For the SCAD method, the value suggested by Fun and Li (2001) 
was taken as  α=3.7 and  the λ  parameter was set  up as 0.1 increments between [0, 3] as  in the 
LASSO method. For the elastic net method, λ1 and λ2 were scanned simultaneously and the model 
corresponding to the smallest of the BIC values for each result obtained was determined as the 
best model. Sample sizes for all cases were taken as 

. 

We consider three different simulation cases as explained below to show the performance of the 
mentioned methods. 

𝑁𝑁 = 50, 100, 300

Case 1: In this case we consider 8 covariates for the model (1) where 
. 
The  independent  variables  of  the  regression  model  are  independently  generated  from  the 
multivariate normal distribution 
. 
 and 
We use 

. In this case we use AR order 
 for 

𝜷𝜷𝟎𝟎 = (3,1.5,0,0,2,0,0,0)

 and 

 for 

. 

′

𝜙𝜙 = 0.8

′
Case 2:  In this  case  we  would like to  show  the  performance of  the  variable  selection  methods 
under  the  higher  dimensional  case.  We  use  40  covariates  for  the  model  (1).  We  take  20 
insignificant and 20 significant independent variables with the following as  

𝑝𝑝 = 2

𝑝𝑝 = 1

�𝑎𝑎𝑡𝑡~𝑁𝑁𝑝𝑝(𝐎𝐎, 𝚺𝚺)�
𝝓𝝓 = (0.8, −0.2)

𝑝𝑝 = 1,2

𝜎𝜎 = 1

 , 2 … , 2�����
10
We generate the regression coefficients from multivariate normal distribution as in Case 1. 

𝜷𝜷 = �0, … , 0,
�����
10

,  0, … , 0,
�����
10

 2, … ,2���
10

�

.

′

Case 3: In order to show the robustness of the estimators based on the t distribution against outliers, 
 indicate the contamination rate 
the model designed in Case I is reconsidered with outliers. Let 
  being  the 
with  a  value  of  0.1  and 
sample volume. 
 distribution, 
𝑙𝑙
a data set with outliers was created. Actual 
the remaining 
, Taken as 
in case of
values of 
explanatory variables were obtained as in Case I. 

 contaminated data with 
𝐺𝐺 = [𝜖𝜖𝑁𝑁] 
 and 

is  the  full  value  of  the  contaminated  data,  with 

𝑡𝑡 = 1,2, … , 𝐺𝐺
 model parameters 
𝑁𝑁(0,1) 

𝑡𝑡 = 𝐺𝐺, 𝐺𝐺 + 1, 𝐺𝐺 + 2 … , 𝑁𝑁 

, produced from 

𝑁𝑁(50,1)

data 

𝐺𝐺

𝜖𝜖

′

𝛽𝛽 = (3,1.5,0,0,2,0,0)

𝐴𝐴𝑅𝑅(𝑝𝑝)

𝜙𝜙 = 0.8 

 𝑝𝑝 = 1

Variable selection performance. All simulations were repeated 100 times and the ratio of correctly 
predicted zero coefficients under each condition was counted to measure the performance of the 
model selection criteria (Correct). Similarly, the coefficients with zero, which should not be zero 
in the model, were also counted and their ratios were calculated (Incorrect). At the same time, the 
correct zero amounts given in the real model were counted simultaneously in each simulation run, 
and  the  correct  model  ratio  (Cor.fit.)  obtained  in  100  repetitions  was  calculated.  For  the 
autoregressive model, in each case, the number of correct autoregressive order (AR order) in 100 
replicates was counted. 

Parameter estimation performance. In order to measure the parameter estimation performances of 
the proposed methods, the results of the 95% confidence intervals calculated with the standard 

11 

 
 
 
 
 
 
 
 
 
 
errors of the  parameters  obtained with the help of  the  observed  fisher  information  matrix. The 
mean of the mean squared errors (MeanMSE) computed by using the following formula. 

We  give  the  average  of  the  performance  measures  based  on  100  simulated  data  sets  for  each 
simulation setting. 

𝑀𝑀𝑆𝑆𝐸𝐸�𝛽𝛽̂� = �𝛽𝛽̂ − 𝛽𝛽�

′

′

𝐸𝐸(𝑥𝑥𝑥𝑥

)�𝛽𝛽̂ − 𝛽𝛽�

4.3 Simulation results 

The simulation results for variable selection are given in Tables 1 - 10, and the results for mean of 
 and upper bounds 
model parameter estimations, MSE, standard errors

, lower bounds 

 are reported in Tables 11 - 15. 

 (𝑆𝑆𝐸𝐸����)

(𝐿𝐿𝐵𝐵����)

𝑡𝑡3

𝑡𝑡10

 and 

(𝑡𝑡3)

(𝑈𝑈𝐵𝐵����)
In Tables 1 – 4, the results for Case 1 are summarized. Table 1 and Table 2 show the results of the 
 assumptions, respectively. 
calculations with first-order autoregressive error terms, for 
It  can  be  observed  from  Table  1  that  all  methods  have  similar  performances  according  to  the 
parameter estimation. This situation is also supported by the standard error and confidence interval 
estimations of the parameters in Table 11 and Table 12. It is easily seen that if the error distribution 
assumption  is  normal  by  then,  LASSO,  SCAD,  bridge  and  elastic-net  methods  have  equally 
superior performances over other methods for variable selection and selection of the correct AR 
degree.  In Table 2, the  results are presented under the heavy-tailed 
 distribution. From this 
table, it can be said that OLS, LASSO, SCAD and ridge regression methods are affected by the 
heavy taildness both in parameter estimation and in variable selection cases. Tables 3 and 4 show 
the results for the second order autocorrelation error terms. From these tables, we can see that as 
the  autoregressive  order  increases,  all  the  results  obtained  from  the  aforementioned  methods 
deteriorate according to the MSE values. However, LASSO, SCAD, bridge and elastic net methods 
successfully maintain their variable selection performance. High dimensional simulation results 
mentioned in Case 2 are shown in Tables 5 - 8. From Table 5, it can be easily said that bridge and 
elastic net methods have quite good performance in terms of parameter estimation according to 
small MSE values. The results of the variable selection scenario in the high dimension, LASSO, 
SCAD,  bridge  and  elastic  net  methods  are  able  to  identify  the  significant  variables  quite  well. 
Table 6 shows the heavy-tailed error assumption and high dimension results together. From this 
table, the negative impact of parameter estimation performance on OLS, LASSO, SCAD and ridge 
regression methods can be seen under the heavy tail assumption. On the other hand; bridge and 
elastic net have better results than other methods on parameter estimation performance under both 
heavy tail and high dimension effects. From the parameter estimation perspective of view Table 
13 summarizes both heavy and thin tailed results. Bridge and elastic net methods have superior 
performances due to the better variable selection in high dimension case. Tables 7 and 8 show the 
results assuming second degree autoregressive error terms distributed as 
, respectively. 
Similar  to  Case  1,  it  can  be  said  that  the  parameter  estimation  performance  of  the  mentioned 
methods worsens as the autoregressive effect increases. However, bridge and elastic net methods 
perform better in parameter estimation. The most striking part about this situation is seen in the 

 and 

𝑡𝑡10

𝑡𝑡3

12 

 
 
 
 
 
 
𝑡𝑡3

𝑡𝑡10

 and 

correct determination of the AR degree among all the mentioned methods. Although LASSO and 
SCAD  methods  have  equal  performance  with  bridge  and  elastic  net  in  terms  of  determining 
significant  variables  in  the  regression  model,  bridge  and  elastic  net  have  a  much  superior 
performance than other methods. Table 9 and Table 10 contains the simulation results for Case 3. 
For this case 10% contamination is generated for the response variable. Results regarding the data 
obtained from 
 distribution assumptions respectively. From Table 9 when the default 
distribution is normal, it is clear that the parameter estimation and variable selection performances 
of all methods deteriorate if there  are some vertical outliers in the data. In Table 10, while the 
distribution  assumption  is  heavy  tailed,  it  can  be  said  that  bridge  and  elastic-net  methods 
outperform  other  methods  for  both  parameter  estimation  and  variable  selection  performances. 
Especially  when  the  sample  size  increased,  the  variable  selection  performances  of  bridge  and 
elastic net methods improved both for the selection of important variables for the regression model 
and for the correct degree of AR. This results are also supported by Table 14 and 15. The parameter 
and interval estimation performances are effected from the contamination. From these tables under 
the outlier case the heavy tailed distribution assumption have better standard error and confidence 
interval  estimation  performance.  Besides,  bridge  and  elastic  net  methods  cope  better  with  the 
contaminations than the other methods.  

13 

 
 
Table 1. Variable selection performance for 

N 

  Method 
50  OLS 

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

100  OLS 

(𝛾𝛾 = 0.7)

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

300  OLS 

(𝛾𝛾 = 0.7)

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

(𝛾𝛾 = 0.7)

MSE 
0.63022 
0.71577 
0.70708 
0.81457 
0.64991 
0.66223 
0.50160 
0.58037 
0.58469 
0.75763 
0.58469 
0.59624 
0.08640 
0.01501 
0.01446 
0.02276 
0.01936 
0.01994 

 error (Case 1 AR order=1) 
Regression Coefficient 
Incorrect 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 

𝑡𝑡10
Correct 
0.22 
5.00 
5.00 
0.22 
5.00 
5.00 
0.36 
5.00 
5.00 
0.41 
5.00 
5.00 
0.66 
5.00 
5.00 
1.75 
5.00 
5.00 

Cor. fit. 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 

Table 2. Variable selection performance for 

 error (Case 1 AR order=1) 

N 

  Method 
50  OLS 

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

100  OLS 

(𝛾𝛾 = 0.7)

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

300  OLS 

(𝛾𝛾 = 0.7)

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

(𝛾𝛾 = 0.7)

𝑡𝑡3

MSE 
6.14354 
5.25620 
4.24132 
9.02561 
1.19562 
1.29511 
4.58984 
3.23587 
2.52147 
5.66542 
1.10548 
1.16212 
1.41556 
2.25871 
1.95621 
3.25471 
0.75625 
0.88524 

Regression Coefficient 

Incorrect 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 

Cor. fit. 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 

Correct 
0.10 
5.00 
5.00 
0.06 
5.00 
5.00 
0.18 
5.00 
5.00 
0.23 
5.00 
5.00 
0.46 
5.00 
5.00 
0.55 
5.00 
5.00 

14 

AR order 
0 
100 
100 
0 
100 
100 
0 
100 
100 
0 
100 
100 
0 
100 
100 
0 
100 
100 

AR order 
1 
80 
81 
1 
100 
98 
3 
85 
87 
3 
100 
99 
5 
91 
95 
6 
100 
100 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Table 3. Variable selection performance for 

N 
  Method 
50  OLS 

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

100  OLS 

(𝛾𝛾 = 0.7)

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

300  OLS 

(𝛾𝛾 = 0.7)

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

(𝛾𝛾 = 0.7)

MSE 
1.20224 
2.55725 
1.50708 
2.71457 
1.17904 
1.03620 
0.81485 
1.39416 
1.25595 
1.99491 
1.00469 
0.70041 
0.52598 
1.26441 
0.81565 
1.20595 
0.57927 
0.61257 

𝑡𝑡10

 error (Case 1 AR order=2) 
Regression Coefficient 
Incorrect 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 

Correct 
0.12 
5.00 
5.00 
0.13 
5.00 
5.00 
0.25 
5.00 
5.00 
0.24 
5.00 
5.00 
0.60 
5.00 
5.00 
1.64 
5.00 
5.00 

Cor. fit. 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 

Table 4. Variable selection performance for 

 error (Case 1 AR order=2) 

N 
  Method 
50  OLS 

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

100  OLS 

(𝛾𝛾 = 0.7)

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

300  OLS 

(𝛾𝛾 = 0.7)

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

(𝛾𝛾 = 0.7)

𝑡𝑡3

MSE 
8.39124 
6.81415 
6.50981 
10.1145 
2.65604 
2.02209 
6.01922 
5.58925 
3.52912 
7.56630 
1.95292 
1.69129 
4.05256 
4.12623 
2.95562 
5.95624 
0.98527 
0.99814 

Regression Coefficient 
Incorrect 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 

Cor. fit. 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 

Correct 
0.10 
5.00 
5.00 
0.09 
5.00 
5.00 
0.25 
5.00 
5.00 
0.32 
5.00 
5.00 
0.31 
5.00 
5.00 
0.43 
5.00 
5.00 

15 

AR order 
0 
100 
100 
0 
100 
100 
0 
100 
100 
0 
100 
100 
0 
100 
100 
0 
100 
100 

AR order 
1 
77 
75 
1 
100 
95 
2 
83 
80 
3 
100 
99 
4 
85 
90 
5 
100 
100 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Table 5. Variable selection performance for 

 error (Case 2 AR order=1) 

N 
  Method 
50  OLS 

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

100  OLS 

(𝛾𝛾 = 0.8)

LASSO 
SCAD 
ridge 

elastic-net 

300  OLS 

bridge (𝛾𝛾 = 0.8)

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

MSE 
149.9554 
15.57302 
20.42574 
22.06755 
1.838428 
1.549361 
95.9561 
7.30260 
11.4591 
12.6466 
0.95664 
0.65483 
75.6543 
6.55169 
9.65418 
9.87563 
0.56978 
0.45929 

𝑡𝑡10

Regression Coefficient 
Incorrect 
0.02 
0.00 
0.00 
0.01 
0.00 
0.00 
0.01 
0.00 
0.00 
0.01 
0.00 
0.00 
0.01 
0.00 
0.00 
0.01 
0.00 
0.00 

Cor. fit. 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 

Correct 
0.10 
20.00 
20.00 
0.30 
20.00 
20.00 
0.21 
20.00 
20.00 
0.42 
20.00 
20.00 
0.40 
20.00 
20.00 
0.56 
20.00 
20.00 

(𝛾𝛾 = 0.7)
Table 6. Variable selection performance 

 error (Case 2 AR order=1) 

N 
  Method 
50  OLS 

LASSO 
SCAD 
ridge 

elastic-net 

100  OLS 

bridge (𝛾𝛾 = 0.8)

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

300  OLS 

(𝛾𝛾 = 0.7)

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

(𝛾𝛾 = 0.8)

𝑡𝑡3
MSE 
592.955 
59.4949 
40.2942 
69.5491 
2.98516 
2.78965 
254.281 
39.3218 
20.9265 
49.5232 
1.84567 
1.95148 
175.986 
26.5169 
16.5166 
39.9563 
1.55941 
1.46847 

Regression Coefficient 
Incorrect 
0.02 
0.00 
0.00 
0.01 
0.00 
0.00 
0.01 
0.00 
0.00 
0.01 
0.00 
0.00 
0.01 
0.00 
0.00 
0.01 
0.00 
0.00 

Cor. fit. 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 

Correct 
0.06 
20.00 
20.00 
0.12 
20.00 
20.00 
0.12 
20.00 
20.00 
0.25 
20.00 
20.00 
0.21 
20.00 
20.00 
0.33 
20.00 
20.00 

16 

AR order 
0 
100 
100 
0 
100 
100 
0 
100 
100 
0 
100 
100 
0 
100 
100 
0 
100 
100 

AR order 
0 
90 
95 
0 
98 
97 
0 
92 
96 
0 
100 
99 
0 
96 
99 
0 
100 
100 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Table 7. Variable selection performance for 

N 
  Method 
50  OLS 

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

100  OLS 

(𝛾𝛾 = 0.7)

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

300  OLS 

(𝛾𝛾 = 0.7)

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

(𝛾𝛾 = 0.8)

MSE 
198.112 
21.2455 
27.1567 
24.8754 
2.31128 
2.79868 
129.135 
18.3025 
18.2169 
19.6468 
1.64554 
1.98415 
91.6543 
10.1249 
11.6874 
10.1982 
0.82985 
0.95956 

𝑡𝑡10

 error (Case 2 AR order=2) 
Regression Coefficient 
Incorrect 
0.02 
0.00 
0.00 
0.01 
0.00 
0.00 
0.01 
0.00 
0.00 
0.01 
0.00 
0.00 
0.01 
0.00 
0.00 
0.01 
0.00 
0.00 

Correct 
0.08 
20.00 
20.00 
0.27 
20.00 
20.00 
0.11 
20.00 
20.00 
0.30 
20.00 
20.00 
0.40 
20.00 
20.00 
0.56 
20.00 
20.00 

Cor. fit. 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 

Table 8. Variable selection performance for 

 error (Case 2 AR order=2) 

N 
  Method 
50  OLS 

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

100  OLS 

(𝛾𝛾 = 0.8)

LASSO 
SCAD 
ridge 

elastic-net 

300  OLS 

bridge(𝛾𝛾 = 0.7)

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

(𝛾𝛾 = 0.8)

𝑡𝑡3

MSE 
854.541 
61.1156 
63.1565 
84.7545 
6.15694 
5.79868 
651.122 
52.8302 
54.2169 
71.2646 
4.64554 
3.98415 
501.250 
42.8921 
46.1258 
62.4518 
2.12355 
2.12585 

Regression Coefficient 
Incorrect 
0.02 
0.00 
0.00 
0.01 
0.00 
0.00 
0.01 
0.00 
0.00 
0.01 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 
0.00 

Cor. fit. 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 

Correct 
0.02 
20.00 
20.00 
0.18 
20.00 
20.00 
0.08 
20.00 
20.00 
0.24 
20.00 
20.00 
0.16 
20.00 
20.00 
0.45 
20.00 
20.00 

17 

AR order 
0 
100 
100 
0 
100 
100 
0 
100 
100 
0 
100 
100 
0 
100 
100 
0 
100 
100 

AR order 
0 
51 
78 
0 
100 
99 
0 
59 
89 
0 
100 
100 
0 
65 
91 
0 
100 
100 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Table 9. Variable selection performance for 

 error (Case 3) 

N 

50  OLS 

Method 

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

100  OLS 

(𝛾𝛾 = 0.8)

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

300  OLS 

(𝛾𝛾 = 0.7)

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

(𝛾𝛾 = 0.8)

MSE 
8548.54 
2547.11 
125.156 
1587.875 
52.1569 
48.7986 
8548.541 
2547.115 
125.1565 
1587.875 
52.15694 
48.79868 
1058.25 
1012.25 
41.2541 
421.542 
10.1231 
11.2354 

𝑡𝑡10

Regression Coefficient 
Incorrect 
0.21 
0.02 
0.09 
0.20 
0.02 
0.03 
0.21 
0.02 
0.09 
0.20 
0.02 
0.03 
0.00 
0.08 
0.00 
0.02 
0.00 
0.00 

Cor. fit. 
0.00 
0.88 
0.75 
0.00 
0.99 
0.97 
0.00 
0.88 
0.75 
0.00 
0.99 
0.97 
0.00 
1.00 
0.98 
0.00 
1.00 
1.00 

𝜖𝜖 = 0.1
Correct 
0.00 
3.88 
3.75 
0.00 
4.20 
4.03 
0.00 
3.88 
3.75 
0.00 
4.20 
4.03 
0.00 
4.55 
4.31 
0.00 
4.82 
4.81 

Table 10. Variable selection performance for 

N 
  Method 
50  OLS 

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

100  OLS 

(𝛾𝛾 = 0.8)

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

300  OLS 

(𝛾𝛾 = 0.7)

LASSO 
SCAD 
ridge 
bridge 
elastic-net 

(𝛾𝛾 = 0.8)

MSE 
298.651 
259.842 
54.6590 
55.9861 
8.36152 
8.15713 
142.122 
122.830 
38.2169 
47.2646 
6.19841 
7.98415 
109.251 
18.4892 
32.1258 
42.4568 
5.01521 
5.12585 

 error (Case 3) 
Regression Coefficient 
Incorrect 
0.21 
0.00 
0.01 
0.15 
0.00 
0.00 
0.11 
0.00 
0.00 
0.10 
0.00 
0.00 
0.10 
0.00 
0.00 
0.04 
0.00 
0.00 

Cor. fit. 
0.00 
1.00 
0.97 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 
0.00 
1.00 
1.00 

𝑡𝑡3
𝜖𝜖 = 0.1
Correct 
0.07 
4.02 
4.55 
0.08 
4.95 
4.91 
0.11 
4.45 
4.72 
0.18 
5.00 
5.00 
0.20 
4.71 
4.85 
0.22 
5.00 
5.00 

18 

AR order 
0 
32 
46 
0 
89 
82 
0 
32 
46 
0 
89 
82 
0 
72 
79 
0 
100 
100 

AR order 
0 
51 
81 
0 
100 
95 
0 
89 
90 
0 
100 
99 
0 
95 
97 
0 
100 
100 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Table 11. Parameter estimation results  for 

 (Case 1 AR order=1) 

N 
50 

𝑡𝑡10

100 

300 

   OLS 
2.8669 
  0.2056 
0.8353 
1.8426 
4.5439 
1.4632 
  0.0127 
0.1874 
1.3229 
1.6793 
2.2774 
  0.6923 
1.0568 
0.9591 
3.1372 
2.8849 
  0.1470 
0.7016 
1.9461 
4.0616 
1.5091 
  0.0013 
0.1158 
1.3969 
1.5881 
1.9526 
  0.5104 
0.8996 
1.3219 
2.7985 
3.1849 
  0.0470 
0.0016 
2.2461 
3.8609 
1.5659 
  0.0181 
0.0990 
1.4537 
1.5313 
2.0094 

𝛽𝛽̂1
𝑀𝑀𝑆𝑆𝐸𝐸
𝑆𝑆𝐸𝐸����
𝐿𝐿𝐵𝐵����
𝑈𝑈𝐵𝐵����
𝛽𝛽̂2
𝑀𝑀𝑆𝑆𝐸𝐸
𝑆𝑆𝐸𝐸����
𝐿𝐿𝐵𝐵����
𝑈𝑈𝐵𝐵����
𝛽𝛽̂5
𝑀𝑀𝑆𝑆𝐸𝐸
𝑆𝑆𝐸𝐸����
𝐿𝐿𝐵𝐵����
𝑈𝑈𝐵𝐵����
𝛽𝛽̂1
𝑀𝑀𝑆𝑆𝐸𝐸
𝑆𝑆𝐸𝐸����
𝐿𝐿𝐵𝐵����
𝑈𝑈𝐵𝐵����
𝛽𝛽̂2
𝑀𝑀𝑆𝑆𝐸𝐸
𝑆𝑆𝐸𝐸����
𝐿𝐿𝐵𝐵����
𝑈𝑈𝐵𝐵����
𝛽𝛽̂5
𝑀𝑀𝑆𝑆𝐸𝐸
𝑆𝑆𝐸𝐸����
𝐿𝐿𝐵𝐵����
𝑈𝑈𝐵𝐵����
𝛽𝛽̂1
𝑀𝑀𝑆𝑆𝐸𝐸
𝑆𝑆𝐸𝐸����
𝐿𝐿𝐵𝐵����
𝑈𝑈𝐵𝐵����
𝛽𝛽̂2
𝑀𝑀𝑆𝑆𝐸𝐸
𝑆𝑆𝐸𝐸����
𝐿𝐿𝐵𝐵����
𝑈𝑈𝐵𝐵����
𝛽𝛽̂5

𝒕𝒕𝟏𝟏𝟎𝟎

ridge 
3.4934 
0.4057 
1.2716 
1.5691 
4.9199 
1.2202 
0.0711 
0.2752 
1.1838 
1.8102 
2.5224 
0.8292 
1.3351 
0.8289 
3.6184 
2.7458 
0.3565 
1.1169 
1.6533 
4.7023 
1.2811 
0.0609 
0.1152 
1.2536 
1.7049 
2.5108 
0.6860 
0.9718 
1.1815 
3.1949 
3.1458 
0.0565 
0.0869 
1.7533 
4.3023 
1.3379 
0.0441 
0.0984 
1.3104 
1.6481 
2.4540 

LASSO 
3.4320 
0.6891 
1.8533 
1.2407 
4.8874 
1.2230 
0.0700 
0.2990 
1.1370 
1.8531 
2.6285 
0.9656 
1.2937 
0.7899 
3.6710 
3.2669 
0.6454 
1.6581 
1.5158 
4.7221 
1.3168 
0.0580 
0.1982 
1.2538 
1.7463 
2.6096 
0.8703 
0.9703 
1.2099 
3.1733 
3.1669 
0.0754 
0.1015 
1.9158 
4.3821 
1.3736 
0.0412 
0.1814 
1.3106 
1.6895 
2.5528 

19 

SCAD 
3.3545 
0.3830 
1.4886 
1.3597 
4.2134 
1.3386 
0.0605 
0.2685 
1.4002 
1.7207 
2.1193 
0.7865 
1.2685 
0.8695 
3.6330 
3.2796 
0.3373 
1.2231 
1.4332 
4.1899 
1.3598 
0.0424 
0.1790 
1.3352 
1.6837 
2.1134 
0.6943 
1.1136 
1.2931 
3.1377 
3.0796 
0.0573 
0.0231 
1.7332 
3.9899 
1.4166 
0.0256 
0.1622 
1.3920 
1.6269 
2.0566 

bridge 
3.3037 
0.2185 
0.7352 
1.8846 
4.1438 
1.3410 
0.0053 
0.1772 
1.3169 
1.6697 
2.0424 
0.7216 
1.1317 
0.9499 
3.1867 
3.1216 
0.1600 
0.5932 
1.9337 
3.9815 
1.3961 
0.0047 
0.1738 
1.3969 
1.6481 
1.9896 
0.6704 
0.8896 
1.4216 
2.8985 
3.0297 
0.0550 
0.0516 
2.1456 
3.9616 
1.4529 
0.0121 
0.1570 
1.4537 
1.5913 
1.9328 

elastic-net 
3.3934 
0.2357 
0.9720 
1.7693 
4.1169 
1.3825 
0.0161 
0.2796 
1.2838 
1.7103 
2.0662 
0.7288 
1.0986 
0.9337 
3.2182 
3.2455 
0.1577 
0.7774 
1.8276 
4.3065 
1.4011 
0.0109 
0.1952 
1.2796 
1.6049 
2.0308 
0.7076 
0.9718 
1.2818 
2.9649 
3.0458 
0.0565 
0.1169 
1.9533 
3.9813 
1.4579 
0.0059 
0.1784 
1.3364 
1.5481 
1.9740 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  0.4936 
0.8828 
1.3787 
2.7417 

0.6692 
0.9550 
1.2383 
3.1381 

0.8535 
0.9535 
1.2667 
3.1165 

0.6775 
1.0968 
1.3499 
3.0809 

0.6536 
0.8728 
1.4784 
2.8417 

0.6908 
0.9550 
1.3386 
2.9081 

𝑀𝑀𝑆𝑆𝐸𝐸
𝑆𝑆𝐸𝐸����
𝐿𝐿𝐵𝐵����
𝑈𝑈𝐵𝐵����

Table 12. Parameter estimation results for 

N 
50 

𝑡𝑡3

100 

300 

   OLS 
2.7854 
  0.2301 
0.8598 
1.8101 
4.5754 
1.3817 
  0.0372 
0.2119 
1.2904 
1.7108 
2.1959 
  0.7168 
1.0813 
0.9266 
3.1687 
2.8034 
  0.1715 
0.7261 
1.9136 
4.0931 
1.4276 
  0.0232 
0.1403 
1.3644 
1.6196 
1.8711 
  0.5349 
0.9241 
1.2894 
2.8300 
3.1034 
  0.0715 
0.0261 
2.2136 
3.8924 
1.4844 

𝛽𝛽̂1
𝐻𝐻𝐻𝐻𝐻𝐻
𝑆𝑆𝐸𝐸����
𝐿𝐿𝐵𝐵����
𝑈𝑈𝐵𝐵����
𝛽𝛽̂2
𝐻𝐻𝐻𝐻𝐻𝐻
𝑆𝑆𝐸𝐸����
𝐿𝐿𝐵𝐵����
𝑈𝑈𝐵𝐵����
𝛽𝛽̂5
𝐻𝐻𝐻𝐻𝐻𝐻
𝑆𝑆𝐸𝐸����
𝐿𝐿𝐵𝐵����
𝑈𝑈𝐵𝐵����
𝛽𝛽̂1
𝐻𝐻𝐻𝐻𝐻𝐻
𝑆𝑆𝐸𝐸����
𝐿𝐿𝐵𝐵����
𝑈𝑈𝐵𝐵����
𝛽𝛽̂2
𝐻𝐻𝐻𝐻𝐻𝐻
𝑆𝑆𝐸𝐸����
𝐿𝐿𝐵𝐵����
𝑈𝑈𝐵𝐵����
𝛽𝛽̂5
𝐻𝐻𝐻𝐻𝐻𝐻
𝑆𝑆𝐸𝐸����
𝐿𝐿𝐵𝐵����
𝑈𝑈𝐵𝐵����
𝛽𝛽̂1
𝐻𝐻𝐻𝐻𝐻𝐻
𝑆𝑆𝐸𝐸����
𝐿𝐿𝐵𝐵����
𝑈𝑈𝐵𝐵����
𝛽𝛽̂2

𝑡𝑡3

ridge 
3.5749 
0.4302 
1.2961 
1.5366 
4.9514 
1.3017 
0.0956 
0.2997 
1.1513 
1.8417 
2.6039 
0.8537 
1.3596 
0.7964 
3.6499 
2.8273 
0.3810 
1.1414 
1.6208 
4.7338 
1.3626 
0.0854 
0.1397 
1.2211 
1.7364 
2.5923 
0.7105 
0.9963 
1.1490 
3.2264 
3.2273 
0.0810 
0.1114 
1.7208 
4.3338 
1.4194 

 (Case 1 AR order=1) 
LASSO  SCAD 
3.4360 
3.5135 
0.4075 
0.7136 
1.5131 
1.8778 
1.3272 
1.2082 
4.2449 
4.9189 
1.4201 
1.3045 
0.0850 
0.0945 
0.2930 
0.3235 
1.3677 
1.1045 
1.7522 
1.8846 
2.2008 
2.7100 
0.8110 
0.9901 
1.2930 
1.3182 
0.8370 
0.7574 
3.6645 
3.7025 
3.3611 
3.3484 
0.3618 
0.6699 
1.2476 
1.6826 
1.4007 
1.4833 
4.2214 
4.7536 
1.4413 
1.3983 
0.0669 
0.0825 
0.2035 
0.2227 
1.3027 
1.2213 
1.7152 
1.7778 
2.1949 
2.6911 
0.7188 
0.8948 
1.1381 
0.9948 
1.2606 
1.1774 
3.1692 
3.2048 
3.1611 
3.2484 
0.0818 
0.0999 
0.0476 
0.1260 
1.7007 
1.8833 
4.0214 
4.4136 
1.4981 
1.4551 

20 

bridge 
3.3852 
0.2430 
0.7597 
1.8521 
4.1753 
1.4225 
0.0298 
0.2017 
1.2844 
1.7012 
2.1239 
0.7461 
1.1562 
0.9174 
3.2182 
3.2031 
0.1845 
0.6177 
1.9012 
4.0130 
1.4776 
0.0292 
0.1983 
1.3644 
1.6796 
2.0711 
0.6949 
0.9141 
1.3891 
2.9300 
3.1112 
0.0795 
0.0761 
2.1131 
3.9931 
1.5344 

elastic-net 
3.4749 
0.2602 
0.9965 
1.7368 
4.1484 
1.4640 
0.0406 
0.3041 
1.2513 
1.7418 
2.1477 
0.7533 
1.1231 
0.9012 
3.2497 
3.3270 
0.1822 
0.8019 
1.7951 
4.3380 
1.4826 
0.0354 
0.2197 
1.2471 
1.6364 
2.1123 
0.7321 
0.9963 
1.2493 
2.9964 
3.1273 
0.0810 
0.1414 
1.9208 
4.0128 
1.5394 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  0.0064 
0.1235 
1.4212 
1.5628 
1.9279 
  0.5181 
0.9073 
1.3462 
2.7732 

0.0686 
0.1229 
1.2779 
1.6796 
2.5355 
0.6937 
0.9795 
1.2058 
3.1696 

0.0657 
0.2059 
1.2781 
1.7210 
2.6343 
0.8780 
0.9780 
1.2342 
3.1480 

0.0501 
0.1867 
1.3595 
1.6584 
2.1381 
0.7020 
1.1213 
1.3174 
3.1124 

0.0124 
0.1815 
1.4212 
1.6228 
2.0143 
0.6781 
0.8973 
1.4459 
2.8732 

0.0186 
0.2029 
1.3039 
1.5796 
2.0555 
0.7153 
0.9795 
1.3061 
2.9396 

𝐻𝐻𝐻𝐻𝐻𝐻
𝑆𝑆𝐸𝐸����
𝐿𝐿𝐵𝐵����
𝑈𝑈𝐵𝐵����
𝛽𝛽̂5
𝐻𝐻𝐻𝐻𝐻𝐻
𝑆𝑆𝐸𝐸����
𝐿𝐿𝐵𝐵����
𝑈𝑈𝐵𝐵����

N 

𝑡𝑡3

50 

100 

300 

Table 13. Parameter estimation results for Case 2 AR order =1 
   OLS 
0.7766 
  16.387 
0.1530 
0.1776 
1.7069 
0.8581 
  14.525 
0.1398 
0.2101 
1.6754 
0.9396 
  12.663 
0.1266 
0.2426 
1.6439 
0.8581 
  14.030 
0.0598 
0.2101 
1.6754 
0.9396 
  12.168 
0.0466 
0.2426 
1.6439 
1.0211 
  10.306 
0.0334 
0.2751 

LASSO 
0.6320 
17.493 
0.1610 
0.0757 
1.8504 
0.7135 
15.631 
0.1478 
0.1082 
1.8189 
0.7950 
13.769 
0.1346 
0.1407 
1.7874 
0.7135 
15.136 
0.0678 
0.1082 
1.8189 
0.7950 
13.274 
0.0546 
0.1407 
1.7874 
0.8765 
11.412 
0.0414 
0.1732 

ridge 
0.6934 
18.659 
0.1897 
0.0741 
1.8799 
0.7749 
16.797 
0.1765 
0.1066 
1.8484 
0.8564 
14.935 
0.1633 
0.1391 
1.8169 
0.7749 
16.302 
0.0965 
0.1066 
1.8484 
0.8564 
14.440 
0.0833 
0.1391 
1.8169 
0.9379 
12.578 
0.0701 
0.1716 

𝛽𝛽̂̅
𝑀𝑀𝑆𝑆𝐸𝐸
𝑆𝑆𝐸𝐸����
𝐿𝐿𝐵𝐵����
𝑈𝑈𝐵𝐵����
𝛽𝛽̂̅
𝑀𝑀𝑆𝑆𝐸𝐸
𝑆𝑆𝐸𝐸����
𝐿𝐿𝐵𝐵����
𝑈𝑈𝐵𝐵����
𝛽𝛽̂̅
𝑀𝑀𝑆𝑆𝐸𝐸
𝑆𝑆𝐸𝐸����
𝐿𝐿𝐵𝐵����
𝑈𝑈𝐵𝐵����
𝛽𝛽̂̅
𝑀𝑀𝑆𝑆𝐸𝐸
𝑆𝑆𝐸𝐸����
𝐿𝐿𝐵𝐵����
𝑈𝑈𝐵𝐵����
𝛽𝛽̂̅
𝑀𝑀𝑆𝑆𝐸𝐸
𝑆𝑆𝐸𝐸����
𝐿𝐿𝐵𝐵����
𝑈𝑈𝐵𝐵����
𝛽𝛽̂̅
𝑀𝑀𝑆𝑆𝐸𝐸
𝑆𝑆𝐸𝐸����
𝐿𝐿𝐵𝐵����

100 

300 

𝑡𝑡10

50 

21 

SCAD 
0.7545 
10.464 
0.1363 
0.1947 
1.7764 
0.8360 
8.6025 
0.1231 
0.2272 
1.7449 
0.9175 
6.7405 
0.1099 
0.2597 
1.7134 
0.8360 
8.1075 
0.0431 
0.2272 
1.7449 
0.9175 
6.2455 
0.0299 
0.2597 
1.7134 
0.9990 
4.3835 
0.0167 
0.2922 

bridge 
0.8039 
6.3801 
0.1130 
0.2197 
1.7313 
0.8854 
4.5181 
0.0998 
0.2522 
1.6998 
0.9669 
2.6561 
0.0998 
0.2847 
1.6683 
0.8854 
4.0231 
0.0198 
0.2522 
1.6998 
0.9669 
2.1611 
0.0066 
0.2847 
1.6683 
1.0484 
0.2991 
0.0066 
0.3172 

elastic-net 
0.8034 
6.3842 
0.1197 
0.1436 
1.7437 
0.8849 
4.5222 
0.1065 
0.1761 
1.7122 
0.9664 
2.6602 
0.0933 
0.2086 
1.6807 
0.8849 
4.0272 
0.0265 
0.1761 
1.7122 
0.9664 
2.1652 
0.0133 
0.2086 
1.6807 
1.0479 
0.3032 
0.0001 
0.2411 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
1.6124 

1.7854 

1.7559 

1.6819 

1.6368 

1.6492 

𝑡𝑡10

100 

N 
50 

   OLS 
5.6484 

2.9285 
0.4101 
6.0754 
4.2447 

2.2806 
-0.1096 
3.2108 
5.0589 

𝑈𝑈𝐵𝐵����
Table 14. Parameter estimation results for 
 ridge 
𝑡𝑡10
6.2749 
  11.5626  11.7627 
3.3648 
0.1366 
6.4514 
4.0017 
  10.3697  10.4281 
2.3684 
-0.2487 
3.3417 
5.3039 
  11.0493  11.1862 
3.4283 
-0.6036 
5.1499 
5.5273 
10.0135 
3.2101 
0.2208 
6.2338 
4.0626 
9.7179 
2.2084 
-0.1789 
3.2364 
5.2923 
  10.1674  10.3430 
3.0650 
-0.2510 
4.7264 
5.9273 
8.7135 
2.1801 
0.3208 
5.8338 
4.1194 
8.7011 
2.1916 
-0.1221 
3.1796 

3.1500 
-0.4734 
4.6687 
5.6664 
  9.8040 
2.7948 
0.5136 
5.5931 
4.2906 
  9.6583 
2.2090 
-0.0356 
3.1196 
4.7341 

2.9928 
-0.1106 
4.3300 
5.9664 
  8.7040 
2.0948 
0.8136 
5.3924 
4.3474 
  8.6751 
2.1922 
0.0212 
3.0628 

𝛽𝛽̂1
𝑀𝑀𝑆𝑆𝐸𝐸
𝑆𝑆𝐸𝐸����
𝐿𝐿𝐵𝐵����
𝑈𝑈𝐵𝐵����
𝛽𝛽̂2
𝑀𝑀𝑆𝑆𝐸𝐸
𝑆𝑆𝐸𝐸����
𝐿𝐿𝐵𝐵����
𝑈𝑈𝐵𝐵����
𝛽𝛽̂5
𝑀𝑀𝑆𝑆𝐸𝐸
𝑆𝑆𝐸𝐸����
𝐿𝐿𝐵𝐵����
𝑈𝑈𝐵𝐵����
𝛽𝛽̂1
𝑀𝑀𝑆𝑆𝐸𝐸
𝑆𝑆𝐸𝐸����
𝐿𝐿𝐵𝐵����
𝑈𝑈𝐵𝐵����
𝛽𝛽̂2
𝑀𝑀𝑆𝑆𝐸𝐸
𝑆𝑆𝐸𝐸����
𝐿𝐿𝐵𝐵����
𝑈𝑈𝐵𝐵����
𝛽𝛽̂5
𝑀𝑀𝑆𝑆𝐸𝐸
𝑆𝑆𝐸𝐸����
𝐿𝐿𝐵𝐵����
𝑈𝑈𝐵𝐵����
𝛽𝛽̂1
𝑀𝑀𝑆𝑆𝐸𝐸
𝑆𝑆𝐸𝐸����
𝐿𝐿𝐵𝐵����
𝑈𝑈𝐵𝐵����
𝛽𝛽̂2
𝑀𝑀𝑆𝑆𝐸𝐸
𝑆𝑆𝐸𝐸����
𝐿𝐿𝐵𝐵����
𝑈𝑈𝐵𝐵����

300 

 (Case 3 AR order=1) 
LASSO    SCAD    
6.2135 
12.0461 
3.9465 
-0.1918 
6.4189 
4.0045 
10.4270 
2.3922 
-0.2955 
3.3846 
5.4100 
11.3226 
3.3869 
-0.6426 
5.2025 
6.0484 
10.3024 
3.7513 
0.0833 
6.2536 
4.0983 
9.7150 
2.2914 
-0.1787 
3.2778 
5.3911 
10.5273 
3.0635 
-0.2226 
4.7048 
5.9484 
8.7324 
2.1947 
0.4833 
5.9136 
4.1551 
8.6982 
2.2746 
-0.1219 
3.2210 

6.1360 
11.7400 
3.5818 
-0.0728 
5.7449 
4.1201 
10.4175 
2.3617 
-0.0323 
3.2522 
4.9008 
11.1435 
3.3617 
-0.5630 
5.1645 
6.0611 
9.9943 
3.3163 
0.0007 
5.7214 
4.1413 
9.6994 
2.2722 
-0.0973 
3.2152 
4.8949 
10.3513 
3.2068 
-0.1394 
4.6692 
5.8611 
8.7143 
2.1163 
0.3007 
5.5214 
4.1981 
8.6826 
2.2554 
-0.0405 
3.1584 

22 

elastic-net 
bridge 
6.0852 
6.1749 
11.5755  11.5927 
3.0652 
2.8284 
0.3368 
0.4521 
5.6484 
5.6753 
4.1225 
4.1640 
10.3623  10.3731 
2.3728 
2.2704 
-0.1487 
-0.1156 
3.2012 
3.2418 
4.8477 
4.8239 
11.0786  11.0858 
3.1918 
3.2249 
-0.4988 
-0.4826 
4.7497 
4.7182 
6.0270 
5.9031 
9.8147 
9.8170 
2.8706 
2.6864 
0.3951 
0.5012 
5.8380 
5.5130 
4.1826 
4.1776 
9.6679 
9.6617 
2.2884 
2.2670 
-0.1529 
-0.0356 
3.1364 
3.1796 
4.7711 
4.8123 
10.3274  10.3646 
3.0650 
2.9828 
-0.1507 
-0.0109 
4.4964 
4.4300 
5.8273 
5.8112 
8.7135 
8.7120 
2.2101 
2.1448 
0.5208 
0.7131 
5.5128 
5.4931 
4.2394 
4.2344 
8.6629 
8.6691 
2.2716 
2.2502 
-0.0961 
0.0212 
3.0796 
3.1228 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
4.7909 
  9.1506 
2.9760 
-0.0538 
4.2732 

5.2355 
9.3262 
3.0482 
-0.1942 
4.6696 

5.3343 
9.5105 
3.0467 
-0.1658 
4.6480 

4.8381 
9.3345 
3.1900 
-0.0826 
4.6124 

4.7143 
9.3106 
2.9660 
0.0459 
4.3732 

4.7555 
9.3478 
3.0482 
-0.0939 
4.4396 

𝛽𝛽̂5
𝑀𝑀𝑆𝑆𝐸𝐸
𝑆𝑆𝐸𝐸����
𝐿𝐿𝐵𝐵����
𝑈𝑈𝐵𝐵����

𝑡𝑡3

100 

N 
50 

Table 15. Parameter estimation results for 
 ridge 
𝑡𝑡3
4.3564 
4.7872 
1.3893 
0.7041 
5.7829 
2.0832 
2.4526 
0.3929 
0.3188 
2.6732 
3.3854 
3.2107 
1.4528 
-0.0361 
4.4814 
3.6088 
2.7380 
1.2346 
0.7883 
5.5653 
2.1441 
2.4424 
0.2329 
0.3886 
2.5679 
3.3738 
3.0675 
1.0895 
0.3165 
4.0579 
4.0088 
2.4380 
0.2046 
0.8883 

   OLS 
4.5669 
  4.8871 
0.9530 
0.9776 
5.4069 
2.1632 
  2.3942 
0.3051 
0.4579 
2.5423 
2.9774 
  3.0738 
1.1745 
0.0941 
4.0002 
3.5849 
  2.9285 
0.8193 
1.0811 
4.9246 
2.2091 
  2.3802 
0.2335 
0.5319 
2.4511 
2.6526 
  2.8919 
1.0173 
0.4569 
3.6615 
3.8849 
  2.4285 
0.1193 
1.3811 

𝛽𝛽̂1
𝑀𝑀𝑆𝑆𝐸𝐸
𝑆𝑆𝐸𝐸����
𝐿𝐿𝐵𝐵����
𝑈𝑈𝐵𝐵����
𝛽𝛽̂2
𝑀𝑀𝑆𝑆𝐸𝐸
𝑆𝑆𝐸𝐸����
𝐿𝐿𝐵𝐵����
𝑈𝑈𝐵𝐵����
𝛽𝛽̂5
𝑀𝑀𝑆𝑆𝐸𝐸
𝑆𝑆𝐸𝐸����
𝐿𝐿𝐵𝐵����
𝑈𝑈𝐵𝐵����
𝛽𝛽̂1
𝑀𝑀𝑆𝑆𝐸𝐸
𝑆𝑆𝐸𝐸����
𝐿𝐿𝐵𝐵����
𝑈𝑈𝐵𝐵����
𝛽𝛽̂2
𝑀𝑀𝑆𝑆𝐸𝐸
𝑆𝑆𝐸𝐸����
𝐿𝐿𝐵𝐵����
𝑈𝑈𝐵𝐵����
𝛽𝛽̂5
𝑀𝑀𝑆𝑆𝐸𝐸
𝑆𝑆𝐸𝐸����
𝐿𝐿𝐵𝐵����
𝑈𝑈𝐵𝐵����
𝛽𝛽̂1
𝑀𝑀𝑆𝑆𝐸𝐸
𝑆𝑆𝐸𝐸����
𝐿𝐿𝐵𝐵����

300 

 (Case 3 AR order=1) 

LASSO    SCAD    
4.2950 
5.0706 
1.9710 
0.3757 
5.7504 
2.0860 
2.4515 
0.4167 
0.2720 
2.7161 
3.4915 
3.3471 
1.4114 
-0.0751 
4.5340 
4.1299 
3.0269 
1.7758 
0.6508 
5.5851 
2.1798 
2.4395 
0.3159 
0.3888 
2.6093 
3.4726 
3.2518 
1.0880 
0.3449 
4.0363 
4.0299 
2.4569 
0.2192 
1.0508 

4.2175 
4.7645 
1.6063 
0.4947 
5.0764 
2.2016 
2.4420 
0.3862 
0.5352 
2.5837 
2.9823 
3.1680 
1.3862 
0.0045 
4.4960 
4.1426 
2.7188 
1.3408 
0.5682 
5.0529 
2.2228 
2.4239 
0.2967 
0.4702 
2.5467 
2.9764 
3.0758 
1.2313 
0.4281 
4.0007 
3.9426 
2.4388 
0.1408 
0.8682 

bridge 
4.1667 
4.6000 
0.8529 
1.0196 
5.0068 
2.2040 
2.3868 
0.2949 
0.4519 
2.5327 
2.9054 
3.1031 
1.2494 
0.0849 
4.0497 
3.9846 
2.5415 
0.7109 
1.0687 
4.8445 
2.2591 
2.3862 
0.2915 
0.5319 
2.5111 
2.8526 
3.0519 
1.0073 
0.5566 
3.7615 
3.8927 
2.4365 
0.1693 
1.2806 

elastic-net 
4.2564 
4.6172 
1.0897 
0.9043 
4.9799 
2.2455 
2.3976 
0.3973 
0.4188 
2.5733 
2.9292 
3.1103 
1.2163 
0.0687 
4.0812 
4.1085 
2.5392 
0.8951 
0.9626 
5.1695 
2.2641 
2.3924 
0.3129 
0.4146 
2.4679 
2.8938 
3.0891 
1.0895 
0.4168 
3.8279 
3.9088 
2.4380 
0.2346 
1.0883 

23 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
4.7239 
2.2659 
  2.3634 
0.2167 
0.5887 
2.3943 
2.7094 
  2.8751 
1.0005 
0.5137 
3.6047 

5.1653 
2.2009 
2.4256 
0.2161 
0.4454 
2.5111 
3.3170 
3.0507 
1.0727 
0.3733 
4.0011 

5.2451 
2.2366 
2.4227 
0.2991 
0.4456 
2.5525 
3.4158 
3.2350 
1.0712 
0.4017 
3.9795 

4.8529 
2.2796 
2.4071 
0.2799 
0.5270 
2.4899 
2.9196 
3.0590 
1.2145 
0.4849 
3.9439 

4.8246 
2.3159 
2.3694 
0.2747 
0.5887 
2.4543 
2.7958 
3.0351 
0.9905 
0.6134 
3.7047 

4.8443 
2.3209 
2.3756 
0.2961 
0.4714 
2.4111 
2.8370 
3.0723 
1.0727 
0.4736 
3.7711 

𝑈𝑈𝐵𝐵����
𝛽𝛽̂2
𝑀𝑀𝑆𝑆𝐸𝐸
𝑆𝑆𝐸𝐸����
𝐿𝐿𝐵𝐵����
𝑈𝑈𝐵𝐵����
𝛽𝛽̂5
𝑀𝑀𝑆𝑆𝐸𝐸
𝑆𝑆𝐸𝐸����
𝐿𝐿𝐵𝐵����
𝑈𝑈𝐵𝐵����
5. Real data example 

In  this  section,  the  data  set  which  is  used  by  Ramanathan  (1998)  is  discussed  to  compare  the 
proposed  methods  under 
  distribution  assumption  with  normal  distribution  assumption.  We 
discussed the data set in terms of parameter estimation and variable selection cases. The data set 
considers the electricity consumption provided to the households by San Diago Electric and Gas 
Company.  Data  consisted  of  87  quarters  between  1972  and  1993.  The  response  variable  is  the 
logarithmic value of the electricity consumed by the houses in kilowatts (LKWH). The explanatory 
variables  are  incomes  of  householders  (LY),  electricity  kilowatt  price  (LPRICE),  the  energy 
required  to  cool  houses  (CDD)  and  the  energy  required  to  heat  houses  (HDD).  The  regression 
model obtained by Ramanathan (1998) and the expected parameter signs are as follows. 

𝑡𝑡

𝛽𝛽4 > 0.

𝛽𝛽1 > 0,

𝛽𝛽3 > 0,

𝛽𝛽2 < 0,

𝐿𝐿𝐻𝐻𝐿𝐿𝐻𝐻 = 𝛽𝛽0 + 𝛽𝛽1𝐿𝐿𝐿𝐿 + 𝛽𝛽2𝐿𝐿𝐿𝐿𝑅𝑅𝐿𝐿𝐿𝐿𝐸𝐸 + 𝛽𝛽3𝐿𝐿𝐷𝐷𝐷𝐷 + 𝛽𝛽4𝐻𝐻𝐷𝐷𝐷𝐷 + εt
In  case  of  omission  of  autoregressive  error  assumption,  the  sign  of  the  parameter  of  housing 
income (LY) variable was obtained as negative while the positive result was expected. However, 
when  the  error  terms  were  modeled  with  the  autoregressive  model,  a  4th degree  autoregressive 
model  was  obtained  according  to  the  ACF  (autocorrelation  function)  and  PACF  (partial 
autocorrelation  function)  graphs,  and  under  this  assumption,  as  a  result  of  the  re-parameter 
estimations,  the  signs  for  all  parameter  estimates  were  obtained  as  expected.  In  this  study, 
parameter estimations were re-made by using the proposed variable selection methods. The results 
of these estimates are given in Table 16. From this table we can see that if the autoregressive error 
term  assumption  is  not  considered  although  it  exists,  the  expected  sign  of  the  LY  variable  is 
appeared in opposite way. From this result we can understand that the using of autoregressive error 
term assumption is crucial for this type of data. By using AR(4) assumption and t distributed error 
term the performance of the variable selection methods are given in Table 17. According to this 
table OLS and ridge with AR(4) assumption select all the variables significant for the model. On 
the other hand, LASSO, SCAD and the bridge methods are finding the LY insignificant. When we 
model the data by using t distribution with 3 degrees of freedom and 4th degree of autoregression, 
t-ridge, t-LASSO and t-SCAD methods select LY and LPRICE variables. However, t-bridge and 
t-elastic-net methods finds only LPRICE variable significant. From these results we can say that 
only the electricity kilowatt price is affected the amount of electricity consumption in the houses.  

24 

 
 
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Table 16. Parameter estimation results for Electric and Gas Company data. 

OLS 
  -0.00234 
-0.01856 
0.06365 
0.08564 
- 

OLS(AR) 
0.18625 
-0.09354 
0.00029 
0.00022 
4 

LY 
LPRICE 
CDD 
HDD 
AR order 

𝜷𝜷�𝟏𝟏
𝛽𝛽̂2
𝛽𝛽̂3
𝛽𝛽̂4

Table 17. Selected variables for Electric and Gas Company data. 

Method 

OLS 
ridge 
LASSO 
SCAD 
bridge 
elastic-net 
-ridge 
-LASSO 
-SCAD 
-bridge 
-elastic-net 

𝑡𝑡3
𝑡𝑡3
𝑡𝑡3
𝑡𝑡3
𝑡𝑡3

(𝛾𝛾 = 0.7)

(𝛾𝛾 = 0.7)

Selected variables 
(1,2,3,4) 
(1,2,3,4) 
(2,3,4) 
(2,3,4) 
(2,3,4) 
(2,3,4) 
(2,3,4) 
(1,2) 
(1,2) 
(2) 
(2) 

6. Conclusion 

We  have  proposed  the  t  distributed  variable  selection  methods  with  autoregressive  error  term 
regression models to improve the resistance of the mentioned methods against the outliers. The 
variable selection methods; LASSO, SCAD, bridge and elastic-net based on t distributed are robust 
alternatives to the normal ones. We have provided an ECM algorithm to compute the proposed 
methods and performed a simulation study and a real data example to show the performance of the 
proposed  methods.  The  simulation  study  shows  that  t  distributed  methods  have  superior 
performance  under  the  existence  of  the  outliers  among  the  other  methods  in  terms  of  both 
parameter estimation and variable selection. In terms of the determining the correct AR order for 
the  error  model,  LASSO,  SCAD,  bridge  and  the  elastic-net  methods  have  nearly  the  same 
performance. On the other hand, if the degree of autoregressive order is getting higher, than the 
bridge and the elastic-net methods perform better than the others. Using t distribution as an error 
term assumption can be extended to the case of heavy tailed skew distributions. This problem will 
be a further study.  

Acknowledgments 

This study is a part  of PhD dissertation in Ankara University, Graduate School  of Natural  and 
Applied Sciences. 

25 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
References 

Alpuim, T. and El-Shaarawi, A., 2008. On the efficiency of regression analysis with   AR(p) errors. 

Journal of Applied Statistics, 35:7, 717-737.  

Arslan, O. 2016 Penalized MM regression estimation with Lγ penalty: a robust version of bridge 

regression, Statistics, 50:6. 

Beach, C.M. and Mackinnon, J. G., 1978. A Maximum Likelihood Procedure for Regression with 

Autocorrelated Errors. Econometrica, vol. 46, no: 1,  pp. 51- 58. 

Box, E. P. G and Jenkins, M. G., 1976. Time Series Analysis: Forecasting and Control. Holden-

Day: San Fransisco. 

Dempster, A.P., Laird, N.M. and Rubin, D.B. 1977. Maximum likelihood from incomplete data 

via the EM algorithm. Journal of the Royal Statistical Society, Series B, 39,1-38. 

Fun, J. and Li,  R. 2001 Variable Selection via nonconcave penalized likelihood and its oracle 

properties. Journal of American Statistical Assoc. 96. 1348-1360. 

Frank, I.E. and Friedman, J. (1993). A statistical view of some chemometrics regression 

tools. Thechnometrics, Vol. 35, pp. 109-148. 

Hoerl,  A.E.,  and  Kennard,  R.W.,  1970  ridge  regression:  Biased  estimation  for  nonorthagonal 

problems. Technometrics.  

Lange,  K.L.,  Little,  R.J.A.  and  J.M.G.  Taylor,  1989.    Robust  statistical  modeling  using  the  t-

distribution, J. Am. Stat. Assoc. 84 pp. 881–896. 

Liu, Y., Zhang, H.H., Park, C., and Ahn, J. 2007. Support vector machines with adaptive 

 Lq penalty. Computational Statistics & Data Analysis, Vol. 51, pp.6380-6394. 

Lucas, A. 1997. Robustness of the student t-based M-estimator. Communications in Statistics – 

Theory and Methods, 26(5), pp: 1165-1182. 

McLachilan,  G.  J.,  Krishnan,  T.,  (1997)  The  EM  Algorithm    and  Extensions,  Wiley  series  in 

probability and statistics, USA. 

Meng, X. L. and Rubin, D. B. (1993). Maximum likelihood estimation via the ECM algorithm: A 

general framework. Biometrika 80, 267-278 

Park, C., and Yoon, Y. J. 2011. bridge regression: Adaptivity and group selection. 

26 

 
 
Journal of Statistical Planning and Inference, Vol. 141, pp. 3506-3519. 

Tibshiran, R. (1996). Regression shrinkage and selection via the LASSO. Journal of the 

Royal Statistical Society B, Vol. 58, pp. 267-288. 

Tiku, M., Wong, W., & Bian, G., 2007, Estimating Parameters In Autoregressive Models In Non-

Normal  Situations:  Symmetric  Innovations  .  Communications  in  Statistics  -  Theory  and 

Methods, 28 (2), 315-341. 

Tuaç Y., Güney Y., Şenoğlu B. and Arslan O., 2018 Robust parameter estimation of regression 

model with AR(p) error terms, Communications in Statistics – Simulation and Computation, 

47:8, 2343-2359. 

Tuaç, Y., Güney, Y., & Arslan, O. (2020). Parameter estimation of regression model with AR (p) 

error terms based on skew distributions with EM algorithm. Soft Computing, 24(5), 3309-3330. 

Tuaç, Y. 2020, Robust Parameter Estımatıon and Model Selectıon in Autoregressıve Error Term 

Regressıon  Modelsi,  Ph.D  dissertion,  Ankara  University,  Graduate  School  of  Natural  and 

Applied Sciences, Ankara. 

Wang, H., Guodong L., and Chih-L,T., 2007. ‘Regression Coefficient and Autoregressive Order 

Shrinkage and Selection via the Lasso’. Journal of the Royal Statistical Society: Series B 

(Statistical Methodology) 69, no. 1  

Yoon, J.Y., Park, C. and Lee, T., 2012. Penalized Regression Models with Autoregressive Error 

Terms, Journal of Statistical Computation and Simulation, 83:9,1756-1772. 

Zou, H. and Hastie, T., 2005 Regularization and variable selection via the elastic net. J.R. Statist. 

Soc. B. 67, part 2, pp. 301-320. 

27 

 
 
