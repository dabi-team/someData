9
1
0
2

p
e
S
0
2

]
T
S
.
h
t
a
m

[

3
v
5
3
7
9
0
.
0
1
7
1
:
v
i
X
r
a

Estimating long memory in panel random-coeﬃcient AR(1) data

Remigijus Leipus1, Anne Philippe2, Vytaut˙e Pilipauskait˙e3, Donatas Surgailis1

September 23, 2019

1Vilnius University, Faculty of Mathematics and Informatics, Institute of Applied Mathematics, Lithuania
2Universit´e de Nantes, Laboratoire de Math´ematiques Jean Leray, France
3Aarhus University, Department of Mathematics, Denmark.

Abstract

We construct an asymptotically normal estimator (cid:101)βN for the tail index β of a distribution on (0, 1)
regularly varying at x = 1, when its N independent realizations are not directly observable. The estima-
tor (cid:101)βN is a version of the tail index estimator of Goldie and Smith (1987) based on suitably truncated
observations contaminated with arbitrarily dependent ‘noise’ which vanishes as N increases. We apply (cid:101)βN
to panel data comprising N random-coeﬃcient AR(1) series, each of length T , for estimation of the tail
index of the random coeﬃcient at the unit root, in which case the unobservable random coeﬃcients are
replaced by sample lag 1 autocorrelations of individual time series. Using asymptotic normality of (cid:101)βN , we
construct a statistical procedure to test if the panel random-coeﬃcient AR(1) data exhibit long memory.
A simulation study illustrates ﬁnite-sample performance of the introduced inference procedures.

Keywords: random-coeﬃcient autoregression; tail index estimator; measurement error; panel data; long

memory process.

2010 MSC: 62G32, 62M10.

1 Introduction

Dynamic panels (or longitudinal data) comprising observations taken at regular time intervals for the same
individuals such as households, ﬁrms, etc. in a large heterogeneous population, are often described by time
series models with random parameters (for reviews on dynamic panel data analysis, see Arellano (2003), Bal-
tagi (2015)). One of the simplest models for individual evolution is the random-coeﬃcient AR(1) (RCAR(1))
process

Xi(t) = aiXi(t − 1) + ζi(t),

t ∈ Z,

i = 1, 2, . . . ,

(1)

where the innovations ζi(t), t ∈ Z, are independent identically distributed (i.i.d.) random variables (r.v.s)
with Eζi(t) = 0, Eζ 2
i (t) < ∞ and the autoregressive coeﬃcient ai ∈ (0, 1) is a r.v., independent of {ζi(t), t ∈
Z}.
It is assumed that the random coeﬃcients ai, i = 1, 2, . . . , are i.i.d., while the innovation sequences
{ζi(t), t ∈ Z} can be either independent or dependent across i, by inclusion of a common ‘shock’ to each unit;
see Assumptions (A1)–(A4) below. If the distribution of ai is suﬃciently ‘dense’ near unity, then statistical
properties of the individual evolution in (1) and the corresponding panel can diﬀer greatly from those in the
case of ﬁxed a ∈ (0, 1). To be more speciﬁc, assume that the AR coeﬃcient ai has a density function g(x),

1

 
 
 
 
 
 
x ∈ (0, 1), satisfying

for some β > 1 and g1 > 0. Then a stationary solution of RCAR(1) equation (1) has the following autoco-
variance function

g(x) ∼ g1(1 − x)β−1,

x → 1−,

(2)

EXi(0)Xi(t) = Eζ 2

i (0)E a|t|
i
1 − a2
i
and exhibits long memory in the sense that (cid:80)
memory property applies to the contemporaneous aggregate

g1
2

∼

Γ(β − 1)Eζ 2

i (0)t−(β−1),

t → ∞,

(3)

t∈Z | Cov(Xi(0), Xi(t))| = ∞ for β ∈ (1, 2]. The same long

¯XN (t) := N −1/2

N
(cid:88)

i=1

Xi(t),

t ∈ Z,

(4)

(2014) and other works, see Leipus et al.

(2007), Oppenheim and Viano (2004), Puplinskait˙e and Surgailis

of N independent individual evolutions in (1) and its Gaussian limit arising as N → ∞. For the beta
distributed squared AR coeﬃcient a2
i , these facts were ﬁrst uncovered by Granger (1980) and later extended
to more general distributions and/or RCAR equations in Gon¸calves and Gouri´eroux (1988), Zaﬀaroni
(2004), Celov et al.
(2010), Philippe
(2014) for review. Assumption (2) and the parameter β
et al.
play a crucial role for statistical (dependence) properties of the panel {Xi(t), t = 1, . . . , T, i = 1, . . . , N } as
N and T increase, possibly at diﬀerent rates. Particularly, Pilipauskait˙e and Surgailis (2014) proved that
for β ∈ (1, 2) the distribution of the normalized sample mean (cid:80)N
t=1 Xi(t) is asymptotically normal if
i=1
N/T β → ∞ and β-stable if N/T β → 0 (in the ‘intermediate’ case N/T β → c ∈ (0, ∞) this limit distribution
is more complicated and given by an integral with respect to a certain Poisson random measure). In the case
of common innovations ({ζi(t), t ∈ Z} ≡ {ζ(t), t ∈ Z}) the limit stationary aggregated process exists under
a diﬀerent normalization (N −1 instead of N −1/2 in (4)) and is written as a moving-average in the above
innovations with deterministic coeﬃcients Eaj
1, j ≥ 0, which decay as Γ(β)j−β with j → ∞ and exhibit long
memory for β ∈ (1/2, 1); see Zaﬀaroni
(2009). The trichotomy of the
limit distribution of the sample mean for a panel comprising RCAR(1) series driven by common innovations
is discussed in Pilipauskait˙e and Surgailis (2015).

(2004), Puplinskait˙e and Surgailis

(cid:80)T

(2006), Celov et al.

In the above context, a natural statistical problem concerns inference about the distribution of the random
AR coeﬃcient ai, e.g., its cumulative distribution function (c.d.f.) G or the parameter β in (2). Leipus et
al.
(2010) estimated the density g using sample autocovariances of the limit aggre-
gated process. For estimating parameters of G, Robinson (1978) used the method of moments. He proved
asymptotic normality of the estimators for moments of G based on the panel RCAR(1) data as N → ∞
for ﬁxed T , under the condition E(1 − a2
i )−2 < ∞ which does not allow for long memory in {Xi(t), t ∈ Z}.
For parameters of the beta distribution, Beran et al. (2010) discussed maximum likelihood estimation based
on (truncated) sample lag 1 autocorrelations computed from {Xi(1), . . . , Xi(T )}, i = 1, . . . , N , and proved
consistency and asymptotic normality of the introduced estimator as N, T → ∞. In nonparametric context,
(2016) studied the empirical c.d.f. of ai based on sample lag 1 autocorrelations similarly to
Leipus et al.
(2010), and derived its asymptotic properties as N, T → ∞, including those of a kernel den-
Beran et al.
sity estimator. Moreover, Leipus et al. (2016) proposed another estimator of moments of G and proved its
asymptotic normality as N, T → ∞. Except for parametric situations, the afore mentioned results do not
allow for inferences about the tail parameter β in (2) and testing for the presence or absence of long memory
in panel RCAR(1) data.

2

The present paper discusses in semiparametric context, the estimation of β in (2) from RCAR(1) panel
{Xi(t), t = 1, . . . , T, i = 1, . . . , N } with ﬁnite variance EX 2
i (t) < ∞. We use the fact that (2) implies
P(1/(1 − ai) > y) ∼ (g1/β)y−β, y → ∞, i.e. r.v. 1/(1 − ai) follow a heavy-tailed distribution with index β > 1.
Thus, if ai, i = 1, . . . , N , were observed, β could be estimated by a number of tail index estimators, including
the Goldie and Smith estimator in (9) below. Given panel data, the unobservable ai can be estimated by
sample lag 1 autocorrelation (cid:98)ai computed from {Xi(1), . . . , Xi(T )} for each i = 1, . . . , N . This leads to the
general estimation problem of β for ‘noisy’ observations

(cid:98)ai = ai + (cid:98)ρi,

i = 1, . . . , N,

(5)

where the ‘noise’, or measurement error (cid:98)ρi = (cid:98)ai − ai is of unspeciﬁed nature and vanishes with N → ∞.

Related statistical problems where observations contain measurement error were discussed in several papers.
Resnick and St˘aric˘a (1997), Ling and Peng (2004) considered Hill estimation of the tail parameter from
residuals of ARMA series. Kim and Kokoszka (2019a, 2019b) discussed asymptotic properties and ﬁnite
sample performance of Hill’s estimator for observations contaminated with i.i.d. ‘noise’. The last paper
contains further references on inference problems with measurement error.

A major distinction between the above mentioned works and our study is that we estimate the tail behavior
of G at a ﬁnite point x = 1 and therefore the measurement error should vanish with N which is not required
in Kim and Kokoszka (2019a, 2019b) dealing with estimation of the tail index at inﬁnity. On the other
hand, except for the ‘smallness condition’ in (13)–(14), no other (dependence or independence) conditions
on the ‘noise’ in (5) are assumed, in contrast to Kim and Kokoszka (2019a, 2019b), where the measurement
errors are i.i.d. and independent of the ‘true’ observations. The proposed estimator (cid:101)βN in (10) is a ‘noisy’
version of the Goldie and Smith estimator, applied to observations in (5) truncated at a level close to 1.
The main result of our paper is Theorem 2 giving suﬃcient conditions for asymptotic normality of the
constructed estimator (cid:101)βN . These conditions involve β and other asymptotic parameters of G at x = 1 and
the above-mentioned ‘smallness’ condition restricting the choice of the threshold parameter δ = δN → 0
in (cid:101)βN . Theorem 2 is applied to the RCAR(1) panel data, resulting in an asymptotically normal estimator
of β, where the ‘smallness condition’ on the ‘noise’ is veriﬁed provided T = TN grows fast enough with N
(Corollary 4). Based on the above asymptotic result, we construct a statistical procedure to test the presence
of long memory in the panel, more precisely, the null hypothesis H0 : β ≥ 2 vs. the long memory alternative
H1 : β ∈ (1, 2).

The paper is organized as follows. Section 2 contains the deﬁnition of the estimator (cid:101)βN and the main
Theorem 2 about its asymptotic normality for ‘noisy’ observations. Section 3 provides the assumptions
on the RCAR(1) panel model, together with application of Theorem 2 based on the panel data and some
consequences. In Section 4 a simulation study illustrates ﬁnite-sample properties of the introduced estimator
and the testing procedure. Proofs can be found in Section 5.

In what follows, C stands for a positive constant whose precise value is unimportant and which may change
from line to line. We write →p, →d for the convergence in probability and distribution respectively, whereas
→D[0,1] denotes the weak convergence in the space D[0, 1] with the uniform metric. Notation N (µ, σ2) is used
for the normal distribution with mean µ and variance σ2.

3

2 Estimation of the tail parameter from ‘noisy’ observations

In this section we introduce an estimator of the tail parameter β in (2) based on ‘noisy’ observations in
(5), where ai ∈ (0, 1) are i.i.d. satisfying (2), and (cid:98)ρi = (cid:98)ρi,N are measurement errors (i.e., arbitrary random
variables) which vanish with N → ∞ at a certain rate, uniformly in i = 1, . . . , N .

To derive asymptotic results about this estimator, condition (2) is strengthened as follows.

(G) ai ∈ (0, 1), i = 1, 2, . . ., are independent r.v.s with common c.d.f. G(x) := P(ai ≤ x), x ∈ [0, 1]. There
exists (cid:15) ∈ (0, 1) such that G is continuously diﬀerentiable on (1 − (cid:15), 1) with derivative satisfying

g(x) = κβ(1 − x)β−1(1 + O((1 − x)ν)),

x → 1−,

for some β > 1, ν > 0 and κ > 0.

Assumption (G) implies that the tail of the c.d.f. of Yi := 1/(1 − ai) satisﬁes

P(Yi > y) = κy−β(1 + O(y−ν)),

y → ∞.

(6)

(7)

For independent observations Y1, . . . , YN with common c.d.f. satisfying (7), Goldie and Smith (1987)

introduced the following estimator of the tail index β:

βN :=

(cid:80)N

i=1 1(Yi ≥ v)
i=1 1(Yi ≥ v) ln(Yi/v)

(cid:80)N

,

(8)

and proved asymptotic normality of this estimator provided the threshold level v = vN tends to inﬁnity at
an appropriate rate as N → ∞.

For independent realizations a1, . . . , aN under assumption (G), we rewrite the tail index estimator in (8)

as

βN =

where δ := 1/v is a threshold close to 0.

(cid:80)N

i=1 1(ai > 1 − δ)
i=1 1(ai > 1 − δ) ln(δ/(1 − ai))

(cid:80)N

,

(9)

Theorem 1. Assume (G). If δ = δN → 0 and N δβ → ∞ and N δβ+2ν → 0 as N → ∞, then

√

N δβ(βN − β) →d N (0, β2/κ).

Theorem 1 is due to Theorem 4.3.2 in Goldie and Smith (1987). The proof in Goldie and Smith (1987) uses
Lyapunov’s CLT conditionally on the number of exceedances over a threshold. Further suﬃcient conditions
for asymptotic normality of βN were obtained in Novak and Utev (1990). In Section 5 we give an alternative
proof of Theorem 1 based on the tail empirical process. Our proof has the advantage that it can be more
easily adapted to prove asymptotic normality of the ‘noisy’ modiﬁcation of (9) deﬁned as

(cid:101)βN :=

(cid:80)N

i=1 1((cid:101)ai > 1 − δ)
i=1 1((cid:101)ai > 1 − δ) ln(δ/(1 − (cid:101)ai))

(cid:80)N

where δ > 0 is a chosen small threshold and for some r > 1, each

(cid:101)ai := min{(cid:98)ai, 1 − δr}

4

,

(10)

(11)

is the (cid:98)ai of (5) truncated at level 1 − δr much closer to 1 than 1 − δ in (10). An obvious reason for the above
truncation is that in general, ‘noisy’ observations in (5) need not belong to the interval (0, 1) and may exceed
1 in which case the r.h.s. of (10) with (cid:98)ai instead of (cid:101)ai is undeﬁned. Even if (cid:98)ai < 1 as in the case of the AR(1)
estimates in (17), the truncation in (11) seem to be necessary due to the proof of Theorem 2. We note a
similar truncation of (cid:98)ai for technical reasons is used in the parametric context in Beran et al. (2010). On
the other hand, our simulations show that when r is large enough, this truncation has no eﬀect in practice.

Theorem 2. Assume (G). As N → ∞, let δ = δN → 0 so that

N δβ → ∞ and N δβ+2 min{ν,(r−1)β} → 0.

In addition, let

max
1≤i≤N

P(|(cid:98)ρi| > ε) ≤

χ
εp + χ(cid:48),

∀ε ∈ (0, 1),

where χ = χN , χ(cid:48) = χ(cid:48)

N → 0 satisfy

for some p ≥ 1. Then

√

N δβ max

(cid:110) χ(cid:48)
δβ ,

(cid:16) χ

(cid:17)1/(p+1)(cid:111)

δp+β

ln δ → 0

√

N δβ( (cid:101)βN − β) →d N (0, β2/κ).

(12)

(13)

(14)

(15)

3 Estimation of the tail parameter for RCAR(1) panel

:= {Xi(t), t ∈ Z}, i = 1, 2, . . . , be stationary random-coeﬃcient AR(1) processes in (1), where

Let Xi
innovations admit the following decomposition:

ζi(t) = biη(t) + ciξi(t),

t ∈ Z,

i = 1, 2, . . .

(16)

Let the following assumptions hold:

(A1) η(t), t ∈ Z, are i.i.d. with Eη(t) = 0, Eη2(t) = 1, E|η(t)|2p < ∞ for some p > 1.

(A2) ξi(t), t ∈ Z, i = 1, 2, . . ., are i.i.d. with Eξi(t) = 0, Eξ2
(A1).

i (t) = 1, E|ξi(t)|2p < ∞ for the same p > 1 as in

(A3) (bi, ci), i = 1, 2, . . . are i.i.d. random vectors with possibly dependent components bi ≥ 0, ci ≥ 0
satisfying P(bi + ci = 0) = 0 and E(b2

i + c2

i ) < ∞.

(A4) {η(t), t ∈ Z}, {ξi(t), t ∈ Z}, ai and (bi, ci) are mutually independent for each i = 1, 2, . . .

Assumptions (A1)–(A4) about the innovations are very general and allow a uniform treatment of common
shock (case (bi, ci) = (1, 0)) and idiosyncratic shock (case (bi, ci) = (0, 1)) situations. Similar assumptions
about the innovations are made in Leipus et al. (2016). Under assumptions (A1)–(A4) and (G), there exists
a unique strictly stationary solution of (1) given by

Xi(t) =

(cid:88)

s≤t

at−s
i

ζi(s),

t ∈ Z,

5

with EXi(t) = 0 and EX 2

i (t) = E(b2

i + c2

i )E(1 − a2

i )−1 < ∞, see Leipus et al. (2016).

From the panel RCAR(1) data {Xi(t), t = 1, . . . , T, i = 1, . . . , N } we compute sample lag 1 autocorrelation

coeﬃcients

(cid:98)ai :=

(cid:80)T −1

t=1 (Xi(t) − X i)(Xi(t + 1) − X i)
t=1(Xi(t) − X i)2

(cid:80)T

,

(17)

where X i := T −1 (cid:80)T
t=1 Xi(t) is the sample mean, i = 1, . . . , N . By the Cauchy-Schwarz inequality, the
estimator (cid:98)ai in (17) does not exceed 1 in absolute value a.s. Moreover, (cid:98)ai is invariant under the shift and
scale transformations of the RCAR(1) process in (1), i.e., we can replace Xi by {σiXi(t) + µi, t ∈ Z} with
some (unknown) µi ∈ R and σi > 0 for every i = 1, 2, . . ..

To estimate the tail parameter β from ‘noisy’ observations (cid:98)ai, i = 1, . . . , N , in (17) we use the estimator
(cid:101)βN in (10). The crucial ‘smallness condition’ (13) on the ‘noise’ (cid:98)ρi = (cid:98)ai − ai is a consequence of the following
result.

Proposition 3 (Leipus et al. (2016)). Assume (G) and (A1)–(A4). Then for all ε ∈ (0, 1) and T ≥ 1, it
holds

P(|(cid:98)a1 − a1| > ε) ≤ C(T − min{p−1,p/2}ε−p + T −1)

with C > 0 independent of ε, T .

The application of Theorem 2 leads to the following corollary.

Corollary 4. Assume (G) and (A1)–(A4). As N → ∞, let δ = δN → 0 so that

N δβ → ∞ and N δβ+2 min{ν,(r−1)β} → 0,

in addition, let T = TN → ∞ so that

√

N δβ max

√

N δβγ ln δ → 0

if 1 < p ≤ 2,

(cid:111)

(cid:110) 1
T δβ , γ

ln δ → 0

if 2 < p < ∞,

where

Then

γ :=

1
(T min{p−1,p/2}δp+β)1/(p+1)

→ 0.

√

N δβ( (cid:101)βN − β) →d N (0, β2/κ).

(18)

(19)

(20)

(21)

(22)

Remark 1. Condition (18) restricts the choice of δ and reduces to that of Theorem 1 with r increasing. In
particular, if δ = const N −b for some b > 0 then condition (18) for r ≥ 2, ν = 1 < β requires

1
β + 2

< b <

1
β

.

(23)

In view of (22) it makes sense to choose b as large as possible in order to guarantee the fastest convergence
rate of the estimator of β. Assume p > 2 in (A1), (A2). If δ = const N −b and T = N a for some b > 0
satisfying (23) and a > 0, then condition (20) is equivalent to

a > max

(cid:110) 1 + bβ
2

,

1 + bβ
p

+ (2 − β)b + 1

(cid:111)
,

6

which becomes less restrictive with p increasing and in the limit p = ∞ becomes

a > max

(cid:110) 1 + bβ
2

, (2 − β)b + 1

(cid:111)
.

(24)

Since for β ∈ (1, 2), the lower bound in (24) is 1 + (2 − β)b > 4/(β + 2) > 1, we conclude that T should grow
much faster than N . In general, our results apply to suﬃciently long panels.

Similarly as in the i.i.d. case (see Goldie and Smith (1987)), the normalization in (22) can be replaced by
a random quantity expressed in terms of (cid:101)ai, i = 1, . . . , N , alone. That is an actual number of observations
usable for inference.

Corollary 5. Set (cid:101)KN := (cid:80)N

i=1 1((cid:101)ai > 1 − δ). Under the assumptions of Corollary 4,

(cid:113)

(cid:101)KN ( (cid:101)βN − β) →d N (0, β2).

(25)

The CLTs in (22) and (25) provide not only consistency of the estimator but also asymptotic conﬁdence
intervals for the parameter β. The last result can be also used for testing of long memory in independent
RCAR(1) series which occurs if β ∈ (1, 2). Note that β = 2 appears as the boundary between long and short
memory. Indeed, in this case the autocovariance function of RCAR(1) is not absolutely summable, but the
iterated limit of the sample mean of the panel data follows a normal distribution as for β > 2 (see Ned´enyi
and Pap (2016), Pilipauskait˙e and Surgailis (2014)). Since it is more important to control the risk of false
acceptance of long memory, we choose the null hypothesis H0 : β ≥ 2 vs. the alternative H1 : β < 2. We use
the following test statistic

(cid:113)

(cid:101)KN ( (cid:101)βN − 2)/ (cid:101)βN .

(26)

According to Corollary 5, we have

(cid:101)ZN :=

(cid:101)ZN →d


N (0, 1)

+∞

−∞

if β = 2,

if β > 2,

if β < 2.

Fix ω ∈ (0, 1) and denote by z(ω) the ω-quantile of the standard normal distribution. The rejection region
{ (cid:101)ZN < z(ω)} has asymptotic level ω for testing the null hypothesis H0 : β ≥ 2, and is consistent against the
alternative H1 : β < 2.

4 Simulation study

We examine ﬁnite sample performance of the estimator (cid:101)βN in (10) and the testing procedure (cid:101)ZN < z(ω)
for H0 : β ≥ 2 at signiﬁcance level ω. We compare them with the estimator βN in (9) and the test
i=1 1(ai > 1 − δ), both based on i.i.d. (unobservable)
ZN :=
AR coeﬃcients a1, . . . , aN .

KN (βN − 2)/βN < z(ω), where KN := (cid:80)N

√

We consider a panel {Xi(t), t = 1, . . . , T, i = 1, . . . , N }, which comprises N independent RCAR(1) series
of length T . Each of them is generated from i.i.d. standard normal innovations {ζi(t)} ≡ {ξi(t)} in (16) with
AR coeﬃcient ai independently drawn from the beta-type density

g(x) =

2
B(α, β)

x2α−1(1 − x2)β−1,

x ∈ (0, 1),

(27)

7

with parameters α > 0, β > 1, where B(α, β) = Γ(α)Γ(β)/Γ(α+β) denotes the beta function. In this case, the
i is beta distributed with parameters (α, β). Note (27) satisﬁes (6) with κβ = 2β/ B(α, β)
squared coeﬃcient a2
and ν = 1 if 4α + β (cid:54)= 3. Then RCAR(1) process admits explicit (unconditional) autocovariance function

EXi(0)Xi(t) = E a|t|
i
1 − a2
i

=

B(α + |t|/2, β − 1)
B(α, β)

∼

κΓ(β)
2

t−(β−1),

t → ∞,

(28)

which follows by Γ(t)/Γ(t + c) ∼ t−c, t → ∞. The (unconditional) spectral density f (λ), λ ∈ [−π, π], of the
RCAR(1) process satisﬁes

f (λ) =

1
2π

E|1 − ae−ıλ|−2 ∼ κf

β > 2,


1,

ln(1/λ), β = 2,

λ−(2−β), β < 2,

λ → 0+,

(29)

where κf = (2π)−1E(1 − a)−2 (β > 2) and κf = κ(2π)−1 (β = 2), κf = κβ(2π)−1 (cid:82) ∞
0 yβ−1(1 + y2)−1y.
(1 < β < 2) (see Leipus et al.
(2014)). From (28), (29) we see that (unconditionally) Xi behaves as I(0)
process for β > 2 and as I(d) process for β ∈ (1, 2) with fractional integration parameter d = 1−β/2 ∈ (0, 1/2).
Particularly, β = 1.5 corresponds to d = 0.25 (the middle point on the interval (0, 1/2)), whereas β = 1.75 to
d = 0.125. Increasing parameter α ‘pushes’ the distribution of the AR coeﬃcient towards x = 1, see Figure
1 [left], and aﬀects the asymptotic constants of g(x) as x → 1−. A somewhat unexpected feature of this
model is a considerable amount of ‘spurious’ long memory for β > 2. Figure 1 [right] shows the graph of the
spectral density in (29) which is bounded though sharply increases at the origin for β = 2.5, α ≥ 1.5. One
may expect that most time series tests applied to a (Gaussian) process with a spectral density as the one in
Figure 1 [right] for β = α = 2.5 will incorrectly reject the short memory hypothesis in favour of long memory.
See Remark 2.

Figure 1: [left] Probability density g(x), x ∈ (0, 1), in (27) for β = 2.5. [right] Spectral density f (λ), λ ∈ [0, π],
in (29) for the same value of β.

Let us turn to the description of our simulation procedure. We simulate 5000 panels for each conﬁguration

of N , T , α and β, where

8

• (N, T ) = (750, 1000), (750, 2000),

• β = 1.5, 1.75, 2, 2.25, 2.5,

• α = 0.75, 1.5, 2.5.

As usual in tail-index estimation, the most diﬃcult and delicate task is choosing the threshold. We note that
conditions in Theorem 1 and Corollary 4 hold asymptotically and allow for diﬀerent choice of δ; moreover,
they depend on (unknown) β and the second-order parameter ν. Roughly speaking, larger δ increases the
number of the usable observations (upper order statistics) in (10) and (9), hence makes standard deviation
of the estimator smaller, but at the same time increases bias since the density g(x) in (2) is more likely to
deviate from its asymptotic form on a longer interval (1 − δ, 1). In the i.i.d. case or βN , the ‘optimal’ choice
of δ is given by

(cid:16) β(β + ν)2
2τ 2ν3κN
see equation (4.3.8) in Goldie and Smith (1987), which minimizes the asymptotic mean squared error of βN
provided the distribution of ai satisﬁes a generally stronger version of the second-order condition in (6):

(cid:17)1/(β+2ν)

δ∗ :=

(30)

,

P(ai > 1 − x) = κxβ(1 + τ xν + o(xν)),

x → 0+,

(31)

for the same β > 1 and some parameters ν > 0, κ > 0, τ (cid:54)= 0. Then on average the computation of βN uses

N
(cid:88)

E

i=1

1(ai > 1 − δ∗) ∼

(cid:16) (1 − ρ)N −ρ
√

B

−2ρ

(cid:17)2/(1−2ρ)

=: k∗

(32)

upper order statistics of a1, . . . , aN , where the second-order parameters ρ := −ν/β < 0, B := (ν/β)κ−ν/βτ (cid:54)= 0
are more convenient to estimate, see e.g. Paulauskas and Vaiˇciulis (2017). Therefore, given the order statistics
a(1) ≤ · · · ≤ a(N ), we use (random) δ = 1 − a(N −(cid:98)k∗(cid:99)) as a substitute for δ∗. Furthermore, since δ∗ of (30)
yields asymptotic normality of βN in (9) with non-zero mean, we choose a smaller sample fraction (k∗)(cid:15) < k∗
with (cid:15) ∈ (0, 1) and the corresponding

δ = 1 − a(N −(cid:98)(k∗)(cid:15)(cid:99)),

(33)

for which the asymptotic normality of βN holds as in Theorem 1. In our simulations of βN in (9) we use δ
in (33) with several values of (cid:15) ∈ (0, 1) and k∗ is obtained by replacing ρ, B in (32) by their semiparametric
estimates, see Fraga Alves et al. (2003), Gomes and Martins (2002). We calculate the latter estimates from
a1, . . . , aN using the algorithm in Gomes et al. (2009). Because of the lack of the explicit formula minimizing
the mean squared error of (cid:101)βN , in our simulations of the latter estimator we use a similar threshold δ ∈ (0, 1),
viz.,

δ = 1 − (cid:98)a(N −(cid:98)((cid:98)k∗)(cid:15)(cid:99)),
where (cid:98)a(1) ≤ · · · ≤ (cid:98)a(N ) denote the order statistics calculated from the simulated RCAR(1) panel, and (cid:98)k∗ is
the analogue of k∗ computed from (cid:98)a1, . . . , (cid:98)aN . Moreover, for our simulations of (cid:101)βN we use r = 10 in (11),
though any large r could be chosen.

(34)

Table 1 illustrates the eﬀect of (cid:15) in (33), (34) on the performance of βN , (cid:101)βN , respectively, for (N, T ) =
(750, 1000). Choosing smaller (cid:15), the bias of the estimators decreases in most cases, whereas their standard
deviation increases. The choice (cid:15) = 0.9 seems to be near ‘optimal’ in the sense of RMSE.

9

β = 1.5

β = 2

β = 2.5

(cid:15)

α = 0.75

1.5

2.5

0.75

1.5

2.5

0.75

1.5

2.5

1
0.9
0.8
0.7

1
0.9
0.8
0.7

1
0.9
0.8
0.7

1
0.9
0.8
0.7

0.18
0.18
0.17
0.25

0.15
0.13
0.16
0.21

-0.07
-0.01
0.05
0.11

-0.12
-0.06
-0.03
0.00

0.12
0.15
0.23
0.35

0.18
0.16
0.18
0.24

-0.05
0.04
0.13
0.23

-0.14
-0.08
-0.04
0.00

0.11
0.18
0.30
0.47

0.19
0.16
0.19
0.24

-0.01
0.10
0.22
0.36

-0.15
-0.09
-0.04
0.00

RMSE of (cid:101)βN
0.24
0.21
0.24
0.33
RMSE of βN
0.29
0.25
0.25
0.31

0.24
0.20
0.25
0.36

0.31
0.26
0.27
0.33

0.22
0.19
0.21
0.29

0.26
0.21
0.22
0.28

Bias of (cid:101)βN
-0.20
-0.10
-0.01
0.07
Bias of βN
-0.26
-0.17
-0.11
-0.05

-0.20
-0.08
0.02
0.12

-0.28
-0.19
-0.12
-0.06

-0.19
-0.11
-0.04
0.02

-0.23
-0.15
-0.09
-0.05

0.36
0.29
0.29
0.36

0.38
0.31
0.31
0.36

-0.32
-0.21
-0.13
-0.05

-0.35
-0.24
-0.17
-0.10

0.40
0.33
0.33
0.40

0.43
0.36
0.35
0.41

-0.36
-0.25
-0.15
-0.06

-0.40
-0.29
-0.21
-0.13

0.42
0.34
0.33
0.41

0.47
0.39
0.37
0.42

-0.38
-0.25
-0.13
-0.03

-0.44
-0.32
-0.22
-0.14

Table 1: Performance of (cid:101)βN , βN for (N, T ) = (750, 1000), a2
with estimated parameters B, ρ and r = 10. The number of replications is 5000.

i ∼ Beta(α, β), using δ in (34), (33), respectively,

Table 2 presents the performance of (cid:101)βN and βN with (cid:15) = 0.9, for a wider choice of parameters α, β and two
values of T . We see that the sample RMSE of both statistics (cid:101)βN and βN are very similar almost uniformly
in α, β, T (the only exception seems the case α = 2.5, β = 1.25, T = 1000). Surprisingly, in most cases the
statistic (cid:101)βN for T = 1000 seems to be more accurate than the same statistic for T = 2000 and the ‘i.i.d.’
statistic βN . This unexpected eﬀect can be explained by a positive bias introduced by estimated ai for (cid:15) = 0.9
which partly compensates the negative bias of βN , see Table 2.

Next, we examine the performance of the test statistics (cid:101)ZN and ZN using δ in (34) and (33), respectively.
Tables 3, 4 reports rejection rates of H0 : β ≥ 2 in favour of H1 : β < 2 at level ω = 5% using (cid:101)ZN and ZN
for (N, T ) = (750, 2000) and diﬀerent values of α, β and (cid:15), r. The results are almost the same when using
(cid:101)ZN for r = 3 and r = 10. Table 3 shows that choosing (cid:15) = 0.7 for β ≥ 2.25 and all values of α, the incorrect
rejection rates of the null in favour of the long memory alternative are much smaller than 5%, despite the
spurious long memory in Figure 1 [right]. Choosing (cid:15) = 0.9, they increase a bit but are still smaller than 5%
using (cid:101)ZN , see Table 4. However, at the boundary β = 2 between short and long memory, the empirical size
of the tests is not well observed. The deviation from the nominal level is especially noticeable in the case
of the ‘i.i.d.’ statistic ZN . This size distortion may be explained by the fact that the tails of the empirical

10

distribution of ZN and (cid:101)ZN are not well-approximated by tails of the limiting normal distribution. More
extensive simulations of the performance of (cid:101)ZN and ZN for other choices of (cid:15), r, N , T are presented in the
arXiv version Leipus et al. (2018) of this paper.

Remark 2. In time series theory, several semi-parametric tests for long memory were developed, see Giraitis
et al. (2003), Gromykov et al. (2018), Lobato and Robinson (1998). Clearly, these tests cannot be applied
to individual RCAR(1) series, the latter being always short memory a.s., independently of the value of β and
the distribution of the AR coeﬃcient ai. However, in practice one can apply the above-mentioned tests to
the aggregated RCAR(1) series { ¯XN (1), . . . , ¯XN (T )} in (4) whose autocovariance decays as t−(β−1), t → ∞,
see (3). In Leipus et al.
(2018) we report a Monte Carlo analysis of the ﬁnite sample performance of the
V/S test (see Giraitis et al. (2003)) applied to the aggregated RCAR(1) series with short memory (β = 2.5)
for the same model as above. Since the V/S statistic is quite sensitive to the choice of the tuning parameter,
Leipus et al. (2018) derived its data-driven choice by expanding the HAC estimator as proved by Abadir et
al. (2009) and minimizing its mean squared error under the null hypothesis. The simulations in Leipus et
al. (2018) show that the V/S test is not valid, in the sense that its empirical size is not close to the nominal
level. The reason why the V/S test fails for our panel model may be due to the presence of the spurious long
memory (see Figure 1).

11

β = 1.25

β = 1.5

β = 1.75

α = 0.75

1.5

2.5

0.75

1.5

2.5

0.75

1.5

2.5

RMSE

0.11

0.10

0.17

0.12

0.27

0.15

0.18

0.12

0.15

0.14

0.18

0.14

0.15

0.16

0.16

0.17

0.17

0.17

0.10

0.12

0.13

0.13

0.16

0.16

0.17

0.20

0.21

Bias

0.04

0.00

0.12

0.04

0.23

0.09

-0.01

0.04

-0.04

-0.02

0.10

0.01

-0.05

-0.03

0.00

-0.08

-0.08

-0.07

-0.04

-0.05

-0.06

-0.06

-0.08

-0.09

-0.10

-0.12

-0.14

β = 2

β = 2.25

β = 2.5

α = 0.75

1.5

2.5

0.75

1.5

2.5

0.75

1.5

2.5

RMSE

0.19

0.20

0.21

0.22

0.20

0.23

0.23

0.24

0.26

0.28

0.26

0.29

0.29

0.30

0.33

0.34

0.34

0.36

0.21

0.25

0.26

0.26

0.30

0.32

0.31

0.36

0.39

Bias

-0.11

-0.10

-0.08

-0.16

-0.17

-0.17

-0.21

-0.25

-0.25

-0.12

-0.14

-0.14

-0.17

-0.20

-0.21

-0.23

-0.27

-0.28

-0.15

-0.17

-0.19

-0.19

-0.23

-0.25

-0.24

-0.29

-0.32

(cid:101)βN , T = 1000

(cid:101)βN , T = 2000

βN

(cid:101)βN , T = 1000

(cid:101)βN , T = 2000

βN

(cid:101)βN , T = 1000

(cid:101)βN , T = 2000

βN

(cid:101)βN , T = 1000

(cid:101)βN , T = 2000

βN

Table 2: Performance of (cid:101)βN , βN for (N, T ) = (750, 1000) and (N, T ) = (750, 2000), a2
i ∼ Beta(α, β), using δ
in (34), (33), respectively, with estimated parameters B, ρ and (cid:15) = 0.9, r = 10. The number of replications
is 5000.

12

β = 1.25

β = 1.5

β = 1.75

α = 0.75

1.5

2.5

0.75

1.5

2.5

0.75

1.5

2.5

(cid:101)ZN , r = 3

(cid:101)ZN , r = 10
ZN

93.5

76.4

56.2

67.1

50.7

37.6

29.1

23.2

18.6

93.5
97.0

76.4
94.0

56.2
93.5

67.1
76.8

50.7
69.6

37.6
68.7

29.2
36.7

23.2
35.7

18.6
35.8

β = 2

β = 2.25

β = 2.5

α = 0.75

1.5

2.5

0.75

1.5

2.5

0.75

1.5

2.5

(cid:101)ZN , r = 3

(cid:101)ZN , r = 10
ZN

7.8

7.9

6.1

8.0
10.8

7.9
11.6

6.1
12.3

0.8

0.9
1.7

1.5

1.5
2.6

1.8

1.8
3.0

0.1

0.1
0.2

0.2

0.2
0.2

0.4

0.4
0.6

Table 3: Rejection rates (in %) of H0 : β ≥ 2 at level ω = 5% with (cid:101)ZN , ZN for (N, T ) = (750, 2000),
a2
i ∼ Beta(α, β), using δ in (34), (33), respectively, with estimated parameters B, ρ and (cid:15) = 0.7. The number
of replications is 5000.

β = 1.25

β = 1.5

β = 1.75

α =

0.75

1.5

2.5

0.75

1.5

2.5

0.75

1.5

2.5

(cid:101)ZN , r = 3

(cid:101)ZN , r = 10
ZN

100.0

99.9

99.3

98.5

95.3

91.9

72.9

68.3

62.9

100.0
100.0

99.9
99.9

99.3
99.9

98.7
99.2

95.3
97.9

91.9
97.6

76.1
81.0

68.4
78.1

62.9
78.1

β = 2

β = 2.25

β = 2.5

α =

0.75

1.5

2.5

0.75

1.5

2.5

0.75

1.5

2.5

(cid:101)ZN , r = 3

(cid:101)ZN , r = 10
ZN

19.8

25.1

23.8

26.7
32.2

25.5
34.1

23.8
37.0

1.1

2.2
3.6

4.2

4.4
5.8

4.6

4.6
8.0

0.0

0.1
0.1

0.3

0.3
0.5

0.6

0.6
1.2

Table 4: Rejection rates (in %) of H0 : β ≥ 2 at level ω = 5% with (cid:101)ZN , ZN for (N, T ) = (750, 2000),
a2
i ∼ Beta(α, β), using δ in (34), (33), respectively, with estimated parameters B, ρ and (cid:15) = 0.9. The number
of replications is 5000.

13

5 Proofs

Notation. In what follows, let GN (x) := N −1 (cid:80)N
(cid:98)a1, . . . , (cid:98)aN are deﬁned by (5) and a1, . . . , aN are i.i.d. with G(x) := P(a1 ≤ x), x ∈ R.

i=1 1(ai ≤ x) and (cid:98)GN (x) := N −1 (cid:80)N

i=1 1((cid:98)ai ≤ x), where

Proof of Theorem 1. We rewrite the estimator in (9) as

βN =

1 − GN (1 − δ)
(cid:82) 1
1−δ ln(δ/(1 − x))G. N (x)

=

1 − GN (1 − δ)
(cid:82) 1
1−δ(1 − GN (x))

x.
1−x

=

1 − GN (1 − δ)
(cid:82) δ
0 (1 − GN (1 − x))

x.
x

.

Next, we decompose βN − β = D−1 (cid:80)4

i=1 Ii, where

(cid:90) δ

I1

:= β

(GN (1 − x) − G(1 − x))

0
(cid:90) δ

:= −β

0

I3

(1 − κxβ − G(1 − x))

and

x.
x
x.
x

,

,

I2 := −(GN (1 − δ) − G(1 − δ)),

(35)

I4 := 1 − κδβ − G(1 − δ)

D :=

(cid:90) δ

0

(1 − GN (1 − x))

x.
x

=

1
β

(κδβ − I1 − I3).

(36)

According to the assumptions (N δβ)1/2δν → 0 and (G), we get (N δ−β)1/2I4 → 0 and (N δ−β)1/2I3 → 0.

From the tail empirical process theory, see e.g. Theorem 1 in Einmahl (1990), (1.1)–(1.3) in Mason (1988),

we have that

(N δ−β)1/2(GN (1 − xδ) − G(1 − xδ)) →D[0,1] κ1/2B(xβ),

where {B(x), x ∈ [0, 1]} is a standard Brownian motion. Therefore, we can expect that

(N δ−β)1/2(I1 + I2) →d κ1/2(cid:16)

β

(cid:90) 1

0

B(xβ)

x.
x

(cid:17)

− B(1)

.

(37)

(38)

The main technical point to prove (38) is to justify the application of the invariance principle (37) to the
integral (N δ−β)1/2I1, which is not a continuous functional in the uniform topology on the whole space D[0, 1].
For ε > 0, we split I1 := β(I ε

ε ), where

0 + I 1

I ε
0 :=

(cid:90) ε

0

(GN (1 − δx) − G(1 − δx))

x.
x

,

I 1
ε :=

(cid:90) 1

(GN (1 − δx) − G(1 − δx))

x.
x

.

By (37), (N δ−β)1/2I 1
follows from

ε →d κ1/2 (cid:82) 1

ε B(xβ)

x.
x , where E| (cid:82) 1

ε B(xβ)

In the i.i.d. case E|I ε

0|2 = (cid:82) ε

0

lim
ε→0

lim sup
N →∞

E|(N δ−β)1/2I ε

0|2 = 0.

(cid:82) ε
0 Cov(GN (1 − δx), GN (1 − δy))

x. y.
xy , where

ε
x.
x − (cid:82) 1

0 B(xβ)

x.
x |2 → 0 as ε → 0. Hence, (38)

(39)

Cov(GN (x), GN (y)) = N −1G(x ∧ y)(1 − G(x ∨ y)) ≤ N −1(1 − G(x ∨ y)),

and

E|I ε

0|2 ≤

C
N

(cid:90) ε

0

x.
x

(cid:90) x

0

(1 − G(1 − δy))

y.
y

≤

C
N

(cid:90) ε

0

14

x.
x

(cid:90) x

0

(δy)β y.
y

=

C
N δ−β

(cid:90) ε

0

xβ−1x. =

Cεβ
N δ−β ,

(40)

proving (39) and hence (38) too.

Finally, we obtain δ−βD →p κ/β in view of (N δ−β)1/2(I1 + I3) = Op(1) and N δβ → ∞.
We conclude that

(N δβ)1/2(βN − β) →d

(cid:16)

β

β
κ1/2

(cid:90) 1

0

B(xβ)

x.
x

(cid:17)

− B(1)

=: W.

(41)

Clearly, W follows a normal distribution with zero mean and variance

EW 2 =

(cid:16)

2β2

β2
κ

(cid:90) 1

0

x.
x

(cid:90) x

0

yβ−1y. − 2β

(cid:90) 1

0

xβ−1x. + 1

(cid:17)

=

β2
κ

,

which agrees with the one in Goldie and Smith (1987). The proof is complete.

In the proof of Theorem 2 we will use the following proposition.

Proposition 6. Assume (G). As N → ∞, let δ = δN → 0 so that N δβ → ∞ and (13), (14) hold. Then

(N δ−β)1/2( (cid:98)GN (1 − δ) − GN (1 − δ)) = op(1),

(N δ−β)1/2

(cid:90) δ

δr

( (cid:98)GN (1 − x) − GN (1 − x))

x.
x

= op(1).

Proof. For x ∈ [1 − δ, 1], write

(cid:98)GN (x) − GN (x) =

1
N

N
(cid:88)

(1(ai + (cid:98)ρi ≤ x) − 1(ai ≤ x)) = D(cid:48)

N (x) − D(cid:48)(cid:48)

N (x),

i=1

where (cid:98)ρi := (cid:98)ai − ai, i = 1, . . . , N , and

D(cid:48)

N (x)

:=

D(cid:48)(cid:48)

N (x)

:=

1
N

1
N

N
(cid:88)

i=1
N
(cid:88)

i=1

1(x < ai ≤ x − (cid:98)ρi, (cid:98)ρi ≤ 0),

1(x − (cid:98)ρi < ai ≤ x, (cid:98)ρi > 0).

For all γ > 0,

where by (13)

and

0 ≤ D(cid:48)

N (x) ≤

1
N

N
(cid:88)

i=1

1(x < ai ≤ x + γδ) +

1
N

N
(cid:88)

i=1

1(|(cid:98)ρi| > γδ) =: I (cid:48)

N (x) + I (cid:48)(cid:48)
N ,

EI (cid:48)(cid:48)

N ≤ max
1≤i≤N

P(|(cid:98)ρi| > γδ) ≤

χ
(γδ)p + χ(cid:48)

EI (cid:48)

N (x) = P(x < a1 ≤ x + γδ) ≤ C

(1 − u)β−1u. ≤ Cγδβ

(cid:90) x+γδ

x
holds uniformly for all x ∈ [1 − δ, 1] according to (6). Choose

γ := (cid:0) χ
δp+β
then χ/(γδ)p ∼ γδβ and the r.h.s. of (44) does not exceed C(γδβ +χ(cid:48)) ≤ C max{γδβ, χ(cid:48)}. Under the conditions
(13), (14), from (44), (45) it follows that

(cid:1)1/(p+1),

(46)

(N δ−β)1/2

(cid:90) δ

δr

ED(cid:48)

N (1 − x)

x.
x

≤ C| ln δ|(N δ−β)1/2(cid:0)EI (cid:48)(cid:48)

N + sup
x∈[0,δ]

EI (cid:48)

N (1 − x)(cid:1) = o(1),

15

(42)

(43)

(44)

(45)

hence

by Markov’s inequality. Since

(N δ−β)1/2

(N δ−β)1/2

(cid:90) δ

δr

(cid:90) δ

δr

D(cid:48)

N (1 − x)

D(cid:48)(cid:48)

N (1 − x)

x.
x

x.
x

= op(1)

= op(1)

is analogous, this proves (43). The same proof works for the relation (42).

Proof of Theorem 2. Rewrite

(cid:101)βN =

1 − (cid:98)GN (1 − δ)
(cid:82) δ
δr (1 − (cid:98)GN (1 − x))

x.
x

.

Split (cid:101)βN − β = (cid:101)D−1((cid:80)4

i=1 Ri), where Ii, i = 1, . . . , 4, are deﬁned in (35) and

i=1 Ii + (cid:80)4
(cid:90) δ

δr
(cid:90) δr

R1

:= β

( (cid:98)GN (1 − x) − GN (1 − x))

R3

:= β

(G(1 − x) − GN (1 − x))

x.
x
x.
x

,

,

R2 := GN (1 − δ) − (cid:98)GN (1 − δ),

R4 := β

(cid:90) δr

0

(1 − G(1 − x))

x.
x

and

0

(cid:101)D :=

(cid:90) δ

δr

(1 − (cid:98)GN (1 − x))

x.
x

= D −

1
β

(R1 + R3 + R4)

with D given by (36). By Proposition 6, (N δ−β)1/2R2 = op(1) and (N δ−β)1/2R1 = op(1). In view of (40),
we have E|(N δ−β)1/2R3|2 ≤ Cδrβ = o(1) and so (N δ−β)1/2R3 = op(1). Finally, (N δ−β)1/2R4 = o(1) as
N δ(2r−1)β → 0.

Proof of Corollary 4. Let χ := T − min{p−1,p/2}, χ(cid:48) := χ1/ min{p−1,p/2} = T −1. Then (19)–(20) agree with (14)
and the result follows from Theorem 2.

Proof of Corollary 5. Let KN = (cid:80)N
∞, Markov’s inequality yields

i=1 1(ai > 1−δ). Since Var(KN ) ≤ N (1−G(1−δ)) and N (1−G(1−δ)) →

KN
N (1 − G(1 − δ))

→p 1,

consequently, (N δβ)−1KN →p κ. By Proposition 6, we have (N δβ)−1( (cid:101)KN − KN ) = op(1). We conclude that
(N δβ)−1 (cid:101)KN →p κ.

Acknowledgments

The authors are grateful to two anonymous referees for criticisms and helpful suggestions. We thank Marijus
Vaiˇciulis for helping us with the choice of the threshold in the simulation experiment. Vytaut˙e Pilipauskait˙e
acknowledges the ﬁnancial support from the project “Ambit ﬁelds: probabilistic properties and statistical
inference” funded by Villum Fonden.

Data availability statement

Data sharing is not applicable to this article as no new data were created or analysed in this study.

16

References

Abadir, K., Distaso, W. and Giraitis, L. 2009. Two estimators of the long-run variance: beyond short memory. Journal of

Econometrics 150, 56–70.

Arellano, M. 2003. Panel Data Econometrics. Oxford University Press.

Baltagi, B.H. 2015. The Oxford Handbook of Panel Data. Oxford Handbooks.

Beran, J., Sch¨utzner, M. and Ghosh, S. 2010. From short to long memory: Aggregation and estimation. Computational Statistics

and Data Analysis 54, 2432–2442.

Celov, D., Leipus, R. and Philippe, A. 2007. Time series aggregation, disaggregation and long memory. Lithuanian Mathematical

Journal 47, 379–393.

Celov, D., Leipus, R. and Philippe, A. 2010. Asymptotic normality of the mixture density estimator in a disaggregation scheme.

Journal of Nonparametric Statistics 22, 425–442.

Einmahl, J.H.J. 1990. The empirical distribution function as a tail estimator. Statistica Neerlandica 44, 79–82.

Fraga Alves, M.I., Gomes, M.I. and de Haan, L. 2003. A new class of semi-parametric estimators of the second order parameter.

Portugaliae Mathematica 60, 193–214.

Giraitis L., Kokoszka P., Leipus R. and Teyssi`ere G. 2003. Rescaled variance and related tests for long memory in volatility and

levels. Journal of Econometrics 112, 256–294.

Goldie, C.M. and Smith, R.L. 1987. Slow variation with remainder: theory and applications. The Quarterly Journal of Mathe-

matics 38, 45–71.

Gomes, M.I. and Martins, M.J. 2002. “Asymptotically unbiased” estimators of the tail index based on external estimation of the

second order parameter. Extremes 5, 5–31.

Gomes, M.I., Pestana, D. and Caeiro, F. 2009. A note on the asymptotic variance at optimal levels of a bias-corrected Hill

estimator. Statistics and Probability Letters 79, 295–303.

Gon¸calves, E. and Gouri´eroux, C. 1988. Aggr´egation de processus autoregressifs d’ordre 1. Annales d’Economie et de Statistique

12, 127–149.

Granger, C.W.J. 1980. Long memory relationship and the aggregation of dynamic models. Journal of Econometrics 14, 227–238.

Gromykov G., Ould Haye, M. and Philippe, A. 2018. A frequency-domain test for long range dependence. Statistical Inference

for Stochastic Processes 21, 513–526.

Kim, M. and Kokoszka, P. 2019a. Consistency of the Hill estimator for time series observed with measurement errors. Preprint.

Available at https://www.researchgate.net/publication/330352018.

Kim, M. and Kokoszka, P. 2019b. Asymptotic normality of the Hill estimator applied to data observed with measurement errors.

Preprint. Available at https://www.researchgate.net/publication/334119192.

Ling, S. and Peng, L. 2004. Hill’s estimator for the tail index of an ARMA model. Journal of Statistical Planning and Inference

123, 279–293.

Leipus, R., Oppenheim, G., Philippe, A. and Viano, M.-C. 2006. Orthogonal series density estimation in a disaggregation scheme.

Journal of Statistical Planning and Inference 136, 2547–2571.

Leipus, R., Philippe, A., Puplinskait˙e, D. and Surgailis, D. 2014. Aggregation and long memory: recent developments. Journal

of Indian Statistical Association 52, 71–101.

17

Leipus, R., Philippe, A., Pilipauskait˙e, V. and Surgailis, D. 2017. Nonparametric estimation of the distribution of the autore-

gressive coeﬃcient from panel random-coeﬃcient AR(1) data. Journal of Multivariate Analysis 153, 121–135.

Leipus, R., Philippe, A., Pilipauskait˙e, V. and Surgailis, D. 2018. Testing for long memory in panel random-coeﬃcient AR(1)

data. Preprint. Available at arXiv:1710.09735v2 [math.ST].

Lobato, I.N. and Robinson, P.M. 1998. A nonparametric test for I(0). The Review of Economic Studies 65(3), 475–495.

Mason, D.M. 1988. A strong invariance theorem for the tail empirical process Annales de l’Institut Henri Poincar´e 24, 491–506.

Ned´enyi, F. and Pap, G. 2016. Iterated scaling limits for aggregation of random coeﬃcient AR(1) and INAR(1) processes,

Statistics and Probability Letters 118, 16–23.

Novak, S.Y. and Utev, S. 1990. On the asymptotic distribution of the ratio of sums of random variables. Siberian Mathematical

Journal 31, 781–788.

Oppenheim, G. and Viano, M.-C. 2004. Aggregation of random parameters Ornstein-Uhlenbeck or AR processes: some conver-

gence results. Journal of Time Series Analysis 25, 335–350.

Paulauskas, V. and Vaiˇciulis, M. 2017. Comparison of the several parametrized estimators for the positive extreme value index.

Journal of Statistical Computation and Simulation 87, 1342–1362.

Philippe, A., Puplinskait˙e, D. and Surgailis, D. 2014. Contemporaneous aggregation of triangular array of random-coeﬃcient

AR(1) processes. Journal of Time Series Analysis 35, 16–39.

Pilipauskait˙e, V. and Surgailis, D. 2014. Joint temporal and contemporaneous aggregation of random-coeﬃcient AR(1) processes.

Stochastic Process and their Applications 124, 1011–1035.

Pilipauskait˙e, V. and Surgailis, D. 2015. Joint aggregation of random-coeﬃcient AR(1) processes with common innovations.

Statistics and Probability Letters 101, 73–82.

Puplinskait˙e, D. and Surgailis, D. 2009. Aggregation of random coeﬃcient AR(1) process with inﬁnite variance and common

innovations. Lithuanian Mathematical Journal 49, 446–463.

Puplinskait˙e, D. and Surgailis, D. 2010. Aggregation of random coeﬃcient AR(1) process with inﬁnite variance and idiosyncratic

innovations. Advances in Applied Probability 42, 509–527.

Resnick, S. and St˘aric˘a, C. 1997. Asymptotic behavior of Hill’s estimator for autoregressive data. Communications in Statistics.

Stochastic Models 13, 703–721.

Robinson, P.M. 1978. Statistical inference for a random coeﬃcient autoregressive model. Scandinavian Journal of Statistics 5,

163–168.

Zaﬀaroni, P. 2004. Contemporaneous aggregation of linear dynamic models in large economies. Journal of Econometrics 120,

75–102.

18

