1
2
0
2

t
c
O
6
2

]

M
M

.
s
c
[

1
v
8
3
5
3
1
.
0
1
1
2
:
v
i
X
r
a

SHECS: A Local Smart Hands-free Elderly Care
Support System on Smart AR Glasses with AI
Technology

Donghuo Zeng∗, Jianming Wu∗, Bo Yang, Tomohiro Obara, Akeri Okawa,
Nobuko Iino, Gen Hattori, Ryoichi Kawada, and Yasuhiro Takishima
KDDI Research, Inc., Saitama, Japan.
{do-zeng, ji-wu, bo-yang, to-obara, ak-ookawa,
no-iino, ge-hattori, ry-kawada, takisima}@kddi-research.jp

Abstract—Some elderly care homes attempt to remedy the
shortage of skilled caregivers and provide long-term care for the
elderly residents, by enhancing the management of the care sup-
port system with the aid of smart devices such as mobile phones
and tablets. Since mobile phones and tablets lack the ﬂexibility
required for laborious elderly care work, smart AR glasses have
already been considered. Although lightweight smart AR devices
with a transparent display are more convenient and responsive
in an elderly care workplace, fetching data from the server
through the Internet results in network congestion not to mention
the limited display area. To devise portable smart AR devices
that operate smoothly, we ﬁrst present a non-intrusive, privacy-
compliant, and no-keepalive-Internet required smart hands-free
elderly care support system that employs smart glasses with
facial recognition and text-to-speech synthesis technologies. Our
support system utilizes automatic lightweight facial recognition
to identify residents, and information about each resident in
question can be obtained hands-free link with a local database.
Moreover, a resident’s information can be displayed immediately
on just a portion of the AR smart glasses on the spot. Due to the
limited size of the display area, it cannot show all the necessary
information. We therefore exploit synthesized voice in the system
to read out the elderly care-related information in its entirety. By
using the support system, caregivers can gain an understanding
of each resident’s condition immediately, instead of having to
devote considerable time in advance in obtaining the complete
information of all elderly residents, especially of new residents.
Our experiments on this support system were conducted at an
elderly care home in Tokyo. Our lightweight facial recognition
model achieved high accuracy with fewer model parameters than
current state-of-the-art methods. The validation rate of our facial
recognition system in the practical use scenario was 99.3% or
higher with the false accept rate of 0.001, and caregivers rated
the acceptability at 3.6 (5 levels) or higher.

Index Terms—Elderly care support system, smart AR glasses,

AI technology

I. INTRODUCTION

To facilitate the caregivers of the elderly care homes to
provide the best care services to elderly residents, it is very
important to always keep track of the elderly residents’ in-
formation, such as the current health status of each resident
and what kind of care they need. Currently, this health status
is recorded manually by the caregivers. However, the elderly
care industry suffers from staff shortages as the mobility of

*Both authors contributed equally to this research

Fig. 1. The Mechanism of our ”Local Smart Hands-free Elderly Care Support
System”, where a rehabilitation room is located on the second ﬂoor.

staff is very high compared to other industries. In addition,
as the number of new elderly residents is tending to increase,
there are limitations to how well optimal care services can
be provided to individuals. For example, some new elderly
residents are not equipped with ID tags for personal identiﬁ-
cation, and caregivers therefore need to expend considerable
time acquiring the health and eating&drinking status of new
residents manually.

With the development of Artiﬁcial Intelligence (AI) tech-
nology, it is possible to reduce the caregiver workload and
provide better care to residents through the use of AI. Some
elderly care support systems with AI capabilities have been
developed by a number of research teams, such as developing
AI robots [1, 2] to facilitate the elderly care work.

The main challenge facing elderly care support systems is
to quickly ascertain the condition of the elderly in the elderly
care home and provide long-term care services smoothly by
using information and communications technology (ICT). For
example, the residents in our experimental elderly care home
are unwilling to carry any ID tags or devices, and worry about

 
 
 
 
 
 
the reliability of a robot, as they argue that this adversely
affects their living experience and quality of life. In this case,
it requires the caregivers to adopt and apply smart devices that
help to maintain a normal life for residents.

In the past, caregivers at the elderly care home at Zenk-
oukai 1, a enterprise that operates and manages elderly care
welfare facilities, could use smartphones and tablets to access
residents’ data on SCOP 2, which residents’ health data such
as the body temperature and blood pressure are centrally
managed. However, in the ﬁeld of elderly care, there are many
tasks that require the use of both hands, such as bathing.
Sometimes, there are cases where a rapid response is required,
such as when a resident has a fall. In these cases, having to
use the hands to operate a smartphone or tablet is a hindrance.
When caregivers are communicating with a resident face-
to-face and needs to ascertain the resident’s condition im-
mediately through the care support system, speed is of the
essence in improving care. However, in the extreme case, these
400 residents in one building fetch data from remote servers
synchronously can cause an increase in data transmission
levels, resulting in network delay and congestion. Moreover, to
make the device easier to use when the caregiver is in physical
contact with the elderly person’s body, we hope to eliminate
cables as these can get tangled, tripping over the cable may
result in damage, and it may make the device uncomfortable
to wear. In this case, we need to develop a hands-free care
support system by using AR smart glasses to recognize the
face of the resident, and which is also capable of accessing
the local server that manages the resident’s healthcare data,
and present this information on the device for caregivers.

In this paper, we introduce a SHECS system executed
on the smart glasses. After installing the system on smart
glasses,
the device conducts a two-stage face recognition
to achieve person identiﬁcation including face detection and
face recognition. Our face recognition system beneﬁts from a
lightweight network that saves computational resources. More-
over, the system is capable of correcting various orientations
such as face skewed to the side or presented front on to
improve the system’s performance. Once the ID of somebody
is obtained, the system will access the local server inside
the device (which is updated asynchronously from the SCOP
server) to acquire the resident’s relevant
information in a
short period of time. Beside, it was important to reduce the
load of the AI’s operation and the computational time. For
example, by using AI technology to automatically eliminate
useless information from data when extracting features from
mugshots, and by exploiting a fast matching algorithm. In the
end, our lightweight facial recognition model maintains high
accuracy with fewer model parameters than current state-of-
the-art methods, and our system was able to identify 10,000
mugshots in 1 second and 100 mugshots in 0.1 seconds on the
smart glasses.

Furthermore, the display screen on the smart glasses is too

1https://www.zenkoukai.jp/japanese/
2https://bi.biopapyrus.jp/db/scop.html

narrow to present the resident’s full information. Accordingly,
we exploit a Japanese text-to-speech technology to instantly
read the information in its entirety out using N2 Speech
Synthesis Software 3. The advantages of this technology are
that it employs a lightweight model and requires little memory.
Following the results obtained from our preliminary exper-
iments, we perform a real-world experiment at actual long-
term care facilities with accuracy and acceptability to both
caregivers and residents used as metrics. We achieved high
recognition accuracy regardless of the orientation of the face,
without increasing the amount of computation.

The remainder of this paper is structured as follows. Section
II introduces related work on smart care support systems
and technologies related to our system. Section III presents
the architecture of our model and Section IV reports the
experimental results and analysis. Finally, in Section VI, we
present our conclusions.

II. RELATED WORK

A. AI for Smart Care System

Now more than ever, a high-quality smart care system
depends on the computational power when interacting with
the vast, unstructured, and ever-changing clinical information
or resident condition. AI brings a promising direction for the
practice of care system because AI has stupendous potential
to develop productively care system and provide huge oppor-
tunities. The review paper on application scenarios [3] was
published in 2020, they selected 699 full papers and 6,818
other texts and found that there are few publications for elderly
care (nursing) homes and home-care related.

In the past, to improve the care support system in elderly
this study [4] proposed an elderly care support
facilities,
system that can be applied to information storage devices
to record information about all patients by implementing
a domain name system. [5] proposed a AR-based surgical
navigation system to reduce the risk and improve the chances
of successful surgery by using a transparent display worn on
the head. This study [6] presented online smart AR glasses
that can display patient information and record a video of the
patient, with the goal of improving patient care and reducing
healthcare or nursing costs. Another study [7] investigated
how health information system data can be matched with the
corresponding patient using smart glasses.

With the use of AI technology in the smart care systems,
more and more relevant works are emerging, [8] discusses the
methods of AI and give some examples of how AI can help
healthcare, especially nursing. In this paper [9], an AI system
was presented that helps blind people detect the location, gaze
direction, and identity of a nearby person. When wearing the
device, the user can receive the voice and visual information
of other people, which can help them communicate with other
people who are also healthy.

With the great success of AI technology like computer
vision [10–12], robots have also become very helpful assistants

3https://www.kddi-research.jp/english/products/n2.html

for long-term care [13]. For example, AI robots can reduce
the workload of caregivers and improve patient-caregiver in-
teraction. The RIBA robot [1, 2] has human-like arms and is
designed to complete heavy physical works and replace human
workers. For example, the RIBA robot can carry a patient from
a couch to a wheelchair and back again, and also work with
caregivers. To learn more about the robot’s potential as a real-
world care assistant, [14] launched a 10-week study was con-
ducted in a nursing home using the Pepper robotic platform,
with residents and caregivers among the participants. They
found out the facilitating and hindering factors of the robot
and were able to better understand the supportive physical
activation, cognitive training and social facilitation in human-
robot interaction with the elderly. HomeMate [15] is the next
generation of care robots for the elderly, they develop ﬁve
service scenarios by optimizing synergy with an ecosystem of
care robots for the elderly.

B. Facial Recognition and Speech Synthesis

1) Facial recognition: Conducting facial recognition re-
search to help the elderly is a long-standing research. Robot
companion and caregiver for elderly [16] is a taking hold idea,
face recognition is the main factor of such robots that can
recognise the elderly face then take next action. Nursebot is
another different kinds of robot that involves multi-disciplinary
technology, for example, the Pear Nursebot [17] is mobile
robotic assistant for elderly including recognition function to
identity the elderly people then telling them the routine ac-
tivities or guiding them through their environment. Healthcare
systems for the safety of elderly people at home [18] is to
solve the social problem in the growing population of elderly,
the face recognition and other technique here can help elderly
to track their behavior such as detecting falls of elderly. With
the development of face recognition technologies [19–22], it
is not limited in the above applications for elderly. Recent
years, face recognition can be applied in many ﬁelds. For
example, the work [23] developed a smart door lock system
with face recognition to help elderly open the door without any
additional device such as smartcard, keys or smartphone. Real-
world face recognition systems [24] can help elderly through
deploying IP cameras in the elderly care center in Singapore,
such system have 4 functions including the time of a resident
in toilet, the location of a resident, who left the emergency
room, and who left center. Long-Term Health Monitoring sys-
tem for elderly with the combination of facial recognition [25]
and non-intrusive, privacy-compliant wearable sensors is very
useful technology to track their activities.

2) Speech synthesis: Automatic personal synthetic voice
construction [26] can be achieved via two types of software:
InvTool and BCC. InvTool 4 enables the user to create his/her
own corpus of speech suitable for a speech synthesis appli-
cation. BCC is the other program. It can compile the speech
corpus recorded with InvTool into a set of ﬁles. The study
of turn-taking in conversations includes the issue of “additive

4http://www.asel.udel.edu/speech/InvTutor/index.htm

effect”.
[27] examined this effect by using the synthetic
voice to leverage the generated turn-taking cues and found
that in dialogues natural human speech could be replaced by
the synthetic voice.
[28] presented a voice-based synthetic
assistant to facilitate the training of medical ﬁrst-responders
to improve efﬁciency during an emergency. This research [29]
further exploited the different impacts of synthetic voice by
inviting sixty people to listen to ten other people in three
different modes: natural speech, and low or high intelligibility.
The results showed that different modes affect the quality
of synthetic voices. A new CNN-LSTM structure, called the
objective prediction model [30] for synthetic voice, was used
to evaluate the quality of synthetic voice in different situations.
They show that the reliability of deep learning-based natu-
ralness prediction can be improved by transfer learning from
speech quality prediction models that are trained on objective
POLQA scores.

Facial recognition technology is expected to be used in
elderly care homes. To our knowledge, there are no existing
AR glass-based systems for elderly care. On the other hand,
iFalcon Face Control5 proposed an smart AR glasses solution
for facial recognition, in which the face data is transferred
from the smart AR glasses to a portable computer and the
matching result retrieved from the computer via Bluetooth.
However, such a solution cannot avoid the network delays and
congestion in an elderly care home with hundreds of residents.

III. ARCHITECTURE

We apply the SCOP server to manage residents’ healthcare
data, and recognize the faces of the residents through the smart
AR glasses based on photos taken beforehand from the front or
from an oblique angle. The residents’ healthcare information
will be projected onto the lens of the AR glass, at the same
time, the system will read out the resident’s information by
audibly providing the information through a bone conduction
earphone 6. The content of the voice can be set in various
ways depending on the situation. This section consists of three
subsection including facial recognition technology to identify
the resident, the content displayed on the AR glasses, and
speech synthesis.

A. Facial Recognition

To quickly ascertain someone’s identity by facial recog-
nition technology requires a recognition system that can be
integrated with the work process of caregivers when the care-
givers need to check the condition of residents. In this case,
the existing high-level accuracy deep neural network model
needs to be modiﬁed by reducing the number of parameters.
Inspired by [31], we extract the discriminative representations
of identities from the middle layer of a high accuracy DNN
model, then feed these representations into a pre-trained net-
work (VGG-retrain-v1), which comprises several lightweight
CNN-based layers and global max pooling as shown in Fig. 2.

5https://nntc.digital/projects/ifalcon-face-control-mobile-facial-recognition-

for-smart-glasses/

6https://aftershokz.jp/products/aeropex

Fig. 2. Overview of our facial recognition architecture, comprising three parts: pre-trained network, lightweight network, and embedding space. By combining
the middle layer features of the learned model with the lightweight network, we could apply a lightweight learning model that signiﬁcantly reduces computational
resources. We employ triplet loss to update the parameters of the whole deep neural network and output the ﬁnal representation of each input in the embedding
space, where the “Anchor” image and the “Positive” image are pulled closer together, while at the same time, pushing the “Anchor” image and “Negative”
image further apart.

identities being marked as negative.

As a result, we were able to provide a native library on
Android and achieved identiﬁcation performance of 10,000
people per second on the android smartphone and the AR
glass. In general, the facial recognition system has two steps:
step 1. the facial recognition model retrieves facial features
from the photograph of a face, step 2. it compares the facial
features with a database of known faces to ﬁnd a match.
The more registered known faces there are in the database,
the greater the CPU search time and the more memory is
consumed. Therefore, we implemented a native-c based search
program for 2-step processing. In step 1, the face recognition
model converts the facial features of each elderly resident into
128 bytes and allocates its ID into 4 bytes (unsigned int). This
means that each elderly resident only needs 132 bytes for its
registration. Then in step 2, we concatenate all the registration
data of the elderly residents into one binary ﬁle and implement
a binary search with pointers in C language to enable fast
execution.

In addition, we need to consider the various face orientations
in order to improve the effective features more efﬁciently. In
cases where the orientation angle of the face in the electronic
data is not always a frontal face view, facial orientation
recognition [34] is important to improve the accuracy of the
machine perception system. In this paper, we calibrate each
non-frontal face orientation with a mixture of data which
contain various face orientations. The process can be seen in
Fig. 3. The ﬁrst step is to detect the face and determine the
orientation of the face, the second step is to calibrate each
face orientation to the frontal face view using an external
library [35].

B. Display Content

When the caregiver wears the AR glasses, the camera will
capture the resident’s face, and the resident’s healthcare infor-

Fig. 3. The process of calibration for each face orientation includes two steps:
Detect facial features and match from known faces

To implement

the lightweight facial recognition system
and enable the system to be applied in a small Internet-of-
Things (IoT) device, we design the entire architecture for
facial recognition incorporating three main components: 1)the
learned network consists of three CNN-based blocks of a pre-
trained network, and the pre-trained network refers to the
CNN-B [32] that includes eight convolution layers, four pool-
ing layers, and two fully connected layers. 2)the lightweight
network contains two CNN-like blocks in order to reduce
the number of parameters. Compared with the convolutional
layers of the pre-trained network [33], we apply a depthwise
separable convolutional
layers to
reduce the computational cost by separating the depthwise
convolutional layer and the point-wise convolutional layer. As
a consequence, the lightweight layers can reduce the number
of parameters by about 90%. 3) the embedding space is to
apply triplet loss to directly reﬂect what kind of features we
want in the facial recognition training network. In particular,
we select one facial image as an anchor, all other images
with the same identity are marked as positive, with different

layer as the lightweight

Fig. 4. The Demonstration of the ”Smart Hands-free Elderly Care Support System” and Collage of Images

mation will be displayed on the right-hand lens of the glasses,
as seen in Fig. 4. The resolution of the AR glass is 1920x1080
pixels. The displayed information includes “resident’s name”,
“level of care required”, “elderly care home name and room
number”, “blood pressure”, “body temperature”, “Meal sta-
tus”, “degree of mobility”, “eaten or not”, and “excretion or
not” and ”local time”.

The beneﬁt of displaying this content on the glasses is that
the caregiver can take action immediately according to the
displayed content. For example, if the caregiver notices the
resident’s blood pressure is abnormal, he/she can make an
informed decision. If the resident currently has high blood
pressure, the bathing time should be reduced. If the resident
has not passed urine or feces, the caregiver would ask the
resident if he/she needs to go to the bathroom. In addition,
even if the caregiver has not yet obtained detailed information
the caregiver can easily take
such as for a new resident,
appropriate measures because the new resident’s information
and his/her current physical condition can be displayed by the
devices.

C. Speech Synthesis

After the AR smart glasses recognize the face of a resident,
the smart glasses will read the information out in Japanese
using speech synthesis technology, as seen in the Fig. 1. We
apply a lightweight text-to-speech technique on a standalone
micro-controller board for Internet of Things (IoT), which is a
memory-saving and lightweight operation. In detail, the input
of the speech synthesis system is data in text form, and the
system will generate the intermediate data with the help of
an initial word dictionary. By using the hidden Markov model
(HMM)-based text-to-speech method, the system will generate

a lightweight model and reduce the number of parameters. Fi-
nally, synthetic speech will be generated by multi-band signals
system (MBSS) technology [36] that uses the multiband sine
wave synthesis method.

Furthermore, it is possible to freely add new words to the
word dictionary that are not in the current word dictionary.
Homonyms such as people’s names can be read out properly
by assigning reading kana, for example, the Japanese name
Kohara (コハラKohara, オハラOhara, オバラObara).

IV. EXPERIMENTS

A. Evaluation Metric

To evaluate our smart hands-free elderly care support sys-
tem, we designed some evaluation metrics to leverage and
investigate the acceptability of the devices while we conducted
the experiments.

In the preliminary experiments on the premises, the evalua-
tion was focused on the accuracy and robustness of face recog-
nition against differences in face orientation and illumination
of subjects, which included ofﬁcial full-time caregivers and
temporary employees. The evaluation conditions are divided
into three levels with a range of variations in face orientation.
Finally, we evaluated our facial recognition methods under a
combination of two evaluation conditions. Some examples of
these combinations of evaluation conditions are as follows.

• Case one: bright environment, the orientation of the face
on the AR Smart Glasses screen varies within a range of
60° from the normal line of the screen.

• Case two: normal environment, the orientation of the face
on the AR Smart Glasses screen varies within a range of
40° from the normal line of the screen.

TABLE I
THE CONFIGURATION PARAMETERS OF THE DEVICE WE APPLIED.

Parameter name

Operating System
Processor
Companion APP
Memory
Camera
Battery

Value

Android OS
ARM Cortex-A53
Android 6.0 or lateriOS 11.0 or later
5.91 GB (expandable via microSD card)
8MP with 720p video
470mAh Lithium polymer

Fig. 5. The accuracy of facial recognition is according to the validation rate
for the same person and the false accept rate for a different person, where the
validation rate is where the same people are recognized, while false accept
rate is where different people are perceived as being the same person.

B. Setting

• Case three: dark environment, the orientation of the face
on the AR Smart Glasses screen varies within a range of
20° from the normal line of the screen.

There are nine cases in total according to the combination of
evaluation conditions, 20°, 40°, and 60° for face orientation,
and bright, normal, and dark for illumination. In our paper,
we will show the result of face orientation be set as 0° and
45° in the normal environment. Beside, the distance between
caregiver and resident is also important, in our experiments,
we set the similar distance ranges from 50 cm to 100 cm.

In addition, we tested acceptability as another evaluation in
the production experiment at the Zenkoukai site in 2020/10/13
to 2020/12/18. The evaluation items include:1) Facial recog-
nition accuracy. 2) Display content on AR smart glasses. 3)
Content to be read to earphones. 4) Time taken to display. 5)
Time taken to read out. 6) Do you actually want caregivers to
use it. 7) Impressions on residents of caregiver wearing this
device. The AR smart glasses, chargers, earphones, and etc.
will be discussed separately from the viewpoint of handling
personal information. And we also investigated the impression
of caregivers using a 5-point evaluation based on the mean
opinion score (MOS). The product was evaluated on a 5-point
scale of [very good], [good], [normal], [bad], and [very bad].
As for the accuracy of facial recognition, refer to [22],
we deﬁne the percentage of correctly recognized face images
as the validation rate, while the percentage of incorrectly
recognized face images as the false accept rate, as seen in the
Eq.(1). In our experiments, the validation rate for the same
person and the false accept rate for a different person are the
main evaluation indicators. There is a trade-off between the
validation rate and the false accept rate, as shown in Fig. 5,
where the threshold is determined by the false positive (FP) or
false accept rate that is set in advance. For example, if the false
accept rate is 0.001, the threshold decides the validation rate
should be 1 − threshold = 99.9%. If Val@FAR=0.001 is the
rate that faces are successfully accepted (TP/(TP+FN)) when
the rate that faces are incorrectly accepted (FP/(TN+FP)) is
0.001. Overall, a good facial recognition system requires both
a high validation rate and a low false accept rate.

We utilize Google’s FaceNet algorithm [22] as the baseline
of our proposed approach. We map each face image to a
compact Euclidean space using a deep convolutional network.
If the Euclidean distance of two images is close, this indicates
they belong to the same person. During iteratively training the
model, we created a triplet that consists of Anchor, Positive,
and Negative to compute the cost function, then applied triplet
loss as an objective function to update the parameters of
our deep neural networks. The preliminary experiment that
investigated the execution speed was performed on an Android
device released in 2016 and our AR device, the detail is seen
in Table. I.

C. Results

In our experiment, we apply the LFW (pair.txt ﬁle) [40]
dataset to evaluate our system, since it will consume too much
time if we use all the cross-pairs of the LFW dataset. Accord-
ingly, we select the same 6,000 pairs as [40] from the LFW
dataset as the evaluation data. The results of a comparative
evaluation of open sets by evaluating the recognition accuracy
and execution speed are shown in the Table. II. Overall, our
system achieves accuracy of 99.6% or higher and is superior
in terms of both recognition accuracy and execution speed.
In particular, our model with VGGFace2 or CASIA-WebFace
can achieve the best accuracy and VAL/FAR in the experiment.
An accuracy of 99.68% and 99.62%, with a VAL of 98.63%
and 98.83%, and a FAR of 0.1% and 0.1%, respectively, were
achieved. We also assess the execute time in relation to the
change in the number of people, the execute time includes
the facial feature retrieval cost and the face matching cost.
We select 100, 1,000, and 10,000 people to be recognized.
Since most of the existing methods were implemented only
by Java- or Python-based functions for matching faces, we
apply our C-based fast matching algorithm to all methods
to provide a fair comparison of execution speed. The result
proves that our lightweight network structure maintains high
accuracy with fewer model parameters than the current state-
of-the-art methods, which can reduce the time cost of face
recognition and search time, and as the number of people
increases, the time saving becomes more obvious. On the other
hand, the accuracy of the 0.25Fast-FaceNet model was only
0.7953, which does not meet the requirements, even though it
has fewer parameters.

V alidation Rate(V AL) = T P/(T P + F N )

F alse Accept RAT E(F AR) = F P/(F P + T N )

(1)

The face orientation and illumination of the image will
inﬂuence the accuracy, VAL, and FAR of the system. By

TABLE II
THE COMPARATIVE EVALUATION OF OPEN SETS INCLUDING THE RECOGNITION ACCURACY AND EXECUTION SPEED

Evaluation
conditions

Our model+
VGGFace2 [37]

Our model+
CASIA-WebFace [38]

Google Facenet [22]
+VGGFace2

Google Facenet [22]
+CASIA-WebFace

1.00Fast-FaceNet
VGGFace2 [39]

0.25Fast-FaceNet
VGGFace2 [39]

LFW pair.txt

Params
100 people
1000 people
10000 people

Accuracy: 0.99680
VAL 0.98633@
FAR=0.00100
2.4m
86ms
270ms
925ms

Accuracy: 0.99620
VAL: 0.98833@
FAR=0.00100
2.4m
86ms
270ms
925ms

Accuracy: 0.99630
VAL: 0.98600@
FAR=0.00100
7.5m
632ms
System Halted
System Halted

Accuracy: 0.99550
VAL: 0.98357@
FAR=0.00100
7.5m
632ms
System Halted
System Halted

Accuracy: 0.98630
VAL: 0.97452@
FAR=0.00100
4.8m
196ms
671ms
1487ms

Accuracy: 0.79530
VAL: 0.77910@
FAR=0.00100
0.9m
63ms
206ms
704ms

TABLE III
THE ACCURACY OF FACE RECOGNITION IN AR SYSTEM WITH
CHANGES IN VIEWING ANGLE.

TABLE IV
A CAREGIVERS’ RESPONSES TO QUESTIONNAIRE CONCERNING THEIR
FEELING ABOUT THE AR DEVICE.

Viewing angle

Image pairs

VAL

FAR

0-degree (front)
45-degree

6000
6000

99.9% 0.00100
99.3% 0.00100

Feeling

Number of People

Scared
Funny looking
Especially Scared

5
9
2

light

illumination was set as normal

considering the limitations of the experimental environment,
in this paper,
in all
experiments, and the face orientation was changed between
frontal face and face at a 45-degree angle. From the Table. III,
we have obtained excellent results for VAL and FAR using
face recognition AR technology regardless of the changes in
viewing angle. We use different 6000 image pairs (the same
standard as LFW) for two different viewing angles. In the end,
when the images are front face, the VAL is 99.9% and the FAR
is 0.001%, while the VAL is 99.3% and the FAR is 0.001%
where the facial image is at a 45-degree viewing angle.

Since the developed recognition system needs to be im-
plemented on smart glasses to assist
the caregivers, apart
from the requirement for high-level accuracy, it is also very
important to investigate the system’s acceptability to caregivers
in relation to several factors. We administered a questionnaire
to caregivers about acceptability when they used the smart
glasses. They were asked to ﬁll out
the questionnaire by
selecting one of ﬁve pre-deﬁned ratings. The rating scale
is as follows, 5 (Strongly agree), 4 (agree), 3 (undecided),
2(disagree), and 1 (completely disagree). The questions of the
questionnaire were as follows: (A) Did the displayed contents
match the residents? (B) Are you satisﬁed with the content
displayed on the right lens of the smart AR glasses? (C) Is
there is any additional information you would like to see on
the smart AR glasses? (D) Is there any information displayed
on the smart AR glasses you feel is unnecessary? (E) If the
above questions do not cover all your concerns, please give
us speciﬁc examples. For example, the display takes too long
to appear, or it is hard to identify whose information is being
displayed, etc.

According to the answers from six caregivers when asked
about using the smart glasses, some of the satisﬁed caregivers
expressed the following: “I appreciate it because I don’t have
to search for information on using my cellphone”, or “It is
intuitive and easy to use, even for new caregivers who have

TABLE V
A CAREGIVERS’ RESPONSES TO QUESTIONNAIRE ON ACCEPTABILITY.

Participant’s Role

Number of People

Average Rate Score

Caregivers

6

3.6

never used it before”, or “The displayed content is easier
to read than I expected”. But some caregivers expressed the
concern that “the smart AR glasses were heavy”, or “they look
like sunglasses”.

Six caregivers participated in the experiments, the average

rating of caregivers is 3.6, as shown in Table. V.

D. Analysis

Our facial recognition achieved a high level of accuracy with
a two-stage process incorporating face detection and facial
recognition. However, facial detection relies on an external
library [35] and this may lead to the system being unable to
obtain facial features of some residents. In the future, we will
consider improving facial detection for elderly people’s faces
and detection in different posts, such as when they are lying
in bed.

Long-term care facilities. It is conﬁrmed that the accuracy
of face recognition is high in the practical use scenario, such
as ascertaining the location of residents in the corridor or

Fig. 6. Questionnaire for caregivers in elderly care homes on spot.

dining room. In addition, by using the smart glasses, such
information as that on excretion and body temperature could
be obtained quickly hands-free, conﬁrming the usefulness of
our system. Due to the COVID-19 pandemic, we could not
directly evaluate the questionnaire of the elderly residents. In
the future, we will conduct more experiments with the change
of location and invite more people to participate.

Privacy concern. An important factor of the SHECS system
is the concerns for privacy when the caregivers review the
residents’ information. Residents may be concerned about
information being displayed and read aloud on AR glasses
during care. However, the truth is that the information dis-
played can only be seen by the caregiver, who retrieves it
from the local server, and the voice read aloud is played by
bone conduction headphones, which are also harder for people
around the caregiver to hear. So the residents’ individual
information is safe.

Extension of usage. We believe that the technology of the
SHECS system can be used not only in elderly care but also
in other ﬁelds. For example, in a nursery school, information
about each child and their body temperature measured in the
morning could be immediately detected, which could improve
the efﬁciency of information sharing among the caregivers and
understanding the health status of the children. With children
sometimes hugging or falling, the handsfree system is effective
in allowing both hands to be used. We think it can also be used
to keep track of the names of kids’ parents.

V. CONCLUSION

In order to instantly master the condition of elderly people in
elderly care homes, it is necessary to promote the management
of the care support system, such as providing permanent care
for the elderly residents. In this paper, we develop a smart care
system in conjunction with facial recognition technology and
speech synthesis technology on smart AR glasses, in which
the residents’ information is provided by displaying part of
the information and reading out the complete information out.
The emergence of the SHECS system is motivated by the re-
quirements of both new and old elderly residents. This system
promotes communication especially between caregivers and
new residents so that they get to know each other beforehand.
In the future, we will continue to utilize the power of com-
munication and work with partners to conduct research that
leads to solutions to social issues, with the aim of achieving
sustainable growth and the development of society. Moreover,
We will achieve experiments with more inﬂuence factors such
as the the distance between caregivers and residents.

ACKNOWLEDGMENT

The authors would like to thank Zenkokai and Vuzix
Corporation for providing the venue for experiments and
for supporting software implementation on smart glasses,
respectively. We also express our sincere thanks to the experts
from Systemsoft Inc: Zhiguang Zhou, Zheshuang Lyu, Wataru
Nishioka, Megumi Komiya for their great support and advice
on our proposed method and its optimization.

REFERENCES

[1] T. Mukai, S. Hirano, H. Nakashima, Y. Kato, Y. Sakaida,
S. Guo, and S. Hosoe, “Development of a nursing-care
assistant robot riba that can lift a human in its arms,” in
2010 IEEE/RSJ International Conference on Intelligent
Robots and Systems.

IEEE, 2010, pp. 5996–6001.

[2] T. Mukai, S. Hirano, M. Yoshida, H. Nakashima, S. Guo,
and Y. Hayakawa, “Tactile-based motion adjustment for
the nursing-care assistant robot riba,” in 2011 IEEE
International Conference on Robotics and Automation.
IEEE, 2011, pp. 5435–5441.

[3] K. Seibert, D. Domhoff, D. Bruch, M. Schulte-Althoff,
D. F¨urstenau, F. Biessmann, and K. Wolf-Ostermann,
“A rapid review on application scenarios for artiﬁcial
intelligence in nursing care,” JMIR Preprints, vol. 16,
no. 12, p. 2020, 2020.

[4] H. Watanabe and T. Fujino, “Design and implementation
of a simple home nursing care support system using
domain name system,” in 2009 International Conference
on Networking, Sensing and Control.
IEEE, 2009, pp.
468–473.

[5] X. Chen, L. Xu, Y. Wang, H. Wang, F. Wang, X. Zeng,
Q. Wang, and J. Egger, “Development of a surgical
navigation system based on augmented reality using an
optical see-through head-mounted display,” Journal of
biomedical informatics, vol. 55, pp. 124–131, 2015.
[6] S. Mitrasinovic, E. Camacho, N. Trivedi, J. Logan,
C. Campbell, R. Zilinyi, B. Lieber, E. Bruce, B. Taylor,
D. Martineau et al., “Clinical and surgical applications
of smart glasses,” Technology and Health Care, vol. 23,
no. 4, pp. 381–401, 2015.

[7] J. Ruminski, M. Smiatacz, A. Bujnowski, A. Andru-
shevich, M. Biallas, and R. Kistler, “Interactions with
recognized patients using smart glasses,” in 2015 8th
International Conference on Human System Interaction
(HSI).

IEEE, 2015, pp. 187–194.

[8] T. R. Clancy, “Artiﬁcial intelligence and nursing: The
future is now,” JONA: The Journal of Nursing Adminis-
tration, vol. 50, no. 3, pp. 125–127, 2020.

[9] M. Grayson, A. Thieme, R. Marques, D. Massiceti,
E. Cutrell, and C. Morrison, “A dynamic ai system for
extending the capabilities of blind people,” in Extended
Abstracts of the 2020 CHI Conference on Human Factors
in Computing Systems, 2020, pp. 1–4.

[10] Y. Yu, S. Tang, K. Aizawa, and A. Aizawa, “Category-
based deep cca for ﬁne-grained venue discovery
from multimodal data,” IEEE transactions on neural
networks and learning systems., vol. Vol.30, no.
no.99, pp. pp.1–9, 2018.
[Online]. Available: https:
//doi.org/10.1109/TNNLS.2018.2856253

[11] D. Zeng, Y. Yu, and K. Oyama, “Deep triplet neural
networks with cluster-cca for audio-visual cross-modal
retrieval,” ACM Transactions on Multimedia Computing,
Communications, and Applications (TOMM), vol. 16,
no. 3, pp. 1–23, 2020.

[12] Y. Yu, S. Tang, F. Raposo, and L. Chen, “Deep
cross-modal correlation learning for audio and lyrics
in music retrieval,” TOMCCAP., vol. Vol.15, no.
no.1, pp. pp.20:1–20:16, 2019.
[Online]. Available:
https://doi.org/10.1145/3281746

[13] D. Watson, J. Womack, and S. Papadakos, “Rise of the
robots: Is artiﬁcial intelligence a friend or foe to nursing
practice?” Critical Care Nursing Quarterly, vol. 43,
no. 3, pp. 303–311, 2020.

[14] F. Carros,

J. Meurer, D. L¨ofﬂer, D. Unbehaun,
S. Matthies, I. Koch, R. Wieching, D. Randall, M. Has-
senzahl, and V. Wulf, “Exploring human-robot
inter-
action with the elderly: results from a ten-week case
study in a care home,” in Proceedings of the 2020 CHI
Conference on Human Factors in Computing Systems,
2020, pp. 1–12.

[15] S. Lee and A. M. Naguib, “Toward a sociable and
dependable elderly care robot: design, implementation
and user study,” Journal of Intelligent & Robotic Systems,
vol. 98, no. 1, pp. 5–17, 2020.

[16] A. Sharkey and N. Sharkey, “Children, the elderly, and
interactive robots,” IEEE Robotics & Automation Maga-
zine, vol. 18, no. 1, pp. 32–38, 2011.

[17] M. E. Pollack, L. Brown, D. Colbry, C. Orosz, B. Peint-
ner, S. Ramakrishnan, S. Engberg, J. T. Matthews,
J. Dunbar-Jacob, C. E. McCarthy et al., “Pearl: A mobile
robotic assistant for the elderly,” in AAAI workshop on
automation as eldercare, vol. 2002, 2002, pp. 85–91.

[18] C. Rougier, J. Meunier, A. St-Arnaud, and J. Rousseau,
“Monocular 3d head tracking to detect falls of elderly
people,” in 2006 international conference of the IEEE
engineering in medicine and biology society.
IEEE,
2006, pp. 6384–6387.

[19] M. Andrejevic and N. Selwyn, “Facial recognition tech-
nology in schools: Critical questions and concerns,”
Learning, Media and Technology, vol. 45, no. 2, pp. 115–
128, 2020.

[20] I. Q. Mundial, M. S. U. Hassan, M. I. Tiwana, W. S.
Qureshi, and E. Alanazi, “Towards facial recognition
problem in covid-19 pandemic,” in 2020 4rd Interna-
tional Conference on Electrical, Telecommunication and
Computer Engineering (ELTICOM).
IEEE, 2020, pp.
210–214.

[21] S. A. Manssor and S. Sun, “Tirfacenet: thermal ir facial
recognition,” in 2019 12th International Congress on
Image and Signal Processing, BioMedical Engineering
and Informatics (CISP-BMEI).

IEEE, 2019, pp. 1–7.

[22] F. Schroff, D. Kalenichenko, and J. Philbin, “Facenet: A
uniﬁed embedding for face recognition and clustering,” in
Proceedings of the IEEE conference on computer vision
and pattern recognition, 2015, pp. 815–823.

[23] Y. J. Park, H. Ro, N. K. Lee, and T.-D. Han, “Deep-care:
Projection-based home care augmented reality system
with deep learning for elderly,” Applied Sciences, vol. 9,
no. 18, p. 3897, 2019.

real world,” in 2016 14th International Conference on
Control, Automation, Robotics and Vision (ICARCV).
IEEE, 2016, pp. 1–5.

[25] Z. Luo, J.-T. Hsieh, N. Balachandar, S. Yeung, G. Pusiol,
J. Luxenberg, G. Li, L.-J. Li, N. L. Downing, A. Milstein
et al., “Computer vision-based descriptive analytics of
seniors’ daily activities for long-term health monitoring,”
Machine Learning for Healthcare (MLHC), vol. 2, p. 1,
2018.

[26] H. T. Bunnell, C. Pennington, D. Yarrington, and J. Gray,
“Automatic personal synthetic voice construction,” in
Ninth European Conference on Speech Communication
and Technology, 2005.

[27] A. Hjalmarsson, “The additive effect of turn-taking cues
in human and synthetic voice,” Speech Communication,
vol. 53, no. 1, pp. 23–35, 2011.

[28] P. Damacharla, P. Dhakal, S. Stumbo, A. Y. Javaid,
S. Ganapathy, D. A. Malek, D. C. Hodge, and V. Devab-
haktuni, “Effects of voice-based synthetic assistant on
performance of emergency care provider in training,”
International Journal of Artiﬁcial Intelligence in Edu-
cation, vol. 29, no. 1, pp. 122–143, 2019.

[29] C. R. Paris, R. D. Gilson, M. H. Thomas, and N. C.
Silver, “Effect of synthetic voice intelligibility on speech
comprehension,” Human Factors, vol. 37, no. 2, pp. 335–
340, 1995.

[30] G. Mittag and S. M¨oller, “Deep learning based assess-
ment of synthetic speech naturalness,” arXiv preprint
arXiv:2104.11673, 2021.

[31] Y. Wang, J. Wu, and K. Hoashi, “Lightweight deep con-
volutional neural networks for facial expression recog-
nition,” in 2019 IEEE 21st International Workshop on
Multimedia Signal Processing (MMSP).
IEEE, 2019,
pp. 1–6.

[32] K. Simonyan and A. Zisserman, “Very deep convolu-
tional networks for large-scale image recognition,” arXiv
preprint arXiv:1409.1556, 2014.

[33] Y. Sun, X. Wang, and X. Tang, “Deeply learned face
representations are sparse, selective, and robust,” in Pro-
ceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2015.

[34] K. D. Gupta, M. Ahsan, S. Andrei, and K. M. R. Alam,
“A robust approach of facial orientation recognition from
facial features,” BRAIN. Broad Research in Artiﬁcial
Intelligence and Neuroscience, vol. 8, no. 3, pp. 5–12,
2017.

[35] K. Zhang, Z. Zhang, Z. Li, and Y. Qiao, “Joint face
detection and alignment using multitask cascaded con-
volutional networks,” IEEE Signal Processing Letters,
vol. 23, no. 10, pp. 1499–1503, 2016.

[36] I. Alim and M. O. Kolawole, “Enhancement of speech
communication technology performance using adaptive-
control factor based spectral subtraction method,” Jour-
nal of Telecommunications and Information Technology,
2013.

[24] B. Mandal, “Face recognition: Perspectives from the

[37] Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zisser-

man, “Vggface2: A dataset for recognising faces across
pose and age,” in 2018 13th IEEE international confer-
ence on automatic face & gesture recognition (FG 2018).
IEEE, 2018, pp. 67–74.

[38] D. Yi, Z. Lei, S. Liao, and S. Z. Li, “Learning face repre-
sentation from scratch,” arXiv preprint arXiv:1411.7923,
2014.

[39] X. Xu, M. Du, H. Guo, J. Chang, and X. Zhao,
“Lightweight facenet based on mobilenet,” International
Journal of Intelligence Science, vol. 11, no. 1, pp. 1–16,
2020.

[40] G. B. Huang, M. Mattar, T. Berg, and E. Learned-Miller,
“Labeled faces in the wild: A database forstudying face
recognition in unconstrained environments,” in Workshop
on faces in’Real-Life’Images: detection, alignment, and
recognition, 2008.

