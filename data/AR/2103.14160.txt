1
2
0
2

r
a

M
5
2

]

C
H
.
s
c
[

1
v
0
6
1
4
1
.
3
0
1
2
:
v
i
X
r
a

Design and Test of an adaptive augmented reality interface to manage
systems to assist critical missions.

Dany Naser Addin and Benoˆıt Ozell
{dany.naser-addin,benoit.ozell}@polymtl.ca

Department of Computer and Software Engineering,

Polytechnique Montr´eal, Qu´ebec, Canada

March 29, 2021

Abstract

We present a user interface (UI) based on augmented real-
ity (AR) with head-mounted display (HMD) for improv-
ing situational awareness during critical operation and
improve human eﬃciency on operations. The UI dis-
plays contextual information as well as accepts orders
given from the headset to control unmanned aerial vehi-
cles (UAVs) for assisting the rescue team. We established
experiments where people had been put in a stressful sit-
uation and are asked to resolve a complex mission using
a headset and a computer. Comparing both technologies,
our results show that augmented reality has the poten-
tial to be an important tool to help those involved in the
emergency situation.

Keywords:

augmented reality,

autonomous system,
awareness, critical situations, firefighters

drone swarm,

interactions,
situation

1

Introduction

During critical situations, many humans risk their lives
to save others. Relying more on technology can allevi-
ate such risk and enhance the emergency response suc-
cess rate. Previous works implemented unmanned aerial
vehicles (UAVs) based systems to prevent human from
accessing dangerous areas themselves.

Information and communication technology recent de-
velopments have made data more accessible than ever,
challenging human capacity to quickly process massive
loads of information. In case of emergency response for
instance, being able to process information in time leads
to risk reduction and success rate increase. We propose
to use Augmented Reality (AR) to improve such system
and provide better environment visualization and assets
control.

AR is a technology that superimposes virtual informa-
tion on the environment and is becoming more and more
common in our daily lives. The advanced development

1

of mobile phones available to the public makes it pos-
sible to provide numerous applications in AR or virtual
reality (VR). Today, augmented and virtual reality has
become accessible to everyone through the use of HMD,
which allows the use of AR from a diﬀerent approach.
These visual representations are displayed on top of the
real environment to provide additional information with
the aim to assist people in complex tasks or for simple
entertainment purposes.

This article is divided into three sections. First, we
present emergency situations and the concept of situ-
ational awareness. We also review teleoperation with
mixed reality and UAV-based rescue system. Second, we
explain details on our AR-based UI design and present our
implementation. Third, we present the tests conducted
and discuss the results regarding the impact on humans’
stress in the emergency response.

This work was carried out during the covid pandemic.
Some adjustments in the experimentation have been set
up . The experiment was to initially involve ﬁreﬁght-
ers in order to use their knowledge of the emergency
environment to evaluate the augmented reality technol-
ogy in emergency situations. As a substitute, students
from Polytechnique Montr´eal made the experiment with-
out having any emergency’s knowledge. The experimen-
tation has been revised in order to evaluate the eﬀective-
ness of AR in emergency situations with these people.

2 Related work

2.1 Emergency Response’s Strategies

The arrival of a natural disaster or an attack alert can rep-
resent diﬀerent cases of an emergency operation. Many
actions are carried out under considerable stress for res-
cue or military missions.
It has been shown that 71%
of aviation accidents are due to human error [1]. These
errors are the result of high workloads and high levels of
stress. Researchers report that reducing the operator’s

 
 
 
 
 
 
workload and information to be processed has a positive
impact on stress. On the contrary, if a person doesn’t
have enough information about the situation, both men-
tal and physical workloads become too great and this
generates frustration which leads to errors. The term
“Situational awareness” was introduced in 2003 by End-
sley et al. [2] as the understanding of what is happening
around us, the current situation and the one to come.
The concept also regroups the ﬁltering of important in-
formation to perform tasks at hand. Stress as well as a
highly physical and mental workloads are factors that re-
strict situational awareness. This distracts the individual
and prevents him or her from making the right decisions.
In emergency situations, actors need to be careful about
their situational awareness.

To measure the situational awareness of a person, End-
sley et al. created the Situation Awareness Global Assess-
ment Technique (SAGAT) [3] which measures the situa-
tional awareness of pilots in ﬂight operations. Questions
are asked to the pilot during the mission to measure the
stress, they explained that humans tend to generalize if
their feeling is asked at the end of the test which prevents
a correct assessment. They use immersive technologies to
design exercises for emergency responders and test usabil-
ity with three immersive displays: a simple screen, a giant
curved screen and a VR HMD. Each of these technologies
has very diﬀerent properties and each of them can be use-
ful depending on the scenario. For training ﬁreﬁghters,
Cliﬀord et al. found out two important limitation factors:
cost and environmental impact [4]. Another method cre-
ated was the NASA-TLX form [5] consists of a set of
questions that requires one to scale one’s eﬀort from 0 to
100 in several types of mental and physical actions. Lee et
al. set up a simulation of a helicopter attack to evaluate
how the pilots feel [1]. They obtained results using this
NASA-TLX to evaluate the physical and mental loads
and frustration related to factors to evaluate situational
awareness.

An autonomous system (AS) is an Unmanned Aerial
Vehicles (UAVs) which are robotic machines acting un-
der human control. UAVs operate synchronously to com-
plete tasks. Drones are from the UAV group and Yuan
et al. listed numerous use cases [6]. Research found vari-
ous possibilities with UAV and several sensors, including
cameras depending on the situation (video, surveillance,
etc.) [7]. An autonomous system poorly handled by hu-
man supervision can degrade the eﬃciency of using the
machines [8]. Atyabi et al. present the necessity that a
stand-alone system is not to become totally independent
of any form of directive, but for that it must signiﬁcantly
reduce the mental and physical workloads of the user to
improve situational awareness. In order to manage UAVs,
people use a control station to optimally manage several
air or ground vehicles as well called as a mission plan-

ner [9]. It allows several objectives to be accomplished in
dynamic environments. It is an essential element in the
progress of an emergency operation. Reducing the hu-
man presence in dangerous areas impacts on the human
risk, which can be done by using autonomous systems.
Carrillo-Zapata et al. found that using a robotics swarm
is really eﬃcient in ﬁre and rescue ﬁelds [10]. In a mutual
work with ﬁreﬁghters, they found the swarm is improving
performance if the data provided by the swarm is relevant
and it is important to let the decision-making to humans.

2.2 Mixed Reality Technologies

Figure 1: Virtual Reality and Augmented Continuum
Milgram

Milgram deﬁned the concept of AR as the overlaying
of visual information into the real world [11]. Display of
a hologram in a real room corresponds to the left side of
the scale (ﬁgure 1), considering it’s based mainly on a real
environment. Conversely on the right part of the scale,
VR cuts oﬀ all interaction from the real world to immerse
users in a diﬀerent and synthetic reality. The concept of
“Mixed reality” (MR) at the centre of the scale repose of
using information from real environment (room conﬁgu-
ration, use of real objects) to display digital information.
Navigation through AR implies the use of interfaces,
deﬁned by Hookway [12] as a bridge between human expe-
rience and allowing actions that it can’t undertake alone.
Companies such as Microsoft [13] or Apple [14] designed
their own “user-friendly” AR interfaces for support like
a computer or mobile device. Their solution is based
on a 2D interface (e.g., touch screen) whereas AR using
HMD includes the use of the third dimension. Argenta et
al. designed interface to support tactical (military) mis-
sions and consider gestures, voice and touch with AR [15].
Bertrand et al. reviewed this change of dimensions with
common interface principles like e-mail applications [16],
they made adjustments to perform a pleasant and intu-
itive interface in 3D AR environment.

For interactions in AR, the ﬁrst design of the aug-
mented reality headset using the ﬁngers of the hand as
an interaction tool was made by Goos [17]. It was one
of the ﬁrst researchers to submit the prototype of an AR
HMD. Their design consists of a user menu displayed on
the hand, superimposing a button on each of the ﬁngers.

2

The user can then select by placing the index ﬁnger of
the other hand on a button. Today, we can interact in
AR using emerging technologies with headsets in diﬀerent
ways. By using the gaze to select elements [18], recent AR
headsets now support voice recognition and the tracking
of user’s hands and eyes with Hololens 2 [19] and Magic
Leap 1 [20]. However, the ﬁeld of view of HMD remains
limited today. People can now create holographic content
on a computer and stream it directly to a HMD via net-
work by Park et al. [21]. The latter proposed the idea
as a solution to relocate computations and so enhance
performance.

Das and his or her partners worked on a very complete
system for improving the situational awareness of mili-
tary agent to analyze environments using a swarm [22].
Unfortunately, they just mentioned AR for simple visu-
alization and didn’t use AR for the improvement of sit-
uational awareness. In another work, the same authors
used a headset sensor to control robots [23]. They de-
tect the movements of hands to place passage points to
guide robots. Their objective was to design a human-
machine interface adapted in mixed reality to control a
machine architecture. The interface wasn’t designed for
emergency environment. Cousins et al. and Brizzi et al.
interacted on AR for teleoperations with diﬀerent arm
robots [24] [25]. With the improvement of AR, Trzcielin-
ski et al. found that humans learned faster to control the
robotic arm thanks to augmented reality technology us-
ing a HMD [26]. The teleoperation was adapted here for
learning and not for use in concrete situations. Patel et
al. researched about controlling a swarm of small robots
using AR with a tactile computer [27] [28]. They made
a comparison between robot-oriented and environment-
oriented methods using AR with one or multiple partici-
pants at the same time.

Looking at the research on Mixed Reality technologies
and emergency operation, we observed that some of AR
applications were made and adapt for teleoperation, but
not for users in emergency situations and not designed
to reduce stress load. HMD nowadays can be eﬃcient to
improve humans situational awareness and our goal is to
know if AR can help during emergency cases controlling
a swarm of drones. Is AR using HMD capable of assist-
ing humans during an emergency operation? Knowing
that the ﬁeld of emergency regroups various types of sit-
uations, we decided to focus on ﬁreﬁghters operation in
ﬁre in a High-Rise Building (HRB). Recently Sullivan et
al. developed a ﬁrst AR application using the function-
alities from the headset Magic Leap 1 for managing a
swarm [29]. They explored the methods of giving orders
to the swarm and didn’t focus of the data provided by
the swarm.

Table 1: Fireﬁghters’ questions on ﬁre emergency in HRB

Field Questions

P1
P2

P3

Where are the ﬁreﬁghters in the danger zone?
Where’s the seat of the ﬁre?
How will the intensity of the ﬁre progress in a
short time? (louder/weaker)
How many human and animal casualties are
there, and where are they?
What are the diﬀerent needs of :

1. water

2. man

3. material

4. aerie (air)

What is the situation in the risk zone?

1. Where are the load-bearing walls?

2. What are the building materials used?

3. What is the layout of the pipes?

4. What is the plan of the area?

5. Where are the emergency exits?

3 Methodology and Experimenta-

tion

3.1 Fireﬁghters of Montr´eal (SIM)

Conan [30] surveyed and analyzed the habits and proce-
dures of professional ﬁreﬁghters of Montr´eal. In particu-
lar, three main problems have been identiﬁed in case of a
high-rise building (HRB) intervention. For each problem,
some important information is referenced in the table 1.

1. P1 - Location at all times: Location is gener-
ally one of the critical and fundamental points of the
emergency services, especially for ﬁreﬁghters where
there is a lack of visibility between the coordinator
and all the ﬁreﬁghters during a ﬁre.

2. P2 - Evolution of the ﬂames: Obtaining the max-
imum amount of information about an emergency
operation is important for the progress of the mis-
sion. This includes the visualization of the situation
from various angles.

3. P3 - Optimization of understanding: In emer-
gency situations, operations generate information to
process for humans. Therefore this implies a con-
sequent information processing to facilitate the un-
derstanding of the situation in which ﬁreﬁghters ﬁnd
themselves.

3

3.2 Objectives

Our goal is to evaluate the eﬀectiveness of AR with the
use of a HMD during a virtualized ﬁre operation in a
HRB. From our literature review in the previous section 2,
we studied these hypotheses:

1. Ha1 - This interface in AR allows for a better understanding

of information in the ﬁeld of emergency response.

2. Ha2 - This interface in AR improves situational awareness.

3. Ha3 - This interface in AR makes decision-making easier.

4. Ha4 - This interface in AR makes it easier for mission objec-

tives completion.

First we understood the needs of Montr´eal’s ﬁreﬁght-
ers (SIM). Then we designed an AR application managing
swarm drones for critical situations based on our research.
To support our assumptions, we also designed a scenario
for simulation with a ﬁre in HRB. We confronted technol-
ogy including an AR application for HMD with a simple
computer software to manage a swarm in stressful situa-
tions. We tested our scenario case with people of diﬀerent
kinds of ages and experience to get their feedback about
our work. This section contains the needs of ﬁreﬁght-
ers in real case, the conception of our application, the
designed simulation scenario and methods for evaluating
our experience.

Regarding the questions mentioned in the section 3.1,
our technology uses an ad hoc network, it is possible to
geolocate (without the presence of mobile networks or in-
ternet) all the electronic tools that can be either attach
on robots or present on humans to obtain in real time the
position of each. In order to understand the whole situa-
tion and to act quickly, UAVs composing the autonomous
system are technologies with great ease of movement that
allow to quickly photograph and map the area. Manag-
ing this data then displaying it on a 3D and 2D medium
must increase knowledge of the issues in the ﬁeld.

3.3 Proposed Technology

We created an AR application where the user controlled
an AS composed by drones. These drones have cameras
for mapping. Each of these UAVs achieves a common
objective, they evaluate the work in order to reduce the
time cost of the mission. All this infrastructure communi-
cates under an independent ad hoc network developed by
the industrial partner Humanitas Solutions allowing de-
ployment without the use of internet or mobile network.
To lead this swarm, we propose a combination of an AR
HMD to assist human supervision and a touch-sensitive
tablet acting as a mission planner.

This combination of these two synchronized technolo-
gies send actions and receive information from the UAVs.

Figure 2: Diagram representative of a use case with the
system to improve situational awareness

Figure 3: Representation of the sending and receiving
of information in augmented reality to the autonomous
system in action

On the one hand, the tablet allows the UAV to act as a
mission planner from a screen, i.e. to plan mapping and
building analysis missions, but also to send the objectives
of these missions to the UAV, which shares the tasks inde-
pendently. The mission planner also obtains information
from the UAVs to inform the user of the status of each
robot. On the other hand, the headset allows the user
to view the data by hologram in the real environment
in three dimensions. The user is able to see the infor-
mation in an innovative way and positions it in the real
world in order to facilitate and accelerate the consump-
tion of information. The proposed technology consists of
an Augmented Reality Mission Planner. A representative
schema can be found in the ﬁgure 2. The objective of our
proposed technology is to reduce humans’ confusion and
to maximize user situational awareness.

We devised an experiment to put our technology to
the test. The possible actions were divided into interac-
tions from the headset or tablet (hands-based mainly).
These interactions were necessary to consume informa-
tion from the machines and to generate guidelines for the
swarm. For this purpose, static, dynamic and situational

4

Table 2: List and function of the application widgets

Name
Main menu

3D Model

Mission creation

Notiﬁcation

UAV list

UAV data

Situational compass

Improved visibility

Details
Main menu of the application including
widgets activation and settings control
to move widgets
Display 3D models dynamically for help-
ing user to observe the situation
Adding a series of waypoints to create a
path for the swarm to follow
Diﬀerent kinds of notiﬁcation to alert the
user of some events
Allow drone selection by looking directly
at or by selecting it from the list contain-
ing all drones in the swarm
Panel containing all information about
the drones, like GPS coordinate, battery
level, speed, and pictures taken from the
camera
Situational widgets helping user to lo-
cate himself in the environment contain-
ing distance and location of all drones
and mission
Extra visibility for swarm by colour
highlighter and missions
to improve
tracking in the real world

Fireﬁghters SIM form Speciﬁc form adapted for SIM operation

with ﬁre building case

tools have been developed to improve the users’ situa-
tional awareness in an emergency operation. These tools
responded to information needs during an emergency mis-
sion. Each tool was represented in two or three dimen-
sions (2D/3D) depending on the type. A schematic ex-
ample of a use case can be found in the ﬁgure 3, the
standalone system mapped a designated area to retrieve
information. The person could use various actions to in-
teract in a 3D AR universe to process information or send
commands to the system. The user could interact around
him with an interface, but could also visualize informa-
tion in AR at the building area. He also used the mission
planner on a 2D surface for more eﬃcient actions.

Our application regrouped diﬀerent kinds of widgets.
We proposed tools listed on the table 2 as proper aware-
ness improvement of the user. Widgets were also con-
ceived for receiving data from the swarm and controlling
it. Widgets could be placed all around the user (180 de-
grees in front of him). They could be easily dragged and
dropped around the user using the participant’s hands.
We tried to imitate some mechanics from common com-
puter actions (reduce windows, drag windows) to allow
some freedom to the user in the interface and also to re-
mind him moves that he already knew from using actual
technology. The ﬁgure 4 shows the representation of wid-
gets. The application is directly synchronizing with the
mission planner (tablet) and communicates through the
ad hoc network of industrial partners allowing the sys-
tem to work in every situation without 4G/5G mobile

Figure 4: Image of diﬀerent widgets from the application
(a) Main menu (b) Notiﬁcation (c) Situational compass
(d) Mission creation (e) UAV data (f) SIM form (g) 3D
model (h) UAV List (i) Improved Visibility

or internet. The mission planner acts as a main receiver
which transmits information to the HMD. If necessary,
the HMD sends command to the mission planner which
transmits to the swarm.

To simulate a real environment and test our work,
Square Victoria in Montr´eal has been reproduced in a
virtual 3D environment and allowed us to experiment
our emergency scenario in an environment close to re-
ality. Virtual drones were inside the virtual environment
using their real GPS coordinates. A four-story high-rise
building (HRB) was created to set up a ﬁre simulation. A
virtual ﬁre started in the building, and a swarm of UAVs
was available to follow the mission planner’s instructions
to scour the building. The ﬁgure 5 is a representation
of the virtual space of the place. The ﬁgure 6 contains
the representation of the complete infrastructure of our
experience.

Our technology is an application developed with Un-

5

Figure 5: The Square Victoria Square built by the Indus-
trial Partner Humanitas Solutions

Figure 7: Pictures of virtual environment (a) HRB in ﬁre
(b) Sample of virtual people in help

3.4 Context of the Simulation

To experiment our technology and to validate our as-
sumptions required elements that may be hazardous to
human health. It wasn’t possible to test our technology
in real life situations during the COVID-19 pandemic.
Initially, the objective of this experiment was to test the
eﬀect of AR by using a headset to direct a swarm of au-
tonomous UAVs using a mission planner (tablet) to in-
spect the surroundings of a burning building to retrieve
important information. Instead we used a video projec-
tor on a giant screen to display the virtual environment in
ﬁgure 7 containing critical elements of the emergency sce-
nario involving a HRB caught by ﬂames. The user was
positioned in front of this immersive screen. Equipped
with the headset and mission planner, the AR was cali-
brated to operate over the virtual display. For the expe-
rience, the user didn’t need to make any movements and
had an overview of the situation.

Considering the COVID-19 pandemic, we adapted our
experiment for testing our technology. We experimented
with participants of Polytechnique Montr´eal University
without knowledge of emergency situations. We reviewed
our tests to set up a stressful situation with our tech-
nology. We virtualized a building on ﬁre and used the
swarm of UAVs to patrol around it. The participant had
to manage and understand the feedback of the real-time
swarm with a technology he was not familiar with over
a short period of time. We removed the mission plan-
ner tablet tool and the swarm was completely automatic
and did not receive any orders from the user. We mainly
focus on understanding the situation using information
consumption tools. To evaluate the AR performance, we
also developed a similar application for 2D computer to
compare the situation using the two technologies in our
emergency experience.

We virtualized a HRB caught in a ﬁre containing vir-
tual injured people. The participant was accompanied by
an autonomous system composed of 4 drones equipped
with cameras patrolling the building from the outside.
The challenge for the participant was to retrieve multiple

Figure 6: Complete Infrastructure of the Simulation

real Engine 4.24. We used the MagicLeap 1 from Mag-
icLeap as AR HMD [20]. Our contribution regroups
the augmented reality app and a network communication
plug-in designed for the ad hoc system. The industrial
partner Humanitas Solutions created the mission plan-
ner application using Unity 5. An Amazon ﬁre tablet
was used for the experience. Drones were virtually de-
ployed as Docker-based simulation agents through Hu-
manitas Solutions’ Hyper-X-Space (HxS) rapid develop-
ment platform1. Each simulated drone is composed of
i) a software-in-the-loop computing element running the
embedded swarming intelligence (related to the work of
Karydes and Saussi´e [31] and Costa et al. [32]) and ii)
a software-in-the-loop PX4 ﬂight controller, which is de-
ﬁned as a peripheral element. The AR headset and the
mobile mission planner could communicate with the sim-
ulated drones through a bridge powered by Humanitas
Solutions’ ad hoc networking technology. Every 3D vir-
tual world was also designed by them.

1www.hxs.ai

6

information linking knowledge and location in a limited
time in order to simulate a complex and stressful situa-
tion. The participant needed ﬁrst to indicate the location
of the source of the ﬁre as accurately as possible. Then he
or she reports the number of adults and children people
seen in the building. And ﬁnally, for each kind of per-
son, the participant needed to specify the most precise
location as possible (Example: “I saw a child on the 3rd
ﬂoor of the building, he was on the northeast side of the
building very close to a window”).

A mission had a 6-minute time limit. The participant
needed to report everything he or she observed on a re-
port to assist in a ﬁreﬁghter intervention. The movement
of the drones from the swarm was totally autonomous in
order to patrol simultaneously around the building pro-
viding the participant with several “real time” video feed-
back. In order to compare the AR performances, the user
experienced two diﬀerent missions and for each situation
he used a diﬀerent monitoring system. From the ﬁgure
6, the ﬁrst one was a workstation or personal computer
(PC) with a “traditional” 2D 24-inch monitor screen from
computer receiving data in real time including videos of
the drones in order to perceive the location of the drone’s
position. The participant used the 2D monitor and the
view of the simulation environment from the video pro-
jector to improve the participant’s situational awareness
and report what he understood. The second was based
on work with an AR headset in order to develop our stud-
ies and establish an eﬃciency comparison with the ﬁrst
system. The participant was equipped with an AR head-
set. As in the ﬁrst case, the drones sent information and
the user used the tools at his or her disposal with 3D
hologram’s visual environment to understand the situa-
tion and report as much as he or she learned. For this
experiment, we had two diﬀerent conﬁgurations for the
exercise (number and location of people, ﬁre location) to
avoid any form of repetitiveness. Half of the participants
started with the AR while others start with the PC to
avoid any signiﬁcant order bias in the tests. The same
applies to the order of the two ﬁre conﬁgurations used.
Representations of our experience are shown in the ﬁg-
ure 8.

Our experience was composed of three software appli-
cations developed with Unreal Engine 4.24. We used
our proposed technology to create this exercise. First
we made a Windows application containing the building
on ﬁre, virtual drones and virtual people according to the
“real ﬁeld” emergency environment. This was projected
onto a projection screen (image (a) of ﬁgure 8). The sec-
ond app was the AR Lumin application for monitoring
the swarm and contained widgets of our work. Finally,
the last software application was an arrangement of the
widgets we had but adapted to a windows software for
PC using the mouse and keyboard. These applications

Figure 8: Experimentation with Magic Leap 1 (a) User
interaction during the mission (b) Side view of the user
during experience

communicate through a single private network.

Before each mission, a short training session was con-
ducted beforehand to initiate the user with the software.
He or she started the exercise by facing the projection
screen displaying a HRB composed of 4 ﬂoors and 4
drones.

The participant didn’t need to move around during the
exercise. Only the virtual drones moved. As the mission
started, the ﬁre appeared and the drones started their
turn. The monitoring technology received video feedback
and location from each drone throughout the exercise.
Each UAV circled the building from outside on a speciﬁc
ﬂoor, meaning that drones were on the same position but
at diﬀerent altitudes. Video feedback was unique from
each drone. The swarm patrolled twice around the build-
ing for 4:30 minutes and then the mission stopped. The
limit duration of a mission was 6 minutes but participants
were allowed to stop earlier. Then they needed to com-
plete a mission report for each mission and at the end ﬁll
a questionnaire to get subjective opinion on the course of
operations and technologies.

Both technologies had the following capabilities. Ap-
plications contained video feedback information on the
status of the 4 UAVs simultaneously. A mini-map with an
upside view was displayed containing positions of UAVs
all around the building with cardinal points. Target but-
tons were available for each drone to save drone location
and mark its position on a mini-map.
It was possible
to focus on a particular UAV by selecting it. The par-
ticularity on the PC was that all those tools were on one
screen whereas widgets were displayed in front of the par-
ticipant in AR. Selection was with the mouse on the PC
and the index ﬁnger touch in AR. The augmented reality
added a main menu and possibilities to improve visibility
of UAVs and also their trajectories around the building
over the projector rendering. Virtual environment and
AR applications are shown on ﬁgure 8.

7

Table 3: Evaluation of participants’ feedback results dur-
ing experience

Score
4

3
2

1
0

4
3
2
1
0

4
3
2
1
0

4

3
2
1
0
+0.5

1

0

Location of the ﬁre outbreak
Accurately indicated location (correct ﬂoor and ap-
proximate ﬂoor location)
Location indicated without precision (good ﬂoor only)
Location indicated with a precision error (good ﬂoor
and bad location)
No indication
Wrong location (wrong ﬂoor)
Number of adults present in the building
Exact number
Exact number around plus or minus 1 person
Incorrect number less than at least two persons
No indication
Incorrect number greater than at least two persons
Number of children present in the building
Exact number
Exact number around plus or minus 1 person
Incorrect number less than at least two persons
No indication
Incorrect number greater than at least two persons
Average of each response on each person’s lo-
cation
Precisely indicated location (correct ﬂoor and approx-
imate location on the ﬂoor)
Location indicated without precision (good ﬂoor only)
Location wrongly indicated but with the good ﬂoor
No indication
Wrong Location with the wrong ﬂoor
Bonus: People correctly identiﬁed (adult or child)
Operation time
Participant ﬁnished the exercise before the 6-minute
time limit
Participant completed the exercise at the end of the
6-minute time limit.

3.5 Evaluation

The scenario in section 3.4 has been designed to obtain a
response to the assumptions made in the section 3.2. First
we evaluated a score depending on the results of all par-
ticipants from each mission. Raw results were processed
to measure the eﬃciency of the operations according to
the technologies used and to obtain a percentage success
score. Each question was evaluated according to a score
indicated below. This score resulted from the following
correction in the table 3. From this table, the ﬁnal score
or each participant was a sum of every section for a total
of 17 points converted in a percentage.

We also analyzed the subjective opinions of partici-
pants regarding their feelings on the use of the diﬀerent
technologies. The questionnaire in the table 4 contained
questions directly related to our assumptions to under-
stand the experience of participants. Each question in
the table 4 was answered with values between 0 and 5,
with the following meanings:

0 - No review
1 - Totally disagree
2 - Rather disagree

3 - Neutral
4 - Pretty much agree.
5 - I couldn’t agree more.

1

Table 4: Personalized questionnaire for our experiment
Technical questions on the proposed Augmented
Reality technology
Interacting with the hands on the buttons in augmented
reality was simple and intuitive.
Information from the drones that make up the au-
tonomous system was relevant.

1.1

1.2

1.3 Viewing the video from the UAV cameras in augmented

reality was easy.

1.4 The mini-map tool (2D map) in augmented reality was

useful.

1.5 The UAV fact sheet tool was useful.
1.6 The superimposition of AR on the simulation environ-
ment enhanced a better understanding of the operation
of the UAV system and the situation.

1.7 AR had allowed me to be more conﬁdent in my decisions

during the exercise.
Issues about system diﬃculty

2
2.1 Generally speaking, AR tested for the experiment was

simple to use as a ﬁrst experiment.

2.2 Overall, the 2D computer station technology tested for

2.3

the experiment was simple to use.
Subjectively, what technology you enjoyed most about
exercising.
Logistics Issues

3
3.1 AR had not obstructed my vision in the real world when

I needed it.

3.2 Wearing the headset on my head had not obstructed my

movements in the real world.

3.3 Wearing the helmet on my head had not bothered or hurt

4
4.1

4.2

me.
Questions on Research Issues
I found that there was a lot of information and the expe-
rience was diﬃcult.
For a ﬁrst use, I didn’t have too much diﬃculty to adapt
myself with the AR technology.

4.3 AR allowed me to understand as well or better the situ-

ation compared to the computer.
I was not confused during the experiment.

4.4
4.5 The simulation scenario was relevant.
4.6

I found the experiment interesting.

It was useful to measure the user’s cognitive eﬀort in
order to understand the complexity of the tasks and to
assess whether the information was correctly processed
by the proposed system so that it could be easily under-
stood. On the one hand, it helped to understand the level
of diﬃculty experienced by the participant in terms of the
physical use of the technologies. On the other hand, it im-
proved the management of all the information and actions
to be undertaken for the mental eﬀort. Our questionnaire
contained questions to also compare the comfort of both
technology and especially with the experience with aug-
mented reality systems. For each question, an additional
ﬁeld was present to collect comments in order to detail the
answer choices. Another ﬁeld was also included to collect
additional opinions and comments from the participant.

Hypothesis validation criteria were needed to validate
our research questions. The table 5 contains the set of
validation criteria.

8

Table 5: Criteria for hypothesis validation

Hypothesis Validation criteria
Ha1

Question 1.1 ≥ 3
Question 1.2 ≥ 3
(Question 1.3 + Question 1.4) / 2 ≥ 3
Question 2.1 ≥ 3
Question 1.5 ≥ 3
Question 3.1 ≥ 3
Question 3.2 ≥ 3
Question 3.3 ≥ 4
Question 4.3 ≥ 3
(Question 1.6 + Question 4.2) / 2 ≥ 3
Question 4.2 ≥ 3

Ha2

Ha3
Ha4

Figure 9: Box plot of missions’ results by technology

4 Results

For the proposed experiment comparing PC and AR.
N = 24 participants with no experience with AR at all
and no familiarities with critical situations were asked to
understand a situation where HRB took ﬁre. The repar-
tition of participants was 33% of women and 67% of men
with no familiarity with AR with an HMD. They had
to gather as much information as they feel possible in a
short amount of time with a software application they
never used before. The mission was to report multiple
types of information including ﬁre location, number of
adults and children inside the building with locations for
each person within a time limit of 6 minutes. We set
up a stressful situation by having the participant do the
exercise with unknowns, much feedback during the ex-
ercise and pressure to accomplish the mission in a short
amount of time. Each participant had to complete the
exercise twice. On each mission, the exercise was diﬀer-
ent (we set up two diﬀerent ﬁre scenarios). Half of the
participants started with AR while the other half started
with the PC to get diﬀerent feedback. For each mission,
they reported what they learned which results a score by
the sum of each point earned depending on how right the
information was (correction listed in the table 3). The
score, on a scale of 0 to 17 points is the sum of each sec-
tion. We converted that into a percentage of success (0
to 100%) for readability. The ﬁgure 9 shows the average
score for each technology depending on when they tried
a given technology for the ﬁrst time or second one.
In
order to measure any improvements between the use of
technology depending on if it was used of ﬁrst or second
attempt, we computed an average for each type of infor-
mation reported by participants depending on what they
tried ﬁrst. The ﬁgure 10 represents the average score
converted in percentage for each section of the mission
report to analyze the improvement for both technologies
and make a comparison.

At the end of the test, participants were asked to ﬁll
a personalized questionnaire in the table 4 to have some
feedback from the experience and understand their feel-

Figure 10: Details of missions report results

ings. The average of all answers of these questions (be-
tween 1- strongly disagree to 5- strongly agree) is reported
on the ﬁgure 11.

Figure 11: Responses of personalized questionnaire

5 Discussion

We tried to measure the usefulness of AR to manage an
emergency situation using a complex system. Instead of
testing our work with ﬁreﬁghters due to COVID-19 re-
lated complications, we adapted our experience with peo-
ple without experience in urgency situations. We put the
participant in a stressful situation instead of simple simu-
lation exercise by using a short complex mission with the
pressure of a strong and intense ﬂow of information. The
participant used our technology and also a PC to compare
the relative performance of augmented reality. It was im-
portant to create a feeling of pressure in this situation to
have a real feedback from the participant under pressure
during the exercise with both technologies. It was obvi-
ous that the second attempt was to be more comfortable

9

and less stressful than the ﬁrst one, but we used that to
measure the improvement of the technology after a second
try and evaluated the progress between PC and AR. We
also took into account the interaction method in our work
in augmented reality was fully based on hand interaction
without a controller and was a ﬁrst-time experience for
each participant.

Because each participant needed to do two exercises,
two diﬀerent missions were designed for the experience.
Every information was diﬀerent regarding the mission but
the diﬃculty of both were equivalent. There was no im-
portant diﬀerence according to the score of each technol-
ogy for each kind of mission. Both missions were used for
both technologies and both attempts.

According to the boxplot ﬁgure 9, the average (white
square) of the PC and AR in general
is respectively
Mbc = 68, 44% and Mar = 58, 01% showing a small dif-
ference of Mbc − Mar = 10, 43%. This ﬁgure contains the
minimum (Q0) and maximum (Q4), the median(Q2), the
ﬁrst and third quartile (Q1, Q3). If we check the top 5
best scores we had among all participants, three of them
were from the PC and two of them were from the AR
headset with a score between 82% and 89%. Despite
the fact of a small diﬀerence, the boxplot results from
PC were more stable than AR either the ﬁrst or second
attempt. This stability we explained by the familiarity
of people with the computer. People are normally more
comfortable with a technology they use every day. But
for a ﬁrst try with a new technology in stressful and dif-
ﬁcult exercise, the augmented reality shows here a really
good score relatively close to the eﬃciency of the com-
puter. As an emergent technology, we also see that it
could result in very bad score because people took too
much diﬃculty to handle a critical situation with a tool
they were not well trained for.
In the same ﬁgure 9,
we notice an increase of the averages from ﬁrst attempt
(Mbc1 = 65, 12% and Mar1 = 54, 78%) to the second at-
tempt (Mbc2 = 71, 75% and Mar2 = 61, 25%). Respec-
tively, there is an increase for PC of Mbc2 −Mbc2 = 6, 63%
and for AR of Mar2 − Mar1 = 6, 47%. It makes sense by
the increase of comfort generate by the experience ac-
quired from the ﬁrst attempt. We explore in detail that
the increase is diﬀerent regarding the technology in the
next paragraph. Even if the interquartile range of PC is
smaller than AR and justify the stability of the device,
the minimum and maximum of boxplots are quite large
for the headset resulting a good performance for some
participants even in the ﬁrst try. This shows the possibil-
ity that with practice and habit, the performance could
be much more stable and even better than the computer.
The ﬁgure 10 allows a detailed look at the improvement
of each technology. These numbers are the percentage of
success in each ﬁeld of the mission report following the
indications of the table 3. The best improvement of the

PC is shown on the ﬁre location and time limit. The PC
in blue show a good increase of M f irebc2 − M f irebc1 =
89, 58 − 77, 08 = 12.5% compared to the augmented real-
ity with M f irear2 − M f irear1 = 72, 2 − 68, 75 = 3, 97%
for the ﬁre. “Time limit respected” means if people ﬁn-
ished the exercise before the limit of 6 minutes. We saw
a huge improvement of M timebc2 − M timebc1 = 75, 0 −
41, 67 = 33.33% compared to the headset M timear2 −
M timear1 = 66, 67 − 58, 33 = 8, 34%. But the aver-
age time taken by both technologies were really equal
around 5:40 at the end. We noticed a similar improve-
ment about the number of adults found (M adultbc2 −
M adultbc1 = 2, 09% and M adultar2 − M adultar1 =
2, 08%). On the last two sections (number of children
and people locations), the AR showed a better improve-
ment respectively M childrenar2−M childrenar1 = 62, 5−
52, 08 = 10, 42% and M locationar2 − M locationar1 =
51, 98 − 43, 23 = 8, 75% compared to the PC respectively
M childrenbc2 − M childrenbc1 = 77, 08 − 75, 0 = 2, 08%
and M locationbc2 − M locationbc1 = 59, 13 − 55, 94 =
3, 19%. In general the PC was always better on perfor-
mance for all sections here and could improve better at
some point. But for a ﬁrst performance on stressful and
diﬃcult missions, the augmented reality headset with new
ways of interoperability demonstrated here that perfor-
mance and improvement are nearly close to the computer
for a very ﬁrst use.

Following the missions, participants needed to answer
some questions referred in the table 4 to understand their
feelings mainly about the usability of augmented real-
ity. The average of answers of each question is shown
in the ﬁgure 11. The ﬁrst part was technical questions
about AR technology. We started to ask about the in-
teraction with the hands on the system on Q1.1, an av-
erage of MP Q1.1 = 3, 38 explains an optimism neutral
position on that point. Some participants were really re-
luctant whereas some people really liked the fact to inter-
act on buttons with their ﬁngers which is really promis-
ing for the future. We also ask in Q1.2 how easy it was
to read 2D content (a video) in hologram resulting with
MP Q1.2 = 3, 38. Participants reported that the ﬁeld of
view of the headset is really small and they couldn’t see
well all video correctly and interact with the system at
the same time whereas the computer had everything on
the screen and the use of a mouse and keyboard controller
are more common. The questions Q1.3, Q1.4 and Q1.5
were about widgets’ utility. Q1.3 referred to the mini-map
widget which was really well approved to improve situa-
tional awareness during missions with MP Q1.3 = 3, 91.
Q1.4 wasn’t considered useful at all with a poor result of
MP Q1.4 = 1.95.
In Q1.5, overlaying information above
the high-rise building and drones was a mitigated solu-
tion judged by an average of MP Q1.5 = 3, 38. Indeed, the
most common speciﬁcity of AR is to superimpose digi-

10

tal information on real ﬁeld. After reviewing our experi-
ment, the participant was sitting and didn’t move during
the experience restricting the possibility of the functional-
ity. However knowing that limitation proved that a result
above the neutral level of 3 is not bad at all. Q1.6 ask
the participant of his or her self-conﬁdence by using AR.
The answer MP Q1.6 = 2, 86 indicates that the question
was too early after one use during the experiment and
couldn’t still be correctly measured at that time. The
next part is about the complexity of use about both tech-
nologies (Q2.1 referring to AR and Q2.2 to PC). Both
results present a really good average with MP Q2.1 = 4, 04
and MP Q2.2 = 4.46. The last question Q2.3 was about
the most favourite technology for the experience purpose,
65, 22% of participants choose the PC and 34, 78% the
AR according to MP Q2.3 = 1, 74. This poor result for the
headset can be explained by many reasons. To accomplish
a diﬃcult mission requires a good self-conﬁdence using a
known technology. The headset was a ﬁrst try to all par-
ticipants and never used before which brings unknowns
and uncertainties about the expected results. Partici-
pants seems to spontaneously choose the technology there
were more comfortable with in order to ensure the reli-
ability of their results for the mission. Some comments
were that the augmented reality could be more favorite on
the future if they have more practice with it. The third
part of this questionnaire was about the logistics of Magic
Leap 1 headset. During the exercise, participants took
notes while using the AR or PC. Some of them found out
it was diﬃcult to write and see with the system due to the
hologram’s placement in the software. The question Q3.1
approaches the vision obstruction, Q3.2 about the move-
ment obstruction and Q3.3 on headset-related discomfort
or injury from the Magic Leap 1. Every question resulted
in good average:
[MP Q3.1, MP Q3.2, MP Q3.3] ≥ 4. It was
reported that the headset wasn’t well suitable for small
heads and the cable could disturb a little the use of hands
for interaction. But in general, the headset was really well
accepted by everyone. The last section was about the ex-
perimentation on itself. Q4.1 referred to the diﬃculty of
missions revealing with MP Q4.1 = 3, 29 that participants
were mitigated. It depends again on the self-conﬁdence
about the result of each participant. The next question
Q4.2 asked if their situational awareness was equal or bet-
ter in AR compared to the computer. MP Q4.2 = 3, 92
presented here an important fact that according to peo-
ple, the headset was nearly equal to the computer and it’s
a really great proof of opportunity to demonstrate that
AR could (with more use) be really accepted by people in
general. Q5.3 measured the level of confusion during the
test. A result of MP Q4.3 = 3, 22 shows that missions were
complicated but manageable. The last two questions of
this form was the relevance (Q4.4) and the interest (Q4.5)
of the experimentation. We conclude with MP Q4.4 = 4, 71

and MP Q4.5 = 4, 96 participants really liked the exercises
and considered relevant the experimentation for the aug-
mented reality technology in stressful conditions.

From this experience and our conditions to our
assumptions, we conﬁrm that all of our hypotheses
(Ha1,Ha2,Ha3,Ha4) are valid using our results with the
exception of the condition with the mean of MP Q1.3 and
MP Q1.4 which is just slightly below the required value.
This is explained by the fact that widgets about giving
GPS coordinates and information about drones weren’t
well relevant for the mission. We could say that AR has
an opportunity to make its way into critical situations
one day. We saw here that technology could provide good
results in urgency operations and provide real-time infor-
mation in a real situation instead of managing it all from
a monitoring station. However, this technology is too
emergent for the moment and needs more applications
and tests to deﬁnitively prove something for the future.
Our results showed that people accepted the AR tech-
nology and obtained scores really close to the PC even
if PC was always better. This does not exclude the fact
that with training and more comfort, the augmented re-
ality headset could perform equally or even superior to
the computer. Even in a stressful situation with an over-
ﬂow of information, participants not accustomed to these
conditions showed a good accommodation on this new
technology with novel interactions. We also see that the
improvement in AR was really close to the computer even
in a ﬁrst experiment which is very promising. An impor-
tant complement to these tests could have been an ex-
perience with our ﬁrst work or another application with
people knowing the emergency ﬁeld and could point out
how good or bad that technology could be on a real sit-
uation and not during a simulation. Another good way
for testing it would be to test this exercise on a real situ-
ation outside on a real building simulating people at risk
and the ﬁre with real drones and real video feedback.
The diﬃculty to see holograms outside with the sunlight
is still a question because holograms are made of light
and become really transparent in a lighted ﬁeld. But we
know that technology is increasing day by day really fast
and the new augmented reality headsets showed good im-
provements compared to the Hololens 1.0 in 2016. There
is still some time until we see applications like our work
used in emergency ﬁeld, but we hope that this work will
help to present the opportunity of augmented reality us-
age into critical situations.

6 Conclusion

In this project, we analyzed conditions of Montr´eal’s ﬁre-
ﬁghters and reported about how new technology using
augmented reality could improve humans in critical situ-

11

ations. After the objective’s deﬁnitions, we designed and
developed an augmented reality application as an inter-
face for an AR headset to improve situational awareness
of humans. This interface allowed the user to understand
information provided by a swarm of drones with 2D and
3D widgets in a virtual environment and also allowed him
or her to control a swarm of drones easily to inspect exter-
nal areas. To test our work, we set up a simulation with
virtual environment and augmented reality in a stressful
situation. Our results show that augmented reality has
potential to improve the eﬃciency of humans and improve
the success of critical missions.

7 Acknowledgments

This work was supported by Mitacs [grant number
IT10647] and Humanitas Solutions.
It couldn’t be
achieved without Computer Graphics and Virtual Re-
ality Laboratory (LIRV) from Polytechnique Montr´eal.
We also thank Humanitas Solutions team for all their
work and support for this project. Icon made by Vitaly
Gorbachev, Freepik, Nikita Golubev, Pixelmeetup from
www.ﬂaticon.com.

References

[1] E. Lee, Y. Kwon, The eﬀect of Tactical Situation Display on attack
helicopter pilot’s workload, in: 2014 11th International Conference
on Informatics in Control, Automation and Robotics (ICINCO),
Vol. 02, 2014, pp. 680–684.

[2] M. R. Endsley, B. Bolte, D. G. Jones, B. Bolte, D. G. Jones, De-
signing for Situation Awareness : An Approach to User-Centered
Design, CRC Press, 2003. doi:10.1201/9780203485088.
URL https://www.taylorfrancis.com/books/9781135729493

[3] M. R. Endsley, Situation awareness global assessment technique
(SAGAT), in: Proceedings of the IEEE 1988 National Aerospace
and Electronics Conference, 1988, pp. 789–795 vol.3. doi:10.1109/
NAECON.1988.195097.

[4] R. M. Cliﬀord, H. Khan, S. Hoermann, M. Billinghurst, R. W.
Lindeman, Development of a multi-sensory virtual reality train-
ing simulator for airborne ﬁreﬁghters supervising aerial wildﬁre
suppression, in: 2018 IEEE Workshop on Augmented and Vir-
tual Realities for Good, VAR4Good 2018, March 18, 2018, 2018
IEEE Workshop on Augmented and Virtual Realities for Good,
VAR4Good 2018, Institute of Electrical and Electronics Engineers
Inc., 2018, pp. 1–5. doi:10.1109/VAR4GOOD.2018.8576892.

[5] NASA, NASA-TLX (2019).

URL https://humansystems.arc.nasa.gov/groups/TLX/

[6] Z. Yuan, J. Jin, L. Sun, K.-W. Chin, G.-M. Muntean, Ultra-
Reliable IoT Communications with UAVs: A Swarm Use Case,
IEEE Communications Magazine 56 (12) (2018) 90–96, conference
Name: IEEE Communications Magazine. doi:10.1109/MCOM.2018.
1800161.

[7] N. M. Noor, A. Abdullah, M. Hashim, Remote sensing UAV/drones
and its applications for urban areas: a review, IOP Conference
Series: Earth and Environmental Science 169 (2018) 012003. doi:
10.1088/1755-1315/169/1/012003.

[8] A. Atyabi, S. MahmoudZadeh, S. Nefti-Meziani, Current advance-
ments on autonomous mission planning and management systems:
An AUV and UAV perspective, Annual Reviews in Control 46
(2018) 196–215. doi:10.1016/j.arcontrol.2018.07.002.

URL
S1367578818300257

https://linkinghub.elsevier.com/retrieve/pii/

[9] B. L. Brumitt, A. Stentz, Dynamic mission planning for multiple
mobile robots, in: Proceedings of IEEE International Conference
on Robotics and Automation, Vol. 3, 1996, pp. 2396–2401 vol.3.
doi:10.1109/ROBOT.1996.506522.

[10] D. Carrillo-Zapata, E. Milner, J. Hird, G. Tzoumas, P. J. Var-
danega, M. Sooriyabandara, M. Giuliani, A. F. T. Winﬁeld,
S. Hauert, Mutual Shaping in Swarm Robotics: User Studies
in Fire and Rescue, Storage Organization, and Bridge Inspec-
tion, Frontiers in Robotics and AI 7, publisher: Frontiers. doi:
10.3389/frobt.2020.00053.
URL https://www.frontiersin.org/articles/10.3389/frobt.2020.
00053/full

[11] P. Milgram, H. Takemura, A. Utsumi, F. Kishino, Augmented
reality: a class of displays on the reality-virtuality continuum,
in: Telemanipulator and Telepresence Technologies, Vol. 2351,
International Society for Optics and Photonics, 1995, pp. 282–293.
doi:10.1117/12.197321.
URL
conference-proceedings-of-spie/2351/0000/
Augmented-reality--a-class-of-displays-on-the-reality/10.
1117/12.197321.short

https://www.spiedigitallibrary.org/

[12] B. Hookway, Interface, Cultural studies, MIT Press, 2014.
URL https://books.google.ca/books?id=BQM_AwAAQBAJ

[13] Microsoft, Conception et interface utilisateur pour UWP - Win-

dows UWP applications, date : 2019-05-20 (2019).
URL https://docs.microsoft.com/fr-fr/windows/uwp/design/

[14] Apple, Design User interface, date : 2019-06-13 (2019).

URL
human-interface-guidelines/ios/overview/themes/

https://developer.apple.com/design/

[15] C. Argenta, A. Murphy, J. Hinton, J. Cook, T. Sherrill, S. Snarski,
Graphical user interface concepts for tactical augmented reality,
in: P. L. Marasco, P. R. Havig (Eds.), Head- and Helmet-
Mounted Displays XV: Design and Applications,, Vol. Head- and
Helmet-Mounted Displays XV: Design and Applications,, Orlando,
Florida, 2010, p. 76880I. doi:10.1117/12.849462.
URL
aspx?doi=10.1117/12.849462

http://proceedings.spiedigitallibrary.org/proceeding.

[16] T. Bertrand, L. Moccozet, J.-H. Morin, Augmented Human-
In-
Workplace Interaction: Revisiting Email,
formation Visualisation, At Salerno, Italy, 2018, pp. 1–8. doi:
10.1109/iV.2018.00042.

in: Conference:

[17] H. Sasaki, T. Kuroda, Y. Manabe, K. Chihara, Augmented Real-
ity Based Input Interface for Wearable Computers, in: G. Goos,
J. Hartmanis, J. van Leeuwen, J.-C. Heudin (Eds.), Virtual Worlds,
Vol. 1834, Springer Berlin Heidelberg, Berlin, Heidelberg, 2000, pp.
294–302. doi:10.1007/3-540-45016-5_27.
URL http://link.springer.com/10.1007/3-540-45016-5_27

[18] A. Ajanki, M. Billinghurst, H. Gamper, T. J¨arvenp¨a¨a, M. Kan-
demir, S. Kaski, M. Koskela, M. Kurimo, J. Laaksonen, K. Puo-
lam¨aki, T. Ruokolainen, T. Tossavainen, An augmented reality in-
terface to contextual information, Virtual Reality 15 (2-3) (2011)
161–173. doi:10.1007/s10055-010-0183-5.
URL http://link.springer.com/10.1007/s10055-010-0183-5

[19] Microsoft, HoloLens | Mixed Reality Technology for Business, date

: 2019-05-16 (2019).
URL https://www.microsoft.com/en-us/hololens

[20] M. Leap, Magic Leap One: Creator Edition (2019).
URL https://www.magicleap.com/magic-leap-one

[21] J. Park, P. A. Chou, J. Hwang, Volumetric Media Streaming for
Augmented Reality, in: 2018 IEEE Global Communications Con-
ference (GLOBECOM), 2018, pp. 1–6. doi:10.1109/GLOCOM.2018.
8647537.

[22] A. Das, P. Kol, C. Lundberg, K. Doelling, H. E. Sevil, F. Lewis,
A Rapid Situational Awareness Development Framework for Het-
erogeneous Manned-Unmanned Teams, in: NAECON 2018 - IEEE
National Aerospace and Electronics Conference, 2018, pp. 417–424,
iSSN: 2379-2027. doi:10.1109/NAECON.2018.8556769.

12

[23] A. N. Das, K. Doelling, C. Lundberg, H. E. Sevil, F. Lewis,
A Mixed reality based hybrid swarm control architecture for
manned-unmanned teaming (MUM-T), in: ASME 2017 Interna-
tional Mechanical Engineering Congress and Exposition, IMECE
2017, November 3, 2017 - November 9, 2017, Vol. 14 of ASME
International Mechanical Engineering Congress and Exposition,
Proceedings (IMECE), American Society of Mechanical Engineers
(ASME), 2017, p. ASME. doi:10.1115/IMECE2017-72076.

[24] M. Cousins, C. Yang, J. Chen, W. He, Z. Ju, Development of
a mixed reality based interface for human robot interaction, in:
2017 International Conference on Machine Learning and Cyber-
netics (ICMLC). Proceedings, Vol. vol.1, Piscataway, NJ, USA,
2017, pp. 27 – 34. doi:10.1109/ICMLC.2017.8107738.

[25] F. Brizzi, L. Peppoloni, A. Graziano, E. Di Stefano, C. Avizzano,
E. Ruﬀaldi, Eﬀects of Augmented Reality on the Performance of
Teleoperated Industrial Assembly Tasks in a Robotic Embodiment,
IEEE Transactions on Human-Machine Systems 48 (2) (2018) 197
– 206. doi:10.1109/THMS.2017.2782490.
URL http://dx.doi.org/10.1109/THMS.2017.2782490

[26] S. Trzcielinski, W. Karwowski, W. Karwowski, Advances in Er-
gonomics in Manufacturing, CRC Press, 2012. doi:10.1201/b12322.
URL https://www.taylorfrancis.com/books/9780429107603

[27] J. Patel, Y. Xu, C. Pinciroli, Mixed-Granularity Human-Swarm
Interaction, in: 2019 International Conference on Robotics and
Automation (ICRA), 2019, pp. 1059–1065, iSSN: 2577-087X. doi:
10.1109/ICRA.2019.8793261.

[28] J. Patel, C. Pinciroli, Improving Human Performance Using Mixed
Granularity of Control in Multi-Human Multi-Robot Interaction,
in: 2020 29th IEEE International Conference on Robot and Hu-
man Interactive Communication (RO-MAN), 2020, pp. 1135–1142,
iSSN: 1944-9437. doi:10.1109/RO-MAN47096.2020.9223553.

[29] D. Sullivan, E. Arthur, P. Gardias, Augmented Reality as a Means
of Improving Eﬃciency and Immersion of Human-Swarm Interac-
tion, Major Qualifying Projects (All Years).
URL https://digitalcommons.wpi.edu/mqp-all/7382

[30] E.-A. Conan, Projet supervis´e : Intervenir en situation d’urgence :
L’´ecosyst`eme d’Humanitas Solutions en support aux pompiers de
Montr´eal., hEC Montr´eal, Qu´ebec, Canada (Oct. 2019).

[31] F. Karydes, D. Saussi´e, Distributed algorithm for the navigation of
a swarm of nano-quadrotors in cluttered environments*, in: 2020
International Conference on Unmanned Aircraft Systems (ICUAS),
2020, pp. 100–110, iSSN: 2575-7296. doi:10.1109/ICUAS48674.2020.
9214055.

[32] L. R. Costa, D. Aloise, L. G. Gianoli, A. Lodi, The Covering-
Assignment Problem for Swarm-powered Ad-hoc Clouds: A Dis-
tributed 3D Mapping Use-case, Accepted by IEEE Transactions
on IoTs.

13

