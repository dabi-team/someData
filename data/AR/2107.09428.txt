Streaming End-to-End ASR based on Blockwise Non-Autoregressive Models

Tianzi Wang1, Yuya Fujita2, Xuankai Chang1,3, Shinji Watanabe1,3

1Johns Hopkins University
2Yahoo Japan Corporation
3Carnegie Mellon University
wtianzi1@jhu.edu, yuyfujit@yahoo-corp.jp, {xuankaic,swatanab}@andrew.cmu.edu

1
2
0
2

l
u
J

0
2

]
S
A
.
s
s
e
e
[

1
v
8
2
4
9
0
.
7
0
1
2
:
v
i
X
r
a

Abstract

Non-autoregressive (NAR) modeling has gained more and more
attention in speech processing. With recent state-of-the-art
attention-based automatic speech recognition (ASR) structure,
NAR can realize promising real-time factor (RTF) improve-
ment with only small degradation of accuracy compared to the
autoregressive (AR) models. However, the recognition infer-
ence needs to wait for the completion of a full speech utter-
ance, which limits their applications on low latency scenarios.
To address this issue, we propose a novel end-to-end stream-
ing NAR speech recognition system by combining blockwise-
attention and connectionist temporal classiﬁcation with mask-
predict (Mask-CTC) NAR. During inference, the input audio is
separated into small blocks and then processed in a blockwise
streaming way. To address the insertion and deletion error at
the edge of the output of each block, we apply an overlapping
decoding strategy with a dynamic mapping trick that can pro-
duce more coherent sentences. Experimental results show that
the proposed method improves online ASR recognition in low
latency conditions compared to vanilla Mask-CTC. Moreover,
it can achieve a much faster inference speed compared to the
AR attention-based models. All of our codes will be publicly
available at https://github.com/espnet/espnet.
Index Terms: Non-autoregressive speech recognition, stream-
ing ASR, blockwise-attention, Mask-CTC

1. Introduction

Over the past years, the advances in deep learning have dra-
matically boosted the performance of end-to-end (E2E) auto-
matic speech recognition (ASR) [1–3]. Most of these E2E-
ASR studies were based on autoregressive (AR) models and
they achieved state-of-the-art performance [4]. However, there
is a disadvantage of the AR models in that the inference
time linearly increases with the output length. Recently, non-
autoregressive (NAR) models has gained more and more atten-
tion in sequence-to-sequence tasks, including machine trans-
lation [5–7], speech recognition (ASR) [1, 8–11], and speech
translation [12]. In contrast to the AR modeling, NAR mod-
eling can predict the output tokens concurrently, the inference
speed of which is dramatically faster than AR modeling. Espe-
cially, connectionist temporal classiﬁcation (CTC) is a popular
and simple NAR modeling [1,6]. However, CTC makes a strong
conditional independent assumptions between the predicted to-
kens, leading to an inferior performance compared to the AR
attention-based models [13].

To overcome this issue, several NAR studies have been pro-
posed in the ASR ﬁeld. A-FMLM [14] is designed to predict the
masked tokens conditioning on the unmasked ones and the in-
put speech. However, it needs to ﬁrst predict the output length,
which is difﬁcult and easily leads to a long output sequence.

Imputer [8] directly used the length of input feature sequence
to address the issue and achieves comparable performance with
AR models, but the computational cost can be very large. ST-
NAR [15] used CTC to predict the target length and to guide the
decoding. It is fast but suffers from large accuracy degradation
compared with AR models. Different from previous methods,
Mask-CTC [16] ﬁrst generates the output tokens with greedy
decode of a CTC, and then reﬁnes the tokens which have low
conﬁdence by a mask-predict decoder [7]. Mask-CTC usually
predicts sequences with reasonable length and can achieve a fast
inference speed, 7x faster than the AR attention-based model.

In addition to fast inference, latency is an important fac-
tor to be considered for the ASR system used in a real-time
speech interface. There have been a lot of prior studies for
low-latency E2E ASR based on Recurrent Neural Networks
Transducer (RNN-T) [2, 13, 17, 18] and online attention-based
encoder decoder (AED) with AR models, such as monotonic
chunkwise attention (MoChA) [19,20], triggered attention [21],
and blockwise-attention [22, 23].

Motivated by such online AR AED studies and emergent
NAR research trends, we propose a novel end-to-end streaming
NAR speech recognition system, by combining the blockwise-
attention and Mask-CTC models. During inference, the in-
put audio is ﬁrst separated into small blocks with 50% over-
lap between consecutive blocks. CTC ﬁrstly predicts the pre-
liminary tokens per block with an efﬁcient greedy forward
pass based on the output of a blockwise-attention encoder. To
address the insertion and deletion error of CTC outputs fre-
quently appeared at the boundary of each block, we apply a dy-
namic overlapping strategy [24] to produce coherent sentences.
Then, low-conﬁdence tokens are masked and re-predicted by
a mask-predict NAR decoder [25] conditioning on the rest to-
kens. The greedy CTC, dynamic overlapping decoding, and
mask-prediction all perform very fast, thus can achieve quite
low RTF. We evaluated our approach on TEDLIUM2 [26] and
AISHELL1 [27]. Compared to vanilla full-attention Mask-
CTC, our proposed method decreases the online ASR recog-
nition error rate in a low latency condition, with very fast infer-
ence speed. To the best of our knowledge, this is the ﬁrst work
that extending the NAR mechanism into streaming ASR.

1.1. Relationship with other streaming ASR studies

The most important success of streaming ASR recently is RNN-
T and its variants. RNN-T based systems achieve state-of-the-
art ASR performance for streaming applications and are suc-
cessfully deployed in production systems [17, 18, 28, 29]. How-
ever, the recurrent mechanism predicts the token of the current
input frame based on all previous tokens using recurrent lay-
ers, to which NAR cannot be easily applied. Besides, several
ideas in this paper are inspired from online AR AED architec-

 
 
 
 
 
 
3.1. blockwise-attention Encoder

To build a streaming AED-based ASR system, the encoder
is only allowed to access limited future context. We use a
blockwise-attention (BA) based encoder [22,23] instead of nor-
mal multi-headed self-attention (MHSA). In a BA based en-
coder, the input sequence X is divided into ﬁx-length blocks
X = [Xb]B
b=0, where Xb = [xbl, . . . , x(b+1)l−1]. Here b is the
block index, B is the index of the last block in the whole in-
put and l is the block length. In the computation of blockwise-
attention, each block only attends to the former layer’s output
within the current block and the previous block. Blockwise-
attention at the b-th block is deﬁned as:

Z i

b = BA(Z i−1

b

) = MHSA(Z i−1

b

, [Z i−1

b−1, Z i−1

b

], [Z i−1

b−1, Z i−1
(4)

b

]),

where Z i
b is the output of encoder layer i at b-th block, Z 0 = X.
The three arguments of MHSA(·) are query, key, and value ma-
trix variables, respectively. Likewise, the blockwise-depthwise-
convolution (BDC) in Conformer encoder is deﬁned as:

BDC(Z i−1

b

) = CONV(BPAD(Z i−1

b

, [Z i−1

b−1, Z i−1

b

])),

(5)

where CONV(·) is 1D depth-wise convolution and BPAD(·)
refers to blockwise-padding, that pads zeros at right edges and
pads Z i
to keep the input/output dimen-
sion identical. The rest operations, such as point-wise convolu-
tion and activation function, is the same as in Conformer [31].

b−1 at left edges of Z i−1

b

3.2. Blockwise Mask-CTC

As shown in Figure 1, the proposed E2E streaming NAR speech
recognition system consists of a blockwise-attention based en-
coder, a CTC, a dynamic mapping process, and an MLM de-
coder. During training, we use full audio as the input for conve-
nience. But the CTC output within a block only depends on the
input in the current and previous block, since the computation
of the encoder is blockwise. In this paper, following the NAR
manner, we applied greedy decoding for CTC, which selects the
token with the highest probability at each time step. The output
of each block from greedy decoding CTC is:

πb = Concatyt [arg max
yti,b

P (yti,b |BAEncoder(Xb))]

(6)

where b ∈ [0, .., B], ti,b denotes the i-th time step belonging
to the b-th block, ti,b ∈ [t0,b, . . . , tl−1,b]. BAEncoder refers to
the blockwise-attention encoder as mentioned in Sec 3.1, X−1
is set to be a zero-matrix with the same size as Xb during com-
putation. At the same time, the ground-truth target tokens are
randomly masked and re-predicted by the MLM decoder.

During inference, the input is online segmented into ﬁxed-
length blocks with 50% overlap and fed into the encoder in a
streaming way. The encoder forward pass and the CTC decod-
ing follow the same way as in the training. As shown in Figure
1, the dotted line denotes CTC output yb−1 corresponds to in-
put block Xb−1, and solid line denotes CTC output yb of Xb.
A dynamic mapping trick is applied on the overlap tokens be-
tween yb−1 and yb to make a coherent output. More details will
be shown in Sec. 3.3. Then the tokens with low-conﬁdence
scores from CTC decoding outputs, π0..B, are masked and re-
predicted by MLM decoder. The predicting process is done in
several iterations. In each iteration, tokens with higher predict-
ing probability are ﬁlled into masked positions and the re-ﬁlled

Figure 1: Architecture of proposed streaming NAR

tures [22–24], since they have more technical connections with
NAR models based on the similar encoder-decoder framework.

2. Mask-CTC

Mask-CTC is a non-autoregressive model trained with both
CTC objective and mask-prediction objective [16], where the
mask-predict decoder predicts the masked tokens based on CTC
output tokens and encoder output. CTC predicts a frame-level
input-output alignment based on conditional independence as-
sumption between frames. It models the probability P (Y |X) of
output sequence by summing up all possible alignments, where
Y denotes the sequence of output and X denotes the input au-
dio. However, due to the conditional independence assumption,
CTC loses the ability of modeling correlations between output
tokens and consequently loses performance.

Mask-CTC was designed to mitigate this issue by adopting
an attention-based decoder as a masked language model (MLM)
[7, 30], and iterative reﬁning the output of CTC greedy decod-
ing. During training, the tokens in the ground-truth are ran-
domly selected and replaced by a special (cid:104)mask(cid:105) token. Then
the decoder is trained to predict the actual tokens at the masked
positions, Ymask, conditioning on the rest unmasked tokens, Yobs,
and attention-based encoder output. Henc.

Henc = MHSAEncoder(X),
(cid:89)

(1)

PMLM(Ymask|Yobs, Henc) =

P (ymask|Yobs, Henc). (2)

ymask∈Ymask

where the MHSAEncoder(·) denotes a multi-headed self-
attention based encoder. The Mask-CTC model is optimized
by a weighted sum of the CTC and MLM objectives:

L =γ log Pctc(Y |X) + (1 − γ) log PMLM(Ymask|Yobs, X), (3)

where γ is a tunable hyper-parameter.

3. Streaming NAR

The overall architecture of the proposed E2E streaming NAR
speech recognition system is shown in Figure 1. The main dif-
ference compared with Mask-CTC is that the normal MHSA-
based encoder (e.g. Transformer/Conformer) as shown in
Eq. (1) is replaced by a blockwise-attention encoder to make
the model streamable.

sequences are used as input for the next iteration.

ˆY0 = mask(πb)

yk,m = arg max log PMLM(ymask| ˆYm−1, X)

(7)

(8)

where ˆY0 denotes the masked token sequence from CTC output,
m denotes the iteration number of re-prediction, yk,m denotes k
re-predicted tokens in mth iteration, yk,m are then inﬁlled into
the predicted sequences ˆYm−1 at corresponding masked posi-
tions to form the mth predition ˆYm. ˆYN would be the ﬁnal
output as N is total number of iterations.

3.3. Dynamic mapping for overlapping inference

Although splitting the input audio into small blocks with ﬁxed-
length is a straightforward approach to form streaming ASR,
it will result in horrible performance degradation at the block
boundaries. A segment boundary may appear in the middle of a
token, leading to that one token may have repetitive recognition
or non-recognition in two consecutive blocks. The VAD-based
segmentation method is a way to solve the issue, but it is sensi-
tive to the threshold and may lead to large latency.

In this paper, we applied overlapping inference with dy-
namic mapping tricks as in [24] to recover the erroneous output
at the boundary, as shown in Algorithm 1. During inference,
we use 50% overlap when segmenting the input audio, which
ensures any frame of input audio is predicted twice by Encoder
and CTC. By locating the token index in yb that is closest to the
center point of Cb, we dynamically search for the best align-
ment between (yb,:idxb and yb−1,idxb−1:). Normalize refers to
removing repeated and blank token from CTC output Cb. Fol-
lowing the scoring function: Score(tb
j) = −|j − (l − 1)/2|, we
select one of the token in token pairs on the best alignment path
as the output of the overlapped segment. Here tb
j refers to the
j-th token in b-th block, and l is block length.

Algorithm 1 CTC overlap decode and dynamic map
1: yout = [∅];
2: X = Audio blocks iterator with 50% overlap
3: for b = 0 to B do
4:
5:
6:
7:

Cb = CTC Predict(BAEncoder(Xb))
if b > 0 then

yb = remove repeated tokens in Cb
idxb = token index in yb that is closest to Cb, 1
2 l
pathb = Alignment(yb,:idxb , yb−1,idxb−1:)
yout,b = [∅]
for (p, q) in pathb do

token = p if Score(p)>Score(q) else q
APPEND token to yout,b

8:
9:
10:
11:
12:
13:
14:
15:
16:

end for
yout,b = Normalize(yout,b)

else

yout,b = Normalize(Cb, 1

2 l)

end if
APPEND yb to yout

17:
18:
19: end for

4. Experiment

4.1. Experimental setup and Dataset

We evaluate our proposed model on both Chinese and English
Speech corpora: TEDLIUM2 [26] and AISHELL1 [27]. For

all experiments, the input features are 80-dimensional log-mel
ﬁlter-banks with pitch computed with frame length of 25ms
and frame shift of 10ms. We use Kaldi toolkit [32] for feature
extraction. We also apply speed perturbation(speed rate=0.9,
1.0, 1.1) and spectrum augmentation [33] for data augmenta-
tion. Models are evaluated with both full-context decoding and
streaming decoding.

All experiments are conducted using the open-source, E2E
speech processing toolkit ESPnet [31, 34, 35]. The encoder ﬁrst
contains 2 CNN blocks to downsample the input to 1/4, fol-
lowed by 12 MHSA-based layers. In streaming NAR, BA-based
layers(Eq. 4, 5) is used in encoder to replace MHSA. Decoders
have a similar stacked-block structure with 6 layers and only
full-attention transformer blocks. For any self-attention block
in this paper, we use h = 4 parallel attention heads, with di-
mension datt = 256. For feed-forward layer, we use dimension-
ality dffn = 2048 and apply swish as activation functions. In
self-attention, relative position embedding is augmented in the
input [36]. We use 15 as the kernel size for Conformer convolu-
tion. Models on TEDLIUM2 are trained for 200 epochs and on
AISHELL1 are trained for 150 epochs. The evaluation is done
on the averaged model over the best 10 checkpoints. We do not
integrate Language Model during decoding.

4.2. Results

Table 1 shows TEDLIUM2 results, including word error rates
(WERs) on dev/test sets, averaged latency and real-time factor
(RTF). Latency and RTF were measured on the CPU platform
(Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.3GHz) with 8 paral-
lel jobs. We set block length l = 16, corresponding to 640ms
before subsampling. The full-context evaluation is done on the
utterance level, while the streaming evaluation is done on the
unsegmented streaming audio for each speaker. Since it is hard
to deﬁne the latency with long unsegmented audio, we calcu-
lated the averaged latency per utterance with dev:

(cid:80)

Latency =

utt(last token emitted − end of speech)
#total number of utterance

.

(9)

The last token emitted time is the time stamp that model pre-
dicts last token, and the end of speech is determined by forced
alignment with external model. This latency considered both
look-ahead latency and computation time. For comparison, we
also report the latency for full-context decoding by measuring
the average decoding time per utterance, since in full-context
case decoding can not start before entire audio is fed.

Compared to the vanilla Mask-CTC, the proposed stream-
ing NAR model performs much better in streaming mode. With
the Conformer encoder, the WER on the dev of Mask-CTC is
23.5 with 310ms averaged latency and that of streaming NAR
is 12.1 with 140ms averaged latency. The RTF is about 2x/10x
faster than the AR attention-based model with beam size=1/10
respectively. Mask-CTC models are trained with full context
input and work better with full future information. We can
observe a signiﬁcant WER degradation in streaming decoding
mode, which is due to the input mismatch during training and
decoding. On the other hand, the proposed streaming NAR can
recognize the current frame with only a small future context,
thus ﬁt well with the streaming inference. However, the WERs
increased from full context to streaming mode in the proposed
model (from 10.4/9.4 to 12.1/11.7). The reason would be the
error raised at the segment boundaries. From our observation,
these errors can be relieved but not totally solved by dynamic
mapping and still exist even with large block lengths.

Table 1: TEDLIUM2: WERs on dev/test, averaged Latency and RTF are reported. 640ms input segments is used for all streaming
decode mode. The attention-based AR and Mask-CTC models trained with conventional attention, while the proposed streaming NAR
use blockwise attention with 640ms block length.

Encoder type

Decode Mode WER on dev WER on test Latencyutt(ms) RTF

Attention-based AR

Mask-CTC

Streaming NAR (Proposed)

Transformer

+ beamsize=10

Conformer

+ beamsize=10

Transformer
Conformer
Transformer
Conformer

Transformer
Conformer
Transformer
Conformer

full-context
full-context
full-context
full-context

full-context
full-context
streaming
streaming

full-context
full-context
streaming
streaming

11.7
10.9
11.0
11.1

11.0
10.0
18.2
23.5

12.2
10.4
14.2
12.1

9.9
9.1
8.4
8.1

10.7
8.8
16.4
21.3

11.2
9.4
14.0
11.7

5220
34160
6130
37780

790
1070
300
310

910
1030
120
140

0.46
3.01
0.54
3.33

0.07
0.09
0.20
0.26

0.08
0.09
0.22
0.32

Table 2: AISHELL1: WERs, averaged latency and RTF are reported. 1280ms input segments is used for all streaming inference.
The attention-based AR and Mask-CTC models work with conventional attention, while the proposed streaming NAR use blockwise
attention with 1280ms block length

Encoder type

Decode Mode WER on dev WER on test Latencyutt(ms) RTF

Attention-based AR

Mask-CTC

Streaming NAR (Proposed)

Transformer

+ beamsize=10

Transformer
Transformer

Transformer
Transformer

full-context
full-context

full-context
streaming

full-context
streaming

6.7
6.6

6.9
9.0

8.6
8.6

7.6
7.4

7.8
10.4

9.9
9.9

2040
11640

220
280

230
320

0.45
2.56

0.05
0.18

0.05
0.20

Table 3: WERs and RTF on TEDLIUM2 conduct on proposed
Streaming NAR model with different block length(BL), Experi-
ments are all conducted under streaming mode.

Encoder Dev

BA-TF

BA-CF

12.8
12.8
13.0
14.2

10.5
10.6
11.2
12.1

Test

12.0
11.9
12.2
14.0

10.3
10.4
10.7
11.7

BL(ms) Latencyutt(ms) RTF
0.17
0.18
0.18
0.22

1530
700
290
120

5120
2560
1280
640

5120
2560
1280
640

1700
790
380
140

0.29
0.29
0.30
0.32

Table 2 shows the results on AISHELL1. Since the hyper-
parameters for Conformer and Mask-CTC need careful tuning
and the model is easy to be overﬁtted in our preliminary exper-
iments, we only report transformer results on the AISHELL1
task. The vanilla Mask-CTC works better when full-context
is provided during decoding while streaming NAR is better on
streaming decode, but the beneﬁts are smaller than those in
TEDLIUM2. A possible reason is that the training utterance
in AISHELL1 is much shorter than TEDLIUM2. Full-context
attention can also gain the ability to recognize with only short
future context.

To understand the performance of streaming NAR under
different latency, in Table 3 we compare the WERs with differ-
ent block lengths for blockwise-attention Transformer (BA-TF)

and blockwise-attention Conformer (BA-CF) on TEDLIUM2.
We observe that as the block length get shorter, the latency
becomes smaller while RTF rises since the shorter blocks re-
quire more iterations to forward the whole input audio. Besides,
when block length decreases from 5120ms to 1280ms, the result
rarely changes. It indicates that in streaming ASR, the closer fu-
ture context is a much more active player than distant ones.

5. Conclusion

In this paper, we proposed a novel E2E streaming NAR speech
recognition system. Speciﬁcally, we combined the blockwise-
attention based Encoder and Mask-CTC. Beside, we applied
the dynamic overlapping inference to mitigate the errors at
the boundary. Compared to vanilla Mask-CTC, the proposed
streaming NAR model achieves competitive performance in
full-context decoding and outperforms the vanilla Mask-CTC
streaming decoding with very low utterance latency. Moreover,
the decoding speed of the proposed model is about 2x/10x faster
than the AR attention-based model with beam size=1/10 respec-
tively. Our future plan is developing better boundaries localiza-
tion method to replace the overlapping inference, and integrat-
ing external language model during decoding.

6. Acknowledgements

We would like to thank Mr. Yusuke Higuch of Waseda Univer-
sity for his valuable information about the Mask-CTC.

7. References
[1] A. Graves, S. Fern´andez, F. Gomez, and J. Schmidhuber, “Con-
nectionist
labelling unsegmented se-
quence data with recurrent neural networks,” in Proceedings of
the 23rd international conference on Machine learning, 2006, pp.
369–376.

temporal classiﬁcation:

[2] A. Graves, “Sequence transduction with recurrent neural net-

works,” arXiv preprint arXiv:1211.3711, 2012.

[3] J. Chorowski, D. Bahdanau, K. Cho, and Y. Bengio, “End-to-end
continuous speech recognition using attention-based recurrent nn:
First results,” arXiv preprint arXiv:1412.1602, 2014.

[4] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han,
S. Wang, Z. Zhang, Y. Wu et al., “Conformer: Convolution-
augmented transformer for speech recognition,” arXiv preprint
arXiv:2005.08100, 2020.

[5] J. Gu, J. Bradbury, C. Xiong, V. O. Li, and R. Socher,
“Non-autoregressive neural machine translation,” arXiv preprint
arXiv:1711.02281, 2017.

[6] J. Libovick`y and J. Helcl, “End-to-end non-autoregressive neural
machine translation with connectionist temporal classiﬁcation,”
arXiv preprint arXiv:1811.04719, 2018.

[7] M. Ghazvininejad, O. Levy, Y. Liu, and L. Zettlemoyer, “Mask-
predict: Parallel decoding of conditional masked language mod-
els,” in in Proc. EMNLP-IJCNLP, 2019, pp. 6114–6123.

[8] W. Chan, C. Saharia, G. Hinton, M. Norouzi, and N. Jaitly,
“Imputer: Sequence modelling via imputation and dynamic pro-
gramming,” in International Conference on Machine Learning.
PMLR, 2020, pp. 1403–1413.

[9] Y. Fujita, S. Watanabe, M. Omachi, and X. Chang, “Insertion-
based modeling for end-to-end automatic speech recognition,”
Proc. Interspeech 2020, pp. 3660–3664, 2020.

[10] E. A. Chi, J. Salazar, and K. Kirchhoff, “Align-reﬁne: Non-
autoregressive speech recognition via iterative realignment,”
arXiv preprint arXiv:2010.14233, 2020.

[11] R. Fan, W. Chu, P. Chang, and J. Xiao, “Cass-nat: Ctc
alignment-based single step non-autoregressive transformer for
speech recognition,” arXiv preprint arXiv:2010.14725, 2020.

[12] H. Inaguma, Y. Higuchi, K. Duh, T. Kawahara, and S. Watanabe,
“Orthros: Non-autoregressive end-to-end speech translation with
dual-decoder,” arXiv preprint arXiv:2010.13047, 2020.

[13] E. Battenberg, J. Chen, R. Child, A. Coates, Y. G. Y. Li, H. Liu,
S. Satheesh, A. Sriram, and Z. Zhu, “Exploring neural trans-
ducers for end-to-end speech recognition,” in 2017 IEEE Auto-
matic Speech Recognition and Understanding Workshop (ASRU).
IEEE, 2017, pp. 206–213.

[14] N. Chen, S. Watanabe, J. Villalba, and N. Dehak, “Listen and ﬁll
in the missing letters: Non-autoregressive transformer for speech
recognition,” arXiv preprint arXiv:1911.04908, 2019.

[15] Z. Tian, J. Yi, J. Tao, Y. Bai, S. Zhang, and Z. Wen, “Spike-
triggered non-autoregressive transformer for end-to-end speech
recognition,” arXiv preprint arXiv:2005.07903, 2020.

[16] Y. Higuchi, S. Watanabe, N. Chen, T. Ogawa, and T. Kobayashi,
“Mask ctc: Non-autoregressive end-to-end asr with ctc and mask
predict,” arXiv preprint arXiv:2005.08700, 2020.

[17] A. Tripathi, J. Kim, Q. Zhang, H. Lu, and H. Sak, “Transformer
transducer: One model unifying streaming and non-streaming
speech recognition,” arXiv preprint arXiv:2010.03192, 2020.

[18] M. Jain, K. Schubert, J. Mahadeokar, C.-F. Yeh, K. Kal-
gaonkar, A. Sriram, C. Fuegen, and M. L. Seltzer, “Rnn-t for la-
tency controlled asr with improved beam search,” arXiv preprint
arXiv:1911.01629, 2019.

[19] C.-C. Chiu and C. Raffel, “Monotonic chunkwise attention,”

arXiv preprint arXiv:1712.05382, 2017.

[20] H. Inaguma, M. Mimura, and T. Kawahara, “Enhancing mono-
tonic multihead attention for streaming asr,” arXiv preprint
arXiv:2005.09394, 2020.

[21] N. Moritz, T. Hori, and J. Le, “Streaming automatic speech recog-
nition with the transformer model,” in ICASSP 2020-2020 IEEE
International Conference on Acoustics, Speech and Signal Pro-
cessing (ICASSP).

IEEE, 2020, pp. 6074–6078.

[22] H. Miao, G. Cheng, C. Gao, P. Zhang, and Y. Yan, “Transformer-
based online ctc/attention end-to-end speech recognition architec-
ture,” in ICASSP 2020-2020 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP).
IEEE, 2020,
pp. 6084–6088.

[23] E. Tsunoo, Y. Kashiwagi1, and S. Watanabe, “Streaming trans-
former asr with blockwise synchronous beam search,” in Proc.
SLT, 2021, pp. 22–29.

[24] C.-C. Chiu, W. Han, Y. Zhang, R. Pang, S. Kishchenko,
P. Nguyen, A. Narayanan, H. Liao, S. Zhang, A. Kannan et al.,
“A comparison of end-to-end models for long-form speech recog-
nition,” in 2019 IEEE Automatic Speech Recognition and Under-
standing Workshop (ASRU).

IEEE, 2019, pp. 889–896.

[25] Y. Higuchi, H.

and
T. Kobayashi, “Improved mask-ctc for non-autoregressive end-to-
end asr,” arXiv preprint arXiv:2010.13270, 2020.

Inaguma, S. Watanabe, T. Ogawa,

[26] A. Rousseau, P. Del´eglise, Y. Esteve et al., “Enhancing the ted-
lium corpus with selected data for language modeling and more
ted talks.” in LREC, 2014, pp. 3935–3939.

[27] H. Bu, J. Du, X. Na, B. Wu, and H. Zheng, “Aishell-1: An
open-source mandarin speech corpus and a speech recognition
baseline,” in 2017 20th Conference of the Oriental Chapter of
the International Coordinating Committee on Speech Databases
and Speech I/O Systems and Assessment (O-COCOSDA).
IEEE,
2017, pp. 1–5.

[28] B. Li, S.-y. Chang, T. N. Sainath, R. Pang, Y. He, T. Strohman, and
Y. Wu, “Towards fast and accurate streaming end-to-end asr,” in
ICASSP 2020-2020 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP).
IEEE, 2020, pp. 6069–
6073.

[29] J. Mahadeokar, Y. Shangguan, D. Le, G. Keren, H. Su, T. Le, C.-F.
Yeh, C. Fuegen, and M. L. Seltzer, “Alignment restricted stream-
ing recurrent neural network transducer,” in 2021 IEEE Spoken
Language Technology Workshop (SLT).
IEEE, 2021, pp. 52–59.
[30] N. Chen, S. Watanabe, J. Villalba, and N. Dehak, “Nonautoregres-
sive transformer automatic speech recognition,” arXiv preprint
arXiv:1911.04908, 2019.

[31] P. Guo, F. Boyer, X. Chang, T. Hayashi, Y. Higuchi, H. Inaguma,
N. Kamo, C. Li, D. Garcia-Romero, J. Shi et al., “Recent devel-
opments on espnet toolkit boosted by conformer,” arXiv preprint
arXiv:2010.13956, 2020.

[32] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek,
N. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz,
J. Silovsky, G. Stemmer, and K. Vesely, “The kaldi speech recog-
nition toolkit,” in IEEE 2011 Workshop on Automatic Speech
Recognition and Understanding.
IEEE Signal Processing So-
ciety, Dec. 2011, iEEE Catalog No.: CFP11SRW-USB.

[33] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D.
Cubuk, and Q. V. Le, “Specaugment: A simple data augmen-
tation method for automatic speech recognition,” arXiv preprint
arXiv:1904.08779, 2019.

[34] S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishitoba,
Y. Unno, N. Enrique Yalta Soplin, J. Heymann, M. Wiesner,
N. Chen, A. Renduchintala, and T. Ochiai, “ESPnet: End-to-end
speech processing toolkit,” in Proceedings of Interspeech, 2018,
pp. 2207–2211. [Online]. Available: http://dx.doi.org/10.21437/
Interspeech.2018-1456

[35] S. Karita, N. Chen, T. Hayashi, T. Hori, H. Inaguma, Z. Jiang,
M. Someki, N. E. Y. Soplin, R. Yamamoto, X. Wang et al., “A
comparative study on transformer vs rnn in speech applications,”
in 2019 IEEE Automatic Speech Recognition and Understanding
Workshop (ASRU).

IEEE, 2019, pp. 449–456.

[36] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, and R. Salakhut-
dinov, “Transformer-xl: Attentive language models beyond a
ﬁxed-length context,” arXiv preprint arXiv:1901.02860, 2019.

