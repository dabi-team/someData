A NOVEL 3D-UNET DEEP LEARNING FRAMEWORK BASED ON HIGH-DIMENSIONAL
BILATERAL GRID FOR EDGE CONSISTENT SINGLE IMAGE DEPTH ESTIMATION

Mansi Sharma1, Abheesht Sharma2, Kadvekar Rohit Tushar3, Avinash Panneer4

Department of Electrical Engineering, Indian Institute of Technology Madras, India1,3,4
Department of Computer Science & Information Systems, BITS Pilani, K K Birla Goa Campus2

1
2
0
2

y
a
M
1
2

]

V
C
.
s
c
[

1
v
9
2
1
0
1
.
5
0
1
2
:
v
i
X
r
a

ABSTRACT

The task of predicting smooth and edge-consistent depth maps is no-
toriously difﬁcult for single image depth estimation. This paper pro-
poses a novel Bilateral Grid based 3D convolutional neural network,
dubbed as 3DBG-UNet, that parameterizes high dimensional fea-
ture space by encoding compact 3D bilateral grids with UNets and
infers sharp geometric layout of the scene. Further, another novel
3DBGES-UNet model is introduced that integrate 3DBG-UNet for
inferring an accurate depth map given a single color view. The
3DBGES-UNet concatenates 3DBG-UNet geometry map with the
inception network edge accentuation map and a spatial object’s bound-
ary map obtained by leveraging semantic segmentation and train the
UNet model with ResNet backbone. Both models are designed with
a particular attention to explicitly account for edges or minute de-
tails. Preserving sharp discontinuities at depth edges is critical for
many applications such as realistic integration of virtual objects in
AR video or occlusion-aware view synthesis for 3D display applica-
tions.The proposed depth prediction network achieves state-of-the-
art performance in both qualitative and quantitative evaluations on
the challenging NYUv2-Depth data. The code and corresponding
pre-trained weights will be made publicly available.

Index Terms— Monocular depth estimation, computational pho-
tography, edge-aware image processing, convolutional neural net-
work, deep learning, bilateral grid, VR/AR, 3D display.

1. INTRODUCTION

Monocular depth estimation is an ill-posed problem yet highly de-
manding for augmented or mixed reality, robotics and in general for
3D computer vision applications. With the advent of deep learn-
ing, especially convolutional neural networks, several methods have
been proposed to solve this problem with desirable results. The algo-
rithms either rely on supervised learning or self-supervised learning
approaches [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].

Despite recent advances in monocular depth estimation, preserv-
ing sharp discontinuities at depth edges or object’s boundaries re-
main difﬁcult to estimate correctly. Some methods incorporate ad-
ditional information of sharp edges or occluding contours that con-
strain the depth predictions during learning. For example, Rama-
monjisoa and Lepetit [8] enforce depths and normals geometry con-
sensus to learn the object’s contours. Swami et al. [9] added a fully
differentiable variant of ordinal regression in depth estimates. Chen
et al. [8] progressively add ﬁner structures while recovering coarser
layout at multiple scales. In general, the major reasons of lack of
precise edges or object’s boundary details in predicting depth are
many. First, the networks build on depth annotations of training im-
ages obtained with a stereo algorithm or a structured RGB-D camera
are probable to be inaccurate around the object’s boundaries. This

Fig. 1: Sharp geometric layout of the scene recovered using pro-
posed 3DBG-UNet model.

is the case with NYUv2-Depth dataset [23]. The 3D points around
the edges or occluding contours may not be visible in all training
views. The structured RGB-D cameras based on stereo technique,
where one viewpoint is replaced with the known pattern suffer from
the same problem. Secondly, the minute edges around objects and
occluding regions may not inﬂuence the loss function during train-
ing unless explicitly handled with deﬁnite care. Most of the existing
solutions for monocular depth estimation often produce blurry ap-
proximations of low resolution. While latest algorithms have shown
steadily increasing performance, there are still major problems in
both the quality and the resolution of estimating depth maps. This
paper addresses these critical problems in monocular depth estima-
tion. Our contributions are twofold:

• We introduce a novel Bilateral Grid based 3D CNN model,
dubbed as ‘3DBG-UNet’, that infers sharp geometric layout
of the scene. Given a single color image, the model learns to
generate a geometric map that captures minute details and ob-
ject boundaries more faithfully. The key insight of the 3DBG-
UNet model is to naturally replace spatial convolution layers
with 3D bilateral grids and learn on high-dimensional image
representation that enables localization and capture the scene
context precisely.

• We propose another new convolutional neural network, called
3DBGES-UNet, for computing detailed high resolution depth
map given a single color view. This integrated model consists
of a new building block (3DBG-UNet) that lifts the image
onto a 3D bilateral grid in order to extract geometric infor-
mation in an edge-preserving way. Subsequently, it combines
the geometry map of 3DBG-UNet with the inception network

 
 
 
 
 
 
Fig. 2: An overview of proposed 3DBG-UNet architecture for sharp geometric layout of the scene.

edge accentuation map and a spatial object’s boundary map to
reﬁne the depth via a UNet architecture on the ResNet back-
bone. Numerical results indicate state-of-the-art performance
on NYUv2-Depth data.

The results demonstrate that integrating the geometry perspective
of bilateral grids in our 3D UNet models augment the training and
guiding a network to learn better weights for edge consistent output
with fewer parameters and less training iterations. Both proposed
models are versatile and could be useful in providing complementary
geometry-suggestions to any existing supervised or self-supervised
depth prediction schemes, not just our own.

2. RELATED WORK

We brieﬂy categorize the learning techniques according to the dif-
ferent generalizations of this work. We summarize algorithms for
single-image depth estimation and also discussed models that learn
general forms of high-dimensional data for vision and image pro-
cessing applications.

2.1. Monocular depth estimation

We discuss the recent methods for monocular depth estimation us-
ing supervised and self-supervised learning technique. Godard et al.
[1] proposed a self-supervised monocular depth estimation method
based on a minimum re-projection loss, an auto-masking loss, a full-
resolution multi-scale sampling. The network deals with occlusions
between frames in monocular video and ﬂexible to train with monoc-
ular video data, stereo data, or mixed monocular and stereo data.

Tosi et al.

[2] leverage stereo matching in order to improve
monocular depth estimation. The deep architecture, monoResMatch,
proposed by them infers depth from a single image by synthesizing
features from a virtual right viewpoint, horizontally aligned with the
original view, and further performing stereo matching between the
two cues. The network is trained end-to-end from scratch.

Xu et al. [3] integrate continuous Conditional Random Fields
into an end-to-end deep architecture. This helps in fusing multi-scale
information derived from different layers of a CNN. Their structured
attention model automatically regulates the information transferred
between corresponding features at different scales.

Watson et al.

[4] analysed ambiguous re-projection in depth-
prediction from stereo-based self-supervision. They alleviate this
effect by introducing complementary depth-suggestions, dubbed as

Depth Hints. Such hints are computed from simple off-the-shelf
stereo algorithms to enhance the photometric loss function and guid-
ing the network to learn better weights.

Hu et al. [5] considered fusion strategy of extracting features at
different scales and also minimize inference errors used in training
by measuring loss functions. They reconstructed depth maps with
ﬁner resolution with small objects and object boundaries. Alhashim
and Wonka [6] transfer learning approach recovers high-resolution
depth map that capture object boundaries. Their CNN model lever-
ages standard encoder-decoder models alongside transfer learning to
predict depth of a single RGB image.

Ramamonjisoa and Lepetit [7] SharpNet introduced geometry
consensus in multi-task training to pay particular attention to oc-
cluding contours. Swami et al. [9] proposed a fully differentiable
variant of ordinal regression for depth estimation. They trained their
network in end-to-end fashion. Chen et al. [10] predict multi-scale
depth maps by progressively adding ﬁner structures at a speciﬁc
scale while preserving the coarser layout.

While the performance of monocular depth estimation meth-
ods has been increasing steadily, general problems with resolution,
preserving accurate object’s boundaries and quality of the predicted
depth maps leave a lot of room for improvement. This paper utilizes
the principle of bilateral grids in a clever way that separate pixels by
their spatial and range dimension and allow edge-aware operations
with simple 3D “fully convolutional network” for depth estimation
at expected resolution.

2.2. Neural networks for high-dimensional data processing

The CNNs designed for high-dimensional data representation made
good progress in recent years. The diverse roles of machine learning
algorithms where the coordinates of underlying data representation
have a grid structure have been demonstrated for several computer
vision and image/video processing applications. Bruna et al. [11]
propose deep neural network on graphs following the convolutions
on irregular grids. Their graph deﬁnes high-dimensional convolu-
tion using “locally” connected and pooling layers. This constraint
the graph structure for adding new samples since the weights in the
graph nodes determine a notion of locality. The relatively weak reg-
ularity assumption on their graph with ﬁxed local receptive ﬁelds
make construction complex to induce weight sharing across differ-
ent locations, and thus reusability is a challenge with their costly
spectral construction.

Jaderberg et al.

[12] introduce a learnable module called the

Fig. 3: An overview of the proposed integrated 3DBGES-UNet model for edge-consistent depth estimate from a single image.

‘Spatial Transformer’, which explicitly allows the spatial data ma-
nipulation within the neural network. This spatial transformation
capability enables to learn invariance for a given task in a parameter
efﬁcient manner. Ionescu et al. [13] present a sound mathematical
framework that generalizes adjoint matrix variations for backpropa-
gation architectures. This enables global structured matrix computa-
tions, inclusion of structural layers such as normalized-cuts, higher-
order pooling into deep computation architectures. Ben Graham [14]
extends 2D CNNs to sparse 3D CNNs. The experiments with CNNs
on the 2D triangular-lattice and 3D tetrahedral-lattice are presented
in [14] for 3D object recognition and space-time analysis of objects.
Jampani et al. [15] suggests to replace the spatial convolution
layers of CNN architectures with bilateral ﬁlters, which have a vary-
ing spatial receptive ﬁeld depending on the image content. This
helps to overcome constraints caused by ﬁxed local receptive ﬁelds,
and enables 3D ﬁltering with sparse samples handled by a permuto-
hedral lattice data structure.

The idea proposed in this paper is absolutely different from one
[15] and
“splat-blur-slice” paradigm presented by Jampani et al.
other lattice based 3D CNN models [11, 12, 13, 14]. There is no
processing performed inside the grids. Instead, we use the princi-
ple of bilateral grid data structure in respecting strong edges that
come from the inclusion of the range in extra third dimension. This
enables Euclidean edge-aware image manipulation meaningful with
grid-based deep network layers in determining good localization and
capturing scene context at the same time by learning from the data
itself.

3. PROPOSED CNN ARCHITECTURES

Nonlinear ﬁlters have enabled a variety of image enhancement and
editing or manipulation techniques where image structure is taken
into account. In particular, the bilateral ﬁlter is a nonlinear process
that respects strong edges while smoothing an image [16]. Bilateral
ﬁltering has been used in a variety of contexts in computational pho-
tography or image processing applications such as tone mapping, in-
painting, style transfer, denoising, relighting, and more applications.
A common drawback of nonlinear ﬁlters such as bilateral ﬁlter is
the computational complexity for processing high-deﬁnition content
[16].

Paris and Durand [21] extend the fast bilateral ﬁlter presented

by Durand and Dorsey [22]. They recast the bilateral ﬁltering as
a higher-dimensional space linear convolution followed by trilinear
interpolation and a division. This linear ﬁltering process, further,
speeds up the operation. Chen et al. [16] generalize the ideas behind
Paris and Durand [21] higher dimensional space into a new com-
pact data structure, the bilateral grid, that enables a number of edge-
aware image manipulations on high resolution images in real time.
A bilateral grid is a 3D representation of a 2D image that separates
the pixels not only by spatial position or coordinate, but also by re-
spective intensity value or range coordinate. Chen et al. [16] deﬁned
bilateral grid as a three dimensional array, where the ﬁrst two dimen-
sions (x, y) in spatial domain correspond to the 2D position in the
image plane, while the third dimension z corresponds to a reference
range. The reference axis corresponds to the image intensity in most
cases. Given a reference image I, Chen et al. [16] describes the im-
age manipulation as a three step procedure. First, create a bilateral
grid from an image. Then, perform processing inside the grid. Fi-
nally, extract a 2D value map by accessing the grid at (x, y, I(x, y)),
typically performing data interpolation and slicing operation. Thus,
processing on the grid between construction and slicing (symmetric
operations) is what enables edge-aware operations in their approach.
The required sampling rate is determined by the particular grid op-
eration. In practice, most grid operations require only a coarse res-
olution. Usually, the number of grid cells is much smaller than the
number of image pixels.

In our proposed framework, we do not perform the process-
ing inside the grid. Instead, edge-aware properties of the bilateral
grid are exploited in learning deep 3D-UNet model for edge con-
sistent depth map prediction from the data itself.
It is critical to
note that in the spatial dimension of an image, although two pix-
els across an edge are close, but from the bilateral grid perspective,
they are distant because their values differ widely in the range di-
mension of the data structure. This ensures that only a limited num-
ber of intensity values will get affected around edge pixels, while
performing 3D convolution operations. Besides, modeling bilateral
grids in CNN layers allow the network to simultaneously localize in
two-dimensional spatial domain while operating on one-dimensional
range dimension to see the context.

We propose two architectures in the next section. The ﬁrst 3DBG-
UNet model is used for recovering sharp geometric layout of the
scene. It operates taking bilateral grids as input and gives a bilateral

3DBG-UNet for Coarse Geometry EstimationBilateral GridsBilateral Scene Geometry MapDexiNedDense Extreme Inception Network forEdge DetectionRGB ImageThin Edge MapLight WeightRefineNetRefine-Net for Semantic SegmentationRGB ImageSegmentation Map2D U-NetWith ResNet50 BackboneEdge Consistent Depth Map(480, 640, 1)(480, 640, 1)(480, 640, 3)(480, 640, 5)(concatenate)grid as an output. The 3DBG-UNet architecture is inspired from
previous 2D U-Net architecture of Ronneberger et al. [17]. All oper-
ations in our 3DBG-UNet are performed in three dimensional space.
In particular, it is built on 3D convolutions, 3D max pooling, and 3D
up-convolutional layers. Besides, we avoid bottlenecks in the net-
work architecture for smooth processing of bilateral grids and use
3D batch normalization for faster convergence.

The second one, dubbed as 3DBGES-UNet, is designed to re-
cover detail high-resolution depth map. It combines geometric out-
put of the 3DBG-UNet network with an edge and a semantic seg-
mentation map to reﬁne the ﬁnal depth via a deep U-Net architec-
ture. Both models are designed to learn from a single monocular
image with a particular attention to faithfully account for ﬁne details
and prominent edges.

3.1. Proposed 3DBG-UNet architecture for learning scene geo-
metric layout

The 3DBG-UNet architecture is illustrated in Fig. 2. The ﬁrst build-
ing block of 3DBG-UNet lifts the original image in a 3D bilateral
grid. The high-dimensional image representation via bilateral grid
is obtained by accumulating the value of each input pixel into the
appropriate grid voxel. Let a grayscale image I be represented as
I(x, y) = ι, where (x, y) encodes the 2D pixel position in the image
plane and form the spatial domain. The ι denotes intensity value at
the pixel position (x, y). Let an image I be normalized to [0, 1]. The
corresponding bilateral grid ΓBG is constructed as follows:

• For all grid nodes (a, b, c) ∈ ΓBG, initialize as,

ΓBG(a, b, c) = (0, 0)

(1)

• For each pixel position (x, y):

ΓBG([

x
srs

], [

y
srs

], [

ι
srr

]) + = (ι, 1)

(2)

where, [·] is the closest-integer operator, srs is sampling rate of the
spatial axis, and srr denotes the sampling rate of the range axis. The
range axis is image intensity. The amount of smoothing is governed
by srs, while the degree of edge preservation is controlled by srr.
The bilateral grid representation (2) is constructed for a grayscale
image. To handle color images, the higher dimensional 5D grids
need to be created considering dimension for RGB color channels.
Constructing and processing 5D grids in CNNs is practically difﬁ-
cult. In proposed formulation, we instead operate on three bilateral
grids separately, one for each color channel. That is, we consider
each color channel image as a grayscale image and apply (2) to it.
Let the RGB image be represented as I(x, y) = (ιr, ιg, ιb). We,
therefore, represent the bilateral grid for each channel c ∈ (r, g, b),
as follows:

• For all grid nodes (a, b, c) ∈ ΓBGc , initialize as,

ΓBGc (a, b, c) = (0, 0)

(3)

• For each pixel position (xc, yc):

ΓBGc ([

xc
srs

], [

yc
srs

], [

ιc
srr

]) + = (ιc, 1)

(4)

After lifting the original image in high dimensional space, the
bilateral grid is processed along the 3D contracting path and a 3D ex-
panding path. On the left side, the architecture consists of a 3D con-
tracting path. A 3D expanding path is depicted on the right side. The

typical architecture along the 3D contracting path, i.e. the encoder
of 3DBG-UNet, is composed of two 3D sub-blocks corresponding to
each block. Each sub-block consists of a 3D convolutional layer with
a kernel size of 5, stride of 1 and padding of size 2. This is followed
by a rectiﬁed linear unit (ReLU) which act as the activation func-
tion and a 3D batch-normalization layer [30]. Each encoder block is
followed by 3D max-pooling layer with kernel size 2. The 3D max-
pooling layer is used to perform downsampling between blocks of
the encoder.

Along the expanding path of the network, each block contains
repeated two sub-blocks similar as in the encoder. Every step in the
expanding path consists of an upsampling of the feature map, a con-
catenation with the output of the corresponding encoder layer from
the contracting path and the previous decoder layer. For upsampling
between decoder blocks, a transposed 3D convolutional layer is em-
ployed with a kernel size of 4, stride of 2 and padding of size 1. The
feature channels in upsampling part in 3DBG-UNet allows to prop-
agate scene context and layout details to higher resolution layers.
The ﬁnal layer in our proposed 3DBG-UNet architecture consists of
sigmoid activation, which restricts the pixel values of the Bilateral
geometry grid to the range [0, 1].

The Mean Squared Error or MSE is the default loss function

used in the proposed model. The MSE loss is deﬁned as follows:

M SEloss =

1
n

(cid:88)

(y∗

i − yi)2

yi∈|n|

(5)

where y∗ is the predicted depth map and y is the ground truth depth
map, and n is the number of pixels in the batch-size training images.
The 3DBG-UNet model is trained for 150 epochs on NYU-Depth
v2 data using around 20K samples for training and 694 samples for
testing.

The 3DBG-UNet architecture is easily regulated for RGB im-
ages by modifying the number of input channels from 1 to 3. We
make some modiﬁcations to extend all operations in 3D along the
contracting and expanding path of 3DBG-UNet. First, the image’s
resolution can be quite large, and adding a third dimension can cause
a increase in grid size. Therefore, we decrease the spatial resolution
of the bilateral grids by 2 or 4. Usually, the range of the pixel values
in images be 0−255. However, since bilateral grids will have a third
dimension representing the intensity or range axis, the size will be
large if we take the range of the pixel values to be 0 − 255. Thus,
we restrict the pixel values to lie in a smaller range. If the range is
0 − 31, the intensity dimension is 32, i.e., we choose a sampling rate
for the reference axis to be 8. If the range is 0 − 63, the intensity
dimension is 64, respectively, i.e., the sampling rate for the reference
axis is 4.

3.2. Proposed 3DBGES-UNet for monocular depth estimation

We proposed a new 3DBGES-UNet architecture for monocular depth
estimation based on 3DBG-UNet model. The coarse geometric lay-
out of the scene recovered by 3DBG-UNet model is further pro-
cessed for estimating reﬁned depth map from single monocular im-
age. The geometry map of the scene computed from 3DBG-UNet,
following the procedure in section 3.1, is highly pixelated, but well
highlights the edges and preserve object’s boundaries (Fig. 1). This
is due to the fact that we reduce the spatial dimensions of the image,
and keep only 0−31 or 0−63 range for the intensity or range dimen-
sion for fast computation. However, this limits to encode the depth
variance in the scene properly. We reﬁne the geometry output in our
3DBGES-UNet architecture. The integrated model is illustrated in
Fig. 3.

The 3DBG-UNet geometry output is converted into the grayscale
image. The geometric information recovered has pronounced edges
or sharp details (see Fig. 1). To recover high-resolution depth map,
the 3DBGES-UNet concatenate 3DBG-UNet geometry map with the
inception network edge accentuation map and a spatial semantic seg-
mentation map, both extracted from the RGB color image.

We adopt a Light-Weight ReﬁneNet [19] for semantic image
segmentation. ResNet-152 is considered as the backbone in [19].
The semantic segmentation information distinguishes boundary of
the object in an image and helps in reﬁning pixelated output of 3DBG-
UNet. Further, we adopt a DexiNed model [20] for accounting thin
edge-maps that are plausible for human eyes, but not discernible in
3DBG-UNet course geometric layout.

We leverage information from the 3DBG-UNet geometry map,
semantic segmentation map and the edge map in our proposed inte-
grated 3DBGES-UNet model. The shapes of bilateral-grid geome-
try map, the segmentation map and the edge map are (480, 640, 1),
(480, 640, 3) and (480, 640, 1) respectively. The three maps in par-
allel are concatenated along the channel axis to form a tensor of the
shape (480, 640, 5). Further, it is fed into a 2D-UNet model with
a ResNet-50 backbone [18] and trained end-to-end. The 3DBGES-
UNet model is trained for 150 epochs on NYU-Depth v2 data con-
sidering 20K samples for training and 694 samples for testing.

4. EXPERIMENTS AND RESULTS

In this section, we describe experimental results and perform com-
parative analysis of proposed 3DBGES-UNet model with the state-
of-the-art CNN algorithms for monocular depth estimation. We eval-
uated different methods on benchmark NYU-Depth V2 dataset in
order to compare the robustness of our models.

4.1. Implementation Details

We implemented proposed 3DBG-UNet and 3DBGES-UNet models
in PyTorch. Both training and evaluation are performed on a single
high-end HP OMEN X 15-DG0018TX Gaming laptop with 9th Gen
i7-9750H, 16 GB RAM, RTX 2080 8 GB Graphics and Windows 10
operating system. Training took around 8 hours for the 3DBG-UNet
and 11 hours for the 3DBGES-UNet network. Test prediction takes
0.61 seconds per image with 3DBG-UNet model and 0.18 seconds
per image with 3DBGES-UNet model.

4.1.1. NYU-Depth V2 data

NYU-Depth V2 dataset is comprised of a variety of indoor scenes,
captured by Microsoft Kinect as RGB-D video sequences [23]. The
color images and depth maps are provided at a resolution of 640 ×
480. The training set has 120K samples [26]. We train our models
on a 20K subset with ﬁlled in depth values using the colorization
scheme of [27]. We validated on 694 image pairs from the NYU-
Depth v2 test data.

• root mean squared error (RMSE):

(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
n

n
(cid:88)

(yp − ˆyp)2

p

(6)

• average (log10) error:

1
n

n
(cid:88)

p

|log10(yp) − log10(ˆyp)|

(7)

where, yp is a pixel in depth image y, ˆyp is a pixel in the predicted
depth image ˆy, and n is the total number of pixels for each depth
image. Note that numerical results in Table 1 might vary from the
original papers because we evaluated these methods with the same
code and used the authors provided pre-trained models to compute
their depth map predictions.

In Table 3, we compare RMSE values of several state-of-the-art
algorithms on NYU-Depth V2 data given at the website [41]. We
choice to compare models not using extra training data. The com-
parison is performed with Zhu et al. [31] (SOM), Xu et al. [32]
DeepLabV3+ [33], Multi-Task Light-Weight-ReﬁneNet [34], Rel-
ativeDepth [35], SC-SfMLearner-ResNet18 [36], SDC-Depth [37],
ACAN [38], DORN [39], and SENet-154 [40] depth estimation meth-
ods. The results in Table 1 and Table 2 are quite impressive and on-
par with state-of-the-art methods.
Qualitative Analysis: We perform qualitative analysis of different
methods on the NYU-Depth V2 test data. The perception-based
qualitative metric and depth edge reliability metric are computed,
as deﬁned by [6]. We computed the mean structural similarity score
(mSSIM) that measures the similarity of resulting depth maps in the
image space. The mSSIM scores are computed considering gray
scale visualization of the ground truth as a reference and estimated
depth map as a predicted image. Mathematically, the mSSIM quality
score is computed as:

1
N

T
(cid:88)

i

SSIM (yi, ˆyi)

(8)

We assess the mean structural similarity term over the entire NYU-
Depth V2 test data.

The second measure, depth edge reliability metric (DERM), anal-
yse edges in the predicted depth maps and ground truth. The gradient
magnitude image of both the ground truth and the predicted depth
image are determined for each sample. The standard Sobel gradient
operator is considered to generate gradient of the depth image [28].
Further, the gradient magnitude image is threshold and the F score
(also called F1 score) is determined to measure the model accuracy
[29]. We choose threshold values greater than 0.5. The F (or F1)
scores given in Table 1 and Table 2 are averaged across the test set.
The visual comparison results are shown in Fig. 4. Our proposed
3DBGES-UNet model produces higher quality depth maps compare
with those generated by other CNN algorithms. The edges are better
predicted and matched those of the ground truth with fewer artifacts.

4.1.2. Evaluation & Comparative Analysis

4.2. Relevance of 3DBG-UNet model

We quantitatively compare our 3DBGES-UNet model with CNN al-
gorithms Cantrell et al. [18] (Serial U-Net), Ramamonjisoa et al.
[8] (SharpNet), Eigen et al. [26] (MSDN) in Table 1. We evaluate
these methods using most common error metrics from prior works
[18, 8, 26]. The error metrics are deﬁned as:

We performed a critical experiment to assert the signiﬁcance of our
3DBG-UNet model in the integrated 3DBGES-UNet model. We
consider different combinations of inputs and trained three mod-
els, called as “RGB+Seg+Edge”, “RGB+Seg” and “RGB+Edge” re-
spectively. In “RGB+Seg+Edge” model, we concatenate the RGB

(a) Input

(b) GT

(c) 3DBGES-UNet (Ours)

(d) Serial U-Net

(e) MSDN

(f) SharpNet

Fig. 4: Visual comparison of estimated depth maps.

image (instead of considering 3DBG-UNet CNN model geometry
map), the segmentation map (Seg), and the edge map (Edge). The
RGB+Seg+Edge inputs are fed to the 2D-UNet stage of the architec-
ture (See Fig. 3). In “RGB+Seg” model, we concatenate the RGB
image and the segmentation map and fed to the 2D-UNet. Likewise,
in “RGB+Edge” model, the RGB image and the edge map inputs are
fed to the 2D-UNet stage respectively. All models are trained for 150
epochs. The output of three different models is compared with our
3DBGES-UNet model. The metric values in Table 2 validate that the
depth quality decreases with a different combination of input in the
three models. Thus, the numerical results in Table 2 and depth maps
depicted in Fig. 5 established that 3DBG-UNet CNN geometry map
is essential in our 3DBGES-UNet model for inferring high quality
detailed depth map with sharp edges and object’s boundaries.

5. CONCLUSIONS

We proposed novel deep neural network architectures, 3DBG-UNet
and 3DBGES-UNet, based on high dimensional bilateral grid space
for detailed high-resolution depth estimation from a single RGB im-
age. Integrating geometry perspective of bilateral grids with learning
strategies enable faithful capturing of strong edges and object bound-
aries in a scene. Evaluation on challenging benchmark NYUv2-
Depth dataset demonstrate that our proposed model achieves state-
of-the-art performance, both quantitatively and qualitatively. Our
proposed self-supervised 3DBG-UNet is a versatile model that can
provide complementary geometry-suggestions to any existing super-
vised or self-supervised depth prediction schemes, not just our own,

to augment the training and guiding a network to learn better weights
for edge consistent output.

Following our simple 3DBG-UNet architecture, one avenue for
future work is to combine with other best practices. We plan to sub-
stitute proposed bilateral grid encoder-decoder with a more compact
one with parallel architectures, associated design choices with an ef-
fective loss functions and train with monocular video data, stereo
data, or mixed monocular and stereo data, for superior predictions.
Another future direction of research is to consider how to efﬁciently
handle higher dimensional 6D bilateral grids in proposed learning
approach for depth extraction from video. We shall look for another
possibility in coupling dimensionality reduction techniques and plug-
ging proxy-supervision in Bilateral CNNs at training time that could
lead to more efﬁcient networks. We will further explore the possi-
bilities of deploying proposed ideas on interesting applications such
as image manipulation or editing, image-to-image transfer, 3D view
rendering, contrast enhancement, refocusing, object recognition, re-
alistic integration of virtual objects in augmented reality.

6. REFERENCES

[1] Clement Godard, Oisin Mac Aodha, Michael Firman, Gabriel
Brostow, Digging Into Self-Supervised Monocular Depth Esti-
mation, arXiv:1806.01260, 2018.

[2] F. Tosi, F. Aleotti, M. Poggi and S. Mattoccia, Learning Monoc-
ular Depth Estimation Infusing Traditional Stereo Knowledge,
CVPR 2019, pp. 9791-9801.

Table 1: Comparison of different methods on the NYU-Depth V2 data. Higher values indicate better quality for mSSIM and DERM measures
while lower values are better for RMSE and log10.

Serial U-Net [18]

SharpNet [8] MSDN [26]

RMSE ↓
log10 ↓
mSSIM ↑
DERM ↑

0.2089
0.2442
0.7482
0.3608

0.4654
0.3926
0.5440
0.4685

0.2071
0.2093
0.8042
0.3334

Ours
epochs 50
0.1907
0.2061
0.8980
0.6266

Ours
epochs 75
0.1929
0.2083
0.9010
0.6256

Ours
epochs 100
0.1885
0.2112
0.9072
0.6325

Ours
epochs 125
0.1855
0.2069
0.9044
0.6446

Ours
epochs 150
0.1857
0.2070
0.9145
0.6523

Table 2: Relevance of 3DBG-UNet model in 3DBGES-UNet model. Here higher values indicate better quality for mSSIM and DERM
measures, while lower values are better for RMSE and log10.

RGB+Seg+Edge RGB+Seg RGB+Edge

RMSE ↓
log10 ↓
mSSIM ↑
DERM ↑

0.2078
0.2456
0.7758
0.4222

0.2089
0.2442
0.7482
0.3608

0.2321
0.2745
0.7565
0.4007

Ours
0.1857
0.2070
0.9145
0.6523

Table 3: Comparison of RMSE values of proposed depth estimation
method with state-of-the-art models on NYU-Depth V2 data [41].

Methods
Zhu et al. [31] (SOM)
Xu et al. [32]
DeepLabV3+ [33]
Multi-Task Light-Weight-ReﬁneNet [34]
RelativeDepth [35]
SC-SfMLearner-ResNet18 [36]
SDC-Depth [37]
ACAN [38]
DORN [39]
SENet-154 [40]
Our (3DBGES-UNet)

RMSE
0.604
0.586
0.575
0.565
0.538
0.536
0.497
0.496
0.509
0.530
0.1857

[11] Joan Bruna, Wojciech Zaremba, Arthur Szlam, Yann Lecun,
Spectral networks and locally connected networks on graphs,
International Conference on Learning Representations, CBLS,
April 2014.

[12] Max Jaderberg, Karen Simonyan, Andrew Zisserman,
Transformer Networks,

Spatial

Koray Kavukcuoglu,
arXiv:1506.02025, 2016.

[13] C. Ionescu, O. Vantzos, and C. Sminchisescu, Matrix Back-
propagation for Deep Networks with Structured Layers, IEEE
International Conference on Computer Vision, 2015.

[14] Ben Graham, Sparse 3D convolutional neural networks,

abs/1505.02890, 2015.

[15] M. Kiefel, V.

and Peter V. Gehler, Sparse
Convolutional Networks using the Permutohedral Lattice,
arXiv:1503.04949, 2015.

Jampani,

[3] Dan Xu, Wei Wang, Hao Tang, Hong Liu, Nicu Sebe, Elisa
Ricci, Structured Attention Guided Convolutional Neural Fields
for Monocular Depth Estimation, CVPR 2018.

[4] J. Watson, M. Firman, G. J. Brostow, D. Turmukhambetov,
Self-Supervised Monocular Depth Hints, ICCV 2019, pp. 2162-
2171.

[5] Junjie Hu, Mete Ozay, Yan Zhang, Takayuki Okatani, Revisit-
ing Single Image Depth Estimation: Toward Higher Resolution
Maps With Accurate Object Boundaries, WAVC 2019.

[6] I. Alhashim and P. Wonka, High Quality Monocular Depth Es-
timation via Transfer Learning, arXiv:1812.11941, 2018.
[7] S. Khamis, S. Fanello, C. Rhemann, A. Kowdle, J. Valentin,
S. Izadi, StereoNet: Guided Hierarchical Reﬁnement for Real-
Time Edge-Aware Depth Prediction, ECCV 2018.

[8] M. Ramamonjisoa and V. Lepetit, SharpNet: Fast and Accurate
Recovery of Occluding Contours in Monocular Depth Estima-
tion, In ICCV workshop - 3D Reconstruction in the Wild, 2019.
[9] Kunal Swami, Prasanna Vishnu Bondada, Pankaj Kumar Bajpai,
AcED: Accurate and Edge-consistent Monocular Depth Estima-
tion, IEEE ICIP 2020.

[16] Jiawen Chen, Sylvain Paris, and Fr´edo Durand, Real-time
edge-aware image processing with the bilateral grid, ACM
Trans. Graph., 26(3), 2007.

[17] O. Ronneberger, P. Fischer, and T. Brox, U-Net: Convo-
lutional Networks for Biomedical Image Segmentation, arXiv
1505.04597, 2015.

[18] Kyle J. Cantrell, Craig D. Miller and Carlos W. Morato, Prac-
tical Depth Estimation with Image Segmentation and Serial U-
Nets, In proceedings of the 6th International Conference on
Vehicle Technology and Intelligent Transport Systems, Vol. 1,
2020, pp. 406-414.

[19] Vladimir Nekrasov, Chunhua Shen, Ian Reid, Light-Weight Re-
ﬁneNet for Real-Time Semantic Segmentation, BMVC 2018.

[20] Xavier Soria and Edgar Riba and Angel Sappa, Dense Extreme
Inception Network: Towards a Robust CNN Model for Edge De-
tection, IEEE WACV, 2020.

[21] S. Paris and F. Durand, A Fast Approximation of the Bilat-
eral Filter Using a Signal Processing Approach, Proc. European
Conf. Computer Vision, 2006.

[10] Xiaotian Chen , Xuejin Chen and Zheng-Jun Zha, Structure-
Aware Residual Pyramid Network for Monocular Depth Esti-
mation, IJCAI 2019: 694-700.

[22] Fredo Durand and Julie Dorsey, Fast bilateral ﬁltering for
the display of high-dynamic-range images, ACM Trans. Graph.,
21(3), July 2002, pp. 257-266.

(a) Input

(b) GT

(c) 3DBGES-UNet (Ours)

(d) RGB+Seg+Edge

(e) RGB+Seg

(f) RGB+Edge

Fig. 5: Visual comparison of 3DBGES-UNet against models with different combination of inputs.

[34] Nekrasov et al., Real-Time Joint Semantic Segmenta-
tion and Depth Estimation Using Asymmetric Annotations,
arXiv:1809.04766, 2019.

[35] J. Lee and C. Kim, Monocular Depth Estimation Using Rela-

tive Depth Maps, IEEE CVPR, pp. 9721-9730, 2019.

[36] Bian et al., Unsupervised Depth Learning in Challenging In-
door Video: Weak Rectiﬁcation to Rescue, arXiv:2006.02708,
2020.

[37] L. Wang, J. Zhang, O. Wang, Z. Lin and H. Lu, SDC-Depth:
Semantic Divide-and-Conquer Network for Monocular Depth
Estimation, IEEE CVPR, pp. 538-547, 2020.

[38] Yuru Chen, Haitao Zhao, Zhengwei Hu, Attention-based Con-
text Aggregation Network for Monocular Depth Estimation,
arXiv:1901.10137, 2019.

[39] Fu et al., Deep Ordinal Regression Network for Monocular

Depth Estimation, arXiv:1806.02446, 2018.

[40] J. Hu, M. Ozay, Y. Zhang, and T. Okatani, Revisiting Single
Image Depth Estimation: Toward Higher Resolution Maps with
Accurate Object Boundaries, arXiv:1803.08673, 2018.

[41] https://paperswithcode.com/sota/

monocular-depth-estimation-on-nyu-depth-v2

[23] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus, Indoor seg-
mentation and support inference from RGBD images, In pro-
ceedings of the 12th European conference on Computer Vision,
Volume Part V. Springer-Verlag, Berlin, Heidelberg, 746–760.

[24] Faraj Alhwarin, Alexander Ferrein Ingrid Scholl, IR Stereo
Improving Depth Images by Combining Structured
Kinect:
Light with IR Stereo, Paciﬁc Rim International Conference on
Artiﬁcial Intelligence, PRICAI 2014, pp 409-421.

[25] I. Laina, C. Rupprecht, V. Belagiannis, F. Tombari and N.
Navab, Deeper Depth Prediction with Fully Convolutional
Residual Networks, Fourth International Conference on 3D Vi-
sion, Stanford, CA, 2016, pp. 239-248.

[26] David Eigen, Christian Puhrsch, and Rob Fergus, Depth map
prediction from a single image using a multi-scale deep network,
In Proceedings of the 27th International Conference on Neu-
ral Information Processing Systems, Volume 2 (NIPS’14). MIT
Press, Cambridge, MA, USA, 2366-2374.

[27] Anat Levin, Dani Lischinski, and Yair Weiss, Colorization
using optimization, ACM Trans. Graph. 23, 3, August 2004,
689–694.

[28] Sobel and G. Feldman, A 3x3 Isotropic Gradient Operator for
Image Processing, Pattern Classiﬁcation and Scene Analysis,
1973, pp. 271-272.

[29] Cyril Goutte and Eric Gaussier, A probabilistic interpretation
of precision, recall and F-score, with implication for evaluation,
In Proceedings of the 27th European conference on Advances in
Information Retrieval Research, Springer-Verlag, Berlin, Hei-
delberg, pp. 345-359.

[30] Sergey Ioffe and Christian Szegedy, Batch Normalization: Ac-
celerating Deep Network Training by Reducing Internal Covari-
ate Shift, arXiv:1502.03167v3, 2015.

[31] Jing Zhu, Yunxiao Shi, Mengwei Ren, Yi Fang, Kuo-Chin
Lien, Junli Gu, Structure-Attentioned Memory Network for
Monocular Depth Estimation, arXiv:1909.04594, 2019.

[32] D. Xu, E. Ricci, W. Ouyang, X. Wang and N. Sebe, Multi-scale
Continuous CRFs as Sequential Deep Networks for Monocular
Depth Estimation, IEEE CVPR, pp. 161-169, 2017.

[33] S. Gur and L. Wolf, Single Image Depth Estimation Trained via
Depth From Defocus Cues, IEEE CVPR, pp. 7675-7684, 2019.

