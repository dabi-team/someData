On the precision matrix of an irregularly
sampled AR(1) process

Benjamin Allévius1

1Department of Mathematics, Stockholm University, Sweden

Abstract

Irregularly sampled AR(1) processes appear in many computationally demanding
applications. This text provides an analytical expression for the precision matrix of
such a process, and gives eﬃcient algorithms for density evaluation and simulation,
implemented in the R package irregulAR1.

Keywords: AR(1) process, time series, precision matrix, missing data.

8
1
0
2

y
a
M
9
2

]
E
M

.
t
a
t
s
[

2
v
1
9
7
3
0
.
1
0
8
1
:
v
i
X
r
a

 
 
 
 
 
 
1

Introduction

Autoregressive (AR) processes are widely used to model time series with de-
pendence. One particular application is in disease surveillance, where miss-
ing data is a commonly occurring phenomenon (see e.g. Gharbi et al., 2011;
Sumi et al., 2011) that needs to be accounted for in both inferential proce-
dures and prediction. AR processes of order one are a particularly parsimo-
nious model choice, in which the next value of the process depends only on
the previous one. When data is missing for such a process, what can be said
about the sample of irregularly spaced values?

In this paper, we consider a zero-mean stationary autoregressive process

of order one with Gaussian errors, which can be expressed as

Normal

X1 ∼
Xt = ρXt−1 + ǫt,

(cid:18)

0,

1

σ2

ρ2

,

(cid:19)
−
t = 2, 3, . . . ,

(1)

ǫt

iid
∼

Normal(0, σ2),

|

ρ
|

< 1, and, to eliminate the trivial case, ρ

where
= 0. We consider the
zero-mean AR(1) process here because a mean term can always be added
back later. As shown in Lindsey (2004, p. 217) the joint distribution of
˜x = (X1, X2, . . . , Xn)′ is multivariate normal with zero (vector) mean and a
covariance matrix given by

˜Σ =

σ2

ρ2

1

−

ρ
1
ρ
1
...
...
ρn−2 ρn−3
ρn−1 ρn−2

. . . ρn−2 ρn−1
. . . ρn−3 ρn−2
...
...
. . .
ρ
. . .
1
ρ
. . .
1



.







is tridiagonal and may be expressed

(2)










Further, the precision matrix ˜Q = ˜Σ−1
as

˜Q =

1
σ2

ρ

1
−
ρ 1 + ρ2
−
...
...
0
0
0
0

. . .
. . .
. . .
. . .
. . .

0
0
...
1 + ρ2
ρ

−

0
0
...
ρ
−
1



















.

(3)

As shown in Rue and Held (2005), the sparsity of the precision matrix ˜Q
carries over to its Cholesky decomposition, which enables very fast (linear

1

6
in n) density evaluation and random number generation.
In comparison,
a Cholesky or eigendecomposition of the (dense) covariance matrix has a
computational complexity of order n3.

In some applications it may be impossible to sample a process modeled
by Equation (1) at consecutive time points. In such circumstances, it may
still be of interest have a computationally convenient representation of the
distribution of the sample x = (Xt1, Xt2, . . . , Xtm)′, where 1
t1 < t2 <
. . . < tm ≤
n. For this irregularly sampled stationary AR(1) process, we see
from the expression of ˜Σ in Equation (2) that the (marginal) distribution
of x will also be multivariate normal, with the zero vector (of length m) as
mean, and a covariance matrix with elements (Σ)ij = σ2
1−ρ2 ρ|ti−tj | The aim
is now to ﬁnd an expression for Q = Σ−1 similar in neatness to ˜Q, with
implications for density evaluation and simulation of the irregularly sampled
process x.

≤

2 Results

Theorem 1. Let x = (Xt1, Xt2, . . . , Xtm)′ be the values of the AR(1) process
described in Equation (1), sampled at times t1 < t2 < . . . < tm. Assume the
process is in its stationary state, and for brevity of notation, assume σ = 1.
The precision matrix Q of x then has elements

Q1,1 =

Qm,m =

1

1

1

−

ρ2
ρ2(t2−t1) ,
−
ρ2
1
ρ2(tm−tm−1) ,
−
−
ρ2)
(1

Qi,i =

(1

−

Qi+1,i = Qi,i+1 =

−
Qi+k,i = Qi,i+k = 0,

ρ2(ti+1−ti−1)

−

1
−
ρ2(ti−ti−1)) (1
(cid:0)
(1
1

ρ2(ti+1−ti))

(cid:1)

−

ρ2) ρti+1−ti
−
ρ2(ti+1−ti)
−
k = 2, 3, . . . , m

,

,

1 < i < m,

1

−

≤
i,

i < m,

1

≤

i < m

1.

−

Q is thus a tridiagonal matrix.

Proof. The fact that Q is a tridiagonal matrix follows from the interpreta-
tion of the oﬀ-diagonal elements of Q as the negated and scaled conditional
correlations of x (Theorem 2.2 in Rue and Held, 2005):

Corr(Xi, Xj|

x−ij) =

Qij
QiiQjj

−
Qii = Prec(Xi|
p

x−i),

, where

(4)

(5)

2

and the analogously deﬁned Qjj are the conditional precisions (variance re-
ciprocals). Here, x−ij denotes all elements of x except the ith and jth.

Now take ti, ti+1, tj−1, tj ∈ {

t1, t2, . . . , tm}
that Equation (1) allows the representation

with i + 1

j

−

≤

1. Note then

ti+1−ti−1

Xti = ρ−(ti+1−ti)

Xti+1 −

k=0
X

tj −tj−1−1

ρkǫti+1−k

!

Xtj = ρtj −tj−1Xtj−1 +

ρkǫtj −k.

k=0
X

(6)

(7)

The quantities Xti+1 and Xtj−1 are assumed known in the conditional cor-
x−ij), and no same error term appears in both sums
relation Corr(Xti, Xtj |
above. Thus, the correlation equals zero, which implies that Qij = 0 when
we have at least one observation between ti and tj. Q is then (at most)
tridiagonal.

With this knowledge, one way to determine the explicit form of the ele-
ments of Q is to solve the system of equations ΣQ = I m, where I m is the
m identity matrix. For notational convenience, we deﬁne ρji = ρj−i with
m
j > i. Because Q is symmetric (Qk,k+1 = Qk+1,k) and tridiagonal, we only
1 unknowns to solve for, and the system ΣQ = I m can be reduced
have 2m
to the set of equations

−

×

Q11 + ρ21Q12 = 1
ρ21Q11 + Q12 = 0

(

ρ2

,

−

(8)

ρk,k−1Qk−1,k + Qkk + ρk+1,kQk,k+1
= 1
ρk+1,k−1Qk−1,k + ρk+1,kQkk + Qk,k+1 = 0

(

ρ2

−

,

1 < k < m,

(9)

ρm,m−1Qm−1,m + Qmm = 1

ρ2.

−

(10)

n

The system (8) may be solved for Q11 and Q12, after which system (9) can
be solved iteratively for k = 2, 3, . . . , m
1 by inserting Qk−1,k found in the
previous iteration. Lastly, Qmm is easily solved for in Equation (10) when
Qm−1,m has been found in the previous step.

−

If the assumption σ = 1 is relaxed, the non-zero elements of Q should be

divided by σ2 to yield the correct precision matrix.

Since Q has only 2m

(m) ﬂops (ﬂoating point operations). Likewise, it needs only

1 non-zero elements, it can be constructed using only
(m) space

−

O
for storage if stored in a sparse format.

O

3

 
Corollary 1.1. Let Yti = Xti + µti, with Xti an element of x as in Theorem
1 and the µti’s ﬁxed. Also let y = (Yt1, . . . , Ytm)′. Then by Theorem 2.3 of
Rue and Held (2005), the following conditional expected values hold for the
elements of y:
E[Yt1|
E[Yti|

y−1] = µt1 + ρt2−t1(yt2 −
µt2),
ρ2(ti+1−ti)
y−i] = µti + ρti−ti−1 1
ρ2(ti+1−ti−1) (yti−1 −
−
1
−
ρ2(ti−ti−1)
+ ρti+1−ti 1
ρ2(ti+1−ti−1) (yti+1 −
−
1
−
y−m] = µtm + ρtm−tm−1(ytm−1 −
µtm−1),

E[Ytm|
where y−j denotes all elements of y except the jth, for 1
m. To-
gether with the expression for the conditional precision given in Equation
(5), Corollary 1.1 speciﬁes the full conditional (normal) distributions of the
irregularly sampled AR(1) process.

µti+1), 1 < i < m,

µti−1)

(13)

(12)

(11)

≤

≤

j

3

Implications

Typical density evaluation and random number generation of multivariate
normal variables involves a Cholesky or eigendecomposition of the covariance
m matrix, the computational
matrix (Barr and Slezak, 1972). For an m
(m3)
cost (in terms of the number of ﬂops) associated with either method is
(Trefethen and Bau, 1997). For large m, this cost becomes prohibitive, and
even with m smaller such decompositions can become the bottleneck when
they need to be performed repeatedly. For example, a MCMC algorithm
trying to infer the distribution of ρ may need to evaluate the density of x
thousands of times, if not more. However, if the sparse structure of Q can
be used, this cost can be drastically reduced.

O

×

3.1 Cholesky factorization

Indeed, the sparsity of Q carries over to its Cholesky decomposition. By
Theorem 2.9 in Rue and Held (2005), the (lower) Cholesky decomposition L
of Q, i.e. Q = LLT , will have a lower bandwidth of 1—that is, only the main
diagonal and (ﬁrst) subdiagonal will have non-zero elements. This decom-
position is computable in linear time using Algorithm 2.9 in Rue and Held
(2005), described next.

Let v be a vector of length m and let vi:j be elements i to j of this vector.
Denote by Qi:k,j elements i to k in column j of Q, and let the same notation

4

be applicable to L and its elements Li,j, which are initialized to zero. The
matrix L can then be computed using the following algorithm:

Algorithm 1 Band-Cholesky factorization of Q (bandwidth 1)
1: for j = 1 to m do
2:
3:
4:
5:

j + 1, m
}

min
{
Qj:λ,j

λ
←
vj:λ ←
if j > 1 then
vj −
vj ←
vj:λ/√vj
Lj:λ,j ←
7: Return L

L2

6:

j,j−1

This algorithm is seen to involve only
O
in a sparse format, the storage is of size

(m) ﬂops. Additionally, if L is stored

(m) as well.

O

3.2 Unconditional simulation

µ, Q−1
If we wish to sample x
as in Theorem 1, but now with
a mean vector µ, we can use the following algorithm (Algorithm 2.4 in
Rue and Held, 2005):

MVN

∼

(cid:1)

(cid:0)

∼

Normal(µ, Q−1)

Algorithm 2 Sampling x
1: Compute L using Algorithm 1.
2: Sample m standard normal variables and store them in a vector z.
3: Solve LT v = z using sparse back substitution (see Algorithm 3 below).
4: Compute x = µ + v.
5: Return x

This algorithm is seen to be of order
(m) in computational complexity. The
O
sparse back substitution in step 3 of Algorithm 2 computes the elements of
v as follows:

Algorithm 3 Solving LT v = z when L has bandwidth 1
1: vm = zm/Lm,m
2: for i = m
3:

1 to 1 do
Li+1,ivi+1)/Li,i

−
vi = (zi −

4: Return v

Only 3m

−

2 ﬂops are used to produce the solution v.

5

3.3 Conditional simulation

{

Assume now that to =
are the time points at which observa-
t1, . . . , tm}
tions xo are available, and let tp =
be another set of time points
s1, . . . , sk}
{
disjoint from to. Suppose that we wish to simulate values xp from the dis-
tribution of the process at times tp, conditional on xo. From standard facts
about the multivariate normal distribution (see e.g. Rue and Held, 2005), we
know that

xp

∼

xo
Normal
|
µp|o = µp + ΣpoΣ−1
(cid:0)
ΣpoΣ−1
Σp|o = Σpp
oo

µp|o, Σp|o
oo (xo

(cid:1)
−
Σop,

−

, where
µo) ,

and

µa =

and Σa =

x∗
p
x∗
o(cid:21)
(cid:21)
are the mean vector and covariance matrix of xa = (xT
p , xT
even if the matrix Σ−1
for k
is to be computed as well.

o )T . In general,
oo is sparse, the computation of Σp|o will be demanding
m, and further so if a Cholesky or eigendecomposition of the result

Σpp Σpo
Σop Σoo

≫

(cid:20)

(cid:20)

|

≫

A more eﬃcient way of sampling from the distribution of xp

xo, espe-
cially when k
m, was described by Hoﬀman and Ribak (1991). To use
this method, we need to be able to sample (unconditionally) from the joint
distribution of xa. One way of doing so is to just iteratively simulate from the
deﬁnition of the process (Equation 1) using a starting value drawn from the
stationary distribution (and with mean terms added back to the right hand
side), and then pick out the values for the times tp
to. Another is to ﬁrst
∪
to (if necessary), order and combine the mean vectors accordingly,
sort tp
create the corresponding precision matrix Qa = Σ−1
a , draw samples using
Algorithm 2, and re-order the samples according to tp and to.

∪

Let Qo be the precision matrix of xo, i.e. Qo = Σ−1

oo . Then we can sample

from the distribution of xp

|

xo as follows (Hoﬀman and Ribak, 1991):

Algorithm 4 Sampling xp

xo

Normal

µp|o, Σp|o

|

∼

1: Sample x∗

x∗
p
x∗
o(cid:21)
p + ΣpoQo(xo
2: Return xp = x∗

a =

∼

(cid:20)

x∗

o).

−

(cid:0)
Normal (µa, Σa) as discussed above.

(cid:1)

Because Qo is tridiagonal, the matrix product ΣpoQo involves only
ﬂops, rather than the

(km)
(km2) ﬂops that are needed if Q is dense. Thus the

O

O

6

complexity of Algorithm 4 is
is to be done in Step 1.

O

(km), plus

O

((k + m) log(k + m)) if sorting

3.4 Density evaluation

Many applications require the evaluation of probability density functions.
For example, typical MCMC algorithms calculate a quotient (or log diﬀer-
ence) of densities repeatedly in the evaluation of acceptance ratios.
It is
therefore of interest to make this computation as eﬃcient as possible for the
type of irregularly sampled AR(1) process considered in this paper. Typi-
cally, the evaluation of a m-dimensional multivariate normal density involves
the Cholesky decomposition of the covariance matrix, which as discussed pre-
(m3). With a sparse precision
viously has a computational complexity of
matrix Q however, this cost can be drastically reduced.

O

µ, Q−1

∼

Let x

Normal

be a vector of values from an irregularly sam-
pled AR(1) process as in Theorem 1, and let p(x) be probability density
function evaluated at x. To calculate log p(x) of a sample x from this distri-
bution, ﬁrst calculate the Cholesky decomposition L of Q using Algorithm
1. Then, by Rue and Held (2005, p. 35), the log-density can be computed as
follows:

(cid:0)

(cid:1)

log p(x) =

m
2

−

log(2π) +

q = (x

−

µ)T Q (x

1
2

m

log Li,i −
µ) .

i=1
X
−

q, where

(14)

(15)

If x was generated using Algorithm 2, q simpliﬁes to q = zT z. By utilizing
the sparsity of Q, the evaluation of the log-density has a computational
complexity of

(m).

O

4 Summary

This paper provides analytical expressions for the elements of the precision
matrix Q of a stationary Gaussian AR(1) process sampled with irregular
spacing. The sparsity of this matrix was shown in Section 3 to yield eﬃcient
algorithms for density evaluation and simulation of such a process. Appli-
cations of AR(1) processes are abound in biostatistics and ﬁnance, and the
results of this paper should prove relevant for those in need of computational
eﬃciency. More generally, the results are valuable from a missing data per-
spective. A simple extension of this paper is calculate exactly how the results
derived carry over to the distribution of a irregularly spaced sample from

7

an Ornstein-Uhlenbeck process, which is the continuous-time analog of the
AR(1) process. A memory-eﬃcient implementation of the given algorithms,
enabled by the RcppArmadillo library (Eddelbuettel and Sanderson, 2014), is
provided by the R package irregulAR1, available on the Comprehensive R
Archive Network (CRAN).

Acknowledgements

Funding: This work was supported by the Swedish Research Council [grant
2013:05204].

References

Barr, D. R. and Slezak, N. L. (1972). A Comparison of Multivariate Normal

Generators. Communications of the ACM, 15(12):1048–1049.

Eddelbuettel, D. and Sanderson, C. (2014). RcppArmadillo: Accelerating R
with high-performance C++ linear algebra. Computational Statistics and
Data Analysis, 71:1054–1063.

Gharbi, M., Quenel, P., Gustave, J., Cassadou, S., Ruche, G. L., Girdary,
L., and Marrama, L. (2011). Time series analysis of dengue incidence in
Guadeloupe, French West Indies: Forecasting models using climate vari-
ables as predictors. BMC Infectious Diseases, 11(1):166.

Hoﬀman, Y. and Ribak, E. (1991). Constrained realizations of Gaussian

ﬁelds - a simple algorithm. The Astrophysical Journal, 380:L5–L8.

Lindsey, J. K. (2004). Statistical Analysis of Stochastic Processes in Time.

Cambridge University Press.

Rue, H. and Held, L. (2005). Gaussian Markov Random Fields: Theory and

Applications. Chapman and Hall/CRC.

Sumi, A., ichi Kamo, K., Ohtomo, N., Mise, K., and Kobayashi, N. (2011).
Time Series Analysis of Incidence Data of Inﬂuenza in Japan. Journal of
Epidemiology, 21(1):21–29.

Trefethen, L. N. and Bau, D. I. (1997). Numerical Linear Algebra. SIAM.

8

