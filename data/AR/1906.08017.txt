0
2
0
2

r
p
A
6

]
T
S
.
h
t
a
m

[

3
v
7
1
0
8
0
.
6
0
9
1
:
v
i
X
r
a

Bump detection in the presence of dependency:
Does it ease or does it load?

Farida Enikeeva

farida.enikeeva@math.univ-poitiers.fr
Laboratoire de Math´ematiques et Applications, UMR CNRS 7348, Universit´e
de Poitiers, France
and
The A. A. Kharkevich Institute for Information Transmission Problems,
Russian Academy of Science, Moscow, Russia
Axel Munk, Markus Pohlmann

munk@math.uni-goettingen.de,
markus.pohlmann@mathematik.uni-goettingen.de
Institute for Mathematical Stochastics, University of G¨ottingen
and
Felix Bernstein Institute for Mathematical Statistics in the Bioscience,
University of G¨ottingen
and
Max Planck Institute for Biophysical Chemistry, G¨ottingen, Germany
Frank Werner1

frank.werner@mathematik.uni-wuerzburg.de
Institut f¨ur Mathematik, University of Wuerzburg, Germany

We provide the asymptotic minimax detection boundary for a bump, i.e. an abrupt
change, in the mean function of a stationary Gaussian process. This will be charac-
terized in terms of the asymptotic behavior of the bump length and height as well
as the dependency structure of the process. A major ﬁnding is that the asymptotic
minimax detection boundary is generically determined by the value of its spectral
density at zero. Finally, our asymptotic analysis is complemented by non-asymptotic
results for AR(p) processes and conﬁrmed to serve as a good proxy for ﬁnite sample
scenarios in a simulation study. Our proofs are based on laws of large numbers for
non-independent and non-identically distributed arrays of random variables and the
asymptotically sharp analysis of the precision matrix of the process.

Keywords: minimax testing, time series, ARMA processes, change point detection, weak laws of
large numbers, Toeplitz matrices

AMS classiﬁcation numbers: Primary 62F03, 62M07, Secondary 60G15, 62H15.

1 Introduction

1.1 Model and problem statement

In this paper we consider observations of a triangular array of Gaussian vectors, Y “ µn ` ξn,
n P N with the coordinates

Yi,n “ µi,n ` ξi,n,

ξn “ pξ1,n, . . . , ξn,nqT „ Nn p0, Σnq ,

(1.1)

1Corresponding author

1

 
 
 
 
 
 
with a known positive deﬁnite covariance matrix Σn P Rnˆn, but an unknown mean vector µn “
pµ1,n, . . . , µn,nqT P Rn. We will furthermore assume that the noise ξn in (1.1) consists of n
consecutive samples of a stationary process pZtqtPZ.
For a proper asymptotic treatment, we will assume that µn is obtained from equidistantly sampling
T
a function mn : r0, 1s Ñ R at sampling points i
.
Our goal is to analyze how diﬃcult it is to detect abrupt changes of the function mn based on the
observations Y “ pY1,n, ..., Yn,nqT coming from (1.1). Therefore, we focus on functions mn of the
form

n , i “ 1, . . . , n, i.e. µn “

, . . . , mn

mn

˘˘

n
n

1
n

`

`

`

˘

#

mn pxq “

δn
0

if x P In,
else,

(1.2)

i.e. mn has a bump located at the interval In Ă r0, 1s of height δn P R, see also Figure 1 for an
illustration. We assume throughout the paper that the matrix Σn in (1.1) as well as the length
of the bump λn P p0, 1q are known, but that its amplitude δn and the exact position of the bump
itself are unknown.
To formalize the detection problem, let us introduce some notation. For an interval I Ă r0, 1s we
use 1I P Rn as abbreviation for the vector with entries

#

1I piq “

1
0

n P I,

if i
else,

1 ď i ď n.

Consequently, µn “ δn1In whenever mn is of the form (1.2). Furthermore let

(cid:32)
ra, bq

ˇ
(
ˇ 0 ď a ă b ď 1

I :“

be the set of all right-open intervals in r0, 1s, and for a given length λ P p0, 1q we introduce by

I pλq :“

(cid:32)
ra, bq

ˇ
ˇ 0 ď a ă b ď 1, b ´ a “ λ

(

the set of all right-open intervals in r0, 1s of length λ.
Now the problem to detect a bump of length λn in the signal µn from (1.1) can be understood as
the hypothesis testing problem

H n

0 : Y „ Nn p0, Σnq
against

(1.3)

H n

1 : DI P I pλnq , Dδ P R : |δ| ě ∆n

such that Y „ Nn pδ1I , Σnq

0 and H n

with a minimal amplitude value ∆n ą 0 to ensure distinguishability of H n
1 . Note that
I and δ in (1.3) are allowed to depend on n (as the length λn and the minimal amplitude value
∆n do), but we suppress this dependency in the following. Similarly we write H0 instead of H n
0
as Σn is assumed to be known. Note that we will consider the situation λn Ñ 0 as n Ñ 8 below,
corresponding to a vanishing bump, which avoids trivial cases such as EH n
1 rYis “ δ ą 0 for all
1 ď i ď n in (1.3).
The aim of this paper is to provide insight on how the dependency structure in (1.1) encoded
in terms of Σn inﬂuences the detection of such a bump. More precisely, we would like to derive
asymptotic conditions1 on the minimal detectable bump amplitude ∆n depending on Σn, λn and
n. To the best of our knowledge, there is no systematic understanding of this problem from the
minimax point of view. We will therefore provide (asymptotic) lower and upper bounds for the
amplitude of asymptotically detectable signals in the following sense (cf. [26, 25]). Let α, β P p0, 1q
be arbitrary error levels.

1Let panqnPN and pbnqnPN two sequences of positive numbers.

lim inf nÑ8 an{bn ď lim supnÑ8 an{bn ă 8, and an — bn if limnÑ8 an{bn “ 1.

In the following we write an „ bn if 0 ă

2

upper detection bound: Whenever the bump amplitude ∆n satisﬁes ∆n “ cϕn, c ě c˚ with a
constant c˚ ą 0 and a rate ϕn depending on n, λn and Σn, then there is a sequence of
tests for (1.3) with (asymptotic) type I error ď α and (asymptotic) type II error ď β.

lower detection bound: Whenever the bump amplitude ∆n satisﬁes ∆n “ c ˜ϕn, c ď c˚ with a
constant c˚ ą 0 and a rate ˜ϕn depending on n, λn and Σn, then no sequence of tests
for (1.3) can have type (asymptotic) I error ď α and at the same time (asymptotic) type II
error ď β.

`

˘

0, 1
2

and argue in Section 2.1 that this is suﬃcient.

Precise deﬁnitions of the (asymptotic) type I and type II errors and comments on the validity
of these particular notions of the detection bounds will be given in Section 2.1. Note that the
minimax separation rate ϕn might depend on the prescribed signiﬁcance levels α and β, and that
the deﬁnitions become trivial if β ě 1 ´ α, as then any standard Bernoulli experiment with success
probability α deﬁnes a corresponding test. However, in our case neither the constants c˚ and c˚
nor the rate depend on the error levels α and β. That is why in the following we will always choose
α “ β P
If ˜ϕn “ ϕn in the above upper and lower bound, then we speak of the (asymptotic) minimax
If furthermore c˚ “ c˚, then ∆n — c˚ϕn “ c˚ ˜ϕn is called the
separation rate ∆n „ ϕn.
(asymptotic) minimax detection boundary over all possible amplitudes ∆n ą 0 and positions
I P I pλnq. We will provide explicit expressions for this under weak assumptions on the covariance
matrix Σn.
We will provide lower and upper bounds in terms of sums over diagonal blocks within Σn (cf.
Section 2.3 and Lemmas 5.7 and 5.8), and for the case of noise generated by subsequent samples
of a stationary time series we will show that these lower and upper bounds coincide.
In case of i.i.d. observations, this is Σn “ σ2idn in (1.1), the minimax detection boundary is
well-known and given by (see [14, 11, 17])

c

∆n — σ

´2 log λn
nλn

.

Here, and in the following, we require

λn Ñ 0

and nλn Ñ 8

as

n Ñ 8.

(1.4)

(1.5)

Signals for which the left-hand side in (1.4) is asymptotically larger than the right-hand side
can be detected consistently (in the sense of an upper detection bound as described above),
whereas they can not be detected consistently once the left-hand side in (1.4) is asymptotically
smaller than the right-hand side (in the sense of a lower detection bound as described above).
Although (1.4) is known for a long time when the errors are i.i.d., to the best of our knowledge,
the inﬂuence of the error dependency structure on the detection boundary (1.4) is an issue that
is much less investigated systematically, although many methods to estimate such abrupt changes
In
in the signal corrupted by serially dependent errors have been suggested (see Section 1.3).
this sense, this paper contributes a benchmark to such methods. Let us illustrate the eﬀect of
the dependency on (1.4) with ξn in (1.1) arising from an AR(1) process with unit variance and
pZ1, ..., ZnqT where Zt ´ ρZt´1 “ ζt with
1 ´ ρ2
auto-correlation coeﬃcient ρ, this is ξn “
i.i.d. standard Gaussian noise ζt, t P Z. In Figure 1 we illustrate three diﬀerent situations encoded
in terms of ρ, namely positively correlated noise (ρ “ 0.7), independent noise (ρ “ 0), and
negatively correlated noise (ρ “ ´0.7). It seems intuitively clear that the value of ρ inﬂuences
the diﬃculty of detecting a bump substantially, and especially positively correlated noise hinders
the ﬁrst plot in Fig. 1, where noise and bump
eﬃcient detection dramatically. Compare e.g.
appear hardly to distinguish. Furthermore, due to the positive correlation, there appear several
regions which suggest a bump in signal, which is not there. In contrast, the middle and bottom
plot allow for simpler identiﬁcation of the bump region. Our main result makes these intuitive
ﬁndings precise.

1{2

`

˘

3

Positively correlated noise (ρ “ 0.7)

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Independent noise (ρ “ 0)

mn
Yi
In

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Negatively correlated noise (ρ “ ´0.7)

∆n
0

∆n
0

∆n
0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Figure 1: Model (1.1) in case of AR(1) noise for diﬀerent values of ρ: Data together with the

function mn, where the model parameters are set to be n “ 512 and ∆n “ 1, σ “ 1.

1.2 Results

To describe our results concerning the detection boundary for serially dependent data we require
some more terminology. Let the autocovariance function γZ of the stationary process pZtqtPZ be
given by γZphq “ Cov rZt, Zt`hs for h P Z. Assume that γZ is square summable, then the process
Z has the spectral density fZ P L2 r´1{2, 1{2q deﬁned by

fZpνq “

8ÿ

h“´8

γZphqe´2πihν,

ν P r´1{2, 1{2q .

i.e. one naturally has
In fact, fZ can also be considered as a function on the unit sphere,
limνÑ1{2 fZpνq “ fZp´1{2q. We will also assume that the autocovariance function is symmet-
ric, which is equivalent to fZ being real-valued. In the following, we will omit the subscript Z in
the notation of the spectral density of Z when it does not create ambiguities.
With this notation introduced, we will show under mild conditions that the detection boundary
for the hypothesis testing problem (1.3) is given by

d

∆n —

´2f p0q log λn
nλn

.

It is immediately clear, that in case of independent observations where Σn “ σ2idn, one has
f p0q “ σ2, which reproduces (1.4). In the general case, note that

ÿ

f p0q “

γ phq ,

hPZ

4

i.e. the detection boundary solely depends on the value of the spectral density at zero which is
known as long-run variance.2
In case of the AR(1)-based noise ξn :“ p1´ρ2q1{2pZ1, . . . , ZnqT with unit variance as shown in Fig-
ure 1, the auto-covariance of the underlying AR(1) process Zt is given by γZ phq “ γZp0qρ|h|, where
γZp0q “ p1 ´ ρ2q´1. Thus the spectral density at zero of the noise process ξ “
is

p1 ´ ρ2q1{2Zi

iPN

˘

`

and hence the detection boundary is given by

`

˘ 8ÿ

fξ p0q “

1 ´ ρ2

γZphq “

1 ` ρ
1 ´ ρ

,

h“´8

c

c

∆n —

1 ` ρ
1 ´ ρ

´2 log λn
nλn

.

(1.6)

b

As an immediate consequence, this shows that bump detection is easier under a negative correlation
ρ than in case of positive correlations. For the three values employed in Figure 1 we compute for
1`ρ
1´ρ in (1.6) the values 2.38 when ρ “ 0.7 and 0.42 when ρ “ ´0.7. This means that
the factor
the amplitude of detectable signals for ρ “ 0.7 and ρ “ ´0.7 diﬀers approximately by a factor of
5.6. Also, given the bump length λn, the detection of a bump of the same size ∆n for ρ “ 0.7
requires approximately a 6 times larger sample size than for ρ “ 0, and even a 31 times larger
sample size than for ρ “ ´0.7. This is in good agreement with the intuitive ﬁndings from Figure
1 and conﬁrmed in ﬁnite sample situations in Section 4. In the simulations we also investigate the
inﬂuence of several bumps instead of one, and ﬁnd that independent of ρ, multiple bumps always
help detection, as to be expected.
Remarkably, as in the case of i.i.d. noise with variance σ2, where we have f p0q “ σ2, certain
dependent error processes might also satisfy f p0q “ σ2, and hence obey the same diﬃculty to
detect a bump as for the independent case. As an example, consider the stationary and causal
ARp2q process given by Zt “ 1
In this case
fZ p0q “ 1
2 ` 1 “ 1, even though the process Zt is clearly not independent (see Section 3 for a
comprehensive treatment of ARMA processes).

2 Zt´2 ` ζt, where ζt „ N p0, 1q for t P Z.

2 Zt´1 ´ 1

2 ´ 1

Proof strategy. To prove a lower detection bound, we will employ techniques dating back to
Ingster [26] and D¨umbgen and Spokoiny [14] developed for independent observations. To generalize
this approach to our dependent case, we will use a recent weak law of large numbers due to Wang
and Hu [43] for triangular arrays of random variables that are non-independent within each row
and non-identically distributed between rows (see Section 5.1.1 for the precise statement and also
[20, 8, 40, 35] for related results).
For the upper detection bound, we will provide an explicit test based on the supremum of the
moving average process
. A valid critical value will be given based on a chaining
technique. Note that this cannot be obtained by a continuous upper bound of the stochastic
process (as e.g. provided in Theorem 6.1 in [14]) due to the fact that the dependency structure is
allowed to change with n and hence there is no continuous analog of

1T
I Y

IPIpλnq

˘

`

`

˘

.

1T
I Y

IPIpλnq

1.3 Related work

Bump detection for dependent data appears to be relevant to a variety of applications where piece-
wise constant signals (i.e. several bumps) are observed under dependent noise. Exemplary, we
mention molecular dynamics (MD) simulations, where collective motion characteristics of protein
[31] and the references therein). For certain proteins it
atoms are studied over time (see e.g.
has been shown that the noise process can be well modeled by a stationary ARMA(p, q) process

2The long-run variance of a process pZtqtPZ with spectral density f is deﬁned as lim
nÑ8
n´1Var rSns “ f p0q (see [23] and Section 5.1.3 for details).

nř

Zi. It holds that lim
nÑ8

i“1

n´1Var rSns, where Sn “

5

with small p and q, see [39]. Another application is the analysis of ion channel recordings, where
one aims to identify opening and closing states of physiologically relevant channels (see [32] and
the references therein). Here, the dependency structure is induced by a known band-pass ﬁlter,
ensuring that Σn in (1.1) can be precomputed explicitly (which corresponds to our setting of
known Σn), and allowing for a good approximation by stationary and m-dependent noise with
small m, see [34].
In fact, bump detection as discussed here is closely related to estimation of a signal which consists
of piece-wise constant segments, often denoted as change point estimation. We refer to the classical
works of Ibragimov and Has’minskii [24], Cs¨org˝o and Horv´ath [12], Brodsky and Darkhovsky [7],
Carlstein et al. [9], and Siegmund [38] for a survey of the existing results as well as to the review
article by Aue and Horv´ath [1]. Indeed, if the bumps have been properly identiﬁed by a detection
method, posterior estimation of the signal is relatively easy, see [17] for such a combined approach
in case of i.i.d. errors, and [13] in case of dependent data. We also mention [10], who presented a
robust approach for AR(1) errors.
Model (1.1) can be seen as prototypical for the more complex situation when several bumps are
to be detected. We do not intend to provide novel methodology for this situation in this paper,
rather Theorem 2.1 provides a benchmark for detecting such a bump which then can be used
to benchmark the detection power of any method designed for this task. Minimax detection
the seminal series of papers by Ingster [26] or the monograph by
has a long history, see e.g.
Tsybakov [42]. More recently, Goldenshluger et al. [19] provided a general approach based on
convex optimization. In case of independent observations, the problem of detecting a bump has
been considered in [2, 5, 17, 11, 15, 28], and our strategy of proof for the lower bound is adopted
from [14]. We also mention [16] for a model with a simultaneous bump in the variance, and [33]
for heterogeneous noise, however still restricted to independent observations.
The literature on minimax detection for dependent noise is much less developed, and most similar
in spirit to our work are the papers by Hall and Jin [21] and Keshavarz et al. [30]. In the former,
the minimax detection boundary for an unstructured version of the model (1.1) in a Bayesian
“ ρn and P
setting is derived, that is P
“ 1 ´ ρn with a probability
ρn tending to 0. In contrast to [21], in the present setting we can borrow strength from neighboring
observations in a bump. Still, we can exploit a result in [21] about the decay behavior of inverses
of covariance matrices (see Section 5.1.2) to validate Assumption 2. Keshavarz et al. [30] deal with
the classical change-point in mean problem, i.e. with the problem to detect whether mnpi{nq ” 0
for all 1 ď i ď n, or if there exists τ P r1, ns such that mnpi{nq “ ´ 1
2 ∆n1ti ą τ u
for 1 ď i ď n. The authors derive upper and lower bounds for detection from dependent data as
in (1.1), similar in spirit to our Theorem 2.1. Their bounds, however, do not coincide with ours,
i.e. they do not derive the precise minimax detection boundary, as they are mostly interested in
the rate of estimation. However, as we see from Theorem 2.1, the
´ log λn rate does not change,
it is the constant f p0q which matters. We will employ several of their computations concerning
covariance structures of time series (while correcting a couple of technical inaccuracies).
If λn is unknown,
We ﬁnally comment on the assumption of knowing Σn and the length λn.
estimation of the function mn can be performed in the independent noise case by SMUCE [17] via
a multiscale approach. SMUCE is known to achieve the asymptotic detection boundary (1.4) in
case of i.i.d. Gaussian errors. For the dependent case with a (partially) unknown covariance matrix
Σn, further methods for estimation of mn such as H-SMUCE [33], J-SMURF [22] or JULES [34]
have been developed. They all rely on a local estimation of the covariance structure in combination
with a multiscale approach. None of these methods achieves the detection boundary derived in
this paper, and hence it remains unclear if not knowing Σn and / or λn would aﬀect it. Developing
a test which achieves a corresponding upper bound by multiscale methods is beyond the scope of
this paper and is postponed to future work.

2 ∆n1ti ď τ u ` 1

‰
“ 0

“
mn

“ ∆n

mn

?

i
n

i
n

˘

`

˘

`

‰

“

1.4 Organization of the paper

The remaining part of this paper is organized as follows: In Section 2 we give a precise statement of
our assumptions and formulate our main theorem. Also non-asymptotic results are discussed here.

6

The implications for ARMA models are then given in Section 3, where the previously mentioned
non-asymptotic results are speciﬁed for AR(p) noise. In Section 4 we present some simulations
which support that our asymptotic theory is already useful for small samples. All proofs are
deferred to Section 5.

2 Main results

2.1 Notation and assumptions
To treat the testing problem (1.3), we will consider tests Φn : Rn Ñ t0, 1u, n P N, where Φn pY q “ 0
means that the null hypothesis H0 is accepted, and Φn pY q “ 1 means that the null hypothesis is
rejected, i.e. the presence of a bump is concluded.
Denote by P0 the measure Nn p0, Σnq of Y under the null hypothesis and by PI,δ the measure
Nnpδ1I , Σnq of Y given that there is a bump of height δ within the interval I. With this we will
denote the corresponding expectations accordingly by E0 and EI,δ. We deﬁne the type I error of
Φn by

¯α pΦn, Σnq :“ E0 rΦn pY qs “ P0 rΦn pY q “ 1s .
Furthermore, we say that a sequence pΦnqnPN of such tests has asymptotic level α P r0, 1s if
lim supnÑ8 ¯α pΦn, Σnq ď α. The type II error depending on the parameters Σn, ∆n and λn is
deﬁned as

¯β pΦn, Σn, ∆n, λnq :“ sup

PI,δ rΦn pY q “ 0s .

sup
|δ|ě∆n

IPIpλnq

For a sequence pΦnqnPN of such tests we deﬁne its asymptotic type II error to be lim supnÑ8
¯β pΦn, Σn, ∆n, λnq. For
The asymptotic power of such a family is then given by 1 ´ lim supnÑ8
the sake of brevity, we might suppress the dependency on the parameters in the following and
write only ¯α pΦnq and ¯β pΦnq, respectively.
With this notation, we can now precisely recall the requirements for lower and upper bounds on
detectability as discussed in the introduction:

upper detection bound: For any α P

of asymptotic level α such that @c ą c˚,

`

˘

0, 1
2

, there exist c˚ ą 0 and a sequence of tests Φ˚

n,α, n P N

¯β pΦn, Σn, ∆n, λnq.

lim sup
nÑ8

¯β pΦn, Σn, cϕn, λnq ď α.

Note that this notion of the upper detection bound is in accordance with the usual minimax
testing paradigm (cf. Ingster and Suslina [25]), as it implies that

lim
nÑ8

inf
ΦPΨn

r¯α pΦ, Σnq ` ¯β pΦ, Σn, cϕn, λnqs “ 0,

as n Ñ 8, since α was arbitrary. Here, Ψn is the collection of all tests for the testing problem
(1.3) given n observations.

`

˘

0, 1
2

lower detection bound: For any α P

, there exists c˚ ą 0 such that @c ă c˚, and for any

sequence of tests Φn, n P N of asymptotic level α,

lim inf
nÑ8

¯β pΦn, Σn, c ˜ϕn, λnq ě 1 ´ α.

This implies that

lim
nÑ8

inf
ΦPΨn

r¯α pΦ, Σnq ` ¯β pΦ, Σn, cϕn, λnqs “ 1.

The choice of 1 ´ α as the lower bound of the limit of the type II errors in the lower detection
bound is justiﬁed by the fact that the minimax testing risk is bounded from below as follows (see
[25], p. 55, Theorem 2.1):

r¯α pΦ, Σnq ` ¯β pΦ, Σn, cϕn, λnqs ě 1 ´

inf
ΦPΨ

1
2

}rP0s, rP1s}1,

7

where }rP0s, rP1s}1 is the L1-distance between the convex hulls of measures corresponding to the
null and the alternative hypotheses and Ψ is the set of all possible tests. It implies that the type
II error of the α-level test will be always greater or equal 1 ´ α for non-distinguishable null and
alternative hypotheses.
To derive lower and upper bounds in this sense, we will now pose some assumptions on the possible
lengths λn of intervals and the covariance structure Σn:

Assumption 1. We assume that

(i) nλn

log n Ñ 8 as n Ñ 8,

´

¯

(ii) λn “ o

1
log n

as n Ñ 8.

The ﬁrst part of Assumption 1 assures that the number of observations within any interval of
length λn is at least of logarithmic order as n Ñ 8. The second condition of Assumption 1,
however, gives a bound for the maximal length of the considered intervals, which ensures less
than n{ log n observations in the bump interval. Roughly speaking both conditions are required
to have enough complementary observations (outside respectively inside the bump) to guarantee
asymptotic detection. Note that, in particular, Assumption 1(ii) means that λn Ñ 0 as n Ñ 8,
i.e. Assumption 1 especially implies (1.5). We emphasize that conditions as in (ii) restricting λn
from being too large are common. Assumption 1 plays a crucial role in the proof of the upper
bound, whereas the lower bound can be established under milder conditions (1.5).
However, note that when we consider a slightly modiﬁed version of the testing problem (1.3)
where the bump may not occur in any interval of length λn, but only within a candidate set
Ik :“ rpk ´ 1qλn, kλnq, 1 ď k ď t1{λnu of non-overlapping intervals, then Assumption 1 can be
replaced by (1.5) and the detection boundary will remain the same (cf. Section 2.3).
Instead of posing assumptions on Σn directly, we will again employ the spectral density f of the
underlying stationary process Z as mentioned in the introduction. To do so, we require some more
terminology. For a function g P L2 r´1{2, 1{2q, we denote by T pgq the Toeplitz matrix generated
by g, i.e. the matrix with entries pT pgqqi,jPN “ gj´i, where

gk “

ż 1

2

´ 1
2

gpuqe´2πikudu,

k P Z,

is the k-th Fourier coeﬃcient of g. Note that this allows us to encode the covariance matrix Σn
completely in terms of f . More precisely, the covariance matrix Σn of the noise ξn in (1.1) has
entries Σnpi, jq “ γp|i ´ j|q “ f|i´j|, and we see that Σn “: Tnpf q is the n-th truncated Toeplitz
matrix generated by f , i.e. the upper left n ˆ n submatrix, of T pf q. Consequently, we will also
pose the corresponding assumptions in terms of the function f , which allows us to derive results
for any sequence pΣnqně1 of covariance matrices which are generated by such an f (and not only
for speciﬁc dependent processes):

Assumption 2. Let pΣnqně1 be a sequence of covariance matrices such that Σn “ Tnpf q as intro-
duced above with a function f : r´1{2, 1{2q Ñ R, that is continuous and satisﬁes limνÑ1{2 f pνq “
f p´1{2q and essinf νPr´1{2,1{2qf pνq ą 0. Further, suppose that the Fourier coeﬃcients fh, h P Z of
f decay suﬃciently fast, i.e. there are constants C ą 0 and κ ą 0, such that

h P Z.
I Y and 1T

|fh| ď Cp1 ` |h|q´p1`κq,
Assumption 2 ensures that the dependency between 1T
I 1Y for two candidate intervals
I, I 1 P I pλnq, will be asymptotically small as soon as they are disjoint. It excludes trivial cases
such as total dependence described by Σn pi, jq “ 1 for all i, j P t1, ..., nu, but also permits spectral
densities f with only slowly decaying Fourier coeﬃcients such as discontinuous functions.
, 1 ď i, j ď n, n P N,
Note that also sequences of covariance matrices of the form pΣnqi,j “ g
where g is some kernel function, are prohibited due to this assumption. Covariance matrices of
this kind would have the undesired eﬀect to make the dependency between 1T
I 1Y even
for disjoint candidate intervals I, I 1 P I pλnq stronger as the length λn vanishes.

I Y and 1T

|i´j|
n

¯

´

8

2.2 Asymptotic detection boundary

Our main theorem will be the following.

Theorem 2.1. If Assumptions 1 and 2 hold for the bump regression model (1.1), then the asymp-
totic minimax detection boundary for the testing problem (1.3) is given by

d

∆n —

´2f p0q log λn
nλn

,

as n Ñ 8.

For the details of the proof we refer to Section 5. The upper bound will be achieved by a speciﬁc
test Φa

n, which scans over all intervals of length λn, given by

$
&

Φa

npY q “

%

1

0

if supIPIpλnq
else,

?

|1T
I Y |
1T
I Σn1I

ą cα,n,

(2.1)

where the threshold cα,n will be determined in the proof of Lemma 5.7 in Section 5. Note that
this test is not a likelihood ratio type test (as the LRT relies on 1T
For the proof of the lower bound we employ a strategy from D¨umbgen and Spokoiny [14], and use
a very speciﬁc law of large numbers for arrays of non-independent and non-identically distributed
random variables.

n Y instead of 1T

I Σ´1

I Y ).

2.3 Non-asymptotic results

Note that Theorem 2.1 yields only an asymptotic result. In this section we give non-asymptotic
results in the case of a seemingly simpler testing problem with possible bumps that belong to a
set of non-overlapping intervals. This is formalized by considering the set I 0 of non-overlapping
candidate intervals given by

I 0 :“

(cid:32)
Ik

ˇ
(
ˇ 1 ď k ď tλ´1
n u

,

“

˘

Ik :“

pk ´ 1qλn, kλn

,

1 ď k ď tλ´1

n u.

(2.2)

The goal is still to detect the presence of the bump (but with position being only in I 0) and to
derive non-asymptotic results on the detection boundary for the testing problem

H0 : Y „ Nn p0, Σnq
against
n u, Dδ P R : |δ| ě ∆n

H n

1 : D1 ď k ď tλ´1

such that Y „ Nn pδ1Ik , Σnq

(2.3)

Note that this testing problem might seem much simpler than (1.3) at a ﬁrst glance, but we will
see, however, that the (asymptotic) detection boundary is in fact the same. Concerning the lower
bound, this can be seen readily from the proof of Theorem 2.1, cf. Lemma 5.8.
To detect a bump, we will here employ the maximum likelihood ratio test

Φd

npY q “ 1

(cid:32)
T 0
npY q ą cα,n

(

based on the statistic

T 0
npY q “ sup
IPI0

|1T
b

n Y |

I Σ´1
I Σ´1
1T

n 1I

“ sup

1ďkďtλ´1
n u

|1T
Σ´1
n Y |
Ik
?
˜σk

,

where we denote

˜σk “ 1T
Ik

Σ´1

n 1Ik ,

k “ 1, . . . , tλ´1

n u.

9

(2.4)

(2.5)

The quantities ˜σk are in fact the variances of 1T
variables with covariance structure given by the Ik-block of Σ´1
test Φd

I Σ´1

n Y corresponding to the sum of tnλnu random
n . The type I and II errors of the

n are deﬁned as
`

˘

˜α

Φd
n

:“ P0

“

‰
n pY q “ 1

Φd

and

˘

`

˜β

Φd
n

:“ sup
IPI0

sup
|δ|ě∆n

“

‰
n pY q “ 0

Φd

.

PI,δ

Then the following result establishes basic properties of the test Φd
n.

Theorem 2.2. Consider the testing problem (2.3) and let α P p0, 1q be any ﬁxed signiﬁcance level.
For the maximum likelihood ratio test Φd

n set

c

cα,n :“

2 log

2
αλn

.

(2.6)

Then it holds ˜α

˘

`

Φd
n

ď α for all n P N and

˘

`

˜β

Φd
n

«

ď P

|Z| ą ∆n

a

ﬀ

inf
1ďkďtλ´1
n u

˜σk ´ cα,n

with a standard Gaussian random variable Z „ N p0, 1q and ˜σk as in (2.5).

The proof is obtained by straightforward computations, see Section 5 for details. Theorem 2.2
yields explicit non-asymptotic bounds for the test Φd
n, but those do also yield an asymptotic upper
bound for the detection boundary:

Corollary 2.3. Let pεnqnPN be a positive sequence satisfying
c

c

a

εn

´ log λn ě

log

2
α

`

log

1
α

,

and suppose that the bump altitude ∆n in the testing problem (2.3) obeys

∆n

inf
1ďkďtλ´1
n u

a

˜σk ě

?

a

2 p1 ` εnq

´ log λn.

Then the asymptotic type II error of Φd

n with cα,n as in (2.6) satisﬁes

˘

`

˜β

Φd
n

ď α,

lim sup
nÑ8

(2.7)

(2.8)

?

This shows that the upper bound to be obtained by Φd
n depends only on the asymptotic behavior
˜σk with ˜σk as in (2.5). Inspecting the proof of Lemma 5.8, we ﬁnd that we can
of inf 1ďkďtλ´1
n u
derive an according lower bound depending only on the asymptotic behavior of sup1ďkďt 1
˜σk.
In case of AR(p) noise we will see in Section 2.3 that these quantities can be computed explicitly
and will asymptotically equal in agreement with Theorem 2.1.

?

λn

u

3 ARMA processes and ﬁnite sample results

3.1 Application to ARMA processes

Suppose that the noise vector ξn “ pZ1, . . . , ZnqT in (1.1) is sampled from n consecutive realiza-
tions of a stationary ARMApp, qq time series Zt, with p ě 0, q ě 0 deﬁned as

φpBqZt “ θpBqζt,

ζt

i.i.d.
„ N p0, 1q,

t P Z.

(3.1)

10

Here B is the so-called backshift operator, deﬁned by BXt “ Xt´1, and φpzq and θpzq, z P C, are
polynomials of degrees p and q, respectively, given by

φpzq “ 1 `

pÿ

i“1

φizi,

θpzq “ 1 `

qÿ

i“1

θizi.

(3.2)

We further suppose that φ and θ have no common roots, and that all roots of both φ and θ lie
outside of the unit circle tz P C : |z| ď 1u (see [6] for more details).
Denote by γ the auto-covariance function of Z, i.e. γphq “ E rZtZt`hs for h P Z (as clearly
E rZts “ 0 for all t P Z). It is well-known (see for example [6], Theorem 4.4.2), that in the case of
an ARMApp, qq time series, its spectral density is given by

f pνq “

|θpe´2πiνq|2
|φpe´2πiνq|2 ,

ν P r´1{2, 1{2q.

(3.3)

Note that the spectral density f is continuous at 0 as well as the function 1{f , since the process
is reversible and causal under the posed assumptions on φ and θ. Thus, applying Theorem 2.1 to
this setting immediately yields the following:

Theorem 3.1. Assume that we are given observations from (1.1), where the noise ξn is given
by n consecutive samples of an ARMApp, qq time series as in (3.1) with the polynomials φ and θ
in (3.2) having no common roots and no roots within the unit circle. Furthermore, assume that
Assumption 1 holds. Then the asymptotic detection boundary of the hypothesis testing problem
(1.3) is given by

d

c

ˇ
ˇ
ˇ
ˇ

1 `
1 `

ř
q
i“1 θi
ř
p
i“1 φi

ˇ
ˇ
ˇ
ˇ

´2 log λn
nλn

,

(3.4)

∆n —

´2f p0q log λn
nλn

“

as n Ñ 8.

We ﬁnd that the presence of dependency either eases or loads the bump detection, depending on
f p0q “ |θp1q{φp1q| 2 (which is 1 in the independent noise case). If f p0q ă 1, then the detection
becomes simpler (and smaller bumps are still consistently detectable), but if f p0q ą 1 detection
becomes more diﬃcult. For AR(1) noise, this issue was already discussed in the introduction.

3.2 Non-asymptotic results for AR(p)

In this Section we will derive non-asymptotic results for the speciﬁc case of AR(p) noise. Let us
therefore specify (3.1) to a stationary AR(p) process Zt,

pÿ

i“0

φiZt´i “ ζt,

t P Z

(3.5)

p

ř
p

i“0 φi|´2.

with independent standard Gaussian innovations ζt.
ř

In the notation of (3.1), we have φpzq “
i“0 φizi and θpzq ” 1. Again, we work under the standard assumptions that the characteristic
|z| ď 1u. Note that in this case

polynomial φpzq has no zeros inside the unit circle tz P C :
f p0q “ |
We have seen in the discussion succeeding Theorem 2.2 that the upper and lower bounds depend
on the quantities ˜σk “ 1T
n 1Ik and correspondingly, their minimal and maximal values. Theo-
Ik
rem 3.1 gives the detection boundary condition for ARMA noise with an asymptotic risk constant.
Since ˜σk is just the sum over the block of Σ´1
n , using the exact inverse of Σn (see the appendix for
the exact formula of Σ´1
n obtained by Siddiqui [37]), we can calculate the minimax risk constants
exactly.

Σ´1

Lemma 3.2. Let Σn be the auto-covariance matrix induced by an AR(p) process Zt and ˜σk “
1T
n u. Assume that 1 ď tnλnu ď n ´ 2p and n ą 3p.
Ik

n 1Ik , k “ 1, . . . , tλ´1

Σ´1

11

1. If tnλnu ď p, then

inf
1ďkďtλ´1
n u

˜σk “

˜

i´1ÿ

tnλnuÿ

¸2

φt

,

i“1

t“0

p´tnλnuÿ

˜

tnλnuÿ

¸2

pÿ

˜

p´iÿ

¸2

(3.6)

sup
1ďkďtλ´1
n u

˜σk “

inf
1ďkďtλ´1
n u

˜σk `

φt`i

`

φt`i

.

(3.7)

i“0

t“1

i“p´tnλnu

t“0

2. If p ă tnλnu ď n ´ 2p, then

inf
1ďkďtλ´1
n u

˜

pÿ

¸
2

˜

i´1ÿ

pÿ

¸2

˜σk “ ptnλnu ´ pq

φt

`

φt

,

t“0
pÿ

˜

p´iÿ

i“1

t“0

¸2

sup
1ďkďtλ´1
n u

˜σk “

inf
1ďkďtλ´1
n u

˜σk `

i“1

t“0

φt`i

.

(3.8)

(3.9)

We can now use the results of Theorem 2.2 and get the exact detection boundaries for two diﬀerent
regimes, when tnλnu ď p and p ă tnλnu ď n ´ 2p. Note that condition (5.4) is automatically
satisﬁed since the inverse covariance matrix Σ´1

n is 2p ` 1-diagonal.

Corollary 3.3. Assume that possible locations k of the bump Ik P I 0 are separated from the
endpoints of the interval: p ă k ă n ´ p ´ tnλnu. Then the upper and lower bound constants
match in both cases and are given by formulas (3.6) and (3.8) for the case of tnλnu ď p and
p ă tnλnu ď n ´ 2p, respectively. This follows immediately from the discussion in Section 5.3.1,
in particular equations (5.15) and (5.16).

Remark 3.4. It seems reasonable, that, in case of bumps of length smaller than p, we would need
to analyze the type I error with some ﬁner technique than just the union bound.
On the other hand , we observe that if nλn Ñ 8 and λn Ñ 0 as n Ñ 8, then

˜

pÿ

¸
2

sup
1ďkďtλ´1
n u

˜σk — nλn

φt

t“0

—

inf
1ďkďtλ´1
n u

˜σk,

in accordance with Theorem 3.1.

4 Simulations

In this Section we will perform numerical studies to examine the ﬁnite sample accuracy of the
asymptotic upper bounds for the detection boundary. We focus on the situation that the noise ξn
in (1.1) is generated by an AR(1) process, given by φpzq “ 1 ´ ρz and θpzq ” 1 (in the notation of
(3.1)), where |ρ| ă 1. More precisely, the AR(1) process is given by the equation Zt ´ ρZt´1 “ ζt,
t P Z where ζt „ N p0, 1q are i.i.d.. Note the slight diﬀerence to the setting considered in the
introduction and Figure 1, as here the noise does not have standardized margins.
From Theorem 3.1 we obtain the detection boundary

c

?

2
1 ´ ρ

∆n —

´ log λn
nλn
a

.

(4.1)

In the following we ﬁx the value of the detection rate
´ log λn{pnλnq in (4.1) to be roughly 1/6
and consider three diﬀerent situations, namely small sample size (λn “ 0.1, n “ 829), medium
sample size (λn “ 0.05, n “ 2157) and large sample size (λn “ 0.025, n “ 5312). Thus, the

12

remaining free parameters are ρ and ∆n, and the detection boundary (4.1) connects them by the
asymptotic relation

?

∆n —

2
1 ´ ρ

¨

1
6

«

0.236
1 ´ ρ

.

(4.2)

Let us now specify the investigated tests. For the (general) test from Section 2.2, the critical value
p1 ` op1qq, we will
cα,n is only given implicitly, cf. (5.2). To simplify, in view of cα,n “
therefore use

2 log 2
αλn

b

b

?

|1T
I Y |
1T
I Σn1I

ą

2 log 2
αλn

,

(4.3)

Φa

n pY q :“

$
&

%

1 if

sup
IPIpλnq

0 else

as an asymptotic version. Further we would like to investigate the maximum likelihood ratio test
relying only on non-overlapping intervals Ik “ rpk ´ 1q tnλnu ` 1, ktnλnuq from Section 2.3 given
by

b

$
’&

Φd

n pY q :“

’%

1 if

sup
1ďkďt 1
λn

u

|1T
Ik
b
1T
Ik

0

else.

Σ´1

n Y |
n 1Ik

Σ´1

ą

2 log 2
αλn

,

(4.4)

Note that the latter requires to scan only over t1{λnu intervals, whereas the former requires to
scan over tn p1 ´ λnqu intervals. Consequently, the maximum likelihood ratio test from Section 2.3
can be computed faster by a factor of

n p1 ´ λnq
1{λn

“ nλn p1 ´ λnq

independent of Σn. For the three situations mentioned above this yields values of « 74 in the
small sample regime, « 102 in the medium sample regime, and « 129 in the large sample regime.
However, our results from Theorem 2.1 and the discussion succeeding Theorem 2.2 imply, that
the testing problems (1.3) and (2.3) are of the same diﬃculty in the sense that they both have the
same separation rate.
In the following we examine the type I and type II errors ¯α pΦ˚q and ¯β pΦ˚q with ˚ P ta, du by 2000
simulation runs for α “ 0.05 with diﬀerent choices of ρ, n, λn and ∆n. The position I P I pλnq is
always drawn uniformly at random. Furthermore, we investigate the situation of 2 and 5 disjoint
bumps within r0, 1s.
The ﬁnite sample type I error of both Φa
n in all three sample size situations are shown in
Figure 2 versus the correlation parameter ρ P t´0.99, ´0.98, ..., 0.99u. We ﬁnd that Φd
n is somewhat
conservative, which is clearly due to the usage of the union bound in deriving the critical value in
(4.4), see the proof of Theorem 2.2. Opposed, Φa
n is conservative for ρ ą 0, and liberal for ρ ă 0.
This is clearly due to the simpliﬁed critical value in (4.3), which is only asymptotically valid, and
furthermore the employed asymptotics depend on ρ due to the result by Ibragimov and Linnik
[23], see also Section 5.1.3. However, it seems that already in the small sample size regime our
asymptotic results provide a very good approximation for both tests.
Next we computed the ﬁnite sample type II error in all three sample size situations for ρ P
t´0.99, ´0.98, ..., 0.99u and ∆n P t0.01, 0.02, ..., 0.5u. The corresponding results are shown in
Figures 3–5. We also depict the contour line of equation (4.2) for a comparison and ﬁnd a
remarkably good agreement with the contour lines of the power function already in the small
sample regime, which strongly supports the ﬁnite sample validity of our asymptotic theory. Finally,
we conclude that detection becomes easier for a larger number of bumps.

n and Φd

5 Proofs

We begin this section by stating several useful results from various sources that we are going to
use in our proofs. The proof of Theorem 2.1 will then be split into three parts. We will provide

13

Small sample size

Medium sample size

Large sample size

0.4

0.3

0.2

0.1

0.4

0.3

0.2

0.1

0.4

0.3

0.2

0.1

0
´1 ´0.5

0
ρ

0.5

1

0
´1 ´0.5

0
ρ

0.5

1

0
´1 ´0.5

0.5

1

0
ρ

Figure 2: Type I error of the tests Φa

n (

) for the AR(1) case vs. ρ (x-axis)
simulated by 2000 Monte Carlo simulations with the nominal type I error α “ 0.05
) in three diﬀerent situations: small sample size λn “ 0.1, n “ 829 (left), medium
(
sample size λn “ 0.05, n “ 2157 (middle) and large sample size λn “ 0.025, n “ 5312
(right).

n (

) and Φd

One bump

Two bumps

Five bumps

0.4

0.2

0.4

0.2

0.4

0.2

0
´1 ´0.5 0

0.5

1

0
´1 ´0.5 0

0.5

1

0
´1 ´0.5 0

0.5

1

0.4

0.2

0.4

0.2

0.4

0.2

0
´1 ´0.5 0

0.5

1

0
´1 ´0.5 0

0.5

1

0
´1 ´0.5 0

0.5

1

1

0.8

0.6

0.4

0.2

0

1

0.8

0.6

0.4

0.2

0

Figure 3: Type II error in the small sample regime λn “ 0.1, n “ 829 (top row: Φa

n; bottom
row: Φd
n) for the AR(1) case for ρ (x-axis) vs. ∆n (y-axis) with one bump (left column)
together with the contour line of the detection boundary equation (4.2), two bumps
(middle column) and ﬁve bumps (right column), each simulated by 2000 Monte Carlo
simulations.

asymtotic upper and lower bounds in subsections 5.2.1 and 5.2.2, respectively. These results will in
fact hold for a wider class of covariance matrices than those allowed by Assumption 2. Finally, in
subsection 5.2.3, this will be used to show that the upper and lower bound coincide asymptotically
in the setting of Theorem 2.1, and this will yield the desired result.

14

One bump

Two bumps

Five bumps

0.4

0.2

0.4

0.2

0.4

0.2

0
´1 ´0.5 0

0.5

1

0
´1 ´0.5 0

0.5

1

0
´1 ´0.5 0

0.5

1

0.4

0.2

0.4

0.2

0.4

0.2

0
´1 ´0.5 0

0.5

1

0
´1 ´0.5 0

0.5

1

0
´1 ´0.5 0

0.5

1

1

0.8

0.6

0.4

0.2

0

1

0.8

0.6

0.4

0.2

0

Figure 4: Type II error in the medium sample regime λn “ 0.05, n “ 2157 (top row: Φa

n; bottom
row: Φd
n) for the AR(1) case for ρ (x-axis) vs. ∆n (y-axis) with one bump (left column)
together with the contour line of the detection boundary equation (4.2), two bumps
(middle column) and ﬁve bumps (right column), each simulated by 2000 Monte Carlo
simulations.

One bump

Two bumps

Five bumps

0.4

0.2

0.4

0.2

0.4

0.2

0
´1 ´0.5 0

0.5

1

0
´1 ´0.5 0

0.5

1

0
´1 ´0.5 0

0.5

1

0.4

0.2

0.4

0.2

0.4

0.2

0
´1 ´0.5 0

0.5

1

0
´1 ´0.5 0

0.5

1

0
´1 ´0.5 0

0.5

1

1

0.8

0.6

0.4

0.2

0

1

0.8

0.6

0.4

0.2

0

Figure 5: Type II error in the large sample regime λn “ 0.025, n “ 5312 (top row: Φa

n; bottom
row: Φd
n) for the AR(1) case for ρ (x-axis) vs. ∆n (y-axis) with one bump (left column)
together with the contour line of the detection boundary equation (4.2), two bumps
(middle column) and ﬁve bumps (right column), each simulated by 2000 Monte Carlo
simulations.

15

5.1 Auxiliary results

5.1.1 Weak law of large numbers for arrays of dependent variables

Deﬁnition 5.1 (Sung et al. [40]). Let tXnk : n P N, un ď k ď vnu be an array of random
variables with vn ´ un Ñ 8 as n Ñ 8. Additionally, let r ą 0, and pknqnPN be a sequence of
positive integers, such that kn Ñ 8 as n Ñ 8.
Let phpnqqnPN be a sequence of positive constants, such that hpnq Õ 8 as n Ñ 8. The array
tXnk : n P N, un ď k ď vnu is said to be h-integrable with exponent r if

1
kn

sup
nPN

vnÿ

k“un

E r|Xnk|rs ă 8,

and

lim
nÑ8

1
kn

vnÿ

k“un

E r|Xnk|r1 t|Xnk|r ą hpnqus “ 0.

With this, we have the following.

Theorem 5.2 (Wang and Hu [43]). Let m be a positive integer. Suppose that tXnk, un ď k ď
vn, n ě 1u is an array of non-negative random variables with CovpXnk, Xnkq ď 0 whenever
|j ´ k| ě m, un ď j, k ď vn, for each n ě 1 and is h-integrable with exponent r “ 1 for a sequence
kn Ñ 8 and hpnq Ò 8, such that hpnq{kn Ñ 0 as n Ñ 8. Then

1
kn

vnÿ

k“un

pXnk ´ EXnkq Ñ 0

in L1 and hence in probability, as n Ñ 8.

Remark 5.3. In fact, the original theorem in Wang and Hu [43] is slightly stronger, but Theorem
5.2 as stated above is suﬃcient for our purposes.

Remark 5.4. We can relax the condition CovpXnj, Xnkq ď 0 whenever |j´k| ě m, un ď j, k ď vn
in Theorem 5.2 to requiring only that

lim sup
nÑ8

1
k2
n

vnÿ

j,k“un
|j´k|ěm

CovpXnj, Xnkq ď 0.

5.1.2 Decay of precision matrices

The following result is due to Jaﬀard [27] and was used in [21] as a key tool in the analysis of a
higher criticism test for detection of sparse signals observed in correlated noise. Here it is stated
as it was formulated and proven in [21].

Lemma 5.5 (Hall and Jin [21]). Let Σn, n ě 1 be a sequence of n ˆ n correlation matrices, such
that }Σn} ě c ą 0, where }Σn} is the operator norm of Σn as an operator from Rn to Rn. If for
some constants κ ą 0, C ą 0,

|Σnpi, jq| ď Cp1 ` |i ´ j|q´p1`κq,

then there is a constant C 1 ą 0 depending on κ, C, and c, such that

|Σ´1

n pi, jq| ď C 1p1 ` |i ´ j|q´p1`κq.

5.1.3 Long-run variance of partial sums of a stationary time series

Here we recall the well-known result given in Theorem 28.2.1 of Ibragimov and Linnik [23] on the
explicit formula for the variance of the sum of n consecutive realizations of a stationary process.
We adapt the notation to our case.
Suppose that pXnqnPZ is a centered stationary sequence with the autocovariance function γphq,
h P Z and the spectral density f pνq, ν P r´1{2, 1{2q. Let Sn “

Xi.

nř

i“1

16

Theorem 5.6 (Ibragimov and Linnik [23]). The variance of Sn in terms of γphq and f pνq is given
by

ÿ

VarrSns “

pn ´ |h|qγphq “

|h|ăn

ż

1{2

´1{2

sin2pπnνq
sin2pπνq

f pνq dν.

If the spectral density f pνq is continuous at ν “ 0, then

VarrSns “ f p0qn ` opnq, n Ñ 8.

5.2 Proofs for Section 2

5.2.1 Upper detection bound

For I P Ipλnq we deﬁne

σnpIq :“ 1T

I Σn1I .

Lemma 5.7 (Upper detection bound). Fix α P p0, 1q, consider the testing problem (1.3) and
suppose that Assumption 1 and Assumption 2 hold. In addition, assume that the sequence pΣnqnPN
of covariance matrices satisﬁes

∆n

inf
IPIpλnq

tnλnua
σnpIq

?

Á p

a

2 ` ˜εnq

´ log λn,

(5.1)

?

?

´ log λn´

log log n Ñ

as n Ñ 8, where p˜εnqnPN is a sequence of real numbers that satisﬁes ˜εn Ñ 0 and ˜εn
8 as n Ñ 8.
Then the sequence of level α tests pΦa
for all n P N and lim supnÑ8

nqnPN as in (2.1) with suitably chosen cα,n satisﬁes ¯αpΦa

nq ď α.

¯βpΦa

nq ď α

Proof. Deﬁne the test statistic

TnpY q “ sup

IPIpλnq

ˇ
ˇ

ˇ
ˇ1T
I Y
a
σnpIq

,

and recall that Φa
We begin by noting that although the random process

npY q :“ 1tTnpY q ą cα,nu, for some threshold cα,n to be determined later.

ˇ
ˇ

˜ ˇ
ˇ1T
I Y
a
σnpIq

¸

IPIpλnq

has an inﬁnite index set, it only takes ﬁnitely many diﬀerent values. Thus, we can ﬁnd a ﬁnite
representative system Iﬁnpλnq Ď Ipλnq, such that for any I P Ipλnq there is I 1 P Iﬁnpλnq, such
that

ˇ
ˇ

ˇ
ˇ1T
I Y
a
σnpIq

“

ˇ
ˇ

ˇ
ˇ1T
I 1Y
a
σnpI 1q

,

which implies that

TnpY q “ sup

IPIfinpλnq

ˇ
ˇ

ˇ
ˇ1T
I Y
a
σnpIq

i.e. TnpY q can be written as the supremum over the absolute values of a Gaussian process with a
ﬁnite index set. Let Mn such that E0TnpY q ď Mn. Then, for any λ ą 0, it follows that

P0 pTnpY q ą λ ` Mnq ď P0 pTnpY q ´ E0TnpY q ą λq

˜ˇ
ˇ
ˇ
ˇ
ˇ

ď P0

sup
IPIfinpλnq

|1T
I Y |a
σnpIq

´ E0

sup
IPIfinpλnq

|1T
I Y |a
σnpIq

17

¸

ˇ
ˇ
ˇ
ˇ
ˇ ą λ

ď 2e´ λ2
2 ,

where the last inequality follows the results of Talagrand [41] and can be found in Theorem 2.1.20
of [18]. Thus, if we let

c

cα,n “

2 log

2
α

` Mn,

n has level α for any n.

Φa
In order to ﬁnd a suitable bound Mn we consider an even coarser ﬁnite subset of Ipλnq. Let

"„

k
n

,

k
n

Cn “

˙

*

` λn

: 1 ď k ď tnp1 ´ λnqu

Ď Ipλnq.

Clearly, #Cn “ tnp1 ´ λnqu ď n ă 8. For any I P Ipλnq there is I 1 P Cn, such that 1I diﬀers from
1I 1 in at most one entry. Thus, it is easy to see that

TnpY q ď sup
IPCn

ˇ
ˇ

ˇ
ˇ1T
I Y
a
σnpIq

`

sup1ďiďn |Yi,n|

a

inf IPIpλnq

σnpIq

,

Thus, we can set

where

and

Mn “ ˜Mn ` κn,

˜Mn “ E0 sup
IPCn

ˇ
ˇ

ˇ
ˇ1T
I Y
a
σnpIq

,

κn “ E0

sup1ďiďn |Yi,n|

a

inf IPIpλnq

σnpIq

.

The latter term is easy to handle: We have

a

|Yi,n| ď

2f0 logp2nq,

E0 sup
1ďiďn

since Yi,n has variance f0 for any n and 1 ď i ď n, and

inf
IPIpλnq

σnpIq “ nλnf p0qp1 ` op1qq

by Theorem 18.2.1 of Ibragimov and Linnik [23]. Thus,

˜c

¸

,

log n
nλn

κn “ O

and thus, κn Ñ 0 by Assumption 1. The next part of the proof will be devoted to computing ˜Mn.
Note that under H0, we have

1T
I Ya
σnpIq

„ N p0, 1q,

for any I P Cn. For any I, I 1 P Cn, we have

ˇ
ˇ
ˇ
ˇ
ˇ

1T
I Ya
σnpIq

´

1T
I 1Ya
σnpI 1q

˜

ˇ
ˇ
ˇ
ˇ
ˇ “

ˇ
ˇ
ˇ
ˇ
ˇ

1T
Ia
σnpIq

´

1T
I 1a
σnpI 1q

¸

ˇ
ˇ
ˇ
ˇ
ˇ “

Y

˜

2 ´ 2

a

1T
I Σn1I 1
σnpIqσnpI 1q

¸ 1

2

|ZI,I 1|,

for some random variable ZI,I 1 „ N p0, 1q. Note that the system tZI,I 1
necessarily independent. Let

: I, I 1 P Cnu is not

ˆ

dnpI, I 1q :“

2 ´ 2

˙ 1

2

.

1T
I Σn1I 1
σnpIq

18

Since Σn is a Toeplitz matrix, it follows that σnpIq “ σnpI 1q for any I, I 1 P Cn, and thus, dnpI, I 1q “
dnpI 1, Iq. Since Σn is also positive deﬁnite, it is then easy to see that dn is a metric on Cn.
Now let En Ď Cn be an ηn-net for pCn, dnq, i.e. for any I P Dn there is J P En, such that

For any I P Cn and J P En we have

ˇ
ˇ

ˇ
ˇ1T
I Y
a
σnpIq

and thus,

dnpI, Jq ď ηn.

ˇ
ˇ
ˇ
ˇ
ˇ

ď

1T
I Ya
σnpIq

´

1T
J Ya
σnpJq

ˇ
ˇ
ˇ
ˇ
ˇ `

ˇ
ˇ

ˇ
ˇ1T
J Y
a
σnpJq

,

ˇ
ˇ

ˇ
ˇ1T
I Y
a
σnpIq

sup
IPCn

ď sup
IPCn

inf
JPEn

“ sup
IPCn

inf
JPEn

ˇ
ˇ
ˇ
ˇ
ˇ

1T
I Ya
σnpIq

´

1T
J Ya
σnpJq

ˇ
ˇ

ˇ
ˇ1T
J Y
a
σnpJq

ˇ
ˇ
ˇ
ˇ
ˇ ` sup
JPEn
ˇ
ˇ
ˇ1T
ˇ
J Y
a
σnpJq

dnpI, Jq|ZI,J | ` sup
JPEn
ˇ
ˇ1T
J Y
a
σnpJq

|ZI,J | ` sup
JPEn

ˇ
ˇ

.

ď ηn sup
IPCn

inf
JPEn

It follows that

a

a

˜Mn ď ηn
ď ηn

a

2 logp2nq `
a

2 log p2N pCn, dn, ηnqq

a

2 log n `

2 log N pCn, dn, ηnq ` p1 ` ηnq

2 log 2,

where N pCn, dn, ηnq is the ηn-covering number of pCn, dnq. Now let I, I 1 P Cn, I ‰ I 1, with
dH pI, I 1q ď λn
log n , where dH denotes the Hausdorﬀ metric on the set of subintervals of r0, 1s (with
respect to the euclidean distance on r0, 1s), i.e. dH pI, I 1q “ | inf I ´ inf I 1|. In addition, we assume
that inf I ă inf I 1 without loss of generality. Then

I Σn1I 1 “ p1T
1T
“ 1T

IXI 1 ` 1T
IXI 1 Σn1IXI 1 ` 1T

IzI 1qΣnp1IXI 1 ` 1I 1zI q

IzI 1Σn1IXI 1 ` 1T

IXI 1 Σn1I 1zI ` 1T

IzI 1Σn1I 1zI .

Due to Σn being symmetric and Toeplitz, we have 1T

IXI 1 Σn1I 1zI “ 1T

IXI 1Σn1IzI 1, and thus,

I Σn1I 1 “ 1T
1T

I Σn1I ´ 1T

IzI 1Σn1IzI 1 ` 1T

IzI 1Σn1I 1zI .

It follows that

d2
npI, I 1q “ 2 ´ 2

`

1T
I Σn1I
˘

´1

˘

´1

”
I Σn1I ´ 1T
1T

”
IzI 1Σn1IzI 1 ´ 1T
1T

`

“ 2

1T
I Σn1I

IzI 1Σn1IzI 1 ` 1T

IzI 1Σn1I 1zI

ı

IzI 1Σn1I 1zI

.

ı

Since 1T
entry is ftnλnu´1´rn , we ﬁnd the trivial bound

IzI 1Σn1I 1zI is the sum over a submatrix with rn “ n| inf I ´ inf I 1| rows, and its lower left

ˇ
ˇ
ˇ1T

IzI 1Σn1I 1zI

ˇ
ˇ
ˇ ď

nλn
log n

8ÿ

M p1 ` |h|q´1´κ “ o

h“tnλnp1´1{ log nqu´1

ˆ

˙

,

nλn
log n

´

as n Ñ 8. We use Theorem 18.2.1 of Ibragimov and Linnik [23] to ﬁnd 1T

I Σn1I “ nλnf p0q

1 `

¯

op1q

and

1T
IzI 1Σn1IzI 1 ď f p0q

nλn
log n

p1 ` op1qq,

19

as n Ñ 8. This yields

´

¯

dnpI, I 1q ď

c

2
log n

` ζn,

where ζn “ o
inclusion

plog nq´ 1
"

2

. This implies that for any I P Cn and for large enough n we have the

I 1 P Cn : dH pI, I 1q ď
b

*

"

Ď

I 1 P Cn : dnpI, I 1q ď

c

2
log n

*

` ζn

.

λn
log n

Thus, if we choose ηn “

2
log n ` ζn, this yields the bound

N pCn, dn, ηnq ď

log n
2λn

,

and consequently,

˜Mn ď 2 ` ζn

Thus, if we choose
a

a

c

2 log n `

2 log

ˆ

c

`

1 ` ζn `

˙

a

2
log n

log n
2λn

2 log 2.

cα,n “ 2 ` ζn

2 log n `

2 log

c

c

2
α

`

2 log

log n
2λn

ˆ

c

`κn`

1 ` ζn `

2
log n

˙

a

2 log 2,

(5.2)

n will have level α for all n P N. Note that ζn

?

n, recall that, under H1, i.e.

the test Φa
the type II error of the test Φa
|δ| ą ∆n, and I P Ipλnq, we have

2 log n “ op1q as n Ñ 8. Concerning
if Y „ N pδ1I , Σnq for some δ with

˜

1T
I 1Ya
σnpI 1q

„ N

δ1T
I 11Ia
σnpI 1q

¸

, 1

.

for all I 1 P Ipλnq. For n large enough, it follows from plugging in (5.1) and (5.2), that

¯βpΦa

nq “ sup

IPIpλnq

sup
|δ|ě∆n

PI,δ rΦa
«

npY q “ 0s
ˇ
ˇ
ˇ
ˇ
ˇZI 1 `

“ sup

IPIpλnq

sup
|δ|ě∆n

ď sup

IPIpλnq

sup
|δ|ě∆n

P

P

sup
I 1PIpλnq
«ˇ
ˇ
ˇ
ˇ
ˇZI `
«

δ1T
I 1Ia
σnpIq

ﬀ

ˇ
ˇ
δ1T
ˇ
I 11Ia
ˇ
ˇ ď cα,n
σnpI 1q
ˇ
ﬀ
ˇ
ˇ
ˇ
ˇ ď cα,n

ﬀ

ď sup

IPIpλnq
«

sup
|δ|ě∆n

P

|δ|

1T
I 1Ia
σnpIq

ď P

|Z| ą ∆n

inf
IPIpλnq

1T
I 1Ia
σnpIq

´ |ZI | ď cα,n

ﬀ

´ cα,n

,

where pZI qIPIpλnq and Z are (not necessarily independent) standard Gaussian random variables.
Plugging in (5.1), we have

∆n

inf
IPIpλnq
c

1T
I 1Ia
σnpIq

´ cα,n

a

ě ˜εn

log

1
λn

´ 2 ´ ζn

2 log n ´

2 log

´

c

c

2
α
?

?

2 log

log n
2

ˆ

c

´κn´

1 ` ζn `

˙

a

2 log 2,

2
log n

log log n Ñ 8 by assumption and κn “ op1q and
2 log “ op1q as n Ñ 8, it follows that the right-hand side diverges to 8. This ﬁnishes the

´ log λn´

?

for n large enough. Since ˜εn
ζn
proof.

20

5.2.2 Lower detection bound

We start by giving some technicalities on LR-statistics required throughout the paper at several
places. As λn and Σn are known, the likelihood ratio Lδ,I “ Lδ,I pY q between the distributions of
Y under H0 and H n

δ,I is given by

„

LI,δ “ exp

δ1T

I Σ´1

n Y ´

1
2

δ21T

I Σ´1

n 1I



.

Note that, under H0, the likelihood ratio Lδ,I follows a log-normal distribution, i.e.

log LI,δ “ δ1T

I Σ´1

n Y ´

1
2

δ21T

I Σ´1

n 1I

H0„ N1

ˆ

´

1
2

˙

δ2 ˜σnpIq, δ2 ˜σnpIq

,

where

˜σnpIq :“ 1T

I Σ´1

n 1I .

Note that, under H0, for I, I 1 P Cn and δ P R, we have ELI,δ “ 1, Var LI,δ “ exp
˘
and CovpLI,δ, LI 1,δq “ exp

`

n 1I 1

´ 1. Finally, let

δ21T
I Σ´1
(
(cid:32)
rpk ´ 1qλn, kλnq : 1 ď k ď tλ´1
n u

I 0 :“

Ď Ipλnq

`

˘
δ2 ˜σnpIq

´ 1

be a system of non-overlapping intervals of length λn as deﬁned in (2.2).

Lemma 5.8 (Lower detection bound). Fix α P p0, 1q, and suppose that (1.5) holds. Let pΣnqnPN
be a sequence of covariance matrices, such that

a

∆n sup

˜σnpIq À

IPIpλnq

´?

¯ a

2 ´ εn

´ log λn,

(5.3)

where pεnqnPN is a sequence that satisﬁes εn Ñ 0 and εn
assume that for some m P N0

?

´ log λn Ñ 8 as n Ñ 8. In addition,

lim
nÑ8

1
tλ´1
n u2

ÿ

I,I 1PI0
n| inf I´inf I 1|ąm

Cov pLI,∆n , LI 1,∆n q “ 0,

(5.4)

as n Ñ 8.
Then any sequence of tests pΦnqnPN with lim supnÑ8 ¯α pΦnq ď α will obey lim supnÑ8
1 ´ α, i.e. the bump is asymptotically undetectable.

¯β pΦnq ě

Proof. We employ the same strategy as in the proof of Theorem 3.1(a) of D¨umbgen and Spokoiny
[14]. We bound the type II error of any given test by an expression that does not depend on the
test anymore, and then employ an appropriate L1-law of large numbers for dependent arrays of
random variables.
For any sequence of tests Φn with asymptotic level α under H0 we have

21

¯β pΦnq “ sup

IPIpλnq

sup
|δ|ě∆n

EI,δ r1 ´ ΦnpY qs

EI,δ r1 ´ ΦnpY qs

EI,δ r1 ´ ΦnpY qs

sup
|δ|ě∆n
ÿ

EI,∆n rΦnpY qs

sup
|δ|ě∆n
ÿ

ě sup
IPI0
1
tλ´1
n u

ě

ě 1 ´

ě 1 ´

IPI0
1
tλ´1
n u

1
tλ´1
n u
«˜

„

E0

ΦnpY q

IPI0
ÿ

IPI0



dPI,∆n
dP0

¸

´ ΦnpY q
ﬀ

´ α ` op1q

“ 1 ´ E0

ě 1 ´ α ´ E0

1
tλ´1
n u
ˇ
ˇ
ˇ
ˇ
ˇ

1
tλ´1
n u

ÿ

IPI0
ÿ

IPI0

LI,∆n ´ 1

ΦnpY q

´ α ` o p1q

ˇ
ˇ
ˇ
ˇ
ˇ ` o p1q .
LI,∆n ´ 1

Next, we show that the array
nition 5.1 or see Deﬁnition 1.5 of Sung et al. [40]), i.e. we show that

(cid:32)
L∆n,I : I P I 0, n P N

(

is h-integrable with exponent 1 (recall Deﬁ-

1
tλ´1
n u

sup
nPN

ÿ

IPI0

E0 r|LI,∆n |s ă 8,

and

lim
nÑ8

1
tλ´1
n u

IPI0

ÿ

E0 r|LI,∆n |1 t|LI,∆n | ą hpnqus “ 0,

(5.5)
. Since E0|LI,∆n | “ 1 for all n P N and I P I 0, the ﬁrst

where hpnq “ tλ´1
n u
condition is satisﬁed.
Further, if n large is enough, we have

2 p1`εnqp

2´εnq2

1

?

1
tλ´1
n u

ÿ

IPI0

E0 rLI,∆n 1 tLI,∆n ą hpnqus ď sup
IPI0

“ sup
IPI0
˜

˜

P

Z ď

E0 rLI,∆n 1 tLI,∆n ą hpnqus

¸

¸

1

2 ∆2

n ˜σnpIq ´ log hpnq
˜σnpIq

a

∆n
a

∆n

˜σnpIq ´

log hpnq
a

supIPI0 ∆n
¯

˜σnpIq

a

Z ď sup
IPI0

´

1
2

?

ď P

(a)
ď P

Z ď ´εnp

2 ´ εnq

´ log λn

,

where Z is a standard Gaussian random variable. The inequality (a) follows immediately from (5.3)
and the deﬁnition of hpnq. The claim follows from the assumption that limnÑ8 εn
´ log λn “ 8
as n Ñ 8.
Then, given that (5.4) and (5.5) hold, it follows from an L1-law of large numbers for dependent
arrays (recall Theorem 5.2 or see Theorem 3.2 of Wang and Hu [43]), that
ˇ
ˇ
ˇ
ˇ
ˇ

ˇ
ˇ
ˇ
ˇ
ˇ Ñ 0,

L∆n,I ´ 1

(5.6)

E0

ÿ

?

1
tλ´1
n u

IPI0

as n Ñ 8, which ﬁnishes the proof.

5.2.3 Proof of Theorem 2.1

In the setting described in Theorem 2.1 the noise vector ξn in model (1.1) is given by n consecutive
realizations of a stationary centered Gaussian process with the square summable autocovariance

22

function γphq, h P Z and the spectral density f . We suppose that Assumption 2 is satisﬁed, i.e.
the autocovariance of ξn has a polynomial decay. In terms of Σn, this means

|Σnpi, jq| ď Cp1 ` |i ´ j|q´p1`κq,

for 1 ď i, j ď n and some constants C ą 0 and κ ą 0.
In order to apply Lemma 5.8 in such a setting, ﬁrst, we need to examine the asymptotic behavior
of the coeﬃcients ˜σn pIq, and second, we need to verify that condition (5.4) is satisﬁed under the
lower detection boundary condition (5.3) and Assumption 2.
For the setting of Theorem 2.1, we will do the former in Lemma 5.9 and the latter in Lemma 5.10.

Lemma 5.9. If Assumption 2 holds, then for any I P Ipλnq, it follows that

˜σnpIq “

nλn
f p0q

p1 ` op1qq,

as n Ñ 8.

Proof. We are inspired by the proof of Proposition C.1 in Keshavarz et al. [29], that was dropped
from the ﬁnal paper [30], although we are able to make some simpliﬁcations, since a slightly
weaker result suﬃces for our purposes. In addition, we use this opportunity to ﬁx several minor
inaccuracies in their proof.
Recall that T pf q is the inﬁnite Toeplitz matrix generated by the spectral density f and that
Σn “ Tnpf q is the corresponding truncated Toeplitz matrix.
Let T pgq be the inﬁnite Toeplitz matrix generated by g “ 1{f , i.e. the matrix with elements
T pgqpi, jq “ g|i´j|, where g0, g1, . . . are the Fourier coeﬃcients of g. Let Hpf q and Hpgq be the
Hankel matrices generated by f and g, respectively, i.e. the matrices

Hpf q “

¨

˚
˚
˚
˝

f1
f2
f3
...

f2
f3
f4
...

f3
f4
f5
...

. . .
. . .
. . .
. . .

˛

‹
‹
‹
‚ and Hpgq “

¨

˚
˚
˚
˝

g1
g2
g3
...

g2
g3
g4
...

g3
g4
g5
...

˛

‹
‹
‹
‚

. . .
. . .
. . .
. . .

It follows from Proposition 1.12 of B¨ottcher and Silbermann [4], that

T pf q´1 “ T pgq ` T pf q´1Hpf qHpgq.

Let vI be the extension of the vector 1I to an element of l2 by zero-padding. As in Keshavarz
et al. [30], from the above identity and the deﬁnition of the operator norm, we ﬁnd
ˇ
@
ˇ
Hpf qT pf q´1vI , HpgqvI
“
ď }Hpf qT pf q´1}}vI }(cid:96)2}HpgqvI }(cid:96)2
a

I T pf q´1vI ´ vT

I T pgqvI

ˇ
ˇvT

Dˇ
ˇ

ÿ

»

ﬁ

ˇ
ˇ

ď }Hpf qT pf q´1}

–

nλn

}Hpgqer}(cid:96)2

tr:vI prq“1u

ﬂ

where er “ p0, . . . , 0, 1, 0, . . .qT is the sequence whose r-th entry is 1, and }Hpf qT pf q´1} is the
operator norm of Hpf qT pf q´1 as an operator from (cid:96)2 to (cid:96)2. Since }T pf q} “ supνPr0,1q f pνq ă 8,
we have }T pf q´1} ă 8 by the inverse mapping theorem.
It follows that }Hpf qT pf q´1} ă 8,
because clearly }Hpf q} ă 8. Since f is bounded away from 0, it is well known that the Fourier
coeﬃcients gk, k P Z of g decay at the same rate as the Fourier coeﬃcients of f , i.e.

|gk| ď C 1p1 ` |k|q´p1`κq,

23

for k P Z. Following Keshavarz et al. [30]) we see that
¸ 1

˜

ÿ

ÿ

tr:vI prq“1u

}Hpgqer}(cid:96)2 “

tr:vI prq“1u
ÿ

ď C 2

|gj|2

2

ď

8ÿ

j“r

r

tr:vI prq“1u
tnλnuÿ

r´p 1

2 `κq ď C 2

r´p 1

2 `κq.

ÿ

ˆż

8

˙ 1

2

x´2p1`κqdx

tr:vI prq“1u

´

r“1
¯

It is then easy to see that the last expression is O
Lastly, it is also clearly bounded if κ ą 1

pnλnq

1
2 ´κ

if κ ă 1

2 , and O plogpnλnqq if κ “ 1
2 .

2 . Hence, in any of these cases it holds that

ÿ

tr:vI prq“1u

´a

¯

}Hpgqer}(cid:96)2 “ o

nλn

.

Thus,

ˇ
ˇvT

I T pf q´1vI ´ vT

I T pgqvI

ˇ
ˇ

“ o pnλnq .

We now need to bound vT
density g. Then

I T pgqvI . Let pXtqtPN be a stationary random process with spectral

vT
I T pgqvI “ Var

¨

˝

ÿ

tt:vI ptq“1u

˛

‚“ nλnpgp0q ` op1qq,

Xt

as n Ñ 8, where the last equality is due to Theorem 18.2.1 of Ibragimov and Linnik [23], see
Section 5.3 of the Appendix for the precise statement of the theorem. (Note that g is continuous
at 0 and gp0q ą 0.) Thus,

I T pf q´1vI “ nλnpgp0q ` op1qq.
vT
Finally, by Theorem 2.11 of B¨ottcher and Grudsky [3], we have

˜σnpIq “ vT

I Σ´1

n vI “ vT

I T pf q´1vI ` ˜vT
I

“

T pf q´1 ´ T pgq

‰

˜vI ` vT

I DnvI ,

where }Dn} Ñ 0, as n Ñ 8, and ˜vI arises from vI through the transformation

As above, we have

˜vI “ pvI pnq, . . . , vI p1q, 0, 0, . . .q .

ˇ
ˇ˜vT

I

“

‰
T pf q´1 ´ T pgq

ˇ
ˇ

˜vI

“ o pnλnq ,

and clearly, by Cauchy-Schwarz,
ˇ
ˇvT

I DnvI

ˇ
ˇ ď }vI }2}Dn} “ o pnλnq .

This concludes the proof.

Lemma 5.10. If Assumption 2 holds, and given that

a

∆n sup

˜σnpIq À p

IPIpλnq

?

a

2 ´ εnq

´ log λn

(5.7)

for a sequence pεnqnPN satisfying εn Ñ 0 and εn
holds with m “ 1, i.e.

?

´ log λn Ñ 8 as n Ñ 8, then condition (5.4)

ÿ

lim
nÑ8

λ2
n

I,I 1PI0
n| inf I´inf I 1|ą1

∆2

n1T

I Σ´1

n 1I 1

˘

´ 1 “ 0,

`

exp

24

Proof. For I, I 1 P I 0 with n| inf I ´ inf I 1| ą 1. Write

`

exp

∆2

n1T

I Σ´1

n 1I 1

˘

´ 1 “

“

8ÿ

p“1
8ÿ

p“1

1
p!

1
p!

“

∆2

n1T

I Σ´1

n 1I 1

‰p

„

a

1
2

∆2
n

«



p

˜σnpIq˜σnpI 1q

2

a

1T
I Σ´1
n 1I 1
˜σnpIq˜σnpI 1q

ﬀ
p

.

If nλn is an integer, the latter term 1T
n , and if
nλn is not an integer, then the number of non-zero entries of 1I and 1I 1 cannot diﬀer by more
than 1. From Lemma A.1 of [21] (see also Section 5.1.2 in the appendix), it trivially follows that

n 1I 1 is the sum over a square submatrix of Σ´1

I Σ´1

ˇ
ˇ1T

I Σ´1

n 1I 1

ˇ
ˇ

ď C 1 rnλns

ď C 1 rnλns

rnλnsÿ

t“1
rnλnsÿ

t“1

`

˘
n| inf I ´ inf I 1| tnλnu ` t

´p1`κq

ptnλnuq´p1`κq “ o pnλnq .

From Lemma 5.9, we know that

a

´1

a

˜σnpIq˜σnpI 1q “ nλn

f p0q p1 ` op1qq as n Ñ 8, and thus, it follows

that

˜σnpIq˜σnpI 1q

1T
I Σ´1

n 1I 1 Ñ 0 as n Ñ 8. Hence, for n large enough, we have

„

ˇ
ˇ
ˇ
ˇ
ˇ

8ÿ

p“1

1
p!

a

1
2

∆2
n

«



p

˜σnpIq˜σnpI 1q

2

a

1T
I Σ´1
n 1I 1
˜σnpIq˜σnpI 1q

ď 2

ﬀ
p

ˇ
ˇ
ˇ
ˇ
ˇ

ˇ
ˇ

ˇ
ˇ1T
I Σ´1
n 1I 1
a
˜σnpIq˜σnpI 1q

„

a

1
2

∆2
n

exp



˜σnpIq˜σnpI 1q

.

(5.8)

Note that from the lower detection boundary condition (5.7) it immediately follows that

„

1
2

∆2
n

exp

a



˜σnpIq˜σnpI 1q

?

ď λ´ 1
2 p

n

2´εnq2

ď λ´1
n

(5.9)

for n large enough. Applying Lemma A.1 of [21] again, it follows that |Σ´1
n pi, jq| ď Cp1 ` |i ´
j|q´p1`κq for some C ą 0. Let Φn be the n ˆ n-matrix with entries Φnpi, jq “ Cp1 ` |i ´ j|q´p1`κq,
and let Φpνq “

h“´8 Cp1 ` |i ´ j|q´p1`κqe´2πihν. Then

ř

8

ÿ

I,I 1PI0
n| inf I´inf I 1|ą1

|1T

I Σ´1

n 1I 1| ď

ÿ

I,I 1PI0
I‰I 1

1T
I Φn1I 1 ď

nÿ

i,j“1

Φnpi, jq ´

ÿ

IPI0

1T
I Φn1I

(a)
“ opnq,

(5.10)

where (a) follows from Theorem 18.2.1 of Ibragimov and Linnik [23], since it yields that
nΦp0q ` opnq and 1T
I Φn1I “ nλnΦp0q ` o pnλnq for any I P I 0.
Finally, combining (5.8), (5.9) and (5.10), and once again using that
as n Ñ 8, we ﬁnd

˜σnpIq˜σnpI 1q “ nλn

a

f p0q p1`op1qq

ř
n
i,j“1 Φnpi, jq “

ÿ

I,I 1PI0
n| inf I´inf I 1|ą1

ˇ
ˇ

ˇ
ˇ1T
I Σ´1
n 1I 1
a
˜σnpIq˜σnpI 1q

2

„

1
2

∆2
n

exp

a



ˆ

˜σnpIq˜σnpI 1q

“ o

˙

,

1
λ2
n

which concludes the proof.

Since Lemma 5.10 guarantees that Lemma 5.8 can be applied in the setting of Theorem 2.1, the
proof of the latter now follows immediately from Lemmas 5.7 and 5.8.

25

Proof of Theorem 2.1. The two Lemmas 5.7 and 5.8 yield that the asymptotic detection boundary
is (in terms of ∆n) given by
c

c

d

d

?
p

2 ´ εnq

´ log λn
nλn

sup
IPIpλnq

nλn
˜σnpIq

À ∆n À p

?

2 ` ˜εnq

´ log λn
nλn

inf
IPIpλnq

σnpIq
nλn

,

(5.11)

as n Ñ 8. For any I P Ipλnq, it follows from Theorem 18.2.1 of Ibragimov and Linnik [23] that

and Lemma 5.9 yields

σnpIq “ nλnf p0qp1 ` op1qq,

˜σnpIq “

nλn
f p0q

p1 ` op1qq,

as n Ñ 8. Plugging this into (5.11) ﬁnishes the proof.

5.2.4 Remaining proofs

Proof of Theorem 2.2. Note that for any 1 ď k ď tλ´1
with ˜σk as in (2.5) are identically distributed (dependent) standard Gaussian. Note that ˜σk “
˜σn pIkq with our former notation. The union bound and the elementary tail inequality P r|Z| ą xs ď
2e´x2{2 for Z „ N p0, 1q, yield

n u, under H0, the random variables

1T
Ik

Σ´1
n Y
?
˜σk

˜αpΦd

nq “ P0 rTnpY q ą cα,ns

«

ď tλ´1
n u

sup
1ďkďtλ´1
n u

P0

|1T
Σ´1
n Y |
Ik
?
˜σk

ﬀ

ą cα,n
˜

¸

“ tλ´1

n uP r|Z| ą cα,ns ď 2tλ´1

n u exp

c2
α,n
2

´

ď α.

`

˘

ď α for all n P N.
This proves ˜α
Concerning the type II error, note that, under H1, i.e.
t1, . . . , tλ´1

Φd
n

n uu, we have for all local test statistics on the right-hand side of (2.4) that

if Y „ N pδn1Ik , Σnq for some k P

1T
Im
?

Σ´1
n Y
σm

„ N

ˆ

n 1Ik

δn1Im Σ´1
?
σm

˙

, 1

, m “ 1, . . . , tλ´1

n u.

Plugging in (2.6) and (2.8), it follows that the type II error satisﬁes

˜βpΦd

n, Σn, ∆n, λnq “ sup

1ďkďtλ´1
n u

sup
|δn|ě∆n

Pδn,k
«

“

‰
npY q “ 0

Φd

ﬀ

ˇ
ˇ
ˇ
ˇ ď cα,n

ˇ
ˇ
ˇ
ˇZm `
ˇ
ˇ
ˇ ď cα,n

˜σk

δn1Ik Σ´1
n 1Im
?
σm
ı

“ sup

1ďkďtλ´1
n u

sup
|δn|ě∆n

ď sup

1ďkďtλ´1
n u

sup
|δn|ě∆n

ď sup
«

1ďkďtλ´1
n u

sup
|δn|ě∆n

P

P

P

sup
1ďmďtλ´1
n u
”ˇ
a
ˇ
ˇZk ` δn
”
|δn|

a

ı

˜σk ´ |Zk| ď cα,n
ﬀ

ď P

|Z| ą ∆n

which proves the claim.

a

inf
1ďkďtλ´1
n u

˜σk ´ cα,n

26

Proof of Corollary 2.3 . The claim follows directly from Theorem 2.2 and the standard Gaussian
tail bound P r|Z| ą xs ď 2e´x2{2 via

«

P

|Z| ą ∆n

“P

ďP

”
|Z| ą p1 ` εnq
”
|Z| ą εn
´

a

ˆ

inf
1ďkďtλ´1
n u
a

ﬀ

a

˜σk ´ cα,n

a

ı

´2 log λn ´

´2 logpλnq ` 2 logp2α´1q

´2 log λn ´
a

a

ı
2 logp2α´1q
a

¯

˙

2

ď exp

´

ˆ

ď exp

´

εn
´a

´2 log λn ´
˙

¯
2

2 logpα´1q

2 logp2α´1q

“ α.

1
2
1
2

5.3 Proofs for Section 3

Proof of Theorem 3.1. It is well-known (see, for example, [36], Sections 3.3–3.4), that the autoco-
variance function γ of an ARMA process is exponentially decaying, i.e.

|Σnpi, jq| “ |γpi ´ jq| ď Ce´κ|i´j|,

for some C ą 0, some κ ą 0 and all 1 ď i, j ď n. Thus, Assumption 2 is satisﬁed, and Theorem
3.1 follows immediately from Theorem 2.1.

5.3.1 Properties of the precision matrix of an AR(p) process

Let Zt be a stationary AR(p) process deﬁned in (3.5) and Σn be the covariance matrix of n
consecutive realizations of Zt. Then the precision matrix Σ´1
n pi, jqq, i, j “ 1, . . . , n is a
n ˆ n symmetric 2p ` 1-diagonal matrix with the upper-triangle elements given by (see [37])

n “ pΣ´1

Σ´1

n pi, jq “

φtφt`j´i, 1 ď i ď n ´ p, maxpi, p ` 1q ď j ď i ` p

(5.12)

$

’’’’’’’’’&
’’’’’’’’’%

i´1ř

t“0
p`i´jř

t“0
n´jř

t“0
0,

φtφt`j´i,

1 ď i ď j ď p

φtφt`j´i,

n ´ p ` 1 ď i ď j ě n

i ` p ă j ď n, i ď n ´ p.

n is symmetric with respect to both the main diagonal and the antidiagonal, so that
n pj, iq and Σ´1

Note that Σ´1
Σ´1
n pi, jq “ Σ´1
We can see from (5.12) that Σ´1
the elements related as lij “ rp`1´i,p`1´j “ Σ´1
The other elements of Σ´1
i “ p ´ k ` 1, k “ 1, . . . , p (blue parts of the matrix in Fig. 5.3.1), where

n has two symmetric blocks L “ plijq and R “ prijq of size p with
n pi, jq, i, j “ 1, . . . , p (red blocks in Fig. 5.3.1).
n pi, i ` kq “ Dk,

n are constant on the diagonals and are given by Σ´1

n pn ` 1 ´ j, n ` 1 ´ iq.

n pi, jq “ Σ´1

p´kÿ

Dk “

φtφt`k,

k “ 0, . . . , p.

t“0

We are interested in the diagonal block sums of Σ´1
1 ď r ă tn{2u ´ p. The block sums of interest are

n over the blocks of size r. We suppose that

Sr,m “ 1T

r,mΣ´1

n 1r,m, m “ 1, . . . , n ´ r ` 1

(5.13)

27

p

L

0

Σ´1

n “

D

n ´ 2p

0

R

p

Figure 6: The matrix Σ´1

n is symmetric 2p ` 1-diagonal, the blocks L and R of size p are of size
p, the blue part is has the same values Dk on the diagonals. The white part consists of
zeros.

where 1r,m P Rn is the vector with entries
#

1r,mpiq “

i “ m, . . . , m ` r ´ 1,

1,
0, otherwise

Note that the key quantities ˜σk that appear in the lower and upper bounds of testing (5.3) and
(2.8) are related to (5.13) as follows,

˜σk “ Stnλnu,pk´1qtnλnu`1,

k “ 1, . . . , tλ´1

n u.

Lemma 5.11. Suppose that 1 ď r ď n ´ 2p and that n ě 3p. The quantities Sr,m, m “
1, . . . , n ´ r ` 1 can be calculated directly using the following recursive formulas.

1. The ﬁrst block sum is given by
ˆ

Sr,1 “

$
’’’&
’’’%

rř

j´1ř

j“1
př

ˆ

t“0
j´1ř

˙2

φt

,

˙2

φt

` pr ´ pq

1 ď r ď p,

˙

2

φt

,

p ď r ď n ´ p

ˆ

př

t“0

(5.14)

j“1

t“0

2. If r ď p, then

Sr,m`1 “ Sr,m `

ˆ

ˆ

˙2

φt`i

˙2

φt`i

,

,

r´1ř

t“0
p´iř

t“0

1 ď m ď p ` 1 ´ r

p ` 1 ´ r ď m ď p

$

’’’’’’’’’’’’’’’&
’’’’’’’’’’’’’’’%

0,

´

´

˜

r´1ř

ˆ

t“n´i´p
r´1ř

φn´i´t

t“0

p ` 1 ď m ď n ´ p ´ r

(5.15)

, n ´ p ´ r ` 1 ď m ď n ´ p

¸
2

φn´i´t
˙2

,

n ´ p ď m ď n ´ r

28

3. If p ď r ď n ´ 2p, then

Sr,m`1 “ Sr,m `

$

’’’’’’&
’’’’’’%

ˆ

p´iř

t“0

0,

˜

´

˙2

φt`i

,

1 ď m ď p

p ` 1 ď im ď n ´ p ´ r

(5.16)

¸
2

φn´i´t

, n ´ p ´ r ` 1 ď m ď n ´ r.

r´1ř

t“n´i´p

The proof of the lemma is omitted. It follows from simple algebra and the relation

pÿ

D0 ` 2

Dk “

k“1

pÿ

t“0

φ2

t ` 2

pÿ

p´kÿ

˜

pÿ

¸
2

φtφt`k “

φt

.

k“1

t“0

t“0

Using the result of Lemma 5.11, we can calculate the constants ˜σk.

Proof of Lemma 3.2. According to deﬁnition 5.13, the quantities ˜σk can be written as

˜σk “ Stnλnu,pk´1qtnλnu`1,

k “ 1, . . . , tλ´1

n u.

Note that it follows immediately from Lemma 5.11 that for any ﬁxed 1 ď r ď n ´ 2p the function
Sr,m, m “ 1, . . . , n´r`1 is monotone increasing for m ď p`1, constant for p`1 ď m ď n´p´r`1
and decreasing for m ě n ´ p ´ r ` 1. Moreover, this function is symmetric in a sense that
Sr,m “ Sr,n´r´m`2, m “ 1, . . . , n ´ r ` 1. Therefore, it follows that

inf
1ďkďtλ´1
n u

˜σk “ min

1ďkďtλ´1
n u

Stnλnu,pk´1qtnλnu`1 “ Stnλnu,1

and

sup
1ďkďtλ´1
n u

σk “ max

1ďkďtλ´1
n u

Stnλnu,pk´1qtnλnu`1 “ Stnλnu,p`1.

Note that the condition tnλnu ă n´2p will guarantee that the maximum is attained at the interval
where the function S is constant (for some k that satisﬁes p ` 1 ď pk ´ 1qtnλnu ` 1 ď n ´ p ´ r ` 1)
and, consequently, will be equal to Stnλnu,p`1.
We obtain the statement of the lemma applying the recursive formulas of Lemma 5.11.

Acknowledgments

Axel Munk and Frank Werner gratefully acknowledge ﬁnancial support by the German Research
Foundation DFG through subproject A07 of CRC 755, and Markus Pohlmann acknowledges sup-
port through RTG 2088. We are furthermore grateful to helpful comments of two anonymous
referees and the associate editor.

References

[1] Aue, A. and Horv´ath, L. (2013). Structural breaks in time series. Journal of Time Series

Analysis, 34(1):1–16.

[2] Baraud, Y. (2002). Non-asymptotic minimax rates of testing in signal detection. Bernoulli,

8(5):577–606.

[3] B¨ottcher, A. and Grudsky, S. M. (2000). Toeplitz matrices, asymptotic linear algebra and

functional analysis, volume 67. Springer.

29

[4] B¨ottcher, A. and Silbermann, B. (2012). Introduction to large truncated Toeplitz matrices.

Springer.

[5] Boysen, L., Kempe, A., Liebscher, V., Munk, A., and Wittich, O. (2009). Consistencies and
rates of convergence of jump-penalized least squares estimators. Ann. Statist., 37(1):157–183.

[6] Brockwell, P. J. and Davis, R. A. (1991). Time Series: Theory and Methods. Springer New

York.

[7] Brodsky, E. and Darkhovsky, B. S. (1993). Nonparametric methods in change-point problems.

Kluwer Acad. Publ., the Netherlands.

[8] Cabrera, M. O. and Volodin, A. I. (2005). Mean convergence theorems and weak laws of large
numbers for weighted sums of random variables under a condition of weighted integrability.
Journal of Mathematical Analysis and Applications, 305(2):644–658.

[9] Carlstein, E., M¨uller, H.-G., and Siegmund, D., editors (1994). Change-point problems, volume
Volume 23 of Lecture Notes–Monograph Series. Institute of Mathematical Statistics, Hayward,
CA.

[10] Chakar, S., Lebarbier, E., L´evy-Leduc, C., Robin, S., et al. (2017). A robust approach for

estimating change-points in the mean of an AR(1) process. Bernoulli, 23(2):1408–1447.

[11] Chan, H. P. and Walther, G. (2011). Detection with the scan and the average likelihood ratio.

Statist. Sinica, 23:409–428.

[12] Cs¨org˝o, M. and Horv´ath, L. (1997). Limit Theorems in Change-Point Analysis. Wiley &

Sons.

[13] Dette, H., Sch¨uler, T., and Vetter, M. (2018). Multiscale change point detection for dependent

data. arXiv preprint arXiv:1811.05956.

[14] D¨umbgen, L. and Spokoiny, V. G. (2001). Multiscale testing of qualitative hypotheses. Ann.

Statist., 29(1):124–152.

[15] D¨umbgen, L. and Walther, G. (2008). Multiscale inference about a density. Ann. Statist.,

36(4):1758–1785.

[16] Enikeeva, F., Munk, A., and Werner, F. (2018). Bump detection in heterogeneous gaussian

regression. Bernoulli, 24(2):1266–1306.

[17] Frick, K., Munk, A., and Sieling, H. (2014). Multiscale change point inference. J. R. Stat.
Soc. Ser. B. Stat. Methodol., 76(3):495–580. With discussions and a rejoinder by the authors.

[18] Gin´e, E. and Nickl, R. (2016). Mathematical Foundations of Inﬁnite-Dimensional Statistical

Models. Cambridge University Press.

[19] Goldenshluger, A., Juditsky, A., and Nemirovski, A. (2015). Hypothesis testing by convex

optimization. Electron. J. Statist., 9(2):1645–1712.

[20] Gut, A. (1992). The weak law of large numbers for arrays. Statistics & Probability Letters,

14(1):49 – 52.

[21] Hall, P. and Jin, J. (2010). Innovated higher criticism for detecting sparse signals in correlated

noise. The Annals of Statistics, 38(3):1686–1732.

[22] Hotz, T., Sch¨utte, O. M., Sieling, H., Polupanow, T., Diederichsen, U., Steinem, C., and
Idealizing ion channel recordings by a jump segmentation multiresolution

Munk, A. (2013).
ﬁlter. IEEE Transactions on NanoBioscience, 12(4):376–386.

30

[23] Ibragimov, I. and Linnik, J. (1971). Independent and stationary sequences of random variables.

Wolters-Noordhoﬀ.

[24] Ibragimov, I. A. and Has’minskii, R. Z. (2013). Statistical estimation: asymptotic theory.

Springer Science & Business Media.

[25] Ingster, Y. and Suslina, I. (2003). Nonparametric goodness-of-ﬁt testing under Gaussian

models. Springer, New York.

[26] Ingster, Y. I. (1993). Asymptotically minimax hypothesis testing for nonparametric alterna-

tives. I-III. Math. Methods Statist., 2:85–114, 171–189, 249–268.

[27] Jaﬀard, S. (1990). Propri´et´es des matrices bien localis´ees pr`es de leur diagonale et quelques

applications. Annales de l’I.H.P. Analyse non lin ˜A c(cid:13)aire, 7(5):461–476.

[28] Jeng, X. J., Cai, T. T., and Li, H. (2010). Optimal sparse segment identiﬁcation with
application in copy number variation analysis. J. Amer. Statist. Assoc., 105(491):1156–1166.

[29] Keshavarz, H., Scott, C., and Nguyen, X. L. (2015). Optimal change point detection in

gaussian processes. arXiv preprint 1506.01338.

[30] Keshavarz, H., Scott, C., and Nguyen, X. L. (2018). Optimal change point detection in

gaussian processes. Journal of Statistical Planning and Inference, 193:151–178.

[31] Krivobokova, T., Briones, R., Hub, J. S., Munk, A., and de Groot, B. (2012). Partial least-
squares functional mode analysis: Application to the membrane proteins aqp1, aqy1, and clc-ec1.
Biophysical Journal, 103(4):786–796.

[32] Neher, E. and Sakmann, B. (1995). Single-channel recording. Plenum Press, New York.

[33] Pein, F., Sieling, H., and Munk, A. (2017). Heterogeneous change point inference. J. R. Stat.

Soc. Ser. B. Stat. Methodol., 79(4):1207–1227.

[34] Pein, F., Tecuapetla-Gomez, I., Sch ˜A 1

4 tte, O. M., Steinem, C., and Munk, A. (2018). Fully-
automatic multiresolution idealization for ﬁltered ion channel recordings: Flickering event de-
tection. IEEE Trans. NanoBioscience, 17(3):300–320.

[35] Shen, A. and Volodin, A. (2017). Weak and strong laws of large numbers for arrays of rowwise

end random variables and their applications. Metrika, 80(6-8):605–625.

[36] Shumway, R. H. and Stoﬀer, D. S. (2000). Time series analysis and its applications. Studies

In Informatics And Control, 9(4):375–376.

[37] Siddiqui, M. M. (1958). On the inversion of the sample covariance matrix in a stationary

autoregressive process. The Annals of Mathematical Statistics, 29(2):585–588.

[38] Siegmund, D. (1985). Sequential analysis: tests and conﬁdence intervals. Springer Series in

Statistics. Springer.

[39] Singer, M., Krivobokova, T., and Munk, A. (2017). Kernel partial least squares for stationary

data. J. Mach. Learn. Res., 18:Paper No. 123, 41.

[40] Sung, S. H., Lisawadi, S., and Volodin, A. (2008). Weak laws of large numbers for arrays

under a condition of uniform integrability. J. Korean Math. Soc, 45(1):289–300.

[41] Talagrand, M. (2006). The generic chaining: upper and lower bounds of stochastic processes.

Springer Science & Business Media.

[42] Tsybakov, A. B. (2009). Introduction to nonparametric estimation, volume 11. Springer.

[43] Wang, X. and Hu, S. (2014). Weak laws of large numbers for arrays of dependent random

variables. Stochastics, 86(5):759–775.

31

