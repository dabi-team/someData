VisAR: Bringing Interactivity to Static Data Visualizations through
Augmented Reality

Taeheon Kim, Bahador Saket, Alex Endert, Blair MacIntyre

7
1
0
2

g
u
A
4

]

C
H
.
s
c
[

1
v
7
7
3
1
0
.
8
0
7
1
:
v
i
X
r
a

Fig. 1. This ﬁgure illustrates a presentation setting where augmented reality solutions can be used to support visual data exploration.
Users in the audience are able to independently view virtual content superimposed on the static visualization and perform interactive
tasks (e.g., ﬁltering) to explore the presented data without interrupting the presenter.

Abstract—Static visualizations have analytic and expressive value. However, many interactive tasks cannot be completed using
static visualizations. As datasets grow in size and complexity, static visualizations start losing their analytic and expressive power for
interactive data exploration. Despite this limitation of static visualizations, there are still many cases where visualizations are limited to
being static (e.g., visualizations on presentation slides or posters). We believe in many of these cases, static visualizations will beneﬁt
from allowing users to perform interactive tasks on them. Inspired by the introduction of numerous commercial personal augmented
reality (AR) devices, we propose an AR solution that allows interactive data exploration of datasets on static visualizations. In particular,
we present a prototype system named VisAR that uses the Microsoft Hololens to enable users to complete interactive tasks on static
visualizations.

Index Terms—Information visualization, augmented reality, immersive analytics.

1 INTRODUCTION

While it has been shown that static visualizations have analytic and
expressive value, they become less helpful as datasets grow in size and
complexity [18]. For example, many interactive tasks such as zooming,
panning, ﬁltering, and brushing and linking cannot be completed using
static visualizations. That is, interactivity of information visualizations
becomes increasingly important [24]. While adding interactivity to
visualizations is a common practice today, there are still cases where
visualizations are limited to being static. For example, visualizations

• Taeheon Kim, Bahador Saket, Alex Endert, and Blair MacIntyre are with

Georgia Tech. E-mail: {tkim, saket, endert, blair}@gatech.edu,
blair@cc.gatech.edu.

Manuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of Publication
xx xxx. 201x; date of current version xx xxx. 201x. For information on
obtaining reprints of this article, please send e-mail to: reprints@ieee.org.
Digital Object Identiﬁer: xx.xxxx/TVCG.201x.xxxxxxx

presented during presentations are restricted from being interactive to
the audience. However, interactivity in data visualizations is one of the
key components for wide and insightful visual data exploration [20].
In fact, interactivity enables users to seek various aspects of their data
and gain additional insights. This importance of interaction in data
visualizations raises a question — How can we enable users to perform
tasks that require interactivity using static visualizations?

Augmented Reality (AR) has been a persistently used technology
for bringing life into static content [6, 10]. By looking through a hand-
held or head-mounted device, users are able to view virtual content
superimposed onto static scenes. Users can interact with the virtual
content using various channels such as gesture or/and voice [11]. This
combination of visualization and interaction creates a unique capability
to animate static material and has made AR a popular choice for enter-
tainment and education [12, 22]. Studies have repeatedly reported that
using AR enhances engagement and motivation of users compared to
using non-AR material [9].

 
 
 
 
 
 
In this paper, we present how augmented reality can be used to bring
interactivity to static visualizations. In particular, we present a new
solution built on the Microsoft Hololens, enabling users to perform
interactive tasks such as ﬁltering, highlighting, linking, and hovering
on static visualizations. Users can use gestures or voice commands to
interact with visualizations and observe changes in real-time on the AR
device. Through interactivity, our solution enables users to customize
views of static visualizations and answer personal data-driven questions.

2 RELATED WORK

Augmented reality allows users to have a seamless experience between
their world and content others have created. One interesting aspect of
AR is that it can breath life into static content. In the AR domain, there
have been numerous projects that animate static objects by adding inter-
activity to the once-inanimate experience. For example, Billinghurst’s
MagicBook was a novel AR interface that allowed static components
of a physical book to be interactive to the reader [6]. This lead to
various follow up studies that experimented with adding interactivity
to physical books [10]. Researchers investigated how AR solutions
affect user experience while performing tasks. For example, in one
study, researchers observed that students using AR-enhanced books
were better motivated and more engaged in the material [5] compared
to using other methods.

The power of animating static content has also been proven by the
widespread usage of the commercial product Layar [2]. Layar provides
AR experiences that enhance printed material to be digitally interactive
on everyday smartphones. The resulting artifacts showed an 87% click-
through rate which is overwhelming compared to single digit click-
through rates of other advertisement methods. This shows that when
properly designed, using AR for the purpose of adding interactivity to
static content has a potential of increasing user engagement.

In the information visualization domain, there have been other at-
tempts to merge AR with information visualization. In the sense that
AR inherently couples virtual content with the user’s physical surround-
ings, White deﬁnes situated visualization as visualizations that are
coupled with the context of physical surroundings [23]. Our approach
is somewhat different from White’s view of situated visualization as
we ignore the physical context in which the visualization is placed in
and focus on extending the capabilities of the visualization regardless
of the context.

In the interaction space, Cordeil et al. coined the concept of spatio-
data coordination which deﬁnes the mapping between the physical
interaction space and the virtual visualization space [8]. When struc-
turing our approach through this concept, we see that bystanders of
visualizations cannot interact with the visualizations, having no coor-
dination between their interaction space and the visualization space.
By introducing additional elements in the visualization space that have
spatio-data coordination with personal interaction spaces, we allow
users to directly interact with the visualization and view personalized
content in their own display space.

Our work is also inspired by the information-rich virtual environ-
ment concept introduced by Bowman et al. [7]. The concept looks into
interactions between virtual environments and abstract information. If
we consider a synthetic static visualization as a virtual representation
of data, our solution provides abstract information about that environ-
ment. Users can perform interaction tasks that point from the virtual
environment to abstract information; that is, users can retrieve details
associated with a synthetic visualization.

Other studies have investigated view management systems that adap-
tively annotate scenes with virtual content [4, 21]. These systems adjust
annotations according to various factors such as the user’s viewpoint
or the amount of crowded content on the screen. The results of these
studies have direct implications to our system as we rely on annotations
on static visualizations.

3 MOTIVATION

In this section, we discuss a couple of scenarios that motivate using AR
techniques to enable people to gain more insights through interacting
with static visualizations.

Fig. 2. A live screenshot taken from the viewpoint of the user while using
the VisAR prototype. Gazing at a data point reveals detailed information
on it.

3.1 Data Exploration of Audience During Presentations

Using visualizations is a common method to convey information to an
audience during presentations and meetings. In a presentation setting
where a projector is used to show digital slides, information delivered to
the audience is limited to content presented in the slides and delivered
by a presenter verbally. That is, the audience cannot directly interact
with the visualization presented on the slides to explore different as-
pects of the underlying data. This leaves limited room for answering
questions that individuals in the audience might have. Personal AR
devices would enable users to individually interact with static visualiza-
tions shown on digital slides to conduct open-ended exploration. Fig. 1
shows an example of VisAR being used in a presentation setting.

3.2 Credential-based Data Exploration

In a group where individuals have different levels of credentials, AR
can provide a way of differentiating visualizations and exploration
capabilities among the users. According to an individual’s security
clearance level, they can be allowed different sets of interactions or be
provided with distinct sets of data. For instance, when an entry-level
employee and a senior employee are both attending a presentation on
the company’s annual reports and the screen is currently showing a
chart of this year’s proﬁts, only the senior employee can be allowed to
explore through sensitive parts of the data on her personal AR device
by interacting with annotations that lead to other related information.

4 METHOD

The analytical workﬂow starts with a preparation stage which involves
accessing the AR application that contains the underlying dataset that
was used for creating the visualization. The user then observes a typical
static visualization (visualization target) shown on a poster or digital
screen through an AR device. The device uses it’s camera and an
image-based tracking system such as PTC’s Vuforia [15] to track the
visualization as a target. Depending on the number of trackable features,
the target can either be a ﬁducial marker that is carefully positioned on
the visualization or simply the entire static visualization itself. Once the
AR device recognizes the target, it superimposes virtual content onto
the static visualization. For example, in the case of a static scatterplot
visualization, the system renders a new set of virtual data points and
overlays it onto the static version of the visualization. Users are then
able to see and interact with the virtual content that is superimposed on
the static visualization. User interaction with the virtual content can be
through voice or/and gestures. For example, users can ﬁlter a speciﬁc
set of data points by saying “Filter out cars above $40,000”. Similarly
users can see detailed information about a speciﬁc data point by simply
looking directly at the data point.

5 VISAR PROTOTYPE
To indicate the feasibility of our idea, we implemented a prototype
system on the Microsoft Hololens [14] which has accurate tracking and
rendering capabilities and an RGB camera that can be used for target
recognition. No additional adjustment to the device was required. For
the computer vision solution on tracking target images we use PTC’s
Vuforia which has support for the Hololens platform. We prepared
an example static visualization and deﬁned the entire visualization as
the target visualization because it had enough features to qualify as an
image target. The visualization system was built using the Unity Editor
for Hololens while the example static visualization was built with the
D3 library.

5.1 Techniques and Interactions Supported by VisAR
The current version of VisAR supports two types of visualization tech-
niques (bar chart and scatterplot) and four main interactions including
details on demand, highlighting, ﬁltering, and linking views.

• Details on Demand: VisAR supports one of the most commonly
provided interactions in visualization tools, details on demand, to
let the user inspect data points on-the-ﬂy. When users “point” at a
data point using their head, the system visualizes a pop-up box that
contains detailed information on the speciﬁc data point(Fig. 2). Users
are able to turn this feature on or off by means of voice commands.
Because our device uses head gaze as the pointing interface, we
consider head gaze as the direction that the user is looking at. The
point where the user’s head gaze contacts the visualization is the
location of the gaze pointer, analogous to a mouse pointer. On
devices that support eye gaze tracking, it would be preferable to use
eye gaze instead of head gaze.

• Highlighting: Another interaction that VisAR supports is highlight-
ing. Highlighting enables users to ﬁnd and emphasize the relevant
points of interest. Users can highlight data points of interest either
by tapping on the provided buttons or using voice commands. As
a result, the relevant points are emphasized through highlighting of
the points. To do this, we render another layer of data points with
slightly increased brightness onto the exact points(Fig. 3). Because
the rendered data points have a different contrast and brightness than
the original data points, it works as a highlighting method.

• Filtering: Similar to highlighting, VisAR also supports a classic
ﬁltering method through diminished reality [13]. This is achieved
by overlaying virtual “patches” to cover data points that should be
hidden. The patches are colored with the default background color,
shaped according to the data point shape, and sized slightly larger
than the data points. The result is a visualization that only shows the
data point that the user is interested in. For instance, if a user applies
a ﬁlter to see certain data points, irrelevant data points are “removed”
from the static visualization to let the user focus on the relevant ones.

• Linking Views: The fourth interaction VisAR supports is linking
views. Upon ﬁltering or highlighting a subset of data points, VisAR
reveals a bar chart visualization to the right of the main scatterplot.
The bar chart provides additional information about the ﬁltered or
highlighted data points by visualizing other data attributes of the
dataset. The same set of interactions are provided for both visual-
izations and the visualizations are linked, so interacting with one
visualization updates the other visualization.

5.2 The VisAR Interface
VisAR receives user input through two different modalities: speech and
gesture. Below, we describe how VisAR receives user input using each
of these modalities.

• Voice Input: Among the two channels of input we support, voice
input is the most direct and efﬁcient way to interact with VisAR. The
head-mounted platform used in VisAR provides a powerful natural
language user interface, allowing users to perform voice commands.

Fig. 3. A live screenshot taken from the viewpoint of the user while using
the VisAR prototype. A subset of data points are highlighted (blue) and a
linked view is displayed.

Users can ﬁlter or highlight desired data points with a single voice
command such as “Filter out countries in Asia.” Voice commands
also allow custom features that are not supported by gesture such as
“Filter out countries with GDP larger than $10,000.”

• Gesture Input: The other input channel supported by VisAR is
gesture. This is a useful input mode in extremely quiet or noisy envi-
ronments where voice input is impractical. Users can perform select
or exit commands through gestures identical to what the Microsoft
Hololens provides by default. For selecting, the gesture input needs
to be accompanied by a gaze pointer. Using the gaze pointer, the
user hovers over a static graphical component and gestures a select
command to interact with it. For example, if a user wants to highlight
a data point, she would look at the data point and gesture a click.

Upon receiving an input from the user, VisAR provides feedback
about what action has been done and what has been accomplished.
VisAR provides feedback through visual and auditory channels.

• Auditory Feedback: Each time that the system receives user input,
audio feedback will be provided through the head-mounted device.
A simple system chime is used to indicate the success of an input. As
the Microsoft Hololens has personal speakers attached right next to
the user’s ears, any auditory feedback is only audible to the individual
user without the need of additional headphones.

• Visual Feedback: After each user interaction with the visualization,
VisAR updates the represented view accordingly. For example, after
ﬁltering a speciﬁc set of data points, VisAR immediately overlays
virtual patches to cover data points that are supposed to be ﬁltered.

5.3 Usage Scenario
Assume Bob plans to attend a workshop on the future of interactions in
data visualizations. Workshop organizers have provided the presenta-
tion slides and the accompanying AR application on the workshop’s
website. Before the workshop, Bob navigates to the workshop’s web-
site on his AR device and downloads the AR application that would
augment the presentation slides. During one presentation, the presenter
brings up a slide that has a scatterplot on it. Each point in the scatterplot
represents the ratio of the number of information visualization faculty
members to the number of students in a university. Bob notices that
one university has an unusual faculty-to-student ratio compared to the
ratio of other schools. He is curious to know what might be the reason
for the outlier. Bob points his head to the data point that interests
him. The data point is highlighted and a small text-box appears with
simple information on the school. The details are not enough for Bob to
understand the reason so he gestures a “click” motion which brings up
a separate bar chart on the side of the original scatterplot. By quickly

going through the pop-up visualization, he sees that the school has been
investing more on hiring information visualization faculty members
compared to other schools in the country. Bob assumes that this might
be the reason. Now that he feels that he has a better understanding of
the outlier, he gestures an “exit” motion and continues listening to the
presentation.

6 DISCUSSION

We developed VisAR to show the feasibility of using AR devices to
bring interactivity to static data visualizations. The current version
of VisAR supports two types of visualization techniques (bar chart
and scatterplot) and four interaction techniques (details on demand,
highlighting, ﬁltering and linking views). We view the current version
of VisAR as the early step towards exploring the applications of aug-
mented reality in data visualization. However, generalizing the usage
of AR devices for interacting with static visualizations requires support
of more sophisticated analytic operations and visualization techniques.
For example, how can users perform brushing and linking or zooming
on static visualizations using AR technologies? Multiple avenues for
future work lie in improving the VisAR interface. We envision expand-
ing VisAR to include other visualization techniques (e.g., linecharts)
and interaction tasks (e.g., zooming, panning).

Previous work in the AR community indicates that using AR im-
proves user engagement compared to using non-AR material [5]. How-
ever, it is not clear to the visualization community how usage of AR
solutions affect user experience during the visual data exploration pro-
cess. An important avenue for continued research is conducting an
in-depth study utilizing both qualitative and quantitative techniques to
measure the impact of AR solutions in data visualization compared to
non-AR solutions, using various usability (e.g., time and error) and
user experience (e.g., engagement [16, 17]) metrics. We hypothesize
that using AR solutions increases user engagement in working with
data visualizations, but this remains to be formally studied.

Many AR solutions rely on two interaction modalities: speech and
gesture. There are advantages in using speech and gesture in visual
data exploration since they enable users to express their questions and
commands more easily [19]. However, these interaction modalities
also bring challenges such as lack of discoverability. How does a user
recognize what the possible interactions are? How does a user know
what the exact gesture or voice command is for performing a speciﬁc
interaction? One interesting research avenue is to investigate methods
that make possible interactions more discoverable in such systems.

In the current version of VisAR, we are using a high-end AR device
(Microsoft Hololens) which currently might not be a feasible option to
use for most people. However, with recent advances in smartphone tech-
nology, currently available off-the-shelf smartphones are also powerful
enough to be used as a personal AR device. By using a smartphone as
a hand-held AR device, the current implementation can be ported to
have identical capabilities. We recommend using cross-platform AR
systems such as Argon [1] to allow users to be able to access the same
experience regardless of the underlying device or operating system.

7 CONCLUSION

While the beneﬁt of 3D visualizations in immersive AR settings are still
yet to be further discussed [3], in this paper we examined a different
aspect of AR that takes advantage of 2D visualizations. As AR allows
us to add visual interactive components to static scenes, we utilize this
capability to add interactivity to static visualizations and enable users
to independently address personal data-driven questions. Through the
prototype of our solution, we demonstrate that the idea is feasible to be
implemented on currently available AR devices. Although further study
is needed to measure the advantages of the system, with the abundance
of personal AR devices such as smartphones, we believe our solution is
not only powerful but also practical.

REFERENCES

[1] argon.js. https://www.argonjs.io/, 2017.
[2] Layar. https://www.layar.com/, 2017.

[3] D. Belcher, M. Billinghurst, S. Hayes, and R. Stiles. Using augmented
reality for visualizing complex graphs in three dimensions. In Proceedings
of the 2Nd IEEE/ACM International Symposium on Mixed and Augmented
Reality, ISMAR ’03, pp. 84–. IEEE Computer Society, Washington, DC,
USA, 2003.

[4] B. Bell, S. Feiner, and T. H¨ollerer. View management for virtual and
augmented reality. In Proceedings of the 14th annual ACM symposium on
User interface software and technology, pp. 101–110. ACM, 2001.
[5] M. Billinghurst and A. Duenser. Augmented reality in the classroom.

Computer, 45(7):56–63, 2012.

[6] M. Billinghurst, H. Kato, and I. Poupyrev. The magicbook-moving seam-
lessly between reality and virtuality. IEEE Computer Graphics and appli-
cations, 21(3):6–8, 2001.

[7] D. A. Bowman, C. North, J. Chen, N. F. Polys, P. S. Pyla, and U. Yilmaz.
Information-rich virtual environments: theory, tools, and research agenda.
In Proceedings of the ACM symposium on Virtual reality software and
technology, pp. 81–90. ACM, 2003.

[8] M. Cordeil, B. Bach, Y. Li, E. Wilson, and T. Dwyer. A design space
for spatio-data coordination: Tangible interaction devices for immersive
information visualisation. In Proceedings of IEEE Paciﬁc Visualization
Symposium (Paciﬁc Vis), 2017.

[9] M. Dunleavy, C. Dede, and R. Mitchell. Affordances and limitations of
immersive participatory augmented reality simulations for teaching and
learning. Journal of science Education and Technology, 18(1):7–22, 2009.
[10] R. Grasset, A. Duenser, H. Seichter, and M. Billinghurst. The mixed
reality book: a new multimedia reading experience. In CHI’07 extended
abstracts on Human factors in computing systems, pp. 1953–1958. ACM,
2007.

[11] S. Irawati, S. Green, M. Billinghurst, A. Duenser, and H. Ko. An evaluation
of an augmented reality multimodal interface using speech and paddle
gestures. Advances in Artiﬁcial Reality and Tele-Existence, pp. 272–283,
2006.

[12] A. M. Kamarainen, S. Metcalf, T. Grotzer, A. Browne, D. Mazzuca,
M. S. Tutwiler, and C. Dede. Ecomobile: Integrating augmented reality
and probeware with environmental education ﬁeld trips. Computers &
Education, 68:545–556, 2013.

[13] S. Mann and J. Fung. Videoorbits on eye tap devices for deliberately
diminished reality or altering the visual perception of rigid planar patches
of a real world scene. EYE, 3:P3, 2001.
HoloLens.

https://www.microsoft.com/en-us/

[14] Microsoft.

hololens, 2017.

[15] PTC Inc. Vuforia. https://www.vuforia.com/, 2017.
[16] B. Saket, A. Endert, and J. Stasko. Beyond usability and performance: A
review of user experience-focused evaluations in visualization. Beyond
Time And Errors: Novel Evaluation Methods For Visualization (BELIV),
2016.

[17] B. Saket, C. Scheidegger, and S. Kobourov. Comparing node-link and
node-link-group visualizations from an enjoyment perspective. In Com-
puter Graphics Forum, vol. 35, pp. 41–50. Wiley Online Library, 2016.

[18] B. Saket, P. Simonetto, S. Kobourov, and K. Borner. Node, node-link, and
node-link-group diagrams: An evaluation. Visualization and Computer
Graphics, IEEE Transactions on, 20(12):2231–2240, 2014.

[19] A. Srinivasan and J. Stasko. Natural language interfaces for data anal-
ysis with visualization: Considering what has and could be asked. In
Proceedings of EuroVis ’17, pp. 55–59, 2017.

[20] J. Stasko. Value-driven evaluation of visualizations. In Proceedings of
the Fifth Workshop on Beyond Time and Errors Novel Evaluation Methods
for Visualization - BELIV ’14, pp. 46–53, 2014. doi: 10.1145/2669557.
2669579

[21] M. Tatzgern, D. Kalkofen, R. Grasset, and D. Schmalstieg. Hedgehog
labeling: View management techniques for external labels in 3d space. In
2014 IEEE Virtual Reality (VR), pp. 27–32, March 2014. doi: 10.1109/VR
.2014.6802046

[22] D. Wagner, T. Pintaric, and D. Schmalstieg. The invisible train: a collab-
orative handheld augmented reality demonstrator. In ACM SIGGRAPH
2004 Emerging technologies, p. 12. ACM, 2004.

[23] S. M. White. Interaction and presentation techniques for situated visual-

ization. Columbia University, 2009.

[24] J. S. Yi, Y. A. Kang, and J. Stasko. Toward a deeper understanding of
the role of interaction in information visualization. IEEE Transactions on
Visualization and Computer Graphics, 13(6):1224–1231, Nov 2007. doi:
10.1109/TVCG.2007.70515

