2
2
0
2

y
a
M
1
1

]
E
M

.
t
a
t
s
[

4
v
2
9
4
4
0
.
0
1
0
2
:
v
i
X
r
a

Autoregressive Networks

Department of Applied Mathematics, Hong Kong Polytechnic University

Binyan Jiang

Hong Kong, by.jiang@polyu.edu.hk

Jialiang Li

Department of Statistics and Applied Probability, National University of Singapore

Singapore, stalj@nus.edu.sg

Qiwei Yao

Department of Statistics, London School of Economics, London, WC2A 2AE

United Kingdom, q.yao@lse.ac.uk

12 May 2022

Abstract

We propose a ﬁrst-order autoregressive (i.e. AR(1)) model for dynamic network processes

in which edges change over time while nodes remain unchanged. The model depicts the dy-

namic changes explicitly. It also facilitates simple and eﬃcient statistical inference methods

including a permutation test for diagnostic checking for the ﬁtted network models. The pro-

posed model can be applied to the network processes with various underlying structures but

with independent edges. As an illustration, an AR(1) stochastic block model has been inves-

tigated in depth, which characterizes the latent communities by the transition probabilities

over time. This leads to a new and more eﬀective spectral clustering algorithm for identifying

the latent communities. We have derived a ﬁnite sample condition under which the perfect

recovery of the community structure can be achieved by the newly deﬁned spectral cluster-

ing algorithm. Furthermore the inference for a change point is incorporated into the AR(1)

stochastic block model to cater for possible structure changes. We have derived the explicit

error rates for the maximum likelihood estimator of the change-point. Application with three

real data sets illustrates both relevance and usefulness of the proposed AR(1) models and the

associate inference methods.

Keywords: AR(1) networks; Change point; Dynamic stochastic block model; Hamming distance;

Maximum likelihood estimation; Spectral clustering algorithm; Yule-Walker equation.

1

 
 
 
 
 
 
1

Introduction

Understanding and being able to model the network changes over time are of immense importance

for, e.g., monitoring anomalies in internet traﬃc networks, predicting demand and setting prices

in electricity supply networks, managing natural resources in environmental readings in sensor

networks, and understanding how news and opinion propagates in online social networks. In spite

of the existence of a large body of literature on dynamic networks, the development of the foun-

dation for dynamic network models is still in its infancy (Kolaczyk, 2017). As for dealing with

dynamic changes of networks, early attempts are based on the evolution analysis of network snap-

shots over time (Aggarwal and Subbian, 2014; Donnat and Holmes, 2018). Although this reﬂects

the fact that most networks change slowly over time, it provides little insight on the dynamics

underlying the changes and is almost powerless for future prediction. The popular approaches

for modelling dynamic changes include, among others, Markov process models (Snijders, 2005;

Ludkin et al., 2018), the exponential random graph models (Hanneke et al., 2010; Krivitsky and

Handcock, 2014), and latent process based models (Friel et al., 2016; Durante et al., 2016; Ma-

tias and Miele, 2017). The estimation for those models is compute-intensive, relying on various

MCMC or EM algorithms.

In this paper we propose a simple ﬁrst-order autoregressive (i.e. AR(1)) model for dynamic

network processes of which the edges changes over time while the nodes are unchanged. Though

our setting is a special case of the Markov chain network models (see Yudovina et al. (2015),

and also Snijders (2005) and Ludkin et al. (2018)), a simple AR(1) structure makes it possible

to measure explicitly the underlying dynamic properties such as autocorrelation coeﬃcients, and

the Hamming distance. It facilitates the maximum likelihood estimation (MLE) in a simple and

direct manner with uniform error rates. Furthermore diagnostic checking for the ﬁtted network

models can be performed in terms of an easy-to-use permutation test, which is impossible under

a merely Markovian structure.

Our setting can be applied to any network processes with various underlying structures as long

as the edges are independent with each other, which we illustrate through an AR(1) stochastic

block model. The latent communities in our setting are characterized by the transition probabil-

ities over time, instead of the (static) connection probabilities – the approach often adopted from

static stochastic block models; see Pensky (2019) and the references therein. This new structure

also paves the way for a new spectral clustering algorithm which identiﬁes the latent communities

more eﬀectively – a phenomenon corroborated by both the asymptotic theory and the simulation

results. To cater for possible structure changes of underlying processes, we incorporate a change

point detection mechanism in the AR(1) stochastic block modeling. Again the change point is es-

2

timated by the maximum likelihood method. The AR(1) continuous time stochastic block model

of Ludkin et al. (2018) is based on a sophisticated construction.

Its estimation is based on a

reversible jump MCMC, though a discrete-time version of their model admits the same Markov

Chain representation (2.3) below.

Theoretical developments for dynamic stochastic block models in the literature were typically

based on the assumption that networks observed at diﬀerent times are independent; see Pensky

(2019); Bhattacharjee et al. (2020) and references therein. The autoregressive structure considered

in this paper brings the extra complexity due to serial dependence. By establishing the α-mixing

property with exponentially decaying coeﬃcients for the AR(1) network processes, we are able to

show that the proposed spectral clustering algorithm leads to a consistent recovery of the latent

community structure. On the other hand, an extra challenge in detecting a change point in the

dynamic stochastic block network process is that the estimation for latent community structures

before and after a possible change point is typically not consistent during the search for the

change point. To overcome this obstacle, we introduce a truncation technique which breaks the

searching interval into two parts such that the error bounds for the estimated change point can

be established.

The proposed methods in this paper only apply to the dynamic networks observed on discrete

times. Even so the relevant literature is large, across mathematics, computer science, engineer,

statistics, biology, genetics and social sciences. We can only list a small selection of more statistics-

oriented papers in addition to the aforementioned references. Fu et al. (2009) proposed a state

space mixed membership stochastic block model (with a logistic normal prior). Crane et al. (2016)

studied the limit properties of Markovian, exchangeable and c`adl`ag (i.e. every edge remains

in each state which it visits for a positive amount of time) dynamic network. Pensky (2019)

studied the theoretical properties (such as the minimax lower bounds for the risk) of a dynamic

stochastic block model, assuming ‘smooth’ connectivity probabilities. The literature on change

point detection in dynamic networks includes Yang et al. (2011); Wang et al. (2018); Wilson et al.

(2019); Zhao et al. (2019); Bhattacharjee et al. (2020); Zhu et al. (2020a). Knight et al. (2016);

Zhu et al. (2017, 2019); Chen et al. (2020); Zhu et al. (2020b) adopted autoregressive models for

modelling continuous responses observed from the nodes of a network process. Kang et al. (2017)

used dynamic network as a tool to model non-stationary vector autoregressive processes. For the

development on continuous-time dynamic networks, we refer readers to Snijders (2005), Matias

et al. (2018), Ludkin et al. (2018) and Corneli et al. (2018).

The new contributions of this paper include: (i) We propose a new and simple AR(1) model

for edge dymanics (see (2.1) below), which facilitates the easy-to-use inference methods including

a permutation test for model diagnostic checking. (ii) The AR(1) setting can be applied to various

3

network processes with speciﬁc underlying structures such as dynamic stochastic block models, as

illustrated in Section 3 below, and also dynamic dot product model, dynamic graphon model, etc.

(iii) The AR(1) structure also makes it possible to develop the theoretical guarantees for the serial

dependent network processes. For example, based on a concentration inequality, we have derived

a ﬁnite sample condition, under which the perfect recovery of the community structure can be

achieved by the newly deﬁned spectral clustering algorithm (Theorems 1 and 2 in Section 3.2.2

below). Furthermore, we have shown that the MLE for the change-point in the AR(1) stochastic

block process is consistent with explicit error rates (Theorem 5 in Section 3.3 below). Those

results are based on some rigorous technical development for the dependent network processes.

Note that both Pensky (2019) and Bhattacharjee et al. (2020) assume that networks observed at

diﬀerent times are independent with each other in their asymptotic theories for dynamic stochastic

block models. Illustration with the three real network data sets indicates convincingly that the

proposed AR(1) model and the associated inference methods are practically relevant and fruitful.

The rest of the paper is organized as follows. A general framework of AR(1) network processes,

the probabilistic properties, and the MLE are presented in Section 2. It also contains a new and

easy-to-use permutation test for the diagnostic checking for the ﬁtted network models. Section

3 deals with AR(1) stochastic block models. The asymptotic theory is developed for the new

spectral clustering algorithm based on the transition probabilities. Further extension of both the

inference method and the asymptotic theory to the setting with a change point is established.

Simulation results are reported in Section 4, and the illustration with three real dynamic network

data sets is presented in Section 5. All technical proofs are relegated to the Appendix.

2 Autoregressive network models

2.1 AR(1) models

Let {Xt, t = 0, 1, 2, · · · } be a dynamic network process deﬁned on the p ﬁxed nodes, denoted by
{1, · · · , p}, where Xt ≡ (X t
networks are Erd¨os-Renyi in the sense that X t

i,j) denotes the p×p adjacency matrix at time t. We also assume that all
i,j, (i, j) ∈ J , are independent and take values either
1 or 0, where J = {(i, j) : 1 ≤ i ≤ j ≤ p} for undirected networks, J = {(i, j) : 1 ≤ i < j ≤ p}

for undirected networks without selﬂoops, J = {(i, j) : 1 ≤ i, j ≤ p} for directed networks, and

J = {(i, j) : 1 ≤ i (cid:54)= j ≤ p} for directed networks without selﬂoops. Note that an edge from node

i to j is indicated by Xi,j = 1, and no edge is denoted by Xi,j = 0. For undirected networks,
X t

i,j = X t

j,i.

4

Deﬁnition 2.1. An AR(1) network process is deﬁned as

X t

i,j = X t−1

i,j I(εt

i,j = 0) + I(εt

i,j = 1),

t ≥ 1,

(2.1)

where I(·) denotes the indicator function, the innovations εt

i,j, (i, j) ∈ J , are independent, and

P (εt

i,j = 1) = αt

i,j, P (εt

i,j = −1) = βt

i,j, P (εt

i,j = 0) = 1 − αt

i,j − βt

i,j.

(2.2)

In the above expression, αt

i,j, βt

i,j are non-negative constants, and αt

i,j + βt

i,j ≤ 1.

Equation (2.1) is an analogue of the noisy network model of Chang et al. (2020c). The

innovation (or noise) εt

i,j is ‘added’ via the two indicator functions to ensure that X t

i,j is still

binary. Obviously, {Xt, t = 0, 1, 2, · · · } is a Markov chain, and

P (X t

i,j = 1|X t−1

i,j = 0) = αt

i,j, P (X t

i,j = 0|X t−1

i,j = 1) = βt

i,j,

or collectively,

P (Xt|Xt−1, · · · , X0) = P (Xt|Xt−1) =

(cid:89)

P (X t

i,j|X t−1
i,j )

(2.3)

(2.4)

(αt

i,j)X t

i,j (1−X t−1

i,j )(1 − αt

i,j)(1−X t

i,j )(βt

i,j)(1−X t

i,j )X t−1

i,j (1 − βt

i,j)X t

i,j X t−1
i,j .

(i,j)∈J
i,j )(1−X t−1

(cid:89)

=

(i,j)∈J

It is clear that the smaller αt

i,j is, the more likely the no-edge status at time t − 1 (i.e. X t−1

will be retained at time t (i.e. X t
t − 1 (i.e. X t−1
(such as social networks), we expect αt

i,j = 1) will be retained at time t (i.e. X t

i,j = 0); and the smaller βt

i,j and βt

i,j to be small.

i,j = 0)
i,j is, the more likely an edge at time
i,j = 1). For most slowly changing networks

It is natural to model dynamic networks by a Markov chain. See, e.g. Hanneke et al. (2010);

Krivitsky and Handcock (2014); Yudovina et al. (2015); Friel et al. (2016); Crane et al. (2016);

Matias and Miele (2017); Rastelli et al. (2017); Ludkin et al. (2018). For example, the Markovian

transition probabilities under a discrete version of the stationary independent arcs network model

of Snijders (2005, Section 5) can be written equivalently as (2.3) with αt

i,j ≡ β.
In this paper we build the Markovian structure based on the explicit AR(1) model (2.1), which

i,j ≡ α and βt

enables us to study the theoretical properties of the network processes, and to develop simple and

eﬃcient inference methods with appropriate theoretical guarantee.

2.2 Stationarity

Note that {Xt} is a homogeneous Markov chain if

αt

i,j ≡ αi,j

and βt

i,j ≡ βi,j

for all t ≥ 1 and (i, j) ∈ J .

(2.5)

5

Specify the distribution of the initial network X0 = (X 0

i,j) as follows:

P (X 0

i,j = 1) = πi,j = 1 − P (X 0

i,j = 0),

(2.6)

where πi,j ∈ (0, 1), (i, j) ∈ J , are constants.

Proposition 1. Let the homogeneity condition (2.5) hold with αi,j + βi,j ∈ (0, 1], and

πi,j = αi,j/(αi,j + βi,j), (i, j) ∈ J .

(2.7)

Then {Xt, t = 0, 1, 2, · · · } is a strictly stationary process. Furthermore for any (i, j), ((cid:96), m) ∈ J

and t, s ≥ 0,

E(X t

i,j) =

αi,j
αi,j + βi,j

,

Var(X t

i,j) =

αi,jβi,j
(αi,j + βi,j)2 ,

ρi,j(|t − s|) ≡ Corr(X t

i,j, X s

(cid:96)m) =

(cid:110) (1 − αi,j − βi,j)|t−s|

if (i, j) = ((cid:96), m),

0

otherwise.

(2.8)

(2.9)

The Hamming distance counts the number of diﬀerent edges in the two networks, and is a

measure the closeness of two networks (Donnat and Holmes, 2018).

Deﬁnition 2.2. For any two matrices A = (Ai,j) and B = (Bi,j) of the same size, the Hamming
distance is deﬁned as DH (A, B) = (cid:80)

i,j I(Ai,j (cid:54)= Bi,j).

Proposition 2. Let {Xt, t = 0, 1, · · · } be a stationary network process satisfying the condition of

Proposition 1. Let dH (|t − s|) = E{DH (Xt, Xs)} for any t, s ≥ 0. Then dH (0) = 0, and it holds

for any k ≥ 1 that

dH (k) = dH (k − 1) +

(cid:88)

(i,j)∈J

2αi,jβi,j
αi,j + βi,j

(1 − αi,j − βi,j)k−1

(cid:88)

=

(i,j)∈J

2αi,jβi,j
(αi,j + βi,j)2 {1 − (1 − αi,j − βi,j)k}.

(2.10)

(2.11)

Proposition 2 indicates that the expected Hamming distance dH (d) = E{DH (Xt, Xt+k)}

increases strictly, as k increases, initially from dH (1) = (cid:80) 2αi,j βi,j
αi,j +βi,j
(cid:80) 2αi,j βi,j

towards the limit dH (∞) =
(αi,j +βi,j )2 which is also the expected Hamming distance of the two independent networks

sharing the same marginal distribution of Xt.

Proposition 3 below shows that {Xt, t = 0, 1, · · · } is α-mixing with exponentially decaying

coeﬃcients. Note that the conventional mixing results for ARMA processes do not apply here,

as they typically require that the innovation distribution is continuous; see, e.g., Section 2.6.1 of

6

Fan and Yao (2003). Let F b

a be the σ-algebra generated by {X k

i,j, a ≤ k ≤ b}. The α-mixing

coeﬃcient of process {X t

i,j, t = 0, 1, · · · } is deﬁned as

αi,j(τ ) = sup
k∈N

sup
0 ,B∈F ∞

A∈F k

|P (A ∩ B) − P (A)P (B)|.

k+τ ,
Proposition 3. Let condition (2.5) hold, αi,j, βi,j > 0, and αi,j + βi,j ≤ 1. Then αi,j(τ ) ≤
ρi,j(τ ) = (1 − αi,j − βi,j)τ for any τ ≥ 1.

2.3 Estimation

To simplify the notation, we assume the availability of the observations X0, X1, · · · , Xn from a

stationary network process which satisﬁes the condition of Proposition 1. Without imposing any

further structure on the model, the parameters (αi,j, βi,j), for diﬀerent (i, j), can be estimated

separately. Condition on X0, the maximum likelihood estimators are

(cid:98)αi,j =

(cid:80)n

t=1 X t
(cid:80)n

i,j(1 − X t−1
i,j )
t=1(1 − X t−1
i,j )

,

(cid:98)βi,j =

(cid:80)n

i,j)X t−1
i,j

t=1(1 − X t
(cid:80)n

t=1 X t−1
i,j

.

(2.12)

See (2.4). For deﬁniteness we shall set 0/0 = 1. To state the asymptotic properties, we list some

regularity conditions ﬁrst.

C1. There exists a constant l > 0 such that αi,j, βi,j ≥ l and αi,j + βi,j ≤ 1 for all (i, j) ∈ J .

C2. n, p → ∞, and (log n)(log log n)

(cid:113) log p

n → 0.

Condition C1 deﬁnes the parameter space, and condition C2 indicates that the number of nodes
is allowed to diverge in a smaller order than exp (cid:8)

(cid:9).

n
(log n)2(log log n)2

Proposition 4. Let conditions (2.5), C1 and C2 hold. Then it holds that

max
(i,j)∈J

|(cid:98)αi,j − αi,j| = Op

(cid:32)(cid:114)

(cid:33)

log p
n

and max
(i,j)∈J

| (cid:98)βi,j − βi,j| = Op

(cid:32)(cid:114)

(cid:33)

.

log p
n

Proposition 4 provides a uniform convergence rate for the MLEs in (2.12). To state the

joint asymptotic normality. Let J1 = {(i1, j1), . . . , (im1, jm1)}, J2 = {(k1, (cid:96)1), . . . , (km2, (cid:96)m2)}
be two arbitrary subsets of J with m1, m2 ≥ 1 ﬁxed. Denote ΘJ1,J2 = (αi1,j1, . . . , αim1 ,jm1
,
)(cid:62), and correspondingly denote the MLEs as (cid:98)ΘJ1,J2 = ((cid:98)αi1,j1, . . . , (cid:98)αim1 ,jm1
βk1,(cid:96)1, . . . , βkm2 ,(cid:96)m2
)(cid:62).
(cid:98)βk1,(cid:96)1, . . . , (cid:98)βkm2 ,(cid:96)m2

,

Proposition 5. Let conditions (2.5), C1 and C2 hold. Then

√

n( (cid:98)ΘJ1,J2 −ΘJ1,J2) → N (0, ΣJ1,J2),

where ΣJ1,J2 = diag(σ11, . . . , σm1+m2,m1+m2) is a diagonal matrix with

σrr =

σrr =

αir,jr (1 − αir,jr )(αir,jr + βir,jr )
βir,jr
βkr,(cid:96)r (1 − βkr,(cid:96)r )(αkr,(cid:96)r + βkr,(cid:96)r )
αkr,(cid:96)r

, 1 ≤ r ≤ m1,

, m1 + 1 ≤ r ≤ m1 + m2.

7

2.4 Model diagnostic check

Based on estimators in (2.12), we deﬁne ‘residual’ (cid:98)εt
data, as the estimated value of E(εt
i,j ), i.e.

i,j, X t−1

i,j|X t

i,j, resulted from ﬁtting model (2.1) to the

i,j = (cid:98)αi,j
(cid:98)εt
1 − (cid:98)βi,j

I(X t

i,j = 1, X t−1

i,j = 1) −

I(X t

i,j = 0, X t−1

i,j = 0)

(cid:98)βi,j
1 − (cid:98)αi,j
i,j = 0, X t−1

+I(X t

i,j = 1, X t−1

i,j = 0) − I(X t

i,j = 1),

(i, j) ∈ J , t = 1, · · · , n.

One way to check the adequacy of the model is to test for the independence of (cid:98)Et ≡ ((cid:98)εt
i,j) for
t = 1, · · · , n. Since (cid:98)εt
i,j, t = 1, · · · , n, only take 4 diﬀerent values for each (i, j) ∈ J , we adopt
the two-way, or three-way contingency table to test the independence of (cid:98)Et and (cid:98)Et−1, or (cid:98)Et, (cid:98)Et−1
and (cid:98)Et−2. For example the test statistic for the two-way contingency table is

T =

1
n |J |

(cid:88)

4
(cid:88)

{ni,j(k, (cid:96)) − ni,j(k, ·)ni,j(·, (cid:96))/(n − 1)}2(cid:14){ni,j(k, ·)ni,j(·, (cid:96))/(n − 1)}, (2.13)

(i,j)∈J

k,(cid:96)=1

where |J | denotes the cardinality of J , and for 1 ≤ k, (cid:96) ≤ 4,

ni,j(k, (cid:96)) =

ni,j(k, ·) =

n
(cid:88)

t=2
n
(cid:88)

t=2

I{(cid:98)εt

i,j = ui,j(k), (cid:98)εt−1

i,j = ui,j((cid:96))},

I{(cid:98)εt

i,j = ui,j(k)}, ni,j(·, (cid:96)) =

n
(cid:88)

t=2

I{(cid:98)εt−1

i,j = ui,j((cid:96))}.

In the above expressions, ui,j(1) = −1, ui,j(2) = − (cid:98)βi,j
1−(cid:98)αi,j
calculate the P -values of the test T based on the following permutation algorithm:

, ui,j(3) = (cid:98)αi,j
1− (cid:98)βi,j

and ui,j(4) = 1. We

1. Permute (cid:98)E1, · · · , (cid:98)En to obtain a new sequence E(cid:63)
the same manner as T with {(cid:98)Et} replaced by {E(cid:63)

1, · · · , E(cid:63)
t }.

n. Calculate the test statistic T (cid:63) in

2. Repeat 1 above M times, obtaining permutation test statistics T (cid:63)

j , j = 1, · · · , M , where
M > 0 is a large integer. The P -value of the test (for rejecting the stationary AR(1) model)

is then

1
M

M
(cid:88)

j=1

I(T < T (cid:63)

j ).

3 Autoregressive stochastic block models

The general setting in Section 2 may apply to various network processes with some speciﬁc

underlying structures as long as the edges are independent with each other. In this section we

illustrate the idea with a new dynamic stochastic block (DSB) model.

8

3.1 Models

The DSB networks are undirected (i.e. X t

i,j ≡ X t
i,i ≡ 0). Most
available DSB models assume that the networks observed at diﬀerent times are independent

j,i) with no self-loops (i.e. X t

(Pensky, 2019; Bhattacharjee et al., 2020) or conditionally independent (Xu and Hero, 2014;

Durante et al., 2016; Matias and Miele, 2017) as connection probabilities and node memberships

evolve over time. We take a radically diﬀerent approach as we impose autoregressive structure

(2.1) in the network process. Furthermore, instead of assuming that the members in the same

communities share the same (unconditional) connection probabilities, we entertain the idea that

the transition probabilities (2.3) for the members in the same communities are the same. This

reﬂects more directly the dynamic behavior of the process, and implies the former assumption on

the unconditional connection probabilities under the stationarity. See (2.7). Furthermore, since

the information on both αi,j and βi,j, instead of that on πi,j = αi,j/(αi,j +βi,j) only, will be used in

estimation, we expect that the new approach leads to more eﬃcient estimation. This is conﬁrmed

by both the theory (Theorem 1 and also Remark 3 below) and the numerical experiments (Section

4.2 below).

Let νt be the membership function at time t, i.e. for any 1 ≤ i ≤ p, νt(i) takes an integer value

between 1 and q (≤ p); indicating that node i belongs to the νt(i)-th community at time t, where

q is a ﬁxed integer. This eﬀectively assumes that the p nodes are divided into the q communities.

We assume that q is ﬁxed though some communities may contain no nodes at some times.

Deﬁnition 3.1. An AR(1) stochastic block network process {Xt = (X t

i,j), t = 0, 1, 2, · · · } is

deﬁned by (2.1), where for 1 ≤ i < j ≤ p,

P (εt

P (εt

i,j = θt

i,j = 1) = αt
i,j = 0) = 1 − αt

νt(i),νt(j), P (εt
i,j = 1 − θt

i,j = −1) = βt
νt(i),νt(j) − ηt

i,j − βt

νt(i),νt(j).

i,j = ηt

νt(i),νt(j),

(3.1)

In the above expressions, θt

k,(cid:96), ηt

k,(cid:96) are non-negative constants, and θt

k,(cid:96) + ηt

k,(cid:96) ≤ 1 for all 1 ≤ k ≤

(cid:96) ≤ q.

The evolution of membership process νt and/or the connection probabilities was often assumed

to be driven by some latent (Markov) processes. The statistical inference for those models is

carried out using computational Bayesian methods such as MCMC or EM. See, for example,

Yang et al. (2011); Xu and Hero (2014); Durante et al. (2016); Matias and Miele (2017); Rastelli

et al. (2017). Bhattacharjee et al. (2020) adopted a change point approach: assuming both the

membership and the connection probabilities remain constants either before or after a change

point. See also Ludkin et al. (2018); Wilson et al. (2019). This reﬂects the fact that many

networks (e.g. social networks) hardly change, and a sudden change is typically triggered by some

external events.

9

We adopt a change point approach in this paper. Section 3.2 considers the estimation for both

the community membership and transition probabilities when there are no change points in the

process. This will serve as a building block for the inference with a change point in Section 3.3.

Note that detecting change points in dynamic networks is a surging research area. In addition to

the aforementioned references, more recent developments include Wang et al. (2018); Zhu et al.

(2020a). Also note that the method of Zhao et al. (2019) can be applied to detect multiple change

points for any dynamic networks.

3.2 Estimation without change points

We ﬁrst consider a simple scenario of no change points in the observed period, i.e.

νt(·) ≡ ν(·)

and (θt

k,(cid:96), ηt

k,(cid:96)) ≡ (θk,(cid:96), ηk,(cid:96)),

t = 1, · · · , n, 1 ≤ k ≤ (cid:96) ≤ q.

(3.2)

Then ﬁtting the DSB model consists of two steps: (i) estimating ν(·) to cluster the p nodes into

q communities, and (ii) estimating transition probabilities θk,(cid:96) and ηk,(cid:96) for 1 ≤ k ≤ (cid:96) ≤ q. To

simplify the presentation, q is assumed to be known, which is the assumption taken by most

papers on change point detection for DSB networks.

In practice, one can determine q by, for

example, the jittering method of Chang et al. (2020b), or a Bayesian information criterion; see

an example in Section 5.2 below.

3.2.1 Why does it work?

We ﬁrst provide a theoretical underpinning (Proposition 6 below) on identifying the latent com-

munities based on αi,j and βi,j. The stochastic block model with p nodes and q communities
can be parameterized by a pair of matrices (Z, Ω), where Z = (zi,j) ∈ {0, 1}p×q is the member-

ship matrix such that it has exactly one 1 in each row and at least one 1 in each column, and
Ω = (ωk,(cid:96))q×q ∈ [0, 1]q×q is a symmetric and full rank connectivity matrix, with ωk,(cid:96) = θk,(cid:96)
θk,(cid:96)+ηk,(cid:96)
Then zi,j = 1 if and only if the i-th node belongs to the j-th community. On the other hand, ωk,(cid:96)

.

is the connection probability between the nodes in community k and the nodes in community (cid:96),
and sk ≡ (cid:80)p
are the two equivalent representations for the community membership of the network nodes.

i=1 zi,k is the size of community k ∈ {1, . . . , q}. Clearly, matrix Z and function ν(·)

Let W = ZΩZ(cid:62). Under model (3.2), the marginal edge formation probability is given as

E(Xt) = W − diag(W). Deﬁne

Ω1 = (θk,(cid:96))q×q,

Ω2 = (1 − ηk,(cid:96))q×q,

W1 = ZΩ1Z(cid:62) = (αi,j)p×p, W2 = ZΩ2Z(cid:62) = (1 − βi,j)p×p,

10

where αi,j = θν(i),ν(j), βi,j = ην(i),ν(j). Then W1 − diag(W1) can be viewed as the edge formation
probability matrix of the latent noise process ε1,t
i,j = 1). Furthermore, under model (3.2),
the latent network process {(ε1,t
i,j )1≤i,j≤p, t = 0, 1, 2, . . .} has the same membership structures as
{Xt = (X t
i,j = 1 is implied by εt
i,j = 1 under model (2.1), the elements
in W1 − diag(W1) are thus positively correlated with the elements in W − diag(W). Similarly

i,j), t = 0, 1, 2, · · · }. Since X t

i,j ≡ I(εt

i,j ≡ I(εt

W2 − diag(W2) can be viewed as the edge formation probability matrix of the latent noise
process ε2,t
as {Xt, t = 0, 1, 2, · · · }. Since εt

i,j )1≤i,j≤p, t = 0, 1, 2, . . .} has the same membership structure
i,j = 0, the elements in W2 − diag(W2) are also
positively correlated to those in W − diag(W). Let D1 and D2 be two p × p diagonal matrices

i,j (cid:54)= −1), and {(ε2,t

i,j = −1 implies X t

with, respectively, di,1, di,2 as their (i, i)-th elements, where

di,1 =

p
(cid:88)

j=1

αi,j,

di,2 =

p
(cid:88)

j=1

(1 − βi,j).

The normalized Laplacian matrices based on W1 and W2 are then deﬁned as:

L1 = D−1/2

1 W1D−1/2

1

,

L2 = D−1/2

2 W2D−1/2

2

,

L = L1 + L2.

(3.3)

Correspondingly, let s = (s1, . . . , sq)(cid:62). We denote the degree corrected connectivity matrices as

(cid:101)Ω1 = (cid:101)D−1/2

1 Ω1 (cid:101)D−1/2

1

,

(cid:101)Ω2 = (cid:101)D−1/2

2 Ω2 (cid:101)D−1/2

2

,

(cid:101)Ω = (cid:101)Ω1 + (cid:101)Ω2,

where (cid:101)D1 = diag{Ω1s} and (cid:101)D2 = diag{Ω2s} are diagonal matrices with, respectively, the degrees

of the nodes in the i-th community corresponding to Ω1 and Ω2 as their (i, i)-th elements. The

following lemma shows that the block structure in the membership matrix Z can be recovered by

the leading eigenvectors of L.

Proposition 6. Suppose (cid:101)Ω is full rank, we have, rank(L) = q. Let ΓqΛΓ(cid:62)
q be the eigen-
decomposition of L, where Λ = diag{λ1, . . . , λq} is the diagonal matrix consisting of the nonzero
eigenvalues of L arranged in the order |λ1| ≥ · · · ≥ |λq| > 0. There exists a matrix U ∈ Rq×q

such that Γq = ZU. Furthermore, for any 1 ≤ i, j ≤ p, ziU = zjU if and only if zi = zj, where

zi denotes the i-th row of Z.

Remark 1. The q columns of Γq are the orthonormal eigenvectors of L corresponding to the q

non-zero eigenvalues. Proposition 6 implies that there are only q distinct rows in the p × q matrix

Γq, and two nodes belong to a same community if and only if the corresponding rows in Γq are

the same. Intuitively the discriminant power of Γq can be understood as follows. For any unit
vector γ = (γ1, · · · , γp)(cid:62),

γ(cid:62)Lγ = 2 −

(cid:88)

1≤i<j≤p

αi,j

(cid:16) γi
(cid:112)di,1

−

γj
(cid:112)dj,1

(cid:17)2

−

(cid:88)

(1 − βi,j)

1≤i<j≤p

(cid:16) γi
(cid:112)di,2

−

(cid:17)2

.

γj
(cid:112)dj,2

(3.4)

11

For γ being an eigenvector corresponding to the positive eigenvalue of L, the sum of the 2nd and

the 3rd terms on the RHS (3.4) is minimized. Thus |γi − γj| is small when αi,j and/or (1 − βi,j)

are large; noting that di,k = dj,k for k = 1, 2 when nodes i and j belong to the same community.

The communities in a network are often formed in the way that the members within the same

community are more likely to be connected with each other, and the members belong to diﬀerent

communities are unlikely or less likely to be connected. Hence when nodes i and j belong to the

same community, αi,j tends to be large and βi,j tends to be small (see (2.3)). The converse is

true when the two nodes belong to two diﬀerent communities. The eigenvectors corresponding to

negative eigenvalues are capable to identify the so-called heterophilic communities, see pp.1892-3

of Rohe et al. (2011).

3.2.2 Estimating membership ν(·)

It follows from Proposition 1, (3.1) and (3.2) that

P (X t

ij = 1) = θν(i),ν(j)/(θν(i),ν(j) + ην(i),ν(j)) ≡ ων(i),ν(j),

1 ≤ i < j ≤ p,

provided that X 0

ij is initiated with the same marginal distribution. A simple approach adopted
in literature is to apply a community detection method for static stochastic block models using
the averaged data ¯X = (cid:80)

1≤t≤n Xt/n to detect the latent communities characterized by the
connection probabilities {ωk,(cid:96), 1 ≤ k ≤ (cid:96) ≤ q}. We take a diﬀerent approach based on estimators
{((cid:98)αi,j, (cid:98)βi,j), 1 ≤ i < j ≤ p} deﬁned in (2.12) to identify the clusters determined by the transition
probabilities {(θk,(cid:96), ηk,(cid:96)), 1 ≤ k ≤ (cid:96) ≤ q} instead. More precisely, we propose a new spectral

clustering algorithm to estimate Γq speciﬁed in Proposition 6 above.

Let (cid:99)W1, (cid:99)W2 be two p×p matrices with, respectively, (cid:98)αi,j, (1− (cid:98)βi,j) as their (i, j)-th elements for
i (cid:54)= j, and 0 on the main diagonals. Let (cid:98)D1, (cid:98)D2 be two p × p diagonal matrices with, respectively,

(cid:98)di,1, (cid:98)di,2 as their (i, i)-th elements, where

(cid:98)di,1 =

p
(cid:88)

j=1

(cid:98)αi,j,

(cid:98)di,2 =

p
(cid:88)

j=1

(1 − (cid:98)βi,j).

Deﬁne two (normalized) Laplacian matrices

(cid:98)L1 = (cid:98)D−1/2

1 (cid:99)W1 (cid:98)D−1/2

1

,

(cid:98)L2 = (cid:98)D−1/2

2 (cid:99)W2 (cid:98)D−1/2

2

Perform the eigen-decomposition for the sum of L1 and L2:

(cid:98)L ≡ (cid:98)L1 + (cid:98)L2 = (cid:98)Γ diag((cid:98)λ1, · · · , (cid:98)λp)(cid:98)Γ

(cid:62)

,

.

(3.5)

(3.6)

where the eigenvalues are arranged in the order (cid:98)λ2
p, and the columns of the p × p
orthogonal matrix (cid:98)Γ are the corresponding eigenvectors. We call (cid:98)λ1, . . . , (cid:98)λq the q leading eigen-

1 ≥ . . . ≥ (cid:98)λ2

values of (cid:98)L. Denote by (cid:98)Γq the p × q matrix consisting of the ﬁrst q columns of (cid:98)Γ, which are called

12

the leading eigenvectors of (cid:98)L. The spectral clustering applies the k-means clustering algorithm
to the p rows of (cid:98)Γq to obtain the community assignments for the p nodes (cid:98)ν(i) ∈ {1, · · · , q} for
i = 1, · · · , p.

Remark 2. Proposition 6 implies that the true memberships can be recovered by the q distinct

rows of Γq. Note that

(cid:98)L = (cid:98)L1 + (cid:98)L2 ≈ L1 − diag(L1) + L2 − diag(L2) = L − diag(L).

We shall see that the eﬀect of the term diag(L) on the eigenvectors Γq is negligible when p is

large (see for example (A.6) in the proof of Lemma 5 in Appendix A), and hence the rows of (cid:98)Γq

should be slightly perturbed versions of the q distinct rows in Γq.

The following theorem justiﬁed the validity of using (cid:98)L for spectral clustering. Note that (cid:107) · (cid:107)2

and (cid:107) · (cid:107)F denote, respectively, the L2 and the Frobenius norm of matrices.

Theorem 1. Let conditions (2.5), C1 and C2 hold, and λ−2

q

∞. Then it holds that

(cid:18)(cid:113) log(pn)

np + 1

n + 1

p

(cid:19)

→ 0, as n, p →

max
i=1,...,p

|λ2

i − (cid:98)λ2

i | ≤ (cid:107)(cid:98)L(cid:98)L − LL(cid:107)2 ≤ (cid:107)(cid:98)L(cid:98)L − LL(cid:107)F = Op

(cid:32)(cid:115)

log(pn)
np

+

1
n

+

(cid:33)

.

1
p

Moreover, for any constant B > 0, there exists a constant C > 0 such that the inequality

(cid:107)(cid:98)Γq − ΓqOq(cid:107)F ≤ 4λ−2

q C

(cid:32)(cid:115)

(cid:33)

log(pn)
np

+

1
n

+

1
p

(3.7)

(3.8)

holds with probability greater than 1 − 16p (cid:2)(pn)−(1+B) + exp{−B

√

p}(cid:3), where Oq is a q × q or-

thogonal matrix.

It follows from (3.7) that the leading eigenvalues of L can be consistently recovered by the

leading eigenvalues of (cid:98)L. By (3.8), the leading eigenvectors of L can also be consistently estimated,

subject to a rotation (due to the possible multiplicity of some leading eigenvalues L). Proposition

6 indicates that there are only q distinct rows in Γq, and, therefore, also q distinct rows in ΓqOq,

corresponding to the q latent communities for the p nodes. This paves the way for the k-means

algorithm stated below. Put

Mp,q = {M ∈ Rp×q : M has q distinct rows}.

The k-means clustering algorithm: Let

((cid:98)c1, · · · , (cid:98)cp)(cid:62) = arg min

M∈Mp,q

(cid:107)(cid:98)Γq − M(cid:107)2
F .

There are only q distinct vectors among (cid:98)c1, · · · , (cid:98)cp, forming the q communities. Theorem 2 below
shows that they are identical to the latent communities of the p nodes under (3.8) and (3.9). The

13

latter holds if

√

smaxλ−2

q C

the largest community.

(cid:18)(cid:113) log(pn)

np + 1

n + 1

p

(cid:19)

→ 0, where smax = max{s1, . . . , sq} is the size of

Theorem 2. Let (3.8) hold and

(cid:114) 1
smax

√

> 4

2λ−2

q C

(cid:32)(cid:115)

log(pn)
np

+

1
n

+

(cid:33)

.

1
p

(3.9)

Then (cid:98)ci = (cid:98)cj if and only if ν(i) = ν(j), 1 ≤ i, j ≤ p.

Remark 3. By Lemma A.1 of Rohe et al. (2011), the error bound for the standard spectral

clustering algorithm (with n = 1) is Op

p reﬂects the bias caused by
p
the inconsistent estimation of diagonal terms (see equation (A.5) and subsequent derivations in

, where the term 1

p + 1

(cid:16) log p√

(cid:17)

Rohe et al. (2011)). This bias comes directly from the removal of the diagonal elements of L, as

pointed out in Remark 2 above. Although the algorithm was designed for static networks, it has
often been applied to dynamic networks using 1
t Xt in the place of a single observed network;
n
see, e.g. Bhattacharjee et al. (2020). With some simple modiﬁcation to the proof of Lemma A.1

(cid:80)

of Rohe et al. (2011), it can be shown that the error bound is then reduced to

Op

(cid:18) log(pn)
√
np

+

(cid:19)

,

1
p

(3.10)

provided that the observed networks are i.i.d. The error would only increase when the observa-

tions are not independent. On the other hand, our proposed spectral clustering algorithm for

(dependent) dynamic networks entails the error rate speciﬁed in (3.7) and (3.8) which is smaller

than (3.10) as long as n is suﬃciently large (i.e. (p/n)

1
2 / log(np) → 0). Note that we need n to

be large enough in relation to p in order to capture the dynamic dependence of the networks.

3.2.3 Estimation for θk,(cid:96) and ηk,(cid:96)

For any 1 ≤ k ≤ (cid:96) ≤ q, we deﬁne

Sk,l =

(cid:110) {(i, j) : 1 ≤ i (cid:54)= j ≤ p, ν(i) = k, ν(j) = (cid:96)}

if k (cid:54)= l,

{(i, j) : 1 ≤ i < j ≤ p, ν(i) = k = ν(j) = (cid:96)}

if k = l,

(3.11)

Clearly the cardinality of Sk,(cid:96) is nk,(cid:96) = sks(cid:96) when k (cid:54)= (cid:96) and nk,(cid:96) = sk(sk − 1)/2 when k = (cid:96).

Based on the procedure presented in Section 3.2.2, we obtain an estimated membership func-

tion (cid:98)ν(·). Consequently, the MLEs for (θk,(cid:96), ηk,(cid:96)), 1 ≤ k ≤ (cid:96) ≤ q, admit the form

(cid:98)θk,(cid:96) =

(cid:88)

X t

i,j(1 − X t−1
i,j )

(cid:46) (cid:88)

n
(cid:88)

(1 − X t−1

i,j ),

n
(cid:88)

t=1

(i,j)∈ (cid:98)Sk,(cid:96)

(cid:88)

(cid:98)ηk,(cid:96) =

n
(cid:88)

(1 − X t

i,j)X t−1
i,j

(i,j)∈ (cid:98)Sk,(cid:96)

t=1

14

(i,j)∈ (cid:98)Sk,(cid:96)

t=1

(cid:46) (cid:88)

(i,j)∈ (cid:98)Sk,(cid:96)

n
(cid:88)

t=1

X t−1
i,j ,

(3.12)

(3.13)

where

(cid:98)Sk,(cid:96) =

(cid:110) {(i, j) : 1 ≤ i (cid:54)= j ≤ p, (cid:98)ν(i) = k, (cid:98)ν(j) = (cid:96)}
{(i, j) : 1 ≤ i < j ≤ p, (cid:98)ν(i) = (cid:98)ν(j) = k}

if k (cid:54)= (cid:96),

if k = (cid:96).

See (2.12) and also (3.1).

Theorem 2 implies that the memberships of the nodes can be consistently recovered. Conse-

quently, the consistency and the asymptotic normality of the MLEs (cid:98)θk,(cid:96) and (cid:98)ηk,(cid:96) can be established
in the same manner as for Propositions 4 and 5. We state the results below.

Let K1 = {(i1, j1), . . . , (im1, jm1)} and K2 = {(k1, (cid:96)1), . . . , (km2, (cid:96)m2)} be two arbitrary subsets

of {(k, (cid:96)) : 1 ≤ k ≤ (cid:96) ≤ q} with m1, m2 ≥ 1 ﬁxed. Let

ΨK1,K2 = (θi1,j1, . . . , θim1 ,jm1

, ηk1,(cid:96)1, . . . , ηkm2 ,(cid:96)m2

)(cid:48),

and let (cid:98)ΨK1,K2 denote its MLE. Put NK1,K2 = diag(ni1,j1, . . . , nim1 ,jm1
nk,(cid:96) is the cardinality of Sk,(cid:96) deﬁned as in (3.11).

, nk1,(cid:96)1, . . . , nkm2 ,(cid:96)m2

) where

Theorem 3. Let conditions (2.5), C1 and C2 hold, and

√

smax
λ2
q

it holds that

(cid:18)(cid:113) log(pn)

np + 1

n + 1

p

(cid:19)

→ 0. Then

max
1≤k,(cid:96)≤q

|(cid:98)θk,(cid:96) − θk,(cid:96)| = Op

(cid:32)(cid:115)

(cid:33)

log q
ns2

min

and max

1≤k,(cid:96)≤q

|(cid:98)ηk,(cid:96) − ηk,(cid:96)| = Op

(cid:32)(cid:115)

(cid:33)

,

log q
ns2

min

where smin = min{s1, . . . , sq}.

Theorem 4. Let the condition of Theorem 3 hold. Then

√

nN

1
2
K1,K2

( (cid:98)ΨK1,K2 − ΨK1,K2) → N (0, (cid:101)ΣK1,K2),

where (cid:101)ΣK1,K2 = diag((cid:101)σ11, . . . , (cid:101)σm1+m2,m1+m2) with

(cid:101)σrr =

(cid:101)σrr =

θir,jr (1 − θir,jr )(θir,jr + ηir,jr )
ηir,jr
ηkr,(cid:96)r (1 − ηkr,(cid:96)r )(θkr,(cid:96)r + ηkr,(cid:96)r )
θkr,(cid:96)r

, 1 ≤ r ≤ m1,

, m1 + 1 ≤ r ≤ m1 + m2.

Finally to prepare for the inference in Section 3.3 below, we introduce some notations. First
we denote (cid:98)ν by (cid:98)ν1,n, to reﬂect the fact that the community clustering was carried out using the
data X1, · · · , Xn (conditionally on X0). See Section 3.2.2 above. Further we denote the maximum

log likelihood by

to highlight the fact that both the node clustering and the estimation for transition probabilities

(cid:98)l(1, n; (cid:98)ν1,n) = l({(cid:98)θk,(cid:96), (cid:98)ηk,(cid:96)}; (cid:98)ν1,n)

(3.14)

are based on the data X1, · · · , Xn.

15

3.3 Inference with a change point

Now we assume that there is a change point τ0 at which both the membership of nodes and the

transition probabilities {θk,(cid:96), ηk,(cid:96)} change. It is necessary to assume n0 ≤ τ0 ≤ n − n0, where n0

is an integer and n0/n ≡ c0 > 0 is a small constant, as we need enough information before and

after the change in order to detect τ0. We assume that within the time period [0, τ0], the network

follows a stationary model (3.2) with parameters {(θ1,k,(cid:96), η1,k,(cid:96)) : 1 ≤ k, l ≤ q} and a membership
map ν1,τ0(·). Within the time period [τ0 + 1, n] the network follows a stationary model (3.2) with
parameters {(θ2,k,(cid:96), η2,k,(cid:96)) : 1 ≤ k, l ≤ q} and a membership map ντ0+1,n(·). Though we assume
that the number of communities is unchanged after the change point, our results can be easily

extended to the case that the number of communities also changes.

We estimate the change point τ0 by the maximum likelihood method:

(cid:98)τ = arg max

n0≤τ ≤n−n0

{ (cid:98)l(1, τ ; (cid:98)ν1,τ ) + (cid:98)l(τ + 1, n; (cid:98)ντ +1,n)},

(3.15)

where (cid:98)l(·) is given in (3.14).

To measure the diﬀerence between the two sets of transition probabilities before and after the

change, we put

∆2

F =

1
p2

(cid:0)(cid:107)W1,1 − W2,1(cid:107)2

F + (cid:107)W1,2 − W2,2(cid:107)2
F

(cid:1),

where the four p × p matrices are deﬁned as

W1,1 = (θ1,ν1,τ0 (i),ν1,τ0 (j)), W1,2 = (1 − η1,ν1,τ0 (i),ν1,τ0 (j)),

W2,1 = (θ2,ντ0+1,n(i),ντ0 (j)+1,n), W2,2 = (1 − η1,ντ0+1,n(i),ντ0+1,n(j)).

Note that ∆F can be viewed as the signal strength for detecting the change point τ0. Let smax, smin

denote, respectively, the largest, and the smallest community size among all the communities

before and after the change. Similar to (3.3), we denote the normalized Laplacian matrices

corresponding to Wi,j as Li,j for i, j = 1, 2. Let |λi,1| ≥ |λi,2| ≥ . . . ≥ |λi,q| be the absolute

nonzero eigenvalues of Li,1 + Li,2 for i = 1, 2, and we denote λmin = min{|λ1,q|, |λ2,q|}. Now some

regularity conditions are in order.

C3. For some constant l > 0, θi,k,(cid:96), ηi,k,(cid:96) > l, and θi,k,(cid:96) + ηi,k,(cid:96) ≤ 1 for all i = 1, 2 and 1 ≤ k ≤

(cid:96) ≤ q.

C4. log(np)/

√

p → 0, and

√

smaxλ−2
min

C5.

log(np)/n+

∆2
√
F
log(np)/(np2)

→ ∞.

(cid:0)(cid:112)log(pn)/np + 1

n + 1

p + log(np)/n+

log(np)/(np2)

(cid:1) → 0.

√

∆2
F

16

Condition C3 is similar to C1. The condition log(np)/

√

p → 0 in C4 controls the misclassiﬁcation

rate of the k-means algorithm. Recall that there is a bias term O(p−1) in spectral clustering

caused by the removal of the diagonal of the Laplacian matrix (see Remark 2 above). Intuitively,

as p increases, the eﬀect of this bias term on the misclassiﬁcation rate of the k-means algorithm

becomes negligible. On the other hand, note that the length of the time interval for searching

for the change point in (3.15) is of order O(n); the log(n) term here in some sense reﬂects the

eﬀect of the diﬃculty in detecting the true change point when the searching interval is extended

as n increases. The second condition in C4 is similar to (3.9), which ensures that the true

communities can be recovered. Condition C5 requires that the average signal strength ∆2
p−2(cid:2)(cid:107)W1,1 − W2,1(cid:107)2
point detection.

(cid:3) is of higher order than log(np)

F + (cid:107)W1,2 − W2,2(cid:107)2
F

(cid:113) log(np)
np2

F =
for change

n +

Yudovina et al. (2015) deals with the MLE for a change-point in a network Markov chain but

without a latent community structure. Hence it does not have the complication to estimate the

community memberships in addition. Allowing the membership change in our setting leads to an

extra challenge: in the process of searching for the location of the change-point, the estimation

for the latent communities before or after a speciﬁed location may not be consistent. To overcome

this obstacle, we introduce a truncation which breaks the searching interval into two parts such

that the error in the estimated change-point can be bounded. Bhattacharjee et al. (2020) also

allows the membership change. But it assumes that the networks observed at diﬀerent times are

independent with each other.

Theorem 5. Let conditions C2-C5 hold. Then the following assertions hold.

(i) When ν1,τ0 ≡ ντ0+1,n,



= Op



|τ0 − (cid:98)τ |
n

(ii) When ν1,τ0 (cid:54)= ντ0+1,n,

(cid:113) log(np)
np2

log(np)

n +
∆2
F

(cid:40)

min

(cid:110)

1, (n−1p2 log(np))

× min

1,

∆F smin

(cid:111)

1
4

(cid:41)
 .

|τ0 − (cid:98)τ |
n

= Op

(cid:113) log(np)
np2

(cid:32) log(np)

n +
∆2
F

× min




1,



(cid:110)

1, (n−1p2 log(np))

(cid:111)

1
4

min

∆F smin

(cid:41)
 .

+

1
∆2
F

Notice that for τ < τ0, the observations in the time interval [τ + 1, n] are a mixture of the two
diﬀerent network processes if ν1,τ0 (cid:54)= ντ0+1,n. In the worst case scenario then, all q communities
can be changed after the change point τ0. This causes the extra estimation error term 1
∆2
F

in

Theorem 5(ii).

17

4 Simulations

4.1 Parameter estimation

We generate data according to model (2.1) in which the parameters αij and βij are drawn inde-

pendently from U [0.1, 0.5], 1 ≤ i, j ≤ p. The initial value X0 was simulated according to (2.6)

with πij = 0.5. We calculate the estimates according to (2.12). For each setting (with diﬀerent p

and n), we replicate the experiment 500 times. Furthermore we also calculate the 95% conﬁdence

intervals for αij and βij based on the asymptotically normal distributions speciﬁed in Proposition

5, and report the relative frequencies of the intervals covering the true values of the parameters.

The results are summarized in Table 1.

Table 1: The mean squared errors (MSE) of the estimated parameters in AR(1) network model
(2.1) and the relative frequencies (coverage rates) of the event that the asymptotic 95% conﬁdence
intervals cover the true values in a simulation with 500 replications.

(cid:98)αi,j

(cid:98)βi,j

p MSE Coverage (%) MSE Coverage (%)
100
200
100
200
100
200
100
200
100
200

39.2
39.3
86.1
86.1
92.3
92..2
93.7
93.8
94.5
94.6

.130
.131
.038
.037
.012
.011
.005
.005
.002
.002

.131
.131
.037
.037
.012
.012
.005
.005
.002
.002

39.3
39.4
86.0
86.0
92.2
92.2
93.8
93.9
94.5
94.5

n
5
5
20
20
50
50
100
100
200
200

The MSE decreases as n increases, showing steadily improvement in performance. The cover-

age rates of the asymptotic conﬁdence intervals are very close to the nominal level when n ≥ 50.

The results hardly change between p = 100 and 200.

4.2 Community Detection

We now consider model (3.1) with q = 2 or 3 clusters, in which θi,i = ηi,i = 0.4 for i = 1, · · · , q,

and θi,j and ηi,j, for 1 ≤ i, j ≤ q, are drawn independently from U [0.05, 0.25]. For each setting,

we replicate the experiment 500 times.

We identify the q latent communities using the newly proposed spectral clustering algorithm

based on matrix (cid:98)L = (cid:98)L1 + (cid:98)L2 deﬁned in (3.6). For the comparison purpose, we also implement

the standard spectral clustering method for static networks (cf. Rohe et al. (2011)) but using the

18

average

¯X =

1
n

n
(cid:88)

t=1

Xt

(4.1)

in place of the single observed adjacency matrix. This idea has been frequently used in spectral

clustering for dynamic networks; see, for example, Wilson et al. (2019); Zhao et al. (2019); Bhat-

tacharjee et al. (2020). We report the normalized mutual information (NMI) and the adjusted

Rand index (ARI): Both metrics take values between 0 and 1, and both measure the closeness

between the true communities and the estimated communities in the sense that the larger the

values of NMI and ARI are, the closer the two sets of communities are; see Vinh et al. (2010). The

results are summarized in Table 2. The newly proposed algorithm based on (cid:98)L always outperforms
the algorithm based on ¯X, even when n is as small as 5. The diﬀerences between the two methods

are substantial in terms of the scores of both NMI and ARI. For example when q = 2, p = 100

and n = 5, NMI and ARI are, respectively, 0.621 and 0.666 for the new method, and they are
merely 0.148 and 0.158 for the standard method based on ¯X. This is due to the fact that the

new method identiﬁes the latent communities using the information on both αi,j and βi,j while
the standard method uses the information on πi,j = αi,j

only.

αi,j +βi,j

After the communities were identiﬁed, we estimate θi,j and ηi,j by (3.12) and (3.13), respec-

tively. The mean squared errors (MSE) are evaluated for all the parameters. The results are

summarized in Table 3. For the comparison purpose, we also report the estimates based on the
identiﬁed communities by the ¯X-based clustering. The MSE values of the estimates based on the

communities identiﬁed by the new clustering method are always smaller than those of based on
¯X. Noticeably now the estimates with small n such as n = 5 are already reasonably accurate, as

the information from all the nodes within the same community is pulled together.

5

Illustration with real data

We illustrate the proposed methodology through three real data examples in this section. More

real data analysis can be found in Appendix B.

5.1 RFID sensors data

Contacts between patients, patients and health care workers (HCW) and among HCW represent

one of the important routes of transmission of hospital-acquired infections. Vanhems et al. (2013)

collected records of contacts among patients and various types of HCW in the geriatric unit

of a hospital in Lyon, France, between 1pm on Monday 6 December and 2pm on Friday 10

December 2010. Each of the p = 75 individuals in this study consented to wear Radio-Frequency

IDentiﬁcation (RFID) sensors on small identiﬁcation badges during this period, which made it

19

Table 2: Normalized mutual information (NMI) and adjusted Rand index (ARI) of the true com-
munities and the estimated communities in the simulation with 500 replications. The communities
are estimated by the spectral clustering algorithm (SCA) based on either matrix (cid:98)L in (3.6) or
matrix ¯X in (4.1).

q
2

p
100

2

200

3

100

3

200

n
5
20
50
100
5
20
50
100
5
20
50
100
5
20
50
100

SCA based on (cid:98)L SCA based on ¯X
NMI
.621
.733
.932
.994
.808
.850
.949
.994
.542
.686
.931
.988
.729
.779
.954
.994

NMI
.148
.395
.572
.692
.375
.569
.712
.790
.078
.351
.581
.696
.195
.550
.726
.822

ARI
.666
.755
.938
.995
.839
.857
.953
.995
.536
.678
.929
.987
.731
.763
.952
.994

ARI
.158
.402
.584
.696
.406
.589
.722
.796
.057
.325
.562
.670
.175
.542
.711
.802

Table 3: The mean squared errors (MSE) of the estimated parameters in AR(1) network models
with q communities. The communities are estimated by the spectral clustering algorithm (SCA)
based on either matrix (cid:98)L in (3.6) or matrix ¯X in (4.1).

q
2

p
100

2

200

3

100

3

200

n
5
20
50
100
5
20
50
100
5
20
50
100
5
20
50
100

SCA based on (cid:98)L SCA based on ¯X
(cid:98)θi,j
.0149
.0120
.0075
.0058
.0099
.0093
.0068
.0061
.0194
.0156
.0093
.0081
.0143
.0134
.0090
.0079

(cid:98)ηi,j
.0312
.0233
.0177
.0148
.0248
.0248
.0145
.0118
.0325
.0255
.0193
.0162
.0301
.0205
.0153
.0131

(cid:98)ηi,j
.0170
.0141
.0083
.0061
.0116
.0111
.0073
.0062
.0211
.0181
.0104
.0085
.0162
.0156
.0093
.0083

(cid:98)θi,j
.0298
.0229
.0178
.0147
.0223
.0219
.0140
.0117
.0318
.0251
.0193
.0163
.0287
.0200
.0156
.0130

20

possible to record when any two of them were in face-to-face contact with each other (i.e. within

1-1.5 meters) in every 20-second interval during the period. This data set is now available in R

packages igraphdata and sand.

Following Vanhems et al. (2013), we combine together the recorded information in each 24

hours to form 5 daily networks (n = 5), i.e. an edge between two individuals is equal to 1 if they

made at least one contact during the 24 hours, and 0 otherwise. Those 5 networks are plotted in

Figure 1. We ﬁt the data with stationary AR(1) model (2.1) and (2.5). Some summary statistics

of the estimated parameters, according to the 4 diﬀerent roles of the individuals, are presented
in Table 4, together with the direct relatively frequency estimates (cid:101)πi,j = ¯Xi,j = (cid:80)5
i,j/5.
We apply the permutation test (2.13) (with 500 permutations) to the residuals resulted from the

t=1 X t

ﬁtted AR(1) model. The P -value is 0.45, indicating no signiﬁcant evidence against the stationarity

assumption.

Since the original data were recorded for each 20 seconds, they can also be combined into

half-day series with n = 10. Figure 2 presents the 10 half-day networks. We repeat the above

exercise for this new sequence. Now the P -value of the permutation test is 0.008, indicating the

stationary AR(1) model should be rejected for this sequence of 10 networks. This is intuitively

understandable, as people behave diﬀerently at the diﬀerent times during a day (such as daytime

or night). Those within-day nonstationary behaviour shows up in the data accumulation over

every 12 hours, and it disappears in the accumulation over 24 hour periods. Also overall the

adjacent two networks in Figure 2 look more diﬀerent from each other than the adjacent pairs in

Figure 1.

There is no evidence of the existence of any communities among the 75 individuals in this data

set. Our analysis conﬁrms this too. For example the results of the spectral clustering algorithm
based on, respectively, (cid:98)L and ¯X do not corroborate with each other at all as the NMI is smaller
than 0.1.

5.2 French high school contact data

Now we consider a contact network data collected in a high school in Marseilles, France (Mas-

trandrea et al., 2015). The data are the recorded face-to-face contacts among the students from 9

classes during n = 5 days in December 2013, measured by the SocioPatterns infrastructure. Those

are students in the so-called classes preparatoires – a part of the French post-secondary education

system. We label the 3 classes majored in mathematics and physics as MP1, MP2 and MP3,

the 3 classes majored in biology as BIO1, BIO2 and BIO3, the 2 classes majored in physics and

chemistry as PC1 and PC2, and the class majored in engineering as EGI. The data are available at

www.sociopatterns.org/datasets/high-school-contact-and-friendship-networks/. We

21

.
s
d
o
i
r
e
p

r
u
o
h
-
4
2

e
v
ﬁ

e
h
t

f
o

h
c
a
e

n
i
h
t
i
w
n
o
i
t
a
m
r
o
f
n
i

e
h
t

r
e
h
t
e
g
o
t

g
n
i
n
i
b
m
o
c

y
b

d
e
n
i
a
t
b
o

s
k
r
o
w
t
e
n

5

e
h
t

:
a
t
a
d

s
r
o
s
n
e
s
D
I
F
R
e
h
T

:
1

e
r
u
g
i
F

.
s
r
u
o
l
o
c

t
n
e
r
e
ﬀ
i
d

r
u
o
f

n
i

d
e
k
r
a
m
e
r
a

s
l
a
u
d
i
v
i
d
n
i

e
h
t

f
o

s
e
i
t
i
t
n
e
d
i

t
n
e
r
e
ﬀ
d

i

r
u
o
f

e
h
T

22

Time=1Time=2Time=3Time=4Time=5ADM1NUR1NUR2NUR3NUR4NUR5NUR6NUR7MED1NUR8MED2MED3NUR9MED4MED5MED6NUR10MED7ADM2NUR11NUR12MED8NUR13NUR14NUR15NUR16NUR17ADM3NUR18MED9ADM4NUR19NUR20NUR21MED10NUR22NUR23PAT1PAT2PAT3PAT4PAT5PAT6PAT7PAT8PAT9PAT10PAT11PAT12PAT13PAT14PAT15PAT16PAT17PAT18PAT19NUR24ADM5ADM6PAT20NUR25NUR26NUR27ADM7MED11PAT21PAT22PAT23PAT24PAT25ADM8PAT26PAT27PAT28PAT29ADM MED NUR PAT ADM MED NUR PAT .
s
d
o
i
r
e
p

r
u
o
h
-
2
1

n
e
t

e
h
t

f
o

h
c
a
e

n
i
h
t
i
w
n
o
i
t
a
m
r
o
f
n
i

e
h
t

r
e
h
t
e
g
o
t

g
n
i
n
i
b
m
o
c

y
b

d
e
n
i
a
t
b
o

s
k
r
o
w
t
e
n

0
1

e
h
t

:
a
t
a
d

s
r
o
s
n
e
s
D
I
F
R
e
h
T

:
2

e
r
u
g
i
F

.
s
r
u
o
l
o
c

t
n
e
r
e
ﬀ
i
d

r
u
o
f

n
i

d
e
k
r
a
m
e
r
a

s
l
a
u
d
i
v
i
d
n
i

e
h
t

f
o

s
e
i
t
i
t
n
e
d
i

t
n
e
r
e
ﬀ
d

i

r
u
o
f

e
h
T

23

Time=1Time=2Time=3Time=4Time=5Time=6Time=7Time=8Time=9Time=10ADM1NUR1NUR2NUR3NUR4NUR5NUR6NUR7MED1NUR8MED2MED3NUR9MED4MED5MED6NUR10MED7ADM2NUR11NUR12MED8NUR13NUR14NUR15NUR16NUR17ADM3NUR18MED9ADM4NUR19NUR20NUR21MED10NUR22NUR23PAT1PAT2PAT3PAT4PAT5PAT6PAT7PAT8PAT9PAT10PAT11PAT12PAT13PAT14PAT15PAT16PAT17PAT18PAT19NUR24ADM5ADM6PAT20NUR25NUR26NUR27ADM7MED11PAT21PAT22PAT23PAT24PAT25ADM8PAT26PAT27PAT28PAT29ADM MED NUR PAT ADM MED NUR PAT Table 4: Mean estimated coeﬃcients (standard errors) for the four types of individuals in RFID
data. Status codes: administrative staﬀ (ADM), medical doctor (MED), paramedical staﬀ, such
as nurses or nurses’ aides (NUR), and patients (PAT).
(cid:98)αij

ADM

Status
ADM .1249 (.2212)
NUR
MED
PAT

NUR
.1739 (.2521)
.2347 (.2927)

MED
.1666 (.2641)
.2398 (.3022)
.3594 (.3883)

ADM

Status
ADM .1666 (.3660)
NUR
MED
PAT

ADM

Status
ADM .2265 (.3900)
NUR
MED
PAT

ADM

Status
ADM .1250 (.3312)
NUR
MED
PAT

(cid:98)βij

NUR
.2326 (.3883)
.3714 (.4470)

MED
.2925 (.4235)
.3001 (.4167)
.4187 (.3973)

(cid:98)πij = (cid:98)αij/((cid:98)αij + (cid:98)βij)
MED
NUR
.1893 (.3119)
.2478 (.3672)
.2729 (.3491)
.2488 (.3244)
.3310 (.3674)

(cid:101)πi,j = ¯Xij

NUR
.1583 (.3652)
.1854 (.3887)

MED
.1704 (.3764)
.1730 (.3784)
.3901 (.4881)

PAT
.1113 (.2021)
.1922 (.2513)
.1264 (.2175)
.0089 (.0552)

PAT
.2061 (.3798)
.3656 (.4498)
.2311 (.4066)
.0198 (.1331)

PAT
.1239 (.2490)
.2088 (.3016)
.1398 (.2660)
.0124 (.0928)

PAT
.0887 (.2845)
.1542 (.3612)
.0927 (.2902)
.0090 (.0946)

have removed the individuals with missing values, and include the remaining p = 327 students in

our clustering analysis based on the AR(1) stochastic block network model (see Deﬁnition 3.1).

We start the analysis with q = 2. The detected 2 clusters by the spectral clustering algorithm
(SCA) based on either (cid:98)L in (3.6) or ¯X are reported in Table 5. The two methods lead to almost
identical results: 3 classes majored in biology are in one cluster and the other 6 classes are in the

other cluster. The number of ‘misplaced’ students is 2 and 1, respectively, by the SCA based on
(cid:98)L and ¯X. Figure 3 shows that the identiﬁed two clusters are clearly separated from each other
across all the 5 days. The permutation test (2.13) on the residuals indicates that the stationary

AR(1) stochastic block network model seems to be appropriate for this data set, as the P -value

is 0.676. We repeat the analysis for q = 3, leading to equally plausible results: 3 biology classes

are in one cluster, 3 mathematics and physics classes are in another cluster, and the 3 remaining

classes form the 3rd cluster. See also Figure 4 for the graphical illustration with the 3 clusters.

To choose the number of clusters q objectively, we deﬁne the Bayesian information criteria

24

A
C
S

y
b

d
e
n
i
m
r
e
t
e
d

s
r
e
t
s
u
l
c

2
=
q

e
h
t

t
n
e
s
e
r
p
e
r

s
r
u
o
l
o
c

o
w
t

n
i

d
e
k
r
a
m

s
e
d
o
n

e
h
t

:
s
y
a
d

5

r
e
v
o

s
k
r
o
w
t
e
n

t
c
a
t
n
o
c

l
o
o
h
c
s

h
g
i
h

h
c
n
e
r
F

:
3

e
r
u
g
i
F

.
)
4
.
3
(

n
i

(cid:98)L
n
o

d
e
s
a
b

25

Table 5: French high school contact network data: the detected clusters by spectral clustering
algorithm (SCA) based on either (cid:98)L in (3.4) or ¯X. The number of clusters is set at q = 2.

SCA based on (cid:98)L

SCA based on ¯X

Class Cluster 1 Cluster 2 Cluster 1 Cluster 2
BIO1
BIO2
BIO3
MP1
MP2
MP3
PC1
PC2
EGI

0
1
1
33
29
38
44
39
34

1
0
0
33
29
38
44
39
34

37
32
39
0
0
0
0
0
0

36
33
40
0
0
0
0
0
0

(BIC) as follows:

BIC(q) = −2 max log(likelihood) + log{n(p/q)2}q(q + 1).

For each ﬁxed q, we eﬀectively build q(q + 1)/2 models independently and each model has 2

parameters θk,(cid:96) and ηk,(cid:96), 1 ≤ k ≤ (cid:96) ≤ q. The number of the available observations for each model
is approximately n(p/q)2, assuming that the numbers of nodes in all the q clusters are about the
same, which is then p/q. Thus the penalty term in the BIC above is (cid:80)
1≤k≤(cid:96)≤q 2 log{n(p/q)2} =
log{n(p/q)2}q(q + 1).

Table 6 lists the values of BIC(q) for diﬀerent q. The minimum is obtained at q = 9, exactly

the number of original classes in the school. Performing the SCA based on (cid:98)L with q = 9, we

obtain almost perfect classiﬁcation: all the 9 original classes are identiﬁed as the 9 clusters with

only in total 4 students being placed outside their own classes. Figure 5 plots the networks

with the identiﬁed 9 clusters in 9 diﬀerent colours. The estimated θi,j and ηi,j, together with

their standard errors calculated based on the asymptotic normality presented in Theorem 4, are

reported in Table 7. As (cid:98)θi,j for i (cid:54)= j are very small (i.e. ≤ 0.027), the students from diﬀerent

classes who have not contacted with each other are unlikely to contact next day. See (3.1) and

(2.3). On the other hand, as (cid:98)ηi,j for i (cid:54)= j are large (i.e. ≥ 0.761), the students from diﬀerent
classes who have contacted with each other are likely to lose the contacts next day. Note that (cid:98)θi,i
are greater than (cid:98)θi,j for i (cid:54)= j substantially, and (cid:98)ηi,i are smaller than (cid:98)ηi,j for i (cid:54)= j substantially.
This implies that the students in the same class are more likely to contact with each other than

those across the diﬀerent classes.

To apply the variational EM algorithm of Matias and Miele (2017) to analyze this data set, we

use the R package dynsbm. The algorithm is designed to identify time-varying dynamic stochastic

block structure in the sense that both the membership of nodes and the transition probabilities

may vary with time. Furthermore it also identiﬁes the nodes not belonging to any clusters. The

26

Table 6: Fitting AR(1) stochastic block models to the French high school data: BIC values for
diﬀerent cluster numbers q.

q
BIC(q)

2
43624

3
40586

5
37726

7
36112

8
35224

9
34943

10
35002

11
35120

number of the clusters selected by the so-called integrated classiﬁcation likelihood criterion is also

9. The identiﬁed 9 clusters are always dominated by the 9 original classes in the school, though

they vary from day to day. The number of the identiﬁed students not belonging to any of the 9

clusters was 15, 17, 24, 32 and 28, respectively, in those 5 days. Furthermore the number of the

students who were not put in their own classes was 14, 9, 12, 10 and 12, respectively. The more

detailed results are reported in Appendix B. Those ﬁndings are less clear-cut than those obtained

from our method above. This is hardly surprising as Matias and Miele (2017) adopts a general

setting without imposing stationarity.

5.3 Global trade data

Our last example concerns the annual international trades among p = 197 countries between 1950

and 2014 (i.e. n = 65). We deﬁne an edge between two countries to be 1 as long as there exist

trades between the two countries in that year (regardless the direction), and 0 otherwise. We

take this simplistic approach to illustrate our AR(1) stochastic block model with a change point.

The data used are a subset of the openly available trade data for 205 countries in 1870 – 2014

(Barbieri et al., 2009; Barbieri and Keshk, 2016). We leave out several countries, e.g. Russia and

Yugoslavia, which did not exist for the whole period concerned.

Setting q = 2, we ﬁt the data with an AR(1) stochastic block model with two clusters. The

P -value of the permutation test for the residuals resulted from the ﬁtted model is 0, indicating

overwhelmingly that the stationarity does not hold for the whole period. Applying the maximum

likelihood estimator (3.15), the estimated change point is at year 1991. Before this change point,

the identiﬁed Cluster I contains 26 countries, including the most developed industrial countries

such as USA, Canada, UK and most European countries. Cluster II contains 171 countries,

including all African and Latin American countries, and most Asian countries. After 1991, 41

countries switched from Cluster II to Cluster I, including Argentina, Brazil, Bulgaria, China,

Chile, Columbia, Costa Rica, Cyprus, Hungary, Israel, Japan, New Zealand, Poland, Saudi Ara-

bia, Singapore, South Korea, Taiwan, and United Arab Emirates. There was no single switch

from Cluster I to II. Note that 1990 may be viewed as the beginning of the globalization. With the

collapse of the Soviet Union in 1989, the fall of Berlin Wall and the end of the Cold War in 1991,

the world became more interconnected. The communist bloc countries in East Europe, which had

27

y
b

d
e
n
i
m
r
e
t
e
d

s
r
e
t
s
u
l
c

3
=
q

e
h
t

t
n
e
s
e
r
p
e
r

s
r
u
o
l
o
c

e
e
r
h
t

n
i

d
e
k
r
a
m

s
e
d
o
n

e
h
t

:
s
y
a
d

5

r
e
v
o

s
k
r
o
w
t
e
n

t
c
a
t
n
o
c

l
o
o
h
c
s

h
g
i
h

h
c
n
e
r
F

:
4

e
r
u
g
i
F

.
)
4
.
3
(

n
i

(cid:98)L
n
o

d
e
s
a
b
A
C
S

28

Day 1Day 2Day 3Day 4Day 5y
b

d
e
n
i
m
r
e
t
e
d

s
r
e
t
s
u
l
c

9
=
q

e
h
t

t
n
e
s
e
r
p
e
r

s
r
u
o
l
o
c

n
e
v
e
s

n
i

d
e
k
r
a
m

s
e
d
o
n

e
h
t

:
s
y
a
d

5

r
e
v
o

s
k
r
o
w
t
e
n

t
c
a
t
n
o
c

l
o
o
h
c
s

h
g
i
h

h
c
n
e
r
F

:
5

e
r
u
g
i
F

.
)
4
.
3
(

n
i

(cid:98)L
n
o

d
e
s
a
b
A
C
S

29

Day 1Day 2Day 3Day 4Day 5Table 7: Fitting AR(1) stochastic block models with q = 9 clusters to the French high school
data: the estimation parameters and their standard errors (in parentheses).

Cluster
1

1
.246
(.008)

2
.001
(.001)
.136
(.009)

3
.004
(.001)
.024
(.002)
.252
(.011)

4
.006
(.001)
.0018
(.001)
.001
(.001)
.234
(.010)

5
.001
(.001)
.001
(.001)
.002
(.001)
.020
(.002)
.196
(.008)

.563
(.015)

.999
(.001)
.472
(.024)

.959
(.036)
.761
(.036)
.453
(.016)

.976
(.098)
.888
(.097)
.999
(.000)
.509
(.017)

.999
(.001)
.999
(.001)
.928
(.066)
.868
(.028)
.544
(.017)

(cid:98)θi,j

(cid:98)ηi,j

2

3

4

5

6

7

8

9

1

2

3

4

5

6

7

8

9

6
.009
(.001)
.007
(.001)
.007
(.001)
.001
(.001)
.001
(.001)
.181
(.008)

.867
(.054)
.866
(.054)
.864
(.048)
.999
(.000)
.999
(.001)
.589
(.019)

7
.003
(.001)
.001
(.000)
.001
(.001)
.024
(.002)
.020
(.002)
.001
(.001)
.252
(.009)

.870
(.001)
.999
(.001)
.999
(.000)
.784
(.029)
.929
(.021)
.999
(.001)
.480
(.014)

8
.024
(.002)
.001
(.001)
.001
(.001)
.002
(.001)
.002
(.000)
.010
(.001)
.003
(.001)
.202
(.006)

.792
(.000)
.999
(.000)
.999
(.000)
.956
(.041)
.842
(.078)
.793
(.040)
.999
(.000)
.504
(.127)

9
.003
(.001)
.027
(.002)
.022
(.002)
.001
(.001)
.004
(.001)
.007
(.001)
.006
(.001)
.001
(.001)
.219
(.008)
.909
(.051)
.866
(.026)
.772
(.031)
.999
(.000)
.935
(.041)
.923
(.036)
.814
(.051)
.999
(.000)
.471
(.014)

been isolated from the capitalist West, began to integrate into the global market economy. Trade

and investment increased, while barriers to migration and to cultural exchange were lowered.

Figure 6 presents the average adjacency matrix of the 197 countries before and after the change

point, where the cold blue color indicates small value and the warm red color indicates large value.

Before 1991, there are only 26 countries in Cluster 1. The intensive red in the small lower left

corner indicates the intensive trades among those 26 countries. After 1991, the densely connected

lower left corner is enlarged as now there are 67 countries in Cluster 1. Note some members of

30

Cluster 2 also trade with the members of Cluster 1, though not all intensively.

The estimated parameters for the ﬁtted AR(1) stochastic block model with q = 2 clusters

are reported in Table 8. Since estimated values for (cid:98)θ1,2, (cid:98)η1,2 before and after the change point
are always small, the trading status between the countries across the two clusters are unlikely to

change. Nevertheless (cid:98)θ1,2 is 0.154 after 1991, and 0.053 before 1991; indicating greater possibility

for new trades to happen after 1991.

Table 8: Fitting AR(1) stochastic block model with a change point and q = 2 to the Global trade
data: the estimated AR coeﬃcients before and after 1991.

t ≤ 1991

t > 1991

Coeﬃcients Estimates

θ1,1
θ1,2
θ2,2
η1,1
η1,2
η2,2

.062
.053
.023
.003
.037
.148

SE
.0092
.0008
.0002
.0005
.0008
.0012

Estimates
.046
.154
.230
.144
.047
.006

SE
.0005
.0013
.0109
.0016
.0007
.0003

A ﬁnal remark. We proposed in this paper a simple AR(1) setting to represent the dynamic

dependence in network data explicitly.

It also facilitates easy inference such as the maximum

likelihood estimation and model diagnostic checking. A new class of dynamic stochastic block

models illustrates the usefulness of the setting in handling more complex underlying structures

including structure breaks due to change points.

It is conceivable to construct AR(p) or even ARMA network models following the similar

lines. However a more fertile exploration is perhaps to extend the setting for the networks with

dependent edges, incorporating in the model some stylized features of network data such as

transitivity, homophily. The development in this direction will be reported in a follow-up paper.

On the other hand, dynamic networks with weighted edges may be treated as matrix time series for

which eﬀective modelling procedures have been developed based on various tensor decompositions

(Wang et al., 2019; Chang et al., 2020a).

References

Aggarwal, C. and Subbian, K. (2014). Evolutionary network analysis: A survey. ACM Computing

Surveys (CSUR), 47(1):1–36.

Barbieri, K. and Keshk, O. M. G. (2016). Correlates of War Project Trade Data Set Codebook,

Version 4.0. Online: http://correlatesofwar.org.

Barbieri, K., Keshk, O. M. G., and Pollins, B. (2009). Trading data: Evaluating our assumptions

and coding rules. Conﬂict Management and Peace Science, 26(5):471–491.

31

d
e
d
o
c
-
r
u
o
l
o
c

e
r
a

1

o
t

0
m
o
r
f

s
e
u
l
a
v

e
h
T

.
1
9
9
1

r
e
t
f
a

d
n
a

e
r
o
f
e
b

s
e
i
r
t
n
u
o
c

7
9
1

e
h
t

g
n
o
m
a

s
e
d
a
r
t

e
h
t

r
o
f

x
i
r
t
a
m
y
c
n
e
c
a
j
d
a

e
g
a
r
e
v
A

:
6

e
r
u
g
i
F

.
d
e
r

o
t

d
e
r

t
h
g
i
l

,
e
u
l
b

t
h
g
i
l

,
e
u

l

b
m
o
r
f

32

Bennett, G. (1962). Probability inequalities for the sum of independent random variables. Journal

of the American Statistical Association, 57(297):33–45.

Bhattacharjee, M., Banerjee, M., and Michailidis, G. (2020). Change point estimation in a dy-

namic stochastic block model. Journal of Machine Learning Research, 21(107):1–59.

Bradley, R. C. (2007). Introduction to strong mixing conditions. Kendrick press.

Chang, J., He, J., and Yao, Q. (2020a). Modelling matrix time series via a tensor cp-

decomposition. Under preparation.

Chang, J., Kolaczyk, E. D., and Yao, Q. (2020b). Discussion of “network cross-validation by edge

sampling”. Biometrika, 107(2):277–280.

Chang, J., Kolaczyk, E. D., and Yao, Q. (2020c). Estimation of subgraph densities in noisy

networks. Journal of the American Statistical Association, (In press):1–40.

Chen, E. Y., Fan, J., and Zhu, X. (2020). Community network auto-regression for high-

dimensional time series. arXiv:2007.05521.

Corneli, M., Latouche, P., and Rossi, F. (2018). Multiple change points detection and clustering

in dynamic networks. Statistics and Computing, 28(5):989–1007.

Crane, H. et al. (2016). Dynamic random networks and their graph limits. The Annals of Applied

Probability, 26(2):691–721.

Donnat, C. and Holmes, S. (2018). Tracking network dynamics: A survey of distances and

similarity metrics. The Annals of Applied Statistics, 12(2):971–1012.

Durante, D., Dunson, D. B., et al. (2016). Locally adaptive dynamic networks. The Annals of

Applied Statistics, 10(4):2203–2232.

Durrett, R. (2019). Probability: theory and examples, volume 49. Cambridge university press.

Fan, J. and Yao, Q. (2003). Nonlinear Time Series: Nonparametric and Parametric Methods.

Springer, New York.

Friel, N., Rastelli, R., Wyse, J., and Raftery, A. (2016). Interlocking directorates in irish companies
using a latent space model for bipartite networks. Proceedings of the national academy of
sciences, 113(24):6629–6634.

Fu, W., Song, L., and Xing, E. P. (2009). Dynamic mixed membership blockmodel for evolving
networks. In Proceedings of the 26th Annual International Conference on Machine Learning,
pages 329–336.

Hanneke, S., Fu, W., and Xing, E. P. (2010). Discrete temporal models of social networks.

Electronic Journal of Statistics, 4:585–605.

Kang, X., Ganguly, A., and Kolaczyk, E. D. (2017). Dynamic networks with multi-scale temporal

structure. arXiv preprint arXiv:1712.08586.

Knight, M., Nunes, M., and Nason, G. (2016). Modelling, detrending and decorrelation of network

time series. arXiv preprint arXiv:1603.03221.

Kolaczyk, E. D. (2017). Topics at the Frontier of Statistics and Network Analysis. Cambridge

University Press.

33

Krivitsky, P. N. and Handcock, M. S. (2014). A separable model for dynamic networks. Journal

of the Royal Statistical Society, B, 76(1):29.

Lin, Z. and Bai, Z. (2011). Probability inequalities. Springer Science & Business Media.

Ludkin, M., Eckley, I., and Neal, P. (2018). Dynamic stochastic block models: parameter estima-
tion and detection of changes in community structure. Statistics and Computing, 28(6):1201–
1213.

Mastrandrea, R., Fournet, J., and Barrat, A. (2015). Contact patterns in a high school: A com-
parison between data collected using wearable sensors, contact diaries and friendship surveys.
PLoS ONE, 10(9):e0136497.

Matias, C. and Miele, V. (2017). Statistical clustering of temporal networks through a dynamic

stochastic block model. Journal of the Royal Statistical Society, B, 79(4):1119–1141.

Matias, C., Rebafka, T., and Villers, F. (2018). A semiparametric extension of the stochastic

block model for longitudinal networks. Biometrika, 105(5):989–1007.

Merlev`ede, F., Peligrad, M., Rio, E., et al. (2009). Bernstein inequality and moderate deviations
under strong mixing conditions. In High dimensional probability V: the Luminy volume, pages
273–292. Institute of Mathematical Statistics.

Pensky, M. (2019). Dynamic network models and graphon estimation. Annals of Statistics,

47(4):2378–2403.

Rastelli, R., Latouche, P., and Friel, N. (2017). Choosing the number of groups in a latent

stochastic block model for dynamic networks. Online: http://arxiv.org/abs/1702.01418.

Rohe, K., Chatterjee, S., Yu, B., et al. (2011). Spectral clustering and the high-dimensional

stochastic blockmodel. The Annals of Statistics, 39(4):1878–1915.

Snijders, T. A. B. (2005). Models for longitudinal network data. In Carrington, P., Scott, J.,
and Wasserman, S. S., editors, Models and Methods in Social Network Analysis, chapter 11.
Cambridge University Press, New York.

Vanhems, P., Barrat, A., Cattuto, C., Pinton, J.-F., Khanafer, N., Regis, C., a. Kim, B., and
B. Comte, N. V. (2013). Estimating potential infection transmission routes in hospital wards
using wearable proximity sensors. PloS ONE, 8:e73970.

Vinh, N. X., Epps, J., and Bailey, J. (2010).

Information theoretic measures for clusterings
comparison: Variants, properties, normalization and correction for chance. Journal of Machine
Learning Research, 11:2837–2854.

Wang, D., Liu, X., and Chen, R. (2019). Factor models for matrix-valued high-dimensional time

series. Journal of Econometrics, 208(1):231–248.

Wang, D., Yu, Y., and Rinaldo, A. (2018). Optimal change point detection and localization in

sparse dynamic networks. arXiv preprint arXiv:1809.09602.

Wilson, J. D., Stevens, N. T., and Woodall, W. H. (2019). Modeling and detecting change in
temporal networks via the degree corrected stochastic block model. Quality and Reliability
Engineering International, 35(5):1363–1378.

34

Xu, K. S. and Hero, A. O. (2014). Dynamic stochastic blockmodels for time-evolving social

networks. IEEE Journal of Selected Topics in Signal Processing, 8(4):552–562.

Yang, T., Chi, Y., Zhu, S., Gong, Y., and Jin, R. (2011). Detecting communities and their
evolutions in dynamic social networks?a bayesian approach. Machine learning, 82(2):157–189.

Yu, Y., Wang, T., and Samworth, R. J. (2015). A useful variant of the davis–kahan theorem for

statisticians. Biometrika, 102(2):315–323.

Yudovina, E., Banerjee, M., and Michailidis, G. (2015). Changepoint inference for erd¨os-r´enyi ran-
dom graphs. In Stochastic Models, Statistics and Their Applications, pages 197–205. Springer.

Zhao, Z., Chen, L., and Lin, L. (2019). Change-point detection in dynamic networks via graphon

estimation. arXiv preprint arXiv:1908.01823.

Zhu, T., Li, P., Yu, L., Chen, K., and Chen, Y. (2020a). Change point detection in dynamic
networks based on community identiﬁcation. IEEE Transactions on Network Science and En-
gineering.

Zhu, X., Huang, D., Pan, R., and Wang, H. (2020b). Multivariate spatial autoregressive model

for large scale social networks. Journal of Econometrics, 215(2):591–606.

Zhu, X., Pan, R., Li, G., Liu, Y., and Wang, H. (2017). Network vector autoregression. The

Annals of Statistics, 45(3):1096–1123.

Zhu, X., Wang, W., Wang, H., and H¨ardle, W. K. (2019). Network quantile autoregression.

Journal of Econometrics, 212(1):345–358.

35

“Autoregressive Networks” by B. Jiang, J. Li and Q. Yao

Appendix: Technical proofs and further real data analysis

A.1 Proof of Proposition 1

Note all X t

i,j take binary values 0 or 1. Hence

P (X 1

i,j = 1) = P (X 0

i,j = 1)P (X 1

i,j = 1) + P (X 0

i,j = 0)P (X 1

i,j = 1|X 0
αi,j
αi,j + βi,j

(1 − βi,j) +

βi,j
αi,j + βi,j

i,j = 0)

i,j = 1|X 0
αi,j
αi,j + βi,j

αi,j =

= πi,j.

=πi,j(1 − βi,j) + (1 − πi,j)αi,j =

i,j) = L(X 0

i,j). Since all Xt are Erd´os-Renyi, L(X1) = L(X0). Condition (2.5) ensures
Thus L(X 1
that {Xt} is a homogeneous Markov chain. Hence L(Xt) = L(X0) for any t ≥ 1. This implies

the required stationarity.
i,j) = P (X t

As E(X t

i,j = 1), and Var(X t

i,j) = E(X t

i,j) − {E(X t

i,j)}2, (2.8) follows from the

stationarity, (2.6) and (2.7).

Note that (2.1) implies a Yule-Walker equation

γi,j(k) = (1 − αi,j − βi,j)γi,j(k − 1),

k = 1, 2, · · · ,

(A.1)

where γi,j(k) = Cov(X t+k
i,j

, X t

i,j).

Since the networks are all Erd¨os-Renyi, (2.9) follows from the Yule-Walker equation (A.1)

immediately, noting ρi,j(k) = γi,j(k)/γi,j(0) and ρi,j(0) = 1. To prove (A.1), it follows from (2.1)

that for any k ≥ 1,

E(X t+k

i,j X t

i,j) = E(X t+k−1

i,j X t

i,j)P (εt+k

i,j = 1)EX t
i,j

= (1 − αi,j − βi,j)E(X t+k−1

i,j/(αi,j + βi,j).

i,j = 0) + P (εt+k
i,j) + α2
i,j X t

Thus

γi,j(k) = E(X t+k

i,j X t

i,j) − (EX t

i,j)2 = E(X t+k

i,j X t

i,j) −

α2
i,j
(αi,j + βi,j)2

= (1 − αi,j − βi,j)E(X t+k−1

i,j X t

i,j) +

α2
i,j
αi,j + βi,j

(1 −

1
αi,j + βi,j

)

= (1 − αi,j − βi,j){E(X t+k−1

i,j X t

i,j) −

α2
i,j

(αi,j + βi,j)2 } = (1 − αi,j − βi,j)γi,j(k − 1).

This completes the proof.

A.2 Proof of Proposition 2

We only prove (2.11), as (2.10) follows from (2.11) immediately. To prove (2.11), we only need to

show

di,j(k) ≡ P (X t

i,j (cid:54)= X t+k

i,j ) =

2αi,jβi,j
(αi,j + βi,j)2 {1 − (1 − αi,j − βi,j)k},

k = 1, 2, · · · .

(A.2)

1

We Proceed by induction. It is easy to check that (A.2) holds for k = 1. Assuming it also holds

for k ≥ 1, then

di,j(k + 1) = P (X t

= P (X t

i,j = 0, X t+k

i,j = 0, X t+k+1

i,j
i,j = 1, X t+k+1

i,j
i,j = 0, X t+k+1

+ P (X t

i,j = 1, X t+k

= P (X t

i,j = 0, X t+k

i,j
i,j = 1)(1 − βi,j) + {P (X t

= 1) + P (X t

i,j = 1, X t+k+1

= 0)

= 1) + P (X t

i,j
i,j = 0, X t+k

= 0) + P (X t

i,j = 1, X t+k

i,j = 0) − P (X t

= 1)

i,j = 0, X t+k+1

i,j
i,j = 1, X t+k+1
i,j
i,j = 0, X t+k

i,j = 1)}αi,j

= 0)

+ P (X t

i,j = 1, X t+k

i,j = 0)(1 − αi,j) + {P (X t

= {P (X t

i,j = 0, X t+k

i,j = 1) + P (X t

i,j = 1, X t+k

= di,j(k)(1 − αi,j − βi,j) +

2αi,jβi,j
αi,j + βi,j

=

i,j = 1, X t+k

i,j = 1) − P (X t

i,j = 0)}(1 − αi,j − βi,j) +

i,j = 0)}βi,j
2αi,jβi,j
αi,j + βi,j
2αi,jβi,j
(αi,j + βi,j)2 {1 − (1 − αi,j − βi,j)k+1}.

Hence (A.2) also holds for k + 1. This completes the proof.

A.3 Proof of Proposition 3

Proof. Note that for any nonempty elements A ∈ F k
B0 ∈ F ∞

k+τ +1 such that A = A0 × {0}, A0 × {1}, or A0 × {0, 1}, and B = B0 × {0}, B0 × {1},
or B0 × {0, 1}. We ﬁrst consider the case where B = B0 × {xk} and A = A0 × {xk+τ } where

0 , B ∈ F ∞

k+τ , there exist A0 ∈ F k−1

0

and

xk, xk+τ = 0 or 1. Note that

P (A0, X k

i,j = xk, B0, X k+τ
i,j = xk+τ )P (X k+τ

i,j = xk+τ )

i,j = xk+τ , A0, X k

= P (B0|X k+τ

= P (B0, X k+τ

i,j = xk+τ )P (A0, X k

i,j = xk) ·

= P (B0, X k+τ

i,j = xk+τ )P (A0, X k

i,j = xk) ·

On the other hand, note that

i,j = xk)

P (X k+τ

i,j = xk)
i,j = xk+τ |X k
P (X k+τ
i,j = xk+τ )
i,j = xk+τ , X k
i,j = xk+τ )P (X k

P (X k+τ
P (X k+τ

i,j = xk)
i,j = xk)

P (X k+τ

i,j = 1, X k

i,j = 1) − P (X k+τ

i,j = 1)P (X k

i,j = 1) = ρi,j(τ );

P (X k+τ

i,j = 1, X k
i,j = 1) − P (X k+τ

i,j = 0) − P (X k+τ
i,j = 1, X k

i,j = 1)P (X k
i,j = 0)
i,j = 1) − P (X k+τ

= P (X k+τ

i,j = 1)[1 − P (X k

i,j = 1)]

= −ρi,j(τ );

P (X k+τ

i,j = 0, X k
i,j = 1) − P (X k+τ

i,j = 1) − P (X k+τ
i,j = 1, X k

i,j = 0)P (X k
i,j = 1) − [1 − P (X k+τ

i,j = 1)

= P (X k

i,j = 1)]P (X k

i,j = 1)

= −ρi,j(τ );

2

P (X k+τ

i,j = 0, X k
i,j = 0) − P (X k+τ

i,j = 0) − P (X k+τ
i,j = 0, X k

i,j = 0)P (X k
i,j = 0)
i,j = 1) − P (X k+τ

= P (X k+τ

i,j = 0)[1 − P (X k

i,j = 1)]

= ρi,j(τ ).

Consequently, we have

|P (A0, X k
(cid:12)
(cid:12)
P (A0, X k
(cid:12)
(cid:12)
(cid:12)
≤ ρi,j(τ ).

=

i,j = xk, B0, X k+τ

i,j = xk)P (B0, X k+τ

i,j = xk+τ ) − P (A0, X k
(cid:34) P (X k+τ
P (X k+τ

i,j = xk)P (B0, X k+τ
i,j = xk+τ , X k
i,j = xk+τ )P (X k

i,j = xk+τ )

i,j = xk+τ )|
(cid:35)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

i,j = xk)
i,j = xk)

− 1

In the case where A = A0 × {0, 1} and/or B = B0 × {0, 1}, since A and B are nonempty,
there exist integers 0 < k1 < k and/or k2 > k + 1, and correspondingly A1 ∈ F k1−1
and/or B ∈ F ∞

× {xk1}
k2+τ +1 × {xk2+τ } with xk1, xk2+τ = 0 or 1, such that P (A ∩ B) − P (A)P (B) =
P (A1 ∩ B1) − P (A1)P (B1). Following similar arguments above we have P (A ∩ B) − P (A)P (B) ≤
ρi,j(τ + k2 − k1) < ρij(τ ). We thus proved that αi,j(τ ) ≤ ρi,j(τ ). The conclusion of Proposition 3

0

follows from Proposition 1.

A.4 Proof of Proposition 4

We introduce some technical lemmas ﬁrst.

Lemma 1. For any (i, j) ∈ J , denote Y t

i,j)1≤i,j≤p be the
p × p matrix at time t. Under the assumptions of Proposition 1, we have {Yt, t = 1, 2 . . .} is

i,j ), and let Yt = (Y t

i,j := X t

i,j(1 − X t−1

stationary such that for any (i, j), (l, m) ∈ J , and t, s ≥ 1, t (cid:54)= s,

EY t

i,j =

αi,jβi,j
αi,j + βi,j

, Var(Y t

i,j) =

αi,jβi,j(αi,j + βi,j − αi,jβi,j)
(αi,j + βi,j)2

,

ρYi,j (|t − s|) ≡ Corr(Y t

i,j, Y s

lm) =




− αi,j βi,j (1−αi,j −βi,j )|t−s|−1
αi,j +βi,j −αi,j βi,j


0

if

(i, j) = (l, m),

otherwise.

E(Y t

i,j ) = (1 − X t−1

Proof. Note that Y t
i,j) = P (X t−1
i,j) = E(Y t
For k = 1 we have E(Y t

i,j(1 − X t−1
i,j = X t
i,j = 0)αi,j = (1 − EX t−1
i,j)] = αi,j βi,j
i,j)[1 − E(Y t
αi,j +βi,j
i,j ) = E[(1 − X t−1
i,jY t+1

i,j )αi,j = αi,j βi,j
.
αi,j +βi,j
(cid:17)
1 − αi,j βi,j
αi,j +βi,j
i,j )X t

i,j )I(εt

Var(Y t

(cid:16)

i,j(1 − X t

i,j = 1). We thus have:

= αi,j βi,j (αi,j +βi,j −αi,j βi,j )
(αi,j +βi,j )2
i,j)X t+1

i,j ] = 0. For any k ≥ 2, using

.

3

the fact that E(X t

ijX t+k
ij

) =

αij

(αij +βij )2 {βij(1 − αij − βij)k + αij}, we have

E(Y t

i,jY t+k
i,j

) = E[X t

i,j(1 − X t−1

i,j )(1 − X t+k−1

)I(εt+k

i,j = 1)]

i,j(1 − X t−1
= 0|X t

i,j
i,j )(1 − X t+k−1
i,j = 1)P (X t

i,j

)]

i,j = 1|X t−1

i,j = 0)P (X t−1

i,j = 0)

= αi,jE[X t

i,j

=

α2

= αi,jP (X t+k−1
i,jβi,j
αi,j + βi,j
i,jβi,j
αi,j + βi,j

α2

=

(cid:34)

1 −

α2

i,jβi,j
αi,j + βi,j
i,jβ2
α2

=

=

[1 − P (X t+k−1

i,j

= 1|X t

i,j = 1)]

(cid:35)

E(X t+k−1
i,j X t
EX t
i,j

i,j)

(cid:20)

1 −

βi,j(1 − αi,j − βi,j)k−1 + αi,j
αi,j + βi,j

(cid:21)

i,j[1 − (1 − αi,j − βi,j)k−1]

.

(αi,j + βi,j)2

Therefore we have for any k ≥ 1,

Cov(Y t

i,j, Y t+k
i,j

) = E(Y t
i,jβ2
α2

) − EY t

i,jY t+k
i,j
i,j[1 − (1 − αi,j − βi,j)k−1]

i,jEY t+k

i,j

=

= −

(αi,j + βi,j)2
i,j(1 − αi,j − βi,j)k−1

i,jβ2
α2

(αi,j + βi,j)2

.

−

i,jβ2
α2
i,j
(αi,j + βi,j)2

Consequently, for any |t − s| = 1, 2, . . ., the ACF of the process {Y t

i,j, t = 1, 2 . . .} is given as:

ρYi,j (|t − s|) = −

= −

i,jβ2
α2

i,j(1 − αi,j − βi,j)|t−s|−1

(αi,j + βi,j)2
αi,jβi,j(1 − αi,j − βi,j)|t−s|−1
αi,j + βi,j − αi,jβi,j

·

(αi,j + βi,j)2
αi,jβi,j(αi,j + βi,j − αi,jβi,j)

.

Since the mixing property is hereditary, Y t

i,j is also α-mixing. From Proposition 3 and Theorem

1 of Merlev`ede et al. (2009), we obtain the following concentration inequalities:

Lemma 2. Let conditions (2.5) and C1 hold. There exist positive constants C1 and C2 such that

for all n ≥ 4 and ε <

1
(log n)(log log n) ,

P

(cid:32)(cid:12)
(cid:12)
n−1
(cid:12)
(cid:12)
(cid:12)
(cid:32)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

P

n−1

t=1
n
(cid:88)

t=1

n
(cid:88)

X t

i,j − EX t
i,j

i,j − EY t
Y t
i,j

(cid:33)

> ε

≤ exp{−C1nε2},

(cid:33)

> ε

≤ exp{−C2nε2}.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Now we are ready to prove Proposition 4.

4

Proof of Proposition 4:
(cid:113) log p

Let ε = C
ε = o(cid:0)

n with C2C1 > 2 and C2C2 > 2. Note that under condition (C2) we have

(cid:1). Consequently by Lemma 2, Proposition 1 and Lemma 1, we have

1
(log n)(log log n)
(cid:32)(cid:12)
(cid:12)
n−1
(cid:12)
(cid:12)
(cid:12)

P

n
(cid:88)

t=1

X t

i,j −

αi,j
αi,j + βi,j

P

(cid:32)(cid:12)
(cid:12)
n−1
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

t=1

Y t
i,j −

αi,jβi,j
αi,j + βi,j

(cid:114)

(cid:33)

log p
n

> C

(cid:114)

(cid:33)

log p
n

> C

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ exp{−C2C1 log p},

≤ exp{−C2C2 log p}.

Consequently, with probability greater than 1 − exp{−C2C1 log p} − exp{−C2C2 log p},

αi,j βi,j
αi,j +βi,j

− C

βi,j
αi,j +βi,j

+ 1

n + C

(cid:113) log p
n
(cid:113) log p
n

≤ (cid:98)αi,j ≤

αi,j βi,j
αi,j +βi,j

+ C

βi,j
αi,j +βi,j

− 1

n − C

.

(cid:113) log p
n
(cid:113) log p
n
(cid:113) log p

Note that when n and n

log p are large enough such that, 1

n ≤ C

n ≤ l/4, we have

αi,j −

αi,j βi,j
αi,j +βi,j

− C

βi,j
αi,j +βi,j

+ 1

n + C

(cid:113) log p
n
(cid:113) log p
n

2Cαi,j

≤

(cid:113) log p
n

(cid:113) log p

n + C
βi,j
αi,j +βi,j

≤ 3l−1C

(cid:114)

log p
n

,

and

αi,j βi,j
αi,j +βi,j

+ C

(cid:113) log p
n
(cid:113) log p
n

2Cαi,j

− αi,j ≤

(cid:113) log p
n

(cid:113) log p

n + C
− l
2

βi,j
αi,j +βi,j

≤ 6l−1C

(cid:114)

log p
n

,

βi,j
αi,j +βi,j

− 1

n − C
Therefore we conclude that when when n and n

log p are large enough,

(cid:32)
|(cid:98)αi,j − αi,j| ≥ 6l−1C

(cid:114)

P

(cid:33)

log p
n

≤ exp{−C2C1 log p} + exp{−C2C2 log p}.

(A.3)

As a result, we have

(cid:32)

P

max
(i,j)∈J

|(cid:98)αi,j − αi,j| < 6l−1C

(cid:114)

(cid:33)

log p
n

≥ 1 − p2 exp{−C2C1 log p} − p2 exp{−C2C2 log p} → 1.

Consequently we have max(i,j)∈J |(cid:98)αi,j − αi,j| = Op
similarly.

(cid:18)(cid:113) log p

n

(cid:19)

. Convergence of (cid:98)βi,j can be proved

A.5 Proof of Proposition 5

Note that the log-likelihood function for (αi,j, βi,j) is:

l(αi,j, βi,j) = log(αi,j)

n
(cid:88)

X t

i,j(1 − X t−1

i,j ) + log(1 − αi,j)

n
(cid:88)

(1 − X t

i,j)(1 − X t−1
i,j )

t=1
n
(cid:88)

(1 − X t

i,j)X t−1

i,j + log(1 − βi,j)

t=1
n
(cid:88)

X t

i,jX t−1
i,j .

+ log(βi,j)

t=1

t=1

5

Our ﬁrst observation is that, owing to the independent edge formation assumption, all the

((cid:98)αi,j, (cid:98)βi,j), (i, j) ∈ J pairs are independent. For each pair (αi,j, βi,j), the score equations of
the log-likelihood function are:

∂l(αi,j, βi,j)
∂αi,j

∂l(αi,j, βi,j)
∂βi,j

=

=

=

=

=

1
αi,j
(cid:18) 1
αi,j

1
βi,j

1
βi,j
(cid:18) 1
βi,j

n
(cid:88)

t=1

X t

i,j(1 − X t−1

i,j ) −

1
1 − αi,j

n
(cid:88)

t=1

(1 − X t

i,j)(1 − X t−1

i,j ),

+

1
1 − αi,j

(cid:19) n
(cid:88)

t=1

Y t
i,j −

n
(cid:88)

(1 − X t

i,j)X t−1

i,j −

1
1 − βi,j

1
1 − αi,j
n
(cid:88)

n
(cid:88)

(1 − X t

i,j) + O(1),

t=1

X t

i,jX t−1
i,j

t=1
n
(cid:88)

t=1

X t−1

i,j +

+

1
1 − βi,j

(cid:18) 1
βi,j
(cid:19) n
(cid:88)

t=1

+

1
1 − βi,j

t=1
(cid:19) n
(cid:88)

t=1

(Y t

i,j − X t

i,j)

Y t
i,j −

1
1 − βi,j

n
(cid:88)

t=1

X t

i,j + O(1).

Clearly, for any 0 < αi,j, βi,j, αi,j + βi,j ≤ 1,

(cid:16) 1
αi,j
linearly independent. On the other hand, from Proposition 3, Lemma 2 and classical central limit
theorems for weakly dependent sequences (Bradley, 2007; Durrett, 2019), we have 1√
t=1 Y t
i,j
n
and 1√
i,j and any of their nontrivial linear combinations are asymptotically normally
n
, (i, j) ∈ J1 and

distributed. Consequently, any nontrivial linear combination of

, −1
1−βi,j

(cid:16) 1
βi,j

t=1 X t

1
1−αi,j

+ 1

+ 1

1−αi,j

1−βi,j

and

(cid:80)n

(cid:80)n

are

(cid:17)

(cid:17)

,

1√
n

∂l(αi,j ,βi,j )
∂αi,j

1√
n

∂l(αi,j ,βi,j )
∂βi,j

, (i, j) ∈ J2 converges to a normal distribution. By standard arguments for con-
√
n( (cid:98)βi,j − βi,j))(cid:48) converges to the normal
n((cid:98)αi,j − αi,j),
sistency of MLEs, we conclude that (
distribution with mean 0 and covariance matrix I(αi,j, βi,j)−1, where I(αi,j, βi,j) is the Fisher

√

information matrix given as:

I(αi,j, βi,j) =






1
n

E

Note that

(cid:80)n

t=1 X t

i,j (1−X t−1
i,j )
α2
i,j

+

(cid:80)n

t=1(1−X t

i,j )(1−X t−1
i,j )

(1−αi,j )2

0

0
i,j )X t−1

i,j

(cid:80)n

t=1(1−X t




 .

+

(cid:80)n

i,j X t−1

t=1 X t
(1−βi,j )2

i,j

β2
i,j

αi,jβi,j
αi,j + βi,j

,

1
n

1
n

1
n

E

E

E

n
(cid:88)

t=1
n
(cid:88)

t=1
n
(cid:88)

t=1

X t

i,j(1 − X t−1

i,j ) =

1
n

E

n
(cid:88)

t=1

(1 − X t

i,j)X t−1

i,j =

(1 − X t

i,j)(1 − X t−1

i,j ) =

βi,j
αi,j + βi,j

−

αi,jβi,j
αi,j + βi,j

=

(1 − αi,j)βi,j
αi,j + βi,j

,

X t

i,jX t−1

i,j =

αi,j(1 − βi,j)
αi,j + βi,j

.

6

We thus have

I(αi,j, βi,j) =

=









Consequently, we have

βi,j
αi,j (αi,j +βi,j ) +

βi,j
(αi,j +βi,j )(1−αi,j )
0

0

αi,j
(1−βi,j )(αi,j +βi,j )





αi,j
βi,j (αi,j +βi,j ) +


βi,j
αi,j (αi,j +βi,j )(1−αi,j )
0

0
αi,j
βi,j (αi,j +βi,j )(1−βi,j )

 .



√



√


n((cid:98)αi,j − αi,j)
n( (cid:98)βi,j − βi,j)

 → N

(cid:32)



0,



αi,j (αi,j +βi,j )(1−αi,j )
βi,j
0

0
βi,j (αi,j +βi,j )(1−βi,j )
αi,j





(cid:33)
.

This together with the independence among the ((cid:98)αi,j, (cid:98)βi,j), (i, j) ∈ J pairs proves the proposition.

A.6 Proof of Proposition 6

Denote N = diag{

√

s1, . . . ,

√

sq}. Note that

+ D−1/2
2
Z(cid:62) + Z (cid:101)D−1/2

1

ZΩ1Z(cid:62)D−1/2

1 Ω1 (cid:101)D−1/2

L = D−1/2
1
= Z (cid:101)D−1/2
1
= Z( (cid:101)Ω1 + (cid:101)Ω2)Z(cid:62)
= (ZN−1)N (cid:101)ΩN(ZN−1)(cid:62).

ZΩ2Z(cid:62)D−1/2

2

2 Ω2 (cid:101)D−1/2

2

Z(cid:62)

Note that the columns of ZN−1 are orthonormal, we thus have rank(L) = q. Let QΛQ(cid:62) = N (cid:101)ΩN
be the eigen-decomposition of N (cid:101)ΩN, we immediately have L = (ZN−1)QΛQ(cid:62)(ZN−1)(cid:62). Again,
since the columns of ZN−1 are orthonormal, we conclude that Γq = ZN−1Q, and U = N−1Q.

On the other hand, note that U is invertible, we conclude that zi,·U = zj,·U and zi,· = zj,· are

equivalent.

A.7 Proof of Theorem 1

The key step is to establish an upper bound for the Frobenius norm (cid:107)(cid:98)L(cid:98)L − LL(cid:107)F , and the

theorem can be proved by Weyl’s inequality and the Davis-Kahan theorem. We ﬁrst introducing

some technical lemmas.

Lemma 3. Under the assumptions of Proposition 1, we have, there exists a constant Cl > 0 such

that

Cov

(cid:32) n
(cid:88)

t=1

Y t
i,j,

n
(cid:88)

t=1

(cid:33)

(1 − X t−1
i,j )

= −Cov

(cid:32) n
(cid:88)

Y t
i,j,

n
(cid:88)

t=1

(cid:33)

X t−1
i,j

=

nαi,jβi,j[2αi,j(1 − βi,j) + αi,j + βi,j − 2β2
(αi,j + βi,j)3

t=1
i,j]

+ Ci,j,

with |Ci,j| ≤ Cl for any Ci,j, (i, j) ∈ J .

7

Proof. In the following we shall be using the fact that for any 0 ≤ x < 1, (cid:80)n−1
1−x + o(1), and (cid:80)n−1
1
under condition C1, we have 2l ≤ 1 − x < 1, the O(1) term in will become bounded uniformly

1−x =
= O(1). In particular, when x = 1 − αi,j − βi,j,

h=1 hxh−1 = 1−xn−n(1−x)xn−1

h=1 xh−1 = 1−xn

(1−x)2

for any (i, j) ∈ J . In what follows, with some abuse of notation, we shall use Ol(1) to denote a

generic constant term with magnitude bounded by a large enough constant Cl that depends on l

only.

(cid:32) n
(cid:88)

Cov

t=1
n
(cid:88)

n
(cid:88)

t=1
n
(cid:88)

s=1
n
(cid:88)

(cid:26)

t=1
n
(cid:88)

s=1
n
(cid:88)

t=1
n
(cid:88)

s=1
n
(cid:88)

= −

= −

+

= −

Y t
i,j,

n
(cid:88)

t=1

(cid:33)

(1 − X t−1
i,j )

= −Cov

(cid:32) n
(cid:88)

n
(cid:88)

Y t
i,j,

X t−1
i,j

(cid:33)

(cid:20)
E(1 − X t−1

i,j )X t

i,jX s−1

i,j −

t=1
αi,jβi,j
αi,j + βi,j

t=1
αi,j
αi,j + βi,j

·

(cid:21)

αi,j
(αi,j + βi,j)2

(cid:104)

βi,j(1 − αi,j − βi,j)|t−s+1| + αi,j

(cid:105)

−

α2

i,jβi,j
(αi,j + βi,j)2

(cid:27)

E(X t−1

i,j X t

i,jX s−1
i,j )

αi,jβi,j(1 − αi,j − βi,j)|t−s+1|
(αi,j + βi,j)2
(cid:88)

−

n2α2
i,j(1 − βi,j)
(αi,j + βi,j)2
(cid:88)

i,j X t

i,j) +

E(X t−1

i,j X t

i,jX s−1

i,j ) +

t=1

s=1
+(2n − 1)E(X t−1

E(X t−1

i,j X t

i,jX s−1

i,j ).

(A.4)

For the ﬁrst three terms on the right hand side of (A.4), we have

s<t

s>t+1

n
(cid:88)

n
(cid:88)

−

αi,jβi,j(1 − αi,j − βi,j)|t−s+1|
(αi,j + βi,j)2

t=1

s=1
αi,jβi,j
(αi,j + βi,j)2
(cid:104)

(cid:20)

n +

2n(1 − αi,j − βi,j)
αi,j + βi,j
(cid:105)

2nαi,j

βi,j(1 − αi,j − βi,j) + αi,j

= −

+

(αi,j + βi,j)2

+ Ol(1)

−

(cid:21)

i,j(1 − βi,j)

n2α2
(αi,j + βi,j)2 + (2n − 1)E(X t−1
n2α2
i,j(1 − βi,j)
(αi,j + βi,j)2

−

i,j X t

i,j)

=

3nαi,jβi,j
(αi,j + βi,j)2 −

2nαi,jβi,j
(αi,j + βi,j)3 −

2nαi,jβi,j
αi,j + βi,j

+

2nα2
i,j
(αi,j + βi,j)2 −

i,j(1 − βi,j)

n2α2
(αi,j + βi,j)2 + Ol(1).

8

For the last two terms on the right hand side of (A.4), we have

(cid:88)

s<t
(cid:88)

s<t

=

E(X t−1

i,j X t

i,jX s−1

i,j ) +

(cid:88)

s>t+1

E(X t−1

i,j X t

i,jX s−1
i,j )

P (X t

i,j = 1|X t−1

i,j = 1)P (X t−1

i,j = 1, X s−1

i,j = 1)

(cid:88)

+

s>t+1

P (X s−1

i,j = 1|X t

i,j = 1)P (X t

i,j = 1, X t−1

i,j = 1)

= (1 − βi,j)

(cid:88)

s<t

=

(1 − βi,j)αi,j
(αi,j + βi,j)2

E(X t−1

i,j X s−1

i,j ) + (1 − βi,j)

(cid:88)

s>t+1

E(X s−1

i,j X t

i,j)

n−1
(cid:88)

(n − h)[βi,j(1 − αi,j − βi,j)h + αi,j]

h=1

+

(1 − βi,j)αi,j
(αi,j + βi,j)2

n−1
(cid:88)

(n − h)[βi,j(1 − αi,j − βi,j)h−1 + αi,j]

h=2

=

(n − 1)2α2

i,j(1 − βi,j)

(αi,j + βi,j)2

+

2n(1 − βi,j)αi,jβi,j
(αi,j + βi,j)3

+ Ol(1).

Consequently, we have

Cov

(cid:32) n
(cid:88)

t=1

Y t
i,j,

n
(cid:88)

t=1

(cid:33)

(1 − X t−1
i,j )

= −Cov

(cid:32) n
(cid:88)

n
(cid:88)

Y t
i,j,

X t−1
i,j

(cid:33)

2nαi,jβi,j
(αi,j + βi,j)3 −

2nαi,jβi,j
αi,j + βi,j

+

t=1

t=1
2nα2
i,j
(αi,j + βi,j)2 −

n2α2
i,j(1 − βi,j)
(αi,j + βi,j)2

=

=

3nαi,jβi,j
(αi,j + βi,j)2 −
(n − 1)2α2

+

i,j(1 − βi,j)

+

(αi,j + βi,j)2

3nαi,jβi,j
(αi,j + βi,j)2 −
2n(1 − βi,j)αi,jβi,j
(αi,j + βi,j)3

+

+ Ol(1)

2n(1 − βi,j)αi,jβi,j
(αi,j + βi,j)3
2nαi,jβi,j
αi,j + βi,j

+

+ Ol(1)

2nα2

i,jβi,j

(αi,j + βi,j)2

2nαi,jβi,j
(αi,j + βi,j)3 −

=

3nαi,jβi,j
(αi,j + βi,j)2 −

2nαi,jβ2

i,j(1 + αi,j + βi,j)
(αi,j + βi,j)3

+ Ol(1).

This proves the lemma.

Lemma 4. (Bias of (cid:98)αi,j and (cid:98)βi,j) Ket conditions C1, C2 and the assumptions of Proposition
1 hold. We have

E (cid:98)αi,j − αi,j = −

αi,j[2αi,j(1 − βi,j) + αi,j + βi,j − 2β2
n(αi,j + βi,j)βi,j

i,j]

+

R(1)
i,j
n

+ O(n−2),

E (cid:98)βi,j − βi,j =

βi,j[2αi,j(1 − βi,j) + αi,j + βi,j − 2β2
n(αi,j + βi,j)αi,j

i,j]

+

R(2)
i,j
n

+ O(n−2),

where R(1)

i,j and R(2)

i,j are constants such that when n is large enough we have 0 ≤ R(1)

i,j , R(2)

i,j ≤ Rl

for some constant Rl and all (i, j) ∈ J .

9

Proof. From Lemma 2 we have, under Condition C2, the event {|n−1 (cid:80)n
πi,j)/2, 1 ≤ i, j ≤ p} holds with probability larger than 1−O(n−2). Denote I := I(|n−1 (cid:80)n
πi,j| ≤ (1 − πi,j)/2, 1 ≤ i, j ≤ p). By expanding

i,j − πi,j| ≤ (1 −
t=1 X t−1
i,j −

t=1 X t−1

, we have

around

1
1−n−1 (cid:80)n
t=1 X t−1

i,j

1
1−πi,j

E (cid:98)αi,jI

n−1 (cid:80)n

= E

=

1
n

E

t=1

t=1 X t

i,j(1 − X t−1
i,j )
t=1(1 − X t−1
i,j )
(cid:34)

I

n−1 (cid:80)n
n
(cid:88)

X t

i,j(1 − X t−1
i,j )

1
1 − πi,j

+

(n−1 (cid:80)n

i,j − πi,j)

t=1 X t−1
(1 − πi,j)2

+

∞
(cid:88)

k=2

(n−1 (cid:80)n

i,j − πi,j)k

t=1 X t−1
(1 − πi,j)k+1

(cid:35)

I.

:= E (cid:80)n

Write R(1)
i,j
grange remainder we have there exist random scalars rt

i,j(1 − X t−1
i,j )

t=1 X t

(cid:16) (cid:80)∞
k=2

(n−1 (cid:80)n

i,j −πi,j )k

t=1 X t−1
(1−πi,j )k+1

(cid:17)

i,j ∈ [n−1 (cid:80)n

I. By Taylor series with La-
t=1 X t−1

i,j , πi,j] such that

R(1)

i,j = E

n
(cid:88)

t=1

On the other hand, note that

X t

i,j(1 − X t−1
i,j )

(cid:32) (n−1 (cid:80)n

t=1 X t−1
i,j − πi,j)2
(1 − rt
i,j)3

(cid:33)

I > 0.

∞
(cid:88)

k=2

|n−1 (cid:80)n

i,j − πi,j|k

t=1 X t−1
(1 − πi,j)k+1

(cid:32)

I ≤

n−1

(cid:32)

=

n−1

X t−1

i,j − πi,j

X t−1

i,j − πi,j

(cid:33)2 ∞
(cid:88)

k=0

1
(1 − πi,j)32k

(cid:33)2

2
(1 − πi,j)3 .

n
(cid:88)

t=1

n
(cid:88)

t=1

Therefore,

R(1)

i,j ≤ E

n
(cid:88)

(cid:16) ∞
(cid:88)

|n−1 (cid:80)n

i,j − πi,j|k

t=1 X t−1
(1 − πi,j)k+1

t=1
(cid:32)

≤ V ar

k=2

1
√
n

n
(cid:88)

t=1

(cid:33)

X t−1
ij

2
(1 − πi,j)3

2

(1 − πi,j)3 V ar(X t
ij)

(cid:34)

1 +

2
n

(cid:17)

I

(cid:35)

(n − h)ρij(h)

n−1
(cid:88)

h=1

(cid:34)

1 +

2
n

(cid:20)

1 +

αijβij
(αij + βij)2

αijβij
(αij + βij)2

2
(1 − πi,j)3 ·
2
(1 − πi,j)3 ·
2
(1 − πi,j)4πi,j

·

2 − αij − βij
αij + βij

+ O(n−1).

n−1
(cid:88)

(n − h)(1 − αij − βij)h

(cid:21)

h=1

2(1 − αij − βij)
αij + βij

(cid:21)

+ O(n−1)

=

=

=

=

Again, since 0 < l ≤ αi,j, βi,j, αi,j + βi,j ≤ 1 holds for all (i, j) ∈ J , we conclude that there exists

10

a constant Rl such that R(1)

i,j ≤ Rl. Together with Lemma 3, we have

E (cid:98)αi,j = E (cid:98)αi,jI + E (cid:98)αi,j(1 − I)
n
(cid:88)

X t

i,j(1 − X t−1
i,j )

= E

1
n

t=1

(cid:34)

1
1 − πi,j

+

(n−1 (cid:80)n

i,j − πi,j)

t=1 X t−1
(1 − πi,j)2

(cid:35)

I +

R(1)
i,j
n

+ E (cid:98)αi,j(1 − I)

= αi,j +

Cov((cid:80)n

i,j, (cid:80)n
t=1 Y t
n2(1 − πi,j)2

t=1 X t

i,j)

R(1)
i,j
n

+

+ O(n−2)

= αi,j −

αi,j[2αi,j(1 − βi,j) + αi,j + βi,j − 2β2
n(αi,j + βi,j)βi,j

i,j]

+

R(1)
i,j
n

+ O(n−2).

Similarly, write R(2)
i,j
i,j)X t−1

t=1(1 − X t

:= E (cid:80)n

t=1 X t
i,j | ≤ πi,j/2}. We have,

i,j(1 − X t−1
i,j )

I{|n−1 (cid:80)n

(cid:16) (cid:80)∞
k=2

E (cid:98)βi,j = E

n−1 (cid:80)n

t=1(1 − X t

i,j)X t−1
i,j

n−1 (cid:80)n

t=1 X t−1
i,j

(n−1 (cid:80)n

i,j −πi,j )k

t=1 X t−1
(−1)kπk+1

i,j

(cid:17)

I (cid:48) where I (cid:48) :=

−

(cid:104) 1
πi,j
i,j − πi,j)k

(n−1 (cid:80)n

i,j − πi,j)

t=1 X t−1
π2
i,j

(cid:105)

I (cid:48) + E (cid:98)βi,j(1 − I (cid:48))

= E

1
n

n
(cid:88)

t=1

+

∞
(cid:88)

k=2

= βi,j −

(1 − X t

i,j)X t−1
i,j

(n−1 (cid:80)n

Cov((cid:80)n

t=1 X t−1
(−1)kπk+1
i,j, (cid:80)n

t=1 Y t

i,j

i,j − X n

i,j + X 0

i,j)

t=1 X t
n2π2
i,j

= βi,j +

βi,j[2αi,j(1 − βi,j) + αi,j + βi,j − 2β2
n(αi,j + βi,j)αi,j

i,j]

+

+

R(2)
i,j
n
R(2)
i,j
n

+ O(n−2)

+ O(n−2).

Here in the second last step we have used the fact that En−1(X 0

i,j − X n

i,j)(n−1 (cid:80)n

t=1 X t−1

i,j − πi,j) =

O(n−2), and in the last step we have used the fact that

n
(cid:88)

n−2E

X t

i,j(1 − X t−1

i,j )(X n

i,j − X 0

i,j)

= n−2E

t=1
(cid:34) n
(cid:88)

t=1

X t−1

i,j X t

i,jX 0

i,j −

(cid:35)

X t−1

i,j X t

i,jX n
i,j

n
(cid:88)

t=1

+ n−2[E(X n

i,j)2 − E(X n

i,jX 0

i,j)]

= n−2(cid:104) n

(cid:88)

t=1

P (X t

i,j = 1|X t−1

i,j = 1)P (X t−1

i,j = 1|X 0

i,j = 1)P (X 0

i,j = 1)

−

n
(cid:88)

t=1

P (X n

i,j = 1|X t

i,j = 1)P (X t

i,j = 1|X t−1

i,j = 1)P (X t−1

(cid:105)
i,j = 1)

+ O(n−2)

= O(n−2)

On one hand, similar to R(1)
that R(2)

i,j ≤ Rl for any (i, j) ∈ J .

i,j , we can show that when n is large enough, there exists a Rl such

11

Lemma 4 implies that the bias of the MLEs is of order O(n−1). The bound Rl here also

implies that the O(n−1) order of the bias holds uniformly for all (i, j) ∈ J .

Lemma 5. Let conditions (2.5), C1 and C2 hold. For any constant B > 0, there exists a large

enough constant C > 0 such that

(A.5)

(cid:40)

P

(cid:107)(cid:98)L1 (cid:98)L1 − L1L1(cid:107)F ≥ C

(cid:40)

P

(cid:107)(cid:98)L2 (cid:98)L2 − L2L2(cid:107)F ≥ C

(cid:40)

P

(cid:107)(cid:98)L1 (cid:98)L2 − L1L2(cid:107)F ≥ C

(cid:40)

P

(cid:107)(cid:98)L2 (cid:98)L1 − L2L1(cid:107)F ≥ C

(cid:32)(cid:115)

(cid:32)(cid:115)

(cid:32)(cid:115)

(cid:33)(cid:41)

(cid:33)(cid:41)

(cid:33)(cid:41)

log(pn)
np

+

log(pn)
np

+

log(pn)
np

+

1
n

1
n

1
n

+

+

+

1
p

1
p

1
p

(cid:32)(cid:115)

(cid:33)(cid:41)

log(pn)
np

+

1
n

+

1
p

≤ 8p

(cid:104)
(pn)−(1+B) + exp{−B

√

(cid:105)
p}

,

≤ 8p

(cid:104)
(pn)−(1+B) + exp{−B

√

(cid:105)
p}

,

≤ 8p

(cid:104)
(pn)−(1+B) + exp{−B

√

(cid:105)
p}

,

≤ 8p

(cid:104)
(pn)−(1+B) + exp{−B

√

(cid:105)
p}

.

Proof. We only prove the ﬁrst inequality in (A.5) here as the other three inequalities can be

proved similarly. Denote

(cid:101)L1 := L1 − diag(L1) = D−1/2

1

[W1 − diag(W)1] D−1/2

1

,

and for any 1 ≤ i, j ≤ p we denote the (i, j)th element of (cid:101)L1 (cid:101)L1 −L1L1 as δi,j. Correspondingly, for

any (cid:96) = 1, . . . , p, we deﬁne (cid:101)d(cid:96),1 := d(cid:96),1 − α(cid:96),(cid:96). We ﬁrst evaluate the error introduced by removing
the diag(L1) term. With some abuse of notation, let (cid:101)αi,j = αi,j for 1 ≤ i (cid:54)= j ≤ p and (cid:101)αi,i = 0 for
i = 1, . . . , p. We have W − diag(W) = ((cid:101)αi,j)1≤i,j≤p. Therefore,
p
(cid:88)

p
(cid:88)

αi,kαk,j
(cid:112)di,1dj,1

dk,1

≤

αi,iαi,j
(cid:112)di,1dj,1

di,1

+

αi,jαj,j
(cid:112)di,1dj,1

dj,1

≤

2
(p − 1)2l2 .

(cid:101)αi,k (cid:101)αk,j
(cid:112)di,1dj,1

dk,1

−

|δi,j| =

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

k=1

k=1
Consequently, we have

(cid:107)(cid:98)L1 (cid:98)L1 − L1L1(cid:107)2

F = (cid:107)((cid:98)L1 (cid:98)L1 − (cid:101)L1 (cid:101)L1) + ((cid:101)L1 (cid:101)L1 − L1L1)(cid:107)2
F

≤ 2

(cid:104)
(cid:107)(cid:98)L1 (cid:98)L1 − (cid:101)L1 (cid:101)L1(cid:107)2
= 2(cid:107)(cid:98)L1 (cid:98)L1 − (cid:101)L1 (cid:101)L1(cid:107)2

F + (cid:107)(cid:101)L1 (cid:101)L1 − L1L1(cid:107)2
F

(cid:105)

F + 2

(cid:88)

δ2
i,j

≤ 2(cid:107)(cid:98)L1 (cid:98)L1 − (cid:101)L1 (cid:101)L1(cid:107)2

F +

1≤i,j≤p
8p2
(p − 1)4l4 .

(A.6)

Next, we derive the asymptotic bound for (cid:107)(cid:98)L1 (cid:98)L1 − (cid:101)L1 (cid:101)L1(cid:107)2
F .

For any 1 ≤ i, j ≤ p, we denote the (i, j)th element of (cid:98)L1 (cid:98)L1 − (cid:101)L1 (cid:101)L1 as ∆i,j. By deﬁnition we

have,

∆i,j =


 (cid:98)αi,k (cid:98)αk,j
(cid:113)
(cid:98)dk,1

(cid:98)di,1 (cid:98)dj,1

(cid:88)

1≤k≤p
k(cid:54)=i,j

−

αi,kαk,j
(cid:112)di,1dj,1

dk,1



 ,

12

k=1 (cid:98)α(cid:96),k and d(cid:96),1 = (cid:80)p
i,k := V ar((cid:98)αi,k), and τ 2

where (cid:98)d(cid:96),1 = (cid:80)p
k=1 α(cid:96),k for l = 1, . . . , p. Note that (cid:98)αi,1, . . . , (cid:98)αi,p are indepen-
k=1 σ2
dent. Denote σ2
i,k. Similar to the proofs of Lemma 3 we can
show that, when n is large enough, their exists a constant Cσ > (2l)−1 and cσ := l(1 − l) such that
cσn−1 ≤ σ2
i (cid:39) O(n−1p). On the other hand, from
Lemma 4 we know that there exists a large enough constant Cα > 0 such that |E (cid:98)αi,j − αi,j| ≤ Cα
n
for all (i, j) ∈ J , and consequently, |E (cid:98)d(cid:96),1−d(cid:96),1|
p for any l = 1, . . . , p.

i,k ≤ Cσn−1 for any (i, j) ∈ J . Consequently, τ 2

≤ |E (cid:98)d(cid:96),1− (cid:101)d(cid:96),1|
p

p < Cα

n + 1

:= (cid:80)p

+ 1

i

p
We next break our proofs into three steps:
Step 1. Concentration of p−1 (cid:98)d(cid:96),1.

Note that |(cid:98)α(cid:96),j| ≤ 1. By Bernstein’s inequality (Bennett, 1962; Lin and Bai, 2011) we have,

for any constant Cd > 0:

(cid:32)

(cid:32)

(cid:32)

P

≤ P

≤ P

| (cid:98)d(cid:96),1 − d(cid:96),1|
p

≥ Cd

(cid:115)

log(pn)
np

+

Cα
n

+

1
p

(cid:33)

| (cid:98)d(cid:96),1 − E( (cid:98)d(cid:96),1)|
p

≥ Cd

| (cid:98)d(cid:96),1 − E( (cid:98)d(cid:96),1)|
p

≥ Cd

(cid:115)

(cid:115)

+

(cid:33)

log(pn)
np

log(pn)
np

Cα
n

+

1
p

−

|E( (cid:98)d(cid:96),1) − d(cid:96),1|
p

(cid:33)

(cid:40)

≤ 2 exp

−

(cid:40)

= 2 exp

−

√

2(

√

2(

(cid:41)

√

d n−1 log(pn)

pC2
pCσ/n + aCd

(cid:112)log(pn)/n)
pC2

√

d n−1 log(pn)
pCσ/n + Cde(6l−1 + Cα)(cid:112)log n/(C3n)(cid:112)log(pn)/n)

(cid:41)

.

(A.7)

√

pCσ/n > Cde(6l−1 + Cα)(cid:112)log n/(C3n)(cid:112)log(pn)/n), for any constant B > 0, by choosing

When
Cd > 2(cid:112)(B + 1)Cσ, (A.7) reduces to

(cid:32)

P

| (cid:98)d(cid:96),1 − d(cid:96),1|
p
√

(cid:26)

(cid:115)

≥ Cd

log(pn)
np
(cid:27)

d n−1 log(pn)
pC2
√
pCσ/n
4

≤ 2 exp

−

(cid:33)

+

Cα
n

+

1
p

< 2(pn)−(B+1).

(A.8)

When
√

Cα)/

√

pCσ/n ≤ Cde(6l−1 + Cα)(cid:112)log n/(C3n)(cid:112)log(pn)/n, by choosing Cd = 4Be(6l−1 +

C3, (A.7) reduces to

(cid:32)

P

| (cid:98)d(cid:96),1 − d(cid:96),1|
p

≥ Cd

(cid:115)

log(pn)
np

+

Cα
n

+

1
p

(cid:33)

(cid:40)

≤ 2 exp

−

√

pC2

d n−1 log(pn)

4Cde(6l−1 + Cα)(cid:112)log n/(C3n)(cid:112)log(pn)/n
√

≤ 2 exp {−B

p} .

(cid:41)

(A.9)

From (A.7), (A.8) and (A.9) we conclude that for any B > 0, by choosing Cd to be large enough,

13

we have,

(cid:32)

P

max
l=1,...,p

| (cid:98)d(cid:96),1 − d(cid:96),1|
p

≥ Cd

(cid:115)

log(pn)
np

+

Cα
n

+

1
p

(cid:33)

(cid:104)

≤ 2p

(pn)−(1+B) + exp{−B

√

(cid:105)

.

p}

(A.10)

Step 2. Concentration of ∆i,j.

Using the fact that (cid:98)αk,k = 0 for k = 1, . . . , p, we have,

∆i,j =

p
(cid:88)

k=1


 (cid:98)αi,k (cid:98)αk,j
(cid:113)
(cid:98)dk,1

(cid:98)di,1 (cid:98)dj,1

− (cid:98)αi,k (cid:98)αk,j
dk,1

(cid:112)di,1dj,1



 +

(cid:32)

(cid:88)

1≤k≤p
k(cid:54)=i,j

(cid:98)αi,k (cid:98)αk,j
(cid:112)di,1dj,1

dk,1

−

αi,kαk,j
(cid:112)di,1dj,1

dk,1

(cid:33)

.

We next bound the two terms on the right hand side of the above inequality. For the ﬁrst term,

denote ek := ( (cid:98)dk,1 − dk,1)/p. From (A.10) we have there exists a large enough constant CB such

that

(cid:40)

P

max
k=1,...,p

|ek| ≤ CB

(cid:26)

(cid:32)(cid:115)

(cid:33)(cid:41)

log(pn)
np

+

1
n

+

1
p

≥ 1 − 2p

(cid:104)
(pn)−(1+B) + exp{−B

√

(cid:105)
p}

.

maxk=1,...,p |ek| ≤ CB

Denote the event
n and p are large enough, (cid:112)p−1dk,1 + ek = (cid:112)p−1dk,1 + ek/(2(cid:112)p−1dk,1) + O(e2
exists a large enough constant Cl,B > 0 such that for any 1 ≤ i, j ≤ p,

np + 1

n + 1

p

(cid:18)(cid:113) log(pn)

(cid:19)(cid:27)

as EB. Under EB, we have, when

k), and hence there

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

− (cid:98)αi,k (cid:98)αk,j
dk,1

(cid:112)di,1dj,1

(cid:98)αi,k (cid:98)αk,j
(cid:113)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:98)dk,1
(cid:12)
(cid:12)
(cid:12)p−1dk,1
(cid:12)
p2(p−1dk,1 + ek)
= O(p−2(|ei| + |ej| + |ek|))

(cid:113)

≤

(cid:98)di,1 (cid:98)dj,1
(cid:12)
(cid:112)p−1di,1p−1dj,1 − (p−1dk,1 + ek)(cid:112)(p−1di,1 + ei)(p−1dj,1 + ej)
(cid:12)
(cid:12)
(cid:112)p−1di,1p−1dj,1

(p−1di,1 + ei)(p−1dj,1 + ej)p−1dk,1

≤

Cl,B
p2

(cid:32)(cid:115)

log(pn)
np

+

1
n

+

(cid:33)

.

1
p

Consequently, we have, under EB,

 (cid:98)αi,k (cid:98)αk,j
(cid:113)
(cid:98)dk,1

(cid:12)
p
(cid:12)
(cid:88)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:98)di,1 (cid:98)dj,1

k=1

− (cid:98)αi,k (cid:98)αk,j
dk,1

(cid:112)di,1dj,1





(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

Cl,B
p

(cid:32)(cid:115)

log(pn)
np

+

1
n

+

(cid:33)

.

1
p

(A.11)

For the second term, note that for any 1 ≤ i, j ≤ p and k (cid:54)= i, j,

|E (cid:98)αi,k (cid:98)αk,j − αi,kαk,j|

= |E((cid:98)αi,k − αi,k)((cid:98)αk,j − αk,j) + E((cid:98)αi,k − αi,k)αk,j + Eαi,k((cid:98)αk,j − αk,j)|
2Cα
≤ |E((cid:98)αi,k − αi,k)((cid:98)αk,j − αk,j)| +
n

.

(A.12)

14

(A.13)

(cid:33)(cid:41)

When i (cid:54)= j, by Lemma 4 and the fact that (cid:98)αi,k and (cid:98)αk,j are independent (since k (cid:54)= i, j), we
have |E (cid:98)αi,k (cid:98)αk,j − αi,kαk,j| ≤ Cl,1n−1 for some large enough constant Cl,1 > 0. Using the same
arguments for obtaining (A.10), we have, there exists a large enough constant Dl,B > 0 such that

when n and p are large enough,







P

max
1≤i(cid:54)=j≤p

(cid:32)

(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1≤k≤p
(cid:12)
k(cid:54)=i,j

(cid:98)αi,k (cid:98)αk,j
(cid:112)di,1dj,1

dk,1

−

αi,kαk,j
(cid:112)di,1dj,1

dk,1

(cid:32)(cid:115)

≥

Dl,B
p

log(pn)
np

+

1
n

+

1
p

(cid:33)







(cid:33)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ 2p

(cid:104)
(pn)−(1+B) + exp{−B

√

(cid:105)
p}

.

Denote the event

(cid:40)

max
1≤i(cid:54)=j≤p

(cid:18)

(cid:12)
(cid:12)
(cid:80)
(cid:12)
(cid:12)

1≤k≤p
k(cid:54)=i,j

(cid:98)αi,k (cid:98)αk,j
√

− αi,kαk,j
√
dk,1

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ Dl,B
p

(cid:32)

(cid:113) log(pn)

np + 1

n + 1

p

dk,1
as AB. From (A.11) and (A.13) we conclude that, when n and p are large enough,

di,1dj,1

di,1dj,1

Cl,B + Dl,B
p

(cid:32)(cid:115)

(cid:33)(cid:41)

log(pn)
np

+

1
n

+

1
p

(cid:40)

P

|∆i,j| >

max
1≤i(cid:54)=j≤p
B) + P (Ac
≤ P (E c
(cid:104)
(pn)−(1+B) + exp{−B

< 4p

B)

√

(cid:105)
p}

.

(A.14)

When i = j, by applying Lemma 4 and (A.3) to (A.12), we have, there exists a large enough

constant Cl,2 > 0, such that

|E (cid:98)αi,k (cid:98)αk,i − αi,kαk,i| ≤ Cl,2

(cid:18) log(pn)
n

+

1
n

+

(cid:19)

.

1
p

Consequently, similar to (A.14), we have, there exists a large enough constant Cl,3 > 0, such that

(cid:40)

P

max
1≤i≤p

|∆i,i| >

(cid:32)(cid:115)

Cl,3
p

log(pn)
np

+

log(pn)
n

+

1
p

(cid:33)(cid:41)

(cid:104)

< 4p

(pn)−(1+B) + exp{−B

(cid:105)

√

p}

.(A.15)

Step 3. Proof of the ﬁrst inequality in (A.5).
1≤i,j≤p ∆2

Note that (cid:107)(cid:98)L1 (cid:98)L1 − (cid:101)L1 (cid:101)L1(cid:107)F =

(cid:113)(cid:80)

From (A.6), (A.14), (A.15) and the fact that 1√
p

np + log(pn)

n + 1

p

= o

i,j ≤ p max1≤i(cid:54)=j≤p |∆i,j| +

(cid:18)(cid:113) log(pn)

(cid:19)

√

p max1≤i≤p |∆i,i|.
(cid:19)

(cid:18)(cid:113) log(pn)

np + 1

n + 1

p

we immediately have that there exists a large enough constant C > 0 such that when n and p are

large enough,

(cid:40)

P

(cid:107)(cid:98)L1 (cid:98)L1 − L1L1(cid:107)F ≥ C

(cid:32)(cid:115)

(cid:33)(cid:41)

log(pn)
np

+

1
n

+

1
p

≤ 8p

(cid:104)
(pn)−(1+B) + exp{−B

√

(cid:105)
p}

.

This proves the ﬁrst inequality in (A.5).

15

Lemma 6. Let conditions (2.5), C1 and C2 hold. For any constant B > 0, there exists a large

enough constant C > 0 such that

(cid:40)

P

(cid:107)(cid:98)L(cid:98)L − LL(cid:107)F ≥ 4C

(cid:32)(cid:115)

(cid:33)(cid:41)

log(pn)
np

+

1
n

+

1
p

≤ 16p

(cid:104)
(pn)−(1+B) + exp{−B

√

(cid:105)
p}

.

(A.16)

Proof. Note that from the triangle inequality we have

(cid:107)(cid:98)L(cid:98)L − LL(cid:107)F

= (cid:107)((cid:98)L1 + (cid:98)L2)((cid:98)L1 + (cid:98)L2) − (L1 + L2)(L1 + L2)(cid:107)F

= (cid:107)((cid:98)L1 (cid:98)L1 − L1L1) + ((cid:98)L1 (cid:98)L2 − L1L2) + ((cid:98)L2 (cid:98)L1 − L2L1) + ((cid:98)L2 (cid:98)L2 − L2L2)(cid:107)F

≤ (cid:107)(cid:98)L1 (cid:98)L1 − L1L1(cid:107)F + (cid:107)(cid:98)L1 (cid:98)L2 − L1L2(cid:107)F + (cid:107)(cid:98)L2 (cid:98)L1 − L2L1(cid:107)F + (cid:107)(cid:98)L2 (cid:98)L2 − L2L2(cid:107)F .

Together with Lemma 5 we immediately conclude that (A.16) hold.

Proof of Theorem 1

From Weyl’s inequality and Lemma 6, we have,

max
i=1,...,p

|λ2

i − (cid:98)λ2

i | ≤ (cid:107)(cid:98)L(cid:98)L − LL(cid:107)2 ≤ (cid:107)(cid:98)L(cid:98)L − LL(cid:107)F = Op

(cid:32)(cid:115)

log(pn)
np

+

1
n

+

(cid:33)

.

1
p

(3.8) is a direct result of the Davis-Kahan theorem (Rohe et al., 2011; Yu et al., 2015) theorem

and Lemma 6.

A.8 Proof of Theorem 2

Recall that Γq = ZU where U is deﬁned as in the proof of Proposition 6. For any 1 ≤ i (cid:54)= j ≤ n

such that zi (cid:54)= zj, we need to show that (cid:107)ziUOq − zjUqOq(cid:107)2 = (cid:107)ziU − zjU(cid:107)2 is large enough,

so that the perturbed version (i.e. the rows of (cid:98)Γq) is not changing the clustering structure.

Denote the ith row of ΓqOq and (cid:98)Γq as γi and (cid:98)γi, respectively, for i = 1, . . . , p. Notice that
q }.

from the proof of Proposition 6, we have UU(cid:62) = N−1QQ(cid:62)N−1 = N−2 = diag{s−1

1 , . . . , s−1

Consequently, for any zi (cid:54)= zj, we have:

(cid:107)γi − γj(cid:107)2 = (cid:107)ziUOq − zjUqOq(cid:107)2 = (cid:107)ziU − zjU(cid:107)2 ≥

(cid:114) 2
smax

.

(A.17)

We ﬁrst show that zi

(cid:54)= zj implies (cid:98)ci

(cid:54)= (cid:98)cj. Notice that ΓqOq ∈ Mp,q. Denote (cid:98)C =

((cid:98)c1, · · · , (cid:98)cp)(cid:62). By the deﬁnition of (cid:98)C we have

(cid:107)ΓqOq − (cid:98)C(cid:107)2

F ≤ (cid:107)(cid:98)Γq − (cid:98)C(cid:107)2

F + (cid:107)(cid:98)Γq − ΓqOq(cid:107)2

F ≤ 2(cid:107)(cid:98)Γq − ΓqOq(cid:107)2
F .

(A.18)

16

Suppose there exist i, j ∈ {1, . . . , p} such that zi (cid:54)= zj but (cid:98)ci = (cid:98)cj. We have

(cid:107)ΓqOq − (cid:98)C(cid:107)2

F ≥ (cid:107)ziUOq − (cid:98)ci(cid:107)2

2 + (cid:107)zjUOq − (cid:98)cj(cid:107)2

2 ≥ (cid:107)ziUOq − zjUOq(cid:107)2
2.

(A.19)

Combining (A.17), (3.8), (A.18) and (A.19), we have:

(cid:114) 2
smax

≤ (cid:107)ΓqOq − (cid:98)C(cid:107)F ≤

√

2(cid:107)(cid:98)Γq − ΓqOq(cid:107)F ≤ 4

√

2λ−2

q C

(cid:32)(cid:115)

log(pn)
np

+

1
n

+

(cid:33)

.

1
p

We have reach a contradictory with (3.9). Therefore we conclude that (cid:98)ci (cid:54)= (cid:98)cj.

Next we show that if zi = zj we must have (cid:98)ci = (cid:98)cj. Assume that there exist 1 ≤ i (cid:54)= j ≤ p
such that zi = zj and (cid:98)ci (cid:54)= (cid:98)cj. Notice that from the previous conclusion (i.e., that diﬀerent zi
implies diﬀerent (cid:98)ci), since there are q distinct rows in Z, there are correspondingly q diﬀerent rows
in (cid:98)C. Consequently for any zi = zj, if (cid:98)ci (cid:54)= (cid:98)cj there must exist a k (cid:54)= i, j such that zi = zj (cid:54)= zk
and (cid:98)cj = (cid:98)ck. Let (cid:98)C∗ be (cid:98)C with the jth row replaced by (cid:98)ci. We have

F − (cid:107)(cid:98)Γq − (cid:98)C(cid:107)2
F
2 − (cid:107)(cid:98)γj − (cid:98)ck(cid:107)2

(cid:107)(cid:98)Γq − (cid:98)C∗(cid:107)2
= (cid:107)(cid:98)γj − (cid:98)ci(cid:107)2
= (cid:107)(cid:98)γj − γj + γi − (cid:98)ci(cid:107)2
≤ (cid:107)(cid:98)γj − γj + γi − (cid:98)ci(cid:107)2
≤ 2(cid:107)(cid:98)Γq − ΓqOq(cid:107)2

2

2 − (cid:107)(cid:98)γj − γj + γi − γk + γk − (cid:98)ck(cid:107)2
2 + (cid:107)(cid:98)γj − γj + γk − (cid:98)ck(cid:107)2
F −

2 − (cid:107)γi − γk(cid:107)2
2

2

F + (cid:107)ΓqOq − (cid:98)C(cid:107)2

2
smax

(cid:40)

(cid:32)(cid:115)

≤ 4

4λ−2

q C

log(pn)
np

+

1
n

+

1
p

(cid:33)(cid:41)2

−

2
smax

< 0.

Again, we reach a contradiction and so we conclude that if zi = zj we must have (cid:98)ci = (cid:98)cj.

A.9 Proof of Theorem 4

Note that from Theorem 2, we have the memberships can be recovered with probability tending

to 1, i,e, P ((cid:98)ν (cid:54)= ν) → 0. On the other hand, given (cid:98)ν = ν, we have, the log likelihood function of
(θk,(cid:96), ηk,(cid:96)), 1 ≤ k ≤ (cid:96) ≤ q, is

l({θk,(cid:96), ηk,(cid:96)}; ν) =

(cid:88)

n
(cid:88)

(i,j)∈Sk,l

t=1

(cid:110)

X t

i,j(1 − X t−1

i,j ) log θk,(cid:96) + (1 − X t

i,j)(1 − X t−1

i,j ) log(1 − θk,(cid:96))

+(1 − X t

i,j)X t−1
i,j

log ηk,(cid:96) + X t

i,jX t−1
i,j

log(1 − ηk,(cid:96))

(cid:111)
.

Using the same arguments as in the proof of Proposition 5, we can conclude that when (cid:98)ν = ν,
√
( (cid:98)ΨK1,K2 − ΨK1,K2) → N (0, (cid:101)ΣK1,K2). Let Y ∼ N (0, (cid:101)ΣK1,K2). For any Y ⊂ Rm1+m2, let

nN

1
2
J1,J2

17

Φ(Y) := P (Y ∈ Y), we have:

√

|P (

nN

1
2
K1,K2

( (cid:98)ΨK1,K2 − ΨK1,K2) ∈ Y) − Φ(Y)|

≤ P ((cid:98)ν (cid:54)= ν) + |P (
= o(1).

√

nN

1
2
K1,K2

( (cid:98)ΨK1,K2 − ΨK1,K2) ∈ Y|(cid:98)ν = ν) − Φ(Y)|

This proves the theorem.

A.10 Proof of Theorem 5

Without loss of generality, we consider the case where τ ∈ [n0, τ0], as the convergence rate for

τ ∈ [τ0, n − n0] can be similarly derived. The idea is to break the time interval [n0, τ0] into two

consecutive parts: [n0, τn,p] and [τn,p, τ0], where τn,p =

(cid:106)

τ0 − κn∆−2
F

(cid:104) log(np)

n +

(cid:113) log(np)
np2

(cid:105)(cid:107)

for some

large enough κ > 0. Here (cid:98)·(cid:99) denotes the least integer function. We shall show that when τ ∈ [n −
n0, τn,p], in which (cid:98)ντ +1,n might be inconsistent in estimating ντ0+1,n, we have supτ ∈[n0,τn,p][Mn(τ )−
Mn(τ ) holds in
Mn(τ0)] < 0 in probability. Hence arg maxτ ∈[n0,τ0]
probability. On the other hand, when τ ∈ [τn,p, τ0], we shall see that the membership maps

Mn(τ ) = arg maxτ ∈[τn,p,τ0]

can be consistently recovered, and hence the convergence rate can be obtained using classical

probabilistic arguments. For simplicity, we consider the case where ν1,τ0 = ντ0+1,n = ν ﬁrst, and

modiﬁcation of the proofs for the case where ν1,τ0 (cid:54)= ντ0+1,n will be provided subsequently.

A.10.1 Change point estimation with ν1,τ0 = ντ0+1,n = ν.

We ﬁrst consider the case where the membership structures remain unchanged, while the con-

nectivity matrices before/after the change point are diﬀerent. Speciﬁcally, we assume that
ν1,τ0 = ντ0+1,n = ν for some ν, and (θ1,k,(cid:96), η1,k,(cid:96)) (cid:54)= (θ2,k,(cid:96), η2,k,(cid:96)) for some 1 ≤ k ≤ l ≤ q.

For brevity, we shall be using the notations Sk,l, sk, smin and nk,(cid:96) deﬁned as in Section 3, and

introduce some new notations as follows:

Deﬁne

θτ
2,k,(cid:96) =

τ0−τ
n−τ
τ0−τ
n−τ

θ1,k,(cid:96)η1,k,(cid:96)
θ1,k,(cid:96)+η1,k,(cid:96)
η1,k,(cid:96)
θ1,k,(cid:96)+η1,k,(cid:96)

+ n−τ0
n−τ
+ n−τ0
n−τ

θ2,k,(cid:96)η2,k,(cid:96)
θ2,k,(cid:96)+η2,k,(cid:96)
η2,k,(cid:96)
θ2,k,(cid:96)+η2,k,(cid:96)

,

ητ
2,k,(cid:96) =

τ0−τ
n−τ

τ0−τ
n−τ

θ1,k,(cid:96)η1,k,(cid:96)
θ1,k,(cid:96)+η1,k,(cid:96)
θ1,k,(cid:96)
θ1,k,(cid:96)+η1,k,(cid:96)

+ n−τ0
n−τ
+ n−τ0
n−τ

θ2,k,(cid:96)η2,k,(cid:96)
θ2,k,(cid:96)+η2,k,(cid:96)
θ2,k,(cid:96)
θ2,k,(cid:96)+η2,k,(cid:96)

.

Clearly when τ = τ0 we have θτ0

2,k,(cid:96) = θ2,k,(cid:96) and ητ0

2,k,(cid:96) = η2,k,(cid:96).

18

Correspondingly, we denote the MLEs as

(cid:98)θτ
1,k,(cid:96) =

(cid:98)ητ
1,k,(cid:96) =

(cid:98)θτ
2,k,(cid:96) =

(cid:98)ητ
2,k,(cid:96) =

(cid:88)

τ
(cid:88)

(i,j)∈ (cid:98)Sτ

1,k,(cid:96)

t=1

(cid:88)

τ
(cid:88)

(i,j)∈ (cid:98)Sτ

1,k,(cid:96)

t=1

(cid:88)

n
(cid:88)

(i,j)∈ (cid:98)Sτ

2,k,(cid:96)

t=τ +1

X t

i,j(1 − X t−1
i,j )

(cid:46) (cid:88)

τ
(cid:88)

(1 − X t−1

i,j ),

(1 − X t

i,j)X t−1
i,j

(i,j)∈ (cid:98)Sτ

1,k,(cid:96)

t=1

(cid:46) (cid:88)

τ
(cid:88)

(i,j)∈ (cid:98)Sτ

1,k,(cid:96)

t=1

X t−1
i,j ,

X t

i,j(1 − X t−1
i,j )

(cid:46) (cid:88)

n
(cid:88)

(1 − X t−1

i,j ),

(cid:88)

n
(cid:88)

(1 − X t

i,j)X t−1
i,j

(i,j)∈ (cid:98)Sτ

2,k,(cid:96)

t=τ +1

(i,j)∈ (cid:98)Sτ

2,k,(cid:96)

t=τ +1

(cid:46) (cid:88)

n
(cid:88)

(i,j)∈ (cid:98)Sτ

2,k,(cid:96)

t=τ +1

X t−1
i,j ,

1,k,(cid:96) and (cid:98)Sτ

where (cid:98)Sτ
estimated memberships (cid:98)ν1,τ and (cid:98)ντ +1,n, respectively.

2,k,(cid:96) are deﬁned in a similar way to (cid:98)Sk,(cid:96) (cf. Section 3.2.3), based on the

Denote

Mn(τ ) := l({(cid:98)θτ

1,k,(cid:96)}; (cid:98)ν1,τ ) + l({(cid:98)θτ
M(τ ) := El({θ1,k,(cid:96), η1,k,(cid:96)}; ν1,τ ) + El({θτ

1,k,(cid:96), (cid:98)ητ

2,k,(cid:96), (cid:98)ητ
2,k,(cid:96), ητ

2,k,(cid:96)}; (cid:98)ντ +1,n),
2,k,(cid:96)}; ντ +1,n).

We ﬁrst evaluate several terms in (i)-(v), and all these results will be combined to obtain the

error bound in (vi). In particular, (vi) states that as a direct result of (v), we can focus on the

small neighborhood of [τn,p, τ0] when searching for the estimator (cid:98)τ . Further, the inequality (A.38)
transforms the error bound for τ0 − (cid:98)τ into the error bounds of the terms that we derived in (i)-(iv).
(i) Evaluating M(τ ) − M(τ0).
Note that τ0 = arg maxn0≤τ ≤n−n0

M(τ ), and for any τ ∈ [n0, τ0],

M(τ ) − M(τ0) = El({θ1,k,(cid:96), η1,k,(cid:96)}; ν1,τ ) + El({θτ

2,k,(cid:96), ητ

2,k,(cid:96)}; ντ +1,n)

−El({θ1,k,(cid:96), η1,k,(cid:96)}; ν1,τ0) − El({θ2,k,(cid:96), η2,k,(cid:96)}; ντ0+1,n)

= El({θτ

2,k,(cid:96), ητ

2,k,(cid:96)}; ντ +1,τ0) − El({θ1,k,(cid:96), η1,k,(cid:96)}; ντ +1,τ0)

+El({θτ

2,k,(cid:96), ητ

2,k,(cid:96)}; ντ0+1,n) − El({θ2,k,(cid:96), η2,k,(cid:96)}; ντ0+1,n).

Recall that

l({θk,(cid:96), ηk,(cid:96)}; ν) =

(cid:88)

(cid:88)

n
(cid:88)

1≤k≤(cid:96)≤q

(i,j)∈Sk,l

t=1

(cid:110)

X t

i,j(1 − X t−1

i,j ) log θk,(cid:96)

+(1 − X t

i,j)(1 − X t−1

i,j ) log(1 − θk,(cid:96)) + (1 − X t

i,j)X t−1
i,j

log ηk,(cid:96) + X t

i,jX t−1
i,j

log(1 − ηk,(cid:96))

(cid:111)
.

By Taylor expansion and the fact that the partial derivative of the expected likelihood evaluated

at the true values equals zero we have, there exist θ∗

k,(cid:96) ∈ [θ1,k,(cid:96), θτ

2,k,(cid:96)], η∗

k,(cid:96) ∈ [η1,k,(cid:96), ητ

2,k,(cid:96)], 1 ≤ k ≤

19

(cid:96) ≤ q, such that

El({θτ

2,k,(cid:96), ητ

= −

1
2

(cid:88)

1≤k≤(cid:96)≤q

2,k,(cid:96)}; ντ +1,τ0) − El({θ1,k,(cid:96), η1,k,(cid:96)}; ντ +1,τ0)
2,k,(cid:96) − θ1,k,(cid:96)
(cid:17)2
θ1,k,(cid:96)η1,k,(cid:96)
θ∗
θ1,k,(cid:96) + η1,k,(cid:96)
k,(cid:96)

nk,(cid:96)(τ0 − τ )

(cid:16) θτ

(cid:40)

+

(1 − θ1,k,(cid:96))η1,k,(cid:96)
θ1,k,(cid:96) + η1,k,(cid:96)

(cid:16) θτ

2,k,(cid:96) − θ1,k,(cid:96)
1 − θ∗
k,(cid:96)

(cid:17)2

+

θ1,k,(cid:96)η1,k,(cid:96)
θ1,k,(cid:96) + η1,k,(cid:96)

≤ −C1(τ0 − τ )

(cid:16) ητ

(cid:88)

(cid:17)2

2,k,(cid:96) − η1,k,(cid:96)
η∗
k,(cid:96)
nk,(cid:96)[(θ1,k,(cid:96) − θ2,k,(cid:96))2 + (η1,k,(cid:96) − η2,k,(cid:96))2]

(1 − η1,k,(cid:96))θ1,k,(cid:96)
θ1,k,(cid:96) + η1,k,(cid:96)

2,k,(cid:96) − η1,k,(cid:96)
1 − η∗
k,(cid:96)

+

(cid:16) ητ

(cid:41)

(cid:17)2

1≤k≤(cid:96)≤q

≤ −C1(τ0 − τ )(cid:2)(cid:107)W1,1 − W2,1(cid:107)2

F + (cid:107)W1,2 − W2,2(cid:107)2
F

(cid:3),

for some constant C1 > 0. Here in the ﬁrst step we have used the fact that for any (i, j) ∈ Sk,(cid:96) and
i,j ) = (1−θ1,k,(cid:96))η1,k,(cid:96)
i,j) = θ1,k,(cid:96)η1,k,(cid:96)
t ≤ τ0, EX t
i,j (1 − X t
θ1,k,(cid:96)+η1,k,(cid:96)
θ1,k,(cid:96)+η1,k,(cid:96)
. Similarly, there exist θ†
and EX t
k,(cid:96) ∈ [η2,k,(cid:96), ητ
2,k,(cid:96)], 1 ≤

i,j(1 − X t−1
i,j = (1−η1,k,(cid:96))θ1,k,(cid:96)
θ1,k,(cid:96)+η1,k,(cid:96)

, E(1 − X t
k,(cid:96) ∈ [θ2,k,(cid:96), θτ

i,j)(1 − X t−1
2,k,(cid:96)], η†

i,j ) = EX t−1

,

i,jX t−1
k ≤ (cid:96) ≤ q, such that

El({θτ

2,k,(cid:96), ητ

= −

1
2

(cid:88)

1≤k≤(cid:96)≤q

2,k,(cid:96)}; ντ0+1,n) − El({θ2,k,(cid:96), η2,k,(cid:96)}; ντ0+1,n)
2,k,(cid:96) − θ2,k,(cid:96)
(cid:17)2
θ2,k,(cid:96)η2,k,(cid:96)
θ†
θ2,k,(cid:96) + η2,k,(cid:96)
k,(cid:96)

nk,(cid:96)(n − τ0)

(cid:16) θτ

(cid:40)

+

(1 − θ2,k,(cid:96))η2,k,(cid:96)
θ2,k,(cid:96) + η2,k,(cid:96)

(cid:16) θτ

2,k,(cid:96) − θ2,k,(cid:96)
1 − θ†
k,(cid:96)

(cid:17)2

+

θ2,k,(cid:96)η2,k,(cid:96)
θ2,k,(cid:96) + η2,k,(cid:96)

≤ −C(cid:48)

2(n − τ0)

(cid:16) ητ

(cid:88)

+

(cid:17)2

2,k,(cid:96) − η2,k,(cid:96)
η†
k,(cid:96)
nk,(cid:96)(τ0 − τ )2
(n − τ )2

1≤k≤(cid:96)≤q

(1 − η2,k,(cid:96))θ2,k,(cid:96)
θ2,k,(cid:96) + η2,k,(cid:96)

(cid:16) ητ

2,k,(cid:96) − η2,k,(cid:96)
1 − η†
k,(cid:96)

(cid:41)

(cid:17)2

[(θ1,k,(cid:96) − θ2,k,(cid:96))2 + (η1,k,(cid:96) − η2,k,(cid:96))2]

≤ −

C2(τ0 − τ )2
n − τ

(cid:2)(cid:107)W1,1 − W2,1(cid:107)2

F + (cid:107)W1,2 − W2,2(cid:107)2
F

(cid:3),

for some constants C(cid:48)

2, C2 > 0. Consequently, we conclude that there exists a constant C3 > 0

such that for any n0 ≤ τ ≤ τ0, we have

M(τ ) − M(τ0) ≤ −C3(τ0 − τ )(cid:2)(cid:107)W1,1 − W2,1(cid:107)2

F + (cid:107)W1,2 − W2,2(cid:107)2
F

(cid:3).

(A.20)

(ii) Evaluating supτ ∈[τn,p,τ0] P ((cid:98)ν(τ ) (cid:54)= ν).
Let (cid:98)ν(τ ) be either (cid:98)ν1,τ or (cid:98)ντ +1,n. Note that the membership maps of the networks before/after
τ remain to be ν. From Theorems 1 and 2, we have, under conditions C2-C4, for any constant

B > 0, there exists a large enough constant CB such that

sup
τ ∈[τn,p,τ0]

P ((cid:98)ν(τ ) (cid:54)= ν) ≤ CB(τ0 − τn,p)p[(pn)−(B+1) + exp{−B

√

p}].

Note that by choosing B to be large enough, we have p(τ0−τn,p)(pn)−(B+1) = o

On the other hand, the assumption that log(np)

√

p → 0 in condition C4 implies pn

20

(cid:18)(cid:114)

(τ0−τn,p) log(np)
n2s2
(cid:113) (τ0−τn,p)s2
log(np)

min

min

(cid:19)
.

=

√

o(exp{B
(cid:18)(cid:114)

(cid:19)

p}) for some large enough constant B. Consequently, we have (τ0−τn,p)p exp{−B

(τ0−τn,p) log(np)
n2s2

min

, and hence we conclude that supτ ∈[τn,p,τ0] P ((cid:98)ν(τ ) (cid:54)= ν) = o

o
(iii) Evaluating supτ ∈[τn,p,τ0][Mn(τ ) − M(τ )] when (cid:98)ν(τ ) = ν.
From (ii) we have with probability greater than 1 − o

(τ0−τn,p) log(np)
, (cid:98)ν(τ ) = ν for all τ ∈
n2s2
[τn,p, τ0]. For simplicity, in this part we assume that (cid:98)Sτ
2,k,(cid:96) = Sk,l (or equivalently (cid:98)ν1,τ =
(cid:98)ντ +1,n = ν) holds for all 1 ≤ k ≤ (cid:96) ≤ q and τn,p ≤ τ ≤ τ0 without indicating that this holds in
probability.

1,k,(cid:96) = (cid:98)Sτ

(cid:18)(cid:114)

(cid:19)

min

(τ0−τn,p) log(np)
n2s2

min

(cid:18)(cid:114)

√

p} =

(cid:19)

.

Denote

g1,i,j(θ, η; τ ) =

τ
(cid:88)

(cid:110)

X t

i,j(1 − X t−1

i,j ) log θ

+(1 − X t

t=1
i,j)(1 − X t−1

i,j ) log(1 − θ) + (1 − X t

i,j)X t−1
i,j

and

g2,i,j(θ, η; τ ) =

n
(cid:88)

(cid:110)

X t

i,j(1 − X t−1

i,j ) log θ

+(1 − X t

t=τ +1
i,j)(1 − X t−1

i,j ) log(1 − θ) + (1 − X t

i,j)X t−1
i,j

When (cid:98)ν = ν, we have,

log η + X t

i,jX t−1
i,j

log(1 − η)

(cid:111)
,

log η + X t

i,jX t−1
i,j

log(1 − η)

(cid:111)
.

(A.21)

Mn(τ ) − M(τ )
(cid:88)

(cid:88)

g1,i,j((cid:98)θτ

1,k,(cid:96), (cid:98)ητ

1,k,(cid:96); τ ) +

(cid:88)

(cid:88)

g2,i,j((cid:98)θτ

2,k,(cid:96), (cid:98)ητ

2,k,(cid:96); τ )

1≤k≤(cid:96)≤q

g1,i,j(θ1,k,(cid:96), η1,k,(cid:96); τ ) − E

(i,j)∈Sk,(cid:96)
(cid:88)

(cid:88)

g2,i,j(θτ

2,k,(cid:96), ητ

2,k,(cid:96); τ )

(cid:88)

(cid:88)

g1,i,j((cid:98)θτ

1,k,(cid:96), (cid:98)ητ

1,k,(cid:96); τ ) +

1≤k≤(cid:96)≤q
(cid:88)

(cid:88)

(i,j)∈Sk,(cid:96)

g2,i,j((cid:98)θτ

2,k,(cid:96), (cid:98)ητ

2,k,(cid:96); τ )

=

=

g1,i,j(θ1,k,(cid:96), η1,k,(cid:96); τ ) −

1≤k≤(cid:96)≤q
(cid:88)

(i,j)∈Sk,(cid:96)
(cid:88)

g1,i,j(θ1,k,(cid:96), η1,k,(cid:96); τ ) +

1≤k≤(cid:96)≤q
(cid:88)

(i,j)∈Sk,(cid:96)
(cid:88)

1≤k≤(cid:96)≤q

(i,j)∈Sk,(cid:96)

g2,i,j(θτ

2,k,(cid:96), ητ

2,k,(cid:96); τ )

g2,i,j(θτ

2,k,(cid:96), ητ

2,k,(cid:96); τ )

g1,i,j(θ1,k,(cid:96), η1,k,(cid:96); τ ) − E

(cid:88)

(cid:88)

1≤k≤(cid:96)≤q

(i,j)∈Sk,(cid:96)

g2,i,j(θτ

2,k,(cid:96), ητ

2,k,(cid:96); τ )

1≤k≤(cid:96)≤q

(i,j)∈Sk,(cid:96)

(cid:88)

(cid:88)

−E

1≤k≤(cid:96)≤q

(i,j)∈Sk,(cid:96)

1≤k≤(cid:96)≤q
(cid:88)

−

(i,j)∈Sk,(cid:96)
(cid:88)

1≤k≤(cid:96)≤q
(cid:88)

(i,j)∈Sk,(cid:96)
(cid:88)

+

1≤k≤(cid:96)≤q
(cid:88)

−E

(i,j)∈Sk,(cid:96)
(cid:88)

1≤k≤(cid:96)≤q

(i,j)∈Sk,(cid:96)

Note that {(cid:98)θτ
lor’s expansion we have, there exist random scalars θ−

1,k,(cid:96)} is the maximizer of (cid:80)

1,k,(cid:96), (cid:98)ητ

1≤k≤(cid:96)≤q

(cid:80)

(i,j)∈Sk,(cid:96)
k,(cid:96) ∈ [(cid:98)θτ

g1,i,j(θk,(cid:96), ηk,(cid:96); τ ). Applying Tay-

1,k,(cid:96), θ1,k,(cid:96)], η−

k,(cid:96) ∈ [(cid:98)ητ

1,k,(cid:96), η1,k,(cid:96)] such

21

that

(cid:88)

(cid:88)

1≤k≤(cid:96)≤q

(i,j)∈Sk,(cid:96)

g1,i,j((cid:98)θτ

1,k,(cid:96), (cid:98)ητ

1,k,(cid:96); τ ) −

(cid:88)

(cid:88)

g1,i,j(θ1,k,(cid:96), η1,k,(cid:96); τ )

≤

1
2

(cid:88)

nk,(cid:96)τ

1≤k≤(cid:96)≤q

(cid:40)

(cid:16) θ1,k,(cid:96) − (cid:98)θτ

1,k,(cid:96)

θ−
k,(cid:96)

(cid:17)2

+

(i,j)∈Sk,(cid:96)

1≤k≤(cid:96)≤q
(cid:16) θ1,k,(cid:96) − (cid:98)θτ
1 − θ−
k,(cid:96)

1,k,(cid:96)

(cid:17)2

+

(cid:16) η1,k,(cid:96) − (cid:98)ητ

1,k,(cid:96)

η−
k,(cid:96)

(cid:17)2

+

(cid:16) η1,k,(cid:96) − (cid:98)ητ
1 − η−
k,(cid:96)

1,k,(cid:96)

(cid:17)2

(cid:41)
.

On the other hand, when (cid:98)ν = ν, similar to Proposition 4 and Theorem 3, we can show that for any
B > 0, there exists a large enough constant C− such that max1≤k≤(cid:96)≤q,τ ∈[τn,p,τ0] |(cid:98)θτ
1,k,(cid:96) − θ1,k,(cid:96)| ≤
C−(cid:113) log(np)
hold with probability greater
than 1 − O((np)−B). Consequently, we have, when (cid:98)ν = ν, there exits a large enough constant
C4 > 0 such that

1,k,(cid:96) − η1,k,(cid:96)| = C−(cid:113) log(np)

, and max1≤k≤(cid:96)≤q,τ ∈[τn,p,τ0] |(cid:98)ητ

ns2

ns2

min

min

(cid:88)

(cid:88)

g1,i,j((cid:98)θτ

1,k,(cid:96), (cid:98)ητ

1,k,(cid:96); τ ) −

1≤k≤(cid:96)≤q

(i,j)∈Sk,(cid:96)

≤ C4τ

(cid:88)

nk,(cid:96)

1≤k≤(cid:96)≤q
C4τ p2 log(np)
ns2

min

.

≤

log(np)
ns2

min

(cid:88)

(cid:88)

1≤k≤(cid:96)≤q

(i,j)∈Sk,(cid:96)

g1,i,j(θ1,k,(cid:96), η1,k,(cid:96); τ )

(A.22)

Similarly, we have there exists a large enough constant C5 > 0 such that with probability greater
than 1 − O((np)−B),

(cid:88)

(cid:88)

g2,i,j((cid:98)θτ

2,k,(cid:96), (cid:98)ητ

2,k,(cid:96); τ ) −

(cid:88)

(cid:88)

1≤k≤(cid:96)≤q

(i,j)∈Sk,(cid:96)

g2,i,j(θτ

2,k,(cid:96), ητ

2,k,(cid:96); τ )

.

(A.23)

1≤k≤(cid:96)≤q

(i,j)∈Sk,(cid:96)
C5(n − τ )p2 log(np)
ns2

min

≤

On the other hand, similar to Lemma 2, there exists a constant C6 > 0 such that with probability
greater than 1 − O((np)−B),

g1,i,j(θ1,k,(cid:96), η1,k,(cid:96); τ ) − E

(cid:88)

(cid:88)

1≤k≤(cid:96)≤q

(i,j)∈Sk,(cid:96)

(cid:12)
(cid:12)
(cid:12)
g1,i,j(θ1,k,(cid:96), η1,k,(cid:96); τ )
(cid:12)
(cid:12)
(cid:12)

(A.24)

(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1≤k≤(cid:96)≤q
(cid:115)

≤ C6τ p2

(cid:88)

(i,j)∈Sk,(cid:96)

log(np)
τ p2

,

and

(A.25)

(cid:88)

g2,i,j(θτ

2,k,(cid:96), ητ

2,k,(cid:96); τ ) − E

(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1≤k≤(cid:96)≤q

(i,j)∈Sk,(cid:96)
(cid:115)

≤ C6(n − τ )p2

log(np)
(n − τ )p2 .

(cid:88)

(cid:88)

1≤k≤(cid:96)≤q

(i,j)∈Sk,(cid:96)

g2,i,j(θτ

2,k,(cid:96), ητ

(cid:12)
(cid:12)
(cid:12)
2,k,(cid:96); τ )
(cid:12)
(cid:12)
(cid:12)

22

Combining (A.21), (A.22), (A.23), (A.24) and (A.25) we conclude that when (cid:98)ν = ν, there exists
a large enough constant C0 > 0 such that with probability greater than 1 − O((np)−B),

sup
τ ∈[τn,p,τ0]

|Mn(τ ) − M(τ )| ≤ C0np2

(cid:40)

log(np)
ns2

min

+

(cid:115)

log(np)
np2

(cid:41)

(cid:32)

(cid:115)

= O

np2

(cid:33)

.

log(np)
ns2

min

(A.26)

(iv) Evaluating E supτ ∈[τn,p,τ0] |Mn(τ ) − M(τ ) − Mn(τ0) + M(τ0)|.
Notice that when (cid:98)ν = ν,

Mn(τ ) − M(τ ) − Mn(τ0) + M(τ0)

(cid:88)

(cid:88)

=

1≤k≤(cid:96)≤q

(i,j)∈Sk,(cid:96)

g1,i,j((cid:98)θτ

1,k,(cid:96), (cid:98)ητ

1,k,(cid:96); τ ) +

(cid:88)

(cid:88)

g2,i,j((cid:98)θτ

2,k,(cid:96), (cid:98)ητ

1,k,(cid:96); τ )

(cid:88)

(cid:88)

g1,i,j(θ1,k,(cid:96), η1,k,(cid:96); τ ) − E

1≤k≤(cid:96)≤q

(i,j)∈Sk,(cid:96)
(cid:88)

(cid:88)

g2,i,j(θτ

2,k,(cid:96), ητ

2,k,(cid:96); τ )

1≤k≤(cid:96)≤q
(cid:88)

(i,j)∈Sk,(cid:96)
(cid:88)

g1,i,j((cid:98)θτ0

1,k,(cid:96), (cid:98)ητ0

1,k,(cid:96); τ0) −

1≤k≤(cid:96)≤q

(i,j)∈Sk,(cid:96)
(cid:88)

(cid:88)

g2,i,j((cid:98)θτ0

2,k,(cid:96), (cid:98)ητ0

2,k,(cid:96); τ0)

−E

−

1≤k≤(cid:96)≤q
(cid:88)

+E

(i,j)∈Sk,(cid:96)
(cid:88)

1≤k≤(cid:96)≤q

(i,j)∈Sk,(cid:96)

1≤k≤(cid:96)≤q

(i,j)∈Sk,(cid:96)

g1,i,j(θ1,k,(cid:96), η1,k,(cid:96); τ0) + E

(cid:88)

(cid:88)

1≤k≤(cid:96)≤q

(i,j)∈Sk,(cid:96)

g2,i,j(θ2,k,(cid:96), η2,k,(cid:96); τ0)

Note that

g1,i,j((cid:98)θτ
(cid:40)
τ
(cid:88)

1,k,(cid:96), (cid:98)ητ

1,k,(cid:96); τ ) − g1,i,j((cid:98)θτ0

1,k,(cid:96), (cid:98)ητ0

1,k,(cid:96); τ0) − E[g1,i,j(θ1,k,(cid:96), η1,k,(cid:96); τ ) − g1,i,j(θ1,k,(cid:96), η1,k,(cid:96); τ0)]

=

X t

i,j(1 − X t−1

i,j ) log

t=1

+(1 − X t

i,j)X t−1
i,j

(cid:98)θτ
1,k,(cid:96)
(cid:98)θτ0
1,k,(cid:96)

+ (1 − X t

i,j)(1 − X t−1

i,j ) log

log (cid:98)ητ
(cid:98)ητ0

1,k,(cid:96)

1,k,(cid:96)

+ X t

i,jX t−1
i,j

log

1 − (cid:98)ητ
1 − (cid:98)ητ0

1,k,(cid:96)

1,k,(cid:96)

(cid:41)

−

1,k,(cid:96)

1 − (cid:98)θτ
1 − (cid:98)θτ0
1,k,(cid:96)
(cid:40)
τ0(cid:88)

X t

i,j(1 − X t−1

i,j ) log (cid:98)θτ0

1,k,(cid:96)

+(1 − X t

i,j)(1 − X t−1

i,j ) log(1 − (cid:98)θτ0

1,k,(cid:96)) + (1 − X t

i,j)X t−1
i,j

t=τ +1

log (cid:98)ητ0

1,k,(cid:96) + X t

i,jX t−1
i,j

(cid:41)

log(1 − (cid:98)ητ0

1,k,(cid:96))

τ0(cid:88)

+E

(cid:110)

X t

i,j(1 − X t−1

i,j ) log θ1,k,(cid:96) + (1 − X t

i,j)(1 − X t−1

i,j ) log(1 − θ1,k,(cid:96))

t=τ +1
+(1 − X t

i,j)X t−1
i,j

log η1,k,(cid:96) + X t

i,jX t−1
i,j

log(1 − η1,k,(cid:96))

(cid:111)
.

When sum over all (i, j) ∈ Sk,(cid:96) and 1 ≤ k ≤ (cid:96) ≤ q, the last two terms in the above inequality

can be bounded similar to (A.22) and (A.24), with τ replaced by τ0 − τ . For the ﬁrst term, with

some calculations we have there exists a constant c1 > 0 such that with probability larger than
1 − O(np)−B),

(cid:12)
1,k,(cid:96) − (cid:98)θτ0
(cid:12)(cid:98)θτ
(cid:12)

1,k,(cid:96)

(cid:12)
(cid:12)
(cid:12) ≤ c1

(cid:12)
(cid:12)(cid:98)ητ
(cid:12)

1,k,(cid:96) − (cid:98)ητ0

1,k,(cid:96)

(cid:12)
(cid:12)
(cid:12) ≤ c1

(cid:114) τ0 − τ
τ0
(cid:114) τ0 − τ
τ0

(cid:115)

(cid:115)

log(np)
ns2

min

,

log(np)
ns2

min

.

(A.27)

sup
1≤k≤(cid:96)≤q

sup
1≤k≤(cid:96)≤q

23

Brief derivations of (A.27) are provided in Section A.10.3. Consequently, similar to (A.26), we

have there exists a large enough constant c2 > 0 such that

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

(cid:88)

1≤k≤(cid:96)≤q

(i,j)∈Sk,(cid:96)

(cid:104)
g1,i,j((cid:98)θτ

1,k,(cid:96), (cid:98)ητ

1,k,(cid:96); τ ) − g1,i,j((cid:98)θτ0

1,k,(cid:96), (cid:98)ητ0

(cid:105)
1,k,(cid:96); τ0)

(cid:88)

(cid:88)

(cid:104)

−E

g1,i,j(θ1,k,(cid:96), η1,k,(cid:96); τ ) − g1,i,j(θ1,k,(cid:96), η1,k,(cid:96); τ0)

(cid:12)
(cid:12)
(cid:105)
(cid:12)
(cid:12)
(cid:12)

1≤k≤(cid:96)≤q
(cid:115)

(i,j)∈Sk,(cid:96)

(τ0 − τ ) log(np)
s2
min

≤ c2p2

.

(A.28)

Here in the last step we have used the fact that τ0 (cid:39) O(n),
(cid:16)(cid:113) (τ0−τ ) log(np)
s2
min

. Similarly, note that,

(cid:17)

o

(cid:113) log(np)

p2 ≤

(cid:113) log(np)
s2
min

, and (τ0−τ ) log(np)

ns2

min

=

(A.29)

2,k,(cid:96), (cid:98)ητ
g2,i,j((cid:98)θτ
(cid:40)
n
(cid:88)

X t

2,k,(cid:96); τ ) − g2,i,j((cid:98)θτ0
2,k,(cid:96), (cid:98)ητ0
(cid:98)θτ
2,k,(cid:96)
(cid:98)θτ0
2,k,(cid:96)

(cid:34)
i,j(1 − X t−1
i,j )

log

t=τ0+1

− log

θτ
2,k,(cid:96)
θ2,k,(cid:96)

2,k,(cid:96); τ0) − E[g2,i,j(θτ
(cid:35)

=

+ (1 − X t

i,j)(1 − X t−1

i,j ) ·

(cid:34)

log

2,k,(cid:96)

1 − (cid:98)θτ
1 − (cid:98)θτ0

2,k,(cid:96)

2,k,(cid:96), ητ

2,k,(cid:96); τ ) − g2,i,j(θ2,k,(cid:96), η2,k,(cid:96); τ0)]

− log

− log

1 − θτ
2,k,(cid:96)
1 − θ2,k,(cid:96)

1 − ητ
2,k,(cid:96)
1 − η2,k,(cid:96)

(cid:35)

(cid:34)
i,j(1 − X t−1
i,j )

+ X t

(cid:35)(cid:41)

log (cid:98)ητ
(cid:98)ητ0

2,k,(cid:96)

2,k,(cid:96)

− log

(cid:35)

ητ
2,k,(cid:96)
η2,k,(cid:96)

(cid:34)

+ X t

i,jX t−1
i,j

log

1 − (cid:98)ητ
1 − (cid:98)ητ0

2,k,(cid:96)

2,k,(cid:96)

+ [g2,i,j(θτ

2,k,(cid:96), ητ

2,k,(cid:96); τ0) − g2,i,j(θ2,k,(cid:96), η2,k,(cid:96); τ0)]

−E[g2,i,j(θτ

2,k,(cid:96), ητ

2,k,(cid:96); τ0) − g2,i,j(θ2,k,(cid:96), η2,k,(cid:96); τ0)] +

(cid:40)

X t

i,j(1 − X t−1

i,j ) log (cid:98)θτ

2,k,(cid:96)

τ0(cid:88)

t=τ +1

+(1 − X t

i,j)(1 − X t−1

i,j ) log(1 − (cid:98)θτ

2,k,(cid:96)) + (1 − X t

i,j)X t−1
i,j

log (cid:98)ητ

2,k,(cid:96) + X t

i,jX t−1
i,j

(cid:41)

log(1 − (cid:98)ητ

2,k,(cid:96))

τ0(cid:88)

−E

(cid:110)

X t

i,j(1 − X t−1

i,j ) log θτ

2,k,(cid:96) + (1 − X t

i,j)(1 − X t−1

i,j ) log(1 − θτ

2,k,(cid:96))

t=τ +1
+(1 − X t

i,j)X t−1
i,j

log ητ

2,k,(cid:96) + X t

i,jX t−1
i,j

log(1 − ητ

2,k,(cid:96))

(cid:111)

:= I + II − III + IV − V.

For II − III, from Lemma 2 and the fact that
c3(τ0−τ )
n−τ

(cid:12)
(cid:12)
2,k,(cid:96) − θ2,k,(cid:96)
(cid:12) ≤
for some large enough constant c3, we have there exists a large enough constant c4 > 0

(cid:12)
(cid:12) ≤ c3(τ0−τ )
(cid:12)

2,k,(cid:96) − η2,k,(cid:96)

(cid:12)
(cid:12)ητ
(cid:12)

(cid:12)
(cid:12)θτ
(cid:12)

and

n−τ

,

such that with probability greater than 1 − O((np)−B),

(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1≤k≤(cid:96)≤q

(cid:88)

(i,j)∈Sk,(cid:96)

(cid:12)
(cid:12)
(cid:12)
(II − III)
(cid:12)
(cid:12)
(cid:12)

≤ c4p2 τ0 − τ
n − τ

(cid:115)

24

log(np)
τ0p2 = o

(cid:32)

(cid:115)

p2

(τ0 − τ ) log(np)
s2
min

(cid:33)
.

(A.30)

When sum over all (i, j) ∈ Sk,(cid:96) and 1 ≤ k ≤ (cid:96) ≤ q, the IV − V term can be bounded similar to

(A.22) and (A.24), with τ replaced by τ0 − τ , i.e., there exist a constant c5 > 0 such that with
probability greater than 1 − O((np)−B),

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

(cid:88)

1≤k≤(cid:96)≤q

(i,j)∈Sk,(cid:96)

(cid:12)
(cid:12)
(cid:12)
(IV − V )
(cid:12)
(cid:12)
(cid:12)

(cid:34)

≤ c5p2

(τ0 − τ ) log(np)
ns2

min

√

+

τ0 − τ

(cid:115)

(cid:35)

log(np)
p2

(cid:32)

(cid:115)

= O

p2

(τ0 − τ ) log(np)
s2
min

(cid:33)

.

(A.31)

Lastly, similar to (A.27), we can show that there exists a constant c6 > 0 such that with probability
larger than 1 − O(np)−B),

sup
1≤k≤(cid:96)≤q

sup
1≤k≤(cid:96)≤q

(cid:12)
(cid:98)θτ
(cid:12)
2,k,(cid:96)
(cid:12)
log
(cid:12)
θτ
(cid:12)
2,k,(cid:96)
(cid:12)
log (cid:98)ητ
(cid:12)
2,k,(cid:96)
(cid:12)
(cid:12)
ητ
(cid:12)
2,k,(cid:96)

− log

(cid:98)θτ0
2,k,(cid:96)
θ2,k,(cid:96)
− log (cid:98)ητ0
2,k,(cid:96)
η2,k,(cid:96)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ c6

≤ c6

(cid:114) τ0 − τ
n
(cid:114) τ0 − τ
n

(cid:115)

log(np)
ns2

min

,

(cid:115)

log(np)
ns2

min

.

(A.32)

A brief proof of (A.32) is provided in Section A.10.3. Consequently, we can show that there exists
a constant c7 > 0 such that with probability larger than 1 − O(np)−B),

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

(cid:88)

1≤k≤(cid:96)≤q

(i,j)∈Sk,(cid:96)

(cid:104)
g2,i,j((cid:98)θτ

2,k,(cid:96), (cid:98)ητ

2,k,(cid:96); τ ) − g2,i,j((cid:98)θτ0

2,k,(cid:96), (cid:98)ητ0

2,k,(cid:96); τ0)

(cid:105)

(cid:88)

(cid:88)

(cid:104)

−E

g2,i,j(θτ

2,k,(cid:96), ητ

2,k,(cid:96); τ ) − g2,i,j(θ2,k,(cid:96), η2,k,(cid:96); τ0)

(cid:12)
(cid:12)
(cid:105)
(cid:12)
(cid:12)
(cid:12)

1≤k≤(cid:96)≤q
(cid:115)

(i,j)∈Sk,(cid:96)

(τ0 − τ ) log(np)
s2
min

≤ c7p2

.

(A.33)

Now combining (A.28) and (A.33) and the probability for (cid:98)ν (cid:54)= ν in (ii), we conclude that there

exists a constant C0 > 0 such that

E sup

|Mn(τ ) − M(τ ) − Mn(τ0) + M(τ0)|

τ ∈[τn,p,τ0]
(cid:40)(cid:115)

≤ C0np2

(cid:115)

≤ 2C0p2

(τ0 − τn,p) log(np)
n2s2

min

+ o

(cid:32)(cid:115)

(cid:33)(cid:41)

(τ0 − τn,p) log(np)
n2s2

min

(τ0 − τn,p) log(np)
s2
min

.

(A.34)

(v) Evaluating supτ ∈[n0,τn,p][Mn(τ ) − Mn(τ0)].
In this part we consider the case when τ ∈ [n − n0, τn,p]. We shall see that supτ ∈[n0,τn,p][Mn(τ ) −
Mn(τ ) holds in
Mn(τ0)] < 0 in probability and hence arg maxτ ∈[n0,τ0]
probability. Note that for any τ ∈ [n − n0, τn,p],

Mn(τ ) = arg maxτ ∈[τn,p,τ0]

Mn(τ ) − Mn(τ0) = Mn(τ ) − M(τ ) − Mn(τ0) + M(τ0) − [M(τ0) − M(τ )].

(A.35)

25

Given (cid:98)ν1,τ and (cid:98)ντ +1,n, we deﬁne an intermediate term

M∗

n(τ ) := l({θ−

τ,k,(cid:96), η−

τ,k,(cid:96)}; (cid:98)ν1,τ ) + l({θ∗

τ,k,(cid:96), η∗

τ,k,(cid:96)}; (cid:98)ντ +1,n).

where

and

θ−
τ,k,(cid:96) =

(cid:80)

(i,j)∈ (cid:98)Sτ

1,k,(cid:96)

(cid:80)

(i,j)∈ (cid:98)Sτ

1,k,(cid:96)

θ1,ν(i),ν(j)η1,ν(i),ν(j)
θ1,ν(i),ν(j)+η1,ν(i),ν(j)
η1,ν(i),ν(j)
θ1,ν(i),ν(j)+η1,ν(i),ν(j)

,

η−
τ,k,(cid:96) =

(cid:80)

(i,j)∈ (cid:98)Sτ

1,k,(cid:96)

(cid:80)

(i,j)∈ (cid:98)Sτ

1,k,(cid:96)

θ1,ν(i),ν(j)η1,ν(i),ν(j)
θ1,ν(i),ν(j)+η1,ν(i),ν(j)
θ1,ν(i),ν(j)
θ1,ν(i),ν(j)+η1,ν(i),ν(j)

,

(cid:80)

(cid:80)

(i,j)∈ (cid:98)Sτ
(cid:80)

2,k,(cid:96)

(i,j)∈ (cid:98)Sτ

(i,j)∈ (cid:98)Sτ
(cid:80)

2,k,(cid:96)

(i,j)∈ (cid:98)Sτ

2,k,(cid:96)

(cid:104) (τ0−τ )θ1,ν(i),ν(j)η1,ν(i),ν(j)
θ1,ν(i),ν(j)+η1,ν(i),ν(j)
(cid:104)
(τ0−τ )η1,ν(i),ν(j)
θ1,ν(i),ν(j)+η1,ν(i),ν(j)
(cid:104) (τ0−τ )θ1,ν(i),ν(j)η1,ν(i),ν(j)
θ1,ν(i),ν(j)+η1,ν(i),ν(j)
(cid:104)
(τ0−τ )θ1,ν(i),ν(j)
θ1,ν(i),ν(j)+η1,ν(i),ν(j)

2,k,(cid:96)

+

+

+

+

(n−τ0)θ2,ν(i),ν(j)η2,ν(i),ν(j)
θ2,ν(i),ν(j)+η2,ν(i),ν(j)
(cid:105)
(n−τ0)η2,ν(i),ν(j)
θ2,ν(i),ν(j)+η2,ν(i),ν(j)

(n−τ0)θ2,ν(i),ν(j)η2,ν(i),ν(j)
θ2,ν(i),ν(j)+η2,ν(i),ν(j)
(cid:105)
(n−τ0)θ2,ν(i),ν(j)
θ2,ν(i),ν(j)+η2,ν(i),ν(j)

(cid:105)

(cid:105)

,

.

θ∗
τ,k,(cid:96) =

η∗
τ,k,(cid:96) =

We have

Mn(τ ) − M(τ ) = Mn(τ ) − EM∗

n(τ ) + EM∗

n(τ ) − M(τ ).

Note that the expected log-likelihood E (cid:80)
θ1,ν(i),ν(j), β1,i,j = η1,ν(i),ν(j), and E (cid:80)
β2,i,j = ητ,ν(i),ν(j), we have

1≤i≤j≤p g1,i,j(α1,i,j, β1,i,j, τ ) is maximized at α1,i,j =

1≤i≤j≤p g2,i,j(α2,i,j, β2,i,j, τ ) is maximized at α2,i,j = θτ,ν(i),ν(j),

EM∗

n(τ ) − M(τ ) ≤ 0.

τ,k,(cid:96)} is the maximizer of El({θk,(cid:96), ηk,(cid:96)}; (cid:98)ν1,τ ) and
τ,k,(cid:96)} is the maximizer of El({θk,(cid:96), ηk,(cid:96)}; (cid:98)ντ +1,n). Similar to (A.26), there exists a large

On the other hand, notice that given (cid:98)ν, {θ−
{θ∗
enough constant C7 > 0 such that with probability greater than 1 − O((np)−B),

τ,k,(cid:96), η−

τ,k,(cid:96), η∗

sup
τ ∈[n0,τn,p]

|Mn(τ ) − EM∗

n(τ )| ≤ C7np2

(cid:40)

log(np)
n

+

(cid:115)

(cid:41)

.

log(np)
np2

Consequently we have, with probability greater than 1 − O((np)−B),

sup
τ ∈[n0,τn,p]

(cid:2)Mn(τ ) − M(τ )(cid:3) ≤ C7np2

(cid:40)

log(np)
n

+

(cid:115)

(cid:41)

.

log(np)
np2

(A.36)

We remark that since the membership structure (cid:98)ντ +1,n can be very diﬀerent from the original ν,
the smin in (A.26) is simply replaced by the lower bound 1, and hence the upper bound in (A.36)
is independent of (cid:98)ν1;τ and (cid:98)ντ +1,n.

26

Combining (A.35), (A.36), (A.20), (A.26) (with τ = τ0), and choosing κ > 0 to be large

enough, we have with probability greater than 1 − O((np)−B),

(cid:2)Mn(τ ) − Mn(τ0)(cid:3)

sup
τ ∈[n0,τn,p]
(cid:40)

≤ C7np2

log(np)
n

(cid:115)

+

(cid:41)

log(np)
np2

+ C0np2

(cid:40)

log(np)
ns2

min

+

(cid:115)

(cid:41)

log(np)
np2

−C3(τ0 − τn,p)(cid:2)(cid:107)W1,1 − W2,1(cid:107)2

F + (cid:107)W1,2 − W2,2(cid:107)2
F

(cid:3)

< 0.

Consequently we have,

(cid:32)

P

arg max
τ ∈[n0,τ0]

(cid:2)Mn(τ ) − Mn(τ0)(cid:3) = arg max
τ ∈[τn,p,τ0]

(cid:2)Mn(τ ) − Mn(τ0)(cid:3)

(cid:33)

≥ 1 − O((np)−B).

(A.37)

(vi) Error bound for τ0 − (cid:98)τ .

One of the key steps in the proof of (v) is to compare Mn(τ ), the estimated log-likelihood

evaluated under the MLEs at a searching time point τ , with M (τ ), the maximized expected log-

likelihood at time τ . The error between Mn(τ ) and M (τ ), which is of order O

(cid:33)

(cid:17)

(cid:113) log(np)
np2

reﬂects the noise level. On the other hand, the signal is captured by M (τ0) − M (τ ) =

O(|τ0 − τ |p2∆2

F ), i.e., the diﬀerence between the maximized expected log-likelihood evaluated at
the true change point τ0 and the maximized expected log-likelihood evaluated at the searching

time point τ . Consequently, when |τ0 − τ |p2∆2

(cid:34)
F > κ

np2(cid:16) log(np)

n +

(cid:113) log(np)
np2

(cid:17)

(cid:35)

(cid:32)
np2(cid:16) log(np)

n +

enough constant κ > 0, we are able to claim that |τ0 − (cid:98)τ | ≤ |τ0 − τ | = Op
(cid:113) log(np)
np2

. By further deriving the estimation errors for any τ in the neighborhood of τ0 with

(cid:105)(cid:17)

radius O

(cid:16)

∆−2
F

(cid:104) log(np)

n +

(cid:113) log(np)
np2

(cid:105)(cid:17)

, we obtained a better bound based on Markov’s inequality

for some large

(cid:16)

n∆−2
F

(cid:104) log(np)

n +

(see (A.38) below).

From (A.37) we have for any 0 < (cid:15) ≤ τ0 − τn,p,

(cid:18)

P (τ0 − (cid:98)τ > (cid:15)) ≤ P

sup
τ ∈[τn,p,τ0−(cid:15)]

Mn(τ ) − Mn(τ0) ≥ 0

(cid:19)

+ O((np)−B).

27

Note that from (i) and (iv) we have

P

≤ P

(cid:18)

(cid:18)

(cid:18)

sup
τ ∈[τn,p,τ0−(cid:15)]

sup
τ ∈[τn,p,τ0−(cid:15)]

≤ P

sup
τ ∈[τn,p,τ0−(cid:15)]
E supτ ∈[τn,p,τ0−(cid:15)]

≤

≤

(cid:114)

2C0p2

(τ0−τn,p) log(np)
s2
min
C3(cid:15)p2∆2
F

.

Mn(τ ) − Mn(τ0) ≥ 0

(cid:19)

(cid:2)(Mn(τ ) − M(τ ) − Mn(τ0) + M(τ0)) − (M(τ0) − M(τ ))(cid:3) ≥ 0

(A.38)

(cid:19)

|Mn(τ ) − M(τ ) − Mn(τ0) + M(τ0)| ≥ C3(cid:15)p2∆2
F

(cid:19)

(cid:12)Mn(τ ) − M(τ ) − Mn(τ0) + M(τ0)(cid:12)
(cid:12)
(cid:12)

C3(cid:15)p2∆2
F

We thus conclude that τ0−(cid:98)τ = Op
C5 we have,

(cid:114)

(cid:18)

∆−2
F

(τ0−τn,p) log(np)
s2
min

(cid:19)

. By the deﬁnition of τn,p and condition

(cid:115)

∆−2
F

(τ0 − τn,p) log(np)
s2
min

= O

(cid:32)

τ0 − τn,p
∆F

(cid:115)

(cid:34)

log(np)
ns2

min

log(np)
n

+

(cid:115)

log(np)
np2

(cid:35)−1/2(cid:33)
.

Consequently, we conclude that



(cid:40)

min

(cid:110)

1, (n−1p2 log(np))

τ0 − (cid:98)τ = Op

(τ0 − τn,p) min

1,

∆F smin

(cid:111)

1
4

(cid:41)
 .

A.10.2 Change point estimation with ν1,τ0 (cid:54)= ντ0+1,n.

We modify steps (i)-(v) to the case where ν1,τ0 (cid:54)= ντ0+1,n.

With some abuse of notations, we put W1,1 = (α1,i,j)p×p with α1,i,j = θ1,ν1,τ0 (i),ν1,τ0 (j), W1,2 =
(1 − β1,i,j)p×p with β1,i,j = η1,ν1,τ0 (i),ν1,τ0 (j), W2,1 = (α2,i,j)p×p with α2,i,j = θ2,ντ0+1,n(i),ντ0+1,n(j),
and W2,2 = (1 − β2,i,j)p×p with β2,i,j = η2,ντ0+1,n(i),ντ0+1,n(j). Similar to previous proofs we deﬁne

Mn(τ ) :=

(cid:88)

1≤i≤j≤p
(cid:88)

M(τ ) := E

g1,i,j((cid:98)ατ

1,i,j, (cid:98)βτ

1,i,j, τ ) +

(cid:88)

1≤i≤j≤p

g2,i,j((cid:98)ατ

2,i,j, (cid:98)βτ

2,i,j, τ ),

g1,i,j(α1,i,j, β1,i,j, τ ) + E

(cid:88)

1≤i≤j≤p

g2,i,j(ατ

2,i,j, βτ

2,i,j, τ ),

1≤i≤j≤p

where

ατ

2,i,j =

βτ
2,i,j =

τ0−τ
n−τ
τ0−τ
n−τ

τ0−τ
n−τ
τ0−τ
n−τ

α1,i,j β1,i,j
α1,i,j +β1,i,j
β1,i,j
α1,i,j +β1,i,j
α1,i,j β1,i,j
α1,i,j +β1,i,j
α1,i,j
α1,i,j +β1,i,j

+ n−τ0
n−τ
+ n−τ0
n−τ
+ n−τ0
n−τ
+ n−τ0
n−τ

α2,i,j β2,i,j
α2,i,j +β2,i,j
β2,i,j
α2,i,j +β2,i,j
α2,i,j β2,i,j
α2,i,j +β2,i,j
α2,i,j
α2,i,j +β2,i,j

,

,

28

and

(cid:98)ατ
1,i,j = (cid:98)θτ
2,i,j = (cid:98)θτ
(cid:98)ατ

1,(cid:98)ν1,τ (i),(cid:98)ν1,τ (j),
2,(cid:98)ντ +1,n(i),(cid:98)ντ +1,n(j),

1,i,j = (cid:98)ητ
(cid:98)βτ
2,i,j = (cid:98)η2
(cid:98)βτ

1,(cid:98)ν1,τ (i),(cid:98)ν1,τ (j),

τ,(cid:98)ντ +1,n(i),(cid:98)ντ +1,n(j).

Note that the deﬁnition of M (τ ) here is now slightly diﬀerent from the previous deﬁnition in

that the ατ

2,i,j and βτ

2,ντ0+1,n(i),ντ0+1,n(j) and ητ

2,i,j will generally be diﬀerent from θτ

2,ντ0+1,n(i),ντ0+1,n(j),
unless ν1,τ0 = ντ0+1,n. We ﬁrst of all point out the main diﬀerence we are facing in the case where
ν1,τ0 (cid:54)= ντ0+1,n. Consider a detection time τ ∈ [τn,p, τ0]. In the case where (cid:98)ν1,τ = (cid:98)ντ +1,n = ν,
we have ατ
for all

2,i,j = θτ
1 ≤ k ≤ (cid:96) ≤ q, or equivalently, |(cid:98)ατ
when (cid:98)ν1,τ = ν1,τ0
(cid:16)(cid:113) log(np)
. Here τ0−τ
Op
ns2
n
issue is that the the following terms from the deﬁnition of (cid:98)θτ

for all 1 ≤ i ≤ j ≤ p. However,
(cid:98)ντ +1,n = ντ0+1,n but ν1,τ0 (cid:54)= ντ0+1,n, the order of the estimation error becomes
(cid:17)
is a bias terms brought by the fact that (cid:98)ν1,τ (cid:54)= (cid:98)ντ +1,n. The main

2,k,(cid:96) for all (i, j) ∈ Sk,(cid:96), and we have |(cid:98)θτ

2,k,(cid:96) − θτ
(cid:17)

(cid:16)(cid:113) log(np)
ns2

(cid:16)(cid:113) log(np)
ns2

2,ν(i),ν(j)| = Op

2,k,(cid:96)| = Op

2,i,j − θτ

+ τ0−τ
n

min

min

min

(cid:17)

2,k,(cid:96):

(cid:88)

τ0(cid:88)

(i,j)∈ (cid:98)Sτ

2,k,(cid:96)

t=τ +1

X t

i,j(1 − X t−1

i,j ),

(cid:88)

τ0(cid:88)

(1 − X t−1

i,j ),

(i,j)∈ (cid:98)Sτ

2,k,(cid:96)

t=τ +1

are no longer unbiased estimators (subject to a normalization) of the following corresponding

terms in the deﬁnition of θτ

2,k,(cid:96):

θ1,k,(cid:96)η1,k,(cid:96)
θ1,k,(cid:96) + η1,k,(cid:96)

,

θ1,k,(cid:96),
θ1,k,(cid:96) + η1,k,(cid:96)

.

The proof of (i) does not involve any parameter estimators and hence can be established

similarly.

2,i,j − α2,i,j| ≤ |(cid:98)ατ

For (ii), note that |(cid:98)ατ

(cid:1) holds for all 1 ≤ i < j ≤ p,
2,i,j| + O(cid:0) τ0−τ
(cid:1) is independent of i, j. This implies that when estimating the α2,i,j, we have
(cid:1) by including the τ0 − τ samples before the change point. From

where the O(cid:0) τ0−τ
n
introduced a bias term O(cid:0) τ0−τ
n
the proofs of Lemma 5, and condition C4, we conclude that (ii) hold for (cid:98)ντ +1,n.

2,i,j − ατ

n

For (iii), replacing the order of the error bound for (cid:98)θ+

τ,k,(cid:96) and (cid:98)θ+

τ,k,(cid:96) from

τ0−τ
n , we have there exists a large enough constant C0 > 0 such that

(cid:113) log(np)
ns2

min

to

(cid:113) log(np)
ns2

min

+

sup
τ ∈[τn,p,τ0]

|Mn(τ ) − M(τ )| ≤ C0np2

(cid:32)

= O

np2

(cid:115)

+

log(np)
np2 +

(cid:40)

log(np)
ns2
(cid:40)(cid:115)

min

log(np)
ns2

min

+

(τ0 − τn,p)2
n2

(cid:41)

(τ0 − τn,p)2
n2
(cid:41) (cid:33)
.

For (iv), the error bounds related to g1,i,j(·, ·; ·) remain unchanged. Note that the decom-
2,k,(cid:96) replaced be ατ

position (A.29) still holds with θτ

2,k,(cid:96) replaced be

2,i,j and (cid:98)θτ

2,k,(cid:96), ητ

2,i,j, βτ

2,k,(cid:96), (cid:98)ητ

29

2,i,j, (cid:98)βτ
(cid:98)ατ

2,i,j. The bound for (A.30) still holds owing to the fact that |ατ

2,i,j − α2,i,j| = O

|βτ

2,i,j − β2,i,j| = O

(cid:17)

(cid:16) τ0−τ
n

. The bound for (A.31) would become O

(cid:32)

p2(cid:113) (τ0−τ ) log(np)

s2
min

+ (τ0−τn,p)2
n2

(cid:17)

(cid:16) τ0−τ
n

and
(cid:33)
.

Notice that similar to (A.32), we have with probability larger than 1 − O((np)−B),

(cid:12)
log (cid:98)ατ
(cid:12)
(cid:12)
(cid:12)
ατ
(cid:12)

2,i,j

2,i,j

log

(cid:98)βτ
2,i,j
βτ

2,i,j

− log (cid:98)ατ0
2,k,(cid:96)
α2,k,(cid:96)
(cid:98)βτ0
2,k,(cid:96)
β2, k, (cid:96)

− log

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

sup
1≤i≤j≤p
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

sup
1≤i≤j≤p

= O

= O

(cid:32)(cid:114) τ0 − τ

(cid:115)

n
(cid:32)(cid:114) τ0 − τ

n

log(np)
ns2

min

+

τ0 − τ
n

(cid:115)

log(np)
ns2

min

+

τ0 − τ
n

(cid:33)

,

(cid:33)

.

Consequently, we have

E sup

τ ∈[τn,p,τ0]

|Mn(τ ) − M(τ ) − Mn(τ0) + M(τ0)| ≤ C0p2

(cid:40)(cid:115)

(τ0 − τn,p) log(np)
s2
min

(cid:41)

+ (τ0 − τn,p)

.

By noticing that {α1,i,j, β1,i,j, ατ

2,i,j, βτ

2,i,j} is the maximizer of M(τ ), we conclude that (v) also

holds. Consequently, for (vi), we have

(cid:18)

P

sup
τ ∈[τn,p,τ0−(cid:15)]

Mn(τ ) − Mn(τ0) ≥ 0

(cid:19)

(cid:114)

C0p2

≤

(τ0−τn,p) log(np)
s2
min

+ C0p2(τ0 − τn,p)

C3(cid:15)p2∆2
F

.

Consequently, we conclude that



(cid:40)

min

(cid:110)

1, (n−1p2 log(np))

(cid:111)

1
4

τ0 − (cid:98)τ = Op

(τ0 − τn,p) min

1,

∆F smin

(cid:41)
 .

+

1
∆2
F

A.10.3 Proofs of (A.27) and (A.32) when (cid:98)ν = ν

For (A.27), note that

(cid:12)
1,k,(cid:96) − (cid:98)θτ0
(cid:12)(cid:98)θτ
(cid:12)

1,k,(cid:96)

(cid:12)
(cid:12)
(cid:12) =

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:80)

(i,j)∈Sk,(cid:96)
(cid:80)

(i,j)∈Sk,(cid:96)

(cid:80)τ

t=1 X t
(cid:80)τ

i,j(1 − X t−1
i,j )
t=1(1 − X t−1
i,j )

−

(cid:80)

(i,j)∈Sk,(cid:96)
(cid:80)

(i,j)∈Sk,(cid:96)

(cid:80)τ0

t=1 X t
(cid:80)τ0

i,j(1 − X t−1
i,j )
t=1(1 − X t−1
i,j )

(A.39)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

Similar to Lemma 2, we can show that for any constant B > 0, there exists a large enough
constant B1 such that with probability larger than 1 − O((np)−(B+2)),

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
τ nk,(cid:96)

1
τ nk,(cid:96)

(i,j)∈Sk,(cid:96)

t=1

(cid:88)

τ
(cid:88)

(i,j)∈Sk,(cid:96)

t=1

(cid:88)

τ
(cid:88)

(1 − X t−1

i,j ) −

η1,k,(cid:96)
θ1,k,(cid:96) + η1,k,(cid:96)

(cid:115)

≤ B1

log(np)
τ nk,(cid:96)

,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

X t

i,j(1 − X t−1

i,j ) −

η1,k,(cid:96)
θ1,k,(cid:96) + η1,k,(cid:96)

(cid:115)

≤ B1

log(np)
τ nk,(cid:96)

,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

30

and

1
τ (τ0 − τ )n2
k,(cid:96)

(cid:20) (cid:88)

−

τ
(cid:88)

t=1

(cid:12)
(cid:20) (cid:88)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
τ0(cid:88)

(i,j)∈Sk,(cid:96)

X t

i,j(1 − X t−1
i,j )

X t

i,j(1 − X t−1
i,j )

(cid:21)(cid:20) (cid:88)

τ0(cid:88)

(cid:21)
(1 − X t−1
i,j )

(cid:21)(cid:20) (cid:88)

τ
(cid:88)

(i,j)∈Sk,(cid:96)

t=τ +1
(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(1 − X t−1
i,j )

(cid:115)

≤ B1

log(np)
(τ0 − τ )nk,(cid:96)

.

(i,j)∈Sk,(cid:96)

t=τ +1

(i,j)∈Sk,(cid:96)

t=1

Plug these into (A.39) we have with probability larger than 1 − O((np)−(B+2)),

(cid:12)
1,k,(cid:96) − (cid:98)θτ0
(cid:12)(cid:98)θτ
(cid:12)

1,k,(cid:96)

(cid:12)
(cid:12)
(cid:12) ≤

(cid:115)

c0τ (τ0 − τ )n2
k,(cid:96)
τ0τ n2
k,(cid:96)

log(np)
(τ0 − τ )nk,(cid:96)

≤

(cid:115)

√

c0

τ0 − τ
τ0

log(np)
nk,(cid:96)

,

for some constant c0 > 0. Since τ0 (cid:39) O(n), and nk,(cid:96) ≥ s2
constant c1 > 0 such that with probability larger than 1 − O(np)−B),

min, we conclude that there exists a

sup
1≤k≤(cid:96)≤q

(cid:12)
1,k,(cid:96) − (cid:98)θτ0
(cid:12)(cid:98)θτ
(cid:12)

1,k,(cid:96)

(cid:12)
(cid:12)
(cid:12) ≤ c1

(cid:115)

(cid:114) τ0 − τ
τ0

log(np)
ns2

min

.

For (A.32), note that

(cid:98)θτ0
2,k,(cid:96)
θ2,k,(cid:96)

log

= log

− log

(cid:98)θτ
2,k,(cid:96)
θτ
2,k,(cid:96)
1
nk,(cid:96)(n−τ )
τ0−τ
n−τ

(cid:80)

(i,j)∈Sk,(cid:96)
θ1,k,(cid:96)η1,k,(cid:96)
θ1,k,(cid:96)+η1,k,(cid:96)

− log

(cid:80)

1
nk,(cid:96)(n−τ )
τ0−τ
n−τ

(i,j)∈Sk,(cid:96)

η1,k,(cid:96)
θ1,k,(cid:96)+η1,k,(cid:96)

(cid:80)n

t=τ +1 X t

i,j(1 − X t−1
i,j )
η2,k,(cid:96)η2,k,(cid:96)
+ n−τ0
n−τ
θ2,k,(cid:96)+η2,k,(cid:96)
(cid:80)n
t=τ +1(1 − X t−1
i,j )
η2,k,(cid:96)
θ2,k,(cid:96)+η2,k,(cid:96)

+ n−τ0
n−τ

− log

(cid:80)

+ log

(cid:80)

(i,j)∈Sk,(cid:96)

(cid:80)n

t=τ0+1 X t
nk,(cid:96)(n − τ0) · η2,k,(cid:96)η2,k,(cid:96)
θ2,k,(cid:96)+η2,k,(cid:96)

i,j(1 − X t−1
i,j )

(cid:80)n

(i,j)∈Sk,(cid:96)
nk,(cid:96)(n − τ0) ·

t=τ0+1(1 − X t−1
i,j )
η2,k,(cid:96)
θ2,k,(cid:96)+η2,k,(cid:96)

.

It suﬃces to establish a bound for
1
nk,(cid:96)(n−τ )
τ0−τ
n−τ

(i,j)∈Sk,(cid:96)
θ1,k,(cid:96)η1,k,(cid:96)
θ1,k,(cid:96)+η1,k,(cid:96)

∆τ0,τ :=

(cid:80)

(cid:80)n

t=τ +1 X t

+ n−τ0
n−τ

i,j(1 − X t−1
i,j )
η2,k,(cid:96)η2,k,(cid:96)
θ2,k,(cid:96)+η2,k,(cid:96)

−

(cid:80)

(i,j)∈Sk,(cid:96)

(cid:80)n

t=τ0+1 X t
nk,(cid:96)(n − τ0) · η2,k,(cid:96)η2,k,(cid:96)
θ2,k,(cid:96)+η2,k,(cid:96)

i,j(1 − X t−1
i,j )

.

Note that for any B > 0, there exists a large enough constant B2 such that with probability
greater than 1 − O((np)−(B+2)),

|∆τ0,τ | ≤

1
nk,(cid:96)(n−τ )

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:80)

(i,j)∈Sk,(cid:96)

(cid:104)

(cid:80)τ0

t=τ +1
θ1,k,(cid:96)η1,k,(cid:96)
θ1,k,(cid:96)+η1,k,(cid:96)

X t

i,j(1 − X t−1
η2,k,(cid:96)η2,k,(cid:96)
θ2,k,(cid:96)+η2,k,(cid:96)

+ n−τ0
n−τ

τ0−τ
n−τ

i,j ) − θ1,k,(cid:96)η1,k,(cid:96)
θ1,k,(cid:96)+η1,k,(cid:96)

(cid:105)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)





(cid:12)
(cid:12)
(cid:12)
+
(cid:12)
(cid:12)

1
nk,(cid:96)(n−τ )

τ0−τ
n−τ

θ1,k,(cid:96)η1,k,(cid:96)
θ1,k,(cid:96)+η1,k,(cid:96)

+ n−τ0
n−τ

η2,k,(cid:96)η2,k,(cid:96)
θ2,k,(cid:96)+η2,k,(cid:96)

−

1
nk,(cid:96)(n−τ0)
η2,k,(cid:96)η2,k,(cid:96)
θ2,k,(cid:96)+η2,k,(cid:96)





(cid:88)

n
(cid:88)

(cid:104)

(i,j)∈Sk,(cid:96)

t=τ0+1

X t

i,j(1 − X t−1
i,j )

−

θ2,k,(cid:96)η2,k,(cid:96)
θ2,k,(cid:96) + η2,k,(cid:96)

(cid:105)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:115)

≤ B2

τ0 − τ
n − τ

log(np)
(τ0 − τ )nk,(cid:96)

+ B2

τ0 − τ
n − τ

(cid:115)

(A.32) then follows by noticing that τ0−τ
n−τ

log(np)
(n − τ0)nk,(cid:96)
(cid:16) τ0−τ
n−τ

= o

.

(cid:113) log(np)

(τ0−τ )nk,(cid:96)

(cid:17)

.

(cid:113) log(np)

(n−τ0)nk,(cid:96)

31

Appendix B: Real data analysis

B.1 French high school contact data (cont.)

The more details of the data analysis in Section 5.2 are presented below.

We now compare the dynamic stochastic block model method in Matias and Miele (2017)

which is implemented in an R package dynsbm. We note that the model in Matias and Miele

(2017) allows the membership probabilities and the transition parameters to vary over time.

We use the dynsbm package to analyze the same French high school contact data as we reported

in the main text. The function selection.dynsbm can automatically select the number of clusters

by maximizing the so-called integrated classiﬁcation likelihood (ICL) criterion. Figure 7 shows

that the optimal cluster number is selected to be 9 using ICL for this data set. This agrees with

our ﬁndings reported in the main text when using the BIC selection criterion. We then compare

Figure 7: Integrated classiﬁcation likelihood (ICL) and log-likelihood corresponding to diﬀerent

cluster numbers for French high school data.

the detected clusters from dynsbm with the actual class types in Table 9 at all the ﬁve time points.

32

Table 9: Clusters for French high school data by using the model in Matias and Miele (2017).

Class types
t = 1
BIO1
BIO2
BIO3
MP1
MP2
MP3
PC1
PC2
EGI
t = 2
BIO1
BIO2
BIO3
MP1
MP2
MP3
PC1
PC2
EGI
t = 3
BIO1
BIO2
BIO3
MP1
MP2
MP3
PC1
PC2
EGI
t = 4
BIO1
BIO2
BIO3
MP1
MP2
MP3
PC1
PC2
EGI
t = 5
BIO1
BIO2
BIO3
MP1
MP2
MP3
PC1
PC2
EGI

0

1
1
0
3
3
2
4
0
1

0
4
1
0
2
1
3
3
3

5
2
3
0
1
0
6
4
3

2
3
7
1
1
4
4
4
6

3
2
5
1
4
3
4
3
3

1

2

Detected clusters
6
3

5

4

7

8

9

0
0
0
30
0
0
0
0
0

0
0
0
31
0
0
0
0
0

0
0
0
27
0
0
0
0
0

0
0
0
28
1
0
0
0
0

0
0
0
26
0
0
0
0
0

35
0
0
0
0
0
0
0
0

36
0
0
0
0
0
0
0
0

31
0
0
0
0
0
0
0
0

35
0
0
0
0
0
0
0
0

33
0
0
0
0
0
0
0
0

0
0
0
0
0
31
0
0
0

0
0
0
0
0
34
0
0
0

0
0
0
0
0
36
0
0
0

0
0
0
0
0
34
0
0
0

0
0
0
0
0
33
0
0
0

0
0
0
0
0
0
0
0
32

0
0
0
0
0
0
0
0
31

0
0
0
0
0
0
0
0
30

0
0
0
0
0
0
0
0
26

0
0
0
0
0
0
0
0
30

0
1
0
0
26
5
5
1
1

0
0
0
2
27
3
2
1
0

0
0
0
6
28
2
2
0
1

0
0
0
4
27
0
3
0
2

0
0
0
6
25
2
2
0
1

0
0
40
0
0
0
0
0
0

0
0
39
0
0
0
0
0
0

0
0
37
0
0
0
0
0
0

0
0
33
0
0
0
0
0
0

0
0
35
0
0
0
0
0
0

0
0
0
0
0
0
0
38
0

0
0
0
0
0
0
0
35
0

0
0
0
0
0
0
0
35
0

0
0
0
0
0
0
0
35
0

0
0
0
0
0
0
0
36
0

1
31
0
0
0
0
0
0
0

1
29
0
0
0
0
0
0
0

1
31
0
0
0
0
0
0
0

0
30
0
0
0
0
0
0
0

1
31
0
0
0
0
0
0
0

0
0
0
0
0
0
35
0
0

0
0
0
0
0
0
39
0
0

0
0
0
0
0
0
36
0
0

0
0
0
0
0
0
37
0
0

0
0
0
0
0
0
38
0
0

We may notice that dynsbm method reserves one group as “0” for subjects with no edges (the

absence nodes). Our algorithm, in comparison, is not aﬀected by those subjects.

The grouping results from dynsbm method is quite stable over the ﬁve time points. Such results

lend support to our method which assumes the constant cluster structure over time. Furthermore,

33

Table 10: Detected clusters for the French high school data by using our method.

Class types
BIO1
BIO2
BIO3
MP1
MP2
MP3
PC1
PC2
EGI

1
0
0
0
33
0
0
0
0
0

2
0
1
1
0
1
0
44
0
0

3
1
32
0
0
0
0
0
0
0

Detected clusters
5
0
0
39
0
0
0
0
0
0

6
0
0
0
0
0
38
0
0
0

4
0
0
0
0
28
0
0
0
0

7
0
0
0
0
0
0
0
0
34

8
0
0
0
0
0
0
0
39
0

9
36
0
0
0
0
0
0
0
0

our clustering results, shown in Table 10, appear to be more accurate and agree more closely to

the true grouping (class types) for this data analysis.

Another practical advantage of our method is its relatively short computing time. Using a

computer with Intel(R) Core(TM) i7-10875H CPU and 32.0 GB RAM, we need to spend 0.36 and

252.39 seconds to obtain the community detection results with our method and dynsbm method,

respectively.

B.2 Enron email data

The Enron email dataset contains approximately 500,000 emails generated by employees of the

Enron Corporation. It was obtained by the Federal Energy Regulatory Commission during its

investigation of Enron’s collapse. The data ﬁle was published at https://www.cs.cmu.edu/~.
/enron/.

Rastelli et al. (2017) developed a latent stochastic block model for directed dynamic network

data. They applied their methods for the Enron email data. To compare with their results, we

used data for all the emails exchanged from January 2000 to March 2002 (n = 27) between the

Enron members. The number of nodes in our analysis is 184 (which is diﬀerent from Rastelli

et al. (2017) where they have kept 148 subjects).

Using our spectral clustering algorithm for the whole network data and using the BIC, we

obtain the best number of cluster is 13. Rastelli et al. (2017) used the exact ICL criterion and

found 17 clusters but 4 groups seem to be extremely small or just contain inactive nodes. Their

algorithm is for directed graph and also assume time-varying membership. It seems our results

are still quite close to Rastelli et al. (2017). Similar to their analysis, we can see that there is a

high degree of heterogeneity with 13 diﬀerent clusters for this data set. When applying dynsbm

package on this data the optimal number of cluster is only 6.

Next we considered the change point analysis. Using our binary segmentation methods, we

detect one change point at October 2001 which is exactly the month corresponding to the disclo-

34

sure of Enron bankruptcy (see the log-likelihood functions plotted in Figure 8). This again agrees

with the empirical ﬁndings in Rastelli et al. (2017).

Figure 8: Log-likelihood functions corresponding to diﬀerent change points in time for Enron

email data.

35

