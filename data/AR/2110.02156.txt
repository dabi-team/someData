Bayesian autoregressive spectral estimation

Alejandro Cuevas1

Sebasti´an L´opez1

Danilo Mandic2

Felipe Tobar3,4

1Department of Mathematical Engineering, Universidad de Chile
2Department of Electrical and Electronic Engineering, Imperial College London
3Center for Mathematical Modeling, Universidad de Chile
4Initiative for Data & Artiﬁcial Intelligence, Universidad de Chile

1
2
0
2

t
c
O
5

]
P
S
.
s
s
e
e
[

1
v
6
5
1
2
0
.
0
1
1
2
:
v
i
X
r
a

Abstract—Autoregressive (AR) time series models are widely
used in parametric spectral estimation (SE), where the power
spectral density (PSD) of the time series is approximated by that
of the best-ﬁt AR model, which is available in closed form. Since
AR parameters are usually found via maximum-likelihood, least
squares or the method of moments, AR-based SE fails to account
for the uncertainty of the approximate PSD, and thus only yields
point estimates. We propose to handle the uncertainty related to
the AR approximation by ﬁnding the full posterior distribution
of the AR parameters to then propagate this uncertainty to the
PSD approximation by integrating out the AR parameters; we
implement this concept by assuming two different priors over
the model noise. Through practical experiments, we show that
the proposed Bayesian autoregressive spectral estimation (BASE)
provides point estimates that follow closely those of standard
autoregressive spectral estimation (ASE), while also providing
error bars. BASE is validated against ASE and the Periodogram
on both synthetic and real-world signals.

Index Terms—Spectral estimation, autoregressive models,

Bayesian inference, Markov chain Monte Carlo

I. INTRODUCTION

In Signal Processing, spectral estimation (SE) refers to
the determination of the energy that is contributed by each
component in a time series. This work focuses, in particular, on
the spectral representation given by the Fourier power spectral
density (PSD). In practice, the challenge is to estimate the
PSD from a set of noisy observations of the time series; this
is seen in applications such as biomedical engineering [1] and
telecommunications [2]. Classical methods for computing the
PSD can be roughly divided in two categories: parametric
and nonparametric. Parametric methods impose a generative
model on the data such as autoregressive moving average
(ARMA) models [3], sums of sinusoids (Lomb-Scargle [4]),
Bernstein polynomials [5] or Gaussian mixtures [6]. On the
other hand, nonparametric methods for spectral estimation do
not assume any structure on the data and are thus more ﬂexible,
yet they do not discriminate the signal from the noise, which
critically affects the spectral estimates. Typical examples of
the nonparametric SE methods are the classic Periodogram
[7] which can be considered as a histogram of frequencies
as well as recent Bayesian nonparametric approaches [8], [9],
[10].

Even though the randomness conveyed by time series ob-
servations is evident, dealing with the uncertainty related to
the data-driven computation of the PSD is not a standard

practice within SE. In parametric approaches, the model ﬁt
to the data through an optimisation procedure, e.g. using
least squares or maximum likelihood. Therefore, the PSD is
approximated deterministically, by a point estimate, and all the
information about the mismatch between the chosen model and
the data, which could lead to probabilistic PSD estimates, is
unfortunately usually neglected.

To address this void in the SE literature, which is particu-
larly detrimental in noisy data and model mismatch, we aim
to cater for uncertainty in PSD estimates ﬁnding a distribution
over PSDs, rather than point estimates. This can be achieved
by equipping the existing framework for parametric spectral
estimation with a Bayesian treatment. Instead of committing
to a point-estimate of parameters of the time-series model,
we can ﬁnd their posterior distribution, thus constructing a
posterior distribution over models for time series. Then, we can
transport this distribution via the Fourier transform to ﬁnd the
posterior distribution over PSDs, where posterior in this case
is in the sense of conditional to the time series observations.
This rationale becomes of particular importance for model
misspeciﬁcation, since, strictly speaking, a parametric model
may always be considered to be misspeciﬁed for real data and
therefore the distribution of models allows us to account for
the limited capacity of the parametric representation.

In this work, we apply this generic idea to the particular
class of autoregressive time series models, thus our proposed
method is referred to as Bayesian autoregressive spectral
estimation (BASE). In particular, we propose two models,
termed Model I and Model II which difference stems from
the noise prior considered: Model I uses a half-Normal prior
while Model II uses an Inverse-Gamma (and thus conjugate)
prior. Both models assume a normal prior over the AR weights
and Model I is implemented using Markov chain Monte Carlo
(MCMC) [11] due to the nonconjugate prior. Via numerical
simulations, we validate the proposed BASE method and the
two models proposed on two synthetic signals and a real
world example, these experiments show the advantage of the
proposed BASE in different settings against its deterministic
counterparts.

 
 
 
 
 
 
II. BACKGROUND: PARAMETRIC SPECTRAL ESTIMATION
USING AN AR MODEL
A discrete-time sequence {xt}t∈N is called a p-order au-
toregressive process, denoted AR(p), if it is given by the
relationship [7]

xt =

p
(cid:88)

k=1

akxt−k + (cid:15)t,

(1)

where p ∈ N, [a1, . . . , ap] ∈ Rp are the (autoregressive)
parameters and {(cid:15)t}t∈N is a noise process. We consider the
noise to be Gaussian independent and identically distributed
random variables with zero mean and variance σ2.

A. Power spectral density of AR(p) processes

AR(p) processes are ubiquitous in parametric spectral es-
timation as they have closed-form Fourier PSDs [12] given
explicitly by the AR parameters. Speciﬁcally, using the Z-
transform [3] the PSD of AR(p) can be expressed by

σ2

(2)

i.e.,

Sx(ξ) =

k=1 ake−i2πξk|2 .

|1 − (cid:80)p
The modes (or peaks) of the PSD in eq. (2),

the
frequencies where the AR process convey more power, are
given by the roots of the denominator. These roots are the poles
of the dynamical system that generates the AR(p) process
{xt}t∈N. Since we can regard the model order p as the degrees
of freedom of the parametric PSD for the AR model, the
larger the p the more ﬂexible the PSD. This is in line with
the interpretation in the temporal domain, where a model with
more lags can cater for more complex time series and thus
represents a more general model.

The fact that the PSD above is directly parametrised by
{a1, . . . , ap, σn} is perhaps the main motivation for using the
AR model for spectral estimation. However, even though there
have been Bayesian approaches to ﬁt the AR parameters they
have not been applied to the SE problem. In practice, the
literature shows that the use of AR parameters in SE has
been mainly deterministic, e.g., by computing the PSD in
eq. (2) with a plug-in approximation of the AR parameters
such as those found by least squares, the method of moments,
or maximum likelihood. Such an approach, does not allow
for modelling the uncertainty related to the peaks of the
resulting PSD, due to noisy data, or to account for model
misspeciﬁcation (i.e., incorrect model order) which can lead
to either peak splitting (over-estimation) or onto fewer peaks
(under-estimation).

III. BAYESIAN SPECTRAL ESTIMATION OF
AUTOREGRESSIVE SIGNALS

Our aim is to compute the posterior distribution of the
PSD for the AR case, thus providing a natural account for
model uncertainty, which arises from i) the consideration of
a ﬁnite dataset, ii) model mismatch, iii) noisy observations.
From now on, we will refer to the standard (deterministic)
autoregressive spectral estimation approach as ASE and to the
proposed Bayesian counterpart as BASE.

A. Generic Bayesian parametric spectral estimation

The posterior distribution of Sx, the PSD of a stationary
stochastic process {x}t∈N, conditional to a set of observations
x = [x1, x2, . . . , xn], can be computed by integrating out the
model of the time series. Denoting the space of all possible
time series models by M, the posterior PSD is given by

p(Sx|x) =

=

(cid:90)

M

(cid:90)

M

p(Sx, M |x)dM

(3)

p(Sx|M )p(M |x)dM,

where the last expression only takes the (reasonable) as-
the PSD, Sx, and the observations, x, are
sumption that
conditionally independent given the model M . This follows
from the fact that the data cannot say more about the PSD
than the model itself.

Notice that the above integral is deﬁned over the entire,
possibly inﬁnite-dimensional, model space M. To calculate
this integral we need to assume a structure for the time-series
models to be considered; this can be achieved by choosing
a ﬁnite-dimensional prior over the models, namely p(M ). A
straightforward way to choose this prior is to ﬁrst assume a
parametrisation over models, say M = Mθ, θ ∈ Rd, and then
choose a prior over the parameters p(θ). This construction
is know as the push-forward measure [13] that transports a
distribution over the ﬁnite-parameter, θ ∈ Rd, towards the
space of models through the mapping θ (cid:55)→ Mθ.

With this parametrisation, the posterior in eq. (3) can be
expressed with respect to the model parameters, θ, via the
change of variable theorem as

p(Sx|x) =

(cid:90)

Θ

p(Sx|θ)p(θ|x)dθ,

(4)

where the integration is now performed over the (ﬁnite)
parameter space, that is, Θ = Rd.

Under the assumption that the process {xt}t∈N is stationary
and given by a model Mθ, its PSD is uniquely deﬁned by
the model’s parameters θ and thus the distribution p(Sx|θ) is
a Dirac measure supported on a single points on the space
of PSDs. Therefore, sampling from the posterior over PSDs,
p(Sx|x), is straightforward: simply sample a parameter from
the posterior distribution over parameters θ∗ ∼ p(θ|x), and
then map this parameter sample to PSDs according to

θ∗ → Mθ∗ → Sx = PSD(Mθ∗ ),

(5)

where PSD(M ) denotes the PSD corresponding to the model
M .

In order to perform this procedure, we need to choose: i) a
model space Mθ, ii) a prior over the parameters p(θ), and iii)
a sampling procedure. We refer to these in the remaining part
of this section.

B. Model I: Half-normal noise

We shall choose the model space as that of AR models
owing to their appealing properties for spectral estimation
outlined in Section II-A. Furthermore, we will assume that

all parameters in the AR model are independent, that a1:p,
p ∈ N, are Normally distributed and the noise variance σ2
follows a half-Normal prior. Thus, the priors are

p(a1:p) = MN (0, σ2
p(σ2) = half-N (0, σ2

aIp)
(cid:15) ),

(6)

(7)

where the half-Normal distribution corresponds to the multipli-
cation between a Normal distribution and an indicator function
1{σ2≥0}, and then properly renormalised (i.e., multiplied by
2).

The choice of normality for the autoregressive parameters
follows from the fact that the Gaussian prior is conjugate to
the (conditional) likelihood as veriﬁed in the next section, and
thus the posterior is also normally distributed.

Regarding the deﬁnition of the likelihood, given a set of
observations {x1:T }, T ∈ N consider the conditional likeli-
hood of the AR(p), that is, the expression p(xp+1:T |x1:p, θ),
where θ = [a1, . . . , ap, σ2](cid:62) denotes all model parameters and
we consider {x1:p} to be ﬁxed and not part of the generative
model for simplicity of presentation.

Following eq. (1), the conditional likelihood of the AR(p)

model is given by

p(xp+1:T |x1:p, θ) =

T
(cid:89)

p(xτ |xτ −1:τ −p, θ)

(8)

τ =p+1

T
(cid:89)

τ =p+1

=

√

1
2πσ2

exp

(cid:18) −(xτ − a(cid:62)xτ −1)2
2σ2

(cid:19)

,

where we have adopted the notation a = [a1, . . . , ap] and
xτ −1 = [xτ −1, . . . , xτ −p].

The conditional

likelihood is thus Gaussian on the AR
parameter vector a, meaning that
the Gaussian prior on
a established on the previous section is conjugate to the
conditional likelihood and results on the following Gaussian
posterior

p(θ|x1:p, xp+1:T ) =

p(xp+1:T |x1:p, θ)p(θ)
p(xp+1:T , x1:p)
∝ p(xp+1:T |x1:p, θ)p(a)p(σ2)

T
(cid:89)

∝

τ =p+1

· exp

exp

1
σ
(cid:18) −1
2σ2
a

(cid:18) −(xτ − a(cid:62)xτ −1)2
2σ2
(cid:18) −σ2
2σ2
(cid:15)

exp

(cid:19)

(cid:19)

.

aT a

(9)

(cid:19)

Remark. Since the complete (rather than conditional) likeli-
hood is given by

p(x1:T |, θ) = p(xp+1:T |x1:p, θ)p(x1:p|θ),

(10)

it requires calculating the distribution p(x1:p|θ); this is chal-
involves the inﬁnite-order moving-average
lenging since it
the AR(p) process. Therefore, under the
representation of
assumption that the ﬁrst p observations are ﬁxed they become
independent
from θ, and the density p(x1:p|θ) becomes a
constant for theta in eq. (10). A similar reasoning applies

for the marginal distribution p(xp+1:T , x1:p). Notice that the
discrepancy between the conditional and complete likelihoods
becomes negligible for increasing amounts of data (i.e., T (cid:29)
1). For this reason, we will consider only the conditional
likelihood.

With this choice, generating a sample, θ∗, from the posterior
requires the evaluation of p(xp+1:T |x1:p, θ)p(a)p(σ2). Then,
this sample can be used to compute the PSD explicitly, ac-
cording to eq. (2). This procedure avoids explicit computation
of the model Mθ, since the PSD is computed directly from
the parameters.

Notice that, by construction, all the posterior PSD samples
will have the form as in eq. (2) which means that no probability
will be assigned to expressions that do not follow this form.
This advantage of the parametric approach to SE results in
that, using a set of samples of PSDs constructed as explained
above, we can numerically compute the mean PSD and error
bars conditional to a set of signal values x.

C. Model II: Inverse-Gamma noise

In order to derive a closed-form expression for the posterior
over the AR parameters, we can assume the noise variance
σ2 follows an Inverse-Gamma prior distribution, rather than
the Half-Normal one in the previous section. This posterior is
known as the Normal-Inverse-Gamma distribution.
Speciﬁcally, let us consider the following priors

p(a1:p|σ2) = MN

(cid:18)

µ0,

(cid:19)

σ2
λ

Ip

p(σ2) = Γ−1(α, β),

(11)

(12)

where Ip denotes the p−dimensional identity matrix. Then
the posterior distribution for θ = (a1:p, σ2) is proportional to
a Normal-Inverse-Gamma:

where, using the notation Xp+1:T = [x(cid:62)

p(θ|x1:T ) ∝ N Γ−1(µ, Σ−1, α, β),
i−1:i−p]T

i=p+1,

µ = Σ(λµ0 + X (cid:62)

p+1:T xp+1:T )

p+1:T Xp+1:T

Σ−1 = λIp + X (cid:62)
T − p
2
0 µ0 + x(cid:62)
λµ(cid:62)

α = α +

β = β +

p+1:T xp+1:T − µ(cid:62)Σ−1µ

2

(13)

.

From this Norma-Inverse-Gamma posterior, we can sample
the parameters (a1:p, σ2) simply by specifying the hyper-
parameters (µ0, λ, α, β) and using standard sampling tech-
niques. Our conjecture is that avoiding the MCMC stage
required above can result in faster posterior computations.

The MAP estimators for the parameters (a1:p, σ2) are given

by

(cid:32)

aM =

λIp +

m
(cid:88)

X (cid:62)X

(cid:33)−1 (cid:32)

λµ0 +

(cid:33)

m
(cid:88)

X (cid:62)x

(14)

i=1

2β + λ||aM − µ0||2 + (cid:80)m

i=1
i=1 ||x − XaM ||2

2(α + 1) + mT + (m − 1)p

σ2
M =

,

(15)

where recall our notation X := Xp+1:T and x := xp+1:T
and we have assumed that we have m observations of the
signal (though in practice usually only m = 1 observation is
available).

In this model, the hyperparameters can be chosen using a
Grid search and cross-validation with a 5-fold split: for each
set of hyperparameters, compute the MAP estimate on the train
split, to then score it on the test split using the likelihood given
that MAP for a1:p and σ2 as the score function. The ﬁnal score
assigned to the hyperparameters is the average of the scores
obtained on each fold. Lastly, the set of hyperparameters with
the best score will be used in the posterior.

The grid to be used for the hyper-parameters is

Fig. 2. Log-PSD estimate for a synthetic AR(4) signal using the BASE
method, true PSD (dashed black line), Periodogram (blue line), ASE (green
line), and proposed BASE (red line, 95% error bars in light red).

λ, α, β ∈ {0.1, 1, 10, 100}








µ0 ∈





−10
−10
...
−10






,











, . . . ,











−8
−8
...
−8












10
10
...
10

(16)

⊂ Rp

(17)

IV. SIMULATIONS

We validated the proposed Bayesian autoregressive spectral
estimation method, BASE, over three case studies, two for
synthetic data and one for real world data. The ﬁrst one
considered a synthetic AR signal, where the true and approx-
imate model orders were different, this simulation is aimed
to show that BASE is robust to model misspeciﬁcation due
to the chosen prior. The second experiment illuminates the
unbiasedness and concentration properties of the approximate
posterior over PSDs by applying BASE on a continuous-
time signal generated by a Gaussian process (GP) [14] with
known PSD. Finally, the third experiment validates the ability
of the proposed BASE model to detect periodicities from
a sub-sampled real world astronomical time series. We also
compare, over all the case studies, the performance of the
method in both versions: the Model I in Sec. III-B based on
a half-Normal noise prior (using MCMC) and the Model II in
Sec. III-B based on the conjugate Inverse-Gamma noise prior
with closed-form posterior. In the experiments using MCMC
we used a NUTS sampler [15] in the PyMC3 Python toolbox
[16].

A. Misspeciﬁed model order: AR-generated time series

We implemented a stable AR(4) time-series to generate
data that was later processed by BASE (Model I) with the
assumption of a AR(10) model. Fig. 1 shows the true values
of the regression coefﬁcients and their approximate poste-
rior using MCMC; notice that these posteriors contain the
true AR(4) coefﬁcients, and they also converge to narrow
posteriors centred around zero for higher orders coefﬁcients.
This validates the self-regularisation property, or robustness
to misspeciﬁed model order, within the proposed BASE-
framework which will be key when computing the sample
PSDs.

Fig. 1. Approximate posterior distributions over AR parameters using MCMC
(NUTS) and the true AR(4) values shown in yellow stars.

Next, using the approximate posterior over the AR parame-
ters (coefﬁcients and variance), we generated PSD samples
according to eq. (2). Fig. 2 shows the density over PSDs
using the proposed BASE (95% error bars),
the standard
autoregressive spectral estimation (ASE), the true PSD and
the Periodogram. Notice that the proposed BASE succeeded
in estimating the true PSD while remaining unbiased and
concentrated around the true value.

B. Continuous-time signal: Gaussian process with Laplace
covariance

Let us consider a synthetic signal generated from a Gaussian
process (GP) [14] with Laplace covariance function given by

K(τ ) = σ2 exp(−|τ |/l),

(18)

where σ2 denotes the marginal covariance and l is known
as the process lenghtscale. Furthermore, we considered zero-
mean Gaussian noise added to the GP sample. Recall that a
sample from a GP can be regarded as a signal generated by
an AR(∞) model.

The proposed BASE (Model I) was implemented on this
synthetic signal with an AR(4) model, the order of which was
chosen from the rate of decay of the Laplace kernel. Then, the
PSD estimate of the BASE model was compared against those
of the standard ASE of the same order and the Periodogram.
Fig. 3 shows the PSD estimates (95% error bars for BASE)
as well as the true PSD of the signal, given by the Fourier
Transform of the Laplace kernel. Notice that the PSD posterior
within BASE was concentrated around the true PSD and was
able to capture the mean of its deterministic counterpart.

0.000.250.50Frequency303PSDPosterior of the Log-PSD for Synthethic ARPeriodogramASEBASETrue PSDa1a2a3a4a5a6a7a8a9a10AR coef202Estimation of AR(4) parametersReal valuesFig. 3. Spectral estimation of a GP trajectory: BASE shown in red, true PSD
in dashed black, Periodogram in blue and ASE in green. The ﬁgure only
shows the region [0, 0.05], since this is where almost all the spectral content
of the data is contained.

C. Finding the periodicity of an astronomical time series

For the third experiment, we considered a real-world time
series which may not be an AR process and therefore its
PSD will not be precisely given by the expression in eq. (2).
This experiment was constructed to validate the Bayesian
approach to handle model misspeciﬁcation by averaging over
the posterior models. The signal considered was the sunspots
dataset [17], known for having a period of 11 years (frequency
≈ 0.0909[years−1]), we implemented BASE (Model I) to
detect this periodicity only using 1/6 of the available data.

We ﬁrst computed both the autocorrelation (ACF) and
partial autocorrelation (PCF) functions of the sunspots series.
Fig. 4 shows the ACF and PCF, and show that due to the slow
decay of PCF, the order of the autoregressive component of
the signal cannot be speciﬁed with any certainty; this supports
the evidence that the sunspot series is not an AR process. We
implemented BASE (Model I) with an AR(9) model to then
compare it against the ASE and the Periodogram, where ASE
and the Periodogram used all the available data. Fig. 5 shows
the peak estimates of the PSDs using BASE, where the main
peak (colour-coded in red) is 0.09181278[years−1] and thus
in line with the expected frequency peak. Fig. 6 shows the
PSD estimates for the three models, with the proposed BASE
model exhibiting its peak near the correct known period of the
sunspots signal, as it was expected from Fig. 5.

Notice that the errors bars of BASE are now larger than
those of the previous experiments, this is because the signal
considered in this experiment is not coming from an AR
process and thus we are in the scenario of model mismatch. In
this sense, due to the fact that BASE averages over possible
AR models, its error bars are larger but they contain other
peaks of the Periodogram. Finally, observe that despite the
fact that an order p = 9 for the model was chosen, this did
not result in multiple peaks which validates BASE’s robustness
to overﬁtting arising from its Bayesian nature.

D. Performance of the closed-form posterior

BASE (Model II) was also implemented in the same experi-
ments described above for comparison. Since this model has a
closed form posterior, we refer to it as “CF” in the remaining
part of this section.

Fig. 4.
autocorrelation functions.

Sunspots time series (top) alongside autocorrelation and partial

Fig. 5. Peaks of the PSD computed by BASE for the sunspots time series.
The red stem denotes the main peak.

Following the experiment in Sec. IV-A, the proposed CF
method was implemented on the AR(4) synthetic time series,
where the real parameters are known. Fig. 7 shows the PSD
estimate (95% error bars) along with the ASE estimation, the
true PSD and the Periodogram. By comparing these results
with those shown in Fig. 2, note that the estimation of the mean
is only slightly better, likewise, the variance of the estimation
is slightly reduced.

Next, we implemented CF on the same synthetic Gaussian
process signal generated in IV-B. Fig. 8 shows the PSD
estimates (95% error bars for CF) as well as the true PSD
of the signal, given by the Fourier Transform of the Laplace
kernel. The estimates provided by the CF method (BASE
Model II) are very similar to those of BASE Model I, which
uses MCMC, as they also concentrates around the true PSD
and its mean captures the estimation of the ASE. Notice that
the error bars are slightly better for CF than those given by
BASE Model I. This suggest that the posterior distribution is
more concentrated on the true model.

Lastly, following Sec. IV-C we validated the CF model on
real data by testing it on the sunspots data-set [17]. Fig. 9
shows the results with the CF approach (BASE Model II),
which again exhibits slightly less variance than BASE Model
I using MCMC. Although the peak frequency found by CF
has shifted to the left,
the posterior PSD is still precise
and concentrated on the true peak (found by computing the
Periodogram over the entire dataset). This suggest that the CF
could also recover the fundamental frequency of the data from
a limited number of observations.

0.0000.0250.050Frequency0.00.51.0PSDPSD posterior for GP with Laplace kernelPeriodogramASEBASETrue PSD1750180018501900195020000200Sunspots Dataset05101520253035Lags01Autocorrelation05101520253035Lags01Partial Autocorrelation0.00.20.40.60.81.0Frequency0.000.250.500.75Spectral peaks estimation with rootsFig. 6. PSD estimates for the sunspots series: BASE (95% error bars),
ASE and Periodogram. The frequency of the well-known 11-year period is
illustrated by a vertical dashed black line.

Fig. 8. Spectral estimation of a GP trajectory: CF shown in red, true PSD in
dashed black, Periodogram in blue and ASE in green. The ﬁgure only shows
the region [0, 0.05], since this is where almost all the spectral content of the
data is contained.

Fig. 7. Log-PSD estimate for a synthetic AR(4) signal using the CF method;
True PSD (dashed black line), Periodogram (blue line), ASE (green line), and
proposed CF (red line, 95% error bars).

Fig. 9. PSD estimates for the sunspots series: CF (95% error bars), ASE and
Periodogram. The frequency of the well-known 11-year period is illustrated
by a vertical dashed black line.

V. DISCUSSION AND FUTURE WORK

We have proposed a novel

framework termed BASE,
a Bayesian approach to autoregressive spectral estimation.
BASE exploits the closed-form properties of the AR model
to compute power spectral densities (PSD), and also comple-
ments it with a probabilistic treatment, thus allowing us to
quantify uncertainty in the estimates of the PSDs through the
use of conﬁdence intervals.

The description of the proposed method follows the idea
that we can sample from the posterior distribution over PSDs
(given a set of observations of the time series) by (i) sampling
the posterior parameters of an AR model, to then (ii) compute
the corresponding PSD of such sample parameter. In this
setting, we proposed two models: both assume a multivariate
normal prior on the AR parameters by they differ in that
one assumes a half-Normal prior for the noise and the other
one adopts an Inverse-Gamma; we dubbed them Model I and
Model II respectively. BASE Model I uses MCMC to ﬁnd the
posterior samples while Model II features a conjugate prior
and this results in a closed-form posterior.

i) data generated by an AR model,

Both BASE models have been implemented on three ex-
ii)
periments using:
data generated by a Gaussian process with Laplace kernel,
and iii) a real-world astronomical time-series. Through these
simulations, we have validated BASE in terms of robustness
to model misspeciﬁcation, unbiasedness, accuracy, and peri-
odicity detection. Regarding the comparison between Model I
and Model II, we can say that, Model II gives posteriors that
are tighter than those of Model I, arguably due to its use of a

closed from posterior. Furthermore, the main feature of Model
II is the reduction on the computation time due to avoiding
the use of MCMC. The main setback of Model II, however,
is that it requires the use of a speciﬁc prior.

Future work includes: i) heuristics to determine the hyper-
parameters µ0, λ, α, β, ii) a quantitative assessment of the
reduction in computational complexity provided by Model II,
iii) a development of a hierarchical prior to simultaneously
identify the AR model order (e.g., a Dirichlet prior), and
iv) extensions to ARMA models to cater for moving-average
spectral components and spectra with zeros, iv) comparisons
against nonparametric models such as [18].

ACKNOWLEDGEMENTS

This work was funded by Fondecyt-Regular 1210606,

ANID-AFB170001 (CMM) and ANID-FB0008 (AC3E).

REFERENCES

[1] C. Park, D. Looney, P. Kidmose, M. Ungstrup, and D. P. Mandic, “Time-
frequency analysis of EEG asymmetry using bivariate empirical mode
decomposition,” 2011, vol. 19, pp. 366–373.

[2] G. Vazquez-Vilar, R. L´opez-Valcarce, C. Mosquera, and N. Gonz´alez-
Prelcic, “Wideband spectral estimation from compressed measurements
exploiting spectral a priori information in cognitive radio systems,” in
Proc. of IEEE ICASSP, 2010, pp. 2958–2961.

[3] R. Davis and P. Brockwell, Time Series: Theory and Methods, Springer-

Verlag New York, 1991.

[4] N. R. Lomb, “Least-squares frequency analysis of unequally spaced

data,” Astrophysics and Space Science, p. 447–462, 1976.

[5] N. Choudhuri, S. Ghosal, and A. Roy, “Bayesian estimation of the
spectral density of a time series,” Journal of the American Statistical
Association, vol. 99, no. 468, pp. 1050–1059, 2004.

0.000.050.100.150.20Frequency03060PSDPosterior of the PSD for SunspotsPeriodogramASEBASE0.000.250.50Frequency303PSDPosterior of the Log-PSD for Synthethic ARPeriodogramASECFTrue PSD0.0000.0250.050Frequency0.00.51.0PSDPSD posterior for GP with Laplace kernelPeriodogramASECFTrue PSD0.000.050.100.150.20Frequency03060PSDPosterior of the PSD for SunspotsPeriodogramASECF[6] G. Parra and F. Tobar,

“Spectral mixture kernels for multioutput
Gaussian processes,” in Advances in Neural Information Processing
Systems 30, 2017, pp. 6681–6690.

[7] D. Williams and V. Madisetti, Eds., Digital Signal Processing Hand-
book, CRC Press, Inc., Boca Raton, FL, USA, 1st edition, 1997.
[8] F. Tobar, T. Bui, and R. Turner, “Learning stationary time series using
Gaussian processes with nonparametric kernels,” in Advances in Neural
Information Processing Systems 28, 2015, pp. 3483–3491.

[9] F. Tobar, “Bayesian nonparametric spectral estimation,” in Advances in
Neural Information Processing Systems 31, 2018, pp. 10148–10158.

[10] Y. Wang, R. Khardon, and P. Protopapas, “Nonparametric Bayesian
estimation of periodic light curves,” The Astrophysical Journal, vol.
756, no. 1, pp. 67, aug 2012.

[11] C. Andrieu, N. de Freitas, A. Doucet, and M. Jordan, “An introduction
to MCMC for machine learning,” Machine Learning, vol. 50, no. 1, pp.
5–43, 2003.

[12] G. Box and G. Jenkins, Time Series Analysis, Forecasting and Control,

1990.

[13] P. R. Halmos, Measure Theory, Graduate Texts in Mathematics. Springer

New York, 1976.

[14] C. Rasmussen and C. Williams, Gaussian Processes for Machine

Learning, The MIT Press, 2006.

[15] M. D. Hoffman, A. Gelman, et al., “The No-U-Turn sampler: adaptively
setting path lengths in Hamiltonian Monte Carlo,” Journal of Machine
Learning Research, vol. 15, no. 1, pp. 1593–1623, 2014.

[16] J. Salvatier, T. V. Wiecki, and C. Fonnesbeck, “Probabilistic program-
ming in Python using PyMC3,” PeerJ Computer Science, vol. 2, pp.
e55, 2016.

[17] SILSO World Data Center,

“The International Sunspot Number,”
International Sunspot Number Monthly Bulletin and online catalogue,
1749-2017.

[18] F. Tobar, L. Araya-Hern´andez, P. Huijse, and P. M. Djuri´c, “Bayesian re-
construction of Fourier pairs,” IEEE Transactions on Signal Processing,
vol. 69, pp. 73–87, 2021.

