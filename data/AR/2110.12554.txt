Draft version October 26, 2021
Typeset using LATEX default style in AASTeX631

1
2
0
2

t
c
O
4
2

]

R
S
.
h
p
-
o
r
t
s
a
[

1
v
4
5
5
2
1
.
0
1
1
2
:
v
i
X
r
a

Implementation paradigm for supervised ﬂare forecasting studies:
a deep learning application with video data∗

Sabrina Guastavino,1 Francesco Marchetti,2 Federico Benvenuto,3 Cristina Campi,1 and Michele Piana4

1The MIDA group, Dipartimento di Matematica, Universit`a di Genova,
Via Dodecaneso, 35, Genova, Italy
2Istituto Nazionale di Alta Matematica (INdAM) - Dipartimento di Matematica “Tullio Levi-Civita”, Universit`a di Padova,
Via Trieste, 63, Padova, Italy
3The MIDA group, Dipartimento di Matematica, , Universit`a di Genova,
Via Dodecaneso, 35, Genova, Italy
4The MIDA group, Dipartimento di Matematica, Universit`a di Genova and CNR - SPIN Genova,
Via Dodecaneso, 35, Genova, Italy

ABSTRACT

Solar ﬂare forecasting can be realized by means of the analysis of magnetic data through artiﬁcial
intelligence techniques. The aim is to predict whether a magnetic active region (AR) will originate
solar ﬂares above a certain class within a certain amount of time. A crucial issue is concerned with the
way the adopted machine learning method is implemented, since forecasting results strongly depend
on the criterion with which training, validation, and test sets are populated. In this paper we propose
a general paradigm to generate these sets in such a way that they are independent from each other
and internally well-balanced in terms of AR ﬂaring eﬀectiveness. This set generation process provides
a ground for comparison for the performance assessment of machine learning algorithms. Finally, we
use this implementation paradigm in the case of a deep neural network, which takes as input videos
of magnetograms recorded by the Helioseismic and Magnetic Imager on-board the Solar Dynamics
Observatory (SDO/HMI). To our knowledge, this is the ﬁrst time that the solar ﬂare forecasting
problem is addressed by means of a deep neural network for video classiﬁcation, which does not
require any a priori extraction of features from the HMI magnetograms.

Keywords: Astronomy data analysis (1858); Neural networks (1933); Solar ﬂares (1496); Solar activity

(1475); Solar active region magnetic ﬁelds (1975)

1. INTRODUCTION

The problem of solar ﬂare forecasting can be deﬁned as a binary prediction problem, in which the aim is to predict
whether, within a certain amount of time, an active region (AR) will originate solar ﬂares of above a given ﬂare class
according to the GOES classiﬁcation (Schrijver & Siscoe 2010). The two main strategies to address this problem rely
on either statistical methods (Song et al. 2009; Mason & Hoeksema 2010; Bloomﬁeld et al. 2012; Barnes et al. 2016)
or machine learning algorithms (see (Georgoulis et al. 2021) and references therein).

Approaches formulated within the machine learning framework typically need three ingredients: a supervised classi-
ﬁcation algorithm for classiﬁcation, a historical data set for its training, and a score for the assessment of performance.
However, one of the most intriguing aspects of the ﬂare forecasting game addressed with machine learning is that the
studies performed so far lead to signiﬁcantly diﬀerent skill scores, although they are applied to the same data archive.
Just as a very partial example, Table 1 contains the performance outcomes of twelve ﬂare forecasting studies realized
by means of machine learning approaches as applied to observations provided by the same space instrument, the
Helioseismic and Magnetic Imager on-board the Solar Dynamics Observatory (SDO/HMI). For each one of the twelve
studies, the table reports whether a conﬁdence interval on the skill score is computed; whether data belonging to the
same AR are split between training and test set; whether a validation set is exploited to optimize the machine learning

∗ Released on March, 1st, 2021

 
 
 
 
 
 
2

Guastavino et al.

algorithms; which is the machine learning algorithm applied; which are the values obtained for a speciﬁc skill score in
the case of the prediction of ﬂares with GOES class higher than C (C1+ ﬂares) and M (M1+ ﬂares), respectively. In
our opinion, the reason for the heterogeneity of the skill score values in the table is in the fact that there is no general
agreement among ﬂarecasters about a validation strategy for the prediction methods. Indeed, given a speciﬁc historical
archive, these twelve methods generate the training, validation, and test sets according to completely diﬀerent rules;
further, not all methods compute a conﬁdence interval in order to assess the statistical reliability of the scores; and,
ﬁnally, not all methods distinguish between validation and testing.

The ﬁrst objective of the present paper is to propose a general paradigm for the implementation and assessment of
ﬂare forecasting processes based on supervised machine and deep learning approaches. Speciﬁcally, we believe that
this kind of strategy should suggest a common perspective about two speciﬁc issues: the way data preparation is
realized, with speciﬁc focus on the way the historical data set is split into a training set, a validation set and a test
set; and the way the prediction performances are presented, with speciﬁc focus on the computation of the statistical
reliability of results. As far as the ﬁrst issue is concerned, we propose a standardized approach to data splitting based
on an accurate deﬁnition of data sample, which accounts for the uniformity of training, validation and test sets with
respect to both ﬂare classes, and the diﬀerent possible typologies of null events (this standardized approach should
also account for the fact that ARs can be represented by diﬀerent types of data such as point-in-time vectors or time
series of physical features, and HMI images or videos). As far as the second issue is concerned, we point out that any
machine learning algorithm should be repeatedly trained on data subsets generated by means of random extractions
of ARs from the HMI archive, in order to associate a conﬁdence interval to the skill score values computed on the test
set.

The second objective of this paper is to present, for the ﬁrst time, a deep learning technique that takes as input
HMI videos and provides as output a binary prediction with no intermediate processing of the computed features. Our
technique is based on a Long-term Recurrent Neural Network (LRCN) architecture (Yu et al. 2019), which combines
the use of a Convolutional Neural Network (CNN) for the extraction of morphological features of the ARs together
with a Long Short-Term Memory (LSTM) network (Hochreiter & Schmidhuber 1997) for the temporal analysis of the
sequences. CNNs for time series of HMI images have been already used by Chen et al. (2019), following an approach
that ﬁrst extracts features from the images by means of an auto-encoder network, then artiﬁcially removes redundant
features extracted by the CNN according to a p-value analysis, and ﬁnally organizes the extracted features in time
series given as input to an LSTM network computing a binary prediction.

Diﬀerently from the technique used in Chen et al. (2019), our proposed method does not separate the CNN analysis
from the LSTM one, and therefore it does not need an a posteriori processing of the features extracted by the
CNN. This is a crucial point, since the weights updating process for the autoenconder network in Chen et al. (2019),
depends on the optimization of a regression loss, which measures the discrepancy between the reconstructed images
and the experimental ones, whereas the weights updating process for the CNN in the LRCN network depends on the
optimization of a classiﬁcation loss, which measures the discrepancy between the predicted probability that an event
occurs with the YES-NO actual labels.

As a ﬁnal technical detail, in the present study we propose to use a Score Oriented Loss (SOL) function (Marchetti
et al. 2021) for the network optimization in the training phase, which allows an automated optimization of a given
skill score without the need of an a posteriori choice of the optimal threshold that converts the probabilistic outcomes
into binary classiﬁcation. Speciﬁcally, the SOL function applied in this paper is based on the optimization of the TSS,
which is highly insensitive to the class imbalance ratio in the training set.

The plan of the paper is as follows. Section 2 describes the implementation paradigm. Section 3 focuses on such
strategy when applied to video data preparation. Section 4 discusses the design of the applied deep learning model.
Section 5 is devoted to the description of the prediction results. Our conclusions are oﬀered in Section 6.

2. IMPLEMENTATION AND ASSESSMENT PARADIGM

2.1. Sample deﬁnition

Given an AR, we split the data associated with it into contiguous samples, each one corresponding to a time interval
of ﬁxed duration. When the interval is reduced to only one time point, each sample can be a set of numerical values,
for example values of physical features of that AR, or an AR image. Alternatively, when the time interval is bigger
than one time point, a sample can be a time series of features, or a video of magnetograms. Each data sample has

AASTeX v6.3.1 Sample article

3

paper

data

multiple test
realizations

AR data split

validation method

score - C1+ score - M1+

Bobra & Couvidat (2015)

Liu et al. (2017)

Nishizuka et al. (2018)

Florios et al. (2018)

Jonas et al. (2018)

Campi et al. (2019)

Liu et al. (2019)*

Wang et al. (2020)

Park et al. (2018)

Huang et al. (2018)

Li et al. (2020)

Yi et al. (2021)

point in time
features (SHARP)
point in time
features (SHARP)
point in time
features (SHARP + others)
point in time
features (FLARECAST)
point in time
features (SHARP+others)
point in time
features (FLARECAST)
time series
features (SHARP)
time series
features (SHARP)
HMI and
MDI images
HMI and
MDI images
HMI
images
HMI
images

yes

yes

no

yes

yes

yes

yes

..

no

y

yes

no

yes

yes

no

yes

no?

no

no

no

no

yes

no

no

yes

yes

no

yes

yes

yes

yes

yes

yes

-

no

yes

SVM

RF

MLP

RF

RF

-

-

0.63

0.60

-

0.74

0.76

0.80

0.74

0.74 − 0.81

hybrid lasso

0.54

LSTM

0.61

0.67

0.79

LSTM

0.55?

0.68?

CNN

CNN

CNN

CNN

0.63

0.49

0.68

0.65

..

0.66

0.75

-

Table 1. Description of twelve ﬂare forecasting studies based on machine learning. For each study, the table reports: the main
author (column ”paper”); the kind of data used (column ”data”); whether a conﬁdence strip has been computed for the skill
score (column ”multiple test realizations”); whether data belonging to the same AR are split between the training and test
sets (column ”AR data split”); whether a validation set has been used to optimize the machine learning algorithm (column
”validation”); which method has been used (column ”method”); the score values for the prediction of C1+ and M1+ ﬂares,
respectively (columns ”score - C1+” and ”score - M1+”, respectively).

then been labelled with a mark of type X, M, C, NO1, NO2, NO3, NO4 depending on the ability of the AR to which
the sample belongs either to generate or not to generate a ﬂare:

• X class sample: sample of AR that originated a ﬂare in the next 24 hours after the sample time, with maximum

ﬂare class X1 or above.

• M class sample: sample of AR that originated a ﬂare in the next 24 hours after the sample time, with maximum

ﬂare class M1 or above but under class X.

• C class sample: sample of AR that originated a ﬂare in the next 24 hours after the sample time, with maximum

ﬂare class C1 or above but under class M.

• NO1 class sample: sample of AR that never originated a C1+ ﬂare.

• NO2 class sample: sample of AR that did not originate a C1+ ﬂare in the next 24 hours after the sample time,

it did not originate a C1+ ﬂare in the past, but it did originate a C1+ ﬂare in the future.

• NO3 class sample: sample of AR that did not originate a C1+ ﬂare in the next 24 hours after the sample time,

but it did originate a C1+ ﬂare in the 48 hours before the sample time.

• NO4 class sample: sample of AR that did not originate a C1+ ﬂare in the next 24 hours after the sample time
and did not originate a C1+ ﬂare in the 48 hours before the sample time, but it did originate a C1+ ﬂare before
the 48 hours before the sample time.

We can therefore think of an AR as a set of data samples labeled according to the previous criterion. For example,
suppose that the AR number 12645 includes 4 X samples, 10 M samples, 24 C samples and 13 NO2 samples; then this
AR can be described by means of the notation: AR12645 = {4X, 10M, 24C, 13NO2} .

4

Guastavino et al.

2.2. Well-balanced data sets

The procedure for the generation of training, validation and test sets was based on two criteria.

1. Proportionality. We required the sets to have almost equal rates of samples for each sample type described
above. In order to construct sets as reliable as possible, we required the rates to be similar to the ones char-
acterizing the historical archive; i.e., in our experiments we set the following rates, coherent to the ones in the
HMI archive for the time interval between 2012, September 14 and 2017, September 30 (where pX denotes the
rate of the X class sample, pM that of the M class sample, and so on):

• pX ≈ 0.13%
• pM ≈ 3.21%
• pC ≈ 18.08%
• pNO1 ≈ 45.94%
• pNO2 ≈ 3.57%
• pNO3 ≈ 12.06%
• pNO4 ≈ 17.01%

2. Parsimony. We wanted that each subset of samples came from as few ARs as possible. In this way, we promoted
training, validation and test sets to be independent from each other, in the sense that samples belonging to the
same AR must fall into the same data set.

2.3. Procedure for data sets generation

We ﬁrst set the size of the data set we wanted to create equal to n. This implied that we needed nX = n · pX samples

labelled with X, nM = n · pM samples labelled with M, and so on. The procedure is as follows:

1. We randomly took an AR containing X ﬂares and put all the samples of this AR in our data set. We kept on
including new ARs with X ﬂares, until the number of samples labelled with X became nX. If an AR contained
more X ﬂares than needed, we discarded those in excess.

2. We checked the amount of M ﬂares in the data set under construction. If we had more than nM samples with M
ﬂares, we randomly discarded those in excess; in the case the number was smaller, we randomly included ARs
containing M ﬂares (but not X ﬂares) up to the correct rate.

3. The process continued until we had the prescribed number of samples for each type.

In order to collect NO1, NO2, NO3, and NO4 data samples, we used just ARs not containing X and M ﬂares, since
these latter ones are precious for the construction of other independent and well balanced data sets according to the
parsimony and proportionality criterion, respectively.

We point out that the obtained algorithm is sub-optimal, since it may not ﬁnd the smallest number of ARs needed,

but it allows constructing a wide variety of independent data sets as it operates randomly.

2.4. Algorithm validation

Each machine learning algorithm depends on some parameters (e.g. weights utilized by each neuron of the neural
network), which must be optimised on the basis of the historical data set. To this aim, the performances of the
algorithm have been evaluated on a validation set and the weights’ values corresponding to the best validation score
are employed in the test phase.

In order to compare the performances of diﬀerent machine learning methods in ﬂare forecasting, the following items

should be accounted for:

2.5. Assessment of results

AASTeX v6.3.1 Sample article

5

1. The classiﬁcation results should be evaluated by considering appropriate skill scores deﬁned on the so-called
confusion matrix, which is characterized by four elements: true positives (TPs), i.e. the number of samples
labeled with YES and correctly predicted as positive; true negatives (TNs), i.e. the number of samples labeled
with NO and correctly predicted as negative; false positives (FPs), i.e. the number of samples labeled with NO
incorrectly predicted as positive; and false negatives (FNs), i.e. the number of samples labeled with YES and
incorrectly predicted as negative. In solar ﬂare forecasting the most meaningful skill scores are the ones speciﬁc
for imbalanced data classiﬁcation. Indeed, solar events are relatively seldom, as already pointed out in Section
2.1. Therefore, a chosen score needs to be able to represent the performance of the classiﬁer concerning data sets
with a small number of positive events. Among all possible skill scores, the True Skill Statistic (TSS) (Hanssen
& Kuipers 1965) is deﬁned as

TSS =

TP
TP + FN

−

FP
FP + TN

= POD − FAR ,

(1)

and its values have range in the interval [−1, 1]: when TSS = 1, the performance is optimal, while TSS > 0
means that the rates of positive and negative events are mixed up. The TSS is insensible to the class-imbalance
ratio (Bloomﬁeld et al. 2012), and therefore this is the skill score adopted in the present study.

2. The strategy outlined throughout Sections 2.1, 2.2, 2.3 ought to be repeated several times in order to achieve
some statistical signiﬁcance. Therefore, many classiﬁcation tests should be carried out by generating diﬀerent
triples of training, validation and test sets by randomly extracting AR images from the HMI archive.

Once the results are obtained, some statistical indicators such as the mean, standard deviation, maximum and
minimum value should be reported. Obviously, the results achieved on the test set should not be produced by
applying any validation procedure directly on the test set.

3. VIDEO DATA PREPARATION

In general, the archive of the SDO/HMI mission includes 2D images of continuous intensity, of the full three-
component magnetic ﬁeld vector, and of the line-of-sight magnetic intensity. In the present study, we considered the
Near Realtime Space Weather HMI Archive Patch (SHARP) data products associated to the line-of-sight components
in the time range between 2012, September 14 and 2017, September 30. More speciﬁcally, our data products were 24
hour long videos made of 40 SHARP images of an AR, with 36 minutes cadence. Each image in these time series have
been resized to a 128 × 128 pixels dimension, due to computational reasons.

Figure 1 and Figure 2 are iconographical representations of how these videos are categorized with respect to the
deﬁnitions given in Section 2. In particular, on the one hand Figure 1 illustrates a typical temporal history (from its
birth to its death) of an AR that originates X1+, M1+, and C1+ ﬂares (we point out that when the aim is to predict
C1+ ﬂares then the three kinds of videos are labeled with 1, whereas when the aim is to predict M1+ ﬂares then just
the ﬁrst two kinds of videos are labeled with 1). On the other hand, Figure 2 provides schematic examples of NO2,
NO3, and NO4 videos. NO1 data samples are not included in the ﬁgure, since they correspond to ARs that never
originated a ﬂaring event.

4. DEEP LEARNING METHOD FOR HMI VIDEO

The analysis of the video data samples has been performed by means of a Long-term Recurrent Convolutional
Network (LRCN), which is a mixed deep learning model made of a convolutional neural network (CNN) and a Long-
Short Term Memory (LSTM) network. The ﬁrst part of the LRCN network is made of the following sequence of layers
(see Figure 3, top panel):

• a 7 × 7 convolutional layer of 32 units; a 2 × 2 max-pooling layer;

• a 5 × 5 convolutional layer of 32 units; a 2 × 2 max-pooling layer;

• a 3 × 3 convolutional layer of 32 units; a 2 × 2 max-pooling layer;

• a 3 × 3 convolutional layer of 32 units; a 2 × 2 max-pooling layer;

• a dense layer of 64 units, where dropout is applied with a fraction of 0.1 input units dropped.

6

Guastavino et al.

(a) X class video

(b) M class video

Figure 1. From top to bottom: examples of X class, M class and C class video.

(c) C class video

Height and width strides are set to 2 for the convolutional layers and to 1 for the max-pooling. Each convolutional
layer is L2-regularized and the corresponding output is standardized. Before applying the dense layer, the last pooling
layer is ﬂattened. The Rectiﬁed Linear Unit (ReLU) is used as activation function in all layers. We also point out

AASTeX v6.3.1 Sample article

7

(a) NO2 video

(b) NO3 video

Figure 2. From top to bottom: examples of NO2, NO3 and NO4 class video.

(c) NO4 video

that the input videos, which consist of 40 frames of 128 × 128 images each, are treated as time series, so that the CNN
architecture described above is applied to each video frame in parallel.

In the second part of the LRCN, the outputs of the CNNs (40 vectors, each one composed by 64 features) are
sequentially considered in time and then passed to the LSTM (see 3, bottom panel), which consists of 50 units.

8

Guastavino et al.

Similarly to the dense layer, here dropout is applied with a fraction of 0.5 active units. Finally, a dense sigmoid unit
drives the output of the LSTM to be in the interval [0, 1], in order to perform binary classiﬁcation. The CNN-LSTM

(a) LRCN architecture

Figure 3. Top panel: The overall LRCN design. Bottom panel: the CNN architecture.

(b) CNN architecture

network is trained for 100 epochs by taking batches of 128 samples. Moreover, the Adam scheme (Kingma & Ba 2015)
is adopted for the optimization of the weights.

As far as the loss function utilized in the training phase is concerned, we have adopted the score-driven strategy
illustrated in Marchetti et al. (2021). The classical confusion matrix depends on a ﬁxed threshold parameter τ ∈ (0, 1),
i.e.,

CM(τ ) =

.

(2)

(cid:32)

TN(τ ) FP(τ )
FN(τ ) TP(τ )

(cid:33)

For the construction of score-oriented loss (SOL) functions, the threshold parameter τ is dealt with as a random
variable associated to a speciﬁc probability density function. Letting Eτ [·] be the expected value with respect to τ , we
took an expected confusion matrix

Eτ [CM(τ )] =

(cid:32)Eτ [TN(τ )] Eτ [FP(τ )]
Eτ [FN(τ )] Eτ [TP(τ )]

(cid:33)

.

From this matrix it was possible to construct the expected TSS

Eτ [TSS(τ )] =

Eτ [TP(τ )]
Eτ [TP(τ ) + FN(τ )]

−

Eτ [FP(τ )]
Eτ [FP(τ ) + TN(τ )]

− 1 ,

(3)

(4)

AASTeX v6.3.1 Sample article

and from this the TSS-driven loss function

(cid:96)TSS := −Eτ [TSS(τ )] .

9

(5)

This function is diﬀerentiable and therefore can be easily minimized in the training phase, with the crucial advantage
that the corresponding skill score is automatically optimized, without the need of any a posteriori tuning of the
thresholding parameter τ , which is set to the default value 0.5.

5. RESULTS

The LRCN described in the previous section has been applied to video data generated as described in Section 3,
using the validation strategy illustrated in Section 2. We ﬁrst ﬁlled up a training set, a validation set, and a test set
made of 3000, 750, and 750 data samples, respectively, and we used data augmentation to increase the cardinality of
these sets up to 15000, 3750, and 3750, respectively (the data augmentation process here relied on image rotation and
reﬂection). We repeated this set generation process ten times in order to create ten random realizations of this triple
of sets. Table 2 shows the prediction results obtained by the LRCN in the case of the realization of the validation and
test sets: speciﬁcally, the table focuses on TSS, and provides its mean and standard deviation values, the minimum
value, the 25th percentile value, the median value, the 75th percentile value and the maximum value for the prediction
of both C1+ and M1+ ﬂares. The numbers in this table show that image-based deep learning is more eﬀective in
predicting M1+ ﬂares than C1+ ﬂares, in line with most results in the scientiﬁc literature. Further, both the mean and
the median values for the test sets are rather close to the ones provided by the network when applied to the validation
sets, and in all cases the standard deviations are nicely small.

Figure 4 and Figure 5 aim at providing a quantitative conﬁrmation of the implementation paradigm introduced in
Section 2. The boxplots in Figure 4 represent the rates of TPs and TNs computed on the ten random realizations of
the test set. They show that:

• When the aim is to predict C1+ ﬂares, X class and M class ﬂares are predicted with a higher success rate with
respect to C ﬂares, coherently to the fact that ARs labelled as either M or X are more distinguishable from ARs
associated to NO-class ﬂares (this is particularly true for all cases when ﬂares are close to strong B events).

• The prediction of null events does not have the same success rate for all NO-class ﬂares. Indeed, the rate of TNs
is signiﬁcantly higher when the LRCN aims to predict NO1 events, coherently to the fact that data sampled
belonging to the other three NO-classes are similar to C1+ data samples (this is particularly true for NO3 data
samples, whose prediction has indeed the lowest success rate).

• When the aim is to predict M1+ ﬂares, the biggest number of TNs refer to C class data samples, coherently to
the fact that such videos may be associated to events with energy budget close to the ones of weak M ﬂares.

In Figure 5 we computed the TSS values while successively excluding data samples of diﬀerent classes from the test
sets. We found out that, in the case of predicting C1+ events, the TSS values are the smallest ones when all classes
are represented in the test sets, and nicely increases while successively and cumulatively excluding NO2, NO3, and
NO4 events. On the other hand, while predicting M1+ ﬂares, the TSS values signiﬁcantly increase when data samples
belonging to all classes but class M1+ are excluded from the test sets.

6. COMMENTS AND CONCLUSIONS

The scientiﬁc rationale of the present study was two-fold. On the one hand, we aimed at verifying the feasibility of
a fully automated ﬂare forecasting procedure that takes as input videos of line-of-sight magnetograms and provides
binary predictions as output. On the other hand, we aimed at studying the impact of the data set preparation on
the deep learning performances. The conclusions we can draw from this analysis are that, ﬁrst of all, deep learning
is able to realize ﬂare videos classiﬁcation with prediction performances that are in line with the ones obtained by
machine learning approaches that require an a priori extraction of features from the HMI images by means of pattern
recognition and feature computation algorithms. Further, in our implementation of the convolutional network, the use

10

Guastavino et al.

Table 2. Mean, standard deviation, minimum, 25th percentile, median, 75th percentile and maximum value of the TSS
distribution computed on 10 validation sets and 10 test sets, separately.

TSS (C1+ ﬂares)

Validation
Test

Validation
Test

Mean
0.57
0.55

Mean
0.76
0.68

Std Min 25th perc Median 75th perc Max
0.61
0.02
0.61
0.05

0.59
0.60

0.55
0.46

0.56
0.52

0.57
0.54
TSS (M1+ ﬂares)

Std Min 25th perc Median 75th perc Max
0.85
0.07
0.82
0.09

0.77
0.69

0.82
0.72

0.65
0.55

0.67
0.61

(a) C1+ ﬂares prediction

(b) M1+ ﬂares prediction

(c) C1+ ﬂares prediction

(d) M1+ ﬂares prediction

Figure 4. Percentages of correctly predicted as TP (top panels) and as TN (bottom panels) computed on the 10 test set for
the C1+ ﬂares prediction (ﬁrst) and for the M1+ ﬂares prediction (second column) with respect to each kind of samples as
shown in the x-axis.

of a SOL function allows an a priori optimization of the TSS, which allows avoiding the application of (often critical)
a posteriori thresholding on the score value.

Second, the results in Figure 4 and in Figure 5 clearly show that the way the training and validation sets are prepared
for the network optimization has a really signiﬁcant impact on the prediction performances (Campi et al. 2019). In

AASTeX v6.3.1 Sample article

11

(a) C1+ ﬂares prediction

(b) M1+ ﬂares prediction

Figure 5. Boxplots of TSS computed on 10 test sets for the prediction of C1+ class ﬂares (top panel) and M1+ class ﬂares
(bottom panel) by excluding each time a kind of No labeled videos. Top panel. From left to right: (1) TSS computed on overall
samples; (2) TSS computed on samples by excluding the NO3 videos; (3) TSS computed on samples by excluding the NO3 and
NO4 videos; (4) TSS computed on samples by excluding the NO3, NO4 and NO2 videos. Bottom panel. From left to right: (1)
TSS computed on overall samples; (2) TSS computed on samples by excluding the NO3 videos; (3) TSS computed on samples
by excluding the NO3 and NO4 videos; (4) TSS computed on samples by excluding the NO3, NO4 and NO2 videos; (5) TSS
computed on samples by excluding the NO3, NO4, NO2 and C class videos.

particular, these ﬁgures prove that an appropriate balancing of these sets should account for not only the presence of
ARs generating ﬂares but even the presence of ARs associated to null events of diﬀerent kinds.

We ﬁnally point out the that TSS values obtained by this analysis are distinctively diﬀerent from 1, as typically
occurs in most ﬂare forecasting studies based on machine learning. This seems to conﬁrm once more (Campi et al. 2019)
the presence of an intrinsically stochastic component of the ﬂaring process, which implies that the ﬂare forecasting
challenge has still a predominant probabilistic aspect. In this perspective, two possible futher research lines are the
combination of image-based features with features that rely on non-iconographic information within fully data-driven
models; and the exploitation of physical information in the design and optimization of the networks.

Barnes, G., Leka, K. D., Schrijver, C. J., et al. 2016, The

Bloomﬁeld, D. S., Higgins, P. A., McAteer, R. J., &

Astrophysical Journal, 829, 89,

doi: 10.3847/0004-637x/829/2/89

Gallagher, P. T. 2012, The Astrophysical Journal Letters,

747, L41

REFERENCES

12

Guastavino et al.

Bobra, M. G., & Couvidat, S. 2015, The Astrophysical

Li, X., Zheng, Y., Wang, X., & Wang, L. 2020, The

Journal, 798, 135

Astrophysical Journal, 891, 10

Campi, C., Benvenuto, F., Massone, A. M., et al. 2019, The

Astrophysical Journal, 883, 150

Chen, Y., Manchester, W. B., Hero, A. O., et al. 2019,

Space Weather, 17, 1404

Liu, C., Deng, N., Wang, J. T., & Wang, H. 2017, The

Astrophysical Journal, 843, 104

Liu, H., Liu, C., Wang, J. T., & Wang, H. 2019, The

Florios, K., Kontogiannis, I., Park, S.-H., et al. 2018, Solar

Astrophysical Journal, 877, 121

Physics, 293, 1

Georgoulis, M. K., Bloomﬁeld, D. S., Piana, M., et al. 2021,
Journal of Space Weather and Space Climate, 11, 39,
doi: 10.1051/swsc/2021023

Hanssen, A., & Kuipers, W. 1965, On the Relationship

Between the Frequency of Rain and Various
Meteorological Parameters: (with Reference to the
Problem Ob Objective Forecasting), Koninkl. Nederlands
Meterologisch Institut. Mededelingen en Verhandelingen
(Staatsdrukkerij- en Uitgeverijbedrijf).
https://books.google.it/books?id=nTZ8OgAACAAJ

Hochreiter, S., & Schmidhuber, J. 1997, Neural

computation, 9, 1735

Marchetti, F., Guastavino, S., Piana, M., & Campi, C.

2021, arXiv preprint arXiv:2103.15522

Mason, J. P., & Hoeksema, J. T. 2010, The Astrophysical

Journal, 723, doi: 10.1088/0004-637X/723/1/634

Nishizuka, N., Sugiura, K., Kubo, Y., Den, M., & Ishii, M.

2018, The Astrophysical Journal, 858, 113

Park, E., Moon, Y.-J., Shin, S., et al. 2018, The

Astrophysical Journal, 869, 91

Schrijver, C. J., & Siscoe, G. L. 2010, Heliophysics: Space

Storms and Radiation: Causes and Eﬀects

Song, H., Tan, C., Jing, J., et al. 2009, Solar Physics, 254,

Huang, X., Wang, H., Xu, L., et al. 2018, The

101, doi: 10.1007/s11207-008-9288-3

Astrophysical Journal, 856, 7

Jonas, E., Bobra, M., Shankar, V., Hoeksema, J. T., &

Recht, B. 2018, Solar Physics, 293, 1

Kingma, D. P., & Ba, J. 2015, in 3rd International

Conference on Learning Representations, ICLR 2015, San
Diego, CA, USA, May 7-9, 2015, Conference Track
Proceedings, ed. Y. Bengio & Y. LeCun.
http://arxiv.org/abs/1412.6980

Wang, X., Chen, Y., Toth, G., et al. 2020, The

Astrophysical Journal, 895, 3

Yi, K., Moon, Y.-J., Lim, D., Park, E., & Lee, H. 2021, The

Astrophysical Journal, 910, 8

Yu, Y., Si, X., Hu, C., & Zhang, J. 2019, Neural

computation, 31, 1235

