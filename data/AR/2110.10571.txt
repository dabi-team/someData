CobotAR: Interaction with Robots using Omnidirectionally Projected
Image and DNN-based Gesture Recognition

Elena Nazarova1, Oleg Sautenkov1, Miguel Altamirano Cabrera1, Jonathan Tirado1,
Valerii Serpiva1, Viktor Rakhmatulin2, and Dzmitry Tsetserukou1

1
2
0
2

t
c
O
0
2

]

O
R
.
s
c
[

1
v
1
7
5
0
1
.
0
1
1
2
:
v
i
X
r
a

Abstract— Several technological solutions supported the cre-
ation of interfaces for Augmented Reality (AR) multi-user
collaboration in the last years. However, these technologies
require the use of wearable devices. We present CobotAR -
a new AR technology to achieve the Human-Robot Interaction
(HRI) by gesture recognition based on Deep Neural Network
(DNN) - without an extra wearable device for the user. The
system allows users to have a more intuitive experience with
robotic applications using just their hands. The CobotAR
system assumes the AR spatial display created by a mobile
projector mounted on a 6 DoF robot. The proposed technology
suggests a novel way of interaction with machines to achieve
safe, intuitive, and immersive control mediated by a robotic
projection system and DNN-based algorithm. We conducted
the experiment with several parameters assessment during this
research, which allows the users to deﬁne the positives and
negatives of the new approach. The mental demand of CobotAR
system is twice less than Wireless Gamepad and by 16% less
than Teach Pendant.

I. INTRODUCTION

Several

technological solutions related to the creation
of user interfaces for multi-user collaboration, education,
maintenance, or safer and more effortless industrial operation
(Microsoft Dynamics [1], OmniPack [2], RoMA [3], Loki
[4]) had emerged in the last years. The majority of them
are based on Augmented Reality or Virtual Reality (AR-
VR). Commonly, these technologies are complemented by
additional devices that provide the visualization of virtual
objects. They can be wearable devices, e.g., SixthSense
[5], AAR [6], HoloLens [7], Google Glass [8], or portable
devices such as tablets or cellphones. These technologies
provide good advantages and facilities that can contribute to
the development of more Intuitive HRI for easy interaction
between Robotic Systems and Humans.

Industry 4.0 trends show that Cyber-Physical Systems
(CPS) are high-level linked with human interaction. That
is one of the main reasons why the new technologies
are devoted to support and emphasize this interaction [9].
HRI requires intuitive and efﬁcient interfaces. User-Centered
Design (UCD) systems decrease the complexity of human-
interfaces and increase the system’s intuitive op-
Robot
eration. The result
is an apparent enhancement of CPS
efﬁciency and a substantial reduction of human errors [11].

1 The authors are with the Space Center and 2 Center
for
Design, Manufacturing and Materials, Skolkovo Institute of Science
and Technology (Skoltech), 121205 Bolshoy Boulevard 30, bld. 1,
{elena.nazarova, oleg.sautenkov,
Moscow,
jonathan.tirado, miguel.altamirano,
valerii.serpiva, viktor.rakhmatulin,
d.tsetserukou}@skoltech.ru

Russia.

Fig. 1: CobotAR interface by projection and DNN-based hand
gesture recognition to control a collaborative robot UR3.

AR technologies permit the creation of HRI interfaces in
real environments, where the virtual and physical elements
are incorporated together [?]. AR can reduce the gap between
simulation and implementation by enabling the prototyping
of algorithms on a combination of physical and virtual
objects where robots and humans can interact in the same
environment [12].

Projection mapping-related technologies are used to aug-
ment the interface with projected elements. Thanks to projec-
tion mapping, we can generate visual illusions by changing
the shapes of real objects or adding extra features. However,
when projection mapping is applied in dynamic systems,
it has critical
limitations in the process latency and the
target tracking. Additionally, if the capture-projection system
is installed in a static position, the projection area is also
restricted [13].

The interactive projection technology introduced by Mistry
et al. [5] allows users to interact with projected objects or
images almost on any surface. The hand movements and
gestures are tracked and recognized by the camera and hand-
worn marker. This technology allows users to interact with
the projection on different surfaces, such as navigating a
map, drawing on any surface, and displaying and showing
photos. However, the interaction is only with the projected
information and is required to use a color marker on the
user’s ﬁnger to track the hand gestures.

Hartmann et al. [6] introduced a similar technology by
combining a head-mounted actuated pico projector with a

 
 
 
 
 
 
Hololens AR headset. Demonstrations showcase ways to use
head-mounted displays and the projected together, such as
distributing content across depth surfaces, expanding the ﬁeld
of view, and enabling bystander collaboration.

Projector-based AR is one of the most suitable approaches
for HRI applications because of two main advantages. Firstly,
the user’s environment is augmented with images that are
integrated directly into the real objects, not only in their
visual ﬁeld [14]. Secondly, this technology allows the user to
have more natural interaction with the robot since the person
is not wearing or using any tracking or projection device on
his body.

We introduce CobotAR, a novel HRI system that imple-
ments AR projector-based spatial displays by a Camera-
Projector Module (CPM) mounted on a 6 DoF collaborative
robot, and DNN-based gesture recognition to interact with
a collaborative robot. In Section II, an overview of the
system is presented, where the system’s main components
are described. An experimental evaluation is presented in
Section III, where the proposed system was compared with
other interfaces to control a collaborative robot and evaluate
its advantages and limitations.

II. SYSTEM OVERVIEW

The CobotAR is a novel HRI system containing AR
projector-based spatial displays, which generates an inter-
active projected Graphic User Interface (GUI) on a UR3
collaborative robot. Using DNN-based gesture recognition,
the system tracks the position of the user’s hand, it allows
users to interact with the projected GUI and control the
position of the robot in an intuitive way. The projector-
camera module is mounted on a second robot, not ﬁxed
statically, to provide a bigger projection area and avoid the
shadows in the projection.

The hardware components of the CobotAR system are a
mobile projector Dell M115HD, a Logitech HD Webcam
C930e,
two 6 DoF collaborative robots from Universal
Robots UR3 and UR10, and a desktop computer. Three
computational modules are responsible for data processing
and robot controller: a) gesture recognition based on DNN,
b) image processing through OpenCV library, and c) URX
python library to communicate with Universal Robots. The
system architecture is shown in Fig. 2, and system overview
is shown in Fig. 3.

The DNN-based algorithm detects the user’s hand gestures
while the CPM follows the target UR3 robot position. It
allows the users to interact with any type of GUI projection
and to control the robot’s position more intuitively. The CPM
is mounted on a UR10 robot to provide a more extensive
projection area and avoid the shadows in the projection. It
follows constantly the UR3 robot target surface position to
maintain the right projection. Simultaneously, the Computer
Vision (CV) algorithm processes the Webcam image using
the hand tracking module. It estimates the position of the
ﬁngers in a speciﬁc area and deﬁnes it as a ”press button”
command. As a result of the gesture recognition process, the
central program deﬁnes the required robotic action. CobotAR

Fig. 2: User controls the UR3 collaborative robot through projected
interface from ultra-compact mobile projector Dell M115HD and
Logitech HD Webcam C930e mounted on the end-effector of a
robot UR10.

Fig. 3: System architecture.

can present different GUI with buttons and interactive robotic
parts (user visualize the inner structure of the robot without
disassembling it).

A. Robots kinematics

The GUI is projected on the UR3 robot, as shown in Fig.
4. The UR10 robot, with the CPM, continuously follows the
position and orientation of the middle point of the projected
link, calculated by inverse kinematics. The position and ori-
entation of the UR10 are calculated by forward kinematics.
The image of the GUI does not change with the motion.

The Denavit-Hartenberg (DH) parameters of the UR3
robot are represented in Table I, where Joint N is each joint
in the robot, where Θ is the joint variable, a, d and α are

the ﬁxed link parameters. [15]

Table II describes DH parameters of our system for the

GUI projection.

We process received values from UR3 robot joints and get
the ﬁnal transformation matrix. We extract the position and
orientation of the desired point[15]. After all, we transfer it
to the UR10 robot with projector-camera system.

TABLE I: Denavit-Hartenberg representation of UR3

Joint
Joint 1
Joint 2
Joint 3
Joint 4
Joint 5
Joint 6

Θ
Θ1
Θ2
Θ3
Θ4
Θ5
Θ6

a
0
-0.24355
-0.2132
0
0
0

d
0.15185
0
0
0.13105
0.08535
0.0921

α
π/2
0
0
π/2
−π/2
0

TABLE II: Denavit-Hartenberg representation of the model

Joint
Joint 1
Joint 2

Θn
Θ1
Θ2

a
0
-0.12176

d
0.15185
0.40

α
π/2
0

The UR10 is initially positioned to project GUI on the
UR3 link. Further motions were conducted relative to this
reference position.

B. DNN-based gesture recognition

The DNN-based gesture recognition module is imple-
mented on the basis of the Mediapipe framework. It provides
high-ﬁdelity tracking of the hand by employing Machine
Learning (ML) to infer 21 key points of a human hand per
a single captured frame.

If the index ﬁnger coordinates are located in the button’s
area, and the gesture has been changed from ”Palm” to
”One”, the corresponding button will activate. The algorithm
sends a number of activated buttons by ROS framework to
the robot control system.

DNN was trained to recognize 8 gestures, two gestures

were chosen to perform pressing the buttons.

III. EXPERIMENTAL EVALUATION

The principal approach of this paper is to design a new
AR-technology to control a collaborative robot by a projected
interface and DNN-based gesture recognition without an
extra wearable device for the user. We conducted a series
of user evaluations to determine the advantages and disad-
vantages of the proposed system in comparison with existing
interfaces. The interfaces to compare were the Teach Pendant
from UR3 and the Logitech F710 Wireless Gamepad [16].

before the experiment. Moreover, we ﬁxed a 150x150 mm
square printed on white paper on the table where the robots
are located. Participants were asked to repeat the printed path
of the 150x150 mm square trajectory manipulating by the
Teach Pendant, the Logitech F710 Wireless Gamepad, and
the novel CobotAR system to evaluate the controllability of
the robot by the three different interfaces.

We used data such as the coordinates of each participant’s
TCP UR3 robot trajectory and the comparison time for the
experimental evaluation. Moreover, each participant com-
pleted a questionnaire for each interface based on The NASA
Task Load Index (NASA-TLX) in order to determine the
more user-friendly system and give some comments about
the advantages and disadvantages of the use of CobotAR
system relative to the other interfaces.

Twelve participants (3 females) volunteering conducted
the test, aged from 22 to 43 years. None of them reported
any problems and difﬁculties in CobotAR functionality.

Before starting the experiment, the participants performed
a training session, where each participant familiarized him-
self/herself with each interface and tested it. After the
training session, the UR3 robot returned to the predetermined
initial position, and the experiment started.

B. CobotAR system controlling

the UR3 robot

The GUI to control

through CobotAR
system is represented in Fig. 4. By pressing on the projected
green buttons, the robot TCP moves in the y-axis, with
a positive or negative direction, respectively. The circle in
the top simulates the front part of an arrow, and the cross
the back part of an arrow, to specify the direction of the
TCP movement. By pressing on the projected red buttons,
the robot TCP moves in the x-axis, with a positive or
negative direction according to the direction of the arrows.
The user can use only one button simultaneously. During the
experiment, the user can correct each time the trajectory of
the robot.

Fig. 4: CobotAR control
interface. The interface contains four
buttons. Each button controls one direction of the robot in the XY
plane. The gesture recognition system detects the pressed button
and moves the robot in the desired direction.

A. Experimental Design

C. UR3 Teach Pendant

Our team designed, printed using a 3D printer, and assem-
bled a marker holder on the end-effector of the UR3 robot

The interface to control the UR3 robot TCP through the
Teach Pendant is represented in Fig. 5. By pressing on the

buttons in the orange box, user has the ability to move the
robot TCP in positive and negative X and Y directions. The
user can use only one button simultaneously. During the
experiment, the user could correct the trajectory of the robot.

Fig. 7: The time performance comparison of the participants work
on each interface.

Fig. 5: Teach Pendant control interface. It is the original UR control
interface. The touch Teach Pendant contains several buttons and
functions through which the user can manipulate each of the 6-
DoF of the robot.

D. Logitech F710 Wireless Gamepad

The Wireless Gamepad stick was used to control
the
position of the robot TCP as shown in Fig. 6. The position
of the TCP changes in the x and y axis by the movement of
the right stick in the orange box. The system works in the
Wireless Gamepad control way. The user could change robot
movement not only in a straight lines, but also in diagonal.

Fig. 6: Wireless Gamepad interface. The interface uses the thumb
Wireless Gamepad of a Logitech Wireless Gamepad F710. This
device allows the user to control the movements of the UR robot
in the plane XY.

IV. EXPERIMENTAL RESULTS
Figure 7 presents a summary of the participants’ time
spent to draw the square using each interface by boxplots.
Considering average values, the fastest interface is the Log-
itech F710 Wireless Gamepad (20,07s), second the CobotAR
system (27,61s), and third the UR3 Teach Pendant (38,16s).

Fig. 8: The average error between the printed square and the
trajectory designed by the users using each of the interfaces.

In order to evaluate the statistical signiﬁcance of the
differences between the time from each of the interfaces, we
analyzed the results using single factor repeated-measures
ANOVA, with a chosen signiﬁcance level of α < 0.05. Ac-
cording to the ANOVA results, there is a statistical signiﬁcant
difference in the operation time in each of the interfaces,
F (2, 36) = 6.9962, p = 6.03 · 10−3. The ANOVA showed
that
the robot signiﬁcantly
the interface used to control
inﬂuence the time of the operation.

The paired t-tests showed statistically signiﬁcant differ-
ences between the time from the use of the proposed system
CobotAR and the Logitech F710 Wireless Gamepad (p =
0.0468 < 0.05), between the time from the Logitech F710
Wireless Gamepad and the UR3 Teach Pendant (p = 2.9 ·
10−3 < 0.05).

We see that the UR3 Teach Pendant presents the longest
time because of the interaction between the user’s ﬁnger and
the resistive sensor display, which has more delay and miss-
clicks than the other interfaces. The Logitech F710 Wireless
Gamepad is the fastest because the user does not need to
spend the time to switch the direction with an analog stick
sensor. CobotAR stands between them.

TABLE III: NASA TLX rating for each interface

Teach pendant Wireless Gamepad

Mental Demand
Physical Demand
Temporal Demand
Overall performance
Effort
Frustration
Average time
STD(mm)
Average TLX

3,92
4,25
4,25
2,00
4,17
4,00
34,75
1,1
32,26

6,08
5,33
4,58
5,58
5,33
5,92
20,08
2,5
46,90

CobotAR
2,83
3,08
4,08
2,17
3,50
3,67
27,34
5,1
27,62

participants had the lowest mental workload using the Cob-
otAR system (27,6 out of 100), whereas the UR3 Teach
Pendant
interface mental workload (32 out of 100) was
slightly higher. The Logitech F710 Wireless Gamepad had
the worst conditions (47 out of 100) according to the NASA
TLX rating.

B. Post-Experience Questionnaire

Generally, people were very inspired by CobotAR system.
Some of the participants noticed that they could push the
robot only by ﬁnger in a desirable direction. The comment:
”I liked the projector interface (CobotAR), it felt like I am
pushing the robotic arm, and it is moving. If it would be a
bit faster, it will feel even better.”

Some of the participants were confused about dot and
cross on the buttons and suggested to use different signs:
”The dot and cross may not be very intuitive; you can replace
them with the words: ”on” and ”from” or something else.”
”I enjoyed driving the CobotAR. It was both engaging and
exciting. I want to try to manage it a couple more times. It
involves you directly in the operation.”

The main advantage of this interface is that a person can
move the robot with one hand, and the second hand is free.
Secondly, it is non-obligatory to switch constantly the view
from interface to end-effector. The person can observe them
simultaneously. Third, the operator gets a kind of haptic
feedback from the robot, and he can feel where he is moving
the robot’s hand.

V. APPLICATIONS

The CobotAR technology could be potentially used in
various applications: interactive education programs and en-
tertainment, industrial human-robot collaborative scenarios
as shown in Figure 12.

a) CobotAR.Skin is a dynamically projected skin on the
robot’s surface, reﬂecting the robot’s state and its current
task. For instance, the skin may indicate the proximity to
the robot in collaborative tasks.

b) CobotAR.Zoom is a robot repairing support service
based on online teleconferencing. The faces of the supporting
team and relevant information are projected on the robot
joints.

c) CobotAR.Soldering is an AR interface to support col-
laborative soldering. The robot equipped with a fan could be
easily moved in the workspace and the human worker could
perform operations with two hands. d) CobotAR.Education is

(a) CobotAR graph.

(b) CobotAR drawing.

Fig. 9: End effector trajectory followed by CobotAR interface.

(a) Logitech F710 Wireless Gamepad
graph.

(b) Logitech F710 Wireless Gamepad
drawing.

Fig. 10: End effector trajectory followed by Logitech F710 Wireless
Gamepad.

(a) UR3 Teach Pendant graph.

(b) UR3 Teach Pendant drawing.

Fig. 11: End effector trajectory followed by UR3 Teach Pendant
interface.

The average error between the printed square and the
trajectory drawed by the users using each of the interfaces
is presented in Fig. 8. Participants were, on average, 1.1 cm
from the right path using the UR3 Teach Pendant interface.
Using the Logitech F710 Wireless Gamepad, the average
error was 2.5 cm, and using CobotAR, it was 5.1 cm.

Fig. 9, Fig.10, and Fig.11 show the graphs of the ideal
trajectory of a square and the real trajectory of user 1, which
he drew using the different interfaces. The data shown in
these graphs were captured during the experiment of user 1.

A. Average score of NASA TLX rating

Table III:NASA TLX rating showed the results of the
NASA TLX rating for each interface. Results showed that

CobotAR time performance and accuracy. Moreover, we plan
to develop a virtual scene with two digital twins in Unity
Engine, allowing us to both simulate and adjust the behavior
of the robotic arms in HRI scenarios as considered in [?].

ACKNOWLEDGMENT

The reported study was funded by RFBR and CNRS

according to the research project No. 21-58-15006.

REFERENCES

[1] Microsoft,
2019.
us/mixedreality/overview/

“Mixed

[Online]. Available:

Reality: Microsoft

365,”
https://dynamics.microsoft.com/en-

Dynamics

[2] NativeRobotics,

“OmniPack,”

2019.

[Online].

Available:

https://nativerobotics.com/omnipack

[3] H. Peng, J. Briggs, C.-Y. Wang, K. Guo, J. Kider, S. Mueller, P.
Baudisch, and F. Guimbretiere, “Roma: Interactive fabrication with
augmented reality and a robotic 3d printer,” in Proceedings of the
2018 CHI Conference on Human Factors in Computing Systems, ser.
CHI ’18. New York, NY, USA: Association for Computing Machinery,
2018, p. 1–12.

[4] B. Thoravi Kumaravel, F. Anderson, G. Fitzmaurice, B. Hartmann,
and T. Grossman, “Loki: Facilitating remote instruction of physical
tasks using bi-directional mixed-reality telepresence,” in Proceedings
of the 32nd Annual ACM Symposium on User Interface Software
and Technology, ser. UIST ’19. New York, NY, USA: Association for
Computing Machinery, 2019, p. 161–174

[5] P. Mistry and P. Maes, “Sixthsense: A wearable gestural interface,” in
ACM SIGGRAPH ASIA 2009 Sketches, ser. SIGGRAPH ASIA ’09.
New York, NY, USA: Association for Computing Machinery, 2009.

[6] J. Hartmann, Y.-T. Yeh, and D. Vogel, “Aar: Augmenting a wearable
augmented reality display with an actuated head-mounted projector,”
in Proceedings of the 33rd Annual ACM Symposium on User Interface
Software and Technology, ser. UIST ’20. New York, NY, USA:
Association for Computing Machinery, 2020, p. 445–458.

[7] Microsoft,

“Hololens

2,”

2020.

[Online].

Available:

https://www.microsoft.com/en-us/hololens/apps

[8] Google,

2020.
https://www.google.com/glass/start/

“Glass,”

[Online].

Available:

[9] H. Kagermann, W. Wahlster, and J. Helbig, “Recommendations for
implementing the strategic initiative industrie 4.0 – securing the future
of german manufacturing industry,” acatech – National Academy of
Science and Engineering, Munchen, Final Report of the Industrie 4.0
Working Group, apr 2013.

[10] I. Mal´y, D. Sedl´aˇcek, and P. Leit˜ao, “Augmented reality experiments
with industrial robot in industry 4.0 environment,” in 2016 IEEE 14th
International Conference on Industrial Informatics (INDIN), 2016, pp.
176–181.

[11] M. Ostanin, R. Yagfarov, and A. Klimchik, “Interactive robots con-trol
using mixed reality,”IFAC-PapersOnLine, vol. 52, no. 13, pp.695–700,
2019, 9th IFAC Conference on Manufacturing Modelling,Management
and Control MIM 2019.

[12] W. H¨onig, C. Milanes, L. Scaria, T. Phan, M. Bolas, and N. Ayanian,
“Mixed reality for robotics,” in 2015 IEEE/RSJ International Confer-
ence on Intelligent Robots and Systems (IROS), 2015, pp. 5382–5387.
[13] M. A. Cabrera, I. Usachev, J. Heredia, J. Tirado, and D. Tsetserukou,
“Maskbot: Real-time robotic projection mapping with head motion
tracking,” in SIGGRAPH Asia 2020 Emerging Technologies, ser. SA
’20. New York, NY, USA: Association for Computing Machinery,
2020

[14] O. Bimber and R. Raskar, “Modern approaches to augmented reality,”
in ACM SIGGRAPH 2006 Courses, ser. SIGGRAPH ’06. New York,
NY, USA: Association for Computing Machinery, 2006, p. 1–es.

[15] J. J. Craig, “Introduction to Robotics,” pp. 42–43, 2005.
[16] “F710 wireless gamepad - console style - logitech g.” [Online].
Available: https://www.logitechg.com/en-us/products/gamepads/f710-
wireless-gamepad.940-000117.html

[17] A. Fedoseev, N. Chernyadev, and D. Tsetserukou, “Development of
mirrorshape: High ﬁdelity large-scale shape rendering framework for
virtual reality,” in 25th ACM Symposium on Virtual Reality Software
and Technology, ser. VRST ’19. New York, NY, USA: Association for
Computing Machinery, 2019.

(a) CobotAR.Skin

(b) CobotAR.Zoom

(c) CobotAR.Soldering

(d) Cobotar.Education

Fig. 12: Applications of CobotAR technology.

an AR system for immersive robotic lectures. The application
allows a teacher to visualize the interactive internal parts of
the robot using hand gestures.

Besides this, CobotAR could provide engaging activities
on robotic exhibitions and demonstrations. For instance,
CobotAR.Drawing could be a simple interface, where a user
guides the robot with hand gestures to draw a name or any
shape on the whiteboard by a marker.

VI. CONCLUSION AND FUTURE WORK

This paper introduced a novel HRI approach based on the
graphical interface, projected directly on the robot’s links,
and DNN-based gesture recognition. The supporting robot
move and orients the projected interface according to the
movements of the controlled robot. The developed approach
does not require an application of any wearable devices. We
performed a controlled experiment among three interfaces to
compare the overall performance, ease of use, and acceptance
rating.

The CobotAR interface provides immersive and intuitive
control to the user. Using CobotAR, participants achieved the
best results at Nasa TLX ratings, such as Mental Demand,
Physical Demand, Temporal Demand, Effort, and Frustra-
tion. Besides, the approach demonstrated accuracy and time
performance comparable to other interfaces. This suggests
that the CobotAR is suitable for robot operation, especially
for demonstration and education. The mental demand of the
CobotAR system is twice less than Wireless Gamepad and
by 16 percent less than Teach Pendant.

For future work, we consider improving the gesture recog-
nition algorithm and decreasing system delay to improve the

