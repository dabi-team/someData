1
2
0
2

t
c
O
9
1

]
T
S
.
h
t
a
m

[

1
v
5
8
7
9
0
.
0
1
1
2
:
v
i
X
r
a

Submitted to the Annals of Statistics

EFFICIENT AND CONSISTENT DATA-DRIVEN MODEL SELECTION FOR
TIME SERIES

BY JEAN-MARC BARDET 1, KAMILA KARE 1 AND WILLIAM KENGNE 2

1University Paris 1 Panthéon-Sorbonne, SAMM, France, bardet@univ-paris1.fr, kamilakare@gmail.com

2CY Cergy Paris Université, THEMA, France, william.kengne@u-cergy.fr

Abstract This paper studies the model selection problem in a large class
of causal time series models, which includes both the ARMA or AR(∞) pro-
cesses, as well as the GARCH or ARCH(∞), APARCH, ARMA-GARCH
and many others processes. We ﬁrst study the asymptotic behavior of the
ideal penalty that minimizes the risk induced by a quasi-likelihood estima-
tion among a ﬁnite family of models containing the true model. Then, we
provide general conditions on the penalty term for obtaining the consistency
and efﬁciency properties. We notably prove that consistent model selection
criteria outperform classical AIC criterion in terms of efﬁciency. Finally, we
derive from a Bayesian approach the usual BIC criterion, and by keeping all
the second order terms of the Laplace approximation, a data-driven criterion
denoted KC’. Monte-Carlo experiments exhibit the obtained asymptotic re-
sults and show that KC’ criterion does better than the AIC and BIC ones in
terms of consistency and efﬁciency.

CONTENTS

M

. . . .

. . .
. . .

. . . .
. . . .

. . . .
. . . .

Introduction . . .

. . .
1
2 Model selection framework . .

. . . .
. . . .
3.1 Notations and main assumptions . . . .
3.2 New asymptotic results satisﬁed by[θm .
. . . .

. . . .
. . . .
of parametric afﬁne causal models .
. . . .
. . . .
. . . .
. . . .
. . . .
. . . .
. . . .

2.1 Finite family
2.2 Maximum Likelihood Estimation . . . .
. . .
2.3 Quasi-Maximum Likelihood Estimation . . .
. . .
2.4 The penalization procedure . . .
. . .
3 Asymptotic behavior of the QMLE . .
. . .
. . .
. . .

. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
4 Efﬁcient model selection . . .
. . .
5 From a Bayesian model selection to a data-driven consistent model selection .
6 Examples of computations of the asymptotic expectation of ideal penalties . .
. . .
7 Numerical Studies . . . .
. . .
. . . .
8 Proofs . . .
. . .
. . .
. . .

. . .
. . .
. . .
8.1 Proofs of Section 3 . . .
8.2 Proofs of Section 4 . . .
. . .
. . .

. . . .
. . . .
. . . .
. . . .
. . . .
. . . .
. . . .
. . . .
. . . .
. . . .

. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .

. . . .
. . . .
. . . .
. . . .
. . . .

. . . .
. . . .
. . . .
. . . .
. . . .

. . . .
. . . .
. . . .
. . . .
. . . .

. . . .
. . . .
. . . .
. . . .
. . . .

. . .
. . .
. . .
. . .
. . .

. . .
. . .
. . .
. . .
. . .

References . . .

. . . .

. . . .

. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .

1
3
3
4
5
6
6
6
8
9
12
14
17
18
18
24
32

1. Introduction. Model selection is one of the fundamental tasks in Statistics and Data
Science. It aims at providing a model (or an algorithm) that is the best, following a criterion,
to represent observed data. Two leading model selection procedures have received a lot of
attention in the literature. On one hand, the resampling methods such as hold out or more
generally V -fold cross-validation are widely used in the machine learning community. On
the other hand, the methods based on the minimization of a penalized risk are also now very

MSC 2010 subject classiﬁcations: Primary 62G05, 62G20; secondary 62M05.

1

imsart-aos ver. 2020/08/06 file: data_driven_aos_3.tex date: October 20, 2021

 
 
 
 
 
 
2

popular in the community of applied or theoretical statisticians. They are certainly more ap-
propriate to be applied to time series since they take into account the dependence between
data. This will be our choice in this paper.

The main challenging task when designing a penalized based criterion is the calibration of
the penalty. This is mainly dependent on the goal one would like the ﬁnal criterion achieves.
For instance, the objective could be the consistency, the efﬁciency or the adaptive nature in
the minimax sense to name a few.

The consistency property aims at identifying the data generating process with high prob-
ability. Hence, it requires the assumption whereby there exists a true model in the set of
competitive models and the goal is to select this with probability approaches one as the sam-
ple size tends to inﬁnity. Although the consistency is a convincing mathematical property,
this asymptotic property is not always the most interesting when switching to a practical im-
plementation. Indeed, the true underlying process is generally unknown and trying to identify
the true model for any data is quite ambitious. It is often more plausible to assume that the
true data generating process is inﬁnite-dimensional, and that one tries to identify a "good"
ﬁnite-dimensional model based on the data ([19]). Therefore, it is common in this framework
to let the dimension of the competitive models to depend on the number of observations in
order to obtain a better approximation and to reduce the prediction’s risk. Hence, the model
selection is said to be efﬁcient when its risk is asymptotically equivalent to the risk of the
oracle.

In this work, we are interested by providing efﬁcient and consistent penalized data-driven
criteria for afﬁne causal time series, which are deﬁned by:

Class

AC

(1.1)

(M, f ) : A process X = (Xt)t
∈
ξt + f

Xt = M

(Xt

i)i

N∗

AC
i)i

N∗

(Xt

Z belongs to

(M, f ) if it satisﬁes:

for any t

Z.

∈

−
T is a sequence of zero-mean independent and identically distributed random
where (ξt)t
(cid:0)
∈
vectors (i.i.d.r.v) satisfying E(
R are two measur-
with r
ξ0|
|
able functions, where R∞ is the set of numeric sequence with ﬁnite number of non-zero terms.

1 and M , f : R∞ →

r) <

∞

≥

(cid:1)

(cid:0)

(cid:1)

−

∈

∈

For instance,

N∗

• if M

−

i)i

(Xt
∈
AR(p) process;
(cid:0)
(Xt

• if M

i)i
is an ARCH(p) process.
q

=

N∗

−

∈

(cid:1)

(cid:1)

(cid:0)

= σ and f

(Xt

−

i)i

N∗

∈

= φ1Xt

−

1 +

· · ·

+ φpXt

−

p, then (Xt)t
∈

Z is an

(cid:0)
a0 + a1X 2
t
−

1 +

(cid:1)

· · ·

+ apX 2
t
−

p and f

(Xt

−

i)i

N∗

∈

= 0, then (Xt)t
∈

Z

(cid:0)

(cid:1)

Note that numerous classical time series models such as ARMA(p, q), GARCH(p, q),
ARMA(p, q)-GARCH(p, q) (see [13] and [27]) or APARCH(δ, p, q) processes (see [13]) be-
long to
The study of these causal afﬁne time series more often requires the classical regularity condi-
tions on the functions M and f that are not really restrictive and remain valid for many time
series.

(M, f ).

AC

We will consider the semi-parametric class of models
N), where (fθ)θ
subset of Rd, d
∈
R and Mθ : R∞ →
Θ, fθ : R∞ →
θ
A ﬁnite family of models
M

Θ (a compact
Θ are two families of functions such as for
∈
) are known and the distribution of ξ0 is unknown.
pmax)

(for instance, the class of AR(p) processes where 0

Θ and (Mθ)θ
∈

(Mθ, fθ) where θ

AC

[0,

∞

≤

≤

∈

∈

p

DATA-DRIVENMODELSELECTIONFORTIMESERIES

3

will be considered, where a model m
tory (X1,
supposed to be observed (see Section 2).

∈ M
, Xn) generated from the class

· · ·

corresponds to a linear subspace of Rd. A trajec-
is
(Mθ∗, fθ∗) with the "true" model m∗ ∈ M

AC

There already exist several important contributions devoted to the model selection for time
series; we refer to the book of [31] and the references therein for an overview on this topic.
Also, the time series model selection literature is very extensive and still growing; we refer
to the monograph of [33], which provided an excellent summary of existing model selection
procedure, including the case of time series models as well as the recent review paper of [12].
The asymptotically efﬁcient selection property has already been tackled in case of linear pro-
cess type AR(

) by [37], [36], [22], [21], [20], and recently by [18].

∞

The study of a consistent model selection in this class of afﬁne causal processes has been
also considered by [4] and [25]. As in these papers, we consider here a risk built from the
Gaussian conditional log-Likelihood, which is naturally deduced for all causal afﬁne mod-
els
(Mθ, fθ), and consider a model selection criterion as a penalized Gaussian Quasi-
Maximum conditional log-Likelihood (see for instance [16] or [5]). This allows us:

AC

1. To study the asymptotic behavior of an ideal penalty that is deﬁned as providing a mini-

mization of the risk;

2. To determine the conditions for obtaining (or not) the asymptotic consistency of a crite-

rion, i.e. that it allows asymptotically to select the true model;

3. To determine the conditions for approaching (or not) in 1/n or even in o(1/n) the minimal

risk, thus to obtain an asymptotic efﬁciency;

4. To determine from a Bayesian approach the BIC criterion as well as a second data-driven
criterion called KC’, which is obtained by keeping all the second order terms in the
Laplace approximation and to prove that they verify the properties of asymptotic con-
sistency and efﬁciency in o(1/n).

In the end, we show that in the chosen framework the BIC and KC’ criteria offer all the
advantages with respect to the classical AIC criterion, which allows neither the asymptotic
consistency nor the same efﬁciency. Numerical simulations conﬁrm these results and also
show that the new data-driven KC’ criterion clearly outperforms the BIC criterion both in
terms of consistency and efﬁciency.

The paper is organized as follows. The model selection framework based on Gaussian Likeli-
hood risk is described in Section 2. In Section 3, the precise assumptions are stated and they
lead to new asymptotic results satisﬁed by the Quasi-Maximum Likelihood Estimator. The
asymptotic behavior of the ideal penalty is studied in Section 4 as well as some conditions for
obtaining the asymptotic efﬁciency or consistency of a criterion. In Section 5, the usual BIC
criterion as well as the data-driven criterion KC’ are studied. Finally, examples are detailed
in Section 6, numerical results are presented in Section 7 and Section 8 contains the proofs.

2. Model selection framework.

2.1. Finite family

of parametric afﬁne causal models. Assume a trajectory (X1, . . . , Xn)

is observed from a causal stationary solution of (1.1) where M and f are two known func-
tions depending on an unknown ﬁnite dimensional vector of parameters θ∗.

M

Now consider a ﬁnite family
In Proposition 1 of [4], due to the linearity of such models and because

of models belonging to parametric afﬁne causal models.
is a ﬁnite family,

M

M

4

N∗ and a unique couple
it was established that it is always possible to ﬁnd a dimension d
be-
of known functions (Mθ, fθ) with θ
∈
(Mθ, fθ). More precisely, there is a one-to-one correspondence between
longs to the class
each model m
the number of
and a linear subspace Θm ⊂
unknown parameters of the model m. As a consequence, if we denote m∗ the "true" model
(Mθ∗, fθ∗), we will say:
corresponding to

Rd such as in such a way that any model m

Rd and dim(Θm) =

AC
∈ M

∈ M

m
|

∈

|

AC

∈ M

• if m

• if m

Θm and Θm∗

is such that Θm∗
this is an overﬁtting’s case;
is such that Θm∗

⊂
Θm (also denoted m∗ 6⊂
For example, if m∗ corresponds to a AR(2) process and if
and ARCH(qmax), we have d = 1 + pmax + qmax and for θ = (θi)0

∈ M

M

6⊂

= Θm (also denoted m∗ ⊂

m and m∗ 6

= m),

m) , this is a misspeciﬁed case.

contains AR(pmax) processes

fθ((Xt

−

k)k

≥

1) =

pmax

Xi=1

θi Xt

−

i

and

Mθ((Xt

−

(θ0, θ1, θ2, 0, . . . , 0), (θ0, θ1, θ2)

Then Θm∗ =
ting, while an AR(1) or an ARCH(2) process implies a misspeciﬁed case.
In the sequel, we will always assume that

∈

(cid:9)

(cid:8)

≤

d,

i

≤

pmax+qmax

k)k

1) =

θ0 +

≥

Xi=pmax+1
, an AR(4) process implies an overﬁt-

(cid:17)

(cid:16)

R3

1/2

.

i

θi X 2
t
−

m∗

.
∈ M

This true model m∗ is supposed to be unknown. After observing the trajectory (X1, . . . , Xn),
our goal is to ﬁnd the most probable model (see Section 5) among the ﬁnite family
or a
"best" model that forecasts with a minimum risk. Here we have chosen a risk that is derived
from a Quasi-Maximum Likelihood contrast, which is presented below.

M

2.2. Maximum Likelihood Estimation. For each θ

by:

Θ, we will begin by deﬁning its risk

∈

(2.1) R(θ) := Pγ(θ) = E[γ(θ, X1)]

and

θ)2
f t

+ log(H t
θ)

(Xt −
H t
θ

with γ(θ, Xt) :=

:= fθ
θ := Mθ
(cid:0)
M t
θ :=
θ
(cid:0)
By referring to [30] or [16], the contrast γ(θ, .) is
2 times the Gaussian conditional
log-density of Xt. Moreover, the Gaussian Maximum Likelihood Estimator (MLE) is de-
) log-likelihood of
rived from the conditional (with respect to the ﬁltration σ
(X1, . . . , Xn) when (ξt) is supposed to be a Gaussian standard white noise. We deduce that
this conditional log-likelihood (up to an additional constant) Ln is deﬁned for a parameter θ
by:

(Xt
−
(Xt
2

k)k
≥
k)k

(Xt)t




1
(cid:1)
≥



−

(cid:8)

(cid:9)

(cid:0)

(cid:1)

(cid:1)

−

≤

0

1

.

f t
θ
M t
H t

(2.2)

Ln(θ) :=

1
2

−

n

γ(θ, Xt).

t=1
X

As it has been proved in [5], under a classical identiﬁability assumption, the risk function
R achieves its unique minimum at the "true"’ parameter θ∗ over any parameter set Θ, when
θ∗ ∈
(2.3)

Θ, i.e.

R(θ).

θ∗ = argmin
θ

Θ

∈

6
DATA-DRIVENMODELSELECTIONFORTIMESERIES

5

Therefore θ∗ is considered as an ideal predictor for the model selection procedure and serves
and Θm its parameter space
as a benchmark to compare predictors. Given a model m
that does not necessarily contains θ∗, let us deﬁne

∈ M

(2.4)

As a consequence, we have:

θ∗m = argmin
Θm
θ

∈

R(θ).

and if m∗ ⊂
Besides of minimizing the risk R, we also consider the minimization of its associated loss
function, which is deﬁned as

θ∗m∗ = θ∗

θ∗m = θ∗.

m,

(2.5)

ℓ(θ, θ∗) := R(θ)

R(θ∗)

0.

≥

−

This is a well-known measure of separation between the candidate model generated by θ and
the true one indexed by θ∗.

Let set by γn the associated empirical risk deﬁned by

γn(θ) := Pnγ(θ, .) =

1
n

γ(θ, Xt) =

2
n

−

Ln(θ),

n

t=1
X

so that maximizing the log-likelihood is equivalent to minimize the empirical criterion γn.

2.3. Quasi-Maximum Likelihood Estimation. Since the white noise is not necessarily a
Gaussian one and since the log-likelihood (and then the empirical risk) Ln(θ) depends on
Ln(θ) can be used as an approximation
(Xt)t
of the log-likelihood. It It consists of replacing γ(θ, Xt) by an approximation
γ(θ, Xt) and
Θ by
those statistics are deﬁned for all θ

0 that are unknown, a quasi-log-likelihood

≤

b

∈

b

(2.6)

Ln(θ) :=

1
2

−

b

with

γ(θ, Xt) :=

n

γ(θ, Xt)

t=1
X
b
(Xt −
H t
θ

θ)2
f t

b

b

b
for any deterministic sequence u = (un)n
u = 0 without loss of generality).
In addition the computable empirical risk is then:

b

∈

+ log(

H t
θ)

:= fθ(Xt
−
θ := Mθ(Xt
−
θ)2
M t
θ := (
N with ﬁnitely many non-zero values (we will use

, X1, u)
, X1, u)

2,
· · ·
2,
· · ·

and 


1, Xt
−
1, Xt

−

f t
θ
M t
b
H t
c
b



c

γn(θ) = Pn

γ(θ, .) =

1
n

γ(θ, Xt) =

2
n

−

Ln(θ).

n

t=1
X

b
∈ Mn, we deﬁne a Gaussian Quasi-Maximum Likelihood

b

Finally, for each speciﬁc model m
b
Estimator (QMLE)

b
θm as

(2.7)

b

The estimator

θm is commonly called the Empirical Risk Minimizer (ERM).

b

b

argmax
Θm

θ

∈

Ln(θ) = argmin
Θm

θ

∈

γn(θ).

θm ∈
b

b

6

2.4. The penalization procedure. For m

The goal is to come up with a model that minimizes the excess loss over

∈ M

, the ERM provides an estimator in Θm.

M

(2.8)

ℓ(

θm, θ∗).

argmin
m

∈M

This model is unknown since (2.8) depends on θ∗ and the distribution P(X1,...,Xn) that are
unknown.

b

A classical way to solve (2.8) problem is to design for every m
θm)
and we naturally choose
θm). First, it is well known that the empirical criterion
θm)
γn(
b
is an optimistic version of R(
θm) and decreases with the dimension of the model. Therefore,
b
b
it is common to add a penalty term to counteract this bias.
As a consequence, deﬁne a function pen: m
b
penalty function and is possibly data-dependent. We will only require that pen(m1)
m2. Then deﬁne the penalized contrast and the model selected by it:
pen(m2) when m1 ⊂
(2.9)

an estimator of R(
γn(

R+, which is called the

∈ M 7→

+ pen(m).

mpen = argmin

Cpen(m) :=

Cpen(m)

pen(m)

∈ M

with

≤

∈

b

b

θm

γn

In order to achieve (2.8), the ideal penalty to consider in (2.9) is

b

b

b

(cid:0)

b

(cid:1)

b

m

∈M (cid:8)

(cid:9)

(2.10)

penid(m) = R(

θm)

γn(

θm).

−

Using this deﬁnition, we obtain an "ideal" model deﬁned by:

(2.11)

mid :

∈

argmin
m

ℓ(

θm, θ∗)

∈M (cid:8)

b

b
R(

b
θm)

= argmin
m

∈M (cid:8)

(cid:9)

= argmin
m

∈M (cid:8)

(cid:9)

Cpenid (m)

.

(cid:9)

b

However, the function R is unknown except for very few particular and parametric cases and
therefore penid cannot generally be used directly. Therefore the question is how to choose the
penalty in (2.9) so that
mid. Hence, we would like our ﬁnal estimator
θ bmpen to behave asymptotically like the oracle. That is to satisfy:

mpen mimics the oracle

b

b

b

(2.12)
b

and/or for any n

n0

≥

b

P

(cid:16)

ℓ(

θ bmpen, θ∗)

≤

min
m
∈M

b

b
θm, θ∗)
ℓ(

(cid:8)

b

(cid:9)

+

C
n

1

−→n
→∞

(cid:17)

E

(2.13)

min
m
∈Mn
The aim of this paper is to ﬁnd a good choice of pen(m) in order to obtain the asymptotic
efﬁciency (2.13) or (2.12).

θ bmpen , θ∗)

θm, θ∗)

(cid:3)o

ℓ(

ℓ(

≤

+

b

b

(cid:2)

(cid:3)

(cid:2)

.

E

C
n

3. Asymptotic behavior of the QMLE. Before considering the problem of model se-
, i.e. as well
lection, we establish a central limit theorem satisﬁed by
if m is an overﬁtted or a misspeciﬁed model. Before this, some notations and assumptions
have to be precised.

θm for any model m

∈ M

b

3.1. Notations and main assumptions.

In the sequel, we will consider a subset Θ of Rd

which is compact. We will use the following norms:

denotes the usual Euclidean norm on Rν , with ν

.
k
k

•
• for a matrix A, denote

the subordinate matrix norm such that

1;

≥

A
k
k

• if X is a Rν -random variable and r

1, we set

≥

X
k

kr =

E

X
k

r
k

= sup
=0
v

A
k
k
1/r

[0,

∈

∞

;

A v
v

k
k

k
k
];

(cid:0)

(cid:2)

(cid:3)(cid:1)

6
DATA-DRIVENMODELSELECTIONFORTIMESERIES

7

E where E = Rν or E is a set of square matrix, denote

θ

∈

Θ

∈

⊂

R is a

Rd, if Ψθ : R∞ →
• for θ
Θ
∈
⊂
;
Ψθ(
)
kΘ = sup
)
Ψθ(
k
·
k
k
·
Rd, if Ψθ : R∞ →
(cid:9)
(cid:8)
• for θ
Θ
∂
)
Ψθ(
=
∂θi
·
• consider Ψθ : R∞ →
Assumption A(Ψθ, Θ):
numbers

) =
∂θΨθ(
·

≤
(cid:17)
(cid:0)
R for any θ
∈
kΘ <
Ψθ(0)
k
1 such that

αk(Ψθ, Θ)

Θ

(cid:16)

≤

k

d

1

i

≥

⊂

2(Θ

C

×

R∞) function, we will denote
∂2
∂θi∂θj
Rd. Then, we deﬁne the assumption:

) =
θ2Ψθ(
·

and ∂2

(cid:16)

(cid:1)

≤

≤

d

1

i

)
∂θiΨθ(
·

)
Ψθ(
·

(cid:17)

;

i,j

1

≤

≤

d

and there exists a sequence of non-negative real

∞
∞k=1 αk(Ψθ, Θ) <

and satisfying:

∞

(cid:0)

Ψθ(x)
k

−

(cid:1)
Ψθ(y)

kΘ ≤

∞

P
xk −
αk(Ψθ, Θ)
|

yk|

for all x, y

R∞.

∈

Xk=1

Several assumptions on the AC class will be considered thereafter:

Assumption A0: The process X

∈ AC
• the white noise (ξt)t is such as
ξ0kr <
k
R∞, the functions θ
• for any x
→
Rd is a compact set such as
• Θ

∈

(Mθ∗, fθ∗) where θ∗ ∈
with 8 < r;
fθ are

∞
Mθ and θ

→

C

2(Θ) functions:

Θ is deﬁned in (1.1) where:

∈

(3.1) Θ

θ

∈

⊂

n

Rd, A(fθ,

θ
{

) and A(Mθ,
}

θ
{

) hold with
}

∞

Xk=1

αk(fθ,

θ
{

) +
}

ξ0kr
k

∞

Xk=1

αk(Mθ,

θ
{

.

) < 1
}

o

Under this assumption, [14] showed that there exists a stationary causal (i.e. Xt is depending
Z) and ergodic solution of (1.1) with r-order moment for any
only on (Xt
Θ.
θ

N for any t
∈

k)k

∈

−

∈

Now the assumption A0 holds. We will also add several assumptions required for insur-
ing the strong consistency and the asymptotic normality of the QMLE:

The ﬁrst following classical assumption ensures the identiﬁability of θ∗.

Assumption A1: For all θ, θ′ ∈

Θ, (f 0

θ = f 0

θ′ and M 0

θ = M 0

θ′) a.s. =

⇒

θ = θ′.

REMARK 1. Even if this assumption is a classic one in an M-estimation framework,
it is important to remark that it does not cover all the cases of model selection of usual
causal time series. Indeed, in the case of the family of ARMA processes, it is well known
that a model is unique when both the polynomials P0 and Q0 of AR and MA parts are
coprime. Hence, for instance, if the true model is an ARMA(p0, q0) process, any ARMA(p0 +
r) and Q(X) =
1, q0 + 1) representation with respective polynomials P (X) = P0(X)(X
R.
Q0(X)(X
Then, Assumption A1 is never satisﬁed for ARMA processes in case of overﬁtting. However,
by initializing θ around 0 in the optimization algorithm, we have noticed from Monte-Carlo
experiments that the algorithm always converges to θ∗ and not other solution.

r) of AR and MA parts, is identically the same as the true model whatever r

−

−

∈

8

Next, the following Assumption ensures the invertibility of the asymptotic covariance matrix
G and F (see below) that is necessary to prove the asymptotic normality of the QMLE (see
for instance [5]).

Assumption A2: < α, ∂θf 0

θ >= 0 =

⇒

α = 0 a.s. or < α, ∂θM 0

θ >= 0 =

⇒

α = 0 a.s.

The deﬁnition of the computable empirical risk and requires that its denominators do not
vanish. Hence, we are going to assume throughout this paper that the lower bound of Hθ(
) =
·
)
Mθ(
·

2 is strictly positive:

(cid:0)
(cid:1)
Assumption A3:

h > 0 such that inf
Θ
∈

θ

∃

(Hθ(x))

≥

h for all x

R∞.

∈

The following assumption is a technical classical condition (see [28]).

Assumption A4: For every m

, if (θm,n) is a sequence of Θm satisfying θm,n

∈ M

a.s.
−→n
+
→

∞

θ∗,

then

(3.2)

E

lim sup

n

→∞ n

h(cid:16)(cid:13)
(cid:13)
(cid:13)

1
n

(cid:0)

∂2
θiθj Ln(θm)

i,j

(cid:1)

m

∈

REMARK 2. Note that under assumption A0, if θm,n

1
n

∂2
θiθj Ln(θm)

i,j

m

∈

1

8

−

a.s.
−→n
+
→

(cid:16)

(cid:13)
(cid:13)
(cid:13)

(cid:0)

(cid:1)

(cid:17)

(cid:13)
(cid:13)
(cid:13)

−

(cid:16)(cid:0)

∞ (cid:13)
(cid:13)
(cid:13)

Thus, from the Egorov’s Theorem, we can ﬁnd an event
Ω with sufﬁciently large probability
such that the relation (3.2) in the assumption A4 holds if the expectation is taken on the event
Ω. For the particular case of the linear processes, the assumption A4 holds true under a mild
condition on the distribution of X, see for instance [32] and [15].
e
Finally, the decrease rates of (αj(fθ, Θ))j , (αj(Mθ, Θ))j , (αj(∂θfθ, Θ))j and (αj(∂θMθ, Θ))j
have to be fast enough for insuring the strong consistency and the asymptotic normality of
the QMLE:

e

Assumption A5: Conditions A(fθ, Θ), A(Mθ, Θ), A(∂θfθ, Θ), A(∂θMθ, Θ), A(∂2
and A(∂2

θ2Mθ, Θ) hold with

θ2fθ, Θ)

αj(fθ, Θ) + αj(Mθ, Θ) + αj(∂θfθ, Θ) + αj(∂θMθ, Θ) = O(j−

δ) where

δ > 7/2.

Note that Assumption A5 does not allow to consider long-range dependent processes, but
usual short memory causal time series satisfy this assumption.

3.2. New asymptotic results satisﬁed by

already established in [5] when m = m∗ and in [4] when m∗ ⊂
can also be extended in the case of misspeciﬁed model, i.e. when m∗ 6⊂
First, the following corollary is a particular case of a more general result, Proposition 4,

m.

b

b

θm. The asymptotic normality of

θm has been
m (overﬁtting). This property

1

8

−

<

∞

(cid:17)

io
θ∗m then

(cid:13)
(cid:13)
(cid:13)
a.s.
−→n
+
→
1
∂2
θiθj γ(θ∗m)
2

∞

.

1

8

−

i,j

m

∈

(cid:17)

(cid:1)

(cid:13)
(cid:13)
(cid:13)

DATA-DRIVENMODELSELECTIONFORTIMESERIES

9

which is stated in Section 8 devoted to the proofs.
To begin with, and from Assumption A2 and A5, we can deﬁne the deﬁnite positive matrix

(3.3)

(3.4)

Now, for any m

Cov

∂θiγ(θ, X0) , ∂θj γ(θ, Xt)

(cid:0)

(cid:1)(cid:17)

i,j

1

≤

≤

d

∂2
θi θj γ(θ, X0)
h

(cid:16)
Θ, denote:

.

i,j

d

≤

1
i(cid:17)

≤

(3.5)

Gm(θ) :=

Cov

∂θiγ(θ, X0) , ∂θj γ(θ, Xt)

i,j

m

∈

(cid:1)(cid:17)

G(θ) :=

F (θ) :=

1
4

−

Z
(cid:16)Xt
∈
1
E
2

and θ

∈

∈ M
1
4

Z
(cid:16)Xt
∈
Gm(θ∗) =

(cid:0)
1
4

(cid:16)

=

⇒

Cov

∂θiγ(θ∗, X0) , ∂θj γ(θ∗, X0)

if m∗ ⊂

m

m

∈

i,j

(cid:1)(cid:17)

(3.6)

E

Fm(θ) :=

∂2
θi θj γ(θ, X0)
h
COROLLARY 1. Let m

−

(cid:16)

.

m

∈

i,j

i(cid:17)

1
2

(cid:0)

∈ M

deﬁned in (2.4),

(3.7)

Using mainly this new result, we also obtain:

1
√n

(cid:16)

∂θj Ln(θ∗m)

(cid:17)

D
→∞N
−→n

j

m

∈

0 , Gm(θ∗m)

.

(cid:0)

(cid:1)

and suppose that Assumptions A0-A5 hold. Then, with θ∗m

THEOREM 3.1. Under Assumptions A0-A5, for any m

,

√n

(3.8)

θm)i −
(
D
→∞ N
−→n
with Gm and Fm deﬁned in (3.5) and (3.6).
b

(θ∗m)i

m

(cid:0)

(cid:1)

∈

i

0 ,

Fm(θ∗m)

(cid:16)

(cid:0)

(cid:1)

∈ M
1Gm(θ∗m)

−

Fm(θ∗m)

1

−

,

(cid:0)

(cid:17)

(cid:1)

Hence, even in the misspeciﬁed case,
θm satisﬁes a central limit theorem. We will use this
result several times, in particular to prove that the probability of selecting a misspeciﬁed
model tends quickly enough towards 0. Another technical result will also be useful for the
sequel:

b

(3.9)

1) where δ > 7/2 is given in Assumption A5 and for any m

PROPOSITION 1. Under Assumptions A0-A5, with 8/3 < r′ ≤
∈ M
<

θm)i −
(
b
Note that we also have supn
θm)i −
(
tial for establishing the asymptotic behavior of the expectation of the ideal penalty.
b

(cid:1)
(θ∗m)i

, then we have

(cid:0)
√n

(cid:13)
(cid:13)
(cid:13)
N∗

sup
N∗
n

(cid:13)
(cid:13)
(cid:13)
m

(θ∗m)i

2 <

√n

∞

∞

(cid:13)
(cid:13)

(cid:13)
(cid:13)

m

r′

(cid:1)

(cid:0)

∈

∈

∈

∈

.

i

i

(8 + r)/6 and r′ < 2(δ

−

. This result will be essen-

4. Efﬁcient model selection. The expectation of the ideal penalty (2.10) has been com-
puted (or asymptotically approximated) in several frameworks (see [29], [1], [34], [19], [7];
etc) and it is most often proportional to the dimension of the model (denoted
in the
sequel).

m
|

|

• the penalty is 2
criterion [29];

m
|

|

σ2/n in the regression setting, leading to the famous Mallows’s Cp

10

• the penalty is 2

mous AIC criterion [1];

m
|

/n in the density estimation framework and others, leading to the fa-
|

m
|

|

• the penalty is

log(n)/n in the Bayesian density estimation setting and other frame-

works, leading to the famous BIC criterion [34] ;
Bn A−
n

• the penalty is Trace

/n where An is the opposite of the Hessian matrix of the
log-likelihood and Bn the Fisher Information matrix in a general framework issued from
a Bayesian setting [28].

(cid:0)

(cid:1)

1

In order to approximate (2.10) in this framework, let ﬁrst provide a decomposition of this
term in order to facilitate the computation. For any model m

, write

∈ M

(4.1)

penid(m) := R(

θm)

−

γn(

θm) = I1(m) + I2(m) + I3(m),

with

b




b
θm)
I1(m) := R(
b
γn(θ∗m)
I2(m) :=
b
I3(m) := R(θ∗m)

.

−
−
−

R(θ∗m)
θm)
γn(
γn(θ∗m)
b
b
b

b
Next we provide a preliminary result about the asymptotic behavior of the terms I1(m) and
I2(m). Then we obtain:



LEMMA 1. Under Assumptions A0-A5, for any model m

distribution U ∗(m) such that

, there exists a probability

∈ M

−

Trace

Fm(θ∗m)

−

(cid:16)(cid:0)

(cid:1)

1 Gm(θ∗m)
(cid:17)

.

1.

n I1(m) = n

R(

θm)

R(θ∗m)

−

(4.2)

(cid:0)
and E
b

n I1(m)

U ∗(m)

D
−→n
→∞
E[U ∗(m)] =

(cid:1)
−→n
→∞

2.

n I2(m) = n

(cid:2)
γn(θ∗m)

−

(cid:3)
θm)
γn(

(4.3)

(cid:0)
and E
b

n I2(m)
b

U ∗(m)

D
−→n
→∞

(cid:1)
b
→∞ −
−→n

(cid:3)

Trace

Fm(θ∗m)

1

−

Gm(θ∗m)

.

(cid:16)(cid:0)

(cid:1)

(cid:17)

(cid:2)

The proof of this lemma, as well as all the other proofs, can be found in Section 8. This result
leads to our ﬁrst main result:

PROPOSITION 2. Under Assumptions A0-A5, there exists N0 ∈

N0,

N such as for any n

≥

(4.4)

argminm

∈M

E

ℓ(

θm, θ∗)

= m∗.

(cid:2)
Another application of Lemma 1 is devoted to an expansion of the expectation of the ideal
penalty deﬁned in (2.10):

b

(cid:3)

sequence (v∗n)n

PROPOSITION 3. Under Assumptions A0-A5 and for any m
N∗ , not depending on m when m∗ ⊂
penid(m)

Fm(θ∗m)

Trace

(4.5)

E

−

∈

1 Gm(θ∗m)

m, and satisfying

∈ M

2
n

→∞ −
∼n

+

v∗n
n

.

(cid:16)(cid:0)
Note that the Slope Heuristic Procedure, which allows to estimate a so called minimal penalty
(see [2]) consists in evaluating the slope of a linear regression of
m

(cid:17)(cid:17)

(cid:1)

(cid:3)

(cid:2)

, there exists a bounded

and this is equivalent to estimating the slope of

1
n

Trace

(cid:0)

θm) onto
γn(
1
Gm(θ∗m)Fm(θ∗m)−

m
|

|
onto

for m∗ ⊂
m
|
|

from

b

b

(cid:1)

DATA-DRIVENMODELSELECTIONFORTIMESERIES

11

∈ M

1

(cid:0)

Gm(θ∗m)Fm(θ∗m)−

behaves as a linear function of

in
(4.3). We will see that Trace
many cases, which also gives legitimacy to this approach in the case of time series after
having obtained it in the case of linear regression. The minimal penalty is then
the
2
×
estimated slope and this ﬁnally corresponds to an approximation of E
. The trace
of the matrix mentioned above is easily computable in some cases using the explicit forms
of the matrices F (θ∗), G(θ∗) in [5]. Hence, even if the ideal penalty cannot be explicitely
obtained, we can replace it with its expectation, i.e. this trace of matrix. Then, deﬁne for
m

penid(m)

m
|

−

(cid:1)

(cid:2)

(cid:3)

|

,

−

−

(4.6)

Trace

F (θ∗m)

pen(m) :=

2
n
(cid:16)(cid:0)
in some
As shown in Section 6, this trace is proportional to the dimension of the model
. In case of Gaussian process, we will also
cases, but could be more complex functions of
m
|
see that it corresponds to the AIC penalty, 2/n. However, we will see that this penalty does
not provide a consistent model selection and contrary to the ideal penalty, does not provide
an optimal efﬁcient model criterion.
Before this, we study the probability of not selecting a misspeciﬁed model under a general
condition on the penalty that is satisﬁed for instance by

1 G(θ∗m)
(cid:17)

pen or by BIC criterion:

m
|

f

(cid:1)

|

|

.

THEOREM 4.1. Under Assumptions A0-A5 and if for any ε > 0,

f

(4.7)

Then,

(4.8)

n P

pen(m)

(cid:0)

ε

≥

(cid:1)

−→n
→∞

0

for any m

.

∈ M

n P

m∗

mpen

6⊂

(cid:0)

0.

−→n
→∞

(cid:1)

Theorem 4.1 says that if the penalty asymptotically decreases to 0 in probability, then the
b
criterion
Now, we state the main results of this paper, which specify the convergence rate of pen to
obtain an excess loss close to the minimal one over

Cpen does not select a misspeciﬁed model asymptotically.

:

b

M

THEOREM 4.2. Under Assumptions A0-A5, and if for any ε > 0 there exists Kε > 0

such as

(4.9)

lim sup

n

→∞

max
m
∈M

P

n pen(m)

Kε

≥

ε.

≤

Then for any ε > 0, there exists Mε > 0 and Nε ∈
(4.10)
ℓ(

θ bmpen, θ∗)

ℓ(

P

θm, θ∗)

(cid:16)

(cid:8)

≤

min
m
∈M

(cid:17)
N∗ such as for any n
Mε
n

+

≥

−

1

ε.

(cid:9)

(cid:17)

Nε,

≥

(cid:16)

b

b
REMARK 3. Let notice that this asymptotic optimality is quite a bit different from the
classical one about asymptotic efﬁciency, where both the cardinal of the collection
and
the dimension of competitive models are allowed to depend on n. However, this is done in
the framework where the parameter θ∗ is inﬁnite-dimensional (see for example [37], [26],
[18]).

M

Now, we provide a condition on the penalty allowing to obtain a consistent criterion. More-
over, we also prove that such criterion satisﬁes a sharper efﬁciency inequality than (4.10):

12

THEOREM 4.3. Under Assumptions A0-A5, if the penalty pen satisﬁes (4.7), and if for

0

−→n
→∞

(cid:12)
(cid:3)
= m∗. Then we have,
(cid:12)

any m

(4.11)

∈ M

such as m∗ ⊂
n E[en(m)]

m and m

= m∗,

and

n E

en(m)

E[en(m)]

with en(m) = pen(m)

(4.12)

→∞ ∞
−→n
−
(cid:2)(cid:12)
m and m
pen(m∗) > 0 since m∗ ⊂
(cid:12)
P
mpen = m∗

1.

−

−→n
→∞
Moreover, for any ε > 0 and η > 0, there exists Nε,η ∈
P
(4.13)
θm∗ , θ∗) +
ℓ(

θ bmpen , θ∗)

ℓ(

b

(cid:0)

(cid:1)

and there exists Nη ∈
(4.14)

≤
N such as for any n
b

(cid:16)

E

ℓ(

θ bmpen , θ∗)

Nη,
b

≥

E

≤

min
m
∈M

ℓ(

θm, θ∗)

+

η
n

.

Nε,η,

≥

N∗ such as for any n
η
n

ε,

−

≥

1

(cid:17)

(cid:2)

b

(cid:3)

(cid:2)

(cid:3)

b
m

m

. This is also such a case for Hannan-Quinn criterion (pen(m) = log(log n)

and therefore en(m) = log n
m
n
| −
|
, see [17])

The best known criterion satisfying the conditions of this theorem and in particular (4.11) is
certainly the BIC criterion for which pen(m) = log n
n |
m∗|
|
or if pen(m) = √n
as we used it in [4]. Note also that both the consistent data-driven
(cid:1)
n |
criteria mentioned in the next section (see (5.4) deﬁned in [23], and (5.5)) also verify the
conditions of Theorem 4.3.
On the contrary, the AIC criterion with pen(m) = 2
pen(m)
n |
do not satisfy these conditions. The following theorem even shows that these criteria asymp-
totically overﬁt and have a less good asymptotic efﬁciency than consistent criteria satisfying
Theorem 4.3:

, or the criterion with penalty

m
|

f

m

(cid:0)

n

|

|

|

|

THEOREM 4.4. Assume that there exists g :

[ such as pen(m) = g(m)/n for
M →
. Then, under Assumptions A0-A5, the probability of overﬁtting is asymptotically

[0,

∞

and there exists M > 0 such as for n large enough,

(cid:1)

(4.16)

E

ℓ(

θ bmpen , θ∗)

P

mpen overﬁts

> 0.

lim inf
n
→∞

(cid:0)

b

E

≥

min
m
∈M

(cid:2)

(cid:3)

ℓ(

θm, θ∗)

+

M
n

.

(cid:3)
m

b
COROLLARY 2. Theorem 4.4 is valid for pen(m) = 2
n |

b

(AIC criterion) or pen =

pen.

|

To conclude, in this context where a true model belonging to a ﬁnite family of models exists,
f
the consistent criteria are those which also propose the best asymptotic efﬁciency. The next
section focuses on these. However, a criterion like the AIC criterion or more generally the
criterion with penalty
pen regain all their optimality properties in asymptotic efﬁciency when
the model family is inﬁnite or when the true model does not belong to the family.

(cid:2)

f

5. From a Bayesian model selection to a data-driven consistent model selection. An-
other classical paradigm for model selection is the Bayesian one, leading typically to the BIC
criterion (see [34]). In this approach, the construction of the model selection criterion is ﬁrst
done by assuming that the parameter vector θ∗ is a random vector. Let recall the hierarchical
, a model
prior sampling scheme in the Bayesian setting: given the ﬁnite family of models

M

any m
∈ M
positive i.e.

(4.15)

6
6
DATA-DRIVENMODELSELECTIONFORTIMESERIES

13

m is drawn according to a prior distribution (πm)m
then, conditionally on m, θ is sampled according to some prior distribution µm(θ).

(generally a uniform distribution) and

∈M

The goal of this model selection procedure is to choose the most probable model after ob-
serving the trajectory X := (X1,
(5.1)

, Xn), i.e.
· · ·
mB = argmax

m

X

P

.

Using Bayes Formula, we can write P

b

m

m

∈M (cid:8)
=
X

|

|

(cid:0)
πm P

X
|
P(X)
(cid:0)
θ, m

(cid:1)(cid:9)
m

(cid:1)

. Moreover, we have:

dµm(θ).

P

X

m

|

(cid:0)
=

(cid:1)
P

X

|

ZΘm

(cid:0)
(cid:1)
In addition, since P(X) does not depend on m, and P
, maximizing P
θ

Θm and m

m

X

(cid:0)

(cid:1)
X

|
is equivalent to maximize

|

(cid:0)

(cid:1)

∈

∈ M

θ, m

is the likelihood of X given

Sn(m, X) := log

P

X

(cid:0)
m
|

(cid:1)
= log

πm exp

Ln(θ)

dµm(θ)

.

b

From now on, we will assume that πm = 1/
of the models in the family
function θ

M

bm(θ) such as dµm(θ) = bm(θ) dθ. Then we have:

(cid:0)

(cid:1)(cid:1)

(cid:0)
(cid:0)
, a priori uniform distribution
∈ M
. We can also assume that there exists a non-negative Borel

|M|

(cid:1)

(cid:17)

(cid:16) ZΘm
for any m

→

(5.2)

Sn(m, X) =

−

log(

|M|

) + log

(cid:16) ZΘm

bm(θ) exp

Ln(θ)

dθ

.

(cid:0)

(cid:17)

(cid:1)

b

Let us give an asymptotic expansion of the a posteriori probability in order to derive a BIC
type criterion that is coherent with our framework where the observed trajectory is that of a
causal afﬁne process. This could be obtained from a Laplace approximation, leading to the
following theorem:

THEOREM 5.1. Under Assumptions A0, A1, A2, A3, A5 and if for any x

6(Θ) functions satisfying A(∂k

R∞, the
∈
θk fθ, Θ) and A(∂k
θk Mθ, Θ)

functions θ
for any 0

≤

Mθ and θ
6. Then

→
k

≤

fθ are

C

→

(5.3)

Sn(m, X) =

Ln(

θm)

log(n)
2

m
|

|

−

+ log

bm(

θm)

b

+

log(2π)
b
b
2
γn(

m
|
θm)

∂2
θiθj

where

Fn(m) :=

1
2

| −

log

det

(cid:1)
b
Fn(m)

(cid:0)

−

−

log(

|M|

) + O(n−

1) a.s.

.

m

∈

(cid:0)

(cid:0)

b

(cid:1)(cid:1)

i,j

(cid:0)
In the above equation, it is clear that
a.s.. This gives
Sn(m, X)
legitimacy to the usual BIC criterion within the framework of causal afﬁne processes since:

θm) + log(n)

m
|

≃ −

Ln(

−

b

b

b

2

2

(cid:1)

|

mBIC = argminm

b

∈M

b

b

Ln(

θm) + log(n)

m
|

2

−

,

|
o

mBIC maximizes the main terms of

and we see that
From the relation (5.3), considering certain second order terms of the asymptotic expansion
of
Sn(m, X), we also obtain the Kashyap criterion (see Kashyap [23], Sclove [35], Bozdogan
[6]), denoted KC criterion, deﬁned for all m

Sn(m, X).
b

b
by

b

b

b

n

∈ M

KC(m) :=

−

2

Ln(

θm) + log(n)

+ log

det

m
|

|

d

b

b

(cid:0)
and

Fn(m)

−
(cid:0)
mKC = argminm

(cid:1)(cid:1)

b

∈M

b

KC(m)

.

(cid:8)

d

(cid:9)

b
(5.4)

14

(cid:0)

b

−

det

m
|

Fn(m)

|
mKC can be more interesting that

is added to the usual BIC criterion. Several exam-
Therefore the term log
but not always, are provided in
ples of computations of this term, generally equal to c
(cid:0)
(cid:1)(cid:1)
the forthcoming Section 6. It is clear that
mBIC in terms
of consistency only for non asymptotic framework (typically for n of the order of a hundred
or several hundred). Note also that the data-driven criteria KC that is "optimal" in the sense
of the a posteriori probability (see Kashyap [23]) is also asymptotically consistent under the
Assumption A5.
However, this choice of second order terms of the asymptotic expansion of
Sn(m, X) is
somewhere arbitrary. A criterion taking account of all the second order terms could also be
deﬁned. For this, we could deﬁne a uniform distribution bm on a compact set included in Θm.
As a consequence, using condition (3.1) of Assumption A0, there always exists 0 < C1 ≤
C2
such as C1
. As a consequence, we could deﬁne a new data-driven consis-
m
tent criterion, called KC ′, such as for any m

bm(Θm)

C2
m
|

| ≤

≤

b

b

b

|

|

∈ M

(5.5)
KC ′(m) :=

−

2

Ln(

θm) +

log(n)

log(2π)

−

d

b

b

(cid:0)

(cid:1)

m
|

|
and

+ log

det

Fn(m)

−
(cid:0)
mKC ′ = argminm

(cid:0)

b

+ 2 log

m
|

(cid:0)
KC′(m)

|
(cid:1)
.

(cid:1)(cid:1)
∈M

a.s.
−→n
+
→

∞

(cid:8)
d
F (θ∗m) where

(cid:9)

b

b

REMARK 4. We also know that under Assumptions A0-A5,

b

Fn(m)

F is deﬁned in (3.4). Therefore the term log
log

in the expression of

det

Fm(θ∗m)

det
KC ′(m).
(cid:0)
(cid:0)

−

−

b
(cid:1)(cid:1)

Fn(m)

can also be replaced by

(cid:0)
COROLLARY 3. The criteria BIC, KC and KC’ are consistent model selection criteria
d

(cid:1)(cid:1)

(cid:0)

and satisfy Theorem 4.3.

Thus, these three criteria are asymptotically consistent and asymptotically efﬁcient following
the inequalities (4.13) and (4.14). Monte-Carlo experiments in Section 7 will also exhibit
that
mBIC in terms of consistency and
efﬁciency when the n size of the trajectory is of the order of a hundred or a thousand.

mKC ′, which is a data-driven criterion, outperforms

b

[5], with µ4 = E[ξ4

0], f 0

θ and H 0

6. Examples of computations of the asymptotic expectation of ideal penalties. From
θ deﬁned in (2.1), we have for m∗ ⊂
∂θiH 0
θ∗
m
(H 0
θ∗
m
∂θj H 0
θ∗
m
)2

∂θif 0
θ∗
m
H 0
θ∗
m
∂θif 0
θ∗
m
H 0
θ∗
m

∂θj H 0
θ∗
m
)2

(µ4 −
4

i,j = E

m and i, j

Gm(θ∗m)

∂θj f 0
θ∗
m

∂θj f 0
θ∗
m

Fm(θ∗m)

i,j =

m:

1
2

1)

+

+

−

E

∈

i

h

(cid:1)

(cid:0)

,

(6.1)

(cid:0)

(cid:1)

h

m:

Here there are 3 frameworks where Trace
m∗ ⊂
1/ A ﬁrst and well-known case is the Gaussian case. Indeed, when (ξt) is a Gaussian white
noise, then µ4 = 3 and then from (6.1), for any i, j

Fm(θ∗m)

(cid:16)(cid:0)

m,

(cid:1)

−

∂θiH 0
θ∗
m
(H 0
θ∗
m
1 Gm(θ∗m)
(cid:17)

i
can be computed for

∈

Gm(θ∗m)

i,j =

Fm(θ∗m)

i,j

−

(cid:0)

(cid:1)

(cid:0)

=

(cid:1)
⇒ −

2 Trace

Fm(θ∗m)

−

(cid:0)(cid:0)

(cid:1)

1 Gm(θ∗m)
(cid:1)

= 2 Trace

I

m
|

|

= 2

m
|

,
|

(cid:0)

(cid:1)

DATA-DRIVENMODELSELECTIONFORTIMESERIES

15

∈

N∗. As a consequence, in the Gaussian framework, for
m, the expectation of the ideal penalty is exactly the classical Akaike Criterion (AIC).

with Iℓ the identity matrix of size ℓ
m∗ ⊂
2/ A frequent case is when the parameter θ identifying an afﬁne causal model Xt = M t
can be decomposed as θ = (θ1, θ2)′ with f t
and
p1 =
θ2|
|
In such a case, from (6.1), it is clear that all the terms Fm(θ∗m)i,j and Gm(θ∗m)i,j are equals
e
, p2 implying
to zero for i = 1, . . . , p1 and j = 1,

θ ξt +f t
θ
. Let p1, p2 such that

= p1 + p2.

and M t

, p2 =

θ1|
|

M t
θ2

m
|

θ =

θ =

f t
θ1

f

|

· · ·

Fm(θ∗m) =

Op1,p2

A1,p1
Op2,p1 Bp1+1,p1+p2 (cid:19)

−

(cid:18)

and Gm(θ∗m) =

A1,p1
Op2,p1

(cid:18)

Op1,p2

(µ4−

1)

2 Bp1+1,p1+p2 (cid:19)

where O is the null matrix and from the expressions of matrix Gm(θ∗m) and Fm(θ∗m) in (6.1),
∂θi f 0
∂θj f 0
θ∗
θ∗
and Bp1+1,p1+p2 =
A1,p1 =
m
m
H 0
θ∗
p1
m
≤
As a consequence,
(cid:16)

∂θi H 0
∂θj H 0
θ∗
θ∗
m
m
(H 0
)2
θ∗
m

p1+1

i(cid:17)

i(cid:17)

E

E

(cid:16)

i,j

i,j

1
2

h

h

≤

≤

≤

1

p1+p2

.

(µ4 −
2
1)

1)

Bp1+1,p1+p2

Ip2

Diag

A−

1
1,p1

(cid:0)

×

(cid:17)

1

, B−

p1+1,p1+p2

(cid:1)

Gm(θ∗m) Fm(θ∗m)−

1 =

=

−

−

Diag

A1,p1,

(cid:16)
Ip1,

Diag

(cid:0)

(µ4 −
2

and we obtain

(6.2)

−

This setting includes many classical times series:

(cid:1)

(cid:16)(cid:0)

• For ARMA(p, q) processes, we have Xt = f t

(cid:1)
1 Gm(θ∗m)
(cid:17)

2 Trace

Fm(θ∗m)

−

= 2 p1 + (µ4 −

1) p2.

1 +

ξt + b1 ξt

for all t
σ
The penalty term is slightly different according to σ is known or not:
(a) if σ is known, then θ = θ1 and Gm(θ∗) =

+ bq ξt

· · ·

∈

(cid:0)

(cid:1)

(cid:0)

−

−

q

AIC penalty term:

Fm(θ∗), so that we recover exactly the

−

θ + σ ξt since Xt + a1 Xt
−

p =
1 +
a1, . . . , ap, b1, . . . , bq) and θ2 = σ.

Z. Then θ1 =

+ ap Xt

· · ·

−

2 Trace

Gm(θ∗m)Fm(θ∗m)−

1

−

= 2

m
|

|

= 2 (p + q);

(b) if σ is unknown, θ = (θ1, σ) and simple computations lead to

(cid:0)

(cid:1)

Fm(θ∗) =

(Fm(θ∗)

(cid:18)

(cid:1)

i,j

1

m

|−

≤|

1
≤
0

0
1
2 σ4 (cid:19)

−

and Gm(θ∗) =

(Gm(θ∗)

(cid:1)

i,j

1

m

|−

≤|

1
≤
0

0
1)
(µ4−
4 σ4 !

where (Gm(θ∗)

1

i,j

m

≤
Thus, we obtain Gm(θ∗)Fm(θ∗)−

|−

≤|

(cid:1)

1 =

1 =

−

(Fm(θ∗)

m

1

i,j
≤
I1

≤|
i,j

≤

1.
m

|−

|−

≤|
0

(cid:1)
−

(cid:18)

= p + q + 1 in this case,

m
|

|

1

0
µ4−

1
2 (cid:19)

and therefore, with

2 Trace

Gm(θ∗m)Fm(θ∗m)−

1

−

= 2

m
|

|

+ (µ4 −

3) = 2(p + q) + (µ4 −

1),

and therefore once again the expectation of the ideal penalty leads to the AIC model
selection.

(cid:0)

(cid:1)

 
16

Z,

t

∈

• For GARCH(p, q) processes (see [16]), we have fθ = 0 and Xt = M t

θ ξt since for any

Xt = σt ξt
t = ω0 + a1 X 2
σ2
t
−

(cid:26)

1 +

+ ap X 2
t
−

p + b1 σ2
t
−

1 +

· · ·

· · ·

+ bq σ2
t
−

q

.

Denote θ = θ2 = (ω0, a1, . . . , ap, b1, . . . , bq).
Then we have Ap1 = 0 and therefore Gm(θ∗) =
−
= (µ4 −

Gm(θ∗m)Fm(θ∗m)−

2 Trace

−

1

(µ4−

1)

2 Fm(θ∗). As a result:
m
|

= (µ4 −

1) (p + q + 1).

|

1)

• For APARCH(δ, p, q) processes (see [13]), we also have fθ = 0 and Xt = M t

(cid:0)

(cid:1)

θ ξt since for

any t

∈

Z,




Xt = σt ξt
σδ
t = ω0 + a1 (Xt

−

1 −

Xt

γ1|

−

)δ +

1|

· · ·

+ ap (Xt

p −
−
+b1 σδ
t
−

γp|
1 +

Xt

)δ
p|
−
+ bq σδ
t
· · ·
−

.

q

For such a process, θ = θ2 = (ω0, a1, . . . , ap, γ1, . . . , γp, b1, . . . , bq) when we assume that δ
is known, and, mutatis mutandis, the result is the same than for GARCH processes:



1

(cid:0)

−

1)

2 Trace

Gm(θ∗m)Fm(θ∗m)−

= (µ4 −
3/ Otherwise, the computations are no longer easy. Let us see the example of the family
Z we have Xt = φXt
1 + Zt where
of AR(1)
−
1/2. As a consequence, with θ = (φ, α0, . . . , αp)′, we
Zt = ξt
obtain for any t
(cid:0)
Xt = fθ(Xt

ARCH(p) processes. Then for any t
+ αpZ 2
t
−

α0 + α1Z 2
t
Z,
−

1) (2p + q + 1).

= (µ4 −

1) + Mθ(Xt

1, . . . , Xt

m
|

1) ξt

1 +

· · ·

−

∈

∈

(cid:1)

(cid:1)

p

p

|

−

with

(cid:26)

−
fθ(Xt
−
Mθ(Xt

−

−

1)
1, . . . , Xt

−

= φ Xt

1
−
α0 +

p) =

−

p
i=1 αi(Xt
−

i −

1/2 .

φXt

−

1)2

i

−

Thus the parameter φ is present in fθ as well as in Mθ. From (6.1), and with the notations of
1/, we obtain:

P

(cid:1)

(cid:0)

Fm(θ∗m) =

−

(cid:18)

A1,1 O1,p+1
Op+1,1 Op+1,p+1

B1,p+2

−

(cid:19)

As a consequence,

and Gm(θ∗m) =

A1,1 O1,p+1
Op+1,1 Op+1,p+1

(cid:18)

(cid:19)

+

1)

(µ4 −
2

B1,p+2.

Gm(θ∗m) =

1)

(µ4 −
2

−

Fm(θ∗m) +

3)

(µ4 −
2

(cid:18)

A1,1 O1,p+1
Op+1,1 Op+1,p+1

.

(cid:19)

Thus, with

= p + 2,

m
|

|

Gm(θ∗m) F −

m (θ∗m) =

1

1)

(µ4 −
2

−

I

|

m
|

+

(µ4 −
2

3)

A1,1 O1,p+1
Op+1,1 Op+1,p+1 (cid:19)
m (θ∗m) =

F −

(cid:18)
A1,1 O1,p+1
Op+1,1 Op+1,p+1 (cid:19)
m,
2 c(θ∗) + (µ4 −

m. Then for all m∗ ⊂
=

−

(cid:18)

1

1

(cid:18)

1)

m
|

,
|

Whatever the matrix F −

with c(θ∗m) = c(θ∗)

1

m (θ∗m), we have
R since m∗ ⊂

∈

2 Trace

Gm(θ∗m)Fm(θ∗m)−

−

2 c(θ∗) does not depend on m.

(cid:0)

(cid:1)

where

−

1

F −

m (θ∗m).

c(θ∗m) O1,p+1
Op+1,1 Op+1,p+1 (cid:19)

DATA-DRIVENMODELSELECTIONFORTIMESERIES

17

7. Numerical Studies. This section aims to investigate the numerical behavior of the

model selection criteria studied in Section 4 and Section 5 using R software.
To do that, three Data Generating Processes (DGP) have been considered:

DGP I
AR(2)
DGP II ARMA(1,1) Xt −
DGP III GARCH(1,1) Xt = σt ξt with σ2
where (ξt)t is a Gaussian white noise with variance unity.

Xt = 0.4 Xt
0.5 Xt

1 + 0.4 Xt
1 = ξt + 0.6 ξt

2 + ξt,
1,
−
t = 1 + 0.35X 2
t
−

−

−

−

1 + 0.4σ2
t
−

1,

REMARK 5. As already observed in the Remark 1, Assumption A1 is never satisﬁed for
ARMA processes in case of overﬁtting. However, in the used optimization under constraint
algorithm (program nloptr), we initialized θ at 0 (except for the variance estimator). By
this way, we have noticed in Monte-Carlo experiments that the algorithm always converges
to θ∗ and not other solution due to the overﬁtting.

In order to illustrate the obtained theoretical asymptotic behaviors, we have realized Monte-
Carlo experiments where the performance of the AIC, BIC and KC’ criteria are compared
using the following parameters:

• The considered family of competitive models is the same for the three DGP

=

ARMA(p, q) and GARCH(p, q) processes with 0

p, q

6

.

≤

≤

M

• Several values of n, the observed trajectory length, are considered: 200, 500, 1000, 2000.
• For each n and DGP, we have generated 500 independent replications of the trajectories.

(cid:9)

(cid:8)

Hence, for each replication, the selected models
Then,

mAIC,

mBIC and

mKC ′ are computed.

b

b

1. The consistency property is illustrated by the computation of the frequency (percentage)
of selecting the true model versus a model other than the true one (called here "wrong").
2. For the efﬁciency property (Theorem 4.2, 4.3 and 4.4), we ﬁrst compute a very sharp
γN computed from an independent
estimator
and very large (N = 106) trajectory of the DGP. By this way, and we obtain an estimator
mKC ′. Then, we
ℓ(
compute
e

R of the risk function R for each DGP:

R(θ∗) of ℓ(

θ bm, θ∗) for

mBIC and

e
θ bm, θ∗) =

mAIC,
b

e
m =

θ bm)

R =

R(

−

b

b

e

e

b

b

b
M E := n

ℓ(

b
θ bm, θ∗)
−

b
b
θm∗, θ∗)

ℓ(

(cid:16)

(cid:17)

where
E
n

ℓ is the average of
E
θ bm, θ∗)
ℓ(
e
b

min
m
∈M

−

(cid:3)

b

c
ℓ over the 500 replications. Therefore
ℓ(
e

e
, which appears in (4.14) and (4.16).

θm, θ∗)

d

b

e

M E is an estimator of

(cid:2)

(cid:0)

(cid:2)
The results of Monte-Carlo experiments are reported in Table 1, devoted to the consistency
property, and in Table 2, devoted to the efﬁciency property.

(cid:3)(cid:1)

b

Conclusions of numerical experiments:

• Concerning the consistency properties, the numerical results of Table 1 show that the per-
centages of choice of the true model tend towards 100 for increasing n and with the criteria

18

DGP I

DGP II

DGP III

n

True
Wrong

True
Wrong

True
Wrong

AIC

17.2
82.8

27.8
72.2

00.4
99.6

200
BIC KC’ AIC

500
BIC KC’ AIC

1000
BIC KC’ AIC

2000
BIC KC’

36.2
63.8

80.8
19.2

10.8
89.2

35.6
64.4

92.0
08.0

14.8
85.2

30.4
69.6

30.6
69.7

01.4
98.6

73.2
26.8

88.4
11.6

32.2
67.8

78.2
21.8

96.6
03.4

55.8
44.2

36.4
63.6

31.0
69.0

01.0
99.0

87.4
13.6

89.1
10.9

54.8
45.2

92.2
7.8

97.5
02.5

82.0
18.0

32.4
67.6

33.3
66.7

02.0
98.0

96.2
03.8

95.2
04.8

75.8
24.2

98.4
01.6

99.9
00.1

93.8
06.2

TABLE 1
Percentage of "true" selected models depending on the criterion and sample’s length for DGP I-III.

n

200
BIC

AIC

KC’ AIC

500
BIC KC’ AIC

1000
BIC KC’ AIC

2000
BIC KC’

DGP I

4.91

2.59

5.35

3.46

1.11

1.18

3.08

0.98

0.75

3.05

0.38

0.29

DGP II

3.66

0.87

0.54

3.37

0.42

0.11

2.62

0.15

0.05

2.5

0.10

0.04

DGP III

2.39

4.63

13.16

2.53

4.08

9.54

2.69

2.96

2.52

3.21

2.06

0.76

TABLE 2
dM E of selected models depending on the criterion and the sample’s length for DGP I-III.

BIC and KC’, and this corresponds well to the obtained asymptotic result (Corollary 3).
And as it could also be deduced from the theory (see Corollary 2) the AIC criterion is not a
consistent one. Moreover, we observe that the KC’ criterion outperforms BIC when deal-
ing as well as small and large samples for all considered DGP. These results conﬁrm that
it is important to also consider the neglected terms in the derivation of the BIC criterion.

• From the results of Table 2, we notice a decrease of the residual term

M E to 0 for in-
creasing n for the consistent criteria BIC and KC’. This corresponds well to the o(1/n)
term observed in (4.14). We also observe that this convergence to 0 is globally faster with
the KC’ criterion than with the BIC one. Thus, in terms of efﬁciency as well as in terms
of consistency, the KC’ criterion performs even better than the BIC one for the selected
DGPs. Finally, as shown by Theorems 4.2 and 4.4, the statistic
M E seems asymptotically
bounded and does not converge to 0 when the AIC criterion is applied to select the model.
This conﬁrms that BIC and especially KC’ criteria are more accurate in terms of efﬁciency
than AIC criterion.

d

d

8. Proofs.

8.1. Proofs of Section 3. The asymptotic normality of

1
m was estab-
n
∈
m using a central limit theorem for stationary martingale
(cid:0)

∂θiLn(θ∗m)

(cid:0)

(cid:1)

i

lished in [5] and [4] when m∗ ⊂
difference. Here we extend this result to any m

:

∈ M

PROPOSITION 4. Under Assumption A0-A5, for any θ

Θ, we have

∈

(8.1) √n

1
n

(cid:16)

∂θLn(θ) +

E

1
2

∂θγ(θ, X0)

0 , G(θ)

(cid:2)

with G(θ) :=

1
4

D
→∞N
−→n
Cov

(cid:3)(cid:17)

Z
(cid:16)Xt
∈

(cid:0)

(cid:0)

(cid:1)

∂θiγ(θ, X0) , ∂θj γ(θ, Xt)

.

i,j

d

≤

1
(cid:1)(cid:17)

≤

DATA-DRIVENMODELSELECTIONFORTIMESERIES

19

The main tool we use here for establishing Theorem 4 is the notion of τ -dependence for sta-
tionary time series and more precisely, the τ -dependence coefﬁcients, which are a version of
the coupling coefﬁcients introduced in [11] and used for stationary inﬁnite memory chains.
The reader is deferred to the lecture notes [10] for complements and details on coupling,
based on the Wasserstein distance between probabilities deﬁned as below. Its stationary ver-
sion is:

DEFINITION 1. Let (Ω,

, P) be a probability space,

random variable with values in E. Assume that

C

a σ-subalgebra of

and Z a
M
and deﬁne the coefﬁcient τ (p) as

C

τ (p)(

, Z) =

M

sup
Λ1(E)

∈

f

(cid:13)
(cid:13)
(cid:13)

Z

n(cid:12)
(cid:12)
(cid:12)

−

Z

f (x)PZ(dx)
(cid:12)
(cid:12)
(cid:12)

o(cid:13)
(cid:13)
(cid:13)

.

p

Z and its
Using the deﬁnition of τ , the dependence between the past of the sequence (Zt)t
∈
future k-tuples may be assessed: consider the norm
ykk
xk −
+
k
on Ek, set

p) and deﬁne

x1 −
k

y1k

x
k

· · ·

−

+

=

k

y

Z
k
f (x)PZ

kp <
(dx)

∞

|M

Mp = σ(Zt, t
max
k
l
1
≤
≤

≤
1
l

τ (p)
Z (s) = sup
k>0

n

n
Z is τ (p)
Finally, the time series (Zt)t
∈
0 as s tends to inﬁnity.

sup

τ (p)(

Mp, (Zj1, . . . , Zjl)) with p + s

· · ·
Z -weakly dependent when its coefﬁcients τ (p)

j1 <

≤

oo
Z (s) tend to

< jl

.

LEMMA 2. Under Assumption A0, then for p

αk(fθ, Θ) for any j

∈

N∗,

r and b(p)

k = αk(Mθ, Θ)

ξ0kp +
k

≤

(8.2)

τ (p)
X (s)

≤

C λs with λs = inf
r
≤

≤

1

∞

b(p)
k

s/r

+

∞

b(p)
t

n(cid:16)

Xk=1

(cid:17)

t=r+1
X

o

s

for s

1.

≥

PROOF OF LEMMA 2. This Lemma can be directly deduced from Proposition 3.1 of [14]

where T (x, ξ0) = Mθ(x) ξ0 + fθ(x) for any x

R∞ and therefore

T (x, ξ0)

T (y, ξ0)

−

p ≤ k

inducing

(cid:13)
T (x, ξ0)
(cid:13)

T (y, ξ0)

−

(cid:13)
(cid:13)
p ≤

∈
Mθ(x)

ξ0kp
(cid:12)
∞k=1 b(p)
k .
(cid:12)

Mθ(y)

+

fθ(x)

fθ(y)

−

−

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:4)

(cid:13)
(cid:13)
and therefore τ (p)

REMARK 6. Using Assumption A0 and A5, we deduce that b(p)
s1

δ log s

P

(cid:13)
(cid:13)

λs = O

−

.

X (s)

t = O

≤

δ

t−

with δ > 7/2,

(cid:0)

(cid:1)

Now, under the Assumption A0, since X is a causal time series, deﬁne for any j = 1, . . . , d
and θ

Θ,

(cid:0)

(cid:1)

φ(j)
θ

(Xt

−

k)k

0

≥

(cid:0)
Then we have:

(cid:1)

:= ∂θj γ(θ, Xt) =

2 ∂θj M t
θ

−

(Xt −
M t
θ

θ)2
f t
3 −

2 ∂θj f t
θ

Xt −
M t
θ

f t
θ
2 + 2

∂θj M t
θ
M t
θ

.

(cid:0)

(cid:1)

(cid:0)

(cid:1)

∈

LEMMA 3. Under Assumption A0-A5, for any j = 1, . . . , d, for any θ
φ(j)
θ

Z is a causal stationary sequence that is τ (p)
φ(j)
θ

Θ, the sequence
-weakly dependent where

(Xt

k)k

∈

0

−

≥

t
∈

(cid:0)

(cid:0)

(cid:1)(cid:1)

20

its coefﬁcients τ (1)
φ(j)

θ

(s) satisﬁes:

(8.3)

τ (1)
φ(j)
θ

(s)

C

≤

(cid:16)

s

Xℓ=1

(cid:0)
∞

+

αℓ(fθ, Θ) + αℓ(Mθ, Θ) + αℓ(∂θj Mθ, Θ) + αℓ(∂θj fθ, Θ)

λs+1

−

ℓ

αℓ(fθ, Θ) + αℓ(Mθ, Θ) + αℓ(∂θj Mθ, Θ) + αℓ(∂θj fθ, Θ)

,

(cid:1)

Xℓ=s+1

(cid:0)

0 where (λs) is deﬁned in (8.2).

for any s

≥

(cid:1)(cid:17)

PROOF OF LEMMA 3. In the proof of Proposition 4.1 of [3], it has been proven for
that there exists C > 0

1 and V = (Vi)i

1 such as supi

<

U = (Ui)i
satisfying

≥

≥

(8.4) E

sup
Θ
θ

∈

(cid:2)

φ(j)
θ (U )

−

φ(j)
θ (V )
(cid:13)
(cid:13)

C

≤

(cid:16)

(cid:3)

(cid:13)
(cid:13)

∞

+

(cid:0)

Xi=2
Using coupling techniques, if (
Z satisfying the assumptions with (
(
Xt)t
∈
Then for s
e
τ (1)
φ(j)
θ

0, using (8.4),
≥
φ(j)
θ

e
φ(j)
θ

(Xs

k)k

(s)

0

−

≥

−

≤

e
Xs
(
−

αi(fθ, Θ) + αi(Mθ, Θ) + αi(∂θj fθ, Θ) + αi(∂θj Mθ, Θ)

Ui −
k
(cid:1)
Z is an independent replication of (ξt)t
ξt)t
∈
∈
Xt
(

Z instead of (ξt)t
ξt)t
∈
∈

φ(j)
θ

Z and

Vik4

.

(cid:17)

Z, deﬁne also
Z.
t
∈

k)k

≥

−

0

k)k

0

≥

1

(cid:0)

(cid:0)

e

(cid:1)(cid:1)

Vik4

(cid:13)
(cid:13)

(cid:9)

≥

1

Uik4 ∨
(cid:8)(cid:13)
(cid:13)
U1 −
k

V1k4

∞

(cid:13)
(cid:13)
C

≤

(cid:0)
X1 −
k

∞

+

(cid:16)

(cid:1)

(cid:0)

e

X1k4
e
αi(fθ, Θ) + αi(Mθ, Θ) + αi(∂θj fθ, Θ) + αi(∂θj Mθ, Θ)

(cid:1)(cid:13)
(cid:13)

Xi=2

(cid:0)

∞

C

αℓ(fθ, Θ) + αℓ(Mθ, Θ) + αℓ(∂θj fθ, Θ) + αℓ(∂θj Mθ, Θ)

Xi −
k

(cid:1)

λs+1

−

ℓ,

(cid:1)

Xik4
e

(cid:17)

(cid:4)

≤

Xℓ=1
(cid:0)
that implies (8.3).

REMARK 7. Under Assumption A0 and A5, and therefore with λs = O
(s) = O

δ > 7/2, we also deduce that τ (1)
φ(j)
θ

δ log s

s1

−

.

s1

−

δ log s

with

(cid:0)

(cid:1)

κ] <

(cid:0)
PROOF OF PROPOSITION 4. If Z is a τZ -dependent centered stationary time series satis-
fying E[
∞s=1 s1/(κ
, we deduce from Lemma
Z0|
|
2, point 2. of [9] that condition D(2, θ/2, X) is satisﬁed as θ-weakly dependent coefﬁ-
cients are smaller than τ -weakly dependent coefﬁcients, see (2.2.13) p.16 of [10], and
0 <

from Proposition 2 of [9]. Then,

(cid:1)
2)τZ(s) <

with κ > 2, and

E[Z0Zt]

P

∞

∞

<

−

Z

t
∈

∞

P

(cid:12)
(cid:12)

(cid:12)
(cid:12)

1
√n

n

t=1
X

Zt D
→∞ N
−→n

0 ,

(cid:16)

Z
Xt
∈

.

E[Z0Zt]
(cid:17)

We can apply this central limit theorem to

Zt :=

d

cj

φ(j)
θ

Xj=1

(cid:0)

(cid:0)

(Xt

−

k)k

0

≥

−

E

φ(j)
θ

(Xt

−

k)k

0

≥

with (cj)1

≤

j

d ∈

≤

Rd.

(cid:1)

(cid:2)

(cid:0)

(cid:1)(cid:3)(cid:1)

DATA-DRIVENMODELSELECTIONFORTIMESERIES

21

Indeed, using Lemma 3, we easily obtain for s

0

≥

τZ (s)

C

≤

d

(cid:16)

Xj=1

cj |
|
(cid:17)

∞

Xℓ=1

(cid:0)

αℓ(fθ, Θ) + αℓ(Mθ, Θ) + αℓ(∂θj fθ, Θ) + αℓ(∂θj Mθ, Θ)

λs+1

−

ℓ,

and therefore under Assumption A0 and A5, τZ (s) = O
Lemma 6, we deduce E
Z0|
|
Therefore, we deduce for any θ
P

is satisﬁed since δ > 7/2.
(cid:3)
Θ,

∞s=1 s3/2 τZ(s) <

−
. Then with κ = 8/3,

∞

∞

8/3

<

∈

(cid:0)

(cid:2)

P

s1

δ log s

(cid:1)
. Moreover, using
2)τZ (s) =

−

∞s=1 s1/(κ
(cid:1)

d

√n

cj

Xj=1

(cid:16)

1
n

∂θj Ln(θ) +

E

1
2

(cid:2)

∂θj γ(θ, X0)

(cid:3)(cid:17)

d

d

ci cj

Cov

∂θiγ(θ, X0) , ∂θj γ(θ, Xt)

,

0 ,

1
4

D
→∞ N
−→n

(cid:16)

Xi=1

Xj=1

Z
Xt
∈

(cid:0)

(cid:1)(cid:17)

(cid:4)

which implies the multidimensional central limit theorem (8.1).

∂θiγ(θm, Xt)
N

PROOF OF COROLLARY 1. Firstly, it was already established in [4] that if m∗ ⊂
(Xt
(cid:0)

m
Ft =
, from the deﬁnition of θ∗m as a local minimum of R on Θm, and
(cid:4)

then
σ
k)k
(cid:1)
Secondly, for all m
∈ M
(cid:0)
from Assumption A0-A5, then ∂θj R(θ∗m) = E

Z is a stationary martingale difference process with respect to
t
∈

∂θiγ(θ, X0) , ∂θj γ(θ, Xt)

. As a consequence Cov

∂θj γ(θ∗m, X0)

= 0 for all j

= 0 if t

= 0.

m.

(cid:1)

(cid:0)

(cid:1)

−

∈

∈

(cid:2)
PROOF OF THEOREM 3.1. We use here a standard proof, allowing to show the asymptotic

(cid:3)

normality of the QMLE and already used in [5].
Firstly, it was established in [4] that

θm

a.s.
−→n
+
→

∞

θ∗m.

b

Secondly, a Taylor-Lagrange expansion is applied to

∂θj Ln(

θm)

j

m around θ∗m:
∈

(8.5)

1
√n

(cid:0)

∂θj Ln(

θm)

j

m =
∈

1
√n

∂θj Ln(θ∗m)

(cid:0)

j

m

∈

(cid:1)

b

(cid:1)

b

(cid:0)

+

1
n

(cid:1)

∂2
θiθj Ln(θm)

i,j

m ×
∈

√n

−

c)θ∗m and 0 < c < 1.
θ∗m and the ergodic theorem 1
∂2
θiθj
n
, we obtain:
(cid:0)

∂2
θ2γ(θ, X0)

<

Θ

(cid:0)

∞

(cid:1)

Ln(θm)

i,j

m

∈

(cid:1)

θm)i −
(
(cid:0)
b
a.s.
−→n
+
→

∞

(θ∗m)i

i

m

∈

(cid:1)

Fm(θm) for any

Fm(θ∗m).

1
n

(cid:3)

(cid:13)
∂2
θiθj Ln(θm)
(cid:13)

a.s.
−→n
+
→
(cid:17)
θm) = 0 for any j

i,j

m

∈

∞

(cid:16)
θm, ∂θj

Ln(

b

1
b
√n

∂θj Ln(
b

θm)

j

m

∈

∈

0,

P
−→n
→∞

m. As a consequence,

(cid:1)
using a Markov Inequality and E
b
0 established in (5.11)
Ln(θ)
−
of [5]. Considering (8.5), (8.6) and (8.7), and with the central limit theorem satisﬁed by
(cid:4)
1
provided in Corollary 1, this achieves the proof.
√n

∂θj Ln(θ∗m)

∂θLn(θ)

−→n
→∞

(cid:0)
1
√n

kΘ

∂θ

(cid:13)
(cid:13)

b

m

(cid:3)

(cid:2)

j

(cid:0)

(cid:1)

∈

θm + (1
a.s.
θm
−→n
+
b
→
∞
Θm since E
b

with θm = c
Using
θm ∈
(8.6)

(cid:2)(cid:13)
(cid:13)

Finally, by deﬁnition of

(8.7)

6
22

Now, before establishing Proposition 1, three technical lemmas can be stated:

LEMMA 4. Under Assumptions A0-A5, with 8/3 < r′ ≤
N∗

δ > 7/2 is given in Assumption A5, for any m
n

∈ M

r/3 and r′ < 2(δ

1) where
, there exists C > 0 such as for any

−

∈
(8.8)

PROOF. First, for any m

1
√n

∂θj Ln(θ∗m)

(cid:17)

(cid:16)

(cid:13)
(cid:13)
(cid:13)
∈ M

and n
r′

∈
m

N∗,
r′/2
|

j

m

∈

≤ |

C.

r′ ≤

j

m

∈

(cid:13)
(cid:13)
(cid:13)

∂θj Ln(θ∗m)

1

∂θj Ln(θ∗m)

r′

−

(8.9)

(cid:0)

(cid:13)
(cid:13)

(cid:1)

(cid:13)
(cid:13)

m
|

≤

−

r′/2
|
2r′

∈

m,

∂θj γ(θ∗m, Xt)
-weakly dependent where its coefﬁcients

Now, for all j
ary τ (p)
φ(j)
θ
Moreover, from the proof of Proposition 4, τ (1)
φ(j)
θ
In Proposition 5.5 of [10], since E
lished that:

∂θj γ(θ∗m, X0)

(cid:1)

(cid:0)

(cid:12)
(cid:12)
∂θj γ(θ∗m, Xt)
(cid:12)
(cid:12)
(cid:12)

r′

.

t=1
X

s
δ log s

.

s1
(cid:1)
−

m
Xj
∈

(cid:12)
(cid:12)

1

n

m (cid:12)
Xj
∈
(cid:12)
(cid:12)
τ (1)
φ(j)
θ
(s) = O
(cid:0)
r′

<

(cid:0)
∞

Z is a centered (from the proof of Corollary 1) station-
t
∈
satisﬁes (8.3) (see Lemma 3).

(s)

from Lemma 6, it has been estab-

(cid:1)

n

E

t=1
X

h(cid:12)
(cid:12)
(cid:12)

∂θj γ(θ∗m, Xt)
(cid:12)
(cid:12)
(cid:12)

r′

Cr′

≤

i

(cid:12)
(cid:2)(cid:12)
(cid:12)
(cid:12)
Mr′,n + M r′/2

2,n

(cid:3)

(cid:0)

(cid:1)

where Mm,n := 2n

n

−

1
(i + 1)m
−

Xi=0

2 τ (1)
φ(j)

θ

(i).

C ′′ n. As a consequence, there exists C > 0 such as for

(cid:0)

(cid:1)

Using 8/3 < r′ < 2(δ

−

1) with δ > 7/2, we obtain that

Mr′,n ≤

C n

n

ir′

−

1

−

δ log(i)

C ′ n1+r′

−

δ log(n) = O

nr′/2

≤

C n

n
i=1 i1
−

P

and M2,n ≤
N∗,
any n
∈

(8.10)

Xi=1
δ log(i)

≤

n

E

i
Then, using (8.9) and (8.10), the proof is established.

h(cid:12)
(cid:12)
(cid:12)

t=1
X

∂θj γ(θ∗m, Xt)
(cid:12)
(cid:12)
(cid:12)

r′

C nr′/2.

≤

LEMMA 5. Under Assumptions A0-A5, then for any m

for any n

∈

N∗,

(cid:4)

, there exists C > 0 such as

∈ M

C.

1
√n

(cid:16)

(cid:13)
(cid:13)
(cid:13)

∂θj Ln(

θm)

j

m

∈

r/3 ≤

(cid:13)
(cid:17)
(cid:13)
(cid:13)
θm, we have ∂θj

b

PROOF. First, from the deﬁnition of

Ln(

θm) = 0 for any j

m. Then,

∈

1
√n

(cid:16)

(cid:13)
(cid:13)
(cid:13)

∂θj Ln(

θm)

(cid:17)

b

1
b
√n

(cid:0)

=

(cid:16)

(cid:13)
(cid:13)
(cid:13)

j

m

∈

r/3

(cid:13)
(cid:13)
(cid:13)

∂θj Ln(

θm)

b
−

∂θj
Ln(
b

θm)

b

b

b

(cid:1)(cid:17)

j

m

∈

r/3

(cid:13)
(cid:13)
(cid:13)

DATA-DRIVENMODELSELECTIONFORTIMESERIES

23

m
|

≤

6)/2r

(r
|

−
√n

1/2
m
|
|
2 √n

≤

∂θj Ln(

θm)

∂θj

Ln(

θm)

−

m (cid:13)
Xj
∈
(cid:13)
n
(cid:13)
∂θj γ(

b
θm, Xt)

b
γ(
∂θj

−

r/3

(cid:13)
(cid:13)
b
(cid:13)
θm, Xt)

b

b

(cid:13)
(cid:13)
(cid:13)

.

r/3

m
Xj
∈

t=1 (cid:13)
X
(cid:13)
(cid:13)

b
From the proof of Lemma 2 in [4], there exists C > 0 such as

∂θj

γ(θ, Xt)

E

sup
Θ
θ

∈

h

∂θj γ(θ, Xt)

(cid:13)
(cid:13)
(cid:13)

≤

−

C

Therefore,

b

t
(cid:16)Xk
≥

r/3

i

(cid:13)
(cid:13)
(cid:13)

1
√n

(cid:16)

(cid:13)
(cid:13)
(cid:13)

∂θj Ln(

θm)

(cid:17)

b

j

m

∈

(cid:13)
(cid:13)
(cid:13)

C |

3/2
m
|
2 √n

r/3 ≤

n

t=1
X

t
Xk
≥

(cid:0)

αk(fθ, Θ) + αk(Mθ, Θ) + αk(∂fθ, Θ) + αk(∂Mθ, Θ)

r/3

.

(cid:17)

αk(fθ, Θ) + αk(Mθ, Θ)

+αk(∂fθ, Θ) + αk(∂Mθ, Θ)
n

(cid:1)

C ′
√n

≤

n

t=1
X

t
Xj
≥

δ

j−

C ′′
√n

≤

δ

t1
−

C ′′′,

≤

t=1
X

with C ′ > 0, C ′′ > 0 and C ′′′ > 0 and where the last inequality holds since δ > 7/2 under
(cid:4)
Assumption A5.

LEMMA 6. Under Assumptions A0-A5, for any m

∈ M
.
<

∞

m

∈

r/3

(8.11)

∂θj γ(θ, X0)

j

PROOF. For j

(cid:13)
(cid:13)
(cid:13)
m, we have for any θ
3(X0 −
2(X0 −
Therefore, with Assumption A3 and Minkowski Inequality,

(cid:13)
(cid:1)
(cid:13)
(cid:13)
Θm,
2 (M 0

∈
θ ) ∂θj f 0
f 0
θ −

∂θj γ(θ, X0) =

∈
2 (M 0

θ )−

θ )−

−

(cid:0)

and any θ

Θm,

∈

θ )2∂θj M 0
f 0

θ +2 (M 0

θ )−

1∂θj M 0
θ .

∂θj γ(θ, X0)

j

m

∈

r/3 ≤

2
h3/2

(cid:0)

(cid:13)
(cid:13)

(cid:1)

(cid:13)
(cid:13)

h1/2

∂θj f 0
θ

,

(cid:16)

(cid:0)

(cid:13)
(cid:13)

(cid:1)
∂θj M 0
θ

+

f 0
θ

r/3

X0 −
(cid:0)

X0 −
(cid:1) (cid:12)
(cid:12)

(cid:1)(cid:13)
(cid:13)
2
f 0
θ

r/3 + h

∂θp M 0
θ

.

r/3

Now, applying the Hölder Inequality, we obtain that there exists C > 0 such that for any
θ

Θm,

(cid:12)
(cid:12)

(cid:13)
(cid:13)

(cid:0)

(cid:13)
(cid:13)

(cid:1)(cid:13)
(cid:13)

(cid:13)
(cid:0)
(cid:13)

∈

(8.12) E

∂θp γ(θ∗, X0)

r/3

h(cid:12)
(cid:12)

i

(cid:12)
(cid:12)

∂θj f 0
θ

C

≤

(cid:16)(cid:13)
(cid:13)

2r/3

(cid:13)
(cid:13)

+

X0 −
(cid:13)
∂θj M 0
(cid:13)
θ

f 0
θ

2r/3

(cid:13)
X0 −
(cid:13)

r

f 0
θ

2
r +

∂θpM 0
θ

.

r/3

Using Assumption A0 and A5 and the proof of Lemma 1 in [5], all the right side terms in
(cid:4)
(8.12) are ﬁnite for any θ

(cid:13)
(cid:13)
Θm and this achieves the proof.

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

∈

(cid:17)

(cid:17)

Then Proposition 1 can be established:

24

PROOF OF PROPOSITION 1. From (8.5) and (8.6) with F (θ∗m) the positive deﬁnite matrix

deﬁned in (3.4), we know that for n large enough,

(8.13)

√n

(cid:13)
(cid:13)

(cid:0)

i

m

r′

(θ∗m)i
θm)i −
(
1
=
b
n

∈
∂2
θiθj Ln(θm)

(cid:13)
(cid:13)

(cid:1)

Therefore, using Hölder and Minkowski inequalities, we obtain:
b

(cid:0)

1

−

1
√n

×

∂θj Ln(

θm)

∂θj Ln(θ∗m)

−

j

m

∈

i,j

m

∈

(cid:17)

(cid:1)

.

r′

(cid:13)
(cid:13)
(cid:13)

(cid:1)

(cid:16)(cid:0)

(cid:13)
(cid:13)
(cid:13)
(θ∗m)i

√n

θm)i −
(
b

(cid:0)

(cid:13)
(cid:13)

i

m

∈

(cid:1)

r′ ≤
(cid:13)
(cid:13)

(cid:16)(cid:0)

(cid:13)
(cid:13)
(cid:13)

1
n

1
n

∂2
θiθj Ln(θm)

i,j

(cid:1)

1
√n

×

(cid:13)
(cid:13)
(cid:13)

∂2
θiθj Ln(θm)

(cid:1)
θm)

∂θj Ln(

1
√n

1

−

rr′
r−3r′

m

∈

(cid:17)
∂θj Ln(

(cid:13)
(cid:13)
(cid:13)
θm)

1
b

−

(cid:0)

i,j

m

∈

(cid:17)

rr′
r−3r′

(cid:13)
(cid:13)
(cid:13)
+

j

m

∈

r
3

(cid:13)
(cid:13)
(cid:13)

(cid:1)

b

≤

(cid:16)(cid:0)

(cid:13)
(cid:13)
(cid:13)
×

(cid:16)(cid:13)
(cid:13)
(cid:13)

∂θj Ln(θ∗m)

−

j

m

∈

(cid:1)

r
3

(cid:13)
(cid:13)
(cid:13)

1
√n

(cid:13)
(cid:13)
(cid:13)

∂θj Ln(θ∗m)

(cid:0)

(cid:1)

j

m

∈

r

.
3 (cid:17)
(cid:4)

(cid:13)
(cid:13)
(cid:13)

Now using Assumption A4, Lemmas 4 and 5, we deduce (3.9).

(cid:0)

8.2. Proofs of Section 4.

PROOF OF LEMMA 1. 1. From the assumptions, the function R : θ
2(Θ) function and the Hessian matrix ∂θ2R =
C
Therefore, from a Taylor-Lagrange expansion:

R(θ) is a
2 F is a deﬁnite positive matrix (see (3.4)).

7→

Θ

−

∈

n

R(

θm)

−

(cid:0)

b

R(θ∗m)

= n

R(θ∗m) +

(cid:1)

(cid:16)

θm −
b

(cid:0)

(8.14)

=

√n(

with θ = θ∗m + c
θm −
mapping Theorem, we deduce that:
(cid:0)
b
∂θ2R(θ) =

(cid:1)
2 F (θ) P

(8.15)

∈

θ∗m)

θm −
(cid:1)
Θm since c
b

∈

1
2
(cid:0)
θ∗m

−

Moreover, using the asymptotic normality of

θm)i −
b

(8.16)

√n((

(θ∗m)i)i

m D

∈

As a consequence, with Zn =
and from (8.14), we have

Gm(θ)

(cid:0)
Z ⊤n

(cid:1)
Gm(θ)

n

R(

θm)

R(θ∗m)

=

−

−

(cid:0)

b

(cid:1)

=

Z ⊤n

(cid:0)

(cid:1)
Gm(θ)

Deﬁne U ∗(m) :=
Then using (8.15) we obtain

Z ⊤

−

(cid:0)

−
(cid:0)
Gm(θ∗m)

θ∗m

⊤ ∂θR(θ∗m)

+

(cid:1)
1
2

θm −
b

(cid:0)
⊤ ∂θ2R(θ)

(cid:1)
√n(

θ∗m

⊤ ∂θ2R(θ)

θm −
(cid:0)
b
,

θ∗m

−

(cid:1)

R(θ∗m)

(cid:17)

θ∗m)

θm −
b

(cid:1)
[0, 1]. Using Lemma 4 of [5] and continuous

(cid:0)

→∞ −
−→n

2 F (θ∗m)

and G(θ) P

−→n
→∞

G(θ∗m).

θm established in [5] and [4], we have:
1
1Gm(θ∗m)
(cid:0)
(θ∗m)i)i

Fm(θ∗m)

Fm(θ∗m)

→∞ N
−→n
(cid:16)
1/2
Fm(θ)√n((
−

m D

0 ,
b

(cid:0)

−

−

.

(cid:1)
→∞ N
−→n

(cid:17)
(0 , I

)

m
|

|

∈

1/2

Fm(θ)

1

−

1/2

(cid:0)

(cid:1)
Fm(θ)

−

1

−

Fm(θ)
1/2 Zn.
(cid:1)

1

Gm(θ)

1/2

Zn

(cid:0)

(cid:1)

1/2

(cid:1)
(cid:0)
Fm(θ∗m)

1

−

(cid:1)
(cid:0)
Gm(θ∗m)

1/2 Z where Z D
(cid:1)

∼ N

(0 , I

).

m
|

|

(cid:1)
θm)i −
b
Fm(θ)

(cid:0)
Gm(θ)

(cid:1)

(cid:0)

n

R(

θm)

−

(cid:0)

b

(cid:1)
R(θ∗m)

(cid:1)

(cid:1)

U ∗(m).

(cid:0)

D
−→n
→∞

DATA-DRIVENMODELSELECTIONFORTIMESERIES

25

(cid:16)(cid:0)
1 Gm(θ∗m)
(cid:17)

R(θ∗m)

−

Fm(θ∗m)

1

−

Gm(θ∗m)

1/2

(cid:1)

(cid:0)

(cid:1)

(cid:0)

.

(cid:17)

(cid:1)

, we have to prove that there

−→n
→∞

E

U ∗m

(cid:2)

(cid:3)

(cid:1)(cid:3)

The computation of the expectation of U ∗m follows from
1/2

E

U ∗m

= E

Trace

U ∗m

=

Trace

−

Gm(θ∗m)

(cid:2)

(cid:3)

=

(cid:2)
Trace
−

(cid:0)

(cid:1)(cid:3)
Fm(θ∗m)

−

(cid:16)(cid:0)
Finally, for establishing E
exists n0 ∈
(8.17)

N such as

n

R(

(cid:1)
θm)

(cid:2)

b
E

(cid:0)
sup
n0
n

≥

Indeed, from (8.14), we have:

(8.18) n

R(

θm)

R(θ∗m)

−

1
2

≤

(cid:12)
(cid:12)

b

since there exists λmax <
where Θ is a compact set.
Using Proposition 1, we know that

∞

Finally using (8.18), we deduce (8.17).

n

R(

θm)

R(θ∗m)

−

<

.

∞

(cid:2)

(cid:12)
(cid:12)

b

(cid:12)
(cid:3)
(cid:12)

∂2
θ2R(θ)

sup
Θ
θ

∈
E

(cid:12)
(cid:12)
=

⇒
such as

(cid:13)
(cid:13)
n

R(

θm)

(cid:13)
(cid:13)
−

(cid:2)

(cid:12)
∂θ2R(θ)
b
(cid:12)

≤

2

θ∗m)

√n(

θm −
(cid:13)
b
(cid:13)
R(θ∗m)

(cid:13)
λmax
(cid:13)
2
(cid:3)
λmax for any θ

≤

(cid:12)
(cid:12)

∈

(cid:13)
(cid:13)

√n

(cid:13)
(cid:13)
θm −
b

(cid:0)

θ∗m

2 <

.

∞

(cid:1)(cid:13)
(cid:13)

sup
N∗
n

∈

(cid:13)
(cid:13)

E

2

√n(

θ∗m)

θm −
(cid:2)(cid:13)
Θ from Assumption A5
b
(cid:13)

(cid:13)
(cid:13)

(cid:3)

,

2. As in the proof of 1., we use a Taylor-Lagrange expansion of
∂θ

θm) = 0. Then,

γn(

γn(θ∗m) around

θm since

b

b

n

γn(θ∗m)

−

γn(

θm)

=

1
2

√n(

θ∗m)⊤

∂2
θ2

γn(θm)

(cid:1)

θ∗m and E
b
b

Ln(θ)

Ln(θ)

θm −
1
b
n
−

b
√n

θm −
b
0, we have

(cid:0)

(cid:1)

b

θ∗m

.

(cid:1)

But using

(cid:0)
a.s.
θm
b
−→n
+
→

∞

b

(cid:0)
Therefore, using the same reasoning as in 1., we deduce that

(cid:1)

(cid:0)

Θ

b
−→n
→∞
(cid:13)
i
(cid:13)
2 F (θ∗m).
(cid:13)

γn(θm)

b
P
→∞ −
−→n

1
n

h(cid:13)
(cid:13)
∂2
(cid:13)
θ2

b
γn(θ∗m)

n

γn(

θm)

−

U ∗(m).

D
−→n
→∞

With Hölder Inequality and using 8/3 < r′ deﬁned in Proposition 1, we obtain for n large
b
enough

b

(cid:0)
b

(cid:1)

E

n

γn(θ∗m)

γn(

θm)

−

≤

∂2
θ2

γn(θm)

∂2
θ2

(cid:1)
γn(θm)
b

1/2 √n(

θ∗m)

2
2

θm −
√n(
b

θm −
b
N∗ E
∈

(cid:2)

(cid:0)

b

(cid:1)(cid:3)

(8.19)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
4 since r′ > 8/3, and therefore

(cid:13)
(cid:13)
Finally, with Proposition 1 and Lemma 4 of [5], we have supn
and r′
r′
−

(cid:1)(cid:13)
(cid:13)
(cid:13)

r′
r′ −2

2 ≤

≤

b

b

b

(cid:0)

(cid:0)

which concludes the proof.

sup
N∗
n

∈

E

n

γn(θ∗m)

γn(

θm)

−

<

,

∞

(cid:2)

(cid:0)

b

(cid:1)(cid:3)

b

b

(cid:13)
θ∗m)
(cid:13)

2
r′.

∂2
θ2

(cid:13)
(cid:13)

γn(θm)

4
Θ

<

∞

(cid:2)(cid:13)
(cid:13)

b

(cid:3)

(cid:13)
(cid:13)

(cid:4)

26

PROOF OF PROPOSITION 2. For m∗ ⊂
R(θ∗) and using (4.2), we have
θm)

R(

−

b

E

ℓ(

θm, θ∗)

=

(cid:2)
But the matrix G(θ∗m) and
b
m and m
if m∗ ⊂

(cid:3)
−
= m∗, then

m we have θ∗m = θ∗ and therefore ℓ(

θm, θ∗) =

E[I1(m)]

1
n
F (θ∗m) are positive deﬁnite function from Assumption A2. Thus

1 Gm(θ∗m)
(cid:17)

→∞ −
∼n

Fm(θ∗m)

Trace

1
n

(cid:16)(cid:0)

(cid:1)

−

.

b

Trace

Fm∗(θ∗)

1

−

Gm∗ (θ∗)

<

Trace

Fm(θ∗)

1

−

Gm(θ∗)

−

(cid:17)

(cid:1)
F (θ∗)

1

−

G(θ∗) are positive. This implies E

(cid:1)

(cid:16)(cid:0)

−

(cid:16)(cid:0)

θm, θ∗)

since all the eigenvalues of
E
ℓ(
If m∗ 6⊂
(cid:2)
b

m, then:
(cid:3)

for n large enough.

−

(cid:0)

ℓ(

θm, θ∗) =

Using (4.2), we also have

b

E

ℓ(

θm, θ∗m)

=

(cid:0)

b
E[I1(m)]

1
n

(cid:2)

(cid:3)

But as it was established in [4] that
Therefore, E
ℓ(

b
θm∗, θ∗)
ℓ(

= o

E

(cid:1)

R(

θm)

−

R(θ∗m)

+

R(θ∗m)

R(θ∗)

.

−

(cid:1)

(cid:0)

(cid:1)

1
n

→∞ −
∼n

Trace

Fm(θ∗m)

−

R(θ∗m)
θm, θ∗)
(cid:0)

(cid:16)(cid:0)
(cid:1)
= 2 DKL(θ∗k
R(θ∗)
−
for any m such as m∗ 6⊂

(cid:1)

1 Gm(θ∗m)
(cid:17)

.

θ∗m) > 0 since m
m.

m∗.

6⊂

(cid:4)

(cid:17)
ℓ(

θm∗, θ∗)

<

(cid:2)

b

(cid:3)

(cid:2)

b

(cid:3)

(cid:0)

(cid:2)

b

(cid:3)(cid:1)

PROOF OF PROPOSITION 3. The proof of this proposition can be deduced from

(8.20)

for any m

∈ M

E

n I3(m)

= E

n

R(θ∗m)

γn(θ∗m)

= v∗n

−
. For establishing (8.20), we begin by
(cid:3)

(cid:0)

(cid:2)

(cid:2)

γn(θ∗m)

+

γn(θ∗m)

−

= R(θ∗m) and (Xt)t
∈

(cid:1)

(cid:0)

b

−

b

(cid:1)(cid:3)

γn(θ∗m)

:= I31(m) + I32(m).

Z is a stationary times series, then for any

(cid:1)

(8.21)

I3(m) =

R(θ∗m)

Firstly, since E
n

N∗,

∈

(cid:0)
γ(θ∗m, X0)

(cid:2)

(cid:3)

(8.22)

E

γn(θ∗m)

=

1
n

n

Secondly, from Assumption A0 and [5], there exists C > 0 such that for any t

1

(cid:2)

(cid:3)

t=1
X

(cid:2)

(cid:3)

E

γ(θ∗m, Xt)

= R(θ∗m) =

⇒

E

I31(m)

= 0.

E

γ(θ, Xt)

γ(θ, Xt)

−

C

Θ

≤

αs(fθ, Θ) + αs(Mθ, Θ)

(cid:2)(cid:13)
(cid:13)

(cid:3)

(cid:13)
(cid:13)

t
Xs
≥

(cid:0)

b
Therefore, there exist C > 0 and C ′ > 0 such that for any m

E

γn(θ∗m)

γn(θ∗m)

−

Θ

≤

(cid:2)(cid:13)
(cid:13)

(cid:3)

(cid:13)
(cid:13)

b

≤

C
n

C
n

(8.23)

n

t=1
X
n

t
Xs
≥

(cid:0)

δ

t1
−

≤

C ′
n

,

(cid:1)

,

∈ M

αs(fθ, Θ) + αs(Mθ, Θ)

(cid:1)

since δ > 7/2 from Assumption A5. Moreover, for any m
setting), we have γn(θ∗m)

γn(θ∗m) = γn(θ∗m∗)

m (overﬁtting
such as m∗ ⊂
γn(θ∗m∗). Using this and (8.23) we deduce

∈ M

−

t=1
X

−

(cid:2)

(cid:3)

≥
.

b

b

6
DATA-DRIVENMODELSELECTIONFORTIMESERIES

27

m satisfying

∈ M

that for any m
m∗ ⊂
(8.24)

, there exists a bounded sequence (v∗n)n

N∗ not depending on m when
∈

E

I32(m)

=

v∗n
n

.

Using also Lemma 1, this implies the asymptotic behavior of E

(cid:3)

(cid:2)

penid(m)

.

(cid:4)

Now we establish a preliminary lemma that is an important step towards the proof of Theorem
4.2.

(cid:2)

(cid:3)

LEMMA 7. Let pen : m

pen(m)

∈ Mn 7→
+

ℓ(

θm, θ∗)

pen(

mid)

(8.25)
ℓ(

θ bmpen , θ∗)

≤

min
m
∈M

(cid:0)
(cid:8)
b
PROOF. By deﬁnition, for any m

(cid:9)

b

∈

−

R+. Then,

pen(

mpen)

penid(

mid)

−

−

penid(

mpen)

.

(cid:1)

(cid:0)

b

b

(cid:1)

b

,
b
∈ M
Cpenid(m) = R(

θm) = ℓ(

θm, θ∗) + R(θ∗).

(8.26)

As a consequence,

(8.27)

For any m

∈ M

b
θm, θ∗)

ℓ(

min
m
∈M
, we also have

(cid:8)

b

(cid:9)

b

b

= ℓ(

θ bmid, θ∗) = min
∈M

m

b

Cpenid (m)

R(θ∗).

−

(cid:8)

b

(cid:9)

Cpen(m) =

Cpenid (m) + pen(m)

penid(m).

−

Cpen(

mid). Therefore,

By deﬁnition of

mpen, we have
b

b
mpen) + pen(

mpen)

Cpenid(

≤
mpen)

mpen)
Cpen(
b
Cpen(
b
penid(
b

b

−

mpen)
b

Cpenid(
b
Cpenid(
b

b
≤

≤

mid) + pen(

mid)

mid) + pen(
b

mid)
b

penid(

mid)

penid(

mid).
b

−

−

By replacing
b
b
is established.

Cpenid(m) by ℓ(

b

θm, θ∗) + R(θ∗) following (8.26) and using (8.27), then (8.25)
(cid:4)

b

b

b

b

b

b

b

PROOF OF THEOREM 4.1. Let

M∗ =

m

, m∗ ⊂

∈ M

m

and

M′ =

M \M∗. Let m

∈

M′. We have:
P
mpen = m

(cid:0)

b

(cid:1)

≤

≤

≤

P

Cpen(m)

Cpen(m∗)

≤

(cid:8)

(cid:9)

P

(cid:0)

γn(
b

θm)

γn(
b

θm∗)

−

(cid:1)
≤

pen(m∗)

−

pen(m)

n

P

n
b

n

P

≤
n
pen(m)

γn(
b

θm)
b
−

γn(θ∗m)
b

+ n

γn(θ∗m)

o
R(θ∗m)

−

+ n

R(θ∗)

γn(θ∗)

−

(cid:0)
+n
b

γn(θ∗)
b

(cid:1)
θm∗ )

γn(
b
−

(cid:0)
n
b

≤

R(θ∗)

−

(cid:1)
R(θ∗m)

(cid:0)
+ n

pen(m∗)
b
−

(cid:1)
pen(m)

(cid:1)o

(cid:0)

b

Z1 + Z2 + Z3 + Z4 + Z5

b

(cid:1)

b

(cid:0)
≤ −
and with R(θ∗)
c)
+ Z5 ≤

2 n DKL(θ∗

(cid:1)
θ∗m)
k
R(θ∗m) =
P(Z1 ≤

−
c/5) +

(cid:0)

o
2 DKL(θ∗k
· · ·

+ P(Z5 ≤

θ∗m) < 0 since
c/5) for

with Z5 = n
−
m
≤
any random variables Zi and real number c, we obtain:

m∗ from [4]. Now, using P(Z1 +

pen(m∗)

· · ·

6⊂

−

(cid:0)

(cid:1)

(8.28)

P

mpen = m

(cid:0)

b

5

P

Xi=1

(cid:0)

≤

(cid:1)

Zi ≤

cn

,

(cid:1)

28

2

where cn =
Let Z1 := n
r′ ≤

θm)
r/3 and r′ < 2(δ
(cid:0)
b

5 n DKL(θ∗k
−
γn(
−
−
b
Z1

E

b

3r
8

θ∗m).

(cid:1)

′

(cid:3)

≤

≤

(cid:2)(cid:12)
(cid:12)

(cid:12)
(cid:12)

Therefore, using Proposition 1,

(8.29) P

Z1 ≤

cn

≤

(cid:0)

(cid:1)

′

3r
8

P

Z1|
|
(cid:16)

. Following the same computations than in (8.19), with 8/3 <

γn(θ∗m)
1) deﬁned in Proposition 1, and Hölder Inequality,

∂2
θ2

γn(θm)

(cid:1)
b
γn(θm)

∂2
θ2

(cid:1)(cid:13)
(cid:13)
(cid:13)

≤

(cid:17)

b

′

3r
8

cn|
|
(cid:1)
(cid:0)

(cid:0)

(cid:13)
(cid:13)

(cid:0)

(cid:13)
(cid:13)
(cid:13)

≥

θ∗m)

′

3r
4
3r′
4

1/2 √n(

θm −
b
√n(

3r′
2

(cid:13)
(cid:13)

(cid:13)
(cid:13)
θ∗m)

′

3r
4
r′

.

(cid:13)
(cid:13)

θm −
b

E

Z1

3r
8

(cid:12)
(cid:12)
P

(cid:2)(cid:12)
(cid:12)
⇒

=

′

3r′
8

1
cn|
|
(cid:3)
cn
Z1 ≤

= O

1
3r′
8

n

= o

1
n

,

(cid:1)
(cid:0)
γn(θ∗)

−

(cid:0)

b

(cid:17)
since 3r′/8 > 1. The same kind of computations can also be done for Z4 := n
γn(

and we also obtain P

θm∗ )

= o

cn

(cid:16)

(cid:1)

(cid:0)

.

1
n

(cid:1)

Consider now Z2 := n
b

b

Z4 ≤
(cid:0)
γn(θ∗m)
−

(cid:1)
R(θ∗m)

(cid:1)
(cid:0)
. Then,

E

Z2

8/3

≤

25/3

(cid:0)
E

Ln(θ)
b

−

Ln(θ)

(cid:1)
8/3
Θ

+ n8/3 E

n

γ(θ∗m, Xk)

R(θ∗m)

−

(cid:16)

(cid:3)

(cid:12)
(cid:12)

(cid:2)(cid:12)
(cid:12)

(cid:2)(cid:13)
(cid:13)b
from Assumption A5 and
Using [5], we know that supn
−
Z is a stationary
since δ > 7/2 > 2. Now, consider Yk := γ(θ∗m, Xk)
time series, τY -weakly dependent because, using the same type of arguments as in the proof
of Lemma 3, we have:

(cid:2)(cid:12)
(cid:12)
8/3
Θ
∞
R(θ∗m). Then, (Yk)k
(cid:13)
(cid:13)

(cid:13)
(cid:3)
(cid:13)
Ln(θ)

Ln(θ)

(cid:2)(cid:13)
(cid:13)b

N∗ E

(cid:1)(cid:12)
(cid:12)

−

(cid:3)

∈

∈

Xk=1
(cid:0)
<

8/3

.

(cid:3)(cid:17)

τY (s)

≤

αℓ(fθ, Θ) + αℓ(Mθ, Θ)

λs+1

−

ℓ,

(cid:1)

∞

Xℓ=1

(cid:0)

with λ deﬁned in Lemma 2. Therefore, using Assumption A5, we also have τY (s) =
sδ
, with δ > 7/2. Now, using the same type of arguments as in the proof of
O
−
Lemma 4,

1 log(s)

(cid:0)

(cid:1)

n

E

8/3

Yk

(cid:2)(cid:12)
(cid:12)
and M2,n ≤
ists C > 0 such that for any n

Xk=1
C n while M8/3,n ≤
∈

(cid:12)
(cid:12)
C n
N∗,

E

C8/3

M8/3,n + M 4/3

2,n

,

≤

(cid:3)

(cid:0)
n
1
i=1 i8/3
−

−

δ log(i) = o

(cid:1)
n4/3

. Therefore, there ex-

8/3

Yk

C n4/3.

≤

(cid:3)

(cid:0)

(cid:1)

N∗,

∈

Finally, we deduce that there exists C > 0 such that for any n

(8.30)

Z2
This result and Markov Inequality imply,

C n4/3.

≤

(cid:3)

(8.31) P

Z2 ≤

cn

P

≤

(cid:0)

(cid:1)

8/3

Z2|
|
(cid:16)

≥

E

Z2

8/3

≤

(cid:2)(cid:12)
(cid:12)
cn
Z2 ≤

(cid:12)
(cid:3)
(cid:12)
= O

(cid:1)

1
8/3
cn|
|
n4/3
(cid:16)

1
8/3
cn|
|

= O

(cid:17)

(cid:16)

1
n4/3

,

(cid:17)

(cid:17)
P

(cid:0)

P
n

Xk=1

(cid:2)(cid:12)
(cid:12)

E

(cid:2)(cid:12)
(cid:12)
cn|
|
(cid:1)

(cid:0)
=

⇒

(cid:12)
(cid:12)
8/3

(cid:12)
(cid:12)
8/3

DATA-DRIVENMODELSELECTIONFORTIMESERIES

29

We obtain the same bound for Z3 := n
Finally using the assumption (4.7), we have:

R(θ∗)

γn(θ∗)

.

−

(8.32) n P

Z5 ≤
(cid:0)

cn

= n P

pen(m)

(cid:1)

(cid:16)(cid:0)

−

By this way, (4.8) is established.

(cid:0)

b
pen(m∗)

(cid:1)
2
5

≤ −

DKL(θ∗

θ∗m)
k

(cid:17)

(cid:1)
n P

pen(m∗)

(cid:16)

2
5

≥

≤

DKL(θ∗

θ∗m)
k

(cid:17)

0.

−→n
→∞

(cid:4)

PROOF OF THEOREM 4.2. The proof is mainly based on Lemma 7. From the proof of
, there exists two positive random variables Y (m)
Lemma 1, we deduce that for any m
N∗. More-
and Z(m) such as n
Y (m) and n
I1(m) + I2(m)
over, Y (m) and Z(m) have bounded expectations. Therefore, using Markov Inequality, since
is supposed to be a ﬁnite family of models, for any ε > 0 there exists K ′ε > 0 such as

Z(m) for any n

∈ M
≤

I3(m)

≤

∈

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

M

lim sup

n

→∞

max
m
∈M

(cid:16)

P

n penid(m)

K ′ε

≥

ε.

≤

(cid:17)

Therefore, using this inequality and (4.9), we deduce that for any ε > 0 there exist Mε > 0
N∗ such that for any n
and Nε ∈
P
(8.33)

≥
mpen)

mpen)

pen(

pen(

Nε,

ε.

mid)

mid)

Mε

n

1

≤
The proof of (4.10) is now completed from (8.25) of Lemma 7 and (8.33).

−

−

−

(cid:16)

penid(

penid(

≥

−

(cid:17)

(cid:1)

(cid:0)

(cid:12)
(cid:0)
(cid:12)

b

b

b

(cid:1)(cid:12)
(cid:12)

b

PROOF OF THEOREM 4.3. Using the same tricks than in Lemma 7, we obtain:

(cid:4)

.

(8.34)

θ bmpen , θ∗)

θm∗, θ∗) +

ℓ(
≤
Let
, m∗ ⊂
b
the beginning of the proof of Theorem 4.1, we have:
P

ℓ(
M∗ =
b
(cid:8)
mpen = m

pen(m∗)
M′ =

(cid:9)
Cpen(m∗)

Cpen(m)

mpen)
−
M \ M∗. Now, for m
b

∈ M

pen(

and
(cid:0)

m

m

−

P

(cid:1)

(cid:0)

penid(m∗)

penid(

mpen)

−
∈ M∗ and m

= m∗, as in

(cid:1)

b

≤
θm)

≤

(cid:0)

b

(cid:1)

≤

≤

P

(cid:0)

n
b

γn(

γn(θ∗)

(cid:1)

+ n

γn(θ∗)

b
−

γn(

θm∗)

−

≤

n

pen(m∗)

−

pen(m)

n

P

n

(cid:0)
γn(θ∗)
b
b

n

(cid:1)
θm)

γn(
b

−

≥

(cid:0)

b

(cid:1)

b

b

(cid:0)

E[fn(m)]
b

+ P
b

n
b

(cid:1)
γn(

(cid:0)
θm∗)

o

+P

(cid:0)
fn(m)
b
b

n
3
−
n
pen(m∗) > 0 since m∗ ⊂
(cid:0)

γn(θ∗)

−
E[fn(m)]
b

(cid:1)o
E[fn(m)]

E[fn(m)]

≥

(cid:1)
≥
= m∗.

o

o

with fn(m) = n
−
Using exactly the same arguments as in the proof of Theorem 4.1, there exists C1 > 0 such
that for n large enough,

3 en(m) and en(m) = pen(m)

m and m
(cid:1)

(8.35) P

n

γn(θ∗)

n

(cid:0)
b

γn(

θm)

−

≥

(cid:1)

b

b

E[fn(m)]

+ P

n

γn(

θm∗)

γn(θ∗)

−

≥

E[fn(m)]

o

n

(cid:0)
b

b

b

(cid:1)
≤

C1
E[fn(m)]

o
3r′
8

where r′ > 8

3 . Moreover, from Markov Inequality we have

(8.36) P

3

fn(m)

n

(cid:0)

E[fn(m)]

−

≥

(cid:1)

E[fn(m)]

o

fn(m)

P

≤

n(cid:12)
(cid:12)

E[fn(m)]

E[fn(m)]

1
3
≥
E[fn(m)]

o

.

(cid:3)

(cid:12)
(cid:12)

fn(m)

(cid:12)
(cid:12)
−
E[fn(m)]

−
3 E

≤

(cid:2)(cid:12)
(cid:12)

6
6
30

As a consequence, from (8.35) and (8.36), with κ > 1, for m
enough

∈ M∗ and m

= m∗ and n large

P

mpen = m

(8.37)

≤
Using this result as well as (8.37), one ﬁnally obtain (4.12).
Moreover,

(cid:2)(cid:12)
(cid:12)

b

(cid:0)

(cid:1)

(n E[en(m)])κ + 3

C ′1

n E

en(m)

E[en(m)]

−
n E[en(m)]

0.

−→n
→∞

(cid:3)

(cid:12)
(cid:12)

E

pen(m∗)

pen(

mpen)

h(cid:12)
(cid:12)

E

i
(cid:12)
pen(m∗)
(cid:12)

−

=

∗

Xm
∈M

b
h(cid:12)
(cid:12)

pen(

mpen)

mpen = m

P

mpen = m

+

E

′

Xm
∈M
pen(m∗)

h(cid:12)
(cid:12)
pen(m)

(cid:12)
(cid:12)

(cid:12)
(cid:12)
b
(cid:12) b
pen(m∗)
−

i

n

o

pen(

mpen)

b
mpen = m

P

mpen = m

P

b
mpen = m

(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) b

i

n

b

o

−

−

+

E

′

Xm
h(cid:12)
∈M
(cid:12)
C ′1
(n E[en(m)])κ

−

i
n
(cid:12)
(cid:12)
pen(m∗)

o

pen(m)

P

mpen = m

1 + n E

en(m)

i
n
b
E[en(m)]

(cid:12)
(cid:12)

−

o
C ′2
n

+

b
−

(cid:2)(cid:12)
(cid:12)

E[en(m)],

′

(cid:3)(cid:17)

(cid:12)
(cid:12)

Xm
∈M

=

E

∗

Xm
∈M

h(cid:12)
(cid:12)

1
n

≤

Xm
∈M

∗

(cid:16)

from (8.37), where κ > 1 and assumption (4.7). As a consequence, using the conditions (4.11)
of Theorem 4.3,

(8.38)

−→n
→∞
h(cid:12)
Moreover, using (8.23), there exists C3 > 0 such as for any m
(cid:12)

−

i

n E

pen(m∗)

pen(

mpen)

0.

Using once again the decomposition on

h(cid:12)
(cid:12)

we deduce

(8.39)

n E

penid(m∗)

(cid:12)
(cid:12)
b
penid(m)
≤
i
(cid:12)
M′, and P
(cid:12)

−
M∗ and

,

∈ M
C3.

n E

penid(m∗)

penid(

mpen)

−

h(cid:12)
(cid:12)

(cid:8)

b
−→n
→∞

0.

i

(cid:12)
(cid:12)

b

Using the limit (8.39) as well as (8.38), we deduce with Markov inequality that

n

pen(m∗)

pen(

mpen)

−

penid(m∗)

−

−

(cid:1)
inducing the proof of (4.13) from (8.34).
Now, using the expectation of (8.34), we also obtain

b

(cid:0)

h(cid:0)

penid(

mpen)

(cid:1)i

b

0

P
−→n
→∞

mpen = m

0 for m

= m∗,

−→n
→∞

(cid:9)

E

ℓ(

θ bmpen , θ∗)

≤

E

ℓ(

θm∗ , θ∗)

+ E

pen(m∗)

−

pen(

(cid:2)

b

(cid:3)

(cid:2)

b

(cid:3)

(cid:2)(cid:12)
(cid:12)

mpen)
+ E
b

(cid:2)(cid:12)
(cid:12)

(cid:3)

penid(m∗)
(cid:12)
(cid:12)

−

penid(

mpen)

.

(cid:3)

(cid:12)
(cid:12)

b

Now, by using (8.38) and (8.39) as well as Proposition 2, we obtain the proof of (4.14). (cid:4)

PROOF OF THEOREM 4.4. First we will prove that the probability of overﬁtting is asymp-

totically positive, which is

(8.40)

lim inf
n
→∞

P

mpen

(cid:16)

b

∗
∈ M

m∗

\ {

> 0.

}
(cid:17)

6
6
DATA-DRIVENMODELSELECTIONFORTIMESERIES

31

1/2

Gm(θ∗m)
1/2
(cid:0)

Z(m) and Z(m) D
1

∼ N
1/2 =

0, I

m
|

|

.

(cid:1)
P(m)D(m)P ⊤(m)

(cid:0)
−

(cid:17)
g(m∗)

,

. But using

(cid:17)

Indeed, let m

P

Cpen(m)

∈ M∗ \ {

m∗}
Cpen(m∗)

≤

(cid:16)

b

= P

n
b

(cid:17)
θm∗)

γn(

(cid:16)

(cid:0)

b

b

. We have:

γn(θ∗)

+ n

γn(θ∗)

γn(

θm)

−

g(m)

−

≥

−

g(m∗)

b

(cid:1)

(cid:0)
= P
b

n I2(m)
b
b
−

(cid:1)
n I2(m∗)

g(m)

≥

−

using the notations of Lemma 1 and since pen(m) = g(m)/n for any m
Lemma 1 and Proposition 2, we know that if m

then:

∈ M

(cid:16)

∈ M∗ \ {
U ∗m∗

m∗}
and E[U ∗m∗] < E[U ∗m]

n I2(m) D
−→n
→∞

U ∗m, n I2(m∗) D
−→n
→∞
Fm(θ∗m)

Gm(θ∗m)

1/2

1

−

Moreover, U ∗m =

Z ⊤(m)

−

(cid:0)
With a symmetric matrix diagonalization
Gm(θ∗m)
where D(m) is a diagonal matrix with positive diagonal components, leading to
(cid:1)

Gm(θ∗m)

Fm(θ∗m)

(cid:1)

(cid:0)

(cid:1)

(cid:1)

(cid:0)

(cid:1)

−

Therefore we can write for m
e

U ∗m =

Z ⊤(m) D(m)

(cid:0)
Z(m)

(cid:0)
Z(m) D

(cid:1)
0, I

and

m
∼ N
|
|
(cid:0)
(cid:1)
m∗ with
, U ∗m = V ∗m∗ + W ∗m
e
\

.

m∗}
∈ M∗ \ {
e
Z(m
m∗)
\

W ∗m
m∗ =
\

Z ⊤(m
m∗) D(m
m∗)
\
\

and

Z(m
m∗) D
\

∼ N

0, I

m

|−|

|

m∗

|

m∗ are two independent random variables. Moreover, it is clear that
and V ∗m∗ and W ∗m
e
e
e
\
m∗ behaves as a weighted χ2(
) random variable, and therefore for any
m
W ∗m
|
\
c > 0, P
(V ∗m∗

| − |
> 0. Using n I2(m)
m∗ > c
W ∗m
\
m∗ independent random variables, we deduce that
U ∗m∗) and W ∗m
(cid:1)
(cid:0)
\
n I2(m)

m∗ with
U ∗m∗) + W ∗m
\

n I2(m∗) D
−→n
→∞

m∗|
−

pm,m∗ > 0,

n I2(m∗)

g(m∗)

(8.41)

(V ∗m∗

g(m)

−

−

P

(cid:1)

(cid:0)

and this proves (8.40).

(cid:16)

−

≥

−

−→n
→∞

(cid:17)

Now, using previous notations, we have:

E

ℓ(

θ bmpen, θ∗)

=

ℓ(

θm, θ∗)

P

mpen = m

+

E

ℓ(

θm, θ∗)

P

mpen = m

(cid:2)

b

(cid:3)

Xm
∈M
ℓ(

= E

(cid:2)
b
θm∗, θ∗)

E

∗

(cid:2)

b

(cid:3)

(cid:0)
b
E

∗

+
m∗
Xm
(cid:3)
(cid:0)
\
∈M
+

(cid:2)
E

(cid:1)
θm, θ∗)

ℓ(

(cid:3)
b
θm, θ∗)
ℓ(

′

Xm
∈M
E
ℓ(

(cid:2)

b

θm∗, θ∗)

−

(cid:0)

(cid:3)
b
mpen = m

P

(cid:1)

(cid:2)
E

(cid:3)(cid:1)

b
θm∗ , θ∗)
ℓ(

(cid:0)
P

(cid:1)
b
mpen = m

.

−

(cid:2)
Now using Proposition 2, we know that for n large enough E
0 for any m

. Moreover, for m

b

(cid:0)

(cid:2)

(cid:3)

m∗ and n large enough,

b
θm, θ∗)

ℓ(

(cid:0)
E

(cid:1)
θm∗ , θ∗)

b
ℓ(

(cid:3)(cid:1)

−

≥

(cid:2)

b

(cid:3)

(cid:2)

b

(cid:3)(cid:1)

′

Xm
∈M

∈ M∗ \

∈ M

E

ℓ(

θm, θ∗)

E

ℓ(

θm∗, θ∗)

−

(cid:2)
≥

1
b
2n

(cid:3)
Trace

(cid:16)

(cid:2)
−

(cid:16)(cid:0)

(cid:3)
Fm(θ∗m)
b

−

(cid:1)

1 Gm(θ∗m)
(cid:17)

−

Trace

(cid:16)(cid:0)

−

Fm∗(θ∗m∗ )

1 Gm∗ (θ∗m∗)

−

(cid:1)

1
n

≥

(cid:17)(cid:17)

K(m, m∗),

32

where M (m, m∗) > 0. As a consequence, for n large enough, with p(m, m∗) deﬁned in
(8.41),

E

ℓ(

θ bmpen, θ∗)

E

≥

ℓ(

θm∗, θ∗)

+

(cid:2)

b

with M =

(cid:3)

(cid:2)

b

(cid:3)

1
K(m, m∗) p(m, m′) > 0 and this achieves the proof.
2
m∗
Xm
\
∈M

∗

1
2n

K(m, m∗) p(m, m′)
m∗
Xm
\
∈M

∗

E

≥

ℓ(

θm∗, θ∗)

+

M
n

,

(cid:2)

b

(cid:3)

(cid:4)

−

L(

θm)

∂2
θiθj

PROOF OF THEOREM 5.1. We ﬁrst verify conditions (C1) and (C2) of [8] that are suf-
σn the
0, which is satisﬁed

ﬁcient to imply Conditions (i), (ii) and (iii) of [24]. Condition (C1) requires that
largest eigenvalue of

i,j
since it was already established that 1
∂2
(cid:1)
θiθj
n
b
negative deﬁnite matrix. Moreover, condition (C2) is also satisﬁed because θm ∈
(cid:0)
∂2
∂2
θiθj
θiθj
1
n large enough. Therefore, using hn =
(cid:0)(cid:0)
(cid:0)
n
satisﬁed and this implies that:

a.s.
σn
−→n
+
→
a.s.
Fm(θ∗m) and Fm(θ∗m) is a
−→n
+
b
→
Θm 7→
b
b
1 are continuous functions for
L(θm)
Ln, the assumptions of Theorem 1 of [24] are
b

m and θm ∈

1 satisﬁes

Θm 7→

L(θm)

θm)

L(
(cid:1)

−

i,j

i,j

i,j

∞

∞

b

b

b

m

m

m

(cid:0)

(cid:0)

(cid:1)

(cid:1)

(cid:1)

(cid:1)

−

−

∈

∈

∈

∈

bm(θ) exp

Ln(θ)

dθ = exp

b
θm)

Ln(

2 π

/2

m
|

|

ZΘm

(cid:0)

b

(cid:1)
det

×

As a consequence, we have:

n

(cid:16)

(cid:0)

−

b
b
∂2
L(
θiθj

(cid:1) (cid:0)
θm)

(cid:0)
1
n

(cid:1)

i,j

m

∈

(cid:1)

b

b

1/2

−

(cid:17)

(cid:16)

bm(

θm) + O(n−

b

a.s.

1)
(cid:17)

S(m, X) =

−

log(

|M|

) + log

b

log(n)
2

=

Ln(

θm)

−
log(2π)
2
and Theorem 5.1 follows.

+

b

b

bm(θ) exp

Ln(θ)

dθ

h ZΘm
m
|

|

+ log

(cid:0)
b
θm)

bm(

i

(cid:1)

m
|

| −

(cid:0)
log

1
2

b
det

(cid:1)

−

Fn(m)

−

(cid:0)

(cid:0)

b

(cid:1)(cid:1)

log(

) + O(n−

|M|

1) a.s.

(cid:4)

Aknowledgments. This work has received funding from the European Union’s Horizon 2020
research and innovation programme under the Marie Sklodowska-Curie grant agreement No
754362. We also thank Christian Francq for some really important suggestions.

REFERENCES

[1] AKAIKE, H. (1973). Information theory and an extension of the maximum likelihood principle. Proceedings
of the 2nd international symposium on information, Akademiai Kiado, Budapest. MR0483125
[2] ARLOT, S. and MASSART, P. (2009). Data-driven calibration of penalties for least-squares regression. Jour-

nal of Machine learning research 10 245–279.

[3] BARDET, J. M., DOUKHAN, P. and WINTENBERGER, O. (2020). Contrast estimation of general locally

stationary processes using coupling. Preprint arXiv:2005.07397.

[4] BARDET, J. M., KAMILA, K. and KENGNE, W. (2020). Consistent model selection criteria and goodness-

of-ﬁt test for common time series models. Electronic Journal of Statistics 14 2009–2052.

[5] BARDET, J. M. and WINTENBERGER, O. (2009). Asymptotic normality of the quasi-maximum likelihood
estimator for multidimensional causal processes. The Annals of Statistics 37 2730–2759. MR2541445
[6] BOZDOGAN, H. (1987). Model selection and Akaike’s information criterion (AIC): The general theory and

its analytical extensions. Psychometrika 52 345–370.

DATA-DRIVENMODELSELECTIONFORTIMESERIES

33

[7] CAVANAUGH, J. E. (1997). Unifying the derivations for the Akaike and corrected Akaike information cri-

teria. Statistics & Probability Letters 33 201–208. MR1458291

[8] CHEN, C.-F. (1985). On asymptotic normality of limiting density functions with Bayesian omplications. J.

R. Statist. Soc. B 47 540–546.

[9] DEDECKER, J. and DOUKHAN, P. (2003). A new covariance inequality and applications. Stochastic Pro-

cesses and Applications 106 63-80.

[10] DEDECKER, J., DOUKHAN, P., LANG, G., LEÓN, J. R., LOUHICHI, S. and PRIEUR, C. (2007). Weak
dependence: With Examples and Applications. Lecture Notes in Statistics 190, Springer-Verlag, New
York.

[11] DEDECKER, J. and PRIEUR, C. (2004). Coupling for τ −Dependent Sequences and Applications. Journal

of Theoretical Probability 17 861-885.

[12] DING, J., TAROKH, V. and YANG, Y. (2018). Model Selection Techniques: An Overview. IEEE Signal

Processing Magazine 35 16–34.

[13] DING, Z., GRANGER, C. and ENGLE, R. F. (1993). A long memory property of stock market returns and a

new model. Journal of empirical ﬁnance 1 83–106.

[14] DOUKHAN, P. and WINTENBERGER, O. (2008). Weakly dependent chains with inﬁnite memory. Stochastic

Processes and their Applications 118 1997–2013. MR2462284

[15] FINDLEY, D. F. and WEI, C. Z. (2002). AIC, overﬁtting principles, and the boundedness of moments of
inverse matrices for vector autotregressions and related models. Journal of Multivariate Analysis 83
415–450.

[16] FRANCQ, C. and ZAKOIAN, J. M. (2010). GARCH models: structure, statistical inference and ﬁnancial

applications. John Wiley & Sons. MR3185978

[17] HANNAN, E. J. and QUINN, B. G. (1979). The Determination of the Order of an Autoregression. Journal

of the Royal Statistical Society. Series B (Methodological) 41 190–195.

[18] HSU, H. L., ING, C. K. and TONG, H. (2019). On model selection from a ﬁnite family of possibly mis-

speciﬁed time series models. The Annals of Statistics 47 1061–1087.

[19] HURVICH, C. M. and TSAI, C. L. (1989). Regression and time series model selection in small samples.

Biometrika 76 297–307. MR1016020

[20] ING, C. K., SIN, C. Y. and YU, S. H. (2012). Model selection for integrated autoregressive processes of

inﬁnite order. Journal of Multivariate Analysis 106 57–71. MR2887680

[21] ING, C. K. and WEI, C. Z. (2005). Order selection for same-realization predictions in autoregressive pro-

cesses. The Annals of Statistics 33 2423–2474. MR2211091

[22] KARAGRIGORIOU, A. (1997). Asymptotic efﬁciency of the order selection of a nongaussian AR process.

Statistica Sinica 407–423.

[23] KASHYAP, R. L. (1982). Optimal choice of AR and MA parts in autoregressive moving average models.

IEEE Transactions on Pattern Analysis and Machine Intelligence 4 99–104.

[24] KASS, R. E., TIERNEY, L. and KADANE, J. B. (1990). The Validity of Posterior Expansions Based on
Laplace’s Method. Essays in Honor of George Barnard, eds. S. Geisser, J. S. Hodges, S. J. Press, and
A. Zellner, Amsterdam: North-Holland 473–188.

[25] KENGNE, W. (2021). Strongly consistent model selection for general causal time series. Statistics & Prob-

ability Letters 171 109000.

[26] LI, K. C. (1987). Asymptotic optimality for C_p, C_L, cross-validation and generalized cross-validation:

Discrete index set. The Annals of Statistics 15 958–975. MR902239

[27] LING, S. and MCALEER, M. (2003). Asymptotic theory for a vector ARMA-GARCH model. Econometric

theory 19 280–310. MR1966031

[28] LV, J. and LIU, J. S. (2014). Model selection principles in misspeciﬁed models. Journal of the Royal Sta-

tistical Society: Series B 76 141–167. MR3153937

[29] MALLOWS, C. L. (1973). Some comments on Cp. Technometrics 15 661–675.
[30] MASSART, P. (2007). Concentration inequalities and model selection. Springer. MR2319879
[31] MCQUARRIE, A. and TSAI, C. L. (1998). Regression and Time Series Model Selection. World Scientiﬁc

Pub Co Inc. MR1641582

[32] PAPANGELOU, F. (1994). On a distributional bound arising in autoregressive model ﬁtting. Journal of ap-

plied probability 31 401–408.

[33] RAO, C. R., WU, Y., KONISHI, S. and MUKERJEE, R. (2001). On model selection. Lecture Notes-

Monograph Series 1–64.

[34] SCHWARZ, G. (1978). Estimating the dimension of a model. The Annals of Statistics 6 461–464. MR468014
[35] SCLOVE, S. L. (1987). Application of model-selection criteria to some problems in multivariate analysis.

Psychometrika 52 333–343.

[36] SHAO, J. (1997). An asymptotic theory for linear model selection. Statistica sinica 221–242.
[37] SHIBATA, R. (1980). Asymptotically efﬁcient selection of the order of the model for estimating parameters

of a linear process. The Annals of Statistics 147–164. MR557560

