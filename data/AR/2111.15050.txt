AssistSR: Task-oriented Video Segment Retrieval for Personal AI
Assistant

Stan Weixian Lei, Difei Gao, Yuxuan Wang, Dongxing Mao,
Zihan Liang, Lingmin Ran, Mike Zheng Shou†

Show Lab, National University of Singapore

2
2
0
2

t
c
O
0
1

]

V
C
.
s
c
[

4
v
0
5
0
5
1
.
1
1
1
2
:
v
i
X
r
a

Abstract

It is still a pipe dream that personal AI as-
sistants on the phone and AR glasses can as-
sist our daily life in addressing our questions
like “how to adjust the date for this watch?”
and “how to set its heating duration? (while
pointing at an oven)”. The queries used in
conventional tasks (i.e. Video Question An-
swering, Video Retrieval, Moment Localiza-
tion) are often factoid and based on pure text.
In contrast, we present a new task called Task-
oriented Question-driven Video Segment Re-
trieval (TQVSR). Each of our questions is an
image-box-text query that focuses on affor-
dance of items in our daily life and expects
relevant answer segments to be retrieved from
a corpus of instructional video-transcript seg-
ments. To support the study of this TQVSR
task, we construct a new dataset called As-
sistSR. We design novel guidelines to create
high-quality samples. This dataset contains
3.2k multimodal questions on 1.6k video seg-
ments from instructional videos on diverse
daily-used items. To address TQVSR, we
develop a simple yet effective model called
Dual Multimodal Encoders (DME) that signif-
icantly outperforms several baseline methods
while still having large room for improvement
in the future. Moreover, we present detailed
ablation analyses. Code and data are available
at https://github.com/StanLei52/TQVSR.

1

Introduction

As shown in Fig. 1a, a user is looking at his watch
and wondering “how to adjust the date for this
watch". It would be great if our phone or glasses
can be powered by an intelligent agent, i.e. AI assis-
tant, which perceives exactly what the user sees and
can ﬁnd out the answer in the instructional video of
this watch, either provided by the watch’s seller or
posted on YouTube. However, such a video is usu-
ally quite long and thus time-consuming for users

†

Corresponding author.

(a)

(b)

Figure 1: (a) AI assistant on phone/glasses addresses
user’s question in daily life. (b) Questions in previous
tasks vs. our TQVSR task.

to watch, let alone there are a lot of online tutorial
videos for various items. To save a user’s time, we
aim to retrieve only the relevant short segments in
a long video for that speciﬁc watch.

This problem is related with several exist-
ing tasks including Video Question Answering
(VideoQA), Video Retrieval and Moment Localiza-
tion. Yet, to unlock the aforementioned application
in Fig. 1a, we need a new task and a new bench-
mark, which have two major differences compared
to the existing tasks when designing questions:

• Factoid vs. Task-oriented. Existing tasks and
datasets mainly focus on factoid questions or
queries. For example in Fig. 2, Question 1 in
ActivityNetQA (Yu et al., 2019), Query 2 in
DiDeMo dataset (Anne Hendricks et al., 2017),
and Query 3 in MSR-VTT (Xu et al., 2016) are

AI: Sir, pls watch this inst-ructionalvideo segment.AI AssistantOnPhone/ AR GlassesUser: How to adjust the date for this watch?Q1: What is the man holding?Q2: How to clean up the plastic dust bucket of my RoidmeX20S vacuum?Q3: How to clean up <this part> of the vacuum?FactoidtoTask-orientedTextualtoMultimodal 
 
 
 
 
 
Figure 2: Illustration for TQVSR & TQVSR vs. other video-related tasks. TQVSR focuses on task-oriented and
multimodal questions: an image or referring regions as visual part and a natural language query as textual part.
TQVSR is set-based: given a question, the system is to retrieve from the video corpus the relevant segments.

all about simple facts, e.g. basic attributes, rela-
tionships between common objects. In contrast,
we expect the AI assistant to go beyond simple
facts and tackle Task-oriented questions. As
shown in Fig. 1b, instead of asking “what is
the man holding”, we focus on questions like
“how to clean up <this part> of the vaccum”.
Such questions regarding the affordance (Gib-
son, 1977) of objects or devices are often asked
by people in daily life.

• Pure text vs. Multimodal. Questions or
queries in existing datasets are mostly pure text.
However, psychology literature shows that point-
ing to the interesting objects is one of the initial
manners that a baby conveys intention (Mani
et al., 2020; Oates and Grayson, 2004; Malle
et al., 2001). It is unnatural for human to ask
the AI assistant with phrases like Q2 in Fig 1b
or “on the top-right of the vacuum”. It is more
straightforward to directly point to that part and
ask: “How to clean up <this part
> of the
vacuum?”. This is a multimodal question con-
sisting of a textual question, the current image
seen by the AI assistant, and the visual region
pointed out by the user’s ﬁnger.

Task.

Given a multimodal,

task-oriented
question, we propose a new task called Task-
oriented Question-driven Video Segment Retrieval
(TQVSR), which expects the model to retrieve the
relevant video segments that can address the user’s
question. As shown in Fig. 2, (1) unlike video-
text retrieval which focuses on returning a whole
video, TQVSR takes a step further to locate the
relevant segment within a long instructional video.
(2) Unlike moment localization whose query di-

rectly describes the segment’s content, we ﬁnd it
is often difﬁcult for different annotators to agree
on where the exact start and end times are for task-
oriented question. To provide a fair comparisons,
TQVSR ﬁrst asks annotators to chunk a long video
into short segments and then models can only focus
on whether retrieve a segment or not. Details in
Sec. 3.

Dataset. To support studying TQVSR, we create
a new dataset called AssistSR. Throughout multi-
ple iterations, we form a novel annotation pipeline
and guideline, which can effectively address anno-
tation challenges including (1) source high-quality
videos that support asking task-oriented questions;
(2) annotators tend to ask factoid and simple ques-
tions; (3) how to alleviate ambiguity for answer
segment annotation. In total, we have collected
3,214 high-quality questions on 1,607 segments
from 210 videos.

Model. To develop model for TQVSR, we
propose Dual Multimodal Encoders (DME), a
straightforward yet effective approach that has two
transformer-based multi-modal encoders for the
user question and a video segment respectively. We
further show DME outperforms previous methods
and each modality in question and video matters.

2 Related Work

Video Question Answering. This task requires
the intelligent system to automatically answer a
natural language question according to the con-
tent of a given video. In recent years, multiple
VideoQA datasets and tasks have been proposed to
facilitate research towards this goal, several video-
based QA datasets have also been proposed, e.g.
TGIF-QA (Jang et al., 2017), MovieFIB (Ma-

Task-oriented Question-driven Video Segment RetrievalMoment LocalizationText-Video RetrievalVideo QATextualQuery2.Thelittlegirljumpsbackupafterfalling.Video CorpusTextual Query 3. Cars are racing down a narrow road.…TextualQuestion1.Whatcoloristhegloveswornbythepersonwhoisskiing?ActivityNetQAMSR-VTTDiDeMoAnswer: BlackStart time: 00:18End time: 00:24TQVSRMultimodalQuestion4.Howtocleanup<thispart>ofthevacuum?Video Corpus…haraj et al., 2017), VideoQA (Zhu et al., 2017),
LSMDC (Rohrbach et al., 2015), TRECVID (Over
et al., 2014), and MarioQA (Mun et al., 2017). Ad-
ditionally, Video Question Answering (Lei et al.,
2018; Tapaswi et al., 2016; Kim et al., 2017), with
naturally occurring subtitles are particularly inter-
esting, as it combines both visual and textual in-
formation for question answering. Different from
VideoQA, where a system is required to generate
an answer in a video-instance based setting, the
task of TQVSR is to ground the answer segments
from the video corpus for a given question.

Video Retrieval. The objective of Video Retrieval
is that given a text query and a pool of candi-
date videos, select the video which corresponds
to the text query. A good amount of work has
been done in the area of natural language query
based video search for complete videos, such as
MSR-VTT (Xu et al., 2016), DiDeMo (Anne Hen-
dricks et al., 2017), ActivityNet Captions (Krishna
et al., 2017), CharadesSTA (Gao et al., 2017), and
TACoS (Regneri et al., 2013). However, return-
ing the whole video is not always desirable, since
sometimes they can be quite long (e.g.
from a
few minutes to hours). What’s more, most of the
queries for Video Retrieval are based on declara-
tive sentences, but not question-based, thus lead-
ing to the gap to real world application. However,
the setting of video retrieval is far from practical
application due to the factoid query. In TQVSR,
we focus more on the task-oriented user question
expressed in a multimodal manner and adopt the
video segment level retrieval, whereby the returned
video segments within proper time-span are able to
address the user’s question.

Moment Localization. The task of Moment
Localization is to localize moments from a
video given a natural language query. Various
datasets (Anne Hendricks et al., 2017; Gao et al.,
2017; Lei et al., 2020b; Krishna et al., 2017; Reg-
neri et al., 2013) have been proposed or repurposed
for the task. While these datasets for moment local-
ization collect only a single moment for each query-
video pair, we annotate one or more video seg-
ments for each query in our dataset, which is more
ﬂexible. Meanwhile, most existing datasets (Sun
et al., 2014; Gygli et al., 2016; Song et al., 2016;
Garcia del Molino and Gygli, 2018) for moment
localization are query-agnostic, which do not pro-
vide customized moments for a speciﬁc user query.
Recently (Lei et al., 2021a) introduced a dataset

for query-based video moment retrieval and high-
light detection, which also lies in the category of
moment localization. Unlike moment localization,
TQVSR is question-driven and aims to retrieve all
relevant segments. Sec.3 analyzes the challenges
in adapting moment localization for TQVSR and
our rationales behind formulating it as a segment
retrieval problem.

3 Task Formulation for TQVSR

Challenges. Given a user question, naturally we
would think of annotating the start time and end
time for the answer span, as moment localization
does. However, this leads to some problems. (1)
From the perspective of application, different
users have different preference for answer clips:
some prefer to longer clips to learn more contexts,
while others just tend to watch the key parts. This
leads to the difﬁculty in the deﬁnition of correct
answer span. (2) From the perspective of met-
ric evaluation, in the task of moment localization,
models are forced to generate a prediction with
high tIoU to the ground truth annotation. However,
tIoU might not measure the correctness of the pre-
dicted answer span well. For example, a predicted
answer span with higher tIoU might miss some vi-
tal information for addressing the user’s question,
while another candidate answer span with lower
tIoU to ground truth might work better qualita-
tively. What’s more, a prediction span generated
by localization might not be semantically intact,
which is not user friendly.
Our proposed solution of evaluating on seg-
ments. To avoid such drawbacks and remove ambi-
guities in answer annotation, we propose to chunk
a video into segments with well-designed rules and
then annotate the answer segments among the can-
didate video segments. With predeﬁned segments,
the model no longer needs to predict the times-
tamp, but only needs to predict whether a segment
contains the content for answering the user’s ques-
tion or not, which signiﬁcantly alleviates ambiguity
during evaluation.

Notably, throughout multiple iterations of im-
proving annotation guidelines, we arrived at the
following design principles of how to segment a
video: (1) Keep consistent level of semantic gran-
ularity within one video segment. (2) Soft thresh-
old for video segment duration: 30 seconds as
minimum and 2 minutes as maximum. (3) Adjust
segmentation according to the list of qualiﬁed ques-

To source videos, we ﬁrstly maintain a scenario
list which covers the majority of the highest level in
HowTo100M (Miech et al., 2019) to ensure diver-
sity. To obtain high-quality videos, we further set
some criteria, which could be found in appendix.

• To utilize auto-generated ASR captions, we ex-
clude videos without voice-over or with only
scene-text.

• Videos should contain actions or salient motion
for showcasing the feature of the subject item;
• Video contents should be rich enough to support

addressing user’s question.

Overall, these videos are captured via different
devices (e.g., mobile phone or GoPro) with differ-
ent view angles (e.g., ﬁrst-person view or third-
person view), posing important challenges to com-
puter vision systems. All the transcripts generated
by ASR are in English.

In total, we collected 210 videos with an average
duration of 539 seconds. All collected videos are
mainly from the domain of commonly used items,
including sub-categories ranging from home ap-
pliances, digital gadgets to smart devices. Fig. 4
shows some scenarios with high frequency in our
In Tab.1, we compared AssistSR with
dataset.
some recently proposed datasets related to task-
oriented operations or intelligent assistants: #1.
TC-QA (Tan et al., 2020), #2. TutorialVQA (Colas
et al., 2020) and #3. ScreenCast QA (Zhao et al.,
2020). We can see that our AssistSR has compa-
rable size to these datasets in terms of #scenarios,
#videos, #segments, #QAs and total duration.

Figure 4: High frequency scenarios in our dataset.

Figure 3: Data collection procedure for AssistSR.

tions. More details and examples could be found
in appendix.
Task deﬁnition of TQVSR. The task of TQVSR
addresses an task-oriented user question by retriev-
ing from the video corpus the relevant video seg-
ments. Taking a question Q with multiple modali-
ties as input, from the corpus of video segments V,
the task is to retrieve all the relevant segments V Q
ans
which can provide answer contents for the given
question: V Q
i ∈ V}.

i | q = Q, i ∈ |V| , vq

ans = {vq

4 A New Benchmark: AssistSR

Overview. Our pipeline of data collection and an-
notation procedure is summarized in Fig. 3. In
short, we conduct video collection, question col-
lection, video segmentation and answer segment
annotation for data collection. Quality control is
carefully designed to ensure the data and annota-
tion quality. We will explain them one by one.

Challenges. Here we list the challenging points
in data collection and annotation, and provide our
(1) Source di-
solutions and rationales behind.
verse videos of high-quality. We show our prin-
ciples in Sec. 4.1. (2) How to ensure the quality
of questions: ask the ones people would naturally
ask and balance the question types. We provide a
veriﬁcation-involved process in Sec. 4.2.
Summary of our AssistSR dataset.

In total,
we collected 3,214 questions associated with 1,607
segments in 210 videos for the AssistSR dataset.
We provide some examples of AssistSR along with
qualitative results in Fig. 8.

4.1 Video Collection

We collect a set of videos which cover a wide range
of scenarios and contain interesting and diverse
contents. Here, each scenario refers to a category of
commonly used item in daily life, such as vacuums,
digital watches and so on.

4.2 Question Collection

Procedure for annotating questions. To collect
high-quality questions, the annotators were asked
to:

1. Go through and understand the whole video.

TrainingCurriculumAnnotatorPassedAuditorVideoCollectionQuestionCollectionVideoSegmentationAnnotation ProcessInitialData PoolAuditingFinalData PoolPassedNot Passed-Collect questions:•Natural / Interesting-Balance query types:•Text / Video / Text + Video-Split the video into segments:         •30s –2min•Consistent semantic levelAnswer AnnotationAuditingQualityControl                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Datasets
#1
#2
#3
AssistSR

#Scen.
1
1
1
92

#Vid.
-
76
76
210

#Seg.
495
408
-
1607

#QAs Tot. Duration
991
6195
17768
3214

41.25 min
-
333 min
1887.9 min

Table 1: Statistics on various datasets related to topics
of task-oriented operations or intelligent assistants.

2. Come up with a list of questions which can be
clearly tackled after watching the video. We
exclude the questions that a user would not be
interested in. For example, “What color is this
juicer?” which is superﬁcial.

3. Conduct self-veriﬁcation and cross-veriﬁcation
on the question list. Each annotator checks its
question list and ﬁlters out the unqualiﬁed ques-
tions while for cross-veriﬁcation, another an-
notator who has not watched the same video
checks the questions from the view of a real
user.

4. Collect and annotate the qualiﬁed questions. Af-
ter harvesting the qualiﬁed textual questions, the
annotators are required to collect and annotate
the query images.

Next, we provide details for our question collec-

tion and annotation.

Multimodal query. Each question annotated in
AssistSR is multimodal, which means that it is com-
posed of a textual part and a visual part. The textual
part is a free-form query written by our annotators.
The visual part is an image for the object the anno-
tator poses a question on. Inspired by (Mani et al.,
2020; Zellers et al., 2019), we allow annotators to
pose a question on speciﬁc parts of an object and
locate its position with a bounding box. By incor-
porating the visual information, annotators are able
to write more natural questions. For example, they
can ask “How to clean up <this part
> ?” in-
stead of “How to clean up the plastic dust bucket
of my Roidme X20S vacuum?”, with a bounding
> referring to the speciﬁc
box tag <this part
part of the vacuum cleaner.

We also provide some strategies to avoid col-
lecting query images from the target ground-truth
video segment: searching different instances on the
Internet, sampling a query image from non-GT seg-
ments with different contexts from GT segments,
and editing images if they have to be sampled from
GT segments. This makes TQVSR more difﬁcult
than simple visual matching. Details in appendix.
Query type. In practice, videos are associated

with multi-modalities such as audio and text, e.g.
subtitles for TV shows, transcripts for instructional
videos or audience discourse accompanying live
stream, which could be also important sources for
retrieving relevant segments. Following (Lei et al.,
2020b, 2018), we encourage annotators to write
questions that are related to different modalities,
aiming at enabling the model to learn the knowl-
edge from the video in a systematic way. To test
the model’s ability to retrieve the relevant segment
for a given question, we categorize all the questions
into 3 types:

1. t type: the visual part of the annotated question
only helps to match the subject in the video, one
can retrieve the relevant clips by textual clues
from videos besides such matching.

2. v type: one can only locate the relevant clips by

visual clues from videos.

3. v+t type: one should locate the relevant seg-
ments by leveraging both visual and textual
clues from videos. In this case, the visual part of
the question not only helps to match the subject,
but also provides vital information for segment
retrieval.

Similar to (Lei et al., 2020b, 2018), in our pi-
lot test, we observed that our annotators preferred
to write t type questions, of which the answers
segment can be easily retrieved by reading the tran-
scripts. To ensure that we collect a balance of
queries requiring one or both modalities, we set
up awarding mechanism for asking v type and t+v
type type questions. More examples of query types
could be found in appendix.
Overview of the collected questions. We col-
lected 3,214 questions in total. On average, our
question contains 8.65 words. Each question is as-
sociated with one user image and with 0.31 bound-
ing boxes annotated on average. Fig. 6b shows the
distribution of question length, showing that most
of the queries in our dataset have more than 10
words.

In Fig. 5, by visualizing the Alluvial diagram
of the most frequent ﬁrst three words in the ques-
tions of our dataset, we observe that the text query
pattern in our dataset is not limited to “how to”
type questions. In real jobs, annotators can also
ask questions with “what”, “when”, “where” and
“why”. Also, they are allowed to state the goal (e.g.
“I want to ...”) with a declarative sentence ﬁrst and
then ask a question. We can observe from Fig. 5
that “what” type question is the most dominant

video might be separated by irrelevant content.

4.5 Quality Control

To ensure data and annotation quality, we’ve de-
signed a detailed quality assurance mechanism,
which is presented in appendix. Brieﬂy, annota-
tors were required to take the training curriculum
and pass the qualiﬁcation test before working on
real jobs. Auditors are to evaluate the workers per-
formance on the qualiﬁcation test and check the
quality of sampled annotations from the initial data
pool. Typical issues in early training include ask-
ing unnatural questions, misunderstanding the rules
for chunking a video, etc. Evaluation scores are
rated on a scale of 1 (bad), 2 (minor errors) and 3
(good). Workers should retake the training curricu-
lum when the rating score is deemed insufﬁcient.
In practice, the performance of an annotator is sat-
isfying and acceptable if its average rating is above
2.5. More details could be found in appendix.

5 Method

We introduce Dual Multimodal Encoders (DME)
for TQVSR. Fig. 7 gives an overview of our model.
Formally, we deﬁne the inputs into the model as: a
candidate video segment associated with transcripts
and a user question composed of an image with or
without referring regions, and a textual query.

Model Architecture. DME is composed of two
multimodal encoders based on the transformer net-
work (Vaswani et al., 2017). In TQVSR, both user
question and a context video segment are multi-
modal, i.e. both of them are composed of vision
input and language input, requiring a model for
learning joint contextualized representations. In-
spired by the recent success of the vision and lan-
guage model (Lu et al., 2019; Su et al., 2020; Li
et al., 2019), we follow (Li et al., 2019) to use the
self-attention mechanism within the Transformer
to implicitly align elements of the input text and
regions in the question and model the contextual
information of the input video segment.

Input Representation. We extract visual fea-
tures for video appearance, query image and region
with ResNet-50 backbone (He et al., 2016) with
weights from grid-feat (Jiang et al., 2020), which
is trained on Visual Genome (Krishna et al., 2016).
We extract contextualized text features using a 6-
layer pretrained DistilBert (Sanh et al., 2019). Both
visual features and textual features are projected
into the same dimension, and added with token

Figure 5: Alluvial diagram of the most frequent ﬁrst
three words in the collected questions.

Figure 6: Distribution of segmentation and query
length.

in our dataset, followed by “how to” and “could”.
The “what” type questions mainly ask about the
functions of an item, and the third word in “how to”
type questions are mainly verbs, showcasing the
diversity and complexity of our questions, which is
task-oriented.

4.3 Video Segmentation

Following Sec. 3, we ask annotators to chunk a
video into predeﬁned segments and detailed analy-
sis can be found in Sec. 3 and appendix.

Finally we collected 1,607 segments upon 210
videos. The average duration for all chunked video
segments is 70.5 seconds. Fig. 6a shows the distri-
bution of the length of the chunked video segments.
Most of the video segments in our dataset have a
duration longer than 50 seconds.

4.4 Answer Annotation

With predeﬁned segments, annotators are required
to select the answer segments for each question.
Each query with contents that can address the given
question should be annotated. In AssistSR, there
could be multiple disjoint answer segments paired
with a single query (on average 1.03 answer seg-
ments per question), while all the moment localiza-
tion or video retrieval datasets can only have one
single moment or one single video. This is a more
realistic setup as relevant content to a query in a

whatisthe[function]howItoshouldcancouldyoutellusewethedoesinstallthisitchangesetareImakeremovecanwherewhenFirst WordSecond WordThird Word0%4%8%12%16%20%30-4040-5050-6060-7070-8080-9090-100100-110110-120120+Segment Duration (sec)(a)Distribution of Video Seg Duration0%5%10%15%20%25%30%4-67891011121314+Query Length (words)(b) Distribution of Query LengthFigure 7: Illustration of our Dual Multimodal Encoders for TQVSR.

type embedding and positional embedding. We
concatenate the text feature and visual feature to
create a single sequence embedding and a learnable
[cls] token (Devlin et al., 2019) is concatenated
to the beginning of the input feature, which is used
to produce the ﬁnal output representation of each
transformer-based encoder.

Training and Inference. During training, one
relevant segment is randomly sampled to be paired
with a question. For training, we use loss function
as in (Zhai and Wu, 2018) for segment retrieval
setting, where matching question-segment pairs in
a batch are treated as positives, and all other pair-
wise combinations in the same batch are treated as
negatives. We maximize the score between posi-
tive pairs and minimize the score between negative
pairs. At inference time, the DME model requires
only the dot product between the multimodal ques-
tion embedding and candidate video segment em-
beddings. This retrieval inference is of trivial cost
since questions and video segments are indexable
and therefore it is scalable to large scale retrieval.
More details are in appendix.

6 Experiment

6.1 Baseline

To explore the performance of other video-related
tasks on TQVSR, we evaluate the following meth-
ods and modify the original implementation to ﬁt
in our inputs if necessary. More details could be
found in appendix.

#1 Random guess. For each question, we ran-
domly shufﬂe the list of all video segments as
the ranked results. #2 SiameseNet (Chopra et al.,
2005) for image-video matching. #3 TVQA. (Lei
et al., 2018) A multi-modal videoQA method
in which QA pairs are used to fuse with visual
and textual features in the video separately. We
also replace LSTM (Hochreiter and Schmidhuber,
1997) in the original implementation with trans-
former (Vaswani et al., 2017). #4 XML (Lei et al.,

2020b) is a late fusion approach for Video Cor-
pus Moment Retrieval (VCMR). XML(VR) views
each video segment in TQVSR as a video, and
just compiles the Video Retrieval part in XML.
XML(VR+ML) uses the rough start-end time in
AssistSR annotation for VCMR setting. #5 Clip-
BERT (Lei et al., 2021b) is a video-language
framework that sparsely and randomly sample
video clips and frames within a clip for end-to-end
training.

6.2 Experimental Setting

Dataset. We split our AssistSR into 80% train,
10% val and 10% test such that videos and their
associated queries appear in only one split. We
include scenarios which are unseen in the train set
into the validation set and test set. The distribution
of query patterns and query types are kept aligned
among these splits.

Evaluation metrics. To evaluate the perfor-
mance of TQVSR, we use mean average precision
(mAP) as in the information retrieval system. We
also report standard metric Recall@1 (R@1) and
Recall@5 (R@5) used in text-to-video retrieval
and single moment retrieval. For TQVSR, we use
mAP as the main metric. Note mAP is able to mea-
sure the model performance when the number of
ground-truth video segments is greater than one.

Method

mAP

R@1

R@5

All

Unseen

All

Unseen

All

Unseen

Random guess
SiameseNet
TVQA-LSTM
TVQA-Trans.
ClipBERT
XML(VR)
XML(VR + ML)
DME(ours)
DME(Unicoder)

2.66
11.77
13.30
14.46
18.14
17.45
18.37
22.92
25.18

2.65
11.50
11.51
12.99
17.67
16.44
14.76
20.44
22.75

0.45
3.90
1.24
5.63
7.43
5.93
7.92
11.94
13.51

0.45
3.42
0.69
3.59
6.92
5.56
5.56
9.61
10.65

2.18
14.19
23.51
19.52
27.23
25.18
27.23
30.13
34.61

2.18
13.68
20.04
18.06
26.92
22.22
19.44
25.62
33.45

Table 2: Experimental results on various methods for
TQVSR test set.

Question EncoderVideo Encoder[cls]… it’ll glow blue to indicate…Clip#1Clip#N… press this button and………Clip#1Clip#NA Multimodal Question: What does <this> indicate?…it’ll glow blue to indicate… Just press this button and …A Candidate Videosegment: [cls]what     does    <this>   indicate?…、、Score、Learnable EmbeddingVisual Feature ExtractorTextual Feature Extractor………(A Multimodal Question)(A Candidate Video Segment)………Figure 8: DME prediction examples for TQVSR, for AssistSR dataset. Dashed box indicates the GT segments.

6.3 Quantitative Results

Performances on all scenarios. AssistSR val set
results are shown in Tab. 2. We can observe that:
(1) All the methods designed for video-related
tasks are clearly better than random guess. DME
clearly outperforms other methods for video-
related tasks, indicating that models designed for
other tasks can not generalize well to the TQVSR
task.

(2) SiameseNet for image-video matching per-
forms worse than other baselines because it only
considers visual matching, discarding the textual
information in questions and transcripts. This also
indicates that TQVSR is not a simple matching
task, yet it requires the model to learn the textual
knowledge behind.

(3) In TVQA, we see that TVQA-Trans. out-
performs TVQA-LSTM, indicating that the trans-
former can better capture information from video
segments. We also see that DME works better than
TVQA, and this implies that the design of sepa-
rate query-video appearance context matching and
query-video transcript context matching may ig-
nore to capture the alignment of video appearance
and transcripts.

(4) In ClipBERT, the sparsely sampled frames
and their temporally aligned transcripts generated
by ASR might fail to capture the global feature. As
in HowTo100M (Miech et al., 2019), it is likely that
the transcript is not depicting the content in the sam-
pled frame. This modeling design for multimodal
video might fail to learn a good representation for
global information while DME’s fusion strategy
for multimodal can avoid such a situation.

(5) DME also outperforms both XML(VR) and
XML(VR+ML). XML(VR+ML) with more super-
vision higher than XML(VR) in mAP. In XML,
separate self-attention modules are used for dif-

ferent modalities in video retrieval.
Its inferior
performance compared to DME indicates that the
multimodal encoder might be able to learn better
representations for multimodal question and video
representation, which leads to better performance
on the TQVSR task.

Therefore, to tackle TQVSR, we need more cus-
tomized designs for multi-modal video context
modeling and query-video interaction.

Performances on unseen scenarios. From Tab. 2
We can observe that ClipBERT, XML and DME
generalize poorly to the unseen scenarios:
the
mAP score on the Unseen set is clearly lower
than that on the All set. This is because for items
within the same scenario could share similar knowl-
edge, while for items for totally different scenarios,
model may fail to bridge the gap of the difference
in appearance, language style, etc. Here we leave
seeking better generalization on TQVSR for future
work. All these conﬁrm the challenging nature of
TQVSR task and the need of our new benchmark.

Would pretrained models help? For our pro-
posed task, a promising direction is: leverage mod-
els pre-trained on other massive data and then only
need a small amount of training data to ﬁnetune
on our downstream task. We therefore initialized
DME encoders from various pre-trained models:
ViLBert (Lu et al., 2019), VisualBert (Li et al.,
2019) and Unicoder-VL (Li et al., 2020), and then
ﬁnetuned using 25%, 50%, 75%, 100% of AssistSR
training set respectively. Fig. 9 shows that initializ-
ing DME encoders with various pre-trained models
can improve DME (ﬁnetune w/ our whole train
set). Also, when using only 75% training data,
using pre-trained model can achieve comparable
mAP to DME with 100% training data, indicating
that proper vision and language model pretraining
would help in the TQVSR task.

Multimodal Question 4: What is the function of <this part>?Multimodal Question 1: What is the function of <thisbutton>?…Video Segment  Rank#1: Video Segment Rank#2: Video Segment Rank#3: … indicator will illuminate specific color on the rooms air quality... also, when we switch the mode…… the “Speed” button is used to change the speed or mode… the ”Filter Reset” button is used to... … andfinally, the Ambient Light button, which is used to switch on the night light... Video Segment  Rank#1: Video Segment Rank#2: Video Segment Rank#57: … just ahead of that you have this large sensor this is how the cane detect the obstacles……… but with this one you got a little button so you can press that...  It is a little bit tricky… … simply pushes in and clicks, now you can take this up your stairs... It is possible to use the machine… Multimodal Question 5: How to use <this> with my air fryer?Multimodal Question 2: How to connect <it> to Bluetooth speaker?Video Segment  Rank#1: Video Segment Rank#2: Video Segment Rank#3: … so in the display settings you got the folder slideshow settings which you can go under…… to connect open Bluetooth settings and look for... so now I can go into my Bluetooth... … Now we are on the setting, let’s start off by trying to connect the Bluetooth... Video Segment  Rank#1: Video Segment Rank#2: Video Segment Rank#110: … I love using instant read thermometers, checking the temperatures of chick or pork……… forwards opening the control unit back, open the control unit cover using a screwdriver… … I note there is still quite a bit of clean solution left, so hopefully I will have enough try to tack … Multimodal Question 6: How to can I download the app for <this speaker>?Multimodal Question 3: What is <this> used for?Video Segment  Rank#1: Video Segment Rank#2: Video Segment Rank#3: … there is no package inside anywhereso I don’t start anything on fire…… Ok, let’s start with this one. You may want to fluff up egg whites when you’re making ... … Sowe have got, let’s see, six. I know that a few of these attachments are for the sausages... Video Segment  Rank#1: Video Segment Rank#2: Video Segment Rank#91: … “welcome to Google home, to get started, download the Google home app on phone or tablet”……… so whenever you give a command it will automatically the top of this part so that you know… … so it is a small little pack, on the bottom here you have a nice soft surface so that it doesn’t move… ……Question

Video

mAP

R@1

Text
#1 (cid:88)
(cid:55)
#2
(cid:55)
#3
#4 (cid:88)
#5 (cid:88)
#6 (cid:88)
#7 (cid:88)

Image Trans. Visual
(cid:55)
(cid:88)
(cid:55)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:55)
(cid:88)
(cid:55)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:55)
(cid:55)
(cid:88)

All
13.36
11.31
10.29
17.86
18.87
15.84
22.92

T
12.94
9.06
8.61
20.43
25.65
23.08
27.37

V
12.8
12.85
12.08
9.36
9.21
7.18
18.29

T + V
14.08
13.33
11.56
18.24
14.79
10.79
19.54

All
4.5
4.65
3.00
6.68
8.36
8.86
11.94

T
3.95
1.97
0.66
8.55
12.50
14.47
13.82

V
9.62
9.62
7.69
0.00
2.88
1.92
11.54

T + V
3.1
5.81
3.88
7.17
5.68
5.04
9.88

All
17.42
12.91
14.11
27.20
26.25
19.97
30.13

R@5

T
16.45
11.84
14.47
30.59
38.16
27.30
40.13

V
9.62
13.46
9.62
17.31
11.54
11.54
19.23

T + V
21.71
13.95
15.50
27.2
18.15
14.73
22.74

Table 3: Ablation experiment results on various modalities for TQVSR.

Considering the complexity of our task, masking
any modality could affect the model’s performance.
Therefore, using multi-modalities is a direct way
to improve the interaction quality.

6.4 Qualitative Results

We present qualitative examples in Fig. 8. DME
works well for many cases: for example, in the
left column, it correctly retrieved the most relevant
answer segment. Particularly, rank #2 and rank
#3 results in col 1 row 2 show that it can retrieve
segments with highly correlated contents. The right
column shows the case where DME missed the
correct segment.

7 Conclusion and Future Work

This paper presents a new task, Task-oriented
Question-driven Video
Segment Retrieval
(TQVSR), requiring intelligent systems to address
the given task-oriented question by retrieving
relevant segments from a video corpus. To support
the evaluation of such a task, we construct a
dataset, AssistSR, which contains 3.2k questions
on 1.6k video segments from instructional videos
on diverse daily-used items. Further, we provide
analyses of this new dataset as well as several
baselines and a multi-stream end-to-end trainable
neural network framework for TQVSR. The
experimental results show that TQVSR is still a
formidable challenge for all current methods. We
hope this benchmark can help drive the vision
and language models to be deployed in real-world
applications and boost the development of personal
AI Assistants.

Acknowledgement

This project is supported by the National Research
Foundation, Singapore under its NRFF Award
NRF-NRFF13-2021-0008, and Mike Zheng Shou’s
Start-Up Grant from NUS. The computational work

Figure 9: Results of DME with pretrained weights on
different proportion training data.

Modality removal from video source. For the
video encoder, we respectively mask the transcript
(shown as Trans.) and the video appearance (shown
as Visual). From #1 and #4 in Tab. 3, we can see
that mAP on All drops signiﬁcantly when either
transcript or video appearance is masked. Perfor-
mance drops more on T and T+V when masking
transcript and leads to obvious degradation on V
when masking video appearance.
What if removing modality from the user’s
question? For question encoder, we mask the tex-
tual and visual part of the query (shown as Text and
Image in Tab. 3) respectively and analyze the effect
on the model performance. From #2 and #5, we
see that mAP on T+V declines to almost the same
when the textual or the visual part is masked. For
V questions, the mAP declines less when masking
the textual part. In contrast, the mAP on T declines
more when the text query is masked.
Textual vs. Visual. Comparing #1-#3 with #4-#6
on All set, masking textual modality leads to a more
drastic drop on mAP: this indicates that the textual
modality is more important than visual modality.
The reasons could be that: (1) the textual part es-
pecially the text query drives the retrieval task; (2)
DME over-relies on the textual information and
lacks the alignment between text and vision, thus
undermining the contribution of visual information.
This can be observed from #7: with all modalities,
the mAP of V questions is lower than T and T+V.

25%50%75%100%ProportionofTrainingDataUsed1618202224mAP(%)ViLBERTVisualBERTUnicoder-VLDMEw/opretrainedw100%trainingdatafor this article was partially performed on resources
of the National Supercomputing Centre, Singapore.

Limitations

For the limitations of this work, we realize that the
size of our dataset is limited due to the difﬁculties
in the time consuming data collection and annota-
tion process. We will make further efforts to create
large-scale datasets in this topic, including widen-
ing for more scenarios and designing strategies to
scale up data collection in a more efﬁcient man-
ner. For societal impact, we believe the proposed
TQVSR is an important stepping stone towards
building an intelligent agent. But to prevent the
abuse of this technology, such as assisting unautho-
rized users to use dangerous items, we will take this
into account and prohibit misuse cases explicitly
in our license when releasing the developed codes
and models.

References

2020.

How long should videos be?
for marketing.
length

ideal
https://www.talkingtreecreative.
com/blog/video-marketing-2/
the-impact-of-video-length-on-engagement/.

the
[online].

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. In NAACL.

Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Neva-
tia. 2017. Tall: Temporal activity localization via
language query. In Proceedings of the IEEE interna-
tional conference on computer vision, pages 5267–
5275.

Ana Garcia del Molino and Michael Gygli. 2018. Phd-
gifs: Personalized highlight detection for automatic
gif creation. pages 600–608.

James J Gibson. 1977. The theory of affordances. Hill-

dale, USA, 1(2):67–82.

Ross Girshick. 2015. Fast r-cnn. In Proceedings of the
IEEE international conference on computer vision,
pages 1440–1448.

Ross Girshick, Jeff Donahue, Trevor Darrell, and Jiten-
dra Malik. 2014. Rich feature hierarchies for accu-
rate object detection and semantic segmentation. In
Proceedings of the IEEE conference on computer vi-
sion and pattern recognition, pages 580–587.

Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo,
Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P
Bigham. 2018. Vizwiz grand challenge: Answering
visual questions from blind people. In Proceedings
of the IEEE Conference on Computer Vision and Pat-
tern Recognition, pages 3608–3617.

2020. Video length: 4 tips that will help you boost en-
[online]. https://meetmaestro.com/

gagement.
blogs/how-long-should-your-video-be/.

Michael Gygli, Yale Song, and Liangliang Cao. 2016.
Video2gif: Automatic generation of animated gifs
from video. pages 1001–1009.

Lisa Anne Hendricks, Oliver Wang, Eli Shechtman,
Josef Sivic, Trevor Darrell, and Bryan Russell. 2017.
Localizing moments in video with natural language.
In Proceedings of the IEEE international conference
on computer vision, pages 5803–5812.

Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-
garet Mitchell, Dhruv Batra, C Lawrence Zitnick,
and Devi Parikh. 2015. Vqa: Visual question an-
swering. In Proceedings of the IEEE international
conference on computer vision, pages 2425–2433.

Sumit Chopra, Raia Hadsell, and Yann LeCun. 2005.
Learning a similarity metric discriminatively, with
application to face veriﬁcation. In 2005 IEEE Com-
puter Society Conference on Computer Vision and
Pattern Recognition (CVPR’05), volume 1, pages
539–546. IEEE.

Anthony Colas, Seokhwan Kim, Franck Dernoncourt,
Siddhesh Gupte, Zhe Wang, and Doo Soon Kim.
2020. TutorialVQA: Question answering dataset
for tutorial videos. In Proceedings of the 12th Lan-
guage Resources and Evaluation Conference, pages
5450–5455, Marseille, France. European Language
Resources Association.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
In Proceedings of the IEEE conference on
nition.
computer vision and pattern recognition, pages 770–
778.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim,
and Gunhee Kim. 2017. Tgif-qa: Toward spatio-
temporal reasoning in visual question answering. In
Proceedings of the IEEE conference on computer vi-
sion and pattern recognition, pages 2758–2766.

Huaizu Jiang, Ishan Misra, Marcus Rohrbach, Erik
Learned-Miller, and Xinlei Chen. 2020. In defense
In
of grid features for visual question answering.
Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, pages 10267–
10276.

Kyung-Min Kim, Min-Oh Heo, Seong-Ho Choi, and
Byoung-Tak Zhang. 2017. Deepstory: Video story
qa by deep embedded memory networks.
pages
2016–2022.

Diederik P. Kingma and Jimmy Ba. 2015. Adam:
CoRR,

A method for stochastic optimization.
abs/1412.6980.

Arjun Mani, Nobline Yoo, Will Hinthorn, and Olga
Russakovsky. 2020. Point and ask: Incorporating
pointing into visual question answering.

Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei,
and Juan Carlos Niebles. 2017. Dense-captioning
events in videos. In Proceedings of the IEEE inter-
national conference on computer vision, pages 706–
715.

Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,
Makarand Tapaswi, Ivan Laptev, and Josef Sivic.
2019. HowTo100M: Learning a Text-Video Embed-
ding by Watching Hundred Million Narrated Video
Clips. In ICCV.

Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-
son, Kenji Hata, Joshua Kravitz, Stephanie Chen,
Yannis Kalantidis, Li-Jia Li, David A Shamma,
Michael Bernstein, and Li Fei-Fei. 2016. Visual
genome: Connecting language and vision using
crowdsourced dense image annotations.

Jie Lei, Tamara L Berg, and Mohit Bansal. 2021a.
Qvhighlights: Detecting moments and highlights in
videos via natural language queries. arXiv preprint
arXiv:2107.09609.

Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L.
Berg, Mohit Bansal, and Jingjing Liu. 2021b. Less
is more: Clipbert for video-and-language learn-
ingvia sparse sampling. In CVPR.

Jie Lei, Licheng Yu, Mohit Bansal, and Tamara L Berg.
2018. Tvqa: Localized, compositional video ques-
tion answering. In EMNLP.

Jie Lei, Licheng Yu, Tamara Berg, and Mohit Bansal.
2020a. Tvqa+: Spatio-temporal grounding for video
question answering. pages 8211–8225.

Jie Lei, Licheng Yu, Tamara L Berg, and Mohit Bansal.
2020b. Tvr: A large-scale dataset for video-subtitle
moment retrieval. In ECCV.

Gen Li, Nan Duan, Yuejian Fang, Ming Gong, and
Daxin Jiang. 2020. Unicoder-vl: A universal en-
coder for vision and language by cross-modal pre-
In Proceedings of the AAAI Conference
training.
on Artiﬁcial Intelligence, volume 34, pages 11336–
11344.

Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui
Hsieh, and Kai-Wei Chang. 2019. Visualbert: A
simple and performant baseline for vision and lan-
guage. arXiv preprint arXiv:1908.03557.

Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan
Lee. 2019. Vilbert: Pretraining task-agnostic visi-
olinguistic representations for vision-and-language
tasks. In NeurIPS.

Tegan Maharaj, Nicolas Ballas, Anna Rohrbach, Aaron
Courville, and Christopher Pal. 2017. A dataset and
exploration of models for understanding video data
through ﬁll-in-the-blank question-answering.
In
Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 6884–6893.

Bertram F Malle, Louis J Moses, and Dare A Baldwin.
2001. Intentions and intentionality: Foundations of
social cognition. MIT press.

Jonghwan Mun, Paul Hongsuck Seo, Ilchae Jung, and
Bohyung Han. 2017. Marioqa: Answering ques-
tions by watching gameplay videos. In Proceedings
of the IEEE International Conference on Computer
Vision, pages 2867–2875.

John Ed Oates and Andrew Ed Grayson. 2004. Cog-
nitive and language development in children. Open
University Press.

Paul Over, Jon Fiscus, Gregory Sanders, David Joy,
Martial Michel, George Awad, Alan Smeaton, Wes-
sel Kraaij, and Georges Quénot. 2014. Trecvid 2014
– an overview of the goals, tasks, data, evaluation
mechanisms, and metrics.

Michaela Regneri, Marcus Rohrbach, Dominikus Wet-
zel, Stefan Thater, Bernt Schiele, and Manfred
Pinkal. 2013. Grounding action descriptions in
videos. Transactions of the Association for Compu-
tational Linguistics, 1:25–36.

Anna Rohrbach, Marcus Rohrbach, Niket Tandon, and
Bernt Schiele. 2015. A dataset for movie descrip-
tion. In Proceedings of the IEEE conference on com-
puter vision and pattern recognition, pages 3202–
3212.

Victor Sanh, Lysandre Debut, Julien Chaumond, and
Thomas Wolf. 2019. Distilbert, a distilled version
of bert: smaller, faster, cheaper and lighter. arXiv
preprint arXiv:1910.01108.

Yale Song, Miriam Redi, Jordi Vallmitjana, and Ale-
jandro Jaimes. 2016. To click or not to click: Auto-
matic selection of beautiful thumbnails from videos.
pages 659–668.

Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu,
Furu Wei, and Jifeng Dai. 2020. Vl-bert: Pre-
training of generic visual-linguistic representations.
In International Conference on Learning Represen-
tations.

Min Sun, Ali Farhadi, and Steve Seitz. 2014. Rank-
ing domain-speciﬁc highlights by analyzing edited
videos. In ECCV.

Hui Li Tan, Mei Chee Leong, Qianli Xu, Liyuan Li,
Fen Fang, Yi Cheng, Nicolas Gauthier, Ying Sun,
and Joo Hwee Lim. 2020. Task-oriented multi-
modal question answering for collaborative applica-
In 2020 IEEE International Conference on
tions.
Image Processing (ICIP), pages 1426–1430. IEEE.

Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen,
Antonio Torralba, Raquel Urtasun, and Sanja Fidler.
2016. Movieqa: Understanding stories in movies
through question-answering. In Proceedings of the
IEEE conference on computer vision and pattern
recognition, pages 4631–4640.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information process-
ing systems, 30.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander M. Rush. 2020.
Transformers: State-of-the-art natural language pro-
cessing. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing:
System Demonstrations, pages 38–45, Online. Asso-
ciation for Computational Linguistics.

Jun Xu, Tao Mei, Ting Yao, and Yong Rui. 2016. Msr-
vtt: A large video description dataset for bridging
IEEE International Confer-
video and language.
ence on Computer Vision and Pattern Recognition
(CVPR).

Zhou Yu, Dejing Xu,

Jun Yu, Ting Yu, Zhou
Zhao, Yueting Zhuang, and Dacheng Tao. 2019.
Activitynet-qa: A dataset for understanding complex
web videos via question answering. In Proceedings
of the AAAI Conference on Artiﬁcial Intelligence,
volume 33, pages 9127–9134.

Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin
Choi. 2019. From recognition to cognition: Vi-
sual commonsense reasoning. In The IEEE Confer-
ence on Computer Vision and Pattern Recognition
(CVPR).

Andrew Zhai and Hao-Yu Wu. 2018. Classiﬁcation is
a strong baseline for deep metric learning. arXiv
preprint arXiv:1811.12649.

Wentian Zhao, Seokhwan Kim, Ning Xu, and Hailin
Jin. 2020. Video question answering on screen-
In Proceedings of the Twenty-Ninth
cast tutorials.
International Joint Conference on Artiﬁcial Intel-
ligence, IJCAI-20, pages 1061–1068. International
Joint Conferences on Artiﬁcial Intelligence Organi-
zation. Main track.

Linchao Zhu, Zhongwen Xu, Yi Yang, and Alexan-
der G Hauptmann. 2017. Uncovering the temporal
context for video question answering. International
Journal of Computer Vision, 124(3):409–421.

Appendix

A. More Details of AssistSR Dataset.
B. More Details on Dataset Creation.
C. More Details of DME Method.
D. More Details of Experiment.

A More details of AssistSR Dataset

A.1 Comparison with other datasets.

In Tab. 4, we compare AssistSR to previous
VideoQA, Video Retrieval, Video Moment Local-
ization datasets. In summary, our dataset is pos-
In addition,
sessed with multimodal questions.
we provide affordance-centric questions, aiming
to drive the models to learn the knowledge from
multimodal instructional videos to address the user
problem beyond simple facts. Besides, AssistSR
can have multiple disjoint answer segments paired
with a single query (on average 1.03 answer seg-
ments per question), while all the moment localiza-
tion or video retrieval datasets can only have one
single moment or one single video. This is a more
realistic setup as relevant content to a query in a
video might be separated by irrelevant content.

A.2 Visualization Examples

In Fig. 10, we show some visualization examples
in our datasets. Speciﬁcally, we show examples of
different scenarios (different devices as shown in
row 1 - row 6 in Fig. 10) and examples of different
functions of one scenario (as shown in row 7 - row
8 in Fig. 10).

In AssistSR, each multimodal question is as-
sociated with an image (with or without bound-
ing boxes for referring to a speciﬁc region) and a
human-written query. GT video segments contain
the content which can address the given question.

B More Details on Dataset Creation

B.1 Video Collection

For video collection, we focus on commonly used
items which can support asking affordance-centric
questions. To improve efﬁciency for sourcing
video, we provide some tips for sourcing pattern:

• “Tutorials/Instructional videos for ..."
• “Best tips & tricks for ..."
• “How to use a ..."
• Searching by the name of a brand, e.g. Xiaomi.
• Check the recommendation list.
• Check the Youtuber’s channel if you found sev-

- List of Contents in Appendix:

eral videos of his/hers are qualiﬁed.

Figure 10: Examples in the AssistSR dataset.

Multimodal Question 1: How to calibrate <this hand> for my watch?Multimodal Question 2: What is the function of <it> on my exercise bike?…to check this one it just has to point to the correct day of the week which is also ok now in the case these are not pointing to where they should you’re gonna have to correct them and to correct these hands while in the home screen you press and hold the adjust button for 5seconds so you have to ignore the first beep……and that’s right here so if you don’t want any resistance you would go here but if you want to start having resistance you would click according to how much resistance you want so this is a pretty large dial here and very easy to turn and very self-explanatory of how to use it…Multimodal Question 3: What is <suction switch> of the food cutter used for ?…hold it and turn the suction switch to on they should keep the base attached to your counter. Now you can see it’s not moving it’s attached to the counter there are three blades included. The blade just slide right into the slot and there’s two more blade here you canstore…Multimodal Question 4: How to customize the layout for <this>?…This camera let you customize whatever you see back in that camera, so you just have to go again to your control center and select the little frame with the person right here, and that’s gonnagive you all the options you can use. You can do the actual screen and this is what you’re gonnalook at…Multimodal Question 5: What is <that> with my earbuds?…The fit of these has been great. Definitely with the tip sizes in the box though I found the median-sized tip fit great for oneear while the smaller tip fit better in another. One other great thing about the fit of these earbuds is they sit pretty much flush in yourear so they’re less easy to knock out…Multimodal Question 6: What does <the light> indicate of the vacuum?…and then of course plug the adapter into the wall socket, so you can have this on your kitchen worktop charging or next to aconvenient power point. When the cleaner is charging you will see a solid blue light when it’s fully charged the light goes out. Ok I’ve fullycharged my Dyson V7…Ground Truth Video Segment to be Retrieved:WatchExercise BikeFood CutterGoProGround Truth Video Segment to be Retrieved:Ground Truth Video Segment to be Retrieved:Ground Truth Video Segment to be Retrieved:Ground Truth Video Segment to be Retrieved:Ground Truth Video Segment to be Retrieved:EarbudsVacuum CleanerMultimodal Question 7: What is the function of <knobs> of oscilloscope?Multimodal Question 8: What is the function of <buttons> of oscilloscope?OscilloscopeGround Truth Video Segment to be Retrieved:Ground Truth Video Segment to be Retrieved:…These are the horizontal and vertical positioning knobs. Turing the horizontal position knob lets you adjust where the waveformstarts and stops on the screen and allows you to line it up with divisions should you choose. Turing the vertical position knob allows you to move the……By turning the horizontal control knob for example, we can make the divisions represent a longer or shorter period of time. Or by turning the vertical knob, we can make the divisions represent a smaller or larger voltage scale. This is in effect a zoom feature of  the oscilloscope …OscilloscopeDataset

Video source

Query source

Query modal

Textual Visual Textual Visual

Affordance-centric

Movie-QA (Tapaswi et al., 2016)
TVQA (Lei et al., 2018)
TACoS (Regneri et al., 2013)
DiDeMo (Anne Hendricks et al., 2017)
ActivityNet Cap (Krishna et al., 2017)
TVR (Lei et al., 2020b)
ScreenCast QA (Zhao et al., 2020)
ours

movie
TV show
cooking
Flickr
Activity
TV show
Software tutorial
Tutorial for devices

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

-
(cid:88)
-
-
-
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

-
-
-
-
-
-
-
(cid:88)

-
-
-
-
-
-
(cid:88)
(cid:88)

Table 4: Comparison of AssistSR to various existing VideoQA/Video Retirieval/Moment Localization datasets.

We also set some criteria for quality assurance:

• To utilize auto-generated ASR captions, we ex-
clude videos without voice-over or with only
scene-text.

• Videos should contain actions or salient motion
for showcasing the feature of the subject item;
• Video contents should be rich enough to support

addressing user’s question.

B.2 Question Collection

Figure 11: Query type distribution of the AssistSR
dataset.

Query type. To test the model’s ability to retrieve
the relevant segment for a given question, we cate-
gorize all the questions into 3 types: v type, t type
and t+v type.

Similar to (Lei et al., 2020b, 2018), in our pi-
lot test, we observed that our annotators preferred
to write t type questions, of which the answers
segment can be easily retrieved by reading the tran-
scripts. To ensure that we collect a balanced set
of queries requiring one or both modalities, we set
up awarding mechanism for asking v type and t+v
type questions. In Fig. 12 we provide examples
of different query types and also explain why it
belongs to that speciﬁc type. We also provide these
examples in Supp video.

In Fig. 11, we show the distribution of the query
types for our collected questions in the AssistSR
dataset. With the help of the awarding mechanism,
we balanced the number of t type and not-t type
questions in our dataset.

Multimodal query. Human psychology litera-
ture shows that pointing to interesting objects or
situations is one of the ﬁrst ways by which babies
communicate intention (Mani et al., 2020; Oates
and Grayson, 2004; Malle et al., 2001). The multi-
modal query adopted in AssistSR not only helps to
provide an unambiguous link between the textual
description of an object and the corresponding im-
age region, but also avoids unnatural questions in
pure text format.

In AssistSR, the annotators are required to pro-
vide an image from a user’s view, indicating the
subject one would pose a question on. They are
allowed to annotate bounding boxes for a speciﬁc
part if it is needed in asking a question. Overall,
each collected question is associated with an image,
and on average 0.31 bounding boxes. In Fig. 13,
we show the distribution of the size of bounding
boxes, in terms of the bounding box area over the
whole image.

Figure 13: Distribution for Area(bbox)
Area(img) .

Source of User Image. Most query images are
NOT from the Ground-Truth (GT) segments to be
retrieved. To do so,

Query Types in AssistSRt typev typet+v typet+vtype34.32%v type15.41%ttype50.37%0%5%10%15%20%25%30%0-2%2-5%5-10%10-20%20-30%30-50%50-100%Bounding Box Area DistributionPercentage of Bounding BoxesFigure 12: Examples of different query types in the AssistSR dataset.

Question: How to delete my record in this blood pressure monitor?Relevant segment: transcripts: “Pressing and holding the mem button for more than five seconds  will erase all the measurements from the device…”t type: image only helps to match the device. One can retrieve by only reading the transcripts.importantcluesintranscriptsforthemodeltoretrievethecorrectsegmentQuestion: What does <this light> indicate?Relevant segment: transcripts: “When you place them back in the case, the case will show the indicator light at the bottom, to indicate the level of charge…”t+v type: this light in image and “light” in transcripts appear in several segments. One should combine textual and visual information in the question to retrieve the correct segment.important clues in video for retrieving the correct segmentimportant clues in transcripts for retrieving the correct segmentQuestion: How to install batteries for this?Relevant segment: transcripts: “The unit doesn’t come with the power adapter,  but it could be purchased separately …”v type: Only video appearance shows how to install batteries in the video segment.important clues in video for the model to retrieve the correct segment1. Annotators are asked to ﬁrst search on Internet
to ﬁnd images of the same device but different
instance.

2. If not successful, annotators will ﬁrst source
query image from non-GT segments of different
contexts.

3. If still not successful, annotators have to crop
query image from GT segments, but we ask
them to edit the original images (e.g. jiterring,
ﬂipping, brightness).

As a result, 69.15% of the query images are
sourced from the internet and 8.28% are from a
non-ground-truth segment. All rows in Fig. 8 the
2nd column show such examples. Thus, most query
images in AssistSR are of different instances and/or
different contexts.

B.3 Video Segmentation

Principles and rationales for video segmenta-
tion. We provide principles for video segmentation
in Sec. 3 in our paper. Here we provide more de-
tails: give a video, we ask the annotator to segment
the videos following these principles:

1. Keep consistent level of semantic granularity
within one video segment. The idea is that we
want the content within one video segment to be
semantically consistent. For example, a quali-
ﬁed segment could be the one introducing dif-
ferent aspects of a subject or introducing partial
contents of an aspect (i.e. we can divide the
content of this aspect into several segments).
2. Soft threshold for video segment duration: 30
seconds as minimum and 2 minutes as maximum.
User study (how, 2020; vid, 2020), found that
video clips should fully engage the audience
within the ﬁrst 30 seconds to attract their atten-
tion. Also, it is observed that keeping videos
shorter than 2 minutes achieves the most viewer
engagement while engagement drops off sharply
after 2 minutes. Therefore, in video segmenta-
tion, we restrict the duration of each segment by
setting a soft threshold.

3. Adjust segmentation according to the list of qual-
iﬁed questions. Since we adopt a ﬂexible manner
for segmenting a video, the result segmentation
is not unique and thus adjustable. In order to
test the models ability on retrieving the correct
answer segment, we further ask our annotators
to adjust the segmentation based on the raised
question. For example, if one video segment

contains both contents of correct answer and
distraction of a question, the annotator should
separate this into a positive segment and a hard
negative segment.

We provide examples for video segmentation in
Supp video.

B.4 Quality Control

Here we provide the detailed quality assurance
guideline for the auditor’s evaluation. As shown in
our main paper, auditors are to evaluate the workers
performance on the qualiﬁcation test and check the
quality of the sampled annotations from the initial
data pool. To ensure the annotation quality, (1)
Before working on real jobs, the annotator should
carefully go through our guideline and annotate 5
videos (video collection, question collection, video
segmentation, answer annotation) and auditors will
audit annotators’ performance. If the performance
is not satisfying, the annotator should retake the
training curriculum. (2) During the real annotation
process, we conduct auditing for each annotator ev-
ery iteration by randomly inspecting samples they
completed in that iteration.

In the following, we ﬁrstly introduce the rating
score deﬁnition for auditing, followed by details
for different sub-tasks in annotation.

B.4.1 Rating score deﬁnition
1, bad: The sourced data / annotation is of low
quality. E.g, the annotator sourced unqualiﬁed
videos for subsequent annotations; asked unnat-
ural/uninteresting questions too often, chunked the
videos mistakenly; wrong query type. 2, medium:
occasional error. E.g less than 20% questions are
unnatural/uninteresting, but can be improved. The
main purpose of this intermediate level is to reﬂect
the need of improvement for the annotator while
acknowledging the annotator’s correct understand-
ing of the guideline. 3, good: source high-quality
videos. Good questions, segmentations, etc.

B.4.2 Video collection
1, bad: videos without voice (thus cannot obtain
transcripts by ASR); videos without salient ac-
tion(only scene text; only introduce the features
by words); videos cannot support any interesting
questions; more than 20% of videos do not meet
the requirements. 2, medium: 10%-20% videos
do not meet the requirements in the guideline. 3,
good: satisﬁes the requirements in our guideline:
videos with voice (someone is explaining how to

use the device/ ﬁnish a task; transcripts can be au-
tomatically generated); videos with salient action
(someone is showing how to use the item); videos
can support asking affordance-centric questions.

B.4.3 Question collection

We ﬁrstly deﬁne unnatural questions and uninterest-
ing questions with examples. Unnatural questions
refer to the ones a user would not naturally ask
in daily life. For example, for a knob on a mi-
crowave, a user might not ask “What would happen
if I turn this clockwise?”. Instead, he would ask
“How to set the heating time to 90 seconds” for
using purpose. Uninteresting questions refer to the
ones which are easy, e.g. “How to turn it on?”, the
answer of which is obvious. 1, bad: more than
20% questions are unnatural or uninteresting; miss
v type questions if it is obvious that one video is
found to support asking several questions of this
type; mistakes in annotating query type: the cor-
rect type should be t type but annotated as t+v type;
the correct type should be t+v type / v type but
annotated as t type, the correct type should be t
type but annotated as v type, the correct type is v
type but annotated as t type. 2, medium: occasion-
ally ask unnatural/uninteresting questions, but the
questions can be rephrased to be good ones; oc-
casional minor mistakes in annotating query type:
the correct type should be t+v type but annotated
as v type , the correct type should be v type but an-
notated as t+v type. 3, good: Annotation satisﬁes
the requirements in our guideline: ask natural and
interesting questions, the textual and visual parts
of the query make the question clear and easy to
understand; understand the meaning of the query
type and annotate correctly.

B.4.4 Video segmentation

1, bad: non-consistent semantic level within one
segment; there exists short segments (obviously
less than 30s) which should be merged with other
segments; do not reﬁne the segmentation based on
the questions. 2, medium: occasionally missed the
segmentation adjusting the segmentation. 3, good;
follow the three principles for video segmentation.

B.4.5 Answer annotation

1, bad: miss important contents to answer that
question; mistakenly include totally irrelevant seg-
ments. 3, good: the annotator correctly annotates
all the relevant segments.

B.5 Annotator Recruitment

For dataset creation, we hired annotators from a
university. We set up a training curriculum and hire
those who pass the pilot test (criteria in B.4). Fi-
nally 8 annotators are proceeded to the real job for
data collection and annotation. The dataset creation
costs around 600 man hours, with 10 USD/(man
hour).

C More Details of DME Method

Fig. 7 gives an overview of DME for TQVSR. For-
mally, we deﬁne the inputs into the model as: a can-
didate video segment associated with transcripts
and a user question composed of an image with or
without referring regions, and a textual query.

In our method, we represent each chunked video
segment vi as a list of consecutive short clips, i.e.,
vi = [ci,1, ci,2, . . . , ci,l], where l is the length of
video segment vi (#clips). In AssistSR, each short
clip is also associated with temporally aligned tran-
scripts.
Model architecture DME is composed of two mul-
timodal encoders based on the transformer net-
work (Vaswani et al., 2017). In TQVSR, both user
question and a context video segment are multi-
modal, i.e. both of them are composed of vision
input and language input, requiring a model for
learning joint contextualized representations. In-
spired by the recent success of the vision and lan-
guage model (Lu et al., 2019; Su et al., 2020; Li
et al., 2019), we follow (Li et al., 2019) to use the
self-attention mechanism within the Transformer
to implicitly align elements of the input text and
regions in the question and model the contextual
information of the input video segment.
Input representations. To represent a video seg-
ment, we consider appearance features. For each
frame, we used Resnet-50 (He et al., 2016) with
weights from grid-feat (Jiang et al., 2020), which is
trained on Visual Genome (Krishna et al., 2016) for
object detection and attribute classiﬁcation and pro-
duces effective features for image VQA tasks (An-
tol et al., 2015; Gurari et al., 2018). We resize a
frame to 448 × 746 and max-pool feature map after
the C5 block to get the 2048D representation. We
extract 2048D ResNet-50 features at 10 FPS and
max-pool the features every 1.5 seconds to obtain
a clip-level feature. To represent transcripts, we
extract contextualized text features using a 6-layer
pretrained DistilBert (Sanh et al., 2019). We used
the implementation of DistilBert from (Wolf et al.,

Method

TVQA (Lei et al., 2018)
ClipBERT (Lei et al., 2021b)
XML(VR) (Lei et al., 2020b)
XML(VR + ML) (Lei et al., 2020b)
DME(ours)

mAP
Seen Unseen
11.51
14.29
17.67
19.00
16.44
17.99
14.76
20.36
20.44
27.50

All
13.30
18.14
17.45
18.37
22.92

R@1
Seen Unseen
1.54
8.33
6.13
9.23
16.24

0.69
6.92
5.56
5.56
9.61

All
1.24
7.43
5.93
7.92
11.94

R@5
Seen Unseen
20.14
25.38
26.92
27.78
22.22
26.77
19.44
31.54
38.46
25.62

All
23.51
27.23
25.18
27.23
30.13

R@10
Seen Unseen
33.33
36.15
42.69
38.89
47.44
49.11
38.19
46.92
46.49
46.15

All
35.15
41.34
48.52
43.81
46.37

R@50
Seen Unseen
70.83
83.46
83.85
75.00
73.50
81.48
75.69
85.38
87.18
81.48

All
78.96
80.69
78.68
81.93
83.48

Table 5: Test set results on various methods for TQVSR. “All” means all queries; “Seen” means queries for seen
scenarios; “Unseen” means queries for unseen scenarios.

2020) to extract contextualized token embeddings
from its second-to-last layer (Lei et al., 2020a). Af-
ter extracting the token-level embedding, we then
max-pool them every 1.5 seconds to get a 768D
clip-level feature vector. We use a 768D zero vec-
tor if encountering no transcripts.

To represent the image and referring region in
the question, we used Resnet-50 (He et al., 2016)
with weights from grid-feat (Jiang et al., 2020). For
the image, similar to video appearance, we feed it
into the Resnet-50 and max-pool the feature map
after the C5 block to obtain a 2048D feature. For
a referring region located by a bounding box, we
apply RoI pooling (Girshick et al., 2014; Girshick,
2015) on the feature map after Resnet-50 C5 block
to obtain a 2048D region feature. To represent
the text query in the question, we directly used the
extracted token embeddings from the second-to-
last layer of DistilBert (Sanh et al., 2019).

Following (Li et al., 2019), all the extracted vi-
sual features are projected into 768D features via
a linear layer. Without ambiguity, we use the used
the symbols by denoting the processed symbols as
EQt ∈ Rlq×d, EQv ∈ RNr×d, EVf ∈ Rl×d and
EVt ∈ Rl×d, where Qt represents the textual query
of the question, lq is the length of this query; Qv
means the visual feature of the question input, Nr
is the number of regions, including the whole im-
age and regions being referred to; Vf and Vt means
frames and transcripts of the video segment, l is the
number of clips in this video segment and d is the
hidden size (d is set to be 768 in our experiments).

For multimodal encoders, we inject different
types of input attribute into E by adding two addi-
tional embedding layers: (1) token type encoding
that informs the type of information: using [vis]
for visual features in the question and appearance
feature in the video segment, while using [txt]
for features of textual query in the question and
transcript feature in video segment. (2) position
encoding that is used to inject signals of the token
ordering. For textual query in the question, posi-
tion encoding is following the sequence order. For

appearance and transcript features in the video seg-
ment is following the order of clips sequence. As
for visual features in the question, position encod-
ing is used when alignments between words and
bounding regions are provided as part of the input,
and is set to the sum of the position embeddings
corresponding to the aligned words as in (Li et al.,
2019).

These layers are trainable to enable models to
learn the dynamics of input features and are mod-
eled to have the same feature dimension d. We
combine all encoding layers through element-wise
summation for each modality in a multimodal en-
coder. Speciﬁcally for m ∈ {Qv, Qt, Vf , Vt}, the
result representation is:

Zm = Em + Em

tok + Em

pos.

For input representation of each encoder, we
concatenate the text feature and visual feature to
create a single sequence embedding:

ZQ = (cid:2)ZQt; ZQv (cid:3)
ZV = (cid:2)ZVt; ZVf (cid:3) .

Multimodal encoding Given the input features
ZQ, ZV , as shown in Fig. 7, we use two encoders to
compute their representation respectively. For each
multimodal encoder, a learned [cls] token (Devlin
et al., 2019) is concatenated to the beginning of the
input feature, which is used to produce the ﬁnal
output representation of the transformer. We de-
note the output of the Query Encoder as H Q ∈ Rd
for the question Q, and the output of the Video
Encoder as H V ∈ Rd for the video segment V .
Training and Inference During training, one rel-
evant segment is randomly sampled to be paired
with a question. For training loss, we employ (Zhai
and Wu, 2018) for segment retrieval setting, where
matching question-segment pairs in a batch are
treated as positives, and all other pairwise combi-
nations in the same batch are treated as negatives.
We maximize the score between positive pairs and
minimize the score between negative pairs. We

minimise the sum of two losses:

L1 = −

L2 = −

1
B

1
B

B
(cid:88)

i

B
(cid:88)

i

log

log

(cid:16)

exp

(cid:62)H Q

H V
i
(cid:16)

(cid:80)B

j=1 exp
(cid:16)

exp

H V
i

(cid:62)

(cid:17)

i /σ
(cid:62)H Q
j /σ
(cid:17)

H Q
i
(cid:16)

H V

i /σ
(cid:62)

(cid:80)B

j=1 exp

H Q
i

H V

j /σ

(cid:17)

(cid:17) ,

i and H Q

where H V
j here are the normalized em-
beddings of the i-th video segment and the j-th
question in a batch of size B and σ is the tempera-
ture. The overall loss function is L = L1 + L2 for
DME.

At inference time, the DME model requires only
the dot product between the multimodal question
embedding and candidate video segment embed-
dings. This retrieval inference is of trivial cost
since questions and video segments are indexable
and therefore it is scalable to large scale retrieval.
Speciﬁcally, DME conducts simple dot-product
at the feature-level and hence features of video
segments can be pre-computed and cached. We use
the popular similarity ranking library faiss-gpu 1 to
test the run time retrieval. We test on a server with
a server with 8 RTX3090 GPUs and AMD EPYC
7413 24-Core Processor: with pre-computed video
features, the retrieval time of one query is 16.65ms
for a 1K video corpus and 162.47ms for a 100M
video corpus. Although there is an increase in
similarity ranking time, it is still fast and acceptable
in practice.

D More Details of Experiment

D.1 Details for Baseline Methods

# SiameseNet (Chopra et al., 2005) for image-
video matching. As a simple baseline, we use
the SiameseNet to match the query image and the
frames of the answer segments. This SiameseNet
baseline does not include any textual inputs (ques-
tion and transcripts).
# TVQA. (Lei et al., 2018) proposed a multi-
stream end-to-end trainable neural network for
Multi-Modal VideoQA. In this model, the question-
answer pairs are used to fuse with visual features
and text features in the video separately. We modify
the input module to ﬁt in our multimodal question,
and fuse the question with different modalities from
the paired video segment separately.

1https://github.com/facebookresearch/faiss/

blob/main/tutorial/python/5-Multiple-GPUs.py

# XML (Lei et al., 2020b) is a late fusion approach
for Video Corpus Moment Retrieval (VCMR). In
XML, separated self-cross-encoders are used to en-
code visual and textual features of a video. The
query is feeded into a self-attention module fol-
lowed by a FC-layer to generate modularized query
representations. Late fusion is then applied for
Video Retrieval and moment retrieval. We add
a module image query in AQVSR, which is the
same to the original query branch in XML. We
use element-wise adding for image query and text
query, to generate modularized query. XML(VR)
views each video segment in AQVSR as a video,
and just compiles the Video Retrieval part in XML.
XML(VR+ML) uses the rough start-end time in As-
sistSR annotation for VCMR setting. At inference
time, we pick the highest score of the moments in
video segments as the score for that video segment;
for each query, we rank the scores of all candidate
video segment.
# ClipBERT (Lei et al., 2021b) is a generic frame-
work for video-language tasks. We modify Clip-
BERT a bit to ﬁt in our multi-modal query: we
linearly project the ResNet-50 (He et al., 2016)(pre-
trained weights from (Jiang et al., 2020)) C5 max-
pooled feature of the image query to text query fea-
ture. We also concatenate corresponding transcript
features for sampled frames to the transformer en-
coder used in ClipBert (Lei et al., 2021b) for visual-
textual fusion.

D.2

Implementation Details

We use the same set of ofﬂine extracted features for
DME, TVQA (Lei et al., 2018) and XML (Lei et al.,
2020b). We use ResNet-50 (He et al., 2016) pre-
trained by (Jiang et al., 2020) as feature extractor
for image query and use a 6-layer DistilBert (Sanh
et al., 2019) for text feature extraction. For all
transformer-based encoders, we set the number of
hidden layers to be 4 and set hidden size to be 768.
In the baseline experiment, we do not use any pre-
trained model for the transformer encoder for fair
comparison. For ClipBERT (Lei et al., 2021b), we
follow the text-video retrival protocol in the origi-
nal implementation. We employed 4 × 1 (randomly
sample 4 video clips and randomly sample 1 frame
within each clip) during training. We sampled 16
video clips and the middle frame of each clip dur-
ing testing. The duration of each video clip is 1.5
seconds, which is the same in other baselines. We
keep other settings the same as the original im-

plementation for TVQA2, XML3 and ClipBERT4.
For DME, We use Adam (Kingma and Ba, 2015)
optimizer and a learning rate of 3 × 10−5. The
hyper-parameter σ for the training loss is set to be
0.05.

D.3 More Experimental Results on AssistSR

show that applying concatenating fusion yields bet-
ter performance over adding fusion. This suggests
that to give full play to the advantages of Trans-
former in multimodal reasoning, it is necessary
to decouple multi-modal information into separate
sequences.

5, we

show the

performance
In Tab.
(mAP/Recall@1/Recall@5/Recall@10/Recall@50)
of different methods on AssistSR. We show the
performance on all questions, seen scenarios
(related to 73.7% quesitons) and unseen scenarios
(related to 26.3% questions). We can observe
that (1) DME clearly outperforms other methods
for video-related tasks,
indicating that models
designed for other tasks can not generalize well to
the TQVSR task. (2) For DME, ClipBERT (Lei
et al., 2021b) and XML (Lei et al., 2020b),
performance on seen scenarios is better than that
indicating that for these
on unseen scenarios,
methods,
they are able to capture something
in common within the same scenario but fail
to generalize well to unseen scenarios. This is
because for items within the same scenario could
share similar knowledge, such as structure and
functionality, while for items for totally different
scenarios, models may fail to bridge the gap of the
difference in appearance, language style, etc.

D.4 Ablation for input fusion type.

Method
Adding Fusion
Concatenating Fusion

mAP R@1 R@5 R@10
40.49
17.86
46.37
22.92

26.88
30.13

7.21
11.94

Table 6: Results for different input fusion types for
DME.

We study the effect of different fusion types for
the input of DME model. Here we conduct ex-
periments on two fusion types for the multimodal
encoder. (1) Adding fusion. For each multimodal
encoder in DME, fuse visual and textual features of
a video with positional alignment via element-wise
adding as in (Su et al., 2020); token type embed-
ding is not adopted in this case. (2) Concatenating
fusion. For each multimodal encoder in DME, we
concatenate the textual feature and visual feature
into a single sequence embedding. This is used for
our DME baseline. Experiment results in Tab. 6

2https://github.com/jayleicn/TVQA
3https://github.com/jayleicn/TVRetrieval
4https://github.com/jayleicn/ClipBERT

