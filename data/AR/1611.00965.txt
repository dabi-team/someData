Faster ARMA Maximum Likelihood
Estimation

A.I. McLeoda and Y. Zhangb

aDepartment of Statistical and Actuarial Sciences,
The University of Western Ontario,
London, Ontario Canada N6A 5B7

bDepartment of Mathematics and Statistics,
Acadia University,
Wolfville, Nova Scotia, Canada B4P 2R6

Preprint: A.I. McLeod andY. Zhang (2008), Faster ARMA maximum

likelihood estimation, Computational Statistics & Data Analysis, 52-4,

2166-2176. doi: 10.1016/j.csda.2007.07.020

6
1
0
2

v
o
N
3

]
T
S
.
h
t
a
m

[

1
v
5
6
9
0
0
.
1
1
6
1
:
v
i
X
r
a

1

 
 
 
 
 
 
Abstract

A new likelihood based AR approximation is given for ARMA models. The

usual algorithms for the computation of the likelihood of an ARMA model

require O(n) ﬂops per function evaluation. Using our new approximation,

an algorithm is developed which requires only O(1) ﬂops in repeated

likelihood evaluations. In most cases, the new algorithm gives results

identical to or very close to the exact maximum likelihood estimate (MLE).

This algorithm is easily implemented in high level Quantitative

Programming Environments (QPEs) such as Mathematica, MatLab and R.

In order to obtain reasonable speed, previous ARMA maximum likelihood

algorithms are usually implemented in C or some other machine eﬃcient

language. With our algorithm it is easy to do maximum likelihood

estimation for long time series directly in the QPE of your choice. The new

algorithm is extended to obtain the MLE for the mean parameter.

Simulation experiments which illustrate the eﬀectiveness of the new

algorithm are discussed. Mathematica and R packages which implement the

algorithm discussed in this paper are available (McLeod and Zhang, 2007).

Based on these package implementations, it is expected that the interested

researcher would be able to implement this algorithm in other QPE’s.

Keywords: Autoregressive approximation; Eﬃciency of the sample

mean; Maximum likelihood estimator; High-order autoregression; Long

time series and massive datasets; Quantitative programming environments

2

1. Introduction

The ARMA(p, q) model may be written in operator notation as

φ(B)(zt − µ) = θ(B)at, where B is the backshift operator on t,

φ(B) = 1 − φ1B − ... − φpBp, θ(B) = 1 − θ1B − ... − θpBq, µ is the mean of zt

and at is assumed to be Gaussian white noise with mean zero and variance

σ2
a. It is assumed that zt is causal-stationary and invertible so that all roots

of φ(B)θ(B) = 0 are outside the unit circle. For model identiﬁability it is

assumed that φ(B) and θ(B) have no common factors. Given n consecutive

observations from this time series model, z1, . . . , zn, the log-likelihood

function was discussed by Box, Jenkins and Reinsel (1994), as well as many

other authors. Other asymptotically ﬁrst-order eﬃcient methods are

available, such as the HR algorithm (Hannan and Rissanen, 1982) but

many researchers prefer methods of estimation and inference based on the

likelihood function (Barnard, Jenkins and Winsten, 1962; Fisher, 1973; Box

and Luce˜no, 1997, §12B) and Taniguchi (1983) has shown that MLE is

second-order eﬃcient. Some of the widely used algorithms for ARMA

likelihood evaluation are listed in Box and Luce˜no (1997, §12B). All of

these algorithms require O(n) ﬂops per likelihood evaluation. The

algorithm presented in §3 requires only O(1) ﬂops per evaluation and so is

much more eﬃcient for longer time series. This is especially important

when implementing the algorithm in a high level QPE. For example, one

may be interested in forecasting long time series in biomedical signal

processing using MatLab (Baura, 2002, §7.1). In §2 we discuss the AR(p)

case and in §3 the extension to the ARMA(p, q) case.

3

2. AR(p) Case

2.1. Exact Likelihood Function

It follows from Champernowne (1948, eq. 3.5) and Box, Jenkins and

Reinsel (1994, eqn. A7.4.10) that the log-likelihood function may be written

L(φ, µ, σ2

a) = −

n
2

log(σ2

a) −

1
2

log(gp) − S(φ, µ)/(2σ2

a),

(1)

where φ = (φ1, . . . , φp), gp = det(Γnσ−2

a ) = det(Γpσ−2

a ), Γn is the covariance

matrix of n successive observations,

S(φ, µ) = β′Dβ,

(2)

where D, the Champernowne matrix, is the (p + 1) × (p + 1) matrix with

(i, j)-entry,

Di,j = Dj,i = (zi − µ)(zj − µ) + . . . + (zn+1−j − µ)(zn+1−i − µ)

(3)

and β = (−1, φ). It should be pointed out that Champernowne (1948,

p.206) assumes n > 2p. However, it may be shown (McLeod and Zhang,

2007) that eqn. (2) is valid if and only if n ≥ 2p.

Maximizing over σ2

a, the concentrated log-likelihood may be written

Lc(φ, µ) = −

n
2

log(S(φ, µ)/n) −

1
2

log(gp).

As in Jones (1980), the parametrization using partial autocorrelations

(Barndorﬀ-Nielsen and Schou, 1973),

(φ1, . . . , φp) ←→ (ζ1, . . . , ζp)

4

(4)

(5)

may be used to constrain the optimization. In the reparameterized model,

p

gp =

(1 − ζ 2

j )−j.

Y
j=1

(6)

The Burg estimators are used as initial estimates since they are more

accurate than the Yule-Walker estimates in many situations (Percival and

Walden, 1993, p.414; Zhang and McLeod, 2006b). Like the Yule-Walker

estimates, the Burg estimates are always inside the admissible region and

may be eﬃciently computed using the Durbin-Levinsion recursion (Percival

and Walden, 1993, p.452). Modern QPEs provide various built-in

algorithms for nonlinear function optimization which may be used to obtain

the MLE of φ. Since the sample mean, ¯z = (z1 + · · · + zn)/n, is an

asymptotically fully eﬃcient estimate of µ, it is often used in place of the

MLE. This algorithm using the sample mean to estimate µ and then MLE

for the other parameters will be denoted by SampleMean in the following

sections.

If the sample mean is used, µ may be replaced by ¯z in (3) and so after

the initial evaluation, repeated evaluations of (4) require O(1) ﬂops, which

explains why the new algorithm is eﬃcient for long time series. Since it

practice p is considered ﬁxed, it is not included in the asymptotic ﬂop count.

2.2. Exact MLE for the Mean Parameter

The exact MLE for the mean may be obtained by simply optimizing the

log-likelihood function given in (4). However, this would then require O(n)

ﬂops per function evaluation. A more eﬃcient approach is now presented.

5

Assuming that φ is known, the exact MLE is given by,

ˆµ =

1′
nΓ−1
n z
nΓ−1
1′
n 1n

,

(7)

where 1n denotes the n dimensional column vector with all entries equal to

1, 1′

n denotes its transpose and z = (z1, . . . , zn). Since ˆµ does not depend on

a, we may assume without loss of generality that σ2
σ2

a = 1. Direct evaluation

of (7) using the exact inverse matrix derived by Siddiqui (1958) would

require O(n2) ﬂops. A more eﬃcient approach may be developed using the

inverse matrix result of Zinde-Walsh (1988). Zinde-Walsh (1988, eqn. 3.2)

showed that

n = ˙Γn − Ω,
Γ−1

(8)

where ˙Γn denotes the n × n matrix with (i, j)-entry given by γ(u)
γ(u)
k = Cov (ut, ut−k), ut = φ(B)at and Ω is a zero matrix except for p × p

i−j, where

submatrices in the upper-left and lower-right corners. The (i, j)-entry of

the submatrix of Ω in the upper-left corner is

Ωi,j =

p−|i−j|

X
k=min(i,j)

φkφk+|i−j|.

(9)

The matrix in the lower-right corner is just the transpose of the upper-left

corner submatrix. Using the above results it was found that,

nΓ−1
1′

n = 1′

nφ2(1) − (ǫ1, . . . , ǫp, 0, . . . , 0, ǫp, . . . , ǫ1)

−(κ1, . . . , κp, 0, . . . , 0, κp, . . . , κ1),

(10)

where φ(1) = 1 − φ1 − . . . − φp, ǫ = 1′

nΩ,

ǫ = (ǫ1, . . . , ǫp, 0, . . . , 0, ǫp, . . . , ǫ1)

(11)

6

and

κi =

i

X
k=1

γ(u)
k .

(12)

Using (10), ˆµ can now be evaluated in O(n) ﬂops. Note that this evaluation

will only typically be two or three times in the full MLE algorithm outlined

below.

An iterative algorithm, MeanMLE, is used for the simultaneous joint

MLE of (φ1, . . . , φp, µ),

Step 0 Set the maximum number of iterations, M ← 5. Set the iteration

counter, i ← 0. Set ˆµ(0) ← ¯z, where ¯z is the sample mean. Obtain
initial parameter values ˆφ(0)
set ˆφ(0)

k = 0, k = 1, . . . , p. Set ℓ0 = Lc( ˆφ(0), ˆµ(0)).

k , k = 1, . . . , p using the Burg algorithm or

Step 1 Obtain ˆφ(i+1)

k

, k = 1, . . . , p by numerically maximizing Lc(φ, ˆµ(i))

over φ. Set ℓi+1 = Lc( ˆφ(i+1), ˆµ(i)).

Step 2 Using ˆφ(i+1) evaluate ˆµ(i+1).

Step 3 Terminate when ℓi+1 has converged or i > M. Otherwise set

i ← i + 1 and return to Step 1 to perform the next iteration.

Convergence usually occurs in two or three iterations.

2.3. Champernowne Matrix Computation

Di,j has n − (i + 1) − (j + 1) terms so each term requires O(n) ﬂops. If

the sample mean is used, this computation only has to be done once, but if

the exact MLE for the mean is used, D must be computed several times. It

7

may be shown that D = C − E, where the (i, j)-entry of the matrix C may

be written, C|i−j|, where Ck = z1zk + . . . + zn−kzn. The (i, j)-entry for the

matrix E may be computed sequentially Ei+1,j+1 =

Ei,j + zizj + zn+1−izn+1−j, i < j. Using the above results reduces the ﬂop

count for the matrix D slightly.

3. ARMA Maximum Likelihood Estimation

Previous AR-approximation methods for ﬁtting MA(q) and

ARMA(p, q) were based on ﬁrst ﬁtting a suitable high-order autoregressive

approximation (Durbin, 1959; Parzen, 1969; Hannan and Rissanen, 1982;

Wahlberg, 1989; Choi, 1992 §4.1). The next step is to use the ﬁtted AR

model to estimate an MA(q) or ARMA(p, q) model. As noted by McClave

(1973), this approach can lead to biased estimates which have larger

mean-square error than the MLE.

Instead of directly ﬁtting an autoregressive model to the time series,

our new method is based on approximating the exact likelihood function for

the ARMA(p, q) model by the likelihood function for a suitable high-order

autoregression. The approximating autoregression of order r is determined

as the minimum mean-square error (MMSE) linear predictor of order r for

the ARMA(p, q) model, ϕ(B)(zt − µ) = at, where

ϕ(B) = 1 − ϕ1B − . . . − ϕrBr. By taking r suﬃciently large, an accurate

approximation to the exact ARMA(p, q) likelihood may be obtained. In

practice r = 30 is suﬃcient for many ARMA models as we will now show.

The Kullback-Leibler discrepancy may be used to choose a suitable r.

Letting Σφ,θ and Σϕ denote the covariance matrices for the ARMA(p, q)

8

and its AR(r) approximation, the Kullback-Leibler discrepancy may be

written (Ullah, 2002, eqn. 5),

I =

1
2

( tr Σφ,θΣ−1

ϕ − log |Σφ,θ|/|Σϕ| − n).

(13)

Figure 1 displays a plot of I in the case of an MA (1) model with θ1 = 0.9

and n = 200. It is seen that r = 30 works well even for this model with a

parameter near the non-invertible boundary. It appears that r = 30 is

adequate for many sorts of models occurring in applications although as the

parameters move very close to the non-invertible boundary, our

approximates requires larger r and fails entirely when the boundary is

reached. A Mathematica notebook to compute and plot the

Kullback-Leibler discrepancy for the ARMA(p, q) and its AR(r)

approximation is available (McLeod and Zhang, 2007).

[Figure 1 here]

In practice, as shown by simulation in §4.2, our method with r = 30 can

still be used even when there is a root on the boundary but the statistical

eﬃciency relative to existing exact MLE algorithms is reduced. Models

with a root on the non-invertible boundary usually indicate

over-diﬀerencing and may be avoided by reﬁtting with an alternative model

speciﬁcation (Zhang and McLeod, 2006a).

After a suitable r has been chosen, the ARMA likelihood may be

obtained from (4),

Lc(φ, θ, µ) = Lc(ϕ, µ),

(14)

where ϕ = (ϕ1, . . . , ϕr). Then Lc(φ, θ, µ) may be maximized using a built-in

optimization function. The algorithm given in §2.2 may be used to compute

9

the exact MLE for the mean by using this AR(r) approximation. As shown

in §4.3, this algorithm works as well as existing exact MLE algorithms for

the mean in ARMA(1, 1) models.

In Mathematica, MatLab and in R, nonlinear optimization functions

which can handle box constraints are available. In this case it is useful to

reparametrize the ARMA model as suggested by Monahan (1984) using the

transformation of Barndorﬀ-Nielsen and Schou (1973). Alternatively, if

only an unconstrained optimization function is available then a penalty

function approach may be used to constrain the parameters to the

admissible region. This penalty function approach has been used for many

years with the Powell (1964) algorithm in our MHTS Time Series Package

(McLeod and Hipel, 2007) for a wide variety of MLE problems in time

series analysis (Hipel and McLeod, 1994).

Usually it is most expedient to set the initial parameter estimates to

zero. In case of diﬃculty with convergence, initial estimates may be

obtained (Hannan and Rissanen, 1982) by ﬁtting a high order

autoregression to provide estimates of the innovations and then using linear

regression to estimate the parameters φ and θ. Experience suggests, as is

illustrated in §4.1, computing initial parameter estimates in the ARMA

case usually does not signiﬁcantly increase the speed and, in practice,

convergence is rarely an issue. In particular, convergence was obtained for

all models ﬁtted in §4 without diﬃculty.

A simple alternative to the MMSE linear predictor approximation is to

just use the truncated inverted form of model (Box, Jenkins and Reinsel,

1994, §4.2.3), π(B)(zt − µ) = at, where π(B) = 1 − π1B − . . . − πrBr. The

10

coeﬃcients πk, k = 1, . . . , r are obtained from

πk = φk + θ1πk−1 − . . . − θqπq − φk using boundary conditions

π0 = 1; πk = 0 if k < 0 and φk = 0 if k > p. When r is chosen large enough,

this approximates the MMSE predictor (Brockwell and Davis, 1991, §5).

However, for ﬁxed r there will always be parameter values in the admissible

ARMA(p, q) region for which ϕ(B) = 0 has roots outside the admissible

region for a causal-stationary AR(r). As shown in Table 1, the MMSE

predictor provides a much more accurate approximation in terms of the

Kullback-Leibler discrepancy. For these reasons the MMSE linear predictor

approximation is used.

[Table 1 here]

4. Illustrative Examples

The primary purpose of the illustrative examples presented in this

section is to demonstrate the usefulness of our algorithm and correctness of

our implementations in R and Mathematica. For this purpose, our

algorithm is also compared with existing MLE algorithms.

4.1. Timings

Timings for the algorithms described in §3 were obtained in

Mathematica and R on a Windows XP PC Pentium 4. The ARMA(1, 1)

model with φ1 = 0.9 and θ = 0.5 was selected as typical of order (1, 1)

models which might occur in practice. This model was simulated 25 times

for series of length n = 10k, k = 2, 3, . . . , 6 and the average time needed for

ﬁtting the model was determined. Timings were also compared to HR

11

(Hannan and Rissanen, 1982). The HR algorithm does not require

non-linear optimization and only requires linear least squares and residual

computation. The built-in least squares algorithms in Mathematica and R

were used. The eﬀects of initial values and MLE estimation of the mean

were also examined. The initial value options also examined were Origin,

XInit and HRInit corresponding respectively to initializing the nonlinear

optimization algorithm at 0.0 for all parameter values except the mean,

using exact known parameter values or using the Hannan-Rissanen

estimates as initial parameter settings. The algorithms for estimating the

mean, SampleMean and MeanMLE , are also compared. The MeanMLE

refers to the algorithm in §2.2 and SampleMean to just using ¯z as in §2.1.

In the R timings we also compared our algorithms with the built-in R

algorithms arima and arima0. These algorithms implement the state-space

Kalman ﬁlter algorithm given in Durbin and Koopman (2001). Further

details of this implementation (Ripley, 2002) indicate that this algorithm is

coded in C and then interfaced to R.

[Table 2 about here]

Comparing Origin with HR, our algorithm is much faster for larger n.

Although HR is faster than Origin for small n this is probably not

important since both algorithms are very fast and Origin which uses the

MLE method is preferred anyway – especially for small n. Since the

computing time required by HRInit does not include the initialization

times needed by HR itself, it is clear from Table 2, that if these are added

to HRInit, the initialization is normally not worthwhile in terms of

reducing computer time. Even with XInit when the exact initial values are

12

used, this only results in a modest improvement in speed. It is seen that in

terms of speed Mathematica outperforms R except when n is very large.

These timings also demonstrate that the Mathematica and R

implementations of our algorithms are suitable for even very large n. Given

the high-overhead imposed by the interpretive R language, the performance

of our algorithms is not unreasonable in practice even though in most cases

it is slower than arima and arima0.

4.2. Comparison with Durbin’s Algorithm

The statistical eﬃciency of Durbin, the algorithm of Durbin (1959) for

MA(q) estimation, is compared with SampleMean and exact MLE as

implemented in R in arima. For each parameter value

θ1 = 0, ±0.3, ±0.5, ±0.9, ±1, and for each series length n = 50, 100, 200, 400

one thousand time series were simulated. The empirical statistical eﬃciency

may be taken as the empirical MSE of the exact MLE algorithm divided by

the empirical MSE of SampleMean. Similarly, for the eﬃciency for the

Durbin algorithm. The variance of the estimated eﬃciency may be derived

using a Taylor series linearization. Details of this derivation as well as a

comparison with the bootstrap variance estimate are given in our online

supplement (McLeod and Zhang, 2007). In Figure 2, a trellis plot compares

these eﬃciencies. In each plot, the vertical line running through the plotted

point indicates a 95% conﬁdence interval for that eﬃciency. From this plot,

we see that SampleMean has eﬃciency very close to 1 except when the

parameter θ1 = ±1 when it is less eﬃcient and when θ1 = 0.9 it is

super-eﬃcient. In the super-eﬃciency cases, the eﬃciency approaches 1 as

13

n increases. The eﬃciency of Durbin is generally much less than

SampleMean but it approaches 1 as n gets larger provided the parameter is

not on the boundary.

The results shown in Figure 2 were replicated using our Mathematica

implementation of SampleMean and the exact MLE algorithm for the

MA (1) given in McLeod and Quenneville (2001).

[Figure 2 here]

4.3. Finite Sample Eﬃciency of the Sample Mean

If the parameters φ1, ..., φp, θ1, . . . , θq are known, the exact MLE for the

mean is given by eqn. (7). It is also the best linear unbiased estimate

BLUE. Another estimate of µ is simply the sample mean,

¯z = (z1 + . . . + zn)/n. The exact eﬃciency for ¯z vs. the BLUE for a series

of length n may be written,

E = n2/((1′

nΓ1n)(1′

nΓ−11n)).

(15)

In actual applications, the ARMA parameters are not known. In our

simulation study, we compare two MLE methods for estimating the mean.

The MLE methods are the MeanMLE algorithm of §2.2 and the R function

arima. With each of these MLE methods, the empirical eﬃciency of ¯z vs.

the MLE estimate of µ based on 103 simulations for series of lengths

n = 50, 100, 200 for the ARMA(1, 1) model at each parameter setting.

These empirical eﬃciencies are compared with the exact eﬃciency of ¯z vs.

BLUE given in eqn. (15) and all results are displayed in Table 3. Both

MeanMLE and arima are closely eﬃcient and there is general agreement

14

with the BLUE except when φ1 = 0 and θ1 = 0.9, 0.95. The simulation

experiment conﬁrms that MeanMLE is working correctly as expected and this

was its main purpose.

Since the sample mean is asymptotically eﬃcient in ARMA(p, q) models

(Brockwell and Davis, 1991, §7.1) it would be expected the eﬃciencies

would get closer to 1 as n increases and it is seen that in many cases this

holds. However it is surprising that even for n = 200, some sample

eﬃciencies are quite low for both MeanMLE and arima. This fact does not

previously appear to have been observed in the ARMA case although

Samarov and Taqqu (1988) found asymptotic ineﬃciency in a situation

which we will now discuss brieﬂy.

It should be noted that the ARMA models where the sample mean

eﬃciency is low have an extremely high frequency spectrum. The spectral

density and autocorrelation plots of the models in Table 3 are given in

McLeod and Zhang (2007). The models for which the sample mean is

ineﬃcient all have strong negative autocorrelation but are better

characterized in terms of the spectral density function. All models for

which the sample mean eﬃciency is less than 10% eﬃcient are all

characterized by a high frequency spectrum in which the high frequencies

are more than one hundred times the power of the low frequencies, that is,

the ratio of the spectral density evaluated at the Nyquist frequency divided

by the spectral density evaluated at the origin is larger than 100. This

situation may be called, infrared-catastrophe since it seems unrealistic in

any time series applications with actual scientiﬁc data.

Previously Samarov and Taqqu (1988) showed that asymptotically the

15

sample mean can be very ineﬃcient for hyperbolic decay time series

(McLeod, 1988) in the antipersistent case which corresponds to the

infrared-catastrophe case for these models. In all other hyperbolic-decay

cases, including the fractional ARMA case in eqn. (16), the asymptotic

eﬃciency is above 98% (Samarov and Taqqu, 1988, Table 1).

This simulation experiment was repeated using the SampleMean

algorithm implemented in Mathematica and similar results were obtained

(McLeod and Zhang, 2007).

[Table 3 here]

5. Conclusion

Mathematica and R packages that implement the ARMA maximum

likelihood algorithms described in this paper are available (McLeod and

Zhang, 2007). In addition simulation scripts to obtain the results reported

in this article are also available so the interested can easily reproduce

and/or extend our simulation results using either Mathematica or R.

Our algorithms are suitable for use with long time series. But the

principal advantage of our algorithms for maximum likelihood estimation of

ARMA models is that they may easily be implemented directly in

high-level QPEs. Using the R and Mathematica packages, it is relatively

straightforward to implement ARMA maximum likelihood in other high

level QPEs. QPEs such as MatLab and Strata as well as R and

Mathematica are becoming important in teaching statistical methods so it

is expected our algorithm will be useful teaching time series analysis in such

computing environments.

16

The AR-likelihood approximation technique of this paper could be used

for other types of linear time series models. It would be relatively

straightforward to extend the methods of this paper to multiplicative

seasonal and subset ARMA models. It may also be possible to develop an

extension to the vector ARMA models case. Another interesting family of

linear time series models are the fractional ARMA time series (Hipel and

McLeod, 1994, Ch. 11; Brockwell and Davis, §13.2) deﬁned by

φ(B)∇d(zt − µ) = θ(B)at,

(16)

where d ∈ (−0.5, 0.5). Figure 3 shows the Kullback-Leibler discrepancy, I

for the case of fractionally diﬀerenced white noise, p = 0 and q = 0, with

long-memory parameter d = 0.1, 0.2, 0.3, 0.4. When d ∈ (0, 0.2), r = 30 is

adequate but much higher orders may be needed for more strongly

persistent time series such as when d ≥ 0.4. In the case such strongly

persistent time series our suggested AR approximation may not be useful.

[Figure 3 here]

Acknowledgements Both authors were supported by NSERC Discovery

Grants. The authors would like to thank the referees for helpful comments

and their careful reading of our work.

17

Referemces

Barnard, G.A., Jenkins, G.M. and Winston, C.B. 1962. Likelihood

inference and time series. Journal of the Royal Statistical Society, B

125, 321–372.

Barndorﬀ-Nielsen, O. and Schou, G., 1973. On the parametrization of

autoregressive models by partial autocorrelations. Journal of

Multivariate Analysis 3, 408–419.

Baura, G.D., 2002. System Theory and Practical Applications of Biomedical

Signals. Wiley, New York.

Box, G.E.P., Jenkins, G.M. and Reinsel, G.C., 1994. Time Series Analysis:

Forecasting and Control . 3rd Ed., Holden-Day, San Francisco.

Box, G.E.P. and Luce˜no, A., 1997. Statistical Control by Monitoring and

Feedback Adjustment. Wiley, New York.

Brockwell, P.J. and Davis, R.A., 1991. Time Series: Theory and Methods.

(2nd edn.) Springer-Verlag, New York.

Champernowne, D.G., 1948. Sampling theory applied to autoregressive

sequences. Journal of the Royal Statistical Society B 10, 204–242.

Choi, B., 1992. ARMA Model Identiﬁcation. Springer-Verlag, New York.

Durbin, J., 1959. Eﬃcient estimation of parameters in moving-average

models. Biometrika 46, 306–316.

Durbin, J. and Koopman, S.J., 2001. Time Series Analysis by State Space

Methods. Oxford University Press, Oxford.

Fisher, R.A. 1973. Statistical methods and scientiﬁc inference. Hafner

Press, New York.

Hannan, E.J. and Rissanen, J., 1982. Recursive estimation of mixed

18

autoregressive-moving average order. Biometrika 69, 81–94.

Hipel, K.W. and McLeod, A.I., 1994. Time Series Modelling of Water

Resources and Environmental Systems. Elsevier, Amesterdam.

Reprint, www http://www.stats.uwo.ca/faculty/aim/1994Book/.

Jones, R.H., 1980. Maximum likelihood ﬁtting of ARMA models to time

series with missing observations. Technometrics 22, 389–395.

McClave, E.J., 1973. On the bias of AR approximation to moving averages.

Biometrika 60, 599-605.

McLeod, A.I., 1998. Hyperbolic decay time series. Journal of Time Series

Analysis 19, 473–484.

McLeod, A.I. and Quenneville, B., 2001. Mean likelihood estimators.

Statistics and Computing 11, 57–65.

McLeod, A.I. and Hipel, K.W., 2007. McLeod-Hipel Time Series Package,

(www http://www.stats.uwo.ca/faculty/aim/epubs/mhts/).

McLeod, A.I. and Zhang, Y., 2007. Online supplements to “Faster ARMA

Maximum Likelihood Estimation”, www

(http://www.stats.uwo.ca/faculty/aim/2007/faster/).

Monahan, J.F., 1984. A note on enforcing stationarity in

autoregressive-moving average models. Biometrika 71, 403–404.

Parzen, E., 1969. Multiple time series modeling. In Multivariate Analysis II

ed. P. Krishnaiah, 389-409. Academic Press, New York.

Percival, D.B. and Walden, A.T., 1993. Spectral Analysis for Physical

Applications. Cambridge University Press, Cambridge.

Powell, M.J.D., 1964. An eﬃcient method for ﬁnding the minimum of a

function of several variables without calculating derivatives. Computer

19

Journal 7, 155–162.

Ripley, B.D., 2002. Time Series in R. R News 2, 2–7.

Samarov, A. and Taqqu, M., 1988. On the eﬃciency of the sample mean in

long memory noise. Journal of Time Series Analysis 9, 191–200.

Siddiqui, M.M., 1958. On the inversion of the sample covariance matrix in

a stationary autoregressive process. Annals of Mathematical Statistics

29, 585–588.

Taniguchi, M., 1983. On the second order asymptotic eﬃciency of

estimators of gaussian ARMA processes. The Annals of Statistics 11,

157–169.

Ullah, A., 2002. Use of entropy and divergence measures for evaluating

econometric approximations and inference. Journal of Econometrics

107, 313–326.

Wahlberg, B., 1989. Estimation of autoregressive moving-average models

via high-order autoregressive approximations. Journal of Time Series

Analysis 10, 283–299.

Zhang, Y. and McLeod, A.I., 2006a. Fitting MA(q) models in the closed

invertible region. Statistics and Probablity Letters 76, 1331–1334.

Zhang, Y. and McLeod, A.I., 2006b. Computer algebra derivation of the

bias of Burg estimators. Journal of Time Series Analysis 27, 157–165.

Zinde-Walsh, V., 1988. Some exact formulae for autoregressive moving

average processes. Econometric Theory 4, 384–402.

20

Table 1: Kullback-Leibler discrepancy for AR(r) approximation to a MA (1)
with θ1 = 0.95 and n = 200 using the MMSE approximation and the approx-
imation based on truncating the inverted form of the model.

n MMSE Truncated
10
20
30
40
50

31.20
10.70
3.77
1.42
0.62

4.99
1.23
0.38
0.12
0.04

21

Table 2: Average CPU time in seconds with R and Mathematica for ﬁtting
the ARMA(1, 1) model with φ1 = 0.9 and θ = 0.5 using SampleMean, Mean-
MLE and the Hannan-Rissanen estimator. In the R case, built-in functions
arima and arima0 are also used. Twenty-ﬁve replications for series of length
n = 10k, k = 2, 3, . . . , 6 were done. The case where the mean is estimated by
the sample average is compared with the MLE for each algorithm. The eﬀect
of initial parameter settings is also examined. The settings Origin, XInit and
HRInit correspond to setting (φ1, θ1) equal to (0, 0), (0.9, 0.5) or using the
estimator of Hannan-Rissanen respectively.

method 102

n
104

103
Timings in R

105

106

SampleMean

Origin
HR
XInit
HRInit
arima
arima0

Origin
XInit
HRInit
arima
arima0

Origin
HR
XInit
HRInit

Origin
XInit
HRInit

0.47
0.24
0.32
0.29
0.02
0.01

1.00
0.90
0.80
0.05
0.03

0.47
1.05
0.27
0.27
0.04
0.01

0.85
0.68
0.58
0.19
0.03

0.70
10.2
0.46
0.57
0.24
0.02
MeanMLE
1.03
0.82
0.89
0.74
0.07

2.13
92.0
1.40
1.70
1.85
0.18

3.14
2.80
2.91
3.94
0.88

8.63
902.
6.81
7.78
13.6
1.78

16.70
17.15
16.59
32.54
13.62

Timings in Mathematica

SampleMean

0.26
0.01
0.27
0.27

0.75
0.78
0.75

0.29
0.05
0.30
0.29

0.89
0.90
0.89

0.36
0.54
0.36
0.35
MeanMLE
1.14
1.14
1.10

0.90
5.00
0.89
0.88

4.10
4.11
4.10

4.97
47.4
4.96
4.99

30.46
30.71
30.49

22

Table 3: Empirical eﬃciency of the sample mean vs. three other methods:
BLUE, MeanMLE and arima. Each empirical eﬃciency is based on 1000
simulations for ARMA(1, 1) models with n = 50, 100, 200.

φ1

algorithm n

−0.95 −0.9 −0.5

0.

0.5

0.9

θ1

−0.95 BLUE
50
−0.95 MeanMLE 50
−0.95
50
arima
−0.95 BLUE
100
−0.95 MeanMLE 100
−0.95
100
arima
−0.95 BLUE
200
−0.95 MeanMLE 200
−0.95
200
arima
BLUE
−0.9
50
−0.9 MeanMLE 50
−0.9
50
arima
BLUE
−0.9
100
−0.9 MeanMLE 100
−0.9
100
arima
BLUE
−0.9
200
−0.9 MeanMLE 200
−0.9
200
arima
BLUE
−0.5
50
−0.5 MeanMLE 50
−0.5
50
arima
BLUE
−0.5
100
−0.5 MeanMLE 100
−0.5
100
arima
BLUE
−0.5
200
−0.5 MeanMLE 200
−0.5
200
arima

1.00
1.00
1.02
1.00
1.00
1.01
1.00
1.00
1.01
1.00
1.00
1.02
1.00
1.00
1.01
1.00
1.00
1.01
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00

0.97
0.99
1.00
0.98
1.00
1.00
0.99
0.99
0.99
0.99
1.00
1.01
0.99
1.00
1.00
1.00
1.00
1.00
1.00
1.01
1.03
1.00
1.00
1.01
1.00
1.00
1.00

0.75
0.74
0.74
0.85
0.87
0.87
0.92
0.91
0.91
0.86
0.85
0.86
0.92
0.95
0.95
0.96
0.95
0.95
0.99
1.00
1.00
0.99
1.00
1.00
1.00
1.00
1.00

0.25
0.39
0.40
0.38
0.58
0.58
0.54
0.67
0.67
0.39
0.59
0.59
0.56
0.74
0.74
0.71
0.81
0.81
0.83
0.96
0.96
0.91
0.95
0.95
0.95
0.96
0.96

0.01
0.02
0.02
0.02
0.04
0.04
0.03
0.06
0.06
0.02
0.05
0.05
0.03
0.07
0.07
0.06
0.11
0.12
0.13
0.26
0.26
0.19
0.31
0.32
0.30
0.43
0.44

1.00
1.00
1.02
1.00
1.00
1.01
1.00
1.00
1.00
1.00
1.01
1.01
1.00
1.00
1.01
1.00
1.00
1.01
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00

23

0.95

0.01
0.01
0.01
0.01
0.01
0.01
0.01
0.02
0.02
0.01
0.02
0.02
0.01
0.02
0.02
0.02
0.03
0.03
0.06
0.13
0.13
0.07
0.13
0.13
0.10
0.17
0.17

φ1

algorithm n

−0.95 −0.9 −0.5

0.

0.5

0.9

θ1

arima
BLUE

arima
BLUE

BLUE
0.
50
MeanMLE 50
0.
0.
50
arima
BLUE
0.
100
MeanMLE 100
0.
0.
100
arima
BLUE
0.
200
MeanMLE 200
0.
0.
200
arima
BLUE
0.5
50
0.5 MeanMLE 50
0.5
50
0.5
100
0.5 MeanMLE 100
0.5
100
0.5
200
0.5 MeanMLE 200
0.5
200
arima
BLUE
0.9
50
0.9 MeanMLE 50
0.9
50
0.9
100
0.9 MeanMLE 100
0.9
100
0.9
200
0.9 MeanMLE 200
0.9
200
arima
0.95 BLUE
50
0.95 MeanMLE 50
0.95
50
arima
0.95 BLUE
100
0.95 MeanMLE 100
0.95
100
arima
0.95 BLUE
200
0.95 MeanMLE 200
0.95
200

arima
BLUE

arima
BLUE

arima

0.99
1.02
1.03
1.00
1.01
1.01
1.00
1.00
1.00
0.97
0.94
0.94
0.99
0.96
0.96
0.99
0.98
0.98
0.89
0.81
0.80
0.93
0.85
0.85
0.96
0.92
0.92
0.88
0.78
0.77
0.89
0.80
0.80
0.93
0.87
0.87

0.99
1.02
1.03
1.00
1.01
1.01
1.00
1.00
1.00
0.97
0.94
0.94
0.99
0.96
0.96
0.99
0.98
0.98
0.89
0.81
0.80
0.93
0.85
0.85
0.96
0.92
0.92
0.88
0.78
0.77
0.89
0.80
0.80
0.93
0.87
0.87

1.00
1.02
1.03
1.00
1.01
1.01
1.00
1.00
1.00
0.98
0.96
0.96
0.99
0.97
0.97
0.99
0.99
0.99
0.90
0.82
0.81
0.93
0.86
0.86
0.96
0.92
0.92
0.88
0.79
0.79
0.89
0.81
0.81
0.93
0.87
0.87

1.00
1.03
1.02
1.00
1.00
1.01
1.00
1.00
1.00
0.99
1.00
1.00
0.99
0.99
0.99
1.00
1.00
1.00
0.91
0.94
1.58
0.94
0.93
0.93
0.96
0.95
0.95
0.89
0.91
0.91
0.90
0.89
0.89
0.93
0.92
0.92

0.96
1.03
1.03
0.98
1.01
1.01
0.99
1.00
1.00
1.00
1.03
1.03
1.00
1.02
1.02
1.00
1.01
1.01
0.93
0.91
0.91
0.95
0.93
0.93
0.97
0.96
0.96
0.91
0.86
0.91
0.91
0.87
0.87
0.94
0.91
0.91

0.34
1.03
1.03
0.44
1.01
1.01
0.58
1.00
1.00
0.67
0.71
0.74
0.75
0.72
0.74
0.84
0.82
0.84
1.00
1.05
1.08
1.00
1.10
1.09
1.00
1.06
1.04
0.99
1.02
1.01
0.98
1.08
1.08
0.98
1.11
1.13

0.95

0.18
1.03
1.03
0.19
1.01
1.01
0.26
1.00
1.00
0.44
0.50
0.55
0.44
0.43
0.44
0.54
0.55
0.56
0.97
1.03
1.02
0.96
1.06
1.04
0.96
0.96
0.97
1.00
1.03
1.01
1.00
1.18
1.14
1.00
1.32
1.29

1.2

1

0.8

I

0.6

0.4

0.2

0

10

20

30
r

40

50

Figure 1:
MA (1) with θ1 = 0.9 and n = 200.

Kullback-Leibler discrepancy for AR(r) approximation to a

25

i

y
c
n
e
c
i
f
f
e
e
v
i
t
a
e
r

l

400
Durbin

200
Durbin

100
Durbin

50
Durbin

−1.0

−0.5

0.0

0.5

1.0

400
SampleMean

200
SampleMean

100
SampleMean

50
SampleMean

2.5
2.0
1.5
1.0
0.5
0.0

2.5
2.0
1.5
1.0
0.5
0.0

2.5
2.0
1.5
1.0
0.5
0.0

2.5
2.0
1.5
1.0
0.5
0.0

−1.0

−0.5

0.0

0.5

1.0

q 1

Figure 2: The vertical lines show the length of the 95% conﬁdence interval
for the statistical eﬃciency of SampleMean and Durbin vs. the MLE based
on 103 simulations.

26

 
0.9

I

0.1

10

0.9

I

0.1

10

d =

0.1

d =

0.2

0.9

I

0.1

10

50

30
r

50

30
r

d =

0.3

d =

0.4

0.9

I

0.1

10

50

30
r

50

30
r

Figure 3: Kullback-Leibler discrepancy for AR(r) approximation to fraction-
ally diﬀerenced white noise, ∇dzt = at for d = 0.1, 0.2, 0.3, 0.4 and n = 200.

27

