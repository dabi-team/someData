Improving Fluency of Non-Autoregressive Machine Translation

Zdenˇek Kasner and Jindˇrich Libovick´y and Jindˇrich Helcl
Charles University, Faculty of Mathematics and Physics,
Institute of Formal and Applied Linguistics,
Malostransk´e n´amˇest´ı 25, 118 00 Prague, Czech Republic
{kasner, libovicky, helcl}@ufal.mff.cuni.cz

0
2
0
2

r
p
A
7

]
L
C
.
s
c
[

1
v
7
2
2
3
0
.
4
0
0
2
:
v
i
X
r
a

Abstract

Non-autoregressive (nAR) models for ma-
chine translation (MT) manifest superior de-
coding speed when compared to autoregres-
sive (AR) models, at the expense of impaired
ﬂuency of their outputs. We improve the ﬂu-
ency of a nAR model with connectionist tem-
poral classiﬁcation (CTC) by employing addi-
tional features in the scoring model used dur-
ing beam search decoding. Since the beam
search decoding in our model only requires
to run the network in a single forward pass,
the decoding speed is still notably higher than
in standard AR models. We train models for
three language pairs: German, Czech, and Ro-
manian from and into English. The results
show that our proposed models can be more
efﬁcient in terms of decoding speed and still
achieve a competitive BLEU score relative to
AR models.

1 Introduction

One of the challenges that the research commu-
nity faces today is improving the latency of neural
machine translation (NMT) models. The decoders
in modern NMT models operate autoregressively,
which means that the target sentence is generated
in steps from left to right (Bahdanau et al., 2015;
Vaswani et al., 2017). In each step, a token is gen-
erated and it is supplied as the input for the next
step.

Recently, nAR models for NMT tackled this is-
sue by reformulating translation as sequence la-
beling. As long as the model and the data ﬁt
in a GPU memory, all computation steps can be
done in parallel (Gu et al., 2018; Lee et al., 2018;
Libovick´y and Helcl, 2018; Ghazvininejad et al.,
2019). However, such models suffer from less ﬂu-
ent outputs.

In phrase-based statistical machine translation
(SMT; Koehn, 2009), the translation ﬂuency is

handled by a language model component, which
is responsible for arranging the phrases selected
by the decoder into a coherent sentence.
In AR
NMT, there is no external language model. The
decoder part of the neural model plays the role of
a conditional language model, which estimates the
probability of the translation given the source sen-
tence signal as processed by the encoder part.

In automatic speech recognition (ASR), Graves
and Jaitly (2014) proposed a beam search algo-
rithm which combines an n-gram language model
trained using CTC
with scores from a model
(Graves et al., 2006).

In this paper, we adopt and generalize this ap-
proach for nAR NMT by extending a CTC-based
model by Libovick´y and Helcl (2018). We experi-
ment with these models on six language pairs and
we ﬁnd that the generalized decoding algorithm
helps narrowing the performance gap between the
CTC-based and the standard AR models.

2 Non-autoregressive MT with CTC

Non-autoregressive models for MT formulate the
translation problem as sequence labeling. The
states of the ﬁnal decoder layer are independently
labeled with target sentence tokens. The models
can parallelize all steps of the computation and
thus reduce the decoding time substantially. The
nAR models were enabled by the invention of the
self-attentive Transformer model (Vaswani et al.,
2017), which allows arbitrary reordering of the
states in each layer. Most of the nAR models need
a prior estimate of the sentence length, either ex-
plicitly (Lee et al., 2018) or via a specialized fer-
tility model (Gu et al., 2018) and rely on the atten-
tion mechanism for re-ordering.

We base our work on an alternative approach
that does not depend on the target length estima-
Instead, it constrains the upper bound of
tion.

 
 
 
 
 
 
Algorithm 1 Beam Search Algorithm with CTC
1: B ← {∅}
2: for step i = 1 . . . k · Tx do
3:

⊲ Hypothesis → CTC score

⊲ Beam

H ← ∅
W ← 2n-best tokens in step i
for hypothesis h ∈ B do
for token w ∈ W do

s ← Pi(w) · P(h) ⊲ derivation score
H[h + w] ← H[h + w] + s

B ← select nbest(H, n)

4:
5:

6:

7:

8:

9:

10: return B

the target sentence length to the source sentence
length multiplied by a ﬁxed number k and uses
CTC to compute the training loss (Libovick´y and
Helcl, 2018).

The architecture consists of three components:
encoder, state splitter, and decoder. The encoder
is the same as in the Transformer model. The
state splitter takes each state from the ﬁnal encoder
layer and projects it into k states of the original di-
mension, making the sequence k times longer. The
decoder consists of additional Transformer layers
which attend to both encoder and state splitter out-
puts.

CTC enables the model to generate variable-
length sequences using a special blank symbol that
is included in the vocabulary. The resulting train-
ing loss is a sum of cross entropy of all possible
interleavings of the reference sequence with the
blank symbols. Even though enumerating all the
combinations is intractable, the cross-entropy sum
can be efﬁciently computed using a dynamic pro-
gramming forward-backward algorithm.

inference time,

Since each token can be decoded independently
of other tokens at
the model
reaches a signiﬁcant speedup over the AR models.
However, this speedup is achieved at the expense
of the translation quality which manifests mostly
in the reduced ﬂuency.

3 Proposed Method

We tackle the reduced ﬂuency problem using beam
search and employing additional features in its
scoring model. Our approach is inspired by sta-
tistical MT and ASR.

3.1 Beam Search with CTC

highest probability in each step independently,
beam search operates sequentially. However, the
speedup gained from the parallelization is pre-
served because the output probability distributions
are still conditionally independent and thus can be
computed in a single pass through the network – as
opposed to the AR models, which need to re-run
the entire stack of decoder layers every step.

The beam search algorithm for the CTC-based
model (Graves and Jaitly, 2014) is shown in Algo-
rithm 1. Unlike standard beam search in NMT, the
algorithm needs to deal with the issue that a single
hypothesis may have various derivations, depend-
ing on the positions of the blank symbols. The
score of a single derivation is the product of the
conditionally independent probabilities of the out-
put tokens (line 7).

The beam search score of a hypothesis is then
the sum of the scores of its derivations formed in
the current beam search step (line 8).

3.2 Scoring Model

For selecting n best hypotheses (line 9 in Algo-
rithm 1), we employ a linear model to compute
the score:

score = log P(y|x) + w · Φ(y)

(1)

where P(y|x) is the CTC score of the generated
sentence y given a source sentence x, Φ is a fea-
ture function of y and w is a trainable feature
weight vector.

We use structured perceptron for beam search
to learn the feature weights (Huang et al., 2012).
During training, we run the beam search algorithm
and if the reference translation falls off the beam,
we apply the perceptron update rule:

w ← w + α (Φ(y) − Φ(ˆy))

(2)

where α is the learning rate, Φ(y) are the feature
values of the preﬁx of the reference translation in
the given time step, and Φ(ˆy) are the feature val-
ues of the highest-scoring hypothesis in the beam.
Alternatively, we found that applying the percep-
tron update rule multiple times with all hypotheses
that scored higher than the reference leads to faster
convergence. In order to stabilize the training, we
do not train the weight of the CTC score and set it
to 1.

Unlike greedy decoding, which can be per-
formed in parallel by selecting tokens with the

In the following paragraphs, we describe the fea-

tures Φ used within our beam search algorithm.

Method

German WMT15

Romanian WMT16

Czech WMT18

en → de

de → en

en → ro

ro → en

en → cs

cs → en

Decoding
time [ms]

Non-autoregressive
Transformer, greedy
Transformer, beam 5

Ours, beam 1
Ours, beam 5
Ours, beam 10
Ours, beam 20

21.67
29.84
30.23

22.68
25.50
25.93
26.03

25.57
32.62
33.43

26.44
29.45
30.05
30.15

19.88
25.89
26.46

19.74
22.46
23.33
24.11

28.99
33.54
34.06

29.65
33.01
33.29
33.51

16.27
21.57
22.20

16.98
19.31
19.47
19.58

17.63
27.89
28.49

18.78
23.33
23.95
24.32

233
1664
3848

337
408
526
1097

Table 1: Quantitative results of the models in terms of BLEU score and average decoding times per sentence
in milliseconds. Results on WMT14 English-German translation and results without back-translation are in the
Appendix.

Language Model. The main component improv-
ing the ﬂuency is a language model (LM). For efﬁ-
ciency, we use an n-gram LM. Since the hypothe-
ses contain blank symbols, the beam may consist
of hypotheses of different lengths. Because shorter
sequences are favored by the LM, we divide the
log-probability of each hypothesis by its length in
order to normalize the scores.

Blank/non-blank symbols. To guide the decod-
ing towards sentences of correct length, we com-
pute the ratio of blank vs. non-blank symbols as
follows:

max

0,

(cid:18)

# blanks
# non-blanks

− δ

(cid:19)

where δ is a hyperparameter that thresholds the pe-
nalization for too high blank/non-blank symbol ra-
tio. Based on the distribution properties of the ra-
tio, we use δ = 4.

Trailing blank symbols. We observed that the
outputs produced by the CTC-based model tend to
be too short. To prevent that, we count the trailing
blank symbols:

max (0, # trailing blanks − source length) .

4 Experiments

We perform experiments on three language pairs
in both directions: English-Romanian, English-
German, and English-Czech.

For training the base NMT models, we use
WMT parallel data,1 which consists of 0.6M sen-
tences for English-Romanian, 4.5M sentences for

1http://statmt.org/wmt19/translation-task.html

English-German, and 57M sentences for English-
Czech.

Further, we use the WMT monolingual data:
20M sentences for English, German and Czech
and 2.2M sentences for Romanian for training the
LM and for back-translation.

We preprocess all data using SentencePiece2
(Kudo and Richardson, 2018). We train the
SentencePiece models with a vocabulary size of
50,000.

We implement

the proposed architecture us-
ing Neural Monkey3 (Helcl and Libovick´y, 2017).
The parameters we used for the training are listed
in Appendix A. We will release the code upon pub-
lication.

We used the AR baselines trained on the parallel
data for generating back-translated synthetic train-
ing data (Sennrich et al., 2016). When training
on back-translated data, authentic parallel data are
upsampled to match the size of the back-translated
data. We thus train our ﬁnal models using the mix
of authentic and backtranslated data, so both AR
baselines and the proposed models use the same
amount of data for training. If we only used the
parallel data for training the neural models and
kept the monolingual data only for the language
model, the proposed model would have beneﬁted
from having access to more data than the AR base-
lines.

We train a 5-gram KenLM model (Heaﬁeld,
2011) on the monolignual data tokenized using
the same SentencePiece vocabulary as the parallel
data.

For the perceptron training, we split the valida-

2https://github.com/google/sentencepiece
3https://github.com/ufal/neuralmonkey

s
d
n
o
c
e
s
n
i

e
m

i
t

g
n
i
d
o
c
e
d

3

2

1

0

Beam Size

1

5

10

20

c + l + r + t
c + l + r
c + l
c

22.68
22.21
22.05
21.67

25.50
24.92
24.64
22.06

25.93
25.12
24.77
22.13

26.03
25.35
25.12
22.17

CTC
ours
autoreg.

0

10

20

30

40

50

# source subwords

Figure 1: Comparison of the CPU decoding time of the
autoregressive (AR), non-autoregressive (nAR) Trans-
former models and the proposed method with beam
size of 10.

tion data for each language pair in halves and use
one half as the training set and the second half as a
held-out set. We use the score on the held-out set
during the perceptron training as an early-stopping
criterion. The scoring model is initialized with
zero weights for all features and a ﬁxed weight of
1 for the CTC score.

5 Results

We evaluate our models on the standard WMT
test sets that were previously used for evalu-
ation of nAR NMT. We use newstest2015
for English-German, newstest2016 for English-
Romanian, and newstest2018 for English-Czech
(Bojar et al., 2015, 2016, 2018). We compute
the BLEU scores (Papineni et al., 2002) as im-
plemented in SacreBLEU4 (Post, 2018). We also
measure the average decoding time for a single
sentence.

Table 1 shows the measured quantitative re-
sults of the experiments. We observe that the
beam search greatly improves the translation qual-
the CTC-based nAR models (“Non-
ity over
“Ours”). Additionally, we
autoregressive” vs.
have control over the speed/quality trade-off by ei-
ther lowering or increasing the beam size.

Increasing the beam size from 1 to 5 systemat-
ically increases the translation quality by at least
3 BLEU points. Decoding with a beam size of 20
matches the quality of greedy autoregressive de-
coding while maintaining 1.5× speedup.

Figure 1 plots the time required to translate a

4https://github.com/mjpost/sacreBLEU

Table 2: BLEU scores for English-to-German transla-
tion for different beam sizes and feature sets: CTC
score (c), language model (l), ratio of the blank sym-
bols (r), and the number of trailing blank symbols (t).

sentence with respect to its length. As expected,
beam search decoding is more time-consuming
than the CTC-based labeling (greedy). However,
our method is still substantially faster than the AR
model, especially for longer sentences.

Table 2 shows how features used in the scoring
model contribute to the BLEU score. We can see
that combining the features is beneﬁcial and that
the improvement is substantial with larger beam
sizes. The feature weights were trained separately
for each beam size.

Our cursory manual evaluation indicates that ad-
ditional features help to tackle the most signiﬁcant
problems of nAR NMT – repeated or malformed
words and too short sentences (see Appendix C for
examples).

6 Related Work

The earliest work on nAR translation includes
work by Gu et al. (2018) and Lee et al. (2018)
which are the closest to our model beside our base-
line. Unlike our approach, they do not include
state splitting. Gu et al. (2018) used a latent fertil-
ity model to copy a sequence of embeddings which
is then used for the target sentence generation. Lee
et al. (2018) use two decoders. The ﬁrst decoder
generates a candidate translation, which is then it-
eratively reﬁned by the second decoder a denois-
ing auto-encoder or a masked LM (Ghazvininejad
et al., 2019).

Junczys-Dowmunt et al. (2018) exploit the au-
toregressive architectures (Bahdanau et al., 2015;
Vaswani et al., 2017) and try to optimize the de-
coding speed. Using model quantization and state
memoization they achieve a two-times speedup.

7 Conclusions

We introduced a MT model with beam search that
combines nAR CTC-based NMT model with an

n-gram LM and other features.

We performed experiments on six language
pairs and evaluated the models on the standard
WMT sets. Our approach narrows the quality gap
between the nAR and AR models while still main-
taining a substantial speedup.

The experiments show that the main beneﬁt of
the proposed approach is the opportunity to bal-
ance the trade-off between translation quality and
translation speed. The autoregressive models are
still superior in translation quality for most of
the language pairs, even though by a narrow mar-
gin. In contrast, the non-autoregressive models are
very fast, but often lack in translation quality. Our
approach enhances constant-time neural network
run with a fast beam search utilizing a scoring
model to improve the translation quality. By al-
tering the beam size, we can adjust the speed and
the quality ratio to achieve acceptable results both
in terms of speed and translation quality.

Acknowledgements

This research has been supported by the from
the European Union’s Horizon 2020 research
and innovation programme under grant agreement
No. 825303 (Bergamot), Czech Science Foundtion
grant No. 19-26934X (NEUREM3), and Charles
University grant No. 976518, and has been us-
ing language resources distributed by the LIN-
DAT/CLARIN project of the Ministry of Educa-
tion, Youth and Sports of the Czech Republic
(LM2015071). This research was partially sup-
ported by SVV project number 260 453.

References

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
In 3rd Inter-
learning to align and translate.
national Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA.

Ondˇrej Bojar, Rajen Chatterjee, Christian Federmann,
Yvette Graham, Barry Haddow, Matthias Huck, An-
tonio Yepes, Philipp Koehn, Varvara Logacheva,
Christof Monz, Matteo Negri, Aurelie N´ev´eol, Mar-
iana Neves, Martin Popel, Matt Post, Raphael Ru-
bino, Carolina Scarton, Lucia Specia, Marco Turchi,
Karin Verspoor, and Marcos Zampieri. 2016. Find-
ings of the 2016 conference on machine translation
(WMT16). In Proceedings of the First Conference
on Machine Translation, Volume 2: Shared Task Pa-
pers, volume 2, pages 131–198, Berlin, Germany.
Association for Computational Linguistics.

Ondˇrej Bojar, Rajen Chatterjee, Christian Federmann,
Barry Haddow, Matthias Huck, Chris Hokamp,
Philipp Koehn, Varvara Logacheva, Christof Monz,
Matteo Negri, Matt Post, Carolina Scarton, Lucia
Specia, and Marco Turchi. 2015. Findings of the
2015 workshop on statistical machine translation. In
Proceedings of the Tenth Workshop on Statistical
Machine Translation, pages 1–46, Lisbon, Portugal.
Association for Computational Linguistics.

Ondˇrej Bojar, Christian Federmann, Mark Fishel,
Yvette Graham, Barry Haddow, Matthias Huck,
Philipp Koehn, and Christof Monz. 2018. Find-
ings of the 2018 conference on machine translation
In Proceedings of the Third Confer-
(WMT18).
ence on Machine Translation, pages 272–307, Brus-
sels, Belgium. Association for Computational Lin-
guistics.

Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and
Luke Zettlemoyer. 2019. Mask-predict: Parallel de-
coding of conditional masked language models. In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP), pages 6111–
6120, Hong Kong, China. Association for Computa-
tional Linguistics.

Alex Graves, Santiago Fern´andez, Faustino Gomez,
and J¨urgen Schmidhuber. 2006.
Connectionist
temporal classiﬁcation:
labelling unsegmented se-
quence data with recurrent neural networks. In Pro-
ceedings of the 23rd International Conference on
Machine Learning, pages 369–376, Pittsburgh, PA,
USA. ACM.

Alex Graves and Navdeep Jaitly. 2014. Towards end–
to-end speech recognition with recurrent neural net-
In International Conference on Machine
works.
Learning, pages 1764–1772, Bejing, China. PMLR.

Jiatao Gu, James Bradbury, Caiming Xiong, Victor
O. K. Li, and Richard Socher. 2018. Non-autore-
In 6th Inter-
gressive neural machine translation.
national Conference on Learning Representations,
ICLR 2018, Vancouver, BC, Canada.

Kenneth Heaﬁeld. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
187–197, Edinburgh, United Kingdom. Association
for Computational Linguistics.

Jindˇrich Helcl and Jindˇrich Libovick´y. 2017. Neural
Monkey: An open-source tool for sequence learn-
ing. The Prague Bulletin of Mathematical Linguis-
tics, (107):5–17.

Liang Huang, Suphan Fayong, and Yang Guo. 2012.
In Pro-
Structured perceptron with inexact search.
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
142–151, Montr´eal, Canada. Association for Com-
putational Linguistics.

Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In I. Guyon, U. V. Luxburg, S. Bengio,
H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-
nett, editors, Advances in Neural Information Pro-
cessing Systems 30, pages 6000–6010. Curran Asso-
ciates, Inc.

Marcin Junczys-Dowmunt, Kenneth Heaﬁeld, Hieu
Hoang, Roman Grundkiewicz, and Anthony Aue.
2018. Marian: Cost-effective high-quality neural
machine translation in C++. In Proceedings of the
2nd Workshop on Neural Machine Translation and
Generation, pages 129–135, Melbourne, Australia.
Association for Computational Linguistics.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
In 3rd Inter-
method for stochastic optimization.
national Conference on Learning Representations,
ICLR 2015, Conference Track Proceedings, San
Diego, CA, USA.

Philipp Koehn. 2009. Statistical machine translation.

Cambridge University Press, Cambridge, UK.

Taku Kudo and John Richardson. 2018. SentencePiece:
A simple and language independent subword tok-
enizer and detokenizer for neural text processing. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing: System
Demonstrations, pages 66–71, Brussels, Belgium.
Association for Computational Linguistics.

Jason Lee, Elman Mansimov, and Kyunghyun Cho.
2018. Deterministic non-autoregressive neural se-
In Pro-
quence modeling by iterative reﬁnement.
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1173–
1182, Brussels, Belgium. Association for Computa-
tional Linguistics.

Jindˇrich Libovick´y and Jindˇrich Helcl. 2018. End–
to-end non-autoregressive neural machine transla-
tion with connectionist temporal classiﬁcation.
In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, pages
3016–3021, Brussels, Belgium. Association for
Computational Linguistics.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
In Proceedings of
uation of machine translation.
40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318, Philadelphia,
PA, USA. Association for Computational Linguis-
tics.

Matt Post. 2018. A call for clarity in reporting BLEU
scores. In Proceedings of the Third Conference on
Machine Translation, Volume 1: Research Papers,
pages 186–191, Brussels, Belgium. Association for
Computational Linguistics.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
Improving neural machine translation mod-
2016.
In Proceedings of the
els with monolingual data.
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
86–96, Berlin, Germany. Association for Computa-
tional Linguistics.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz

A Appendix: Parameters

The autoregressive baseline models use roughly
the same set of hyperparameters as the Trans-
former base model (Vaswani et al., 2017). En-
coder and decoder have 6 layers each, model di-
mension is 512, and the dimension of the feed-
forward layer is 2,048. We use 16 attention heads
in both self-attention and encoder-decoder atten-
tion. During training, we use label smoothing of
0.1 and we use dropout rate of 0.1. We use Adam
optimizer (Kingma and Ba, 2015) with parameters
β1 = 0.9, β2 = 0.997, and ǫ = 10−9 with ﬁxed
learning rate of 10−4. Due to the GPU memory
limitations, we use batches of 20 sentences each,
but we accumulate the gradients and perform the
only update the model parameters every 10 steps.
(This makes our batch to have an effective size of
200 sentences.)

The hyperparameters of the CTC-based models
were selected to be as comparative as possible to
the autoregressive models, with the following ex-
ceptions. The splitting factor between the encoder
and the decoder was selected to be k = 3, follow-
ing the setup of Libovick´y and Helcl (2018). We
lowered the number of attention heads between the
encoder and the decoder to 8 instead of 16. We
changed the hyperparameter because it lead to bet-
ter results in preliminary experiments. For train-
ing, instead of batching by a ﬁxed number of sen-
tences, we use batches of maximum size of 400
tokens. We use the same delayed update interval
of 10 steps per update.

B Appendix: Additional Results

results without

Quantitative
the use back-
translation, i.e., when the monolingual data are
used only for training the target-side language
model are shown in in Table 4.

Quantitative results on WMT14 English-to-
German Data for comparison with related work
are presented in Table 3.

Method

German WMT14

en → de

de → en

Non-autoregressive
Transformer, greedy
Transformer, beam 5

Ours, beam 1
Ours, beam 5
Ours, beam 10
Ours, beam 20

19.55
27.29
27.71

20.59
23.61
24.27
24.41

23.04
31.06
31.85

24.11
27.19
27.83
28.14

Table 3: Quantitative results of the models in terms of
BLEU on the WTM14 data.

C Appendix: Examples

We include a few selected examples from the
English-to-German (Table 5), German-to-English
(Table 6), and Czech-to-English (Table 7) system
outputs.

Method

German WMT15

Romanian WMT16

Czech WMT18

en → de

de → en

en → ro

ro → en

en → cs

cs → en

Decoding
time [ms]

Non-autoregressive
Transformer, greedy
Transformer, beam 5

Ours, beam 1
Ours, beam 5
Ours, beam 10
Ours, beam 20

19.71
26.39
26.99

20.81
23.29
23.99
24.01

21.64
28.56
29.39

22.68
25.96
26.19
26.59

18.45
19.91
20.81

18.45
20.88
21.52
22.02

25.48
27.33
27.99

26.52
29.67
29.88
29.94

13.92
16.00
17.08

14.86
17.16
17.20
17.24

14.87
22.72
23.54

16.11
20.87
21.52
21.87

314
1637
4093

326
398
518
1162

Table 4: Quantitative results in terms of BLEU without the use of back-translation.

Source
nAR

On account of their innate aggressiveness, songs of that sort were no longer played on the console.
Aufgrund ihrergeboren Aggressivit¨ativit¨at wurden Lieder dieser Art nicht mehr auf der Konsole gespielt.

(cid:30) Two unrelated words are connected (red), malformed word with repeated subwords (blue).

nAR + LM Aufgrund ihrer angeborenen Aggressivit¨at wurden Lieder dieser Art nicht mehr auf der Konsole gespielt

(cid:30) Correct but too literal adjective was chosen.

AR
Reference

Aufgrund ihrer angeborenen Aggressivit¨at wurden Songs dieser Art nicht mehr auf der Konsole gespielt.
Aufgrund ihrer ureigenen Aggressivit¨at wurden Songs dieser Art nicht mehr auf der Konsole gespielt.

Ailinn didn’t understand.
Source
A hat nicht.
nAR
nAR + LM Aili hat nicht verstanden.
Ailinn verstand es nicht.
AR
Ailinn verstand das nicht.
Reference

→ Fail to copy infrequent proper name.
→ Non-LM features ensured more text is copied, but still incorrect.
→ Correct.

Source
nAR

Further trails are signposted, which lead up towards Hochrh¨on and offer an extensive hike.
Weitere Wege sindschilder, die nach Hochrh¨on und eine ausgedehnte Wanderung.

(cid:30) Two unrelated words are connected (red), missing verb in the second clause (blue).

nAR + LM Weitere Wege sind ausgeschilder, die in Hochrh¨on und eine ausgedehnte Wanderung.

(cid:30) Connected words got corrected, the second clause (blue) still does not make sense.

AR

Weitere Wege sind ausgeschildert, die in Richtung Hochrh¨on f¨uhren und eine ausgedehnte Wanderung
bieten.

→ Correct.

Reference Weitere Wege sind ausgeschildert, die Richtung Hochrh¨on hinaufsteigen und zu einer ausgedehnten Wan-

derung einladen.

Table 5: Manually selected examples of system outputs for English-to-German translation containing the most
frequent error types. ‘nAR’ is the purely non-autoregressive system, ‘nAR + LM’ is the proposed system with
beam size 20.

Aber diese Selbstzufriedenheit ist unangebracht.
But comp complacency is misguided.

Source
nAR
nAR + LM But complacency is misguided.
AR
Reference

But this complacency is inappropriate.
But such complacency is misplaced.

Source

nAR

Als ich also sehr, sehr ¨ubergewichtig wurde und Symptome von Diabetes zeigte, sagte mein Arzt ”Sie
m¨ussen radikal sein.
So when I very, very overweight and and showed symptoms of diabetes, my my doctor said ”You must
be radical.

nAR + LM So when I became very, very overweight and showed symptoms of diabetes, my doctor said ”You must

AR

Reference

be radical.
So when I was very, very overweight and showed symptoms of diabetes, my doctor said ”You must be
radical.
So when I became very, very overweight and started getting diabetic symptoms, my doctor said, ’You’ve
got to be radical.

Table 6: German-to-English examples.

Probl´emem mohou b´yt tak´e jednor´azov´e pleny.
Singleaperslso be problem.

Source
nAR
nAR + LM One can diapers be the problem.
AR
Reference

Single diapers may also be the problem.
Disposable incontinence pants may also be a problem.

Source

Pere se ve mnˇe adolescentn´ı potˇreba uchechtnout se s obdivem nad t´ım, s jak´ym v´aˇzn´ym t´onem je mi
v´yklad pod´av´an.
I adolescent need tohuck with admiration the serious tone my interpret.
nAR
nAR + LM I have a adolescent need to chuck with wonderation of the serious tone my interpret.
AR
Reference

I’m asking for an adolescent need to laugh at the admiration of the serious tone of my interpretation.
I feel the adolescent need to chuckle with admiration for the serious tone with which my comment is
handled.

Table 7: Czech-to-English examples.

