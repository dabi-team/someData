6
1
0
2

t
c
O
7

]
T
S
.
h
t
a
m

[

1
v
7
0
2
2
0
.
0
1
6
1
:
v
i
X
r
a

NEW TESTING PROCEDURES
FOR STRUCTURAL EQUATION MODELING

STEFFEN GRØNNEBERG AND NJ˚AL FOLDNES

Abstract. We introduce and evaluate a new class of hypothesis testing proce-
dures for moment structures. The methods are valid under weak assumptions

and includes the well-known Satorra-Bentler adjustment as a special case. The

proposed procedures applies also to diﬀerence testing among nested models.
We prove the consistency of our approach. We introduce a bootstrap selection

mechanism to optimally choose a p-value approximation for a given sample.
Also, we propose bootstrap procedures for assessing the asymptotic robustness

(AR) of the normal-theory maximum likelihood test, and for the key assump-

tion underlying the Satorra-Bentler adjustment (Satorra-Bentler consistency).
Simulation studies indicate that our new p-value approximations performs well

even under severe nonnormality and realistic sample sizes, but that our tests

for AR and Satorra-Bentler consistency require very large sample sizes to work
well. R code for implementing our methods is provided.

1. Introduction

In testing hypotheses in psychometrics, test statistics often converge in law to
a mixture of independent chi squares, under the null hypothesis of correct model
speciﬁcation. This paper presents novel methods for calculating p-values based on
such test statistics, and a novel selection procedure aimed at identifying the best p-
value among any given set of candidate p-value procedures. Although the proposed
methods can be used in the general setting of moment structure inference, we here
focus on the framework of structural equation modeling (SEM).

As shown in Shapiro (1983) and Satorra (1989), a large class of p-values in the
context of SEM originates from convergence in distribution results (derived under
the null hypothesis) for a test statistic Tn based on n observations, of the form

(1)

Tn

D
−−−−→n
→∞

λj Z 2
j ,

d

j=1
X

Z1, . . . , Zd ∼

N (0, 1) IID,

where λ = (λ1, . . . , λd)′ consists of unknown population parameters.
known, eq. (1) motivates the “oracle” p-value

If λ was

(2)

pn = P

Date: October 10, 2016.

d





j=1
X

.

λjZ 2

j > Tn


1

 
 
 
 
 
 
2

STEFFEN GRØNNEBERG AND NJ˚AL FOLDNES

The above probability is with respect to Z1, . . . , Zd, while Tn is considered ﬁxed.
In a practical setting λ is however unknown. Let ˆλ be a a consistent estimator of
λ, i.e., ˆλ

λ. In the present article we propose to estimate pn by

P
−−−−→n
→∞

(3)

ˆpn = P



d

j=1
X



ˆλjZ 2

,

j > Tn


where the probability is with respect to Z1, . . . , Zd.

We show that for large samples, the error originating from replacing λ with ˆλ is
pn converges in probability to 0. The estimator ˆpn deﬁned
vanishing, so that ˆpn −
above is the canonical member of a new class of estimators, obtained by grouping
the ˆλj by magnitude and replacing them by group means in order to reduce variance
in ˆpn. Although the idea behind this new class of p-value approximations is simple,
we are unaware that it is found previously in the literature.

Since we introduce a whole class of p-value approximations, where no member
seems to be uniformly best in all conditions, we also introduce a selector to aid
the user in choosing which p-value approximation to apply. The core idea of this
selector is to choose the p-value approximation whose distribution is closest to the
uniform, as measured by the supremum distance. This is achieved through the non-
parametric bootstrap and is seen to work very well in our simulation experiments.
The paper is structured as follows. In Section 2 we review ﬁt statistics of mo-
ment structures with a special emphasis on the well-known Satorra-Bentler (SB)
statistic. Section 3 proposes a class of new procedures, that incorporates the SB
statistic as a special case, to evaluate model ﬁt and parameter restrictions in co-
variance models. We give conditions under which the estimators are consistent,
which implies the fundamental p-value property of converging in distribution to a
uniform distribution. In Section 4 we introduce a bootstrap procedure that selects,
for a given sample, a good candidate among a list of p-value approximations. Next,
in Section 5 we introduce a bootstrap test for assessing whether the normal-theory
maximum likelihood test statistic may be trusted, i.e., whether asymptotic robust-
ness holds. Also, we introduce a test for the consistency of the SB statistic, which
may help decide whether the SB statistic may be trusted. Monte Carlo results on
the performance of the proposed new methods are presented in Section 6. In the
ﬁnal section we discuss our ﬁndings and point out further directions for research.
Proofs of theoretical results are presented in the appendix.

2. Fit statistics for moment structure models

Consider a p-dimensional vector of population moments σ◦. In covariance mod-
eling, σ◦ consists of second-order moments, but in more general structural equation
models the means may also be included in σ◦. The corresponding sample mo-
σ◦, and
ment vector s is assumed to converge in probability to σ◦, i.e., s

P
−−−−→n
→∞

NEW TESTING PROCEDURES FOR MOMENT STRUCTURE MODELS

3

−

be asymptotically normal, i.e., √n(s

D
σ◦)
−−−−→n
→∞
model implies a certain parametrization θ
σ(θ) with θ varying in a set Θ. Let
the free parameters in the proposed model be contained in the q-vector θ. The
model has degrees of freedom given by d = p

N (0, Γ). A structural equation

7→

−

q.

The model is said to be correctly speciﬁed if there is a θ◦

Θ such that σ(θ◦) =
σ◦. A very general class of estimators for θ◦ introduced by Browne (1982, 1984) is
obtained by minimising discrepancy functions F = F (s, σ) that obey the following
0 for all s, σ; F (s, σ) = 0 if and only if s = σ; and F is
three conditions: F (s, σ)
twice continuously diﬀerentiable jointly. That is, we consider estimators obtained
as

≥

∈

ˆθ = argmin
θ

Θ

∈

F (s, σ(θ)).

It is well known that the widely used normal-theory maximum likelihood (NTML)
estimator is such a minimal discrepancy estimator.

Similarly, we may deﬁne the least false parameter conﬁguration, which we denote

with θ◦. That is,

θ◦ = argmin

F (σ◦, σ(θ)).

Θ

θ

∈

Irrespective of the correctness of the model, we have ˆθ
larity conditions.

P
−−−−→n
→∞

θ◦ under mild regu-

= σ(θ◦), then Tn → ∞

One, out of several mainly asymptotically equivalent (see Satorra, 1989) ways of
assessing the correctness of the model is to study Tn = nF (s, σ(ˆθ)). If the model
= σ(θ◦). Under
is misspeciﬁed, i.e. if σ◦
correct model speciﬁcation and other assumptions presented in Shapiro (1983) and
Satorra (1989), we have Tn = √n(s
σ◦) + oP (1). Assuming (for
simplicity) that ∆′V ∆ is non-singular (see comment immediately following eq. (9)
in Satorra (1989)) where ∆ is the p
q derivative matrix ∂σ(θ)/∂θ′ evaluated at
θ◦, and V = 1
2

F (s,σ)
∂s∂σ , evaluated at (σ◦, σ◦), we have

P
−−−−→n
→∞

σ◦)′U √n(s

since s

σ◦

×

−

−

∂

2

(4)

U = V

V ∆

∆′V ∆
−
}
{

−

1 ∆′V.

Note that U has rank d. Since we assume √n(s

continuous mapping theorem now implies that Tn
Box (1954), we have

σ◦)

−

D
−−−−→n
→∞

Q

D
−−−−→n
→∞
Q′U Q. By Theorem 1 in

N (0, Γ), the

∼

(5)

Tn

D
−−−−→n
→∞

Q′U Q =

λjZ 2
j ,

d

j=1
X

Z1, . . . , Zd ∼

N (0, 1) IID,

where λ1, . . . , λd are the d non-zero eigenvalues of U Γ under the standard scaling
of the eigenvectors. That is, the parameters λ in eq. (1) are eigenvalues of a cer-
tain matrix that depends both on the underlying distribution and on the proposed
model. Note that estimating U and Γ is a standard problem in moment models
which we will not discuss in technical detail. The usual estimators are based on

6
6
4

STEFFEN GRØNNEBERG AND NJ˚AL FOLDNES

replacing expectations with averages of the observed data, and the true least-false
parameter θ◦ by the estimator ˆθ. This is the estimator readily available in software
packages such as the R package lavaan (Rosseel, 2012). We here assume that con-
sistent estimators ˆU , ˆΓ and ˆλ are given. We may use the plug-in method to form
ˆλ, so that ˆλ is the d non-zero eigenvalues of ˆU ˆΓ under the standard scaling of the
eigenvectors.

Note that the asymptotically distribution-free (ADF) estimator of Browne (1984),
where the estimate is obtained by minimising a quadratic form whose weight matrix
is the inverse of a distribution-free estimate of Γ, yields a test statistic TADF whose
population eigenvalues are all equal to one. Hence ADF estimation leads to consis-
tent p-values for model ﬁt. However, ADF estimation is unstable in small samples,
and it is well-known that TADF has unacceptably poor performance in small and
medium samples sizes (Curran et al., 1996; Hu et al., 1992). Another test statistic
with consistent p-value approximation is the residual-based test statistic (Browne,
1984, eq. 2.20), which is not of the form Tn = nF (s, σ(ˆθ)) investigated in the
present article. Unfortunately this statistic suﬀers from the same lack of accept-
able ﬁnite-sample performance as TADF. Therefore, a more popular approach has
been to use normal-theory based estimators, and to correct the test statistic for
non-normality in the data. We now proceed to describe such methods.

Based on the convergence result in eq. (1), Satorra & Bentler (1994) proposed

to rescale Tn by dividing it by the mean value of the eigenvalues to form

TSB =

Tn
ˆc

,

Pd

j=1 ˆλj
d

. Using TSB as a test statistic is a widely used SEM practice
where ˆc =
under conditions of non-normal data. Simulation studies report that TSB out-
performs the NTML ﬁt statistic TML in such conditions, but that Type I error
rates under TSB are seriously inﬂated under substantial excess kurtosis in the data
(Bentler & Yuan, 1999; Nevitt & Hancock, 2004; Savalei, 2010; Foldnes & Olsson,
2015). Also, Yuan & Bentler (2010) theoretically demonstrated that TSB departs
from a chi-square with increasing dispersion of the eigenvalues in (1).

Recently Asparouhov & Muth´en (2010) proposed a test statistic that agrees with
the reference chi-square distribution in both asymptotic mean and variance, ob-
tained from TML by scaling and shifting. This statistic, found to perform slightly
better (Foldnes & Olsson, 2015) than a Sattertwaithe type test statistic proposed
by Satorra & Bentler (1994), is given by

TSS =

d
( ˆU ˆΓ)2
(cid:16)

(cid:17)

·

tr

v
u
u
t

Tn + d

−

v
u
u
u
u
t

2

.

d

tr( ˆU ˆΓ)
(cid:17)
(cid:16)
( ˆU ˆΓ)2
tr
(cid:16)

(cid:17)

NEW TESTING PROCEDURES FOR MOMENT STRUCTURE MODELS

5

1/2

A quite diﬀerent testing methodology is oﬀered by the so-called Bollen-Stine
bootstrap (Bollen & Stine, 1992), which is based on the non-parametric boot-
strap (Efron & Tibshirani, 1994).
Instead of starting with the fundamental re-
sult in eq. (1), one starts by transforming the sample observations Xi into ˜Xi =
n Xi for i = 1, 2, . . . , n, where Sn and Σ(ˆθ) are the sample and model-
Σ(ˆθ)1/2S−
implied covariance matrices, respectively. Noting that the model holds exactly in
this transformed sample, we proceed by assuming that the transformed sample
may serve as a proxy for the population from which the original sample was drawn.
The Bollen-Stine p-value is now obtained by drawing bootstrap samples from the
transformed sample, and calculating the proportion of bootstrap test statistics that
exceed the test statistic obtained from the original sample. The validity of this ap-
proach is derived in Beran & Srivastava (1985). Nevitt & Hancock (2001) report
that Bollen-Stine bootstrapping outperformed the SB scaling approach under cor-
rect model speciﬁcation at realistic sample sizes, having Type I errors slightly below
the nominal level. Despite the promising performance of the Bollen-Stine bootstrap,
it seems to be relatively understudied. In fact, we are not aware of any later simu-
lation study that systematically evaluates its performance relative to other robust
test statistics.

Our upcoming selection methodology to be described in Section 4 will fuse the
two ideas discussed above, trying to combine the strength of the convergence in
eq. (1) with the power of the non-parametric bootstrap. Our tests for AR and
Satorra-Bentler consistency described in Section 5, are also based on the non-
parametric bootstrap. Before describing these bootstrap based methods, we return
to the fundamental convergence result in eq. (1) and present new approximations
for the oracle p-value.

3. A new class of p-value approximations

In this section we introduce and establish the consistency of a new computational
technique for p-values. The proposed methodology applies as long as the null
distribution of a test statistic is a weighted sum of independent chi squares and the
weights can be estimated consistently. This means that the method may be used
both for conventional goodness-of-ﬁt testing of a single proposed model, and for
nested model comparison tests. Consistency is established in Theorem 1, and the
proof is found in Appendix A.

The convergence result in eq. (1) is only valid if the model is correctly speciﬁed.
But we here note that U Γ is deﬁned also when the model is misspeciﬁed, and that
the number of non-zero eigenvalues is known to be d from the model conﬁguration.
We may therefore speak of and estimate λ = (λ1, λ2, . . . , λd)′ without knowing if
the model is correctly speciﬁed. We refer to the p-value in (3) as the full p-value
approximation. We will also shortly introduce other estimators by combining the

6

STEFFEN GRØNNEBERG AND NJ˚AL FOLDNES

ˆλj in eq.(3) in ways that may reduce variability in ˆpn, although at the expense
of consistency. This may be reasonable in situations where the full estimates are
unstable, e.g, under small sample sizes and highly non-normal data. In fact, the
familiar TSB procedure may conceptualized as an (inconsistent) p-value approxima-
tion where the λj are replaced by the mean value of the canonical estimates, i.e.,
ˆλSB
j =

, j = 1, . . . , d, and clearly

Pd

j=1 ˆλj
d

ˆpSB = P



ˆλSB
j Z 2

d

j=1
X



.

j > Tn


We obtain a valid approximation as long as ˆλ

λ, as the following theorem
shows. Note that we make no assumptions on Tn. That is, the approximation
holds irrespective of the correctness of the model. Note also that we typically have
ˆλ

pn] stays bounded in probability.

= OP (n−

P
−−−−→n
→∞

1/2), i.e., √n[ˆpn −

k

−

λ
k

Theorem 1. Let (Tn) be a sequence of random variables, and let pn = 1

H(Tn; ˆλ) where H(q; λ1, . . . , λr) = P (
pn =

λ where λ only has positive elements, then ˆpn −

−

d
j=1 λjZ 2
ˆλ
λ
P
k
k

−

−
q). If

j ≤
OP (1), and

H(Tn; λ) and ˆpn = 1
ˆλ

P
−−−−→n
→∞
hence, ˆpn −
Proof. See Appendix A.

P
−−−−→n
→∞

pn

0.

(cid:3)

·

(1, 1, . . . , 1).

We see that the TSB procedure is a valid large sample approximation to pn
if λ = c
If this is true in the population, Theorem 1 implies the
consistency of the TSB procedure. The only crucial assumption of the theorem is
that each λj > 0. Recall that in goodness of ﬁt testing in SEM, we are guaranteed
d non-zero eigenvectors by the Box Theorem, see the discussion near eq. (5). Hence
this assumption is innocuous.

A direct consequence of Theorem 1 is that ˆpn fulﬁlls the following property

considered fundamental to p-values.

Corollary 1.

Suppose the conditions of Theorem 1 holds.

d
j=1 λj Z 2

j for Z1, . . . , Zd ∼

N (0, 1) IID, then ˆpn

D
−−−−→n
→∞

U [0, 1].

If Tn

D
−−−−→n
→∞

P
Proof. Since (1) holds, it follows that pn

D
−−−−→n
→∞
from the standard asymptotic result that if Xn −
also Yn

Z.

D
−−−−→n
→∞

U [0, 1]. Then the corollary follows

Yn = oP (1) and Xn

D
−−−−→n
→∞

Z then

(cid:3)

From our perspective of aiming at consistent p-values, the TSB procedure is well
motivated under an equality constraint among all eigenvalues. But if the eigenvalues
diﬀer considerably in the population, this restriction may lead to poor estimates

NEW TESTING PROCEDURES FOR MOMENT STRUCTURE MODELS

7

due to a high bias. In contrast, ˆpn is always a valid approximation for pn in that
it is consistent – and hence asymptotically unbiased. However, in ﬁnite samples
the variability of ˆλ may lead to excessive variability in ˆpn. We therefore wish to
ﬁnd middle-grounds between the SB approximation and ˆpn. This amounts to using
the consistent estimates ˆλj to calculate new weights that may reduce the sample
variability of pn, and at the same time reduce the eﬀect of inconsistency in SB.
Consider for instance the following split-half approximation, where the lower half
of the eigenvalues are replaced by their mean value, and likewise for the upper half
of the eigenvalues.

d

where

and

˜λ1 =

· · ·

ˆpn,half = P

,

˜λj Z 2

j > Tn






j=1
X

= ˜λ
⌈

d/2

⌉

=

1
d/2

⌈

d/2

⌈

⌉

⌉

j=1
X

ˆλj

d

˜λ
⌈

d/2

+1 =
⌉

= ˜λd =

· · ·

1
d/2

− ⌈

d

ˆλj.

⌉

d/2
Xj=
⌉
⌈

+1

This procedure allows the p-value approximation an additional degree of freedom
compared to the SB statistic, where all eigenvalues are estimated to be equal to
each other. A whole class of middle-grounds between the full ˆpn and ˆpn,SB can
< τk < d with
be deﬁned as follows. Choose cut-oﬀ integers 1 < τ1 < τ2 <
1

k < d. For τl

k < τl let

· · ·

≤

1 ≤

−

(6)

˜λk =

1
τl
τl −

1

−

1

τl−

ˆλj

j=τl−1
X

where τ0 = 1 and τk+1 = d. Let us denote this choice by ˜λ(τ ) = (˜λ1(τ ), . . . , ˜λr(τ ))′.
The proposed p-value estimator is then

ˆpn(τ ) = P



˜λj(τ )Z 2

d

j=1
X



.

j > Tn


An extension of the above framework is tests that assess nested hypotheses in
SEM. Due to its great practical importance, we here include a short discussion on
this special case. We again focus on the statistic Tn, since this statistic is typically
asymptotically equivalent to other tests of interests, as described in Satorra (1989).
Θ0
for some continuously diﬀerentiable function a. We

Following Satorra (1989), let H : σ = σ(θ), θ

Θ and H0 : σ = σ(θ), θ

∈

∈

where Θ0 =
Θ : a(θ) = 0
θ
assume that the matrix ∂a(θ)

∈

{

}

∂θ has full row rank, say m. We let

ˆθ = argmin
θ

Θ

∈

F (s, σ(θ)),

˜θ = argmin
Θ0
θ

∈

F (s, σ(θ))

8

STEFFEN GRØNNEBERG AND NJ˚AL FOLDNES

and Tn = nF (s, σ(ˆθ)) and ˜Tn = nF (s, σ(˜θ)). Under H0 and the conditions of
Lemma 1 (iv) in Satorra (1989) we have

Tn = √n(s
˜Tn = √n(s

−

−

σ◦)′U √n(s
σ◦)′ ˜U √n(s

−

−

σ◦) + oP (1)

σ◦) + oP (1),

for matrices U and ˜U following the formula of eq. (4) under H and H0, respectively.
Using the basic algebraic fact that x′(A + B)x = x′Ax + x′Bx we conclude that
the diﬀerence statistic is of the form

Tn = √n(s

˜Tn −
U has rank m.

−

σ◦)′Ud√n(s

−

σ◦) + oP (1),

where Ud = ˜U

−

By the continuous mapping theorem, the convergence √n(s

and Theorem 1 in Box (1954), we therefore have that

σ◦) D

−−−−→n
→∞

−

N (0, Γ),

(7)

˜Tn −

Tn

D
−−−−→n
→∞

αjZ 2
j ,

m

j=1
X

Z1, . . . , Zm ∼

N (0, 1) IID,

where α1, . . . , αm are the m non-zero eigenvalues of UdΓ.

Distribution-free consistent estimators ˆUd and ˆΓ for Ud and Γ are found and
discussed in Satorra & Bentler (2001), and we do not review them here. Again
the standard estimators can be found in software such as the R package lavaan.
One then forms ˆα = (ˆα1, . . . , ˆαm)′ equal to the m largest eigenvectors of ˆUd ˆΓ and
calculates the full p-value approximation

ˆpn = P





m

j=1
X

ˆαjZ 2

j > ˜Tn −

.

Tn


We remark that a single equality constraint, say, βi,j = 0, can be treated as
special case of the above framework. In this case, the number of restrictions is 1, and
hence the limiting distribution in eq. (7) is a scaled χ2
1. The SB and the proposed
p-value approximations then coincide exactly. Note that Theorem 1 implies that
these procedures are consistent.

4. A selection algorithm for p-value approximations

The framework of the last section leads to several competing p-value approxi-
mations, and we next introduce a way of selecting among these. Our selector is
inspired by Beran & Srivastava (1985), the Bollen-Stine bootstrap (Bollen & Stine,
1992), and the non-parametric focused information criterion of Jullum & Hjort
(2016, forthcoming).

We wish to select the p-value approximation ˆpn whose distribution is closest to
the uniform distribution under the null hypothesis. We formalize this by estimating

NEW TESTING PROCEDURES FOR MOMENT STRUCTURE MODELS

9

the supremum distance between the cumulative distribution function of ˆpn under
the null hypothesis and the uniform distribution, i.e. we approximate

Dn = sup
x
≤

≤

0

PH0 (ˆpn ≤

1 |

x)

x
|

−

for each p-value approximation, and select the method with the least value of Dn.
The probability PH0 is the probability measure induced by the data-generating
distribution that is closest to fulﬁlling the null hypothesis compared to the true
data-generating mechanism, which we let be the data generating distribution of
Σ(θ◦)1/2Σ−
1/2Xi, where Σ is the true covariance matrix. Under PH0 , we know
that p-values should be uniformly distributed. If we consider asymptotically con-
sistent p-values, minimizing Dn will mean that we choose the approximation whose
convergence has been best achieved at our sample-size n.

The approximation to Dn is done via the non-parametric bootstrap, based on
1/2
the transformed sample ˜Xi = Σ(ˆθ)1/2S−
n Xi for i = 1, 2, . . . , n, as described in
Algorithm 1. The supremum in Algorithm 1 is the test statistic of the Kolmogorov-
Smirnov test, which is implemented in most statistical software packages. Formally,
what we do is to use the empirical distribution function ˆPn of ( ˜Xi) as an approx-
imation to PH0 , and approximate this probability through re-sampling. We then
plug this approximation into Dn to generate ˆDn for each p-value approximation.
We note that we may use this selector among any p-value approximation for
hypothesis testing in moment structures, and not just the suggestions in Section 3.
Also, Dn is only one out of many possible success criteria. One could also investigate
x) to
the mean square error of the approximation, or the distance from PH0 (ˆpn ≤
x at a particular point x. In our simulations, the performance of Dn as a selection
criterion was overall satisfactory.

Algorithm 1 Selection algorithm

1: procedure Select(sample, B)

Draw with replacement from transformed sample ˜Xi

n Xi for i = 1, 2, . . . , n.

based on boot.sample

˜Xi = Σ(ˆθ)1/2S−
for k

1, . . . , B do

1/2

←

boot.sample
for l

←
1, . . . , L do

∈
ˆpn,l ←
end for

end for
for l

1, . . . , L do
sup0

∈
ˆDB,n,l ←

≤

2:

3:

4:

5:

6:

7:

8:

9:

10:

11:

end for
return argmin1

12:
13: end procedure

P

ˆDB,n,l

L

l

≤

≤

1

B−

x

1 |

≤

B
k=1 I

{

ˆpn,l < x

} −

x
|

10

STEFFEN GRØNNEBERG AND NJ˚AL FOLDNES

5. Hypothesis tests for Satorra-Bentler consistency
and asymptotic robustness

In this section we propose a bootstrap procedure for testing Satorra-Bentler
consistency, that is, that all non-zero eigenvalues are equal. This also leads naturally
to a test for asymptotic robustness (AR), that is, that all non-zero eigenvalues are
equal to 1. Such tests may help a practitioner to decide whether it is advisable to
apply the NTML test, the SB test, or to instead use the Bollen-Stine bootstrap or
the new procedures proposed in the present article.

There is a substantial body of theoretical literature on AR (Shapiro, 1987;
Browne & Shapiro, 1988; Amemiya & Anderson, 1990; Satorra & Bentler, 1990),
where exact conditions are given that involve certain relationships between Γ and
∆ that must hold for TML to retain its asymptotic chi-square distribution under non-
normality. However, these conditions are hard to check in practice, and currently no
practical procedure exist for verifying asymptotic robustness in a real-world setting
(Yuan, 2005, p. 118). Similarly, we unaware of the existence of tests for SB consis-
tency. This lack of tests might be due to the fact that testing statements concerning
the eigenvalues of U Γ involves testing statements about high moment properties
of a distribution. Without detailed parametric assumptions on the data it seems
very diﬃcult to construct tests that perform well in small-sample situations. It is
therefore expected that our proposed procedures will require a large sample size to
attain Type I error rates close to the nominal level. This is conﬁrmed to be the
case in the simulation experiment in Section 6.3.

The proposed bootstrap test is summarized in Algorithm 2 and is inspired by Sec-
tion 4.2 in Beran & Srivastava (1985). A proof of its consistency, which we do not
provide, seems to require a non-trivial extension of the theory contained in Beran
(1984); Beran & Srivastava (1985). We note that an important diﬀerence between
our suggested test and the procedures in Section 4.2 in Beran & Srivastava (1985),
who work with eigenvalues of empirical covariance (i.e., symmetric) matrices, is
that U Γ is typically not symmetrical.

Let E be the matrix of normalised (complex) eigenvectors sorted by descending
1 (Meyer, 2000, p.514)

values of the eigenvalues λ of U Γ. We have U Γ = E∆E−

where ∆ =

Deﬁne

∆d 0
0 0!

(8)

, and where ∆d is the diagonal matrix with elements λ1, . . . , λd.

A = c1/2

E

·

1/2

∆−
d
0

0
0!

E−

1,

where c denotes the mean value of the eigenvalues λ1, . . . , λd. We propose the
following bootstrap procedure. Let ˆA be estimated from the original sample, by
replacing E, ∆, c with ˆE, ˆ∆, ˆc. For each bootstrap sample drawn from the original

 
 
NEW TESTING PROCEDURES FOR MOMENT STRUCTURE MODELS

11

sample, we calculate ˆUbootˆΓboot and form the matrix

W ∗n = ˆA ˆUbootˆΓboot ˆA.

The crucial observation is now that W ∗n converges to a matrix for which the null-
hypothesis is true, that is, whose non-zero eigenvalues are all equal. To see this,
note that

W ∗n

P
−−−−→n
→∞

cE

1/2

∆−
d
0

0
0!

1

E−

U Γ

E

·

·

1/2

∆−
d
0

0
0!

1

E−

= cE

= cE

1/2

∆−
d
0

1/2

∆−
d
0

E−

1E∆E−

1E

0
0!

1/2

∆−
d
0

0
0!

1

E−

0
0!  

∆d 0
0 0!  

∆−
d
0

1/2

E−

1 = E

0
0!

cId 0
0 0!

E−

1,

where the last matrix has d non-zero eigenvalues equal to d.
In the bootstrap
sample, the d largest eigenvalues of W ∗n is then computed as ˆλboot. This process is
repeated many times, and we get realizations ˆλk,boot, giving us information about
the sampling variability of the estimated eigenvalues under the null hypothesis of
identical eigenvalues.

The above procedure may also be adapted to test for asymptotic robustness of
the NTML statistic TML, that is, whether λj = 1 for all j = 1, . . . , d. By setting
c = 1 in eq. (8), Algorithm 2 then produces a p-value for the test of consistency of
TML based testing. Since this test does not need to estimate c, it should converge to
the correct level I error rate slightly faster than the general case. The test statistic
that is bootstrapped is then

W ∗n = ˆA1 ˆUbootˆΓboot ˆA1.

1/2

1.

E−

0
0!

where ˆA1 is the estimator of A1 = E

∆−
d
0
We suppose that an extension of Corollary 4 in Beran & Srivastava (1985) holds
also in our setting. That corollary requires the test statistic h(λ) to be non-negative
and zero under the null hypothesis, and that it has partial derivatives that are
zero under the null hypothesis and that its double derivative matrix is positive
deﬁnite under the null hypothesis. The additional restriction that also the partial
derivatives vanish under the null-hypothesis means we must consider two diﬀerent
test statistics, adapted from the examples in Section 4.3 in Beran & Srivastava
(1985). For asymptotic robustness, this holds for hAR(λ) = d log[d−
log[
λd)2]
Q

−
d
j=1 λj ]. For Satorra-Bentler consistency, this holds for hSB(λ) = log[(λ1 +

log[4λ1λd]. Algorithm 2 summarizes this discussion.

d
j=1 λj]

P

1

−

 
 
 
 
 
 
 
12

STEFFEN GRØNNEBERG AND NJ˚AL FOLDNES

Algorithm 2 Bootstrap testing for Satorra-Bentler consistency and Asymptotic
Robustness

2:

3:

The d largest eigenvalues of ˆU ˆΓ

1: procedure Bootstrap(sample, B)
Calculate ˆU , ˆΓ, ˆA, ˆA1 from sample
ˆλ
Tn,SB = hSB(ˆλ)
Tn,AR = hAR(ˆλ)
for k

1, . . . , B do

←

5:

6:

4:

7:

8:

9:

10:

11:

12:

13:

14:

←

Based on boot.sample

boot.sample
←
ˆUbootˆΓboot ←
ˆA ˆUbootˆΓboot ˆA
W ∗n,SB ←
ˆλk,boot = (ˆλk,1,boot, . . . , ˆλk,d,boot)′
hSB(ˆλk,boot)
Tn,k,SB ←
ˆA1 ˆUbootˆΓboot ˆA1
W ∗n,AR ←
ˆλk,boot = (ˆλk,1,boot, . . . , ˆλk,d,boot)′
hAR(ˆλk,boot)
Tn,k,AR ←

Draw with replacement from sample

the d largest eigenvalues of W ∗n,SB

the d largest eigenvalues of W ∗n,AR

←

←

15:

end for
return B−
16:
17: end procedure

1

B
k=1 I

Tn,k,SB > Tn,SB}

{

and B−

1

B
k=1 I

Tn,k,AR > Tn,AR}

{

P

P

6. Monte Carlo evaluations

In this section we evaluate the proposed procedures by Monte Carlo methods.
We ﬁrst evaluate our new class of p-value approximations in the setting of goodness-
of-ﬁt testing for a single model. Speciﬁcally, two members of this class are eval-
uated, ˆpn and ˆpn,half, referred to as the full and half eigenvalue approximations,
respectively. We then consider chi-square diﬀerence testing for two nested mod-
els. In both cases we evaluate the selection procedure in Algorithm 1 where the
candidates for selection are SB and the full and half eigenvalue approximations.
Finally, we evaluate the empirical performance of the proposed bootstrap test for
SB consistency and for AR. We remark that we here limit ourselves to study the
empirical performance of the procedures when it comes to controlling type I error
rates, leaving the topic of power for future studies.

Our model is the political democracy model discussed by Bollen in his textbook
(Bollen, 1989), see Figure 1, where the residual errors are not depicted for ease
of presentation. There are four measures of political democracy measured twice
(in 1960 and 1965), and three measures of industrialization measured once (in
M1 has d = 35 degrees of freedom. For nested
1960). The unconstrained model
model testing, we also consider a constrained model
M1, with
d = 46 degrees of freedom, which impose ten equalities among unique variances
and residual covariances, and one equality between two factor loadings.

M0, nested within

NEW TESTING PROCEDURES FOR MOMENT STRUCTURE MODELS

13

Model estimation and eigenvalues were computed using the R package lavaan
(Rosseel, 2012), while the p-values of type ˆpn were calculated with the imhof pro-
cedure in the package CompQuadForm (Duchesne & de Micheaux, 2010). In each
simulation cell we generated 2000 samples. For each sample, 1000 bootstrap sam-
ples were drawn.

Figure 1. Bollen’s political democracy model. dem60: Democ-
racy in 1960. dem65: Democracy in 1965. ind60: Industrialisation
in 1960.

Y1

Y2

Y3

dem60
dem60

ind60
ind60

dem65
dem65

Y4

Y5

Y6

Y7

Y8

Y9

Y10

Y11

Distribution 2

Distribution 3

1.87 1.59 1.49 1.44 1.43 1.42 1.38
1.36 1.35 1.34 1.31 1.29 1.26 1.13
1.12 1.11 1.11 1.10 1.10 1.09 1.09
1.08 1.08 1.07 1.07 1.07 1.06 1.05
1.04 1.03 1.03 1.03 1.02 1.02 1.01
4.16 3.24 2.88 2.82 2.70 2.67 2.51
2.41 2.35 2.31 2.16 2.12 2.03 1.52
1.50 1.47 1.43 1.40 1.38 1.36 1.35
1.33 1.32 1.29 1.27 1.25 1.21 1.20
1.13 1.13 1.11 1.09 1.08 1.08 1.06

Table 1. Eigenvalues λi, for i = 1, . . . , 35, for Bollen’s political
democracy model, assuming correct model speciﬁcation. Distribu-
tion 2 and 3 have univariate skewness and kurtosis s = 1, k = 7
and s = 2, k = 21, respectively.

14

STEFFEN GRØNNEBERG AND NJ˚AL FOLDNES

SEL ORAC
0.077
0.055
0.068
0.057
0.057
0.059
0.024
0.045
0.046

0.050 0.051
0.043 0.045
0.064 0.065
0.048 0.042
0.045 0.045
0.051 0.051
0.072 0.031
0.050 0.038
0.042 0.038

Distribution

NTML

SB

SS BOST EFULL EHALF

Normal

Distribution 2

Distribution 3

n
100
300
900
100
300
900
100
300
900

0.036
0.037
0.063
0.021
0.024
0.037
0.009
0.013
0.015

0.077 0.086 0.050
0.055 0.053 0.052
0.068 0.067 0.050
0.215 0.108 0.019
0.197 0.070 0.018
0.219 0.063 0.033
0.488 0.164 0.017
0.591 0.094 0.013
0.685 0.076 0.017

0.023
0.037
0.059
0.035
0.053
0.054
0.038
0.068
0.059
Table 2. Type I error rates for testing model
M1. Normal: mul-
tivariate normal distribution, Distribution 2: skewness 1 and kur-
tosis 7. Distribution 3: skewness 2 and kurtosis 7. NTML=normal-
theory likelihood ratio test. SB=Satorra-Bentler. SS=Scaled and
shifted. BOST=Bollen-Stine bootstrap. EFULL= Full eigen-
value approximation, ˆpn. EHALF= half eigenvalue approximation,
ˆpn,half. SEL = p-value obtained from selection algorithm. ORAC=
oracle p-value pn.

Distribution

Normal

Distribution 3

Distribution 3

n
100 0.054
300 0.448
900 0.507
100 0.001
300 0.050
900 0.153
100 0.000
300 0.001
900 0.004

SB EHALF EFULL
0.015
0.931
0.036
0.516
0.231
0.263
0.135
0.865
0.055
0.894
0.063
0.783
0.551
0.449
0.267
0.733
0.150
0.846

Table 3. Choice proportions for selection algorithm, testing
M1. SB=Satorra-Bentler. EHALF=half eigenvalue approx-
model
imation, ˆpn,half. EFULL= Full eigenvalue approximation, ˆpn.

6.1. Goodness-of-ﬁt testing for
M1. Three population distributions were con-
sidered. Distribution 1 was a multivariate normal distribution. The non-normal
distributions were generated using the transform of Vale & Maurelli (1983), with
Distribution 2 having univariate skewness 1 and kurtosis 7, and Distribution 3 hav-
ing skewness 2 and kurtosis 21. These distributional characteristics are the same

NEW TESTING PROCEDURES FOR MOMENT STRUCTURE MODELS

15

as those used in the inﬂuential study by Curran et al. (1996), and replicated in the
bootstrap study by Nevitt & Hancock (2001). The ”oracle” eigenvalues associated
with Distribution 2 and 3 are given in Table 1, numerically calculated from very
large samples, where we clearly see that the values are quite spread out, and that
the spread increases when we move from Distribution 2 to Distribution 3. Note
that under the Distribution 1, we have λ = (1, 1, . . . , 1)′.

Three sample sizes n were used: 100, 300 and 900. Hence the resulting full fac-
torial design has nine conditions. In each sample we calculated p-values associated
with the established test statistics associated with normal-theory maximum like-
lihood ratio (NTML), Satorra-Bentler scaling (SB), the scale-and-shifted statistic
(SS) and the Bollen-Stine (BOST) test. Also, we calculated in each sample the
full eigenvalue approximation ˆpn (EFULL) and the split-half eigenvalue estimation
ˆpn,half (EHALF). The selection algorithm (SEL) p-value was calculated using a
candidate set with members SB, EHALF and EFULL, and using ˆDn as criterion
function. Finally, the oracle (ORAC) p-value pn was calculated, using the values in
Table 1. This allows us to evaluate how well the asymptotic result in eq.(1) applies
in ﬁnite-sample conditions.

In Table 2 we present Type I error rates at the the 5% signiﬁcance level. As
expected, NTML becomes inﬂated when data is non-normal. The mean-scaling of
SB reduces the inﬂation, but with non-normal data and small sample sizes, Type I
error rates are still higher than 10%. The scaled-and-shifted statistic on the other
hand, leads to rejection rates much lower than the nominal 5%. These ﬁndings are
in accord with Foldnes & Olsson (2015). The Bollen-Stine bootstrap test performs
better than SB and SS, coming close to the nominal level even for highly non-normal
data and medium sample size. Among the new p-value approximations, it is the
middle-ground approximation EHALF that performs the best. While EFULL yields
far too low rejection rates with non-normal data. EHALF as well as BOST with
non-normal data. The selection algorithm SEL also performs generally well, on
par with EHALF and BOST. It is notable that for normal data, SEL outperforms
NTML. Table 3 presents the selection proportions for SEL in each of the nine
conditions. It is seen that the selection algorithm wisely chooses EHALF in the
majority of conditions. It is however unexpected that SEL chooses EFULL in 55%
of the samples under Distribution 3 and n = 100, given the poor performance of
EFULL in that condition, with a 1% rejection rate. The ﬁnal column in Table 2
gives the oracle solution, and demonstrates that the asymptotic result in (1) is far
from realized at n = 100 under Distribution 3.

6.2. Testing nested models. The chi-square diﬀerence test has 11 degrees of
freedom, and the corresponding 11 oracle eigenvalues for Distribution 2 and 3 are
given in Table 4. The spread in eigenvalues is substantial, especially for Distribution
3.

16

STEFFEN GRØNNEBERG AND NJ˚AL FOLDNES

Distribution 2
Distribution 3

3.92 3.49 3.19 2.99 2.94 2.78 2.72 1.85 1.56 1.54 1.30
10.64 8.79 8.06 7.58 7.37 6.94 6.76 4.09 3.16 3.10 2.04

Table 4. Eigenvalues of UdΓ for nested model testing. Distribu-
tion 2 has skewness 1 and kurtosis 7; Distribution 3 has skewness
2 and kurtosis 21. Rounded to two decimal places.

Distribution

Normal

Distribution 2

Distribution 3

SEL ORAC
0.068
0.054
0.051
0.028
0.035
0.046
0.005
0.018
0.043

SB BOST EFULL EHALF

0.062
0.053
0.051
0.076
0.052
0.035
0.115
0.062
0.044

0.069 0.075
0.055 0.058
0.052 0.053
0.099 0.096
0.066 0.062
0.043 0.045
0.159 0.135
0.089 0.082
0.064 0.061

ML
n
100 0.068 0.080
300 0.054 0.059
900 0.051 0.053
100 0.582 0.137
300 0.659 0.088
900 0.702 0.059
100 0.911 0.221
300 0.961 0.126
900 0.976 0.087

0.037
0.046
0.051
0.096
0.081
0.053
0.129
0.118
0.089
Table 5. Type I error rates for nested model testing. Nor-
mal: multivariate normal distribution, Distribution 2: skewness
1 and kurtosis 7. Distribution 3:
skewness 2 and kurtosis 7.
NTML=normal-theory likelihood ratio test. SB=Satorra-Bentler.
BOST=Bollen-Stine bootstrap. EFULL= Full eigenvalue approxi-
mation, ˆpn. EHALF= half eigenvalue approximation, ˆpn,half. SEL
= p-value obtained from selection algorithm. ORAC= oracle p-
value pn.

Rejection rates observed at the nominal 5% level of signﬁcance are reported in
Table 5. Again, the NTML statistic is inﬂated by non-normality in the data, a
tendency only partially corrected for by SB. For instance, under the most harsh
condition, with Distribution 3 and n = 100, SB rejection rates are 22%, far better
than the 91% obtained with NTML. But in this condition, as in all conditions,
BOST performs better, with a rejection rate of 13%. However, the new procedure
EFULL performs still better in this condition, while the selection algorithm is only
slightly worse than BOST. Overall EFULL outperforms the other test statistics,
including SB and BOST. EHALF, which was found to have best performance in
the non-nested case, does not perform as well as EFULL in the nested case. The
selection algorithm SEL also performs well, with better performance than SB and
BOST in most conditions, and only sligthly worse then EFULL. The selection
proportions are given in Table 6, where EHALF is unexpectedly found to be the

NEW TESTING PROCEDURES FOR MOMENT STRUCTURE MODELS

17

most chosen procedure, despite the slightly better performance of EFULL in most
conditions.

Distribution

Normal

Distribution 3

Distribution 3

n
100 0.601
300 0.672
900 0.593
100 0.116
300 0.209
900 0.263
100 0.012
300 0.059
900 0.104

SB EHALF EFULL
0.042
0.357
0.122
0.205
0.316
0.091
0.170
0.714
0.128
0.662
0.142
0.595
0.325
0.663
0.215
0.725
0.182
0.714

Table 6. Choice proportions for selection algorithm, nested mod-
els. SB=Satorra-Bentler. EHALF=half eigenvalue approximation,
ˆpn,half. EFULL= Full eigenvalue approximation, ˆpn.

6.3. Tests for AR and for SB consistency. To evaluate Type I error rates of the
SB consistency and AR tests proposed in Algorithm 2, we simulated multivariate
normal data for the Bollen model. Under normal data both AR and SB consistency
holds. We simulated 2000 samples for sample sizes n = 200, 400, 800 and 2000. For
each sample 1000 bootstrap samples were drawn. The rejection rates are given in
Table 7, and clearly demonstrates that these procedures need large sample sizes in
order to reach acceptable Type I error rates.

Test n = 200 n = 400 n = 800 n = 2000
0.035
0.203
AR
0.195
0.033
SB
Table 7. Type I error rates for tests of asymptotic robustness
(AR) and Satorra-Bentler (SB) consistency.

0.081
0.070

0.354
0.369

7. Discussion

This paper deals with the fundamental problem of hypothesis testing in moment
structure models. We present new insight and practically applicable statistical
methodology for SEM and related models.

Some of our conclusions may seem surprising, as they go against what is often
taught in standard courses on SEM. For example, the simulation summarized in
Table 2 shows that our selector can have better ﬁnite sample performance than

18

STEFFEN GRØNNEBERG AND NJ˚AL FOLDNES

the NTML test also when data are exactly normal. Since this paper have focused
exclusively on Type I error, “better” here means having a rejection rate closer to
the nominal one.

Since this conclusion may seem counterintuitive, it is worth pausing and consid-
ering what the NTML does. Firstly, we must keep in mind that the NTML is a
test based on asymptotic theory, also when data are exactly normally distributed.
That is, the Type I error rate of an NTML test at level α converges to α under
normality, and for the model considered in Table 2, convergence is still not quite
achieved for n = 100. Secondly, we note that under normality, NTML calculates
the oracle p-value exactly. That is, it is the ultimate approximation to the oracle
test, which has a rejection rate of 7.7 %. Hence, the NTML has only one source of
approximation error: the validity of the fundamental convergence of the oracle.

All methods considered in this paper – with the important exception of the
Bollen-Stine bootstrap and the selector – tries to approximate the oracle, and
thereby introducing another source of approximation error. Let us say they are
oracle-based. Except the NTML, which calculates the oracle perfectly – but only
under exact normality, the oracle-based methods have varying degrees of success in
their approximation. Strictly speaking, oracle-based methods should be judged on
whether they manage to achieve what they set out to do: approximate the oracle.
But that is not the success criteria of interests to the user: When a level α test
is employed, the Type I error rate should be very close to α. As is clear from our
simulation studies, this may not be the case even when using the actual oracle.
When oracle-based tests have Type I error rate considerably closer to α than the
oracle, it is tempting to say that they are performing well. This temptation should
be avoided, as the deviation in Type I error compared to the oracle is then solely
due to chance variations caused by the estimation of λ.

The selector overcomes this hurdle by being anchored not in the fundamental
convergence of the oracle, but by transforming the data to a setting where the null
hypothesis holds. It is then known that a correct p-value is uniformly distributed,
i.e., the Type I error rate of a test with level α is to be exactly α.
It is this
anchoring that allows us to search for the procedure which best achieves this goal,
without having to compare our methods to the ﬁnite sample performance of the
oracle. And so when the selector has a Type I error rate close to the nominal, it
is by design, and not solely due to chance variations. This is a property shared
with the Bollen-Stine bootstrap procedure, but the Bollen-Stine procedure rests on
the quality of the approximation of the empirical distribution function compared to
the data’s actual distribution function. So do we, since we use the non-parametric
bootstrap in our selector, but we are able to combine the fundamental convergence
of the oracle with the non-parametric bootstrap. We have seen that this allows us
to combine the strengths of both methods.

NEW TESTING PROCEDURES FOR MOMENT STRUCTURE MODELS

19

Let us return to the NTML, and look at the proposed methods from a slightly
diﬀerent perspective that elaborates on the above. It is well-known that the NTML
usually has a much too high Type I error rate under non-normality. The major
source of the mismatch between nominal and actual Type I error rate is that the
NTML need not be a consistent approximation to the oracle. The NTML can be
seen as estimating λ always by the constant (1, . . . , 1)′. When λ is far away from
(1, . . . , 1)′, NTML performs poorly. And for a user, it typically performs poorly in
a particularly bad way: even when a hypothesized theory holds, the NTML will
most likely reject it.

The Satorra-Bentler test has previously been reported to have inﬂated Type I
error rates under non-normality, and this behaviour is also observed in simulations
in the present paper. This is mainly due to two reasons: ﬁrstly, it may be that the
Satorra-Bentler procedure is inconsistent, i.e. λ has variation among its elements.
While inconsistency is an asymptotic property, which may seem irrelevant in small
samples, it does mean that the procedure does not aim to estimate what the user
wants, and may therefore be reﬂected also in small-sample situations when the
procedure is used uncritically. Secondly, the Satorra-Bentler procedure estimates
λ, and the variability of the resulting p-value approximation may give inﬂated Type
I errors even when the procedure is consistent.

These two problems, consistency and ﬁnite sample approximation error, are
shared also by our suggested p-value approximations. However, the contextual
framework presented in the present paper allows us to argue about balancing these
issues, and selecting among competing approximations. This perspective may lead
to further insight in future research, and has already led to our proposed selector.
We note that while ˆpn and ˆpn,half can be computed just as fast as the Satorra-
Bentler test statistics, both the selector and the Bollen-Stine bootstrap procedure
takes considerable more computation time. Our simulation experiments indicate
that the selector and the Bollen-Stine bootstrap are comparable in performance,
but that the selector works slightly better, especially in small sample situations.
Our recommendations to practitioners are therefore clear: use the selector in small
sample situations, and use the selector or the Bollen-Stine bootstrap in medium
sample situations.
In large sample situations, consistent p-value approximation
gives similar answers. Since the assumptions underlying asymptotic robustness
and Satorra-Bentler consistency rests on delicate properties of high order moments
that can only be properly tested in large sample situations, we do not recommend
using the NTML nor the Satorra-Bentler statistic without assessing its performance
with the selector. In many cases, the Satorra-Bentler statistic will be the superior
test, but it is diﬃcult to know this without using techniques such as the re-sampling
methods underlying the selector.

20

STEFFEN GRØNNEBERG AND NJ˚AL FOLDNES

With current and future computers containing multiple units that can perform
computation simultaneously (multi-core central processing units and multi-core
graphical processing units supporting general purpose calculations), using the se-
lector does not take much time to run. In our prototype implementation in the
scripting language R (which means our code is not compiled, and therefore slow),
it takes a few additional minutes compared to standard p-value approximations
that we have seen often performs considerably worse. Considering the enormous
amount of time and eﬀort many researchers use in gathering and analyzing data,
the extra time spent on using the selector is vanishing in comparison.

Applied researchers are often personally interested in controlling Type I error
as well as possible, as their research hypothesis is often the null hypothesis.
If
they use testing procedures, such as the NTML with non-normal data, where the
Type I error is seriously inﬂated, this is to their disadvantage. This point is also
connected to the use of pragmatic ﬁt indices available in the literature. The p-value
approximations discussed in this paper are all based on solid statistical theory. The
ad-hoc nature of some of these ﬁt indices, with somewhat arbitrary cut-oﬀ points
being interpreted in various ways, are not based on statistical theory.

Finally, we mention that the ideas contained in this paper can be generalized in
several directions, including SEM with ordinal variables and in multi-group settings.
Also, additional simulation experiments should be performed on the proposed meth-
ods, such as power studies, allowing the selector more options, and experimenting
with diﬀerent selection criterias.

References

Amemiya, Y. & Anderson, T. (1990). Asymptotic chi-square tests for a large

class of factor analysis models. The Annals of Statistics , 1453–1463.

Asparouhov, T. & Muth´en, B. (2010). Simple second order chi-square cor-
rection. Retrieved from Mplus website: http://www. statmodel. com/download-
/WLSMV new chi2 1.

Bentler, P. M. & Yuan, K.-H. (1999). Structural equation modeling with small

samples: Test statistics. Multivariate Behavioral Research 34, 181–197.

Beran, R. (1984). Bootstrap methods in statistics. Jahresbericht der Deutschen

Mathematiker-Vereinigung , 14–30.

Beran, R. & Srivastava, M. S. (1985). Bootstrap tests and conﬁdence regions

for functions of a covariance matrix. The Annals of Statistics , 95–115.

Bollen, K. A. (1989). Structural equations with latent variables. New York:

Wiley.

Bollen, K. A. & Stine, R. A. (1992). Bootstrapping goodness-of-ﬁt measures
in structural equation models. Sociological Methods & Research 21, 205–229.

NEW TESTING PROCEDURES FOR MOMENT STRUCTURE MODELS

21

Box, G. (1954). Some theorems on quadratic forms applied in the study of analysis
of variance problems, 1. eﬀect of inequality of variance in the one-way classiﬁca-
tion. The Annals of Mathematical Statistics 25, 290–302.

Browne, M. & Shapiro, A. (1988). Robustness of normal theory methods in the
analysis of linear latent variable models. British Journal of Mathematical and
Statistical Psychology 41, 193–208.

Browne, M. W. (1982). Covariance structures. Topics in applied multivariate

analysis , 72–141.

Browne, M. W. (1984). Asymptotically distribution-free methods for the anal-
ysis of covariance structures. British Journal of Mathematical and Statistical
Psychology 37, 62–83.

Curran, P. J., West, S. G. & Finch, J. F. (1996). The robustness of test
statistics to nonnormality and speciﬁcation error in conﬁrmatory factor analysis.
Psychological Methods 1, 16–29.

Duchesne, P. & de Micheaux, P. L. (2010). Computing the distribution of
quadratic forms: Further comparisons between the liu-tang-zhang approximation
and exact methods. Computational Statistics and Data Analysis 54, 858–862.
Efron, B. & Tibshirani, R. J. (1994). An introduction to the bootstrap. CRC

press.

Foldnes, N. & Olsson, U. H. (2015). Correcting too much or too little? the
performance of three chi-square corrections. Multivariate Behavioral Research
50, 533–543.

Hu, L.-T., Bentler, P. M. & Kano, Y. (1992). Can test statistics in covariance

structure analysis be trusted? Psychological Bulletin 112, 351–62.

Jullum, M. & Hjort, N. L. (2016, forthcoming). Parametric or nonparametric:

The ﬁc approach. Statistica Sinica .

Laury-Micoulaut, C. (1976). The n-th centered moment of a multiple convolu-
tion and its applications to an intercloud gas model. Astronomy and Astrophysics
51, 343–346.

Meyer, C. D., ed. (2000). Matrix Analysis and Applied Linear Algebra. Philadel-

phia, PA, USA: Society for Industrial and Applied Mathematics.

Nevitt, J. & Hancock, G. (2001). Performance of Bootstrapping Approaches
to Model Test Statistics and Parameter Standard Error Estimation in Structural
Equation Modeling. Structural Equation Modeling: A Multidisciplinary Journal
8, 353–377.

Nevitt, J. & Hancock, G. (2004). Evaluating small sample approaches for model
test statistics in structural equation modeling. Multivariate Behavioral Research
39, 439–478.

Rosseel, Y. (2012). lavaan: An r package for structural equation modeling. Jour-

nal of Statistical Software 48, 1–36.

22

STEFFEN GRØNNEBERG AND NJ˚AL FOLDNES

Satorra, A. (1989). Alternative test criteria in covariance structure analysis: A

uniﬁed approach. Psychometrika 54, 131–151.

Satorra, A. & Bentler, P. M. (1990). Model conditions for asymptotic robust-
ness in the analysis of linear relations. Computational Statistics & Data Analysis
10, 235–249.

Satorra, A. & Bentler, P. M. (1994). Corrections to test statistics and stan-
dard errors in covariance structure analysis. In Latent variable analysis: applica-
tions for developmental research, A. V. Eye & C. Clogg, eds., chap. 16. Newbury
Park, CA: Sage, pp. 399–419.

Satorra, A. & Bentler, P. M. (2001). A scaled diﬀerence chi-square test

statistic for moment structure analysis. Psychometrika 66, 507–514.

Savalei, V. (2010). Small sample statistics for incomplete nonnormal data: ex-
tensions of complete data formulae and a monte carlo comparison. Structural
Equation Modeling: A Multidisciplinary Journal 17, 241–264.

Shapiro, A. (1983). Asymptotic distribution theory in the analysis of covariance
structures - a uniﬁed approach. South African Statistical Journal 17, 33–81.
Shapiro, A. (1987). Robustness properties of the mdf analysis of moment struc-

tures. South African Statistical Journal 21, 39–62.

Vale, C. & Maurelli, V. (1983). Simulating nonnormal distributions. Psy-

chometrika 48, 465–471.

Yuan, K. (2005). Fit indices versus test statistics. Multivariate Behavioral Re-

search 40, 115–148.

Yuan, K.-H. & Bentler, P. M. (2010). Two simple approximations to the
distributions of quadratic forms. British Journal of Mathematical and Statistical
Psychology 63, 273–291.

Appendix A. Proof of Theorem 1

−

P

H(Tn; ˆλ) = 1

rn ≤
≤
j=1(ˆλj −

1 so that ˆpn = 1
−
λj)Hj (Tn; λ + rn(ˆλ

λ)) and Hj(q, (l1, . . . , ld)′) = ∂H(q,(l1,...,ld)

Proof of Theorem 1. By the mean value theorem, there is a sequence of random
H(Tn; λ)+Rn = pn+Rn where
variables 0
d
.
Rn =
∂lj
−
The statement of the theorem therefore holds if we show that Hj(Tn; λ+rn(ˆλ
λ)) =
OP (1). To show this, we calculate Hj. The cumulative distribution function of
q
d
j=1 λj Z 2
S =
0 hS(s) ds where hS is the density of S. Denote the
density of λjZ 2
j=1 contains independent variables, so does
P
(λjZ 2
j )d
j=1. Hence hS is given by d-times convolution, i.e. apply the well-known
convolution formula iteratively, and see that

j is HS(q) =
j by hj(z). Since (Zj)d

−

R

′

(9)

gS(s) =

d

j=2
Y

hj(xj

1)



−



−

s

h1 


d

1

−

j=1
X

xj 


R · · ·

R 

Z

Z



dx1 · · ·

dxr

1,

−

NEW TESTING PROCEDURES FOR MOMENT STRUCTURE MODELS

23

see also Laury-Micoulaut (1976) for some basic properties of d-times convolution.
We wish to calculate

(10)

∂
∂λj

HS(q) =

q

∂
∂λj

hS(s) ds.

0
Z

hS is a weighted sum of densities, which implies that ∂
∂λj

It turns out that ∂
HS is a
∂λj
weighted sum of cumulative distribution functions that is easy to bound uniformly.
We now show this by calculating ∂
hS. Since summation is commutative, the
∂λj
r
j=1 λπ(j)Z 2
π(j) is the same for any permutation π(1), . . . , π(r) of
distribution of
. We may therefore, without loss of generality, assume that j = d. Using

1, . . . , r

{
}
eq. (9), we have

P

(11)

∂
∂λd

hS(s)

=

R · · ·

Z

R (cid:26)
Z

∂
∂λd

hd(xd

−

d

1

−

1)

[

hj(xj

1)]h1(s

−

(cid:27)

j=2
Y
1). Since λjZ 2

d

1

−

j=1
X

−

xj ) dx1 · · ·

dxd

1

−

∂
∂λd

2

1

1

2

1

−

z

0

}

{

dz

1/2

1/2

≥

−

−

1/2

e−

√2π

j ∼

z/(2λd)

1
d ] =

z/2 + (

z1/2e−

z/2 = d

2 )z1/2e−

d hχ2 (z/λd)

z/2 = 1
1/2

d hχ2 (z/λd) =
λ−

d hχ2 (z/λd)+λ−
λ−

j is a linear transformation of
z/2I
is the

We hence need to calculate
hd(xd
1, we have hj(z) = hχ2 (z/λj)/λj where hχ2 (z) = z
χ2
Z 2
density of Z 2
j ∼
We have ∂
∂λd
d hχ2 (z/λd)
λ−

χ2.
hd(z) = ∂
∂λd
−
3
d zh′χ2(z/λd). For z < 0 then hd(z) = 0 and so ∂
λ−
∂λd

d h′χ2 (z/λd)[ ∂
zλ−
∂λd
hd(z) = 0.
−
The event z = 0 has probability zero and can be ignored. For z > 0 we have
√2πh′χ2(z) = √2π d
z
z/2 so
1/2e−
e−
2 z−
√2π
2 λ1/2
that h′χ2 (z/λd) = 1
z/(2λd) Inserting this into the
z1/2e−
1/2e−
d z−
−
2 λ1/2
2
expression obtained for ∂
hr(z) gives ∂
1/2e−
λ−
d z−
d fχ2 (z/λd)
∂λd
∂λd
1
1
1
z3/2e−
z1/2e−
z/(2λd)] =
z/(2λd).
2 λ−
d λ−
λ−
d
1
d hχ2(z/λd) is a density, since it is the density of λjZ 2
λ−
We now note that z
j .
z/(2λd) are proportional to Gamma-distributions. Re-
z/(2λd) and z3/2e−

dz z1/2e−
1
2 λ−
d
hd(z) =
1
2 λ−
d

In conclusion, we see that
7/2

Also, z1/2e−
call that the Gamma(α, β) density for α > 0, β > 0 is hG(α,β)(z) = βαzα
u du.
0

0 uz
in which Γ(z) =
}
5/2
2
1
(2λd)3/2 hG(3/2,1/(2λd))(z)+ 1
2 λ−
2 λ−
λ−
d hχ2 (z/λd)
R
d
d
5/2λ−
λ−
d hχ2 (z/λd)
d Γ(3/2)hG(3/2,1/(2λd))(z)+2−
2−
By the linearity of integration and xd
1), and xd

1e−
−
∂
∂λd
Γ(5/2)
(2λd)5/2 hG(5/2,1/(2λd))(z) =
d Γ(5/2)hG(5/2,1/(2λd))(z)
1/λd)/λd, and xd
1 7→
hG(5/2,1/(2λd))(xd
1) are densities, eq. (11) is a
weighted sum of convolutions of densities that result in new densities hA, hB and hC.
7/2λ−
5/2λ−
d Γ(3/2)hB(z) + 2−
d Γ(5/2)fC(z).
That is,
−
1
Returning to eq. (10) we therefore see that ∂
5/2λ−
d hA(s)
λ−
∂λd
−
6
1
7/2λ−
d Γ(5/2)HC(q)
2 λ−
2−
where HA, HB, HC are the cumulative distribution functions of hA, hB, hC.

q
0 −
d Γ(3/2)HB(q)+2−
R

−
hG(5/2,1/(2λd))(xd

d Γ(5/2)hC(s) ds =

3
d z[ 1
λ−
7/2
2 λ−
d

−
z/(2λd)+ 1

HS(q) =
4

d HA(q)
λ−

d hA(z)
λ−

βx/Γ(α)I

hd(z) =

−
Γ(3/2)

hS(s) =

hχ2(xd

7/2λ−

7/2λ−

−
7→

1 7→

1 7→

∂
∂λd

1e−

−
5/2

2−

2−

−

−

−

−

−

−

−

−

∞

−

−

−

−

−

4

1

4

6

2

6

6

4

1

Recalling that cumulative distribution functions are probabilities, and hence has
λj + rn(ˆλj −

absolute values bounded by 1, we see that

Hj(Tn; x + rnhn)

| ≤ |

|

z/(2λd)

−

z

{

≥

d Γ(3/2)hB(s)+

24

λj)
|
0

≤

STEFFEN GRØNNEBERG AND NJ˚AL FOLDNES

5/2

−

1 + 2−
rn ≤

|
1 and ˆλj

λj + rn(ˆλj −
P
−−−−→n
→∞

λj)
|

4Γ(3/2) + 2−

−

λj > 0, we see that

|

7/2

λj + rn(ˆλj −
Hj(Tn; x + rnhn)
|

|

λj)
|

−

6Γ(5/2). Since
(cid:3)

= OP (1).

Department of Economics, BI Norwegian Business School, Oslo, Norway 0484
E-mail address: steffeng@gmail.com

Department of Economics, BI Norwegian Business School, Stavanger, Norway 4014
E-mail address: njal.foldnes@bi.no

