Uncertainty-Aware Camera Pose Estimation from Points and Lines

Alexander Vakhitov1

Luis Ferraz Colomina2

Antonio Agudo3

Francesc Moreno-Noguer3

1SLAMcore, UK
2Kognia Sports Intelligence, Spain
3Institut de Rob`otica i Inform`atica Industrial, CSIC-UPC, Spain

1
2
0
2

l
u
J

8

]

V
C
.
s
c
[

1
v
0
9
8
3
0
.
7
0
1
2
:
v
i
X
r
a

Abstract

Perspective-n-Point-and-Line (PnPL) algorithms aim at
fast, accurate, and robust camera localization with respect
to a 3D model from 2D-3D feature correspondences, being
a major part of modern robotic and AR/VR systems. Current
point-based pose estimation methods use only 2D feature
detection uncertainties, and the line-based methods do not
take uncertainties into account. In our setup, both 3D co-
ordinates and 2D projections of the features are considered
uncertain. We propose PnP(L) solvers based on EPnP [20]
and DLS [14] for the uncertainty-aware pose estimation.
We also modify motion-only bundle adjustment to take 3D
uncertainties into account. We perform exhaustive syn-
thetic and real experiments on two different visual odometry
datasets. The new PnP(L) methods outperform the state-
of-the-art on real data in isolation, showing an increase in
mean translation accuracy by 18% on a representative sub-
set of KITTI, while the new uncertain reﬁnement improves
pose accuracy for most of the solvers, e.g. decreasing mean
translation error for the EPnP by 16% compared to the
standard reﬁnement on the same dataset. The code is avail-
able at https://alexandervakhitov.github.io/uncertain-pnp/.

1. Introduction

Camera localization using sparse feature correspon-
dences is a major part of augmented or virtual reality and
robotic systems. The Perspective-n-Point(-and-Line), or
PnP(L), methods can be successfully used to estimate the
pose of a calibrated camera from sparse feature correspon-
dences. Line features can increase localization accuracy
in man-made self-similar environments which lack surfaces
with distinctive textures [36, 43, 15, 29], motivating the use
of PnP(L) methods [41].

While vision-based localization with respect to an au-

This work has been partially funded by the Spanish government under
projects HuMoUR TIN2017-90086-R, ERA-Net Chistera project IPALM
PCI2019-103386 and Mar´ıa de Maeztu Seal of Excellence MDM-2016-
0656.

Figure 1. We propose globally convergent PnP(L) solvers leverag-
ing a complete set of 2D and 3D uncertainties for camera pose es-
timation. A 3D scene model with sparse features is reconstructed
from images with known poses (right cameras), and we need to
ﬁnd a pose of a camera on the left. The point has 2D detection un-
certainty (blue ellipsoid), and 3D model uncertainty (red ellipsoid
in the scene).

tomatically reconstructed map of sparse features is an im-
portant part of current robotic and AR/VR systems, com-
monly used PnP methods treat the features as absolutely
accurate [47, 14, 20, 16]. The more recent 2D covariance-
aware methods relax this assumption [40, 6], but only for
the feature detections, still assuming perfect accuracy of the
3D feature coordinates.

In maps reconstructed with structure-from-motion, 3D
feature coordinate accuracy can vary. Stereo triangula-
tion errors grow quadratically with respect to the object-to-
sensor distance, so the accuracy in estimating point depth
can vary in several orders of magnitude, while the line
stereo triangulation accuracy depends on the angle between
the line direction and the baseline. Nevertheless, to the best
of our knowledge, no prior method for PnP(L) was designed
to take both 3D and 2D uncertainties into account.

We propose to integrate the feature uncertainty into the
PnP(L) methods, see Fig. 1, which is our main contribu-
tion. We build on the classical DLS [14] and EPnP [20]

1

 
 
 
 
 
 
methods. Additionally, we propose a modiﬁcation to the
standard nonlinear reﬁnement, which is normally used after
the PnP(L) solver, to take 3D uncertainties into account. An
exhaustive evaluation on synthetic data and on the two real
indoor and outdoor datasets demonstrates that new PnP(L)
methods are signiﬁcantly more accurate than state-of-the-
art, both in isolation and in a complete pipeline, e.g.
the
proposed DLSU method reduces the mean translation error
on KITTI by 18%, see Section 4. The proposed uncertain
pose reﬁnement can improve the pose accuracy by up to
16% in exchange of an extra 5-10% of the computational
time. In a synthetic setting with noise in 2D feature detec-
tions, the new methods have the same accuracy as the most
accurate 2D uncertainty-aware methods [6, 40]. The code
is available at https://alexandervakhitov.github.io/uncertain-
pnp/.

2. Related work

We start with discussing how proposed methods relate
to known PnP methods for arbitrary number of correspon-
dences n, PnPL and 2D covariance-aware methods.
Perspective-n-point. Geometric gold standard cost [13]
for pose estimation for arbitrary n is highly non-convex, and
direct pose solvers rely on simpliﬁed algebraic costs. Early
methods [28, 22, 35, 31, 8, 4, 1] were slow and inaccurate.
Starting from [24] fast direct solvers were developed [21,
14, 16, 47]. EPnP [24] was the ﬁrst to provide fast and
accurate pose estimate solving a least-squares system with
nonlinear constraints.

EPnP was developed further in [20, 7, 6, 41]: [20] pro-
posed adaptive PCA-based choice of control points and
a fast iterative reﬁnement step, [7] designed an EPPnP
method for robust pose estimation in presence outliers.
Structure-from-motion [34], visual SLAM [25], object pose
estimation [38] rely on EPnP due to its robustness and fast
computational time. A proposed solver EPnPU is a deriva-
tive of EPnP, providing better accuracy when feature uncer-
tainty information is available.

Groebner basis solvers for polynomial systems are more
accurate but are computationally more demanding [14, 48,
47, 16, 12]. The DLS method [14] is fast enough for real-
time use and has higher accuracy compared to the EPnP,
while the most accurate method OPnP [47] is signiﬁcantly
slower. DLS minimizes the object space error and uses the
Cayley parameterization, and has a singularity which can be
avoided [27]. In this work, we build on the DLS and take
feature uncertainties into account, however relying on the
OPnP-like algebraic cost instead of the object space error.
PnPL methods. An early DLT method [13] as well as an
algorithm [4] can compute camera pose from n line cor-
respondences, but have inferior accuracy compared to new
polynomial solver-based approaches [23, 30, 45, 18, 49].
Extending EPnP or OPnP to PnPL [41, 42] is practical

Figure 2. Schematic representation of the PnPL(U) problem, see
text.

since a PnPL method uses all available mixed correspon-
dences at once. In this work, we propose EPnPLU/DLSLU
methods, which take line uncertainty into account in order
to improve the pose estimates.
PnP with uncertainty. Features are detected with vary-
ing uncertainty, and 2D uncertainty-aware PnP meth-
ods [6, 40] use it to improve the estimated pose accu-
racy. CEPPnP [6] builds on EPPnP and inherits the base
method’s low computational complexity. MLPnP [40]

is more accurate and computationally demanding than
CEPPnP, because it combines both a linear solver and a re-
ﬁnement into one method.

Both CEPPnP and MLPnP use only 2D feature detection

uncertainty and work only for points.

In contrast, the approach we present in this paper is more
accurate due to the use of both 2D and 3D feature uncer-
tainty and works for a mixed set of line and point corre-
spondences.

3. Method

We start with formulating the problem, and then proceed
to introduce the uncertainty-aware pose solvers and the non-
linear reﬁnement method. We conclude the section with de-
scribing the approach for obtaining the feature covariances.
We denote matrices, vectors and scalars with capital, bold
and italic letters, e.g. R, x, γ, and x(i) denotes the i-th com-
ponent of x.

3.1. PnP(L) with Uncertainty

We are given a set of np 3D points {xi}np
i=1 and nl 3D
i=1, {qi}nl
line segments deﬁned by their endpoints {pi}nl
i=1.
Points and line segment endpoints are corrupted by zero-
mean Gaussian noises with covariances Σxi, Σpi and Σqi.
The point projections {ui}np
i=1 are corrupted with zero-
mean Gaussian noises with covariances Σui. The line seg-
ments projections {li}nl
i=1 are represented as normalized
line coefﬁcients, so (cid:107)l(1:2)
(cid:107) = 1. We model the line seg-
ment detection uncertainty as a zero-mean Gaussian added
to the distance between the line and any point on the image

i

2

p∑p∑xq∑qxlσl2u∑uR, t -?plane, with a variance σ2
l,i, see Fig. 2. We consider a camera
with known intrinsics, assuming that the camera calibration
matrix K is an identity matrix.

Our problem is to estimate a rotation matrix R and a
translation vector t aligning the camera coordinate frame
with the world frame. We assume knowledge of an estimate
of the average scene depth ¯d, and we consider also the case
when there is a rough initial hypothesis ^R, ˆt available.

3.2. Uncertainty for Pose Estimation Solvers

In this section, we derive methods for uncertainty-aware
pose estimation. We ﬁnd the uncertainties for the algebraic
feature projection residuals and incorporate them into the
pose solvers in the form of the residual covariances. We
start with point features, then move to lines.
Point residuals. Let us parameterize a point in the camera
coordinate frame as ˆx(θ, x) = Rx + t, where θ encodes
the camera parameters R, t. The algebraic point residual is
based on perspective point projection:

rpt(θ, x, u) = ˆx(1:2)(θ, x) − uˆx(3)(θ, x),

(1)

where u is the projected point.

By our assumptions x and u are corrupted with addi-
tive zero-mean Gaussian noises with covariances Σx, Σu:
ˆx(θ, x) = Eˆx + ξ and u = Eu + ζ, where ξ, ζ are zero-
mean Gaussian noise vectors. The covariance of ˆx(θ, x)
is:

(cid:20) S w
wT
γ

(cid:21)

.

(2)

Σˆx = RΣxRT =

Substituting into (1), we obtain

rpt = E{ˆx(1:2) − uˆx(3)} + ξ(1:2) − uξ(3) − ζ ˆx(3) − ξ(3)ζ,
(3)
omitting the function arguments for clarity. By expressing
the covariance of (3) as Erpt(rpt)T , using the independence
of ξ and ζ we obtain the residual covariance:

Σrpt = S + γuuT + (ˆx(3))2Σu − (uwT + wuT ).

(4)

To compute Σrpt , we need to know R and ˆx(3). If we ap-
proximate the model point covariance Σx ≈ σ2I, where
I denotes the identity, then Σˆx ≈ σ2I as well, as fol-
If we have a rough pose hypothesis ^R, ˆt,
lows from (2).
we can use it instead to approximate Σrpt . We propose
the new solvers in two modiﬁcations. In the ﬁrst case, the
solver uses the average scene depth estimate ¯d to approx-
imate the point depths, and an isotropic approximation to
3D point covariance (see Section 3.6 below), we dub these
solvers EPnPU and DLSU. In the second case, the solver
uses the pose hypothesis, typically available as an output
of the RANSAC loop, to approximate the point depths and
compute the 3D point covariance estimates; we denote these
methods DLSU*, EPnPU*.

Line residuals. We are given the normalized 2D line seg-
ment coefﬁcients l as well as the segment 3D endpoints
p, q, and consider the algebraic line residual following [41]:

rln(θ, p, q, l) =

(cid:20) lT ˆp(θ, p)
lT ˆq(θ, q)

(cid:21)

,

(5)

where ˆp(θ, p), ˆq(θ, q) are the 3D endpoints in the camera
coordinate frame, ln stands for ’line’. Let us decompose
the endpoint ˆp(θ, p) = Eˆp(θ, p) + ηp, where ηp has the
covariance Σˆp = RΣpRT . Under our model for the line de-
tection noise, the noise-corrupted signed line-point distance
is lT yh = E{lT }yh + νy, where νy is a zero-mean Gaus-
sian with variance σ2
l , yh is an arbitrary image point, in
homogeneous coordinates. If yh is a projection of a point
y = λyyh with depth λy, then lT y = E{lT y} + λyνy.
Therefore,

lT ˆp(θ, p) = E{lT ˆp(θ, p)} + λpνp + lT

i ηPi,

(6)

where νPi is the line detection noise. The variance is

E(lT ˆp − ElT ˆp)2 = λ2

pσ2

l + lT Σˆpl,

(7)

where we omit the function arguments for brewity. Under
our model, the noise in p, q and the line detection noises
are assumed independent. We acknowledge that this is a
simpliﬁcation, however it speeds up the computations, and
works in practice, as we show in the experiments. More-
over, other works, e.g. [29], rely on such a model as well,
while in the ofﬂine setting one could follow [10] in using
more advanced noise models for the lines. The covariance
for the line residual is

Σrln = σ2

l diag(λ2

p, λ2

q) + diag(lT Σˆpl, lT Σˆql).

(8)

In order to compute the covariance we need R and the point
depths λp, λq. For EPnPLU and DLSLU, we approximate
the point depths with ¯d given in the problem formulation
as isotropic, for EPnPLU*
, Σ ˆQi
and the covariances Σ ˆPi
and DLSLU* we use a pose hypothesis ^R, ˆt to compute
these values.

So far we obtained a general form of residual covariances
for point and line features under our noise model. Next we
show, how to use it in the PnP(L) solvers.

3.3. EPnP with Uncertainty

We generalize the EPnP [20] and EPnPL [41] to leverage
2D and 3D uncertainties in pose prediction. EPnP starts
with computing an SE3-invariant barycentric representation
α of a point x:

x = Cα,

(9)

where C = [c1, . . . , c4] is a matrix of the four speciﬁ-
cally chosen control points in the world coordinate frame.

3

[20] proposed to choose the ﬁrst point c1 as a mean of xi
and c2, c3 and c4 as the maximum variance directions com-
puted using principal component analysis (PCA). Prelim-
inary experiments show, that when 3D noise is added to
x, the accuracy of the PCA version degrades. This moti-
vated us to modify the control points choice to use the 3D
uncertainties. We can get a straightworward theoretically
solid PCA generalization under isotropic approximation of
3D point covariances Σx ≈ σ2
3 trace(Σx).
In particular, classical PCA solves a following problem to
obtain the j-th principal direction:

xI, where σ2

x = 1

(cid:88)

i

(zT ˜xi)2 → max

z

s.t. (cid:107)z(cid:107) = 1,

(10)

where ˜xi are the centered points with subtracted projections
on the ﬁrst j − 1 components, and z is the sought princi-
pal direction. The covariance is cov(zT ˜x) = zT ˜x˜xT z =
zT Σ˜xz = σ2
x using the fact that (cid:107)z(cid:107) = 1. Then, we modify
the problem as

(cid:88)

i

σ−2
x,i (zT ˜xi)2 → max

z

s.t. (cid:107)z(cid:107) = 1,

(11)

see more details in the supp. mat.

The camera pose in EPnPL is represented through the
control points in the camera coordinate frame, so θEP nP =
^C = [ˆc1, . . . , ˆc4], and ˆci = Rci + t. The camera frame
point is ˆx(θEP nP , x) = ^Cα(x). EPnP(L) uses the alge-
braic residuals for lines and points (1, 5), solving a problem

(cid:107)Mvec(^C)(cid:107)2 → min
,

^C

(12)

where vec(·) denotes a vectorized matrix. The solution is
given by an eigendecomposition of a 12 × 12 matrix MT M.
The method then looks for ^C in the subspace of the eigen-
vectors of MT M with smallest eigenvalues.

The proposed EPnP(L)U method follows the same strat-
egy, constructing the uncertainty-augmented matrix MU . In
the previous section we noted, that to estimate the covari-
ances of the point and line algebraic residuals (1, 5) we
need to know R and t. However, approximating the point
covariances by isotropic covariance matrices as Σx ≈ σ2
xI,
Σp ≈ σ2
3 trace(Σ·), we get rid
of the dependency on R in the residuals. By replacing the
point depth with the approximate scene depth ¯d, we com-
pute the point (1) and line (5) residual covariances as

pI, Σq ≈ σ2

qI, where σ2

· = 1

ΣEP nP
rpt

= σ2

ΣEP nP
rln

= σ2
l

xI + ¯d2Σu + σ2
¯d2I + (cid:107)l(cid:107)2diag(σ2

xuuT .
p, σ2

q).

(13)

(14)

We also consider a case when a rough pose hypothesis is
given.
In this case, we still use the isotropic approxima-
tion of uncertainties, but use the pose to compute estimates
of the depths of points. The method proceeds as the basis
version. Next we describe our DLS-based approach.

4

3.4. DLS with Uncertainty

The DLS method [14] employs Cayley rotation param-
eterization to solve a least-squares polynomial system of
the algebraic residuals for the point correspondences with
It relies on so-called ob-
the Groebner basis techniques.
ject space error PnP, when one minimizes the distance be-
tween the backprojection ray of the point detection and the
3D point. However, we decided to use the algebraic resid-
ual (1), which allows for faster computations and results in
a method with similar accuracy, see supp.mat. for the com-
parison. DLS performs eigendecomposition of a 27 × 27
matrix. We keep the Cayley parameterization of DLS, but
reformulate the equations, and generate the new solver of
the same dimension using the generator of [19].

The DLS uses the following parameterization of a point
in camera coordinates: ˆx(θ, x) = R(s)x + t, so θDLS =
[sT , t]T , where s ∈ R3 is a vector of the Cayley rotation
parameters:

R(s) =

1
1 + (cid:107)s(cid:107)2

(cid:0)(1 − ssT )I + 2[s]x + 2ssT (cid:1) ,

(15)

[s]x denotes a cross product matrix. We use the residuals
for lines and points (5,1) with the camera parameterization
θDLS. The residual covariances are obtained as in a case of
EPnP. The point or line residual rk(s, t) can be expressed
using the DLS parameterization as

rk(s, t) = Akvec(R(s)) + Tkt,

(16)

and we denote its covariance as Σrk . The cost function of
the method is

1
2

nr(cid:88)

k=1

k (s, t)Σ−1
rT
rk

rk(s, t) → min
s,t

.

(17)

We constrain the gradient of the cost by t to be zero,
express t using the remaining unknowns and obtain the cost
that depends only on R(s). Following DLS, we multiply this
cost by (1 + (cid:107)s(cid:107)2)2, so it becomes a polynomial of s. We
constrain its gradient by s to be zero and obtain a third order
polynomial system with three unknowns. It is solved using
the generated solver, then t is found using an expression
obtained before, see the details of the derivation in the supp.
mat.

To improve accuracy, we also use an optional non-linear
reﬁnement stage by reﬁning the cost (17) with a Newton
method starting from the output of a solver, computing the
Hessian of the cost analytically.

One often reﬁnes the output of the pose solver with non-
linear minimization of the gold-standard feature reprojec-
tion errors [13]. In the following section we propose a new
formulation of a reﬁnement method in order to take the full
set of feature uncertainties into account.

3.5. Uncertainty-aware Pose Reﬁnement

When the structure is ﬁxed, to obtain optimal estimates
of the camera pose one uses motion-only bundle adjust-
ment [39], that is formulated as a non-linear least squares-
based log-likelihood maximization. In feature-based pose
estimation, one runs it as a ﬁnal reﬁnement step, initializing
with the output of a pose solver. A standard 2D covariance-
aware formulation of the motion-only bundle adjustment
cost is:

L(θ) =

np
(cid:88)

i=1

(cid:107)¯rpt

i (cid:107)2
Σ
pt
¯r
i

+

nl(cid:88)

i=1

(cid:107)¯rln

i (cid:107)2

Σ¯rln
i

→ min θ,

(18)

where θ is the camera pose, ¯rpt
i are the gold standard
point and line feature residuals, and (cid:107) · (cid:107)Σ denotes the Ma-
halanobis distance with covariance Σ. The ’gold standard’
residual for a point [13] is

i , ¯rln

¯rpt(x, R, t) = u − π(Rx + t),

(19)

1

where the projection function π is deﬁned as π(ˆx) :=
ˆx(3) x(1:2), and ˆx = Rx + t. A ’gold standard’ residual
for a line is

¯rln(p, q, R, t) =

(cid:20) lT π(Rp + t)
lT π(Rq + t)

(cid:21)

.

(20)

Visual odometry systems, e.g. [25, 26], use the 2D co-
variance of the feature detection as the residual covariance.
This corresponds to setting Σ¯rpt = Σu, Σ¯rln = σ2
l I, and
we will dub this scheme as standard reﬁnement. It is of-
ten implemented based on a fast and efﬁcient Levenberg-
Marquardt method.

In our case, we wish to use the full residual covariance,
including both 2D and 3D uncertainty. For the point resid-
ual it is

Σ¯rpt = Σu + J(ˆx)RΣxRT JT (ˆx),
where J is a Jacobian of π with respect to ˆx; for the line
residual the covariance is

(21)

Σ¯rln = σ2

l I + diag (cid:0)lT Σπ

ˆp l, lT Σπ

ˆq l(cid:1) ,

(22)

where Σπ
ˆf
{p, q}.

= J(ˆf )RΣf RT JT (ˆf ), for ˆf = {ˆp, ˆq}, f =

The covariances in the form (21, 22) are not constant
with respect to the camera pose. They cannot be used in a
classical Gauss-Newton scheme.The cost (18) can be mini-
mized using non-linear minimization, deﬁning a full uncer-
tain reﬁnement method.

However, the full uncertain reﬁnement has a downside
of being computationally inefﬁcient compared to a stan-
dard reﬁnement. Therefore, we propose a technique re-
sembling Iterative Reweighted Least Squares, in which we
make Gauss-Newton iterations, but update the estimate of

5

the covariances (21, 22) on each step. We call it (itera-
tive) uncertain reﬁnement. This technique results in similar
accuracy to the full uncertain reﬁnement, but in terms of
computational efﬁciency is comparable to the standard re-
ﬁnement, as we show in the experiments section below. In
the following section, we explain our approach to obtaining
the point and line uncertainties.

3.6. Obtaining the Uncertainties

The 2D feature uncertainties can be obtained from a fea-
ture detector, e.g. for a multiscale pyramidal detector with
a scale step of κ we estimate Σu = σ2
o, where
σo = κo−1(cid:15), and o is a level of the image pyramid to which
the feature belongs, (cid:15) is the feature detection accuracy.

l = σ2

oI, σ2

Uncertainty for the 3D point can be estimated after the
triangulation following a standard error propagation tech-
nique, e.g. [13], Chapter 5. While there exists a single nat-
ural 3D point parameterization, the situation with line fea-
tures is less clear. The line covariance formulation depends
on the representation used for line triangulation, and there
are several known parameterizations of lines,see [5, 46, 29].
As long as we represent a line in 3D through the endpoints
of some 3D line segment, we require a method to accu-
rately ﬁnd these endpoints and their covariances. We re-
construct the endpoints as the unknowns, and for the ﬁrst
camera we use the point-based reprojection residuals, while
for the other cameras we use the line-based residuals, see
the supp. mat. This way, we can use error propagation to
obtain the line endpoint uncertainties after triangulating the
line.

xI, where σ2

If we have an arbitrary positive semi-deﬁnite covariance
Σx of a 3D point, an isotropic approximation for it would
be σ2
3 trace(Σx). This approximation is
optimal in the Frobenius norm sense: (cid:107)Σx − σ2
F =
(cid:80)3
i=1(ρ2
i are the singular values of Σx.
We have described the new methods and now continue

x)2, where ρ2

x = 1

i − σ2

xI(cid:107)2

with evaluating them in a synthetic and real settings.

4. Experiments

We compare the proposed uncertainty-aware pose
solvers to competitive pose estimation methods, in isola-
tion and combined with a standard or uncertain reﬁnement,
see Section 3.5. We use asterisk ∗ to denote the methods
receiving pose hypothesis. We compare against EPnP [20],
DLS [14], 2D covariance-aware methods CEPPnP [6] and
MLPnP [40], the state-of-the-art PnP method OPnP [47]
in case of points, and EPnPL and OPnPL [41] in case of
points and lines mixture.

We use RANSAC [9] with P3P [17] to estimate inliers
before feeding them into the solvers, while also comparing
against the P3P baseline which does not use any PnP pose
solver. After the solvers, we optionally run inlier ﬁltering
using the obtained pose, followed by a motion-only bundle

Figure 3. Pose errors in synthetic experiments. Point-based pose estimation, increasing the number of points in case of 2D noise (Top) and
2D+3D noise (Center), left legend. Bottom: increasing the number of lines and points in case of 2D+3D noise, right legend. We report
mean and median rotation and translation errors. Asterisk denotes rough pose hypothesis input. In case of 2D noise the new approaches
In case of the line features, the new
reach the state-of-the-art accuracy, in case of 3D noise they outperform the published methods.
solvers outperform the published EPnPL and OPnPL as well as the proposed uncertainty aware point-only methods. The access to a pose
hypothesis does not result in better accuracy.

adjustment, inspired by the localization modules of ORB-
SLAM2 [26] or COLMAP [34]. We use MATLAB imple-
mentations of the methods, run our experiments on a laptop
with Core i7 1.3 GHz with 16Gb RAM.

4.1. Synthetic experiments

In the synthetic setting we compare the proposed pose
estimation methods EPnP(L)U and DLS(L)U against the
baselines in isolation, as well as the proposed uncertain re-
ﬁnement against the standard reﬁnement, as deﬁned in Sec-
tion 3.5.

Metrics. We evaluate the results in terms of the absolute
rotation error erot = |acos(0.5(trace(RT
trueR)−1))| in degrees
and relative translation error etrans = (cid:107)ttrue − t(cid:107)/(cid:107)ttrue(cid:107) ×
100, in %, where Rtrue, ttrue is the true pose and R, t is the
estimated one.

Data generation. We assume a virtual calibrated cam-
era with an image size of 640 × 480 pix., a focal length
of 800 and a principal point in the image center.
3D
points and endpoints of 3D line segments are generated in

the box [−2, 2] × [−2, 2] × [4, 8] deﬁned in camera coordi-
nates. 3D-to-2D correspondences are then deﬁned by pro-
jecting the 3D points under the random rotation matrix and
translation vector. We move the 3D line endpoints randomly
along the line by a randomly generated Gaussian shift with
a standard deviation equal to 10% of the 3D line length,
see [41].

We add noise of varying magnitude to the 2D point or
line endpoint projections, as well as to the 3D points or line
endpoints,

splitting npt points into 10 subsets with an equal number
of points in order to introduce differences in the noise mag-
nitude. Each subset is corrupted by Gaussian noise with
an increasing value of standard deviation, from σ = 0.05
to σ = 0.5. We consider anisotropic covariances, which
are computed by randomly picking a rotation and a triplet
{σ, σ1, σ2}, where σ1, σ2 are random values chosen within
the interval (0, σ]. The covariance axes are scaled and ro-
tated according to a triplet of standard deviations and the
rotation value, respectively. We perform exactly the same

6

0204060801001516171819202122Points, 2D Noise10305070901100.00.51.01.5Rotation Err. (deg.)Mean Rotation10305070901100.00.51.01.5Median Rotation10305070901100.00.10.20.30.4Translation Err. (%)Mean Translation10305070901100.00.10.10.20.2Median Translation0204060801001516171819202122Points, 3D+2D Noise10305070901100.01.02.03.0Rotation Err. (deg.)10305070901100.01.02.03.010305070901100.00.51.01.5Translation Err. (%)10305070901100.00.51.01.50204060801001516171819202122Points+Lines, 3D+2D Noise1030507090110Number of Points0.01.02.03.0Rotation Err. (deg.)1030507090110Number of Points0.01.02.03.01030507090110Number of Points0.00.51.01.5Translation Err. (%)1030507090110Number of Points0.00.51.01.5EPnP+GNDLSOPnPCEPPnPMLPnPDLSUDLSUxEPnPUEPnPU*EPnPLOPnPLDLSLUEPnPLUDLSLU*EPnPLU*DLSUEPnPUPoints

Points + 2D Uncertainty

Points + Full Uncertainty, Proposed

P3P [17] EPnP [20] DLS [14] OPnP [47] CEPPnP [6] MLPnP [40] EPnPU* EPnPU DLSU*
erot

DLSU
erot etrans erot etrans erot etrans erot etrans

erot etrans

etrans erot

etrans

etrans

etrans

erot

N 8.6
S 5.1
U 5.0

35.2 4.5
14.4 4.0
14.0 3.5

24.0
12.8
13.2

5.5 18.1
5.0 12.2
5.0 12.9

N 15.7
S 9.2
U 9.1

3.3
1.2
1.2

9.5
9.0
9.0

1.5
1.2
1.2

9.3
9.0
9.0

1.4
1.2
1.2

10.0
9.7
10.3

7.8 277.6 8.2
7.2 242.2 5.3
7.6 325.5 5.1

erot

erot

etrans
KITTI [11], sequences 00-02
27.2
49.5
14.4
20.6
35.3
17.4
TUM [37], ’freiburg1’ sequences
1.6
1.7
1.5
1.2
1.3
1.3
2.0
1.2
1.3

10.0
9.2
9.6

10.0
9.4
9.2

5.8
5.3
6.3

4.2 22.2 5.1 23.9 5.6 32.2 6.0 14.9
3.7 12.6 3.9 13.1 5.1 25.5 5.1 12.1
3.3 10.6 3.5 13.4 5.0 12.6 5.0 10.9

9.3
9.0
9.0

1.3
1.2
1.1

9.4
9.0
9.0

1.4
1.2
1.2

9.3
9.0
9.0

1.3
1.2
1.1

9.1
9.0
9.0

1.2
1.2
1.1

Table 1. Motion estimation from 2D-3D point correspondences on KITTI [11] TUM [37] in terms of mean absolute rotation erot
(in 0.1×deg.) and translation etrans (in cm.) errors. We compare proposed full uncertainty-aware methods against point-based
PnP and 2D uncertainty-aware methods in isolation (N), with standard (S) and proposed uncertain (U) reﬁnement. Methods with
’*’ receive a pose from RANSAC, best for the dataset is in bold italic, best for each protocol (N,S or U) is in bold. The new
methods outperform the baselines in most metrics,e.g. DLSU in isolation improves etrans on KITTI by 3 cm (18%) compared to
the best performing baseline DLS. Uncertain (U) is mostly better than standard (S) for the proposed methods, e.g. etrans by 2 cm
(16%) for EPnPU* on KITTI.

addition of noise to the 3D endpoints of line segments. We
add noise with different variance to the point and line end-
point projections using the same mechanism increasing the
standard deviation from σ = 1 to σ = 10.

We perform 400 simulation trials. The experiment set-
tings are consistent with [47, 7, 6, 41]. We evaluate the
pose solvers in isolation in a point-only and a point+line set-
ting, providing the methods marked by ’*’ a pose hypothe-
sis computed from a randomly chosen subset of three points
using P3P [17]. We change npt = 10 to 110, in two differ-
ent setups: introducing noise to the projected 2D features
and introducing noise also to the 3D points or endpoints. In
the case of experiments with lines and points, we generate
nl = npt line correspondences in addition to points.

Results. Fig. 3 summarizes the results of the experi-
ments.
In the 2D noise experiment for points (top row),
the proposed methods perform similarly with 2D methods,
however for npt < 30 the MLPnP delivers slightly better
results, probably due to additional reprojection error reﬁne-
ment step used in this method and not used in the other ones.
When we use both 3D and 2D noise for the points (cen-
tral row), the proposed methods are the most accurate, fol-
lowed by the classical PnP solvers, and the 2D covariance-
based methods. In point+line experiment, the new methods
clearly outperform the baselines. Fig. 4 shows an analysis in
terms of computational cost. The fastest are EPnPU, EPnP,
CEPPnP, MLPnP, followed by DLSU, DLS and OPnP.

In Fig. 5, we compare the proposed uncertain reﬁne-
ment against the standard method for point features, see
Section 3.5. For the inlier ﬁltering, we use a threshold
τ 2 = 62 for the covariance-weighted squared residuals (19)
corresponding to the standard or the uncertain reﬁnement.
The data is generated as in the experiment with 3D and 2D
noise. We consider EPnP, MLPnP and EPnPU*. The un-
certain reﬁnement is beneﬁcial for all considered solvers;
the margin between different pose solvers after reﬁnement

Figure 4. Runtime (ms). Methods based on points (left) or points
and lines (right). See Fig. 3 for the legends.

Figure 5. Comparison of methods with standard(+SRef) or uncer-
tain(+URef) reﬁnement, in a 2D + 3D noise setup as in Fig. 3, cen-
tral row. Uncertain reﬁnement improves accuracy, the uncertainty-
aware EPnPU* is slightly better than the other methods.

decreases, but remains, because the more accurate pose
solvers can provide a better set of inliers for the ﬁnal reﬁne-
ment step; see additional results on timing and comparison
against the full uncertain reﬁnement in the supp. mat.

Summarizing, the new methods outperform baselines in
a synthetic setting. In the next section, we show that the
same holds for the real scenarios.

4.2. Real experiments

Data. We use three monocular RGB sequences 00-02
from the KITTI dataset [11] and the ﬁrst three ’freiburg1’
monocular RGB sequences of the TUM-RGBD dataset [37]
to evaluate the methods. We use KITTI in a monocular

7

1030507090110Number of Points0.05.010.015.0Med. runtime (ms)Points1030507090110Number of Points0.05.010.015.0Med. runtime (ms)Points+Lines30507090110Number of Points0.01.02.03.0Rotation Err. (deg.)Mean Rotation30507090110Number of Points0.00.51.01.5Translation Err. (%)Mean TranslationP3P+URefP3P+SRefEPnP+URefEPnP+SRefMLPnP+URefMLPnP+SRefEPnPU+URefEPnPU+SRefPoints+Lines

Points+Lines+Uncertainty

EPnPL [41] OPnPL [41] DLSLU*
erot
N 2.5
S 1.8
U 1.4

erot
erot etrans erot
10.2 650.1 6.3 18.2 3.4
267.4 5.2 12.2 1.8
6.8
497.7 5.2 12.0 1.4
9.0

etrans
37.1
20.4
12.1

etrans

EPnPLU*
etrans
25.2
9.8
9.3

Table 2. Motion estimation from 2D-3D point and line correspon-
dences on KITTI [11] sequences 00-02. We report the mean ro-
tation errors in 0.1×degrees and translation errors in cm, for the
solvers in isolation (N), after standard (S) and uncertain (U) re-
ﬁnement, see Section 3.5. Proposed EPnPLU*, DLSLU* mostly
outperform the baselines OPnPL and EPnPL, e.g. etrans by 23%-
52% (3 - 11 cm.), while EPnPL has the best rotation accuracy in
isolation.

P3P EPnP DLS OPnP

N 3.1
4.6
S 12.1 13.2
U 11.3 12.7

16.5
25.1
24.5

10.7
19.5
18.8

ML-
CEP-
PnP
PnP
5.0
7.6
13.7 16.0
13.1 15.2

EPnPU DLSU

6.1
14.7
13.9

8.0
16.7
16.0

Table 3. Average running time (ms) for the compared methods
on KITTI in isolation (N), with standard (S) or uncertain (U)
reﬁnement.

mode, taking a temporal window of two left frames ( three
frames with a pose distance > 2.5cm for TUM) to detect
and describe features, relying on FAST [32] and ORB [33]
for points and EDLines [3] and LBD [44] for lines, OpenCV
implementations. We use an image pyramid with no = 8
levels and a factor κ = 1.2, the detection error (cid:15) = 1 pix, the
uncertainty is calculated as described in Section 3.6. The
features are matched using standard brute-force approach,
and triangulated using ground-truth camera poses. Trian-
gulation results are reﬁned with Ceres [2], producing also
the 3D feature covariances, see supp. mat. for the detailed
formulation. The next left frame in KITTI (the next RGB
frame in TUM) is used for evaluation. Line detections are
ﬁltered by length (less than 25 pix. removed). We use a
threshold of τ = 5.991 for the covariance-weighted residual
norms in RANSAC. In case of point+line combination, we
generate minimal sets using only points, include the lines
into the motion-only bundle adjustment for the line-aware
solvers.

Protocol and metrics. We compare the methods using
absolute rotation error in deg. and absolute translation error
in cm. If a pose solver fails or produces less than 3 inliers,
we do not use its output, but use the output of RANSAC
instead, following [34, 26].

Results. We evaluate the pose solvers in isolation, see
Table 1, and show a signiﬁcant increase in accuracy com-
pared to the state-of-the-art, e.g. DLSU on KITTI outper-
forms the closest baseline DLS by 18% in mean transla-
tion. On the TUM sequences, the mean rotation errors of
the proposed methods are similar to the ones of the base-
lines, while there are more signiﬁcant gains in translation

Figure 6. CDF plots for real experiments on KITTI [11] and
TUM [37], U mode. EPnPU* and DLSU* are the most accurate.

Figure 7. Comparison of inlier sets from EPnPU* (red) and
EPnP (green), blue squares show the true inliers. EPnPU* inlier
sets are more complete.

errors. The proposed uncertain reﬁnement improves accu-
racy over standard reﬁnement for most of the solvers, e.g.
by 16% in case of EPnP on KITTI, see Table 1 and CDF
error plots in Fig. 6; the running time of the methods is in
Table 3. In the supp. mat. we give additional results, in-
cluding median errors. See Fig. 7 for a visual comparison
of the inlier sets estimated by EPnP and EPnPU*.

In Table 2 we report the mean errors of the points-and-
lines-based pose estimation for the proposed solvers. We
observe an improvement in the translation errors by almost
50% for the solvers in isolation and by 24% after the stan-
dard reﬁnement, compared to the uncertainty-free EPnPL
and OPnPL [41] methods.

5. Conclusions

We have generalized PnP(L) methods to estimate the
camera pose with uncertain 2D feature detections and
3D feature locations and proposed a new pose reﬁnement
scheme. Our methods demonstrate increased accuracy and

8

0204060801001516171819202122KITTI05100.9960.9970.9980.9991Rotation CDF020400.9980.99850.9990.99951Translation CDF0204060801001516171819202122TUM0510Rotation Err. (deg.)0.9960.998100.050.1Translation Err., m.0.9980.9991EPnPDLSOPnPCEPPnPMLPnPEPnPU*DLSU*robustness both in synthetic and real experiments.

References

[1] Y. Abdel-Aziz, H. Karara, and M. Hauck. Direct linear trans-
formation from comparator coordinates into object space co-
ordinates in close-range photogrammetry. Photogrammetric
Engineering & Remote Sensing, 81(2):103–107, 2015. 2
[2] S. Agarwal and K. Mierle. Ceres solver: Tutorial & refer-

ence. Google Inc, 2:72, 2012. 8

[3] C. Akinlar and C. Topal. EDLines: A real-time line segment
detector with a false detection control. Pattern Recognition
Letters, 32(13):1633–1642, 2011. 8

[4] A. Ansar and K. Daniilidis. Linear pose estimation from
points or lines. Pattern Analysis and Machine Intelligence,
IEEE Transactions on, 25(5):578–589, 2003. 2

[5] A. Bartoli and P. Sturm.

Structure-from-motion using
lines: Representation, triangulation, and bundle adjustment.
Computer vision and image understanding, 100(3):416–441,
2005. 5

[6] L. Ferraz, X. Binefa, and F. Moreno-Noguer. Leveraging
In Proceedings of
feature uncertainty in the PnP problem.
the BMVC 2014 British Machine Vision Conference, pages
1–13, 2014. 1, 2, 5, 7

[7] L. Ferraz, X. Binefa, and F. Moreno-Noguer. Very fast so-
lution to the PnP problem with algebraic outlier rejection.
In Computer Vision and Pattern Recognition (CVPR), 2014
IEEE Conference on, pages 501–508. IEEE, 2014. 2, 7
[8] P. D. Fiore. Efﬁcient linear solution of exterior orientation.
IEEE Transactions on Pattern Analysis & Machine Intelli-
gence, (2):140–148, 2001. 2

[9] M. A. Fischler and R. C. Bolles. Random sample consen-
sus: a paradigm for model ﬁtting with applications to image
analysis and automated cartography. Communications of the
ACM, 24(6):381–395, 1981. 5

[10] W. F¨orstner and B. P. Wrobel. Photogrammetric computer

vision. Springer, 2016. 3

[11] A. Geiger, P. Lenz, and R. Urtasun. Are we ready for
autonomous driving? the KITTI vision benchmark suite.
In 2012 IEEE Conference on Computer Vision and Pattern
Recognition, pages 3354–3361. IEEE, 2012. 7, 8

[12] S. Hadﬁeld, K. Lebeda, and R. Bowden. HARD-PnP:
PnP optimization using a hybrid approximate representation.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, 41(3):768–774, 2019. 2

[13] R. Hartley and A. Zisserman. Multiple view geometry in
computer vision. Cambridge university press, 2003. 2, 4,
5

[14] J. A. Hesch and S. I. Roumeliotis. A direct least-squares
(DLS) method for PnP. In Computer Vision (ICCV), 2011
IEEE International Conference on, pages 383–390. IEEE,
2011. 1, 2, 4, 5, 7

[15] T. Holzmann, F. Fraundorfer, and H. Bischof. Direct stereo
visual odometry based on lines. In Proceedings of the 11th
International Joint Conference on Computer Vision, Imag-
ing and Computer Graphics Theory and Applications, 2016,
pages 1–11, 2016. 1

[16] L. Kneip, H. Li, and Y. Seo. UPnP: An optimal O(n) solution
to the absolute pose problem with universal applicability.
In Computer Vision–ECCV 2014, pages 127–142. Springer,
2014. 1, 2

[17] L. Kneip, D. Scaramuzza, and R. Siegwart. A novel
parametrization of the perspective-three-point problem for a
direct computation of absolute camera position and orienta-
tion. In Computer Vision and Pattern Recognition (CVPR),
2011 IEEE Conference on, pages 2969–2976. IEEE, 2011.
5, 7

[18] Y. Kuang, Y. Zheng, and K. Astrom. Partial symmetry in
polynomial systems and its applications in computer vision.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 438–445, 2014. 2

[19] V. Larsson, K. Astrom, and M. Oskarsson. Efﬁcient solvers
In Pro-
for minimal problems by syzygy-based reduction.
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 820–829, 2017. 4

[20] V. Lepetit, F. Moreno-Noguer, and P. Fua. EPnP: An accurate
O(n) solution to the PnP problem. International Journal of
Computer Vision, 81(2):155–166, 2009. 1, 2, 3, 4, 5, 7
[21] S. Li, C. Xu, and M. Xie. A robust O(n) solution to the
perspective-n-point problem. Pattern Analysis and Machine
Intelligence, IEEE Transactions on, 34(7):1444–1450, 2012.
2

[22] C.-P. Lu, G. D. Hager, and E. Mjolsness. Fast and glob-
ally convergent pose estimation from video images. Pattern
Analysis and Machine Intelligence, IEEE Transactions on,
22(6):610–622, 2000. 2

[23] F. M. Mirzaei, S. Roumeliotis, et al. Globally optimal pose
estimation from line correspondences. In Robotics and Au-
tomation (ICRA), 2011 IEEE International Conference on,
pages 5581–5588. IEEE, 2011. 2

[24] F. Moreno-Noguer, V. Lepetit, and P. Fua. Accurate non-
iterative O (n) solution to the PnP problem. In 2007 IEEE
11th International Conference on Computer Vision, pages 1–
8. IEEE, 2007. 2

[25] R. Mur-Artal, J. Montiel, and J. D. Tardos. ORB-SLAM: a
versatile and accurate monocular SLAM system. Robotics,
IEEE Transactions on, 31(5):1147–1163, 2015. 2, 5

[26] R. Mur-Artal and J. D. Tard´os. ORB-SLAM2: An open-
source slam system for monocular, stereo, and RGB-D cam-
IEEE Transactions on Robotics, 33(5):1255–1262,
eras.
2017. 5, 6, 8

[27] G. Nakano. Globally optimal DLS method for PnP prob-
lem with cayley parameterization. In British Machine Vision
Conference 2015, Proceedings of. BMVA, 2015. 2

[28] D. Oberkampf, D. F. DeMenthon, and L. S. Davis. Iterative
pose estimation using coplanar feature points. Computer Vi-
sion and Image Understanding, 63(3):495–511, 1996. 2
[29] A. Pumarola, A. Vakhitov, A. Agudo, A. Sanfeliu, and
F. Moreno-Noguer. PL-SLAM: real-time monocular visual
In Robotics and Automa-
SLAM with points and lines.
tion (ICRA), 2017 IEEE International Conference on, pages
4503–4508. IEEE, 2017. 1, 3, 5

[30] B. Pˇribyl, P. Zemˇc´ık, et al. Camera pose estimation from
lines using Pluecker coordinates. In British Machine Vision
Conference 2015, Proceedings of. BMVA, 2015. 2

9

[46] Y. Zhao and P. A. Vela. Good line cutting: towards accurate
pose tracking of line-assisted VOVSLAM. In Proceedings
of the European Conference on Computer Vision (ECCV),
pages 516–531, 2018. 5

[47] Y. Zheng, Y. Kuang, S. Sugimoto, K. Astrom, and M. Oku-
tomi. Revisiting the PnP problem: a fast, general and optimal
solution. In Computer Vision (ICCV), 2013 IEEE Interna-
tional Conference on, pages 2344–2351. IEEE, 2013. 1, 2,
5, 7

[48] Y. Zheng, S. Sugimoto, I. Sato, and M. Okutomi. A general
and simple method for camera pose and focal length determi-
nation. In Computer Vision and Pattern Recognition (CVPR),
2014 IEEE Conference on, pages 430–437. IEEE, 2014. 2

[49] L. Zhou, Y. Yang, M. Abello, and M. Kaess. A robust and
efﬁcient algorithm for the PnL problem using algebraic dis-
tance to approximate the reprojection distance. In AAAI Con-
ference on Artiﬁcial Intelligence, AAAI, Honolulu, Hawaii,
USA, Jan. 2019. 2

[31] L. Quan and Z. Lan. Linear n-point camera pose determi-
nation. Pattern Analysis and Machine Intelligence, IEEE
Transactions on, 21(8):774–780, 1999. 2

[32] E. Rosten and T. Drummond. Machine learning for high-
In Computer Vision–ECCV 2006,

speed corner detection.
pages 430–443. Springer, 2006. 8

[33] E. Rublee, V. Rabaud, K. Konolige, and G. Bradski. ORB:
an efﬁcient alternative to SIFT or SURF. In Computer Vi-
sion (ICCV), 2011 IEEE International Conference on, pages
2564–2571. IEEE, 2011. 8

[34] J. L. Schonberger and J.-M. Frahm. Structure-from-motion
revisited. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 4104–4113,
2016. 2, 6, 8

[35] G. Schweighofer and A. Pinz. Globally optimal O(n) so-
lution to the PnP problem for general camera models.
In
BMVC, pages 1–10, 2008. 2

[36] J. Sola, T. Vidal-Calleja, J. Civera, and J. M. M. Montiel. Im-
pact of landmark parametrization on monocular EKF-SLAM
with points and lines. International journal of computer vi-
sion, 97(3):339–368, 2012. 1

[37] J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cre-
mers. A benchmark for the evaluation of RGB-D SLAM sys-
tems. In Proc. of the International Conference on Intelligent
Robot Systems (IROS), Oct. 2012. 7, 8

[38] B. Tekin, S. N. Sinha, and P. Fua. Real-time seamless sin-
In Proceedings of the
gle shot 6d object pose prediction.
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 292–301, 2018. 2

[39] B. Triggs, P. F. McLauchlan, R. I. Hartley, and A. W. Fitzgib-
In Inter-
bon. Bundle adjustment—a modern synthesis.
national workshop on vision algorithms, pages 298–372.
Springer, 1999. 5

[40] S. Urban, J. Leitloff, and S. Hinz. Mlpnp - a real-time maxi-
mum likelihood solution to the perspective-n-point problem.
ISPRS Annals of Photogrammetry, Remote Sensing and Spa-
tial Information Sciences, III-3:131–138, 2016. 1, 2, 5, 7
[41] A. Vakhitov, J. Funke, and F. Moreno-Noguer. Accurate
and linear time pose estimation from points and lines.
In
European Conference on Computer Vision, pages 583–599.
Springer, 2016. 1, 2, 3, 5, 6, 7, 8

[42] C. Xu, L. Zhang, L. Cheng, and R. Koch. Pose estimation
from line correspondences: A complete analysis and a se-
ries of solutions. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 39(6):1209–1222, 2017. 2

[43] G. Zhang, J. H. Lee, J. Lim, and I. H. Suh. Building a 3-D
line-based map using stereo SLAM. IEEE Transactions on
Robotics, 31(6):1364–1377, 2015. 1

[44] L. Zhang and R. Koch. An efﬁcient and robust line segment
matching approach based on LBD descriptor and pairwise
Journal of Visual Communication
geometric consistency.
and Image Representation, 24(7):794–805, 2013. 8

[45] L. Zhang, C. Xu, K.-M. Lee, and R. Koch. Robust and ef-
ﬁcient pose estimation from line correspondences. In Asian
Conference on Computer Vision, pages 217–230. Springer,
2012. 2

10

