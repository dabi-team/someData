AR-sieve Bootstrap for High-dimensional Time Series

Daning Bi
The Australian National University

Han Lin Shang
Macquarie University

Yanrong Yang *
The Australian National University

Huanjun Zhu
Xiamen University

December 2, 2021

Abstract

This paper proposes a new AR-sieve bootstrap approach on high-dimensional time series.
The major challenge of classical bootstrap methods on high-dimensional time series is two-
fold: the curse dimensionality and temporal dependence. To tackle such difﬁculty, we utilise
factor modelling to reduce dimension and capture temporal dependence simultaneously.
A factor-based bootstrap procedure is constructed, which conducts AR-sieve bootstrap on
the extracted low-dimensional common factor time series and then recovers the bootstrap
samples for original data from the factor model. Asymptotic properties for bootstrap mean
statistics and extreme eigenvalues are established. Various simulations further demonstrate
the advantages of the new AR-sieve bootstrap under high-dimensional scenarios. Finally,
an empirical application on particulate matter (PM) concentration data is studied, where
bootstrap conﬁdence intervals for mean vectors and autocovariance matrices are provided.

Keywords: Autocovariance matrices; Factor models; High-dimensional time series; AR-sieve bootstrap;
Temporal dependence

1
2
0
2

c
e
D
1

]
E
M

.
t
a
t
s
[

1
v
4
1
4
0
0
.
2
1
1
2
:
v
i
X
r
a

*Postal address: Research School of Finance, Actuarial Studies and Statistics, Australian National University, 1

Kingsley Street, ACT, 2601, Australia; Telephone: +61(2) 6125 8975; Email: yanrong.yang@anu.edu.au

1

 
 
 
 
 
 
1 Introduction

The bootstrap is a computer-intensive resampling-based methodology that arises as an alter-

native to asymptotic theory. The bootstrap method, initially introduced by Efron (1979) for

independent sample observations, is later extended to more complicated dependent data in the

literature. As an important extension to stationary time series, blockwise bootstrap (K ¨unsch

1989), autoregressive (AR) sieve bootstrap (Kreiss 1988, B ¨uhlmann 1997), and frequency-domain

bootstrap (Franke & Hardle 1992, Dahlhaus & Janas 1996) have received the most discussions

and developments in the past few years. A few variants of block bootstrap methods have

appeared, such as the block bootstrap for time series with ﬁxed regressors (Nordman & Lahiri

2012), the double block bootstrap (Lee & Lai 2009), and the stationary bootstrap (Politis &

Romano 1994), among others. An apparent disadvantage of blockwise bootstrap is neglected

dependence between different blocks. The AR-sieve bootstrap method takes up the “sieve”

strategy, which approximates stationary time series by an AR model with a large number of time

lags. Compared with the blockwise bootstrap, the AR-sieve bootstrap samples are conditionally

stationary and keep the dependence structure well. The AR-sieve bootstrap is introduced by

Kreiss (1988) and has been well studied from stationary linear processes (B ¨uhlmann 1997) to
strictly stationary time series fulﬁlling a general moving average MA (∞) representation (Kreiss

et al. 2011). After this work, the theoretical requirement and validity of a general AR-sieve boot-

strap method for certain type of statistics have been discussed for univariate (Kreiss et al. 2011),

multivariate (Meyer & Kreiss 2015) and functional time series (Paparoditis 2018, Paparoditis

& Shang 2021), respectively. The frequency-domain bootstrap to implement the resampling

schemes is based on frequency-domain methods. The motivation behind this method comes

from the observation that periodogram ordinates at a ﬁnite number of frequencies are approx-

imately independent distributed so that Efron’s ideas may be employed. Compared to the

AR-sieve bootstrap, this method could deal with more general dependence structures for time

series (Meyer et al. 2020, Hidalgo 2021).

The main goal of this paper is to extend the AR-sieve bootstrap to high-dimensional time

series. Due to the curse of dimensionality, traditional AR-sieve bootstrap fails in the high-

dimensional case. This is because the AR model approximation for high-dimensional time series

could result in a large approximation error, and the bootstrap procedure on high-dimensional

i.i.d residual is also inaccurate. The curse of dimensionality on traditional bootstrap methods

is demonstrated vividly in El Karoui & Purdom (2018). As a remedy, reducing the parameter

space is essential for successful modifying bootstrap methods. Fitting sparse models and

2

low-rank models to high-dimensional data are two commonly used techniques to eliminate

the curse of dimensionality. Chernozhukov et al. (2017) provide theoretical guarantee on the

bootstrap approximation for the distribution of the sample mean vector for high-dimensional

i.i.d data. Chen (2018) studies the bootstrap approximation for U-statistics constructed by

high-dimensional i.i.d data. Ahn & Reinsel (1988) propose a nested reduce-rank structure for

coefﬁcients in multivariate AR time series model. For high-dimensional time series, Krampe

et al. (2021) consider AR-sieve bootstrap for vector AR time series with sparse coefﬁcients.

In this article, we will contribute to proposing an appropriate low-rank model for AR-sieve

bootstrap on high-dimensional stationary time series.

Factor modelling or low-rank representation can project high-dimensional data into low-

dimensional subspace. Principal component analysis (PCA) is a common technique to pursue

projections or subspace with the most variation in the original data (Bai & Ng 2002, Fan et al.

2011). Identifying the low-dimensional representation for high-dimensional time series is more

complicated because keeping the temporal dependence in dimension reduction is a crucial

requirement. Earlier literature of this ﬁeld on multivariate time series is vast and includes the

canonical correlation analysis (Box & Tiao 1977), the factor models (Pena & Box 1987), and the

scalar component model (Tiao & Tsay 1989). Later Lam et al. (2011) study a factor model for

high-dimensional time series based on an accumulation of autocovariance matrices, aiming to

capture all temporal dependence by common factors.

In this article, we reduce high-dimensional time series based on a factor model whose com-

mon factors possess all temporal dependence of the original time series. Efﬁcient estimation for

such factor model is borrowed from the idea of Lam et al. (2011), which conduct eigendecompo-

sition for a set of autocovariance matrices with various time-lags. With the lower-dimensional

common factors time series, the AR-sieve bootstrap is feasible and produces bootstrap samples

for common factors. Finally, the AR-sieve bootstrap could recover the relationship between

common factors and the original high-dimensional time series.

We also study the theoretical properties of the proposed AR-sieve bootstrap on two com-

monly used statistics - the mean statistics and spectral statistics of autocovariance matrices.

Common factors stand at a “representation and activation” position in the whole bootstrap

method. Under the scenario of comparable N (the dimension) and T (the time-serial length), we

ﬁrst provide convergence rates for the estimation of common factors, which could affect statisti-

cal properties of the ﬁnal AR-sieve bootstrap statistics. Further, for the two high-dimensional

statistics under consideration, the consistency of the bootstrap versions to the population ver-

sions is established. Finite-sample experiments demonstrate the inﬂuences of the dimension,

3

the sample size and factors’ strength on the bootstrap results. Moreover, we also conduct an em-

pirical application on PM10 data. As a by-product of interest, we apply the proposed AR-sieve

bootstrap for high-dimensional series on sparsely-observed discrete functional time series and

compare them with the results from AR-sieve bootstrap for functional time series (Paparoditis

2018). Due to the smoothing inaccuracy for sparsely-observed discrete functional time series,

the high-dimensional bootstrap method sometimes results in better statistical inferences than

the functional approach by smoothing them. Various simulations in the appendix could reﬂect

this point.

The rest of this paper is organised as follows. Section 2 introduces factor models for high-

dimensional time series and discusses the AR representation of the factor time series, a building

block of the general AR-sieve bootstrap. In Section 3, the estimation procedure for factor models

and the AR-sieve bootstrap procedure for factor time series is introduced with regularity

conditions on factor models. The additional assumptions and asymptotic validity of our novel

AR-sieve bootstrap method are discussed in Section 4. Mean statistics of factor time series and

spiked eigenvalues of symmetrised autocovariance matrices are introduced. In Section 5, via

several simulation experiments, we verify the validity of our novel AR-sieve bootstrap methods

on the mean statistics and the spiked eigenvalues of symmetrised autocovariance matrices.

Section 6 provides an example of applying our novel AR-sieve bootstrap method to PM10 data.

Conclusions are presented in Section 7. In the supplementary, Appendix A explores the impact

of density of discrete functional time-series observations on pre-smoothing results. Technical

proofs and auxiliary lemmas are presented in Appendices B and C in additional supplementary

documents.

2 Factor-based AR-sieve Representation

In this section, we ﬁrst propose a factor model to project the high-dimensional time series into

a lower-dimensional subspace. Common-factors time series could represent the original data

to capture the most temporal dependence. Secondly, an AR-sieve representation for common

factors is provided, which plays a signiﬁcant role in the AR-sieve bootstrap.

Consider a stationary N-dimensional time series {yt ∈ RN, t ∈ Z} following a general

unobservable factor model

yt = Qft + ut,

t = 1, 2, . . . , T,

(1)

where {ft ∈ Rr, t ∈ Z} are unobserved r-dimensional factor time series, Q is an N × r factor

4

loading matrix and {ut ∈ Rr, t ∈ Z} are N-dimensional white noises with zero means and
covariance matrix Σu.

Factor models have received numerous discussions, and there are various identiﬁcation

conditions and assumptions on Q, ft and ut depending on various aims. In our work, we adapt

the identiﬁcation condition in Lam et al. (2011) to consider a factor model where temporal

dependence of {yt} can be fully captured by the factors {ft}. It is noteworthy that we do

not require a direct dynamic system on {ft}. Therefore we still maintain a static relationship

between {yt} and {ft}.

Next, we introduce an AR-sieve representation for multivariate common-factors time series.

For the r × 1 common factors {ft}, we know via Wold’s theorem (see, e.g., B ¨uhlmann 1997) that

{ft} can be written as a one-sided inﬁnite-order moving-average (MA) process

ft =

∞
∑
l=1

Ψ

let−l + et, t ∈ Z,

(2)

where {et ∈ Rr, t ∈ Z} are full rank uncorrelated white-noise innovation processes with
E(et) = 0 and E(ete(cid:62)
l ∈ Rr×r, l ∈
N} are the coefﬁcients matrices.

s ) = 1t=sΣe, with Σe a full rank r × r covariance matrix. {Ψ

Under the requirement on invertibility of the process in (2), which would narrow the class of

stationary processes a little bit, we can represent {ft} as a one-sided inﬁnite-order autoregressive
(AR) process. That is, there exists an inﬁnite sequence of r × r matrices {Al ∈ Rr×r, l ∈ N}
such that factors {ft} can be expressed as

ft =

∞
∑
l=1

Alft−l + et, t ∈ Z,

(3)

where the coefﬁcient matrices of the expansion for the power series
{Ψ

are
l ∈ Rr×r, l ∈ N}. Here |z| ≤ 1 (Brockwell & Davis 1991). Note that (2) is a representation

(cid:1)

(cid:0)

Ir − ∑∞

l=1 Alzl

−1

instead of an imposed assumption or model. AR-sieve bootstrap is based on an approximated

AR representation for (3), i.e.

ft ≈

p
∑
l=1

Alft−l + et, t ∈ Z,

(4)

where p is a large integer that tends to inﬁnity, in this sense, AR-sieve bootstrap is a nonpara-

metric approach although (4) looks like a “fake“ parametric model.

The (vector) AR representation in (3) is more attractive for statistical applications and has

5

received more attention since it relates ft to its past values. AR-sieve bootstrap, on the other

hand, utilises the AR-representation in (3) to generate bootstrap common factors by resampling

from the de-centred innovations. In practice, since neither the factors {ft} or their loadings Q

are observable, AR-sieve bootstrap is performed on estimates of {ft} rather than true factors.

Hence, we will introduce the estimation and bootstrap procedure in the following section.

3 Factor-based AR-sieve bootstrap

In this section, we ﬁrst introduce the estimation approach for the factor model in (1) and then

provide the AR-sieve bootstrap procedure. Further, a ﬂow chart is shown to summarise the

whole procedure of AR-sieve bootstrap for high-dimensional time series.

3.1 Analysis on common-factors estimation

Recall that common factors {ft} in model (3) are assumed to contain all the temporal depen-

dence of {yt} because the error components {ut} have no temporal dependence. As analysed

by Bathia et al. (2010) and Lam et al. (2011), the factor loading space, that is, the r-dimensional

linear space spanned by the columns of the factor loading matrix Q, denoted by M(Q), is

uniquely deﬁned. Further, this subspace M(Q) is spanned by the eigenvectors of an accumu-

lated symmetrised autocovariance matrices below, corresponding to its nonzero eigenvalues,

L =

k0∑

k=1

Γy(k)Γy(k)(cid:62),

where Γy(k) = Cov(yt, yt+k) is the autocovariance of {yt} at lag k, for k = 1, 2, ..., k0. Intuitively
speaking, the matrix L collects the temporal dependence of {yt} by pooling up the information

contained in ﬁrst k0-lags of autocovariance with the squared (symmetrised) form facilitating the

spectral decomposition on L.

Remark 3.1. The reason of not to consider the covariance matrix Σy into L is undemanding. As
discussed by Lam et al. (2011), for the factor model (1), Σy = Γy(0) = QΓf (0)Q(cid:62) + Σu, where
Γf (0) is the covariance matrix of {yt} and Σu is the covariance matrix of {ut}. Hence to exclude
Σy from L can ﬁlter out the impact of covariance on {ut}, especially for N → ∞.

It is then straightforward to use spectral (eigenvalue) decomposition on L to estimate the

factor loading matrix Q, and the factors {ft}. Before the details of the estimation procedure, we

6

summarise the assumptions and identiﬁcation conditions for the factor model deﬁned in (1)

ﬁrst.

Assumptions 3.1 (Conditions on Factor models). For factor models (1), we impose the following

assumptions,

(i) {ft} are strictly stationary time series with Eft = 0 and E (cid:107)ft(cid:107)2 < ∞; {ut} ∼ W N(0, Σu)
are uncorrelated white noises with covariance matrix Σu, and all eigenvalues of Σu are uniformly
bounded as N → ∞; {ft} are independent of {us} for any t, s ∈ Z.

(ii)

1

N Q(cid:62)Q = Ir and for a prescribed integer k0 > 0, the r × r matrix Γ
of full rank for any k = 0, 1, ..., k0, i.e. the eigenvalues {λi(f ), i = 1, 2, ..., r} of the matrix
∑k0
f (k)(cid:62) satisfy ∞ > λ1(f ) ≥ λ2(f ) ≥ · · · ≥ λr(f ) > 0 as N → ∞.

f (k) = Cov(ft, ft+k) is

f (k)Γ

Γ

k=1

(iii) {yt}, therefore {ft}, are ψ-mixing with the mixing coefﬁcients ψ(·) satisfying the condition that

∑t≥1 ψ(t)1/2 < ∞, and E|yj,t|4 < ∞ element-wisely.

Next, we provide some comments and justiﬁcations on Assumption 3.1.

1. Assumption 3.1 (i) states the strict stationarity on {ft}, which has been used in literature

of factor models, such as Fan et al. (2013) and is commonly seen in AR-sieve bootstrap

literature, such as Kreiss et al. (2011) and Meyer & Kreiss (2015). Apart from the station-

arity, Assumption 3.1 (i) also states that factor time series {ft} and error terms {ut} are

independent at any time lags, which is stronger than the assumption in Lam et al. (2011),

but is required for us to apply bootstrap methods by resampling from the innovations

{et} in Wold representation of {ft} as in (2), since AR-sieve bootstrap does not work for

high-dimensional noises {ut}.

2. We impose Assumption 3.1 (ii) to identify the factor components {Qft} from the original
N Q(cid:62)Q = Ir and eigenvalues {λi(f ), i =
high-dimensional data. The conditions that 1
1, 2, ..., r} of ∑k0
f (k)(cid:62) fulﬁl ∞ > λ1(f ) ≥ λ2(f ) ≥ · · · ≥ λr(f ) > 0 as N → ∞
are sufﬁcient for {Qft} to be identiﬁable from {ut} when N → ∞, since the N × N matrix

f (k)Γ

k=1

Γ

L can be represented as

L =

k0∑

k=1

Γy(k)Γy(k)(cid:62) = NQ

k0∑

(cid:40)

k=1

Γf (k)Γf (k)(cid:62)

Q(cid:62),

(cid:41)

(5)

with the ﬁrst r eigenvalues of 1

N2 L non-vanishing. In other words, the columns of Q can
be considered as the eigenvectors of L corresponding to r nonzero eigenvalues scaled by

7

√

N. As a consequence, Assumption 3.1 (ii) implies the pervasiveness of r factors {ft}

when N goes to inﬁnity, which is equivalent to the strong factors’ case according to the

deﬁnition in Lam et al. (2011).

3. The ψ-mixing in Assumption 3.1 (iii) is introduced to specify the week dependence

structure of {ft}, which is considered in Lam et al. (2011) to simplify the technical proof

of consistency on loading matrix Q. However, it is not the weakest possible. In the

meantime, Assumption 3.1 (ii) together with the mixing condition in (iii) is also sufﬁcient
for the absolute summability condition on {ft} when N → ∞, which is preliminary for

AR-sieve bootstrap to be applicable on {ft}, since otherwise the Wold representation is

not guaranteed to exist (Cheng & Pourahmadi 1993).

To further explain the use of Assumption 3.1 with the estimation procedure, ﬁrst notice

that {ft} are strong factors and no linear combinations of the components of {ft} are white

be represented as in (5) with 1

noises (WN) as implied by Assumption 3.1 (ii). Recall that L is non-negative deﬁnite and can
Γf (k)Γf (k)(cid:62), the middle part of (5), is
Q(cid:62),

N Q(cid:62)Q = Ir. Since ∑k0
k=1
symmetric, we can apply spectral decomposition on it and recognise L as NQ

SDS(cid:62)

1

where S is an r × r orthonormal matrix and D is an r × r diagonal matrix. Furthermore, since
N (QS)(cid:62)(QS) = Ir, the columns of QS are in fact the eigenvectors of L corresponding to those
N. Therefore, we can treat QS as Q for inferences’ purpose
non-zero eigenvalues scaled by

√

(cid:0)

(cid:1)

and estimate Q and ft based on the spectral decomposition of L.

With regular conditions in Assumptions 3.1, we can estimate the factors and their loadings,

and then generate a sample of time series with AR-sieve bootstrap. To facilitate the estimation

Q as the (unscaled) orthonormal factor loading matrix such that

process, we deﬁne Qo = 1√
N
Qo(cid:62)Qo = Ir and f o

t + ut is equivalent to model (1)
with different scaling on Q and {ft}. Details of the proposed method, including the estimation

t as the scaled factors such that yt = Qof o

and the bootstrap procedure, are illustrated in the following subsection.

3.2 The procedure of factor-based AR-sieve bootstrap

In this part, we present the whole procedure for the proposed factor-based AR-sieve bootstrap.

Then, a ﬂow chart is provided to clarify the essential idea of this procedure further.

Below we divide the proposed method into four steps.

Step 1: Estimation of Q: To utilise the idea in Lam et al. (2011) to estimate Q and {ft}

using L, the accumulated symmetrised autocovariance matrices of {yt} up to a prescribed

8

lag k0 > 0, we ﬁrst deﬁne the accumulation of symmetrised sample autocovariance up to

lag k0 as

L =

k0∑

k=1

Γy(k)

Γy(k)(cid:62),

with

Γy(k) the sample autocovariance at lag k deﬁned as

(cid:101)

(cid:101)

(cid:101)

(cid:101)

Γy(k) =

1
T − k

T−k
∑
t=1

(yt+k − y)(yt − y)(cid:62).

(cid:101)
By applying spectral (eigenvalue) decomposition on

L, we can obtain

Qo = (

with

qo
i the eigenvector of

L corresponding to the ith largest eigenvalue of
(cid:99)
natural estimator of the unscaled loading matrix Qo. And by scaling up

(cid:101)

L.

(cid:98)

(cid:101)

square root of dimension, we ended up with

Q =

N

√

Qo with
(cid:99)
(cid:101)
Qo as the estimator of Q.

(cid:99)

qo
qo
r )
qo
2, ...,
1,
Qo is then a
(cid:98)
(cid:98)

(cid:98)
√

N, the

As discussed in Lam et al. (2011), the estimation results are not sensitive to the choice

(cid:98)

(cid:99)

of k0, and the numeral results associated with k0 = 1 to k0 = 5 are similar. In general,

when dimension N is large compared with T, a relatively larger k0 may be considered for

better accuracy of sample estimates, while k0 = 1 is computational more efﬁcient when

the sample size T is large compared with dimension N. Besides, for ﬁnite samples, some

of the non-spiked eigenvalues of

L may not be exactly zero, therefore we can use the

ratio-based estimator as discussed in Lam et al. (2011) to estimate the number of factor r.

(cid:101)

As deﬁned in Lam et al. (2011), the ratio-based estimator for r is

r = argmin
1≤j≤R

λj+1/

λj,

with

λ1 ≥

λ2 ≥ · · · ≥

(cid:98)
λN the eigenvalues of

(cid:98)

(cid:98)
L and R an integer satisfying r ≤ R < N.

And practically, R can be taken as N/2 or N/3 for computation efﬁciency (Lam et al.

(cid:98)

(cid:98)

(cid:101)

(cid:98)
2011).

Step 2: Estimation of {ft}:

With

Q the estimator of Q, it is then straightforward to estimate {ft} by

(cid:98)

ft =

Q(cid:62)yt.

(cid:98)

(cid:98)

Step 3: AR-sieve bootstrap on {

ft}:

To apply the AR-sieve bootstrap on {

(cid:98)

ft}, we can, ﬁrst of all, ﬁt a pth order VAR model on

(cid:98)

9

the r-dimensional time series {

ft} as

ft =

(cid:98)
Al,p(r)

p
∑
l=1

ft−l +

et,p, t = p + 1, p + 2, ..., T,

where

et,p denote the residuals and the order p of the VAR model can be selected based on

(cid:98)

(cid:98)

(cid:98)

(cid:98)

an information criterion such as AIC (Akaike 1974) and SC (Schwarz 1978). Equivalently,

(cid:98)
we have

et,p =

ft −

p
∑
l=1

Al,p(r)

ft−l, t = p + 1, p + 2, ..., T,

where {
coefﬁcient matrices. We can then generate {e∗

Al,p, l = 1, 2, ..., p; t = p + 1, p + 2, ..., T} are Yule-Walker estimators of the AR
t } the bootstrap sample of residuals by

(cid:98)

(cid:98)

(cid:98)

(cid:98)

(cid:98)

resampling from the empirical distribution of the centered residual vectors. Consequently,

based on the idea of AR-sieve bootstrap (see, e.g. Kreiss 1992, Meyer & Kreiss 2015, Papar-
oditis 2018), we can generate the r-dimensional pseudo-time series {f ∗
by simulating the VAR model with bootstrap residuals {e∗
bootstrap sample of {f ∗

t , t = 1, 2, ..., T}
t }. Therefore, an AR-sieve

t } is generated by

f ∗
t =

p
∑
l=1

Al,p(r)f ∗

t−l + e∗
t ,

where {e∗

t } are i.i.d. random vectors following the empirical distribution of the centered

(cid:98)

residual vectors {

et}, where

et,p =

et,p −

eT,p and

eT,p = 1/(T − p) ∑T

t=p+1

et,p.

Step 4: Generating bootstrap data {y∗
(cid:101)
(cid:98)

(cid:101)

t }:

(cid:98)

(cid:98)

(cid:98)

Lastly, the bootstrap time series {y∗

t } can be constructed as

y∗
t =

r
∑
j=1

f ∗
j,t

qj,

(cid:98)

(6)

√

(cid:98)

(cid:98)

qj =

where
Following this AR-sieve bootstrap procedure, the pseudo-time series {y∗

qo
j is the scaled eigenvector of

L corresponding to the jth largest eigenvalue.

t } can mimic the

N

(cid:98)

temporal dependence of the original data {yt} via a factor model.

Remark 3.2. It is noteworthy that the bootstrap version in (6) is constructed via the bootstrap

version of the common factors. We could also modify it to involve an additional term related

ft, under some regular
to the error components. For instance, with the estimated
sparse conditions on the population covariance matrix Σu, we can get an appropriate estimator

ut = yt −

Q

(cid:98)

(cid:98)

(cid:98)

10

Σu (Fan et al. 2013). Then a modiﬁed bootstrap version is

(cid:98)

y∗∗
t =

r
∑
j=1

f ∗
j,t

qj +

Σ1/2
u

ut,

(7)

(cid:98)
ut is N-dimensional random vector generated from standard normal distribution N (0N, IN).

(cid:101)

(cid:98)

where
In this way, the bootstrap version y∗∗
t
sample observations, the covariance matrix of y∗∗
t

(cid:101)

is not of low-rank. For instance, conditional on the original

is of full rank. Due to high-dimensionality of

the error components {

ut}, non-parametric bootstrap on error would incur curse of dimension-

ality again (El Karoui & Purdom 2018).

(cid:98)

For simplicity, we study the mean statistics and the largest eigenvalue of sample autoco-

variance matrices based on the bootstrap version in (6), because (6) and (7) produce bootstrap

statistics with similar asymptotic properties. The major reason is based on assumptions that (1)

the population mean of error components is zero and (2) the spiked eigenvalues of autocovari-

ance matrices are assumed to tend to inﬁnity.

To clarify the four steps mentioned above, a ﬂow chart is provided in Figure 1, which

summarises the basic logic and procedure for the proposed factor-based AR-sieve bootstrap

method.

4 Asymptotic theory

In this section, some regular assumptions and justiﬁcations are present ﬁrst. Then, we establish

the asymptotic properties for two commonly-used statistics: the mean statistics and the largest

eigenvalues of accumulated autocovariance matrices.

4.1 Regularity assumptions

Before introducing the additional regularity assumptions, we ﬁx some notations ﬁrst. We use

(cid:107) · (cid:107)2 to denote the L2 norm (also known as spectral norm or operator norm) of a matrix or

vector, and (cid:107) · (cid:107)F to denote the Frobenius norm of a matrix. And we use a (cid:16) b to denote the

case that a = OP(b) and b = OP(a).

In addition to Assumptions 3.1 made on the factor model (1), to apply the AR-sieve bootstrap

on {

ft}, the estimates of factors {ft}, we also need some regularity conditions on {ft} for the

AR-sieve bootstrap to be consistent and valid. Denoted by W(·), the spectral density matrix of

(cid:98)

a vector process for all frequencies ω ∈ (0, 2π], then the spectral density matrix of {ft} can be

11

Figure 1 Flow chart for AR-sieve bootstrap method

12

High-dimensionaltimeseriesyComputetheaccumulatedsquaredautocovarianceaseL=Pk0k=1eΓy(k)eΓy(k)>EstimatethefactorloadingmatrixasbQ=(bq1,bq2,...,bqr)Estimatethefactorsasbft=bQ>yt(Vector)ARsieveboot-strapbfttoobtainf∗tObtainbootstrappedtimeseriesy∗t=bQf∗tSpectraldecompositionHigh-dimensionaltimeseriesDimensionreductionSievebootstrapdeﬁned as

Wf (ω) =

1
2π

∞
∑
k=−∞

Γf (k)e−iωk, ω ∈ (0, 2π].

Assumptions 4.1. In model (1), we strengthen Assumption 3.1 such that {ft} are strictly stationary
and purely nondeterministic stochastic processes of full rank with Eft = 0 and E (cid:107)ft(cid:107)2 < ∞. Γ
the autocovariance matrix of ft at lag k fulﬁls the matrix norm summability condition ∑∞
|k|)γ

< ∞ for some γ ≥ 0 that will be speciﬁed later on.

f (k),
k=−∞(1 +

Γ

f (k)

F

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Justiﬁcation for Assumption 4.1: Assumption 4.1 is introduced to fulﬁl the requirement for

the existence of a general VAR representation (3). This type of conditions is commonly used

in literature of AR-sieve bootstraps, such as Kreiss et al. (2011) and Meyer & Kreiss (2015). In

addition, following the heredity of mixing properties in Assumption 3.1, {ft} is strict stationary
and also ψ−mixing, which in turn implies the decaying of Γf (k) as k → ∞. The matrix norm
summability condition on Γ

f (k), as in Assumption 4.1, then speciﬁes the rate of decaying that

is required for a vector AR representation to be valid as stated in the next lemma. Besides,
the assumption Eft = 0 can be relaxed to Eft = µ f with the cost of a more lengthy proof of

theorems in this work.

Lemma 4.1. Let σj(ω) be the jth largest eigenvalue of the spectral density matrix Wf (ω) for {ft},

j = 1, 2, ..., r, ω ∈ (0, 2π]. Under Assumption 3.1 and 4.1 with γ = 0, σi(ω) fulﬁls the following

so-called boundedness condition (Wiener & Masani 1958):

c ≤ σj(ω) ≤ d, for all ω ∈ (0, 2π],

where 0 < c ≤ d < ∞.

Proof of Lemma 4.1. The upper bound d for all ω ∈ (0, 2π] follows directly from the norm

summability condition stated in Assumption 4.1. The assumption of strong factors in Assump-

tion 3.1 implies the positivity on eigenvalues of the spectral density matrix Wf (ω). Denoted

by σi(ω), the minimum eigenvalue of Wf (ω) for i = 1, 2, ..., r, then σi(ω) is continuous in

(0, 2π] and strictly positive. Denoted by σmin = minω∈(0,2π](σi(ω)), the minimum eigenvalue
of the spectral density matrix of {ft}, then there exists a constant c > 0 so that σmin ≥ c for all

frequencies ω ∈ (0, 2π].

The continuity and boundedness properties in Lemma 4.1 then entail the existence of a vector

AR representation for any vector process satisfying Assumption 4.1 (see, e.g. Meyer & Kreiss

13

2015, Cheng & Pourahmadi 1993, Wiener & Masani 1958). That is, the AR representation (3)

and Wold representation (2) are valid under Assumption 4.1.

The validity of AR-sieve bootstrap on a class of strictly stationary vector series fulﬁlling

Assumption 4.1 has been discussed in Meyer & Kreiss (2015), where some additional conditions

on the convergence of Yule-Walker estimators of the ﬁnite predictor coefﬁcients on {ft} are also

introduced. We summarise these conditions in Assumption 4.2 and leave the results of Meyer &

Kreiss (2015) to Lemma C.5 in Appendix C, as they are preliminary for showing the bootstrap

consistency and validity.

Assumptions 4.2. The Yule-Walker estimators {
the ﬁnite predictor coefﬁcients matrices on the VAR representation of {ft}, fulﬁls that p2 ∑p
Al,p(cid:107)F = OP (1) , as T → ∞ and p → ∞.

Al,p, l = 1, 2, ..., p} of {Al,p, l = 1, 2, ..., p} in (3),

l=1 (cid:107)

(cid:101)

Al,p −

(cid:101)

Justiﬁcation for Assumption 4.2: Assumption 4.2 requires p → ∞ at a relatively slower

rate of sample size T, which is required for the convergence of the Yule-Walker estimator

of Ap = (A1,p, ..., Ap,p). In other words, the order p of the AR terms in AR-sieve bootstrap

depends on the sample size T and has to be chosen properly. For {ft} fulﬁlling Assumption 4.1,

Assumption 4.2 is also satisﬁed if we choose p = O

(T/ ln T)1/6

(e.g., Meyer & Kreiss 2015).

Assumptions 4.1 and 4.2 are widely discussed in literature of AR-sieve bootstrap, for example,

(cid:1)

(cid:0)

in Kreiss et al. (2011) and Meyer & Kreiss (2015). In summary, Assumption 4.1 ensures the

existence of VAR representation in (3) and speciﬁes the rate of decaying for the coefﬁcient

matrices and Assumption 4.2 relates to the convergence of Yule-walker estimators {

Al,p} to the

ﬁnite predictor coefﬁcient matrices {Al,p}.

(cid:101)

Assumptions 4.3. The dimension N and AR(p) satisfy N → ∞, p → ∞ when T → ∞ such that
p11/2(N−1/2 + T−1/2) → 0.

Justiﬁcation for Assumption 4.3: In addition to Assumption 4.2, Assumption 4.3 is intro-

duced as the bootstrap procedure is performed on the estimated factors {

ft} rather than true

unobservable factors {ft}, where the error comes from both the estimation of factors and ﬁnite

(cid:98)

order approximation of AR-sieve representations. In other words, we need to control the error

imposed by the bootstrap procedure by restricting the speed that the AR order p goes to inﬁnity.

On the other hand, the order on dimension N in Assumption 4.3 also indicates ‘blessing of

dimensionality’, since the increase of the dimension N will enhance the strength of common

factors {ft} (Lam et al. 2011).

14

4.2 Bootstrap validity for mean statistics

The validity of general AR and VAR sieve bootstrap has been well discussed in Kreiss et al.

(2011) and Meyer & Kreiss (2015). It is worth noting that the general AR and VAR sieve bootstrap

do not mimic the behaviour of the underlying processes in (2) or (3), but the behaviour of a
so-called companion processes { ˘ft}. The companion processes { ˘ft} are deﬁned in the same

form as {ft} but with i.i.d. white noises { ˘et} rather than the uncorrelated white noises {et}

in (2) or (3), although {et} and { ˘et} share the same distribution. That means, without additional
assumptions on the distribution of {et}, the higher-order properties of { ˘ft} and {ft} are not

necessarily the same. In other words, except for the Gaussian case, the general AR and VAR

sieve bootstrap work for statistics that only depend on up-to-second-order quantities of {ft}.

To summarise our ﬁrst result on bootstrap consistency of QfT, the mean statistics of the
unobservable factor component {Qft}, we use E∗ to denote the expectation with respect to the

measure assigning probability 1/(T − p) to each observation.

Theorem 4.2. Suppose that Assumptions 3.1, 4.1 (γ = 1), 4.2 and 4.3 are satisﬁed for ﬁxed and known

number of factors r. In addition, if we further assume that

(a) The empirical distribution of {et} converges weakly to the distribution function of L(et).

(b) limT→∞ V(

√

TfT) = ∑k∈Z

Γf (k) > 0.

Then, for any vector c ∈ RN such that (cid:107)c(cid:62)Q(cid:107)(cid:96)1 < ∞ and 0 < ∑k∈Z c(cid:62)QΓf (k)Q(cid:62)c < ∞ as N → ∞,
we can conclude that

√

dK

L

(cid:16)

(cid:16)

Tc(cid:62)

Q

T − E∗f ∗
f ∗

T

y1, y2, ..., yT

, L

√

Tc(cid:62)Q

fT − EfT

p
→ 0,

when N → ∞ and T → ∞, where L and dK denote the probability distribution and Kolmogorov distance,

(cid:0)

(cid:98)

(cid:1)(cid:12)
(cid:12)
(cid:12)

(cid:17)

(cid:16)

(cid:0)

(cid:1)(cid:17)(cid:17)

respectively.

Theorem 4.2 states the validity of the proposed AR-sieve bootstrap methods on estimated

factors {

ft}. In general, the bootstrap inferences can be considered an alternative statistical tool

for practical use compared with the asymptotic results, which can be rather difﬁcult to derive,

(cid:98)

especially for high-dimensional time series. The factor model in (1) ﬁlters out the time-invariant

noises {ut} and project the original time series onto a low-dimensional subspace where the

AR-sieve bootstrap procedure can be developed.

Remark 4.1. As discussed in Kreiss et al. (2011) and Meyer & Kreiss (2015), AR-sieve bootstrap
in fact mimics the behaviour of a companion process ˘ft which shares the same ﬁrst and second-

15

order quantities as {ft}. Hence for the mean statistics, AR-sieve bootstrap works without

any additional assumptions made on the higher-order moments of {ft}. Also, for AR-sieve

bootstrap to be asymptotically valid on {ft}, the dimension r needs not to go to inﬁnity. To

study the impact of the factor strength on the validity of AR-sieve bootstrap, we consider

various factor strengths in simulation studies in Section 5.

4.3 Bootstrap consistency for autocovariance matrices

For high-dimensional i.i.d. data, the covariance matrix plays an important role in dimension-

reduction techniques, such as factor models and principal component analysis. However, for

high-dimensional dependent data, the autocovariance matrices are vital or even more crucial

than the covariance matrix. Lam et al. (2011) provide a discussion on the use of autocovariance

in dimension reduction. Therefore, it is critical to establish the bootstrap consistency for

the autocovariance matrices under the proposed AR-sieve bootstrap method. In the next

theorem, we show that the proposed AR-sieve bootstrap method can guarantee the asymptotic

consistency on the autocovariance matrices, which in turn implies the validity of using bootstrap
data {y∗

t } to approximate the original data {yt}.

Recall that Γ

f (k) = Cov(ft, ft+k) is the autocovariance of unobservable factors {ft} at

lag k, for k > 0. Without the loss of generality, we again assume the means of factors are
0 to simplify the notations and deﬁne Cov∗ as the covariance with respect to the measure
assigning probability 1/(T − p) to each observation. Denoted by Γ∗
the autocovariance of bootstrap factors {f ∗
asymptotic consistency of Γ∗

t+k)
t } at lag k, we have the following theorem on the

f (k) = Cov∗(f ∗

t , f ∗

f (k).

Theorem 4.3. Suppose that Assumptions 3.1, 4.1 (γ = 1) and 4.2 are satisﬁed for ﬁxed and known

number of factors r. In addition, if we further assume that The empirical distribution of {et} converges
weakly to the distribution function of L(et). Then for k ∈ N, we have

when N → ∞ and T → ∞.

Γ∗
f (k) − Γ

f (k)

(cid:13)
(cid:13)
(cid:13)

p
→ 0,

2

(cid:13)
(cid:13)
(cid:13)

Let {δi(k)}r

i=1 be the ordered spiked eigenvalues of 1
N2
i (k)}r

variance matrices of {yt} at lag k > 0. And deﬁne {δ∗
of 1
N2
where Γ∗

i=1 to be the ﬁrst r largest eigenvalues
t } at lag k > 0,
t+k). As a consequence of Theorem 4.3, we immediately have the

y(k)(cid:62), the bootstrap symmetrised autocovariance matrices of {y∗

Γ∗
y(k)Γ∗
y(k) = Cov∗(y∗

t , y∗

Γy(k)Γy(k)(cid:62), the symmetrised autoco-

16

following proposition on the convergence of spiked eigenvalues of the bootstrap symmetrised

autocovariance matrices to their population counterparts.

Proposition 4.4. Under the same Assumptions of Theorem 4.3, for i = 1, 2, ..., r and k ∈ N, we have

Γ∗
y(k) − Γy(k)

(cid:13)
(cid:13)
(cid:13)

p
→ 0,

2

(cid:13)
(cid:13)
(cid:13)

|δ∗

i (k) − δi(k)|

p
→ 0,

and

when N → ∞ and T → ∞.

The asymptotic property of spiked eigenvalues of symmetrised autocovariance matrices of

high-dimensional time series is signiﬁcant in many applications. However, there is no literature
due to the difﬁculties and complexities of studying dependent data when N → ∞. Proposi-

tion 4.4 veriﬁes the bootstrap consistency on spiked eigenvalues of symmetrised autocovariance

matrices and provides statistical tools to study the properties of spiked eigenvalues based on

the AR-sieve bootstrap.

Remark 4.2. Despite that Γ∗

t+k) are the autocovariances deﬁned conditionally
on the sample observations, the results of convergence in Proposition 4.4 are on the whole

y(k) = Cov∗(y∗

t , y∗

probability space, which allows for the use of autocovariances and their spiked eigenvalues
computed from a bootstrap sample {y∗

t } to approximate the autocovariances and corresponding

spiked eigenvalues of the original data {yt}.

5 Simulation studies

In this section, we ﬁrst study the performance of AR-sieve bootstrap conﬁdence intervals for

the mean statistics, where empirical coverage probabilities are computed, and the impacts of

sample size T, data dimension N and factor strength are discussed. Secondly, we examine the

proposed AR-sieve bootstrap method’s performance on constructing conﬁdence intervals for

the eigenvalues of the symmetrised autocovariance matrix. These types of statistical inference

are particularly important for high-dimensional factor modelling.

17

5.1 AR-sieve bootstrap for mean statistics

We study the validity and consistency of our proposed AR-sieve bootstrap method for high-

dimensional factor time series models. To achieve this, we use simulation to evaluate the

empirical coverage and average width of bootstrap conﬁdence intervals for the mean statistics

deﬁned in Theorem 4.2 ﬁrst. Recall model (1) that yt = Qft + ut and its equivalent form
yt = Qof o
t + ut with different scales on Q and ft. To address the problem under a general
high-dimensional factor time series model, we generate the factor loading matrix Qo by an

arbitrary QR decomposition on standard multivariate normal random variables, where Qo
fulﬁls Qo(cid:62)Qo = Ir with r denoting the number of factors. We then assume the observations

{yt} are from a two-factor model

yt = Qof o

t + ut,

(8)

where {ui,t} are independent N (0, 1) random noises, Q is an N × 2 matrix with each column
an orthogonal basis, and both factors of f o

t follow an AR(1) model with mean 0 and the AR

coefﬁcient 0.5. In other words, the two factors are generated from

fi,t = 0.5 fi,t−1 + ei,t, for i = 1, 2.

To study the impact of factor strength and signal to noise ratio, we simulate data in various

cases where factor strengths are assumed to be different. In particular, we follow the deﬁnition

of factor strengths considered by Lam et al. (2011) and assume the error terms {ei,t} in the AR(1)
model (8) for both factors are independent N (0, Nν) and N (0, 0.5Nν), respectively, where

ν ∈ (0, 1] with ν = 1 corresponding to the case of the strongest factors. In this section, we

generate simulations to evaluate the performance of the AR-sieve bootstrap for different factor

strengths, ν = 1, 0.8, 0.6, 0.4 and 0.2. The use of different scales 1 and 0.5 in the variance of

ei,t for i = 1, 2 is to ensure that the ﬁrst two largest eigenvalues of accumulated symmetrised

autocovariance matrices that are associated with the two factors are spiked and unequal. The

use of 0.5 as the AR coefﬁcient in both cases reﬂects a moderate temporal dependence within

each factor. Generally speaking, a larger AR coefﬁcient or stronger temporal dependence within

each factor also demands a relatively large sample size T for better AR-sieve bootstrap results.

In comparison, a smaller AR coefﬁcient or weaker temporal dependence within each factor can

lead to the overestimating problem on the number of factors, which is already considered when

ν is relatively small. For all cases, we repeat the simulation by 1000 times and each time we

18

generate B = 999 bootstrap samples to create a conﬁdence interval for a (standardised) mean
√
Nν 1(cid:62)Qµ f for factors with various strengths. It is worth noting that
T√
the mean statistics are standardised by the factor strengths for comparison of the length of

statistic deﬁned as θy :=

conﬁdence intervals across different factor strengths.

Speciﬁcally, we ﬁrst compute B = 999 AR-sieve bootstrap estimates of the (standardised)

mean statistic as y∗ =

Qf ∗, and then create bootstrap intervals based on then. In this

√
Nν 1(cid:62)
T√

example, we investigate the performance of our proposed AR-sieve bootstrap method based

(cid:98)

on two types of bootstrap intervals, the nonparametric bootstrap interval using quantiles and

the parametric bootstrap interval based on normality. Both bootstrap intervals are practically

popular, computationally efﬁcient and easy to implement. For an arbitrary statistic θ and its

sample estimate

θ, the nonparametric bootstrap interval using quantiles are calculated as

(cid:98)

2

θ − θ∗

(1−α/2), 2

θ + θ∗

(α/2)

,

where θ∗

(cid:98)
(1−α/2) is the (1 − α/2) percentile of the bootstrap estimates θ∗. The nonparametric
bootstrap interval using quantiles are sometimes referred to as reverse percentile interval as

(cid:98)

(cid:16)

(cid:17)

the order of upper and lower quantiles are reversed in the formula. The idea of nonparametric
bootstrap interval using quantiles is to use the bootstrap distribution of (θ∗ −

θ) to approximate

the distribution of (

θ − θ). On the other hand, the parametric bootstrap interval based on

(cid:98)

normality can be computed as

(cid:98)

√

θ − b∗ −

v∗z(1−α/2),

θ − b∗ +

√

v∗z(1−α/2)

,

where b∗ and v∗ are the bootstrap estimates of bias and variance of

θ, and z(1−α/2) is the

(cid:16)

(cid:98)

(cid:98)

(cid:17)

(1 − α/2) percentile of standard normal distribution. Similar to the nonparametric bootstrap

(cid:98)

interval using quantiles, the parametric bootstrap interval based on normality also assumes
the bootstrap distribution of (θ∗ −

θ) correctly approximates the distribution of (

θ − θ), but are

constructed in a parametric way. To achieve the improved empirical coverage and width of

(cid:98)

(cid:98)

intervals, more sophisticated intervals with additional corrections on bias and variance may

also be constructed, such as double bootstrap, with a higher cost of computations. Since this

example’s main purpose is to inspect the validity and consistency of our proposed AR-sieve

bootstrap method under various cases, we only use these two ways of bootstrap intervals as

they are simple and computationally efﬁcient. Finally, to get a comprehensive comparison on

the performance of two types of intervals, we compute the empirical coverage, average width,

19

and interval score (Gneiting & Raftery 2007) of bootstrap intervals under various combinations

of N and T. The interval score of a bootstrap interval (l, u) is computed as

Sα = (u − l) +

2
α

(l − θ)1{θ < l} +

(θ − u)1{θ > u},

2
α

where α denotes a level of signiﬁcance. The idea of this interval score is rewarding narrower

intervals but putting penalties on intervals missing true statistics θ. It is worth noting that when

the empirical coverage and average width of two bootstrap intervals are close, the average

interval score can be used for overall comparison.

In Table 1, we present the empirical coverage, average width and interval score of non-

parametric bootstrap intervals using quantiles and parametric bootstrap intervals based on

normality for θy with ν = 1. The nominate coverages we investigated are 95%, 90%, and 80%

with various combinations of N and T for comparison. As shown in both tables, when the

sample size T is large enough and the factors are the strongest, the empirical coverage is reason-

ably close to the nominated coverage and are not largely affected by the ratio of N/T. Besides,

bootstrap intervals’ average width is also similar for various combinations of N and T. This

result is often referred to as the ‘blessing of dimensionality’ in the literature of high-dimensional

statistics. The performance of bootstrap conﬁdence intervals generally beneﬁts from the increase

of both N and T. Between nonparametric bootstrap intervals using quantiles and parametric

bootstrap intervals based on normality, the average interval scores are very close for almost all

combinations of N and T. Hence, we conclude that both intervals perform well in the strong

factors’ case. Similar results of the empirical coverage, average width, and interval score of

nonparametric bootstrap intervals using quantiles and parametric bootstrap intervals based on

normality for θy with ν = 0.8 can be observed in Table 2, where the factors are relatively strong

but not the strongest as ν = 1.

However, as shown in Tables 3 to 5, when ν is further reduced from 0.6 to 0.2 and the

factors are weakened, the empirical coverage tends to increase with N/T, and the bootstrap

intervals become wider and wider. This suggests that the AR-sieve bootstrap overestimates

the standard error of the (standardised) mean statistic when N increases. When the factors

become weaker, the spikiness of the ﬁrst two largest eigenvalues of accumulated symmetrised

autocovariance matrices decreases. The number of factors can be overestimated, which brings

the noises into bootstrap samples. As a result, neither of the two types of bootstrap intervals

performs well when factors are very weak (especially when ν = 0.2) and N/T is large. The

bootstrap distribution of the (standardised) mean statistic suffers from comparably fatter tails.

20

Table 1 Empirical coverage, average width and interval score of nonparametric bootstrap

intervals using quantiles for θy with ν = 1

95%

90%

80%

T

N

Empirical
coverage

Average
width

Average
interval score

Empirical
coverage

Average
width

Average
interval score

Empirical
coverage

Average
width

Average
interval score

Nonparametric bootstrap intervals using quantiles

200

500

1000

200

500

1000

50
100
200
500
1000

50
100
200
500
1000

50
100
200
500
1000

50
100
200
500
1000

50
100
200
500
1000

50
100
200
500
1000

0.941
0.948
0.941
0.935
0.943

0.936
0.940
0.943
0.946
0.941

0.935
0.944
0.938
0.946
0.944

0.944
0.947
0.941
0.935
0.942

0.939
0.943
0.942
0.944
0.942

0.938
0.945
0.942
0.947
0.948

8.369
8.407
8.366
8.438
8.513

8.501
8.275
8.430
8.354
8.147

8.594
8.428
8.194
8.469
8.479

8.402
8.449
8.407
8.481
8.555

8.548
8.318
8.470
8.395
8.190

8.632
8.470
8.232
8.516
8.525

11.572
11.339
11.868
12.514
13.615

12.354
11.693
12.900
11.818
12.547

13.142
13.273
12.472
11.918
11.928

0.892
0.901
0.889
0.876
0.889

0.882
0.887
0.891
0.902
0.894

0.898
0.892
0.888
0.894
0.884

7.029
7.067
7.038
7.098
7.161

7.160
6.964
7.096
7.023
6.850

7.219
7.088
6.889
7.123
7.133

10.686
10.466
10.745
11.470
11.759

11.352
10.804
11.465
10.678
10.905

11.658
11.662
11.300
11.077
11.177

Parametric bootstrap intervals based on normality

11.457
11.321
11.698
12.343
13.575

12.167
11.609
12.784
11.648
12.434

13.181
13.145
12.587
11.874
11.815

0.893
0.903
0.890
0.878
0.888

0.886
0.890
0.897
0.903
0.895

0.898
0.891
0.891
0.895
0.888

7.051
7.090
7.055
7.117
7.180

7.174
6.981
7.109
7.046
6.873

7.244
7.108
6.908
7.147
7.154

10.646
10.444
10.657
11.414
11.747

11.276
10.837
11.384
10.592
10.928

11.639
11.652
11.266
11.015
11.097

0.799
0.811
0.787
0.778
0.792

0.781
0.781
0.792
0.797
0.802

0.777
0.777
0.784
0.806
0.783

0.795
0.818
0.789
0.775
0.793

0.775
0.781
0.794
0.797
0.806

0.778
0.780
0.786
0.810
0.782

5.480
5.511
5.488
5.536
5.584

5.579
5.427
5.531
5.484
5.344

5.629
5.531
5.371
5.559
5.565

5.493
5.524
5.497
5.545
5.594

5.590
5.439
5.538
5.489
5.355

5.644
5.538
5.382
5.568
5.574

9.449
9.289
9.568
10.394
10.170

10.212
9.808
9.978
9.566
9.448

10.220
10.442
9.943
9.865
10.141

9.482
9.271
9.534
10.379
10.211

10.198
9.814
9.980
9.532
9.406

10.206
10.419
9.908
9.826
10.119

21

Table 2 Empirical coverage, average width and interval score of nonparametric bootstrap

intervals using quantiles for θy with ν = 0.8

95%

90%

80%

T

N

Empirical
coverage

Average
width

Average
interval score

Empirical
coverage

Average
width

Average
interval score

Empirical
coverage

Average
width

Average
interval score

Nonparametric bootstrap intervals using quantiles

200

500

1000

200

500

1000

50
100
200
500
1000

50
100
200
500
1000

50
100
200
500
1000

50
100
200
500
1000

50
100
200
500
1000

50
100
200
500
1000

0.948
0.955
0.950
0.940
0.951

0.939
0.943
0.943
0.949
0.940

0.933
0.942
0.936
0.950
0.949

0.950
0.955
0.948
0.942
0.954

0.942
0.945
0.943
0.949
0.945

0.933
0.943
0.937
0.951
0.952

8.391
8.432
8.388
8.472
8.610

8.507
8.274
8.463
8.392
8.173

8.590
8.428
8.195
8.490
8.498

8.423
8.476
8.427
8.516
8.653

8.546
8.316
8.498
8.429
8.219

8.631
8.472
8.233
8.534
8.540

11.301
10.869
11.421
11.870
12.637

12.423
11.467
12.846
11.535
12.197

13.273
13.244
12.470
11.764
11.801

0.897
0.903
0.894
0.887
0.899

0.884
0.893
0.894
0.907
0.902

0.892
0.896
0.894
0.892
0.887

7.044
7.087
7.051
7.125
7.248

7.161
6.962
7.120
7.048
6.876

7.216
7.097
6.894
7.138
7.147

10.418
10.158
10.315
10.800
10.940

11.384
10.626
11.297
10.397
10.698

11.784
11.703
11.246
10.971
11.066

Parametric bootstrap intervals based on normality

11.218
10.796
11.318
11.765
12.561

12.259
11.383
12.714
11.348
12.119

13.249
13.150
12.517
11.722
11.670

0.898
0.905
0.900
0.893
0.897

0.880
0.892
0.900
0.910
0.900

0.891
0.891
0.894
0.894
0.888

7.068
7.113
7.072
7.147
7.262

7.172
6.979
7.131
7.074
6.898

7.243
7.110
6.909
7.162
7.167

10.412
10.086
10.254
10.735
10.991

11.316
10.629
11.261
10.308
10.728

11.750
11.681
11.213
10.896
10.998

0.807
0.816
0.802
0.788
0.807

0.777
0.788
0.800
0.798
0.811

0.774
0.769
0.784
0.809
0.782

0.802
0.818
0.805
0.790
0.803

0.778
0.786
0.796
0.804
0.815

0.773
0.774
0.783
0.808
0.784

5.493
5.530
5.499
5.559
5.647

5.578
5.429
5.546
5.506
5.363

5.631
5.532
5.376
5.571
5.571

5.507
5.542
5.510
5.568
5.658

5.588
5.437
5.556
5.512
5.374

5.643
5.540
5.383
5.580
5.584

9.236
9.068
9.181
9.881
9.628

10.242
9.662
9.901
9.322
9.249

10.288
10.485
9.867
9.820
10.094

9.258
9.046
9.149
9.860
9.636

10.236
9.664
9.882
9.311
9.217

10.288
10.458
9.839
9.769
10.071

22

This phenomenon can be observed especially for large T in Tables 5, where both the average

widths and the empirical coverages of bootstrap intervals are increasing with sample size N

while the average interval scores are decreasing.

Table 3 Empirical coverage, average width and interval score of nonparametric bootstrap

intervals using quantiles for θy with ν = 0.6

95%

90%

80%

T

N

Empirical
coverage

Average
width

Average
interval score

Empirical
coverage

Average
width

Average
interval score

Empirical
coverage

Average
width

Average
interval score

Nonparametric bootstrap intervals using quantiles

200

500

1000

200

500

1000

50
100
200
500
1000

50
100
200
500
1000

50
100
200
500
1000

50
100
200
500
1000

50
100
200
500
1000

50
100
200
500
1000

0.957
0.965
0.965
0.970
0.968

0.939
0.949
0.947
0.960
0.952

0.931
0.944
0.937
0.953
0.954

0.961
0.966
0.968
0.966
0.970

0.940
0.950
0.947
0.965
0.957

0.932
0.946
0.942
0.953
0.954

8.423
8.551
8.490
8.742
9.090

8.521
8.288
8.543
8.525
8.343

8.581
8.441
8.209
8.547
8.584

8.465
8.590
8.533
8.783
9.127

8.555
8.330
8.575
8.560
8.389

8.629
8.481
8.249
8.585
8.626

10.729
10.317
10.791
10.666
11.055

12.639
11.068
12.417
10.822
11.234

13.487
13.101
12.268
11.405
11.455

0.911
0.913
0.928
0.927
0.946

0.880
0.893
0.904
0.929
0.916

0.886
0.889
0.891
0.900
0.891

7.080
7.186
7.136
7.351
7.643

7.164
6.970
7.183
7.157
7.016

7.213
7.105
6.905
7.189
7.214

9.856
9.506
9.597
9.590
9.641

11.564
10.284
10.928
9.732
10.067

11.923
11.734
11.084
10.701
10.683

Parametric bootstrap intervals based on normality

10.704
10.243
10.748
10.614
10.937

12.396
11.046
12.331
10.681
11.215

13.388
13.061
12.290
11.349
11.343

0.910
0.921
0.932
0.927
0.948

0.879
0.896
0.909
0.931
0.917

0.888
0.891
0.895
0.899
0.894

7.104
7.209
7.162
7.371
7.659

7.180
6.991
7.196
7.184
7.040

7.242
7.118
6.922
7.205
7.239

9.847
9.485
9.574
9.592
9.675

11.407
10.306
10.927
9.697
10.082

11.920
11.670
11.074
10.641
10.612

0.819
0.830
0.839
0.828
0.854

0.774
0.791
0.818
0.829
0.836

0.774
0.768
0.792
0.815
0.795

0.822
0.830
0.839
0.829
0.854

0.774
0.786
0.822
0.831
0.842

0.774
0.769
0.791
0.817
0.799

5.522
5.601
5.570
5.732
5.954

5.583
5.438
5.597
5.591
5.472

5.631
5.538
5.383
5.603
5.630

5.535
5.617
5.580
5.743
5.968

5.594
5.447
5.607
5.597
5.485

5.642
5.546
5.394
5.614
5.640

8.810
8.642
8.476
8.717
8.444

10.313
9.466
9.619
8.822
8.676

10.433
10.550
9.744
9.635
9.868

8.816
8.632
8.453
8.697
8.458

10.283
9.439
9.607
8.802
8.660

10.429
10.503
9.684
9.597
9.817

5.2 AR-sieve bootstrap for spiked eigenvalues of the symmetrised autocovariance

matrix

The study on spiked eigenvalues of high-dimensional covariance matrix has received massive

attention in the past decades. For time-series data, researchers are particularly interested in the

spiked eigenvalues of the symmetrised autocovariance matrix. However, the theoretical results

of these spiked eigenvalues of the symmetrised autocovariance matrix for high-dimensional

23

Table 4 Empirical coverage, average width and interval score of nonparametric bootstrap

intervals using quantiles for θy with ν = 0.4

95%

90%

80%

T

N

Empirical
coverage

Average
width

Average
interval score

Empirical
coverage

Average
width

Average
interval score

Empirical
coverage

Average
width

Average
interval score

Nonparametric bootstrap intervals using quantiles

200

500

1000

200

500

1000

50
100
200
500
1000

50
100
200
500
1000

50
100
200
500
1000

50
100
200
500
1000

50
100
200
500
1000

50
100
200
500
1000

0.969
0.980
0.982
0.989
0.992

0.943
0.962
0.957
0.978
0.984

0.934
0.943
0.941
0.967
0.972

0.971
0.979
0.985
0.989
0.993

0.945
0.958
0.960
0.978
0.986

0.932
0.944
0.945
0.968
0.972

8.513
8.821
8.868
9.648
10.190

8.567
8.367
8.743
8.974
8.998

8.624
8.486
8.277
8.709
8.939

8.555
8.868
8.915
9.685
10.228

8.597
8.403
8.775
9.016
9.054

8.666
8.531
8.317
8.749
8.994

9.931
9.634
10.416
10.149
10.407

12.859
10.292
11.581
9.992
10.014

13.608
12.923
11.882
10.811
11.083

0.933
0.944
0.960
0.973
0.980

0.874
0.903
0.925
0.945
0.959

0.885
0.891
0.888
0.917
0.919

7.154
7.417
7.451
8.111
8.557

7.197
7.030
7.352
7.549
7.580

7.250
7.142
6.959
7.320
7.525

9.136
8.730
8.854
8.870
9.079

11.536
9.770
10.183
8.930
8.876

12.105
11.624
10.814
9.967
9.874

Parametric bootstrap intervals based on normality

9.934
9.685
10.326
10.176
10.403

12.755
10.410
11.546
10.091
10.033

13.475
12.906
11.877
10.722
10.924

0.934
0.947
0.956
0.975
0.982

0.876
0.908
0.928
0.947
0.959

0.883
0.894
0.893
0.923
0.926

7.180
7.442
7.481
8.128
8.583

7.215
7.052
7.364
7.566
7.599

7.273
7.159
6.979
7.343
7.548

9.109
8.695
8.877
8.891
9.054

11.405
9.762
10.254
8.916
8.919

12.096
11.577
10.803
9.938
9.814

0.845
0.865
0.887
0.915
0.939

0.765
0.796
0.846
0.863
0.898

0.771
0.785
0.805
0.842
0.831

0.843
0.862
0.889
0.918
0.939

0.763
0.799
0.846
0.866
0.897

0.775
0.779
0.802
0.846
0.835

5.585
5.782
5.817
6.323
6.672

5.614
5.487
5.733
5.889
5.916

5.653
5.570
5.426
5.711
5.875

5.594
5.798
5.829
6.333
6.688

5.621
5.494
5.737
5.895
5.920

5.666
5.578
5.438
5.721
5.881

8.188
7.949
7.638
7.439
7.607

10.393
9.049
9.016
7.966
7.694

10.618
10.552
9.487
9.070
9.059

8.200
7.935
7.644
7.463
7.594

10.363
9.016
9.006
7.941
7.688

10.598
10.503
9.443
9.033
9.025

24

Table 5 Empirical coverage, average width and interval score of nonparametric bootstrap

intervals using quantiles for θy with ν = 0.2

95%

90%

80%

T

N

Empirical
coverage

Average
width

Average
interval score

Empirical
coverage

Average
width

Average
interval score

Empirical
coverage

Average
width

Average
interval score

Nonparametric bootstrap intervals using quantiles

200

500

1000

200

500

1000

50
100
200
500
1000

50
100
200
500
1000

50
100
200
500
1000

50
100
200
500
1000

50
100
200
500
1000

50
100
200
500
1000

0.980
0.989
0.994
1.000
0.997

0.940
0.973
0.981
0.997
0.999

0.938
0.950
0.961
0.989
0.990

0.983
0.990
0.993
1.000
0.997

0.946
0.970
0.982
0.996
0.999

0.937
0.952
0.959
0.989
0.992

8.677
9.119
9.297
10.850
12.374

8.714
8.591
9.123
9.799
10.222

8.793
8.668
8.495
9.152
9.789

8.717
9.150
9.347
10.907
12.421

8.751
8.635
9.160
9.850
10.263

8.833
8.713
8.527
9.179
9.832

9.368
9.628
9.702
10.850
12.521

12.959
9.978
10.594
9.868
10.344

13.505
12.120
11.213
9.786
10.293

0.952
0.971
0.980
0.998
0.994

0.888
0.930
0.957
0.984
0.998

0.878
0.887
0.910
0.962
0.972

7.291
7.647
7.819
9.119
10.399

7.330
7.229
7.673
8.236
8.591

7.395
7.288
7.133
7.686
8.216

8.417
8.297
8.338
9.131
10.670

11.577
9.165
9.256
8.625
8.743

12.148
11.303
10.264
8.759
8.920

Parametric bootstrap intervals based on normality

9.392
9.628
9.719
10.907
12.583

12.804
10.047
10.503
9.968
10.343

13.426
12.132
11.119
9.783
10.339

0.951
0.968
0.980
0.998
0.994

0.892
0.934
0.960
0.986
0.998

0.881
0.891
0.913
0.965
0.979

7.316
7.679
7.844
9.153
10.424

7.344
7.246
7.687
8.266
8.613

7.413
7.312
7.156
7.703
8.251

8.443
8.317
8.342
9.157
10.694

11.463
9.145
9.220
8.619
8.772

12.134
11.276
10.135
8.700
8.918

0.877
0.900
0.944
0.985
0.988

0.786
0.837
0.897
0.942
0.977

0.775
0.787
0.826
0.880
0.910

0.880
0.901
0.945
0.985
0.988

0.786
0.843
0.893
0.946
0.977

0.776
0.784
0.824
0.882
0.914

5.687
5.967
6.098
7.120
8.101

5.711
5.632
5.977
6.433
6.700

5.759
5.691
5.561
5.986
6.416

5.700
5.983
6.112
7.131
8.122

5.722
5.646
5.989
6.440
6.710

5.776
5.697
5.576
6.002
6.429

7.451
7.243
6.886
7.272
8.424

10.325
8.327
7.953
7.148
6.994

10.722
10.382
9.021
7.852
7.662

7.458
7.247
6.911
7.277
8.438

10.259
8.309
7.935
7.129
7.004

10.703
10.336
8.983
7.826
7.648

25

time series are much more involved and hard to be applied for practical analysis. As an alterna-

tive, the AR-sieve bootstrap can be considered for real data applications when the theoretical

results do not exist or are hard to be implemented. As discussed in Proposition 4.4, the boot-
strap estimates δ∗

i (k) are generally consistent to δi(k). However, without a central limit theorem
δi(k), the spiked eigenvalues of the symmetrised sample autocovariance matrix, it

(CLT) on

is generally hard to derive the validity of the AR-sieve bootstrapped estimate theoretically.

(cid:98)

In this section, we use simulations to study our AR-sieve bootstrap method’s performance

on estimating δi(k). To be more speciﬁc, the data we generated are based on the strongest

factor model considered in Section 5.1 where ν = 1. We continue the study on the validity and

consistency of our AR-sieve bootstrap method by accessing the empirical coverage of bootstrap

intervals on the ﬁrst two largest eigenvalues δ1 and δ2 of symmetrised lag-1 autocovariance

matrix. In order to make a comprehensive comparison based on average width and interval

score of bootstrap intervals for various combination of N and T, the bootstrap intervals are

created based on standardised eigenvalues δ0

1 =

√
N2 δ1 and δ0

T

2 =

√
T
N2 δ2 rather than δ1 and δ2.

First of all, we compute the empirical coverage, average width, and interval score for

nonparametric bootstrap intervals using quantiles and parametric bootstrap intervals based

on normality for δ0

1 and δ0

2. As shown in Tables 6 to 7, neither of the two types of bootstrap

intervals can provide the desired result as the empirical coverage probabilities are consistently

lower than the nominal probabilities for each interval, especially when T is small. While the

‘blessing of dimensionality’ may improve the empirical coverage of both intervals on δ1 and δ2

for large N, the results are not as good for the (standardised) mean statistic. They consistently

underestimated empirical coverage probabilities are mainly due to the skewness of sampling

distribution of

δi(k), especially for a relatively small T. In general, the parametric bootstrap

interval based on normality, which is symmetric, and the nonparametric bootstrap interval

(cid:98)

using quantiles, which is reversely skewed, perform well when the sampling distributions are

symmetric but do not perform well when the sample statistic follows a skewed distribution. To

consider for this skewness, an unreversed nonparametric bootstrap interval using quantiles,

computed as

(α/2), θ∗
θ∗

(1−α/2)

,

(cid:16)

(cid:17)

can also be computed and compared since the skewness of sample statistics is retained by the

bootstrap estimates.

As shown in Tables 8 and 9, unreversed nonparametric bootstrap intervals using quantiles

26

Table 6 Empirical coverage, average width and interval score of nonparametric bootstrap

intervals using quantiles for δ0

1 with ν = 1

95%

90%

80%

T

N

Empirical
coverage

Average
width

Average
interval score

Empirical
coverage

Average
width

Average
interval score

Empirical
coverage

Average
width

Average
interval score

Nonparametric bootstrap intervals using quantiles

200

500

1000

200

500

1000

50
100
200
500
1000

50
100
200
500
1000

50
100
200
500
1000

50
100
200
500
1000

50
100
200
500
1000

50
100
200
500
1000

0.846
0.855
0.854
0.874
0.858

0.887
0.892
0.891
0.885
0.884

0.943
0.935
0.934
0.920
0.928

0.901
0.907
0.904
0.915
0.919

0.928
0.927
0.927
0.930
0.924

0.953
0.953
0.942
0.941
0.944

11.881
11.999
11.732
11.805
12.077

11.377
11.326
11.444
11.426
11.357

11.440
11.322
11.263
11.281
11.221

12.147
12.304
12.012
12.088
12.365

11.518
11.463
11.582
11.553
11.501

11.535
11.426
11.349
11.380
11.310

27.227
26.475
26.724
25.093
26.443

22.661
22.973
23.008
23.913
23.069

17.729
17.117
18.888
18.128
18.426

0.819
0.835
0.837
0.846
0.841

0.858
0.873
0.864
0.858
0.866

0.907
0.901
0.886
0.891
0.888

9.775
9.895
9.676
9.730
9.967

9.481
9.441
9.541
9.521
9.478

9.582
9.490
9.422
9.457
9.395

18.896
18.426
18.463
17.536
18.380

16.962
16.895
17.078
17.425
17.031

14.185
14.079
15.027
15.059
14.828

Parametric bootstrap intervals based on normality

19.992
19.677
19.824
19.296
20.463

18.289
18.825
18.808
19.663
18.687

15.854
15.294
16.814
16.037
16.318

0.873
0.878
0.876
0.896
0.890

0.887
0.890
0.883
0.881
0.877

0.915
0.915
0.909
0.901
0.906

10.194
10.326
10.081
10.145
10.377

9.666
9.620
9.720
9.696
9.652

9.681
9.589
9.524
9.550
9.492

16.073
15.899
15.926
15.414
16.048

15.434
15.334
15.619
15.970
15.431

13.768
13.457
14.387
14.190
13.903

0.771
0.794
0.798
0.789
0.795

0.777
0.800
0.797
0.782
0.775

0.810
0.803
0.809
0.804
0.795

0.796
0.809
0.820
0.824
0.813

0.800
0.819
0.814
0.799
0.785

0.826
0.810
0.809
0.809
0.808

7.470
7.587
7.390
7.444
7.623

7.347
7.317
7.391
7.366
7.339

7.446
7.372
7.324
7.347
7.299

7.943
8.045
7.854
7.904
8.085

7.531
7.495
7.573
7.554
7.520

7.542
7.471
7.421
7.441
7.395

13.697
13.157
13.200
12.811
13.385

13.539
12.991
13.353
13.648
13.398

12.196
12.127
12.756
12.544
12.433

13.071
12.788
12.677
12.334
13.031

13.180
12.654
13.063
13.382
13.158

12.143
11.985
12.580
12.428
12.276

27

Table 7 Empirical coverage, average width and interval score of nonparametric bootstrap

intervals using quantiles for δ0

2 with ν = 1

95%

90%

80%

T

N

Empirical
coverage

Average
width

Average
interval score

Empirical
coverage

Average
width

Average
interval score

Empirical
coverage

Average
width

Average
interval score

Nonparametric bootstrap intervals using quantiles

200

500

1000

200

500

1000

50
100
200
500
1000

50
100
200
500
1000

50
100
200
500
1000

50
100
200
500
1000

50
100
200
500
1000

50
100
200
500
1000

0.820
0.795
0.807
0.809
0.816

0.894
0.897
0.892
0.898
0.894

0.919
0.926
0.915
0.928
0.919

0.857
0.835
0.833
0.841
0.837

0.905
0.917
0.914
0.920
0.920

0.936
0.938
0.927
0.941
0.936

2.264
2.225
2.176
2.185
2.185

2.614
2.550
2.576
2.599
2.564

2.720
2.697
2.672
2.668
2.682

2.314
2.271
2.224
2.235
2.233

2.644
2.579
2.606
2.628
2.591

2.742
2.717
2.693
2.691
2.706

6.900
8.224
7.376
7.212
7.343

5.478
5.205
5.342
5.202
5.193

4.608
4.502
4.554
4.549
4.672

0.753
0.748
0.764
0.761
0.761

0.846
0.844
0.853
0.860
0.862

0.879
0.876
0.869
0.884
0.868

1.876
1.838
1.801
1.810
1.809

2.184
2.130
2.148
2.167
2.139

2.280
2.259
2.237
2.237
2.248

5.059
5.988
5.334
5.172
5.224

4.263
4.062
4.154
4.049
4.052

3.899
3.766
3.775
3.683
3.869

Parametric bootstrap intervals based on normality

6.008
7.695
6.523
6.175
6.293

4.733
4.528
4.706
4.551
4.597

4.331
4.205
4.023
4.160
4.329

0.781
0.783
0.789
0.778
0.768

0.845
0.868
0.868
0.868
0.867

0.891
0.893
0.887
0.892
0.881

1.942
1.906
1.867
1.875
1.874

2.219
2.164
2.187
2.206
2.175

2.301
2.280
2.260
2.259
2.271

5.015
5.851
5.131
4.979
5.109

4.177
3.892
4.027
3.896
3.867

3.802
3.679
3.624
3.581
3.769

0.634
0.649
0.660
0.646
0.655

0.731
0.746
0.768
0.764
0.753

0.795
0.768
0.789
0.794
0.762

0.658
0.660
0.677
0.663
0.680

0.743
0.761
0.780
0.769
0.765

0.800
0.771
0.796
0.801
0.763

1.442
1.415
1.387
1.393
1.391

1.691
1.652
1.667
1.678
1.656

1.772
1.753
1.739
1.737
1.749

1.513
1.485
1.454
1.461
1.460

1.729
1.686
1.704
1.719
1.694

1.793
1.777
1.761
1.760
1.769

4.259
4.609
4.147
4.127
4.185

3.682
3.420
3.439
3.402
3.360

3.324
3.292
3.192
3.156
3.362

4.216
4.534
4.102
4.092
4.125

3.656
3.382
3.408
3.390
3.342

3.305
3.266
3.177
3.135
3.363

28

outperform the other two competitors for δ1 with almost all combinations of N and T and for δ2

with small T. Meanwhile, the failure of nonparametric bootstrap intervals using quantiles and

parametric bootstrap intervals based on normality veriﬁes the skewness on the distribution of

δi(k). Although some bias-corrected intervals may also be constructed, for example, by double

bootstrap, to improve the empirical coverage probabilities further, those methods on reducing
(cid:98)
the error of bootstrap intervals generally have signiﬁcant requirements on computations and

are beyond the scope of this work.

Table 8 Empirical coverage, average width and interval score of unreversed nonparametric

bootstrap intervals using quantiles for δ0

1 with ν = 1

Unreversed nonparametric bootstrap intervals using quantiles

T

N

200

500

1000

50
100
200
500
1000

50
100
200
500
1000

50
100
200
500
1000

Empirical
coverage
0.956
0.959
0.959
0.960
0.954

0.947
0.951
0.947
0.941
0.946

0.955
0.958
0.944
0.951
0.957

95%

Average
width
11.881
11.999
11.732
11.805
12.077

11.377
11.326
11.444
11.426
11.357

11.440
11.322
11.263
11.281
11.221

Average
interval score
15.015
14.554
14.986
14.519
15.885

Empirical
coverage
0.913
0.909
0.909
0.912
0.914

15.161
14.969
15.947
16.354
15.300

14.654
14.361
15.158
14.943
14.608

0.901
0.900
0.904
0.901
0.896

0.910
0.914
0.906
0.901
0.905

90%

Average
width
9.775
9.895
9.676
9.730
9.967

9.481
9.441
9.541
9.521
9.478

9.582
9.490
9.422
9.457
9.395

Average
interval score
13.388
12.973
13.065
12.925
13.801

Empirical
coverage
0.815
0.813
0.823
0.844
0.820

80%

Average
width
7.470
7.587
7.390
7.444
7.623

Average
interval score
11.862
11.561
11.500
11.144
11.833

13.763
13.533
14.044
14.474
13.906

13.399
13.157
13.922
13.633
13.326

0.793
0.818
0.810
0.792
0.775

0.818
0.810
0.811
0.811
0.817

7.347
7.317
7.391
7.366
7.339

7.446
7.372
7.324
7.347
7.299

12.370
11.918
12.305
12.665
12.620

12.072
11.781
12.264
12.262
12.017

6 Empirical application: Particulate matter concentration

We apply the proposed AR-sieve bootstrap methods on an empirical data set of high-dimensional

time series. The raw data are observations of PM10 particles in the air, collected on a half-hour

basis in Graz, Austria, from 1 Oct. 2010 to 31 Mar. 2011. PM10 particles represent a common

type of air pollutant that can be found in smoke and dust with an aerodynamic diameter of less

than 0.01mm.

This data set has been studied by H ¨ormann et al. (2015) for topics of dynamic functional

principal component analysis (FPCA) and by Shang (2018) for comparisons of bootstrap meth-

ods for stationary functional time series. The original data are pre-processed by a square-root

transformation to stabilise the variance and avoid heavy-tailed observations as directed by

Aue et al. (2015) and H ¨ormann et al. (2015). The square-root of PM10 levels contained in a

29

Table 9 Empirical coverage, average width and interval score of unreversed nonparametric

bootstrap intervals using quantiles for δ0

2 with ν = 1

Unreversed nonparametric bootstrap intervals using quantiles

Empirical
coverage
0.861
0.846
0.848
0.851
0.848

95%

Average
width
2.264
2.225
2.176
2.185
2.185

Average
interval score
5.674
6.225
6.303
6.057
6.018

Empirical
coverage
0.786
0.769
0.783
0.776
0.780

0.908
0.919
0.917
0.923
0.915

0.938
0.938
0.929
0.934
0.934

2.614
2.550
2.576
2.599
2.564

2.720
2.697
2.672
2.668
2.682

4.757
4.468
4.606
4.329
4.479

4.155
4.219
4.115
4.295
4.280

0.861
0.854
0.874
0.870
0.867

0.887
0.879
0.872
0.886
0.873

90%

Average
width
1.876
1.838
1.801
1.810
1.809

2.184
2.130
2.148
2.167
2.139

2.280
2.259
2.237
2.237
2.248

T

N

200

500

1000

50
100
200
500
1000

50
100
200
500
1000

50
100
200
500
1000

Average
interval score
4.698
5.036
4.997
4.935
4.899

Empirical
coverage
0.675
0.649
0.667
0.651
0.652

80%

Average
width
1.442
1.415
1.387
1.393
1.391

Average
interval score
3.843
4.077
3.959
4.008
3.965

4.092
3.880
3.928
3.799
3.891

3.698
3.706
3.709
3.715
3.787

0.757
0.751
0.778
0.777
0.758

0.798
0.777
0.780
0.802
0.767

1.691
1.652
1.667
1.678
1.656

1.772
1.753
1.739
1.737
1.749

3.467
3.382
3.300
3.250
3.320

3.267
3.272
3.240
3.185
3.342

48 × 182 matrix are then plotted in Figure 2a as high-dimensional time series over 182 days with

dimension of 48 and in Figure 2b as 182 repeats of 48 half-hourly observations within each day.

In general, the PM10 concentration levels are relatively high in winters when the temperatures

are low and the pollutants relating to daily life such as trafﬁcs and heating lack space to disperse

in the atmosphere. The day-to-day PM10 levels in winter, therefore, are highly temporally

dependent, while the half-hourly observations in each day experience similar patterns which

are mainly related to people’s day-to-day life and temperature.

(a) Univariate time series plot

(b) Functional time series plot

Figure 2 Observed time series of (square-root) PM10 levels

In H ¨ormann et al. (2015) and Shang (2018), observations of half-hourly PM10 levels as in

30

Figure 2b are assumed to come from a functional curve. In general, for a functional time series,

the original observations are smoothed before further studies such as FPCA and functional

bootstrap. Hence, according to H ¨ormann et al. (2015) and Shang (2018), there are 182 temporal

dependent functional curves each smoothed from 48 observations. However, as illustrated in

Appendix A of this work, the pre-smoothing results rely heavily on the smoothness condition of

the functional curve. When the observations are not dense enough, pre-smoothing may cause a

loss of information, especially on local patterns. To maintain the original features of time-series

observations to the greatest extent, we treat the data as a multivariate or high-dimensional time

series. We then perform the proposed AR-sieve bootstrap methods with a factor model on this

48 by 182 matrix of time series. This creates a bootstrap conﬁdence interval for the mean levels

of (square root) PM10 which are temporal dependent at each half-hourly time point, and to

create a bootstrap conﬁdence surface for the lag-1 autocovariance matrix of (square root) PM10

levels.

In Figure 3, a 90% nonparametric bootstrap interval using quantiles is created on the mean

levels of (square root) PM10, deﬁned as θy := Qµ f with µ f denoting the population mean of

temporal dependent factors {ft}. From this plot of sample estimate and conﬁdence interval of

θy, it is clear that local patterns, for example, between 4th and 10th half-hourly time points, are

preserved ﬂawlessly by our proposed AR-sieve bootstrap methods based on high-dimensional

time series. Similarly, a sample estimate and a 90% unreversed nonparametric bootstrap interval

using quantiles for lag-1 autocovariance matrix Cov(yt, yt+1) of temporal dependent (square

root) PM10 levels at 48 half-hourly time points are also computed and presented in Figure 4. This

unreversed nonparametric bootstrap interval using quantiles provides interval estimates on

autocovariance of (square root) PM10 levels between two consecutive days, where, as shown in

Figure 4, the local patterns are again completely preserved by our proposed AR-sieve bootstrap

methods.

7 Conclusions and discussions

We apply dimension-reduction methods, such as factor models, to pursue a new approach -

AR-sieve bootstrap on high-dimensional data. Speciﬁcally, we suggest using autocovariance

to estimate the factor model and perform an AR-sieve bootstrap on the estimated factors to

provide ultimate inferences on the original time series. Our proposed AR-sieve bootstrap

methods using factor models provide valid statistical inferences on the mean statistic and

maintain consistency on bootstrap estimates of spiked eigenvalues of autocovariance matrices.

31

Figure 3 90% AR-sieve bootstrap conﬁdence interval for the mean of temporal dependent

(square root) PM10 levels at 48 half-hourly time

Figure 4 90% AR-sieve bootstrap conﬁdence surface for lag-1 autocovariance of temporal de-

pendent (square root) PM10 levels at 48 half-hourly time point

32

Simulation studies provide numerical evidence on the ﬁnite-sample performance of the AR-

sieve bootstrap methods on high-dimensional time series following strong factor models. At

last, we apply our methods to PM10 data for constructing bootstrap conﬁdence intervals for

mean vector and autocovariance matrix, respectively.

Our work is crucial as a building block of bootstrap methods for high-dimensional time

series. We propose a low-rank model for AR-sieve bootstrap on high-dimensional stationary

time series. There are two ways in which the present paper could be further extended: 1) The

asymptotics of the bootstrap validity on the mean statistics can be extended for weaker factor

models; 2) While AR-sieve bootstrap is only valid for stationary time series, alternative bootstrap

methods can be considered on the factors where the dimension has been reduced.

References

Ahn, S. K. & Reinsel, G. C. (1988), ‘Nested reduced-rank autoregressive models for multiple

time series’, Journal of the American Statistical Association: Theory and Methods 83(403), 849–856.

Akaike, H. (1974), ‘A new look at the statistical model identiﬁcation’, IEEE Transactions on

Automatic Control 19(6), 716–723.

Aue, A., Norinho, D. D. & H ¨ormann, S. (2015), ‘On the prediction of stationary functional time

series’, Journal of the American Statistical Association: Theory and Methods 110(509), 378–392.

Bai, J. & Ng, S. (2002), ‘Determining the number of factors in approximate factor models’,

Econometrica 70(1), 191–221.

Bathia, N., Yao, Q. & Ziegelmann, F. (2010), ‘Identifying the ﬁnite dimensionality of curve time

series’, The Annals of Statistics 38(6), 3352–3386.

Box, G. E. P. & Tiao, G. C. (1977), ‘A canonical analysis of multiple time series’, Biometrika

64(2), 355–365.

Brockwell, P. J. & Davis, R. A. (1991), Time Series: Theory and Methods, Springer Series in Statistics,

2nd edn, Springer-Verlag, New York.

B ¨uhlmann, P. (1997), ‘Sieve bootstrap for time series’, Bernoulli 3(2), 123–148.

Chen, X. (2018), ‘Gaussian and bootstrap approximations for high-dimensional U-statistics and

their applications’, The Annals of Statistics 46(2), 642 – 678.

33

Cheng, R. & Pourahmadi, M. (1993), ‘Baxter’s inequality and convergence of ﬁnite predictors of

multivariate stochastic processess’, Probability Theory and Related Fields 95(1), 115–124.

Chernozhukov, V., Chetverikov, D. & Kato, K. (2017), ‘Central limit theorems and bootstrap in

high dimensions’, The Annals of Probability 45(4), 2309–2352.

Cram´er, H. & Wold, H. (1936), ‘Some theorems on distribution functions’, Journal of the London

Mathematical Society s1-11(4), 290–294.

Dahlhaus, R. & Janas, D. (1996), ‘A frequency domain bootstrap for ratio statistics in time series

analysis’, The Annals of Statistics 24(5), 1934 – 1963.

Efron, B. (1979), ‘Bootstrap methods: another look at the jackknife’, The Annals of Statistics

7(1), 1–26.

El Karoui, N. & Purdom, E. (2018), ‘Can we trust the bootstrap in high-dimensions? the case of

linear models’, The Journal of Machine Learning Research 19(1), 170–235.

Fan, J., Liao, Y. & Mincheva, M. (2011), ‘High-dimensional covariance matrix estimation in

approximate factor models’, The Annals of Statistics 39(6), 3320–3356.

Fan, J., Liao, Y. & Mincheva, M. (2013), ‘Large covariance estimation by thresholding principal

orthogonal complements’, Journal of the Royal Statistical Society. Series B (Statistical Methodology)

75(4), 603–680.

Franke, J. & Hardle, W. (1992), ‘On bootstrapping kernel spectral estimates’, The Annals of

Statistics 20(1), 121 – 145.

Gneiting, T. & Raftery, A. E. (2007), ‘Strictly proper scoring rules, prediction, and estimation’,

Journal of the American Statistical Association: Review Article 102(477), 359–378.

Hidalgo, J. (2021), ‘Bootstrap long memory processes in the frequency domain’, The Annals of

Statistics 49(3), 1407 – 1435.

H ¨ormann, S., Kidzi ´nski, Ł. & Hallin, M. (2015), ‘Dynamic functional principal components’,

Journal of the Royal Statistical Society: Series B (Statistical Methodology) 77(2), 319–348.

Krampe, J., Kreiss, J.-P. & Paparoditis, E. (2021), ‘Bootstrap based inference for sparse high-

dimensional time series models’, Bernoulli 27(3), 1441 – 1466.

Kreiss, J.-P. (1988), Asymptotic statistical inference for a class of stochastic processes, PhD thesis,

Department of Mathematics, University of Hamburg.

34

Kreiss, J.-P. (1992), Bootstrap procedures for AR (∞) — processes, in K.-H. J ¨ockel, G. Rothe &

W. Sendler, eds, ‘Bootstrapping and Related Techniques’, Lecture Notes in Economics and

Mathematical Systems, Springer, Berlin, Heidelberg, pp. 107–113.

Kreiss, J.-P., Paparoditis, E. & Politis, D. N. (2011), ‘On the range of validity of the autoregressive

sieve bootstrap’, The Annals of Statistics 39(4), 2103–2130.

K ¨unsch, H. R. (1989), ‘The jackknife and the bootstrap for general stationary observations’, The

Annals of Statistics 17(3), 1217–1241.

Lam, C., Yao, Q. & Bathia, N. (2011), ‘Estimation of latent factors for high-dimensional time

series’, Biometrika 98(4), 901–918.

Lee, S. M. S. & Lai, P. Y. (2009), ‘Double block bootstrap conﬁdence intervals for dependent

data’, Biometrika 96(2), 427–443.

Meyer, M. & Kreiss, J.-P. (2015), ‘On the vector autoregressive sieve bootstrap’, Journal of Time

Series Analysis 36(3), 377–397.

Meyer, M., Paparoditis, E. & Kreiss, J.-P. (2020), ‘Extending the validity of frequency domain

bootstrap methods to general stationary processes’, The Annals of Statistics 48(4), 2404 – 2427.

Nordman, D. J. & Lahiri, S. N. (2012), ‘Block bootstraps for time series with ﬁxed regressors’,

Journal of the American Statistical Association: Theory and Methods 107(497), 233–246.

Paparoditis, E. (2018), ‘Sieve bootstrap for functional time series’, The Annals of Statistics

46(6B), 3510–3538.

Paparoditis, E. & Shang, H. L. (2021), ‘Bootstrap prediction bands for functional time series’,

Journal of the American Statistical Association: Theory and Methods In press.

Pena, D. & Box, G. E. P. (1987), ‘Identifying a simplifying structure in time series’, Journal of the

American Statistical Association: Theory and Methods 82(399), 836–843.

Politis, D. N. & Romano, J. P. (1994), ‘The stationary bootstrap’, Journal of the American Statistical

Association: Theory and Methods 89(428), 1303–1313.

Politis, D. N., Romano, J. P. & Wolf, M. (1997), ‘Subsampling for heteroskedastic time series’,

Journal of Econometrics 81(2), 281–317.

Schwarz, G. (1978), ‘Estimating the dimension of a model’, The Annals of Statistics 6(2), 461–464.

35

Shang, H. L. (2018), ‘Bootstrap methods for stationary functional time series’, Statistics and

Computing 28(1), 1–10.

Sowell, F. (1989), A decomposition of block toeplitz matrices with applications to vector time

series, Technical report, Carnegie Mellon University.

Tiao, G. C. & Tsay, R. S. (1989), ‘Model speciﬁcation in multivariate time series’, Journal of the

Royal Statistical Society. Series B (Methodological) 51(2), 157–213.

Wiener, N. & Masani, P. (1958), ‘The prediction theory of multivariate stochastic processes, II:

The linear predictor’, Acta Mathematica 99, 93–137.

36

Supplement to “AR-sieve Bootstrap for High-dimensional Time Series”

Daning Bi, Han Lin Shang, Yanrong Yang, Huanjun Zhu

This supplementary material contains discussions of applying the proposed AR-sieve boot-

strap on sparsely-observed functional time series, and technical proofs of results in the original

paper “AR-sieve Bootstrap for High-dimensional Time Series”. In Appendix A, we introduce

the smoothing problem on sparsely-observed functional time series and then propose to treating

it as high-dimensional data when applying the AR-sieve bootstrap. Some simulations are also

provided. In Appendix B, proofs of main theorems are presented, while some auxiliary lemmas

and their proofs are left in Appendix C.

A Applications on Sparsely-observed Functional Time Series

The second contribution of this work is that we compare the proposed novel AR-sieve bootstrap

for high-dimensional time series with the AR-sieve bootstrap method for functional time

series (Paparoditis 2018) in terms of their applications on sparse and unsmoothed functional

observations. And we suggest that the sparse and unsmoothed observations need to be treated

as high-dimensional time series and the AR-sieve bootstrap proposed in this work needs to be

applied. In the literature of functional time series studies, a very fundamental assumption is

that the actual observations come from a smoothed functional curve and statistical inferences

for functional data usually require the observations to be dense. In a classic functional set-up,

dense and discrete points are observed on a sample of T curves. Denoted by Nt the number

of observations for the curve t, the discussions on the density of observations in functional

data literature are generally through assumptions made on Nt. Typically, when Nt is much

larger than the sample size T, the data can be considered dense functional data where each

curve can be well smoothed before analysis. However, in the case where Nt is small compared

with sample size T for all t, the discrete observations should be considered as sparse along

the population functional curve. The fundamental problem of sparse functional data is that

the local patterns of population functional curve are generally not captured by those sparse

observations.

To illustrate the potential problems of pre-smoothing sparse observations for functional

time series analysis, we consider a toy example. For a square-integrable functional process

{X (u), u ∈ I}, let yi,t be the ith observation of {Xt(·)}, observed at a random time t with the

measurement errors deﬁned as (cid:101)i,t for t = 1, 2, ..., T and i = 1, 2, ..., N. Consider now for a model

1

of functional observations

yi,t = Xt(ui) + (cid:101)i,t, ui ∈ I,

(9)

where (cid:101)i,t is independent and identically distributed (i.i.d.) with E((cid:101)i,t) = 0, V((cid:101)i,t) = σ2 and I
is a functional support. In this model, the observations of {Xt(·)} are assumed to be equally

spaced, and the number of measurements N assesses the density and design of the actual

observations. In the functional data analysis, Xt(ui) can be estimated or recovered by some

smoothing methods such as a linear smoother as follows,

Xt(ui) =

N
∑
j=1

(cid:99)

wi(uj)yt,i,

where wi(uj) is the weight of jth point on the ith point with ∑N
j=1 wi(uj) = 1 for t = 1, 2, ..., T
and i = 1, 2, ..., N. The accuracy of the smoothing curve is highly related to the density of

observations and measurement errors. If observations along the curve are equally spaced,

the change of density can affect the quality of smoothness and its recovering power to the

population curve. For a relatively sparse curve, smoothing can fail to work under certain

situations; for example, when there are local patterns that observations are too sparse to capture.

To visually depict this phenomenon, we provide a toy example by simulations in the following

part. We consider a contaminated functional time series model generated from three Fourier

bases with different frequencies reﬂecting local patterns. The details of the simulation setting

can be found in Section A.1. The curves in Figure 5 are plotted based on 401 grid points deﬁned

on a functional support [0, 1], whereas the actual number of observations N along each curve

are chosen as 51, 21 and 5 to address different observation densities. As shown in Figure 5,

when the observations (red points) become sparse (but still equally spaced), the (red) smoothing

curve can lead to an obvious misleading result with local patterns not accurately captured by

the smoothing curve. The errors associated with pre-smoothing on those sparse observations

are generally large. In this situation, the assumption of dense functional data suffers from

insufﬁcient observations along each curve. As a result, we cannot adopt the pre-smoothing

results based on functional set-up but instead treat the data as multivariate time series with

growing dimensions. In other words, when N grows with sample size T but at a relatively

slower rate, the real data may adapt to a high-dimensional set-up rather than a functional

set-up, which makes statistical inferences and applications rather different. This phenomenon

is associated with an area where functional data analysis and high-dimensional data analysis

2

Figure 5 Example of smoothing error of sparse functional time series observations

may overlap yet follow different assumptions and produce quite different asymptotic results.

In contrast to functional data analysis, where the increase of observations along a curve

can practically improve pre-smoothing and recovering the functional curve, the growing of

dimensions is associated with the increase of complexity for high-dimensional data analysis.

This key difference makes it vital to choose between functional time series and high-dimensional

time series methods. In the following part, we consider the situation where N is growing but

not fast enough. The curve smoothed from the sparse observations is inaccurate, especially to

local patterns of a functional curve. We apply the proposed AR-sieve bootstrap method for

studying the inferences of this type of high-dimensional time series.

A.1 Smoothing on sparse discrete functional time series

To study the impact of smoothing on the sparse functional time series observations, we can

compare bootstrap samples’ empirical distributions under various densities of observations.

To start, we ﬁrst assume the data are originated from functional curves, which are temporal

dependent. Recall model (9) that

yt,i = Xt(ui) + (cid:101)t,i, ui ∈ I,

where (cid:101)t,i is i.i.d. with E((cid:101)t,i) = 0 and V((cid:101)t,i) = σ2, for t = 1, 2, ..., T and i = 1, 2, ..., N. In this

model, the number of measurements N reﬂect the density of the actual observations. To study

the impact of density, we assume the observations are equally spaced and generated from a

3

three factors’ model

yt = Qft + ut,

where ut,i, the element in {ut}, is independent N (0, 1) random noise, Q is a N × 3 matrix
with each column a Fourier basis and cos(2πi/N), cos(4πi/N), 0.5 cos(16πi/N) as ith element,

respectively. The factors {ft} follows a VAR(1) model with a coefﬁcient matrix

0.5 0.1 0.1

0.1 0.5 0.1

0.1 0.1 0.5















and errors independent simulated from N (0, 1). The Fourier basis is selected to produce

a smoothed population curve, with the third basis reﬂecting local patterns. Hence, we can

generate discrete observations from a functional curve with local patterns. In Section 1, we have

presented plots of {yt} at a particular time t with three different densities of observations to

illustrate a smoothing’s potential issue. This section takes it one step further and considers a

wider choice of densities so that the actual dimensions of observations along each curve are

N = 101, 51, 21, 17, 11 and 5.

For the same choice of time t as in Section 1, we have generated 6 plots under various

densities in Figure 6 to compare the smoothing results with the population true curve and noisy

curve with small measurement errors. The smoothing results are obtained using B-splines with

the number of basis functions set to N, the actual number of observations in each case, and the

roughness penalties selected based on generalised cross-validation (GCV). As depicted in Figure

6, when the actual number of observations N is relatively small, for example, N < 21, some

local patterns of the population curve are generally not captured. In addition, the smoothing

curve sometimes also averaged out the actual observations to achieve relatively ﬂat results, for

example, when N = 21, 17 and 5 as in Figure 6. As a result, the observations after smoothing are

generally less spread than the original observations, which produces very different bootstrap

samples and inferences’ results. To see that, we generate B = 499 AR-sieve bootstrap samples

and computed two summary statistics to compare the bootstrap distribution based on original

observations with smoothed observations. We use AR-sieve bootstrap to obtain estimates of

a so-called (standardised) mean statistic, computed as y∗ =
4.2, and δ∗

1 , the estimate of (standardised) largest eigenvalue of symmetrised lag-1 sample

1(cid:62)

Qf ∗ according to Theorem

√
T√
N

(cid:98)

autocovariance matrix as deﬁned in Proposition 4.4, to compare bootstrap samples from original

4

Figure 6 Example of smoothing errors on sparse functional observations

observations with bootstrap samples from pre-smoothed observations.

Figures 7 and 8 compare the histograms and boxplots of δ∗

1 , the AR-sieve bootstrap estimates

of largest eigenvalue of symmetrised lag-1 autocovariance matrix, while Figures 9 and 10 com-

pare the histograms and boxplots of y∗, the AR-sieve bootstrap estimates of the (standardised)

mean statistic. As seen in Figure 6, when N = 21, 17 and 5, the pre-smoothed observations are

averaged out compared with the original observations. As a result, the bootstrap estimates

of the two statistics perform differently before and after smoothing, when N = 21, 17 and 5.
Figures 7 and 9 use boxplots to present the difference of empirical distributions of y∗ and δ∗

1 for
N = 21, 17 and 5, whereas Figures 8 and 10 illustrate the impact of smoothing by comparing
the histograms of y∗ and δ∗
1 .

The last example we presented in Figure 11 illustrates results of AR-sieve bootstrap estimates

(bootstrap average) of the functional mean curve when we pre-smooth the observations under

various densities of data. As shown in Figure 11, when the actual observations are relatively

dense, for example, N ≥ 51, AR-sieve bootstrap estimates of the mean functional curve are

close to the pre-smoothed curve and the population curve. However, when the observations are

5

Figure 7 Histograms of δ∗

1 , the AR-sieve bootstrap estimates of the largest eigenvalue of sym-

metrised lag-1 sample autocovariance matrix

6

Figure 8 Boxplots of δ∗

1 , the AR-sieve bootstrap estimates of the largest eigenvalue of sym-

metrised lag-1 sample autocovariance matrix

7

Figure 9 Histograms of y∗, the AR-sieve bootstrap estimates of the mean statistic

8

Figure 10 Boxplots of y∗, the AR-sieve bootstrap estimates of the mean statistic

9

Figure 11 Example of errors of the AR-sieve bootstrap mean curve for sparse functional obser-

vations

10

sparse, for example, N ≤ 21, AR-sieve bootstrap estimates of the mean functional curve do not

correctly capture the local patterns of the population curve, which is due to the unacceptable

smoothing results. This result is also typical evidence of the impact of pre-smoothing on

AR-sieve bootstrap for functional time series. Hence, when the actual functional time series

observations are sparse, pre-smoothing may signiﬁcantly impact statistical inferences, including

bootstrap. In fact, for many real-world time series data, the rule on considering a data set

as dense functional time series is generally not clear and often varies across researchers and

problems. Practically speaking, the impact of observations’ density is only about whether to

pre-smooth the functional time series before performing bootstrap or other statistical analysis.

Nonetheless, the theoretical assumptions behind functional time series and high-dimensional

time series vary, leading to very different theoretical results on statistical inferences, includ-

ing AR-sieve bootstrap. On the other hand, this difference in data structure assumptions

demonstrates the importance of developing statistical methods on sparse functional time series

observations. It veriﬁes our contributions on the building blocks of AR-sieve bootstrap for

high-dimensional time series.

B Technical proof of theorems

l=1

Al,pf b

t−l + eb

t,p, where {

Proof of Theorem 4.2. Let f b

t = ∑p
Al,p, l = 1, 2, ..., p} are the estimators
of AR coefﬁcient matrices based on true factors {ft}, and {eb
t,p, t = p + 1, p + 2, ..., T} are gener-
(cid:101)
et,p −
Aft−l
t } are bootstrap pseudo-variables generated based
t } are bootstrapped based on the

t=p+1
on the true factors {ft} rather than {

ated by i.i.d. resampling from the centered residuals (

ft}. Recall that {f ∗

et,p. Therefore, {f b

et,p = ft − ∑p

eT,p = 1
T−p

eT,p) with

and

∑T

l=1

(cid:101)

(cid:101)

(cid:101)

(cid:101)

(cid:101)

(cid:101)

(cid:101)
et,p −

ft − ∑p
centered residuals (
et,p, and
we deﬁne E∗ and Cov∗ as the expectation and covariance with respect to the measure assigning
probability 1/(T − p) to each observation, respectively. Therefore, E∗f ∗

eT,p = 1
T−p

fT by deﬁnition and

eT,p) with

et,p =
(cid:98)

ft−l and

Al,p

t=p+1

∑T

l=1

(cid:98)

(cid:98)

(cid:98)

(cid:98)

(cid:98)

(cid:98)

(cid:98)

(cid:98)

T =

we can write

(cid:98)

√

Tc(cid:62)

Q

T − E∗f ∗
f ∗

T

(cid:0)

(cid:98)

=: M1 + M2 + M3

√

(cid:1)

=

Tc(cid:62)Q

T − E∗f b
f b

T

√

+

Tc(cid:62)Q

(cid:16)

(cid:17)
T − E∗f ∗
f ∗
T

+

−

with obvious deﬁnitions of M1, M2 and M3.

(cid:104)(cid:0)

(cid:16)

(cid:1)

√

Tc(cid:62)

Q − Q

T − E∗f ∗
f ∗

T

(cid:16)
T − E∗f b
f b

(cid:98)

T

(cid:17) (cid:0)
,

(cid:17)(cid:105)

(cid:1)

For the term M1, under Assumptions 3.1 (iii), 4.1 and the additional assumption in Theorem

11

4.2 that limT→∞ V(

√

TfT) = ∑k∈Z
√

have the following CLT for

T fT

Γf (k) < ∞, using Theorem 2.1 in Politis et al. (1997), we

√

T

fT − EfT

d→ N

(cid:0)

(cid:1)

0, ∑
k∈Z

(cid:32)

Γf (k)

.

(cid:33)

Moreover, under the additional assumptions in Theorem 4.2, c(cid:62)Q is an r-dimensional vector
such that (cid:107)c(cid:62)Q(cid:107)(cid:96)1 < ∞ for a ﬁxed r, Therefore, under Assumptions 3.1 (ii) and 4.1, we can use
Cramer-Wold Theorem (Cram´er & Wold 1936) to conclude for the scalar

Tc(cid:62)QfT that

√

√

Tc(cid:62)Q

fT − EfT

d→ N

(cid:0)

(cid:1)

0, c(cid:62)Q

(cid:32)

∑
k∈Z

(cid:32)

Γf (k)

Q(cid:62)c

,

(cid:33)

(cid:33)

when T, N → ∞.

Besides, under the strong mixing condition on true factors {ft}, the empirical moments of

{et} converge to its population counterpart. Therefore, under all the assumptions of 4.2, we

fulﬁl all the conditions of Theorem 4.1 in Meyer & Kreiss (2015). Consequently, we can use

Theorem 4.1 in Meyer & Kreiss (2015) to conclude that the general VAR-sieve bootstrap is valid
Tc(cid:62)QfT shares the same CLT with its counterpart generated from the

Tc(cid:62)QfT since

for

√

√

companion process as discussed in Meyer & Kreiss (2015). Hence

√

dK

L

Tc(cid:62)

Q

T − E∗f ∗
f ∗

T

y1, y2, ..., yT

, L

√

Tc(cid:62)Q

fT − EfT

= oP (1)

(cid:16)

(cid:16)
as T, N → ∞.

(cid:0)

(cid:98)

(cid:1)(cid:12)
(cid:12)
(cid:12)

(cid:17)

(cid:16)

(cid:0)

(cid:1)(cid:17)(cid:17)

Therefore, to see the assertion in Theorem 4.2, we need to show that when T, N → ∞, both

M2 and M3 tend to 0 in probability, then apply Slutsky’s theorem.

To show M2 → 0 in probability for T, N → ∞, we ﬁrst of all notice that

√

Tc(cid:62)

Q − Q

T − E∗f ∗
f ∗

T

=

(cid:16)

(cid:98)

(cid:17) (cid:0)

(cid:1)

1
√
T

c(cid:62)

Q − Q

(cid:16)

(cid:98)

T
∑
t=1 (cid:16)

(cid:17)

f ∗
t −

fT

.

(cid:17)

(cid:98)

12

Therefore, we can show that

√

E

Tc(cid:62)

Q − Q

T − Ef ∗
f ∗

T

2

(cid:104)

(cid:34)

1
T

=E

=

≤

(cid:34)

1
T

c(cid:62)

1
T

c(cid:62)

c(cid:62)

(cid:13)
(cid:13)
(cid:13)
=OP

(cid:32)

(cid:16)
(cid:98)
Q − Q

(cid:16)

(cid:98)
Q − Q

(cid:17) (cid:0)
T
∑
t=1 (cid:16)
(cid:17)
T
T
∑
∑
s=1
t=1

(cid:17)

(cid:16)

(cid:98)
Q − Q

2

(cid:16)
(cid:98)
T
1
∑
T2 (cid:13)
t=1
(cid:13)
(cid:13)
(cid:13)
(cid:13)
t e∗(cid:62)
e∗
t

(cid:17)(cid:13)
(cid:13)
T
(cid:13)
∑
E
s=1

(cid:16)

, then

(cid:1)(cid:105)

f ∗
t −

fT

(cid:35) (cid:34)

(cid:17)

E

(cid:98)
f ∗
t −

fT

f ∗
s −

T
∑
s=1 (cid:16)
f ∗
s −

fT

fT

(cid:17)
(cid:98)
(cid:62)

(cid:62)

(cid:62)

Q − Q

(cid:16)

(cid:98)
Q − Q

(cid:17)
(cid:62)

c

c

(cid:35)

(cid:35)

(cid:16)
T
∑
s=1

T
∑
t=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
f ∗
t −
(cid:13)

E

(cid:16)

(cid:17) (cid:16)

(cid:98)
f ∗
t −

fT

(cid:98)
f ∗
s −

f ∗
s −

fT

(cid:98)

(cid:17) (cid:16)

(cid:17) (cid:16)
(cid:62)

(cid:98)
fT

(cid:17)

(cid:98)

F(cid:33)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:17)

(cid:16)

fT

(cid:98)

,

(cid:17)

(cid:17)

(cid:98)
(cid:62)

F

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Deﬁne Σ∗

e,p := E∗

(cid:0)

(cid:1)

T
∑
t=1

T
∑
s=1

E∗

f ∗
t −

fT

f ∗
s −

fT

(cid:62)

=

(cid:16)

(cid:17) (cid:16)

(cid:98)

(cid:17)

(cid:98)

=

=

T
∑
t=1

T
∑
s=1

T
∑
t=1

T
∑
s=1

T
∑
t=1

T
∑
s=1

∞
∑
l=0

Ψ

l1,pe∗

∞
∑
l1=0
∞
∑
l2=0 (cid:16)
(cid:98)
t−le∗(cid:62)
e∗
E∗
t−l

l1,pe∗

(cid:98)
Ψ

t−l1

(cid:16)

(cid:17)

E∗



(cid:32)

E∗


∞
∑
l1=0

Ψ

l,p

(cid:98)

(cid:62)

∞
∑
l2=0

Ψ

l2,pe∗

s−l2 (cid:33)

t−l1 (cid:33) (cid:32)





(cid:98)
Ψ(cid:62)
l2,p

e∗(cid:62)
s−l2

Ψ(cid:62)

(cid:98)
s−t+l,p

(cid:17)

(cid:98)
= 0 for l1 (cid:54)= l2.

where e∗

t−l1

and e∗

t−l2

are i.i.d. bootstrapped therefore E∗

e∗
t−l1

e∗(cid:62)
t−l2

Hence we can show that

T
∑
t=1

T
∑
s=1

1
T2 (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

E∗

f ∗
t −

fT

f ∗
s −

fT

(cid:16)

(cid:17) (cid:16)

(cid:98)

(cid:17)

(cid:98)

(cid:62)

≤

1
T2

=OP

F

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:16)

F

(cid:13)
(cid:13)
(cid:13)
,

(cid:17)

Ψ

∞
∑
l=0 (cid:13)
(cid:13)
(cid:13) (cid:98)

l,p

F

(cid:13)
(cid:13)
(cid:13)

T
∑
t=1

Ψ

T
∑
s=1 (cid:13)
(cid:13)
(cid:13) (cid:98)

s−t+l,p

F

(cid:13)
(cid:13)
(cid:13)

Σ∗
e,p

(cid:13)
(cid:13)
(cid:13)

(cid:18)

1
T

(cid:19)

where we note that Lemmas C.5 and C.7 imply the summability of
is bounded for T → ∞. Therefore, 1
T

∑∞
l=0

∑T

∑T

Ψ

Ψ

and we can conclude that E∗

for M2 → 0 in probability conditional on the sample.

(cid:104)

√

Tc(cid:62)

(cid:13)
Q − Q
(cid:13)
(cid:13) (cid:98)

(cid:16)

(cid:98)

l,p

t=1
s=1
F
(cid:13)
T − E∗f ∗
f ∗
(cid:13)
(cid:13)
(cid:17) (cid:0)

T

2
(cid:13)
(cid:13)
(cid:13) (cid:98)
(cid:1)(cid:105)

Ψ

Ψ

l,p

, hence ∑T
s−t+l,p
s=1
F
(cid:13)
is bounded for T → ∞,
(cid:13)
(cid:13)
→ 0 in probability, which sufﬁces

(cid:13)
(cid:13)
s−t+l,p
(cid:13) (cid:98)

(cid:13)
(cid:13)
(cid:13) (cid:98)

F

(cid:13)
(cid:13)
F
(cid:13)
(cid:13)
(cid:13)
(cid:13)

13

For M3, we ﬁrst write

√

E∗

Tc(cid:62)Q

T − E∗f ∗
f ∗

T

−

T − E∗f b
f b

T

2

=E∗

√

(cid:104)

(cid:110)(cid:0)

Tc(cid:62)Q

f ∗
T −

fT

(cid:1)
−

(cid:16)
f b
T −

fT

(cid:17)(cid:111)(cid:105)
2

(cid:13)
≤(cid:107)c(cid:62)Q(cid:107)2 1
(cid:13)
(cid:13)
T

=OP

1
T

T
∑
t=1

(cid:32)

T
∑
t=1
T
∑
s=1

(cid:110)(cid:16)
T
∑
s=1

E∗

E∗

(cid:98)

(cid:16)
(cid:17)
f ∗
t −

fT

(cid:17)(cid:111)(cid:13)
(cid:13)
f b
t −
(cid:13)

fT

(cid:101)
−

f ∗
s −

fT

−

f b
s −

fT

(cid:62)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:110)(cid:16)
f ∗
t −

(cid:110)(cid:16)

(cid:17)

(cid:98)
−

(cid:16)
f b
t −

(cid:17)

(cid:16)

(cid:101)

(cid:17)(cid:111) (cid:110)(cid:16)
f ∗
s −

(cid:17)(cid:111) (cid:110)(cid:16)

(cid:17)

(cid:98)
−

(cid:16)
f b
s −

(cid:17)

(cid:16)

fT

(cid:98)

fT

(cid:101)

fT

(cid:101)

fT

(cid:98)

F

,

(cid:13)
(cid:13)
(cid:13)
(cid:13)
F(cid:33)

(cid:17)(cid:111)
(cid:62)

(cid:101)

(cid:17)(cid:111)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

where the last line follows from the fact that (cid:107)c(cid:62)Q(cid:107)2 is bounded when N → ∞. To proceed,

ﬁrst note that

1
T

1
T

1
T

1
T

1
T

T
∑
t=1

T
∑
s=1

T
∑
t=1

T
∑
s=1

T
∑
t=1

T
∑
s=1

T
∑
t=1

T
∑
s=1

T
∑
t=1

T
∑
s=1

=

=

+

=:

(cid:110)(cid:16)

E∗

(cid:40)

∞
∑
l1=0

E∗

E∗

∞
∑
l1=0

∞
∑
l1=0

(cid:40)

(cid:40)

Ψ

(cid:98)
Ψ

(cid:98)
Ψ

(cid:101)
(H1 + H2) ,

E∗

f ∗
t −

fT

−

f b
t −

fT

f ∗
s −

fT

−

f b
s −

fT

(cid:62)

(cid:16)

(cid:17)
(cid:98)
l1,pe∗

t−l1,p −

(cid:17)(cid:111) (cid:110)(cid:16)

(cid:101)
l1,peb

Ψ

t−l1,p

(cid:41) (cid:40)

∞
∑
l2=0

(cid:17)
(cid:16)
l2,pe∗

(cid:98)
Ψ

(cid:101)
Ψ
s−l2,p −

(cid:17)(cid:111)

l2,peb

s−l2,p

(cid:62)

(cid:41)

(cid:101)

∞
∑
l2=0

l1,pe∗

t−l1,p

l1,peb

t−l1,p

(cid:41) (cid:40)

(cid:41) (cid:40)

Ψ

l2,pe∗

s−l2,p −

l2,peb

s−l2,p −

∞
∑
l2=0

(cid:98)
Ψ

(cid:101)

l2,peb

s−l2,p

l2,pe∗

s−l2,p

(cid:62)
(cid:101)

(cid:41)

(cid:62)

(cid:41)

(cid:98)
Ψ

(cid:101)
Ψ

(cid:98)

with an obvious notation for H1 and H2. Then, we only consider H1 as H2 can be dealt with

similarly.

For H1, we can further decompose it as

H1 =

+

=

+

T
∑
s=1

T
∑
s=1

T
∑
s=1
T
∑
s=1

E∗

E∗

∞
∑
l=0
∞
∑
l=0

(cid:98)
Ψ

E∗

l,p

=:H11 + H12.

(cid:98)

∞
∑
l1=0

∞
∑
l1=0

(cid:40)

(cid:40)

Ψ

l1,pe∗

t−l1,p

(cid:98)
Ψ

l1,pe∗

t−l1,p

∞
∑
l2=0

∞
∑
l2=0

(cid:41) (cid:40)

(cid:41) (cid:40)

Ψ

l2,pe∗

s−l2,p −

Ψ

l2,pe∗

s−l2,p

(cid:98)
Ψ

l2,pe∗

s−l2,p −

(cid:101)
Ψ

l2,peb

s−l2,p

(cid:62)

(cid:62)

(cid:41)

(cid:41)

(cid:98)
E∗

Ψ

l,p

t−l,pe∗(cid:62)
e∗
t−l,p

Ψ

(cid:101)
l+s−t,p −

Ψ

(cid:101)
l+s−t,p

(cid:62)

(cid:110)

(cid:110)

(cid:111) (cid:110)
(cid:98)
t−l,p)(cid:62)
t−l,p − eb

t−l,p(e∗
e∗

(cid:111)

(cid:101)
Ψ(cid:62)
l+s−t,p

(cid:111)

(cid:101)

14

where the second last line follows from the bootstrap independence for l1 (cid:54)= l2. Hence we can

conclude for H11 that

1
T

T
∑
t=1

T
∑
s=1

(cid:107)H11(cid:107)F =

1
T

T
∑
t=1

≤

Σ∗
e,p

T
∑
s=1 (cid:13)
(cid:13)
(cid:13)
1
(cid:13)
(cid:13)
T

F

l,p

Ψ

∞
∑
l=0
∞
∑
l=0 (cid:13)
(cid:13)
(cid:13) (cid:98)
Ap −
Ap

Ψ

(cid:98)

(cid:13)
(cid:13)
3
(cid:13)
p
2

Σ∗
e,p

Ψ

l+s−t,p −

Ψ

l+s−t,p

(cid:101)
l+s−t,p −

Ψ

T
∑
s=1 (cid:13)
(cid:13)
(cid:13) (cid:98)

(cid:110)
T
(cid:98)
∑
t=1

F

(cid:17)

(cid:101)

l,p

(cid:13)
(cid:13)
(cid:13)
F

(cid:13)
(cid:13)
(cid:13)

(cid:62)

(cid:111)
Ψ

F

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
l+s−t,p

F

(cid:13)
(cid:13)
(cid:13)

(cid:16)

=oP (1) ,

(cid:13)
(cid:13)
(cid:13) (cid:98)

(cid:101)

(cid:13)
(cid:13)
(cid:13)
=OP

where the second last line follows from the results in Lemmas C.5 and C.7, and the last line

follows the result in Lemma C.7.

For H12 we can show that

1
T

T
∑
t=1

T
∑
s=1

(cid:107)H12(cid:107)F ≤

(cid:114)

2

E∗

e∗
t,p

E∗

e∗
t,p − eb
t,p

(cid:114)

(cid:13)
(cid:13)
(cid:13)
e∗
t,p − eb
t,p

(cid:13)
(cid:13)
(cid:13)

E∗

(cid:13)
(cid:13)
(cid:13)
(cid:32)(cid:114)

(cid:13)
(cid:13)
(cid:13)

,

2

(cid:33)

=OP

2 1
T

Ψ

∞
∑
l=0 (cid:13)
(cid:13)
(cid:13) (cid:98)

l,p

F

(cid:13)
(cid:13)
(cid:13)

T
∑
t=1

Ψ

T
∑
s=1 (cid:13)
(cid:13)
(cid:13) (cid:98)

l+s−t,p

F

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
where the last line follows from the same arguments on summability properties in Lemmas C.5.
→ 0 in probability. Recall that E∗ deﬁnes expectation
Hence it remains to show E∗

(cid:13)
(cid:13)
(cid:13)

2

e∗
t,p − eb
t,p

with respect to the measure assigning probability 1/(T − p) to each observation, this follows as

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

E∗

e∗
t,p − eb
t,p

2

=E∗

e∗
t,p − eb
t,p

e∗
t,p − eb
t,p

(cid:62)

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

(cid:26)(cid:16)
1
T − p

=

=

1
T − p

≤

2
T − p

≤

2
T − p

T
∑
t=p+1 (cid:110)
T
∑
t=p+1 (cid:110)
T
∑
t=p+1

T
∑
t=p+1

(cid:17) (cid:16)
et,p −

(

(cid:27)

(cid:17)
et,p −
eT,p) − (

eT,p)

(

et,p −

eT,p) − (

et,p −

eT,p)

(cid:111) (cid:110)

(cid:98)
et,p −

(

(cid:98)
et,p) − (

(cid:101)
eT,p −

(cid:101)
eT,p)

(cid:98)
et,p −

(

(cid:98)
et,p) − (

(cid:101)
eT,p −

(cid:101)
eT,p)

(cid:111) (cid:110)

(cid:101)
et,p

(cid:98)
2 + 2

2

(cid:101)
eT,p

+

(cid:98)
eT,p

2

(cid:101)
− 2

(cid:98)

eT,p

eT,p

(cid:13)
(cid:13)

(cid:101)
et,p

2 + 4

(cid:26)(cid:13)
(cid:13)
(cid:13)(cid:101)

eT,p

(cid:13)
(cid:13)
(cid:13)

2

(cid:13)
(cid:13)
(cid:13)

2

(cid:13)
(cid:13)
(cid:13)(cid:98)

eT,p

+

.

(cid:13)
(cid:13)
(cid:13)(cid:101)

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)(cid:98)

(cid:101)

(cid:27)

(cid:13)
(cid:13)
(cid:13)

(cid:98)
et,p −

(cid:13)
(cid:13)(cid:98)
et,p −

(cid:62)

(cid:111)

(cid:62)

(cid:111)

(cid:13)
(cid:13)(cid:98)

(cid:26)(cid:13)
(cid:13)
(cid:101)
(cid:13)(cid:101)
ft} have non-zero means,

(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
et,p =

(cid:27)

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)(cid:98)
ft − fT

− ∑p

l=1

Al,p

ft−l − f T

Al,p
l=1
(cid:98)

ft−l −

f T

. Without altering the idea of proof, to simplify
(cid:1)
(cid:1)

(cid:0)

(cid:0)

(cid:101)
ft} to denote the demeaned factors

(cid:17)

(cid:101)
ft − fT

and their

Recall that when {ft} and {
− ∑p

et,p =

ft −

and

fT

the notations used, we use {ft} and {
(cid:98)

(cid:98)

(cid:98)

(cid:98)

(cid:16)

(cid:17)

(cid:16)

(cid:98)

(cid:98)

sample counterparts

ft −

fT

(cid:16)

(cid:98)

(cid:17)

(cid:98)

, respectively. Therefore, with the same arguments in the proof

(cid:0)

(cid:1)

(cid:98)

15

of Lemma C.8, we have

2
T − p

T
∑
t=p+1

et,p −

et,p

(cid:13)
(cid:13)(cid:98)

(cid:13)
(cid:13)

(cid:101)

2 =

2
T − p

≤

4
T − p

≤

4
T − p

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

2

Al,p

ft−l

(cid:13)
(cid:13)
(cid:13)
(cid:98)
(cid:13)
ft−l − ft−l
(cid:13)

(cid:98)

2

(cid:13)
(cid:13)
(cid:13)

(

(cid:98)
ft − ft

T
∑
t=p+1 (cid:13)
(cid:13)
(cid:13)
(cid:13)
T
∑
(cid:13)
t=p+1 (cid:13)
(cid:13)
(cid:13) (cid:98)
T
∑
ft − ft
t=p+1 (cid:13)
(cid:13)
(cid:13) (cid:98)
Al,p −

Al,p

ft − ft) +

p
∑
l=1

(

Al,pft−l −

Al,p

ft−l)

2

+

(cid:101)

4
T − p

(cid:13)
(cid:13)
(cid:13)

2

+ 8

(cid:13)
(cid:13)
(cid:13)
1
T − p

p
∑
l=1 (cid:13)
(cid:13)
(cid:13) (cid:98)
T
∑
t=p+1

(cid:98)

(cid:98)
p
∑
l=1

T
∑
t=p+1 (cid:13)
(cid:13)
(cid:13)
(cid:101)
(cid:13)
1
2
(cid:13)
T − p
F

Al,p

(cid:13)
(cid:13)
(cid:13)
ft−l

2

Al,pft−l −

T
∑
t=p+1 (cid:13)
(cid:13)
(cid:13) (cid:98)

(cid:17)

(cid:101)
ft − ft

(cid:13)
(cid:13)
(cid:13) (cid:98)
1
√
N (cid:19)

(cid:13)
(cid:13)
(cid:13)

2

(cid:33)

+

2

(cid:33)

+ OP 

p8

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:32)

(cid:18)

+ OP

F

(cid:13)
(cid:13)
(cid:13)
(cid:13)
p
(cid:13)
∑
l=1 (cid:16)
1
√
T

2

F





Ap −

Ap

(cid:98)
+

(cid:101)

1
√
N (cid:19)

(cid:17)
2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:33)

p
∑
l=1 (cid:16)

+8

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
=OP

(cid:98)
sup
p+1≤t≤T

(cid:32)

1
√
T

=OP

(cid:32)(cid:18)
=oP(1),

(10)

2

F

is summable, which is implied by

where the third last line follows from the fact that

Al,p

Assumption 4.2 and Lemma C.3. The second last line is then a direct result of Lemmas C.3 and

C.4, and Assumption 4.3 implies the last line.

Furthermore,

eT,p = 1
T−p

∑T

t=p+1

et,p = 1
T−p

∑T

t=p+1

ft − ∑p

l=1

Al,p

ft−l

and we can show

that

(cid:98)

2

≤2

eT,p

(cid:13)
(cid:13)
(cid:13)(cid:98)

(cid:13)
(cid:13)
(cid:13)

This is because, ﬁrstly

(cid:98)

1
T − p

T
∑
t=p+1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

+ 2

ft

(cid:98)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:16)

(cid:98)

p
∑
l=1

Al,p

1
T − p

T
∑
t=p+1

(cid:17)

= oP(1).

(11)

(cid:98)

ft−l

(cid:98)

(cid:98)

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13) (cid:98)

(cid:13)
(cid:13)
(cid:13)

(cid:98)

2

1
T − p

T
∑
t=p+1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

ft

(cid:98)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
=OP

=OP

1
T − p

T
∑
t=p+1

1
T − p

1
T − p

(cid:19)

(cid:19)

(cid:18)

(cid:18)

ft

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
+ OP

+ 2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
1
(cid:13)
T − p

(cid:32)

+ OP

1
√
T

(cid:32)(cid:18)

1
T − p

T
∑
t=p+1 (cid:16)

T
∑
t=p+1 (cid:13)
(cid:13)
(cid:13) (cid:98)
2
1
√
N (cid:19)

+

(cid:33)

ft − ft

(cid:98)
ft − ft

2

2

(cid:17)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:33)

(cid:13)
(cid:13)
(cid:13)

= oP(1),

where the second last line follows as we have assumed the population mean of {ft} is 0 for

16

technical convenience. Moreover,

1
T − p

T
∑
t=p+1

p
∑
l=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Al,p

(cid:98)

ft−l

(cid:98)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤

Al,p

p
∑
F (cid:13)
l=1 (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13) (cid:98)
(cid:13)
(cid:13)
=OP (1) × OP
(cid:32)

(cid:13)
(cid:13)
(cid:13)

1
T − p

T
∑
t=p+1

1
T − p

+

ft−l

(cid:13)
(cid:13)
(cid:13)
(cid:98)
(cid:13)
1
√
(cid:13)
+
T

1
√
N (cid:33)

= oP(1),

where the second last line follows from the summability conditions in Lemma C.5, the order

(cid:112)

of

ft − ft

in Lemma C.3 and the fact that the mean of {

ft} is assumed to be 0 for technical

(cid:13)
convenience.
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13) (cid:98)
Lastly, we can show that

2

eT

→ 0 in probability with the same technique as stated

(cid:98)

eT

above for

probability. Together with the result that 1
T

. Hence with (10) and (11), we can conclude that 1
T

s=1 (cid:107)H12(cid:107)F → 0 in
s=1 (cid:107)H11(cid:107)F → 0 in probability, we have
s=1 (cid:107)H1(cid:107)F → 0 in probability. Therefore, it sufﬁces to conclude that M3 → 0 in

(cid:13)
(cid:13)
(cid:13)(cid:98)

1
T

∑T

t=1

∑T

∑T

∑T

(cid:13)
(cid:13)
(cid:13)

t=1

t=1

∑T

∑T

(cid:13)
(cid:13)
(cid:13)(cid:101)

(cid:13)
(cid:13)
(cid:13)

probability conditional on the sample.

Consequently, by utilizing Slutsky’s theorem conditional on the sample, we can conclude

that

√

dK

L

(cid:16)

(cid:16)

Tc(cid:62)

Q

T − E∗f ∗
f ∗

T

y1, y2, ..., yT

, L

√

Tc(cid:62)Q

fT − EfT

p
→ 0,

(cid:0)

(cid:98)

(cid:1)(cid:12)
(cid:12)
(cid:12)

(cid:17)

(cid:16)

(cid:0)

(cid:1)(cid:17)(cid:17)

Proof of Theorem 4.3. Without loss of generality, we again assume {ft} are the demeaned factors

(or the mean of factors are all 0) in this proof to simplify the notations.
t−l + e∗

Firstly, notice that f ∗

t = ∑∞
l=1

t = ∑p

t−l + e∗

Al,pf ∗

l,pe∗

Ψ

l=1

t = ∑∞
l=0

Ψ

l,pe∗

t−l. We can

then represent Γ∗

f (k) as

(cid:98)

(cid:98)

(cid:98)

f (k) = Cov∗(f ∗
Γ∗

t , f ∗

t+k)

∞
∑
l1=0

Ψ

l1,pe∗

t−l1

,

∞
∑
l2=0

Ψ

l2,pe∗

(cid:98)

Ψ

l1,pCov∗(e∗

t−l1

(cid:98)
, e∗
t+k−l2

t+k−l2 (cid:33)
Ψ(cid:62)
l2,p

)

(cid:98)

Ψ

l1,pCov∗(e∗

t−l1

, e∗

t−l1

Ψ(cid:62)

(cid:98)
l1+k,p

)

(cid:98)
Ψ
l,p

Σe,p

Ψ(cid:62)

l+k,p,

(cid:98)

(cid:32)
∞
∑
l2=0

= Cov∗

=

=

=

∞
∑
l1=0
∞
∑
l1=0
∞
∑
l=0

(cid:98)
where we stress the fact that Cov∗(e∗

(cid:98)

t−l1

(cid:98)
, e∗

t−l2

) = 0 for l1 (cid:54)= l2 and Cov∗(e∗

t−l1

, e∗

t−l1

) =

17

) =

Σe,p for all l1 ∈ Z, since e∗

t is uniformly distributed on the set of centered

E∗(e∗

t e∗(cid:62)
t
residuals (

et,p −
(cid:98)

eT). Similarly,

(cid:98)

(cid:98)

Γ

f (k) = Cov (ft, ft+k)

∞
∑
l1=0

Ψ

l1et−l1,

∞
∑
l2=0

Ψ

l2et+k−l2

(cid:33)

Ψ

l1Cov (et−l1, et+k−l2) Ψ(cid:62)
l2

Ψ

l1Cov (et−l1, et−l1) Ψ(cid:62)

l1+k

(cid:32)
∞
∑
l2=0

= Cov

=

=

=

∞
∑
l1=0
∞
∑
l1=0
∞
∑
l=0

Ψ

ΣeΨ(cid:62)

l+k,

l

where we write Σe = Cov(et, et) and use the fact that ft = ∑∞
et = ∑∞
l=0

let−l.

Ψ

l=1 Alft−l + et = ∑∞
l=1

Ψ

let−l +

To see the assertion in this theorem, we ﬁrst of all deﬁne an intermediate term Γ

f ,p(k) :=

l,p

Ψ

Σe,pΨ(cid:62)

l+k,p, where {Ψ

∑∞
l=0
for |z| ≤ 1, and Σe,p = Cov(et,p, et,p) where et,p = ft − ∑p
l=1 Al,pft−l with {Al,p, l ∈ N} the
ﬁnite predictor coefﬁcients matrices of {Al, l ∈ N}. Hence by triangular inequality, we have

l,p, l ∈ N} are the power series coefﬁcients matrices of

Ir − ∑p

(cid:0)

l=1 Al,pzl

−1

(cid:1)

Γ∗
f (k) − Γ

f (k)

≤

2

Γ∗
f (k) − Γ

f ,p(k)

+

2

Γ

f ,p(k) − Γ

f (k)

2 .

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

It is then sufﬁcient to show both terms on the right side converge to 0 in probability. For

Γ∗
f (k) − Γ

f ,p(k)

, we have

2

(cid:13)
(cid:13)
(cid:13)
Γ∗
f (k) − Γ

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

f ,p(k)

2

(cid:13)
(cid:13)
(cid:13)

(cid:98)

l,p

Ψ

∞
∑
l=0
∞
∑
l=0 (cid:104)(cid:16)
(cid:98)
Σe,p

l,p

=

=

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
+ Ψ
(cid:13)

Σe,p

Ψ(cid:62)

l+k,p −

∞
∑
l=0

Ψ

l,p

Σe,pΨ(cid:62)

l+k,p

(cid:98)
(cid:98)
l,p − Ψ
Ψ

Σe,p

Ψ(cid:62)

l+k,p + Ψ

l,p

l,p

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
Σe,p − Σe,p
(cid:13)

(cid:17)
(cid:98)
(cid:98)
l+k,p − Ψ
l+k,p

Ψ

(cid:62)

(cid:16)

(cid:98)

Ψ(cid:62)

l+k,p

(cid:17)

(cid:98)

= OP

(cid:32)

(cid:16)

(cid:98)
Ψ

∞
∑
l=1 (cid:13)
(cid:13)
(cid:13) (cid:98)

l,p − Ψ

l,p

(cid:13)
(cid:13)
(cid:13)

F(cid:33)

2

(cid:21)(cid:13)
(cid:17)
(cid:13)
(cid:13)
+ OP
(cid:13)

Σe,p − Σe,p

(cid:16)(cid:13)
(cid:13)
(cid:13)(cid:98)

,

(cid:17)

F

(cid:13)
(cid:13)
(cid:13)
Ψ

where the second last line follows from the norm summable conditions on

we can use the results of Lemma C.7 and C.8 to conclude that

(cid:13)
(cid:13)
(cid:13)

18

l,p and Ψ
f ,p(k)

l,p. Hence

→ 0 in

Γ∗
f (k) − Γ
(cid:98)

2

(cid:13)
(cid:13)
(cid:13)

probability. Similarly, we have

l,p

Ψ

∞
∑
l=0
∞
∑
l=0 (cid:104)(cid:0)
Ψ
Σe

l

Ψ

=

=

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
+ Ψ
(cid:13)
(cid:13)

= OP

∞
(cid:0)
∑
l=1

(cid:32)

(cid:13)
(cid:13)

Γ

f ,p(k) − Γ

f (k)

2

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Σe,pΨ(cid:62)

l+k,p −

∞
∑
l=0

Ψ

ΣeΨ(cid:62)
l+k

l

l,p − Ψ

l

Σe,pΨ(cid:62)

l+k,p + Ψ

l

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
Σe,p − Σe
(cid:13)

Ψ(cid:62)

l+k,p

(cid:0)

(cid:1)

Σe,p − Σe

,

F

(cid:1)

(cid:13)
(cid:13)

→ 0 in probability by Lem-

2

(cid:1)
l+k,p − Ψ

l+k

(cid:62)

Ψ

l,p − Ψ

l

2

(cid:105)(cid:13)
(cid:13)
+ OP
(cid:13)

(cid:1)
F(cid:33)

Γ

(cid:0)(cid:13)
(cid:13)
f (k)

(cid:13)
(cid:13)
f ,p(k) − Γ
Γ∗
f (k) − Γ

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

since Ψ

l,p and Ψ

l are norm summable. Hence

(cid:13)
mas C.7 and C.8. Therefore we can conclude that
(cid:13)

f (k)
(cid:13)
(cid:13)

→ 0 in probability.

2

Proof of Proposition 4.4. To see the assertions, we ﬁrst note that,

Γ∗
y(k) − Γy(k)

(cid:13)
(cid:13)
(cid:13)

2

(cid:13)
(cid:13)
(cid:13)

=

QΓ∗

f (k)

QT − QΓ

f (k)QT

Γ∗

f (k)

Q − Q
(cid:98)
(cid:17)
Q − Q

QT

T
(cid:98)

+

2

Γ∗
f (k) − Γ

f (k)

QT

(cid:16)

(cid:17)

(cid:98)

2

(cid:13)
(cid:13)
(cid:13)

2

(cid:13)
(cid:13)
Q
(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤

(cid:13)
(cid:13)
(cid:13) (cid:98)
(cid:13)
(cid:16)
(cid:13)
QΓ
f (k)
(cid:98)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
= OP
(cid:13)

+

(cid:16)
N1/2

(cid:17)
(cid:98)
Q − Q

+ OP

N

Γ∗
f (k) − Γ

f (k)

= oP(1),

2

(cid:13)
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2
(cid:17)

(cid:13)
(cid:13)
(cid:13)
where the last line follows from Assumption 3.1, Lemma C.1 and Theorem 4.3. To see that
p
→ 0 for N → ∞ and T → ∞, we can apply Weyl’s Eigenvalue Theorem (Fan
|δ∗

(cid:13)
(cid:13)
(cid:13) (cid:98)

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

(cid:16)

(cid:16)

(cid:17)

i (k) − δi(k)|
et al. 2013), that is

|δ∗

i (k) − δi(k)| ≤

1
N2

(cid:13)
(cid:13)
(cid:13)

y(k)Γ∗
Γ∗

y(k)(cid:62) − Γy(k)Γy(k)(cid:62)

.

2

(cid:13)
(cid:13)
(cid:13)

Furthermore,

y(k)Γ∗
Γ∗

y(k)(cid:62) − Γy(k)Γy(k)(cid:62)

1
N2

(cid:13)
(cid:13)
(cid:13)

2

(cid:13)
(cid:13)
(cid:13)

=

≤

+

1
N2
1
N2
1
N2

Γ∗
y(k) − Γy(k)

y(k)(cid:62) + Γy(k)
Γ∗

Γ∗
y(k) − Γy(k)

Γ∗
y(k) − Γy(k)

(cid:105)

y(k)(cid:62)
Γ∗
(cid:62)

(cid:104)

.

(cid:105)
Γ∗
y(k) − Γy(k)

(cid:104)

(cid:105)

(cid:104)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:104)
(cid:13)
Γy(k)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

(cid:13)
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:62)

(cid:105)

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)

It is then sufﬁcient to consider one of the two terms on the right side since the other one

can be dealt with similarly. To study 1
N2

Assumption 3.1, Lemma C.1 and Theorem 4.3,

(cid:104)

(cid:13)
(cid:13)
(cid:13)

Γ∗
y(k) − Γy(k)
Γ∗
(cid:105)
=
y(k)

Γ∗
y(k)(cid:62)
QΓ∗

2

(cid:13)
(cid:13)
(cid:13)
19

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13) (cid:98)

2
(cid:13)
f (k)
(cid:13)
(cid:13)

, we ﬁrst notice that from

(cid:16) N. Therefore, we

QT

(cid:98)

2

(cid:13)
(cid:13)
(cid:13)

have

1
N2

(cid:104)

(cid:13)
(cid:13)
(cid:13)

Γ∗
y(k) − Γy(k)

Γ∗
y(k)(cid:62)

(cid:105)

= OP

= OP

2

(cid:13)
(cid:13)
(cid:13)

1
N

(cid:13)
(cid:13)
N−1/2
(cid:13)

(cid:18)

(cid:16)

Γ∗
y(k) − Γy(k)

2

(cid:19)
(cid:13)
(cid:13)
+ OP
(cid:13)
(cid:17)

2

(cid:13)
(cid:13)
(cid:13)

(cid:16)(cid:13)
(cid:13)
(cid:13)

Q − Q

(cid:13)
(cid:13)
(cid:13) (cid:98)

Γ∗
f (k) − Γ

f (k)

,

2

(cid:17)

(cid:13)
(cid:13)
(cid:13)

where both terms on the right side converge to 0 in probability as shown in Lemma C.2 and

Theorem 4.3.

C Auxiliary lemmas and proofs

In this section, we present some auxiliary results that facilitate the proofs of theorems in this

paper. Those auxiliary results are divided into two subsections according to the related topics.

In the ﬁrst subsection, we present some results for factor models’ estimates, and in the second

subsection, the results for AR-sieve bootstrap of factor models are summarised.

C.1 Auxiliary results for estimates of factor models

Lemma C.1. Denoted by (cid:107)V(cid:107)min the positive square root of the minimum eigenvalue of VV(cid:62) or V(cid:62)V,
under Assumption 3.1, we have

and

Γ

f (k)

2

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:16) 1 (cid:16)

Γ

f (k)

min ,

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Γ

f (k) − Γ

f (k)

= OP

T−1/2

.

2

(cid:13)
(cid:13)
(cid:13)
Lemma C.1 is a modiﬁcation of the results in Lemma 1 and 2 of Lam et al. (2011) for the
strong factors’ case, since we have assumed Q(cid:62)Q = NIr but not Q(cid:62)Q = Ir as in Lam et al.

(cid:13)
(cid:13)
(cid:13)(cid:101)

(cid:16)

(cid:17)

(2011). Therefore, the proof of Lemma C.1 is similar to the proofs of Lemma 1 and 2 in Lam et al.

(2011), hence omitted.

Lemma C.2. Under Assumption 3.1,

and

Q − Q

(cid:13)
(cid:13)
(cid:13) (cid:98)

2

(cid:13)
(cid:13)
(cid:13)

= OP

N1/2T−1/2

,

(cid:16)

(cid:17)

N−1/2

Q

ft − Qft

(cid:13)
(cid:13)
(cid:13) (cid:98)

(cid:98)

= OP

T−1/2 + N−1/2

.

(cid:16)

(cid:17)

2

(cid:13)
(cid:13)
(cid:13)

20

Although compared with the model introduced in Lam et al. (2011), we scale the columns in

√

Q by

N in our factor models’ setting, the above convergence rate is the same as that of strong

factors’ case in Theorem 3 of Lam et al. (2011). Besides, the proof of Lemma C.2 is the case for

strong factors in the proof of Theorem 3 in Lam et al. (2011) with the only difference on scaled

factor loading matrix Q and factors f . Therefore, the proof is omitted here.

Lemma C.3. Deﬁne

Γ

f (k) = 1
T−k

∑T−k
t=1

ft

ft+k and

Γ

f (k) = 1
T−k

∑T−k

t=1 ftft+k, for some k ≤ p, where

p fulﬁls Assumption 4.3. It then holds that
(cid:98)
(cid:98)

(cid:98)

(cid:101)

Γ

f (k) −

Γ

f (k)

(cid:13)
(cid:13)
(cid:13)(cid:98)

(cid:101)

(cid:13)
(cid:13)
(cid:13)

2

= OP

N−1/2 + T−1/2

.

(cid:16)

(cid:17)

Lemma C.3 illustrates the convergence rate on autocovariance matrices of estimated factors

under strong factors’ case, which is an extension to the convergence rate of estimated factors

obtained in Theorem 3 in Lam et al. (2011).

Proof of Lemma C.3. First of all, we notice that

Γ

f (k) −

Γ

f (k) =

1
T − k

(cid:98)

(cid:101)

=

1
T − k

T−k
∑
t=1 (cid:16)
T−k
∑
t=1 (cid:104)(cid:16)

ft

ft+k − ftft+k

(cid:98)
(cid:98)
ft − ft

(cid:17)

ft+k + ft

ft+k − ft+k

.

(cid:98)

(cid:17)

(cid:98)

(cid:16)

(cid:98)

(cid:17)(cid:105)

Hence,

Γ

f (k) −

Γ

f (k)

(cid:13)
(cid:13)
(cid:13)(cid:98)

(cid:101)

≤

≤

2

(cid:13)
(cid:13)
(cid:13)

1
T − k

(cid:13)
(cid:13)
(cid:13)
1
(cid:13)
(cid:13)
T − k

T−k
∑
t=1 (cid:16)
T−k
∑
t=1 (cid:13)
(cid:13)
(cid:13)

(cid:16)

ft − ft

ft+k

(cid:98)
ft − ft

(cid:98)

(cid:17)

(cid:17)

(cid:98)
ft+k

(cid:98)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

+

+

2

2

1
T − k

(cid:13)
(cid:13)
(cid:13)
1
(cid:13)
(cid:13)
T − k

T−k
∑
t=1

T−k
∑
t=1 (cid:13)
(cid:13)
(cid:13)

ft

ft+k − ft+k

ft

(cid:16)

(cid:16)

(cid:98)
ft+k − ft+k

(cid:98)

2

2

.

(cid:17)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:17)(cid:13)
(cid:13)
(cid:13)

And it is sufﬁcient to consider only one of the two terms on the right-hand side above since the

other one can be dealt with in precisely the same way. For the ﬁrst term on the right-hand side

above, notice that under the factor model deﬁned in (3), we have

ft − ft =

(cid:98)

=

=

=

1
N
1
N
1
N
1
N

Q(cid:62)yt − ft
(cid:62)

Q − Q
(cid:98)

(cid:16)

(cid:16)

(cid:16)

Q − Q
(cid:98)

Q − Q
(cid:98)

(cid:98)

(cid:17)

(cid:62)

(cid:17)

(cid:62)

(cid:17)

yt +

yt +

yt +

1
N
1
N
1
N

Q(cid:62)yt − ft

Q(cid:62)yt −

1
N

Q(cid:62)Qft

Q(cid:62)ut.

21

Hence

ft − ft

≤

2

1
N

Q − Q

(cid:62)

yt

1
N

Q(cid:62)ut

(cid:17)

+

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

,

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13) (cid:98)

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:16)
(cid:13)
(cid:13)
(cid:98)
(cid:13)
N Q(cid:62)ut
1
1√
1√
1√
(cid:13)
(cid:13)
qr
q2, ...,
(cid:13)
(cid:13)
N
N
N
L. Observe that E

q1,

q(cid:62)
2 , ﬁrst consider the random variables 1√
i ut for
, where 1√
qi for i = 1, 2, ..., r are unscaled eigen-
N
Σuqi ≤
= 0 and V
q(cid:62)
i ut
= 1 and λmax (Σu) is the largest eigenvalue of Σu. Consequently,

q(cid:62)
i ut

N q(cid:62)

(cid:17)
1√
N

= 1

1√
N

(cid:16)

(cid:16)

(cid:17)

(cid:17)

N

i

by triangular inequality. To study

each 1√
N

qi in 1√
N

Q =

(cid:16)
vectors estimated from
λmax (Σu) < ∞, since

(cid:13)
(cid:13)
q(cid:62)
i ut = OP (1) and
(cid:13)

1√
N

=
of Σu are assumed to be bounded when N → ∞ under Assumption 3.1.

= OP

(cid:114)

i=1

(cid:17)

(cid:16)

(cid:0)

2

1√
N

q(cid:62)
i ut

∑r

1
N

N−1/2

2

Recall that

Q − Q

= OP

by Lemma C.2, we then have

, as the eigenvalues

(cid:1)

1
N

Q − Q

(cid:16)

(cid:98)

(cid:17)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:62)

yt

≤

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

qi

1√
(cid:98)
N
(cid:13)
(cid:13)
N Q(cid:62)ut
(cid:13)

1

(cid:13)
(cid:13)

2

(cid:13)
(cid:13)
N1/2T−1/2

(cid:0)
T−1/2

(cid:1)

, and

(cid:0)

(cid:1)

(cid:13)
(cid:13)
(cid:107)yt(cid:107)2 = OP
(cid:13)

1
N

Q − Q

(cid:16)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:98)

(cid:17)

(cid:13)
(cid:62)
(cid:13)
(cid:13) (cid:98)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

ft − ft

(cid:13)
(cid:13)
(cid:13) (cid:98)

2

(cid:13)
(cid:13)
(cid:13)

1
N

≤

(cid:13)
(cid:13)
(cid:13)
= OP
(cid:13)

Q − Q

(cid:62)

yt

+

2
(cid:13)
(cid:17)
(cid:16)
(cid:13)
N−1/2 + T−1/2
(cid:13)
(cid:98)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

,

1
N

Q(cid:62)ut

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)

uniformly for t. Finally, we can conclude that

(cid:16)

(cid:17)

Γ

f (k) −

Γ

f (k)

(cid:13)
(cid:13)
(cid:13)(cid:98)

(cid:101)

2

(cid:13)
(cid:13)
(cid:13)

≤

1
T − k

= OP

(cid:16)

ft − ft

ft+k

T−k
∑
t=1 (cid:13)
(cid:13)
(cid:13)

(cid:16)
N−1/2 + T−1/2

(cid:98)

+

1
T − k

T−k
∑
t=1 (cid:13)
(cid:13)
(cid:13)

2

(cid:13)
(cid:13)
(cid:13)

ft

ft+k − ft+k

(cid:16)

(cid:98)

2

(cid:17)(cid:13)
(cid:13)
(cid:13)

(cid:98)

.

(cid:17)

(cid:17)

C.2 Auxiliary results for AR-sieve bootstrap of factor models

Lemma C.4. Let

Ap =

A1,p,

A2,p, ...,

Ap,p

be the matrix of the Yule-Walker estimators of the

(cid:16)
ﬁnite predictor coefﬁcients on true factors {ft}, and

(cid:17)

(cid:101)

(cid:101)

(cid:101)

(cid:101)

Ap =

A1,p,

A2,p, ...,

Ap,p

be the matrix of the

Yule-Walker estimators of the ﬁnite predictor coefﬁcients on estimated factors {
(cid:98)

(cid:98)

(cid:98)

(cid:98)

(cid:16)

(cid:17)

ft}, then

(cid:98)

Ap −

Ap

(cid:13)
(cid:13)
(cid:13) (cid:98)

(cid:13)
(cid:13)
(cid:13)

(cid:101)

= OP

F

p4

N−1/2 + T−1/2

.

(cid:16)

(cid:16)

(cid:17)(cid:17)

Proof of Lemma C.4. Recall that the Yule-Walker estimators are solved from the Yule-Walker

22

equations on the ﬁnite predictors’ coefﬁcients matrices as follows,

Ap =

A1,p, A2,p, ..., Ap,p

= Π

Π−1
0,p,

1

(cid:0)

(cid:1)

where Π

1 = (Γf (1), Γf (2), ..., Γf (p)) is an r × (rp) block matrix of autocovariance matrices and

Γf (0)
Γf (−1)
...

Γf (1)
Γf (0)
...

Γf (−p + 1) Γf (−p + 2)

· · ·

· · · Γf (p − 1)
· · · Γf (p − 2)
. . .

...
Γf (0)

,











Π0,p =











is then an (rp) × (rp) block matrix of autocovariance matrices (Brockwell & Davis 1991). Write
1 and Π0,p but
Π

Π0,p the same matrices as Π

A2,p, ...,
Γ

0,p with

Ap =

1 and

A1,p,

Π−1

Π−1

Ap,p

=

Π

Π

Π

(cid:16)

A2,p, ...,

Ap,p

=

1

0,p with

1 and

1
(cid:17)
f rather than Γ
(cid:98)
f rather than Γ

deﬁned based on
(cid:98)
(cid:98)
(cid:98)
Γ
Π0,p deﬁned based on

Ap =
A1,p,
f . Similarly,
(cid:98)
(cid:98)
(cid:98)
(cid:16)
Γ
Γ
f and
f . Recall that
(cid:101)
(cid:101)
(cid:101)
matrices deﬁned in Lemma C.3, then we have
(cid:101)
(cid:101)

(cid:98)

(cid:98)

(cid:98)

(cid:101)

f are sample lag-k autocovariance
(cid:101)

(cid:101)

(cid:101)

(cid:101)

(cid:17)

≤

Π−1

0,p −

Π−1
0,p

Ap −

Ap

F

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13) (cid:98)
(cid:101)
(cid:13) (cid:98)
, we ﬁrst compute

(cid:13)
(cid:13)
(cid:13)

Π

1

+

F

Π−1
0,p

F

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13) (cid:98)

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13) (cid:101)

F

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13) (cid:98)

(cid:13)
(cid:13)
(cid:13)

(cid:101)

Π

1 −

Π

1

.

F

(12)

. Recall the recursive derivation based on the parti-

To ﬁnd
F
tioned inverse formula for Π−1

Π−1
0,p

(cid:13)
(cid:13)
(cid:13) (cid:101)

(cid:13)
(cid:13)
(cid:13)

F
(cid:13)
0,p+1 as in Sowell (1989),
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

(cid:101)

Π−1
0,p

Π−1

0,p+1 =

=









Π−1

0,p + JpApv−1
p A
(cid:62)
−v−1
p Jp

p A

(cid:62)
p Jp −JpApv−1
p

v−1
p



+

Π−1

0,p 0

0

0




0



0 −JpApv−1/2

p
v−1/2
p



0
−v−1/2

p A









0

(cid:62)
p Jp v−1/2

p





,

(13)

where Jp = Jp ⊗ Ir with Jp the p × p matrix with ones on the anti-diagonal and Ir the r × r iden-
tity matrix, v = E
l=1 Al,pft+l
the coefﬁcient matrices minimizing the forward prediction variance E

A
ft − ∑p
(cid:16)

(cid:62)
p,p
ft − ∑p
(cid:17)

l=1 Al,pft+l

(cid:62)
2,p, ..., A

and Ap =

ft − ∑p

ft − ∑p

(cid:62)
1,p, A

(cid:1) (cid:0)

(cid:1)

(cid:0)

(cid:62)

l=1 Fl,pft+l

l=1 Fl,pft+l

Denoted by Sp the second term on the right-hand side of (13), we can then get the recursive
expression of Π−1

(cid:1) (cid:0)

(cid:0)

0,p as

Π−1

0,p =

Γf (0)−1 0

0





0


+

p−1
∑
l=1

Sl.

23

(cid:62)

.

(cid:1)

For Sl, note that

(cid:107)Sl(cid:107)F ≤

v−1/2
l

(cid:13)
(cid:13)
(cid:13)

≤

v−1/2
l

(cid:13)
(cid:13)
(cid:13)
= O (1) ,

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

2

F

2

(cid:0)

F (cid:32)

1 +

JlAl

2

F

(cid:13)
l
(cid:13)
∑
1 +
j=1

2

F(cid:33)

(cid:1)

(cid:13)
(cid:13)
Aj,l

(cid:13)
(cid:13)

(cid:13)
(cid:13)

l=1 Sl

≤ ∑p−1

uniformly for l = 1, 2, ..., p, where we use the deﬁnition of vl and Lemma C.5. Hence
∑p−1
min = O (1),
(cid:13)
where λi is the ith eigenvalue of Γf (0), λmin is the smallest eigenvalue of Γf (0) and we use
(cid:13)
(cid:13)
(cid:13)
(cid:13)
Assumption 4.1 that Γf (0) is full rank. Thus, we have shown

l=1 (cid:107)Sl(cid:107)F = O (p). Besides,

Γf (0)−1

i=1 λ−2

= O (p) .

i ≤

rλ−1

(cid:13)
(cid:13)
(cid:13)

∑r

(cid:113)

√

(cid:13)
(cid:13)

=

F

F

To ﬁnd (cid:107)

Π−1

0,p −

Π−1

0,p(cid:107)F, note that for invertible matrices

Π−1
0,p
(cid:13)
Π0,p and
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

F
Π0,p,

Π−1

0,p −

(cid:98)
Π−1
0,p

(cid:13)
(cid:13)
(cid:13) (cid:98)

(cid:101)

F

(cid:13)
(cid:13)
(cid:13)

(cid:101)
=

=

≤

Π0,p −

Π0,p)

Π−1
0,p

0,p −
(cid:101)

Π−1

0,p)(
(cid:98)

Π0,p −
(cid:101)

(cid:98)

(cid:101)

Π−1

0,p +

Π−1
0,p(

Π0,p −

Π0,p)

Π−1
0,p

F
(cid:13)
Π0,p)
(cid:13)
(cid:13)

Π−1
(cid:101)
0,p

F

Π0,p
Π0,p −
(cid:98)
(cid:101)

(cid:101)

Π−1
(cid:101)
0,p

+
(cid:101)

Π0,p −
(cid:98)

Π0,p
(cid:101)

(cid:13)
(cid:13)
(cid:13) (cid:101)
And for large enough N and T such as

(cid:13)
(cid:13)
(cid:13)

(cid:101)

(cid:98)
f (k) −

Γ

F

(cid:13)
(cid:13)
(cid:13)
Γ

(cid:13)
(cid:13)
(cid:13) (cid:101)
f (k)

F

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13) (cid:101)
(cid:13)
→ 0 and

2

(cid:13)
(cid:13)
(cid:98)
(cid:13)
Π0,p −

Π−1
0,p(
Π−1

(cid:13)
(cid:13)
(
(cid:13) (cid:98)
(cid:13)
Π−1
(cid:13)
0,p −
(cid:98)
(cid:13)
(cid:13)
(cid:13)
(cid:13) (cid:98)

probability, we can write

(cid:13)
(cid:13)
(cid:13)(cid:98)

(cid:101)

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13) (cid:101)

(cid:13)
(cid:13)
(cid:13)

(cid:98)

F
Π−1
0,p

F

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13) (cid:101)
Π0,p

F

2

F

.

(cid:13)
(cid:13)
(cid:13)
→ 0 in

2

Π−1
0,p
F
(cid:13)
Π−1
(cid:13)
0,p
(cid:13)

Π0,p −

Π0,p

F
(cid:13)
Π0,p
(cid:13)
(cid:13)

Π−1

0,p −

Π−1
0,p

≤

F

F

(cid:101)

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13) (cid:98)

Π0,p −
(cid:98)

(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
Π0,p
(cid:13)
(cid:13)
(cid:13)
(cid:13)
Π0,p
Π0,p −
(cid:98)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
Π0,p −
where the last line follows since when N, T → ∞, (cid:107)

(cid:13)
(cid:13)
1 −
(cid:13)
(cid:13)
(cid:13) (cid:101)
(cid:13) (cid:101)
F
(cid:13)
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
Π−1
Π0,p −
(cid:13) (cid:101)
(cid:13) (cid:101)
(cid:13)
0,p
F
(cid:13)
(cid:13)
Π−1
Π0,p −
(cid:13)
(cid:13)
(cid:13) (cid:101)
(cid:98)
0,p
(cid:13)
F
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
Π−1
(cid:13) (cid:101)
(cid:13)
0,p
F
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13) (cid:101)
(cid:13)

(cid:13)
(cid:13)
(cid:13) (cid:101)
(cid:18)(cid:13)
(cid:13)
(cid:13)

(cid:13)
1 −
(cid:13)
(cid:13)

= OP

Π0,p
(cid:98)

(cid:13)
(cid:13)
(cid:13) (cid:101)

≤

+

(cid:19)

(cid:98)

F

F

,

2

F

Π−1

0,p − Π−1
0,p
Π−1
0,p

1 −

(cid:13)
(cid:13)
(cid:13)
F

(cid:13)
Π0,p −
(cid:13)
(cid:13) (cid:101)

Π0,p −

Π0,p

(cid:13)
(cid:13)
(cid:13) (cid:101)

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13) (cid:101)

F

Π0,p
(cid:98)

(cid:98)

(cid:13)
(cid:13)
(cid:13)
F

(cid:13)
(cid:13)
(cid:13)

Π0,p(cid:107)F → 0 in probability and the

ﬁrst term in the second last line is the leading term. In addition, we have

(cid:101)

(cid:98)

Π0,p −

Π0,p

(cid:13)
(cid:13)
(cid:13) (cid:101)

(cid:98)

F

(cid:13)
(cid:13)
(cid:13)

Γ

≤

p
∑
l=1

p
∑
j=1 (cid:13)
(cid:13)
(cid:13)(cid:98)
≤ p2 max
|k|≤p−1

= OP

p5/2

(cid:16)

24

f (l − j) −

Γ

f (l − j)

Γ

f (k) −

(cid:101)
Γ

f (k)

F

(cid:13)
(cid:13)
(cid:13)
(cid:13)
N−1/2 + T−1/2
(cid:13)(cid:98)
(cid:13)
(cid:16)

(cid:101)

(cid:17)(cid:17)

F

(cid:13)
(cid:13)
(cid:13)

,

(14)

where for r × r matrices

Γ

f (k) and

Γ

f (k),

Γ

f (k) −

Γ

f (k)

(cid:16)

Γ

f (k) −

Γ

f (k)

as shown in Lemma C.3. Therefore, with (14) we can conclude that
(cid:101)

(cid:98)

(cid:101)

(cid:13)
(cid:13)
(cid:13)(cid:98)

F

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)(cid:98)

Π−1

0,p −

Π−1
0,p

(cid:13)
(cid:13)
(cid:13) (cid:98)

(cid:101)

F

(cid:13)
(cid:13)
(cid:13)

= OP

Π−1
0,p

= OP

(cid:18)(cid:13)
(cid:13)
p4
(cid:13)

(cid:16)

(cid:16)

2

F

Π0,p −

Π0,p

(cid:13)
(cid:13)
N−1/2 + T−1/2
(cid:13) (cid:101)
(cid:98)

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
.
(cid:13)
(cid:17)(cid:17)

(cid:101)

F

(cid:19)

Lastly,

Π

1

(cid:13)
(cid:13)
(cid:13) (cid:98)

(cid:13)
(cid:13)
(cid:13)

≤

F

≤

Γ

p
∑
k=1 (cid:13)
(cid:13)
p
(cid:13)(cid:98)
∑
Γ
k=1

f (k)

f (k)

(cid:13)
(cid:13)
(cid:13)

F

F

(cid:13)
(cid:13)
(cid:13)
(cid:13)
= O (1) + OP

Γ

f (k) − Γ

p
∑
k=1 (cid:13)
(cid:13)
(cid:13)(cid:98)
N−1/2 + T−1/2

f (k)

+

p

F

(cid:13)
(cid:13)
(cid:13)
,

= OP

N−1/2 + T−1/2

(cid:0)

(cid:1)

2

(cid:13)
(cid:13)
(cid:13)

(15)

(16)

where the ﬁrst term follows from the summability condition in Assumption 4.1. Moreover,

(cid:16)

(cid:16)

(cid:17)(cid:17)

Π

1 −

Π

1

(cid:13)
(cid:13)
(cid:13) (cid:98)

(cid:101)

F

(cid:13)
(cid:13)
(cid:13)

≤

Γ

p
∑
k=1 (cid:13)
(cid:13)
(cid:13)(cid:98)
p

= OP

f (k) −

Γ

f (k)

F

(cid:13)
(cid:13)
(cid:13)

N−1/2 + T−1/2

(cid:101)

(cid:16)

(cid:16)

.

(cid:17)(cid:17)

Hence we can conclude that the ﬁrst term in (12) is the leading term, and

Ap −

Ap

(cid:13)
(cid:13)
(cid:13) (cid:98)

(cid:101)

F

(cid:13)
(cid:13)
(cid:13)

= OP

p4

N−1/2 + T−1/2

,

(cid:16)

(cid:16)

(cid:17)(cid:17)

by (15) and (16).

Lemma C.5. Let {ft} be factor processes fulﬁlling Assumptions 3.1 and 4.1 for some γ ≥ 0. Write

Al,p, l = 1, 2, ..., p
coefﬁcients {Al, l ∈ N} and the MA coefﬁcients {Ψ
(cid:8)
(cid:8)

l,p, l = 1, 2, ..., p

and

(cid:9)

(cid:9)

Ψ

as the ﬁnite predictor coefﬁcients matrices of the AR

l, l ∈ N} as in (3) and (2), respectively.

(i) Norm summability: the coefﬁcients matrices Al and Ψ

l fulﬁl the following summability properties:

∑∞
l=1(1 + l)γ (cid:107)Al(cid:107)F < ∞ and ∑∞

l=1(1 + l)γ (cid:107)Ψ

l(cid:107)F < ∞.

(ii) (Lemma 3.1 of Meyer & Kreiss (2015)) For some γ ≥ 0 as in Assumption 4.1, there exist p0 ∈ N

and d < ∞ such that

p
∑
l=1

(1 + l)γ

Al,p − Al

≤ d

F

(cid:13)
(cid:13)

(cid:13)
(cid:13)

∞
∑
l=p+1

25

(1 + l)γ (cid:107)Al(cid:107)F , for p ≥ p0,

and the right side converges to 0 when p → ∞.

(iii) (Lemma 3.2 of Meyer & Kreiss (2015)) Let Ap(z) := Ir − ∑p

l=1 Al,pzl, then there exist p1 ∈ N

and c < ∞ such that

inf
|z|≤1+1/p

det

Ap(z)

≥ c, for p ≥ p1.

(cid:12)
(cid:12)

(cid:0)

(cid:1)(cid:12)
(cid:12)

(iv) (Lemma 3.3 of Meyer & Kreiss (2015)) Let {Ψ

l,p, l ∈ N} be the power series coefﬁcients matrices
, for |z| ≤ 1. For p1 as deﬁned in (iii) and some γ ≥ 0 in Assumption 4.1,

Ir − ∑p

l=1 Al,pzl

of
there exist p2 ≥ p1 and d < ∞ such that
(cid:1)

(cid:0)

−1

∞
∑
l=1

(1 + l)γ

Ψ

l,p − Ψ

l

≤ d

F

(cid:13)
(cid:13)

(cid:13)
(cid:13)

∞
∑
l=p+1

and the right side converges to 0 when p → ∞.

(1 + l)γ (cid:107)Al(cid:107)F , for p ≥ p2,

Lemma C.5 (ii) is the vector form of Baxter’s inequality on the AR coefﬁcients matrices {Al}

and its ﬁnite predictor coefﬁcients matrices {Al,p}, whereas Lemma C.5 (iv) relates Baxter’s
inequality of AR coefﬁcients to the MA coefﬁcients matrices {Ψ
coefﬁcients matrices {Ψ

l,p}. The proofs of Lemma C.5 can be found in Meyer & Kreiss (2015),

l} and its ﬁnite predictor

hence it is omitted here.

Lemma C.6. (Lemma 3.5 of Meyer & Kreiss (2015)) Let {ft} be factor processes deﬁned under the
assumptions of Lemma C.5 and also fulﬁl Assumption 4.2. Deﬁne Ψ

l,p as the coefﬁcients matrices in

the power series of

Ir − ∑p

l=1 Al,pzl

−1

Ir − ∑p
coefﬁcients matrices of
such that it holds uniformly in l ∈ N and for all p ≥ p3,

Al,pzl
(cid:1)

l=1

(cid:16)

(cid:17)

(cid:0)

, for |z| ≤ 1 with

(cid:101)

(cid:101)

, for |z| ≤ 1 with Ψ0,q := Ir and
−1

l,p as the power series
Ψ0,q := Ir. Then, there exists p3 ∈ N

Ψ

(cid:101)

≤

1 +

(cid:18)

F

1
p

(cid:19)

−l 1

p2 OP (1) .

Ψ

l,p − Ψ

l,p

(cid:13)
(cid:13)
(cid:13) (cid:101)

(cid:13)
(cid:13)
(cid:13)

The proof of Lemma C.6 can be found in Meyer & Kreiss (2015).

Lemma C.7. Let {ft} be factor processes fulﬁlling Assumptions 3.1, 4.1 (γ = 1), 4.2 and 4.3. Deﬁne
, for |z| ≤ 1 with Ψ0,q :=
{Ψ
Ir − ∑p
Al,pzl

l=1 Al,pzl
l,p} as the power series coefﬁcients matrices of
(cid:1)

l,p} as the coefﬁcients matrices in the power series of

Ir. Similarly, deﬁne {

Ir − ∑p

−1

−1

Ψ

(cid:0)

|z| ≤ 1 with

Ψ0,q := Ir, and {

(cid:101)

Ψ

l,p} as the power series coefﬁcients matrices of

(cid:16)

l=1
Ir − ∑p
(cid:101)
l=1

(cid:17)
Al,pzl

, for
−1

,

(cid:101)

(cid:98)

(cid:16)

(cid:17)

(cid:98)

26

for |z| ≤ 1 with

Ψ0,q := Ir. Then, there exists p3 ∈ N such that for all p ≥ p3 as in Lemma C.6,

(cid:98)

Ψ

∞
∑
l=1 (cid:13)
∞
(cid:13)
(cid:13) (cid:101)
∑
Ψ
l=1
∞
∑
l=1 (cid:13)
∞
(cid:13)
(cid:13) (cid:98)
∑
Ψ
l=1 (cid:13)
(cid:13)
(cid:13) (cid:98)
when N → ∞ and T → ∞.

(cid:13)
(cid:13)

Ψ

l,p − Ψ

l,p

l,p − Ψ

l

(cid:13)
(cid:13)
l,p

l,p −

Ψ

(cid:101)
l,p − Ψ

l,p

= oP(1),

= OP

F

1
p

(cid:19)

(cid:18)

= o (1) ,

(cid:13)
(cid:13)
(cid:13)
F

= OP

p3/2

Ap −

Ap

(cid:16)
= oP(1),

(cid:13)
(cid:13)
(cid:13) (cid:98)

(cid:13)
(cid:13)
(cid:13)

(cid:101)

F

(cid:17)

= oP(1),

F

F

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

Proof of Lemma C.7. For large enough N, T and p > p3 as in Lemma C.6, ∑∞
l=1

Ψ

l,p − Ψ

l,p

follows directly from Lemma C.6 as

l,p − Ψ

l,p

Ψ

∞
∑
l=1 (cid:13)
(cid:13)
(cid:13) (cid:101)

≤

≤

F

(cid:13)
(cid:13)
(cid:13)

1
p2

1
p2

= OP

(cid:13)
(cid:13)
(cid:13) (cid:101)

−l

OP (1)

1 +

1
p

(cid:19)
(1 + p)OP (1)

∞
∑
l=1 (cid:18)
p
1 + p
1
p

(cid:18)

(cid:19)

.

The order of ∑∞
l=1

Ψ

l,p − Ψ

l

F follows directly from Lemma C.5 (i) and (iv), as

F

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)

∞
∑
l=1

∞
∑
l=1

Ψ

l,p − Ψ

l

≤

F

(1 + l)γ

Ψ

l,p − Ψ

l

(cid:13)
(cid:13)

(cid:13)
(cid:13)

≤ d

∞
∑
l=p+1

(cid:13)
(cid:13)

(1 + l)γ (cid:107)Al(cid:107)F

F

(cid:13)
(cid:13)

= o (1) .

= oP (1) , ﬁrst notice that

(cid:13)
(cid:13)

To show ∑∞
l=1

Ψ

l,p −

Ψ

l,p

(cid:13)
(cid:13)
(cid:13) (cid:98)

(cid:101)

where

Ψ(u,v)
l,p

and

Ψ(u,v)
l,p

F

Ψ

(cid:13)
(cid:13)
(cid:13)
∞
∑
l=1 (cid:13)
(cid:13)
(cid:13) (cid:98)

l,p −

Ψ

l,p

(cid:101)

(cid:13)
(cid:13)
(cid:13)

≤

F

∞
∑
l=1

r
∑
u=1

Ψ(u,v)

l,p −

r
∑
v=1 (cid:12)
(cid:12)
(cid:12) (cid:98)

are the (u, v)th elements of the matrices

Ψ(u,v)
l,p

,

(cid:12)
(cid:12)
(cid:12)
l,p and

(cid:101)
Ψ

Ψ

l,p, respectively. We

then apply Cauchy’s inequality for holomorphic functions on the (u, v)th element of

(cid:98)

(cid:101)

(cid:98)

(cid:101)

Ψ

l,p and

(cid:101)

27

Ψ

l,p, that is

Ψ(u,v)

l,p −

Ψ(u,v)
l,p

(cid:12)
(cid:12)
(cid:12) (cid:98)

(cid:101)

(cid:12)
(cid:12)
(cid:12)

≤

1 +

(cid:18)

≤

1 +

(cid:18)

1
p

1
p

−l

(cid:19)

−l

max
|z|=1+ 1

p (cid:13)
(cid:13)
(cid:13) (cid:98)

max
|z|=1+ 1
p

| det(

(cid:19)

(cid:34)

A−1

p (z) −

A−1

p (z)

(cid:101)

1
Ap(z))|

(cid:13)
(cid:13)
(cid:13) (cid:98)

F

(cid:13)
(cid:13)
(cid:13)
Aadj
p (z) −

Aadj

p (z)

F

(cid:13)
(cid:13)
(cid:13)

+ max
|z|=1+ 1

1 +

=:

(cid:18)

1
Ap(z))

−

det(

−l

(cid:98)
max
|z|=1+ 1
(cid:34)
p

(cid:19)

1
(cid:98)
Ap(z)) (cid:12)
det(
(cid:13)
(cid:12)
(cid:13)
(cid:12)
(cid:13) (cid:101)
(cid:12)
(cid:12)
K2,z

(cid:101)
K1,z + max
|z|=1+ 1
p

p (cid:12)
(cid:12)
(cid:12)
(cid:12)
1
(cid:12)
p

,

(cid:35)

Aadj

p (z)

(cid:101)

F(cid:35)

(cid:13)
(cid:13)
(cid:13)

where we use Aadj to denote the adjugate matrix of A, and write the two terms above as K1,z

and K2,z.

To study K1,z, with Assumption 4.2, Lemmas C.2 and C.4, we show that with sufﬁciently

large N and T, we can choose p > p3 such that

Ap −

Ap

= oP(1) and sup|z|≤1+ 1

p

Ap(z) −

Ap(z)

F

(cid:13)
oP(1). Furthermore, since determinants are continuous functions of the elements, it can be
(cid:13)
(cid:13) (cid:98)
(cid:101)
Ap(z)

→ 0 in probability, with

Ap(z) − det

extended to sup|z|≤1+ 1

(cid:13)
(cid:13)
(cid:13) (cid:98)

det

(cid:13)
(cid:13)
(cid:13)

(cid:101)

p

=

F

(cid:13)
(cid:13)
(cid:13)

(cid:12)
(cid:12)
(cid:12)

(cid:98)
≥ c and

det

(cid:12)
(cid:12)
(cid:101)
(cid:12)
Ap(z)

det

Ap(z)

≥ c in probability, for |z| ≤ 1 +

1
p

,

(cid:17)(cid:12)
(cid:12)
(cid:12)
and for some c > 0 as in Lemma C.5. Then, for p > p3 and any |z| = 1 + 1/p we can show that

(cid:17)(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:16)

(cid:16)

(cid:101)

(cid:98)

Aadj

p (z) −

Aadj

p (z)

F
(cid:13)
(cid:13)
p (z)(u,v) −
(cid:13)

(cid:101)

Aadj

p (z)(u,v)

K1,z ≤

≤

≤

≤

1
c
1
c

1
c

1
c

(cid:13)
r
∑
(cid:13)
(cid:13) (cid:98)
u=1
r
∑
u=1

Aadj

r
∑
v=1 (cid:12)
(cid:12)
r
(cid:12) (cid:98)
∑
sup
|z|≤1+ 1
v=1

r
∑
u=1

r
∑
v=1

sup
|z|≤1+ 1
p

p (cid:12)
(cid:12)
(cid:12)
r

≤ sup
|z|≤1+ 1

Ap(z) −

p (cid:13)
(cid:13)
(cid:13) (cid:98)

det

A

(cid:101)
(−v,−u)
p

(cid:12)
(cid:12)
(cid:12)

(z) − det

A

(−v,−u)
p

(z)

(cid:12)
(cid:12)
(cid:12)

(cid:101)
OP (1)

F

(cid:13)
(cid:13)
(cid:13)

(cid:98)
Ap(z) −

Ap(z)

(cid:101)

(cid:13)
(cid:13)
(cid:13) (cid:98)
Ap(z)

(cid:101)

,

F

(cid:13)
(cid:13)
(cid:13)

where

A

(−v,−u)
p

(z) is a matrix generated by removing the vth row and the uth column of

Ap(z).

(cid:101)

(cid:101)

28

And for sup|z|≤1+ 1

p

Ap(z) −

Ap(z)

, we have

(cid:13)
(cid:13)
(cid:13) (cid:98)

(cid:101)
Ap(z) −

sup
|z|≤1+ 1

p (cid:13)
(cid:13)
(cid:13) (cid:98)

F

(cid:13)
(cid:13)
(cid:13)
Ap(z)

(cid:101)

F

(cid:13)
(cid:13)
(cid:13)

≤ sup
|z|≤1+ 1
p

≤

1 +

(cid:18)
= OP

Al,p −

p
∑
l=1 (cid:13)
(cid:13)
(cid:13) (cid:98)
p p
1
∑
p
l=1 (cid:13)
√
(cid:13)
(cid:13) (cid:98)
Ap −
Ap

(cid:19)

p

(cid:101)
Al,p −

Al,p

|Z|l

F

(cid:13)
(cid:13)
(cid:13)
Al,p

F

(cid:13)
(cid:13)
(cid:13)

(cid:101)
.

F

(cid:17)

Hence we can conclude that for K1,z,

max
|z|=1+ 1
p

K1,z = OP

since the bound does not depend on z.

(cid:16)

(cid:13)
(cid:13)
(cid:13) (cid:98)

(cid:13)
(cid:13)
(cid:13)

(cid:101)

√

(cid:16)

p

Ap −

Ap

(cid:13)
(cid:13)
(cid:13) (cid:98)

(cid:13)
(cid:13)
(cid:13)

(cid:101)

,

F

(cid:17)

For K2,z, note that max|z|=1+ 1
Ap(z)

p

C.5, therefore, max|z|=1+ 1
c,

p

(cid:13)
(cid:13)
F

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13) (cid:101)

Ap(z)

F

≤ (1 + 1/p)p ∑p

l=1

Al,p

F

= OP (1) by Lemma

(cid:13)
= OP (1) by Assumption 4.2. Similarly, for some constants
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

max
|z|=1+ 1
p

K2,z ≤

1
c2 max
|z|=1+ 1
√

= OP

p (cid:12)
(cid:12)
(cid:98)
(cid:12)
Ap −
Ap

p

det

Ap(z) − det

Ap(z)

Aadj

p (z)

(cid:101)

(cid:12)
(cid:12)
(cid:12)

(cid:13)
(cid:13)
(cid:13) (cid:101)

.

F

(cid:17)

F

(cid:13)
(cid:13)
(cid:13)

As a result,

(cid:16)

(cid:13)
(cid:13)
(cid:13) (cid:98)

(cid:13)
(cid:13)
(cid:13)

(cid:101)

(cid:101)

≤

Then, we can conclude that

Ψ

∞
∑
l=1 (cid:13)
(cid:13)
(cid:13) (cid:98)

l,p − Ψ

l,p

F

(cid:13)
(cid:13)
(cid:13)

l,p −

Ψ

l,p

Ψ

∞
∑
l=1 (cid:13)
(cid:13)
(cid:13) (cid:98)

F

(cid:13)
(cid:13)
(cid:13)

≤

∞
∑
l=1

r
∑
u=1

r
∑
v=1

|

Ψ(u,v)

l,p −

Ψ(u,v)
l,p

|

= OP

p3/2

(cid:98)
Ap −

Ap

(cid:16)

(cid:13)
(cid:13)
(cid:13) (cid:98)

(cid:101)

.

F

(cid:17)

(cid:101)

(cid:13)
(cid:13)
(cid:13)

Ψ

∞
∑
l=1 (cid:13)
(cid:13)
1
(cid:13) (cid:101)
p
(cid:18)

l,p − Ψ

l,p

+ OP

(cid:19)

(cid:13)
(cid:13)
(cid:13)

(cid:16)

= OP

Ψ

∞
∑
l=1 (cid:13)
(cid:13)
(cid:13) (cid:98)
Ap −

+

F

p3/2

(cid:13)
(cid:13)
(cid:13) (cid:98)

F

(cid:13)
(cid:13)
(cid:13)
.

l,p −

Ψ

l,p

Ap

(cid:101)

(cid:101)
F

(cid:13)
(cid:13)
(cid:13)

(cid:17)

Lemma C.8. Let {ft} be factor processes deﬁned under the assumptions of Lemma C.7. Write et = ft −
∑∞
et,p = ft − ∑p

l=1 Alft−l, et,p = ft − ∑p

Al,pft−l and

ft − ∑p

l=1 Al,pft−l,

et,p =

Al,p

ft−l.

l=1

l=1

(cid:101)

29

(cid:101)

(cid:98)

(cid:98)

(cid:98)

(cid:98)

∑T

t=p+1

et,p, and

Furthermore, deﬁne the corresponding covariance

eT,p)(
eT,p)(cid:62) with
1
eT,p = 1
T−p
T−p
(cid:101)
(cid:101)
(cid:101)
the expectation deﬁned on the measure of assigning probability 1
T−p to each observation.
(cid:98)

et,p −
∑T

Σe,p = E∗(

et,p −
(cid:101)

(cid:98)
If we additionally assume that the empirical distribution of {et} converges weakly to the distribution

eT,p =
et,p, where E∗ is

t=p+1
(cid:101)

eT,p)(

et,p −

et,p −

eT,p)(cid:62) with

Σe,p = E∗(

(cid:98)

(cid:98)

(cid:98)

(cid:101)

(cid:101)

(cid:98)

(cid:98)

function of L(et), then, there exists p3 ∈ N such that for all p ≥ p3 as in Lemma C.6,

(cid:107)

Σe,p − Σe,p(cid:107)F = oP(1),

(cid:107)Σe,p − Σe(cid:107)F = o(1),
(cid:101)
Σe,p −

Σe,p(cid:107)F = OP

(cid:107)

p3/2

Ap −

Ap

(cid:16)
Σe,p − Σe,p(cid:107)F = oP(1),
(cid:98)

(cid:101)

(cid:107)

(cid:13)
(cid:13)
(cid:13) (cid:98)

(cid:13)
(cid:13)
(cid:13)

(cid:101)

= oP(1),

F

(cid:17)

(cid:98)
when N → ∞ and T → ∞.

Proof of Lemma C.8. To show

Σe,p − Σe,p

→ 0 in probability, ﬁrst note that by deﬁnition,

(cid:13)
(cid:13)
(cid:13)(cid:101)
Σe,p − Σe,p

(cid:13)
(cid:13)
(cid:13)(cid:101)

F

(cid:13)
(cid:13)
(cid:13)

=

+

+

F

(cid:13)
(cid:13)
(cid:13)
1
T − p

(cid:13)
(cid:13)
(cid:13)
(cid:13)
1
(cid:13)
T − p
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)(cid:101)

eT,p

e

(cid:62)
T,p

F

(cid:13)
(cid:13)
(cid:13)

=: E1 + E2 + E3,
(cid:101)

et,p

t,p − et,pe(cid:62)
e(cid:62)
t,p

T
∑
t=p+1 (cid:16)
T
∑
t=p+1

(cid:101)
(cid:101)
et,pe(cid:62)
t,p − E

F

(cid:13)
(cid:13)
(cid:17)
(cid:13)
(cid:13)
et,pe(cid:62)
(cid:13)
t,p

(cid:16)

(cid:17)

F

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

with straightforward notations for E1, E2 and E3. Next, we show that the three terms above

converge to zero in probability. For E1, we know that by triangular inequality,

E1 ≤

1
T − p

T
∑
t=p+1

=: E1,1 + E1,2,

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

et,p − et,p

e(cid:62)
t,p

(cid:0)

(cid:101)

(cid:1)

(cid:101)

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

F

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T − p

T
∑
t=p+1

et,p

et,p − et,p

(cid:0)

(cid:101)

(cid:62)

(cid:1)

F

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

with obvious notations for E1,1 and E1,2. It is then sufﬁcient to show E1,1 → 0 in probability since

30

E1,2 can be dealt with in a similar way. We can now bound E1,1 by

T
∑
t=p+1

T
∑
t=p+1

T
∑
t=p+1

p
∑
l=1 (cid:16)
p
∑
l=1

(cid:0)
∞
∑
l=p+1

E1,1 ≤

+

+

1
T − p

(cid:13)
(cid:13)
(cid:13)
(cid:13)
1
(cid:13)
T − p
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T − p

Al,p − Al,p

ft−l

e(cid:62)
t,p

(cid:101)

Al,p − Al

(cid:1)
e(cid:62)
t,p

Alft−l

(cid:17)

ft−l

(cid:101)
e(cid:62)
t,p

F

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

F

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:101)

.

F

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Since both {ft} and {

(cid:101)
et,p} are r × 1 vectors, by Assumption 4.2 and Lemma C.5, we have

(cid:101)
E1,1 = OP

p
∑
l=1 (cid:16)

Al,p − Al,p

(cid:101)

+

∞
∑
l=p+1

(1 + l) (cid:107)Al(cid:107)F

,

(cid:33)

(cid:17)

F

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:32)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

which tends to zero in probability.

E2 → 0 in probability can be shown similarly, since {ft} is stationary. For E3, ﬁrst write that

,

F

(cid:13)
(cid:13)
(cid:13)
as

(cid:13)
(cid:13)
(cid:13)

E3 =

eT,p

e

(cid:62)
T,p

≤

where

eT,p

(cid:13)
(cid:13)
(cid:13)(cid:101)
(cid:13)
(cid:16)
(cid:13)
(cid:13)
(cid:101)
(cid:13)
= OP

F

(cid:13)
(cid:13)
(cid:13)

eT,p − eT,p

(cid:101)

eT,p − eT,p

(cid:62)

+ 2

eT,p − eT,p

e(cid:62)
T,p

+

eT,pe(cid:62)
T,p

(cid:17) (cid:16)

(cid:101)

(T − p)−1/2

(cid:17)

(cid:13)
(cid:13)
(cid:13)
. Hence it is sufﬁcient to consider

(cid:17)

(cid:16)

(cid:101)

F

(cid:13)
(cid:13)
(cid:13)
(cid:13)

F

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
eT,p − eT,p

(cid:13)
(cid:13)

(cid:13)
(cid:13)
eT,p − eT,p

(cid:13)
(cid:13)
(cid:13)(cid:101)

(cid:16)

(cid:13)
(cid:13)
(cid:13)

eT,p − eT,p

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
Al,pft−l −

(cid:1)

∞
∑
l=1

Alft−l

(cid:101)
Al,p − Al,p

ft−l

(cid:13)
(cid:13)
(cid:13)(cid:101)

(cid:33)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:17)

T
∑
t=p+1

(cid:0)

p
(cid:101)
∑
l=1

T
∑
t=p+1

T
∑
t=p+1 (cid:32)
p
∑
l=1 (cid:16)
p
∑
l=1

T
∑
t=p+1

=

1
T − p

=

≤

1
T − p

1
T − p

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
1
(cid:13)
T − p
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
= OP

+

(cid:0)
Al,p − Al,p

p
∑
l=1 (cid:16)

(cid:101)

Al,p − Al

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
+

(cid:17)

ft−l

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
+ OP

(cid:1)

(cid:13)
(cid:13)
(cid:13)
∞
(cid:13)
∑
(cid:13)
l=p+1

1
T − p

T
∑
t=p+1

∞
∑
l=p+1

Alft−l

(1 + l) (cid:107)Al(cid:107)F

p
→ 0,

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
where the last line follows from Assumption 4.2 and Lemma C.5, and we use the same arguments

(cid:32)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

F(cid:33)

(cid:32)

(cid:33)

(cid:17)

(cid:101)

for E1,1 as above. Therefore, we can conclude that

Σe,p − Σe,p

(cid:13)
(cid:13)
(cid:13)(cid:101)

31

→ 0 in probability.

F

(cid:13)
(cid:13)
(cid:13)

To see

Σe,p − Σe

→ 0, note that

F

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Σe,p − Σe

=

F

E

et,pe(cid:62)

t,p − ete(cid:62)

t

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Hence it sufﬁces to show

have

≤

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
E

(cid:16)

E

et,p − et

e(cid:62)
t,p

(cid:110)(cid:0)
et,p − et

(cid:1)
e(cid:62)
t,p

(cid:110)(cid:0)

(cid:13)
(cid:13)
(cid:13)

(cid:1)

(cid:111)(cid:13)
(cid:13)
(cid:13)

F

F

F

E

+

et,p

et,p − et

(cid:17)(cid:13)
(cid:13)
(cid:13)
(cid:111)(cid:13)
(cid:13)
(cid:13)
→ 0. For this, by triangular inequality, we

(cid:111)(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

(cid:110)

(cid:0)

(cid:1)

(cid:62)

F

.

E

et,p − et

e(cid:62)
t,p

(cid:110)(cid:0)

(cid:13)
(cid:13)
(cid:13)

(cid:1)

F

(cid:111)(cid:13)
(cid:13)
(cid:13)

≤

E

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
= O

p
∑
l=1

(cid:0)
p
∑
l=1

(cid:32)

Al,p − Al

(cid:1)
Al,p − Al

ft−le(cid:62)
t,p

+

E

(cid:13)
(cid:13)
(cid:13)
∞
(cid:13)
∑
(cid:13)
l=p+1

F

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
+ O

(cid:32)

F(cid:33)

∞
∑
l=p+1

Alft−le(cid:62)
t,p

(cid:107)Al(cid:107)F

→ 0,

(cid:33)

F

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
where we stress the fact that (cid:107)ft(cid:107) (cid:16) (cid:107)et,p(cid:107) (cid:16) 1 and use the results in Lemma C.5.

(cid:13)
(cid:13)

With similar arguments, we can show that

that

Σe,p −

Σe,p

can be expressed as

→ 0 in probability. Firstly, notice

Σe,p −

Σe,p

(cid:13)
(cid:13)
(cid:13)(cid:98)

(cid:101)

F

(cid:13)
(cid:13)
(cid:13)

(cid:16)

(cid:98)
Σe,p −

(cid:17)

(cid:101)
Σe,p =

1
T − p

(cid:98)

(cid:101)

=

1
T − p

−

1
T − p

+

1
T − p

+

1
T − p

T
∑
t=p+1 (cid:20)(cid:16)
T
∑
t=p+1 (cid:104)(cid:16)
T
∑
t=p+1 (cid:104)(cid:16)
T
∑
t=p+1 (cid:20)(cid:16)
T
∑
t=p+1 (cid:20)(cid:16)

(cid:62)

(cid:21)

(cid:17)

et,p −

eT,p

et,p −

eT,p

(cid:62)

−

et,p −

eT,p

et,p −

eT,p

(cid:98)
et,p −

(cid:98)
et,p −

(cid:98)
et,p −

(cid:98)
et,p −

(cid:17) (cid:16)

(cid:17)

(cid:16)

(cid:98)
eT,p

(cid:98)
−

(cid:98)
et,p −

eT,p

(cid:101)
et,p −

(cid:101)
et,p

(cid:17) (cid:16)
(cid:62)

(cid:101)

(cid:101)

(cid:1)
(cid:101)
eT,p

(cid:62)

(cid:17)

(cid:101)

(cid:17)

(cid:16)

(cid:98)
eT,p

−

(cid:101)
et,p −

(cid:101)
eT,p

(cid:17)(cid:105) (cid:0)

(cid:98)
eT,p −

(cid:17)

(cid:16)

(cid:101)
et,p −

(cid:101)
eT,p

(cid:101)
et,p −

(cid:101)
eT,p

(cid:98)
eT,p

(cid:98)
eT,p

(cid:17) (cid:16)

(cid:17) (cid:16)

(cid:17)(cid:105) (cid:16)
(cid:62)

(cid:98)

(cid:21)

(cid:17)

(cid:62)

(cid:21)

(cid:17)

.

Recall that

eT,p = 1
T−p

∑T

t=p+1

(cid:101)
et,p and

(cid:98)
(cid:101)
eT,p = 1
T−p
∑T

(cid:98)
∑T

t=p+1

it is sufﬁcient to study the leading term 1
T−p

t=p+1

(cid:101)

(cid:101)

(cid:98)

For this, it is sufﬁcient to consider the order of

et,p, therefore, by triangular inequality,
et,p)(cid:62).

et,p) − (

et,p −

et,p −

(

et,p −
(cid:98)

et,p)(
(cid:101)

et,p −
(cid:101)

. We then
(cid:101)

F

(

et,p −
(cid:98)
(cid:104)
∑T
t=p+1(
(cid:98)

et,p)
(cid:105)
et,p)(cid:62)
(cid:98)
(cid:13)
(cid:13)
(cid:13)

(cid:101)

(cid:98)

(cid:101)

(cid:98)

1
T−p

(cid:13)
(cid:13)
(cid:13)

32

have the bound

1
T − p

T
∑
t=p+1

et,p −

et,p

(cid:13)
(cid:13)(cid:98)

(cid:13)
(cid:13)

(cid:101)

2 ≤ 3

Al,p −

Al,p

p
∑
l=1 (cid:13)
(cid:13)
(cid:13) (cid:98)
3
T − p

+

= OP

(cid:13)
(cid:13)
(cid:101)
(cid:13)
ft − ft

T
∑
t=p+1 (cid:13)
(cid:13)
(cid:13) (cid:98)
Ap −
Ap

2

F

(cid:19)

(cid:18)(cid:13)
(cid:13)
(cid:13) (cid:98)

(cid:13)
(cid:13)
(cid:13)

(cid:101)

2

F

1
T − p

2

+ 3

(cid:13)
(cid:13)
(cid:13)
+ OP

ft−l

T
∑
t=p+1 (cid:13)
(cid:13)
p
(cid:13) (cid:98)
∑
Al,p
l=1 (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13) (cid:101)
(cid:13)
ft − ft
p

2

F

(cid:18)

(cid:13)
(cid:13)
(cid:13) (cid:98)

2

(cid:13)
(cid:13)
(cid:13)
1
T − p

2

,

(cid:19)

(cid:13)
(cid:13)
(cid:13)

ft−l − ft−l

T
∑
t=p+1 (cid:13)
(cid:13)
(cid:13) (cid:98)

2

(cid:13)
(cid:13)
(cid:13)

which converges to 0 in probability by the results of Lemmas C.3 and C.4. Hence we can

Σe,p

conclude that

Σe,p −
(cid:13)
Σe,p − Σe,p
(cid:13)
(cid:13)(cid:98)
oP (1), and the triangular inequality.

Lastly,

F

(cid:101)
F
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)(cid:98)

→ 0 in probability.

(cid:13)
= oP (1) follows directly from
(cid:13)
(cid:13)

Σe,p −

Σe,p

(cid:13)
(cid:13)
(cid:13)(cid:98)

(cid:101)

F

(cid:13)
(cid:13)
(cid:13)

= oP (1),

Σe,p − Σe,p

(cid:13)
(cid:13)
(cid:13)(cid:101)

=

F

(cid:13)
(cid:13)
(cid:13)

33

