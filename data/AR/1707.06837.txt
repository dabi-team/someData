An Alternative Estimation Method of a Time-Varying
Parameter Model

Mikio Itoa, Akihiko Nodab,c and Tatsuma Wadad∗
a Faculty of Economics, Keio University, 2-15-45 Mita, Minato-ku, Tokyo 108-8345, Japan

b Faculty of Economics, Kyoto Sangyo University, Motoyama, Kamigamo, Kita-ku, Kyoto 603-8555, Japan

c Keio Economic Observatory, Keio University, 2-15-45 Mita, Minato-ku, Tokyo 108-8345, Japan

d Faculty of Policy Management, Keio University, 5322 Endo, Fujisawa, Kanagawa, 252-0882, Japan

This Version: December 22, 2017

Abstract: A non-Bayesian, regression-based or generalized least squares (GLS)-based
approach is formally proposed to estimate a class of time-varying AR parameter mod-
els. This approach has partly been used by Ito et al. (2014, 2016a,b), and is proven
to be eﬃcient because, unlike conventional methods, it does not require Kalman ﬁlter-
ing and smoothing procedures, but yields a smoothed estimate that is identical to the
Kalman-smoothed estimate. Unlike the maximum likelihood estimator, the possibility
of the pile-up problem is negligible. In addition, this approach enables us to deal with
stochastic volatility models, models with a time-dependent variance-covariance matrix,
and models with non-Gaussian errors that allow us to deal with abrupt changes or struc-
tural breaks in time-varying parameters.

Keywords: Kalman Filter; Non-Bayesian Time-Varying Model, Generalized Least Squares,
Vector Autoregressive Model.

JEL Classiﬁcation Numbers: C13; C22; C32; C51.

7
1
0
2

c
e
D
1
2

]
E
M

.
t
a
t
s
[

2
v
7
3
8
6
0
.
7
0
7
1
:
v
i
X
r
a

∗Corresponding Author. E-mail: twada@sfc.keio.ac.jp, Tel. +81-466-49-3451, Fax. +81-466-49-3451

 
 
 
 
 
 
1 Introduction

It has been widely recognized among Macroeconomists that the time-varying parameter
models are ﬂexible enough to capture the complex nature of a macroeconomic system,
thereby yielding better forecasts and a better ﬁt to data than models with constant
In the literature on dynamic econometrics models, the instability of the
parameters.
parameters in the model has been often incorporated in Markov-switching models (e.g.,
Hamilton (1989)) or structural change models (e.g., Perron (1989)). However, time-
varying models allow the parameters to change gradually over time, which is the main
diﬀerence between time-varying models and Markov switching or structural break models.
In the literature on the application of time-varying vector autoregressive (TV-VAR)
models to macroeconomics, Bernanke and Mihov (1998) consider the possibility of au-
toregressive parameters being time-varying. However, after conﬁrming the stability of
the parameters using the parameter consistency test of Hansen (1992), they employ the
time-invariant (usual) VAR model. Regarding this modeling strategy, Cogley and Sar-
gent (2005) ﬁnd that Hansen’s (1992) test has low power and is unreliable. They instead
propose a TV-VAR model with stochastic volatility in the error term. A study by Prim-
iceri (2005) sheds light on a technical aspect of the time-varying model, particularly, the
Bayesian estimation technique for the time-varying parameters. In general, diﬃculties in
dealing with time-varying parameter models arise when free parameters and unobserved
variables need to be estimated. Primiceri (2005) presents a clear estimation procedure
based on the Bayesian Markov Chain Monte Carlo (MCMC) method.

Several studies, including Primiceri (2005), claim that the Bayesian method is pre-
ferred to the maximum likelihood (ML) method because the former (i) is less likely to
suﬀer from the so-called pile-up problem (Sargan and Bhargava (1983)); (ii) is less likely
to have computation problems, such as a degenerated likelihood function or multiple local
minima; and (iii) facilitate the ﬁnding of statistical inferences such as standard errors.
However, both the Bayesian and ML methods require Kalman ﬁltering to estimate an
unobservable state vector that includes the time-varying parameters.1

Attempts to understand Kalman ﬁltering through the lens of conventional regres-
sion literature are given, for example, by Duncan and Horn (1972), Maddala and Kim
(1998), and Durbin and Koopman (2012). To our knowledge, Duncan and Horn (1972)
is the ﬁrst study to show that the generalized least squares (GLS) estimator for basic
state-space models equivalently uncovers the unobserved state vector estimated through
Kalman ﬁltering. Similarly, a series of papers by Ito et al. (2014, 2016a,b) apply the
TV-VAR, time-varying autoregressive (TV-AR), and time-varying vector error correc-
tion (TV-VEC) models to stock prices and exchange rates, without using the Kalman
ﬁlter but using the regression method. In this paper, following the spirit of Duncan and
Horn (1972), we elucidate the statistical properties of the regression-based approach or
the GLS-based approach that utilizes ordinary least squares (OLS) or GLS in lieu of the
Kalman smoother. To be more precise, hereafter, our GLS-based approach includes OLS
as a variety of GLS. In recent studies, this approach is employed by Ito et al. (2014,

1An alternative to those two methods is Cooley and Prescott (1976), who do not utilize Kalman

ﬁltering but employ the likelihood method to estimate unknown parameters.

1

2016a,b) to evaluate market eﬃciency in stock markets and foreign exchange markets.2
In this paper, we ﬁrst show that the class of TV-AR models, which includes the TV-AR,
TV-VAR, and TV-VEC models, is readily estimated using the GLS method. The esti-
mates are, in fact, tantamount to the Kalman-smoothed estimates. This ﬁnding may not
be surprising given Duncan and Horn (1972) or extensions thereof. Additionally, one may
argue that the main purpose of employing the Kalman ﬁlter (or smoother) is to avoid
using a system of large matrices required by GLS. This argument was reasonably strong
until computers became capable of handling large matrices.

The equivalence between GLS and the Kalman smoother leads us to the following
question: If GLS yields the Kalman-smoothed estimates, then how good is the GLS-based
approach in recovering time-varying parameters? This question is practical and important
because, in general, ﬁnite sample properties of the GLS estimator are unknown.3 Another
question pertains to the seriousness of the pile-up problem. The pile-up problem is said
to occur when the ML estimate of the variance of the state equation error is zero, even
though its true value is small but not zero. While our proposed method is not identical
to ML because we do not maximize the likelihood function with respect to the variances
of errors, it is not immediately obvious whether our GLS-based approach suﬀers from the
pile-up problem to the same degree as ML.

Also considered are the possibilities of non-independent and identically distributed
(i.i.d.) or non-Gaussian errors in the model. The former is repeatedly used in this area of
study because it is reasonable to assume that the variance of errors has a variance that
may be time-varying. The latter is important in empirical studies because it allows us to
model abrupt changes or structural breaks in time-varying parameters, which is a similar
strategy employed by Perron and Wada (2009) and elsewhere.

To sum up, the contributions of the study are the following: We present the equivalence
of Kalman smoothing and GLS for the class of TV-AR models. We then show that the
GLS estimates the true time-varying parameters fairly well even when the errors are not
i.i.d. or not Gaussian, provided an appropriate way to implement feasible GLS (FGLS)
is carefully chosen based primarily on the relative size of the variances of the errors or
signal-to-noise ratio (SNR). The pile-up problem that is often cumbersome to ML is shown
to be negligible.

The rest of this paper is organized as follows. Section 2 presents our model together
with its likelihood function. We analyze the statistic properties of the GLS-based approach
for the class of TV-AR models in Section 3. Section 4 evaluates the GLS-based approach
under a variety of conditions such as a small SNR, non i.i.d. errors, and non-Gaussian
errors. An application to macroeconomic data, including a comparison with the Bayesian
MCMC method, is demonstrated in Section 5. Section 6 concludes the paper.

2Note that Ito et al. (2014, 2016a) do not formally prove that their regression-based approach gener-

ates estimates that are equivalent to the Kalman-smoothed estimates.

3Note that we have unknown parameters such as the variances of error terms in our model. In such

a case, we have to rely on Feasible GLS (FGLS), which may not be equivalent to GLS.

2

2 Model

In this section, we present our model, which admits the class of TV-AR models. We then
show that our model permits two diﬀerent matrix forms. The ﬁrst matrix form is that of
Durbin and Koopman (2012), which they use as a device to ﬁnd the Kalman-smoothed
estimate of an unobserved state vector. The second matrix form is an extended version
of Maddala and Kim (1998), which we employ in this paper. As it will become clear, this
form allows us to use GLS for the estimation of time-varying parameters. We can then
formally demonstrate that the Kalman-smoothed estimate from the ﬁrst matrix form is
equivalent to the GLS estimates of the second matrix form, proving that GLS estimates
are an alternative estimation method to the Kalman smoother.

2.1 A Basic State-Space Model of the Class of Time-Varying AR

Models

Our model is given by:

yt = Ztβt + εt
βt = βt−1 + ηt,

(1)
(2)

where yt is a k × 1 vector of observable variables; Zt is a k × m matrix of observable
variables; βt is an m × 1 vector of time-varying parameters; and εt and ηt are k × 1 and
m × 1 vectors of normally distributed error terms with variance covariance matrices of Ht
and Qt, respectively:


















 ∼ N





 ,





 .

0

0

Ht

0

0 Qt

εt

ηt

Note that the variance-covariance matrices Ht and Qt are allowed to be time dependent,
as in the stochastic volatility model. For the initial value of βt, we assume

β0 ∼ N (b0, P0) .

If we assume b0 and P0 are known, it is reasonable to utilize the diﬀuse prior for P0 because
βt follows a non-stationary process. In this case, the diagonal elements of P0 should be
large numbers (e.g., see Harvey (1989); Koopman (1997)). Alternatively, we can simply
ignore P0 as zero when we assume β0 is known and not stochastic.

Equations (1) and (2) can be utilized for a variety of TV-AR models. For example,
when k = 1, Zt = yt−1 yields a TV-AR(1) model. Similarly, the TV-VAR(1) model
(cid:1) and
yt = Atyt−1 + εt with At = At−1 + ηt is expressed by setting Zt = (cid:0)y(cid:48)
βt = vec (At). It is also possible to include intercepts that vary with time. For a TV-
AR(1) model, for example, one can set Zt = (1, yt−1), and then, the ﬁrst element of βt is
the time-varying intercept.

t−1 ⊗ Ik

We present two speciﬁcations of our model, (1) and (2), below. The ﬁrst speciﬁcation
allows us to derive the Kalman-smoothed estimate as explained by Durbin and Koopman
(2012). The second speciﬁcation is in the same spirit as Duncan and Horn (1972), lead-
ing us to the GLS-based approach. As we shall see, both speciﬁcations yield the same
smoothed estimate.

3

2.2 Model Matrix Formulation of the State-Space Model

Following Durbin and Koopman (2012), we employ the matrix formulation of equations
(1) and (2). For t = 1, . . . , T , we have a system of equations:

YT = Zβ + ε
β = C (b∗

0 + η)

where

ε ∼ N (0, H) ,

η ∼ N (0, Q) ,

(3)
(4)

with

YT =

H =

b∗
0 =































0










b0

0
...
0











, Z =











yp+1

yp+2
...
yT

Hp+1

Hp+2

. . .

Zp+1

Zp+2

. . .











0

ZT

, β =











βp+1

βp+2
...
βT











, ε =











εp+1

εp+2
...
εT











0

0

HT











, C =











I 0 · · ·

I
...
I

I
...
I

. . .

I











,

0
...
0

I

, P ∗

0 =











P0 0

0

0
...
0

0

. . .

0 · · ·

0











,

η =











ηp+1

ηp+2
...
ηT











, Q =











Qp+1

0

Qp+2

. . .











.

0

QT

Unlike a more general state-space model where equation (2) has a transition matrix
that includes unknown parameters to be estimated, the matrix formulation of the TV
parameter model is largely simpliﬁed. For example, matrix C is often called the random
walk generating matrix (e.g., Tanaka (2017)), which is non-singular, and there are no free
parameters to be estimated in the matrix. In addition, if Ht and Qt are time invariant;
that is, if there are no GARCH eﬀects or stochastic volatility in the model, the matrices
H and Q are simpliﬁed substantially.

For simplicity, we assume b0 is known and non-stochastic; hence, P0 = 0.4

4This assumption does not change our conclusions below. The main diﬀerence is that V ar (β) =
0 + Q) C (cid:48)Z (cid:48) + H = Ω. An exception is when the diﬀuse prior is
0 + Q) C (cid:48) and V ar (YT ) = ZC (P ∗
C (P ∗
used, and the likelihood function is computed excluding the ﬁrst few observations. In such a case, the
estimates of the unknown intercept parameters would be diﬀerent across the two approaches.

4

2.3 Model with Time-Invariant Intercepts

While our model, (1) and (2), and its matrix formulation, (3) and (4), are ﬂexible enough
to admit time-varying coeﬃcients, it is sometimes assumed that the class of TV-AR
models has time-invariant intercepts. For the purpose of deriving the likelihood function,
here, we modify our model to admit time-invariant intercepts. Suppose we have a k × k
vector of time-invariant intercepts, v, in our model. Then, (1) and (2) become

yt = v + Ztβt + εt
βt = βt−1 + ηt.

(5)
(6)

In this case, it is convenient to use a matrix form to derive the likelihood function. With
the vector of intercepts, our model in matrix form, (3) and (4), is then modiﬁed to

YT = Iv + Zβ + ε
β = C (b∗

0 + η) ,

(7)
(8)

(cid:104)

(cid:105)(cid:48)

Ik

where I =
, and Ik is a k × k identity matrix. Similar to our assump-
tion that time-varying intercepts, if they exist, are unknown, we assume that the vector
of time-invariant intercepts, v, is the unknown parameter vector.

· · ·

Ik

Ik

2.4 The Likelihood Function

Since we have the assumption that ε and η are normally distributed, our matrix formula-
tion of (7) and (8) allows us to write the log likelihood function for YT given the covariance
matrices of the errors (H and Q), intercepts (v), and the initial value vector (b∗

0) as:

(T − p) k
2

log 2π −

1
2

log |Ω|

(9)

(YT − ZCb∗

0 − Iv)(cid:48) Ω−1 (YT − ZCb∗

0 − Iv) ,

log p (YT |H, Q, v, b∗

0) = −

−

where

Ω = H + ZCQC (cid:48)Z (cid:48).

1
2

The likelihood function is further simpliﬁed when we have no time-invariant intercepts.
This is true even if we have time-varying intercepts because the time-varying intercepts
are included in vector β. As a result, in such a case, our log likelihood function becomes

log p (YT |H, Q, b∗

0) = −

−

(T − p) k
2

log 2π

1
2

log |Ω| −

1
2

(YT − ZCb∗

0)(cid:48) Ω−1 (YT − ZCb∗

0) .

(10)

Interestingly, provided that H, Q, and b∗

0 are known, the likelihood function does not

involve the parameter vector of our main interest, β.

5

3 Estimation of the Time-Varying AR Models

3.1 Regression Lemma and Kalman Smoothing

Before showing the equivalence of our estimator and the Kalman smoother, let us clarify
what the Kalman smoother does when the model is described by equations (1) and (2).
According to Durbin and Koopman (2012), the Kalman-smoothed state of β is given by
the expectation of β conditional on the information pertaining to all observations of yt:

(cid:101)β = E [β|YT ] = E [β] + Cov (β, YT ) V ar (YT )−1 (YT − E [YT ]) .

(11)

To derive equation (11), note that we assume normal errors. The variance of β, given all
the observations YT , is

V ar (β|YT ) = V ar (β) − Cov (β, YT ) V ar (YT )−1 Cov (β, YT )(cid:48) .

(12)

Note that the Kalman-smoothed estimate and its mean squared error (MSE) are given
by (11) and (12), respectively. Durbin and Koopman (2012) call these equations the
regression lemma, which derives the mean and variance of the distribution of β conditional
on YT , assuming the joint distribution of β and YT is a multivariate normal distribution.
It follows that for (3) and (4), the Kalman-smoothed estimate is

(cid:101)β = E [β|YT ] = Cb∗

0 + CQC (cid:48)Z (cid:48)Ω−1 (YT − ZCb∗

0) ,

and the conditional variance (or MSE) of the smoothed estimate is

V ar (β|YT ) = CQC (cid:48) − CQC (cid:48)Z (cid:48)Ω−1ZCQC (cid:48),

(13)

(14)

where Ω = H + ZCQC (cid:48)Z (cid:48). Equations (13) and (14) are obtained by utilizing the fact
that Cov (β, YT ) = V ar (β) Z (cid:48) = CQC (cid:48)Z (cid:48) and V ar (YT ) = ZCQC (cid:48)Z (cid:48) + H = Ω, and by
substituting them into equations (11) and (12). It is well known that (13) is a minimum-
variance linear unbiased estimate of β, given YT , even though we do not assume the errors
are normally distributed (see Durbin and Koopman (2012), among others).

3.2 The Equivalence of the GLS-Based Estimator and the Kalman

Smoother

It is possible to write equations (3) and (4) in another matrix form to apply a conventional
regression analysis:





YT

−b∗
0





 =



Z

−C −1





 β +





 .

ε

η

(15)

This speciﬁcation is similar to that of Duncan and Horn (1972) and of Maddala and Kim
(1998). The main diﬀerence between our speciﬁcation and that of Duncan and Horn
(1972) is that the former applies to a time-varying parameter model, while the latter

6

is for a more general state-space model, which allows the transition equation to have a
transition matrix F (i.e., when equation (2) is βt = F βt−1 + ηt). Since we do not need to
estimate the transition matrix, our regressors in equation (15) are all known. In contrast,
Duncan and Horn (1972) assume the matrix F is known, which renders their estimation
impractical. The original form of Maddala and Kim (1998, pp.469–470) is similar to ours,
but it is a general form for a scalar yt. Hence, seemingly, it does not aim to deal with
the autoregressive part of the time-varying parameter models nor does it consider vector
processes. For a simple scalar case, however, Maddala and Kim (1998) point out that
GLS is equivalent to the Kalman-smoothed estimate without formal proof.

As mentioned by Duncan and Horn (1972), the confusion concerning the similarities
and diﬀerences between Kalman ﬁltering and the conventional regression model stems
from the fact that the former is the expectation of β, conditional on the information
about YT , which is the linear projection of β onto the space spanned by YT (provided that
the errors are normally distributed); while the latter is a linear projection of the dependent
variable onto the space spanned by the regressor, which is the projection of the left hand

. However, Duncan
side on equation (15) onto the space spanned by
and Horn (1972) essentially show that GLS for (15) up through the time-t observation
yields the Kalman ﬁltered estimate. Therefore, a natural conjecture is that we obtain the
Kalman-smoothed estimate of β when GLS is applied to all the observations, YT . In fact,
this conjecture is correct, and we have the following Proposition.

Z (cid:48) −C −1(cid:48)

(cid:104)

(cid:105)(cid:48)

Proposition 1 The GLS estimator of model (15) yields the Kalman-smoothed estimates
(13) and its mean squared error matrix (14).

Proof. See the Appendix.

3.3 The GLS Estimator Under the Presence of Time-Invariant

Intercepts

As we discuss in the previous section, our model admits time-invariant intercepts. There-
fore, it is straightforward to deﬁne the GLS estimator for such models. To do so, assuming
that the time-invariant intercepts are unknown, let us deﬁne the vector of unknown pa-
rameters, β∗ =
is

. Then, the matrix form for regression that is analogous to (15)

v(cid:48) β(cid:48)

(cid:105)(cid:48)

(cid:104)





YT

−b∗
0





 =



I

Z

0 −C −1


 β∗ +







 .

ε

η

(16)

Here, one of the advantages of utilizing the regression approach (16) over Kalman
smoothing (8) is that the unknown intercept vector v is estimated simultaneously with
β. Then, it can be shown that the GLS estimate (cid:98)v is indeed the maximum likelihood
estimate.

Proposition 2 The GLS estimate (cid:98)v of model (16) is the maximum likelihood estimate
(MLE) of (8), (cid:98)vM L conditional on H, Q, and b∗
0.

7

Proof. From the likelihood function, (9), the normal equations pertaining to v are

I (cid:48)Ω−1 (YT − ZCb∗

0 − I(cid:98)vM L) = 0.

Therefore, the MLE for v is

(cid:98)vM L = (cid:0)I (cid:48)Ω−1I(cid:1)−1 I (cid:48)Ω−1 (YT − ZCb∗
0) .

Now, the GLS estimates for β∗ in model (16) are

(cid:98)β∗ =





 =







(cid:98)v
(cid:98)β

I (cid:48)H −1I

I (cid:48)H −1Z

Z (cid:48)H −1I Z (cid:48)H −1Z + C (cid:48)−1Q−1C −1










−1







I (cid:48)H −1

Z (cid:48)H −1

 YT +




 b∗

0



 .

O

C −1(cid:48)Q−1

(17)

(18)

Using the Lemma, we arrive at the following (see the appendix for details):

(cid:98)v = (cid:0)I (cid:48)Ω−1I(cid:1)−1 I (cid:48)Ω−1 (YT − ZCb∗
0) .

This proves (cid:98)vM L = (cid:98)v.

Proposition 3 The GLS estimate (cid:98)β of model (16) is the Kalman-smoothed estimate of
model (8).

Proof. Thanks to the intercept, the Kalman-smoothed estimate is now

(cid:101)β = Cb∗

0 + CQC (cid:48)Z (cid:48)Ω−1 (YT − Iv − ZCb∗

0) .

(19)

From (18), it follows that

(cid:98)β = Cb∗

0 + CQC (cid:48)Z (cid:48)Ω−1 (YT − I(cid:98)v − ZCb∗

0) .

We prove the equivalence.

It is clear that the GLS-based approach can compute the Kalman-smoothed β and
estimate the unknown intercepts, v, simultaneously. The next question is how we can
obtain the statistical inference about (cid:98)β. More precisely, at issue is whether the GLS-based
approach yields the same MSE as the Kalman smoother. The answer to this question is
negative for (cid:98)β.

Proposition 4 The mean squared error of the Kalman smoothed estimate is

V ar (β|YT ) = CQC (cid:48) − CQC (cid:48)Z (cid:48)Ω−1ZCQC (cid:48),

(20)

whereas the variance estimated from the GLS-based approach (18) is

V ar

(cid:16)

(cid:17)
(cid:98)β

= CQC (cid:48) −CQC (cid:48)Z (cid:48)Ω−1ZCQC +CQC (cid:48)Z (cid:48)Ω−1I (cid:0)I (cid:48)Ω−1I(cid:1)−1 I (cid:48)Ω−1ZCQC (cid:48). (21)

8

Proof. See the Appendix.

(cid:16)

(cid:16)

The diﬀerence between the Kalman-smoothed V ar (β|YT ) and the GLS-based vari-
(cid:17)
is CQC (cid:48)Z (cid:48)Ω−1I (I (cid:48)Ω−1I)−1 I (cid:48)Ω−1ZCQC (cid:48), which pertains to the estima-
(cid:98)β
ance V ar
If we did not have to estimate v, (as we assume for the Kalman-smoothed
tion of v.
would be the same. In other words, if v is known, the
estimate)V ar (β|YT ) and V ar
MSE of the Kalman-smoothed estimate is the same as the variance of the GLS-based
estimate. As a matter of fact, if V ar ((cid:98)v) = (I (cid:48)Ω−1I)−1 = 0, the two estimates would be
identical. This result reﬂects that the two approaches yield the same estimate and MSE,
as in Proposition 1. Nevertheless, what is important here is that we can obtain (20) by
utilizing the estimated variance of (cid:98)β∗ of (18). More speciﬁcally, we can estimate the MSE
of the Kalman-smoothed estimate by

(cid:17)
(cid:98)β

V ar (β|YT ) = V ar

(cid:16)

(cid:17)
(cid:98)β

− Cov

(cid:16)

(cid:17)
(cid:98)β, (cid:98)v

V ar ((cid:98)v)−1 Cov

(cid:16)

(cid:17)(cid:48)

(cid:98)β, (cid:98)v

.

3.4 GLS in Practice

As we have seen in previous subsections, under the condition that the variance-covariance
matrices of errors (H and Q) are known, the GLS estimator of β is identical to the
Kalman smoothed estimates. However, in practice, those variance-covariance matrices are
generally unknown. To ﬁnd the FGLS estimator, it is often used the two-step approach:
First, one can estimate β by ordinary least squares (OLS). Then, the OLS residuals are
used to compute the estimates of H and Q, denoted as (cid:98)H and (cid:98)Q, respectively. As the
second step, FGLS is applied to our model assuming (cid:98)H and (cid:98)Q are the variance-covariance
matrices of ε and η, respectively.

However, there are two problems pertaining to FGLS. First, H and Q may involve too
many unknown parameters. For example, when we deal with a TV-VAR(p) model with
k variables, H has (T − p) of k × k matrices, and Q has the same number of k (kp + 1) ×
k (kp + 1) matrices. The second problem is possible heteroskedasticity. Suppose that ε is
much greater in magnitude than η. More precisely, when the average trace of H is much
larger than the average trace of Q. Then, in such a case, our GLS-based approach has
heteroskedasticity in regression equation (15), potentially causing imprecise estimation of
β. This concern is largely mitigated when the average trace of H and the average trace
of Q are a similar size.

As a solution to these two problems, we propose the following FGLS procedure.

• Step 1. We estimate model (15) by OLS, and obtain the estimate of β by OLS, (cid:98)βO.
From the OLS residuals, (cid:98)εt and (cid:98)ηt, we construct the ﬁrst step estimates of Ht and
Qt:

(cid:98)Ht =

1
T − p

T
(cid:88)

t=p+1

(cid:98)εt(cid:98)ε(cid:48)

t and (cid:98)Qt =

1
T − p

T
(cid:88)

t=p+1

(cid:98)ηt(cid:98)η(cid:48)
t.

Then, to construct the estimates of H and Q, denoted (cid:98)H O and (cid:98)QO, respectively,
we set (cid:98)Hp+1 = (cid:98)Hp+2 = · · · = (cid:98)HT and (cid:98)Qp+1 = (cid:98)Qp+2 = · · · = (cid:98)QT . This is to assume

9

that the variances of ε and η are time-invariant. This assumption is by no means
desirable because a number of studies pertaining to the class of TV-VAR models have
focused on the stochastic volatility models, which requires (cid:98)Qt (cid:54)= (cid:98)Qt+1, for example.
The simulations in the next section will reveal how severely this assumption aﬀects
our estimation when stochastic volatility is present. With (cid:98)H O and (cid:98)QO, the log
likelihood is computed by (9) or (10).

• Step 2 (1FGLS). Given (cid:98)H O and (cid:98)QO, we apply FGLS to obtain (cid:98)βG1, which is the
FGLS or 1FGLS estimate of β. We also compute the estimates of H and Q, denoted
as (cid:98)H G1 and (cid:98)QG1, respectively, in the same way as we computed (cid:98)H O and (cid:98)QO in the
ﬁrst step. Then, the value of the log likelihood function is computed.

• Step 3 (2FGLS). We repeat Step 2, computing (cid:98)βG2, which is the (second-time) FGLS

or 2FGLS of β. Then, the value of the log likelihood function is computed.

In summary, our procedure is based on the assumptions that the error terms have
time-invariant variances and that heteroskedasticity arising from diﬀerent sizes of H and
Q can be correctly handled by repeated use of FGLS. To validate our assumptions and
procedure, we investigate the degree to which our procedure precisely estimate the true
β via simulations in the next section.

4 Simulations

Among some inﬂuential empirical studies in the literature of TV-VAR, both Cogley and
Sargent (2001, 2005) and Primiceri (2005) employ a three-variable TV-VAR(2) model.
Hence, in our simulation study, we employ the same speciﬁcation and use simulations
to assess how well the GLS-based approach recovers the true time-varying parameters.
First, we compute the means and variances of the estimated time-varying parameters,
and compare them with the means and variances of the true time-varying parameters.
This is to evaluate the GLS-based approach in terms of its accuracy in estimating the
time-varying parameter. While comparing the ﬁrst and second moments of the estimates
to those of the true process may not be adequate to determine whether the GLS-based
approach yields precise estimates, it is a useful way to grasp the overall accuracy of the
estimates.5

Second, we consider the possibility of the pile-up problem. According to Primiceri
(2005), the Bayesian approach is preferred when estimating time-varying parameter mod-
els. This is because, among other reasons, the Bayesian approach can potentially avoid
the pile-up problem. It is not immediately obvious to what extent the problem aﬀects our
estimate, because the literature (e.g., Shephard and Harvey (1990)) provides theoretical
explanations only for limited (simple) cases. On the other hand, our model can have a
vector of time-varying terms (βt), unlike prior studies that analyzed scalar time-varying
terms for simplicity. Therefore, it is reasonable to conduct a simulation study to reveal

5In addition, we can compute the values of the log likelihood function to evaluate whether the repeated
use of FGLS improves accuracy in estimation. A general tendency throughout our simulation is that
aforementioned 2FGLS has a higher likelihood value than 1FGLS.

10

the extent to which our GLS-based approach suﬀers from the pile-up problem. Because
the concern over the pile-up problem becomes stronger when the variance of the state-
equation error is small, or when SNR is small, we study the performance of the GLS-based
approach more comprehensively by altering SNR in the data generating process.

Third, we also evaluate the performance of the GLS-based approach when stochastic
volatility and non-Gaussian errors are present. The reason for investigating the eﬀect of
stochastic volatility on the GLS-based approach is that macroeconomic research, including
Cogley and Sargent (2005) and Primiceri (2005), has been allowing such shocks in the
TV-VAR model. While the GLS-based approach does not require the assumption of i.i.d.
errors to obtain the estimate of βt, we are interested in the extent to which the accuracy of
the GLS-based approach is aﬀected by the stochastic volatility of the errors. For the non-
Gaussian errors, our focus is possible structural breaks in the time-varying coeﬃcients,
βt. By allowing a mixture of normal errors, as explained in the following subsection, we
can model structural breaks or abrupt changes in βt, as opposed to gradual changes that
the time-varying model generally assumes. Our simulation study is expected to shed light
on the performance of the GLS-based approach when such errors are present.

Finally, as we mention in Section 1, we consider OLS as a component of the GLS-
based approach, and hence, we study the performance of OLS using simulations. This is
because, generally speaking, the performance FGLS relative to OLS is not clear especially
when we have a small sample.

4.1 The Data Generating Process

We generate pseudo data by the system of equations (3) and (4) with T = {100, 250},
H = {0.022I, 0.22I, 12I, 102I}, and Q = {0.032I}. By changing the variance of the error to
the observation equation, we consider the role of SNR. In what follows, we deﬁne the SNR
as the average trace of the variance-covariance matrix of ηt relative to the average trace of
the variance-covariance matrix of εt: In our simulation, we consider SNRs for 0.032/0.022,
0.032/0.22, 0.032/12, and 0.032/102. The SNR is particularly important when we consider
the possibility of the pile-up problem, which will be discussed in the next section. For the
initial values, we set b∗

0 = 0.

4.1.1 Non-Gaussian Errors

The original motivation to employ time-varying models for macroeconomic research was
to allow for gradual change in βt. However, it is possible that there are some structural
breaks or abrupt changes in βt, which means that βt is almost constant over time until
some point in the sample, for example, Tb; it then jumps to a diﬀerent level afterward.
One way to model such a break is to assume non-Gaussian errors for ηt. In particular,
we assume mixtures of normal distributions (among others, Perron and Wada (2009)) for
each element of error vector ηt:

ηit = λtζ1,t + (1 − λt) ζ2,t

11

where

λt ∼ i.i.d.Bernoulli (0.95)
ζ1,t ∼ N (cid:0)0, 0.032(cid:1) , ζ2,t ∼ N (cid:0)0, 0.12(cid:1) .

Intuitively, with a probability of 95%, ηt is ζ1,t, which is drawn from a normal distri-
bution with a small variance. This small ηt keeps βt nearly constant over time. However,
a large ηt, which is ζ2,t, is drawn from a normal distribution with a (relatively) large
variance. This ηt causes βt to jump to a new level, with 5% probability. Since we use
the assumption of Gaussian error to derive the equivalence between GLS and the Kalman
smoothed estimator, the eﬀect of non-Gaussian errors on the accuracy of the GLS esti-
mator in estimating βt should be evaluated via simulations.

4.1.2 Stochastic Volatility and Autoregressive Stochastic Volatility

As Cogley and Sargent (2005) argue, in response to the criticisms of Cogley and Sargent
(2001), it is more ﬂexible and realistic to assume that the variance of the shock εt is
time varying. Intuitively, not all shocks are generated from the same i.i.d. process. One
peculiar feature of the GLS-based approach is that it can handle the heteroskedasticity in
Ht and Qt. This means that, at least theoretically, we can estimate the time-varying model
with stochastic volatility, such as the one used by Primiceri (2005). It is also possible
that the error term εt follow the autoregressive stochastic volatility process described by
Taylor (2007) and elsewhere.

However, in general, FGLS is merely a remedy to more precisely estimate the coef-
ﬁcients (in our case, βt) when heteroskedasticity is present, and FGLS is not primarily
designed to estimate the process that the error term (or its variance) follows.

Nevertheless, we use the following data generating process to assess the performance

of the GLS-based approach.

εit = (cid:112)hi,tξt
log hi,t = ρ log hi,t−1 + et

where ρ = 1 when stochastic volatility is considered, and ρ = 0.9 when autoregressive
stochastic volatility is considered; and εit is the i-th element of εt. We assume log hi,0 = 0,
et ∼ N (0, 0.022), and ξt ∼ N (0, 1).

4.2 The Mean and the Variance of Estimated βt, and the Likeli-

hood

Since our simulation is of a TV-VAR(2) model with time-varying intercepts, βt is a 21 × 1
vector. Let βt,i,n denote the true (DGP) βt,i,n (i.e., the i-th element of vector βt,n)and let
(cid:98)βG
t,i,n denote the GLS-based estimate for βt,i,n. Since, in practice, we do not know b∗
0 when
estimating βt,i,n, we estimate b∗
0 as the coeﬃcients vector from a full-sample time-invariant
(usual) VAR(2) model before estimating βt,i,n by GLS. The sample means and the sample

12

standard deviations of the estimate over the sample period are then computed:

G
i,n =
(cid:98)β

(cid:17)

(cid:16)

(cid:98)βG
i,n

sd

=

1
T − p

T
(cid:88)

t=p+1

(cid:98)βG
t,i,n

(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
T − p − 1

T
(cid:88)

t=p+1

(cid:18)

G
(cid:98)βG
t,i,n − (cid:98)β

i,n

(cid:19)2

.

Similarly, we compute those of the true (data generating) process:

βi,n =

sd (βi,n) =

1
T − p

T
(cid:88)

t=p+1

βt,i,n

(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
T − p − 1

T
(cid:88)

t=p+1

(cid:0)βt,i,n − βi,n

(cid:1)2

.

(22)

(23)

(24)

(25)

By (22) and (23) and there DGP counterparts, (24) and (25), we have 21 means and
standard deviations for each replication. After N = 1, 000 replications, we compute the

G
i,n, βi,n, sd
averages of (cid:98)β
of time-varying parameters and 21 means of standard deviations (i.e., i = 1, 2, . . . , 21).

, and sd (βi,n) over the replications. We then have 21 means

(cid:16)
(cid:98)βG
i,n

(cid:17)

mG

i =

sG
i =

1
N

1
N

N
(cid:88)

n=1
N
(cid:88)

n=1

G
i,n; mi =
(cid:98)β

1
N

N
(cid:88)

n=1

βi,n

(cid:17)

(cid:16)

(cid:98)βG
i,n

sd

; si =

1
N

N
(cid:88)

n=1

sd (βi,n)

(26)

(27)

Since both m and s are aggregate means, a small diﬀerence between m and mG or between
s and sG is only an indication that the GLS-based approach works well. Hence, we further
investigate the similarities of β and (cid:98)βG. Comparing each element of β, we deﬁne the
distance, “dist, ” as follows.

disti =

1
N (T − p)

N
(cid:88)

T
(cid:88)

n=1

t=p+1

(cid:12)
(cid:12)βt,i,n − (cid:98)βG
(cid:12)

t,i,n

(cid:12)
(cid:12)
(cid:12) .

(28)

Similarly, we compare the standard deviations of each element of β as a ratio of the

standard deviation of (cid:98)βG

i,n to the standard deviation of the true process, βi,n:

rati =

1
N

N
(cid:88)

n=1

(cid:16)

(cid:17)

sd

(cid:98)βG
i,n
sd (βi,n)

.

(29)

In this simulation study, we focus on both disti and rati. Our criteria for a good
estimator are whether disti of an estimate is close to zero and whether rati of that
estimate is close to one.

13

4.3 Simulation Results 1: The Signal-to-noise Ratio, Sample Size,

and the Precision of Estimation

(Tables 1 and 2 around here)

Tables 1 and 2 display the medians of disti and rati as well as the medians of mi and
si for T = 100 and T = 250, respectively. OLS works relatively well when the SNR is
relatively large because, as Tables 1 and 2 show, the median distance of the estimate from
the true process (i.e., disti) is small, and the median sample variance of the estimated βt
is close to that of the true process (i.e., rati is close to one).6 On the contrary, 2FGLS
works relatively well when the SNR is small. General tendencies from Table 1 can be
summarized as follows. First, OLS and 1FGLS share largely the same characteristics.
However, estimated βt by 2FGLS is much smaller in magnitude than those estimated
by OLS and 1FGLS. Second, OLS, 1FGLS, and 2FGLS all tend to have larger rati as
SNR increases. More precisely, OLS, 1FGLS, and 2FGLS overestimate the volatility of
βt when SNR is very small, and underestimate when SNR is very large. Third, as for the
median distance of the estimate from the true process (i.e., disti), the best case is when
SNR is 2.25. This phenomenon is easy to understand because both too small and too
large SNR make the estimation of βt diﬃcult since SNR far from one means the degree
of heterogeneity is quite serious. In such a situation, it is easy to imagine that OLS does
not do a good job in recovering βt; and 1FGLS is probably not a good way to implement
FGLS.

What is the eﬀect of increasing the sample size? A comparison of Tables 1 and 2
shows that the degrees of overestimating or underestimating the volatility of βt are largely
mitigated for OLS and 1FGLS when the sample size increases from 100 to 250. At the
same time, the median distances of the estimate from the true process for OLS and 1FGLS
become smaller as the sample size increases. It goes to to show that the accuracy of OLS
and 1FGLS improves with the sample size. Notably, however, such eﬀects of increased
sample size do not clearly hold for 2FGLS.

4.4 Simulation Results 2: The Eﬀects of Non-i.i.d. and Non-

Gaussian Errors

(Table 3 around here)

Table 3 demonstrates the eﬀect of non-Gaussian errors as well as stochastic volatility
and stochastic autoregressive errors. The general tendencies that appear in the Gaussian
error case (Tables 1 and 2) are preserved: both OLS and 1FGLS overestimate the volatility
of βt; yet the degree of overestimation is largely mitigated when the sample size increases;
2FGLS underestimates the volatility of βt, and increasing the sample size does not help
2FGLS improve the accuracy in the estimation of βt. Remarkably, given the value of the
autoregressive parameter ρ, there is negligible diﬀerence between the stochastic volatility
and autoregressive stochastic volatility cases.

6Throughout this simulation study, we use bold numbers to highlight the best (the smallest median

dist and the median rat closest to one) estimation method of the three (OLS, 1FGLS, and 2FGLS).

14

(Tables 4 and 5 around here)

When only the non-Gaussian error is considered, as Tables 4 and 5 show, we obtain
mostly the same results as those presented in Tables 1 and 2. Once again, SNR is key
to determining the estimated sample variance of βt relative to its true sample variance.
In other words, the degree of overestimation (underestimation) depends on SNR. Similar
to the results in Tables 1 and 2, the larger sample size generally helps the estimation by
OLS and 1FGLS in that the degree of overestimation or underestimation is largely reduced
when the sample size increases. In addition, for OLS and 1FGLS, the median distance
between true and estimated βt also becomes smaller with the sample size. However, this
tendency does not apply to 2FGLS.

(Table 6 around here)

What is the eﬀect of scholastic volatility or autoregressive volatility in the observation
equation error (εt) on our estimation? Table 6 reveals that the results arising from such
errors are similar to the small SNR cases in Tables 1 and 2. This is because the observation
ht) term. There
error (εt) has a variance larger than one due to the stochastic volatility (
is little diﬀerence between the results of the stochastic volatility case and the result of the
autoregressive stochastic volatility case.

√

4.5 Discussion: The Pile-Up Problem

From our results, the GLS-based approach does not suﬀer from the pile-up problem.
Lower SNRs often lead to overestimation of the volatility of βt, rather than its underes-
timation, especially when OLS or 1FGLS is used (Tables 1 and 2). Moreover, the degree
of overestimation of the sample variance of βt becomes more severe when the sample size
is small. This may be puzzling given the fact that OLS and ML are generally equivalent
or the fact that GLS and ML are equivalent if the errors are not i.i.d. (i.e., errors hav-
ing heteroskedasticity or autocorrelation). However, this statement is not true if FGLS
fails to deal with non-i.i.d. errors appropriately. It is likely that both OLS and 1FGLS
are unable to obtain the estimate of βt that is equivalent to ML. This is the reason the
GLS-based approach does not suﬀer from the pile-up problem.

All in all, our simulations seem to suggest that the use of 2FGLS is recommended
when the sample size is small; and OLS (and 1FGLS) does a fairly good job in recovering
the time-varying parameters when the sample size is large.

5 An Application to the TV-VAR(2) with the Interest

Rate, Inﬂation, and Unemployment

(Figures 1 through 4 around here)

A number of studies that employ TV-VAR models, including Cogley and Sargent
(2005) and Primiceri (2005), focus on recovering the structural parameters from the es-
timated reduced form. Although our focus is not to identify fundamental shocks or to

15

compute impulse responses, we present the estimated TV-VAR(2) parameter using OLS
(Figure 1), FGLSs (Figure 2 for 1FGLS and Figure 3 for 2FGLS), and the posterior mean
of the time-varying approach using the Bayesian MCMC method (Figure 4)7.

While the Bayesian MCMC posterior means are virtually time invariant, and the
estimates by 2FGLS are slightly more volatile, the estimates by OLS and 1FGLS have
much larger volatility.

Interestingly, as detailed in the online appendix, the coeﬃcients on the interest rate
vary noticeably over time, and exhibit distinct patterns in the early 1980s (dip), the
late 1990s (up), and early 2000s (down). Similar to the Bayesian posterior means, the
intercepts (three time-varying coeﬃcients) are largely stable over time.

6 Conclusion

The (non-Bayesian) regression-based or GLS-based approach for the time-varying param-
eter model is presented and assessed from theoretical and simulation aspects. Although
this approach has already been (at least partly) used by Ito et al. (2014, 2016a,b), it
is shown that there are, at least, following four advantages to the GLS-based approach.
First, this approach does not necessitate Kalman ﬁltering or smoothing, but it does pro-
duce equivalent estimates. In addition, this approach is readily applicable to a wide range
of time-varying parameter models, such as the TV-AR, TV-VAR, and TV-VEC models,
by adjusting the regression matrix accordingly. Second, it is revealed that the GLS-based
approach works reasonably well in practice in that it can estimate the time-varying pa-
rameters even with non-i.i.d. errors or non-Gaussian errors in the model. This is because
GLS can take into account generally heteroskedastic error terms. The ability to deal with
non-Gaussian errors is particularly important in empirical studies because it allows us to
consider possible abrupt changes in time-varying parameters, instead of gradual changes
that are due to Gaussian errors. One caveat is that depending on the sample size and
depending on SNR, the most appropriate method, either OLS, 1FGLS, or 2FGLS, should
be chosen. More precisely, OLS is acceptable when SNR is not very far from one or when
the sample size is not small. However, when SNR is far from one or when the sample
size is small, 2FGLS is recommended. The reason why the sample size and SNR are
important in choosing one method over the other two methods is that our 1FGLS and
2FGLS are not ideal GLS; hence, they cannot fully take care of heterogeneity arising from
our regression equation that includes both the observation equation errors and the state
equation errors. However, because we do not maximize unconditional likelihood function
with respect to the variances of the errors, and because 1FGLS and 2FGLS are not ideal
GLS, the true variances are not precisely estimated, and our GLS-based approach does
not suﬀer from the pile-up problem that often occurs with ML.

7We use the data and MATLAB codes provided by Koop and Korobilis (2010).

16

Acknowledgments

We would like to thank James Morley, Daniel Rees, Yunjong Eo, Yohei Yamamoto, Eiji
Kurozumi, and conference participants at the 91th Annual Conference of the Western
Economic Association International, First International Conference on Econometrics and
Statistics, and Macro Reading Group Workshop at the Reserve Bank of Australia for their
helpful comments and suggestions. We also acknowledge the ﬁnancial assistance provided
by the Japan Society for the Promotion of Science Grant in Aid for Scientiﬁc Research
No.26380397 (Mikio Ito), No.15K03542 (Akihiko Noda), No.15H06585 (Tatsuma Wada),
Murata Science Foundation Research Grant (Tatsuma Wada), and Okawa Foundation
Research Grant (Tatsuma Wada). All data and programs used for this paper are available
on request.

17

References

Bernanke, B. S. and Mihov, I. (1998), “Measuring Monetary Policy,” Quarterly Journal

of Economics, 113, 869–902.

Cogley, T. and Sargent, T. J. (2001), “Evolving Post-World War II U.S. Inﬂation Dynam-

ics,” NBER Macroeconomics Annual, 16, 331–373.

— (2005), “Drifts and Volatilities: Monetary Policies and Outcomes in the Post WWII

US,” Review of Economic Dynamics, 8, 262–302.

Cooley, T. F. and Prescott, E. C. (1976), “Estimation in the Presence of Stochastic Pa-

rameter Variation,” Econometrica, 44, 167–184.

Duncan, D. B. and Horn, S. D. (1972), “Linear Dynamic Recursive Estimation from the
Viewpoint of Regression Analysis,” Journal of the American Statistical Association, 67,
815–821.

Durbin, J. and Koopman, S. J. (2012), Time Series Analysis by State Space Methods,

Princeton University Press, 2nd ed.

Hamilton, J. D. (1989), “A New Approach to the Economic Analysis of Nonstationary

Time Series and the Business Cycle,” Econometrica, 57, 357–384.

Hansen, B. E. (1992), “Testing for Parameter Instability in Linear Models,” Journal of

Policy Modeling, 14, 517–533.

Harvey, A. C. (1989), Forecasting, Structural Time Series Models and the Kalman Filter,

Cambridge University Press.

Ito, M., Noda, A., and Wada, T. (2014), “International Stock Market Eﬃciency: A Non-

Bayesian Time-Varying Model Approach,” Applied Economics, 46, 2744–2754.

— (2016a), “The Evolution of Stock Market Eﬃciency in the US: A Non-Bayesian Time-

Varying Model Approach,” Applied Economics, 48, 621–635.

— (2016b),

“Time-Varying Comovement

of

Foreign Exchange Markets,”

[arXiv:1610.04334], Available at https://arxiv.org/pdf/1610.04334.pdf.

Koop, G. and Korobilis, D. (2010), “Bayesian Multivariate Time Series Methods for Em-

pirical Macroeconomics,” Foundations and Trends in Econometrics, 3, 267–358.

Koopman, S. J. (1997), “Exact Initial Kalman Filtering and Smoothing for Nonstationary
Time Series Models,” Journal of the American Statistical Association, 92, 1630–1638.

Maddala, G. S. and Kim, I. (1998), Unit Roots, Cointegration, and Structural Change,

Cambridge University Press.

Perron, P. (1989), “The Great Crash, the Oil Price Shock, and the Unit Root Hypothesis,”

Econometrica, 57, 1361–1401.

18

Perron, P. and Wada, T. (2009), “Let’s Take a Break: Trends and Cycles in US Real

GDP,” Journal of Monetary Economics, 56, 749–765.

Primiceri, G. E. (2005), “Time Varying Structural Vector Autoregressions and Monetary

Policy,” Review of Economic Studies, 72, 821–852.

Sargan, J. D. and Bhargava, A. (1983), “Maximum Likelihood Estimation of Regression
Models with First Order Moving Average Errors When the Root Lies on the Unit
Circle,” Econometrica, 51, 799–820.

Shephard, N. G. and Harvey, A. C. (1990), “On the Probability of Estimating a Deter-
ministic Component in the Local Level Model,” Journal of Time Series Analysis, 11,
339–347.

Tanaka, K. (2017), Time Series Analysis: Nonstationary and Noninvertible Distribution

Theory, Wiley-Interscience, 2nd ed.

Taylor, S. (2007), Modelling Financial Time Series, World Scientiﬁc Publishing Co Pte

Ltd, 2nd ed.

19

Table 1: Simulation Results T = 100

H

Q

True

OLS

1FGLS 2FGLS

0.0022

0.032 median m

-0.000

-0.000

-0.001

0.003

median s

0.113

0.048

0.034

0.010

median dist

0.165

0.176

0.210

SN R =

225

median rat

0.455

0.325

0.097

0.022

0.032 median m

0.000

0.008

0.007

0.003

median s

0.113

0.058

0.039

0.013

median dist

0.138

0.149

0.188

SN R =

2.25

median rat

0.552

0.369

0.120

0.22

0.032 median m

0.001

-0.006

-0.006

0.005

median s

0.113

0.159

0.107

0.039

median dist

0.157

0.133

0.127

SN R = 0.0225 median rat

1.598

1.070

0.366

1

0.032 median m

0.000

-0.006

-0.007

0.003

median s

0.113

0.318

0.325

0.115

median dist

0.272

0.280

0.141

SN R = 0.0009 median rat

3.216

3.306

1.149

102

0.032 median m

0.001

-0.006

-0.005

-0.007

median s

0.113

0.402

0.406

0.143

median dist

0.330

0.337

0.153

SN R = 0.00007 median rat

4.053

4.119

1.437

Notes: 1) The numbers in the column under “True” are computed from the data generating process described
in Section 4.1.
2) “m” “s” “dist” “rat” stand for the mean, the standard deviation, the distance from the true values, and
the ratio of the standard deviation of the estimates to that of the true values of β.
3) The bold numbers are the smallest (for median “dist”) and the closest to one (for median “rat”), indicating
the best method out of the three (OLS, 1FGLS, 2FGLS).

20

Table 2: Simulation Results T = 250

H

Q

True

OLS

1FGLS 2FGLS

0.0022

0.032 median m

-0.002

-0.002

-0.002

0.005

median s

0.174

0.136

0.115

0.029

median dist

0.147

0.164

0.337

SN R =

225

median rat

0.818

0.705

0.183

0.022

0.032 median m

-0.005

-0.003

-0.002

0.003

median s

0.173

0.143

0.116

0.031

median dist

0.125

0.142

0.313

SN R =

2.25

median rat

0.852

0.699

0.191

0.22

0.032 median m

-0.001

-0.003

-0.002

0.001

median s

0.173

0.211

0.169

0.058

median dist

0.150

0.130

0.238

SN R = 0.0225 median rat

1.301

1.016

0.351

1

0.032 median m

-0.000

-0.003

-0.003

-0.003

median s

0.173

0.327

0.326

0.112

median dist

0.237

0.238

0.192

SN R = 0.0009 median rat

2.093

2.091

0.695

102

0.032 median m

-0.000

-0.006

-0.003

-0.000

median s

0.172

0.394

0.395

0.138

median dist

0.282

0.284

0.163

SN R = 0.00007 median rat

2.559

2.564

0.870

Notes: 1) The numbers in the column under “True” are computed from the data generating process described
in Section 4.1.
2) “m” “s” “dist” “rat” stand for the mean, the standard deviation, the distance from the true values, and
the ratio of the standard deviation of the estimates to that of the true values of β.
3) The bold numbers are the smallest (for median “dist”) and the closest to one (for median “rat”), indicating
the best method out of the three (OLS, 1FGLS, 2FGLS).

21

Table 3: Stochastic Volatility, Autoregressive Stochastic Volatility, and Mixtures of Nor-

mals

T

RW/AR

True

OLS

1FGLS 2FGLS

100 RW

median m

0.003

-0.004

-0.003

-0.003

median s

0.136

0.310

0.317

0.102

median dist

median rat

0.262

0.270

0.171

2.623

2.679

0.850

AR

median m

0.003

-0.004

-0.003

-0.003

median s

0.136

0.310

0.317

0.102

median dist

median rat

0.262

0.270

0.171

2.623

2.680

0.850

250 RW

median m

-0.002

-0.003

-0.003

0.000

median s

0.204

0.327

0.327

0.103

median dist

median rat

0.221

0.223

0.264

1.733

1.734

0.542

AR

median m

0.000

-0.002

-0.002

0.004

median s

0.205

0.326

0.327

0.103

median dist

median rat

0.221

0.223

0.264

1.730

1.732

0.543

Notes: 1) The numbers in the column under “True” are computed from the data generating process:

ηit = λtζ 1

t + (1 − λt) ζ 2
t

λt ∼ i.i.d.Bernoulli (0.95)
ζ1,t ∼ N (cid:0)0, 0.032(cid:1) , ζ2,t ∼ N (cid:0)0, 0.12(cid:1)

where

and

εit = (cid:112)hi,tξt

log hi,t = ρ log hi,t−1 + et

where ρ = 1 when stochastic volatility is considered (labeled as RW), ρ = 0.9 when autoregressive stochastic
volatility is considered (labeled as AR), εit is the i-th element of εt; log hi,0 = 0, et ∼ N (cid:0)0, 0.022(cid:1), and
ξt ∼ N (0, 1).
2) “m” “s” “dist” “rat” stand for the mean, the standard deviation, the distance from the true values, and
the ratio of the standard deviation of the estimates to that of the true values of β.
3) The bold numbers are the smallest (for median “dist”) and the closest to one (for median “rat”), indicating
the best method out of the three (OLS, 1FGLS, 2FGLS).

22

Table 4: Mixtures of Normals T = 100

H

Q

True

OLS

1FGLS 2FGLS

0.0022 M ixture median m

-0.002

-0.003

-0.002

0.001

median s

0.136

0.069

0.056

0.015

median dist

median rat

0.183

0.194

0.262

0.550

0.436

0.123

0.022

median m

-0.002

0.005

0.005

0.003

median s

0.136

0.077

0.058

0.017

median dist

median rat

0.160

0.171

0.242

0.612

0.468

0.137

0.22

median m

-0.001

-0.007

-0.006

0.004

median s

0.136

0.166

0.124

0.042

median dist

median rat

0.159

0.140

0.165

1.356

1.006

0.329

1

median m

-0.001

-0.003

-0.005

-0.004

median s

0.136

0.308

0.315

0.103

median dist

median rat

0.260

0.267

0.168

2.585

2.641

0.852

102

median m

-0.001

-0.003

-0.003

-0.001

median s

0.136

0.385

0.386

0.129

median dist

median rat

0.311

0.316

0.163

3.237

3.261

1.078

Notes: 1) The numbers in the column under “True” are computed from the data generating process:

ηit = λtζ 1

t + (1 − λt) ζ 2
t

where

λt ∼ i.i.d.Bernoulli (0.95)
ζ1,t ∼ N (cid:0)0, 0.032(cid:1) , ζ2,t ∼ N (cid:0)0, 0.12(cid:1) .

2) “m” “s” “dist” “rat” stand for the mean, the standard deviation, the distance from the true values, and
the ratio of the standard deviation of the estimates to that of the true values of β.
3) The bold numbers are the smallest (for median “dist”) and the closest to one (for median “rat”), indicating
the best method out of the three (OLS, 1FGLS, 2FGLS).

23

Table 5: Mixtures of Normals T = 250

H

Q

True

OLS

1FGLS 2FGLS

0.0022 M ixture median m

-0.004

0.004

0.004

0.000

median s

0.205

0.166

0.145

0.039

median dist

median rat

0.150

0.164

0.375

0.843

0.747

0.204

0.022

median m

-0.003

0.003

0.002

-0.002

median s

0.206

0.173

0.149

0.042

median dist

median rat

0.136

0.150

0.365

0.866

0.755

0.214

0.22

median m

0.000

-0.002

-0.001

0.004

median s

0.203

0.227

0.193

0.062

median dist

median rat

0.148

0.135

0.303

1.172

0.976

0.316

1

median m

0.001

0.002

0.001

0.001

median s

0.202

0.326

0.326

0.105

median dist

median rat

0.223

0.224

0.245

1.755

1.755

0.552

102

median m

0.001

0.001

0.000

0.001

median s

0.201

0.385

0.385

0.129

median dist

median rat

0.260

0.262

0.204

2.106

2.104

0.689

Notes: 1) The numbers in the column under “True” are computed from the data generating process:

ηit = λtζ 1

t + (1 − λt) ζ 2
t

where

λt ∼ i.i.d.Bernoulli (0.95)
ζ1,t ∼ N (cid:0)0, 0.032(cid:1) , ζ2,t ∼ N (cid:0)0, 0.12(cid:1) .

2) “m” “s” “dist” “rat” stand for the mean, the standard deviation, the distance from the true values, and
the ratio of the standard deviation of the estimates to that of the true values of β.
3) The bold numbers are the smallest (for median “dist”) and the closest to one (for median “rat”), indicating
the best method out of the three (OLS, 1FGLS, 2FGLS).

24

Table 6: Stochastic Volatility and Autoregressive Stochastic Volatility T = 100

T

Q

True

OLS

1FGLS 2FGLS

100

0.032 median m

-0.002

-0.010

-0.011

-0.011

RW median s

0.113

0.317

0.324

0.113

median dist

median rat

0.272

0.279

0.141

3.218

3.298

1.150

100

0.032 median m

-0.002

-0.010

-0.010

-0.011

AR median s

0.113

0.317

0.324

0.113

median dist

median rat

0.272

0.279

0.141

3.217

3.298

1.150

250

0.032 median m

-0.003

-0.008

-0.009

-0.005

RW median s

0.174

0.327

0.328

0.113

median dist

median rat

0.236

0.238

0.182

2.084

2.093

0.703

250

0.032 median m

-0.003

-0.010

-0.010

0.004

AR median s

0.174

0.327

0.327

0.113

median dist

median rat

0.236

0.238

0.182

2.085

2.092

0.702

Notes: 1) The numbers in the column under “True” are computed from the data generating process:

εit = (cid:112)hi,tξt

log hi,t = ρ log hi,t−1 + et

where ρ = 1 when stochastic volatility is considered (labeled as RW), ρ = 0.9 when autoregressive stochastic
volatility is considered (labeled as AR), εit is the i-th element of εt; log hi,0 = 0, et ∼ N (cid:0)0, 0.022(cid:1), and
ξt ∼ N (0, 1).
2) “m” “s” “dist” “rat” stand for the mean, the standard deviation, the distance from the true values, and
the ratio of the standard deviation of the estimates to that of the true values of β.
3) The bold numbers are the smallest (for median “dist”) and the closest to one (for median “rat”), indicating
the best method out of the three (OLS, 1FGLS, 2FGLS).

25

Figures 1 through 4: The Estimated Time-Varying Parameters

Figure 1: OLS

Figure 2: 1FGLS

Figure 3: 2FGLS

Figure 4: Bayesian

26

196519701975198019851990199520002005-1-0.500.511.5196519701975198019851990199520002005-1-0.500.511.5196519701975198019851990199520002005-1-0.500.511.5196519701975198019851990199520002005-1-0.500.511.5Appendix 1: Proof of Propositions

• Proof of Proposition 1

Lemma 1 (S − T U −1V )−1 = S−1 + S−1T (U − V S−1T )−1 V S−1 provided S−1 ex-
ists.

The GLS estimate of β is

(cid:98)βGLS =


(cid:104)


(cid:104)

×

Z (cid:48)H (cid:48)−1/2 C (cid:48)−1Q(cid:48)−1/2

(cid:105)





Z (cid:48)H (cid:48)−1/2 −C (cid:48)−1Q(cid:48)−1/2

(cid:105)





−1





H −1/2Z

Q−1/2C −1


H −1/2YT



−Q−1/2b∗
0





= (cid:0)Z (cid:48)H −1Z + C (cid:48)−1Q−1C −1(cid:1)−1 (cid:0)Z (cid:48)H −1YT + C −1(cid:48)Q−1b∗
(cid:1)
= (cid:2)CQC (cid:48) − CQC (cid:48)Z (cid:48)Ω−1ZCQC (cid:48)(cid:3) (cid:0)Z (cid:48)H −1YT + C −1(cid:48)Q−1b∗
= CQC (cid:48)Z (cid:48)Ω−1YT + (cid:2)C − CQC (cid:48)Z (cid:48)Ω−1ZC(cid:3) b∗
= Cb∗

0 + CQC (cid:48)Z (cid:48)Ω−1 (YT − ZCb∗

0) .

0

0

0

(cid:1)

(30)

(31)

Here, we used Lemma,

(cid:0)Z (cid:48)H −1Z + C (cid:48)−1Q−1C −1(cid:1)−1 = CQC (cid:48) − CQC (cid:48)Z (cid:48) (H + Z ∗CQC (cid:48)Z (cid:48))−1 ZCQC (cid:48).

From (31), the conditional variance of (cid:98)βGLS is

(cid:16)

V ar

(cid:98)βGLS|YT

(cid:17)

= (cid:0)Z (cid:48)H −1Z + C (cid:48)−1Q−1C −1(cid:1)−1
= CQC (cid:48) − CQC (cid:48)Z (cid:48)Ω−1ZCQC (cid:48).

• Detailed Proof of Proposition 2

Lemma 2 If G−1 and the inverse of F = A − BG−1E exist,





A B

E G



−1





=



F −1

−F −1BG−1

−G−1EF −1 G−1 + G−1EF −1BG−1



 .

In our case,

A = I (cid:48)H −1I, B = I (cid:48)H −1Z, G = Z (cid:48)H −1Z + C (cid:48)−1Q−1C −1, E = Z (cid:48)H −1I;

and

F = I (cid:48)H −1I − I (cid:48)H −1Z (cid:0)Z (cid:48)H −1Z + C (cid:48)−1Q−1C −1(cid:1)−1 Z (cid:48)H −1I

= I (cid:48) (cid:104)

H −1−H −1Z (cid:0)Z (cid:48)H −1Z + C (cid:48)−1Q−1C −1(cid:1)−1 Z (cid:48)H −1(cid:105)

I,

27

whose inverse is
I (cid:48) (cid:104)
(cid:110)

F −1 =

H −1−H −1Z (cid:0)Z (cid:48)H −1Z + C (cid:48)−1Q−1C −1(cid:1)−1 Z (cid:48)H −1(cid:105)

I

(cid:111)−1

(cid:104)
I (cid:48) (H + ZCQC (cid:48)Z (cid:48))−1 I

=
= (cid:0)I (cid:48)Ω−1I(cid:1)−1 .

(cid:105)−1

Other useful equations are

G−1 = (cid:0)Z (cid:48)H −1Z + C (cid:48)−1Q−1C −1(cid:1)−1

= CQC (cid:48) − CQC (cid:48)Z (cid:48) (H + ZCQC (cid:48)Z (cid:48))−1 ZCQC (cid:48)
= CQC (cid:48) − CQC (cid:48)Z (cid:48)Ω−1ZCQC (cid:48);

Ω−1 = (H + ZCQC (cid:48)Z (cid:48))−1

= H −1−H −1Z (cid:0)Z (cid:48)H −1Z + C (cid:48)−1Q−1C −1(cid:1)−1 Z (cid:48)H −1
= H −1−H −1ZG−1Z (cid:48)H −1.

Then, for (18), we arrive at

(cid:98)v = (cid:0)F −1I (cid:48)H −1 − F −1BG−1Z (cid:48)H −1(cid:1)
(cid:125)

(cid:124)

(cid:123)(cid:122)
1

YT − F −1BG−1C −1(cid:48)Q−1
(cid:125)

(cid:124)

b∗
0

(cid:123)(cid:122)
2

(cid:98)β = (cid:2)−G−1EF −1I (cid:48)H −1 + (cid:0)G−1 + G−1EF −1BG−1(cid:1) Z (cid:48)H −1(cid:3)
(cid:125)

YT

(cid:124)
+(cid:0)G−1 + G−1EF −1BG−1(cid:1) C −1(cid:48)Q−1
(cid:125)
(cid:123)(cid:122)
4

(cid:123)(cid:122)
3

(cid:124)

b∗
0.

(32)

(33)

(34)

1.

F −1I (cid:48)H −1 − F −1BG−1Z (cid:48)H −1 = (cid:0)I (cid:48)Ω−1I(cid:1)−1 I (cid:48)H −1

× (cid:0)I − ZCQC (cid:48)Z (cid:48)H −1 + ZCQC (cid:48)Z (cid:48)Ω−1ZCQC (cid:48)Z (cid:48)H −1(cid:1)

= (cid:0)I (cid:48)Ω−1I(cid:1)−1 I (cid:48)H −1 (cid:2)I − (Ω − H) H −1 + (Ω − H) Ω−1 (Ω − H) H −1(cid:3)
= (cid:0)I (cid:48)Ω−1I(cid:1)−1 I (cid:48)H −1 (cid:0)I − ΩH −1 + I + ΩH −1 − I − I + HΩ−1(cid:1)
= (cid:0)I (cid:48)Ω−1I(cid:1)−1 I (cid:48)Ω−1.

2.
F −1BG−1C −1(cid:48)Q−1 = (cid:0)I (cid:48)Ω−1I(cid:1)−1 I (cid:48)H −1Z (cid:0)CQC (cid:48) − CQC (cid:48)Z (cid:48)Ω−1ZCQC (cid:48)(cid:1) C −1(cid:48)Q−1
= (cid:0)I (cid:48)Ω−1I(cid:1)−1 I (cid:48)H −1Z (cid:0)C − CQC (cid:48)Z (cid:48)Ω−1ZC(cid:1)
= (cid:0)I (cid:48)Ω−1I(cid:1)−1 I (cid:48)H −1 (cid:0)ZC − ZCQC (cid:48)Z (cid:48)Ω−1ZC(cid:1)
= (cid:0)I (cid:48)Ω−1I(cid:1)−1 I (cid:48)H −1 (cid:0)I − ZCQC (cid:48)Z (cid:48)Ω−1(cid:1) ZC
= (cid:0)I (cid:48)Ω−1I(cid:1)−1 I (cid:48)H −1 (cid:0)I − (Ω − H) Ω−1(cid:1) ZC
= (cid:0)I (cid:48)Ω−1I(cid:1)−1 I (cid:48)Ω−1ZC.

28

Therefore,

(cid:98)v = (cid:0)I (cid:48)Ω−1I(cid:1)−1 I (cid:48)Ω−1YT − (cid:0)I (cid:48)Ω−1I(cid:1)−1 I (cid:48)Ω−1ZCb∗

0

= (cid:0)I (cid:48)Ω−1I(cid:1)−1 I (cid:48)Ω−1 (YT − ZCb∗
0)

−G−1EF −1I (cid:48)H −1 + (cid:0)G−1 + G−1EF −1BG−1(cid:1) Z (cid:48)H −1

(from 32)

= −G−1EF −1 (cid:0)I (cid:48)H −1 − BG−1Z (cid:48)H −1(cid:1) + G−1Z (cid:48)H −1
= −G−1EF −1I (cid:48) (cid:0)H −1 − H −1ZG−1Z (cid:48)H −1(cid:1) + G−1Z (cid:48)H −1
= −G−1EF −1I (cid:48)Ω−1 + G−1Z (cid:48)H −1
= −G−1Z (cid:48)H −1IF −1I (cid:48)Ω−1 + G−1Z (cid:48)H −1
= −G−1Z (cid:48)H −1 (cid:0)I − IF −1I (cid:48)Ω−1(cid:1)
= − (cid:0)CQC (cid:48) − CQC (cid:48)Z (cid:48)Ω−1ZCQC (cid:48)(cid:1) Z (cid:48)H −1 (cid:0)I − IF −1I (cid:48)Ω−1(cid:1)
= −CQC (cid:48)Z (cid:48)H −1 (cid:0)I − IF −1I (cid:48)Ω−1(cid:1) + CQC (cid:48)Z (cid:48)Ω−1 (Ω − H) H −1 (cid:0)I − IF −1I (cid:48)Ω−1(cid:1)
= CQC (cid:48)Z (cid:48)Ω−1 (cid:0)I − IF −1I (cid:48)Ω−1(cid:1)
= CQC (cid:48)Z (cid:48)Ω−1 − CQC (cid:48)Z (cid:48)Ω−1I (cid:0)I (cid:48)Ω−1I(cid:1)−1 I (cid:48)Ω−1
= CQC (cid:48)Z (cid:48)Ω−1 (cid:104)

I − I (cid:0)I (cid:48)Ω−1I(cid:1)−1 I (cid:48)Ω−1(cid:105)

3.

4.

(cid:0)G−1 + G−1EF −1BG−1(cid:1) C −1(cid:48)Q−1
= G−1C −1(cid:48)Q−1 + G−1EF −1BG−1C −1(cid:48)Q−1
= (cid:0)CQC (cid:48) − CQC (cid:48)Z (cid:48)Ω−1ZCQC (cid:48)(cid:1) C −1(cid:48)Q−1 + G−1E (cid:0)I (cid:48)Ω−1I(cid:1)−1 I (cid:48)Ω−1ZC (from 2)
= C − CQC (cid:48)Z (cid:48)Ω−1ZC

+ (cid:0)CQC (cid:48) − CQC (cid:48)Z (cid:48)Ω−1ZCQC (cid:48)(cid:1) Z (cid:48)H −1I (cid:0)I (cid:48)Ω−1I(cid:1)−1 I (cid:48)Ω−1ZC

= C − CQC (cid:48)Z (cid:48)Ω−1ZC

+CQC (cid:48) (cid:0)Z (cid:48)H −1 − Z (cid:48)Ω−1ZCQC (cid:48)Z (cid:48)H −1(cid:1) I (cid:0)I (cid:48)Ω−1I(cid:1)−1 I (cid:48)Ω−1ZC

= C − CQC (cid:48)Z (cid:48)Ω−1ZC

+CQC (cid:48) (cid:2)Z (cid:48)H −1 − Z (cid:48)Ω−1 (Ω − H) H −1(cid:3) I (cid:0)I (cid:48)Ω−1I(cid:1)−1 I (cid:48)Ω−1ZC

= C − CQC (cid:48)Z (cid:48)Ω−1ZC + CQC (cid:48)Z (cid:48)Ω−1I (cid:0)I (cid:48)Ω−1I(cid:1)−1 I (cid:48)Ω−1ZC
(cid:105)
I − CQC (cid:48)Z (cid:48)Ω−1Z + CQC (cid:48)Z (cid:48)Ω−1I (cid:0)I (cid:48)Ω−1I(cid:1)−1 I (cid:48)Ω−1Z

=

(cid:104)

C

Therefore,

I − I (cid:0)I (cid:48)Ω−1I(cid:1)−1 I (cid:48)Ω−1(cid:105)

(cid:98)β = CQC (cid:48)Z (cid:48)Ω−1 (cid:104)
(cid:104)
I − CQC (cid:48)Z (cid:48)Ω−1Z + CQC (cid:48)Z (cid:48)Ω−1I (cid:0)I (cid:48)Ω−1I(cid:1)−1 I (cid:48)Ω−1Z
0 + CQC (cid:48)Z (cid:48)Ω−1 (YT − I(cid:98)v − ZCb∗

= Cb∗

0) .

YT

+

(cid:105)

Cb∗
0

(35)

29

• The means squared error matrix

V ar (β|YT ) = CQC (cid:48) − CQC (cid:48)Z (cid:48)Ω−1ZCQC (cid:48)

From (18) and Lemma, we can show

V ar

(cid:16)

(cid:17)
(cid:98)β

= G−1 + G−1EF −1BG−1
= (cid:0)CQC (cid:48) − CQC (cid:48)Z (cid:48)Ω−1ZCQC (cid:48)(cid:1) + G−1Z (cid:48)H −1I (cid:0)I (cid:48)Ω−1I(cid:1)−1 I (cid:48)H −1ZG−1
= CQC (cid:48) − CQC (cid:48)Z (cid:48)Ω−1ZCQC

+ (cid:0)CQC (cid:48) − CQC (cid:48)Z (cid:48)Ω−1ZCQC (cid:48)(cid:1) Z (cid:48)H −1I (cid:0)I (cid:48)Ω−1I(cid:1)−1 I (cid:48)H −1Z
× (cid:0)CQC (cid:48) − CQC (cid:48)Z (cid:48)Ω−1ZCQC (cid:48)(cid:1)

= CQC (cid:48) − CQC (cid:48)Z (cid:48)Ω−1ZCQC

+ (cid:0)CQC (cid:48)Z (cid:48)H −1 − CQC (cid:48)Z (cid:48)Ω−1ZCQC (cid:48)Z (cid:48)H −1(cid:1) I (cid:0)I (cid:48)Ω−1I(cid:1)−1 I (cid:48)
× (cid:0)H −1ZCQC (cid:48) − H −1ZCQC (cid:48)Z (cid:48)Ω−1ZCQC (cid:48)(cid:1)

= CQC (cid:48) − CQC (cid:48)Z (cid:48)Ω−1ZCQC

+ (cid:0)CQC (cid:48)Z (cid:48)H −1 − CQC (cid:48)Z (cid:48)Ω−1 (Ω − H) H −1(cid:1) I (cid:0)I (cid:48)Ω−1I(cid:1)−1 I (cid:48)
× (cid:0)H −1ZCQC (cid:48) − H −1 (Ω − H)(cid:48) Ω−1ZCQC (cid:48)(cid:1)

= CQC (cid:48) − CQC (cid:48)Z (cid:48)Ω−1ZCQC

+CQC (cid:48)Z (cid:48)Ω−1I (cid:0)I (cid:48)Ω−1I(cid:1)−1 I (cid:48)Ω−1ZCQC (cid:48).

• Note also that

−F −1BG−1 = − (cid:0)I (cid:48)Ω−1I(cid:1)−1 I (cid:48)H −1Z (cid:0)CQC (cid:48) − CQC (cid:48)Z (cid:48)Ω−1ZCQC (cid:48)(cid:1)

= − (cid:0)I (cid:48)Ω−1I(cid:1)−1 I (cid:48) (cid:0)H −1ZCQC (cid:48) − H −1ZCQC (cid:48)Z (cid:48)Ω−1ZCQC (cid:48)(cid:1)
= − (cid:0)I (cid:48)Ω−1I(cid:1)−1 I (cid:48) (cid:0)H −1ZCQC (cid:48) − H −1 (Ω − H) Ω−1ZCQC (cid:48)(cid:1)
= − (cid:0)I (cid:48)Ω−1I(cid:1)−1 I (cid:48)Ω−1ZCQC (cid:48)

−G−1EF −1 = − (cid:0)CQC (cid:48) − CQC (cid:48)Z (cid:48)Ω−1ZCQC (cid:48)(cid:1) Z (cid:48)H −1I (cid:0)I (cid:48)Ω−1I(cid:1)−1

= − (cid:0)CQC (cid:48)Z (cid:48)H −1 − CQC (cid:48)Z (cid:48)Ω−1ZCQC (cid:48)Z (cid:48)H −1(cid:1) I (cid:0)I (cid:48)Ω−1I(cid:1)−1
= − (cid:0)CQC (cid:48)Z (cid:48)H −1 − CQC (cid:48)Z (cid:48)Ω−1 (Ω − H) H −1(cid:1) I (cid:0)I (cid:48)Ω−1I(cid:1)−1
= −CQC (cid:48)Z (cid:48)Ω−1I (cid:0)I (cid:48)Ω−1I(cid:1)−1

• Therefore,

V ar (β|YT ) = V ar

(cid:16)

(cid:17)
(cid:98)β

− Cov

(cid:16)

(cid:17)
(cid:98)β, (cid:98)v

V ar ((cid:98)v)−1 Cov

(cid:16)

(cid:17)(cid:48)

(cid:98)β, (cid:98)v

.

30

Appendix 2: TV-VAR(2) with Time-Varying Intercepts

VAR(2) Case: p = 2 (i.e., 2 lags) and k = 3 (i.e., 3 variables)

To make the matrix Z, ﬁrst deﬁne

Zt = (cid:0)(cid:2)1, y(cid:48)
(cid:124)
= (cid:0)(cid:2)1, y(cid:48)

t−2

t−1, y(cid:48)
(cid:123)(cid:122)
k×(pk+1)k
t−1, y(cid:48)

t−2

(cid:3) ⊗ Ik

(cid:3) ⊗ Ik

(cid:1)

(cid:125)
(cid:1) .

Then,

Z
(cid:124)(cid:123)(cid:122)(cid:125)
k(T −p)×(pk+1)k(T −p)

=

=





















Z3

0

Z4

. . .











0

ZT

[1, y(cid:48)

2, y(cid:48)

1] ⊗ Ik

[1, y(cid:48)

3, y(cid:48)

2] ⊗ Ik

0

= z ⊗ Ik

where

0

. . .

(cid:2)1, y(cid:48)

T −1, y(cid:48)

T −2











(cid:3) ⊗ Ik

z
(cid:124)(cid:123)(cid:122)(cid:125)
(T −p)×(kp+1)(T −p)

=











For the regression:

[1, y(cid:48)

2, y(cid:48)
1]

0

[1, y(cid:48)

3, y(cid:48)
2]

. . .











.

(cid:3)

(cid:2)1, y(cid:48)

T −1, y(cid:48)

T −2





YT











=

−b∗
0
(cid:123)(cid:122)
k(T −p)(kp+2)×1

(cid:124)

(cid:125)





−C −1
(cid:123)(cid:122)
k(T −p)(kp+2)×(kp+1)k(T −p)

(cid:124)

(cid:125)

β∗
(cid:124)(cid:123)(cid:122)(cid:125)
(T −p)k(kp+1)×1

+





ε



,

η
(cid:124) (cid:123)(cid:122) (cid:125)
k(T −p)(kp+2)×1

0

Z

one needs to deﬁne

31

X =





Z

−C −1



 .

Then,

X (cid:48)X
(cid:124) (cid:123)(cid:122) (cid:125)
(kp+1)k(T −p)×(kp+1)k(T −p)

(cid:104)

=

Z (cid:48) −C −1(cid:48)

(cid:105)









Z

−C −1

= (cid:2)Z (cid:48)Z + C −1(cid:48)C −1(cid:3) ,

I 0 · · ·

1 0 · · ·

0
...
0

1

⊗I(kp+1)k = c⊗I(kp+1)k.











(cid:125)

1 1
...
...
1 1

. . .

1
(cid:123)(cid:122)
(T −p)×(T −p)

where

C
(cid:124)(cid:123)(cid:122)(cid:125)
(kp+1)k(T −p)×(kp+1)k(T −p)

=











I
...
I

Here,

c
(cid:124)(cid:123)(cid:122)(cid:125)
(T −p)×(T −p)

=











1 0 · · ·

1 1
...
...
1 1

. . .

1

I
...
I

0
...
0

1

. . .

I











.











0
...
0

I

=











(cid:124)

32

The rest of the matrices needed for GLS are:

=

YT
(cid:124)(cid:123)(cid:122)(cid:125)
(T −p)k×1

Z
(cid:124)(cid:123)(cid:122)(cid:125)
k(T −p)×(pk+1)k(T −p)

=

β
(cid:124)(cid:123)(cid:122)(cid:125)
(T −p)k(kp+1)×1

=

η
(cid:124)(cid:123)(cid:122)(cid:125)
(T −p)k(kp+1)×1

=

C
(cid:124)(cid:123)(cid:122)(cid:125)
(kp+1)kp(T −p)×(kp+1)k(T −p)

=

Q
(cid:124)(cid:123)(cid:122)(cid:125)
(T −p)k(kp+1)×(T −p)k(kp+1)

=

H
(cid:124)(cid:123)(cid:122)(cid:125)
(T −p)k×(T −p)k

=







































































0

ZT












,











,

εp+1

εp+2
...
εT











,

yp+1

yp+2
...
yT

Zp+1

Zp+2

. . .

0

βp+1

βp+2
...
βT

ηp+1

ηp+2
...
ηT





















,

ε
(cid:124)(cid:123)(cid:122)(cid:125)
(T −p)k×1

=









,











,

0
...
0

I

I 0 · · ·

. . .

I

I
...
I

I
...
I

Qp+1

Qp+2

0

QT

,











0











,

Hp+2

. . .

HT

0

Hp+1

0

33

b∗
0
(cid:124)(cid:123)(cid:122)(cid:125)
(T −p)k(kp+1)×1

=





















b0

0
...
0

, P ∗

0 =











P0 0

0

0
...
0

0

. . .

0 · · ·

0











.

34

