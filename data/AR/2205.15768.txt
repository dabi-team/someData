SAMURAI:ShapeAndMaterialfromUnconstrainedReal-worldArbitraryImagecollectionsMarkBoss†UniversityofTübingenAndreasEngelhardt†UniversityofTübingenAbhishekKarGoogleResearchYuanzhenLiGoogleResearchDeqingSunGoogleResearchJonathanT.BarronGoogleResearchHendrikP.A.LenschUniversityofTübingenVarunJampaniGoogleResearchAbstractInverserenderingofanobjectunderentirelyunknowncaptureconditionsisafun-damentalchallengeincomputervisionandgraphics.NeuralapproachessuchasNeRFhaveachievedphotorealisticresultsonnovelviewsynthesis,buttheyre-quireknowncameraposes.Solvingthisproblemwithunknowncameraposesishighlychallengingasitrequiresjointoptimizationovershape,radiance,andpose.Thisproblemisexacerbatedwhentheinputimagesarecapturedinthewildwithvaryingbackgroundsandilluminations.Standardposeestimationtech-niquesfailinsuchimagecollectionsinthewildduetoveryfewestimatedcor-respondencesacrossimages.Furthermore,NeRFcannotrelightasceneunderanyillumination,asitoperatesonradiance(theproductofreﬂectanceandil-lumination).Weproposeajointoptimizationframeworktoestimatetheshape,BRDF,andper-imagecameraposeandillumination.Ourmethodworksonin-the-wildonlineimagecollectionsofanobjectandproducesrelightable3Dassetsforseveraluse-casessuchasAR/VR.Toourknowledge,ourmethodistheﬁrsttotacklethisseverelyunconstrainedtaskwithminimaluserinteraction.Projectpage:https://markboss.me/publication/2022-samurai/1IntroductionCapturinghigh-quality3Dshapesandmaterialsofreal-worldobjectsisessentialformanygraphicsapplicationsinAR,VR,games,movies,etc.Usingactivemulti-viewobjectcapturesetupscanpro-videhigh-quality3Dassets[6,43]butcannotscaletoalarge-scalesetofobjectsthatarepresentintheworld.Bycontrast,imagecollectionsprovidedbyimagesearchresultsorproductreviewimagesexistfornearlyeveryobject.Inthiswork,weproposeacategory-agnostictechniquetoestimatethe3DshapeandmaterialpropertiesofobjectsfromsuchInternetimagecollections.Estimating3DshapesandmaterialsfromInternetobjectimagecollectionsposesseveralchallengesastheimagesarehighlyunconstrainedwithvaryingbackgrounds,illuminations,andcameraintrinsics.Fig.1(left)showsasampleimagecollectionofanobjectwhichformstheinputtoourtechnique.Concretely,weestimatethe3DshapeandBRDFmaterialproperties[16]whilealsoestimatingper-imageillumination,cameraposes,andintrinsics.Severalcontemporaryworksonshapeandmaterialestimation[6,11,12,52,62]assumeconstantcameraintrinsics,near-perfectsegmentationmasksaswellasalmost-correctcameraposesgivenbyCOLMAP[49,50].However,itistedioustoannotateobjectmasksintheinputimagesmanually.WealsoobservethatCOLMAPoftenfailsduetoinsufﬁcientcorrespondencesinreal-worldimagecollectionswithhighlyvaryingilluminationsandbackgroundsevenwhenweconstrainthecorrespondencestoliewithinobjectmasks.Instead,weusearoughquadrant-basedposeinitialization,e.g.(Front,Above,Right),(Front,Below,Left)etc.,asinNeRS[60],whichusuallytakesonlyafewminutesofannotationtimeperimagecollection.†WorkdoneduringaStudentResearcherpositionatGoogle.arXiv:2205.15768v1  [cs.CV]  31 May 2022......Shape+BRDFCameraPosesIn-the-wildImagesARGameInsertionMaterialEditingNeuralFieldOptimizationTargetsPer-ImageIlluminationApplicationsFigure1:SampleSAMURAIoutputsandapplications.Sampleinputcollectionandtheout-putsonachallengingreal-worldunconstrainedimagecollection.WeextractmesheswithmaterialpropertiesfromthelearnedvolumesenablingseveralapplicationsinAR/VR,materialeditingetc.WebaseourtechniqueontherecentNeural-PIL[12]methodthatproposestolearnilluminationpri-orsalongwithanovelpre-integrationilluminationnetworkforestimatinganeuralvolumewith3Dshape,BRDF,andper-imageillumination.Neural-PIL[12]assumesperfectcameraposesandthesamecameraintrinsicsacrossimages.GiventhattraditionalcameraposeestimationslikeCOLMAPmayfailonin-the-wildimages,weproposeSAMURAItojointlyoptimizecameraposesaswellasintrinsicswithacarefullydesignedoptimizationprotocol(Fig.1).Furthermore,Neural-PILrequiresperfectobjectmasks,whereasweleverageautomaticallyestimatedobjectmasksanddealwithnoisymasksusingaposteriorscalingloss.SomekeydistinguishingfeaturesofSAMURAIinclude:•Flexiblecameraparametrizationforvaryingdistances.StandardtechniquessuchasNeRF[40]assumeﬁxednear/farclippingplaneswithequidistantcamerastotheobject.Incontrast,wedeﬁnetheneuralvolumeinglobalcoordinatesandproposetolearnclippingplanesperimage.•Cameramultiplexoptimization.Optimizingasinglecameraperimageispronetogettingstuckinlocalminima.Weproposeusingamultiplexcamerawhereweoptimizeseveralcameraposesperimageandthenphaseouttheincorrectposesthroughouttheoptimization.Althoughcameramultiplexesarepreviouslyusedinmeshoptimization[21],optimizingcameramultiplexwithneuralvolumesischallengingduetoinefﬁcientray-basedneuralvolumerendering.•Posteriorscalingofinputimages.Asdifferentinputimageshavedifferentnoisecharacteristics(e.g.,noisymasks),someimageswouldbemoreusefulfortheoptimization.Weproposetouseposteriorscalingofinputimageswhichweighstheinﬂuenceofdifferentimagesontheoptimiza-tion.•Meshextraction.WeextractexplicitmesheswithBRDFtexturefromthelearnedneuralvolumemakingtheresulting3Dmodelsreadilyusableinexistinggraphicsengines.WeobservethatexistingrelateddatasetssuchasNeRD-dataset[11]donotcapturethevariationspresentinin-the-wildimagecollections.Forinstance,NeRD-datasetimageshavenon-varyingback-groundmakingiteasierforCOLMAPtowork.Inaddition,theilluminationvariationsaremoredrasticininternetimagescapturedbydifferentpeople/camerasandatdifferenttimes.Toevaluatethepracticalin-the-wildsetting,wecollectedimagecollectionswith8objectsinwhicheachimageiscapturedunderuniquebackgroundandilluminationconditions.Inaddition,wealsovarythecam-erasusedforcapturingtheimages.Experimentsonournewdatasetandexistingdatasetsdemon-stratebetterviewsynthesisandrelightingresultswithSAMURAIcomparedtoexistingworks.Inaddition,explicitmeshextractionallowsforseamlessuseoflearned3Dassetsingraphicsapplica-tionssuchasobjectinsertioninARorgamesandmaterialeditingetc.Fig.1(right)showssomesampleapplicationresultswith3DassetsestimatedusingSAMURAI.2RelatedworksNeuralﬁeldsencodespatialinformationintheMLPnetworkweights,andwecanretrievetheinformationbysimplyqueryingthecoordinates[15,39,45,53].WiththeseMLPs,wecanstorealphaordensityvaluesandthenexplicitlyrenderthevolumeusingraymarching[36].RecentworkssuchasNeRF[40]leveragethisneuralvolumerenderingtoachievephoto-realisticviewsynthesisresultswithview-dependentappearancevariations.Rapidresearchinneuralﬁeldsfollowed,which2alternatedthesurfacerepresentations[44,55],providedgeneralimprovementstothemethod[4,54],reducedthelongtrainingtimes[14,35,41,48,56]andinferencetimes[11,22,26,35,41,59],enabledextractionof3Dgeometryandmaterials[11,42,60],addedgeneralizationcapabilities[13,56,64]orenabledrelightingofscenes[5,11,12,27,37,52,62,63].However,mostmethodsrelyonCOLMAPposes,whichcanfailincomplexsettings,suchasvaryingilluminationandlocations.Jointcameraandshapeestimationisahighlyambiguoustask.Anaccurateshapereconstructionisonlypossiblewithaccurateposesandviceversa.Oftentechniquesrelyoncorrespondencesacrossimagestoestimatecameraposes[49,50].Recently,severalmethodscombinedcameracalibrationwithajointneuralvolumetraining.Jeongetal.[24](SCNeRF)relyoncorrespondences,andBARF[34]proposesacoarsetoﬁneoptimizationusingavaryingnumberofFourierfrequenciesandrequiresroughcameraposesandNeRF--[57]requirestrainingtheneuralvolumetwicewhilekeepingthepreviouscameraparameteroptimization.GNeRF[38]proposestouseadiscriminatoronrandomlysampledviewstolearnaposeestimationnetworkonsynthesizedviewsjointly.Overtimetheposeestimationnetworkcanestimatetherealcameraposes,whichcanthenbeusedforthefullneuralvolumetraining.NeRS[60]deformsaspheretoaspeciﬁcshapeusingcoordinate-basedMLPsandconvertsthedeformationﬁeldtoamesh;whilealsooptimizingcameraposesandsingleillumination.Comparedtopreviouswork,ourmethoddoesnotrelyoncorrespondences(vs.SCNeRF),whichmightbehardtoobtaininvaryingilluminations,canhaveextremelycoarseposesduetothecameramultiplexing(vs.BARF),worksinmultipleIllumination(vs.allpriorart),doesnotrequiretrainingtwice(vs.NeRF--)oraGAN-styletraining(vs.GNeRF).BRDFandilluminationestimationisachallengingambiguousresearchproblem.Oneneedscon-trolledlaboratorycapturesetupsforhigh-qualityBRDFcapture[3,9,28–30].Casualestimationenableson-sitematerialacquisitionwithsimplecamerasandaco-locatedcameraﬂash.Thesetech-niquesoftenconstraintheproblemtoplanarsurfaceswitheitherasingleshot[1,8,17,23,32,47],few-shot[1]ormulti-shot[2,9,18–20]captures.ThiscasualcapturesetupcanalsobeextendedtoajointBRDFandshapereconstruction[5–7,10,25,43,47,61]orentirescenes[33,51].Mostofthesemethodsrequireaknownactiveilluminationlikeaco-locatedﬂash.RecoveringaBRDFunderunknownpassiveilluminationissigniﬁcantlymorechallengingandambiguousasitrequiresdisentanglingtheBRDFfromtheillumination.Oftenthespecularparameterisconstrainedtobenon-spatiallyvaryingoromitted[31,58,62].Recently,neuralﬁeld-baseddecompositionachieveddecompositionofscenesundervaryingillumination[11,12]orﬁxedillumination[62],butrequireknown,near-perfectcameraposes.Thiscanfailonchallengingdatasets,andourmethodenablesthedecompositionofthesein-the-wilddatasets.3MethodProblemsetup.TheinputisacollectionofqobjectimagesCj∈Rsj×3;j∈{1,...,q}capturedwithdifferentbackgrounds,camerasandilluminations;andcanalsohavevaryingresolutions.WedenotethevalueofspeciﬁcpixelasCs.Inaddition,weroughlyannotatecameraposequadrantswith3simplebinaryquestions:Leftvs.Right,Abovevs.Below,andFrontvs.Back.Weautomat-icallyestimateforegroundsegmentationmasksMj∈{0,1}s×1usingU2-Net[46],whichcanbeimperfect.Giventhese,wejointlyoptimizea3DneuralvolumewithshapeandBRDFmaterialinformationalongwithper-imageillumination,cameraposes,andintrinsics.Thispracticalcapturesetupallowstheconversionofmost2Dimagecollectionsintoa3Drepresentationwithlittlemanualwork.Theroughposequadrantannotationtakesaboutafew(3-5)minutesforatypical80imagecollection.Ateachpointx∈R3inthe3DneuralvolumeV,weestimatetheBRDFparametersfortheCook-Torrancemodel[16]b∈R5(basecolorbc∈R3,metallicbm∈R1,roughnessbr∈R),unit-lengthsurfacenormaln∈R3andvolumedensityσ∈R.Wealsoestimatethelatentper-imageilluminationvectorszlj∈R128;j∈{1,...,q}usedinNeural-PIL[12].Wealsoestimateper-imagecameraposesandintrinsics,whichwerepresentusinga‘look-at’parameterizationthatweexplainlater.Next,weprovideabriefoverviewofprerequisites:NeRF[40]andNeural-PIL[12].BriefoverviewofNeRF[40].NeRF[40]modelsaneuralvolumefornovelviewsynthesiswithtwoMulti-Layer-Perceptrons(MLP).TheMLPstake3Dlocationx∈R3andviewdirectiond∈R3asinputandoutputsaview-dependentoutputcolorc∈R3andvolumedensityσ∈R.Theseoutputcolorsforatargetviewarecomputedbycastingacamerarayr(t)=o+tdintothevolume,withrayorigino∈R3andviewdirectiond.Theﬁnalcoloristhenapproximatedvianumericalquadratureoftheintegral:ˆc(r)=RtftnT(t)σ(t)c(t)dtwithT(t)=exp(−Rttnσ(t)dt),usingthenearandfarboundsoftheraytnandtfrespectively[40].TheﬁrstMLPistrainedtolearnacoarse3PositionsxtCsjDirectiondAnnealedFourierIntrinsicsˆfjExtrinsicsrj,uj,tjMLPMLP...MLPReﬂectanceFieldBRDFsbtDensitiesσtIlluminationzjRPIL-RendererColorˆcj:OptimizablePrametersFigure2:Overview.Wejointlyoptimizetheintrinsic(ˆfj)andextrinsiccamera(rj,uj,tj)param-etersalongsidetheshape(σ)andBRDF(b)inaReﬂectanceFieldandper-imageillumination(zj).TheshapeisencodedinthedensityσandalsousedtointegrateallBRDFsalongarayindirectiond.ThecomposedBRDFisthenrenderedusingNeural-PIL[12]inadeferredrendering-style.representationbysamplingthevolumeinaﬁxedsamplingpatternalongeachray.AﬁnersamplingpatterniscreatedusingthecoarsedensitydistributioninthecoarseMLP,placingmoresamplesonhigh-densityareas.ThesecondMLPisthentrainedontheﬁneandcoarsesamplingcombined.BriefoverviewofNeural-PIL[12].SimilartoNeRF[40],Neural-PIL[12]alsousestwoMLPstolearnaneuralvolume.TheﬁrstMLPissimilartoNeRFwithanadditionalGLO(generativelatentoptimization)embeddingthatmodelsthechangesinappearances(duetodifferentillumi-nations)acrossimages.InthesecondMLP,breakingwithNeRF,Neural-PILpredictsnotonlyaview-dependentoutputcolorbutalsoBRDFparametersateach3Dlocation.ThesecondMLPtakes3DlocationasinputandoutputsvolumedensityandBRDFparameters(diffuse,specular,rough-ness,andnormals).UnlikeNeRF,thissecondMLPdoesnottaketheviewdirectionasinputbutcanmodelview-dependentandrelightingeffectsintherenderingduetoexplicitBRDFdecomposi-tion.AkeydistinguishingfactorofNeural-PIListheuseoflatentilluminationembeddingsandaspecializedilluminationpre-integration(PIL)networkforfastrendering,whichwerefertoas‘PILrendering’.Moreconcretely,Neural-PILoptimizesper-imageilluminationembeddingzljtomodelimage-speciﬁcillumination.TherenderedoutputcolorˆcisequivalenttoNeRF’soutputc,butduetotheexplicitBRDFdecompositionandilluminationmodeling,itenablesrelighting.3.1SAMURAI-Jointoptimizationofshape,BRDF,cameras,andilluminationsThemainlimitationsofNeural-PILincludetheassumptionofnear-perfectcameraposesandtheavailabilityofperfectobjectsegmentationmasks.WeobservethatCOLMAPeitherproducesin-correctposesorcompletelyfailsduetoaninsufﬁcientnumberofcorrespondencesacrossimageswhenthebackgroundsandilluminationsarehighlyvaryingacrosstheimagecollection.Inaddition,cameraintrinsicscouldvaryacrossimagecollections,andtheautomaticallyestimatedobjectmaskscouldalsobenoisy.Weproposeatechnique(werefertoas‘SAMURAI’)forjointoptimizationof3Dshape,BRDF,per-imagecameraparameters,andilluminationsforagivenin-the-wildimagecol-lection.Thisisahighlyunder-constrainedandchallengingoptimizationproblemwhenonlyimagecollectionsandroughcameraposequadrantsaregivenasinput.Weaddressthishighlychallengingproblemwithacarefullydesignedcameraparameterizationandoptimizationschemes.Architectureoverview.Ahigh-leveloverviewofSAMURAIarchitectureisshowninFig.2,whichmostlyfollowsthearchitectureofNeural-PIL[12]andNeRD[11].However,wedonotuseacoarsenetworkforefﬁciencyreasonsandonlyusetheﬁneordecompositionnetworkwithanMLPwith8layersof128features.Overallwesample128pointsalongtherayinﬁxedsteps.Alayerwith1featureoutputfollowsthebasenetworkforthedensity,andanMLPwithahiddenlayerfortheviewandappearanceconditionedradiance.WealsoleverageaBRDFdecodersimilartoNeRD[11],whichﬁrstcompressesthefeatureoutputofthemainnetworkto16featuresandexpandsthemagaintotheBRDF(basecolor,metallic,roughness).Weencouragesparsityintheembeddingspaceusing:LDecSparsity=1/NPNi|ei|,anL1-LossontheBRDFembeddingeandasmoothnesslossLDecSmooth=1/NPNi|f(θ;ei)−f(θ;ei+ε)|,whereNdenotesthenumberofrandomrays,f(θ)theBRDFdecoderwiththeweightsθandεisnormaldistributedGaussiannoisewithastandarddeviationof0.01.SimilartoNeRD,wealsopredictaregulardirection-dependentradiance˜cintheearlystagesofthetraining.Thisismainlyusedforstabilizationintheearlystages.Asthisdirectcolorpredictionisonlyusedintheearlystages,weomitteditforclarityinFig.2.Inspired4VolumeBoundvFarBoundNearBoundOriginLook-atrjRayCameraMultiplexFigure3:RayparametrizationandCameraMultiplex.(Left)Ourrayboundsaredeﬁnedbyaworldspacesphere.Thedistancefromtheorigintothenearandfarpointsofthespheredeﬁnethesamplingrange.Bydeﬁningaglob-allyconsistentsamplingrangeourcamerascanbeplacedinarbitrarydistances.(Right)Whenoptimizingmultiplecamerahypotheses,onlythebestcamerashouldoptimizetheshapeandappearance,herevisualizedinadeepgreen.Cameraswhicharenotalignedthatwellarevisualizedinoff-whiteandcannotinﬂuencethereﬂectanceﬁeld.Whenthenon-alignedcameraposesimproveduringthetraining,theymaybecomeapplicableforthenetworkoptimization.00.51·10500.51AnnealingFocalLengthStepsLossScalerλaAdvancedλcCoarse2FineλbBRDFFigure4:Optimizationscheme.Ourmethodperformsasmoothﬂowofopti-mizationparametersusingthreeλvari-ablesforlossscaling.Additionally,weperformaFourierfrequencyannealingintheﬁrstphaseofthetrainingandde-laythetrainingofthefocallengthforlaterstagesinthetraining.TheBRDFestimationismainlyregulatedbytheλbparameter.byTanciketal.[53]weaddaGaussiandistributednoisetotheFourierembedding.However,aswealsoleverageBARF’s[34]Fourierannealing,weaddtheserandomfrequenciesasoffsetstothelogarithmicallyspacedfrequencies.Withouttheserandomoffsets,artifactsfromtheaxis-alignedfrequencieslikestripescanoccur.Furtherdetailsareavailableinthesupplementarymaterial.Roughcameraposequadrantinitialization.Weobservethatcameraposeoptimizationisahighlynon-convexproblemandtendstoquicklygetstuckinlocalminima.Tocombatthis,weproposetoannotatecameraposequadrantswith3simplebinaryquestions:Leftvs.Right,Abovevs.Below,andFrontvs.Back.Thisonlytakesabout4-5minutesforatypical80imagecollection.NotethatourposequadrantinitializationismuchnoisiercomparedtoaddingsomenoisearoundGTcameraposesasinsomerelatedworkssuchasNeRF--[57].ThisroughposeinitializationisinlinewithrecentworkssuchasNeRS[60]thatalsouseroughmanualposeinitialization.Flexibleobject-centriccameraparameterizationforvaryingcameradistances.Wedeﬁnethetrainableper-imagecameraparametersusinga‘look-at’parameterizationwitha3Dlook-atvectorrj∈R3;j∈{1,...,q},ascalaruprotationuj∈R[−π,π]anda3Dcamerapositiontj∈R3aswellasafocallengthfj∈R.Furthermore,thesearestoredasoffsetstotheinitialparame-ters,enablingeasierregularization.WeadditionallystoretheoffsetverticalfocallengthsfjinacompressedmannersimilartoNeRF--:ˆfj=pfj/h[57],wherehistheimageheightinpixels.Thecamerasareinitializedbasedonthegivenposequadrantsandaninitialﬁeldofviewof53.13degrees.Weoptimizeaperspectivepinholecamerawithaﬁxedprincipalpointbutper-imagefocallengths.Thecamerasarenotalwaysequidistanttotheobjectforthein-the-wildimagecollections.Toaccountforvariablecamera-objectdistances,wedonotsetﬁxednearandfarboundsforeachraywhichisastandardpracticeinneuralvolumetricoptimizationssuchasNeRF[40].Instead,wedeﬁneasamplingrangebasedonthecameradistancetoorigin,e.g.thenearboundis|o|−vandthefarboundis|o|+v,wherevisdeﬁnedasoursamplingradiuswithadiameterof1.WeillustratethisspherewithnearandfarboundsinFig.3.Thisexplicitcomputationofnearandfarboundsforeachrayenablesplacingthecamerasatarbitrarydistancesfromtheobject.Thisisnotpossiblewiththeexistingneuralvolumeoptimizationtechniquesthatuseﬁxednearandfarboundsforeachcameraray.Thecamerasarethenplacedbasedonthequadrantsandplacedatadistancetomaketheentireneuralvolumevvisible.Thislook-atparameterizationismoreﬂexibleforoptimizingobject-centricneuralvolumesthanmorecommonlyused3Drotationmatrices.Cameramultiplexes.Weobservethatcameraposeoptimizationgetsstuckinlocalminimaevenwithroughquadrantposeinitialization.Tocombatthis,inspiredbymeshoptimizationworks(e.g.[21]),weproposetooptimizeacameramultiplexwith4randomlyjitteredposesaroundthe5quadrantcenterdirectionforeachimage.Optimizingmultiplecamerasperimagewouldreducethenumberofrayswecancastinasingleoptimizationstepduetomemoryandcomputationallimita-tions.Thismakescameramultiplexoptimizationnoisyandchallenginginlearningneuralvolumes.Weproposetechniquestomakecameramultiplexlearningmorerobustbydynamicallyre-weighingthelossfunctionsassociatedwithdifferentcamerasinamultiplexduringtheoptimization.ThisprocessisvisualizedinFig.3.Speciﬁcally,wecomputethemaskreconstructionlossLMaskij∈Rassociatedwitheachcameraiandimagej.Wethenre-weigheachcameralossinamultiplexwithSj=softmax(−λsLMaskij),whereSj∈R4andλsisascalarthatisgraduallyincreasedduringtheoptimization.Thatis,were-weighthelosswithLNetworkj=PiSijLNetworkij.Thisdynamicre-weighingreducestheinﬂuenceofbadcameraposeswhilelearningtheshapeandmaterials.Sincewecanonlyrenderarandomsetofrayswithineachbatch,weupdatethecameramultiplexweightsSjwithamemorybankandmomentumacrossthebatches.Seethesupplementsformoredetails.Posteriorscalingofinputimages.Someimagesarenoisierthanothers(e.g.,duetocamerashake)ornoisyobjectmasksMj.Toberobustagainstsuchnoisydata,weproposetore-weighimagesinthegivencollection.Wekeepacircularbufferofaround1000elementswiththerecentmasklossesandrenderedimagelosseswithmultiplexscalingapplied.Weusethisbuffertocalculatethemeanµlandstandarddeviationσloftheselosses.Giventherecentlossstatisticswealsocreatealossscalarusing:spj=max(tanh(cid:16)µl−(LMaskj+LImagej)σl(cid:17)+1,1).Inasimilarwaytothecameraposeriorscaling,weemployitonaper-imagebasisusing:LNetworkj=spjLNetworkj.3.2LossesandOptimizationImagereconstructionlossisaChabonnierloss:LImage(g,p)=p(g−p)2+0.0012betweentheinputcolorfromCforpixelsandthecorrespondingpredictedcolorofthenetworks˜c.Weadditionallycalculatethelosswiththerenderedcolorˆc.Masklossesconsistoftwoterms.Oneisthebinarycross-entropylossLBCEbetweenthevolume-renderedmaskandestimatedforegroundobjectmask.ThesecondoneisthebackgroundlossLBackgroundfromNeRD[11],whichforcesallsamplesforrayscasttowardsthebackgroundtobe0.Wecombinetheselossesasthemaskloss:LMask=LBCE+LBackgroundRegularizationlossesWecomputethegradientofthedensitytoestimatethesurfacenormals.WeusethenormaldirectionlossLndirfrom[54]toconstrainthenormalstofacethecamerauntiltherayreachesthesurface.Thishelpsinprovidingsharpersurfaceswithoutcloud-likeartifacts.BRDFlosses.ThetaskofjointestimationofBRDFandilluminationisquitechallenging.Forex-ample,weobservethattheilluminationcanfallintoalocalminimum.Theobjectisthentintedinabluishcolor,andtheilluminationisanorangecolortoexpressamoreneutralcolortone.Asourimagecollectionshavemultipleilluminations,wecanforcethebasecolorbctoreplicatethepixelcolorfromtheimages.Thisway,ameancoloroverthedatasetislearnedandpreventsfallingintothelocalminima.WeleveragetheMeanSquaredError(MSE)forthis:LInit=LMSE(Cs,bc).Ad-ditionally,weﬁndthatasmoothnesslossLSmoothforthenormal,roughness,andmetallicparameterssimilartotheoneusedinUNISURF[44]furtherregularizesthesolution.Overallnetworkandcameralosses.TheﬁnallosstooptimizethedecompositionnetworkisthendeﬁnedasLNetwork=λbLImage(Cs,˜c)+(1−λb)LImage(Cs,ˆc)+LMask+λaLInit+λndirLndir+λSmoothLSmooth+λDecSmoothLDecSmooth+λDecSparsityLDecSparsity.Here,λbandλaaretheoptimizationschedulingvariables.Furthermore,thecameraposteriorscalingisappliedtotheselosses.Forthecameraoptimization,weleveragethesamelossesasdescribedinLNetwork.However,asthecamerashouldalwaysbeoptimized,wedonotapplyposteriorscalingtothelosseswhenoptimizingcameras.Thisenablescamerasthatarenotalignedwelltobeoptimizedandallowseachcameratoleavethelocalminima.Here,weonlycalculatethemeanloss,andthereforebadlyinitializedcameraposescanstillrecoveroverthetrainingduration.Additionally,wedeﬁneanL1lossonthelook-atvectorrjtoconstrainthecameraposetolookattheobject.ThelossisdeﬁnedasLlookat.Wealsouseavolumepaddingloss,whichpreventscamerasfromgoingtoofarintoourvolumeboundv:LBounds=max((v−|tj|)2,0).Optimizationscheduling.Fig.4showstheoptimizationscheduleofdifferentlossweights.Weusethreefadingλvariablestotransitiontheoptimizationschedulesmoothly.Theλcismainlyusedtoincreaseimageresolutionandreducethenumberofactivemultiplexcameras.Thedirectcolor6˜coptimizationisfadedtotheBRDFoptimizationusingλbandsomelossesarescaledbyλaasdeﬁnedearlier.Furthermore,weperformtheBARF[34]frequencyannealingintheearlystagesofthetraininganddelaythefocallengthoptimizationtothelaterstagesofthetraining.Weusetwodifferentoptimizers.ThenetworksareoptimizedbyanAdamoptimizerwithalearningrateof1e-4andexponentiallydecayedbyanorderofmagnitudeevery300ksteps.Thecameraoptimizationisperformedwithalearningrateof3e-3andexponentiallydecayedbyanorderofmagnitudeevery70ksteps.Furtherdetailsoftheoptimizationscheduleareavailableinthesupplements.PosesNotKnown(10)PosesAvailable(5)MethodPSNR↑SSIM↑PSNR↑SSIM↑Translation↓Rotation°↓BARF-A16.90.7919.70.7323.382.99SAMURAI23.460.9022.840.898.610.86NeRD[11]——26.880.95——Neural-PIL[12]——27.730.96——Table1:NovelViewSynthesisonvaryingil-luminationdatasets.Wesplitourdatasetsintothosewherewehaveposes,andhighlychalleng-ingoneswheretheposeswerenotrecoverablewithclassicalmethods.SAMURAIachievescon-siderablybetterperformancecomparedtoBARF-A.Forreference,wealsoshowthemetricsfromNeRDandNeural-PILwhichrequireGTposesanddonotworkonimageswithunknownposes.GT1Ours1GT2Ours2Figure5:SAMURAI-DatasetNovelViews.Here,weshownovelviewsandilluminationsfromtheSAMURAIdatasetsalongsideourrecon-struction.Theilluminationconditionsareaccu-ratelyreproduced.MethodPSNR↑SSIM↑w/oCameraMultiplex23.010.87w/oPosteriorScaling23.510.88w/oCoarse2Fine22.730.83w/oRandomFourierOffsets24.010.91w/oRegularization21.770.86Full24.310.92Table2:Ablationstudy.viewsynthesisandre-lightingresultsontwoscenes(GarbageTruckandNeRDcar)showthatablatinganyoftheproposedaspectsofSAMURAIcanresultsinworseresultsdemonstratingtheirimportance.MethodPoseInitPSNR↑SSIM↑Translation↓Rotation°↓BARF[34]Directions14.960.4734.640.86GNeRF[38]Random20.30.6181.222.39NeRS[60]Directions12.840.6832.270.77SAMURAIDirections21.080.7633.950.71NeRD[11]GT23.860.88——Neural-PIL[12]GT23.950.90——Table3:NovelViewSynthesisonsingleillu-minationdatasets.Fortwoscenesundersingleilluminationtheposesareeasilyrecoverable.Furthermore,wecannowcomparewithGNeRFwhichdoesnotrequireposeinitialization.Asseenourmethodachievesagoodperformance.ForreferencetheviewsynthesismetricswithNeRDandNeural-PILthatuseGTknownposesarealsoshown.4ExperimentsDatasetsForevaluations,wecreatednewdatasetsof8objects(eachwith80images)capturedunderuniqueilluminationsandlocationsandafewdifferentcameras.WerefertothisdatasetastheSAMURAIdataset.Theimagesreﬂectthepracticalandchallenginginputscenariowearetargetinginthiswork.CommonmethodssuchasCOLMAPfailtoestimatecorrespondencesandcameraposesforthisdataset.Therefore,wecannotrunmethodsthatrequireposesonthisdataset.Additionally,weevaluateon2CC-licensedimagecollectionsfromonlinesourcesofthestatueoflibertyandachair.Wealsousethe3syntheticand2real-worlddatasetsofNeRD[11]undervaryingillumination,whereposesareavailable.Lastly,toshowcasetheperformancewithothermethods,weusethe2real-worlddatasetsfromNeRD,whicharetakenunderﬁxedillumination.Intotal,weevaluateSAMURAIon17scenes.PleaserefertothesupplementarymaterialforanoverviewoftheSAMURAIdatasetsalongwithotherdatasetswithexperimentedwith.Baselines.Currently,thereexistnopriorartthatcantacklevaryingilluminationinputimageswhilejointlyestimatingcameraposes.So,wecomparewithamodiﬁedBARF[34]technique,whichcanstoreper-imageappearancesinalatentvector.WecallthisbaselineBARF-A.Additionally,onsceneswithﬁxedillumination,wecancomparewithGNeRF[38],theregularBARF,andamodiﬁedversionofNeRS[60](detailsinthesupplement).Onthedatasetswhereposesareeasilyrecoveredorgiven,wecanalsocomparewithNeRD[11]andNeural-PIL[12],whichrequireknown,near-perfectcameraposes.WesupplyBARF,BARF-A,andNeRSwiththesameposeinitializationasusedinSAMURAI.7Robot1Robot2FireEngine1FireEngine2OursBARF-AFigure6:ComparisonwithBARF-A.WhencomparingnovelviewsynthesisandrelightingresultsofSAMURAI(top)withBARF-A(bottom),SAMURAIproducesmoreaccuratecameraposesandcapturestheobjectbetter.BARF-Aissometimesunabletorecovertheshapeandposes.InputBasecolorMetallicRoughnessNormalRe-renderOursNeural-PILFigure7:ComparisonwithNeural-PILdecomposition.Noticeouraccurateposealignment,plausiblegeometry,reducedﬂoatersandaccurateBRDFdecomposition,comparedtoNeural-PIL,evenwithoutrelyingonnearperfectposes.Evaluation.WeperformnovelviewsynthesisusinglearnedvolumesandusestandardPSNRandSSIMmetricsw.r.t.ground-truthviews.Weheld-outevery16thimagefortesting.Forevaluationpurposes,weoptimizethecamerasandilluminationsonthetestimagesbutdidnotallowthetestimagestoaffectthemaindecompositionnetworkorcameratraining.Additionally,weperformtheProcrustesanalysisontherecoveredcameraposestoevaluatetheposeoptimization.AblationStudy.WeperformanablationstudywhereweablatedifferentaspectsoftheSAMU-RAImodeltoanalyzetheirimportance.Table2showsthenovelviewsynthesisaveragemetricsontheGarbageTruckcollectionfromtheSAMURAIdatasetandthesyntheticcardatasetfromNeRD[11].Metricsshowthatregularizationandthecoarsetoﬁneoptimizationarethemostsignif-icantcontributingfactorstotheﬁnalreconstructionquality.Themultiplexcamerasandtheposteriorscalingalsoimprovethereconstructionquality,stabilizingthetrainingandpreventingcamerastogetstuckinlocalminima.Visualcomparisonsofthespeciﬁcablationsareavailableinthesupplements.Resultsonvaryingilluminationdatasets.WedividethevaryingilluminationdatasetsintotheoneswithoutGTposesandoneswithaccuratecameraposes(eitherviaGTorCOLMAP).Sincethesedatasetshavevaryingillumination,weneedtoperformbothviewsynthesisandrelightingtoobtaintargetviews.Table1showsthemetricscomputedw.r.t.testviews.Resultsshowthatweconsider-ablyoutperformBARF-AinbothPSNRandSSIMmetricswhileatthesametimesolvingamorechallengingBRDFdecompositiontask.VisualresultsinFig.6alsoclearlydemonstratebetterviewsynthesisandrelightingresultscomparedtoBARF-A.BARF-Afailedtoalignthecameraposes,whereasSAMURAIachievesmoreaccuratecameraposesanddrasticallyimprovedreconstructionquality.OnlyslightlyperturbedposesareleveragedasstartingpositionsintheoriginalBARFmethod.Ourcoarseposeinitializationistoonoisyforthemethodtoworkaccurately.SAMURAIovercomesthisissuewiththecameramultiplexandotheroptimizationstrategies.Fig.7showsthevisualcomparisonofBRDFdecompositionsonMotherChilddatasetfromNeRD[11]alongwiththecorrespondingresultsfromNeural-PIL[12].Ingeneral,ourmethodcandecomposethesceneevenwithunknowncameraposes.SAMURAIalsoproducesfewerﬂoat-ingartifactsandcreatesamorecoherentsurface.Theroughnessparameterisalsomoreplausibleinourresult,astheobjectisrough,whereasNeural-PILestimatedanearmirror-likesurface.FurtherresultsfromtheSAMURAIdatasetareshowninFig.5.Weshownovelviewsandrelightingresultsw.r.tthetargettestviews.VisualclearlyshowthatSAMURAIcanrecovertheposeandprovideaconsistentilluminationw.r.ttheground-truthtargetviews.Resultsonﬁxedilluminationdatasets.Forimagecollectionscapturedunderﬁxedillumination,wecancomparewithmoretechniques.WecomparewithGNeRF,thedefaultBARFandNeRS.We8Head1Head2SAMURAIBARFSAMURAIBARFGNeRFNeRSGNeRFNeRSCape1Cape2SAMURAIBARFSAMURAIBARFGNeRFNeRSGNeRFNeRSFigure8:ComparisonwithBaselines.WhencomparingSAMURAIwiththebaselines(GNeRF,BARF,andNeRS)oursoutperformsallmethodsinreconstructionqualityandposeestimation.additionallycancomparewithNeural-PIL[12]andNeRD[11]onthenear-perfectcameraposesre-coveredfromCOLMAP.ResultsinTable3showthatSAMURAIoutperformsthebaselinesBARF,GNeRF,andNeRSandisalsoclosetoNeural-PILandNeRDthatusesGTcameraposes.GNeRFdoesnotrequirearoughposeinitialization.Overallourmethodalsoachievesagoodposerecovery,whereNeRSonlyslightlyoutperformsourmethodinthetranslationalerrorduetosomeoutliersinourcase.Theseoutliersdonotdegradeourreconstructionsduetoourimageposteriorloss.Fig.8showssampleviewsynthesisresultsofSAMURAI,BARF,GNeRF,andNeRSonsamplesin-gleilluminationdatasets.VisualsindicatebetterresultswithSAMURAIcomparedtoGNeRFandBARF.NeRSseemstocapturemoreapparentdetail,butthegeneraldecompositionqualityissignif-icantlybetterinourmethod,wherethecapegoldmaterialisrepresentedmoreaccurately.NeRSalsointroducesamisalignedfacetextureintheHeadscene,wheretwofacesarevisible.Furthermore,NeRSisnotcapableofperfectlyoptimizingtheposes.ThisisvisibleinCape1andHead1.Applications.Oneofthecontributionsofthisworkistheextractionofexplicitmeshwithmaterialpropertiesfromthelearnedneuralreﬂectancevolume.Theprocessisdescribedinthesupplements.TheresultingmeshcanberealisticallyplacedinanAugmentedReality(AR)sceneorina3Dgame.Inaddition,onecouldedittheBRDFmaterialsontherecoveredmesh.SeeFig.1forsampleresultsoftheseapplications,whereourrecovered3Dassetsblendwellinagiven3Dscene.Limitations.SAMURAIachieveslargestridesinthedecompositionofin-the-wildimagecollec-tionscomparedtopriorart.However,westillrelyonroughposeinitialization.GNeRFproposesareconstructiontechniquewithoutanyposeinitializationbutitfailsonthechallengingin-the-wilddatasets.Furthermore,SAMURAIproducesslightlyblurrytextures.ThisisespeciallynoticeableinthecapesceneinFig.8.Here,thecapehasarepeating,high-frequenttexture.Reconstructionofthishigh-frequencytexturerequiresnear-perfectcameraposes.Sincethisdatasetisinasinglelocationandillumination,COLMAP-basedposeestimationoutperformsSAMURAIbasedposealignment.However,SAMURAIenablesthereconstructionofhighlychallengingdatasetsofonlineimagecol-lectionswhereCOLMAPcompletelyfails.OurBRDFandilluminationdecompositionisalsonotcapableofmodellingshadowingandinter-reﬂections.Aswemainlytackleobjectdecomposition,theshadowsandinter-reﬂectionsarenotcrucial.Removingtheneedforposeinitializationalongwithmodelingshadowsandinter-reﬂectionsformanimportantfuturework.5ConclusionSAMURAIisacarefullydesignedoptimizationframeworkforjointcamera,shape,BRDF,andillu-minationestimation.Itcanworkonin-the-wildimagecollectionscapturedinvaryingbackgroundsandilluminationsandwithdifferentcameras.Resultsonexistingandournewchallengingdatasetdemonstrategoodviewsynthesisandrelightingresults,whereseveralexistingtechniquesfail.Inaddition,ourmeshextractionallowstheresulting3DassetstobereadilyusedinseveralgraphicsapplicationssuchasAR/VR,gaming,materialeditingetc.9AcknowledgmentsandDisclosureofFundingThisworkhasbeenpartiallyfundedbytheDeutscheForschungsgemeinschaft(DFG,GermanRe-searchFoundation)underGermany’sExcellenceStrategy–EXCnumber2064/1–Projectnumber390727645andSFB1233,TP02-Projectnumber276693517.ItwassupportedbytheGermanFederalMinistryofEducationandResearch(BMBF):TübingenAICenter,FKZ:01IS18039A.References[1]MiikaAittala,TimoAila,andJaakkoLehtinen.Reﬂectancemodelingbyneuraltexturesyn-thesis.InACMTransactionsonGraphics(ToG),2018.[2]RachelAlbert,DorianYaoChan,DanB.Goldman,andJamesF.O’Brian.ApproximatesvBRDFestimationfrommobilephonevideo.InEurographicsSymposiumonRendering,2018.[3]Louis-PhilippeAsselin,DenisLaurendeau,andJean-FrançoisLalonde.DeepSVBRDFesti-mationonrealmaterials.InInternationalConferenceon3DVision(3DV),2020.[4]JonathanT.Barron,BenMildenhall,MatthewTancik,PeterHedman,RicardoMartin-Brualla,andPratulP.Srinivasan.Mip-nerf:Amultiscalerepresentationforanti-aliasingneuralradianceﬁelds.IEEEInternationalConferenceonComputerVision(ICCV),2021.[5]SaiBi,ZexiangXu,PratulSrinivasan,BenMildenhall,KalyanSunkavalli,MilošHašan,Yan-nickHold-Geoffroy,DavidKriegman,andRaviRamamoorthi.Neuralreﬂectanceﬁeldsforappearanceacquisition.ArXive-prints,2020.[6]SaiBi,ZexiangXu,KalyanSunkavalli,MilošHašan,YannickHold-Geoffroy,DavidKrieg-man,andRaviRamamoorthi.Deepreﬂectancevolumes:Relightablereconstructionsfrommulti-viewphotometricimages.InEuropeanConferenceonComputerVision(ECCV),2020.[7]SaiBi,ZexiangXu,KalyanSunkavalli,DavidKriegman,andRaviRamamoorthi.Deep3dcapture:Geometryandreﬂectancefromsparsemulti-viewimages.InIEEEConferenceonComputerVisionandPatternRecognition(CVPR),2020.[8]MarkBossandHendrikP.A.Lensch.Singleimagebrdfparameterestimationwithacondi-tionaladversarialnetwork.InArXive-prints,2019.[9]MarkBoss,FabianGroh,SebastianHerholz,andHendrikP.A.Lensch.DeepDualLossBRDFParameterEstimation.InWorkshoponMaterialAppearanceModeling,2018.[10]MarkBoss,VarunJampani,KihwanKim,HendrikP.A.Lensch,andJanKautz.Two-shotspatially-varyingBRDFandshapeestimation.InIEEEConferenceonComputerVisionandPatternRecognition(CVPR),2020.[11]MarkBoss,RaphaelBraun,VarunJampani,JonathanT.Barron,CeLiu,andHendrikP.A.Lensch.NeRD:Neuralreﬂectancedecompositionfromimagecollections.InIEEEInterna-tionalConferenceonComputerVision(ICCV),2021.[12]MarkBoss,VarunJampani,RaphaelBraun,CeLiu,JonathanT.Barron,andHendrikP.A.Lensch.Neural-pil:Neuralpre-integratedlightingforreﬂectancedecomposition.InAdvancesinNeuralInformationProcessingSystems(NeurIPS),2021.[13]EricChan,MarcoMonteiro,PetrKellnhofer,JiajunWu,andGordonWetzstein.pi-GAN:Periodicimplicitgenerativeadversarialnetworksfor3D-awareimagesynthesis.InIEEECon-ferenceonComputerVisionandPatternRecognition(CVPR),2021.[14]AnpeiChen,ZexiangXu,AndreasGeiger,JingyiYu,andHaoSu.TensoRF:Tensorialradianceﬁelds.InArXive-prints,2022.[15]ZhiqinChenandHaoZhang.Learningimplicitﬁeldsforgenerativeshapemodeling.IEEEConferenceonComputerVisionandPatternRecognition(CVPR),2019.10[16]RobertL.CookandKennethE.Torrance.Areﬂectancemodelforcomputergraphics.ACMTransactionsonGraphics(ToG),1982.[17]ValentinDeschaintre,MiikaAitalla,FredoDurand,GeorgeDrettakis,andAdrienBousseau.Single-imageSVBRDFcapturewitharendering-awaredeepnetwork.InACMTransactionsonGraphics(ToG),2018.[18]ValentinDeschaintre,MiikaAitalla,FredoDurand,GeorgeDrettakis,andAdrienBousseau.FlexibleSVBRDFcapturewithamulti-imagedeepnetwork.InEurographicsSymposiumonRendering,2019.[19]ValentinDeschaintre,GeorgeDrettakis,andAdrienBousseau.Guidedﬁne-tuningforlarge-scalematerialtransfer.InEurographicsSymposiumonRendering,2020.[20]DuanGao,XiaoLi,YueDong,PieterPeers,andXinTong.Deepinverserenderingforhigh-resolutionSVBRDFestimationfromanarbitrarynumberofimages.InACMTransactionsonGraphics(SIGGRAPH),2019.[21]ShubhamGoel,AngjooKanazawa,andJitendraMalik.Shapeandviewpointwithoutkey-points.InEuropeanConferenceonComputerVision(ECCV),2020.[22]PeterHedman,PratulP.Srinivasan,BenMildenhall,JonathanT.Barron,andPaulDebevec.Bakingneuralradianceﬁeldsforreal-timeviewsynthesis.InIEEEInternationalConferenceonComputerVision(ICCV),2021.[23]PhilippHenzler,ValentinDeschaintre,NiloyJMitra,andTobiasRitschel.Generativemod-ellingofBRDFtexturesfromﬂashimages.ACMTransactionsonGraphics(SIGGRAPHASIA),2021.[24]YoonwooJeong,SeokjunAhn,ChristopherChoy,AnimashreeAnandkumar,MinsuCho,andJaesikPark.Self-calibratingneuralradianceﬁelds.InIEEEInternationalConferenceonComputerVision(ICCV),2021.[25]BerkKaya,SuryanshKumar,CarlosOliveira,VittorioFerrari,andLucVanGool.Uncalibratedneuralinverserenderingforphotometricstereoofgeneralsurfaces.InIEEEInternationalConferenceonComputerVision(ICCV),2021.[26]PetrKellnhofer,LarsJebe,AndrewJones,RyanSpicer,KariPulli,andGordonWetzstein.Neurallumigraphrendering.InIEEEConferenceonComputerVisionandPatternRecognition(CVPR),2021.[27]ZhengfeiKuang,KyleOlszewski,MengleiChai,ZengHuang,PanosAchlioptas,andSergeyTulyakov.NeROIC:Neuralobjectcaptureandrenderingfromonlineimagecollections.ArXive-prints,2022.[28]JasonLawrence,SzymonRusinkiewicz,andRaviRamamoorthi.EfﬁcientBRDFimportancesamplingusingafactoredrepresentation.ACMTransactionsonGraphics(ToG),2004.[29]HendrikP.A.Lensch,JanKautz,MichaelGosele,andHans-PeterSeidel.Image-basedrecon-structionofspatiallyvaryingmaterials.InEurographicsConferenceonRendering,2001.[30]HendrikP.A.Lensch,JochenLang,M.SaAsla,andHans-PeterSeidel.PlannedsamplingofspatiallyvaryingBRDFs.InComputerGraphicsForum,2003.[31]XiaoLi,YueDong,PieterPeers,andXinTong.Modelingsurfaceappearancefromasinglephotographusingself-augmentedconvolutionalneuralnetworks.InACMTransactionsonGraphics(ToG),2017.[32]ZhengqinLi,KalyanSunkavalli,andManmohanChandraker.Materialsformasses:SVBRDFacquisitionwithasinglemobilephoneimage.InEuropeanConferenceonComputerVision(ECCV),2018.11[33]ZhengqinLi,MohammadShaﬁei,RaviRamamoorthi,KalyanSunkavalli,andManmohanChandraker.Inverserenderingforcomplexindoorscenes:Shape,spatially-varyinglightingandSVBRDFfromasingleimage.InIEEEConferenceonComputerVisionandPatternRecognition(CVPR),2020.[34]Chen-HsuanLin,Wei-ChiuMa,AntonioTorralba,andSimonLucey.Barf:Bundle-adjustingneuralradianceﬁelds.InIEEEInternationalConferenceonComputerVision(ICCV),2021.[35]LingjieLiu,JiataoGu,KyawZawLin,Tat-SengChua,andChristianTheobalt.Neuralsparsevoxelﬁelds.InAdvancesinNeuralInformationProcessingSystems(NeurIPS),2020.[36]StephenLombardi,TomasSimon,JasonSaragih,GabrielSchwartz,AndreasLehrmann,andYaserSheikh.Neuralvolumes:Learningdynamicrenderablevolumesfromimages.ACMTransactionsonGraphics(ToG),2019.[37]RicardoMartin-Brualla,NohaRadwan,MehdiS.M.Sajjadi,JonathanT.Barron,AlexeyDosovitskiy,andDanielDuckworth.NeRFintheWild:NeuralRadianceFieldsforUncon-strainedPhotoCollections.InIEEEConferenceonComputerVisionandPatternRecognition(CVPR),2021.[38]QuanMeng,AnpeiChen,HaiminLuo,MinyeWu,HaoSu,LanXu,XumingHe,andJingyiYu.GNeRF:GAN-basedNeuralRadianceFieldwithoutPosedCamera.InIEEEInternationalConferenceonComputerVision(ICCV),2021.[39]LarsMescheder,MichaelOechsle,MichaelNiemeyer,SebastianNowozin,andAndreasGeiger.Occupancynetworks:Learning3dreconstructioninfunctionspace.InIEEECon-ferenceonComputerVisionandPatternRecognition(CVPR),2019.[40]BenMildenhall,PratulSrinivasan,MatthewTancik,JonathanT.Barron,RaviRamamoorthi,andRenNg.NeRF:Representingscenesasneuralradianceﬁeldsforviewsynthesis.InEuropeanConferenceonComputerVision(ECCV),2020.[41]ThomasMüller,AlexEvans,ChristophSchied,andAlexanderKeller.Instantneuralgraphicsprimitiveswithamultiresolutionhashencoding.ACMTransactionsonGraphics(ToG),2022.[42]JacobMunkberg,JonHasselgren,TianchangShen,JunGao,WenzhengChen,AlexEvans,ThomasMueller,andSanjaFidler.ExtractingTriangular3DModels,Materials,andLightingFromImages.IEEEConferenceonComputerVisionandPatternRecognition(CVPR),2022.[43]GiljooNam,DiegoGutierrez,andMinH.Kim.PracticalSVBRDFacquisitionof3dobjectswithunstructuredﬂashphotography.InACMTransactionsonGraphics(SIGGRAPHASIA),2018.[44]MichaelOechsle,SongyouPeng,andAndreasGeiger.Unisurf:Unifyingneuralimplicitsur-facesandradianceﬁeldsformulti-viewreconstruction.InIEEEInternationalConferenceonComputerVision(ICCV),2021.[45]JeongJoonPark,PeterFlorence,JulianStraub,RichardNewcombe,andStevenLovegrove.Deepsdf:Learningcontinuoussigneddistancefunctionsforshaperepresentation.InIEEEConferenceonComputerVisionandPatternRecognition(CVPR),June2019.[46]XuebinQin,ZichenZhang,ChenyangHuang,MasoodDehghan,OsmarZaiane,andMartinJagersand.U2-net:Goingdeeperwithnestedu-structureforsalientobjectdetection.InPatternRecognition,volume106,2020.[47]ShenSangandManmohanChandraker.Single-shotneuralrelightingandSVBRDFestimation.InEuropeanConferenceonComputerVision(ECCV),2020.[48]SaraFridovich-KeilandAlexYu,MatthewTancik,QinhongChen,BenjaminRecht,andAngjooKanazawa.Plenoxels:Radianceﬁeldswithoutneuralnetworks.InIEEEConferenceonComputerVisionandPatternRecognition(CVPR),2022.[49]JohannesLutzSchönbergerandJan-MichaelFrahm.Structure-from-motionrevisited.InIEEEConferenceonComputerVisionandPatternRecognition(CVPR),2016.12[50]JohannesLutzSchönberger,EnliangZheng,MarcPollefeys,andJan-MichaelFrahm.Pixel-wiseviewselectionforunstructuredmulti-viewstereo.InEuropeanConferenceonComputerVision(ECCV),2016.[51]SoumyadipSengupta,JinweiGu,KihwanKim,GuilinLiu,DavidW.Jacobs,andJanKautz.Neuralinverserenderingofanindoorscenefromasingleimage.InIEEEInternationalCon-ferenceonComputerVision(ICCV),2019.[52]PratulP.Srinivasan,BoyangDeng,XiumingZhang,MatthewTancik,BenMildenhall,andJonathanT.Barron.NeRV:Neuralreﬂectanceandvisibilityﬁeldsforrelightingandviewsynthesis.InIEEEConferenceonComputerVisionandPatternRecognition(CVPR),2021.[53]MatthewTancik,PratulP.Srinivasan,BenMildenhall,SaraFridovich-Keil,NithinRaghavan,UtkarshSinghal,RaviRamamoorthi,JonathanT.Barron,andRenNg.Fourierfeaturesletnetworkslearnhighfrequencyfunctionsinlowdimensionaldomains.AdvancesinNeuralInformationProcessingSystems(NeurIPS),2020.[54]DorVerbin,PeterHedman,BenMildenhall,ToddZickler,JonathanT.Barron,andPratulP.Srinivasan.Ref-neRF:Structuredview-dependentappearanceforneuralradianceﬁelds.InIEEEConferenceonComputerVisionandPatternRecognition(CVPR),2022.[55]PengWang,LingjieLiu,YuanLiu,ChristianTheobalt,TakuKomura,andWenpingWang.NeuS:Learningneuralimplicitsurfacesbyvolumerenderingformulti-viewreconstruction.InAdvancesinNeuralInformationProcessingSystems(NeurIPS),2021.[56]QianqianWang,ZhichengWang,KyleGenova,PratulSrinivasan,HowardZhou,JonathanT.Barron,RicardoMartin-Brualla,NoahSnavely,andThomasFunkhouser.Ibrnet:Learningmulti-viewimage-basedrendering.InIEEEConferenceonComputerVisionandPatternRecognition(CVPR),2021.[57]ZiruiWang,ShangzheWu,WeidiXie,MinChen,andVictorAdrianPrisacariu.NeRF−−:Neuralradianceﬁeldswithoutknowncameraparameters.ArXive-prints,2021.[58]WenjieYe,XiaoLi,YueDong,PieterPeers,andXinTong.Singleimagesurfaceappearancemodelingwithself-augmentedcnnsandinexactsupervision.ComputerGraphicsForum,2018.[59]AlexYu,RuilongLi,MatthewTancik,HaoLi,RenNg,andAngjooKanazawa.PlenOctreesforreal-timerenderingofneuralradianceﬁelds.InIEEEInternationalConferenceonCom-puterVision(ICCV),2021.[60]JasonZhang,GengshanYang,ShubhamTulsiani,andDevaRamanan.NeRS:Neuralre-ﬂectancesurfacesforsparse-view3dreconstructioninthewild.InAdvancesinNeuralInfor-mationProcessingSystems(NeurIPS),2021.[61]JianzhaoZhang,GuojunChen,YueDong,JianShi,BobZhang,andEnhuaWu.Deepinverserenderingforpracticalobjectappearancescanwithuncalibratedillumination.InAdvancesinComputerGraphics,2020.[62]KaiZhang,FujunLuan,QianqianWang,KavitaBala,andNoahSnavely.PhySG:InverserenderingwithsphericalGaussiansforphysics-basedmaterialeditingandrelighting.InIEEEConferenceonComputerVisionandPatternRecognition(CVPR),2021.[63]XiumingZhang,PratulP.Srinivasan,BoyangDeng,PaulDebevec,WilliamT.Freeman,andJonathanT.Barron.NeRFactor:Neuralfactorizationofshapeandreﬂectanceunderanun-knownillumination.InACMTransactionsonGraphics(SIGGRAPHASIA),2021.[64]YuxuanZhang,WenzhengChen,HuanLing,JunGao,YinanZhang,AntonioTorralba,andSanjaFidler.ImageGANsmeetdifferentiablerenderingforinversegraphicsandinterpretable3dneuralrendering.InInternationalConferenceonLearningRepresentations(ICLR),2021.13SupplementaryMaterialforSAMURAI:ShapeAndMaterialfromUnconstrainedReal-worldArbitraryImagecollectionsMarkBossUniversityofTübingenGoogleResearchAndreasEngelhardtUniversityofTübingenGoogleResearchAbhishekKarGoogleResearchYuanzhenLiGoogleResearchDeqingSunGoogleResearchJonathanT.BarronGoogleResearchHendrikP.A.LenschUniversityofTübingenVarunJampaniGoogleResearchInthesupplementweﬁrstintroduceadditionaldetailsinSectionA1,A2,andA3.Themeshex-tractionisdescribedinSectionA4.WesummarizedifferentdatasetsusedintheexperimentsinSectionA5andstatethecategorizationperdataset.Lastly,weshowmoreexperimentsandresultsofSAMURAIinSectionA6.Foranoverviewofthisworkandfurtherresultspleasealsoconsiderwatchingthesupplementalvideo.A1RandomOffsetAnnealedFourierEncodingTheFourierEncodingγ:R37→R3+6LusedinNeRF[7]encodesa3DcoordinatexintoLfrequencybasis:γ(x)=(x,Γ1,...,ΓL−1)(1)whereeachfrequencyisencodedas:Γk(x)=(cid:2)sin(2kx),cos(2kx)(cid:3)(2)BARF[5]andNerﬁes[8]introducedanannealingoftheFourierFrequenciesusingaweighting:Γk(x;α)=wk(α)(cid:2)sin(2kx),cos(2kx)(cid:3)(3)wk(α)=1−cos(πclamp(α−k,0,1))2(4)whereα∈[0,L].ThiscanbeseenasatruncatedHannwindow.Onedownsideofthisformofencodingisthatallfrequenciesareaxis-aligned.InTanciketal.[11]thebeneﬁtsofaddingrandomfrequenciesaredemonstrated.However,combiningthiswiththeslidingcosinewindowisnoteasilypossible.Therefore,weproposetoaddrandomGaussianoffsetsR∈RL×3tothefrequencies.TheoffsetsRaresampledfromN(0,0.1).Thiscanbethoughtofrandomlyrotatingeachfrequencyband:Γk(x;α)=wk(α)(cid:2)sin(2kx+2kRk),cos(2kx+2kRk)(cid:3)(5)A2WeightUpdatingforCameraMultiplexAswestochasticallysamplepointsforeachbatch,apotentialbadcameracanhavefavorablesam-plesandoutperformabettercamera.WealleviatethisissuebystoringtheweightsforeachofourarXiv:2205.15768v1  [cs.CV]  31 May 2022PositionsxAnnealedFourierMLP,128,ReLUMLP,128,ReLUMLP,128,ReLUMLP,128,ReLUMLP,128,ReLUMLP,128,ReLUMLP,128,ReLUMLP,128,ReLUSkipConnectionMainNetworkMLP,64,ReLUBottleneckMLP,1,SoftplusDensityσMLP,16,ReLUBRDFLatentMLP,64,ReLUMLP,64,ReLUMLP,1,Sig.MLP,3,Sig.MLP,1,Sig.BRDFDecoderMetallicbmBasecolorbcRoughnessbr∇σxNormalnMLP,64,ReLUMLP,3,SigmoidConditionalNetworkDirectColor˜cDirectiondConditionalFourierAppearancezaFigureA1:Architecture.Thedetailedarchitectureofournetwork.Notethattheconditionalnetworkandthedirectcolorisonlyusedintheearlystagesofthetrainingforstabilization.Itdoesnotcontributetotheﬁnaldecompositionresult.OurmainoutputsincludetheDensityσ,thenormalnandtheBRDF(bc,bm,br),whichareusedforrenderingouractualoutputcolorˆcwithNeural-PIL[2].optimizationimagesinamemorybankW∈Rj×4.Thesecanthenbeupdatedduringtheopti-mizationandreducetheimpactofthesampledistributions.Furthermore,westoreamemorybankofvelocitiesV∈Rj×4tospeeduptheselectionofthebestcamerapose.TheweightmatrixisthenupdatedwiththenewweightsSjusing:W∗j=max(Wj+mV∗j+g,0)(6)V∗j=m∗Vj+g(7)g=s(Sj−Wj)(8)wherethenewweightsW∗jandvelocitiesV∗jreplacetheoldones,theparametersmrepresentthemomentumandsthelearningrate.Thevaluesfortheseare0.75and0.3,respectively.A3NetworkArchitectureandFurtherTrainingDetailsTheinputimagesforournetworkareusedwithoutcropping.Wesampletheforegroundareathriceasoftenasthebackgroundregionstocircumventthepotentiallargebackgroundareas.Asthereso-lutionvariesdrasticallyandcanbelarge,wefurtherresizetheimagessothatthelargestdimensionis400pixels.ThedetailedconﬁgurationofournetworkisshowninFig.A1.Weuse10RandomOffsetAnnealedFourierFrequenciesforthepositionalencoding.Theseareannealedover50000stepsusingEq.4.Thedirectionsareencodedusing4non-annealedandnon-offsetFourierfrequencies.Thelossesinsection3.2-Overallnetworkandcameralossesofthemainpaperareweightedwiththefollowingscalarsbesidestheoptimizationschedulescalarsλb,λa:λndirλSmoothλDecSparsityλDecSmooth0.0050.010.010.12Thecoarsetoﬁneoptimizationisfurthergovernedbyλc.Thisparametermainlyinterpolatesbe-tweentheavailableresolutionofthelargestdimensionfrom100to400pixelsandthenumberofcamerasfrom4to1.Thesoftmaxscalarλsisalsodrivenbyλcandfadesfromascalarof1to10.Furthermore,weapplygradientscalingtothegradientsforthenetworkbythenormof0.1.Thecameragradientsarenotclippedorscaled.A4MeshextractionSimilartoNeRD[1]weperformameshextractionfromthelearnedreﬂectanceneuralvolume.However,wedifferfromtheirmethod.Intheﬁrststep,weperformamarchingcubesextractionstepsimilartotheoneproposedinNeRF[7].However,asthenaivemarchingcubealgorithmcanhaveblockartifacts,wesample2millionpointsonthemeshsurfaceandcastraystowardsthesurface.TheresultingpointcloudisconvertedtoareﬁnedmeshusingPoissonreconstruction.Thisreﬁnedmeshprovidesmoredetailsandsmoothersurfaces.WethenUVunwraptheresultingmeshinBlender’s[3]automaticUVunwrappingtoolandbaketheworldspacepositionsintothetexturemap.WecanthenqueryallsurfacelocationsinourﬁnenetworkandcomputetheBRDFtexturemaps.WethensavethetexturedmeshintheGLBformatforeasydeployment.Theextractionofameshtakesaround3-5minutes.A5AdditionalDetailsofDatasetsSceneMulti-IlluminationKnownPosesNotesGoldCape73FromNeRD[1]Head73FromNeRD[1]Syn.CarWreck33SyntheticfromNeRD[1]Syn.Globe33SyntheticfromNeRD[1]Syn.Chair33SyntheticfromNeRD[1]MotherChild33FromNeRD[1]Gnome33FromNeRD[1]StatueofLiberty37OnlinecollectionChair37OnlinecollectionDuck37Self-collectedFireEngine37Self-collectedGarbageTruck37Self-collectedKeywest37Self-collectedPumpkin37Self-collectedRCCar37Self-collectedRobot37Self-collectedShoe37Self-collectedTableA1:ListofDatasets.Listofalldatasetsandtheclassiﬁcationintomulti-illuminationandknownposes.AlistofourdatasetsusedintheevaluationisshowninTableA1.OurnewdatasetinthelastsectionofTableA1consistsofaround70imageseach.Wetriedtoreplicatetheonlinecollectionsettingasmuchaspossiblebyusingdifferentcameras(Pixel4a,iPhone7Plus,Sonyalpha6000),capturingtheobjectsindifferentuniqueenvironments,andreplicatingthehand-heldcapturesetupwithvaryingdistances.Evenwiththeextensivemanualtuningofparameters,wearenotabletoestimatethecameraposesintraditionalmethodssuchasCOLMAP[9,10].Fig.A2showsanoverviewoftheimagesintwoimagecollections.3(a)Robot(b)FireEngineFigureA2:DatasetOverview.Noticethecomplexilluminationconditionsandthedrasticallyvaryinglocations.Alsothedistancesarevaryingquiteseverely.GTw/oRegular-izationw/oRandomFourierOffsetsw/oCoarse2Finew/oPosteriorScalingw/oCameraMultiplexFullFigureA3:VisualAblation.Eachofournoveladditionsimprovethereconstruction.Inthispar-ticularscenetheregularizationiscriticalforthedecomposition.Thecoarse2ﬁne,posteriorscalingandcameramultiplexablationsmainlyhaveareducedsharpnessinthestickeronthetop.IntherandomFourieroffsetannealingstripingpatternsareapparentinthesomeareaswhicharealleviatedwithourfullmodel.4A6AdditionalExperimentsAblation.InFig.A3theresultofourablationstudyisshown.Thebeneﬁtofourregularizationiseasilyapparentinthisscene.Furthermore,ourcoarse2ﬁne,posteriorscaling,andcameramultiplexhelprecoverslightlysharperdetailsbutespeciallyhelpstabilizetheoptimization.TherandomFourieroffsetsalsoalleviatesomeslightstripingartifacts.VisualResults.InFig.A4weshowadditionalresultsofSAMURAIonseveralhighlychalleng-ing,multipleilluminationscenes.Ourmethodcancreateplausibledecomposition,whichproducesconvincingresultswhenre-renderedinunseenviewsandilluminationconditions.EvenmostﬁnedetailsliketheRCCar’santennaarepreservedwell.Onlythelegsofthechairobjectarenotre-producedwell.However,thelegsarealsonotdetectedwellbyourautomaticsegmentationwithU2-Net.NeRSmodiﬁcations.ThedefaultimplementationofNeRS[12]doesnotimplementmini-batching.Thismeansallimagesareoptimizedsimultaneouslyinaresolutionof256×256.Thisworkswellforafewimages,buttheGPUmemoryrunsoutwithlargerimagecollections.Wehavecreatedamodiﬁedversionthatimplementsmini-batchingforafaircomparison.Still,NeRSisonlycapableofworkingonsingleilluminationdatasets.ProcrustesAnalysisofCameraPoses.WeevaluatethequalityofthereconstructedcameraposesagainstareferenceobtainedfromCOLMAP[9,10].ReferencesareonlyavailableforthescenesoftheNeRDdataset.First,wealignthecameralocationsusingProcrustesanalysis[4]asin[5].Therotationerrorisreportedasameandeviationindegrees,whilethetranslationerroriscomputedasthemeandifferenceinsceneunitsofthereferencescene.Incontrasttotheevaluationoftheviewsynthesisandrenderingperformance,wehereuseallcamerasfromthetrainingdataforcomparisonlikeithasbeendoneinconcurrentworks[5,6].References[1]MarkBoss,RaphaelBraun,VarunJampani,JonathanT.Barron,CeLiu,andHendrikP.A.Lensch.NeRD:Neuralreﬂectancedecompositionfromimagecollections.InIEEEInterna-tionalConferenceonComputerVision(ICCV),2021.[2]MarkBoss,VarunJampani,RaphaelBraun,CeLiu,JonathanT.Barron,andHendrikP.A.Lensch.Neural-pil:Neuralpre-integratedlightingforreﬂectancedecomposition.InAdvancesinNeuralInformationProcessingSystems(NeurIPS),2021.[3]BlenderOnlineCommunity.Blender-a3Dmodellingandrenderingpackage.BlenderFoun-dation,StichtingBlenderFoundation,Amsterdam,2018.URLhttp://www.blender.org.[4]JohnCGowerandGarmtBDijksterhuis.Procrustesproblems,volume30.OUPOxford,2004.[5]Chen-HsuanLin,Wei-ChiuMa,AntonioTorralba,andSimonLucey.Barf:Bundle-adjustingneuralradianceﬁelds.InIEEEInternationalConferenceonComputerVision(ICCV),2021.[6]QuanMeng,AnpeiChen,HaiminLuo,MinyeWu,HaoSu,LanXu,XumingHe,andJingyiYu.GNeRF:GAN-basedNeuralRadianceFieldwithoutPosedCamera.InIEEEInternationalConferenceonComputerVision(ICCV),2021.[7]BenMildenhall,PratulSrinivasan,MatthewTancik,JonathanT.Barron,RaviRamamoorthi,andRenNg.NeRF:Representingscenesasneuralradianceﬁeldsforviewsynthesis.InEuropeanConferenceonComputerVision(ECCV),2020.[8]KeunhongPark,UtkarshSinha,JonathanT.Barron,SoﬁenBouaziz,DanBGoldman,StevenM.Seitz,andRicardoMartin-Brualla.Deformableneuralradianceﬁelds.IEEEIn-ternationalConferenceonComputerVision(ICCV),2021.[9]JohannesLutzSchönbergerandJan-MichaelFrahm.Structure-from-motionrevisited.InIEEEConferenceonComputerVisionandPatternRecognition(CVPR),2016.5GT1SAMURAIGT2SAMURAIDuckFireEngineGarbageTruckKeywestPumpkinRCCarRobotShoeStatueofLibertyChairFigureA4:AdditionalResults.Renderingswithcameraposesandilluminationfromtestviewsdemonstrateplausiblenovelviewsynthesisandre-lightingonvariousdatasets.6[10]JohannesLutzSchönberger,EnliangZheng,MarcPollefeys,andJan-MichaelFrahm.Pixel-wiseviewselectionforunstructuredmulti-viewstereo.InEuropeanConferenceonComputerVision(ECCV),2016.[11]MatthewTancik,PratulP.Srinivasan,BenMildenhall,SaraFridovich-Keil,NithinRaghavan,UtkarshSinghal,RaviRamamoorthi,JonathanT.Barron,andRenNg.Fourierfeaturesletnetworkslearnhighfrequencyfunctionsinlowdimensionaldomains.AdvancesinNeuralInformationProcessingSystems(NeurIPS),2020.[12]JasonZhang,GengshanYang,ShubhamTulsiani,andDevaRamanan.NeRS:Neuralre-ﬂectancesurfacesforsparse-view3dreconstructioninthewild.InAdvancesinNeuralInfor-mationProcessingSystems(NeurIPS),2021.7