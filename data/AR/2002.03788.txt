0
2
0
2

b
e
F
6

]
S
A
.
s
s
e
e
[

1
v
8
8
7
3
0
.
2
0
0
2
:
v
i
X
r
a

GENERATING DIVERSE AND NATURAL TEXT-TO-SPEECH SAMPLES USING A
QUANTIZED FINE-GRAINED VAE AND AUTOREGRESSIVE PROSODY PRIOR

Guangzhi Sun1∗ Yu Zhang2 Ron J. Weiss2 Yuan Cao2 Heiga Zen2 Andrew Rosenberg2
Bhuvana Ramabhadran2 Yonghui Wu2

gs534@eng.cam.ac.uk,{ngyugh,ronw,yuancao,heigazen,rosenberg,bhuv,yonghui}@google.com

1University of Cambridge

2Google Inc.

ABSTRACT

Recent neural text-to-speech (TTS) models with ﬁne-grained latent
features enable precise control of the prosody of synthesized speech.
Such models typically incorporate a ﬁne-grained variational autoen-
coder (VAE) structure, extracting latent features at each input token
(e.g., phonemes). However, generating samples with the standard
VAE prior often results in unnatural and discontinuous speech, with
dramatic prosodic variation between tokens. This paper proposes a
sequential prior in a discrete latent space which can generate more
naturally sounding samples. This is accomplished by discretizing
the latent features using vector quantization (VQ), and separately
training an autoregressive (AR) prior model over the result. We
evaluate the approach using listening tests, objective metrics of au-
tomatic speech recognition (ASR) performance, and measurements
of prosody attributes. Experimental results show that the proposed
model signiﬁcantly improves the naturalness in random sample gen-
eration. Furthermore, initial experiments demonstrate that randomly
sampling from the proposed model can be used as data augmentation
to improve the ASR performance.

Index Terms— text-to-speech, Tacotron 2, ﬁne-grained VAE

1. INTRODUCTION

The fast-paced development of neural end-to-end TTS synthesis has
enabled the generation of speech approaching human levels of natu-
ralness [1–4]. Such models directly map an input text to a sequence of
acoustic features using an encoder-decoder network architecture [5].
In addition to the input text, there are many other sources of variation
in speech, including speaker, background noise, channel properties
(e.g., reverberation), and prosody. These attributes can be accounted
for in the synthesis model by learning a latent representation as an
additional input to the decoder [6–9].

Prosody [10] collectively refers to stress, intonation and rhythm
in speech. Annotations for such factors are rarely available for train-
ing. Many recent end-to-end TTS models aiming to capture these
factors extract a latent representation from the target speech, and
factorize the observed attributes, such as speaker and text informa-
tion, out of the prosody latent space [6, 8, 11, 12]. These approaches
extract a single latent variable for an entire utterance, requiring a sin-
gle global representation to capture the full space of variation across
speech signals of arbitrary length. In contrast, the model proposed
in [13] uses a ﬁne-grained structure to encode the prosody associated
with each phoneme in the input sequence from the aligned target spec-
trogram. This system can synthesize speech which closely resembles

∗Work performed while interning at Google Brain.

the prosody of a provided reference speech and control local prosody
by varying the values of corresponding latent features.

There exist applications where generating samples of natural
speech corresponding to the same text with different prosody is desir-
able. For example, samples with diverse prosodic variations could be
useful in data augmentation for ASR where a limited amount of real
speech is available. Using a generative framework, such as VAE [14],
to represent the ﬁne-grained latent variable, naturally enables sam-
pling of different prosody features for each phoneme. The prior over
each latent variable is commonly modeled using a standard Gaussian
distribution N (0, I). Since the prior is independent at each phoneme,
the generated audio often exhibits discontinuous and unnatural arti-
facts such as long pauses between syllables or sudden increases in
energy or fundamental frequency (F0).

A simple way to ameliorate these unnatural samples is to scale
down the standard deviation of the prior distribution during gen-
eration, which decreases the likelihood of sampling outlier values.
However, this also suppresses the diversity of the generated audio
samples and does not eliminate discontinuities since consecutive sam-
ples are still independent. Attempts to introduce temporal correlation
to sequential latent representations [15,16] often adopt autoregressive
decomposition of the prior and posterior distributions, parameteriz-
ing both using neural networks. More recently, [17] introduced the
vector-quantized VAE (VQ-VAE), and a two-stage training approach
for generate high ﬁdelity speech samples, in which the posterior is
trained for reconstruction and an autoregressive (AR) prior is trained
separately to ﬁt the posteriors extracted from the training set.

This paper utilizes a two-staged training approach similar to [17].
We extend Tacotron 2 [4] to incorporate a quantized ﬁne-grained VAE
(QFVAE) where the latent representation is quantized into a ﬁxed
number of classes. We ﬁnd that using a quantized representation
improves the naturalness over audio samples generated from the
continuous latent space, while still ensuring reasonable diversity
across samples. In the ﬁrst stage, the TTS model is trained in a
teacher-forced setting to maximize the likelihood of the training set.
In the second stage, an autoregressive (AR) prior network is trained
to ﬁt the VAE posterior distribution over the training data learned in
the ﬁrst stage. This network learns to model the temporal dynamics
across latent features. Samples characterized by latent features can be
drawn from the AR prior by providing an initial state. We compare
two AR prior ﬁtting schemes, one in the continuous space and another
in the quantized latent space.

We evaluate the proposed model from a few different perspectives.
Sample naturalness and completeness are measured using an ASR
system trained on real speech data in addition to subjective listening
tests evaluating the naturalness of the generated speech. Sample
diversity is evaluated by the average standard deviation per phoneme

To appear in Proc. ICASSP2020, May 04-08, 2020, Barcelona, Spain

c(cid:13) IEEE 2020

 
 
 
 
 
 
2.1. Vector Quantization

Vector quantization in Fig. 2 is performed after the latents are drawn
from the posterior distribution by assigning a quantized embedding
ek to each latent vector zn by minimizing the Euclidean distance.
k = argminj||zn − ej||2

(2)

2

Fig. 1. Fine-grained VAE encoder structure. Location-sensitive
attention is used to align a reference spectrogram with the encoded
phoneme sequence. The latent features z and encoded phonemes are
concatenated and passed to the decoder.

Fig. 2. Illustration of the vector quantization process. The embedding
codebook contains K different embeddings ek. Continuous latents z
are quantized to their nearest neighbors in the codebook by Euclidean
distance. The red line shows the straight-through gradient path.

in three measurable prosody attributes. Lastly, its beneﬁt as a data
augmentation method is demonstrated by the ASR system trained on
audio samples generated from TTS systems.

2. QUANTIZED FINE-GRAINED VAE TTS MODEL

The ﬁne-grained VAE structure used to model the prosody at
phoneme-level, similar to that in [13], is shown in Fig. 1. The VAE
component is integrated with the encoder of the Tacotron-2 model [4]
and the target spectrogram is provided as an extra input to the encoder
in order to extract latent prosody features.

The spectrogram is ﬁrst aligned with the phoneme sequence using
attention [18] according to phoneme encodings from the output of
the encoder. The aligned spectrogram is then sent to the VAE to
extract a sequence of latent representations for the prosody which
is also aligned with the phoneme sequence. Finally, the phoneme
encodings are concatenated with the latent representations and sent
to the decoder. The system is trained by optimizing the ﬁne-grained
evidence lower bound (ELBO) loss:

L(p, q) = Eq(z|X) [log p(X | Y, z)]
− β (cid:80)N

n=1 DKL(q(zn | X, Yn) (cid:107) p(zn)) ,

(1)

where the ﬁrst term is the reconstruction loss and the second term is
the KL divergence between prior and posterior. The prior is chosen
to be N (0, I). z represents the sequence of latent features and zn
corresponds to the latent representation for the n-th phoneme. X is
the aligned spectrogram and Y represents the phoneme encoding.

2

Unlike the original VQ-VAE [17] which used a one-hot posterior
distribution, we maintain the Gaussian form of the posterior from
the standard VAE to make it possible to experiment with continuous
or discrete representations within AR prior. Therefore the phoneme-
level ELBO loss is used to train the VQ-VAE as before. The gradient
from the reconstruction loss term is back-propagated to the latent
encoder by directly copying the gradient from previous layers to
quantized embeddings at each step to the latent vectors in the contin-
uous space. Furthermore, to update the quantized embeddings, the
following quantization and commitment losses are optimized together
with the ELBO:

LVQ = (cid:80)N

n=1

(cid:16)

||sg[zn] − en||2

2 + γ||zn − sg[en]||2
2

(cid:17)
,

(3)

where sg[·] is the stop-gradient operator, which is identity for the
forward path and has zero partial derivatives for the backward path.
The ﬁrst term is the quantization loss which moves the embeddings
towards the latent vectors computed by the encoder, in order to min-
imize the error introduced in quantization. The second term is the
commitment loss which encourages the continuous latent vector to
remain close to the quantized embedding, preventing the embedding
space from expanding too fast. The total loss optimized by the model
is the sum of the VAE loss L(p, q) and the VQ loss LVQ.

3. AUTOREGRESSIVE PROSODY PRIOR

Once the TTS model is trained, the ﬁne-grained VAE encoder shown
in Fig. 1 can be used to compute parameters for the posterior distribu-
tion over latents from a reference spectrogram. Since the posterior is
derived from a real speech spectrogram, samples from it will be natu-
ral and coherent across phonemes. However, without such a reference
spectrogram, the model does not expose a method to generate natural
samples. We therefore train an AR prior to model such temporal
coherency in the latent feature sequence from the posterior. This
AR prior is trained separately without affecting the training of the
posterior. Because the encoder network uses vector quantization after
sampling from an underlying posterior distribution in the continuous
latent space, we compare two different prior ﬁtting schemes.

The ﬁrst scheme aims to ﬁt an Gaussian AR prior in the continu-
ous latent space so that the prior and the posterior at each time step
come from the same family of distributions. A single layer LSTM is
used to model the prior, and is trained using teacher forcing from the
latent feature sequence from the posterior. The output at each step
of the sequence is a diagonal Gaussian distribution whose mean and
standard deviation are functions of the previous latent features. This
prior is also conditioned on the phoneme encoding Y:

p(zn | z<n, Y) = N (zn; µ(z<n, Y), σ(z<n, Y)) ,

(4)

where µ(z<n, Y) and σ(z<n, Y) are outputs of the prior LSTM.
The LSTM is trained by additionally minimizing the same KL diver-
gence in the continuous latent space as Eq. (1) except that the original
prior p(zn) is replaced with p(zn | z<n, Y). During generation, the
network is only provided with the phoneme encoding and an all-zero
initial state. Samples drawn from the continuous AR prior for each
phoneme are quantized using the same embedding space.

Encoder“b  y   t  h  e   w  a  y”Phone alignment… … To decoderVAE123zzDistance CalculationGradientsEmbedding Spacee1e2e3eke8e1e8e3e53e3To decoderAlternatively, the AR prior can be ﬁt directly in the discrete
latent space, in which case the prior at each step is a categorical
distribution over the quantization embeddings space {ek}. This is
similar to training a neural language model [19]. To enable training
the prior with KL divergence in the discrete space, single sample
estimation of the posterior distribution is used, hence the training
objective becomes the cross-entropy loss:

DKL (Q(e) (cid:107) P (e)) = EQ(e)

log

(cid:20)

(cid:21)

Q(e)
P (e)

≈ − log P (ek),

(5)

where Q(e) and P (e) are the discrete posterior and prior multinomial
distributions, respectively, and ek is a single sample drawn from the
posterior distribution. Hence the estimated posterior distribution is
one-hot at embedding ek (i.e. ˜Q(ej) = 1 only when j = k, and 0
otherwise) which yields the cross-entropy loss form. As before, a
single layer LSTM is used to model the prior and the output at each
step is a categorical distribution after a softmax activation function.
The phoneme encoding Y is also used during training and generation.

4. EXPERIMENTS

We evaluated multispeaker TTS models on the LibriTTS dataset [20],
a multispeaker English corpus of approximately 585 hours of read
audiobooks sampled at 24kHz. It covers a wide range of speakers,
recording conditions and speaking styles. We used a model following
[8], replacing the Gaussian mixture VAE (GMVAE) with a QFVAE.
Output audio is synthesized using a WaveRNN vocoder [21].

To evaluate how much information was lost by quantizing the la-
tent prosody representation, we measured reconstruction performance
by encoding a reference signal and using the resulting posterior to
reconstruct the same signal. In addition, we measured the naturalness
of the synthesized speech using subjective listening tests, and speech
intelligibility using speech recognition performance with a pretrained
ASR model. We also evaluated the diversity of the prosody in samples
from different models. A good system should be able to generate
natural audio samples with a relatively large prosody diversity.

Finally, we demonstrated that samples from the proposed model
could be used to augment real speech training data to improve per-
formance of a speech recognition system. We encourage readers to
listen to audio examples on the accompanying web page1.

4.1. Reconstruction Performance

Reconstruction, or copy synthesis, performance is measured using
F0 frame error (FFE) [22] and mel-cepstral distortion (MCD) [23]
computed from the ﬁrst 13 MFCCs, which reﬂect how well a model
captures the pitch and timbre of the original speech, respectively.
Lower values are better for both metrics. Results are shown in Table 1,
illustrating that quantizing the prosody latents in the QFVAE degrades
reconstruction compared to the baseline, since some information is
discarded as part of the quantization process.

Compared to the global VAE model, which used a single 32-
dimensional latent vector to represent the prosody across a full ut-
terance, the baseline ﬁne-grained VAE used a 3-dimensional latent
for each phoneme and achieves a signiﬁcantly better reconstruction
performance. The QFVAE models have higher FFE and MCD than
the baseline, however their performances improve as the number
of classes increased. With 1024 classes QFVAE performance ap-
proaches that of the baseline.

Model

FFE MCD

Global VAE
Baseline ﬁne-grained VAE

QFVAE 32 classes
QFVAE 256 classes
QFVAE 1024 classes

0.52
0.18

0.32
0.26
0.22

16.0
8.6

11.3
10.2
9.5

Table 1. Reconstruction performance for different TTS models. The
number of classes refers to the number of latent embeddings in the
VQ codebook. The global VAE baseline uses a single latent vector to
represent the prosody of an utterance.

Prosody stddev
E F0 Dur.

Model Prior

Real speech

MOS

WER

7.2%

-

Baseline Indep. scale=1.0
94.7% 0.50
Baseline Indep. scale=0.2 2.80 ± 0.08 26.8% 0.35
Baseline Indep. scale=0.0 4.04 ± 0.06 10.6% 0.09

-

Baseline AR continuous
QFVAE Indep.
QFVAE AR discrete
QFVAE AR continuous

2.37 ± 0.07 32.0% 0.48
3.43 ± 0.06 11.9% 0.38
3.45 ± 0.07 11.5% 0.39
3.98 ± 0.06
8.4% 0.29

-

58
38
8

32
32
32
24

-

35
16
8

21
24
16
12

Table 2. Naturalness and diversity metrics computed across samples
from different models. All models use a 3-dimensional ﬁne-grained
VAE and sample independently from a continuous latent at each step
(Indep.) or from an autoregressive (AR) prior over continuous or
discrete latents. The scale factor for the baseline scales the standard
deviation of the prior when sampling. Prosody metrics include rela-
tive energy within each phonene (E), fundamental frequency in Hertz
(F0), and duration in ms (Dur.).

4.2. Sample Naturalness and Diversity

We conducted subjective listening tests over 10 speakers each synthe-
sized 100 utterances with native English speakers asked to rate the
naturalness of speech samples on a 5-point scale in increments of 0.5.
Results are reported in terms of mean opinion score (MOS).

Complementary to naturalness, we also report the word error rate
(WER) from an ASR model trained on real speech from the LibriTTS
training set and evaluated on speech synthesized from transcripts in
the LibriTTS test set2. We used the sequence-to-sequence ASR model
from [24]. This metric veriﬁes that the synthesized speech contains
the full content of the input text. However, even if the full text is
synthesized, we expect the WER to increase for speech samples with
prosody that is very inconsistent with that of real speech.

Finally, we evaluated the diversity of samples from the proposed
prior by measuring the standard deviation of three prosody attributes
computed for each phoneme: relative energy, fundamental frequency
(F0), and duration. The duration of a phoneme is represented by
the number of frames where the decoder attention assigns maximum
values to that phoneme, F0 is computed by the YIN pitch tracker [25],
and the relative energy is the ratio of the average signal magnitude
within a phoneme with the average magnitude of the entire utterance.
The standard deviations for each of these metrics is computed within
each phoneme, and the average standard deviation across all the

1 https://google.github.io/tacotron/publications/prosody_prior

2WER on LibriTTS is different from WER on LibriSpeech.

3

phonemes is reported. To estimate these statistics, we synthesized
100 random samples for each of 3 randomly selected utterances from
the test set, each using 3 different speaker IDs.

Results are shown in Table 2. Comparing independent sampling
strategies from the baseline model, decreasing the scale reduced the
diversity in each attribute, while increasing naturalness MOS and
improving WER. This indicates a trade-off between the diversity and
naturalness that results from naive sampling from the baseline system.
A scaling factor of 0.2 is the largest under which the system still
generated somewhat natural audio samples. Samples using a scale
of 1.0 were too poor (as reﬂected in the very high WER) that we did
not conduct listening tests with them. Fitting an AR prior over the
baseline’s continuous latent space results in worse naturalness than
sampling independently for each phoneme with moderate scale of
0.2, although the diversity becomes higher.

QFVAE samples always result in reasonable MOS and WER
regardless of the type of prior. This indicates the beneﬁt of the reg-
ularization imposed by quantizing the latent space during training,
even if the discrete representation is not used directly by the prior.
The most natural results with highest MOS and lowest WER resulted
from using an AR prior ﬁt in the continuous space. Samples gen-
erated using this prior have MOS close to the baseline with neutral
prosody (scale = 0.0), but with lower WER. However independent
samples had better diversity metrics, once again reﬂecting a similar,
but less pronounced, diversity-quality trade-off to the baseline model.
Finally, the AR prior in the discrete space gives a similar diversity and
naturalness to the independent prior. We conjecture that using a single
sample to estimate the discrete KL divergence brings uncertainty to
the model, resulting in a prior with large variance at each step.

4.3. Data Augmentation

One potential application of the proposed TTS model is to sample
synthetic speech to help training ASR models, in a data augmentation
procedure. We trained ASR models on synthesized audio from dif-
ferent TTS models and evaluated how well the resulting recognizers
generalized when evaluated on real speech.

For each TTS model, we synthesized speech for the full set of
training transcripts in LibriTTS with randomized speaker IDs. The
synthesized speech was downsampled to a rate of 16 kHz and then
used to train the ASR model. We used an end-to-end encoder-decoder
ASR model with additive attention [26]. The model, training strategy,
and associated hyperparameters followed LAS-4-1024 in [27].

As shown in Table 3, the WER of the ASR model when trained
on real LibriTTS speech is 7.2%. Similar to [28], synthesizing speech
using a baseline multispeaker Tacotron 2 model results in a signif-
icantly degraded WER of 20.0%, indicating that samples from this
model did not capture the full space of variation of real speech.

In a copy synthesis setting where the ground truth reference
speech is provided, the global VAE [8] improves on the Tacotron
baseline, and the ﬁne-grained VAE (Baseline) improves even further.
These results indicate the importance of explicitly modeling prosody
variation (VAE models) as well as speaker variation (Tacotron 2).

Randomly sampling independently at each phoneme using the
baseline ﬁne-grained VAE performs signiﬁcantly worse than copy
synthesis. The lowest WER is obtained when the scale is set to an
intermediate value, reﬂecting a reasonable trade-off between natu-
ralness and diversity. As in Table 2, the best QFVAE performance
results from sampling independently at each phoneme.

To explore the improvement from diversity, the best performing
QFVAE was used to generate 10 copies with varying prosody of the
training data to train the ASR system. This gives 12.5% WER which

4

Model

Prior

Real speech
Multispeaker Tacotron 2
GMVAE-Tacotron [8] copy synthesis
Baseline copy synthesis

Baseline
Baseline
Baseline
Baseline
QFVAE
QFVAE
QFVAE

10 samples per transcript

Tacotron 2
Baseline
QFVAE
Real speech + QFVAE

Indep. scale=1.0
Indep. scale=0.2
Indep. scale=0.0
AR continuous
Indep.
AR discrete
AR continuous

Indep. scale=0.2
Indep.
Indep.

WER

7.2%
20.0%
16.4%
11.8%

25.6%
19.9%
31.3%
32.0%
16.9%
17.6%
22.3%

19.8%
13.8%
12.5%
6.0%

Table 3. WER on real speech from the LibriTTS test set for ASR
models trained only on synthesized speech sampled from TTS models.

Training data

test

test-other

Real speech
4.4% 12.4%
Real speech + QFVAE (Indep.) 10 samples 3.8% 11.4%

Table 4. WER on real speech LibriSpeech test sets for ASR models
trained on the combination of real and synthesized speech.

is close to the copy synthesize result. Negligible improvement is
found when adding more copies for Tacotron 2 model since they do
not contain much variation in prosody. Finally, we found that training
on synthetic speech (oversampled ten times) with real speech in a data
augmentation conﬁguration resulted in a 16% relative WER reduction
compared to the model trained on real speech alone.

We repeated the evaluation of the data augmentation experiment
on the LibriSpeech corpus [29] in Table 4. The real speech used in
this comparison was the union of LibriTTS and LibriSpeech train-
ing sets, the TTS model was trained on LibriTTS, which contained
material that was not in LibriSpeech. Using the QFVAE for data
augmentation resulted in relative WER reductions of 14% and 8% on
the LibriSpeech test-clean and test-other sets, respectively.

5. CONCLUSIONS

This paper proposed a quantized ﬁne-grained VAE TTS model, and
compared different prosody priors to synthesize natural and diverse
audio samples. A set of evaluations for naturalness and diversity was
provided. Results showed that the quantization improved the sample
naturalness while retaining a similar diversity. Sampling from an AR
prior further improved the naturalness. When generated samples were
used to train ASR systems, we demonstrated a potential application
that used prosody variations for data augmentation.

6. ACKNOWLEDGEMENTS

The authors thank Daisy Stanton, Eric Battenberg, and the Google
Brain and Perception teams for their helpful feedback and discussions.

7. REFERENCES

[1] J. Sotelo, S. Mehri, K. Kumar, J. Santos, K. Kastner,
A. Courville, and Y. Bengio., “Char2Wav: End-to-end speech
synthesis.,” in Proc. International Conference on Learning
Representations (ICLR), 2017.

[2] Y. Wang, R. J. Skerry-Ryan, D. Stanton, Y. Wu, R. J. Weiss,
N. Jaitly, Z. Yang, Y. Xiao, Z. Chen, S. Bengio, Q. Le,
Y. Agiomyrgiannakis, R. Clark, and R. A. Saurous., “Tacotron:
Towards end-to-end speech synthesis.,” in Proc. Interspeech,
2017, pp. 4006–4010.

[3] W. Ping, K. Peng, A. Gibiansky, S. O. Arik, A. Kannan,
S. Narang, J. Raiman, and J. Miller., “Deep voice 3: 2000-
speaker neural text-to-speech.,” in Proc. International Confer-
ence on Learning Representations (ICLR), 2018.

[4] J. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang,
Z. Chen, Y. Zhang, Y. Wang, and R. J. Skerry-Ryan, “Natural
TTS synthesis by conditioning WaveNet on mel spectrogram
predictions.,” in Proc. ICASSP, 2018, pp. 4779–4783.

[5] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence
learning with neural networks,” in Advances in Neural Informa-
tion Processing Systems, 2014.

[6] R. J. Skerry-Ryan, E. Battenberg, Y. Xiao, Y. Wang, D. Stanton,
J. Shor, R. Weiss, R. Clark, and R. A. Saurous, “Towards
end-to-end prosody transfer for expressive speech synthesis
with Tacotron,” in Proc. International Conference on Machine
Learning (ICML), 2018.

[7] Y. Wang, D. Stanton, Y. Zhang, R. J. S. Ryan, E. Battenberg,
J. Shor, Y. Xiao, Y. Jia, F. Ren, and R. A. Saurous, “Style
tokens: Unsupervised style modeling, control and transfer in
end-to-end speech synthesis,” in Proc. International Conference
on Machine Learning (ICML), 2018, pp. 5167–5176.

[8] W.-N. Hsu, Y. Zhang, R. J. Weiss, H. Zen, Y. Wu, Y. Wang,
Y. Cao, Y. Jia, Z. Chen, J. Shen, P. Nguyen, and R. Pang, “Hier-
archical generative modeling for controllable speech synthesis,”
in Proc. International Conference on Learning Representations
(ICLR), 2019.

[9] S. Ma, D. Mcduff, and Y. Song., “A generative adversarial
network for style modeling in a text-to-speech system,” in Proc.
International Conference on Learning Representations (ICLR),
2019.

[10] M. Wagner and D. G. Watson, “Experimental and theoretical
advances in prosody: A review,” in Language and Cognitive
Processes, 2010, pp. 905–945.

[11] W.-N. Hsu, Y. Zhang, R. J. Weiss, Y.-A. Chung, Y. Wang,
Y. Wu, and J. Glass., “Disentangling correlated speaker and
noise for speech synthesis via data augmentation and adversarial
factorization,” in Proc. ICASSP, 2019.

[12] E. Battenberg, S. Mariooryad, D. Stanton, R. J. Skerry-Ryan,
M. Shannon, D. Kao, and T. Bagby, “Effective use of variational
embedding capacity in expressive end-to-end speech synthesis,”
arXiv: 1906.03402, 2019.

[13] Y. Lee and T. Kim, “Robust and ﬁne-grained prosody control
of end-to-end speech synthesis,” in Proc. ICASSP, 2019, pp.
5911–5915.

[14] D. P. Kingma and M. Welling, “Auto-encoding variational
bayes,” in Proc. International Conference on Learning Repre-
sentations (ICLR), 2014.

5

[15] J. Chung, K. Kastner, L. Dinh, K. Goel, A. Courville, and
Y. Bengio, “A recurrent latent variable model for sequential
data,” in Advances in Neural Information Processing Systems,
2016.

[16] O. Fabius and J. R. van Amersfoort, “Variational recurrent
auto-encoders,” in Proc. International Conference on Learning
Representations (ICLR), 2015.

[17] A. van den Oord, O. Vinyals, and K. Kavukcuoglu, “Neu-
ral discrete representation learning,” in Advances in Neural
Information Processing Systems, 2017.

[18] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,”
in Advances in Neural Information Processing Systems, 2017.
[19] T. Mikolov, M. Karaﬁ´at, L. Burget, J. ˇCernock`y, and S. Khu-
danpur, “Recurrent neural network based language model,” in
Proc. Interspeech, 2010.

[20] H. Zen, V. Dang, R. Clark, Y. Zhang, R. J. Weiss, Y. Jia, Z. Chen,
and Y. Wu, “LibriTTS: A corpus derived from LibriSpeech for
text-to-speech,” in Proc. Interspeech, 2019.

[21] N. Kalchbrenner, E. Elsen, K. Simonyan, S. Noury,
N. Casagrande, E. Lockhart, F. Stimberg, A. Oord, S. Dieleman,
and K. Kavukcuoglu, “Efﬁcient neural audio synthesis,” in
Proc. International Conference on Machine Learning (ICML),
2018, pp. 2415–2424.

[22] W. Chu and A. Alwan, “Reducing F0 frame error of F0 tracking
algorithms under noisy conditions with an unvoiced/voiced
classiﬁcation frontend,” in Proc. ICASSP, 2009.

[23] R. Kubichek, “Mel-cepstral distance measure for objective
speech quality assessment,” in Proc. IEEE Paciﬁc Rim Con-
ference on Communications Computers and Signal Processing,
1993, vol. 1, pp. 125–128.

[24] K. Irie, R. Prabhavalkar, A. Kannan, A. Bruguier, D. Rybach,
and P. Nguyen, “On the choice of modeling unit for sequence-
to-sequence speech recognition,” in Proc. Interspeech, 2019,
pp. 3800–3804.

[25] A. de Cheveign´e and H. Kawahara, “YIN, a fundamental fre-
quency estimator for speech and music,” The Journal of the
Acoustical Society of America, vol. 111, no. 4, pp. 1917–1930,
2002.

[26] C.-C. Chiu, T. N. Sainath, Y. Wu, R. Prabhavalkar, P. Nguyen,
Z. Chen, A. Kannan, R. J. Weiss, K. Rao, E. Gonina, et al.,
“State-of-the-art speech recognition with sequence-to-sequence
models,” in Proc. ICASSP, 2018, pp. 4774–4778.

[27] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D.
Cubuk, and Q. V. Le, “SpecAugment: A simple data aug-
mentation method for automatic speech recognition,” in Proc.
Interspeech, 2019.

[28] J. Li, R. Gadde, B. Ginsburg, and V. Lavrukhin, “Training neural
speech recognition systems with synthetic speech augmentation,”
arXiv preprint arXiv:1811.00707, 2018.

[29] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Lib-
riSpeech: An ASR corpus based on public domain audio books,”
in Proc. ICASSP, 2015, pp. 5206–5210.

