1
2
0
2

r
p
A
2

]

V
C
.
s
c
[

1
v
5
8
0
1
0
.
4
0
1
2
:
v
i
X
r
a

End-to-end learning of keypoint detection and matching for relative pose
estimation

Antoine Fond
Blippar
antoine.fond@blippar.com

Luca Del Pero
Blippar
luca.delpero@blippar.com

Nikola Sivacki
Blippar
nikola.sivacki@blippar.com

Marco Paladini
Blippar
marco.paladini@blippar.com

Abstract

We propose a new method for estimating the relative
pose between two images, where we jointly learn keypoint
detection, description extraction, matching and robust pose
estimation. While our architecture follows the traditional
pipeline for pose estimation from geometric computer vi-
sion, all steps are learnt in an end-to-end fashion, including
feature matching. We demonstrate our method for the task
of visual localization of a query image within a database
of images with known pose. Pairwise pose estimation has
many practical applications for robotic mapping, naviga-
tion, and AR. For example, the display of persistent AR ob-
jects in the scene relies on a precise camera localization
to make the digital models appear anchored to the physical
environment. We train our pipeline end-to-end speciﬁcally
for the problem of visual localization. We evaluate our pro-
posed approach on localization accuracy, robustness and
runtime speed. Our method achieves state of the art local-
ization accuracy on the 7 Scenes dataset.

1. Introduction

Our work ﬁts in the domain of visual localisation, i.e.
the problem of estimating in real-time the camera pose of
a query image within a known map of the environment.
We focus on visual localization methods that use an RGB
database of images with associated pose, and a single RGB
image as query. In this paper, we present a novel method
to jointly solve for keypoints detection, keypoints matching
and camera pose estimation between a database reference
image and a live query image. We assume that the environ-
ment has been mapped using either Simultaneous Localiza-
tion and Mapping (SLAM) or Structure from Motion (SFM)
which recovers the 6 DoF camera pose for each RGB image

and the associated 3D structure in a consistent 3D coordi-
nate system.

Traditional solutions to the problem of relative pose es-
timation rely on four steps: detecting keypoints, comput-
ing their descriptors, a data association step to match key-
points across the two images, and a robust pose estimation
step to compute the pose that minimizes the reprojection of
the matched points, while discarding outliers. Localization
methods typically use hand-crafted features [17, 22, 15].
Recent work has proposed replacing some of those steps
with learning methods [9, 27, 12] or avoid computing key-
points altogether [13, 28, 25]. We propose a novel archi-
tecture that still explicitly performs all the traditional four
steps, but learns them all jointly in an end-to-end fashion.
Our contributions are as follows:

• End-to-end learning of all the traditional four steps of

relative pose estimation

• A novel trainable layer for feature matching

• A self-supervised data augmentation method to gener-

ate ground-truth matches from RGB-D images

We evaluate our method on the indoor localization bench-
mark 7 Scenes [11] which comprises of sequences of regis-
tered RGB-D images for training and for testing in a variety
of environments with a variety of camera trajectories.

2. Related work

Visual

localization is a fundamental problem for
robotics, autonomous vehicles and augmented reality where
the robot or device needs to rely on the current view from
the camera to compute its pose. The goal is to obtain the
camera pose relative to a known map of the environment.
Visual localization methods typically rely on distinctive lo-
cal features or whole-image descriptors [17]. Sattler et al.

1

 
 
 
 
 
 
Figure 1. Overview of the model, images are single-channel grayscale, the superpoint [7] layers (SP) outputs keypoints and descriptor
tensors, the keypoint indices are propagated to the matching layer (ML) trough softargmax, while the descriptors are used in the cross-
correlation layer (CL). The 4D cross-correlation tensor is mapped into a 3D tensor via pseudo-hilbert curve mapping (see Figure 3)

[22] demonstrated the use of 2D-3D matching and robust
pose estimation in large 3D maps. Li et al. [15] use a planet-
wide point cloud with image feature descriptors for place
recognition and precise localization.

Visual localization methods that do not rely on large 3D
maps typically employ image retrieval techniques such as
NetVLAD [1]. Such retrieval methods compute whole-
image descriptors, which allows matching a query image
to a large database of geo-tagged images, without using a
3D model [24]. Camposeco et al. [4] proposed to leverage
both 2D-2D feature matching between images and 2D-3D
matches followed by robust pose estimation.

Machine learning methods have been employed to solve
the keypoint detection and matching problems. For exam-
ple, Han et al. [12] proposed a learning method to jointly
solve feature extraction and matching, Mishchuk et al. [19]
have used deep learning to compute feature descriptors, and
DeTone et al. [7] proposed a network architecture to ex-
tract keypoints and descriptors that could run on devices
with low power consumption. Brachmann et al. [3] solve
the robust pose estimation problem via machine learning
via differentiable RANSAC (DSAC). Alternative localiza-
tion methods that employ end-to-end learning without ex-
plicit feature extraction learn to predict pose directly such
as PoseNet [13]. Shotton et al. [25] propose a localization
method using RGB-D images that does not rely on keypoint
matching, but directly regress the pose of a query image
within a global model of the environment.

Our approach is learning based, but it preserves the
traditional, interpretable steps of keypoint extraction and
matching. We learn this end-to-end by maximising inlier
count and minimising the pose error using a set of train-
ing RGB-D images with known pose. We show that on the
problem of visual localization our method achieves better

speed and robustness than traditional methods.

3. Proposed method

Figure 2. Details of the V-Net [18] encoder-decoder process used
inside the matching layer to compute the matching map

3.1. Architecture

Our novel architecture recovers the pose of a query im-
age Iq given another reference image Ir of the same scene
from which the pose and geometry is known. This typi-
cally requires 4 steps: keypoints extraction, local descrip-
tors computation, matching and robust pose estimation. We
propose a novel CNN architecture for end-to-end relative
pose estimation that explicitly performs the traditional four
steps above. In this section we will show how we integrate
the ﬁrst three steps into one single differentiable network.
Fig. 1 pictures an overview of the network. Both images are
ﬁrst processed through Superpoint [7] to obtain keypoints
and descriptors, then the 2 descriptor tensors are fused using
a correlation layer [8]. From the output of this correlation
layer our novel matching layer produces points correspon-
dences that are used to predict the pose from which the ﬁnal
loss function is computed using [6]. Last, we discuss how
to use our network for localization. (Sec. 3.4)

3.1.1 Superpoint

For the keypoints and descriptors extraction we use the
encoder-decoder architecture from Superpoint [7]. In Su-
perpoint,
the image is subdivided in a regular grid of
cells (patches) each 8 × 8 pixel, and the network extracts
two semi-dense feature maps. The ﬁrst submap K en-
codes the position of the keypoints, the second submap
D their descriptors. Hence, for a grayscale input im-
age of size (H, W ), the outputs of both decoders are re-
8 × 65(cid:1) for the keypoints K (i, j) and
spectively (cid:0) H
(cid:0) H
8 × 256(cid:1) for the descriptor D (i, j). The 65 =
8 × W
8 × 8 + 1 channel dimensions speciﬁes the keypoint po-
sition (or its absence) within each patch. Softmax is applied
channel-wise ignoring the 65th channel in the normalization
for the keypoint map and the descriptor map is normalized
using L2. Moreover each patch is also associated with one
single descriptor of size 256.

8 × W

3.1.2 Correlation layer

Given the query and the reference image we ﬁrst compute
both keypoint and descriptor maps respectively (Kq, Dq)
and (Kr, Dr) using Superpoint [7]. Following the way
matching is done classically we compute the dot product
between all pairs of descriptors from the 2 images. This
operation is differentiable and sometimes referred to as cor-
relation layer [8]. A high correlation score between descrip-
tors can be interpreted as a putative match. In our case this
score is also weighted by the keypoint score K (cid:48) (i, j) =
1 − K (i, j, 65) of both descriptors leading to the following
4D tensor:

Cq,r (i, j, i(cid:48), j(cid:48)) = K (cid:48)

q (i, j) K (cid:48)

r (i(cid:48), j(cid:48)) Dq (i, j)T Dr (i(cid:48), j(cid:48))
(1)

3.1.3 Matching layer

While we could follow the standard matching process by as-
sociating each patch (i, j) with its highest correlated patch
(i(cid:48), j(cid:48)) in Cq,r, we aim to learn the structure of that 4D ten-
sor so to ﬁnd better matches. Our intuition is that a set of
matches is good if they are spatially consistent, for example
two putative matches are more likely to be reliable if their
points land close to each other in both query and reference
image. While this idea of exploiting the spatial relationship
of putative matches has been formalised in previous work
through graph theory [14, 26], we aim to learn the local
structure in that 4D space using a CNN. Thus the matching
layer produces a sparse map by convolving the 4D corre-
lation map with a set of ﬁlters, which we train to generate
peaks at positions (i, j, i(cid:48), j(cid:48)) if (i, j) and (i(cid:48), j(cid:48)) should be
matched together. This has the potential to leverage global

information, while traditional descriptor matching is inher-
ently local.

In practise and for efﬁciency the 4D tensor from the cor-
relation layer is ravelled into a 3D tensor. Thus the indices
(i(cid:48), j(cid:48)) are transformed into a single index k following a 2D
Hilbert’s curve H [29] (Fig. 3) to better preserve spatial lo-
cality (Fig. 4). Instead of expensive 4D convolutions we
can then apply 3D convolutions on that 3D tensor of size
(H, W, H × W ). Actually some keypoints in image Iq may
have no matching keypoint in image Ir. This situation is
handled with an extra dustbin class (or ”no-match” class)
in the matching layer output M . The dimensions of that
matching map M are then (H, W, H × W + 1). Process-
ing 3D convolutions does not change the number of chan-
nels so we add an extra zeros-ﬁlled dimension to the input
correlation map to be of the same size (H, W, H × W + 1).

Figure 3. Pseudo-Hilbert’s curve H for H = 15, W = 20

To capture the local structure at different scale we use a
similar encoder-decoder architecture as in Vnet [18] which
is a generalisation of Unet [20] to 3D tensors. The connec-
tions between the encoder and decoder stages of the same
resolution enable ﬁnegrained details in the matching map
which is important for the overall accuracy. The networks
details can be seen in ﬁgure Fig. 2.

At this stage peaks in 3D tensors represent keypoints
and matches. To be able to train the network end-to-end
so that an actual pose can be predicted from those key-
points and matches we need an additional step to convert
the peaks into correspondences between points coordinates
(kp (i, j) , kp(cid:48) (i, j)). This is done here using softargmax
[5] and through the following steps:

kp (i, j)x = 8 × i +

(cid:88)

msK (m, n, i, j)

kp (i, j)y = 8 × j +

m,n
(cid:88)

m,n

nsK (m, n, i, j)

(2)

with sK (m, n, i, j) = K (i, j, 8 × m + n)

Lpose (M (cid:48)) = ˜eT X T M (cid:48)X ˜e

(4)

To avoid the trivial solution where all weights drop to
zero, we use instead a differentiable version of the num-
ber of inliers using a sigmoid function σ.
Indeed as we
assumed the depth and global pose of both images to be
known, we can compute the reprojection of the keypoints
from the reference image kp(cid:48) (i, j) into the query image.
Let Ccalib denotes the camera calibration matrix and Pr,q =
(Rr,q|Tr,q) the relative pose between the images then the re-
projection can be written R (kp(cid:48) (i, j)) = CcalibPr,qkp(cid:48)
n =
CcalibPr,q

depthr (i, j).

C−1
(cid:107) C−1

calibkp(cid:48)(i,j)
calibkp(cid:48)(i,j)(cid:107)

Linliers (W ) = exp (−κsi)
(cid:88)

with si =

M (cid:48) (i, j) σ (τ − (cid:107) kp (i, j) − R (kp(cid:48) (i, j))(cid:107))

Figure 4. Example of local maxima in the 3D correlation map,
First row: Computing the correlation map using Superpoint (SP)
and the correlation layer (CL). Bottom row: local maxima in the
correlation map. Left: line-by-line raveling, right: Hilbert’s curve
raveling shown in Figure 3

kp(cid:48) (i, j)x =

kp(cid:48) (i, j)y =

(cid:88)

i(cid:48),j(cid:48)
(cid:88)

i(cid:48),j(cid:48)

sM (i, j, i(cid:48)j(cid:48)) kp (i(cid:48), j(cid:48))x

sM (i, j, i(cid:48)j(cid:48)) kp (i(cid:48), j(cid:48))y

(3)

with sM (i, j, i(cid:48)j(cid:48)) =

exp (M (i, j, H (i(cid:48), j(cid:48))))
(cid:80)H×W
exp (M (i, j, n))
n=1

3.2. Losses

The last part of a keypoint-based method for localiza-
tion generally aims to ﬁnd the pose from the set of cor-
respondences. As some correspondences can be outliers
this step is usually done using a robust estimator such as
RANSAC [10]. In [6], the authors proposed a practical al-
ternative to DSAC [3] to make that loss differentiable. The
loss is derived from the solution of the Weighted Direct Lin-
ear Transform and made of 3 components: ˜e that depends
on the ground-truth relative pose (Rr,q, Tr,q), X that de-
pends on the correspondences qi,j = (kp (i, j) , kp(cid:48)
n (i, j))
and a weight for each correspondence. The formula are de-
scribed in the section about the PnP problem from [6] us-
ing the same notations as here. We adapted that loss to
our problem by deﬁning the correspondences weights as
M (cid:48) (i, j) = 1 − M (i, j, H × W + 1) (i.e.
the inverse of
the dustbin class) and by dropping the regularization term.

i,j

(5)

Eventually, we noticed that without any additional con-
straint keypoints tend to converge to a dense solution where
there is a keypoint in each patch centered in the middle of
the patch even if it means losing some accuracy in pose. To
overcome that we added a unitary term that enforce key-
points to be close to the keypoint position detected by Su-
perpoint [7]. That additional loss Lkeypoints is the sum of the
cross-entropy loss on the keypoints map Kq and Kr with
Superpoint’s [7] keypoints being the ground-truth.

The ﬁnal loss is a weighted sum of those losses with
hyperparameters α, β. Empirically we have noticed that
α = β = 2 give good results. We also use κ = 0.1 and
τ = 16 for the other hyperparameters.

Lﬁnal = Lpose + αLinliers + βLkeypoints

(6)

3.3. Training

3.3.1 Pairs generation

To train our end-to-end approach we need to generate pairs
(Iq, Ir) with known relative pose and depth. We use the
Stanford 2D-3D Semantic dataset [2] that provides 70496
RGB images with corresponding depth and global pose.
46575 were used for training (area 2,4 and 5) and the rest for
testing. We propose here a procedure to ﬁnd pairs of them
(with their relative pose) that are suitable for training our
method. Note that, since this dataset contains only global
poses, we cannot just use all possible pairs in the dataset
(e.g. we cannot use a pair where the images are in different
rooms, or have no overlap in the ﬁeld of view). Here we
outline a procedure to ﬁnd pairs suitable for training.

For each training image It we then use the Pitts250k pre-
trained NetVLAD [1] to retrieve the 64 closest images Ir.
As the global pose is known for each image, we ﬁrst discard

246810122.55.07.510.012.515.017.5050100150200250300246810122.55.07.510.012.515.017.5050100150200250the poses among those 64 retrievals that are incompatible
with the current training image. Two poses are incompati-
ble if their relative distance is more than 20m and if there
is no intersection between the 2 ﬁelds of view. The use of
a retrieval step beforehand instead of simply checking the
incompatibility between all pairs is twofold. First it deals
with occlusions as some parts may not be visible (because
of walls or furnitures) even if their pose are compatible.
Second, it biases the training to be closer to the localiza-
tion use-case where we ﬁrst get the references images from
image retrievals using NetVLAD [1].

Eventually we remove images pairs which have very
close poses ((cid:107) Tt,r(cid:107) ≤ 0.5m and ∠Rt,r ≤ 5◦) to force the
network to solve non trivial cases for pose estimation (i.e.
wide baseline).

3.3.2 Scene adaptation

Instead of using Superpoint’s [7] keypoints as ground-truth
for the keypoint unitary loss Lkeypoints, we propose to ex-
tend the concept of Homography Adaptation introduced
in Superpoint [7] to produce more repeatable keypoints
over different viewpoints. We call that self-supervision
scheme Scene adaptation and it simply consists of aggre-
gating points from reprojections of different viewpoints of
the scene using ground-truth poses and depth rather than
through random homography.

In practise we use the same generated training pairs for
the different viewpoints. However it can happen that a key-
point visible in one image is not visible in the other because
of occlusion. We discard those situations by comparing
the depth of the reprojected keypoint depthq (CcalibPr,qkp(cid:48)
n)
with its z-value before reprojection (0, 0, 1) .Pr,qkp(cid:48)
n.

Following the original idea of Homography Adaptation,
these new keypoints are iteratively updated during training.
It means that every 20 epochs, we recompute the keypoints
ground-truth used in Lkeypoints by aggregating across view-
points the keypoints extracted using the last learned state of
the network.

3.3.3 Implementation details

We use Pytorch framework with ADAM solver for learning.
We run the optimization over 100 epochs with a learning
rate of 10−5 and mini-batches of size 32. To overcome the
non-linearity of the softmax layers and sigmoid function,
we ﬁrst train the network with a learning rate multiplier of
0 on the keypoints encode-decoder part initialized with Su-
perpoints’s [7] weights. These weights being ﬁxed the net-
work is forced to focus on learning the ﬁlters of the match-
ing layer. Then in a second phase, the network is trained
end-to-end (i.e. including the keypoints) with the weights
of the matching layer initialized to those of that ﬁrst phase.

3.4. Pose reﬁnement

To use our network on the problem of localising a query
image in a database of images with known pose, as in most
retrieval methods based on keypoints, we ﬁrst ﬁnd the clos-
est N = 16 putative database images to the query using
NetVLAD [1]. We then predict the pose between the query
and each putative image using our network, and keep the
pose Pmax with most inliers.

At this stage, we perform a further reﬁnement step,
where we add correspondences from the putative im-
ages whose predicted pose is not too far from Pmax (i.e.
(cid:107) Tr − Tmax(cid:107) ≤ 1m). Last, we recompute the pose on
this larger set of matches using P3P-RANSAC and reﬁne it
by non-linear optimization on the inliers using Levenberg-
Marquardt. This step improves the accuracy of our method,
and has a low computational cost because our network
extracts few keypoints with a high inlier ratio on which
RANSAC is very efﬁcient.

4. Experiments

4.1. Results on relative pose estimation

We ﬁrst evaluate our architecture on relative pose esti-
mation on a test set from Stanford 2D-3D Semantic dataset
[2]. We generate 50000 image pairs from the 23921 images
of areas 1,3,6. In this experiment we aim to show that our
matching layer improves matching compared to the stan-
dard matching process of keypoint-based methods. For this,
we use standard keypoint matching as a baseline, i.e. we
match keypoints in the ﬁrst image to keypoints in the second
image with minimum descriptor distance and under Lowe’s
ratio constrains. Note that also the baseline uses Superpoint
[7] for keypoints and descriptors for fair comparison. We
also compare to Superpoint without its post-process step.
Superpoint [7] uses a post-processing step that, unravels the
keypoint map to the size of the input image and applies Non
Maximum Suppression (NMS) to extract the keypoints af-
ter the forward pass. The descriptors are also interpolated at
those locations from the neighbouring patches. We compare
to Superpoint with and without the post-processing step.
We note that post-processing takes 20% of the running-time
and, since it is not differentiable, it cannot beneﬁt from end-
to-end learning.

We choose two different metrics to evaluate the improve-
ment of our matching layer. The ﬁrst is the inliers-ratio α.
An inlier is a keypoint kp whose the corresponding key-
point kp(cid:48) in the other image is reprojected sufﬁciently close
to kp ((cid:15) ≤ 8 pixels) using the ground-truth relative pose
and depth. The inliers-ratio is the ratio between the number
of inliers and the total number of correspondences. Table 1
shows that our method predict much more reliable matches
with an average inliers-ratio of α = 0.76 which is more than
2 times higher than Superpoints [7] with standard matching

(Fig. 5). That allow us to make only about 16 RANSAC
iterations instead of about 400 with the same conﬁdence of
ﬁnding the inliers.

The second metric is the error between the ground truth
relative pose and the pose estimated from the correspon-
dences. The pose is estimated using P3P-RANSAC and
reﬁned on inliers using non-linear optimization. N% cor-
responds to the percentage of poses that have been esti-
mated by P3P-RANSAC (i.e. pairs with at least 4 corre-
spondences). Table 1 shows here that our method is more
robust as it always gives at least 4 matches while Super-
points fails in 4% of the cases. ra and rm are respectively
the average and median error of the rotation in degree. Like-
wise ta and tm are respectively the average and median er-
ror of the translation in meters. With few number of inliers
and a small inliers-ratio, RANSAC may fail to estimate the
best pose, all samples being equivalent. However the better
matches of our methods enables a more accurate estimation
in average. Also thanks to the better localized keypoints,
our method also outperforms the baseline in accuracy when
RANSAC succeeds which is measured with the gain in me-
dian error. That gain in accuracy is even bigger if we con-
sider Superpoint [7] without its post-process.

ours
0.76
α
100
N%
5.06◦
ra
rm 0.73◦
1.8
ta
0.06
tm

SP [7] w NMS
0.37
96
11.66◦
0.76◦
49
0.07

SP [7] wo NMS
0.36
94
24.54◦
1.23◦
∞
0.12

Table 1. Results on Stanford 2D-3D pairs (Sec. 4.1). α: average
inlier ratio, N%: percentage of queries estimated (i.e. pairs with
at least 4 correspondences) ra, rm: average and median rotation
error respectively (degrees). ta, tm: average and median transla-
tion error in meters. Superpoint performance improve with a Non
Maximum Suppression (NMS) Our method shows better accuracy
as well as robustness.

4.2. Results on localisation

We also evaluate our method using our efﬁcient localiza-
tion pipeline (Sec. 3.4) on the 7scenes dataset [11] which is
a standard benchmark for indoor localization. The dataset
consists of images with associated pose and depth from sev-
eral tracks. It is split into a training set that we use for as
references and a test set we use for queries.

The pipeline takes query image Iq as input and ﬁrst re-
trieve the N closest references {Ir,j}1≤i≤N . Knowing the
depth and global pose of those retrieval references, we com-
pute the global pose of the query using P3P-RANSAC us-
ing the keypoint correspondences predicted by our method.
Thus we select either the query global pose from the one
corresponding to the maximum number of inliers among

the retrieval or with pose reﬁnement (PR) to measure the
impact of that last step. We constrains the pipeline to make
it practicable for a real-time application. Particularly we use
a small number of retrieval N = 16 and very low-resolution
images (160 × 120).

For fair comparisons we focused on methods that shows
real-time compatibility. We speciﬁcally compare to other
keypoint-based approaches to estimate the pose within the
same pipeline. That includes Superpoint with standard
matching, ORB [21] and SIFT [16] with Lowe’s criteria on
matches. SIFT is also used for direct 2D-3D matching with
ActiveSearch [23]. For approaches that extracts SIFT or
ORB features we use the full resolution image (640 × 480)
as with low-resolution there were to many failure cases. We
also compare to deep-learning methods that are fast but re-
quires a speciﬁc training per scene when in our case the
same network is used for all the 7 scenes.

The poses from Stanford 2D-3D dataset [2] are very bi-
ased. Because they comes from sparsely located panorama
there is much less variation in translation than in rotations.
To compensate for that lack of data, we ﬁnetune our net-
work on the concatenated training data of 7Scenes [11]. Our
method shows competitive results on 7 scenes [11] (Tables 2
and 3). We especially outperforms all other keypoint-based
methods even with low-resolution input both in accuracy
(median error) and robustness (average error).

The efﬁciency of the method is measured by its average
running time of 167 ms per query. This is possible by the
use of low-resolution images which is also important for an
online localization service as it reduces data transfer. We
also make use of the ability of the framework to process
the 16 retrieved images in parallel by loading them into one
batch.

5. Conclusions

We have proposed an end-to-end learning method that
explicitly includes the 4 standards part of a keypoint-
based localization pipeline: keypoints extraction, descrip-
tion, matching and pose estimation. Our method achieves
state-of-the-art results on standard benchmark for local-
ization among keypoint-based methods, signiﬁcantly out-
performing other end-to-end learning approaches. Its efﬁ-
ciency makes it suitable for real-time applications, for ex-
ample Augmented Reality. We note that it can be very pow-
erful when combined with odometry methods like Visual
Inertial Odometry.

Our method requires poses and depth for training. While
we evaluated on RGB-D data, note that our method could
be used also on RGB datasets after applying dense SLAM
or SfM methods.
In future work, we will investigate
whether out network can be trained in an unsupervised
fashion from data without explicit pose and depth ground
truth.

Figure 5. Examples of keypoint matching (Sec 4.1) with Superpoints (left) and our method (right). The inliers from the best pose estimated
by RANSAC are in green and the outliers in red.

NV+ours+PR NV+ours NV+SP[7] NV+ORB[21] NV+SIFT[16] AS [23]

PoseNet[13] DSAC[3]

3/5
4/8
2/4
4/9
7/76
4/11
13/114

Chess
Fire
Heads
Ofﬁce
Pumpkin
Red. kit.
Stairs

4/12
6/13
6/∞
7/∞
11/∞
7/∞
21/∞
Table 2. Median and average translation errors on the 7Scenes [11]. The values are respectively median error in translation (cm) / average
error in translation (cm). NV stands for NetVLAD [1], PR is our pose reﬁnement step, SP stands for Superpoint [7] and AS for ActiveSearch
[23]

4/7
5/12
3/5
5/9
9/154
5/16
16/224

5/7
6/11
3/5
6/9
11/200
6/20
16/∞

2/−
4/−
3/−
4/−
5/−
5/−
117/−

3/5
5/9
5/∞
5/∞
10/∞
6/∞
15/∞

13/−
27/−
17/−
19/−
26/−
23/−
35/−

4/−
3/−
2/−
9/−
8/−
7/−
3/−

Chess
Fire
Heads
Ofﬁce
Pumpkin
Red. kit.
Stairs

NV+ours+PR NV+ours NV+SP[7] NV+ORB[21] NV+SIFT[16] AS [23]
1.9/−
1.5/−
1.4/−
3.6/−
3.1/−
3.3/−
2.2/−

1.6/5.1
2.4/9.5
2.4/9.5
2.2/9.7
2.7/21.6
2.2/13.3
5.1/28.2
Table 3. Median and average rotation errors on the 7Scenes [11]. The values are respectively median error in rotation (degrees) / aver-
age error in rotation (degrees). NV stands for NetVLAD [1], PR is our pose reﬁnement step, SP stands for Superpoint [7] and AS for
ActiveSearch [23]

1.2/1.8
2.1/5.1
1.4/2.8
1.4/2.8
2.4/15.7
1.8/5.5
4.3/14.2

1.9/2.7
2.5/3.9
2.4/3.6
1.8/2.6
2.7/8.7
1.9/8.4
4.4/11.7

1.3/1.9
1.8/3.9
1.7/3.8
1.5/2.2
2.2/5.4
1.5/3.8
3.5/8.4

1.2/−
1.5/−
2.7/−
1.6/−
2.0/−
2.0/−
33.1/−

4.5/−
11.3/−
13.0/−
5.5/−
4.5/−
5.3/−
12.4/−

1.8/2.8
2.3/4
2.1/3.7
1.8/2.9
2.8/6.7
1.8/5.3
3.9/12

PoseNet[13] DSAC[3]

References

[1] R. Arandjelovic, P. Gronat, A. Torii, T. Pajdla, and J. Sivic.
Netvlad: Cnn architecture for weakly supervised place
recognition. In Conference on Computer Vision and Pattern
Recognition (CVPR), pages 5297–5307, 2016. 2, 4, 5, 7

[2] I. Armeni, S. Sax, A. R. Zamir, and S. Savarese. Joint 2d-3d-
semantic data for indoor scene understanding. arXiv preprint

arXiv:1702.01105, 2017. 4, 5, 6

[3] E. Brachmann, A. Krull, S. Nowozin, J. Shotton, F. Michel,
S. Gumhold, and C. Rother. Dsac-differentiable ransac for
camera localization. In Conference on Computer Vision and
Pattern Recognition (CVPR), volume 3, 2017. 2, 4, 7

[4] F. Camposeco, A. Cohen, M. Pollefeys, and T. Sattler. Hy-
brid Camera Pose Estimation. In Conference on Computer
Vision and Pattern Recognition (CVPR), 2018. 2

International Conference on Medical image computing and
computer-assisted intervention, pages 234–241. Springer,
2015. 3

[21] E. Rublee, V. Rabaud, K. Konolige, and G. Bradski. Orb:
In Computer Vi-
An efﬁcient alternative to sift or surf.
sion (ICCV), 2011 IEEE international conference on, pages
2564–2571. IEEE, 2011. 6, 7

[22] T. Sattler, B. Leibe, and L. Kobbelt. Fast Image-Based Lo-
calization using Direct 2D-to-3D Matching. In International
Conference on Computer Vision (ICCV), 2011. 1, 2

[23] T. Sattler, B. Leibe, and L. Kobbelt. Efﬁcient & effective pri-
oritized matching for large-scale image-based localization.
IEEE Transactions on Pattern Analysis & Machine Intelli-
gence, (9):1744–1756, 2017. 6, 7

[24] T. Sattler, A. Torii, J. Sivic, M. Pollefeys, H. Taira, M. Oku-
tomi, and T. Pajdla. Are large-scale 3d models really neces-
sary for accurate visual localization? In 2017 IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
pages 6175–6184, July 2017. 2

[25] J. Shotton, B. Glocker, C. Zach, S. Izadi, A. Criminisi, and
A. Fitzgibbon. Scene coordinate regression forests for cam-
era relocalization in rgb-d images. IEEE, June 2013. 1, 2
[26] L. Torresani, V. Kolmogorov, and C. Rother. Feature corre-
spondence via graph matching: Models and global optimiza-
In European Conference on Computer Vision, pages
tion.
596–609. Springer, 2008. 3

[27] S. Zagoruyko and N. Komodakis. Learning to compare im-
age patches via convolutional neural networks. In The IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), June 2015. 1

[28] H. Zhan, R. Garg, C. Saroj Weerasekera, K. Li, H. Agarwal,
and I. Reid. Unsupervised learning of monocular depth es-
timation and visual odometry with deep feature reconstruc-
tion. In The IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR), June 2018. 1

[29] J. Zhang, S.-i. Kamata, and Y. Ueshige. A pseudo-hilbert
scan algorithm for arbitrarily-sized rectangle region. In Ad-
vances in Machine Vision, Image Processing, and Pattern
Analysis, pages 290–299. Springer, 2006. 3

[5] O. Chapelle and M. Wu. Gradient descent optimization
of smoothed information retrieval metrics. Information re-
trieval, 13(3):216–235, 2010. 3

[6] Z. Dang, K. M. Yi, Y. Hu, F. Wang, P. Fua, and M. Salz-
mann. Eigendecomposition-free training of deep networks
with zero eigenvalue-based losses. In European Conference
on Computer Vision (ECCV), 2018. 2, 4

[7] D. DeTone, T. Malisiewicz, and A. Rabinovich. Super-
point: Self-supervised interest point detection and descrip-
tion. CoRR, abs/1712.07629, 2017. 2, 3, 4, 5, 6, 7

[8] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas,
V. Golkov, P. Van Der Smagt, D. Cremers, and T. Brox.
Flownet: Learning optical ﬂow with convolutional networks.
In Proceedings of the IEEE International Conference on
Computer Vision, pages 2758–2766, 2015. 2, 3

[9] M. Dymczyk, I. Gilitschenski, J. Nieto, S. Lynen, B. Zeisl,
and R. Siegwart. Landmarkboost: Efﬁcient visual context
classiﬁers for robust localization, 2018. 1

[10] M. A. Fischler and R. C. Bolles. Random sample consen-
sus: a paradigm for model ﬁtting with applications to image
analysis and automated cartography. Communications of the
ACM, 24(6):381–395, 1981. 4

[11] B. Glocker, S. Izadi, J. Shotton, and A. Criminisi. Real-time
rgb-d camera relocalization. IEEE, October 2013. 1, 6, 7
[12] X. Han, T. Leung, Y. Jia, R. Sukthankar, and A. C. Berg.
Matchnet: Unifying feature and metric learning for patch-
In 2015 IEEE Conference on Computer
based matching.
Vision and Pattern Recognition (CVPR), pages 3279–3286,
June 2015. 1, 2

[13] A. Kendall, M. Grimes, and R. Cipolla. Posenet: A convo-
lutional network for real-time 6-dof camera relocalization.
In The IEEE International Conference on Computer Vision
(ICCV), December 2015. 1, 2, 7

[14] M. Leordeanu and M. Hebert. A spectral technique for corre-
spondence problems using pairwise constraints. In Computer
Vision, 2005. ICCV 2005. Tenth IEEE International Confer-
ence on, volume 2, pages 1482–1489. IEEE, 2005. 3
[15] Y. Li, N. Snavely, D. Huttenlocher, and P. Fua. Worldwide
pose estimation using 3d point clouds. In European confer-
ence on computer vision, pages 15–29. Springer, 2012. 1,
2

[16] D. G. Lowe. Distinctive image features from scale-invariant
International Journal of Computer Vision,

keypoints.
60(2):91–110, Nov 2004. 6, 7

[17] S. Lowry, N. S¨underhauf, P. Newman, J. J. Leonard, D. Cox,
P. Corke, and M. J. Milford. Visual place recognition: A sur-
vey. IEEE Transactions on Robotics, 32(1):1–19, Feb 2016.
1

[18] F. Milletari, N. Navab, and S.-A. Ahmadi. V-net: Fully
convolutional neural networks for volumetric medical im-
age segmentation. In 3D Vision (3DV), 2016 Fourth Inter-
national Conference on, pages 565–571. IEEE, 2016. 2, 3

[19] A. Mishchuk, D. Mishkin, F. Radenovic, and J. Matas. Work-
ing hard to know your neighbor’s margins: Local descriptor
learning loss. CoRR, abs/1705.10872, 2017. 2

[20] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convo-
In
lutional networks for biomedical image segmentation.

