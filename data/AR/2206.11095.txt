A High Resolution Multi-exposure Stereoscopic
Image & Video Database of Natural Scenes

Rohit Choudhary 1, Mansi Sharma 1, and Aditya Wadaskar 1

1

2
2
0
2

n
u
J

2
2

]

V
C
.
s
c
[

1
v
5
9
0
1
1
.
6
0
2
2
:
v
i
X
r
a

Abstract—Immersive displays such as VR headsets, AR glasses,
Multiview displays, Free point televisions have emerged as a
new class of display technologies in recent years, offering a
better visual experience and viewer engagement as compared
to conventional displays. With the evolution of 3D video and
display technologies, the consumer market for High Dynamic
Range (HDR) cameras and displays is quickly growing. The
lack of appropriate experimental data is a critical hindrance for
the development of primary research efforts in the ﬁeld of 3D
HDR video technology. Also, the unavailability of sufﬁcient real-
world multi-exposure experimental dataset is a major bottleneck
for HDR imaging research,
thereby limiting the quality of
experience (QoE) for the viewers. In this paper, we introduce
a diversiﬁed stereoscopic multi-exposure dataset captured within
the campus of Indian Institute of Technology Madras, which
is home to a diverse ﬂora and fauna. The dataset is captured
using ZED stereoscopic camera and provides intricate scenes
of outdoor locations such as gardens, roadside views, festival
venues, buildings and indoor locations such as academic and
residential areas. The proposed dataset accommodates wide depth
range, complex depth structure, complicate object movement,
illumination variations, rich color dynamics, texture discrepancy
in addition to signiﬁcant randomness introduced by moving
camera and background motion. The proposed dataset is made
publicly available to the research community. Furthermore, the
procedure for capturing, aligning and calibrating multi-exposure
stereo videos and images is described in detail. Finally, we
have discussed the progress, challenges, potential use cases and
future research opportunities with respect to HDR imaging, depth
estimation, consistent tone mapping and 3D HDR coding.

Index Terms—Display technology, HDR video, Stereoscopic

images, 3D video database, 3D TV

I. INTRODUCTION

The vast research in immersive visual media aims to pro-
vide the viewer’s realistic experience. Immersive technology
creates a sense of immersion by blurring the lines between
the actual, virtual, and simulated worlds [41]. The ISO/IEC,
JCT-3V, ITU-T, MEPG/VCEG industry and research groups
are focusing their efforts on building markets for immersive
HDR/WCG video, UltraHD,
light ﬁeld displays, 3D TVs,
omnidirectional 360◦ video, Free viewpoint television (FTV)
and Mobile 3D apps [4]. Extracting 3D depth information,
3D content development and optimization, quality evaluations
of virtual objects, and digital hologram patterns are all ex-
amples of immersive media approaches. With the release of
the AVC (Advanced Video Coding), HEVC (High Efﬁciency
Video Coding) and following advances in coding systems

1Department

of

Electrical

Engineering,

Indian

of
Technology Madras,
ee20s002@smail.iitm.ac.in,
sisharma@ee.iitm.ac.in, ay.wadaskar@gmail.com)

Tamil Nadu,

mansisharmaiitd@gmail.com,

600036,

India

Institute
(e-mail:
man-

such as MVC (Multiview Video Coding), 3D-HEVC, MV-
HEVC, JPEG Pleno and NGVC (Next-Generation Video Cod-
ing technology), immersive media technologies are attaining
momentum. As several of these technologies are still in their
early stages, the lack of publicly available 3D HDR datasets
is a signiﬁcant roadblock to further research and development.
With the rise in demand for realistic and immersive mul-
timedia, high dynamic range (HDR) imaging has recently
received much attention from academia and industry. The
ultimate aim is to provide the end-users a close to natural
quality of experience (QoE). Typically natural scenes have
a very high range of luminance. The dynamic range of an
image is determined as the ratio of the maximum luminance
to the minimum luminance. In real scenes, the dynamic range
of irradiance may exceed up to 100,000,000:1. Depending
upon certain circumstances, the human eye can detect dynamic
ranges varying from 10,000:1 to 1,000,000:1, whereas a con-
ventional display can exhibit a low dynamic range of 100:1 to
300:1 [40].

Low dynamic range (LDR) images represent

luminance
in 256 steps, i.e., from 0 to 255 using 8-bits. As a result,
pixel values outside this range are replaced with 0 or 255,
resulting in loss of details. HDR imaging signiﬁcantly reduces
this contrast
loss by allocating additional bits to indicate
luminance, hence expanding the luminance dynamic range
close to Human Visual System (HVS) perceived gamut. Many
algorithms are there to construct HDR images from multiple
LDR images. With the help of ﬂoating-point capabilities on
graphics cards, the visual industries are fast transitioning to
an HDR rendering pipeline. Traditional display devices are
not designed to provide the required luminance range for a
full HDR experience. Also, they fail to display ﬂoating-point
data [3]. Consequently, to prepare HDR images for display on
regular screens, dynamic range reduction (i.e., tone mapping)
is done. During tone mapping most of the color and brightness
information is preserved. Additionally, fusion of SDR images
at varying exposure levels gives an output SDR image that
contains more details than input images. Unlike tone mapping,
this approach does not need the creation of an HDR irradiance
map at all.

Despite a surge in scientiﬁc interest for HDR imaging,
its industrial use is still limited, majorly due to the lack of
standard HDR image/video coding schemes [36]. The lack of
publicly available HDR databases hinder the research growth
in HDR computational photography, HDR video compression,
HDR quality assessment, HDR image coding, and HDR video
coding.

While HDR technology adds a more realistic gamut of light

 
 
 
 
 
 
2

TABLE I
RELEVANT FEATURES OF THE ZED CAMERA

Dimensions

Video mode &
output resolution

Depth

Lens

Sensors

Camera controls

Connectivity

SDK System
requirements

175 × 30 × 33mm
HD2K: 2208 × 1242 (15 FPS)
HD1080:1920 × 1080 (30, 15 FPS)
HD720: 1280 × 720 (60, 30, 15 FPS)
WVGA: 672 × 376 (100, 60, 30, 15 FPS)
Depth range: 0.5m to 20m
Format: 32 bits
Baseline: 120mm
Field of View: 110◦
f /2.0 aperture
Sensor size: 1/3
Pixel Size: 2µ pixels
Format: 16 : 9
Adjust: Contrast, Frame rate, Gamma,
Brightness, Resolution, Saturation,
Sharpness, Exposure & White Balance
USB 3.0 (5V /380mA)
0◦C to +45◦C
Linux or Windows
Dual−core 2.3 GHz
4 GB RAM
Nvidia GPU

Banitalebi-Dehkordi [5] used two commercially available side-
by-side HDR cameras to produce HDR stereo data.

Encouraged by SHDR video database released by [5], we in-
troduce a large-scale public database of multi-exposure stereo
images and videos featuring complex natural scenes. The
database comprises of diverse ﬂora and fauna, water streams,
irregular reﬂective surfaces, dynamic background giving an
impressive composition which is rich in texture, illumination
variations, colors and details. It’s worth noting that we address
the problems associated with HDR reconstruction using multi-
exposure images or frames, stereo depth processing for 3D
creation and 3D HDR reconstruction. Also relevant research
areas for interested readers. As the research area of 3D HDR
video becomes more popular, reliable benchmark datasets of
natural scenes are required for further study. The proposed
dataset is available at [1].

A shorter conference version to lay the foundation of this
dataset is published at IEEE IC3D 2019 [75]. In this journal
extension, we elaborate on the features of the proposed dataset
with an extensive description of the intricate details of the
captured scenes. Section II discusses the process of creation
of the proposed database in detail. It includes speciﬁcation,
conﬁguration and calibration of the ZED camera, followed
by the description of database capturing process, format and
rectiﬁcation. Section III gives an elaborate explanation of
the intricacy and diversity of the database. Section IV gives
insight into the progress, use cases, challenges and potential
research opportunities in the domain of HDR image/video
reconstruction, depth estimation for 3D content generation,
tone mapping and dynamic range compression. As a result,
extensive analysis of hindrance and future research prospects
in the ﬁeld of 3D-HDR are discussed. Section V presents
the conclusion and discuss the implications of the proposed
database for research and development IN 3D HDR technolo-
gies.

Fig. 1. The ZED Camera is used for scene acquisition.

and colour, 3D technologies adds more realism to the viewing
experience by introducing the depth. 3D reconstruction is
essential in applications like smart robotics, virtual reality,
augmented reality and autonomous driving. The process of
creating 3D videos is usually divided into three categories.
First, 3D content can be collected using two or more syn-
chronised cameras. Second, 3D video can be produced from
2D videos using post-production video processing techniques
[64]. Third, depth information obtained by a sensor can be
used to supplement the video output. Stereo vision technol-
ogy utilize two different cameras separated by a baseline
distance, to capture images of the same scenes from two
different viewpoints which enables the viewer to perceive
scene depth. Standardizing stereo and multi-view video are
popular commercialization initiatives in 3D technology. In
general, a standard video compression method is critical for
transmitting and storing stereoscopic and multi-view data [74].
The latest commercialization step in 3D technology includes
the usage of glass-free automultiscopic displays, which deliv-
ers a signiﬁcantly more realistic 3D experience to numerous
viewers. It is achieved by converting stereoscopic 3D con-
tent into high quality multi-view 3D content using stereo to
multi-view video conversion methods [34], [65]. However, as
compared to the HDR immersive video content, 3D databases
(mostly stereo image and video) are still available [11], [21],
[22], [49], [67]. Multi-view RGB video plus depth datasets
such as [38], [84] are also available. As a result, there is a lot
of research going on in the ﬁelds of 3D video compression
[51], [69], 3D visual attention modelling [6], [9] and quality
assessment [27], [63].

With the introduction of High Efﬁciency Video Coding
(HEVC) standard and subsequent introduction of coding tech-
niques, such as 3D-HEVC and MV-HEVC, advanced im-
mersive media technologies are gaining momentum [26]. A
relatively advanced and less explored technology is 3D HDR
video, which combines both 3D and HDR video technolo-
gies. To the best of our knowledge, we only know of one
stereo HDR video database, which was publicly introduced
by Amin Banitalebi Dehkordi [5]. The majority of the work
in generating 3D HDR video revolves around merging various
views of the same scene taken at different time instances to
create synthetic stereo HDR images. Methods such as [61]
generated HDR images using low-dynamic images acquired
by two inexpensive LDR cameras for the same scene. Making
an advancement to this approach [62] used one LDR camera
and an HDR camera to generate stereo HDR data. Amin

TABLE II
ZED CAMERA CALIBRATION PARAMETERS

Parameter
Focal length fx
Focal length fy
Principal point cx
K1
K2
K3
p1
p2

L/R Sensor Parameters (2K)
Left
1400.64
1400.64
1078.91
−0.17233
1400.64
0
0.002236491
−0.000661606 −0.000573855

Right
1396.97
1396.97
1072.43
−0.17312
1396.97
0
0.00240052

Parameter
baseline
Translational TY
Translational TZ
Principal CV
Rotational RX
Rotational RZ

Stereo Parameters (2K)
value
119.996
−0.00121791
0.00502657
0.00840017
0.000188929
0.000412625

II. CREATION OF MULTI-EXPOSURE STEREOSCOPIC
IMAGE & VIDEO DATABASE

The multi-exposure stereo image & video database has
been captured using ZED stereoscopic camera [2]. In the next
sections, ZED camera speciﬁcations, capturing setup, and the
procedure for acquiring stereoscopic multi-exposure images
and videos are described.

A. Stereo camera speciﬁcation & conﬁguration

The ZED camera has synchronized dual sensors mounted
on a single frame to capture the left and right views as shown
in Figure 1. Each sensor has a 4M pixels resolution, with large
2µ sized pixels. The sensors are spaced by 12 cm horizontally
and have a ground height of roughly 3.5−4 cm. The lenses are
wide angle, having a ﬁeld of vision of 90 degrees horizontally,
60 degrees vertically, and 110 degrees diagonally, with a f /2.0
aperture. To capture the scenes, the camera is put on a small
tripod and placed on a sturdy surface (ground level or higher).
The depth range of the camera is 0.5m to 20m. The ZED
SDK software support is used to capture simultaneous, frame-
aligned left & right views of images & videos. Software
controls also aid in varying exposure, resolution, frame rate,
aspect ratio and brightness settings. Important features of ZED
camera are mentioned in Table I.

B. Stereo camera calibration

The intrinsic and extrinsic parameters of the cameras are de-
termined through the calibration procedure. The focal lengths
(fx, fy), principal point coordinates (cx, cy), radial (K1, K2,
K3), and tangential (p1, p2) distortion factors are among the
intrinsic parameters. Intrinsic parameters are often employed
to acquire images free of distortions induced by lenses and
camera structure, and to obtain three-dimensional represen-
tations of a scene. Extrinsic parameters, on the other hand,
link real-world reference systems to the camera, specifying the
device’s location and orientation in the real-world coordinate
system (using translation and rotation vector).

3

In addition to intrinsic and extrinsic calibrations, stereo
calibration permits obtaining information that links the coor-
dinates of the two cameras (left and right camera) in space.
The ZED camera has been factory calibrated, and external
calibration is not needed as such. However, to ensure highly
accurate stereo and sensor calibration, we performed re-
calibration using the in-built ZED calibration tool included in
the SDK. The camera parameters obtained on calibration have
been summarised in Table II. The table contains parameters
for 2K captures only.

C. Stereo rectiﬁcation & depth estimation

Stereo rectiﬁcation is the method of correcting two stereo
they appear to have
images of the same scene such that
been captured by two cameras with row-aligned image planes.
Image rectiﬁcation is obtained through ZED SDK. Rectiﬁed
images simplify the estimation of stereo disparity which is
a fundamental procedure before estimating the corresponding
depth map. The depth information is computed by ZED
camera using triangulation from the geometric model of non-
distorted rectiﬁed cameras [53].

D. Capturing procedure & data format speciﬁcation

The multi-exposure stereoscopic image database contains
39 natural scenes. Each scene has been captured under 3 − 4
different exposure settings achieved through the SDK controls.
All images are acquired in 2K (full HD) resolution. The left
and right views, each with a resolution of 2208 × 1242, are
combined into a single output image of resolution 4416×1242.
The camera is kept ﬁxed between the successive multi-
exposure captures. Hence,
the database stereo images are
frame-level aligned.

Similarly, our database of eighteen 3D HDR videos contain
short ﬁxed-frame captures of natural scenes with slight to
medium, partially traceable object motion at varying depth
ranges. All videos are captured in 1920 × 1080 for each
view, at a frame rate of 30 fps. Video datasets has been
recorded with ZED SDK. The SDK stores video clips in
the Stereolabs SVO format [2], with extra metadata such as
timestamps and sensor data. Each scene has been captured
under 3−4 different exposures sequentially using the software
controls, while ensuring perfect frame alignment between
successive captures. However, since multiple exposures are
captured sequentially, there is slight variation in the motion of
object’s in the acquired scene. For example, a scene with trees
swaying due to the breeze, ﬂowing water, etc. The complexity
of captured scenes opens new challenges and opportunities to
the researchers.

III. STEREO DATA CHARACTERISTICS

Most of IIT Madras campus is a protected forest carved
out of the Guindy National Park. It is home to about 300
ﬂoral species unique to India’s tropical dry evergreen regions.
The campus has a large numbers of Blackbuck, spotted deer,
bonnet macaque, and other rare wildlife species, that con-
tribute signiﬁcantly to the campus’s biodiversity. The proposed

stereoscopic database assists in the understanding of campus
scenes for several computer vision and 3D tasks such as depth
estimation, 2D/3D HDR reconstruction, development of tone
mapping algorithms, etc. The database comprises of 39 scenes
captured at varying exposures. In addition, 18 video scenes
have also been provided. We attempted to capture scenes with
a wide depth range, depth structure, camera motion, object
motion, illumination and scene complexity. While capturing
the scenes, great care was taken to prevent violating the 3D
window [66]. The 3D window violation occurs when moving
objects in the scene are clipped and overlap with the captured
image border, causing retinal revelry. During post-processing,
any discrepancies in the proposed database are rectiﬁed.

The database captures a wide range of natural outdoor
scenes and a few indoor scenes, providing researchers with
fresh opportunities and problems to work on. Outdoor scenes
include garden, roadside view, campus festival venue, build-
ings and other architectures. Indoor scenes are captured inside
academic and residential areas. The images and videos chosen
for this database have a strong depth bracket and offers rich
variations of color, texture, and illuminations over the dynamic
range. Occlusions and texture-less regions further adds to the
complexity of the captured scenes. Our stereoscopic dataset
can be categorized based on following characteristics:

• Rare Flora and Fauna: Forest scenes with great details,
rich shades of green, rigorous movement of trees in the wind,
changing shadow pattern, fascinating interplay of sunlight,
altering sunbeams, silhouette, and occlusion due to vegetation
are included in the dataset. The data is rich and full of intricate
details because of features like silhouetted leaves against the
sky, sunlight ﬂashing through foliage, or swinging banyan
roots in the wind.

• Sky-scapes: The dataset also features rich sky-scapes with
brightly illuminated sky, vivid cloud patterns and saturated
evening sky. Conﬂict arises in capturing a scene with sky-
scapes as details of the sky can be accurately recorded with
very short exposures, while the tree shoots/leaves require a
very high exposure to capture their details accurately. It is
impossible to get features of the sky with long exposures as
it makes the sky seem extraordinarily brilliant. As a result of
the broad depth bracket and complicated lighting setup, such
scenarios are perfect for enhancing dynamic range.

• Reﬂective Surfaces: Features like car window panels,
metal surfaces, glass buildings, wet roads, and water bod-
ies contribute to the database’s complexity. The Lambertian,
glossy and specular surfaces in the outdoor scenes cause abrupt
light intensity variations. Therefore, to get the various scene
features, it is necessary to capture highly disparate exposures
of the same scene. Such captures are more complicated than
capturing plain skyscapes because the bright elements in
the scenes are dispersed, irregular, and fragmented. Bright
patches change with time in scenarios featuring a water
surface or moving streams, resulting in no two consecutive
multi-exposure images/video frames being similar. Moreover,
underwater objects, in our case underwater ﬂora, add a whole
new complexity to the scene.

• Outdoor & Indoor Low-lighting: Low lighting condi-
tions lead to less saturation, low illumination, and less texture

4

in outdoor scenarios thus creating a considerable domain shift
compared to daytime outdoor scenes. The domain shift is the
transition from daytime conditions (well-lit and uniform illu-
mination) to night-time conditions (poor illumination/visibility
and non-uniform illumination). Not many datasets exist, which
include outdoor night/evening scenes. Our database incorpo-
rates such complex scenes as well. Indoor photography under
artiﬁcial illumination is also investigated. In artiﬁcially light
conditions, most HDR cameras fail to deliver good results.
Our dataset uses the ZED stereo camera’s outstanding low-
light sensitivity and includes artiﬁcially illuminated interior
scenes. Furthermore, the complexity of the indoor scene from
the dataset is increased by interference caused by the artiﬁcial
light source.

• Complex Movements: The dataset features scenes with
many objects moving independently in a complex environ-
ment. It accommodates the non-rigid motion of human beings,
animals, vehicles and other objects. To maintain precise frame-
level alignment, the camera is kept stationary between subse-
quent shots. As the scenes are real, the objects in the image,
such as forest trees swinging in the breeze, rustling leaves,
birds ﬂying, ﬂowing water, moving vehicles and so on, move
somewhat between different exposure captures. These types of
movements are inherent while capturing natural scenes since
it is beyond our control. Few images/videos are explicitly
captured indoors to analyze the motion in the foreground while
having a stable background. Additionally, to diversify the data,
a few scenes are shot with certain camera movement to serve
the purpose of real-time data.

It is in fact, these characteristics which distinguishes our
dataset, and open up new research possibilities beyond tra-
ditional depth and tone estimation algorithms. The proposed
dataset presents a new set of challenges for researchers,
including the use of learning algorithms to create high-quality
3D HDR images and videos [29], [77].

A. Multi-exposure stereoscopic image dataset

Our image dataset includes 39 stereoscopic multi-exposure
scenes. The resolution of each view is 2208×1242 pixels (full
HD). Forests, highways, houses, skylines, water fountains, in-
door and low-lit scenes are all included in the dataset. Leaves,
branches swinging in the wind (Banyan1, BanyanAvenue)
introduce complexity due to intricate light interplay. Fig. 2
shows left view of 20 scenes (corresponding to a single
exposure).

Scenes featuring objects with varying reﬂectance quali-
ties such as glasses of buildings (N ACBuilding, Glass
Building), car windows panels (Roadside1, Roadside4),
highly reﬂective building walls (House1, House2), road
(Roadside3, M arsh), water
stream (Lotus, F ountain),
metal/ plastic (Canopy) etc., are included in the proposed
dataset. Specular reﬂection, a fundamental and universal re-
ﬂection process,
is viewpoint dependent. Even with slight
changes in perspective, corresponding regions in stereo images
are poorly correlated due to specular reﬂection-shift on the
object surface [8]. This impacts depth estimation of outdoor
stereo scenes, thus limiting the 3D HDR research domain.

5

(a) Banyan1

(b) BanyanAvenue

(c) Canopy

(d) P ergola

(e) GCLong

(f) GlassBuilding

(g) Hall

(h) HimalayaBuilding

(i) House1

(j) House2

(k) M arsh

(l) N ACBuilding

(m) Lotus

(n) F ountain

(o) N ACSilhoutte

(p) Roadside1

(q) Roadside3

(r) Roadside4

(s) SkylineBT 1

(t) SkylineBT 2

Fig. 2. Multi-exposure stereoscopic image database scenes. The images shown are left views captured with single camera exposure.

HDR reconstruction of images having clouds and brightly
illuminated sky in the background (N ACsilhoutte, GCLong,
P ergola, HimalayaBuilding, SkylineBT 1, SkylineBT 2)
is challenging. As for high camera exposure, not much detail
is obtained for the cloud pattern. Features related to the clouds
can only be achieved at signiﬁcantly low exposures for such
scenarios. While outdoor scenes have natural light (well-lit
and uniform illumination) and indoor scenes have artiﬁcial
light (poorly-lit and non-uniform illumination), semi-outdoor
scenes such as Hall have both features provided by artiﬁcial
and natural light. A brief description of a few scenes with
images captured under varying exposure is given in Table III.

B. Multi-exposure stereoscopic video dataset

The proposed video dataset contains 18 stereoscopic video
clips. Among these clips, 16 are captured with a still cam-
era mounted on a rotatable tripod. Two video clips,
i.e.,
CoconutT ree and F orestDusk, are captured with signiﬁcant
camera motion while the camera is handheld. The proposed
scenes consist of slightly moving objects such as swaying
trees and grass blades, objects with traceable motion such as
Ferris wheel (F errisW heel) and Pirate ship ride movement
(Rides), and objects with non-traceable signiﬁcant motion
such as moving people and vehicles. Each video clip is
captured at a frame rate of 30 frames per second and has

TABLE III
A MULTI-EXPOSURE IMAGE SET FROM THE PROPOSED DATABASE. EACH ROW CONSISTS OF THE CAPTURED LEFT VIEWS OF A SCENE TAKEN FROM THE
ZED CAMERA AT MULTIPLE EXPOSURES (HERE AT THREE DIFFERENT CAMERA EXPOSURE LEVELS). SCENE DETAIL AND RELATED INTRICACY ARE
MENTIONED. THE SET CONSIST OF SCENES: Lotus, GlassBuilding, Canopy, Baba AND Hall (TOP TO BOTTOM).

6

Scene captures wide dynamic range - layers of veg-
etation, sky, building, rich variations of green, slight
object motion, high detail, complex depth structure,
reﬂection from water.

Scene depicts rich variation of features; reﬂection
from glass surface, rich shades of green, slight mo-
tion of trees, cloud patterns, high detail, complex
depth structure.

Branches and leaves silhouetted against the sky; rich
shades of greens, moderate object motion, intricate
details, complex depth structure, complex interplay
of sunlight.

Distant buildings seen clearly at lower exposures,
intricate details of plants seen at higher exposures.
Wide dynamic range, complex depth structure, veg-
etation motion.

Two starkly different lighting conditions – artiﬁcially
interior(brightly lit, non-uniformly illuminated)
lit
and outside dusk-time view (lowly lit, uniformly
illuminated). Moderate details, moderate depth struc-
ture.

a resolution of 1920 × 1080. Each clip has about 100−200
frames, making it long enough for experimentation and re-
search. About 3−6 video clips are provided for each scene
captured at different camera exposures. All of the videos are
synced in time. To avoid frame misalignment, each frame
is left undisturbed between successive shots. The motion of
objects within the frame makes it difﬁcult to duplicate the
same motion/video sequence at successive exposures since
many exposures are collected sequentially.

Classroom and Singer are the two videos captured in-
doors. Classroom shot with an artiﬁcial light source within
the frame shows ﬂickering introduced by the source. While
sunlight is a steady light source, artiﬁcial light has a pulsating
characteristic. However,
the human eye cannot notice this
ﬂickering, a camera is capable of detecting these high pulse
frequencies. Although Singer does not have the interference
of light in the video as the artiﬁcial light source is not directly
in the frame, the intricate movements of the instrument playing
hands possesses some crucial features to be analyzed.

With several other artifacts, lens ﬂare can be observed in the
N ACBuilding. Lens ﬂare is a photographic effect that occurs
when an intense light enters the camera lens, strikes the sensor,
and is dispersed. Lens ﬂare causes limited access to scene
features of the ﬂare-affected region, thus severely affecting
depth estimation and HDR video reconstruction. Apart from
the complexity introduced by ﬂowing water in Lotus, the
complexity also lies in the analysis of underwater objects,
i.e., underwater ﬂora. Scattering and attenuation of light as
it propagates through the water leads to unclear appearance of
underwater objects.

Estimating the depth of moving objects from videos, where
both the camera and the objects in the frame are moving, is
a challenge that is open and underconstrained. CoconutT ree
and F orestDusk consist of a scenario where the handheld
camera moves, and there are slightly swaying trees in the
background. The camera motion is kept random to exploit
all the features caused due to misalignment of the successive
frames. Realistic constraints such as shadows as provided
in GC1, F errisW heel, Rides, GC2, Caf e, Gurunath,
Sarang, Avenue; together with sunbeams as provided in
N ACBuilding, profoundly affects the present HDR Recon-
struction and depth estimation methods. It’s obvious that
abrupt illumination or saturation shift disrupts the texture and
colour consistency in the scene, resulting in shadows being
identiﬁed as new raised surfaces or foregrounds. The most
prominent features of the video dataset scenes are described
in Table IV and V. The left and right views of some selected
scenes along with the obtained depth maps is shown in Table
VI.

To understand the scene’s depth, texture, and colour pattern
dynamics from the multi-exposure stereo recordings, new deep
learning-based methods are required for such type of video
dataset with intricate object motion. These algorithms must
be able to detect object motion between multi-exposure shots
and make the necessary corrections. Given the tremendous
spatial complexity of the scenes, it’s critical to have reliable
algorithms that produce high-quality, exact 3D HDR videos.
As a result, the research community faces a new challenge in
developing intelligent algorithms to manage our dataset.

TABLE IV
MULTI-EXPOSURE STEREOSCOPIC VIDEO DATABASE SCENE DESCRIPTION, PART (A). EACH SCENE IS REPRESENTED BY A STEREO PAIR CAPTURED AT A
PARTICULAR EXPOSURE LEVEL USING ZED CAMERA.

No.

Name

3D HDR Video scene

Characteristics

7

1

GC 2

2

Garden Baba

3

Lotus

4

Ferris wheel

5

Coconut tree

6

GC 1

7

Singer

8

Gurunath

9

Saarang

Road scene with moving vehicles; signiﬁcant
object motion, intricate background motion, no
camera motion, high detail, complex depth struc-
ture, rich chromatic variation, natural light.

Person sitting in garden; rigorous movement
of tree branches in wind; slight object motion
(man), no camera motion, complex depth struc-
ture, rich chromatic variation, high details.

Lotus in pond; ﬂowing water, reﬂections and
transparency,
intricate
background motion, no camera motion, high
detail, complex depth structure, natural light.

slight object motion,

Ferris wheel in motion; partially traceable object
motion,
, no camera motion, moderate detail,
simple depth structure, complex light interplay
due to direct sunlight, silhouette of objects &
trees.

Moving video of coconut trees; stationary ob-
jects, hand held camera motion, high detail due
to complex occlusion patterns of coconut leaves,
complex depth structure, natural light.

People crossing a junction while chatting; Com-
plex object motion, no camera motion, intricate
background motion (vegetation), moderate de-
tail, complex depth structure, natural light.

Person singing & playing stringed instrument;
complex ﬁnger and body motion, no background
motion, no camera motion, moderate detail,
medium depth structure, artiﬁcial light.

Cyclists on road; Complex object motion of
passers by on cycle and on foot, no camera mo-
tion, intricate background motion (vegetation),
complex depth structure, natural light.

College festival; numerous vehicles and stu-
dents, complex unpredictable motion, signiﬁcant
background motion (vegetation), no camera mo-
tion, high detail, natural light.

IV. PROGRESS, CHALLENGES, USE CASES AND POTENTIAL
RESEARCH OPPORTUNITIES IN IMMERSIVE TECHNOLOGIES

A. Progress in HDR image/video reconstruction

In this section, we present the advancements, application
cases, issues, and possible research opportunities in the do-
main of HDR image/video reconstruction, depth estimation
for 3D content generation, tone mapping and dynamic range
compression.

In traditional HDR imaging, special HDR cameras are used
to capture HDR images [52] [73]. However, these cameras
happen to be too expensive for most users. As a result, one
typical method for HDR imaging is to use reconstruction
techniques to create an HDR image from images acquired by
a low-dynamic-range (LDR) camera.

TABLE V
MULTI-EXPOSURE STEREOSCOPIC VIDEO DATABASE SCENE DESCRIPTION, PART (B). EACH SCENE IS REPRESENTED BY A STEREO PAIR CAPTURED AT A
PARTICULAR EXPOSURE LEVEL USING ZED CAMERA.

No.

Name

3D HDR Video scene

Characteristics

8

10

Cafe

11

Rides

12

Cafe skyline

13

Avenue

14

Grass

15

Classroom

16

Forest dusk

17

House
verandah

18

NAC Building

Evening view of canteen;
foreground mo-
tion(people, cycle, animal), intricate background
motion, no camera motion, high detail, complex
depth structure, natural light.

Ferris wheel & Pirate ship ride in motion; people
walking past (complex movements), no camera
motion, moderate detail, medium depth struc-
ture, complex direct sunlight interplay.

Banyan Roots & tree branches swaying in wind;
Complex object motion, no camera motion, sky
& clouds, high detail, medium depth structure,
natural light, silhouette of leaves.

Road through a forest; varied objects (motorcy-
cles, buses and cars),moderate background mo-
tion (vegetation), no camera motion, high detail,
complex moving depth structure, natural light.

Trees and blades of grass moving in strong wind
(complex motion), changing patterns of lights
and shadows, no camera motion, complex depth
structure, high detail, direct sunlight.

Classroom scene; group presentation, complex
unpredictable motion of students, no camera
motion, simple detail, moderate depth structure,
interference due to artiﬁcial light source.

Plants in low natural light; hand held camera
motion, low saturation, cloud patterns, vegeta-
tion occlusion patterns, moderate depth struc-
ture, silhouette of vegetation, static background.

Roadside view from house; rigorous movement
of vegetation, people moving on road, high
details, complex depth structure, no camera mo-
tion, rich chromatic variation, bright sky.

Building & Garden; Lens ﬂare, sunbeam, com-
plex light interplay, reﬂection, no camera mo-
tion, high details, complex depth structure, rig-
orous movement of vegetation in the wind.

1) HDR Image Reconstruction using Multi-Exposure LDR:
One of the most prevalent approach for reconstructing HDR
images is to fuse numerous LDR images [17], [23], [45].
In this approach, one image in the stack of multi-exposed
LDR images (generally the image with median exposure)
is taken as the reference image. The remaining images are
utilized to compensate for the lost features caused by over-
/under-exposure of some local regions in the reference image.

However, low-quality HDR images are obtained if the LDR
images are not aligned properly, which is a practical scenario
for most scenes arising because of foreground object motion.
This leads to ghosting and blur artifacts. The foreground
object’s substantial movement causes misalignment between
LDR images. Also, because of occlusion induced by moving
objects, some content information is lost in the over-/under-
exposed zones.

TABLE VI
LEFT AND RIGHT VIEWS OF SOME SELECTED SCENES ALONG WITH THE OBTAINED DEPTH MAPS.

Sr. no.

Left view

Right view

Depth map

9

1

2

3

4

5

For a minor object movement, a few methods [25], [30],
[32], [68], [73], [81] have been developed to detect apparent
motion in multiple LDR images followed by removal of these
regions in the fusion. It’s clear that these methods lead to major
loss of information for large object motions. To overcome
these issues, several classic approaches use optical ﬂow [82],
[89] to align the LDR pictures in the pre-processing step before
fusing them. Nonetheless, for optical ﬂow based alignment,
motion in over-saturated and under-saturated regions causes
visible distortions in reconstruction results. Better alignment
methods include feature alignment through correlation guid-
ance [16], [57], image translation based feature alignment [24],
static multi-exposure fusion based alignment [44], [56], [78],
[79], etc.

Lin et al. [76] showed that feature alignment

through
correlation guidance is ﬂexible and effective. However, it is
sensitive to over-saturated areas, which frequently results in
the loss of textual details due to feature exclusion and needs
high computational cost. From a practical standpoint, real-time

HDR imaging is preferred for real-life applications. Real-time
HDR imaging research is lacking due to limited availability
of real-time multi-exposure LDR data.

2) HDR Image Reconstruction using Single-Exposure LDR:
Reconstruction of an HDR image from a single exposure,
unlike multi-exposure LDR images, does not have limitations
due to misalignment. Conventional methods for HDR image
reconstruction from single exposure LDR image employ in-
verse tone mapping operators (iTMOs) to increase the dy-
namic range of LDR images [7], [28], [37]. Although such
methods give an impression of enhanced dynamic range by
improving highlights, upon inspection, it turns out that little
information has been reconstructed in the saturated regions.
Recent methods proposed to adopt CNNs for single image
HDR reconstruction. HDR image generation for real complex
scenes using single-exposure LDR is quite challenging. It
has to perform tasks such as suppression of highlights in
over-exposed areas (light source and reﬂection area), noise
elimination in under-exposed areas area (strong noise area

and under-brightness area) and several other task arising due
to scene speciﬁc complexity. Our captured dataset seeks to
provide the research community with complex natural images
as training and validation data to enhance the single exposure
based HDR depth estimation methods.

3) Stereo-Based HDR Image Reconstruction: Lin et al. [43]
proposed a basic stereo based HDR reconstruction frame-
work. The framework pipeline includes scale-invariant fea-
ture transform (SIFT) matching for ﬁnding the corresponding
features in stereo pairs for modeling the camera response
function (CRF). This is followed by obtaining disparity image
from stereo matching. To deal with the stereo mismatch,
the camera response curve is used to transform the image
with the viewpoint for HDR image synthesis into an image
with the same exposure as the other image from the stereo
pair. Subsequently, the ghost removal method is used. With
such basic framework the HDR image reconstruction quality
depends upon the performance of each processing step.

Deep neural networks have recently been introduced in
stereo-based HDR imaging. Chen et al. [13] proposed a repre-
sentative framework for stereo-based HDR imaging consisting
of exposure view transfer and image fusion steps. The State-of-
the-Art method [12] substituted the conventional pipeline [43]
with DNNs. Though, introduction of DNNs increases the com-
putational cost. Multiple modular DNN methods require large
dataset for better training and ﬁne-tuning for the reconstruction
of HDR images in an end-to-end manner. Also, unavailability
of large multi-exposure stereo dataset of real-world scenes
hinder the scope of DNNs in real-world scenarios.

4) HDR video reconstruction: Deep learning methods learn
HDR visual content from input LDR video or LDR frame
sequence. Optical ﬂow methods are commonly used to align
consecutive frames, but when dealing with multiple exposures,
the optical ﬂow algorithm produces inferior HDR video.
Upholding temporal consistency in consecutive frames of
multi-exposure sequences is a relatively challenging task that
requires more attention from the research community.

Capturing an LDR sequence with single alternating expo-
sures and reconstructing the missing content at each frame is a
viable approach to make an HDR video. Kalantari et al. [33]
proposed a two-step procedure where the initial step aligns
the successive frames with the current frame using an optical
ﬂow network. Next, the ﬁnal HDR frames are produced from
the aligned images using a merge network. By contrast, Xu
et al. [80] performed gamma correction to convert the LDR
resources back to real scenes, which were then used to create
the HDR video. Moreover, they presented a novel approach for
developing a deep learning-based video inverse tone mapping
algorithm that aims to eliminate ﬂickering issues induced by
temporal inconsistencies.

Another approach for making HDR video is by using
multiple alternating exposures sequences as input. Chen et
al. [10] introduced a coarse-to-ﬁne framework, where initially
coarse HDR video reconstruction is executed through optical
ﬂow alignment and fusion in the image space followed by
reﬁnement of the coarse predictions in the feature space. Jiang
et al. [31], on the other hand developed an HDR reconstruction
method that uses tri-exposure quad-bayer sensors. The lack

10

of high-quality datasets restrict the growth of HDR video
reconstruction. Despite the fact that certain synthetic datasets
[10], [31] have been created for HDR video reconstruction
domain, there remains a signiﬁcant domain gap with real-
world data.

B. Consistent
compression

tone mapping challenges & dynamic range

Tone mapping attempts to transform one set of colours into
another, approximating HDR visual information on screens
with restricted dynamic range. However, tone-mapping quality
is subjective, and tone-mapping style choice varies from
application to application depending upon user requirements.
Broadly, traditional tone mapping operators (TMOs) are cat-
egorized as: Global tone mapping methods [20], [39], [59],
[71] apply a tone mapping curve to each and every pixel,
whereas local tone mapping methods [18], [19], [42], [48],
[83] apply variations on each pixel while taking properties of
the neighboring pixels into consideration.

that,

Despite the fact

tone mapping methods
traditional
produce good results, hyperparameter tuning is usually needed
to get the best visual result. Deep learning-based approaches
[54], [58], [85] have recently been presented that do not re-
quire parameter tuning and signiﬁcantly cut computation time
by employing powerful GPUs. There is a need to formulate
an adaptive tone mapping operator which can quickly alter
itself to wide variability in real-world HDR scene in order
to reproduce the best subjective quality output without any
perceptual damage to it’s content on a low resolution display
and as well as a high-resolution display.

Tone-mapping is considerably more challenging in the case
of 3D-HDR video because not only is temporal coherency
required, but also inter-view coherency is essential. Also, it is
essential to maintain tone difference between the two views
lower than the minimum degree of tone difference arising due
to binocular suppression.

C. Depth Estimation Challenges

Stereo-based depth estimation methods use stereo matching
algorithms to derive depth information from stereo image
pairs. Most stereo matching algorithms are categorized as
global and local methods, and aim to reduce the match-
ing ambiguity factors arising from saturation region, texture
region, and illumination variation. Existing monocular and
stereo-based depth estimation methods are trained and tested
using LDR or standard dynamic range(SDR) images/frame se-
quences. Present stereo matching methods provide inaccurate
depth estimates in under- and over-exposed regions of LDR
images. Also, depth estimation of a scene from an HDR image
or multi-exposure image stack using current stereo matching
approaches remains an ill-posed problem in speciﬁc scenarios.
Certain features such as natural lighting, visibility tran-
sition arising due to viewpoint variation, scale variations,
non-Lambertian reﬂections or partially transparent surfaces,
illumination variations, the impact of low-textured regions,
discontinuities in natural structures and high details, make
consistent depth estimation from multi-exposure stereo views

of the natural scene even more difﬁcult. Also, stereo algo-
rithms are more prone to subpixel calibration errors and their
performance is dependent on the scene complexity. Further,
depth estimation for multi-exposure stereo images for natural
complex scenes are prone to inconsistencies arising due to
temporal mismatch caused by motion associated to minute
objects. As a result, there is still potential for more research
into depth estimation from stereo HDR footage.

Video-based depth estimation algorithms use successive
frame sequences which are closely correlated and spatially
near in 3D space. The aim is to predict depth such that
the obtained depth has matching values along the temporal
correspondences. Many video-based depth estimation algo-
rithms have been proposed, with the central concept being
information. According to various
exploiting the temporal
monocular sequence based depth prediction techniques [55],
[70], [87] and stereo sequence based depth estimation tech-
niques [86], [88], modelling the temporal associations or
imposing optimization regulations among frames can enhance
the depth predictions. Because of moving objects, camera
posture and complex features of the scene, HDR video depth
estimation has been a difﬁcult challenge. As a result, the HDR
video depth estimation approach is apt to erroneously textured
regions, duplicated patterns and occlusions.

D. 3D HDR encoding challenges

As opposed to LDR and SDR images and videos, main-
taining ﬂoating-point accuracy for processing HDR images
and movies necessitates much greater storage and transmis-
sion costs. As a result, many HDR image/video compression
algorithms that converts ﬂoating point images/frames to ﬁle
formats suited for LDR video codecs have been developed.
Following that, the data are encoded into compressed HDR
video streams.

Based on allowance for backward compatibility, HDR com-
pression techniques are generally classiﬁed into two cate-
gories. The ﬁrst category encodes HDR image or video without
taking backward compatibility into account [47], [50]. The
second category takes backward compatibility into account
[35], [46], [60]. HDR compression methods with backward
compatibility typically comprises two layers: the base and
enhancement layers. The base layer consists of tone-mapped
LDR image/frame, further encoded using a conventional 8-
bit codec. The second layer includes tone-mapping residual
information that HDR application may utilize.

Multi-view images/frames are essential for creating 3D
images/videos but are challenging to encode. Some existing
coding methods for multi-view images/videos include Multi-
View Coding (MVC) [14] and Multi-View High Efﬁcient
Video Coding(MV-HEVC) [26]. Chiang et al. [15] effectively
encode the multi-exposure multi-view images. This method
transmit single LDR image per view, with the exposure of
the multi-view images ordered in an interlaced method.After
disparity map degeneration, HDR is constructed utilising the
information from the neighbouring view at the decoder. This is
followed by view wrapping. Due to the efﬁcient compression
of the multi-exposure multi-view LDR images, HDR images

11

are reconstructed at the receiver side with less bandwidth use.
Video compression standards for 3D and HDR videos are
being developed by MPEG/ITU, however much less research
has been done in encoding of 3D-HDR videos. Thus, standards
for 3D-HDR video have yet to be developed.

V. CONCLUSION

We present a new diversiﬁed dataset consisting of multi-
exposure stereo images/videos that provide a natural glance
of the IIT Madras campus. The dataset stands out because
of the complexity of diverse scenes it incorporates. Attributes
of the dataset include Illumination variation, complex object
movements, intricate texture variation of the background due
to wind, Lambertian and specular reﬂection, cloud patterns,
complicated depth structure, rich color dynamics, etc. The
proposed dataset aims to provide various research possibilities
to create a backward-compatible end-to-end content produc-
tion methodology for 3D-HDR video. Additionally, challenges
like depth estimation, encoding/decoding, tone mapping, dy-
namic range compression, visual attention modelling, quality
assessment, and optimal stereoscopic/ autostereoscopic dis-
play presentation can be investigated with such a complex
is to analyze the dataset particularly to
dataset. Our goal
show the potential for novel 3D-HDR technologies such as
multimedia-centric Mobile 3DTV apps. In the future, we will
concentrate on generating HDR stereo content with better
rendering techniques. Further, the focus will be on developing
a ﬂexible yet scalable 3D-HDR error-resilient video encoding
with error concealment awareness capabilities adjusted for
reliable transmission of 3D-HDR content over the existing
broadcasting pipeline.

ACKNOWLEDGEMENT

The scientiﬁc efforts leading to the results reported in this
paper have been carried out under the supervision of Dr.
Mansi Sharma, INSPIRE Hosted Faculty, IIT Madras. This
work has been supported,
in part, by the Department of
Science and Technology, Government of India project “Tools
and Processes for Multi-view 3D Display Technologies”,
DST/INSPIRE/04/2017/001853.

REFERENCES

[1] A high-resolution multi-exposure

database
multi-exposure-stereo-data/.

natural

of

scenes.

stereoscopic

image & video
https://sites.google.com/view/

[2] Stereolabs zed website. https://www.stereolabs.com/zed/.
[3] Ahmet Oundeﬁneduz Aky¨uz, Roland Fleming, Bernhard E. Riecke, Erik
Reinhard, and Heinrich H. B¨ulthoff. Do hdr displays support ldr content?
a psychophysical evaluation. ACM Trans. Graph., 26(3):38–es, jul 2007.
[4] Pedro Amado Assuno and Atanas Gotchev. 3D Visual Content Creation,
Coding and Delivery. Springer Publishing Company, Incorporated, 1st
edition, 2018.

[5] Amin Banitalebi-Dehkordi. Introducing a public stereoscopic 3d high

dynamic range (shdr) video database. 3D Research, 2017.

[6] Amin Banitalebi-Dehkordi, Eleni Nasiopoulos, Mahsa T. Pourazad, and
Panos Nasiopoulos. Benchmark 3d eye-tracking dataset for visual
saliency prediction on stereoscopic 3d video. arXiv: Image and Video
Processing, 2018.

[7] Francesco Banterle, Patrick Ledda, Kurt Debattista, and Alan Chalmers.
page 349–356. Association for Computing

Inverse tone mapping.
Machinery, 2006.

[8] Dinkar N. Bhat and Shree K. Nayar. Stereo and specular reﬂection.

International Journal of Computer Vision (IJCV), 26, 1998.

[9] Maude Chagnon-Forget, Ghazal Rouhafzay, A. Cr´etu, and St´ephane
Bouchard. Enhanced visual-attention model for perceptually improved
3d object modeling in virtual environments. 3D Research, 7:1–18, 2016.
[10] Guanying Chen, Chaofeng Chen, Shi Guo, Zhetong Liang, Kwan-Yee K
Wong, and Lei Zhang. HDR video reconstruction: A coarse-to-ﬁne
network and a real-world benchmark dataset. 2021.

[11] Ming-Jun Chen, Che-Chun Su, Do-Kyoung Kwon, Lawrence K. Cor-
mack, and Alan C. Bovik. Full-reference quality assessment of stereo
pairs accounting for rivalry. Signal Processing: Image Communication,
28(9):1143–1155, 2013.

[12] Yeyao Chen, Gangyi Jiang, Mei Yu, You Yang, and Yo-Sung Ho.
Learning stereo high dynamic range imaging from a pair of cameras with
IEEE Transactions on Computational
different exposure parameters.
Imaging, 6:1044–1058, 2020.

[13] Yeyao Chen, Mei Yu, Ken Chen, Gangyi Jiang, Yang Song, Zongju Peng,
and Fen Chen. New stereo high dynamic range imaging method using
generative adversarial networks. In 2019 IEEE International Conference
on Image Processing (ICIP), pages 3502–3506, 2019.

[14] Ying Chen, Ye-Kui Wang, Kemal Ugur, Miska M. Hannuksela, Jani
Lainema, and Moncef Gabbouj. The emerging mvc standard for 3d
video services. EURASIP J. Adv. Signal Process, 2009, jan 2008.
[15] Jui-Chiu Chiang, Po-Han Kao, Yao-Sheng Chen, and Wei-Ren Chen.
High-dynamic-range image generation and coding for multi-exposure
multi-view images. Circuits Syst. Signal Process., 36(7):2786–2814, jul
2017.

[16] Sungil Choi, Jaehoon Cho, Wonil Song, Jihwan Choe, Jisung Yoo,
and Kwanghoon Sohn. Pyramid inter-attention for high dynamic range
imaging. Sensors, 20(18), 2020.

[17] Paul E. Debevec and Jitendra Malik. Recovering high dynamic range
radiance maps from photographs. In ACM SIGGRAPH 2008 Classes,
SIGGRAPH ’08. Association for Computing Machinery, 2008.

[18] Fr´edo Durand and Julie Dorsey. Fast bilateral ﬁltering for the display
of high-dynamic-range images. ACM Trans. Graph., 21(3):257–266, jul
2002.

[19] Zeev Farbman, Raanan Fattal, Dani Lischinski, and Richard Szeliski.
Edge-preserving decompositions for multi-scale tone and detail manip-
ulation. ACM Trans. Graph., 27(3):1–10, aug 2008.

[20] James A. Ferwerda, Sumanta N. Pattanaik, Peter Shirley, and Donald P.
Greenberg. A model of visual adaptation for realistic image synthesis.
Proceedings of the 23rd annual conference on Computer graphics and
interactive techniques, 1996.

[21] Lutz Goldmann, Francesca De Simone, and Touradj Ebrahimi. A
comprehensive database and subjective evaluation methodology for
quality of experience in stereoscopic video. In Atilla M. Baskurt, editor,
Three-Dimensional Image Processing (3DIP) and Applications, volume
7526, pages 242 – 252. International Society for Optics and Photonics,
SPIE, 2010.

[22] Lutz Goldmann, Francesca De Simone, and Touradj Ebrahimi. Impact

of acquisition distortions on the quality of stereoscopic images, 2010.

[23] Miguel Granados, Boris Ajdin, Michael Wand, Christian Theobalt, Hans-
Peter Seidel, and Hendrik P. A. Lensch. Optimal hdr reconstruction with
linear digital cameras. In 2010 IEEE Computer Society Conference on
Computer Vision and Pattern Recognition, pages 215–222, 2010.
[24] S GreenRoshK., Anmol Biswas, Mandakinee Singh Patel, and
B. H. Pawan Prasad. Deep multi-stage learning for hdr with large object
motions. 2019 IEEE International Conference on Image Processing
(ICIP), pages 4714–4718, 2019.

[25] Thorsten Grosch. Fast and robust high dynamic range image generation

with camera and object movement, 2006.

[26] Miska M. Hannuksela, Ye Yan, Xuehui Huang, and Houqiang Li.
Overview of the multiview high efﬁciency video coding (mv-hevc)
standard. In 2015 IEEE International Conference on Image Processing
(ICIP), pages 2154–2158, 2015.

[27] Chaminda T. E. R. Hewage, Stewart T. Worrall, Safak Dogan, Stephane
Villette, and Ahmet M. Kondoz. Quality evaluation of color plus depth
map-based stereoscopic video. IEEE Journal of Selected Topics in Signal
Processing, 3(2):304–318, 2009.

[28] Yongqing Huo, Fan Yang, and Vincent Brost.

Inverse tone mapping

based upon retina response. The Scientiﬁc World Journal, 2014.
[29] Sunghoon Im, Hae-Gon Jeon, Stephen Lin, and In-So Kweon. Dpsnet:
End-to-end deep plane sweep stereo. ArXiv, abs/1905.00538, 2019.
[30] Katrien Jacobs, Celine Loscos, and Greg Ward. Automatic high-dynamic
range image generation for dynamic scenes. IEEE Computer Graphics
and Applications, 28(2):84–93, 2008.

12

[31] Yitong Jiang, Inchang Choi, Jun Jiang, and Jinwei Gu. Hdr video recon-
struction with tri-exposure quad-bayer sensors. ArXiv, abs/2103.10982,
2021.

[32] Nima Khademi Kalantari and Ravi Ramamoorthi. Deep high dynamic
range imaging of dynamic scenes. ACM Trans. Graph., 36(4), jul 2017.
[33] Nima Khademi Kalantari and Ravi Ramamoorthi. Deep hdr video from
sequences with alternating exposures. Computer Graphics Forum, 38,
2019.

[34] Petr Kellnhofer, Piotr Didyk, Szu-Po Wang, Pitchaya Sitthi-Amorn,
William Freeman, Fredo Durand, and Wojciech Matusik.
3dtv at
home: Eulerian-lagrangian stereo-to-multiview conversion. ACM Trans.
Graph., 36(4), jul 2017.

[35] Ishtiaq Rasool Khan. A backward compatible hdr encoding scheme. In
ACM SIGGRAPH 2010 Posters. Association for Computing Machinery,
2010.

[36] Pavel Korshunov, Philippe Hanhart, Thomas Richter, Alessandro Artusi,
Rafał Mantiuk, and Touradj Ebrahimi. Subjective quality assessment
In 2015 Seventh
database of hdr images compressed with jpeg xt.
International Workshop on Quality of Multimedia Experience (QoMEX),
pages 1–6, 2015.

[37] Rafael P. Kovaleski and Manuel M. Oliveira. High-quality reverse tone
In 2014 27th SIBGRAPI

mapping for a wide range of exposures.
Conference on Graphics, Patterns and Images, pages 49–56, 2014.
[38] Kevin Lai, Liefeng Bo, Xiaofeng Ren, and Dieter Fox. A large-scale
hierarchical multi-view rgb-d object dataset. In 2011 IEEE International
Conference on Robotics and Automation, pages 1817–1824, 2011.
[39] G.W. Larson, H. Rushmeier, and C. Piatko. A visibility matching tone
reproduction operator for high dynamic range scenes. IEEE Transactions
on Visualization and Computer Graphics, 3(4):291–306, 1997.

[40] Quyet-Tien Le, Patricia Ladret, Huu-Tuan Nguyen, and Alice Caplier.
Study of naturalness in tone-mapped images. Computer Vision and
Image Understanding, 196:102971, 2020.

[41] Hyuck-Gi Lee, Sungwon Chung, and Won-Hee Lee.

Presence in
virtual golf simulators: The effects of presence on perceived enjoyment,
perceived value, and behavioral intention. New Media & Society, 15:930
– 946, 2013.

[42] Zhetong Liang, Jun Xu, David Zhang, Zisheng Cao, and Lei Zhang.
A hybrid l1-l0 layer decomposition model for tone mapping. In 2018
IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pages 4758–4766, 2018.

[43] Huei-Yung Lin and Wei-Zhe Chang. High dynamic range imaging for
In 2009 16th IEEE International

stereoscopic scene representation.
Conference on Image Processing (ICIP), pages 4305–4308, 2009.
[44] Kede Ma, Zhengfang Duanmu, Hanwei Zhu, Yuming Fang, and Zhou
Wang. Deep guided learning for fast multi-exposure image fusion. IEEE
Transactions on Image Processing, 29:2808–2819, 2020.

[45] S. Mann and R. W. Picard. On being ’undigital’ with digital cameras:
Extending dynamic range by combining differently exposed pictures. In
PROCEEDINGS OF IS&T, pages 442–448, 1995.

[46] Rafał Mantiuk, Alexander Efremov, Karol Myszkowski, and Hans-
Peter Seidel. Backward compatible high dynamic range mpeg video
compression. In ACM SIGGRAPH 2006 Papers, SIGGRAPH ’06, page
713–723. Association for Computing Machinery, 2006.

[47] Rafal Mantiuk, Grzegorz Krawczyk, Karol Myszkowski, and Hans-
Peter Seidel. Perception-motivated high dynamic range video encoding.
23(3):733–741, aug 2004.

[48] Rafal Mantiuk, Karol Myszkowski, and Hans-Peter Seidel. A perceptual
framework for contrast processing of high dynamic range images. ACM
Trans. Appl. Percept., 3(3):286–308, jul 2006.

[49] Anush K. Moorthy, Che-Chun Su, Anish Mittal, and Alan Conrad Bovik.
Subjective evaluation of stereoscopic image quality. Signal Process.
Image Commun., 28:870–883, 2013.

[50] Ratnajit Mukherjee, Kurt Debattista, Thomas-Bashford Rogers, Max-
imino Bessa, and Alan Chalmers. Uniform color space-based high
dynamic range video compression. IEEE Transactions on Circuits and
Systems for Video Technology, 29(7):2055–2066, 2019.

[51] Karsten M¨uller, Heiko Schwarz, Detlev Marpe, Christian Bartnik, Sebas-
tian Bosse, Heribert Brust, Tobias Hinz, Haricharan Lakshman, Philipp
Merkle, Franz Hunn Rhee, Gerhard Tech, Martin Winken, and Thomas
Wiegand. 3d high-efﬁciency video coding for multi-view video and
depth data. IEEE Transactions on Image Processing, 22(9):3366–3378,
2013.

[52] A. J. Nayana and Anoop K. Johnson. High dynamic range imaging-a

review. 2015.

[53] Luis Enrique Ortiz, Viviana Elizabeth Cabrera, and Luiz MG Goncalves.
Depth data error modeling of the zed 3d vision sensor from stereolabs.

13

[76] Lin Wang and Kuk-Jin Yoon. Deep learning for hdr imaging: State-of-
the-art and future trends. IEEE Transactions on Pattern Analysis and
Machine Intelligence, pages 1–1, 2021.

[77] Shangzhe Wu, Jiarui Xu, Yu-Wing Tai, and Chi-Keung Tang. Deep high
dynamic range imaging with large foreground motions. In Proceedings
of the European Conference on Computer Vision (ECCV), September
2018.

[78] Han Xu, Jiayi Ma, Zhuliang Le, Junjun Jiang, and Xiaojie Guo.
Fusiondn: A uniﬁed densely connected network for image fusion.
In Proceedings of
the Thirty-Fourth AAAI Conference on Artiﬁcial
Intelligence (AAAI), pages 12484–12491, 2020.

[79] Han Xu, Jiayi Ma, and Xiao-Ping Zhang. Mef-gan: Multi-exposure
IEEE Transactions

image fusion via generative adversarial networks.
on Image Processing, 29:7203–7216, 2020.

[80] Yucheng Xu, Li Song, Rong Xie, and Wenjun Zhang. Deep video
In 2019 IEEE Fifth International Conference

inverse tone mapping.
on Multimedia Big Data (BigMM), pages 142–147, 2019.

[81] Qingsen Yan, Jinqiu Sun, Haisen Li, Yu Zhu, and Yanning Zhang.
High dynamic range imaging by sparse representation. Neurocomput.,
269(C):160–169, dec 2017.

[82] Qingsen Yan, Yu Zhu, and Yanning Zhang. Robust artifact-free high
dynamic range imaging of dynamic scenes. Multimedia Tools and
Applications, 78, 2019.

[83] J. Yang, A. Hore, and O. Yadid-Pecht. Local tone mapping algorithm
and hardware implementation. Electronics Letters, 54:560–562, may
2018.

[84] Jun Yang, Yizhou Gao, Dong Li, and Steven L. Waslander. Robi: A

multi-view dataset for reﬂective objects in robotic bin-picking, 2021.

[85] Xin Yang, Ke Xu, Yibing Song, Qiang Zhang, Xiaopeng Wei, and
Rynson W.H. Lau. Image correction via deep reciprocating hdr trans-
formation. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018.

[86] Huangying Zhan, Ravi Garg, Chamara Saroj Weerasekera, Kejie Li,
Harsh Agarwal, and Ian Reid. Unsupervised learning of monocular
depth estimation and visual odometry with deep feature reconstruction.
In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.

[87] Haokui Zhang, Chunhua Shen, Ying Li, Yuanzhouhan Cao, Yu Liu, and
Youliang Yan. Exploiting temporal consistency for real-time video depth
estimation. 2019.

[88] Yiran Zhong, Hongdong Li, and Yuchao Dai. Open-world stereo video

matching with deep rnn. In ECCV, 2018.

[89] H. Zimmer, A. Bruhn, and J. Weickert. Freehand HDR imaging of
moving scenes with simultaneous resolution enhancement. Computer
Graphics Forum (Proceedings of Eurographics), 30(2):405–414, 2011.

ELCVIA: electronic letters on computer vision and image analysis,
17(1):1–15, 2018.

[54] Vaibhav Amit Patel, Purvik Shah, and Shanmuganathan Raman. A
generative adversarial network for tone mapping hdr images.
In
Computer Vision, Pattern Recognition, Image Processing, and Graphics,
pages 220–231, Singapore, 2018. Springer Singapore.

[55] Vaishakh Patil, Wouter Van Gansbeke, Dengxin Dai, and Luc Van Gool.
Don’t forget the past: Recurrent depth estimation from monocular video.
IEEE Robotics and Automation Letters, 5(4):6813–6820, 2020.

[56] K. Prabhakar, V. Sai Srikar, and R. Venkatesh Babu. Deepfuse: A deep
unsupervised approach for exposure fusion with extreme exposure image
pairs. 2017 IEEE International Conference on Computer Vision (ICCV),
pages 4724–4732, 2017.

[57] Zhiyuan Pu, Peiyao Guo, M. Salman Asif, and Zhan Ma. Robust
high dynamic range (hdr) imaging with complex motion and parallax.
In Proceedings of the Asian Conference on Computer Vision (ACCV),
November 2020.

[58] Aakanksha Rana, Praveer Singh, Giuseppe Valenzise, Frederic Dufaux,
Nikos Komodakis, and Aljosa Smolic. Deep tone mapping operator for
high dynamic range images. IEEE Transactions on Image Processing,
29:1285–1298, 2020.

[59] E. Reinhard and K. Devlin. Dynamic range reduction inspired by
IEEE Transactions on Visualization and

photoreceptor physiology.
Computer Graphics, 11(1):13–24, 2005.

[60] Dmytro Rusanovskyy, Done Bugdayci Sansli, Adarsh Ramasubramo-
nian, Sungwon Lee, Joel Sole, and Marta Karczewicz. High dynamic
range video coding with backward compatibility. In 2016 Data Com-
pression Conference (DCC), pages 289–298, 2016.

[61] Dominic R¨ufenacht. Stereoscopic high dynamic range video, master

thesis, 2011.

[62] Elmedin Selmanovic, Kurt Debattista, Thomas Bashford-Rogers, and
Alan Chalmers. Enabling stereoscopic high dynamic range video. Signal
Processing: Image Communication, 29(2):216–228, 2014. Special Issue
on Advances in High Dynamic Range Video Research.

[63] Feng Shao, Weisi Lin, Shanbo Gu, Gangyi Jiang, and Thambipillai
Srikanthan. Perceptual full-reference quality assessment of stereoscopic
images by considering binocular visual characteristics. IEEE Transac-
tions on Image Processing, 22(5):1940–1953, 2013.

[64] Mansi Sharma. Uncalibrated camera based content generation for 3D

multi-view displays, Series/Report no. : TH-5223. PhD thesis, 2017.

[65] Mansi Sharma, Santanu Chaudhury, Brejesh Lall, and M. S. Venkatesh.
A ﬂexible architecture for multi-view 3dtv based on uncalibrated cam-
Journal of Visual Communication and Image Representation,
eras.
25(4):599–621, may 2014.

[66] Aljoscha Smolic, Peter Kauff, Sebastian Knorr, Alexander Hornung,
Matthias Kunter, Marcus M¨uller, and Manuel Lang. Three-dimensional
the IEEE,
video postproduction and processing.
99(4):607–625, 2011.

Proceedings of

[67] Rui Song, Hyunsuk Ko, and C. C. Jay Kuo. Mcl-3d: a database
for stereoscopic image quality assessment using 2d-image-plus-depth
source, 2014.

[68] Abhilash Srikantha and D´esir´e Sidib´e. Ghost detection and removal
Image Commun.,

for high dynamic range images: Recent advances.
27(6):650–662, jul 2012.

[69] Gary J. Sullivan, Jill M. Boyce, Ying Chen, Jens-Rainer Ohm, C. An-
Standardized extensions of high
IEEE Journal of Selected Topics in

drew Segall, and Anthony Vetro.
efﬁciency video coding (hevc).
Signal Processing, 7(6):1001–1016, 2013.

[70] Denis Tananaev, Huizhong Zhou, Benjamin Ummenhofer, and Thomas
Brox. Temporally consistent depth estimation in videos with recurrent
architectures. In ECCV Workshops, 2018.

[71] J. Tumblin and H. Rushmeier. Tone reproduction for realistic images.
IEEE Computer Graphics and Applications, 13(6):42–48, 1993.
[72] Okan Tarhan Tursun, Ahmet O˘guz Aky¨uz, Aykut Erdem, and Erkut
Erdem. The state of the art in hdr deghosting: A survey and evaluation.
Comput. Graph. Forum, 34(2):683–707, may 2015.

[73] Okan Tarhan Tursun, Ahmet O˘guz Aky¨uz, Aykut Erdem, and Erkut
Erdem. The state of the art in hdr deghosting: A survey and evaluation.
Comput. Graph. Forum, 34(2):683–707, may 2015.

[74] Anthony Vetro, Thomas Wiegand, and Gary J. Sullivan. Overview of
the stereo and multiview video coding extensions of the h.264/mpeg-4
avc standard. Proceedings of the IEEE, 99(4):626–642, 2011.

[75] Aditya Wadaskar, Mansi Sharma, and Rohan Lal. A rich stereoscopic
3d high dynamic range image amp; video database of natural scenes.
In 2019 International Conference on 3D Immersion (IC3D), pages 1–8,
2019.

