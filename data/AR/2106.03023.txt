The Bayesian Context Trees State Space Model:
Interpretable mixture models for time series

Ioannis Papageorgiou ∗

Ioannis Kontoyiannis †

May 23, 2022

Abstract

A general hierarchical Bayesian framework is introduced for mixture modelling of real-valued
time series, including a collection of eﬀective tools for learning and inference. At the top level,
a discrete context (or ‘state’) is extracted for each sample, consisting of a discretised version of
some of the most recent observations preceding it. The set of all relevant contexts are repre-
sented as a discrete context tree. At the bottom level, a diﬀerent real-valued time series model
is associated with each context (i.e., with each state). This deﬁnes a very general framework
that can be used in conjunction with any existing model class to build ﬂexible and interpretable
mixture models. We introduce algorithms that allow for eﬃcient, exact Bayesian inference; in
particular, the maximum a posteriori probability (MAP) model, including the relevant MAP
context tree, can be identiﬁed exactly. These algorithms can be updated sequentially, facilitat-
ing eﬃcient online forecasting. The utility of the general framework is illustrated in detail when
autoregressive (AR) models are used at the bottom level, resulting in a nonlinear AR mixture
model. Our methods are found to outperform several state-of-the-art techniques on both sim-
ulated and real-world data from economics and ﬁnance, both in terms of forecasting accuracy
and computational requirements.

Keywords. Time series, Interpretable mixture models, Exact Bayesian inference, Forecasting,
State space models, Autoregressive models, Context trees.

2
2
0
2

y
a
M
0
2

]
E
M

.
t
a
t
s
[

3
v
3
2
0
3
0
.
6
0
1
2
:
v
i
X
r
a

∗Department of Engineering, University of Cambridge, Trumpington Street, Cambridge CB2 1PZ, UK. Email:

ip307@cam.ac.uk.

†Statistical Laboratory, Centre for Mathematical Sciences, University of Cambridge, Wilberforce Road, Cam-

bridge CB3 0WB, UK. Email: yiannis@maths.cam.ac.uk.

1

 
 
 
 
 
 
1 Introduction

Time series modelling, inference and forecasting are critical tasks in statistics and machine learn-
ing (ML), with important applications throughout the sciences and engineering. A wide range
of approaches exist, including classical statistical methods [10, 29, 19, 33] as well as modern
ML techniques, notably matrix factorisations [87, 21], Gaussian processes (GP) [61, 66, 23], and
neural networks [7, 3, 88]. Despite their popularity, there has not been conclusive evidence that
in general the latter outperform the former in the time series setting [50, 49, 1]. Motivated in
part by the two well-known limitations of neural network models (see, e.g., [7]), namely, the lack
of interpretability and data eﬃciency, in this work we propose a general class of ﬂexible hier-
archical Bayesian models, which are both naturally interpretable and suitable for applications
with limited training data. Also, we provide computationally eﬃcient – linear complexity –
algorithms for inference and prediction, oﬀering another practical advantage compared to stan-
dard ML methods which have greater computational requirements due to the heavier training
involved [50].

Roughly speaking, time series models can be broadly categorised in two classes, depending
on the absence or presence of an underlying hidden state process. The ﬁrst class includes the
family of autoregressive (AR) and autoregressive integrated moving average (ARIMA) models
along with their extensions and generalisations, which directly model the mapping from previous
to current observations. Besides the classical AR and ARIMA models [10, 76, 73], nonlinear
AR models have been proposed, using both GPs [66, 27, 11, 78, 53] and neural networks to
model nonlinear dynamics. Feed-forward neural networks have been used since the 90s in this
setting [88], as in the Neural Network AR (NNAR) model. More recently, recurrent neural
networks (RNN) have also been employed [28, 69, 54], with the DeepAR model of [69] that uses
Long Short-Term Memory (LSTM) cells [31] being one of the most successful approaches.

The second class consists of State Space Models (SSM) [19, 33], which can describe more
complex dynamics by allowing an underlying hidden state process (state-transition) together
with an observation model (emission). Classical approaches here include the linear Gaussian
SSM, for which the Kalman ﬁlter [37] can be used for inference, and exponential smoothing
methods; see [33] for a detailed review. Again, various extensions have been proposed by making
the transition and/or emission equations nonlinear, using both GPs [77, 25, 24, 20, 78] and
neural networks [45, 44, 89, 38, 59, 16]. However, inference in these settings becomes a quite
challenging task, requiring sophisticated and computationally-intense approximate techniques,
including Particle MCMC [18, 6] and variational methods like stochastic gradient variational
Bayes (SGVB) [40, 62].

In this work, we introduce an intermediate Bayesian modelling approach that combines
important features of both the above classes: We ﬁrst identify meaningful discrete states, but
these are observable rather than hidden, and given by the discretised values of some of the most
recent samples. Then we associate a diﬀerent time series model with each of these discrete
context-based states. In technical terms, we deﬁne a hierarchical Bayesian model, which at the
top level selects the set of relevant states (that can be viewed as providing an adaptive partition
of the state space), and at the bottom level associates an arbitrary time series model to each
state. These collections of states (equivalently, the corresponding state space partitions) are
naturally represented as discrete context-tree models [42], which are shown to admit a natural
interpretation and to enable capturing important aspects of the structure present in the data. We
refer to the resulting model class as the Bayesian Context Trees State Space Model (BCT-SSM).

2

Although we refer to the BCT-SSM as a ‘model’, it is in fact a general framework for building
Bayesian mixture models for time series, that can be used in conjunction with any existing model
class. The resulting model family is rich, ﬂexible, and much more general than the class one
starts with. For example, using any of the standard linear families (like the classical AR or
ARIMA) leads to much more general models that can in fact capture highly nonlinear trends
in the data, and are also easily interpretable. We show that in addition to resulting in the
construction of ﬂexible model classes, employing this particular type of observable state process
(as opposed to a conventional hidden state process) also makes it is possible to perform eﬀective
Bayesian inference. This is achieved by exploiting the structure of context-tree models and by
extending the algorithms of [42, 57], which were previously used only in the restricted setting of
discrete-valued time series.

These tools make the BCT-SSM a powerful Bayesian framework that allows for exact and
computationally very eﬃcient Bayesian inference. In particular, the evidence [48] can be com-
puted exactly, with all models and parameters integrated out. Furthermore, the a posteriori
most likely (MAP) partition (i.e., the MAP set of discrete states) can be identiﬁed, along with
its exact posterior probability. Following the Bayesian approach here is particularly important,
as it means that the data automatically select the set of relevant states. In contrast, maximum
likelihood would overﬁt and select a large number of irrelevant states, with consequences both
in the interpretation and the out-of-sample prediction performance of the ﬁtted model. Finally,
it is shown that our algorithms allow for eﬃcient sequential updates, which are ideally suited
for online forecasting, and provide an important practical advantage compared to standard ML
time series approaches.

To illustrate the application of the general framework, we study in detail the case where AR
models are used as building blocks for the BCT-SSM, with a diﬀerent AR model associated to
each discrete state. We refer to the resulting model class as the Bayesian context tree autore-
gressive (BCT-AR) model; it is shown to be a ﬂexible, nonlinear mixture of AR models that
generalises popular AR mixtures, including the threshold AR (TAR) models [74, 75, 73] and
the mixture AR (MAR) models of [83]. The BCT-AR model is found to outperform several
state-of-the-art methods in simulated experiments and standard applications of nonlinear time
series from economics and ﬁnance, both in terms of forecasting accuracy and computational
requirements.

Finally, we note that a number of earlier approaches employ discrete patterns in the analysis
of real-valued time series [4, 5, 8, 26, 32, 47, 55, 68]. These works illustrate the fact that
useful and meaningful information can indeed be extracted from discrete contexts. However, in
most cases the methods are either application- or task-speciﬁc, and typically resort to ad hoc
considerations for performing inference. In contrast, in this work discrete contexts are used in a
natural manner, by deﬁning a hierarchical Bayesian modelling structure upon which principled
Bayesian inference is performed.

2 The Bayesian Context Trees State Space Model

2.1 Discrete contexts

A key element of our development is the use of an observable state for each sample xn, based
on discretised versions of some of the samples (. . . , xn−2, xn−1) preceding it. We refer to the
string consisting of these discretised previous samples as the discrete context; it plays the role
of a discrete-valued feature vector that can be used to identify additional useful structure in the

3

data. In order to extract these contexts, we consider simple piecewise constant quantisers from
R to a ﬁnite alphabet A = {0, 1, . . . , m − 1}, of the form,

Q(x) =






x < c1,
ci ≤ x ≤ ci+1, 1 ≤ i ≤ m − 2,

0,
i,
m − 1, x > cm−1,

(1)

where, throughout this section, the thresholds {c1, . . . , cm−1} and the resulting quantiser Q are
considered ﬁxed. A systematic way to infer the thresholds from data is described in Section 3.2.
We note that this general framework can be used in conjunction with an arbitrary way of
extracting discrete features, based on an arbitrary mapping to a discrete alphabet, not necessarily
of the form in (1). However, the quantisation needs to be meaningful in order to lead to useful
results. Quantisers as in (1) oﬀer a generally reasonable choice although, depending on the
application at hand, there are other useful approaches, e.g., quantising percentage diﬀerences
between successive samples.

2.2 Context trees

Given a quantiser Q with m levels as above, a maximum con-
text length D ≥ 0, and a proper m-ary context tree T , the
context (or ‘state’) of each sample xn is obtained as follows.
Let t = (Q(xn−1), . . . , Q(xn−D)) be the discretised string of
length D preceding xn; the context s of xn is the unique leaf
of T that is a suﬃx of t. For example, for the context tree of
Figure 1, if Q(xn−1) = 0 and Q(xn−2) = 1 then s = 01, whereas
if Q(xn−1) = Q(xn−2) = 1 then s = 1. The leaves of the tree
deﬁne the set of discrete states in our hierarchical model. So,
for the example BCT-SSM of Figure 1, the set of states is is S = {1, 01, 00}. Equivalently, this
process can be viewed as deﬁning a partition of R2 into three regions indexed by the contexts
S in T. [A tree T is proper if any node in T that is not a leaf has exactly m children. This
means that for any string t = (Q(xn−1), . . . , Q(xn−D)) there is always a unique context s that is
a leaf of T . Equivalently, proper trees deﬁne proper partitions, so that the resulting state space
regions are disjoint and their union is the whole space RD.]

Figure 1: Example of a binary con-
text tree T used for deﬁning the set
of discrete states-contexts

To complete the speciﬁcation of the BCT-SSM, we need to associate a diﬀerent time series
model Ms to each leaf s of the context tree T , giving a diﬀerent conditional density for xn. At
time n, given the context s determined by the past D samples (xn−1, . . . , xn−D), the distribution
of xn is given by the model Ms assigned to s. Although general non-parametric models could
also be used, for the rest of this paper we consider parametric models with parameters θs at
each leaf s. Altogether, the BCT-SSM consists of an m-ary quantiser Q, a proper m-ary tree T
that deﬁnes the set of discrete states, and a collection of parameter vectors θs for the parametric
models at the leaves of T .

Identifying T with the collection of its leaves S, and writing xj

i for the segment (xi, xi+i, . . . , xj),

the likelihood induced by the BCT-SSM is,

p(x|θ, T ) := p(xn

1 |T, θ, x0

−D+1) =

n
(cid:89)

i=1

p(xi|T, θ, xi−1

−D+1) =

(cid:89)

(cid:89)

s∈T

i∈Bs

p(xi|T, θs, xi−1

−D+1),

(2)

where Bs is the set of indices i ∈ {1, 2, . . . , n} such that the context of xi is s, and θ = {θs ; s ∈ T }.

4

01θ1θ01θ002.3 Bayesian modelling and inference

For the top level of the BCT-SSM, we consider collections of states represented by context
trees T in the class T (D) [42] of all proper m-ary trees with depth no greater than D.

Prior structure. For the trees T ∈ T (D) with maximum depth D ≥ 0 at the top level of the
hierarchical model, we use the Bayesian Context Trees (BCT) prior of [42],

π(T ) = πD(T ; β) = α|T |−1β|T |−LD(T ) ,

(3)

where β ∈ (0, 1) is a hyperparameter, α is given by α = (1 − β)1/(m−1), |T | is the number of
leaves of T , and LD(T ) is the number of leaves of T at depth D. This prior penalises larger trees
by an exponential amount, which is reasonable as it is naturally desirable to a priori penalise
BCT-SSM models with large numbers of parameters to avoid overﬁtting. Given a tree model
T ∈ T (D), we place an independent prior on each θs, so that π(θ|T ) = (cid:81)

s∈T π(θs).

Typically, the main obstacle in performing Bayesian inference is the computation of the
normalising constant p(x) of the posterior (sometimes referred to as the evidence), in this case,

p(x) =

(cid:88)

π(T )p(x|T ) =

(cid:88)

π(T )

T ∈T (D)

T ∈T (D)

(cid:90)

θ

p(x|T, θ)π(θ|T ) dθ.

(4)

The power of the proposed Bayesian structure comes, in part, from the fact that, although T (D)
is enormously rich, consisting of doubly-exponentially many models in D, it is actually possible
to perform exact Bayesian inference eﬃciently. To that end, we introduce the Continuous Con-
text Tree Weighting (CCTW) algorithm, and the Continuous Bayesian Context Tree (CBCT)
algorithm, generalising the corresponding algorithms for discrete time series in [42]. It is shown
that CCTW computes the normalising constant p(x) exactly (Theorem 1), and CBCT identiﬁes
the MAP tree model (Theorem 2). In the Appendix we brieﬂy discuss how the k-BCT algo-
rithm of [42] can be similarly modiﬁed to obtain the top-k a posteriori most likely trees, but the
details are omitted. The main diﬀerence from the discrete case in [42], both in the algorithmic
descriptions and in the proofs of the theorems (given in the Appendix), is that we introduce a
new generalised form of estimated probabilities that is needed in place of their simple discrete
versions. These are given by,

Pe(s, x) =

(cid:90) (cid:89)

i∈Bs

p(xi|T, θs, xi−1

−D+1) π(θs) dθs.

(5)

Let x = xn

−D+1 be a time series, and let yi = Q(xi) denote the corresponding quantised samples.

CCTW: The continuous context-tree weighting algorithm
1. Build the tree TMAX, whose leaves are all the discrete contexts yi−1

Compute Pe(s, x) as given in (5) for each node s of TMAX.

i−D, i = 1, 2, . . . , n.

2. Starting at the leaves and proceeding recursively towards the root compute:

Pw,s =

(cid:26) Pe(s, x),

if s is a leaf,

βPe(s, x) + (1 − β) (cid:81)m−1

j=0 Pw,sj, otherwise,

where sj is the concatenation of context s and symbol j.

5

CBCT: The continuous Bayesian context tree algorithm

1. Build the tree TMAX and compute Pe(s, x) for each node s of TMAX, as in CCTW.

2. Starting at the leaves and proceeding recursively towards the root compute:

Pm,s =






if s is a leaf at depth D,

Pe(s, x),
β,
if s is a leaf at depth < D,
max (cid:8)βPe(s, x), (1 − β) (cid:81)m−1

j=0 Pm,sj

(cid:9), otherwise.

3. Starting at the root and proceeding recursively with its descendants, for each node s: If
the maximum above is achieved by the ﬁrst term, prune all its descendants from TMAX.

Theorem 1. The weighted probability Pw,s at the root is exactly the normalising constant p(x)
of (4).

Theorem 2. For all β ≥ 1/2, the tree T ∗
tree model.

1 produced by the CBCT algorithm is the MAP

Even in cases where the integrals in (5) are not tractable, the fact that they are in the
form of standard marginal likelihoods makes it possible to compute them approximately using
standard methods, e.g., [14, 15, 22, 86]. The above algorithms can then be used with these
approximations as a way of performing approximate inference for the BCT-SSM. However, in
this paper we do not investigate this further. Instead, we illustrate the general principle via
an interesting example where the estimated probabilities can be computed explicitly and the
resulting mixture model is a ﬂexible nonlinear model of practical interest. This is described
in the next section, where AR models Ms are associated to each context s. We refer to the
resulting model as the Bayesian context tree autoregressive (BCT-AR) model, which is just a
particular instance of the general BCT-SSM.

2.4 Remarks: Discrete models for continuous data

Context-tree models were introduced as “tree sources” by Rissanen [63, 64, 65] in the 1980s in the
information-theoretic literature, and they have been employed widely since then in connection
with problems on discrete data. Tree source models were used with great success in the context of
data compression [79, 81, 80], and more recently they have been found to be very eﬀective in the
analysis of discrete-valued time series within the Bayesian Context Trees (BCT) framework [42,
57, 56].

The central conceptual novelty of the present work is in showing that discrete context trees
can in fact also be utilised in a very eﬀective way for modelling real-valued time series, by repre-
senting meaningful context-based discrete states that are used to build ﬂexible and interpretable
mixture models of practical interest. We introduce a new construction for building context-
dependent mixtures of models in any existing model class, combining a discrete context tree
with an arbitrary existing family of continuous models. In fact, this is the ﬁrst attempt of doing
so, perhaps because at ﬁrst glance context trees seem to be naturally suited only for discrete-
valued data. An important step in the development of the BCT-SSM class is the introduction
of a quantiser that extracts discrete contexts (Section 2.1), along with a principled Bayesian
procedure for actually selecting an appropriate quantiser in practice (Section 3.2). Finally we
are able to prove that, perhaps somewhat surprisingly, small modiﬁcations of the discrete BCT

6

algorithms of [42] can be used for exact inference in the much more general BCT-SSM setting
(Section 2.3). The BCT-AR model described in the next section is new, as is the analysis of the
estimated probabilities, the relevant posterior distributions, and the computational complexity
of the CCTW and CBCT algorithms in this case.

3 The Bayesian context tree autoregressive model

Here we consider the BCT-SSM model class where an AR model of order p is associated to each
leaf s of a context tree T , as,

xn = φs,1xn−1 + · · · + φs,pxn−p + en = φs

T

(cid:101)xn−1 + en,

en ∼ N (0, σ2

s ) ,

(6)

where φs = (φs,1, . . . , φs,p)T and (cid:101)xn−1 = (xn−1, . . . , xn−p)T.

The parameters of the model are the AR coeﬃcients and the noise variance, so that θs =
s ). We use an inverse-gamma prior for the noise variance, and a Gaussian prior for the

(φs, σ2
AR coeﬃcients, so that the joint prior on the parameters is π(θs) = π(φs|σ2

s ), with,

s )π(σ2

π(σ2

s ) = Inv-Gamma(τ, λ),

π(φs|σ2

s ) = N (µo, σ2

s Σo) ,

(7)

where (τ, λ, µo, Σo) are the prior hyperparameters. This prior speciﬁcation allows the exact
computation of the estimated probabilities of (5), and also gives closed-form posteriors for the
AR coeﬃcients and the noise variance. These are given in Lemmas 1 and 2; their proofs are in
Appendix B.

Lemma 1. For the AR model, the estimated probabilities Pe(s, x) as in (5) are given by,

Pe(s, x) = C−1

s

Γ (τ + |Bs|/2) λτ
Γ(τ ) (λ + Ds/2)τ +|Bs|/2

,

(8)

where |Bs| is the cardinality of the set Bs in (2), i.e., the number of observations with context s,
and,

Cs =

(cid:113)

(2π)|Bs|det(I + ΣoS3), Ds = s1 + µT

o Σ−1

o µo − (s2 + Σ−1

o µo)T(S3 + Σ−1

o )−1(s2 + Σ−1

o µo),

with the sums s1, s2, S3 deﬁned as:

s1 =

x2
i

(cid:88)

i∈Bs

,

s2 =

(cid:88)

i∈Bs

xi (cid:101)xi−1

, S3 =

(cid:88)

i∈Bs

(cid:101)xi−1(cid:101)xT

i−1 .

(9)

Lemma 2. Given a tree model T , at each leaf s, the posterior distributions of the AR coeﬃcients
and the noise variance are given by,

π(σ2

s |T, x) = Inv-Gamma (τ + |Bs|/2, λ + Ds/2) ,

π(φs|T, x) = tν(ms, Ps) ,

(10)

where tν denotes a multivariate t-distribution with ν degrees of freedom. Here, ν = 2τ + |Bs|,
and,

ms = (S3 + Σ−1

o )−1(s2 + Σ−1

o µo) , P −1

s =

2τ + |Bs|
2λ + Ds
s are given, respectively, by,

(S3 + Σ−1

o ) .

Corollary. The MAP estimators of φs and σ2

(11)

(12)

MAP

(cid:99)φs

= ms , (cid:99)σ2
s

MAP

= (2λ + Ds)/(2τ + |Bs| + 2) .

7

1 , with an initial segment x0

3.1 Computational complexity and sequential updates
For a time series xn
−D+1, the tree TMAX in the CCTW algorithm has
no more than nD + 1 nodes. For each symbol xi in xn
1 , exactly D + 1 nodes of TMAX need to be
updated, corresponding to its contexts of length 0, 1, . . . , D. For each one of these nodes, only
the quantities {|Bs|, s1, s2, S3} need to be updated, which can be done eﬃciently by just adding
an extra term to each sum. Using these and Lemma 1, the estimated probabilities Pe(s, x)
can be computed for all nodes of TMAX with a constant number of operations per node. Since
the recursive step only performs operations on TMAX, the complexity of all three algorithms
as a function of n and D is only O(nD), which is linear in the length of the time series and
the maximum depth. This clearly indicates that the present methods are computationally very
[Taking into account m and p as
eﬃcient and scale well with large numbers of observations.
well, it is easy to see that the complexity is O (cid:0)nD(p3 + m)(cid:1).]

The above discussion also shows that, importantly, all our algorithms can be updated se-
quentially. For example, when observing a new sample xn+1 after executing CCTW for xn
1 ,
only D + 1 nodes need to be updated, and Pe(s, x) and Pw,s have to be re-computed only at
these nodes. This takes O(D) operations, i.e., O(1) as a function of n. In particular, this im-
plies that sequential prediction can be performed very eﬃciently. Empirical running times for
all forecasting experiments are reported in Appendix E, showing that our methods are much
more eﬃcient than essentially all the alternatives examined. The diﬀerence is in fact very large,
especially when comparing with state-of-the-art ML models that require heavy training and
do not allow for eﬃcient sequential updates, giving empirical running times that are typically
larger by several orders of magnitude than ours; see also [50] for a relevant review comparing
the computational requirements of ML versus statistical techniques.

3.2 Choosing the hyperparameters, quantiser and AR order
It can be seen from Lemma 2 that the posterior distributions of φs and σ2
s are typically not
very sensitive to the prior hyperparameters (i.e., when reasonably many observations exist with
context s). In all the experimental results below we make the simple choice µo = 0 and Σo = I
in the AR coeﬃcients’ prior. In view of equation (10), τ and λ should be chosen to be relatively
small in order to minimise their eﬀect on the posterior, while keeping the mode of the inverse-
gamma prior, λ/(τ + 1), reasonable. For the context tree prior, we use the default value of
β = 1 − 2−m+1 [42], and the maximum depth D = 10.

Finally, we introduce a principled Bayesian way for selecting the quantiser thresholds {ci}
of (1) and the AR order p. Viewing them as extra parameters on an additional layer above
everything else, we place uniform priors on {ci} and p, and perform Bayesian model selection [60,
48, 61] to obtain their MAP values. The resulting posterior p({ci}, p|x) is proportional to the
evidence p(x|{ci}, p), which can be computed exactly using the CCTW algorithm (Theorem 1).
So, in order to select appropriate values, we specify a suitable range of possible {ci} and p,
and select the ones with the higher evidence. For the AR order we take 1 ≤ p ≤ pmax for an
appropriate pmax (e.g., pmax = 5 in our experiments), and for the {ci} we perform a grid search
in a reasonable range (e.g., between the 10th and 90th percentiles of the data). Even though a
uniform prior is used for p, we note that the Bayesian approach implicitly penalises more complex
models (i.e., larger values of p), by averaging over a larger number of parameters.
[This well-
known phenomenon [60, 48, 71, 39] is often referred to as “automatic Occam’s Razor”. In fact,
the popular BIC model selection criterion [70] can be derived as an asymptotic approximation
to the evidence [41].]

8

3.3 Comparison with other AR mixtures

Threshold AR models. Threshold autoregressive (TAR) models were introduced in [75], and
have been used extensively in the analysis of nonlinear time series; see, e.g., the reviews [74, 30]
and the texts [17, 76, 73]. Although numerous diﬀerent versions of TAR models have been
employed (see, e.g., the discussion in [74]), the most commonly used one is the self-exciting
threshold AR (SETAR) model, which considers partitions of the state space based on the quan-
tised value of xn−d, for some delay parameter d > 0. Clearly, the BCT-AR model class is much
richer and more general.

Mixture AR models. The mixture autoregressive (MAR) models of [83] are a generalisation
of the Gaussian mixture transition distribution (GMTD) models of [46]: They consist of a
simple linear mixture of K Gaussian AR components. Extensions of MAR models include the
conditional heteroscedastic (MAR-ARCH) model [85], the use of exogenous variables [84], and
the use of the Student-t distribution to model heavy tails [82], but these models seem less relevant
for the type of applications considered here, as their beneﬁts are limited to examples of datasets
possessing these speciﬁc characteristics (conditional heteroscedasticity, heavy tails, etc.).

When the BCT-AR posterior essentially concentrates on K models, T1, . . . , TK, (which was

commonly observed in practice), the posterior predictive distribution can be written as,

p(xn+1|x) =

K
(cid:88)

k=1

π(Tk|x) p(xn+1|Tk, x) ,

so that BCT-AR can be viewed as a generalised MAR model, with components corresponding
to the AR models at the leaves of each Tk, and with Bayesian weights automatically selected
as π(Tk|x).

Therefore, the BCT-AR model can be viewed as a generalisation of both the MAR and
SETAR model classes. Also, it allows for eﬀective Bayesian inference, implicitly overcoming
important challenges that arise naturally with SETAR and MAR models, including choosing
the delay parameter, the number of components, and the AR orders. The superiority of the
proposed approach is veriﬁed in the experiments reported in the next section.

4 Experimental results

We evaluate the performance of the BCT-AR model on simulated data and real data from
standard applications of nonlinear (univariate) time series from economics and ﬁnance, with
possibly limited training data. Among all the methods discussed in the Introduction, we compare
with the most successful previous approaches for these types of applications, considering both
classical and modern ML methods. Useful resources include the R package forecast [35] and
the Python library ‘GluonTS’ [3, 2], containing implementations of state-of-the-art classical and
ML methods, respectively. We brieﬂy discuss the methods used, and refer to the packages’
documentation and Appendix D for more details on the methods themselves and the training
procedures carried out.

Among classical statistical approaches, we compare with SETAR and MAR models (using
the R packages TSA [13] and mixAR [9]), and with ARIMA and Exponential smoothing state
space (ETS) models (implemented as auto.arima [35] and ets [33, 36] in forecast). Among
ML-based techniques, we compare with the Neural Network AR (NNAR) model (implemented
in forecast), with the most-successful RNN-based approaches, deepAR [69] and N-BEATS [54]

9

(both implemented in GluonTS), and with the GP time series model from [27], (also implemented
in GluonTS). Finally, we note that ‘Prophet’ [72] is another popular classical approach which
decomposes the time series to a trend and seasonal component, but it only uses time as a
regressor and “treats the forecasting problem as a curve-ﬁtting exercise.” So, it is not well-
suited for the applications considered here (which are governed by complex nonlinear dynamics)
and, as expected, in practice it was not competitive compared to the presented alternatives.

4.1 Simulated data

We ﬁrst perform a simulated experiment illustrating that our methods are consistent and eﬀec-
tive with data generated by BCT-SSM models. The context tree model used here is the tree of
Figure 1, the threshold of the quantiser is c = 0, and the AR order is p = 2. The exact BCT-AR
model is given by:

xn =






0.7 xn−1 − 0.3 xn−2 + en,
−0.3 xn−1 − 0.2 xn−2 + en,

0.5 xn−1 + en,

en ∼ N (0, 0.15),
en ∼ N (0, 0.10),
en ∼ N (0, 0.05),

if s = 1: xn−1 > 0,
if s = 01: xn−1 ≤ 0, xn−2 > 0,
if s = 00: xn−1 ≤ 0, xn−2 ≤ 0.

We ﬁrst examine the posterior over trees, π(T |x). On a time series consisting of only n = 100
observations, the MAP tree identiﬁed by the CBCT algorithm is the empty tree corresponding
to a single AR model, with posterior probability 99.9%. This means that the data do not
provide suﬃcient evidence to support a more complex state space partition. With n = 300
observations, the MAP tree is now the true underlying model, with posterior probability 57%.
And with n = 500 observations, the posterior of the true model is 99.9%. Therefore, the posterior
indeed concentrates on the true model, indicating that the BCT-AR inferential framework can
be very eﬀective even with limited training data. The complete BCT-AR model ﬁtted from
n = 1000 observations with its MAP estimated parameters, shown in (13), clearly indicates that
all estimates have essentially converged. In Appendix C.1, we also report values of the evidence
p(x|c, p), which is maximised at the true values of c = 0 and p = 2, verifying that our inferential
procedure for choosing them is also eﬀective.

xn =






0.66 xn−1 − 0.19 xn−2 + en,
−0.39 xn−1 − 0.27 xn−2 + en,
0.45 xn−1 − 0.03 xn−2 + en,

en ∼ N (0, 0.16),
en ∼ N (0, 0.12),
en ∼ N (0, 0.058),

if xn−1 > 0,
if xn−1 ≤ 0, xn−2 > 0,
if xn−1 ≤ 0, xn−2 ≤ 0.

(13)

Forecasting. We evaluate the performance of our methods in out-of-sample 1-step ahead
forecasts, and compare with state-of-the-art approaches in four simulated and three real datasets.
The complete descriptions of all datasets can be found in Appendix C. The ﬁrst simulated
dataset (sim 1) is generated by the BCT-AR model used above (Appendix C.1), and the second
(sim 2) by a BCT-AR model with a ternary tree of depth 2 (Appendix C.2). The third and
fourth ones (sim 3 and sim 4) are generated from SETAR models of orders p = 1 and p = 5,
respectively (Appendix C.3 and C.4). In each case, the training set consists of the ﬁrst 50% of
the observations; also, we update all models at every time-step in the test set. For BCT-AR, the
MAP tree with its MAP parameters is used at every time-step, which can be updated eﬃciently
(Section 3.1). From Table 1, it is observed that our methods outperform the alternatives, and
achieve the lowest mean-squared error (MSE) even on the two datasets generated from SETAR
models. As discussed in Section 3.2, the BCT-SSM methods also outperform the alternatives in
terms of empirical running times, reported in Appendix E.

10

Table 1: Mean squared error (MSE) of forecasts in simulated and real experiments

BCT-AR ARIMA ETS NNAR deepAR N-BEATS GP

SETAR MAR

sim 1
sim 2
sim 3
sim 4

unemp
gnp
ibm

0.131
0.035
0.216
0.891

0.034
0.324
78.02

0.150
0.050
0.267
1.556

0.040
0.364
82.90

0.178
0.054
0.293
1.614

0.042
0.378
77.52

0.143
0.048
0.252
1.287

0.036
0.393
78.90

0.148
0.061
0.273
1.573

0.036
0.473
75.71

0.232
0.112
0.357
2.081

0.054
0.490
77.90

0.181
0.086
0.359
2.046

0.050
0.562
87.27

0.141
0.050
0.243
0.951

0.038
0.394
81.07

0.151
0.064
0.283
1.543

0.037
0.384
77.02

4.2 US unemployment rate

An important application of SETAR models is in modelling the US unemployment rate [30,
52, 76, 67, 43]. As described in [52, 76], the unemployment rate moves countercyclically with
business cycles, and rises quickly but decays slowly, indicating nonlinear behaviour. Here, we
study the quarterly US unemployment rate in the time period from 1948 to 2019 (dataset unemp,
288 observations). Following [52], we consider the diﬀerence series ∆xn = xn − xn−1, and also
include a constant term in the AR model. For the quantiser alphabet size, m = 2 is a natural
choice here, as will become apparent below. The threshold selected using the procedure of
Section 3.2 is c = 0.15, and the resulting MAP tree is the tree of Figure 1, with depth d = 2,
leaves {1, 01, 00}, and posterior 91.5%. The complete BCT-AR model with its MAP parameters
is given below, where en ∼ N (0, 1),

∆xn =






0.09 + 0.72 ∆xn−1 − 0.30 ∆xn−2 + 0.42 en,
0.04 + 0.29 ∆xn−1 − 0.32 ∆xn−2 + 0.32 en,
−0.02 + 0.34 ∆xn−1 + 0.19 ∆xn−2 + 0.20 en,

if ∆xn−1 > 0.15,
if ∆xn−1 ≤ 0.15, ∆xn−2 > 0.15,
if ∆xn−1 ≤ 0.15, ∆xn−2 ≤ 0.15.

Interpretation. The MAP BCT-AR model ﬁnds signiﬁcant structure in the data, providing a
very natural interpretation. It identiﬁes 3 meaningful states: First, jumps in the unemployment
rate higher than 0.15 signify economic contractions (context 1). If there is not a jump at the
most recent time-point, the model looks further back to determine the state. Context 00 signiﬁes
a stable economy, as there are no jumps in the unemployment rate for two consecutive quarters.
Finally, context 01 identiﬁes an intermediate state: “stabilising just after a contraction”. An
important feature identiﬁed by the BCT-AR model is that the volatility is diﬀerent in each case:
Higher in contractions (σ = 0.42), smaller in stable economy regions (σ = 0.20), and in-between
for context 01 (σ = 0.32).

Forecasting. In addition to its appealing interpretation, the BCT-AR model outperforms
all benchmarks in forecasting (Table 1), giving a 6% lower MSE than the second-best methods.

4.3 US Gross National Product

Another important example of nonlinear time series in economics is the US Gross National
Product (GNP) [58, 30]. We study the quarterly US GNP in the time period from 1947 to 2019
(dataset gnp, 291 observations). Following [58], we consider the diﬀerence in the logarithm of
the series, yn = log xn − log xn−1.

11

As above, m = 2 is a natural choice for the quantiser size,
helping to diﬀerentiate economic expansions from contractions
– which govern the underlying dynamics. The MAP BCT-AR
tree is shown in Figure 2: It has depth d = 3, its leaves are
{0, 10, 110, 111} and its posterior is 42.6%. The complete set
of parameters at the leaves are shown in Appendix C.6.

Figure 2: MAP tree model

Interpretation. Compared with the previous example, here the MAP BCT-AR model ﬁnds
even richer structure in the data and identiﬁes four meaningful states. First, as before, there
is a single state corresponding to an economic contraction (which now corresponds to s = 0
instead of s = 1 as the GNP obviously increases in expansions and decreases in contractions).
And again, the model does not look further back whenever a contraction is detected. Here,
the model also shows that the eﬀect of a contraction is still present even after three quarters
(s = 110), and that the exact ‘distance’ from a contraction is also important, with the dynamics
changing depending on how much time has elapsed. Finally, the state s = 111 corresponds
to a ﬂourishing, expanding economy, without a contraction in the recent past. An important
feature captured by the BCT-AR model is again that the volatility is diﬀerent in each case.
More speciﬁcally, it is found that the volatility strictly decreases with the distance from the last
contraction, starting with the maximum σ = 1.23 for s = 0, and decreasing to σ = 0.75 for
s = 111 (see Appendix C.6).

Forecasting. The BCT-AR model outperforms all benchmarks in forecasting (Table 1),
giving a 12% lower MSE than the second-best method; also, it is computationally very eﬃcient
(see Appendix E).

4.4 The stock price of IBM

Finally, we examine the daily IBM common stock closing price
from May 17, 1961 to November 2, 1962 (dataset ibm, 369
observations), taken from [10]. This is a well-studied dataset
(e.g. [10, 51, 73, 51, 83]), with [10] ﬁtting an ARIMA model, [73]
ﬁtting a SETAR model and [83] ﬁtting a MAR model to the data.
Following previous approaches, we consider the ﬁrst-diﬀerence se-
ries, ∆xn = xn − xn−1. For the alphabet size of the quantiser we
choose m = 3, with contexts {0, 1, 2} naturally corresponding to the states {down, steady, up}
for the stock price. Using the procedure of Section 3.2 to select the thresholds, the resulting
quantiser regions are: s = 0 if ∆xn−1 < −7, s = 2 if ∆xn−1 > 7, and s = 1 otherwise. The
MAP tree is shown in Figure 3: It has depth d = 2, and its leaves are {0, 2, 10, 11, 12}, hence
identifying ﬁve states. Its posterior is 99.3%, suggesting that there is very strong evidence in the
data supporting this exact structure, even with only 369 observations. The complete BCT-AR
model with its MAP parameters is given in Appendix C.7.

Figure 3: MAP tree model

Interpretation. The BCT-AR model reveals important information about apparent struc-
ture in the data, which has not been identiﬁed before. Firstly, it admits a very simple and
natural interpretation: In order to determine the AR model generating the next value, we need
to look back until there is a signiﬁcant enough price change (corresponding to contexts 0, 2, 10,
12), or until we reach the maximum depth of 2 (context 11). Another important feature captured
by this model is the commonly observed asymmetric response in volatility due to positive and
negative shocks, sometimes called the leverage eﬀect [76, 10]. Even though there is no suggestion

12

01120of that in the prior, the MAP model shows that negative shocks increase the volatility much
more: Context 0 has the highest volatility (σ = 12.3), with 10 being a close second (σ = 10.8),
showing that the eﬀect of a past shock is still present. In all other cases the volatility is much
smaller (between σ = 5.17 and σ = 6.86).

Forecasting. From the results of Table 1, it is observed that in 1-step ahead forecasts
deepAR performs marginally better than the alternatives, with BCT-AR, MAR and N-BEATS
also having comparable performance. However, as presented in Appendix C.7, the BCT-AR
model is found to have the best performance in multi-step ahead forecasts (2-step and 3-step).
And because of its eﬃcient sequential updates, it also gives the smallest running times (Ap-
pendix E).

5 Limitations and future work

The main requirement potentially limiting the applicability of the proposed methods is that the
“estimated probabilities” of (5) need to be evaluated. This leads to several possible directions
for future work. First, as discussed in Section 2.3, when the integrals in (5) are not tractable,
they could be approximated and used to perform approximate inference for the general BCT-
SSM. This way, more general models like ARIMA, ARCH, or even ﬂexible models including GPs
and neural networks could be used as building blocks for the BCT-SSM mixture model. Also,
more general classes of quantisers can be considered for extracting discrete contexts (see Sec-
tion 2.1), allowing greater ﬂexibility in the state space partitions and enabling the identiﬁcation
of more complex dependencies in the data. Lastly, extending our methods to multivariate time
series is also feasible, and would greatly broaden the scope of applications of our hierarchical
Bayesian model.

References

[1] N.K. Ahmed, A.F. Atiya, N.E. Gayar, and H. El-Shishiny. An empirical comparison of machine

learning models for time series forecasting. Econometric reviews, 29(5-6):594–621, 2010.

[2] A. Alexandrov, K. Benidis, M. Bohlke-Schneider, V. Flunkert, J. Gasthaus, T. Januschowski, D.C.
Maddix, S. Rangapuram, D. Salinas, J. Schulz, et al. Gluonts: Probabilistic time series models in
python. arXiv preprint arXiv:1906.05264, 2019.

[3] A. Alexandrov, K. Benidis, V. Bohlke-Schneider, M.and Flunkert, J. Gasthaus, T. Januschowski,
D.C. Maddix, S.S. Rangapuram, D. Salinas, J. Schulz, et al. GluonTS: Probabilistic and neural time
series modeling in Python. Journal of Machine Learning Research, 21(116):1–6, 2020.

[4] F.M. Alvarez, A. Troncoso, J.C. Riquelme, and J.S.A. Ruiz. Energy time series forecasting based on
pattern sequence similarity. IEEE Transactions on Knowledge and Data Engineering, 23(8):1230–
1243, 2010.

[5] S. Alvisi, M. Franchini, and A. Marinelli. A short-term, pattern-based model for water-demand

forecasting. Journal of Hydroinformatics, 9(1):39–50, 2007.

[6] C. Andrieu, A. Doucet, and R. Holenstein. Particle Markov chain Monte Carlo methods. Journal

of the Royal Statistical Society: Series B (Statistical Methodology), 72(3):269–342, 2010.

[7] K. Benidis, S.S. Rangapuram, V. Flunkert, B. Wang, D. Maddix, C. Turkmen, J. Gasthaus,
M. Bohlke-Schneider, D. Salinas, L. Stella, et al. Neural forecasting: Introduction and literature
overview. arXiv preprint arXiv:2004.10240, 2020.

13

[8] D.J. Berndt and J. Cliﬀord. Using dynamic time warping to ﬁnd patterns in time series. In KDD

Workshop, volume 10, pages 359–370. Seattle, WA, USA, 1994.

[9] G. N. Boshnakov and D. Ravagli. mixAR: Mixture Autoregressive Models, 2021. R package version

0.22.5. https://CRAN.R-project.org/package=mixAR.

[10] G.E.P. Box, G.M. Jenkins, G.C. Reinsel, and G.M. Ljung. Time series analysis: forecasting and

control. John Wiley & Sons, 2015.

[11] J.Q. Candela, A. Girard, J. Larsen, and C.E. Rasmussen. Propagation of uncertainty in bayesian
kernel models-application to multiple-step ahead forecasting. In 2003 IEEE International Conference
on Acoustics, Speech, and Signal Processing, 2003. Proceedings.(ICASSP’03)., volume 2, pages II–
701. IEEE, 2003.

[12] K.S. Chan. Consistency and limiting distribution of the least squares estimator of a threshold

autoregressive model. Annals of Statistics, 21(1):520–533, 1993.

[13] K.S. Chan and B. Ripley. TSA: Time Series Analysis, 2020. R package version 1.3. https://CRAN.

R-project.org/package=TSA.

[14] S. Chib. Marginal likelihood from the Gibbs output. Journal of the american statistical association,

90(432):1313–1321, 1995.

[15] S. Chib and I. Jeliazkov. Marginal likelihood from the Metropolis–Hastings output. Journal of the

American statistical association, 96(453):270–281, 2001.

[16] J. Chung, K. Kastner, L. Dinh, K. Goel, A.C. Courville, and Y. Bengio. A recurrent latent variable

model for sequential data. Advances in neural information processing systems, 28, 2015.

[17] J.D. Cryer and K.S. Chan. Time series analysis: with applications in R. Springer Science & Business

Media, 2008.

[18] A. Doucet, N. De Freitas, N.J. Gordon, et al. Sequential Monte Carlo methods in practice. Springer,

2001.

[19] J. Durbin and S.J. Koopman. Time series analysis by state space methods, volume 38. OUP Oxford,

2012.

[20] S. Eleftheriadis, T. Nicholson, M. Deisenroth, and J. Hensman. Identiﬁcation of Gaussian process

state space models. Advances in neural information processing systems, 30, 2017.

[21] C. Faloutsos, J. Gasthaus, T. Januschowski, and Y. Wang. Forecasting big time series: old and new.

Proceedings of the VLDB Endowment, 11(12):2102–2105, 2018.

[22] N. Friel and A.N. Pettitt. Marginal likelihood estimation via power posteriors. Journal of the Royal

Statistical Society: Series B (Statistical Methodology), 70(3):589–607, 2008.

[23] R. Frigola. Bayesian time series learning with Gaussian processes. PhD thesis, University of Cam-

bridge, 2015.

[24] R. Frigola, Y. Chen, and C.E. Rasmussen. Variational Gaussian process state-space models. Advances

in neural information processing systems, 27, 2014.

[25] R. Frigola, F. Lindsten, T.B. Sch¨on, and C.Ed. Rasmussen. Bayesian inference and learning in Gaus-
sian process state-space models with particle MCMC. Advances in neural information processing
systems, 26, 2013.

[26] T.C. Fu, F.L. Chung, R. Luk, and C.M. Ng. Stock time series pattern matching: Template-based
vs. rule-based approaches. Engineering Applications of Artiﬁcial Intelligence, 20(3):347–364, 2007.

[27] A. Girard, C. Rasmussen, J.Q. Candela, and R. Murray-Smith. Gaussian process priors with uncer-
tain inputs application to multiple-step ahead time series forecasting. Advances in neural information
processing systems, 15, 2002.

14

[28] A. Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850,

2013.

[29] J.D. Hamilton. Time series analysis. Princeton university press, 2020.

[30] B.E. Hansen. Threshold autoregression in economics. Statistics and its Interface, 4(2):123–127, 2011.

[31] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780,

1997.

[32] Q. Hu, P. Su, D. Yu, and J. Liu. Pattern-based wind speed prediction based on generalized principal

component analysis. IEEE Transactions on Sustainable Energy, 5(3):866–874, 2014.

[33] R. Hyndman, A.B. Koehler, J.K. Ord, and R.D. Snyder. Forecasting with exponential smoothing:

the state space approach. Springer Science & Business Media, 2008.

[34] R. J. Hyndman.

fma: Data sets from ”Forecasting: methods and applications” by Makridakis,
Wheelwright & Hyndman (1998), 2020. R package version 2.4. https://cran.r-project.org/
package=fma.

[35] R.J. Hyndman and Y. Khandakar. Automatic time series forecasting: the forecast package for
R. Journal of Statistical Software, 26(3):1–22, 2008. https://CRAN.R-project.org/package=
forecast.

[36] R.J. Hyndman, A.B. Koehler, R.D. Snyder, and S. Grose. A state space framework for automatic
forecasting using exponential smoothing methods. International Journal of forecasting, 18(3):439–
454, 2002.

[37] R.E. Kalman. A new approach to linear ﬁltering and prediction problems. Transactions of the

American Society of Mathematical Engineering, Journal of Basic Engineering, 82(D):35–45, 1960.

[38] M. Karl, M. Soelch, J. Bayer, and P. Van der Smagt. Deep variational bayes ﬁlters: Unsupervised

learning of state space models from raw data. arXiv preprint arXiv:1605.06432, 2016.

[39] R.E. Kass and A.E. Raftery. Bayes factors. Journal of the American Statistical Association,

90(430):773–795, 1995.

[40] D.P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114,

2013.

[41] S. Konishi and G. Kitagawa.

Information criteria and statistical modeling. Springer Science &

Business Media, 2008.

[42] I. Kontoyiannis, L. Mertzanis, A. Panotopoulou, I. Papageorgiou, and M. Skoularidou. Bayesian
Context Trees: Modelling and exact inference for discrete time series. Journal of the Royal Statistical
Society: Series B (Statistical Methodology), April 2022. https://doi.org/10.1111/rssb.12511.

[43] G. Koop and S.M. Potter. Dynamic asymmetries in US unemployment. Journal of Business &

Economic Statistics, 17(3):298–312, 1999.

[44] R. Krishnan, U. Shalit, and D. Sontag. Structured inference networks for nonlinear state space

models. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 2017.

[45] R.G. Krishnan, U. Shalit, and D. Sontag. Deep kalman ﬁlters. arXiv preprint arXiv:1511.05121,

2015.

[46] N.D. Le, R.D. Martin, and A.E. Raftery. Modeling ﬂat stretches, bursts outliers in time series
using mixture transition distribution models. Journal of the American Statistical Association,
91(436):1504–1515, 1996.

[47] X. Liu, Z. Ni, D. Yuan, Y. Jiang, Z. Wu, J. Chen, and Y. Yang. A novel statistical time-series
pattern based interval forecasting strategy for activity durations in workﬂow systems. Journal of
Systems and Software, 84(3):354–376, 2011.

15

[48] D.J.C. MacKay. Bayesian interpolation. Neural Computation, 4(3):415–447, 1992.

[49] S. Makridakis, E. Spiliotis, and V. Assimakopoulos. The M4 Competition: Results, ﬁndings, con-

clusion and way forward. International Journal of Forecasting, 34(4):802–808, 2018.

[50] S. Makridakis, E. Spiliotis, and V. Assimakopoulos. Statistical and machine learning forecasting

methods: Concerns and ways forward. PloS one, 13(3):e0194889, 2018.

[51] S. Makridakis, S. Wheelwright, and R.J. Hyndman. Forecasting: methods and applications. John

Wiley & Sons, 1998.

[52] A.L. Montgomery, V. Zarnowitz, R.S. Tsay, and G.C. Tiao. Forecasting the US unemployment rate.

Journal of the American Statistical Association, 93(442):478–493, 1998.

[53] R. Murray-Smith and A. Girard. Gaussian Process priors with ARMA noise models. In Irish Signals

and Systems Conference, Maynooth, volume 147, page 152. Citeseer, 2001.

[54] B.N. Oreshkin, D. Carpov, N. Chapados, and Y. Bengio. N-BEATS: Neural basis expansion analysis

for interpretable time series forecasting. arXiv preprint arXiv:1905.10437, 2019.

[55] G. Ouyang, C. Dang, D.A. Richards, and X. Li. Ordinal pattern based similarity analysis for eeg

recordings. Clinical Neurophysiology, 121(5):694–703, 2010.

[56] I. Papageorgiou and I. Kontoyiannis. Posterior representations for Bayesian Context Trees: Sam-

pling, estimation and convergence. arXiv preprint arXiv:2202.02239, 2022.

[57] I. Papageorgiou, I. Kontoyiannis, L. Mertzanis, A. Panotopoulou, and M. Skoularidou. Revisiting
context-tree weighting for Bayesian inference. In 2021 IEEE International Symposium on Informa-
tion Theory (ISIT), pages 2906–2911, 2021.

[58] S.M. Potter. A nonlinear approach to US GNP. Journal of applied econometrics, 10(2):109–125,

1995.

[59] S.S. Rangapuram, M.W. Seeger, J. Gasthaus, L. Stella, Y. Wang, and T. Januschowski. Deep state
space models for time series forecasting. Advances in neural information processing systems, 31,
2018.

[60] C.E. Rasmussen and Z. Ghahramani. Occam’s razor. Advances in Neural Information Processing

Systems, pages 294–300, 2001.

[61] C.E. Rasmussen and C.K.I. Williams. Gaussian processes for machine learning. MIT Press, 2006.

[62] D.J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate infer-
ence in deep generative models. In International conference on machine learning, pages 1278–1286.
PMLR, 2014.

[63] J. Rissanen. A universal data compression system.

IEEE Transactions on Information Theory,

29(5):656–664, 1983.

[64] J. Rissanen. A universal prior for integers and estimation by minimum description length. Annals

of Statistics, 11(2):416–431, 1983.

[65] J. Rissanen. Complexity of strings in the class of markov sources. IEEE Transactions on Information

Theory, 32(4):526–532, 1986.

[66] S. Roberts, M. Osborne, M. Ebden, S. Reece, N. Gibson, and S. Aigrain. Gaussian processes for
time-series modelling. Philosophical Transactions of the Royal Society A: Mathematical, Physical
and Engineering Sciences, 371(1984):20110550, 2013.

[67] P. Rothman. Forecasting asymmetric unemployment rates. Review of Economics and Statistics,

80(1):164–168, 1998.

16

[68] E. Sabeti, P.X.K. Song, and A.O. Hero. Pattern-based analysis of time series: Estimation. In 2020

IEEE International Symposium on Information Theory (ISIT), pages 1236–1241, 2020.

[69] D. Salinas, V. Flunkert, J. Gasthaus, and T. Januschowski. DeepAR: Probabilistic forecasting with
autoregressive recurrent networks. International Journal of Forecasting, 36(3):1181–1191, 2020.

[70] G. Schwarz. Estimating the dimension of a model. Annals of Statistics, 6(2):461–464, 1978.

[71] A.F.M. Smith and D.J. Spiegelhalter. Bayes factors and choice criteria for linear models. Journal

of the Royal Statistical Society: Series B (Methodological), 42(2):213–220, 1980.

[72] S.J. Taylor and B. Letham. Forecasting at scale. The American Statistician, 72(1):37–45, 2018.

[73] H. Tong. Non-linear time series: a dynamical system approach. Oxford University Press, 1990.

[74] H. Tong. Threshold models in time series analysis—30 years on. Statistics and its Interface, 4(2):107–

118, 2011.

[75] H. Tong and K.S. Lim. Threshold autoregression, limit cycles and cyclical data. Journal of the Royal

Statistical Society: Series B (Statistical Methodology), 42(3):245–268, 1980.

[76] R.S. Tsay. Analysis of ﬁnancial time series, volume 543. John Wiley & Sons, 2005.

[77] R. Turner, M. Deisenroth, and C.E. Rasmussen. State-space inference and learning with Gaussian
processes. In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence and
Statistics, pages 868–875. JMLR Workshop and Conference Proceedings, 2010.

[78] R. D. Turner. Gaussian processes for state space models and change point detection. PhD thesis,

University of Cambridge, 2012.

[79] M.J. Weinberger, N. Merhav, and M. Feder. Optimal sequential probability assignment for individual

sequences. IEEE Transactions on Information Theory, 40(2):384–396, 1994.

[80] F.M.J. Willems. The context-tree weighting method: extensions. IEEE Transactions on Information

Theory, 44(2):792–798, 1998.

[81] F.M.J. Willems, Y.M. Shtarkov, and T.J. Tjalkens. The context-tree weighting method: basic

properties. IEEE Transactions on Information Theory, 41(3):653–664, 1995.

[82] C. S. Wong, W. S. Chan, and P. L. Kam. A student t-mixture autoregressive model with applications

to heavy-tailed ﬁnancial data. Biometrika, 96(3):751–760, 2009.

[83] C. S. Wong and W. K. Li. On a mixture autoregressive model. Journal of the Royal Statistical

Society: Series B (Statistical Methodology), 62(1):95–115, 2000.

[84] C. S. Wong and W. K. Li. On a logistic mixture autoregressive model. Biometrika, 88(3):833–846,

2001.

[85] C. S. Wong and W. K. Li. On a mixture autoregressive conditional heteroscedastic model. Journal

of the American Statistical Association, 96(455):982–995, 2001.

[86] S.N. Wood. Fast stable restricted maximum likelihood and marginal likelihood estimation of semi-
parametric generalized linear models. Journal of the Royal Statistical Society: Series B (Statistical
Methodology), 73(1):3–36, 2011.

[87] H.F. Yu, N. Rao, and I.S. Dhillon. Temporal regularized matrix factorization for high-dimensional

time series prediction. Advances in neural information processing systems, 29, 2016.

[88] G. Zhang, B.E. Patuwo, and M.Y. Hu. Forecasting with artiﬁcial neural networks: The state of the

art. International journal of forecasting, 14(1):35–62, 1998.

[89] X. Zheng, M. Zaheer, A. Ahmed, Y. Wang, E.P. Xing, and A.J. Smola. State space LSTM models

with particle MCMC inference. arXiv preprint arXiv:1711.11179, 2017.

17

Appendix

A Proofs of Theorems 1 and 2

The important observation here is that using our diﬀerent form of the estimated probabilities
Pe(s, x) in (5), it is still possible to factorise the marginal likelihoods p(x|T ) for the general
BCT-SSM as,
(cid:90)

(cid:19)

(cid:90) (cid:89)

(cid:18) (cid:89)

p(xi|T, θs, xi−1

−D+1) π(θs) dθs

(cid:89)

=

Pe(s, x),

p(x|T ) =

p(x|θ, T )π(θ|T )dθ =

s∈T

i∈Bs

s∈T

where the second equality follows from the likelihood of the general BCT-SSM in (2), and the fact
that we use independent priors on the parameters at the leaves, so that π(θ|T ) = (cid:81)
s∈T π(θs).
Then, the proofs of Theorems 1 and 2 follow along the same lines as the proofs of the
corresponding results for discrete time series in [42], with the main diﬀerence being that our
new general version of the estimated probabilities Pe(s, x) of (5) need to be used in place of their
simple discrete versions. Before giving the proofs of the theorems, we recall a useful property
for the BCT prior πD(T ). Let Λ = {λ} denote the empty tree consisting only of the root node
λ. Any tree T (cid:54)= Λ can be expressed as the union T = ∪jTj of a collection of m subtrees
T0, T1, . . . , Tm−1, and its prior can be decomposed as [42]:

Lemma A.
Tj ∈ T (D − 1), then,

If T ∈ T (D), T (cid:54)= Λ, is expressed as the union T = ∪jTj of the subtrees

πD(T ) = αm−1

m−1
(cid:89)

j=0

πD−1(Tj).

(14)

A.1 Proof of Theorem 1

The proof is by induction. We want to show that:

Pw,λ = p(x) =

(cid:88)

T ∈T (D)

π(T )p(x|T ) =

(cid:88)

πD(T )

T ∈T (D)

(cid:89)

s∈T

Pe(s, x).

(15)

We claim that the following more general statement holds: For any node s at depth d with
0 ≤ d ≤ D, we have,

Pw,s =

πD−d(U )

Pe(su, x),

(16)

(cid:88)

(cid:89)

u∈U
where su denotes the concatenation of contexts s and u.

U ∈T (D−d)

Clearly (16) implies (15) upon taking s = λ (i.e., with d = 0). Also, (16) is trivially true for

nodes s at level D, since it reduces to the fact that Pw,s = Pe,s for leaves s, by deﬁnition.

Suppose (16) holds for all nodes s at depth d for some ﬁxed 0 < d ≤ D. Let s be a node at

depth d − 1; then, by the inductive hypothesis,

Pw,s =βPe(s, x) + (1 − β)

=βPe(s, x) + (1 − β)

m−1
(cid:89)

j=0

m−1
(cid:89)

Pw,sj





(cid:88)

πD−d(Tj)

j=0

Tj ∈T (D−d)



Pe(sjt, x)

 ,

(cid:89)

t∈Tj

where sjt denotes the concatenation of context s, then symbol j, then context t, in that order.

18

So,

Pw,s =βPe(s, x) + (1 − β)

(cid:88)

m−1
(cid:89)

T0,T1,...,Tm−1∈T (D−d)

j=0



πD−d(Tj)

(cid:89)

t∈Tj





Pe(sjt, x)

=βPe(s, x) +

1 − β
αm−1

(cid:88)

πD−d+1(∪jTj)





m−1
(cid:89)



(cid:89)

Pe(sjt, x)

 ,

T0,T1,...,Tm−1∈T (D−d)

j=0

t∈Tj

where for the last step we have used (14) from Lemma A.

Concatenating every symbol j with every leaf of the corresponding tree Tj, we end up with

all the leaves of the larger tree ∪jTj. Therefore,

Pw,s = βPe(s, x) +

1 − β
αm−1

(cid:88)

πD−d+1(∪jTj)

T0,T1,...,Tm−1∈T (D−d)

(cid:89)

t∈∪j Tj

Pe(st, x),

and since 1 − β = αm−1 and πd(Λ) = β for all d ≥ 1,

Pw,s =πD−d+1(Λ)Pe(s, x) +

(cid:88)

πD−d+1(∪jTj)

T0,T1,...,Tm−1∈T (D−d)

(cid:89)

t∈∪j Tj

Pe(st, x)

=πD−d+1(Λ)Pe(s, x) +

(cid:88)

T ∈T (D−d+1),T (cid:54)=Λ

(cid:88)

=

πD−d+1(T )

T ∈T (D−d+1)

Pe(st, x).

(cid:89)

t∈T

πD−d+1(T )

Pe(st, x)

(cid:89)

t∈T

This establishes (16) for all nodes s at depth d − 1, completing the inductive step and the proof
(cid:3)
of the theorem.

A.2 Proof of Theorem 2

As the proof follows very much along the same lines as that of Theorem 3.2 of [42], most of the
details are omitted here.

The proof is again by induction. First, we claim that:

Pm,λ = max
T ∈T (D)

p(x, T ) = max

T ∈T (D)

πD(T )

(cid:89)

s∈T

Pe(s, x).

(17)

As in the proof of Theorem 1, in fact we claim that the following more general statement holds:
For any node s at depth d with 0 ≤ d ≤ D, we have,

Pm,s = max

U ∈T (D−d)

πD−d(U )

(cid:89)

u∈U

Pe(su, x),

(18)

where su denotes the concatenation of contexts s and u. The proof of this is by an inductive
step similar to that of Theorem 1. Taking s = λ in (18) implies (17).

Then, it is suﬃcient to show that for the tree T ∗

1 that is produced by the CBCT algorithm,
1 ). This is again proved by induction, via an argument similar to the ones in the

Pm,λ = p(x, T ∗
previous two cases.

Finally, using (17) and dividing both sides with p(x) completes the proof, since we get:

max
T ∈T (D)

π(T |x) = π(T ∗

1 |x).

19

(cid:3)

A.3 The k-CBCT algorithm

The k-BCT algorithm of [42] can be generalised in a similar manner to the way the CTW and
BCT algorithms were generalised. The resulting algorithm identiﬁes the top-k a posteriori most
likely context trees. The proof of the theorem claiming this is similar to the proof of Theorem 3.3
of [42] and thus omitted. Again, the important diﬀerence, both in the algorithm description and
in the proof, is that the estimated probabilities Pe(s, x) are used in place of their simple discrete
version Pe(as).

B Proofs of Lemmas 1 and 2

The proofs of these lemmas are mostly based on explicit computations. Recall that, for each
context s, the set Bs consists of those indices i ∈ {1, 2, . . . , n} such that the context of xi is s.
The important step in the following two proofs is the factorisation of the likelihood using the
sets Bs. In order to prove the lemmas for the AR model with parameters θs = (φs, σ2
s ), we ﬁrst
consider an intermediate step in which we assume the noise variance to be known and equal
to σ2.

B.1 Known noise variance

Here, to any leaf s of the context tree T , we associate an AR model with known variance σ2, so
that,

xn = φs,1xn−1 + · · · + φs,pxn−p + en = φs

T

(cid:101)xn−1 + en,

en ∼ N (0, σ2).

(19)

In this setting, the parameters of the model are only the AR coeﬃcients θs = φs. For these, we
use a Gaussian prior,

where µo, Σo are hyperparameters.
probabilities Pe(s, x).

θs ∼ N (µo, Σo) ,

(20)

In this setting we prove the following for the estimated

Lemma B. The estimated probabilities Pe(s, x) for the known-variance case are given by,

(21)

(22)

Pe(s, x) =

1
(2πσ2)|Bs|/2

1
det(I + ΣoS3/σ2)

(cid:112)

(cid:26)

exp

−

(cid:27)

,

Es
2σ2

where I is the identity matrix and Es is given by:

Es = s1 + σ2µT

o Σ−1

o µo − (s2 + σ2Σ−1

o µo)T(S3 + σ2Σ−1

o )−1(s2 + σ2Σ−1

o µo) .

Proof. For the AR model of (19),

p(xi|T, θs, xi−1

−D+1) =

√

1
2πσ2

(cid:26)

exp

−

1
2σ2 (xi − θs

T

(cid:101)xi−1)2

(cid:27)

,

so that,

(cid:89)

i∈Bs

p(xi|T, θs, xi−1

−D+1) =

1

2πσ2)|Bs|

√

(

(cid:26)

exp

−

1
2σ2

(cid:88)

i∈Bs

(xi − θs

T

(cid:101)xi−1)2

(cid:27)

.

20

Expanding the sum in the exponent gives,

(cid:88)

i∈Bs

(xi − θs

T

(cid:101)xi−1)2 =

(cid:88)

i − 2θT
x2
s

(cid:88)

xi(cid:101)xi−1 + θT

s

i∈Bs
= s1 − 2θT

i∈Bs
s s2 + θT
s S3θs,

(cid:88)

i∈Bs

(cid:101)xi−1(cid:101)xT

i−1θs

from which we obtain that,

(cid:89)

i∈Bs

p(xi|T, θs, xi−1

−D+1) =

(cid:26)

exp

−

1
2σ2 (s1 − 2θT

s s2 + θT

s S3θs)

(cid:27)

1

√
(
√

=(

2πσ2)|Bs|
2π)pρs N (θs; µ, S) ,

by completing the square, where µ = S−1

3 s2, S = σ2S−1

3 , and,

(cid:115)

ρs =

det(σ2S−1
3 )
(2πσ2)|Bs|

(cid:26)

exp

−

So, multiplying with the prior:

1
2σ2 (s1 − sT

2 S−1

3 s2)

(cid:27)

.

(23)

p(xi|T, θs, xi−1

−D+1)π(θs) = (

√

(cid:89)

i∈Bs

2π)pρs N (θs; µ, S) N (θs; µo, Σo) = ρsZs N (θs; m, Σ),

where Σ−1 = Σ−1

o + S−1, m = Σ (Σ−1

o µo + S−1µ), and,

Zs =

(cid:113)

1
det(Σo + σ2S−1
3 )

(cid:26)

exp

−

1
2

(µo − S−1

3 s2)T(Σo + σ2S−1

3 )−1(µo − S−1

3 s2)

(cid:27)

.

(24)

Therefore,

and hence,

(cid:89)

i∈Bs

p(xi|T, θs, xi−1

−D+1)π(θs) = ρsZs N (θs; m, Σ),

(25)

Pe(s, x) =

(cid:90) (cid:89)

i∈Bs

p(xi|T, θs, xi−1

−D+1) π(θs) dθs = ρsZs.

Using standard matrix inversion properties, after some algebra the product ρsZs can be rear-
(cid:3)
ranged to give exactly the required expression in (21).

B.2 Proof of Lemma 1

Now, we move back to the original case, as described in the paper, where the noise variance is
considered to be a parameter of the AR model, so that θs = (φs, σ2
s ). Here, the joint prior on
the parameters is π(θs) = π(φs|σ2

s ), where,

s )π(σ2
σ2
s ∼ Inv-Gamma(τ, λ) ,
φs|σ2

s ∼ N (µo, σ2

s Σo) ,

(26)

(27)

and where (τ, λ, µo, Σo) are hyperparameters.

21

For Pe(s, x), we just need to compute the integral:
(cid:90) (cid:89)

Pe(s, x) =

p(xi|T, θs, xi−1

−D+1) π(θs) dθs

i∈Bs

π(σ2
s )

(cid:90)

=

(cid:32)(cid:90) (cid:89)

i∈Bs

p(xi|T, φs, σ2

s , xi−1

−D+1) π(φs|σ2

s ) dφs

(28)

(29)

(cid:33)

dσ2
s .

The inner integral has exactly the form of the estimated probabilities Pe(s, x) from the previous
section, where the noise variance was ﬁxed. The only diﬀerence is that the prior π(φs|σ2
s ) of (27)
now has covariance matrix σ2
s Σo,
we get,

s Σo instead of Σo. So, using (21)-(22), with Σo replaced by σ2

Pe(s, x) =

(cid:90)

(cid:26)

π(σ2
s )

C−1
s

(cid:18) 1
σ2
s

(cid:19)|Bs|/2

(cid:18)

(cid:19)(cid:27)

exp

−

dσ2
s ,

Ds
2σ2
s

with Cs and Ds as in Lemma 1. And using the inverse-gamma prior π(σ2

s ) of (26),

Pe(s, x) = C−1

s

λτ
Γ(τ )

(cid:90) (cid:18) 1
σ2
s

(cid:19)τ (cid:48)+1

(cid:18)

exp

−

(cid:19)

dσ2
s ,

λ(cid:48)
σ2
s

(30)

with τ (cid:48) = τ + |Bs|

2 and λ(cid:48) = λ + Ds
2 .

The integral in (30) has the form of an inverse-gamma density with parameters τ (cid:48) and λ(cid:48).

Its closed-form solution, as required, completes the proof of the lemma:

Pe(s, x) = C−1

s

λτ
Γ(τ )

Γ(τ (cid:48))
(λ(cid:48))τ (cid:48) .

(cid:3)

B.3 Proof of Lemma 2

In order to derive the required expressions for the posterior distributions of φs and σ2
leaf s of model T , ﬁrst consider the joint posterior π(θs|T, x) = π(φs, σ2
s |T, x), given by,
n
(cid:89)

(cid:89)

p(xi|T, θs, xi−1

−D+1)π(θs) ∝

p(xi|T, θs, xi−1

−D+1)π(θs),

π(θs|T, x) ∝ p(x|T, θs)π(θs) =

s , for a

where we used the fact that, in the product, only the terms involving indices i ∈ Bs are functions
of θs. So,

i=1

i∈Bs

π(φs, σ2

s |T, x) ∝

(cid:32)

(cid:89)

i∈Bs

p(xi|T, φs, σ2

s , xi−1

−D+1) π(φs|σ2
s )

(cid:33)

π(σ2

s ) .

Here, the ﬁrst two terms can be computed from (25) of the previous section, where the noise
variance was known. Again, the only diﬀerence is that we have to replace Σo with σ2
s Σo because
of the prior π(φs|σ2

s ) deﬁned in (27). After some algebra, this gives,

π(φs, σ2

s |T, x) ∝

(cid:19)|Bs|/2

(cid:18) 1
σ2
s

N (φs; ms, Σs) π(σ2

s ) ,

with ms deﬁned as in Lemma 2, and Σs = σ2

(cid:19)

(cid:18)

exp

−

Ds
2σ2
s
o )−1.
s (S3 + Σ−1

22

Substituting the prior π(σ2

s ) in the last expression,

π(φs, σ2

s |T, x) ∝

(cid:19)τ +1+|Bs|/2

(cid:18)

exp

−

(cid:18) 1
σ2
s

(cid:19)

λ + Ds/2
σ2
s

N (φs; ms, Σs) .

(31)

From (31), it is easy to integrate out φs and get the posterior of σ2
s ,

(cid:90)

π(σ2

s |T, x) =

π(φs, σ2

s |T, x) dφs ∝

(cid:19)τ +1+|Bs|/2

(cid:18)

exp

−

(cid:18) 1
σ2
s

λ + Ds/2
σ2
s

(cid:19)
,

which is of the form of an inverse-gamma distribution with parameters τ (cid:48) = τ + |Bs|
2
λ(cid:48) = λ + Ds

2 , proving the ﬁrst part of the lemma.

However, as Σs is a function of σ2

s , integrating out σ2

s requires more algebra. We have,

and

N (φs; ms, Σs) ∝

(cid:112)

1
det(Σs)
(cid:19)p/2

(cid:26)

exp

−

1
2

(φs − ms)TΣ−1

s (φs − ms)

(cid:27)

∝

(cid:18) 1
σ2
s

(cid:26)

exp

−

1
2σ2
s

(φs − ms)T(S3 + Σ−1

o )(φs − ms)

and substituting this in (31) gives that π(φs, σ2

s |T, x) is proportional to,

(cid:19)τ +1+ |Bs|+p

2

(cid:18) 1
σ2
s

(cid:26)

exp

−

1
2σ2
s

(cid:18)

2λ + Ds + (φs − ms)T(S3 + Σ−1

o )(φs − ms)

,

which as a function of σ2
out σ2

s . Denoting L = 2λ + Ds + (φs − ms)T(S3 + Σ−1

s has the form of an inverse-gamma density, allowing us to integrate

(cid:27)

,

(cid:19)(cid:27)

o )(φs − ms), and (cid:101)τ = τ + |Bs|+p
(cid:19)
(cid:19)(cid:101)τ +1

(cid:18)

2

L
2σ2
s

dσ2

s =

Γ((cid:101)τ )
(L/2)(cid:101)τ

exp

−

,

.

(cid:90)

π(φs|T, x) =

π(φs, σ2

s |T, x) dσ2

s ∝

(cid:90) (cid:18) 1
σ2
s

So, as a function of φs, the posterior π(φs|T, x) is,

π(φs|T, x) ∝ L−(cid:101)τ =

2λ + Ds + (φs − ms)T(S3 + Σ−1

o )(φs − ms)

(cid:18)

(cid:19)− 2τ +|Bs|+p

2

(cid:18)

∝

1 +

1
2τ + |Bs|

(φs − ms)T (S3 + Σ−1

o )(2τ + |Bs|)

(cid:19)− 2τ +|Bs|+p

2

(φs − ms)

(cid:18)

∝

1 +

1
ν

(φs − ms)TP −1

s

(φs − ms)

,

(2λ + Ds)
(cid:19)− ν+p

2

which is exactly in the form of a multivariate t-distribution, with p being the dimension of φs,
(cid:3)
and with ν, ms and Ps exactly as given in Lemma 2, completing the proof.

23

C List of datasets

C.1 sim 1

The dataset sim 1 is a simulated dataset, which consists of n = 600 observations generated from
a BCT-AR model with the tree model of Figure 1, quantiser threshold c = 0, and AR order
p = 2. The complete speciﬁcation of this BCT-AR model, is also given in the main text,

xn =






0.7 xn−1 − 0.3 xn−2 + en,
−0.3 xn−1 − 0.2 xn−2 + en,

0.5 xn−1 + en,

en ∼ N (0, 0.15),
en ∼ N (0, 0.10),
en ∼ N (0, 0.05),

if s = 1: xn−1 > 0,
if s = 01: xn−1 ≤ 0, xn−2 > 0,
if s = 00: xn−1 ≤ 0, xn−2 ≤ 0.

Here, we also report the evidence p(x|c, p) for a range of values of c and p. Although maximising
the evidence is a very common, well-justiﬁed Bayesian practice [60, 48, 61], here we report some
values as a sanity check, to show that the evidence is indeed maximised at the true values of
c = 0.0 and p = 2, and thus verify in practice that our inferential procedure for choosing c and
p is eﬀective.

Table 2: Using the evidence p(x|c, p) to choose the AR order and the quantiser threshold

AR order p

Threshold c

1

2

3

4

5

−0.1 −0.05

0

0.05

0.1

− log2 p(x|c, p)

533 519 526

531

535

558

539

519

555

577

C.2 sim 2

The dataset sim 2 is a simulated dataset, which consists
of n = 500 observations generated from a BCT-AR model
with respect to the ternary tree in Figure 4. The thresh-
olds of the quantiser are {c1 = −0.5, c2 = 0.5}, and the
AR order is p = 1. The complete speciﬁcation of this
BCT-AR model, is given by,

xn =

(cid:26) 0.5 en,

0.99 xn−1 + 0.005 en,

if s = 1, 01, 02, 20, 21,
if s = 00, 22,

with en ∼ N (0, 1).

C.3 sim 3

Figure 4: Tree model of sim 2

The dataset sim 3 is a simulated dataset, which consists of n = 150 observations generated from
a SETAR model of order p = 1, given by,

xn =

(cid:26) −0.25 + 0.9 xn−1 + 0.5 en,
0.15 − 0.8 xn−1 + 0.2 en,

if xn−1 > 0.15,
if xn−1 ≤ 0.15,

where here, as it is common practice, for simplicity we include the noise variance in the coeﬃ-
cients of en, so that en ∼ N (0, 1).

24

012C.4 sim 4

The dataset sim 4 is a simulated dataset, which consists of n = 200 observations generated from
a SETAR model of order p = 5, given by,

xn =

(cid:26) −0.1 + 0.9 xn−1 + 0.9 xn−2 − 0.2 xn−5 + en,

0.2 + 0.1 xn−1 + 0.9 xn−5 + en,

if xn−1 > −0.2,
if xn−1 ≤ −0.2,

with en ∼ N (0, 1).

C.5 unemp

The dataset unemp is a real dataset, which consists of n = 288 observations of the quarterly
US unemployment rate in the time period from 1948 to 2019. It is publicly available from the
US Bureau of Labor Statistics (BLS), at https://data.bls.gov/timeseries/LNS14000000?
years_option=all_years.

C.6 gnp

The dataset gnp is a real dataset, which consists of n = 291 observations of the quarterly US
Gross National Product (GNP) in the time period from 1947 to 2019. It is available from the
US Bureau of Economic Analysis (BEA), and can be retrieved from the Federal Reserve Bank
of St. Louis (FRED) at https://fred.stlouisfed.org/series/GNP.

For this dataset, the MAP BCT tree model is given in the main text, in Figure 2: It has
depth d = 3, four leaves {0, 10, 110, 111}, and posterior 42.6%. The threshold of the binary
quantiser selected using the procedure of Section 3.2 is c = 0.2, so that s = 1 if yn−1 > 0.2 and
s = 0 if yn−1 ≤ 0.2. The AR order selected is p = 2. The complete BCT-AR model, with its
MAP estimated parameters is given by,




1.156 + 0.714 yn−1 + 0.186 yn−2 + 1.23 en,
0.177 + 0.679 yn−1 − 0.260 yn−2 + 1.19 en
−1.054 + 1.403 yn−1 + 0.194 yn−2 + 1.04 en
0.593 + 0.281 yn−1 + 0.305 yn−2 + 0.75 en

if s = 0,
if s = 10,
if s = 110,
if s = 111,

yn =

with en ∼ N (0, 1).

C.7 ibm

The dataset ibm is a real dataset, which consists of n = 369 observations of the daily IBM
common stock closing price, in the time period from May 17, 1961 to November 2, 1962. The
dataset is taken from [10], and it is also available from the R package fma [34]. The MAP tree
model ﬁtted to the dataset is shown in the main text at Figure 3. The complete BCT-AR model,
with its MAP estimated parameters, is given by,

xn =






1.03 xn−1 − 0.03 xn−2 + 12.3 en,
1.17 xn−1 − 0.17 xn−2 + 6.86 en,
−0.11 xn−1 + 1.11 xn−2 + 10.8 en,
1.22 xn−1 − 0.22 xn−2 + 5.32 en,
0.15 xn−1 + 0.85 xn−2 + 5.17 en,

if s = 0,
if s = 2,
if s = 10,
if s = 11,
if s = 12,

with en ∼ N (0, 1).

25

Multi-step ahead forecasting. As explained in the main text, in this example many
approaches were found to have similar prediction performance when considering 1-step ahead
forecasts. For this reason, we perform more extensive comparisons, and also consider multi-
step ahead forecasts (2-step and 3-step). For multi-step-ahead forecasts, we use the parametric
bootstrap of [76, p. 192]: We sample trajectories from the model, and use the sample average
as the point forecast. From the results of Table 3, it is observed that even though deepAR has
the best performance in 1-step ahead forecasts, its performance quickly deteriorates with larger
prediction horizons, and BCT-AR has the best performance in the multi-step ahead forecasts
(both 2-step and 3-step).

Table 3: Mean squared error (MSE) of multi-step ahead forecasts in the ibm dataset

BCT-AR ARIMA ETS NNAR deepAR N-BEATS GP

SETAR MAR

1-step
2-step
3-step

78.02
75.72
75.79

82.90
79.20
78.42

77.52
77.99
77.57

78.90
75.92
76.21

75.71
83.74
83.87

77.90
75.78
78.55

87.27
87.16
85.32

81.07
78.68
78.64

77.02
76.49
76.58

D Training details

Here we specify the training details for all the methods used in the forecasting experiments.
In all the examples the training set consists of the ﬁrst 50% of the observations, and we allow
updates at every timestep for all methods. From the R package forecast [35], the functions
auto.arima and ets for ﬁtting the ARIMA and ETS models are completely automated, so
there are no parameters to specify here. For NNAR, we use the function nnetar which is also
contained in the forecast package, and search over AR orders between p = 1 and p = 5. For
the BCT-AR model, we choose the AR order and the quantiser thresholds at the end of the
training set, and then use the MAP tree model and parameters updated at every timestep, as
explained in detail in the main text.

For deepAR, N-BEATS, and GP, we use the implementations in the Python library Glu-
onTS [3]. As the computational cost per iteration diﬀers for these methods, we use slightly diﬀer-
ent numbers of epochs and batches-per-epoch for each of them, in order to give similar empirical
running times – which are still much higher than those of our methods; see below. For deepAR
we use 5 epochs with 50 batches/epoch, for GP we use 10 epochs with 100 batches/epoch, and
for N-BEATS we use 3 epochs with 20 batches/epoch. In all cases we try AR orders between
p = 1 and p = 5. For the SETAR model, we use the R package TSA [13], along with the commonly
used conditional least squares method of [12]. We search over AR orders between p = 1 and
p = 5, and values of the delay parameter between d = 1 and d = 5. For the MAR model, we
use the R package mix-AR [9], and try K = 2 and K = 3 components with AR orders between
p = 1 and p = 5.

E Empirical running times

As discussed in Section 3.1, an important advantage of the BCT-AR framework is that the
associated algorithms allow for very eﬃcient sequential updates, making it very practical for
online forecasting applications. This is in sharp contrast with deepAR, N-BEATS, and GP,

26

whose current implementation in GluonTS does not allow for incremental training, so the models
are re-trained in each timestep from scratch; something which is computationally very costly.
The classical statistical approaches lie somewhere in-between the two extremes, as they are re-
trained at every timestep using the forecast package, but the cost required per timestep is
lower than that of the ML methods; see also [50]. From Table 4 it is observed that, because
of its eﬃcient sequential updates, BCT-AR clearly outperforms all the benchmarks in terms
of empirical running times. The only method achieving somewhat comparable performance is
ETS, but BCT-AR was found to perform much better in terms of MSE. All experiments were
carried out on a common laptop.

Table 4: Empirical running times*

BCT-AR ARIMA ETS NNAR deepAR N-BEATS GP

SETAR MAR

sim 1
sim 2
sim 3
sim 4

7.4 s
8.1 s
1.6 s
2.4 s

unemp 3.1 s
2.2 s
gnp
4.6 s
ibm

1.1 min
1.0 min
22 s
28 s

42 s
1.3 min
58 s

31 s
23 s
3.2 s
5.2 s

11 s
10 s
16 s

4.9 min
3.2 min
9.3 s
48 s

1.2 min
1.5 min
32 s

2.4 h
2.1 h
36 min
55 min

59 min
1.5 h
2.2 h

7.4 h
6.3 h
2.5 h
4.0 h

4.1 h
5.2 h
5.3 h

19 min
5.2 h 1.3 min
4.3 h 2.0 min 2.5 min
2.2 min
1.1 h
2.7 min
2.2 h

5.2 s
11 s

1.6 h
2.0 h
3.6 h

17 s
19 s
28 s

6.4 min
6.6 min
7.6 min

*The empirical running times of deepAR, N-BEATS and GP could be greatly reduced if the
model was not updated at every timestep. In this case, the diﬀerence from BCT-AR would not
be so substantial. Nonetheless, even the fact that gradient optimisation is needed (involving a
signiﬁcant number of iterations) makes their training per-timestep much more computationally
expensive compared to BCT-AR; see also the review [50] discussing the computational require-
ments of recent ML methods in time series forecasting.

27

