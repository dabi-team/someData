9
1
0
2

r
a

M
4

]
T
S
.
h
t
a
m

[

1
v
3
5
2
1
0
.
3
0
9
1
:
v
i
X
r
a

Multiscale Inference
and Long-Run Variance Estimation
in Nonparametric Regression
with Time Series Errors

Marina Khismatullina1
University of Bonn

Michael Vogt2
University of Bonn

In this paper, we develop new multiscale methods to test qualitative hypotheses
about the function m in the nonparametric regression model Yt,T = m(t/T ) + εt
with time series errors εt. In time series applications, m represents a nonpara-
metric time trend. Practitioners are often interested in whether the trend m has
certain shape properties. For example, they would like to know whether m is
constant or whether it is increasing/decreasing in certain time regions. Our mul-
tiscale methods allow to test for such shape properties of the trend m. In order
to perform the methods, we require an estimator of the long-run error variance
σ2 = (cid:80)∞
(cid:96)=−∞ Cov(ε0, ε(cid:96)). We propose a new diﬀerence-based estimator of σ2 for
the case that {εt} is an AR(p) process. In the technical part of the paper, we
derive asymptotic theory for the proposed multiscale test and the estimator of
the long-run error variance. The theory is complemented by a simulation study
and an empirical application to climate data.

Key words: Multiscale statistics; long-run variance; nonparametric regression; time
series errors; shape constraints; strong approximations; anti-concentration bounds.
AMS 2010 subject classiﬁcations: 62E20; 62G10; 62G20; 62M10.

1

Introduction

The analysis of time trends is an important aspect of many time series applications.
In a wide range of situations, practitioners are particularly interested in certain shape
properties of the trend. They raise questions such as the following: Does the observed
time series have a trend at all? If so, is the trend increasing/decreasing in certain time
regions? Can one identify the regions of increase/decrease? As an example, consider
the time series plotted in Figure 1 which shows the yearly mean temperature in Central
England from 1659 to 2017. Climatologists are very much interested in learning about

1Address: Bonn Graduate School of Economics, University of Bonn, 53113 Bonn, Germany. Email:
marina.k@uni-bonn.de.
2Address: Department of Economics and Hausdorﬀ Center for Mathematics, University of Bonn, 53113
Bonn, Germany. Email: michael.vogt@uni-bonn.de.

1

 
 
 
 
 
 
Figure 1: Yearly mean temperature in Central England from 1659 to 2017 measured in ◦C.

the trending behaviour of temperature time series like this; see e.g. Benner (1999) and
Rahmstorf et al. (2017). Among other things, they would like to know whether there
is an upward trend in the Central England mean temperature towards the end of the
sample as visual inspection might suggest.
In this paper, we develop new methods to test for certain shape properties of a nonpara-
metric time trend. We in particular construct a multiscale test which allows to identify
local increases/decreases of the trend function. We develop our test in the context of
the following model setting: We observe a time series {Yt,T : 1 ≤ t ≤ T } of the form

Yt,T = m

(cid:16) t
T

(cid:17)

+ εt

(1.1)

for 1 ≤ t ≤ T , where m : [0, 1] → R is an unknown nonparametric regression function
and the error terms εt form a stationary time series process with E[εt] = 0. In a time
series context, the design points t/T represent the time points of observation and m is
a nonparametric time trend. As usual in nonparametric regression, we let the function
m depend on rescaled time t/T rather than on real time t. A detailed description of
model (1.1) is provided in Section 2.
Our multiscale test is developed step by step in Section 3. Roughly speaking, the
procedure can be outlined as follows: Let H0(u, h) be the hypothesis that m is constant
in the time window [u − h, u + h] ⊆ [0, 1], where u is the midpoint and 2h the size
of the window.
In a ﬁrst step, we set up a test statistic (cid:98)sT (u, h) for the hypothesis
H0(u, h).
In a second step, we aggregate the statistics (cid:98)sT (u, h) for a large number
of diﬀerent time windows [u − h, u + h]. We thereby construct a multiscale statistic
which allows to test the hypothesis H0(u, h) simultaneously for many time windows
[u − h, u + h]. In the technical part of the paper, we derive the theoretical properties of
the resulting multiscale test. To do so, we come up with a proof strategy which combines
strong approximation results for dependent processes with anti-concentration bounds
for Gaussian random vectors. This strategy is of interest in itself and may be applied
to other multiscale test problems for dependent data. As shown by our theoretical
analysis, our multiscale test is a rigorous level-α-test of the overall null hypothesis

2

167517251775182518751925197520257891011H0 that H0(u, h) is simultaneously fulﬁlled for all time windows [u − h, u + h] under
consideration. Moreover, for a given signiﬁcance level α ∈ (0, 1), the test allows to
make simultaneous conﬁdence statements of the following form: We can claim, with
statistical conﬁdence 1 − α, that there is an increase/decrease in the trend m on all
time windows [u − h, u + h] for which the hypothesis H0(u, h) is rejected. Hence, the
test allows to identify, with a pre-speciﬁed statistical conﬁdence, time regions where
the trend m is increasing/decreasing.
For independent data, multiscale tests have been developed in a variety of diﬀerent
contexts in recent years. In the regression context, Chaudhuri and Marron (1999, 2000)
introduced the so-called SiZer method which has been extended in various directions; see
e.g. Hannig and Marron (2006) where a reﬁned distribution theory for SiZer is derived.
Hall and Heckman (2000) constructed a multiscale test on monotonicity of a regression
function. D¨umbgen and Spokoiny (2001) developed a multiscale approach which works
with additively corrected supremum statistics and derived theoretical results in the
context of a continuous Gaussian white noise model. Rank-based multiscale tests for
nonparametric regression were proposed in D¨umbgen (2002) and Rohde (2008). More
recently, Proksch et al. (2018) have constructed multiscale tests for inverse regression
models. In the context of density estimation, multiscale tests have been investigated
in D¨umbgen and Walther (2008), Ruﬁbach and Walther (2010), Schmidt-Hieber et al.
(2013) and Eckle et al. (2017) among others.
Whereas a large number of multiscale tests for independent data have been developed
in recent years, multiscale tests for dependent data are much rarer. Most notably,
there are some extensions of the SiZer approach to a time series context. Park et al.
(2004) and Rondonotti et al. (2007) have introduced SiZer methods for dependent data
which can be used to ﬁnd local increases/decreases of a trend and which may thus be
regarded as an alternative to our multiscale test. However, these SiZer methods are
mainly designed for data exploration rather than for rigorous statistical inference. Our
multiscale method, in contrast, is a rigorous level-α-test of the hypothesis H0 which
allows to make simultaneous conﬁdence statements about the time regions where the
trend m is increasing/decreasing. Some theoretical results for dependent SiZer methods
have been derived in Park et al. (2009), but only under a quite severe restriction: Only
time windows [u − h, u + h] with window sizes or scales h are taken into account that
remain bounded away from zero as the sample size T grows. Scales h that converge to
zero as T increases are excluded. This eﬀectively means that only large time windows
[u − h, u + h] are taken into consideration. Our theory, in contrast, allows to simul-
taneously consider scales h of ﬁxed size and scales h that converge to zero at various
diﬀerent rates. We are thus able to take into account time windows of many diﬀerent
sizes.
Our multiscale approach is also related to Wavelet-based methods: Similar to the latter,
it takes into account diﬀerent locations u and resolution levels or scales h simultaneously.

3

However, while our multiscale approach is designed to test for local increases/decreases
of a nonparametric trend, Wavelet methods are commonly used for other purposes.
Among other things, they are employed for estimating/reconstructing nonparametric
regression curves [see e.g. Donoho et al. (1995) or Von Sachs and MacGibbon (2000)]
and for change point detection [see e.g. Cho and Fryzlewicz (2012)].
The test statistic of our multiscale method depends on the long-run error variance
σ2 = (cid:80)∞
(cid:96)=−∞ Cov(ε0, ε(cid:96)), which is usually unknown in practice. To carry out our
multiscale test, we thus require an estimator of σ2. Indeed, such an estimator is required
for virtually all inferential procedures in the context of model (1.1). Hence, the problem
of estimating σ2 in model (1.1) is of broader interest and has received a lot of attention in
the literature; see M¨uller and Stadtm¨uller (1988), Herrmann et al. (1992) and Hall and
Van Keilegom (2003) among many others. In Section 4, we discuss several estimators
of σ2 which are valid under diﬀerent conditions on the error process {εt}. Most notably,
we introduce a new diﬀerence-based estimator of σ2 for the case that {εt} is an AR(p)
process. This estimator improves on existing methods in several respects.
The methodological and theoretical analysis of the paper is complemented by a simu-
lation study in Section 5 and an empirical application in Section 6. In the simulation
study, we examine the ﬁnite sample properties of our multiscale test and compare it to
the dependent SiZer methods introduced in Park et al. (2004) and Rondonotti et al.
(2007). Moreover, we investigate the small sample performance of our estimator of σ2
in the AR(p) case and compare it to the estimator of Hall and Van Keilegom (2003).
In Section 6, we use our methods to analyse the temperature data from Figure 1.

2 The model

We now describe the model setting in detail which was brieﬂy outlined in the Intro-
duction. We observe a time series {Yt,T : 1 ≤ t ≤ T } of length T which satisﬁes the
nonparametric regression equation

Yt,T = m

(cid:16) t
T

(cid:17)

+ εt

(2.1)

for 1 ≤ t ≤ T . Here, m is an unknown nonparametric function deﬁned on [0, 1] and
{εt : 1 ≤ t ≤ T } is a zero-mean stationary error process. For simplicity, we restrict
attention to equidistant design points xt = t/T . However, our methods and theory can
also be carried over to non-equidistant designs. The stationary error process {εt} is
assumed to have the following properties:

(C1) The variables εt allow for the representation εt = G(. . . , ηt−1, ηt, ηt+1, . . .), where

ηt are i.i.d. random variables and G : RZ → R is a measurable function.

(C2) It holds that (cid:107)εt(cid:107)q < ∞ for some q > 4, where (cid:107)εt(cid:107)q = (E|εt|q)1/q.

4

Following Wu (2005), we impose conditions on the dependence structure of the error
process {εt} in terms of the physical dependence measure dt,q = (cid:107)εt − ε(cid:48)
t(cid:107)q, where
t = G(. . . , η−1, η(cid:48)
ε(cid:48)
t} being an i.i.d. copy of {ηt}. In
particular, we assume the following:

0, η1, . . . , ηt−1, ηt, ηt+1, . . .) with {η(cid:48)

(C3) Deﬁne Θt,q = (cid:80)

|s|≥t ds,q for t ≥ 0. It holds that Θt,q = O(t−τq (log t)−A), where

A > 2

3(1/q + 1 + τq) and τq = {q2 − 4 + (q − 2)(cid:112)q2 + 20q + 4}/8q.

The conditions (C1)–(C3) are fulﬁlled by a wide range of stationary processes {εt}. As
a ﬁrst example, consider linear processes of the form εt = (cid:80)∞
i=0 ciηt−i with (cid:107)εt(cid:107)q < ∞,
where ci are absolutely summable coeﬃcients and ηt are i.i.d. innovations with E[ηt] = 0
and (cid:107)ηt(cid:107)q < ∞. Trivially, (C1) and (C2) are fulﬁlled in this case. Moreover, if |ci| =
O(ρi) for some ρ ∈ (0, 1), then (C3) is easily seen to be satisﬁed as well. As a special
case, consider an ARMA process {εt} of the form εt − (cid:80)p
j=1 bjηt−j
with (cid:107)εt(cid:107)q < ∞, where a1, . . . , ap and b1, . . . , br are real-valued parameters. As before,
we let ηt be i.i.d. innovations with E[ηt] = 0 and (cid:107)ηt(cid:107)q < ∞. Moreover, as usual, we
suppose that the complex polynomials A(z) = 1 − (cid:80)p
j=1 bjzj
do not have any roots in common.
If A(z) does not have any roots inside the unit
disc, then the ARMA process {εt} is stationary and causal. Speciﬁcally, it has the
representation εt = (cid:80)∞
i=0 ciηt−i with |ci| = O(ρi) for some ρ ∈ (0, 1), implying that
(C1)–(C3) are fulﬁlled. The results in Wu and Shao (2004) show that condition (C3)
(as well as the other two conditions) is not only fulﬁlled for linear time series processes
but also for a variety of non-linear processes.

j=1 ajzj and B(z) = 1 + (cid:80)r

i=1 aiεt−i = ηt + (cid:80)r

3 The multiscale test

In this section, we introduce our multiscale method to test for local increases/decreases
of the trend function m and analyse its theoretical properties. We assume throughout
that m is continuously diﬀerentiable on [0, 1]. The test problem under consideration
can be formulated as follows: Let H0(u, h) be the hypothesis that m is constant on
the interval [u − h, u + h]. Since m is continuously diﬀerentiable, H0(u, h) can be
reformulated as

H0(u, h) : m(cid:48)(w) = 0 for all w ∈ [u − h, u + h],

where m(cid:48) is the ﬁrst derivative of m. We want to test the hypothesis H0(u, h) not only
for a single interval [u − h, u + h] but simultaneously for many diﬀerent intervals. The
overall null hypothesis is thus given by

H0 : The hypothesis H0(u, h) holds true for all (u, h) ∈ GT ,

5

where GT is some large set of points (u, h). The details on the set GT are discussed at
the end of Section 3.1 below. Note that GT in general depends on the sample size T ,
implying that the null hypothesis H0 = H0,T depends on T as well. We thus consider
a sequence of null hypotheses {H0,T : T = 1, 2, . . .} as T increases. For simplicity of
notation, we however suppress the dependence of H0 on T . In Sections 3.1 and 3.2,
we step by step construct the multiscale test of the hypothesis H0. The theoretical
properties of the test are analysed in Section 3.3.

3.1 Construction of the multiscale statistic

We ﬁrst construct a test statistic for the hypothesis H0(u, h), where [u − h, u + h] is a
given interval. To do so, we consider the kernel average

(cid:98)ψT (u, h) =

T
(cid:88)

t=1

wt,T (u, h)Yt,T ,

where wt,T (u, h) is a kernel weight and h is the bandwidth. In order to avoid boundary
issues, we work with a local linear weighting scheme. We in particular set

wt,T (u, h) =

Λt,T (u, h)

{(cid:80)T

t=1 Λt,T (u, h)2}1/2

,

(3.1)

where

Λt,T (u, h) = K

(cid:17)(cid:104)

(cid:16) t

T − u
h

ST,0(u, h)

(cid:16) t

T − u
h

(cid:17)

(cid:105)
,
− ST,1(u, h)

ST,(cid:96)(u, h) = (T h)−1 (cid:80)T
the following properties:

t=1 K(

t
T −u
h )(

t
T −u
h )(cid:96) for (cid:96) = 0, 1, 2 and K is a kernel function with

(C4) The kernel K is non-negative, symmetric about zero and integrates to one. More-
over, it has compact support [−1, 1] and is Lipschitz continuous, that is, |K(v) −
K(w)| ≤ C|v − w| for any v, w ∈ R and some constant C > 0.

The kernel average (cid:98)ψT (u, h) is nothing else than a rescaled local linear estimator of the
derivative m(cid:48)(u) with bandwidth h.3
A test statistic for the hypothesis H0(u, h) is given by the normalized kernel average
(cid:98)ψT (u, h)/(cid:98)σ, where (cid:98)σ2 is an estimator of the long-run variance σ2 = (cid:80)∞
(cid:96)=−∞ Cov(ε0, ε(cid:96))
of the error process {εt}. The problem of estimating σ2 is discussed in detail in Section
4. For the time being, we suppose that (cid:98)σ2 is an estimator with reasonable theoretical
properties. Speciﬁcally, we assume that (cid:98)σ2 = σ2 + op(ρT ) with ρT = o(1/ log T ). This is
3Alternatively to the local linear weights deﬁned in (3.1), we could also work with the weights
wt,T (u, h) = K (cid:48)(h−1[u − t/T ])/{(cid:80)T
t=1 K (cid:48)(h−1[u − t/T ])2}1/2, where the kernel function K is as-
sumed to be diﬀerentiable and K (cid:48) is its derivative. We however prefer to use local linear weights as
these have superior theoretical properties at the boundary.

6

a fairly weak condition which is in particular satisﬁed by the estimators of σ2 analysed in
Section 4. The kernel weights wt,T (u, h) are chosen such that in the case of independent
errors εt, Var( (cid:98)ψT (u, h)) = σ2 for any location u and bandwidth h, where the long-run
error variance σ2 simpliﬁes to σ2 = Var(εt). In the more general case that the error
terms satisfy the weak dependence conditions from Section 2, Var( (cid:98)ψT (u, h)) = σ2 + o(1)
for any u and h under consideration. Hence, for suﬃciently large sample sizes T , the
test statistic (cid:98)ψT (u, h)/(cid:98)σ has approximately unit variance.
We now combine the test statistics (cid:98)ψT (u, h)/(cid:98)σ for a wide range of diﬀerent locations
u and bandwidths or scales h. There are diﬀerent ways to do so, leading to diﬀerent
types of multiscale statistics. Our multiscale statistic is deﬁned as

(cid:98)ΨT = max
(u,h)∈GT

(cid:110)(cid:12)
(cid:12)
(cid:12)

(cid:98)ψT (u, h)
(cid:98)σ

(cid:12)
(cid:111)
(cid:12)
,
(cid:12) − λ(h)

(3.2)

where λ(h) = (cid:112)2 log{1/(2h)} and GT is the set of points (u, h) that are taken into
consideration. The details on the set GT are given below. As can be seen, the statis-
tic (cid:98)ΨT does not simply aggregate the individual statistics (cid:98)ψT (u, h)/(cid:98)σ by taking the
supremum over all points (u, h) ∈ GT as in more traditional multiscale approaches. We
rather calibrate the statistics (cid:98)ψT (u, h)/(cid:98)σ that correspond to the bandwidth h by sub-
tracting the additive correction term λ(h). This approach was pioneered by D¨umbgen
and Spokoiny (2001) and has been used in numerous other studies since then; see e.g.
D¨umbgen (2002), Rohde (2008), D¨umbgen and Walther (2008), Ruﬁbach and Walther
(2010), Schmidt-Hieber et al. (2013) and Eckle et al. (2017).
To see the heuristic idea behind the additive correction λ(h), consider for a moment
the uncorrected statistic

(cid:98)ΨT,uncorrected = max
(u,h)∈GT

(cid:12)
(cid:12)
(cid:12)

(cid:98)ψT (u, h)
(cid:98)σ

(cid:12)
(cid:12)
(cid:12)

and suppose that the hypothesis H0(u, h) is true for all (u, h) ∈ GT . For simplicity,
assume that the errors εt are i.i.d. normally distributed and neglect the estimation
error in (cid:98)σ, that is, set (cid:98)σ = σ. Moreover, suppose that the set GT only consists of the
points (uk, h(cid:96)) = ((2k − 1)h(cid:96), h(cid:96)) with k = 1, . . . , (cid:98)1/2h(cid:96)(cid:99) and (cid:96) = 1, . . . , L. In this case,
we can write

(cid:98)ΨT,uncorrected = max
1≤(cid:96)≤L

max
1≤k≤(cid:98)1/2h(cid:96)(cid:99)

(cid:12)
(cid:12)
(cid:12)

(cid:98)ψT (uk, h(cid:96))
σ

(cid:12)
(cid:12)
(cid:12).

Under our simplifying assumptions, the statistics (cid:98)ψT (uk, h(cid:96))/σ with k = 1, . . . , (cid:98)1/2h(cid:96)(cid:99)
are independent and standard normal for any given bandwidth h(cid:96). Since the maximum
over (cid:98)1/2h(cid:99) independent standard normal random variables is λ(h)+op(1) as h → 0, we
obtain that maxk (cid:98)ψT (uk, h(cid:96))/σ is approximately of size λ(h(cid:96)) for small bandwidths h(cid:96).
As λ(h) → ∞ for h → 0, this implies that maxk (cid:98)ψT (uk, h(cid:96))/σ tends to be much larger

7

in size for small than for large bandwidths h(cid:96). As a result, the stochastic behaviour of
the uncorrected statistic (cid:98)ΨT,uncorrected tends to be dominated by the statistics (cid:98)ψT (uk, h(cid:96))
corresponding to small bandwidths h(cid:96). The additively corrected statistic (cid:98)ΨT , in con-
trast, puts the statistics (cid:98)ψT (uk, h(cid:96)) corresponding to diﬀerent bandwidths h(cid:96) on a more
equal footing, thus counteracting the dominance of small bandwidth values.
The multiscale statistic (cid:98)ΨT simultaneously takes into account all locations u and band-
widths h with (u, h) ∈ GT . Throughout the paper, we suppose that GT is some subset
of Gfull
T = {(u, h) : u = t/T for some 1 ≤ t ≤ T and h ∈ [hmin, hmax]}, where hmin and
hmax denote some minimal and maximal bandwidth value, respectively. For our theory
to work, we require the following conditions to hold:

(C5) |GT | = O(T θ) for some arbitrarily large but ﬁxed constant θ > 0, where |GT |

denotes the cardinality of GT .

(C6) hmin (cid:29) T −(1− 2

q ) log T , that is, hmin/{T −(1− 2

q ) log T } → ∞ with q > 4 deﬁned in

(C2) and hmax < 1/2.

According to (C5), the number of points (u, h) in GT should not grow faster than T θ
for some arbitrarily large but ﬁxed θ > 0. This is a fairly weak restriction as it allows
the set GT to be extremely large compared to the sample size T . For example, we may
work with the set

GT = (cid:8)(u, h) : u = t/T for some 1 ≤ t ≤ T and h ∈ [hmin, hmax]

with h = t/T for some 1 ≤ t ≤ T (cid:9),

which contains more than enough points (u, h) for most practical applications. Condi-
tion (C6) imposes some restrictions on the minimal and maximal bandwidths hmin and
hmax. These conditions are fairly weak, allowing us to choose the bandwidth window
[hmin, hmax] extremely large. The lower bound on hmin depends on the parameter q
deﬁned in (C2) which speciﬁes the number of existing moments for the error terms εt.
As one can see, we can choose hmin to be of the order T −1/2 for any q > 4. Hence, we
can let hmin converge to 0 very quickly even if only the ﬁrst few moments of the error
terms εt exist. If all moments exist (i.e. q = ∞), hmin may converge to 0 almost as
quickly as T −1 log T . Furthermore, the maximal bandwidth hmax is not even required
to converge to 0, which implies that we can pick it very large.

Remark 3.1. The above construction of the multiscale statistic can be easily adapted
to hypotheses other than H0. To do so, one simply needs to replace the kernel weights
wt,T (u, h) deﬁned in (3.1) by appropriate versions which are suited to test the hypothesis
of interest. For example, if one wants to test for local convexity/concavity of m, one may
deﬁne the kernel weights wt,T (u, h) such that the kernel average (cid:98)ψT (u, h) is a (rescaled)
estimator of the second derivative of m at the location u with bandwidth h.

8

3.2 The test procedure

In order to formulate a test for the null hypothesis H0, we still need to specify a critical
value. To do so, we deﬁne the statistic

ΦT = max
(u,h)∈GT

(cid:110)(cid:12)
(cid:12)
(cid:12)

φT (u, h)
σ

(cid:12)
(cid:111)
(cid:12)
,
(cid:12) − λ(h)

(3.3)

where φT (u, h) = (cid:80)T
t=1 wt,T (u, h) σZt and Zt are independent standard normal random
variables. The statistic ΦT can be regarded as a Gaussian version of the test statistic (cid:98)ΨT
under the null hypothesis H0. Let qT (α) be the (1 − α)-quantile of ΦT . Importantly, the
quantile qT (α) can be computed by Monte Carlo simulations and can thus be regarded
as known. Our multiscale test of the hypothesis H0 is now deﬁned as follows: For a
given signiﬁcance level α ∈ (0, 1), we reject H0 if (cid:98)ΨT > qT (α).

3.3 Theoretical properties of the test

In order to examine the theoretical properties of our multiscale test, we introduce the
auxiliary multiscale statistic

(cid:98)ΦT = max
(u,h)∈GT

(cid:110)(cid:12)
(cid:12)
(cid:12)

(cid:98)φT (u, h)
(cid:98)σ

(cid:12)
(cid:12)
(cid:12) − λ(h)

(cid:111)

(3.4)

with (cid:98)φT (u, h) = (cid:98)ψT (u, h) − E[ (cid:98)ψT (u, h)] = (cid:80)T
t=1 wt,T (u, h)εt. The following result is
central to the theoretical analysis of our multiscale test. According to it, the (known)
quantile qT (α) of the Gaussian statistic ΦT deﬁned in Section 3.2 can be used as a proxy
for the (1 − α)-quantile of the multiscale statistic (cid:98)ΦT .

Theorem 3.1. Let (C1)–(C6) be fulﬁlled and assume that (cid:98)σ2 = σ2 + op(ρT ) with
ρT = o(1/ log T ). Then

P(cid:0)

(cid:98)ΦT ≤ qT (α)(cid:1) = (1 − α) + o(1).

A full proof of Theorem 3.1 is given in the Supplementary Material. We here shortly
outline the proof strategy, which splits up into two main steps. In the ﬁrst, we replace
the statistic (cid:98)ΦT for each T ≥ 1 by a statistic (cid:101)ΦT with the same distribution as (cid:98)ΦT and
the property that

(cid:12)
(cid:12)(cid:101)ΦT − ΦT

(cid:12)
(cid:12) = op(δT ),

(3.5)

where δT = o(1) and the Gaussian statistic ΦT is deﬁned in Section 3.2. We thus replace
the statistic (cid:98)ΦT by an identically distributed version which is close to a Gaussian statistic
whose distribution is known. To do so, we make use of strong approximation theory
for dependent processes as derived in Berkes et al. (2014). In the second step, we show

9

that

(cid:12)P((cid:101)ΦT ≤ x) − P(ΦT ≤ x)(cid:12)
(cid:12)

(cid:12) = o(1),

sup
x∈R

(3.6)

which immediately implies the statement of Theorem 3.1. Importantly, the convergence
result (3.5) is not suﬃcient for establishing (3.6). Put diﬀerently, the fact that (cid:101)ΦT can
be approximated by ΦT in the sense that (cid:101)ΦT − ΦT = op(δT ) does not imply that the
distribution of (cid:101)ΦT is close to that of ΦT in the sense of (3.6). For (3.6) to hold, we
additionally require the distribution of ΦT to have some sort of continuity property.
Speciﬁcally, we prove that

P(cid:0)|ΦT − x| ≤ δT

(cid:1) = o(1),

sup
x∈R

(3.7)

which says that ΦT does not concentrate too strongly in small regions of the form
[x − δT , x + δT ]. The main tool for verifying (3.7) are anti-concentration results for
Gaussian random vectors as derived in Chernozhukov et al. (2015). The claim (3.6) can
be proven by using (3.5) together with (3.7), which in turn yields Theorem 3.1.
The main idea of our proof strategy is to combine strong approximation theory with
anti-concentration bounds for Gaussian random vectors to show that the quantiles of the
multiscale statistic (cid:98)ΦT can be proxied by those of a Gaussian analogue. This strategy is
quite general in nature and may be applied to other multiscale problems for dependent
data. Strong approximation theory has also been used to investigate multiscale tests
for independent data; see e.g. Schmidt-Hieber et al. (2013). However, it has not been
combined with anti-concentration results to approximate the quantiles of the multiscale
statistic. As an alternative to strong approximation theory, Eckle et al. (2017) and
Proksch et al. (2018) have recently used Gaussian approximation results derived in
Chernozhukov et al. (2014, 2017) to analyse multiscale tests for independent data.
Even though it might be possible to adapt these techniques to the case of dependent
data, this is not trivial at all as part of the technical arguments and the Gaussian
approximation tools strongly rely on the assumption of independence.
We now investigate the theoretical properties of our multiscale test with the help of
Theorem 3.1. The ﬁrst result is an immediate consequence of Theorem 3.1. It says that
the test has the correct (asymptotic) size.

Proposition 3.1. Let the conditions of Theorem 3.1 be satisﬁed. Under the null hy-
pothesis H0, it holds that

P(cid:0)

(cid:98)ΨT ≤ qT (α)(cid:1) = (1 − α) + o(1).

The second result characterizes the power of the multiscale test against local alterna-
tives. To formulate it, we consider any sequence of functions m = mT with the following

10

property: There exists (u, h) ∈ GT with [u − h, u + h] ⊆ [0, 1] such that

m(cid:48)

T (w) ≥ cT

(cid:114)

log T
T h3

for all w ∈ [u − h, u + h],

(3.8)

where {cT } is any sequence of positive numbers with cT → ∞. Alternatively to (3.8), we
(cid:112)log T /(T h3) for all w ∈ [u − h, u + h]. According
may also assume that −m(cid:48)
to the following result, our test has asymptotic power 1 against local alternatives of the
form (3.8).

T (w) ≥ cT

Proposition 3.2. Let the conditions of Theorem 3.1 be satisﬁed and consider any
sequence of functions mT with the property (3.8). Then

P(cid:0)

(cid:98)ΨT ≤ qT (α)(cid:1) = o(1).

The proof of Proposition 3.2 can be found in the Supplementary Material. To formulate
the next result, we deﬁne

T = (cid:8)Iu,h = [u − h, u + h] : (u, h) ∈ A±
Π±
T = (cid:8)Iu,h = [u − h, u + h] : (u, h) ∈ A+
Π+
T = (cid:8)Iu,h = [u − h, u + h] : (u, h) ∈ A−
Π−

(cid:9)
T and Iu,h ⊆ [0, 1](cid:9)
T and Iu,h ⊆ [0, 1](cid:9)

T

together with

(cid:110)

(cid:110)

(cid:110)

A+

A±

T =

T =

(u, h) ∈ GT :

(u, h) ∈ GT :

> qT (α) + λ(h)

(cid:12)
(cid:111)
(cid:12)
(cid:12) > qT (α) + λ(h)
(cid:111)

(cid:12)
(cid:98)ψT (u, h)
(cid:12)
(cid:12)
(cid:98)σ
(cid:98)ψT (u, h)
(cid:98)σ
(cid:98)ψT (u, h)
(cid:98)σ
Π±
T is the collection of intervals Iu,h = [u−h, u+h] for which the (corrected) test statistic
| (cid:98)ψT (u, h)/(cid:98)σ| − λ(h) lies above the critical value qT (α), that is, for which our multiscale
T and Π−
test rejects the hypothesis H0(u, h). Π+
T can be interpreted analogously but
take into account the sign of the statistic (cid:98)ψT (u, h)/(cid:98)σ. With this notation at hand, we
consider the events

> qT (α) + λ(h)

(u, h) ∈ GT : −

T =

A−

(cid:111)

.

(cid:110)

(cid:110)

(cid:110)

E±

T =

E+

T =

E−

T =

∀Iu,h ∈ Π±

∀Iu,h ∈ Π+

∀Iu,h ∈ Π−

(cid:111)
T : m(cid:48)(v) (cid:54)= 0 for some v ∈ Iu,h = [u − h, u + h]
(cid:111)
T : m(cid:48)(v) > 0 for some v ∈ Iu,h = [u − h, u + h]
(cid:111)
T : m(cid:48)(v) < 0 for some v ∈ Iu,h = [u − h, u + h]
.

E±

T (E+

T , E−

T ) is the event that the function m is non-constant (increasing, decreasing)

11

T , Π−
T , Π−

T (Π+
T (Π+

T ). More precisely, E±
on all intervals Iu,h ∈ Π±
T ) is the event that for
each interval Iu,h ∈ Π±
T ), there is a subset Ju,h ⊆ Iu,h with m being a non-
constant (increasing, decreasing) function on Ju,h. We can make the following formal
statement about the events E±
T , whose proof is given in the Supplementary
Material.

T and E−

T (E+

T , E−

T , E+

Proposition 3.3. Let the conditions of Theorem 3.1 be fulﬁlled. Then for (cid:96) ∈ {±, +, −},
it holds that

P(cid:0)E(cid:96)

T

(cid:1) ≥ (1 − α) + o(1).

According to Proposition 3.3, we can make simultaneous conﬁdence statements of the
following form: With (asymptotic) probability ≥ (1 − α), the trend function m is non-
T (Π+
constant (increasing, decreasing) on some part of the interval Iu,h for all Iu,h ∈ Π±
T ,
Π−
T ). Hence, our multiscale procedure allows to identify, with a pre-speciﬁed conﬁdence,
time regions where there is an increase/decrease in the time trend m.

T and Π−

Remark 3.2. Unlike Π±
T , the sets Π+
T only contain intervals Iu,h = [u−h, u+h]
which are subsets of [0, 1]. We thus exclude points (u, h) ∈ A+
T which
lie at the boundary, that is, for which Iu,h (cid:42) [0, 1]. The reason is as follows: Let
T with Iu,h (cid:42) [0, 1]. Our technical arguments allow us to say, with asymptotic
(u, h) ∈ A+
conﬁdence ≥ 1 − α, that m(cid:48)(v) (cid:54)= 0 for some v ∈ Iu,h. However, we cannot say whether
m(cid:48)(v) > 0 or m(cid:48)(v) < 0, that is, we cannot make conﬁdence statements about the sign.
Crudely speaking, the problem is that the local linear weights wt,T (u, h) behave quite
diﬀerently at boundary points (u, h) with Iu,h (cid:42) [0, 1]. As a consequence, we can include
boundary points (u, h) in Π±

T and (u, h) ∈ A−

T but not in Π+

T and Π−
T .

The statement of Proposition 3.3 suggests to graphically present the results of our
multiscale test by plotting the intervals Iu,h ∈ Π(cid:96)
T for (cid:96) ∈ {±, +, −}, that is, by plotting
the intervals where (with asymptotic conﬁdence ≥ 1 − α) our test detects a violation
of the null hypothesis. The drawback of this graphical presentation is that the number
of intervals in Π(cid:96)
T is often quite large. To obtain a better graphical summary of the
results, we replace Π(cid:96)
which is constructed as follows: As in D¨umbgen
T
(2002), we call an interval Iu,h ∈ Π(cid:96)
T minimal if there is no other interval Iu(cid:48),h(cid:48) ∈ Π(cid:96)
T
with Iu(cid:48),h(cid:48) ⊂ Iu,h. Let Π(cid:96),min
T for (cid:96) ∈ {±, +, −}
and deﬁne the events

be the set of all minimal intervals in Π(cid:96)

T by a subset Π(cid:96),min

T

E±,min
T

=

E+,min
T

=

E−,min
T

=

(cid:110)

∀Iu,h ∈ Π±,min

T

(cid:110)

∀Iu,h ∈ Π+,min

T

(cid:110)

∀Iu,h ∈ Π−,min

T

It is easily seen that E(cid:96)

T = E(cid:96),min

T

: m(cid:48)(v) (cid:54)= 0 for some v ∈ Iu,h = [u − h, u + h]

: m(cid:48)(v) > 0 for some v ∈ Iu,h = [u − h, u + h]

: m(cid:48)(v) < 0 for some v ∈ Iu,h = [u − h, u + h]

(cid:111)

(cid:111)

(cid:111)
.

for (cid:96) ∈ {±, +, −}. Hence, by Proposition 3.3, it holds

12

that

P(cid:0)E(cid:96),min

T

(cid:1) ≥ (1 − α) + o(1)

for (cid:96) ∈ {±, +, −}. This suggests to plot the minimal intervals in Π(cid:96),min
the whole collection of intervals Π(cid:96)
particular use this way of presenting the test results in our application in Section 6.

rather than
T as a graphical summary of the test results. We in

T

4 Estimation of the long-run error variance

In this section, we discuss how to estimate the long-run variance σ2 = (cid:80)∞
(cid:96)=−∞ Cov(ε0, ε(cid:96))
of the error terms in model (2.1). There are two broad classes of estimators: residual-
and diﬀerence-based estimators. In residual-based approaches, σ2 is estimated from the
residuals (cid:98)εt = Yt,T − (cid:98)mh(t/T ), where (cid:98)mh is a nonparametric estimator of m with the
bandwidth or smoothing parameter h. Diﬀerence-based methods proceed by estimating
σ2 from the (cid:96)-th diﬀerences Yt,T −Yt−(cid:96),T of the observed time series {Yt,T } for certain or-
ders (cid:96). In what follows, we focus attention on diﬀerence-based methods as these do not
involve a nonparametric estimator of the function m and thus do not require to specify
a bandwidth h for the estimation of m. To simplify notation, we let ∆(cid:96)Zt = Zt − Zt−(cid:96)
denote the (cid:96)-th diﬀerences of a general time series {Zt} throughout the section.

4.1 Weakly dependent error processes

We ﬁrst consider the case that {εt} is a general stationary error process. We do not
impose any time series model such as a moving average (MA) or an autoregressive (AR)
model on {εt} but only require that {εt} satisﬁes certain weak dependence conditions
such as those from Section 2. These conditions imply that the autocovariances γε((cid:96)) =
Cov(ε0, ε(cid:96)) decay to zero at a certain rate as |(cid:96)| → ∞. For simplicity of exposition,
we assume that the decay is exponential, that is, |γε((cid:96))| ≤ Cρ|(cid:96)| for some C > 0 and
0 < ρ < 1. In addition to these weak dependence conditions, we suppose that the trend
m is smooth. Speciﬁcally, we assume m to be Lipschitz continuous on [0, 1], that is,
|m(u) − m(v)| ≤ C|u − v| for all u, v ∈ [0, 1] and some constant C < ∞.
Under these conditions, a diﬀerence-based estimator of σ2 can be obtained as follows: To
start with, we construct an estimator of the short-run error variance γε(0) = Var(ε0).
As m is Lipschitz continuous, it holds that ∆qYt,T = ∆qεt + O(q/T ). Hence, the
diﬀerences ∆qYt,T of the observed time series are close to the diﬀerences ∆qεt of the
unobserved error process as long as q is not too large in comparison to T . Moreover,
since |γε(q)| ≤ Cρq, we have that E[(∆qεt)2]/2 = γε(0) − γε(q) = γε(0) + O(ρq). Taken
together, these considerations yield that γε(0) = E[(∆qYt,T )2]/2+O({q/T }2 +ρq), which

13

motivates to estimate γε(0) by

(cid:98)γε(0) =

1
2(T − q)

T
(cid:88)

(∆qYt,T )2,

t=q+1

(4.1)

where we assume that q = qT → ∞ with qT / log T → ∞ and qT /
T → 0. Estimators
of the autocovariances γε((cid:96)) for (cid:96) (cid:54)= 0 can be derived by similar considerations. Since
γε((cid:96)) = γε(0) − E[(∆(cid:96)εt)2]/2 = γε(0) − E[(∆(cid:96)Yt,T )2]/2 + O({(cid:96)/T }2), we may in particular
deﬁne

(cid:98)γε((cid:96)) = (cid:98)γε(0) −

1
2(T − |(cid:96)|)

T
(cid:88)

t=|(cid:96)|+1

(∆|(cid:96)|Yt,T )2

(4.2)

√

for any (cid:96) (cid:54)= 0. Diﬀerence-based estimators of the type (4.1) and (4.2) have been used
in diﬀerent contexts in the literature before. Estimators similar to (4.1) and (4.2) were
analysed, for example, in M¨uller and Stadtm¨uller (1988) and Hall and Van Keilegom
(2003) in the context of m-dependent and autoregressive error terms, respectively. In
order to estimate the long-run error variance σ2, we may employ HAC-type estima-
tion procedures as discussed in Andrews (1991) or De Jong and Davidson (2000). In
particular, an estimator of σ2 may be deﬁned as

(cid:98)σ2 =

(cid:88)

|(cid:96)|≤bT

W

(cid:17)

(cid:16) (cid:96)
bT

(cid:98)γε((cid:96)),

(4.3)

where W : [−1, 1] → R is a kernel (e.g. of Bartlett or Parzen type) and bT is a bandwidth
parameter with bT → ∞ and bT /qT → 0. The additional bandwidth bT comes into play
because estimating σ2 under general weak dependence conditions is a nonparametric
problem.
In particular, it is equivalent to estimating the (nonparametric) spectral
density fε of the process {εt} at frequency 0 (assuming that fε exists).
Estimating the long-run error variance σ2 under general weak dependence conditions
is a notoriously diﬃcult problem. Estimators of σ2 such as (cid:98)σ2 from (4.3) tend to be
quite imprecise and are usually very sensitive to the choice of the smoothing parameter,
that is, to bT in the case of (cid:98)σ2 from (4.3). To circumvent this issue in practice, it may
be beneﬁcial to impose a time series model on the error process {εt}. Estimating
σ2 under the restrictions of such a model may of course create some misspeciﬁcation
bias. However, as long as the model gives a reasonable approximation to the true error
process, the produced estimates of σ2 can be expected to be fairly reliable even though
they are a bit biased. Which time series model is appropriate of course depends on
the application at hand. In the sequel, we follow authors such as Hart (1994) and Hall
and Van Keilegom (2003) and impose an autoregressive structure on the error terms
{εt}, which is a very popular error model in many application contexts. We thus do
not dwell on the nonparametric estimator (cid:98)σ2 from (4.3) any further but rather give an

14

in-depth analysis of the case of autoregressive error terms.

4.2 Autoregressive error processes

Estimators of the long-run error variance σ2 in model (2.1) have been developed for
diﬀerent kinds of error processes {εt}. A number of authors have analysed the case
of MA(m) or, more generally, m-dependent error terms. Diﬀerence-based estimators
of σ2 for this case were proposed in M¨uller and Stadtm¨uller (1988), Herrmann et al.
(1992) and Tecuapetla-G´omez and Munk (2017) among others. Under the assumption
of m-dependence, γε((cid:96)) = 0 for all |(cid:96)| > m. Even though m-dependent time series are
a reasonable error model in some applications, the condition that γε((cid:96)) is exactly equal
to 0 for suﬃciently large lags (cid:96) is quite restrictive in many situations. Presumably the
most widely used error model in practice is an AR(p) process. Residual-based methods
to estimate σ2 in model (2.1) with AR(p) errors can be found for example in Truong
(1991), Shao and Yang (2011) and Qiu et al. (2013). A diﬀerence-based method was
proposed in Hall and Van Keilegom (2003).
In what follows, we introduce a diﬀerence-based estimator of σ2 for the AR(p) case which
improves on existing methods in several respects. As in Hall and Van Keilegom (2003),
we consider the following situation: {εt} is a stationary and causal AR(p) process of
the form

p
(cid:88)

εt =

ajεt−j + ηt,

(4.4)

j=1

where a1, . . . , ap are unknown parameters and ηt are i.i.d. innovations with E[ηt] = 0
and E[η2
t ] = ν2. The AR order p is known and m is Lipschitz continuous on [0, 1], that
is, |m(u) − m(v)| ≤ C|u − v| for all u, v ∈ [0, 1] and some constant C < ∞. Since {εt}
is causal, the variables εt have an MA(∞) representation of the form εt = (cid:80)∞
k=0 ckηt−k.
The coeﬃcients ck can be computed iteratively from the equations

ck −

p
(cid:88)

j=1

ajck−j = bk

(4.5)

for k = 0, 1, 2, . . ., where b0 = 1, bk = 0 for k > 0 and ck = 0 for k < 0. Moreover, the
coeﬃcients ck can be shown to decay exponentially fast to zero as k → ∞, in particular,
|ck| ≤ Cρk with some C > 0 and 0 < ρ < 1.
Our estimation method relies on the following simple observation: If {εt} is an AR(p)
process of the form (4.4), then the time series {∆qεt} of the diﬀerences ∆qεt = εt − εt−q
is an ARMA(p, q) process of the form

∆qεt −

p
(cid:88)

j=1

aj∆qεt−j = ηt − ηt−q.

(4.6)

15

As m is Lipschitz, the diﬀerences ∆qεt of the unobserved error process are close to the
diﬀerences ∆qYt,T of the observed time series in the sense that

∆qYt,T = (cid:2)εt − εt−q

(cid:3) +

(cid:17)

(cid:104)
m

(cid:16) t
T

− m

(cid:16) t − q
T

(cid:17)(cid:105)

= ∆qεt + O

(cid:17)

.

(cid:16) q
T

(4.7)

Taken together, (4.6) and (4.7) imply that the diﬀerenced time series {∆qYt,T } is ap-
proximately an ARMA(p, q) process of the form (4.6). It is precisely this point which
is exploited by our estimation methods.
We ﬁrst construct an estimator of the parameter vector a = (a1, . . . , ap)(cid:62). For any
q ≥ 1, the ARMA(p, q) process {∆qεt} satisﬁes the Yule-Walker equations

γq((cid:96)) −

γq((cid:96)) −

p
(cid:88)

j=1
p
(cid:88)

j=1

ajγq((cid:96) − j) = −ν2cq−(cid:96)

for 1 ≤ (cid:96) < q + 1

(4.8)

ajγq((cid:96) − j) = 0

for (cid:96) ≥ q + 1,

(4.9)

where γq((cid:96)) = Cov(∆qεt, ∆qεt−(cid:96)) and ck are the coeﬃcients from the MA(∞) expansion
of {εt}. From (4.8) and (4.9), we get that

Γqa = γq + ν2cq,

(4.10)

where cq = (cq−1, . . . , cq−p)(cid:62), γq = (γq(1), . . . , γq(p))(cid:62) and Γq denotes the p × p covari-
ance matrix Γq = (γq(i − j) : 1 ≤ i, j ≤ p). Since the coeﬃcients ck decay exponentially
fast to zero, cq ≈ 0 and thus Γqa ≈ γq for large values of q. This suggests to estimate
a by

(cid:101)aq = (cid:98)Γ

−1

q (cid:98)γq,

(4.11)

√

T → 0.

where (cid:98)Γq and (cid:98)γq are deﬁned analogously as Γq and γq with γq((cid:96)) replaced by the
sample autocovariances (cid:98)γq((cid:96)) = (T − q)−1 (cid:80)T
t=q+(cid:96)+1 ∆qYt,T ∆qYt−(cid:96),T and q = qT goes to
inﬁnity suﬃciently fast as T → ∞, speciﬁcally, q = qT → ∞ with qT / log T → ∞ and
qT /
The estimator (cid:101)aq depends on the tuning parameter q, which is very similar in nature
to the two tuning parameters of the methods in Hall and Van Keilegom (2003). An
appropriate choice of q needs to take care of the following two points: (i) q should be
chosen large enough to ensure that the vector cq = (cq−1, . . . , cq−p)(cid:62) is close to zero.
As we have already seen, the constants ck decay exponentially fast to zero and can be
computed from the recursive equations (4.5) for given AR parameters a1, . . . , ap. In
the AR(1) case, for example, one can readily calculate that ck ≤ 0.0035 for any k ≥ 20
and any |a1| ≤ 0.75. Hence, if we have an AR(1) model for the errors εt and the error
process is not too persistent, choosing q such that q ≥ 20 should make sure that cq is
close to zero. Generally speaking, the recursive equations (4.5) can be used to get some

16

idea for which values of q the vector cq can be expected to be approximately zero. (ii)
q should not be chosen too large in order to ensure that the trend m is appropriately
eliminated by taking q-th diﬀerences. As long as the trend m is not very strong, the
two requirements (i) and (ii) can be fulﬁlled without much diﬃculty. For example, by
choosing q = 20 in the AR(1) case just discussed, we do not only take care of (i) but
also make sure that moderate trends m are diﬀerenced out appropriately.
When the trend m is very pronounced, in contrast, even moderate values of q may
be too large to eliminate the trend appropriately. As a result, the estimator (cid:101)aq will
have a strong bias. In order to reduce this bias, we reﬁne our estimation procedure as
follows: By solving the recursive equations (4.5) with a replaced by (cid:101)aq, we can compute
estimators (cid:101)ck of the coeﬃcients ck and thus estimators (cid:101)cr of the vectors cr for any r ≥ 1.
Moreover, the innovation variance ν2 can be estimated by (cid:101)ν2 = (2T )−1 (cid:80)T
t,T ,
where (cid:101)rt,T = ∆1Yt,T −(cid:80)p
j=1 (cid:101)aj∆1Yt−j,T and (cid:101)aj is the j-th entry of the vector (cid:101)aq. Plugging
the expressions (cid:98)Γr, (cid:98)γr, (cid:101)cr and (cid:101)ν2 into (4.10), we can estimate a by

t=p+1 (cid:101)r2

−1

(cid:98)ar = (cid:98)Γ

r ((cid:98)γr + (cid:101)ν2

(cid:101)cr),

(4.12)

where r is any ﬁxed number with r ≥ 1. In particular, unlike q, the parameter r does
not diverge to inﬁnity but remains ﬁxed as the sample size T increases. As one can see,
the estimator (cid:98)ar is based on diﬀerences of some small order r; only the pilot estimator
(cid:101)aq relies on diﬀerences of a larger order q. As a consequence, (cid:98)ar should eliminate the
trend m more appropriately and should thus be less biased than the pilot estimator (cid:101)aq.
In order to make the method more robust against estimation errors in (cid:101)cr, we ﬁnally
average the estimators (cid:98)ar for a few small values of r. In particular, we deﬁne

(cid:98)a =

1
r

r
(cid:88)

r=1

(cid:98)ar,

(4.13)

where r is a small natural number. For ease of notation, we suppress the dependence
of (cid:98)a on the parameter r. Once (cid:98)a = ((cid:98)a1, . . . , (cid:98)ap)(cid:62) is computed, the long-run variance σ2
can be estimated by

(cid:98)σ2 =

(cid:98)ν2
j=1 (cid:98)aj)2 ,
(1 − (cid:80)p
t,T with (cid:98)rt,T = ∆1Yt,T − (cid:80)p

t=p+1 (cid:98)r2

where (cid:98)ν2 = (2T )−1 (cid:80)T
the innovation variance ν2 and we make use of the fact that σ2 = ν2/(1 − (cid:80)p
the AR(p) process {εt}.
We brieﬂy compare the estimator (cid:98)a to competing methods. Presumably closest to our
approach is the procedure of Hall and Van Keilegom (2003). Nevertheless, the two
approaches diﬀer in several respects. The two main advantages of our method are as
follows:

j=1 (cid:98)aj∆1Yt−j,T is an estimator of
j=1 aj)2 for

(4.14)

17

(a) Our estimator produces accurate estimation results even when the AR process {εt}
is quite persistent, that is, even when the AR polynomial A(z) = 1 − (cid:80)p
j=1 ajzj
has a root close to the unit circle. The estimator of Hall and Van Keilegom (2003),
in contrast, may have very high variance and may thus produce unreliable results
when the AR polynomial A(z) is close to having a unit root. This diﬀerence in
behaviour can be explained as follows: Our pilot estimator (cid:101)aq = ((cid:101)a1, . . . , (cid:101)ap)(cid:62) has
the property that the estimated AR polynomial (cid:101)A(z) = 1 − (cid:80)p
j=1 (cid:101)ajzj has no root
inside the unit disc, that is, (cid:101)A(z) (cid:54)= 0 for all complex numbers z with |z| ≤ 1.4
Hence, the ﬁtted AR model with the coeﬃcients (cid:101)aq is ensured to be stationary
and causal. Even though this may seem to be a minor technical detail, it has a
huge eﬀect on the performance of the estimator: It keeps the estimator stable even
when the AR process is very persistent and the AR polynomial A(z) has almost
a unit root. This in turn results in a reliable behaviour of the estimator (cid:98)a in
the case of high persistence. The estimator of Hall and Van Keilegom (2003), in
contrast, may produce non-causal results when the AR polynomial A(z) is close to
having a unit root. As a consequence, it may have unnecessarily high variance in
the case of high persistence. We illustrate this diﬀerence between the estimators
by the simulation exercises in Section 5.3. A striking example is Figure 5, which
presents the simulation results for the case of an AR(1) process εt = a1εt−1 + ηt
with a1 = −0.95 and clearly shows the much better performance of our method.

(b) Both our pilot estimator (cid:101)aq and the estimator of Hall and Van Keilegom (2003)
tend to have a substantial bias when the trend m is pronounced. Our estimator
(cid:98)a reduces this bias considerably as demonstrated in the simulations of Section 5.3.
Unlike the estimator of Hall and Van Keilegom (2003), it thus produces accurate
results even in the presence of a very strong trend.

We now derive some basic asymptotic properties of the estimators (cid:101)aq, (cid:98)a and (cid:98)σ2. The
√
following proposition shows that they are

T -consistent.

Proposition 4.1. Let {εt} be a causal AR(p) process of the form (4.4). Suppose that
the innovations ηt have a ﬁnite fourth moment and let m be Lipschitz continuous. If
T → 0, then (cid:101)aq − a = Op(T −1/2) as well as
q → ∞ with q/ log T → ∞ and q/
(cid:98)a − a = Op(T −1/2) and (cid:98)σ2 − σ2 = Op(T −1/2).

√

It can also be shown that (cid:101)aq, (cid:98)a and (cid:98)σ2 are asymptotically normal. In general, their
asymptotic variance is somewhat larger than that of the estimators in Hall and Van Kei-
legom (2003). They are thus a bit less eﬃcient in terms of asymptotic variance. How-
ever, this theoretical loss of eﬃciency is more than compensated by the advantages

4More precisely, (cid:101)A(z) (cid:54)= 0 for all z with |z| ≤ 1, whenever the covariance matrix ((cid:98)γq(i − j) : 1 ≤ i, j ≤
p + 1) is non-singular. Moreover, ((cid:98)γq(i − j) : 1 ≤ i, j ≤ p + 1) is non-singular whenever (cid:98)γq(0) > 0,
which is the generic case.

18

discussed in (a) and (b) above, which lead to a substantially better small sample per-
formance as demonstrated in the simulations of Section 5.3.

5 Simulations

To assess the ﬁnite sample performance of our methods, we conduct a number of sim-
ulations. In Sections 5.1 and 5.2, we investigate the performance of our multiscale test
and compare it to the SiZer methods for time series developed in Park et al. (2004),
Rondonotti et al. (2007) and Park et al. (2009). In Section 5.3, we analyse the ﬁnite
sample properties of our long-run variance estimator from Section 4.2 and compare it
to the estimator of Hall and Van Keilegom (2003).

5.1 Size and power properties of the multiscale test

Our simulation design mimics the situation in the application example of Section 6.
We generate data from the model Yt,T = m(t/T ) + εt for diﬀerent trend functions
m, error processes {εt} and time series lengths T . The error terms are supposed to
have the AR(1) structure εt = a1εt−1 + ηt, where a1 ∈ {−0.5, −0.25, 0.25, 0.5} and
In addition, we consider the AR(2) speciﬁcation εt =
ηt are i.i.d. standard normal.
a1εt−1 + a2εt−2 + ηt, where ηt are normally distributed with E[ηt] = 0 and E[η2
t ] = ν2.
We set a1 = 0.167, a2 = 0.178 and ν2 = 0.322, thus matching the estimated values
obtained in the application of Section 6. To simulate data under the null hypothesis,
we let m be a constant function. In particular, we set m = 0 without loss of generality.
To generate data under the alternative, we consider the trend functions m(u) = β(u −
0.5) · 1(0.5 ≤ u ≤ 1) with β = 1.5, 2.0, 2.5. These functions are broken lines with a kink
at u = 0.5 and diﬀerent slopes β. Their shape roughly resembles the trend estimates
in the application of Section 6. The slope parameter β corresponds to a trend with the
value m(1) = 0.5β at the right endpoint u = 1. We thus consider broken lines with the
values m(1) = 0.75, 1.0, 1.25. Inspecting the middle panel of Figure 7, the broken lines
with the endpoints m(1) = 1.0 and m(1) = 1.25 (that is, with β = 2.0 and β = 2.5)
can be seen to resemble the local linear trend estimates in the real-data example the
most (where we neglect the nonlinearities of the local linear ﬁts at the beginning of
the observation period). The broken line with β = 1.5 is closer to the null, making it
harder for our test to detect this alternative.5

5The broken lines m are obviously non-diﬀerentiable at the kink point. We could replace them by
slightly smoothed versions to satisfy the diﬀerentiability assumption that is imposed in the theoretical
part of the paper. However, as this leaves the simulation results essentially unchanged but only creates
additional notation, we stick to the broken lines.

19

.

α

s
e
z
i
s

l
a
n
i
m
o
n

d
n
a

T

s
e
z
i
s

e
l
p
m
a
s

,
2
a

d
n
a

1
a

s
r
e
t
e
m
a
r
a
p
R
A
t
n
e
r
e
ﬀ
i
d

r
o
f

t
s
e
t

e
l
a
c
s
i
t
l
u
m

r
u
o

f
o

e
z
i
S

:
1

e
l
b
a
T

)
8
7
1
.
0
,
7
6
1
.
0
(
=
)
2
a
,
1
a
(

5
.
0
=
1
a

5
2
.
0
=
1
a

α

e
z
i
s

l
a
n
i
m
o
n

α

e
z
i
s

l
a
n
i
m
o
n

α

e
z
i
s

i

l
a
n
m
o
n

5
2
.
0
−
=
1
a

α

e
z
i
s

i

l
a
n
m
o
n

5
.
0
−
=
1
a

α

e
z
i
s

l
a
n
i
m
o
n

1
.
0

5
0
.
0

1
0
.
0

1
.
0

5
0
.
0

1
0
.
0

1
.
0

5
0
.
0

1
0
.
0

1
.
0

5
0
.
0

1
0
.
0

1
.
0

5
0
.
0

1
0
.
0

7
1
1
.
0

4
1
1
.
0

7
0
1
.
0

2
5
0
.
0

9
5
0
.
0

6
5
0
.
0

1
1
0
.
0

0
1
0
.
0

5
1
0
.
0

8
0
1
.
0

0
9
0
.
0

6
0
1
.
0

2
4
0
.
0

9
4
0
.
0

2
4
0
.
0

3
1
0
.
0

0
1
0
.
0

5
1
0
.
0

6
1
1
.
0

6
9
0
.
0

1
0
1
.
0

6
4
0
.
0

5
5
0
.
0

8
4
0
.
0

1
1
0
.
0

9
0
0
.
0

8
1
0
.
0

0
2
1
.
0

5
9
0
.
0

0
0
1
.
0

7
5
0
.
0

5
5
0
.
0

7
4
0
.
0

4
1
0
.
0

0
1
0
.
0

5
1
0
.
0

7
2
1
.
0

0
2
1
.
0

8
2
1
.
0

0
5
0
.
0

7
6
0
.
0

3
5
0
.
0

5
1
0
.
0

9
0
0
.
0

5
1
0
.
0

0
5
2
=

0
5
3
=

0
0
5
=

T

T

T

)
c
(
–
)
a
(

s
l
e
n
a
p

e
e
r
h
t

e
h
T

.

α

s
e
z
i
s

l
a
n
i
m
o
n

d
n
a

T

s
e
z
i
s

e
l
p
m
a
s

,
2
a

d
n
a

1
a

s
r
e
t
e
m
a
r
a
p

R
A

t
n
e
r
e
ﬀ
i
d

r
o
f

t
s
e
t

e
l
a
c
s
i
t
l
u
m

r
u
o

f
o

r
e
w
o
P

:
2

e
l

b
a
T

.

m
e
n
i
l

n
e
k
o
r
b

e
h
t

f
o

β

s
r
e
t
e
m
a
r
a
p

e
p
o
l
s

t
n
e
r
e
ﬀ
i
d

o
t

s
d
n
o
p
s
e
r
r
o
c

5
.
1
=
β

)
a
(

)
8
7
1
.
0
,
7
6
1
.
0
(
=
)
2
a
,
1
a
(

5
.
0
=
1
a

5
2
.
0
=
1
a

α

e
z
i
s

l
a
n
i
m
o
n

α

e
z
i
s

l
a
n
i
m
o
n

α

e
z
i
s

i

l
a
n
m
o
n

5
2
.
0
−
=
1
a

α

e
z
i
s

i

l
a
n
m
o
n

5
.
0
−
=
1
a

α

e
z
i
s

l
a
n
i
m
o
n

1
.
0

5
0
.
0

1
0
.
0

1
.
0

5
0
.
0

1
0
.
0

1
.
0

5
0
.
0

1
0
.
0

1
.
0

5
0
.
0

1
0
.
0

1
.
0

5
0
.
0

1
0
.
0

2
1
6
.
0

0
7
7
.
0

7
0
9
.
0

0
6
4
.
0

4
5
6
.
0

5
1
8
.
0

9
6
2
.
0

0
9
3
.
0

3
2
6
.
0

1
8
1
.
0

1
2
2
.
0

5
8
2
.
0

7
9
0
.
0

1
4
1
.
0

2
6
1
.
0

6
3
0
.
0

0
5
0
.
0

0
6
0
.
0

4
2
3
.
0

5
8
3
.
0

1
5
5
.
0

7
7
1
.
0

3
7
2
.
0

9
8
3
.
0

7
7
0
.
0

6
1
1
.
0

5
9
1
.
0

2
0
7
.
0

4
3
8
.
0

2
7
9
.
0

8
4
5
.
0

3
5
7
.
0

5
2
9
.
0

9
1
3
.
0

3
6
4
.
0

5
7
7
.
0

3
5
8
.
0

5
5
9
.
0

7
9
9
.
0

6
2
7
.
0

3
1
9
.
0

8
8
9
.
0

4
8
4
.
0

5
3
7
.
0

5
4
9
.
0

0
5
2
=

0
5
3
=

0
0
5
=

T

T

T

20

0
.
2
=
β

)
b
(

)
8
7
1
.
0
,
7
6
1
.
0
(
=
)
2
a
,
1
a
(

5
.
0
=
1
a

5
2
.
0
=
1
a

α

e
z
i
s

l
a
n
i
m
o
n

α

e
z
i
s

l
a
n
i
m
o
n

α

e
z
i
s

i

l
a
n
m
o
n

5
2
.
0
−
=
1
a

α

e
z
i
s

i

l
a
n
m
o
n

5
.
0
−
=
1
a

α

e
z
i
s

l
a
n
i
m
o
n

1
.
0

5
0
.
0

1
0
.
0

1
.
0

5
0
.
0

1
0
.
0

1
.
0

5
0
.
0

1
0
.
0

1
.
0

5
0
.
0

1
0
.
0

1
.
0

5
0
.
0

1
0
.
0

1
5
8
.
0

8
5
9
.
0

4
9
9
.
0

4
2
7
.
0

2
2
9
.
0

3
8
9
.
0

9
4
5
.
0

9
5
7
.
0

3
3
9
.
0

9
5
2
.
0

4
3
3
.
0

1
5
4
.
0

3
4
1
.
0

1
3
2
.
0

9
0
3
.
0

2
6
0
.
0

2
9
0
.
0

7
3
1
.
0

0
2
5
.
0

5
1
6
.
0

1
2
8
.
0

0
4
3
.
0

3
8
4
.
0

6
1
7
.
0

4
6
1
.
0

2
6
2
.
0

9
6
4
.
0

6
1
9
.
0

6
8
9
.
0

9
9
9
.
0

6
4
8
.
0

9
6
9
.
0

7
9
9
.
0

3
6
6
.
0

3
6
8
.
0

3
8
9
.
0

5
8
9
.
0

0
0
0
.
1

0
0
0
.
1

1
6
9
.
0

7
9
9
.
0

0
0
0
.
1

9
6
8
.
0

9
7
9
.
0

0
0
0
.
1

0
5
2
=

0
5
3
=

0
0
5
=

T

T

T

5
.
2
=
β

)
c
(

)
8
7
1
.
0
,
7
6
1
.
0
(
=
)
2
a
,
1
a
(

5
.
0
=
1
a

5
2
.
0
=
1
a

α

e
z
i
s

l
a
n
i
m
o
n

α

e
z
i
s

l
a
n
i
m
o
n

α

e
z
i
s

i

l
a
n
m
o
n

5
2
.
0
−
=
1
a

α

e
z
i
s

i

l
a
n
m
o
n

5
.
0
−
=
1
a

α

e
z
i
s

l
a
n
i
m
o
n

1
.
0

5
0
.
0

1
0
.
0

1
.
0

5
0
.
0

1
0
.
0

1
.
0

5
0
.
0

1
0
.
0

1
.
0

5
0
.
0

1
0
.
0

1
.
0

5
0
.
0

1
0
.
0

8
5
9
.
0

7
9
9
.
0

0
0
0
.
1

8
1
9
.
0

8
8
9
.
0

9
9
9
.
0

4
0
8
.
0

0
5
9
.
0

4
9
9
.
0

7
6
3
.
0

1
8
4
.
0

9
4
6
.
0

4
2
2
.
0

1
6
3
.
0

3
7
4
.
0

0
0
1
.
0

2
6
1
.
0

5
8
2
.
0

3
0
7
.
0

3
3
8
.
0

8
6
9
.
0

3
4
5
.
0

7
3
7
.
0

9
1
9
.
0

2
2
3
.
0

0
7
4
.
0

3
7
7
.
0

3
9
9
.
0

0
0
0
.
1

0
0
0
.
1

1
7
9
.
0

0
0
0
.
1

0
0
0
.
1

1
0
9
.
0

0
9
9
.
0

9
9
9
.
0

0
0
0
.
1

0
0
0
.
1

0
0
0
.
1

0
0
0
.
1

0
0
0
.
1

0
0
0
.
1

9
8
9
.
0

0
0
0
.
1

0
0
0
.
1

0
5
2
=

0
5
3
=

0
0
5
=

T

T

T

To implement our test, we choose K to be an Epanechnikov kernel and deﬁne the set
GT of location-scale points (u, h) as

GT = (cid:8)(u, h) : u = 5k/T for some 1 ≤ k ≤ T /5 and

h = (3 + 5(cid:96))/T for some 0 ≤ (cid:96) ≤ T /20(cid:9).

(5.1)

We thus take into account all rescaled time points u ∈ [0, 1] on an equidistant grid
with step length 5/T . For the bandwidth h = (3 + 5(cid:96))/T and any u ∈ [h, 1 − h],
the kernel weights K(h−1{t/T − u}) are non-zero for exactly 5 + 10(cid:96) observations.
Hence, the bandwidths h in GT correspond to eﬀective sample sizes of 5, 15, 25, . . . up to
approximately T /4 data points. As a robustness check, we have re-run the simulations
for a number of other grids. As the results are very similar, we do however not report
them here. The long-run error variance σ2 is estimated by the procedures from Section
4.2: We ﬁrst compute the estimator (cid:98)a of the AR parameter(s), where we use r = 10
and the pilot estimator (cid:101)aq with q = 25. Based on (cid:98)a, we then compute the estimator
(cid:98)σ2 of the long-run error variance σ2. As a further robustness check, we have re-run
the simulations for other choices of the parameters q and r, which yields very similar
results. The dependence of the estimators (cid:98)a and (cid:98)σ2 on q and r is further explored in
Section 5.3. To compute the critical values of the multiscale test, we simulate 1000
values of the statistic ΦT deﬁned in Section 3.2 and compute their empirical (1 − α)
quantile qT (α).
Tables 1 and 2 report the simulation results for the sample sizes T = 250, 350, 500 and
the signiﬁcance levels α = 0.01, 0.05, 0.10. The sample size T = 350 is approximately
equal to the time series length 359 in the real-data example of Section 6. To produce our
simulation results, we generate S = 1000 samples for each model speciﬁcation and carry
out the multiscale test for each sample. The entries of Tables 1 and 2 are computed
as the number of simulations in which the test rejects divided by the total number of
simulations. As can be seen from Table 1, the actual size of the test is fairly close to
the nominal target α for all the considered AR speciﬁcations and sample sizes. Hence,
the test has approximately the correct size. Inspecting Table 2, one can further see that
the test has reasonable power properties. For all the considered AR speciﬁcations, the
power increases quickly (i) as the sample size gets larger and (ii) as we move away from
the null by increasing the slope parameter β. The power is of course quite diﬀerent
across the various AR speciﬁcations. In particular, it is much lower for positive than
for negative values of a1 in the AR(1) case, the lowest power numbers being obtained
for the largest positive value a1 = 0.5 under consideration. This reﬂects the fact that it
is more diﬃcult to detect a trend when there is strong positive autocorrelation in the
data. For the AR(2) speciﬁcation of the errors, the sample size T = 350 and the slopes
β = 2.0 and β = 2.5, which yield the two model speciﬁcations that resemble the real-
life data in Section 6 the most, the power of the test is above 92% for the signiﬁcance

21

levels α = 0.05 and α = 0.1 and above 75% for α = 0.01. Hence, our method has
substantial power in the two simulation scenarios which are closest to the situation in
the application.

5.2 Comparison with SiZer

T

T

T and ΠSiZer

We now compare our multiscale test to SiZer for times series which was developed in
Park et al. (2004), Rondonotti et al. (2007) and Park et al. (2009). Roughly speak-
ing, the SiZer method proceeds as follows: For each location u and bandwidth h in a
h(u) of the derivative m(cid:48)(u) and a cor-
pre-speciﬁed set, SiZer computes an estimator (cid:98)m(cid:48)
responding conﬁdence interval. For each (u, h), it then checks whether the conﬁdence
interval includes the value 0. The set ΠSiZer
of points (u, h) for which the conﬁdence
interval does not include 0 corresponds to the set of intervals Π±
T for which our multi-
scale test ﬁnds an increase/decrease in the trend m. In order to explore how our test
performs in comparison to SiZer, we compare the two sets Π±
in diﬀerent
ways to each other in what follows.
In order to implement SiZer for time series, we follow the exposition in Park et al.
(2009).6 The details are given in Section S.3 in the Supplementary Material. To simplify
the implementation of SiZer, we assume that the autocovariance function γε(·) of the
error process and thus the long-run error variance σ2 is known. Our multiscale test is
implemented in the same way as in Section 5.1. To keep the comparison fair, we treat
σ2 as known also when implementing our method. Moreover, we use the same grid GT of
points (u, h) for both methods. To achieve this, we start oﬀ with the grid GT from (5.1).
We then follow Rondonotti et al. (2007) and Park et al. (2009) and restrict attention
to those points (u, h) ∈ GT for which the eﬀective sample size ESS∗(u, h) for correlated
T = {(u, h) ∈ GT : ESS∗(u, h) ≥ 5}.
data is not smaller than 5. This yields the grid G∗
A detailed discussion of the eﬀective sample size ESS∗(u, h) for correlated data can be
found in Rondonotti et al. (2007).
In the ﬁrst part of the comparison study, we analyse the size and power of the two
methods. To do so, we treat SiZer as a rigorous statistical test of the null hypothesis
H0 that m is constant on all intervals [u − h, u + h] with (u, h) ∈ G∗
T . In particular,
we let SiZer reject the null if the set ΠSiZer
is non-empty, that is, if the value 0 is not
included in the conﬁdence interval for at least one point (u, h) ∈ G∗
T . We simulate data
from the model Yt,T = m(t/T ) + εt with diﬀerent AR(1) error processes and diﬀerent
trends m. In particular, we let {εt} be an AR(1) process of the form εt = a1εt−1 + ηt
with a1 ∈ {−0.25, 0.25} and i.i.d. standard normal innovations ηt. To simulate data
under the null, we set m = 0 as in the previous section. To generate data under the

T

6We have also examined the somewhat diﬀerent implementation from Rondonotti et al. (2007). As
this yields worse simulation results than the procedure from Park et al. (2009), we however do not
report them here.

22

Table 3: Size of our multiscale test (MT) and SiZer for diﬀerent model speciﬁcations.

a1 = −0.25

a1 = 0.25

α = 0.05
α = 0.01
MT SiZer MT SiZer MT SiZer

α = 0.1

α = 0.05
α = 0.01
MT SiZer MT SiZer MT SiZer

α = 0.1

T = 250
T = 350
T = 500

0.018 0.112 0.040 0.374 0.104 0.575
0.012 0.140 0.058 0.426 0.080 0.621
0.005 0.140 0.041 0.489 0.097 0.680

0.017 0.106 0.034 0.347 0.092 0.522
0.012 0.130 0.046 0.399 0.074 0.578
0.006 0.136 0.039 0.452 0.097 0.639

Table 4: Power of our multiscale test (MT) and SiZer for diﬀerent model speciﬁcations. The
three panels (a)–(c) corresponds to diﬀerent slope parameters β of the linear tend m.

(a) β = 1.0 for negative a1 and β = 2.0 for positive a1

a1 = −0.25

a1 = 0.25

α = 0.05
α = 0.01
MT SiZer MT SiZer MT SiZer

α = 0.1

α = 0.01
α = 0.05
MT SiZer MT SiZer MT SiZer

α = 0.1

T = 250
T = 350
T = 500

0.218 0.544 0.454 0.869 0.664 0.949
0.385 0.707 0.665 0.958 0.753 0.986
0.581 0.899 0.862 0.993 0.949 0.999

0.359 0.717 0.653 0.947 0.829 0.989
0.599 0.888 0.864 0.995 0.913 0.998
0.851 0.981 0.983 1.000 0.999 1.000

(b) β = 1.25 for negative a1 and β = 2.25 for positive a1

a1 = −0.25

a1 = 0.25

α = 0.05
α = 0.01
MT SiZer MT SiZer MT SiZer

α = 0.1

α = 0.01
α = 0.05
MT SiZer MT SiZer MT SiZer

α = 0.1

T = 250
T = 350
T = 500

0.426 0.771 0.705 0.969 0.878 0.996
0.645 0.912 0.882 0.993 0.954 1.000
0.915 0.994 0.993 1.000 0.998 1.000

0.537 0.861 0.791 0.987 0.932 0.999
0.773 0.955 0.948 0.999 0.985 1.000
0.962 0.999 1.000 1.000 0.999 1.000

(c) β = 1.5 for negative a1 and β = 2.5 for positive a1

a1 = −0.25

a1 = 0.25

α = 0.05
α = 0.01
MT SiZer MT SiZer MT SiZer

α = 0.1

α = 0.05
α = 0.01
MT SiZer MT SiZer MT SiZer

α = 0.1

T = 250
T = 350
T = 500

0.701 0.942 0.911 0.992 0.972 1.000
0.895 0.994 0.981 1.000 0.996 1.000
0.995 1.000 1.000 1.000 1.000 1.000

0.698 0.941 0.908 0.993 0.970 1.000
0.893 0.993 0.980 1.000 0.996 1.000
0.995 1.000 1.000 1.000 1.000 1.000

alternative, we consider the linear trends m(u) = β(u − 0.5) with diﬀerent slopes β. As
it is more diﬃcult to detect a trend m in the data when the error terms are positively
autocorrelated, we choose the slopes β larger in the AR(1) case with a1 = 0.25 than in
the case with a1 = −0.25. In particular, we let β ∈ {1.0, 1.25, 1.5} when a1 = −0.25 and
β ∈ {2.0, 2.25, 2.5} when a1 = 0.25. Further model speciﬁcations with nonlinear trends
are considered in the second part of the comparison study. To produce our simulation
results, we generate S = 1000 samples for each model speciﬁcation and carry out the
two methods for each sample.

23

The simulation results are reported in Tables 3 and 4. Both for our multiscale test and
SiZer, the entries in the tables are computed as the number of simulations in which
the respective method rejects the null hypothesis H0 divided by the total number of
simulations. As can be seen from Table 3, our test has approximately correct size in
all of the considered settings, whereas SiZer is very liberal and rejects the null way
too often. Examining Table 4, one can further see that our procedure has reasonable
power against the considered alternatives. The power numbers are of course higher for
SiZer, which is a trivial consequence of the fact that SiZer is extremely liberal. These
numbers should thus be treated with caution. All in all, the simulations suggest that
SiZer can hardly be regarded as a rigorous statistical test of the null hypothesis H0 that
m is constant on all intervals [u − h, u + h] with (u, h) ∈ G∗
T . This is not very surprising
as SiZer is not designed to be such a test but to produce informative SiZer maps. In
particular, the conﬁdence intervals of SiZer are not constructed to control the level α
under H0. In what follows, we thus attempt to compare the two methods in a diﬀerent
way which goes beyond mere size and power comparisons.
Both our method and SiZer can be regarded as statistical tools to identify time regions
where the curve m is increasing/decreasing.7 Suppose that m is increasing/decreasing
in the time region R ⊂ [0, 1] but constant otherwise, that is, m(cid:48)(u) (cid:54)= 0 for all u ∈ R
and m(cid:48)(u) = 0 for all u /∈ R. A natural question is the following: How well can the
two methods identify the time region R? In our framework, information on the region
R is contained in the minimal intervals of the set Π±
T of
the minimal intervals in Π±
T can be regarded as an estimate of R. This follows from
the results in Propositions 3.2 and 3.3. Let RSiZer
be the union of the minimal intervals
in ΠSiZer
to the region R. This gives
T
us information on how well the two methods approximate the true region where m is
increasing/decreasing.8
We consider the same simulation setup as in the ﬁrst part of the comparison study,
only the trend function m is diﬀerent. We let m be deﬁned as m(u) = 2 · 1(u ∈
[0.4, 0.6]) · (1 − 100{u − 0.5}2)2, which implies that R = (0.4, 0.5) ∪ (0.5, 0.6). The
function m is plotted in the two upper panels of Figure 2. We set the signiﬁcance level to
α = 0.05 and the sample size to T = 500. For each AR parameter a1 ∈ {−0.25, 0.25}, we
simulate S = 100 samples and compute R±
for each sample. The simulation
results are depicted in Figure 2, the two subﬁgures (a) and (b) corresponding to diﬀerent
AR parameters. The upper panel of each subﬁgure displays the time series path of a
representative simulation together with the trend function m. The middle panel shows
the regions R±
T produced by our multiscale approach for the 100 simulation runs: On

T . In particular, the union R±

In what follows, we compare R±

T and RSiZer

T and RSiZer

T

T

T

.

7More precisely speaking, SiZer is usually interpreted as investigating the curve m, viewed at diﬀerent
levels of resolution, rather than the curve m itself. Put diﬀerently, the underlying object of interest
is a family of smoothed versions of m rather than m itself.
8The same exercise could of course also be carried out separately for the time region where the trend
m increases and the region where it decreases.

24

(a) a1 = −0.25

(b) a1 = 0.25

Figure 2: Comparison of the regions R±
T and RSiZer
. Subﬁgure (a) corresponds to the model
setting with the AR parameter a1 = −0.25, subﬁgure (b) to the setting with a1 = 0.25. The
upper panel of each subﬁgure shows a simulated time series path together with the underlying
trend function m. The middle panel depicts the regions R±
T produced by our multiscale test
for 100 simulation runs. The lower panel presents the regions RSiZer

produced by SiZer.

T

T

T

in an analogous way.

the y-axis, the simulation runs i are enumerated for 1 ≤ i ≤ 100, and the black line at
y-level i represents R±
T for the i-th simulation. Finally, the lower panel of each subﬁgure
depicts the regions RSiZer
Inspecting Figure 2, our multiscale method can be seen to approximate the region R
fairly well in both simulation scenarios under consideration. In particular, R±
T gives
a good approximation to the region R for most simulations. Only in some simulation
runs, R±
T is too large compared to R, which means that our method is not able to
locate the region R suﬃciently precisely. Overall, the SiZer method also produces quite
satisfactory results. However, the SiZer estimates of R are not as precise as ours. In
particular, SiZer spuriously ﬁnds regions of decrease/increase outside the interval R
much more often than our method. It thus frequently mistakes ﬂuctuations in the time
series which are due to the dependence in the error terms for increases/decreases in the
trend m.
To sum up, our multiscale test exhibits good size and power properties in the simu-
lations, and the minimal intervals produced by it identify the time regions where m
increases/decreases in a quite reliable way. SiZer performs clearly worse in these re-
spects. Nevertheless, it may still produce informative SiZer plots. All in all, we would
like to regard the two methods as complementary rather than direct competitors. SiZer

25

0.00.20.40.60.81.0−2024grid_points0.00.20.40.60.81.0020406080100Our testIndex0.00.20.40.60.81.0020406080100SiZer0.00.20.40.60.81.0−2024grid_points0.00.20.40.60.81.0020406080100Our testIndex0.00.20.40.60.81.0020406080100SiZeris an explorative tool which aims to give an overview of the increases/decreases in m
by means of a SiZer plot. Our method, in contrast, is tailored to be a rigorous sta-
tistical test of the hypothesis H0. In particular, it allows to make rigorous conﬁdence
statements about the time regions where the trend m increases/decreases.

5.3 Small sample properties of the long-run variance estimator

In the ﬁnal part of the simulation study, we examine the estimators of the AR para-
meters and the long-run error variance from Section 4.2. We simulate data from the
model Yt,T = m(t/T )+εt, where {εt} is an AR(1) process of the form εt = a1εt−1+ηt. We
consider the AR parameters a1 ∈ {−0.95, −0.75, −0.5, −0.25, 0.25, 0.5, 0.75, 0.95} and
let ηt be i.i.d. standard normal innovation terms. We report our ﬁndings for a speciﬁc
sample size T , in particular for T = 500, as the results for other sample sizes are very
similar. For simplicity, m is chosen to be a linear function of the form m(u) = βu
with the slope parameter β. For each value of a1, we consider two diﬀerent slopes β,
one corresponding to a moderate and one to a pronounced trend m.
In particular,
(cid:112)Var(εt) with sβ ∈ {1, 10}. When sβ = 1, the slope β is equal to the
we let β = sβ
standard deviation (cid:112)Var(εt) of the error process, which yields a moderate trend m.
When sβ = 10, in contrast, the slope β is 10 times as large as (cid:112)Var(εt), which results
in a quite pronounced trend m.
For each model speciﬁcation, we generate S = 1000 data samples and compute the
following quantities for each simulated sample:

(i) the pilot estimator (cid:101)aq from (4.11) with the tuning parameter q.

(ii) the estimator (cid:98)a from (4.13) with the tuning parameter r as well as the long-run

variance estimator (cid:98)σ2 from (4.14).

(iii) the estimators of a1 and σ2 from Hall and Van Keilegom (2003), which are de-
noted by (cid:98)aHvK and (cid:98)σ2
HvK for ease of reference. The estimator (cid:98)aHvK is computed
as described in Section 2.2 of Hall and Van Keilegom (2003) and (cid:98)σ2
HvK as deﬁned
at the bottom of p.447 in Section 2.3. The estimator (cid:98)aHvK (as well as (cid:98)σ2
HvK) de-
pends on two tuning parameters which we denote by m1 and m2 as in Hall and
Van Keilegom (2003).

(iv) oracle estimators (cid:98)aoracle and (cid:98)σ2

oracle of a1 and σ2, which are constructed under the
assumption that the error process {εt} is observed. For each simulation run, we
compute (cid:98)aoracle as the maximum likelihood estimator of a1 from the time series
of simulated error terms ε1, . . . , εT . We then calculate the residuals rt = εt −
(cid:98)aoracle εt−1 and estimate the innovation variance ν2 = E[η2
oracle = (T −
1)−1 (cid:80)T

t ] by (cid:98)ν2

oracle = (cid:98)ν2

oracle/(1 − (cid:98)aoracle)2.

t . Finally, we set (cid:98)σ2

t=2 r2

26

Figure 3: MSE values for the estimators (cid:98)a, (cid:98)aHvK, (cid:98)aoracle and (cid:98)σ2, (cid:98)σ2
scenarios with a moderate trend (sβ = 1).

HvK, (cid:98)σ2

oracle in the simulation

Figure 4: MSE values for the estimators (cid:98)a, (cid:98)aHvK, (cid:98)aoracle and (cid:98)σ2, (cid:98)σ2
scenarios with a pronounced trend (sβ = 10).

HvK, (cid:98)σ2

oracle in the simulation

Throughout the section, we set q = 25, r = 10 and (m1, m2) = (20, 30). We in
particular choose q to be in the middle of m1 and m2 to make the tuning parameters of
the estimators (cid:101)aq and (cid:98)aHvK more or less comparable. In order to assess how sensitive
our estimators are to the choice of q and r, we carry out a number of robustness checks,
considering a range of diﬀerent values for q and r.
In addition, we vary the tuning
parameters m1 and m2 of the estimators from Hall and Van Keilegom (2003) in order
to make sure that the results of our comparison study are not driven by the particular
choice of any of the involved tuning parameters. The results of our robustness checks
are reported in Section S.3 of the Supplementary Material. They show that the results
of our comparison study are robust to diﬀerent choices of the parameters q, r and
(m1, m2). Moreover, they indicate that our estimators are rather insensitive to the
choice of tuning parameters.
For each estimator (cid:98)a, (cid:98)aHvK, (cid:98)aoracle and (cid:98)σ2, (cid:98)σ2
oracle and for each model speciﬁcation,
the simulation output consists in a vector of length S = 1000 which contains the 1000
simulated values of the respective estimator. Figures 3 and 4 report the mean squared

HvK, (cid:98)σ2

27

0.0000.0020.0040.006a1MSEllllllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95aaHvKaoracle−50510a1log(MSE)llllllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95s2sHvK2soracle20.000.010.020.030.04a1MSEllllllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95aaHvKaoracle−50510a1log(MSE)llllllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95s2sHvK2soracle2Figure 5: Histograms of the simulated values produced by the estimators (cid:98)a, (cid:98)aHvK, (cid:98)aoracle and
(cid:98)σ2, (cid:98)σ2
oracle in the scenario with a1 = −0.95 and sβ = 1. The vertical red lines indicate
the true values of a1 and σ2.

HvK, (cid:98)σ2

error (MSE) of these 1000 simulated values for each estimator. On the x-axis of each
plot, the various values of the AR parameter a1 are listed which are considered. The
solid line in each plot gives the MSE values of our estimators. The dashed and dotted
lines specify the MSE values of the HvK and the oracle estimators, respectively. Note
that for the long-run variance estimators, the plots report the logarithm of the MSE
rather than the MSE itself since the MSE values are too diﬀerent across simulation
scenarios to obtain a reasonable graphical presentation. In addition to the MSE values
presented in Figures 3 and 4, we depict histograms of the 1000 simulated values pro-
duced by the estimators (cid:98)a, (cid:98)aHvK, (cid:98)aoracle and (cid:98)σ2, (cid:98)σ2
oracle for two speciﬁc simulation
scenarios in Figures 5 and 6. The main ﬁndings can be summarized as follows:

HvK, (cid:98)σ2

(a) In the simulation scenarios with a moderate trend (sβ = 1), the estimators (cid:98)aHvK
and (cid:98)σ2
HvK of Hall and Van Keilegom (2003) exhibit a similar performance as our
estimators (cid:98)a and (cid:98)σ2 as long as the AR parameter a1 is not too close to −1. For
strongly negative values of a1 (in particular for a1 = −0.75 and a1 = −0.95),
the estimators perform much worse than ours. This can be clearly seen from the
much larger MSE values of the estimators (cid:98)aHvK and (cid:98)σ2
HvK for a1 = −0.75 and
a1 = −0.95 in Figure 3. Figure 5 gives some further insights into what is happening

28

Frequency−1.4−1.3−1.2−1.1−1.0−0.9050100150200250300aFrequency−1.4−1.3−1.2−1.1−1.0−0.9050100150200250300aHvKFrequency−1.4−1.3−1.2−1.1−1.0−0.9050100150200250300aoracleFrequency0.20.40.60.81.00100200300400s2Frequency0.20.40.60.81.00100200300400sHvK2Frequency0.20.40.60.81.00100200300400soracle2Figure 6: Histograms of the simulated values produced by the estimators (cid:98)a, (cid:98)aHvK, (cid:98)aoracle and
(cid:98)σ2, (cid:98)σ2
oracle in the scenario with a1 = 0.25 and sβ = 10. The vertical red lines indicate
the true values of a1 and σ2.

HvK, (cid:98)σ2

here. It shows the histograms of the simulated values produced by the estimators
(cid:98)a, (cid:98)aHvK, (cid:98)aoracle and the corresponding long-run variance estimators in the scenario
with a1 = −0.95 and sβ = 1. As can be seen, the estimator (cid:98)aHvK does not obey the
causality restriction |a1| ≤ 1 but frequently takes values substantially smaller than
−1. This results in a very large spread of the histogram and thus in a disastrous
performance of the estimator.9 A similar point applies to the histogram of the
HvK. Our estimators (cid:98)a and (cid:98)σ2, in contrast, exhibit a
long-run variance estimator (cid:98)σ2
stable behaviour in this case.
Interestingly, the estimator (cid:98)aHvK (as well as the corresponding long-run variance
estimator (cid:98)σ2
HvK) performs much worse than ours for large negative values but not
for large positive values of a1. This can be explained as follows: In the special case
of an AR(1) process, the estimator (cid:98)aHvK may produce estimates smaller than −1
but it cannot become larger than 1. This can be easily seen upon inspecting the
deﬁnition of the estimator. Hence, for large positive values of a1, the estimator

9One could of course set (cid:98)aHvK to −(1 − δ) for some small δ > 0 whenever it takes a value smaller than
−1. This modiﬁed estimator, however, is still far from performing in a satisfying way when a1 is close
to −1.

29

Frequency0.100.200.300.40020406080aFrequency0.100.200.300.40020406080aHvKFrequency0.100.200.300.40020406080aoracleFrequency1.52.02.53.03.5020406080s2Frequency1.52.02.53.03.5020406080sHvK2Frequency1.52.02.53.03.5020406080soracle2(cid:98)aHvK performs well as it satisﬁes the causality restriction that the estimated AR
parameter should be smaller than 1.

(b) In the simulation scenarios with a pronounced trend (sβ = 10), the estimators of
Hall and Van Keilegom (2003) are clearly outperformed by ours for most of the
AR parameters a1 under consideration. In particular, their MSE values reported in
Figure 4 are much larger than the values produced by our estimators for most pa-
rameter values a1. The reason is the following: The HvK estimators have a strong
bias since the pronounced trend with sβ = 10 is not eliminated appropriately by the
underlying diﬀerencing methods. This point is illustrated by Figure 6 which shows
histograms of the simulated values for the estimators (cid:98)a, (cid:98)aHvK, (cid:98)aoracle and the corre-
sponding long-run variance estimators in the scenario with a1 = 0.25 and sβ = 10.
As can be seen, the histogram produced by our estimator (cid:98)a is approximately cen-
tred around the true value a1 = 0.25, whereas that of the estimator (cid:98)aHvK is strongly
biased upwards. A similar picture arises for the long-run variance estimators (cid:98)σ2
and (cid:98)σ2
Whereas the methods of Hall and Van Keilegom (2003) perform much worse than
ours for negative and moderately positive values of a1, the performance (in terms
of MSE) is fairly similar for large values of a1. This can be explained as follows:
When the trend m is not eliminated appropriately by taking diﬀerences, this creates
spurious persistence in the data. Hence, the estimator (cid:98)aHvK tends to overestimate
the AR parameter a1, that is, (cid:98)aHvK tends to be larger in absolute value than a1.
Very loosely speaking, when the parameter a1 is close to 1, say a1 = 0.95, there is
not much room for overestimation since (cid:98)aHvK cannot become larger than 1. Con-
sequently, the eﬀect of not eliminating the trend appropriately has a much smaller
impact on (cid:98)aHvK for large positive values of a1.

HvK.

6 Application

The analysis of time trends in long temperature records is an important task in clima-
tology. Information on the shape of the trend is needed in order to better understand
long-term climate variability. The Central England temperature record is the longest
instrumental temperature time series in the world. It is a valuable asset for analysing
climate variability over the last few hundred years. The data is publicly available on
the webpage of the UK Met Oﬃce. A detailed description of the data can be found in
Parker et al. (1992). For our analysis, we use the dataset of yearly mean temperatures
which consists of T = 359 observations covering the years from 1659 to 2017.
We assume that the data follow the nonparametric trend model Yt,T = m(t/T ) + εt,
where m is the unknown time trend of interest. The error process {εt} is supposed to
have the AR(p) structure εt = (cid:80)p
j=1 ajεt−j + ηt, where ηt are i.i.d. innovations with

30

Figure 7: Summary of the application results. The upper panel shows the Central England
mean temperature time series. The middle panel depicts local linear kernel estimates of the
time trend for a number of diﬀerent bandwidths h. The lower panel presents the minimal
intervals in the set Π+
T produced by the multiscale test. These are [1686, 1742], [1831, 2007],
[1866, 2012] and [1871, 2017].

mean 0 and variance ν2. As pointed out in Mudelsee (2010) among others, this is
the most widely used error model for discrete climate time series. To select the AR
order p, we proceed as follows: We estimate the AR parameters and the corresponding
variance of the innovation terms for diﬀerent AR orders by our methods from Section
4.2 and choose p to be the minimizer of the Bayesian information criterion (BIC). This
yields the AR order p = 2. We then estimate the parameters a = (a1, a2) and the
long-run error variance σ2 by the estimators (cid:98)a = ((cid:98)a1, (cid:98)a2) and (cid:98)σ2, which gives the values
(cid:98)a1 = 0.167, (cid:98)a2 = 0.178 and (cid:98)σ2 = 0.749. To select the AR order p and to produce the
estimators (cid:98)a and (cid:98)σ2, we set q = 25 and r = 10 as in the simulation study of Section
5.1.10

10As a robustness check, we have repeated the process of order selection and parameter estimation for
other values of q and r as well as for other criteria such as FPE, AIC and AICC, which gave similar
results.

31

167517251775182518751925197520257891011grid_timey_data167517251775182518751925197520258910IndexNAh = 0.01h = 0.05h = 0.10h = 0.15h = 0.2167517251775182518751925197520251.752.002.25NAWith the help of our multiscale method from Section 3, we now test the null hypothesis
H0 that m is constant on all intervals [u − h, u + h] with (u, h) ∈ GT , where we use
the grid GT deﬁned in (5.1). To do so, we set the signiﬁcance level to α = 0.05 and
implement the test in exactly the same way as in the simulations of Section 5.1. The
results are presented in Figure 7. The upper panel shows the raw temperature time
series, whereas the middle panel depicts local linear kernel estimates of the trend m for
diﬀerent bandwidths h. As one can see, the shape of the estimated time trend strongly
diﬀers with the chosen bandwidth. When the bandwidth is small, there are many local
increases and decreases in the estimated trend. When the bandwidth is large, most of
these local variations get smoothed out. Hence, by themselves, the nonparametric ﬁts
do not give much information on whether the trend m is increasing or decreasing in
certain time regions.
Our multiscale test provides this kind of information, which is summarized in the lower
panel of Figure 7. The plot depicts the minimal intervals contained in the set Π+
T , which
is deﬁned in Section 3.3. The set of intervals Π−
T is empty in the present case. The height
at which a minimal interval Iu,h = [u−h, u+h] ∈ Π+
T is plotted indicates the value of the
corresponding (additively corrected) test statistic (cid:98)ψT (u, h)/(cid:98)σ − λ(h). The dashed line
speciﬁes the critical value qT (α), where α = 0.05 as already mentioned above. According
to Proposition 3.3, we can make the following simultaneous conﬁdence statement about
the collection of minimal intervals in Π+
T . We can claim, with conﬁdence of about 95%,
that the trend function m has some increase on each minimal interval. More speciﬁcally,
we can claim with this conﬁdence that there has been some upward movement in the
trend both in the period from around 1680 to 1740 and in the period from about 1870
onwards. Hence, our test in particular provides evidence that there has been some
warming trend in the period over approximately the last 150 years. On the other hand,
as the set Π−
T is empty, there is no evidence of any downward movement of the trend.

References

Andrews, D. W. K. (1991). Heteroskedasticity and autocorrelation consistent covariance

matrix estimation. Econometrica, 59 817–858.

Benner, T. C. (1999). Central england temperatures: long-term variability and teleconnec-

tions. International Journal of Climatology, 19 391–403.

Berkes, I., Liu, W. and Wu, W. B. (2014). Koml´os-Major-Tusn´ady approximation under

dependence. Annals of Probability, 42 794–817.

Chaudhuri, P. and Marron, J. S. (1999). SiZer for the exploration of structures in curves.

Journal of the American Statistical Association, 94 807–823.

Chaudhuri, P. and Marron, J. S. (2000). Scale space view of curve estimation. Annals of

Statistics, 28 408–428.

32

Chernozhukov, V., Chetverikov, D. and Kato, K. (2014). Gaussian approximation of

suprema of empirical processes. Annals of Statistics, 42 1564–1597.

Chernozhukov, V., Chetverikov, D. and Kato, K. (2015). Comparison and anti-
concentration bounds for maxima of Gaussian random vectors. Probability Theory and
Related Fields, 162 47–70.

Chernozhukov, V., Chetverikov, D. and Kato, K. (2017). Central limit theorems and

bootstrap in high dimensions. Annals of Probability, 45 2309–2352.

Cho, H. and Fryzlewicz, P. (2012). Multiscale and multilevel technique for consistent

segmentation of nonstationary time series. Statistica Sinica, 22 207–229.

De Jong, R. M. and Davidson, J. (2000). Consistency of kernel estimators of heteroscedas-

tic and autocorrelated covariance matrices. Econometrica, 68 407–423.

Donoho, D., Johnstone, I., Kerkyacharian, G. and Picard, D. (1995). Wavelet shrink-

age: Asymptopia? Journal of the Royal Statistical Society: Series B, 57 301–369.

D¨umbgen, L. (2002). Application of local rank tests to nonparametric regression. Journal

of Nonparametric Statistics, 14 511–537.

D¨umbgen, L. and Spokoiny, V. G. (2001). Multiscale testing of qualitative hypotheses.

Annals of Statistics, 29 124–152.

D¨umbgen, L. and Walther, G. (2008). Multiscale inference about a density. Annals of

Statistics, 36 1758–1785.

Eckle, K., Bissantz, N. and Dette, H. (2017). Multiscale inference for multivariate

deconvolution. Electronic Journal of Statistics, 11 4179–4219.

Hall, P. and Heckman, N. E. (2000). Testing for monotonicity of a regression mean by

calibrating for linear functions. Annals of Statistics, 28 20–39.

Hall, P. and Van Keilegom, I. (2003). Using diﬀerence-based methods for inference in
nonparametric regression with time series errors. Journal of the Royal Statistical Society:
Series B, 65 443–456.

Hannig, J. and Marron, J. S. (2006). Advanced distribution theory for SiZer. Journal of

the American Statistical Association, 101 484–499.

Hart, J. D. (1994). Automated kernel smoothing of dependent data by using time series

cross-validation. Journal of the Royal Statistical Society: Series B, 56 529–542.

Herrmann, E., Gasser, T. and Kneip, A. (1992). Choice of bandwidth for kernel regression

when residuals are correlated. Biometrika, 79 783–795.

Ledoux, M. (2001). Concentration of Measure Phenomenon. Amer. Math. Soc.

Mudelsee, M. (2010). Climate time series analysis: classical statistical and bootstrap meth-

ods. New York, Springer.

M¨uller, H.-G. and Stadtm¨uller, U. (1988). Detecting dependencies in smooth regression

33

models. Biometrika, 75 639–650.

Park, C., , Hannig, J. and Kang, K.-H. (2009). Improved SiZer for time series. Statistica

Sinica, 19 1511–1530.

Park, C., Marron, J. S. and Rondonotti, V. (2004). Dependent SiZer: goodness-of-ﬁt

tests for time series models. Journal of Applied Statistics, 31 999–1017.

Parker, D. E., Legg, T. P. and Folland, C. K. (1992). A new daily central england

temperature series, 1772-1991. International Journal of Climatology, 12 317–342.

Proksch, K., Werner, F. and Munk, A. (2018). Multiscale scanning in inverse problems.

Forthcoming in Annals of Statistics.

Qiu, D., Shao, Q. and Yang, L. (2013). Eﬃcient inference for autoregressive coeﬃcients in

the presence of trends. Journal of Multivariate Analysis, 114 40–53.

Rahmstorf, S., Foster, G. and Cahill, N. (2017). Global temperature evolution: recent

trends and some pitfalls. Environmental Research Letters, 12.

Rohde, A. (2008). Adaptive goodness-of-ﬁt tests based on signed ranks. Annals of Statistics,

36 1346–1374.

Rondonotti, V., Marron, J. S. and Park, C. (2007). SiZer for time series: a new

approach to the analysis of trends. Electronic Journal of Statistics, 1 268–289.

Rufibach, K. and Walther, G. (2010). The block criterion for multiscale inference about
a density, with applications to other multiscale problems. Journal of Computational and
Graphical Statistics, 19 175–190.

Schmidt-Hieber, J., Munk, A. and D¨umbgen, L. (2013). Multiscale methods for shape
constraints in deconvolution: conﬁdence statements for qualitative features. Annals of
Statistics, 41 1299–1328.

Shao, Q. and Yang, L. J. (2011). Autoregressive coeﬃcient estimation in nonparametric

analysis. Journal of Time Series Analysis, 32 587–597.

Tecuapetla-G´omez, I. and Munk, A. (2017). Autocovariance estimation in regression with
a discontinuous signal and m-dependent errors: a diﬀerence-based approach. Scandinavian
Journal of Statistics, 44 346–368.

Truong, Y. K. (1991). Nonparametric curve estimation with time series errors. Journal of

Statistical Planning and Inference, 28 167–183.

Von Sachs, R. and MacGibbon, B. (2000). Non-parametric curve estimation by Wavelet
thresholding with locally stationary errors. Scandinavian Journal of Statistics, 27 475–499.

Wu, W. B. (2005). Nonlinear system theory: another look at dependence. Proc. Natn. Acad.

Sci. USA, 102 14150–14154.

Wu, W. B. and Shao, X. (2004). Limit theorems for iterated random functions. Journal of

Applied Probability 425–436.

34

Supplement to
“Multiscale Inference and
Long-Run Variance Estimation
in Nonparametric Regression
with Time Series Errors”

Marina Khismatullina
University of Bonn

Michael Vogt
University of Bonn

In this supplement, we provide the technical details and proofs that are omitted
in the paper. In addition, we report the results of some robustness checks which
complement the simulation exercises in Section 5 of the paper.

S.1 Proofs of the results from Section 3

In this section, we prove the theoretical results from Section 3. We use the following
notation: The symbol C denotes a universal real constant which may take a diﬀerent
value on each occurrence. For a, b ∈ R, we write a+ = max{0, a} and a ∨ b = max{a, b}.
For any set A, the symbol |A| denotes the cardinality of A. The notation X D= Y means
that the two random variables X and Y have the same distribution. Finally, f0(·) and
F0(·) denote the density and distribution function of the standard normal distribution,
respectively.

Auxiliary results using strong approximation theory

The main purpose of this section is to prove that there is a version of the multiscale
statistic (cid:98)ΦT deﬁned in (3.4) which is close to a Gaussian statistic whose distribution is
known. More speciﬁcally, we prove the following result.

Proposition S.1. Under the conditions of Theorem 3.1, there exist statistics (cid:101)ΦT for
T = 1, 2, . . . with the following two properties: (i) (cid:101)ΦT has the same distribution as (cid:98)ΦT
for any T , and (ii)

(cid:12)
(cid:12)(cid:101)ΦT − ΦT

(cid:12)
(cid:12) = op

(cid:16) T 1/q
√

T hmin

(cid:112)log T

(cid:17)

,

+ ρT

where ΦT is a Gaussian statistic as deﬁned in (3.3).

1

Proof of Proposition S.1. For the proof, we draw on strong approximation theory
for stationary processes {εt} that fulﬁll the conditions (C1)–(C3). By Theorem 2.1 and
Corollary 2.1 in Berkes et al. (2014), the following strong approximation result holds
true: On a richer probability space, there exist a standard Brownian motion B and a
sequence {(cid:101)εt : t ∈ N} such that [(cid:101)ε1, . . . , (cid:101)εT ] D= [ε1, . . . , εT ] for each T and

max
1≤t≤T

(cid:12)
(cid:12)
(cid:12)

t
(cid:88)

s=1

(cid:12)
(cid:12) = o(cid:0)T 1/q(cid:1)
(cid:101)εs − σB(t)
(cid:12)

a.s.,

(S.1)

where σ2 = (cid:80)
we deﬁne

k∈Z Cov(ε0, εk) denotes the long-run error variance. To apply this result,

(cid:101)ΦT = max
(u,h)∈GT

(cid:110)(cid:12)
(cid:12)
(cid:12)

(cid:101)φT (u, h)
(cid:101)σ

(cid:12)
(cid:111)
(cid:12)
,
(cid:12) − λ(h)

where (cid:101)φT (u, h) = (cid:80)T
m(t/T ) + εt replaced by (cid:101)Yt,T = m(t/T ) + (cid:101)εt for 1 ≤ t ≤ T . In addition, we let

t=1 wt,T (u, h)(cid:101)εt and (cid:101)σ2 is the same estimator as (cid:98)σ2 with Yt,T =

ΦT = max
(u,h)∈GT

Φ(cid:5)

T = max
(u,h)∈GT

(cid:110)(cid:12)
(cid:12)
(cid:12)
(cid:110)(cid:12)
(cid:12)
(cid:12)

φT (u, h)
σ
φT (u, h)
(cid:101)σ

(cid:12)
(cid:12)
(cid:12) − λ(h)
(cid:12)
(cid:12)
(cid:12) − λ(h)

(cid:111)

(cid:111)

with φT (u, h) = (cid:80)T
can write

t=1 wt,T (u, h)σZt and Zt = B(t) − B(t − 1). With this notation, we

(cid:12)
(cid:12)(cid:101)ΦT − ΦT

(cid:12) ≤ (cid:12)
(cid:12)

(cid:12)(cid:101)ΦT − Φ(cid:5)
T

(cid:12) + (cid:12)
(cid:12)

(cid:12)Φ(cid:5)

T − ΦT

(cid:12) = (cid:12)
(cid:12)

(cid:12)(cid:101)ΦT − Φ(cid:5)
T

(cid:12)
(cid:12) + op

(cid:0)ρT

(cid:112)log T (cid:1),

(S.2)

where the last equality follows by taking into account that φT (u, h) ∼ N (0, σ2) for all
(u, h) ∈ GT , |GT | = O(T θ) for some large but ﬁxed constant θ and (cid:101)σ2 = σ2 + op(ρT ).
Straightforward calculations yield that

(cid:12)
(cid:12)(cid:101)ΦT − Φ(cid:5)
T

(cid:12)
(cid:12) ≤ (cid:101)σ−1 max

(u,h)∈GT

(cid:12)(cid:101)φT (u, h) − φT (u, h)(cid:12)
(cid:12)
(cid:12).

Using summation by parts, we further obtain that

(cid:12)(cid:101)φT (u, h) − φT (u, h)(cid:12)
(cid:12)

(cid:12) ≤ WT (u, h) max
1≤t≤T

= WT (u, h) max
1≤t≤T

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

t
(cid:88)

s=1
t
(cid:88)

s=1

(cid:101)εs − σ

t
(cid:88)

s=1

(cid:8)B(s) − B(s − 1)(cid:9)(cid:12)
(cid:12)
(cid:12)

(cid:101)εs − σB(t)

(cid:12)
(cid:12)
(cid:12),

where

WT (u, h) =

T −1
(cid:88)

t=1

|wt+1,T (u, h) − wt,T (u, h)| + |wT,T (u, h)|.

2

Standard arguments show that max(u,h)∈GT WT (u, h) = O(1/
strong approximation result (S.1), we can thus infer that

√

T hmin). Applying the

(cid:12)
(cid:12)(cid:101)ΦT − Φ(cid:5)
T

(cid:12)
(cid:12) ≤ (cid:101)σ−1 max

(u,h)∈GT

(cid:12)(cid:101)φT (u, h) − φT (u, h)(cid:12)
(cid:12)
(cid:12)

≤ (cid:101)σ−1 max

(u,h)∈GT

WT (u, h) max
1≤t≤T

(cid:12)
(cid:12)
(cid:12)

t
(cid:88)

s=1

(cid:101)εs − σB(t)

(cid:12)
(cid:12)
(cid:12) = op

(cid:16) T 1/q
√

T hmin

(cid:17)

.

(S.3)

Plugging (S.3) into (S.2) completes the proof.

Auxiliary results using anti-concentration bounds

In this section, we establish some properties of the Gaussian statistic ΦT deﬁned in
(3.3). We in particular show that ΦT does not concentrate too strongly in small regions
of the form [x − δT , x + δT ] with δT converging to zero.

Proposition S.2. Under the conditions of Theorem 3.1, it holds that

(cid:16)

P

sup
x∈R

|ΦT − x| ≤ δT

(cid:17)

= o(1),

where δT = T 1/q/

√

T hmin + ρT

√

log T .

Proof of Proposition S.2. The main technical tool for proving Proposition S.2 are
anti-concentration bounds for Gaussian random vectors. The following proposition
slightly generalizes anti-concentration results derived in Chernozhukov et al. (2015), in
particular Theorem 3 therein.

Proposition S.3. Let (X1, . . . , Xp)(cid:62) be a Gaussian random vector in Rp with E[Xj] =
µj and Var(Xj) = σ2
j > 0 for 1 ≤ j ≤ p. Deﬁne µ = max1≤j≤p |µj| together with
σ = min1≤j≤p σj and σ = max1≤j≤p σj. Moreover, set ap = E[max1≤j≤p(Xj − µj)/σj]
and bp = E[max1≤j≤p(Xj − µj)]. For every δ > 0, it holds that

P

(cid:16)(cid:12)
(cid:12) max
1≤j≤p

sup
x∈R

Xj − x(cid:12)

(cid:12) ≤ δ

(cid:17)

≤ Cδ(cid:8)µ + ap + bp + (cid:112)1 ∨ log(σ/δ)(cid:9),

where C > 0 depends only on σ and σ.

The proof of Proposition S.3 is provided at the end of this section for completeness. To
apply Proposition S.3 to our setting at hand, we introduce the following notation: We
write x = (u, h) along with GT = {x : x ∈ GT } = {x1, . . . , xp}, where p := |GT | ≤ O(T θ)
for some large but ﬁxed θ > 0 by our assumptions. Moreover, for j = 1, . . . , p, we set

X2j−1 =

φT (xj1, xj2)
σ

− λ(xj2)

X2j = −

φT (xj1, xj2)
σ

− λ(xj2)

3

with xj = (xj1, xj2). This notation allows us to write

ΦT = max
1≤j≤2p

Xj,

:= E[Xj] = −λ(xj2) and thus µ = max1≤j≤2p |µj| ≤ C

where (X1, . . . , X2p)(cid:62) is a Gaussian random vector with the following properties: (i)
:=
µj
Var(Xj) = 1 for all j. Since σj = 1 for all j, it holds that a2p = b2p. Moreover, as the
variables (Xj − µj)/σj are standard normal, we have that a2p = b2p ≤ (cid:112)2 log(2p) ≤
log T . With this notation at hand, we can apply Proposition S.3 to obtain that
C

log T , and (ii) σ2
j

√

√

P

(cid:16)(cid:12)
(cid:12)ΦT − x(cid:12)

(cid:12) ≤ δT

(cid:17)

≤ CδT

(cid:104)(cid:112)log T + (cid:112)log(1/δT )

(cid:105)

= o(1)

sup
x∈R

with δT = T 1/q/

T hmin + ρT

√

√

log T , which is the statement of Proposition S.2.

Proof of Theorem 3.1

To prove Theorem 3.1, we make use of the two auxiliary results derived above. By
Proposition S.1, there exist statistics (cid:101)ΦT for T = 1, 2, . . . which are distributed as (cid:98)ΦT
for any T ≥ 1 and which have the property that

(cid:12)
(cid:12)(cid:101)ΦT − ΦT

(cid:12)
(cid:12) = op

(cid:16) T 1/q
√

T hmin

(cid:112)log T

(cid:17)

,

+ ρT

(S.4)

where ΦT is a Gaussian statistic as deﬁned in (3.3). The approximation result (S.4)
allows us to replace the multiscale statistic (cid:98)ΦT by an identically distributed version (cid:101)ΦT
which is close to the Gaussian statistic ΦT . In the next step, we show that

(cid:12)P((cid:101)ΦT ≤ x) − P(ΦT ≤ x)(cid:12)
(cid:12)

(cid:12) = o(1),

sup
x∈R

(S.5)

which immediately implies the statement of Theorem 3.1. For the proof of (S.5), we
use the following simple lemma:

Lemma S.1. Let VT and WT be real-valued random variables for T = 1, 2, . . . such that
VT − WT = op(δT ) with some δT = o(1). If

then

P(|VT − x| ≤ δT ) = o(1),

sup
x∈R

(cid:12)P(VT ≤ x) − P(WT ≤ x)(cid:12)
(cid:12)

(cid:12) = o(1).

sup
x∈R

(S.6)

(S.7)

The statement of Lemma S.1 can be summarized as follows: If WT can be approximated
by VT in the sense that VT − WT = op(δT ) and if VT does not concentrate too strongly

4

in small regions of the form [x − δT , x + δT ] as assumed in (S.6), then the distribution
of WT can be approximated by that of VT in the sense of (S.7).

Proof of Lemma S.1. It holds that

(cid:12)E(cid:2)1(VT ≤ x) − 1(WT ≤ x)(cid:3)(cid:12)
(cid:12)
(cid:12)E(cid:2)(cid:8)1(VT ≤ x) − 1(WT ≤ x)(cid:9)1(|VT − WT | ≤ δT )(cid:3)(cid:12)

(cid:12)
(cid:12)P(VT ≤ x) − P(WT ≤ x)(cid:12)
(cid:12)
= (cid:12)
≤ (cid:12)
≤ E(cid:2)1(|VT − x| ≤ δT , |VT − WT | ≤ δT )(cid:3) + o(1)
≤ P(|VT − x| ≤ δT ) + o(1).

(cid:12) + (cid:12)

(cid:12)E(cid:2)1(|VT − WT | > δT )(cid:3)(cid:12)
(cid:12)

We now apply this lemma with VT = ΦT , WT = (cid:101)ΦT and δT = T 1/q/
log T :
From (S.4), we already know that (cid:101)ΦT − ΦT = op(δT ). Moreover, by Proposition S.2, it
holds that

T hmin + ρT

√

√

(cid:16)

P

sup
x∈R

|ΦT − x| ≤ δT

= o(1).

(S.8)

(cid:17)

Hence, the conditions of Lemma S.1 are satisﬁed. Applying the lemma, we obtain (S.5),
which completes the proof of Theorem 3.1.

Proof of Proposition 3.2

t=1 wt,T (u, h)εt and (cid:98)ψB

To start with, we introduce the notation (cid:98)ψT (u, h) = (cid:98)ψA
(cid:80)T
t=1 wt,T (u, h)mT ( t

T (u, h) = (cid:80)T
(u0, h0) ∈ GT with [u0 − h0, u0 + h0] ⊆ [0, 1] such that m(cid:48)
all w ∈ [u0 − h0, u0 + h0]. (The case that −m(cid:48)
treated analogously.) Below, we prove that under this assumption,

T (u, h)+ (cid:98)ψB
T (u, h) with (cid:98)ψA
T (u, h) =
T ). By assumption, there exists
(cid:112)log T /(T h3
0) for
0) for all w can be

T (w) ≥ cT
(cid:112)log T /(T h3

T (w) ≥ cT

(cid:98)ψB
T (u0, h0) ≥

κcT

√

log T
2

(S.9)

for suﬃciently large T , where κ = ((cid:82) K(ϕ)ϕ2dϕ)/((cid:82) K 2(ϕ)ϕ2dϕ)1/2. Moreover, by
arguments very similar to those for the proof of Proposition S.1, it follows that

max
(u,h)∈GT

T (u, h)| = Op((cid:112)log T ).

| (cid:98)ψA

With the help of (S.9), (S.10) and the fact that λ(h) ≤ λ(hmin) ≤ C
infer that

(S.10)

√

log T , we can

(cid:98)ΨT ≥ max
(u,h)∈GT

= max
(u,h)∈GT

| (cid:98)ψB

| (cid:98)ψB

T (u, h)|
(cid:98)σ
T (u, h)|
(cid:98)σ

− max
(u,h)∈GT

(cid:110)| (cid:98)ψA

T (u, h)|
(cid:98)σ

(cid:111)

+ λ(h)

+ Op((cid:112)log T )

5

κcT

≥

√

log T
2(cid:98)σ

+ Op((cid:112)log T )

(S.11)

log T ) for any ﬁxed α ∈ (0, 1), (S.11)
for suﬃciently large T . Since qT (α) = O(
immediately yields that P( (cid:98)ΨT ≤ qT (α)) = o(1), which is the statement of Proposition
3.2.

√

Proof of (S.9). Write mT ( t
T − u0), where ξu0,t,T is an inter-
mediate point between u0 and t/T . The local linear weights wt,T (u0, h0) are constructed
such that (cid:80)T

t=1 wt,T (u0, h0) = 0. We thus obtain that

T ) = mT (u0) + m(cid:48)

T (ξu0,t,T )( t

(cid:98)ψB
T (u0, h0) =

T
(cid:88)

t=1

wt,T (u0, h0)

(cid:16) t

T − u0
h0

(cid:17)

h0m(cid:48)

T (ξu0,t,T ).

(S.12)

Moreover, since the kernel K is symmetric and u0 = t/T for some t, it holds that
ST,1(u0, h0) = 0, which in turn implies that

wt,T (u0, h0)

(cid:16) t

= K

(cid:17)

(cid:16) t

T − u0
h0
T − u0
h0

(cid:17)(cid:16) t

T − u0
h0

(cid:17)2(cid:46)(cid:110) T
(cid:88)

t=1

K 2(cid:16) t

T − u0
h0

(cid:17)(cid:16) t

T − u0
h0

(cid:17)2(cid:111)1/2

≥ 0.

(S.13)

From (S.12), (S.13) and the assumption that m(cid:48)
[u0 − h0, u0 + h0], we get that

T (w) ≥ cT

(cid:112)log T /(T h3

0) for all w ∈

(cid:98)ψB
T (u0, h0) ≥ cT

(cid:114)log T
T h0

T
(cid:88)

t=1

wt,T (u0, h0)

(cid:16) t

T − u0
h0

(cid:17)

.

(S.14)

Standard calculations exploiting the Lipschitz continuity of the kernel K show that for
any (u, h) ∈ GT and any given natural number (cid:96),

(cid:12)
(cid:12)
(cid:12)

1
T h

T
(cid:88)

t=1

K

(cid:16) t

T − u
h

(cid:17)(cid:16) t

T − u
h

(cid:17)(cid:96)

−

(cid:90) 1

0

1
h

K

(cid:16)w − u
h

(cid:17)(cid:16) w − u

(cid:17)(cid:96)

h

dw

(cid:12)
(cid:12)
(cid:12) ≤

C
T h

,

(S.15)

where the constant C does not depend on u, h and T . With the help of (S.13) and
(S.15), we obtain that for any (u, h) ∈ GT with [u − h, u + h] ⊆ [0, 1],

(cid:12)
(cid:12)
(cid:12)

T
(cid:88)

t=1

wt,T (u, h)

(cid:16) t

T − u
h

(cid:17)

− κ

√

(cid:12)
(cid:12)
(cid:12) ≤
T h

√

C
T h

,

(S.16)

where the constant C does once again not depend on u, h and T . (S.16) implies that
(cid:80)T
T h/2 for suﬃciently large T and any (u, h) ∈ GT with

t=1 wt,T (u, h)( t

T − u)/h ≥ κ

√

[u − h, u + h] ⊆ [0, 1]. Using this together with (S.14), we immediately obtain (S.9).

6

Proof of Proposition 3.3

In what follows, we show that

P(E+

T ) ≥ (1 − α) + o(1).

(S.17)

The other statements of Proposition 3.3 can be veriﬁed by analogous arguments. (S.17)
is a consequence of the following two observations:

(i) For all (u, h) ∈ GT with

(cid:12)
(cid:12)
(cid:12)

(cid:98)ψT (u, h) − E (cid:98)ψT (u, h)
(cid:98)σ

(cid:12)
(cid:12)
(cid:12) − λ(h) ≤ qT (α) and

(cid:98)ψT (u, h)
(cid:98)σ

− λ(h) > qT (α),

it holds that E[ (cid:98)ψT (u, h)] > 0.

(ii) For all (u, h) ∈ GT with [u−h, u+h] ⊆ [0, 1], E[ (cid:98)ψT (u, h)] > 0 implies that m(cid:48)(v) > 0

for some v ∈ [u − h, u + h].

Observation (i) is trivial, (ii) can be seen as follows: Let (u, h) be any point with
(u, h) ∈ GT and [u − h, u + h] ⊆ [0, 1]. It holds that E[ (cid:98)ψT (u, h)] = (cid:98)ψB
T (u, h), where
(cid:98)ψB
T (u, h) has been deﬁned in the proof of Proposition 3.2. As already shown in (S.12),

(cid:98)ψB
T (u, h) =

T
(cid:88)

t=1

wt,T (u, h)

(cid:16) t

T − u
h

(cid:17)

hm(cid:48)(ξu,t,T ),

where ξu,t,T is some intermediate point between u and t/T . Moreover, by (S.13), it
holds that wt,T (u, h)( t
T (u, h) can only
take a positive value if m(cid:48)(v) > 0 for some v ∈ [u − h, u + h].
From observations (i) and (ii), we can draw the following conclusions: On the event

T − u)/h ≥ 0 for any t. Hence, E[ (cid:98)ψT (u, h)] = (cid:98)ψB

(cid:8)

(cid:98)ΦT ≤ qT (α)(cid:9) =

(cid:110)

max
(u,h)∈GT

(cid:16)(cid:12)
(cid:12)
(cid:12)

(cid:98)ψT (u, h) − E (cid:98)ψT (u, h)
(cid:98)σ

(cid:12)
(cid:17)
(cid:12)
(cid:12) − λ(h)

≤ qT (α)

(cid:111)
,

it holds that for all (u, h) ∈ A+
v ∈ Iu,h = [u − h, u + h]. We thus obtain that {(cid:98)ΦT ≤ qT (α)} ⊆ E+
that

T with [u − h, u + h] ⊆ [0, 1], m(cid:48)(v) > 0 for some
T . This in turn implies

P(E+

T ) ≥ P(cid:0)

(cid:98)ΦT ≤ qT (α)(cid:1) = (1 − α) + o(1),

where the last equality holds by Theorem 3.1.

7

Proof of Proposition S.3

The proof makes use of the following three lemmas, which correspond to Lemmas 5–7
in Chernozhukov et al. (2015).

Lemma S.2. Let (W1, . . . , Wp)(cid:62) be a (not necessarily centred) Gaussian random vector
in Rp with Var(Wj) = 1 for all 1 ≤ j ≤ p. Suppose that Corr(Wj, Wk) < 1 whenever
j (cid:54)= k. Then the distribution of max1≤j≤p Wj is absolutely continuous with respect to
Lebesgue measure and a version of the density is given by

f (x) = f0(x)

p
(cid:88)

j=1

eE[Wj ]x−E[Wj ]2/2 P(cid:0)Wk ≤ x for all k (cid:54)= j (cid:12)

(cid:12) Wj = x(cid:1).

Lemma S.3. Let (W0, W1, . . . , Wp)(cid:62) be a (not necessarily centred) Gaussian random
vector with Var(Wj) = 1 for all 0 ≤ j ≤ p. Suppose that E[W0] ≥ 0. Then the map

x (cid:55)→ eE[W0]x−E[W0]2/2 P(cid:0)Wj ≤ x for 1 ≤ j ≤ p (cid:12)

(cid:12) W0 = x(cid:1)

is non-decreasing on R.

Lemma S.4. Let (X1, . . . , Xp)(cid:62) be a centred Gaussian random vector in Rp with
max1≤j≤p E[X 2

X > 0. Then for any r > 0,

X for some σ2

j ] ≤ σ2

(cid:16)

P

max
1≤j≤p

(cid:104)
Xj ≥ E

max
1≤j≤p

Xj

(cid:105)

(cid:17)

+ r

≤ e−r2/(2σ2

X ).

The proof of Lemmas S.2 and S.3 can be found in Chernozhukov et al. (2015). Lemma
S.4 is a standard result on Gaussian concentration whose proof is given e.g. in Ledoux
(2001); see Theorem 7.1 therein. We now closely follow the arguments for the proof of
Theorem 3 in Chernozhukov et al. (2015). The proof splits up into three steps.

Step 1. Pick any x ≥ 0 and set

Wj =

Xj − x
σj

+

µ + x
σ

.

By construction, E[Wj] ≥ 0 and Var(Wj) = 1. Deﬁning Z = max1≤j≤p Wj, it holds that

P

(cid:16)(cid:12)
(cid:12)
(cid:12) max

1≤j≤p

Xj − x

(cid:17)

(cid:12)
(cid:12)
(cid:12) ≤ δ

≤ P

Xj − x
σj

(cid:16)(cid:12)
(cid:12)
(cid:12) max
1≤j≤p
(cid:16)(cid:12)
(cid:12)
(cid:12) max

P

1≤j≤p

(cid:12)
(cid:12)
(cid:12) ≤
Xj − x
σj
δ
σ

(cid:17)

.

|Z − y| ≤

≤ sup
y∈R

(cid:16)

P

= sup
y∈R

(cid:17)

δ
σ

+

µ + x
σ

− y

(cid:12)
(cid:12)
(cid:12) ≤

(cid:17)

δ
σ

Step 2. We now bound the density of Z. Without loss of generality, we assume that

8

Corr(Wj, Wk) < 1 for k (cid:54)= j. The marginal distribution of Wj is N (νj, 1) with νj =
E[Wj] = (µj/σj + µ/σ) + (x/σ − x/σj) ≥ 0. Hence, by Lemmas S.2 and S.3, the random
variable Z has a density of the form

fp(z) = f0(z)Gp(z),

(S.18)

where the map z (cid:55)→ Gp(z) is non-decreasing. Deﬁne Z = max1≤j≤p(Wj − E[Wj]) and
set z = 2µ/σ + x(1/σ − 1/σ) such that E[Wj] ≤ z for any 1 ≤ j ≤ p. With these
deﬁnitions at hand, we obtain that

(cid:90) ∞

z

f0(u)du Gp(z) ≤

(cid:90) ∞

z

f0(u)Gp(u)du = P(Z > z)

≤ P (Z > z − z) ≤ exp

(cid:16)

−

(z − z − E[Z])2
+
2

(cid:17)
,

where the last inequality follows from Lemma S.4. Since Wj − E[Wj] = (Xj − µj)/σj,
it holds that

(cid:104)
E[Z] = E

max
1≤j≤p

(cid:111)(cid:105)

(cid:110)Xj − µj
σj

=: ap.

Hence, for every z ∈ R,

Gp(z) ≤

1
1 − F0(z)

(cid:16)

−

exp

(z − z − ap)2
+
2

(cid:17)
.

(S.19)

Mill’s inequality states that for z > 0,

z ≤

f0(z)
1 − F0(z)

≤ z

1 + z2
z2

.

Since (1 + z2)/z2 ≤ 2 for z ≥ 1 and f0(z)/{1 − F0(z)} ≤ 1.53 ≤ 2 for z ∈ (−∞, 1), we
can infer that

f0(z)
1 − F0(z)

≤ 2(z ∨ 1)

for any z ∈ R.

This together with (S.18) and (S.19) yields that

fp(z) ≤ 2(z ∨ 1) exp

(cid:16)

−

(z − z − ap)2
+
2

(cid:17)

for any z ∈ R.

Step 3. By Step 2, we get that for any y ∈ R and u > 0,

P(|Z − y| ≤ u) =

(cid:90) y+u

y−u

fp(z)dz ≤ 2u max

z∈[y−u,y+u]

fp(z) ≤ 4u(z + ap + 1),

where the last inequality follows from the fact that the map z (cid:55)→ ze−(z−a)2/2 (with
a > 0) is non-increasing on [a + 1, ∞). Combining this bound with Step 1, we further

9

obtain that for any x ≥ 0 and δ > 0,

P

(cid:16)(cid:12)
(cid:12)
(cid:12) max

1≤j≤p

Xj − x

(cid:17)

(cid:12)
(cid:12)
(cid:12) ≤ δ

≤ 4δ

(cid:110)2µ
σ

+ |x|

(cid:16) 1
σ

−

(cid:17)

1
σ

+ ap + 1

(cid:111)(cid:14)σ.

(S.20)

This inequality also holds for x < 0 by an analogous argument, and hence for all x ∈ R.
Now let 0 < δ ≤ σ and deﬁne bp = E max1≤j≤p{Xj − µj}. For any |x| ≤ δ + µ + bp +
σ(cid:112)2 log(σ/δ), (S.20) yields that

P

(cid:16)(cid:12)
(cid:12)
(cid:12) max

1≤j≤p

Xj − x

(cid:17)

(cid:12)
(cid:12)
(cid:12) ≤ δ

≤

(cid:110)

µ

4δ
σ

(cid:16) 3
σ
(cid:16) σ
σ

+

−

(cid:17)

1
σ

+ ap +

(cid:17)(cid:114)

− 1

2 log

(cid:16) 1
σ
(cid:17)
(cid:16)σ
δ

−

(cid:17)

bp

1
σ

+ 2 −

(cid:111)

σ
σ

≤ Cδ(cid:8)µ + ap + bp + (cid:112)1 ∨ log(σ/δ)(cid:9)

(S.21)

with a suﬃciently large constant C > 0 that depends only on σ and σ. For |x| ≥
δ + µ + bp + σ(cid:112)2 log(σ/δ), we obtain that

P

(cid:16)(cid:12)
(cid:12)
(cid:12) max

1≤j≤p

Xj − x

(cid:17)

(cid:12)
(cid:12)
(cid:12) ≤ δ

≤

δ
σ

,

(S.22)

which can be seen as follows: If x > δ + µ, then | maxj Xj − x| ≤ δ implies that
|x| − δ ≤ maxj Xj ≤ maxj{Xj − µj} + µ and thus maxj{Xj − µj} ≥ |x| − δ − µ. Hence,
it holds that

P

(cid:16)(cid:12)
(cid:12)
(cid:12) max

1≤j≤p

Xj − x

(cid:17)

(cid:12)
(cid:12)
(cid:12) ≤ δ

(cid:16)

≤ P

max
1≤j≤p

(cid:8)Xj − µj} ≥ |x| − δ − µ

(cid:17)

.

(S.23)

If x < −(δ + µ), then | maxj Xj − x| ≤ δ implies that maxj{Xj − µj} ≤ −|x| + δ + µ.
Hence, in this case,

P

(cid:16)(cid:12)
(cid:12)
(cid:12) max

1≤j≤p

Xj − x

(cid:17)

(cid:12)
(cid:12)
(cid:12) ≤ δ

(cid:16)

(cid:16)

≤ P

≤ P

max
1≤j≤p

max
1≤j≤p

(cid:8)Xj − µj} ≤ −|x| + δ + µ
(cid:17)

(cid:8)Xj − µj} ≥ |x| − δ − µ

,

(cid:17)

(S.24)

where the last inequality follows from the fact that for centred Gaussian random vari-
ables Vj and v > 0, P(maxj Vj ≤ −v) ≤ P(V1 ≤ −v) = P (V1 ≥ v) ≤ P(maxj Vj ≥ v).
With (S.23) and (S.24), we obtain that for any |x| ≥ δ + µ + bp + σ(cid:112)2 log(σ/δ),

P

(cid:16)(cid:12)
(cid:12)
(cid:12) max
1≤j≤p
(cid:16)

≤ P

(cid:17)

(cid:12)
(cid:12)
(cid:12) ≤ δ
(cid:8)Xj − µj

Xj − x

max
1≤j≤p

max
1≤j≤p
(cid:104)

(cid:16)

≤ P

(cid:8)Xj − µj} ≥ |x| − δ − µ

(cid:17)

(cid:9) ≥ E

max
1≤j≤p

(cid:8)Xj − µj

(cid:9)(cid:105)

(cid:17)
+ σ(cid:112)2 log(σ/δ)

≤

δ
σ

,

the last inequality following from Lemma S.4. To sum up, we have established that for

10

any 0 < δ ≤ σ and any x ∈ R,

P

(cid:16)(cid:12)
(cid:12)
(cid:12) max

1≤j≤p

Xj − x

(cid:17)

(cid:12)
(cid:12)
(cid:12) ≤ δ

≤ Cδ(cid:8)µ + ap + bp + (cid:112)1 ∨ log(σ/δ)(cid:9)

(S.25)

with some constant C > 0 that does only depend on σ and σ. For δ > σ, (S.25) trivially
follows upon setting C ≥ 1/σ. This completes the proof.

S.2 Proofs of the results from Section 4

In what follows, we prove Proposition 4.1 from Section 4. The notation is the same as
in the previous section. In particular, we use the symbol C to denote a generic constant
which may take a diﬀerent value on each occurrence.

Auxiliary results

To start with, we derive some auxiliary results needed for the proof of Proposition 4.1.
The ﬁrst lemma analyses the term

ξ((cid:96)1, (cid:96)2, L) =

1
T − L

T
(cid:88)

t=L+1

εt−(cid:96)1εt−(cid:96)2,

where (cid:96)1, (cid:96)2 and L are natural numbers with 0 ≤ (cid:96)1, (cid:96)2 ≤ L that may depend on the
sample size T , that is, L = LT as well as (cid:96)1 = (cid:96)1,T and (cid:96)2 = (cid:96)2,T .

Lemma S.5. For any L = LT with LT /T → 0, it holds that

(cid:104)(cid:8)ξ((cid:96)1, (cid:96)2, L) − γε((cid:96)2 − (cid:96)1)(cid:9)2(cid:105)
E

= O(T −1),

where γε((cid:96)) = Cov(εt, εt−(cid:96)).
Proof of Lemma S.5. Since the variables εt have the expansion εt = (cid:80)∞
γε((cid:96)) = ((cid:80)∞

k=0 ckck+(cid:96))ν2, it holds that

k=0 ckηt−k and

E(cid:2)ξ2((cid:96)1, (cid:96)2, L)(cid:3) =

1
(T − L)2

T
(cid:88)

t,t(cid:48)=L+1

E(cid:2)εt−(cid:96)1εt−(cid:96)2εt(cid:48)−(cid:96)1εt(cid:48)−(cid:96)2

(cid:3),

where

E(cid:2)εt−(cid:96)1εt−(cid:96)2εt(cid:48)−(cid:96)1εt(cid:48)−(cid:96)2

(cid:3)

(cid:16) ∞
(cid:88)

=

ckck+(cid:96)1−(cid:96)2ck+t(cid:48)−tck+t(cid:48)−t+(cid:96)1−(cid:96)2

(cid:16) ∞
(cid:88)

(cid:17)

κ +

ckck+(cid:96)1−(cid:96)2

(cid:17)2

ν4

k=0

k=0
(cid:16) ∞
(cid:88)

k=0

+

ckck+t(cid:48)−t

(cid:17)2

ν4 +

(cid:16) ∞
(cid:88)

ckck+t(cid:48)−t−(cid:96)1+(cid:96)2

k=0

11

(cid:17)(cid:16) ∞
(cid:88)

k=0

ckck+t(cid:48)−t+(cid:96)1−(cid:96)2

(cid:17)

ν4

(cid:16) ∞
(cid:88)

=

ckck+(cid:96)1−(cid:96)2ck+t(cid:48)−tck+t(cid:48)−t+(cid:96)1−(cid:96)2

(cid:17)

κ + γ2

ε ((cid:96)1 − (cid:96)2)

k=0
ε (t(cid:48) − t) + γε(t(cid:48) − t − (cid:96)1 + (cid:96)2)γε(t(cid:48) − t + (cid:96)1 − (cid:96)2)
+ γ2

with κ = E[η4

0] − 3ν4 and ck = 0 for k < 0. Noting that
(cid:104)(cid:8)ξ((cid:96)1, (cid:96)2, L) − γε((cid:96)1 − (cid:96)2)(cid:9)2(cid:105)
E

= E(cid:2)ξ2((cid:96)1, (cid:96)2, L)(cid:3) − γ2

ε ((cid:96)1 − (cid:96)2),

we can infer that

(cid:104)(cid:8)ξ((cid:96)1, (cid:96)2, L) − γε((cid:96)1 − (cid:96)2)(cid:9)2(cid:105)
E

=

1
(T − L)2

T
(cid:88)

(cid:16) ∞
(cid:88)

t,t(cid:48)=L+1

k=0

ckck+(cid:96)1−(cid:96)2ck+t(cid:48)−tck+t(cid:48)−t+(cid:96)1−(cid:96)2

(cid:17)

κ +

1
(T − L)2

T
(cid:88)

t,t(cid:48)=L+1

ε (t(cid:48) − t)
γ2

+

1
(T − L)2

T
(cid:88)

t,t(cid:48)=L+1

= O(T −1),

γε(t(cid:48) − t − (cid:96)1 + (cid:96)2)γε(t(cid:48) − t + (cid:96)1 − (cid:96)2)

the last equality following from the fact that the autocovariances γε((cid:96)) are absolutely
summable and the coeﬃcients ck decay exponentially fast to zero.

We next show that the empirical autocovariances

(cid:98)γq((cid:96)) =

1
T − q

T
(cid:88)

t=q+(cid:96)+1

∆qYt,T ∆qYt−(cid:96),T

of the process {∆qYt,T } have the following property.

√

Lemma S.6. For any q = qT with qT /

T → 0 and any 1 ≤ (cid:96) ≤ p + 1, it holds that

(cid:98)γq((cid:96)) − γq((cid:96)) = Op(T −1/2),

where γq((cid:96)) = Cov(∆qεt, ∆qεt−(cid:96)).

Proof of Lemma S.6. To analyse the term (cid:98)γq((cid:96)), we decompose it as follows:

where

(cid:98)γq((cid:96)) = (cid:98)γ∗

q ((cid:96)) + RA + RB + RC,

(cid:98)γ∗
q ((cid:96)) =

1
T − q

T
(cid:88)

t=q+(cid:96)+1

∆qεt ∆qεt−(cid:96)

as well as RA = (T − q)−1 (cid:80)T
and RC = (T − q)−1 (cid:80)T

t=q+(cid:96)+1 ∆qmt∆qεt−(cid:96), RB = (T − q)−1 (cid:80)T

t=q+(cid:96)+1 ∆qmt∆qmt−(cid:96) with ∆qmt = m( t

t=q+(cid:96)+1 ∆qεt∆qmt−(cid:96)
T ). With the

T ) − m( t−q

12

help of Lemma S.5, it is straightforward to show that

q ((cid:96)) − γq((cid:96)) = Op(T −1/2).
(cid:98)γ∗

Moreover, the Cauchy-Schwarz inequality yields that

E[R2

A] ≤

(cid:110) 1

T − q

T
(cid:88)

(cid:104)
(∆qmt)2(cid:111)
E

t=q+(cid:96)+1

1
T − q

T
(cid:88)

(∆qεt−(cid:96))2(cid:105)

.

t=q+(cid:96)+1

Since m is Lipschitz by assumption, we get that (T −q)−1 (cid:80)T
In addition, it obviously holds that E[(T − q)−1 (cid:80)T
can infer that

t=q+(cid:96)+1(∆qmt)2 ≤ C(q/T )2.
t=q+(cid:96)+1(∆qεt−(cid:96))2] = O(1). Hence, we

E[R2

A] = O

(cid:16)(cid:110) q
T

(cid:111)2(cid:17)

,

which implies that RA = op(T −1/2). Similar arguments yield that Rj = op(T −1/2) for
j = B, C as well. Putting everything together, we arrive at the statement of Lemma
S.6.

Proof of Proposition 4.1

We ﬁrst show that the pilot estimator (cid:101)aq converges to a.
In particular, we verify
that (cid:101)aq − a = Op(T −1/2). By Lemma S.6, it holds that (cid:98)Γq = Γq + Op(T −1/2) and
(cid:98)γq = γq + Op(T −1/2). Since Γq is invertible, this implies that

(cid:101)aq = Γ−1

q γq + Op(T −1/2).

With the help of equation (4.10), we can further infer that

(cid:101)aq − a = −ν2Γ−1

q cq + Op(T −1/2).

As already noted in Section 4.2, the entries of the vector cq = (cq−1, . . . , cc−p)(cid:62) decay
exponentially fast to zero, that is, |ck| ≤ Cρk for some 0 < ρ < 1. Moreover, it holds
that γq((cid:96)) → 2γε((cid:96)) for any ﬁxed (cid:96) as q → ∞. Consequently, (cid:107)ν2Γ−1
q cq(cid:107)∞ = o(T −1/2),
where (cid:107) · (cid:107)∞ denotes the usual supremum norm for vectors. As a result, we obtain that
(cid:101)aq − a = Op(T −1/2).
We next show that (cid:98)ar − a = Op(T −1/2), where r ≥ 1 is any ﬁxed integer that does not
(cid:101)cr). From
grow with the sample size T . By deﬁnition, it holds that (cid:98)ar = (cid:98)Γ
r = Γ−1
r + Op(T −1/2) and (cid:98)γr = γr + Op(T −1/2). Moreover,
Lemma S.6, it follows that (cid:98)Γ
with the help of the fact that (cid:101)aq − a = Op(T −1/2), it is straightforward to verify that
(cid:101)ν2 − ν2 = Op(T −1/2) and (cid:101)cr − cr = Op(T −1/2). Hence, we arrive at

r ((cid:98)γr + (cid:101)ν2

−1

−1

(cid:98)ar = Γ−1

r (γr + ν2cr) + Op(T −1/2) = a + Op(T −1/2),

(S.26)

13

where the last equality is due to equation (4.10).
From (S.26), it immediately follows that (cid:98)a − a = Op(T −1/2), which in turn allows us to
infer that (cid:98)ν2 − ν2 = Op(T −1/2) and (cid:98)σ2 = σ2 + Op(T −1/2) by straightforward arguments.

S.3 Robustness checks and implementation details

for the simulations in Section 5

Robustness checks for Section 5.3

HvK, (cid:98)σ2

In what follows, we carry out some robustness checks to assess how sensitive the esti-
mators (cid:98)a and (cid:98)σ2 are to the choice of the tuning parameters q and r. To do so, we repeat
the simulation exercises of Section 5.3 for diﬀerent values of q and r. In addition, we
consider diﬀerent choices of the tuning parameters (m1, m2) on which the estimators of
Hall and Van Keilegom (2003) depend. As in Section 5.3, we choose m1 and m2 such
that q lies between these values. We thus keep the parameters q and (m1, m2) roughly
comparable.
To start with, we consider the simulation scenarios with a moderate trend (sβ = 1).
The MSE values of the estimators (cid:98)a, (cid:98)aHvK, (cid:98)aoracle and (cid:98)σ2, (cid:98)σ2
oracle for these scenarios
are presented in Figure 3 of Section 5.3. These MSEs are re-calculated in Figures S.1
and S.2 for a range of diﬀerent choices of q, r and (m1, m2). As one can see, the MSEs
in the diﬀerent plots of Figures S.1 and S.2 are very similar. Hence, the MSE results
reported in Section 5.3 for the scenarios with a moderate trend appear to be fairly
robust to diﬀerent choices of the tuning parameters. In particular, our estimators (cid:98)a
and (cid:98)σ2 seem to be quite insensitive to the choice of tuning parameters, at least as far
as their MSEs are concerned.
We next turn to the simulation designs with a pronounced trend (sβ = 10). The MSE
values of the estimators in these scenarios are reported in Figure 4 of Section 5.3.
Analogously as before, we re-calculate these MSEs for diﬀerent tuning parameters in
Figures S.3–S.5. Figure S.4 is a zoomed-in version of Figure S.3 which is added for
better visibility. As can be seen, our estimators appear to be barely inﬂuenced by the
choice of q. However, the MSE values become somewhat larger when r is chosen bigger.
This is of course not very surprising: The main reason why the estimator (cid:98)a works well
in the presence of a strong trend is that it is only based on diﬀerences of small orders.
If we increase r, we use larger diﬀerences to compute (cid:98)a, which results in not eliminating
the trend m appropriately any more. This becomes visible in somewhat larger MSE
values. Nevertheless, overall, our estimators appear not to be strongly inﬂuenced by
the choice of tuning parameters (in terms of MSE) as long as these are chosen within
reasonable bounds.

14

Figure S.1: MSE values for the estimators (cid:98)a, (cid:98)aHvK and (cid:98)aoracle in the scenario with a moderate
trend (sβ = 1).

15

0.0000.0020.0040.006a1MSEllllllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95aaHvKaoracleq = 20, r = 10, (m1, m2) = (15,25)0.0000.0020.0040.006a1MSEllllllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95aaHvKaoracleq = 20, r = 15, (m1, m2) = (15,25)0.0000.0020.0040.006a1MSEllllllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95aaHvKaoracleq = 25, r = 10, (m1, m2) = (20,30)0.0000.0020.0040.006a1MSEllllllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95aaHvKaoracleq = 25, r = 15, (m1, m2) = (20,30)0.0000.0020.0040.006a1MSEllllllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95aaHvKaoracleq = 30, r = 10, (m1, m2) = (25,35)0.0000.0020.0040.006a1MSEllllllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95aaHvKaoracleq = 30, r = 15, (m1, m2) = (25,35)0.0000.0020.0040.0060.008a1MSEllllllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95aaHvKaoracleq = 35, r = 10, (m1, m2) = (30,40)0.0000.0020.0040.0060.008a1MSEllllllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95aaHvKaoracleq = 35, r = 15, (m1, m2) = (30,40)Figure S.2: Logarithmic MSE values for the estimators (cid:98)σ2, (cid:98)σ2
with a moderate trend (sβ = 1).

HvK and (cid:98)σ2

oracle in the scenario

16

−50510a1log(MSE)llllllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95s2sHvK2soracle2q = 20, r = 10, (m1, m2) = (15,25)−50510a1log(MSE)llllllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95s2sHvK2soracle2q = 20, r = 15, (m1, m2) = (15,25)−50510a1log(MSE)llllllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95s2sHvK2soracle2q = 25, r = 10, (m1, m2) = (20,30)−50510a1log(MSE)llllllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95s2sHvK2soracle2q = 25, r = 15, (m1, m2) = (20,30)−50510a1log(MSE)llllllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95s2sHvK2soracle2q = 30, r = 10, (m1, m2) = (25,35)−50510a1log(MSE)llllllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95s2sHvK2soracle2q = 30, r = 15, (m1, m2) = (25,35)−50510a1log(MSE)llllllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95s2sHvK2soracle2q = 35, r = 10, (m1, m2) = (30,40)−50510a1log(MSE)llllllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95s2sHvK2soracle2q = 35, r = 15, (m1, m2) = (30,40)Figure S.3: MSE values for the estimators (cid:98)a, (cid:98)aHvK and (cid:98)aoracle in the scenario with a pronounced
trend (sβ = 10).

17

0.000.010.020.030.04a1MSEllllllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95aaHvKaoracleq = 20, r = 10, (m1, m2) = (15,25)0.000.010.020.030.04a1MSEllllllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95aaHvKaoracleq = 20, r = 15, (m1, m2) = (15,25)0.000.010.020.030.04a1MSEllllllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95aaHvKaoracleq = 25, r = 10, (m1, m2) = (20,30)0.000.010.020.030.04a1MSEllllllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95aaHvKaoracleq = 25, r = 15, (m1, m2) = (20,30)0.000.040.080.12a1MSEllllllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95aaHvKaoracleq = 30, r = 10, (m1, m2) = (25,35)0.000.040.080.12a1MSEllllllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95aaHvKaoracleq = 30, r = 15, (m1, m2) = (25,35)0.000.050.100.15a1MSEllllllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95aaHvKaoracleq = 35, r = 10, (m1, m2) = (30,40)0.000.050.100.15a1MSEllllllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95aaHvKaoracleq = 35, r = 15, (m1, m2) = (30,40)Figure S.4: MSE values for the estimators (cid:98)a, (cid:98)aHvK and (cid:98)aoracle in the scenario with a pronounced
trend (sβ = 10). The plots are zoomed-in versions of the respective plots in Figure S.3.

18

0.0000.0050.0100.0150.020a1MSElllllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95aaHvKaoracleq = 20, r = 10, (m1, m2) = (15,25)0.0000.0050.0100.0150.020a1MSElllllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95aaHvKaoracleq = 20, r = 15, (m1, m2) = (15,25)0.0000.0050.0100.0150.020a1MSElllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95aaHvKaoracleq = 25, r = 10, (m1, m2) = (20,30)0.0000.0050.0100.0150.020a1MSElllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95aaHvKaoracleq = 25, r = 15, (m1, m2) = (20,30)0.0000.0050.0100.0150.020a1MSEllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95aaHvKaoracleq = 30, r = 10, (m1, m2) = (25,35)0.0000.0050.0100.0150.020a1MSEllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95aaHvKaoracleq = 30, r = 15, (m1, m2) = (25,35)0.0000.0050.0100.0150.020a1MSEllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95aaHvKaoracleq = 35, r = 10, (m1, m2) = (30,40)0.0000.0050.0100.0150.020a1MSEllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95aaHvKaoracleq = 35, r = 15, (m1, m2) = (30,40)Figure S.5: Logarithmic MSE values for the estimators (cid:98)σ2, (cid:98)σ2
with a pronounced trend (sβ = 10).

HvK and (cid:98)σ2

oracle in the scenario

19

−50510a1log(MSE)llllllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95s2sHvK2soracle2q = 20, r = 10, (m1, m2) = (15,25)−50510a1log(MSE)llllllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95s2sHvK2soracle2q = 20, r = 15, (m1, m2) = (15,25)−50510a1log(MSE)llllllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95s2sHvK2soracle2q = 25, r = 10, (m1, m2) = (20,30)−50510a1log(MSE)llllllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95s2sHvK2soracle2q = 25, r = 15, (m1, m2) = (20,30)−50510a1log(MSE)llllllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95s2sHvK2soracle2q = 30, r = 10, (m1, m2) = (25,35)−50510a1log(MSE)llllllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95s2sHvK2soracle2q = 30, r = 15, (m1, m2) = (25,35)−50510a1log(MSE)llllllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95s2sHvK2soracle2q = 35, r = 10, (m1, m2) = (30,40)−50510a1log(MSE)llllllllllllllllllllllll−0.95−0.75−0.5−0.250.250.50.750.95s2sHvK2soracle2q = 35, r = 15, (m1, m2) = (30,40)Implementation of SiZer in Section 5.2

The SiZer methods for the comparison study in Section 5.2 are implemented as follows:

(a) Computation of the grid G∗
T :
To start with, we compute the variance of ¯Y = T −1 (cid:80)T

t=1 Yt,T , which is given by

Var( ¯Y ) =

γε(0)
T

+

2
T

T −1
(cid:88)

k=1

(cid:16)

1 −

(cid:17)

k
T

γε(k).

Since the autocovariance function γε(·) is known by assumption, we can calculate
the value of Var( ¯Y ) by using the formula γε(k) = ν2a|k|
1) together with the
true parameters a1 and ν2 = E[η2

t ]. We next compute

1 /(1 − a2

T ∗ =

γε(0)
Var( ¯Y )

,

which can be interpreted as a measure of information in the data. For each point
(u, h) ∈ GT from (5.1), we ﬁnally calculate the eﬀective sample size for dependent
data

ESS∗(u, h) =

(cid:80)T

T ∗
T

t=1 Kh(t/T − u)
Kh(0)

with Kh(v) = h−1K(v/h) and set G∗

T = {(u, h) ∈ GT : ESS∗(u, h) ≥ 5}.

(b) Computation of the local linear estimators and their standard deviations:

T , we compute a standard local linear estimator (cid:98)m(cid:48)

For each (u, h) ∈ G∗
derivative m(cid:48)(u) together with its standard deviation sd( (cid:98)m(cid:48)
by sd( (cid:98)m(cid:48)

h(u) of the
h(u)). The latter is given
h(u)) = e(cid:62)V e with e = (0 1)(cid:62) and

h(u))}1/2, where Var( (cid:98)m(cid:48)

h(u)) = {Var( (cid:98)m(cid:48)

V = (X T W X)−1(X T ΣX)(X T W X)−1.

The matrices X, W and Σ are deﬁned as follows: Σ is a T × T matrix with the
elements

Σst = γε(s − t)Kh

(cid:16) s
T

(cid:17)

− u

Kh

(cid:16) t
T

(cid:17)

− u

,

W is a T × T diagonal matrix with the diagonal entries Kh(t/T − u) and









X =

1 (1/T − u)
1 (2/T − u)
...
...
(1 − u)
1









.

20

(c) Computation of the conﬁdence intervals:

For a given conﬁdence level α and for each bandwidth value h with (u, h) ∈ G∗
compute the quantile

T , we

q(h) = Φ−1(cid:16)(cid:16)

1 −

(cid:17)1/(θg)(cid:17)

,

α
2

where Φ is the distribution function of a standard normal random variable, g is
the number of locations u in the grid GT , and the cluster index θ is deﬁned on
p.1519 in Park et al. (2009). The conﬁdence interval of (cid:98)m(cid:48)
h(u) is then computed as
[ (cid:98)m(cid:48)

h(u) − q(h) sd( (cid:98)m(cid:48)

h(u) + q(h) sd( (cid:98)m(cid:48)

h(u)), (cid:98)m(cid:48)

h(u))].

21

