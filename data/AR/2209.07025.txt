Dynamic X-Ray Vision in Mixed Reality

Hung-Jui Guo
hxg190003@utdallas.edu
The University of Texas at Dallas
Richardson, Texas, USA

Jonathan Z. Bakdash
Laura R. Marusich
jonathan.z.bakdash.civ@army.mil
laura.m.cooper20.civ@army.mil
U.S. DEVCOM Army Research
Laboratory
Texas, USA

Balakrishnan Prabhakaran
bprabhakaran@utdallas.edu
The University of Texas at Dallas
Richardson, Texas, USA

2
2
0
2

p
e
S
5
1

]

C
H
.
s
c
[

1
v
5
2
0
7
0
.
9
0
2
2
:
v
i
X
r
a

Figure 1: (a) Side view of the created collaborative scene understanding generated augmented reality environment based on
an indoor room, (b) Dynamic X-ray vision window used by a user to navigate the indoor environment

ABSTRACT
X-ray vision, a technique that allows users to see through walls and
other obstacles, is a popular technique for Augmented Reality (AR)
and Mixed Reality (MR). In this paper, we demonstrate a dynamic
X-ray vision window that is rendered in real-time based on the
user’s current position and changes with movement in the physical
environment. Moreover, the location and transparency of the win-
dow are also dynamically rendered based on the user’s eye gaze.
We build this X-ray vision window for a current state-of-the-art
MR Head-Mounted Device (HMD) – HoloLens 2 [5] by integrating
several different features: scene understanding, eye tracking, and
clipping primitive.

CCS CONCEPTS
• Human-centered computing → Mixed / augmented reality.

Authors’ addresses: Hung-Jui Guo, hxg190003@utdallas.edu, The University of Texas
at Dallas, 800 W Campbell Rd, Richardson, Texas, USA, 75080; Jonathan Z. Bakdash,
jonathan.z.bakdash.civ@army.mil; Laura R. Marusich, laura.m.cooper20.civ@army.mil,
U.S. DEVCOM Army Research Laboratory, 800 W Campbell Rd, Texas, USA, 75080;
Balakrishnan Prabhakaran, bprabhakaran@utdallas.edu, The University of Texas at
Dallas, 800 W Campbell Rd, Richardson, Texas, USA, 75080.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
© 2018 Association for Computing Machinery.
2476-1249/2018/8-ART111 $15.00
https://doi.org/XXXXXXX.XXXXXXX

KEYWORDS
X-ray vision, HoloLens, Mixed Reality

ACM Reference Format:
Hung-Jui Guo, Jonathan Z. Bakdash, Laura R. Marusich, and Balakrishnan
Prabhakaran. 2018. Dynamic X-Ray Vision in Mixed Reality. Proc. ACM
Meas. Anal. Comput. Syst. 37, 4, Article 111 (August 2018), 3 pages. https:
//doi.org/XXXXXXX.XXXXXXX

1 INTRODUCTION
Generally, in a real environment it is almost impossible for a user
to obtain information occluded by environmental objects, such as
a chair behind a wall, when the user has no prior understanding
of the environment. Even when advanced information about the
structure of the environment is available and presented with a tool
such as a minimap, physical occlusions still challenge users’ ability
to obtain direct visual information about objects and the spatial
layout of the beyond-line-of-sight environment.

A virtual X-ray vision window allows a user to directly view a
target object through a physical occlusion at a fixed point, enabling
quick and accurate judgments about the occluded target object
while not sacrificing knowledge about the occluding surrounding
environment. By providing information otherwise unavailable in
the real-world environment, X-ray vision has potential law enforce-
ment and military applications (e.g., knowledge of the locations of
beyond-line-of-sight threats and civilians prior to entering a room)
[6]. Examples of previous implementations of X-ray vision systems

 
 
 
 
 
 
Conference’17, July 2017, Washington, DC, USA

H.,J., Guo, J. Z. Bakdash, L. R. Marusich and B. Prabhakaran

include robot with a stereo depth camera [6] and image-based ren-
dering [1]. In both of these implementations, the X-ray window is
a flat image aligned with a physical surface.

In contrast to previous approaches, virtual content in our X-ray
window is rendered dynamically using scene understanding. That
is, the virtual window updates in real-time based upon the user’s
position, movement, and eye-gaze in the physical environment.
Consequently, our X-ray vision system mimics a physical window. It
provides non-pictorial depth cues such as motion parallax (changes
in the perspectives of stationary objects with self-motion) and
binocular disparity (slightly different visual images in each eye
due to their horizontal separation) typically available in real-world
environments [2]. This allows users to move and still naturally
see through physical occlusions (such as real walls, created by the
scene understanding observer) as if looking through a window in
the real-world.

2 DYNAMIC X-RAY VISION WINDOW
The dynamic X-ray vision window is built in the HoloLens 2 de-
vice [5], a Head-Mounted Device (HMD) that includes the Mixed
Reality (MR) environment. This system incorporated the virtual en-
vironment created by HoloLens 2 Scene Understanding feature pre-
built in the Mixed Reality ToolKit (MRTK) (https://docs.microsoft.
com/en-us/windows/mixed-reality/mrtk-unity/mrtk2/). We use the
long-throw depth camera to observe the surrounding real-world
environment; we call this observed environment a collaborative
scene understanding generated augmented reality environment,
see Figure 1 (a).

In addition, to further help users see through the occluding real-
world surface from a distance, the X-ray vision window uses the
clipping primitive feature created by the MRTK. This makes the
contact area of the touched scene objects partially transparent, as
shown in Figure 1 (b). Moreover, we also use the gaze-tracking
feature in HoloLens 2 to obtain real-time eye direction/movements
and its intersection with scene objects to determine the location of
the X-ray vision window.

Using a dynamic window for X-ray vision based on user posi-
tion/movement and eye-gaze is an advancement over image based
X-ray vision. This X-ray vision has the potential to provide benefits
over prior implementations. However, the usability and human
performance with our system needs to be evaluated. For example,
it is possible users may find that interacting with the X-ray vision
window is unfamiliar, uncomfortable, or even distracting. Using
previous behavioral research on X-ray vision in AR/MR as a refer-
ence (see [4] for a summary), we propose a user evaluation of the
dynamic X-ray vision window demonstrated here.

will incorporate participants’ physical movement, allowing them to
explore the environment, while dynamically updating virtual infor-
mation in real-time. This feature allows us to explore the benefit of
our X-ray vision window system and design a more realistic user
study with visual depth cues that are not present when stationary
(e.g., motion parallax).

Therefore, we plan to build a game-like user study to explore
how a dynamic X-ray vision window (as compared to other forms
of X-ray vision or other types of advanced spatial knowledge) may
benefit human performance, measured by physical movement di-
rection and/or reaction time. The study will include target objects
placed behind occluders in a physical environment (such as an
indoor room), and will require participants to move in the environ-
ment and acquire target objects with and without the X-ray vision
window.

3 CONCLUSION AND FUTURE WORK
In this paper, we present a new dynamic X-ray vision window
built in HoloLens 2, which can render beyond-line-of-sight content
in real-time based on user position and movement with eye-gaze
direction to potentially improve usability. In the future, we will
evaluate and improve this proposed system according to the user
study design described in the previous section and make this system
more realistic and close to the physical world by attaching textures
to virtual surfaces.

ACKNOWLEDGMENTS
This research was sponsored by the DEVCOM U.S. Army Research
Laboratory under Cooperative Agreement Number W911NF-21-2-
0145 to B.P.
The views and conclusions contained in this document are those
of the authors and should not be interpreted as representing the
official policies, either expressed or implied, of the DEVCOM Army
Research Laboratory or the U.S. Government. The U.S. Government
is authorized to reproduce and distribute reprints for Government
purposes notwithstanding any copyright notation.

REFERENCES
[1] Benjamin Avery, Christian Sandor, and Bruce H. Thomas. 2009. Improving Spatial
Perception for Augmented Reality X-Ray Vision. In 2009 IEEE Virtual Reality
Conference. 79–82. https://doi.org/10.1109/VR.2009.4811002

[2] James E Cutting and Peter M Vishton. 1995. Perceiving layout and knowing
distances: The integration, relative potency, and contextual use of different infor-
mation about depth. In Perception of space and motion. Elsevier, 69–117.

[3] Yuki Kitajima, Sei Ikeda, and Kosuke Sato. 2015. [POSTER] Vergence-Based AR
X-ray Vision. In 2015 IEEE International Symposium on Mixed and Augmented
Reality. 188–189. https://doi.org/10.1109/ISMAR.2015.58

[4] Mark A. Livingston, Arindam Dey, Christian Sandor, and Bruce H. Thomas. 2013.
Pursuit of “X-Ray Vision” for Augmented Reality. Springer New York, New York,
NY, 67–107. https://doi.org/10.1007/978-1-4614-4205-9_4

2.1 Proposed User Study Design
Among previous evaluations of X-ray vision in AR/MR, the most
common method is to assess how an X-ray vision window may
affect participants’ depth perception of physical/virtual objects
([6, 8]). Other methods include testing whether the participants
can understand occlusion layers [3], or asking the participants to
acquire target objects under different X-ray vision settings [7].

According to [4], most previous X-ray vision studies are con-
ducted while participants are stationary. In our proposed study, We

[5] Microsoft. 2015. HoloLens. https://www.microsoft.com/en-us/hololens.
[6] Nate Phillips, Farzana Alam Khan, Brady Kruse, Cindy Bethel, and J. Edward Swan.
2021. An X-Ray Vision System for Situation Awareness in Action Space. In 2021
IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops
(VRW). 593–594. https://doi.org/10.1109/VRW52623.2021.00179

[7] Christian Sandor, Andrew Cunningham, Arindam Dey, and Ville-Veikko Mattila.
2010. An Augmented Reality X-Ray system based on visual saliency. In 2010
IEEE International Symposium on Mixed and Augmented Reality. 27–36. https:
//doi.org/10.1109/ISMAR.2010.5643547

[8] Stefanie Zollmann, Raphael Grasset, Gerhard Reitmayr, and Tobias Langlotz. 2014.
Image-Based X-Ray Visualization Techniques for Spatial Understanding in Out-
door Augmented Reality. In Proceedings of the 26th Australian Computer-Human
Interaction Conference on Designing Futures: The Future of Design (Sydney, New

Dynamic X-Ray Vision in Mixed Reality

Conference’17, July 2017, Washington, DC, USA

South Wales, Australia) (OzCHI ’14). Association for Computing Machinery, New

York, NY, USA, 194–203. https://doi.org/10.1145/2686612.2686642

