1

7
1
0
2

n
u
J

8

]

C
N
.
o
i
b
-
q
[

1
v
1
5
4
2
0
.
6
0
7
1
:
v
i
X
r
a

Differential Covariance: A New Class of Methods
to Estimate Sparse Connectivity from Neural Recordings
Tiger W. Lin1, 2, Anup Das1, 5, Giri P. Krishnan4, Maxim Bazhenov4,
Terrence J. Sejnowski1, 3
1Howard Hughes Medical Institute, Computational Neurobiology Laboratory, Salk In-
stitute for Biological Studies, La Jolla, CA 92037.
2Neurosciences Graduate Program, University of California San Diego, La Jolla, CA
92092.
3Institute for Neural Computation, University of California San Diego, La Jolla, CA
92092.
4Department of Medicine, University of California San Diego, La Jolla, CA 92092.
5Jacobs School of Engineering, University of California San Diego, La Jolla, CA
92092.

Keywords: Functional connectivity, correlation estimation, spiking neural network,

sparse connectivity, neural recordings, local ﬁeld potential, calcium imaging

Abstract

With our ability to record more neurons simultaneously, making sense of these data
is a challenge. Functional connectivity is one popular way to study the relationship
between multiple neural signals. Correlation-based methods are a set of currently well-
used techniques for functional connectivity estimation. However, due to explaining
away and unobserved common inputs (Stevenson et al., 2008), they produce spurious
connections. The general linear model (GLM), which models spikes trains as Poisson
processes (Okatan et al., 2005; Truccolo et al., 2005; Pillow et al., 2008), avoids these
confounds. We develop here a new class of methods by using differential signals based
on simulated intracellular voltage recordings. It is equivalent to a regularized AR(2)
model. We also expand the method to simulated local ﬁeld potential (LFP) recordings
and calcium imaging. In all of our simulated data, the differential covariance-based
methods achieved better or similar performance to the GLM method and required fewer
data samples. This new class of methods provides alternative ways to analyze neural
signals.

 
 
 
 
 
 
1

Introduction

Simultaneous recording of large population of neurons is an inexorable trend in current
neuroscience research (Kandel et al., 2013). Over the last ﬁve decades, the number of
simultaneously recorded neurons doubles approximately every 7 years (Stevenson and
Kording, 2011). One way to make sense of this big data is to measure the functional
connectivity between neurons (Friston, 2011), and link the function of the neural cir-
cuit to behavior. As previously reviewed (Stevenson et al., 2008), correlation-based
methods have been used to estimate functional connectivity for a long time. However,
they are suffering from the problem of explaining away and unobserved common inputs
(Stevenson et al., 2008), which make it difﬁcult to interpret the link between the esti-
mated correlation and the physiological network. More recently, Okatan et al. (2005);
Truccolo et al. (2005); Pillow et al. (2008) applied the generalized linear model to spike
train data and showed good probabilistic modeling of the data.

To overcome these issues, we developed a new class of methods that use not only
the spike trains but the voltages of the neurons. They achieve better performance to
the GLM method but are free from the Poisson process model and require fewer data
samples. They provide directionality information about sources and sinks in a network,
which is important to determine the hierarchical structure of a neural circuit.

In this paper, we further show that our differential covariance method is equivalent to
a regularized second-order multivariate autoregressive model. Multivariate autoregres-
sive (MAR) model has been used before to analyze neuroscience data (Friston, 2011;
McIntosh and Gonzalez-Lima, 1991). In particular, MAR model with an arbitrary or-
der has been discussed before (Harrison et al., 2003). Continuous AR process, which is
known as Ornstein-Uhlenbeck (OU) process (Uhlenbeck and Ornstein, 1930), has also
been applied to model neuronal activity (Burkitt, 2006; Ricciardi and Sacerdote, 1979;
L´ansk`y and Rospars, 1995). However, the modiﬁed AR(2) model in this paper haven’t
been used before.

In this paper, all of the data that we analyze are simulated data so that we can
compare different methods with ground truth. We ﬁrst generated data using a simple
passive neuron model, and provide theoretical proof for why the new methods perform
better. Then, we used a more realistic Hodgkin-Huxley (HH) based thalamocortical
model to simulate intracellular recordings and local ﬁeld potential data. This model
can successfully generate sleep patterns such as spindles and slow waves (Bazhenov
et al., 2002; Chen et al., 2012; Bonjean et al., 2011). Since the model has a cortical
layer and a thalamic layer, we further assume that the neural signals in the cortical layer
are visible by the recording instruments while those from the thalamic layer are not.
This is a reasonable assumption for many experiments. Since, thalamus is a deep brain
structure, most experiments involve only measurements from cortex.

Next, we simulated 1000 Hodgkin-Huxley neurons networks with 80% excitatory
neurons and 20% inhibitory neurons sparsely connected. As in real experiments, We
recorded simulated calcium signals from only a small percentage of the network (50
neurons) and compared the performance of different methods. In all simulations, our
differential covariance-based methods achieve better or similar performance to the GLM
method. And in the LFP and calcium imaging dataset, they achieve the same perfor-
mance with fewer data samples.

2

The paper is organized as follow: In section 2, we introduce our new methods. In
section 3, we show the performance of all methods and explain why our methods per-
form better. In section 4, we discuss the advantage and generalizability of our methods.
We also propose a few improvements that can be done in the future.

2 Methods

2.1 Simulation models used to benchmark the methods

2.1.1 Passive neuron model

To validate and test our new methods, we ﬁrst developed a passive neuron model. Be-
cause of its simplicity, we can provide some theoretical proof for why our new class of
methods are better. Every neuron in this model has a passive cell body with capacitance
C and a leakage channel with conductance gl. Neurons are connected with a directional
synaptic conductance gsyn; For example, neuron i receiving inputs from neurons i − 4
and i − 3:

= gsynVi−4 + gsynVi−3 + glVi + Ni

(1)

C

dVi
dt

Here, we let C = 1, gsyn = 3, gl = −5, and Ni is a Gaussian noise with standard
deviation of 1. The connection pattern is shown in Fig.3A. There are 60 neurons in
this circuit. The ﬁrst 50 neurons are connected with a pattern that: neuron i projects to
neuron i + 3 and i + 4. To make the case more realistic, aside from these 50 neurons that
are visible, we added 10 more neurons (neuron 51 to 60 in Fig.3A) that are invisible
during our estimations (i.e. only the membrane voltages of the ﬁrst 50 neurons are used
to estimate connectivity). These 10 neurons send latent inputs to the visible neurons
and introduce external correlations into the system. Therefore, we update our passive
neuron’s model as:
dVi
dt

= gsynVi−4 + gsynVi−3 + glatentVlatent1 + glVi + Ni

(2)

C

where glatent = 10. We choose the latent input’s strength in the same scale as other
connections in the network. We tested multiple values between [0, 10], higher value
generates more interference and therefore makes the case more difﬁcult.

We added the latent inputs to the system because unobserved common inputs ex-
ist in real-world problems (Stevenson et al., 2008). For example, one could be using
two-photon imaging to record calcium signals from the cortical circuit. The cortical
circuit might receive synaptic inputs from deeper layers in the brain, such as the tha-
lamus, which is not visible to the microscopy. Each invisible neuron projects to many
visible neurons leading to common synaptic currents to the cortical circuit and causes
neurons in the cortical circuit to be highly correlated. Later, we discuss how to remove
interference from the latent inputs.

2.1.2 Thalamocortical model

To test and benchmark the differential covariance-based methods in a more realistic
model, we simulated neural signals from a Hodgkin-Huxley based spiking neural net-

3

Figure 1: Network model for the thalamocortical interactions, which included four lay-
ers of neurons. Thalamocortical (TC), reticular nucleus (RE) of the thalamus, corti-
cal pyramidal (PY) and inhibitory (IN) neurons. Small solid dots indicate excitatory
synapses. Small open dots indicate inhibitory synapses.

4

PYPYPYPYPYPYPYPYININRERERERETCTCTCTCAMPAAMPAAMPAAMPA + NMDAAMPA + NMDAGABA-AGABA-AGABA-A + GABA-Bwork. The thalamocortical model used in this study was based on several previous stud-
ies, which were used to model spindle and slow wave activity (Bazhenov et al., 2002;
Chen et al., 2012; Bonjean et al., 2011). A schematic of the thalamocortical model in
this work is shown in Fig. 1. As shown, the thalamocortical model was structured as
a one-dimensional, multi-layer array of cells. The thalamocortical network consisted
of 50 cortical pyramidal (PY) neurons, 10 cortical inhibitory (IN) neurons, 10 thalamic
relay (TC) neurons and 10 reticular (RE) neurons. The connections between the 50
PY neurons follow the pattern in the passive neuron model and are shown in Fig. 7A.
For the rest of the connection types, a neuron connects to all target neurons within the
radius listed in Table 1 (Bazhenov et al., 2002). The network is driven by spontaneous
oscillations. Details of the model are explained in appendix C.

Table 1: Connectivity properties

PY→TC PY→RE TC→PY TC→IN PY→IN IN→PY RE→RE TC→RE RE→TC
1

15

6

8

3

5

5

8

8

Radius

For each simulation, we simulated the network for 600 secs. The data were sampled

at 1000 Hz.

2.1.3 Local ﬁeld potential (LFP) model

To simulate local ﬁeld potential data, we expanded the previous thalamocortical model
by 10 times (Fig. 2). Each connection in the previous network (Fig. 1) is transformed
into a ﬁber bundle connecting 10 pairs of neurons in a sub-network. For cortical pyra-
midal neurons (PY), we connect each neuron to its 4 neighboring neurons inside the
sub-network. Other settings for the neurons and the synapses are the same as the pre-
vious thalamocortical model. We simulated the network for 600 secs. The data were
sampled at 1000 Hz.

We plant a LFP electrode at the center of each of the 50 PY neuron local circuits.
The distance between the 10 local neurons is 100 µm, and the distance between the
PY sub-networks is 1 cm. The LFP is calculated according to the “standard model” as
previously mentioned in Bonjean et al. (2011); Destexhe (1998); Nunez and Srinivasan
(2005). The LFPs are mainly contributed by elongated dendrites of the cortical pyrami-
dal neurons. In our model, each cortical pyramidal neuron has a 2 mm long dendrite.

For each LFP Si,

Si =

(cid:88)
(

j

Isyn
ri
d

−

Isyn
ri
s

)j

(3)

Where the sum is taken over all excitatory cortical neurons. Isyn is the post-synaptic
current of neuron j. rd is the distance from the electrode to the center of the dendrite of
neuron j. rs is the distance from the electrode to the soma of neuron j.

2.1.4 Calcium imaging model

Vogelstein et al. (2009) proposed a transfer function between spike trains and calcium

5

Figure 2: The LFP model was transformed from the thalamocortical model. Shown in
this ﬁgure, we used the TC → PY connection in the red box as an example. Each neuron
in the original thalamocortical model was expanded into a sub-network of 10 neurons.
Each connection in the original thalamocortical model was transformed into 10 parallel
connections between two sub-networks. Moreover, sub-networks transformed from PY
neurons have local connections. Each PY neuron in this sub-network connects to its 2
nearest neighbors on each side. We put a LFP electrode at the center of each PY sub-
network. The electrode received neurons’ signals inversely proportional to the distance.

6

PYPYPYPYPYPYPYPYPYPYTCTCTCTCTCTCTCTCTCTCPYPYPYPYPYPYPYPYININRERERERETCTCTCTCAMPAAMPAAMPAAMPA + NMDAAMPA + NMDAGABA-AGABA-AGABA-A + GABA-BLFP electrodeﬂorescence signals.

[Ca]t − [Ca]t−1 = − ∆t
τCa
F = [Ca]

[Ca]+Kd

[Ca]t−1 + ACant
+ ηt

(4)

Where ACa = 50 µM is a step inﬂux of calcium molecules at each action potential.
nt is the number of spikes at each time step. Kd = 300 µM is the saturation concentra-
tion of calcium. ηt is a Gaussian noise with a standard deviation of 0.000003. Since our
data is sampled at 1000 Hz, we can resolve every single action potential. So in our data,
there is no multiple spikes at one time step. τCa = 1 s is the decay constant for calcium
molecules. To maintain the information in the differential signal, instead of setting a
hard cutoff value and transform the intracellular voltages to binary spike trains, we use
a sigmoid function to transform the voltages to the calcium inﬂux activation parameter
(nt).

nt =

1
1 + e−(V (t)−Vthre)
Where Vthre = −50 mV is the threshold potential.
In real experiments, we can only image a small percentage of neurons in the brain.
Therefore, we simulated HH-based networks of 1000 neurons and only record from 50
neurons. We used the 4 connection patterns (normal-1 ∼ normal-4) provided on-line
(https://www.kaggle.com/c/connectomics ) (Stetter et al., 2012). Brieﬂy,
the networks have 80% excitatory neurons and 20% inhibitory neurons. The connection
probability is 0.01, i.e. one neuron connects to about 10 neurons, so it is a sparsely
connected network.

(5)

Similar to our thalamocortical model, we used AMPA and NMDA synapses for the
excitatory synapses, and GABA synapses for the inhibitory synapses. The simulations
ran 600secs and were sampled at 1000Hz. The intracellular voltages obtained from
the simulations were then transferred to calcium ﬂorescence signals and down sampled
to 50 Hz. For each of the 4 networks, we conducted 25 recordings. Each recording
contains 50 randomly selected neurons’ calcium signals.

For accurate estimations, the differential covariance-based methods require the re-
constructed action potentials from the calcium imaging. While this is an active research
area and many methods have been proposed (Quan et al., 2010; Rahmati et al., 2016), In
this study, we simply reversed the transfer function. By assuming the transfer function
from action potentials to calcium ﬂuorescence signals is known, we can reconstruct the
action potentials as:

Given ˆF as the observed ﬂuorescence signal

ˆ[Ca] = ˆF ∗ Kd/(1 − ˆF )
ˆnt = ( ˆd[Ca] + 1/τCa[Ca]t)/(A/∆t)
ˆV = log(1/nt − 1)

(6)

2.2 Differential covariance-based Methods

In this section, we introduce a new class of methods to estimate the functional connec-
tivity of neurons (code is provided on-line at https://github.com/tigerwlin/
diffCov).

7

2.2.1 Step 1: differential covariance

The input to the method, V (t), is a N × T neural recording dataset. N is the number of
neurons/channels recorded, and T is the number of data samples during recordings. We
compute the derivative of each time series with dV (t) = (V (t + 1) − V (t − 1))/(2dt).
Then, the covariance between V(t) and dV(t) is computed and denoted as ∆C, which is
a N × N matrix deﬁned as the following:

∆Ci,j = cov(dVi(t), Vj(t))

(7)

where dVi(t) is the differential signal of neuron/channel i, Vj(t) is the signal of neu-
ron/channel j, and cov() is the sample covariance function for two time series. In ap-
pendix A, we provide a theoretical proof about why the differential covariance estimator
generates less false connections than the covariance-based methods.

2.2.2 Step 2: applying partial covariance method

As previously mentioned in Stevenson et al. (2008), one problem of the correlation
method is the propagation of correlation. Here we designed a customized partial co-
variance algorithm to reduce this type of error in our methods. We use ∆Pi,j to denote
the differential covariance estimation after applying the partial covariance method.

Using the derivation from appendix B.2, we have:

∆Pi,j = ∆Ci,j − COVj,Z · COV −1

Z,Z · ∆T

Ci,Z

(8)

Where Z is a set of all neurons/channels except {i, j}.

Z = {1, 2, ..., i − 1, i + 1, ..., j − 1, j + 1, ...N }

∆Ci,j and ∆Ci,Z were computed from section 2.2.1, and COVZ,Z is the covariance
matrix of set Z. COVj,Z is a ﬂat matrix denoting the covariance of neuron j and neurons
in set Z. · is the matrix dot product.

As explained in appendix B.2, the partial covariance of the differential covariance is
not equivalent to the inverse of the differential covariance estimation. The two are only
equivalent for the covariance matrix and when the partial correlation is controlling on
all variables in the observed set. In our case, the differential covariance matrix is non-
symmetric because it is the covariance between recorded signals and their differential
signals. We have signals and differential signals in our observed set, however, we are
only controlling on the original signals for the partial covariance algorithm. Due to
these differences, we developed this customized partial covariance algorithm (Eq. 8),
which performs well for neural signals in the form of Eq. 2.

2.2.3 Step 3: sparse latent regularization

Finally, we applied the sparse latent regularization method to partial covariance version
of the differential covariance (Chandrasekaran et al., 2011; Yatsenko et al., 2015). As
explained in appendix B.3, in the sparse latent regularization method, we made the as-
sumption that there are observed neurons and unobserved common inputs in a network.

8

If the connections between the observed neurons are sparse and the number of unob-
served common inputs is small, this method can separate the covariance into two parts
and the sparse matrix is the intrinsic connections between the observed neurons.

Here we deﬁne ∆S as the sparse result from the method, and L as the low-rank

result from the method.
Then by solving:

under the constraint that

arg min
∆S ,L

||∆S||1 + α ∗ tr(L)

∆P = ∆S + L

(9)

(10)

Where, || ||1 is the L1-norm of a matrix, and tr() is the trace of a matrix. α is the
N for all
penalty ratio between the L1-norm of ∆S and the trace of L. It was set to 1/
our estimations. ∆P is the partial differential covariance computed from section 2.2.2.
We receive a sparse estimation, ∆S, of the connectivity.

√

2.2.4 Computing derivative

In differential covariance, computing the derivative using dV (t) = (V (t + 1) − V (t −
1))/(2dt) provides better estimation results than using dV (t + 1) = (V (t + 1) −
V (t))/dt. To elaborate this point, we ﬁrst explain our method’s connection to the au-
toregressive (AR) method.

Detailed discussion of the multivariate autoregressive model and its estimators has
been discussed in Harrison et al. (2003). In discrete space, our differential covariance
estimator is similar to the mean squared error (MSE) estimator of the AR model. Fol-
lowing the deﬁnition in Eq. 2, we have:

C dV (t)

dt = G · V (t) + N

(11)

where, V (t) are neurons’ membrane voltages. G is the connection matrix that describes
the conductance between each pair of neurons. N is the Gaussian noise.

For dV (t) = (V (t + 1) − V (t − 1))/(2dt), we note here:

dt = C V (t+1)−V (t−1)

C dV (t)
V (t + 1) = 2∆t

= G · V (t) + N
C G · V (t) + V (t − 1) + 2∆t
C N

2∆t

(12)

As explained in Harrison et al. (2003), in AR model, the MSE estimator of G is
(V (t+1)−V (t−1))V (t)T
. We note that the numerator of this MSE estimator is our differen-
V (t)V (t)T
tial covariance estimator. Therefore, in this case, the model we proposed is equivalent
to a regularized AR(2) model, where the transition matrix of the 2nd order is restricted
to be an identity matrix.

In the dV (t) = (V (t + 1) − V (t))/dt case, we note that the differential covariance

estimator is similar to an AR(1) estimator:

dt = C V (t+1)−V (t)

C dV (t)
V (t + 1) = ( ∆t

C G + I) · V (t) + ∆t

= G · V (t) + N
C N

∆t

(13)

And the MSE estimator of ( ∆t

C G + I) is V (t+1)V (t)T

V (t)V (t)T . Therefore:

9

∆t
C

G =

=

V (t + 1)V (t)T

V (t)V (t)T − I
(V (t + 1) − V (t))V (t)T
V (t)V (t)T

(14)

where, the numerator of this MSE estimator is our differential covariance estimator.

As explained above, using different methods to compute the derivative will produce
different differential covariance estimators and they are equivalent to estimators from
different AR models for the connection matrix. In section 3, we show that the perfor-
mance of these two estimators are signiﬁcantly different.

2.3 Performance quantiﬁcation

The performance of each method is judged by 4 quantiﬁed values. The ﬁrst 3 values
indicate the method’s abilities to reduce the 3 types of false connections (Fig. 4). The
last one indicates the method’s ability to correctly estimate the true positive connections
against all possible interference.

Let’s deﬁne G as the ground truth connectivity matrix, where:

Gi,j =






1,
−1,
0,

if neuron i projects to neuron j with excitatory synapse
if neuron i projects to neuron j with inhibitory synapse
otherwise

(15)

Then, we can use a 3-dimensional tensor to represent the false connections caused
by common inputs. For example, neuron j and neuron k receive common input from
neuron i:

(cid:40)

Mi,j,k =

iff Gi,j = 1 and Gi,k = 1

1,
0, otherwise

Therefore, we can compute a mask that labels all the type 1 false connections:

mask1j,k =

(cid:88)

Mi,j,k

i∈{observed neurons}

For the type 2 false connections (e.g. neuron i projects to neuron k, then neuron k

projects to neuron j), the mask is deﬁned as:

mask2i,j =

(cid:88)

Gi,kGk,j

k∈{observed neurons}

or, in simple matrix notation:

mask2 = G · G

(18)

(19)

Similar to mask1, the false connections caused by unobserved common inputs is:

mask3j,k =

(cid:88)

Mi,j,k

i∈{unobserved neurons}

(20)

10

(16)

(17)

Lastly, mask4 is deﬁned as:

mask4i,j =

(cid:40)

1,
if Gi,j = 0
0, otherwise

(21)

Given a connectivity matrix estimation result: Est, the 4 values for the performance
the true

quantiﬁcation are computed as the area under the ROC curve for two sets:
positive set and the false positive set.

|Esti,j| ∈ {true positive set}k, if Gi,j (cid:54)= 0 and maskki,j = 0
|Esti,j| ∈ {false positive set}k, if maskki,j (cid:54)= 0 and Gi,j = 0

(22)

3 Results

3.1 False connections in correlation-based methods

When applied to neural circuits, the commonly used correlation-based methods produce
systematic false connections. As shown, Fig. 3 A is the ground truth of the connections
in our passive neuron model (Neurons 1-50 are the observed neurons). Fig. 3B is from
the correlation method, Fig. 3C is the precision matrix, and Fig. 3D is the sparse+latent
regularized precision matrix. As shown, all of these methods produce extra false con-
nections.

For the convenience of explanation, we deﬁne the diagonal strip of connections in
the ground truth (ﬁrst 50 neurons in Fig. 3A) as the -3 and -4 diagonal lines, because
they are 3 and 4 steps away from the diagonal line of the matrix. As shown in Fig. 3, all
these methods produce false connections on the ±1 diagonal lines. The precision matrix
method (Fig. 3C) also has square box shape false connections in the background.

3.1.1 Type 1 false connections

Shown in Fig. 4A, the type 1 false connections are produced because two neurons re-
ceive the same input from another neuron. The same synaptic current that passes into
the two neurons generates positive correlation between the two neurons. However, there
is no physiological connection between these two neurons. In our connection pattern
(Fig. 3 A), we notice that two neurons next to each other receive common synaptic
inputs, therefore there are false connections on the ±1 diagonal lines of the correlation-
based estimations.

3.1.2 Type 2 false connections

Shown in Fig. 4B, the type 2 false connections are due to the propagation of correlation.
Because one neuron VA connects to another neuron VB and neuron VB connects to
another neuron VC, the correlation method presents correlation between VA and VC,
which do not have a physical connection. This phenomenon is shown in Fig. 3B as the
extra diagonal strips. Shown in Fig. 3C, this problem can be greatly reduced by the
precision matrix/partial covariance method.

11

Figure 3: Ground truth connections from a passive neuron model and estimations from
correlation-based methods. A) Ground truth connection matrix. neurons 1-50 are ob-
served neurons. neurons 51-60 are unobserved neurons that introduce common inputs.
B) Estimation from the correlation method. C) Estimation from the precision matrix.
D) Estimation from the sparse+latent regularized precision matrix. E) Zoom in of panel
A. F) Zoom in of panel B. G) Zoom in of panel C. H) Zoom in of panel D.

12

A: Ground Truth  204060102030405060−1−0.500.51B: COV  10203040501020304050−505x 10−4C: Precision Matrix  10203040501020304050−200002000D: SL−reg Precision Matrix  10203040501020304050−2000−1000010002000E: Ground Truth zoom in  253035253035−1−0.500.51F: COV zoom in  253035253035−505x 10−4G: Precision Matrix zoom in  253035253035−200002000H: SL−reg zoom in  253035253035−2000−1000010002000Figure 4: Illustrations of the 3 types of false connections in the correlation-based meth-
ods. Solid lines indicate the physical wiring between neurons, and the black circles at
the end indicate the synaptic contacts (i.e. the direction of the connections). The dotted
lines are the false connections introduced by the correlation-based methods. A) Type 1
false connections, which are due to two neurons receiving the same synaptic inputs. B)
Type 2 false connections, which are due to the propagation of correlation. C) Type 3
false connections, which are due to unobserved common inputs.

13

VBVCVAAType 1VBVCVABType 2VAVBInvisibleVisibleCVlatentType 33.1.3 Type 3 false connections

Shown in Fig. 4C, the type 3 false connections are also due to the common currents
pass into two neurons. However, in this case, they are from the unobserved neurons.
For this particular passive neuron model, it is due to the inputs from the 10 unobserved
neurons (Neurons 51-60) as shown on Fig. 3A. Because the latent neurons have broad
connections to the observed neurons, they introduce a square box shape correlation
pattern into the estimations (Fig. 3C. Fig. 3B also contains this error, but it is hard to
see). Since, the latent neurons are not observable, partial covariance method cannot
be used to regress out this type of correlation. On the other hand, the sparse latent
regularization can be applied if the sparse and low-rank assumption is valid, and the
sparse+latent regularized result is shown in Fig. 3D. However, even after using this
regularization, the correlation-based methods still leave false connections in Fig. 3D.

3.2 Estimations from differential covariance-based methods

Table 2: Performance quantiﬁcation (area under the ROC curve) of different methods
with respect to their abilities to reduce the 3 types of false connections and their abil-
ities to estimate the true positive connections under 5 different passive neuron model
settings.

Cov

Precision Precision+SL-reg

∆C

∆P

∆S

cxcx34 g5
Error 1
Error 2
Error 3
True Positive
cxcx34 g30
Error 1
Error 2
Error 3
True Positive
cxcx34 g50
Error 1
Error 2
Error 3
True Positive
cxcx56789 g5
Error 1
Error 2
Error 3
True Positive
cxcx56789 g50
Error 1
Error 2
Error 3
True Positive

0
0.1469
0.4638
0.7312

0
0.0056
0.2164
0.5591

0
0
0.3179
0.9140

0
0.1896
0.3573
0.6884

0
0.1083
0.4256
0.9256

0
0.9520
0.9362
1.0000

0
0.8927
0.9188
1.0000

0
0.7034
0.8000
1.0000

0.0053
0.6229
0.6085
0.9442

0
0.5312
0.4927
0.9116

0
0.9915
0.9797
1.0000

0
0.9972
0.9942
0.9892

0
0.9944
0.9894
0.9946

0.0053
0.7896
0.7659
0.6674

0
0.8240
0.7762
0.6698

0.6327
0.3757
0.6541
0.9677

0.0510
0.2881
0.5430
0.6559

0
0.0523
0.4145
0.9516

0.6895
0.5240
0.6957
0.9930

0.0263
0.2844
0.5091
0.9395

0.3469
0.8347
0.8391
0.9946

0.5816
0.9548
0.9662
1.0000

0.2041
0.9054
0.9309
0.9785

0.6263
0.7896
0.7591
0.9605

0.2816
0.6990
0.7116
0.9279

0.8776
1.0000
0.9986
1.0000

0.9490
1.0000
1.0000
1.0000

0.6531
1.0000
1.0000
1.0000

0.8526
0.9938
0.9817
0.9837

0.6842
0.9979
0.9835
0.9419

Comparing the ground truth connections in Fig. 5A with our ﬁnal estimation in
Fig. 5D, we see that our methods essentially transformed the connections in the ground
truth into a map of sources and sinks in a network. An excitatory connection, i → j, in
our estimations have negative value for ∆Sij and positive value for ∆Sji, which means

14

Figure 5: differential covariance analysis of the passive neuron model. The color in
B,C,D,F,G,H indicates direction of the connections. For element Aij, warm color indi-
cates i is the sink, j is the source, i.e. i ← j, and cool color indicates j is the sink, i
is the source, i.e. i → j. A) Ground truth connection matrix. B) Estimation from the
differential covariance method. C) Estimation from the partial differential covariance
method. D) Estimation from the sparse+latent regularized partial differential covari-
ance method. E) Zoom in of panel A. F) Zoom in of panel B. G) Zoom in of panel C.
H) Zoom in of panel D.

15

A: Ground Truth  204060102030405060−1−0.500.51B: ∆C  10203040501020304050−505x 10−6C: ∆P  10203040501020304050−0.0500.05D: ∆S  10203040501020304050−0.0500.05E: Ground Truth zoom in  253035253035−1−0.500.51F: ∆C zoom in  253035253035−505x 10−6G: ∆P zoom in  253035253035−0.0500.05H: ∆S zoom in  253035253035−0.0500.05Figure 6: Performance quantiﬁcation (area under the ROC curve) of different methods
with respect to their abilities to reduce the 3 types of false connections and their abilities
to estimate the true positive connections using the passive neuron dataset.

16

Data samples ( log10 )246Area under ROC00.10.20.30.40.50.60.70.80.91Error type 1Data samples ( log10 )24600.10.20.30.40.50.60.70.80.91Error type 2Data samples ( log10 )24600.10.20.30.40.50.60.70.80.91Error type 3Data samples ( log10 )24600.10.20.30.40.50.60.70.80.91True positiveCovPrecisionPrecision+SLreg∆C∆P∆Sthe current is coming out of the source i, and goes into the sink j. We note that there
is another ambiguous case, an inhibitory connection j → i, which produces the same
results in our estimations. Our methods can not differentiate these two cases, instead,
they indicate sources and sinks in a network.

3.2.1 The differential covariance method reduces type 1 false connections

By comparing Fig. 3 B with Fig. 5 B, we see that the type 1 false connections on the
±1 diagonal lines of Fig. 3 B is reduced in Fig. 5 B. This is reﬂecting the theorems
we proved in appendix A, in particular theorem 5, which shows that the strength of the
type 1 false connections is reduced in the differential covariance method by a factor of
gl/gsyn. Moreover, the differential covariance method’s performance on reducing the
type 1 false connections is quantiﬁed in Fig. 6.

3.2.2 The partial covariance method reduces type 2 false connections

Second, we see that, due to the propagation of correlation, there are extra diagonal strips
in Fig. 5B. These are removed in Fig. 5C by applying the partial covariance method.
And each estimator’s performance for reducing type 2 false connections is quantiﬁed in
Fig. 6.

3.2.3 The sparse+latent regularization reduces type 3 false connections

Third, the sparse+latent regularization to remove the correlation introduced by the latent
inputs. As mentioned in the method section, when the observed neurons’ connections
are sparse and the number of unobserved common inputs is small, the covariance in-
troduced by the unobserved common inputs can be removed. As shown in Fig. 5D, the
external covariance in the background of Fig. 5C is removed, while the true diagonal
connections and the directionality of the connections are maintained. This regulariza-
tion is also effective for correlation-based methods, but type 1 false connections main-
tain in the estimation even after applying this regularization (Fig. 3D). Each estimator’s
performance for reducing type 3 false connections is quantiﬁed in Fig. 6.

3.2.4 The differential covariance-based methods provide directionality informa-

tion of the connections

Using this passive neuron model, in appendix A.3, we provide a mathematical explana-
tion for why the differential covariance-based methods provide directional information
for the connections. Given an excitatory connection gi→j (neuron i projects to neuron
j), from Theorem 6 in appendix A.3, we have:

∆Cj,i > 0
∆Ci,j < 0

(23)

However, we wish to note here that, there is another ambiguous setting that provides
the same result, which is an inhibitory connection gj→i. Conceptually, the differential
covariance indicates the current sources and sinks in a neural circuit, but the exact type
of synapse is unknown.

17

3.2.5 Performance quantiﬁcation for the passive neuron dataset

In Figure 6, we quantiﬁed the performance of the estimators for one example dataset.
We see that, with the increase of the sample size, our differential covariance-based
methods reduce the 3 types of false connections, while maintaining high true positive
rates. Also as we apply more advanced techniques (∆C → ∆P → ∆S), the estimator’s
performance increases in all 4 panels of the quantiﬁcation indices. Although the pre-
cision matrix and the sparse+latent regularization help the correlation method reduce
the type 2 and type 3 error, all correlation-based methods handle poorly of the type 1
false connections. We also note that the masks we used to quantify each type of false
connections are not mutually exclusive (i.e. there are false connections that belong to
more than one type of false connections). Therefore, in Figure 6, it seems like a regular-
ization is reducing multiple types of false connections. For example, the sparse+latent
regularization is reducing both type 2 and type 3 false connections.

In Table 2, we provide quantiﬁed results (area under the ROC curve) for two dif-
ferent connection patterns (cxcx34, and cxcx56789) and three different conductance
settings (g5, g30, and g50). We see that, the key results in Fig. 6 are also generalized
here. By applying more advanced techniques to the original differential covariance es-
timator (∆C → ∆P → ∆S, the performance increases with respect to the 3 types of
error, while the true positive rate is not sacriﬁced. We also note that, although the preci-
sion matrix and the sparse+latent regularization help the correlation method reduce the
type 2 and the type 3 error, all correlation-based methods handle poorly of the type 1
error.

3.3 Thalamocortical model results

We further tested the methods in a more realistic Hodgkin-Huxley based model. Be-
cause the synaptic conductances in the Hodgkin-Huxley model are no longer constants
but become nonlinear dynamic functions, which depend on the pre-synaptic voltages,
the derivations above can only be considered as a ﬁrst-order approximation.

Shown in Fig. 7A is the ground truth connections between the cortical neurons.
These cortical neurons also receive latent inputs from and sending feedback currents to
inhibitory neurons in the cortex (IN) and thalamic neurons (TC). For clarity of repre-
sentation, these latent connections are not shown here, but the detailed connections are
described in the Method section.

Similar to the passive neuron model, in Fig. 7B, the correlation method still suffers
from those 3 types of false connections. As shown, the latent inputs generate false
correlations in the background. And the ±1 diagonal line false connections, which are
due to the common currents, exist in all correlation-based methods (see Fig. 7B,C,D).
Comparing Fig. 7C, D, because the type 1 false connections are strong in the Hodgkin-
Huxley based model, the sparse+latent regularization removed the true connections but
kept these false connections in its ﬁnal estimation.

As shown in Fig. 8B, differential covariance method reduces the type 1 false con-
nections. Then in Fig. 8C, the partial differential covariance method reduces type 2
false connections in Fig. 8B (yellow color connections around the red strip in Fig. 8B).
Finally, in Fig. 8D, the sparse latent regularization removes the external covariance in

18

Figure 7: Analysis of the thalamocortical model with correlation-based methods. A)
Ground truth connections of the PY neurons in the thalamocortical model. B) Estima-
tion from the correlation method. C) Estimation from the precision matrix method. D)
Estimation from the sparse+latent regularized precision matrix method. E) Zoom in of
panel A. F) Zoom in of panel B. G) Zoom in of panel C. H) Zoom in of panel D.

19

A: Ground Truth  10203040501020304050−1−0.500.51B: COV  10203040501020304050−0.200.2C: Precision Matrix  10203040501020304050−0.200.2D: SL−reg Precision Matrix  10203040501020304050−0.100.1E: Ground Truth zoom in  253035253035−1−0.500.51F: COV zoom in  253035253035−0.200.2G: Precision Matrix zoom in  253035253035−0.200.2H: SL−reg Precision Matrix zoom in  253035253035−0.100.1Figure 8: Analysis of the thalamocortical model with differential covariance-based
methods. The color in B,C,D,F,G,H indicates direction of the connections. For ele-
ment Aij, warm color indicates i is the sink, j is the source, i.e. i ← j, and cool color
indicates j is the sink, i is the source, i.e. i → j. A) Ground truth connection matrix.
B) Estimation from the differential covariance method. C) Estimation from the partial
differential covariance method. D) Estimation from the sparse+latent regularized par-
tial differential covariance method. E) Zoom in of panel A. F) Zoom in of panel B. G)
Zoom in of panel C. H) Zoom in of panel D.

20

A: Ground Truth  10203040501020304050−1−0.500.51B: ∆C  10203040501020304050−0.0200.02C: ∆P  10203040501020304050−0.02−0.0100.010.02D: ∆S  10203040501020304050−0.02−0.0100.010.02E: Ground Truth zoom in  253035253035−1−0.500.51F: ∆C zoom in  253035253035−0.0200.02G: ∆P zoom in  253035253035−0.02−0.0100.010.02H: ∆S zoom in  253035253035−0.02−0.0100.010.02Figure 9: Performance quantiﬁcation (area under the ROC curve) of different methods
with respect to their abilities to reduce the 3 types of false connections and their abilities
to estimate the true positive connections using the thalamocortical dataset.

21

Data samples ( log10 )456Area under ROC00.10.20.30.40.50.60.70.80.91Error type 1Data samples ( log10 )45600.10.20.30.40.50.60.70.80.91Error type 2Data samples ( log10 )45600.10.20.30.40.50.60.70.80.91Error type 3Data samples ( log10 )45600.10.20.30.40.50.60.70.80.91True positiveCovPrecisionPrecision+SLregGLMAR(1)∆C∆P∆Sthe background of Fig. 8C. The current sources (positive value, red color) and current
sinks (negative value, blue color) in the network are also indicated on our estimators.

In Fig. 9, each estimator’s performance on each type of false connections is quanti-
ﬁed. In this example, our differential covariance-based methods achieve similar perfor-
mance to the GLM method.

3.4 Simulated LFP results

For population recordings, our methods have similar performance to the thalamocortical
model example. While the correlation-based methods still suffering from the problem of
type 1 false connections (Fig. 10), our differential covariance-based methods can reduce
all 3 types of false connections (Fig. 11). In Fig. 12, each estimator’s performance on
LFP data is quantiﬁed. In this example, with sufﬁcient data samples, our differential
covariance-based methods achieve similar performance to the GLM method. However,
for smaller sample sizes, our new methods perform better than the GLM method.

3.5 Simulated calcium imaging results

Lastly, because current techniques only allow recording of a small percentage of neu-
rons in the brain, we tested our methods on a calcium imaging dataset of 50 neurons
recorded from 1000 neurons networks. In this example, our differential covariance-
based methods (Fig. 14) match better with the ground truth than the correlation-based
methods (Fig. 13).

In Fig. 15, We performed 25 sets of recordings with 50 neurons randomly selected
in each of the 4 large networks and quantiﬁed the results. The markers on the plots
are the average area under the ROC curve values across the 100 sets, and the error bars
indicate the standard deviations across these 100 sets of recordings. Our differential
covariance-based methods perform better than the GLM method, and the performance
differences seem to be greater in situations with fewer data samples.

22

Figure 10: Analysis of the simulated LFP data with correlation-based methods. A)
Ground truth connection matrix B) Estimation from the correlation method. C) z-score
of the correlation matrix. D) Estimation from the precision matrix method. E) Estima-
tion from the sparse+latent regularized precision matrix method. F) Zoom in of panel
A. G) Zoom in of panel B. H) Zoom in of panel C. I) Zoom in of panel D. J) Zoom in
of panel E.

23

A: Ground Truth  10203040501020304050−1−0.500.51B: COV  10203040501020304050−0.500.5C: COV zscore  10203040501020304050−2−1012D: Precision Matrix  10203040501020304050−0.500.5E: SL−reg Precision Matrix  10203040501020304050−101F: Ground Truth zoom in  253035253035−1−0.500.51G: COV zoom in  253035253035−0.500.5H: COV zscore zoom in  253035253035−2−1012I: Precision Matrix zoom in  253035253035−0.500.5J: SL−reg Precision Matrix zoom in  253035253035−101Figure 11: Analysis of the simulated LFP data with differential covariance-based meth-
ods. The color in B,C,D,F,G,H indicates direction of the connections. For element Aij,
warm color indicates i is the sink, j is the source, i.e. i ← j, and cool color indicates j
is the sink, i is the source, i.e. i → j. A) Ground truth connection matrix. B) Estimation
from the differential covariance method. C) Estimation from the partial differential co-
variance method. D) Estimation from the sparse+latent regularized partial differential
covariance method. E) Zoom in of panel A. F) Zoom in of panel B. G) Zoom in of panel
C. H) Zoom in of panel D.

24

A: Ground Truth  10203040501020304050−1−0.500.51B: ∆C  10203040501020304050−0.02−0.0100.010.02C: ∆P  10203040501020304050−0.02−0.0100.010.02D: ∆S  10203040501020304050−0.0100.01E: Ground Truth zoom in  253035253035−1−0.500.51F: ∆C zoom in  253035253035−0.02−0.0100.010.02G: ∆P zoom in  253035253035−0.02−0.0100.010.02H: ∆S zoom in  253035253035−0.0100.01Figure 12: Performance quantiﬁcation (area under the ROC curve) of different methods
with respect to their abilities to reduce the 3 types of false connections and their abilities
to estimate the true positive connections using the simulated LFP dataset.

25

Data samples ( log10 )246Area under ROC00.10.20.30.40.50.60.70.80.91Error type 1Data samples ( log10 )24600.10.20.30.40.50.60.70.80.91Error type 2Data samples ( log10 )24600.10.20.30.40.50.60.70.80.91Error type 3Data samples ( log10 )24600.10.20.30.40.50.60.70.80.91True positiveCovPrecisionPrecision+SLregGLMAR(1)∆C∆P∆SFigure 13: Analysis of the simulated calcium imaging dataset with correlation-based
methods. A) Ground truth connection matrix B) Estimation from the correlation
method. C) Estimation from the precision matrix method. D) Estimation from the
sparse+latent regularized precision matrix method. For clarity purpose, panel B,C,D
are thresholded to show only the most strong connections, so one can compare it with
the ground truth.

26

A: Ground Truth  20401020304050−1−0.500.51B: COV  20401020304050−0.0200.02C: Precision Matrix  20401020304050−0.0200.02D: SL−reg Precision Matrix  20401020304050−0.02−0.0100.010.02Figure 14: Analysis of the simulated calcium imaging dataset with differential
covariance-based methods. The color in B,C,D indicates direction of the connections.
For element Aij, warm color indicates i is the sink, j is the source, i.e. i ← j, and
cool color indicates j is the sink, i is the source, i.e. i → j. A) Ground truth con-
nection matrix. B) Estimation from the differential covariance method. C) Estimation
from the partial differential covariance method. D) Estimation from the sparse+latent
regularized partial differential covariance method. For clarity purpose, panel B,C,D are
thresholded to show only the most strong connections, so one can compare it with the
ground truth.

27

A: Ground Truth  20401020304050−1−0.500.51B: ∆C  20401020304050−505x 10−3C: ∆P  20401020304050−505x 10−3D: ∆S  20401020304050−505x 10−3Figure 15: Performance quantiﬁcation (area under the ROC curve) of different methods
with respect to their abilities to reduce the 3 types of false connections and their abilities
to estimate the true positive connections using the simulated calcium imaging dataset.
Error bar is the standard deviation across 100 sets of experiments. Each experiment
randomly recorded 50 neurons in a large network. The markers on the plots indicate the
average area under the ROC curve values across the 100 sets of experiments.

28

Data samples ( log10 )2345Area under ROC00.10.20.30.40.50.60.70.80.91Error type 1Data samples ( log10 )234500.10.20.30.40.50.60.70.80.91Error type 2Data samples ( log10 )234500.10.20.30.40.50.60.70.80.91Error type 3Data samples ( log10 )234500.10.20.30.40.50.60.70.80.91True positiveCovPrecisionICOVGLMAR(1)∆C∆P∆S4 Discussion

4.1 Generalizability and applicability of the differential covariance-

based methods to real experimental data

Many methods have been proposed to solve the problem of reconstructing the con-
nectivity of a neural network. Winterhalder et al. (2005) reviewed the non-parametric
methods and Granger causality based methods. Many progress have been made recently
using kinetic Ising model and Hopﬁeld network (Huang, 2013; Dunn and Roudi, 2013;
Battistin et al., 2015; Capone et al., 2015; Roudi and Hertz, 2011) with sparsity regu-
larization (Pernice and Rotter, 2013). The GLM method (Okatan et al., 2005; Truccolo
et al., 2005; Pillow et al., 2008) and the maximum entropy method (Schneidman et al.,
2006) are two popular classes of methods, which are the main modern approaches for
modeling multi-unit recordings (Roudi et al., 2015).

In the trend of current research, people are recording more and more neurons and
looking for new data analysis techniques to handle bigger data with higher dimension-
ality. The ﬁeld is in favor of algorithms that require fewer samples and scale well with
dimensionality, but at the same time not sacriﬁcing the accuracy. Also, an algorithm
that is model free or make minimum assumptions about the hidden structure of the data
has the potential to be applied to multiple types of neural recordings.

The key difference between our methods and other methods is that we use the re-
lationship between a neuron’s differential voltage and a neuron’s voltage rather than
ﬁnding the relationship between voltages. This provides better performance because
the differential voltage is a proxy of a neuron’s synaptic current. And the relationship
between a neuron’s synaptic current and its input voltages is more linear, which is suit-
able for data analysis techniques like the covariance method. While this linear relation-
ship hold only for our passive neuron model, we still see similar or better performance
of our methods in our Hodgkin-Huxley model based examples, where we relaxed this
assumption and allowed loops in the networks. This implies that this class of methods
are still applicable even when the ion channels’ conductances vary non-linearly with
the voltages, which makes the linear relationship only weakly holds.

4.2 Caveats and future directions

One open question for the differential covariance-based methods is how to improve the
way they handle neural signals that are non-linearly transformed from the intracellular
voltages. Currently, to achieve good performance in the calcium imaging example, we
need to assume knowing the exact transfer function and reversely reconstruct the action
potentials. We ﬁnd this reverse transform method to be prone to additive Gaussian
noise. Further study is needed to ﬁnd better way to preprocess calcium imaging data
for the differential covariance-based methods.

Throughout our simulations, the differential covariance method has better perfor-
mance and need fewer data samples in some simulations, but not in other simulations.
Further investigation is needed to understand where that improvement comes from.

Our Hodgkin-Huxley simulations did not include axonal or synaptic delays, which
is a critical feature of a real neural circuit. Unfortunately, it is non-trivial to add this

29

feature to our Hodgkin-Huxley model. Nevertheless, we tested our methods with the
passive neuron model using the same connection patterns but with random synaptic
delays between neurons. In appendix D, we show that for up to a 10 ms uniformly
distributed synaptic delay pattern, our methods still outperform the correlation-based
methods.

Acknowledgement

We would like to thank Dr. Thomas Liu, and all members of the Computational Neuro-
biology Lab for providing helpful feedback. This research is supported by ONR MURI
(N000141310672), Ofﬁce of Naval Research, MURI N00014-13-1-0205, Swartz Foun-
dation and Howard Hughes Medical Institute.

Appendix

A Differential covariance derivations

In this section, we ﬁrst build a simple 3-neuron network to demonstrate that our dif-
ferential covariance-based methods can reduce the type 1 false connections. Then we
develop a generalized theory, which shows that the type 1 false connections’ strength is
always lower in our differential covariance-based methods than the original correlation-
based methods.

A.1 A 3-neuron network

Let us assume a network of 3 neurons, where neuron A projects to neuron B and C:

IA = dVA/dt = glVA + NA
IB = dVB/dt = g1VA + glVB + NB
IC = dVC/dt = g2VA + glVC + NC

(24)

Here, the cell conductance is gl, neuron A’s synaptic connection strength to neuron B
is g1, and neuron A’s synaptic connection strength to neuron C is g2. NA, NB, NC are
independent white Gaussian noises.

From Eq.18 of Fan et al. (2011), we can derive the covariance matrix of this net-

work:

Where,

vec(COV ) = −(G ⊗ In + In ⊗ G)−1(D ⊗ D)vec(Im)

G =





gl g1 g2
0
gl
0
gl
0
0


T



is the transpose of the ground truth connection of the network. And,

D =









1 0 0
0 1 0
0 0 1

30

(25)

(26)

(27)

since each neuron receives independent noise. In is an identity matrix of the size of G
and Im is an identity matrix of the size of D. ⊗ is the Kronecker product and vec() is
the column vectorization function.

Therefore, we have the covariance matrix of the network as:





COV =

−1/(2 ∗ gl)
l ) −(g2
g1/(4 ∗ g2
g2/(4 ∗ g2
l )

g1/(4 ∗ g2
l )
l )/(4 ∗ g3
l )
−(g1 ∗ g2)/(4 ∗ g3
l )

1 + 2 ∗ g2

g2/(4 ∗ g2
l )
−(g1 ∗ g2)/(4 ∗ g3
l )
l )/(4 ∗ g3
l )

2 + 2 ∗ g2

−(g2



 (28)

When computing the differential covariance, we plug in Eq. 24. For example:

COV (IC, VB) = g2COV (VA, VB) + glCOV (VC, VB)

(29)

Therefore, from Eq. 28, we can compute the differential covariance as:

∆P =





−1/2
−g1/(4 ∗ gl)
−g2/(4 ∗ gl)

g1/(4 ∗ gl) g2/(4 ∗ gl)

−1/2
0

0
−1/2





(30)

Notice that, because the ratio between COV (VA, VB) and COV (VC, VB) is −gl/g2,

in differential covariance, the type 1 false connection COV (IC, VB) has value 0.

A.2 Type 1 false connection’s strength is reduced in differential co-

variance

In this section, we propose a theory. Given a network, which consists of passive neurons
in the following form:

Ii(t) = C

dVi(t)
dt

=

(cid:88)

k∈{prei}

gk→iVk(t) + glVi(t) + dBi(t)

(31)

Where {prei} is the set of neurons that project to neuron i, gk→i is the synaptic conduc-
tance for the projection from neuron k to neuron i. Bi(t) is a Brownian motion.

And further assume that:

• All neurons’ leakage conductance gl and membrane capacitance C are constants

and the same.

• There is no loop in the network.

• gsyn << gl, where gsyn is the maximum of |gi→j|, for ∀i, j

Then, we prove below that:

• For two neurons that have physical connection, their covariance is O( gsyn
g2
l

).

• For two neurons that do not have physical connection, their covariance is O( g2
syn
g3
l

).

• For two neurons that have physical connection, their differential covariance is

O( gsyn
gl

).

31

• For two neurons that do not have physical connection, their differential covariance

is O( g3
syn
g3
l

).

• The type 1 false connection’s strength is reduced in differential covariance.

Lemma 1 The asymptotic auto-covariance of a neuron is

Cov[Vi, Vi] = −(1 + 2

(cid:88)

k∈{prei}

gk→iCov[Vk, Vi])/2gl

(32)

Proof From Eq.9 of Fan et al. (2011), we have,

d(Vi − E[Vi]) =

(cid:88)

k∈{prei}

gk→i(Vk − E[Vk])dt + gl(Vi − E[Vi]) + dBi(t)

(33)

Where E[] is the expectation operation. (t) is dropped from Vi(t) when the meaning is
unambiguous.

From Theorem 2 of Fan et al. (2011), integrating by parts using Itˆo calculus gives

d((Vi − E[Vi])(Vi − E[Vi])) = d(Vi − E[Vi]) · (Vi − E[Vi])
+(Vi − E[Vi]) · d(Vi − E[Vi]) + d[Vi − E[Vi], Vi − E[Vi]]

(34)

Taking the expectation of both sides with Eq. 33 gives

dCov[Vi, Vi] = 2(

(cid:88)

k∈{prei}

gk→iCov[Vk, Vi] + glCov[Vi, Vi])dt + dE[[Bi(t), Bi(t)]] (35)

when t → +∞, Eq. 35 becomes

0 = 2( (cid:80)
Cov[Vi(+∞), Vi(+∞)] = −(1 + 2 (cid:80)

k∈{prei}

gk→iCov[Vk(+∞), Vi(+∞)] + glCov[Vi(+∞), Vi(+∞)]) + 1

gk→iCov[Vk(+∞), Vi(+∞)])/2gl

(36)

Lemma 2 The asymptotic covariance between two neurons is

k∈{prei}

Cov[Vi, Vj] = −(

(cid:88)

gk→iCov[Vk, Vj] +

k∈{prei}

(cid:88)

k∈{prej }

gk→jCov[Vk, Vi])/2gl

(37)

Proof From Eq.9 of Fan et al. (2011), we have,

d(Vi − E[Vi]) =

(cid:88)

k∈{prei}

gk→i(Vk − E[Vk])dt + gl(Vi − E[Vi]) + dBi(t)

(38)

From Theorem 2 of Fan et al. (2011), integrating by parts using Itˆo calculus gives

d((Vi − E[Vi])(Vj − E[Vj])) = d(Vi − E[Vi]) · (Vj − E[Vj])
+(Vj − E[Vj]) · d(Vi − E[Vi]) + d[Vi − E[Vi], Vj − E[Vj]]

(39)

32

Taking the expectation of both sides with Eq. 38 gives

dCov[Vi, Vi] = ( (cid:80)

gk→iCov[Vk, Vi] + (cid:80)

gk→jCov[Vk, Vj]

k∈{prei}
+2glCov[Vi, Vj])dt + dE[[Bi(t), Bj(t)]]

k∈{prej }

(40)

when t → +∞, Eq. 40 becomes

(cid:88)

0 =

gk→iCov[Vk(+∞), Vi(+∞)]

k∈{prei}

(cid:88)

+

k∈{prej }

gk→jCov[Vk(+∞), Vj(+∞)]

Cov[Vi(+∞), Vj(+∞)] = −(

(cid:88)

gk→iCov[Vk(+∞), Vi(+∞)

+ 2glCov[Vi(+∞), Vj(+∞)]

(41)

k∈{prei}
(cid:88)

+

k∈{prej }

gk→jCov[Vk(+∞), Vj(+∞)])/2gl

Theorem 1 The auto-covariance of a neuron is O( 1
gl
neurons with or without physical connection is O( gsyn
g2
l

). The covariance of two different
).

Proof We prove this by induction.

The basis: The base case contains two neurons:

From Lemma 1, we have:

C dV1
C dV2

dt = glV1 + N1
dt = g1→2V1 + glV2 + N2

Cov[V1, V1] = −1/2gl

Then, from Lemma 2, we have:

Cov[V1, V2] = −g1→2Cov[V1, V1]/2gl
Cov[V1, V2] = g1→2/4g2
l

And, from Lemma 1, we have:

Cov[V2, V2] = −(1 + 2g1→2Cov[V1, V2])/2gl
Cov[V1, V2] = −1/2gl − g2

1→2/4g3
l

So the statement holds.

(42)

(43)

(44)

(45)

The inductive step:
one more neuron to it.

If the statement holds for a network of n − 1 neurons, we add

33

Part 1: First, let’s prove the covariance of any neuron with n is also O( gsyn
g2
l

From Lemma 2, we have:

).

Cov[Vi, Vn] = −(

(cid:88)

gk→iCov[Vk, Vn] +

k∈{prei}

(cid:88)

k∈{pren}

gk→nCov[Vk, Vi])/2gl

(46)

where, {prei} are the neurons projecting to neuron i, {pren} are the neurons projecting
to neuron n.

Note that, because {pren} are neurons from the old network, Cov[Vk, Vi], k ∈

{pren} is at most O( 1
gl

), and it is O( 1
gl
Now, we need to prove that Cov[Vk, Vn], k ∈ {prei} is also O( 1
gl

) only when k = i.

by contradiction. Let’s suppose that Cov[Vk, Vn], k ∈ {prei} is larger than O( 1
gl
similar to Eq. 46, we have:

). We prove this
). Then

For p ∈ {prei}

Cov[Vp, Vn] = −(

(cid:88)

k∈{prep}

gk→pCov[Vk, Vn] +

(cid:88)

k∈{pren}

gk→nCov[Vk, Vp])/2gl

(47)

Here we separate the problem into two situations,

Case 1: neuron i projects to neuron n Since there is no loop in the network,
n /∈ {prei}. Therefore, Cov[Vk, Vp], k ∈ {pren}, p ∈ {prei} is the covariance of two
neurons from the old network and is O( 1
). Cov[Vk, Vn], k ∈ {prep} must be larger
gl
than O( 1
).
gl

), such that Cov[Vp, Vn], p ∈ {prei} is larger than O( 1
gl

Therefore, if a neuron’s covariance with neuron n is larger than O( 1
), one of its
gl
antecedents’ covariance with neuron n is also larger than O( 1
). Since we assume there
gl
is no loop in this network, there must be at least one antecedent (say, neuron m) whose
covariance with neuron n is larger than O( 1
gl

) and it has no antecedent.

However, from Lemma 2:

Cov[Vm, Vn] = −(

(cid:88)

gk→nCov[Vk, Vm])/2gl

(48)

k∈{pren}

). This is a contradiction. So Cov[Vk, Vn], k ∈ {prei} is no larger than O( 1
gl

), Cov[Vm, Vn] is O( gsyn
g2
l

), which is smaller
).

Since Cov[Vm, Vk]), k ∈ {pren} is O( 1
gl
than O( 1
gl
Therefore, Cov[Vi, Vn] is O( gsyn
g2
l

).

Case 2: neuron i does not project to neuron n Now, in Eq. 47, it is possible
that n ∈ {prei}. However, in case 1, we just proved that the covariance of any neuron
that projects to neuron n is O( gsyn
). Therefore, Cov[Vk, Vp], k ∈ {pren}, p ∈ {prei} is
g2
l
O( gsyn
g2
l
Then, similar to case 1, there must be an antecedent of neuron i (say neuron m),
) and it has no antecedent. Then
).

whose covariance with neuron n is larger than O( 1
gl
from Eq. 48 we know this is a contradiction. So, Cov[Vi, Vn] is O( gsyn
g2
l

) regardless of whether p = n.

34

Part 2: Then, for the auto-covariance of neuron n:

Cov[Vn, Vn] = −(1 + 2

(cid:88)

k∈{pren}

gk→nCov[Vk, Vn])/2gl

(49)

As we already proved that any neuron’s covariance with neuron n is O( gsyn
), the
g2
l
dominant term in Eq. 49 is −1/2gl. Therefore, the auto-covariance of neuron n is also
O( 1
).
gl
End of proof.

Theorem 2 The covariance of two neurons that are physically connected is O( gsyn
g2
l
The covariance of two neurons that are not physically connected is O( g2
syn
g3
l

).

).

Proof From Lemma 2, we have:

Cov[Vi, Vj] = −(

(cid:88)

gk→iCov[Vk, Vj] +

k∈{prei}

(cid:88)

k∈{prej }

gk→jCov[Vk, Vi])/2gl

(50)

If neuron i and neuron j are physically connected, let’s say i → j, then i ∈ {prej}.
). Since there is
). Therefore, Cov[Vi, Vj]

Thus one of the Vk for k ∈ {prej} is Vi. Therefore, Cov[Vk, Vi] is O( 1
gl
no loop in the network, j /∈ {prei}, so Cov[Vk, Vj] is O( gsyn
g2
l
is O( gsyn
g2
l

).

If neuron i and neuron j are not physically connected, we have i /∈ {prej}, so
). Therefore,

). And j /∈ {prei}, so Cov[Vk, Vj]) is O( gsyn
g2
l

Cov[Vk, Vi]) is O( gsyn
g2
l
Cov[Vi, Vj] is O( g2
syn
g3
l
End of proof.

).

Lemma 3 The differential covariance of two neurons,

Cov[

dVi
dt

, Vj] + Cov[Vi,

dVj
dt

] = 0

(51)

Proof From Lemma 2, we have,
2glCov[Vi, Vj] + (cid:80)
Cov[ (cid:80)

k∈{prei}

gk→iCov[Vk, Vj] + (cid:80)

gk→jCov[Vk, Vi] = 0

gk→iVk + glVi, Vj] + Cov[ (cid:80)

k∈{prej }

gk→jVk + glVj, Vi] = 0

(52)

k∈{prei}

k∈{prej }

Cov[ dVi

dt , Vj] + Cov[Vi, dVj

dt ] = 0

End of proof.

Theorem 3 The differential covariance of two neurons that are physically connected
is O( gsyn
gl

).

35

Proof Assume two neurons have physical connection as i → j. The differential
covariance of them is:

Cov[ dVi

dt , Vj] = Cov[ (cid:80)

gk→iVk + glVi, Vj]

k∈{prei}

gk→iCov[Vk, Vj] + glCov[Vi, Vj]

= (cid:80)

k∈{prei}

From Theorem 2, we know Cov[Vi, Vj] is O( gsyn
g2
l

), and

• If Vk is projecting to Vj, Cov[Vk, Vj] is O( gsyn
g2
l

).

• If Vk is not projecting to Vj, Cov[Vk, Vj] is O( g2
syn
g3
l

).

Therefore, the dominant term is Cov[Vi, Vj]. And Cov[ dVi

dt , Vj] is O( gsyn
gl

From Lemma 3,

Cov[Vi,

dVj
dt

] = −Cov[

dVi
dt

, Vj]

(53)

).

(54)

Therefore, Cov[Vi, dVj
End of proof.

dt ] is O( gsyn
gl

).

36

Figure 16: The network used in Theorem 4’s proof.

37

ViVjVpreVpre_pVpre_iVpre_jTheorem 4 The differential covariance of two neurons that are not physically con-
nected is O( g3
syn
g3
l

).

Proof First, let’s deﬁne the antecedents of neuron i and neuron j. Shown in Fig. 16,
{pre} is the set of common antecedents of neuron i and neuron j. {prep} is the set of
antecedents of p ∈ {pre}. {prei} is the set of exclusive antecedents of neuron i. {prej}
is the set of exclusive antecedents of neuron j.

From Lemma 2, we have, for any neuron p ∈ {pre}
(cid:88)

(cid:88)

Cov[Vi, Vp] = −(

gk→iCov[Vk, Vp] +

gk→iCov[Vk, Vp]

k∈{prei}

k∈{pre}

(cid:88)

+

k∈{prep}

gk→pCov[Vk, Vi])/2gl

Cov[Vj, Vp] = −(

(cid:88)

k∈{prej }

gk→jCov[Vk, Vp] +

(cid:88)

k∈{pre}

gk→jCov[Vk, Vp]

(cid:88)

+

k∈{prep}

gk→pCov[Vk, Vj])/2gl

For simplicity, we deﬁne

(cid:80)

k∈{prej }
(cid:80)

k∈{prei}
(cid:80)

k∈{prep}
(cid:80)

k∈{prep}

gk→jCov[Vk, Vp] = Cp

gk→iCov[Vk, Vp] = Dp

gk→pCov[Vk, Vj] = Ep

gk→pCov[Vk, Vi] = Fp

From Lemma 2, we also have,

Cov[Vi, Vj] = −(

(cid:88)

k∈{prei}

gk→iCov[Vk, Vj] +

(cid:88)

k∈{prej }

gk→jCov[Vk, Vi]

(cid:88)

+

k∈{pre}

gk→iCov[Vk, Vj] +

(cid:88)

k∈{pre}

gk→jCov[Vk, Vi])/2gl

For simplicity, we deﬁne

(cid:80)

k∈{prei}
(cid:80)

k∈{prej }

gk→iCov[Vk, Vj] = A

gk→jCov[Vk, Vi] = B

Plug in Eq. 55, Eq. 56 to Eq. 58, we have,

(55)

(56)

(57)

(58)

(59)

Cov[Vi, Vj] =

(cid:88)

+

p∈{pre}
(cid:88)

p∈{pre}

gp→i
4g2
l
gp→j
4g2
l

gk→jCov[Vk, Vp] + Cp + Ep)

gk→iCov[Vk, Vp] + Dp + Fp)

(60)

−

A
2gl

−

B
2gl

(cid:88)

(
k∈{pre}
(cid:88)

(

k∈{pre}

38

Now, we look at the differential covariance between neuron i and neuron j,

Cov[ dVi

dt , Vj] = (cid:80)

k∈{prei}

gk→iCov[Vk, Vj] + (cid:80)

gk→iCov[Vk, Vj] + glCov[Vi, Vj]

k∈{pre}

Plug in Eq. 56, Eq. 60, we have,

Cov[

dVi
dt

, Vj] = A +

(cid:88)

(cid:88)

gk→jCov[Vk, Vp] + Cp + Ep)

(

gk→jCov[Vk, Vp] + Cp + Ep)

−gp→i
2gl
gp→i
4g2
l

gp→j
4g2
l

(

k∈{pre}
(cid:88)

(
k∈{pre}
(cid:88)

p∈{pre}
(cid:88)

+ gl[

p∈{pre}
(cid:88)

+

−

−

−

p∈{pre}
A
2gl
B
2

+

gk→iCov[Vk, Vp] + Dp + Fp)

k∈{pre}

gp→j
4gl

(Dp + Fp) −

(cid:88)

p∈{pre}

gp→i
4gl

(Cp + Ep)

]

B
2gl
(cid:88)

p∈{pre}

=

A
2

Note,

(61)

(62)

• There is no physical connection between a neuron in {prei} and neuron j, other-
wise, this neuron belongs to {pre}. Therefore, from Theorem 2, A is O( g2
syn
) ∗
g3
l
gk→i = O( g3
syn
g3
l

).

• There is no physical connection between a neuron in {prej} and neuron i, other-
wise, this neuron belongs to {pre}. Therefore, from Theorem 2, B is O( g2
syn
) ∗
g3
l
gk→j = O( g3
syn
g3
l

).

• There could be physical connections between neurons in {prep} and {prej}, so

Cp is O( gsyn
g2
l

) ∗ gk→j = O( g2
syn
g2
l

).

• There could be physical connections between neurons in {prep} and {prei}, so

Dp is O( gsyn
g2
l

) ∗ gk→i = O( g2
syn
g2
l

).

• There could be physical connections between neurons in {prep} and neuron j, so

Ep is O( gsyn
g2
l

) ∗ gk→p = O( g2
syn
g2
l

).

• There could be physical connections between neurons in {prep} and neuron i, so

Fp is O( gsyn
g2
l

) ∗ gk→p = O( g2
syn
g2
l

).

Therefore,

39

Cov[

dVi
dt

, Vj] = O(

g3
syn
g3
l

) − O(

g3
syn
g3
l

) +

(cid:88)

(cid:88)

gp→j
4gl

(O(

g2
syn
g2
l

) + O(

g2
syn
g2
l

))

k∈{pre}

p∈{pre}
g2
syn
g2
l

(O(

gp→i
4gl

) + O(

g2
syn
g2
l

))

(cid:88)

(cid:88)

−

p∈{pre}

k∈{pre}

= O(

g3
syn
g3
l

)

From Lemma 3, we know, Cov[Vi, dVj
End of proof.

dt ] is also O( g3

syn
g3
l

(63)

).

Theorem 5 The type 1 false connection’s strength is reduced in differential covariance

Proof From theorem 1 and theorem 2, we know that, in the correlation method, the
strength of a non-physical connection (O( gsyn
times that of a physical connec-
g2
l
tion (O( g2
syn
g3
l

)) is gsyn
gl

)).

From theorem 3 and theorem 4, we know that, in the differential covariance method,
times that of a physical

the strength of a non-physical connection (O( gsyn
gl
connection (O( g3
syn
g3
l

)).

)) is g2
syn
g2
l

Because gsyn << gl, the relative strength of the non-physical connections is reduced

in the differential covariance method.

End of proof.

A.3 Directionality information in differential covariance

Theorem 6 If neuron i projects to neuron j with an excitatory connection, Cov[ dVi
0 and Cov[Vi, dVj

dt ] < 0

dt , Vj] >

Proof Given the model above, similar to Theorem 4, from Lemma 2, we have, for any
neuron p ∈ {pre}

Cov[Vi, Vp] = −(

(cid:88)

k∈{prei}

gk→iCov[Vk, Vp] +

(cid:88)

k∈{pre}

gk→iCov[Vk, Vp]

(cid:88)

+

k∈{prep}

gk→pCov[Vk, Vi])/2gl

Cov[Vj, Vp] = −(

(cid:88)

gk→jCov[Vk, Vp] +

(cid:88)

k∈{pre}

gk→jCov[Vk, Vp]

k∈{prej }
(cid:88)

+

k∈{prep}

gk→pCov[Vk, Vj] + gi→jCov[Vi, Vp])/2gl

40

(64)

(65)

For simplicity, we deﬁne

(cid:80)

k∈{prej }
(cid:80)

k∈{prei}
(cid:80)

k∈{prep}
(cid:80)

k∈{prep}

gk→jCov[Vk, Vp] = Cp

gk→iCov[Vk, Vp] = Dp

gk→pCov[Vk, Vj] = Ep

gk→pCov[Vk, Vi] = Fp

(66)

From Lemma 2, we also have,

Cov[Vi, Vj] = −(

(cid:88)

gk→iCov[Vk, Vj] + gi→jCov[Vi, Vi] +

k∈{prei}

(cid:88)

k∈{prej }

gk→jCov[Vk, Vi]

(cid:88)

+

k∈{pre}

gk→iCov[Vk, Vj] +

(cid:88)

k∈{pre}

gk→jCov[Vk, Vi])/2gl

For simplicity, we deﬁne

(cid:80)

k∈{prei}
(cid:80)

k∈{prej }

gk→iCov[Vk, Vj] = A

gk→jCov[Vk, Vi] = B

(67)

(68)

Plug in, we have,

Cov[Vi, Vj] =

(cid:88)

p∈{pre}

gp→i
4g2
l

(cid:88)

(
k∈{pre}

gk→jCov[Vk, Vp] + Cp + Ep + gi→jCov[Vi, Vp])

(cid:88)

+

p∈{pre}

gp→j
4g2
l

(

(cid:88)

k∈{pre}

gk→iCov[Vk, Vp] + Dp + Fp)

−

A
2gl

−

B
2gl

−

gi→jCov[Vi, Vi]
2gl

(69)

Now, we look at the differential covariance between neuron i and neuron j,

Cov[

dVi
dt

, Vj] =

A
2

−

B
2

+

(cid:88)

p∈{pre}

gp→j
4gl

(Dp + Fp)

(cid:88)

−

p∈{pre}

gp→i
4gl

(Cp + Ep + +gi→jCov[Vi, Vp]) −

(70)

gi→jCov[Vi, Vi]
2

Note, in Theorem 4, we already proved the scale of A, B, Cp, Dp, Ep, Fp. We also

have,

• There are physical connections between neurons in {pre} and neuron i, so gi→jCov[Vi, Vp]

is O( gsyn
g2
l

) ∗ gk→p = O( g2
syn
g2
l

).

41

• From Lemma 1, the auto-covariance of neuron i is O( 1
gl

O( 1
gl

) ∗ gi→j = O( gsyn
gl

).

). So gi→jCov[Vi, Vi] is

Therefore, gi→jCov[Vi, Vi] is the dominant term in Cov[ dVi

dt , Vj]. Since Cov[Vi, Vi] >

0, for excitatory connection gi→j > 0, Cov[ dVi

dt , Vj] < 0.

From Lemma 3,

Cov[Vi,

dVj
dt

] = −Cov[

dVi
dt

, Vj]

(71)

Therefore, Cov[Vi, dVj
End of proof.

dt ] > 0.

Corollary 1 If neuron i projects to neuron j with an inhibitory connection, Cov[ dVi
0 and Cov[Vi, dVj

dt ] < 0

dt , Vj] >

Proof The proof is similar to Theorem 6. Again, we know gi→jCov[Vi, Vi] is the
dominant term in Cov[ dVi
dt , Vj]. Since Cov[Vi, Vi] > 0, now for an inhibitory connection
gi→j < 0, Cov[ dVi

dt , Vj] > 0.

From Lemma 3,

Cov[Vi,

dVj
dt

] = −Cov[

dVi
dt

, Vj]

(72)

Therefore, Cov[Vi, dVj
End of proof.

dt ] < 0.

B Benchmarked methods

We compared our methods to a few popular methods.

B.1 Covariance method

The covariance matrix is deﬁned as:

COVx,y =

1
N

N
(cid:88)

(xi − µx)(yi − µy)

i=1

Where x and y are two variables. µx and µy are their population mean.

B.2 Precision matrix

The precision matrix is the inverse of the covariance matrix:

P = COV −1,

(73)

(74)

It can be considered as one kind of partial correlation. Here we brieﬂy review this
derivation, because we use it to develop our new method. The derivation here is based
on and adapted from Cox and Wermuth (1996).

42

We begin by considering a pair of variables (x, y), and remove the correlation in

them introduced from a control variable z.

First, we deﬁne the covariance matrix as:

COVxyz =





σxx σxy σxz
σyx σyy σyz
σzx σzy σzz





By solving the linear regression problem:

we have:

wx = arg min

w

wy = arg min

w

E(x − w ∗ z)2

E(y − w ∗ z)2

wx = σxzσ−1
zz
wy = σyzσ−1
zz

then, we deﬁne the residual of x, y as,

rx = x − wx ∗ z
ry = y − wy ∗ z

Therefore, the covariance of rx, ry is:

COVrx,ry = σxy − σxz ∗ σ−1

zz ∗ σyz

On the other hand, if we deﬁne the precision matrix as:

Pxyz =





pxx pxy pxz
pyx pyy pyz
pzx pzy pzz





Using Cramer’s rule, we have:

−

(cid:12)
σxy σxz
(cid:12)
(cid:12)
σzy σzz
(cid:12)
|COVxyz|

(cid:12)
(cid:12)
(cid:12)
(cid:12)

pxy =

Therefore,

pxy =

−σzz
|COVxyz|

(σxy − σxz ∗ σ−1

zz ∗ σyz)

So pxy and COVrx,ry are differed by a ratio of −σzz

|COVxyz| .

B.3 Sparse latent regularization

(75)

(76)

(77)

(78)

(79)

(80)

(81)

(82)

Prior studies(Banerjee et al., 2006; Friedman et al., 2008) have shown that regulariza-
tions can provide better estimation if the ground truth connection matrix has a known
structure (e.g. sparse). For all data tested in this paper, the sparse latent regulariza-
tion(Yatsenko et al., 2015) worked best. For a fair comparison, we applied the sparse

43

latent regularization to both the precision matrix method and our differential covariance
method.

In the original sparse latent regularization method, people made the assumption that
a larger precision matrix S is the joint distribution of the p observed neurons and d
latent neurons (Yatsenko et al., 2015). i.e.

S =

(cid:18) S11 S12
S21 S22

(cid:19)

Where S11 corresponds to the observable neurons.

If we can only measure the
observable neurons, the partial correlation computed from the observed neural signals
is,

C −1

ob = Sob = S11 − S12 · S−1

22 · S21

(83)

because the invisible latent neurons as shown in Eq. 2 introduce correlations into the
measurable system. We denote this correlation introduced from the latent inputs as

L = S12 · S−1

22 · S21

(84)

If we can make the assumption that the connection between the visible neurons are
sparse, i.e. S11 is sparse and the number of latent neurons is much smaller than the
number of visible neurons, i.e. d << p. Then, prior works (Chandrasekaran et al.,
2011) have shown that if Sob is known, S11 is sparse enough and L’s rank is low enough
(within the bound deﬁned in Chandrasekaran et al. (2011)), then the solution of

S11 − L = Sob

(85)

is uniquely deﬁned and can be solved by the following convex optimization problem

arg min
S11,L

||S11||1 + α ∗ tr(L)

under the constraint that

Sob = S11 − L

(86)

(87)

Here, || ||1 is the L1-norm of a matrix, and tr() is the trace of a matrix. α is the penalty
N for all our
ratio between the L1-norm of S11 and the trace of L and is set to 1/
estimations.

√

However, the above method is used to regularize precision matrix. For our differen-
tial covariance estimation, we need to make small changes to the derivation. Note that
if we assume the neural signals of the latent neurons are known, and let l be the indexes
of these latent neurons, then from our previous section (section 2.2.2),

∆Si,j = ∆Pi,j − COVj,l · COV −1
l,l

· ∆T

Ci,l

(88)

removes the Vlatent terms in Eq. 2.

Even if l is unknown,

COVj,l · COV −1
l,l

· ∆T

Ci,l

44

is low-rank, because it is bounded by the dimensionality of COVl,l, which is d. And
∆S is the internal connections between the visible neurons, which should be a sparse
matrix. Therefore, letting

Sob = ∆P
S11 = ∆S

L = −COVj,l · COV −1
l,l

· ∆T

Ci,l

(89)

we can use the original sparse+latent method to solve for ∆S. In this paper, we used the
inexact robust PCA algorithm ( http://perception.csl.illinois.edu/matrix-rank/
sample_code.html ) to solve this problem (Lin et al., 2011).

B.4 the generalized linear model method

As summarized by Roudi et al. (2015), GLMs assume that every neuron spikes at a
time-varying rate which depends on earlier spikes (both those of other neurons and its
own) and on external covariates (such as a stimulus or other quantities measured in the
experiment). As they explained, the inﬂuence of earlier spikes on the ﬁring probability
at a given time is assumed to depend on the time since they occurred. For each pre-
postsynaptic pair i, j, it is described by a function Jij(τ ) of this time lag (Roudi et al.,
2015). In this paper, we average this temporal dependent function Jij(τ ) over time to
obtain the functional connectivity estimation of this method.

The Spike trains used for the GLM method were computed using the spike detection
algorithm from Quiroga et al. (2004) (https://github.com/csn-le/wave_
clus). The paper’s default parameter set is used except that the maximum detec-
tion threshold is increased. The code is provided in our github repository (https://
github.com/tigerwlin/diffCov/blob/master/spDetect.m) for refer-
ence. The GLM code was obtained from Pillow et al. (2008) (http://pillowlab.
princeton.edu/code_GLM.html) and applied on the spike trains.

C Details about the thalamocortical model

C.1 Intrinsic currents

For the thalamocortical model, a conductance-based formulation was used for all neu-
rons. The cortical neuron consisted of two compartments: dendritic and axo-somatic
compartments, similar to previous studies (Bazhenov et al., 2002; Chen et al., 2012;
Bonjean et al., 2011) and is described by the following equations,

Cm

dVD
dt = −I K−leak

d − I N a

d − I N ap

d − I Ca

d − I Km

d − I syn

c(VS − VD) = −I N a
gs

d
S − I K

− I leak
S − I N ap

S

(90)

where the subscripts s and d correspond to axo-somatic and dendritic compartment, Ileak
is the Cl− leak currents, IN a is fast Na+ channels, IN ap is persistent sodium current, IK
is fast delayed rectiﬁer K+ current, IKm is slow voltage-dependent non-inactivating K+
current, IKCa is slow Ca2+ dependent K+ current, ICa is high-threshold Ca2+ current,

45

Ih is hyperpolarization-activated depolarizing current and Isyn is the sum of synaptic
currents to the neuron. All intrinsic currents were of the form: g(V − E), where g
is the conductance, V is the voltage of the corresponding compartment and E is the
reversal potential. The detailed descriptions of individual currents are provided in pre-
vious publications (Bazhenov et al., 2002; Chen et al., 2012). The conductance of the
leak currents were 0.007 mS/cm2 for IK−leak
. The maxi-
mal conductance for different currents were, IN ap
d : 0.8 mS/cm2, IKm
:
d
0.012 mS/cm2, IKCa
s : 200
d
: 15 mS/cm2. Cm was 0.075 µF/cm2.
mS/cm2 and IN ap

and 0.023 mS/cm2 for Ileak

: 0.015 mS/cm2 , IKm

: 0.012 mS/cm2, IN a

: 3000mS/cm2, IK

: 2.0 mS/cm2, IN a

d

d

d

d

s

s

The following describes the IN neurons:

Cm

dVD
dt = −I K−leak

− I leak

d − I N a

d − I Ca

d − I KCa

d − I Km

d − I syn

c(VS − VD) = −I N a
gs

d
S − I K
S

(91)

The conductance for leak currents for IN neurons were 0.034 mS/cm2 for IK−leak
0.006 mS/cm2 for Ileak
mS/cm2, ICa
mS/cm2 and IK

. Maximal conductance for other currents were, IN a
d
: 0.012 mS/cm2, IN a

d : 0.012 mS/cm2, IKCa
s : 200 mS/cm2 .

: 0.015 mS/cm2 , IKm

and
: 0.8
: 2500

d

d

d

d

s

The TC neurons consisted of only single compartment and was described as follows,

dVD
dt = −I K−leak − I leak − I N a − I K − I LCa − I h − I syn

(92)

The conductance of leak currents were, Ileak: 0.01 mS/cm2, IK−leak: 0.007 mS/cm2. The
maximal conductance for other currents were, fast Na+ (IN a) current: 90 mS/cm2, fast
K+ (Ik) current: 10 mS/cm2, low threshold Ca2+(ILCa) current: 2.5 mS/cm2, hyperpolarization-
activated depolarizing current (Ih): 0.015 mS/cm2.

The RE cells were also modeled as a single compartment neuron as follows,

dVD
dt = −I K−leak − I leak − I N a − I K − I LCa − I h − I syn

(93)

The conductance for leak currents were, Ileak: 0.05 mS/cm2, IK−leak: 0.016 mS/cm2.
The maximal conductance for other currents were, fast Na+ (IN a) current: 100 mS/cm2,
fast K+ (IK) current: 10 mS/cm2, low threshold Ca2+(ILCa) current: 2.2 mS/cm2.

C.2 Synaptic currents

GABA-A, NMDA and AMPA synaptic currents were described by ﬁrst-order activation
schemes (Timofeev et al., 2000). The equations for all synaptic currents used in this
model are given in our previous publications (Bazhenov et al., 2002; Chen et al., 2012).
Brieﬂy, below we mention only the relevant equations.

I AM P A
syn
I N M DA
syn
I GABA
syn

=gsyn[O](V − EAM P A)
=gsyn[O](V − EN M DA)
=gsyn[O](V − EGABA)

(94)

D Supplementary ﬁgures

46

Figure 17: Supplementary1, passive neuron model with 5 ms ﬁxed synaptic delay. Re-
sults from correlation-based methods. A) Ground truth connection matrix. neurons 1-50
are visible neurons. neurons 51-60 are invisible neurons. B) Estimation from the corre-
lation method. C) Estimation from the precision matrix. D) Sparse+latent regularized
precision matrix.

47

A: Ground Truth  204060102030405060−10−50510B: COV  10203040501020304050−0.500.5C: Precision Matrix  10203040501020304050−505D: SL−reg Precision Matrix  10203040501020304050−505Figure 18: Supplementary2, differential covariance analysis of the passive neuron
model with 5 ms ﬁxed synaptic delay. The color in B,C,D indicates direction of the
connections. For element Aij, warm color indicates i is the sink, j is the source, i.e.
i ← j, and cool color indicates j is the sink, i is the source, i.e. i → j. A) Ground
truth connection matrix. B) Estimation from the differential covariance method. C)
Estimation from the partial differential covariance method. D) Estimation from the
sparse+latent regularized partial differential covariance method.

48

A: Ground Truth  204060102030405060−10−50510B: ∆C  10203040501020304050−0.500.5C: ∆P  10203040501020304050−0.1−0.0500.050.1D: ∆S  10203040501020304050−0.04−0.0200.020.04Figure 19: Supplementary3, passive neuron model with 0-10 ms uniformly distributed
synaptic delay. Results from correlation-based methods. A) Ground truth connection
matrix. neurons 1-50 are visible neurons. neurons 51-60 are invisible neurons. B)
Estimation from the correlation method. C) Estimation from the precision matrix. D)
Sparse+latent regularized precision matrix.

49

A: Ground Truth  204060102030405060−10−50510B: COV  10203040501020304050−0.500.5C: Precision Matrix  10203040501020304050−505D: SL−reg Precision Matrix  10203040501020304050−505Figure 20: Supplementary4, differential covariance analysis of the passive neuron
model with 0-10 ms uniformly distributed synaptic delay. The color in B,C,D indi-
cates direction of the connections. For element Aij, warm color indicates i is the sink,
j is the source, i.e. i ← j, and cool color indicates j is the sink, i is the source, i.e.
i → j. A) Ground truth connection matrix. B) Estimation from the differential co-
variance method. C) Estimation from the partial differential covariance method. D)
Estimation from the sparse+latent regularized partial differential covariance method.

50

A: Ground Truth  204060102030405060−10−50510B: ∆C  10203040501020304050−0.4−0.200.20.4C: ∆P  10203040501020304050−0.0500.05D: ∆S  10203040501020304050−0.04−0.0200.020.04References

Banerjee, O., Ghaoui, L. E., d’Aspremont, A., and Natsoulis, G. (2006). Convex opti-
mization techniques for ﬁtting sparse gaussian graphical models. In Proceedings of
the 23rd international conference on Machine learning, pages 89–96. ACM.

Battistin, C., Hertz, J., Tyrcha, J., and Roudi, Y. (2015). Belief propagation and replicas
for inference and learning in a kinetic ising model with hidden spins. Journal of
Statistical Mechanics: Theory and Experiment, 2015(5):P05021.

Bazhenov, M., Timofeev, I., Steriade, M., and Sejnowski, T. J. (2002). Model of thalam-
ocortical slow-wave sleep oscillations and transitions to activated states. The Journal
of Neuroscience, 22(19):8691–8704.

Bonjean, M., Baker, T., Lemieux, M., Timofeev, I., Sejnowski, T., and Bazhenov, M.
(2011). Corticothalamic feedback controls sleep spindle duration in vivo. The Jour-
nal of Neuroscience, 31(25):9124–9134.

Burkitt, A. N. (2006). A review of the integrate-and-ﬁre neuron model: I. homogeneous

synaptic input. Biological cybernetics, 95(1):1–19.

Capone, C., Filosa, C., Gigante, G., Ricci-Tersenghi, F., and Del Giudice, P. (2015).
Inferring synaptic structure in presence of neural interaction time scales. PloS one,
10(3):e0118412.

Chandrasekaran, V., Sanghavi, S., Parrilo, P. A., and Willsky, A. S. (2011). Rank-
sparsity incoherence for matrix decomposition. SIAM Journal on Optimization,
21(2):572–596.

Chen, J.-Y., Chauvette, S., Skorheim, S., Timofeev, I., and Bazhenov, M. (2012).
Interneuron-mediated inhibition synchronizes neuronal activity during slow oscilla-
tion. The Journal of physiology, 590(16):3987–4010.

Cox, D. R. and Wermuth, N. (1996). Multivariate dependencies: Models, analysis and

interpretation, volume 67. CRC Press.

Destexhe, A. (1998). Spike-and-wave oscillations based on the properties of gabab

receptors. The Journal of neuroscience, 18(21):9099–9111.

Dunn, B. and Roudi, Y. (2013). Learning and inference in a nonequilibrium ising model

with hidden nodes. Physical Review E, 87(2):022127.

Fan, H., Shan, X., Yuan, J., and Ren, Y. (2011). Covariances of linear stochastic differ-
ential equations for analyzing computer networks. Tsinghua Science & Technology,
16(3):264–271.

Friedman, J., Hastie, T., and Tibshirani, R. (2008). Sparse inverse covariance estimation

with the graphical lasso. Biostatistics, 9(3):432–441.

Friston, K. J. (2011). Functional and effective connectivity: a review. Brain connectiv-

ity, 1(1):13–36.

51

Harrison, L., Penny, W. D., and Friston, K. (2003). Multivariate autoregressive model-

ing of fmri time series. Neuroimage, 19(4):1477–1491.

Huang, H. (2013). Sparse hopﬁeld network reconstruction with 1 regularization. The

European Physical Journal B, 86(11):1–7.

Kandel, E. R., Markram, H., Matthews, P. M., Yuste, R., and Koch, C. (2013). Neuro-
science thinks big (and collaboratively). Nature Reviews Neuroscience, 14(9):659–
664.

L´ansk`y, P. and Rospars, J. P. (1995). Ornstein-uhlenbeck model neuron revisited. Bio-

logical cybernetics, 72(5):397–406.

Lin, Z., Liu, R., and Su, Z. (2011). Linearized alternating direction method with adap-
tive penalty for low-rank representation. In Advances in neural information process-
ing systems, pages 612–620.

McIntosh, A. and Gonzalez-Lima, F. (1991). Structural modeling of functional neural
pathways mapped with 2-deoxyglucose: effects of acoustic startle habituation on the
auditory system. Brain research, 547(2):295–302.

Nunez, P. and Srinivasan, R. (2005). Electric ﬁelds of the brain oxford university press.

New York.

Okatan, M., Wilson, M. A., and Brown, E. N. (2005). Analyzing functional connec-
tivity using a network likelihood model of ensemble neural spiking activity. Neural
computation, 17(9):1927–1961.

Pernice, V. and Rotter, S. (2013). Reconstruction of sparse connectivity in neural net-
works from spike train covariances. Journal of Statistical Mechanics: Theory and
Experiment, 2013(03):P03008.

Pillow, J. W., Shlens, J., Paninski, L., Sher, A., Litke, A. M., Chichilnisky, E., and Si-
moncelli, E. P. (2008). Spatio-temporal correlations and visual signalling in a com-
plete neuronal population. Nature, 454(7207):995–999.

Quan, T., Liu, X., Lv, X., Chen, W. R., and Zeng, S. (2010). Method to reconstruct neu-
ronal action potential train from two-photon calcium imaging. Journal of biomedical
optics, 15(6):066002–066002.

Quiroga, R. Q., Nadasdy, Z., and Ben-Shaul, Y. (2004). Unsupervised spike detection
and sorting with wavelets and superparamagnetic clustering. Neural computation,
16(8):1661–1687.

Rahmati, V., Kirmse, K., Markovi´c, D., Holthoff, K., and Kiebel, S. J. (2016). Infer-
ring neuronal dynamics from calcium imaging data using biophysical models and
bayesian inference. PLoS Comput Biol, 12(2):e1004736.

Ricciardi, L. M. and Sacerdote, L. (1979). The ornstein-uhlenbeck process as a model

for neuronal activity. Biological cybernetics, 35(1):1–9.

52

Roudi, Y., Dunn, B., and Hertz, J. (2015). Multi-neuronal activity and functional con-

nectivity in cell assemblies. Current opinion in neurobiology, 32:38–44.

Roudi, Y. and Hertz, J. (2011). Mean ﬁeld theory for nonequilibrium network recon-

struction. Physical review letters, 106(4):048702.

Schneidman, E., Berry, M. J., Segev, R., and Bialek, W. (2006). Weak pairwise cor-
relations imply strongly correlated network states in a neural population. Nature,
440(7087):1007–1012.

Stetter, O., Battaglia, D., Soriano, J., and Geisel, T. (2012). Model-free reconstruction
of excitatory neuronal connectivity from calcium imaging signals. PLoS Comput
Biol, 8(8):e1002653.

Stevenson, I. H. and Kording, K. P. (2011). How advances in neural recording affect

data analysis. Nature neuroscience, 14(2):139–142.

Stevenson, I. H., Rebesco, J. M., Miller, L. E., and K¨ording, K. P. (2008). Inferring func-
tional connections between neurons. Current opinion in neurobiology, 18(6):582–
588.

Timofeev, I., Grenier, F., Bazhenov, M., Sejnowski, T., and Steriade, M. (2000). Ori-
gin of slow cortical oscillations in deafferented cortical slabs. Cerebral Cortex,
10(12):1185–1199.

Truccolo, W., Eden, U. T., Fellows, M. R., Donoghue, J. P., and Brown, E. N. (2005). A
point process framework for relating neural spiking activity to spiking history, neural
ensemble, and extrinsic covariate effects. Journal of neurophysiology, 93(2):1074–
1089.

Uhlenbeck, G. E. and Ornstein, L. S. (1930). On the theory of the brownian motion.

Physical review, 36(5):823.

Vogelstein, J. T., Watson, B. O., Packer, A. M., Yuste, R., Jedynak, B., and Paninski, L.
(2009). Spike inference from calcium imaging using sequential monte carlo methods.
Biophysical journal, 97(2):636–655.

Winterhalder, M., Schelter, B., Hesse, W., Schwab, K., Leistritz, L., Klan, D., Bauer,
R., Timmer, J., and Witte, H. (2005). Comparison of linear signal processing tech-
niques to infer directed interactions in multivariate neural systems. Signal processing,
85(11):2137–2160.

Yatsenko, D., Josi´c, K., Ecker, A. S., Froudarakis, E., Cotton, R. J., et al. (2015). Im-
proved estimation and interpretation of correlations in neural circuits. PLoS Compu-
tational Biology, 2:199–207.

53

