Linguistic-Acoustic Similarity Based Accent Shift for Accent Recognition

Qijie Shao1, Jinghao Yan2, Jian Kang2, Pengcheng Guo1, Xian Shi1, Pengfei Hu2, Lei Xie1‚àó

1Audio, Speech, and Language Processing Group (ASLP@NPU),
School of Computer Science, Northwestern Polytechnical University, Xi‚Äôan, China
2Tencent Research, Beijing, China
{qjshao,xshi}@npu-aslp.org, pcguo@nwpu-aslp.org, lxie@nwpu.edu.cn
{jinghaoyan,jiankang,alanpfhu}@tencent.com

2
2
0
2

l
u
J

1

]

D
S
.
s
c
[

2
v
8
9
3
3
0
.
4
0
2
2
:
v
i
X
r
a

Abstract

General accent recognition (AR) models tend to directly extract
low-level information from spectrums, which always signiÔ¨Å-
cantly overÔ¨Åt on speakers or channels. Considering accent can
be regarded as a series of shifts relative to native pronunciation,
distinguishing accents will be an easier task with accent shift
as input. But due to the lack of native utterance as an anchor,
estimating the accent shift is difÔ¨Åcult. In this paper, we pro-
pose linguistic-acoustic similarity based accent shift (LASAS)
for AR tasks. For an accent speech utterance, after mapping the
corresponding text vector to multiple accent-associated spaces
as anchors, its accent shift could be estimated by the similarities
between the acoustic embedding and those anchors. Then, we
concatenate the accent shift with a dimension-reduced text vec-
tor to obtain a linguistic-acoustic bimodal representation. Com-
pared with pure acoustic embedding, the bimodal representation
is richer and more clear by taking full advantage of both lin-
guistic and acoustic information, which can effectively improve
AR performance. Experiments on Accented English Speech
Recognition Challenge (AESRC) dataset show that our method
achieves 77.42% accuracy on Test set, obtaining a 6.94% rela-
tive improvement over a competitive system in the challenge.
Index Terms: accent shift, accent recognition, accent classiÔ¨Å-
cation, AESRC, linguistic-acoustic bimodal

1. Introduction

Accents are special pronunciations that are generally affected
by many factors, e.g., region, native language, and education
level [1]. Accent recognition (AR) or accent classiÔ¨Åcation can
be used for advertising recommendations and regionally differ-
entiated services. Furthermore, AR is also an important precur-
sor to many speech tasks, such as automatic speech recognition
(ASR) and voice assistant. For these tasks, AR has a signiÔ¨Å-
cant impact on their performance. Therefore, high-performance
AR solutions have received extensive attention recently. Accent
problems usually include international accents and regional ac-
cents. This study mainly focuses on international accents, but
our methods can also be used for regional accents.

Some earlier research directly applied x-vector based
speaker recognition model [2] in AR tasks by simply chang-
ing the speaker label to accent label [3, 4, 5]. Such models
tend to directly extract low-level features (such as the frequency
and timbre) from spectrums, resulting in signiÔ¨Åcant overÔ¨Åt-
ting to speakers or channels [6].
In recent years, consider-
ing that accent is language-related, researchers began to allevi-
ate overÔ¨Åtting by introducing linguistic information, e.g., from
ASR. Shi et al. [7] proposed to initialize an AR model with

*Corresponding author.

a well-trained ASR encoder and achieved obvious improve-
ment. Many researchers [8, 9, 10] used multi-task architectures
to combine AR and ASR into a union model, which leads the
model to focus more on linguistic information. Furthermore,
Hamalainen et al. [11] used self-supervised pre-training mod-
els [12, 13] to respectively extract linguistic and acoustic em-
beddings and concatenated them together for an AR task.

While signiÔ¨Åcant progress has been achieved with the help
of linguistic information, the accuracy rates of AR models are
still not ideal for downstream tasks.
In [14, 15], researchers
showed that accent can be considered as a set of deviations or
shifts from standard pronunciation. Therefore, if a native ut-
terance that has the same words as the accent utterance could
be obtained, the accent shift could be measured. With this in
mind, distinguishing different accents will be an easier task for
AR models with accent shift as input. Inspired by this, similar
methods [16, 17, 18] have been applied in pronunciation as-
sessment tasks. In these studies, the pronunciation similarities
between the utterance out of the same transcripts from native
and non-native speakers are evaluated. SpeciÔ¨Åcally, accent shift
was extracted by aligning an accent utterance with a standard
pronunciation and comparing their differences. Results showed
that accent shift can effectively represent the Ô¨Åne-grained accent
characteristics between native and non-native speakers.

However, in AR tasks, it is difÔ¨Åcult to obtain such paired
utterances which have the same transcripts. In the above meth-
ods, native utterance acts as an anchor for accent shift extrac-
tion. We consider building a virtual anchor to replace the native
utterance in this study. In [19, 20, 21], word embeddings were
regarded as anchors to extract emotion shift from visual and
acoustic features. Inspired by these works, in this paper, we pro-
pose linguistic-acoustic similarity based accent shift (LASAS)
for AR tasks. SpeciÔ¨Åcally, we align the accent speech and its
corresponding text by force alignment and map the resulted
phoneme-level text sequence into multiple Euclidean spaces.
The obtained mapping text vectors are regarded as anchors for
accent shift estimation which tries to capture the pronunciation
variants of the same word on different accents. Then we use
the scale dot-product to calculate the similarities between the
acoustic embeddings extracted by a Conformer [22] encoder
and the mapping text vectors. These similarities show differ-
ent shift distances and directions for different accents. So their
combination could be regarded as an accent shift. Finally, we
concatenate the accent shift with a dimension-reduced text vec-
tor to obtain a bimodal representation for AR model training.
Compared with pure acoustic embedding, the bimodal repre-
sentation is richer and more clear by taking full advantage of
both linguistic and acoustic information, which can effectively
improve AR performance. Extensive experiments conducted on
the AESRC challenge dataset [7] demonstrate that LASAS is

 
 
 
 
 
 
Figure 1: The framework of our accent recognition system.

effective and has a clear physical meaning in line with linguis-
tics, which apparently outperforms the direct concatenation of
text vector and acoustic embeddings. With ASR-generated text,
LASAS achieves 77.42% accuracy on Test set, signiÔ¨Åcantly
surpassing a competitive system in the challenge.

2. Method

2.1. Overall System Structure

The purpose of our research is to measure the shift of accent
with the help of linguistic information. By incorporating the
shift of each accent into the AR training, the model is able to
learn more discriminative accent representations over the con-
ventional acoustic features, resulting in better recognition per-
formance. To achieve this goal, we propose a LASAS-based
AR model.

As shown in Fig. 1 (a), our model consists of three major
blocks: (1) Input block, which takes acoustic features such as
log-Mel Ô¨Ålter banks (Fbank) or Mel-frequency cepstral coefÔ¨Å-
cient (MFCC) and aligned text as inputs. We feed the acoustic
features into a Conformer [22] encoder and then concatenate
the outputs of certain encoder layers together as acoustic em-
beddings. (2) LASAS block, which is used to map aligned text
and the acoustic embeddings to calculate the similarities as ac-
cent shifts, and linguistic-acoustic bimodal representations are
obtained. (3) AR block, which takes these bimodal representa-
tions as input. First, context-sensitive accent information is ex-
tracted through a lightweight Transformer [23] encoder. Then,
the utterance-level accent prediction is obtained after a DNN
classiÔ¨Åer followed by a statistical pooling layer.

2.2. Input Block

Compared with standard pronunciation, accent shift is often
manifested in special words or phonemes. Therefore, to eval-
uate it, pronunciation units need to be given Ô¨Årst. In this paper,
we use the byte pair encoding (BPE) [24] method to obtain sub-
words, and then take them as pronunciation units.

In [17], Ô¨Åne-grained accent features of pronunciation units
are extracted by comparing one standard pronunciation with an-
other accented pronunciation. The two sentences are aligned
in time before comparison. After the alignment, the accent
and standard pronunciation frame at the same time will corre-
spond to the same pronunciation unit. Only in this way can

the comparison between two frames be meaningful. As men-
tioned above, this method needs pairs of accent and standard
pronunciation utterances with the same text, which are difÔ¨Åcult
to obtain in an AR task. So we use aligned text to construct
virtual anchors instead of standard pronunciation utterances to
measure accent shift. That is, we need an additional ASR sys-
tem for alignment. The text mentioned here can be manually
transcribed or generated by an ASR system, which will be com-
pared in experiments later.

In addition, accents are human voice-related information.
Study in [7] shows that the encoder of an ASR model can be
used to extract voice-related acoustic embeddings. So we use
a Conformer block as an encoder, which is the state-of-the-art
architecture in ASR. Finally, several layers‚Äô outputs of the en-
coder are concatenated to form an acoustic embedding.

2.3. LASAS Block

As mentioned in [7], by comparing the differences between a
standard pronunciation and another accent utterance, we can ex-
tract the physical accent shift. But for AR tasks, this physical
shift cannot be obtained under the lack of data pairs. If we build
a virtual shift that shows different shift distances and directions
on the same pronunciation unit for different accents, such a shift
is also meaningful for the subsequent AR model.

To achieve this goal, an anchor is needed, which is related
to the speech content and aligned with the speech. We map the
aligned text vectors to multiple Euclidean spaces as an anchor
set. This method ensures that when the same pronunciation unit
appears in utterances with different accents, the anchor is the
same. Anchor Vi

t is deÔ¨Åned as:

Vi

t = Xt ¬∑ Wi

t, i ‚àà [1, N ]

(1)

where N is the number of mapping spaces. Xt is an frame-
level aligned text vector. Wi
t is the text mapping matrix, and
t ‚àà RD2√óC . C is the dimension of hidden features, which is
Wi
shown in Fig. 1 (b).

Then, we map an acoustic embedding to the same dimen-
sion as the text anchor and calculate the similarity of them by
scale dot-product [23] frame by frame. This measuring method
of similarity for two different dimension vectors is widely used
in the attention mechanism [23, 25]. Details are as follows:

(cid:26)

Vi
Si = DotProd(Vi

a = Xa ¬∑ Wi
a
a, Vi
t)/

‚àö

dk

(2)

where Xa is acoustic embedding. Wi
a is the acoustic mapping
a ‚àà RD1√óC . And dk is a scale factor, we set
matrix, and Wi
dk = C/N . Si is the similarity value in the ith mapping space.
As shown in Fig. 1 (b), we use multiple sets of mappings to
obtain similarities. All Wa and Wt are trainable. Therefore,
different mappings can measure the accent shifts from differ-
ent aspects. This method is similar to the multi-head attention
mechanism [23]. A similarity value can only indicate the prox-
imity between one mapping acoustic vector and one mapping
text anchor. However, the combination of multiple similarities
can reÔ¨Çect the shift directions and similar degrees of different
accents. Therefore, the accent shift S is calculated as:

S = Concat(S1, S2, ..., SN )

(3)

Accent shift is a relative representation. For an AR model,
it is also necessary to know the reference of this shift. That is,
we need to offer the pronunciation unit information correspond-
ing to accent shift. Since each Vt mentioned above is related
to different accents, the reference should preferably only con-
tain pronunciation unit information. So any Vt is not suitable
for reference. Therefore, we set up a dimension reduction ma-
trix separately to reduce the dimension of the input text OneHot
vector for representing the pure subword information. Then, we
directly concatenate the accent shift and the dimension-reduced
subword together to form a linguistic-acoustic bimodal repre-
sentation, which is used as the input for the subsequent AR
model. We believe that explicitly preserving the accent shift
is better for an AR task than adding it to the subword refer-
ence [19]. The bimodal representation Ybm is obtained:
(cid:26) Vtd = Xt ¬∑ Wtd

(4)

Ybm = Concat(S, Vtd)

2.4. Accent Recognition Block

Since the bimodal representation is frame-level information, we
Ô¨Årst use a lightweight Transformer encoder with only a few lay-
ers to extract context-sensitive accent information. After that,
similar to common AR models, DNN is used to reduce the di-
mension of representations. At last, An utterance-level accent
prediction is extracted after a statistical pooling layer.

3. Experiments

3.1. Data And Experimental Settings

In this study, we use an open-source English accent recognition
challenge dataset named AESRC [7] for experiments. SpeciÔ¨Å-
cally, we use the ofÔ¨Åcial Dev and Track1 Test sets for evalua-
tion. There is no speaker overlap in the three sets. The duration
of each accent in the training set is 20 hours, while each test set
is 2 hours.

As mentioned above, we used an ASR model to align the
text, initialize the Conformer encoder, and generate the text
in Table 3. This ASR model is trained by Librispeech (960
hours) [26] and AESRC data sets with Wenet tools [27]. It con-
tains 12 Conformer layers without language model. The CER
of this ASR model in the AESRC Dev set is 6.06%.

In all of our experiments, the Dev set does not participate
in training. Our experiments include SpecAugment and model
average (top 5), and no other data augmentation. We set the
attention dimension of Conformer and Transformer encoders,
and the parameter C of the LASAS block in Fig. 1 (b) to 256.
The number of Transformer and DNN layers is 3. And the DNN
dimension is halved for each layer. To exclude interference, we

use real text in the train and test stage, except system21 and
system22 in Table 3.

3.2. Results And Analysis

3.2.1. Effectiveness of LASAS

In Table 1, system1 is similar to the ofÔ¨Åcial baseline in the
AESRC challenge with the encoder being replaced from Trans-
former to Conformer. For further comparison, we remove the
LASAS block in Fig. 1 (a) and the rest parts are noted as
system2. System3 is a typical LASAS-based AR model and
is used as a base model for the following experiments. As
shown in Table 1, system3 is signiÔ¨Åcantly better than system2
and system1. Without LASAS, even if system2 is more
reÔ¨Åned and complex, its performance is not signiÔ¨Åcantly im-
proved than system1. These experiments fully prove that
LASAS is effective.

Table 1: AR accuracy of LASAS effectiveness experiments.

ID

System

ConÔ¨Ågure

1

2

3

Baseline1

Baseline2

LASASbase

Conformer(12L)+DNN
Conformer(3 6 9L)
+Transformer+DNN
Conformer(3 6 9L)+LASAS
+Transformer+DNN

Dev
(%)
73.53

Test
(%)
68.08

75.68

67.71

84.05

75.64

Figure 2: T-SNE transform of subword ‚Äúthe‚Äù.

Figure 3: PCA transform of subword ‚Äúthe‚Äù and ‚ÄúI‚Äù. The
circles are Gaussian contours. The straight lines connect the
mean subword centroids of all accents and each individual
accent subword centroid, which show accent shift distances.

We choose subwords ‚Äúthe‚Äù and ‚ÄúI‚Äù to visualize accent
shift. These two subwords are chosen because, on the one
hand, some accents are more signiÔ¨Åcant on ‚Äúthe‚Äù, while most
accents have little shift on ‚ÄúI‚Äù, on the other hand, these two
words appear more frequently so the results are more credible.
We average frame-level accent shifts of each subword segment
in the Dev and Test sets. And then project the shifts into 2-
dimensional space by t-SNE and PCA transforms, respectively.
As shown in Fig. 2 and Fig. 3, the distinguishability of accent

                               $ F F H Q W  6 K L I W  > W K H @ , 1 ' 8 . 8 6                               $ F F H Q W  6 K L I W  > , @shift is signiÔ¨Åcantly higher than that of acoustic embedding be-
fore LASAS, and the shift distance of ‚Äúthe‚Äù is signiÔ¨Åcantly
larger than ‚ÄúI‚Äù. In particular, for the subword ‚Äúthe‚Äù in the In-
dian accent, the clustering effect is prominent in Fig. 2, and the
shift distance in Fig. 3 is signiÔ¨Åcantly larger than British and
American accents. This is because ‚Äú[D@]‚Äù is pronounced like
‚Äú[d@]‚Äù in the Indian accent, which is unusual in other accent.
These phenomena fully prove that the accent shift extracted by
LASAS conforms to the laws of linguistics.

3.2.2. Architecture of LASAS

Table 2 shows the effects of LASAS with different structures.
First, comparing system4 and system5, it makes sense to
change the number of mapping spaces. An increment of N
can signiÔ¨Åcantly improve AR performance, which indicates that
each mapping space learns different accent features. From
system6, we can see, better results can be obtained with richer
acoustic embeddings. Finally, in system7, we replace the
reference from dimension-reduced text to acoustic embedding.
The results of system7 show even still includes accent shift,
using acoustic embedding as reference is not good as LASAS
in system3. It shows that using clear and simple linguistic in-
formation as a reference will allow the model to learn accent
information more easily.

Table 2: AR accuracy of the structure comparison of LASAS.

Space
Num
(N)
8
4
16
8
8

Encoder
Output
Layers
3,6,9
3,6,9
3,6,9
1‚àº12
3,6,9

ID

3
4
5
6
7

Reference

Text
Text
Text
Text
Acoustic

Dev
(%)

84.05
83.46
85.42
84.63
81.47

Test
(%)

75.64
74.22
77.16
78.19
73.3

3.2.3. Comparison of LASAS Versus Direct Concatenation

Considering the introduction of linguistic information may im-
prove AR performance even without LASAS, we conducted
comparative experiments (system8 ‚àº 15) by directly concate-
nating (DC) the text vector and acoustic embedding as the bi-
modal representation in Fig. 1. As shown in Fig. 4, when us-
ing 1-3 layers of encoder output as acoustic embeddings, both
methods are less effective. When 4-6 layers were used, LASAS
signiÔ¨Åcantly outperforms DC and achieves the best results. Af-
ter that, with deeper layers are used, the two approaches get
closer. This is because the middle layers of the encoder contain
more human-voice information, which is required by LASAS.
The deeper layers contain too much linguistic information but
little related to accents, which leads the performance degener-
ation when used for LASAS. Overall, LASAS can more fully
utilize linguistic-acoustic bimodal information than DC.

Figure 4: LASAS Comparison with direct concatenation (DC)
the acoustic embedding and text vector.

3.2.4. Comparison of Final System Versus AESRC Top-N

Table 3 shows the results of LASAS and top-level schemes
In system16 and system17,
in the AESRC challenge [7].
phone posteriorgram (PPG) feature extracted from a TDNN-
In ad-
based ASR model is used as inputs for AR models.
dition, system16 used model fusion and many kinds of data
augmentation methods including TTS. Without model fusion
and comprehensive data augmentation (except SpecAug), our
schemes system20 ‚àº 22 achieve similar performance with
In system18, researchers trained
system17 in the Dev set.
a hybrid CTC/attention-based ASR model and made the model
learned to predict text and accent at the same time by trans-
In system19, an ASR-AR multitask architec-
fer learning.
ture is used. And the ASR-generated features are used for the
AR task, which contained sufÔ¨Åcient implicit linguistic informa-
tion. Our schemes system20 ‚àº 22 are signiÔ¨Åcantly surpassing
system18 and system19, which fully proves the advantages
of LASAS.

Table 3: AR accuracy of LASAS and AESRC top-level schemes.
Real and ASR in the table represent using real text or
ASR-generated text, respectively. Details of the ASR model are
mentioned in Section 3.1.

ID

16
17
18
19
20
21
22

System

Top1: AR (PPG) + Data Aug (TTS)
Top1 w/o Data Aug
Top2: AR+ASR Fusion
Top3: AR+ASR Multitask

LASASf inal:
N=16,
Enc=(4‚àº9)L

Realtrain+Realtest
Realtrain+ASRtest
ASRtrain+ASRtest

Dev
(%)
91.3
84.51
80.98
81.1
85.39
85.12
84.88

Test
(%)
83.63
-
72.39
69.63
77.79
77.18
77.42

In system20 ‚àº 22, we further optimize the model accord-
ing to the conclusions in Section 3.2.2 and Section 3.2.3 by us-
ing 4-9 layers‚Äô outputs of the Conformer encoder and setting the
number of mapping spaces to 16. Then, we investigate the im-
pact of text reliability on LASAS. As expected, in system20,
LASAS achieves the best results when using manually anno-
tated transcripts (real text). If using real text for training, and
ASR-generated text in the test stage, the accuracy of the Dev
and Test sets drops by 0.32% and 0.78%, respectively. If in the
training stage we also use ASR-generated text, this gap may be
alleviated, and the respective accuracy of system22 on the Test
set drops by only 0.48%. This means that the LASAS has high
robustness to text errors. Since LASAS integrates the informa-
tion of the whole utterance, it can maintain good performance
even if some characters are wrong. At last, system22 achieves
84.88% and 77.42% accuracy on the Dev and Test set, obtain-
ing a 4.81% and 6.94% relative improvement over system18.

4. Conclusions

In this paper, we propose a LASAS-based AR model. We ex-
tract accent shift by LASAS block and concatenate it with a
text reference to form a bimodal representation for AR tasks.
Experiments show that our method effectively improve AR per-
formance. We visualize the accent shift and show its rationality.
Furthermore, our scheme signiÔ¨Åcantly outperforms the method
that directly concatenates acoustic embedding and text vector,
and also shows superior performance in the AESRC challenge
dataset. In the future, we hope to improve LASAS for accent
ASR tasks or text-dependent speaker veriÔ¨Åcation tasks.

                       ( Q F R G H U  2 X W S X W  / D \ H U                 $ F F X U D F \     / $ 6 $ 6  ' ( 9   ' &  ' ( 9 / $ 6 $ 6  7 ( 6 7   ' &  7 ( 6 7[23] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, ≈Å. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù
Advances in neural information processing systems, 2017.

[24] R. Sennrich, B. Haddow, and A. Birch, ‚ÄúNeural machine transla-
tion of rare words with subword units,‚Äù in Proc. ACL, 2016.

[25] M.-T. Luong, H. Pham, and C. D. Manning, ‚ÄúEffective approaches
to attention-based neural machine translation,‚Äù in Proc. EMNLP,
2015.

[26] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, ‚ÄúLib-
rispeech: An ASR corpus based on public domain audio books,‚Äù
in Proc. ICASSP, 2015, pp. 5206‚Äì5210.

[27] B. Zhang, D. Wu, C. Yang, X. Chen, Z. Peng, X. Wang, Z. Yao,
X. Wang, F. Yu, L. Xie et al., ‚ÄúWenet: Production Ô¨Årst and pro-
duction ready end-to-end speech recognition toolkit,‚Äù in Proc. In-
terspeech, 2021.

5. References
[1] R. Lippi-Green, English with an accent: Language, ideology, and

discrimination in the United States. Routledge, 2012.

[2] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudan-
pur, ‚ÄúX-vectors: Robust DNN embeddings for speaker recogni-
tion,‚Äù in Proc. ICASSP, 2018, pp. 5329‚Äì5333.

[3] M. A. T. Turan, E. Vincent, and D. Jouvet, ‚ÄúAchieving multi-
accent ASR via unsupervised acoustic model adaptation,‚Äù in Proc.
Interspeech, 2020.

[4] A. Hanani and R. Naser, ‚ÄúSpoken Arabic dialect recognition using
x-vectors,‚Äù Natural Language Engineering, pp. 691‚Äì700, 2020.

[5] S. Shon, A. Ali, Y. Samih, H. Mubarak, and J. Glass, ‚ÄúADI17:
A Ô¨Åne-grained Arabic dialect identiÔ¨Åcation dataset,‚Äù in Proc.
ICASSP, 2020, pp. 8244‚Äì8248.

[6] S. A. Chowdhury, A. M. Ali, S. Shon, and J. R. Glass, ‚ÄúWhat
does an end-to-end dialect identiÔ¨Åcation model learn about non-
dialectal information?‚Äù in Proc. Interspeech, 2020, pp. 462‚Äì466.

[7] X. Shi, F. Yu, Y. Lu, Y. Liang, Q. Feng, D. Wang, Y. Qian, and
L. Xie, ‚ÄúThe accented English speech recognition challenge 2020:
Open datasets, tracks, baselines, results and methods,‚Äù in Proc.
ICASSP, 2021, pp. 6918‚Äì6922.

[8] Z. Zhang, Y. Wang, and J. Yang, ‚ÄúAccent recognition with hybrid

phonetic features,‚Äù Sensors, 2021.

[9] J. Zhang, Y. Peng, P. Van Tung, H. Xu, H. Huang, and E. S. Chng,
‚ÄúE2E-based multi-task learning approach to joint speech and ac-
cent recognition,‚Äù arXiv preprint arXiv:2106.08211, 2021.

[10] Q. Gao, H. Wu, Y. Sun, and Y. Duan, ‚ÄúAn end-to-end speech
accent recognition method based on hybrid CTC/attention trans-
former ASR,‚Äù in Proc. ICASSP, 2021, pp. 7253‚Äì7257.

[11] M. H¬®am¬®al¬®ainen, K. Alnajjar, N. Partanen, J. Rueter et al., ‚ÄúFinnish
dialect identiÔ¨Åcation: The effect of audio and text,‚Äù in Proc.
EMNLP, 2021.

[12] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, ‚ÄúWav2vec 2.0:
A framework for self-supervised learning of speech representa-
tions,‚Äù Advances in Neural Information Processing Systems, pp.
12 449‚Äì12 460, 2020.

[13] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‚ÄúBert: Pre-
training of deep bidirectional transformers for language under-
standing,‚Äù in Proc. NAACL-HLT, 2019, pp. 4171‚Äì4186.

[14] J. C. Wells and J. C. Wells, Accents of English: Volume 1. Cam-

bridge University Press, 1982.

[15] M. Tjalve and M. Huckvale, ‚ÄúPronunciation variation modelling

using accent features,‚Äù in Proc. EuroSpeech, 2005.

[16] A. Lee, Y. Zhang, and J. Glass, ‚ÄúMispronunciation detection via
dynamic time warping on deep belief network-based posterior-
grams,‚Äù in Proc. ICASSP, 2013, pp. 8227‚Äì8231.

[17] A. Lee and J. Glass, ‚ÄúPronunciation assessment via a comparison-
based system,‚Äù in Speech and Language Technology in Education,
2013.

[18] Y. Xiao, F. K. Soong, and W. Hu, ‚ÄúPaired phone-posteriors ap-
proach to ESL pronunciation quality assessment,‚Äù in Proc. Inter-
speech, 2018.

[19] Y. Wang, Y. Shen, Z. Liu, P. P. Liang, A. Zadeh, and L.-P.
Morency, ‚ÄúWords can shift: Dynamically adjusting word repre-
sentations using nonverbal behaviors,‚Äù in Proc. AAAI, 2019, pp.
7216‚Äì7223.

[20] W. Rahman, M. K. Hasan, S. Lee, A. Zadeh, C. Mao, L.-P.
Morency, and E. Hoque, ‚ÄúIntegrating multimodal information in
large pretrained transformers,‚Äù in Proc. ACL, 2020, p. 2359.

[21] M. Nicolao, A. V. Beeston, and T. Hain, ‚ÄúAutomatic assessment
of English learner pronunciation using discriminative classiÔ¨Åers,‚Äù
in Proc. ICASSP, 2015, pp. 5351‚Äì5355.

[22] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han,
S. Wang, Z. Zhang, Y. Wu et al., ‚ÄúConformer: Convolution-
augmented transformer for speech recognition,‚Äù in Proc. Inter-
speech, 2020.

