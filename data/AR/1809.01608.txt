A Scalable Strategy for the Identiﬁcation of
Latent-variable Graphical Models

Daniele Alpago, Mattia Zorzi, Augusto Ferrante

1

8
1
0
2

p
e
S
5

]

C
O
.
h
t
a
m

[

1
v
8
0
6
1
0
.
9
0
8
1
:
v
i
X
r
a

Abstract—In this paper we propose an identiﬁcation method
for latent-variable graphical models associated to autoregressive
(AR) Gaussian stationary processes. The identiﬁcation procedure
exploits the approximation of AR processes through stationary
reciprocal processes thus beneﬁting of the numerical advantages
of dealing with block-circulant matrices. These advantages be-
come more and more signiﬁcant as the order of the process
gets large. We show how the identiﬁcation can be cast in a
regularized convex program and we present numerical examples
that compares the performances of the proposed method with
the existing ones.

Index Terms—Latent-variable graphical models, Reciprocal
processes, Maximum likelihood, Maximum entropy, Regulariza-
tion, System identiﬁcation.

I. INTRODUCTION

T HE ideas behind graphical models have their origins in

several scientiﬁc areas, such as statistical physics and
genetics back at the beginning of the last century. However,
only recent developments of such ideas allowed to employ
graphical models in identiﬁcation problems involving high
dimensional data [1], [2], [3], [4], [5], [6], [7], [8]. In this
direction, particularly useful are sparse graphical models, i.e.
graphs with few edges that describe the interactions between
a large number of variables. Such models have become very
popular in the literature in the recent years because, beside giv-
ing a concise representation of the phenomenon under scruting,
sparsity implies a limited number of model’s parameters thus
avoiding overﬁtting in the identiﬁcation procedure.
Although the latter is a desirable property, enforcing sparsity
in the identiﬁcation procedure is not always the best choice, as
it may prevent a sufﬁciently rich description of the underlying
phenomenon. Indeed, in many practical situations, the pres-
ence of a few common, hidden behaviors between the variables
of interest explaining the most part of the interactions between
the observed variables can be crucial. The fact that a sparse
graphical model is not able to describe the essential features of
this kind of phenomena motivates the introduction of the so-
called latent-variable graphical models. The latter consist in
a two-layer graph where the conditional dependence relations
between the observed variables are mainly due to the latent
variables (i.e. variables not accessible to observations): each
latent-variable (on the top-layer) is connected to the majority

of

and A.

D. Alpago, M. Zorzi
Information

partment
Padova,
(D.
augusto@dei.unipd.it (A. Ferrante)

Engineering,

Alpago)

email:

Italy;

Ferrante

are with
of

the De-
Padova,
alpagodani@dei.unipd.it
Zorzi)

University

(M.

zorzimat@dei.unipd.it

of the observed variables (on the bottom-layer), making the
latter a sparse subgraph. Since the number of latent-variables is
small, the overall graph has a reduced number of edges. In the
simplest possible setting, one can associate this kind of models
to a Gaussian random vector [8]. The particular graphical
structure translates in a sparse plus low-rank decomposition of
its concentration matrix. In [8] the identiﬁcation of the sparse
and the low-rank part of the concentration matrix has been cast
in a regularized maximum-likelihood optimization problem.
A dynamic version of this problem, i.e. the identiﬁcation of
latent-variable graphical models for AR Gaussian processes,
has been considered in [9] where the problem has been shown
to be strictly connected to a maximum-entropy problem. As
showed in [10], this identiﬁcation problem can be effectively
solved by an ADMM-type algorithm. The optimization pro-
cedure, however, involves the inversion and the eigenvalue
decomposition of matrices whose dimension is proportional
to the product of the order of the process by the dimension of
the process, making the procedure numerically critical when
the order of the AR process is high, as it happens, for example,
when the AR process is an approximation of an ARMA one.
In this paper we consider the problem of identifying latent-
variable graphical models for stationary Gaussian reciprocal
processes. The latter are periodic stationary processes [11],
[12], [13], [14], [15], [16], [17] and they have been proven
to be a worthy approximation of Gaussian AR processes,
provided that the period N is sufﬁciently large [18], [16].
We will show that the proposed identiﬁcation procedure is in
fact an approximation of the maximum entropy and maximum
likelihood identiﬁcation paradigms proposed for the classical
AR processes. The fact that stationary reciprocal process can
be modeled by means of block-circulant matrices represents a
big numerical advantage as the inversion and the eigenvalue
decomposition of such matrices can be performed robustly
[19] making the proposed procedure attractive also for the
identiﬁcation AR processes of high order and hence for
ARMA processes.
The paper is organized as follows: In Section II we ﬁx the
notation and we recall the fundamental results used in the
rest of the paper. In Section III we introduce reciprocal pro-
cesses and we explain how they are related to AR processes.
In Section IV we characterize graphical models associated
to reciprocal processes while, in Section V, we propose a
convex optimization problem for the identiﬁcation of such
models. Section VI is devoted to the ADMM formulation of
the optimization problem and Section VII reports numerical
experiments concerning the implementation of the proposed
procedure. Finally, in Section VIII we draw the conclusions.

 
 
 
 
 
 
II. NOTATION AND BACKGROUND

where ζ := ei 2π

N is the N-th root of unity.

2

∈

[
−

In this paper we will deal both with real matrices and with
matrix-valued functions deﬁned on the unit-circle T :=
eiθ :
{
π, π]
. For such functions we will omit the dependence
θ
}
on θ when it is clear from the context, i.e. we will write F in
place of F(eiθ ). The rank of a matrix G is denoted by rank(G)
while the (normal) rank of any Cp
p-valued analytic function
×
F deﬁned on T, is deﬁned as

rank(F) := max

rank(F(eiθ )).

(1)

θ

π,π]

[
−
∈
In the same fashion, the following notations will be used
indifferently in the case that G is a Cp
p-valued function
×
deﬁned on T or a square constant matrix: G(cid:62) denotes the
Cp
transpose of G, G∗ its transpose-conjugate and diag(G)
denotes the vector whose entries are the diagonal elements
of G. ker(G) indicates the kernel of G. G > 0 and G
0
denote that G is a positive deﬁnite and, respectively, positive
1 denote the trace of G, the
semideﬁnite. tr(G), det(G) and G−
determinant of G and its inverse, respectively. Ip denotes the
identity matrix of order p.
We deﬁne the cone

≥

∈

Sp :=

F
{

∈

Hp : Φ

α Ip ≥

−

0 a.e. on T, for some α > 0
}

,

where Hp is the space of square integrable coercive functions
deﬁned on the unit circle and taking values in the space of p
p Hermitian matrices. For any F
the notations

×
Hp we will use equivalently

∈
dθ
2π

,

(cid:90) π

π

−

F(eiθ )

(cid:90)

F

for the integral of F over [
π, π] with respect to the normal-
ized Lebesgue measure on T. We deﬁne also the family of
matrix pseudo-polynomials

−

Qp,n :=

(cid:40) n
∑
k=
n
−

Qk eiθ k, Q

k = Q(cid:62)k ∈

−

(cid:41)

Rp

×

p

.

⊂

x : x1 < x < x2}
{

of an interval
R, we denote with (x1, x2)c the complement set of

For any sub-interval (x1, x2) :=
(a, b)
(x1, x2) in (a, b). E[
] denotes the expectation operator.
·

In this paper we will always consider AR processes of
order n and reciprocal processes of period N, i.e. completely
speciﬁed in a ﬁnite interval of length N. All such processes
are understood with zero mean throughout the paper. It will
be always assumed that N > 2n and that N is an even number.
The case with N odd can be dealt in a similar way. We deﬁne
mN of the (real) symmetric, block-
the vector space C
×
circulant matrices

RmN

⊂

C = circ

C0,C1, . . . ,C N
{

2 −

1,C N

2

,C(cid:62)N

1
2 −

,
, . . . ,C(cid:62)1 }

1,C N

whose ﬁrst block-column is composed by the m
C0,C1, . . . ,C N
1
2 −
C, D
with the inner product
(cid:104)
block-circulant matrix C
polynomial

m blocks
, . . . ,C(cid:62)1 . The space C is endowed
C := tr(C(cid:62)D). The symbol of the
(cid:105)
C is deﬁned as the m
m pseudo-

,C(cid:62)N

2 −

×

×

∈

2

Φ(ζ ) :=

N
1
−
∑
k=0

Ck ζ −

k,

k for k >
with Ck = C(cid:62)N
−

N
2

,

(2)

Proposition 1: Let C be a block-circulant matrix with

symbol Φ(ζ ) deﬁned by (2). Then

C = F∗diag (cid:8)

Φ(ζ 0), Φ(ζ 1),

, Φ(ζ N

1)(cid:9)
−

F,

(3)

· · ·

where F is the (Fourier) unitary block-matrix








0I
·
0I
·

0
ζ −
1
ζ −
...

F =

1
√N

1
·
1I
·

0
ζ −
1
ζ −
...

(N

ζ −

1)
0I
·

−

(N

ζ −

1)
1I
·

−

1)I
−
1)I
−

0
ζ −
1
ζ −

(N
·
(N
·

...








.

(N

ζ −

(N
1)
·

−

1)I
−

· · ·
· · ·
. . .

· · ·

This is a classical result in the scalar case; technical details for
the block-circulant case can be found, for instance, in [20, page
C of symmetric, banded block-
6]. We deﬁne the subspace B
mN matrices of bandwidth n, with N > 2n,
circulant mN
containing the matrices of the form

⊆

×

B = circ

B0, B1,
{

· · ·

, Bn, 0,

, 0, B(cid:62)n ,

,
, B(cid:62)1 }

· · ·

· · ·

(4)

that inherits the inner product deﬁned on C. Note that, accord-
B is
ing to deﬁnition (2), the symbol of a banded matrix B

∈

Ψ(ζ ) =

n
∑
k=
n
−

Bk ζ −

k,

B

k = B(cid:62)k .
−

The projection operator PB : C

B is deﬁned as

PB(C) := circ

C0,C1,
{

· · ·
, the projection operator PΩ :
}
C is deﬁned such that PΩ(C) is a block-circulant matrix

(i, j) : i, j = 1, . . . , m

Given Ω =
C
whose blocks have support Ω.

· · ·

· · ·

→

{

, 0,C(cid:62)n ,

,C(cid:62)1 }

.

→
,Cn, 0,

III. RECIPROCAL PROCESSES

{

Let

y(k), k = 1, 2, . . . , N

, be an m-dimensional Gaussian
}
stationary stochastic process deﬁned on a ﬁnite interval [1, N].
Rm,
For k = 1, . . . , N, we have y(k) := [y1(k) . . . ym(k)](cid:62) ∈
therefore the process is completely characterized by the
random vector y := [y1(1) . . . ym(1) . . . . . . y1(N) . . . ym(N)](cid:62) ∈
RmN. In [11] it has been shown that y is a restriction of a wide-
sense stationary periodic process of period N deﬁned on the
whole integer line Z if and only if the mN
mN covariance
matrix Σ of y is symmetric block-circulant:

×

,

−

(5)

Σ = circ

, . . . , Σ(cid:62)1 }
Σ0, Σ1, . . . , Σ N
{
2
where E[y(i)y( j)(cid:62)] = Σi
j, i, j = 1, . . . , N, are the covariance
lags of the process such that Σk = Σ(cid:62)N
k for k > N/2. In view
−
of the above equivalence, we will denote with y both the wide-
sense stationary periodic process deﬁned in the whole line Z
and its restriction, depending on the context. A particular class
of stationary periodic processes is represented by reciprocal
processes.

Deﬁnition 1: y is a reciprocal process of order n on [1, N]
if, for all t1,t2 ∈
[1, N], the random variables of the process
[1, N] are conditionally independent to
in the interval (t1,t2)
the random variables in (t1,t2)c, given the 2n boundary values
1), where the sums
y(t1 −
t
−

k and t + k are to be understood modulo N.

n + 1), . . . , y(t1), y(t2), . . . , y(t2 + n

−

⊂

Theorem 1: A non-singular mN

The following result has been proved in [11, Theorem 3.3]:
it states that a reciprocal process is completely speciﬁed by a
block-circulant matrix whose inverse has a banded structure.
mN-dimensional matrix
Σ is the covariance matrix of a periodic reciprocal process of
order n if and only if its inverse is a positive deﬁnite sym-
metric block-circulant matrix which is banded of bandwidth
1
n, namely Σ−

B.

×

∈

Let ˆΣ0, . . . , ˆΣn be given estimates of the ﬁrst n+1 covariance
lags Σ0, . . . , Σn of the underlying reciprocal process. In view
of Theorem 1, the identiﬁcation of a reciprocal process can be
formulated as the following matrix completion problem.

Problem 1: Given the n + 1 estimates ˆΣ0, . . . , ˆΣn, compute
, in such a way to form a symmetric,

a sequence Σn+1, . . . , Σ N
2
positive deﬁnite block-circulant matrix

, . . . , Σ(cid:62)n+1, ˆΣ(cid:62)n , . . . , ˆΣ(cid:62)1 }
,

Σ = circ

ˆΣ0, . . . , ˆΣn, Σn+1, . . . , Σ N
{
B.

2

∈

1
with Σ−
It has been shown in [16], [11] that a particular solution to
Problem 1 is the one which solves the following maximum
entropy problem:

argmax
C

Σ
subject to

∈

log det Σ

Σ > 0
PB(Σ

ˆΣ) = 0.

−

whose dual problem has been proven to be

log det X + (cid:10)

(cid:11)
X, ˆΣ
C

argmin

X

B
subject to X > 0

∈

−

(6)

(7)

B is the symmetric, banded block-circulant matrix

ˆΣ0, ˆΣ1, . . . , ˆΣn, 0, . . . , 0, ˆΣ(cid:62)n , . . . , ˆΣ(cid:62)1 }
,
{

containing the covariance lags estimated from the data and the
1, i.e. the
optimal value of dual variable X is indeed equal to Σ−
inverse of the solution of (6). Strong duality between (6) and
(7) implies that (6) and (7) are equivalent. In what follows we
assume that ˆΣ > 0 as it is a necessary condition for Problem
(6) to be feasible. In the case that ˆΣ is not positive deﬁnite, we
can consider a positive deﬁnite banded block-circulant matrix
sufﬁciently close to ˆΣ which can be obtained by solving a
structured covariance estimation problem, see [21], [22].

AR approximation: Next we recall how reciprocal processes
can be seen as an approximation of autoregressive (AR)
processes. More precisely, let y :=
be an m-
dimensional, AR, full-rank, Gaussian wide-sense stationary
process of order n,

y(t) : t

Z
}

∈

{

n
∑
k=0

Bk y(t

−

k) = e(t),

e(t)

∼

N(0, Im),

Z,

t

∈

(8)

and let Rk := E[y(t)y(t
Z, be its k-th covariance lag.
The spectrum of y is the Fourier transform of the sequence
Rk with k

k)(cid:62)], k

−

∈

∈
Φ(eiθ ) =

Z, i.e.
∞
∑
k=
∞
−

Rk e−

iθ k,

∈

where ˆΣ
of bandwidth n,
ˆΣ = circ

3

Suppose now that T observations y(1), . . . , y(T ) of the process
y are available, and let

ˆRk =

1
T

T
∑
t=k

y(t)y(t

k)(cid:62),

−

k = 0, 1, . . . , n,

(10)

be estimates of the ﬁrst n + 1 covariance lags R0, . . . , Rn. The
identiﬁcation of such a process can be cast to a covariance
extension problem.

ˆR0, . . . , ˆRn, complete
Problem 2: Given n + 1 estimates
them with a sequence Rn+1, Rn+2, . . .
in such a way that
the Fourier transform of the extended (inﬁnite) sequence is
a power spectral density.

A particular solution of Problem 2 is the one proposed by J.
P. Burg in [23]: choose Rn+1, Rn+2, . . . maximizing the entropy
rate of the process, i.e. that solves the following optimization
problem

(cid:90)

(cid:90)

argmax
Sm
Φ
∈

subject to

log det Φ

eiθ k Φ = ˆRk,

k = 0, 1, . . . , n.

(11)

The dual of (11) has been shown to be, see for instance [24]:

argmin
Qm,n
1
Φ−
∈

subject to

ˆΦ(eiθ ) =

where

(cid:90)

−

log det Φ−

1 + (cid:10)

(cid:11)
1, ˆΦ

Φ−

ˆΦ > 0

n
∑
k=
n
−

ˆRk e−

iθ k,

ˆR

k = ˆR(cid:62)k ,
−

(12)

(13)

is the truncated periodogram of the process y. These kind of
problems have been extensively studied and generalized in
the recent years, see for instance [25], [26], [27], [28], [29],
[30], [31].

→

i.e.

We recall

that, for N

∞, Toeplitz matrices can be
approximated arbitrarily well by circulant matrices [32,
Lemma 4.2]; hence, for N
∞, Problem 1 consists in
→
leads to an inﬁnite positive
searching a completion that
deﬁnite block-Toeplitz covariance matrix,
such that
the Fourier transform of the resulting extended sequence
is a power spectral density. By Theorem 3.1 in [18], for
N
∞, Problem (6) is the classical Burg’s maximum entropy
problem whose solution is an AR process of order n. In
light of this observation, we can understand the reciprocal
process associated to the solution of (7) as an approximation
of the AR process solution of the Burg’s maximum entropy
problem (11). In the following sections we will exploit this
approximation for the identiﬁcation of latent-variable AR
graphical models.

→

The reciprocal approximation just explained has also an
interesting interpretation in the frequency domain. Indeed, it
corresponds to sampling the spectrum (9) of the AR process
y, over the interval [
π, π], with sample period 2π/N, thus
obtaining the symbol of the covariance matrix of the corre-
sponding reciprocal process:

−

R

k = R(cid:62)k , θ
−

π, π].

[
−

∈

(9)

Φ(ζ ) =

N
1
−
∑
k=0

Σk ζ −

k,

k for k >
Σk = Σ(cid:62)N
−

N
2

.

Figure 1 illustrates this relation. According to Proposition
1, the covariance matrix Σ of the reciprocal process y that
approximates y writes as

Σ = F∗circ

Φ(ζ 0), Φ(ζ 1), . . . , Φ(ζ N
{

1)
F,
−
}

(14)

hence, its inverse

xi is conditionally independent from x j given the remaining
random variables xk, k

= i, j, i.e.

4

x j | {
if and only if the element ki j in position (i, j) of the concen-
tration matrix K is equal to zero. Formally,

xi ⊥⊥

xk}k

(16)

=i, j,

Σ−

1 = F∗circ

Φ(ζ 0)−
{

1, Φ(ζ 1)−

1, . . . , Φ(ζ N

1
1)−
−

F,

}

(15)

can be robustly computed by inverting the N blocks
Φ(ζ 0), Φ(ζ 1), . . . , Φ(ζ N
m. As a ﬁnal
remark, we recall that eigevalues and eigenvectors of circulant
matrices can be robustly computed as well, thanks to the
availability of closed-form formulas, see for instance [32].

1), all of size m
−

×

Fig. 1: Spectrum Φ(eiθ ) and its sampled version Φ(ζ ) with
N = 12 samples.

As highlighted by the frequency-domain interpretation, the
goodness of the approximation depends on the regularity of the
spectrum: the larger is the rate of variation of the spectrum, the
larger N has to be chosen in order to get a good approximation
of the AR process. The frequency-domain interpretation makes
even more explicit the relationship between Burg’s maximum
the
entropy problem (11) and Problem (6): provided that
number of samples N is sufﬁciently large, by sampling the
spectrum solution of (11) we obtain an approximation of
the symbol of Σ
the matrix Σ solution of (6); viceversa,
can be extended over the whole interval [
π, π] in order to
approximate the solution of (11). Figure 2 summarizes this
bi-directional relationships.

−

Fig. 2: Schematic representation of the reciprocal approxima-
tion of the AR process in terms of solutions of problems (11)
and (6).

IV. GRAPHICAL MODELS

Consider a Gaussian random vector x

N(0, Σ) taking
values in Rm, where Σ = Σ(cid:62) > 0 so that the concentration
1 is well-deﬁned. If we denote the
matrix K = [ki j] := Σ−
= j, we have that
components of x as x1, . . . , xm, for any i

∼

ki j = 0.

xi ⊥⊥

x j | {

xk}k

=i, j ⇐⇒
The previous relation allows to construct an undirected graph
G = (V, E), with V =
V , associated
to the random vector x by taking the components x1, . . . , xm
of x as nodes and such that the edges reﬂect the conditional
dependence relations between the random variables, i.e.

1, . . . , m
{

and E

⊂

×

V

}

(17)

(i, j) /
∈

E

⇐⇒

xi ⊥⊥

x j | {

xk}k

=i, j.

(18)

The graph G is called the graphical model associated to
x and it gives a visual representation of the conditional
dependence relations between the components of x. Observe
that G is completely characterized by the sparsity pattern of
the concentration matrix of the random vector.

A characterization of conditional independence can be given
also in the dynamic setting. In particular, we consider an m-
dimensional, Gaussian, wide-sense stationary AR process x
V , deﬁne
described by a model like (8). For any index set I

XI := span
{

x j(t) : j

I, t

∈
as the closure of the set containing all
combinations of the variables x j(t). For any i

∈

,

Z
}
the ﬁnite linear
= j, the notation

⊂

j

}

\{

i, j

V

} |

XV

} ⊥⊥

X
i
{

xk(t), k
{

X
{
generalizes (16) and it means that for all t1,t2, xi(t1) and
x j(t2) are conditionally independent given the space linearly
Z
i, j
. One can prove that
generated by
∈
}
[Φ(eiθ )−

} |
π, π], see [2], [3], which is the natural general-
for any θ
ization of (17). Accordingly, we can construct the undirected
graph G = (V, E) representing the conditional dependence
relations between the components of the process x by deﬁning
the set of edges as follows:

X
i
} ⊥⊥
{

1]i j = 0,

} ⇐⇒

,t
}

X
{

[
−

(19)

XV

\ {

∈

∈

i, j

\{

j

(i, j) /
∈

E

⇐⇒

X
i
} ⊥⊥
{

X
{

j

} |

XV

i, j

\{

.

}

(20)

In this framework, the graph G is completely characterized by
the sparsity pattern of the inverse power spectral density of the
process. Identiﬁcation of sparse graphical models of reciprocal
processes have been studied in [33].
In many practical situations there is the presence of a few
common, latent, behaviors between the variables of interest
that are responsible of the most part of the interactions
between the observed variables and that cannot be captured
by considering only a sparse model structure. This leads to
a particular type of graphical models called latent-variable
graphical models or sparse plus low-rank graphical models,
[8]. Such models admit a two-layer graphical structure in
which the nodes in the upper layer stand for the (few) latent-
variables, while the nodes in the bottom layer represent the
observed variables.

θ−π=0N−1π2πNΦ(eiθ)Φ(ζ)Φ(eiθ)>0solutionof(11)Φ(ζ)symbolofΣ>0,Σ∈Csolutionof(6)Φ−1∈Qm,nARprocessyΣ−1∈Breciprocalprocessysamplingextensionover[−π,π]samplingextensionover[−π,π](cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:54)
Latent-variable graphical models associated to Gaussian ran-
dom vectors have been considered in [8] and then generalized
in [9] to AR stochastic processes. The latter, say z :=
∈
Z
, is assumed to be of the form z = [y(cid:62) x(cid:62)](cid:62) where y is the
}
Rm-valued process containing the observed variables while x
is the process containing l latent variables. Let Φy denotes the
spectral density of y. Under the assumptions that l
m and the
dependence relations among the observed variables are mostly
through the latent variables, we have the decomposition

z(t), t
{

(cid:28)

1
y = Γ
Φ−

Λ,

−

(21)

where Γ > 0 is sparse and its support reﬂects the conditional
dependencies among the observed variables, while Λ
0 is
low-rank and its rank equals the number l of latent-variables.
We are now ready to extend the previous results for Gaus-
sian reciprocal processes. Let z := [y(cid:62) x(cid:62)](cid:62) be a Gaussian,
periodic, reciprocal process of order n deﬁned on the interval
[1, N], where y plays the role of the m-dimensional observed
process and x is the l-dimensional latent process, respectively.
The covariance matrix Σz of z and its inverse can be partitioned
as

≥

(cid:35)

,

1
z =
Σ−

(cid:34)

(cid:35)

,

S A

A(cid:62) R

(22)

(cid:34)

Σz =

Σyx
Σx

Σy
Σ(cid:62)yx
C and Σx ∈

Cl are the covariance matrices of y
where Σy ∈
and x, respectively. Here, Cl denotes the vector space of block-
circulant, symmetric matrices as C, except that the blocks have
dimension l
l. Applying the Schur complement, we obtain
the relation

×

1
y = S
Σ−

L,

−

(23)

where S > 0 is the concentration matrix of process y condi-
1 A(cid:62). In order to
tioned on x, and L
≥
1
B we assume both
ensure that, according to Theorem 1, Σ−
y ∈
S and L to be symmetric, block-circulant, banded of bandwidth
n, i.e.

0 is deﬁned as L := A R−

S = circ
{
L = circ
{

S0, S1, . . . , Sn, 0, . . . , 0, S(cid:62)n , . . . , S(cid:62)1 }
,
.
L0, L1, . . . , Ln, 0, . . . , 0, L(cid:62)n , . . . , L(cid:62)1 }

(24)

(cid:28)

By construction, the matrix L has rank equal to the number of
latent variables l, therefore under the assumption that l
m, it
is a low-rank matrix. If S is a sparse matrix, then we will refer
1
to (23) as sparse plus low-rank decomposition of Σ−
y which
is the analogue of (21) for reciprocal processes. It remains
to show that an appropriate sparsity pattern of S reﬂects
that the dependence relations among observed variables are
mostly through the few latent variables. For this purpose, let
yi := [yi(1) . . . yi(N)](cid:62), i = 1, . . . , m, be the i-th component of
the process y and let x j := [x j(1) . . . x j(N)](cid:62), j = 1, . . . , l, be
the j-th component of the process x. Although the components
Z, by
of the reciprocal processes are deﬁned for any k
periodicity it is sufﬁcient to impose conditional independence
[1, N]. We assume that the blocks S0, S1, . . . , Sn of
only for k
namely,
S have common support Ω

(i, j) : i, j = 1, . . . , m

∈

∈

(Sk)i j = (Sk) ji = 0,

⊆ {
k = 0, . . . , n,

}
Ωc,

(25)

(i, j)

∀

∈

where Ω is the set of pairs that contains all the (i, i), i =
1, . . . , m. By property (17), equation (25) is equivalent to

5

⊥⊥

yi(t1)

y j(t2)

= i, j, s = 1, . . . , N,
= t1, y j(s2), s2 (cid:54)
[1, N] and for any pair (i, j)
∈

= t2, x
Ωc.
Proposition 2: Condition (26) is equivalent to

yh(s), h
| {
yi(s1), s1 (cid:54)

for any t1, t2 ∈

(26)

,
}

| {

y j(t2)

yh(s), h

= i, j, s = 1, . . . , N, x
}

yi(t1)
⊥⊥
for any t1, t2 ∈
Proof: The proof exploits basic results of the theory
of Hilbert spaces of second-order random variables, see for
instance [34, Chapter 2]. First of all, let

[1, N] and for any (i, j)

Ωc.

(27)

∈

ε :=

(cid:21)

(cid:20)ε i
ε j

=

(cid:21)

(cid:20)

yi
y j

E

−

(cid:20)(cid:20)

yi
y j

(cid:21) (cid:12)
(cid:12)
(cid:12)
(cid:12)

yh(s), h

(cid:21)
= i, j, s = 1, . . . , N, x

denotes the error affecting the projection of [y(cid:62)i y(cid:62)j ](cid:62) onto the
= i, j, s = 1, . . . , N, x
subspace generated by
, for any
}
Ωc. It can be shown that ε is
t1, t2 ∈
∈
a zero-mean, Gaussian, random vector. Accordingly, proving
(27) is equivalent to prove that

[1, N] and for any (i, j)

yh(s), h
{

(cid:105)

E

= 0

(cid:104)
ε i ε (cid:62)j
Ωc, [34]. Let now Π
for any t1, t2 ∈
be a permutation matrix that permutes the rows of z = [y(cid:62) x(cid:62)](cid:62)
in order to obtain

[1, N] and for any (i, j)

ε j(t2)

ε i(t1)

⇐⇒

(28)

⊥⊥

∈

¯z := Π z =







yi
y j

yh

=i, j
x







(cid:20) ¯z1
¯z2

=

(cid:21)

,

=i, j

is the vector containing the random variables
= i, j, s = 1, . . . , N. We partition the covariance matrix

where yh
yh(s), h
Σ¯z of ¯z as

(cid:34)

Σ¯z =

Σ¯z1
Σ¯z2 ¯z1

Σ¯z1 ¯z2
Σ¯z2

(cid:35)

,

where Σ¯z1 and Σ¯z2 are the covariance matrices of ¯z1 and ¯z2,
respectively. It is well known that its inverse can be partitioned
conformably as

1
1
z Π(cid:62) =
¯z = Π Σ−
Σ−

(cid:34) ¯S

∗

(cid:35)

,

∗

∗

where

¯S := (cid:0)

Σ¯z1 −

1
¯z2 Σ¯z2 ¯z1
Σ¯z1 ¯z2Σ−

(cid:1)−
1

(29)

is a permuted version of matrix S, according to the permutation
matrix Π. By construction, the Schur complement formula
applied on Σ¯z gives

Σε = Σ¯z1 −

1
Σ¯z1 ¯z2Σ−

¯z2 Σ¯z2 ¯z1 = ¯S−
1,

(30)

that relates the covariance matrix Σε of the projection error ε to
the covariance matrix Σ¯z of ¯z. Condition (25) is equivalent to
say that ¯S, and therefore ¯S−
1, is block-diagonal. Accordingly,
by (30), Σε is block-diagonal, i.e. ε i and ε j are independent,
which is equivalent to (28) as we wanted to prove.

The above result reﬂects the fact that the random variables
yi(s1), y j(s2), s1 (cid:54)
do not play any role in the
{

= t1, s2 (cid:54)

= t2}

(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:54)
conditioning (26). Moreover, the group-sparsity condition (25)
translates in the fact that the conditional dependence relations
between the observed variables are mainly due to the few latent
variables. Accordingly, condition (27) represents the reciprocal
1
counterpart of condition (17). We conclude that Σ−
in (22)
z
together with (25) deﬁne an undirected graph for the Gaussian
random vector z which admits a two-layer structure where

- The nodes in the upper-layer represent the l variables of
the latent-process x1, . . . , xl while the nodes in the bottom-
layer represent the m variables of the observed process
y1, . . . , ym.

- The edges are given by the entries of the concentration
. In particular, the edge (i, j), between two

1
matrix Σ−
z
vectors yi and y j, i

= j, is described by

[(S0)i j (S1)i j . . . (Sn)i j 0 . . . 0 (Sn) ji . . . (S1) ji] .

Example 1: Consider the case in which N = 2, m = 7, l = 2,
and suppose that the graphical model associated to the vector
z is the one depicted in Figure 3.

Fig. 3: Example of a latent-variable graphical model: x1, x2
are the latent-variables and y1, y2, . . . , y7 are the manifest
variables.

In this case, the concentration matrix of vector z will have the
structure (22) with

(cid:34)

S =

(cid:35)

,

S0
S1

S(cid:62)1
S0

S0, S1 ∈

R7

7,
×

×

and R is a 2
2 matrix. The presence of an edge between y2
and y4 implies that at least one of the two elements (S0)24
and (S1)24 is different from zero. Similar arguments holds for
the edge between y5 and y6. Thus, Ω =
(2, 4), (5, 6)
}
{

(i, i) : i = 1, . . . , 7
{

} ∪

.

V. IDENTIFICATION OF LATENT-VARIABLE
RECIPROCAL GRAPHICAL MODELS

The problem of identifying a latent-variable graphical model
associated to a Gaussian random vector has been ﬁrstly
considered in [8] where the solution is obtained by solving a
regularized maximum likelihood problem. In [9] the problem
has been extended to a dynamic setting, by considering an
AR Gaussian process. More precisely, in [9] a regularized
version of Problem (12) that relies on the sparse plus low-

6

rank decomposition of the inverse of the observed spectrum
in (21), has been considered:

(cid:90)

argmin
Qm,n
Γ, Λ
∈

log det(Γ

−

Λ) +

Γ
(cid:104)

−

Λ, ˆΦy(cid:105)

−

(Λ)

(31)

subject to

+ γS φ1(Γ) + γL φ
∗
Γ

Λ > 0
0.

Λ

−

≥

∗

Here, γS, γL > 0 are the regularization parameters that balance
inducing sparsity
the effects of the two regularizers φ1 and φ
and low-rank on Γ and Λ, respectively, while ˆΦy is the trun-
cated periodogram of the observed process y. In this section
we propose a procedure for the identiﬁcation of a latent-
variable graphical model associated to an AR Gaussian process
that exploits the approximation of an AR process through
a reciprocal process in the sense explained in Section III.
Recalling that a latent-variable graphical model of a reciprocal
process is characterized by (23),
the system identiﬁcation
problem can be stated as follows.

Problem 3: Consider an m-dimensional AR process y and
let ˆR0, . . . , ˆRn be the estimates of the ﬁrst n + 1 covariance lags
of y computed as in (10). Set Σ0 := ˆR0, . . . , Σn := ˆRn. Compute
the blocks Σn+1, . . . , Σ N
of the block-circulant covariance ma-
2
trix Σy = circ
Σ0, Σ1, . . . , Σ N
such that
1
{
2 −
1
y = S
0 are as in (24) with
Σ−
S0, . . . , Sn having the smallest possible common support Ω, as
in (25), and the rank of L is as small as possible.

1, Σ N
2
L, where S > 0 and L

, . . . , Σ(cid:62)1 }

, Σ(cid:62)N

2 −

−

≥

We stress the fact that only samples of the observed pro-
cesses are available. Clearly, the matrix Σy solving Problem 3
is the covariance of the reciprocal process y approximating the
observed process y. Since we are going to identify a model
for a reciprocal process, we can exploit the maximum entropy
dual problem (7) recalled in Section III. It is worth noting
that the support Ω is not known in advance, thus it has to be
estimated from the data. In order to do that, inspired by [4],
we consider the following regularizer
(cid:26)

h∞(S) = ∑
k>h

max

(S0)hk|
|

, 2 max

j=1,...,n |

(S j)hk|

, 2 max

j=1,...,n |

(cid:27)
(S j)kh|

.

The latter is a generalization of the (cid:96)∞-norm used to induce
group-sparsity on vectors, and it is used to enforce on S the
group sparsity in (25). The trace (as a tractable proxy of the
nuclear norm) is used instead for inducing low-rankness in L.
Therefore, the paradigm for the estimation of the sparse plus
1
y now
low-rank decomposition of the concentration matrix Σ−
directly follows from (7) by setting X = S
0,
L, with L
−
and by adding the regularizers just introduced:
L) + (cid:10) ˆΣy, S

log det(S

(cid:11)
L
C

≥

−

−

argmin
S,L

B −
∈

(32)

subject to

+ λS h∞(S) + λL tr(L)
S

L > 0, L

0

≥

−

where λL, λS > 0 are the two regularization parameters and

ˆΣy = circ
{

ˆR0, ˆR1, . . . , ˆRn, 0, . . . , 0, ˆR(cid:62)n , . . . , ˆR(cid:62)1 }

is the symmetric, banded block-circulant matrix of band-
width n, containing the covariance lags estimated from the

y1y2y3y4y5y6y7x1x2(cid:54)
observations. As a further motivation, observe that Problem
(32) is precisely the reciprocal counterpart of Problem (31)
considered in [9]. By replacing S with X := S
L, it becomes
log det(X) + tr( ˆΣy X) + λS h∞(X + L) + λL tr(L)

−

argmin
X,L

B −
∈

subject to X > 0, L

0.

≥

(33)
We address the previous constrained optimization problem
using the Lagrange multipliers theory. In doing that we add a
new dummy variable Y

log det(X) + tr( ˆΣy X) + λS h∞(Y) + λL tr(L)

−

argmin

C
B

X
∈
Y,L
∈

subject to X > 0, L
Y = X + L.

0

≥

(34)

L(X, Y, L, V, Z) =

The Lagrangian function for this problem is
(cid:11)
C + λS h∞(Y)
Z, X + L
Y
(cid:104)

log det(X) + (cid:10) ˆΣy, X
C +
V, L
(cid:105)

−
+ λL tr(L)

where, V
simple computations we have

B, because L

∈

∈

0, while Z

∈

≥

− (cid:104)
B, and V

C
(cid:105)
−
(35)
C. After

L(X, Y, L, V, Z) =

(cid:11)
C

log det(X) + (cid:10) ˆΣy + Z, X
−
+
λLImN −
(cid:104)
+ λS h∞(Y)

V + Z, L
C
(cid:105)
C .
Z, Y
(cid:105)

− (cid:104)

The dual objective function is the inﬁmum over X, Y and L
of the Lagrangian. The unique term on L that depends on Y is
λS h∞(Y)
C. The latter is bounded below if and only
(cid:105)
if

Z, Y

− (cid:104)

7

If (36), (37), (39) hold, it remains to minimize the strictly
convex function

¯L(X) := inf
Y,L

L =

−

log det(X) + (cid:10) ˆΣy + Z, X

(cid:11)
C

over the cone of the symmetric, positive deﬁnite, banded
C, any
block-circulant matrices. Observe that, for any Z
ˆΣy ∈
B, and for any sequence Xk > 0 converging to a singular
matrix,

∈

¯L(Xk) = ∞.

lim
k
∞
→

Accordingly, we can assume that
the solution lies in the
interior of the cone so that a necessary and sufﬁcient condition
for Xo to be a minimum point for ¯L is that its ﬁrst Gateaux
derivative computed at X = Xo is equal
to zero in every
direction δ X, namely
δ ¯L(Xo; δ X) = tr (cid:2)(cid:0)
−

(cid:1)
1
o + ˆΣy + Z
X−

(cid:3) = 0,

δ X

δ X

∀

C.
∈
(40)

Notice that ¯L is bounded below if and only if

ˆΣy + Z > 0,

(41)

therefore condition (40) is satisﬁed if and only if Xo = ( ˆΣy +
Z)−

1. Hence,

L =

inf
Y,L,X





log det( ˆΣy + Z) + mN,
if (36), (37), (39), (41) hold,

∞

−

otherwise.

Therefore, the dual problem of (32) is

diag(Z j) = 0,
n
∑
j=1 |

(Z0)kh|
|

+

2

j = 0, . . . , n,

(Z j)kh|

+

(Z j)hk| ≤
|

λS
N

,

k > h,

(36)

(37)

argmin
C −
B, Z
V
∈
∈
subject to

V

≥

log det( ˆΣy + Z)

mN

−

(42)

0, (36), (37), (39), (41).

in which case the inﬁmum is zero. Accordingly,
log det(X) + (cid:10) ˆΣy + Z, X
λLImN −
(cid:104)
(36), (37) hold,

(cid:11)
C +

−
if

V + Z, L
C
(cid:105)





L =

inf
Y

∞ otherwise.

−

The only term that depends on L is
Recalling that L, V
operator PB, we have that

V + Z, L
λLImN −
C.
(cid:105)
(cid:104)
B, by using the linearity of the projection

∈

λLImN −
(cid:104)

V + Z, L
C =
(cid:105)

λLImN −
(cid:104)
which is linear in L, and therefore it is bounded below if and
only if

V + PB(Z), L
C
(cid:105)

(38)

λLImN −

V + PB(Z) = 0.

(39)

In this case, the minimum of (38) is zero. Accordingly,





L =

inf
Y,L

log det(X) + (cid:10) ˆΣy + Z, X
(36), (37), (39) hold,

(cid:11)
C

−
if

∞ otherwise.

−

Notice that we can remove the variable V. Indeed, recalling
that V
0. Accordingly, the dual problem takes the form

0, the constraint (39) becomes λLImN + PB(Z) = V

≥

≥

argmin
C

Z
subject to

∈

log det( ˆΣy + Z)

−
(36), (37), (41)
λLImN + PB(Z)

0.

≥

mN

−

(43)

Problem (43) admits a unique solution.

Proposition 3: Under the assumption that ˆΣy ∈
Proof: Deﬁne f (Z) := log det( ˆΣy + Z). Let

B and ˆΣy > 0,

Z
{

Q :=

0 hold

C

(36), (37), (41) and λLImN + PB(Z)

|

∈

≥

}
be the set of constraints of Problem (43). First of all, notice
that constraints (36) and (37) ensure that Q is a bounded subset
Q are bounded by λS/N so
of C. Indeed, the entries of any Z
C < ∞ for any Z
N be a generic
that
Z
(cid:107)
∈
(cid:107)
∈
sequence of elements of Q converging to some ¯Z
C, such
that ˆΣy + ¯Z

Q. Let now (Z(k))k

0 is singular. Then

∈

∈

≥

log det( ˆΣy + Z(k)) = +∞,

lim
k
→

∞ −

and therefore Z(k) is not an inﬁmizing sequence. Hence, we
can restrict the research of the minimum to the closed subset
of Q deﬁned by

¯Q :=

Z
{

∈

C

|

ˆΣy + Z
and λLImN + PB(Z)

εImN, (36), (37)

≥

0 hold

}

≥

with ε > 0 small enough. By what we have shown till now, the
function f is continuous on the compact set ¯Q and therefore it
admits at least one minimum point. Since f is strictly convex,
the minimum is also unique.

Proposition 4: Under the assumption that ˆΣy ∈

Problem (33) admits a solution (Xo, Lo) and Xo is unique.

B and ˆΣy > 0,

Proof: Notice that Problem (33) is a strictly feasible
convex optimization problem (for instance, pick X = ImN and
L = 0). Accordingly, Slater’s condition holds, hence strong
duality holds between (33) and its dual. The strong duality
between problems (33) and (43) and the existence of a unique
optimum Zo for the dual problem (43), imply that there exists
a unique Xo ∈
which solves the
primal problem (33).
It remains to show that there exists an Lo ∈
optimization problem

B so that Xo = (cid:0) ˆΣy + Zo

B that solves the

(cid:1)−
1

8

conditional distribution of y(n+1), y(n+2), . . . , y(n+T ) given
y(1), . . . , y(n). Let

Tn := Toepl

ˆR0, ˆR1 ,
{

· · ·

, ˆRn}

be the block-Toeplitz matrix having in the ﬁrst rows the
estimates of the ﬁrst n + 1 covariance lags of the process
ˆR0, ˆR1, . . . , ˆRn computed as in (10). For T large enough, the
conditional negative log-likelihood function of the AR process
can be well approximated by

(cid:96)(B) :=

(T

n) log det B0 +

tr(B Tn B(cid:62))

−

−
Bn] is the (n + 1)m-dimensional vector
where B := [B0 B1 · · ·
containing the coefﬁcients of the process. Applying Jensen’s
formula, it turns out that

T

n

−
2

log det B0 =

(cid:90)

1
2

log det Φy(eiθ ),

moreover, if ˆΦy(eiθ ) is the truncated periodogram of the AR
process in (13), it is easy to see that

(cid:90)

ˆΦy(eiθ ) eiθ k = ˆR
k = ˆR(cid:62)k .
−

argmin
B

L
subject to

∈

λS h∞(Xo + L) + λL tr(L)

0.

L

≥

(44)

Accordingly,
likelihood can be rewritten as

the approximated conditional negative log-

Notice that, the objective function in (44) is continuous. Since
L = 0 is a feasible point, the problem is equivalent to ﬁnd
B that minimizes λS h∞(Xo + L) + λL tr(L) over the set
L
(cid:26)

(cid:27)

∈

K :=

B

L

∈

L

≥

0, λS h∞(Xo + L) + λL tr(L)

λS h∞(Xo)

.

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)

It is easy to see that K is a closed and bounded (and thus
compact) subset of B. Hence, by Weierstrass’ Theorem, Prob-
lem (44) admits a solution Lo. At this point we can conclude
that the primal problem (33) admits a solution (Xo, Lo).

A. Interpretations

In the remaining of this section we will show how Problem
(32) can be interpreted either as a regularized maximum-
likelihood problem or as a dual of a maximum entropy
problem.

Maximum likelihood interpretation: The reciprocal approx-
imation of AR processes illustrated in Section III allows to
interpret Problem (32) as a regularized (conditional) maximum
likelihood problem. Indeed, in the following we will show that
the ﬁtting function in (32), i.e.

log det(S

L) + tr (cid:0) ˆΣy (S

L)(cid:1) ,

(45)

−

−

−
is the approximation of
the (conditional) negative log-
likelihood of the AR process (8) that should be understood
in the sense explained in Section III. Following [35], consider
the observed AR process y whose spectrum is denoted by
Φy, and suppose that T observations y(1), . . . , y(T ) of the
process are available. The conditional likelihood of the process
y is deﬁned as the likelihood function associated to the

(cid:90)

n

T

1(cid:105)

(cid:104)
ˆΦy(eiθ ) Φy(eiθ )−

−
2

(cid:96)(B) =

log det Φy(eiθ ) + tr

.
(46)
A natural way to approximate (46) is to approximate the
π, π].
integral with a ﬁnite sum, i.e. to discretize the interval [
This is precisely the frequency interpretation of the reciprocal
approximation explained in Section III that consists in sam-
pling the spectrum of the process to obtain the corresponding
symbol (see Figure 1). In fact, considering as sample fre-
quency ∆θ = 2π/N, the Backward Euler approximation leads
to the discrete approximation

−

(cid:96)(B)

T

n

∆θ
2π

−
2

(cid:39)

N
1
−
∑
k=0

log det Φy(eiθk )+tr

(cid:104)
ˆΦy(eiθk )Φy(eiθk )−

1(cid:105)

.

where θk = k ∆θ
be rewritten straightforward in terms of symbols as

π. The conditional log-likelihood can now

−

(cid:96)(B)

n

T
−
2N

(cid:39)

(cid:34)N

1
−
∑
k=0

log det Φy(ζ k) + tr

(cid:35)

ˆΦy(ζ k)Φy(ζ k)−
1

.

N
1
−
∑
k=0

Observe now that Φy(ζ ) is precisely the symbol of the block-
circulant covariance matrix Σy of the reciprocal process y
approximating the process y and ˆΦy(ζ ) is the symbol of the
block-circulant matrix ˆΣy in Problem (32). Accordingly, form
Proposition 1, it follows that

(cid:96)(B)

n

T
−
2N

(cid:2)
−

(cid:39)

1
log det Σ−

y + tr (cid:0) ˆΣy Σ−

y

1

(cid:1)(cid:3) .

1
y = S
Since Σ−
equal to (45).

−

L, this is precisely (up to a scaling factor)

(47)

(48)

Maximum entropy interpretation: We will show that Prob-
lem (32) can be interpreted a regularized version of the dual of
a maximum entropy problem, see [36] for a general overview
of these problems. Consider the regularized solution (So, Lo)
of (32) and let Ω be the support of So, i.e. So satisﬁes (25).
mN, there
Since Lo ∈
exists

0 and rank Lo = lN

(cid:28)

B is so that Lo ≥
G = circ
{

G0, G1, . . . , Gn, 0, . . . , 0
}

Rm

l and Lo = G(cid:62)G. Accordingly, we can
such that Gk ∈
×
consider a modiﬁed version of Problem (32) where the reg-
ularizers are replaced by the corresponding hard-constraints
C : PΩc (S) = 0
S
}
∈
and VG :=
is such that
VG ⊆

VG, where VΩ :=
×

B. Thus, the resulting problem is

S
{
∈
l, H = H(cid:62)}

∈
G(cid:62)(IN ⊗
{

VΩ and L

H)G : H

Rl

∈

log det(S

−

L) + (cid:10) ˆΣy, S

(cid:11)
L
C

−

argmin
S,L
subject to

B −
∈

−

S

S

L > 0, L
VΩ, L

0,
≥
VG.

∈
Proposition 5: The primal of Problem (47) is

∈

argmax
C
Σy∈
subject to

ˆΣy) = 0,
ˆΣy)G(cid:62)E

0,

≥

log det Σy

PΩPB(Σy −
E∗ G(Σy −
0].

· · ·

· · ·

where E∗ := 1
√N

[Il

0

E = F∗1 where 1 := 1
√N
Problem (48) writes as

Proof: We derive the dual of Problem (48). Observing that
Il](cid:62), the Lagrangian of

[Il

Il

L(Σy, W, H) = log det Σy + (cid:10)PΩ
(cid:68)
1(cid:62)F G(Σy −
+
Rl
l is a positive semideﬁnite symmet-
where W
×
∈
∈
ric matrix, and PΩ
B(S) = PΩPB(S). The last term of the
∪
Lagrangian can be rewritten as

B( ˆΣy −
∪
ˆΣy)G(cid:62)F∗1, H

Σy), W
(cid:69)

C, H

(cid:11)
C

C

,

(cid:105)

tr

(cid:104)
F(Σy −
(cid:104)
F(Σy −
(cid:104)
(Σy −

ˆΣy)F∗ FG(cid:62)F∗1H1(cid:62) FGF∗
ˆΣy)F∗ FG(cid:62)F∗(IN ⊗
(cid:105)
ˆΣy) G(cid:62)(IN ⊗
,

H) G

= tr

= tr

H) FGF∗

(cid:105)

where we have exploited the fact that F(Σy −
are block-diagonal matrices and the fact that F∗(IN ⊗
IN ⊗

H. Accordingly,

ˆΣy)F∗ and FGF∗
H)F =

B(W)(cid:11)
(cid:69)

C

L(Σy, W, H) = log det Σy + (cid:10) ˆΣy −

Σy, PΩ

∪
H) G

+

ˆΣy, G(cid:62)(IN ⊗
Σy, S

(cid:68)
Σy −
= log det Σy + (cid:10) ˆΣy −
B(W) belongs to VΩ and L := G(cid:62)(IN ⊗
where S := PΩ
B, i.e. they satisfy all the constraints
H) G
in (47). Similar arguments as the ones used to prove formula
(40), allow us to assert that a necessary and sufﬁcient condition
for Σo to be a minimum point for L is that its ﬁrst Gateaux

0 belongs to VG ⊆

C
(cid:11)
C ,
L

≥

−

∪

9

derivative computed at Σy = Σo is equal to zero in every
direction δ Σ, namely
δ L(Σo; δ Σ) = tr (cid:2)(cid:0)

(cid:1)
S + L

(cid:3) = 0,

δ Σ

δ Σ

C.

1
Σ−
o −

∀

∈

−

L > 0 thus, the substitution
By assumption we have that S
−
1 in the Lagrangian L leads
of the optimum Σo = (S
L)−
precisely to the objective function in (47).
Some observations on the two constraints of (48) are in
ˆΣy) = 0 ﬁxes the entries
order. The ﬁrst constraint PΩPB(Σy −
corresponding to the indexes in Ω of the ﬁrst n + 1 lags of
the reciprocal process. Concerning the second constraint, let
Ψ(ζ ) and Φy(ζ ) be the symbols of G and Σy, respectively.
By Proposition 1, we have that

E∗ G Σy G(cid:62)E =

1
N

N
1
−
∑
k=0

Ψ(ζ k) Φy(ζ k) Ψ(ζ k)∗,

(49)

k=0 Gk ζ −

l ﬁlter Ψ(ζ ) =
which is the covariance of the output of the m
∑n
k fed with the reciprocal process y. Accordingly,
the second constraint in (48) states that the covariance matrix
of the process at the output of the ﬁlter is lower-bounded by
E∗ G ˆΣy G(cid:62)E. We conclude that Problem (48) can be seen as
the reciprocal counterpart of the maximum entropy problem
[9],

×

argmax
Sm
Φy
∈

subject to

(cid:90)

log det Φy

(cid:18)(cid:90)

(cid:19)

eiθ k Φy −

ˆRk

= 0,

pq

k=0,1,...,n
(p,q)
Ω

∈

(50)

(cid:90)

Ψ (Φy −
k=0 Gk e−

ˆΦy) Ψ∗

0,

≥

where Ψ(eiθ ) = ∑n
iθ k. Indeed, the second constraint
in (50) can be approximated with the backward Euler approx-
imation with sample frequency ∆θ = 2π/N obtaining (49).

VI. ALTERNATING DIRECTION METHOD OF
MULTIPLIERS

The solution of Problem (43) requires the joint enforcement
of the constraints (36), (37) and λL ImN + PB(Z)
0, which
task. In this section we will use the
may be a difﬁcult
alternating direction methods of multipliers (ADMM) [37] to
solve Problem (43) by showing that the constraints can be
separated and each one can be enforced in an alternating way.
First of all observe that, by deﬁning the variable P := λLImN +
PB(Z), Problem (43) rewrites as

≥

argmin
Z,P
subject to

C −
∈

log det( ˆΣy + Z)

mN

−

(36), (37)
P = λLImN + PB(Z)
P

0.

≥

(51)

where we have omitted the domain of the objective function
ˆΣy +Z > 0 since it will be checked in the stepsize-choice stage
of the algorithm. The augmented Lagrangian for the problem
is
Lρ (Z, P, M) =

M, P

λLImN −

PB(Z)
C
(cid:105)

(cid:1)
log det (cid:0) ˆΣy + Z
ρ
λLImN −
2 (cid:107)

−

P

−
+

− (cid:104)
−
2
PB(Z)
C
(cid:107)

C is the Lagrange
where ρ > 0 is the penalty term and M
multiplier associated to the equality constraint on P. Accord-
ingly, the ADMM updates are the following:

∈

Then, the positive semideﬁnite matrix that better approximates
A in the Frobenius norm is the projection of A onto the cone
of positive semideﬁnite matrices P+, namely

10

1) The Z-minimization step

Zk+1 = argmin
C

Z

∈
subject to

Lρ (Z, Pk, Mk)

Z.

Z

∈

(52)

where

PP+(A) := argmin

0 (cid:107)

X

≥

X

(cid:107)F = U ∗ diag
A

−

1 , . . . , γ o
γ o
n }
{

U,

γ o
i =

(cid:40)

λii,
0,

0,
if λii ≥
if λii < 0.

2) The P-minimization step
Pk+1 = argmin

C

P

∈

subject to

Lρ (Zk+1, P, Mk)

0.

P

≥

(53)

The following proposition ensures that the projection of a
symmetric, block-circulant matrix onto the cone of positive
semi-deﬁnite matrices is still block-circulant.

Proposition 6: Let C be a symmetric, block-circulant matrix

3) Dual variable update
(cid:16)
Pk+1

Mk+1 = Mk

ρ

(cid:17)
PB(Zk+1)

Z

−

−

λLImN −
where Z :=
and we have considered a
constant value of ρ in order simplify the notation. We will
discuss later how to update ρ to get a faster convergence.
Updates 1) and 2) are not in an implementable format. The
Z-update step (52) is equivalent to the minimization of

C : (36), (37)
}

(54)

∈

{

.

I(Z) :=

−
+

log det( ˆΣy + Z) +
(cid:68)
Mk

2
PB(Z)
C
(cid:107)
(cid:69)
λLImN), PB(Z)

ρ (Pk

ρ
2 (cid:107)

−

−

,

C

over the set Z, which has no closed-form solution, as noticed in
[4] where the solution is approximated by a projective-gradient
step. Following the same lines, the new Z-update step starts
from a known feasible point Z0 = ¯Z and continue the iterations
following the update rule

Zk+1 = PZ

(cid:16)
Zk

−

(cid:17)
tk ∇I(Zk)

(55)

where

∇I(Zk) =

( ˆΣy +Zk)−

1 +PB(Mk)+ρ PB

−

(cid:16)
Zk

−

Pk + λLImN

C = F∗diag (cid:8)C(ζ 0), C(ζ 1), . . . , C(ζ N

1)(cid:9)
−

F,

and let C(ζ k) = VkΛkV ∗k with

V ∗k Vk = VkV ∗k = Im

and Λk = diag

,
λk1, . . . , λkm}
{

being the eigen-decomposition of the (Hermitian) block C(ζ k),
for k = 0, . . . , N
1. Then the eigen-decomposition of C can
be written as

−

C = W∗ΛW,

W = V∗F,

where V = diag
Then,

V0, . . . ,VN
{

1}
−

and Λ = diag

Λ0, . . . , ΛN
{

1}
−

.

PC+(C) := argmin

X

C

C = W∗ diag
(cid:107)

Γ0, . . . , ΓN
{

1}
−

W

where Γk = diag

X

≥

−

0 (cid:107)
γk1, . . . , γkm}
{
(cid:40)
λki,
0,

γki =

and

0,
if λki ≥
if λki < 0.

(cid:17)

for k = 0, . . . , N

1.

−

Proof: The result follows from applying Lemma 1 with

is the gradient of the cost-function I computed in Zk, tk is
the stepsize founded by the Armijo condition, and PZ is the
projection operator onto the constraints space Z.
The optimization problem involved in the P-update step (53)
is equivalent to minimize the functional

J(P) :=

ρ
2 (cid:107)

2
P
C
(cid:107)

−

(cid:68)
P, Mk + ρ

(cid:16)
λLImN + PB(Zk+1)

(cid:17)(cid:69)

C

≥

0. Since J is a quadratic functional of P the
over all P
minimization of J over the whole vector space C admits the
closed form solution
1
ρ

Mk + λLImN + PB(Zk+1)

Po =

but it is not a positive semideﬁnite matrix in general. Accord-
ingly, in order to ﬁnd our solution, we have to ﬁnd the positive
semideﬁnite block-circulant matrix that better approximates
Po in the norm induced by the scalar product on C (i.e. the
Frobenius norm on C). Recall the following well-known result.
n be an Hermitian matrix whose
×
eigenvalue decomposition is given by A = U ∗ΛU, with
λ1, . . . , λn}
{

Lemma 1: Let A

U ∗U = UU ∗ = I

Λ = diag

and

Cn

∈

.

U = W. Of course,

PC+(C) = F∗diag

V0Γ0V ∗0 , . . . ,VN
{

1ΓN

−

1V ∗N
F
1}
−
−

is a block-circulant matrix because it is block-diagonalized by
the Fourier-block matrix.

According to Proposition 6, the positive semideﬁnite block-
circulant matrix that better approximates Po in the C norm is
the projection of Po onto the cone of the symmetric, positive
semideﬁnite, block-circulant matrices C+, that is

Pk+1 = PC+(Po) = PC+

Mk + λLImN + PB(Zk+1)

(cid:19)

. (56)

(cid:18) 1
ρ

We conclude that the ADMM algorithm for the estimation
of the sparse and the low-rank component of the inverse of
the covariance matrix of the reciprocal process consists in the
following updates
(cid:104)
Zk
−
(cid:20) 1
(cid:21)
ρ k Mk + λLImN + PB(Zk+1)
(cid:105)
ρ k (cid:104)
PB(Zk+1)
λLImN −

(cid:105)
tk ∇I(Zk)

Pk+1 = PC+

Mk+1 = Mk

Zk+1 = PZ

Pk+1

(57)

−

−

,

,

.

A typical update for ρ is ρ k+1 = αρ k, with α > 1 being a
certain growth coefﬁcient that needs to be properly tuned.
Notice that the matrices involved in (57) are all symmetric and
block-circulant. Accordingly, as explained in Section III, the
introduction of the reciprocal approximation allows to obtain
a robust identiﬁcation procedure even in the case when n is
large. Indeed, relations (14) and (15), allow to compute inverse
matrices and eigenvalues in a robust way. Moreover, it is worth
noting from (15), that the dimensions of the matrices whose
eigenvalues must be computed in the optimization procedure,
depend only on m, hence the identiﬁcation algorithm we are
proposing scales with respect to n gaining robustness in the
results even if the order of the AR process is large.
Following [37], the basic stopping criterium for the algorithm
is based on the primal and dual residuals of the optimality
conditions that respectively measure the satisfaction of the
inequality constraint P
0 and the distance between two
successive iterates of the variable P. More precisely, the primal
residual at iteration k + 1 is deﬁned as
rk+1 := Pk+1

PB(Zk+1),

≥

λLImN −
while the dual residual turns out to be
sk+1 := PBc(Mk)
Pk+1

ρ k (cid:104)

−

−

(cid:105)
PB(Pk)

.

−

It is reasonable that the primal and dual residual must be small,
that is

rk

(cid:107)

C
(cid:107)

≤

ε p

and

(cid:107)

sk

C
(cid:107)

≤

ε d,

where ε p > 0 and ε d > 0 are feasibility tolerances for the
primal and dual feasibility conditions. The latter are deﬁned
as

(cid:110)

λL√mN,

Zk
(cid:107)

C,
(cid:107)

(cid:107)

Pk

C

(cid:107)

(cid:111)

,

ε p := mN ε abs + ε rel max
Mk
(cid:107)

ε d := mN ε abs + ε rel

C.
(cid:107)
Here, ε abs and ε rel are predeﬁned absolute and relative toler-
ances for the problem. Accordingly, the algorithm converges
if all the conditions

rk

ε p,

(cid:107)

C
(cid:107)

C
(cid:107)
hold true, where ρmax > 0 is the maximum value allowed for
the penalty parameter ρ k, selected by the user.

≤

≤

(58)

ρ k = ρmax

sk
(cid:107)

ε d,

VII. NUMERICAL EXAMPLES

In this section we compare the performances of our method
to which we will refer to as approximated algorithm with the
one proposed in [10] for the solution of Problem (31), which
will be referred to as exact algorithm. In particular we will
show how the two algorithms behave considering both the case
in which the observed process has low dimension and the case
in which we have an high dimensional observed process.

Low-dimensional case: Synthetic data ere generated from

the AR latent-variable model of order n = 8,

y(t) =

n
∑
k=1

Ak y(t

−

k) + η(t),

(59)

with m = 20 observed variables and l = 1 latent vari-
ables. Here, η(t) is white Gaussian noise with variance

11

E[η(t)(cid:62)η(t)] = 21.14 and T = 1000 samples have been used
to compute the estimated covariance lags ˆRk, k = 0, . . . , n.
Figure 4 (center) reports the sparsity pattern of the underlying
model, randomly generated so that
the non-zero elements
represents the 5% of the total elements. For the approxi-

Fig. 4: Sparsity pattern estimated by the approximated algo-
rithm with α = 1.007, λS = 95, λL = 5.4 (left), true sparsity
pattern (center), sparsity pattern estimated by the exact al-
gorithm with α = 1.002, γS = 2.6, γL = 2.95 (right). The red
squares indicate the conditional dependent pairs while the
white squares indicates the conditional independent pairs. ˆla,
l and ˆle denote the number of latent variables.

×

5 and ε rel = 10−

[60, 130] and λL ∈

mated algorithm we have considered N = 30 samples of the
spectrum. In both the ADMM implementations we have set
ε abs = 10−
4 while ρmax = 104. In order to
tune the update of the penalty term ρ in the ADMM, we
have ran both the algorithms for different values of the growth
[1.001, 1.1]. More precisely, for each value
coefﬁcient α
∈
of α, a 5
5 grid of candidate estimated models has been
produced, corresponding to ﬁve linearly spaced values of the
[3, 7.8] for
regularization parameters λS ∈
the approximated algorithm, and ﬁve linearly spaced values of
[2.425, 2.95] for the exact algorithm.
[1.42, 2.6] and γL ∈
γS ∈
The values of the regularization parameters that identify the
grids have been selected so that the estimated models capture
a range of features as complete as possible: from a very sparse
model with a relatively high rank, to a quasi-full model with
the lowest rank possible. Figure 5 shows the supports and the
ranks estimated by the approximated algorithm corresponding
to the different values of λS and λL. For both methods the value
of α that gives the better performances, i.e. that guarantees the
minimum gap between ε p / ε d and the primal/dual residual at
the ﬁnal iteration, respectively, has been selected. Accordingly,
we have chosen α = 1.007 for the approximated algorithm
while α = 1.002 has been chosen for the exact algorithm.
Let s(h) and ε(h) denote the vectors containing the dual
residual and its feasibility tolerance for the model h = 1, . . . , 25
respectively. Figure 6 displays the (logarithm of the) averages

µs =

1
25

25
∑
h=1

s(h),

µε =

1
25

25
∑
h=1

ε(h),

obtained by our method with α = 1.007 (left) and by the
exact method for α = 1.002 (right). For both algorithms, the
primal residual always satisﬁes the condition in the stopping
criterium (58) therefore there is no need to displaying it.
We observe that the exact algorithm does not converge for
in
any value of α we have considered. Indeed,
Figure 6 (right) clearly shows that the mean dual-residual

the plot

ˆΩa,ˆla=1Ω,l=1ˆΩe,ˆle=112

2
F

π, π] are

process y approximating y. The squared-estimation errors for
the two algorithms are depicted in Figure 7; the corresponding
mean values over [
Φy −
Φy(cid:107)
(cid:107)
Φy −
Φy(cid:107)
(cid:107)

Ea(eiθ ) = 0.0358,

Ee(eiθ ) = 0.0443.

−
ˆΦa(cid:107)

Ea := (cid:107)

Ee := (cid:107)

ˆΦe(cid:107)

¯Ea :=

¯Ee :=

The approximated algorithm performs better both in terms of
the mean value and in terms of the height of the peaks of the
relative error.

2
F

2
F

2
F

(cid:90)

(cid:90)

,

,

Fig. 5: Supports and ranks estimated by the approximated
[3, 7.8]. The growth
[60, 130] and λL ∈
algorithm for λS ∈
coefﬁcient is set α = 1.007.

Fig. 6: Logarithm of the average dual residual for the approx-
imated method (left) and for the exact method (right). The
dashed lines correspond to the logarithm of the associated
average feasibility tolerances.

µs stays signiﬁcantly above the threshold µε . The optimal
values of the regularization parameters have then been selected
by cross-validation, using a test data set of 500 samples.
Figure 4 compares the optimal sparsity pattern provided by the
approximated algorithm ˆΩa (left), corresponding to λS = 95
and λL = 5.4, and the optimal sparsity pattern estimated by
the exact algorithm ˆΩe (right) corresponding to γS = 2.6 and
γL = 2.95, together with the estimates of the number of latent
variables, ˆla and ˆle, respectively. Notice that both algorithms
estimates the correct number of latent variables but only the
approximated one produces an estimate of the sparsity pattern
comparable with the true one. Let ˆΦe and ˆΦa be the estimates
of the spectra of the true observed process Φy obtained by the
solutions of problems (31) and (32), respectively. According to
Figure 2, ˆΦa is the extension over the whole interval [
π, π] of
the symbol of the estimated covariance matrix of the reciprocal

−

Fig. 7: Relative errors in the estimated spectra: approximated
algorithm (left), exact algorithm (right).

High-dimensional case: We consider now an AR latent-
variable model as in (59) where we have m = 80 observed
variables and l = 1 latent variable, n = 24 and the variance of
the noise is E[η(t)(cid:62)η(t)] = 87.1. The number of samples used
to estimate the covariance lags Rk is T = 15000. The number
of conditionally dependent pairs in the true model is 158 so
= 396. Table
that the cardinality of the true support is
8 compares the performances of our approximated algorithm
with the exact algorithm proposed in [10] for different values
of the sparsity regularization parameters λS and γS,
that
have been chosen in order to have approximatively the same
variety on the results. The notation
indicates the error
on the sparsity pattern in terms of number of misclassiﬁed
entries. Both algorithms estimate the correct number of latent

Ω
|
|

Ω
|

ˆΩ

−

|

Fig. 8: Summary of the performances of the two algorithm
for λS = 100, 146.25, 350 and γS = 0.7, 0.826, 1.7. The values
of the low-rank regularization parameters are λL = 8.6875 for
the approximated algorithm (left) and γL = 2.3 for the exact
algorithm (right). These results have been obtained on a 2014
1.4GHz MacBook Air.

variables, but the approximated algorithm gives a result very
close to the true one (highlighted in red in Figure 8) while for
the exact algorithm, even if the cardinality of the true support
has been correctly estimated, the error in the reconstruction
of the sparsity pattern is quite high. This is due to the fact
that the higher is the order of the process n, the less accurate

|ˆΩa|=20,ˆla=3|ˆΩa|=20,ˆla=2|ˆΩa|=20,ˆla=1|ˆΩa|=20,ˆla=0|ˆΩa|=20,ˆla=0|ˆΩa|=20,ˆla=3|ˆΩa|=20,ˆla=2|ˆΩa|=22,ˆla=1|ˆΩa|=24,ˆla=0|ˆΩa|=24,ˆla=0|ˆΩa|=20,ˆla=3|ˆΩa|=24,ˆla=2|ˆΩa|=30,ˆla=1|ˆΩa|=34,ˆla=0|ˆΩa|=34,ˆla=0|ˆΩa|=26,ˆla=3|ˆΩa|=54,ˆla=3|ˆΩa|=74,ˆla=1|ˆΩa|=90,ˆla=0|ˆΩa|=90,ˆla=0|ˆΩa|=96,ˆla=3|ˆΩa|=228,ˆla=1|ˆΩa|=264,ˆla=0|ˆΩa|=264,ˆla=0|ˆΩa|=264,ˆla=0λSλL6077.595112.513034.25.46.67.816601322−3−0.270.86−2.22kµsµε1250050001.83−1.91−3.22kµsµε−π0π0.020.040.060.11θEa(eiθ)−π0π0.020.110.05θEe(eiθ)¯Ea¯Eeˆla|ˆΩa||Ω−ˆΩa|Eatime[s]1803160.2944166091394100.317016953110646680.190217352ˆle|ˆΩe||Ω−ˆΩe|Eetime[s]1883080.56113487513961560.51683468119947060.570135072is the computation of eigenvalues and inverse matrices by
the exact algorithm. Figure 8 shows that such an issue is
avoided in the approximated version, thanks to the availability
of closed-form formulas for the computation of the eigenvalues
of block-circulant matrices. Moreover, we see that the run
time of the exact algorithm is about twice the run time of the
approximated one. This conﬁrm the fact that the approximated
algorithm scales with the order n of the AR process we
are approximating as suggested in Section V. This kind of
scenario agrees with what we have discussed in Section
V: high-order AR process are quite challenging instances
for the exact procedure proposed in [10]; in this cases, the
reciprocal approximation leads to remarkable beneﬁts in the
performances of the identiﬁcation procedure.

VIII. CONCLUSIONS

In this paper an identiﬁcation paradigm for latent-variable
graphical models associated to reciprocal processes has been
the proposed paradigm
presented. It has been shown that
is theoretically strongly sustained, being an approximation
of the corresponding problem for AR processes both in a
maximum likelihood and in a maximum entropy sense. The
performances of the proposed method have been compared
with the approach proposed in [10] where no approximation is
introduced. The numerical examples have shown that for high-
order AR processes reciprocal approximation gives substantial
improvements in terms of robustness and scalability of the
identiﬁcation procedure.

REFERENCES

[1] S. Lauritzen, Graphical Models. Oxford, U.K.: Oxford university press,

1996.

[2] R. Dahlhaus, “Graphical interaction models for multivariate time series,”

Metrika, vol. 51, no. 2, pp. 157–172, 2000.

[3] E. Avventi, A. G. Lindquist, and B. Wahlberg, “Arma identiﬁcation of
graphical models,” IEEE Transactions on Automatic Control, vol. 58,
pp. 1167–1178, May 2013.

[4] J. Songsiri and L. Vandenberghe, “Topology selection in graphical
models of autoregressive processes,” J. Mach. Learn. Res., vol. 11,
pp. 2671–2705, 2010.

[5] J. Songsiri, J. Dahl, and L. Vandenberghe, “Maximum-likelihood es-
timation of autoregressive models with conditional independence con-
straints,” in 2009 IEEE International Conference on Acoustics, Speech
and Signal Processing, pp. 1701–1704, April 2009.

[6] S. Maanan, B. Dumitrescu, and C. Giurcaneanu, “Conditional inde-
pendence graphs for multivariate autoregressive models by convex
optimization: Efﬁcient algorithms,” Signal Processing, vol. 133, 11 2016.
[7] S. Maanan, B. Dumitrescu, and C. Giurcaneanu, “Maximum entropy
expectation-maximization algorithm for ﬁtting latent-variable graphical
models to multivariate time series,” Entropy, vol. 20, p. 76, 01 2018.
[8] V. Chandrasekaran, P. A. Parrilo, and A. S. Willsky, “Latent variable
graphical model selection via convex optimization,” Ann. Statist., vol. 40,
pp. 1935–1967, 08 2012.

[9] M. Zorzi and R. Sepulchre, “AR identiﬁcation of

latent-variable
graphical models,” IEEE Transactions on Automatic Control, vol. 61,
pp. 2327–2340, Sept 2016.

[10] R. Ligeois, B. Mishra, M. Zorzi, and R. Sepulchre, “Sparse plus low-
rank autoregressive identiﬁcation in neuroimaging time series,” in 2015
54th IEEE Conference on Decision and Control (CDC), pp. 3965–3970,
Dec 2015.

[11] F. P. Carli, A. Ferrante, M. Pavon, and G. Picci, “A maximum entropy
solution of the covariance extension problem for reciprocal processes,”
IEEE Transactions on Automatic Control, vol. 56, pp. 1999–2012, Sept
2011.

[12] B. C. Levy and A. Ferrante, “Characterization of stationary discrete-time
gaussian reciprocal processes over a ﬁnite interval,” SIAM Journal on
Matrix Analysis and Applications, vol. 24, no. 2, pp. 334–355, 2002.

13

[13] B. C. Levy, “Regular and reciprocal multivariate stationary gaussian
reciprocal processes over z are necessarily markov,” J. Math. Syst. Est.
Control, vol. 2, no. 2, pp. 134–154, 1992.

[14] B. C. Levy, R. Frezza, and A. J. Krener, “Modeling and estimation
of discrete-time gaussian reciprocal processes,” IEEE Transactions on
Automatic Control, vol. 35, pp. 1013–1023, Sep 1990.

[15] A. Ringh, J. Karlsson, and A. Lindquist, “Multidimensional rational
covariance extension with applications to spectral estimation and image
compression,” SIAM Journal on Control and Optimization, vol. 54, no. 4,
pp. 1950–1982, 2016.

[16] A. G. Lindquist and G. Picci, “The circulant rational covariance exten-
sion problem: The complete solution,” IEEE Transactions on Automatic
Control, vol. 58, pp. 2848–2861, Nov 2013.

[17] A. Chiuso, A. Ferrante, and G. Picci, “Reciprocal realization and mod-
eling of textured images,” in Proceedings of the 44th IEEE Conference
on Decision and Control, pp. 6059–6064, Dec 2005.

[18] F. Carli, A. Ferrante, M. Pavon, and G. Picci, “An efﬁcient algorithm
for maximum entropy extension of block-circulant covariance matrices,”
Linear Algebra and its Applications, vol. 439, no. 8, pp. 2309 – 2329,
2013.

[19] A. Ringh and J. Karlsson, “A fast solver for the circulant rational
covariance extension problem,” in Control Conference (ECC), 2015
European, pp. 727–733, IEEE, 2015.

[20] D. Alpago, “On the identiﬁcation of sparse plus low-rank graphical
models,” Master’s thesis, University of Padova, Dept. of Information
Engineering, Padova, Italy, 2017.

[21] A. Ferrante, M. Pavon, and M. Zorzi, “A maximum entropy enhancement
for a family of high-resolution spectral estimators,” IEEE Transactions
on Automatic Control, vol. 57, pp. 318–329, Feb 2012.

[22] M. Zorzi and A. Ferrante, “On the estimation of structured covariance

matrices,” Automatica, vol. 48, no. 9, pp. 2145 – 2151, 2012.

[23] J. Burg, Maximum entropy spectral analysis. PhD thesis, Stanford

University, Dept. of Geophysics, Stanford, CA, 1975.

[24] M. Zorzi, “An interpretation of the dual problem of the THREE-like

approaches,” Automatica, vol. 62, pp. 87 – 92, 2015.

[25] M. Zorzi, “A new family of high-resolution multivariate spectral estima-
tors,” IEEE Transactions on Automatic Control, vol. 59, pp. 892–904,
April 2014.

[26] A. Ringh, J. Karlsson, and A. Lindquist, “Multidimensional rational
covariance extension with applications to spectral estimation and image
compression,” SIAM Journal on Control and Optimization, vol. 54, no. 4,
pp. 1950–1982, 2016.

[27] T. Georgiou and A. Lindquist, “Kullback-leibler approximation of
spectral density functions,” Information Theory, IEEE Transactions on,
vol. 49, pp. 2910 – 2917, 12 2003.

[28] A. Ferrante, C. Masiero, and M. Pavon, “Time and spectral domain
relative entropy: A new approach to multivariate spectral estimation,”
IEEE Transactions on Automatic Control, vol. 57, pp. 2561–2575, Oct
2012.

[29] T. Georgiou, “Relative entropy and the multivariable multidimensional
moment problem,” IEEE Trans. on Information Theory, vol. 52, no. 3,
pp. 1052–1066, 2006.

[30] C. Byrnes, T. Georgiou, and A. Lindquist, “A new approach to spectral
estimation: A tunable high-resolution spectral estimator,” IEEE Trans.
on Signal Processing, vol. 48, no. 11, pp. 3189–3205, 2000.

[31] C. I. Byrnes, P. Enqvist, and A. Lindquist, “Identiﬁability and well-
posedness of shaping ﬁlter parametrizations: A global analysis ap-
proach,” SIAM Journal on Control and Optimization, vol. 41, no. 1,
pp. 23–59, 2002.

[32] R. M. Gray, “Toeplitz and circulant matrices: A review,” Foundations
and Trends R(cid:13) in Communications and Information Theory, vol. 2, no. 3,
pp. 155–239, 2006.

[33] D. Alpago, M. Zorzi, and A. Ferrante, “Identiﬁcation of sparse reciprocal
graphical models,” IEEE Control Systems Letters, vol. 2, pp. 659–664,
Oct 2018.

[34] A. G. Lindquist and G. Picci, Linear Stochastic Systems: a Geometric
Approach to Modeling, Estimation and Identiﬁcation. Berlin, Germany:
Springer, 2015.

[35] J. Songsiri, J. Dahl, and L. Vandenberghe, “Graphical models of au-
toregressive processes,” Convex optimization in signal processing and
communications, pp. 89–116, 2010.

[36] M. Pavon and A. Ferrante, “On the geometry of maximum entropy

problems,” SIAM Review, vol. 55, no. 3, pp. 415–439, 2013.

[37] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, “Distributed
optimization and statistical learning via the alternating direction method
of multipliers,” Found. Trends Mach. Learn., vol. 3, pp. 1–122, Jan.
2011.

