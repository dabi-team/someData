OKVIS2: Realtime Scalable Visual-Inertial SLAM with Loop Closure

Stefan Leutenegger1,2

2
2
0
2

g
u
A
2
1

]

V

I
.
s
s
e
e
[

2
v
9
9
1
9
0
.
2
0
2
2
:
v
i
X
r
a

Abstract— Robust and accurate state estimation remains a
challenge in robotics, Augmented, and Virtual Reality (AR/VR),
even as Visual-Inertial Simultaneous Localisation and Mapping
(VI-SLAM) getting commoditised. Here, a full VI-SLAM system
is introduced that particularly addresses challenges around long
as well as repeated loop-closures. A series of experiments reveals
that it achieves and in part outperforms what state-of-the-art
open-source systems achieve. At the core of the algorithm sits
the creation of pose-graph edges through marginalisation of
common observations, which can ﬂuidly be turned back into
landmarks and observations upon loop-closure. The scheme
contains a realtime estimator optimising a bounded-size factor
graph consisting of observations, IMU pre-integral error terms,
and pose-graph edges––and it allows for optimisation of larger
loops re-using the same factor-graph asynchronously when
needed.

I. INTRODUCTION

The ability to estimate states, such as pose and veloc-
ity, constitutes a fundamental capability for mobile robots
enabling autonomy, or to power Augmented and Virtual
Reality (AR/VR) applications. The combination of visual,
spatial constraints with temporal ones provided by an Inertial
Measurement Unit (IMU) has proven to be of particular
interest providing high robustness and accuracy, therefore
ﬁnding its way into various commercial systems in the
AR/VR and mobile robotics space.

Pushing the boundary of robustness and accuracy with
open-source systems, while maintaining realtime operation,
remains a much researched challenge. Consequently,
the
past few years have seen the emergence and release of
several Visual-Inertial Odometry (VIO) and Simultaneous
Localisation and Mapping (VI-SLAM) systems that exhibit
impressive characteristics. Interestingly, state-of-the-art VIO
and VI-SLAM systems such as BASALT [1], Kimera [2],
tightly-
VINS-Fusion [3], and ORB-SLAM3 [4] are all
coupled systems, i.e. jointly considering visual and inertial
measurements with their internal states; and they further em-
ploy explicit sparse data associations provided by keypoint
detection and tracking and/or descriptor matching (indirect
approach).

Scalability to any environment, indoor and outdoor, as well
as sensor suite and motion characteristic–especially when
it comes to realtime handling of potentially frequent loop
closures–still constitute a challenge, which this work aims

This work has been supported by the EPSRC grant Aerial ABM
EP/N018494/1, EPSRC ORCA Partnership Resource Fund (PRF) SWIFT,
Imperial College London, and the Technical University of Munich.

1The author is with the Smart Robotics Lab, Technical University of

Munich, Germany. stefan.leutenegger@tum.de.

2The author is also with the Smart Robotics Lab, Imperial College

London, UK.

to address: speciﬁcally, the scheme constructs pose-graph
edges from joint observations that are used as part of the
realtime estimator, jointly with visual reprojection error and
pre-integrated inertial factors. This allows to increase the
window of optimised states, therefore limiting detrimental
effects of ﬁxing older states. Upon loop closure, this incre-
mentally constructed posegraph, together with inertial factors
and observations, can be asynchronously optimised and,
importantly, the posegraph edges can be seamlessly turned
back into observations, thereby reviving old landmarks and
reprojection errors. Furthermore, a light-weight segmentation
Convolutional Neural Network (CNN) is run on keyframes
asynchronously, on the CPU, to remove observations into
dynamic regions. In a series of experiments on popular
datasets, EuRoC [5] and [6], the competitive and in parts
superior accuracy of OKVIS2 is demonstrated in comparison
with state-of-the art approaches.

In summary, the following contributions are made:
• A realtime capable multi-camera VI-SLAM system
called OKVIS2 that supports place recognition and loop
closure, which I aim to release as open-source software
within the coming months.

• An algorithm constructing posegraph factors from
marginalised observations, which are included into both
the realtime tracking estimator to allow for an enlarged
optimisation window, as well as in loop closure optimi-
sation running asynchronously.

• A scheme of leveraging a light-weight semantic seg-
mentation CNN for removal of dynamic objects (hu-
mans, clouds, . . . ) running on the CPU for maximum
portability.

The remainder of this paper is organised as follows:
in Section II, the most directly related work is reviewed,
followed by a brief introduction to the notation used in
Section III, and a system overview in Section IV. Sections V
and VI then introduce the contributions in terms of estimator
and the frontend, respectively. Experimental results are later
presented in Section VII.

II. RELATED WORK

We brieﬂy overview the most directly relevant works
on realtime vision-based and visual-inertial odometry and
SLAM.

A. Visual Odometry and SLAM

While very early vision-only SLAM systems used sparse
observations in an Extended Kalman Filtering (EKF) scheme
[7], later systems quickly converged to employing non-linear
least-squares optimisation thanks to superior computational

1

 
 
 
 
 
 
efﬁciency and accuracy. PTAM [8], and later ORB-SLAM
1 [9] and 2 [10] all use some form of windowed Bundle
Adjustment, the latter in combination with re-localisation and
pose-graph optimisation upon loop closures. All of the above
employ keypoint detection and tracking and/or descriptor
matching, thus explicit correspondences.

A new paradigm was proposed with DTAM [11], which
reconstructs dense depth frames and performs per-pixel direct
photometric alignment for tracking, while maintaining real-
time capabilities if implemented on a GPU. As such, DTAM
offers a much more expressive map representation, however,
tracking accuracy proved not quite competitive with sparse
systems. As a compromise towards more computational
efﬁciency, while maintaining direct photometric alignment,
LSD-SLAM [12] reconstructs depth in pixels of sufﬁcient
contrast only, and offers additionally loop closure detection
and optimisation, therefore making for an efﬁcient CPU-
based full SLAM system. Finally,
the concept of direct
photometric alignment was also adopted for an otherwise
sparse SLAM systems, SVO [13] and DSO [14], the latter
ultimately with stereo [15] and loop closure [16] extensions.

B. Dense Depth-Camera-based SLAM

The emergence of RGB-D (depth) cameras has fuelled
the development of dense SLAM systems. KinectFusion
[17] reconstructs a volumetric Truncated Signed Distance
(TSDF) map and performs geometric alignment in tracking,
by application of the Iterative Closest Point (ICP) algorithm
for every pixel of the live frame. Various attempts to improve
scalability in space followed, e.g. [18], [19]. Alternative map
representations have also been explored, e.g. surfels, which
allowed for inclusion of loop closure in ElasticFusion [20].
Common to all these systems is the alternation of tracking
(optimisation of pose holding the map ﬁxed) and mapping
(holding the pose ﬁxed and fusing new information into
the map), which is the likely cause for inferior trajectory
accuracy in large-scale scenarios. Note, however, that dense
mapping backends may be combined with other, e.g. sparse
visual (-inertial) SLAM systems.

C. Visual-Inertial Odometry and SLAM

Among the ﬁrst attempts to tightly couple visual and
inertial measurements, the seminal work [21] introduces the
MSCKF, a ﬁltering-based approach in which several past
states are maintained and updated with observations under
marginalisation of landmarks. Many later derived approaches
improve accuracy and computational efﬁciency, making the
method ideally suited in robotics, Augmented and Virtual
Reality (AR/VR). An excellent open-source implementation
is provided by OpenVINS [22]. ROVIO [23] also employs a
light-weight ﬁltering scheme, but using a photometric update
rather than indirect keypoint associations. Tight integration
of loop closure and large-scale map management constitutes,
however, an inherent challenge to schemes that employ
marginalisation of old states and landmarks. An attempt of
more loosely integrating large-scale sparse mapping and VIO
is made by the popular framework Maplab [24].

2

Inspired by successes in vision-only works, a second
stream of VIO and VI-SLAM systems employ nonlinear
least-squares optimisation of typically reprojection error and
integrated IMU measurements jointly, and in some form
of a windowed approach to keep computational tractability.
OKVIS [25] achieves this by marginalisation and keeping a
set of old keyframes held together with linearised error terms.
ORB-SLAM3 [4], extending [10], conversely, ﬁxes old
states––a scheme that is simpler, more ﬂexible to integrate
with its loop closure optimisations, but also inherently not
a conservative approximation, as it effectively ignores past
estimation uncertainties. In an attempt to more rigorously
assess the errors introduced by ﬁxation, [26] analyses resid-
ual statistics and chooses the ﬁxation adaptively leading to a
shrinking and expanding state window that is optimised. This
proposed work aims to alleviate the potential downsides of
ﬁxation by including a larger window of old keyframes into
the optimisation problem, which is constrained by reletive
pose factors, instead of solely computationally expensive
observations.

It is worth noting that multi-sensor odometry and SLAM
systems have much proﬁted from open-source factor-graph
optimisation libraries, most notably g2o [27], ceres [28], and
GTSAM [29]. The latter also integrates iSAM 1 [30] and 2
[31], which allow to efﬁciently solve full-batch underlying
factor graphs such as typical in VI-SLAM in an inherently
incremental manner.

VINS-Fusion [3] operates similarly to OKVIS, but in-
cludes loop closure by posegraph optimisation, where rel-
ative poses edges of consecutive-only keyframes are con-
structed from marginalisation of visual and inertial mea-
surements. The system supports magnetometer and GPS
measurements in addition to multiple cameras and an IMU.
In contrast, this paper more rigorously constructs pose-graph
edges from visual co-observations and uses these also as part
of the realtime frame-by-frame tracking estimation.

Similarly, Kimera [2] is a state-of-the-art sparse VI-SLAM
system also supporting dense mesh mapping, that combines
a VIO-frontend with a pose-graph optimisation backend used
upon loop closure, both relying on GTSAM.

BASALT [1] employs a similar idea as the one presented
in this work. Non-linear factors connecting several keyframes
are recovered from joint observations and inertial measure-
ments. However,
in this work pre-integrated
inertial factors are kept and only relative pose errors from
observations common to two keyframes are computed.

in contrast,

III. NOTATION AND DEFINITIONS
The VI SLAM problem consists of tracking a moving body
with a mounted IMU and N cameras relative to a static
World coordinate frame F−→W . The IMU coordinate frame is
denoted as F−→S and the camera frames as F−→Ci, i = 1 . . . N .
Left-hand indices denote coordinate representation. Ho-
mogeneous position vectors (denoted in italic font) can be
transformed with TAB , meaning ArP = TAB BrP .

The following state representation is used:
a ]T ,
g , bT
S , qT

W S , W vT , bT

x = [W rT

(1)

V. VISUAL-INERTIAL ESTIMATOR

In the following,

the different components of the VI
Estimator are outlined. Parts of the frontend, as well as
visual and inertial error terms are largely adopted from
OKVIS [25]. The non-linear least squares costs as described
below are minimised using Google’s Ceres Solver [32].

The estimator will be minimising visual,

inertial, and
relative pose errors (pose graph edges), which are described
separately in what follows.

A. Reprojection Error

We use the standard reprojection error ei,j,k

of the j-th

landmark W lj into the i-th camera image at time step k:

r

r = ˜zi,j,k
ei,j,k

r − h(T −1
SCi

TSkW W lj),

(3)

with the keypoint detection ˜zi,j,k
and h(.) denoting the cam-
r
era projection, where in this work pinhole projection is used,
optionally with distortion (radial-tangential or equidistant).
Jacobians are omitted here, as they are analogous to [25].

B. IMU Error

The IMU error ek0

s between time instance k and n is used:

s = ˆxn(xk, ˜zk,n
ek

s

)) (cid:12) xn ∈ R15,

(4)

with ˆxn(xk) denoting the predicted state at step n based on
the estimate xk and the IMU measurements (rate gyro and
accelerometer readings) ˜zk,n
from in-between these frames.
The (cid:12) operator becomes the regular subtraction for all states
but quaternions, where it is deﬁned as

s

q (cid:12) q(cid:48) = Log

(cid:16)

q ⊗ q(cid:48)−1(cid:17)

,

(5)

with the Log (.) denoting the quaternion group logarithm that
includes the transformation from the Lie Algebra to a rotation
vector.

A formulation of this error term using a pre-integration
scheme adopted from [33] is used, rendering its evaluation
tractable for any number of IMU samples used.

C. Relative Pose Error

Furthermore, relative pose errors ek,r
p

between time steps

r and k are used:

p = er,c
er,c

p,0 +

(cid:20)
Sr rSc − Sr˜rSc
qSrSc (cid:12) ˜qSrSc

(cid:21)

,

(6)

where er,c
denotes a constant (to be explained below),
0
and Sk˜rSr , ˜qScSr stand for nominal relative position and
orientation, respectively (expressed in the reference IMU
coordinate frame F−→Sr ).

3

Fig. 1: Overview of OKVIS2: the frontend takes a multi-
frame and associated IMU messages to initialise the new
state, match keypoints, triangulate new landmarks, and to
try place recognition, relocalisation, and loop closure. The
realtime estimator will then optimise the underlying factor
graph and take care of posegraph maintainance and ﬁxation
of old states. A full graph optimisation around detected loops
is carried out asynchronously and later synchronised.

where W rS denotes the position of the origin of F−→S relative
to F−→W , qW S is the Hamiltonian Quaternion of orientation
describing the attitude of F−→S relative to F−→W , and W v
stands for the velocity of F−→S relative to F−→W . Furthermore,
the rate gyro biases bg and accelerometer biases ba are
included. The state is estimated at every time step k where
frames from the camera(s) are obtained.

For quaternions, we employ the multiplicative perturbation

around a linearisation point qAB

qAB = Exp (δαAB) ⊗ qAB ,

(2)

where Exp (.) stands for the quaternion group exponential
that includes the transformation of a (small) rotation vector
δαAB ∈ R3 into the Lie Algebra.

IV. SYSTEM OVERVIEW

The VI SLAM system is split

into frontend and re-
altime estimator that process images and IMU messages
synchronously whenever a new (multi-) frame arrives. To
deal with loop closures, a full factor graph loop optimisa-
tion is executed asynchronously. As shown in Fig. 1, the
frontend deals with state initialisation, keypoint matching,
stereo triangulation (of successive frames and from stereo
images of the same multi-frame), running the segmentation
CNN, as well as place recognition and, if the latter was
successful, re-localisation and loop closure initialisation. The
realtime estimator will then optimise the respective factor
graph, and is also responsible for the creation of posegraph
edges by marginalising old observations, as well as for
ﬁxation of old states. Upon loop closure, it further turns
posegraph edges back into observations, and then triggers
the optimisation of the full factor graph around a loop, which
runs asynchronously, and which will be synchronised with
the realtime factor graph upon completion.

Frontend- new state initialisation- keypoint matching- motion stereo landmark triangul.- stereo landmark triangulation- seg. CNN & observation removal- place recognition- re-localisation & loop closure init.Realtime estimator- factor graph optimisation- posegraph construction- fixation of old states- reviving observations Full graph estimator- fixation to start of loop- async. optimisationSynchronise factor graphsafter async. optimisationimagesIMU readingsbetween framesLegend- steps executed at every frame- steps executed if keyframe- steps executed upon loop closureD. Realtime Estimation Problem

The realtime estimator running at least at camera frame
rate will minimise the following non-linear least squares cost

c(x) =

+

1
2

1
2

(cid:88)

(cid:88)

(cid:88)

(cid:16)
ei,j,k
r

T

ρ

Wrei,j,k
r

(cid:17)

(a) In the beginning.

(b) Later: posegraph creation.

i

k∈K
(cid:88)

j∈J (i,k)

T

ek
s

Wk

s ek

s +

k∈P (cid:83) K\f

1
2

(cid:88)

(cid:88)

r∈P

c∈C(r)

er,c
p

T Wr,c

p er,c
p .

(7)

Here, Wr stands for the visual weight as the inverse co-
variance of the reprojection error, and Wk
for the IMU
s
error weight. Further, the Cauchy robustiﬁer ρ(.) is used on
observations. The set K denotes all poses with observations
of the respectively visible landmarks in the set J (i, k). K
contains the T most recent frames as well as M keyframes
in the past; frame f denotes the current frame. The set P
contains the posegraph frames, i.e. those linked together with
relative pose errors obtained from original co-observations.
The weight Wr,c
is computed from marginalisation of old
p
observations, K. Note that P reaches substantially further
into the past than the frames with observations. The set
C(r) ⊂ P describes all the posegraph frames connected to
frame r with a posegraph edge. IMU errors are considered for
all (key-) frames k, n in succession. The IMU error weight
Wk
s is obtained from linearised error propagation as part of
the IMU error (pre-) integration.

A new frame is selected as a keyframe based on a co-
visibility criterion with currently active keyframes, further
detailed in Section VI-B.

If the oldest frame in the T most recent frames is not a
keyframe, the respective states are simply removed and IMU
measurements are appended to the previous IMU error using
the pre-integration scheme up to the next frame. If, however,
the oldest frame in the T most recent frames is a keyframe,
we apply the posegraph creation scheme detailed below in
Section V-D.1.

Fig. 2 visually overviews the construction of the underly-
ing factor graph as time progresses, which is further detailed
below.

Note that while not further explained here, we may op-
tionally leave the camera extrinsics TSCi as variables to be
optimised, i.e. performing online calibration.

1) Posegraph Creation: To keep the problem complexity
bounded, whenever the number of keyframes |K| exceeds a
bound K, the frames exhibiting least co-visibility with either
the current frame or current keyframe are moved from K to P
by marginalisation of common observations under insertion
of relative pose error terms of the form (6). The current
keyframe is determined as the frame with most visual overlap
with the current (most recent) frame (see also VI).

One important exception is applied: the oldest keyframe is
kept for as long as there are common observations with either
the current frame or current keyframe, in order to keep some
very long-term co-visibilities helping to maintain an accurate
directional estimate.

4

(c) Loop closure.

Fig. 2: Initially a full batch VI factor graph (Fig. 2a) is
created and optimised. Later (Fig. 2b), frames with least
overlap with the live frame and current keyframe are turned
into posegraph poses by construction of relative pose errors
under marginalisation of common observations; also, old
poses and speed/bias variables are ﬁxed (dashed lines) to
keep the problem realtime capable. When a loop-closure
occurs (Fig. 2c), respective observations and landmarks are
re-activated.

To create only a tractable subset of pose graph edges, the
following heuristic is applied, which is visually explained in
Fig. 3.

• From the poses with observations (i.e. in K), those
which already have posegraph edges connected are
collected and inserted into a set T .

• Let the pose index r denote the states to be converted

into a posegraph node. r is inserted into T .

• The frame with maximum visual overlap with r is

inserted into T .

• A Maximum Spanning Tree (MST) is computed on T
using the number of co-observations between two poses
as the criterion.

• Now, all the MST edges connecting to the pose r are
created with the scheme described below, which uses
all joint observations into the two frames r and c.

Note that

this method will ultimately duplicate obser-
vations and landmarks, if observed in more than the two
frames r and c, which introduces an inconsistency by design.
However, the argument is made that this approximation will
still produce superior results than simply creating relative
pose errors with identity weight matrices, which is the de-
facto standard in visual(-inertial) SLAM systems. . .

Speciﬁcally, we ﬁrst

transform the landmark into the
reference coordinates F−→Sr , i.e. the IMU coordinate frame
at index r. And we temporarily view as the state solely
]. Therefore, the reprojection errors into
p = [Sr

, qSrSc

rSc

tttkeyframe pose (withobserv.)loop-closure posespeed/biases(many) landmarksIMU errorposegraph errorspose priormany reprojection errorsspeed/bias priorposegraph pose (no observ.)active error terminactive error termestimated variablefixed parametersnon-key pose (withobserv.)(a) Keyframe pose r selected for conversion into posegraph node.

(b) Computation of the Maximum Spanning Tree (MST).

(c) Creation of pose graph edges (here: just one).

Fig. 3: Posegraph edge creation scheme: when keyframe r
is selected for conversion into a posegraph node, because
of least overlap with the current (key-)frame (Fig. 3a), a
Maximum Spanning Tree (MST) is computed 3b) from
the poses in the set T (nodes with posegraph edges and
node with most visual overlap), after which all MST edges
connected to r are created (Fig. 3c).

reference frame index r and the other frame with index c,
respectively, become

ei,j,r
r
ei,j,c
r

.

= ˜zi,j,r
= ˜zi,j,c

r − h(T −1
r − h(T −1
SCi

SCi Sr lj),

TScSr Sr lj).

(8)

(9)

Jacobians is omitted for brevity, note that for the important
case of qSrSc approaching ˜qSrSc , the Jacobian becomes
Identity.

The remaining terms in (11) are computed applying the
Schur complement operation on the Gauss-Newton system
underlying the cost in (10) for marginalisation of landmarks
as follows. First, we construct the Gauss-Newton system that
will assume the following structure









Hp,p
...
HT
p,j
...

. . . Hp,j
. . .
0
0 Hj,j

0

0



. . .







0
0
. . .









δp
...
δlj
...









=









bp
...
bj
...









.

(13)

Please note the variable ordering of the pose (p) ﬁrst,
followed by all the landmarks (Sr lj). While the derivation of
the necessary Jacobians is omitted here, they directly follow
from the reprojection error term (3) with the notable property
that landmarks observed in the reference frame need not be
transformed, as they already are expressed in the reference
frame, therefore the respective Jacobians become zero.

Now,

landmarks are marginalised out using the Schur

complement:

H∗ = Hp,p −

(cid:88)

Hp,jH+

jjHT

p,j,

b∗ = bp −

j
(cid:88)
Hp,jH+

jjbj,

j

which yields the reduced Gauss-Newton system

H∗δp = b∗,

(14)

(15)

(16)

which is now linearised around the current pose ˜p, therefore

H∗δp = b∗ − H∗(p (cid:12) ˜p)

(17)

The supposedly equivalent Gauss-Newton system of a re-
spective pose graph error term takes the form

Adopting landmark marginalisation to all joint observa-

tions, we consider the respective joint cost

Er,c
p

T Wr,c

p Er,c

p δp = −Er,c
p

T Wr,c

p (er,c

p,0 + p (cid:12) p),

(18)

cr,c
p =

1
2

N
(cid:88)

(cid:88)

i=1

j∈
J (i,r)

T

ei,j,r
r

Wrei,j,r

r +

(cid:88)

T

ei,j,c
r

Wrei,j,c
r

,

j∈
J (i,c)

(10)
with the sets J (i, r), J (i, c) denoting all landmarks that are
visible at time steps r and c that have a measurement in the
respective image i, and that have a low reprojection error to
start with. Now we can approximate this cost as

cr,c
p ≈

1
2

er,c
p

p = er,c
er,c

p,0 +

T Wr,c
p er,c
p ,
(cid:20)
Sr rSc − Sr˜rSc
qSrSc (cid:12) ˜qSrSc

(11)

(12)

(cid:21)

,

where Sr˜rSc, ˜qSrSc are the relative position and orientation,
respectively, as a selected linearisation point at the time
of pose graph edge creation. While the derivation of the

5

Where Er,c
(17) at the linearisation point (i.e. at p = ˜p where Er,c
we now obtain

p denotes the Jacobian. For this to be equivalent to
p = I6),

Wr,c

p = H∗,
p,0 = −H∗+b∗.
er,c

(19)

(20)

Since H∗, b∗, and ˜p are constants, the Jacobians w.r.t. the
poses at steps r and c are fairly straightforward to determine
after substituting TSrSc = T −1

W Sr TW Sc .

2) Fixation of Old States: Furthermore, in order to keep
the number of poses being estimated limited, only the A most
recent states are kept variable. The number A is determined
as the maximum of a constant parameter Amin and A∆T ,
the latter standing for the number of states covering the time
interval ∆T into the past from the current frame.

r r MSTr keyframe pose (with observ.)IMU errorposegraph errorposegraph pose (no observ.)non-key pose (with observ.)E. Place Recognition, Relocalisation, and Loop Closure

Whenever a query of the current frame to the DBoW2 [34]
database returns a match, say to pose index l, and geometric
veriﬁcation using 3D-2D RANSAC passes,
the currently
active window of states and landmarks is transformed to be
re-aligned to the matched pose l. Then, the pose graph edges
connecting l will be “revived” and turned back into land-
marks and observations; and observations with the current
matching frame will also be created. This may also trigger
merging of landmarks, if already existing new landmarks are
matched to old landmarks in l. This scheme will produce
a set of loop-closure frames, L, that now has observations
again; but it will in general be part of the ﬁxed states of
the realtime estimation graph, as these loop closure frames
tend to lie arbitrarily far in the past. This concludes the
relocalisation.

To optimise the loop inconsistency, the following scheme
is applied: ﬁrst,
the error is distributed around the loop
using rotation averaging followed by position inconsistency
distribution, all in equal parts to the (keyframe) poses around
the loop. Then, a background optimisation process of the
factor graph is triggered. Hereby, a copy of the realtime
estimator graph is used, however, with different ﬁxation of
states (i.e. the states inside the closed loop remain variable).
Note how all the IMU error terms are also considered in this
loop closure optimisation, adding further constraints, most
notably regarding consistency of the orientation relative to
gravity. After the optimisation has ﬁnished, a synchronisation
process imports the optimised states and landmarks into the
realtime estimator, and re-aligns new states and landmarks
created in the meantime. During this optimisation, relocali-
sation attempts are suspended. Upon completion, the number
of loopclosure frames |L| is limited to a constant L applying
the same pose graph edge creation scheme as V-D.1.

VI. FRONTEND OVERVIEW

In the following, the remaining elements of the frontend

are brieﬂy overviewed.

A. Matching and initialisation

The visual frontend extracts BRISK 2 [35], [36] keypoints
and descriptors in every image of a multi-frame, and matches
them to the 3D landmarks already in the map; hereby both
descriptor distance and– conservatively–reprojected image
distance are considered. New 3D landmarks are then ini-
tialised both from stereo triangulation within the N images
of a keyframe, as well as from triangulation between live
frame and any of the keyframes.

B. Keyframe selection

The decision of whether a new frame is considered a
keyframe is taken depending on the fraction of matched
landmarks in the live frame ol, as well as the fraction of
current matches visible in any of the existing keyframes
(with index k ∈ K), ok. Speciﬁcally, these overlap fractions
are determined by the number of matched keypoint areas
divided by the total of all keypoint areas in the respective

(multi-image) frames; whereby the respective keypoint areas
are obtained from the respective union of ﬁlled circles of
radius rkpt around the keypoints. If the overlap

o = min(ol, max
k∈K

ok),

(21)

falls below a threshold to, we accept the (multi-)frame as a
new keyframe.

C. Segmentation CNN

The semantic segmentation network Fast-SCNN [37] is
run on all images of a keyframe. The network was ﬁne-
tuned on the Cityscapes dataset [38] for 80 epochs, where
images were ﬁrst converted to grayscale, in order to best
perform with gray images used as input to OKVIS2. For
maximum portability and ﬂexibility, inference is carried out
as an asynchronous background process on the CPU, which
is tractable, due to the efﬁcient network as well as the
fact that only keyframes are processed. Once completed,
matches into images regions that are almost guaranteed to
be dynamically changing are simply removed (currently only
the sky, but people, animals, etc. could be considered with
appropriate training). As will be shown in the experiments,
this scheme signiﬁcantly improves accuracy in presence of
slow-moving scene content, particularly clouds, where obser-
vations aren’t already automatically discarded via the Cauchy
robustiﬁcation. Please see Fig. 4 for example classiﬁcations.

Fig. 4: TUM-VI outdoors1 example sky classiﬁcations
(red) vs. used keypoints: indoors (top), and outdoors (bot-
tom). Notice how some erroneous sky classiﬁcations occur
also indoors. Further it can be seen that most of the sky
there are
keypoints are identiﬁed correctly outdoors, but
some false negatives (on the rightmost cloud) and false
positives (close to the horizon, especially on the leaf-less
tree branches).

6

VII. EXPERIMENTAL RESULTS
Results are provided for two commonly used datasets
in VI Odometry/SLAM evaluation: the ETHZ-ASL EuRoC
Dataset [39] and the TUM VI Benchmark [6], all providing
ground truth trajectories, but featuring different character-
istics in cameras, motion, (visual) environment and spatial
extent of the trajectories.

Two quantities are evaluated that
believes should be treated separately.

the author strongly

• The ﬁrst one is the accuracy in terms of odometry drift,
where loop-closures are disabled. The same method-
ology as in [25] is used, where statistics of relative
motion error (position/orientation) are aggregated for in
buckets of distance travelled. For completeness, relative
position/orientation errors with loop-closure enabled are
also reported, but note that the results are consequently
highly dependent on the motion, i.e. how frequently
loop-closures occur.

• The second one is the Absolute Trajectory Error (ATE)
where loop closure is taken into account, and where we
simply align the estimated and ground truth trajectories
in terms of position and yaw angle (roll and pitch are
globally observable and are thus not aligned) and report
statistics of the position differences. Here, it is important
to note that we can either evaluate the estimates that use
all measurements up to the respective timestamp, i.e. in
a causal manner, or the ﬁnal fully loop-closed trajectory
where all measurements are taken into account for all
estimates, i.e. as an non-causal evaluation. Both are
valid methods, but it should be stated clearly; and note
that in the former case, the drift (as already assessed in
the odometry evaluation) may dominate the ATE error
statistics as the trajectory will contain the characteristic
“jumps” when correcting for loop-closures.

The same estimator parameters were used throughout,
namely the T = 3 most recent frames are kept, K = 5
keyframes, and L = 5 loop-closure frames. Furthermore, at
least Amin = 12 pose-graph frames are optimised, where
all frames no older than ∆T = 2 sec are also always op-
timised. Frontend and IMU parameters are dataset-speciﬁc,
and respective conﬁguration ﬁles will be released together
with the code.

A. ETH-ASL EuRoC Dataset

Relative error statistics for the MH 05 difficult se-
quence of the on the EuRoC Dataset [5] are shown in Fig. 5
as an illustrative example of how OKVIS2 in VIO mode,
okvis2-vio, (i.e. with disabled loop closures) improves
over original OKVIS [25], okvis, and how loop closures
will reduce long-term drift.

What can also be easily seen is how the smoothness of
the non-causal evaluation of the ﬁnal optimised trajectory
okvis2-slam-final leads to substantially higher accu-
racy over the causal version okvis2-slam.

In Table I, Absolute Trajectory Error (ATE) results are
presented, also in comparison to state-of-the-art competitors
from both the VIO and VI-SLAM world.

Fig. 5: Relative trajectory error statistics for different
sub-trajectory lengths (y-axis) of the exemplary EuRoC
MH 05 difficult sequence.

)
s
r
u
o
(

O

I
V

2
S
I
V
K
O

1

]
3
[

n
o
i
s
u
F
S
N
I
V

)
s
r
u
o
(

l
a
s
u
a
c

2
S
I
V
K
O

1

]
4
[

3
M
A
L
S
-
B
R
O

)
s
r
u
o
(

2
S
I
V
K
O

1

]
1
[

T
L
A
S
A
B

1

]
2
[

a
r
e
m
K

i

2

]
5
2
[

S
I
V
K
O

Causal evaluation

Non-causal

VIO

VI-SLAM

MH 01 0.080
MH 02 0.060
MH 03 0.050
MH 04 0.100
MH 05 0.080

V1 01 0.040
V1 02 0.020
V1 03 0.030

V2 01 0.030
V2 02 0.020
V2 03
-

0.080
0.090
0.110
0.150
0.240

0.050
0.110
0.120

0.070
0.100
0.190

0.079
0.044
0.096
0.197
0.206

0.050
0.066
0.071

0.062
0.077
0.028

0.057
0.044
0.082
0.189
0.141

0.043
0.037
0.036

0.044
0.044
0.063

0.166
0.152
0.125
0.280
0.284

0.076
0.069
0.114

0.066
0.091
0.096

0.044
0.036
0.050
0.089
0.112

0.037
0.021
0.035

0.036
0.024
0.045

0.036
0.033
0.035
0.051
0.082

0.038
0.014
0.024

0.032
0.014
0.024

0.027
0.023
0.028
0.066
0.068

0.035
0.013
0.019

0.023
0.015
0.020

Avg

-

0.119

0.089

0.071

0.138

0.048

0.035

0.031

1 Results taken from [4].
2 Original OKVIS VIO re-run with IMU and frontend parameters as used
for OKVIS2.

TABLE I: EuRoC Absoltue Trajectory Error (ATE) in [m].

As can be seen, results on-par with or even slightly better

thatn ORB-SLAM3 [4] are achieved.

B. TUM VI Benchmark

The system is also benchmarked on TUM VI [6], with

respective ATE given in Table II.

As can be seen, OKVIS2 performs on-par with ORB-
SLAM3 on the shorter sequences corridor and room;
however, OKVIS2 clearly shows best results in the longer

7

010203040506070809010000.20.40.6Position error [m]010203040506070809010000.51Tilt error [deg]0102030405060708090100Distance travelled [m]012Azimuth error [deg]okvisokvis2-viookvis2-slamokvis2-slam-final)
s
r
u
o
(

O
I
V

2
S
I
V
K
O

)
s
r
u
o
(

l
a
s
u
a
c

2
S
I
V
K
O

1

]
4
[

3
M
A
L
S
-
B
R
O

1

]
3
2
[

O
I
V
O
R

1

]
1
[

T
L
A
S
A
B

2

]
5
2
[

S
I
V
K
O

C. Timings

)
s
r
u
o
(

2
S
I
V
K
O

s
e
r
u
s
o
l
c

p
o
o
l

]

m

[

h
t
g
n
e
l

The per-frame timings obtained are broken down in Ta-
ble III on EuRoC MH 05 difficult. These were mea-
sured on a PC running Ubuntu 20.04 featuring an 11th Gen
Intel® Core™ i7-11700K with 8 cores running at up to
3.60GHz.

Causal evaluation

Non-causal

Function

Timings (std. dev.) [ms]

VIO

VI-SLAM

r 1
o
2
d
3
i
r
4
r
o
5
c

Avg

e 1
l
2
a
3
r
t
4
s
i
5
g
a
6
m

0.47
0.75
0.85
0.13
2.09

0.86

4.52
13.43
14.80
39.73
3.47
-

Avg

-

0.34
0.42
0.35
0.21
0.37

0.34

1.20
1.11
0.74
1.58
0.60
3.23

1.41

0.65
0.70
1.58
0.15
0.41

0.47
0.49
0.44
0.11
0.47

0.09
0.06
0.02
0.18
0.09

0.03
0.02
0.02
0.21
0.01

0.09
0.06
0.02
0.18
0.09

0.70

0.40

0.09

0.06

0.09

2.91
2.76
1.08
3.03
1.18
2.26

1.79
3.03
1.66
3.43
1.36
2.85

0.09
0.52
0.93
0.76
0.17
0.63

0.24
0.52
1.86
0.16
1.13
0.97

0.03
0.03
0.93
0.04
0.01
0.63

305 (cid:88)
322 (cid:88)
300 (cid:88)
114
270 (cid:88)

262
918 (cid:88)
561 (cid:88)
566
688 (cid:88)
458 (cid:88)
771 ((cid:88))

2.20

2.35

0.52

0.81

0.28

660

1
2
3
4
5
6
7
8

101.95 255.04 109.75 22.60 24.92 32.32 24.92 2656
21.67
26.10
-
54.32
149.14
49.01
36.03

8.84 10.42
20.26 12.35
21.37 22.97 27.14 54.77 27.14 1531
7.42
928
7.43 11.61
14.05
6.67 1168 (cid:88)
14.74
7.57
6.67
32.26 20.54 26.37 10.70 26.37 2045
13.63
28.63

64.61
38.26
17.53
7.89
65.50
4.07
13.53

0.15 1748 (cid:88)
986 ((cid:88))
0.05

0.15
4.58
0.43 11.02

0.04 1601 ((cid:88))

2.92
1.20

9.68
5.99

Avg

-

58.30

31.83 12.28 11.75 17.87 11.60 1583

0.16
0.33
0.15
0.09
0.12
0.05

0.15

13.73
0.81
4.68

0.09
0.07
0.13
0.05
0.13
0.02

0.08

0.32
0.32
0.89

0.51

0.08
0.09
0.07
0.04
0.08
0.03

0.04
0.07
0.05
0.02
0.02
0.02

0.07

0.04

2.55
2.01
2.44

1.16
1.37
1.62

0.01
0.01
0.01
0.01
0.01
0.01

0.01

1.47
0.16
1.35

0.01
0.01
0.01
0.01
0.01
0.01

0.01
0.01
0.01
0.01
0.01
0.01

0.01

0.01

0.41
0.49
0.47

0.10
0.16
1.35

146 (cid:88)
142 (cid:88)
135 (cid:88)
68 (cid:88)
131 (cid:88)
67 (cid:88)

115
289 ((cid:88))
299 ((cid:88))
383

2.33

1.38

0.99

0.45

0.54

324

Avg

6.40

s
r
o
o
d
t
u
o

m
o
o
r

1
2
3
4
5
6

Avg

s 1
e
2
d
i
3
l
s

1 Results taken from [4].
2 Original OKVIS VIO re-run with IMU and frontend parameters as used
for OKVIS2.
3 ((cid:88)) signiﬁes loop closures only identiﬁed with OKVIS2.

TABLE II: TUM-VI Absolute Trajectory Error (ATE) in [m].

sequences of magistrale amd slides, as well as,
perhaps most notably, in the long outdoors sequences.
What should be noted is that especially in magistrale
and slides, OKVIS2 achieves more loop closures than
reported by ORB-SLAM3, denoted by ((cid:88)).

Regarding outdoors, the approach chosen by ORB-
SLAM3 to deal with slow-moving clouds is to remove all
observations to landmarks further than 40 metres, whereas
OKVIS2 uses CNN-based un-matching. Note that with ﬁne-
tuning to the heavily distorted ﬁsheye images of the dataset,
as well as deeper networks, there is room for improvement.

8

Detect & Describe
Match keypoints and triangulate
Attempt loop closures
Optimise realtime graph
Pose graph edge creation & handling

7.1
26.6
17.7
33.2
14.0

(±1.8)
(±12.4)
(±24.0)
(±9.2)
(±8.6)

TABLE III: Per-frame timings obtained on the EuRoC
sequence MH 05 difficult. Note that the loop closure
attempts are skipped in VIO mode. The synchronisation of
full graph optimisations running in the background is only
needed sporadically and amounts to 15.1(±7.4) ms, when
occurring. Furthermore, the detection and description is run
while the realtime optimisation is still ongoing. Therefore
the numbers cannot simply be summed here to check for
realtime capabilities.

Note that the realtime estimator will always carry out 10
optimisation steps here. A realtime mode is also implemented
that adjusts the optimisations per frame according to remain-
ing time budget after matching and loop-closure attempts–
which only minimally affects accuracy.

Loop closure optimisations carried out in the background
typically take tens of milliseconds up to around a second
for the very long loops e.g. in the TUM-VI outdoors
sequences.

VIII. CONCLUSIONS
A multi-camera VI-SLAM system was presented that
builds a factor graph of visual reprojection errors, IMU-
preintegrated error terms, as well as two-pose factors ob-
tained from marginalisation of common observations. A
realtime estimator minimises these in a bounded-size window
of recent keyframes and pose-graph frames. Upon loop-
closure, old pose-graph edges can seamlessly be turned back
into reprojection errors and landmarks. Furthermore, longer
loops can be asynchronously optimised re-using the same
factor-graph whereby keeping all states around the loop as
part of the optimised variables. A series of experiments
demonstrates that the method achieves competitive results
relative to state-of-the-art VI-SLAM systems.

As part of future work, we may want to address robust
handling of the monocular VI-SLAM case, as well as inclu-
sion of additional sensors, such as absolute positions. Fur-
thermore, we plan to release an open-source implementation
of OKVIS2.

Moreover, as a long-term goal, we will be exploring to
what extent dense and semantic map representations can be
used in a tightly-coupled manner in such a multi-sensor
fusion setting, further pushing the boundaries of an inte-
grated, robust and accurate Spatial AI solution empowering
the mobile robotics and AR/VR applications of the future.

International Conference on Robotics and Automation.
pp. 3565–3572.

IEEE, 2007,

[22] P. Geneva, K. Eckenhoff, W. Lee, Y. Yang, and G. Huang, “Openvins:
A research platform for visual-inertial estimation,” in 2020 IEEE
International Conference on Robotics and Automation (ICRA).
IEEE,
2020, pp. 4666–4672.

[23] M. Bloesch, M. Burri, S. Omari, M. Hutter, and R. Siegwart, “Iterated
extended kalman ﬁlter based visual-inertial odometry using direct pho-
tometric feedback,” The International Journal of Robotics Research,
vol. 36, no. 10, pp. 1053–1072, 2017.

[24] T. Schneider, M. Dymczyk, M. Fehr, K. Egger, S. Lynen, I. Gilitschen-
ski, and R. Siegwart, “maplab: An open framework for research in
visual-inertial mapping and localization,” IEEE Robotics and Automa-
tion Letters, vol. 3, no. 3, pp. 1418–1425, 2018.

[25] S. Leutenegger, S. Lynen, M. Bosse, R. Siegwart, and P. Furgale,
“Keyframe-based visual–inertial odometry using nonlinear optimiza-
tion,” The International Journal of Robotics Research, vol. 34, no. 3,
pp. 314–334, 2015.

[26] N. Keivan, A. Patron-Perez, and G. Sibley, “Asynchronous adap-
tive conditioning for visual-inertial slam,” in Experimental Robotics.
Springer, 2016, pp. 309–321.

[27] G. Grisetti, R. K¨ummerle, H. Strasdat, and K. Konolige, “g2o: A
general framework for (hyper) graph optimization,” in Proceedings
of the IEEE International Conference on Robotics and Automation
(ICRA), Shanghai, China, 2011, pp. 9–13.

[28] S. Agarwal and K. Mierle, “Ceres solver: Tutorial & reference,”

Google Inc, vol. 2, no. 72, p. 8, 2012.

[29] F. Dellaert, “Factor graphs and gtsam: A hands-on introduction,”

Georgia Institute of Technology, Tech. Rep., 2012.

[30] M. Kaess, A. Ranganathan, and F. Dellaert, “isam: Incremental
smoothing and mapping,” IEEE Transactions on Robotics, vol. 24,
no. 6, pp. 1365–1378, 2008.

[31] M. Kaess, H. Johannsson, R. Roberts, V. Ila, J. J. Leonard, and
F. Dellaert, “isam2: Incremental smoothing and mapping using the
bayes tree,” The International Journal of Robotics Research, vol. 31,
no. 2, pp. 216–235, 2012.

[32] S. Agarwal, K. Mierle, and Others, “Ceres solver,” http://ceres-solver.

org.

[33] C. Forster, L. Carlone, F. Dellaert, and D. Scaramuzza, “On-manifold
preintegration for real-time visual–inertial odometry,” IEEE Transac-
tions on Robotics, vol. 33, no. 1, pp. 1–21, 2016.

[34] D. G´alvez-L´opez and J. D. Tard´os, “Bags of binary words for fast place
recognition in image sequences,” IEEE Transactions on Robotics,
vol. 28, no. 5, pp. 1188–1197, October 2012.

[35] S. Leutenegger, M. Chli, and R. Y. Siegwart, “Brisk: Binary robust
invariant scalable keypoints,” in 2011 International conference on
computer vision.

Ieee, 2011, pp. 2548–2555.
[36] S. Leutenegger, “Unmanned solar airplanes: Design and algorithms for
efﬁcient and robust autonomous operation,” Ph.D. dissertation, ETH
Zurich, 2014.

[37] R. P. Poudel, S. Liwicki, and R. Cipolla, “Fast-scnn: Fast semantic
segmentation network,” arXiv preprint arXiv:1902.04502, 2019.
[38] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benen-
son, U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset for
semantic urban scene understanding,” in Proc. of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2016.

[39] M. Burri, M. Bloesch, Z. Taylor, R. Siegwart, and J. Nieto, “A
framework for maximum likelihood parameter identiﬁcation applied
on MAVs,” Journal of Field Robotics, vol. 35, no. 1, pp. 5–22, 2018.

REFERENCES

[1] V. Usenko, N. Demmel, D. Schubert, J. St¨uckler, and D. Cre-
mers, “Visual-inertial mapping with non-linear factor recovery,” IEEE
Robotics and Automation Letters, vol. 5, no. 2, pp. 422–429, 2019.

[2] A. Rosinol, M. Abate, Y. Chang, and L. Carlone, “Kimera: an open-
source library for real-time metric-semantic localization and mapping,”
in 2020 IEEE International Conference on Robotics and Automation
(ICRA).

IEEE, 2020, pp. 1689–1696.

[3] T. Qin, S. Cao, J. Pan, and S. Shen, “A general optimization-based

framework for global pose estimation with multiple sensors,” 2019.

[4] C. Campos, R. Elvira, J. J. G. Rodr´ıguez, J. M. Montiel, and
J. D. Tard´os, “Orb-slam3: An accurate open-source library for visual,
visual–inertial, and multimap slam,” IEEE Transactions on Robotics,
2021.

[5] M. Burri, J. Nikolic, P. Gohl, T. Schneider, J. Rehder, S. Omari,
M. W. Achtelik, and R. Siegwart, “The euroc micro aerial vehicle
datasets,” The International Journal of Robotics Research, 2016.
[Online]. Available: http://ijr.sagepub.com/content/early/2016/01/21/
0278364915620033.abstract

[6] D. Schubert, T. Goll, N. Demmel, V. Usenko,

J. Stueckler,
and D. Cremers, “The tum vi benchmark for evaluating visual-
in International Conference on Intelligent
inertial odometry,”
Robots and Systems (IROS), October 2018.
[Online]. Available:
https://arxiv.org/abs/1804.06120

[7] A. J. Davison, I. D. Reid, N. D. Molton, and O. Stasse, “Monoslam:
Real-time single camera slam,” IEEE transactions on pattern analysis
and machine intelligence, vol. 29, no. 6, pp. 1052–1067, 2007.
[8] G. Klein and D. Murray, “Parallel tracking and mapping for small ar
workspaces,” in 2007 6th IEEE and ACM international symposium on
mixed and augmented reality.

IEEE, 2007, pp. 225–234.

[9] R. Mur-Artal, J. M. M. Montiel, and J. D. Tardos, “Orb-slam: a
versatile and accurate monocular slam system,” IEEE transactions on
robotics, vol. 31, no. 5, pp. 1147–1163, 2015.

[10] R. Mur-Artal and J. D. Tard´os, “Orb-slam2: An open-source slam
system for monocular, stereo, and rgb-d cameras,” IEEE transactions
on robotics, vol. 33, no. 5, pp. 1255–1262, 2017.

[11] R. A. Newcombe, S. J. Lovegrove, and A. J. Davison, “Dtam: Dense
tracking and mapping in real-time,” in 2011 international conference
on computer vision.

IEEE, 2011, pp. 2320–2327.

[12] J. Engel, J. St¨uckler, and D. Cremers, “Large-scale direct slam
with stereo cameras,” in 2015 IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS).
IEEE, 2015, pp. 1935–1942.
[13] C. Forster, M. Pizzoli, and D. Scaramuzza, “Svo: Fast semi-direct
monocular visual odometry,” in 2014 IEEE international conference
on robotics and automation (ICRA).

IEEE, 2014, pp. 15–22.

[14] J. Engel, V. Koltun, and D. Cremers, “Direct sparse odometry,” IEEE
transactions on pattern analysis and machine intelligence, vol. 40,
no. 3, pp. 611–625, 2017.

[15] R. Wang, M. Schworer, and D. Cremers, “Stereo dso: Large-scale
direct sparse visual odometry with stereo cameras,” in Proceedings
of the IEEE International Conference on Computer Vision, 2017, pp.
3903–3911.

[16] X. Gao, R. Wang, N. Demmel, and D. Cremers, “Ldso: Direct
sparse odometry with loop closure,” in 2018 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS).
IEEE, 2018,
pp. 2198–2204.

[17] R. A. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux, D. Kim,
A. J. Davison, P. Kohi, J. Shotton, S. Hodges, and A. Fitzgibbon,
“Kinectfusion: Real-time dense surface mapping and tracking,” in
2011 10th IEEE international symposium on mixed and augmented
reality.

IEEE, 2011, pp. 127–136.

[18] T. Whelan, M. Kaess, M. Fallon, H. Johannsson, J. Leonard, and
J. McDonald, “Kintinuous: Spatially extended kinectfusion,” in RSS
Workshop on RGB-D: Advanced Reasoning with Depth Cameras,
2012.

[19] E. Vespa, N. Funk, P. H. Kelly, and S. Leutenegger, “Adaptive-
resolution octree-based volumetric slam,” in 2019 International Con-
ference on 3D Vision (3DV).

IEEE, 2019, pp. 654–662.

[20] T. Whelan, R. F. Salas-Moreno, B. Glocker, A. J. Davison, and
S. Leutenegger, “Elasticfusion: Real-time dense slam and light source
estimation,” The International Journal of Robotics Research, vol. 35,
no. 14, pp. 1697–1716, 2016.

[21] A. I. Mourikis and S. I. Roumeliotis, “A multi-state constraint kalman
ﬁlter for vision-aided inertial navigation,” in Proceedings 2007 IEEE

9

