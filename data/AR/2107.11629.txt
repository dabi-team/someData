Journal Submission

DOI 10.1007/s41095-xxx-xxxx-x

Research Article

Vol. x, No. x, ** 2021, xx–xx

ASOD60K: An Audio-Induced Salient Object Detection
Dataset for Panoramic Videos

Yi Zhang1

© The Author(s) 2021. This article is published with open access at Springerlink.com

1
2
0
2

v
o
N
2
1

]

V
C
.
s
c
[

4
v
9
2
6
1
1
.
7
0
1
2
:
v
i
X
r
a

Abstract Exploring to what humans pay attention
in dynamic panoramic scenes is useful
for many
fundamental applications, including augmented reality
(AR) in retail, AR-powered recruitment, and visual
in mind, we
language navigation. With this goal
propose PV-SOD, a new task that aims to segment
salient objects from panoramic videos. In contrast to
existing ﬁxation-/object-level saliency detection tasks,
we focus on audio-induced salient object detection
(SOD), where the salient objects are labeled with
the guidance of audio-induced eye movements. To
support this task, we collect the ﬁrst large-scale dataset,
named ASOD60K, which contains 4K-resolution video
frames annotated with a six-level hierarchy,
thus
distinguishing itself with richness, diversity and quality.
Speciﬁcally, each sequence is marked with both its
super-/sub-class, with objects of each sub-class being
further annotated with human eye ﬁxations, bounding
boxes, object-/instance-level masks, and associated
These
attributes
coarse-to-ﬁne annotations enable detailed analysis
for PV-SOD modeling, e.g., determining the major
challenges for existing SOD models, and predicting
long-term eye ﬁxation
scanpaths
behaviors of humans. We systematically benchmark
11 representative approaches on ASOD60K and derive
several
interesting ﬁndings. We hope this study
could serve as a good starting point for advancing
SOD research towards panoramic videos. The dataset
and benchmark will be made publicly available at
https://github.com/PanoAsh/ASOD60K.

geometrical distortion).

to study the

(e.g.,

Keywords 360° video salient object detection, saliency
detection, panoramic videos, benchmark.

1 Institut National des Sciences Appliqu´ees de Rennes

(INSA Rennes), France.

Manuscript received: 2021-07-24; accepted: 2021-XX-XX.

1

Introduction

cameras,

Surround360,

Recently, AI companies and manufacturers have
such as
developed several panoramic
Insta360 One, Ricoh
Facebook’s
and Google Jump VR, which produce
Theta,
omnidirectional1
images (Fig. 1 (b)) capturing a
scene with a 360◦ × 180◦ ﬁeld-of-view (FoV). Thus,
exploring human attention in dynamic scenes captured
by these devices
importance to
augmented/virtual reality (AR/VR) applications, e.g.,
shopping, online recruitment, piloting [32], automatic
cinematography [67], and immersive games.

signiﬁcant

is of

In practice, we have found that existing object-
level saliency detection (i.e., SOD) techniques and the
datasets that underpin their progress are subject to
two important limitations. First, the input source
only includes the visual information from images (e.g.,
I-SOD (Image SOD) [1, 2, 17, 76], CoSOD [20, 21,
25], RGB-D SOD [22, 27, 28, 50, 59, 101], RGB-
T SOD [68, 71], LFSOD [38, 60], HRSOD [91, 93],
Remote Sensing SOD [94]), or videos (e.g., V-SOD
(Video SOD) [23, 36, 74]) ignoring the auditory cues
that are ubiquitous in dynamic scenes [69, 70, 72].
Furthermore, all the above mentioned SOD tasks focus
on 2D images/videos, which are regarded as perspective
images with local FoVs (compared with 360°×180°
FoV), thus failing to capture the surrounding context
and corresponding layout of immersive real-life daily
scenes. However, these rich global geometrical cues are
crucial for human attention modeling.

To this end, we envision that segmenting salient
objects from panoramic videos with audio-visual data
will beneﬁt not only our research community but
also commercial products. To facilitate the study
of panoramic video salient object detection (PV-
SOD), we collect ASOD60K 2, the ﬁrst large-scale

1In the following sections, we use ‘omnidirectional’, ‘panoramic’,

and ‘360°’ interchangeably.

2Collecting the six types of

labels was a costly and time-

1

 
 
 
 
 
 
2

Y. Zhang

Fig. 1 Annotation examples from the proposed ASOD60K dataset. (a) Illustration of head movement (HM) [85]. The
subjects wear Head-Mounted Displays (HMDs) and observe 360° scenes by moving their head to control a ﬁeld-of-view (FoV) in the
range of 360°×180°. (b) Each subject (i.e., Subject 1 to Subject N) watches the video without restriction. (c) The HMD-embedded
eye tracker records their eye ﬁxations. (d) According to the ﬁxations, we provide coarse-to-ﬁne annotations for each FoV including
(e) super/sub-classes, instance-level masks and attributes (e.g., GD-Geometrical Distortion).

PV-SOD dataset providing professional annotations.
ASOD60K has several distinctive features:

• Hierarchical categories. All videos in the database
are labeled in a hierarchical manner, i.e., with
the super-class and sub-class.
The two-level
semantic categories provide a solid foundation for
not only weakly supervised approaches but also
fully supervised models.

• Diverse

subjects’

including

object-level masks,

annotations.
we

For
provide
the

each
video
coarse-to-ﬁne
sequence/frame,
annotations,
head
movement (HM) and eye ﬁxations, bounding
boxes,
and instance-level
labels, which can greatly beneﬁt diﬀerent
vision tasks (e.g., scanpath prediction, ﬁxation
prediction, SOD and salient instance detection).
• Attribute labels. Each sequence is annotated with
speciﬁc attributes, e.g., geometrical distortion,
occlusions, and motion blur.
Combined with
the performance of the evaluated models, these
attributes (Tab. 2 & Fig. 2) shed new light on the
experimental analysis.

• High quality. All video sequences are in high-
resolution (4K) to adapt to VR devices such
as Head-Mounted Displays (HMDs). Moreover,
cross-checking (i.e., more than three-fold) by
multiple experts and volunteers is conducted to
maintain reliability, accuracy, and consistency
during the whole annotation process.

The
important

aforementioned
support

for

provide
together
aspects
studying human attention

consuming work, and it took us about 1 year to set up this large-scale
database.

2

in panoramic videos.
Further, we summarize the
design rules that a balanced PV-SOD dataset should
fulﬁll, which can be used as reference for similar ﬁelds
when collecting and labeling data.

To reveal the challenges of PV-SOD, we perform
a set of empirical studies based on the collected
ASOD60K dataset. We obtain three interesting
observations. i) According to the overall (Sα < 0.7) and
attribute-based performance of the tested models, this
task is still far from being solved. ii) We ﬁnd that the
eye ﬁxations with audio are relatively consistent across
subjects while the data without audio exhibit large
ﬂuctuations between diﬀerent subjects. iii) A sparsely
labeled database is more beneﬁcial for image-based
models but more challenging for video-based models.
These ﬁndings clearly show the challenges of salient
object detection in panoramic videos.

In a nutshell, our main contributions are twofold: i)
We introduce ASOD60K, the ﬁrst large-scale dataset
for PV-SOD, which consists of 62,455 high-resolution
(4K) video frames from 67 carefully selected 360°
10,465 key frames are
panoramic video sequences.
annotated with rich labels, namely, super-class, sub-
class, attributes, HM data, eye ﬁxations, bounding
boxes, object masks, and instance masks.
ii) Based on
the established ASOD60K, we present a comprehensive
study on 11 representative models, which serves as the
ﬁrst standard leaderboard. Based on the evaluation
results, we present insightful conclusions that may
inspire novel ideas toward new research directions.

FoV(0°, 90°)(0°, 180°)(0°, 0°)(360°, 0°)(90°, 0°)(180°, 0°)HM  Position360°180°360 Panoramic VideoSubject 1Subject NFixationInstance(a)(b)(d)(c)(e)FoVTargetTargetCandidateSub-class: guiding Attributes: MP/OCaudioaudioSup-class: monologueSubject K FoV(0°, 90°)(0°, 180°)(0°, 0°)(360°, 0°)(90°, 0°)(180°, 0°)HM  Position360°180°360°Panoramic VideoSubject 1Subject NFixationInstance(b)(d)(c)(e)FoVTargetTargetCandidateSub-class: DirectorAttributes: e.g.,GDaudioaudioSuper-class: SpeakingSubject K (a)ASOD60K: An Audio-Induced Salient Object Detection Dataset for Panoramic Videos

3

Tab. 1 Summary of widely used salient object detection (SOD) datasets and the proposed panoramic video SOD (PV-SOD) dataset.
#Img: The number of images/frames. #GT: The number of ground-truth masks. Pub. = Publication. Obj.-Level = Object-Level.
Ins.-Level = Instance-Level. Fix. GT = Fixation-guided ground truths. † denotes equirectangular (ER) images.

Dataset

HKU-IS [44]

PASCAL-S [48]

SegTrack V2 [41] V-SOD 2013

ECSSD [88]
DUT-OMRON [89]

MCL [40] V-SOD 2015
ViSal [79] V-SOD 2015

Task
Pub.
Year
I-SOD 2013 CVPR
I-SOD 2013 CVPR
ICCV
I-SOD 2014 CVPR
FBMS [54] V-SOD 2014 TPAMI
I-SOD 2015 CVPR

1,000
5,168
1,065
850
720
4,447
463
193
3,455
15,572 15,572
1,000
3262
6,000
7,467
23,938 23,938
107
500
1,105
ASOD60K (OUR) PV-SOD 2021 CVMJ 62,455† 10,465

DAVIS2016 [58] V-SOD 2016 CVPR
I-SOD 2017 CVPR
I-SOD 2017 CVPR
UVSD [51] V-SOD 2017 TCSVT
I-SOD 2018 ECCV

#Img #GT min(W, H) max(W, H) Obj.-Level Ins.-Level Attribute Fix. GT Audio
(cid:88)
1,000
(cid:88)
5,168
(cid:88)
1,065
(cid:88)
850
(cid:88)
13,860
(cid:88)
4,447
(cid:88)
3,689
(cid:88)
963
(cid:88)
3,455
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

107†
500†
360-SSOD [53] PI-SOD 2020 TVCG 1,105†

139
139
212
139
253
100
270
240
900
100
142
240
161
312
360
1,024
512
546
1,920

400
401
640
500
960
500
480
512
1,920
500
400
877
849
800
640
2,048
1,024
1,024
3,840

DAVSOD [23] V-SOD 2019 CVPR
ICIP

SOC [17]
VOS [47] V-SOD 2018

360-SOD [46] PI-SOD 2020 JSTSP

1,000
3262
6,000
116,103

F-360I-SOD [95] PI-SOD 2020

DUTS [75]
ILSO [42]

TIP
TIP

(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)

TIP

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

2 Related Work

Human attention modeling in panoramic videos can
be roughly split into four levels: HM prediction [85],
eye ﬁxation/gaze prediction [81, 86], salient object
detection (SOD), and salient instance detection. Our
work mainly focuses on the object-level task, leaving
other tasks to our future studies.
In this section,
we only brieﬂy discuss some closely related works,
i.e., datasets, models, and techniques for 360° image
processing.

2.1 Datasets

Image Salient Object Detection (I-SOD). The
image-based SOD task has gained signiﬁcant attention
in the past few years. The remarkable progress of I-
SOD is highly related to the development of several
representative datasets [17, 42, 44, 48, 75, 88, 89].
ECSSD [88], DUT-OMRON [89], PASCAL-S [48], and
HKU-IS [44] are four early, small-scale datasets with
limited image resolution. To increase the amount
of training data, DUTS [75] was introduced and
has become one of the most popular benchmarks.
Furthermore, ILSO [42] and SOC [17] were recently
proposed with the goal of enabling not only object-level
but also instance-level SOD tasks. We refer the reader
to the survey paper by [76] for a thorough review.
Video Salient Object Detection (V-SOD). In
addition to I-SOD datasets, several V-SOD benchmarks
have also been introduced. Tab. 1 summarizes their
details. As can be seen, DAVSOD [23] is the largest
dataset and provides comprehensive annotations for the
V-SOD task.

Panoramic Image Salient Object Detection (PI-
SOD). There are three attempts toward establishing
datasets for 360° panoramic image-based SOD [46, 53,
95], all of which provide pixel-wise object-level ground
truths (GTs) with similar resolution (e.g., max(w, h) =
2, 048). 360-SOD [46] is the pioneering work for SOD
in 360◦ scenes. It consists of 500 equirectangular (ER)
images (the most widely used planar representation of
360° image without any loss of spatial information)
representing both the indoor/outdoor scenes with
360-SSOD [53] is a larger
object-level annotations.
public PI-SOD dataset that has 1,105 semantically
balanced panoramic (ER) images. Besides, F-360iSOD
[95] is so far the only 360° image SOD dataset that
provides pixel-wise instance-level GTs.
Other Datasets. Other closely related datasets are
either designed to simulate human HM (360VHMD [11]
and PVS-HMEM [85]), or eye ﬁxations (i.e., saliency
detection) in 360°/panoramic images (Salient!360 [63])
or 360° videos (e.g., Wild-360 [5], VR-Scene [86], 360-
Saliency [96]). As far as we know, Chao et al.’s[3]
work is the only 360◦ audio-visual saliency dataset,
which contains 12 videos with HM-based annotations
under mute, mono, and ambisonics modalities. Finally,
datasets such as [7, 33, 90] focus on bounding-box-level
object detection in 360°.

As summarized in Tab. 1, no work exists to study
the segmentation of salient objects in free-viewing
panoramic videos with audio. The closest works are
audiovisual saliency detection from 2D videos [35, 70] in
a plane and salient object detection in omnidirectional
images [46, 53, 95] without audio. We refer readers to
the survey paper about 360° data processing [16, 84] for

3

4

Y. Zhang

Fig. 2 Examples of challenging attributes (see Tab. 2) on ER images from our ASOD60K, with instance-level GT and ﬁxations as
annotation guidance. fk,l,m denote random frames of a given video. Best viewed in color. More examples are shown in Fig. 17.

more details.

2.2 SOD Models

Since no SOD approaches currently exist for the PV-
SOD task, we present the SOD methodologies for I-
SOD, V-SOD, and PI-SOD.
Algorithms for I-SOD. In the past few years,
convolutional neural networks (CNNs) have been the
most commonly used architecture in state-of-the-art
(SOTA) I-SOD models [26, 29, 39, 49, 55, 61, 62, 77,
82, 83, 97, 100, 102, 103], which are trained on large-
scale datasets (e.g., DUTS [75]) in a fully supervised
manner. ASNet [77] uses eye ﬁxations to aid salient
object localization, while models such as AFNet [26],
BASNet [61], PoolNet [49], EGNet [97], SCRN [83],
and LDF[80] emphasize object appearance (boundaries
or skeletons) as guidance for the accurate segmentation
of salient objects. With comparable accuracy, methods
such as CPD [82], ITSD [100], and CSNet [29] also
achieve improved inference speed.
Models for V-SOD. The recent development of large-
scale video datasets such as DAVIS [58] and DAVSOD
[23] have enabled deep learning-based V-SOD. Several
works [43, 45, 87] have achieved success by introducing
optical ﬂow cues into the network. There is, however,
the long-standing and often ignored issue of saliency
shift, which was ﬁrst highlighted and modeled in
SSAV [23]. According to the open benchmark results,

COSNet [52], RCRNet [87], and PCSA [30] obtain best
performances in the V-SOD task.
Methods for PI-SOD. To the best of our knowledge,
DDS [46], stage-wise SOD [53] and FANet [34] are so
far the only models exclusively designed for PI-SOD.
They all emphasize the importance of mitigating the
geometrical distortion brought by ER projection via
speciﬁc modules.
2.3 360° Image Processing Approaches

The vast majority of current 360° image processing
techniques are CNN-based, proposed for either ER or
stereoscopic images (e.g., spheres and icosahedrons).
ER projection is the
CNNs on ER Images.
most widely used approach for the 2D representation
of a 360◦ image.
It applies a uniform grid-based
sampling method on the spherical surface,
followed
by an inevitable over-sampling of spherical regions
near poles. Therefore, salient objects in ER images
may suﬀer geometrical distortions to varying extents,
depending on the distance between their geometrical
locations and the equator of the ER image (Fig. 2).
SphereNet [10], which consists of a location-adaptive
kernel, was proposed for the classiﬁcation and detection
of objects in ER images. Similar location-dependent
convolutional kernels are also applied in [65, 66].
CNNs on Stereoscopic Images. As there is no
perfect 2D representation for 360◦ images, SO(3)-

4

MOOCLSMBGDOVSingingDancingQuestionsBeachBrothersSpanishChineseAdGuitar,fkGuitar,flGuitar,fmCSASOD60K: An Audio-Induced Salient Object Detection Dataset for Panoramic Videos

5

based spherical CNNs
[8, 15] were proposed to
directly generalize convolutions on a sphere. However,
these reparameterized 3D convolutional kernels hinder
the use of classical backbones (e.g., ResNet [31]
or VGGNet [64]) pre-trained on large-scale datasets
(e.g., ImageNet [13]), which play an essential role
in CNN-based SOD models (see Sec. 2.2). Recent
researches
[9, 37, 92] generalize convolutions on
subdivided icosahedral faces, which contain much less
geometrical distortion compared to ER images [14]. In
addition, tangent image [14] was proposed to enable
the implementation of semantic segmentation in 4K-
resolution 360◦ images.

3 Proposed ASOD60K Dataset

We elaborate our ASOD60K in terms of stimuli
annotation
collection,
pipeline and dataset statistics. Our goal is to introduce
a new challenging dataset to the PV-SOD community.

experimentation,

subjective

3.1 Stimuli Collection

The

with

(e.g.,

travel,

sports,

stimuli

diﬀerent

keywords

of ASOD60K were

searched
on YouTube
(e.g.,
360°/panoramic/omnidirectional video, spatial audio,
ambisonics [56]). As a result, our collected stimuli cover
various real-world scenes (e.g., indoor/outdoor scenes),
diﬀerent occasions
concerts,
interviews, dramas), diﬀerent motion patterns (e.g.,
static/moving camera), and diverse object categories
(e.g., humans,
instruments, animals). They possess
a wide range of major challenges found in 360°
content3, providing us with a solid foundation to
In this way, we
build a representative benchmark.
obtained about 1,000 noisy videos, e.g., videos with a
shaking camera, dark-screen transitions, without key
content, displaying too many objects, of low quality.
In line with the video dataset creation rules in [23, 78],
we then carefully collected 67 high-quality video
sequences with a total of 62,455 frames recorded with
62,455×40 HM and eye ﬁxations. The 67 sequences
are selected based on the following two criteria, 1) The
source video must be in good visual quality, i.e., 4K
resolution for each video frame. 2) The scenes must
include meaningful objects on which high saliency
constantly focuses.
In other words, scenes such as
busy streets and carnivals where subjects’ attention
simultaneously scattering on multiple non-relevant
objects, are removed from the dataset. Similar to [11],
the frame rate of each collected video is not ﬁxed
3Objects scattered far from the equator thus suﬀering from

serious geometrical distortions in ER projections.

(varying from 24fps to 60fps), which did not inﬂuence
the results of following subjective experiments since
human attention is mainly event-related, rather than
frame rate-dependent. Note that we manually trimmed
the videos into small clips (29.6s on average) to avoid
fatigue during the collection of human eye ﬁxations.
As a result, the ﬁnal duration is 1983s in total.

3.2 Subjective Experimentation

Equipment. All the video clips were displayed using
a HTC Vive HMD embedded with a Tobii eye tracker
with 120Hz sample rate to collect eye ﬁxations.
Observers. We recruited 40 participants (8 females
and 32 males) aging from 18 to 34 years old who
reported normal or corrected-to-normal visual and
audio acuity. Twenty participants were randomly
selected to watch videos with mono sound, while the
other participants watched videos without sound. Note
that the two groups own the same gender and age
distributions. Hence, each video with each audio
modality (i.e., with or without sound) was viewed by
20 participants, and each participant viewed each video
only once. We performed task-free viewing sessions.
Settings. All the participants seated in a swivel chair,
wearing a HMD with headphones, and asked to explore
the panoramic videos without any speciﬁc intention.
During the experiments, the starting position was ﬁxed
to the center (θ = 0◦ and φ = 0◦) at the beginning of
every video display. To avoid motion sickness and eye
fatigue, we inserted a short rest of a ﬁve-second gray
screen between two successive videos and a long break
of 20 minutes after every 20 videos. We calibrated the
system for each participant at the beginning and end
stage of every long break.

3.3 Professional Annotation

Super-/Sub-Class Labeling. As shown in Fig. 3,
our ASOD60K contains 67 videos representing three
super-categories of audio-introduced scenes, including
speaking (e.g., monologue, conversation), music (e.g.,
instrument playing) and miscellanea
human singing,
(e.g., the sound of vehicle engines and horns on the
streets, crowd noise in the open air). Each video is
named in terms of its audio-visual information.
Head Movement and Eye Fixations. The recent
video object segmentation dataset DAVIS [58] contains
only one or several foreground objects per frame, where
the salient objects can be easily deﬁned. In contrast,
other recent video SOD datasets, such as VOS [47] and
DAVSOD [23], collect video stimuli representing more
challenging scenes with multiple salient objects. In such

5

6

Y. Zhang

Fig. 3 Statistics of the proposed ASOD60K. (a) Super-/sub-category information. (b) Instance density of each sub-class. (c) Main
sound sources of ASOD60K scenes, such as musical instruments, human instances and animals. Best viewed in color.

cases, ﬁxation-based annotations (e.g., saliency-shift
[23]) are used as guidance to alleviate the ambiguity
of deﬁning salient objects. Based on the subjects’
per-frame HM and eye ﬁxations gained by conducting
the subjective experiments (Sec. 3.2) with audio-visual
stimuli, we produced the ﬁnal annotations. Speciﬁcally,
inspired by the experimental IoU threshold, i.e., AP50
(threshold set as 50%), which is widely used in the ﬁeld
of object detection, we choose to keep the 50% of the
smoothed ﬁxation map regions which thus covers the
In this way, we regard the
top 50% of the saliency.
top 50% saliency as high saliency and locate the salient
objects overlapped with the high saliency regions.
Bounding Box Annotations. Generally, there are
i.e.,
two types of
bounding FoVs [7, 90, 98] and bounding boxes [90]. As
the vast majority of our collected video frames contain
multiple salient objects near the 360 camera, bounding
FoVs may introduce serious annotation ambiguities due
to the divergence of projection angle selections between
multiple annotators [90], thus not being suitable for
the salient object annotation in the scenes from our
ASOD60K. Following [90], we directly annotated the
salient objects with bounding boxes in ER images.

labels in 360° object detection,

Tab. 2 Attributes description (see examples in Fig. 2).

Att. Description
MO Multiple Objects. (cid:62) three objects occur simultaneously.
OC Occlusions. Object is partially occluded.
Low Space. Object occupies (cid:54) 0.5% of image area.
LS
MB Motion Blur. Moving object with fuzzy boundaries.
OV Out-of-View. Object is cut in half in ER projection.
GD Geometrical Distortion. Distorted object in ER projection.
CS

Competing Sounds. Sound objects compete for attention.

6

is

threefold:

the key frames.

Our annotation protocol

i) We
uniformly extracted 10,465 key frames from the total
62,455 frames with a sampling rate of 1/6.
ii)
We ﬁltered the Gaussian-smoothed eye ﬁxation maps
corresponding to each of
iii)
We adopted the widely used CVAT toolbox as our
annotation platform, and recruited an expert
to
manually annotate the bounding box of each salient
object in each of the ER key frames, under the guidance
of the corresponding ﬁxations overlaid (see Fig. 2).
Finally, we obtained total 19,904 salient objects labeled
with instance-level bounding boxes from 10,465 key
frames. To the best of our knowledge, this is the ﬁrst
attempt to annotate salient objects with the guidance
of audio-visual attention data.
Object-Level Annotations. With the coarse
annotations (i.e., bounding boxes) in hand, we needed
to further label the data in a ﬁne manner. Thus, three
experts were recruited to manually annotate the salient
objects in the ∼10K key frames. To ensure satisfying
annotations, they were ﬁrst required to pass a training
session4 during which they had to correctly segment (by
ﬁnely tracing objects’ boundaries rather than drawing
rough polygons) all the salient objects in a given video
(previously shown to three senior researchers, with GTs
acquired by consistent opinions), with the guidance
of overlaid per-frame bounding boxes. Followed by a
session during which they were asked to annotate all
the deﬁned salient objects in the rest of the panoramic
videos. Finally, a thorough inspection was conducted
by the same three senior researchers, to ensure the
Following the same
accuracy of the annotations.

4Note that it took the experts about 10 hours.

JazzClarinetSingingDancingBluesBassWaitingRoomStudioGuitarParkingLotMelodramaViolinsSkiingCannonScenePlayQuestionsTennisPianoSaxophoneSubwayBadmintonConvoDirectorSpanishBrothersFilmingSiteCookingDebateAudiAdPianoMonoFootballTrumpetFrenchChurchDogTelephoneCarriageRuralDrivingBridgeBadmintonGymMICOSingingAudiIntroBreakfastGroveActionGymnasiumCorusDuetJapaneseLawnInterviewExhibitionPianoConvoInVehicleParkSnowfieldPassagewayWarehouseDieselRapChineseAdWalkingTrainBeachRacingCarSurfingEllenLionUrbanDrivingPlatformGroveConvo200400600ViolinSaxophoneDrumSetBassFlutePianoTrumpetSingingSpeechAnimalsExhibition161635SpeakingMusicMiscellanea(a)(b)(c)ASOD60K: An Audio-Induced Salient Object Detection Dataset for Panoramic Videos

7

Tab. 3 Attribute details. General attributes: MO = multiple
objects. OC = occlusions. LS = low space. MB = motion
360° geometrical attributes: OV = out-of-view. GD
blur.
= geometrical distortion.
Spatial audio attributes: CS =
competing sounds.

Fig. 4 Average object-level GT maps of 360-SOD [46], 360-
SSOD [53] and our ASOD60K.

Sequence

General

360° Audio

MO OC LS MB OV GD CS

No.

Fig. 5 The average object-level GT maps of our ASOD60K at
super-class level.

pipeline as [20], we obtained 10,465 object annotations.
Instance-Level Annotations. Another three well-
trained experts were then recruited to further draw
pixel-wise instance-level masks by carefully tracing
boundaries (rather than rough polygons) of the deﬁned
salient objects in each of the 10,465 key frames. To
ensure high quality annotations, all the masks were
sent to a quality check procedure implemented by
the same three senior researches. As a result, we
acquired 19,904 instance-level masks representing all
the salient objects in all the 10,465 key frames (the
number of instances in each video are shown in Fig. 3
(b)). Further, to reﬁne the annotations quality, we
transferred all the instance-level masks to object-level
binary masks. The bounding boxes were also reﬁned
by the object-level masks. Please refer to Fig. 17 for
annotation examples.
Attribute Labels. Following two large-scale video
object segmentation datasets [23, 58], we also provide
seven attributes in the proposed ASOD60K, including
multiple objects (MO), occlusions (OC),
low space
(LS), out-of-view (OV), motion blur (MB), geometrical
distortion (GD) and competing sounds (CS) (Table 2).
It is worth mentioning that, OV and GD (Fig. 2) are

)
5
3
(

g
n
i
k
a
e
p
S

)
6
1
(

c
i
s
u
M

(cid:52)

(cid:52)

(cid:52)

(cid:52)

(cid:52)

(cid:52)
(cid:52)

(cid:52)
(cid:52) (cid:52) (cid:52)

(cid:52) (cid:52) (cid:52) (cid:52)
(cid:52) (cid:52) (cid:52) (cid:52)
(cid:52) (cid:52) (cid:52) (cid:52)
(cid:52)

(cid:52) (cid:52) (cid:52) (cid:52)
(cid:52) (cid:52)
(cid:52) (cid:52) (cid:52) (cid:52)
(cid:52)
(cid:52) (cid:52) (cid:52)

(cid:52)
(cid:52)
(cid:52) (cid:52) (cid:52) (cid:52) (cid:52) (cid:52)
(cid:52)
(cid:52) (cid:52) (cid:52)
(cid:52)
(cid:52)
(cid:52)
(cid:52) (cid:52) (cid:52)
(cid:52) (cid:52) (cid:52) (cid:52)
(cid:52) (cid:52)
(cid:52) (cid:52) (cid:52)
(cid:52) (cid:52) (cid:52) (cid:52)
(cid:52)

French
WaitingRoom
Cooking
AudiIntro
Ellen
GroveAction
Warehouse
GroveConvo
Surﬁng
Passageway
RuralDriving
Lawn
AudiAd
ScenePlay
UrbanDriving
Interview
Telephone
Walking
Bridge
Breakfast
Debate
BadmintonConvo (cid:52) (cid:52) (cid:52) (cid:52) (cid:52) (cid:52)
(cid:52)
(cid:52) (cid:52) (cid:52) (cid:52)
Director
(cid:52) (cid:52) (cid:52) (cid:52) (cid:52) (cid:52)
ChineseAd
Exhibition
PianoConvo
FilmingSite
Brothers
Rap
Spanish
Questions
PianoMono
Snowﬁeld
Melodrama
Gymnasium

(cid:52)
(cid:52) (cid:52) (cid:52) (cid:52)
(cid:52) (cid:52) (cid:52) (cid:52)
(cid:52) (cid:52) (cid:52) (cid:52)
(cid:52) (cid:52) (cid:52) (cid:52)
(cid:52) (cid:52) (cid:52)
(cid:52) (cid:52) (cid:52) (cid:52)
(cid:52)

(cid:52) (cid:52) (cid:52)
(cid:52) (cid:52) (cid:52) (cid:52)

(cid:52)
(cid:52)
(cid:52)
(cid:52)

(cid:52)
(cid:52)

(cid:52)

(cid:52)

(cid:52)

(cid:52)

(cid:52)

(cid:52)

(cid:52) (cid:52) (cid:52)
Guitar
(cid:52) (cid:52) (cid:52) (cid:52)
Subway
(cid:52) (cid:52) (cid:52)
Jazz
(cid:52) (cid:52) (cid:52)
Bass
(cid:52) (cid:52) (cid:52)
Canon
(cid:52) (cid:52)
MICOSinging
(cid:52) (cid:52) (cid:52)
Clarinet
(cid:52) (cid:52) (cid:52)
Trumpet
PianoSaxophone (cid:52) (cid:52) (cid:52)
(cid:52) (cid:52) (cid:52)
Chorus
(cid:52) (cid:52) (cid:52) (cid:52)
Studio
(cid:52) (cid:52) (cid:52)
Church
(cid:52) (cid:52)
Duet
(cid:52) (cid:52) (cid:52)
Blues
(cid:52) (cid:52) (cid:52)
Violins
(cid:52) (cid:52) (cid:52) (cid:52)
SingingDancing

(cid:52)
(cid:52)
(cid:52)

(cid:52)
(cid:52)

(cid:52)

(cid:52)

(cid:52)

(cid:52)

) Beach
6
1
(

a
e
n
a
l
l
e
c
s
i

M

BadmintonGym
InVehicle
Japanese
Tennis
Diesel
Park
Lion
Carriage
Platform
Dog
RacingCar
Train
Football
ParkingLot
Skiing

(cid:52) (cid:52) (cid:52) (cid:52)
(cid:52) (cid:52) (cid:52) (cid:52)
(cid:52) (cid:52) (cid:52)

(cid:52)
(cid:52) (cid:52) (cid:52)
(cid:52)
(cid:52)

(cid:52) (cid:52) (cid:52) (cid:52)
(cid:52) (cid:52) (cid:52)
(cid:52) (cid:52) (cid:52) (cid:52)
(cid:52) (cid:52)
(cid:52) (cid:52) (cid:52) (cid:52) (cid:52) (cid:52)
(cid:52)
(cid:52) (cid:52) (cid:52) (cid:52)
(cid:52)
(cid:52) (cid:52)
(cid:52)
(cid:52)
(cid:52)
(cid:52)
(cid:52)

(cid:52)

(cid:52)
(cid:52) (cid:52) (cid:52) (cid:52)
(cid:52) (cid:52) (cid:52) (cid:52)
(cid:52) (cid:52) (cid:52) (cid:52)

(cid:52)
(cid:52)

(cid:52)
(cid:52)

(cid:52)

(cid:52)

(cid:52)

(cid:52)

(cid:52)

(cid:52)
(cid:52)
(cid:52)

(cid:52)
(cid:52)
(cid:52)

(cid:52)

(cid:52)
(cid:52)

(cid:52)

(cid:52)
(cid:52)
(cid:52)
(cid:52)
(cid:52)

(cid:52)
(cid:52)
(cid:52)
(cid:52)
(cid:52)
(cid:52)
(cid:52)
(cid:52)

(cid:52)

(cid:52)
(cid:52)

(cid:52)
(cid:52)

5
5
5
3
1
5
2
5
3
4
4
2
6
5
3
4
5
3
4
5
3
7
6
6
1
3
5
6
4
5
4
5
3
5
5

4
5
5
5
4
4
5
3
5
4
5
4
4
4
5
6

4
4
4
4
5
4
4
2
6
5
4
4
4
4
6
6

No.

56

52 59 40

8

39

35

289

exclusive geometrical attributes of ER images, and CS
is a novel attribute attached to audio-visual stimuli
(Please see per-video attributes’ statistics in Tab. 3).

3.4 Dataset Features and Statistics

Attribute Distribution. The attributes summarize
natural daily scenes viewed in omni-direction, also
inspire model development for PI-SOD and PV-SOD.

Fig. 6 Attributes statistics. (a)/(b) represent the correlation
and frequency of ASOD60K ’s attributes, respectively.

7

360-SOD360-SSODASOD60K(OUR)SpeakingMusicMiscellaneaCSMOOCLSMBOVGDLSMOOCMBGDCSOV2040600(a)(b)59565239358408

Y. Zhang

Fig. 7 Passed and rejected examples of annotation quality control.

As shown in Fig. 6, the seven proposed attributes are
closely related to each other, representing challenging
common scenarios.
Equator Center Bias. Fig. 4 and Fig. 5 visualize the
global and super-class center bias [17, 24] of ASOD60K.
Compared to 360-SOD and 360-SSOD, our dataset
shows stronger center bias as it is the only one with
salient objects annotated according to participants’ eye
ﬁxations, with the starting point set to the center of
each video display during the subjective experiments.
It has been broadly proved that photographers tend
to capture the main content of a 360° video at the
equator center, and that users also usually pay more
attention to such an area during watching [12, 16, 84].
Our ASOD60K is hence more able to reﬂect real-world
viewing behaviors compared to the 360-SOD [46] and
360-SSOD [53].
Following [17], we compute the
Instance Size.
normalized instances’ size of our ASOD60K. The size
distribution ranges from 0.03% ∼ 23.00%, covering very
small objects.
Quality Control. High-quality annotation is one of
the most important aspects of training for learning-
based models. As illustrated in Fig. 7, we carefully
conduct
three-fold cross-validation to ensure the
annotation quality.

8

4 Empirical Studies

4.1 Settings

instance-/object-level GTs.

Dataset Splits. All 67 videos are split into separate
training and test sets with a random selection strategy,
in a ratio of about 6:4. Therefore, we reach a unique
split of 40 training and 27 test videos (5,796/4,669
key frames respectively), with corresponding per-
pixel
The testing set
is further divided into test0/test1/test2 with 6/6/15
videos, respectively, according to super-class labels
(i.e., Miscellanea/Music/Speaking).
Metrics. We apply three widely used SOD metrics
to quantitatively compare the SOTA I-SOD/V-SOD
models. These metrics include structural measure (S-
Measure, Sα) [4, 6], maximum enhanced-alignment
measure (E-Measure, Eφ) [18, 19] and mean absolute
error (MAE) [57]. The MAE [57] focuses on the local
(per-pixel) match between ground truth and prediction,
while S-Measure (Sα) [4] pays attention to the object
structure similarities. Besides, E-Measure (Eφ) [18]
considers both the local and global information.
MAE computes the mean absolute error between the
ground truth G ∈ {0, 1} and a normalized predicted
saliency map P ∈ [0, 1], i.e.,
H
(cid:88)

W
(cid:88)

| G(i, j) − P (i, j) |,

(1)

M AE =

1
W × H

j=1
where H and W denotes height and width, respectively.

i=1

PassRejectRejectPassRejectPassASOD60K: An Audio-Induced Salient Object Detection Dataset for Panoramic Videos

9

Tab. 4 Performance comparison of 7/3 state-of-the-art conventional I-SOD/V-SOD methods and one PI-SOD method [34] over
ASOD60K. ↑/↓ denotes a larger/smaller value is better. Best result of each column is bolded.

Type

Publication

Methods

CPD [82]
CVPR’19
SCRN [83]
ICCV’19
F3Net [39]
AAAI’20
MINet [55]
CVPR’20
LDF [80]
CVPR’20
ECCV’20
CSF [29]
ECCV’20 GateNet [99]
CVPR’19
COSNet [52]
ICCV’19 RCRNet [87]
PCSA [30]
AAAI’20
FANet [34]
SPL’20

I-SOD

V-SOD

PI-SOD

Miscellanea (Test0)

Music (Test1)

Speaking (Test2)

Sα ↑
0.654
0.665
0.655
0.650
0.663
0.652
0.677
0.610
0.661
0.602
0.610

Eφ ↑
0.584
0.564
0.557
0.557
0.557
0.575
0.596
0.535
0.576
0.549
0.513

M ↓

0.035
0.046
0.040
0.050
0.044
0.033
0.044
0.031
0.034
0.034
0.030

Sα ↑
0.608
0.683
0.662
0.670
0.671
0.665
0.673
0.577
0.695
0.655
0.646

Eφ ↑
0.823
0.841
0.801
0.789
0.828
0.833
0.852
0.825
0.839
0.764
0.814

M ↓

0.018
0.023
0.021
0.020
0.023
0.018
0.018
0.016
0.019
0.021
0.018

Sα ↑
0.588
0.636
0.626
0.590
0.625
0.636
0.633
0.572
0.632
0.572
0.566

Eφ ↑
0.756
0.739
0.716
0.680
0.761
0.791
0.739
0.722
0.775
0.679
0.696

M ↓

0.026
0.034
0.027
0.053
0.037
0.026
0.034
0.023
0.030
0.026
0.027

Tab. 5 Performance comparison of 7/3/1 state-of-the-art I-SOD/V-SOD/PI-SOD methods based on each of the attributes.

Attr.Metrics

CPD [82] SCRN [83] F3Net [39] MINet [55] LDF [80] CSF [29] GateNet [99] COSNet [52] RCRNet [87] PCSA [30] FANet [34]

I-SOD

V-SOD

PI-SOD

MO

OC

LS

MB

OV

GD

CS

0.610
Sα ↑
0.741
Eφ ↑
0.027
M ↓
0.606
Sα ↑
0.772
Eφ ↑
0.023
M ↓
0.605
Sα ↑
0.721
Eφ ↑
0.025
M ↓
0.622
Sα ↑
0.728
Eφ ↑
0.021
M ↓
0.634
Sα ↑
Eφ ↑
0.844
M ↓ 0.018
0.630
Sα ↑
0.680
Eφ ↑
0.037
M ↓
0.625
Sα ↑
0.748
Eφ ↑
0.029
M ↓

0.657
0.740
0.034
0.655
0.768
0.029
0.649
0.723
0.034
0.651
0.718
0.029
0.661
0.786
0.021
0.662
0.690
0.042
0.680
0.759
0.035

0.644
0.702
0.030
0.641
0.725
0.026
0.639
0.693
0.028
0.630
0.692
0.027
0.568
0.571
0.029
0.639
0.641
0.040
0.667
0.712
0.031

0.624
0.691
0.045
0.619
0.699
0.043
0.618
0.665
0.045
0.620
0.675
0.047
0.633
0.711
0.038
0.633
0.666
0.045
0.654
0.718
0.035

0.648
0.742
0.033
0.645
0.771
0.028
0.637
0.719
0.037
0.646
0.717
0.029
0.636
0.854
0.039
0.659
0.672
0.043
0.664
0.747
0.034

0.649
0.752
0.027
0.645
0.780
0.023
0.644
0.740
0.025
0.638
0.749
0.021
0.636
0.837
0.021
0.646
0.695
0.035
0.670
0.745
0.028

0.653
0.733
0.034
0.650
0.755
0.030
0.647
0.715
0.033
0.645
0.701
0.030
0.639
0.841
0.025
0.658
0.684
0.042
0.676
0.762
0.033

0.588
0.722
0.024
0.577
0.744
0.020
0.585
0.697
0.022
0.582
0.709
0.019
0.582
0.817
0.021
0.588
0.662
0.032
0.592
0.722
0.026

0.661
0.746
0.029
0.652
0.763
0.025
0.650
0.723
0.029
0.642
0.734
0.024
0.630
0.848
0.029
0.651
0.695
0.037
0.680
0.736
0.030

0.606
0.681
0.027
0.599
0.704
0.024
0.609
0.674
0.026
0.586
0.702
0.022
0.600
0.700
0.021
0.578
0.655
0.036
0.620
0.689
0.029

0.605
0.695
0.025
0.593
0.720
0.022
0.598
0.669
0.025
0.587
0.688
0.020
0.611
0.820
0.018
0.599
0.657
0.034
0.616
0.710
0.028

evaluates

S-Measure
similarities
the
between salient objects in GT foreground maps and
predicted saliency maps:

structure

(2)

Sα = α × So + (1 − α) × Sr.
where So and Sr denotes the object-/region-based
structure similarities, respectively. α ∈ [0, 1] is set
as 0.5 so that equal weights are assigned to both the
object-level and region-level assessments [4].
E-Measure is a cognitive vision-inspired metric to
evaluate both the local and global similarities between
two binary maps. Speciﬁcally, it is deﬁned as:

Eφ =

1
W × H

W
(cid:88)

H
(cid:88)

i=1

j=1

φs(i, j),

(3)

represents

the enhanced alignment

where φs(i, j)
matrix [18].
Training Protocols. To provide a comprehensive
benchmark, we collect the released codes of 10 SOTA I-
SOD/V-SOD methods and one PI-SOD model, re-train
these models with the training set of ASOD60K and

the widely used I-SOD training set, DUTS-train [75]
(except for FANet [34], which is designed for ER images
only). The selected baselines (CPD[82], SCRN[83],
F3Net[39], MINet[55], LDF[80], CSF+Res2Net[29],
GateNet [99], RCRNet[87], COSNet[52] and PCSA
classical
[30]) meet
architectures, ii) recently published and open-sourced,
iii) achieve SOTA performance on existing I-SOD/V-
SOD benchmarks. Note that all the baselines are
trained with the recommended parameter settings.

following

criteria:

the

i)

5 Discussion

the

I-SOD models

From the benchmark results, we observe that,
gain comparable
generally,
performance to their V-SOD counterparts.
One
possible reason is that, since the annotations of our
ASOD60K are only based on key frames, the relatively
information may prevent the
sparse spatiotemporal
V-SOD models from acquiring full performance.
In
contrast, the visual cues are easily learned by the

9

10

Y. Zhang

Fig. 8 E-Measure (E-M) curves of all baselines upon our ASOD60K.

Fig. 9 Attribute-based E-Measure (E-M) curves of all baselines upon our ASOD60K.

For

speciﬁc scenes

I-SOD model with such sparsely labeled data.
Overall Performance. From the evaluation in Tab. 4,
we observe that, in most cases, the I-SOD methods
(e.g., GateNet, and CSF) achieve better performance
than the V-SOD (e.g., COSNet, RCRNet, and PCSA)
and PI-SOD models.
(e.g.,
speaking), however, SCRN obtains a very competitive
performance to GateNet, while performs worse than
CSF. The E-Measure performances are shown in Fig. 8.
Attribute Performance. To provide deeper insights
for the challenging cases, we report the performances
of all 11 baselines on our seven attributes. A detailed
attributes-based E-Measure is shown in Fig. 9. As
shown in Tab. 5, the average Sα score among the
models for the diﬀerent attributes are: 0.631 (MO),
0.626 (OC), 0.626 (LS), 0.623 (MB), 0.621 (OV), 0.631
(GD) and 0.649 (CS). Out-of-view (OV) is the most
challenging attribute as the objects usually appear in
the corner of the ER images. Besides, the scores on all
attributes are less than 0.65, demonstrating the strong
challenge of our ASOD60K and leaving large room for
future improvement.
General Attributes. As shown in Table 3, every

collected video owns at least one general attributes
(i.e., multiple objects (MO), occlusions (OC), motion
blur (MB) and low space (LS)), indicating that our
ASOD60K contains the main challenges faced in so
many computer vision ﬁelds, such as object detection,
video object segmentation, etc. It is worth mentioning
that, as the 360° image captures a wide ﬁeld-of-view
(FoV) with the range of 360°×180°, salient objects far
from the panoramic camera may perform extremely
small sizes, making the LS a more challenging situation

Fig. 10
Eye ﬁxation distributions of all participants
watching PianoConvo sequence without (a)/with (c) audio.
Corresponding ﬁxations are overlaid in (b) and (d), respectively.

10

0.20.40.60.813977115153191229CPDSCRNF3NetMINetLDFCSFGateNetCOSNetRCRNetPCSAFANet0.2250.450.6750.913977115153191229CPDSCRNF3NetMINetLDFCSFGateNetCOSNetRCRNetPCSAFANet0.150.30.450.613977115153191229CPDSCRNF3NetMINetLDFCSFGateNetCOSNetRCRNetPCSAFANet255ThresholdThresholdThreshold(a)E-McurveonTest0(b)E-McurveonTest1(c)E-McurveonTest20002550.150.30.450.613977115153191229CPDSCRNF3NetMINetLDFCSFGateNetCOSNetRCRNetPCSAFANet0.20.40.60.813977115153191229CPDSCRNF3NetMINetLDFCSFGateNetCOSNetRCRNetPCSAFANet0.1750.350.5250.713977115153191229CPDSCRNF3NetMINetLDFCSFGateNetCOSNetRCRNetPCSAFANet0.2250.450.6750.913977115153191229CPDSCRNF3NetMINetLDFCSFGateNetCOSNetRCRNetPCSAFANet0.20.40.60.813977115153191229CPDSCRNF3NetMINetLDFCSFGateNetCOSNetRCRNetPCSAFANet0.20.40.60.813977115153191229CPDSCRNF3NetMINetLDFCSFGateNetCOSNetRCRNetPCSAFANet0.20.40.60.813977115153191229CPDSCRNF3NetMINetLDFCSFGateNetCOSNetRCRNetPCSAFANet0.20.40.60.813977115153191229CPDSCRNF3NetMINetLDFCSFGateNetCOSNetRCRNetPCSAFANetThresholdThresholdThreshold(a)E-McurveonMO(b)E-McurveonOC(c)E-McurveonLSThreshold(g)E-McurveonCSThresholdThresholdThreshold(d)E-McurveonMB(e)E-McurveonOV(f)E-McurveonGD0.150.30.450.613977115153191229CPDSCRNF3NetMINetLDFCSFGateNetCOSNetRCRNetPCSAFANet(a)(b)(c)(d)ASOD60K: An Audio-Induced Salient Object Detection Dataset for Panoramic Videos

11

Fig. 12 An illustration of typical geometrical attributes. (a)
Out-of-view (OV) in equirectangular (ER) image.
(b) OV in
spherical ﬁeld-of-view (FoV). (c) Geometrical distortion (GD) in
ER image. (d) GD in spherical FoV.

where visual cues are the only evidence. As shown in
Figure 13, most of the baselines tend to detect all visual
salient objects while ignore the audio-visual attention
shifts among the key frames. It is necessary for future
works to model both the visual and audio cue for a
better performance on our ASOD60K.
360° Geometrical Attributes. The 360° image
captures a scene covering omnidirectional (360°×180°)
spatial information, thus including more comprehensive
object structures when compared to 2D image, which
owns a FoV with limited range. For instance, as
shown in Figure 12, the out-of-view (OV) object in
equirectangular (ER) image has a well shape when re-
projected to a speciﬁc FoV on sphere. However, OV
objects in 2D images or videos permanently lose spatial
information due to the limited viewing range of the
normal cameras. Geometrical distortion (GD) is the
other important attribute of 360° images under ER
projection (Figure 12 (c)), which is largely alleviated
in spherical FoV (Figure 12 (d)). As there is no perfect
2D (planar) representation for 360° images, a trade-
oﬀ between geometrical distorted extent and spatial
information retention will always exist. Future methods
may take advantage of multiple projection methods for
improved performance when conducting PV-SOD.
Small Objects. We deﬁne small objects LS (Tab. 2)
as those that occupy an area smaller than 0.5% of the
whole image. LS, as one of the well-known challenges
in the image segmentation task, is still not completely
solved. As stated in Sec. 3.4, due to the wide FoV range
of 360°×180°, the smallest object in our ASOD60K only
occupies 0.03% of the ER image, making it more
challenging. As for the attribute-based performance,
we observe that models under this situation achieve
a comparable performance to the others. However,

Fig. 11 Visual comparison of visual and audio-visual attention.
The objects with high saliency values (≥ 50%) are marked with
white/red bounding boxes. Zoom in for details.

within PV-SOD.
Eye Fixations With/Without Audio. Fig. 10
shows an example of 20 participants watching videos
without and with audio in longitude, respectively. We
ﬁnd that the eye ﬁxation recordings with audio are
highly consistent across subjects, while the recordings
without audio are not. Fig. 11 highlights this ﬁnding by
vividly showing a signiﬁcant divergence between human
attentions (converging/scattering) with and without
the guidance of audio information, respectively. Both
the quantitative (Fig. 10) and qualitative (Fig. 11)
experimental results indicate that human attention
highly depends on a co-guidance of audio-visual
information.
Audio-Induced Attributes. Creating realistic VR
experiences requires 360° videos to be captured with
their surrounding visual and audio stimuli.
The
audio cue plays a signiﬁcant role in informing the
viewers about the location of sounding salient objects
in the 360° environment [73], providing an immersive
multimedia experience. As shown in Figure 13, the
ground truths (GTs) of our videos (especially for
those attached with attribute-competing sounds) may
be largely inﬂuenced by the audio cues (e.g., silent
visual salient objects are sometimes regarded as non-
salient objects in our case, for they are unable to gain
a majority of the observers’ attention in an acoustic
environment), thus being very diﬀerent when compared
with the GTs of existing mute-modal V-SOD datasets

11

ER FramesV-AttentionAV-AttentionGT(a)(b)(c)(d)12

Y. Zhang

considering that the applied metrics may be biased
toward the true negative, the true positive score tends
to be worse than the existing shown performance.
Novel Metric. In this benchmark, we only introduce
widely used metrics for I-SOD. However, PV-SOD
involves context (e.g., audio, spatial and temporal)
relationship between salient/non-salient objects, which
is quite important for PV-SOD assessment. Thus,
designing a more suitable evaluation metric for PV-
SOD is an interesting and open issue.
Future Directions. Currently, we only focus on the
object-level task. However, the instance-level task is
more diﬃcult and may be suitable for many image-
editing applications. In addition, as described in [17],
studying non-salient objects will provide rich context
for reasoning the salient objects in a scene. Besides,
in this study, we only provide sparse annotations for
the proposed dataset. However, dense annotations
like those given in DAVIS [58] dataset can provide
more valuable information (e.g., sequence-to-sequence
modeling or audio-visual matching) for both traditional
I-SOD and PV-SOD models.
Finally, based on
the ﬁndings of signiﬁcant diﬀerences between visual
and audio-visual saliency attributes as illustrated in
“Audio-Induced Attributes” of Section Discussion, we
ﬁnd that an absence of audio cues may signiﬁcantly
limit current SOD models from acquiring their best
performance on ASOD60K. Future researches may
focus more on utilizing the audio cues for better object-
level saliency detection in panoramic scenes.

6 Conclusion

We have proposed ASOD60K, the ﬁrst large-scale
dataset for the PV-SOD task. Compared with the
traditional SOD task, PV-SOD is more challenging.
The hierarchical annotations enable ASOD60K to
easily be extended to diﬀerent-level tasks, such as
weakly supervised learning, multi-modality learning,
and head movement/ﬁxation prediction.
In addition,
we provide several empirical rules for creating high-
quality datasets. We have further investigated 11
cutting-edge methods at both the overall and attribute
levels.
The obtained ﬁndings indicate that PV-
SOD is far from being solved. We hope that our
studies will facilitate SOD research towards panoramic
videos and thus inspiring more novel ideas for AR/VR
applications.

Appendix

Per Video Performance & Visual Results. The
per-video quantitative results are shown in Tab. 6 and

12

Tab. 7. Please refer to Fig. 14. Fig. 15 and Fig. 16 for
visual results.

References

[1] A. Borji, M.-M. Cheng, Q. Hou, H. Jiang, and J. Li.
Salient object detection: A survey. CVMJ, 5(2):117–
150, 2019.

[2] A. Borji, M.-M. Cheng, H. Jiang, and J. Li.
Salient object detection: A benchmark. IEEE TIP,
24(12):5706–5722, 2015.

[3] F.-Y. Chao, C. Ozcinar, C. Wang, E. Zerman,
L. Zhang, W. Hamidouche, O. Deforges,
and
A. Smolic. Audio-visual perception of omnidirectional
video for virtual reality applications.
In IEEE
ICMEW, pages 1–6, 2020.

[4] M.-M. Chen and D.-P. Fan. Structure-measure: A
new way to evaluate foreground maps. IJCV, 2021.
[5] H.-T. Cheng, C.-H. Chao, J.-D. Dong, H.-K. Wen,
T.-L. Liu, and M. Sun. Cube padding for weakly-
supervised saliency prediction in 360 videos. In IEEE
CVPR, pages 1420–1429, 2018.

[6] M.-M. Cheng and D.-P. Fan. Structure-measure: A
new way to evaluate foreground maps. IJCV, 2021.
[7] S.-H. Chou, C. Sun, W.-Y. Chang, W.-T. Hsu,
M. Sun, and J. Fu. 360-indoor: Towards learning
real-world objects in 360deg indoor equirectangular
images. In IEEE WACV, pages 845–853, 2020.

[8] T. S. Cohen, M. Geiger, J. K¨ohler, and M. Welling.

Spherical cnns. ICLR, 2018.

Gauge

equivariant

[9] T. S. Cohen, M. Weiler, B. Kicanaoglu, and
convolutional

M. Welling.
networks and the icosahedral cnn. ICML, 2019.
[10] B. Coors, A. Paul Condurache, and A. Geiger.
Spherenet: Learning spherical representations for
detection and classiﬁcation in omnidirectional images.
In ECCV, pages 518–533, 2018.

[11] X. Corbillon, F. De Simone, and G. Simon. 360-degree
video head movement dataset. In ACM MMSys, pages
199–204, 2017.

[12] E. J. David, J. Guti´errez, A. Coutrot, M. P. Da Silva,
and P. L. Callet. A dataset of head and eye movements
for 360 videos. In ACM MMSys, pages 432–437, 2018.
[13] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and
L. Fei-Fei. Imagenet: A large-scale hierarchical image
database. In IEEE CVPR, pages 248–255, 2009.
[14] M. Eder, M. Shvets, J. Lim, and J.-M. Frahm.
Tangent images for mitigating spherical distortion. In
IEEE CVPR, pages 12426–12434, 2020.

[15] C. Esteves, C. Allen-Blanchette, A. Makadia,
Learning so (3) equivariant
and K. Daniilidis.
representations with spherical cnns. In ECCV, pages
52–68, 2018.

[16] C.-L. Fan, W.-C. Lo, Y.-T. Pai, and C.-H. Hsu.
A survey on 360 video streaming: Acquisition,
transmission, and display. ACM Computing Surveys,
52(4):1–36, 2019.

[17] D.-P. Fan, M.-M. Cheng, J.-J. Liu, S.-H. Gao, Q. Hou,
and A. Borji. Salient objects in clutter: Bringing

ASOD60K: An Audio-Induced Salient Object Detection Dataset for Panoramic Videos

13

Fig. 13 An illustration of the unique audio-visual attribute, competing sounds. Img = image. GT = ground truth.

salient object detection to the foreground. In ECCV,
pages 186–202, 2018.

[18] D.-P. Fan, C. Gong, Y. Cao, B. Ren, M.-M. Cheng,
and A. Borji. Enhanced-alignment measure for binary
foreground map evaluation. In IJCAI, pages 698–704,
2018.

Cognitive vision inspired object segmentation metric
and loss function. SCIENTIA SINICA Informationis,
6, 2021.

[20] D.-P. Fan, T. Li, Z. Lin, G.-P. Ji, D. Zhang, M.-M.
Cheng, H. Fu, and J. Shen. Re-thinking co-salient
object detection. IEEE TPAMI, 2021.

[19] D.-P. Fan, G.-P. Ji, X. Qin, and M.-M. Cheng.

[21] D.-P. Fan, Z. Lin, G.-P. Ji, D. Zhang, H. Fu, and M.-

13

StudioMelodramaGTCPDSCRNF3NetMINetLDFCSFGateNetCOSNetRCRNetPCSAFANetImg14

Y. Zhang

Tab. 6 Sequence performance comparison of 7/3/1 SOTA I-SOD/V-SOD/PI-SOD methods. Sp. = Speaking. Mu. = Music.

Super-class/Sequence

Metrics

CPD [82] SCRN [83] F3Net [39] MINet [55] LDF [80] CSF [29] GateNet [99] COSNet [52] RCRNet [87] PCSA [30] FANet [34]

I-SOD

V-SOD

PI-SOD

Sp./Debate

Sp./BadmintonConvo

Sp./Director

Sp./ChineseAd

Sp./Exhibition

Sp./PianoConvo

Sp./FilmingSite

Sp./Brothers

Sp./Rap

Sp./Spanish

Sp./Questions

Sp./PianoMono

Sp./Snowﬁeld

Sp./Melodrama

Sp./Gymnasium

Mu./Studio

Sα ↑
max Eφ ↑
mean Eφ ↑
M ↓
Sα ↑
max Eφ ↑
mean Eφ ↑
M ↓
Sα ↑
max Eφ ↑
mean Eφ ↑
M ↓
Sα ↑
max Eφ ↑
mean Eφ ↑
M ↓
Sα ↑
max Eφ ↑
mean Eφ ↑
M ↓
Sα ↑
max Eφ ↑
mean Eφ ↑
M ↓
Sα ↑
max Eφ ↑
mean Eφ ↑
M ↓
Sα ↑
max Eφ ↑
mean Eφ ↑
M ↓
Sα ↑
max Eφ ↑
mean Eφ ↑
M ↓
Sα ↑
max Eφ ↑
mean Eφ ↑
M ↓
Sα ↑
max Eφ ↑
mean Eφ ↑
M ↓
Sα ↑
max Eφ ↑
mean Eφ ↑
M ↓
Sα ↑
max Eφ ↑
mean Eφ ↑
M ↓
Sα ↑
max Eφ ↑
mean Eφ ↑
M ↓
Sα ↑
max Eφ ↑
mean Eφ ↑
M ↓
Sα ↑
max Eφ ↑
mean Eφ ↑
M ↓

0.547
0.764
0.600
0.014
0.712
0.867
0.749
0.027
0.679
0.852
0,735
0.031
0.601
0.895
0.483
0.009
0.487
0.614
0.486
0.013
0.577
0.871
0.774
0.037
0.578
0.727
0.562
0.013
0.673
0.778
0.688
0.018
0.498
0.830
0.530
0.006
0.606
0.838
0.651
0.038
0.505
0.925
0.763
0.009
0.598
0.702
0.682
0.057
0.729
0.739
0.677
0.032
0.609
0.782
0.699
0.108
0.551
0.806
0.584
0.007
0.741
0.878
0.745
0.008

0.620
0.855
0.752
0.016
0.669
0.822
0.663
0.034
0.753
0.880
0.729
0.038
0.645
0.883
0.524
0.009
0.469
0.658
0.365
0.061
0.652
0.851
0.673
0.035
0.633
0.762
0.627
0.023
0.686
0.806
0.677
0.024
0.477
0.816
0.387
0.087
0.765
0.870
0.807
0.030
0.640
0.921
0.609
0.011
0.555
0.855
0.736
0.054
0.811
0.816
0.778
0.029
0.685
0.835
0.744
0.084
0.514
0.683
0.545
0.013
0.770
0.889
0.731
0.009

0.605
0.854
0.818
0.014
0.617
0.611
0.555
0.032
0.701
0.891
0.852
0.032
0.551
0.908
0.527
0.042
0.480
0.605
0.460
0.009
0.579
0.847
0.745
0.035
0.603
0.681
0.626
0.023
0.638
0.747
0.713
0.023
0.521
0.831
0.548
0.021
0.746
0.851
0.835
0.032
0.740
0.901
0.870
0.006
0.573
0.766
0.688
0.044
0.778
0.741
0.720
0.031
0.655
0.837
0.732
0.068
0.492
0.830
0.461
0.021
0.753
0.898
0.832
0.010

0.553
0.800
0.592
0.012
0.712
0.850
0.815
0.034
0.677
0.832
0.773
0.037
0.477
0.695
0.391
0.069
0.469
0.597
0.350
0.042
0.607
0.875
0.833
0.038
0.610
0.766
0.636
0.030
0.655
0.772
0.705
0.024
0.343
0.471
0.260
0.371
0.679
0.819
0.727
0.035
0.563
0.926
0.576
0.007
0.572
0.796
0.739
0.056
0.800
0.784
0.764
0.028
0.673
0.811
0.773
0.083
0.501
0.700
0.593
0.011
0.788
0.904
0.826
0.006

0.566
0.842
0.829
0.012
0.613
0.814
0.659
0.068
0.756
0.902
0.849
0.029
0.631
0.906
0.576
0.027
0.428
0.689
0.270
0.139
0.639
0.880
0.847
0.033
0.637
0.799
0.707
0.017
0.652
0.746
0.706
0.025
0.507
0.814
0.484
0.025
0.793
0.873
0.865
0.025
0.605
0.920
0.855
0.010
0.629
0.861
0.746
0.054
0.819
0.819
0.792
0.026
0.667
0.835
0.784
0.079
0.501
0.813
0.469
0.020
0.758
0.892
0.847
0.009

0.576
0.853
0.802
0.013
0.647
0.826
0.662
0.033
0.772
0.900
0.810
0.028
0.630
0.908
0.635
0.011
0.492
0.770
0.508
0.011
0.636
0.858
0.807
0.036
0.645
0.766
0.654
0.014
0.697
0.816
0.715
0.017
0.525
0.824
0.678
0.012
0.713
0.854
0.797
0.036
0.691
0.915
0.700
0.010
0.522
0.842
0.758
0.048
0.779
0.763
0.716
0.029
0.664
0.841
0.717
0.095
0.507
0.686
0.512
0.016
0.739
0.899
0.756
0.009

0.628
0.849
0.768
0.015
0.652
0.846
0.737
0.046
0.726
0.894
0.681
0.034
0.605
0.913
0.544
0.012
0.486
0.735
0.459
0.013
0.586
0.885
0.693
0.036
0.636
0.787
0.613
0.020
0.685
0.792
0.629
0.022
0.463
0.761
0.400
0.095
0.701
0.865
0.822
0.040
0.671
0.922
0.574
0.009
0.637
0.755
0.696
0.060
0.823
0.836
0.788
0.027
0.617
0.816
0.710
0.100
0.537
0.754
0.487
0.020
0.724
0.891
0.601
0.010

0.514
0.844
0.410
0.009
0.613
0.845
0.603
0.032
0.716
0.899
0.744
0.029
0.553
0.910
0.641
0.015
0.487
0.811
0.514
0.008
0.603
0.888
0.706
0.035
0.578
0.708
0.540
0.012
0.662
0.784
0.661
0.015
0.482
0.828
0.513
0.007
0.724
0.877
0.784
0.032
0.576
0.935
0.569
0.009
0.506
0.859
0.500
0.039
0.601
0.864
0.485
0.033
0.467
0.788
0.296
0.076
0.520
0.863
0.584
0.007
0.637
0.904
0.629
0.008

0.559
0.843
0.809
0.013
0.668
0.804
0.773
0.039
0.755
0.883
0.774
0.037
0.542
0.899
0.632
0.028
0.473
0.576
0.349
0.040
0.718
0.880
0.803
0.028
0.640
0.738
0.628
0.013
0.664
0.813
0.728
0.019
0.506
0.858
0.590
0.020
0.700
0.862
0.800
0.037
0.676
0.907
0.667
0.014
0.611
0.831
0.736
0.047
0.794
0.779
0.753
0.030
0.608
0.816
0.730
0.100
0.520
0.760
0.518
0.017
0.778
0.893
0.800
0.009

0.571
0.836
0.605
0.012
0.550
0.743
0.434
0.033
0.731
0.918
0.718
0.031
0.569
0.910
0.470
0.010
0.510
0.773
0.512
0.014
0.509
0.882
0.420
0.033
0.633
0.805
0.652
0.017
0.666
0.820
0.650
0.016
0.495
0.832
0.566
0.009
0.543
0.839
0.486
0.042
0.595
0.909
0.540
0.012
0.502
0.796
0.397
0.037
0.580
0.812
0.514
0.035
0.604
0.831
0.521
0.079
0.503
0.798
0.468
0.027
0.756
0.901
0.729
0.008

0.557
0.755
0.702
0.015
0.635
0.830
0.704
0.030
0.672
0.844
0.768
0.030
0.595
0.850
0.502
0.007
0.475
0.560
0.329
0.024
0.632
0.863
0.804
0.033
0.522
0.793
0.727
0.016
0.623
0.729
0.681
0.016
0.532
0.818
0.733
0.009
0.602
0.728
0.503
0.035
0.549
0.757
0.703
0.013
0.502
0.715
0.633
0.043
0.578
0.775
0.618
0.040
0.568
0.794
0.770
0.098
0.505
0.752
0.642
0.010
0.760
0.895
0.859
0.007

M. Cheng. Taking a deeper look at co-salient object
detection. In IEEE CVPR, pages 2919–2929, 2020.

[22] D.-P. Fan, Z. Lin, Z. Zhang, M. Zhu, and M.-M.
Cheng. Rethinking rgb-d salient object detection:
Models, data sets, and large-scale benchmarks. IEEE
TNNLS, 2021.

[23] D.-P. Fan, W. Wang, M.-M. Cheng, and J. Shen.
Shifting more attention to video salient object
detection. In IEEE CVPR, pages 8554–8564, 2019.

[24] D.-P. Fan, J. Zhang, G. Xu, M.-M. Cheng, and

L. Shao. Salient objects in clutter. arXiv preprint
arXiv:2105.03053, 2021.

[25] Q. Fan, D.-P. Fan, H. Fu, C.-K. Tang, L. Shao, and
Y.-W. Tai. Group collaborative learning for co-salient
object detection. In IEEE CVPR, 2021.

[26] M. Feng, H. Lu, and E. Ding. Attentive feedback
network for boundary-aware salient object detection.
In IEEE CVPR, 2019.

[27] K. Fu, D.-P. Fan, G.-P. Ji, and Q. Zhao. Jl-dcf: Joint
learning and densely-cooperative fusion framework for

14

ASOD60K: An Audio-Induced Salient Object Detection Dataset for Panoramic Videos

15

Tab. 7 Sequence performance comparison of 7/3/1 SOTA I-SOD/V-SOD/PI-SOD methods. Mu. = Music. Mi. = Miscellanea.

Super-class/Sequence

Metrics

CPD [82] SCRN [83] F3Net [39] MINet [55] LDF [80] CSF [29] GateNet [99] COSNet [52] RCRNet [87] PCSA [30] FANet [34]

I-SOD

V-SOD

PI-SOD

Mu./Church

Mu./Duet

Mu./Blues

Mu./Violins

Mu./SingingDancing

Mi./Dog

Mi./RacingCar

Mi./Train

Mi./Football

Mi./ParkingLot

Mi./Skiing

Sα ↑
max Eφ ↑
mean Eφ ↑
M ↓
Sα ↑
max Eφ ↑
mean Eφ ↑
M ↓
Sα ↑
max Eφ ↑
mean Eφ ↑
M ↓
Sα ↑
max Eφ ↑
mean Eφ ↑
M ↓
Sα ↑
max Eφ ↑
mean Eφ ↑
M ↓
Sα ↑
max Eφ ↑
mean Eφ ↑
M ↓
Sα ↑
max Eφ ↑
mean Eφ ↑
M ↓
Sα ↑
max Eφ ↑
mean Eφ ↑
M ↓
Sα ↑
max Eφ ↑
mean Eφ ↑
M ↓
Sα ↑
max Eφ ↑
mean Eφ ↑
M ↓
Sα ↑
max Eφ ↑
mean Eφ ↑
M ↓

0.527
0.868
0.451
0.007
0.662
0.879
0.810
0.041
0.580
0.879
0.598
0.016
0.589
0.852
0.649
0.017
0.506
0.804
0.500
0.026
0.497
0.548
0.460
0.013
0.770
0.428
0.315
0.089
0.604
0.618
0.581
0.024
0.653
0.833
0.634
0.004
0.635
0.666
0.641
0.028
0.697
0.784
0.705
0.015

0.589
0.917
0.575
0.012
0.704
0.891
0.705
0.058
0.742
0.889
0.688
0.015
0.668
0.877
0.655
0.020
0.601
0.820
0.587
0.034
0.516
0.685
0.457
0.015
0.769
0.438
0.338
0.115
0.616
0.700
0.553
0.030
0.696
0.856
0.676
0.004
0.627
0.645
0.551
0.041
0.728
0.764
0.645
0.024

0.621
0.933
0.731
0.012
0.698
0.892
0.792
0.044
0.776
0.890
0.830
0.013
0.537
0.578
0.477
0.015
0.582
0.815
0.758
0.037
0.571
0.535
0.493
0.014
0.763
0.365
0.283
0.087
0.614
0.531
0.486
0.020
0.618
0.790
0.755
0.004
0.624
0.649
0.600
0.048
0.689
0.782
0.669
0.027

0.566
0.747
0.576
0.021
0.653
0.876
0.821
0.039
0.722
0.802
0.698
0.027
0.692
0.861
0.775
0.016
0.560
0.820
0.565
0.025
0.560
0.551
0.511
0.007
0.770
0.439
0.349
0.109
0.607
0.676
0.493
0.041
0.656
0.830
0.633
0.003
0.564
0.614
0.597
0.059
0.727
0.730
0.661
0.025

0.518
0.932
0.715
0.018
0.751
0.898
0.808
0.033
0.771
0.871
0.789
0.015
0.661
0.883
0.722
0.022
0.561
0.673
0.618
0.042
0.569
0.593
0.424
0.020
0.772
0.459
0.332
0.102
0.629
0.589
0.554
0.012
0.668
0.835
0.770
0.004
0.640
0.650
0.625
0.041
0.632
0.829
0.517
0.044

0.624
0.903
0.601
0.011
0.648
0.903
0.693
0.033
0.734
0.844
0.766
0.015
0.631
0.845
0.724
0.019
0.594
0.782
0.589
0.026
0.557
0.572
0.532
0.004
0.771
0.458
0.287
0.085
0.594
0.665
0.558
0.013
0.658
0.846
0.721
0.003
0.562
0.646
0.602
0.035
0.757
0.781
0.675
0.015

0.651
0.950
0.657
0.008
0.730
0.883
0.735
0.036
0.740
0.893
0.640
0.015
0.656
0.872
0.569
0.017
0.568
0.813
0.547
0.026
0.562
0.589
0.515
0.009
0.791
0.448
0.370
0.107
0.663
0.780
0.634
0.022
0.676
0.820
0.663
0.004
0.625
0.663
0.610
0.045
0.695
0.761
0.605
0.030

0.562
0.887
0.487
0.006
0.553
0.901
0.542
0.036
0.595
0.884
0.473
0.015
0.578
0.851
0.597
0.015
0.521
0.791
0.452
0.023
0.523
0.671
0.494
0.005
0.760
0.449
0.276
0.083
0.501
0.556
0.351
0.016
0.648
0.811
0.649
0.003
0.548
0.659
0.482
0.027
0.624
0.814
0.573
0.014

0.676
0.900
0.635
0.008
0.731
0.889
0.776
0.033
0.765
0.894
0.834
0.013
0.669
0.868
0.724
0.021
0.569
0.810
0.637
0.030
0.562
0.612
0.548
0.004
0.772
0.453
0.293
0.087
0.524
0.671
0.462
0.016
0.710
0.866
0.732
0.002
0.624
0.661
0.612
0.038
0.745
0.806
0.716
0.016

0.623
0.942
0.577
0.018
0.538
0.873
0.509
0.036
0.743
0.904
0.715
0.017
0.671
0.856
0.625
0.020
0.558
0.812
0.608
0.034
0.539
0.693
0.544
0.005
0.755
0.426
0.261
0.085
0.515
0.526
0.416
0.028
0.635
0.810
0.630
0.003
0.501
0.622
0.501
0.029
0.641
0.781
0.613
0.016

0.679
0.866
0.774
0.007
0.643
0.876
0.765
0.031
0.600
0.852
0.612
0.014
0.604
0.790
0.749
0.017
0.557
0.759
0.705
0.033
0.520
0.345
0.325
0.003
0.762
0.285
0.260
0.081
0.489
0.432
0.386
0.016
0.556
0.742
0.477
0.002
0.627
0.665
0.593
0.026
0.590
0.744
0.500
0.012

rgb-d salient object detection. In IEEE CVPR, pages
3052–3062, 2020.

[28] K. Fu, D.-P. Fan, G.-P. Ji, Q. Zhao, J. Shen, and
C. Zhu. Siamese network for rgb-d salient object
detection and beyond. IEEE TPAMI, 2021.

[29] S.-H. Gao, Y.-Q. Tan, M.-M. Cheng, C. Lu, Y. Chen,
and S. Yan. Highly eﬃcient salient object detection
with 100k parameters. In ECCV, 2020.

[30] Y. Gu, L. Wang, Z. Wang, Y. Liu, M.-M. Cheng, and
S.-P. Lu. Pyramid constrained self-attention network
for fast video salient object detection. In AAAI, 2020.
[31] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual
learning for image recognition. In IEEE CVPR, pages
770–778, 2016.

[32] H.-N. Hu, Y.-C. Lin, M.-Y. Liu, H.-T. Cheng, Y.-J.
Chang, and M. Sun. Deep 360 pilot: Learning a deep
agent for piloting through 360 sports videos. In IEEE
CVPR, pages 1396–1405, 2017.

[33] H.-N. Hu, Y.-C. Lin, M.-Y. Liu, H.-T. Cheng, Y.-J.
Chang, and M. Sun. Deep 360 pilot: Learning a deep

agent for piloting through 360 sports videos. In IEEE
CVPR, pages 1396–1405, 2017.

[34] M. Huang, Z. Liu, G. Li, X. Zhou, and O. Le Meur.
Features adaptation network for 360°
Fanet:
omnidirectional salient object detection. IEEE SPL,
27:1819–1823, 2020.

[35] S. Jain, P. Yarlagadda, R. Subramanian, and
V. Gandhi. Avinet: Diving deep into audio-visual
saliency prediction. arXiv preprint arXiv:2012.06170,
2020.

[36] G.-P. Ji, K. Fu, Z. Wu, D.-P. Fan, J. Shen, and
Full-duplex strategy for video object

L. Shao.
segmentation. In IEEE ICCV, 2021.

[37] C. Jiang, J. Huang, K. Kashinath, P. Marcus,
M. Niessner, et al. Spherical cnns on unstructured
grids. ICLR, 2019.

[38] Y. Jiang, T. Zhou, G.-P. Ji, K. Fu, Q. Zhao, and D.-P.
Fan. Light ﬁeld salient object detection: A review and
benchmark. arXiv preprint arXiv:2010.04968, 2020.

[39] Q. H. Jun Wei, Shuhui Wang. F3net: Fusion, feedback

15

16

Y. Zhang

Fig. 14 Visual results of all baselines on the ASOD60K -test0 (Miscellanea). Img = image. GT = ground truth.

and focus for salient object detection. In AAAI, 2020.
[40] H. Kim, Y. Kim, J.-Y. Sim, and C.-S. Kim.
Spatiotemporal saliency detection for video sequences
IEEE TIP,
based on random walk with restart.
24(8):2552–2564, 2015.

[41] F. Li, T. Kim, A. Humayun, D. Tsai, and J. M. Rehg.
Video segmentation by tracking many ﬁgure-ground

segments. In IEEE ICCV, pages 2192–2199, 2013.
[42] G. Li, Y. Xie, L. Lin, and Y. Yu. Instance-level salient
In IEEE CVPR, pages 2386–

object segmentation.
2395, 2017.

[43] G. Li, Y. Xie, T. Wei, K. Wang, and L. Lin. Flow
guided recurrent neural encoder for video salient
object detection. In IEEE CVPR, pages 3243–3252,

16

FootballSkiing GTCPDSCRNF3NetMINetLDFCSFGateNetCOSNetRCRNetPCSAFANetImgASOD60K: An Audio-Induced Salient Object Detection Dataset for Panoramic Videos

17

Fig. 15 Visual results of all baselines on the ASOD60K -test1 (Music). Img = image. GT = ground truth.

2018.

[44] G. Li and Y. Yu. Visual saliency based on multiscale
In IEEE CVPR, pages 5455–5463,

deep features.
2015.

[45] H. Li, G. Chen, G. Li, and Y. Yizhou. Motion guided
attention for video salient object detection. In IEEE
ICCV, 2019.

[46] J. Li, J. Su, C. Xia, and Y. Tian. Distortion-adaptive
salient object detection in 360◦ omnidirectional
images. IEEE JSTSP, 14(1):38–48, 2020.

[47] J. Li, C. Xia, and X. Chen. A benchmark dataset and
saliency-guided stacked autoencoders for video-based
salient object detection.
IEEE TIP, 27(1):349–364,
2018.

17

SingingDancingDuet GTCPDSCRNF3NetMINetLDFCSFGateNetCOSNetRCRNetPCSAFANetImg18

Y. Zhang

Fig. 16 Visual results of all baselines on the ASOD60K -test2 (Speaking). Img = image. GT = ground truth.

[48] Y. Li, X. Hou, C. Koch, J. M. Rehg, and A. L. Yuille.
The secrets of salient object segmentation. In IEEE
CVPR, pages 280–287, 2014.

[49] J.-J. Liu, Q. Hou, M.-M. Cheng, J. Feng, and J. Jiang.
A simple pooling-based design for real-time salient
object detection. In IEEE CVPR, 2019.

selective mutual attention and contrast for rgb-d
saliency detection. In IEEE CVPR, 2020.

[51] Z. Liu, J. Li, L. Ye, G. Sun, and L. Shen. Saliency
detection for unconstrained videos using superpixel-
level graph and spatiotemporal propagation.
IEEE
TCSVT, 27(12):2527–2542, 2017.

[50] N. Liu, N. Zhang, L. Shao, and J. Han. Learning

[52] X. Lu, W. Wang, C. Ma, J. Shen, L. Shao, and

18

Spanish Brothers GTCPDSCRNF3NetMINetLDFCSFGateNetCOSNetRCRNetPCSAFANetImgASOD60K: An Audio-Induced Salient Object Detection Dataset for Panoramic Videos

19

Fig. 17 Sample key frames from ASOD60K, with ﬁxations and instance-level ground truth overlaid.

F. Porikli.
See more, know more: Unsupervised
video object segmentation with co-attention siamese
networks. In IEEE CVPR, 2019.

[53] G. Ma, S. Li, C. Chen, A. Hao, and H. Qin. Stage-wise
salient object detection in 360° omnidirectional image

via object-level semantical saliency ranking.
TVCG, 26(12):3535–3545, 2020.

IEEE

[54] P. Ochs, J. Malik, and T. Brox. Segmentation of
IEEE

moving objects by long term video analysis.
TPAMI, 36(6):1187–1200, 2014.

19

20

Y. Zhang

[55] Y. Pang, X. Zhao, L. Zhang, and H. Lu. Multi-scale
In
interactive network for salient object detection.
IEEE CVPR, pages 9413–9422, 2020.

[56] T. L. Pedro Morgado, Nuno Vasconcelos and
O. Wang. Self-supervised generation of spatial audio
for 360◦ video. In NeurIPS, 2018.

[57] F. Perazzi, P. Kr¨ahenb¨uhl, Y. Pritch, and A. Hornung.
Saliency ﬁlters: Contrast based ﬁltering for salient
region detection.
In IEEE CVPR, pages 733–740,
2012.

[58] F. Perazzi,

J. Pont-Tuset,

B. McWilliams,
L. Van Gool, M. Gross, and A. Sorkine-Hornung. A
benchmark dataset and evaluation methodology for
video object segmentation.
In IEEE CVPR, pages
724–732, 2016.

[59] Y. Piao, W. Ji, J. Li, M. Zhang, and H. Lu. Depth-
induced multi-scale recurrent attention network for
saliency detection. In IEEE ICCV, pages 7254–7263,
2019.

[60] Y. Piao, Z. Rong, S. Xu, M. Zhang, and H. Lu.
Dut-lfsaliency: Versatile dataset and light ﬁeld-to-rgb
saliency detection. arXiv preprint arXiv:2012.15124,
2020.

[61] X. Qin, D.-P. Fan, C. Huang, C. Diagne, Z. Zhang,
A. C. Sant’Anna, A. Su`arez, M. Jagersand, and
Boundary-aware segmentation network
L. Shao.
for mobile and web applications.
arXiv preprint
arXiv:2101.04704, 2021.

[62] X. Qin, Z. Zhang, C. Huang, C. Gao, M. Dehghan,
and M. Jagersand. Basnet: Boundary-aware salient
object detection. In IEEE CVPR, 2019.

[63] Y. Rai, J. Guti´errez, and P. Le Callet. A dataset of
head and eye movements for 360 degree images. In
ACM MMSys, pages 205–210, 2017.

[64] K. Simonyan and A. Zisserman.

convolutional
recognition. In ICLR, 2015.
[65] Y.-C. Su and K. Grauman.

networks

for

Very deep
image

large-scale

Learning spherical
In

convolution for fast features from 360 imagery.
NeurIPS, pages 529–539, 2017.

[66] Y.-C. Su and K. Grauman. Kernel transformer
networks for compact spherical convolution. In IEEE
CVPR, pages 9442–9451, 2019.

[67] Y.-C. Su, D. Jayaraman, and K. Grauman. Pano2vid:
Automatic cinematography for watching 360° videos.
In ACCV, pages 154–171, 2016.

[68] J. Tang, D. Fan, X. Wang, Z. Tu, and C. Li.
Rgbt salient object detection: benchmark and a
novel cooperative ranking approach. IEEE TCSVT,
30(12):4421–4433, 2019.

[69] H. R. Tavakoli, A. Borji, E. Rahtu, and J. Kannala.
Dave: A deep audio-visual embedding for dynamic
saliency prediction. arXiv preprint arXiv:1905.10693,
2019.

[70] A. Tsiami, P. Koutras, and P. Maragos.

Spatio-temporal audiovisual saliency network.
IEEE CVPR, pages 4766–4776, 2020.

[71] Z. Tu, T. Xia, C. Li, X. Wang, Y. Ma, and J. Tang.

20

Stavis:
In

Rgb-t image saliency detection via collaborative graph
learning. IEEE TMM, 22(1):160–173, 2019.

[72] E. Van der Burg, C. N. Olivers, A. W. Bronkhorst,
and J. Theeuwes.
Audiovisual events capture
attention: Evidence from temporal order judgments.
JOV, 8(5):2–2, 2008.

[73] A. B. Vasudevan, D. Dai, and L. Van Gool. Semantic
object prediction and spatial sound super-resolution
with binaural sounds. In ECCV, pages 638–655, 2020.
[74] G. Wang, C. Chen, D.-P. Fan, A. Hao, and H. Qin.
From semantic categories to ﬁxations: A novel
weakly-supervised visual-auditory saliency detection
approach. In IEEE CVPR, pages 15119–15128, 2021.
[75] L. Wang, H. Lu, Y. Wang, M. Feng, D. Wang, B. Yin,
and X. Ruan. Learning to detect salient objects with
image-level supervision. In IEEE CVPR, pages 136–
145, 2017.

[76] W. Wang, Q. Lai, H. Fu, J. Shen, H. Ling, and
R. Yang. Salient object detection in the deep learning
era: An in-depth survey. IEEE TPAMI, 2021.

[77] W. Wang, J. Shen, X. Dong, and A. Borji. Salient
object detection driven by ﬁxation prediction.
In
IEEE CVPR, pages 1711–1720, 2018.

[78] W. Wang, J. Shen, F. Guo, M.-M. Cheng, and
A. Borji. Revisiting video saliency: A large-scale
benchmark and a new model. In IEEE CVPR, pages
4894–4903, 2018.

[79] W. Wang, J. Shen, and L. Shao. Consistent video
saliency using local gradient ﬂow optimization and
global reﬁnement.
IEEE TIP, 24(11):4185–4196,
2015.

[80] J. Wei, S. Wang, Z. Wu, C. Su, Q. Huang, and
Q. Tian. Label decoupling framework for salient
object detection. In IEEE CVPR, pages 13025–13034,
2020.

[81] C. Wu, R. Zhang, Z. Wang, and L. Sun. A spherical
convolution approach for learning long term viewport
prediction in 360 immersive video.
In AAAI, pages
14003–14040, 2020.

[82] Z. Wu, L. Su, and Q. Huang. Cascaded partial decoder
for fast and accurate salient object detection. In IEEE
CVPR, pages 3907–3916, 2019.

[83] Z. Wu, L. Su, and Q. Huang. Stacked cross reﬁnement
network for edge-aware salient object detection.
In
IEEE ICCV, pages 7264–7273, 2019.

[84] M. Xu, C. Li, S. Zhang, and P. Le Callet. State-
of-the-art in 360 video/image processing: Perception,
assessment and compression. IEEE JSTSP, 14(1):5–
26, 2020.

[85] M. Xu, Y. Song, J. Wang, M. Qiao, L. Huo, and
Z. Wang. Predicting head movement in panoramic
video: A deep reinforcement learning approach. IEEE
TPAMI, 41(11):2693–2708, 2019.

[86] Y. Xu, Y. Dong, J. Wu, Z. Sun, Z. Shi, J. Yu, and
S. Gao. Gaze prediction in dynamic 360 immersive
videos. In IEEE CVPR, pages 5333–5342, 2018.
[87] P. Yan, G. Li, Y. Xie, Z. Li, C. Wang, T. Chen, and
L. Lin. Semi-supervised video salient object detection

ASOD60K: An Audio-Induced Salient Object Detection Dataset for Panoramic Videos

21

using pseudo-labels. In IEEE ICCV, pages 7284–7293,
2019.

[88] Q. Yan, L. Xu, J. Shi, and J. Jia. Hierarchical saliency

detection. In IEEE CVPR, pages 1155–1162, 2013.

[89] C. Yang, L. Zhang, H. Lu, X. Ruan, and M.-H. Yang.
Saliency detection via graph-based manifold ranking.
In IEEE CVPR, pages 3166–3173, 2013.

[90] W. Yang, Y. Qian, J.-K. Kamarainen, F. Cricri,
and L. Fan. Object detection in equirectangular
panorama. In IEEE ICPR, pages 2190–2195, 2018.

[91] Y. Zeng, P. Zhang, J. Zhang, Z. Lin, and H. Lu.
Towards high-resolution salient object detection. In
IEEE ICCV, pages 7234–7243, 2019.

[92] C. Zhang, S. Liwicki, W. Smith, and R. Cipolla.
on
In IEEE ICCV, pages 3533–

segmentation

semantic

Orientation-aware
icosahedron spheres.
3541, 2019.

[93] P. Zhang, W. Liu, Y. Zeng, Y. Lei, and H. Lu. Looking
for the detail and context devils: High-resolution
salient object detection. IEEE TIP, 2021.

[94] Q. Zhang, R. Cong, C. Li, M.-M. Cheng, Y. Fang,
X. Cao, Y. Zhao, and S. Kwong. Dense attention ﬂuid
network for salient object detection in optical remote
sensing images. IEEE TIP, 2020.

[95] Y. Zhang, L. Zhang, W. Hamidouche,

and
A ﬁxation-based 360◦ benchmark
O. Deforges.
dataset for salient object detection. In IEEE ICIP,
2020.

[96] Z. Zhang, Y. Xu, J. Yu, and S. Gao. Saliency detection
in 360 videos. In ECCV, pages 488–503, 2018.
[97] J.-X. Zhao, J.-J. Liu, D.-P. Fan, Y. Cao, J. Yang,
and M.-M. Cheng. Egnet: Edge guidance network for
salient object detection. In IEEE ICCV, pages 8779–
8788, 2019.

[98] P. Zhao, A. You, Y. Zhang, J. Liu, K. Bian, and
Y. Tong. Spherical criteria for fast and accurate 360°
object detection. In AAAI, pages 12959–12966, 2020.
[99] X. Zhao, Y. Pang, L. Zhang, H. Lu, and L. Zhang.
Suppress and balance: A simple gated network for
salient object detection. In ECCV, 2020.

[100] H. Zhou, X. Xie, J.-H. Lai, Z. Chen, and L. Yang.
Interactive two-stream decoder for accurate and fast
saliency detection. In IEEE CVPR, pages 9141–9150,
2020.

[101] T. Zhou, D.-P. Fan, M.-M. Cheng, J. Shen, and
L. Shao. Rgb-d salient object detection: A survey.
CVMJ, pages 1–33, 2021.

[102] M. Zhuge, D.-P. Fan, N. Liu, D. Zhang, D. Xu,
and L. Shao. Salient object detection via integrity
learning. arXiv preprint arXiv:2101.07663, 2021.
[103] C. Zuyao, X. Qianqian, C. Runmin, and H. Qingming.
Global context-aware progressive aggregation network
for salient object detection. In AAAI, 2020.

21

22

Y. Zhang

Yi Zhang received his bachelor degree
in 2016 and master degree in 2019
respectively from Southeast University
both in biomedical engineering. He
is pursuing his PhD degree at INSA

Rennes, France. His current research
interests include omnidirectional vision,
salient object detection, camouﬂaged

object detection and deep learning.

22

