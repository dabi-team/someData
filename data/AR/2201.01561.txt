Multi-layer VI-GNSS Global Positioning Framework with Numerical
Solution aided MAP Initialization

Bing Han1, Zhongyang Xiao1, Shuai Huang1 and Tao Zhang1,∗

2
2
0
2

n
a
J

5

]

O
R
.
s
c
[

1
v
1
6
5
1
0
.
1
0
2
2
:
v
i
X
r
a

Abstract— Motivated by the goal of achieving long-term drift-
free camera pose estimation in complex scenarios, we propose
inertial and
a global positioning framework fusing visual,
Global Navigation Satellite System (GNSS) measurements in
multiple layers. Different from previous loosely- and tightly-
coupled methods, the proposed multi-layer fusion allows us
to delicately correct the drift of visual odometry and keep
reliable positioning while GNSS degrades. In particular, local
motion estimation is conducted in the inner-layer, solving the
problem of scale drift and inaccurate bias estimation in visual
odometry by fusing the velocity of GNSS, pre-integration of
Inertial Measurement Unit (IMU) and camera measurement in
a tightly-coupled way. The global localization is achieved in the
outer-layer, where the local motion is further fused with GNSS
position and course in a long-term period in a loosely-coupled
way. Furthermore, a dedicated initialization method is proposed
to guarantee fast and accurate estimation for all state variables
and parameters. We give exhaustive tests of the proposed
framework on indoor and outdoor public datasets. The mean
localization error is reduced up to 63%, with a promotion of
69% in initialization accuracy compared with state-of-the-art
works. We have applied the algorithm to Augmented Reality
(AR) navigation, crowd sourcing high-precision map update
and other large-scale applications.

I. INTRODUCTION

Real-time estimation of 6-DOF camera pose has become a
fundamental requirement in many automated systems such as
robotics, autonomous vehicle navigation, and AR. Advanced
solutions [1], [2], [3] simultaneously estimated the camera
pose while constructing the environment map by detecting
and tracking visual features, which is named as simulta-
neous localization and mapping (SLAM). Among SLAM
approaches, monocular SLAM has the advantages of low
cost, small size and easy installation. However, it suffers
from inherent problems of scale uncertainty and scale drift.
Though stereo [4] and RGB-D [5] camera systems can
solve the problem of scale uncertainty, the cost of hardware
and computation complexities are signiﬁcant. Recently devel-
oped visual-inertial (VI) systems [2], [6] solve the problem at
a low cost. On one hand, IMU measurement helps recover
the metric scale. On the other hand, the IMU bias can be
corrected by vision information.

Due to the cumulative error in both visual and inertial
systems, however,
the above-mentioned VI systems can
guarantee positioning accuracy only in a short time period,
which limits its application in long-term and large-scale

1Bing Han, Zhongyang Xiao, Shuai Huang and Tao Zhang are
with Alibaba Group, Beijing, China, nikchyhahn@aliyun.com,
young.xzy@autonavi.com, { hanzhe.hs, zt107579}
@alibaba-inc.com
* corresponding author

Fig. 1. Global localization result of the proposed method

area. Loop closure can reduce the positioning drift, but
will increase the computational cost. Moreover, large-scale
outdoor applications can hardly beneﬁt from loop closure [7],
[8]. Another drawback in VI systems is that the estimated
pose is not aligned to the world frame, which is essential for
applications (e.g, the intelligent vehicle) using a priori global
map. Among other commonly used localization methods,
GNSS provides global positioning information in the world
frame with time-independent accuracy. It can help eliminate
the cumulative error and correct
the scale drift. Visual,
inertial and GNSS (VI-GNSS) integration forms a promising
framework for outdoor long-term positioning solution [9],
[10], [11], [12].

In this paper, we propose an innovative VI-GNSS global
positioning framework, which can realize rapid initialization
and online localization in the world frame. Even in challeng-
ing scenarios, it guarantees reliable positioning performance
with robustness to GNSS degradation and visual instability.
The main contributions of this paper are as follows:
1. We propose a novel Maximum a Posteriori (MAP)-
based VI-GNSS joint initialization framework. Compared
with other visual initialization algorithms, it has the ad-
vantages of shorter initialization time, more accurate scale
estimation, and online estimation of the extrinsic parameters
between the local and world frames.

2. We propose an innovative multi-layer fusion framework,
in which the translation and rotation estimation is dedicatedly
decoupled according to the observation properties from dif-
ferent sources to achieve optimized estimation in each layer.
3. Exhaustive experiments and applications are conducted
to show that, compared with the existing algorithms, our
proposed method can achieve higher accuracy up to 69%

 
 
 
 
 
 
in the initialization and 63% in the online localization in
large-scale challenging scenarios.

II. RELATED WORKS

In recent decades, there have been many efforts to realize
camera pose estimation by fusing visual, inertial sesnors and
GNSS, wheel encoder, etc . Among them, Visual SLAM is
the basis for fusing other sensors.

Visual SLAM The mainstream visual SLAM methods
include indirect methods [13], [4], [1] and direct methods
[14], [15]. Since the depth of visual feature is ambiguous, the
monocular visual SLAM can provide only up-to-scale poses.
Some solutions use a binocular or RGB-D camera [16] or
lidar [17] to determine the depth directly. Other methods that
introduce inertial sensor to restore the real scale [18], [19]
gain more interest due to its relatively low cost. However,
when the acceleration excitation is low (e.g., the vehicle
moves at a constant speed), the metric scale is unobservable
and easily diverged.

Visual SLAM can estimate the pose only relative to the
ﬁrst camera frame. However, many applications (e.g., the
intelligent vehicles, etc.) require the global pose in the
it can be aligned to a prior map.
world frame so that
Therefore, multi-source sensors should be fused to obtain
the global pose. According to the different frameworks where
multiple sensors are fused, the fusion algorithms are usually
divided into loosely-coupled positioning and tightly-coupled
positioning.

Loosely-coupled positioning v.s. Tightly-coupled posi-
tioning. In the framework of loosely-coupled fusion [20],
[21], [10] , parts of the original sensor data (e.g., visual and
inertial measurements) are fused at the ﬁrst stage and produce
the estimation as a new measurement. The pre-estimated data
is further fused with other sensors. The process of modeling
errors in the pre-esitmation may cause inaccuracy. Moreover,
the drift of the metric scale in visual SLAM cannot be
corrected for a long time, which may eventually cause a large
error in the ﬁnal positioning result. The limitations can be
overcome by directly modeling the noise of various sensors’
observations and fusing them in one single sliding window.
The improvement leads to the tightly-coupled positioning
that guarantees higher accuracy [22], [11]. However, the
tightly-coupled system is more nonlinear and has a higher
degree of freedom. Abnormal data from single sensor may
cause large drifts in the ﬁnal positioning.

The proposed framework in this paper is between loosely-
and tightly- coupled methods: we ﬁrstly fuse a portion of
data from visual, inertial and GNSS in the inner-layer in a
small time period using a tightly-coupled way. In addition
to accurately modeling the observation noise, we deal with
the visual-inertial scale drift by introducing GNSS velocity.
Then we fuse the GNSS position and course in a longer time
period in the outer-layer to further improve the global pose
accuracy. Also, outliers are eliminated in this layer, making
it more robust in scenarios with large GNSS drift and short-
term visual measurement degradation.

Parameter initialization methods In the mainstream VI
SLAM algorithms [2], [4], [23], initial state variables (e.g.,
camera pose, metric scale, etc. ) are obtained by solving a
numerical analytic solution, which are not accurate enough.
In the latest research Orb-slam3 [24], MAP method is used
to estimate the initial state variables in an optimization
framework, which is prone to achieve more accurate initial-
ization. However, Orb-slam3 does not ﬁnely set the initial
values in MAP optimization, and may result
in a local
optimum result. Our proposed algorithm roughly estimates
initial values using the analytical method and then performs
further optimization by MAP estimation. In addition, we
involve the initialization of local-global transformation to
guarantee accurate estimation of global poses.

III. INITIALIZATION

In the MAP based monocular VI-GNSS online optimiza-
tion system, a pure monocular camera can provide relative
pose information where the scale is normalized. IMU helps
recover the real metric scale, however the accuracy suffers
from the drift of bias. GNSS provides the position, velocity
and course information in the world frame(i.e., East-North-
Up (ENU) navigation coordinate system) with relatively low
accuracy. The initialization procedure should guarantee good
initial values for all state variables and parameters, e.g. the
metric scale, IMU bias, extrinsic parameters of the world
frame, etc.,
to avoid diverging or converging to a local
minimal.

In this section, we propose a numerical solution aided
MAP initialization algorithm where initial values are cal-
culated in a uniform framework. It can also be applied to
other VI GNSS and/or odometer systems.

The entire initialization algorithm includes 3 steps:
1) Estimate the initial pose with real metric scale using
sliding window visual odometry (VO) aided by GNSS
velocity.

2) Solve the extrinsic parameters of the initial camera

frame against the world frame.

3) Estimate and reﬁne all parameters using the VI-GNSS

MAP optimization.

A. Sliding window VO aided by GNSS velocity

In this step, we roughly estimate the initial pose with
real metric scale while constructing the initial 3D feature
point map simultaneously. In order to guarantee the real-
time calculation performance, we use a ﬁxed-length sliding
window to maintain the data from latest several frames. For
all the matched frames, when the number of tracking points
and the parallax are sufﬁcient and the GNSS signals are good
, we calculate the relative rotation R0 and translation t0
by the 7-point method [25]. Furthermore, we retrieve the
metric scale by comparing the relative translation of the
two matched frames against the GNSS velocity. The initial
3D visual point map is then constructed by triangulating all
matched feature points. Poses of other frames are estimated
by perspective-n-point (PnP) method [26]. Finally, we take

Fig. 2.

Initialization Procedure

a GNSS-aided visual bundle adjustment (BA) in Equation 1
to optimize poses of all frames in the sliding window.

X = [δs, pc0
c0
X ∗ = arg min
X
−1 ⊗
r(zci, χ) = qc0
cj
1
2

r(z∆p, X) =

, qc0
c1

, pc0
c1

, qc0
, qc0
c0
ck
{ρ((cid:107)r(zck , X)(cid:107)2) + (cid:107)r(z∆p, X)(cid:107)2}

, λ0, λ1...λm]

...pc0
ck

(cid:104)

qc0
ci

∗ pi/λl + pc0
ci

− pc0
cj

(cid:105)

− pj

(1)

∗ (vi + vj) ∗ ∆t − s ∗ (cid:107)pc0
cj

− pc0
ci

(cid:107)

is

, qc0

where

r(zck , X)

the visual

re-projection error.
r(z∆p, X) is the relative translation error of corresponding
frames. pc0
ci is the pose of frame i relative to initial frame.
ci
ρ(x) is Huber robust loss function which is used to reduce
the inﬂuence of outliers [27]. There are few outliers in the
translation error estimates, so ρ(x) is not applied to relative
translation error. λl is the inverse depth of feature point in the
ﬁrstly observed camera frame. pi is the position of feature in
the normalized plane. vi, vj are the velocities of GNSS at the
time of i-th and j-th frames, and ∆t the time interval between
the i-th and j-th frames. We estimate the metric scale using
the relative translation derived from GNSS velocities which
are considered more accurate than GNSS positions thanks to
the Doppler measurement.

B. Numerical solution for world frame extrinsic parameter

The poses acquired in the former step are relative to
the initial camera frame, however the global positions are
required to support outdoor large-scale applications. So it is
essential to estimate the relative rotation extrinsic parameters
between the initial camera frame and the world frame.

To estimate the 3-DOF rotation extrinsic parameters, the
system should render the roll, pitch, and yaw angles ob-
servable. Firstly, we align the GNSS positions in the world
frame with the camera positions in the ﬁrst frame to derive
the initial yaw angle.

Then, we estimate roll and pitch angles using the accel-
eration of IMU, where the motion acceleration is eliminated
by differentiating the GNSS velocity.

qw
c0

(r, p) ⊗ qc0
ck

⊗ qi
c

−1

∗ ai = (G + al)

(3)

where qw
c0

(r, p) are roll and pitch angles of extrinsic
parameters. ai is the acceleration measurement from IMU.
al is the motion acceleration in the world frame.

Finally, we combine qw
c0

(r, p, y)
as the initial rotation extrinsic parameters between the world
frame and the initial camera frame.

(y) to form qw
c0

(r, p), qw
c0

C. VI-GNSS MAP optimization

In the last two steps, we have roughly estimated the initial
metric scale by GNSS and VO fusion, however it can be
further optimized using the IMU measurements. Besides, the
rotation extrinsic parameters are only estimated by single or
a few frames and is not accurate enough. We use the roughly
estimated parameters as the ﬁrst guess and conduct a MAP
optimization using all information in the ﬁrst few frames to
obtain more accurate initial values.

(cid:9)

, v0, v1, v2...vn
(cid:110)
(cid:107)r(zp, X)(cid:107)2
Ωg
− qc0
ck

⊗ {s ∗ pc0
ck

2(cid:111)

, X)(cid:107)

+ (cid:107)r(zbk+1
−1

bk
⊗ (ti

⊗ qi
c

g − ti

c)} − pw
gk

X0 = (cid:8)bg, s, qw
c0
X ∗

0 = argminX

r(zp, X) = qw
c0
r(zbk+1
, X) =
bk

αbk+1
bk

− qc0
bk

−1 ⊗ (pw
∆t)

−1 ⊗ qw
c0
1
2 G∆t2 − vw
bk
−1 ⊗ qw
−1 ⊗ (vw
c0
bk+1
(cid:18)

⊗ γbk

bk+1

⊗

βbk+1
bk

− qc0
bk
−1 ⊗ qc0
bk

qc0
bk+1








− pw
bk

+

bk+1

+ G∆t)
(cid:19)2

− vw
bk
1
2 ∗ J γ

bg

1

δbg










(4)

−1

pw
gk

⊗ qb
c

− qc0
ck

(y) ⊗ {s ∗ pc0
ck

= qw
c0
where qw
c0
qb
c, tb
camera and IMU frame. tb
and GNSS antenna in the IMU body frame.

(y) is the yaw angle in the extrinsic parameters.
c are the rotation and translation parameters between
g is the translation between IMU

g − tb

⊗ (tb

c)}

(2)

where r(zp, X) is the translation residual and r(zbk+1
, X)
the pre-integration residual [2]. bg is the bias of gyroscope. δs
is the scale error. qw
c0 is the extrinsic parameter between the
initial camera frame and the world frame. vi is the velocity
of IMU relative to the world frame. We can solve this
MAP problem with roughly estimated parameters as initial

bk

values using Levenberge-Marquart algorithm to obtain more
accurate states. At each iteration, we substitute the latest
bg, s, qw

c0 in case of getting into local minimal.

r(zbj+1
bj

, χ) = [rq, rp, rv, rbω , rba]

IV. ONLINE POSE ESTIMATOR

rq = ∆qbk+1

bk

The online estimation process is shown in Figure 3. This
framework consists of two layers. 1) The inner-layer is the
sliding window BA optimization. And, 2) the outer-layer is
4-DOF pose graph optimization in a large scale.

A. Sliding window BA

In the sliding window BA, we mainly fuse visual inertial
measurements and GNSS velocity to get accurate poses,
which can simultaneously calibrate the extrinsic parameters
between camera and IMU body frames. The states to be
estimated in the sliding window are

χ = (cid:2)ξ0, ξ1, ξ2...ξm, T b
ξk = (cid:2)pw0
, qw0
bk
bk
c = (cid:2)pb
(cid:3)
T b
c, qb
c

, vw0
bk

c , ρ0, ρ1...ρn
(cid:3)

, bω, ba

(cid:3)

(5)

where ξk is the state of IMU at k-th frame. pw0
bk

, vw0
bk
are the pose and velocities of IMU at k-th frame. bω, ba are
the biases of gyroscope and accelerometer respectively. ρk
is the inverse depth of visual feature. m is the number of
frames in the sliding window and n the number of features
in the visual feature point map.

, qw0
bk

By optimizing the residuals of multiple sensors in the
sliding window , we can obtain accurate poses and camera-
IMU extrinsic parameters. A marginal method is required to
avoid the loss of information caused by the direct removal
of historical frames. The Mahalanobis norm is used to
normalize the observation residuals. The overall cost function
is

(cid:40)

χ∗ = argminχ

(cid:107)rm − Hm ∗ χ(cid:107)2 +

n
(cid:88)

i=0

ρ((cid:107)r(zci, χ)(cid:107)2

Ωci

)

+

m−1
(cid:88)

j=0

(cid:13)
(cid:13)r(zbj+1
(cid:13)

bj

, χ)

(cid:13)
2
(cid:13)
(cid:13)

Ωbj

+

m
(cid:88)

k=0

ρ((cid:107)r(zgvk , χ)(cid:107)2

Ωgvk




)


(6)

where Hm is the information matrix of a priori residuals.
r(zci) is the re-projection residual. r(zbj+1
) is the IMU
pre-integration residual. r(zgvk ) is the velocity residual.
Ωci, Ωbj , Ωgvk
are Hessian matrixes of re-projection, IMU
pre-integration and velocity.

bj

The residual of re-projection is

r(zci, χ) = qb
c

−1 (cid:104)

qw0
bi

−1 ∗ (X w0

i − pw0
bi

) − tb
c

(cid:105)

− pci

(7)

where pci is the normalized position of visual feature in
is the 3D position of visual feature

the camera frame. X w0
point in the initial world frame.

i

The residual for IMU pre-integration is

−1

⊗ qw0
bk
(cid:18)

pw0
bk+1

−1 ⊗ qw0

bk+1

− pw0
bk

− vw0
bk

rp = qw0
bk

−1 ⊗

rv = qw0
bk

−1 ⊗

(cid:16)

vw0
bk+1

− vw0
bk

+ g∆t

(cid:17)

rbω = bωk+1 − bωk
rba = bak+1 − bak

∆t +

g∆t2

1
2
− ∆vbk+1

bk

(cid:19)

− ∆pbk+1

bk

(8)

where ∆qbk+1

, ∆vbk+1
are the pre-integration of
bk
rotation, position and velocity between two consecutive
frames.

, ∆pbk+1
bk

bk

The residual of velocity is

r(zgvk , χ) = qw
w0

⊗ vw0
bk

− vw
k

(9)

where qw
w0 is the rotation error of the world frame estimated
in the initialization step. We will optimize it using a large
scale pose graph in the next section. vw
k is the velocity of
GNSS/RTK in the world frame. This term plays an important
role to restrain the scale divergence of VI SLAM, and also
help correct the course of motion.

B. Large scale pose graph

The step of pose graph integrates the poses estimated in
multiple sliding windows, as well as GNSS positions and
courses in a large scale. Furthermore, it also performs online
correction for extrinsic parameters between GNSS antenna
and IMU. The pose graph is performed in 4DOF since
GNSS measurements in z-axis are ambiguous which may
deteriorate the estimation of roll and pitch. Note that the
IMU measurements have provided observable roll and pitch
angles in BA process, we estimate only the yaw angle which
gains the observability from GNSS measures.

The overall state variables are

χ = (cid:2)ζ0, ζ1, ζ2...ζn, qw
w0
ζk = (cid:2)pw
, φw
bk
bk

(cid:3)

(cid:3)

, tg
b

χ∗ = arg min
χ

(cid:40) n
(cid:88)

(cid:13)
(cid:13)r(pbk+1
(cid:13)

bk

, χ)

(cid:13)
2
(cid:13)
(cid:13)

Ωpk

+

(cid:13)
(cid:13)r(φbk+1
(cid:13)

bk

, χ)

k=0
(cid:13)
2
(cid:13)
(cid:13)

Ωφk

+

n
(cid:88)

k=0

(cid:13)
(cid:13)r(φbw
(cid:13)

bk

, χ)

(cid:13)
2
(cid:13)
(cid:13)

Ωφvk

(10)

+

ρ((cid:13)

(cid:13)r(φw

g , χ)(cid:13)
2
(cid:13)
Ωφwk

) +

n
(cid:88)

k=0

ρ((cid:13)

(cid:13)r(pw

g , χ)(cid:13)
2
(cid:13)
Ωpwk

(cid:41)

)

n
(cid:88)

k=0
n
(cid:88)

k=0

Fig. 3.

Factor Graph of Multi-layer VI-GNSS Global Positioning Framework

− pw
bk

)−

monocular camera, IMU and global positions. All tests are
run on an Intel Xeon(R) W-2125 CPU at 3.0GHz with 32
GB memeory.

bk+1

, θw

bk+1

, γw

bk+1

)}−1

A. Experiment on Euroc

r(pbk+1
bk

r(φbk+1
bk

, θw
, χ) = q(φw
bk
bk
−1 ⊗ (pw0
qw0
bk
bk+1
, χ) = {q(φw
, θw
, γw
bk
bk
bk
−1 ⊗ qw0
⊗ {qw0
bk
⊗ qw0
r(φbw
, χ) = (qw
w0
bk
bk
g , χ) = φw
r(φw
− φw
bk
gk
g , χ) = pw
r(pw
+ q(φw
bk
bk

, θw
bk

bk+1

, γw
)−1 ⊗ (pw
bk
− pw0
bk
)−1 ⊗ q(φw

)

}
bk+1
)−1 ⊗ q(φw
bk

, θw
bk

, γw
bk

)

, γw
bk

) ⊗ tb

g − pw
gk

(11)

, θw
bk

, γw
bk

where, pw
bk

is the position and φw
bk

are the yaw,
pitch, roll angles of IMU in the world frame. pw0
, qw0
bk
bk
are the IMU poses in the initial world frame estimated
in the BA process. qw
is the rotation error between
w0
the initial estimated world frame and real world frame.
, φw
pw
gk are the position and course measurements of GNSS.
gk
r(pbk+1
, χ),r(φbk+1
, χ),r(φbw
, χ) are the prior residuals and
bk
bk
bk
Ωpk , Ωφk , Ωφvk
the information matrixes for BA esti-
mated relative translation, rotation and absolute yaw angle.
r(φw
g , χ) are the residuals and Ωpwk
the in-
formation matrixes for GNSS measured course and position.
ρ(x)is the huber loss function. We keep a large-scale sliding
window to fuse global GNSS positions and courses. This
plays an important role to avoid global position and yaw drift.
Furthermore, outer-layer estimator run at a low frequency to
reduce computational cost.

g , χ), r(pw

, Ωφwk

V. EXPERIMENTAL RESULTS

The proposed framework is evaluated on two public

datasets.

1. The famous indoor dataset EuRoc [28]: the performance
of the whole framework, especially the initialization is eval-
uated on each of 11 sequences, in comparison with other
state-of-the-art methods.

2. The outdoor dataset Kaist [29]: We validate the long-
term practicability to outdoor challanges, e.g., the GNSS
low excitation of
degradation, sparsity of feature points,
IMU, etc.

In addition, we also apply the proposed method on self-
developed intelligent vehicle and conduct large-scale tests.
In each test, we estimate the camera pose fusing data from

We ran the experiments using only images from the left
camera at 20Hz and IMU measurements at 200Hz. We add
Gaussian noise to the ground-truth measurements to simulate
global positions(refer to [30]). The standard deviation of the
noise in each direction is 0.2m. The frequency of the signal
is the same as the camera frequency. We conducted a total
of 11 sequences of the test. 5 sequences in the ofﬁce are
labeled as MH , and others in the factory scene as V .

1) Initialization performance: As shown in Table I, the
initialization performance is evaluated in terms of the scale
error and the time cost. On average, the scale errors of
the proposed algorithm before and after BA are 22.82%
and 8.13% respectively, which are signiﬁcantly better than
52.15% and 25.90% of VINS-Mono. This is because we
introduce the velocity of GNSS and use a tightly-coupled
framework to efﬁciently and accurately correct the scale.
The time consumption of calculating the initial value tInit
and BA tBA is close to that of VINS-Mono. Note that
VINS-Mono only performs visual-inertial initialization, the
proposed method additionally complete the world frame
alignment and therefore requires longer time tTot to complete
the overall initialization process.

2) Localization results: In Figure 4, we take sequence
V2 03 as an example to show intuitively the localization
results. It can be clearly seen that VINS-Mono trajectory
diverges badly from the ground truth due to the accumulated
position error and scale drift. VINS-Fusion [20] performs
signiﬁcantly better thanks to the fusion of GNSS to eliminate
accumulated errors. However, our proposed method provides
the best performance. The improvement derives from more
accurate initialization, as well as efﬁcient usage of global
measurements in the multi-layer estimator.

Table II shows comparative results of each sequence in
terms of the average translation error. As can be seen,
methods fusing GNSS are signiﬁcantly more accurate than
the VI only method. Among the VI-GNSS fusion methods,

TABLE I
INITIALIZATION COMPARATION: VINS-MONO WITH THE PROPOSED METHOD

VINS-Mono Initialization [2]

The proposed method

scale error (%)

scale error (%)

VI Align

VI Align.
+ BA

tInit
(s)

tBA
(s)

tT ot
(s)

VI Align.
+GNSS

VI + GNSS
+ BA

tInit
(s)

tBA
(s)

tT ot
(s)

62.23
65.65
72.45
45.21
44.14
70.36
25.20
29.72
53.39
49.22
57.09

51.65
36.83
49.15
33.26
13.84
27.75
13.34
12.39
14.99
10.93
20.80

0.10
0.10
0.12
0.15
0.11
0.12
0.13
0.11
0.10
0.11
0.11

0.15 1.28
0.14 1.45
0.14 3.17
0.21 1.09
0.14 1.13
0.15 2.80
0.19 2.49
0.17 1.56
0.15 1.86
0.16 1.18
0.15 1.33

33.78
26.83
30.31
22.77
12.85
30.14
10.01
33.52
17.47
10.74
22.55

10.25
7.23
12.84
7.07
4.35
4.34
5.26
5.29
3.49
14.65
14.67

0.08
0.09
0.06
0.05
0.06
0.07
0.06
0.08
0.08
0.05
0.06

0.13 3.63
0.13 1.14
0.08 8.40
0.09 1.54
0.10 1.27
0.09 6.34
0.10 2.52
0.11 1.59
0.11 1.64
0.08 2.03
0.09 1.06

Seq.
Name

V1 01
V1 02
V1 03
V2 01
V2 02
V2 03
MH 01
MH 02
MH 03
MH 04
MH 05

Mean Values

52.15

25.90

0.11

0.13 1.76

22.82

8.13

0.07

0.10 2.83

TABLE II

LOCALIZATION RESULTS ON THE EUROC DATASET

Translation mean error (m)

VI only

VINS-Mono

VINS-
Fusion [20]

VI-GNSS
Tightly-
coupled [30]

Proposed

0.216
0.236
0.273
0.191
0.551
0.139
0.112
0.259
0.104
0.152
0.385

0.178
0.060
0.090
0.044
0.083
0.048
0.055
0.034
0.100
0.048
0.343

0.031
0.036
0.048
0.068
0.056
0.041
0.048
0.068
0.038
0.046
0.098

0.036
0.026
0.044
0.026
0.040
0.035
0.026
0.025
0.022
0.033
0.029

Seq.
Name

MH 01
MH 02
MH 03
MH 04
MH 05
V1 01
V1 02
V1 03
V2 01
V2 02
V2 03

is Pointgrey Flea3 with the resolution of 1600×1200 and
the acquisition frequency of 10Hz. The IMU is Xsens’s MTi-
300, with frequency of 100Hz. The RTK-GNSS is SOKKIA’s
GRX2, with frequency of 1 Hz. Since the GNSS data has a
low frequency and is not synchronized with other sensors, we
linearly interpolate the GNSS data at each image frame. The
dataset also parovide ground-truth. It should be noted that,
as the authors mentioned [29], due to the very complicated
experimental scene, the ground-truth also has noise.

1) Localization results: The ﬁrst three sequences contain
typical urban scenes, and are labeled as urban . The last three
sequences are highway scenes, and are labeled as highway .
The total length for urban and highway are 29.99km and
8.8km respectively.

Figure 5 shows comparatively the trajectories of urban 1.
Since VINS-Fusion does not estimate the local-global trans-
formation in the real-time, the trajectory is produced by
ofﬂine optimization. Note that the trajectory of our proposed
method is generated in the online mode.

It is obvious from Figure 5 that VINS-Fusion improves the
trajectory smoothness in urban canyons where GNSS drift
occurs. However, continuous GNSS errors (refer to the upper
ﬁgures in Figure 5) will reveal the limitations of accumulated
scale drift in VI fusion and fragile local-global alignment in
the VI-GNSS loosely-coupled framework. Our proposed al-
gorithm eliminates scale drift in the inner-layer using global

Fig. 4. Localization result on EuRoC V2 03

the tightly-coupled one1 has better performance than the
loosely-coupled one (i.e., VINS-Fusion) due to the enhanced
optimization from global measurements. However, it can also
be sensitive to measurement noises. Our proposed method
overcomes the problem using a multi-layer framework and
achieves the best performance.

B. Outdoor experiments

We conduct experiments on the large-scale dataset Kaist
[29] and compare the result with VINS-Fusion. The camera

1Note that the result of [30] is directly quoted from the paper since
the algorithm is not open-sourced. We listed the result of N = 1 which
is consistent with our setup.

Fig. 5. Localization result on urban 1

C. Application on intelligent vehicle

We apply the proposed framework to a self-developed
intelligent vehicle. The localization platform is equipped
with a monocular rolling shutter camera which costs as
low as 10$. The resolution is 1920 × 1080. The IMU
module costs 2$, and its data frequency is 412Hz. The GNSS
module costs 10$ working under the RTK mode at 10Hz.
The sensors are synchronized using Pulse Per Second (PPS)
signal. The ground truth is obtained by post-process of a
high-proformance GNSS-inertial integration device Trimble
AP60.

We conducted tests on expressways and urban roads in
Chaoyang District, Beijing. The total length is 36.04km. As
shown in Table III, the localization error reduced by 63%
compared with VINS-Fusion.

LOCALIZATION ERROR ON SELF-DEVELOPED EXPERIMENTAL PLATFORM

TABLE III

Method.
GNSS
VINS-Fusion
Proposed

Translation error (m)
Mean Max 90% 95%
0.52 32.51 1.34 1.51
0.42 14.43 0.78 1.53
7.51 0.43 0.86
0.19

VI. CONCLUSIONS

This paper proposes a high-precision localization method
based on vision-inertial and GNSS fusion. A novel hi-
erarchical framework is developed to fuse various sensor
information efﬁciently in multiple layers to achieve accurate
and reliable real-time positioning. In order to quickly obtain
an accurate initial values, we also propose a numerical
solution aided MAP initialization method. Tests on indoor
and outdoor datasets, as well as on our self-developed
platform show that the algorithm in this paper can achieve
more accurate real-time localization compared to existing
algorithms in large-scale scenarios.

Fig. 6. Localization error distribution of Kaist dataset

REFERENCES

measurement and estimates the local-global transformation
more accurately in the outer-layer. The improvement ensures
enhanced robustness to long-term GNSS degradation.

Figure 6 shows the localization error statistics with box
plots of all sequences. Our proposed method signiﬁcantly
outperforms VINS-Fusion in both urban and highway sce-
narios. In urban scenarios, the improvement derives from the
enhanced robustness to GNSS drift, as we mentioned above.
The highway scenario faces challenges in two aspects: 1)
the visual features are sparse and are far away from the
camera; 2) the IMU excitation is weak due to the limited
speed variation. The problems therefore cause accumulated
scale drift. Our proposed method fuses the GNSS velocity in
the inner-layer BA to continuously correct the metric scale.
As can be seen, tests on all three highway sequences achieve
very high localization accuracy.

[1] Andrew J Davison, Ian D Reid, Nicholas D Molton, and Olivier Stasse.
IEEE Trans Pattern Anal

Monoslam: real-time single camera slam.
Mach Intell, 29(6):1052–1067, 2007.

[2] Tong Qin, Peiliang Li, and Shaojie Shen. Vins-mono: A robust and
versatile monocular visual-inertial state estimator. IEEE Transactions
on Robotics, 34(4):1004–1020, 2018.

[3] Henning Lategahn, Andreas Geiger, and Bernd Kitt. Visual slam for
autonomous ground vehicles. In 2011 IEEE International Conference
on Robotics and Automation, pages 1732–1737. IEEE, 2011.

[4] Raul Mur-Artal, J.M.M. Montiel, and Juan D. Tardos. Orb-slam: A
versatile and accurate monocular slam system. IEEE Transactions on
Robotics, 31(5):1147–1163, 2017.

[5] Qiang Fu, Hongshan Yu, Lihai Lai, Jingwen Wang, Xia Peng, Wei
Sun, and Mingui Sun. A robust rgb-d slam system with points and
IEEE Sensors Journal,
lines for low texture indoor environments.
19(21):9908–9920, 2019.

[6] Weibo Huang and Hong Liu. Online initialization and automatic
camera-imu extrinsic calibration for monocular visual-inertial slam.
In 2018 IEEE International Conference on Robotics and Automation
(ICRA), pages 5182–5189. IEEE, 2018.

[7] Xiwu Zhang, Yan Su, and Xinhua Zhu. Loop closure detection for
In 2017
visual slam systems using convolutional neural network.
23rd International Conference on Automation and Computing (ICAC),
pages 1–6. IEEE, 2017.

The euroc micro aerial vehicle datasets. The International Journal of
Robotics Research, 35(10):1157–1163, 2016.

[29] Jinyong Jeong, Younggun Cho, Young-Sik Shin, Hyunchul Roh, and
Ayoung Kim. Complex urban dataset with multi-level sensors from
The International Journal of
highly diverse urban environments.
Robotics Research, page 0278364919843996, 2019.

[30] Giovanni Ciofﬁ and Davide Scaramuzza. Tightly-coupled fusion of
global positional measurements in optimization-based visual-inertial
odometry. 2020.

[8] Tayyab Naseer, Michael Ruhnke, Cyrill Stachniss, Luciano Spinello,
and Wolfram Burgard. Robust visual slam across seasons. In 2015
IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS), pages 2529–2535. IEEE, 2015.

[9] Young-Sik Shin, Yeong Sang Park, and Ayoung Kim. Dvl-slam:
sparse depth enhanced direct visual-lidar slam. Autonomous Robots,
44(2):115–130, 2020.

[10] Julian Surber, Lucas Teixeira, and Margarita Chli. Robust visual-
inertial localization with weak gps priors for repetitive uav ﬂights.
In 2017 IEEE International Conference on Robotics and Automation
(ICRA), pages 6300–6306. IEEE, 2017.

[11] Yang Yu, Wenliang Gao, Chengju Liu, Shaojie Shen, and Ming
Liu. A gps-aided omnidirectional visual-inertial state estimator in
ubiquitous environments. In 2019 IEEE/RSJ International Conference
on Intelligent Robots and Systems (IROS), pages 7750–7755. IEEE,
2019.

[12] Urs Niesen, Venkatesan N Ekambaram, Jubin Jose, Lionel Garin,
and Xinzhou Wu. Robust positioning from visual-inertial and gps
In Proceedings of the 29th International Technical
measurements.
Meeting of the Satellite Division of The Institute of Navigation (ION
GNSS+ 2016), pages 788–793, 2016.

[13] Georg Klein and David Murray. Parallel tracking and mapping for
In 2007 6th IEEE and ACM international
small ar workspaces.
symposium on mixed and augmented reality, pages 225–234. IEEE,
2007.

[14] Jakob Engel, Jurgen Sturm, and Daniel Cremers. Semi-dense visual
the IEEE
odometry for a monocular camera.
international conference on computer vision, pages 1449–1456, 2013.
[15] Christian Forster, Matia Pizzoli, and Davide Scaramuzza. Svo: Fast
semi-direct monocular visual odometry. In 2014 IEEE international
conference on robotics and automation (ICRA), pages 15–22. IEEE,
2014.

In Proceedings of

[16] Gibson Hu, Shoudong Huang, Liang Zhao, Alen Alempijevic, and
In 2012
Gamini Dissanayake. A robust rgb-d slam algorithm.
IEEE/RSJ International Conference on Intelligent Robots and Systems,
pages 1714–1719. IEEE, 2012.

[17] Young-Sik Shin, Yeong Sang Park, and Ayoung Kim. Direct visual
In 2018 IEEE
slam using sparse depth for camera-lidar system.
International Conference on Robotics and Automation (ICRA), pages
5144–5151. IEEE, 2018.

[18] Michael Bloesch, Sammy Omari, Marco Hutter, and Roland Siegwart.
Robust visual inertial odometry using a direct ekf-based approach.
In 2015 IEEE/RSJ international conference on intelligent robots and
systems (IROS), pages 298–304. IEEE, 2015.

[19] Tong, Qin, Peiliang, Li, Shaojie, and Shen. Vins-mono: A robust and
versatile monocular visual-inertial state estimator. IEEE Transactions
on Robotics, 2018.

[20] Tong Qin, Shaozu Cao, Jie Pan, and Shaojie Shen. A general
optimization-based framework for global pose estimation with multiple
sensors. arXiv preprint arXiv:1901.03642, 2019.

[21] Ruben Mascaro, Lucas Teixeira, Timo Hinzmann, Roland Siegwart,
and Margarita Chli. Gomsf: Graph-optimization based multi-sensor
In 2018 IEEE International
fusion for robust uav pose estimation.
Conference on Robotics and Automation (ICRA), pages 1421–1428.
IEEE, 2018.

[22] Giovanni Ciofﬁ and Davide Scaramuzza. Tightly-coupled fusion of
global positional measurements in optimization-based visual-inertial
odometry. arXiv preprint arXiv:2003.04159, 2020.

[23] Ra´ul Mur-Artal and Juan D Tard´os. Visual-inertial monocular slam
with map reuse. IEEE Robotics and Automation Letters, 2(2):796–803,
2017.

[24] Carlos Campos, Jos´e MM Montiel, and Juan D Tard´os.

Inertial-
In 2020 IEEE
only optimization for visual-inertial
International Conference on Robotics and Automation (ICRA), pages
51–57. IEEE, 2020.

initialization.

[25] Richard Hartley and Andrew Zisserman. Multiple view geometry in

computer vision. Cambridge university press, 2003.

[26] Vincent Lepetit, Francesc Moreno-Noguer, and Pascal Fua. Epnp: An
accurate o (n) solution to the pnp problem. International journal of
computer vision, 81(2):155, 2009.

[27] Jonathan T Barron. A general and adaptive robust loss function. In
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 4331–4339, 2019.

[28] Michael Burri, Janosch Nikolic, Pascal Gohl, Thomas Schneider, Joern
Rehder, Sammy Omari, Markus W Achtelik, and Roland Siegwart.

