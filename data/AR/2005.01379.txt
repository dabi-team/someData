Detecting Abrupt Changes in the Presence of
Local Fluctuations and Autocorrelated Noise

0
2
0
2

y
a
M
4

]
E
M

.
t
a
t
s
[

1
v
9
7
3
1
0
.
5
0
0
2
:
v
i
X
r
a

Gaetano Romano
Department of Mathematics and Statistics,
Lancaster University, Lancaster, UK

Guillem Rigaill
Universit Paris-Saclay, CNRS, INRAE, Univ Evry,
Institute of Plant Sciences Paris-Saclay (IPS2),
91405, Orsay, France

Vincent Runge
Universit Paris-Saclay, CNRS, Univ Evry,
Laboratoire de Mathmatiques et Modlisation d’Evry
91037, Evry, France

Paul Fearnhead
Department of Mathematics and Statistics,
Lancaster University, Lancaster, UK

May 5, 2020

Abstract

Whilst there are a plethora of algorithms for detecting changes in mean in univari-
ate time-series, almost all struggle in real applications where there is autocorrelated
noise or where the mean ﬂuctuates locally between the abrupt changes that one
wishes to detect. In these cases, default implementations, which are often based on
assumptions of a constant mean between changes and independent noise, can lead
to substantial over-estimation of the number of changes. We propose a principled
approach to detect such abrupt changes that models local ﬂuctuations as a random
walk process and autocorrelated noise via an AR(1) process. We then estimate the
number and location of changepoints by minimising a penalised cost based on this

1

 
 
 
 
 
 
model. We develop a novel and eﬃcient dynamic programming algorithm, DeCAFS,
that can solve this minimisation problem; despite the additional challenge of de-
pendence across segments, due to the autocorrelated noise, which makes existing
algorithms inapplicable. Theory and empirical results show that our approach has
greater power at detecting abrupt changes than existing approaches. We apply our
method to measuring gene expression levels in bacteria.

Keywords: Breakpoints; Changepoints; Dynamic programming; FPOP; Optimal partition-
ing; Structural breaks.

2

Figure 1: Segmentations of well-log data: wild binary segmentation using the strengthened
Schwarz information criteria (top); segmentation under square error loss with penalty in-
ﬂated to account for autocorrelation in measurement error (middle); optimal segmentation
from DeCAFS with default penalty (bottom). Each plot shows the data (black line) the
estimated mean (red line) and changepoint location (vertical blue dashed lines).

1

Introduction

Detecting changes in data streams is a ubiquitous challenge across many modern appli-
cations of statistics. It has been identiﬁed as one of the key open problems for modern
analysis of large data (National Research Council 2013) and is important in such diverse
areas as bioinformatics (Olshen et al. 2004, Futschik et al. 2014), ion channels (Hotz et al.
2013), climate records (Reeves et al. 2007), oceonographic data (Killick et al. 2010) and
ﬁnance (Kim et al. 2005). The most common and important change detection problem is
that of detecting changes in mean, and there have been a large number of diﬀerent ap-
proaches to this problem that have been proposed (e.g. Olshen et al. 2004, Killick et al.
2012, Fryzlewicz 2014, Frick et al. 2014, Maidstone et al. 2017, Eichinger & Kirch 2018,
Fearnhead & Rigaill 2019, Fryzlewicz 2018b, amongst many others). Almost all of these
methods are based on modelling the data as having a constant mean between changes and

3

01000200030004000100000120000140000Time01000200030004000100000120000140000Time01000200030004000100000120000140000Timethe noise in the data being independent. Furthermore, all changepoint methods require
specifying some threshold or penalty that aﬀects the amount of evidence that there needs
to be for a change before an additional changepoint is detected. In general the methods
have default choices of these thresholds or penalties that have good theoretical properties
under strong modelling assumptions.

Whilst these methods perform well when analysing simulated data where the assump-
tions of the method hold, they can be less reliable in real applications, particularly if the
default threshold or penalties are used. Reasons for this include the noise in the data being
autocorrelated, or the underlying mean ﬂuctuating slightly between the abrupt changes
that one wishes to detect. To see this, consider change detection for the well-log data
(taken from Ruanaidh & Fitzgerald 2012, Fearnhead & Liu 2011) shown in Figure 1. This
data comes from lowering a probe into a bore-hole, and taking measurements of the rock
structure as the probe is lowered. The data we plot has had outliers removed. As the
probe moves from one rock strata to another we expect to see an abrupt change in the sig-
nal from the measurements, and it is these changes that an analyst would wish to detect.
Previous analyses of this data have shown that, marginally, the noise in the data is very
well approximated by a Gaussian distribution; but by eye we can see local ﬂuctuations in
the data that suggest either autocorrelation in the measurement error, or structure in the
mean between the abrupt changes.

The top plot shows an analysis of the well-log data that uses wild binary segmen-
tation (Fryzlewicz 2014) with the standard cusum test for a change in mean, and then
estimates the number of changepoints based on a strengthened Schwarz information cri-
teria. Both the cusum test and the strengthened Schwarz information criteria are based
on modelling assumptions of a constant mean between changepoints and independent,
identically-distributed (IID) Gaussian noise, and are known to consistently estimate the
number and location of the changepoints if these assumptions are correct. However in this
case we can see that it massively overﬁts the number of changepoints. Similar results are
obtained for standard implementation of other algorithms for detecting changes in mean,
see Figure 11 in the Supplementary Material.

Lavielle & Moulines (2000) and Bardwell et al. (2019) suggest that if we estimate
changepoints by minimising the squared error loss of our ﬁt with a penalty for each change,
then we can correct for potential autocorrelation in the noise by inﬂating the penalty used
for adding a changepoint. The middle plot of Figure 1 shows results for such an approach
(Bardwell et al. 2019); this gives an improved result but it still noticeably overﬁts.

By comparison, the method we propose models both autocorrelation in the noise and
local ﬂuctuations in the mean between changepoints – and analysis of the data using default
settings produces a much more reasonable segmentation of the data (see bottom plot of
Figure 1). This method is model-based, and assumes that the local ﬂuctuations in the
mean are realisations of a random walk and that the noise process is an AR(1) process. We
then segment the data by minimising a penalised cost that is based on the log-likelihood
of our model together with a BIC penalty for adding a changepoint.

4

The key algorithmic challenge with our approach is minimising the penalised cost.
In particular many existing dynamic programming approaches (e.g. Jackson et al. 2005,
Killick et al. 2012) do not work for our problem due to the dependence across segments
caused by the autocorrelated noise. We introduce a novel extension of the functional
pruned optimal partitioning algorithm of Maidstone et al. (2017), and we call the resulting
algorithm DeCAFS, for Detecting Changes in Autocorrelated and Fluctuating Signals. It
is both computationally eﬃcient (analysis of the approx 4000 data points in the well-log
data taking a fraction of a second on a standard laptop) and guaranteed to ﬁnd the best
segmentation under our criteria.

Whilst we are unaware of any previous method that tries to model both autocorrela-
tion and local ﬂuctuations, Chakar et al. (2017) introduced AR1Seg which aims to detect
changes in mean in the presence of autocorrelation. Their approach is similar to ours if
we remove the random walk component, as they aim to minimise a penalised cost where
the cost is the negative of the log-likelihood under a model with an AR(1) noise process.
However they were unable to minimise this penalised cost, and instead minimised an ap-
proximation that removes the dependence across segments. One consequence of using this
approximation is that it often estimates two consecutive changes at each changepoint, and
AR1Seg uses a further post-processing step to try and correct this. Moreover, our simula-
tion results show that using the approximation leads to a loss of power, particularly when
the autocorrelation in the noise is high.

The outline of the paper is as follows.

In the next section we introduce our model-
In Section 3 we present DeCAFS, a
based approach and the associated penalised cost.
novel dynamic programming algorithm that can exactly minimise the penalised cost. To
implement our method we need estimates of the model parameters, and we present a
simple way of pre-processing the data to obtain these in Section 4. We then look at the
theoretical properties of the method. These justify the use of the BIC penalty, show that
our method has more power at detecting changes when our model assumptions are correct
than standard approaches, and also that we have some robustness to model error – in that
we can still consistently estimate the number and location of the changepoints in such
cases by adapting the penalty for adding a changepoint. Sections 6 and 7 evaluate the new
method on simulated and real data; and the paper ends with a discussion.

Code implementing the new algorithm is available in the R package DeCAFS. This pack-
age and full code from our simulation study is available at github.com/gtromano/DeCAFS.

2 Modelling and Detecting Abrupt Changes

2.1 Model
Let y1:n = (y1, . . . , yn) ∈ Rn be a sequence of n observations, and assume we wish to detect
abrupt changes in the mean of this data in the presence of local ﬂuctuations and autocor-
related noise. We take a model-based approach where the signal vector is a realisation of a

5

random walk process with abrupt changes, and we super-impose an AR(1) noise process.

So for t = 1, . . . , n,

where for t = 2, . . . , n

yt = µt + (cid:15)t,

µt = µt−1 + ηt + δt, with ηt ∼
iid

N (0, σ2

η), δt ∈ R,

(1)

(2)

and δt = 0 except at time points immediately after a set of m changepoints, 0 < τ1 <
· · · < τm < n. That is δt = 0 unless t = τj + 1 for some j. This model is unidentiﬁable at
changepoints. If τ is a changepoint, then whilst the data is informative about µτ and µτ −1,
we have no further information about the speciﬁc value of δτ relative to ητ . We thus take
the convention that δτ = µτ − µτ −1 and ητ = 0, which is consistent with maximising the
likelihood for ητ . The noise process, (cid:15)t is a stationary AR(1) process with, for t = 2, . . . , n,

(cid:15)t = φ(cid:15)t−1 + νt with νt ∼
iid

N (0, σ2

ν),

(3)

Special cases of our model occur when φ = 0 or when σ2

for some autocorrelation parameter, φ, such that 0 ≤ φ < 1; and (cid:15)1 ∼ N (0, σ2

ν/(1 − φ2)).
η = 0. When φ = 0 our noise
process (cid:15)t is then IID, and the model is equivalent to a random walk plus noise with abrupt
changes. When σ2
η = 0 we are detecting changes in mean with an AR(1) noise process,
resulting in a formulation equivalent to the one of Chakar et al. (2017).

2.2 Penalised Maximum Likelihood Approach

In the following we will assume that φ, σ2
η and σ2
ν are known; we consider robust approaches
to estimate these parameters from the data in Section 4. We can then write down a
likelihood for our model as a function of µ1:n and δ2:n. Writing f (·|·) for a generic conditional
density, we have that the likelihood is

(cid:33)

f (µt|µt−1, δt)

f (y1|µ1)

(cid:32) n
(cid:89)

t=2

(cid:33)

f (yt|yt−1, µt−1, µt)

(cid:32) n
(cid:89)

t=2

L(y1:n; µ1:n, δ2:n) =

(cid:32) n
(cid:89)

(cid:26)

exp

−

∝

(µt − µt−1 − δt)2
2σ2
η

(cid:27)(cid:33)

(cid:26)

exp

−

(cid:27)

2σ2

(y1 − µ1)2
ν/(1 − φ2)
(cid:27)(cid:33)

(cid:26)

exp

−

((yt − µt) − φ(yt−1 − µt−1))2
2σ2
ν

.

t=2
(cid:32) n
(cid:89)

t=2

×

We have used the speciﬁc Gaussian densities of our model, and dropped multiplicative
constants, to get the second expression.

6

If we knew the number of changepoints we could estimate their position by maximising
this likelihood subject to the constraints on the number of non-zero entries of δ2:n. How-
ever, as we need to also estimate the number of changepoints we proceed by maximising
a penalised version of the log of the likelihood where we introduce a penalty β > 0 for
each changepoint – this is a common approach to changepoint detection, see e.g. Maid-
stone et al. (2017). It is customary to restate this as minimising a penalised cost, rather
than maximising a penalised likelihood, where the cost is minus twice the log-likelihood.
That is we estimate the number and location of the changepoints by solving the following
minimisation problem:

(cid:110)

(1 − φ2)γ(y1 − µ1)2 +

Fn = min
µ1:n
δ2:n
(cid:20)
λ(µt − µt−1 − δt)2 + γ

n
(cid:88)

(cid:16)

(yt − µt) − φ(yt−1 − µt−1)

(cid:17)2

+ β 1δt(cid:54)=0

(cid:21) (cid:111)
,

(4)

t=2

where λ = 1/σ2
case of a constant mean between changepoints, corresponding to σ2
µt−1 + δt ∀ t = 2, . . . , n and simply drop the ﬁrst term in the sum.

ν, and 1 ∈ {0, 1} is an indicator function. For the special
η = 0, we require µt =

η, γ = 1/σ2

2.3 Dynamic Programming Recursion

We will use dynamic programming to minimise the penalised cost (4). The challenge
here is to deal with the dependence across changepoints due to the AR(1) noise process
which means that some standard dynamic approaches for changepoint detection, such as
optimal partitioning (Jackson et al. 2005) and PELT (Killick et al. 2012), cannot be used.
To overcome this, as in Rigaill (2015) or Maidstone et al. (2017), we deﬁne the function
µ (cid:55)→ Qt(µ) to be the minimum penalised cost for data y1:t conditional on µt = µ,

(cid:110)

(1 − φ2)γ(y1 − µ1)2 +

Qt(µ) = min
µ1:t
δ2:t,µt=µ
(cid:20)

t
(cid:88)

λ(µi − µi−1 − δt)2 + γ

(cid:16)

(yi − µi) − φ(yi−1 − µi−1)

(cid:17)2

+ β 1δt(cid:54)=0

(cid:21) (cid:111)

.

i=2

So Fn = minµ∈R Qn(µ); and the following proposition gives a recursion for Qt(µ).

Proposition 1 The set of functions {µ (cid:55)→ Qt(µ) , t = 1, . . . , n} satisﬁes

Q1(µ) = (1 − φ2)γ(y1 − µ)2 and, for t = 2, . . . , n,

(cid:26)

Qt(µ) = min
u∈R

Qt−1(u) + min{λ(µ − u)2, β} + γ

(cid:16)

(yt − µ) − φ(yt−1 − u)

(cid:17)2(cid:27)

.

(5)

7

The intuition behind the recursion is that we ﬁrst condition on µt−1 = u, with the term
in braces being the minimum penalised cost for y1:t given u and µt = µ, and then minimise
over u. The cost in braces is the sum of three terms: (i) the minimum penalised cost for
y1:t−1 given u; (ii) the cost for the change in mean from u to µ; and (iii) the cost of ﬁtting
data point yt with µt. The cost for the change in mean, (ii), is just the minimum of the
constant cost for adding a change and the quadratic cost for a change due to the random
walk. The recursion applies to the special case of a constant mean between changepoints,
where λ = ∞, if we replace min{λ(µ − u)2, β} with its limit as λ → ∞, which is β1µ(cid:54)=u.

3 Computationally Eﬃcient Algorithm

3.1 The DeCAFS Algorithm

Algorithm 1 gives pseudo code for solving the dynamic programming recursion introduced
in Proposition 1. The key to implementing this algorithm is performing the calculations in
line 5, and how this can be done eﬃciently will be described below. Throughout we give
the algorithm for the case where there is a random walk component, i.e. λ < ∞, though it
is trivial to adapt the algorithm to the λ = ∞ case.

As well as solving the recursion for Qt(µ), Algorithm 1 shows how we can also obtain
the estimate of the mean, through a standard back-tracking step. The idea is that our
estimate of µn, ˆµn, is just the value of µ that maximises Qn(µ). We then loop backwards
through the data, and our estimate of µt is the value that minimises the penalised cost for
the data y1:t conditional on µt+1 = ˆµt+1, which can be calculated as Bt(µ) in line 11.

Finally, as we obtain the estimates of the mean, we can also directly obtain the estimated
changepoint locations. It is straightforward to see, by examining the form of the penalised
cost, that the optimal solution for δ2:n has δt+1 (cid:54)= 0 (and hence t is a changepoint) if and
only if λ(ˆµt+1 − ˆµt)2 > β.

3.2 The Inﬁmal Convolution

The main challenge with Algorithm 1 is implementing line 5. Firstly this needs a compact
way of characterising Qt(µ). This is possible as Q1(µ) is a quadratic function; and the
recursion maps piecewise quadratic functions to piecewise quadratic functions. Hence Qt(µ)
will be piecewise quadratic and can be deﬁned by storing a partition of the real-line together
with the coeﬃcients of the quadratics for each interval in this partition.

Next we can simplify line 5 of Algorithm 1. As written it involves minimising a two-
dimensional function, in (u, µ) ∈ R2, over the variable u. We can recast this operation into
a one-dimensional problem by introducing the concept of an inﬁmal convolution.

8

Qt−1(u) + min{λ(µ − u)2, β} + γ

(cid:16)

(yt − µ) − φ(yt−1 − u)

(cid:17)2(cid:27)

9

10

11

12

13

14

15

Bt(µ) ←− Qt(µ) + min{λ(µ − ˆµt+1)2, β} + γ
ˆµt ←− argmin Bt(µ)
if (ˆµt − ˆµt+1)2 > β/λ then

ˆτ ←− (t, ˆτ )

end

(cid:16)

(yt+1 − ˆµt+1) − φ(yt − µ)

(cid:17)2

Algorithm 1: DeCAFS

Data: y = y1:n a time series of length n
Input: β > 0, λ > 0, γ > 0 and 0 ≤ φ < 1.

Q1(µ) ←− (1 − φ2)γ(y1 − µ)2

1 begin Initialisation
2
3 end
4 for t = 2 to n do

(cid:26)

5

Qt(µ) ←− min

u

6 end
7 begin Backtracking
8

ˆµn ←− argmin Qn(µ)
ˆτ ←− n
for t = n − 1 to 1 do

end

16
17 end
18 Return ˆµ1:n, ˆτ

9

Deﬁnition 1 Let f be a real-valued function deﬁned on R and ω a non-negative scalar.
We deﬁne INFf,∞(θ) = f (θ) and for ω > 0,

INFf,ω(θ) = min
u∈R

(cid:0)f (u) + ω(u − θ)2(cid:1) ,

(6)

as the inﬁmal convolution of f with a quadratic term.

A good review of this transformation can be found in Chapter 12 of the book of Bauschke
& Combettes (2011); it is closely related to the proximal mapping of f , which corresponds
to the special case ω = 1/2.

A property of the convolution is its stability for quadratics: the inﬁmal transformation
of a quadratic is a quadratic. Indeed, one can easily prove that the quadratic q : µ (cid:55)→
aµ2 + bµ + c with (a, b, c) ∈ R+ × R2 is transformed into

INFq,ω : µ (cid:55)→

aω
a + ω

µ2 +

bω
a + ω

µ + c −

b2
4(a + ω)

.

We can also show that q and INFq,ω have the same minimum and argminimum. Moreover,
INFq,ω ≤ q, resulting in a ﬂattening of the quadratics.

The following proposition presents a reformulation of the update-rule into a minimiza-

tion involving inﬁmal convolutions. The proof is in Appendix B.

Proposition 2 The set of functions {Qt(µ) , t = 2, . . . , n} can be written as

Qt(µ) = min

Q=

t (µ), Q(cid:54)=

t (µ)

(cid:110)

(cid:111)

,

where

and

Q=

Q(cid:54)=

t (µ) = INFQt−1,γφ+λ(µ) + γ
1−φ
(cid:16)
t (µ) = INFQt−1,γφ(µ) + γ
1−φ

yt − φyt−1 − (1 − φ)µ
(cid:17)2

yt − φyt−1 − (1 − φ)µ

(cid:16)

(cid:17)2

,

+ β ,

Qt−1(u) = Qt−1(u) − γφ(1 − φ)

(cid:18)

u −

yt − φyt−1
1 − φ

(cid:19)2

.

3.3 Fast Inﬁmal Convolution Computation

t , ..., qs

As noted above we can represent Qt by Qt = (q1
t is a quadratic deﬁned
on some interval [di, di+1[ with d1 = −∞ and ds+1 = +∞. It is this representation of Qt
that we update at each time step. Some operations involved in solving the recursion, such
as adding a quadratic to a piecewise quadratic, or calculating the pointwise minimum of
two piecewise quadratics are easy to perform with a computational cost that is linear in
the number of intervals (see e.g. Rigaill 2015) . The following theorem shows that a fast
update for the inﬁmal convolution of a piecewise quadratic is also possible.

t ) where each qi

10

Theorem 1 Let Qt = (q1
t ) be the representation of the functional cost Qt. For all
ω ≥ 0, the representation returned by the inﬁmal convolution INFQt,ω has the following
order-preserving form:

t , ..., qs

INFQt,ω = (INFqu1
t

, INFqu2
t

, ..., INFqus∗−1

t

, INFqus∗

t

) ,

with 1 = u1 < u2 < ... < us∗−1 < us∗ = s and s∗ ≤ s.

In this algorithm we have input qi

The proof of this theorem is given in a general setting in Appendix C.
Algorithm 2 shows how we can now calculate INFQt,ω in a linear-in-piece O(s) time
t is the ith piece-wise
complexity.
quadratic from Qt with i ∈ {1, ..., s}. Algorithm 2 computes the intervals, domi
∗ such
that {domui
∗ , i = 1, ..., s∗} is the partition of the real line for INFQt,ω, with Q∗ storing the
associated quadratics for each interval in this partition. In Algorithm 2 we use the list-
operator Last(l) to designate the last element of the list l; index Last(l), delete Last(l) to
get the associated index of the last element or to delete this element.

t, where qi

∗ = INFqi

Algorithm 2:

INFQt,ω pruning

Q∗ ←− (q1

Input: List of ordered quadratics (q1

∗, q2
1 begin Initialization: Q∗ means ”Remaining quadratics” and LB ”Left Bound”
2
3 end
4 for i = 2 to s do
5

∗); LB ←− (−∞)

∗, . . . , qs−1

, qs
∗)

∗

j ←− index Last(Q∗)
∗(µi) − qj
µi : qi
while µi < Last(LB) do

∗(µi) = 0 with qi

∗(µ) < qj

∗(µ) for µ > µi close to µi

delete Last(Q∗); delete Last(LB)
j ←− index Last(Q∗)
∗(µi) − qj
µi : qi

∗(µi) = 0 with qi

∗(µ) < qj

∗(µ) for µ > µi close to µi

6

7

8

9

10

11

end
Q∗ ←− (Q∗, qi

∗); LB ←− (LB, µi)

12
13 end
14 s∗ = #LB (the number of element in LB)
15 for i = 1 to s∗ − 1 do
16
17 end
18 doms∗
∗ =]LB(s∗), +∞[
19 Return Q∗ and (dom1

∗ =]LB(i), LB(i + 1)]

∗, ..., doms∗
∗ )

domi

4 Robust Parameter Estimation

Our optimisation problem (4) depends on three unknown parameters: σ2
ν and φ. We
estimate these parameters by ﬁtting to robust estimates of the variance of the k-lag diﬀer-

η, σ2

11

enced data, zk

t = yt+k − yt, for k ≥ 1.

Proposition 3 With the model deﬁned by (1) – (3),

zk
t ∼ N

(cid:16) t+k
(cid:88)

i=t+1

δi, kσ2

η + 2

1 − φk
1 − φ2 σ2

ν

(cid:17)

,

t = 1, . . . , n − k.

Providing k is small relative to the lengths of segments, the mean of zk

t will be zero for
most t. This suggests that we can estimate the variance of zk
t using a robust estimator,
such as the median absolute diﬀerence from the median, or MAD, estimator. Fix K, and
let vk be the MAD estimator of the variance of zk
for k = 1, . . . , K. We estimate the
t
parameters by minimising the least square ﬁt to these estimates,

Sφ(σ2

η, σ2

ν) =

K
(cid:88)

(cid:16)

k=1

kσ2

η + 2

1 − φk
1 − φ2 σ2

ν − vk

(cid:17)2

.

In practice we can minimise this criteria by using a grid of values for φ and then for each φ
value analytically minimise with respect to σ2
ν ≥ 0. Obviously, if we are ﬁtting
a model without the random walk component we can set σ2
η = 0, or if we wish to have
uncorrelated noise we set φ = 0.

η ≥ 0 and σ2

An empirical evaluation of this method for estimating the parameters is shown in the
Supplementary material E.1. In our simulation study we use K = 15, though similar results
were obtained as we varied K.

5 Theoretical Properties

As is common with change-in-mean problems, we can reformulate our model as linear-
regression. To do this it is helpful to introduce new variables, ˜η1:n, that give the cumulative
eﬀect of the random-walk ﬂuctuations. To simplify exposition it is further helpful to deﬁne
this process so it has an invertible covariance matrix. So we will let ˜η1 ∼ N (0, σ2
η) and
˜ηt = ˜ηt−1 + ηt for t = 2, . . . , n. For a set of m changepoints τ1:m, and deﬁning τ0 = 0, we
can introduce a n × (m + 1) matrix Xτ0:m where the ith column is a column of τi−1 zeros
followed by n − τi−1 ones. Our model is then

y1:n = Xτ0:m∆ + ζ1:n,

(7)

where ζ1:n is a vector of Gaussian random variables with

Var(ζ1:n) = Var((cid:15)1:n) + Var(˜η1:n) := ΣAR + ΣRW

the sum of the variance matrices for the AR component of the model, (cid:15)1:n, and the random
walk component of the model, ˜η1:n; and ∆ is a (m + 1) × 1 vector whose ﬁrst entry is µ1 − ˜η1
and whose ith entry is δτi−1+1 the change at the (i − 1)th changepoint.

12

As shown in Appendix D, the unpenalised version of the cost that we minimise, condi-

tional on a speciﬁc set of changepoints, can be written as

C(τ1:m) = min

∆,˜η1:n,˜η1=0

(cid:2)(y1:n − Xτ0:m∆ − ˜η1:n)T Σ−1

AR(y1:n − Xτ0:m∆ − ˜η1:n) + ˜ηT

1:nΣ−1

RW ˜η1:n

(cid:3) ,

where ˜η1:n is assumed to be a column vector. Thus the penalised cost (4) is Fn =
minm,τ1:m [C(τ1:m) + mβ].
In the remainder of this section we will call C(τ1:m) the cost,
and C(τ1:m) + mβ the penalised cost.

Whilst our cost is obtained by minimising over η2:n, the following result shows that it

is equal to the weighted residual sum of squares from ﬁtting the linear model (7).

Proposition 4 The cost for ﬁtting a model with changepoints, τ1:m is

C(τ1:m) = min
∆

(y1:n − Xτ0:m∆)T (ΣAR + ΣRW)−1 (y1:n − Xτ0:m∆)

(8)

Let C0 denote the cost if we ﬁt a model with no changepoints. The following corollary,
which follows from standard arguments, gives the behaviour of the cost under a null model
of no changepoints. This includes a bound on the impact of mis-specifying the covariance
matrix, for example due to mis-estimating the parameters of the AR(1) or random walk
components of the model, or if our model for the residuals is incorrect.

Corollary 1 Assume that data is generated from model (7) with m = 0 but with ζ1:n
a mean-zero Gaussian vector with Var(ζ1:n) = Σ. Let α+
n be the largest eigenvalue of
(ΣAR + ΣRW)−1Σ. If Σ = ΣAR + ΣRW then C0 − C(τ1:d) ∼ χ2
d. Otherwise, for any x

Pr(C0 − C(τ1:d) > x) ≤ Pr(χ2

d > x/α+

n ),

Furthermore, if we estimate the number of changepoints using the penalised cost (4) with
penalty β = Cα+
n log n for any C > 2, then the estimated number of changepoints, ˆm,
satisﬁes Pr( ˆm = 0) → 1 as n → ∞.

To gain insight into the behaviour of the procedure in the presence of changepoints, and
how it diﬀers from standard standard change-in-mean procedures, it is helpful to consider
the reduction in cost if we add a single changepoint.

Proposition 5 Given a ﬁxed changepoint location τ1:

(i) The reduction in cost for adding a single changepoint at τ1 can be written as

for some vector v deﬁned as

C0 − C(τ1) = (vT y1:n)2,

v =

1

(cid:26)

(cid:113)

cτ1 − c2

0,τ1/c0

(ΣAR + ΣRW)−1uτ1 −

(ΣAR + ΣRW)−1u0

(cid:27)

,

c0,τ1
c0

where u0 is a column vector of n ones, uτ1 is a column vector of τ1 zero followed by
n − τ0 ones, and
c0 = uT

0 (ΣAR + ΣRW)−1uτ1, cτ1 = uT

0 (ΣAR + ΣRW)−1u0,

τ1(ΣAR + ΣRW)−1uτ1.

c0,τ1 = uT

13

(ii) The vector v in (i) satisﬁes (cid:80)n
(iii) For any vector w that satisﬁes (cid:80)n

i=1 vi = 0 and vT (ΣAR + ΣRW)v = 1.

i=1 wi = 0 and wT (ΣAR + ΣRW)w = 1,

(cid:32) n

(cid:88)

(cid:33)2

wi

≤

(cid:32) n

(cid:88)

(cid:33)2

vi

.

i=τ1+1

i=τ1+1

The vector v in part (i) of this proposition deﬁnes a projection of the data that is used
to determine whether to add a changepoint at τ1. The properties in part (ii) mean that
this projection is invariant to shifts of the data, and that the distribution of the reduction
in cost if our model is correct and there are no changes will be χ2
1. The statistic vT y1:n
can be viewed as analogous to the cusum statistic (Hinkley 1971) that is often used for a
standard change-in-mean problem, and in fact if we set φ = 0 and ση = 0 so as to remove
the auto-regressive and random-walk aspects of the model, |vT y1:n| is just the standard
cusum statistic. The power of our method to detect a change at τ1 will be governed by the
distribution of this projection applied to the data in the segments immediately before and
after τ1. For a single changepoint where the mean changes by δ this distribution is a non-
central chi-squared with 1 degree of freedom and non-centrality parameter δ2((cid:80)n
i=τ1+1 vi)2.
Thus part (iii) shows that v is the best linear projection, in terms of maximising the non-
centrality parameter, over all projections that are invariant to shifts in the data and that
are scaled so that the null distribution is χ2
1.

To gain insight into how the auto-regressive and random-walk parts of the model aﬀect
the information in the data about a change we have plotted diﬀerent projections v for
diﬀerent model scenarios in the top row of Figure 2. The top-left plot shows the projections
if we have φ = 0 for diﬀerent values of the random walk variance. The projection, naturally,
places more weight to data near the putative changepoint, and the weight decays essentially
geometrically as we move away from the putative changepoint. In the top-right plot we
show the impact of increasing the autocorrelation of the AR(1) process, with the absolute
value of the weight given to data points immediately before and after the putative change
increasing with φ.

A key feature of the random walk model is that for any ﬁxed σ2

η > 0 the amount of
information about a change will be bounded as we increase the segment lengths either
side of the change. This is shown in the bottom-left plot of Figure 2 where we show the
non-centrality parameter for detecting a change in the middle of the data as we vary n.
For comparison we also show the non-centrality parameter of a test based on the cusum
statistic (scaled so that it also has a χ2
1 distribution under the null of no change). We can
see that ignoring local ﬂuctuations in the mean, if they exist and come from a random
walk model, by using the cusum statistic leads to a reduction of power as segment lengths
increase. For comparison in the bottom right we show an equivalent comparison where we
consider an inﬁll asymptotic regime, so that as n increases we let the random walk variance
decay at a rate proportion to 1/n and we increase the lag-1 autocorrelation appropriately.
In this case using the optimal projection gives a non-centrality parameter that increases

14

Figure 2: Top row: projections of data v for detecting a change in the middle of n = 100
data-points. Random walk model (top-left) for varying σ2
η of 0.03 (black), 0.02 (red) and
0.01 (green); AR(1) plus random walk model (top-right) for σ2
η = 0.01 and varying φ of
In both plots the blue line shows the standard
0.4 (black), 0.2 (red) and 0.1 (green).
cusum projection. Bottom row: non-centrality parameter for a χ2
1 test of a change using
the optimal projection (solid line) and the cusum projection (dashed line) for a change of
size 1 in the middle of the data as we vary n. Out-ﬁll asymptotics (bottom-left) where
(σ2
η, φ) is (0.0025,0) (black), (0.01,0) (red), (0.0025,0.5) (green) and (0.01,0.5) (blue); In-ﬁll
asymptotics (bottom-right) where for n = 50 (σ2
η, φ) is (0.0025,0) (black), (0.01,0) (red),
(0.0025,0.5) (green) and (0.01,0.5) (blue).

15

020406080100−0.2−0.10.00.10.2nv020406080100−0.3−0.2−0.10.00.10.20.3nv1002003004005000246810nc100200300400500051015202530ncwith n, whereas the cusum statistic has power that can be shown to be bounded as we
increase n.

We now turn to the property of our method at detecting multiple changes. Based on

the above discussion, we will consider in-ﬁll asymptotics as n → ∞.

(C1) Let y1, . . . , yn be generated as a ﬁnite sample from a Gaussian process on [0, 1]; that
is yi = z(i/n) where, for t ∈ [0, 1] z(t) = µ(t) + ζ(t), µ(t) is a piecewise constant with
m0 changepoints at locations r1, . . . , rm0, and ζ(t) is a mean zero Gaussian process.
For a given n deﬁne the true changepoint locations as τ 0
i (cid:99). The change in
mean at each changepoint is ﬁxed and non-zero.

i = (cid:98)nr0

(C2) Assume there exists strictly positive constants cη, cν and cφ, such that we implement
ν = cν; or (ii) φ = exp{−cφ/n}

η = cη/n and either (i) φ = 0 and σ2

DeCAFS with σ2
and σ2

ν = cν(1 − exp{−2cφ/n}).

(C3) There exists an α such that for any large enough n if Σ0

noise in the data generating model (C1), and Σ(n)
AR + Σ(n)
by DeCAFS in (C2) then the largest eigenvalue of (Σ(n)

n is the covariance of the
RW is the covariance assumed
AR + Σ(n)
n is less than α.

RW)−1Σ0

The key condition here is (C3) which governs how accurate the model assumed by
DeCAFS is to the true data generating procedure. Clearly if the model is correct then
(C3) holds with α = 1. The following proposition gives upper bound on α in the the case
where the covariance of the data generating model is that of a random walk plus AR(1)
process, but with diﬀerent parameter values to those assumed by DeCAFS in (C2).

Proposition 6 Assume the noise process ζ(t) of the data generating process (C1) is equal
to a random walk plus an AR(1) process.

(i) If Cov(ζ(t), ζ(s)) = c0

η min(t, s) for t (cid:54)= s and Var(ζ(t)) = c0
implemented as in (C2)(i), then (C3) holds with α = max{c0

ηt + cν, and DeCAFS is
ν/cν, c0

η/cη}.

(ii) If Cov(ζ(t), ζ(s)) = c0

η min(t, s) + c0

ν exp{−c0

φ|t − s|} and DeCAFS is implemented as

in (C2)(ii), then for any (cid:15) > 0 (C3) holds with

α = max

(cid:40)

c0
ν
cν

c0
φ
cφ

(1 + (cid:15)),

(cid:32)

1 +

(cid:33)

cφ
c0
φ

c0
ν
cν

(1 + (cid:15)),

(cid:41)

c0
η
cη

The following result shows that we can consistently estimate the number of changepoints
and gives a bound on the error in the estimate of changepoint locations, if we use DeCAFS
under an assumption of a maximum number of changepoints (the assumption of a maximum
number changes is for technical convenience, though is common in similar results, e.g. Yao
1988).

16

Theorem 2 Assume data, y1:n, is generated as described in (C1), and let ˆm and ˆτ1: ˆm be
the estimated number and location of the changepoints from DeCAFS implemented with
parameters given by (C2), penalty β = Cα log n for some C > 2, and a maximum number
of changes mmax ≥ m0. Then as n → ∞: if φ > 0

(cid:18)

Pr

ˆm = m0, max

i=1,...,m0

(cid:12)
(cid:12)ˆτi − τ 0
i

(cid:19)

(cid:12)
(cid:12) = 0

→ 1;

and if φ = 0

Pr

(cid:18)

ˆm = m0, max

i=1,...,m0

(cid:12)
(cid:12)ˆτi − τ 0
i

(cid:12)
(cid:12) ≤ (log n)2

(cid:19)

→ 1.

The most striking part of this result is the very diﬀerent behaviour between φ = 0 and
φ > 0. In the latter case, asymptotically we detect the position of the changepoints without
error. This is because the positive autocorrelation in the noise across the changepoint helps
us detect it. In fact, as n → ∞ the signal for a change at t comes just from the lag-1
diﬀerence, yt+1 − yt. The variance of (yt+1 − yt) is O(1/n), and its mean is 0 except at
changepoints, where it takes a ﬁxed non-zero value. A simple rule based on detecting a
change at t if and only if (yt+1 − yt)2 is above some threshold, c1(log n)/n for some suitably
large constant c1, would consistently detect the changes. For the inﬁll asymptotics we
consider, empirically DeCAFS converges to such an approach as n → ∞.

6 Simulation Study

We now assess the performances of our algorithm in a simulation study on four diﬀerent
change scenarios, illustrated in Figure 3.

Simulations were performed over a range of evenly-spaced values of φ, ση, σν. There are
no current algorithms that directly model local ﬂuctuations in the mean, so we compare
with two approaches the assume a constant mean between changes: FPOP (Maidstone
et al. 2017) which also assumes IID noise, and AR1Seg (Chakar et al. 2017) that models
the noise as an AR(1) process. We compare default implementation of each method,
which involves robust estimates of the assumed model parameters. We also compare an
implementation of FPOP with an inﬂated penalty (Bardwell et al. 2019) to account for the
autocorrelated noise. To see the impact of possible misestimation of the model parameters,
we also implement DeCAFS and AR1Seg using the true parameters when this is possible.
We focus on the accuracy of these methods at detecting the changepoints. We deem a
predict change as correct if it is within ±2 observations of a true changepoint. As a measure
of accuracy we use the F1 score, which is deﬁned as the harmonic mean of the precision
(the proportion of detected changes which are correct) and the recall (the proportion of
true changes that are detected). The F1 score ranges from 0 to 1, where 1 corresponds to a
perfect segmentation. Results reported are based over 100 replications of each simulation
experiment.

17

Figure 3: Four diﬀerent change scenarios. Top-left, no change present, top-right, change
pattern with 19 diﬀerent changes, bottom-left up changes only, bottom-right, up-down
changes of the same magnitude. In this particular example data were generated from an
AR model with φ = 0.7, σν = 2.

In Figure 6A we report performances of the various algorithms as we vary φ for ﬁxed
values of σν = 2 and ση = 0. In Figure 6B, we additionally ﬁx φ = 0.85, but we vary the
size of changes. In these cases there is no random walk component and the model assumed
by AR1Seg is correct.

There are a number of conclusions to draw from these results. First we see that the
impact of estimating the parameters on the performance of DeCAFS and AR1Seg is small.
Second, we see that using a method which ignores autocorrelation but just inﬂates the
penalty for a change does surprisingly well unless the autocorrelation is large, φ > 0.5, this
is inline with results on the robustness of using a square error cost for detecting changes
in mean (Lavielle & Moulines 2000). For high values of φ, DeCAFS is the most accurate
algorithm. The one exception are the simulations where there are no changes: the default
penalty choice for AR1Seg is such that it rarely introduces a false positive.

In Figure 6C we explore the eﬀect of local ﬂuctuations in the mean by varying ση. We
see a quick drop oﬀ in performance for all methods as ση increases, consistent with the
fact that it is harder to detect abrupt changes when the local ﬂuctuations of the mean are
greater. Across all experiments, DeCAFS was the most accurate algorithm.

One word of caution when ﬁtting the full DeCAFS model, is that when ση is large it can
be diﬃcult to estimate the parameters, as a model with a very high random walk variance
produces data similar to that of a model with constant mean but high autocorrelation.
Whilst the impact on detecting changes of any errors when estimating the parameters is

18

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllupupdownnonerand1010002000300040005000010002000300040005000010002000300040005000010002000300040005000−20−10010−1001020−10−50510050100150200tSignalFigure 4: F1 Scores on the 4 diﬀerent scenarios. In A a pure AR(1) over a range of values
of φ, for ﬁxed values of σν = 2, ση = 0 and a change of magnitude 10. In B a pure AR(1)
process with ﬁxed φ = 0.85 and changes in the signal of various magnitudes. In C the full
model with φ = 0.85 for a range of values of ση. The grey line represent the cross-section
between parameters values in A, B and C. AR1Seg est. and DeCAFS est. refer to the
segmentation of the relative algorithms with estimated parameters. Note, in B the results
from DeCAFS and DeCAFS est overlap so only one line is visible. Other algorithms use
the true parameter values.

19

upupdownnonerand10.000.250.500.751.000.000.250.500.751.000.000.250.500.751.000.000.250.500.751.00fF1ScoreAupupdownnonerand105101520051015200.000.250.500.751.000.000.250.500.751.00Jump SizeF1ScoreBupupdownnonerand1012301230.000.250.500.751.000.000.250.500.751.00shF1ScoreCAlgorithmAR1SegAR1Seg estDeCAFSDeCAFS estfpopfpop InfFigure 5: F1 score on diﬀerent scenarios with AR(2) noise as we vary φ2. Data simulated
ﬁxing σν = 2, ση = 0 and φ1 = 0.3 over a change of size 20.

small, it can lead to larger errors in the estimate of the signal, µt: as diﬀerent parameter
estimates mean that the ﬂuctuations in the data are viewed as either ﬂuctuations in the
noise process or in the signal. An example of this is shown in Appendix E.1.

Finally we investigate the performance of DeCAFS when its model is incorrect. First
we follow Chakar et al. (2017) and simulate data with a constant mean between changes
but with the noise process being AR(2), i.e. (cid:15)t = φ1(cid:15)t−1 + φ2(cid:15)t−2 + νt. In Figure 5 we report
F1 Scores for DeCAFS and AR1Seg as we vary range φ2. Obviously as |φ2| increases, all
algorithms perform worse, but the segmentations returned from DeCAFS are the more
reliable as we increase the level of model error.

Second, we consider local ﬂuctuations in the mean that are generated by a sinusoidal
process rather than the random walk model, see Figure 6B. In Figure 6A we compare
performance of DeCAFS and AR1Seg as we vary the frequency of the sinusoidal process.
Again we see that DeCAFS gives more reliable segmentations in these cases. In the three
change scenarios performance decrease as we increase the frequency of the process. In these
cases it becomes signiﬁcantly harder to detect any changepoints, however DeCAFS still has
higher scores than AR1Seg since it is more robust and returns fewer false positives.

For the no change scenario, interestingly, we observe an increase in DeCAFS perfor-
mances: for low frequencies, in roughly half of the simulations, the estimated parameters
used by DeCAFS correspond to incorrectly modelling the process as a pure AR(1) process
(i.e. ˆφ (cid:54)= 0, ˆση = 0) which results in an increased number of false positives. If we knew
that the noise was independent we could overcome this problem by enforcing φ = 0.

20

upupdownnonerand1−1.00−0.75−0.50−0.250.00−1.00−0.75−0.50−0.250.000.40.60.81.00.40.60.81.0f2F1ScoreAlgorithmAR1Seg estDeCAFS estFigure 6: In A the F1Score on the 4 scenarios for the Sinusoidal Model for ﬁxed amplitude
of 15, changes of size 5 and IID Gaussian noise with a variance of 4, as we vary the frequency
of the sinusoidal process. In B an example of a realization with frequency 0.003 for the
updown scenario, vertical segments refer to estimated changepoint locations of DeCAFS
(in light green) and AR1Seg (in blue).

21

upupdownnonerand10.00000.00250.00500.00750.01000.00000.00250.00500.00750.01000.250.500.751.000.250.500.751.00frequencyF1ScoreAllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−30−20−1001020010002000300040005000tyBAlgorithmAR1Seg estDeCAFS estFigure 7: Data on 2000 bp of the plus-strand of the Bacilus subtilis chromosome. Grey dots
show the original data. The plain red line represents the estimated signal of DeCAFS with
a penalty of 10 log(n). The dashed black line represents the estimated signal of hmmTiling.

7 Gene Expression in Bacilus subtilis

We now evaluate DeCAFS on estimating the expression of cells in the bacteria Bacilus
subtilis. Speciﬁcally we analyze data from Nicolas et al. (2009), which is data from tiling
arrays with a resolution of less than 25 base pairs. The array contains several hundred
thousand probes which are ordered according to their position on the bacterial chromosome.
For a probe, labelled t say, we get an RNA expression measure, Yt. Figure 7 shows data
from 2000 probes. Code and data used in our analyses, presented below, are available on
forgemia : https://forgemia.inra.fr/guillem.rigaill/l2fpop_tiling_array_data.
The underlying expression level is believed to undergo two types of transitions, large
changes which Nicolas et al. (2009) call shifts and small changes which they call drifts.
Thus it naturally ﬁts our modelling framework of abrupt changes, the shifts, between
which there are local ﬂuctuations caused by the drifts. To evaluate the performance of
DeCAFS at estimating how the gene expression levels vary across the genome we will
compare to the hmmTiling method of Nicolas et al. (2009). This method ﬁts a discrete
state hidden Markov model to the data, with the states being the gene expression level,
and the dynamics of the hidden Markov model corresponding to either drifts or shifts. As a
comparison of computational cost for of the two methods, DeCAFS takes about 7 minutes
to analyse data from one of the strands, each of which contains around 192,000 data points.
Nicolas et al. (2009) reported a runtime of 5 hours and 36 minutes to analyse both strands.
A comparison of the estimated gene expression level from DeCAFS and from hmmTiling,

22

Figure 8: Benchmark comparisons. The number of promoters (left) and terminators (right)
correctly predicted, M (δ) using a 22 bp distance cutoﬀ, as a function of the number of
predicted breakpoints, R(δ). Plain black lines are the results of hmmTiling (as reported
in Figure 4 of Nicolas et al. 2009)). Dotted black lines are the results of hmmTiling when
considering all probes rather than only those called transitions. Plain red lines are the
results of DeCAFS using β = 10 log(n). The thin dark-green leaning line represent y = x.

for a 2000 base pair region of the genome, is shown in Figure 7. We see a close agreement in
the estimated level for most of the region, except for a couple of regions where hmmTiling
estimates abrupt changes in gene expression level that DeCAFS does not.

To evaluate which of DeCAFS and hmmTiling is more accurate, we follow Nicolas et al.
(2009) and see how well the estimated gene expression levels align with bioinformatically
predicted promoters and terminators. A promoter roughly corresponds to the start of
a gene, and a terminator the end, and we expect gene expression to increase around a
promoter and decrease around a terminator.

For promoters, consider all probe locations t from the tiling chip and consider a threshold
parameter δ. We can count the number of probe locations with a predicted diﬀerence
ˆdt = ˆµt+1 − ˆµt strictly greater than δ. We call this R(δ). Among those probes, we can
count how many have a promoter nearby (within 22 base pairs). We call this M (δ). By
symmetry we can deﬁne an equivalent measure for terminators. A method is better than
another if for the same R(δ) it achieves a larger M (δ).

Figure 8 plots M (δ) against R(δ) as we vary δ for DeCAFS and two diﬀerent estimates
from hmmTiling. In the case of promoters the prediction of hmmTiling is slightly better
than DeCAFS for lower thresholds but noticeably worse for higher thresholds.
In the
case of terminators the prediction of DeCAFS are clearly better than those of hmmTiling.

23

Given that DeCAFS was not developed to analyze such data we believe that its relatively
good performances for promoters and better performances for terminators is a sign of its
versatility.

8 Discussion

There are various ways of developing the DeCAFS algorithm, that build on other extensions
of the functional pruning version of optimal partitioning. For example, to make the method
robust to outliers, we can use robust losses, such as the bi-weight loss, instead of square
error loss to measure our ﬁt to the data (Fearnhead & Rigaill 2019). Alternatively we can
incorporate additional constraints on the underlying mean such as monotonicity (Hocking
et al. 2017) or that the mean decays geometrically between changes (Jewell & Witten 2018,
Jewell et al. 2019). Finally, the algorithm is inherently sequential and thus should be
straightforward to adapt to an online analysis of a data stream.

We do not claim that the method we present in Section 4 for estimating the param-
eters in our model is best.
It is likely that more eﬃcient or more robust methods are
possible, for example using diﬀerent robust estimates of the variances of the k-lag diﬀer-
ence data (Rousseeuw & Croux 1993); or using iterative procedures where we estimate
the changepoints, and then conditional on these changepoints re-estimate the parameters.
Using better estimates should lead to further improvement on the statistical performance
we observed in Section 6. Our theoretical results suggest that for estimating changes, mis-
estimation of the parameters, or errors in our model for the noise or local ﬂuctuations, can
be corrected by inﬂating the penalty for adding a changepoint. As such, in applications we
would suggest implementing the method for a range of penalty values, for example using
the CROPS algorithm (Haynes et al. 2017), and then choosing the number of penalties
using criteria that consider how the ﬁt to data improves as we add more changes (e.g.
Arlot et al. 2016, Fryzlewicz 2018a, Arlot 2019).
Acknowledgements This work was supported by EPSRC grant EP/N031938/1, and an
ATIGE grant from Genopole. The IPS2 beneﬁts from the support of the LabEx Saclay
Plant Sciences-SPS. We thank Pierre Nicolas for providing the output of hmmTiling on
the Bacillus subtilis data and his R code allowing us to generate Figure 8, which closely
resembles Figure 4 of Nicolas et al. (2009).

References

Arlot, S. (2019), ‘Minimal penalties and the slope heuristics: a survey’, arXiv:1901.07277 .

Arlot, S., Brault, V., Baudry, J.-P., Maugis, C. & Michel, B. (2016), capushe: CAlibrating

Penalities Using Slope HEuristics. R package version 1.1.1.
URL: https://CRAN.R-project.org/package=capushe

24

Bardwell, L., Fearnhead, P., Eckley, I. A., Smith, S. & Spott, M. (2019), ‘Most recent

changepoint detection in panel data’, Technometrics 61(1), 88–98.

Bauschke, H. H. & Combettes, P. L. (2011), Convex analysis and monotone operator theory

in Hilbert spaces, Vol. 408, Springer.

Chakar, S., Lebarbier, E., L´evy-Leduc, C. & Robin, S. (2017), ‘A robust approach for
estimating change-points in the mean of an AR(1) process’, Bernoulli 23(2), 1408–1447.

Eichinger, B. & Kirch, C. (2018), ‘A mosum procedure for the estimation of multiple

random change points’, Bernoulli 24(1), 526–564.

Fearnhead, P. & Liu, Z. (2011), ‘Eﬃcient Bayesian analysis of multiple changepoint models

with dependence across segments’, Statistics and Computing 21(2), 217–229.

Fearnhead, P. & Rigaill, G. (2019), ‘Changepoint detection in the presence of outliers’,

Journal of the American Statistical Association 114(525), 169–183.

Frick, K., Munk, A. & Sieling, H. (2014), ‘Multiscale change-point inference’, Journal of

the Royal Statistical Society: Series B 76(3), 495–580.

Fryzlewicz, P. (2014), ‘Wild Binary Segmentation for Multiple Change-Point Detection’,

Annals of Statistics 42, 2243–2281.

Fryzlewicz, P. (2018a), ‘Detecting possibly frequent change-points: Wild binary segmenta-

tion 2 and steepest-drop model selection’. arXiv:1812.06880.

Fryzlewicz, P. (2018b),

‘Tail-greedy bottom-up data decompositions and fast multiple

change-point detection’, The Annals of Statistics 46(6B), 3390–3421.

Futschik, A., Hotz, T., Munk, A. & Sieling, H. (2014), ‘Multiscale DNA partitioning:

statistical evidence for segments’, Bioinformatics 30(16), 2255–2262.

Haynes, K., Eckley, I. A. & Fearnhead, P. (2017), ‘Computationally eﬃcient changepoint
detection for a range of penalties’, Journal of Computational and Graphical Statistics
26(1), 134–143.

Hinkley, D. V. (1971),

‘Inference about the change-point from cumulative sum tests’,

Biometrika 58(3), 509–523.

Hocking, T. D., Rigaill, G., Fearnhead, P. & Bourque, G. (2017), ‘A log-linear time algo-

rithm for constrained changepoint detection’, arXiv:1703.03352 .

Hotz, T., Sch¨utte, O. M., Sieling, H., Polupanow, T., Diederichsen, U., Steinem, C. &
Munk, A. (2013), ‘Idealizing ion channel recordings by a jump segmentation multireso-
lution ﬁlter’, IEEE Transactions on Nanobioscience 12(4), 376–386.

25

Jackson, B., Scargle, J. D., Barnes, D., Arabhi, S., Alt, A., Gioumousis, P., Gwin, E., Sang-
trakulcharoen, P., Tan, L. & Tsai, T. T. (2005), ‘An algorithm for optimal partitioning
of data on an interval’, IEEE Signal Processing Letters 12(2), 105–108.

Jewell, S., Hocking, T. D., Fearnhead, P. & Witten, D. (2019), ‘Fast nonconvex deconvo-

lution of calcium imaging data’, Biostatistics .

Jewell, S. & Witten, D. (2018), ‘Exact spike train inference via l0 optimization’, The Annals

of Applied Statistics 12(4), 2457–2482.

Killick, R., Eckley, I. A., Ewans, K. & Jonathan, P. (2010), ‘Detection of changes in
variance of oceanographic time-series using changepoint analysis’, Ocean Engineering
37(13), 1120–1126.

Killick, R., Fearnhead, P. & Eckley, I. A. (2012),

‘Optimal detection of changepoints
with a linear computational cost’, Journal of the American Statistical Association
107(500), 1590–1598.

Kim, C.-J., Morley, J. C. & Nelson, C. R. (2005), ‘The structural break in the equity

premium’, Journal of Business & Economic Statistics 23(2), 181–191.

Laurent, B. & Massart, P. (2000), ‘Adaptive estimation of a quadratic functional by model

selection’, The Annals of Statistics 28(5), 1302–1338.

Lavielle, M. & Moulines, E. (2000), ‘Least-squares estimation of an unknown number of

shifts in a time series’, Journal of Time Series Analysis 21(1), 33–59.

Maidstone, R., Hocking, T., Rigaill, G. & Fearnhead, P. (2017), ‘On optimal multiple

changepoint algorithms for large data’, Statistics and Computing 27(2), 519–533.

Muller, K. E. & Stewart, P. W. (2006), Linear model theory: univariate, multivariate, and

mixed models, John Wiley & Sons.

National Research Council (2013), ‘Frontiers in massive data analysis’.

Nicolas, P., Leduc, A., Robin, S., Rasmussen, S., Jarmer, H. & Bessi`eres, P. (2009), ‘Tran-
scriptional landscape estimation from tiling array data using a model of signal shift and
drift’, Bioinformatics 25(18), 2341–2347.

Olshen, A. B., Venkatraman, E. S., Lucito, R. & Wigler, M. (2004), ‘Circular Binary
Segmentation for the Analysis of Array-Based DNA Copy Number Data’, Biostatistics
5, 557–572.

Reeves, J., Chen, J., Wang, X. L., Lund, R. & Lu, Q. Q. (2007), ‘A Review and Comparison
of Changepoint Detection Techniques for Climate Data’, Journal of Applied Meteorology
and Climatology 46(6), 900–915.

26

Rigaill, G. (2015), ‘A pruned dynamic programming algorithm to recover the best segmen-
tations with 1 to kmax change-points’, Journal de la Societe Francaise de Statistique
156(4), 180–205.

Rousseeuw, P. J. & Croux, C. (1993), ‘Alternatives to the median absolute deviation’,

Journal of the American Statistical Association 88(424), 1273–1283.

Ruanaidh, J. J. O. & Fitzgerald, W. J. (2012), Numerical Bayesian methods applied to

signal processing, Springer Science & Business Media.

Yao, Y.-C. (1988), ‘Estimating the number of change-points via Schwarz’s criterion’, Statis-

tics & Probability Letters 6(3), 181–189.

Zheng, C., Eckley, I. A. & Fearnhead, P. (2019), ‘Consistency of a range of penalised cost

approaches for detecting multiple changepoints’. arXiv:1911.01716.

27

SUPPLEMENTARY MATERIAL

A Proof of Proposition 1

The initial condition for Q1(µ) follows immediately from its deﬁnition.

Then, for t ∈ {2, ..., n}, we need to condition the problem separately on whether or not
we have a changepoint. If we consider no change in the mean of the signal, then we can we
can re-arrange the cost at time t based on the cost at time t − 1 in the following way:

Qt(µ|δt = 0) = min

u

(cid:26)

Qt−1(u) + λ(µ − u)2 + γ

(cid:16)

(yt − µ) − φ(yt−1 − u)

(cid:17)2(cid:27)

.

Similarly, when we have a change:

Qt−1(u) + λ(µ − u − δ)2 + γ

(cid:16)

(yt − µ) − φ(yt−1 − u)

(cid:17)2

(cid:27)

+ β

Qt(µ|δt (cid:54)= 0) = min
u,δ

(cid:26)

(cid:26)

= min

u

Qt−1(u) + γ

(cid:16)

(yt − µ) − φ(yt−1 − u)

(cid:27)

(cid:17)2

+ β

where the second equality comes from minimising over δ.

Lastly, to obtain the whole cost at time t we take the minimum of these two functions:

Qt(µ) = min {Qt(µ|δt = 0), Qt(µ|δt (cid:54)= 0)}

(cid:26)

= min

u

Qt−1(u) + min{λ(µ − u)2, β} + γ

(cid:16)

(yt − µ) − φ(yt−1 − u)

(cid:17)2(cid:27)

.

(cid:3)

B Proof of Proposition 2

From the result obtained in Appendix A, simple, albeit tedious, algebraic manipulation
enables us to re-write the recursions for Qt(µ|δt (cid:54)= 0) and Qt(µ|δt = 0) in terms of the
inﬁmal convolution operator. Let zt = yt − φyt−1.

For Qt(µ|δt (cid:54)= 0), we can rearrange

(cid:16)

γ

(cid:17)2

(yt − µ) − φ(yt−1 − u)
= γ(zt − µ)2 + γφ2u2 + 2γφuzt − 2γφuµ
= γ(zt − µ)2 + γφ2u2 + 2γφuzt + γφ(u − µ)2 − γφu2 − γφµ2

= γ(zt − µ + φu)2

= γφ(u − µ)2 − γφ(1 − φ)

(cid:18)

u −

(cid:19)2

zt
1 − φ

+ γφ

z2
t
1 − φ

+ γ(zt − µ)2 − γφµ2

28

Hence, we have

Qt(µ|δt (cid:54)= 0) = min
u∈R

Qt−1(u) − γφ(1 − φ)

u −

(cid:34)

(cid:18)

(cid:19)2

zt
1 − φ

(cid:35)

+ γφ(u − µ)2

+

γ
1 − φ

(zt − (1 − φ)µ)2 + β

= INFQt−1,γφ(µ) +

(cid:16)

γ
1 − φ

zt − (1 − φ)µ

(cid:17)2

+ β = Q(cid:54)=

t (µ),

where

Qt−1(u) = Qt−1(u) − γφ(1 − φ)

(cid:18)

u −

(cid:19)2

.

zt
1 − φ

Similar, for Qt(µ|δt = 0), we can rearrange

λ(µ − u)2 + γ

(cid:16)

(yt − µ) − φ(yt−1 − u)
(cid:18)

(cid:17)2

= (γφ + λ)(u − µ)2 − γφ(1 − φ)

u −

(cid:19)2

zt
1 − φ

+ γφ

z2
t
1 − φ

+ γ(zt − µ)2 − γφµ2.

Hence

Qt(µ|δt = 0) = INFQt−1,γφ+λ(µ) +

(cid:16)

γ
1 − φ

zt − (1 − φ)µ

(cid:17)2

= Q=

t (µ),

where Qt−1 is deﬁned above.

(cid:3)

C Proof of Theorem 1

The proof is based on the following lemmas.

Lemma 1 For any lower-bounded function Q : R → R, we deﬁne the proxy operator

(cid:40) R → R

ˆuω :

θ (cid:55)→ min

(cid:110)

(cid:16)

argmin
u∈R

Q(u) + ω(u − θ)2(cid:17)(cid:111)

.

The function ˆuω is non-decreasing on R.

Notice that we use a minimum in the deﬁnition of ˆuω only to get a single-valued function
(we could have done another choice). Indeed, taking Q = min(q1, q2) with q1(θ) = (θ + 1)2
and q2(θ) = (θ − 1)2, we have ˆu1(0) = argmin
2} and we need to make

(Q(u) + u2) = {− 1

2, 1

u∈R

a choice (here the smallest value) to get a well-deﬁned function.

29

Proof: We consider θ1, θ2 ∈ R such that θ1 < θ2 and deﬁne ˆu1 = ˆuω(θ1), ˆu2 = ˆuω(θ2).

Using the deﬁnition of ˆu1 and ˆu2 we can write

Q(ˆu1) + ω(ˆu1 − θ1)2 ≤ Q(ˆu2) + ω(ˆu2 − θ1)2 ,

Q(ˆu2) + ω(ˆu2 − θ2)2 ≤ Q(ˆu1) + ω(ˆu1 − θ2)2 .

Summing the two inequalities, the Q terms cancel out and we get

(ˆu2 − ˆu1)(θ2 − θ1) ≥ 0 ,

which shows that ˆu1 ≤ ˆu2 and the result is proven.

(cid:3)
In our stochastic models the function Q is described by a list of functions Q = (q1, ..., qs)
= qi where Di = [di, di+1[⊂ R is an interval and {Di}i=1,...,s a partition of the

with Q|Di
real line. To compute the convolution, we deﬁne the functions

qi(u) =

(cid:26) qi(u) ,
+∞ ,

if u ∈ Di ,
if u (cid:54)∈ Di .

The inﬁmal convolution of this kind of functions can be analytically described.

Lemma 2 The inﬁmal convolution of a function q given by

q(u) =

(cid:26) q(u) ,
+∞ ,

if u ∈ [m1, m2] ,
if u (cid:54)∈ [m1, m2] ,

with any function q continuously diﬀerentiable (C 1) on [m1, m2] is given by

INFq,ω(θ) =






(cid:16)

q(u) + ω(u − θ)2(cid:17)

min
u∈[m1,m2]
q(m1) + ω(m1 − θ)2 ,
q(m2) + ω(m2 − θ)2 ,

,

if

if
if

2] ,

θ ∈ [m∗

1, m∗
θ < m∗
1 ,
θ > m∗
2 ,

(9)

with [m∗

1, m∗

2] = [ 1

2ω q(cid:48)(m1) + m1, 1

2ω q(cid:48)(m2) + m2].

1, m∗

2 ∈ R such that for all θ ∈ [m∗

Proof: Using Lemma 1 we know that the proxy operator ˆuω with Q = q is a non-decreasing
function in θ. Thus, there exist m∗
2], the argminimum
of qω : u (cid:55)→ q(u) + ω(u − θ)2 belongs to the interval [m1, m2] and q = q on this interval.
As q is C 1, the stationary points of qω are solutions of the equation 1
2ω q(cid:48)(u) + u = θ. At
1 = 1
point m1 (resp. m2) we have the argminimum m∗
2 =
1
2ω q(cid:48)(m2) + m2). If we have θ < m∗
1, then the argminimum of qω is less than m1 and then
attained at u = m1 (as q(u) = +∞ if u < m1) and we get INFq,ω(θ) = q(m1) + ω(m1 − θ)2.
(cid:3)
With the same reasoning in case θ > m∗

2ω q(cid:48)(m1) + m1 (resp. m∗

1 with m∗

2 the lemma is proven.

1, m∗

Using these two lemmas, we can prove the following proposition.

30

Proposition 7 The inﬁmal convolution of the functional cost Q = (q1, ..., qs) is given by
INFQ,ω = (INFq1,ω, ..., INFqs,ω).

Proof: With previously introduced notations we have Q(θ) = min
i=1,...,s

{qi(θ)}. Then

(cid:18)

INFQ,ω(θ) = min
u∈R

min
i=1,...,s

{qi(θ)} + ω(u − θ)2

(cid:19)

(cid:18)

= min
u∈R

min
i=1,...,s

{qi(θ) + ω(u − θ)2}

(cid:19)

(cid:26)

= min
i=1,...,s

min
u∈R

(cid:27)
(cid:0)qi(θ) + ω(u − θ)2(cid:1)

,

{INFqi,ω(θ)} for all θ ∈ R.

which gives us INFQ,ω(θ) = min
INFQ,ω can be described by
i=1,...,s
a list (INFqν(1),ω, INFqν(2),ω, ..., INFqν(r),ω) with ν(i) ∈ {1, ..., s}. The function i (cid:55)→ ν(i) is
(cid:3)
increasing due to Lemma 1 (and ν(r) = s).
In order to prove Theorem 1 we only need to show that we can remove the overline sign in
(INFqν(1),ω, INFqν(2),ω, ..., INFqν(r),ω) without consequences. We assume that Q is continuously
diﬀerentiable (C 1) except at the points di for i = 2, ..., s. The left and right derivatives at
point θ are respectively designated by Q(cid:48)
+(θ). With these assumptions we can
prove the following result.

−(θ) and Q(cid:48)

Lemma 3 If at points θ = di we have Q(cid:48)
for the convolution.

−(di) > Q(cid:48)

+(di) then di is never an argminimum

Proof: We study the stationary points of Qω : u (cid:55)→ Q(u) + ω(u − θ)2. The necessary

condition for optimality Qω(u) ≤ Qω(u + (cid:15)) for all (cid:15) leads to the inequalities

1
2ω

Q(cid:48)

−(u) + u ≤ θ ≤

1
2ω

Q(cid:48)

+(u) + u .

−(u) > Q(cid:48)

In case Q(cid:48)
can not be used in any minimization of Qω and ˆuω never takes this value.

+(u) there exists no such θ satisfying the two inequalities so that this u
(cid:3)
With this result the di never appear as an argminimum for the convolution and using
Lemma 2, we get (INFqν(1),ω, INFqν(2),ω, ..., INFqν(r),ω) = (INFqν(1),ω, INFqν(2),ω, ..., INFqν(r),ω) in
Proposition 7.

By looking at updates in Propositions 1 and 2, it remains to prove that at any time step,
no slope discontinuity at θ = d in Qt = Q satisﬁes the inequality Q(cid:48)
+(d). We prove
this result by recursion: at the initialisation step, there is no such breakpoint in the cost
function and all the min operators involved can not produce them. We eventually have to
prove that the inﬁmal transformation in Lemma 2 can not introduce these discontinuities.

−(d) < Q(cid:48)

Around m∗

1 in (9) we have:

d
dθ

INFQ,ω(θ) =

(cid:26) dˆuω(θ)

dθ q(cid:48)(ˆuω(θ)) + 2ω( dˆuω(θ)

dθ − 1)(ˆuω(θ) − θ) ,

−2ω(m1 − θ) ,

if
if

θ ≥ m∗
1 ,
θ < m∗
1 ,

31

with the function θ (cid:55)→ ˆuω(θ) being the argminimum of the inﬁmal convolution (see Lemma
1). By direct computation with ˆuω(m∗
1) = m1 and m∗
1) =
q(cid:48)(m1) = INF
1). This result achieves the proof of Theorem 1.

2ω q(cid:48)(m1)+m1 we get INF

Q,ω+(m∗

Q,ω−(m∗

1 = 1

(cid:48)

(cid:48)

D Proofs for Section 5

By deﬁnition of the random-walk model for ˜η1:n in Equation (2) and the auto-regressive
model for (cid:15)1:n in Equation (3) we have that the covariance matrices have entries

[ΣAR]ij =

σ2
ν
1 − φ2 φ|i−j|,

[ΣRW]ij = σ2

η min{i, j}.

It is straightforward to ﬁnd that their inverses have entries

and

[Σ−1

AR]ij =






1/σ2
ν
(1 + φ2)/σ2
ν
−φ/σ2
ν
0

if i = j = 1 or n,
if i = j (cid:54)= 1 or n,
if |i − j| = 1,
otherwise,

[Σ−1

RW]ij =






1/σ2
η
2/σ2
η
−1/σ2
η
0

if i = j = n,
if i = j (cid:54)= n,
if |i − j| = 1,
otherwise,

The unpenalised cost conditional on the set of changepoints is

(cid:40)

C(τ1:m) = min

(1 − φ2)γ(y1 − µ1)2 +

n
(cid:88)

(cid:20)

λ(µt − µt−1 − δt)2 + γ

(cid:16)

(yt − µt) − φ(yt−1 − µt−1)

(cid:17)2(cid:21)(cid:41)

(cid:40)

= min

(1 − φ2)γ(y1 − µ1)2 +

t=2
(cid:20)

λ(˜ηt − ˜ηt−1)2 + γ

(cid:16)

n
(cid:88)

t=2

(yt − µt) − φ(yt−1 − µt−1)

(cid:17)2(cid:21)(cid:41)

where the minimisation is over µ1:n, and δ2:n consistent with the set of changepoints; and
we have made a change of variables such that ˜ηi − ˜ηi−1 = µi − µi−1 − δi for i = 2, . . . , n in
the second equality.

This change of variables is not unique, and we get the same value for any choice of ˜η1.

Thus we trivially have that

C(τ1:m)

(cid:40)

= min

(1 − φ2)γ(y1 − µ1)2 +

n
(cid:88)

(cid:20)
λ(˜ηt − ˜ηt−1)2 + γ

(cid:16)

t=2

(yt − µt) − φ(yt−1 − µt−1)

(cid:17)2(cid:21)

(cid:41)

+ λ˜η2
1

,

where the minimisation is now also over ˜η1, and the minimum is attained with ˜η1 = 0.

32

By our deﬁnition of the matrix Xτ1:m we have that if ∆ = (µ1 − ˜η1, δτ1:m) we can write

µ1:n = Xτ0:m∆ + ˜η1:n. Thus by re-writing the sums, e.g.

n
(cid:88)

t=2

λ{˜ηt − ˜ηt−1}2 + λ ˜η1

as λ = 1/σ2

η, gives that

2 = ˜ηT

1:nΣ−1

RW ˜η1:n,

C(τ1:m) = min
∆,˜η1:n

(cid:2)(y1:n − Xτ0:m∆ − ˜η1:n)T Σ−1

AR(y1:n − Xτ0:m∆ − ˜η1:n) + ˜ηT

1:nΣ−1

RW ˜η1:n

(cid:3) .

(10)

Proof of Proposition 4. To simplify notation we will write ˜η for ˜η1:n, y for y1:n and X
for Xτ0:m. Re-writing right-hand side of (10) gives
RW ˜η(cid:3)

AR(y − X∆ − ˜η) + ˜ηT Σ−1

min
∆,˜η

(cid:2)(y − X∆ − ˜η)T Σ−1
(cid:2){˜η − (Σ−1
+(y − X∆)T (cid:8)Σ−1

= min
∆,˜η

AR − Σ−1

AR(Σ−1

AR + Σ−1

RW)−1Σ−1

AR

= min

∆

(cid:2)(y − X∆)T (cid:8)Σ−1

AR − Σ−1

AR(Σ−1

AR + Σ−1

RW)−1Σ−1

AR

(cid:9) (y − X∆)(cid:3) .

AR + Σ−1
(cid:9) (y − X∆)(cid:3)

AR + Σ−1

RW)−1Σ−1

AR(y − X∆)}T (Σ−1

RW){˜η − (Σ−1

AR + Σ−1

RW)−1Σ−1

AR(y − X∆)}

Finally using the Woodbury matrix identity, for symmetric invertible matrices A and

B, (A + B)−1 = A−1 − A−1(A−1 + B−1)−1A−1. Thus we have

(cid:8)Σ−1

AR − ΣAR(Σ−1

AR + Σ−1

RW)−1Σ−1

AR

The result follows immediately.
Proof of Corollary 1.

(cid:9) = (ΣAR + ΣRW)−1 .

(cid:3)

As before write y for y1:n and X for Xτ0:d; further let X0 = Xτ0. The value of ∆ that

minimises the right-hand side of (8) is

ˆ∆ = {X T (ΣAR + ΣRW)−1X}−1X T (ΣAR + ΣRW)−1y.

To further simplify notation let A = (ΣAR + ΣRW)−1 and let Φ be such that A = ΦΦT
with Φ invertible; and let Ψ be a matrix such that Σ = ΨΨT . Then the reduction in cost
over ﬁtting no change is
C0 − C(τ0:d) = yT (cid:16)
= yT ΦT Φ−T (cid:16)

AX(X T AX)−1X T A − AX0(X T

0 AX0)−1X T
0 A
(cid:17)

AX(X T AX)−1X T A − AX0(X T

Φ−1Φy = yT ΦT BΦy,

0 AX0)−1X T

0 A

(cid:17)

y

for the matrix B = Φ−T (cid:16)
Φ−1. By standard
properties of linear models, as our model includes an intercept term this quadratic form is
invariant to adding a constant to all entries of y. Thus as our model assumes no change
we can, without loss of generality assume the mean of y is the zero vector.

AX(X T AX)−1X T A − AX0(X T

0 AX0A)−1X T
0

(cid:17)

33

Now it is straightforward to show that B2 = B and that B has rank d. Furthermore
as under our assumptions y is Gaussian with variance Σ, Φy has variance ΦΣΦT . From
standard results for quadratic forms of Gaussian random variables, see for example Theorem
9.5 of Muller & Stewart (2006), the distribution of our quadratic form, yT ΦT BΦy is

d
(cid:88)

i=1

αiZ 2
i ,

where αi are the non-zero eigenvalues of ΦT ΨT BΨΦ, and each Z 2
distributed random variables.

i are independent χ2
1

The result follows by ﬁrst noting that as B is a projection its eigenvalues are 1 or 0.
Thus αi ≤ α+, where α+ is the largest eigenvalue of ΦT ΨT ΨΦ, which by standard results
is also the largest eigenvalue of ΦΦT ΨΨT = (ΣAR + ΣRW)−1Σ. Thus

d
(cid:88)

i=1

αiZ 2

i ≤

d
(cid:88)

i=1

α+Z 2

i = α+

d
(cid:88)

i=1

Z 2
i ,

and the right-hand side has the same distribution as α+ times a χ2
Σ = ΣAR + ΣRW then we further have that αi = 1 and hence the distribution is χ2
d.

d random variable. If

To prove the consistency of ˆm we need to show that the probability of

C0 − C(τ1:d) < dβ

jointly for all d and τ1:d tends to 1. A standard argument (see the proof of Proposition 3.1
in Zheng et al. 2019), is to use a union bound:

n!
d!(n − d)!

Pr

(cid:18)

χ2

d >

(cid:19)

dβ
α+

Pr (cid:0)χ2

d > dC log(n)(cid:1)

Pr( ˆm = 0) ≥ 1 −

≥ 1 −

≥ 1 −

≥ 1 −

n
(cid:88)

d=1
n
(cid:88)

d=1
n
(cid:88)

d=1
n
(cid:88)

d=1

n!
d!(n − d)!

(cid:40)

nd exp

−d

(cid:40)

(cid:32)

exp

−d

(cid:33)(cid:41)

(cid:32)

C log(n) − (cid:112)2C log(n) − 1
2
(C − 2) log(n) − (cid:112)2C log(n) − 1
2

(cid:33)(cid:41)

with the second inequality using a tail bound for a χ2
& Massart 2000). The ﬁnal expression will tend to 1 as n → ∞ as C > 2.
Proof of Proposition 5.

d random variable (Lemma 1 in Laurent
(cid:3)

We use the notations A = (ΣAR +ΣRW)−1, u1 = uτ1, and write c0 = uT

0 Au0, c0,1 = uT

0 Au1

and c1 = uT

1 Au1.

34

The optimal cost is equal to yT Ay−(X T Ay)T (X T AX)−1X T Ay. If X is simply a column
, and X T Ay = uT

of ones, X = u0 then (X T AX)−1 = 1
c0
If X is the concatenation of u0 and u1, X = (u0 u1) we can compute

0 Ay.

X T AX =

(cid:20) c0
c0,1

(cid:21)

c0,1
c1

and (X T AX)−1 =

(cid:20)

1
c0c1 − c2
0,1

c1 −c0,1

−c0,1

c0

(cid:21)

.

We also have

Finally

(cid:20) U0
U1

(cid:21)

=

(cid:20) uT
0 Ay
uT
1 Ay

(cid:21)

= X T Ay .

C(τ1) = yT Ay −

(cid:16)

1
c0c1 − c2
0,1

U0c1U0 − 2U0c0,1U1 + U1c0U1

(cid:17)

.

Hence we can write the reduction in cost for ﬁtting a change as

C0 − C(τ1) =

(cid:16)

1
c0c1 − c2
0,1

c1U 2

0 − 2c0,1U0U1 + c0U 2
1

(cid:17)

−

=

1
0c1 − c0c2
c2
0,1

(cid:16)

c2
0,1U 2

0 − 2c0,1c0U0U1 + c2

0U 2
1

1
c0
(cid:17)

U 2
0

.

Simple algebraic rearrangement gives the result in (i).

For part (ii) note that (cid:80)n

i=1 vi = uT

0 v, using the deﬁnition of v gives

uT
0 v =

(cid:113)

1

c1 − c2

0,1/c0

(cid:26)

c0,1 −

(cid:27)

c0

= 0.

c0,1
c0

Similarly

vT (ΣAR + ΣRW)v =

1
c1 − c2
0,1/c0

(cid:40)

c1 − 2

c0,1
c0

c0,1 +

(cid:18) c0,1
c0

(cid:19)2

(cid:41)

c0

= 1.

Part (iii) is a standard result on the optimality of the weighted least squares estimator.
1 w)2
0 w = 0 and wT (ΣAR + ΣRW)w = 1. Using Lagrange multipliers we have that

To show it we can directly solve the constrained optimisation problem of maximising (uT
subject to uT
for constants α and δ

2(uT

1 w)u1 = αu0 + 2δ(ΣAR + ΣRW)w.

Deﬁning δ(cid:48) = (uT

1 w)/δ, and α(cid:48) = −α/(2δ), we get

w = δ(cid:48)(ΣAR + ΣRW)−1u1 + α(cid:48)(ΣAR + ΣRW)−1u0.

This means that w is a linear combination of the vectors (ΣAR + ΣRW)−1u1 and (ΣAR +
ΣRW)−1u0, with the constants uniquely deﬁned by the constraints. However this is the form
(cid:3)
that v as deﬁned in part (i) takes, hence part (iii) of the proposition holds.

35

Proof of Theorem 2

We will ﬁrst consider the case where φ = 0. For each n introduce the following sets of

segmentations of the data:

(cid:26)

An

i,m =

τ1:m : min

j=1,...,m

|τj − τ 0

i | > (log n)2

(cid:27)

; i = 1, . . . , m0, m = 1, . . . , mmax;

(cid:26)

Bn

m =

(cid:18)

τ1:m : max

i=1,...,m0

min
j=1,...,m

(cid:19)

|τj − τ 0
i |

(cid:27)

≤ (log n)2

; m = m0 + 1, . . . , mmax.

Thus An
i,m is the set of segmentations with m changepoints which do not contain a change
within a distance (log n)2 of the ith actual changepoint; and Bn
m is the set of segmentations
with m > m0 changepoints and that have one changepoint within a distance of (log n)2 of
each true changepoint. If a segmentation is in none of these sets then it must have the
correct number of chanepoints, and one changepoint within a distance (log n)2 of each true
change. As there are ﬁxed number of these sets, to prove our result we need to show that
Pr(ˆτ1: ˆm ∈ An

i,m) → 0 for each i and m; and Pr(ˆτ1: ˆm ∈ Bn

m) → 0 for each m.

Let C(τ1:m) denote the unpenalised cost for the segmentation τ1:m, with, for example,
C(τ1:m, τ 0
1:m0) the unpenalised cost from the segmentation that has the changepoints in the
union of τ1:m and τ 0
m) →
0. To do this consider a τ1:m ∈ Bn
m, we will compare the cost of this segmentation with that
of the true segmentation. As adding changepoints can only reduce the unpenalised cost we
have the diﬀerence in penalised costs is

1:m0. We ﬁrst show that for any m = m0 + 1, . . . , mmax, Pr(ˆτ1: ˆm ∈ Bn

C(τ1:m) + mβ − C(τ 0

1:m0) − m0β ≥ (m − m0)β − (cid:0)C(τ 0

1:m0) − C(τ1:m, τ 0

1:m0)(cid:1) .

Furthermore, by the same argument used in Corollary 1, (C(τ 0
stochastically bounded by a χ2

m distribution.

As there are fewer than (2(log n)2)m0nm−m0 segmentations in Bn

m we have

1:m0) − C(τ1:m, τ 0

1:m0))/α is

(cid:18)

Pr

min
τ1:m∈Bn
m

C(τ1:m) + mβ < C(τ 0

1:m0) + m0β)

(cid:19)

≤ (2(log n)2)m0nm−m0 Pr(χ2
= (2(log n)2)m0nm−m0 Pr(χ2

m > (m − m0)β/α)
m > (m − m0)C log n).

By a similar argument to that used in the proof of Corollary 1, this probability tends to 0
as required.

Now we consider τ1:m ∈ An

with that of the true segmentation. Let τ 0
τ 0
i .

i,m. Again we will compare the cost of such a segmentation
−i denote the set of true changepoints excluding

C(τ1:m) + mβ − C(τ 0
= {C(τ1:m, τ 0

1:m0) − m0β ≥ C(τ1:m, τ 0

−i) − C(τ 0

−i) − C(τ1:m, τ 0

1:m0) − m0β} + {C(τ1:m, τ 0

1:m0) + (m − m0)β
1:m0) − C(τ 0

1:m0) + mβ}

36

There are fewer than nm segmentations in An

i,m, and (C(τ1:m, τ 0
1:m0))/α is
m random variable. Thus by the same argument as above we

1:m0) − C(τ 0

stochastically bounded by a χ2
have that

(cid:18)

Pr

min
τ1:m∈An

i,m

C(τ1:m, τ 0

1:m0) − C(τ 0

1:m0) + mβ < 0

(cid:19)

→ 0.

To show Pr(ˆτ1:m ∈ An
(cid:18)

i,m) → 0 we only need to show

Pr

min
τ1:m∈An

i,m

C(τ1:m, τ 0

−i) − C(τ1:m, τ 0

1:m0) − m0β < 0

(cid:19)

→ 0.

−i) − C(τ1:m, τ 0

nv = 1, where Σ∗

1:m0) = (vT y1:n)
By the same argument as used in Proposition 5(i), C(τ1:m, τ 0
for some vector v = v1:n. By standard properties of linear models, it is straightforward to
show that v has the following properties: (i) vT Σ∗
n = ΣRW + ΣAR is the
variance of the noise in the ﬁtted model; (ii) v is orthogonal to the column-space of the X
matrix for the linear model (7) corresponding to the changepoints τ1:m, τ 0
1:m0; (iii) among
vectors v that satisfy (i) and (ii) it is the one that maximises the signal for a change at τi,
i.e. that maximises ((cid:80)τi
If we deﬁne ν = ((cid:80)τi

t=1 vi)2.
t=1 vi)2, we can bound ν by choosing any vector w = w1:n that satis-
ﬁes (ii) and then, after normalising using (i), property (iii) gives ν ≥ ((cid:80)τi
nw).
Let h = (cid:98)(log n)2(cid:99). We choose such a w deﬁned as wj = 1 for j = τi − h + 1, . . . , τi,
wj = −1 for τi + 1, . . . , τi + h, and wj = 0 otherwise. The column space of the X matrix in
property (ii) contains vectors whose jth entries are either identically 0 or identically 1 for
for j = τi − h + 1, . . . , τi + h, and hence this vector satisﬁes property (ii).

t=1 wi)2/(wT Σ∗

Now using the fact that we run DeCAFS with φ = 0 and so ΣAR is the identity:
nw = wT ΣARw + wT ΣRWw ≤ 2hcν + h3cη/n, and ν ≥ h2/(2hcν + h3cη/n). Thus
wT Σ∗
there exists c1 > 0 such that for large enough n, vT y1:n is normally distributed with
|E(vT y1:n)| ≥ c1 log n and Var(vT y1:n) ≤ α. So, for large enough n,

(cid:18)

Pr

min
τ1:m∈An

i,m
(cid:18)

≤ nm Pr

C(τ1:m, τ 0

i ) − C(τ1:m, τ 0

(cid:19)

1:m0) − m0β < 0
(cid:19)

Z <

1
√
α

{

(cid:112)

Cα log nm0 − c1 log n}

,

where Z is a standard normal random variable. Using standard tail bounds we get that
this probability tends to 0 as n → ∞ as required.

The argument for the case where φ > 0 is similar. The diﬀerences are just in the

deﬁnition of the sets An

i,m and Bn

m which are now
(cid:26)

(cid:27)

(cid:26)

An

i,m =

τ1:m : min

j=1,...,m

|τj − τ 0

i | > 0

; Bn

m =

τ1:m : max

i=1,...,m0

min
j=1,...,m

(cid:18)

(cid:19)

|τj − τ 0
i |

(cid:27)

= 0

;

and the ﬁnal part of the argument that shows

(cid:18)

Pr

min
τ1:m∈An

i,m

C(τ1:m, τ 0

i ) − C(τ1:m, τ 0

1:m0) − m0β < 0

(cid:19)

→ 0.

(11)

37

For this last part we use a diﬀerent vector w to bound the distribution of C(τ1:m, τ 0
i ) −
1:m0) = (vT y)2. Our choice of w has wτi = 1, wτi+1 = −1 and wj = 0 otherwise.
C(τ1:m, τ 0
nw = wT ΣARw + wT ΣRWw = 2(1 − φ)cν(1 − φ2) + cη/n. Now as
We then have wT Σ∗
φ = exp{−cφ/n} ≥ 1 − cφ/n we have wT Σ∗
nw ≤ c1/n for some constant c1. Thus ν ≥ n/c1.
As this is O(n) it is straightforward to use the same tail bounds of a normal random variable
to show (11)
Proof of Proposition 6

If we ﬁx n, and let Σ0 be the covariance matrix of the generated data then in case (i),
ηi + c0
ν.

η min i, j/n if i (cid:54)= j and [Σ0]ii = Var(ζ(i/n)) = c0

[Σ0]ij = Cov(ζ(i/n), ζ(j, n)) = c0
Whilst in case (ii),

[Σ0]ij = Cov(ζ(i/n), ζ(j, n)) = c0

η min i, j/n + c0

ν(exp{−c0

φ/n})|i−j|.

In both cases we can write Σ0 = Σ0
AR(1) process with auto-correlation parameter, φ0 = exp{−c0
ν and Σ0
c0
c0
η/n.

AR is the covariance matrix of an
φ/n}, and marginal variance
RW is the covariance matrix of a random walk process with variance parameter

RW where Σ0

AR + Σ0

We proceed by calculating a bound for the maximum eigenvalue of Σ−1Σ0, where Σ =
ΣAR + ΣRW and Σ0 = Σ0
RW are respectively the covariance assumed by DeCAFS and
the covariance of the data. We then further bound this as we vary n for the given parameter
regimes for the two covariance matrices. We do this ﬁrst for case (i) where φ = φ0 = 0,
then for the case where both autocorrelation parameters are non-zero.

AR + Σ0

Standard manipulations give that the maximum eigenvalues of Σ−1Σ0 is also the max-
imum eigenvalue of Σ−1/2Σ0Σ−1/2, where Σ−1/2 is a symmetric square root of Σ−1. If v is
an eigenvector of Σ−1/2Σ0Σ−1/2 with eigenvalue ρ, then

Writing w = Σ−1/2v, we have

vT Σ−1/2Σ0Σ−1/2v = ρvT v.

wT Σ0w
wT Σw

= ρ,

from which we have that we can bound the maximum eigenvalue by

max
w:|w|=1

wT Σ0w
wT Σw

= max
w:|w|=1
(cid:26)

≤ max

wT Σ0
ARw + wT Σ0
RWw
wT ΣARw + wT ΣRWw
wT Σ0
ARw
wT ΣARw

max
w:|w|=1

, max
w:|w|=1

wT Σ0
RWw
wT ΣRWw

(cid:27)

.

(12)

The ﬁrst part of the Proposition follows by noting that Σ0
φ0 = 0, Σ0

ν/cν)ΣAR. Hence,

AR = (c0

RW = (c0

η/cη)ΣRW, and, if φ =

max
w:|w|=1

wT Σ0
ARw
wT ΣARw

=

c0
ν
cν

, max
w:|w|=1

wT Σ0
RWw
wT ΣRWw

=

c0
η
cη

.

38

For the case where φ0 (cid:54)= 0 and φ (cid:54)= 0 we use a similar argument but, in addition, need
ARw/wT ΣARw. Now by similar arguments to above, we have that
AR , which in turn is

to bound maxw:|w|=1 wT Σ0
this is just the largest eigenvalue of Σ−1/2

ARΣ−1/2

AR Σ0

max
w:|w|=1

wT Σ−1

ARw
AR)−1w

.

wT (Σ0

To simplify notation and exposition, ﬁx n and let r = φ0. Then

Σ−1

AR =

1
cν(1 − exp{−2cφ/n})

Kφ, and(Σ0

AR)−1 =

1

ν(1 − exp{−2c0
c0

φ/n})

Kr,

where Kφ is an n × n matrix with entries

[Kφ]ij =





1
1 + φ2
−φ
0

if i = j = 1 or n,
if i = j (cid:54)= 1 or n,
if |i − j| = 1,
otherwise,

and similarly for Kr. Clearly we have

max
w:|w|=1

wT Σ−1

ARw
AR)−1w

wT (Σ0

=

c0
ν(1 − exp{−c0
φ/n})
cν(1 − exp{−cφ/n})

max
w:|w|=1

wT Kφw
wT Krw

.

(13)

Let v(i), for i = 1, . . . , n be the eigenvectors of Kr. Standard results, (see, e.g., ”Spectral
decomposition of Kac-Murdock-Szego Matrices”, a technical report by William F Trench
available at https://works.bepress.com/william_trench/133/), are that the eigenval-
ues are of the form 1 − 2r cos θi + r2, for some angles θ1, . . . , θn. Furthermore the entries of
v(i) satisfy

j−1 − 2 cos θiv(i)
v(i)

j + v(i)

j+1 = 0,

for j = 2, . . . , n,

with (2 cos θi − r)v(i)

1 = v(i)
Straightforward calculations then give

2 and (2 cos θi − r)v(i)

n = v(i)

n−1.

Kφv(i) = (1 − 2φ cos θi + φ2)v(i) + φ(r − φ)(v(i)

1 e1 + v(i)

n en),

where e1 and en are the n-vectors of 0s with a 1 in, respectively, the ﬁrst and nth entries.

Now writing w = (cid:80)n

i=1 div(i), we have

(cid:80)n

wT Kφw
wT Krw

=

i=1 d2

i (1 − 2φ cos θi + φ2) + φ(r − φ)(w2

1 + w2
n)

(cid:80)n

i=1 d2

i (1 − 2r cos θi + r2)

.

For any w with |w| = 1 we trivially have that

(cid:80)n
(cid:80)n

i=1 d2
i=1 d2

i (1 − 2φ cos θi + φ2)
i (1 − 2r cos θi + r2)

≤ max

θ

(1 − 2φ cos θ + φ2)
(1 − 2r cos θ + r2)

= max

(cid:26) (1 − φ)2
(1 − r)2 ,

(1 + φ)2
(1 + r)2

(cid:27)

.

39

Now if we write ρi = (1 − 2r cos θi + r2) for the ith eigenvalue of Kr, then

max
w:|w|=1

w2
1
i=1 d2

i ρi

(cid:80)n

(cid:17)2

(cid:16)(cid:80)n

i=1 div(i)
1
(cid:80)n
i=1 d2
i ρi

(cid:32) n

(cid:88)

=

i=1

(cid:33)

(v(i)

1 )2/ρi

,

= max
d:|d|=1

where we have ﬁrst rewritten w and w1 in terms of its expansion in the basis of the
eigenvectors of Kr, and then used the fact that the maximum is achieved with di ∝ v(i)
1 /ρi.
Using the fact that each v(i) is an eigenvector of K −1

r with eigenvalue 1/ρi,

(cid:32) n

(cid:88)

i=1

(cid:33)

(v(i)

1 )2/ρi

= [K −1

r

]11 =

1
1 − r2 .

By a similar argument for the term involving w2

max
w:|w|=1

wT Kφw
wT Krw

≤ max

(cid:26) (1 − φ)2
(1 − r)2 ,

Now using φ = exp{−cφ/n} and r = exp{−c0
φ + O(1/n) if cφ > c0
φ)/c0
c0
combining this with (12) and (13).

φ and 1 + O(1/n) if cφ ≤ c0

n we have
(cid:27)

(cid:26)

(1 + φ)2
(1 + r)2
φ/n} we have this bound is (cφ/c0

(cid:27)
(r − φ)
1 − r2 , 0

+ 2 max

φ

.

φ)2 + (cφ −
φ. The result follows trivially by

E Additional Empirical Results

E.1 Parameter Estimation

We provide a simple simulation study to highlight the behavior of our estimators described
in Section 4 for parameters ση, σν and φ. With K = 10, no change along the data, we
simulate 1200 time-series of length 5000 for each couple (φ, ω2) on a grid for φ ∈ { i−1
20 , i =
1, ..., 20} and ω2 = σ2
ν ∈ [0, 8] with a log scale of 40 elements. In Figure 9 we see that as
ω2 and φ increase, ση tends to be underestimated while σν overestimated. The φ parameter
is better estimated for small values of ω2 and intermediate values of φ. The random walk
variance is less biased than the AR(1) variance with also a better precision. Notice also that
the observed standard deviation for φ is often greater than 0.1 and an important deviation
to the true parameter of order 0.1 − 0.2 is not uncommon.

η/σ2

To see what might happen in case of a distorted parameter estimation, as mentioned
in the simulation study of Section 6, please refer to Figure 10. We can see there, how even
when misspecifying the model, in this case via ﬁtting a pure AR(1) when there was some
drift in the signal, we ﬁnd a distorted signal µ estimation, however we are still able to
reconstruct the changepoint locations relatively well.

E.2 Additional well-log data segmentation

In Figure 11 we report some additional segmentations of the log-well data described in
Section 1.

40

Figure 9: For each cell 1200 time-series of length 5000 have been generated under our model
(1) – (3) with no change. The left column corresponds to an accuracy measure: a percent
error for the variances and the bias for the φ parameter. The right column shows the
precision (standard deviation). We chose K = 10 for the estimators described in Section 4.

41

Figure 10: An example of a sequence generated with ση = 4, σν = 2, φ = 0.14, with
relative signal and changepoints estimates of DeCAFS with real parameter values compared
to DeCAFS with estimated ones. On this particular sequence, our estimator returns values
for initial parameters of ˆση = 0, ˆσν = 4.6, ˆφ = 0.98, resulting in a distorted signal
estimation.

42

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−400−300−200−1000010002000300040005000tyAlgorithmDeCAFSDeCAFS.estFigure 11: Segmentations of well-log data: Optimal segmentation under square error loss
with the default, BIC, penalty (top); segmentation with the AR1-seg method of Chakar
et al. (2017) that models the data as piecewise constant mean with AR(1) noise (middle);
optimal segmentation for constant-mean model with WBS2 and the number of changes
detected by the steepest drop to low levels criteria of Fryzlewicz (2018a) (bottom). Each
plot shows the data (black line) the estimated mean (red line) and changepoint location
(vertical blue dashed lines).

43

01000200030004000100000120000140000Time01000200030004000100000120000140000Time01000200030004000100000120000140000Time