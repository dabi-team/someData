RGB-D-E: Event Camera Calibration for Fast 6-DOF Object Tracking

Etienne Dubeau1*

Mathieu Garon1†

Benoit Debaque2 ‡

Raoul de Charette3§

Jean-Franc¸ois Lalonde1¶

1Universit ´e Laval

2Thales Digital Solutions

3Inria

0
2
0
2

g
u
A
5

]

V
C
.
s
c
[

2
v
1
1
0
5
0
.
6
0
0
2
:
v
i
X
r
a

Figure 1: Our RGB-D-E hardware setup uses a Kinect Azure (RGB-D) and a DAVIS346 event-based camera (E) for continuous
events which are temporally binned. RGB-D-E data streams are spatially and temporally calibrated and used for 6 degree of
freedom object tracking.

ABSTRACT

Augmented reality devices require multiple sensors to perform vari-
ous tasks such as localization and tracking. Currently, popular cam-
eras are mostly frame-based (e.g. RGB and Depth) which impose a
high data bandwidth and power usage. With the necessity for low
power and more responsive augmented reality systems, using solely
frame-based sensors imposes limits to the various algorithms that
needs high frequency data from the environement. As such, event-
based sensors have become increasingly popular due to their low
power, bandwidth and latency, as well as their very high frequency
data acquisition capabilities. In this paper, we propose, for the ﬁrst
time, to use an event-based camera to increase the speed of 3D object
tracking in 6 degrees of freedom. This application requires handling
very high object speed to convey compelling AR experiences. To
this end, we propose a new system which combines a recent RGB-D
sensor (Kinect Azure) with an event camera (DAVIS346). We de-
velop a deep learning approach, which combines an existing RGB-D
network along with a novel event-based network in a cascade fashion,
and demonstrate that our approach signiﬁcantly improves the robust-
ness of a state-of-the-art frame-based 6-DOF object tracker using our
RGB-D-E pipeline. Our code and our RGB-D-E evaluation dataset
are available at https://github.com/lvsn/rgbde_tracking.

Index Terms:
tracking—Augmented reality;

Event camera—Calibration—6-DOF Object

1 INTRODUCTION

Compelling augmented reality (AR) experiences are achieved
through the successful execution of several tasks in parallel. Notably,
simultaneous localization and mapping (SLAM) [31], hand track-
ing [30], and object tracking in 6 degrees of freedom (6-DOF) [7]

*e-mail: etienne.dubeau.1@ulaval.ca
†e-mail: mathieu.garon.2@ulaval.ca
‡e-mail: benoit.debaque@ca.thalesgroup.com
§e-mail: raoul.de-charette@inria.fr
¶e-mail: jean-francois.lalonde@gel.ulaval.ca

must all be executed efﬁciently and concurrently with minimal la-
tency on portable, energy-efﬁcient devices.

This paper focuses on the task of 6-DOF rigid object tracking.
In this scenario, successfully tracking the object at high speed is
particularly important, since freely manipulating an object can eas-
ily result in translational and angular speeds of up to 1 m/s and
360◦/s respectively. Despite recent progress on real time 6-DOF
object tracking at 30 fps [7, 20, 24], these methods still have trouble
with very high object motion and tracking failures are still com-
mon. Increasing the speed of 6-DOF object trackers is of paramount
importance to bring this problem closer to real-world applications.
To increase the speed of object tracking, one can trivially employ
cameras with frame rates higher than 30 fps. Indeed, 90 and even
120 fps off-the-shelf cameras are available and could be used as a
drop-in replacement. However, this comes at signiﬁcant practical
disadvantages: higher data bandwidth, increased power consumption
(since the algorithms must be executed more often), and the necessity
to have sufﬁcient light in the scene since exposure times for each
frame is necessarily decreased.

In this work, we propose a system to increase the speed of 6-DOF
object tracking applications with a minimal increase in bandwidth
and power consumption. Speciﬁcally, we propose to combine an
event camera (speciﬁcally, the DAVIS346 camera) with an RGB-D
camera (the Kinect Azure) into a single “RGB-D-E” capture system.
The event camera offers several key advantages: very low latency
(20 µs), bandwidth, and power consumption (10–30 mW), all while
having much greater dynamic range (120 dB vs 60 dB) than frame-
based cameras.

This paper makes the following contributions. First, we show
how to calibrate the setup both spatially and temporally. Second,
we provide a new challenging publicly available 6-DOF evaluation
dataset that contains approximately 2,500 RGB-D-E frames of a real-
world object with high-speed motion with the corresponding ground
truth pose at each frame. Third, we propose what we believe to be
the ﬁrst 6-DOF object tracker that uses event-based data. Similar to
previous work [7, 20, 24], our approach assumes that the object to
track must be rigid (non-deforming) and its textured 3D model must
be known a priori. Finally, we demonstrate through a quantitative
analysis on our real evaluation dataset that, using an extension of an
existing deep learning approach for 6-DOF object tracking results in
a threefold decrease in the number of tracking failures and achieves

IR FilterDAVIS346Azure Kinect 
 
 
 
 
 
robust tracking results on fast free interaction motions. We believe
this paper brings 6-DOF object tracking one step closer to real-world
augmented reality consumer applications.

2 RELATED WORK

The majority of computer vision systems rely on established frame-
based camera architectures, where the scene irradiance is cap-
tured synchronously at each pixel or in a rapid, rolling shutter se-
quence [21]. However, such cameras need to stream large amount of
data (most of which redundant), making them power- and bandwidth-
hungry. Recently, a newer camera architecture with a event-based
paradigm [22] is gaining popularity. By triggering events on each
pixel asynchronously when the brightness at that pixel changes by
certain threshold, event-based camera can stream at a much higher
frequency while consuming less power. A branch of computer vision
research now focuses on developing algorithms to take advantage of
this new type of data.

Event-based applications. Event-based sensors bring great
promises in the ﬁeld as their low power consumption makes them
ideal for embedded systems such as virtual reality headset [5],
drones [3, 40] or autonomous driving [26]. Their high-speed reso-
lution also enables the design of robust high-frequency algorithms
like SLAM [1, 5, 16, 32, 36, 41, 42] or fast 2D object tracking [9, 28].
While related to our work since we also focus on tracking, all related
works are still restricted to tracking objects in the 2D image plane.
In this paper, we extend the use of event cameras to the challenging
task of fast 6-DOF object tracking by building over a state-of-the-art
frame-based 6-DOF object tracker [6]. Different from other works,
we beneﬁt from RGB, Depth and Event data to propose the ﬁrst
RGB-D-E 6-DOF object tracker.

Deep learning with events. Using event-based data is not
straightforward since the most efﬁcient deep architectures for vision
are designed for processing conventional image data (e.g. CNNs).
In fact, it is still unclear how event-based data should be provided
to networks since each event is a 4-dimensional vector storing time,
2D position, and event polarity. Experimental architectures such as
spiking neural networks [23] holds great promises but are currently
unstable or difﬁcult to train [18]. With conventional deep frame-
works, events can be converted to 2D tensors only by discarding
both time and polarity dimensions [35] or to 3D tensors by discard-
ing either of the two dimensions [26, 46]. Recent work [37, 39]
has demonstrated that conventional grayscale frames can be recon-
structed from event data, opening the way to the use of existing
algorithms on these “generated” images. In this paper, we favor
the Event Spike Tensor formulation from Gehrig et al. [8], where
time dimension is binned. This allows us to exploit event data di-
rectly without requiring the synthesis of intermediate images, while
maintaining a fast convolutional network architecture.

Event-based datasets. Finally, large amount of training data
is required. While a few events datasets exist mostly for localiza-
tion/odometry [3, 19, 29] or 2D object tracking [12], there are, as
of yet, no 6-DOF object tracking dataset which contains event data.
Instead, event data can be synthesized with a simulator such as [34]
which allows various types of data augmentation [38]. Our exper-
iments show that a network can be trained without using real data
and is not critically affected by the real-synthetic domain gap.

3 SYSTEM OVERVIEW AND CALIBRATION

In this section, we describe our novel RGB-D-E hardware setup,
which combines a Microsoft Kinect Azure (RGB-D) with a
DAVIS346 event camera (E).

3.1 System overview

As illustrated in Fig. 1, the DAVIS346 event camera is rigidly
mounted over the Kinect Azure using a custom-designed, 3D-printed
mount. We observed that the modulated IR signal projected on the

(a)

(b)

Figure 2: Comparison between the factory presets and our cali-
bration. (a) The reprojection error from the Depth to RGB image
(TRGB
Depth) computed on 51 matching planar checkerboard images.
(b) Linear regression of the Kinect depth map error compared to the
expected depth, computed on calibration target corners.

Frame t0
Event [t0,t0 + ∆t]

Frame t0 + ∆t
Event [t0,t0 + ∆t]

Figure 3: Events from the DAVIS346 camera projected on Kinect
RGB frames on a moving calibration target. The projection is ob-
tained using the calibrated transformation TRGB
Event. Events of a mov-
ing checkerboard are accumulated and represented as red (negative)
and blue (positive) on both frames. Pixels with more than one event
are presented to reduce distraction by noise. Frames are captured at
a ∆t = 1/15 s interval and events between t0 and t0 + ∆t. A proper
alignment of events with the checkerboard demonstrates that the
system is calibrated both spatially and temporally.

scene by the Time-of-Flight (ToF) sensor in the Kinect triggered
multiple events in the DAVIS346 camera. To remedy this limitation,
an infrared ﬁlter is placed in front of the event camera lens.

3.2 Spatial calibration

Our system contains 3 cameras that must be calibrated: the Kinect
RGB, the Kinect Depth and the DAVIS346 sensor. In this paper, we
describe a coordinate system transformation with the notation Tb
a,
denoting a transformation matrix from coordinate frame a to b.

The intrinsic parameters of each camera can be computed with
a standard method [44]. The checkerboard corners can easily be
found using the color frame and the IR image from the Kinect Azure.
Calibrating an event-based sensor is usually more difﬁcult, however
the DAVIS346 possesses an APS sensor (gray scale frame-based
capture) that is spatially aligned with the event-based capture sensor.
We thus use the APS sensor to detect the target corners that will be
used for the intrinsic and extrinsic calibration.

Intrinsics. We capture images where a checkerboard target (9 ×
14 with 54 mm squares) is positioned in a spatial uniform distribution
in the frustum of each camera. To account for varying ﬁelds of view,
199 images were captured for the Kinect RGB, 112 for the Kinect
Depth, and 50 for the DAVID346. For each sensor, we retrieve the
intrinsic parameters (focal and image center) with a lens distortion
model including 6 radial and 2 tangential parameters.

factory presetsours 0246810pixels500100015002000expected depth (mm)2468101214error (mm)Linear correctionwithoutwith (ours)Extrinsics. We retrieve the rigid transformations TDepth
RGB and
TDepth
Event by capturing images of the target in overlapping frustums.
Once the 3D points are retrieved from the previously computed
camera intrinsic and the known checkerboard geometry, PnP [4] is
used to retrieve the 6-DOF transformation between each camera.

Finally, we compare our calibration procedure with the factory
presets of the Kinect Azure. Motivated by previous work [2] that
demonstrate lower accuracy errors with factories presets calibration
we capture a test dataset of 45 target images and show that we obtain
a lower reprojection error in Fig. 2-(a).

3.3 Depth correction

As [7, 11] reported for the Kinect 2, we also found that the depth
from the Kinect Azure has an offset that changes linearly w.r.t the
depth distance and average in an error in the range of 8.5 mm. We
compare the target points from the calibration dataset with the depth
pixels in each frame and ﬁt a 2nd-degree polynomial to the errors
w.r.t to their distance to the camera. In Fig. 2b, we show the error
with and without the polynomial correction on the test calibration
set. Using the correction, the mean error on the test calibration set is
less than 4 mm.

3.4 Temporal synchronization

In a multi-sensors setup, each sensor acquires data at its own fre-
quency aligned with its inner clock. For time-critical applications,
such as fast object tracking, it is required to synchronize the sen-
sors clocks to ensure temporal alignment of the data. Technically,
this is commonly addressed with synchronization pulses emitted
by a master sensor at the beginning of each data frame acquisition,
subsequently triggering the acquisition of other slave sensors.

In our setup, both Kinect and DAVIS346 support hardware syn-
chronization but we found that the Kinect (master) emits a variable
number of pulses before the ﬁrst RGB-D frame. This led to incorrect
triggering of DAVIS346 (slave) and thus temporal misalignment of
RGB-D and Event data. Because pulses are always emitted at the
same frequency, we ﬁx this by computing the pulses offset δ as

δ = (cid:98)RGBDt

0 × RGBDfps(cid:99) ,

(1)

where RGBDt
0 is the timestamp of the ﬁrst RGB-D frame and
RGBDfps is the Kinect framerate (here, 30). Following this, we can
(cid:1). Fig. 3 illustrates
pair RGBD and Event frames as (cid:0)RGBDi, Ei+δ
the projection of events captured on a moving checkerboard. The
events are captured between the two RGB frames. Alignment with
the borders of the pattern shows the temporal and spatial calibration.

4 FAST 6-DOF OBJECT TRACKING

With the sensors spatio-temporally calibrated, we enhance an exist-
ing tracking framework by the addition of the new event modality
(E). We build on the work of Garon et al. [6, 7] who propose a deep
learning approach of robust 6-DOF object tracking, which relies on
the reﬁnement between a render of the object at the current pose
estimate and the current Kinect RGB-D frame. While this method
is robust to occlusion and small displacements, we notice that it is
signiﬁcantly impacted by larger motions (over 0.5 m/s), possibly
because of the motion blur induced. Additionally, the network in [7]
is fundamentally limited by a maximum pose translation of 2 cm
between two frames. We note that increasing the sensor frame rate is
also not a practical solution as the network computation time is the
main bottleneck. In this section, we improve the tracker reactivity
and robustness with the addition of an event-speciﬁc network. In
the following, we ﬁrst describe the generation of synthetic data for
training and proceed to explain how frame-based and event-based
trackers are jointly used.

4.1 Training data generation

Despite the existence of event datasets [12, 19, 45], none of them
provide event data with 6-DOF object pose. Since capturing a
dataset of sufﬁcient magnitude and variety for training a deep
network is prohibitive, we rely on synthetic data generated from
an event camera simulator [34]. The engine renders a stream of
events that represent changes in pixel brightness, thus mimicking
event-based sensors. We build a training dataset by generating
sequences of events where our target object (here, a toy dragon)
is moved in front of a static camera. We acquire a textured 3D
model of the dragon with a Creaform GoScan™ handheld 3D
scanner at 1 mm voxel resolution, subsequently cleaned manually
using Creaform VxElements™ to remove background and spurious
vertices. As the camera remains stationary, we simulate the
scene background with a random RGB texture from the SUN3D
dataset [43] applied on a plane orthogonal to the virtual camera
optical axis. We next describe the simulation setup followed by
various data augmentation strategies applied to the data sample.

Simulation details. Event sequences are generated by ﬁrst
positioning the object in front of the camera at a random distance
d ∼ U(0.45 m, 0.8 m) (where U(a, b) denotes a uniform distribution
in the [a, b] interval) and a random orientation. The center of mass of
the object is aligned with the optical axis of the camera, so the object
appears in the center of the frame. The object is then displaced by a
random pose transformation over 33 ms and the generated events
are recorded. The transformation is generated by ﬁrst sampling
two directions on the sphere using spherical coordinate (θ , φ ) with
θ ∼ U(−180◦, 180◦) and φ = cos−1(2x − 1), where x ∼ U(0, 1) as
in [7] and then sample the magnitude of the translation and rotation
with U(0 m, 0.04 m) and U(0◦, 35◦) respectively. A 3D bounding
box of size 0.207 m around the object is projected on the image
plane. The event spatial axes are then cropped according to the
projected bounding box and resized with bilinear interpolation to a
spatial resolution of 150 × 150. Each 33 ms pose transformation
generates a set of N events storing {t, x, y, p}i=1..N where t is time, x
and y are pixel coordinates and p the polarity of the event (positive
or negative, indicating a brighter or darker transition respectively).
A total of 10 such event sets are simulated for each background
image, leading to 180,000 training and 18,000 validation sets.

Data augmentation. To maximize the reliability of our simula-
tions, we randomize some parameters as in [38] to increase variabil-
ity in the dataset and reduce the domain gap between synthetic and
real data. The contrast threshold, which deﬁnes the desired change
in brightness to generate an event, is difﬁcult to precisely estimate on
real sensors [5] and is instead sampled from a gaussian distribution
N(0.18, 0.03) (where N(a, b) denotes a gaussian distribution with
mean a and standard deviation b). Subsequently, the proportion
of ambient lighting versus diffuse lighting for the OpenGL render-
ing engine (employed in the simulator) is randomly sampled from
U(0, 1). To simulate tracking errors, the center of the bounding box
is offset by a random displacement of magnitude N(0, 25) pixels.
Finally, we notice the appearance of white noise captured by the
DAVIS346. To quantify the noise, we capture a sequence of a static
scene (which should generate no event) with the DAVIS346 and
count the number of noisy events generated in each 33 ms window.
A gaussian distribution is then ﬁt to the number of noisy events. At
training time, we sample a number k from the ﬁtted distribution
and randomly select k elements in the set (across t, x, and y) to add
uniformly to the input volume. This process is done separately for
each polarity (positive and negative). Fig. 4 shows the qualitative
similarity between real samples acquired with the DAVIS346 and
our synthetic samples at the same pose.

a
t
a
d

l
a
e
R

a
t
a
d

c
i
t
e
h
t
n
y
S

Figure 4: Qualitative comparison between real (top) and synthetic events (bottom). Synthetic frames are generated with the event simulator of
Rebecq et al. [34], where the pose of the synthetic object is adjusted to match the real. Event polarities are displayed as blue (positive) and red
(negative).

pose Pt can be obtained by

Pt = ∆P Pt−1 .

(2)

Note that all poses Pi are expressed in the RGB camera coordinate
system.

In this work, we rely on two deep networks to estimate ∆P. First,
our novel event network fe(e[t−1,t]) that takes event data e[t−1,t]
accumulated during the [t − 1,t] time interval, and cropped ac-
cording to the previous object pose TEvent
RGB =
(TDepth
RGB is the extrinsic camera calibration matrix from
sec. 3.2, necessary to transform the pose estimate in the event cam-
era coordinate system.

RGB Pt−1. Here, TEvent

Event )−1TDepth

Second, we also employ the RGB-D frame network of Garon
et al. [7] f f (ft , Pt−1). Although more recent techniques exist, this
choice was made because it is the only one providing both training
and inference code and offers robust performance. Alternatively,
other approaches such as [14, 25, 27] could also be considered. How-
ever, [14] is already outperformed by [7], the inference code of [25]
is limited to LineMOD objects [10], and [27] is an improvement
over [7] for occlusion handling, which is not the focus here. Note
that our method is not limited to this speciﬁc network and could be
extended to any RGB or RGB-D frame based tracker.

Each network aims to estimate the relative 6-DOF pose of the
object. Interestingly, while events are much more robust to fast
displacement they carry less textural information than RGB-D data
and we found that the event network used on its own is slightly less
accurate. Therefore, we use a cascade approach where the event
network ﬁrst estimate P(cid:48)
t , and subsequently the frame network is
provided with this new estimation for reﬁnement:

P(cid:48)
t = (TRGB

Event fe(e[t−1,t])) Pt−1 ,

Pt = f f (ft , r(P(cid:48)

t )) P(cid:48)
t ,

(3)

(4)

with TRGB
Event obtained from the extrinsic camera calibration matrices
from sec. 3.2 as before. Note that f f () is an iterative method and can
be run multiple time to reﬁne its prediction. To simplify the notation
we show a single iteration, in practice, 3 iterations are used as in
the original implementation. A diagram overview of the method is
provided in ﬁg. 5.

4.3 Event network
Event data is fundamentally different than frame-based data as it
possesses two extra dimensions for time and polarity (T × P × X ×Y ,

Figure 5: Overview of our method for high-speed tracking with both
sensors. On the left, our event network predicts the relative pose
changes ∆P from e[t−1,t]. The predicted pose is transformed to the
RGB referential to estimate the current pose P(cid:48)
t . On the right side,
the frame-based network [7] uses the improved pose P(cid:48)
t to compute
a ﬁnal pose reﬁnement.

4.2 Approach overview

In this paper, we assume that the pose of the object in the previous
frame, Pt−1, is known. In a full system, it could be initialized by a
3D object detector (e.g. SSD-6D [15]) at the ﬁrst frame (t = 0). The
task of a temporal object tracker is to determine the relative pose
change ∆P between two frames such that an estimate of the current

RenderFrame KinectContinuous eventsTimeX Event Network  FrameNetworkInput: e[t−1,t]

Input: r(Pt−1)

Input: ft

kernel-5
conv-3-64
ﬁre-32-64
ﬁre-64-128
ﬁre-128-256
ﬁre-128-512
FC-500
FC-6

Output: ∆P
(a)

conv3-64
ﬁre-32-64

conv3-64
ﬁre-32-64

concatenation
ﬁre-64-256
ﬁre-128-512
ﬁre-256-1024
FC-500
FC-6

Output: ∆P
(b)

Figure 6: Our deep network architecture for the (a) event and (b)
RGB-D frames. We use the same network architecture as [7]. The
main differences between both networks are that in (a) we only
have one head and a learnable kernel is added to merge temporal
information event input data e[t−1,t] [8]. The notation “kernel-x”
represents a learnable kernel of dimensions x convoluted on the
temporal dimensions with the weights shared for every pixel. The
notation “conv-x-y” represent a 2D convolution layer y ﬁlters of size
x × x, “ﬁre-x-y” are “ﬁre” modules [13] reducing the channels to
x and expanding to y and “FC-x” are fully connected network of
size x. Each ﬁre module has skip-links followed by a 2 × 2 max
pooling. Dropout of 30% is used after the activation function of each
“ﬁre” module and both “FC-500”. All layers, (except “FC-6”) are
followed by an activation function.

where T is discretized time and P is polarity.). We use the “Event
Spike Tensor” representation from [8] where the time dimension
is binned (in our case 9 bins for a 33 ms sample), and the polarity
dimension is removed by simply subtracting the negative events
from the positive ones. Finally, the spatial dimensions are resized
as explained in the previous section. The ﬁnal tensor has a shape
of 9 × 150 × 150 where each voxel represents the number of events
recorded per time bin. We normalize that quantity between 0 and 1
by dividing each voxel by the maximum amount of events seen in a
single voxel during training.

Event network architecture. While the event spike tensor can
be processed by a standard CNN, we follow [8] and ﬁrst learn a
1D ﬁlter in the time dimension and then apply a standard image
convolution where the time dimension acts as different channels. In
practice, we use the same backbone from [7] for the RGB-D frame
network and event network but change only the ﬁrst two input layers
to match the event spike tensor. Fig. 6 (a) shows the event network
architecture. The event network is optimized with ADAM [17] at a
learning rate of 0.001 and a batch size of 256. We train for 40 epoch
and apply a learning rate scheduling by multiplying the latter by 0.3
every 8 epochs.

4.4 RGB-D network

The RGB-D network (see [7] for more details) takes as input the
current RGB-D frame cropped according to the previous pose ft and
a rendering of the object at the previous pose r(Pt−1). Both inputs
have a shape of 4 × 184 × 184 and are normalized by subtracting
the mean and dividing by the standard deviation of a subset of the
training dataset. The last layer outputs the predicted 6-DOF pose
difference between both inputs.

RGB-D network architecture. As shown in ﬁg. 6-(b), each
input is individually convoluted then passed to a “ﬁre” module [13].
The module outputs are then concatenate before being max pooled.
The single feature map is fed to multiple “ﬁre” modules before being
applied to two fully connected layers. The RGB-D network is trained
with the same optimizer, hyper-parameters and data augmentations

(a)

(b)

(c)

(d)

Figure 7: Translation and rotation error as a function of object dis-
placement speed, computed over two consecutive frames. The ﬁrst
row shows the test set distribution indicating the number of frames
where the object has a particular (a) translation and (b) rotation
speed. The second row plots the distribution of errors between the
prediction and the ground truth, computed separately for (c) trans-
lation (eq. 5) and (d) rotation (eq. 6). Errors are computed on 10
sequences with a total of 2,472 frames.

from the original work (see [7] for more details).

4.5 Processing Time

The average inference time is 29.02 ms split in 25.06 ms for the
RGB-D network and 3.96 ms for the event network. Note that
inference can be reduced close to 25 ms total by running networks
in parallel since the total memory footprint is 152.48 MB (frame) +
54.96 MB (event) = 207.44 MB (total), which easily ﬁts on a modern
GPU. Runtimes are averaged over 100 samples and computed on an
Intel i5 and Nvidia GeForce GTX 1060.

The numbers reported above include all prepossessing steps such
as calculating the bounding box from the last known position and
rendering the image for [7]. Note that building the “Event Spike
Tensor” representation can be achieved in real time while the pre-
vious frame is being processed by the networks (average of 9ms).
While our current, unoptimized implementation computes those
steps serially, this operation could be trivially parallelized.

5 EXPERIMENTS

We now proceed to evaluate our RGB-D-E system for the high-
speed tracking of 3D objects in 6-DOF. We ﬁrst describe our real
test dataset, then present quantitative and qualitative results.

5.1 Test dataset

In order to compare the RGB-D and the RGB-D-E trackers, we
capture a series of real sequences of a rigid object freely moving at
various speeds with different environment perturbation and record
the corresponding RGB-D frames and events using our capture setup.
To provide a quantitative evaluation, we obtain ground truth pose
of the object at each frame using the approach described below.
We capture a total of 10 sequences with an average duration of
10 seconds, for a total of 2,472 frames and corresponding event data.
Examples from different sequences of the dataset are shown in Fig. 8.
The full dataset is publicly available.

051015202530Translation speed (mm/frame)050100150samples0.02.55.07.510.012.515.0Rotation speed (degree/frame)0100200300samples(0.0, 5.0](5.0, 10.0](10.0, 15.0](15.0, 20.0](20.0, 25.0](25.0, 30.0]Translation speed (mm/frame)02468101214Translation error (mm)Garon et al.Ours(0.0, 2.5](2.5, 5.0](5.0, 7.5](7.5, 10.0](10.0, 12.5](12.5, 15.0]Rotation speed (degree/frame)02468101214Rotation error (degree)Garon et al.Ourse
m
a
r
F

t
n
e
v
E

s
p
m
a
t
s
e
m
T

i

B
G
R

h
t
p
e
D

Figure 8: Visualization of the RGB-D-E dataset. Event amplitude (top) from 0 to 5 events accumulate for 33 ms. Event polarities are displayed
as blue (positive) and red (negative). Timestamp (second row) associated to each event. Only the last timestamp is displayed for each pixel and
range from 0 to 33 ms. Synchronized RGB (third row) and depth map frame (bottom) from the Kinect. The depth map interval is from 0 to 4 m.

For each sequence, we ﬁrst manually align the 3D model with the
object on the ﬁrst RGB frame. Then, we use ICP [33] to align the
visible 3D model vertices with the depth from the RGB-D frame,
back-projected in 3D. To avoid back-projecting the entire depth
frame, only a bounding box of (280 mm)3 centered around the
initial pose is kept. Vertex visibility is computed using raytracing
and updated at each iteration of ICP. If the angular pose difference
between two successive iterations of ICP is less than 10◦, it is
deemed to have converged and that pose is kept. If that condition
is not met after a maximum of 10 iterations, ICP diverges and the
ﬁnal pose is reﬁned manually. For all subsequent frames in the
sequence, ICP is initialized with the pose from the previous frame.
In all, every frame in our test dataset is manually inspected to ensure
a good quality pose is obtained, even when it has been determined
automatically.

5.2 Evaluation
We quantitatively compare our RGB-D-E tracker with the RGB-D
approach of Garon et al. [7], which is the current state-of-the-art in
6-DOF object tracking. We represent a pose P = [R t] by a rotation
matrix R and a translation vector t. The translation error δt between
a pose estimate and its ground truth (denoted by ∗) is reported as the
L2 norm between the two translation vectors
δt(t∗, t) = ||t∗ − t||2 .
The rotation error between the two rotation matrices is computed
using

(5)

δR(R∗, R) = arccos

(cid:18) Tr(RT R∗) − 1
2

(cid:19)

,

(6)

where Tr(·) denotes the matrix trace.

Fig. 7 compares the translation and rotation errors obtained by
both approaches. These plots report the error between two adjacent
frames only: the trackers are initialized to their ground truth pose
at the initial frame. Our method reports lower errors at translation
speeds higher than 20 mm/frame, which corresponds to approxi-
mately 600 mm/s, and similar rotation errors overall. This is not
surprising, given the fact that our method relies on the RGB-D
network of Garon et al. [7] to obtain its ﬁnal pose estimate.

However, visualizing the per-frame error does not tell the whole
story. Indeed, in a practical scenario the trackers estimate a succes-
sion of predictions instead of being reset to the ground truth pose at
every frame. Errors, even small, may therefore accumulate over time
and result in tracking failure. Following [7], we consider a tracking
i , ti) > 3 cm or δR(R∗
failure when either δt(t∗
i , Ri) > 20◦. Results
of this analysis are presented in Tab. 1. The experiment is done
at 30fps, 15fps, and 10fps, which is obtained by down-sampling
the input frame-based frequency. To allow comparison, the event
network is run at the same frequency with the same time window to
accumulate the events of 33 ms. For all frame rates, our RGB-D-E
approach has at least 61% fewer failures than the RGB-D approach
of Garon et al. [7].

Fig. 9 shows representative qualitative results comparing both
techniques with the ground truth. Those results show that the ap-
proach of Garon et al. [7] is affected by the strong motion blur which
arises under fast object motion. In contrast, our approach remains
stable and can follow the object through very fast motion. Please
see video results in the supplementary materials.

]
7
[

.
l
a

t
e

n
o
r
a
G

s
r
u
O

h
t
u
r
t

d
n
u
o
r
G

]
7
[

.
l
a

t
e

n
o
r
a
G

s
r
u
O

h
t
u
r
t

d
n
u
o
r
G

Figure 9: Qualitative comparison between the different approaches on different sequences. The overlay is the position predicted by each
method. From top to bottom, Garon et al. [7] (blue), ours (pink) and the ground truth (yellow). Each frame is continuous in the sequence and
cropped according to the ground truth position.

Method

Failures
15fps

30fps

Garon et al. [7]
Ours

83
28

130
48

10fps

166
64

Table 1: Number of tracking failures for each method with mul-
tiple RGB-D camera’s frame rates. Failures are computed on 10
independent sequences with a total of 2,472 frames.

6 DISCUSSION

We present a novel acquisition setup for simultaneous RGB-D-E
capture which combines a Kinect Azure camera with a DAVIS346
sensor. With the new event modality, we show that a state-of-the-art
RGB-D 6-DOF object tracker can be signiﬁcantly improved in terms
of tracking speed. We capture an evaluation dataset with ground truth
3D object poses that mimics difﬁcult scenarios typically encountered
in augmented reality applications : a user manipulating a small object
with fast free motions. Using this dataset, we demonstrate that our
approach achieves a threefold decrease in loss of tracking over the
previous state-of-the-art, thereby bringing 6-DOF object tracking
closer to applicability in real-life scenarios.

Limitations and future work. First, capturing an evaluation

dataset is time-consuming and obtaining the 6-DOF ground truth
pose of the object is difﬁcult, especially when fast motions are
involved. While our semi-automatic approach provided a way to
acquire a small number of sequences easily, scaling up to larger
RGB-D-E datasets will require more sophisticated apparatus such
as a motion capture (mocap) setup as in [7]. Indeed, mocap systems
are ideal for this use case as they can track the object robustly at
high frame rates. Second, while using a cascade scheme improves
signiﬁcantly the robustness to large motion of the tracker, it is still
inherently limited in accuracy since it always relies on the frame
network. The success of the cascade conﬁguration motivates fur-
ther exploration of better ways to fuse the Event modality with the
previous frame-based modalities. Third, we notice that the trackers
are still sensitive to dynamic backgrounds (see the last example in
the supplementary video). We anticipate that this could be partially
solved by generating training data with spurious structured events
such as those that could be created by a dynamic background (or a
moving camera). These represent exciting future research directions
that we plan to investigate in order to achieve even more robust
and accurate object tracking systems that can be used in real-world
augmented reality applications.

ACKNOWLEDGMENTS

The authors wish to thank J´er´emie Roy for his help with data acqui-
sion. This work was partially supported by a FRQ-NT Samuel de
Champlain grant, the NSERC CRDPJ 524235 - 18 grant, and Thales.
We thank Nvidia for the donation of the GPUs used in this research.

REFERENCES

[1] S. Bryner, G. Gallego, H. Rebecq, and D. Scaramuzza. Event-based,
direct camera tracking from a photometric 3d map using nonlinear
optimization. In International Conference on Robotics and Automation,
2019.

[2] C. Chen, B. Yang, S. Song, M. Tian, J. Li, W. Dai, and L. Fang.
Calibrate multiple consumer RGB-D cameras for low-cost and efﬁcient
3D indoor mapping. Remote Sensing, 10(2):328, 2018.

[3] J. Delmerico, T. Cieslewski, H. Rebecq, M. Faessler, and D. Scara-
muzza. Are we ready for autonomous drone racing? the uzh-fpv
drone racing dataset. In International Conference on Robotics and
Automation (ICRA), 2019.

[4] M. A. Fischler and R. C. Bolles. Random sample consensus: a
paradigm for model ﬁtting with applications to image analysis and
automated cartography. Communications of the ACM, 24(6):381–395,
1981.

[5] G. Gallego, J. E. Lund, E. Mueggler, H. Rebecq, T. Delbruck, and
D. Scaramuzza. Event-based, 6-DOF camera tracking from photomet-
ric depth maps. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 40(10):2402–2412, 2017.

[6] M. Garon and J.-F. Lalonde. Deep 6-DOF tracking. IEEE Transactions

on Visualization and Computer Graphics, 23(11), Nov. 2017.

[7] M. Garon, D. Laurendeau, and J.-F. Lalonde. A framework for evalu-
ating 6-DOF object trackers. In European Conference on Computer
Vision, 2018.

[8] D. Gehrig, A. Loquercio, K. Derpanis, and D. Scaramuzza. End-to-
end learning of representations for asynchronous event-based data.
IEEE/CVF International Conference on Computer Vision, Oct 2019.

[9] A. Glover and C. Bartolozzi. Robust visual tracking with a freely-
moving event camera. In IEEE/RSJ International Conference on Intel-
ligent Robots and Systems, 2017.

[10] S. Hinterstoisser, V. Lepetit, S. Ilic, S. Holzer, G. Bradski, K. Konolige,
and N. Navab. Model based training, detection and pose estimation of
texture-less 3d objects in heavily cluttered scenes. In Asian conference
on computer vision, pp. 548–562. Springer, 2012.

[11] T. Hodan, P. Haluza, ˇS. Obdrˇz´alek, J. Matas, M. Lourakis, and X. Zab-
ulis. T-less: An RGB-D dataset for 6D pose estimation of texture-less
In IEEE Winter Conference on Applications of Computer
objects.
Vision, 2017.

[12] Y. Hu, H. Liu, M. Pfeiffer, and T. Delbruck. Dvs benchmark datasets for
object tracking, action recognition, and object recognition. Frontiers
in neuroscience, 10:405, 2016.

[13] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, and
K. Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer pa-
rameters and < 0.5 MB model size. arXiv preprint arXiv:1602.07360,
2016.

[14] D. Joseph Tan, F. Tombari, S. Ilic, and N. Navab. A versatile learning-
based 3d temporal tracker: Scalable, robust, online. In Proceedings of
the IEEE International Conference on Computer Vision, pp. 693–701,
2015.

[15] W. Kehl, F. Manhardt, F. Tombari, S. Ilic, and N. Navab. SSD-6D:
Making RGB-based 3D detection and 6D pose estimation great again.
In IEEE International Conference on Computer Vision, 2017.

[16] H. Kim, S. Leutenegger, and A. J. Davison. Real-time 3D recon-
In European

struction and 6-DOF tracking with an event camera.
Conference on Computer Vision, 2016.

[17] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization.

arXiv preprint arXiv:1412.6980, 2014.

[18] J. H. Lee, T. Delbruck, and M. Pfeiffer. Training deep spiking neural
networks using backpropagation. Frontiers in neuroscience, 10:508,
2016.

[19] W. Li, S. Saeedi, J. McCormac, R. Clark, D. Tzoumanikas, Q. Ye,
Interiornet: Mega-scale

Y. Huang, R. Tang, and S. Leutenegger.

multi-sensor photo-realistic indoor scenes dataset. arXiv preprint
arXiv:1809.00716, 2018.

[20] Y. Li, G. Wang, X. Ji, Y. Xiang, and D. Fox. Deepim: Deep iterative
matching for 6d pose estimation. In European Conference on Computer
Vision, 2018.

[21] C.-K. Liang, L.-W. Chang, and H. H. Chen. Analysis and compensation
IEEE Transactions on Image Processing,

of rolling shutter effect.
17(8):1323–1330, 2008.

[22] P. Lichtsteiner, C. Posch, and T. Delbruck. A 128x128 120 db 15µs
latency asynchronous temporal contrast vision sensor. IEEE journal of
solid-state circuits, 43(2):566–576, 2008.

[23] W. Maass and H. Markram. On the computational power of circuits of
spiking neurons. Journal of computer and system sciences, 69(4):593–
616, 2004.

[24] F. Manhardt, W. Kehl, N. Navab, and F. Tombari. Deep model-based 6d
pose reﬁnement in rgb. In European Conference on Computer Vision,
2018.

[25] F. Manhardt, W. Kehl, N. Navab, and F. Tombari. Deep model-based 6d
pose reﬁnement in rgb. Lecture Notes in Computer Science, p. 833–849,
2018. doi: 10.1007/978-3-030-01264-9 49

[26] A. I. Maqueda, A. Loquercio, G. Gallego, N. Garc´ıa, and D. Scara-
muzza. Event-based vision meets deep learning on steering prediction
for self-driving cars. In IEEE Conference on Computer Vision and
Pattern Recognition, 2018.

[27] I. Marougkas, P. Koutras, N. Kardaris, G. Retsinas, G. Chalvatzaki, and
P. Maragos. How to track your dragon: A multi-attentional framework
for real-time rgb-d 6-dof object pose tracking, 2020.

[28] A. Mitrokhin, C. Ferm¨uller, C. Parameshwara, and Y. Aloimonos.
Event-based moving object detection and tracking. In IEEE/RSJ Inter-
national Conference on Intelligent Robots and Systems, 2018.
[29] E. Mueggler, H. Rebecq, G. Gallego, T. Delbruck, and D. Scaramuzza.
The event-camera dataset and simulator: Event-based data for pose
estimation, visual odometry, and SLAM. The International Journal of
Robotics Research, 36(2):142–149, 2017.

[30] F. Mueller, D. Mehta, O. Sotnychenko, S. Sridhar, D. Casas, and
C. Theobalt. Real-time hand tracking under occlusion from an egocen-
tric RGB-D sensor. In IEEE International Conference on Computer
Vision Workshops, 2017.

[31] R. A. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux, D. Kim, A. J.
Davison, P. Kohi, J. Shotton, S. Hodges, and A. Fitzgibbon. Kinect-
In IEEE
Fusion: Real-time dense surface mapping and tracking.
International Symposium on Mixed and Augmented Reality, 2011.
[32] A. Nguyen, T.-T. Do, D. G. Caldwell, and N. G. Tsagarakis. Real-
time 6-DOF pose relocalization for event cameras with stacked spatial
LSTM networks. In IEEE Conference on Computer Vision and Pattern
Recognition Workshops, 2019.

[33] F. Pomerleau, F. Colas, R. Siegwart, and S. Magnenat. Comparing ICP
variants on real-world data sets. Autonomous Robots, 34(3):133–148,
Feb. 2013.

[34] H. Rebecq, D. Gehrig, and D. Scaramuzza. ESIM: an open event
camera simulator. In Conference on Robotics Learning, 2018.
[35] H. Rebecq, T. Horstschaefer, and D. Scaramuzza. Real-time visual-
inertial odometry for event cameras using keyframe-based nonlinear
optimization. 2017.

[36] H. Rebecq, T. Horstsch¨afer, G. Gallego, and D. Scaramuzza. Evo:
A geometric approach to event-based 6-DOF parallel tracking and
mapping in real time. IEEE Robotics and Automation Letters, 2(2):593–
600, 2016.

[37] H. Rebecq, R. Ranftl, V. Koltun, and D. Scaramuzza. Events-to-
video: Bringing modern computer vision to event cameras. In IEEE
Conference on Computer Vision and Pattern Recognition, 2019.
[38] H. Rebecq, R. Ranftl, V. Koltun, and D. Scaramuzza. High speed and
high dynamic range video with an event camera. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 2019.

[39] C. Scheerlinck, H. Rebecq, D. Gehrig, N. Barnes, R. Mahony, and
D. Scaramuzza. Fast image reconstruction with an event camera. In
IEEE Winter Conference on Applications of Computer Vision, 2020.

[40] A. R. Vidal, H. Rebecq, T. Horstschaefer, and D. Scaramuzza. Hybrid,
frame and event based visual inertial odometry for robust, autonomous
navigation of quadrotors. arXiv preprint arXiv:1709.06310, 2017.

[41] A. R. Vidal, H. Rebecq, T. Horstschaefer, and D. Scaramuzza. Ultimate
SLAM? combining events, images, and IMU for robust visual SLAM
in HDR and high-speed scenarios. IEEE Robotics and Automation
Letters, 3(2):994–1001, 2018.

[42] D. Weikersdorfer, D. B. Adrian, D. Cremers, and J. Conradt. Event-
based 3d slam with a depth-augmented dynamic vision sensor. In IEEE
International Conference on Robotics and Automation, 2014.

[43] J. Xiao, A. Owens, and A. Torralba. Sun3D: A database of big spaces
In IEEE International

reconstructed using SfM and object labels.
Conference on Computer Vision, 2013.

[44] Z. Zhang. A ﬂexible new technique for camera calibration. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence, 22(11):1330–
1334, 2000.

[45] A. Z. Zhu, Z. Wang, K. Khant, and K. Daniilidis. Eventgan: Lever-
aging large scale image datasets for event cameras. arXiv preprint
arXiv:1912.01584, 2019.

[46] A. Z. Zhu, L. Yuan, K. Chaney, and K. Daniilidis. Unsupervised event-
based learning of optical ﬂow, depth, and egomotion. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 989–997, 2019.

