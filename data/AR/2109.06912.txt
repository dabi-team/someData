FAIRSEQ S2: A Scalable and Integrable Speech Synthesis Toolkit

Changhan Wang(cid:63), Wei-Ning Hsu(cid:63), Yossi Adi, Adam Polyak, Ann Lee,
Peng-Jen Chen, Jiatao Gu, Juan Pino
Facebook AI
{changhan,wnhsu,adiyoss,adampolyak,annl,
pipibjc,jgu,juancarabina}@fb.com

Abstract

This paper presents FAIRSEQ S2, a FAIRSEQ
extension for speech synthesis. We implement
a number of autoregressive (AR) and non-AR
text-to-speech models, and their multi-speaker
variants. To enable training speech synthesis
models with less curated data, a number
of preprocessing tools are built and their
importance is shown empirically. To facilitate
faster iteration of development and analysis, a
suite of automatic metrics is included. Apart
from the features added speciﬁcally for this
extension, FAIRSEQ S2 also beneﬁts from the
scalability offered by FAIRSEQ and can be
easily integrated with other state-of-the-art
systems provided in this framework. The
code, documentation, and pre-trained models
are available at https://github.com/
pytorch/fairseq/tree/master/
examples/speech_synthesis.

1

Introduction

Speech synthesis is the task of generating speech
waveforms with desired characteristics, including
but not limited to textual content (Hunt and Black,
1996; Zen et al., 2009; Shen et al., 2018; Ping
et al., 2017; Li et al., 2019), speaker identity (Jia
et al., 2018; Cooper et al., 2020), and speaking
styles (Wang et al., 2018; Skerry-Ryan et al., 2018;
Akuzawa et al., 2018; Hsu et al., 2018). It is also
more often referred to as Text-to-Speech (TTS)
when text is used as input to the system. Along with
automatic speech recognition (ASR) and machine
translation (MT), these language technologies have
advanced rapidly over the past few years (Tan
et al., 2021). Traditionally, these tasks may be
used in conjunction to form a system (e.g., com-
bining the three for speech-to-speech translation),
but they rarely leverage each other during train-
ing. As a result, each application used to have its
own dedicated open-source toolkit, for example,

(cid:63)

Equal contribution.

Kaldi (Povey et al., 2011) and HTK (Young et al.,
2002) for ASR, HTS (Zen et al., 2007), Merlin (Wu
et al., 2016), STRAIGHT (Kawahara et al., 1999),
and WORLD (Morise et al., 2016) for speech syn-
thesis, and Moses (Koehn et al., 2007) for MT.

Recently, there are growing interactions among
these systems in the learning process. For example,
Hayashi et al. (2018) and Rosenberg et al. (2019)
propose to leverage speech synthesis systems to
generate paired text and speech data for ASR train-
ing; Tjandra et al. (2017), Hori et al. (2019), and
Baskar et al. (2019) chain ASR and TTS together
to form a loop for semi-supervised learning with
cycle-consistency loss; Weiss et al. (2017), Li et al.
(2020), and Jia et al. (2019) demonstrate that it is
possible to build an end-to-end system translating
speech into text or speech in a target language.

Beyond text-based systems, there is also an
emerging research topic that explores the use of
units discovered from self-supervised speech rep-
resentation learning (Oord et al., 2017; Baevski
et al., 2019; Harwath et al., 2019; Hsu et al., 2021)
to replace text for representing the lexical con-
tent in numerous applications, such as language
modeling (Lakhotia et al., 2021), speech resynthe-
sis (Polyak et al., 2021), image captioning (Hsu
et al., 2020), and translation (Tjandra et al., 2020;
Hayashi and Watanabe, 2020). This line of research
bypasses the need for text and makes technologies
applicable even to unwritten languages. However,
to interpret the output of such systems - a sequence
of learned units, a unit-to-speech model is required.
This brings up the need of a framework for broader
speech synthesis systems that can alternatively take
learned units as input. These research directions
can beneﬁt from having a single toolkit with differ-
ent state-of-the-art language technologies.

In this paper, we introduce FAIRSEQ S2, a
FAIRSEQ (Ott et al., 2019) extension for speech
synthesis. FAIRSEQ is a popular open-source se-
quence modeling toolkit based on PyTorch (Paszke

1
2
0
2

p
e
S
4
1

]
S
A
.
s
s
e
e
[

1
v
2
1
9
6
0
.
9
0
1
2
:
v
i
X
r
a

 
 
 
 
 
 
et al., 2019) that allows researchers and developers
to train custom models. It offers great support for
training large models on large scale data, and pro-
vides a number of state-of-the-art models for lan-
guage technologies. We extend FAIRSEQ to support
speech synthesis in this work. In particular, we im-
plement a number of popular text-to-spectrogram
models, with interface to both signal processing-
based and neural vocoders. Multi-speaker vari-
ants of those models are also implemented. While
speech synthesis often relies on subjective metrics
such as mean opinion scores for benchmarking,
we implemented a suite of widely used automatic
evaluation metrics to facilitate faster iteration on
model development. Last but not least, we support
a number of text and audio preprocessing mod-
ules, which allow developers to quickly build a
new dataset from less curated in-the-wild data for
speech synthesis.

The main contribution of this work is threefold.
First, we implement a number of state-of-the-art
models and provide pre-trained checkpoints and
recipes, which can be used by researchers as base-
lines or as building blocks in applications such
as text-to-speech translation. Second, we create
pre-processing tools that enable developers to use
customized data to build a TTS model, and demon-
strate the effectiveness of these tools empirically.
Lastly, as part of the FAIRSEQ codebase, this speech
synthesis extension allows easy integration with nu-
merous state-of-the-art MT, ASR, ST, LM, and self-
supervised systems already built on FAIRSEQ. We
provide an example by building a unit-to-speech
system that can be used for text-free research.

The rest of the paper is organized as follows:
Section 2 describes the features of FAIRSEQ S2.
Experiments are presented in Section 3. Related
work is discussed in Section 4, and we conclude
this work in Section 5.

2 Features

Fairseq Models
FAIRSEQ provides a collection
of MT (Ng et al., 2019), ST (Wang et al., 2020), un-
supervised speech pre-training and ASR (Baevski
et al., 2020b; Hsu et al., 2021) models that demon-
strate state-of-the-art performance on standard
benchmarks. They are open-sourced with pre-
trained checkpoints and can be integrated or ex-
tended easily for other tasks.

Speech Synthesis Extension FAIRSEQ S2 adds
state-of-the-art text-to-spectrogram prediction mod-

els, Tacotron 2 (Shen et al., 2018) and Trans-
former (Li et al., 2019), which are AR with encoder-
decoder model architecture. For the latest advance-
ments on fast non-AR modeling, we provide Fast-
Speech 2 (Ren et al., 2019, 2020) as an example.
All our models support the multi-speaker setting
via pre-trained (Jia et al., 2018) or jointly trained
speaker embeddings (Arik et al., 2017; Chen et al.,
2020). Note that the former enables synthesizing
speech for speakers unseen during training. For
FastSpeech 2, pitch and speed are controllable
during inference. For spectrogram-to-waveform
conversion (vocoding), FAIRSEQ S2 has a built-in
Grifﬁn-Lim (Grifﬁn and Lim, 1984) vocoder for
fast model-free generation.
It also provides ex-
amples for using external model-based vocoders,
such as WaveGlow (Prenger et al., 2019) and HiFi-
GAN (Kong et al., 2020).

Speech Preprocessing. Recent advances in neu-
ral generative models have demonstrated that
neural-based TTS models, can synthesize high-
quality, natural and intelligible speech. However,
such models usually require high-quality, and clean
speech data (Zhang et al., 2021). In order to enable
leveraging noisy data for TTS training, we propose
a speech preprocessing pipeline to enhance and
ﬁlter data. The proposed pipeline is comprised
of three main components: i) Background noise
removal, ii) Voice Activity Detector (VAD), and
iii) Outlier ﬁltering using both Signal-to-Noise Ra-
tio (SNR) and Character Error Rate (CER).

First, a speech enhancement model is applied
over input recordings to remove background noise.
We used the speech enhancement model proposed
by (Defossez et al., 2020) where the ith convo-
lutional layer has 2i−1 ∗ 64 output channels. As
suggested by the authors, we additionally used a
dry/wet knob, i.e. the ﬁnal output is dry · x + (1 −
dry) · ˆy, where x is the noisy input signal and ˆy is
the output of the enhancement model. We exper-
iment with dry ∈ {0.0, 0.01, 0.05, 0.1} and ﬁnd
0.01 to perform the best.

Next, we apply VAD to remove silence from the
denoised utterances, as silence can vary in length
signiﬁcantly which causes increasing uncertainty
and therefore degrades TTS performance. Silence
regions at the beginning and end of the utterances
In case we encounter
are completely removed.
a silence segment in the middle of the signal in
where its length is greater than 300ms we replace
it with a 300ms artiﬁcially generated silence (since

completely removing silence regions produces un-
natural speech). Silence regions of less than 300ms
are left unchanged. We use the open-source imple-
mentation of the Google WebRTC VAD (Wiseman,
2016), of which four aggressiveness levels {0, 1,
2, 3} can be set. A higher aggressiveness level
removes more silences but comes at the risk of re-
moving partial speech. The aggressiveness level
corresponds to the size of the processing window
(a larger processing window will make the VAD
work at a coarser level and remove silence frames
more aggressively).

Lastly, we notice that in extremely noisy record-
ings (SNR close to zero), the generated denoised
samples are often not intelligible enough to train
a TTS or contain distortion artifacts. In addition,
when setting the VAD aggressiveness level high,
speech may be truncated along with silence. To
remedy this, we proposed two outliers ﬁltering
methods. The ﬁrst approach is based on SNR es-
timation. We approximate the noise by subtract-
ing the output of the enhancement model from the
input-noisy speech, then we compute the SNR be-
tween the two. The second approach is based on
applying an Automatic Speech Recognition (ASR)
over the denoised speech and compute the CER
against the target transcription.

Computation FAIRSEQ is implemented in Py-
Torch (Paszke et al., 2019) and provides efﬁcient
batching, gradient accumulation, mixed precision
training (Micikevicius et al., 2017), model paral-
lelism, multi-GPU as well as multi-machine train-
ing for computational efﬁciency on large-scale ex-
periments and enabling training gigantic models.

Quantitative Metrics We provide automatic
metrics for fast evaluation in model development.
Similarly to (Polyak et al., 2020), we report Gross
Pitch Error (GPE) (Nakatani et al., 2008), Voicing
Decision Error (VDE) (Nakatani et al., 2008), and
F0 Frame Error (FFE) (Chu and Alwan, 2009)
to evaluate F0 reconstructions of the generated
speech. We additionally, report Mel Cepstral Dis-
tortion (MCD), Mel Spectral Distortion (MSD),
and CER to evaluate both the overall similarity to
the target speech and content intelligibility (Weiss
et al., 2021).

pitch error of more than 20%.

GPE(p, ˆp, v, ˆv) =

(cid:80)
t

(1)

(cid:80)
t

1[|pt − ˆpt| > 0.2 · pt]1[vt]1[ˆvt]
1[vt]1[v(cid:48)
t]
where pt, p(cid:48)
t are the pitch frames from the target
and generated signals, vt, ˆvt are the voicing deci-
sions from the target and generated signals, and 1
is the indicator function.

(ii) VDE VDE measures the portion of frames
with voicing decision error,

VDE(v, ˆv) =

(cid:80)T −1
t=1

1[vt (cid:54)= ˆvt]
T

,

(2)

where T is the total number of frames.

(iii) FFE Combining GPE and VDE, FFE mea-
sures the percentage of frames that contain a de-
viation of more than 20% in pitch value or have a
voicing decision error.

FFE(p, ˆp, v, ˆv) = VDE(v, ˆv)

(cid:80)T −1
t=1

+

1[|pt − ˆpt| > 0.2 · pt]1[vt]1[ˆvt]

T

.

(3)

(iv) MCD/MSD These are deﬁned as the root
mean squared error of the synthesized speech
against the reference speech computed on the 13-
dimensional MFCC features for MCD and log-mel
spectral features for MSD. Since the reference and
the synthesized speech may not be aligned frame-
by-frame, instead of zero-padding the shorter one
and assuming they are frame-wise aligned as done
in Skerry-Ryan et al. (2018), we follow Weiss et al.
(2021) and use dynamic time warping (Berndt and
Clifford, 1994) to align the frames from the two
sequences. The main difference between these two
metrics lies in the features they compute distortion
on: MFCC features aim to capture phonetic infor-
mation while removing speaker information, while
log-mel spectral features encode both, and hence
MCD addresses phonetic similarity more.

(v) CER CER is computed between the transcrip-
tion of the generated audio against the input text us-
ing an ASR system publicly available in FAIRSEQ.

Visualization FAIRSEQ integrates Tensorboard1
for monitoring holistic metrics during model train-
ing. It also has VizSeq (Wang et al., 2019) inte-
gration for ofﬂine sequence-level error analysis,

(i) GPE GPE is an objective metric which mea-
sures the portion of voiced audio frames with a

1https://github.com/tensorflow/

tensorboard

MCD

CER (S/D/I)

MOS

Audio Preprocessing Hours CER (dev)

Orig. Audio

TFM

FS2

Char
g2pE
espk
Unit

g2pE
Unit

-

4.1
3.8
4.4
3.4

3.8
3.4

3.3 (0.2/0.5/2.5)

4.53±0.05

4.4 (0.8/0.8/2.8)
5.0 (1.1/1.2/2.7)
3.8 (0.5/0.6/2.7)
5.7 (1.4/1.3/3.1)

4.09±0.06
4.18±0.06
4.17±0.06
4.18±0.05

4.9 (1.2/0.9/2.8)
7.6 (2.6/1.8/3.2)

4.15±0.09
3.99±0.05

Table 1: Evaluation on LJSpeech. We compare au-
toregressive model (“TFM") with non-autoregressive
model (“FS2"), as well as 3 different types of inputs:
characters (“char"), phonemes (“g2pE" and “espk")
and HuBERT units (“unit").

where transcript and target/predicted speech are vi-
sualized in Jupyter Notebook interface. FAIRSEQ
S2 further adds generated spectrogram and wave-
form samples to Tensorboard for model debugging.

3 Experiments

We evaluate our models in three settings: single-
speaker synthesis, multi-speaker synthesis and
multi-speaker synthesis using noisy data.

3.1 Experimental Setup

We use either characters, phonemes or discovered
units as input representations. To convert texts into
phonemes, we employ g2pE (Park, 2019) or Phone-
mizer (Bernard, 2015) with espeak-ng1 backend.
We use the Montreal Forced Aligner (McAuliffe
et al., 2017) to obtain phonemes with frame du-
rations for FastSpeech 2 training, which is based
on the same pronunciation dictionary (CMUdict)
as g2pE. For discovered units, we extract frame-
level units using a Base HuBERT model trained
on LibriSpeech1 and collapse consecutive units of
the same kind. We use the run length of identi-
cal units before collapsing as target duration for
FastSpeech 2 training. We use a reduction factor
(number of frames each decoder step predicts) of 4
for Transformer and 1 for FastSpeech 2 by default.
We resample audios to 22,050Hz and extract
log-Mel spectrogram with FFT size 1024, window
length 1024 and hop length 256. We optionally pre-
process audios to improve model training: denois-
ing (“DN"), level-2 or level-3 VAD (“VAD-2" or
"VAD-3"), ﬁltering by SNR> 15 and CER< 10%

1https://github.com/espeak-ng/

espeak-ng

Raw
DN+VAD-1
DN+VAD-2
DN+VAD-3
DN+VAD-3 + FLT

44
33
32
26
20

0.8
1.0
1.2
6.8
1.6

Table 2: Audio preprocessing settings on VCTK. FLT
removes samples with CER > 10%.

(“FLT") and volume normalization (“VN").

We use MCD and CER for automatic evalua-
tion. MCD is computed on Grifﬁn-Lim vocoded
reference and model output spectrograms. We use
vocoded references as opposed to the original ones
to eliminate the error introduced by the vocoder
and focus the evaluation on spectrogram predic-
tion. HiFiGAN vocoders trained on each dataset
are used to generate waveforms for CER evaluation.
The large wav2vec 2.0 (Baevski et al., 2020a) ASR
model, which achieves WERs of 1.8% and 3.3% on
Librispeech test-clean and test-other, respectively
and is provided in FAIRSEQ1, is used both for CER
ﬁltering and evaluation. GPE, VDE, and FFE are
not reported here, because these metrics are more
meaningful when prosody modeling is taken into
account (Polyak et al., 2020; Skerry-Ryan et al.,
2018; Wang et al., 2018). For subjective evaluation,
we conduct a Mean Opinion Score (MOS) test us-
ing the CrowdMOS package (Ribeiro et al., 2011)
using the recommended recipes for detecting and
discarding inaccurate scores. We randomly sample
100 speech utterances from the test set and collect
manual scores using a crowd sourcing framework.
The same samples are used across all tested meth-
ods. Each sample is rated by at least 10 raters
on a scale from 1 to 5 with 1.0 point increments.
Overall, scores for each tested method are averaged
across more than 1000 manual annotations. We re-
port both average MOS scores together with a 95%
conﬁdence interval (CI95).

3.2 Single-Speaker Synthesis on LJSpeech

LJSpeech (Ito and Johnson, 2017) is a single-
speaker TTS corpus with 13,100 English speech
samples (around 24 hours) from audiobooks. We
follow the setting in Ren et al. (2020) to use 349
samples (with document title LJ003) for valida-
tion, 523 samples (with document title LJ001 and
LJ002) for testing and the rest for training.

1https://dl.fbaipublicfiles.com/

hubert/hubert_base_ls960.pt

1https://dl.fbaipublicfiles.com/
fairseq/wav2vec/wav2vec_vox_960h_pl.pt

Audio

Spk.

Red. MCD CER

MOS

the rest for training.

Original

-

Raw LUT

DN+VAD-3

LUT

DN+VAD-3

+FLT

LUT

Emb

-

2
4

2
4

2
4

2
4

-

4.9
3.3

3.6
3.4

3.6
3.4

3.6
3.5

1.8

4.27±0.07

65.2
12.1

1.77±0.08
2.77±0.08

9.8
6.9

9.7
6.0

7.6
5.8

3.34±0.06
3.30±0.06

3.38±0.06
3.42±0.05

3.38±0.06
3.25±0.08

Table 3: Evaluation on VCTK. We use Trans-
former with character inputs, and compare 3 audio pre-
processing settings and 2 types of speaker embeddings.

Audio MCD CER

MOS

Original

VN
DN+VAD-2+VN
DN+VAD-2+FLT+VN

-

5.5
5.6
5.6

3.0

19.6
9.8
9.2

4.0±0.06

2.97±0.08
3.22±0.07
3.17±0.06

Table 4: Evaluation on Common Voice English por-
tion (top 200 speakers only). We use Transformer
model with phoneme (g2pE) inputs and compare 3 au-
dio preprocessing settings.

On this de-facto standard benchmark, we com-
pare autoregressive model (Transformer, “TFM")
with non-autoregressive model (FastSpeech 2,
“FS2"), as well as 3 different types of inputs: char-
acters, phonemes (from g2pE or espeak-ng) and
HuBERT units. We see from Table 1 that Fast-
Speech 2 performs comparably well to Transformer
with phoneme inputs (g2pE), both achieving 4.2
MOS. However, the latter does not require input-
output alignments for model training and supports
more types of inputs—it achieves 4.1 MOS with
characters (no need for phonemization), and 4.2
MOS with simpler phonemes (espeaker-ng). The
task falls into the re-synthesis setting with unit in-
puts. We notice that FastSpeech 2 performs worse
(4.0 vs. 4.2 on MOS) in this setting, likely due to
the ﬁner-grained inputs and its simpliﬁed attention
mechanism.

3.3 Multi-Speaker Synthesis on VCTK

VCTK (Veaux et al., 2017) is a multi-speaker En-
glish TTS dataset that contains 44 hours of read
speech from 109 speakers with various English
accents1. We randomly sample 50 utterances for
validation and 100 utterances for testing, and use

1https://datashare.ed.ac.uk/handle/

10283/3443

Speech recordings from VCTK include consid-
erable amount of silence as shown in Figure 1
(raw); therefore, silence removal is considered a
standard preprocessing step for VCTK (Jia et al.,
2018; Cooper et al., 2020). Figure 1 shows silence-
removed spectrograms with three VAD aggressive-
ness levels. We see that a higher aggressiveness
level removes more silence, but may also truncate
the speech. The dataset durations after silence re-
moval and ﬁltering with CER < 10% are listed in
Table 2, along with the validation CER.

We use this dataset

to study how audio-
preprocessing and speaker representation affect
the performance of TTS. We train a transformer
TTS model with a reduction factor (i.e. how many
frames each decoding step predicts) of 2 or 4 on
three sets of audio: raw data (Raw), DN+VAD-
3, and DN+VAD-3+FLT. A speaker embedding
lookup table (LUT) is used by default. In addition,
we train models on DN+VAD-3+FLT with a ﬁxed
embedding (Emb) for each speaker inferred from
a pre-trained speaker veriﬁcation model (Heigold
et al., 2016), which would enable synthesizing the
voice of an unseen speaker. Results in Table 3 show
that increasing the reduction factor from 2 to 4 im-
proves the performance consistently. Speciﬁcally,
we found that without VAD, the model fails to train
when using a reduction factor of 2. Finally, we
found that using a pre-trained speaker embedder
achieves similar performance than using a learnable
lookup table, while enabling synthesizing speech
for unseen speakers.

3.4 Multi-Speaker Synthesis using Noisy

Data from Common Voice

Common Voice (Ardila et al., 2020) is a multi-
speaker speech corpus with around 4.2K hours of
read speech in 40 languages (version 4). It is crowd-
sourced from around 78K voice contributors in
various accents, age groups and genders. We use
its English portion and select data from the top 200
speakers by duration (total 226 hours).

The audio data in this corpus is expectedly noisy
given the lack of curated recording environments.
We explore if speech processing can counteract
the negative factors (background noise, long si-
lence, variable volume across clips, etc.) during
recordings and improve model training. Speciﬁ-
cally, we examine 3 preprocessing settings with
Transformer model and phoneme (g2pE) inputs:

Multi-Spk Non-AR

TTS

TTS

ASR MT

ST

Speech
Pre-training

Audio

Auto.
Preprocess Metrics

coqui TTS1
OpenSeq2seq2†
ESPnet-TTS3
NeMo4

FAIRSEQ S2

(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)
(cid:88)
(cid:88)

(cid:88)

(cid:88)
(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)‡

(cid:88)

(cid:88)‡

(cid:88)

Table 5: Comparison of FAIRSEQ S2 with counterpart speech synthesis toolkits (as of June 2021). 1 GitHub:
coqui-ai/TTS. 2 Kuchaiev et al. (2018a). 3 Hayashi et al. (2020). 4 GitHub: NVIDIA/NeMo. † Archived and no
longer updated. ‡ Supporting only VAD for audio preprocessing and MCD for automatic metric.

Figure 1: A VCTK example. With VAD level 3, the ﬁrst word “But” is detected as silence and cut off.

VN, DN+VAD-2+VN and DN+VAD-2+FLT+VN.
As shown in Table 4, the original audio has 0.3/0.5
lower MOS than the LJSpeech/VCTK one, con-
ﬁrming its relatively low recording quality. Noise
and silence removal improve synthesis quality sig-
niﬁcantly by 0.2 MOS (DN+VAD-2+* vs. VN).
Filtering by SNR and CER improves both model
ﬁtting (-0.1 MCD) and intelligibility (-1.5 CER)
given the removal of difﬁcult training examples.

4 Related Work

There are many existing open-source repositories
for speech synthesis. The most prominent toolkits
for conventional statistical parametric speech syn-
thesis (SPSS) include HMM/DNN-based Speech
Synthesis System (HTS) (Zen et al., 2007) and
Merlin (Wu et al., 2016). These rely heavily on fea-
ture engineering and use signal processing-based
vocoders like STRAIGHT (Kawahara et al., 1999)
and WORLD (Morise et al., 2016) to synthesize
waveforms from acoustic features (e.g., fundamen-
tal frequency, spectral envelope, and aperiodic in-
formation). Recently, end-to-end models that take
minimally pre-processed features (characters and
mel-spectrograms) have achieved superior perfor-
mance compared to conventional systems (Shen
et al., 2018), especially when paired with neural
vocoders (Prenger et al., 2019; Kong et al., 2020).
There are a number of open-source implementa-

tions available on Github 1, however, these reposi-
tories are solely for text-to-speech synthesis, and
mostly support one model only.

ESPnet (Watanabe et al., 2018; Hayashi et al.,
2020), NeMo, and OpenSeq2Seq (Kuchaiev et al.,
2018b) are the most similar toolkits that also sup-
port multiple tasks. As listed in Table 5, FAIRSEQ
S2 provides more audio preprocessing tools and au-
tomatic metrics for building and evaluating speech
synthesis models on custom datasets. As part of
FAIRSEQ, it can also be easily integrated with nu-
merous state-of-the-art models already provided
in FAIRSEQ for exploring novel ideas. For exam-
ple, we demonstrate that units discovered from a
self-supervised speech pre-training model can be
used to build a unit-to-speech system that converts
output from systems like unit LM (Lakhotia et al.,
2021) or image-to-unit (Hsu et al., 2020) to speech.

5 Conclusion

This paper introduces FAIRSEQ S2, a FAIRSEQ ex-
tension for speech synthesis. We believe this exten-
sion will allow researchers and developers to more
easily test novel ideas for language technologies by
providing great support for scalability, integrability,
and a wealth of tools for curating data as well as
automatically evaluating trained systems.

1coqui-ai/TTS, Kyubyoung/tacotron, NVIDIA/tacotron2,

Rayhane-mamah/Tacotron2, r9y9/deepvoice3_pytorch

References

Kei Akuzawa, Yusuke Iwasawa, and Yutaka Matsuo.
2018. Expressive speech synthesis via modeling
arXiv
expressions with variational autoencoder.
preprint arXiv:1804.02135.

Rosana Ardila, Megan Branson, Kelly Davis, Michael
Kohler, Josh Meyer, Michael Henretty, Reuben
Morais, Lindsay Saunders, Francis Tyers, and Gre-
gor Weber. 2020. Common voice: A massively-
In Proceedings of the
multilingual speech corpus.
12th Language Resources and Evaluation Confer-
ence, pages 4218–4222, Marseille, France. Euro-
pean Language Resources Association.

Sercan Arik, Gregory Diamos, Andrew Gibiansky,
John Miller, Kainan Peng, Wei Ping, Jonathan
Raiman, and Yanqi Zhou. 2017. Deep voice 2:
Multi-speaker neural text-to-speech. arXiv preprint
arXiv:1705.08947.

Alexei Baevski, Steffen Schneider, and Michael Auli.
vq-wav2vec: Self-supervised learning of
arXiv preprint

2019.
discrete speech representations.
arXiv:1910.05453.

Alexei Baevski, Henry Zhou, Abdelrahman Mohamed,
and Michael Auli. 2020a. wav2vec 2.0: A frame-
work for self-supervised learning of speech represen-
tations. arXiv preprint arXiv:2006.11477.

Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,
and Michael Auli. 2020b. wav2vec 2.0: A frame-
work for self-supervised learning of speech represen-
tations. In Advances in Neural Information Process-
ing Systems, volume 33, pages 12449–12460. Cur-
ran Associates, Inc.

Murali Karthick Baskar, Shinji Watanabe, Ramon
Astudillo, Takaaki Hori, Lukáš Burget, and Jan
ˇCernock`y. 2019.
Semi-supervised sequence-to-
sequence asr using unpaired speech and text. arXiv
preprint arXiv:1905.01152.

Mathieu Bernard. 2015.

Phonemizer. https://

github.com/bootphon/phonemizer.

Donald J Berndt and James Clifford. 1994. Using dy-
namic time warping to ﬁnd patterns in time series. In
KDD workshop, volume 10, pages 359–370. Seattle,
WA, USA:.

Mingjian Chen, Xu Tan, Yi Ren, Jin Xu, Hao Sun,
Sheng Zhao, Tao Qin, and Tie-Yan Liu. 2020. Mul-
tispeech: Multi-speaker text to speech with trans-
former. arXiv preprint arXiv:2006.04664.

Erica Cooper, Cheng-I Lai, Yusuke Yasuda, Fuming
Fang, Xin Wang, Nanxin Chen, and Junichi Ya-
magishi. 2020.
Zero-shot multi-speaker text-to-
speech with state-of-the-art neural speaker embed-
In ICASSP 2020-2020 IEEE International
dings.
Conference on Acoustics, Speech and Signal Pro-
cessing (ICASSP), pages 6184–6188. IEEE.

Alexandre Defossez, Gabriel Synnaeve, and Yossi Adi.
2020. Real time speech enhancement in the wave-
form domain. INTERSPEECH.

Daniel Grifﬁn and Jae Lim. 1984. Signal estimation
IEEE
from modiﬁed short-time fourier transform.
Transactions on acoustics, speech, and signal pro-
cessing, 32(2):236–243.

David Harwath, Wei-Ning Hsu, and James Glass.
2019. Learning hierarchical discrete linguistic units
arXiv preprint
from visually-grounded speech.
arXiv:1911.09602.

Tomoki Hayashi and Shinji Watanabe. 2020. Disc-
retalk: Text-to-speech as a machine translation prob-
lem. arXiv preprint arXiv:2005.05525.

Tomoki Hayashi, Shinji Watanabe, Yu Zhang, Tomoki
Toda, Takaaki Hori, Ramon Astudillo, and Kazuya
Takeda. 2018. Back-translation-style data augmen-
tation for end-to-end asr. In 2018 IEEE Spoken Lan-
guage Technology Workshop (SLT), pages 426–433.
IEEE.

Tomoki Hayashi, Ryuichi Yamamoto, Katsuki Inoue,
Takenori Yoshimura, Shinji Watanabe, Tomoki Toda,
Kazuya Takeda, Yu Zhang, and Xu Tan. 2020.
Espnet-tts: Uniﬁed,
reproducible, and integrat-
able open source end-to-end text-to-speech toolkit.
In ICASSP 2020-2020 IEEE International Confer-
ence on Acoustics, Speech and Signal Processing
(ICASSP), pages 7654–7658. IEEE.

Georg Heigold, Ignacio Moreno, Samy Bengio, and
Noam Shazeer. 2016. End-to-end text-dependent
In 2016 IEEE International
speaker veriﬁcation.
Conference on Acoustics, Speech and Signal Pro-
cessing (ICASSP), pages 5115–5119. IEEE.

Takaaki Hori, Ramon Astudillo, Tomoki Hayashi,
Yu Zhang, Shinji Watanabe, and Jonathan Le Roux.
2019. Cycle-consistency training for end-to-end
In ICASSP 2019-2019 IEEE
speech recognition.
International Conference on Acoustics, Speech and
Signal Processing (ICASSP), pages 6271–6275.
IEEE.

Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hu-
bert Tsai, Kushal Lakhotia, Ruslan Salakhutdi-
nov, and Abdelrahman Mohamed. 2021. Hubert:
Self-supervised speech representation learning by
masked prediction of hidden units. arXiv preprint
arXiv:2106.07447.

Wei Chu and Abeer Alwan. 2009. Reducing f0 frame
error of f0 tracking algorithms under noisy con-
ditions with an unvoiced/voiced classiﬁcation fron-
tend. In ICASSP.

Wei-Ning Hsu, David Harwath, Christopher Song, and
James Glass. 2020. Text-free image-to-speech syn-
thesis using learned segmental units. arXiv preprint
arXiv:2012.15454.

Wei-Ning Hsu, Yu Zhang, Ron J Weiss, Heiga Zen,
Yonghui Wu, Yuxuan Wang, Yuan Cao, Ye Jia,
Zhifeng Chen, Jonathan Shen, et al. 2018. Hierar-
chical generative modeling for controllable speech
synthesis. arXiv preprint arXiv:1810.07217.

Andrew J Hunt and Alan W Black. 1996. Unit selec-
tion in a concatenative speech synthesis system us-
In 1996 IEEE Inter-
ing a large speech database.
national Conference on Acoustics, Speech, and Sig-
nal Processing Conference Proceedings, volume 1,
pages 373–376. IEEE.

Keith Ito and Linda Johnson. 2017.

The lj
https://keithito.com/

speech dataset.
LJ-Speech-Dataset.

Ye Jia, Ron J Weiss, Fadi Biadsy, Wolfgang Macherey,
Melvin Johnson, Zhifeng Chen, and Yonghui Wu.
2019. Direct speech-to-speech translation with a
Proc. Interspeech
sequence-to-sequence model.
2019, pages 1123–1127.

Ye Jia, Yu Zhang, Ron J Weiss, Quan Wang, Jonathan
Shen, Fei Ren, Zhifeng Chen, Patrick Nguyen,
Ruoming Pang, Ignacio Lopez Moreno, et al. 2018.
Transfer learning from speaker veriﬁcation to mul-
tispeaker text-to-speech synthesis. arXiv preprint
arXiv:1806.04558.

Hideki Kawahara, Ikuyo Masuda-Katsuse, and Alain
De Cheveigne. 1999. Restructuring speech rep-
resentations using a pitch-adaptive time–frequency
smoothing and an instantaneous-frequency-based f0
extraction: Possible role of a repetitive structure in
sounds. Speech communication, 27(3-4):187–207.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
In Pro-
toolkit for statistical machine translation.
ceedings of the 45th annual meeting of the associ-
ation for computational linguistics companion vol-
ume proceedings of the demo and poster sessions,
pages 177–180.

Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. 2020.
Hiﬁ-gan: Generative adversarial networks for efﬁ-
In Ad-
cient and high ﬁdelity speech synthesis.
vances in Neural Information Processing Systems,
volume 33, pages 17022–17033. Curran Associates,
Inc.

Oleksii Kuchaiev, Boris Ginsburg, Igor Gitman, Vi-
taly Lavrukhin, Carl Case, and Paulius Micikevi-
cius. 2018a. Openseq2seq: extensible toolkit for dis-
tributed and mixed precision training of sequence-to-
sequence models. In Proceedings of Workshop for
NLP Open Source Software (NLP-OSS), pages 41–
46.

Oleksii Kuchaiev, Boris Ginsburg, Igor Gitman, Vi-
taly Lavrukhin, Carl Case, and Paulius Micikevicius.

2018b. OpenSeq2Seq: Extensible toolkit for dis-
tributed and mixed precision training of sequence-
In Proceedings of Workshop
to-sequence models.
for NLP Open Source Software (NLP-OSS), pages
41–46, Melbourne, Australia. Association for Com-
putational Linguistics.

Kushal Lakhotia, Evgeny Kharitonov, Wei-Ning Hsu,
Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh
Nguyen, Jade Copet, Alexei Baevski, Adelrahman
Mohamed, et al. 2021. Generative spoken lan-
arXiv preprint
guage modeling from raw audio.
arXiv:2102.01192.

Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, and
Ming Liu. 2019. Neural speech synthesis with trans-
former network. In Proceedings of the AAAI Confer-
ence on Artiﬁcial Intelligence, pages 6706–6713.

Xian Li, Changhan Wang, Yun Tang, Chau Tran,
Yuqing Tang, Juan Pino, Alexei Baevski, Alexis
Conneau, and Michael Auli. 2020. Multilingual
speech translation with efﬁcient ﬁnetuning of pre-
trained models. arXiv preprint arXiv:2010.12829.

Michael McAuliffe, Michaela Socolof, Sarah Mi-
huc, Michael Wagner, and Morgan Sonderegger.
2017. Montreal forced aligner: Trainable text-
In Interspeech, vol-
speech alignment using kaldi.
ume 2017, pages 498–502.

Paulius Micikevicius, Sharan Narang, Jonah Alben,
Gregory Diamos, Erich Elsen, David Garcia, Boris
Ginsburg, Michael Houston, Oleksii Kuchaiev,
Ganesh Venkatesh, et al. 2017. Mixed precision
training. arXiv preprint arXiv:1710.03740.

Masanori Morise, Fumiya Yokomori, and Kenji Ozawa.
2016. World: a vocoder-based high-quality speech
IE-
synthesis system for real-time applications.
ICE TRANSACTIONS on Information and Systems,
99(7):1877–1884.

Tomohiro Nakatani et al. 2008. A method for funda-
mental frequency estimation and voicing decision:
Application to infant utterances recorded in real
acoustical environments. Speech Communication.

Nathan Ng, Kyra Yee, Alexei Baevski, Myle Ott,
Michael Auli, and Sergey Edunov. 2019. Facebook
FAIR’s WMT19 news translation task submission.
In Proceedings of the Fourth Conference on Ma-
chine Translation (Volume 2: Shared Task Papers,
Day 1), pages 314–319, Florence, Italy. Association
for Computational Linguistics.

Aaron van den Oord, Oriol Vinyals, and Koray
Kavukcuoglu. 2017. Neural discrete representation
learning. arXiv preprint arXiv:1711.00937.

Myle Ott, Sergey Edunov, Alexei Baevski, Angela
Fan, Sam Gross, Nathan Ng, David Grangier, and
fairseq: A fast, extensible
Michael Auli. 2019.
In Proceedings of
toolkit for sequence modeling.
NAACL-HLT 2019: Demonstrations.

Jongseok Park, Kyubyong & Kim. 2019.

g2pe.

https://github.com/Kyubyong/g2p.

Adam Paszke, Sam Gross, Francisco Massa, Adam
Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca
Antiga, et al. 2019. Pytorch: An imperative style,
In Ad-
high-performance deep learning library.
vances in neural information processing systems,
pages 8026–8037.

Wei Ping, Kainan Peng, Andrew Gibiansky, Ser-
can O Arik, Ajay Kannan, Sharan Narang, Jonathan
Raiman, and John Miller. 2017. Deep voice 3:
Scaling text-to-speech with convolutional sequence
learning. arXiv preprint arXiv:1710.07654.

Adam Polyak, Yossi Adi,

Jade Copet, Eugene
Kharitonov, Kushal Lakhotia, Wei-Ning Hsu, Ab-
and Emmanuel Dupoux.
delrahman Mohamed,
2021. Speech resynthesis from discrete disentan-
gled self-supervised representations. arXiv preprint
arXiv:2104.00355.

Adam Polyak, Lior Wolf, Yossi Adi, and Yaniv Taig-
man. 2020. Unsupervised cross-domain singing
voice conversion. arXiv preprint arXiv:2008.02830.

Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas
Burget, Ondrej Glembek, Nagendra Goel, Mirko
Hannemann, Petr Motlicek, Yanmin Qian, Petr
Schwarz, et al. 2011. The kaldi speech recogni-
tion toolkit. In IEEE 2011 workshop on automatic
speech recognition and understanding, CONF. IEEE
Signal Processing Society.

Ryan Prenger, Rafael Valle, and Bryan Catanzaro.
2019. Waveglow: A ﬂow-based generative net-
In ICASSP 2019-
work for speech synthesis.
2019 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), pages
3617–3621. IEEE.

Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao,
Zhou Zhao, and Tie-Yan Liu. 2020. Fastspeech
2: Fast and high-quality end-to-end text to speech.
arXiv preprint arXiv:2006.04558.

Yi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao,
Zhou Zhao, and Tie-Yan Liu. 2019. Fastspeech:
Fast, robust and controllable text to speech. In Ad-
vances in Neural Information Processing Systems,
volume 32. Curran Associates, Inc.

Flávio Ribeiro, Dinei Florêncio, Cha Zhang, and
Michael Seltzer. 2011. Crowdmos: An approach for
crowdsourcing mean opinion score studies. In 2011
IEEE international conference on acoustics, speech
and signal processing (ICASSP), pages 2416–2419.
IEEE.

Jonathan Shen, Ruoming Pang, Ron J Weiss, Mike
Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng
Chen, Yu Zhang, Yuxuan Wang, Rj Skerrv-Ryan,
et al. 2018. Natural tts synthesis by condition-
ing wavenet on mel spectrogram predictions.
In
2018 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), pages
4779–4783. IEEE.

RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan
Wang, Daisy Stanton, Joel Shor, Ron Weiss, Rob
Clark, and Rif A Saurous. 2018. Towards end-to-
end prosody transfer for expressive speech synthesis
In international conference on ma-
with tacotron.
chine learning, pages 4693–4702. PMLR.

Xu Tan, Tao Qin, Frank Soong, and Tie-Yan Liu. 2021.
A survey on neural speech synthesis. arXiv preprint
arXiv:2106.15561.

Andros Tjandra, Sakriani Sakti, and Satoshi Nakamura.
2017. Listening while speaking: Speech chain by
In 2017 IEEE Automatic Speech
deep learning.
Recognition and Understanding Workshop (ASRU),
pages 301–308. IEEE.

Andros Tjandra, Sakriani Sakti, and Satoshi Nakamura.
2020. Speech-to-speech translation without text.

Christophe Veaux, Junichi Yamagishi, Kirsten Mac-
Donald, et al. 2017. Superseded-cstr vctk corpus:
English multi-speaker corpus for cstr voice cloning
toolkit.

Changhan Wang, Anirudh Jain, Danlu Chen, and Ji-
atao Gu. 2019. VizSeq: a visual analysis toolkit
In Proceedings of the
for text generation tasks.
2019 Conference on Empirical Methods in Natu-
ral Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP): System Demonstrations, pages
253–258, Hong Kong, China. Association for Com-
putational Linguistics.

Changhan Wang, Yun Tang, Xutai Ma, Anne Wu,
Dmytro Okhonko, and Juan Pino. 2020. Fairseq s2t:
In Pro-
Fast speech-to-text modeling with fairseq.
ceedings of the 1st Conference of the Asia-Paciﬁc
Chapter of the Association for Computational Lin-
guistics and the 10th International Joint Conference
on Natural Language Processing: System Demon-
strations, pages 33–39.

Yuxuan Wang, Daisy Stanton, Yu Zhang, RJ-Skerry
Ryan, Eric Battenberg, Joel Shor, Ying Xiao, Ye Jia,
Fei Ren, and Rif A Saurous. 2018. Style tokens:
Unsupervised style modeling, control and transfer
In International
in end-to-end speech synthesis.
Conference on Machine Learning, pages 5180–5189.
PMLR.

Andrew Rosenberg, Yu Zhang, Bhuvana Ramabhad-
ran, Ye Jia, Pedro Moreno, Yonghui Wu, and Zelin
Wu. 2019. Speech recognition with augmented syn-
In 2019 IEEE Automatic Speech
thesized speech.
Recognition and Understanding Workshop (ASRU),
pages 996–1002. IEEE.

Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki
Hayashi, Jiro Nishitoba, Yuya Unno, Nelson En-
rique Yalta Soplin, Jahn Heymann, Matthew Wies-
Espnet: End-
ner, Nanxin Chen, et al. 2018.
arXiv preprint
to-end speech processing toolkit.
arXiv:1804.00015.

Ron J Weiss, Jan Chorowski, Navdeep Jaitly, Yonghui
Wu, and Zhifeng Chen. 2017.
Sequence-to-
sequence models can directly translate foreign
speech. arXiv preprint arXiv:1703.08581.

Ron J Weiss, RJ Skerry-Ryan, Eric Battenberg,
Soroosh Mariooryad, and Diederik P Kingma. 2021.
Wave-tacotron: Spectrogram-free end-to-end text-to-
speech synthesis. In ICASSP 2021-2021 IEEE Inter-
national Conference on Acoustics, Speech and Sig-
nal Processing (ICASSP), pages 5679–5683. IEEE.

John Wiseman. 2016. Python interface to the webrtc
voice activity detector. https://github.com/
wiseman/py-webrtcvad.

Zhizheng Wu, Oliver Watts, and Simon King. 2016.
Merlin: An open source neural network speech syn-
thesis system. In SSW, pages 202–207.

Steve Young, Gunnar Evermann, Mark Gales, Thomas
Hain, Dan Kershaw, Xunying Liu, Gareth Moore, Ju-
lian Odell, Dave Ollason, Dan Povey, et al. 2002.
The htk book. Cambridge university engineering de-
partment, 3(175):12.

Heiga Zen, Takashi Nose, Junichi Yamagishi, Shinji
Sako, Takashi Masuko, Alan W Black, and Keiichi
Tokuda. 2007. The hmm-based speech synthesis
In SSW, pages 294–299.
system (hts) version 2.0.
Citeseer.

Heiga Zen, Keiichi Tokuda, and Alan W Black. 2009.
Statistical parametric speech synthesis. speech com-
munication, 51(11):1039–1064.

Chen Zhang, Yi Ren, Xu Tan, Jinglin Liu, Kejun
Zhang, Tao Qin, Sheng Zhao, and Tie-Yan Liu.
to speech
2021. Denoispeech: Denoising text
with frame-level noise modeling. In ICASSP 2021-
2021 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), pages
7063–7067. IEEE.

