Improving Sample Efﬁciency in Evolutionary RL Using Off-Policy Ranking

Eshwar S R, 1 Shishir Kolathaya, 1 Gugan Thoppe 1
1 Indian Institute of Science
{eshwarsr, shishirk, gthoppe}@iisc.ac.in

2
2
0
2

g
u
A
2
2

]

G
L
.
s
c
[

1
v
3
8
5
0
1
.
8
0
2
2
:
v
i
X
r
a

Abstract

Evolution Strategy (ES) is a powerful black-box optimization
technique based on the idea of natural evolution. In each of its
iterations, a key step entails ranking candidate solutions based
on some ﬁtness score. For an ES method in Reinforcement
Learning (RL), this ranking step requires evaluating multiple
policies. This is presently done via on-policy approaches: each
policy’s score is estimated by interacting several times with
the environment using that policy. This leads to a lot of waste-
ful interactions since, once the ranking is done, only the data
associated with the top-ranked policies is used for subsequent
learning. To improve sample efﬁciency, we propose a novel
off-policy alternative for ranking, based on a local approxima-
tion for the ﬁtness function. We demonstrate our idea in the
context of a state-of-the-art ES method called the Augmented
Random Search (ARS). Simulations in MuJoCo tasks show
that, compared to the original ARS, our off-policy variant has
similar running times for reaching reward thresholds but needs
only around 70% as much data. It also outperforms the recent
Trust Region ES. We believe our ideas should be extendable
to other ES methods as well.

1

Introduction

In optimization, features of the objective function such as
linearity, convexity, or differentiability often are either non-
existent, unknown, or impossible to detect. An Evolution
Strategy (ES), due to its derivative-free nature, is a go-to
alternative in such scenarios. Salimans et al. (2017) proposed
the ﬁrst competitive ES method in Reinforcement Learning
(RL) settings. However, its effectiveness relies heavily on
several complicated ideas and the usage of neural networks
for parameterizing the policies. Thankfully, the Augmented
Random Search (ARS), a recent ES method by Mania, Guy,
and Recht (2018), showed that such complications are not
needed for a state-of-the-art RL approach. In particular, it
demonstrated that it sufﬁces to work with only deterministic
linear policies. Our work proposes a novel off-policy variant
of the ARS method that has running times comparable to the
original one yet needs only 70% as much data.

The formal motivation for our work is as follows: explore
ES algorithms in RL that minimize the number of agent-
environment interactions while keeping the control policy as
simple as possible. Clearly, this would be of signiﬁcance in
practical RL. A vital example is robotics, where collecting
samples is very expensive since the process involves active

calibration, maintenance, and safety checks for every compo-
nent, and overlooking these can result in unsafe behaviours.
Under these constraints, any improvement in sampling efﬁ-
ciency would be considered signiﬁcant.

We now provide an overview of the ES philosophy. As
the name suggests, ES is an iterative optimization framework
inspired by evolution; see (Li et al. 2020) for a recent survey.
In each iteration, an ES method i.) obtains a bunch of can-
didate solutions from some sampling distribution, ii.) ranks
them using some ﬁtness score based on the objective func-
tion, and iii.) uses the top-ranked ones to update the sampling
distribution for use in the next iteration.

In a typical ES method for RL, a candidate solution is
a speciﬁc policy, while its ﬁtness is the associated value
function. Existing techniques, including ARS, use an on-
policy approach to ﬁnd this ﬁtness score: interact several
times with the environment using the policy whose score
needs to be estimated. Since multiple policies need to be
ranked in each iteration, current ES approaches end up with
signiﬁcantly high interactions. Notably, most of this data is
discarded in each iteration except those related to the top-
ranked policies. Our proposed ARS variant improves sample
efﬁciency by replacing this wasteful approach with an off-
policy alternative. We remark that this is the ﬁrst ES method
in RL, where an off-policy approach is used for ranking.

Our ranking technique improves sample complexity due
to two novel features: i.) the ﬁtness function choice and ii.)
the use of a kernel approximation to estimate the same.

Fitness Function: Instead of using the value function η(˜π)
of a candidate policy ˜π as its ﬁtness score, our approach em-
ploys its approximation Lπ(˜π) deﬁned in terms of a different
policy π called the behavior policy (Zhang and Ross 2021,
(6)). Indeed, any ranking of policies based on an approxima-
tion to the value function is going to be sub-optimal. However,
it is also the key factor that enables off-policy ranking. As
we shall see, the data generated by the single policy π can
now be used to rank all the candidate policies!

Kernel Approximation: Lπ(˜π) is estimated in (Zhang
and Ross 2021) via importance sampling (see (40) in ibid).
This approach works only when both π and ˜π are stochastic.
However, the sample efﬁciency of ARS crucially relies on the
candidate policies being deterministic. Therefore, simply us-
ing stochastic policies in ARS to incorporate the importance
sampling idea isn’t ideal for sample efﬁciency. To circumvent

 
 
 
 
 
 
this issue, we propose to alternatively smooth the determinis-
tic policies using a suitable kernel function. This approach
is loosely inspired from (Kallus and Zhou 2018; Kallus and
Uehara 2020), which studies extending ideas from discrete
contextual bandit settings to the continuous case.

The role of Lπ(˜π) in (Zhang and Ross 2021) and our
work is quite different. There it is used to approximate an
intermediate objective function within a policy improvement
scheme, i.e., Lπ(˜π) plays a direct role in their update rule
(see (13) in ibid). Here, instead, we use it only for coarsely
ranking multiple candidate policies in a sample-efﬁcient fash-
ion. In other words, Lπ(˜π) needs to be accurately estimated
in (Zhang and Ross 2021), while a rough estimate sufﬁces
for us. Importantly, since the policies there are stochastic, a
complex neural network is additionally needed for improving
their sample efﬁciency. This complication is avoided here
because our ˜π’s and π are all deterministic.

Key Contributions: We now summarize the main high-

lights of this work.

1. We propose a novel off-policy variant of ARS; see Algo-
rithm 1. Speciﬁcally, we replace the wasteful on-policy
ranking step with an efﬁcient off-policy version. This
is the ﬁrst usage of an off-policy ranking in an ES. Also,
while we apply it only for ARS here, the proposed method
can be extended to other ES methods.

2. Our simulations on benchmark MuJoCo locomotion tasks
(Section 5) show that our method reaches reward thresh-
olds in running times comparable to the original ARS. Fur-
ther, it needs only 70% as much data. This is signiﬁcant
when getting data about interactions with the environment
is either hard or expensive, e.g., robotics.

3. We also do sensitivity to seed and hyperparameter tests
similar to (Mania, Guy, and Recht 2018). Our results are
similar in spirit to what was obtained in the ARS case.
That is, the median trajectory crosses the reward threshold
in most environments, conﬁrming that our algorithm is
robust to seed and hyperparameter choices.

2 Relevant Background and Research Gap
We provide here a brief overview of some of the important
advances in RL relevant to our work. At the end, we also
describe the current research gap that our work addresses.

While RL has been around since the 1950s, it is only in the
last decade—thanks to cheap computing and deeplearning
advances—that we have seen any evidence of human and
superhuman level performances. The pioneering work here
was (Mnih et al. 2015) which proposed the Deep Q Network
(DQN). This algorithm combines the popular Q-learning ap-
proach with a class of artiﬁcial neural networks. In particular,
this was the ﬁrst work to demonstrate that instabilities due
to nonlinear function approximators can be handled effec-
tively. A major issue with DQN though is that it can only
handle discrete and low-dimensional action spaces. The semi-
nal Deep Deterministic Policy Gradient (DDPG) algorithm in
(Lillicrap et al. 2016) was proposed to overcome this issue.
In a different direction, Schulman et al. (2015) proposed
a policy iteration algorithm in the discounted reward setting
called Trust Region Policy Optimization (TRPO). This was

recently generalized to the average reward case by Zhang and
Ross (2021). Both these methods, in each iteration, repeatedly
try to identify a better policy in the neighborhood of the
current estimate. The difﬁculty, however, is that the value
functions of neighbourhood policies are unknown. Schulman
et al. (2015) and Zhang and Ross (2021) resolve this by
instead ﬁnding a policy that is best with respect to a local
approximation of the value-function such as Lπ(˜π). In the
Atari domain, TRPO outperforms previous approaches on
some of the games. More recently, Schulman et al. (2017)
introduced the Proximal Policy Optimization (PPO) method.
This is a descendant of TRPO, which is easier to implement
and has a better sample complexity than previous approaches.
On the ES literature side, the ﬁrst competitive RL method
was the one given in (Salimans et al. 2017). It is a derivative-
free optimization method with similar or better sample efﬁ-
ciency than TRPO in most MuJoCo tasks. However, this ad-
vantage is a consequence of several complicated algorithmic
ideas; see Section 3.4 of (Mania, Guy, and Recht 2018) for
the details. It is indeed true that this algorithm has scope for
massive parallelization and, hence, can train policies faster
than other methods. However, this beneﬁt is realizable only if
data from multiple policies can be obtained in parallel. This
is often not the case in practical RL, e.g., in robotics, one
typically has access only to a single robot and, hence, can
obtain data only from a single policy at any given time.

Mania, Guy, and Recht (2018) showed that all these com-
plications and the usage of neural networks are often un-
necessary. Instead, to obtain the state of the art time and
data efﬁciency, it sufﬁces to work with deterministic linear
policies and a basic random search with some minor tweaks.
The LQR simulations in Section 4.4 of (Mania, Guy, and
Recht 2018), however, showed that the current ARS imple-
mentation is not sample efﬁcient in the optimal sense. Our
proposed ARS variant signiﬁcantly improves upon the sam-
ple efﬁciency of ARS without overly complicating its control
policy. We also emphasize that, while this work is in the
context of ARS, our off-policy ranking approach should also
be useful in other ES methods.

3 Preliminaries
Here, we describe our RL setup and provide an overview of
the original ARS algorithm.

3.1 RL Setup and Important Deﬁnitions
Let ∆(U ) denote the set of probability measures on a set
U. At the heart of RL, we have an inﬁnite-horizon Markov
Decision Process (MDP) represented by (S, A, P, r, ρ0). In
this tuple, S denotes the state space, A represents the action
space, P : S × A → ∆(S) and r : S × A × S → R are deter-
ministic functions such that P(s(cid:48)|s, a) ≡ P(s, a, s(cid:48)) speciﬁes
the probability of moving from state s to s(cid:48) after taking ac-
tion a and r(s, a, s(cid:48)) is the one step reward obtained in this
transition, and, ﬁnally, ρ0 is the initial state distribution.

The main goal in RL is to identify the ‘best’ way to interact
with the MDP. We now describe this point in some detail.
Any stationary interaction strategy is deﬁned via a policy
π : S → ∆(A) which gives a rule for picking an action when

the environment is in a particular state. The quality of such a
policy is the expected average1 reward obtained on a single
trajectory. Speciﬁcally, for a policy π, i.) a trajectory refers
to a sequence τ ≡ (s0, a0, s1, a1, . . .) of states and actions,
where s0 ∼ ρ0, a0 ∼ π(s0), s1 ∼ P(·|s0, a0), a1 ∼ π(s1)
and so on, and ii.) its quality is given by its value function

η(π) := lim
H→∞

1
H

Eτ ∼π [ˆη(π)] ,

(1)

where

ˆη(π) ≡ ˆηH (π, τ ) =

H−1
(cid:88)

t=0

r(st, at, st+1).

(2)

The aforementioned goal then is to ﬁnd a policy that solves

max
π

η(π).

(3)

Because of the MDP, the sequence of states observed under
a policy π forms a Markov chain. If this chain has a stationary
distribution dπ and

dπ(s) = lim
H→∞

1
H

H−1
(cid:88)

t=0

Pτ ∼π(st = s),

(4)

then η(π) = Es∼dπ,a∼π[r(s, a)]. Note that this expression is
independent of the initial distribution ρ0.

In our later discussions, we will also be using some terms
related to the value function such as the state-bias function,
action-bias function, and the advantage function. These are
denoted by Vπ, Qπ, and Aπ, and given by
(cid:34) ∞
(cid:88)

(cid:35)

(r(st, at) − η(π))|s0 = s

Vπ(s) := Eτ ∼π

t=0
(cid:34) ∞
(cid:88)

(cid:35)

Qπ(s, a) := Eτ ∼π

(r(st, at) − η(π))|s0 = s, a0 = a

t=0

and Aπ(s, a) := Qπ(s, a) − Vπ(s), respectively.

3.2 Review of ARS
In this subsection, we shall see how ARS solves the optimiza-
tion problem given in (3).

ARS belongs to a family of iterative black-box optimiza-
tion methods called random search (Matyas 1965). Basic
Random Search (BRS) is the simplest member of this fam-
ily and is also where the origins of ARS lie. For ease of
exposition, we ﬁrst explain BRS’s approach to solving (3).
Throughout this subsection, we restrict our attention to ﬁnite-
horizon MDPs where the search space is some parameterized
family of policies. Note that this is often the case in practical
RL and is also what we deal with in our MuJoCo simulations.
For the above setup, the optimization problem in (3) trans-
lates to maxθ Eτ [ˆη(πθ)] ≡ Eτ [ˆηH (πθ, τ )] for some ﬁxed H.

1Alternatively, one can consider the expected sum of discounted
rewards from a single trajectory as a measure of a policy’s quality.
However, we work with average rewards here since our experiments
involve ﬁnite-horizon MDPs, where this choice is natural.

In general, this objective function need not be smooth. To cir-
cumvent this issue, BRS looks at its smooth variant and then
uses the idea of a stochastic gradient ascent. Speciﬁcally, the
alternative objective considered is EδEτ [ˆη(πθ+νδ)], where
ν is a suitably ﬁxed scalar, and δ is a random perturbation
made up of i.i.d. standard Gaussian entries. Further, in each
iteration, the gradient of this function is estimated via a ﬁnite
difference method. That is, N random directions δ1, . . . , δN
are ﬁrst generated in the parameter space, ˆη(πθ+νδk ) and
ˆη(πθ−νδk ) are then estimated by interacting with the envi-
ronment using πθ+νδk and πθ−νδk for each k, and ﬁnally
1
k=1[ˆη(πθ+νδk ) − ˆη(πθ−νδk )]δk is used as a proxy for
N
the gradient at θ. BRS’s update rule, thus, has the form

(cid:80)N

θj+1 = θj +

α
N

N
(cid:88)

k=1

(cid:2)ˆη(πθj +νδk ) − ˆη(πθj −νδk )(cid:3) δk

for some parameter choice α > 0.

Mania, Guy, and Recht (2018) developed ARS by making
the following changes to BRS. To begin with, they restricted
the search space to a class of deterministic and linearly pa-
rameterized policies: a policy now is represented by a matrix
M and the vector M s denotes the deterministic action to be
taken at state s under that policy. Further, they made three
modiﬁcations to the update rule of BRS. The ﬁrst was to
scale the gradient estimate by the standard deviation of the ˆη
values; this yields the ARS-V1 algorithm. The second was to
normalize the states, given as input to the policies, so that all
its components are given equal importance; this yields ARS-
V2. The ﬁnal modiﬁcation was to pick some b < N and use
only the b best-performing search directions for estimating
the gradient in each iteration. The ﬁrst and the third step yield
the ARS-V1t algorithm, while the combination of all three
gives ARS-V2t. The ARS-V2t variant is of most interest to
us and its update rule has the form

Mj+1 = Mj +

α
bσR

b
(cid:88)

[ˆη(πj,(k),+)− ˆη(πj,(k),−)]δ(k), (5)

k=1

where σR is the standard deviation of the 2b ˆη values, δ(k)
denotes the k-th largest direction, decided based on the value
of max{ˆη(πj,k,+), ˆη(πj,k,−)} for different k choices, and
πj,k,+(s) = (Mj + νδk)diag(Σj)−1/2(s − µj)
πj,k,−(s) = (Mj − νδk)diag(Σj)−1/2(s − µj)
with µj and Σj being the mean and covariance of the 2bHj
states encountered from the start of the training.

The reason for focusing on ARS-V2t is that, in MuJoCo
tasks, it typically outperforms the other V1 and V2 variants
of ARS and also the previous state-of-the-art approaches such
as TRPO, PPO, DDPG, the ES method from (Salimans et al.
2017), and the Natural Gradient method from (Rajeswaran
et al. 2017). This demonstrates that normalization of states
and then ranking and only picking the best directions for
updating parameters often helps in improving the sample efﬁ-
ciency. Nevertheless, in each iteration, ARS uses an on-policy
technique to estimate ˆη(πj,k,+) and ˆη(πj,k,−) for each k so
that the b best-performing directions can be identiﬁed. Be-
cause of this, we claim that ARS still does more interactions

Algorithm 1: Off-policy ARS
1: Setup: State space Rn, Action Space Rp
2: Hyperparameters: step-size α, number of directions
sampled per iteration N , standard deviation of the ex-
ploration noise ν, number of top-performing directions
to use b, bandwidth to use for kernel approximation h,
number of behaviour policy trajectories to run nb

3: Initialize: M0 = 0 ∈ Rp×n, µ0 = 0 ∈ Rn and Σ0 =

In ∈ Rn×n (identity matrix), j = 0
4: while ending condition not satisﬁed do
5:

Sample δ1, δ2, . . . , δN in Rp×n with i.i.d. standard
normal entries
Run nb number of trajectories using policy parameter-
ized by Mj, resulting in Nd number of interactions
Sort the directions δk based on fπj (δk, h) scores (us-
ing (9)), denote by δ(k) the k-th largest direction, and
by πj,(k),+ and πj,(k),− the corresponding policies
Collect 2b rollouts of horizon H and their correspond-
ing return (ˆη(·)) using the 2b policies

6:

7:

8:

πj,(k),+(s) = (Mj + νδ(k))diag(Σj)−1/2(s − µj)

πj,(k),−(s) = (Mj − νδ(k))diag(Σj)−1/2(s − µj)

9: Make the update step:

Mj+1 = Mj+

α
bσR

b
(cid:88)

k=1

[ˆη(πj,(k),+)−ˆη(πj,(k),−)]δ(k),

10:

where σR is the standard deviation of 2b returns used
in the update step.
Set µj+1, Σj+1 to be the mean and covariance of the
2bH(j + 1) states encountered from the starting of
training
j ← j + 1

11:
12: end while

with the underlying environment than what is needed. Also,
in each iteration, it discards the data that do not correspond
to the top-ranked policies.

4 Off-policy ARS: Our proposed method
Here, we describe the details of our proposed approach which
improves upon the wasteful on-policy ranking step in ARS.
Intuitively, in each iteration of our ARS variant, we plan
to identify a suitable deterministic policy, interact with the
environment using just this single policy, and then use
the resultant data to rank the 2N deterministic policies
{πj,k,+, πj,k,− : 1 ≤ k ≤ N }. As a ﬁrst step, we come
up with a way to approximate the value function of a deter-
ministic policy ˜π in terms of another deterministic policy π.
We focus on deterministic policies here since ARS’s perfor-
mance crucially depends on this determinism.

If π and ˜π were stochastic in nature, then such an ap-
proximation has already been given in (Zhang and Ross
2021), which itself is inspired from similar estimates given
in (Kakade and Langford 2002) and (Schulman et al. 2015).
We now discuss the derivation of this approximation.

Consider the average reward RL setup described in Sec-
tion 3.1. Suppose that, for every stationary policy π, the in-
duced Markov chain is irreducible and aperiodic and, hence,
has a stationary distribution dπ satisfying (4). In this frame-
work, (Zhang and Ross 2021, Lemma 1) showed that the
value functions of the two stochastic policies π and ˜π satisfy
η(˜π) = η(π) + Es∼d˜π,a∼˜π [Aπ(s, a)] .
Given this relation, a natural question to ask is whether η(˜π)
can be estimated using just the data collected by interacting
with the environment using π. The answer is no, mainly
because the expectation on the RHS is with respect to the
states being drawn from d˜π. In general, this distribution is
not known a priori and is also hard to estimate unless you
interact with the environment with ˜π itself.

(6)

Inspired by (Kakade and Langford 2002) and (Schulman

et al. 2015), Zhang and Ross (2021) proposed using

Lπ(˜π) := η(π) + Es∼dπ,a∼˜π [Aπ(s, a)]

(7)

as a proxy for the RHS in (6) to overcome the above issue.
There are two main reasons why this was a brilliant idea.
The ﬁrst is that Lπ(˜π), since it uses dπ instead of d˜π, can
be estimated from only environmental interactions involving
π. Second, and importantly, (Zhang and Ross 2021, Lem-
mas 2, 3) showed that |Lπ(˜π) − η(˜π)| is bounded by the total
variation distance between π and ˜π. Thus, when π and ˜π
are sufﬁciently close, an estimate for Lπ(˜π) is also one for
η(˜π). Hence, Lπ(˜π) paves the way for estimating η(˜π) in an
off-policy fashion, i.e., using data from a different policy π.
Henceforth, we refer to the policy chosen for interaction (e.g.,
π above) as the behavior policy and the one whose value
needs to be estimated (e.g., ˜π above) as the target policy.

We now extend the above idea to the case with determin-
istic policies, which we emphasize is one of our main con-
tributions. While the idea may look simple on paper, the
actual extension is not at all straightforward. The key issue is
that, in the case of stochastic policies, the idea of importance
sampling and, in particular, the relation

Es∼dπ,a∼˜π [Aπ(s, a)] = Es∼dπ,a∼π

(cid:20) ˜π(a|s)
π(a|s)

(cid:21)

Aπ(s, a)

is used for estimating the second term in (7). However, for
deterministic policies, the ratio ˜π(a|s)/π(a|s) will typically
be 0, which means the estimate for the second term will also
almost always be zero. Hence, this importance sampling idea
for estimating Lπ(˜π) fails for deterministic policies.

The alternative we propose is to modify the deﬁnition
of Lπ(˜π) so that it becomes useful even for deterministic
policies. Speciﬁcally, we redeﬁne Lπ(˜π) as
(cid:104) Kh((cid:107)a−˜π(s)(cid:107))
Kh((cid:107)a−π(s)(cid:107)) Aπ(s, a)

Lπ(˜π) = η(π) + Es∼dπ,a∼π

(cid:105)
, (8)

where Kh(u) = h−1K(u/h) and K : R → R denotes a
suitably chosen kernel function satisfying (cid:82) K(u)du = 1
and (cid:82) uK(u)du = 0. This approach is loosely inspired from
(Kallus and Zhou 2018; Kallus and Uehara 2020) which look
at extending policy evaluation and control algorithms from
discrete contextual bandit settings to the continuous case.

While there are multiple choices for K, we use K(u) =
in this work. Substituting this deﬁnition in (8) gives

e−u2

Lπ(˜π) = η(π) + Es∼dπ

(cid:104)

e−(cid:107)π(s)−˜π(s)(cid:107)2/h2

Aπ(s, π(s))

(cid:105)

.

The reason for the above choice of K is that it performs quite
well in simulations and, importantly, provides a clean intu-
itive explanation for Lπ(˜π). That is, Lπ(˜π) assigns a higher
value to a policy ˜π if it takes actions similar to π at all those
states s where Aπ(s, π(s)) is large. In summary, Lπ(˜π) given
above provides us with the desired expression to approximate
the value function of a deterministic target policy with only
the data from a deterministic behavior policy.

We now discuss incorporating this expression in ARS to
improve its sample efﬁciency. In particular, we now show
how we can rank the 2N policies {πj,k,+, πj,k,−}, generated
randomly in each iteration of ARS, in an off-policy fashion
(see Section 3.2 for further details on ARS).

The ﬁrst thing we need to decide is the choice of the behav-
ior policy for interacting with the environment. Recall from
the discussion below (7) that |η(˜π) − Lπ(˜π)| is small when
π and ˜π are sufﬁciently close. Now, since the policy parame-
terized by Mj is close to each of the 2N policies speciﬁed
above, it is the natural choice for the behavior policy and,
indeed, this is what we use.

In summary, our ranking in each iteration works as follows:

1. Interact with the environment using the behavior policy
πj ≡ πMj on nb number of trajectories. Each trajectory
here is presumed to have H many time steps (or less in
case of premature termination). Suppose these interactions
result in Nd (st, at, rt, st+1) transitions overall.

2. Estimate Qπj (st, at), 0 ≤ t ≤ Nd − 1, using the deﬁni-

tion given in Section 3.1.

3. Estimate

fπj (δk, h) = E[e−(cid:107)νδks(cid:107)2/h2

Qπj (s, a)]

≈

1
Nd

Nd−1
(cid:88)

[e−(cid:107)νδks(cid:107)2/h2

t=0

Qπj (st, at)]

(9)

for each 1 ≤ k ≤ N. Note that fπj (δk, h) is a proxy for
the expression in (8). In that, it ignores all the constant
terms, those that depend only on the behavior policy πj.

4. Use the above estimates to rank {πj,k,+, πj,k,−}.

Once the b best–performing directions are identiﬁed, the
rest of our ARS variant more or less proceeds as the origi-
nal. That is, we come up with better estimates of the value-
functions of these top policies in an on-policy fashion and
improve upon Mj along the lines discussed in (5). The com-
plete details are given in Algorithm 1. A detailed section
discussing the differences in the original ARS from our off-
policy variant is given in Appendix A.

We end our discussion here by pointing out that the original
ARS used 2N interaction trajectories, each of length roughly
H, in each iteration. In our variant, we only need 2b + nb
many trajectories. When b < N, this difference is what leads
to the 30% reduction in interactions seen in our simulations.

5 Experiments
We compare our method with ARS (Mania, Guy, and Recht
2018) and TRES (Liu et al. 2019) on the benchmark MuJoCo
(Todorov, Erez, and Tassa 2012) tasks available in OpenAI
gym (Brockman et al. 2016). More details about the envi-
ronment is given in Appendix B. The implementation details
are given in Appendix C. In this section, we ﬁrst demonstrate
our algorithm’s improvement in sample efﬁciency. Next, we
thoroughly evaluate the algorithm by running it with 100
random seeds to show that it is robust. Later, we show that
our algorithm is not sensitive to hyperparameters’ choices.
We observer that our algorithm is more sample efﬁcient than
ARS on the Linear Quadratic Regulator (LQR) problem too
(see Appendix F).

5.1

Improvement in Sample efﬁciency

Generally, the sample complexity of benchmark MuJoCo
tasks is reported by the number of interactions required to
reach a certain reward threshold. There are many thresholds
used in previous work, namely (Gu et al. 2017), (Rajeswaran
et al. 2017), (Salimans et al. 2017) and (Mania, Guy, and
Recht 2018). We continue to use the same thresholds as in
ARS.

Mania, Guy, and Recht (2018) use three seeds to bench-
mark the performance of the algorithms. We observe that
the results are susceptible to the seeds. Hence, we consider
eight seeds to get better estimates. We use sixteen seeds in
Walker2d-v3 as the success rate in that environment is less.
We compare our algorithm against ARS (Mania, Guy, and
Recht 2018) and Trust Region Evolution Strategies(TRES)
(Liu et al. 2019), which more efﬁciently uses the sample data
than ES. We use the author’s code of ARS and the author’s
code of TRES to run our experiments. We present the results
as two scenarios.

We now discuss the ﬁrst scenario where ARS-V2t per-
forms better than ARS-V2. This is the case with Swimer-v3,
Hopper-v3, Walker2d-v3, and Ant-v3 environments (see Ap-
pendix D). Our algorithm signiﬁcantly improves the number
of interactions in all the above environments except Ant-v3.
Table 1 summarizes the comparison with various algorithms.
Our experiments conﬁrm that the off-policy ARS often
gets stuck at the local optima in the Ant-v3 environment. We
guess the stagnation happens because, near the local max-
ima, the approximate off-policy ranking forces the updates in
different iterations to be in opposing directions. Interleaving
with on-policy estimations in such situations appears to help
overcome this oscillatory behavior. If our above understand-
ing is correct, then the reason for the stagnation is a bit of
both sub-optimal ranking criteria and a poor exploration strat-
egy. We believe additional studies will be needed to develop
a robust strategy to deal with this issue.

We now discuss the second scenario where ARS-V2 is bet-
ter than ARS-V2t in Humanoid-v3. This is because the sam-
ple complexity increases with increase in N . Our algorithm
performs better in these situations; because we can evaluate
more directions without much increase in sample complexity.
Table 1 shows that our method reaches the threshold faster
than both the ARS algorithms in Humanoid-v3. Note that

Environment Threshold

ARS

TRES

Our method

Version TimeSteps Reached TimeSteps Reached TimeSteps Reached

Swimmer-v3
Hopper-v3
HalfCheetah-v3
Walker2d-v3
Ant-v3 *

Humanoid-v3

325
3120
3430
4390
3580

6000

V2-t
V2-t
V2-t
V2-t
V2-t
V2
V2-t

520
833
4480
18802
10492
23594
40852

5/8
6/8
8/8
4/16
8/8
6/8
5/8

720
NA
2400
19640
16802

NA

5/8
NA
8/8
7/16
8/8

NA

440
765
2400
14414
15071

14260

5/8
6/8
7/8
4/16
8/8

6/8

Table 1: Comparison of ARS, TRES, and our method on MuJoCo locomotion tasks. We show the median number of interactions
required to achieve the prescribed reward threshold for each task. The reached column represents the fraction of the experiments
that reach the threshold. The timesteps are in the order of 103. The * in Ant-v3 signiﬁes the interleaving of on-policy evaluations.

Figure 1: Box plots of timesteps required to reach the threshold in ARS, TRES, and Our method. The number next to the
algorithm’s name represents the number of runs in which the threshold was reached.

Humanoid-v3 is considered the most challenging MuJoCo
environment.

Table 1 displays the median of the timesteps needed for the
ﬁrst crossover of the speciﬁed reward threshold. Speciﬁcally,
the numbers in the table conﬁrm that the ﬁrst crossover in
our off-policy ARS happens much earlier than in the origi-
nal variant. However, the variance in results is not captured
well. Figure 1 consists of box plots of the number of inter-
actions required to reach the threshold in ARS, TRES, and
our method. The number of experiments that reached the
threshold is mentioned next to the algorithm’s name. The
variance in our approach is either less or comparable to ARS
in all environments. Interestingly, our method’s median is
lesser in all the environments except Ant-v3.

5.2 Evaluation over 100 random seeds
As discussed in the previous sub-section, our method per-
forms better than ARS and TRES over eight seeds. While it
is well known that RL algorithms suffer high training vari-
ances, we evaluate our algorithm over 100 random seeds
sampled uniquely from [0,10000]. Figure 2 consists of the
plots from this experiment. We see that the median (thick

blue line) crosses the threshold in all the environments except
Swimmer-v3 and Walker2d-v3. This is because of compara-
tively lower success rates in these environments.

5.3 Sensitivity to hyper-parameters

In this subsection, we look at the sensitivity of our algorithm
to the two hyperparameters we introduce: h and nb. We ﬁrst,
identify multiple hyperparameter choices for h and nb (see
Appendix E). Next, we run our algorithm under all possible
combinations of these hyperparameter choices. The perfor-
mance plots of this experiment (see Figure 3) match those
obtained in the 100 seed test done above, thereby demon-
strating that the algorithm’s performance is not sensitive to
hyperparameter choices. Separately, note that we don’t look
at the sensitivity of our algorithm to other hyperparameters
such as ν, α, N, b. Such a study in the context of ARS has
already been done in (Mania, Guy, and Recht 2018). Since
the role of these parameters in both ARS and our off-policy
variant is similar, we expect the sensitivity test to display
similar outputs.

ARS (5)Ours (5)TRES (5)12TimeSteps1e6Swimmer-v3ARS (6)Ours (6)1231e6Hopper-v3ARS (8)Ours (7)TRES (8)2461e6HalfCheetah-v3ARS (4)Ours (4)TRES (7)1234TimeSteps1e7Walker2d-v3ARS (8)Ours (8)TRES (8)1231e7Ant-v3ARS (6)Ours (6)1.01.52.02.53.01e7Humanoid-v3Figure 2: Evaluation over 100 seeds. Average reward is plotted against Episodes. The thick blue line represents the median curve,
and the shaded region corresponds to the percentiles mentioned in the legend.

Figure 3: Evaluation of sensitivity to hyperparameters. Average reward is plotted against Episodes. The thick blue curve is the
median, and the shaded region is of percentiles given in the legend.

6 Conclusion
This work proposes an off-policy ranking method for sample
efﬁcient evolutionary strategy. While traditional off-policy
methods are not directly applicable to deterministic policies,
we use kernel approximations to perform off-policy evalu-
ations. Our experiments show that our method reached the
threshold with only 70% of interactions of what ARS requires
in roughly the same run time of the algorithm. We believe
our approach is easy to extend to other ES or Random Search
methods. A promising direction of future work would be to
investigate the same theoretically.

The recent hybrid algorithms that mix ES and Deep RL
methods have shown to be more sample efﬁcient than the ES
methods. For example, in CEM-RL (Pourchot and Sigaud
2018), half of the policies are trained using off-policy learn-
ing techniques like DDPG (Lillicrap et al. 2016) and TD3 (Fu-
jimoto, Hoof, and Meger 2018), leaving the other half policies

untouched; however, in the end, all the policies are evaluated
by running trajectories. We believe that our off-policy rank-
ing idea can further improve the sample efﬁciency of these
hybrid algorithms.

A Differences in the original ARS from our

off-policy variant

In this section we mention the key differences between the
original ARS and our off-policy variant. Table 2 indicates
the exact steps that differ between the algorithms and its
implications.

B Environment details

B.1 MuJoCo and OpenAI Gym
Open AI gym (Brockman et al. 2016) is an open-source
python library used in benchmarking reinforcement learning

05001000150020000200Average RewardSwimmer-v30-2525-5050-7575-100020004000600080000100020003000Hopper-v30-2525-5050-7575-100020004000600080002500025005000HalfCheetah-v30-2525-5050-7575-100020000400006000080000Episodes0250050007500Average RewardWalker2d-v30-2525-5050-7575-100010000200003000040000Episodes020004000Ant-v30-2525-5050-7575-100050000100000150000200000Episodes02000400060008000Humanoid-v30-2525-5050-7575-10005001000150020002000200Average RewardSwimmer-v30-2525-5050-7575-100020004000600080000100020003000Hopper-v30-2525-5050-7575-1000200040006000020004000HalfCheetah-v30-2525-5050-7575-100020000400006000080000Episodes0200040006000Average RewardWalker2d-v30-2525-5050-7575-10002000040000Episodes2000020004000Ant-v30-2525-5050-7575-100050000100000150000200000Episodes02000400060008000Humanoid-v30-2525-5050-7575-100Step

ARS

there is no corresponding step in
ARS

step 5 in ARS; collects 2N roll-
outs
step 6 in ARS; sorts δk based on
returns from the 2N trajectories

Our algorithm

run nb trajectories

collects only 2b rollouts (note
that b < N )
we sort δk based on the ﬁtness
function (9) derived using off-
policy technique

step 7 in ARS; uses r(π) for the
total reward received from pol-
icy π
step 5, 6 in ARS; ﬁrst collect
data from 2N policies, then rank

we use ˆη(π) to denote the total
reward received from policy π

we ﬁrst rank; then collect only
data from 2b policies

6

8

7

9

7, 8

Implications

the trajectories are run using policy pa-
rameterized by Mj (used as behavior
data for off-policy evaluation)
we collect data required only for up-
date step
we generate less data as we ﬁnd the
approximate ranking of δs using off-
policy technique even before generat-
ing trajectories, unlike in ARS where
trajectories are run for all 2N policies
to ﬁnd the rankings
we use ˆη(·) to denote the total reward
because we use r(·) for one-step re-
ward
we generate less samples as mentioned
earlier

Table 2: Key differences between ARS and our off-policy variant. The step column refers to the step number in Algorithm 1.

algorithms. It contains many environments, for example, sim-
ple environments like pendulum and complex environments
like robot locomotion tasks and Atari games. OpenAI gym
abstracts the environment and provides an easy API to train
RL agents. The agents can observe the current state and ap-
ply an action. The library does the simulation and provides
the agent with the following observation and instantaneous
reward. The robotic locomotion tasks like (HalfCheetah-v3,
Hopper-v3, Humanoid-v3, etc.) use the MuJoCo (Todorov,
Erez, and Tassa 2012) simulator in the backend. Notably,
the MuJoCo robotic locomotion tasks consist of continuous
state-space and action-space. The state representation in each
of these environments is the positions and velocities of the
center of mass and various robot joints. The action space is
the torque to be applied at each of the robot’s joints. The
instantaneous reward is a function of the current state, action,
and next state.

B.2 Linear Quadratic Regulator(LQR)

Linear Quadratic Regulator is a linear dynamical system with
quadratic cost. The goal is to devise a control strategy that
minimizes the cost. Mathematically, it can be written as the
following:

min
u0,u1,...

lim
T →∞

1
T

E[ΣT −1

t=0 xT

t Qxt + uT

t Rut]

s.t.xt+1 = Axt + But + wt

Mania, Guy, and Recht (2018) use the LQR instance

deﬁned by (Dean et al. 2020). Here,

A =

(cid:34)1.01 0.01
0.01 1.01

0
0.01
0.01 1.01

0

(cid:35)

B = I, Q = 10−3I, R = I

We use the same system mentioned above for our experiments
in this paper.

C Implementation details

Mania, Guy, and Recht (2018) implemented a parallel
version of the ARS algorithm using the python package
Ray (Moritz et al. 2018). They create a centralized noise
table with standard normal entries and pass on the starting
indices between workers instead of sending the entire
perturbation δ. This centralized table avoids the bottleneck
of communication. Similar approaches have been taken
previously by Moritz et al. (2018) in implementing ES.
We adopt the code from Mania, Guy, and Recht (2018)
and modify it accordingly to implement our algorithm.
Hopper-v3, Walker2d-v3, Ant-v3, and Humanoid-v3
tasks give a survival reward at every time step if the
robot does not fall over. This survival rewards cause the
ARS and TRES algorithms to learn policies that stand
still for long. Hence, Mania, Guy, and Recht (2018)
suggest subtracting the survival reward during the training
phase. We also adopt the same. The code for ARS is
from https://github.com/modestyachts/ARS
borrowed
and the
is borrowed
https://github.com/benjamin-recht/benjamin-
from
recht.github.io/tree/master/secret playground.

for LQR experiments

code

D ARS-V2 vs ARS-V2t

Mania, Guy, and Recht (2018) benchmarked the ARS algo-
rithms against the v1 versions of MuJoCo locomotion tasks.
However, this version is deprecated and no more supported
in OpenAI gym. The latest version of these MuJoCo envi-
ronments is v3. Hence, we now run the ARS V2 and V2t
algorithms on the v3 environments. The results from our ex-
periments show that the ARS V2t is better than ARS V2 in
all environments except Humanoid-v3 and HalfCheetah-v3
(refer Table 3).

Environment Threshold

ARS-V2

ARS-V2t

Conﬁg

TimeSteps Reached

Conﬁg

TimeSteps Reached

Swimmer-v3
Hopper-v3
HalfCheetah-v3
Walker2d-v3
Ant-v3
Humanoid-v3

325
3120
3430
4390
3580
6000

N=1 b=1
N=4 b=4
N=4 b=4
N=30 b=30
N=20 b=20
N=230 b=230

580
1098
3840
23151
17711
23594

5/8
5/8
7/8
5/16
2/8
6/8

N=2 b=1
N=8 b=4
N=32 b=4
N=40 b=30
N=60 b=20
N=350 b=230

520
833
4480
18802
10492
40852

5/8
6/8
8/8
4/16
8/8
5/8

Table 3: Comparison of ARS-V2 and ARS-V2t. The median timesteps shown are in the order of 103.

v3 environment, which is considered the most challenging
environment.

E Hyperparameters
The ARS algorithm consists of hyperparameters, namely,
number of search directions N, number of top performing
directions k, learning rate α, and noise standard deviation
ν. Our off-policy variant has two more hyperparameters:
the number of trajectories to run using behavior policy nb
and the bandwidth in kernel function h. We use the same
hyperparameters from (Mania, Guy, and Recht 2018), and try
different values for nb and h as mentioned in Table 4. The
ﬁnal set of hyperparameters that gave us the best results are
mentioned in Table 5.

F LQR experiments
As mentioned in Section 4.3 of (Mania, Guy, and Recht
2018), MuJoCo robotic tasks have some drawbacks. Most
importantly, the optimal policies of these environments are
unknown. Therefore, one is unsure how their algorithm’s
learned policy compares to the optimal policy. One good
idea is to apply the algorithms to simple well known, and
well-studied environments whose optimal policy is known.
(Mania, Guy, and Recht 2018) chose Linear Quadratic Regu-
lator (LQR) with unknown dynamics for this benchmarking.
More details about the environment are given in Appendix
B.2.

We use the same system used by (Mania, Guy, and Recht
2018) to compare our method with model-based Nominal
Control, LSPI (Lagoudakis and Parr 2001) and ARS (Mania,
Guy, and Recht 2018). As shown by Mania, Guy, and Recht
(2018), the Nominal method is more sample efﬁcient than
LSPI and ARS by several orders of magnitude, showing that
there is a scope for improvement. Our experiments show that
our method is more sample efﬁcient than ARS as shown in
Figure 4a . Figure 4c shows that our algorithm is better than
ARS in terms of frequency of stability.

G Time Comparison
The hybrid algorithms like CEM-RL (Pourchot and Sigaud
2018) are more sample efﬁcient than vanilla ES methods.
However, their run time is too high. Table 6 provides a quick
summary of the median wall-clock time (in seconds) taken by
each algorithm to reach the threshold. ARS, TRES, and our
method are run on CPU (Threadripper 3990X), while CEM-
RL is run on GPU (Nvidia RTX 3080-TI). A critical point
to note here is that CEM-RL could not solve the Humanoid-

Environment

N

Swimmer-v3
Hopper-v3
HalfCheetah-v3
Walker2d-v3
Ant-v3
Humanoid-v3

1,2
8
32
40
60
230,350

b

1
4
4
30
20
230

α

0.02
0.01
0.02
0.03
0.015
0.02

ν

0.01
0.025
0.03
0.025
0.025
0.0075

h

1.0,0.5,0.25,0.1
1.0,0.5,0.25,0.1
1.0,0.5,0.25,0.1
1.0,0.5,0.25,0.1
1.0,0.5,0.25,0.1
1.0,0.5,0.25,0.1

nb
1,2
1,2
1,2
1,2
1,2
1,2

Table 4: Hyperparameter grid used in each environment

Environment

N

Swimmer-v3
Hopper-v3
HalfCheetah-v3
Walker2d-v3
Ant-v3
Humanoid-v3

2
8
32
40
60
230,350

b

1
4
4
30
20
230

α

0.02
0.01
0.02
0.03
0.015
0.02

ν

0.01
0.025
0.03
0.025
0.025
0.0075

h

0.1
0.25
1.0
0.5
0.25
0.25

nb
2
2
2
2
1
2

Table 5: Best performing hyperparameters used to generate results in Table 1 and Figures 1 and 2

(a) Cost vs TimeSteps

(b) Cost vs Iterations

(c) Frequency of Stability

Figure 4: Comparison of sample efﬁciency and stability of various algorithms on LQR.

Environment

Threshold ARS TRES Our method CEM-RL

Swimmer-v3
Hopper-v3
HalfCheetah-v3
Walker2d-v3
Ant-v3
Humanoid-v3

325
3120
3430
4390
3580
6000

81
48
42
364
322
820

49
-
86
457
1603
-

52
80
110
366
665
543

-
5703
2268
15363
16989
-

Table 6: Comparing median wall-clock time (in seconds) of ARS, TRES, Off-policy ranked ARS(our method) and CEM-RL.

010000200003000040000TimeSteps0100200300400500CostARSOursLSPINominal01020304050Iteration0100200300400500CostARSOurs010000200003000040000TimeSteps0.00.20.40.60.81.0Frequenct of StabilityARSOursLSPINominalLiu, G.; Zhao, L.; Yang, F.; Bian, J.; Qin, T.; Yu, N.; and
Liu, T.-Y. 2019. Trust Region Evolution Strategies. In Pro-
ceedings of the AAAI Conference on Artiﬁcial Intelligence,
volume 33, 4352–4359.
Mania, H.; Guy, A.; and Recht, B. 2018. Simple random
search of static linear policies is competitive for reinforce-
In Proceedings of the 32nd International
ment learning.
Conference on Neural Information Processing Systems, 1805–
1814.
Matyas, J. 1965. Random optimization. Automation and
Remote Control, 26(2): 246–253.
Mnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Veness,
J.; Bellemare, M. G.; Graves, A.; Riedmiller, M.; Fidjeland,
A. K.; Ostrovski, G.; et al. 2015. Human-level control through
deep reinforcement learning. nature, 518(7540): 529–533.
Moritz, P.; Nishihara, R.; Wang, S.; Tumanov, A.; Liaw, R.;
Liang, E.; Elibol, M.; Yang, Z.; Paul, W.; Jordan, M. I.; et al.
2018. Ray: A distributed framework for emerging {AI}
applications. In 13th {USENIX} Symposium on Operating
Systems Design and Implementation ({OSDI} 18), 561–577.
Pourchot, A.; and Sigaud, O. 2018. CEM-RL: Combining
evolutionary and gradient-based methods for policy search.
arXiv preprint arXiv:1810.01222.
Rajeswaran, A.; Lowrey, K.; Todorov, E.; and Kakade, S.
2017. Towards generalization and simplicity in continuous
control. In Proceedings of the 31st International Conference
on Neural Information Processing Systems, 6553–6564.
Salimans, T.; Ho, J.; Chen, X.; Sidor, S.; and Sutskever, I.
2017. Evolution strategies as a scalable alternative to rein-
forcement learning. arXiv preprint arXiv:1703.03864.
Schulman, J.; Levine, S.; Abbeel, P.; Jordan, M.; and Moritz,
P. 2015. Trust region policy optimization. In International
conference on machine learning, 1889–1897. PMLR.
Schulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and
Klimov, O. 2017. Proximal policy optimization algorithms.
arXiv preprint arXiv:1707.06347.
Todorov, E.; Erez, T.; and Tassa, Y. 2012. Mujoco: A physics
engine for model-based control. In 2012 IEEE/RSJ Interna-
tional Conference on Intelligent Robots and Systems, 5026–
5033. IEEE.
Zhang, Y.; and Ross, K. W. 2021. On-policy deep reinforce-
In Inter-
ment learning for the average-reward criterion.
national Conference on Machine Learning, 12535–12545.
PMLR.

References

Brockman, G.; Cheung, V.; Pettersson, L.; Schneider, J.;
Schulman, J.; Tang, J.; and Zaremba, W. 2016. Openai gym.
arXiv preprint arXiv:1606.01540.

Dean, S.; Mania, H.; Matni, N.; Recht, B.; and Tu, S. 2020.
On the sample complexity of the linear quadratic regulator.
Foundations of Computational Mathematics, 20(4): 633–679.

Fujimoto, S.; Hoof, H.; and Meger, D. 2018. Addressing
In
function approximation error in actor-critic methods.
International Conference on Machine Learning, 1587–1596.
PMLR.

Gu, S.; Lillicrap, T.; Ghahramani, Z.; Turner, R. E.; and
Levine, S. 2017. Q-Prop: Sample-Efﬁcient Policy Gradient
with An Off-Policy Critic. In International Conference on
Learning Representations (ICLR 2017). OpenReview. net.

Kakade, S.; and Langford, J. 2002. Approximately optimal
approximate reinforcement learning. In In Proc. 19th Inter-
national Conference on Machine Learning. Citeseer.

Kallus, N.; and Uehara, M. 2020. Doubly Robust Off-Policy
Value and Gradient Estimation for Deterministic Policies.
Advances in Neural Information Processing Systems, 33.

Kallus, N.; and Zhou, A. 2018. Policy evaluation and opti-
mization with continuous treatments. In International con-
ference on artiﬁcial intelligence and statistics, 1243–1251.
PMLR.

Lagoudakis, M. G.; and Parr, R. 2001. Model-free least-
squares policy iteration. Advances in neural information
processing systems, 14.

Li, Z.; Lin, X.; Zhang, Q.; and Liu, H. 2020. Evolution strate-
gies for continuous optimization: A survey of the state-of-
the-art. Swarm and Evolutionary Computation, 56: 100694.

Lillicrap, T. P.; Hunt, J. J.; Pritzel, A.; Heess, N.; Erez, T.;
Tassa, Y.; Silver, D.; and Wierstra, D. 2016. Continuous
control with deep reinforcement learning. In ICLR (Poster).

