9
1
0
2

v
o
N
4

]
T
S
.
h
t
a
m

[

1
v
5
6
0
1
0
.
1
1
9
1
:
v
i
X
r
a

Modeling and estimation of multivariate discrete
and continuous time stationary processes

Marko Voutilainen

November 5, 2019

Abstract

In this paper, we give a AR(1) type of characterization covering all multivari-
ate strictly stationary processes indexed by the set of integers. Consequently, we
derive continuous time algebraic Riccati equations for the parameter matrix of the
characterization providing us with a natural way to deﬁne the corresponding esti-
mator under the assumption of square integrability. In addition, we show that the
estimator inherits consistency from autocovariances of the stationary process and
furthermore, the limiting distribution is given by a linear function of the limiting
distribution of the autocovariances. We also present the corresponding existing
results of the continuous time setting paralleling them to the discrete case.

AMS 2010 Mathematics Subject Classiﬁcation: 60G10, 62M10, 62H12, 62G05

Keywords: time-series analysis, stationary processes, characterization, multivariate Ornstein-
Uhlenbeck processes, generalized Langevin equation, AR(1) representation, algebraic Riccati
equations, estimation, consistency

1 Introduction

Stationary stochastic processes provide a signiﬁcant instrument for modeling numer-
ous temporal phenomena related to different ﬁelds of science. In particular, due to the
evidence of long dependence structures in the real ﬁnancial data, stationary processes
possessing long-memory have been widely applied in mathematical ﬁnance.

When discrete time is considered, stationary data is typically modeled by applying
ARMA processes or their extensions. One focal reason for popularity of ARMA pro-
cesses is that for every stationary process with a vanishing autocovariance γ(·) and for
every n ∈ N there exists an ARMA process X such that γX(k) = γ(k) for |k| ≤ n.
For a comprehensive overview of ARMA processes we mention [9], [16] and [31].
The immense ARMA family include for example SARIMA models, where a seasonal
ARMA process is obtained by differencing the original data, and different GARCH

1

 
 
 
 
 
 
models originating from [14] and [7] that are commonly used in ﬁnancial modeling
taking account of the time-dependent volatility. ARMA processes, their extensions
and estimation have been concerned e.g. in [18], [36], [30], [29], [27], [15], [8], [17],
[3], [26] and [43], to mention but a few. Moreover, in [38] we showed that all univari-
ate strictly stationary processes indexed by the integers are characterized by the AR(1)
equation

Xt − φXt−1 = Zt,

t ∈ Z,

where the noise Z belonging to a certain class of stationary process is not necessarily
white. Established on the characterization, we proposed an estimation method for φ in
the case of a square integrable stationary process that has several advantages over the
conventional methods such as maximum likelihood and least squares ﬁtting of ARMA
models. Furthermore, in [40] we applied our method in estimation of a generalized
ARCH model involving a covariate process that can be interpreted as the liquidity of
an asset.

In the case of continuous time, the Ornstein-Uhlenbeck process X given by the

Langevin equation

dXt = −θXtdt + dBt,

t ∈ Z,

(1)

where θ > 0 and B is a two-sided Brownian motion, can be seen as the analogue
of the discrete time AR(1) process. By posing a suitable initial condition, (1) yields a
stationary solution. The foregoing can be generalized, for example, by replacing Brow-
nian motion with other stationary increment processes satisfying certain integrability
conditions, such as fractional Brownian motion recovering the fractional Ornstein-
Uhlenbeck process introduced in [12]. This kind of generalized Ornstein-Uhlenbeck
processes are applied e.g. in mathematical ﬁnance to describe mean-reverting systems
under the inﬂuence of shocks, and they are a highly active topic of research. Equations
of type (1) with varying driving forces, and estimation in such models have been con-
cerned e.g. in [19], [21], [2], [4], [5], [10], [13], [20], [32], [33], [35], [1] and [28],
to mention but a few. Furthermore, in [41] we showed that a generalized multidimen-
sional version of (1) characterizes all multivariate strictly stationary processes with
continuous paths. Consequently, we proposed an estimation method for the parameter
matrix of (1) under the assumption of square integrability. The method is based on
continuous time algebraic Riccati equations (CAREs) written in terms of the autoco-
variance function of the stationary solution. Algebraic Riccati equations have been
studied intensively in the literature and they occur naturally e.g. in optimal control and
ﬁltering theory. Real-valued CAREs often take the symmetric form

B⊤A + AB − ACA + D = 0,

(2)

where C and D are symmetric, and symmetric solutions A are to be found. For a gen-
eral approach to algebraic Riccati equations the reader may consult for example [24].
The existence and uniqueness of a solution to (2) is a well-studied topic, especially
when C and D are positive semideﬁnite (see e.g. [22], [42] or [34]).

2

The rest of the paper is organized as follows. In Subsection 2.1 we complete our
previous investigations of stationary processes by treating the multivariate discrete
time case. First, we show that the characterization is now given by a multidimen-
sional AR(1) type of equation. Then, by taking a similar approach as in [41] we obtain
a set of symmetric CAREs that serve as a basis for estimation of the model parameter
matrix. Finally, we state theorems for consistency and asymptotic distribution of the
estimator. In Subsection 2.2 we present the main results of [41] while at the same time
comparing them to the results obtained in discrete time. The proofs are postponed to
Section 3.

2 Main results

The considered processes are n-dimensional, real-valued and indexed by I ∈ {Z, R}.
We use the notation Y = (Yt)t∈I, where the ith component of the random vector Yt
is denoted by Y (i)
. Equality of the distributions of two random vectors Yt and Zt is
t
law
= Zt. Similarly, equality of two processes Y and Z in the sense of ﬁnite
denoted by Yt
law
dimensional distributions is denoted by Y = (Yt)t∈I
= (Zt)t∈I = Z. Throughout the
law
paper, we investigate strictly stationary processes meaning that (Xt+s)t∈I
= (Xt)t∈I
for every s ∈ I. Consequently, we omit the word ’strictly’ and simply say that X
is stationary. By writing A ≥ 0 or A > 0 we mean that the matrix A is positive
semideﬁnite or positive deﬁnite, respectively. We denote an eigendecomposition of a
symmetric matrix by A = QΛQ⊤, where Λ = diag(λi). Furthermore, the L2 vector
norm and the corresponding induced matrix norm is denoted by k · k.

By applying the models of stationary processes, which we introduce in this paper,

we consequently obtain symmetric CAREs of the form

B⊤A + AB − ACA + D = 0,

(3)

where C, D ≥ 0, and we are solving the equation for a positive deﬁnite A. There exists
a vast amount of literature on existence and uniqueness of a solution (see e.g. [22] or
[42]) in the described setting. In particular, if C, D > 0, then there exists a unique pos-
itive semideﬁnite solution to (3). Furthermore, there exists several numerical methods
for ﬁnding the positive semideﬁnite solution of (3) (see e.g. [11], [25] or monograph
[6]).

2.1 Discrete time

In this subsection we extend the characterization of stationary processes of [38] to mul-
tivariate settings. Consequently, we derive quadratic equations for the corresponding
model parameter matrix providing us with a natural way to deﬁne an estimator for the
parameter. Finally, we pose theorems for consistency and asymptotic distribution. A
strong analogue with the continuous time case I = R covered in [41] is obtained. We
start by providing some deﬁnitions.

3

Deﬁnition 2.1. Let G = (Gt)t∈Z be a n-dimensional stationary increment process. We
deﬁne a stationary process ∆G = (∆tG)t∈Z by

∆tG = Gt − Gt−1.

As in the univariate case, we deﬁne a class of stationary increment process having

sub-exponentially deviating sample paths.

Deﬁnition 2.2. Let H > 0 and let G = (Gt)t∈Z be a n-dimensional stochastic process
with stationary increments and G0 = 0. If

lim
l→−∞

0

k=l
X

ekH∆kG

exists in probability and deﬁnes an almost surely ﬁnite random variable, we denote
G ∈ GH.

Remark 2.3. Lemma 3.1 shows that existence of a logarithmic moment is sufﬁcient for
G ∈ GH for all H > 0. Particularly, this is the case if G is square integrable. On the
other hand, an example of a stationary increment process G with G0 = 0, but G /∈ GH
for any H > 0 was provided in [37].

The next theorem characterizes all multivariate stationary processes, including pro-

cesses possessing long-memory.

Theorem 2.4. Let H > 0 be ﬁxed and let X = (Xt)t∈Z be a n-dimensional stochastic
process. Then X is stationary if and only if limt→−∞ etH Xt

P
= 0 and

∆tX = (e−H − I)Xt−1 + ∆tG

(4)

for G ∈ GH and t ∈ Z. Moreover, the process G ∈ GH is unique.

Corollary 2.5. Let H > 0 be ﬁxed and let X be stationary. Then X admits an AR(1)
type of representation

Xt − ΦXt−1 = ∆tG,

(5)

where Φ = e−H and G ∈ GH.

By using (5) and the expression (14) from the proof of Theorem 2.4, it is straight-
forward to show that ∆G is centred and square integrable if and only if X is centred
and square integrable, respectively. In what follows, we assume these two attributes
and write γ(t) = EXtX ⊤

0 and r(t) = E(∆tG)(∆0G)⊤. Furthermore, since

t

Gt =

∆kG,

t ≥ 1,

k=1
X

4

in this case also G is centred and square integrable, and we denote v(t) = cov(Gt) =
EGtG⊤
t . We would like to point out that centredness can be assumed without loss of
generality (see Remark 2.11).

Under the assumptions, we obtain an expression for γ(t) in terms of the noise

process.

Remark 2.6. The autocovariance function γ(t) is given by

t

0

γ(t) = e−tH

ekHr(k − j)ejH.

k=−∞
X

j=−∞
X

Furthermore, if G has independent components, we obtain

γ(t) =

e−tH
2

t

0

k=−∞
X

j=−∞
X

ekH (v(k − j + 1) + v(k − j − 1) − 2v(k − j)) ejH.

The following lemma writes the quadratic equations for the model parameter Φ =

e−H presented in [38] in our multivariate setting.

Lemma 2.7. Let H > 0 be ﬁxed and let X be stationary of the form (5). Then

r(t) = Φγ(t)Φ − γ(t + 1)Φ − Φγ(t − 1) + γ(t)

(6)

for every t ∈ Z.

Remark 2.8. In the proof of the lemma, we have utilized the increment process ∆G
of the noise similarly as in [38] yielding quadratic equations for Φ. However, for a
general stationary X, (6) is a symmetric CARE only if t = 0, and even in this case,
existence of a unique positive semideﬁnite solution is not guaranteed.

By applying a similar approach as in [41] by considering the noise G directly, we
obtain a set of symmetric CAREs on which we construct an estimator for the model
parameter Φ = e−H. For this, we deﬁne the following matrix coefﬁcients.

Deﬁnition 2.9. We set

t

Bt =

γ(k − 1) − γ(k)⊤

k=1
X
t

t

Ct =

γ(k − j)

k=1
X

j=1
X

Dt = v(t) − 2γ(0) + γ(t) + γ(t)⊤

for every t ∈ N.

5

Theorem 2.10. Let H > 0 be ﬁxed and set Θ = I − e−H . Let X = (Xt)t∈Z be
stationary of the form (4). Then the CARE

B⊤

t Θ + ΘBt − ΘCtΘ + Dt = 0

(7)

is satisﬁed for every t ∈ N.

Remark 2.11. Equations (6) and (7) are covariance based. Consequently, they hold
also when X and G in Theorem 2.4 are not centred.

Remark 2.12. By Lemma 3.6, the matrix Θ is positive deﬁnite. Since

t

t

Ct = E

Xk−1

Xk−1

k=1
X

k=1
X

⊤

!

= cov

t

k=1
X

Xk−1

,

!

the matrix Ct is positive semideﬁnite. Furthermore, if the smallest eigenvalue of v(t)
grows enough in time, Dt becomes positive deﬁnite (see [41]). This is the case e.g.
when the noise has independent components with growing variances.

We give a couple of examples on how some basic multivariate processes of ARMA
type can be presented in the form (5), and how to derive the corresponding noise G
together with its covariance function v.

Example 2.13. Let X be a n-dimensional stationary AR(1) type of process given by

Xt − φXt−1 = ǫt,

with 0 < φ = QΛQ⊤, kφk < 1 and ǫ ∼ iid(0, Σ). Then, we may set H =
t
−Qdiag(log λi)Q⊤ giving Φ = φ. Now ∆G = ǫ and Gt =
k=1 ǫk. Furthermore,
v(t) =

t
k=1 cov(ǫk) = tΣ for t ≥ 1.

P

P

Example 2.14. Let X be a n-dimensional stationary ARMA(1, q) type of process given
by

Xt − φXt−1 = ǫt + θ1ǫt−1 + . . . + θqǫt−q,

with 0 < φ, kφk < 1 and ǫ ∼ iid(0, Σ). Similarly as above, we may set Φ = φ and
now ∆G equals to the MA(q) process on the right. Consequently, for t ≥ 1,

and

where θ0 = I.

t

Gt =

ǫk + θ1ǫk−1 + . . . + θqǫk−q

k=1
X

q

v(t) =

max(0, t − |i − j|)θiΣθ⊤
j ,

i,j=0
X

6

 
 
In [38] we proposed an estimation method of one-dimensional stationary processes
based on equations (6). In particular, we showed that the method is applicable except
in some special class of stationary processes. In [39] we provided a comprehensive
analysis of the class, and proved that it consists of highly degenerate processes. On the
other hand, due to the strong dependence structure, the failure of different estimation
methods is expected. Fundamentally, a stationary process X belongs to the class if
there exists two values H and ˜H such that the corresponding processes ∆G and ∆ ˜G in
(5) have identical autocovariance functions. Next, we state a lemma showing that these
degenerate processes have a special characteristic also under the new set of equations
(7).

Lemma 2.15. Let X be a one-dimensional stationary process and let H > 0 be ﬁxed.
Set Φ = e−H and

If the equation

Xt − ΦXt−1 = ∆tG,

t ∈ Z.

γ(t)Φ2 − (γ(t + 1) − γ(t − 1))Φ + γ(t) − r(t) = 0

yields the same two solutions Φ, ˜Φ > 0 for every t ∈ Z, then also the equation

CtΘ2 − 2BtΘ − Dt = 0

(8)

(9)

yields the same two solutions Θ = 1 − Φ and ˜Θ = 1 − ˜Φ for every t ∈ N.

For estimation, it is desirable that (7) admits a unique positive semideﬁnite so-
lution guaranteeing convergence to the correct parameter matrix. Hence, we simply
assume that t is chosen in such a way that Ct, Dt > 0, and we omit the subindex t
from Equation (7). We have justiﬁed the assumption of positive deﬁniteness in detail
in continuous time (see Subsection 2.1 and Remark 2.11 in [41]). Furthermore, we
assume that v(t) is known and the stationary process X is observed up to the time
T > t, and the coefﬁcient matrices B, C and D are estimated from these observations
by replacing the autocovariances γ(·) with some estimators ˆγT (·). The coefﬁcient es-
timators are denoted by ˆBT , ˆCT and ˆDT , and we set

∆T B = ˆBT − B, ∆T C = ˆCT − C, ∆T D = ˆDT − D.

Next, we deﬁne an estimator ˆΘT for the matrix Θ = I −Φ = I −e−H . The proofs of the
related asymptotic results allow a certain amount of ﬂexibility in the deﬁnition. Thus,
we give a deﬁnition that probably is the most convenient from the practical point of
view. Consistency and the rate of convergence of ˆΘT are inherited from autocovariance
estimators ˆγT (·) of the observed stationary process. In addition, the limiting distribu-
tion is obtained as a linear function of the limiting distribution of the autocovariance
estimators.

7

Deﬁnition 2.16. The estimator ˆΘT is deﬁned as the unique positive semideﬁnite solu-
tion to the perturbed CARE
ˆB⊤
T
whenever ˆCT , ˆDT > 0. Otherwise, we set ˆΘT = 0.
Theorem 2.17. Let C, D > 0. Assume that

ˆΘT + ˆΘT ˆBT − ˆΘT ˆCT ˆΘT + ˆDT = 0

Then

max
s∈{0,1,...,t}

kˆγT (s) − γ(s)k

P

−→ 0.

k ˆΘT − Θk

P

−→ 0,

where ˆΘT is given by Deﬁnition 2.16.
Theorem 2.18. Let l(T ) be a rate function. If

vec(ˆγT (0) − γ(0))
vec(ˆγT (1) − γ(1))
...
vec(ˆγT (t) − γ(t))

l(T ) 







law
−→ Z,






where Z is a (t + 1)n2-dimensional random vector, then:

(1) Let ˜Z be the permutation of elements of Z corresponding to the order of elements

of

vec
vec

(ˆγT (0) − γ(0))⊤
(ˆγT (1) − γ(1))⊤
(cid:0)
...
(cid:0)
(ˆγT (t) − γ(t))⊤
Deﬁne a linear mapping L1 : R(t+1)n2 → R3n2 by
(cid:1)






vec



(cid:1)
(cid:1)



.






L1(Z) =

˜Z (kn2+1)
...
˜Z ((k+1)n2)

(cid:0)
Z (kn2+1)
...
Z ((k+1)n2)

t−1

k=0(t − k) 

+



t−1

k=1(t − k) 



t
k=1 




P
Z ((k−1)n2+1) − ˜Z (kn2+1)
...
Z (kn2) − ˜Z ((k+1)n2)









P
Z (tn2+1) + ˜Z (tn2+1)
...
Z ((t+1)n2) + ˜Z ((t+1)n2)


Z (1)

...
Z (n2)






− 2 














P



























,


















where

0
1 is an empty sum. Then

P

l(T ) vec(∆T C, ∆T B, ∆T D)

law
−→ L1(Z).

8

(2) If D, C > 0 and ˆΘT is given by Deﬁnition 2.16, then

l(T ) vec( ˆΘT − Θ)

law
−→ L2(L1(Z)),

where L2 : R3n2 → Rn2 is a linear mapping expressible in terms of Θ, t and r.

2.2 Continuous time

We have collected the main results (Theorems 2.20, 2.27 and 2.32) of [41] considering
continuous time stationary processes into this subsection. In addition, in order to com-
plete the analogue between discrete and continuous time, we derive quadratic equa-
tions for the model parameter by using the increments of the noise process (Lemma
2.23). We assume that the processes have continuous paths almost surely and hence,
the related stochastic integrals can be interpreted as pathwise Riemann-Stieltjes inte-
grals. Again, we start by deﬁning the class GH of stationary increment processes for
H > 0.

Deﬁnition 2.19. Let H > 0 and let G = (Gt)t∈R be a n-dimensional stochastic pro-
cess with stationary increments and G0 = 0. If

0

eHudGu

lim
s→−∞

s
Z
exists in probability and deﬁnes an almost surely ﬁnite random variable, we denote
G ∈ GH.

As in discrete time, it can be shown that existence of some logarithmic moments
ensure that G ∈ GH for all H > 0. In particular, square integrability of G sufﬁces,
which is the case in our second moment based estimation method.

The next theorem is the continuous time counterpart of Theorem 2.4 showing that
all stationary processes are characterized by the Langevin equation, whereas in discrete
time, the characterization was given by an AR(1) type of equation.

Theorem 2.20. Let H > 0 be ﬁxed and let X = (Xt)t∈R be a n-dimensional stochastic
process. Then X is stationary if and only if

and

X0 =

0

−∞

Z

eHudGu

dXt = −HXtdt + dGt,

(10)

for G ∈ GH and t ∈ R. Moreover, the process G ∈ GH is unique.
Corollary 2.21. From Theorem 2.20 it follows that X is the unique stationary solution

t

Xt = e−Ht

eHudGu

(11)

to (10).

−∞

Z

9

In order to apply Theorem 2.20 in estimation, we pose the assumption

EkGsk2 < ∞.

sup
s∈[0,1]

This guarantees that G ∈ GH for all H > 0, and square integrability of X and G. On
the other hand, if X is square integrable, then G is also. In addition and without loss of
generality, we assume that the processes are centred. Again, we write γ(t) = EXtX ⊤
0
and v(t) = EGtG⊤
t . Now, the autocovariance function of the following stationary
process is well-deﬁned.

Deﬁnition 2.22. Let G = (Gt)t∈R be a centred square integrable stationary increment
process and let δ > 0. We deﬁne a stationary process ∆δG = (∆δ

t G)t∈R by

∆δ

t G = Gt − Gt−δ

and the corresponding autocovariance function rδ by

rδ(t) = E(∆δ

t G)(∆δ

0G)⊤.

As in discrete time (Lemma 2.7), we obtain quadratic equations for the model

parameter H in terms of rδ.

Lemma 2.23. Let H > 0 be ﬁxed and let X be of the form (11). Then

rδ(t) =2γ(t) − γ(t + δ) − γ(t − δ) +

(cid:18)Z
t

t+δ

t+δ

t

t

t+δ

γ(s)ds −

γ(s)ds

(cid:19)

Z

t−δ

H + H

(cid:18)Z

t−δ

γ(s)ds −

Z
t

γ(s)ds

(cid:19)

(12)

t

+ H

(cid:18)Z

t−δ

(s − t + δ)γ(s)ds +

Z
t

(t − s + δ)γ(s)ds

H

(cid:19)

for every t ∈ R.

Remark 2.24. The advantage of the equations above is that we have to consider γ(s)
only for s ∈ [t − δ, t + δ], but as in the discrete case, for a general stationary X we
obtain a symmetric CARE only when t = 0. In addition, similarly as above, we could
t G := Gt − Gt−k, k ∈ N. However, this would lead to more
set in discrete time ∆k
complicated equations in Lemma 2.7.

A signiﬁcant difference compared to the discrete time equations (6) occurs in the

univariate case. Namely, the ﬁrst order term with respect to H vanishes.

Corollary 2.25. The univariate case yields

rδ(t) = 2γ(t) − γ(t + δ) − γ(t − δ) + H 2

for every t ∈ R.

t

t−δ

 Z

10

(s − t + δ)γ(s)ds +

(t − s + δ)γ(s)ds

t+δ

t

Z

!

One could potentially base a univariate estimation method on the above equations
without the concern of existence of a unique positive solution. However, since we
wish to treat also multivariate settings, we present the most central results of [41] that
are obtained from Theorem 2.20 by considering the noise G directly. First, we deﬁne
matrix coefﬁcients corresponding to Deﬁnition 2.9. Consequently, we write symmetric
CAREs for the parameter H that are similar to the CAREs (7) for the discrete time
parameter Θ.

Deﬁnition 2.26. We set

t

0
Z

t

Bt =

Ct =

0 Z
Z

γ(s) − γ(s)⊤ds

t

0

γ(s − u)duds

Dt = v(t) − 2γ(0) + γ(t) + γ(t)⊤

for every t ≥ 0.

Theorem 2.27. Let H > 0 be ﬁxed and let X = (Xt)t∈R be stationary of the form
(11). Then the CARE

B⊤

t H + HBt − HCtH + Dt = 0

(13)

is satisﬁed for every t ≥ 0.

Remark 2.28. As in discrete time, the equations (12) and (13) are covariance based
and hence, they hold also for a non-centred stationary X.

Remark 2.29. Contrary to the discrete time equations (7), the ﬁrst order term vanishes
in the univariate setting as in (12).

Again, we assume that t is chosen in such a way that Ct, Dt > 0 ensuring the
existence of a unique positive semideﬁnite solution. We have discussed this assump-
tion in detail in [41]. We deﬁne an estimator ˆHT for the model parameter matrix H
identically to the discrete time by replacing the autocovariances γ(·) in the matrix co-
efﬁcients with their estimators ˆγT (·). The below given deﬁnition differs slightly from
the deﬁnition in [41], but the same asymptotic results still apply.

Deﬁnition 2.30. The estimator ˆHT is deﬁned as the unique positive semideﬁnite solu-
tion to the perturbed CARE

ˆB⊤
T

ˆHT + ˆHT ˆBT − ˆHT ˆCT ˆHT + ˆDT = 0

whenever ˆCT , ˆDT > 0. Otherwise, we set ˆHT = 0.

11

As in discrete time, asymptotic properties of ˆHT are inherited from the autocovari-
ance estimators. However, due to the continuous time setting, instead of pointwise
convergence, we have to consider functional form of convergence of ˆγT (·). In [41] we
have provided sufﬁcient conditions in the case of Gaussian noise G with independent
components, under which the assumptions of the following theorems are satisﬁed. In
particular, the results are valid for fractional Brownian motion that is widely applied
in the ﬁeld of mathematical ﬁnance.

Theorem 2.31. Let C, D > 0. Assume that

Then

kˆγT (s) − γ(s)k

P

−→ 0.

sup
s∈[0,t]

k ˆHT − Hk

P

−→ 0,

where ˆHT is given by Deﬁnition 2.30.

Theorem 2.32. Let Y = (Ys)s∈[0,t] be an n2-dimensional stochastic process with con-
tinuous paths almost surely and let l(T ) be a rate function. If

l(T ) vec(ˆγT (s) − γ(s))

law
−→ Ys

in the uniform topology of continuous functions, then:

(1) Let ˜Ys be the permutation of elements of Ys that corresponds to the order of
. Then

elements of vec

(ˆγT (s) − γ(s))⊤

(cid:0)

l(T ) vec(∆T C, ∆T B, ∆T D)

(cid:1)

law

−→ 




t

0 (t − s)(Ys + ˜Ys)ds
t
Ys − ˜Ys
ds
R
0
Yt + ˜Yt − 2Y0
(cid:16)
(cid:17)
R






=: L1(Y ).

(2) If C, D > 0 and ˆHT is given by Deﬁnition 2.30, then

l(T ) vec( ˆHT − H)

law
−→ L2(L1(Y )),

where L2 : R3n2 → Rn2 is a linear mapping expressible in terms of H, t and the
covariance function of G.

3 Proofs

In the following, we denote the smallest eigenvalue of H > 0 by λmin. Consequently
kekHk = kQdiag(eλik)Q⊤k = eλmink for a negative k.

12

3.1 Discrete time

The proof of the next lemma follows the lines of the proof of Theorem 2.2. in [37]
that concerns the one-dimensional continuous time case. However, in our setting, we
obtain a weaker sufﬁcient condition for G ∈ GH for all H > 0.

Lemma 3.1. Let G = (Gt)t∈Z be a stationary increment process with G0 = 0. Assume
that

E

log kG1k1{kG1k>1}

1+δ

< ∞

for some δ > 0. Then G ∈ GH for all H > 0.

(cid:12)
(cid:12)

(cid:12)
(cid:12)

Proof. Let H > 0. We apply the Borel-Cantelli lemma together with Markov’s in-
equality to show that kekH∆kGk → 0 almost surely as k → −∞. Let ǫ > 0 be ﬁxed
below.

P

kekH∆kGk > ǫ

(cid:0)

(cid:1)

≤ P
= P

eλminkk∆kGk > ǫ

(cid:0)

kG1k >

ǫ
eλmink

= P

eλminkkG1k > ǫ
= P (log kG1k > log ǫ − λmink) ,
(cid:1)

(cid:0)

(cid:1)

(cid:16)
since ∆G is stationary and G0 = 0. Furthermore, k( log ǫ
C > 0 and k ≤ kǫ. Thus, for k ≤ kǫ,

(cid:17)

k − λmin) ≥ −Ck for some

P

kekH∆kGk > ǫ

(cid:0)

(cid:1)

≤ P (log kG1k ≥ −Ck) = P
1+δ

E

≤

log kG1k1{kG1k>1}
(−Ck)1+δ

log kG1k1{kG1k>1} ≥ −Ck

(cid:0)
≤ c

1
(−k)1+δ

(cid:1)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

giving the wanted result. We conclude the proof by noting that

0

kekH ∆kGk ≤

k=−∞
X
almost surely.

0

k=−∞
X

1

2 kH kke

ke

1

2 kH ∆kGk ≤ sup
k

ke

1

2 kH ∆kGk

1

2 λmink < ∞

e

0

k=−∞
X

We next extend the concept of self-similarity to discrete time multivariate pro-

cesses.

Deﬁnition 3.2. Let H > 0 and let Y = (Yet)t∈Z be a n-dimensional stochastic process.
Then Y is H-self-similar if

(Yet+s)t∈Z

law
= (esHYet)t∈Z

for every s ∈ Z.

The following transform and the corresponding theorem giving one-to-one corre-
spondence between self-similar and stationary processes were originally introduced by
Lamperti in the univariate continuous time setting ([23]).

13

Deﬁnition 3.3. Let H > 0, and let X = (Xt)t∈Z and Y = (Yet)t∈Z be n-dimensional
stochastic processes. We deﬁne

and

(LHX)et = etH Xt

(L−1

H Y )t = e−tH Yet.

Theorem 3.4. The operator LH together with its inverse L−1
H deﬁne a bijection be-
tween n-dimensional stationary processes and n-dimensional H-self-similar processes.

Proof. First, let X be stationary and set Zet = (LHX)et. Then

2+s

1+s

Zet
Zet
...
Zetm+s

e(t1+s)HXt1+s
e(t2+s)HXt2+s
...
e(tm+s)HXtm+s





= 




for every m ∈ N, t ∈ Zm and s ∈ Z. Hence, Z is H-self-similar.

= 





law= 





























Now, let Y be H-self-similar and set Zt = (L−1

H Y )t. Then

e(t1+s)H Xt1
e(t2+s)H Xt2
...
e(tm+s)H Xtm

1

1

esHZet
esHZet
...
esHZetm








Zt1+s
Zt2+s
...
Ztm+s















= 





1+s

e−(t1+s)H Yet
e−(t2+s)H Yet
...
e−(tm+s)H Yetm+s

2+s

1

e−t1HYet
e−t2HYet
...
e−tmHYetm

2

Zt1
Zt2
...
Ztm















= 












law
= 





for every m ∈ N, t ∈ Zm and s ∈ Z. Hence, Z is stationary completing the proof.

Before the proof of Theorem 2.4 we state an auxiliary lemma.

Lemma 3.5. Let H > 0 and let (Yet)t∈Z be a n-dimensional H-self-similar process.
We deﬁne a process G = (Gt)t∈Z by

t

k=1 e−kH∆kYek,

Gt =

0,
P
−

P






0

k=t+1 e−kH∆kYek,

t ≥ 1
t = 0
t ≤ −1.

Then G ∈ GH.

Proof. It is straightforward to verify that ∆tG = e−tH∆tYet for every t ∈ Z.
addition

In

lim
l→−∞

ekH∆kG = lim
l→−∞

0

0

k=l
X

∆kYek = Y1 − lim
l→−∞

Yel−1,

k=l
X

where by self-similarity of Y

P(kYel−1k ≥ ǫ) = P(keH(l−1)Y1k ≥ ǫ) ≤ P(eλmin(l−1)kY1k ≥ ǫ) → 0.

14

Hence, we set

lim
l→−∞

0

k=l
X

ekH∆kG = Y1.

Proof of Theorem 2.4. Assume that limt→−∞ etH Xt
Then, by using (4) repeatedly

P
= 0 and (4) holds for G ∈ GH.

Xt = e−HXt−1 + ∆tG = e−(n+1)H Xt−n−1 +

t

e−jH ∆t−jG

n

j=0
X

= e−(n+1)HXt−n−1 + e−tH

ekH ∆kG = e−tH

e(t−n−1)H Xt−n−1 +

k=t−n
X

t

k=t−n
X

ekH ∆kG

!

for every n ∈ N. Since, as n → ∞, the limit of the sum above is well-deﬁned, and
limn→∞ e(t−n−1)H Xt−n−1

P
= 0, we obtain that

t

Xt = e−tH

ekH∆kG.

k=−∞
X

(14)

Let m ∈ N, t ∈ Zm and s ∈ Z. Then, by stationary increments of G, we have

e−(t1+s)H

t1
j=−M e(j+s)H∆j+sG

e−t1H

t1
j=−M ejH∆jG

...

P

e−(tm+s)H






tm

j=−M e(j+s)H∆j+sG

...

P

e−tmH

tm

j=−M ejH∆jG

law= 













P
for every −M < min{ti}. Since the random vectors above converge in probability as
M → ∞, we obtain that

P

e−(t1+s)H

Xt1+s
...
Xtm+s



= 



e−(tm+s)H






P
and hence, X is stationary.




t1
j=−∞ e(j+s)H ∆j+sG

...

tm
j=−∞ e(j+s)H ∆j+sG

P

e−t1H

t1
j=−∞ ejH ∆jG

law
= 




e−tmH






P

P

...


tm
j=−∞ ejH ∆jG



Xt1
...
Xtm






= 




Next, assume that X is stationary. Then, by Theorem 3.4 there exists a H-self-

similar Y such that

∆tX = e−tHYet − e−(t−1)H Yet−1 = (e−H − I)Xt−1 + e−tH ∆tYet.

Deﬁning G as in Lemma 3.5 completes the proof of the other direction.
To prove uniqueness, we use (14). Assume that, for G, ˜G ∈ GH ,

t

t

etH Xt =

ekH∆kG =

ekH∆k ˜G

k=−∞
X

k=−∞
X

15

 
for every t ∈ Z. Then

etH Xt − e(t−1)H Xt−1 = etH ∆tG = etH ∆t ˜G.

Since etH is invertible and both processes start from zero, we conclude that G =
˜G.

Lemma 3.6. The matrix Θ = I − e−H is positive deﬁnite.

Proof. Let a be a real vector of length n, and let H = QΛQ⊤ be an eigendecomposi-
tion of H. Then

where

a⊤(I − e−H)a = kak2 − a⊤e−Ha,

|a⊤e−H a| ≤ kak2e−λmin < kak2

completing the proof.

Proof of Lemma 2.7. We have that

∆tG(∆0G)⊤ = (Xt − ΦXt−1)(X ⊤

0 − X ⊤

−1Φ).

Taking expectations yields

r(t) = Φγ(t)Φ − γ(t + 1)Φ − Φγ(t − 1) + γ(t).

Proof of Theorem 2.10. Let

Then for t ∈ N we have

∆tX = −ΘXt−1 + ∆tG.

t

t

t

t

Gt =

∆kG =

∆kX + Θ

Xk−1 = Xt − X0 + Θ

Xk−1.

k=1
X

k=1
X

k=1
X

k=1
X

Hence

cov(Gt) = cov(Xt − X0) + E 


(Xt − X0)

t

X
k=1

X⊤

k−1





Θ + ΘE 


t

X
k=1

Xk−1(Xt − X0)

⊤



+ ΘE 


t

X
k=1

Xk−1

t

X
k=1





Xk−1

⊤





Θ





giving (7) since

E

(Xt − X0)

"

and

t

k=1
X

X ⊤

k−1

#

t

=

γ(t − k + 1) − γ(−k + 1) =

k=1
X

γ(k) − γ(k − 1)⊤

t

k=1
X

cov(Xt − X0) = E

(Xt − X0)(Xt − X0)

⊤

= 2γ(0) − γ(t) − γ(−t) = 2γ(0) − γ(t) − γ(t)

⊤

(cid:2)

(cid:3)

16

Proof of Lemma 2.15. Assume that ˜Φ = e− ˜H satisﬁes (8) for every t ∈ Z and set

Xt − ˜ΦXt−1 = ∆t ˜G,

t ∈ Z.

Consequently

˜r(t) = γ(t) ˜Φ2 − (γ(t + 1) − γ(t − 1)) ˜Φ + γ(t) = r(t),

t ∈ Z,

where ˜r(t) is the autocovariance function of (∆t ˜G)t∈Z. Now, since G0 = ˜G0 = 0, we
obtain that

t

t

t

var(Gt) = var

∆kG

=

cov(∆kG, ∆jG) =

r(k−j) = var

k,j=1
X
for all t ∈ N. Hence, both Θ and ˜Θ are solutions to (9).

k,j=1
X

k=1
X

!

t

k=1
X

∆k ˜G

!

= var( ˜Gt)

In order to show that ˆΘT is consistent, we simply need to ﬁnd suitable bounds for
∆T B, ∆T C and ∆T D in terms of the autocovariance estimators. After that, the same
strategy as in [41] can be applied.

Lemma 3.7. Set

Mt,T = max

s∈{0,1,...,t}

kˆγT (s) − γ(s)k.

Then the coefﬁcients of the perturbed CARE satisfy

k∆T Dk ≤ 4Mt,T
k∆T Ck ≤ t2Mt,T
k∆T Bk ≤ 2tMt,T ,

Proof. First, we recall ﬁrst that

kˆγT (−s) − γ(−s)k = kˆγT (s)⊤ − γ(s)⊤k = kˆγT (s) − γ(s)k.

Now, since v(t) is known,

k∆T Dk ≤ 2kˆγT (0) − γ(0)k + kˆγT (t) − γ(t)k + kˆγT (t)⊤ − γ(t)⊤k ≤ 4Mt,T .

Moreover

Finally

t

t

k∆T Ck ≤

kˆγT (k − j) − γ(k − j)k ≤ t2Mt,T .

k=1
X

j=1
X

t

k∆T Bk ≤

kˆγT (k − 1) − γ(k − 1)k + kγ(k)⊤ − ˆγT (k)⊤k ≤ 2tMt,T .

k=1
X

17

 
 
Proof of Theorem 2.17. The result follows by replacing sups∈[0,t] kˆγT (s) − γ(s)k with
Mt,T in Corollary 3.14 and in the proof of Theorem 2.9 of [41]. The details are left to
the reader.

Proof of Theorem 2.18. For the ﬁrst part of the theorem, we notice that

t

t

t

k−1

−1

t+l

t−1

t

Ct =

γ(k − j) =

γ(l) =

γ(l) +

γ(l)

k=1
X
−1

j=1
X

k=1
X

t−1

l=k−t
X

l=1−t
X

t−1

k=1
X

l=0
X
t−1

k=l+1
X

=

(t + l)γ(l) +

(t − l)γ(l) =

(t − l)γ(l) +

(t − l)γ(l)⊤,

1−t
X
−1
0 and

where

l=0
X

l=0
X

l=1
X

0
1 are interpreted as empty sums. Now we have that

P

P

t−1

∆T C =

(t − k)(ˆγT (k) − γ(k)) +

k=0
X
t

t−1

k=1
X

(t − k)

ˆγT (k)⊤ − γ(k)⊤

(cid:0)

(cid:1)

∆T B =

ˆγT (k − 1) − γ(k − 1) − ˆγT (k)⊤ + γ(k)⊤

k=1
X

∆T D = 2(γ(0) − ˆγT (0)) + ˆγT (t) − γ(t) + ˆγT (t)⊤ − γ(t)⊤

and furthermore

l(T ) vec(∆T C, ∆T B, ∆T D) = l(T ) 

P

t−1
k=0(t − k) vec(ˆγT (k) − γ(k)) +

t−1
k=1(t − k) vec

t
k=1 vec(ˆγT (k − 1) − γ(k − 1)) − vec
−2 vec(ˆγT (0) − γ(0)) + vec(ˆγT (t) − γ(t)) + vec

(ˆγT (k) − γ(k))⊤
(cid:0)

(cid:0)
(ˆγT (t) − γ(t))⊤

P

(cid:1)

P

(ˆγT (k) − γ(k))⊤

(cid:1)




(cid:1)



vec(ˆγT (0) − γ(0))
vec(ˆγT (1) − γ(1))
..
.
vec(ˆγT (t) − γ(t))








(cid:0)

law
−→ L1(Z)















= L1

l(T )








by the continuous mapping theorem. For the second part of the theorem, the proof of
the continuous time case of [41] can be applied just by replacing sups∈[0,t] kˆγT (s) −
γ(s)k with Mt,T in the deﬁnition of the set AT .

3.2 Continuous time

We only provide the proof of Lemma 2.23, while the other proofs can be found from
[41].

Proof of Lemma 2.23. Integrating (10) from 0 to t gives

Gt = Xt − X0 + H

Xsds.

t

0

Z

18

Hence

∆δ

t G(∆δ

0G)⊤ =

Xt − Xt−δ + H

Xsds

t

(cid:18)

t−δ

Z

(cid:19) (cid:18)

X ⊤

0 − X ⊤

−δ +

0

−δ

Z

X ⊤

s dsH

.

(cid:19)

Taking expectations yields

rδ(t) = 2γ(t)−γ(t+δ)−γ(t−δ)+

0

Z

−δ

γ(t−s)−γ(t−δ−s)dsH+H

t

Z

t−δ

γ(s)−γ(s+δ)ds+H

t

0

Z

t−δ Z

−δ

γ(s−u)dudsH,

where the ﬁrst order terms can be treated with a simple change of variables. For the
second order term we obtain that

t

0

t−δ Z
Z

−δ

γ(s − u)duds =

t

s+δ

t

x

t+δ

t

Z

t−δ Z
s
t

γ(x)dxds =

γ(x)dsdx +

γ(x)dsdx

t
Z

x−δ

Z

t−δ

t−δ Z
Z
t+δ

=

t−δ

Z

(x − t + δ)γ(x)dx +

(t − x + δ)γ(x)dx.

t
Z

Acknowledgments

I would like to thank Lauri Viitasaari for his comments and suggestions.

References

[1] David Applebaum et al.

Inﬁnite dimensional Ornstein-Uhlenbeck processes

driven by Lévy processes. Probability Surveys, 12:33–54, 2015.

[2] Ehsan Azmoodeh and Lauri Viitasaari. Parameter estimation based on discrete
observations of fractional Ornstein-Uhlenbeck process of the second kind. Sta-
tistical Inference for Stochastic Processes, 18(3):205–227, 2015.

[3] Richard T Baillie, Tim Bollerslev, and Hans Ole Mikkelsen. Fractionally in-
tegrated generalized autoregressive conditional heteroskedasticity. Journal of
econometrics, 74(1):3–30, 1996.

[4] Salwa Bajja, Khalifa Es-Sebaiy, and Lauri Viitasaari. Least squares estimator
of fractional Ornstein-Uhlenbeck processes with periodic mean. Journal of the
Korean Statistical Society, 46(4):608–622, 2017.

[5] Maoudo Faramba Balde, Khalifa Es-Sebaiy, and Ciprian A Tudor. Ergodic-
ity and drift parameter estimation for inﬁnite-dimensional fractional Ornstein-
Uhlenbeck process of the second kind. Applied Mathematics & Optimization,
pages 1–30, 2018.

19

[6] Dario A Bini, Bruno Iannazzo, and Beatrice Meini. Numerical solution of alge-

braic Riccati equations, volume 9. Siam, 2012.

[7] Tim Bollerslev. Generalized autoregressive conditional heteroskedasticity. Jour-

nal of econometrics, 31(3):307–327, 1986.

[8] Tim Bollerslev. Glossary to ARCH (GARCH). CREATES Research paper, 49,

2008.

[9] Peter J Brockwell and Richard A Davis. Time Series: Theory and Methods.

Springer Science & Business Media, 1991.

[10] Alexandre Brouste and Stefano M. Iacus. Parameter estimation for the discretely
observed fractional Ornstein-Uhlenbeck process and the Yuima R package. Com-
putational Statistics, 28(4):1529–1547, 2013.

[11] Ralph Byers. Solving the algebraic Riccati equation with the matrix sign func-

tion. Linear Algebra and its Applications, 85:267–279, 1987.

[12] Patrick Cheridito, Hideyuki Kawaguchi, Makoto Maejima, et al. Fractional
Ornstein-Uhlenbeck processes. Electronic Journal of probability, 8, 2003.

[13] Soukaina Douissi, Khalifa Es-Sebaiy, and Ciprian A Tudor. Hermite Ornstein-
Uhlenbeck processes mixed with a Gamma distribution. Publicationes Mathe-
maticae Debrecen, in press, 2019.

[14] Robert F Engle. Autoregressive conditional heteroskedasticity with estimates of
the variance of United Kingdom inﬂation. Econometrica: Journal of the Econo-
metric Society, pages 987–1007, 1982.

[15] Christian Francq, Jean-Michel Zakoian, et al. Maximum likelihood estimation of
pure GARCH and ARMA-GARCH processes. Bernoulli, 10(4):605–637, 2004.

[16] James Douglas Hamilton. Time series analysis, volume 2. Princeton university

press, 1994.

[17] Heejoon Han and Dennis Kristensen. Asymptotic theory for the QMLE in
GARCH-X models with stationary and nonstationary covariates. Journal of busi-
ness & economic statistics, 32(3):416–429, 2014.

[18] Edward J Hannan. The asymptotic theory of linear time-series models. Journal

of Applied Probability, 10(1):130–145, 1973.

[19] Yaozhong Hu and David Nualart. Parameter estimation for fractional Ornstein-
Uhlenbeck processes. Statistics & Probability Letters, 80(11-12):1030–1038,
2010.

20

[20] Yaozhong Hu, David Nualart, and Hongjuan Zhou. Parameter estimation for
fractional Ornstein-Uhlenbeck processes of general Hurst parameter. Statistical
Inference for Stochastic Processes, 22(1):111–142, 2019.

[21] M. L. Kleptsyna and A. Le Breton. Statistical analysis of the fractional Ornstein-
Uhlenbeck type process. Statistical Inference for Stochastic Processes, 5(3):229–
248, 2002.

[22] Vladimir Kucera. A contribution to matrix quadratic equations. IEEE Transac-

tions on Automatic Control, 17(3):344–347, 1972.

[23] John Lamperti. Semi-stable stochastic processes. Transactions of the American

Mathematical Society, 104:62–78, 1962.

[24] Peter Lancaster and Leiba Rodman. Algebraic Riccati equations. Clarendon

press, 1995.

[25] Alan Laub. A Schur method for solving algebraic Riccati equations. IEEE Trans-

actions on automatic control, 24(6):913–921, 1979.

[26] Shiqing Ling and Wai Keung Li. On fractionally integrated autoregressive
moving-average time series models with conditional heteroscedasticity. Journal
of the American Statistical Association, 92(439):1184–1194, 1997.

[27] Shiqing Ling and Michael McAleer. Asymptotic theory for a vector ARMA-

GARCH model. Econometric theory, 19(2):280–310, 2003.

[28] Marcin Magdziarz. Fractional Ornstein–Uhlenbeck processes. Joseph effect in
models with inﬁnite variance. Physica A: Statistical Mechanics and its Applica-
tions, 387(1):123–133, 2008.

[29] Jose Alberto Mauricio.

ary vector ARMA models.
90(429):282–291, 1995.

Exact maximum likelihood estimation of station-
Journal of the American Statistical Association,

[30] Thomas Mikosch, Tamar Gadrich, Claudia Kluppelberg, and Robert J Adler. Pa-
rameter estimation for ARMA models with inﬁnite variance innovations. The
Annals of Statistics, 23(1):305–326, 1995.

[31] Klaus Neusser et al. Time series econometrics. Springer, 2016.

[32] Ivan Nourdin and TT Diu Tran. Statistical inference for Vasicek-type model
Stochastic Processes and their Applications,

driven by Hermite processes.
129(10):3774–3791, 2019.

[33] Tommi Sottinen and Lauri Viitasaari. Parameter estimation for the Langevin
Statistical Inference for

equation with stationary-increment Gaussian noise.
Stochastic Processes, 2017.

21

[34] Ji-guang Sun. Perturbation theory for algebraic Riccati equations. SIAM Journal

on Matrix Analysis and Applications, 19(1):39–65, 1998.

[35] Katsuto Tanaka. Maximum likelihood estimation for the non-ergodic fractional
Statistical Inference for Stochastic Processes,

Ornstein-Uhlenbeck process.
18(3):315–332, 2015.

[36] George C Tiao and Ruey S Tsay. Consistency properties of least squares esti-
mates of autoregressive parameters in ARMA models. The Annals of Statistics,
11(3):856–871, 1983.

[37] Lauri Viitasaari. Representation of stationary and stationary increment processes
via Langevin equation and self-similar processes. Statistics & Probability Letters,
115:45–53, 2016.

[38] Marko Voutilainen, Lauri Viitasaari, and Pauliina Ilmonen. On model ﬁtting
and estimation of strictly stationary processes. Modern Stochastics: Theory and
Applications, 4(4):381–406, 2017.

[39] Marko Voutilainen, Lauri Viitasaari, and Pauliina Ilmonen. Note on AR(1)-
characterisation of stationary processes and model ﬁtting. Modern Stochastics:
Theory and Applications, 6(2):195–207, 2019.

[40] Marko Voutilainen, Lauri Viitasaari, Pauliina Ilmonen, Soledad Torres, and
Ciprian Tudor. On generalized ARCH model with stationary liquidity. ArXiv
e-prints: 1806.08608, 2018.

[41] Marko Voutilainen, Lauri Viitasaari, Pauliina Ilmonen, Soledad Torres, and
Ciprian Tudor. Vector-valued Generalised Ornstein-Uhlenbeck Processes. ArXiv
e-prints: 1909.02376, 2019.

[42] William M Wonham. On a matrix Riccati equation of stochastic control. SIAM

Journal on Control, 6(4):681–697, 1968.

[43] Michael Yuanjie Zhang, Jeffrey R Russell, and Ruey S Tsay. A nonlinear au-
toregressive conditional duration model with applications to ﬁnancial transaction
data. Journal of Econometrics, 104(1):179–207, 2001.

22

