9
1
0
2

v
o
N
8

]
T
S
.
h
t
a
m

[

2
v
0
8
3
5
0
.
1
0
9
1
:
v
i
X
r
a

Joint temporal and contemporaneous aggregation of

random-coeﬃcient AR(1) processes with inﬁnite variance

Vytaut˙e Pilipauskait˙e1, Viktor Skorniakov2, Donatas Surgailis2

1Aarhus University, Department of Mathematics, Ny Munkegade 118, 8000 Aarhus C, Denmark
2Vilnius University, Faculty of Mathematics and Informatics, Naugarduko 24, 03225 Vilnius, Lithuania

November 12, 2019

Abstract

We discuss joint temporal and contemporaneous aggregation of N independent copies of random-
coeﬃcient AR(1) process driven by i.i.d. innovations in the domain of normal attraction of an α-stable
distribution, 0 < α ≤ 2, as both N and the time scale n tend to inﬁnity, possibly at a diﬀerent rate.
Assuming that the tail distribution function of the random autoregressive coeﬃcient regularly varies at the
unit root with exponent β > 0, we show that, for β < max(α, 1), the joint aggregate displays a variety of
stable and non-stable limit behaviors with stability index depending on α, β and the mutual increase rate
of N and n. The paper extends the results of Pilipauskait˙e and Surgailis (2014) from α = 2 to 0 < α < 2.

Keywords: Autoregressive model; Panel data; Mixture distribution; Inﬁnite variance, Long-range de-

pendence; Scaling transition; Poisson random measure; Asymptotic self-similarity.

2010 MSC: 60F05, 62M10.

1 Introduction

Contemporaneous aggregation of random-coeﬃcient AR(1) (RCAR(1)) processes is an important model for
long-range dependence (LRD, also often referred to as long memory) in econometrics, see Granger [12],
Robinson [33], Zaﬀaroni [36], Beran et al. [3]. It explains how LRD can arise in a time series of macroeconomic
variable, which is aggregate such as average or sum over a very large number of diﬀerent micro-variables, each
evolving by AR(1) with a random coeﬃcient. The concentration of the distribution of a random autoregressive
coeﬃcient a at the unit root a = 1, governed by the parameter β > 0 in

P(a > 1 − x) ∼ const xβ,

x ↓ 0,

(1.1)

determines various properties of both the RCAR(1) process and the (limit) aggregate. Particularly, for
1 < β < 2, the RCAR(1) process exhibits LRD in the sense that its autocovariance function is absolutely non-
summable since it decays slowly like t1−β as the time lag t between two observations increases. Furthermore,
the limit of the normalized aggregate of independent RCAR(1) processes is a Gaussian process, which has
the same autocovariance function and can obey a particular case of ARFIMA model, see Granger [12] and
the review paper Leipus et al. [19].

1

 
 
 
 
 
 
Statistical inference for RCAR(1) model, especially estimation of the distribution of a random coeﬃcient,
has been extensively studied, see Robinson [33], Beran et al. [2], Celov et al. [4, 5], Jirak [14], Leipus et al. [18,
20, 21]. Most of these papers deal with a panel {Xi(t), i = 1, . . . , N, t = 1, . . . , n} of N independent RCAR(1)
processes observed over the time-period of length n. As N and n increase, possibly at diﬀerent rate, statistical
(dependence) properties of such a panel are determined by the parameter β in (1.1). Particularly, Pilipauskait˙e
n
and Surgailis [27] proved that, for 1 < β < 2, the distribution of the sample mean (N n)−1
t=1 Xi(t)
is asymptotically normal if N 1/β/n → ∞, and it is symmetric β-stable if N 1/β/n → 0. In the ‘intermediate’
case N 1/β/n → µ ∈ (0, ∞), this limit distribution is more complicated and has an integral representation
with respect to (w.r.t.) a certain Poisson random measure. Leipus et al. [22] studied the limit distribution of
sample variance and sample covariances for such an RCAR(1) panel.

N
i=1

P

P

All the above works refer to the case of ﬁnite-variance innovations, however, the RCAR(1) model with
inﬁnite variance also presents considerable interest since heavy tails are important in ﬁnancial modeling
(see, e.g. Mikosch [24] and the references therein). Puplinskait˙e and Surgailis [31] studied contemporaneous
aggregation of independent copies {Xi(t), t ∈ Z}, i = 1, 2, . . . , of an RCAR(1) process

X(t) = aX(t − 1) + ε(t),

t ∈ Z,

(1.2)

where {ε(t), t ∈ Z} is a sequence of i.i.d. random variables (r.v.s) belonging to the domain of normal attraction
of an α-stable distribution, 0 < α ≤ 2, and the autoregressive coeﬃcient a ∈ [0, 1) is an r.v. independent of
{ε(t), t ∈ Z} and having a density φ(x), x ∈ [0, 1), such that

φ(x) ∼ ψ1(1 − x)β−1,

x ↑ 1,

(1.3)

N
for some β > 0 and ψ1 > 0. In [31] it was proved that, for β > 1, the normalized aggregate {N −1/α
i=1 Xi(t),
t ∈ Z} tends (in the sense of weak convergence of ﬁnite-dimensional distributions) to the α-stable mixed
moving average process ¯X given by

P

¯X(t) :=

Z[0,1)

Xs≤t

xt−sMs(dx),

t ∈ Z,

(1.4)

where {Ms(dx), s ∈ Z} are independent copies of an α-stable random measure M (dx) on [0, 1) with control
measure P(a ∈ dx). For 1 < β < α, the limit aggregate ¯X has distributional LRD in the sense that its partial
sums normalized by nH, H := 1 − (β − 1)/α ∈ (1/α, 1), tend to an α-stable, H-self-similar process Λα,β with
stationary dependent increments. See Section 2 for its deﬁnition.

In this paper we study joint temporal and contemporaneous aggregation of independent copies of RCAR(1)
process in (1.2), driven by i.i.d. α-stable or related inﬁnite variance innovations with a random autoregressive
coeﬃcient as in (1.3). We assume that both the number N of individual processes and the time scale n
tend to inﬁnity, possibly at a diﬀerent rate and extend the results of Pilipauskait˙e and Surgailis [27], who
considered the ﬁnite variance case α = 2. It turns out that, similarly to [27], the limit behavior of the joint
aggregate

N

[nτ ]

SN,n(τ ) :=

Xi(t),

τ ≥ 0,

(1.5)

Xt=1
depends on β and the mutual increase rate of N, n; moreover, it also depends on α leading to a complex
panorama of the limit distributions. Theorem 2.2 below provides a nearly complete description of these limit
distributions of suitably normalized SN,n = {SN,n(τ ), τ ≥ 0} in terms of parameters 0 < α ≤ 2, β > 0 (with

Xi=1

2

Parameter region Mutual increase rate of N, n Limit distribution

N 1/β/n → ∞
1 ≤ β < α
0 < β < min(α, 1) N 1/β/n → ∞
N 1/β/n → 0
0 < β < α
N 1/β/n → µ ∈ (0, ∞)
0 < β < α
N 1/γβ/n → ∞
N 1/γβ/n → 0
N 1/γβ/n → µ ∈ (0, ∞)
arbitrary

α < β < 1
α < β < 1
α < β < 1

β > max(α, 1)

α-stable
(αβ)-stable
β-stable
‘intermediate Poisson’

(αβ)-stable
α-stable
(αβ)-stable + α-stable

α-stable

Table 1: Limit distribution of the sample mean or SN,n(1) in (1.5), for 0 < α ≤ 2, β > 0 with γ := 1−α
1−β .

exception of α = β and α < β = 1), as N, n → ∞. In Table 1 we summarize the results of Theorem 2.2 for
the sample mean (N n)−1SN,n(1), including the cases when the mean of X(t) and SN,n(1) does not exist.

The description in Table 1 is not very precise and needs some comments. Let us ﬁrst note that the
stable distributions in Table 1 are generally not symmetric and in some cases they are supported on R+ :=
(0, ∞). The terminology ‘intermediate Poisson’ (borrowed from [28, 22]) refers to a certain inﬁnitely divisible
distribution written as an integral w.r.t. a Poisson random measure. The sum ‘(αβ)-stable + α-stable’ in
Table 1 indicates the convolution of two distributions with diﬀerent stability indices (a rather unusual result
in limit theorems of the probability theory).

Intuitively, the results in Table 1 can be explained by discussing the results of Theorem 2.1 which deals with
the iterated limits of suitably normalized SN,n in (1.5) when ﬁrst N → ∞ and then n → ∞, or vice versa. The
iterated limits are generally simpler to derive, and the joint limits in Theorem 2.2 can be regarded as some
kind of ‘interpolation’ between the former limits. These limits are generally diﬀerent in diﬀerent parameter
regions leading to three parameter regions: (i) 0 < β < α, (ii) 0 < α < β < 1, and (iii) β > max(α, 1) in
Theorem 2.2.

α

P

N
i=1

First, let us note that, for β > max(α, 1), our all limits are relatively simple and coincide since SN,n(1)
behaves as a sum κ1/α
n
t=1 εi(t) of i.i.d. r.v.s in the domain of attraction of an α-stable distribution
with κα := E(1 − a)−α < ∞; see the proof of Theorem 2.2(iii). Hence, we can turn our attention to the
parameter region 0 < β < max(α, 1), where the iterated limits depend on the order and so the joint limits
depend on the mutual rate of N, n → ∞. Let us note that in the region (i) 0 < β < α the results of Theorems
2.1 and 2.2 naturally extend those of [27] from α = 2 to 0 < α < 2, whereas in the parameter region (ii)
0 < α < β < 1 (which does not occur in [27]) they are less predictable and somewhat surprising.

P

The iterated limits limn→∞ limN→∞ (relations (2.15), (2.16) of Theorem 2.1) essentially follow from [31]
since they reduce to the α-stable partial sums limit Λα,β of ¯X in (1.4) for 1 < β < α, while for β < 1, the
limit aggregate ¯X is a random (αβ)-stable constant Vα,β, see [31, Proposition 2.3] and the proof of (2.16) of
Theorem 2.1. (However, the case 1 = β < α is more delicate and requires a separate treatment, see the proof
of (2.17).) These observations may explain the two ﬁrst lines in Table 1. The third line in Table 1 may be
explained by the iterated limit limN→∞ limn→∞ in (2.18), which in turn relies on the (conditional) α-stable
partial sums limit as n → ∞ in (4.10) of the AR(1) process (1.2) for ﬁxed a ∈ [0, 1). Unconditionally, the last
limits have β-tails and then (2.18) turns out to be a sub-α-stable process with β-stable ﬁnite-dimensional

3

distributions in agreement with the third line of Table 1.

Obviously, the iterated limits are not useful to explain the fourth line in Table 1 which is part of Theo-
rem 2.2(i) and one of the main results of this paper. The intermediate (Poisson) process Zα,β = {Zα,β(τ ), τ ≥
0} is deﬁned in (2.14) and discussed in Section 3. There, we give its integral representation w.r.t. a Poisson
random measure on the product space R+ × D(R), where D(R) is the Skorohod space of cadlag functions
on R, and study its properties. We show that Zα,β plays a role of a bridge between the limiting processes
in the extreme cases µ = ∞ and µ = 0 of Theorem 2.2(i), because it is asymptotically locally and globally
self-similar with these processes being its tangent processes; see Proposition 3.1(v).

Finally, let us turn our attention to lines 5-7 of Table 1 (parameter region 0 < α < β < 1), which may
be described as the very strong dependence (β < 1) and even stronger variability (α < β) in the RCAR(1)
model (1.2). This ‘regime’ is a new one since it could not happen in [27] where α = 2. The results are part
of Theorem 2.2(ii). We see from the iterated limits in (2.16) and (2.19) that the joint limit ‘chooses’ between
two extreme behaviors: the (αβ)-stable random line {Vαβ τ, τ ≥ 0} with ‘inﬁnite memory of increments’,
and the α-stable L´evy process {κ1/α
α ζα(τ ), τ ≥ 0} with ‘zero memory of increments’. The ‘winner’ of this
‘competition of limit behaviors’ is determined by equating respective normalizations: nN 1/(αβ) = (N n)1/α
leads to N = nγβ with γ as in Table 1, which agrees with Table 1 and Theorem 2.2(ii). Needless to say, the
above argument is heuristic, the proof of Theorem 2.2 is more involved and does not follow from Theorem 2.1.
The proofs in the present paper (as well as [27, 22] and some other related work) clearly proﬁt from the
detailed structure of the pre-limit AR(1) process, raising the question of their robustness in a more general
context. Remark 2.2 discusses possible extensions to higher order RCAR models which seem feasible but
technically not easy. We also note that our results can be put in a general framework of limit theorems for
spatio-temporal models with LRD. See the doctoral dissertation [26]. In particular, they are related to the
studies of the accumulated workload in network traﬃc under LRD, as the time scale n and the number N of
independent sources simultaneously increase, possibly at a diﬀerent rate. See [35, 23, 10, 9, 16, 6]. See also [27]
for a comparison between the joint temporal and contemporaneous aggregation of RCAR(1) processes and
that of network traﬃc models with ﬁnite variance and the corresponding limit processes. Interestingly, the
intermediate limit of the accumulated workload process has also an integral though diﬀerent representation
w.r.t. a certain Poisson random measure and can be regarded as a ‘bridge’ between limit processes arising in
the other two scaling regimes. We note that joint aggregation of some network traﬃc models with inﬁnite
variance and LRD was studied in Levy and Taqqu [17], Pipiras et al. [29], Kaj and Taqqu [16].

Notation. In what follows, C stands for a positive constant whose precise value is unimportant and may
change from line to line. We denote by =d, →d the equality in distribution and convergence in distribution,
respectively. We also write →fdd and (fdd) lim for the weak convergence and limit of ﬁnite-dimensional
distributions.

2 Main results

2.1 Assumptions

Deﬁnition 2.1. Let 0 < α ≤ 2. Write ε ∈ D(α) if the distribution of an r.v. ε satisﬁes the following
conditions:

• for α = 2, Eε2 < ∞;

4

• for 0 < α < 2, there exist some ﬁnite constants c1, c2 ≥ 0 such that

lim
x→∞

xαP(ε > x) = c1,

lim
x→−∞

|x|αP(ε ≤ x) = c2,

c1 + c2 > 0;

• in addition to the above, Eε = 0 for 1 < α ≤ 2, and, for α = 1, the distribution of ε is symmetric.

Remark 2.1. Assumption ε ∈ D(α) implies that ε belongs to the domain of normal attraction of an α-stable
distribution. That is, for a sequence {ε(t), t = 1, 2, . . . } of independent copies of ε,

n−1/α

[nτ ]

Xt=1

ε(t) →fdd

ζα(τ ),

(2.1)

where ζα = {ζα(τ ), τ ≥ 0} is an α-stable L´evy process having characteristic function (see [8, pages 574–581])

Eeiθζα(τ ) = e−τ |θ|αω(θ),

θ ∈ R, with

ω(θ) :=

Γ(2−α)

1−α ((c1 + c2) cos( απ
(c1 + c2) π
2 ,
1
2 Eε2,





2 ) − i(c1 − c2) sign(θ) sin( απ

2 )), α 6= 1, 2,

α = 1,

α = 2.

Furthermore, assumption ε ∈ D(α) implies E|ε|p < ∞ for any 0 < p < α.

(2.2)

(2.3)

In what follows, we assume that {ε(t), t ∈ Z} in (1.2) are independent copies of ε ∈ D(α) for some
0 < α ≤ 2. Moreover, we assume that a is an absolutely continuous r.v. having density φ which is supported
on [0, 1) and admits the representation

φ(u) = ψ(u)(1 − u)β−1,

u ∈ [0, 1),

(2.4)

for some β > 0 and some integrable function ψ(u), u ∈ [0, 1), having ﬁnite limit lim
u↑1

ψ(u) =: ψ1 > 0. The

same assumption is made in [27, 31] and other related works. Then there exists a unique stationary solution
of (1.2) given by

X(t) =

at−sε(s),

t ∈ Z,

Xs≤t

(2.5)

where the series on the r.h.s. of (2.5) converges in Lp for 0 < p < α min(β, 1) if 0 < α < 2; and for 0 < p ≤ 2
such that p < 2β if α = 2. For almost every a ∈ [0, 1), the series on the r.h.s. of (2.5) converges conditionally
a.s. and conditionally in Lp for 0 < p < α if 0 < α < 2; and for 0 < p ≤ 2 if α = 2. See [30] for details.

2.2 Limiting processes

For 1 < β < α ≤ 2, we deﬁne a stochastic process Λα,β = {Λα,β(τ ), τ ≥ 0} by

Λα,β(τ ) :=

fτ (x, s)M (dx, ds), where

(2.6)

ZR+×R
τ

fτ (x, s) :=

Z

0

e−x(t−s)1(s ≤ t)dt,

τ ≥ 0, x > 0, s ∈ R,

and M (dx, ds) is an α-stable random measure on R+ × R with a control measure m(dx, ds) := ψ1xβ−1dxds
such that EeiθM (B) = e−|θ|αω(θ)m(B), θ ∈ R, for every Borel set B ⊂ R+ × R with m(B) < ∞ and ω,

5

ψ1 given in (2.3), (2.4). The process Λα,β was introduced in [31]. It is α-stable, H-self-similar with H =
1 − (β − 1)/α ∈ (1/α, 1), has stationary dependent increments, and is related to the integrated superposition
of Ornstein-Uhlenbeck processes discussed in Barndorﬀ-Nielsen [1]. See also [11]. The joint characteristic
function of Λα,β is given by

E exp

d

i

n

Xj=1

θjΛα,β(τj)

o

= exp

−

n

ZR+×R (cid:12)
(cid:12)

Xj=1

θjfτj (x, s)
(cid:12)
(cid:12)

d

αω

d

θjfτj (x, s)
(cid:1)

(cid:0)

Xj=1

m(dx, ds)

,
o

(2.7)

for θj ∈ R, τj ≥ 0, j = 1, . . . , d, d ∈ N. For α = 2, Λ2,β is a Gaussian process with mean zero and the
autocovariance function

EΛ2,β(τ1)Λ2,β(τ2) = Eε2

ZR+×R

fτ1(x, s)fτ2(x, s)m(dx, ds) =

σ2
β
2

(τ 2H

1 + τ 2H

2 − |τ1 − τ2|2H ),

τ1, τ2 ≥ 0. (2.8)

It follows that Λ2,β is a fractional Brownian motion with Hurst index H = (3 − β)/2 and variance EΛ2
β = ψ1Γ(β − 1)Eε2/(2 − β)(3 − β).
σ2
Next, for 0 < λ < 1, 0 < α ≤ 2, β > 0, let Wλ,α,β > 0 be a λ-stable r.v. with Laplace transform

2,β(1) =:

Ee−θWλ,α,β = e−κλ,α,βθλ

,

θ ≥ 0, where

(2.9)

∞

κλ,α,β := ψ1 Z

0

(1 − exp{−(λα/β)−1x−β/λ})xβ−1dx =

ψ1Γ(1 − λ)
(λα/β)λβ

> 0.

It is well-known (see, e.g., [37, Theorem 2.6.1]) that the Laplace transform in (2.9) extends to all complex
numbers θ ∈ C with Re(θ) ≥ 0. Assume that Wλ,α,β is independent of the L´evy process ζα in (2.1). Deﬁne

Vα,β := W 1/α
Wα,β(τ ) := W 1/α

β,α,β ζα(1),

β/α,α,β ζα(τ ),

0 < β < 1,

τ ≥ 0,

0 < β < α.

(2.10)

Then, using (2.9), we obtain for any θ ∈ R,

EeiθVα,β = Ee−|θ|αω(θ)Wβ,α,β = exp{−κβ,α,β|θ|αβ(ω(θ))β},

0 < β < 1,

(2.11)

EeiθWα,β (τ ) = Ee−τ |θ|αω(θ)Wβ/α,α,β = exp{−κβ/α,α,βτ β/α|θ|β(ω(θ))β/α},

0 < β < α,

where

κβ,α,β =

ψ1
αββ

Γ(1 − β),

κβ/α,α,β =

ψ1
β

Γ(1 −

β
α

).

(2.12)

From (2.11), it follows that r.v.s Vα,β and Wα,β(τ ) are stable with respective stability indices αβ < α and
β < α. In a similar way, it follows that Wα,β = {Wα,β(τ ), τ ≥ 0} has β-stable ﬁnite dimensional distributions.
Following [34, Section 3.8], we call the stochastic processes in (2.10) sub-stable. We note that Wα,β enjoys the
stationary increment and H-self-similarity with H = 1/α properties which it inherits from the L´evy process
ζα. For β = 1 < α ≤ 2, introduce also an α-stable r.v. Vα,1 with a characteristic function

EeiθVα,1 = e−(ψ1/α)|θ|αω(θ),

θ ∈ R.

(2.13)

Since limβ↑1(1 − β)Γ(1 − β) = 1, it follows that (1 − β)1/(αβ)Vα,β →d Vα,1 as β ↑ 1. The above discontinuity
of the distribution of Vα,β at β = 1 can be explained by the additional logarithmic normalization in (2.17)
and (2.23).

6

Finally, for 0 < β < α ≤ 2, we deﬁne a random process Zα,β = {Zα,β(τ ), τ ≥ 0} through its joint

characteristic function:

d

d

i

n

o

Xj=1

E exp

= exp

θjZα,β(τj)

,
o
(2.14)
where θj ∈ R, τj ≥ 0, j = 1, . . . , d, d ∈ N and fτ (x, s) is given in (2.6). A stochastic integral representation
and various properties of Zα,β are discussed in Section 3.

θjfτj (x, s)
(cid:12)
(cid:12)

θjfτj (x, s)
(cid:1)

ψ1 ZR+ (cid:16)
n

ZR (cid:12)
(cid:12)

Xj=1

Xj=1

exp

− 1

ds

−

o

n

(cid:0)

xβ−1dx
(cid:17)

d

αω

2.3 Limit theorems

In Theorems 2.1 and 2.2, the process SN,n = {SN,n(τ ), τ ≥ 0} is the joint aggregate in (1.5) of independent
copies of the RCAR(1) process X = {X(t), t ∈ Z} in (2.5) satisfying the above-stated assumptions for some
0 < α ≤ 2, some β > 0 and some ψ1 > 0. Theorem 2.1 discusses iterated limits when N → ∞ followed
by n → ∞ (limits (2.15), (2.16)), or vice versa (limits (2.18), (2.19)). Let κα := E(1 − a)−α when the last
expectation exists.

Theorem 2.1. Let 0 < β < max(α, 1). Then

(fdd) lim
n→∞

lim
N→∞

(fdd) lim
n→∞

n−1+(β−1)/αN −1/αSN,n(τ ) = Λα,β(τ ),

1 < β < α,

n−1N −1/(αβ)SN,n(τ ) = Vα,β τ,

lim
N→∞
n−1(N log N )−1/αSN,n(τ ) = Vα,1τ,

0 < β < 1,

1 = β < α,

(fdd) lim
n→∞

and

lim
N→∞

(fdd) lim
N→∞

lim
n→∞

(fdd) lim
N→∞

lim
n→∞

N −1/βn−1/αSN,n(τ ) = Wα,β(τ ),

0 < β < α,

N −1/αn−1/αSN,n(τ ) = κ1/α

α ζα(τ ),

0 < α < β < 1.

(2.15)

(2.16)

(2.17)

(2.18)

(2.19)

The following Theorem 2.2 discusses joint limits of appropriately normalized SN,n under simultaneous
increase of N, n. As noted in the Introduction, these limits depend on the mutual increase rate of N, n and
the parameters α, β. In (2.29) below, Vα,β and ζα are mutually independent.

Theorem 2.2. (i) Let 0 < β < α. Let N, n → ∞ so that

N 1/β
n

→ µ ∈ [0, ∞].

Then:

N −1/αn−1+(β−1)/αSN,n(τ ) →fdd Λα,β(τ ),

µ = ∞, 1 < β < α,

N −1/(αβ)n−1SN,n(τ ) →fdd Vα,β τ
(N log(N/n))−1/αn−1SN,n(τ ) →fdd Vα,1τ,

N −1/βn−1/αSN,n(τ ) →fdd Wα,β(τ ),
N −1/βn−1/αSN,n(τ ) →fdd µ1/αZα,β(τ /µ),

µ = ∞, 0 < β < min(α, 1),

µ = ∞, 1 = β < α,

µ = 0, 0 < β < α,

µ ∈ (0, ∞), 0 < β < α.

7

(2.20)

(2.21)

(2.22)

(2.23)

(2.24)

(2.25)

(ii) Let 0 < α < β < 1. Let N, n → ∞ so that

N 1/(γβ)
n

→ µ ∈ [0, ∞], where γ :=

1 − α
1 − β

> 1.

Then:

N −1/(αβ)n−1SN,n(τ ) →fdd Vα,β τ,
(N n)−1/αSN,n(τ ) →fdd κ1/α
(N n)−1/αSN,n(τ ) →fdd µ(1/α)−1Vα,β τ + κ1/α

α ζα(τ ),

µ = ∞,

µ = 0,

α ζα(τ ), µ ∈ (0, ∞).

(iii) Let β > max(α, 1). Then, as N, n → ∞ in arbitrary way,

(N n)−1/αSN,n(τ ) →fdd κ1/α

α ζα(τ ).

(2.26)

(2.27)

(2.28)

(2.29)

(2.30)

Remark 2.2. We expect that results in Theorems 2.1 and 2.2, as well as in [27], can be extended to higher
order RCAR models, making suitable assumptions about the mixing distribution. Particularly, Oppenheim
and Viano [25] discussed long memory properties of RCAR(2p), p ≥ 1, model with autoregressive polynomial
having one positive, one negative and p − 1 pairs of nonreal (complex conjugate) roots whose moduli are
assumed to be independent r.v.s whose densities have power-law behavior at 1, similar to (1.3), for possibly
diﬀerent exponents βi. As shown in [25], these assumptions lead to oscillating asymptotics thus seasonal
behavior of the autocovariance function of the RCAR process. A more forthright higher-order version of the
RCAR(1) equation in (1.2) is the RCAR(p) model with real positive roots, viz.,

(1 − a1B) · · · (1 − apB)X(t) = ε(t),

t ∈ Z,

(2.31)

where ai ∈ [0, 1), i = 1, . . . , p, are independent r.v.s and BX(t) = X(t − 1) is the backward shift. The
s≤t b(t − s)ε(s), t ∈ Z, where b(t) :=
stationary solution of (2.31) is written as a MA process X(t) =

0≤s1≤···≤sp−1≤t as1

1 as2−s1

2

P

· · · at−sp−1
p

satisfy

P

∞

Xt=0

p

b(t) =

Yi=1

(1 − ai)−1 =: A.

Particularly, it follows that given a1, . . . , ap, conditionally

n−1/2

[nt]

Xt=1

X(t) →fdd AB(τ )

(2.32)

(2.33)

which agrees with (4.10) below for p = 1, α = 2. In the case when each ai has a density satisfying a similar
relation as in (1.3) for some βi > 0, ψ1i > 0, i = 1, . . . , p, the (random) factor A in (2.33) has a heavy-
tailed distribution with tail parameter βmin := min1≤i≤p βi, see [7, Corollary p. 245], and we can expect that,
for βmin < α = 2, the suitably normalized iterated limit (fdd) limN→∞ limn→∞ SN,n(τ ) is the sub-Gaussian
process W2,βmin(τ ), following the proofs of Theorem 2.1 (2.18) or [27, Theorem 2.1 (2.10)]. We also then
expect that the suitably normalized iterated limit (fdd) limn→∞ limN→∞ SN,n(τ ) is a fractional Brownian
motion with Hurst parameter H = (3 − βmin)/2 (or a stable self-similar process in the case when X(t) has
inﬁnite variance). A challenging open problem is to make the above argument rigorous and to extend it to
joint limits of SN,n as in Theorem 2.2.

8

3 The intermediate process

This section discusses properties of the intermediate process Zα,β introduced in (2.14) via its ﬁnite-dimensional
characteristic function. We study Poisson stochastic integral representation, local and global self-similarity,
a.s. continuity and other properties of Zα,β. The results extend [27, Proposition 3.1] from α = 2 to 0 < α < 2.
Roughly speaking, the Poisson integral representation of Zα,β is obtained by replacing the Brownian motion
in [27] by L´evy process ζα. However, some properties of Zα,β are not ‘continuous’ at α = 2, particularly,
the second moment of Zα,β does not exist for α < 2 while Z2,β may have higher moments than 2, see [27].
Clearly, these moment diﬀerences between the cases α < 2 and α = 2 are related to the diﬀerences between
the α-stable L´evy process ζα, α < 2, and the Brownian motion ζ2 = B.

Assume that the homogeneous L´evy process ζα in (2.1) is extended to the whole real line R and induces a
probability measure Pα on the Borel sets of the Skorohod space D(R) of cadlag functions from R to R. We
start with a family

z(τ ; x) :=

ZR

fτ (x, s)dζα(s),

τ ≥ 0, x > 0,

(3.1)

of integrated Ornstein-Uhlenbeck processes driven by ζα, where fτ (x, s) is deﬁned in (2.6). The process Zα,β
can be deﬁned by ‘mixing’ the above elementary processes of (3.1) on the path space D(R) of the L´evy
process as follows.

Let N (dx, dζ) denote a Poisson random measure on the product space R+ × D(R) with a mean ν(dx, dζ) =
ψ1xβ−1dx × Pα(dζ), where ψ1 > 0, 0 < β < α ≤ 2. Then Zα,β = {Zα,β(τ ), τ ≥ 0} can be deﬁned as a
stochastic integral with respect to the above Poisson measure:

Zα,β(τ ) :=

Z(0,1)×D(R)

z(τ ; x)N (dx, dζ) +

Z[1,∞)×D(R)

z(τ ; x)
(cid:0)

N (dx, dζ) − ν(dx, dζ)1(α > 1)
(cid:1)

.

(3.2)

If 1 < α ≤ 2, 1/α < β < α, then the two integrals in (3.2) can be combined into a single one:

Zα,β(τ ) =

ZR+×D(R)

z(τ ; x)
(cid:0)

.
N (dx, dζ) − ν(dx, dζ)
(cid:1)

(3.3)

These and other properties of Zα,β are stated in the following proposition (we refer to [27, 32] for general
properties of stochastic integrals w.r.t. Poisson random measure).

Proposition 3.1. (i) The process Zα,β in (3.2) is well-deﬁned for any 0 < β < α ≤ 2. It has stationary
increments, inﬁnitely divisible ﬁnite-dimensional distributions, and the joint characteristic function given by
(2.14).

(ii) If 0 < β < α < 2, then E|Zα,β(τ )|p < ∞ for any 0 < p < α min(β, 1).
E|Zα,β(τ )|p < ∞ for any 0 < p < 2β.

If 0 < β < α = 2, then

(iii) For 1 < α ≤ 2, 1/α < β < α, Zα,β can be deﬁned as in (3.3) and EZα,β(τ ) = 0. Moreover, E|Zα,β(τ )|2 <
∞ if and only if 1 < β < α = 2, in which case

E[Z2,β(τ1)Z2,β(τ2)] =

σ2
β
2

(τ 2H

1 + τ 2H

2 − |τ1 − τ2|2H ),

τ1, τ2 ≥ 0,

where H = (3 − β)/2 and σ2

β are the same as in (2.8).

(iv) For 1 < α ≤ 2, 1/α < β < α, Zα,β is a.s. continuous.

9

(v) (Asymptotic self-similarity.) As c → 0,

c−1+(β−1)/αZα,β(cτ ) →fdd Λα,β(τ ),

1 < β < α,

c−1Zα,β(cτ ) →fdd Vα,β τ,
c−1(log(1/c))−1/αZα,β(cτ ) →fdd Vα,1τ,

0 < β < min(α, 1),

1 = β < α,

where Vα,1, Vα,β and Λα,β are deﬁned in (2.13), (2.10) and (2.6), respectively. For 0 < β < α, as c → ∞,

where Wα,β is deﬁned in (2.10).

c−1/αZα,β(cτ ) →fdd Wα,β(τ ),

Remark 3.1. With Proposition 3.1(v) in mind, we may say Zα,β plays the role of a bridge between the limit
processes in Theorem 2.2(i). For α 6= 1, the limit processes Wα,β, Λα,β and r.v. Vα,β in Proposition 3.1(v) have
diﬀerent stability indices β, α and αβ, respectively, so we conclude that one-dimensional distributions Zα,β(τ )
are not stable. For α 6= 1, the process Zα,β is also not self-similar, because Wα,β, Λα,β and {Vα,β τ, τ ≥ 0}
have diﬀerent self-similarity indices.

Remark 3.2. If Z1, Z2, . . . , ZN are independent copies of Z := Zα,β, then, for any N ∈ N,

Z(τ /N 1/β ) =fdd N −1/(αβ)−1/β

N

Xi=1

Zi(τ ).

(3.4)

Relation (3.4) follows from inﬁnite divisibility of Poisson random measure N (dx, dζ) in the stochastic integral
representation (3.2) or the characteristic function (2.14). See also ([27], (3.30)) where the above property
is related to the aggregate-similarity property introduced in Kaj [15]. For 0 < β < min(α, 1), (3.4) and
Proposition 3.1(v) imply that N 1/βZ(τ /N 1/β ) =d N −1/(αβ)
It follows
that for a ﬁxed τ > 0, the (marginal) distribution of Z(τ ) ≡ Zα,β(τ ) belongs to the domain of normal
attraction of an (αβ)-stable distribution, that is, Zα,β(τ ) ∈ D(αβ) except possibly for the case αβ = 1, when
N
the distribution of Zα,β(τ ) is not symmetric. Similarly, N H/βZ(τ /N 1/β) =d N −1/α
i=1 Zi(τ ) →d Λα,β(τ ),
where H = 1 − (β − 1)/α, implying Zα,β(τ ) ∈ D(α) for 1 < β < α. These facts entail the precise asymptotic
behavior of tail probabilities of Zα,β(τ ) for α < 2 and τ > 0 ﬁxed, particularly, they show that condition
p < α min(β, 1) in Proposition 3.1 (ii) cannot be improved.

N
i=1 Zi(τ ) →d Vα,βτ as N → ∞.

P

P

Remark 3.3. Let 0 < α < β < 1. The limit process {Z ∗
α ζα(τ ), τ ≥ 0} in (2.29),
Theorem 2.2(ii) can be also regarded as a ‘bridge’ between the other two limit processes in (2.27) and (2.28)
since it is both locally and globally asymptotically self-similar:

α,β(τ ) := Vα,β τ + κ1/α

c−1Z ∗
c−1/αZ ∗

α,β(cτ ) →fdd Vα,β τ,
α,β(cτ ) →fdd κ1/α

α ζα(τ ),

as c → 0,

as c → ∞.

4 Proofs

We ﬁrst present some preliminary facts that will be used in the proofs.

Let 0 < α ≤ 2. The characteristic function of a r.v. ε ∈ D(α) has the following representation in a

neighborhood of the origin (see, e.g., [13, Theorem 2.6.5]): there exists an ǫ > 0 such that

Eeiθε = e−|θ|αω(θ)h(θ)

for any θ ∈ R, |θ| < ǫ,

(4.1)

10

where h(θ) is a positive function tending to 1 as θ → 0 and ω(θ) = ω(sign(θ)) is the same as in (2.3).

For a ∈ [0, 1) and n ∈ N, let cn(a, s) :=

n
t=1 at−s1(s ≤ t) and note the following elementary inequalities:

P

|cn(a, s)|α ≤

Xs≤0

1
min(α, 1)(1 − a)

min

n,

(cid:0)

1
1 − a (cid:1)

α,

For z ∈ C, Re(z) ≤ 0, we have

n

Xs=1

|cn(a, s)|α ≤ n min

1
1 − a (cid:1)

α.

n,
(cid:0)

|ez − 1| ≤ min(2, |z|),

|ez − 1 − z| ≤ |z|2.

(4.2)

(4.3)

Proof of Theorem 2.1. The iterated limits (2.15), (2.16) follow from [31, Theorems 2.1, 3.1, Proposition 2.3].

Proof of (2.17). It suﬃces to prove that

¯XN (t) := (N log N )−1/α

N

Xi=1

Xi(t) →fdd Vα,1,

or that, for any d ∈ N and θt ∈ R, t = 1, . . . , d,

Eei Pd

t=1 θt ¯XN (t) → Eei Pd

t=1 θtVα,1 = eΘ with Θ := −

d

Xt=1

ψ1
α (cid:12)
(cid:12)

θt

αω
(cid:12)
(cid:12)

d

θt

.

(cid:1)

(cid:0)

Xt=1

(4.4)

(4.5)

Since X1, . . . , XN are independent copies of X in (2.5), the l.h.s. of (4.5) can be rewritten as (1 + ΘN

N )N with

ΘN := N E

exp

i(N log N )−1/α

(cid:2)

(cid:8)

d

Xt=1

θtX(t)

(cid:9)

− 1
(cid:3)

= N

Z[0,1) (cid:0) Ys∈Z

E exp

i(N log N )−1/αc(u, s)ε(s)
(cid:9)

(cid:8)

− 1
(cid:1)

φ(u)du

= N

Z[0,1) (cid:0)

exp

−

(cid:8)

KN (u)
(1 − u)N log N (cid:9)

− 1
(cid:1)

φ(u)du

(4.6)

where

c(u, s) :=

d

Xt=1

θtut−s1(s ≤ t),

KN (u) := (1 − u)

|c(u, s)|α ω

c(u, s)
(cid:1)

h
(cid:0)

(cid:0)

(N log N )−1/αc(u, s)
(cid:1)

,

Xs∈Z

(in (4.6) we used (4.1) and the fact that c(u, s) is bounded uniformly in u ∈ [0, 1), s ∈ Z). For δ ∈ (0, 1) split

ΘN = N

1−δ

1

Z

0

(cid:8)

+

Z

1− δ
N

+

1− δ
N

Z

1−δ (cid:9)(cid:0)

exp

−

(cid:8)

KN (u)
(1 − u)N log N (cid:9)

− 1
(cid:1)

φ(u)du =:

3

Xi=1

Θi

N,δ.

Then (4.5) or limN→∞ ΘN = Θ follows from

lim
δ→0

lim sup
N→∞

|Θi

N,δ| = 0,

i = 1, 2,

lim
δ→0

lim sup
N→∞

|Θ3

N,δ − Θ| = 0.

(4.7)

Here, |Θ2

N,δ| ≤ 2N

s∈Z |c(u, s)|α ≤ C(1 − u)−1. Therefore, |KN (u)| ≤ C, u ∈ [0, 1), and, by (4.3), we obtain |Θ1

φ(u)du ≤ Cδ for all N large enough, implying (4.7) for i = 2. Next, using (4.2),
N,δ| ≤

R

1
1− δ
N

P
C(log N )−1

1−δ
0

R

(1 − u)−1φ(u)du = C(δ log N )−1, proving (4.7) for i = 1.

11

Consider the last relation in (4.7). In view of (2.4), we can replace Θ3

N,δ by Θ4

N,δ := ψ1N

1

log N } − 1)du = ψ1

(exp{−

δN
δ

R

KN (1− x

N )

x log N } − 1)dx, which in turn can be replaced by

Θ5
N,δ := −ψ1 Z

δ

δN

KN (1 − x
N )
x log N

dx

1− δ
1−δ (exp{− KN (u)
N

(1−u)N

R

N,δ − Θ5

N,δ| ≤ C

since |Θ4
similar way:

dx

(x log N )2 = o(1), N → ∞, follows from (4.3). We can rewrite Θ in (4.5) in a

δN
δ
R

Θ = −ψ1K

δN

dx
x log N

Z
δ

with K :=

d

Xt=1

1
α (cid:12)
(cid:12)

αω

θt

(cid:12)
(cid:12)

d

θt

.
(cid:1)

(cid:0)

Xt=1

Thus, the last relation in (4.7) follows from limδ→0 lim supN→∞

δN
δ

|KN (1 − x

N ) − K|

dx

x log N = 0 or

≤ ǫ(δ),

(4.8)

R
− K

lim sup
N→∞

sup
δ<x<δN (cid:12)
(cid:12)

KN

1 −

x
N (cid:1)

(cid:0)
ω := |z|αω(z), z ∈ R, and note that sup0<x<δN |KN (1 −
where limδ→0 ǫ(δ) = 0. To prove (4.8), denote |z|α
N ) − ˜K( N
x

x )| = o(1), N → ∞, where

(cid:12)
(cid:12)

˜K(y) :=

=

→

1
y Xs∈Z (cid:12)
(cid:12)
d

Xt=1

ZR (cid:12)
(cid:12)
0

d

θt

(cid:0)

Xt=1

θt

1 −

(cid:0)

d

Z

−∞ (cid:12)
(cid:12)

Xt=1

θtes

(cid:12)
(cid:12)

1 −

1
y (cid:1)

α
ω

1
y (cid:1)

t−s1(s ≤ t)
(cid:12)
(cid:12)
t−[sy]1([sy] ≤ t)
(cid:12)
(cid:12)

α
ωds

α
ωds = K,

y → ∞,

(4.9)

by the dominated convergence theorem (DCT) using 1 − z ≤ e−z, z ≥ 0. Hence, sup0<x<δN | ˜K( N
ǫ(δ) = o(1), δ → 0, implying (4.8) and (4.7). This completes the proof of (2.17).

x ) − K| ≤

Proof of (2.18). Let us ﬁrst prove that

n−1/αSn(τ ) := n−1/α

[nτ ]

Xt=1

X(t) →fdd (1 − a)−1ζα(τ ),

(4.10)

where ζα, X are the same as in (2.1), (2.5), and a ∈ [0, 1) is ﬁxed. It suﬃces to show that, for any d ∈ N and
τj > 0, θj ∈ R, j = 1, . . . , d,

Ea exp

in−1/α

(cid:8)

d

Xj=1

θjSn(τj)

→ Ea exp

(cid:9)

i(1 − a)−1
(cid:8)

Xj=1

θjζα(τj)

,

(cid:9)

d

(4.11)

where Ea[·] = E[·|a] stands for conditional expectation. For brevity of notation, we restrict the proof of (4.11)
(as well as all the rest in this theorem) to d = 1 and τ1 = τ > 0, θ1 = θ ∈ R. Split Ea[eiθn−1/αSn(τ )(1(a ∈
In) + 1(a ∈ I c
n := [0, 1) \ In. For cn(a, s) in (4.2),
supa∈In,s∈Z |n−1/αc[nτ ](a, s)| = O((log n)−1) = o(1). Hence for all n large enough, we can use (4.1) to rewrite
Φ′

n(θ, a), where In := [0, 1 − log n

n(θ, a) = e−|θ|αω(θ)Kn(a)1(a ∈ In), where

n(θ, a) as Φ′

n(θ, a) + Φ′′

n))] =: Φ′

n1/α ), I c

Kn(a) := n−1

Xs∈Z

|c[nτ ](a, s)|αh(θn−1/αc[nτ ](a, s))

12

and supa∈In,s∈Z |h(θn−1/αc[nτ ](a, s)) − 1| = o(1). Relation (4.11) follows from limn→∞ Kn(a) = τ (1 − a)−α
for every a ∈ [0, 1) and limn→∞ Φ′′
n(θ, a) = 0. Both these facts are completely elementary, and we omit the
details. This proves (4.10) for d = 1. The proof for d > 1 follows similarly.

Let {(1 − ai)−1ζα,i(τ ), τ ≥ 0}, i = 1, 2, . . . , be independent copies of {(1 − a)−1ζα(τ ), τ ≥ 0}. With (4.10)

in mind, (2.18) follows from

N −1/β

N

Xi=1

(1 − ai)−1ζα,i(τ ) →fdd Wα,β(τ ).

Consider the one-dimensional convergence in (4.12) at τ = 1. For θ ∈ R, we have E exp{iθN −1/β
ai)−1ζα,i(1)} = (E exp{iθN −1/β(1 − a)−1ζα(1)})N = (1 + ΘN

N )N , where

P

ΘN := N E[e−N −α/β (1−a)−α|θ|αω(θ) − 1].

(4.12)

N
i=1(1 −

We also have EeiθWα,β(1) = e−κβ/α,α,β|θ|β(ω(θ))β/α
from

, see (2.11). The desired convergence in (4.12) at τ = 1 follows

ΘN → −κβ/α,α,β|θ|β(ω(θ))β/α,

∀θ ∈ R.

(4.13)

By assumption (1.3), there exists an ǫ > 0 and a constant C > 0 such that φ(u) ≤ C(1 − u)β−1 for all
u ∈ [1 − ǫ, 1). Split

ΘN = Θ0

N + Θ1

N := N E

(e−N −α/β (1−a)−α|θ|αω(θ) − 1)
(cid:0)

(cid:2)

1(0 ≤ a < 1 − ǫ) + 1(1 − ǫ ≤ a < 1)

,

(cid:1)(cid:3)

N | ≤ N 1−α/βCǫ−α|θ|αP(0 ≤ a < 1 − ǫ) = o(1) since β < α. Hence, it suﬃces to prove (4.13) for Θ1
N

where |Θ0
instead of ΘN . By change of a variable,

Θ1

N = N

1

Z

1−ǫ

(e−N −α/β (1−u)−α|θ|αω(θ) − 1)φ(u)du =

ǫN 1/β

Z

0

(e−x−α|θ|αω(θ) − 1)N 1−1/β φ
1 −
(cid:0)

where N 1−1/βφ(1 − x
for x ∈ (0, ǫN 1/β ]. Therefore, the DCT implies that

N 1/β ) → ψ1xβ−1 for x ∈ R+ by assumption (1.3). Moreover, N 1−1/βφ(1 − x

x
N 1/β

dx,
(cid:1)
N 1/β ) ≤ Cxβ−1

∞

Θ1
N → ψ1 Z

0

(e−x−α|θ|αω(θ) − 1)xβ−1dx = −κβ/α,α,β|θ|β(ω(θ))β/α,

see (2.12), (2.9). This proves (4.13) and (4.12).

Proof of (2.19). Note that (4.10) also holds for β > α. Let {(1 − ai)−1ζα,i(τ ), τ ≥ 0}, i = 1, 2, . . . , be as in
(4.12). It suﬃces to prove that

N

N −1/α

Xi=1

(1 − ai)−1ζα,i(τ ) →fdd κ1/α

α ζα(τ ).

(4.14)

N
Consider the one-dimensional convergence in (4.14) at τ = 1. For any θ ∈ R, we have E exp{iθN −1/α
i=1(1−
N )N , where ΘN := N E[e−N −1(1−a)−α|θ|αω(θ) − 1] → −κα|θ|αω(θ) = log Eeiθκ1/α
ai)−1ζα,i(1)} = (1 + ΘN
α ζα(1) with
κα = E(1 − a)−α < ∞ by the DCT. The general ﬁnite-dimensional convergence in (4.14) follows in a similar
way. This proves (2.19) and completes the proof of Theorem 2.1.

P

Proof of Theorem 2.2. In each case of Theorem 2.2, we will prove that, for any d ∈ N, 0 < τ1 < · · · < τd < ∞,
and θj ∈ R, j = 1, . . . , d, as N, n → ∞,

EeiA

−1
N,n Pd

j=1 θjSN,n(τj ) → Eei Pd

j=1 θj S(τj ),

(4.15)

13

where AN,n → ∞ denotes a sequence of normalizing constants and {S(τ ), τ ≥ 0} denotes the limit process.
Since X1, X2, . . . are independent processes, we can rewrite the l.h.s. of (4.15) as (1 + ΘN,n
N )N and reduce the
proof to showing that

ΘN,n := N E

exp

(cid:2)

(cid:8)

iA−1
N,n

d

Xj=1

θjS1,n(τj)

(cid:9)

− 1
(cid:3)

→ log Eei Pd

j=1 θj S(τj ) =: Θ

(4.16)

as N, n → ∞, in each case of Theorem 2.2. Conditioning on a, we have

ΘN,n = N

Z[0,1) (cid:2) Ys∈Z

ϕε

A−1

N,nϑn(u, s)
(cid:1)

(cid:0)

− 1
(cid:3)

φ(u)du with ϑn(u, s) :=

d

[nτj]

θj

Xj=1

Xt=1

ut−s1(s ≤ t),

(4.17)

where ϕε(θ) := Eeiθε, θ ∈ R, is the characteristic function of ε ∈ D(α). Next, we need to split the interval
[0, 1) = IN,n ∪ I c
N,n with IN,n := [0, 1 − uN,n), where uN,n → 0 is chosen so that supu∈IN,n,s∈Z |ϑn(u, s)| =
O(u−1
(1−
u)β−1du = O(N uβ
u ∈ IN,n, s ∈ Z, and, taking into account (4.1), (4.16)–(4.17),

N,n) = o(1) is negligible. By doing so, we obtain that h(A−1

N,n) = o(AN,n) and |N E[(exp{iA−1
N,n

d
j=1 θjS1,n(τj)}−1)1(a1 ∈ I c

N,nϑn(u, s)) → 1 uniformly in

N,n)]| ≤ CN P(a1 ∈ I c

N,n) ≤ CN

I c
N,n

P

R

ΘN,n ∼ N

ZIN,n (cid:16)

exp

n

− A−α
N,n

Xs∈Z

|ϑn(u, s)|αω(ϑn(u, s))

o

φ(u)du.

− 1

(cid:17)

In order to avoid this rather tedious step, from now on, we will assume that h(θ) ≡ 1. That is, ε =d ζα(1) has
a stable distribution and we can take uN,n ≡ 0. Moreover, for simplicity of exposition, in cases (i) and (ii)
of Theorem 2.2, we also assume that φ(u) ≡ ψ1(1 − u)β−1 in (4.17). Similar simpliﬁcations are also imposed
in the proof of [27, Theorem 2.2]. Finally, since ω(θ) depends on the sign of θ alone, we shall assume that
ω(θ) ≡ 1. After all, ΘN,n of (4.17) reduces to

ΘN,n = ψ1N

Z[0,1) (cid:0)

exp

(cid:8)

− A−α
N,n

Xs∈Z

|ϑn(u, s)|α

− 1
(cid:1)

(cid:9)

(1 − u)β−1du.

(4.18)

The only exception is the ‘short memory’ case (iii) of Theorem 2.2 where we use (4.18) with φ(u) instead of
ψ1(1 − u)β−1, u ∈ [0, 1).

We consider each case of Theorem 2.2 separately.

Proof of Theorem 2.2(i). Let N, n → ∞ so that µN,n := N 1/β/n → µ ∈ [0, ∞] and set

BN,n :=






n,

N,

N 1/β,

N 1/β,

AN,n := n1+1/α

µβ/α
N,n,
(µN,n log µN,n)1/α,
µ1/α
N,n,
µN,n,

if µ = ∞, 1 < β < α,

if µ = ∞, 1 = β < α,

if µ = ∞, 0 < β < min(α, 1),

if µ ∈ [0, ∞), 0 < β < α.

(4.19)






(BN,n and AN,n are simultaneously deﬁned in the above four cases of µ, α, β.) Note that AN,n agree with
respective normalizations in Theorem 2.2(i). After the change of variable BN,n(1 − u) = x, (4.18) can be
rewritten as ΘN,n = (ψ1N/Bβ
(e−KN,n(x) − 1)xβ−1dx, where

N,n)

BN,n
0

KN,n(x) = K −

N,n(x) + K +

N,n(x) := A−α

N,n

R

ϑn

1 −

(cid:0)

Xs≤0 (cid:12)
(cid:12)

x
BN,n

, s

α + A−α
N,n

(cid:1)(cid:12)
(cid:12)

ϑn

1 −

(cid:0)

Xs>0 (cid:12)
(cid:12)

x
BN,n

, s

α.

(cid:1)(cid:12)
(cid:12)

(4.20)

14

Proof of (2.25) (case µ ∈ (0, ∞), 0 < β < α). The r.h.s. of (4.16) can be written as the integral Θ =
ψ1

∞
0 (e−Kµ(x) − 1)xβ−1dx, see (2.14), where

d

Kµ(x) := µ

ZR (cid:12)
(cid:12)

Xj=1

θjfτj/µ(x, s)
(cid:12)
(cid:12)

αds

(4.21)

and fτ (x, s) as in (2.6). Using BN,n = N 1/β and writing the sums over integers s, t in (4.20), (4.17)
as integrals, after the change of variables s → N 1/βs, t → N 1/βt, the l.h.s. of (4.16) becomes ΘN,n =
ψ1

(e−KN,n(x) − 1)xβ−1dx with KN,n(x) = µN,n

R | ˜ϑN,n(x, s)|αds, where

R

N 1/β
0

R

x
N 1/β
x
N 1/β

R

, [N 1/βs]
(cid:1)

[N 1/β t]−[N 1/βs]1(max([N 1/βs], 1) ≤ [N 1/βt] ≤ [nτj])dt
(cid:1)

˜ϑN,n(x, s) :=

1
N 1/β
d

ϑn

1 −

(cid:0)

1 −

θj ZR (cid:0)

=

→

Xj=1
d

Xj=1

θjfτj /µ(x, s)

(4.22)

for any x ∈ R+, s ∈ R. Using 1−z ≤ e−z, z ∈ [0, 1], we get the dominating bound | ˜ϑN,n(x, s)| ≤ Cf2τd/µ(x, s),
x ∈ (0, N 1/β ], s ∈ R, implying KN,n(x) → Kµ(x), x ∈ R+, by the DCT. Using (4.3), we can extend the last
R |f2τd/µ(x, s)|αds, x ∈ (0, N 1/β ], where the last integral is estimated in
dominating bound to |KN,n(x)| ≤ C
(4.34). Then, another application of the DCT yields the convergence in (4.16), proving (2.25).

R

n

∞
0 K1(x)xβ−1dx,
Proof of (2.21) (case µ = ∞, 1 < β < α). By (2.7), the r.h.s. of (4.16) equals to −ψ1
where K1(x) is as in (4.21) (with µ = 1). After a change of variable, the l.h.s. of (4.16) can be written as
0 µβ
N,n(e−µ
n ϑn(1 −
ΘN,n = ψ1
d
j=1 θjfτj (x, s) for any x ∈ R+, s ∈ R (to justify the last relationship, use (4.22) with N 1/β
x
n , [ns]) →
−β
replaced by n). Therefore, KN,n(x) → K1(x), µβ
N,nKN,n(x) − 1) → −K1(x), x ∈ R+ and, ﬁnally, the
convergence in (4.16) follows using the DCT similarly to the proof before. This proves (2.21).

R | ˜ϑN,n(x, s)|αds and ˜ϑN,n(x, s) := 1
R

−β
N,nKN,n(x) − 1)xβ−1dx, with KN,n(x) :=

N,n(e−µ

P

R

R

Proof of (2.22) (case µ = ∞, 0 < β < min(α, 1)). The r.h.s. of (4.16) can be written as

Θ = −κβ,α,β

d
j=1 θjτj

∞
0

exp

− 1
αx

d
j=1 θjτj

α

xβ−1dx,

(4.23)

(cid:0)
see (2.9)–(2.12). On the other hand, for the l.h.s. of (4.16), after a change of variable, we get ΘN,n =
N,n(x) is given in (4.20) with BN,n = N 1/β.
ψ1
Rewrite K −

(e−KN,n(x) − 1)xβ−1dx, where KN,n(x) = K −
R | ˜ϑN,n(x, s)|αds, where

N,n(x) + K +

N 1/β
0

(cid:12)
(cid:12) P

(cid:12)
(cid:12) P

(cid:9)

(cid:8)

(cid:12)
(cid:12)

R

R

N,n(x) =

αβ = ψ1
(cid:12)
(cid:12)

− 1
(cid:1)

˜ϑN,n(x, s) :=

R
d
j=1 θj

P

1 − x
(cid:0)

N 1/β

R

R

[nt]−[N 1/βs]1(1 ≤ [nt] ≤ [nτj], [N 1/βs] ≤ 0)dt →
(cid:1)

P

d
j=1 θjτjexs1(s ≤ 0)

in view of n/N 1/β → 0, for any x > 0, s ∈ R, s 6= 0. We have the dominating bound | ˜ϑN,n(x, s)| ≤ Cexs1(s ≤
C), x ∈ (0, N 1/β ], s ∈ R. Whence,

K −

N,n(x) →

d

Xj=1

1
αx (cid:12)
(cid:12)

α

θjτj

(cid:12)
(cid:12)

and |K −

N,n(x)| ≤

C
x

(4.24)

for x ∈ (0, N 1/β ]. Relation (4.2) implies

|K +

N,n(x)| ≤

Cn
Aα

N,n

min

N 1/β
x (cid:1)

n,
(cid:0)

α ≤

C
µN,n

min

1,
(cid:0)

µN,n
x (cid:1)

min(α,1) = o(1)

15

uniformly in x > 0 as N, n → ∞. Moreover, we conclude that |KN,n(x)| ≤ Cx− min(α,1) for x ∈ (1, N 1/β ].
Since 0 < β < min(α, 1), in view of (4.3), the DCT implies (4.16), proving (2.22).

Proof of (2.23) (case µ = ∞, 1 = β < α). Write the r.h.s. and l.h.s. of (4.16) respectively as Θ =
− ψ1
α |

d
j=1 θjτj|α and

P

ΘN,n = N ψ1 Z[0,1) (cid:0)

exp

−

(cid:8)

For given δ ∈ (0, 1) split

KN,n(u)
(1 − u)N log N

n (cid:9)

− 1
(cid:1)

du with KN,n(u) := n−α(1 − u)

|ϑn(u, s)|α.

Xs∈Z

ΘN,n = N ψ1

1− δ
n

1

+

Z

1− δ
N

+

Z
0

(cid:8)

By (4.2) and (4.3), for every δ > 0,

1− δ
N

Z

1− δ

n (cid:9)(cid:0)

exp

−

(cid:8)

KN,n(u)
(1 − u)N log N

n (cid:9)

du =:

− 1
(cid:1)

3

Xi=1

Θi

N,n,δ.

|Θ1

N,n,δ| ≤

1− δ
n

n

≤

n,

Z
0

min

1
1 − u

α + n min

C
nα log N
n
C
nα log N
n

1
1 − u (cid:1)
n,
(cid:0)
N,n,δ| ≤ Cδ can be made arbitrary small by a suitable choice of δ > 0. Consider
δ N
n

(cid:0)
α + min

1
1 − u (cid:1)

dx = o(1)

(cid:16)
1
x

n
x (cid:1)

n
x (cid:1)

n,
(cid:0)

n,
(cid:0)

δ (cid:16)

min

δ N
n

du

(cid:17)

(cid:17)

Z

α

α

KN,n(1 − x
N )
x log N
n

− 1
(cid:1)

(cid:9)

dx ∼ −ψ1 Z

δ

KN,n(1 − x
N )
x log N
n

dx =: Θ4

N,n,δ

as N/n → ∞. Also, |Θ2

Θ3
N,n,δ = ψ1 Z

δ

(cid:0)

exp

−

(cid:8)
δ(N/n)
δ

since |Θ3
N,n,δ − Θ4
sup0<x<δN/n |KN,n(1 − x

N,n,δ| ≤ C

R

dx

(x log(N/n))2 = o(1), as N/n → ∞, follows from (4.3) and the bound

N )| ≤ C, which is a consequence of (4.2). Rewrite Θ similarly to Θ4

N,n,δ, viz.,

Θ = −ψ1K

δ N
n

Z
δ

dx
x log N
n

with K :=

d

Xj=1

1
α (cid:12)
(cid:12)

θjτj

α.
(cid:12)
(cid:12)

Thus, (2.23) follows (c.f. (4.8)) from

lim sup
N,n,N/n→∞

where limδ→0 ǫ(δ) = 0. Towards this end, write KN,n(1 − x

sup
δ<x<δ(N/n) (cid:12)
(cid:12)

KN,n

1 −

(cid:0)

− K

x
N (cid:1)
N ) = ˜K( N

≤ ǫ(δ)

(4.25)

(cid:12)
(cid:12)
x , n) similarly as in (4.8) above, where

˜K(y, z) :=

ZR (cid:12)
(cid:12)
(cid:12)

d

1 −

θj ZR (cid:0)

Xj=1

1
y (cid:1)

[zt]−[ys]1(1 ∨ [ys] ≤ [zt] ≤ [zτj])dt

α

(cid:12)
(cid:12)
(cid:12)

ds,

y, z > 0.

(4.26)

˜K(y, z) = K, implying that
Then using the DCT similarly as in (4.9) above, it follows that limy,z,y/z→∞
lim supy,z,y/z→∞ supy/z>1/δ | ˜K(y, z) − K| ≤ ǫ(δ) with ǫ(δ) as in (4.25), hence (4.25), thus completing the
proof of (2.23).

Proof of (2.24) (case µ = 0, 0 < β < α). The r.h.s. of (4.16) can be written as

Θ = log EeiW 1/α

β/α,α,β Pd

i=1(Pd

j=i θj )(ζα(τi)−ζα(τi−1))

= log Ee−Wβ/α,α,β Pd

i=1(τi−τi−1)| Pd

j=i θj |α

= −κβ/α,α,β

∞

= ψ1 Z

0

exp

−

(cid:0)

(cid:8)

1
xα

d

Xi=1

(τi − τi−1)
(cid:12)
(cid:12)

d

Xj=i

16

d

Xi=1

(τi − τi−1)
(cid:12)
(cid:12)

(cid:0)

d

Xj=i

α

β/α

θj

(cid:1)

(cid:12)
(cid:12)

xβ−1dx

α

θj

(cid:12)
(cid:12)

− 1
(cid:1)

(cid:9)

(4.27)

with τ0 := 0 and κβ/α,α,β as in (2.12). After a change of variable, we get ΘN,n = ψ1
1)xβ−1dx, where KN,n(x) = K −
R | ˜ϑN,n(x, s)|αds, where
R

N,n(x) is given as in (4.20) with BN,n = N 1/β. Rewrite K +

N,n(x) + K +

R

˜ϑN,n(x, s) := 1
x

d
j=1 θj

1 −

[nτj]−[ns]+1

1(0 < [ns] ≤ [nτj]) → 1
x

d
j=1 θj1(0 < s ≤ τj)

P

(cid:1)
(cid:0)
d
d
j=i θj|α for x ∈ R+.
for x ∈ R+, s ∈ R, since n/N 1/β → ∞. Therefore, K +
N,n(x) → 1
i=1(τi − τi−1)|
xα
N 1/β )−(1+α) = CµN,nx−(1+α) = o(1) for x ∈ R+. Moreover,
From (4.2), it follows that |K −
|KN,n(x)| ≤ Cx−α, x ∈ (1, N 1/β ]. Since 0 < β < α, the DCT and (4.3) imply ΘN,n → Θ, proving (2.24),
thereby completing the proof of Theorem 2.2(i).

N,n(x)| ≤ CA−α

N,n( x

P

P

P

(cid:1)

1 − x
(cid:0)

N 1/β

N 1/β
0

(e−KN,n(x) −
N,n(x) =

Proof of Theorem 2.2(iii). We use the representation in (4.18) with AN,n = (N n)1/α and φ(u) instead of
1
ψ1(1 − u)β−1, u ∈ [0, 1). We have that ΘN,n =
n (u) +
K +
n (u) := n−1
n (u) =
d
j=1 θj(1 − u[nτj]−[ns]+1)1(0 < [ns] ≤ [nτj]) → (1 −
R |ϑn(u, [ns])|αds, where ϑn(u, [ns]) = (1 − u)−1
R
u)−1

0 N (e−N −1Kn(u) − 1)φ(u)du, where Kn(u) = K −
s>0 |ϑn(u, s)|α with ϑn(u, s) deﬁned in (4.17). Rewrite K +

P
d
j=1 θj1(0 < s ≤ τj) for u ∈ [0, 1), s ∈ R. Hence,

s≤0 |ϑn(u, s)|α + n−1

P

P

R

P

for u ∈ [0, 1), where τ0 := 0. From (4.2), for u ∈ [0, 1), we further obtain that K −

n (u) → 0 and that

K +

n (u) → (1 − u)−α

d
i=1(τi − τi−1)
(cid:12)
(cid:12) P

P

α

d
j=i θj

(cid:12)
(cid:12)

|K −

n (u)| ≤ C

n(1−u) min

n,

1
1−u

(cid:0)

α ≤ C 

(cid:1)

(1 − u)−α, α ≥ 1,

(1 − u)−1, α < 1.

n (u)| ≤ C(1 − u)−α, |Kn(u)| ≤ C(1 − u)− max(α,1) =: ¯K(u), where

Since |K +
fact that β > max(α, 1). Then, by the DCT, we conclude that



¯K(u)φ(u)du < ∞ due to the

1
0

R

ΘN,n → −κα

d

Xi=1

(τi − τi−1)|

d

Xj=i

θj|α = log Eeiκ1/α

α Pd

j=1 θjζα(τj ),

where (recall) κα = E(1 − a)−α < ∞. Theorem 2.2(iii) is proved.

Proof of Theorem 2.2(ii). Recall that 0 < α < β < 1. Set µN,n := N 1/(γβ)/n → µ ∈ [0, ∞], AN,n := N 1/(αβ)n
if µ = ∞; and AN,n := (N n)1/α if µ ∈ [0, ∞). Consider separately terms S±
N,n(τ ) in the decomposition
SN,n(τ ) = S−

N,n(τ ) + S+

N,n(τ ), where

N

[nτ ]

N

[nτ ]

S−
N,n(τ ) :=

Xi=1 (cid:0)

Xt=1

at
i

Xi(0), S+
(cid:1)

N,n(τ ) :=

Xi=1 Xs>0 (cid:0)

Xt=1

at−s
i 1(s ≤ t)
(cid:1)

εi(s).

From the proofs of (2.22), (2.30) (in particular, (4.24), (4.28), (4.29)) we see that

N −1/(αβ)n−1S−

(N n)−1/αS+

N,n(τ ) →fdd Vα,β τ
N,n(τ ) →fdd κ1/α

as N, n, N 1/β/n → ∞,

for 0 < β < 1,

α ζα(τ ) as N, n → ∞ in arbitrary way,

for β > α.

(4.30)

(4.31)

Proof of (2.27) (case µ = ∞) follows from (4.30) and (4.31) since µN,n → ∞ implies (N n)1/α = o(AN,n).

Proof of (2.28) (case µ = 0) follows in view of (4.31), if we prove that

S−
N,n(τ ) = op(AN,n).

17

(4.32)

(4.28)

(4.29)

For any θ ∈ R, consider

ΘN,n := N E

(cid:2)
= ψ1N

exp
1

iθA−1
(cid:8)
exp

Z

0 (cid:0)

(cid:8)

N,nS−

1,n(τ )
(cid:9)

− 1
(cid:3)

− A−α
N,n

|c[nτ ](u, s)|α|θ|α

Xs≤0

(1 − u)β−1du,

− 1
(cid:1)

(cid:9)

where c[nτ ](u, s) is given in (4.2). Using (4.2)–(4.3) and changing a variable, we obtain

|ΘN,n| ≤ CN

1

Z
0

min

1,

(cid:0)

nα−1
N (1 − u) (cid:1)

(1 − u)β−1du ≤ C

N 1/β

min

1,

(cid:0)

µ1−α
N,n
x (cid:1)

Z

0

xβ−1dx = o(1).

This proves (4.32), hence, (2.28).

Proof of (2.29) (case µ ∈ (0, ∞)). For N, n large enough, decompose ΘN,n = Θ−
ψ1(1 − u)β−1 replaced by φ(u), as

N,n + Θ+

N,n in (4.18), with

Θ−

N,n := N

1

(e−K

Z
0

where

−

N,n(u) − 1)φ(u)du, Θ+

N,n := N

1

e−K

Z

0

−

N,n(u)(e−K +

N,n(u) − 1)φ(u)du,

K −

N,n(u) := A−α

N,n

|ϑn(u, s)|α, K +

N,n(u) := A−α

N,n

Xs≤0

|ϑn(u, s)|α.

Xs>0
N,n → log Eeiµ(1/α)−1(Pd

Since µN,n → µ ∈ (0, ∞) implies N 1/β/n → ∞, by (4.30) we have that Θ−
Next, using K −
log Eeiκ1/α
rem 2.2 is proved.

N,n →
j=1 θjζα(τj ). Hence, (2.29) follows, including the independence of Vα,β and {ζα(τ ), τ ≥ 0}. Theo-

N,n(u)| ≤ C, u ∈ [0, 1), similarly to the proof of (4.31), we obtain Θ+

N,n(u) → 0, |e−K

j=1 θj τj)Vα,β .

α Pd

−

Proof of Proposition 3.1. As noted in Section 3, for α = 2, the proposition is proved in [27, Propositions 3.1,
3.2]. The subsequent proof for 0 < α < 2 uses similar argument.

α,β(τ ) + Z +

α,β(τ ), where Z +
(0,1)×D(R) z(τ ; x)N (dx, dζ). Next,

(i) Write Zα,β(τ ) = Z −
1)), Z −
α,β(τ ) :=
R
∞
1 Eα|z(τ ; x)|pxβ−1dx, I −(p, τ ) :=
R
some 0 < p < α in which case

R

1

α,β(τ ) :=

[1,∞)×D(R) z(τ ; x)(N (dx, dζ) − ν(dx, dζ)1(α >
let I(p, τ ) := I −(p, τ ) + I +(p, τ ), where I +(p, τ ) :=
α,β(τ ) are well deﬁned if I ±(p, τ ) < ∞ for

R

0 Eα|z(τ ; x)|pxβ−1dx. Then Z ±

E|Z ±

α,β(τ )|p ≤ CI ±(p, τ )

(4.33)

with C = C(p) depending on p alone; see [27, (3.3)]. By well-known property of an α-stable stochastic integral
in (3.1), Eα|z(τ ; x)|p = C(
R |fτ (x, s)|αds)p/α, ∀p ∈ (0, α); see [34, Property 1.2.17, Proposition 3.4.1]. Then,
R
using (4.3), we obtain

|fτ (x, s)|αds =

ZR

(1 − e−xτ )α

αx1+α +

τ

1
xα Z

0

(1 − e−xs)αds ≤

1
α

min

τ α
x

,

(cid:0)

1
x1+α

τ
xα , τ 1+α

.

(cid:1)

(4.34)

Let ﬁrst 1 < α < 2. Then (4.34) simpliﬁes to

R |fτ (x, s)|αds ≤ C min( τ α

I(p, τ ) ≤ C

1/τ
0

τ p
(cid:0)

R

R

xβ−1−p/αdx + τ p/α

∞
1/τ xβ−1−pdx

(cid:1)

R

≤ Cτ p+p/α−β

(4.35)

for β < p < min(α, αβ) = α min(1, β), with C > 0 independent of τ > 0. Obviously, for given 0 < β < α,
such p exists implying the existence of the Poisson stochastic integrals Z ±

α,β(τ ) and Zα,β(τ ).

18

+ min

(cid:1)

(cid:0)
xα ) implying

x , τ

Next, let 0 < α ≤ 1. Here, we need to discuss the existence of Z ±

α,β(τ ) separately. From (4.34), we have

I +(p+, 1) ≤ C

I −(p−, 1) ≤ C

∞

1 xβ−1−p+
dx ≤ ∞,
R
0 xβ−1−p−/αdx ≤ ∞,
1

R

β < p+ < α,

p− < αβ < α.

(4.36)

Clearly, for any 0 < β < α ≤ 1, such p± satisfying (4.36) exist, implying the existence of Z ±
α,β(τ ) and Zα,β(τ )
for τ = 1, and the last result extends to all τ > 0 in an obvious way. The stationarity of increments is
immediate from (2.14). Inﬁnite divisibility and the form of the characteristic function in (2.14) follow from
general properties of Poisson stochastic integrals, see e.g. [27, (3.1)].

(ii) Follows from (4.35), (4.36) and (4.33).

(iii) Follows from (ii) and E|Zα,β(τ )| ≤ CI(1, τ ) < ∞ since 1 < α min(β, 1) is equivalent to α > 1, αβ > 1.

(iv) Follows from Kolmogorov’s criterion, stationarity of increments of Zα,β, and (4.33), (4.35) since, for
1/α < β < α, we can ﬁnd p suﬃciently close to α min(β, 1) such that the exponent of τ on the r.h.s. of (4.35)
is greater than 1: p + p/α − β > 1.

(v) For brevity, we assume ω(θ) ≡ 1, ψ1 = 1 and restrict the proof to one-dimensional convergence at τ > 0.
Let 1 < β < α and H := 1 − β−1

α > 0. For any θ ∈ R, we have

Eeiθc−H Zα,β(cτ ) = exp

n ZR+ (cid:0)

exp

− cβ|θ|α

(cid:8)

|fτ (cx, s)|αds

ZR

− 1
(cid:1)

(cid:9)

xβ−1dx

o

since

R |fcτ (x, s)|αds = c1+α

R |fτ (cx, s)|αds. By change of variables, we further rewrite

R

R

Eeiθc−H Zα,β(cτ ) = exp

n ZR+

c−β

exp

− cβ|θ|α

|fτ (x, s)|αds

(cid:0)

ZR

(cid:8)
|fτ (x, s)|αxβ−1dxds

− 1
(cid:1)

(cid:9)

xβ−1dx

o

= EeiθΛα,β(τ ),

c → 0,

o

→ exp

n

− |θ|α

ZR+×R

where the convergence follows by the DCT (the domination can be veriﬁed using (4.3) and (4.34)).

where

Next, let 0 < β < min(α, 1). For any θ ∈ R, we have Eeiθc−1Zα,β(cτ ) = exp{
R
αds →
(cid:1)

1 − e−cxτ
cx

1 − e−xs
cx

|fcτ (x, s)|αds =

1
cα ZR

1
αx (cid:0)

Kc(x) :=

α +

R+

Z

cτ

(cid:1)

(cid:0)

0

τ α
αx

,

c → 0.

(e−|θ|αKc(x) − 1)xβ−1dx},

For Kc(x), we can ﬁnd a dominating function using (4.3) and (4.34), because the latter inequality gives
Kc(x) ≤ Cx−1 if α ≥ 1 and Kc(x) ≤ C max(x−1, c1−αx−α) ≤ Cx−α if α < 1 for x > 1. Then, by the DCT,
we obtain Eeic−1Zα,β(cτ ) → exp{
R

− 1)xβ−1dx} = Eeiθτ Vα,β , c → 0, see (2.9)–(2.11).

Let 1 = β < α. For any θ ∈ R, we consider Eeiθ(log(1/c))−1/αc−1Zα,β(cτ ) = eP3

(e−(αx)−1τ α|θ|α

c,δ, where

i=1 I i

R+

3

Xi=1

I i
c,δ :=

∞

+

δ

+

Z

δ
c

Z

0

(cid:8)

δ
c

Z

δ (cid:9)(cid:0)

e−|θ|α(log 1

c )−1Kc(x) − 1
(cid:1)

dx with Kc(x) :=

1
cα ZR

|fcτ (x, s)|αds,

the same as above, given a δ > 0. Then I 1

c,δ = o(1) and by (4.3), (4.34),

1
αx

|I 2

c,δ| ≤

≤

C
cα log 1
c
C
log 1
c

Z
δ

∞

δ
c

Z
∞

(cid:0)

(cid:0)
1
αx

min

min

cτ,

(cid:0)
1
x(cid:1)

τ,
(cid:0)

α + cτ min

1
x (cid:1)
α + τ min

1
x

α

(cid:0)

, τ

1
x

(cid:0)

(cid:1)

α

, cτ

(cid:1)

dx
(cid:1)

dx = o(1),
(cid:1)

c → 0.

19

With the notation ˜K(cx) := xKc(x), x ∈ R+, using (4.3) and ˜K(cx) ≤ C, x > δ, we have that

c,δ ∼ −|θ|α
I 3

δ
c

˜K(cx)
x log 1
c

Z

δ

dx,

c → 0,

for every δ > 0. Furthermore, lim supc→0 supδ<x< δ

c

| ˜K(cx) − τ α

α | < ǫ(δ) with limδ→0 ǫ(δ) = 0, since

1 − e−wτ
w

α + w

(cid:1)

τ

Z

0 (cid:0)

1 − e−ws
w

αds →
(cid:1)

τ α
α

, w → 0.

˜K(w) =

1
α (cid:0)
c,δ+ τ α|θ|α

Hence, limδ→0 lim supc→0 |I 3

α | = 0, ﬁnishing the proof of limc→0 Eeiθ(log(1/c))−1/αc−1Zα,β(cτ ) = Eeiθτ Vα,1.
Finally, consider the large scale limit of Zα,β as c → ∞ for 0 < β < α. Then Eeiθc−1/αZα,β(cτ ) =

(e−|θ|αKc(x) − 1)xβ−1dx}, where, using the scaling property,

exp{
R

R+

Kc(x) :=

1
c ZR

|fcτ (x, s)|αds = cα

|fτ (cx, s)|αds =

ZR

(1 − e−cxτ )α

cαx1+α +

τ

1
xα Z

0

(1 − e−cxs)αds →

τ
xα ,

c → ∞.

It is obvious that, for x > 1, Kc(x) ≤ x−α((cτ )−1 + τ ) = O(x−α). Therefore, by the DCT, Eeiθc−1/αZ(cτ ) →
(e−|θ|αx−ατ − 1)xβ−1dx} = EeiθWα,β(τ ), c → ∞, see (2.9)–(2.11). This proves part (v) and completes
exp{
R
the proof of Proposition 3.1.

R+

Acknowledgments

We thank an anonymous referee for useful comments. Vytaut˙e Pilipauskait˙e acknowledges the ﬁnancial
support from the project “Ambit ﬁelds: probabilistic properties and statistical inference” funded by Villum
Fonden.

References

[1] O.E. Barndorﬀ-Nielsen. Superposition of Ornstein-Uhlenbeck type processes. Theory Probab. Appl. 45 (2001), 175–194.

[2] J. Beran, M. Sch¨utzner and S. Ghosh. From short to long memory: Aggregation and estimation. Comput. Stat. Data Anal.

54 (2010), 2432–2442.

[3] J. Beran, Y. Feng, S. Gosh and R. Kulik. Long-Memory Processes: Probabilistic Properties and Statistical Methods. Springer,

New York 2013.

[4] D. Celov, R. Leipus, and A. Philippe. Time series aggregation, disaggregation, and long memory. Lith. Math. J. 47 (2007),

379–393.

[5] D. Celov, R. Leipus,and A. Philippe. Asymptotic normality of the mixture density estimator in a disaggregation scheme. J.

Nonparametric Statist. 22 (2010), 425–442.

[6] C. Dombry and I. Kaj. The on-oﬀ network traﬃc under intermediate scaling. Queueing Sys. 69 (2011), 29–44.

[7] P. Embrechts and C.M. Goldie. On closure and factorization properties of subexponential and related distributions. J. Austral.

Math. Soc. (Series A) 29 (1980), 243–256.

[8] W. Feller. An Introduction to Probability Theory and Its Applications, volume 2, 2nd ed. Wiley, New York, 1971.

[9] R. Gaigalas. A Poisson bridge between fractional Brownian motion and stable L´evy motion. Stochastic Process. Appl. 116

(2006), 447–462.

20

[10] R. Gaigalas and I. Kaj. Convergence of scaled renewal processes and a packet arrival model. Bernoulli 9 (2003), 671–703.

[11] D. Grahovac, N.N. Leonenko and M.S. Taqqu. The multifaceted behavior of integrated supOU processes: The inﬁnite

variance case. Preprint (2018+). arXiv:1711.09623v1 [math.PR]

[12] C.W.J. Granger. Long memory relationship and the aggregation of dynamic models. J. Econometrics 14 (1980), 227–238.

[13] I. Ibragimov and Y. Linnik. Independent and Stationary Sequences of Random Variables. Wolters-Noordhoﬀ, Groningen,

1971.

[14] M. Jirak. Limit theorems for aggregated linear processes. Adv. Appl. Probab. 45 (2013), 520–544.

[15] I. Kaj. Limiting fractal random processes in heavy-tailed systems. In J. L´evy-V´ehel and E. Lutton, editors, Fractals in

Engineering: New Trends in Theory and Applications, pages 199–217. Springer London, 2005.

[16] I. Kaj and M.S. Taqqu. Convergence to fractional Brownian motion and to the Telecom process: the integral representation
approach. In V. Sidoravicius and M. E. Vares, editors, In and Out of Equilibrium 2, pages 383–427. Birkh¨auser Basel, 2008.

[17] J.B. Levy and M.S. Taqqu. Renewal reward processes with heavy-tailed interrenewal times and heavy-tailed rewards.

Bernoulli 6 (2000), 23–44.

[18] R. Leipus, G. Oppenheim, A. Philippe and M.-C. Viano. Orthogonal series density estimation in a disaggregation scheme.

J. Statist. Plan. Inf. 136 (2006), 2547–2571.

[19] R. Leipus, A. Philippe, D. Puplinskait˙e and D. Surgailis. Aggregation and long memory: recent developments. J. Indian

Statistical Association 52 (2014), 71–101.

[20] R. Leipus, A. Philippe, V. Pilipauskait˙e and D. Surgailis. Nonparametric estimation of the distribution of the autoregressive

coeﬃcient from panel random-coeﬃcient AR(1) data. J. Multiv. Anal. 153 (2017), 121–135.

[21] R. Leipus, A. Philippe, V. Pilipauskait˙e and D. Surgailis. Estimating long memory in panel random-coeﬃcient AR(1) data.

Preprint (2018+). arXiv:1710.09735v2 [math.ST]

[22] R. Leipus, A. Philippe, V. Pilipauskait˙e and D. Surgailis. Sample autocovariances of random-coeﬃcient AR(1) panel model.

To appear in Electron. J. Stat. (2019+). arXiv:1810.11204v1 [math.ST]

[23] T. Mikosch, S. Resnick, H. Rootz´en and A. Stegeman. Is network traﬃc approximated by stable L´evy motion or fractional

Brownian motion? Ann. Appl. Probab. 12 (2002), 23–68.

[24] T. Mikosch. Modelling dependence and tails of ﬁnancial time series. In B. Finkenst¨adt and H. Rootz´en, editors, Extreme

Values in Finance, Telecommunications and the Environment, pages 185–286. Chapman & Hall, New York, 2003.

[25] G. Oppenheim and M.-C. Viano. Aggregation of random parameters Ornstein-Uhlenbeck or AR processes: some convergence

results. J. Time Ser. Anal. 25 (2004), 335–350.

[26] V. Pilipauskait˙e. Limit Theorems for Spatio-Temporal Models with Long-Range Dependence. Doctoral dissertation, Vilnius

2017.

[27] V. Pilipauskait˙e and D. Surgailis. Joint temporal and contemporaneous aggregation of random-coeﬃcient AR(1) processes.

Stochastic Process. Appl. 124 (2014), 1011–1035.

[28] V. Pilipauskait˙e and D. Surgailis. Anisotropic scaling of random grain model with application to network traﬃc. J. Appl.

Probab. 53 (2016), 857–879.

[29] V. Pipiras, M.S. Taqqu, and L.B. Levy. Slow, fast and arbitrary growth conditions for renewal reward processes when the

renewals and the rewards are heavy-tailed. Bernoulli 10 (2004), 121–163.

[30] D. Puplinskait˙e and D. Surgailis. Aggregation of random coeﬃcient AR1(1) process with inﬁnite variance and common

innovations. Lith. Math. J. 49 (2009), 446–463.

[31] D. Puplinskait˙e and D. Surgailis. Aggregation of random coeﬃcient AR1(1) process with inﬁnite variance and idiosyncratic

innovations. Adv. Appl. Probab. 42 (2010), 509–527.

[32] B.S. Rajput and J. Rosinski. Spectral representations of inﬁnitely divisible processes. Probab. Theory Related Fields 82

(1989), 451–487.

[33] P.M. Robinson. Statistical inference for a random coeﬃcient autoregressive model. Scand. J. Statist. 5 (1978), 163–168.

[34] G. Samorodnitsky and M.S. Taqqu. Stable Non-Gaussian Random Processes. Chapman & Hall, New York, 1994.

21

[35] M.S. Taqqu, W. Willinger and R. Sherman. Proof of a fundamental result in self-similar traﬃc modeling. Comput. Commun.

Rev. 27 (1997) 5–23.

[36] P. Zaﬀaroni. Contemporaneous aggregation of linear dynamic models in large economies. J. Econometrics 120 (2004),

75–102.

[37] V.M. Zolotarev. One-Dimensional Stable Distributions, Amer. Math. Soc., Providence, RI, 1986.

22

