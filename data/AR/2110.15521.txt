ARviz – An Augmented Reality-enabled Visualization Platform for ROS
Applications

Khoa C. Hoang, Wesley P. Chan, Steven Lay, Akansel Cosgun, Elizabeth A. Croft
Department of Electrical and Computer Systems Engineering, Monash University

1
2
0
2

t
c
O
9
2

]

O
R
.
s
c
[

1
v
1
2
5
5
1
.
0
1
1
2
:
v
i
X
r
a

Abstract— Current robot interfaces such as teach pendants
and 2D screen displays used for task visualization and interac-
tion often seem unintuitive and limited in terms of information
ﬂow. This compromises task efﬁciency as interacting with
the interface can distract the user from the task at hand.
Augmented Reality (AR) technology offers the capability to
create visually rich displays and intuitive interaction elements
in situ. In recent years, AR has shown promising potential
to enable effective human-robot
interaction. We introduce
ARviz - a versatile, extendable AR visualization platform built
for robot applications developed with the widely used Robot
Operating System (ROS) framework. ARviz aims to provide
both a universal visualization platform with the capability of
displaying any ROS message data type in AR, as well as a
multimodal user interface for interacting with robots over ROS.
ARviz is built as a platform incorporating a collection of plugins
that provide visualization and/or interaction components. Users
can also extend the platform by implementing new plugins to
suit their needs. We present three use cases as well as two
potential use cases to showcase the capabilities and beneﬁts of
the ARviz platform for human-robot interaction applications.
The open access source code for our ARviz platform is available
at: https://github.com/hri-group/arviz.

I. BACKGROUND AND MOTIVATION

The Robot Operating System (ROS) [1] has risen to
become the framework of choice for robotics research and
development. ROS is used in many robotic research areas,
ranging from mobile robots and robotic manipulators to
aerial robotics, as well as applications ranging from fac-
tory and healthcare robots, to disaster response and space
exploration robots. The industrial-compatible version ROS2
is under active development.

When working with robots, the ability for users to vi-
sualize sensor data, understand robot state, and issue com-
mands in an intuitive way is crucial. The successful software
tool, RViz1, offers these capabilities for the development of
robotics applications built on the ROS framework through
interface (Figure 1 (bottom)).
a 2D, screen-based visual
However, providing visualizations and interfaces in a 2D
screen for a 3D world has inevitable challenges with respect
to intuitiveness (especially with respect to spatial under-
in order to see the
standing) for the user. Furthermore,
visualization/information or to command the robot, a screen-
based interface requires the user to frequently shift their
attention to the monitor screen, disconnecting them from the
physical robot/environment. This attention shifting results in
reduced levels of safety and efﬁciency when operating or
collaborating with robots.

1rviz - http://wiki.ros.org/rviz

Fig. 1: Display Plugin used for visualizing TF ROS message data type on
our platform ARviz (top) and on Rviz in ROS (bottom). The similarities
between the two visualization tools makes using ARviz more convenient

Augmented Reality (AR) technology offers the capacity
to create and superimpose virtual objects and information
directly onto the real world, as viewed by the operator, co-
located in the robot’s space and task space. In recent years,
AR technology has risen in popularity becoming a promising
new alternative for creating interfaces that can facilitate more
intuitive and effective interaction and information exchange
in human-robot collaborations [2]. AR-based interfaces offer
a richer visual to the user and more intuitive interaction
through virtual objects superimposed onto the real world.
By rendering relevant information in the user’s ﬁeld of view
directly over the real world, AR interfaces help reduce mental
fatigue and cognitive load of the user when collaborating
with their robot partner [3].

Recognizing the potential of AR technology, researchers
have proposed various AR-based interfaces to assist human
robot interaction over the past few years. Yew et al. [4]
developed an AR-based teleoperation interface for a main-
tenance robot, where a mixture of real and virtual objects
were used to reconstruct the remote maintenance site at
the operator’s environment. Makhataeva et al. [5] increased

 
 
 
 
 
 
a human operator’s awareness of danger in human-robot
collaboration by augmenting their views through an AR
display with a safety aura around the operating robot. Walker
et al. [6] used an AR display to show various visual cues
such as arrows and points to convey motion intent of aerial
robots during collaboration between human and robot. Dinh
et al. [7] proposed a novel interface for collaborating with a
tape dispensing robot. Their system allowed the operator to
monitor the task through an AR headset, and enhanced the
user’s experience working with the robot. Waymouth et al.
[8] developed an interaction design for demonstrating cloth
folding to robots through AR. Chan et al. [9] utilized AR
in conjunction with gestures and tactile feedback to create
a multimodal system for robot programming and execution,
where the AR display was used to assist the human operator
with robot programming in addition to visualizing robot
trajectories. A similar system was later on applied towards
large-scale human-robot collaborative manufacturing tasks
[10].

The number of AR-HRI research papers in recent years
increased rapidly from only 4000 papers in 2015 to 8000
papers in 2019 [11]. However, most of these AR appli-
cations developed are task-speciﬁc (including works men-
tioned above), lacking generalizability, reusability, and scal-
ability; i.e., the AR-HRI applications developed for these
tasks are not easily re-purposed for other tasks. To enable
more efﬁcient application development and research on AR-
enabled robotics and HRI, a universal platform that enables a
systematic approach to implementing data visualization and
interactive interfaces for AR-HRI applications is proposed to
ﬁll an unmet need for the AR-HRI community.

Recognizing this need, we developed ARviz – a versa-
tile AR visualization platform for ROS-based applications.
ARviz is inspired by Rviz, and allows users to visualize
any standard ROS messages data types in AR. Furthermore,
with additional input modalities, such as speech, gesture, and
gaze, enabled by AR devices, ARviz provides a multimodal
interface for users to interact with any ROS applications.
ARviz is designed to be extendable, allowing users to imple-
ment their own plugins for visualizing customized ROS mes-
sages, or for deﬁning their own interaction methods based
on their speciﬁc application needs. By keeping many design
similarities with Rviz, ARviz aims to provide a sense of
familiarity for the large existing user-base of Rviz, as shown
in Figure 1. In the following, Section II describes ARviz’s
platform architecture. Section III presents the collection of
display and tool plugins included in our current implemen-
tation of ARviz. Section IV demonstrates the beneﬁts of
ARviz through a number of use cases featuring different
applications of ARviz. Finally, Section V discusses potential
further extensions and uses cases for ARviz, as well as
some limitations, while Section VI summarises the ARviz
contribution.

II. PLATFORM ARCHITECTURE

ARviz is designed to be a stand-alone AR application
that can connect to any ROS-based application to provide

visualization and interaction capabilities to the user, requiring
minimal-to-no-modiﬁcations to the existing ROS application.
Figure 2 depicts the overall platform architecture for ARViz
and how it interacts with ROS applications.

We developed ARViz using the Unity Game Engine.
ARviz is designed to work with many different plugins,
which can be categorized into two types: Display Plugins,
which are used for visualizing data received from ROS appli-
cations, and Tool Plugins, which enable the use of natural
input methods such as speech and gestures for interacting
with ROS applications. The visual elements (holograms)
generated by the plugins are accurately localized in the real
world with the help of Vuforia Engine2 - a Unity toolkit that
enables the alignment of the virtual world and the real world,
and Unity tf Listener - a component of ARviz that collects
spatial data (transformations between coordinate frames)
from the ROS application. ARviz provides a Hand Menu,
which can be accessed with a raise of hand, as a convenient
means to manage the plugins available on the platform
(Figure 3). The communication channel between ARviz and
the ROS application is established using the Unity package
ROS#3on the ARViz side and a ROS package rosbridge4
running on top of the ROS application. We explain each
component in more detail in the following subsections.
A. ARviz Plugins

Display Plugins are components of the ARviz platform
that allow users to visualize data from ROS applications in
AR by rendering virtual displays that are contextually located
in the real world. Display Plugins can be used for visualizing
application information such as robot transformation frames,
navigation paths, and robot gripper grasp poses (more details
in Section IV-A, IV-B and IV-C). The plugins are designed to
be reusable for any ROS application using the same standard
ROS message types, and users can also implement additional
customized plugins to visualize other ROS message types or
customized ROS message types.

Data from ROS is collected by ROS# through a component
call a subscriber (more details in Section II-D). The Display
Plugin accesses the collected data from the subscriber and
renders corresponding visual elements at 20 Hz. This call-
back frequency was chosen to suit the limited processing
capabilities of AR headsets, whilst being frequent enough
to maintain a smooth motion of the visual elements - only
slightly slower than the 24 Hz frame rate for movies.

Tool Plugins are components of ARviz that utilize input
modalities of the AR device such as gaze tracking and speech
recognition to provide a multimodal communication interface
with ROS. Tool Plugins can be used for communicating
with a robot’s navigation stack to move the robot around the
environment or for issuing command to a robotic manipulator
to start a handover task (more details are provided in Section
IV-A and IV-C). Similar to Display Plugins, users can
implement their own plugins to tailor to their needs for

2Vuforia Engine: https://www.ptc.com/en/products/vuforia/vuforia-engine
3Siemens - ROS#: https://github.com/siemens/ros-sharp
4rosbridge suite: http://wiki.ros.org/rosbridge suite

Fig. 2: Basic Architecture of ARviz Platform

C. Hologram Localization

For AR applications, we need to consider two coordinate
systems: the virtual world coordinate system (VWCS),
where virtual objects/holograms are rendered, and the real
world coordinate system (RWCS), where the physical enti-
ties (e.g., the robot) exist. Typically, when an AR application
is started up, it arbitrarily initializes the base frame of the
VWCS to be where the AR device is physically located in
the real world. On the other hand, in a ROS application, the
RWCS is managed by the tf transformation ROS package,
and the base frame of the RWCS can be deﬁned by the
user to be anywhere in the physical world. In order for
the Display Plugins and Tool Plugins in ARviz to render
virtual display elements at appropriate locations in the real
world, two pieces of information are needed: 1) The pose
(position and orientation) of the virtual world coordinate
system (VWCS) with respect to the real world coordinate
system (RWCS) - i.e., the transformation between VWCS
and RWCS. 2) Where should the display element be located
with respect to the real world coordinate system. These two
pieces of information are provided by Vuforia Engine and
Unity tf Listener as described below.

Aligning Real and Virtual World Coordinate Systems
using Vuforia: To deﬁne the transformation between RWCS
and VWCS, we deﬁne an anchor point that tell us where the
VWCS base frame should be, within the RWCS. Vuforia
Engine allows the speciﬁcation of this anchor through a
physical QR code marker placed in the real world. To create
this anchor, a physical QR code marker is placed at ﬁxed,
known location within the RWCS. In ARviz, the Vuforia
Engine is deployed to detect this QR code marker. Once
the AR devices detects the QR marker, it computes the
transformation between the AR device and the physical QR
marker. Since the position of the AR device with respect
to the VWCS is always tracked by the AR application,
the transformation between the VWCS and RWCS can be
computed. Once this transformation is computed, ARviz
then repositions the VWCS base frame to coincide with the
RWCS base frame, aligning the virtual world with the real
world. The alignment process is illustrated in Figure 4.

Unity tf Listener: Data in ROS are speciﬁed with respect

Fig. 3: Hand Menu appears when the user raise their left hand in front of
the HoloLens 2. Hand Menu provides quick access to the plugins available
on the platform.

communicating with ROS and interacting with the robot.

The implementation of this plugin can be varied depending
on the communication channel. The Tool Plugin is developed
as an extension of the input handler provided by Microsoft’s
Mixed Reality Toolkit5 - a Unity package for mixed reality
application development which provides APIs to input sys-
tem and other building blocks for spatial interactions. User
inputs are collected by Tool Plugins, which then send the
information to ROS using ROS#, through a component called
a publisher (more details in Section II-D).
B. Hand Menu

A menu that can be summoned at the user’s ﬁngertips,
literally, is developed to provide quick and easy control of
all the available plugins implemented in ARviz. By raising
their left hand as shown in Figure 3, the user can make the
hand menu appear and access conﬁgurations of the Display
Plugins and Tool Plugins. Using this menu, users can
toggle the visibility of Display Plugins, enable/disable Tool
Plugins or access/modify each plugin’s speciﬁc properties, as
shown in Figure 6. Additional menu entries can be added and
their behaviors customized for user-deﬁned display plugins
and tool plugins.

5Microsoft Mixed Reality Toolkit Unity - https://docs.microsoft.com/en-

gb/windows/mixed-reality/mrtk-unity/?view=mrtkunity-2021-05

Fig. 4: ARviz’s Virtual World Alignment. The virtual objects would be initially rendered at random location, misaligning with the real world (left). With
the use of the QR code marker in the real world and Vuforia Engine in ARviz (middle), the virtual world is aligned with the real world, allowing the
correct overlay of the virtual objects onto the real world.

to to a given base frame to provide physical spatial context
(e.g., a robot’s coordinate may be speciﬁed relative to the
map frame). ROS is a distributed computing system by
design. Hence, in ROS applications, transformations between
different coordinate frames are asynchronously broadcasted
(published) by different processes (nodes). The ROS com-
ponent named tf Listener collects and assembles all these
transformation information and allows any component of the
ROS application to query transformation between any two
coordinate frames. The Unity tf Listener provides the anal-
ogous functionality in ARviz. Asynchronous transformation
information from the ROS application is passed on to the
Unity tf Listener through ROS#. The Unity tf Listener col-
lects and assembles the transformation information. Display
Plugins and Tool Plugins can then query transformation
between any two coordinate frames, and render display
elements at appropriate locations.

D. Communication with ROS

For the plugins to receive and send data to and from
ROS applications, a connection with the ROS environment is
required. ROS runs on Ubuntu, and within ROS applications,
nodes (i.e, processes) communicate with each other by
broadcasting messages (i.e., data structures) over topics (i.e.,
communication channels). Publishers are used to broadcast
messages and subscribers are used to receive and manage
messages. On the other hand, ARviz is built on the Unity
Game Engine, a platform which does not natively support
ROS APIs for establishing the communication channels
mentioned above. To resolve this problem, a combination
of rosbridge on ROS and ROS# on Unity was used. ros-
bridge and ROS# allow ARviz and any ROS application to
communicate via a common standard JSON string format
over TCP/IP, as shown in Figure 2. rosbridge establishes the
network port for ARviz to connect to. ROS# provides ROS-
like APIs such as ROS# Publisher and ROS# Subscriber,
enabling ARviz plugins to communicate with other ROS
applications through ROS-like communication mechanisms.

III. AVAILABLE PLUGINS

Three Display Plugins and two Tool Plugins were im-
plemented in our initial release of ARviz. These plugins
are developed and tested on the AR-head-mounted-display

Microsoft HoloLens 26 and are summarised in Figure 5.

TF Display Plugin visualizes coordinate frames moni-
tored by the Unity tf Listener. Each coordinate frame is
visualized as a set of 3D axes, with red, green and blue
corresponding to x, y and z axis. Figure 6 (top) shows
examples of visualizing the joint/link frames of two robots.
The plugin can also show the name of each frame and the
connection between it and its parent frame, depicted with
an arrow. The visibility of each elements of this display
plugin can be toggled through a control panel as shown
in Figure 6 (bottom). TF Display Plugin can be used for
visually conﬁrming the connectivity between ARviz and the
ROS applicaiton, as well as verifying the alignment between
the VWCS and the RWVC. Moreover, TF Display Plugin
allows users to visualize many aspects of robot operation,
including robot localization and robot structure, and serves as
a powerful development and debugging tool. Users can utilise
TF Display Plugin to view the connections between the
coordinate frames within the robot for a better understanding
of how the robot base, its multi-link manipulator and other
degrees of freedom, operate as a whole.

VisualizationMarkerArray Display Plugin visualizes a
collection of user-deﬁned markers. These markers include
various primitives such as cubes and spheres, text, or lines.
This plugin is inspired by the MarkerArray7 display in Rviz,
and provides the visualization capabilities similar to its rviz
counterpart. VisualizationMarkerArray Display Plugin al-
lows users to display arbitrary primitives in the virtual world
through a user-deﬁned ROS topic by publishing messages
of type visualization msgs/MarkerArray to this topic. This
plugin provides a generic tool for creating visual markers
with user-deﬁned size, color, shape, and location. This tool
can be used for a wide range of tasks such as creating
text labels, visualizing robot paths, and generating object
indicators.

Stamped Pose Display Plugin visualizes the transla-
tion and orientation of a virtual object with respect
to
a given coordinate frame, given in the format of a ge-
ometry msgs/PoseStamped ROS message type. The default
virtual object used for the visualization is an arrow, but users
can customize this plugin by deﬁning their own pre-loaded

6HoloLens 2 - https://www.microsoft.com/en-AU/hololens/hardware
7rviz Markers - http://wiki.ros.org/rviz/DisplayTypes/Marker

Fig. 5: Summary of available plugins, comparing the similarity between ARviz in AR and Rviz in ROS.

object model or mesh to be displayed instead the default
arrow. The plugin listens for any incoming messages being
published to a topic deﬁned by the users and visualizes the
pose with the virtual object accordingly. This plugin is useful
for visualizing the 3D pose of robots or occluded objects for
example.

2D Arrow Tool Plugin allows users to specify a 2D pose
(position and orientation) by placing an arrow on the ground

plane in AR through the use of hand gestures. When the user
raises their hand, a ray emanating from the hand is displayed
in AR as a dotted line. When the user taps the index ﬁnger
with the thumb (known as an “air tap“ gesture), the location
where the ray intersects the ground plane is identify, and a
an arrow is placed with its tail endpoint ﬁxed at this location.
Next, the user can rotate the arrow on the ﬂoor by moving the
raised hand - the arrow tip will follow the intersecting point

Fig. 6: TF Display Plugin in ARviz. Top: TF Display Plugin being used
with different robots. Bottom: TF Display Plugin control panel, allows
users to toggle the visibility of individual frames or all the frames at once,
as well as other elements such as frame names and arrows linking the frames

between the hand ray and the ground plane. By performing
another “air tap“ gesture, the arrow is ﬁxed and its pose is
sent to be published in a user-speciﬁed ROS topic.

Voice Command Tool Plugin extends the speech handling
capabilities of the Mixed Reality Toolkit and allows users to
interact with the ROS application through speech commands.
When the user says a pre-deﬁned keyword which is identiﬁed
by the AR device, the plugin will publish the corresponding
command code into a topic speciﬁed by the user.

IV. USE CASES

In this section, we present a use case demonstration in
Section IV-A. Section IV-B and Section IV-C highlight two
studies utilizing ARviz, and demonstrate the usability and
beneﬁts of the platform for robotics research studies. Full
details of the studies mentioned in Section IV-B and Section
IV-C can be found in the cited works, while in the following
subsections, we focus on how the ARviz platform enabled
these studies, and summarizing the studies’ key ﬁndings
related to the use of AR elements provided by ARviz.

A. Mobile Robot Navigation

Fig. 7: ARviz Use Case 1 - Navigating a Mobile Robot. Top: The user using
ARviz with HoloLens2, navigating the Fetch Mobile Robot. Bottom: The
augmented reality view with TF Display overlaid on top and 2D Navigation
Tool being used to navigate the robot.

and a demonstration video8. The robot uses the standard ROS
Navigation stack, and it is launched and run independently
from ARviz, without any modiﬁcation needed. With the 2D
Arrow Tool Plugin, users are able communicate the desired
goal pose to the robot’s navigation stack through a standard
ROS topic, as demonstrated in Section III, allowing them
to move the robot to any location. Compared to specifying
the goal with Rviz through a 2D monitor, this Tool Plugin
in ARviz offers the beneﬁt of enabling better spatial and
situational awareness of the goal position, as the user can see
and specify the goal pose directly over the physical space.

B. Visualizing Occluded Robots and Communicating Mo-

tion Intent

In the work by Gu et al. [12], ARviz was used to visualize
the location of the Fetch Mobile Robot using an AR
headset. This allowed the user to visualize the robot even
when the robot is occluded by the environment. Furthermore,

We demonstrate the use of ARviz for mobile robot navi-
gation with the 2D Arrow Tool Plugin, as shown in Figure 7

8ARviz Demo - https://youtu.be/byUUtB5SWsM

Fig. 8: ARviz Use Case 2 - Visualizing Mobile Robot and Communicating
its Motion Intent. Top Left: The mobile robot is not visible to the user at a
T-Junction. Top Right: A representation of what the user sees from the AR
headset. The robot model is visualized, enabling the user to see the robot
through the wall. The cyan colored arrow emanating from the robot model
showing its direction of motion. The ﬂoor is shown as a grid to improve
depth perception. Bottom: User study survey results showing the beneﬁt
of the ARvis implementation in reducing collision risk and increasing task
efﬁciency.(source: [12])

the robot’s direction of motion is conveyed to the user by
visualizing an arrow emanating from the robot model. To
achieve this, two Display Plugins were used to set up the
AR environment, as shown in Figure 8 (top).

The ﬁrst Display Plugin used is Stamped Pose Display.
A pre-load mesh of the mobile robot is used to customize
the visualization. The second Display Plugin is Visualiza-
tionMarkerArray Display. In this system, there are two types
of markers used, LINE and ARROW. The LINE markers are
used to create the grid line on the ﬂoor, which allows better
depth perception for the user wearing the AR headset. The
ARROW marker is used to visualize the motion intent of the
robot. The arrow emanates from the robot model, with its tip
located at the goal point the robot is moving to. The arrow
is continually updated by the ROS application as the robot
moves closer to the goal point.

The AR system was validated in a study with 15 par-
ticipants and their experiences were surveyed with a ques-
tionnaire. The results are summarised in Figure 8 (bottom).
A lower rating in perceived collision risk (2.3 with AR
compared to 4.1 without AR) and higher rating in task
efﬁciency (5.6 with AR vs 4.4 without AR) suggest that
the AR system improves the experience when working in
a shared workspace with the mobile robot with respect to
these aspects.
C. Visualizing Robotic Arm’s Goal Pose During Object’s

Handover

In the work by Newbury et al. [13], ARviz was used to
visualize the motion intent of the Franka Emika Panda
robotic arm when performing handover tasks with a human
collaborator. ARviz was also used to communicate to the
robot when to initiate the handover task. This application
(shown in Figure 9 (left)) used two Display Plugins and
one Tool Plugin.

VisualizationMarkerArray Display is used to visualize a
wireframe of the object at the detected location during the
handover. The wireframe is created with a collection of
LINE markers. The position and orientation of the lines are
deﬁned and updated at a ﬁxed rate by the ROS system. The

Fig. 9: ARviz Use Case 3 - Visualizing a Robotic Arm’s Goal Pose During
Object’s Handover. Top Left: The robot picking up an object from the user’s
hand. The user is wearing an Augmented Reality headset. Bottom Left:
The detected pose of the object, and AR visualization of how the robot
is planning to grasp the object. Right: Survey results from the user study
reported in the paper showing that the ARvis supported interaction improved
task ﬂuency, trust and perceived saftey. (source: [13])

grasp pose of the robotic gripper is visualized with Stamped
Pose Display. A pre-loaded mesh of the end effector of the
Franka Emika Panda is used to customize the Stamped
Pose Display plugin, with low opacity to avoid the virtual
gripper occluding the user’s view of the object during the
handover in this case.

The Tool Plugin used by this AR application is the Voice
Command Tool Plugin. With this Tool Plugin, user can
initiate the handover task by saying a pre-deﬁned keyword.
The ROS application will listen to the incoming ROS mes-
sage being published by the plugin to a pre-deﬁned topic
and start the handover task as soon as the ROS message is
received by the ROS application.

The AR system was validated in a study with 16 partici-
pants and their experiences were surveyed with a question-
naire given at the end of the experiment. The results were
summarised in Figure 9 (right). The AR systems received
a higher rating across three categories: task ﬂuency, trust
toward the robot, and perceived safety. The results suggested
that the use of visualization and speech commands through
ARviz both greatly improves the experience of the users
during the handovers, and also leads to better efﬁciency in
performing the task.

V. POTENTIAL IMPLEMENTATIONS AND

A. Potential Implementations

LIMITATIONS

Section IV demonstrated the capabilities and beneﬁts of
ARviz in a range of human-robot interactions such as mobile
robot navigation, conveying motion intent and improving
robot handover experience. Recognizing the potential of

The second proposed application enables multi-human-
multi-robot collaboration. ARviz was designed to be a stand-
alone AR software that can be deployed to any compatible
AR device. That means ARviz with the same conﬁguration
settings, i.e., connecting to the same network, having the
same Display Plugins and Tool Plugins, can be deployed
to different AR devices and would be able to interact with
the same ROS applications at the same time. An example
of this would be two human operators each using ARviz
on a HoloLens 2 collaborating with two mobile robots, as
shown in Figure 10 (bottom). ARviz is deployed on both AR
devices with identical conﬁgurations, which connects to the
central computer that runs the ROS applications controlling
the two mobile robots. One operator can give navigation
commands as described in Section IV-A. The operator can
choose which robot to operate using a control panel for the
navigation Tool Plugin or can operate both at the same time.
Their collaborator (or trainee) can observe the task being
implemented. The planned path of the robots can also be
visualized on both AR devices, conveying motion intent of
the robots to both human collaborators.
B. Limitations

Whilst we have had early success in demonstrating the
potential of the platform in Section IV, ARviz still has a
number of limitations requiring further development. Two
key development issues are discussed here.

The current implementation is has limited accuracy in
aligning the virtual world and the real world. The semi-
automatic localization method with QR-code marker pro-
vides the simplicity in implementation and usage, but the
trade-off is the spatial accuracy of the overlaid visual ele-
ments, as they are affected by the accuracy of the placement
of the QR code marker. An improved localization algorithm
is needed in future versions of the platform.

ARviz is also limited in its ability to change the topics that
plugins connect to at run-time. The ROS topics that ROS#
collects data from can’t be modiﬁed during run-time and
must to be pre-conﬁgured prior to the deployment of the
platform onto the AR device. This connectivity limitation
for plugins is inconvenient for AR-based ROS applications
where multiple data sources are involved - for example, a
ROS application that needs to visualize data collected from
multiple cameras mounted at different places on the robot.
Future versions of ARviz will allow users to modify ROS
topics in run-time for one plugin, so that the visualization of
multiple data sources with the same ROS message type can
be achieved. A workaround for the current version would
be to create multiple instances of the plugin, with each
connecting to a different topic.

VI. CONCLUSION

In this work, we introduced the ARviz platform, an open
source platform for Augmented Reality interaction with ROS
based robotic systems. ARviz supports visualization of vari-
ous standardized data types that are available on ROS using
Display Plugins. We also introduced the implementation
of Tool Plugins that utilizes the natural
input methods

Fig. 10: Potential Implementations with ARviz. Top: a MoveIt-inspired
interactive interface with the robotic manipulator Franka Emika Panda.
The image shows the MoveIt-implemented interface in ROS. (from: MoveIt
Tutorials [14]). Bottom: A visualization of a potential ROS application that
enables multi-human-multi-robot collaborations.

ARviz, we suggest two other robot applications that can be
enabled by our platform.

The ﬁrst proposed application is an interactive interface for
a robotic manipulator using hand gestures. In this application,
ARviz could create a one-to-one virtual model of the robot,
with an interactive marker that would be attached at the
robot’s end effector. By interacting with this marker, the
operator can move the virtual robot to any desired pose
(similar to how the Moveit! motion planner plugin for Rviz
works, as shown in Figure 10 (top)). With the virtual robot
model in AR, the operator can preview directly in the robot’s
workspace the robot’s motion, and ensure that there are no
collisions with the surrounding physical environment.

[12] M. Gu, A. Cosgun, W. Chan, T. Drummond, and E. Croft, “Seeing
thru walls: Visualizing mobile robots in augmented reality,” in IEEE
International Symposium on Robot and Human Interactive Communi-
cation (RO-MAN), 2021.

[13] R. Newbury, A. Cosgun, T. Crowley-Davis, W. P. Chan, T. Drummond,
and E. Croft, “Visualizing robot intent for object handovers with
augmented reality,” arXiv preprint arXiv:2103.04055, 2021.

[14] D. T. Coleman, I. A. Sucan, S. Chitta, and N. Correll, “Reducing the
barrier to entry of complex robotic software: a moveit! case study,”
JOSER - Journal of Software Engineering for Robotics, vol. 5, pp.
3–16, 2014.

supported by the AR headset such as speech and gesture,
to communicate with the ROS environment. ARviz provides
an easy-to-access and extendable Hand Menu to control the
plugins, allowing future developers to expand the platform.
We present three different use cases of the platform from
our labs, from conveying mobile robot motion intent
to
visualizing hand over tasks. As demonstrated in published
user studies conducted using the ARvis system, the platform
provides a number of beneﬁts, with users feeling safer when
collaborating with the robot and completing the given task
more efﬁciently.

Finally, we proposed two different potential applications
that can highlight
the beneﬁts of ARviz: an interactive
”Moveit” type interface for a robotic manipulator and an
application that enables multi-human-multi-robot collabora-
tion. Despite the success of the platform, there is still a
great deal of room for improvement, including improving
the accuracy of the semi-automatic virtual world alignment
and addressing limitations in connecting plugins to topics
at run-time. We continue to work on this exciting platform
to improve capabilities and to bring a better experience to
end-users.

REFERENCES

[1] M. Quigley, K. Conley, B. Gerkey, J. Faust, T. Foote, J. Leibs,
R. Wheeler, A. Y. Ng, et al., “Ros: an open-source robot operating
system,” in ICRA workshop on open source software, vol. 3, no. 3.2.
Kobe, Japan, 2009, p. 5.

[2] S. A. Green, M. Billinghurst, X. Chen, and J. G. Chase, “Human robot
collaboration: An augmented reality approach—a literature review and
analysis,” in International Design Engineering Technical Conferences
and Computers and Information in Engineering Conference, 2007, pp.
117–126.

[3] S. Brending, A. M. Khan, M. Lawo, M. M¨uller, and P. Zeising,
robots,” in
“Reducing anxiety while interacting with industrial
Proceedings of the 2016 ACM International Symposium on Wearable
Computers, ser. ISWC ’16. New York, NY, USA: Association
for Computing Machinery, 2016, p. 54–55.
[Online]. Available:
https://doi.org/10.1145/2971763.2971780

[4] A. Yew, S. Ong, and A. Nee, “Immersive augmented reality envi-
ronment for the teleoperation of maintenance robots,” Procedia Cirp,
vol. 61, pp. 305–310, 2017.

[5] Z. Makhataeva, A. Zhakatayev, and H. A. Varol, “Safety aura vi-
sualization for variable impedance actuated robots,” in IEEE/SICE
International Symposium on System Integration (SII), 2019, pp. 805–
810.

[6] M. Walker, H. Hedayati, J. Lee, and D. Szaﬁr, “Communicating robot
motion intent with augmented reality,” in ACM/IEEE International
Conference on Human-Robot Interaction, 2018, p. 316–324.

[7] H. Dinh, Q. Yuan, I. Vietcheslav, and G. Seet, “Augmented reality
interface for taping robot,” in 2017 18th International Conference on
Advanced Robotics (ICAR), 2017, pp. 275–280.

[8] B. Waymouth, A. Cosgun, R. Newbury, T. Tran, W. P. Chan, T. Drum-
mond, and E. Croft, “Demonstrating cloth folding to robots: Design
and evaluation of a 2d and a 3d user interface,” arXiv preprint
arXiv:2104.02968, 2021.

[9] W. P. Chan, C. P. Quintero, M. Pan, M. Sakr, H. Van der Loos, and
E. Croft, “A multimodal system using augmented reality, gestures, and
tactile feedback for robot trajectory programming and execution,” in
Proceedings of the ICRA Workshop on Robotics in Virtual Reality,
Brisbane, Australia, 2018, pp. 21–25.

[10] W. P. Chan, G. Hanks, M. Sakr, T. Zuo, H. Machiel Van der Loos, and
E. Croft, “An augmented reality human-robot physical collaboration
interface design for shared, large-scale, labour-intensive manufacturing
tasks,” in 2020 IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS), 2020, pp. 11 308–11 313.

[11] Z. Makhataeva and H. A. Varol, “Augmented reality for robotics: a

review,” Robotics, vol. 9, no. 2, p. 21, 2020.

