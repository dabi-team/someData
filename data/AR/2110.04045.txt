1
2
0
2

t
c
O
8

]

C
H
.
s
c
[

1
v
5
4
0
4
0
.
0
1
1
2
:
v
i
X
r
a

Effect of Visual Cues on Pointing Tasks in Co-located
Augmented Reality Collaboration

Lei Chen
lei.chen02@student.xjtlu.edu.cn
Xi’an Jiaotong-Liverpool University
Suzhou, China

Yilin Liu
yilin.liu1999@gmail.com
Xi’an Jiaotong-Liverpool University
Suzhou, China

Yue Li
yue.li@xjtlu.edu.cn
Xi’an Jiaotong-Liverpool University
Suzhou, China

Lingyun Yu
lingyun.yu@xjtlu.edu.cn
Xi’an Jiaotong-Liverpool University
Suzhou, China

BoYu Gao
bygao@jnu.edu.cn
Jinan University
Guangzhou, China

Maurizio Caon
maurizio.caon@hes-so.ch
University of Applied Sciences and
Arts Western Switzerland (HES-SO)
Fribourg, Switzerland

Yong Yue
yong.yue@xjtlu.edu.cn
Xi’an Jiaotong-Liverpool University
Suzhou, China

Hai-Ning Liang∗
haining.liang@xjtlu.edu.cn
Xi’an Jiaotong-Liverpool University
Suzhou, China

Figure 1: (a) A picture of the experimental setup; Two users are locating a target using (b) Pointing Line (PL) cues and (c) Moving
Track (MT) cues; The view of User A who is locating a target using (f) PL and (g) MT; The view of User B who is now looking
the target that User A has selected using (d) PL and (e) MT.

ABSTRACT

Visual cues are essential in computer-mediated communication. It
is especially important when communication happens in a collabo-
ration scenario that requires focusing several users’ attention on a
specific object among other similar ones. This paper explores the
effect of visual cues on pointing tasks in co-located Augmented
Reality (AR) collaboration. A user study (N = 32, 16 pairs) was
conducted to compare two types of visual cues: Pointing Line (PL)
and Moving Track (MT). Both are head-based visual techniques.

∗Corresponding author (haining.liang@xjtlu.edu.cn)

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SUI ’21, November 09–10, 2021
© 2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00
https://doi.org/10.1145/1122445.1122456

Through a series of collaborative pointing tasks on objects with dif-
ferent states (static and dynamic) and density levels (low, medium
and high), the results showed that PL was better on task perfor-
mance and usability, but MT was rated higher on social presence
and user preference. Based on our results, some design implications
are provided for pointing tasks in co-located AR collaboration.

CCS CONCEPTS
• Human-centered computing → Mixed / augmented reality; Em-
pirical studies in collaborative and social computing; User studies.

KEYWORDS

Augmented Reality, Co-located collaboration, Visual cues, Pointing
Tasks

ACM Reference Format:
Lei Chen, Yilin Liu, Yue Li, Lingyun Yu, BoYu Gao, Maurizio Caon, Yong
Yue, and Hai-Ning Liang. 2021. Effect of Visual Cues on Pointing Tasks in
Co-located Augmented Reality Collaboration. In SUI ’21: ACM Symposium

 
 
 
 
 
 
SUI ’21, November 09–10, 2021

Chen, et al.

on Spatial User Interaction, November 09–10, 2021. ACM, New York, NY, USA,
12 pages. https://doi.org/10.1145/1122445.1122456

1 INTRODUCTION

Augmented Reality (AR) has been used for collaborative experi-
ences. It allows users to interact with shared virtual content while
having a view of the real world [6, 43, 58]. For these collaborative
experiences to be efficient and positive, there needs to be fluid com-
munication between the collaborators. In addition, knowing what
the collaborator is doing or which objects are being looked at can
improve the sense of awareness and social presence [17, 32, 33, 36].
One advantage of using AR head-mounted displays (HMDs) is their
ability to capture users’ head and gaze movements and use the data
to provide visual augmentation cues [15, 26, 57]. Such cues can
enhance situational awareness and social presence [3] and improve
user performance and usability in collaborative AR [45].

Previous research on AR primarily focused on remote collabo-
ration (e.g., [24–26, 47]). It is not clear how visual cues could be
helpful in enhancing user collaboration when they are in the same
physical environment. For pointing tasks specifically, many appli-
cations involve such pointing tasks during co-located collaboration
(e.g., games, education, training). It is therefore important to un-
derstand the appropriate techniques for such pointing tasks with
different object states and density levels. Users could be interacting
with objects that are not easily touchable (e.g., moving objects) and
clustered in dense regions with other objects with similar proper-
ties. Attempting to describe or pinpoint an object of interest using
verbal and hand gestures may not be practical and efficient. In this
research, we explore the use of visual cues to enhance the identifi-
cation of target objects in collaborative AR where these objects can
be static or dynamic and be in various levels of density.

Pointers [15, 34, 60] and annotations [27, 38] are two main visual
cues that have been explored. The majority of prior research focused
on hand-based techniques, e.g., to carry a pointer and to draw an
annotation in a virtual environment. However, these hand-based
techniques may not be efficient or ideal for AR systems [41, 42]. For
one, it can pose severe occlusion because the field-of-view of AR
HMDs is usually narrow. Also, AR systems, with the exception of
the Magic Leap, do not come with a pointing device or controllers.
Therefore, in this research, we explore two hands-free, head-
based techniques to simulate visual cues provided by pointers and
annotations. Pointing Line (PL) is used to simulate pointers. It in-
dicates a user’s line of sight and focus of attention. Moving Track
(MT) simulates the use of annotations. It records a continuous mov-
ing track to help identify a target. Based on the two techniques, we
investigate the effect of visual cues on task performance, usabil-
ity, and social presence when interacting with virtual objects in
a co-located AR. To do this, we ran a user study with two visual
cues (PL and MT) to allow paired users to share information about
the object of interest and identify it. The experiment involved both
static and dynamic objects in three levels of density (low, medium,
and high). Figure 1 (above) shows an overview of the experiment
setup.

In short, the paper makes the following two contributions:

• We report the results of a user study comparing the two types
of visual cues based on hands-free, head-based techniques
in co-located AR scenarios for pointing and selection tasks;
• Based on the results, we propose implications on the design
and use of these visual cues for co-located AR collaboration.

2 RELATED WORK
2.1 Collaborative AR Systems and Social

Presence

Collaboration is the “mutual engagement of participants in a co-
ordinated effort to solve a problem together” [49]. Collaborative
AR systems allow users to interact with shared AR content as natu-
rally as with physical objects, and to complete a task or achieve a
common goal [5, 12, 40]. Researchers have shown that AR systems
can effectively support a group of users to perform collaborative
activities [7, 31, 53, 54].

During collaboration, it is key to be aware of where and which ob-
ject(s) the collaborator is interested in or interacting with [2, 13, 37].
Providing visual, nonverbal cues is beneficial for improving the
awareness of users’ actions and the sense of being together, namely
social presence [10, 18]. In the early days, telepointers and cursors
[19] were explored to support user awareness of others’ actions on
a shared workspace in traditional platforms (e.g., desktop displays).
Recently, multimodal cues are used in collaborative environments,
typically using auditory and visual elements. For example, virtual
avatars have been explored to represent each collaborator and to
provide an increased awareness of others in the shared environ-
ment [21, 30, 46]. In AR environments, researchers explored the use
of virtual arrows to represent collaborators’ head directions [11]
and miniature virtual avatars to show collaborators’ gaze directions
and body gestures [46]. Although using avatars can contribute to
users’ perceived social presence, it adds extra visual elements to
the limited display and field-of-view of current AR HMDs. Thus,
this solution may not be ideal when there are multiple objects in
the environment. Using pointer and annotation cues is an unclut-
tered alternative to support social presence and collaboration in
AR. Specifically, sharing gaze pointing cues has been explored in
remote collaboration with AR devices [24, 29, 39]. For example, Ishii
et al. [29] and Higuchi et al. [24] reported that when users shared
their workspace, gaze pointing cues provided better understanding
where their partner was looking at. Lee et al. [39] and Higuchi et al.
[24] found that sharing gaze pointing cues significantly improved
users’ awareness of each other’s focus and joint attention.

In addition to gaze pointing cues, augmenting hand pointing cues
via gestures was shown to be able to facilitate users’ perception of
others’ actions. For example, Piumsomboon et al. [46] reported that
redirected gaze and gestures in their Mini-Me system improved
users’ awareness of the partner in a collaborative AR interface.
Yang et al. [59] stated that visual head frustum and hand gestures
intuitively demonstrated the remote user’s movements and target
positions. Kim et al. [33] added sketch cues in addition to gestures
and demonstrated an improved task efficiency with the enhanced
visual cues.

Although previous research has proven the positive effects of
visual cues on supporting social presence in collaborative activities,
most of these studies focused on remote AR systems. The effect

Effect of Visual Cues on Pointing Tasks in Co-located Augmented Reality Collaboration

SUI ’21, November 09–10, 2021

of visual cues in co-located AR collaboration is still largely under-
explored. It is not clear how different visual cues may enable and
enhance users’ perceived social presence when they are co-located
in the same physical space. Therefore, we put our focus on the
study of visual cues in co-located AR environments.

2.2 Visual Cues in Collaboration

Two noticeable visual cues were presented in previous work on
collaboration: pointer and annotation [15, 27, 34, 56]. Pointer cues
provide a pointing line, indicating a user’s line of sight and focus
of attention. Annotation cues record a moving track to identify a
target, such as a track of hand or head positions. Here, we discuss
these two types of visual cues in detail.

Pointing Line Cues. Previous work showed that pointing line
cues can effectively support communication [14, 19, 20, 44, 50, 51].
For example, Gupta [20] reported that presenting users’ gaze di-
rections using a pointer significantly improved the sense of co-
presence between users in remote collaboration. Piumsomboon et
al. [45] visualized three types of pointing line cues: the field-of-
view frustum, eye-gaze ray, and head-gaze ray. They reported that
these cues significantly improved user performance, usability, and
subjective preferences. They also found that head-gaze ray was
significantly less confusing to use than field-of-view.

Most of the previous research explored the effect of pointing line
cues with hand gestures or eye gaze [3, 51]. For hand gestures, if
users’ hands are occupied, it requires users to deliberately release
the object in their hand before pointing to a target. Some research
used eye gaze to provide pointing line cues [8, 52]. They stated
that eye gaze can lead to better performance and better teamwork
than head gaze, i.e., the direction that a user is facing towards but
not necessarily looking at. However, people’s eye gaze has a lot of
micro movements. Having to keep their eyes stable when pointing
at an object may lead to fatigue. Head-based techniques, on the
other hand, could mitigate this issue. By providing a gaze ray from a
user’s head to a target object, this can support observers’ awareness
of their collaborator’s attention and allow them to view the same
object [1]. However, studies on head-based pointing line cues are
very limited, especially in co-located AR collaboration.

Moving Track Cues. Moving track is one of the mostly stud-
ied visual communication cues [48, 55]. It has been found to be
more effective than pointing line cues for communicating spatial
information [16, 34]. While the pointing line cues present a point
around the target object, moving track cues provide users with a
track, namely a series of points leading to the target, and can help
guide users’ focus to key locations [41]. Early research has studied
the use of moving track cues on a shared video view [28, 55]. Re-
searchers have explored extensively the use of moving track cues
in remote collaborative physical tasks. For example, Billinghurst
et al. [4] presented a method to record stabilized moving tracks
in an annotation-based AR system. Teo et al. [56] reported that
local users in remote collaboration felt the task was easier and
more enjoyable with moving track cues, and the cues helped them
understand the attention and focus of the remote user. A recent
study [33] compared four combinations of visual cues provided by
hand gesture, pointer and sketch. They reported that participants

completed tasks faster and felt higher level of usability if sketch
(moving track) cues were provided.

Although there have been studies on moving track cues, such as
annotations and drawings, this research mainly used hand-based
techniques and focused on remote collaboration [26, 56]. Besides,
previous research on moving track cues also demonstrated some
limitations. Much of this research explored these cues as an anno-
tation tool. In this case, the track traces needed to be erased after
completing each step of a task [34]. Huang et al. [26] also men-
tioned that moving track cues that blocked users’ view could have
a negative impact on user experience and task performance. Based
on these findings, we have made the moving track cues to gradually
fade away and disappear in our study. This solution neither requires
users to actively erase any virtual element, nor prevents users from
viewing the workspace with additional visual elements.

In summary, our research is framed on previous work and com-
pares two head-based visual techniques, pointing line cues and
moving track cues, to explore their effect on pointing tasks in co-
located AR collaboration.

3 SYSTEM SETUP

To conduct this research, we created a multi-user AR collaborative
system (see Figure 1a). The system was developed with Unity (ver-
sion 2019.2.0f1) and was run on the Windows platform. Each of
the two users used a Meta2 connected to a laptop and with each
other via a private network and Photon Unity Networking. Either
laptop could be the host server. After they have been connected,
one would assume the host role and the two laptops’ timers were
synchronized to begin the experiment.

The equipment set up in this study consisted of (1) two Meta2
HMDs, an AR device with two optical see-through displays with a
2550 x 1440 pixel resolution, 90° field of view, and frame rate of 60
Hz. (see Figure 1a), and (2) two Windows 10 laptops each with an
Intel Core i9-8950HK at 2.9 GHz, 32 GB RAM, and NVIDIA GeForce
GTX 1080. This set up allows a pair of users to work in the same
virtual environment using the two Meta2 HMDs and jointly do any
tasks together. The system is able to collect data (e.g., completion
time and accuracy).

3.1 Visual Techniques

To determine the techniques selected for this study, we first con-
ducted a pilot study and explored both head-based and hand-based
approaches, each with and without a visual cue (i.e., baseline). The
results showed that head-based tended to be better in performance
and user preference for pointing tasks in co-located AR (which
was in line with other studies (e.g., [22, 59]). Given our pilot study
results and those from the literature, we did not include a baseline
technique to keep the study focused and within a reasonable time.
Therefore, our study is based on two head-based visual techniques
(i.e., Pointing Line and Moving Track) to compare their performance
and usability under different scenarios.

The two head-gaze techniques receive input from the user’s head
movement and is captured by the tracking function in the Meta2.
The cursor endpoint is based on a ray cast from the head’s center
toward the AR interface.

SUI ’21, November 09–10, 2021

Chen, et al.

Figure 2: The two visual techniques (a) Pointing Line (PL) and (b) Moving Track (MT) with an example each.

Pointing Line (PL). PL is shown as a red ray representing the
user’s viewing direction. It is emitted from the center of the user’s
head toward the user interface in AR environment. When it inter-
sects with an object, this object becomes highlighted and can be
selected. As shown in Figure 2a (upper row), when a user moves
his/her head, the ray also moves in synchronicity following the
head’s directional movements.

Moving Track (MT). MT uses a different approach to show di-
rectional movement. Instead of showing the ray, it shows a trail that
follows the cursor. To reduce excessive visual clutter, the technique
limits the length of the trail to 10 pixels. That is, the excess part of
the tail fades out and gradually disappears. As shown in Figure 2b
(lower row), as the user moves their head, the trail is displayed,
with end part becoming lighter and fading out gradually.

4 USER STUDY
4.1 Participants

16 pairs of participants (17 males) between the ages of 18 to 23 (M
= 20.78) were recruited from a local university for this experiment.
Only 3 pairs did not know each other before participating in the
experiment. All participants had normal or corrected-to-normal
vision and had no issues distinguishing the colors we used in the AR
device. They reported an average of 3.69 for cooperative ability on
a scale from 1 (‘so bad’) to 5 (‘good’). 13 of them (41%) had limited
prior experience with AR HMDs before.

4.2 Experiment Design

The experiment followed a 2 × 2 × 3 (2 Technique × 2 Object State ×
3 Density) within-subjects design to study the effects of visual cues
on pointing tasks in co-located AR collaboration. In this experiment,
we have the following three independent variables:

(1) Technique. We tested two visual techniques: Pointing Line
(PL) and Moving Track (MT). As mentioned above, PL is based
on the ray-casting approach to show a line to the object on
the direction of the AR HMD (see Figure 3a). MT provides a
continuous trace that follows the pointer (see Figure 3b).

(2) Object State. We explored two states: Static and Dynamic.
Static objects were immobile while the dynamic ones would
move around. As the arrows depicted in Figure 3c indicate,
dynamic objects would be randomly assigned a path with-
out any particular pattern. In dynamic scenarios, the objects
performed uniform rectilinear motion (i.e., constant veloc-
ity) with speeds between 50-120 pixels/second and random
directions. The placement and movement of the objects were
predefined to be same for each pair. All objects (target and
non-targets) would move and were programmed to avoid
each other. A virtual boundary of 120 degrees was set for
the objects. As this is wider than the 90-degree FOV of the
Meta2, users would need to turn their heads during the tasks.
(3) Density. The size of objects used in our study was 70 pixels
in diameter. There were three levels of object density: Low
(6 objects with no occlusion, Figure 3c), Medium (12 objects
with slight occlusion, Figure 3d) and High (18 objects with
severe occlusion, Figure 3e).

Figure 3: Two users completing the task with (a) Pointing
Line (PL) and (b) Moving Track (MT) technique. There were
(c) Low, (d) Medium and (e) High density for Static and Dy-
namic objects. Red indicates the target object; green indi-
cates the object selected.

Effect of Visual Cues on Pointing Tasks in Co-located Augmented Reality Collaboration

SUI ’21, November 09–10, 2021

Figure 4: An example of all the steps involved in a typical trail.

As a within-subjects experiment, each participant experienced
all 12 conditions. Technique was counterbalanced according to a
balanced Latin square design to minimize learning effects. Object
State and Density were randomly and equally distributed. There
were 6 trails for each condition and 72 trials in total for each pair
of participants.

This experiment was classified as low risk research and was
approved by the University Ethics Committee at Xi’an Jiaotong-
Liverpool University (#21-01-09).

4.3 Hypotheses

Based on our literature review and design of our experiment, we
postulated the following four hypotheses:

H1: PL and MT would both perform better with static targets

than with dynamic objects.

H2: PL and MT would have better performance in lower density
scenarios. High density situations could be crowded and have object
occlusion, making it more difficult to identity and select a target.
H3: PL’s more explicit cue showing viewing direction, it would

lead to improved task efficiency and accuracy than MT.

H4: In terms of subjective measures, in general, MT would be
more preferred by users than PL, as MT would make it easier to track
cursors movement and the user’s intention (in our case User A). As
such, it would increase collaborators’ awareness and co-presence.

4.4 Task and Procedure

Task. For each trial, User A was required to choose the target
object (see Step (1) in Figure 4, highlighted in red) by moving the

cursor to the object using head motions and confirming it via a
mouse click. The selected object would turn to green, which can
be seen by User B. Then User B would need to locate the same
object by moving the cursor and doing the confirmation. As such,
the task consisted of the following steps (see Figure 4): (1) system
randomly highlighted in red one object in the scene. Only User A
could see the highlighted target. At this moment, the objects in User
B’s view are all grey. (2) User A would choose the object using the
pre-defined technique. Once confirmed, the selected object would
turn into green and both users could see this object. (3) User B was
then required to locate the green object and confirm it. (4) One trial
was finished and the next would begin when both users were ready
to proceed.

In our experiment, the two users in each pair would do the
selection in turns. We designed the experiment so that they could
not see both the visual cues of their partner and their own at the
same time. During selection, each user could only see one visual
cue (the cue of the user needing to do the selection). In other words,
during the selection of User A, only the cue of User A would be
visible; while during the selection of User B, only the cue of User
B would be shown. Besides, at the beginning, only User A could
see the target highlighted by system. Only after User A completed
the selection would User B be able to see the selected target, now
selected by User A (see the Table 1 for a summary of this process).
This approach was chose to minimize visual clutter and to make
it more aligned with how collaboration takes in places in more
realistic scenarios.

Table 1: Visibility states of the visual cues of User A and User B and the target to be selected according to selection stage

During the selection of User A

During the selection of User B

User A’s
visual cue
Yes
Yes

User B’s
visual cue
No
No

Target to be Selected

Highlighted by System
Visible but not highlighted

User A’s
visual cue
No
No

User B’s
visual cue
Yes
Yes

User A
User B

Target to be Selected

Visible (Already Selected by this User)
Visible and Highlighted (Selected by User A)

SUI ’21, November 09–10, 2021

Chen, et al.

Figure 5: Mean completion time (Left) and accuracy rate (Right) results in all conditions: 2 Technique (PL and MT) × 2 Object
State (Static and Dynamic) × 3 Density (L: Low, M: Medium and H: High).

Procedure. The experiment procedure was divided into 4 phases:
(P1) Informing participants of the experiment goal and the ethics
regulations governing it, and then completing the consent form
plus a short questionnaire to collect anonymized demographic data
(∼5 minutes); (P2) Providing participants several practice trials
to let them become familiar with the AR device and the task (∼5
minutes); (P3) Participants completing the trials and in between con-
ditions each participant filling in the Social Presence, and Usability
questionnaires (∼25 minutes); and (P4) Interviewing participants
to collect further feedback and comments (∼5 minutes). The whole
experiment took about 40 minutes to complete for each pair.

4.5 Results

For the objective data analysis (completion time and accuracy rate),
we employed three-way repeated ANOVAs with an alpha value of
0.05 to determine any differences across conditions and followed by
pair-wise comparisons with Bonferroni correction for the data with
a significant difference. We used a Greenhouse-Geisser adjustment
to correct for violations of the sphericity assumption. We reported

effect sizes (𝜂𝑝 2). For data from the subjective questionnaires that
were ordinal, non-parametric data (e.g., subjective ratings or rank-
ings), we applied Wilcoxon signed-rank test to look for differences.
For simplicity, M and SD are used to denote mean and standard
deviation values.

4.5.1 Task Performance. Participants’ task completion time and ac-
curacy rate were collected to assess their performance. We recorded
the time taken by paired participants to complete each trial. Accu-
racy rate was measured by the number of correct trials among the
total number of trials. Figure 5 shows mean time and accuracy rate
for each condition (2 Technique × 2 Object State × 3 Density).

Completion Time. We found an interaction effect between Tech-
nique × Object State (F1,95 = 3.993, p = .049, 𝜂𝑝 2 = .040), Technique
× Density (F2,190 = 7.461, p = .001, 𝜂𝑝 2 = .073), Technique × Object
State × Density (F2,190 = 4.065, p = .019, 𝜂𝑝 2 = .041) on comple-
tion time, but no significance was found between Object State ×
Density (p = .758). For Technique × Object State, pairwise com-
parisons revealed that PL_Dynamic took significantly less time

Figure 6: Completion Time (a) and Accuracy Rate (b) according to Technique, Object State, and Density.

Effect of Visual Cues on Pointing Tasks in Co-located Augmented Reality Collaboration

SUI ’21, November 09–10, 2021

than MT_Dynamic (p = .001). PL_Static also took less time than
MT_Static, but no significant difference was found (p = .149). In
addition, both PL and MT techniques took significantly less time in
static condition than that in dynamic one (both p < .001).

For Technique × Density, PL_Medium was significantly faster
than MT_Medium (p < .001). Although we noticed that PL_High
and PL_Low were respectively faster than MT_High and MT_Low,
no significant difference was found (both p > .05). Besides, PL_High
was significantly slower than PL_Medium (p = .027) and PL_Low (p
< .001). MT_Low was significantly faster than MT_High (p = .001)
and MT_Medium (p < .001). For Technique × Object State × Density,
PL_Dynamic_Medium took less time than MT_Dynamic_Medium
(p < .001). For other conditions, although using PL was faster than
MT, there was not any significant difference (p > .05).

In addition, there was a significant effect of Technique (F1,95 =
14.559, p < .001, 𝜂𝑝 2 = .133), Object State (F1,95 = 33.360, p < .001,
𝜂𝑝 2 = .260), and Density (F2,190 = 15.303, p < .001, 𝜂𝑝 2 = .139) on
completion time. As shown in Figure 6a, post-hoc analyses indicated
that using PL was significantly faster than using MT. Besides, time
spent on trials with Static objects was significantly lower than with
Dynamic ones. Post-hoc analyses showed that completing High
and Medium density trials took significantly longer time than Low
density trials (both p < .001). We also found an increasing trend

of the mean time spent on Low, Medium and High density tasks.
More details of the main results on completion time are provided
in the table in Appendix A.

Accuracy Rate. Overall, we found a high average accuracy rate
for all conditions (M = 92.8%, SD = .257) in all trials (see Figure 5
(Right)). A further analysis showed that there was no significant
interaction effect of Technique × Object State (p = .688), Technique ×
Density (p = .988), and Technique × Object State × Density (p = .410)
on accuracy rate. When looking at the descriptive data, we found
PL_Dynamic and PL_Static got higher rates than MT_Dynamic and
MT_Static, respectively. Besides, for mean results of accuracy rate,
PL performed better than MT in static trials, while MT performed
better in dynamic trials. For Density, the accuracy rate of two
techniques gradually decreased with the increase in density. In
addition, from Figure 5 (Right), we can see that PL led to a higher
rate than MT in general in any density condition. More details of
all conditions on accuracy rate are shown in Table 2.

Besides, we found that there was a significant effect of Technique
on accuracy rate (F1,95 = 7.355, p = .008, 𝜂𝑝 2 = .072). As shown in
Figure 6b, Post-hoc analyses showed that participants with PL got
significantly higher accuracy rate than ones with MT. In addition,
we noticed that the rate decreased gradually with the increase in

Table 2: Means (Standard Deviations) of accuracy rate on different conditions.

Static
Medium High
92.7%
93.8%
(26.1%)
(24.3%)
91.7%
87.5%
(27.8%)
(33.2%)

Low
97.9%
(14.4%)
93.8%
(24.3%)

PL

MT

Overall Mean

94.8% (22.3%)

91.0% (28.7%)

Low
95.8%
(20.1%)
90.6%
(29.3%)

Dynamic

Medium High
94.8%
95.8%
(22.3%)
(20.1%)
87.5%
92.7%
(33.2%)
(26.1%)

Overall Mean

95.5% (20.8%)

90.3% (29.7%)

Figure 7: Users’ ratings on Social presence in all conditions (2 Techniques × 2 Object States); Significance results are highlighted
in Bold; PL: Pointing Line, MT: Moving Track; S: Static, D: Dynamic; Subscales, CP: Co-presence, AA: Attention Allocation, and
PMU: Perceived Message Understanding.

SUI ’21, November 09–10, 2021

Chen, et al.

density of objects. The Static and Dynamic trials led to the same
rate. However, we did not find any significant difference of Object
State and Density (all p > .05).

4.5.2 User Experience. Participants’ collaboration and user expe-
rience were quantified using the data from post-experiment ques-
tionnaires that contained Likert-scale questions. We collected three
sets of subjective data: Social Presence, System Usability Scale, and
User Preference.

Social Presence. We adapted the Social Presence Questionnaire
[23] according to the nature of our trials. 3 sub-scales, Co-presence
(CP), Attention Allocation (AA) and Perceived Message Understand-
ing (PMU), were used and consisted of 9 rating items on a 7-point
Likert scale (1: Strongly Disagree ∼ 7: Strongly Agree). Overall,
participants had a high feeling of social presence when interacting
in AR (M = 5.507, SD = .895). Besides, we found that MT was rated
higher than PL in all conditions for median scores (see Figure 7).
A Wilcoxon signed-rank test revealed that there was a significant
interaction effect of social presence between PL and MT in dynamic
trials (Z = -1.993, p = .046) but no interaction significance was found
in static trials (p = .180). Besides, there was a significant effect of
Technique on social presence (Z = -2.618, p = .009). Pairwise com-
parisons revealed that overall MT got a significantly higher score
than PL.

For the subscales, the Wilcoxon signed-rank test revealed a signif-
icant difference of CP on PL and MT (Z = -2.142, p = .032). Post-hoc
tests showed that MT got a significantly higher score than PL (Z
= -1.961, p = .050) in Dynamic trials but there was no significance
in Static trials (p = .310). For AA, we found a significant difference
between PL and MT (Z = -2.128, p = .033). MT got a significantly
higher score than MT in Dynamic trials (Z = -1.976, p = .048), but
there was no significance in Static trials (p = .163). We did not find
any significance on the PMU subcale (all p > .05).

System Usability Scale (SUS). SUS [9] was selected to measure
and evaluate the participants’ responses towards the usability of the
two visual techniques. It consisted of 10 items using 5-point Likert
scales (1: Strongly Disagree ∼ 5: Strongly Agree). Participants gave
an average of 78.162 (SD = 12.958) and 73.235 (SD = 14.414) for PL
and MT, respectively. Figure 8 shows the users’ rating with median
and significance results for all conditions. A Wilcoxon signed-rank
test showed that there was a significant difference between PL and
MT (Z = -2.611, p = .009). Pairwise comparisons showed a significant
effect between PL and MT in Dynamic trials (Z = -2.042, p = .041).
No other significance was found (all p > .05).

User Preference. At the end of the experiment, we asked par-
ticipants to choose their preferred technique for different types of
trials (see Figure 9). Overall, MT (54.69%) was more favored by par-
ticipants than PL (45.31%). For the Static_Low and Dynamic_Low
trials, the proportion of MT was not higher than PL. Except for
these 2 types of trials, MT got always a higher number of votes.
One interesting finding was that both in Static and Dynamic trials,
the higher the density of objects, more participants would choose
MT.

Figure 8: Users’ ratings of SUS on the technique’s us-
ability with significance (*: statistically significant); SUS
score ∈ [0, 100], the higher, the better.

Figure 9: User preference of the two visual techniques.

5 DISCUSSION

In this paper, we explored the use of two visual techniques (Point-
ing Line (PL) and Moving Track (MT)) for providing awareness
cues during collaborative exploration in AR systems for static and
dynamic objects and with different levels of object density (Low,
Medium and High). Overall, results from our user study indicated
that these visual cues provided benefits. We next discuss the find-
ings in more detail.

5.1 User Performance and Usability: PL > MT

When completing the trials, the density and state of objects affected
the participants’ performance. This was expected and our results
verified that the time needed for selecting dynamic objects was
significantly longer while the accuracy was lower, which supports
H1. The accuracy rate decreased with the increase of object density,
which also led to increase the time spent on the tasks. In particular,
the time spent on lower density trials was significantly less than
on higher density trials, which is in line with our expectations and
also aligns with H2.

Effect of Visual Cues on Pointing Tasks in Co-located Augmented Reality Collaboration

SUI ’21, November 09–10, 2021

Our results show that PL led to significantly higher performance
than MT, which aligns with H3. This result aligns with the findings
from Kim et al. [33] who found that a pointer could be a main visual
communication cue with fast completion time. On the other hand,
other studies [16, 35] found that users completed assembly tasks
faster with drawing sketch (moving track) cues than with pointing
cues, which differs from our results. However, this difference can
be explained with reference to the nature of the tasks in the 2
experiments. For annotations, once drawn, the moving track cues
remain in the shared task space so the information is available
until it is erased. This can be beneficial for assembling tasks as in
the cited experiment. In our scenario, instead, keeping the moving
track cues would add further visual details to an already busy AR
environment and, as such, may not be so convenient (e.g., in high
density cases). For completion time, specifically, PL contributed
to a significant higher efficiency than MT in dynamic trials with
medium density. In other conditions, PL still performed better than
MT in general. In other words, despite the possibility of PL cues
occluding more users’ view when objects were motion, this did not
lower their performance. One participant said that "PL was easy to
control even if the objects were moving", which could have provided
some degree of counterbalance. As such, this result shows that
regardless of whether objects were moving or not and the density
level, PL cues seem more beneficial on task performance.

While the accuracy rate was high for both visual techniques, PL
was significantly better than MT, which supports H3. For mean
values, PL led to higher accuracy than MT in all conditions. It seems
that PL was more intuitive when showing the position of the target
that one user was watching. Some participants (N = 3) mentioned
that since a pointing ray was emitted from the user’s head, they
would have control over the start point and orientation of the ray,
much like a physical laser pointer. Participants found MT slightly
more difficult to use, which resulted in taking significantly longer
time to complete the trials. In general, our results show that PL was
significantly better than MT on task performance, especially for
improving task efficiency in dynamic trials with medium density
level of objects.

Results from SUS show that participants rated PL significantly
higher than MT. It was preferred significantly more for dynamic
trials. In general, participants found that showing a visual line from
the head to a target allowed them to see the direction of pointing
and this extra implicit information led to improved performance
and higher accuracy. Participants used the words ’intuitive’ and
’natural’ to describe PL but not for MT.

5.2 Social Presence and User Preference: PL <

MT

We found that MT yielded significantly higher ratings on social
presence than PL overall and, specifically, in dynamic trials, which
supports H4. In particular, MT was rated significantly higher than
PL on Co-presence and Attention Allocation for both overall and
in dynamic trials. The visual tail of MT moving behind the cursor
seems to have allowed users to focus and encouraged them to
predict the end target. By focusing on the path, participants said that
they ‘felt more connected’ with the thinking of the other user and
could sense their presence better. MT’s emphasis on the trajectory

path in real-time seems to have enhanced their feeling of social
presence with the other user. One user mentioned that "it is very
interesting to follow the movement path", and another one said that
"I can not only know what the target location is, but also where the
focus view starts. This way, I can strongly feel my partner being with
me together."

Interestingly, when asked which technique they prefer to use,
MT was rated higher than PL in most conditions, except for low
density. This will support our H4. Participants in general said that
they prefer MT over PL but there is a general agreement that PL
has a better usability, is more intuitive to use, and can help them
perform faster. This also explains the findings from the SUS results.
This result shows that when working with another person, the
feeling of being together, of being connected (even by using a
simple visual cue) with the collaborator is an important factor
that affects user experience. Some prior studies seem to support
our results. For example, Teo et al. [56] reported that all of their
participants preferred having visual annotation cues, and also stated
that participants felt that the task was easier and more enjoyable
with dynamic visual cues. Their findings are aligned with our results.
Unlike performance results, these results are more subjective and
reflect the emotional side of working with other users, which is an
important aspect for collaborative systems.

In short, the results show that PL was better on task perfor-
mance and usability but MT was rated on social presence and user
preference.

5.3 Design Implications of our Findings

Our results and findings point to the following implications for
the design and use of visual cues in co-located AR that involves
pointing tasks:

• The objective results showed that users performed better
with Pointing Lines (PL). Therefore, if the goal is to maxi-
mize task performance, especially considering efficiency and
accuracy, a technique like the PL could be helpful.

• PL is considered easy to control and seems more usable,
which can transfer to improved task performance. As such,
when ease of control and high usability is important, a PL-
based technique could be beneficial, especially when deal-
ing with dynamic objects within a medium level of density
(slightly occluded and crowded conditions).

• Based on users’ preference and feedback, it seems that being
able to follow the movement of the partner’s viewing direc-
tion provides a higher focus and attention. In this context,
they rated Moving Tracking (MT) higher than PL. Therefore,
if the goal is to provide higher collaborative experience and
social presence, a technique similar to MT is more suitable.

6 LIMITATIONS AND FUTURE WORK

This research has some limitations, which can serve as directions
for future work. As stated earlier, given the results of our pilot study
and our literature review, we did not include a baseline technique as
by including one it would unnecessarily lengthen our experiment
unnecessarily but without providing additional insights. Future
work could involve more variations and different implementations
of the two visual techniques, a baseline technique can be helpful

SUI ’21, November 09–10, 2021

Chen, et al.

to allow pinpointing the effect of more specific aspects of visual
techniques for pointing tasks in collaborative scenarios.

Although the size of our sample population is in line with some
publications reporting similar experiments (e.g., [27, 45]), the sam-
ple size was not very large. However, our data still had enough
power and was able to show significant differences across the vari-
ous conditions of the experiment, leading to some interesting find-
ings. In the future, it will be useful to run a study with a larger
sample (that has a more diverse group of participants, including
pairs who are familiar and unfamiliar with each other) and see if the
same results hold or new insights are found. Users in the current
study have specific roles, i.e., one initiates the task and the other
follows. It would be interesting to study more complex tasks where
users’ roles are not clearly specified (and can switch back and forth
between roles). Future work may also investigate how visual cues
can facilitate communication without a specific task and in more
fluid and flexible scenarios).

We did not consider the effect of position arrangement of each
pair of users and cases where they are located in separate physical
spaces. There may not be a significant effect with MT. However, it
is not clear what the effect is with PL because it provides implicit
direction information about where the user is looking at. In the
future, it will be helpful to explore cases where users are moving
and positioned at different locations and orientations and when
they are separated physically.

In this experiment, we did not consider other types of non-visual
sensorial cues, like audio and haptic feedback. While the imple-
mentation of such cues requires careful design, it is interesting
and useful because AR systems are multimodal and suitable for
combining several modes of sensorial feedback to create a more
immersive work environment.

7 CONCLUSION

In this paper, we evaluated the effect of visual cues on pointing
task performance, usability, and social presence in co-located AR
collaboration. A user study was conducted by comparing two differ-
ent visual cues (Pointing Line (PL) and Moving Track (MT)) during
collaboration with Static and Dynamic tasks in Low, Medium, and
High levels of object density. Based on the results of an experiment
following a 2 × 2 × 3 (2 Technique × 2 Object State × 3 Density)
within-subjects design with 16 pairs of participants, we found that
users with PL cues performed better than with MT cues on task
performance. Users were more positive about the usability of PL
than MT, especially in dynamic tasks with a medium level of object
density. Besides, we found that MT cues were useful for enhancing
the sense of social presence and user experience when completing
pointing tasks in co-located AR. Overall, our results show that PL
was better on task performance and usability, while MT was better
on social presence and user preference. With these findings, we
discussed the implications for the design and use of visual cues for
co-located AR collaboration.

ACKNOWLEDGMENTS

The authors wish to thank the participants for their time and the
reviewers for their insightful comments that have helped improve

our paper. This research was funded in part by Xi’an Jiaotong-
Liverpool University’s Key Special Fund (#KSF-A-03 and #KSF-A-19)
and Research Development Fund (#RDF-16-02-43), the Natural Sci-
ence Foundation of Guangdong Province (#2021A1515012629), and
Guangzhou Basic and Applied Basic Foundation (#202102021131).

REFERENCES
[1] Christoph Anthes and Jens Volkert. 2005. A toolbox supporting collaboration in
networked virtual environments. In International Conference on Computational
Science. Springer, 383–390.

[2] Miguel Antunes, António Rito Silva, and Jorge Martins. 2001. An abstraction for
awareness management in collaborative virtual environments. In Proceedings of
the ACM symposium on Virtual reality software and technology. 33–39.

[3] Huidong Bai, Prasanth Sasikumar, Jing Yang, and Mark Billinghurst. 2020. A
user study on mixed reality remote collaboration with eye gaze and hand gesture
sharing. In Proceedings of the 2020 CHI conference on human factors in computing
systems. 1–13.

[4] Mark Billinghurst, Adrian Cheok, Simon Prince, and Hirokazu Kato. 2002. Real
world teleconferencing. IEEE Computer Graphics and Applications 22, 6 (2002),
11–13.

[5] Mark Billinghurst and Hirokazu Kato. 1999. Collaborative mixed reality. In
Proceedings of the First International Symposium on Mixed Reality. 261–284.
[6] Mark Billinghurst and Hirokazu Kato. 2002. Collaborative augmented reality.

Commun. ACM 45, 7 (2002), 64–70.

[7] Mark Billinghurst, Hirokazu Kato, Kiyoshi Kiyokawa, Daniel Belcher, and Ivan
Poupyrev. 2002. Experiments with face-to-face collaborative AR interfaces. Vir-
tual Reality 6, 3 (2002), 107–121.

[8] Jonas Blattgerste, Patrick Renner, and Thies Pfeiffer. 2018. Advantages of eye-
gaze over head-gaze-based selection in virtual and augmented reality under
varying field of views. In Proceedings of the Workshop on Communication by Gaze
Interaction. 1–9.

[9] John Brooke et al. 1996. SUS-A quick and dirty usability scale. Usability evaluation

in industry 189, 194 (1996), 4–7.

[10] Bill Buxton. 2009. Mediaspace–meaningspace–meetingspace. In Media space 20+

years of mediated life. Springer, 217–231.

[11] Jeffrey W Chastine, Kristine Nagel, Ying Zhu, and Luca Yearsovich. 2007. Un-
derstanding the design space of referencing in collaborative augmented reality
environments. In Proceedings of graphics interface 2007. 207–214.

[12] Lei Chen, Hai-Ning Liang, Feiyu Lu, Konstantinos Papangelis, Ka Lok Man, and
Yong Yue. 2020. Collaborative behavior, performance and engagement with visual
analytics tasks using mobile devices. Human-centric Computing and Information
Sciences 10, 1 (2020), 1–24.

[13] Paul Dourish and Victoria Bellotti. 1992. Awareness and coordination in shared
workspaces. In Proceedings of the 1992 ACM conference on Computer-supported
cooperative work. 107–114.

[14] Thierry Duval, Thi Thuong Huyen Nguyen, Cédric Fleury, Alain Chauffaut,
Georges Dumont, and Valérie Gouranton. 2014. Improving awareness for 3D
virtual collaboration by embedding the features of users’ physical environments
and by augmenting interaction tools with cognitive feedback cues. Journal on
Multimodal User Interfaces 8, 2 (2014), 187–197.

[15] Austin Erickson, Nahal Norouzi, Kangsoo Kim, Ryan Schubert, Jonathan Jules,
Joseph J LaViola, Gerd Bruder, and Gregory F Welch. 2020. Sharing gaze rays for
visual target identification tasks in collaborative augmented reality. Journal on
Multimodal User Interfaces 14, 4 (2020), 353–371.

[16] Susan R Fussell, Leslie D Setlock, Jie Yang, Jiazhi Ou, Elizabeth Mauer, and
Adam DI Kramer. 2004. Gestures over video streams to support remote collabo-
ration on physical tasks. Human-Computer Interaction 19, 3 (2004), 273–309.
[17] Stéphanie Gerbaud and Bruno Arnaldi. 2008. Scenario sharing in a collaborative
virtual environment for training. In Proceedings of the 2008 ACM symposium on
Virtual Reality Software and Technology. 109–112.

[18] Darren Gergle, Robert E Kraut, and Susan R Fussell. 2013. Using visual infor-
mation for grounding and awareness in collaborative tasks. Human–Computer
Interaction 28, 1 (2013), 1–39.

[19] Saul Greenberg, Carl Gutwin, and Mark Roseman. 1996. Semantic telepointers
for groupware. In Proceedings Sixth Australian Conference on Computer-Human
Interaction. IEEE, 54–61.

[20] Kunal Gupta, Gun A Lee, and Mark Billinghurst. 2016. Do you see what I see?
The effect of gaze tracking on task space remote collaboration. IEEE transactions
on visualization and computer graphics 22, 11 (2016), 2413–2422.

[21] Carl Gutwin, Saul Greenberg, and Mark Roseman. 1996. Workspace awareness in
real-time distributed groupware: Framework, widgets, and evaluation. In People
and Computers XI. Springer, 281–298.

[22] John Paulin Hansen, Kristian Tørning, Anders Sewerin Johansen, Kenji Itoh,
and Hirotaka Aoki. 2004. Gaze typing compared with input by head and hand.
In Proceedings of the 2004 symposium on Eye tracking research & applications.

Effect of Visual Cues on Pointing Tasks in Co-located Augmented Reality Collaboration

SUI ’21, November 09–10, 2021

[46] Thammathip Piumsomboon, Gun A Lee, Jonathon D Hart, Barrett Ens, Robert W
Lindeman, Bruce H Thomas, and Mark Billinghurst. 2018. Mini-me: An adaptive
avatar for mixed reality remote collaboration. In Proceedings of the 2018 CHI
conference on human factors in computing systems. 1–13.

[47] Thammathip Piumsomboon, Youngho Lee, Gun Lee, and Mark Billinghurst. 2017.
CoVAR: a collaborative virtual and augmented reality system for remote collabo-
ration. In SIGGRAPH Asia 2017 Emerging Technologies. 1–2.

[48] Jun Rekimoto and Katashi Nagao. 1995. The world through the computer: Com-
puter augmented interaction with real world environments. In Proceedings of the
8th annual ACM symposium on User interface and software technology. 29–36.

[49] Jeremy Roschelle and Stephanie D Teasley. 1995. The construction of shared
knowledge in collaborative problem solving. In Computer supported collaborative
learning. Springer, 69–97.

[50] Nobuchika Sakata, Takeshi Kurata, Takekazu Kato, Masakatsu Kourogi, and
Hideaki Kuzuoka. 2003. WACL: Supporting Telecommunications Using Wearable
Active Camera with Laser Pointer.. In ISWC, Vol. 2003. Citeseer, 7th.

[51] Maurício Sousa, Rafael Kufner dos Anjos, Daniel Mendes, Mark Billinghurst, and
Joaquim Jorge. 2019. Warping deixis: distorting gestures to enhance collaboration.
In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems.
1–12.

[52] Oleg Špakov, Howell Istance, Kari-Jouko Räihä, Tiia Viitanen, and Harri Siirtola.
2019. Eye gaze and head gaze in collaborative games. In Proceedings of the 11th
ACM Symposium on Eye Tracking Research & Applications. 1–9.

[53] Zsolt Szalavári, Erik Eckstein, and Michael Gervautz. 1998. Collaborative gaming
in augmented reality. In Proceedings of the ACM symposium on Virtual reality
software and technology. 195–204.

[54] Matthew Tait and Mark Billinghurst. 2015. The effect of view independence in a
collaborative AR system. Computer Supported Cooperative Work (CSCW) 24, 6
(2015), 563–589.

[55] John C Tang and Scott L Minneman. 1991. VideoDraw: a video interface for
collaborative drawing. ACM Transactions on Information Systems (TOIS) 9, 2
(1991), 170–184.

[56] Theophilus Teo, Gun A Lee, Mark Billinghurst, and Matt Adcock. 2018. Hand
gestures and visual annotation in live 360 panorama-based mixed reality remote
collaboration. In Proceedings of the 30th Australian Conference on Computer-
Human Interaction. 406–410.

[57] Wenge Xu, Hai-Ning Liang, Anqi He, and Zifan Wang. 2019. Pointing and
Selection Methods for Text Entry in Augmented Reality Head Mounted Displays.
(2019), 279–288. https://doi.org/10.1109/ISMAR.2019.00026

[58] Hirotake Yamazoe and Tomoko Yonezawa. 2014. Synchronized AR environment
for multiple users using animation markers. In Proceedings of the 20th ACM
Symposium on Virtual Reality Software and Technology. 237–238.

[59] Jing Yang, Prasanth Sasikumar, Huidong Bai, Amit Barde, Gábor Sörös, and Mark
Billinghurst. 2020. The effects of spatial auditory and visual cues on mixed reality
remote collaboration. Journal on Multimodal User Interfaces 14, 4 (2020), 337–352.
[60] Difeng Yu, Hai-Ning Liang, Xueshi Lu, Kaixuan Fan, and Barrett Ens. 2019.
Modeling Endpoint Distribution of Pointing Selection Tasks in Virtual Reality
Environments. ACM Trans. Graph. 38, 6, Article 218 (Nov. 2019), 13 pages. https:
//doi.org/10.1145/3355089.3356544

A APPENDIX A: SUMMARY TABLE OF MAIN

RESULTS OF COMPLETION TIME
SHOWING SIGNIFICANT DIFFERENCES
BETWEEN CONDITIONS.

131–138.

[23] Chad Harms and Frank Biocca. 2004. Internal consistency and reliability of the

networked minds measure of social presence. (2004).

[24] Keita Higuch, Ryo Yonetani, and Yoichi Sato. 2016. Can eye help you? Effects of
visualizing eye fixations on remote collaboration scenarios for physical tasks. In
Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems.
5180–5190.

[25] Weidong Huang, Leila Alem, Franco Tecchia, and Henry Been-Lirn Duh. 2018.
Augmented 3D hands: a gesture-based mixed reality system for distributed col-
laboration. Journal on Multimodal User Interfaces 12, 2 (2018), 77–89.

[26] Weidong Huang, Mark Billinghurst, Leila Alem, and Seungwon Kim. 2018.
HandsInTouch: sharing gestures in remote collaboration. In Proceedings of the
30th Australian Conference on Computer-Human Interaction. 396–400.

[27] Weidong Huang, Seungwon Kim, Mark Billinghurst, and Leila Alem. 2019. Shar-
ing hand gesture and sketch cues in remote collaboration. Journal of Visual
Communication and Image Representation 58 (2019), 428–438.

[28] Hiroshi Ishii, Minoru Kobayashi, and Kazuho Arita. 1994. Iterative design of

seamless collaboration media. Commun. ACM 37, 8 (1994), 83–97.

[29] Hiroshi Ishii, Minoru Kobayashi, and Jonathan Grudin. 1993.

Integration of
interpersonal space and shared workspace: ClearBoard design and experiments.
ACM Transactions on Information Systems (TOIS) 11, 4 (1993), 349–375.

[30] Dongsik Jo, Ki-Hong Kim, and Gerard Jounghyun Kim. 2016. Effects of avatar
and background representation forms to co-presence in mixed reality (MR) tele-
conference systems. In SIGGRAPH ASIA 2016 virtual reality meets physical reality:
modelling and simulating virtual humans and environments. 1–4.

[31] Hannes Kaufmann. 2003. Collaborative augmented reality in education. Institute
of Software Technology and Interactive Systems, Vienna University of Technology
(2003).

[32] Seungwon Kim, Mark Billinghurst, and Gun Lee. 2018. The effect of collaboration
styles and view independence on video-mediated remote collaboration. Computer
Supported Cooperative Work (CSCW) 27, 3 (2018), 569–607.

[33] Seungwon Kim, Gun Lee, Weidong Huang, Hayun Kim, Woontack Woo, and
Mark Billinghurst. 2019. Evaluating the combination of visual communication
cues for HMD-based mixed reality remote collaboration. In Proceedings of the
2019 CHI conference on human factors in computing systems. 1–13.

[34] Seungwon Kim, Gun A Lee, and Nobuchika Sakata. 2013. Comparing pointing
and drawing for remote collaboration. In 2013 IEEE International Symposium on
Mixed and Augmented Reality (ISMAR). IEEE, 1–6.

[35] Seungwon Kim, Gun A Lee, Nobuchika Sakata, Andreas Dünser, Elina Vartiainen,
and Mark Billinghurst. 2013. Study of augmented gesture communication cues
and view sharing in remote collaboration. In 2013 IEEE International Symposium
on Mixed and Augmented Reality (ISMAR). IEEE, 261–262.

[36] Robert E Kraut, Mark D Miller, and Jane Siegel. 1996. Collaboration in perfor-
mance of physical tasks: Effects on outcomes and communication. In Proceedings
of the 1996 ACM conference on Computer supported cooperative work. 57–66.
[37] Jérémy Lacoche, Nico Pallamin, Thomas Boggini, and Jérôme Royan. 2017. Col-
laborators awareness for user cohabitation in co-located collaborative virtual
environments. In Proceedings of the 23rd ACM Symposium on Virtual Reality
Software and Technology. 1–9.

[38] Joseph LaViola, Loring S Holden, Andrew S Forsberg, Dom S Bhuphaibool, and
Robert C Zeleznik. 1998. Collaborative conceptual modeling using the sketch
framework. (1998).

[39] Gun A Lee, Seungwon Kim, Youngho Lee, Arindam Dey, Thammathip Piumsom-
boon, Mitchell Norman, and Mark Billinghurst. 2017. Improving Collaboration
in Augmented Video Conference using Mutually Shared Gaze.. In ICAT-EGVE.
197–204.

[40] Bo Li, Ruding Lou, Javier Posselt, Frédéric Segonds, Frédéric Merienne, and
Andras Kemeny. 2017. Multi-view VR system for co-located multidisciplinary
collaboration and its application in ergonomic design. In Proceedings of the 23rd
ACM Symposium on Virtual Reality Software and Technology. 1–2.

[41] Xueshi Lu, Difeng Yu, Hai-NIng Liang, and Jorge Goncalves. 2021. iText: Hands-
free Text Entry on an Imaginary Keyboard for Augmented Reality Systems. (2021),
1–11. https://doi.org/10.1145/3472749.3474788

[42] Xueshi Lu, Difeng Yu, Hai-Ning Liang, Wenge Xu, Yuzheng Chen, Xiang Li, and
Khalad Hasan. 2020. Exploration of Hands-free Text Entry Techniques For Virtual
Reality. In 2020 IEEE International Symposium on Mixed and Augmented Reality
(ISMAR). 344–349. https://doi.org/10.1109/ISMAR50242.2020.00061

[43] Stephan Lukosch, Mark Billinghurst, Leila Alem, and Kiyoshi Kiyokawa. 2015.
Collaboration in augmented reality. Computer Supported Cooperative Work
(CSCW) 24, 6 (2015), 515–525.

[44] Ohan Oda, Carmine Elvezio, Mengu Sukan, Steven Feiner, and Barbara Tversky.
2015. Virtual replicas for remote assistance in virtual and augmented reality.
In Proceedings of the 28th Annual ACM Symposium on User Interface Software &
Technology. 405–415.

[45] Thammathip Piumsomboon, Arindam Dey, Barrett Ens, Gun Lee, and Mark
Billinghurst. 2019. The effects of sharing awareness cues in collaborative mixed
reality. Frontiers in Robotics and AI 6 (2019), 5.

SUI ’21, November 09–10, 2021

Chen, et al.

Variable

Condition

Mean (SD) / s

ANOVA - P

Post-hoc Test Results

Technique

Object State

Density

Technique
×
Object State

Technique
×
Density

Technique
×
Object State
×
Density

PL
MT
S
D
L
M
H
PL_S
PL_D
MT_S
MT_D
PL_L
PL_M
PL_H
MT_L
MT_M
MT_H
PL_S_L
PL_S_M
PL_S_H
PL_D_L
PL_D_M
PL_D_H
MT_S_L
MT_S_M
MT_S_H
MT_D_L
MT_D_M
MT_D_H

3.339 (1.103)
3.556 (1.205)
3.245 (0.960)
3.651 (1.299)
3.202 (1.103)
3.554 (1.181)
3.587 (1.157)
3.195 (0.981)
3.483 (1.197)
3.294 (0.937)
3.818 (1.376)
3.161 (1.114)
3.392 (0.937)
3.551 (1.200)
3.243 (1.094)
3.717 (1.366)
3.622 (1.115)
2.951 (0.894)
3.250 (1.006)
3.385 (0.998)
3.372 (1.267)
3.360 (0.894)
3.718 (1.357)
3.024 (0.909)
3.424 (0.982)
3.435 (0.866)
3.461 (1.218)
4.184 (1.516)
3.809 (1.295)

p < .001***

PL < MT (p < .001***)

p < .001***

S < D (p < .001***)

p < .001***

L < H (p < .001***)
L < M (p < .001***)
M < H (P = .998)

p = .049*

PL_D < MT_D (p = .001**)
PL_S < MT_S (P = .149)

p = .001**

PL_M < MT_M (p < .001***)
PL_H < MT_H (P = .381)
PL_L < MT_L (p = .428 )

p = .019*

PL_D_M < MT_D_M (p < .001***)
PL_D_H < MT_D_H (p = .494)
PL_D_L < MT_D_L (p = .579)
PL_S_H < MT_S_H (p = .588)
PL_S_M < MT_S_M (p = .103)
PL_S_L < MT_S_L (p = .516)

Note: Level of significance: *<0.05, ** <0.01, and *** <0.001; Significant results are highlighted in Bold; PL: Pointing Line, MT: Moving Track;
S: Static, D: Dynamic; L: Low, M: Medium, H: High.

