6
1
0
2

v
o
N
4

]
T
S
.
h
t
a
m

[

1
v
0
4
2
1
0
.
1
1
6
1
:
v
i
X
r
a

COMPUTER ALGEBRA DERIVATION OF
THE BIAS OF BURG ESTIMATORS

Y. ZHANG and A.I. MCLEOD
Department of Statistical and Actuarial Sciences,
The University of Western Ontario, London, Ontario N6A 5B7
Canada

September 14, 2004

Abstract. A symbolic method is discussed which can be used to

obtain the asymptotic bias and variance to order O(1/n) for estimators in

stationary time series. Using this method the bias to O(1/n) of the Burg

estimator in AR(1) and AR(2) models is shown to be equal to that of the

least squares estimators in both the known and unknown mean cases.

Previous researchers have only been able to obtain simulation results for

this bias because this problem is too intractable without using computer

algebra.

Keywords. Asymptotic bias and variance; autoregression;

autoregressive spectral analysis; symbolic computation.

Preprint

Ying Zhang and A. Ian McLeod (2006), Computer Algebra Derivation

of the Bias of Burg Estimators, Journal of Time Series Analysis 27, 157-165

1

 
 
 
 
 
 
1. INTRODUCTION AND SUMMARY

Tjøstheim and Paulsen (1983, Correction 1984) showed that the

Yule-Walker estimates had very large mean-square errors in the AR(2) case

when the parameters were near the admissible boundary and that this

inﬂated mean square error was due to bias. This result was demonstrated

by Tjøstheim and Paulsen (1983) in simulation experiments as well as by

deriving the theoretical bias to order O(1/n). It was also mentioned by

Tjøstheim and Paulsen (1983, p.397, §5) that the bias results from

simulation experiments for the Burg estimates were similar to those

obtained for least squares estimates but that they had not been able to

obtain the theoretical bias term. Using computer algebra we are now able

to compute the bias for the Burg estimator and to show that to order

O(1/n) it is equal to the bias for the least squares estimator in the case of

AR(1) and AR(2) models in both the known and unknown mean cases.

As pointed out by Lysne and Tjøstheim (1987), the Burg estimators

have an important advantage over the least squares estimates for

autoregressive spectral estimation since Burg estimates always lie in the

admissible parameter space whereas the least squares estimates do not.

Burg estimators are now frequently used in autoregressive spectral

estimation (Percival and Walden, 1993, §9.5) since they provide better

resolution of sharp spectral peaks. As the Yule-Walker estimators, the Burg

estimators may be eﬃciently computed using the Durbin-Levinson

recursion. Our result provides further justiﬁcation for the recommendation

to use the Burg estimator for autoregressive spectral density estimation as

2

well as for other autoregressive estimation applications.

It has been shown that symbolic algebra could greatly simplify

derivations of asymptotic expansions in the IID case (Andrews and

Staﬀord, 1993). Symbolic computation is a powerful tool for handling

complicated algebraic problems that arise with expansions of various types

of statistics and estimators (Andrews and Staﬀord, 2000) as well as for

exact maximum likelihood computation (Currie, 1995; Rose and Smith,

2000). Cook and Broemeling (1995) show how symbolic computation can

be used in Bayesian time series analysis. Smith and Field (2001) described

a symbolic operator which calculates the joint cumulants of the linear

combinations of products of discrete Fourier transforms. A symbolic

computation approach to mathematical statistics is discussed by Rose and

Smith (2002). In the following sections, we develop a symbolic computation

method that can be used to solve a wide variety of time problems involving

linear time series estimators for stationary time series. Our symbolic

approach is used to derive the theoretical bias to O(1/n) of the Burg

estimator for the AR(2) model.

2. ASYMPTOTIC EXPECTATIONS AND COVARIANCES

Consider n consecutive observations from a stationary time series,

zt, t = 1, ..., n, with mean µ = E (zt) and autocovariance function

γk = Cov (zt, zt−k). If the mean is known, it may, without loss of generality

be taken to be zero. Then one of the unbiased estimators of autocovariance

3

γ(m − k) may be written as

Sm,k,i =

1
n + 1 − i

n

X
t=i

zt−mzt−k,

(1)

where m, k and i are non-negative integers with max(m, k) < i ≤ n. If the

mean is unknown, a biased estimator of γ(m − k) may be written as

S m,k,i =

1
n

n

X
t=i

(zt−m − zn)(zt−k − zn),

(2)

where zn is the sample mean.

Theorem 1. Let the time series zt be the two-sided moving average,

zt =

∞

X
j=−∞

αjet−j,

(3)

where the sequence {αj} is absolutely summable and the et are independent
N(0, σ2) random variables. Then for i ≤ j,

n→∞ n Cov (Sm,k,i, Sf,g,j) =
lim

∞

Th,

X
h=−∞

(4)

where

Th = γ(g−k+i−j+h)γ(f −m+i−j+h)+γ(f −k+i−j+h)γ(g−m+i−j+h).
Theorem 2. Let a time series {zt} satisfy the assumptions of Theorem

1. Then

n→∞ n E (S m,k,i − γ(m − k)) = −|i − 1|γ(m − k) −
lim

∞

X
h=−∞

γ(h)

(5)

and

where

n→∞ n Cov (S m,k,i, S f,g,j) =
lim

∞

Th,

X
h=−∞

(6)

Th = γ(g−k+i−j+h)γ(f −m+i−j+h)+γ(f −k+i−j+h)γ(g−m+i−j+h).

4

These two theorems may be considered as the extensions of Theorem

6.2.1 and Theorem 6.2.2 of Fuller (1996). Letting p = m − k and q = f − g,

the left side of (4) or (6) can be simpliﬁed,

∞

∞

Th =

X
h=−∞

X
h=−∞

γ(h)γ(h − p + q) + γ(h + q)γ(h − p).

(7)

There is a wide variety of estimators which can be written as a function of

the autocovariance estimators, Sm,k,i or S m,k,i, such as, autocorrelation

estimator, least squares estimator, Yule-Walker estimator, Burg estimator,

etc. The asymptotic bias and variance may be obtained by the Taylor

expansion. Unfortunately, in the most cases, those expansions include a

large number of expectations and covariances of the autocovariance

estimators. It is too intractable manually. Theorems 1 and 2 provide the

basis for a general approach to the symbolic computation of the asymptotic

bias and variance to order O(1/n) for those estimators. The deﬁnition of

(1) or (2) allows an index set {m, k, i} to represent an estimator so that

Theorem 1 or 2 can be easily implemented symbolically.

3. BIAS OF BURG ESTIMATORS IN AR(2)

The stationary second-order autoregressive model may be written as

zt = φ1zt−1 + φ2zt−2 + at, where at are normal and independently
distributed with mean zero and variance σ2 and parameters φ1 and φ2 are

in the admissible region, |φ2| < 1, φ1 + φ2 < 1 and φ2 − φ1 < 1. The Burg

estimate for φ2 may be obtained directly from Percival and Walden (1993,

eqn. 416d) and then the estimate for φ1 may be obtained using the

5

Durbin-Levinson algorithm. After simpliﬁcation, these estimates may be

written as

where

C =

F =

1
n − 2
1
n − 1

ˆφ2 = 1 −

CD2 − 2ED2
CD2 + 8F 2G − 4F HD

, ˆφ1 =

2F
D

(1 − ˆφ2)

(8)

(z2

t + z2

t−2), D =

n

X
t=3
n

(ztzt−1), G =

X
t=2

1
n − 1
n

1
n − 2

X
t=3

n

X
t=2

(z2

t + z2

t−1), E =

1
n − 2

n

X
t=3

(z2

t z2

t−2),

z2
t−1, H =

1
n − 2

n

X
t=3

(ztzt−1 + zt−2zt−1).

Using a Taylor series expansion of ˆφ1 and ˆφ2 about µA = E (A), where

A = C, D, E, F, G and H, the bias to order O(1/n) may be expressed in

terms of asymptotic expectations of products and cross products involving

C, D, E, F, G and H. There are six squared terms and ﬁfteen cross product

terms involved in each expansion, that is, it is required to compute and

simplify for each of these twenty one expansions involving C, D, E, F, G and

H. These terms may all be written in terms of the unbiased estimate of the

autocovariance, Sm,k,i. The required asymptotic expectations of each term

in the expansions are obtained by Theorem 1, that is,

n→∞ n Cov (Sm,k,i, Sf,g,j) =
lim

∞

Th,

X
h=−∞

(9)

where Th = γ(h)γ(h − p + q) + γ(h + q)γ(h − p), p = m − k, q = f − g and

γ(h) =

2 ζ2

1+h − ζ1
1+h + ζ1
ζ2
2 − 1(cid:17) (ζ1 − ζ2) (ζ1 ζ2 − 1) (cid:16)ζ2

(cid:16)ζ2

1+h

2 − 1(cid:17)

2 − 1(cid:17)

(cid:16)ζ1

,

(10)

where h ≥ 0, ζ1 and ζ2 are the roots, assumed distinct, of the polynomial
ζ 2 − φ1ζ − φ2 = 0. The order n−1 coeﬃcient of the covariance expansion of

6

Sm,k,i and Sf,g,j given in eqn. (9) may be evaluated symbolically by deﬁning

an operator of Sm,k,i and Sf,g,j, LCOV [{m, k, i}{f, g, j}]. To illustrate this

symbolic method consider the evaluation of limn→∞ n Cov (2C, H) which is
one of the twenty one order n−1 expansion coeﬃcients involving

C, D, E, F, G and H mentioned above. It may be obtained by

n→∞ n Cov (2C, H) = 2{ LCOV [({0, 0, 3} + {2, 2, 3})({0, 1, 3} + {2, 1, 3})]}
lim
= 2{ LCOV [{0, 0, 3}{0, 1, 3}] + LCOV [{0, 0, 3}{2, 1, 3}]

+ LCOV [{2, 2, 3}{0, 1, 3}] + LCOV [{2, 2, 3}{2, 1, 3}]},

since C = S0,0,3 + S2,2,3, H = S0,1,3 + S2,1,3, and LCOV [·] follows the

linearity and the distributive law.

After algebraic simpliﬁcation the biases to order O(1/n) are found to be

E ( ˆφ1 − φ1)

.
= −(ζ1 + ζ2)/n and E ( ˆφ2 − φ2)

.
= (3ζ1ζ2 − 1)/n. More simply,

in terms of the original parameters we have

and

E ( ˆφ1 − φ1)

.
= −φ1/n

E ( ˆφ2 − φ2)

.
= −(1 + 3φ2)/n.

(11)

(12)

We veriﬁed, using the same approach, that eqns. (11) and (12) also hold for

the case of equal roots of the polynomial ζ 2 − φ1ζ − φ2 = 0.

For the stationary second-order autoregressive model with an unknown

mean, the Burg estimators can be written as the same ratio function of the

biased estimators of the autocovariances, S m,k,i, as was given in eqn. (8).

The symbolic approach is similar to the known mean case, but includes one

more inner product associated with the biases of those autocovariance

7

estimators, S m,k,i. The required asymptotic biases and covariances of S m,k,i

are obtained by Theorem 2. The biases to order O(1/n) are found to be
E ( ˆφ1 − φ1)

.
= ((ζ1ζ2 − ζ1 − ζ2) − 1)/n and E ( ˆφ2 − φ2)

.
= (4ζ1ζ2 − 2)/n.

That is

and

E ( ˆφ1 − φ1)

.
= −(φ2 + φ1 + 1)/n

E ( ˆφ2 − φ2)

.
= −(2 + 4φ2)/n.

(13)

(14)

Once an estimator of a stationary time series is written as a well deﬁned

function composed of Sm,k,i or S m,k,i, by expanding it by a Taylor series,
the estimate bias and variance to order n−1 may be obtained by Theorem 1

or 2 with symbolic computation. This approach can be applied in the bias

derivation of the Burg estimator, ˆρ, in the ﬁrst order autoregressive model,
AR(1). We have obtained that its bias to order n−1 is −2ρ/n in a zero

mean case and −(1 + 3ρ)/n in an unknown mean case. Therefore, for both
of AR(1) and AR(2) cases, the biases to order n−1 of the Burg estimators

are the same as the least squares estimation for a known mean case as well

as for an unknown mean case. These results are consistent with the

simulation study reported by Tjøstheim and Paulsen (1983).

4. CONCLUDING REMARKS

In addition to deriving the bias for the Burg estimator, we used our

computer algebra method to verify the bias results reported by Tjøstheim

& Paulsen (Correction, 1984) and we also carried out simulation

experiments which provided an additional check on our results (Zhang,

8

2002). Mathematica (Wolfram, 1999) notebooks with the complete details

of our derivations are available on request from the authors.

Since many quadratic statistics in a stationary time series can be

expressed in terms of Sm,k,i or S m,k,i, our computer algebra approach can be

applied to derive their laborious moment expansions to order O(1/n). As

examples, using our method, we can easily obtain the results by Bartlett

(1946), Kendall (1954), Marriott and Pope (1954), White (1961) and

Tjøstheim and Paulsen (1983).

REFERENCES

ANDREWS, D. F. and STAFFORD, J. E. (2000) Symbolic Computation

for Statistical Inference, Oxford University Press.

ANDREWS, D. F. and STAFFORD, J. E. (1993) Tools for the Symbolic

Computation of Asymptotic Expansions. J. R. Statist. Soc. B 55,

No. 3, 613–627.

BARTLETT, M. S. (1946) On the Theoretical Speciﬁcation and Sampling

Properties of Autocorrelated Time-Series. J. R. Statist. Soc. Suppl.

8, 27–41.

COOK, P. and BROEMELING, L.D. (1995) Bayesian Statistics Using

Mathematica. The American Statistician 49 70–76

CURRIE, I.D. (1995) Maximum Likelihood Estimation and Mathematica.

Applied Statistics 44, 379–394.

FULLER, W. A. (1996) Introduction to Statistical Time Series, 2nd Ed.,

New York: Wiley.

9

KENDALL, M. G. (1954) Notes on the bias in the estimation of

autocorrelation. Biometrika 41, 403–404.

MARRIOTT, E. H. C. and POPE, J. A. (1954) Bias in the estimation of

autocorrelation. Biometrika 41, 390-402.

LYSNE, D. and TJØSTHEIM, D. (1987) Loss of Spectral Peaks in

Autoregressive Spectral Estimation. Biometrika 74, 200–206.

PERCIVAL, D. B. and WALDEN A. T. (1993) Spectral Analysis For

Physical Applications, Cambridge: Cambridge University Press.

ROSE, C. and SMITH, M.D. (2002) Mathematical Statistics with

Mathematica, New York: Springer-Verlag.

ROSE, C. and SMITH, M.D. (2000) Symbolic maximum likelihood

estimation with Mathematica. The Statistician, 49, 229–240.

SMITH, B. and FIELD, C. (2001) Symbolic Cumulant Calculations for

Frequency Domain Time Series. Statistics and Computing 11 75–82.

TJØSTHEIM, D. and PAULSEN, J. (1983) Bias of some commonly-used

time series estimates. Biometrika 70, 389–399. Correction Biometrika

71, p. 656.

WOLFRAM, S. (1999) The Mathematica Book , 4th Ed. Cambridge:

Cambridge University Press.

WHITE, J. S. (1961) Asymptotic Expansions for the Mean and Variance of

the Serial Correlation Coeﬃcient. Biometrika 48, , p. 85–94.

ZHANG, Y. (2002) Topics in Autoregression, Ph.D. Thesis, University of

Western Ontario.

10

