BIPS: Bi-modal Indoor Panorama Synthesis via Residual Depth-aided
Adversarial Learning

Changgyoon Oh*, Wonjune Cho∗, Daehee Park, Yujeong Chae, Lin Wang and Kuk-Jin Yoon
Visual Intelligence Lab., KAIST, Korea
{changgyoon,wonjune,bag2824,yujeong,wanglin,kjyoon}@kaist.ac.kr

1
2
0
2
c
e
D
2
1

]

V
C
.
s
c
[

1
v
9
7
1
6
0
.
2
1
1
2
:
v
i
X
r
a

Abstract

Providing omnidirectional depth along with RGB infor-
mation is important for numerous applications, e.g., VR/AR.
However, as omnidirectional RGB-D data is not always
available, synthesizing RGB-D panorama data from lim-
ited information of a scene can be useful. Therefore, some
prior works tried to synthesize RGB panorama images from
perspective RGB images; however, they suffer from limited
image quality and can not be directly extended for RGB-D
panorama synthesis. In this paper, we study a new problem:
RGB-D panorama synthesis under the arbitrary conﬁgura-
tions of cameras and depth sensors. Accordingly, we pro-
pose a novel bi-modal (RGB-D) panorama synthesis (BIPS)
framework. Especially, we focus on indoor environments
where the RGB-D panorama can provide a complete 3D
model for many applications. We design a generator that
fuses the bi-modal information and train it with residual-
aided adversarial learning (RDAL). RDAL allows to syn-
thesize realistic indoor layout structures and interiors by
jointly inferring RGB panorama, layout depth, and residual
depth. In addition, as there is no tailored evaluation metric
for RGB-D panorama synthesis, we propose a novel met-
ric to effectively evaluate its perceptual quality. Extensive
experiments show that our method synthesizes high-quality
indoor RGB-D panoramas and provides realistic 3D indoor
models than prior methods. Code will be released upon ac-
ceptance.

1. Introduction

Providing omnidirectional depth along with RGB in-
formation is important for numerous applications, e.g.,
VR/AR. However, as the omnidirectional RGB-D data is
not always available, synthesizing RGB-D panorama data
from the limited information of the scene can be useful.
Even though prior works have tried to synthesize RGB
panorama images from perspective RGB images [21, 61],
these methods show limited performance on synthesizing

*The ﬁrst two authors contributed equally. In alphabetical order.

Figure 1. Overall scheme of RGB-D panorama synthesis. Our
method takes RGB-D input from cameras and depth sensors in
arbitrary conﬁgurations and synthesizes an RGB-D panorama.

panoramas from small partial views and can not be directly
extended for RGB-D panorama synthesis.

By contrast, jointly learning to synthesize depth data
along with the RGB images allows to synthesize RGB-D
panorama with two distinct advantages: (1) RGB images
and depth data share the semantic correspondence that can
improve the quality of the output RGB-D panorama.
(2)
Synthesized depth panorama provides omnidirectional 3D
information, which can be potentially applied to plentiful
applications. Therefore, it is promising to synthesize RGB-
D panorama from the cameras and depth sensors, such that
we can synthesizing realistic 3D indoor models.

In this paper, we consider a novel problem: RGB-D
panorama synthesis from limited input information about
a scene. To maximize the usability, we consider the arbi-
trary conﬁgurations of cameras and depth sensors. To this
end, we design the arbitrary sensor conﬁgurations by ran-
domly sampling the number of sensors, their intrinsic pa-
rameters, and extrinsic parameters, assuming that the sen-
sors are calibrated such that we can align the depth data
with the RGB image. This enables to represent most of the
possible combinations of cameras and depth sensors. Ac-
cordingly, we propose a novel bi-modal panorama synthesis
(BIPS) framework to synthesize RGB-D indoor panoramas
from the camera and depth sensors in arbitrary conﬁgura-
tions via adversarial learning (See Fig. 2). Especially, we
focus on the indoor environments as the RGB-D panorama

1

Perspective RGBD inputArbitrary Sensor Config.(e.g., Camera and LIDAR)SynthesizedRGBD panoramaInteractiveVirtual SpacePersepective RGB-D inputArbitrary Sensor Config.(e.g., Camera and LiDAR)3D Indoor ModelBIPG frameworkSynthesized RGB-D panoramaArbitrary RGBD InputArbitrary Sensor Config.(e.g., Camera,  LiDAR)Virtual SpaceSynthesized RGBD Panorama OutputPersepective RGBD InputArbitrary Sensor Config.(e.g., Camera and LiDAR)Interactive Virtual SpaceRGBD PanoramaArbitrary RGBD InputArbitrary Sensor Config.(e.g., Camera and LiDAR)Virtual SpaceRGBD PanoramaArbitrary RGBD InputArbitrary Sensor Config.(e.g., Camera and LiDAR)Virtual SpaceRGBD Panorama OutputArbitrary RGBD InputArbitrary Sensor Config.(e.g., Camera and LiDAR)Virtual SpaceRGBD Panorama OutputVirtual SpaceVirtual SpaceVirtual Space3D ResultRGBD Panorama OutputPersepective RGBD inputArbitrary Sensor Config.(e.g., Camera and LiDAR)Interactive Virtual SpaceSynthesized RGBD panoramaPersepective RGBD inputArbitrary Sensor Config.(e.g., Camera and LiDAR)Interactive Virtual SpaceSynthesized RGBD panorama 
 
 
 
 
 
can provide the complete 3D model for many applications.
We thus design a generator that fuses the bi-modal (RGB
and depth) features. Through the generator, multiple latent
features from one branch can help the other by providing
the relevant information of different modality.

For synthesizing the depth of indoor scenes, we rely on
the fact that the overall layout usually made of ﬂat surfaces,
while interior components have various structures. Thus,
we propose to separate the depth of a scene I d into two
components: layout depth I d,lay and residual depth I d,res.
Here, I d,lay corresponds to the depth of planar surfaces,
and I d,res corresponds to the depth of other objects, e.g.,
furniture. With this relation, we propose a joint learning
scheme called Residual Depth-aided Adversarial Learn-
ing (RDAL). RDAL jointly trains RGB panorama, layout
depth and residual depth to synthesize more realistic RGB-
D panoramas and 3D indoor models (Sec. 3.2.1).

Previously, some metrics [23, 57] have been proposed
to evaluate the outputs of generative models using latent
feature distribution of a pre-trained classiﬁcation network
[63]. However, the input modality of utilizing off-the-shelf
network is only limited to perspective RGB images. There-
fore, a new tailored evaluation metric for RGB-D panora-
mas is needed. For this reason, we propose a novel met-
ric, called Fr´echet Auto-Encoder Distance (FAED), to eval-
uate the perceptual quality for RGB-D panorama synthesis
(Sec. 3.3). FAED adopts an auto-encoder to reconstruct the
inputs from latent features with unlabeled dataset. Then,
the latent feature distribution of the trained auto-encoder is
used to calculate the Fr´echet distance between the synthe-
sized and real RGB-D data.

Extensive experimental results demonstrate that our
RGB-D panorama synthesis method signiﬁcantly outper-
forms the extensions of the prior image inpainting [47, 62,
89], image outpainting [33, 61], and image-guided depth
synthesis methods [12, 25, 38, 52] modiﬁed to synthesize
RGB-D panorama from partial arbitrary RGB-D inputs.
Moreover, we show the validity of the proposed FAED for
evaluating the quality of synthesized RGB-D panorama by
showing how well it captures the disturbance level [23].

In summary, our main contributions are three-fold: (I)
We introduce a new problem of generating RGB-D panora-
mas from partial and arbitrary RGB-D inputs. (II) We pro-
pose a BIPS framework that allows to synthesize RGB-
D panoramas via residual depth-aided adversarial learning.
(III) We introduce a novel evaluation metric, FAED, for
RGB-D panorama synthesis and demonstrate its validity.

2. Related Works
Image Inpainting Conventional approaches explore diffu-
sion or patch matching [5, 6, 8, 9, 14, 15, 17]. However,
they require visible regions sufﬁciently enough to inpaint
the missing regions, thus limiting their ability to synthe-

size novel textures or structures. The learning-based meth-
ods often use generative adversarial networks (GANs) to
synthesize texture or structures [27, 39, 80, 90], optimized
by the minimax loss [28]. Some works explored differ-
ent convolution layers, e.g., partial convolution [41] and
gated convolution [51, 81], to better handle invalid pixels
in the input data to the convolution kernel. Moreover, at-
tention mechanism [66, 67] has also been applied to bet-
ter capture the contextual information and handle missing
contents [40, 42, 71, 76, 80]. Recently, research has been
made to synthesize high-resolution outputs [53, 60, 73] or
semantically diverse outputs [43, 88]. Although endeavours
have been made to tackle this large completion problem
[47, 62, 89], they often fail to synthesize visually pleasing
panoramas due to only using perspective RGB inputs.
Image Outpainting Conventional methods extend an in-
put image to a larger seamless one; however, they require
manual guidance [4, 7, 87] or image sets of the same scene
category [30, 59, 70]. By contrast, learning-based methods
synthesize large images with novel textures that do not ex-
ist in the input perspective image [19, 20, 31, 32, 35, 48, 56,
75, 83]. Some approaches focus on driving scenes [74, 85]
or synthesize panorama-like landscapes with iterative ex-
tension or multiple perspective images [21, 33, 61, 79]. Al-
though performance has been greatly improved so far, the
existing methods are still afﬂicted by the limited quality
from the perspective images.
Image-guided Depth Synthesis One line of research at-
tempts to fuse the bi-modal information, i.e., the RGB im-
age and sparse depth. Some methods, e.g. [45], fuse the
sparse depth and RGB image via early fusion while oth-
ers [18, 26, 29, 37, 44, 64] utilize a late fusion scheme,
or jointly utilize both the early and late fusion [36, 65, 68].
Another line of research focuses on utilizing afﬁnity or ge-
ometric information of the scene via surface normal, occlu-
sion boundaries, and the geometric convolutional layer [11,
12, 25, 34, 52, 54, 77, 86]. However, these works only gen-
erate dense depth maps that has the same FoV as the input
perspective RGB images.
Evaluation of Generative Models Image quality assess-
ment can be classiﬁed into three groups:
full-reference
(FR), reduced-reference (RR), and no-reference (NR).
There exist many conventional FR metrics, e.g., PSNR,
MSE, and SSIM, and deep learning (DL)-based FR met-
rics, e.g., LPIPS [84]. These metrics typically calculate ei-
ther pixel-wise, or patch-wise similarity to the ground truth
images. By contrast, NR methods, e.g., BRISQUE [49] and
NIQE [50] assess image quality without any reference im-
age. Among the DL-based NR metrics, Inception Score
[57] and Fr´echet Inception Distance (FID) [23] are
(IS)
IS and FID scores are cal-
two popular approaches [2].
culated based on pretrained classiﬁcation models, e.g., In-
ception model [63], aiming to capture the high-level fea-

2

Figure 2. Overall structure of our bi-modal indoor panorama synthesis (BIPS) framework. Our framework takes RGB-D input provided by
arbitrary sensor conﬁgurations, integrates the bi-modal input data with BFF branch in generator network, and jointly trains to synthesize
layout depth and residual depth. Then, perceptual quality of the synthesized RGB-D panorama is measured by our proposed FAED metric.

tures. Unfortunately, these metrics are less applicable for
RGB-D panorama evaluation because (1) they are trained
only with perspective RGB images, and (2) there are no
labeled panorama images to train them. Therefore, they
are highly sensitive to the distortion of panoramic images,
making them hard to capture perceptual quality properly
on panoramic images. Furthermore, naively using them on
RGB-D information leads to imprecise measure of the se-
mantic correspondence between the two different modali-
ties. Therefore, we propose FAED, which aims to evalu-
ate RGB-D panorama quality between the RGB and depth
pairs. FAED can be adaptively applied to generative mod-
els on multi-modal domain, that lacks labeled dataset.

3. Proposed Methods
3.1. Problem Formulation

Previous works, e.g., [21, 61] generate an equirectangu-
lar projection (ERP) image (ERP rgb) from input perspec-
tive image(s) (I rgb
out can be
created via a function G, mapping I rgb
in into a ERP rgb [22],
which can be formulated as I rgb

in ). Then, an RGB panorama I rgb

out = ERP rgb = G(I rgb
in ).

However, as it is crucial to provide omnidirectional depth
information [1, 55] in many applications, many studies tried
to synthesize depth panoramas from input RGB panorama
images and partial depth measurements [24, 69]. One so-
lution to synthesize an RGB-D panorama would be to ﬁrst
synthesize RGB panorama from input perspective images,
and then utilizing the depth synthesis methods to generate
an omnidirectional depth map. However, such an approach
is cumbersome and less effective, as shown in the experi-
mental results (See Table 3). We solve this novel yet chal-
lenging problem by jointly utilizing the input RGB image

3

out , I d

in , I d

in ) and depth data (I d

(I rgb
in). Our goal is to directly gen-
erate the RGB panorama (ERP rgb) and depth panorama
(ERP d) simultaneously via a mapping function G, which
can be described as (I rgb
out) = (ERP rgb, ERP d) =
G(I rgb
in). G can be formulated by learning a single net-
work to synthesize ERP rgb and ERP d using I rgb
in and I d
in
obtained in arbitrary sensor conﬁgurations. As the informa-
tion in the left and right boundaries in ERP images should
be connected, our designed G uses circular padding [58] be-
fore each convolutional operation.

Consequently, we conﬁgure the parameters of cameras
and depth sensors, and randomly sample the parameters to
provide the input to the G during training. These parame-
ters can handle most of the possible sensor conﬁgurations.
Figure 3 shows the input masks, sampled from the sensor
conﬁgurations. To handle the cases where only cameras or
depth sensors are used, we choose whether to use cameras
only, depth sensors only, or both, randomly.

Parameters of RGB Cameras We denote the parameters
of RGB cameras, horizontal FoV as δH , vertical FoV as δV ,
pitch angle as ψ, and number of viewpoints as n. When n >
1, we arrange the viewpoints in a circle having the sampled
pitch angle from the equator and at the same intervals. We
do not consider roll and yaw, as they do not affect the results
(i.e., the output is equivariant to the horizontal shift of input)
thanks to using circular padding. Practically, we sample the
parameters from δH ∼ U[60◦, 90◦], δV ∼ U[60◦, 90◦], ψ ∼
U[−90◦, 90◦], and n ∼ U{1, 2, 3, 4}, where U [·] represents
uniform distribution.
Parameters of Depth Sensors I d
in can be obtained from
mechanical LiDARs or perspective depth sensors, thus we
should generate arbitrary depth input masks for both. For

OneInputGeneratorOutputInputInput MaskGoutrgbDDiscriminatorLadvLoss for G~LDLDZeroCameras andDepth SensorsMultiplication~SimulationLoss for DMeasure (test)GoutresGindGinrgbGoutlayGround TruthOutput DepthRGBD PanoramaInput MaskIrgbGoutresOutputGBMFDGround TruthCamera and Depth SensorsFAEDflatentReconstructionOneLadvLDLDZeroIdLayout DepthResidual DepthDiscriminator3D Restored ResultInputOneLadvLDLDZero~(Eq. 8, 9, 10)LD(Eq. 4)Output DepthLpixelInteractable Virtual Space(Eq. 5)(Eq. 6)Loss③Arbitrary Sensor SimulationRGBD Panorama SynthesisRGBD Panorama Quality Evaluation  with FAED① ② ③ ②①AencoderMultiplicationSimulation~LossMeasure (Test)MultiplicationSimulation~(I)(II)(III)Layout DepthResidual DepthArbitrary Sensor ConfigurationsÎresÎlayÎrgbGoutrgbGoutlayGinrgbGindRGBD Panorama Quality EvaluationLossMeasure (Test)MultiplicationSimulation~Measure (Test)AdecoderLossMultiplicationSimulation~Measure (Test)RGBD Panorama SynthesisRGBD Panorama Quality Evaluation  with FAED RGB-D PanoramaInput MaskGoutresOutputGBFFDGround TruthCamera and Depth SensorsFAEDflatentLayout DepthResidual DepthDiscriminatorInputLadv~(Eq. 4, 5, 6)(Eq. 2)Lpixel(Eq. 3)AencoderLossMeasure (Test)MultiplicationMask Sampling~(I)(II)(III)Layout DepthResidual DepthArbitrary Sensor ConfigurationsGoutrgbGoutlayGinrgbGindepthAdecoder(I) RGB-D Input Generation  (II) Bi-modal Panorama Synthesis (BIPS)(III) RGB-D Panorama Quality Evaluation  with FAEDReconstruction RGBD PanoramaInput MaskIrgbGoutresOutputGBFFDGround TruthCamera and Depth SensorsFAEDflatentIdLayout DepthResidual DepthDiscriminator3D Restored ResultInputLadv~(Eq. 8, 9, 10)LD(Eq. 4)LpixelInteractable Virtual Space(Eq. 5)(Eq. 6)AencoderLossMeasure (Test)MultiplicationSimulation~(I)(II)(III)Layout DepthResidual DepthArbitrary Sensor ConfigurationsÎresÎlayÎrgbGoutrgbGoutlayGinrgbGindepthAdecoder(I) Arbitrary Sensor Simulation      (II) RGBD Panorama Synthesis(III) RGBD Panorama Quality Evaluation  with FAEDReconstructionIrgboutId,layoutId,resoutIrgbinIdinIdgtIrgbgtId,laygtId,resgtIrgbgtOutput DepthIdout3D Indoor ModelVirtual SpaceFigure 3. Sampled input masks. The upper row shows the visi-
ble regions of cameras and perspective dense depth sensors with
parameters (δH , δV , ψ, n), and the lower row shows the visible re-
gions of mechanical LiDARs, with parameters (δL, δU , ψ, ω, η).

the LiDARs, we denote the parameters as lower FoV δL,
upper FoV δU , pitch angle ψ, yaw angle ω, and the number
of channels η. The yaw angle is needed here to consider
the relative yaw motion to the camera arrangement. For the
perspective depth sensors providing dense depth, they have
many similar parameters and viewpoints with those of the
RGB-D cameras. Therefore, we use the same sampled pa-
rameters with the cameras (i.e., (δH , δV , ψ, n)). In practice,
we ﬁrst sample the parameters from ψ ∼ U[−90◦, 90◦],
ω ∼ U[0, 360◦], and η ∼ U{2, 4, 8, 16}. Then, we sam-
ple δL and δU from U{η, 2η, 3η}. Finally, our problem is
formulated as:
(I rgb

out , I d

out) = (ERP rgb, ERP d) =
G(I rgb
in (δH , δV , ψ, n), I d

in(δL,δU , ψ, ω, η, δH , δV , n))
(1)

gt

in and depth I d

gt into layout depth I d,lay

3.2. RGB-D Panorama Synthesis Framework
Overview An overview of the proposed BIPS framework
is depicted in Fig. 2. BIPS consists of a generator G
(Sec.3.2.1) and a discriminator D (Sec. 3.2.2). G takes the
perspective RGB image I rgb
in as inputs. We
notice that the quality of the RGB-D panorama depends on
both the overall (mostly rectangular) layout and how the fur-
niture are arranged in the indoor scene. Inspired by [82],
we separate the depth data I d
, and
gt
residual depth (the interior components) I d,res
gt which is de-
gt - I d,lay
ﬁned as (I d
). The generator G outputs the RGB
panorama image I rgb
and residual
depth I d,res
simultaneously. As these are jointly trained
with adversarial loss, we call this learning scheme as Resid-
ual Depth-aided Adversarial Learning (RDAL).
3.2.1 Generator
Input Branch Gin consists of two encoding branches, Grgb
in
and Gdepth
in and I d
in, respectively. These
branches independently process Grgb
in and Gdepth
with six
conv layers before fusing them. As the inputs have a reso-
lution of 512 ×1024, the ﬁlter size of the ﬁrst conv layer is
set as 7 and then reduced to 3 and 4 gradually.
Bi-modal Feature Fusion (BFF) Branch BFF branch
in ) and Gdepth
GBF F takes Grgb
in) as inputs, as

out , the layout depth I d,lay

which takes I rgb

in (I rgb

(I d

out

out

in

in

in

Figure 4. The proposed generator (G). It consists of two input
branches, a BFF branch, and three output branches. A larger ver-
sion of the image can be found in the suppl. material.

in and depth I d

shown in Fig. 4. Although I rgb
in are in two
different modalities, we assume that the cameras and depth
sensors are well calibrated and synchronized. Then, to uti-
lize this highly correlated bi-modal information in its two
branches, GBF F consists of two-stream encoder-decoder
networks fusing the bi-modal features. These two encoder-
decoder networks have an identical structure (see Fig. 4).

Moreover, the bi-modal features are fused in between
the layers of GBF F . In particular, the features from both
branches are concatenated and fed back to each other. Over-
all, the fusion is done after the features pass two ‘Down-
Blocks’ and before passing two ‘UpBlocks’. In addition,
multi-scale residual connections are used to vitalize transfer
of information between the layers and branches. As multi-
ple latent features from one branch help the other by sharing
the information apart in both ways, GBF F can generate fea-
tures by fully exploiting the information of the 3D scene.
The Output Branch Realistic indoor space comes from
precise layout structure and high perceptual interior. There-
fore, to enable RDAL to jointly train the layout and resid-
ual depth of the indoor scene, we design Gout to have
three decoding branches. Each of them generates RGB
out , layout depth panorama I d,lay
panorama I rgb
, and resid-
out
ual depth panorama I d,res
respectively, as shown in Fig. 4.
Intuitively, I d,lay
out determines the layout structure and I d,res
determines the interior objects. Then, element-wise addi-
tion of I d,lay
gives the output total depth map
panorama.

and I d,res

out

out

out

out

3.2.2 Discriminator
We use the multi-scale discriminator D from [72], but mod-
ify it to have ﬁve input and output channels (three for I rgb,
one for I d,lay, one for I d,res). The detailed discriminator
structure can be found in the suppl. material.

3.2.3 Loss Function
For training G, we use weighted sum of pixel-wise L1 loss
and adversarial loss. The pixel-wise L1 loss between the
GT and the output panorama, denoted as Lpixel, consists of

4

(A)(B)(78, 66, 58, 1)(74, 69, 34, 2)(84, 69, 18, 4)(16, 16, -1, 193, 16)(24, 8, -20, 286, 8)(48, 48, -42, 136, 16)CDownBlock, 128DownBlock, 128DownBlock, 256DownBlock, 256(x6) conv 3x3UpBlock, 256UpBlock, 128UpBlock, 128UpBlock, 64Gbi-fuseUpsample (x2)conv 7x7, 32conv 7x7, 32conv 3x3, 32conv 3x3, 32conv 4x4, 16conv 4x4, 16(x3) conv 3x3(x3) conv 3x3(x4) conv 3x3Upsample (x2)(x4) conv 3x3conv 1x1, 3conv 1x1, 1GoutrgbGinrgbGoutdepth_resconv 4x4, 2C(x3) conv 3x3, 2Cconv 3x3, C/2(x3) conv 3x3, C/2Upsample (x2)DownBlock, CUpBlock, CDownBlock, 128DownBlock, 128DownBlock, 256DownBlock, 256(x6) conv 3x3UpBlock, 256UpBlock, 128UpBlock, 128UpBlock, 64CCUpsample (x2)(x4) conv 3x3conv 1x1, 1Goutdepth_iniCStride 1Stride 2   Element-wise AdditionCConcatenation GBFFGinrgbconv 4x4, 2C(x3) conv 3x3, 2Cconv 3x3, C/2(x3) conv 3x3, C/2Upsample (x2)DownBlock, CUpBlock, CConv (stride 1)Stride 2   Element-wise AdditionCSkip ConnectionConv (stride 2)DownBlockUpBlockUpsampleIinrgbIoutd,layGindepthGoutlayGoutresIindIoutd,resIoutrgbGoutrgbGBFFGinrgbConv (stride 1)CCCCSkip ConnectionConv (stride 2)DownBlockUpBlockUpsampleConcatenation CIinrgbIoutd,layGindepthGoutlayGoutresIindIoutd,resIoutrgbGoutrgbCCCCCUpsampleConcatenation & Feedback CAencoder. However, as we generate the upright ERP image,
it is expected to have a distance metric that is invariant to the
longitudinal shift. This is because an upright ERP panorama
represents the same scene when it’s cyclically shifted in the
longitudinal direction. Therefore, to make the resulting dis-
tance metric invariant to the longitudinal shift, we take the
mean for the longitudinal direction of flatent as:

f (cid:48)
latent(c, h) =

1
W

(cid:88)

w

flatent(c, h, w).

(4)

Latitudinal Equivariance As the ERP has varying sam-
pling rates depending on the latitude φ, we apply differ-
ent weights on f (cid:48)
latent based on the latitude. Speciﬁcally,
we multiply cos(φ) to feature at the latitude φ, because in
ERP, each pixel occupies cos(φ) area in the spherical sur-
face, compared with the pixels in the equator. Formally, the
resulting feature f (cid:48)(cid:48)
latent is expressed as:

latent(c, h) = cos φ · f (cid:48)
f (cid:48)(cid:48)

latent(c, h).

(5)

Fr´echet Distance We treat the resulting f (cid:48)(cid:48)
latent as a vector
and assume that it has a multi-dimensional Gaussian dis-
tribution. Then, we get the distribution of ground truths
N (m, C) and that of generated samples N ( ˆm, ˆC), and cal-
culate the Fr´echet distance d between them as given by [16]:

d2(N (m,C), N ( ˆm, ˆC))
= ||m − ˆm||2

2 + T r(C + ˆC − 2(C ˆC)1/2).

(6)

We use d2 as a perceptual distance metric where m and C
denote mean and covariance, respectively.

4. Experimental Results
Synthetic Dataset Structured3D dataset [91] provides var-
ious textures of indoor scenes with a 512 × 1024 resolu-
tion. We split the dataset into train, validation, and test set
where the numbers of data are 17468, 2183, and 2184, re-
spectively. In addition, with the corner locations provided
in the dataset, we manually generated layout depth maps of
each 3D scene. The residual depth maps are obtained by
subtracting the layout depth from the GT depth map.
Real Dataset We used a combination of two datasets: Mat-
terport3D [10], and 2D-3D-S dataset [3]. Both datasets pro-
vide real-world indoor RGB-D panorama. Since this dataset
does not provide sufﬁcient number of annotated layout, it
can’t be used for training our framework and only used for
test purpose. We excluded data with too many of invalid
pixels from test dataset, then its number of data is 603.
Implementation Details For the details about our imple-
mentation, please refer to the supplementary material.
4.1. Veriﬁcation of FAED

To show the effectiveness of FAED on measuring the
perceptual quality of RGB-D panorama, we corrupt the
Structured3D dataset [91] in two ways: corrupting RGB im-
ages only and corrupting depth maps only. Following [23],

Figure 5. The proposed FAED metric for RGB-D panorama qual-
ity evaluation. It measures the distance of the distributions of la-
tent features extracted from the pre-trained auto-encoder network
on RGB-D panorama.

three terms as the G has three outputs (RGB, layout depth,
residual depth panorama):

pixel = Lrgb
Ltotal

pixel + Ld,lay

pixel + Ld,res
pixel.

(2)

E [(D(I total

For the adversarial loss Ladv, we used LSGAN loss [46]:
Ladv = 1
out ) − 1)2], where I total
is concatena-
out
2
tion of generator outputs I rgb
and I d,res
out , I d,lay
, and D is
a discriminator trained to output one for GT and zero for
I total
out with MSE loss. By decomposing the total depth loss
into Ld,lay and Ld,res, our RDAL scheme allows the gener-
ator G to synthesize RGB-D panorama that generates highly
plausible interior. Finally, the total loss for generator is:

out

out

LG = λLtotal

pixel + LGAN

(3)

where λ is a weighting factor. The generator G is trained
by minimizing the total loss LG. Detailed loss terms can be
found in the suppl. material.

3.3. Fr´echet Auto-Encoder Distance (FAED)
3.3.1 Auto-Encoder Network

Similar to the high-level features in a CNN trained with
latent features flatent in a
large-scale semantic labels,
trained auto-encoder also contain high-level information, as
it is forced to reconstruct the input from the latent features.
Therefore, we propose to train an auto-encoder, which can
be done without any labels in the dataset, and use the latent
features in the auto-encoder to extract perceptually mean-
In this way, performance evaluation
ingful information.
can be performed for any data that lacks a labeled dataset.
The auto-encoder A consists of an encoder and decoder:
Aencoder and Adecoder, as shown in Fig. 5. The detailed
structure of A is given in the suppl. material.

3.3.2 Calculation of FAED for RGB-D Panorama

We denote flatent at c-th channel, h-th row, and w-th col-
umn as flatent(c, h, w). Note that as we use ERP, the h and
w has one-to-one relation to latitude and longitude.
Longitudinal Invariance To evaluate the performance
of G, we extract flatent from generated samples using

5

conv 9x9, 32res 9x9, 32conv 4x4, 64res 7x7, 64conv  4x4, 128res 5x5, 128conv 4x4, 128res 3x3, 128conv 4x4, 128conv 3x3, 128res 3x3, 128Stride 1Stride 2conv 9x9, 32res 9x9, 32conv 4x4, 64res 7x7, 64conv  4x4, 128res 5x5, 128conv 4x4, 128res 3x3, 128conv 4x4, 128conv 4x4, 128res 3x3, 128conv 4x4, 128res 3x3, 128conv 3x3, 256conv 3x3, 128conv 3x3, 128res 3x3, 128conv 3x3, 128res 5x5, 128conv 3x3, 128res 5x5, 128conv 3x3, 32res 9x9, 32conv 3x3, 32res 9x9, 32conv 9x9, 3conv 9x9, 1Upsample (x2)Upsample (x2)Upsample (x2)Upsample (x2)conv 3x3, 64res 7x7, 64conv 3x3, 64res 7x7, 64Upsample (x2)Upsample (x2)Upsample (x2)Upsample (x2)Upsample (x2)flatentAencoderAdecoderGTAencoderAdecoderPredAencoderAdecoderFAEDf’’latentflatentflatentGaussian distrib.Gaussian distrib.⇒f’’latent(Eq. 8, 9)(Eq. 8, 9)(Eq. 10)Frechet DistanceFAED(Eq. 6)AencoderAdecoderRGBD GTGaussian distrib.flatentGTf’’latentGT(Eq. 4, 5)Gaussian distributionf’’latentPred.RGBD Pred.(Eq. 4, 5)FAEDFréchet DistanceflatentPred.(Eq. 10)AencoderAdecoderRGBD GTflatentGTf’’latentGT(Eq. 8, 9)Gaussian distributionf’’latentPred.RGBD Pred.flatentPred.(Eq. 8, 9)FAEDAencoderAdecoderAencoderAdecoderTable 1. Quantitative results of RGB panorama synthesis on Structured3D dataset. As [61] uses 4 identical perspective RGB masks on
horizontal central line, we report our results in same setting. In other cases, we follow designed arbitrary conﬁguration of RGB sensor
that uses 1˜4 number of inputs. Zero number of depth input means that depth map is not used for RGB panorama synthesis. For FAED
calculation, GT depth is used along with synthesized RGB panorama. Bold numbers indicate the best results.

Category

Method

Input no. (n)
RGB

Depth

Inpainting

Outpainting

Panorama syn.

BRGM [47]
CoModGAN [89]
LaMa [62]
Boundless [33]
Ours
Sumantri et al. [61]
Ours

1/2/3/4

4

0

0

PSNR(↑)
14.00
14.35
13.74
13.74
16.21
18.49
17.29

RGB metric
SSIM(↑)
0.5310
0.5837
0.5207
0.5663
0.6161
0.6680
0.6510

LPIPS(↓)
0.6192
0.4768
0.5658
0.6144
0.4549
0.4190
0.3975

Layout metric
2D Corner error(↓)
72.52
62.45
51.12
74.47
39.63
50.76
34.68

Proposed metric
FAED(↓)
442.3
208.2
379.2
429.4
162.3
443.4
103.1

Figure 6. Veriﬁcation of FAED in Structured3D dataset. It can be seen that FAED correlates well perceptual evaluation of human, as FAED
increases as the data becomes more corrupted. For more detailed results, please refer to the suppl. material.

Figure 7. Qualitative comparison to Sumantri et. al. [61]. While
the result from [61] is blurry, our result is sharp and realistic.

we corrupt the dataset by applying various types of noise:
Gaussian blur, Gaussian noise, uniform patches, swirl, and
salt and pepper noise. Here, we only show the plots for
Gaussian blur in Fig. 6 due to the lack of space. Other re-
sults can be found in suppl. material. Note that the eval-
uation is done for RGB-D panorama, neither for RGB im-
age alone nor for depth map alone. As shown in Fig. 6,
the Fr´echet distance for both RGB and depth panorama in-
creases as the disturbance level (Gaussian blur) is increased.
We show that the same applies to the other four types of
noises in the supplementary material. This indicates the
perceptual quality of RGB-D panorama becomes poorer as
the FAED score increases.

4.2. RGB-D Panorama Synthesis
RGB Panorama Evaluation Table 1 shows the quantitative
comparison with the inpainting and outpainting methods on
the Structured3D dataset. We use PSNR, SSIM and LPIPS
to evaluate the quality of RGB panorama. We also mea-
sure 2D corner error, where the 2D GT corner points are
compared with the estimated 2D corner points using DuLa-
Net [78] on the synthesized RGB panorama. We also use
the proposed FAED to jointly evaluate the quality of RGB-
D information.

As shown in Table 1, our method outperforms the im-
age inpainting and outpainting methods: BRGM [47], Co-

6

ModGAN [89], LaMa [62] and Boundless [33], by a large
margin for all metrics. For instance, our method outper-
forms the best inpainting method, CoModGAN, by an 4.6%
decrease of LPIPS score, 36.5% drop of 2D corner error,
and 22% decline of FAED score. The effectiveness can
also be visually veriﬁed in Fig. 8(a). Our method pro-
duces clearer RGB panorama images compared with LaMa
producing blurry images. Although CoModGAN produces
clear RGB outputs, it doesn’t consider the indoor layout and
semantic information of the furniture, e.g. electric cooker is
combined with bookshelves, as shown in Fig. 8. Therefore,
its layout is semantically inconsistent with the input RGB
region and its FAED score is higher than ours.

We also compare with the panorama synthesis method,
Sumantri et al. [61]. Our method shows slightly lower
scores using the conventional metrics, PSNR and SSIM;
however, it shows the much better LPIPS score (0.3975 vs
0.4190), 2D corner error (34.68 vs 50.76) and FAED score
(103.1 vs 443.4), respectively. We argue that PSNR and
SSIM merely measure local photometric similarity, and thus
fail to well reﬂect the perceptual quality. This can be veri-
ﬁed from Fig. 7 where we visually compare with [61]. Our
method synthesizes better textures and shows much higher
visual quality. More results can be found in suppl. material.
Depth Panorama Evaluation We compare our method
with the image-guided depth synthesis methods on Struc-
tured3D dataset. To evaluate the quality, we use AbsREL
and RMSE, and the proposed FAED. We also use layout 2D
IoU, as was done in [13]. The details of the metrics and
results can be found in the supplementary material.

Table 2 shows the quantitative comparison with the

(ii)(i) Depth Full(iii) RGB Simple(iv) RGB Empty(v) RGB Different(ii) RGB Full3001500(i, ii)(i, iii)(i, iv)(i, v)Level of disagreementFréchet Distance4504503001500(i)(ii)(iii)(iv)Disturbance levelFréchet Distance600(i) α = 0(ii) α = 1(iii) α = 2(iv) α = 4Disturbance levelFréchet Distance(i) α = 0(ii) α = 1(iii) α = 2(iv) α = 4(i)(iii)(iv)(i, ii)(i, iii)(i, iv)(i, v)(i) Depth Full(iii) RGB Simple(iv) RGB Empty(v) RGB Different(ii) RGB Full3002001000(i, ii)(i, iii)(i, iv)(i, v)Level of disagreementFréchet Distance4003002001000500(a) Gaussian Blur on RGB(b) Gaussian Blur on Depth 150010005000(i)(ii)(iii)(iv)Disturbance levelFréchet Distance2000(i) α = 0(ii) α = 0.25(iii) α = 0.5(iv) α = 0.7512006009003000Disturbance levelFréchet Distance1500(i) α = 0(ii) α = 0.25(iii) α = 0.5(iv) α = 0.75(i)(ii)(iii)(iv)(a) Gaussian Noise on RGB(b) Gaussian Noise on Depth 120100806040200(a)(b)(c)(d)disturbance levelFréchet Distance(a)α = 0(b)α = 1(d)α = 4(c)α = 2302520151050(a)(b)(c)(d)disturbance levelFréchet Distance35404550(a)α = 0(b)α = 1(d)α = 4(c)α = 2150100500200Masked GT RGB ImageSumantri et. al.OursGT RGB ImageMasked GT RGB ImageSumantri et. al.OursTable 2. Quantitative results of depth panorama synthesis on Structured3D dataset. Depth input type L/P means that we use LiDAR (L)
and dense perspective depth sensor (P) in arbitrary conﬁgurations for the input. Full RGB image is used along with synthesized depth
panorama for FAED calculation. Bold numbers indicate the best results.

Category

Method

Depth syn.

CSPN [12]
NLSPN [52]
MSG-CHN [38]
PENet [25]
Ours

Input type

RGB

Depth

Full

L/P

Depth metric

AbsREL(↓)
0.0855
0.1268
0.1764
0.1740
0.0844

RMSE(↓)
2214
2807
3296
3145
1942

Layout metric
2D IoU(↑)
0.8062
0.7333
0.6724
0.7033
0.8286

Proposed metric
FAED(↓)
428.9
836.1
896.4
906.0
131.5

Figure 8. (a) Visual results for RGB panorama synthesis on Structured3D dataset. Two methods, LaMa and CoMoGAN, are visualized
for comparison. (b) Visual results for depth panorama synthesis on Structured3D dataset. CSPN is also visualized for comparison. More
qualitative results can be found in suppl. material.

Table 3. Ablation study results of BIPS framework. The experi-
ments in four rows take RGB-D input.

Method

IwDS ([89] + [12])
Ours w/o BFF
Ours w/o RDAL
Ours

Metric

2D IoU(↑)
0.7561
0.7859
0.7164
0.8158

FAED(↓)
640.9
381.4
329.0
198.0

depth synthesis methods: CSPN [12], NLSPN [52], MSG-
CHN [38] and PENet [25]. In particular, our method out-
performs on of the best depth synthesis method, CSPN,
with much better AbsREL score (0.0844 vs 0.0855), RMSE
(1942 vs 2214), 2D IoU (0.8286 vs 0.8062) and FAED score
(131.5 vs 428.9). With the proposed RDAL scheme, our
method estimates the best layout depth, which is demon-
strated by the highest layout 2D IoU. This in turn, con-
siderably affects the overall depth error in other metrics
as well. Figure. 8(b) shows the qualitative comparison
with CSPN [12]. CSPN synthesizes the interior compo-
nents, e.g., beds, relatively well; however, the depth of
planes (e.g., walls and ceiling) are not clear. Therefore, it
cannot synthesize a valid layout, failing to generate a real-
istic 3D indoor model. By contrast, our synthesized depth
panorama shows an undisturbed and clear layout.
Evaluation on Real Dataset We evaluated our synthesized
RGB-D panorama on real indoor scenes in Matterport3D
and 2D-3D-S dataset. An output RGB-D panorama and
its 3D indoor model are visualized in Fig. 11. Overall,
our method synthesizes high-quality RGB-D panorama on
real indoor scenes, unseen during training. Our synthesized

depth panorama shows precise indoor layout and plausible
residuals, generating a realistic 3D indoor model. For quan-
titative result, our method achieved better FAED score than
IwDS (4645 vs 5099). Since the FAED score between syn-
thetic and real dataset is 3517, it demonstrates that there
exists distribution shift between the datasets and our result
shows realistic RGB-D panorama and 3D indoor models
consistently in both synthetic and real indoor scenes. More
results can be found in suppl. material.
4.3. Ablation Study and Analysis
Inpainting w/ Depth Synthesis (IwDS) One solution to
obtain RGB-D panorama from partial RGB-D inputs is se-
quentially accomplishing RGB synthesis (inpainting) and
depth synthesis. To be speciﬁc, a RGB panorama is ﬁrst
synthesized from partial RGB input using the image in-
painting method. Then, depth panorama is synthesized
by applying the depth synthesis method to the synthesized
RGB panorama and partial depth input. We chose Co-
ModGAN [89] and CSPN [12] for RGB and depth syn-
thesis methods, which showed the highest FAED score in
Table 1 and Table 2. In Table 3, it can be seen that IwDS
leads lower 2D IoU score and a much higher FAED score
than our method. This indicates that the two-stage, sequen-
tial synthesis of RGB-D panorama is less effective than our
BIPS framework that fuses the bi-modal features, trained
with one-stage, joint learning scheme. Also, IwDS fails to
generate realistic 3D indoor models, with distorted indoor
layouts and severe bumpy surfaces as shown in Fig. 10.
Impact of BFF We study the effectiveness of RGB-D
panorama synthesis by removing the BFF branch in the

7

GT RGB ImageMasked GT RGB Image (Input)OursCoModGANLaMaGT RGB ImageMasked GT RGB ImageBoundlessOursCoModGANLaMaBRGMGT RGB ImageMasked GT RGB Image (Input)OursCoModGANLaMa(a)(b)GT Depth MapGT RGB Image (Input)OursCSPNMasked GT Depth Map (Input)(b)GT Depth MapGT RGB Image (Input)OursCSPNMasked GT Depth Map (Input)Figure 9. Visualization of our synthesized RGB-D panorama results using RGB-D data in arbitrary conﬁgurations. (a) and (b) take both
RGB and depth data, (c) takes only RGB and (d) takes only depth data. More results are visualized in suppl. materials.

Figure 10. Visualization of ablation study results on Structured3D dataset. Result without BFF shows artifacts irrelevant to given input and
result without RDAL infers distorted room layout while our ﬁnal model synthesizes perceptual undistorted indoor room.

designed to learn RGB and total depth panorama, respec-
tively. As shown in Table 3, the 2D IOU score drops, and
FAED score increases without RDAL. It shows that RDAL
is critical for estimating precise indoor layout. The impact
of RDAL is visually veriﬁed in Fig. 10. The result without
RDAL shows distorted indoor layout while having fewer
artifacts than ours w/o BFF. In summary, dividing the to-
tal depth into layout and residual depth helps to synthesize
more structural 3D indoor model.
5. Conclusion

In this paper, we tackled a novel problem of synthesizing
RGB-D indoor panoramas from arbitrary conﬁgurations of
RGB and depth inputs. Our method can synthesize high-
quality RGB-D panoramas with the proposed BIPS frame-
work by utilizing the bi-modal information and jointly train-
ing the layout and residual depth of indoor scenes. More-
over, a novel evaluation metric FAED was proposed and
its validity was demonstrated. Extensive experiments show
that our method achieves the SoTA RGB-D panorama syn-
thesis performance.
Limitation We mainly focused on indoor scenes, and pro-
posed RDAL is hardly applicable to outdoor scenes. Future
work will extend our method to various environments.

Figure 11. Visualization of our synthesized RGB-D panorama and
3D indoor model on Matterport3D dataset.

(I d

In details, GBF F is replaced with a single
generator.
branch network taking the concatenation of Grgb
in ) and
Gdepth
in). As shown in Table 3, the 2D IoU drops and
in
FAED score increases without BFF. Fig. 10 shows that tex-
ture of the RGB-D output is not consistent with the given
arbitrary RGB-D input. This reﬂects that BFF signiﬁcantly
contributes to well-process the bi-modal information.

in (I rgb

Impact of RDAL We further validate the effectiveness of
RDAL by comparing the results without RDAL. The num-
ber of output branches are reduced to two, and each are

8

Ground TruthInputOutputGround TruthInputOutputGT RGBDMasked GT RGBDOursGT RGBDMasked GT RGBDOurs(a)(b)GT RGB-DMasked GT RGB-DOursGT RGB-DMasked GT RGB-DOurs(c)(d)GT RGB ImageMasked GT RGB ImageOurs - RDL OursOurs - BMFMasked GT Depth MapMasked GT RGB-D (Input) Ours w/o BFFOurs w/o RDALOursGTMasked GT RGB-D (Input) Ours w/o BFFOurs w/o RDALOursGTIwDSGT RGB-DMasked GT RGB-DOurs RGB-DGT 3D Indoor ModelOurs 3D Indoor ModelGT RGB-DMasked GT RGB-DOurs RGB-DGT 3D Indoor ModelOurs 3D Indoor ModelReferences

[1] Ghassem Alaee, Amit P Deasi, Lourdes Pena-Castillo, Ed-
ward Brown, and Oscar Meruvia-Pastor. A user study on
augmented virtuality using depth sensing cameras for near-
range awareness in immersive vr. In IEEE VR’s 4th Work-
shop on Everyday Virtual Reality (WEVR 2018), volume 10,
2018. 3

[2] Borji Ali. Pros and cons of gan evaluation measures. Com-
puter Vision and Image Understanding, 179:41–65, 2019. 2
[3] Iro Armeni, Sasha Sax, Amir R Zamir, and Silvio Savarese.
Joint 2d-3d-semantic data for indoor scene understanding.
arXiv preprint arXiv:1702.01105, 2017. 5

[4] Shai Avidan and Ariel Shamir. Seam carving for content-
In ACM SIGGRAPH 2007 Papers,
aware image resizing.
SIGGRAPH ’07, page 10–es, New York, NY, USA, 2007.
Association for Computing Machinery. 2

[5] Coloma Ballester, Marcelo Bertalmio, Vicent Caselles,
Guillermo Sapiro, and Joan Verdera. Filling-in by joint inter-
polation of vector ﬁelds and gray levels. IEEE transactions
on image processing, 10(8):1200–1211, 2001. 2

[6] Connelly Barnes, Eli Shechtman, Adam Finkelstein, and
Dan B Goldman. Patchmatch: A randomized correspon-
dence algorithm for structural image editing. ACM Trans.
Graph., 28(3):24, 2009. 2

[7] Connelly Barnes, Eli Shechtman, Adam Finkelstein, and
Dan B Goldman. Patchmatch: A randomized correspon-
dence algorithm for structural image editing. ACM Trans.
Graph., 28(3), July 2009. 2

[8] Marcelo Bertalmio, Guillermo Sapiro, Vincent Caselles, and
Coloma Ballester. Image inpainting. In Proceedings of the
27th annual conference on Computer graphics and interac-
tive techniques, pages 417–424, 2000. 2

[9] Marcelo Bertalmio, Luminita Vese, Guillermo Sapiro, and
Simultaneous structure and texture im-
IEEE transactions on image processing,

Stanley Osher.
age inpainting.
12(8):882–889, 2003. 2

[10] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Hal-
ber, Matthias Niessner, Manolis Savva, Shuran Song, Andy
Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-
d data in indoor environments. International Conference on
3D Vision (3DV), 2017. 5

[11] X. Cheng, P. Wang, G. Chenye, and R. Yang. Cspn++:
Learning context and resource aware convolutional spatial
propagation networks for depth completion. Proceedings of
the AAAI Conference on Artiﬁcial Intelligence, 34:10615–
10622, 04 2020. 2

[12] X. Cheng, P. Wang, and R. Yang. Learning depth with con-
volutional spatial propagation network. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 42(10):2361–
2379, 2020. 2, 7

[13] Dongho Choi. 3d room layout estimation beyond the man-
hattan world assumption. arXiv preprint arXiv:2009.02857,
2020. 6

[14] Antonio Criminisi, Patrick Perez, and Kentaro Toyama. Ob-
In 2003 IEEE
ject removal by exemplar-based inpainting.
Computer Society Conference on Computer Vision and Pat-
tern Recognition, 2003. Proceedings., volume 2, pages II–II.
IEEE, 2003. 2

[15] Antonio Criminisi, Patrick P´erez, and Kentaro Toyama.

9

Region ﬁlling and object removal by exemplar-based im-
IEEE Transactions on image processing,
age inpainting.
13(9):1200–1212, 2004. 2

[16] DC Dowson and BV Landau. The fr´echet distance between
multivariate normal distributions. Journal of multivariate
analysis, 12(3):450–455, 1982. 5

[17] Alexei A Efros and Thomas K Leung. Texture synthesis
In Proceedings of the sev-
by non-parametric sampling.
enth IEEE international conference on computer vision, vol-
ume 2, pages 1033–1038. IEEE, 1999. 2

[18] Abdelrahman Eldesokey, Michael Felsberg, and Fahad Shah-
baz Khan. Conﬁdence propagation through cnns for guided
sparse depth regression. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, 42(10):2423–2436, Oct 2020.
2

[19] Dewen Guo, Jie Feng, and Bingfeng Zhou. Structure-aware
image expansion with global attention. In SIGGRAPH Asia
2019 Technical Briefs, SA ’19, page 13–16, New York, NY,
USA, 2019. Association for Computing Machinery. 2
[20] Dongsheng Guo, Hongzhi Liu, Haoru Zhao, Yunhao Cheng,
Qingwei Song, Zhaorui Gu, Haiyong Zheng, and Bing
Zheng. Spiral Generative Network for Image Extrapolation,
pages 701–717. 11 2020. 2

[21] Takayuki Hara and Tatsuya Harada. Spherical image genera-
tion from a single normal ﬁeld of view image by considering
scene symmetry. arXiv preprint arXiv:2001.02993, 2020. 1,
2, 3

[22] Yuwen He, Yan Ye, Philippe Hanhart, and Xiaoyu Xiu.
Geometry padding for motion compensated prediction in
In 2017 Data Compression Conference
360 video coding.
(DCC), pages 443–443. IEEE Computer Society, 2017. 3

[23] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. In Advances in neural information processing systems,
pages 6626–6637, 2017. 2, 5

[24] Noriaki Hirose and Kosuke Tahara. Depth360: Monoc-
ular depth estimation using learnable axisymmetric cam-
arXiv preprint
era model for spherical camera image.
arXiv:2110.10415, 2021. 3

[25] Mu Hu, Shuling Wang, Bin Li, Shiyu Ning, Li Fan, and
Xiaojin Gong. Towards precise and efﬁcient image guided
depth completion. 2021. 2, 7

[26] Z. Huang, J. Fan, S. Cheng, S. Yi, X. Wang, and H. Li. Hms-
net: Hierarchical multi-scale sparsity-invariant network for
sparse depth completion. IEEE Transactions on Image Pro-
cessing, 29:3429–3441, 2020. 2

[27] Satoshi Iizuka, Edgar Simo-Serra, and Hiroshi Ishikawa.
Globally and locally consistent image completion. ACM
Transactions on Graphics (ToG), 36(4):1–14, 2017. 2
[28] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Efros.
Image-to-image translation with conditional adver-
sarial networks. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 1125–1134,
2017. 2

[29] Maximilian Jaritz, Raoul De Charette, Emilie Wirbel, Xavier
Perrotton, and Fawzi Nashashibi. Sparse and dense data with
cnns: Depth completion and semantic segmentation. In 2018
International Conference on 3D Vision (3DV), pages 52–60.

IEEE, 2018. 2

[30] Biliana Kaneva, Josef Sivic, Antonio Torralba, Shai Avidan,
and William T. Freeman. Inﬁnite images: Creating and ex-
ploring a large photorealistic virtual space. Proceedings of
the IEEE, 98(8):1391–1407, 2010. 2

[31] Sai Hemanth Kasaraneni and Abhishek Mishra. Image com-
pletion and extrapolation with contextual cycle consistency.
In 2020 IEEE International Conference on Image Processing
(ICIP), pages 1901–1905, 2020. 2

[32] Kyunghun Kim, Yeohun Yun, Keon-Woo Kang, Kyeongbo
Kong, Siyeong Lee, and Suk-Ju Kang. Painting outside as
inside: Edge guided image outpainting via bidirectional re-
arrangement with progressive step learning. 2021 IEEE Win-
ter Conference on Applications of Computer Vision (WACV),
pages 2121–2129, 2021. 2

[33] Dilip Krishnan, Piotr Teterwak, Aaron Sarna, Aaron
Maschinot, Ce Liu, David Belanger, and William Freeman.
Boundless: Generative adversarial networks for image ex-
tension. pages 10520–10529, 10 2019. 2, 6

[34] B. Lee, H. Jeon, S. Im, and I. S. Kweon. Depth comple-
tion with deep geometry and context guidance. In 2019 In-
ternational Conference on Robotics and Automation (ICRA),
pages 3281–3287, 2019. 2

[35] Donghoon Lee, Sangdoo Yun, Sungjoon Choi, Hwiyeon
Yoo, Ming-Hsuan Yang, and Songhwai Oh. Unsupervised
holistic image generation from key local patches. In Vitto-
rio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair
Weiss, editors, Computer Vision – ECCV 2018, pages 21–37,
Cham, 2018. Springer International Publishing. 2

[36] S. Lee, J. Lee, D. Kim, and J. Kim. Deep architecture with
cross guidance between single image and sparse lidar data
for depth completion. IEEE Access, 8:79801–79810, 2020.
2

[37] Ang Li, Zejian Yuan, Yonggen Ling, Wanchao Chi, Chong
Zhang, et al. A multi-scale guided cascade hourglass net-
work for depth completion. In The IEEE Winter Conference
on Applications of Computer Vision, pages 32–40, 2020. 2

[38] A. Li, Z. Yuan, Y. Ling, W. Chi, S. Zhang, and C. Zhang.
A multi-scale guided cascade hourglass network for depth
completion. In Proceedings of the IEEE/CVF Winter Con-
ference on Applications of Computer Vision (WACV), March
2020. 2, 7

[39] Yijun Li, Sifei Liu, Jimei Yang, and Ming-Hsuan Yang. Gen-
In Proceedings of the IEEE con-
erative face completion.
ference on computer vision and pattern recognition, pages
3911–3919, 2017. 2

[40] Liang Liao, Jing Xiao, Zheng Wang, Chia-Wen Lin, and
Image inpainting guided by coherence
Shin’ichi Satoh.
In Proceedings of the
priors of semantics and textures.
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 6539–6548, 2021. 2

[41] Guilin Liu, Fitsum A Reda, Kevin J Shih, Ting-Chun Wang,
Image inpainting for
Andrew Tao, and Bryan Catanzaro.
In Proceedings
irregular holes using partial convolutions.
of the European Conference on Computer Vision (ECCV),
pages 85–100, 2018. 2

[42] Hongyu Liu, Bin Jiang, Yi Xiao, and Chao Yang. Coher-
ent semantic attention for image inpainting. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-

sion, pages 4170–4179, 2019. 2

[43] Hongyu Liu, Ziyu Wan, Wei Huang, Yibing Song, Xintong
Han, and Jing Liao. Pd-gan: Probabilistic diverse gan for im-
age inpainting. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 9371–
9381, 2021. 2

[44] F. Ma, G. V. Cavalheiro, and S. Karaman. Self-supervised
sparse-to-dense: Self-supervised depth completion from li-
In 2019 International Confer-
dar and monocular camera.
ence on Robotics and Automation (ICRA), pages 3288–3295,
2019. 2

[45] Fangchang Mal and Sertac Karaman.

Sparse-to-dense:
Depth prediction from sparse depth samples and a single im-
age. In 2018 IEEE International Conference on Robotics and
Automation (ICRA), pages 1–8. IEEE, 2018. 2

[46] Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen
Wang, and Stephen Paul Smolley. Least squares genera-
tive adversarial networks. In Proceedings of the IEEE inter-
national conference on computer vision, pages 2794–2802,
2017. 5

[47] Razvan V Marinescu, Daniel Moyer, and Polina Golland.
Bayesian image reconstruction using deep generative mod-
els. arXiv preprint arXiv:2012.04567, 2020. 2, 6

[48] Indra Deep Mastan and Shanmuganathan Raman. Deepcﬂ:
Deep contextual features learning from a single image. 2021
IEEE Winter Conference on Applications of Computer Vision
(WACV), pages 2896–2905, 2021. 2

[49] Anish Mittal, Anush Krishna Moorthy, and Alan Con-
rad Bovik. No-reference image quality assessment in the
IEEE Transactions on image processing,
spatial domain.
21(12):4695–4708, 2012. 2

[50] Anish Mittal, Rajiv Soundararajan, and Alan C Bovik. Mak-
ing a “completely blind” image quality analyzer. IEEE Sig-
nal processing letters, 20(3):209–212, 2012. 2

[51] Shant Navasardyan and Marianna Ohanyan. Image inpaint-
In Proceedings of the Asian

ing with onion convolutions.
Conference on Computer Vision, 2020. 2

[52] J. Park, K. Joo, Z. Hu, C. Liu, and I. Kweon. Non-local
spatial propagation network for depth completion. In Proc.
of European Conference on Computer Vision (ECCV), 2020.
2, 7

[53] Jialun Peng, Dong Liu, Songcen Xu, and Houqiang Li. Gen-
erating diverse structure for image inpainting with hierarchi-
In Proceedings of the IEEE/CVF Conference
cal vq-vae.
on Computer Vision and Pattern Recognition, pages 10775–
10784, 2021. 2

[54] J. Qiu, Z. Cui, Y. Zhang, X. Zhang, S. Liu, B. Zeng, and
M. Pollefeys. Deeplidar: Deep surface normal guided depth
prediction for outdoor scene from sparse lidar data and single
color image. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR), June
2019. 2

[55] Paul L Rosin, Yu-Kun Lai, Ling Shao, and Yonghuai Liu.

RGB-D Image Analysis and Processing. Springer, 2019. 3

[56] Mark Sabini and Gili Rusak. Painting outside the box: Image

outpainting with gans, 2018. 2

[57] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki
Cheung, Alec Radford, and Xi Chen. Improved techniques
In Advances in neural information pro-
for training gans.

10

cessing systems, pages 2234–2242, 2016. 2

[58] Stefan Schubert, Peer Neubert, Johannes P¨oschmann, and
Peter Pretzel. Circular convolutional neural networks for
panoramic images and laser data. In 2019 IEEE Intelligent
Vehicles Symposium (IV), pages 653–660. IEEE, 2019. 3
[59] Qi Shan, Brian Curless, Yasutaka Furukawa, Carlos Hernan-
dez, and Steven M. Seitz. Photo uncrop.
In David Fleet,
Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars, editors,
Computer Vision – ECCV 2014, pages 16–31, Cham, 2014.
Springer International Publishing. 2

[60] Maitreya Suin, Kuldeep Purohit, and AN Rajagopalan.
Distillation-guided image inpainting. In Proceedings of the
IEEE/CVF International Conference on Computer Vision,
pages 2481–2490, 2021. 2

[61] Julius Surya Sumantri and In Kyu Park. 360 panorama syn-
thesis from a sparse set of images with unknown ﬁeld of
In The IEEE Winter Conference on Applications of
view.
Computer Vision, pages 2386–2395, 2020. 1, 2, 3, 6

[62] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin,
Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov,
Naejin Kong, Harshith Goka, Kiwoong Park, and Victor
Lempitsky. Resolution-robust large mask inpainting with
arXiv preprint arXiv:2109.07161,
fourier convolutions.
2021. 2, 6

[63] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon
Shlens, and Zbigniew Wojna. Rethinking the inception ar-
chitecture for computer vision. In CVPR, pages 2818–2826,
2016. 2

[64] Jie Tang, Fei-Peng Tian, Wei Feng, Jian Li, and Ping Tan.
Learning guided convolutional network for depth comple-
tion. arXiv preprint arXiv:1908.01238, 2019. 2

[65] W. Van Gansbeke, D. Neven, B. De Brabandere, and L. Van
Gool. Sparse and noisy lidar completion with rgb guidance
and uncertainty. In 2019 16th International Conference on
Machine Vision Applications (MVA), pages 1–6, 2019. 2
[66] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In Advances in neural
information processing systems, pages 5998–6008, 2017. 2
[67] Ziyu Wan, Jingbo Zhang, Dongdong Chen, and Jing Liao.
High-ﬁdelity pluralistic image completion with transform-
ers. arXiv preprint arXiv:2103.14031, 2021. 2

[68] B. Wang and J. An. Fis-nets: Full-image supervised net-

works for monocular depth estimation, 2020. 2

[69] Fu-En Wang, Yu-Hsuan Yeh, Min Sun, Wei-Chen Chiu, and
Yi-Hsuan Tsai. Bifuse: Monocular 360 depth estimation via
bi-projection fusion. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pages
462–471, 2020. 3

[70] Miao Wang, Yu-Kun Lai, Yuan Liang, Ralph R. Martin, and
Shi-Min Hu. Biggerpicture: Data-driven image extrapola-
tion using graph matching. ACM Trans. Graph., 33(6), Nov.
2014. 2

[71] Ning Wang, Jingyuan Li, Lefei Zhang, and Bo Du. Musical:
Multi-scale image contextual attention learning for inpaint-
ing. In IJCAI, pages 3748–3754, 2019. 2

[72] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao,
Jan Kautz, and Bryan Catanzaro. High-resolution image syn-
thesis and semantic manipulation with conditional gans. In

Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 8798–8807, 2018. 4

[73] Wentao Wang, Jianfu Zhang, Li Niu, Haoyu Ling, Xue Yang,
and Liqing Zhang. Parallel multi-resolution fusion network
for image inpainting. In Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision, pages 14559–
14568, 2021. 2

[74] Yi Wang, Xin Tao, Xiaoyong Shen, and Jiaya Jia. Wide-
In 2019 IEEE/CVF
context semantic image extrapolation.
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 1399–1408, 2019. 2

[75] Xian Wu, Rui-Long Li, Fang-Lue Zhang, Jian-Cheng Liu,
Jue Wang, Ariel Shamir, and Shi-Min Hu. Deep portrait im-
age completion and extrapolation. IEEE Transactions on Im-
age Processing, 29:2344–2355, 2020. 2

[76] Chaohao Xie, Shaohui Liu, Chao Li, Ming-Ming Cheng,
Wangmeng Zuo, Xiao Liu, Shilei Wen, and Errui Ding. Im-
age inpainting with learnable bidirectional attention maps.
In Proceedings of the IEEE/CVF International Conference
on Computer Vision, pages 8858–8867, 2019. 2

[77] Y. Xu, X. Zhu, J. Shi, G. Zhang, H. Bao, and H. Li. Depth
completion from sparse lidar data with depth-normal con-
straints. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision (ICCV), October 2019. 2
[78] Shang-Ta Yang, Fu-En Wang, Chi-Han Peng, Peter Wonka,
Min Sun, and Hung-Kuo Chu. Dula-net: A dual-projection
network for estimating room layouts from a single rgb
In Proceedings of the IEEE/CVF Conference
panorama.
on Computer Vision and Pattern Recognition, pages 3363–
3372, 2019. 6

[79] Zongxin Yang, Jian Dong, Ping Liu, Yi Yang, and Shuicheng
Yan. Very long natural scenery image prediction by outpaint-
ing. In Proceedings of the IEEE International Conference on
Computer Vision, pages 10561–10570, 2019. 2

[80] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and
Thomas S Huang. Generative image inpainting with con-
textual attention. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 5505–5514,
2018. 2

[81] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and
Thomas S Huang. Free-form image inpainting with gated
convolution. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pages 4471–4480, 2019. 2
[82] Wei Zeng, Sezer Karaoglu, and Theo Gevers. Joint 3d layout
and depth prediction from a single indoor panorama image.
In European Conference on Computer Vision, pages 666–
682. Springer, 2020. 4

[83] Lingzhi Zhang, Jiancong Wang, and Jianbo Shi. Multimodal
image outpainting with regularized normalized diversiﬁca-
In 2020 IEEE Winter Conference on Applications of
tion.
Computer Vision (WACV), pages 3422–3431, 2020. 2
[84] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-
man, and Oliver Wang. The unreasonable effectiveness of
deep features as a perceptual metric. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 586–595, 2018. 2

[85] Xiaofeng Zhang, Feng Chen, Cailing Wang, Ming Tao, and
Guo-Ping Jiang. Sienet: Siamese expansion network for im-
age extrapolation. IEEE Signal Processing Letters, PP:1–1,

11

08 2020. 2

[86] Y. Zhang and T. Funkhouser. Deep depth completion of a
single rgb-d image. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June
2018. 2

[87] Yinda Zhang, Jianxiong Xiao, James Hays, and Ping Tan.
Framebreak: Dramatic image extrapolation by guided shift-
maps. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 1171–1178, 2013. 2

[88] Lei Zhao, Qihang Mo, Sihuan Lin, Zhizhong Wang, Zhiwen
Zuo, Haibo Chen, Wei Xing, and Dongming Lu. Uctgan:
Diverse image inpainting based on unsupervised cross-space
In Proceedings of the IEEE/CVF Conference
translation.
on Computer Vision and Pattern Recognition, pages 5741–
5750, 2020. 2

[89] Shengyu Zhao, Jonathan Cui, Yilun Sheng, Yue Dong, Xiao
Liang, Eric I Chang, and Yan Xu. Large scale image comple-
tion via co-modulated generative adversarial networks. arXiv
preprint arXiv:2103.10428, 2021. 2, 6, 7

[90] Chuanxia Zheng, Tat-Jen Cham, and Jianfei Cai. Pluralistic
image completion. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 1438–
1447, 2019. 2

[91] Jia Zheng, Junfei Zhang, Jing Li, Rui Tang, Shenghua
Structured3d: A large photo-
Gao, and Zihan Zhou.
realistic dataset for structured 3d modeling. arXiv preprint
arXiv:1908.00222, 2019. 5

12

