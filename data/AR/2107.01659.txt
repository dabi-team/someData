Time Series Graphical Lasso and Sparse VAR
Estimation

Aramayis Dallakyan

Rakheon Kim

Mohsen Pourahmadi

Texas A&M University

Abstract

We improve upon the two-stage sparse vector autoregression (sVAR) method in Davis
et al. (2016) by proposing an alternative two-stage modiﬁed sVAR method which relies
on time series graphical lasso to estimate sparse inverse spectral density in the ﬁrst stage,
and the second stage reﬁnes non-zero entries of the AR coeﬃcient matrices using a false
discovery rate (FDR) procedure. Our method has the advantage of avoiding the inversion of
the spectral density matrix but has to deal with optimization over Hermitian matrices with
complex-valued entries. It signiﬁcantly improves the computational time with a little loss in
forecasting performance. We study the properties of our proposed method and compare the
performance of the two methods using simulated and a real macro-economic dataset. Our
simulation results show that the proposed modiﬁcation or msVAR is a preferred choice when
the goal is to learn the structure of the AR coeﬃcient matrices while sVAR outperforms
msVAR when the ultimate task is forecasting.

Keywords: Time Series Graphical Models, Sparse Vector Autoregression, FDR.

1
2
0
2

l
u
J

4

]

O
C

.
t
a
t
s
[

1
v
9
5
6
1
0
.
7
0
1
2
:
v
i
X
r
a

 
 
 
 
 
 
1

Introduction

A vector autoregressive (VAR) model is a powerful tool for analyzing multivariate time

series. The recent increase in the availability of time series data pivots the interest of

researchers toward high-dimensional VAR models. A common strategy in high-dimensional

VAR estimation is to impose regularization on AR coeﬃcient matrices. These methods

can be grouped into three diﬀerent approaches: regularized least square estimators using

Lasso-type penalties (Song and Bickel, 2011; Basu and Michailidis, 2015; Kock and Callot,

2015; Nicholson et al., 2016; Barigozzi and Brownlees, 2019; Saﬁkhani and Shojaie, 2020);

regularized maximum-likelihood estimators (Basu and Michailidis, 2015; Davis et al., 2016;

Yuen et al., 2018), and regularized Yule-Walker estimators using the CLIME or Dantzig

estimators (Han et al., 2015; Wu and Wu, 2016; Ding et al., 2017).

Regularized least square VAR methods ignore the contemporaneous dependence in the

time series since the loss function does not include the covariance of error terms. Song and

Bickel (2011) discuss the possible impact in ﬁtting a VAR model in which the contempo-

raneous dependence is ignored. Davis et al. (2016) numerically show that the forecasting

performance of the VAR model improves when the information on the error covariance ma-

trix is incorporated in the regularized log-likelihood. They proposed a two-stage approach

to ﬁt sparse VAR models. In the ﬁrst stage, instead of working in a time domain, authors

resort to a frequency domain and estimate the partial spectral coherence (PSC) to iden-

tify possible non-zero autoregressive coeﬃcients (see Section 2.1 for details). Then, using

constrained maximum likelihood estimation, parameters are estimated under the sparsity

constraint. The lag order p and the number of pairs of non-zero AR parameters M are

chosen using the Bayesian Information Criterion (BIC) over the speciﬁed grid values of M

and p. In the second stage, the selected model is reﬁned by identifying spurious non-zero AR

coeﬃcients. In particular, for non-zero AR coeﬃcients, a sequence of t-statistics is created,

and m of them are chosen using the BIC. The rest of the coeﬃcients are considered spurious

2

and shrunk to zero. It is informative to note that the link between zero PSCs and zero AR

coeﬃcients is not exact. We give more details on this relationship in Appendix A.1.

In this paper, we improve the Davis et al. (2016) framework, by proposing a modiﬁcation

of their two-stage sVAR method, calling it modiﬁed sVAR (msVAR), with the following two

key distinctions:

1. In msVAR, zeros of PSC are identiﬁed by employing time series graphical lasso (TS-

Glasso) (Jung et al., 2015; Foti et al., 2016; Tugnait, 2018) to estimate the inverse spectral

density matrix. The main advantage of such modiﬁcation is to avoid inversion of a possibly

high-dimensional matrix. However, TSGlasso involves optimization over Hermitian matrices

with complex-valued entries which needs a special treatment, see Appendix B.

2. We use FDR in the reﬁnement stage. The impetus of the FDR utilization is to

substitute many pairwise hypothesis tests with a multiple hypothesis testing, which provides

a better model selection framework (Benjamini and Gavrilov, 2009; Barber and Cand`es,

2015). In Section 5.1.5, our simulation results show the advantages of the FDR reﬁnement

in the second stage.

The remainder of the paper is organized as follows. Section 2 introduces details on

the multivariate time series analysis, VAR, and the two-stage sVAR method. Time series

graphical models and TSGlasso are discussed in Section 3, where TSGlasso requires tuning

parameter selection to control sparseness and similarity of undirected graphs corresponding

to the inverse spectral density matrices across the Fourier frequencies. Section 4 provides

details of our algorithm for msVAR. In Section 5, we study and compare sVAR and msVAR

using simulated and a real datasets. For the real data, in addition to sVAR and msVAR

models, we consider Bayesian Ridge Regression VAR (BRRVAR) (Banbura et al., 2010),

VAR with Lasso (LASSOVAR) penalty (Song and Bickel, 2011), and VAR with hierarchical

componentwise (HVARC) and Own/Other (HVAROO) (Nicholson et al., 2016) penalties.

Finally, we conclude with the discussion in Section 6.

3

2 Multivariate Stationary VAR Models

In this section, we review some basic properties of multivariate stationary processes, their

spectral density matrices, VAR, and sVAR models.

2.1 Partial Spectral Coherence

In this part, we give a brief introduction to the PSC estimation. A deeper treatment

can be found in Brillinger (1981); Brockwell and Davis (1986). Let {Yt,i} and {Yt,j} be

two distinct marginal series of a K-variate stationary process {Yt}, and {Yt,−ij} denotes the

remaining (K − 2) marginal processes. The conditional correlation between two time series

is computed by adjusting for the linear eﬀect of the remaining marginal series {Yt,−ij}. The

linear eﬀect of {Yt,−ij} is removed from the {Yt,i} by determining the optimal ﬁlter {Dk,j}
(Dahlhaus, 2000), which minimizes E(Yt,i − (cid:80)∞

k=−∞ DkiYt−k,−ij)2. Then, the residual after

removing the linear ﬁlter is:

(cid:15)t,i := Yt,i −

∞
(cid:88)

k=−∞

Dk,iYt−k,−ij.

(1)

The residual (cid:15)t,j after removing the linear eﬀect from Yt,j can be estimated similarly.

Thus, two marginal series {Yt,i} and {Yt,j} are conditionally uncorrelated, if and only if their

residual series {(cid:15)t,i} and {(cid:15)t,j} are uncorrelated at all lags; i.e., cor((cid:15)t+k,i, (cid:15)t,j) = 0 for all

k ∈ Z.

In the frequency domain, {(cid:15)t,i} and {(cid:15)t,j} series being uncorrelated at all lags is

equivalent to the cross-spectral density of two residual series being zero at all normalized

frequencies ω ∈ [0, 1], and the residual (cross)spectral density is deﬁned as the Fourier

transform of the autocovariance sequence

f (cid:15)
ij(ω) :=

1
2π

∞
(cid:88)

k=−∞

cij(k)(cid:15)e(−i2πkω), ω ∈ [0, 1],

(2)

where cij(k)(cid:15) = cov((cid:15)t+k,i, (cid:15)t,j) is the (cross)autocovariance function of two marginal processes

4

in which we tacitly assume (cid:80)|cij(k)|< ∞. The PSC between two distinct marginal series

(Brillinger, 1981; Brockwell and Davis, 1986) is deﬁned as:

P SCij(ω) :=

f (cid:15)
ij(ω)
ii(ω)f (cid:15)
f (cid:15)

(cid:113)

jj(ω)

, ω ∈ [0, 1].

(3)

The residual cross-spectral density f (cid:15)

ij(ω) can be computed from the spectral density

f Y (ω) of the process {Yt} by

ij(ω) = f Y
f (cid:15)

ij (ω) − f Y

i,−ij(ω)(f Y

−ij,−ij(ω))−1f Y

−ij,j(ω),

(4)

where f Y

i,−ij, f Y

−ij,−ij and f Y

inverting (K − 2) × (K − 2) matrices f Y

−ij,j are some partitions of the spectral density matrix. (4) involves
(cid:1) pairs, which is computationally

−ij,−ij(ω) for (cid:0)K

2

challenging for high dimensional data. Dahlhaus (2000) proposed an eﬃcient method to
simultaneously compute PSC for all (cid:0)K
(cid:1) pairs by inverting the K × K spectral density

2

matrix. Thus, setting ΘY (ω) := f Y

ij (ω)−1, the PSC can be computed as follows:

P SCij(ω) = −

ΘY

ij(ω)
ii (ω)ΘY

ΘY

(cid:113)

jj(ω)

,

(5)

where ΘY

ii (ω), ΘY

jj(ω) are the ith and jth diagonal entries and ΘY

ij(ω) the (i, j)th entry

of ΘY (ω). From (2),(3) and (5), it can be seen that {Yt,i} and {Yt,j} are conditionally

uncorrelated iﬀ ΘY

ij(ω) = 0 for all ω ∈ [0, 1]. Note that PSC and the inverse spectral density

matrix are analogs of familiar notions of partial correlations and inverse covariance matrices

in multivariate statistics.

5

2.2 VAR and sVAR Models

Consider K dimensional VAR model of order p (VAR(p)):

Yt = a + A1Yt−1 + · · · + ApYt−p + ut,

(6)

where Yt = (y1t, . . . , yKt)T is a random vector, Ai’s are ﬁxed (K × K) coeﬃcient matrices,

and a = (a1, . . . , aK)T is a (K × 1) vector of intercept. The K -dimensional white noise is

given by ut = (u1t, . . . , uKt)T where E(ut) = 0, E(utuT
otherwise. We further assume that the process is stable, i.e., det(Ik − (cid:80)p

s ) = Σu for t = s, and E(utuT

i=1 Aizi) (cid:54)= 0 for

s ) = 0

z ∈ C, |z|≤ 1.

Given time series observations Y1, . . . , YT , ﬁtting VAR models amounts to estimating the

lag order p and the coeﬃcient matrices A1, . . . , Ap. However, when K is large or even moder-

ate, the VAR model is over-parametrized since the number of parameters grows quadratically

(K 2p). Therefore, there is growing interest in developing sparse methods to overcome the

computational problem and the interpretation of the model parameters, see for example

Song and Bickel (2011); Davis et al. (2016); Nicholson et al. (2016); Ding et al. (2017); Yuen

et al. (2018); Saﬁkhani and Shojaie (2020).

2.2.1 Two-stage sVAR

In this section, we describe the two-stage sVAR approach introduced in Davis et al.

(2016). Algorithm 1 reports Stage 1 and 2 of the sVAR algorithm.

Stage 1: Model Selection: The ﬁrst stage exploits PSC to set to zero certain entries of

coeﬃcient matrices. More precisely,

P SCij(ω) = 0, ω ∈ [0, 1] =⇒ Ak(i, j) = Ak(j, i) = 0, k = 1, . . . , p.

(8)

6

Algorithm 1: sVAR algorithm

Stage 1

1. Invert estimated spectral density matrix and compute the PSC for all K(K −

1)/2 pairs of distinct marginal series.

2. Construct a sequence Q1 by ranking summary statistics Sij’s (see (9)) from

highest to lowest.

3. For each (p, M ) ∈ P × M, set the order of autoregression to p and feed the top
M pairs in the sequence Q1 to the VAR model. Estimate parameters under
this constraint and compute the corresponding BIC(p, M ):

BIC(p, M ) = −2 log L(A1, . . . , Ap) + log T · (K + 2M )p

(7)

4. Choosethe number of lags and non-zero pairs (p∗, M ∗) that gives the minimum

BIC value over P × M

Stage 2

1. For each of the non-zero AR coeﬃcient compute the t-statistic via (10),
2. Construct the sequence Q2 of the (K + 2M ∗)p∗ triplets (i, j, k) by ranking

|ti,j,k| from highest to lowest,

3. For each m ∈ {0, 1, . . . , (K + 2M ∗)p∗} select the m non-zero AR coeﬃcient
corresponding to the top triplets in the sequence Q2 and compute BIC(m) =
−2 log L + log T · m,

4. Choose the number of non-zero m∗ that gives the minimum BIC value:

BIC(m∗) = −2 log L + log T · m∗.

As discussed, this relationship is only an assertion and is not exact for some cases. See

Section A.1 for more details.

Thus, a group of AR coeﬃcient estimates is set to zero if the corresponding PSC estimates

are zero. However, because of the sampling variability, estimated PSCs are not exactly zero

even though two marginal series are conditionally uncorrelated. Davis et al. (2016) overcome

this problem by ranking estimated PSCs from largest to smallest and ﬁnding a threshold that

separates non-zero PSCs in which the supremum of the squared modulus of the estimated

PSC is used as summary statistics:

Sij := sup|P SCij(ω)|2,

(9)

7

where the supremum is taken over all scaled frequencies ω. Thus, a large value of Sij indicates

that two marginal series are conditionally correlated and vice versa.

The output of the ﬁrst stage algorithm is a model with (K + 2M ∗)p∗ non-zero AR coef-

ﬁcients. If the proportion of selected pairs is small, then the number of non-zero parameters

is much smaller than that for the fully-parametrized VAR(p∗), where the number of parame-

ters is K 2p∗. In Step 3, the parameter estimation under the constraint is implemented using

constrained log-likelihood estimation described in L¨utkepohl (2007, Chapter 5).

Stage 2: Reﬁnement:

In Stage 1, the PSC can only be evaluated for pairs of series,

but it does not consider diagonal entries in A1, . . . , Ap and within the group coeﬃcients

for each pair of component series. In other words, Stage 1 may produce spurious non-zero

AR coeﬃcients. To emancipate the model from spurious coeﬃcients, in the (K + 2M ∗)p∗

sequence of non-zero AR coeﬃcients, Davis et al. (2016) rank them according to the absolute

value of their t-statistics, i.e.,

ti,j,k :=

Ak(i, j)
se(Ak(i, j))

,

(10)

where the standard error se(Ak(i, j)) is computed from the asymptotic distribution of the

constrained maximum likelihood estimator (L¨utkepohl, 2007). After the second stage, the

procedure leads to a sparse VAR model that contains m∗ non-zero AR coeﬃcients, denoted

by sVAR(p∗, m∗).

3 Time Series Graphical Lasso

The salient feature of Gaussian graphical models is to represent conditional indepen-

dencies among random variables in multivariate data. An undirected graph is a powerful

tool for visualizing these relationships where the vertices represent the random variables,

and the edge between two vertices indicates the conditional dependence of corresponding

variables. For a K-dimensional random vector Y ∼ NK(0, Σ), a Gaussian graphical model

8

can be constructed from the inverse covariance matrix Θ = Σ−1. More precisely, a zero

oﬀ-diagonal entry of Θij = 0 implies that Yi and Yj are conditionally independent given all

other variables (Whittaker, 1990). When K is large, it is reasonable to impose structure or

regularize Θ directly in the search for sparsity (Banerjee et al., 2008; Friedman et al., 2008),

see Pourahmadi (2013) for an overview.

Given the sample data, a regularized Gaussian graphical model estimation can be for-

mulated as

min
Θ

log det(Θ) − tr(SΘ) + P (Θ, λ),

(11)

where det(·) is determinant of the matrix, S is the sample covariance matrix, P (·) is a

penalization term, and λ is a tuning parameter. In Banerjee et al. (2008); Friedman et al.
(2008) the penalization term is (cid:96)1 norm, i.e., P (Θ, λ) = λ(cid:107)Θ(cid:107)1= λ (cid:80)

ij|θij|. The Glasso

algorithm is extremely popular and has been extended to multiple covariance matrices in

Guo et al. (2011); Danaher et al. (2014) where the data from several populations may have

a similar graphical structure.

Brillinger (1996) and Dahlhaus (2000) have extended the use of graphical models to the

multivariate time series setup. Consider K dimensional stationary process Yt, t = 1, . . . , T .

Let G = (V, E) denote a graph, where each node v ∈ V corresponds to one of the times

series in Yt and the edge between nodes is characterized by the conditional dependence of

the marginal series Yi and Yj, given the rest Y−ij; i.e., Yi−−Yj iﬀ ΘY

ij(ω) = 0, or PSC(ω) =

0 for ω ∈ [0, 1]. From now on, whenever there is no confusion in the context, we drop

the superscript Y from Θ, and note that Θ(·) is a Hermitian matrix-valued function with

complex-valued entries.

A time series extension of the Glasso requires expressing the log-likelihood function in

terms of the discrete Fourier transform of the data and Θ(·). Deﬁne the normalized discrete

9

Fourier transform (DFT) of K dimensional random vector Yt,

d(ωn) =

1
√
T

T −1
(cid:88)

t=0

Ytexp(−i2πωnt),

(12)

where i =

√

−1, ωn = n/T, n = 0, 1, . . . , T −1. Since Yt is real-valued, the complex conjugate

d∗(ωn) = d(−ωn) = d(1−ωn) and for n = 0, 1, . . . , T /2, d(ωn) is completely determined for all

n. Moreover, from Brillinger (1981, Theorem 4.4.1) as T → ∞, d(ωn), n = 1, 2, . . . , (T /2)−1

are independent complex Gaussian Nc(0, f [ωn]) random vectors and for n = {0, T /2}, d(ωn)

are independent real Gaussian Nr(0, f [ωn]). Ignoring n = {0, T /2} frequency points, and

denoting f [ωn] = f [n], Θ[n] = f −1[n], the joint pdf for d(ωn), n = 1, . . . , (T /2) − 1 is

g(d(ω1), . . . , d(ω(T /2)−1)) =

(T /2)−1
(cid:89)

n=1

exp(−dH(ωn)Θ[n]d(ωn))
πKdet(f [n])

(13)

A standard assumption in spectral density estimation is locally smoothness (Brillinger,

1981; Stoica and Moses, 1997), i.e., f [n] is approximately constant over L = 2mt + 1 ≥ K

consecutive frequency points where mt is the half-window size. After carefully picking

˜ωl =

(l − 1)L + mt + 1
T

; M =

(cid:106) T /2 − mt − 1
L

(cid:107)

; l = 1, 2, . . . M,

leads to M equally spaced frequencies ˜ωl. Therefore, the exploitation of the local smooth-

ness assumption results for k = −mt, −mt + 1, . . . , mt

˜ωl,k =

(l − 1)L + mt + 1 + k
T

; f [k] = f [{l, k}].

(14)

10

From (14) and (13), the pdf is

g(d(ω1), . . . , d(ω(T /2)−1)) =

M
(cid:89)

mt(cid:89)

exp(−dH(˜ωl,n)Θ[n]d(ωl,n))
πKdet(f [n])L

n=1

M
(cid:89)

l=1

=

l=−mt
exp[−tr( ˜f [n]Θ[n])]
πLK log detf [n]

(15)

,

where ˜f [n] = (cid:80)mt

l=−mt

d(˜ωl,n)dH(˜ωl,n)/L is the sample spectral density matrix whose entries

are potentially complex-valued. Thus, the log-likelihood function can be written as

W (Θ[·]) =

M
(cid:88)

n=1

(cid:104)

(cid:105)
log det(Θ[n]) − tr( ˜f [n]Θ[n])

.

L

Analogous to Glasso, we introduce sparsity by minimizing the following regularized log-

likelihood

where

W (Θ[·]) + P (Θ[·], λ),

min
Θ[n]

(16)

P (Θ[·], λ) = λ

(cid:118)
(cid:117)
(cid:117)
(cid:116)

N
(cid:88)

(cid:88)

i(cid:54)=j

n=1

|Θij[n]|2, Θ[·] = {Θ[1], . . . , Θ[N ]},

(17)

As in Jung et al. (2015), we appeal to the alternating direction method of multipliers

(ADMM) (Boyd et al., 2011) for minimization. However, in the following we pay due atten-

tion to the fact that the entries of Θ are complex-valued. The ADMM minimizes the scaled

augmented Lagrangian

Lρ(Θ[·], Z[·], U [·]) =

(cid:104)

L

M
(cid:88)

n=1

− log det(Θ[n]) + tr( ˜f [n]Θ[n]))

(cid:105)

+ λP (Z[·])

+ ρ/2

M
(cid:88)

n=1

(cid:107)Θ[n] − Z[n] + U [n](cid:107)2
F ,

(18)

subject to Θ[n] = Z[n] and (cid:107)X[n](cid:107)2

F = (cid:80)
(Θ(k)[·], Z (k)[·], U (k)[·]) matrices in the kth iteration, the ADMM algorithm implements the

for n = 1, . . . , M .

ij|Xij[n]|2

Given

11

following three updates for the next (k + 1) iteration:

(a) Θ(k+1)[·] ← arg minΘ[·] Lρ(Θ[·], Z (k)[·], U (k)[·])

(b) Z (k+1)[·] ← arg minZ[·] Lρ(Θ(k+1)[·], Z[·], U (k)[·])

(c) U (k+1)[·] ← U (k)[·] + (Θ(k+1)[·] − Z (k+1)[·])

It is timely and instructive to note that unlike the formulation in (11) (Banerjee et al., 2008;

Friedman et al., 2008; Danaher et al., 2014) where Θ is real-valued, here we have to deal

with a complex-valued Θ[·] in (18). While Li et al. (2015) establish steps for the complex-

valued ADMM when the penalty function is (cid:96)1 norm, here we resort to Wirtinger calculus

(Wirtinger, 1927; Brandwood, 1983), coupled with the deﬁnition of Wirtinger subgradients

(Bouboulis et al., 2012), to derive updates (a)-(c) for matrices with complex entries. Details

are relegated to Appendix B.

Boyd et al. (2011, Section 3.1) showed that for a given ρ, the convergence of iterates to

the global minimum is guaranteed. The choice of ρ controls the speed of convergence. Boyd

et al. (2011, Section 3.4.1) discuss the adaptive choice of ρ to improve convergence. On the

statistical side, Jung et al. (2015) provide an upper bound on the support recovery of the

TSGlasso.

3.1 Tuning Parameter Selection for TSGlasso

In the TSGlasso algorithm, the tuning parameter λ controls the sparsity and the similarity

of the estimated undirected graphs over the scaled frequencies. In this section, we review

some classical methods for tuning parameter selection.

Ideally, the selected tuning parameter should produce an undirected graph that is suf-

ﬁciently complex to be interesting, suﬃciently sparse to be interpretable, and, more im-

portantly, should be supported by data. The traditional approaches such as the Akaike

information criterion (AIC), Bayesian information criterion (BIC) and cross-validation tend

12

to choose graph that is too large (Meinshausen and B¨uhlmann, 2010). Homrighausen and

McDonald (2018) empirically showed that for the penalized regression, the tuning parame-

ter, selected from the Stein unbiased risk estimator (SURE)-type criterion, tends to perform

better than other considered criteria. For graphical models, extended BIC, introduced in

Foygel and Drton (2010), shows practical superiority compare to discussed criteria.

From (15), the AIC approximation for the time series graphical model is:

AIC(λ) =

M
(cid:88)

n=1

(cid:104)

(cid:105)
− log det ˆΘλ[n] + tr( ˜f (n) ˆΘλ[n])

× L + 2En,

(19)

where ˆΘλ[n] is the estimated inverse spectral density at tuning parameter λ, and En is the
number of non-zero elements in ˆΘλ[n]. Using AIC, we choose λ which gives the minimum

value of (19). Similarly, an approximation of the extended BIC is:

eBIC(λ) =

M
(cid:88)

n=1

(cid:105)
(cid:104)
− log| ˆΘλ[n]|+tr( ˜f [n] ˆΘλ[n])

× L + log(L)En + 4Enγ log(K),

(20)

with a hyper-parameter γ ∈ [0, 1]. If γ = 0, then (20) reduces to the classical BIC. The

higher value of γ leads to the stronger penalization of large graphs. For the moderate and

large values of K, Foygel and Drton (2010) suggest γ = 0.5.

4 Modiﬁed sVAR

In this section, we introduce our msVAR procedure and highlight the key diﬀerences with

the sVAR. Algorithm 2 summarizes the proposed modiﬁcations.

The ﬁrst stage of our Algorithm is designed to avoid the costly matrix inversion and grid

search procedure to compute constrained MLE of the AR parameters. We substitute Steps

1 - 3 of the sVAR’s Stage 1 (see Algorithm 1) by the TSGlasso algorithm, with the ensuing

M ∗ non-zero elements, and use BIC only once to choose the number of lags p∗ compare to

13

Algorithm 2: msVAR algorithm

Stage 1

1. Estimate the inverse spectral density matrix using TSGlasso, and let M ∗ be

its number of non-zero elements (see Section 3).

2. Estimate the AR parameters under the zero constraint and choose the number

of lags (p∗) by minimizing

BIC(p) = −2 log L(A1, . . . , Ap) + log T (K + 2M ∗)p

Stage 2

1. For each of the non-zero AR coeﬃcient compute the t-statistic and p-value.
2. Choose m∗ non-zero coeﬃcients that reject hypothesis in FDR procedure with

the threshold value of FDR-corrected signiﬁcance q.

sVAR (see Step 3 in Stage 1 in Algorithm 1).

In the second stage, instead of using t-statistics of the AR coeﬃcients, our algorithm

relies on the FDR (Benjamini and Hochberg, 1995) procedure for further reﬁnement. There

is a rich literature on the use of FDR for model selection, for example, see Benjamini and

Gavrilov (2009); Barber and Cand`es (2015); G’Sell et al. (2016), etc. The advantage of FDR

utilization in the second stage is twofold: First, instead of pairwise t-statistics, we implement

multiple hypothesis testing. Second, it eliminates the need for Step 3 in Stage two of sVAR

(see Algorithm 1). The empirical analysis in Appendix 5.1.5 further shows the advantages

of using FDR for reﬁnement.

5 Numerical Results

In this section, we use simulated and real datasets to compare the performance of msVAR

and sVAR models. Our analyses indicate that msVAR is a preferred choice when the goal

is to learn the structure of the coeﬃcient matrix. On the other hand, sVAR outperforms

msVAR when the ultimate task is forecasting. The analysis in Section 5.1.4 shows that the

proposed modiﬁcations signiﬁcantly improve the computation time of the algorithm.

14

5.1 Comparing the msVAR and sVAR Models

5.1.1 Evaluation Measures and Visualization

The following metrics are computed to compare the performance of the two methods:

• the squared bias of the AR coeﬃcient estimates:

max(p,ˆp)
(cid:88)

K
(cid:88)

[E[ ˆAk(i, j)] − Ak(i, j)]2;

k=1

i,j=1

• the variance of the estimated AR coeﬃcient:

max(p,ˆp)
(cid:88)

K
(cid:88)

k=1

i,j=1

var( ˆAk(i, j));

• the mean square error (MSE) of the AR coeﬃcient estimates:

max(p,ˆp)
(cid:88)

K
(cid:88)

{[E[ ˆAk(i, j)] − Ak(i, j)]2 + var( ˆAk(i, j))}.

k=1

i,j=1

• the true positive rate (TPR): estimates the ratio between the number of correctly found

edges in estimated graph and the number of true edges in the true graph.

• the false positive rate (FPR): estimates the ratio between the number of incorrectly

found edges in estimated graph and the number of true missing edges in the true graph.

In addition, we utilize a Eichler (2012)’s proposal to visualize an estimated VAR model

using a mixed graph. The edge set Em of the mixed graph Gm = (Vm, Em) consists of

directed and undirected edges, in which

• i → j /∈ Em whenever Ak(i, j) = 0, k = 1, . . . , p

• i−−j /∈ Em whenever (Θu)ij = (Θu)ji = 0.

In other words, the directed edge is in the edge set whenever Yi is Granger-causal for

Yj (L¨utkepohl, 2007), and an undirected edge is in the edge set whenever Yi and Yj are

15

contemporaneously conditionally dependent. However, for the sake of clarity, we present

only the directed part of the mixed graph as in Figures 1-3.

5.1.2 The Simulation Setup

In the simulation study, we consider three diﬀerent stable VAR models to compare per-

formance of sVAR and msVAR methods.

Model 1: yt = A1yt−1 + ut, and K = 10

A1 =










0
0
0
0
0
0.2
0
0
0.2
0

0
0
0.6
0.2
0.3
0
0
0
0
0

0
0.1
0
0
0
0
0
0
0
0

0
0
0
0
0.1
0
0
0
0
0

0
0
0
0.5
0
0.4
0
0
0
0.4

0
0
0
0
0
0
0
0.6
0
0

0
0.4
0
0
0.2
0
0
0
0
0

0.3
0
0
0
0.1
0
0
0
0
0

0
0
0
0
0.3
0
0
0
0.2
0










0
0.4
0
0
0.5
0
0.6
0
0
0

, Θu =





δ
δ/2
...
δ/10

δ/2
1
...
0

. . .
. . .
. . .
. . .

δ/10
0



,

0
1

where δ = 0.5 . The setup of the simulation is borrowed from the Davis et al. (2016,

Section 4). See Figure 1 for illustration.

Model 2: yt = A1yt−1 + ut, and K = 6

A1 =


0
0
0

0
0
0

0.50
0
0.25
0
0
0.50

0.50
0.30
0.50
0
0
0

0.20
0
0
0
0
0

0
0
0
0.33
0
0.17



0
0
0
0.33
0.20
0.33

 , Θu =





0.17
0
0.25
0.03
0
0

0
1.40
0.34
0.25
0.04
0.58

0.25
0.34
0.55
0.05
0
0

0.030
0.25
0.05
0.26
0
0.42

0
0.04
0
0
1.51
0.36





0
0.58
0
0.42
0.36
0.98

The purpose of this setup is to compare methods when for some entries of A1 the

assertion (8) is violated

PSCij(ω) = 0, ω ∈ [0, 1] and A1[i, j] (cid:54)= 0

(21)

See Figure 2 for illustration. Red directed edges indicate entries of A1 that violate

assertion (8).

16

Remark 1. In general, for any element of A1, enforcement of (21) is not a trivial

task. Values and structure of coeﬃcient matrices A1 and Θu are carefully constructed

such that A1[1, 2] and A1[4, 5] satisfy condition (22) located in the Appendix.

Model 3: yt = A1yt−1 + A2yt−2 + ut, and K = 6

A1 =





−0.6
0.4
0
0
0
0.4

0.4
−0.6
0.4
0
0
0

0
0.4
−0.6
0.4
0
0

0
0
0.4
−0.6
0.4
0

0
0
0
0.4
−0.6
0.4



0.4
0
0
0
0.4
−0.6

 , A2 =





−0.3
0.2
0
0
0
0.2

0.2
−0.3
0.2
0
0
0

0
0.2
−0.3
0.2
0
0

0
0
0.2
−0.3
0.2
0

0
0
0
0.2
−0.3
0.2





0.2
0
0
0
0.2
−0.3

Θu =





1
−0.3
0
0
0
−0.3

−0.3
1
−0.3
0
0
0

0
−0.3
1
−0.3
0
0

0
0
−0.3
1
−0.3
0

0
0
0
−0.3
1
−0.3





−0.3
0
0
0
−0.3
1

This setup is borrowed from Yuen et al. (2018, Section 4). The directed graph is

illustrated in Figure 3.

For each model, the corresponding multivariate time series is generated following

L¨utkepohl (2007, Appendix D1) over the 50 replications. For all models, T = 100 and

for the tuning parameter selection in Stage 1, we only report results for the eBIC since

BIC and AIC provide similar outcomes. For Stage 2 of the msVAR, the threshold value of

FDR-corrected signiﬁcance is ﬁxed at q = 0.1.

5.1.3 The Simulation Result

Figures 1 - 3 and Table 1 report the simulation results for Models 1 - 3, respectively. In

each ﬁgure, top left directed graph corresponds to the true case and top right and bottom

left to the msVAR and sVAR, respectively. The width and color shade of the estimated edges

correspond to the proportion of times the edge was detected out of 50 replications; i.e., the

darker and thicker the edge, more frequently it was present and vice versa. In Figure 2, red

17

directed edges correspond to the condition (21). In Table 1, for each method, the minimum

of Bias2, Variance, MSE and FPR metrics, and the maximum of TPR are highlighted.

From Figures 1 - 3, the visual comparison reveals that both msVAR and sVAR were able

to detect true edges for most of the time. For Model 1, msVAR indicates better result on

estimating true edges than sVAR. For example, sVAR failed to detect the edge Y5 ←→ Y10 in

all repetitions, while msVAR detected it around 90% of time. The ﬁrst two rows in Table 1

document the ﬁve metrics comparison for Model 1. It can be seen, that msVAR is the best

for all metrics.

A similar result holds for Model 2. msVAR shows small bias but higher variance and

the best TPR result. More importantly, both algorithms were able to detect the edges

Y 6 −→ Y 2 and Y 1 −→ Y 2 most of the time, even thought the assertion (8) was violated.

For Model 3, the performance is reversed compared to Model 2, i.e., msVAR shows slightly

higher bias but smaller variance.

Figure 1: Model 1 simulation result. The width and color shade of estimated edges indicate
the proportion of the number of times the edge was detected out of 50 replication.

18

Figure 2: Model 2 simulation result. Red directed edges in the true graph correspond to the
condition (21). The width and color shade of estimated edges indicate the proportion of the
number of times the edge was detected out of 50 replication.

Figure 3: Model 3 simulation result. The width and color shade of estimated edges indicate
the proportion of the number of times the edge was detected out of 50 repetition.

19

Table 1: Five metrics from the sVAR and msVAR methods.

Method

Bias2

Variance MSE

TPR

Model 1

Model 2

Model 3

sVAR
0.178
msVAR 0.174

sVAR
0.508
msVAR 0.303

sVAR
msVAR

0.089
0.102

0.788
0.732

0.609
0.951

0.762
0.742

0.966
0.906

1.117
1.254

0.851
0.844

0.595
0.632

0.477
0.537

0.954
0.946

FPR

0.03
0.02

0.08
0.118

0.03
0.05

5.1.4 Running Time Comparison

In this section, we compare relative running times for sVAR and msVAR methods. For

this exercise, we ﬁxed p = 1, and for K = 15, 25, 50, 75, generate a sparse coeﬃcient matrix A

such that the probability of having a non-zero element is equal to 0.25. Then, the coeﬃcient

matrix is rescaled to satisfy the stability condition. For the msVAR method, we select

two options for time comparison: msVAR with tuning parameter selection and without,

respectively. For brevity, we call those methods msVAR with and msVAR without. In the

former case, the tuning parameter is selected over 20 equally spaced values located in (0, 1)

interval, and for the latter case, the tuning parameter is ﬁxed to λ = 0.2. The relative time

is reported with respect to the running time of the msVAR without. The modiﬁed R code for

the msVAR algorithm relies on the sVAR code framework provided in Davis et al. (2016).

Table 2 reports relative times for K = 15, 25, 50, 75.

Table 2: Relative running times for the sVAR and msVAR algorithm. Running times are
reported relative to the msVAR algorithm.

K

15
25
50
100

msVAR with msVAR without

sVAR

10.03
4.91
1.02
1.01

1
1
1
1

4.27
38.80
68.50
−∗

Note:

∗ The algorithm was terminated after 24 hours.

20

It can be seen when K = 15, the msVAR without is the fastest, followed by the sVAR,

which is nearly 4.3 times slower than the msVAR without. Finally, the msVAR with is

almost 10 times slower than the msVAR without. However, sVAR becomes extremely slow

as K grows. For large K, running times for the msVAR without and msVAR with are

almost indistinguishable. The result can be explained by observing that in both methods,

the computationally expensive procedure is the restricted MLE estimation, and msVAR is

fast since it implements it once, compared to sVAR’s grid search approach. Moreover, the

computational expense of restricted MLE estimation overshadows the computation time of

the tuning parameter selection as K grows.

5.1.5 Comparing msVAR stage 1 and stage 2 outputs

In this section, we compare msVAR stage 1 and stage 2 outputs. Recall that in stage

2 of msVAR, we use the FDR procedure for edge selection. For comparison, we use Model

1, described in Section 5, to generate the dataset. Table 3 and Figure 4 report the results.

Results from both table and ﬁgure indicate the performance improvement after the FDR

reﬁnement in stage 2.

Table 3: Three metrics from the msVAR and msVAR St.1 methods.

msVAR St.1
msVAR

Bias2

0.524
0.508

Variance MSE

1.249
0.925

1.774
1.433

5.2 Real Data Analysis

We compare the forecasting performance of various VAR methods on a real-world macro-

economic dataset. In addition to sVAR and msVAR, we consider Bayesian Ridge Regression

VAR (BRRVAR) (Banbura et al., 2010), VAR with Lasso (LASSOVAR) penalty (Song and

Bickel, 2011), and VAR with hierarchical componentwise (HVARC) and Own/Other (HVA-

21

Figure 4: Model 1 simulation result. The width and color shade of estimated edges indicate
the proportion of the number of times the edge was detected out of 50 replication.

ROO) (Nicholson et al., 2016) penalties. Corresponding tuning parameters for the LASSO-

VAR, HVARC, and HVAROO are selected using rolling cross-validation (Nicholson et al.,

2016).

The dataset represents the 168 monthly US macro-economic time series from 01/1959

to 02/2009.

Initially, the dataset was compiled by Stock and Watson (2005) and latter

augmented by Koop (2011). Koop (2011) deﬁnes and analyzes a small (K = 3), medium (K

= 20), medium-large (K = 40) and large (K = 168) VARs. In this paper, we focus only on

the medium-large setup with K = 40 variables. To avoid a policy break, the selected sample

period runs from 01/1990 to 2/2009. Following Koop (2011), we transform the data-set

to make variables approximately stationary. For both, sVAR and msVAR, BIC selects the

number of lags to be p = 2. The same number of lags p = 2 is used for other four sparse

VAR methods.

We compare the out-of-sample forecast performance for the above six VAR methods using

22

the last 24 months (Ttest = 24) as test data. We use the h-step-ahead forecast root mean

squared error (RMSE) as a measure for the comparison.

(cid:34)

RMSE(h) =

K −1(Ttest − h + 1)−1

K
(cid:88)

T +Ttest−h
(cid:88)

( ˆYt+h,k − Yt+h,k)2

(cid:35)1/2

,

k=1

t=T

where ˆYt+h,k is the h-step-ahead forecast of Yt+h,k for k = 1, . . . , K. Table 4 summarizes

RMSE(h) for a forecast horizon h = 1, 2, 3 and 4. It can be seen that the sVAR perform

slightly better than the msVAR for all h = 1, . . . , 4 and HVARC is the best among six

methods.

Table 4: The h-step ahead forecast root mean squared error (RMSE(h)).

h = 1

h = 2

h = 3

h = 4

0.179 (0.07)
msVAR
0.149 (0.055)
sVAR
0.156 (0.004)
BRRVAR
LASSOVAR 0.111(0.008)
HVARC
HVAROO

0.107 (0.008)
0.115 (0.008)

0.145 (0.008)
0.123 (0.008)
0.153(0.007)
0.099(0.012)
0.086 (0.012)
0.097(0.011)

0.151 (0.01)
0.131 (0.009)
0.158 (0.008)
0.088(0.018)
0.074 (0.018)
0.083 (0.017)

0.091(0.004)
0.086 (0.007)
0.149 (0.004)
0.068 (0.008)
0.053 (0.008)
0.069(0.009)

Note:

∗ Parentheses contain estimated Standard Deviations.

6 Conclusion

We have proposed the msVAR method, a modiﬁcation of the two-stage sVAR method

in Davis et al. (2016), where we substitute the ﬁrst stage of the sVAR with the new and

powerful time series graphical lasso algorithm to identify/estimate zeros of the inverse spec-

tral density matrix while recognizing that its entries are complex-valued. The second stage

implements reﬁnement of the non-zero entries using FDR. This paper focuses on algorith-

mic and numerical results. Real data analysis and simulation results show usefulness of our

method. Theoretical properties of our method, such as the consistency and the support

recovery of the msVAR, are left for future research.

23

A Appendix

A.1 Link Between AR coeﬃcient and PSC

As discussed, the assertion (8) is not exact and can be violated for some AR models.

Here, relying on the framework developed in Songsiri et al. (2009), we discuss the conditions

when (8) is exact.

The spectral density of the VAR process can be expressed as

f Y (ω) = A−1(eiω)ΣA−H(eiω),

where A−1(z) is the transfer function from ω to Y , A = I + z−1A1 + · · · + z−pAp, BH is the

Hermitian transpose of matrix B and i =

√

−1. Therefore, the inverse spectrum of an AR

process is a trigonometric matrix polynomial

ΘY (ω) = AH(eiω)ΘA(eiω) = X0 +

p
(cid:88)

(e−ikωXk + eikωX T

k ),

k=1

(22)

where Θ = Σ−1, Xk = (cid:80)p−k

i=0 AT

i ΘAi+k with A0 = I. From (4) and (22) we obtain,

PSCij(ω) = 0 ⇔ (Xk)ij = 0 for k = 0, . . . , p.

(23)

B Derivation of updates using Wirtinger Calculus

Before providing details on solving updates (a) and (b) for (18), we give a brief overview

of Wirtinger calculus. Deeper treatment of the subject can be found in Remmert (1991);

Kreutz-Delgado (2009) and in a pithy presentation by Brandwood (1983).

24

B.1 Wirtinger Calculus

Let z = x + iy, where x, y are real and i =

√

−1. Consider a general complex-valued

function f (z) = u(x, y) + iv(x, y), where we assume that the partial derivatives of u and v

exist. Then the standard complex derivative f (cid:48)(z) exist if f (z) is holomorphic or Cauchy-

Riemann equations are satisﬁed, i.e.,

∂u
∂v

=

∂v
∂y

,

∂v
∂x

= −

∂u
∂y

Unfortunately, those conditions are strong, and the functions that we are usually in-

terested in violate them. For example, f (z) = |z|2= z∗z, where z∗ is the conjugate of z.

However, since real partial derivatives of a non-holomorphic function exist, one can exploit

the real R2 vector space structure, which underlies C, and represent f (z) = f (x, y) : R2 → R.

Remmert (1991) called the diﬀerentiation of this function R-derivative to avoid confusion

with the standard complex derivative. As discussed in Kreutz-Delgado (2009, Section 3.1),

this representation can not be viewed as an admissible generalization of the standard com-

plex derivative, since it, as well, suﬀers from some drawbacks. For example, it does not

reduce to the standard complex derivative when a function f (z) is holomorphic.

The generalization, in a sense discussed above, were developed in the notion of Wirtinger

calculus (Wirtinger, 1927; Brandwood, 1983).

In particular, a complex function f (z) is

viewed as a function of z and its conjugate z∗

f (z) = f (z, z∗) = u(x, y) + iv(x, y).

It can be shown that f (z, z∗) is holomorphic in z for ﬁxed z∗, and, similarly, holomorphic in

z∗ for ﬁxed z. Then the Wirtinger derivative and its conjugate are deﬁned as

∂f (z, z∗)
∂z

,

∂f (z, z∗)
∂z∗

25

For example, for the function f (z) = |z|2= z∗z, ∂f (z,z∗)

∂z = z∗ and ∂f (z,z∗)

∂z∗ = z.

B.2 Solving Updates (a) and (b)

The derived formulas for updates (a) and (b) are given in (28) and (34), respectively. It

is instructive to note that, Lρ(Θ[·], Z[·], U [·]) is separable in n and the update of (a) can be

implemented in parallel by minimizing J(Θ[n]), n = 1, . . . , N , where

J(Θ[n]) = − log det(Θ[n]) + tr( ˜f [n]Θ[n]) + ρ/2(cid:107)Θ[n] − Z (k)[n] + U (k)[n](cid:107)2
F

(24)

To use Wirtinger calculus, we write J(Θ[n]) = J(Θ[n], Θ∗[n]) as a function of Θ and its

complex conjugate Θ∗. Thus, (24) can be written as

J(Θ[n], Θ∗[n]) = −

(cid:104)

1
2

log det(Θ[n]) + log det(Θ∗[n]) + tr( ˜f [n]Θ[n]) + tr( ˜f ∗[n]Θ∗[n])

+ ρtr(Θ[n] − Z (k)[n] + U (k)[n])(Θ[n] − Z (k)[n] + U (k)[n])H)

(cid:105)

(25)

Then in update (a), a necessary and suﬃcient condition for a global optimum is that the

gradient J(Θ[n], Θ∗[n]) with respect to Θ∗[n]) is zero (Brandwood, 1983):

Θ−1[n] + ˜f [n] + ρ(Θ[n] − Z (k)[n] + U (k)[n]) = 0

(26)

The solution to (26) follows as in Boyd et al. (2011, Section 6.5). Let the eigen-

decomposition of the matrix ρ(Z (k)[n] − U (k)[n]) − ˜f [n] be VnCnV H

n . Then

Θ(k+1) = Vn

˜CnV H
n ,

where ˜Cn is the diagonal matrix with the jth diagonal element

( ˜Cn)jj = (1/2ρ)(−(Cn)jj +

(cid:113)

(Cn)jj + 4ρ).

26

(27)

(28)

Finally, the update (a) is completed by obtaining the preceding solution for n = 1, 2, . . . , N .

For an update (b), we consider the following two lemmas. The ﬁrst lemma derives the

Wirtinger subgradient for the penalty term in (17), and the second lemma provides an update

for (b).

Lemma 1. Given x ∈ C N , the Wirtinger subgradient of the function T : C N → R, where

T (x) = (cid:107)x(cid:107)2 is

∂W T (x) =

x
2(cid:107)x(cid:107)2

,





∈ {u| (cid:107)u(cid:107)2≤ 1/2, u ∈ C p},

if x (cid:54)= 0

if x = 0

(29)

Proof. We start from the x (cid:54)= 0 case. The derivative of T (x, x∗) = (cid:107)x(cid:107)2= (xHx)1/2 with

respect to the conjugate x∗ is

∂T (x)
x∗ =

x
2(cid:107)x(cid:107)2

For the case x = 0, to ﬁnd the subgradient ∂W T (x∗) of the function T (x), we exploit

Bouboulis et al. (2012, Deﬁnition 3). From which, ∂W T (x) is a Wirtinger subgradient if it

satisﬁes

T (y) ≥ T (x) + 2Re((y − x)H(∂W T (x))∗), y ∈ C N

(30)

where Re(·) indicates the real part of the complex variable. From (30), we have for x = 0

and any y ∈ C N

(cid:107)y(cid:107)2≥ 2Re((y)H(∂W T (x))∗)

(31)

But (31) is just the deﬁnition of the dual function of (cid:107)x(cid:107)2, which is also (cid:96)2 norm (Horn and

Johnson, 2012), and the result follows.

For the next lemma, we deﬁne the generic function h : C N → R

h(x) =

1
2

(cid:107)a − x(cid:107)2

2+λ(cid:107)x(cid:107)2

27

(cid:4)

(32)

and denote by ˆx = arg minx h(x).

Lemma 2. The ith component of the global minimum of h(x) has the following closed form

solution

where Sµ(ai) = (1 − µ/(cid:107)a(cid:107)2)ai

ˆxi = Sλ/ρ(ai),

(33)

Proof. The proof of the lemma relies on the framework developed in Friedman et al. (2010),

Lemma 1 and Wirtinger calculus. Since h(x) is convex on x, a necessary and suﬃcient

condition for a global minimum x∗ is that the Wirtinger subdiﬀerntial ∂W h(x) ∈ 0. Thus,

we solve

0 ∈

1
2

(x − a) + γ,

where γ is a subgradient from Lemma 1. Then, the result can be derived following steps as

in Friedman et al. (2010, Section 2).

Invoking the Lemma 2, the update (b) for i (cid:54)= j is

Z (k+1)
ij

[n] = Sλ/ρ(Θ(k+1)

ij

[n] + U (k)

ij [n])

and Z (k+1)
ij

[n] = Θ(k+1)

ii

[n] for i = j, since we do not penalize diagonal elements.

(cid:4)

(34)

28

References

Banbura, M., D. Giannone, and L. Reichlin (2010), “Large bayesian vars.” Journal of Applied

Econometrics, 25, 71–92.

Banerjee, Onureena, Laurent El Ghaoui, and Alexandre d’Aspremont (2008), “Model selec-

tion through sparse maximum likelihood estimation for multivariate gaussian or binary

data.” J. Mach. Learn. Res., 9, 485–516.

Barber, RF and E. Cand`es (2015), “Controlling the false discovery rate via knockoﬀs.” The

Annals of Statistics, 43, 2055–2085.

Barigozzi, Matteo and Christian Brownlees (2019), “Nets: Network estimation for time

series.” Journal of Applied Econometrics, 34, 347–364.

Basu, Sumanta and George Michailidis (2015), “Regularized estimation in sparse high-

dimensional time series models.” Ann. Statist., 43, 1535–1567.

Benjamini, Y and Y. Gavrilov (2009), “A simple forward selection procedure based on false

discovery rate control.” The Annals of Applied Statistics, 3, 179–198.

Benjamini, Y and Y. Hochberg (1995), “Controlling the false discovery rate: a practical and

powerful approach to multiple testing.” Journal of the royal statistical society. Series

B, 289–300.

Bouboulis, P., K. Slavakis, and S. Theodoridis (2012), “Adaptive learning in complex repro-

ducing kernel hilbert spaces employing wirtinger’s subgradients.” IEEE Transactions on

Neural Networks and Learning Systems, 23, 425–438.

Boyd, Stephen, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein (2011), “Dis-

tributed optimization and statistical learning via the alternating direction method of

multipliers.” Found. Trends Mach. Learn., 3, 1–122.

29

Brandwood, D. H. (1983), “A complex gradient operator and its application in adaptive

array theory.” IEE Proceedings F - Communications, Radar and Signal Processing, 130,

11–16.

Brillinger, David R. (1981), Time Series: Data Analysis and Theory. Society for Industrial

and Applied Mathematics, Philadelphia, PA, USA.

Brillinger, David R. (1996), “Remarks concerning graphical models for time series and point

processes.” Brazilian Review of Econometrics, 16.

Brockwell, Peter J and Richard A Davis (1986), Time Series: Theory and Methods. Springer-

Verlag New York, Inc., New York, NY, USA.

Dahlhaus, Rainer (2000), “Graphical

interaction models for multivariate time series.”

Metrika, 51, 157–172.

Danaher, Patrick, Pei Wang, and Daniela M Witten (2014), “The joint graphical lasso for

inverse covariance estimation across multiple classes.” Journal of the Royal Statistical

Society: Series B (Statistical Methodology), 76, 373–397.

Davis, Richard A., Pengfei Zang, and Tian Zheng (2016), “Sparse vector autoregressive

modeling.” Journal of Computational and Graphical Statistics, 25, 1077–1096.

Ding, Xin, Ziyi Qiu, and Xiaohui Chen (2017), “Sparse transition matrix estimation for high-

dimensional and locally stationary vector autoregressive models.” Electron. J. Statist.,

11, 3871–3902.

Eichler, M. (2012), “Graphical modelling of multivariate time series.” Probab. Theory Related

Fields, 153, 233–268.

Foti, N., Rahul Nadkarni, A. Lee, and E. Fox (2016), “Sparse plus low-rank graphical models

of time series for functional connectivity in meg.” In 2nd SIGKDD Workshop on Mining

and Learning from Time Series.

30

Foygel, Rina and Mathias Drton (2010), “Extended bayesian information criteria for gaus-

sian graphical models.” In Proceedings of the 23rd International Conference on Neural

Information Processing Systems - Volume 1, NIPS’10, 604–612, Curran Associates Inc.

Friedman, J, T Hastie, and R. Tibshirani (2008), “Sparse inverse covariance estimation with

the graphical lasso.” Biostatistics, 9, 432–441.

Friedman, Jerome H., Trevor J. Hastie, and Robert Tibshirani (2010), “A note on the group

lasso and a sparse group lasso.”

G’Sell, MG, S Wager, A Chouldechova, and R. Tibshirani (2016), “Sequential selection

procedures and false discovery rate control.” Journal of the royal statistical society:

series B, 78, 423–444.

Guo, Jian, Elizaveta Levina, George Michailidis, and Ji Zhu (2011), “Joint estimation of

multiple graphical models.” Biometrika, 98, 1–15.

Han, Fang, Huanran Lu, and Han Liu (2015), “A direct estimation of high dimensional

stationary vector autoregressions.” Journal of Machine Learning Research, 16, 3115–

3150.

Homrighausen, Darren and J. Daniel McDonald (2018), “A study on tuning parameter selec-

tion for the high-dimensional lasso.” Journal of Statistical Computation and Simulation,

88, 2865–2892.

Horn, Roger A. and Charles R. Johnson (2012), Matrix Analysis, 2nd edition. Cambridge

University Press, New York, NY, USA.

Jung, Alexander, Gabor Hannak, and Norbert Goertz (2015), “Graphical lasso based model

selection for time series.” IEEE Signal Processing Letters, 22, 1781–1785.

Kock, Anders and Laurent Callot (2015), “Oracle inequalities for high dimensional vector

autoregressions.” Journal of Econometrics, 186, 325–344.

31

Koop, Gary M (2011), “Forecasting with medium and large bayesian vars.” Journal of Ap-

plied Econometrics, 28, 177–203.

Kreutz-Delgado, Ken (2009), “The complex gradient operator and the cr-calculus.”

Li, Lu, XingyuWang, and GuoqiangWang (2015), “Alternating direction method of multi-

pliers for separable convex optimization of real functions in complex variables.” Mathe-

matical Problems in Engineering, 2015.

L¨utkepohl, Helmut (2007), New Introduction to Multiple Time Series Analysis. Springer,

New York.

Meinshausen, Nicolai and Peter B¨uhlmann (2010), “Stability selection.” Journal of the Royal

Statistical Society: Series B (Statistical Methodology), 72, 417–473.

Nicholson, William B., Jacob Bien, and David S. Matteson (2016), “Hierarchical vector

autoregression.” Arxiv preprint arXiv:1412.5250v2.

Pourahmadi, Mohsen (2013), High-Dimensional Covariance Estimation. John Wiley & Sons,

Ltd.

Remmert, Reinhold (1991), Theory of Complex Functions. Springer, New York.

Saﬁkhani, Abolfazl and Ali Shojaie (2020), “Joint structural break detection and parameter

estimation in high-dimensional nonstationary var models.” Journal of the American

Statistical Association, 0, 1–14.

Song, S. and P. J. Bickel (2011), “Large vector auto regressions.” Arxiv preprint

arXiv:1106.3915.

Songsiri, Jitkomut, Joachim Dahl, and Lieven Vandenberghe (2009), Graphical models of

autoregressive processes, 89–116. Cambridge University Press.

32

Stock, James and Mark Watson (2005), “An empirical comparison of methods for forecasting

using many predictors.” Manuscript, Princton University.

Stoica, P. and R.L. Moses (1997), Introduction to Spectral Analysis. Prentice Hall.

Tugnait, J. K. (2018), “Graphical modeling of high-dimensional time series.” In 2018 52nd

Asilomar Conference on Signals, Systems, and Computers, 840–844.

Whittaker, J. (1990), Graphical models in applied multivariate statistics. John Wiley & Sons,

Ltd.

Wirtinger, W. (1927), “Zur formalen theorie der funktionen von mehr komplexen

ver¨anderlichen.” Mathematische Annalen, 97, 357–375.

Wu, Wei-Biao and Ying Nian Wu (2016), “Performance bounds for parameter estimates of

high-dimensional linear models with correlated errors.” Electron. J. Statist., 10, 352–

379.

Yuen, T.P., H. Wong, and K.F.C. Yiu (2018), “On constrained estimation of graphical time

series models.” Computational Statistics and Data Analysis, 124, 27 – 52.

33

