2
2
0
2

t
c
O
3
1

]

G
L
.
s
c
[

3
v
3
9
6
3
0
.
6
0
2
2
:
v
i
X
r
a

Autoregressive Perturbations for Data Poisoning

Pedro Sandoval-Segura1∗ Vasu Singla1∗ Jonas Geiping1 Micah Goldblum2
Tom Goldstein1 David W. Jacobs1

1University of Maryland

2New York University

{psando, vsingla, jgeiping, tomg, dwj}@umd.edu

goldblum@nyu.edu

Abstract

The prevalence of data scraping from social media as a means to obtain datasets has
led to growing concerns regarding unauthorized use of data. Data poisoning attacks
have been proposed as a bulwark against scraping, as they make data “unlearnable”
by adding small, imperceptible perturbations. Unfortunately, existing methods
require knowledge of both the target architecture and the complete dataset so that a
surrogate network can be trained, the parameters of which are used to generate the
attack. In this work, we introduce autoregressive (AR) poisoning, a method that
can generate poisoned data without access to the broader dataset. The proposed
AR perturbations are generic, can be applied across different datasets, and can
poison different architectures. Compared to existing unlearnable methods, our AR
poisons are more resistant against common defenses such as adversarial training
and strong data augmentations. Our analysis further provides insight into what
makes an effective data poison.

1

Introduction

Increasingly large datasets are being used to train state-of-the-art neural networks [24, 26, 25]. But
collecting enormous datasets through web scraping makes it intractable for a human to review
samples in a meaningful way or to obtain consent from relevant parties [3]. In fact, companies have
already trained commercial facial recognition systems using personal data collected from media
platforms [15]. To prevent the further exploitation of online data for unauthorized or illegal purposes,
imperceptible, adversarial modiﬁcations to images can be crafted to cause erroneous output for a
neural network trained on the modiﬁed data [12]. This crafting of malicious perturbations for the
purpose of interfering with model training is known as data poisoning.

In this work, we focus on poisoning data to induce poor performance for a network trained on
the perturbed data. This kind of indiscriminate poisoning, which seeks to damage average model
performance, is often referred to as an availability attack [1, 2, 40, 18, 9, 10]. Because we assume
the data is hosted on a central server controlled by the poisoner, the poisoner is allowed to perturb
the entire dataset, or a large portion of it. Throughout this work, unless stated otherwise, poisoning
refers to the perturbing of every image in the training dataset. This makes the creation of unlearnable
data different from other poisoning methods, such as backdoor [5, 13] and targeted poisoning
attacks [28, 43].

We introduce autoregressive (AR) data poisoning for degrading overall performance of neural
networks on clean data. The perturbations that we additively apply to clean data are generated by
AR processes that are data and architecture-independent. An AR(p) process is a Markov chain,
where each new element is a linear combination of p previous ones, plus noise. This means AR
perturbations are cheap to generate, not requiring any optimization or backpropagation through
network parameters. AR perturbations are generic; the same set of AR processes can be re-used to

∗Authors contributed equally.

36th Conference on Neural Information Processing Systems (NeurIPS 2022).

 
 
 
 
 
 
Figure 1: The same set of AR processes can generate perturbations for different kinds of data without
access to a surrogate network or other images in a dataset. A ResNet-18 trained on these AR poisons
is unable to generalize to clean test data, even with the help of strong data augmentations.

generate diverse perturbations for different image sizes and new datasets, unlike other poisoning
methods which need to train a surrogate network on the target dataset before crafting perturbations.

Our method also provides new insight into why data poisoning works. We work on top of the result that
effective poisons are typically easy to learn [27] and construct AR perturbations which are separable
by a manually-speciﬁed CNN. Working under the intuition that highly separable perturbations should
be easily learned, we use the manual speciﬁcation of parameters as a way of demonstrating that our
AR perturbations are easily separable. Our manually-speciﬁed CNN makes use of what we call AR
ﬁlters, which are attuned to detect noise from a speciﬁc AR process. AR poisoning’s effectiveness is
competitive or better than error-maximizing, error-minimizing, and random noise poisoning across a
range of architectures, datasets, and common defenses. AR poisoning represents a paradigm shift
for what a successful indiscriminate poisoning attack looks like, and raises the question of whether
strong indiscriminate poisons need to be generated by surrogate networks for a given dataset.

2 Background & Related Work

Error-minimizing and Error-maximizing Noise. To conduct poisoning attacks on neural networks,
recent works have modiﬁed data to explicitly cause gradient vanishing [31] or to minimize the loss
with respect to the input image [18]. Images perturbed with error-minimizing noises are a surprisingly
good data poisoning attack. A ResNet-18 (RN-18) trained on a CIFAR-10 [20] sample-wise error-
minimizing poison achieves 19.9% ﬁnal test accuracy, while the class-wise variant achieves 16.4%
ﬁnal test accuracy after 60 epochs of training [18]. More recently, strong adversarial attacks, which
perturb clean data by maximizing the loss with respect to the input image, have been shown to be the
most successful approach thus far [10]. An error-maximizing poison can poison a network to achieve
6.25% test accuracy on CIFAR-10. But both error-minimizing and error-maximizing poisons require
a surrogate network, from which perturbations are optimized. The optimization can be expensive. For
example, crafting the main CIFAR-10 poison from [10] takes roughly 6 hours on 4 GPUs. In contrast,
our AR perturbations do not require access to network parameters and can be generated quickly,
without the need for backpropagation or a GPU. We provide a technical overview of error-minimizing
and error-maximizing perturbations in Section 3.1.

Random Noise. Given their simplicity, random noises for data poisoning have been explored as
necessary baselines for indiscriminate poisoning. If random noise, constrained by an (cid:96)∞ norm, is
applied sample-wise to every image in CIFAR-10, a RN-18 trained on this poison can still generalize to
the test set, with ~90% accuracy [10, 18]. But if the noise is applied class-wise, where every image of a
class is modiﬁed with an identical additive perturbation, then a RN-18 trained on this CIFAR-10 poison
will achieve around chance accuracy; i.e. ~10% [39, 18, 27]. The random perturbations of [39] consist
of a ﬁxed number of uniform patch regions, and are nearly identical to the class-wise poison, called
“Regions-16,” from [27]. All the random noises that we consider are class-wise, and we conﬁrm they
work well in a standard training setup using a RN-18, but their performance varies across architectures
and they are rendered ineffective against strong data augmentations like Cutout [7], CutMix [41], and
Mixup [42]. Conversely, our AR poisons degrade test performance more than error-maximizing, error-
minimizing, and random poisons on almost every architecture. We show that AR perturbations are
effective against strong data augmentations and can even mitigate some effects of adversarial training.

2

Understanding Poisoning. A few works have explored properties that make for effective poisons.
For example, [27] ﬁnd that poisons which are learned quickly have a more harmful effect on the
poison-trained network, suggesting that the more quickly perturbations help minimize the training
loss, the more effective the poison is. [39] perform a related experiment where they use a single
linear layer, train on perturbations from a variety of poisoning methods, and demonstrate that they
can discriminate whether a perturbation is error-minimizing or error-maximizing with high accuracy.
We make use of ideas from both papers, designing AR perturbations that are provably separable
and Markovian in local regions.

Other Related Work. Several works have also focused on variants of “unlearnable” poisoning
attacks. [9] propose to employ gradient alignment [11] to generate poisons. But their method is
computationally expensive; it requires a surrogate model to solve a bi-level objective. [40] propose
generation of an unlearnable dataset using neural tangent kernels. Their method also requires training
a surrogate model, takes a long time to generate, and does not scale easily to large datasets. In
contrast, our approach is simple and does not require surrogate models. [23] propose an invertible
transformation to control learnability of a dataset for authorized users, while ensuring the data
remains unlearnable for other users. [35] showed that data poisoning methods can be broken using
adversarial training. [30] and [37] propose variants of error-minimizing noise to defend against
adversarial training. Our AR poisons do not focus on adversarial training. While adversarial training
remains a strong defense, our AR poisons show competitive performance. We discuss adversarial
training in detail in Section 4.3.2. A thorough overview of data poisoning methods, including those
that do not perturb the entire training dataset, can be found in [12].

3 Autoregressive Noises for Poisoning

3.1 Problem Statement

We formulate the problem of creating a clean-label poison in the context of image classiﬁcation with
DNNs, following [18]. For a K-class classiﬁcation task, we denote the clean training and test datasets
as Dc and Dt, respectively. We assume Dc, Dt ∼ D. We let fθ represent a classiﬁcation DNN with
parameters θ. The goal is to perturb Dc into a poisoned set Dp such that when DNNs are trained on
Dp, they perform poorly on test set Dt.
i=1 where xi ∈ Rd are
Suppose there are n samples in the clean training set, i.e. Dc = {(xi, yi)}n
the inputs and yi ∈ {1, ..., K} are the labels. We denote the poisoned dataset as Dp = {(x(cid:48)
i=1
i = xi + δi is the poisoned version of the example xi ∈ Dc and where δi ∈ ∆ ⊂ Rd is the
where x(cid:48)
perturbation. The set of allowable perturbations, ∆, is usually deﬁned by (cid:107)δ(cid:107)p < (cid:15) where (cid:107) · (cid:107)p is the
(cid:96)p norm and (cid:15) is set to be small enough that it does not affect the utility of the example. In this work,
we use the (cid:96)2 norm to constrain the size of our perturbations for reasons we describe in Section 3.4.

i, yi)}n

Poisons are created by applying a perturbation to a clean image in either a class-wise or sample-wise
manner. When a perturbation is applied class-wise, every sample of a given class is perturbed in
the same way. That is, x(cid:48)
i = xi + δyi and δyi ∈ ∆C = {δ1, ..., δK}. Due to the explicit correlation
between the perturbation and the true label, it should not be surprising that class-wise poisons
appear to trick the model to learn the perturbation over the image content, subsequently reducing
generalization to the clean test set. When a poison is applied sample-wise, every sample of the training
set is perturbed independently. That is, x(cid:48)
i = xi + δi and δi ∈ ∆S = {δ1, ..., δn}. Because class-wise
perturbations can be recovered by taking the average image of a class, these should therefore be
easy to remove. Hence, we focus our study on sample-wise instead of class-wise poisons. We still
compare to simple, randomly generated class-wise noises shown by [18] to further demonstrate the
effectiveness of our method.

All indiscriminate poisoning aims to solve the following bi-level objective:

max
δ∈∆

E(x,y)∼Dt [L(f (x), y; θ(δ))]

θ(δ) = arg min

θ

E(xi,yi)∼Dc [L(f (xi + δi), yi; θ)]

3

(1)

(2)

Figure 2: To generate 2D noise for a single channel using a 3 × 3 sliding window, we ﬁrst sample
Gaussian noise for the ﬁrst 2 columns and rows. Then, we use an AR(8) process within a sliding
window to compute the next value. Finally, we crop and scale the perturbation to be within our (cid:96)2
constraint.

Eq. 2 describes the process of training a network on poisoned data; i.e. xi perturbed by δi. Eq. 1
states that the poisoned network should maximize the loss, and thus perform poorly, on clean test
data.

Different approaches have been proposed to construct δi. Both error-maximizing [10] and error-
minimizing [18] poisoning approaches use a surrogate network, trained on clean training data, to
optimize perturbations. We denote surrogate network parameters as θ∗. Error-maximizing poisoning
[10] proposes constructing δi that maximize the loss of the surrogate network on clean training data:

max
δ∈∆

E(xi,yi)∼Dc [L(f (xi + δi), yi; θ∗)]

(3)

whereas error-minimizing poisoning [18] solve the following objective to construct δi that minimize
the loss of the surrogate network on clean training data:

min
δ∈∆

E(xi,yi)∼Dc [L(f (xi + δi), yi; θ∗)]

(4)

In both error-maximizing and error-minimizing poisoning, the adversary intends for a network, f ,
trained on the poison to perform poorly on the test distribution Dt, from which Dc was also sampled.
But the way in which both methods achieve the same goal is distinct.

3.2 Generating Autoregressive Noise

Autoregressive (AR) perturbations have a particularly useful structure where local regions throughout
the perturbation are Markovian, exposing a linear dependence on neighboring pixels [38]. This
property is critical as it allows for a particular ﬁlter to perfectly detect noise from a speciﬁc AR
process, indicating the noise is simple and potentially easily learned.

We develop a sample-wise poison where clean images are perturbed using additive noise. For each xi
in the clean training dataset, our algorithm crafts a δi, where (cid:107)δi(cid:107)2 ≤ (cid:15), so that the resulting poison
image is x(cid:48)
i = xi + δi. The novelty of our method is in how we ﬁnd and use autoregressive (AR)
processes to generate δi. In the following, let xt refer to the tth entry within a sliding window of δi.
An autoregressive (AR) process models the conditional mean of xt, as a function of past observations
xt−1, xt−2, ..., xt−p in the following way:

xt = β1xt−1 + β2xt−2 + ... + βpxt−p + (cid:15)t

(5)

where (cid:15)t is an uncorrelated process with mean zero and βi are the AR process coefﬁcients. For
simplicity, we set (cid:15)t = 0 in our work. An AR process that depends on p past observations is called an
AR model of degree p, denoted AR(p). For any AR(p) process, we can construct a size p + 1 ﬁlter
where the elements are βp, ..., β1 and the last entry of the ﬁlter is −1. This ﬁlter produces a zero
response for any signal generated by the AR process with coefﬁcients βp, ..., β1. We refer to this
ﬁlter as an AR ﬁlter, the utility of which is explained in Section 3.3 and Appendix A.1.

4

Figure 3: Left: Normalized samples of perturbations generated by 3 AR processes. Right: Poisoned
images and the corresponding perturbation for a randomly selected CIFAR-10 image.

Suppose we have a K class classiﬁcation problem of H × W × C dimensional images. For each
class label yi, we construct a set Ayi of AR processes, one for each of the C channels. For each
of the C channels, we will be applying an AR process from Ayi inside a V × V sliding window.
Naturally, using an AR process requires initial observations, so we populate the perturbation vector
δi with Gaussian noise for the ﬁrst V − 1 columns and rows. The V × V sliding window starts at the
top left corner of δi. Within this sliding window, we apply the AR(V 2 − 1) process: the ﬁrst V 2 − 1
entries in the sliding window are considered previously generates (or randomly initialized) entries
in the 2D array δi, and the V th entry is computed by Eq. 5. The window is slid left to right, top to
bottom until the ﬁrst channel of δi is ﬁlled. We then proceed to use the next AR(V 2 − 1) process in
Ayi for the remaining C − 1 channels. Finally, we discard the random Gaussian rows and columns
used for initialization, and scale δi to be of size (cid:15) in the (cid:96)2-norm. Note that this sliding window
procedure resembles that of a convolution. That is by design, and we explain why it is important in
Section 3.3. A high-level overview of this algorithm is illustrated in Figure 2. Additional details are
in Appendix A.3.2. While we describe our use of AR processes on C-channel images, our method
could, in principle, be applied to data other than images. Note that these AR perturbations are fast to
generate, do not require a pre-trained surrogate model, and can be generated independently from the
data.

3.3 Why do Autoregressive Perturbations Work?

Perturbations that are easy to learn have been shown to be more effective at data poisoning [27].
Intuitively, a signal that is easily interpolated by a network will be quickly identiﬁed and used as a
“shortcut,” whereas complex and unpredictable patterns may not be learned until after a network has
already extracted useful content-based features [29]. Thus, we seek imperceptible perturbations that
are easy to learn. We propose a simple hypothesis: if there exists a simple CNN that can classify
autoregressive signals perfectly, then these signals will be easy to learn. The signals can then be
applied to clean images and serve as a shortcut for learning by commonly-used CNNs.

Autoregressive perturbations, despite looking visually complex, are actually very simple. To demon-
strate their separability, we manually specify the parameters of a simple CNN that classiﬁes AR
perturbations perfectly by using AR ﬁlters. In the following, we prove AR ﬁlters satisfy an important
property.

Lemma 3.1. Given an AR perturbation δ, generated from an AR(p) with coefﬁcients β1, ..., βp, there
exists a linear, shift invariant ﬁlter where the cross-correlation operator produces a zero response.

We provide a proof in Appendix A.1. The construction of an AR ﬁlter that produces a zero response
for any noise generated from the corresponding AR process is useful because we can construct a CNN
which makes use of solely these AR ﬁlters to classify signals. That is, given any AR perturbation, the
AR ﬁlter with the zero response correctly designates the AR process from which the perturbation was
generated. We verify this claim in Appendix A.2 by specifying the 3-layer CNN that can perfectly
classify AR perturbations.

Crucially, we are not interested in learning classes of AR signals. Rather, we are interested in
how quickly a model can learn classes of clean data perturbed by AR signals. Nevertheless, the

5

characterization of our AR perturbations as easy to learn, demonstrated by the manual speciﬁcation
of a 3-layer CNN, is certainly an indication that, when applied to clean data, AR perturbations can
serve as bait for CNNs. Our experiments will seek to answer the following question: If we perturb
each sample in the training dataset with an imperceptible, yet easily learned AR perturbation, can we
induce a learning “shortcut” that minimizes the training loss but prevents generalization?

3.4 Finding AR Process Coefﬁcients

We generate AR processes using a random search that promotes diversity. We generate processes
one-at-a-time by starting with a random Gaussian vector of coefﬁcients. We then scale the coefﬁcients
so that they sum to one. We then append a −1 to the end of the coefﬁcients to produce the associated
AR ﬁlter, and convolve this ﬁlter with previously generated perturbations. We use the norms of the
resulting convolution outputs as a measure of similarity between processes. If the minimum of these
norms is below a cutoff T , then we deem the AR process too coherent with previously generated
perturbations – the coefﬁcients are discarded and we try again with a different random vector.

Once the AR process coefﬁcients are identiﬁed for a class, we use them to produce a perturbation δi
for each image in the class. This perturbation is scaled to be exactly of size (cid:15) in the (cid:96)2-norm. To level
the playing ﬁeld among all poisoning methods, we measure all perturbations using an (cid:96)2 norm in this
work. A more detailed description of this process can be found in Appendix A.3.1.

4 Experiments

We demonstrate the generality of AR poisoning by creating poisons across four datasets, including
different image sizes and number of classes. Notably, we use the same set of AR processes to poison
SVHN [22], STL-10 [6], and CIFAR-10 [20] since all of these datasets are 10 class classiﬁcation
problems. We demonstrate that despite the victim’s choice of network architecture, AR poisons can
degrade a network’s accuracy on clean test data. We show that while strong data augmentations are
an effective defense against all poisons we consider, AR poisoning is largely resistant. Adversarial
training and diluting the poison with clean data remain strong defenses, but our AR poisoning method
is competitive with other poisons we consider. All experiments follow the same general pattern: we
train a network on a poisoned dataset and then evaluate the trained network’s performance on clean
test data. A poison is effective if it can cause the trained network to have poor test accuracy on clean
data, so lower numbers are better throughout our results.

Experimental Settings. We train a number of ResNet-18 (RN-18) [14] models on different poisons
with cross-entropy loss for 100 epochs using a batch size of 128. For our optimizer, we use SGD
with momentum of 0.9 and weight decay of 5 × 10−4. We use an initial learning rate of 0.1, which
decays by a factor of 10 on epoch 50. In Table 2, we use use the same settings with different network
architectures.

4.1 Error-Max, Error-Min, and other Random Noise Poisons

SVHN [22], CIFAR-10, and CIFAR-100 [20] poisons considered in this work contain perturbations
of size (cid:15) = 1 in (cid:96)2, unless stated otherwise. For STL-10 [6], all poisons use perturbations of size
(cid:15) = 3 in (cid:96)2 due to the larger size of STL-10 images. In all cases, perturbations are normalized and
scaled to be of size (cid:15) in (cid:96)2, are additively applied to clean data, and are subsequently clamped to be in
image space. Dataset details can be found in Appendix A.4. A sampling of poison images and their
corresponding normalized perturbation can be found in Figure 3 and Appendix A.8. In our results,
class-wise poisons are marked with ◦ and sample-wise poisons are marked with •.

Error-Max and Error-Min Noise. To generate error-maximizing poisons, we use the open-source
implementation of [10]. In particular, we use a 250-step (cid:96)2 PGD attack to optimize Eq. (3). To
generate error-minimizing poisons, we use the open-source implementation of [18], where a 20-step
(cid:96)2 PGD attack is used to optimize Eq. (4). For error-minimizing poisoning, we ﬁnd that moving in (cid:96)2
normalized gradient directions is ineffective at reaching the required universal stop error [18], so we
move in signed gradient directions instead (as is done for (cid:96)∞ PGD attacks).

Regions-4 and Regions-16 Noise. Synthetic, random noises are also dataset and network indepen-
dent. Thus, to demonstrate the strength of our method, we include three class-wise random noises

6

Table 1: Dataset Independence. AR noises are effective across a variety of datasets. For SVHN,
STL-10, and CIFAR-10, we use the same 10 AR processes to generate sample-wise noises. We
display clean test accuracy of RN-18 when trained using standard augmentations on different poisons.
Class-wise poisons are marked with ◦ and sample-wise poisons are marked with •.

SVHN STL-10 CIFAR-10 CIFAR-100

Clean

• Error-Max [10]
• Error-Min [18]
◦ Regions-4
◦ Regions-16
◦ Random Noise
• Autoregressive (Ours)

96.40

80.80
96.82
9.80
6.39
9.68
6.77

83.65

17.26
80.71
41.36
31.21
72.07
11.65

93.62

26.94
16.84
20.75
15.75
18.45
11.75

75.30

4.87
74.58
9.14
2.99
73.90
4.24

Table 2: Architecture Independence. CIFAR-10 test accuracy for a variety of model architectures
trained on different poisons. Error-Max and Error-Min poisons are crafted using a RN-18.
RN-18 VGG-19 GoogLeNet MobileNet EfﬁcientNet DenseNet

ViT

• Error-Max [10]
• Error-Min [18]
◦ Regions-4
◦ Regions-16
◦ Random Noise
• AR (Ours)

16.84
26.94
20.75
15.75
18.45
11.75

20.34
22.39
25.39
19.86
81.98
12.35

15.31
32.18
25.25
12.97
12.53
9.24

15.38
21.36
22.23
19.67
46.61
15.17

16.21
23.86
18.64
16.94
86.89
13.47

18.02
28.21
24.21
24.16
9.96
14.90

41.70
40.95
35.18
25.49
74.47
19.66

in our experiments. To generate what we a call a Regions-p noise, we follow [39, 27]: we sample p
RGB vectors of size 3 from a Gaussian distribution and repeat each vector along height and width
dimensions, resulting in a grid-like pattern of p uniform cells or regions. Assuming a square image of
side length L, a Regions-p noise contains patches of size L√

p × L√
p .

Random Noise. We also consider a class-wise random noise poison, where perturbations for each
class are sampled from a Gaussian distribution.

4.2 AR Perturbations are Dataset and Architecture Independent

Unlike error-maximizing and error-minimizing poisons, AR poisons are not dataset-speciﬁc. One
cannot simply take the perturbations from an error-maximizing or error-minimizing poison and apply
the same perturbations to images of another dataset. Perturbations optimized using PGD are known
to be relevant features, necessary for classiﬁcation [10, 19]. Additionally, for both these methods,
a crafting network trained on clean data is needed to produce reasonable gradient information. In
contrast, AR perturbations are generated from dataset-independent AR processes. The same set of
AR processes can be used to generate the same kinds of noise for images of new datasets. Building
from this insight, one could potentially collect a large set of K AR processes to perturb any dataset
of K or fewer classes, further showing the generality of our method.

In Table 1, we use the same 10 AR processes to generate noise for images of SVHN, STL-10,
and CIFAR-10. AR poisons are, in all cases, either competitive or the most effective poison – a
poison-trained RN-18 reaches nearly chance accuracy on STL-10 and CIFAR-10, and being the
second-best on SVHN and CIFAR-100. The generality of AR perturbations to different kinds of
datasets suggests that AR poisoning induces the most easily learned correlation between samples and
their corresponding label.

We also evaluate the effectiveness of our AR poisons when different architectures are used for training.
Recall that error-maximizing and error-minimizing poisoning use a crafting network to optimize
the input perturbations. Because it may be possible that these noises are speciﬁc to the network
architecture, we perform an evaluation of test set accuracy on CIFAR-10 after poison training VGG-19
[32], GoogLeNet [33], MobileNet [16], EfﬁcientNet [34], DenseNet [17], and ViT [8]. Our ViT uses
a patch size of 4. In Table 2, we show that Error-Max and Error-Min poisons generalize relatively

7

Table 3: Strong Data Augmentations. CIFAR-10 test accuracy of RN-18 when training using
standard augmentations plus Cutout, CutMix, or Mixup on different poisons, where clean images are
perturbed within an (cid:96)2 ball of size (cid:15).

Standard Aug

+Cutout

+CutMix

+Mixup

• Error-Max [10]
• Error-Min [18]
◦ Regions-4
◦ Regions-16
◦ Random Noise
• Autoregressive (Ours)

• Error-Max [10]
• Error-Min [18]
◦ Regions-4
◦ Regions-16
◦ Random Noise
• Autoregressive (Ours)

(cid:15) = 0.5

(cid:15) = 1

25.04
48.07
57.48
47.35
93.76
14.28

16.84
26.94
20.75
15.75
18.45
11.75

25.38
44.97
67.47
39.64
93.66
12.36

18.86
29.38
21.89
19.61
12.61
11.90

29.72
53.77
67.72
32.67
91.80
18.02

21.45
25.04
28.61
16.67
12.63
11.23

38.07
53.81
66.80
49.02
94.16
14.59

30.52
43.36
40.60
20.81
23.64
11.40

well across a range of related CNNs, but struggle with ViT, which is a transformer architecture. In
contrast, our AR poison is effective across all CNN architectures and is the most effective poison
against ViT. Our AR poison is much more effective over other poisons in almost all cases, achieving
improvements over the next best poison of 4% on RN-18, 5.8% on ViT, and 7.5% on GoogLeNet.
The design of AR perturbations is meant to target the convolution operation, so it is surprising to see
a transformer network be adversely affected. We believe our AR poison is particularly effective on
GoogLeNet due to the presence of Inception modules that incorporate convolutions using various
ﬁlter sizes. While our AR perturbations are generated using a 3 × 3 window, the use of various ﬁlter
sizes may exaggerate their separability, as described in Section 3.3.

4.3 AR Perturbations Against Common Defenses

4.3.1 Data Augmentations and Smaller Perturbations

Our poisoning method relies on imperceptible AR perturbations, so it is conceivable that one could
modify the data to prevent the learning of these perturbations. One way of modifying data is by using
data augmentation strategies during training. In addition to standard augmentations like random crops
and horizontal ﬂips, we benchmark our AR poison against stronger augmentations like Cutout [7],
CutMix [41], and Mixup [42] in Table 3. Generally, Mixup seems to be the most effective at disabling
poisons. A RN-18 poison-trained using standard augmentations plus Mixup can achieve a boosts in
test set performance of 13.68% on Error-Max, 16.42% on Error-Min, 19.85% on Regions-4, 5.05%
on Regions-16, and 5.19% on Random Noise. However, a RN-18 poison-trained on our AR poison
((cid:15) = 1) using standard augmentations plus Cutout, CutMix, or Mixup cannot achieve any boost in
test set performance.

We also present results for poisons using perturbations of size (cid:15) = 0.5 to explore just how small
perturbations can be made while still maintaining poisoning performance. Under standard augmenta-
tions, going from larger to smaller perturbations ((cid:15) = 1 to (cid:15) = 0.5), poison effectiveness drops by
8.2% for Error-Max, 21.13% for Error-Min, 36.73% for Regions-4, and 31.6% for Regions-16. Our
AR poison achieves the smallest drop in effectiveness: only 2.53%. Random noise can no longer be
considered a poison at (cid:15) = 0.5 – it completely breaks for small perturbations. Under all strong data
augmentation strategies at (cid:15) = 0.5, AR poisoning dominates. For example, under Mixup, the best
runner-up poison is Error-Max with an effectiveness that is more than 23% lower than AR. Unlike all
other poisons, AR poisoning is exceptionally effective for small perturbations.

Note that in all three augmentation strategies pixels are either dropped or scaled. Our method is
unaffected by these augmentation strategies, unlike error-maximizing, error-minimizing, and other
randomly noise poisons. Scaling an AR perturbation does not affect how the corresponding matching
AR ﬁlter will respond,2 and thus, the patterns remain highly separable regardless of perturbation size.

2See condition outlined in Lemma 3.1.

8

Table 4: Adversarial Training. CIFAR-10 test accuracy after adversarially training with different
radii ρa. Top row shows performance of adversarial training on clean data. AR poisons remain
effective for small ρa.

Clean Data

0.125

87.07

ρa

0.25

84.75

0.50

81.19

0.75

77.01

• Error-Max [10]
• Error-Min [18]
◦ Regions-4
◦ Regions-16
◦ Random Noise
• Autoregressive (Ours)

33.30±0.14
70.66±0.41
75.05±0.35
47.99±0.25
86.31±0.42
33.22±0.77

72.27±2.18
84.80±2.38
81.23±0.11
71.43±0.17
84.17±0.20
57.08±0.75

81.15±3.58
83.04±3.24
79.71±0.05
80.47±0.10
80.11±0.06
81.27±2.61

78.73±4.20
79.11±3.46
76.47±0.34
76.65±0.07
76.26±0.07
79.07±3.47

Table 5: Mixing Poisons with Clean Data. CIFAR-10 test accuracy when a proportion of clean
data is used in addition to a poison. Top row shows test accuracy when training on only the clean
proportion of the data; i.e. no poisoned data is used.

40%

90.84

30%

89.92

87.83±0.74
88.32±1.57
88.94±0.85
88.03±0.57
86.40±1.24
87.63±0.68

86.83±0.48
87.23±0.84
86.75±0.86
86.23±0.68
86.99±0.19
85.62±0.62

Clean Proportion
20%

87.90

84.70±0.61
84.56±0.88
83.52±0.20
83.01±0.48
84.98±1.85
83.28±0.90

10%

81.01

5%

74.97

81.63±0.63
78.76±1.83
78.23±0.97
76.52±0.91
78.08±0.94
76.13±2.34

76.48±1.72
67.82±1.92
70.19±3.16
67.24±1.72
70.69±0.87
62.69±5.58

Clean Only

• Error-Max [18]
• Error-Min [10]
◦ Regions-4
◦ Regions-16
◦ Random Noise
• AR (Ours)

Additionally, AR ﬁlters contain values which sum to 0, so uniform regions of an image also produce
a zero response.

4.3.2 Adversarial Training

Adversarial training has also been shown to be an effective counter strategy against (cid:96)p-norm con-
strained data poisons [18, 10, 9, 35]. Using adversarial training, a model trained on the poisoned data
can achieve nearly the same performance as training on clean data [30]. However, adversarial training
is computationally more expensive than standard training and leads to a decrease in test accuracy
[21, 36] when the perturbation radius, ρa, of the adversary is large. In Table 4, we include adversarial
training results on clean data to outline this trade-off where training at large ρa comes at the cost
of test accuracy. A recent line of work has therefore focused on developing better data poisoning
methods that are robust against adversarial training [30, 37] at larger adversarial training radius ρa.

In Table 4, we compare performance of different poisons against adversarial training. We perform (cid:96)2
adversarial training with different perturbation radii, ρa, using a 7-step PGD attack with a step-size
of ρa/4. We report error-bars by training three independent models for each run. We also show the
performance of adversarial training on clean data. Data poisoning methods are fragile to adversarial
training even when the training radius ρa is smaller than poisoning radius (cid:15) [30, 37]. It is desirable
for poisons to remain effective for larger ρa, because the trade-off between standard test accuracy
and robust test accuracy would be exaggerated further. As shown in the Table 4, when the adversarial
training radius ρa increases, the poisons are gradually rendered ineffective. All poisons are nearly
ineffective at ρa = 0.5. Our proposed AR perturbations remain more effective at smaller radius, i.e.
ρa = 0.125 and ρa = 0.25 compared to all other poisons.

4.3.3 Mixing Poisons with Clean Data

Consider the scenario when not all the data can be poisoned. This setup is practical because, to a
practitioner coming into control of poisoned data, additional clean data may be available through
other sources. Therefore, it is common to evaluate poisoning performance using smaller proportions

9

of randomly selected poison training samples [10, 18, 30]. A poison can be considered effective
if the addition of poisoned data hurts test accuracy compared to training on only the clean data. In
Table 5, we evaluate the effectiveness of poisons using different proportions of clean and poisoned
data. The top row of Table 5 shows test accuracy after training on only the subset of clean data, with
no poisoned data present. We report error-bars by training four independent models for each run.
Our AR poisons remain effective compared to other poisons even when clean data is mixed in. AR
poisons are much more effective when a small portion of the data is clean. For example, when 5%
of data is clean, a model achieves ~75% accuracy when training on only the clean proportion, but
using an additional 95% of AR data leads to a ~9% decrease in test set generalization. Our results
on clean data demonstrate that AR poisoned data is worse than useless for training a network, and
a practitioner with access to the data would be better off not using it.

5 Conclusion

Using the intuition that simple noises are easily learned, we proposed the design of AR perturbations,
noises that are so simple they can be perfectly classiﬁed by a 3-layer CNN where all parameters are
manually-speciﬁed. We demonstrate that these AR perturbations are immediately useful and make
effective poisons for the purpose of preventing a network from generalizing to the clean test distribu-
tion. Unlike other effective poisoning techniques that optimize error-maximizing or error-minimizing
noises, AR poisoning does not need access to a broader dataset or surrogate network parameters. We
are able to use the same set of 10 AR processes to generate imperceptible noises able to degrade the
test performance of networks trained on three different 10 class datasets. Unlike randomly generated
poisons, AR poisons are more potent when training using a new network architecture or strong
data augmentations like Cutout, CutMix, and Mixup. Against defenses like adversarial training, AR
poisoning is competitive or among the best for a range of attack radii. Finally, we demonstrated
that AR poisoned data is worse than useless when it is mixed with clean data, reducing likelihood
that a practitioner would want to include AR poisoned data in their training dataset.

Acknowledgments and Disclosure of Funding

This material is based upon work supported by the National Science Foundation under Grant No.
IIS-1910132 and Grant No. IIS-2212182, and by DARPA’s Guaranteeing AI Robustness Against
Deception (GARD) program under #HR00112020007. Pedro is supported by an Amazon Lab126 Di-
versity in Robotics and AI Fellowship. Any opinions, ﬁndings, and conclusions or recommendations
expressed in this material are those of the author(s) and do not necessarily reﬂect the views of the
National Science Foundation.

References

[1] Marco Barreno, Blaine Nelson, Anthony D Joseph, and J Doug Tygar. The security of machine learning.

Machine Learning, 81(2):121–148, 2010.

[2] Battista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks against support vector machines.

arXiv preprint arXiv:1206.6389, 2012.

[3] Abeba Birhane and Vinay Uday Prabhu. Large image datasets: A pyrrhic win for computer vision? In 2021
IEEE Winter Conference on Applications of Computer Vision (WACV), pages 1536–1546. IEEE, 2021.

[4] George EP Box, Gwilym M Jenkins, Gregory C Reinsel, and Greta M Ljung. Time series analysis:

forecasting and control. John Wiley & Sons, 2015.

[5] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep

learning systems using data poisoning. arXiv preprint arXiv:1712.05526, 2017.

[6] Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature
learning. In Proceedings of the fourteenth international conference on artiﬁcial intelligence and statistics,
pages 215–223. JMLR Workshop and Conference Proceedings, 2011.

[7] Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with

cutout. arXiv preprint arXiv:1708.04552, 2017.

[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth
16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.

10

[9] Liam Fowl, Ping-yeh Chiang, Micah Goldblum, Jonas Geiping, Arpit Bansal, Wojtek Czaja, and Tom
Goldstein. Preventing unauthorized use of proprietary data: Poisoning for secure dataset release. arXiv
preprint arXiv:2103.02683, 2021.

[10] Liam Fowl, Micah Goldblum, Ping-yeh Chiang, Jonas Geiping, Wojciech Czaja, and Tom Goldstein.
Adversarial examples make strong poisons. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and
J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages
30339–30351. Curran Associates, Inc., 2021.

[11] Jonas Geiping, Liam H Fowl, W. Ronny Huang, Wojciech Czaja, Gavin Taylor, Michael Moeller, and
Tom Goldstein. Witches’ brew: Industrial scale data poisoning via gradient matching. In International
Conference on Learning Representations, 2021.

[12] Micah Goldblum, Dimitris Tsipras, Chulin Xie, Xinyun Chen, Avi Schwarzschild, Dawn Song, Aleksander
Madry, Bo Li, and Tom Goldstein. Dataset security for machine learning: Data poisoning, backdoor
attacks, and defenses. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.

[13] Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Evaluating backdooring

attacks on deep neural networks. IEEE Access, 7:47230–47244, 2019.

[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.

[15] Kashmir Hill. The secretive company that might end privacy as we know it, Jan 2020.

[16] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco
Andreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolutional neural networks for mobile vision
applications. arXiv preprint arXiv:1704.04861, 2017.

[17] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 4700–4708, 2017.

[18] Hanxun Huang, Xingjun Ma, Sarah Monazam Erfani, James Bailey, and Yisen Wang. Unlearnable
examples: Making personal data unexploitable. In International Conference on Learning Representations,
2021.

[19] Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander Madry.
Adversarial examples are not bugs, they are features. Advances in neural information processing systems,
32, 2019.

[20] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.

[21] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. To-
wards deep learning models resistant to adversarial attacks. In International Conference on Learning
Representations, 2018.

[22] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits

in natural images with unsupervised feature learning. 2011.

[23] Weiqi Peng and Jinghui Chen. Learnability lock: Authorized learnability control through adversarial

invertible transformations. In International Conference on Learning Representations, 2022.

[24] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from
natural language supervision. In International Conference on Machine Learning, pages 8748–8763. PMLR,
2021.

[25] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional

image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.

[26] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and
Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning,
pages 8821–8831. PMLR, 2021.

[27] Pedro Sandoval-Segura, Vasu Singla, Liam Fowl, Jonas Geiping, Micah Goldblum, David Jacobs, and Tom
Goldstein. Poisons that are learned faster are more effective. arXiv preprint arXiv:2204.08615, 2022.

[28] Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, and
Tom Goldstein. Poison frogs! targeted clean-label poisoning attacks on neural networks. Advances in
neural information processing systems, 31, 2018.

[29] Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, and Praneeth Netrapalli. The pitfalls of
simplicity bias in neural networks. Advances in Neural Information Processing Systems, 33:9573–9585,
2020.

11

[30] Fu Shaopeng, He Fengxiang, Liu Yang, Shen Li, and Tao Dacheng. Robust unlearnable examples: Protect-
ing data privacy against adversarial learning. In International Conference on Learning Representations,
2022.

[31] Juncheng Shen, Xiaolei Zhu, and De Ma. Tensorclog: An imperceptible poisoning attack on deep neural

network applications. IEEE Access, 7:41498–41506, 2019.

[32] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-

tion. arXiv preprint arXiv:1409.1556, 2014.

[33] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru
Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of
the IEEE conference on computer vision and pattern recognition, pages 1–9, 2015.

[34] Mingxing Tan and Quoc Le. Efﬁcientnet: Rethinking model scaling for convolutional neural networks. In

International conference on machine learning, pages 6105–6114. PMLR, 2019.

[35] Lue Tao, Lei Feng, Jinfeng Yi, Sheng-Jun Huang, and Songcan Chen. Better safe than sorry: Preventing
delusive adversaries with adversarial training. Advances in Neural Information Processing Systems, 34,
2021.

[36] Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. Robust-
ness may be at odds with accuracy. In International Conference on Learning Representations, 2019.

[37] Zhirui Wang, Yifei Wang, and Yisen Wang. Fooling adversarial training with inducing noise. arXiv

preprint arXiv:2111.10130, 2021.

[38] Herman Wold. A study in the analysis of stationary time series. PhD thesis, Almqvist & Wiksell, 1938.

[39] Da Yu, Huishuai Zhang, Wei Chen, Jian Yin, and Tie-Yan Liu. Indiscriminate poisoning attacks are

shortcuts, 2022.

[40] Chia-Hung Yuan and Shan-Hung Wu. Neural tangent generalization attacks. In Marina Meila and Tong
Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of
Proceedings of Machine Learning Research, pages 12230–12240. PMLR, 18–24 Jul 2021.

[41] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.
Cutmix: Regularization strategy to train strong classiﬁers with localizable features. In Proceedings of the
IEEE/CVF international conference on computer vision, pages 6023–6032, 2019.

[42] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk

minimization. arXiv preprint arXiv:1710.09412, 2017.

[43] Chen Zhu, W Ronny Huang, Hengduo Li, Gavin Taylor, Christoph Studer, and Tom Goldstein. Transferable
clean-label poisoning attacks on deep neural nets. In International Conference on Machine Learning,
pages 7614–7623. PMLR, 2019.

12

A Appendix

A.1 Proof of Lemma 3.1

Lemma A.1. Given an AR perturbation δ, generated from an AR(p) with coefﬁcients β1, ..., βp, there
exists a linear, shift invariant ﬁlter where the cross-correlation operator produces a zero response.

Proof. Consider any size p + 1 window of the AR perturbation δ where, by deﬁnition, the last entry
follows Eq. (5) directly: [xt−p, xt−(p−1), ..., xt−1, xt]. We construct a size p + 1 ﬁlter where the
elements are βp, ..., β1 and the last entry of the ﬁlter is −1. We call this ﬁlter an AR ﬁlter. Notice
that the dot product of a window of δ with this ﬁlter is:

[xt−p, xt−(p−1), ..., xt−1, xt] · [βp, βp−1, ..., β1, −1]
= βpxt−p + βp−1xt−(p−1) + ... + β1xt−1 − xt
= βpxt−p + βp−1xt−(p−1) + ... + β1xt−1 − (β1xt−1 + ... + βp−1xt−(p−1) + βpxt−p)
= 0

which implies:

δ (cid:63) [β1, ..., βp, −1] = (cid:126)0

where (cid:63) denotes the cross-correlation operator.

A.2 Manually-speciﬁed CNN with AR ﬁlters

(6)

(7)

(8)

(9)

Figure 4: A 3-layer CNN can perfectly classify AR perturbations without any training. We specify
each parameter manually.

Assume AR perturbations are expected to be of size 32 × 32, as in CIFAR-10. To illustrate the ease
with which a set of AR perturbations can be classiﬁed, we construct a set of 10 AR(8) processes
A = (cid:126)β1, ..., (cid:126)β10, where (cid:126)βi = [βi,1, ..., βi,8] is a vector of AR(8) coefﬁcients. Our network consists
of 3 layers: a convolutional layer, a max-pool layer, and a fully connected layer. We specify the
parameters of each layer below:

1. Convolutional Layer: 10 AR ﬁlters of size 3 × 3. No bias vector. ReLU activation on output.
The ith AR ﬁlter is meant to produce a zero response (See A.1) on noise generated from AR
process with coefﬁcients (cid:126)βi:

fi =

(cid:35)

(cid:34)βi,8 βi,7 βi,6
βi,5 βi,4 βi,3
βi,2 βi,1 −1

2. Max-Pool layer: Kernel size of 30 × 30, the same dimensions as the output of the convolu-

tional layer.

3. Linear layer: We set W = −I and b = (cid:126)1, where (cid:126)1 is a vector of all ones. Softmax activation

on output.

13

To classify AR perturbation from 10 classes, this network outputs a probability distribution over 10
classes. This simple CNN works by assigning a high value to the ith logit when the ith AR ﬁlter
produces a zero response. For example, when an AR ﬁlter outputs a zero response, the activation
after ReLU and Max-Pool is still 0. The linear layer will then add a bias of 1, so the resulting ith logit
will be 1. Other logits correspond to AR ﬁlters which do not match the AR process, and output some
nonnegative value, after ReLU and Max-Pool. The subsequent linear layer subtracts the nonnegative
activation from 1, resulting in a logit value that is smaller than 1. In this way, the softmax activation
assigns a high probability to the ith class.

To conﬁrm our network design, we generate 5, 000 AR perturbations per AR process, and use the
network deﬁned above to classify the noises. The manually-speciﬁed network achieves perfect
accuracy without any training. We use this result as a demonstration that AR perturbations are
separable. Note that for other simple, linearly separable image datasets it is non-trivial to manually
specify a CNN for perfect accuracy. Again, our motivation in exploring easily classiﬁed noises is that
there is substantial work suggesting dataset separability is key to crafting good poisons.

A.3 Algorithms for Generating AR perturbation and Finding Coefﬁcients

Algorithm 1 Random Search for AR Coefﬁ-
cients
Input: Number of classes, K
Input: Minimum response threshold, T
Output: Set of AR process coefﬁcients, A
1: A ← {}
2: while |A| (cid:54)= K do
(cid:126)β ← N (0, 1)
3:
(cid:126)β ←
δ ← ARGenerate((cid:126)β)
if δ is stable then

(cid:126)β
j βj

4:

(cid:80)

Let m store the min response so far.
for i in 1,...,|A| do

f ← ARFilter(Ai)
r ← ConvResponse(δ, f )
m ← min(m, r)

end for
if m ≥ T then

A ← A ∪ {βi}

end if

5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
end if
16:
17: end while
18: return A

Algorithm 2 Generate C-channel AR Perturba-
tion
Input: Image and label, (xi, yi), where xi ∈
RH×W ×C and yi ∈ {1, ..., K}
Input: Size of sliding window, V
Input: Set of AR(V 2 − 1) processes, A
Input: Size of perturturbation (cid:15) in (cid:96)2
Output: Poisoned image, x(cid:48)
i
1: Sample N (0, 1) for ﬁrst V − 1 rows and

β1, ..., βV 2−1 ← Ak
yi
δk
i ← ARGenerate(β1, ..., βV 2−1)

columns of δi
2: for k in 1, ..., C do
3:
4:
5: end for
6: δi ← (cid:15) δi
(cid:107)δi(cid:107)2
7: x(cid:48)
8: return x(cid:48)
i

i ← xi + δi

We deﬁne important functions and variables of Algorithm 1 and Algorithm 2:

• (cid:126)β is a vector where the entries are the AR process coefﬁcients β1, ..., βp.

• To check whether δ is stable, as in Algorithm 1 Line 6, we check whether the AR process
diverges under a few starting signals. To do this, we check that the (cid:96)2-norm of δ is not large,
inﬁnity, or NaN, under 3 different Gaussian starting signals. If the AR process diverges on
any starting signal, then we say δ is not stable, and the Algorithm 1 continues by sampling a
new set of AR process coefﬁcients.

• ARGenerate((cid:126)β) takes a set of AR process coefﬁcients and uses the AR process within a
sliding window which goes left to right, top to bottom. The ﬁrst V − 1 rows and columns of
δi are sampled from a Gaussian. The output of ARGenerate((cid:126)β) is a two-dimensional array.
The procedure is described in Section 3.2 and illustrated in Figure 2. Algorithm 2 produces
the entire C-channel AR perturbation.

14

• ARFilter(Ai) takes a set of AR process coefﬁcients and returns the corresponding AR ﬁlter,
where the entries are the AR coefﬁcients followed by a −1. The AR ﬁlter is described in
Section 3.2 and used in the construction of our simple AR noise classiﬁer in Appendix A.2.

• ConvResponse(δ, f ) takes a signal and ﬁlter as input. It measures a ﬁlter f ’s “response” on

δ by performing 2D convolution, ReLU, and sum.

A.3.1 Additional Details: Finding AR Process Coefﬁcients

While there exist conditions for AR coefﬁcients for which the AR process converges [38, 4], we
found that randomly searching for coefﬁcients was the simplest way to collect a large number of
suitable AR processes. Additionally, using convergence conditions leads to AR processes which
converge too quickly. For our purposes, converging too quickly is not desirable because most of the
values in our AR perturbation would be zero.

Instead, we conduct a random search for K sets of converging AR coefﬁcients by randomly sampling
values from a Gaussian distribution and ensuring the resulting process is stable; i.e. does not diverge.
We ﬁnd that the likelihood of a convergent AR process is much higher when the coefﬁcients are
normalized to sum to 1. Normalizing the coefﬁcients (Algorithm 1 Line 4) is also important because
we want AR ﬁlters to produce a zero response to uniform regions of an image. If AR coefﬁcients
sum to 1 and the ﬁnal entry of the AR ﬁlter is −1, then the 2D cross-correlation operator produces a
zero response if pixels in a local window are approximately equal.

We optimize so that every AR process is sufﬁciently different from other processes already included
in our growing set. We do this by measuring the response of AR ﬁlters from AR processes already in
our set to noise generated by the currently sampled AR process. It may be possible to optimize a
different criteria, but in our implementation, ConvResponse(δ, f ) measures a ﬁlter’s response by
performing 2D convolution, ReLU, and sum. We fully outline this random search for K sets of AR
process coefﬁcients in Algorithm 1.

We conduct a search for K = 10 sets of AR coefﬁcients using Algorithm 1 with a threshold T = 10.
This search took roughly 11 hours running on a single CPU. For CIFAR-100, we conduct a search for
K = 100 sets of AR coefﬁcients with T = 3 due to the larger number of classes. This search took
roughly 4 hours running on a single GPU. Because Algorithm 1 samples coefﬁcients independently
on every iteration, signiﬁcant speedups can likely be achieved using parallelized code.

A.3.2 Additional Details: Generating AR Perturbations

In addition to normalizing and scaling AR perturbation after generation, we also ﬁnd that cropping
out the ﬁrst few columns and rows, where the Gaussian start signal was generated (see Figure 2),
leads to more effective AR perturbations. We tried cropping only the Gaussian start signal, and
cropping the start signal plus additional columns and rows. We found that cropping the start signal
plus two extra columns and rows worked best. The AR perturbations are also less perceptible this
way, given that the Gaussian noise starting signal has been removed. For 32 × 32 dimensional images,
we generate AR perturbations of size 36 × 36 and crop the ﬁrst 4 rows and columns. For 96 × 96
dimensional images, we generate AR perturbations of size 100 × 100 and crop the ﬁrst 4 rows and
columns.

A.4 Dataset Details

SVHN [22] contains 10 classes, where 73257 images are for training and 26032 images are for
testing. STL-10 [6] conatins 10 classes, where 5000 are for training and 8000 are for testing. Both
CIFAR-10 and CIFAR-100 [20] contain 50000 images for training and 10000 images for testing.
Images of CIFAR-10 belong to one of 10, whereas images from CIFAR-100 belong to one of 100
classes. SVHN, CIFAR-10, and CIFAR-100 contain images of size 32 × 32 × 3; STL-10 contains
images of size 96 × 96 × 3.

A.5 Computing Resources

To run all of our experiments, we use an internal cluster. Jobs were run on a maximum of 4 NVIDIA
GTX 1080Ti.

15

Figure 5: A closeup of a CIFAR-10 image from our AR poison. From left to right, we display
the clean image, the normalized perturbation, and the perturbed image. Looking closely at the
background of the horse, one can make out the checkered blue pattern of this AR perturbation.

A.6 AR Poisoning in (cid:96)∞

Autoregressive perturbations can be projected onto any (cid:96)p-norm ball, including (cid:96)∞. We initially
measured the (cid:96)2-norm because AR perturbations may have single entries which are high and violate a
strict (cid:96)∞ constraint. To demonstrate that AR poisoning can work in the (cid:96)∞-norm constrained setting,
we train a RN-18 on an AR poison with perturbations satisfying (cid:107)δ(cid:107)∞ = 8
255 , and report CIFAR-10
test accuracy in Table 6.

Table 6: Poisoining in (cid:96)∞. CIFAR-10 test accuracy of an RN-18 trained on an AR poison using
standard augmentations plus Cutout, CutMix, or Mixup.

Autoregressive (Ours)

20.49

26.93

17.08

15.22

Standard Aug

+Cutout

+CutMix

+Mixup

Importantly, how one ﬁts an AR perturbation δ within the constraint that (cid:107)δ(cid:107)∞ ≤ (cid:15) = 8
255 affects
performance. In this experiment, we simply scale δ by
, which may be suboptimal. Clipping
values or taking the scaled sign of δ would make the perturbation no longer autoregressive. AR
perturbations could likely be optimized to perform better in (cid:96)∞ by making modiﬁcations to how AR
coefﬁcients are compared in Algorithm 1.

(cid:15)
(cid:107)δ(cid:107)∞

A.7 Effect of Optimizer Hyperparameters

To evaluate the effect of optimizer and learning rate on our poisoning results, we train an RN-18 on
poisons using different optimizers. We consider 3 optimizers: SGD, SGD+Momentum (β = 0.9)
and Adam (β1 = 0.9, β2 = 0.999). We also consider 3 different learning-rate schedules: cosine
learning-rate, single-step decay (at epoch 50) and multi-step decay (at epochs 50 and 75) with decay
factor of 0.1. There are a total of 9 optimizer and learning rate combinations.

Table 7: Effect of Optimizer and LR. Mean CIFAR-10 test accuracy of an RN-18 trained on our
AR poison, over 9 optimizer and LR combinations.

Autoregressive (Ours) Random Noise Regions-4 Regions-16 Error-Max.

Error-Min

12.23±1.22

33.39±31.24

22.89±7.6

18.73±5.52

16.6±2.8

22.96±7.16

In Table 7, we report mean CIFAR-10 test accuracy over the 9 combinations of optimizers and learning
rates. Our Autoregressive poison remains nearly unaffected by the choice of hyperparameters, and
performs better than all other poisons.

16

A.8 Samples of CIFAR-10 Poisons

We plot a random sample of 30 poison images from each CIFAR-10 poison used in this work. In each
plot, we also illustrate the corresponding normalized perturbation for each image. The Error-Max
poison is shown in Figure 6, Error-Min poison in Figure 7, Regions-4 poison in Figure 8, Regions-16
poison in Figure 9, Random Noise poison in Figure 10, and our AR poison is shown in Figure 11.

Figure 6: Error-Max Poison. A random sample of 30 poison images and their corresponding
normalized perturbation.

Figure 7: Error-Min Poison. A random sample of 30 poison images and their corresponding
normalized perturbation.

17

Figure 8: Regions-4 Poison.. A random sample of 30 poison images and their corresponding
normalized perturbation.

Figure 9: Regions-16 Poison. A random sample of 30 poison images and their corresponding
normalized perturbation.

18

Figure 10: Random Noise Poison. A random sample of 30 poison images and their corresponding
normalized perturbation.

Figure 11: AR Poison (Ours). A random sample of 30 poison images and their corresponding
normalized perturbation.

19

Figure 12: t-SNE plots of perturbations from our Error-Max, Error-Min, and AR CIFAR-10 poisons.

A.9

t-SNE of Poisons

In Figure 12, we use t-SNE to plot the perturbation vectors of Error-Max, Error-Min, and Autoregres-
sive (AR) poisons. For every poison CIFAR-10 image, we subtract the corresponding clean image
and plot only the perturbation vectors using t-SNE. We ﬁnd that the clustering of error-minimizing
perturbations appears to be more separable than that of AR.

A.10

10 effective AR Processes

Figure 13: Samples of normalized AR perturbations. We generate three independent samples per AR
process to illustrate how intraclass noises are unique, yet similar. These 10 processes were used to
construct poisons for SVHN, CIFAR-10, and STL-10.

On the ﬁnal pages, we provide coefﬁcients for the 10 effective AR(8) processes which are used to
generate the AR poisons for SVHN, CIFAR-10, and STL-10. Samples of perturbations from these 10
AR processes are shown in Figure 13. Because we apply a different AR process for each channel in
3-channel images, there are really 30 AR(8) processes (each 3 × 3 array deﬁnes an AR(8) process).
Each 3 × 3 × 3 dimensional array deﬁnes the three AR processes needed to perturb a 3-dimensional
image from a class. We separate each class’ AR process with a newline. Note that each 3 × 3 array
has a zero in the ﬁnal entry because there are only 8 AR coefﬁcients when operating in a 3 × 3 sliding
window.

A.11 Code

Code for generating AR poisons and training models is available at https://github.com/
psandovalsegura/autoregressive-poisoning. We provide documentation as well as demo
Jupyter notebooks for generating perturbations using AR processes. AR coefﬁcients from Ap-
pendix A.10 and full poisoned AR datasets used in this work are also available for download.

20

A.12 Limitations

There are a number of modiﬁcations which could potentially make for more effective AR perturbations,
the exploration of which we leave to future work. For example, the size of the sliding window or the
order of the AR process could potentially lead to perturbation values which converge more slowly. A
higher order AR process could also be applied along the channel dimension.

As described in Section 3.2, poisoning a K-class classiﬁcation dataset requires K AR processes. The
random search for AR coefﬁcients is certainly an area of improvement; there may exist more efﬁcient
ways of ﬁnding effective AR coefﬁcients. Nevertheless, having found a set of K AR processes,
Table 1 is evidence that they can be reused for any other classiﬁcation dataset with K or fewer classes.

A.13 Broader Impact Statement

While data poisoning could be used as an attack, the research community has actively been interested
in its use for privacy protection too. Our work has the potential to protect private data or to allow for
the secure release of datasets. The likelihood that an adversary is able to perturb an entire dataset for
the purpose of preventing generalization to a test set is miniscule. On the other hand, the likelihood
that a private entity might want methods that allow them to release a dataset securely is more likely.
Our work is motivated by understanding the kinds of features that deep neural networks choose to
learn.

21

0 . 3 7 4 3 ] ,

0 . 6 0 7 5 ] ,
0 . 0 0 0 0 ] ] ,
0 . 0 4 7 2 ] ,

0 . 0 4 6 1 ,
0 . 0 2 2 6 ,

[ [ [ [ 0 . 1 5 6 1 , − 0 . 0 7 1 0 ,
[ − 0 . 1 8 9 6 ,
[ 0 . 0 5 3 9 ,
[ [ − 0 . 1 0 1 6 ,
[ 0 . 1 4 0 1 ,
[ 0 . 1 7 4 2 ,
[ [ − 0 . 1 1 0 0 ,
[ 0 . 2 6 6 2 , − 0 . 1 1 8 5 ,
[ 0 . 1 8 1 2 ,

0 . 1 5 6 1 ,
0 . 2 4 7 6 ,

0 . 2 1 9 3 ,

0 . 1 1 7 1 ] ,
0 . 0 0 0 0 ] ] ,
0 . 2 7 0 3 , − 0 . 0 0 2 6 ] ,

0 . 0 8 4 6 ] ,

0 . 4 2 8 7 , − 0 . 0 0 0 0 ] ] ] ,

0 . 2 0 5 6 , − 0 . 0 4 8 0 ] ,

0 . 1 3 2 6 ] ,
0 . 0 0 0 0 ] ] ,
0 . 3 9 9 7 ] ,

[ [ [ 0 . 2 3 4 6 ,
[ 0 . 4 1 1 0 ,
0 . 7 5 0 4 ,
[ − 0 . 1 0 4 4 , − 0 . 5 8 1 7 ,
[ [ − 0 . 0 3 0 8 , − 0 . 1 0 8 5 ,
[ 0 . 2 1 8 7 ,
[ − 0 . 1 0 1 7 ,
[ [ 0 . 1 2 4 6 ,
[ 0 . 2 9 8 5 ,
0 . 1 3 4 6 ,
[ 0 . 3 8 0 5 , − 0 . 2 7 1 6 ,

0 . 3 3 8 9 ] ,

0 . 1 8 3 0 ,
0 . 1 0 0 8 , − 0 . 0 0 0 0 ] ] ,
0 . 1 6 6 7 , − 0 . 1 5 1 8 ] ,

0 . 3 1 8 5 ] ,
0 . 0 0 0 0 ] ] ] ,

0 . 5 5 0 1 ,

0 . 0 3 4 4 ] ,

0 . 2 2 3 9 ] ,
0 . 0 0 0 0 ] ] ,
0 . 1 1 2 9 ] ,

[ [ [ 0 . 0 9 5 1 ,
[ − 0 . 3 4 3 1 ,
0 . 0 7 6 7 ,
[ 0 . 4 6 0 2 , − 0 . 0 9 7 2 ,
[ [ 0 . 1 7 1 4 ,
[ 0 . 3 0 3 2 ,
0 . 2 9 5 9 ,
[ 0 . 2 2 0 7 , − 0 . 3 0 2 1 ,
[ [ 0 . 2 7 2 0 , − 0 . 3 4 1 7 ,
[ 0 . 2 4 9 9 ,
0 . 3 6 9 0 ,
[ − 0 . 0 2 8 2 , − 0 . 1 1 5 8 , − 0 . 0 0 0 0 ] ] ] ,

0 . 0 8 6 1 ] ,
0 . 0 0 0 0 ] ] ,
0 . 2 1 1 5 ] ,

0 . 3 8 3 3 ] ,

0 . 1 1 2 1 ,

0 . 4 0 9 1 ] ,

[ [ [ − 0 . 4 2 4 1 , − 0 . 2 6 9 4 ,
[ 0 . 3 9 9 8 ,
[ 0 . 2 7 0 0 ,
[ [ 0 . 1 2 4 6 ,
[ 0 . 0 5 3 8 ,
0 . 1 9 9 7 ,
[ 0 . 0 3 7 2 , − 0 . 2 5 7 0 ,
[ [ 0 . 2 0 3 8 ,
[ 0 . 3 4 6 0 , − 0 . 6 4 3 9 ,
[ − 0 . 2 8 2 6 ,

0 . 2 6 8 3 ] ,

0 . 1 5 7 2 ,
0 . 1 8 9 2 , − 0 . 0 0 0 0 ] ] ,
0 . 2 1 1 0 ] ,

0 . 3 0 2 4 ,

0 . 3 2 8 3 ] ,
0 . 0 0 0 0 ] ] ,
0 . 3 9 7 2 , − 0 . 1 9 6 3 ] ,

0 . 4 6 0 5 , − 0 . 0 0 0 0 ] ] ] ,

0 . 7 1 5 3 ] ,

0 . 2 2 6 1 , − 0 . 2 7 0 4 ] ,
0 . 4 5 8 1 , − 0 . 0 0 0 0 ] ] ,
0 . 2 4 8 8 ] ,

[ [ [ 0 . 7 8 7 3 , − 0 . 1 7 5 6 , − 0 . 3 5 0 9 ] ,
[ 0 . 0 7 6 3 ,
[ 0 . 2 4 9 1 ,
[ [ 0 . 1 2 8 7 , − 0 . 1 6 5 5 ,
[ 0 . 3 8 1 1 , − 0 . 2 3 0 7 ,
[ − 0 . 0 0 7 6 ,
[ [ 0 . 4 2 2 7 ,
[ 0 . 0 8 9 6 ,
[ − 0 . 2 3 4 9 ,

0 . 0 6 5 3 , − 0 . 1 6 5 3 ] ,
0 . 5 0 4 3 , − 0 . 0 0 0 0 ] ] ] ,

0 . 3 4 3 3 , − 0 . 0 0 0 0 ] ] ,
0 . 2 4 9 2 ] ,

0 . 3 0 1 9 ] ,

0 . 0 6 9 0 ,

22

0 . 2 6 6 3 ,

0 . 1 7 6 4 ] ,

0 . 3 4 6 4 ] ,

0 . 0 5 9 0 , − 0 . 0 0 0 0 ] ] ,
0 . 0 3 2 1 ] ,

0 . 5 0 9 4 ,

0 . 2 1 1 0 ] ,

0 . 2 3 3 4 , − 0 . 0 0 0 0 ] ] ,
0 . 3 9 2 8 ] ,

0 . 1 0 9 2 ,

[ [ [ 0 . 1 5 8 5 ,
[ 0 . 0 0 3 1 , − 0 . 0 2 3 7 ,
[ 0 . 0 1 4 0 ,
[ [ 0 . 1 6 3 2 ,
[ 0 . 3 9 3 5 , − 0 . 1 8 0 7 ,
[ − 0 . 3 6 2 0 ,
[ [ − 0 . 2 4 7 4 ,
[ 0 . 2 8 0 8 ,
[ − 0 . 0 6 3 5 ,

0 . 3 9 1 2 ,
0 . 0 1 5 9 ,

0 . 1 2 1 1 ] ,
0 . 0 0 0 0 ] ] ] ,

0 . 6 1 0 5 ] ,

0 . 4 5 4 6 , − 0 . 0 0 0 0 ] ] ,
0 . 1 0 3 9 ] ,

[ [ [ 0 . 8 8 8 6 , − 0 . 2 4 5 9 , − 0 . 4 1 6 9 ] ,
[ − 0 . 4 1 2 0 , − 0 . 1 2 8 2 ,
[ 0 . 2 4 9 5 ,
[ [ 0 . 0 6 7 9 , − 0 . 2 9 8 2 ,
[ 0 . 1 4 3 0 ,
[ − 0 . 2 0 0 2 ,
[ [ − 0 . 2 1 9 5 ,
[ 0 . 3 3 7 3 ,
0 . 1 9 0 7 ,
[ 0 . 1 9 7 7 , − 0 . 0 4 2 7 ,

0 . 6 7 4 3 ] ,
0 . 0 0 0 0 ] ] ,
0 . 2 1 8 3 , − 0 . 2 6 6 5 ] ,

0 . 5 8 4 7 ] ,
0 . 0 0 0 0 ] ] ] ,

0 . 1 5 9 6 ,
0 . 3 4 9 6 ,

[ [ [ − 0 . 3 8 2 9 ,
[ − 0 . 0 6 8 8 ,
[ 0 . 1 4 1 6 ,
[ [ − 0 . 2 2 7 1 ,
[ 0 . 2 6 7 1 , − 0 . 1 2 2 5 ,
[ 0 . 0 2 6 6 ,
[ [ 0 . 8 5 3 9 ,
[ 0 . 6 6 3 5 ,
[ − 0 . 1 3 7 7 , − 0 . 3 4 8 3 ,

0 . 0 1 5 8 ,

0 . 4 0 1 9 ] ,

0 . 2 4 8 1 ] ,

0 . 1 2 0 6 ,
0 . 5 2 3 8 , − 0 . 0 0 0 0 ] ] ,
0 . 3 5 9 3 ] ,

0 . 1 6 8 3 ,

0 . 0 2 1 7 ] ,

0 . 5 0 6 6 , − 0 . 0 0 0 0 ] ] ,
0 . 2 8 9 9 ] ,

0 . 3 6 8 2 ,

0 . 0 1 3 0 , − 0 . 7 0 2 5 ] ,

0 . 0 0 0 0 ] ] ] ,

0 . 2 2 2 0 ,

0 . 0 5 9 8 ] ,

0 . 1 6 8 3 ] ,
0 . 0 0 0 0 ] ] ,
0 . 2 2 0 3 ] ,

[ [ [ 0 . 4 4 8 2 ,
[ 0 . 3 9 6 5 , − 0 . 1 1 4 8 ,
[ − 0 . 3 4 4 4 ,
0 . 1 6 4 4 ,
[ [ − 0 . 1 4 6 3 ,
[ 0 . 4 0 3 9 , − 0 . 2 8 3 2 , − 0 . 1 2 9 0 ] ,
[ 0 . 3 3 6 9 , − 0 . 0 1 4 6 ,
[ [ 0 . 3 7 5 9 ,
0 . 0 5 5 2 ,
[ 0 . 1 9 2 5 ,
[ − 0 . 0 9 6 2 , − 0 . 0 6 7 3 ,

0 . 0 0 0 0 ] ] ,
0 . 1 4 9 4 ] ,

0 . 1 0 9 1 ] ,
0 . 0 0 0 0 ] ] ] ,

0 . 6 1 2 0 ,

0 . 2 8 1 4 ,

[ [ [ 0 . 3 5 5 6 ,
[ 0 . 1 8 1 2 ,
[ 0 . 5 5 3 8 ,
[ [ − 0 . 0 2 9 7 ,
[ 0 . 2 2 4 6 ,
[ 0 . 0 0 7 9 ,
[ [ 0 . 2 4 6 1 ,
[ − 0 . 0 1 6 5 ,
[ 0 . 1 7 7 2 ,

0 . 3 4 7 7 , − 0 . 6 6 2 5 ] ,

0 . 2 9 9 7 , − 0 . 2 1 3 9 ] ,
0 . 1 3 8 5 ,

0 . 0 7 8 0 ,

0 . 1 9 3 1 ,
0 . 1 4 2 6 ,

0 . 1 2 1 7 ,

0 . 0 0 0 0 ] ] ,
0 . 2 4 8 6 ] ,

0 . 1 3 4 9 ] ,
0 . 0 0 0 0 ] ] ,
0 . 1 8 7 9 ] ,

0 . 1 1 6 0 ,
0 . 0 3 2 1 , − 0 . 0 0 0 0 ] ] ] ]

0 . 1 3 5 6 ] ,

23

