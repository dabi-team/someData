Computation Resource Allocation for
Heterogeneous Time-Critical IoT Services in MEC

Jianhui Liu, Qi Zhang
DIGIT, Department of Engineering, Aarhus University, Denmark
Email: {jianhui.liu, qz}@eng.au.dk

0
2
0
2

b
e
F
2
1

]
I

N
.
s
c
[

1
v
1
5
8
4
0
.
2
0
0
2
:
v
i
X
r
a

Abstract—Mobile edge computing (MEC) is one of the promis-
ing solutions to process computational-intensive tasks within
short latency for emerging Internet-of-Things (IoT) use cases,
e.g., virtual reality (VR), augmented reality (AR), autonomous
vehicle. Due to the coexistence of heterogeneous services in MEC
system, the task arrival interval and required execution time
can vary depending on services. It is challenging to schedule
computation resource for the services with stochastic arrivals
and runtime at an edge server (ES). In this paper, we propose
a ﬂexible computation ofﬂoading framework among users and
ESs. Based on the framework, we propose a Lyapunov-based
algorithm to dynamically allocate computation resource for
heterogeneous time-critical services at the ES. The proposed
algorithm minimizes the average timeout probability without
any prior knowledge on task arrival process and required
runtime. The numerical results show that, compared with the
standard queuing models used at ES, the proposed algorithm
achieves at least 35% reduction of the timeout probability, and
approximated utilization efﬁciency of computation resource to
non-cause queuing model under various scenarios.

Index Terms—Mobile edge computing, IoT, computation man-
latency and reliability, Lyapunov optimization, aug-

agement,
mented reality.

I. INTRODUCTION

A variety of emerging Internet-of-Things (IoT) use cases,
e.g., virtual reality (VR), augmented reality (AR), autonomous
vehicle, factory automation, and remote surgery etc., require
real-time control and steering of cyber physical systems.
These use cases often involve intensive computation and have
stringent requirements on latency at millisecond level [1] [2].
However, due to limited computation capability, it is difﬁcult
for mobile devices (MDs) to complete services within the
latency constraint merely by local processing. The mobile
edge computing (MEC) is one of the promising solutions
for time-critical services [3] [4]. Compared to the core cloud
computing, MEC paradigm facilitates MDs to obtain powerful
computation resources in the vicinity and to complete compu-
tation tasks within a low latency.

Although MEC paradigm has potential to reduce the ser-
vice latency, it is a challenge to fulﬁll the stringent latency
requirement of each computation task, in particular, for the
case that heterogeneous time-critical services are ofﬂoaded to
one edge server (ES) from multiple MDs. First of all, the
runtime of one computation task varies with different services.
Even for a given service, the runtime is not deterministic.
Figure 1 presents the latency to detect objects on an image
by Single Shot MultiBox Detector (SSD), which has no

linear relationship with image size. The non-deterministic
runtime makes it difﬁcult to determine the required computing
resource for each service. Moreover, each task can have
different latency constraint. The reason lies in that besides
heterogeneous services with different latency, the delay budget
for computation depends on the time spent on transmission
between MD and ES, even for a single service. Last but
not least, each service has different task arrival process. For
instance, the arrival of object detection for AR service is
deterministic, since the image is captured at a certain rate,
while other services, such as indoor labeling and annotation
are in burst [5].

Computation ofﬂoading for MEC paradigm has attracted
increasing attention in recent years. The state-of-the-art work
[6], [7] studied the allocation of communication and com-
putation resource in MEC system under two impractical as-
sumptions: (i) they assume MDs and ESs are synchronized,
where the tasks of different MDs are ofﬂoaded at the same
time; (ii) the computation intensity, i.e., required CPU cycles
per data bits, is ﬁxed for any task. Lyu et al. [6] jointly
optimized the ofﬂoading decisions and resource allocations
to guarantee task delays and energy saving. Mao et al. [7]
studied a joint radio and computation resource management
for multi-user MEC systems, and minimized the long-term
average weighted energy consumption. The algorithm failed to
consider the latency constraints to complete the computation
tasks.

Considering stochastic task arrival and runtime, the research
work in [8]–[10] allocated resource in MEC system based
on queuing theory. Zhao et al. [8] optimized the ofﬂoading
decision and computation resource allocation to maximize
the probability that tasks can meet the delay requirements.
It assumed all
the users have the same service, and ES
processes tasks of different users following a parallel queuing
model. Park et al. [9] derived the successful edge computing
probability considering multi-tier network and heterogeneous
tasks. The ES is assumed to process different types of tasks
with a sequential queuing model. Duan et al. [10] derived the
closed-form expression of delay distribution with a sharing
queuing model to minimize average latency for services with
small bit sizes. They did not consider the latency constraint
of each service. The existing methods based on queuing
theory results in ﬁxed resource allocation for computation
ofﬂoading, which is not efﬁcient enough for the services with
heterogeneous runtime, latency requirements and task arrival

 
 
 
 
 
 
VGG16 based SSD

User Side

- Computing Results
- Available capacity

Edge Side

MD m

Offloading

Transmission Rate 
Selection

EN Selection

ES n

Dynamic Computation 
Management

s
e
g
a
m

I

f
o

r
e
b
m
u
N

800

600

400

200

0

400

450

500
550
Delay (ms)

600

650

Fig. 1. Histogram of object detection delay by
vgg16-based SSD.

Fig. 2. Proposed computation ofﬂoading framework.

(a) Sequential 

(b) Parallel

(c) Sharing 

0

Fig. 3. Standard queuing models (γk : ratio
of average queue-k trafﬁc load to total trafﬁc
load; nact: the number of inactive queues at the
current system).

process. In addition, the resource allocation derived based
on typical queuing model, e.g. M/M/n and M/D/1, is not
applicable to the services with non-deterministic computation
intensity.

In this paper, we study dynamic computation resource
management in MEC for heterogeneous services. Considering
stochastic task arrivals and computation intensity, we propose
a computation ofﬂoading framework that allows ofﬂoading
decisions of MDs and computation management of ESs to be
optimized independently. Based on the framework, we analyze
different queuing models at ES, and propose a Lyapunov op-
timization [7] based dynamic computation resource allocation
algorithm for heterogeneous tasks. The proposed algorithm
minimizes the average timeout probability by efﬁcient resource
management and selective task dropping, which does not re-
quire any prior knowledge on task arrival process and required
runtime.

The rest of the paper is organized as follows. Section II
describes the system model and problem formulation. The
proposed dynamic computation resource allocation algorithm
for MEC is described in Section III. Sections IV presents the
simulation results. Finally, Section V concludes the paper.

II. SYSTEM MODEL

In this section, we present a computation ofﬂoading frame-
work for MEC system and formulate an optimization problem
for computation resource allocation at the ES.

A. Computation Ofﬂoading Framework

As illustrated in Fig. 2, we consider a network that ESs serve
a set of MDs M (M = {1, 2, . . . , M}). MDs are randomly
distributed and running heterogeneous services which may
task sizes, required CPU
have different arrival processes,
cycles and latency thresholds. The average task arrival rate (in
s−1) of the MD m is denoted by λm. δm denotes the latency
threshold (in seconds) to complete the task of MD m. We
assume there are K types of tasks in the network and each
MD has one type of tasks. The task size (in bits) of each type
is considered as a constant, such as the captured frames size
in AR. However, the required CPU cycles of a task of type k
(1 ≤ k ≤ K) is assumed to follow uniform distribution with
the mean of ¯ck, as it may vary with tasks.

At the user side, to shorten computation latency, MDs can
ofﬂoad tasks to ESs via wireless channel. As different types

of tasks have different arrival process and latency constraints,
it is preferable to allow MDs to make ofﬂoading decision in
a distributed way according to its current channel state and
requirements upon new arrival task. For example, each MD
can dynamically select optimal transmission rate and the ES
to which ofﬂoading the tasks, to minimize the service failure
probability, which is detailed in our previous work [11].

At the edge side, heterogeneous tasks from multiple MDs
randomly arrive at one ES. The sojourn time of each task at the
ES, which is the difference between the arrival and departure
instant of a task, depends not only on its required CPU cycles,
but also on the queuing system at the ES. As shown in Fig.
3, there are three standard queuing models used in the state-
of-the-art work. The sequential queuing model [9] computes
the tasks with the maximal computing capacity of the ES
following the order of task arrival, which can make the most
of the computation resource at the ES [12]. However, if the
required CPU cycles for some types of tasks follow a heavy-
tailed distribution, it may lead to long waiting time for small
tasks arrived after the large task. The parallel queuing model
[8] maintains a queue for each type of task. The computation
resource is proportionally distributed according to the ratio
of average trafﬁc load in each queue to the total trafﬁc at
the ES. Obviously, a ﬁxed resource allocation for each queue
is inefﬁcient as a queue may become temporarily empty. To
improve efﬁciency, the sharing queuing model [10] equally
assigns the computing capacity of the ES to the current active
queues, i.e. non-empty queues, which can uniformly distribute
the computation resource to each types of tasks. However,
it will be difﬁcult to fulﬁll diverse latency requirements of
heterogeneous tasks simultaneously.

To meet latency constraints of heterogeneous services, we
propose a dynamic queuing model, which dynamically allo-
cates the computation resource over time, taking into account
the current queue length and tasks’ waiting time.

B. Problem Formulation

We consider the time is slotted at an ES and indexed by t,
where the slot length is τ. For a given ES, the computation
resource allocation is updated for every time slot. The ES
maintains a queue buffer for each type of task in the network.
The backlog of the queue k (1 ≤ k ≤ K) at time slot t is
denoted by Qk(t) (in cycles) and is evolved by

Qk (t + 1) = max {Qk(t) + Ak(t) − τ fk (t), 0} ,

(1)

 
 
Í

fmax, we have

where fk (t) is the computation resource that the ES assigns
to queue k at slot t. Given the maximal computation resource
K
k=1 fk(t) ≤ fmax for ∀t ≥ 0. Ak(t)
of ES,
is the sum of the required CPU cycles of the arrived tasks
in queue k during time slot t. We denote Mk as the set of
MDs having the task type k, and cm(t) as the required CPU
cycles of the tasks from MD m. Ak(t) can be represented as
Ak(t) =
m∈Mk cm(t) for ∀k ∈ [1, K] and t ≥ 0. Note that
Ak(t) cannot be accurately obtained in practice, as the required
CPU cycles of each task is a random variable. Therefore, we
estimate Ak(t) as ˜Ak(t) = λk ¯ckτ, where λk is the arrival rate
of task type k at the ES.

Í

m , is constrained by δm − T tran

The maximal allowed sojourn time of an arrived task from
MD m, δcmp
m is the
transmission latency of a task of MD m. Due to stochastic
m . Therefore, denoting ¯Tk as
channel state, δcmp
the average sojourn time of the tasks in queue k, the weighted
average timeout probability to complete computation tasks at
the ES is expressed as

m varies with T tran

m , where T tran

¯F to =

K

Õk=1

λk
K
k=1 λk

P

¯Tk > ¯δcmp

k

,

(2)

(cid:8)

(cid:9)

where ¯δcmp
k
type k and ¯δcmp

k

is the average threshold of sojourn time for task
.

Í
δcmp
m , m ∈ Mk

= E

(cid:9)

(cid:8)

Our objective is to minimize the weighted average timeout
probability ¯F to. However, as the dynamic queuing system
follows K independent G/G/1 queue models and the service
rate of each queue varies over time, it is difﬁcult to derive a
closed-form approximation of ¯Tk. Therefore, we estimate the
sojourn time of the new arrived tasks in queue k in time t,
Tk(t), based on the Little’s Law, i.e.

Tk(t) = Qk(t + 1)
ak(t)

,

(3)

t
j=1

where ak(t) = 1
m∈Mk cm( j), and it denotes the average
t
required CPU cycles of the arrived trafﬁc (in cycles/s) in queue
k over t time slots. In this way, the optimization problem of
computation resource management can be formulated as

Í

Í

P1: min
fk (t)

lim
T ←+∞

K

1

T

T

K

Õt=1

Õk=1

I

Tk(t) − ¯δcmp

k

(cid:8)

(cid:9)

s.t.

fk (t) ≤ fmax, ∀t ∈ [1, T ],

(4a)

Õk=1

where I{x} is an indicator function. If x > 0, I{x} = 1;
otherwise, I{x} = 0. The constraint (4a) introduces the
computation resource limit of the ES at each time slot.

i.e. queue length,

The objective of the problem P1 is to minimize the weighted
average timeout probability over long time. The state of the
queuing system,
is time-varying due to
stochastic task arrivals. The computation resource at the ES
has to be dynamically allocated to each queue depending
on the current system state. As Lyapunov optimization is
often used to optimally control a dynamic system and ensure
system stability [7], we propose a Lyapunov optimization

based dynamic computation resource allocation algorithm to
solve the problem, which is elaborated in the following section.

III. COMPUTATION RESOURCE OPTIMIZATION IN MEC

In this section, we describe how to dynamically schedule
computation resource at the ES, and solve the problem P1
based on Lyapunov optimization.

Because the queue length is temporally correlated,
the
computation resource allocation is time-dependent. Since the
Lyapunov optimization technique requires i.i.d. allocations in
different time slots, it cannot be utilized directly here. To
solve this problem, we reformulate the problem and intro-
duce a set of non-negative parameters q (in cycles), where
q = [q1, q2, · · · , qK ]. The goal is to let the length of queue
k, Qk(t), be stabilized around qk at each slot. On the one
hand, it will allow i.i.d. resource allocation over time. On
the other hand, it is beneﬁcial to reduce consecutive task
timeout at the same queue due to long queue length. Denoting
Q(t) = [Q1(t), Q2(t), · · · , Qk(t)], the approximation of Q(t) to
q can be measured by a non-negative Lyapunov function, i.e.

L [Q(t)] =

1
2

K

Õk=1

[Qk(t) − qk]2 .

(5)

Therefore, to both stabilize the queue backlog and minimize
the timeout probability at the ES, the conditional Lyapunov
drift-plus-penalty is introduced

∆V = E

L [Q(t + 1)] − L [Q(t)] + V F to(t)

Q(t)

,

(6)

k

(cid:12)
(cid:12)

K

(cid:8)

(cid:9)

k=1 I

where V is

a non-negative penalty,

(cid:8)
Tk(t) − ¯δcmp

=
. In each time slot, the computation re-
source at the ES is allocated by minimizing ∆V , which is
Í
parameterized by the current system state and penalty V
subject to the constraint of the maximal computing capability
at the ES.

and F to(t)

Since [max (u − v + w, 0)]2 ≤ u2 + v2 + w2 − 2u(v − w) for
∀u, v, w > 0 [13], substituting (1) and (5) into (6), we obtain

(cid:9)

∆V ≤ B + E

K

K

(

Õk=1

[Qk(t) − qk] ˜Ak(t) +

[qk − Qk(t)] fk(t)τ + V F to(t)

Q(t)

,

)

(7)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Õk=1
(τ fmax)2 +

2

h

K
k=1

Ak,max

where B = 1
and it has no effect
2
on optimization performance. Ak,max denotes the maximal
(cid:0)
arriving trafﬁc load of queue k during one time slot. The
inequality presents the upper bound of the drift-plus-penalty
function in each time slot, which means the solution of the
problem P1 can be approximated by minimizing the right side
of (7), i.e.

Í

i

(cid:1)

P2: min
fk (t)

s.t.

K

Õk=1
(cid:8)
(4a).

[qk − Qk(t)] fk(t)τ + VI

Tk(t) − ¯δcmp

k

(cid:2)

(cid:3) (cid:9)

k

(cid:2)

max

Qk(t) + ˜Ak(t)

− ak(t) ¯δcmp
(cid:2)

Tk(t) − ¯δcmp
the
k
will have smaller
(cid:9)

Due to the existence of I(·) in the objective function,
=
the problem P2 is non-convex. Since I
Qk(t) + ˜Ak(t) − τ fk (t), 0
I
queue
(cid:3)
with smaller
timeout
(cid:8)
probability in time slot t. In this way, to simplify the problem,
(cid:3)
we will linearize the objective function as the formation of
K
k=1 wk(t) fk(t)τ, where wk(t) is the weight of queue k in
time slot t and wk(t) ≡ [qk − Qk(t)] + V
, and
Í
prioritize the resource allocation of the queue with smaller
wk(t). Therefore, the problem P2 is transformed to

Qk(t) + ˜Ak(t)

(cid:3)

(cid:2)

(cid:3)

(cid:2)

,

P3: min
fk (t)

s.t.

K

wk(t) fk(t)τ

Õk=1
(4a).

Note that larger V leads to smaller timeout probability, but may
also waste more computation capacity by executing timeout
tasks. Thus, adjusting V can make a tradeoff between the
timeout probability and computation efﬁciency.

It is proven in [14] that, if initial queue k meets Qk(1) ≤
Qk,max and Qk,max = qk + Ak,max, we have Qk(t) ≤ Qk,max for
∀ t ≥ 1. We denote δcmp
k,max as the maximal allowed latency
threshold for task type k. As the newly arrived tasks in queue
k are bound to timeout if the queue length exceeds δcmp
k,max fmax,
δcmp
k,max fmax can be regarded as the maximal allowed length of
queue k. Therefore, we deﬁne qk = δcmp

k,max fmax − Ak,max.

To solve problem P3, the weight of each queue at the
ES will be calculated before each time slot starts. The ES
prioritizes the computation resource allocation to the queue
with smaller weight, e.g. queue i. We assume that δcmp
i,min(t)
latency constraint among the tasks in the
is the smallest
current queue i. If δcmp
i,min(t) ≤ 0, the ES will allocate all its
remaining computation resource to the queue i; otherwise, the
assigned computation resource will be min

, fremain

,

(cid:27)
where fremain denotes the remaining computation resource after
the previous queues are allocated. Note that the empty queue is
not assigned any computation resource. The algorithm will be
terminated when all the queues are served or the computation
resource at the ES runs out.

(cid:26)

Qi (t)
δcmp
i,min(t)

It is worth noticing that the number of cycles in a queue,
i.e. Qk (t), cannot be accurately obtained in practice, as the
required CPU cycles of each task is a random variable.
However, the number of the tasks in the queue can be obtained,
which is denoted by nk(t). Thus, we estimate the Qk(t) as
˜Qk(t) = [nk(t) + 1] ¯ck − csev
k (t) is the executed
cycles of the task being serviced from queue k.

k (t), where csev

Furthermore, the timeout probability can be decreased fur-
ther by dropping the task that stays too long in the queue to
be able to meet its latency constraint. Here, we evaluate the
task in the head of each queue before each time slot starts.
The task will be dropped if the unﬁnished CPU cycles cannot
be completed within the latency constraint even using all the
computation resource at the ES.

IV. SIMULATION RESULTS

In this section, the numerical results are presented to com-
pare the performance of different queuing models at ES. The
maximal computing capacity of the ES is assumed to 3 × 1010
cycles/s. The simulation is based on real-time object detection
application which is an important component of AR system.
The captured video frames of MDs, i.e. computation tasks,
are ofﬂoaded to the ES to complete the object detection.
Three scenarios are analyzed based on different task sizes,
computation intensities (CIs) and task arrival processes. We
consider there are two types of tasks in the network in each
scenario. The proposed dynamic computation resource alloca-
tion algorithm (Dynamic) is compared with the three standard
queuing models (Sequential, Sharing and Parallel). We also
evaluate the performance of dynamic queuing model without
selective task dropping (Dynamic no drop) and non-causal
dynamic queuing model (Dynamic non-causal) which assumes
the required CPU cycles of each task are known beforehand.
The default slot duration of computation allocation is 1 ms.
Considering the tradeoff between the computation efﬁciency
and timeout probability,
to 2. Each
simulation processes 105 tasks.

the penalty V is set

A. Different Task Sizes

In this scenario, MDs have tasks with different data sizes,
like video frames of different resolutions in AR services. Task
type I and type II have frame sizes of 0.6 Mbits and 2.4 Mbits,
respectively, which corresponds to the 320×240 and 640×480
resolution gray-scale frame with depth of 8 bits, respectively.
The video frame is captured and ofﬂoaded with the rate of 25
frame per second (FPS). The average computation intensity
to execute a task is 100 cycles/bit1. The latency to complete
a task, including both transmission and computation latency,
should be shorter than the interval of frame capturing, i.e. 40
ms. Thus, the ranges of sojourn latency constrains2 at the ES
are set to [3, 39] ms and [12, 35] ms, respectively, for the two
types of tasks.

Table I compares the timeout probability of queuing models
to complete tasks under different trafﬁc loads and CI varia-
tions. 30% CI variation means the required CPU cycles of
a task range from 0.7 ¯ck to 1.3 ¯ck, whereas 90% trafﬁc load
means the average required CPU cycles of the arrived tasks
per second are equal to 0.9 fmax. The second row of the table
presents the ratio of the arriving trafﬁc load between type-I
and type-II tasks. For instance, "7/2" denotes the case that
the number of MDs having type-I and type-II tasks is 14 and
4, respectively. The table shows that the proposed dynamic

1The result is estimated by laptop of Intel i5-6300U 2.3GHz CPU, 8G
RAM, where Object detection is based on Google TensorFlow Object Detec-
tion API with pre-trained model "ssd mobilenet v1 coco".

2The upper bound of the latency constraint depends on the highest trans-
mission rate on wireless channel. It will take about 1 ms and 5 ms to transmit
0.6Mbits and 2.4 Mbits data, respectively, using the highest modulation and
coding scheme on LTE channel of 10 MHz. The lower bound is the runtime
that executes 1.5 ¯ck CUP cycles with all the computation resource at the ES.
For instance, the average number of required cycles for 0.6 Mbits task is 60
Mcycles. The lower bound for 0.6 Mbit tasks is 1.5∗60∗106

3∗1010 = 3 ms

TABLE I
TIMEOUT PROBABILITY COMPARISON FOR SCENARIO OF DIFFERENT TASK SIZES.

Type-I/Type-II
Dynamic non-causal
Dynamic
Dynamic no drop
Sequential
Sharing
Parallel

90% trafﬁc load, ﬁxed CI

7/2
0.0044
0.0045
0.0053
0.0100
0.0074
0.0666

5/4
0.0227
0.0239
0.0267
0.0429
0.0412
0.1033

1/2
0.0366
0.0375
0.0421
0.0623
0.0664
0.2015

1/8
0.0090
0.0091
0.0094
0.0109
0.0198
0.2194

90% trafﬁc load, 10% CI variation
1/8
7/2
0.0107
0.0050
0.0107
0.0052
0.0111
0.0057
0.0136
0.0101
0.0198
0.0080
0.2186
0.0653

1/2
0.0369
0.0387
0.0439
0.0642
0.0670
0.2021

5/4
0.0238
0.0253
0.0268
0.0437
0.0410
0.1031

90% trafﬁc load, 30% CI variation
1/8
7/2
0.0207
0.0077
0.0245
0.0082
0.0247
0.0087
0.0276
0.0131
0.0308
0.0118
0.2359
0.0619

5/4
0.0261
0.0300
0.0313
0.0475
0.0461
0.1113

1/2
0.0402
0.0465
0.0475
0.0693
0.0698
0.2077

queuing models have lower timeout probability than the other
queuing models. For instance, when trafﬁc load is 90% and
CI variation is 10%, compared with the sequential queuing
models, the timeout probability of the proposed algorithm is
reduced nearly by 45% and 40% if the trafﬁc load ratio is
5/4 and 1/2, receptively. The parallel queuing model has the
worst performance, due to the resource waste on the inactive
queues. Moreover, the dynamic queuing model shows higher
gains to other queuing model, when difference of the trafﬁc
loads between the type-I and type-II is smaller, i.e., the cases
"5/4" and "1/2". With the CI variation increasing, the timeout
probability of all
the queuing models rises slightly, since
some of arriving tasks may require more CPU cycles. The
higher CI variation also makes it more difﬁcult to determine
if tasks should be dropped, which increases the performance
gaps between the dynamic queuing model with selective task
dropping and the non-causal dynamic queuing model.

cwaste
fmaxTsim

As part of the computation resource at the ES could be
wasted to compute the timeout tasks, we evaluate the wasted
computation resource of different queuing models in Fig. 4.
Denoting cwaste as the sum of wasted CPU cycles executed by
the ES during simulation, the wasted computation resource
, where Tsim is the simulation time. A
is deﬁned as
smaller value implies the queuing system is more efﬁcient. Fig.
4 shows that, due to the selective task dropping, the proposed
dynamic allocation algorithm achieves the best performance,
which is close to that of non-causal dynamic queuing model.
The wasted computation resource of the sequential queuing
model is larger than the proposed algorithm, but much smaller
than the parallel and sharing queuing models. The reason
lies in that, compared to the other two standard models, the
sequential queuing model always utilize all the computing
capability to execute each arrived task. It minimizes the
runtime and fails to consider the queuing delay, whereas the
proposed dynamic queuing model optimizes both of them.

B. Different Computation Intensities

In this subsection, the tasks have different computation
intensities, considering MDs may require diverse detection
algorithms for different types of objects and different re-
quirements of detection precision. We assume the average
computation intensities of task type I and type II are 100
cycles/bit and 400 cycles/bit, respectively. The data size of
each task is set to 0.6 Mbits. In addition, the ranges of the
sojourn latency constraints are [3, 39] ms and [12, 39] ms

TABLE II
TIMEOUT PROBABILITY COMPARISON FOR SCENARIO OF DIFFERENT CIS.

Type-I/Type-II
Dynamic non-causal
Dynamic
Dynamic no drop
Sequential
Sharing
Parallel

90% trafﬁc load, 10% CI variation

7/2
0.00148
0.00149
0.00156
0.01008
0.00604
0.05697

5/4
0.01658
0.01620
0.01745
0.04172
0.03598
0.08658

1/2
0.03124
0.03130
0.03215
0.06144
0.05565
0.18350

1/8
0.00824
0.00834
0.00885
0.01195
0.01599
0.21040

TABLE III
TIMEOUT PROBABILITY COMPARISON FOR SCENARIO OF DIFFERENT TASK
ARRIVAL PROCESSES.

Type-I/Type-II
Dynamic non-causal
Dynamic
Dynamic no drop
Sequential
Sharing
Parallel

90% trafﬁc load, 10% CI variation

7/2
0.06623
0.06666
0.07439
0.07970
0.08988
0.15410

5/4
0.07854
0.08358
0.09082
0.11450
0.12370
0.21500

1/2
0.08014
0.08223
0.09567
0.13980
0.12430
0.27380

1/8
0.03689
0.03827
0.04238
0.06026
0.04915
0.26100

for the two types of tasks, respectively. The frame captured
rate is 25 FPS. Table II shows the timeout probability of
different queuing models for the case of 90% trafﬁc load and
10% CI variation. The dynamic queuing model with selective
task dropping achieves similar performance as the non-causal
dynamic queuing model. Compared to the other models, the
gain of the dynamic queuing models increases, when trafﬁc
load is dominated by tasks of type-I. For example, compared
with the sharing queuing models, the timeout probability is
reduced nearly by 55% and 75% when trafﬁc load ratios are
5/4 and 7/2, respectively.

Fig. 5 compares the wasted computation resource among
queuing models for the case of 90% trafﬁc load and 10% CI
variation. The proposed dynamic queuing model with selective
task dropping achieves the least computation resource waste,
of which performance is approximated to that of the non-
causal queuing model.

C. Different Task Arrival Processes

In the previous two subsections, the arriving interval is
deterministic between two consecutive tasks from the same
MD. In practice, the task arrival process could be different
from heterogeneous services. Therefore, in this subsection,

0.25

0.2

0.15

0.1

0.05

y
t
i
c
a
p
a
c
n
o
i
t
a
t
u
p
m
o
c

d
e
t
s
a
W

Dynamic no drop
Dynamic
Dynamic non-causal
Sequential
Sharing
Parallel

0.2

0.15

0.1

0.05

y
t
i
c
a
p
a
c
n
o

i
t

t

a
u
p
m
o
c
d
e
t
s
a
W

Dynamic no drop
Dynamic
Dynamic non-causal
Sequential
Sharing
Parallel

0.25

0.2

0.15

0.1

0.05

y
t
i
c
a
p
a
c
n
o
i
t
a
t
u
p
m
o
c

d
e
t
s
a
W

Dynamic no drop
Dynamic
Dynamic non-causal
Sequential
Sharing
Parallel

0
7/2

5/4

1/2

1/8

Ratio of traffic load between two types of tasks

0
7/2

5/4

1/2

1/8

Ratio of trafficload between two type tasks

0
7/2

5/4

1/2

1/8

Ratio of trafficload between two type tasks

Fig. 4. Comparison of the wasted computing
capacity for scenario of different
task sizes:
90% trafﬁc load and 10% CI variation.

Fig. 5. Comparison of the wasted computing
capacity for scenario of different CIs: 90%
trafﬁc load and 10% CI variation.

Fig. 6. Comparison of the wasted computing
capacity for scenario of different arrival pro-
cesses: 90% trafﬁc load and 10% CI variation.

[4] ——, “Code-partitioning ofﬂoading schemes in mobile edge computing
for augmented reality,” IEEE Access, vol. 7, pp. 11 222 – 11 236, 2019.
[5] W. Zhang, B. Han, and P. Hui, “On the networking challenges of mobile
augmented reality,” in Proceedings of the Workshop on Virtual Reality
and Augmented Reality Network, 2017, pp. 24–29.

[6] X. Lyu, H. Tian, W. Ni, Y. Zhang, P. Zhang, and R. P. Liu, “Energy-
efﬁcient admission of delay-sensitive tasks for mobile edge computing,”
IEEE Transactions on Communications, vol. 66, no. 6, pp. 2603–2616,
2018.

[7] Y. Mao, J. Zhang, S. Song, and K. B. Letaief, “Stochastic joint radio
and computational resource management for multi-user mobile-edge
computing systems,” IEEE Transactions on Wireless Communications,
vol. 16, no. 9, pp. 5994–6009, 2017.

[8] T. Zhao, S. Zhou, X. Guo, and Z. Niu, “Tasks scheduling and resource
allocation in heterogeneous cloud for delay-bounded mobile edge com-
puting,” in 2017 IEEE International Conference on Communications
(ICC), 2017, pp. 1–7.

[9] C. Park and J. Lee, “Successful edge computing probability analysis
in heterogeneous networks,” in 2018 IEEE International Conference on
Communications (ICC), 2018, pp. 1–6.

[10] Y. Duan, C. She, G. Zhao, and T. Q. Quek, “Delay analysis and
computing ofﬂoading of urllc in mobile edge computing systems,” in
2018 10th International Conference on Wireless Communications and
Signal Processing (WCSP), 2018, pp. 1–6.

[11] J. Liu and Q. Zhang, “Edge computing enabled mobile augmented reality
with imperfect channel knowledge,” in IEEE European Wireless, 2019,
pp. 1–6.

[12] C. She, C. Yang, and T. Q. Quek, “Joint uplink and downlink resource
conﬁguration for ultra-reliable and low-latency communications,” IEEE
Transactions on Communications, vol. 66, no. 5, pp. 2266–2280, 2018.
[13] C.-F. Liu, M. Bennis, and H. V. Poor, “Latency and reliability-aware
task ofﬂoading and resource allocation for mobile edge computing,” in
2017 IEEE Globecom Workshops (GC Wkshps), 2017, pp. 1–7.
[14] M. J. Neely and L. Huang, “Dynamic product assembly and inventory
control for maximum proﬁt,” in 49th IEEE Conference on Decision and
Control (CDC), 2010, pp. 2805–2812.

we assume that the task arrival processes of task type-I and
type-II follow the Poisson process and deterministic process,
respectively. Their average task arrival rates are both 25 FPS.
The other settings follow the scenario in Subsection IV-A.

Table III shows the timeout probability comparison among
different queuing models. The dynamic queuing model has
lower timeout probability than the other queuing models.
For example, compared with the sharing queuing model, the
dynamic queuing model with selective task dropping reduces
the timeout probability nearly by 35%, when the trafﬁc load
ratios are 1/2. Moreover, by comparing Table III and Table I, it
can be seen that different trafﬁc arrival processes increase the
timeout probability. The reason lies in that the tasks arrived
in burst may lead to the trafﬁc load temporarily over 100%.
Fig. 6 shows that both dynamic queuing models with and
without selective task dropping achieve lower computation
resource waste compared with the other standard queuing
models.

V. CONCLUSIONS

In this paper, a ﬂexible computation ofﬂoading frame-
work is proposed for multi-user MEC system. Based on the
framework, we optimize the computation resource allocation
at ES considering heterogeneous time-critical services. The
proposed dynamic computation allocation algorithm mini-
mizes the weighted average timeout probability without any
prior knowledge on the task arrival process and required
runtime. Compared with the three standard queuing models,
the proposed algorithm can achieve at least 35% reduction of
timeout probability under all the tested scenarios, and is the
most efﬁcient algorithm to utilize computation resource. The
performance of the proposed algorithm is approximated to the
non-causal queuing model.

REFERENCES

[1] Q. Zhang and F. H. Fitzek, “Mission critical iot communication in 5g,”
in Future Access Enablers of Ubiquitous and Intelligent Infrastructures.
Springer, 2015, pp. 35–41.

[2] Q. Zhang, J. Liu, and G. Zhao, “Towards 5g enabled tactile robotic

telesurgery,” arXiv preprint arXiv:1803.03586, 2018.

[3] J. Liu and Q. Zhang, “Ofﬂoading schemes in mobile edge computing
for ultra-reliable low latency communications,” IEEE Access, vol. 6, pp.
2169–3536, 2018.

 
 
 
 
 
 
