Confusing Image Quality Assessment:
Towards Better Augmented Reality Experience

Huiyu Duan, Xiongkuo Min, Yucheng Zhu, Guangtao Zhai, Senior Member, IEEE,
Xiaokang Yang, Fellow, IEEE, and Patrick Le Callet, Fellow, IEEE

1

2
2
0
2

r
p
A
1
1

]

V
C
.
s
c
[

1
v
0
0
9
4
0
.
4
0
2
2
:
v
i
X
r
a

Abstract‚ÄîWith the development of multimedia technology,
Augmented Reality (AR) has become a promising next-generation
mobile platform. The primary value of AR is to promote the
fusion of digital contents and real-world environments, however,
studies on how this fusion will inÔ¨Çuence the Quality of Experience
(QoE) of these two components are lacking. To achieve better
QoE of AR, whose two layers are inÔ¨Çuenced by each other, it is
important to evaluate its perceptual quality Ô¨Årst. In this paper, we
consider AR technology as the superimposition of virtual scenes
and real scenes, and introduce visual confusion as its basic theory.
A more general problem is Ô¨Årst proposed, which is evaluating the
perceptual quality of superimposed images, i.e., confusing image
quality assessment. A ConFusing Image Quality Assessment
(CFIQA) database is established, which includes 600 reference
images and 300 distorted images generated by mixing reference
images in pairs. Then a subjective quality perception study
and an objective model evaluation experiment are conducted
towards attaining a better understanding of how humans perceive
the confusing images. An objective metric termed CFIQA is
also proposed to better evaluate the confusing image quality.
Moreover, an extended ARIQA study is further conducted based
on the CFIQA study. We establish an ARIQA database to better
simulate the real AR application scenarios, which contains 20
AR reference images, 20 background (BG) reference images, and
560 distorted images generated from AR and BG references, as
well as the correspondingly collected subjective quality ratings.
We also design three types of full-reference (FR) IQA metrics
to study whether we should consider the visual confusion when
designing corresponding IQA algorithms. An ARIQA metric is
Ô¨Ånally proposed for better evaluating the perceptual quality of
AR images. The dataset, benchmark study and proposed metric
will be released to facilitate the future studies related to AR QoE
assessment and improvement.

Index Terms‚ÄîAugmented Reality (AR), visual confusion, im-

age quality assessment, quality of experience (QoE).

I. INTRODUCTION

With the evolution of multimedia technology,

the next-
generation display technologies aim at revolutionizing the
way of interactions between users and their surrounding
environment rather than limiting to Ô¨Çat panels that are just
placed in front of users (i.e., mobile phone, computer, etc.)
[1], [2]. These technologies, including Virtual Reality (VR),
Augmented Reality (AR), and Mixed Reality (MR), etc., have
been developing rapidly in recent years. Among them, AR
pursues high-quality see-through performance and enriches the
real world by superimposing digital contents on it, which is

Huiyu Duan, Xiongkuo Min, Yucheng Zhu, Xiaokang Yang, Guangtao Zhai
are with the Institute of Image Communication and Network Engineering,
Shanghai Jiao Tong University, Shanghai 200240, China.

Patrick Le Callet is with the Polytech Nantes, Universit¬¥e de Nantes, 44306

Nantes, France.

Fig. 1. Visual confusion theory of AR. ‚ÄúB‚Äù denotes the background view,
i.e., see-through view. ‚ÄúA‚Äù represents the augmented view. ‚ÄúS‚Äù implies the
superimposition of the background view (B) and augmented view (A). ‚ÄúA‚Äù
and ‚ÄúB‚Äù will inÔ¨Çuence the perceptual quality of each other.

promising to become next-generation mobile platform. With
in several
advanced experience, AR shows great potential
attractive application scenarios, including but not limited to
communication, entertainment, health care, education, engi-
neering design, etc.

On account of the complex application scenes, it is impor-
tant to consider the perceptual Quality of Experience (QoE)
of AR, which includes measuring the perceptual quality and
better improving the experience of AR. Lately, some works
have been presented to study the quality effects of typical
degradations that affect digital contents in AR [3]‚Äì[7]. These
studies have performed subjective/objective tests on screen
displays showing videos of 3D meshes or point clouds with
various distortions. Moreover, with the development of Head
Mounted Displays (HMDs) for AR applications, some studies
have considered evaluating the QoE of 3D objects using these
devices. For instance, Alexiou et al. [8] have studied geometry
degradations of point clouds and have conducted a subjective
quality assessment study in a MR HMD system. Zhang et
al. [9] have conducted a study towards the QoE model of
AR applications using Microsoft HoloLens, which mainly
focused on the perceptual factors related to the usability
of AR systems. Gutierrez et al. [10] have proposed several
guidelines and recommendations for subjective testing of QoE
in AR scenarios. However, all these studies only focus on
the degradations of geometry and texture of 3D meshes and
point clouds inside AR, e.g., noise, compression etc., their
see-through scenes are either blank or simple texture, or
even without see-through scenes (opaque images/objects). The
studies discussing the relationship between augmented view
and see-through view are lacking.

To address the above issues, in this paper, we consider

NothingNo confusionNo confusionConfusionBASBA 
 
 
 
 
 
AR technology as the superimposition of digital contents and
see-through contents, and introduce visual confusion [11],
[12] as its basic theory. Fig. 1 demonstrates the concept
of the visual confusion in AR. ‚ÄúB‚Äù, ‚ÄúA‚Äù and ‚ÄúS‚Äù in Fig.
1 represent background (BG) view, augmented view and
superimposed view, respectively. If both ‚ÄúB‚Äù and ‚ÄúA‚Äù are
views with blank/simple textures, there is nothing important
in the superimposed view. If one of ‚ÄúB‚Äù or ‚ÄúA‚Äù has a complex
texture, but another view has a simple texture, there is also no
confusion in the superimposed view. If both ‚ÄúB‚Äù and ‚ÄúA‚Äù have
complex textures, visual confusion is introduced. We assume
that without
introducing speciÔ¨Åc distortions that has been
widely studied in the current QoE studies, visual confusion
itself is a type of distortion, and it signiÔ¨Åcantly inÔ¨Çuences the
AR QoE. Thus, we argue that it is important to study the
assessment of the visual confusion towards better improving
the QoE of AR. Note that it does not mean that no confusion
is better than with confusion, since the objective of AR is
promoting the fusion between virtual world and real world.
Instead, the balance between them is more important.

To this end, in this work, we Ô¨Årst propose a more general
problem, which is evaluating the perceptual quality of visual
confusion. A ConFusing Image Quality Assessment (CFIQA)
dataset is established to make up for the absence of relevant
research. SpeciÔ¨Åcally, we Ô¨Årst collect 600 reference images
and mix them in pairs, which generates 300 distorted images.
We then design and conduct a comprehensive subjective
quality assessment experiment among 17 subjects, which
produces 600 mean opinion scores (MOSs), i.e., for each
distorted image, two MOSs are obtained for two references
respectively. The distorted and reference images, as well as
subjective quality ratings together constitute the ConFusing
Image Quality Assessment (CFIQA) dataset. Based on the
dataset, we analyze several visual characteristics of visual
confusion and propose an attention based deep feature fusion
method towards better evaluating the quality of confusing
images. A specialized learning strategy for this purpose is
proposed. We then compare the proposed method with several
state-of-the-art IQA metrics and conduct a benchmark study.
Moreover, considering the Ô¨Åeld-of-view (FOV) of the AR
image and the background image are usually different in real
application scenarios, we further establish an ARIQA dataset
for better understanding the perception of visual confusion in
real world. The ARIQA dataset is comprised of 20 raw AR
images, 20 background images, and 560 distorted versions
produced from them, each of which is quality-rated by 23
subjects. Besides the visual confusion distortion as mentioned
above, we further introduce three types of distortions: JPEG
compression, image scaling and contrast adjustment to AR
contents. Four levels of the visual confusion distortion are
applied to mix the AR images and the background images.
Two levels of other types of distortions are applied to the
AR contents. To better simulate the real AR scenarios and
control the experiment environment, the ARIQA experiment
is conducted in VR environment. We also design three types of
objective AR-IQA models, which can be differentiated accord-
ing to the inputs of the classical IQA models, to study whether
and how the visual confusion should be considered when

2

designing corresponding IQA metrics. An ARIQA model is
Ô¨Ånally proposed to better evaluate the perceptual quality of
AR images.

Overall, the main contributions of this paper are summarized
as follows. 1) We discuss the visual confusion theory of AR
and argue that evaluating the visual confusion is one of the
most important problem of evaluating the QoE of AR. 2) We
establish the Ô¨Årst ConFusing IQA (CFIQA) dataset, which
can facilitate further objective visual confusion assessment
studies. 3) To better simulate the real application scenarios, we
establish an ARIQA dataset and conduct a subjective quality
assessment experiment in VR environment. 4) Two objective
model evaluation studies are conducted on the two datasets,
respectively. 5) A CFIQA model and an ARIQA model are
proposed for better evaluating the perceptual quality in these
two application scenarios.

Our data collection software, datasets, benchmark studies,
as well as objective metrics will be released to facilitate future
research. We hope this study will motivate other researchers to
consider both the see-through view and the augmented view
when conducting AR QoE studies. The rest of the paper is
organized as follows. In Section II, we give an overview of
the related backgrounds and works. Section III describes the
construction process of CFIQA dataset. The proposed CFIQA
model is then introduced in Section IV. The experimental
results of objective CFIQA evaluation are given in Section
V. Then an extended subjective & objective ARIQA study is
presented in Section VI. Section VII concludes the paper and
discusses several future issues.

II. RELATED WORK

In this section, we shortly review two topics related to this
work, including Augmented Reality and its perceptual theory
basis, as well as image quality assessment.

A. Augmented Reality and Visual Confusion Basis

This work mainly concerns head-mounted AR application
rather than mobile phone based AR application. Considering
rendering methods, there are two main aspects of works in
the Ô¨Åeld of AR visualization, including 2D displaying and
3D rendering. Considering the theory basis of AR devices,
there are two vision theories underlying the AR technologies,
including binocular visual confusion and monocular visual
confusion. We discuss the relationship between these four
aspects and this work as follows.

2D displaying. The most basic application of AR is display-
ing digital contents in a 2D virtual plane [13]. These digital
contents include images, videos, texts, shapes, and even 3D
objects in 2D format, etc. To display 2D digital contents, a
real world plane is needed to attach the virtual plane. The
real world plane and virtual plane are usually in one same
Vieth‚ÄìM¬®uller circle (a.k.a,
isovergence circle), which may
cause visual confusion. This situation is the main consideration
of this paper.

3D rendering. In contrast to 2D displaying, 3D rendering
aims at providing 3D depth cues of virtual objects (note that
this depth cue is a little bit different from the depth of the
above mentioned virtual plane) [14]. Although the 3D depth

3

Fig. 3. An illustration of the subjective experiment methodology for CFIQA.

Fig. 4. Histogram of MOSs from the CFIQA dataset within different Œª value
ranges.

have been proposed. Kang et al. [27] proposed to use multi-
task convolutional neural networks to evaluate the image
quality and use 32 √ó 32 patches for training. Bosse et al.
[28] proposed both FR and NR IQA metrics by joint learning
of local quality and local weights. Some studies located
speciÔ¨Åc distortions by using convolutional sparse coding and
then evaluated the image quality [29]. Recently, some studies
also demonstrated the effectiveness of pre-trained deep neural
network (DNN) features on calculating visual similarity [30],
[31].

AR/VR IQA. As discussed above, most previous AR/VR
IQA studies have focused on the degradations of geometry
and texture of 3D meshes and point clouds inside AR/VR.
Unlike AR, many works towards VR IQA have also studied
omnidirectional (a.k.a, equirectangular) images‚Äô quality [32],
[33], since the format of these images is different with tradi-
tional images. In this paper, we propose that in AR technology,
confusing image is its special ‚Äúimage format‚Äù, and confusing
image quality assessment is equally important with 3D meshes
or point clouds quality assessment, since it is not only related
to 2D displaying but also associated with 3D rendering effects.
Moreover, it signiÔ¨Åcantly inÔ¨Çuences the QoE of AR.

III. STUDY I: SUBJECTIVE CFIQA

A. Confusing Image Collection

To address the problem of subjective confusing IQA data
absence, we Ô¨Årst build a novel ConFusing Image Quality
Assessment (CFIQA) dataset. Since this paper is the Ô¨Årst work
to study confusing IQA, we consider the visual confusion
as the only distortion type in this section to study whether
and how the visual confusion can inÔ¨Çuence the perceptual
quality. We collect 600 images from Pascal VOC dataset [34]
as reference images and split them into two groups. Then

Fig. 2. Distribution of Œª values in this paper. The values are multiplied by
10 and rounded up.

cues will cause the real world scenes and virtual objects to be
located in different Vieth‚ÄìM¬®uller circles, the visual confusion
basis still exists, which makes this situation more complex
(see Section VII for more details).

Binocular visual confusion. The superimposition of two
views of the visual scene (i.e., two images in this paper) allows
people to see two different things in one direction, which
may result in visual confusion [15]. The visual confusion
caused by two views superimposed binocularly (within two
eye respectively) is binocular visual confusion, which may
lead to binocular rivalry [12], [16]. Previous AR devices
such as Google Glass [17], are mainly constructed based
on binocular visual confusion to avoid the barrage produced
by AR devices. However, the binocular rivalry caused by
binocular visual confusion may strongly affect the QoE.

Monocular visual confusion. The visual confusion caused
by two views superimposed monocularly (within one eye) is
monocular visual confusion [12], which may lead to monocu-
lar rivalry [12], [16]. Since monocular rivalry is much weaker
than binocular rivalry [18] and it possibly occurs only with
extended attention [12], most of recent AR technologies are
built based on monocular visual confusion to avoid occluding,
such as Microsoft HoloLens [19], Magic Leap [20], Epson AR
[21], etc. However, the QoE of monocular visual confusion
still lacks thorough discussion, which is mainly considered in
this paper.

B. Image Quality Assessment

Many IQA methods have been proposed in the past decades
[22], [23], which can be roughly grouped into three categories,
including full reference (FR) IQA, reduced reference (RR)
IQA, and no reference (NR) IQA. Considering the possible
application scene (where the digital contents and real environ-
ment scene can be easily obtained), in this paper, we mainly
focus on the FR-IQA metric.

Classical IQA index. In terms of FR IQA methods, many
classical metrics have been proposed and widely used, in-
cluding mean squared error (MSE), peak signal-to-noise ratio
(PSNR), structural similarity (SSIM) index, feature similarity
(FSIM) index, etc. Regarding NR IQA indexes, there are also
many related works such as natural image quality evaluator
(NIQE) [24], blind quality assessment based on pseudo-
reference image (BPRI) [25], and NR free-energy based robust
metric (NFERM) [26], etc.

Learnable IQA. Driven by the rapid development of deep
neural networks recently, some learning based IQA algorithms

Distribution of  values1234567891010020406080Number0102030405060708090100MOS0102030405060708090Image NumberEntire database0.3<<0.70.4<<0.6Gaussian fitting (=21.07)Gaussian fitting (=16.36)Gaussian fitting (=13.24)4

Fig. 5. Sample images from CFIQA dataset. MOS, SSIM, FSIM values, as well as Œª value are given in the Ô¨Ågure. Note that a MOS in this Ô¨Ågure mean the
MOS of the reference layer in the distorted image.

we randomly select two reference images from these two
groups and mixed them in pair with a blending parameter Œª
to generate a distorted image. This can be formulated as:

ID = Œª ‚ó¶ IR1 + (1 ‚àí Œª) ‚ó¶ IR2 ,

(1)

where IR1 is the reference image from the Ô¨Årst group, IR2
is the reference image from the second group, Œª ‚àà [0, 1]
represents the degradation value of mixing, ID denotes the
generated distorted image. All reference images are resized to
the size of 512 √ó 512 and then superimposed. A total of 300
distorted images are Ô¨Ånally generated.

Obviously, Œª value near 0 or 1 will cause one image to be
unnoticeable while closer to the center (i.e., 0.5) will cause
near confusion for both views. Since visual confusion is the
main consideration in this section, it is unreasonable to ran-
domly sample Œª from [0, 1]. In this work, to make the Œª value
to be closer to the center values in range [0, 1], we sampled
Œª value from a Beta distribution, i.e., Œª ‚àº Beta(Œ±, Œ±). The
parameter Œ± is set as 5 in this dataset. Fig. 2 demonstrates the
distribution of Œª values in our dataset.

the continuous quality bar to select their ratings. They are
seated at a distance of about 2 feet from the monitor, and this
viewing distance is roughly maintained during each session.
All images are shown in their raw sizes, i.e., 512 √ó 512 with
random sequence during the experiment.

Testing procedure. As suggested by ITU [35], at least 15
subjects are required to conduct subjective IQA experiment.
A total of 17 subjects are recruited to participate in the study.
Before participating in the test, each subject read and signed
a consent form which explained the human study. All subjects
are determined to have normal or corrected-to-normal vision.
General information about the study is supplied in printed form
to the subjects, along with instructions on how to participate in
the task. Each subject then experiences a short training session
where 20 confusing images (not included in the formal test)
are shown, allowing them to become familiar with the user
interface and the general visual confusion distortions which
may occur. During the experiment, subjects have enough rest
time every 10 minutes to avoid fatigue.

C. Subjective Data Processing and Analysis

B. Subjective Experiment Methodology

Experiment

setup. A subjective experiment

is con-
ducted on the dataset. There are several subjective assessment
methodologies recommended by the ITU [35], for instance,
single-stimulus (SS), double-stimulus impairment scale (DSIS)
and paired comparison (PC). Since the reference images are
available, we adopted a paired comparison continuous quality
evaluation (PCCQE) strategy to obtain the subjective quality
ratings. As shown in Fig. 3, for each distorted image, we
display its two reference images simultaneously and instruct
the subjects to give two opinion scores of the perceptual
quality of two layer views (i.e., two reference images) in
the distorted image, respectively. We suggest the subjects to
only view the distorted image in the center, and two reference
images are just used to determine the quality of which layer is
being given. Two continuous quality rating bars were presented
to the subject. The quality bar is labeled with Ô¨Åve Likert
adjectives: Bad, Poor, Fair, Good and Excellent, allowing
subjects to smoothly drag a slider (initially centered) along

‚àö

We follow the suggestions given in [35] to conduct the
outlier detection and subject rejection. SpeciÔ¨Åcally, the raw
score for an image is considered to be an outlier if it is
outside 2 standard deviations (stds) about the mean score of
20 stds for the
that image for the Gaussian case or outside
non-Gaussian case. A subject is removed if more than 5% of
his/her evaluations are outliers. As a result, only 1 subject is
rejected, and each image is rated by 16 valid subjects. Among
all scores given by the remaining valid subjects, about 2.77%
of the total subjective evaluations are identiÔ¨Åed as outliers
and are subsequently removed. For the remaining 16 valid
subjects, we convert the raw ratings into Z-scores, which are
then linearly scaled to the range [0, 100] and averaged over
subjects to obtain the Ô¨Ånal mean opinion scores (MOSs) as
follows:

zij =

mij ‚àí ¬µi
œÉi

,

z(cid:48)
ij =

100(zij + 3)
6

,

(2)

(3)

M OSj =

1
N

N
(cid:88)

i=1

z(cid:48)
ij,

MOS: 38.15SSIM: 0.606FSIM: 0.815Œª: 0.61MOS: 44.38SSIM: 0.488FSIM: 0.695MOS: 37.74SSIM: 0.682FSIM: 0.804MOS: 44.70SSIM: 0.567FSIM: 0.725Œª: 0.60MOS: 41.45SSIM: 0.451FSIM: 0.678MOS: 42.47SSIM: 0.593FSIM: 0.807Œª: 0.33MOS: 56.77SSIM: 0.689FSIM: 0.829Œª: 0.55MOS: 63.00SSIM: 0.560FSIM: 0.763MOS: 57.25SSIM: 0.668FSIM: 0.740Œª: 0.42MOS: 59.90SSIM: 0.747FSIM: 0.821MOS: 61.73SSIM: 0.733FSIM: 0.876Œª: 0.63MOS: 55.74SSIM: 0.530FSIM: 0.677MOS: 61.06SSIM: 0.568FSIM: 0.765Œª: 0.41MOS: 39.87SSIM: 0.623FSIM: 0.766MOS: 23.86SSIM: 0.561FSIM: 0.693MOS: 73.67SSIM: 0.780FSIM: 0.873Œª: 0.44MOS: 58.80SSIM: 0.602FSIM: 0.740Œª: 0.53MOS: 32.60SSIM: 0.623FSIM: 0.760Reference 1Reference 2DistortedStrong confusionConfusion but acceptableSuppression5

Fig. 6. Attention based deep feature fusion. xD, xR1 , and xR2 are three extracted feature vectors, respectively. We Ô¨Årst compute the feature distance between
the corresponding feature layers of the distorted image and two reference images. Then each feature distance vector is fed into a specially designed channel
attention module and a one dimensional feature map is output. After weighting by a spatial attention operation, the predicted score is computed. Two kinds of
loss functions are used to constrain the learning process, including the ranking loss (middle), and the score regression loss (top and bottom). All loss functions
are based on the cross-entropy loss.

where mij is the raw rating given by the i-th subject to the
j-th image, ¬µi is the mean rating given by subject i, œÉi is the
standard deviation, and N is the total number of subjects.

Fig. 4 plots the histograms of MOSs over the entire database
as well as within different Œª value ranges, showing a wide
range of perceptual quality scores. We also plot the Gaussian
curve Ô¨Åtting for the histogram. It can be observed that when
Œª is closer to 0.5,
the MOSs tend to be more centered
(illustrated by the smaller œÉ value), but still have a wide range
of perceptual quality scores.

Based on the constructed dataset, we further qualitatively
analyze the characteristic of the visual confusion. As shown
in Fig. 5, we roughly classify the visual confusion into three
categories. The Ô¨Årst category is ‚Äústrong confusion‚Äù, which
means that the mixing of two reference layers will cause
strong confusion and may affect the quality rating of the
superimposed image (distorted image). The second category
is ‚Äúconfusion but acceptable‚Äù, which represents that the visual
confusion caused by the superimposition of two reference
layers is acceptable, or uninÔ¨Çuenced, or the perceptual quality
is even improved. The third category is ‚Äúsuppression‚Äù, which
denotes that in the superimposed image, one reference layer
will suppress another reference layer. This results in the situa-
tion that the perceptual quality of one layer is much better than
another layer. First of all, as a general observation from Fig. 5,
we notice that the MOS values (i.e., subjective quality scores)
are commonly sorted in descending order as: 1) the activated
layer in the ‚Äúsuppression‚Äù category (i.e., the clearer layer), 2)
two image layers in the ‚Äúconfusion but acceptable‚Äù category,
3) two image layers in the ‚Äústrong confusion‚Äù category, and
4) the suppressed layer in the ‚Äúsuppression‚Äù category (i.e.,
the fainter layer). Furthermore, with thorough observation, we
notice that the saliency relation between two layers of the
superimposed image are important to the perceptual quality,
which are demonstrated by several IQA metrics in Section
V. We conclude that without introducing other distortions,
visual confusion itself can signiÔ¨Åcantly inÔ¨Çuence the quality
of confusing images.

IV. ATTENTION BASED DEEP FEATURE FUSION METHOD

As shown in Fig. 5, the classical SSIM index and FSIM
index are inconsistent with human perception in some cases
(see Section V for more quantitative comparison). From the
above analysis, we suppose that
the assessment of visual
confusion is related to both low-level visual characteristics and
high-level visual semantic features, since the visual confusion
may disturb semantic information and affect the perceptual
quality. This may cause the failure of the classical metrics,
since most of them only consider low-level visual features.
As discussed in [30], [31], deep features also demonstrate
great effectiveness as perceptual metrics. Moreover, DNN can
extract both low-level and high-level features. Therefore, in
this paper, we propose an attention based deep feature fusion
metric to measure the visual confusion, which is shown in Fig.
6.

Deep feature extraction. We Ô¨Årst employ several state-
of-the-art pre-trained DNNs to extract both low-level and
high-level features, which include SqueezeNet [36], AlexNet
is an
[37], VGG Net [38], and ResNet [39]. SqueezeNet
extremely lightweight DNN with comparable performance on
classiÔ¨Åcation benchmark. The features from the Ô¨Årst conv
layer and subsequent ‚Äúf ire‚Äù modules in SqueezeNet are
extracted and used. We also use a shallow AlexNet net-
work which may more closely match the architecture of
the human visual cortex [40], and we extract the features
from the conv1 ‚àí conv5 layers in AlexNet. Furthermore, 5
conv layers labeled conv1 2, conv2 2, conv3 3/conv3 4,
conv4 3/conv4 4, conv5 3/conv5 4 are extracted from two
VGG networks (VGG-16 and VGG-19), respectively. We also
compare the correlation between each feature layer and the
MOS (see Section V for more details), and analyze if the
features extracted from different layers would inÔ¨Çuence the
results. Finally, considering the effectiveness of ResNet in a
large amount of computer-vision (CV) tasks, we also explore
the utility of features extracted by ResNet in this task. Three
ResNet architectures including ResNet-18, ResNet-34, as well
as ResNet-50 are considered. We use the features extracted

Normalize SubtractNormalize SubtractChannelAttentionChannelAttentionSpatialAttentionSpatial AttentionAvgAvgCross-EntropyLossCross-EntropyLossCross-EntropyLossùëìùê∑ùëìùëÖ1ùëìùëÖ2ùêø√óùê∂√óùëä√óùêªùêø√óùê∂√óùëä√óùêªùêø√ó1√óùëä√óùêªùêø√ó1√ó1√ó1Feature distanceFeature distance3D feature vector2D feature map1D computed scoreConvolutionSigmoidAverage poolingReLUChannel Attention‚úïùëìùëë1ùëìùëë2ùëë1ùëë2ùë†1ùë†2from the Ô¨Årst conv layer and subsequent ‚ÄúBasicBlock‚Äù or
‚ÄúBottleneck‚Äù modules for all of these architectures.

f l
di

(cid:13) f l

(cid:13)
2
2 ,
(cid:13)

Computing feature distance. As discussed above, for a
distorted image ID and corresponding two reference images
IR1, IR2, we extract feature stacks fD, fR1, and fR2 from
L layers of a network F, respectively. Then we follow the
method in [31] and calculate the feature distance between
the distorted image and the reference image by subtracting
normalized feature stacks. This can be expressed as:
= (cid:13)

D ‚àí f l
Ri
where l ‚àà [1, L] represents the l-th layer, i ‚àà {1, 2} denotes
the reference category, f l
is the calculated feature distance.
di
Channel attention for learning feature signiÔ¨Åcance. Since
the signiÔ¨Åcance of each channel of the feature distance vector
is uncertain for this task, it is important to learn the weights of
the channels for each feature distance vector and re-organize
them. We adopt a widely used channel attention [41] method
as shown in Fig. 6 to learn and re-organize features. After
the channel attention, two down-sampling convolutional layers
are followed. The kernel size of all convolutional layers is
1. Through this manipulation, a 2D feature stack di which
represents the distance map can be obtained for fdi, where
i ‚àà {1, 2}.

(4)

Spatial attention. As discussed in Section III-C, high-level
visual features such as saliency may inÔ¨Çuence the perceptual
quality of visual confusion. Since it is hard to optimize spatial
attention with a relatively small dataset, in this work, we
calculate spatial attention by a saliency prediction method. A
state-of-the-art saliency prediction method [42] is used to to
calculate the spatial attention map Wi for a reference image
IRi. By weighting the distance map di with a scaled spatial
attention map Wi, the Ô¨Ånal quality score can be predicted as:
h,w W l
(cid:80)

i hw (cid:12) dl

si = Avg

(5)

(cid:80)

ihw

) .

l (

h,w W l

i hw

Loss function. Different with [31], which aims at

two
alternative forced choice (2AFC) test, our work focuses on
more general quality assessment task (i.e., mean opinion score
(MOS) prediction). Moreover, different from traditional IQA
condition, in our dataset, one distorted image corresponds to
two reference images. Thus, the loss function needs to be
carefully designed. An intuitive loss function is to compare
(rank) the perceptual qualities of two layers in the distorted
image. Therefore, a ranking loss LR is adopted to predict
the probability that one layer suppress another layer, which
is based on the cross-entropy loss function. Although the
above ranking loss can predict the relative perceptual quality
of two layers in a distorted image, the overall qualities of
different images across the whole dataset are not normalized
and compared. Therefore, another score regression loss LS is
introduced to regress the probability of the quality being bad
or excellent, which is also built based on the cross-entropy
loss function. Two linear layers and a sigmoid layer are used
to project the predicted value to the ground-truth space. The
overall loss function can be formulated as:

L = LS1 + LS2 + ŒªLR,

(6)

6

where Œª is empirically set as 2.

Edge feature integration. The edges of objects can help
identify their categories [43]. However, when two images are
superimposed together, the intersection of the edges of two
image layers may strongly inÔ¨Çuence the perceptual quality.
Therefore, we further extract the features from an edge de-
tection model [43] and concatenate them with the features
extracted from one of the aforementioned classiÔ¨Åcation back-
bones as an enhanced model, which is named CFIQA+.

V. STUDY II: OBJECTIVE CFIQA

A. Experimental Protocol

Experimental settings. As mentioned in Section IV, our
proposed method needs some samples to train the feature
fusion module. Therefore, the CFIQA dataset is split into
two parts at a ratio of 1:1, i.e., each part has 150 distorted
images and corresponding 300 reference images. A two-fold
cross-validation experiment is conducted. The two splits of the
dataset are used as the training set and the test set respectively
in each cross-validation fold. During training,
the feature
extraction network is frozen, and we only train the feature
fusion part. We use Adam as the optimizer [56]. The network
is trained for 100 epoch with a learning rate of 0.0001, and
additional 50 epoch with decayed learning rate from 0.0001
to 0. The batch size is set as 10 during training.

Traditional evaluation metrics. To evaluate the various
quality predictors, we use a Ô¨Åve-parameter logistic function
to Ô¨Åt the quality scores:

‚àí

(7)

1
2

Q(cid:48) = Œ≤1(

) + Œ≤4Q + Œ≤5,

1
1 + eŒ≤2(Q‚àíŒ≤3)
where Q and Q(cid:48) are the objective and best-Ô¨Åtting quality,
Œ≤i(i = 1, 2, 3, 4, 5) are the parameters to be Ô¨Åtted during the
evaluation. Four evaluation metrics are adopted to measure the
consistency between the ground-truth subjective ratings and
the Ô¨Åtted quality scores, including Spearman Rank-order Cor-
relation CoefÔ¨Åcient (SRCC), Kendall Rank-order Correlation
CoefÔ¨Åcient (KRCC), Pearson Linear Correlation CoefÔ¨Åcient
(PLCC), and Root Mean Square Error (RMSE). The evaluation
is conducted on the entire database and 2 sub-datasets within
different Œª ranges. Note that evaluating the performance in
the range that the Œª value is near 0.5 is signiÔ¨Åcant, since it
is near the essence of visual confusion and is more common
in AR applications, while the poor cases in this situation may
strongly inÔ¨Çuence the QoE. However, most FR-IQA metrics
cannot perform well
in this range, which may limit their
practicality on CFIQA.

New evaluation methodology. As a complementary, the
receiver operating characteristic (ROC) analysis methodology
[57], [58] is also adopted for metric evaluation, which is based
on two aspects, i.e., whether two stimuli are qualitatively
different and if they are, which of them is of higher quality. The
Fig. 1. in the supplementary material illustrates the framework
of this evaluation methodology. We Ô¨Årst conduct pair-wise
comparison for all possible image pairs, and then classify them
into pairs with and without signiÔ¨Åcant quality differences.
Then the ROC analysis is used to determine whether various
objective metrics can discriminate images with and without

7

TABLE I
PERFORMANCE COMPARISON OF THE STATE-OF-THE-ART FR-IQA MODELS ON THE CFIQA DATASET. THE DATASET IS SPLIT INTO THREE FOLDS,
INCLUDING ‚ÄúENTIRE‚Äù, ‚Äú0.3< Œª <0.7‚Äù, AND ‚Äú0.4< Œª <0.6‚Äù. WE BOLD THE BEST TWO RESULTS IN EACH GROUP OF MODELS. ‚Äú*‚Äù MEANS A
RE-TRAINED MODEL.

Dataset
Model \ Criteria

SRCC‚Üë KRCC‚Üë

PLCC‚Üë RMSE‚Üì

SRCC‚Üë KRCC‚Üë

PLCC‚Üë RMSE‚Üì

SRCC‚Üë KRCC‚Üë

PLCC‚Üë RMSE‚Üì

Entire

0.3< Œª <0.7

0.4< Œª <0.6

Œª

MSE
PSNR
NQM [44]
SSIM [45]
IFC [46]
VIF [47]
IW-MSE [48]
IW-PSNR [48]
IW-SSIM [48]
FSIM [49]
GSI [50]
GSI [50]
GMSD [51]
GMSM [51]
PAMSE [52]
LTG [53]
VSI [54]

SSIM [45] + saliency
FSIM [49] + saliency
GMSM [51] + saliency

LPIPS (Squeeze) [31]
LPIPS (Alex) [31]
LPIPS (VGG) [31]
LPIPS (VGG) [31] *
DISTS [55]
Baseline (SqueezeNet)
Baseline (AlexNet)
Baseline (VGG-16)
Baseline+ (VGG-16)
Baseline (VGG-19)
Baseline (ResNet-18)
Baseline (ResNet-34)
Baseline (ResNet-50)

CFIQA (SqueezeNet)
CFIQA (AlexNet)
CFIQA (VGG-16)
CFIQA (VGG-19)
CFIQA (ResNet-18)
CFIQA (ResNet-34)
CFIQA+ (ResNet-34)
CFIQA (ResNet-50)

0.7700

0.5881

0.8021

8.2909

0.5976

0.4283

0.6097

8.5426

0.3597

0.2538

0.3698

8.6638

0.6944
0.6944
0.7486
0.7907
0.7198
0.8346
0.7468
0.7468
0.8519
0.8331
0.7904
0.7904
0.7609
0.8117
0.6827
0.7922
0.8297

0.8364
0.8542
0.8339

0.8338
0.8474
0.8376
0.8413
0.7709
0.8625
0.8752
0.8494
0.8629
0.8622
0.8784
0.8823
0.8759

0.8712
0.9053
0.9203
0.9178
0.9196
0.9209
0.9232
0.8964

0.5072
0.5072
0.5613
0.6005
0.5362
0.6491
0.5563
0.5563
0.6715
0.6486
0.6002
0.6002
0.5725
0.6238
0.4965
0.6049
0.6455

0.6520
0.6705
0.6470

0.6468
0.6610
0.6512
0.6549
0.5815
0.6806
0.6987
0.6655
0.6820
0.6798
0.7004
0.7053
0.6963

0.6933
0.7347
0.7550
0.7527
0.7580
0.7604
0.7625
0.7241

0.7215
0.7196
0.7683
0.8179
0.7439
0.8556
0.7777
0.7694
0.8700
0.8540
0.8152
0.8152
0.7894
0.8328
0.7104
0.8160
0.8544

0.8528
0.8711
0.8519

0.8521
0.8612
0.8520
0.8607
0.7967
0.8774
0.8870
0.8663
0.8772
0.8769
0.8920
0.8941
0.8885

0.8827
0.9109
0.9258
0.9229
0.9256
0.9256
0.9278
0.9042

9.6118
9.6399
8.8752
7.9970
9.2430
7.1762
8.7497
8.8837
6.8096
7.2151
8.0506
8.0506
8.5356
7.6853
9.7694
8.0271
7.2150

7.2437
6.8162
7.2608

7.2596
7.0603
7.2607
7.0817
8.4070
6.6574
6.4044
6.9274
6.6615
6.6573
6.2736
6.2185
6.3699

6.5180
5.7467
5.2636
5.3597
5.2713
5.2746
5.1966
5.9441

0.5030
0.5030
0.6012
0.6520
0.5971
0.7266
0.5787
0.5787
0.7512
0.7201
0.6507
0.6507
0.6343
0.6930
0.4855
0.6672
0.7106

0.7228
0.7558
0.7227

0.7278
0.7546
0.7439
0.7508
0.6526
0.7760
0.7967
0.7630
0.7847
0.7804
0.8025
0.8098
0.8011

0.7847
0.8438
0.8681
0.8643
0.8677
0.8703
0.8733
0.8275

0.3448
0.3448
0.4272
0.4660
0.4253
0.5349
0.4019
0.4019
0.5607
0.5268
0.4623
0.4623
0.4533
0.5013
0.3323
0.4806
0.5188

0.5318
0.5584
0.5268

0.5341
0.5615
0.5501
0.5550
0.4658
0.5823
0.6061
0.5683
0.5910
0.5847
0.6079
0.6157
0.6060

0.5910
0.6520
0.6782
0.6762
0.6844
0.6883
0.6897
0.6370

0.5073
0.5076
0.6131
0.6707
0.6118
0.7372
0.5999
0.6008
0.7638
0.7263
0.6656
0.6656
0.6472
0.7022
0.4930
0.6779
0.7256

0.7362
0.7664
0.7336

0.7418
0.7647
0.7535
0.7596
0.6810
0.7822
0.8049
0.7712
0.7921
0.7859
0.8087
0.8137
0.8060

0.7933
0.8461
0.8700
0.8646
0.8688
0.8706
0.8741
0.8311

9.3010
9.2988
8.5048
8.0269
8.5491
7.3064
8.6496
8.6407
6.9567
7.4205
8.0538
8.0538
8.2345
7.6849
9.3900
7.9397
7.4266

7.3157
6.9429
7.3292

7.2346
6.9547
7.0982
7.0379
7.9190
6.7219
6.4086
6.8757
6.5973
6.6754
6.3598
6.2847
6.3992

6.5641
5.7758
5.3412
5.4443
5.3661
5.3293
5.2624
6.0228

0.2816
0.2816
0.4742
0.4858
0.5416
0.6244
0.3524
0.3524
0.6633
0.6296
0.5085
0.5085
0.5678
0.5866
0.2648
0.5731
0.5860

0.5797
0.6628
0.6176

0.6117
0.6583
0.6551
0.6673
0.5253
0.6902
0.7162
0.6792
0.7028
0.7010
0.7227
0.7405
0.7250

0.6904
0.7749
0.8055
0.7987
0.8118
0.8204
0.8226
0.7664

0.1909
0.1909
0.3282
0.3394
0.3881
0.4548
0.2348
0.2348
0.4804
0.4501
0.3516
0.3516
0.3986
0.4122
0.1802
0.4071
0.4175

0.4115
0.4724
0.4354

0.4399
0.4793
0.4772
0.4881
0.3665
0.5046
0.5321
0.4970
0.5167
0.5126
0.5318
0.5490
0.5340

0.5050
0.5833
0.6105
0.6068
0.6272
0.6381
0.6341
0.5774

0.3555
0.3053
0.5150
0.5143
0.5610
0.6505
0.3865
0.3871
0.6856
0.6496
0.5447
0.5447
0.6152
0.6070
0.3306
0.6121
0.6285

0.6063
0.6728
0.6223

0.6708
0.6943
0.7013
0.6950
0.5531
0.7170
0.7364
0.7108
0.7265
0.7327
0.7393
0.7570
0.7447

0.6994
0.7878
0.8168
0.8119
0.8291
0.8310
0.8326
0.7929

8.7250
8.8574
7.9967
7.9994
7.6933
7.0875
8.6132
8.6105
6.7796
7.0917
7.8056
7.8056
7.3588
7.4067
8.8155
7.3762
7.2427

7.4211
6.9110
7.2978

6.9190
6.7094
6.6587
6.7167
7.7848
6.5044
6.3125
6.5686
6.4195
6.3490
6.2907
6.1058
6.2347

6.6646
5.7559
5.3859
5.4520
5.2225
5.1876
5.1592
5.6932

termed ‚ÄúDifferent vs. Similar ROC
signiÔ¨Åcant differences,
Analysis‚Äù. Next, the image pairs with signiÔ¨Åcant differences
are classiÔ¨Åed into pairs with positive and negative differences,
and the ROC analysis is used to test if various objective
metrics can distinguish images with positive and negative
differences, termed ‚ÄúBetter vs. Worse ROC Analysis‚Äù. The area
under the ROC curve (AUC) values of two analysis are mainly
reported in this paper, of which the higher values indicate
better performance.

B. Comparisons with State-of-the-arts

We conduct a large-scale benchmark study based on our
dataset
to further discuss the perceptual quality of visual
confusion. The performance comparison results are shown in
Table I.

Classical FR-IQA metrics. The Ô¨Årst intuitive idea is that if
the Œª value can act as the metric for visual confusion since
it is directly related to the generation of distorted images.
its performance and show the results in Table I.
We test

We also test 16 state-of-the-art classical FR on this dataset,
including MSE, PSNR, NQM [44], SSIM [45], IFC [46], VIF
[47], IW-MSE [48], IW-PSNR [48], IW-SSIM [48], FSIM
[49], GSI [50], GMSD [51], GMSM [51], PAMSE [52], LTG
[53], and VSI [54]. Since visual attention is important in
this task as discussed above, we further select 3 widely used
and well-performed metrics (SSIM, FSIM, and GMSM), and
incorporate saliency weights into the quality pooling as new
metrics. The results are denoted as ‚Äú+ saliency‚Äù in Table I.

Deep feature based IQA metrics. Recently, many works
demonstrate the consistence between DNN and human per-
ception [30], [31], [59]. In the benchmark study, we Ô¨Årst
build several baseline models with several state-of-the-art
neural networks, including SqueezeNet [36], AlexNet [37],
VGG (VGG-16 and VGG-19) [38], and ResNet (ResNet-18,
ResNet-34, and ResNet-50) [39]. These baseline models are
constructed by averaging the features of selected layers as
mentioned in Section IV. Considering the features extracted
from the last layer of each ‚Äúcomponent‚Äù of these networks

TABLE II
IMPACT OF DIFFERENT COMPONENTS. (BACKBONE: VGG-16. ‚ÄúCA‚Äù MEANS ‚ÄúCHANNEL ATTENTION‚Äù. ‚ÄúSA‚Äù MEANS ‚ÄúSPATIAL ATTENTION‚Äù).

Dataset
Model \ Criteria

only ranking loss
w/o CA & SA
w/o SA
w/o CA
w/o ranking loss
w/o scoring loss
all combined

SRCC‚Üë KRCC‚Üë

PLCC‚Üë

RMSE‚Üì

SRCC‚Üë KRCC‚Üë

PLCC‚Üë

RMSE‚Üì

SRCC‚Üë KRCC‚Üë

PLCC‚Üë

RMSE‚Üì

Entire

0.3< Œª <0.7

0.4< Œª <0.6

0.8413
0.8647
0.8797
0.9160
0.9069
0.9139
0.9203

0.6549
0.6830
0.7015
0.7490
0.7363
0.7466
0.7550

0.8607
0.8794
0.8917
0.9216
0.9139
0.9195
0.9258

7.0817
6.5994
6.2746
5.4075
5.6435
5.4042
5.2636

0.7508
0.7834
0.8046
0.8618
0.8473
0.8600
0.8681

0.5550
0.5875
0.6094
0.6711
0.6545
0.6697
0.6782

0.7596
0.7885
0.8094
0.8612
0.8469
0.8618
0.8700

7.0379
6.6417
6.3477
5.5065
5.7586
5.4442
5.3412

0.6673
0.6973
0.7216
0.7925
0.7715
0.7917
0.8055

0.4881
0.5122
0.5326
0.5982
0.5746
0.5982
0.6105

0.6950
0.7306
0.7458
0.8039
0.7864
0.8076
0.8168

6.7167
6.3735
6.2209
5.5480
5.7731
5.4694
5.3859

8

Fig. 7. New criteria performance of 19 state-of-art FR IQA models and the proposed metric on the CFIQA database. Left two Ô¨Ågures are the different vs.
similar ROC analysis results. Right two Ô¨Ågures are the better vs. worse analysis results. Note that a white/black square in the signiÔ¨Åcance Ô¨Ågures means the
row metric is statistically better/worse than the column one. A gray square means the row method and the column method are statistically indistinguishable.
The backbone of all networks in these Ô¨Ågures is VGG-16.

may not well reÔ¨Çect the overall performance, in this work, we
propose a method to improve the baseline, which is conducted
based on VGG-16 (denoted as ‚ÄúVGG-16 (baseline+)‚Äù in Table
I). We analyze the SRCC between all 30 layers of VGG-16 and
the MOSs, then extract 5 most correlated layers and compute
their average score as the ‚Äúbaseline+‚Äù. Furthermore, we also
compute the predicted score of two widely used metrics based
on deep feature fusion [31], [55], which are denoted as LPIPS
and DISTS in Table I.
C. Performance Analysis

Results analysis. First of all, it is important to analyze the
performance of all IQA models within different Œª ranges. We
notice that with Œª values closer to 0.5 (i.e., the probability of
causing strong visual confusion increases), nearly all metrics
tend to perform worse. This indicates that the assessment of
strong visual confusion is a difÔ¨Åcult task for most models. As
shown in Table I, Œª can perform as the metric for confusion
evaluation, and even acts better than MSE and PSNR, though
the performance is still limited. Among classical IQA indexes,
IW-SSIM and VIF show the top performances, which denotes
that visual information weighting possibly helps the assess-
ment of visual confusion. The improvements of introducing
saliency into SSIM, FSIM, as well as GMSM demonstrate the
importance of visual attention in visual confusion, which is
worth further and deeper research. Surprisingly, the baseline
deep features show good consistence with human perceiving
on the entire dataset, though they are not well performed
on either of the two sub-datasets. Unexpectedly, the widely
used deep metric LPIPS [31] performs even worse than the
baseline methods, which may indicate that visual confusion
type of degradation compared to other dis-
is a different
tortions. Finally, our method gets relative better results and
different backbone architectures show different optimization
trends, which denotes that the feature extraction network is
also important. Future studies on exploring different feature
extraction methods are also needed.

Fig. 7 illustrates the performance evaluated by the new
criteria on the CFIQA database. First, we observe that the
proposed CFIQA model signiÔ¨Åcantly outperforms other state-
of-the-art models on Different vs. Similar Analysis and Better
vs. Worse Analysis by a large margin. Furthermore, we notice
the AUC values of the CFIQA metric on the Better vs.
Worse classiÔ¨Åcation task are higher than the Different vs.
Similar classiÔ¨Åcation task, which indicates that the Different
vs. Similar classiÔ¨Åcation is a more hard task and there is still
room for improvement in this classiÔ¨Åcation task.

Impact of different components. We further verify the
impact of each component in our method. The analysis is
conducted based on VGG-16 and the results are shown in
Table II. We Ô¨Årst remove all components including channel
attention, spatial attention and projection components (two-
layer MLP), and only regress the weighting layers for fea-
ture fusion. The results shown in the Ô¨Årst row in Table II
demonstrate that the performance of this method is similar
to the baseline in Table I. Then we compare the impacts
of channel attention and spatial attention modules for feature
fusion. It can be observed that spatial attention takes the most
contribution to the Ô¨Ånal score and channel attention contributes
less. Furthermore, we compare the contributions of two loss
functions. It can be observed that ranking loss contributes most
for constraining the optimization process. Though contributing
less, score regression loss still introduces improvement to the
Ô¨Ånal score.

VI. STUDY III: SUBJECTIVE & OBJECTIVE ARIQA
In the above Study I and Study II, we have discussed a
relatively more basic and more general problem, i.e., visual
confusion and its inÔ¨Çuence on the perceptual QoE. As afore-
mentioned, visual confusion has signiÔ¨Åcant inÔ¨Çuence on the
QoE of human vision. However, the situation in the above
studies is quite different with the real AR applications, which
is mainly attributed to the fact that in actual AR applications,
the FOV of the AR scenes is usually smaller than the FOV of

PSNRNQMMSSSIMSSIMIFCVIFIWMSEIWPSNRIWSSIMFSIMGSIGMSDGMSMPAMSELTGVSILPIPSDISTSBaselineCFIQA0.650.70.750.80.85AUC (-)Different/SimilarSignificancePSNRNQMMSSSIMSSIMIFCVIFIWMSEIWPSNRIWSSIMFSIMGSIGMSDGMSMPAMSELTGVSILPIPSDISTSBaselineCFIQAPSNRNQMMSSSIMSSIMIFCVIFIWMSEIWPSNRIWSSIMFSIMGSIGMSDGMSMPAMSELTGVSILPIPSDISTSBaselineCFIQAPSNRNQMMSSSIMSSIMIFCVIFIWMSEIWPSNRIWSSIMFSIMGSIGMSDGMSMPAMSELTGVSILPIPSDISTSBaselineCFIQA0.90.920.940.960.981AUC (-)Better/WorseSignificancePSNRNQMMSSSIMSSIMIFCVIFIWMSEIWPSNRIWSSIMFSIMGSIGMSDGMSMPAMSELTGVSILPIPSDISTSBaselineCFIQAPSNRNQMMSSSIMSSIMIFCVIFIWMSEIWPSNRIWSSIMFSIMGSIGMSDGMSMPAMSELTGVSILPIPSDISTSBaselineCFIQA9

Fig. 8. The illustration of the AR simulation in VR environment. (a) The demonstration of the relationship between the omnidirectional image, the AR image,
and the perceptual viewport image. (b) The omnidirectional images are used as the background scenes, which include outdoor and indoor scenarios. (c) The
AR images are composed of three types of content including web page images, natural images, and graphic images. (d) The perceptual viewport images are
generated by superimposing the AR images on the omnidirectional images (here Œª = 0.58). Note that the perceptual viewports of the subjects are changed
dynamically with the head movement, however, the relative positional relationship between the omnidirectional image and the AR image is Ô¨Åxed.

Fig. 9. Histogram of MOSs from the ARIQA dataset.

the real scenes [60]. Thus, we further conduct another subjec-
tive and objective IQA study towards evaluating the perceptual
quality of AR contents in real-world AR applications.

A. Subjective ARIQA

Subjective experiment methodology. An intuitive way to
conduct subjective AR experiment is wearing AR devices in
various environments and then collecting subjective scores.
However, this way suffers from uncontrollable experimental
environments and limited experimental scenarios [10], e.g.,
the head movement may cause different collected background
images for different users, and it is hard to introduce various
background scenarios in lab environment. Therefore, we adopt
the method of conducting subjective AR-IQA studies in VR
environment for controllable experimental environments and
diverse experimental scenarios.

Fig. 8 illustrates the methodology of the subjective exper-
iment in this ARIQA study. First of all, 20 omnidirectional
images are collected as the background scenes including 10
indoor scenarios and 10 outdoor scenarios. Considering the
real applications of AR, we further collect 20 images as the
reference AR contents, which include 8 web page images, 8
natural images, and 4 graphic images. The resolution of all raw
AR images is 1440√ó900. We generate a much larger set of dis-
torted AR contents by applying quality degradation processes
that would occur in AR applications. Three distortion types

Fig. 10. Distribution of MOS values of raw images, JPEG compressed
images, rescaled images superimposed on the omnidirectional backgrounds
with different mixing values. The mixing values Œª1, Œª2, Œª3, Œª4 are equal to
0.26, 0.42, 0.58, 0.74, respectively.

Fig. 11. Samples of MOS values of raw images, contrast adjusted images
superimposed on the omnidirectional backgrounds with different mixing
values. ‚ÄúN‚Äù denotes negative gamma transfer, ‚ÄúP‚Äù represents positive gamma
transfer.

including image compression, image scaling, image contrast
adjustment, are introduced as follows. 1) JPEG compression
is a widely used method in image compression, and have
been introduced into many quality assessment databases [61].
We set the quality level of the JPEG compression at the two
levels with quality parameters 7 and 3. 2) Image scaling is
widely used in modern video streaming systems, where videos
are often spatially downsampled prior to transmission, and
then upscaled prior to display [62]. Such image scaling can
also simulate the distortions introduced by various resolutions
of AR devices. We create distorted images by downsclaing
original images to 1/5 and 1/10 of the original resolution,
then spatially upscaling them back to the original resolution.

zxyOmnidirectional imageOutdoorIndoorWeb pageNaturalGraphicOutdoorAR imagePerceptual viewportSuperimposedSuperimposedSuperimposed(a) Demonstration of the simulation(b) Omnidirectional image(c) AR image(d) Perceptual viewport2030405060708090MOS020406080100Image NumberRawJPEG1JPEG2Scaling1Scaling2RawJPEG1JPEG2Scaling1Scaling2RawJPEG1JPEG2Scaling1Scaling2RawJPEG1JPEG2Scaling1Scaling2020406080100MOSWebpageNaturalGraphicŒª1Œª2Œª3Œª4Contrast(N)RawContrast(P)Contrast(N)RawContrast(P)Contrast(N)RawContrast(P)Contrast(N)RawContrast(P)020406080100MOSWebpageNaturalGraphicŒª1Œª2Œª3Œª43) Image contrast adjustment
is also an important factor
affecting the human visual perception and has been commonly
introduced into natural IQA [63] and screen content IQA [61].
We also use the gamma transfer function [63] to adjust the
contrast, which is deÔ¨Åned as y = [x ¬∑ 255((1/n)‚àí1)]n, where
n = [1/4, 4] (n < 1 is negative gamma transfer, n > 1
is positive gamma transfer). Hence, for each AR image, we
generate 6 degraded images.

Since the visual confusion strongly inÔ¨Çuence the human
visual perception as aforementioned, we further introduce
the superimposition degradation. We design a program using
Unity [64] to perform the experimental procedure, including
stimuli display, data collection, etc. We Ô¨Årst randomly match
the 20 AR images and 20 omnidirectional images in pairs to
generate 20 scenarios. Hence, for each omnidirectional image,
we have 7 AR images superimposed on it (1 reference image
+ 6 distorted images). During the experiment, the perceptual
viewport can be formulated as:

IS = Œª ‚ó¶ IA + (1 ‚àí Œª) ‚ó¶ IO ,

(8)

where IS denotes the perceptual viewport, i.e., the super-
imposed image, IA represents the AR image, IO indicates
the omnidirectional image, and Œª ‚àà [0.26, 0.42, 0.58, 0.74]
denotes the mixing value used in the experiment, i.e., we
have four superimposing levels in this subjective experiment.
Overall, 560 experimental stimuli are generated for conduct-
ing the subjective experiment (20 scenarios √ó 7 levels √ó
4 mixing values). As demonstrated in Fig. 8 (a),
the
omnidirectional
image is displayed in 360 degrees as the
background scenarios, the AR image is superimposed on the
omnidirectional image which is perceived as the perceptual
viewport. Fig. 8 (b), (c) and (d) present the examples of the
omnidirectional images, the AR images, and the perceptual
viewport images, respectively.

A total of 23 subjects participate in the experiment, who
are not included in the aforementioned CFIQA study. All
subjects are recruited through standard procedures similar
to that described in Section III-B. Before the formal test,
each subject experiences a short training session where 28
stimuli are shown. The same distortion generation procedure
is conducted for the training stimuli as for the test stimuli, and
these training stimuli are not included in the test session. Since
the experiment is conducted under VR-HMD environment,
the single-stimulus (SS) strategy is adopted to collect the
subjective quality ratings of AR images. A 10-point numerical
categorical rating method is used to facilitate the subjective
rating in HMD [32]. We use HTC VIVE Pro Eye [65] as the
HMD on account of its excellent graphics display technology
and high precision tracking ability. During the formal test, all
560 experimental stimuli are displayed in a random order for
each subject.

Subjective data processing and analysis. Similar to the
procedure in Section III-C, we Ô¨Årst process the collected
subjective scores to obtain the MOSs. Only 1 subject
is
rejected, and each image is rated by 22 valid subjects. Among
all scores given by the remaining valid subjects, about 3.21%
of the total subjective evaluations are identiÔ¨Åed as outliers and
removed. Fig. 9 plots the histogram of MOSs over the entire

10

ARIQA database, showing a wide range of perceptual quality
scores.

We further analyze the distribution of MOS values across
different mixing values and various distortions. Fig. 10 shows
the MOS distribution of the images with the degradations of
JPEG compression and image scaling under different mixing
values. We notice that as the Œª value increases, the MOS
value also shows an overall upward trend, of which the reason
is apparent since larger Œª value means clearer AR content.
SpeciÔ¨Åcally, for the superimposition of raw AR images and
background images, we Ô¨Ånd that graphic images provide better
QoE than web page images, and web page images provide
better QoE than natural images in general. It may reveal that
relatively simple AR contents can provide better QoE than
complex AR contents. Moreover, for the superimposed AR
images with JPEG compression and scaling, we notice that
when the mixing value Œª is relatively smaller, the MOSs of
these images are closer to that of superimposed raw images,
though the overall MOSs are smaller than that of the larger Œª
values. It may reveal that the superimposition degradation is
a more inÔ¨Çuential quality factor compared to other distortions
when the Œª value is relatively small. However, it also means
that the superimposition degradation can hide other distortions.
Fig. 11 plots several examples of the MOS values of raw
images and contrast adjusted images superimposed on the
omnidirectional backgrounds with different mixing values,
which shows that appropriate contrast adjustment may even
improve the perceptual quality of AR contents.
B. Objective ARIQA

Benchmark. We Ô¨Årst conduct an objective IQA benchmark
study on the ARIQA dataset. As discussed in Section III
and Section VI-A, the visual confusion may affect the human
visual perception and may degrade the QoE. However, whether
the IQA metrics should consider the superimposed image, the
AR image, and the background image together still needs to
be discussed. Therefore, three variants are introduced in this
benchmark study. We assume the background image, the AR
image, as well as the mixing value are known, which can be
acquired in real applications, and the superimposed image can
be correspondingly calculated. Let IAD denotes the AR image
with distortions, IAR denotes the raw reference AR image, IB
indicates the background image, Œª represents the mixing value,
hence, the displayed AR image IA and the perceptual view-
port image (superimposed image) IS can be correspondingly
expressed as: IA = Œª ¬∑ IAD , and IS = Œª ¬∑ IAD + (1 ‚àí Œª) ¬∑ IB,
respectively. Then, three FR-IQA variants of AR IQA metrics
are deÔ¨Åned as: Type I, the similarity between the displayed
AR image IA and the reference AR image IAR ; Type II, the
similarity between the perceptual viewport image IS and the
reference AR image IAR ; Type III, the SVR fusion [67] of
the similarity between the perceptual viewport image IS and
the reference AR image IAR , and the similarity between the
perceptual viewport image IS and the background image IB.
These three variant types can be expressed as:

QType I = FR(IA, IAR ),
QType II = FR(IS, IAR ),
QType III = SVR(FR(IS, IAR ), FR(IS, IB)),

(9)

(10)

(11)

11

TABLE III
PERFORMANCE OF THE THREE VARIANTS OF THE STATE-OF-THE-ART FR-IQA MODELS ON THE ARIQA DATASET. THE TOP 3 RESULTS OF ALL THREE
VARIANTS ARE IN BOLD FOR EACH GROUP. THE PERFORMANCE CHANGES COMPARED TO TYPE I IN TERMS OF SRCC ARE INDICATED IN GRAY FONTS

Method
Model \ Criteria

Type I
SRCC‚Üë KRCC‚Üë PLCC‚Üë RMSE‚Üì

PSNR
NQM [44]
MS-SSIM [66]
SSIM [45]
IFC [46]
VIF [47]
IW-MSE [48]
IW-PSNR [48]
IW-SSIM [48]
FSIM [49]
GSI [50]
GMSD [51]
GMSM [51]
PAMSE [52]
LTG [53]
VSI [54]

LPIPS (Squeeze) [31]
LPIPS (Alex) [31]
LPIPS (VGG) [31]
DISTS [55]
Baseline (SqueezeNet)
Baseline (AlexNet)
Baseline (VGG-16)
Baseline+ (VGG-16)
Baseline (VGG-19)
Baseline (ResNet-18)
Baseline (ResNet-34)
Baseline (ResNet-50)

0.2197
0.4101
0.6118
0.5327
0.3539
0.5981
0.2287
0.2287
0.6431
0.6323
0.4393
0.6485
0.6386
0.2162
0.6592
0.5190

0.5924
0.5870
0.5436
0.5011
0.5733
0.5273
0.5541
0.6167
0.5612
0.5438
0.5426
0.5753

0.1485
0.2813
0.4414
0.3799
0.2456
0.4273
0.1555
0.1555
0.4663
0.4546
0.3046
0.4718
0.4628
0.1458
0.4830
0.3691

0.4326
0.4273
0.3828
0.3583
0.4166
0.3776
0.3908
0.4454
0.3981
0.3892
0.3862
0.4113

0.2742
0.4268
0.6483
0.5551
0.3294
0.6366
0.2966
0.2998
0.6532
0.6723
0.5034
0.6759
0.6907
0.2736
0.6826
0.5926

0.6160
0.6314
0.5593
0.5280
0.6096
0.5814
0.5706
0.5649
0.5790
0.5750
0.5771
0.5977

12.733
11.974
10.080
11.013
12.501
10.211
12.644
12.631
10.026
9.8010
11.440
9.7575
9.5745
12.735
9.6759
10.665

10.430
10.267
10.975
11.244
10.496
10.772
10.873
10.925
10.795
10.832
10.813
10.615

SRCC‚Üë

0.0064 (‚àí0.2133)
0.5348 (+0.1247)
0.6557 (+0.0439)
0.5399 (+0.0072)
0.5121 (+0.1582)
0.6927 (+0.0946)
0.2406 (+0.0119)
0.2406 (+0.0119)
0.7103 (+0.0672)
0.6538 (+0.0215)
0.3788 (‚àí0.0605)
0.5947 (‚àí0.0537)
0.5863 (‚àí0.0523)
0.0090 (‚àí0.2072)
0.6469 (‚àí0.0123)
0.6096 (+0.0906)

0.6260 (+0.0336)
0.6306 (+0.0436)
0.6202 (+0.0766)
0.5112 (+0.0101)
0.6339 (+0.0606)
0.6450 (+0.1177)
0.6368 (+0.0827)
0.6793 (+0.0626)
0.6561 (+0.0949)
0.6467 (+0.1029)
0.6603 (+0.1177)
0.6510 (+0.0757)

Type II
KRCC‚Üë PLCC‚Üë RMSE‚Üì

0.0027
0.3772
0.4778
0.3797
0.3523
0.5009
0.1689
0.1689
0.5267
0.4716
0.2606
0.4346
0.4142
0.0048
0.4742
0.4318

0.4450
0.4457
0.4426
0.3627
0.4570
0.4690
0.4585
0.4979
0.4750
0.4678
0.4782
0.4688

0.0592
0.5550
0.6609
0.5411
0.5105
0.6869
0.2956
0.2895
0.7100
0.6528
0.3890
0.5959
0.5923
0.0659
0.6422
0.6167

0.6251
0.6352
0.6141
0.5528
0.6358
0.6578
0.6372
0.6814
0.6530
0.6451
0.6660
0.6464

13.217
11.014
9.9366
11.134
11.385
9.6218
12.648
12.673
9.3231
10.029
12.197
10.633
10.667
13.211
10.150
10.422

10.334
10.226
10.450
11.033
10.220
9.9728
10.204
9.6902
10.028
10.117
9.8765
10.102

SRCC‚Üë

0.3809 (+0.1612)
0.5588 (+0.1487)
0.6660 (+0.0541)
0.6090 (+0.0763)
0.5090 (+0.1551)
0.7227 (+0.1245)
0.4126 (+0.1839)
0.3559 (+0.1272)
0.7116 (+0.0685)
0.6663 (+0.0340)
0.4245 (‚àí0.0147)
0.6730 (+0.0245)
0.6294 (‚àí0.0092)
0.3657 (+0.1495)
0.6764 (+0.0172)
0.6321 (+0.1131)

0.6417 (+0.0494)
0.6626 (+0.0757)
0.6373 (+0.0936)
0.6334 (+0.1323)
0.6272 (+0.0539)
0.6460 (+0.1187)
0.6587 (+0.1046)
0.6815 (+0.0649)
0.6613 (+0.1001)
0.6485 (+0.1047)
0.6710 (+0.1284)
0.6619 (+0.0866)

Type III
KRCC‚Üë PLCC‚Üë RMSE‚Üì

0.2662
0.4031
0.4914
0.4430
0.3601
0.5351
0.2906
0.2574
0.5337
0.4865
0.3056
0.4973
0.4587
0.2558
0.4998
0.4590

0.4693
0.4820
0.4606
0.4608
0.4573
0.4768
0.4805
0.5036
0.4838
0.4779
0.4959
0.4831

0.4154
0.5867
0.6741
0.6233
0.5217
0.7222
0.4586
0.4151
0.7201
0.6774
0.4584
0.6801
0.6422
0.4093
0.6818
0.6484

0.6660
0.6767
0.6475
0.6580
0.6493
0.6707
0.6622
0.6896
0.6674
0.6702
0.6903
0.6736

11.901
10.677
9.6721
10.276
11.172
9.2024
11.693
11.879
9.0193
9.5764
11.680
9.5815
10.064
11.941
9.4727
10.039

9.8086
9.6071
9.9848
9.7866
9.8747
9.7499
9.8906
9.4713
9.6720
9.7504
9.4826
9.7163

TABLE IV
PERFORMANCE OF FOUR TRAINABLE MODELS.

Model \ Criteria

SRCC‚Üë KRCC‚Üë

PLCC‚Üë RMSE‚Üì

LPIPS
CFIQA
ARIQA
ARIQA+

0.7624
0.7787
0.7902
0.8124

0.5756
0.5863
0.5967
0.6184

0.7591
0.7695
0.7824
0.8136

8.6935
8.5484
8.3295
7.8018

where QType I, QType II, and QType III denote the quality predic-
tions of the three variants, SVR indicates the support vector
regression deployment.

In terms of our ARIQA dataset, the background image
IB (i.e., viewport of the omnidirectional
image IO) and
the superimposed image IS are captured in Unity, then the
benchmark results are calculated using the aforementioned
methods. Table III presents the performance of the three AR-
IQA metric variants derived from the state-of-the-art FR-IQA
models on the ARIQA dataset. Comparing Type I and Type II,
we notice that for most FR-IQA metrics, using superimposed
images as distorted images can improve the performance of the
algorithm. In addition, as shown in the comparison between
Type III and Type I, when superimposed images, AR images,
the
as well as background images are jointly considered,
performance of almost all FR-IQA metrics can be further
improved.

ARIQA. Based on the aforementioned analysis, using super-
imposed images as the perceptual distorted image, and consid-
ering their similarity with both AR references and background
references may be more effective for evaluating the perceptual
quality of the AR layers. Hence, as demonstrated in Figure
12, we further modify the CFIQA model to an ARIQA model
for better evaluating the perceptual quality of AR contents.

Fig. 12. The framework of the proposed ARIQA model.

Different from CFIQA, the goal for the ARIQA is to predict
the perceptual quality of AR contents rather than both two
views, therefore, the two output results of CFIQA are fused
to predict the AR image quality. Considering the effectiveness
of the training objectives of the LPIPS [31] and our CFIQA,
during the training process, two pathways are introduced to
ARIQA for comparing the perceptual quality of different
distorted images of the one AR and background reference pair.
For fair comparison, we further re-train the LPIPS and CFIQA
models only using AR image as the reference image, which
is similar to the concept of Type II described above. Note
that the CFIQA model here is a modiÔ¨Åed version, since the
original CFIQA aims to compare the similarity between two
reference images for one superimposed image, while here we
focus on comparing two superimposed images for one AR
reference, which is more similar to the concept of LPIPS. We
also improve the ARIQA by incorporating the features from
the RCF net [43] as the ARIQA+, which is similar to the way
of the aforementioned CFIQA+.

Superimposed  Image 1AR ImageBG ImageCFIQACFIQASuperimposed  Image 2Cross-Entropy LossAR ImageBG ImageScore 11Score 12AR Quality 1Score 21Score 22AR Quality 2Cross-Entropy LossCross-Entropy Loss12

Fig. 13. New criteria performance of 19 state-of-art FR IQA models and the proposed metric on the ARIQA database. Left two Ô¨Ågures are the different
vs. similar ROC analysis results. Right two Ô¨Ågures are the better vs. worse analysis results. The black/white/gray squares in the signiÔ¨Åcance Ô¨Ågures have the
same meaning with that in Fig. 7

We conduct a Ô¨Åve-fold cross validation experiment on the
ARIQA dataset. For each fold, we split the 560 samples into
280 training samples and 280 testing samples without scene
repeating, i.e., 280 training samples and 280 testing samples
corresponding to different 10 AR/BG pairs, respectively. Table
IV shows the averaged performance of these four models
after Ô¨Åve-fold cross validation. It can be observed that the
ARIQA model achieves better performance than the LPIPS
model and the CFIQA model, and the ARIQA+ achieves
the best performance compared to other models. Fig. 13
illustrates the performance evaluated by the new criteria on
the ARIQA database. We notice that the proposed ARIQA
model signiÔ¨Åcantly outperforms other state-of-the-art models
on Different vs. Similar Analysis and Better vs. Worse Analysis
by a large margin.

VII. CONCLUSION

In this paper, we discuss several AR devices and appli-
cations (see Section II-A), and clarify the essential theory
underlying AR, i.e., visual confusion. A more general problem
underlying AR QoE assessment is Ô¨Årst proposed, which is
evaluating the perceptual quality of superimposed images, i.e.,
confusion image quality assessment. To this end, we build
a confusing image quality assessment (CFIQA) dataset, and
conduct subjective and objective image quality assessment
studies based on it. A CFIQA model is also proposed for better
evaluating the perceptual quality of visual confusion. The re-
sults show that without extra degradation, the visual confusion
itself can signiÔ¨Åcantly inÔ¨Çuence the perceptual quality of the
superimposed images, and state-of-the-art FR-IQA metrics are
not well performed, especially when the mixing value is closer
to 0.5. The proposed CFIQA model performs better on this
task. Moreover, in order to better study the inÔ¨Çuence of visual
confusion on the perceptual quality of AR images in real
application scenarios, an ARIQA study is further conducted,
which includes the construction of an ARIQA dataset, an
objective model evaluation experiment for ARIQA, as well as
an ARIQA model proposed to solve this problem. The results
show that it is beneÔ¨Åcial to consider visual confusion when
designing IQA models for AR, and our proposed ARIQA
model achieves better performance compared to other state-of-
the-art methods. We hope this work can help other researchers
have a better understanding of the visual confusion mechanism
underlying AR technology. There are many issues related
to visual confusion or AR QoE assessment that need to be
explored in the future. Several key aspects are discussed in
the supplementary material.

REFERENCES

[1] O. Cakmakci and J. Rolland, ‚ÄúHead-worn displays: a review,‚Äù Journal

of display technology, vol. 2, no. 3, pp. 199‚Äì216, 2006.

[2] T. Zhan, K. Yin, J. Xiong, Z. He, and S.-T. Wu, ‚ÄúAugmented reality
and virtual reality displays: Perspectives and challenges,‚Äù Iscience, p.
101397, 2020.

[3] J. Guo, V. Vidal, I. Cheng, A. Basu, A. Baskurt, and G. Lavoue,
‚ÄúSubjective and objective visual quality assessment of textured 3d
meshes,‚Äù ACM Transactions on Applied Perception (TAP), vol. 14, no. 2,
pp. 1‚Äì20, 2016.

[4] H. Su, Z. Duanmu, W. Liu, Q. Liu, and Z. Wang, ‚ÄúPerceptual quality
assessment of 3d point clouds,‚Äù in Proceedings of the IEEE International
Conference on Image Processing (ICIP), 2019, pp. 3182‚Äì3186.

[5] E. Alexiou, T. Ebrahimi, M. V. Bernardo, M. Pereira, A. Pinheiro, L. A.
D. S. Cruz, C. Duarte, L. G. Dmitrovic, E. Dumic, D. Matkovics et al.,
‚ÄúPoint cloud subjective evaluation methodology based on 2d rendering,‚Äù
in Proceedings of the International Conference on Quality of Multimedia
Experience (QoMEX), 2018, pp. 1‚Äì6.

[6] J. Zhang, W. Huang, X. Zhu, and J.-N. Hwang, ‚ÄúA subjective quality
evaluation for 3d point cloud models,‚Äù in Proceedings of the Interna-
tional Conference on Audio, Language and Image Processing, 2014, pp.
827‚Äì831.

[7] E. Zerman, P. Gao, C. Ozcinar, and A. Smolic, ‚ÄúSubjective and objec-
tive quality assessment for volumetric video compression,‚Äù Electronic
Imaging, vol. 2019, no. 10, pp. 323‚Äì1, 2019.

[8] E. Alexiou, E. Upenik, and T. Ebrahimi, ‚ÄúTowards subjective quality
assessment of point cloud imaging in augmented reality,‚Äù in Proceedings
of the IEEE International Workshop on Multimedia Signal Processing
(MMSP), 2017, pp. 1‚Äì6.

[9] L. Zhang, H. Dong, and A. El Saddik, ‚ÄúTowards a qoe model to evaluate
holographic augmented reality devices,‚Äù IEEE MultiMedia, vol. 26,
no. 2, pp. 21‚Äì32, 2018.

[10] J. Guti¬¥errez, T. Vigier, and P. L. Callet, ‚ÄúQuality evaluation of 3d objects
in mixed reality for different lighting conditions,‚Äù Electronic Imaging,
vol. 2020, no. 11, pp. 128‚Äì1, 2020.

[11] R. L. Woods, R. G. Giorgi, E. L. Berson, and E. Peli, ‚ÄúExtended
wearing trial of triÔ¨Åeld lens device for ‚Äòtunnel vision‚Äô,‚Äù Ophthalmic and
physiological optics, vol. 30, no. 3, pp. 240‚Äì252, 2010.

[12] E. Peli and J.-H. Jung, ‚ÄúMultiplexing prisms for Ô¨Åeld expansion,‚Äù
the American

Optometry and vision science: ofÔ¨Åcial publication of
Academy of Optometry, vol. 94, no. 8, p. 817, 2017.

[13] E. Ahn, S. Lee, and G. J. Kim, ‚ÄúReal-time adjustment of contrast
saliency for improved information visibility in mobile augmented re-
ality,‚Äù Virtual Reality, vol. 22, no. 3, pp. 245‚Äì262, 2018.

[14] D. Kalkofen, E. Veas, S. Zollmann, M. Steinberger, and D. Schmalstieg,
‚ÄúAdaptive ghosted views for augmented reality,‚Äù in Proceedings of
the IEEE International Symposium on Mixed and Augmented Reality
(ISMAR), 2013, pp. 1‚Äì9.

[15] H. Apfelbaum and E. Peli, ‚ÄúTunnel vision prismatic Ô¨Åeld expansion:
challenges and requirements,‚Äù Translational vision science & technol-
ogy, vol. 4, no. 6, pp. 8‚Äì8, 2015.

[16] R. Blake and N. K. Logothetis, ‚ÄúVisual competition,‚Äù Nature Reviews

Neuroscience, vol. 3, no. 1, pp. 13‚Äì21, 2002.
[17] Google Glass, https://www.google.com/glass.
[18] R. P. O‚ÄôShea, A. Parker, D. La Rooy, and D. Alais, ‚ÄúMonocular rivalry
exhibits three hallmarks of binocular rivalry: Evidence for common
processes,‚Äù Vision research, vol. 49, no. 7, pp. 671‚Äì681, 2009.

[19] HoloLens, https://www.microsoft.com/hololens.
[20] Magic Leap, https://www.magicleap.com.
[21] EPSON AR, https://epson.com/moverio-augmented-reality.

PSNRNQMMSSSIMSSIMIFCVIFIWMSEIWPSNRIWSSIMFSIMGSIGMSDGMSMPAMSELTGVSILPIPSDISTSBaselineARIQA0.50.550.60.650.7AUC (-)Different/SimilarSignificancePSNRNQMMSSSIMSSIMIFCVIFIWMSEIWPSNRIWSSIMFSIMGSIGMSDGMSMPAMSELTGVSILPIPSDISTSBaselineARIQAPSNRNQMMSSSIMSSIMIFCVIFIWMSEIWPSNRIWSSIMFSIMGSIGMSDGMSMPAMSELTGVSILPIPSDISTSBaselineARIQAPSNRNQMMSSSIMSSIMIFCVIFIWMSEIWPSNRIWSSIMFSIMGSIGMSDGMSMPAMSELTGVSILPIPSDISTSBaselineARIQA0.50.550.60.650.70.750.80.850.90.95AUC (-)Better/WorseSignificancePSNRNQMMSSSIMSSIMIFCVIFIWMSEIWPSNRIWSSIMFSIMGSIGMSDGMSMPAMSELTGVSILPIPSDISTSBaselineARIQAPSNRNQMMSSSIMSSIMIFCVIFIWMSEIWPSNRIWSSIMFSIMGSIGMSDGMSMPAMSELTGVSILPIPSDISTSBaselineARIQA[22] S. Wang, K. Ma, H. Yeganeh, Z. Wang, and W. Lin, ‚ÄúA patch-structure
representation method for quality assessment of contrast changed im-
ages,‚Äù IEEE Signal Processing Letters, vol. 22, no. 12, pp. 2387‚Äì2390,
2015.

[23] X. Yu, C. G. Bampis, P. Gupta, and A. C. Bovik, ‚ÄúPredicting the quality
of images compressed after distortion in two steps,‚Äù IEEE Transactions
on Image Processing (TIP), vol. 28, no. 12, pp. 5757‚Äì5770, 2019.
[24] A. Mittal, R. Soundararajan, and A. C. Bovik, ‚ÄúMaking a ‚Äúcompletely
blind‚Äù image quality analyzer,‚Äù IEEE Signal Processing Letters, vol. 20,
no. 3, pp. 209‚Äì212, 2012.

[25] X. Min, K. Gu, G. Zhai, J. Liu, X. Yang, and C. W. Chen, ‚ÄúBlind quality
assessment based on pseudo-reference image,‚Äù IEEE Transactions on
Multimedia (TMM), vol. 20, no. 8, pp. 2049‚Äì2062, 2017.

[26] K. Gu, G. Zhai, X. Yang, and W. Zhang, ‚ÄúUsing free energy principle
for blind image quality assessment,‚Äù IEEE Transactions on Multimedia
(TMM), vol. 17, no. 1, pp. 50‚Äì63, 2014.

[27] L. Kang, P. Ye, Y. Li, and D. Doermann, ‚ÄúSimultaneous estimation
of image quality and distortion via multi-task convolutional neural
networks,‚Äù in Proceedings of
the IEEE international conference on
image processing (ICIP), 2015, pp. 2791‚Äì2795.

[28] S. Bosse, D. Maniry, K.-R. M¬®uller, T. Wiegand, and W. Samek, ‚ÄúDeep
neural networks for no-reference and full-reference image quality assess-
ment,‚Äù IEEE Transactions on Image Processing (TIP), vol. 27, no. 1,
pp. 206‚Äì219, 2017.

[29] Y. Yuan, Q. Guo, and X. Lu, ‚ÄúImage quality assessment: a sparse

learning way,‚Äù Neurocomputing, vol. 159, pp. 227‚Äì241, 2015.

[30] J. Johnson, A. Alahi, and L. Fei-Fei, ‚ÄúPerceptual losses for real-time
style transfer and super-resolution,‚Äù in Proceedings of the European
conference on computer vision (ECCV). Springer, 2016, pp. 694‚Äì711.
[31] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, ‚ÄúThe
unreasonable effectiveness of deep features as a perceptual metric,‚Äù in
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2018, pp. 586‚Äì595.

[32] H. Duan, G. Zhai, X. Min, Y. Zhu, Y. Fang, and X. Yang, ‚ÄúPerceptual
quality assessment of omnidirectional images,‚Äù in 2018 IEEE interna-
tional symposium on circuits and systems (ISCAS), 2018, pp. 1‚Äì5.
[33] W. Zhou, J. Xu, Q. Jiang, and Z. Chen, ‚ÄúNo-reference quality assessment
for 360-degree images by analysis of multifrequency information and
local-global naturalness,‚Äù IEEE Transactions on Circuits and Systems
for Video Technology (TCSVT), 2021.

[34] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisser-
man, ‚ÄúThe pascal visual object classes (voc) challenge,‚Äù International
Journal of Computer Vision (IJCV), vol. 88, no. 2, pp. 303‚Äì338, 2010.
[35] B. Series, ‚ÄúMethodology for the subjective assessment of the quality of
television pictures,‚Äù Recommendation ITU-R BT, pp. 500‚Äì13, 2012.
[36] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally,
and K. Keutzer, ‚ÄúSqueezenet: Alexnet-level accuracy with 50x fewer
parameters and¬° 0.5 mb model size,‚Äù in Proceedings of
the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), 2017.
[37] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ‚ÄúImagenet classiÔ¨Åcation
with deep convolutional neural networks,‚Äù Proceedings of the Advances
in Neural Information Processing Systems (NeurIPS), vol. 25, pp. 1097‚Äì
1105, 2012.

[38] K. Simonyan and A. Zisserman, ‚ÄúVery deep convolutional networks for
large-scale image recognition,‚Äù arXiv preprint arXiv:1409.1556, 2014.
[39] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning for image
recognition,‚Äù in Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2016, pp. 770‚Äì778.

[40] D. L. Yamins and J. J. DiCarlo, ‚ÄúUsing goal-driven deep learning models
to understand sensory cortex,‚Äù Nature Neuroscience, vol. 19, no. 3, pp.
356‚Äì365, 2016.

[41] J. Hu, L. Shen, and G. Sun, ‚ÄúSqueeze-and-excitation networks,‚Äù in
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2018, pp. 7132‚Äì7141.

[42] R. Droste, J. Jiao, and J. A. Noble, ‚ÄúUniÔ¨Åed image and video saliency
modeling,‚Äù in Proceedings of the European Conference on Computer
Vision (ECCV). Springer, 2020, pp. 419‚Äì435.

[43] Y. Liu, M.-M. Cheng, X. Hu, J.-W. Bian, L. Zhang, X. Bai, and J. Tang,
‚ÄúRicher convolutional features for edge detection,‚Äù IEEE Transactions
on Pattern Analysis and Machine Intelligence (TPAMI), vol. 41, no. 8,
pp. 1939‚Äì1946, 2019.

[44] N. Damera-Venkata, T. D. Kite, W. S. Geisler, B. L. Evans, and A. C.
Bovik, ‚ÄúImage quality assessment based on a degradation model,‚Äù IEEE
Transactions on Image Processing (TIP), vol. 9, no. 4, pp. 636‚Äì650,
2000.

13

[45] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, ‚ÄúImage
quality assessment: from error visibility to structural similarity,‚Äù IEEE
Transactions on Image Processing (TIP), vol. 13, no. 4, pp. 600‚Äì612,
2004.

[46] H. R. Sheikh, A. C. Bovik, and G. De Veciana, ‚ÄúAn information Ô¨Ådelity
criterion for image quality assessment using natural scene statistics,‚Äù
IEEE Transactions on image processing (TIP), vol. 14, no. 12, pp. 2117‚Äì
2128, 2005.

[47] H. R. Sheikh and A. C. Bovik, ‚ÄúImage information and visual quality,‚Äù
IEEE Transactions on Image Processing (TIP), vol. 15, no. 2, pp. 430‚Äì
444, 2006.

[48] Z. Wang and Q. Li, ‚ÄúInformation content weighting for perceptual image
quality assessment,‚Äù IEEE Transactions on Image Processing (TIP),
vol. 20, no. 5, pp. 1185‚Äì1198, 2010.

[49] L. Zhang, L. Zhang, X. Mou, and D. Zhang, ‚ÄúFsim: A feature similarity
index for image quality assessment,‚Äù IEEE Transactions on Image
Processing (TIP), vol. 20, no. 8, pp. 2378‚Äì2386, 2011.

[50] A. Liu, W. Lin, and M. Narwaria, ‚ÄúImage quality assessment based
on gradient similarity,‚Äù IEEE Transactions on Image Processing (TIP),
vol. 21, no. 4, pp. 1500‚Äì1512, 2011.

[51] W. Xue, L. Zhang, X. Mou, and A. C. Bovik, ‚ÄúGradient magnitude
similarity deviation: A highly efÔ¨Åcient perceptual image quality index,‚Äù
IEEE Transactions on Image Processing (TIP), vol. 23, no. 2, pp. 684‚Äì
695, 2013.

[52] W. Xue, X. Mou, L. Zhang, and X. Feng, ‚ÄúPerceptual Ô¨Ådelity aware
mean squared error,‚Äù in Proceedings of the IEEE International Confer-
ence on Computer Vision (ICCV), 2013, pp. 705‚Äì712.

[53] K. Gu, G. Zhai, X. Yang, and W. Zhang, ‚ÄúAn efÔ¨Åcient color image
quality metric with local-tuned-global model,‚Äù in Proceedings of the
IEEE International Conference on Image Processing (ICIP), 2014, pp.
506‚Äì510.

[54] L. Zhang, Y. Shen, and H. Li, ‚ÄúVsi: A visual saliency-induced index
for perceptual image quality assessment,‚Äù IEEE Transactions on Image
Processing (TIP), vol. 23, no. 10, pp. 4270‚Äì4281, 2014.

[55] K. Ding, K. Ma, S. Wang, and E. P. Simoncelli, ‚ÄúImage quality
assessment: Unifying structure and texture similarity,‚Äù arXiv preprint
arXiv:2004.07728, 2020.

[56] D. P. Kingma and J. Ba, ‚ÄúAdam: A method for stochastic optimization,‚Äù

arXiv preprint arXiv:1412.6980, 2014.

[57] L. Krasula, K. Fliegel, P. Le Callet, and M. Kl¬¥ƒ±ma, ‚ÄúOn the accuracy
of objective image and video quality models: New methodology for
the IEEE International
performance evaluation,‚Äù in Proceedings of
Conference on Quality of Multimedia Experience (QoMEX), 2016, pp.
1‚Äì6.

[58] L. Krasula, P. Le Callet, K. Fliegel, and M. Kl¬¥ƒ±ma, ‚ÄúQuality assessment
of sharpened images: Challenges, methodology, and objective metrics,‚Äù
IEEE Transactions on Image Processing (TIP), vol. 26, no. 3, pp. 1496‚Äì
1508, 2017.

[59] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter,
‚ÄúGans trained by a two time-scale update rule converge to a local nash
equilibrium,‚Äù in Proceedings of the Advances in Neural Information
Processing Systems (NeurIPS), 2017, pp. 6629‚Äì6640.

[60] E. Kruijff, J. E. Swan, and S. Feiner, ‚ÄúPerceptual issues in augmented
reality revisited,‚Äù in Proceedings of the IEEE International Symposium
on Mixed and Augmented Reality, 2010, pp. 3‚Äì12.

[61] H. Yang, Y. Fang, and W. Lin, ‚ÄúPerceptual quality assessment of screen
content images,‚Äù IEEE Transactions on Image Processing (TIP), vol. 24,
no. 11, pp. 4408‚Äì4421, 2015.

[62] X. Min, G. Zhai, J. Zhou, M. C. Farias, and A. C. Bovik, ‚ÄúStudy
of subjective and objective quality assessment of audio-visual signals,‚Äù
IEEE Transactions on Image Processing (TIP), vol. 29, pp. 6054‚Äì6068,
2020.

[63] K. Gu, G. Zhai, W. Lin, and M. Liu, ‚ÄúThe analysis of image contrast:
From quality assessment to automatic enhancement,‚Äù IEEE Transactions
on Cybernetics, vol. 46, no. 1, pp. 284‚Äì297, 2015.

[64] Unity, https://unity.com/.
[65] HTC VIVE Pro Eye, https://www.vive.com/us/product/vive-pro-eye/

overview/.

[66] Z. Wang, E. P. Simoncelli, and A. C. Bovik, ‚ÄúMultiscale structural
similarity for image quality assessment,‚Äù in Proceedings of the Asilomar
Conference on Signals, Systems & Computers, 2003, vol. 2, 2003, pp.
1398‚Äì1402.

[67] C.-C. Chang and C.-J. Lin, ‚ÄúLibsvm: a library for support vector
machines,‚Äù ACM Transactions on Intelligent Systems and Technology
(TIST), vol. 2, no. 3, pp. 1‚Äì27, 2011.

