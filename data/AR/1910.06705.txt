9
1
0
2

t
c
O
5
1

]

G
L
.
s
c
[

1
v
5
0
7
6
0
.
0
1
9
1
:
v
i
X
r
a

Published as an arXiv pre-preint

NEURAL APPROXIMATION OF AN AUTO-REGRESSIVE
PROCESS THROUGH CONFIDENCE GUIDED SAMPLING

YoungJoon Yoo, Sanghyuk Chun, Sangdoo Yun, Jung-Woo Ha & Jaejun Yoo∗
Clova AI Research, NAVER Corp.
{youngjoon.yoo, sanghyuk.c, sangdoo.yun, jungwoo.ha, jaejun.yoo}@navercorp.com

ABSTRACT

We propose a generic conﬁdence-based approximation that can be plugged in and
simplify the auto-regressive generation process with a proved convergence. We
ﬁrst assume that the priors of future samples can be generated in an independently
and identically distributed (i.i.d.) manner using an efﬁcient predictor. Given the
past samples and future priors, the mother AR model can post-process the priors
while the accompanied conﬁdence predictor decides whether the current sample
needs a resampling or not. Thanks to the i.i.d. assumption, the post-processing can
update each sample in a parallel way, which remarkably accelerates the mother
model. Our experiments on different data domains including sequences and images
show that the proposed method can successfully capture the complex structures of
the data and generate the meaningful future samples with lower computational cost
while preserving the sequential relationship of the data.

1

INTRODUCTION

The auto-regressive (AR) model, which infers and predicts the causal relationship between the
previous and future samples in a sequential data, has been widely studied since the beginning of
machine learning research. The recent advances of the auto-regressive model brought by the neural
network have achieved impressive success in handling complex data including texts (Sutskever et al.,
2011), audio signals (Vinyals et al., 2012; Tamamori et al., 2017; van den Oord et al., 2016a), and
images (van den Oord et al., 2016b; Salimans et al., 2017).

It is well known that AR model can learn a tractable data distribution p(x) and can be easily extended
for both discrete and continuous data. Due to their nature, AR models have especially shown a
good ﬁt with a sequential data, such as voice generation (van den Oord et al., 2016a) and provide a
stable training while they are free from the mode collapsing problem (van den Oord et al., 2016b).
However, these models must infer each element of the data x ∈ RN step by step in a serial manner,
requiring O(N ) times more than other non-sequential estimators (Garnelo et al., 2018; Kim et al.,
2019; Kingma & Welling, 2014; Goodfellow et al., 2014). Moreover, it is difﬁcult to employ recent
parallel computation because AR models always require a previous time step by deﬁnition. This
mostly limits the use of the AR models in practice despite their advantages.

To resolve the problem, we introduce a new and generic approximation method, Neural Auto-
Regressive model Approximator (NARA), which can be easily plugged into any AR model. We show
that NARA can reduce the generation complexity of AR models by relaxing an inevitable AR nature
and enables AR models to employ the powerful parallelization techniques in the sequential data
generation, which was difﬁcult previously.

NARA consists of three modules; (1) a prior-sample predictor, (2) a conﬁdence predictor,
and (3) original AR model. To relax the AR nature, given a set of past samples, we ﬁrst assume that
each sample of the future sequence can be generated in an independent and identical manner. Thanks
to the i.i.d. assumption, using the ﬁrst module of NARA, we can sample a series of future priors and
these future priors are post-processed by the original AR model, generating a set of raw predictions.
The conﬁdence predictor evaluates the credibility of these raw samples and decide whether the model

∗Corresponding author.

1

 
 
 
 
 
 
Published as an arXiv pre-preint

Figure 1: Conceptual description of Neural Auto-Regressive model Approximator (NARA). First, the
prior-sample predictor fW predicts prior samples m. Next, we draw M samples from conditional
probabilities for the given input x≤i and the prior samples m in parallel. Afterward, the conﬁdence
predictor gV estimates a conﬁdence score σj of each j-th sample in parallel. Finally, we accept k − 1
samples where k = arg minj σj < (cid:15). Here, (cid:15) is a predeﬁned gauge threshold.

needs re-sampling or not. The conﬁdence predictor plays an important role in that the approximation
errors can be accumulated during the sequential AR generation process if the erroneous samples with
low conﬁdence are left unchanged. Therefore, in our model, the sample can be drawn either by the
mixture of the AR model or the proposed approximation method, and ﬁnally the selection of the
generated samples are guided by the predicted conﬁdence.

We evaluate NARA with various baseline AR models and data domains including simple curves,
image sequences (Yoo et al., 2017), CelebA (Liu et al., 2015a), and ImageNet (Deng et al., 2009).
For the sequential data (simple curves and golf), we employed the Long Short-Term Memory models
(LSTM) (Hochreiter & Schmidhuber, 1997) as a baseline AR model while PixelCNN++ (Salimans
et al., 2017) is used for the image generation (CelebA and ImageNet). Our experiments show that
NARA can largely reduce the sample inference complexity even with a heavy and complex model on
a difﬁcult data domain such as image pixels.

The main contributions of our work can be summarized as follows: (1) we introduce a new and
generic approximation method that can accelerate any AR generation procedure. (2) Compared to
a full AR generation, the quality of approximated samples remains reliable by the accompanied
conﬁdence prediction model that measures the sample credibility. (3) Finally, we show that this is
possible because, under a mild condition, the approximated samples from our method can eventually
converge toward the true future sample. Thus, our method can effectively reduce the generation
complexity of the AR model by partially substituting it with the simple i.i.d. model.

2 PRELIMINARY: AUTO-REGRESSIVE MODELS

Auto-regressive generation model is a probabilistic model to assign a probability p(x) of the data
x including n samples. This method considers the data x as a sequence {xi | i = 1, · · · n}, and the
probability p(x) is deﬁned by an AR manner as follows:

p(x) =

n
(cid:89)

i=1

p(xi|x1, ..., xi−1),

(1)

From the formulation, the AR model provides a tractable data distribution p(x). Recently, in training
the model parameters using the training samples ˆxT , the computation parallelization are actively
employed for calculating the distance between the real sample ˆxt ∈ ˆxT and the generated sample xt
from equation (1). Still, for generating the future samples, it requires O(N ) by deﬁnition.

3 PROPOSED METHOD

3.1 OVERVIEW

Figure 1 shows the concept of the proposed approximator NARA. NARA consists of a prior-sample
predictor fW and conﬁdence predictor gV . Given samples x≤i = {x1, · · · , xi}, the prior-sample
predictor predicts a chunk of M number of the prior values m(i,i+M ]. Afterward, using the prior

2

AR modelParallel InferenceConfidencePredictor gVParallel Inferencep(xi+1| x≤i)σi+1 (≥ ε)Acceptp(xi+2| x≤i,mi+1 )σi+2 (≥ ε)Acceptp(xi+j| x≤i,m(i,i+ j-1] )σi+j(< ε)Re-sample[ x1, …, xi, mi+1, …, mi+M]p(xi+M| x≤i,m(i,i+ M-1] )Input:x≤i= [ x1, x2, …, xi ]Prior values: m(i,i+ M] = [ mi+1, …, mi+M]Prior-sample Predictor fWσi+M(< ε)Re-samplePublished as an arXiv pre-preint

samples, we draw the future samples x(i,i+M ] in parallel. We note that this is possible because the
prior m(i,i+M ] is i.i.d. variable from our assumption. Subsequently, for the predicted x(i,i+M ], the
conﬁdence predictor predicts conﬁdence scores σ(i,i+M ]. Then, using the predicted conﬁdence, our
model decides whether the samples of interest should be redrawn by the AR model (re-sample xi+1)
or they are just accepted. The detailed explanation will be described in the following sections.

3.2 APPROXIMATING SAMPLE DISTRIBUTION OF AR MODEL

Given the samples x≤i = {x1, · · · , xi}, a AR model deﬁnes the distribution of future samples
x(i,j] = {xi+1, · · · , xj} as follows:

pθ(x(i,j]|x≤i) =

j
(cid:89)

l=i+1

pθ(xl|x1, ..., xl−1).

(2)

Here, θ denotes the parameters of the AR model. The indices i, j are assumed to satisfy the condition
j > i, ∀i, j ∈ [1, N ]. To approximate the distribution pθ(x(i,j]|x≤i), we introduce a set of prior
samples m(i,j−1] = fW (x≤i; W ), where we assume that they are i.i.d. given the observation x≤i.
Here, W is the model parameter of the prior-sample predictor fW (·).

Base on this, we deﬁne an approximated distribution qθ,W (x(i,j−1]|x≤i, m(i,j−1]) characterized by
the original AR model pθ and the prior-sample predictor fW (·) as follows:

qθ,W (x(i,j]|x≤i, m(i,j−1]) ≡ pθ(xi+1|x≤i)

j
(cid:89)

l=i+2
(cid:124)

pθ(xl|x≤i, mi+1, . . . ml−1)

(cid:123)(cid:122)
Compute in parallel (const time)

(cid:125)

(A)
(cid:39) pθ(xi+1|x≤i)

j
(cid:89)

pθ(xl|x≤i, xi+1, . . . xl−1)

(3)

l=i+2
(cid:124)

(cid:123)(cid:122)
Compute in sequential (linear time)

(cid:125)

= pθ(x(i,j]|x≤i).

Here, approximation (A) is true when m(i,j−1] approaches to x(i,j−1]. Note that it becomes possible
to compute qθ,W in a constant time because we assume the prior variable mi to be i.i.d. while pθ
requires a linear time complexity.

Then, we optimize the network parameters θ and W by minimizing the negative log-likelihood (NLL)
of qθ,W (x(i,j] = ˆx(i,j]|x≤i, fW (x≤i)) where ˆx(i,j] is a set of samples that are sampled from the
baseline AR model. We guide the prior-sample predictor fW (·) to generate the prior samples that is
likely to come from the distribution of original AR model m by minimizing the original AR model θ
and prior-sample predictor W jointly as follows:
− log pθ(x(g)

(i,j]|x≤i) − Epθ(x(i,j]|x≤i)[log qθ,W (x(i,j]|x≤i, fW (x≤i))],

(4)

min
θ,W

where x(g) denotes the ground truth sample value in the generated region of the training samples. Note
that both pθ(x) and its approximated distribution qθ,W (x) approaches to the true data distribution
when (1) our prior-sample predictor generates the prior samples m close to the true samples x and
(2) the NLL of the AR distribution approaches to the data distribution. Based on our analysis and
experiments, we later show that our model can satisfy these conditions theoretically and empirically
in the following sections.

3.3 CONFIDENCE PREDICTION

Using the prior-sample predictor fW (·), our model generates future samples based on the previous
samples. However, accumulation of approximation errors in the AR generation may lead to an
unsuccessful sample generation. To mitigate the problem, we introduce an auxiliary module that
determines whether to accept or reject the approximated samples generated as described in the
previous subsection, referred to as conﬁdence predictor.

3

Published as an arXiv pre-preint

First, we deﬁne the conﬁdence of the generated samples as follows:
σk = qθ,W (xk = ˆxk|x≤i, fW (x≤i)),
(5)
where ˆxk ∼ pθ(xk|x≤k−1) and k ∈ {1, · · · , j}. The conﬁdence value σk provides a measure of how
likely the generated samples from qθ,W (·) is drawn from pθ(xk|x≤k−1). Based on the conﬁdence
value σk, our model decides whether it can accept the sample xk or not. More speciﬁcally, we choose
a threshold (cid:15) ∈ [0, 1] and accept samples which have the conﬁdence score larger than the threshold (cid:15).

When the case (cid:15) = 1, our model always redraws the sample using the AR model no matter how our
conﬁdence is high. Note that our model becomes equivalent to the target AR model when (cid:15) = 1. When
(cid:15) = 0, our model always accepts the approximated samples. In practice, we accept ˆk((cid:15)) − 1 samples
among approximated M samples where ˆk((cid:15)) = arg mink σk > (cid:15). Subsequently, we re-sample xˆk((cid:15)))
from the original AR model and repeat approximation scheme until reach the maximum length.

However, it is impractical to calculate equation (5) directly because we need the samples ˆxk from the
original AR model. We need ﬁrst to go forward using the AR model to see the next sample and come
backward to calculate the conﬁdence to decide whether we use the sample or not, which is nonsense.

To detour this problem, we introduce a network gV (·) that approximates the binary decision variable
k = I(σk ≥ (cid:15)) as follows:
h(cid:15)

where h(cid:15)
with a sigmoid activation output that makes the equation (6) equivalent to the logistic regression.

(6)
j}. The network gV (·) is implemented by a auto-encoder architecture

(i,j] (cid:39) gV (x≤i, fW (x≤i)),

i+1, · · · , h(cid:15)

(i,j] = {h(cid:15)

h(cid:15)

3.4 TRAINING DETAILS

To train the proposed model, we randomly select the sample v(i) ∈ [1, N ] for the sequence x(i) in a
training batch. Then, we predict l(i) = min(B, N − v(i)) sample values after v(i), where B denotes
the number of samples the prediction considers. To calculate equation (4), we minimize the loss of
the training sample x(k), k = 1 · · · K, and the locations v(i) ∈ [1, N ] as,

min
θ,W

−

1
K

K
(cid:88)

i=1

log pθ(x(i)

≤v(i)+l(i) ) −

1
KM

K
(cid:88)

M
(cid:88)

i=1

j=1

log qθ,W (ˆx(i)(j)

(v(i),v(i)+l(i)]|x(i)

≤v(i) ).

(7)

Here, ˆx(i)(j)
(v(i),v(i)+l(i)] for j ∈ {1 · · · M } denotes M number of the sequences from the AR distribution
pθ(x(v(i),v(i)+l(i)]|x(i)
≤v(i) ) for i-th training data. From the experiment, we found that M = 1 sample
is enough to train the model. This training scheme guides the distribution drawn by NARA to ﬁt the
original AR distribution as well as to generate future samples, simultaneously. To train gV (·), binary
cross-entropy loss is used with h(cid:15) in equation (6), with freezing the other parameters.

3.5 THEORETICAL EXPLANATION

Here, we show that the proposed NARA is a regularized version of the original AR model. At the
extremum, the approximated sample distribution from NARA is equivalent to that of the original AR
model. In NARA, our approximate distribution q(x(i,j+1]|x≤i, m(i,j]) is reformulated as follows:

qφ,θ(x(i,j+1]|x≤i, m(i,j]) ≡ pθ(xi+1|x≤i)

j+1
(cid:89)

l=i+2

qφ(xl|x≤i, mi+1, ..., ml−1)

=

pθ(x≤i+1)
pθ(x≤i)

·

pθ(x≤i+2)
pθ(x≤i+1)

·, · · · , ·

pθ(x≤j+1)
pθ(x≤j)

(cid:20) pθ(x≤i+1)
qφ(x≤i, mi+1)

·

·, · · · , ·

qφ(x≤i, m(i,j], xj+1)
pθ(x≤j+1)

(cid:124)

(cid:123)(cid:122)
pθ(x(i,j+1]|x≤i)

(cid:125)

(cid:124)

(cid:123)(cid:122)
R(pθ,qφ,m(i,j])

(cid:21)
,

(cid:125)

(8)
where the parameter φ denotes the network parameters of the approximated distribution q(·). There-
fore, our proposed cost function can be represented as the negative log-likelihood of the AR model
with a regularizer − log R(pθ, qφ, m(i,j]):

min
φ,θ

−Epθ(x(i,j+1]|x≤i)[log pθ(x(i,j+1]|x≤i)) + log R(pθ, qφ, m(i,j])]

(9)

4

Published as an arXiv pre-preint

(a) One-dimensional time series data generation results.

(b) Error, accepts over (cid:15) ∈ [0.0, 1.0].

Figure 2: (a) One-dimensional time series data generation example. Dashed lines denote the generated
samples. Red lines, green, and blue lines are from the approximation model with acceptance ratio
(cid:15) = 0.0, 0.5 and (cid:15) = 1.0. Black line is the generation results from the baseline LSTM. Magenta
line presents the generated prior samples. (b) The graph denoting the mean (cid:96)1 error (blue) and the
acceptance ratio (red) over the threshold (cid:15).

Note that the proposed cost function is equivalent to that of the original AR model when
log R(pθ, qφ, m(i,j]) = 0, which is true under the condition of m(i,j] = x(i,j] and qφ(·|·) = pθ(·|·).
Here, m(i,j] = fW (x≤i). By minimizing the equation (9), R(pθ, qφ, m(i,j]) enforces the direction
of the optimization to estimate the probability ratio of qφ and pθ while it minimize the gap between
qφ(x≤i,m(i,j],xj+1)
qφ(x≤i,m(i,j])

so that m(i,j] = fW (x≤i) approaches to x(i,j].

4 RELATED WORK

Deep AR and regression models: After employing the deep neural network, the AR models han-
dling sequential data has achieved signiﬁcant improvements in handling the various sequential data
including text (Sutskever et al., 2011), sound (Vinyals et al., 2012; Tamamori et al., 2017), and
images (van den Oord et al., 2016b; Salimans et al., 2017). The idea has been employed to “Flow
based model” which uses auto-regressive sample ﬂows (Kingma & Dhariwal, 2018; Germain et al.,
2015; Papamakarios et al., 2017; Kingma et al., 2016) to infer complex distribution, and reported
meaningful progresses. Also, the attempts (Yoo et al., 2017; Garnelo et al., 2018; Kim et al., 2019) to
replace the kernel function of the stochastic regression and prediction processes to neural network
has been proposed to deal with semi-supervised data not imposing an explicit sequential relationship.

Approximated AR methods: Reducing the complexity of the deep AR model has been explored by
a number of studies, either targeting multiple domain (Seo et al., 2018; Stern et al., 2018) or speciﬁc
target such as machine translation (Wang et al., 2018; Ghazvininejad et al., 2019; Welleck et al.,
2019; Wang et al., 2018; 2019) and image generation (Ramachandran et al., 2017).

Adding one step further to the previous studies, we propose a new general approximation method for
AR methods by assuming the i.i.d. condition for the “easy to predict” samples. This differentiates our
approach to (Seo et al., 2018) in that we do not sequentially approximate the future samples by using
a smaller AR model but use a chunk-wise predictor to approximate the samples at once. In addition,
our conﬁdence prediction module can be seen as a stochastic version of the veriﬁcation step in (Stern
et al., 2018), which helps our model to converge toward the original solution. This conﬁdence guided
approximation can be easily augmented to the other domain speciﬁc AR approximation methods
because our method is not limited to a domain speciﬁc selection queues such as quotation (Welleck
et al., 2019; Ghazvininejad et al., 2019) or nearby convolutional features (Ramachandran et al., 2017).

5 EXPERIMENTS

In this section, we demonstrate the data generation results from the proposed NARA. To check
the feasibility, we ﬁrst test our method into time-series data generation problem, and second,
into image generation. The detailed model structures and additional results are attached in the
Supplementary material. The implementation of the methods will be available soon.

5

Published as an arXiv pre-preint

Figure 3: Image sequence generation examples (cropped in an equivalent time-interval). From the top
to the bottom, each row denotes the generation result with (cid:15) ∈ {1.00, 0.75, 0.50, 0.25, 0.00}.

Table 1: Mean (cid:96)1-error and the acceptance ratio over (cid:15) in image sequence generation.

(cid:15)

Acceptance (%)
Mean error ((cid:96)1)

1.0

0.0
21.1

0.9

15.3
26.8

0.8

43.0
24.9

0.7

59.3
21.7

0.6

69.1
18.4

0.5

77.1
19.9

0.4

82.1
18.3

0.3

84.5
20.6

0.2

88.0
19.4

0.1

92.5
25.3

0.0

100
24.3

5.1 EXPERIMENTAL SETTING

Time-series data generation problem: In this problem, we used LSTM as the base model. First, we
tested our method with a simple one-dimensional sinusoidal function. Second, we tested the video
sequence data (golf swing) for demonstrating the more complicated case. In this case, we repeated
the swing sequences 20 times to make the periodic image sequences and resize each image to 64 × 64
resolution. Also, beside the LSTM, we used autoencoder structures to embed the images into latent
space. The projected points for the image sequences are linked by LSTM, similar to (Yoo et al.,
2017). For both cases, we used ADAM optimizer (Kingma & Ba, 2015) with a default setting and a
learning rate 0.001.

Image generation: For the image generation task, we used PixelCNN++ (Salimans et al., 2017) as the
base model. The number of channels of the network was set to 160 and the number of logistic mixtures
was set to 10. See (Salimans et al., 2017) for the detailed explanation of the parameters. In this task,
the baseline AR model (PixelCNN++) is much heavier than those used in the previous tasks. Here,
we show that the proposed approximated model can signiﬁcantly reduce the computational burden of
the original AR model. The prior-sample predictor fW (·) and the conﬁdence estimator gV (·) were
both implemented by U-net structured autoencoder (Ronneberger et al., 2015). We optimized the
models using ADAM with learning rate 0.0001. Every module was trained from scratch. We mainly
used CelebA (Liu et al., 2015b) resizing the samples to 64 × 64 resolution. In the experiments, we
randomly pick 36, 000 images for training and 3, 000 images for validation.

Training and evaluation: For the ﬁrst problem, we use single GPU (NVIDIA Titan XP), and for the
second problem, four to eight GPUs (NVIDIA Tesla P40) were used1. The training and inference
code used in this section are implemented by PyTorch library. For the quantitative evaluation, we
measure the error between the true future samples and the generated one, and also employ Fr´echet
Inception Distance score (FID) (Heusel et al., 2017) as a measure of the model performance and
visual quality of the generated images for the second image generation problem.

5.2 ANALYSIS

5.2.1 TIME-SERIES DATA GENERATION

Figure 2a shows the generation results of the one-dimensional time-series from our approximation
model with different acceptance ratios (red, green, and blue) and the baseline LSTM models (black).

1The overall expreiments were conducted on NSML (Sung et al., 2017) GPU system.

6

Published as an arXiv pre-preint

Figure 4: Generated images of (cid:15) ∈ {0.0, 0.2, 0.4, 0.6, 0.8, 1.0} on the CelebA dataset. The image in
the left of the triplet denotes a accepted region (white: do accept, black: do not accept), the mid is a
conﬁdence map (red: high, blue: low) and the right shows the generated result.

From the ﬁgure, we can see that both models correctly generates the future samples. Please note that,
from the prior sample generation result (magenta), the prior samples m converged to the true samples
x as claimed in Section 3.5.

The graph in Figure 2b shows the acceptance ratio and the (cid:96)1-error over the gauge threshold (cid:15) ∈ (0, 1].
The error denotes the distance between the ground truth samples x and the generated ones. As
expected, our model accepted more samples as the threshold (cid:15) decreases. However, contrary to our
initial expectations, the error-threshold graph shows that the less acceptance of samples does not
always bring the more accurate generation results. From the graph, the generation with an intermediate
acceptance ratio achieved the best result. Interestingly, we report that this tendency between the
acceptance ratio and the generation quality was repeatedly observed in the other datasets as well.

Figure 3 shows the image sequence generation results from NARA. From the result, we can see
that the proposed approximation method is still effective when the input data dimension becomes
much larger and the AR model becomes more complicated. In the golf swing dataset, the proposed
approximation model also succeeded to capture the periodic change of the image sequence. The
table 3 shows that the proper amount of approximation can obtain better accuracy than the none,
similar to the other previous experiments. One notable observation regarding the phenomenon is that
the period of image sequence was slightly changed among different ratio of approximated sample
acceptance (Figure 3). One possible explanation would be that the approximation module suppress
the rapid change of the samples, and this affects the interval of a single cycle.

5.2.2

IMAGE GENERATION

Figure 4 shows that our method can be integrated into PixelCNN++ and generates images with
the signiﬁcant amount of the predicted sample acceptance (white region). We observed that the
conﬁdence was mostly low (blue) in the eyes, mouth, and boundary regions of the face, and the
PixelCNN is used to generate those regions. This shows that compared to the other homogeneous
regions of the image, the model ﬁnds it relatively hard to describe the details, which matches with
our intuition.

The graphs in Figure 5 present the quantitative analysis regarding the inference time and the NLL
in generating images. In Figure 5a, the relation between inference time and the skimming ratio is
reported. The results show that the inference speed is signiﬁcantly improved as more pixels are
accepted. Table 2 further supports this that our approximation method generates a fair quality of
images while it speeds up the generation procedure 5 ∼ 10 times faster than the base model.

In the image generation example also, we found that the fair amount of acceptance can improve the
perceptual visual quality of the generated images compared to the vanilla PixelCNN++ (Table 2).
Our method beneﬁts from increasing the acceptance ratio to some extent in terms of FID showing
a U-shaped trend over the (cid:15) variation, similar to those in Figure 2b. Note that a lower FID score
identiﬁes a better model. Consistent with previous results, we can conjecture that the proposed
approximation scheme learns the mean-prior of the images and guides the AR model to prevent
generating erroneous images. The conﬁdence maps and the graph illustrated in Figure 4, 5a, and 5c

7

Published as an arXiv pre-preint

(a) Generation speed (pixels/sec) over (cid:15) ∈ [1.0, 0.0].

(b) NLL Curve.

(c) (cid:96)1 error of the prior mi.

Figure 5: Quantitative analysis of the proposed method on CelebA dataset. (a) shows the generation
speed over the threshold (cid:15). (b) shows the NLL of the inference with and without any acceptance. (c)
presents the (cid:96)1-loss between the prior z and the pixel value x for every epoch.

Table 2: The speed up and visual quality of the results on CelebA dataset. The speed-up of the
inference time with (SU) and without anchors (SU \A) compared to that of PixelCNN++ is measured.
acceptance ratio (AR) without anchors and FID (lower is better) are also displayed.

(cid:15)

SU
SU \A
AR (%)
FID

1.0

1.0x
1.0x
0.0
56.8

0.9

1.2x
1.2x
15.3
50.1

0.8

1.7x
1.7x
43.0
45.1

0.7

2.2x
2.4x
59.3
42.5

0.6

2.8x
3.1x
69.1
41.6

0.5

3.5x
4.1x
77.1
42.3

0.4

4.1x
5.2x
82.1
43.4

0.3

4.5x
5.9x
84.5
45.9

0.2

5.3x
7.4x
88.0
48.7

0.1

0.0

6.8x
11.0x
92.5
53.9

12.9x
59.4x
100
85.0

support this conjecture. Complex details such as eyes, mouths and contours have largely harder than
the backgrounds and remaining faces.

In Figure 5b and Figure 5c, the graphs show the results supporting the convergence of the proposed
method. The graph in Figure 5b shows the NLL of the base PixelCNN++ and that of our proposed
method under the full-accept case, i.e. we fully believe the approximation results. Note that the NLL
of both cases converged and the PixelCNN achieved noticeably lower NLL compared to the fully
accepting the pixels at every epoch. This is already expected in Section 3.2 that the baseline AR
model approaches more closely to the data distribution than our module. This supports the necessity
of re-generating procedure by using the PixelCNN++, especially when the approximation module
ﬁnds the pixel has a low conﬁdence.

The graph in Figure 5c presents the (cid:96)1 distance between the generated prior pixel m and the corre-
sponding ground-truth pixel x in the test data reconstruction. Again, similar to the previous time-series
experiments, the model successfully converged to the original value (m approaches to x). Combined
with the result in Figure 5b, this result supports the convergence conditions claimed in section 3.2.
Regarding the convergence, we compared the NLL of the converged PixelCNN++ distribution from
the proposed scheme and that of PixelCNN++ with CelebA dataset from the original paper (Salimans
et al., 2017).

6 CONCLUSION

In this paper, we proposed the efﬁcient neural auto-regressive model approximation method, NARA,
which can be used in various auto-regressive (AR) models. By introducing the prior-sampling and
conﬁdence prediction modules, we showed that NARA can theoretically and empirically approximate
the future samples under a relaxed causal relationships. This approximation simpliﬁes the generation
process and enables our model to use powerful parallelization techniques for the sample generation
procedure. In the experiments, we showed that NARA can be successfully applied with different
AR models in the various tasks from simple to complex time-series data and image pixel generation.
These results support that the proposed method can introduce a way to use AR models in a more
efﬁcient manner.

8

0.00.20.40.60.81.001202403604806001.00.90.80.70.60.50.40.30.20.10.0pixels  / secgauge threshold (ε)Generation speedAcceptance ratio59.4x11.0x7.4x5.9x5.2x4.1x3.1x2.4x1.7x1.2x1.0x2.32.62.93.23.501020304050bits / dimepochsNo skimFull skim1.41.72.02.32.601020304050ℓ1lossepochs⨯10-7Test ℓ1/ dimPublished as an arXiv pre-preint

REFERENCES

J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical

Image Database. In CVPR09, 2009.

Marta Garnelo, Dan Rosenbaum, Chris J Maddison, Tiago Ramalho, David Saxton, Murray Shanahan,
Yee Whye Teh, Danilo J Rezende, and SM Eslami. Conditional neural processes. arXiv preprint
arXiv:1807.01613, 2018.

Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. Made: Masked autoencoder for
distribution estimation. In International Conference on Machine Learning, pp. 881–889, 2015.

Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. Constant-time machine
translation with conditional masked language models. arXiv preprint arXiv:1904.09324, 2019.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural informa-
tion processing systems, pp. 2672–2680, 2014.

Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
GANs trained by a two time-scale update rule converge to a local nash equilibrium. In Advances
in Neural Information Processing Systems 30, pp. 6626–6637, 2017.

Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):

1735–1780, 1997.

Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
by reducing internal covariate shift. In Proceedings of the 32nd International Conference on
Machine Learning, Proceedings of Machine Learning Research, pp. 448–456, 2015. URL http:
//proceedings.mlr.press/v37/ioffe15.html.

Hyunjik Kim, Andriy Mnih, Jonathan Schwarz, Marta Garnelo, Ali Eslami, Dan Rosenbaum, Oriol
Vinyals, and Yee Whye Teh. Attentive neural processes. arXiv preprint arXiv:1901.05761, 2019.

Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International

Conference on Learning Representations, 2015.

Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In International Conference

on Learning Representations, 2014.

Durk P Kingma and Prafulla Dhariwal. Glow: Generative ﬂow with invertible 1x1 convolutions. In

Advances in Neural Information Processing Systems, pp. 10215–10224, 2018.

Durk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling.
Improved variational inference with inverse autoregressive ﬂow. In Advances in neural information
processing systems, pp. 4743–4751, 2016.

Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
Proceedings of the IEEE international conference on computer vision, pp. 3730–3738, 2015a.

Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In

The IEEE International Conference on Computer Vision (ICCV), 2015b.

George Papamakarios, Theo Pavlakou, and Iain Murray. Masked autoregressive ﬂow for density

estimation. In Advances in Neural Information Processing Systems, pp. 2338–2347, 2017.

Prajit Ramachandran, Tom Le Paine, Pooya Khorrami, Mohammad Babaeizadeh, Shiyu Chang, Yang
Zhang, Mark A Hasegawa-Johnson, Roy H Campbell, and Thomas S Huang. Fast generation for
convolutional autoregressive models. arXiv preprint arXiv:1704.06001, 2017.

Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical
image segmentation. In International Conference on Medical image computing and computer-
assisted intervention, pp. 234–241. Springer, 2015.

9

Published as an arXiv pre-preint

Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. PixelCNN++: Improving the
pixelCNN with discretized logistic mixture likelihood and other modiﬁcations. In International
Conference on Learning Representations, 2017. URL https://openreview.net/forum?
id=BJrFC6ceg.

Minjoon Seo, Sewon Min, Ali Farhadi, and Hannaneh Hajishirzi. Neural speed reading via skim-RNN.
In International Conference on Learning Representations, 2018. URL https://openreview.
net/forum?id=Sy-dQG-Rb.

Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. Blockwise parallel decoding for deep au-
toregressive models. In Advances in Neural Information Processing Systems, pp. 10086–10095,
2018.

Nako Sung, Minkyu Kim, Hyunwoo Jo, Youngil Yang, Jingwoong Kim, Leonard Lausen, Youngkwan
Kim, Gayoung Lee, Donghyun Kwak, Jung-Woo Ha, et al. Nsml: A machine learning platform
that enables you to focus on your models. arXiv preprint arXiv:1712.05902, 2017.

Ilya Sutskever, James Martens, and Geoffrey E Hinton. Generating text with recurrent neural
networks. In Proceedings of the 28th International Conference on Machine Learning (ICML-11),
pp. 1017–1024, 2011.

Akira Tamamori, Tomoki Hayashi, Kazuhiro Kobayashi, Kazuya Takeda, and Tomoki Toda. Speaker-

dependent wavenet vocoder. In Proceedings of Interspeech, pp. 1118–1122, 2017.

A¨aron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,
Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw
audio. arXiv preprint arXiv:1609.03499, 2016a.

A¨aron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In
Proceedings of The 33rd International Conference on Machine Learning, pp. 1747–1756, 2016b.
URL http://proceedings.mlr.press/v48/oord16.html.

Oriol Vinyals, Suman V Ravuri, and Daniel Povey. Revisiting recurrent neural networks for robust
asr. In 2012 IEEE international conference on acoustics, speech and signal processing (ICASSP),
pp. 4085–4088. IEEE, 2012.

Chunqi Wang, Ji Zhang, and Haiqing Chen. Semi-autoregressive neural machine translation. arXiv

preprint arXiv:1808.08583, 2018.

Yiren Wang, Fei Tian, Di He, Tao Qin, ChengXiang Zhai, and Tie-Yan Liu. Non-autoregressive

machine translation with auxiliary regularization. arXiv preprint arXiv:1902.10245, 2019.

Sean Welleck, Kiant´e Brantley, Hal Daum´e III, and Kyunghyun Cho. Non-monotonic sequential text

generation. arXiv preprint arXiv:1902.02192, 2019.

YoungJoon Yoo, Sangdoo Yun, Hyung Jin Chang, Yiannis Demiris, and Jin Young Choi. Variational
autoencoded regression: High dimensional regression of visual data on complex manifold. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.

10

Published as an arXiv pre-preint

Table 3: Mean (cid:96)1-error and the acceptance ratio over (cid:15) in image generation sequence.

(cid:15)

Acceptance (%)
Mean Error

1.0

0.0
2.20

0.9

18.0
2.61

0.8

35.0
2.21

0.7

44.5
1.79

0.6

56.1
2.28

0.5

61.0
2.10

0.4

70.5
1.91

0.3

85.5
1.87

0.2

92.5
1.94

0.1

99.0
2.38

0.0

100
2.45

A IMPLEMENTATION DETAILS

Time serious sample generation: For the 1-dimensional samples case, the LSTM consists of two
hidden layers, and the dimension of each layer was set to 51. We observed o = 200 steps and
predicted p = 20 future samples. The chunk-wise predictor fW (·) and the conﬁdence predictor gV (·)
were deﬁned by single fully-connected layer with input size 200 and the output size 20. In this task,
our model predicted every 20 samples by seeing the 200 previous samples.

For the visual sequence case, we used autoencoder structures to embed the images into latent space.
The encoder consists of four “Conv-activation” plus one ﬁnal Conv ﬁlters. Here, the term “Conv”
denotes the convolutional ﬁlter. Similarly, the decoder consists of four “Conv transpose-activation”
with one ﬁnal Conv transpose ﬁlters. The channel sizes of the encoder and decoder were set to
{32, 32, 32, 64} and {64, 32, 32, 32}. All the Conv and Conv Transpose ﬁlters were deﬁned by 3 × 3.
The activation function was deﬁned as Leaky ReLU function.

The embedding space was deﬁned to have 10-dimensional space, and the model predicts every 5
future samples given 50 previous samples. The prior sample predictor and the conﬁdence predictor
are each deﬁned by a fully-connected layers, and predict future samples in conjunction to the decoder.
We note that the encoder and decoder were pre-trained by following (Kingma & Welling, 2014).

Image generation: The prior sample and conﬁdence predictors consist of an identical backbone
network except the last block. The network consists of four number of “Conv-BN-activation” blocks,
and the four number of “Conv transpose-BN-Activation” block. All the Conv and Conv Transpose
ﬁlters were deﬁned by 4 × 4. The last activation of the prior sample predictor is tangent hyperbolic
function and that of conﬁdence predictor is deﬁned as sigmoid function. The other activation functions
are deﬁned as Leaky-ReLU. Also, the output channel number of the last block are set to 3 and 1,
respectively. The channel size of each convolution ﬁlter and convolution-transpose ﬁlter was set to be
{64, 128, 256, 512} and {256, 128, 64, 3}.

We used batch normalization (Ioffe & Szegedy, 2015) for both networks and stride is set to two for all
the ﬁlters. The decision threshold (cid:15) is set to the running mean of the κ log(qθ,W (·)). We set κ = 2.5
for every test and conﬁrmed that it properly works for all the cases.

B SUPPLEMENTARY EXPERIMENTS

In addition to the results presented in the paper, we show supplement generation examples in below
ﬁgures. Figure 3 and Table 3 present the image sequence generation result from the other golf swing
sequence. In this case also, we can observe the similar swing cycle period changes and acceptance
ratio-error tendencies reported in the paper. Our approximation slightly affects the cycle of the
time-serious data, and the result also shows that the approximation can achieve even better prediction
results than “none-acceptance” case.

Figure 8 and 9 shows the additional facial image generation results among (cid:15) ∈ [0.0, 1.0]. We can
see that pixels from boundary region were more frequently re-sampled compared to other relatively
simple face parts such as cheek or forehead. Also, we tested our model with ImageNet classes, and
the results were presented in Figure 7.

11

Published as an arXiv pre-preint

Figure 6: Image sequence generation examples (cropped in an equivalent time-interval). From the top
to the bottom rows, each denotes the generation result with (cid:15) ∈ {1.00, 0.75, 0.50, 0.25, 0.00}.

(a) Balloon Flower

(b) Ladybug

(c) Tree-martin

(d) Sandbar

Figure 7: ImageNet generation examples.

12

Published as an arXiv pre-preint

Figure 8: CelebA generation result in 64 × 64. (cid:15) ∈ {0.0, 0.1, 0.2, 0.3, 0.4, 0.5}.

13

Published as an arXiv pre-preint

Figure 9: More selected CelebA generation result in 64×64. according to (cid:15) ∈ {0.6, 0.7, 0.8, 0.9, 1.0}.
14

