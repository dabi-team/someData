Average Biased ReLU Based CNN Descriptor for
Improved Face Retrieval

Shiv Ram Dubey, Soumendu Chakraborty

AB-ReLU

2
2
0
2

n
a
J

3
2

]

V
C
.
s
c
[

2
v
1
5
0
2
0
.
4
0
8
1
:
v
i
X
r
a

Abstract—The convolutional neural networks (CNN), including
AlexNet, GoogleNet, VGGNet, etc. extract features for many
computer vision problems which are very discriminative. The
trained CNN model over one dataset performs reasonably well
whereas on another dataset of similar type the hand-designed
feature descriptor outperforms the same trained CNN model.
The Rectiﬁed Linear Unit (ReLU) layer discards some values in
order to introduce the non-linearity. In this paper, it is proposed
that the discriminative ability of deep image representation using
trained model can be improved by Average Biased ReLU (AB-
ReLU) at the last few layers. Basically, AB-ReLU improves
the discriminative ability in two ways: 1) it exploits some of
the discriminative and discarded negative information of ReLU
and 2) it also neglects the irrelevant and positive information
used in ReLU. The VGGFace model trained in MatConvNet
over the VGG-Face dataset is used as the feature descriptor for
face retrieval over other face datasets. The proposed approach
is tested over six challenging, unconstrained and robust face
datasets (PubFig, LFW, PaSC, AR, FERET and ExtYale) and also
on a large scale face dataset (PolyUNIR) in retrieval framework.
It is observed that the AB-ReLU outperforms the ReLU when
used with a pre-trained VGGFace model over the face datasets.
The validation error by training the network after replacing all
ReLUs with AB-ReLUs is also observed to be favorable over
each dataset. The AB-ReLU even outperforms the state-of-the-
art activation functions, such as Sigmoid, ReLU, Leaky ReLU
and Flexible ReLU over all seven face datasets.

I. INTRODUCTION

The image descriptors are generally used to perform the
image matching. Most of the research in the early days
was focused on designing the hand-crafted descriptors which
have shown promising performance in several computer vision
applications, such as image matching [13], identity veriﬁcation
[40], face recognition [1], [4], image retrieval [17], biomedical
image analysis [15], [16], [14], etc. Several hand-designed
descriptors have been proposed for face retrieval as well [4],
[12], [6], [5], [11], [10]. However, the discriminative power of
these hand-crafted descriptors is limited as they are not able
to utilize the context of data.
Since last few years,

the convolutional neural networks
(CNN) have attracted the full attention of researchers in com-
puter vision community. The AlexNet [23] was the ﬁrst CNN
model that won the Imagenet classiﬁcation challenge [9] in
2012 with a great margin as compared to the best performing
hand-designed features. After Alexnet, several CNN models
have been proposed for the Imagenet classiﬁcation, such as

S.R. Dubey is with the Computer Vision Group, Indian Institute of
Information Technology (IIIT), Sri City, Andhra Pradesh, India (Email:
srdubey@iiits.in).

S. Chakraborty is with the Indian Institute of Information Technology (IIIT),

Lucknow, Uttar Pradesh, India (Email: soum.uit@gmail.com ).

VGGNet [41], GoogLeNet [42] and ResNet [21]. The network
became deeper over time , i.e., AlexNet (8 stages) in 2012,
VGGNet (16 and 19 stages) in 2014, GoogLeNet (22 stages)
in 2012, and ResNet (152 stages) in 2015. The CNNs have
been also used for different applications, such as multimedia
classiﬁcation [39], image denoising [48], and image retrieval
[26].

The deep neural network based models have also been
proposed for the face recognition task. Some recent and
renowned deep learning based approaches are DeepFace [43],
FaceNet [38], and VGGFace [34] amongst others for face
recognition. The DeepFace model has used 9 learnable layers
for face representation [43]. The number of parameters in the
DeepFace model is too high as it does not use the weight
sharing. The DeepFace model has reported an accuracy of
97.35% on the Labeled Faces in the Wild (LFW) dataset [43],
[22]. FaceNet is also proposed as the feature extractor for face
recognition and clustering [38]. The FaceNet model uses the
deep convolutional network for the feature embedding. The
FaceNet model has reported 99.63% accuracy over LFW face
dataset. The VGGFace model has utilized the convolutional
neural network (CNN) based end-to-end learning for face
recognition [34]. It is trained over a very large scale VGGFace
database with 2.6M images from 2.6K subjects. In our work,
we use the VGGFace model as a feature extractor for the face
retrieval experiments.

The pre-trained models are also used for several tasks in the
Computer Vision. The CNN features are fused with the hand-
designed features for content-based image retrieval in [27].
Very recently, it has been claimed that the trained network used
over still face images can also be used for face veriﬁcation
in the videos effectively [2]. Deep features are also used for
image quality assessment [28]. A pre-trained CNN model is
also used for the content based image retrieval [47]. Some
researchers also used the transfer learning to utilize a trained
network meant for a domain in some other domain, such as
Deep transfer [44], etc. The pre-trained VGG network has
also been used for remote-sensing image retrieval [18]. In this
paper also, the pre-trained network equipped with the proposed
ABReLU is used for the face retrieval task.

Some researchers have also focusing over different layers of
the CNN model. A center loss function is used in [49] instead
of the softmax loss function for face recognition. The ReLU
discards the negative values which actually shows the absence
of certain visual features and might be useful to improve the
discriminative ability [33]. In order to get rid of negative values
in ReLU, a Rectiﬁed Factor Network is introduced in [7]. A
Parametric Rectiﬁed Linear Unit (PReLU) is used by He et

This paper is published by Multimedia Tools and Applications, Springer, available at: https://doi.org/10.1007/s11042-020-10269-x.
The copyright has been transferred to Springer.

1

 
 
 
 
 
 
al. It is a generalization of the Rectiﬁed Linear Unit (ReLU)
where the slope of the negative region is considered as one of
the parameters of each neuron [20]. The ReLU also has the
“dying Gradient” problem where the gradient ﬂowing through
a unit can be zero forever [23]. Leaky ReLU (LReLU) tried
to ﬁx the dying gradient problem of ReLU during training
by computing a small slope for the negative inputs [29]. The
LReLU is extended to randomized leaky rectiﬁed linear units
(RReLU) by introducing a random small slope [50]. Flexible
ReLU (FReLU) is an another attempt to use the negative
values based on a threshold [37]. An exponential linear unit
(ELU) is proposed in [8] which also considers the ReLU’s
negative values. One of the recent developments in this area
is the advent of non-static activation functions [52]. Most of
the existing rectiﬁer units do not consider the negative values
which might be important. These rectiﬁer units are also not
dependent upon the input data. In this paper, a new data
dependent rectiﬁer unit is proposed to boost the discriminative
power of VGGFace descriptor.

The main contributions of the this paper are as follows:

• The suitability of the use of pre-trained CNN model over

other databases of similar type is explored.

• A new data dependent Average Biased Rectiﬁed Liner
Unit (AB-ReLU) is proposed to boost the discriminative
power of the pre-trained network.

• The suitability of proposed AB-ReLU is tested at differ-

ent layers of the network.

• The image retrieval experiments are conducted over seven
challenging face datasets, including a large scale face
dataset.

• The effect of the distances is analyzed with VGGFace

feature.

• The performance of AB-ReLU is also analyzed by train-
ing the network after replacing all ReLUs with AB-
ReLUs in VGGFace model.

layer. Moreover,

One of the underlying problems of ReLU is the diminishing
gradient in the negative direction for the negative inputs. In
neural network training, the data are always expected to be
zero-centric having unit variance. Due to the dying ReLU
problem, the variance of activations becomes too low after
some layers. The AB-ReLU is designed to encounter this
problem up to some extent by allowing even negative data
to be passed to the next
it captures the
positive or negative dominancy in the activations and tries
to put through roughly half of the activations to the next
layer. The intuition behind AB-ReLU can be seen from the
following angle of point of view also: The initial
layers
in CNN capture the coarse information, hence suppressing
anything that is less than zero cause’s a signiﬁcant information
loss. Whereas, higher layers extract abstract information which
may be redundant also, so introducing a highly non-linear
activation function increases the classiﬁcation accuracy. The
proposed AB-ReLU is able to capture the coarse and abstract
information effectively.

The rest of the paper is organized as follows: Section 2
reviews the VGGFace model and rectiﬁed linear unit; Section
3 proposes a new data dependent rectiﬁed linear unit and

TABLE I: VGGFace Layer Description. In ‘Filter’ column,
f , s and p represent
the ﬁlter size, stride and padding,
respectively. In ‘Volume Size’ column, the ﬁrst value is a
dimension of volume and the second value is depth of volume,
i.e. 224,3 represents volume size 224×224×3. The last fully
connected layer and softmax layer are not shown because the
output of ‘relu7’ is considered as the 4096-dimensional feature
vector in this work.

No.

0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35

Layer
Name
input
conv1 1
relu1 1
conv1 2
relu1 2
pool1
conv2 1
relu2 1
conv2 2
relu2 2
pool2
conv3 1
relu3 1
conv3 2
relu3 2
conv3 3
relu3 3
pool3
conv4 1
relu4 1
conv4 2
relu4 2
conv4 3
relu4 3
pool4
conv5 1
relu5 1
conv5 2
relu5 2
conv5 3
relu5 3
pool5
fc6
relu6
fc7
relu7

Layer
Type
Image
Conv
Relu
Conv
Relu
Pool
Conv
Relu
Conv
Relu
Pool
Conv
Relu
Conv
Relu
Conv
Relu
Pool
Conv
Relu
Conv
Relu
Conv
Relu
Pool
Conv
Relu
Conv
Relu
Conv
Relu
Pool
Conv
Relu
Conv
Relu

Filter

n/a
f :3,3,64, s:1, p:1
n/a
f :3,64,64, s:1, p:1
n/a
f :2, s:2, p:0
f :3,64,128, s:1, p:1
n/a
f :3,128,128, s:1, p:1
n/a
f :2, s:2, p:0
f :3,128,256, s:1, p:1
n/a
f :3,256,256, s:1, p:1
n/a
f :3,256,256, s:1, p:1
n/a
f :2, s:2, p:0
f :3,256,512, s:1, p:1
n/a
f :3,512,512, s:1, p:1
n/a
f :3,512,512, s:1, p:1
n/a
f :2, s:2, p:0
f :3,512,512, s:1, p:1
n/a
f :3,512,512, s:1, p:1
n/a
f :3,512,512, s:1, p:1
n/a
f :2, s:2, p:0
f :7,512,4096, s:1, p:0
n/a
f :1,4096,4096, s:1, p:0
n/a

Volume
Size
224,3
224,64
224,64
224,64
224,64
112,64
112,128
112,128
112,128
112,128
56,128
56,256
56,256
56,256
56,256
56,256
56,256
28,256
28,512
28,512
28,512
28,512
28,512
28,512
14,512
14,512
14,512
14,512
14,512
14,512
14,512
7,512
1,4096
1,4096
1,4096
1,4096

modiﬁed VGGFace descriptor; Section 4 presents the experi-
mental setup; Section 5 presents the results and discussions;
and ﬁnally Section 6 sets the concluding remarks.

II. RELATED WORKS

In this section, ﬁrst the original VGGFace model used in
this work is described in detail and then the original rectiﬁed
linear unit is presented.

A. VGGFace Model

In this work, the original pre-trained VGGFace model is
taken from the MatConvNet library [45] released by University
of Oxford1. This model is based on the implementation of
VGG-Very-Deep-16 CNN architecture as described in [34].
This model is trained over VGGFace database2 which consists

1http : //www.robots.ox.ac.uk/∼vgg/sof tware/vgg f ace/
2http : //www.robots.ox.ac.uk/∼vgg/data/vgg f ace/

2

Fig. 1: The original rectiﬁed linear unit (ReLU) function [33].
All the Negative (−ive) input values are converted to zero,
whereas all the Positive (+ive) input values are passed as the
identity function.

2.6M face images from 2,622 subjects. The layers of VGGFace
model are summarized in Table I. In this table, the last fully
connected layer and sofmax layer of VGGFace are not listed
as they are not required in this work. The output of ‘relu7’ is
considered as the VGGFace feature descriptor. The ﬁlter size,
stride and padding are mentioned in the ‘Filter’ column with
ﬁelds f , s and p respectively. A ﬁlter size f :3,128,256 means
total 256 ﬁlters of dimension 3×3 and depth 128. Similarly,
a volume size 112,64 means a 3-D volume of dimension
112×112 with depth 64. In this work, changes are made in
selected rectiﬁed linear unit (ReLU) layers, especially in the
last few layers as described in the next section.

B. Rectiﬁed Linear Unit

The rectiﬁed linear unit (ReLU) in a neural network is used
to introduce the non-linearity [33], [23]. The ReLU simply
works like a ﬁlter, ignores the negative signals and pass the
positive signals. Consider I n
is the input volume to ReLU
i
at nth layer of any network and I n
o is the output volume of
ReLU for (n)th layer. Suppose the input volume I n
is of
i
dimension d and Dk is the size of the input volume in the
kth dimension ∀k ∈ [1, d]. Then, an element at position ρ =
(ρ1, ρ2, · · · , ρd) of output volume I n
o is computed from the
corresponding element of input volume I n
i as follows,

(cid:40)

I n
o (ρ) =

I n
i (ρ),
0,

i (ρ) > 0

if I n
otherwise

(1)

where ρ is d-dimensional, Dk is the size of I n
in kth
i
dimension, ρk ∈ [1, Dk] ∀k ∈ [1, d]. The ReLU function
is illustrated in Fig. 1. It is linear in the Positive (+ive)
range, whereas zero in the Negative (−ive) range. The main
drawback with ReLU is that it passes all +ive values even
they are not important and blocks all −ive values irrespective
of their important. This problem is solved by introducing a
data dependent average biased ReLU as elaborated in the next
section.

III. PROPOSED FACE DESCRIPTOR

In this section, ﬁrst a data dependent average biased rectiﬁed
linear unit (AB-ReLU) is proposed,
is applied to
existing pre-trained VGGFace model [34] to create a more
discriminative face descriptor, and ﬁnally AB-ReLU based
VGGFace descriptor is used for face retrieval.

then it

A. Average Biased Rectiﬁed Linear Unit

It can be noticed in ReLU that it is not data dependent.
It rejects all the −ive signals and passes all the +ive signals
which can lead to less discriminative feature extraction. In this
section, the problem is resolved by introducing a new data
dependent ReLU named average biased rectiﬁed linear unit
(AB-ReLU). The AB-ReLU is data dependent by exploiting
the average property of the input volume. It also works like a
ﬁlter and passes only those signals which satisfy the average
biased criteria. The average biased criteria ensures that only
important features get passed irrespective of its sign. Suppose,
i and I n
AB-ReLU is used in any network at nth layer and I n
o
are input volume and output volume for this layer respectively.
Then, the ρth element of output layer I n
o is given by following
equation,

I n
o (ρ) =

(cid:40)

I n
i (ρ) − β,
0,

i (ρ) − β > 0

if I n
otherwise

(2)

where ρ = (ρ1, ρ2, · · · , ρd) represents the position of an
element, d is the dimension of I n
in
kth dimension, ρk ∈ [1, Dk] ∀k ∈ [1, d], and β is the average
biased factor deﬁned as follows,

i , Dk is the size of I n
i

β = α × An
i

where α is a parameter to be set empirically and An
i
average of input volume computed as follows,

(3)

is the

(4)

ρ1=1

An

i =

(cid:80)D2

(cid:80)D1

ρd=1 I n

i (ρ1, ρ2, · · · , ρd)

ρ2=1 · · · (cid:80)Dd
D1 × D2 × · · · × Dd
The AB-ReLU leads to two AB-ReLUs, i.e. +ive AB-
ReLU and −ive AB-ReLU based upon the input data. This
behavior of AB-ReLU is illustrated in Fig. 2 where Fig. 2a
shows the +ive ReLU function and Fig. 2b depicts the −ive
AB-ReLU function. The +ive AB-ReLU signiﬁes the +ive
average biased scenario when the input data volume An
i has
the −ive majority, i.e. An
i < 0 and allows some prominent
−ive signals by converting them into +ive signal with the
addition of an average biased factor of input volume (β).
Similarly, if the input data volume An
i has the +ive majority,
i.e. An
i ≥ 0 then AB-ReLU blocks even some inferior +ive
signals along with all −ive signals by subtracting the average
biased factor of input volume (β). The default value of α is
set to 1.

The average bias computed by the AB-ReLU works like a
controller to control the dominance of the positive and the
negative values. If the negative values are dominant, then
there is a higher possibility that these values are utilized by
the AB-ReLU, as opposed to the ReLU which discards such
values. Even though AB-ReLU considers the negative values,
it discards the higher negatives. In case of positive dominance
of the features there is a possibility of having the irrelevant
lower-range positive values. Such values are discarded by the
average bias of the AB-ReLU. Whereas, the ReLU does not
ﬁlter the irrelevant positive features. The ABReLU does not
simply discard the negative values unlike ReLU. Rather, it
ﬁnds the important negative values and adds the bias to shift
it towards the positive values. The utilization of the positive

3

(a) AB-ReLU if An

i < 0

(b) AB-ReLU if An

i ≥ 0

Fig. 2: The average biased rectiﬁed linear unit (AB-ReLU) function. Here, β represents an average biased factor. The effective
biased is +ive in (a) because the value of β is −ive, whereas effective biased is −ive in (b) because the value of β is +ive.
The −ive β represents that the −ive values are also important, whereas the +ive β represents that all the +ive values are
not important.

feature values is not only speciﬁc to the face retrieval task.
Generally, the positive values are given more importance in
CNN as it is generated by convolving the input with the
kernels. A kernel produces the positive values for a neuron
if such types of features are present in the input, otherwise, it
produces a negative value. Thus, the positive values in CNN
can represent the existence of the important visual cues in the
image. Finer visual information might not be covered through
the learnt positive values. However, such information can play
a crucial role in face related applications where the texture
information in the faces is generally similar. In this paper, we
utilize such features by shifting it towards the positive features.
In the next subsection, the AB-ReLU is used to construct the
descriptor.

B. AB-ReLU based VGGFace Descriptor

In this subsection, the VGGFace model is used with AB-
ReLU to construct the improved VGGFace descriptors. The
description of the different models is summarized in Table II.
The AB-ReLU is applied directly over pre-trained VGGFace
model at some layers instead of simple ReLU. The output of
layer35 (i.e. ReLU) of original pre-trained VGGFace model
after reshaping into a 1-D array is used as the VGGFace
descriptor and represented by VGGFace35ReLU (or just 35R
as shorthand notation). The ﬁrst descriptor is proposed by
simply replacing the last ReLU, i.e. at layer35 with AB-ReLU
and converting its output into a 1-D array. This descriptor is
represented by VGGFace35AB-ReLU (i.e. 35AR) for α = 1.
The other variants of this descriptor are VGGFace35AB-
ReLU2 (i.e. 35AR2) and VGGFace35AB-ReLU5 (i.e. 35AR5)
for α = 2 and α = 5 respectively. Similarly, other descriptors
are generated by replacing some ReLU of VGGFace with AB-
ReLU. In second descriptor i.e. VGGFace33AB-ReLU (i.e.
33AR) for α = 1, layer34 and layer35 are removed, the
ReLU at layer33 is replaced with AB-ReLU and the output of
layer33 is considered as the descriptor after reshaping into
a 1-D array. Its other variants are VGGFace33AB-ReLU2

(i.e. 33AR2) and VGGFace33AB-ReLU5 (i.e. 33AR5) for
α = 2 and α = 5 respectively. In VGGFace33AB-ReLU 35
(i.e. 33AR 35) descriptor, the ReLU at layer33 is replaced
with AB-ReLU while the output of layer35 using ReLU is
considered as the descriptor. AB-ReLU is applied in multiple
layers, i.e. at layer33 and layer35 in VGGFace33,35AB-ReLU
layer30.
(i.e. 33,35AR). The AB-ReLU is also applied at
Two descriptors namely VGGFace30AB-ReLU (i.e. 30AR)
and VGGFace30AB-ReLU 35 (i.e. 30AR 35) are considered
for the experiments. In VGGFace30AB-ReLU,
the output
layer30 (i.e. AB-ReLU) is taken as the descriptor, whereas in
VGGFace30AB-ReLU 35, the AB-ReLU is used at layer30
and the output of last layer (i.e. layer35) is taken as the
descriptor. In experiment section, the shorthand notations of
descriptor are used. Note that all the descriptors are normalized
to unit sum by dividing all the features with sum of features.
The effect of AB-ReLU with pre-trained VGGFace
(VGGFace35AB-ReLU) is illustrated with an example face
image in Fig. 3. The example face image displayed in Fig.
3a is taken from the LFW database [22]. This example face
image is used as the input to the pre-trained VGGFace model
and features are computed before and after layer35. Fig. 3b
shows the input signal for last layer (i.e. layer35). The output
signal of ReLU at layer35 is displayed in Fig. 3c. In Fig. 3d,
3e, and 3f, the output signals of AB-ReLU for α=1, 2, and
5 respectively are illustrated. For this example, An
v < 0 at
layer 35, it can be also observed from the Fig. 3 that the AB-
ReLU function passes more signal as compared to the ReLU
function.

IV. EXPERIMENTAL SETUP

In this paper, the image retrieval framework is adapted for
the experiments. The face retrieval is done using the proposed
AB-ReLU based VGGFace descriptor. In face retrieval, the top
matching faces are returned from a database for a given query
face based on the description of the faces. The best matching
faces are decided based on the similarity scores between the

4

TABLE II: The summary of the different models including name, short name and description.

Name

VGGFace35ReLU
VGGFace35AB-ReLU
VGGFace35AB-ReLU2
VGGFace35AB-ReLU5
VGGFace33ReLU
VGGFace33AB-ReLU
VGGFace33AB-ReLU2
VGGFace33AB-ReLU5

Short
Name
35R
35AR
35AR2
35AR5
33R
33AR
33AR2
33AR5

VGGFace33AB-ReLU 35

33AR 35

VGGFace33,35AB-ReLU
VGGFace30ReLU
VGGFace30AB-ReLU

33,35AR
30R
30AR

VGGFace30AB-ReLU 35

30AR 35

Description

The output of layer35 (i.e. ReLU) of original pre-trained VGGFace model.
The output of layer35 after replacing the ReLU at layer35 with AB-ReLU having α = 1.
The output of layer35 after replacing the ReLU at layer35 with AB-ReLU having α = 2.
The output of layer35 after replacing the ReLU at layer35 with AB-ReLU having α = 5.
The output of layer33 (i.e. ReLU) of original pre-trained VGGFace model.
The output of layer33 after replacing the ReLU at layer33 with AB-ReLU having α = 1.
The output of layer33 after replacing the ReLU at layer33 with AB-ReLU having α = 2.
The output of layer33 after replacing the ReLU at layer33 with AB-ReLU having α = 5.
The ReLU at layer33 is replaced with AB-ReLU while the output of layer35 using ReLU is considered
as the descriptor.
The output of layer35 after applying AB-ReLU at layer33 and layer35.
The output of layer30 (i.e. ReLU) of original pre-trained VGGFace model.
The output of layer30 after replacing the ReLU at layer30 with AB-ReLU having α = 1.
The ReLU at layer30 is replaced with the AB-ReLU while the output of layer35 using ReLU is
considered as the descriptor.

(a) Example Image

(b) Input to Layer 35

(c) Layer 35 (ReLU)

(d) Layer 35 (AB-ReLU, α = 1)

(e) Layer 35 (AB-ReLU, α = 2)

(f) Layer 35 (AB-ReLU, α = 5)

Fig. 3: An example illustrating the ReLU and AB-ReLU in terms of the ﬁnal feature of layer 35 of VGGFace model. (a)
An Example Image from LFW database [22]. (b) Input to Layer 35. (c) Output of Layer 35 (ReLU). (d) Output of Layer 35
(AB-ReLU, α = 1). (e) Output of Layer 35 (AB-ReLU, α = 2). (f) Output of Layer 35 (AB-ReLU, α = 5).

query face and the database faces. In this work, the similarity
scores are considered as the distance between the descriptor
of the query face and the descriptors of the database faces.
The lower distance between two feature descriptors represents
more similarity amongst the corresponding face images and
vice versa.

A. Distances Measures

In image retrieval, the performance also depends upon the
distance measures used for ﬁnding the similarity scores. In
order to compute the performance, top few faces are retrieved.
The Chi-square (Chisq) distance is used in most of the
experiments in this work. The Euclidean, Cosine, Earth Mover
Distance (Emd), L1, and D1 distances are also adapted to ﬁnd

out the more suitable distance in the current scenario [32],
[17].

B. Evaluation Criteria

In order to present the result of face retrieval and compari-
son, the standard evaluation metrics are used in this paper such
as precision, recall, f-score, and retrieval rank. All the images
from a database are treated as the query image (i.e., the probe
sample) one by one and the rest of the images as gallery to
report the average performance over full database. The average
retrieval precision (ARP) and average retrieval rate (ARR) over
full database are computed as the average of mean precisions
(MP) and mean recalls (MR) respectively, over all categories.
The MP and MR for a category is calculated as the mean of

5

1000200030004000Input Volume Array (Reshaped to 1-D)-50-40-30-20-1001020ValuesInput to Layer35 (ReLU)1000200030004000Output Volume Array (Reshaped to 1-D)05101520ValuesOutput of Layer35 (ReLU)1000200030004000Output Volume Array (Reshaped to 1-D)05101520253035ValuesOutput of Layer35 (AB-ReLU)1000200030004000Output Volume Array (Reshaped to 1-D)01020304050ValuesOutput of Layer35 (AB-ReLU2)1000200030004000Output Volume Array (Reshaped to 1-D)020406080100ValuesOutput of Layer35 (AB-ReLU5)precisions and recalls respectively, by turning all the images
in that category as the query one by one. The precision (P r)
and recall (Re) for a query image is calculated as follows,

P r =

Re =

#Correct Retrieved Images
#Retrieved Images

#Correct Retrieved Images
#Similar Images In Database

(5)

The F-score is calculated from the ARP and ARR values with
the help following equation,

F − score = 2 ×

ARP × ARR
ARP + ARR

(6)

In order to test the accurate rank of correctly retrieved faces,
the average normalized modiﬁed retrieval rank (ANMRR)
metric is adapted [16]. The better retrieval performance is
inferred from the higher values of ARP, ARR and F-Score,
and lower value of the ANMRR and vice-versa.

C. Databases Used

Seven challenging, unconstrained and robust face databases
are used to demonstrate the effectiveness of the proposed
AB-ReLU based VGGFace descriptor: PaSC [3], LFW [22],
PubFig [24], FERET [36], [35], AR [30], [31], ExYaleB [19],
[25] and PolyUNIR [51]. Viola Jones object detection method
[46] is adopted in this paper to detect and crop the face
regions in the images. The faces are resized to 224 × 224
and ‘zerocenter’ normalization is applied before feeding to
the proposed AB-ReLU based VGGFace model.

The PaSC still image face database consists of 9376 images
from 293 individuals with 32 images per individual [3].
The PaSC database has the variations like blur, pose, and
illumination and regarded as one of the difﬁcult databases.
This database has 8718 faces after face detection using Viola
Jones detector. In current scenario, the unconstrained face
retrieval is very demanding due to the increasing number of
faces over the Internet. In this paper, LFW [22] and PubFig
[24] databases are considered. These two databases have the
images from the Internet with several unconstrained variations,
such as pose, lighting, expression, scene, camera, etc. In the
image retrieval framework, it is required to retrieve more than
one (typically 5, 10, etc.) top matching images. To achieve
this sufﬁcient number of images should be available for each
category in the database. Considering this fact, individuals
having at least 20 images are taken in the LFW database
(i.e. 2984 faces from 62 individuals) [22]. The Public Figure
database (i.e., PubFig) consists 6472 faces from 60 individuals
[24]. Following the URLs given in the PubFig face database,
the images are downloaded directly from the Internet after
removing the dead URLs.

In order to experiment with the robustness of the descrip-
tor, FERET, AR and Extended Yale B face databases are
used. “Portions of the research in this paper use the FERET
database of facial images collected under the FERET program,
sponsored by the DOD Counterdrug Technology Development
Program Ofﬁce” [36], [35]. The cropped version of the Color-
FERET database having 4053 faces from 141 people (only
subjects having at least 20 faces) is considered in this work.

Several variations like the expression and pose (13 different
poses) are present in the FERET database. The cropped version
of the AR face database is also used for the experiments [30],
[31]. The AR database has the masking effect where some
portions of the face are occluded along with the illumination
and color effect. Total 2600 face images are available for 100
people in AR database. Extended Yale B (ExYaleB) database
is created with severe amount of illumination differences (i.e.
64 types of illuminations) [19], [25]. Total 2432 cropped faces
from 38 persons with 64 faces per person are present in the
ExYaleB database for the face retrieval. The description of
PolyUNIR dataset is provided section 5.4.

V. RESULTS AND DISCUSSIONS

In this work, the content based image retrieval framework is
adopted for the experiments and comparison. In this section,
ﬁrst result comparison is presented by ﬁxing the similar-
ity measure as Chi-square distance,
then the performance
of proposed VGGFace35AB-ReLU descriptor is tested with
different similarity measures, then the training is performed
by replacing all ReLU with AB-ReLU and validation error
is compared over six datasets, after that the experiment over
a large scale dataset is performed, and ﬁnally the results are
compared with other functions.

A. Results Comparison

(35AR),

VGGFace35AB-ReLU2

Several VGGFace descriptors with AB-ReLU at different
layers such as VGGFace35ReLU (35R), VGGFace35AB-
ReLU
(35AR2),
VGGFace35AB-ReLU5 (35AR5), VGGFace33ReLU (33R),
VGGFace33AB-ReLU2
(33AR),
VGGFace33AB-ReLU
(33AR)2, VGGFace33AB-ReLU5 (33AR5), VGGFace33AB-
ReLU 35 (33AR 35), VGGFace33,35AB-ReLU (33,35AR),
VGGFace30AB-ReLU
VGGFace30AB-ReLU
(30AR),
(30AR), and VGGFace30AB-ReLU 35 (30AR 35), etc. are
used for the experiments. Here, 35R, 33R and 30R represent
the ReLU features at layer no. 35, 33 and 30 respectively;
35AR, 33AR and 30AR represent
the AB-ReLU features
for average biased parameter α = 1 at layer no. 35, 33 and
30 respectively; 35AR2 and 33AR2 represent the AB-ReLU
features for average biased parameter α = 2 at layer no.
35 and 33 respectively; 35AR5 and 33AR5 represent
the
AB-ReLU features for average biased parameter α = 5 at
layer no. 35 and 33 respectively; 30AR 35 represents the
ReLU features at layer 35 with the AB-ReLU operator at
layer 30; and 33,35AR represents the AB-ReLU features at
layer 35 with the AB-ReLU operator at layer 33.

The average retrieval precision (ARP) for the topmost match
(i.e. Rank-1 Accuracy) is illustrated in Table III over the
PaSC, LFW, PubFig, FERET, AR, and ExYaleB databases.
It is observed from Table III that the performance of 35AR
and 35AR2 is better, mainly over the unconstrained databases,
whereas the performance of 30R and 30AR is better over
robust databases like AR and ExYaleB. It is also noted that the
performance of AB-ReLU (35AR) is improved as compared
layer 35. The AB-ReLU is better
to the ReLU (35R) at
than ReLU over unconstrained databases like PaSC, LFW

6

TABLE III: Average Retrieval Precision, ARP(%) for topmost match using AB-ReLU and VGGFace based descriptors over
the PaSC, LFW, PubFig, FERET, AR and ExYaleB databases. It is also equivalent to the rank-1 accuracy. The results for best
performing descriptor for a database is highlighted in bold. Here, 35R, 33R and 30R represent the ReLU features at layer no.
35, 33 and 30 respectively; 35AR, 33AR and 30AR represent the AB-ReLU features for average biased parameter α = 1 at
layer no. 35, 33 and 30 respectively; 35AR2 and 33AR2 represent the AB-ReLU features for average biased parameter α = 2
at layer no. 35 and 33 respectively; 35AR5 and 33AR5 represent the AB-ReLU features for average biased parameter α = 5
at layer no. 35 and 33 respectively; 33AR 35 represents the ReLU features at layer 35 with the AB-ReLU operator at layer
33; and 33,35AR represents the AB-ReLU features at layer 35 with the AB-ReLU operator at layer 33.

Database
PaSC
LFW
PubFig
FERET
AR
ExYaleB

35R
93.06
99.10
98.22
95.64
99.73
85.77

35AR
93.88
99.53
98.35
95.87
99.77
86.39

35AR2
93.89
99.31
98.59
95.56
99.81
86.27

35AR5
93.83
99.24
98.54
95.42
99.81
85.90

33R
93.36
99.21
98.25
94.79
99.85
86.92

33AR
93.82
99.36
98.32
94.35
99.81
86.55

33AR2
93.88
99.32
98.52
94.22
99.81
86.18

33AR5
93.84
99.37
98.43
93.57
99.81
85.53

33AR 35
92.96
99.22
98.08
95.74
99.77
85.90

33,35AR 30R
93.04
99.14
98.09
95.74
99.77
85.81

91.95
98.05
94.55
91.89
99.96
92.52

30AR
86.98
94.82
91.76
92.94
99.96
92.52

30AR 35
93.02
99.30
97.63
95.70
99.77
86.72

TABLE IV: ARP(%) for 5 numbers of retrieved images using AB-ReLU and VGGFace based descriptors over the PaSC, LFW,
PubFig, FERET, AR and ExYaleB databases. The results for the best descriptor for a database is highlighted in bold.

Database
PaSC
LFW
PubFig
FERET
AR
ExYaleB

35R
87.91
98.33
96.51
88.04
94.85
77.31

35AR
89.33
98.49
97.04
88.46
95.15
77.97

35AR2
89.60
98.44
97.37
88.01
95.21
77.71

35AR5
89.37
98.39
97.30
87.94
95.12
77.29

33R
87.83
98.39
96.84
84.65
95.35
76.28

33AR
88.79
98.50
97.19
84.73
95.56
76.43

33AR2
89.07
98.51
97.19
84.85
95.65
76.17

33AR5
89.27
98.51
97.13
84.75
95.63
75.84

33AR 35
87.79
98.21
96.53
87.98
94.61
76.97

33,35AR 30R
87.80
98.17
96.53
87.98
94.57
76.98

79.84
94.05
90.28
65.84
94.24
82.71

30AR
68.46
86.31
84.19
66.75
90.83
81.60

30AR 35
86.87
98.11
95.84
86.91
94.78
77.58

TABLE V: ARP(%) for 10 numbers of retrieved images using AB-ReLU and VGGFace based descriptors over the PaSC,
LFW, PubFig, FERET, AR and ExYaleB databases. The results for the best descriptor for a database is highlighted in bold.

Database
PaSC
LFW
PubFig
FERET
AR
ExYaleB

35R
83.11
97.34
95.06
80.28
80.93
70.64

35AR
85.08
97.69
95.71
81.22
81.95
71.54

35AR2
85.39
97.52
95.90
80.92
82.05
71.43

35AR5
85.13
97.34
95.83
80.64
82.07
71.05

33R
82.79
97.45
95.41
75.90
80.47
68.05

33AR
83.94
97.54
95.75
75.83
81.63
68.17

33AR2
84.37
97.63
95.72
76.18
81.95
68.40

33AR5
84.10
97.34
95.54
75.77
82.31
68.68

33AR 35
82.92
97.17
94.91
80.14
80.47
70.27

33,35AR 30R
82.89
97.12
94.92
80.16
80.45
70.26

68.51
89.14
85.85
45.61
80.76
70.39

30AR
54.21
77.52
77.28
45.26
73.83
68.51

30AR 35
81.48
96.87
94.11
77.80
79.29
70.49

TABLE VI: Average Retrieval Rate, ARR(%) for 10 numbers of retrieved images using AB-ReLU and VGGFace based
descriptors over the PaSC, LFW, PubFig, FERET, AR and ExYaleB databases. The results for the best descriptor for a
database is highlighted in bold.

Database
PaSC
LFW
PubFig
FERET
AR
ExYaleB

35R
28.06
31.48
17.44
30.32
31.13
11.04

35AR
28.74
31.61
17.57
30.67
31.52
11.18

35AR2
28.83
31.53
17.64
30.54
31.56
11.16

35AR5
28.74
31.46
17.62
30.43
31.56
11.10

33R
27.95
31.51
17.53
28.63
30.95
10.63

33AR
28.33
31.53
17.61
28.57
31.40
10.65

33AR2
28.48
31.57
17.60
28.73
31.52
10.69

33AR5
28.38
31.44
17.58
28.59
31.66
10.73

33AR 35
27.99
31.42
17.41
30.27
30.95
10.98

33,35AR 30R
27.98
31.39
17.42
30.27
30.94
10.98

23.09
28.31
15.21
17.10
31.06
11.00

30AR
18.29
24.04
13.05
17.10
28.40
10.70

30AR 35
27.50
31.31
17.21
29.32
30.50
11.01

TABLE VII: F-Score(%) for 10 numbers of retrieved images using AB-ReLU and VGGFace based descriptors over the PaSC,
LFW, PubFig, FERET, AR and ExYaleB databases. The results for the best descriptor for a database is highlighted in bold.

Database
PaSC
LFW
PubFig
FERET
AR
ExYaleB

35R
41.89
46.05
26.86
43.46
44.96
19.09

35AR
42.89
46.23
27.06
43.97
45.53
19.33

35AR2
43.04
46.13
27.17
43.79
45.58
19.30

35AR5
42.90
46.02
27.14
43.63
45.59
19.20

33R
41.72
46.10
26.98
41.05
44.71
18.39

33AR
42.30
46.13
27.11
40.97
45.35
18.42

33AR2
42.52
46.18
27.09
41.20
45.53
18.49

33AR5
42.37
46.00
27.06
40.99
45.73
18.56

33AR 35
41.79
45.96
26.82
43.38
44.70
18.99

33,35AR 30R
41.77
45.92
26.83
43.40
44.69
18.99

34.50
41.55
23.57
24.55
44.87
19.03

30AR
27.31
35.42
20.52
24.50
41.02
18.52

30AR 35
41.06
45.80
26.52
42.05
44.05
19.05

7

TABLE VIII: Average Normalized Modiﬁed Retrieval Rank (ANMRR) in % for 10 numbers of retrieved images using AB-
ReLU and VGGFace based descriptors over the PaSC, LFW, PubFig, FERET, AR and ExYaleB databases. The results for the
best performing descriptor (i.e. least ANMRR value) for is highlighted in bold.

Database
PaSC
LFW
PubFig
FERET
AR
ExYaleB

35R
4.40
0.42
0.85
13.01
4.13
10.82

35AR
3.40
0.38
0.66
12.15
3.40
9.98

35AR2
3.43
0.44
0.58
12.50
3.42
9.88

35AR5
3.61
0.49
0.60
12.80
3.41
10.22

33R
4.43
0.41
0.72
17.24
4.25
13.68

33AR
3.98
0.41
0.58
17.43
3.53
13.59

33AR2
3.83
0.40
0.62
17.15
3.32
13.36

33AR5
4.23
0.55
0.64
17.69
3.30
13.12

33AR 35
4.51
0.46
0.91
13.12
4.51
11.18

33,35AR 30R
4.54
0.49
0.89
13.11
4.52
11.17

17.60
5.29
6.65
49.50
3.80
10.76

30AR
33.97
16.33
14.44
49.86
10.54
12.93

30AR 35
5.28
0.32
1.25
15.31
5.24
10.96

TABLE IX: ARP in % for 10 numbers of retrieved images using VGGFace35ReLU (35R) and VGGFace35AB-ReLU (35AR)
descriptors with different distance measures. Note that ARP value highlighted in bold represents the best distance over a
database for AB-ReLU.

Database

PaSC
LFW
PubFig
FERET
AR
ExYaleB

ReLU

79.38
95.06
92.56
78.72
79.25
69.98

Euclidean
AB-ReLU

84.89
97.65
95.64
81.32
81.80
71.45

ReLU

82.83
97.21
94.90
79.88
80.51
70.27

Cosine
AB-ReLU

84.84
97.64
95.63
81.18
81.78
71.34

ReLU

82.96
97.30
94.99
80.21
80.85
70.53

L1
AB-ReLU

85.01
97.66
95.67
81.25
81.93
71.48

ReLU

82.96
97.30
95.00
80.23
80.85
70.51

D1
AB-ReLU

85.01
97.66
95.67
81.25
81.93
71.48

ReLU

83.11
97.33
95.06
80.28
80.93
70.64

Chi-square
AB-ReLU

85.08
97.69
95.71
81.22
81.95
71.54

and PubFig at layer 33 (see the comparison between 33R
and 33AR in Table III). Interestingly, the AB-ReLU performs
better or similar to the ReLU over robust databases such
layer 30 (Please see the
as FERET, AR and ExYaleB at
comparative results of 30R and 30AR in Table III). The reason
behind such a performance is due to more abstract
level
feature learning towards the last layer. So, changing the learned
features at early layers affects feature extraction adversely.

The FaceNet approach [38] has reported the best accuracy
of 99.63% over LFW database. Whereas, the proposed method
has achieved the best accuracy of 99.53% as shown in Table
II, which is very close to the FaceNet result. However, the
FaceNet approach has used 22-layer architecture and the
proposed approach uses the 16-layer pre-trained VGGFace
architecture. One of the major differences with FaceNet ap-
proach is that it uses triplet loss instead of softmax loss are
used in the proposed method.

Table IV listed the ARP values when 5 best faces are
retrieved. This result shows that the performance is generally
better for the parameter α = 2, i.e. 35AR2 and 33AR2. The
picture is clear from Table V, where ARP is reported for
10 retrieved images. Descriptors constructed using AB-ReLU
at the last layer (i.e. layer35) are superior for most of the
databases. However, optimum result is achieved at 33rd layer
over AR database. One possible reason is that the trained faces
of VGGFace database are not masked. The result in Table V
conﬁrms that AB-ReLU is better suited for the descriptor as
compared to ReLU at layer35. The ARR and F-Score values
are summarized in Table VI and Table VII respectively, for 10
numbers of retrieved images. The similar trend is observed in
the results of ARR and F-Score that 35AR and 35AR2 are the
best performing VGGFace network based descriptors. Some
variations can be seen in the ANMRR results for same 10
best matching retrieved images in Table VIII as compared to

the ARP, ARR and F-Score because ANMRR penalizes the
rank heavily for false positive retrieved images. Still 35AR is
better over PaSC and FERET databases and 35AR2 is better
over PubFig and ExYaleB databases. It can be noticed that the
F-Score and ANMRR over LFW database is best for 35AR
and 30AR 35 descriptors respectively. It means that, the true
positive rate for 30AR 35 descriptor over LFW database is
low as compared to 35AR descriptor, whereas the retrieved
faces using 30AR 35 are closer to the query face in terms of
their ranks.

B. Effect of Similarity Measure

In the comparison results of the previous subsection, Chi-
square distance was adopted as the similarity measure. This
experiment is conducted to reveal the best suitable similarity
measure for the proposed descriptor. The ARP values us-
ing VGGFace35AB-ReLU (i.e. 35AR) descriptor over each
database are presented in Table IX. The ARP values using
VGGFace35ReLU (i.e. 35R) are listed to depict
the per-
formance comparison between ReLU and AB-ReLU using
different distance measure. In this experiment, 10 top matching
images are retrieved with different distances. The Euclidean,
Cosine, L1, D1 and Chi-square distances are experimented and
reported in Table IX. It is noticed that the Chi-square distance
is better suited for each database except the FERET database.
It is also observed across the Table IX that the AB-ReLU
consistently performs better than ReLU using each distance
measure.

C. Comparative Study Replacing all ReLU with AB-ReLU

We also did experiments after replacing all ReLUs with
AB-ReLUs in pre-trained VGGFace model. By doing so, the
performance decreased as changing the values of pre-trained

8

(a) Validation error over AR database

(b) Validation error over FERET database

(c) Validation error over LFW database

(d) Validation error over PaSC database

(e) Validation error over PubFig database

(f) Validation error over ExYaleB database

Fig. 4: Validation error over different databases with ReLU and AB-ReLU (α = 0.1) across all layers.

model in initial layers impacts the ﬁnal outcome adversely.
Thus, in order to generalize the feature extraction using AB-
ReLU, we retained the VGGFace model using the Pre-trained
weights for initialization. The training and validation are
performed over each database. In this experiment, following is
the hyper-parameter settings: α = 0.1 for AB-ReLU, Weight
Decay = 0.0001, Optimizer: SGDM, Momentum = 0.95, Batch
Size = 16, No. of Epochs = 100, and Learning rate = 0.000001
for all epochs. The validation errors of ReLU and AB-ReLU
over different databases are shown in Fig. 4. It is evident from

Fig. 4 that the network achieves lower validation error when it
has been trained with AB-ReLU. Moreover, the convergence
using AB-ReLU is faster than ReLU.

D. Experiment over Large Scale PolyUNIR Face Dataset

In order to judge the performance of the proposed feature
extractor on the large scale face dataset, we also performed
an experiment over PolyUNIR face dataset3 [51]. This dataset

3http://www.comp.polyu.edu.hk/ biometrics/NIRFace/polyudb face.htm

9

0102030405060708090100Epoch00.10.20.30.40.50.60.70.80.91Validation ErrorAR DatasetReLUAB-ReLU0102030405060708090100Epoch00.10.20.30.40.50.60.70.80.91Validation ErrorFERET DatasetReLUAB-ReLU0102030405060708090100Epoch00.10.20.30.40.50.60.70.80.91Validation ErrorLFW DatasetReLUAB-ReLU0102030405060708090100Epoch00.10.20.30.40.50.60.70.80.91Validation ErrorPaSC DatasetReLUAB-ReLU0102030405060708090100Epoch00.10.20.30.40.50.60.70.80.91Validation ErrorPubFig DatasetReLUAB-ReLU0102030405060708090100Epoch00.10.20.30.40.50.60.70.80.91Validation ErrorExYaleB DatasetReLUAB-ReLUthe absolute value of LReLU features. The Chi-square distance
measure is considered in this comparison. The experimental
is same as in the
setting for the PolyUNIR face dataset
previous experiment. The results in terms of the ARP values
for 10 numbers of retrieved images are summarized in Table X
using Sigmoid, ReLU, LReLU, FReLU and the proposed AB-
ReLU functions. These results conﬁrm the superiority of the
proposed Average Biased ReLU over Sigmoid, ReLU, Leaky
ReLU and Flexible ReLU. Due to the saturation property of
Sigmoid function, it loses the discriminative ability. Thus,
the results of Sigmoid function are very bad. The LReLU
and FReLU do not utilize the distributive property of the
generated features which is incorporated in the proposed AB-
ReLU function.

VI. CONCLUSION
In this paper, an average biased rectiﬁed linear unit (AB-
ReLU) is proposed for the image representation using CNN
model. The AB-ReLU is data dependent and adjust the thresh-
old based on the positive and negative data dominance. It
considers the average of the input volume to adjust the input
volume itself. The advantage of AB-ReLU is that it allows the
important negative signals as well as blocks the irrelevant pos-
itive signals based on the nature of the input volume. The AB-
ReLU is applied over pre-trained VGGFace model at last few
layers by replacing the conventional ReLU layers. The face
retrieval experiments are conducted to test the performance of
AB-ReLU based VGGFace descriptor. Seven challenging face
datasets are considered, including three unconstrained datasets,
three robust datasets and one large scale dataset. Based on the
experimental analysis, it is concluded that AB-ReLU is better
suited than the simple ReLU at the last layer for a pre-trained
CNN model based feature description. Favorable performance
is reported in both unconstrained as well as robust scenarios.
It is also found that the Chi-square distance is better suited
for the proposed descriptor for face retrieval. It is also noticed
that the network can be trained by replacing all ReLU and
AB-ReLU with improved validation. The suitability of the
proposed method is also conﬁrmed through large scale dataset.
The AB-ReLU also outperforms the activation functions such
as Sigmoid, ReLU, Leaky ReLU and Flexible ReLU using
pre-trained VGGFace model for the face retrieval task.

ACKNOWLEDGEMENT
This research is funded by IIIT Sri City, India through
the Faculty Seed Research Grant. We gratefully acknowledge
the support of NVIDIA Corporation with the donation of the
GeForce Titan X Pascal used for this research.

REFERENCES

[1] Ahonen, T., Hadid, A., Pietikainen, M.: Face description with local
binary patterns: Application to face recognition. IEEE transactions on
pattern analysis and machine intelligence 28(12), 2037–2041 (2006)
[2] Bansal, A., Castillo, C., Ranjan, R., Chellappa, R.: The do’s and don’ts
for cnn-based face veriﬁcation. arXiv preprint arXiv:1705.07426 (2017)
[3] Beveridge, J.R., Phillips, P.J., Bolme, D.S., Draper, B.A., Givens, G.H.,
Lui, Y.M., Teli, M.N., Zhang, H., Scruggs, W.T., Bowyer, K.W., et al.:
The challenge of face recognition from digital point-and-shoot cameras.
In: Biometrics: Theory, Applications and Systems (BTAS), 2013 IEEE
Sixth International Conference on, pp. 1–8. IEEE (2013)

Fig. 5: The ARP (%) using the feature vectors of layer 35
generated using ReLU (i.e., 35R) and AB-ReLU (i.e., 35AR,
35AR2 and 35AR5) methods.

TABLE X: The results comparison between Sigmoid, ReLU,
Leaky ReLU (LReLU with 0.01 leaky factor), Flexible ReLU
(FReLU with 0.2 ﬂexible factor) and the proposed AB-ReLU
at layer 35 in terms of the ARP in % for 10 numbers of
retrieved images using Chi-square distance measure. Note that
ARP value highlighted in bold represents the best result for a
database.

Database
PaSC
LFW
PubFig
FERET
AR
ExYaleB
PolyUNIR

Sigmoid
29.71
53.78
50.66
39.60
32.34
43.84
93.05

ReLU
83.11
97.34
95.06
80.28
80.93
70.64
98.54

LReLU
83.56
97.38
95.39
80.39
81.13
70.66
98.97

FReLU
81.65
97.07
94.78
79.80
81.21
69.80
98.91

AB-ReLU
85.08
97.69
95.71
81.22
81.95
71.54
98.97

contains 38,981 face images from 335 subjects captured using
a near infrared sensor. In this experiment, we have considered
1005 face images as the query image. The retrieval results over
the PolyUNIR face dataset in terms of the ARP are reported
in Fig. 5. The ReLU and three variants of AB-ReLU such
as 35AR, 35AR2 and 35AR5 are used to extract the features
from layer 35. The improved performance of AB-ReLU over
the PolyUNIR face dataset conﬁrms its suitability over large
scale dataset.

E. Comparison with Other Activation Functions

We have conducted this experiment to compare the results
with other activation functions such as Sigmoid, ReLU [33],
Leaky ReLU (LReLU) [29] and non-trainable Flexible ReLU
(FReLU) [37]. The Sigmoid is a traditional activation function
which squashes the input and produces the output between 0
and 1. However, it suffers with the saturation problem. The
LReLU and FReLU are the variants of ReLU. The leaky
factor for LReLU is considered as 0.01. The ﬂexibility factor
in FReLU is considered as 0.2. All the features using these
functions are considered from the layer 35 of VGGNet. Note
that the Sigmoid, ReLU, FReLU and AB-ReLU consist of the
positive values only, whereas the LReLU has negative values
also. Due to negative values most distance measures fail for
LReLU. In order to avoid this problem, we have considered

10

[30] Martinez, A.M.: The ar face database. CVC technical report (1998)
[31] Mart´ınez, A.M., Kak, A.C.: Pca versus lda. IEEE transactions on pattern

analysis and machine intelligence 23(2), 228–233 (2001)

[32] Murala, S., Maheshwari, R., Balasubramanian, R.: Local tetra patterns:
IEEE

a new feature descriptor for content-based image retrieval.
Transactions on Image Processing 21(5), 2874–2886 (2012)

[33] Nair, V., Hinton, G.E.: Rectiﬁed linear units improve restricted boltz-
mann machines. In: Proceedings of the 27th international conference on
machine learning (ICML-10), pp. 807–814 (2010)

[34] Parkhi, O.M., Vedaldi, A., Zisserman, A., et al.: Deep face recognition.

In: BMVC, vol. 1, p. 6 (2015)

[35] Phillips, P.J., Moon, H., Rizvi, S.A., Rauss, P.J.: The feret evaluation
IEEE Transactions on

methodology for face-recognition algorithms.
pattern analysis and machine intelligence 22(10), 1090–1104 (2000)

[36] Phillips, P.J., Wechsler, H., Huang, J., Rauss, P.J.: The feret database
Image and

and evaluation procedure for face-recognition algorithms.
vision computing 16(5), 295–306 (1998)

[37] Qiu, S., Xu, X., Cai, B.: Frelu: Flexible rectiﬁed linear units for
improving convolutional neural networks.
In: 2018 24th International
Conference on Pattern Recognition (ICPR), pp. 1223–1228. IEEE (2018)
[38] Schroff, F., Kalenichenko, D., Philbin, J.: Facenet: A uniﬁed embedding
for face recognition and clustering.
In: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 815–823
(2015)

[39] Shamsolmoali, P., Jain, D.K., Zareapoor, M., Yang, J., Alam, M.A.:
High-dimensional multimedia classiﬁcation using deep cnn and extended
residual units. Multimedia Tools and Applications 78(17), 23867–23882
(2019)

[40] Sharma, S., Dubey, S.R., Singh, S.K., Saxena, R., Singh, R.K.: Identity
veriﬁcation using shape and geometry of human hands. Expert Systems
with Applications 42(2), 821–832 (2015)

[41] Simonyan, K., Zisserman, A.: Very deep convolutional networks for

large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014)

[42] Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D.,
Erhan, D., Vanhoucke, V., Rabinovich, A.: Going deeper with convolu-
tions. In: Proceedings of the IEEE conference on computer vision and
pattern recognition, pp. 1–9 (2015)

[43] Taigman, Y., Yang, M., Ranzato, M., Wolf, L.: Deepface: Closing the
gap to human-level performance in face veriﬁcation.
In: Proceedings
of the IEEE conference on computer vision and pattern recognition, pp.
1701–1708 (2014)

[44] Tzeng, E., Hoffman, J., Darrell, T., Saenko, K.: Simultaneous deep
In: Proceedings of the IEEE

transfer across domains and tasks.
International Conference on Computer Vision, pp. 4068–4076 (2015)

[45] Vedaldi, A., Lenc, K.: Matconvnet: Convolutional neural networks for
matlab. In: Proceedings of the 23rd ACM international conference on
Multimedia, pp. 689–692. ACM (2015)

[46] Viola, P., Jones, M.: Rapid object detection using a boosted cascade
of simple features.
In: Computer Vision and Pattern Recognition,
2001. CVPR 2001. Proceedings of the 2001 IEEE Computer Society
Conference on, vol. 1, pp. I–I. IEEE (2001)

[47] Wan, J., Wang, D., Hoi, S.C.H., Wu, P., Zhu, J., Zhang, Y., Li, J.: Deep
learning for content-based image retrieval: A comprehensive study. In:
Proceedings of the 22nd ACM international conference on Multimedia,
pp. 157–166. ACM (2014)

[48] Wang, Y., Wang, G., Chen, C., Pan, Z.: Multi-scale dilated convolution
of convolutional neural network for image denoising. Multimedia Tools
and Applications pp. 1–16 (2019)

[49] Wen, Y., Zhang, K., Li, Z., Qiao, Y.: A discriminative feature learning
In: European Conference on

approach for deep face recognition.
Computer Vision, pp. 499–515. Springer (2016)

[50] Xu, B., Wang, N., Chen, T., Li, M.: Empirical evaluation of rectiﬁed
activations in convolutional network. arXiv preprint arXiv:1505.00853
(2015)

[51] Zhang, B., Zhang, L., Zhang, D., Shen, L.: Directional binary code with
application to polyu near-infrared face database. Pattern Recognition
Letters 31(14), 2337–2344 (2010)

[52] Zhou, H., Li, Z.: Deep networks with non-static activation function.

Multimedia Tools and Applications 78(1), 197–211 (2019)

[4] Chakraborty, S., Singh, S., Chakraborty, P.: Local gradient hexa pattern:
A descriptor for face recognition and retrieval. IEEE Transactions on
Circuits and Systems for Video Technology (2016)

[5] Chakraborty, S., Singh, S.K., Chakraborty, P.: Centre symmetric quadru-
ple pattern: A novel descriptor for facial image recognition and retrieval.
Pattern Recognition Letters (2017)

[6] Chakraborty, S., Singh, S.K., Chakraborty, P.: Local directional gradient
pattern: a local descriptor for face recognition. Multimedia Tools and
Applications 76(1), 1201–1216 (2017)

[7] Clevert, D.A., Mayr, A., Unterthiner, T., Hochreiter, S.: Rectiﬁed factor
networks. In: Advances in neural information processing systems, pp.
1855–1863 (2015)

[8] Clevert, D.A., Unterthiner, T., Hochreiter, S.: Fast and accurate deep
arXiv preprint

network learning by exponential linear units (elus).
arXiv:1511.07289 (2015)

[9] Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet:
A large-scale hierarchical image database.
In: Computer Vision and
Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pp. 248–
255. IEEE (2009)

[10] Dubey, S.R.: Face retrieval using frequency decoded local descriptor.
Multimedia Tools and Applications 78(12), 16411–16431 (2019)
[11] Dubey, S.R.: Local directional relation pattern for unconstrained and
robust face retrieval. Multimedia Tools and Applications (2019)
[12] Dubey, S.R., Mukherjee, S.: Ldop: Local directional order pattern for

robust face retrieval. arXiv preprint arXiv:1803.07441 (2018)

[13] Dubey, S.R., Singh, S.K., Singh, R.K.: Rotation and illumination invari-
ant interleaved intensity order-based local descriptor. IEEE Transactions
on Image Processing 23(12), 5323–5333 (2014)

[14] Dubey, S.R., Singh, S.K., Singh, R.K.: Local diagonal extrema pattern:
a new and efﬁcient feature descriptor for ct image retrieval. IEEE Signal
Processing Letters 22(9), 1215–1219 (2015)

[15] Dubey, S.R., Singh, S.K., Singh, R.K.: Local wavelet pattern: A new
feature descriptor for image retrieval in medical ct databases.
IEEE
Transactions on Image Processing 24(12), 5892–5903 (2015)

[16] Dubey, S.R., Singh, S.K., Singh, R.K.: Local bit-plane decoded pattern:
a novel feature descriptor for biomedical image retrieval. IEEE Journal
of Biomedical and Health Informatics 20(4), 1139–1147 (2016)
[17] Dubey, S.R., Singh, S.K., Singh, R.K.: Multichannel decoded local
IEEE Transactions

binary patterns for content-based image retrieval.
on Image Processing 25(9), 4018–4032 (2016)

[18] Ge, Y., Jiang, S., Xu, Q., Jiang, C., Ye, F.: Exploiting representations
from pre-trained convolutional neural networks for high-resolution re-
mote sensing image retrieval. Multimedia Tools and Applications pp.
1–27 (2018)

[19] Georghiades, A.S., Belhumeur, P.N., Kriegman, D.J.: From few to many:
Illumination cone models for face recognition under variable lighting and
pose.
IEEE transactions on pattern analysis and machine intelligence
23(6), 643–660 (2001)

[20] He, K., Zhang, X., Ren, S., Sun, J.: Delving deep into rectiﬁers:
Surpassing human-level performance on imagenet classiﬁcation.
In:
Proceedings of the IEEE international conference on computer vision,
pp. 1026–1034 (2015)

[21] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image
recognition. In: Proceedings of the IEEE conference on computer vision
and pattern recognition, pp. 770–778 (2016)

[22] Huang, G.B., Ramesh, M., Berg, T., Learned-Miller, E.: Labeled faces
in the wild: A database for studying face recognition in unconstrained
environments. Tech. rep., Technical Report 07-49, University of Mas-
sachusetts, Amherst (2007)

[23] Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classiﬁcation with
deep convolutional neural networks. In: Advances in neural information
processing systems, pp. 1097–1105 (2012)

[24] Kumar, N., Berg, A.C., Belhumeur, P.N., Nayar, S.K.: Attribute and
simile classiﬁers for face veriﬁcation. In: Computer Vision, 2009 IEEE
12th International Conference on, pp. 365–372. IEEE (2009)

[25] Lee, K.C., Ho, J., Kriegman, D.J.: Acquiring linear subspaces for
face recognition under variable lighting. IEEE Transactions on pattern
analysis and machine intelligence 27(5), 684–698 (2005)

[26] Li, Y., Wan, L., Fu, T., Hu, W.: Piecewise supervised deep hashing for

image retrieval. Multimedia Tools and Applications pp. 1–21 (2019)

[27] Liu, P., Guo, J.M., Wu, C.Y., Cai, D.: Fusion of deep learning and
compressed domain features for content-based image retrieval.
IEEE
Transactions on Image Processing 26(12), 5706–5717 (2017)

[28] Ma, X., Jiang, X.: Multimedia image quality assessment based on deep
feature extraction. Multimedia Tools and Applications pp. 1–12 (2019)
[29] Maas, A.L., Hannun, A.Y., Ng, A.Y.: Rectiﬁer nonlinearities improve
neural network acoustic models. In: Proc. ICML, vol. 30 (2013)

11

