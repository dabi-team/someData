PlenOctrees for Real-time Rendering of Neural Radiance Fields

Alex Yu1
Hao Li1,3

Ruilong Li1,2
Ren Ng1

Matthew Tancik1
Angjoo Kanazawa1

1UC Berkeley

2USC Institute for Creative Technologies

3Pinscreen

1
2
0
2

g
u
A
7
1

]

V
C
.
s
c
[

2
v
4
2
0
4
1
.
3
0
1
2
:
v
i
X
r
a

Abstract

We introduce a method to render Neural Radiance Fields
(NeRFs) in real time using PlenOctrees, an octree-based
3D representation which supports view-dependent effects.
Our method can render 800×800 images at more than 150
FPS, which is over 3000 times faster than conventional
NeRFs. We do so without sacriﬁcing quality while preserv-
ing the ability of NeRFs to perform free-viewpoint rendering
of scenes with arbitrary geometry and view-dependent ef-
fects. Real-time performance is achieved by pre-tabulating
the NeRF into a PlenOctree.
In order to preserve view-
dependent effects such as specularities, we factorize the ap-
pearance via closed-form spherical basis functions. Specif-
ically, we show that it is possible to train NeRFs to pre-
dict a spherical harmonic representation of radiance, re-
moving the viewing direction as an input to the neural net-
work. Furthermore, we show that PlenOctrees can be di-
rectly optimized to further minimize the reconstruction loss,
which leads to equal or better quality compared to com-
peting methods. Moreover, this octree optimization step
can be used to reduce the training time, as we no longer
need to wait for the NeRF training to converge fully. Our
real-time neural rendering approach may potentially en-
able new applications such as 6-DOF industrial and prod-
uct visualizations, as well as next generation AR/VR sys-
tems. PlenOctrees are amenable to in-browser rendering as
well; please visit the project page for the interactive online
demo, as well as video and code: https://alexyu.
net/plenoctrees.

1. Introduction

Despite the progress of real-time graphics, interactive
3D content with truly photorealistic scenes and objects are
still time consuming and costly to produce due to the ne-
cessity of optimized 3D assets and dedicated shaders. In-
stead, many graphics applications opt for image-based solu-
tions. E-commerce websites often use a ﬁxed set of views to
showcase their products; VR experiences often rely on 360

Figure 1: Real-time NeRF with PlenOctrees. Given a set of
posed images of a scene, our method creates a 3D volumetric
model that can be rendered in real-time. We propose PlenOctrees,
which are octrees that can capture view-dependent dependent ef-
fects such as specularities. Rendering using our approach is orders
of magnitude faster than NeRF.

video recordings to avoid the costly production of real 3D
scenes, and mapping services such as Google Street View
stitch images into panoramic views limited to 3-DOF.

Recent advances in neural rendering, such as neural vol-
umes [24] and neural radiance ﬁelds (NeRFs) [30], open a
promising new avenue to model arbitrary objects and scenes
in 3D from a set of calibrated images. NeRFs in partic-
ular can faithfully render detailed scenes and appearances
with non-Lambertian effects from any view, while simulta-
neously offering a high degree of compression in terms of
storage. Partly due to these exciting properties, of late, there
has been an explosion of research based on NeRF.

Nevertheless, for practical applications, runtime perfor-
mance remains a critical limitation of NeRFs: due to the
extreme sampling requirements and costly neural network
queries, rendering a NeRF is agonizingly slow. For illus-
tration, it takes roughly 30 seconds to render an 800x800

Input ImagesNeRF-SHOffline Model GenerationReal-time RenderingPlenOctrees Demo150 FPSPlenOctree 
 
 
 
 
 
image from a NeRF using a high performance GPU, mak-
ing it impractical for real-time interactive applications.

In this work, we propose a method for rendering a NeRF
in real time, achieved by distilling the NeRF into a hier-
archical 3D volumetric representation. Our approach pre-
serves NeRF’s ability to synthesize arbitrarily complex ge-
ometry and view-dependent effects from any viewpoint and
requires no additional supervision.
In fact, our method
achieves and in many cases surpasses the quality of the orig-
inal NeRF formulation, while providing signiﬁcant acceler-
ation. Our model allows us to render an 800x800 image at
167.68 FPS on a NVIDIA V100 GPU and does not rely on
a deep neural network during test time. Moreover, our rep-
resentation is amenable to modern web technologies, allow-
ing interactive rendering in a browser on consumer laptops.
Naive NeRF rendering is slow because it requires dense
sampling of the scene, where every sample requires a neural
network inference. Because these queries depend on the
viewing direction as well as the spatial position, one cannot
naively cache these color values for all viewing directions.
We overcome these challenges and enable real-time
rendering by pre-sampling the NeRF into a tabulated
view-dependent volume which we refer to as a PlenOc-
tree, named after the plenoptic functions of Adelsen and
Bergen [1]. Speciﬁcally, we use a sparse voxel-based oc-
tree where every leaf of the tree stores the appearance and
density values required to model the radiance at a point in
the volume.
In order to account for non-Lambertian ma-
terials that exhibit view-dependent effects, we propose to
represent the RGB values at a location with spherical har-
monics (SH), a standard basis for functions deﬁned on the
surface of the sphere. The spherical harmonics can be eval-
uated at arbitrary query viewing directions to recover the
view dependent color.

Although one could convert an existing NeRF into such
a representations via projection onto the SH basis functions,
we show that we can in fact modify a NeRF network to pre-
dict appearances explicitly in terms of spherical harmonics.
Speciﬁcally, we train a network that produces coefﬁcients
for the SH functions instead of raw RGB values, so that
the predicted values can later be directly stored within the
leaves of the PlenOctree. We also introduce a sparsity prior
during NeRF training to improve the memory efﬁciency of
our octrees, consequently allowing us to render higher qual-
ity images. Furthermore, once the structure is created, the
values stored in PlenOctree can be optimized because the
rendering procedure remains differentiable. This enables
the PlenOctree to obtain similar or better image quality
compared to NeRF. Our pipeline is illustrated in Fig. 2.

Additionally, we demonstrate how our proposed pipeline
can be used to accelerate NeRF model training, making our
solution more practical to train than the original NeRF ap-
proach. Speciﬁcally, we can stop training the NeRF model

early to convert it into a PlenOctree, which can then be
trained signiﬁcantly faster as it no longer involves any neu-
ral networks.

Our experiments demonstrate that our approach can ac-
celerate NeRF-based rendering by 5 orders of magnitude
without loss in image quality. We compare our approach
on standard benchmarks with scenes and objects captured
from 360◦ views, and demonstrate state-of-the-art level per-
formance for image quality and rendering speed.

Our interactive viewer can enable operations such as ob-
ject insertion, visualizing radiance distributions, decompos-
ing the SH components, and slicing the scene. We hope that
these real-time operations can be useful to the community
for visualizing and debugging NeRF-based representations.
To summarize, we make the following contributions:

• The ﬁrst method that achieves real-time rendering of

NeRFs with similar or improved quality.

• NeRF-SH: a modiﬁed NeRF that is trained to output

appearance in terms of spherical basis functions.

• PlenOctree, a data structure derived from NeRFs
which enables highly efﬁcient view-dependent render-
ing of complex scenes.

• Accelerated NeRF training method using an early
training termination, followed by a direct ﬁne-tuning
process on PlenOctree values.

2. Related Work

Novel View Synthesis. The task of synthesizing novel
views of a scene given a set of photographs is a well-
studied problem with various approaches. All methods pre-
dict an underlying geometric or image-based 3D represen-
tation that allows rendering from novel viewpoints. Mesh
based methods represent the scene with surfaces, and have
been used to model Lambertian (diffuse) [56] and non-
Lambertian scenes [60, 5, 3].

Mesh based representations are compact and easy to ren-
der; however, optimizing a mesh to ﬁt a complex scene of
arbitrary topology is challenging.
Image-based rendering
methods [19, 43, 60], on the other hand, enable easy cap-
ture as well as photo-realistic and fast rendering, however
are often bounded in the viewing angle and do not allow
easy editing of the underlying scene.

Volume rendering is a classical technique with a long
history of research in the graphics community [7]. Volume-
based representations such as voxel grids [42, 18, 24, 14,
55, 44] and multi-plane images (MPIs) [49, 35, 64, 48, 29]
are a popular alternative to mesh representations due to their
topology-free nature: gradient-based optimization is there-
fore straightforward, while rendering can still be real-time.
However, such naive volumetric representations are often

Figure 2: Method Overview. We propose a method to quickly render NeRFs by training a modiﬁed NeRF model (NeRF-SH) and
converting it into a PlenOctree, an octree that captures view-dependent effects. a) The NeRF-SH model uses the same optimization
procedure and volume rendering method presented in NeRF [30]. However, instead of predicting the RGB color c directly, the network
predicts spherical harmonic coefﬁcients k. The color c is calculated by summing the weighted spherical harmonic bases evaluated at the
corresponding ray direction (θ, φ). The spherical harmonics enable the representation to model view-dependent appearance. The values in
the orange boxes are used for volume rendering. b) To build a PlenOctree, we densely sample the NeRF-SH model in the volume around
the target object and tabulate the density and SH coefﬁcients. We can further optimize the PlenOctree directly with the training images to
improve its quality.

memory bound, limiting the maximum resolution that can
be captured. Volumetric octrees are a popular approach for
reducing memory and compute in such cases. We refer the
reader to this survey [17] for a historical perspective on oc-
tree volume rendering. Octrees have been used in recent
work to decrease the memory requirements during train-
ing for other 3D tasks [39, 11, 52, 57]. Concurrent with
this work, NeX [59] extends MPIs to encode spherical basis
functions that enable view-dependent rendering effects in
real-time. Also concurrently, Lombardi et al. [25] propose
to model data using geometric primitives, and [13, 9, 38]
also distill NeRFs to enable real-time rendering.

Coordinate-Based Neural Networks.
Recently,
coordinate-based neural networks have emerged as a
popular alternative to explicit volumetric representations,
as they are not limited to a ﬁxed voxel representation.
These methods train a multilayer perceptron (MLP) whose
input is a coordinate and output is some property of space
corresponding to that
These networks have
location.
been used to predict occupancy [28, 4, 34, 40, 31, 20],
signed distance ﬁelds [32, 10, 61, 62], and radiance [30].
Coordinate-based neural networks have been used for
view synthesis in Scene Representation Networks [45],
NeRFs [30], and many NeRF extensions [27, 33, 41, 47].
These networks represent a continuous function that can be
sampled at arbitrarily ﬁne resolutions without increasing
the memory footprint. Unfortunately, this compactness is
achieved at the expense of computational efﬁciency as each
sample must be processed by a neural network. As a result,
these representations are often slow and impractical for
real-time rendering.

NeRF Accelerations. While NeRFs are able to produce
high quality results, their computationally expensive render-
ing leads to slow training and inference. One way to speed

up the process of ﬁtting a NeRF to a new scene is to incorpo-
rate priors learned from a dataset of similar scenes. This can
be accomplished by conditioning on predicted images fea-
tures [53, 63, 58] or meta-learning [51]. To improve infer-
ence speed, Neural Sparse Voxel Fields (NSVF) [23] learns
a sparse voxel grid of features that are input into a NeRF
like model. The sparse voxel grid allows the renderer to skip
over empty regions when tracing a ray which improves the
render time ∼10x. Decomposed Radiance Fields [37] spa-
tially decomposes a scene into multiple smaller networks.
AutoInt [22] modiﬁes the architecture of the NeRF so that
inference requires fewer samples but produces lower qual-
ity results. None of these approaches achieve real-time. The
concurrent work DoNeRF adds a depth classiﬁer to NeRF
in order to drastically improve the efﬁciency of sampling,
but requires ground-truth depth for training. Although not
based on NeRF, recently Takikawa et al. [50] propose a
method to accelerate neural SDF rendering with an octree.
Note that this work does not model appearance properties.
In contrast, we employ a volumetric representation that can
capture photorealistic view-dependent appearances while
achieving even higher framerates.

3. Preliminaries

3.1. Neural Radiance Fields

Neural radiance ﬁelds (NeRF) [30] are 3D representa-
tions that can be rendered from arbitrary novel viewpoints
while capturing continuous geometry and view-dependent
appearance. The radiance ﬁeld is encoded into the weights
of a multilayer perceptron (MLP) that can be queried at
a position x = (x, y, z) from a viewing direction d =
(θ, φ) to recover the corresponding density σ and color
c = (r, g, b). A pixel’s predicted color C(r) is computed

Spherical Harmonics(a) NeRF-SH Training(b) Conversion to PlenOctreeDense SamplesNeRF-SHColorDensityPlenOctreeFine-tuningby casting a ray, r, into the volume and accumulating the
color based on density along the ray. NeRF estimates the
accumulated color by taking N point samples along the ray
to perform volume rendering:

ˆC(r) =

N −1
(cid:88)

i=0

(cid:0)1 − exp(−σiδi)(cid:1) ci ,

Ti



where Ti = exp

−



σjδj



i−1
(cid:88)

j=0

(1)

(2)

Where δi are the distances between point samples. To train
the NeRF network, the predicted colors ˆC for a batch of
rays R corresponding to pixels in the training images are
optimized using Adam [15] to match the target pixel colors:

ﬁne-tune the octree on the training images to further im-
prove image quality, Please see Fig. 2 for a graphical illus-
tration of our pipeline.

The conversion process leverages the continuous nature
of NeRF to dynamically obtain the spatial structure of the
octree. We show that even with a partially trained NeRF,
our PlenOctree is capable of producing results competitive
with the fully trained NeRF.

4.1. NeRF-SH: NeRF with Spherical Harmonics

SHs have been a popular low-dimensional representation
for spherical functions and have been used to model Lam-
bertian surfaces [36, 2] or even glossy surfaces [46]. Here
we explore its use in a volumetric context. Speciﬁcally, we
adapt the NeRF network f to output spherical harmonics
coefﬁcients k , rather than RGB values.

LRGB =

(cid:88)

r∈R

(cid:13)C(r) − ˆC(r)(cid:13)
(cid:13)
2
(cid:13)
2

(3)

f (x) = (k, σ) where k = (km

(cid:96) )m: −(cid:96)≤m≤(cid:96)
(cid:96): 0≤(cid:96)≤(cid:96)max

(4)

To better represent high frequency details in the scene
the inputs are positionally encoded and two stages of sam-
pling are performed, one coarse and one ﬁne. We refer the
interested reader to the NeRF paper [30] paper for details.

Limitations. One notable consequence of this architec-
ture is that each sample along the ray must be fed to the
MLP to obtain the corresponding σi and ci. A total of 192
samples were taken for each ray in the examples presented
in NeRF. This is inefﬁcient as most samples are sampling
free space which do not contribute to the integrated color.
To render a single target image at 800 × 800 resolution, the
network must be run on over 100 million inputs. There-
fore it takes about 30 seconds to render a single frame using
a NVIDIA V100 GPU, making it impractical for real-time
applications. Our use of a sparse voxel octree avoids excess
compute in regions without content. Additionally we pre-
compute the values for each voxel so that network queries
are not performed during inference.

4. Method

We propose a pipeline that enables real-time rendering
of NeRFs. Given a trained NeRF, we can convert it into a
PlenOctree, an efﬁcient data structure that is able to repre-
sent non-Lambertian effects in a scene. Speciﬁcally, it is an
octree which stores spherical harmonics (SH) coefﬁcients at
the leaves, encoding view-dependent radiance.

To make the conversion to PlenOctree more straightfor-
ward, we also propose NeRF-SH, a variant of the NeRF net-
work which directly outputs the SH coefﬁcients, thus elim-
inating the need for a view-direction input to the network.
With this change, the conversion can then be performed by
evaluating on a uniform grid followed by thresholding. We

(cid:96) ∈ R3 is a set of 3 coefﬁcients corresponding to
Each km
the RGB components.
In this setup, the view-dependent
color c at a point x may be determined by querying the SH
functions Y m

: S2 (cid:55)→ R at the desired viewing angle d:

(cid:96)

c(d; k) = S

(cid:32)(cid:96)max(cid:88)

(cid:96)
(cid:88)

(cid:96)=0

m=−(cid:96)

(cid:33)

(cid:96) Y m
km

(cid:96) (d)

(5)

Where S : x (cid:55)→ (1 + exp(−x))−1 is the sigmoid function
for normalizing the colors.
In other words, we factorize
the view-dependent appearance with the SH basis, elimi-
nating the view-direction input to the network and remov-
ing the need to sample view directions at conversion time.
Please see the appendix for more technical discussion of
SHs. With a single evaluation of the network, we can now
efﬁciently query colors from arbitrary viewing angles at in-
ference time. In Fig. 7, one can see that NeRF-SH training
speed is similar to, but slightly faster than, NeRF (by about
10%).

Note that we can also project a trained NeRF to SHs
directly at each point by sampling NeRF at random direc-
tions and multiplying by the SH component values to form
Monte Carlo estimates of the inner products. However, this
sampling process takes several hours to achieve reasonable
quality and imposes a quality loss of about 2 dB.1 Neverthe-
less, this alternative approach offers a pathway to convert
existing NeRFs into PlenOctrees.

Other than SHs, we also experiment with Spherical
Gaussians (SG) [8], a learnable spherical basis which have
been used to represent all-frequency lighting [54, 46, 21].
We ﬁnd that SHs perform better in our use case and provide
an ablation in the appendix.

1With 10000 view-direction samples per point, taking about 2 hours,

the PSNR is 29.21 vs. 31.02 for our main method prior to optimization.

Synthetic NeRF Dataset

best

second-best

Tanks and Temples Dataset

best

second-best

PSNR ↑

SSIM ↑ LPIPS ↓

FPS ↑

PSNR ↑

SSIM ↑ LPIPS ↓

FPS ↑

NeRF (original)
NeRF
SRN
Neural Volumes
NSVF
AutoInt (8 sections)

NeRF-SH
PlenOctree from NeRF-SH
PlenOctree after ﬁne-tuning

31.01
31.69
22.26
26.05
31.75
25.55

31.57
31.02
31.71

0.947
0.953
0.846
0.893
0.953
0.911

0.952
0.951
0.958

0.081
0.068
0.170
0.160
0.047
0.170

0.063
0.066
0.053

0.023
0.045
0.909
3.330
0.815
0.380

0.051
167.68
167.68

Table 1: Quantitative results on the NeRF-synthetic test scenes.
Our approach is signiﬁcantly faster than all existing methods dur-
ing inference while performing on par with NSVF, the current
state-of-the-art method for image quality. We note that NeRF-SH,
the modiﬁed NeRF model that is trained to output SH, performs
similarly to the baseline NeRF model. The octree conversion of
NeRF-SH to PlenOctree w/o ﬁne-tuning negatively impacts the
image quality metrics. This is remedied with the additional ﬁne-
tuning step.

Figure 3: Sparsity loss and conversion robustness. When trained
without the sparsity loss, NeRF can often converge to a solution
where unobserved portions or the background are solid. This de-
grades the spatial resolution of our octree-based representation.

Sparsity prior. Without any regularization, the model is
free to generate arbitrary geometry in unobserved regions.
While this does not directly worsen image quality, it would
adversely impact our conversion process as the extra geom-
etry occupies signiﬁcant voxel space.

To solve this problem, we introduce an additional spar-
sity prior during NeRF training. Intuitively, this prior en-
courages NeRF to choose empty space when both space and
solid colors are possible solutions. Formally,

Lsparsity =

1
K

K
(cid:88)

k=1

|1 − exp(−λσk)|

(6)

Here, {σk}K
k=1 are the evaluated density values at K
uniformly random points within the bounding box, and
λ is a hyperparameter. The ﬁnal training loss is then
βsparsityLsparsity + LRGB, where βsparsity is a hyperparameter.
Fig. 3 illustrates the effect of the prior.

NeRF (original)
NeRF
SRN
Neural Volumes
NSVF

NeRF-SH
PlenOctree from NeRF-SH
PlenOctree after ﬁne-tuning

25.78
27.94
24.10
23.70
28.40

27.82
27.34
27.99

0.864
0.904
0.847
0.834
0.900

0.902
0.897
0.917

0.198
0.168
0.251
0.260
0.153

0.167
0.170
0.131

0.007
0.013
0.250
1.000
0.163

0.015
42.22
42.22

Table 2: Quantitative results on the Tanks and Temples test
scenes. We ﬁnd that our ﬁne-tuned PlenOctree model is signif-
icantly faster than existing methods while performing on par in
terms of image metrics. Note that the images here are 1920×1080
compared to 800×800 in the synthetic dataset.

4.2. PlenOctree: Octree-based Radiance Fields

Once we have trained a NeRF-SH model, we can convert
it into a sparse octree representation for real time rendering.
A PlenOctree stores density and SH coefﬁcients modelling
view-dependent appearance at each leaf. We describe the
conversion and rendering processes below.
Rendering. To render the PlenOctree, for each ray, we
ﬁrst determine ray-voxel intersections in the octree struc-
ture. This produces a sequence of segments between voxel
boundaries with constant density and color, of lengths
{δi}N
i=1. NeRF’s volume rendering model (1) is then ap-
plied to assign a color to the ray. This approach allows
for skipping large voxels in one step while also not miss-
ing small voxels.

At test-time, we further accelerate this rendering process
by applying early-stopping when the ray has accumulated
transmittance Ti less than γ = 0.01.
Conversion from NeRF-SH. The conversion process can
be divided into three steps. At a high level, we evaluate
the network on a grid, retaining only density values, then
ﬁlter the voxels via thresholding. Finally we sample ran-
dom points within each remaining voxel and average them
to obtain SH coefﬁcients to store in the octree leaves. More
details are given below:

Evaluation. We ﬁrst evaluate the NeRF-SH model to ob-
tain σ values on a uniformly spaced 3D grid. The grid is
automatically scaled to tightly ﬁt the scene content.2

Filtering. Next, we ﬁlter this grid to obtain a sparse set
of voxels centered at the grid points sufﬁcient for represent-
ing the scene. Speciﬁcally, we render alpha maps for all the
training views using this voxel grid, keeping track of the
maximum ray weight 1 − exp(−σiδi) at each voxel. We
then eliminate the voxels whose weights are lower than a
threshold τw. The octree is constructed to contain the re-

2By pre-evaluating σ on a larger grid and ﬁnding the bounding box of

all points with σ ≥ τa.

Without sparsity lossWith sparsity lossFigure 4: NeRF-synthetic qualitative results. Randomly sam-
pled qualitative comparisons on a reimplementation of NeRF and
our proposed method. We are unable to ﬁnd any signiﬁcant im-
age quality difference between the two methods. Despite this, our
method can render these examples images more than 3500x faster.

maining voxels as leaves at the deepest level while being
empty elsewhere. Compared to naively thresholding by σ
at each point, this method eliminates non-visible voxels.

Sampling. Finally, we sample a set of 256 random points
in each remaining voxel and set the associated leaf of the
octree to the mean of these values to reduce aliasing. Each
leaf now contains the density σ and a vector of spherical
harmonics coefﬁcients for each of the RGB color channels.
This full extraction process takes about 15 minutes.3

4.3. PlenOctree Optimization

Since this volume rendering process is fully differen-
tiable with respect to the tree values, we can directly ﬁne-
tune the resulting octree on the original training images us-
ing the NeRF loss (3) with SGD in order to improve the
image quality. Note that the tree structure is ﬁxed to that
obtained from NeRF in this process. PlenOctree optimiza-
tion operates at about 3 million rays per second, compared
to about 9000 for NeRF training, allowing us to optimize for
many epochs in a relatively short time. The analytic deriva-
tives for this process are implemented in custom CUDA ker-
nels. We defer technical details to the appendix.

The fast octree optimization indirectly allows us to ac-
celerate NeRF training, as seen in Fig. 7, since we can stop
the NeRF-SH training at an earlier time for constructing the
PlenOctree, with only a slight degradation in quality.

5. Results

5.1. Experimental Setup

For our experiments, we use the NeRF-
Datasets.
synthetic [30] dataset and a subset of the Tanks and Tem-

3Note that sampling 8 points instead of 256 allows for extraction in

about 1.5 minutes, with minimal loss in quality.

Figure 5: Quality vs. speed comparison of various methods.
A comparison of methods on the NeRF-synthetic dataset where
higher PSNR and lower FPS (top right) is most desirable. We
include four variants of the PlenOctree model that tune parts of
the conversion process to trade off accuracy for speed. Please see
Table 3 (adjacent) for descriptions of these variants. Note that the
time axis is logarithmic.

Model Description

GB ↓

PSNR ↑

FPS ↑

Ours-1.9G
Ours-1.4G
Ours-0.4G
Ours-0.3G

Complete Model as in Table 1
Higher Threshold τw = 0.01
w/o Auto Bbox Scaling
Grid Size 256

1.93
1.36
0.44
0.30

31.71
31.64
30.70
29.60

168
215
329
410

Table 3: PlenOctree conversion ablations on NeRF-synthetic.
Average metrics across the NeRF-synthetic scenes for several dif-
ferent methods to construct PlenOctrees are shown. Ours-1.9G:
This is the high-quality model we reported in Table 1. Ours-1.4G:
This is a variant with a higher weight threshold, therefore aggres-
sively sparsifying the tree. Ours-0.4G: Here, we remove the auto
bounding box scaling step and instead use ﬁxed large bounding
boxes, limiting resolution. Ours-0.3G: A version using a 2563
grid instead of 5123.

ples dataset [16]. The NeRF-synthetic dataset consists of
8 scenes where each scene has a central object with 100
inward facing cameras distributed randomly on the upper
hemisphere. The images are 800 × 800 with provided
ground truth camera poses. The Tanks and Temples sub-
set is from NSVF [23] and contains 5 scenes of real ob-
jects captured by an inward facing camera circling the
scene. We use foreground masks provided by the NSVF
authors. Each scene contains between 152-384 images of
size 1920 × 1080.

Baselines. The principal baseline for our experiments is
NeRF [30]; we report results for both the original NeRF
implementation, denoted NeRF (original) as well as a reim-
plementation in Jax [6], denoted simply NeRF, which our
NeRF-SH code is based off of. Unless otherwise stated,
all NeRF results and timings are from the latter implemen-

Ground TruthNeRFOursNeRFNSVFOurs-1.9GOurs-1.4GOurs-0.4GOurs-0.3GNeural VolumesSRNAutoInt-8AutoInt-16AutoInt-240.1110100Figure 6: Qualitative comparisons on Tanks and Temples. We compare NeRF and our proposed method. On this dataset, we ﬁnd that
our method better recovers the ﬁne details in the scene. The results are otherwise similar. Additionally, the render time of our method is
over 3000× faster.

tation. We compare also to two recent papers introducing
NeRF accelerations, neural sparse voxel ﬁelds (NSVF) [23]
and AutoInt [22], as well as two older methods, scene rep-
resentation networks (SRN) [45] and Neural Volumes [24].

5.2. Quality Evaluation

We evaluate our approach against prior works on the syn-
thetic and real datasets mentioned above. The results are
in Tables 1 and Table 2 respectively. Note that none of
the baselines achieve real-time performance; nevertheless,
our quality results are competitive in all cases and better in
terms of some metrics.

In Figures 4 and 6, we show qualitative examples that
demonstrate that our PlenOctree conversion does not per-
ceptually worsen the rendered images compared to NeRF;
rather, we observe that the PlenOctree optimization process
enhances ﬁne details such as text. Additionally, we note
that our modiﬁcations of NeRF to predict spherical func-
tion coefﬁcients (NeRF-SH) does not signiﬁcantly change
the performance.

For the SH, we set (cid:96)max = 3 (16 components) and 4 (25
components) on the synthetic and Tanks & Temples datasets
respectively. We use 5123 grid size in either case. Please re-
fer to the appendix for training details. The inference time
performance is measured on a Tesla V100 for all methods.
Across both datasets we ﬁnd that PlenOctree inference is
over 3000 times faster than NeRF and at least 30 times faster
than all other compared methods. PlenOctree performs ei-
ther best, or second best for all image quality metrics.

5.3. Speed Trade-off Analysis

A number of parameters for PlenOctree conversion and
rendering can be tuned to trade-off between speed and im-
age quality. In Figure 5 and Table 3 we compared image
accuracy and inference time for four variants of PlenOctree

that sweep this trade-off.

5.4. Indirect Acceleration of NeRF Training

Since we can efﬁciently ﬁne-tune the octree on the origi-
nal training data, as brieﬂy discussed in §4.3, we can choose
to stop the NeRF-SH training at an earlier time before con-
verting it to a PlenOctree. Indeed, we have found that the
image quality improvements gained during ﬁne-tuning can
often be greater than continuing to train the NeRF-SH an
equivalent amount of time. Therefore it can be more time
efﬁcient to stop the NeRF-SH training before it has con-
verged and transition to PlenOctree conversion and ﬁne-
tuning.

In Figure 7 we compare NeRF and NeRF-SH mod-
els trained for 2 million iterations each to a sequence of
PlenOctree models extracted from NeRF-SH checkpoints.
We ﬁnd that given a time constraint, it is almost always
preferable to stop the NeRF training and transition to
PlenOctree optimization.

5.5. Real-time and In-browser Applications

Interactive demos. Within our desktop viewer, we are
able to perform a variety of real-time scene operations on
the PlenOctree representation. For example, it is possible to
insert meshes while maintaining proper occlusion, slice the
PlenOctree to visualize a cross-section, or render the depth
map to verify the geometry. Other features include probing
the radiance distribution at any point in space, and inspect-
ing subsets of SH components. These examples are demon-
strated in Figure 9. The ability to perform these actions in
real-time is beneﬁcial both for interactive entertainment and
debugging NeRF-related applications.
Web renderer. We have implemented a web-based
renderer enabling interactive viewing of PlenOctrees in
the browser. This is achieved by rewriting our CUDA-

Ground TruthGround TruthNeRFOursNeRFOursFigure 7: Indirect training acceleration. Training curves for ﬁt-
ting the synthetic NeRF ship scene for 2 million iterations. The
baseline NeRF model and our NeRF-SH model perform similarly
during training. We ﬁnd that by optimizing the PlenOctree con-
verted from NeRF-SH checkpoints, we are able to reach a similar
quality more quickly. The PlenOctree conversion and ﬁne-tuning
adds approximately 1 hour to the training time; despite this, we
ﬁnd that it takes ∼16 hours of NeRF training to match the same
quality as the PlenOctree model after ∼4.5 hours.

Figure 8: Qualitative results on further real scenes. We apply
FastNeRF to the NeRF-360-real and LLFF datasets. While our
method is not designed for unbounded or forward-facing scenes
(where MPIs can be more appropriate), it nevertheless performs
reasonably well. Note for forward-facing scenes, we construct the
octree in NDC coordinates.

based PlenOctree renderer as a WebGL-compatible frag-
ment shader and applying compressions to reduce ﬁle sizes.
Please see the appendix for more information.

6. Discussion

We have introduced a new data representation for NeRFs
using PlenOctrees, which enables real-time rendering capa-
bilities for arbitrary objects and scenes. Not only can we
accelerate the rendering performance of the original NeRF
method by more than 3000 times, but we can produce im-
ages that are either equal or better quality than NeRF thanks
to our hierarchical data structure. As training time poses an-
other hurdle for adopting NeRFs in practice (taking 1-2 days
to fully converge), we also showed that our PlenOctrees can
accelerate effective training time for our NeRF-SH. Finally,
we have implemented an in-browser viewer based on We-
bGL to demonstrate real-time and 6-DOF rendering capa-
bilities of NeRFs on consumer laptops. In the future, our

Figure 9: Real-time interactive demos. Set of real-time opera-
tions that can be performed on PlenOctree within our interactive
viewer. This application will be released to the public.

approach may enable virtual online stores in VR, where any
products with arbitrary complexity and materials can be vi-
sualized in real-time while enabling 6-DOF viewing.

Limitations and Future Work. While we achieve state-
of-the-art rendering performance and frame rates, the octree
representation is much larger than the compact represen-
tation of the original NeRF model and has a larger mem-
ory footprint. The average uncompressed octree size for
the full model is 1.93 GB on the synthetic dataset and 3.53
GB on the Tanks and Temples dataset. For online deliv-
ery, we use lower-resolution compressed models which are
about 30-120 MB; please see the appendix for details. Al-
though already possible in some form (Fig. 8), applying our
method to unbounded and forward-facing scenes optimally
requires further work as the data distribution is different for
unbounded scenes. The forward-facing scenes inherently
do not support 6-DOF viewing, and we suggest MPIs may
be more appropriate in this case [59].

In the future, we plan to explore extensions of our
method to enable real-time 6-DOF immersive viewing of
large-scale scenes, as well as of dynamic scenes. We be-
lieve that real-time rendering of NeRFs has the potential to
become a new standard for next-generation AR/VR tech-
nologies, as photorealistic 3D content can be digitized as
easily as recording 2D videos.

0102030405060Training Time (hours)242526272829PSNRNeRFNeRF-SHPlenOctreee) Zeroth SH Componentf) Higher SH Componentsc) Slicingb) Lumisphere Visualizationa) Object Insertiond) DepthAcknowledgements

We thank Vickie Ye and Ben Recht for help discussions
as well as reviewing the manuscript, Zejian Wang of Pin-
screen for helping with video capture, and BAIR commons
for an allocation of GCP credits.

References

[1] Edward H Adelson, James R Bergen, et al. The plenoptic
function and the elements of early vision, volume 2. Vision
and Modeling Group, Media Laboratory, Massachusetts In-
stitute of Technology, 1991. 2

[2] Ronen Basri and David W Jacobs. Lambertian reﬂectance
and linear subspaces. IEEE transactions on pattern analysis
and machine intelligence, 25(2):218–233, 2003. 4

[3] Chris Buehler, Michael Bosse, Leonard McMillan, Steven
Gortler, and Michael Cohen. Unstructured lumigraph ren-
dering. In SIGGRAPH, pages 425–432, 2001. 2

[4] Zhiqin Chen and Hao Zhang. Learning implicit ﬁelds for

generative shape modeling. In CVPR, 2019. 3

[5] Paul E Debevec, Camillo J Taylor, and Jitendra Malik. Mod-
eling and rendering architecture from photographs: A hybrid
geometry-and image-based approach. In SIGGRAPH, pages
11–20, 1996. 2

[6] Boyang Deng, Jonathan T. Barron, and Pratul P. Srinivasan.
JaxNeRF: an efﬁcient JAX implementation of NeRF, 2020.
6, 14

[7] Robert A Drebin, Loren Carpenter, and Pat Hanrahan. Vol-
ACM SIGGRAPH Computer Graphics,

ume rendering.
22(4):65–74, 1988. 2

[8] Ronald Aylmer Fisher. Dispersion on a sphere. Proceedings
of the Royal Society of London. Series A. Mathematical and
Physical Sciences, 217(1130):295–305, 1953. 4, 11

[9] Stephan J. Garbin, Marek Kowalski, Matthew Johnson,
Jamie Shotton, and Julien Valentin. Fastnerf: High-ﬁdelity
neural rendering at 200fps. arXiv, 2021. 3

[10] Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, and
Yaron Lipman. Implicit geometric regularization for learning
shapes. ICML, 2020. 3

[11] Christian H¨ane, Shubham Tulsiani, and Jitendra Malik. Hi-
erarchical surface prediction for 3d object reconstruction. In
3DV, pages 412–420. IEEE, 2017. 3

[12] Paul Heckbert. Color image quantization for frame buffer

display. SIGGRAPH, 1982. 12

[13] Peter Hedman, Pratul P. Srinivasan, Ben Mildenhall,
Jonathan T. Barron, and Paul Debevec. Baking neural ra-
diance ﬁelds for real-time view synthesis. arXiv, 2021. 3
[14] Abhishek Kar, Christian H¨ane, and Jitendra Malik. Learning

a multi-view stereo machine. NIPS, 2017. 2

[15] Diederik P Kingma and Jimmy Ba. Adam: A method for

stochastic optimization. In ICLR, 2015. 4, 14

[16] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen
Koltun.
Tanks and temples: Benchmarking large-scale
scene reconstruction. ACM Transactions on Graphics (ToG),
36(4):1–13, 2017. 6

[17] Aaron Knoll. A survey of octree volume rendering methods.
GI, the Gesellschaft f¨ur Informatik, page 87, 2006. 3

[18] Kiriakos N Kutulakos and Steven M Seitz. A theory of shape

by space carving. IJCV, 38(3):199–218, 2000. 2
[19] Marc Levoy and Pat Hanrahan. Light ﬁeld rendering.

In

SIGGRAPH, pages 31–42, 1996. 2

[20] Ruilong Li, Yuliang Xiu, Shunsuke Saito, Zeng Huang, Kyle
Olszewski, and Hao Li. Monocular real-time volumetric per-
formance capture. In ECCV, pages 49–67. Springer, 2020. 3
[21] Zhengqin Li, Mohammad Shaﬁei, Ravi Ramamoorthi,
Kalyan Sunkavalli, and Manmohan Chandraker. Inverse ren-
dering for complex indoor scenes: Shape, spatially-varying
In CVPR, pages
lighting and svbrdf from a single image.
2475–2484, 2020. 4, 11

[22] David B Lindell, Julien NP Martel, and Gordon Wetzstein.
Autoint: Automatic integration for fast neural volume ren-
dering. In CVPR, 2021. 3, 7

[23] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and
Christian Theobalt. Neural sparse voxel ﬁelds. NeurIPS,
2020. 3, 6, 7, 11

[24] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel
Schwartz, Andreas Lehrmann, and Yaser Sheikh. Neural
volumes: Learning dynamic renderable volumes from im-
ages. ACM Transactions on Graphics (TOG), 38(4):65:1–
65:14, 2019. 1, 2, 7, 11

[25] Stephen Lombardi, Tomas Simon, Gabriel Schwartz,
Michael Zollhoefer, Yaser Sheikh, and Jason Saragih. Mix-
ture of volumetric primitives for efﬁcient neural rendering.
In SIGGRAPH, 2021. 3

[26] Jean loup Gailly and Mark Adler. zlib, 2017. 12
[27] Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi,
Jonathan T. Barron, Alexey Dosovitskiy, and Daniel Duck-
worth. NeRF in the Wild: Neural Radiance Fields for Un-
constrained Photo Collections. In CVPR, 2021. 3

[28] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-
bastian Nowozin, and Andreas Geiger. Occupancy networks:
In CVPR,
Learning 3d reconstruction in function space.
2019. 3

[29] Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon,
Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and
Abhishek Kar. Local light ﬁeld fusion: Practical view syn-
thesis with prescriptive sampling guidelines. ACM Transac-
tions on Graphics (TOG), 38(4):1–14, 2019. 2

[30] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:
Representing scenes as neural radiance ﬁelds for view syn-
thesis. ECCV, 2020. 1, 3, 4, 6

[31] Michael Niemeyer, Lars Mescheder, Michael Oechsle, and
Andreas Geiger. Differentiable volumetric rendering: Learn-
ing implicit 3d representations without 3d supervision.
In
CVPR, 2020. 3

[32] Jeong Joon Park, Peter Florence, Julian Straub, Richard
Newcombe, and Steven Lovegrove. Deepsdf: Learning con-
tinuous signed distance functions for shape representation.
In CVPR, 2019. 3

[33] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Soﬁen
Bouaziz, Dan B Goldman, Steven M Seitz, and Ricardo-
Martin Brualla. Deformable neural radiance ﬁelds. arXiv,
2021. 3

[34] Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc
Pollefeys, and Andreas Geiger. Convolutional occupancy
networks. In ECCV, 2020. 3

[35] Eric Penner and Li Zhang. Soft 3d reconstruction for view
synthesis. ACM Transactions on Graphics (TOG), 36(6):1–
11, 2017. 2

[36] Ravi Ramamoorthi and Pat Hanrahan. On the relationship
between radiance and irradiance: determining the illumina-
tion from images of a convex lambertian object. JOSA A,
18(10):2448–2459, 2001. 4

[37] Daniel Rebain, Wei Jiang, Soroosh Yazdani, Ke Li,
Kwang Moo Yi, and Andrea Tagliasacchi. DeRF: Decom-
posed radiance ﬁelds. In CVPR, 2021. 3

[38] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas
Geiger. Kilonerf: Speeding up neural radiance ﬁelds with
thousands of tiny mlps. arXiv, 2021. 3

[39] Gernot Riegler, Ali Osman Ulusoy, and Andreas Geiger.
Octnet: Learning deep 3d representations at high resolutions.
In CVPR, 2017. 3

[40] Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Mor-
ishima, Angjoo Kanazawa, and Hao Li. Pifu: Pixel-aligned
implicit function for high-resolution clothed human digitiza-
tion. In ICCV, 2019. 3

[41] Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas
Geiger. GRAF: Generative radiance ﬁelds for 3d-aware im-
age synthesis. In NeurIPS, 2020. 3

[42] Steven M Seitz and Charles R Dyer. Photorealistic scene re-
construction by voxel coloring. IJCV, 35(2):151–173, 1999.
2

[43] Jonathan Shade, Steven Gortler, Li-wei He, and Richard
Szeliski. Layered depth images. In SIGGRAPH, pages 231–
242, 1998. 2

[44] Vincent Sitzmann, Justus Thies, Felix Heide, Matthias
Nießner, Gordon Wetzstein, and Michael Zollhofer. Deep-
Voxels: Learning persistent 3d feature embeddings.
In
CVPR, pages 2437–2446, 2019. 2

[45] Vincent Sitzmann, Michael Zollh¨ofer, and Gordon Wet-
Scene representation networks: Continuous 3d-
In NeurIPS,

zstein.
structure-aware neural scene representations.
2019. 3, 7, 11

[46] Peter-Pike Sloan, Jan Kautz, and John Snyder. Precomputed
radiance transfer for real-time rendering in dynamic, low-
frequency lighting environments. In SIGGRAPH, pages 527–
536, 2002. 4, 11

[47] Pratul P Srinivasan, Boyang Deng, Xiuming Zhang,
Matthew Tancik, Ben Mildenhall, and Jonathan T Barron.
NeRV: Neural reﬂectance and visibility ﬁelds for relighting
and view synthesis. In CVPR, 2021. 3

[48] Pratul P Srinivasan, Richard Tucker, Jonathan T Barron,
Ravi Ramamoorthi, Ren Ng, and Noah Snavely. Pushing the
boundaries of view extrapolation with multiplane images. In
CVPR, pages 175–184, 2019. 2

[49] Richard Szeliski and Polina Golland. Stereo matching with
transparency and matting. In ICCV, pages 517–524. IEEE,
1998. 2

[50] Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten
Kreis, Charles Loop, Derek Nowrouzezahrai, Alec Jacobson,

Morgan McGuire, and Sanja Fidler. Neural geometric level
of detail: Real-time rendering with implicit 3D shapes. In
CVPR, 2021. 3

[51] Matthew Tancik, Ben Mildenhall, Terrance Wang, Divi
Schmidt, Pratul P. Srinivasan, Jonathan T. Barron, and Ren
Ng. Learned initializations for optimizing coordinate-based
neural representations. In CVPR, 2021. 3

[52] Maxim Tatarchenko, Alexey Dosovitskiy, and Thomas Brox.
Octree generating networks: Efﬁcient convolutional archi-
In ICCV, pages
tectures for high-resolution 3d outputs.
2088–2096, 2017. 3

[53] Alex Trevithick and Bo Yang. GRF: Learning a general radi-
ance ﬁeld for 3d scene representation and rendering. arXiv,
2021. 3

[54] Yu-Ting Tsai and Zen-Chung Shih. All-frequency precom-
puted radiance transfer using spherical radial basis functions
and clustered tensor approximation. ACM Transactions on
graphics (TOG), 25(3):967–976, 2006. 4, 11

[55] Shubham Tulsiani, Tinghui Zhou, Alexei A Efros, and Ji-
tendra Malik. Multi-view supervision for single-view recon-
struction via differentiable ray consistency. In CVPR, pages
2626–2634, 2017. 2

[56] Michael Waechter, Nils Moehrle, and Michael Goesele. Let
there be color! large-scale texturing of 3d reconstructions. In
ECCV, pages 836–850. Springer, 2014. 2

[57] Peng-Shuai Wang, Yang Liu, Yu-Xiao Guo, Chun-Yu Sun,
and Xin Tong. O-cnn: Octree-based convolutional neu-
ral networks for 3d shape analysis. ACM Transactions on
Graphics (TOG), 36(4):1–11, 2017. 3

[58] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srini-
vasan, Howard Zhou, Jonathan T Barron, Ricardo Martin-
Brualla, Noah Snavely, and Thomas Funkhouser. IBRNet:
Learning multi-view image-based rendering. In CVPR, 2021.
3

[59] Suttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon
Yenphraphai, and Supasorn Suwajanakorn. NeX: Real-time
view synthesis with neural basis expansion. In 2021. 3, 8

[60] Daniel N Wood, Daniel I Azuma, Ken Aldinger, Brian Cur-
less, Tom Duchamp, David H Salesin, and Werner Stuet-
zle. Surface light ﬁelds for 3d photography. In SIGGRAPH,
pages 287–296, 2000. 2

[61] Qiangeng Xu, Weiyue Wang, Duygu Ceylan, Radomir
Mech, and Ulrich Neumann. DISN: Deep implicit surface
network for high-quality single-view 3d reconstruction. In
H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc,
E. Fox, and R. Garnett, editors, Advances in Neural Infor-
mation Processing Systems 32, pages 492–502. Curran As-
sociates, Inc., 2019. 3

[62] Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan
Atzmon, Basri Ronen, and Yaron Lipman. Multiview neu-
ral surface reconstruction by disentangling geometry and ap-
pearance. In NeurIPS, 2020. 3

[63] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.
pixelNeRF: Neural radiance ﬁelds from one or few images.
In CVPR, 2021. 3

[64] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe,
and Noah Snavely. Stereo magniﬁcation: Learning view syn-
thesis using multiplane images. In SIGGRAPH, 2018. 2

NeRF-SH/SG

Converted PlenOctree

Basis

SH-9
SH-16
SH-25
SG-25

PSNR ↑

SSIM ↑ LPIPS ↓

PSNR ↑

SSIM ↑ LPIPS ↓ GB ↓

FPS↑

31.44
31.57
31.56
31.74

0.951
0.952
0.951
0.953

0.065
0.063
0.063
0.062

31.45
31.71
31.69
31.63

0.956
0.958
0.958
0.958

0.056
0.053
0.052
0.052

1.00
1.93
2.68
2.26

262
168
128
151

Table 4: Spherical Basis Function Ablation. We experiment with
various versions of spherical basis functions, including SH-16, SH-
25 and SG-25.

Appendix

A. Additional Results

A.1. Detailed comparisons

Here we provide further qualitative comparisons with
baselines: SRN [45], Neural Volumes [24], NSVF [23]
in Figure 10. We show more qualitative results of our
method in Figure 11 and Figure 12. We also report a per-
scene breakdown of the quantitative metrics against all ap-
proaches in Table 5, 6, 7, 8.

A.2. Spherical Basis Function Ablation

We also provide ablation studies on the choice of spher-
ical basis functions. We ﬁrst ablate the effect on the num-
ber of spherical harmonics basis, then we explore the use
of a learnable spherical basis functions. All experiments
are conducted on NeRF-synthetic dataset and we report the
average metric directly after training NeRF with spherical
basis functions and after converting it to PlenOctrees with
ﬁne-tuning.

Number of SH basis functions First, we ablate the num-
ber of basis functions used for spherical harmonics. Aver-
age metrics across the NeRF-synthetic dataset are reported
both for the modiﬁed NeRF model and the corresponding
PlenOctree. We found that switching between (cid:96)max = 3
(SH-16) and 4 (SH-25) makes very little difference in terms
of metrics or visual quality.

Spherical Gaussians Furthermore, we also experimented
with spherical Gaussians (SGs) [8], which is another form
of spherical basis functions similar to spherical harmon-
ics, but with learnable Gaussian kernels. Please see §B.1
for a brief introduction of SHs and SGs. SG-25 denotes
our model using 25 SG components instead of SH, all with
learnable lobe axis p and bandwidth λ. However, while
this model has marginally better PSNR, the advantage dis-
appears following PlenOctree conversion and ﬁne-tuning.

B. Technical Details

B.1. Spherical Basis Functions: SH and SG

In the main paper, we used the SH functions without
deﬁning their exact form. Here, we provide a brief technical
discussion of both spherical harmonics (SH) and spherical
Gaussians (SG) for completeness.

Spherical Harmonics. The Spherical Harmonics (SH)
form a complete basis of functions S2 → C. For (cid:96) ∈ N∪{0}
and m ∈ {−(cid:96), . . . , (cid:96)}, the SH function of degree (cid:96) and order
m is deﬁned as:

Y m
(cid:96) (θ, φ) =

(cid:115)

2(cid:96) + 1
4π

((cid:96) − m)!
((cid:96) + m)!

(cid:96) (cos θ)eimφ
P m

(7)

where P m
mials. A real basis of SH Y m
terms of its complex analogue Y m

(cid:96) (cosθ)eimφ are the associated Legendre polyno-
: S2 (cid:55)→ R can be deﬁned in
: S2 (cid:55)→ C by setting

(cid:96)

(cid:96)

Y m
(cid:96) (θ, φ) =






√

2(−1)m Im[Y |m|

(cid:96)

]

Y 0
(cid:96)
√
2(−1)m Re[Y m
(cid:96) ]

if m < 0
if m = 0
if m > 0

(8)

Any real spherical function L : S2 → R may then be

expressed in the SH basis:

L(d) = L(θ, φ) =

∞
(cid:88)

(cid:96)
(cid:88)

(cid:96)=0

m=−(cid:96)

(cid:96) Y m
km

(cid:96) (θ, φ)

(9)

Spherical Gaussians. Spherical Gaussians (SGs), also
known as the von Mises-Fisher distribution [8], is another
form of spherical basis functions that have been widely
adopted to approximate spherical functions. Unlike SHs,
SGs are a learnable basis. A normalized SG is deﬁned as:

G(d; p, λ) = eλ(d·p−1)

(10)

Where p ∈ R2 is the lobe axis, and λ ∈ R is the band-
width (sharpness) of the Gaussian kernel. Due to the vary-
ing bandwidths supported by SGs, they are suitable for rep-
resenting all-frequency signals such as lighting [54, 46, 21].
A spherical function represented using n SGs is formulated
as:

n
(cid:88)

L(d) ≈

k(cid:96)G(cid:96)(d; p, λ)

(11)

(cid:96)=0
Where k(cid:96) ∈ R3 is the RGB coefﬁcients for each SG.

B.2. PlenOctree Compression

The uncompressed PlenOctree ﬁle would be unpleas-
antly time-consuming for users to download for in-browser
rendering. Thus, to minimize the size of PlenOctrees for

Figure 10: Qualitative comparisons on NeRF-synthetic.

viewing in the browser, we use SH-9 instead of SH-16 or
SH-25 and apply a looser bounding box, which reduces the
number of occupied voxels. On top of this, we compress
the PlenOctrees directly in the following ways:

1. We quantize the SH coefﬁcients in the tree using the
popular median-cut algorithm [12]. More speciﬁcally,
the σ values are kept as is; for each SH basis func-
(cid:96) ∈ R3 into
tion, we quantize the RGB coefﬁcients km
216 colors. Afterwards, separately for each SH basis
function, we store a 216 × 3 codebook (as float16)
along with pointers from each tree leaf to a position in
the codebook (as int16).

2. We compress the entire tree, including pointers, using
the standard DEFLATE algorithm from ZLIB [26].

This process reduces the ﬁle size by as much as 20-30
times. The tree is fully decompressed before it is displayed
in the web renderer. We will also release this code.

B.3. Analytic Derivatives of PlenOctree Rendering

In this section, we derive the analytic derivatives of the
NeRF piecewise constant volume rendering model for op-
timizing PlenOctrees directly. Throughout this section we
will consider a ﬁxed ray with a given origin and direction.

B.3.1 Deﬁnitions

For preciseness, we provide deﬁnitions of quantities used in
NeRF volume rendering. The NeRF rendering model con-
siders a ray divided into N consecutive segments with end-
points {ti}N
i=0, where t0 and tN are the near and far bounds.
The segments have constant densities σ = (σ0, . . . , σN −1)
where each σi ≥ 0. If we shine a light of intensity 1 at ti,

then at the camera position t0, the light intensity is given by

Ti(σ) :=

i−1
(cid:89)

j=0

exp(−δjσj),

(12)

Where δi := ti+1 − ti are segment lengths as in §3 of the
main paper. Note that Ti is also known as the accumulated
transmittance from t0 to ti, and is the same as the deﬁni-
tion in (1). It can be shown that this precisely models the
absorption within each segment in the piecewise-constant
setting.

Let c = (c0, . . . cN −1) be the color associated with seg-
ments 0, . . . , N − 1, and cN be the background light inten-
sity; each c0, . . . , cN ∈ [0, 1]3 is an RGB color. We are
interested in the derivative of the rendered color ˆC(σ, c)
with respect to σ and c. Note cN (background) is typically
considered a hyperparameter.

B.3.2 Derivation of the Derivatives

From the original NeRF rendering equation (1), we can ex-
press the rendered ray color ˆC(σ, c) as:

ˆC (σ, c) = TN (σ) cN +

N −1
(cid:88)

i=0

Ti(σ)(cid:0)1 − e−σiδi(cid:1) ci

(13)

=

N
(cid:88)

i=0

wi(σ) ci

(14)

Where wi(σ) = Ti(σ) (1 − exp(−σiδi)) = Ti(σ) −
Ti+1(σ) are segment weights, and wN (σ) = TN (σ).4

Color derivative. Since the rendered color are a convex
combination of the segment colors, it’s immediately clear

4Note that the background color cN was omitted in equation (1) of the

main paper for simplicity.

Ground Truth SRNNeural VolumesNSVFNeRFNeRF-SH (ours)PlenOctree (ours)Figure 11: More qualitative results of our PlenOctrees on NeRF-synthetic.

Figure 12: More qualitative results of our PlenOctrees on Tanks&Temples.

that

Where the derivative of the intensity Tk, is

∂ ˆC
∂ci

(σ, c) = wi(σ)

(15)

Handling spherical harmonics colors is straightforward by
applying the chain rule, noting that the SH basis function
values are constant across the ray.

Density derivative. This is slightly more tricky. We can
write the derivative wrt. σi,

∂Tk
∂σi

(σ) =

∂
∂σi

= −δi





e−δj σj







k−1
(cid:89)

j=0





k−1
(cid:89)

e−δj σj

 1k>i

j=0

= −δiTk(σ) 1k>i

(17)

(18)

(19)

∂ ˆC
∂σi

(σ, c) = cN

∂TN
∂σi

N −1
(cid:88)

+

ck

k=0

(cid:18) ∂Tk
∂σi

−

∂Tk+1
∂σi

(cid:19)

(16)

1k>i denotes an indicator function whose value is 1 if
k > i or 0 else. Basically, we can delete any Tk for k ≤ i
from the original expression, then multiply by −δi. There-

fore we can simplify (16) as follows

(cid:34)

(σ, c) = δi

ciTi+1(σ) −

∂ ˆC
∂σi

(cid:35)

ck wk(σ)

(20)

N
(cid:88)

k=i+1

Remark. Within the PlenOctree renderer, this gradient
can be computed in two rendering passes; the second pass
is needed due to dependency on “future” weights and col-
ors not seen by the ray marching process. The ﬁrst pass
store (cid:80)N
k=0 ck wk(σ), then subtracting a preﬁx from it. The
overhead is still relatively small, and auxiliary memory use
is constant.

If there are multiple colors, we simply add the density
derivatives over all of them. In practice, usually the network
outputs ˜σi ∈ R and we set σi = (˜σi)+, so we also need to
take care of setting the gradient to 0 if ˜σi ≤ 0.

B.4. NeRF-SH Training Details

Our NeRF-SH model is built upon a Jax reimplementa-
tion of NeRF [6]. In our experiments, we use a batch size
of 1024 rays, each with 64 sampled points in the coarse
volume and 128 additional sampled points in the ﬁne vol-
ume. The model is optimized with the Adam optimizer [15]
using a learning rate that starts at 5 × 10−4 and decays ex-
ponentially to 5 × 10−6 over the training process. All of our
models are trained for 2M iterations under the same proto-
col. Training takes around 50 hours to converge for each
model on a single NVIDIA V100 GPU.

B.5. PlenOctree Optimization Details

After converting the NeRF-SH model into a PlenOctree,
we further optimize the PlenOctree on the training set with
SGD using the NeRF loss; note we no longer apply the spar-
sity loss here since the octree is already sparse. For NeRF-
synthetic dataset, we use a constant 1 × 107 learning rate
and optimize for maximum 80 epochs. For Tanks&Temples
dataset, we set the learning rate to 1.5 × 106 and the maxi-
mum epochs to 40. We applied early stopping for the opti-
mization process by monitoring the PSNR on the valida-
tion set5. On average it takes around 10 minutes to ﬁn-
ish the PlenOctree optimization for each scene on a sin-
gle NVIDIA V100 GPU. The entire optimization process
is done in float32 for stability, but after it we storage the
PlenOctree with float16 to reduce the model size.

5For Tanks&Temples dataset, we hold out 10% of the training set as

validation set only for PlenOctree optimization.

PSNR ↑

NeRF (original)
NeRF
SRN
Neural Volumes
NSVF

NeRF-SH
PlenOctree from NeRF-SH
PlenOctree after ﬁne-tuning

Chair Drums

Ficus Hotdog

Lego Materials Mic

Ship Mean

33.00
34.08
26.96
28.33
33.19

33.98
33.19
34.66

25.01
25.03
17.18
22.58
25.18

25.17
25.01
25.31

30.13
30.43
20.73
24.79
31.23

30.72
30.56
30.79

36.18
36.92
26.81
30.71
37.14

36.75
36.15
36.79

32.54
33.28
20.85
26.08
32.29

32.77
32.12
32.95

29.62
29.91
18.09
24.22
32.68

29.95
29.56
29.76

32.91
34.53
26.85
27.78
34.27

34.04
33.01
33.97

28.65
29.36
20.60
23.93
27.93

29.21
28.58
29.42

31.01
31.69
22.26
26.05
31.75

31.57
31.02
31.71

SSIM ↑

NeRF (original)
NeRF
SRN
Neural Volumes
NSVF

NeRF-SH
PlenOctree from NeRF-SH
PlenOctree after ﬁne-tuning

Chair Drums

Ficus Hotdog

Lego Materials Mic

Ship Mean

0.967
0.975
0.910
0.916
0.968

0.974
0.970
0.981

0.925
0.925
0.766
0.873
0.931

0.927
0.927
0.933

0.964
0.967
0.849
0.910
0.960

0.968
0.968
0.970

0.974
0.979
0.923
0.944
0.987

0.978
0.977
0.982

0.961
0.968
0.809
0.880
0.973

0.966
0.965
0.971

0.949
0.952
0.808
0.888
0.854

0.951
0.953
0.955

0.980
0.987
0.947
0.946
0.980

0.985
0.983
0.987

0.856
0.868
0.757
0.784
0.973

0.866
0.863
0.884

0.947
0.953
0.846
0.893
0.953

0.952
0.951
0.958

LPIPS ↓

NeRF (original)
NeRF
SRN
Neural Volumes
NSVF

NeRF-SH
PlenOctree from NeRF-SH
PlenOctree after ﬁne-tuning

Chair Drums

Ficus Hotdog

Lego Materials Mic

Ship Mean

0.046
0.035
0.106
0.109
0.043

0.037
0.039
0.022

0.091
0.085
0.267
0.214
0.069

0.087
0.088
0.076

0.044
0.038
0.149
0.162
0.017

0.039
0.038
0.038

0.121
0.079
0.100
0.109
0.025

0.041
0.044
0.032

0.050
0.040
0.200
0.175
0.029

0.041
0.046
0.034

0.063
0.060
0.174
0.130
0.021

0.060
0.063
0.059

0.028
0.019
0.063
0.107
0.010

0.021
0.023
0.017

0.206
0.185
0.299
0.276
0.162

0.177
0.189
0.144

0.081
0.068
0.170
0.160
0.047

0.063
0.066
0.053

FPS ↑

Chair Drums

Ficus Hotdog

Lego Materials Mic

Ship Mean

NeRF (original)
NeRF
SRN
Neural Volumes
NSVF

NeRF-SH
PlenOctree

0.023
0.045
0.909
3.330
1.044

0.051
352.4

0.023
0.045
0.909
3.330
0.735

0.051
175.9

0.023
0.045
0.909
3.330
0.597

0.051
85.6

0.023
0.045
0.909
3.330
0.660

0.051
95.5

0.023
0.045
0.909
3.330
0.633

0.051
186.8

0.023
0.045
0.909
3.330
0.517

0.051
64.2

0.023
0.045
0.909
3.330
1.972

0.051
324.9

0.023
0.045
0.909
3.330
0.362

0.051
56.0

0.023
0.045
0.909
3.330
0.815

0.051
167.7

Table 5: Per-scene quantitive results on NeRF-synthetic dataset.

NeRF (original)
NeRF
SRN
Neural Volumes
NSVF

NeRF-SH
PlenOctree from NeRF-SH
PlenOctree after ﬁne-tuning

NeRF (original)
NeRF
SRN
Neural Volumes
NSVF

NeRF-SH
PlenOctree from NeRF-SH
PlenOctree after ﬁne-tuning

NeRF (original)
NeRF
SRN
Neural Volumes
NSVF

NeRF-SH
PlenOctree from NeRF-SH
PlenOctree after ﬁne-tuning

PSNR ↑

Barn

Caterpillar

Family

Ignatius Truck Mean

24.05
27.39
22.44
20.82
27.16

27.05
25.78
26.80

23.75
25.24
21.14
20.71
26.44

25.06
24.80
25.29

SSIM ↑

30.29
32.47
27.57
28.72
33.58

32.28
32.04
32.85

25.43
27.95
26.70
26.54
27.91

28.06
27.92
28.19

25.36
26.66
22.62
21.71
26.92

26.66
26.15
26.83

25.78
27.94
24.09
23.70
28.40

27.82
27.34
27.99

Barn

Caterpillar

Family

Ignatius Truck Mean

0.750
0.842
0.741
0.721
0.823

0.838
0.820
0.856

0.860
0.892
0.834
0.819
0.900

0.891
0.889
0.907

LPIPS ↓

0.932
0.951
0.908
0.916
0.954

0.949
0.948
0.962

0.920
0.940
0.920
0.922
0.930

0.940
0.940
0.948

0.860
0.896
0.832
0.793
0.895

0.895
0.889
0.914

0.864
0.904
0.847
0.834
0.900

0.902
0.897
0.917

Barn

Caterpillar

Family

Ignatius Truck Mean

0.395
0.286
0.448
0.479
0.307

0.291
0.296
0.226

0.196
0.189
0.278
0.280
0.141

0.185
0.188
0.148

FPS ↑

0.098
0.092
0.134
0.111
0.063

0.091
0.094
0.069

0.111
0.102
0.128
0.117
0.106

0.091
0.092
0.080

0.192
0.173
0.266
0.312
0.148

0.175
0.180
0.130

0.198
0.168
0.251
0.260
0.153

0.167
0.170
0.131

Barn

Caterpillar

Family

Ignatius Truck Mean

NeRF (original)
NeRF
SRN
Neural Volumes
NSVF

NeRF-SH
PlenOctree (ours)

0.007
0.013
0.250
1.000
10.74

0.015
46.94

0.007
0.013
0.250
1.000
5.415

0.015
54.00

0.007
0.013
0.250
1.000
2.625

0.015
32.33

0.007
0.013
0.250
1.000
6.062

0.015
15.67

0.007
0.013
0.250
1.000
5.886

0.015
62.16

0.007
0.013
0.250
1.000
6.146

0.015
42.22

Table 6: Per-scene quantitive results on Tanks&Temples dataset.

PSNR ↑

Chair Drums

Ficus Hotdog

Lego Materials Mic

Ship Mean

Ours-1.9G
Ours-1.4G
Ours-0.4G
Ours-0.3G

34.66
34.66
32.92
32.03

25.31
25.30
24.82
24.10

30.79
30.82
30.07
29.42

36.79
36.36
36.06
34.46

32.95
32.96
31.61
30.25

29.76
29.75
28.89
28.44

33.97
33.98
32.19
30.78

29.42
29.29
29.04
27.36

31.71
31.64
30.70
29.60

GB ↓

Chair Drums

Ficus Hotdog

Lego Materials Mic

Ship Mean

Ours-1.9G
Ours-1.4G
Ours-0.4G
Ours-0.3G

0.830
0.671
0.176
0.131

1.240
0.852
0.350
0.183

1.791
0.943
0.287
0.286

2.674
1.495
0.419
0.403

2.067
1.421
0.499
0.340

3.682
3.060
0.295
0.503

0.442
0.569
0.327
0.159

2.689
1.881
1.195
0.381

1.93
1.36
0.44
0.30

FPS ↑

Chair Drums

Ficus Hotdog

Lego Materials Mic

Ship Mean

Ours-1.9G
Ours-1.4G
Ours-0.4G
Ours-0.3G

352.4
399.7
639.6
767.6

175.9
222.2
290.0
424.1

85.6
147.3
208.7
203.8

95.5
163.5
273.5
271.7

186.8
247.9
339.0
443.6

64.2
68.0
268.0
189.1

324.9
393.8
522.6
796.4

56.0
75.4
86.7
181.1

167.7
214.7
328.5
409.7

Table 7: Per-scene quantitive results on PlenOctree conversion ablations.

Chair Drums

Ficus Hotdog

Lego Materials Mic

Ship Mean

PSNR ↑

NeRF-SH9
NeRF-SH16
NeRF-SH25
NeRF-SG25

PlenOctree-SH9
PlenOctree-SH16
PlenOctree-SH25
PlenOctree-SG25

33.88
33.98
34.01
34.08

34.38
34.66
34.72
34.37

25.24
25.17
25.10
25.40

25.34
25.31
25.32
25.52

30.69
30.72
30.52
31.21

30.72
30.79
30.68
31.16

36.68
36.75
36.83
36.92

36.68
36.79
36.96
36.67

SSIM ↑

32.73
32.77
32.76
32.93

32.79
32.95
32.85
32.98

29.53
29.95
30.06
29.77

29.16
29.76
29.79
29.41

33.68
34.04
34.08
34.31

33.23
33.97
33.90
33.63

29.11
29.21
29.11
29.28

29.28
29.42
29.29
29.32

31.44
31.57
31.56
31.74

31.45
31.71
31.69
31.63

NeRF-SH9
NeRF-SH16
NeRF-SH25
NeRF-SG25

PlenOctree-SH9
PlenOctree-SH16
PlenOctree-SH25
PlenOctree-SG25

Chair Drums

Ficus Hotdog

Lego Materials Mic

Ship Mean

0.973
0.974
0.973
0.974

0.980
0.981
0.981
0.980

0.928
0.927
0.926
0.930

0.934
0.933
0.935
0.937

0.968
0.968
0.967
0.971

0.970
0.970
0.971
0.973

0.978
0.978
0.978
0.978

0.982
0.982
0.983
0.982

0.966
0.966
0.966
0.967

0.970
0.971
0.971
0.972

0.948
0.951
0.952
0.951

0.950
0.955
0.955
0.953

0.984
0.985
0.985
0.986

0.984
0.987
0.987
0.986

0.864
0.866
0.864
0.867

0.881
0.884
0.883
0.883

0.951
0.952
0.951
0.953

0.956
0.958
0.958
0.958

Chair Drums

Ficus Hotdog

Lego Materials Mic

Ship Mean

LPIPS ↓

NeRF-SH9
NeRF-SH16
NeRF-SH25
NeRF-SG25

PlenOctree-SH9
PlenOctree-SH16
PlenOctree-SH25
PlenOctree-SG25

0.037
0.037
0.038
0.036

0.023
0.022
0.023
0.023

0.086
0.087
0.087
0.083

0.075
0.076
0.072
0.069

0.043
0.039
0.039
0.034

0.041
0.038
0.036
0.034

0.044
0.041
0.040
0.042

0.034
0.032
0.031
0.033

GB ↓

0.042
0.041
0.041
0.041

0.036
0.034
0.034
0.033

0.063
0.060
0.061
0.060

0.068
0.059
0.060
0.064

0.023
0.021
0.021
0.020

0.025
0.017
0.017
0.019

0.180
0.177
0.179
0.176

0.146
0.144
0.145
0.144

0.065
0.063
0.063
0.062

0.056
0.053
0.052
0.052

Chair Drums

Ficus Hotdog Lego Materials Mic

Ship Mean

PlenOctree-SH9
PlenOctree-SH16
PlenOctree-SH25
PlenOctree-SG25

0.45
0.83
1.30
1.03

0.67
1.24
1.97
1.68

1.15
1.79
2.57
2.43

1.27
2.67
3.80
2.66

1.16
2.07
3.61
2.66

1.48
3.68
4.04
4.44

0.16
0.44
0.55
0.49

1.67
2.69
3.61
2.71

1.00
1.93
2.68
2.26

FPS ↑

Chair Drums

Ficus Hotdog

Lego Materials Mic

Ship Mean

PlenOctree-SH9
PlenOctree-SH16
PlenOctree-SH25
PlenOctree-SG25

521.1
352.4
269.2
306.6

255.6
175.9
126.7
151.9

116.7
85.6
67.0
74.1

183.0
95.5
66.4
104.3

275.1
186.8
127.1
153.3

132.3
64.2
48.9
51.0

519.4
324.9
279.2
294.2

90.6
56.0
41.3
69.6

261.7
167.7
128.2
150.6

Table 8: Per-scene quantitive results on spherical basis function ablations.

