2
2
0
2

l
u
J

6
2

]

V
C
.
s
c
[

1
v
4
2
8
2
1
.
7
0
2
2
:
v
i
X
r
a

Compositional Human-Scene Interaction
Synthesis with Semantic Control

Kaifeng Zhao1, Shaofei Wang1, Yan Zhang1, Thabo Beeler2, and Siyu Tang1

1 ETH Z¨urich
{kaifeng.zhao, shaofei.wang, yan.zhang, siyu.tang}@inf.ethz.ch
2 Google
thabo.beeler@gmail.com

Abstract. Synthesizing natural interactions between virtual humans
and their 3D environments is critical for numerous applications, such as
computer games and AR/VR experiences. Recent methods mainly focus
on modeling geometric relations between 3D environments and humans,
where the high-level semantics of the human-scene interaction has fre-
quently been ignored. Our goal is to synthesize humans interacting with
a given 3D scene controlled by high-level semantic specifications as pairs
of action categories and object instances, e.g., “sit on the chair”. The
key challenge of incorporating interaction semantics into the generation
framework is to learn a joint representation that effectively captures het-
erogeneous information, including human body articulation, 3D object
geometry, and the intent of the interaction. To address this challenge, we
design a novel transformer-based generative model, in which the articu-
lated 3D human body surface points and 3D objects are jointly encoded
in a unified latent space, and the semantics of the interaction between the
human and objects are embedded via positional encoding. Furthermore,
inspired by the compositional nature of interactions that humans can
simultaneously interact with multiple objects, we define interaction se-
mantics as the composition of varying numbers of atomic action-object
pairs. Our proposed generative model can naturally incorporate vary-
ing numbers of atomic interactions, which enables synthesizing composi-
tional human-scene interactions without requiring composite interaction
data. We extend the PROX dataset with interaction semantic labels and
scene instance segmentation to evaluate our method and demonstrate
that our method can generate realistic human-scene interactions with se-
mantic control. Our perceptual study shows that our synthesized virtual
humans can naturally interact with 3D scenes, considerably outperform-
ing existing methods. We name our method COINS, for COmpositional
INteraction Synthesis with Semantic Control. Code and data are avail-
able at https://github.com/zkf1997/COINS.

Keywords: human-scene interaction synthesis, semantic composition,
virtual humans.

 
 
 
 
 
 
2

K. Zhao et al.

Fig. 1: Given a pair of action and object instance as the semantic specification,
our method generates virtual humans naturally interacting with the object (first
row). Furthermore, our method retargets interactions on unseen action-object
combinations (second row) and synthesizes composite interactions without re-
quiring any corresponding composite training data (third row).

1

Introduction

People constantly interact with their surroundings, and such interactions have
semantics, specifically as combinations of actions and object instances, e.g., “sit
on the chair” or “lie on the bed”. Incorporating and controlling such interac-
tion semantics is critical for creating virtual humans with realistic behavior. An
effective solution for semantic-aware virtual human generation could advance ex-
isting technologies in AR/VR, computer games, and synthetic data generation
to train machine learning perception algorithms. However, existing methods fo-
cus on modeling geometric relationships between virtual humans and their 3D
environments [17, 56, 59], and thus are not able to synthesize body-scene inter-
actions with semantic control of human actions and specific interaction objects.
This heavily limits their use in practice.

In this paper, we aim to incorporate semantic control into the synthesis of
human-scene interactions. This is a challenging task due to the following reasons.
First, semantic-aware human-scene interactions are characterized by body artic-

“Lie on the sofa”“Lie on the chair”“Sit on the table”“Touch the chair”“Sit on the floor”“Touch the bed”“Sit on the cabinet and touch the bed”“Stand on floor and touch the monitor”“Stand on floor and touch the table”Compositional Human-Scene Interaction Synthesis with Semantic Control

3

ulation, 3D object geometry, and the intent of the interaction. It is challenging to
learn a generalizable representation to capture such heterogeneous information.
Second, given a 3D scene, the space for plausible human-scene interactions is
enormous, and obtaining sufficiently diverse training data remains challenging.
To address these challenges, we introduce a novel interaction synthesis ap-
proach that leverages transformer-based generative models and can synthesize
realistic 3D human bodies, given a 3D scene and a varying number of action-
object pairs. These pairs can specify the intent of the interactions and the inter-
acting objects. The key advantages of our models are two-fold: First, we repre-
sent human body articulation and 3D object geometry as tokens in the proposed
transformer network, and the interaction semantics are embedded via positional
encoding. Combined with a conditional variational auto-encoder (VAE) [22, 43],
we learn a unified latent space that captures the distribution of human articula-
tion conditioned on the given 3D objects and the interaction semantics. Second,
our models can be used to synthesize composite interactions, as they can natu-
rally incorporate a varying number of 3D objects and interaction semantics. By
training only on irreducible atomic interactions (e.g., “sit on the sofa”, “touch
the table”), our model can generate novel composite interactions (e.g., “sit on
the sofa and touch the table”) in unseen 3D scenes without requiring compos-
ite training data (Fig. 1). Furthermore, we decompose the interaction synthesis
task into three stages. First, we infer plausible global body locations and ori-
entations given pairs of actions and objects. Second, we generate detailed body
articulation and body-scene contact that aligns with the object geometry and
interaction semantics. Lastly, we further refine the local body-scene contact by
leveraging the inferred contact map.

To train and evaluate our method, we extend the PROX dataset [16] to
the PROX-S dataset, which includes 3D instance segmentation, SMPL-X body
estimation, and per-frame annotation of interaction semantics. Since there are no
existing methods working on the same task of populating scenes with semantic
control, we adapt PiGraph [41] and POSA [17] to this new task, and train their
modified versions on the PROX-S dataset as baselines. Our perceptual study
shows that our proposed method can generate realistic 3D human bodies in a
3D scene with semantic control, significantly outperforming those baselines.

In summary, our contributions are: (1) a novel compositional representation
for interaction with semantics; (2) a generative model for synthesizing diverse
and realistic human-scene interactions from semantic specifications; (3) a frame-
work for composing atomic interactions to generate composite interactions; and
(4) the extended human-scene interaction dataset PROX-S containing scene in-
stance segmentation and per-frame interaction semantic annotation.

2 Related Work

Human-Scene Interaction Synthesis. Synthesizing human-scene interactions
has been a challenging problem in the computer vision and graphics community.
Various methods have been proposed to analyze object affordances and generate

4

K. Zhao et al.

static human-scene interactions [12, 14, 18, 21, 24, 40, 41]. However, the realism
of synthesized interactions from these methods is often limited by their over-
simplified human body representations. The recently published PSI [59] uses
the parametric body model SMPL-X [33] to put 3D human bodies into scenes,
conditioned on scene semantic segmentation and depth map. In addition, geo-
metric constraints are applied to resolve human-scene contact and penetration
problems. PLACE [56] learns the human-scene proximity and contact in inter-
actions to infer physically plausible humans in 3D scenes. POSA [17] proposes
an egocentric contact feature map that encodes per-vertex contact distance and
semantics information. A stochastic model is trained to predict contact feature
maps given body meshes, with the predicted contact features guiding the body
placement optimization in scenes. Wang et al. [50] generate human bodies at
specified locations and orientations, conditioning on the point cloud of the scene
and implicitly modeling the scene affordance. Concurrent work [51] first gener-
ates scene-agnostic body poses from actions and then places body into scene
using POSA, which is similar to the POSA-I baseline we implement.

Apart from being realistic, semantic control is another important goal in
synthesizing interactions. PiGraph [41] learns a probabilistic distribution for
each annotated interaction category and synthesizes interactions by selecting
the highest scoring samples of fitted categorical distribution. Conditional action
synthesis methods [1, 2, 34] generate human motion conditioned on action cat-
egories or text descriptions while ignoring human-scene interaction. NSM [44]
uses a mixture of experts modulated by the action category to generate human
motion of seven predefined categories. SAMP [15] extends NSM by incorporating
stochastic motion modeling and a GoalNet predicting possible interaction goals
given scene objects and action categories. NSM and SAMP are limited in inter-
action semantics modeling since they can only model interactions with one single
object or the surrounding environment as a whole. Our work, in contrast, models
the compositional nature of interaction semantics and can generate composite
interactions involving multiple objects simultaneously.

Human Behavior Semantics Modelling. Understanding human behavior
is an important problem that has long received a significant amount of attention.
Action recognition [19,20,42,47] classifies human activities into predefined action
categories. Language driven pose and motion generation [1, 2, 45] incorporates
natural languages that can represent more expressive and fine-grained action
semantics. Human-object interaction detection [11,13,53] models interactions as
triplets of (human, action, object), detecting humans and objects from images
along with the relations between them.

One key observation is that humans can perform multiple actions simul-
taneously and such behaviors have composite semantics. Composite semantics
are often modeled as the combination of basic semantic units, like how phrases
are composed by words in natural languages. Semantic composition is termed
as compound in linguistics [25, 35] and studied in natural language process-
ing [29, 30, 54]. PiGraph [41] describes composite human-scene interaction se-
mantics with a set of (verb, noun) pairs. BABEL [36] uses natural language

Compositional Human-Scene Interaction Synthesis with Semantic Control

5

labels to annotate human motion capture data [3, 26, 27, 46, 48] and allow multi-
ple descriptions existing in a single frame to model composite semantics. These
works model semantics on the object category level and ignore the differences
among instances of the same category. Our work goes beyond this limitation to
model interaction semantics with specific object instances, which is critical for
solving ambiguity in scenes with multiple objects of the same category.

3 Method

3.1 Preliminaries

Body Representation. We represent the 3D human body using a simplified
SMPL-X model, consisting of vertex locations, triangle faces, and auxiliary
per-vertex binary contact features, denoted as B = (V, F, C). For computa-
tional efficiency, we downsample the original SMPl-X mesh to have 655 vertices
V ∈ R655×3 and 1296 faces F ∈ {1 · · · 655}1296×3 using the sampling method
of [38]. The binary contact features C ∈ {0, 1}655 indicate whether each body
vertex is in contact with scene objects [17]. Specifically, we consider a body vertex
is in contact if its distance to scene objects is below a threshold value. We in-
troduce the auxiliary contact features because human-object contact is a strong
cue for interaction semantics, e.g., “touch” implies contact on hands. We use the
contact features to further refine human-scene interaction in post-optimization.

3D Scene Representation. We represent a 3D scene as a set of object instances,
denoted as S = {oi}O
i=1 where O is the number of object instances. Each object
instance oi ∈ R8192×9 is an oriented point cloud of 8192 points with attributes of
location, color, and normal. We represent objects as point clouds to incorporate
the geometry information and enable generalization to open-set objects that do
not belong to predefined object categories, unlike previous works [17,41,59] that
only use object category information. Moreover, the effect of intra-class object
variance on interactions can be learned from the object geometry to improve
fine-grained human-scene contacts.

Compositional Interaction Representation. Human-scene interactions involve three
types of components: human bodies, actions, and objects. Inspired by the ob-
servation that humans can perform multiple interactions at the same time, we
propose a compositional interaction representation I = (B, {(ai, oi)}M
i=1), where
B denotes the human body, and {(ai, oi)}M
i=1 is a set of atomic interactions. We
use the term atomic interaction to refer to the basic, irreducible building block
for interaction semantics, and each atomic interaction (ai, oi) is a pair of a one-
hot encoded action and an object instance, e.g., “sit on a chair”. In addition,
interactions can comprise more than one such atomic interaction unit, since one
can simultaneously interact with multiple objects, e.g., “sit on a chair and type
on a keyboard”. We refer to such interactions consisting of more than one atomic
interaction as composite interactions.

6

K. Zhao et al.

Fig. 2: Illustration of our COINS method for human-scene interaction synthesis
with semantic control. Given a 3D scene and semantic specifications of actions
(e.g., “sit on”, “touch”) paired with object instances (highlighted with color),
COINS first generates a plausible human pelvis location and orientation and
then generates the detailed body.

3.2 Interaction Synthesis with Semantic Control

Synthesizing interactions with semantic control requires generating human bod-
ies interacting with given scene objects in a natural and semantically correct way.
Specifically, given a scene S and interaction semantics specified as {(ai, oi)}M
i=1,
the task is to generate a human body B that perceptually satisfies the constraints
of performing action ai with object oi.

We propose a multi-stage method for interaction synthesis with semantic con-
trol as illustrated in Fig. 2. Given interaction semantics specified as action-object
pairs, we first infer possible pelvis locations and orientations.Then we transform
objects to the generated pelvis coordinate frame and sample body and contact
map in the local space. We separately train two transformer-based conditional
VAE (cVAE) [22, 43] models for pelvis and body generation, and name them
as PelvisVAE and BodyVAE respectively. Lastly, we apply an interaction-based
optimization that is guided by contact features to refine the generated interac-
tions.

Architecture. We propose a transformer-based conditional variational auto-encoder
(cVAE) architecture that can handle the heterogeneous inputs of our composi-
tional interaction representation containing body mesh vertices, object points
without predefined topology, and interaction semantics. Specifically, object points
and body vertices are represented as token inputs to the transformer, and learn-
able action embeddings are added to paired object tokens as positional encoding.
The BodyVAE architecture is illustrated in Fig. 3. The PelvisVAE shares a

similar architecture and we refer readers to the Supp. Mat. for illustration.

(“Sit on”,           )“Sit on the chair and touch the table”PelvisGenerationBodyGeneration(“Touch”,           )Compositional Human-Scene Interaction Synthesis with Semantic Control

7

Fig. 3: Illustration of BodyVAE architecture. Human body and objects are rep-
resented as tokens of the transformers while each action category is mapped to
a learnable embedding and added to the corresponding object tokens. Modules
within the dashed box are the semantic conditions of a varying number of action-
object pairs. We denote addition with (cid:76) and positional encoding with “PE”.
We visualize vertices with positive contact features as blue on the body mesh.

Our cVAE networks take varying numbers of action-object pairs as condi-
tional inputs. Each action category is mapped to a learnable embedding, and
each object point cloud is first converted to 256 local keypoints using Point-
Net++ [37]. The action embedding is then added to the tokens of the paired ob-
ject as positional encoding. Compared to treating actions as separate tokens [34],
encoding actions into the positional encoding for object tokens can regulate each
action to only affect the paired object, e.g., the action “sit on” is only related
to the sofa, but not the table, in the example of Fig. 3. The human body input
comprises mesh vertices with location and auxiliary contact features. We apply a
linear layer to map vertices to tokens and vertex index based positional encoding
to encode body topology.

The transformer encoder maps input interactions into a latent space and the
decoder predicts human bodies with contact features from conditional action-
object pairs and the sampled latent code. On top of the encoder transformer
network, we apply average pooling on the output human tokens followed by a
linear layer to predict the latent normal distribution. At the decoder, we use a
template body with person-dependent shape parameters for body tokens. Using
the personalized template body makes our model generalize to different body
shapes and enables explicit control of the body shape. The sampled latent code
is used for keys and values of the encoder-decoder attention layers [49].

BodyVAE shares the same transformer-based architecture and the same con-
ditional input of action-object pairs as PelvisVAE. There are mainly two differ-
ences between BodyVAE and PelvisVAE. First, BodyVAE represents a human
as a body mesh with contact features, while PelvisVAE represents a human as
the pelvis joint location and orientation. Second, all input objects are in the

Encoderμ, ΣBody128DecoderPointNet++“Touch”Semantic Conditions“Sit on”PointNet++...PESelf Attention Feed Forward 128DTemplate4x128DBody128D128D128D4xPESampling......Feed Forward Self Attention Encoder-DecoderAttention 8

K. Zhao et al.

scene coordinate frame in PelvisVAE while input objects are transformed to the
local pelvis coordinate frame in BodyVAE.

To make our sampled bodies compatible with the SMPL-X body model and
ensure a valid body shape, we jointly train a Multi-Layer Perceptron (MLP)
[39] with the BodyVAE to regress the corresponding SMPL-X body parameters
from the sampled body meshes. We refer to the Supp. Mat. for more details on
architecture and the training losses.

Interaction-Based Optimization. Our model is trained with the pseudo ground
truth body estimation [16, 55], which does not guarantee natural human-scene
contact. Therefore, the generated interactions can also be unnatural. We fol-
low [17,56,59] to apply interaction-based optimization to refine the interactions.
We use generated SMPL-X parameters as initialization and optimize body trans-
lation t, global orientation R and pose θ. The optimization objective is given by:

E(t, R, θ) = Linteraction + Lcoll + Lreg,

(1)

where Linteraction encourages body vertices with positive contact features to
have zero distance to objects, Lcoll is the scene SDF based collision term defined
in [59], and the Lreg term penalizes t, R and θ deviating from the initialization.
We refer to the Supp. Mat. for detailed definitions.

3.3 Compositional Interaction Generation

Compositional interaction generation aims to generate composite interactions
using models trained only on atomic interactions. Compositional interaction gen-
eration reflects the compositional nature of interaction semantics and can save
the need to collect composite interaction data. Although our transformer-based
generative model can synthesize bodies given composite interaction semantics,
e.g. “sit on the sofa and touch the wall”, it requires collecting data and training
for all composite interactions we want to synthesize. However, collecting com-
posite interaction data is exponentially more expensive than atomic interactions
considering the vast possible combinations of atomic interactions. Therefore, it is
desirable to have an automatic way to generate composite interactions requiring
only atomic interaction data.

To that end, we propose a simple yet effective two-step approach to gener-
ating composite interactions using models trained only on atomic interactions:
(1) Compositional pelvis generation, where we formulate the task of generating
pelvis frames given composite interaction semantics as finding the intersection of
pelvis distributions of atomic interactions. (2) Compositional body generation,
where we leverage attention masks derived from contact statistics in transformer
inference to compose atomic interactions at the body parts level.

Compositional Pelvis Generation. We formulate compositional pelvis generation
as finding the intersection of pelvis distributions of component atomic interac-
tions. Using our PelvisVAE trained for atomic interactions, we can decode the

Compositional Human-Scene Interaction Synthesis with Semantic Control

9

prior standard Gaussian distribution to distributions of plausible pelvis frames
given atomic interaction semantics. Each component atomic interaction of a com-
posite interaction determines a pelvis frame distribution in scenes, and intuitively
the plausible pelvis of the composite interaction should lie in the intersection of
all pelvis distributions. Therefore, we propose a compositional pelvis generation
method where we sample pelvis frames for each atomic interaction, optimize all
pelvis frames to be close in space, and average the optimized pelvis frames as
the generated composite pelvis frame.

Specifically, we denote the decoder of the PelvisVAE as a function G(zi|ai, oi)
mapping a latent code zi to a pelvis frame conditioned on the action-object pair
of (ai, oi), we obtain the final pelvis frame from the composition of M atomic
interactions from the following optimization problem:

˜Z = arg min
Z={zi}M

i=1

(cid:88)

(cid:88)

(cid:12)G(zi|ai, oi) − G(zj|aj, oj)(cid:12)
(cid:12)

(cid:12) − λ

(cid:88)

log(p(zi)),

1≤i<M

i<j≤M

1≤i≤M

(2)
Z denotes the set of M randomly sampled latent codes for M component
atomic interactions, and λ is a tunable weight. The initial values of the latent
codes are randomly sampled from the prior distribution. The motivation is to
optimize the latent codes of M component atomic interactions to make their
decoded pelvis frames as close as possible in the scene while maintaining high
probabilities in each component pelvis distribution. We take the average of the
M optimized component pelvis frames as the sampled composite pelvis frame.

Compositional Body Generation. For compositional body generation, we use the
attention mechanism of transformer networks to compose atomic interactions at
body part level. Although we use BodyVAE trained only on atomic interaction
data, we can input more than one atomic interaction and achieve compositional
body generation with additional attention masks derived from contact statistics.
Our method is inspired by the observation from contact probability statistics
that interaction semantics can often be determined by contact on certain body
parts instead of the whole body, e.g., sitting is characterized by contact around
the hip area. Specifically, we calculate the per-vertex contact probability for
each action using training data. The vertex contact probability for one action is
defined as the ratio of the number of positive contact features of the vertex and
the number of all contact features of the vertex in the interaction data of the
specified action. Heatmap visualization of such categorical contact statistics is
shown in Fig. 4. High contact probability indicates a high correlation between
the body vertex area and action semantics.

Inspired by the observed action-part correlation, we propose leveraging the
attention mechanism to compose atomic interactions according to three body
parts: upper limbs, lower limbs, and torso. Given composite interaction seman-
tics for body generation, we calculate the sum of contact probability at each part
for each action. If the sum of contact probability of one action at one body part
exceeds those of other actions by a threshold, we mask the attention between the

10

K. Zhao et al.

Fig. 4: Visualization of per-vertex contact probability for two actions from the
side and bottom views. Brighter colors indicate higher contact probability with
objects. We randomly sample a pose within the action category for visualization.

Fig. 5: Illustration of the PROX-S dataset. From left to right, it shows the
RGB image, 3D scene reconstruction with instance segmentation, and the fitted
SMPL-X human body with per-frame interaction semantic labels.

part and all other atomic interactions to be zero. Taking the composite interac-
tion “sit on chair and touch table” as an example, touching has a significantly
greater sum of contact probability on upper limbs so the attention between upper
limb tokens and chair (objects not paired with touching) tokens will be masked
as zero.

We input such attention masks and component atomic interactions to the
transformer for compositional body generation. The derived attention masks
can naturally compose the component atomic interactions via BodyVAE.

4 Experiments

4.1 Dataset

We annotate a new dataset called PROX-S on top of the PROX [16] dataset
to evaluate interaction synthesis with semantic control since there is no existing
dataset suitable for this task. The PROX-S dataset contains (1) 3D instance
segmentation of all 12 PROX scenes; (2) 3D human body reconstructions within
the scenes; and (3) per-frame interaction semantic labels in the form of action-
object pairs, as shown in Fig. 5. We refer to the Supp. Mat. for the detailed
process description and statistics.

“Lie on”-Side“Sit on”-Side“Lie on”-Bottom“Sit on”-BottomCompositional Human-Scene Interaction Synthesis with Semantic Control

11

4.2 Evaluation Metrics

Semantic Contact. Compared to the semantics-agnostic body-scene contact score
used in [59], we propose a semantic contact score to evaluate whether the gener-
ated interactions adhere to the semantic specifications characterized by proper
body-scene contact. Using the action-specific body vertex contact probability
introduced in Sec. 3.3 as weights, we calculate a weighted sum of binary contact
features for each body in the training data and name the sum as the semantic
contact score. This action-dependent semantic contact score reflects how body
contact features align with the semantics of one action. We use the 20 percentile
semantic contact score of training data bodies as the threshold score for each
action to filter low-quality training interaction data with inaccurate contact.
When evaluating a generated interaction sample, we calculate the weighted sum
of contact features and compare it to the action-specific threshold score. The
semantic contact score is one if the sum is higher than the threshold and zero
otherwise.

Physical Plausibility. We evaluate the physical plausibility of generated interac-
tions using the non-collision metric from [10, 17, 56, 59]. It is calculated as the
ratio of the number of body vertices with non-negative scene SDF values and
the number of all body vertices.

Diversity. Following [59], we perform K-Means clustering to cluster the SMPL-X
parameters of generated bodies into 50 clusters and evaluate the diversity using
the entropy of the cluster ID histogram and the mean distance to cluster centers.

Perceptual Study. We evaluate the perceptual realism and semantic accuracy of
generated human-scene interactions by conducting perceptual studies on Ama-
zon Mechanical Turk (AMT). We perform a binary-choice perceptual study
where samples of the same interaction semantics from different sources are shown
and the turkers are asked to choose the more natural one. For semantic accu-
racy, we instruct the turkers to rate the consistency between shown interaction
samples and semantic labels from 1 (strongly disagree) to 5 (strongly agree). For
more details regarding the perceptual studies, please refer to the Supp. Mat.

4.3 Baselines

There is no existing method that directly applies to the problem of generating 3D
human bodies given a 3D scene and interaction semantics. Therefore, we modify
two state-of-the-art interaction synthesis methods for our tasks and train their
models using the same PROX-S dataset.

PiGraph-X. PiGraph [41] is the most related work to our knowledges, which
simultaneously generates scene placement and human skeletons from interaction
category specifications. We remove the scene placement step of the original paper
and replace the skeleton body representation with the more realistic SMPL-X

12

K. Zhao et al.

Table 1: Binary Realism Perceptual Study.

(a) Interaction Synthesis with Seman-
tic Control.

% users rated as “more realistic”

Ours
63.8 %
Ours
79.7 %
Ours
45.3 %

POSA-I
36.2 %
PiGraph-X
20.3 %
PseudoGT
54.7 %

(b) Interaction Synthesis via Semantic
Composition.

% users rated as “more realistic”
Ours-C
81.2 %
Ours-C
42.4 %

PiGraph-X-C
18.2 %
PseudoGT
57.6 %

body model. We refer to this modified PiGraph variant as PiGraph-X. For com-
positional interaction generation, we extend PiGraph by sampling basic bodies
from all comprised atomic interactions and using the average similarity score to
those atomic interactions. We denote this PiGraph extension as PiGraph-X-C.

POSA-I. POSA [17] generates human-scene interactions using optimization guided
by per-vertex contact features. However, POSA lacks global control of the gen-
erated contact features and cannot generate human bodies conditioned on inter-
action semantics. To incorporate global semantics of human-scene interaction,
we modify POSA to generate human bodies with vertex-level contact features
from interaction semantics in the format of action-noun pairs. We denote the
semantic-controllable POSA variant as POSA-I.

4.4 Interaction Synthesis with Semantic Control

We evaluate the synthesized interactions in terms of perceptual realism, per-
ceptual semantic accuracy, semantic contact, physical plausibility, and diversity.
Table 1 and 2 show the results of the perceptual studies on binary realism and
other quantitative metrics, respectively. Our method significantly outperforms
the baselines in the binary realism study and even comes close to the body
poses reconstructed from the real human-scene interactions, which we consider
as the pseudo ground truth. Our method also achieves higher perceptual se-
mantic ratings than all the baselines. Quantitatively, our method achieves the
highest semantic contact score (0.93) which significantly outperforms the base-
lines. Our method performs comparably on physical plausibility as POSA-I and
significantly better than PiGraph-X. For diversity metrics, our method has a
larger cluster entropy and smaller cluster size compared to the baselines. We ob-
serve more frequent unnatural interaction samples from PiGraph-X and POSA-I,
which accounts for their larger cluster sizes. Note that our method is a generative
pipeline where we evaluate every randomly sampled result, while both baselines
need to search thousands of samples of global placement in scenes.

We show the qualitative comparison of interactions generated from ours and
two baselines in Fig. 6. PiGraph-X naively models the human body as indepen-
dent joints and therefore suffers from unnatural body poses, poor human-scene

Compositional Human-Scene Interaction Synthesis with Semantic Control

13

Table 2: Evaluation of semantics, physical plausibility and diversity.

Semantic Accuracy Semantic Contact Non-Collision Entropy Cluster Size Samples

PiGraph-X
POSA-I
Ours

3.90 ± 0.93
4.03 ± 0.85
4.17 ± 0.71

0.84
0.67
0.93

0.85
0.98
0.97

3.67
3.75
3.77

1.07
1.08
0.83

25K
> 1K
1

Fig. 6: Qualitative comparison of interactions generated with our method and
two baselines. Each row shows interactions generated from one method. From
left to right, the columns correspond to different interaction semantics of “lying
on the sofa”, “stepping up on the chair”, “sitting on the chair and touching the
table”, and “standing on the floor and touching the shelving”.

contact, and penetration. POSA-I generates bodies with physically plausible in-
teractions, but since it needs to find plausible body placement in the scene, its
optimization can easily get stuck at local minimums that are not semantically
accurate and have inferior human-scene contact. In contrast, our method learns
how humans interact with objects given different interaction semantics and gen-
erates realistic interactions that are both physically plausible and human-like.

4.5 Compositional Interaction Generation

We conduct the same evaluation for novel composite interaction generation as in-
troduced in Sec. 4.4. We compare our method to the PiGraph-X-C extension for
compositional interaction generation. POSA-I is not included in this part since
it can not be easily extended for semantic composition. In the forced-alternative-
choice (binary-choice) perceptual study (Tab. 1 (b)), our method significantly
outperforms the baseline. The semantic accuracy, semantic contact, non-collision
and diversity comparisons are shown in Tab. 3. Our method achieves significantly

Lie on sofaStep up chairSit on chair and touch tableStand on floor and touch shelvingPiGraph-XPOSA-IOurs14

K. Zhao et al.

Fig. 7: Qualitative comparison of composite interactions generated using ours-C
and PiGraph-X-C. Each row shows interactions generated from one method.

Table 3: Evaluation of semantics, physical plausibility and diversity for novel
compositional interaction generation.

Semantic Accuracy Semantic Contact Non-Collision Entropy Cluster Size Samples

PiGraph-X-C
Ours-C

3.88 ± 1.15
3.95 ± 1.18

0.80
0.76

0.84
0.98

3.72
3.80

1.09
0.68

25K
1

better semantic accuracy, physical plausibility, and sample efficiency. The slight
inferior semantic contact score is because the combinations of actions and objects
in the test scenes may not always afford natural contact, like touching an incom-
plete wall in Fig. 7 (d). Touching the wall and floor simultaneously generates
higher contact scores but perceptually unnatural poses to humans.

We refer to the Supp. Mat. for more qualitative results, failures, abla-

tion study, and limitations.

5 Conclusion

In this paper, we introduce a method to generate humans interacting with 3D
scenes given interaction semantics as action-object pairs. We propose a composi-
tional interaction representation and transformer-based VAE models to stochas-
tically generate human-scene interactions. We further explored generating com-
posite interactions by composing atomic interactions, which does not demand
training on composite interaction data. Our method is a step towards creating
virtual avatars interacting with 3D scenes.
Acknowledgements: We sincerely acknowledge the anonymous reviewers for
their insightful suggestions. We thank Francis Engelmann for help with scene
segmentation and proofreading, and Siwei Zhang for providing body fitting re-
sults. This work was supported by the SNF grant 200021 204840.

(a) Sit on chair and touch table(b) Sit on sofa and touch table(c) Stand on floor and touch monitor(d) Stand on floor and touch wallPiGraph-X-COurs-CCompositional Human-Scene Interaction Synthesis with Semantic Control

15

References

1. Ahn, H., Ha, T., Choi, Y., Yoo, H., Oh, S.: Text2action: Generative adversarial

synthesis from language to action. In: Proc. of ICRA. IEEE (2018)

2. Ahuja, C., Morency, L.P.: Language2pose: Natural language grounded pose fore-

casting. In: Proc. of 3DV. IEEE (2019)

3. Akhter, I., Black, M.J.: Pose-conditioned joint angle limits for 3d human pose

reconstruction. In: Proc. of CVPR (2015)

4. Bowman, S.R., Vilnis, L., Vinyals, O., Dai, A.M., Jozefowicz, R., Bengio, S.: Gen-
erating sentences from a continuous space. arXiv preprint arXiv:1511.06349 (2015)
5. Charbonnier, P., Blanc-Feraud, L., Aubert, G., Barlaud, M.: Two deterministic
half-quadratic regularization algorithms for computed imaging. In: Proc. of ICIP.
vol. 2. IEEE (1994)

6. Chen, S., Fang, J., Zhang, Q., Liu, W., Wang, X.: Hierarchical aggregation for 3d

instance segmentation. In: Proc. of ICCV (2021)

7. Choi, H., Moon, G., Lee, K.M.: Pose2mesh: Graph convolutional network for 3d
human pose and mesh recovery from a 2d human pose. In: Proc. of ECCV (2020)
8. Diederik, K., Jimmy, B., et al.: Adam: A method for stochastic optimization. arXiv

preprint arXiv:1412.6980 (2014)

9. Dutta, A., Zisserman, A.: The VIA annotation software for images, audio
and video. In: Proc. of MM. MM ’19, ACM, New York, NY, USA (2019).
https://doi.org/10.1145/3343031.3350535

10. Engelmann, F., Rematas, K., Leibe, B., Ferrari, V.: From Points to Multi-Object

3D Reconstruction. In: Proc. of CVPR (2021)

11. Gkioxari, G., Girshick, R., Doll´ar, P., He, K.: Detecting and recognizing human-

object interactions. In: Proc. of CVPR (2018)

12. Grabner, H., Gall, J., Van Gool, L.: What makes a chair a chair? In: Proc. of

CVPR. IEEE (2011)

13. Gupta, A., Kembhavi, A., Davis, L.S.: Observing human-object interactions: Using
spatial and functional compatibility for recognition. IEEE Transactions on Pattern
Analysis and Machine Intelligence 31, 1775–1789 (2009)

14. Gupta, A., Satkin, S., Efros, A.A., Hebert, M.: From 3d scene geometry to human

workspace. In: Proc. of CVPR. IEEE (2011)

15. Hassan, M., Ceylan, D., Villegas, R., Saito, J., Yang, J., Zhou, Y., Black, M.J.:

Stochastic scene-aware motion prediction. In: Proc. of ICCV (2021)

16. Hassan, M., Choutas, V., Tzionas, D., Black, M.J.: Resolving 3D human pose

ambiguities with 3D scene constraints. In: Proc. of ICCV (2019)

17. Hassan, M., Ghosh, P., Tesch, J., Tzionas, D., Black, M.J.: Populating 3D scenes

by learning human-scene interaction. In: Proc. of CVPR (2021)

18. Hu, R., Yan, Z., Zhang, J., Van Kaick, O., Shamir, A., Zhang, H., Huang, H.:
Predictive and generative neural networks for object functionality. arXiv preprint
arXiv:2006.15520 (2020)

19. Ji, S., Xu, W., Yang, M., Yu, K.: 3d convolutional neural networks for human action
recognition. IEEE transactions on pattern analysis and machine intelligence 35(1),
221–231 (2012)

20. Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan, S.,
Viola, F., Green, T., Back, T., Natsev, P., et al.: The kinetics human action video
dataset. arXiv preprint arXiv:1705.06950 (2017)

21. Kim, V.G., Chaudhuri, S., Guibas, L.J., Funkhouser, T.: Shape2pose: human-
centric shape analysis. In: ACM Transactions on Graphics, (Proc. SIGGRAPH)
(2014)

16

K. Zhao et al.

22. Kingma, D.P., Welling, M.: Auto-encoding variational bayes. arXiv preprint

arXiv:1312.6114 (2013)

23. Kolotouros, N., Pavlakos, G., Daniilidis, K.: Convolutional mesh regression for

single-image human shape reconstruction. In: Proc. of CVPR (2019)

24. Li, X., Liu, S., Kim, K., Wang, X., Yang, M.H., Kautz, J.: Putting humans in a
scene: Learning affordance in 3d indoor environments. In: Proc. of CVPR (2019)

25. Lieber, R., Stekauer, P.: The oxford handbook of compounding (2011)
26. Mahmood, N., Ghorbani, N., Troje, N.F., Pons-Moll, G., Black, M.J.: AMASS:

Archive of motion capture as surface shapes. In: Proc. of ICCV (2019)

27. Mandery, C., Terlemez, O., Do, M., Vahrenkamp, N., Asfour, T.: The kit whole-

body human motion database. In: Proc. of ICAR (2015)

28. Mihajlovic, M., Saito, S., Bansal, A., Zollhoefer, M., Tang, S.: COAP: Composi-

tional articulated occupancy of people. In: Proc. of CVPR (2022)

29. Mineshima, K., Mart´ınez-G´omez, P., Miyao, Y., Bekki, D.: Higher-order logical
inference with compositional semantics. In: Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Processing (2015)

30. Mitchell, J., Lapata, M.: Vector-based models of semantic composition. In: Proc.

of ACL (2008)

31. Nekrasov, A., Schult, J., Litany, O., Leibe, B., Engelmann, F.: Mix3D: Out-of-

Context Data Augmentation for 3D Scenes. In: Proc. of 3DV (2021)

32. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
T., Lin, Z., Gimelshein, N., Antiga, L., et al.: Pytorch: An imperative style, high-
performance deep learning library. Proc. of NeurIPS (2019)

33. Pavlakos, G., Choutas, V., Ghorbani, N., Bolkart, T., Osman, A.A.A., Tzionas,
D., Black, M.J.: Expressive body capture: 3d hands, face, and body from a single
image. In: Proc. of CVPR (2019)

34. Petrovich, M., Black, M.J., Varol, G.: Action-conditioned 3d human motion syn-

thesis with transformer vae. In: Proc. of ICCV (2021)

35. Plag, I.: Word-formation in English. Cambridge University Press (2018)
36. Punnakkal, A.R., Chandrasekaran, A., Athanasiou, N., Quiros-Ramirez, A., Black,
M.J.: BABEL: Bodies, action and behavior with english labels. In: Proc. of CVPR
(2021)

37. Qi, C.R., Yi, L., Su, H., Guibas, L.J.: Pointnet++: Deep hierarchical feature learn-
ing on point sets in a metric space. arXiv preprint arXiv:1706.02413 (2017)
38. Ranjan, A., Bolkart, T., Sanyal, S., Black, M.J.: Generating 3D faces using convo-

lutional mesh autoencoders. In: Proc. of ECCV (2018)

39. Rumelhart, D.E., Hinton, G.E., Williams, R.J.: Learning internal representations
by error propagation. Tech. rep., California Univ San Diego La Jolla Inst for Cog-
nitive Science (1985)

40. Savva, M., Chang, A.X., Hanrahan, P., Fisher, M., Nießner, M.: Scenegrok: In-
ferring action maps in 3d environments. ACM transactions on graphics (TOG),
(Proc. SIGGRAPH) 33(6), 1–10 (2014)

41. Savva, M., Chang, A.X., Hanrahan, P., Fisher, M., Nießner, M.: PiGraphs: Learn-
ing Interaction Snapshots from Observations. ACM Transactions on Graphics,
(Proc. SIGGRAPH) 35(4) (2016)

42. Simonyan, K., Zisserman, A.: Two-stream convolutional networks for action recog-

nition in videos. Proc. of NeurIPS (2014)

43. Sohn, K., Lee, H., Yan, X.: Learning structured output representation using deep

conditional generative models. Proc. of NeurIPS (2015)

44. Starke, S., Zhang, H., Komura, T., Saito, J.: Neural state machine for character-
scene interactions. In: ACM Trans. Graph.(ACM SIGGRAPH Asia) (2019)

Compositional Human-Scene Interaction Synthesis with Semantic Control

17

45. Tevet, G., Gordon, B., Hertz, A., Bermano, A.H., Cohen-Or, D.: Motionclip: Ex-
posing human motion generation to clip space. arXiv preprint arXiv:2203.08063
(2022)

46. De la Torre, F., Hodgins, J., Bargteil, A., Martin, X., Macey, J., Collado, A.,
Beltran, P.: Guide to the carnegie mellon university multimodal activity (cmu-
mmac) database (2009)

47. Tran, D., Wang, H., Torresani, L., Ray, J., LeCun, Y., Paluri, M.: A closer look at
spatiotemporal convolutions for action recognition. In: Proc. of CVPR (2018)
48. Troje, N.F.: Decomposing biological motion: A framework for analysis and synthe-

sis of human gait patterns. Journal of Vision 2, 371–387 (2002)

49. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,

(cid:32)L., Polosukhin, I.: Attention is all you need. Proc. of NeurIPS (2017)

50. Wang, J., Xu, H., Xu, J., Liu, S., Wang, X.: Synthesizing long-term 3d human

motion and interaction in 3d scenes. In: Proc. of CVPR (2021)

51. Wang, J., Rong, Y., Liu, J., Yan, S., Lin, D., Dai, B.: Towards diverse and natural

scene-aware 3d human motion synthesis. In: Proc. of CVPR (2022)

52. Wang, N., Zhang, Y., Li, Z., Fu, Y., Liu, W., Jiang, Y.G.: Pixel2mesh: Generating

3d mesh models from single rgb images. In: Proc. of ECCV (2018)

53. Yao, B., Fei-Fei, L.: Modeling mutual context of object and human pose in human-

object interaction activities. In: Proc. of CVPR (2010)

54. Yin, D., Meng, T., Chang, K.W.: Sentibert: A transferable transformer-based ar-
chitecture for compositional sentiment semantics. arXiv preprint arXiv:2005.04114
(2020)

55. Zhang, S., Zhang, Y., Bogo, F., Marc, P., Tang, S.: Learning motion priors for 4d

human body capture in 3d scenes. In: Proc. of ICCV (2021)

56. Zhang, S., Zhang, Y., Ma, Q., Black, M.J., Tang, S.: PLACE: Proximity learning

of articulation and contact in 3D environments. In: Proc. of 3DV (2020)

57. Zhang, Y., Black, M.J., Tang, S.: Perpetual motion: Generating unbounded human

motion. arXiv preprint arXiv:2007.13886 (2020)

58. Zhang, Y., Black, M.J., Tang, S.: We are more than our joints: Predicting how 3d

bodies move. In: Proc. of CVPR (2021)

59. Zhang, Y., Hassan, M., Neumann, H., Black, M.J., Tang, S.: Generating 3d people

in scenes without people. In: Proc. of CVPR (2020)

60. Zhou, Y., Barnes, C., Lu, J., Yang, J., Li, H.: On the continuity of rotation repre-

sentations in neural networks. In: Proc. of CVPR (2019)

Compositional Human-Scene Interaction
Synthesis with Semantic Control

Kaifeng Zhao1, Shaofei Wang1, Yan Zhang1, Thabo Beeler2, and Siyu Tang1

1 ETH Z¨urich
{kaifeng.zhao, shaofei.wang, yan.zhang, siyu.tang}@inf.ethz.ch
2 Google
thabo.beeler@gmail.com

In the supplementary, we first provide method details, including architecture
illustrations, training losses, and compositional body generation in Sec. A. We
then elaborate on experiment details in Sec. B. In Sec. C, we present ablation
studies on different body representations and the two-stage generation frame-
work. In Sec. D, we show more qualitative results and discuss typical failure
cases and limitations.

A Method Details

A.1 Architecture Details

PelvisVAE. We illustrate the detailed architecture of PelvisVAE in Fig. S1.
The PelvisVAE encoder and decoder use a stack of 2 transformer layers with an
embedding dimension of 64. PelvisVAE represents a human as a pelvis frame of
location and orientation. At the decoder, PelvisVAE takes a zero vector as the
body token.

PointNet++. We train a PointNet++ [37] network to extract sparse key points
from point clouds, which reduces the number of object nodes to a suitable level
for transformers. We show the architecture of the PointNet++ object encoder
in Fig. S2. The PointNet++ module comprises two set abstraction layers to
extract 256 key points for each object. The output points features are projected
to vectors of dimension 128 for BodyVAE and dimension 64 for PelvisVAE using
a linear layer.

SMPL-X Regressor We train a multi-layer perceptron (MLP) to regress
SMPL-X parameters from body meshes as shown in Fig. S3. The MLP has six lin-
ear layers, and we employ residual connections. The MLP outputs the SMPL-X
body poses using the 6D continuous representation [60], body shape parameters,
and the first six hand pose PCA components for each hand. Following [23], We
detach the gradients of the reconstructed body from the computational graph
and use the concatenation of the reconstructed body and a template T-pose
body as inputs to the MLP.

Compositional Human-Scene Interaction Synthesis with Semantic Control

19

Fig. S1: Detailed architecture of PelvisVAE.

Fig. S2: The architecture of the PointNet++ object encoder, where D denotes
the embedding dimension used by transformers.

A.2 Training Loss

Our BodyVAE is trained to minimize the Lbody loss:

Lbody = Linteraction + Lmesh + Lf eature + LKL + Lregress,

(1)

where the terms represent the interaction loss, body mesh reconstruction loss,
contact feature reconstruction loss, the Kullback-Leibler divergence, and the
SMPL-X regression loss. Weights for each term are omitted for simplicity. The
term Linteraction encourages the vertices with predicted positive contact feature
to have zero distance to the input objects:

Linteraction =

655
(cid:88)

i=1

ˆci · min
vo∈Vo

||ˆvi − vo||2,

(2)

where ˆci and ˆvi denote the predicted contact feature and location of body vertex

i respectively, and Vo =

objects.

M
(cid:83)
i=1

oi denotes the set of all points of input interaction

The body mesh reconstruction loss consists of the vertex coordinate loss
Lvertex, the surface normal loss Lnormal, the edge length loss Ledge following

Encoderμ, Σ128DecoderPointNet++“Touch”Semantic Conditions“Sit on”PointNet++...Self Attention Feed Forward 128D4x128D128D128D128D4xSampling......Feed Forward Self Attention Encoder-DecoderAttention PelvisZeroPelvisSet AbstractionSet Abstraction(8192, 3+6)(1024, 3+96)(256, 3+256)Linear(256, D)20

K. Zhao et al.

Fig. S3: The architecture of the SMPL-X regressor.

[7, 52], and the normal consistency loss Lconsistency that regularizes the normal
of adjacent faces to change smoothly, which helps in particular with details of
the hands. The loss terms are defined by:

Lmesh = Lvertex + Lnormal + Ledge + Lconsistency.

Lvertex = || ˆV − V ||1,

Lnormal =

(cid:88)

(cid:88)

f ∈F

(i,j)∈f

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

⟨nf ,

ˆvi − ˆvj
|| ˆvi − ˆvj||2

(cid:12)
(cid:12)
(cid:12)
⟩
(cid:12)
(cid:12)

,

Ledge =

(cid:88)

(cid:88)

f ∈F

(i,j)∈f

(cid:12)
(cid:12)
(cid:12)
1 −
(cid:12)
(cid:12)

|| ˆvi − ˆvj||2
||vi − vj||2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

Lconsistency =

(cid:88)

1 − ⟨ˆnf i , ˆnf j ⟩,

f i,f j ∈F,f i∩f j ̸=∅

(3)

(4)

(5)

(6)

(7)

where V , F denote the vertices and faces of input body mesh, nf denotes the
normal of triangle f ∈ F and ˆ(·) denotes the corresponding reconstructions.

The contact feature reconstruction loss is calculated as the binary cross-
entropy loss (BCE) between reconstructed ˆC and ground truth C contact fea-
tures:

Lf eature = BCE( ˆC, C).

(8)

We use the robust Kullback-Leibler divergence (KL) [57, 58] to avoid posterior
collapse:

LKL = Ψ (KL(q(z|I)||N (0, I))),

(9)

1 + s2 − 1. The SMPL-X
where Ψ is the Charbonnier function [5] Ψ (s) =
parameter regression loss Lregress comprises parameter error and vertex error:

√

Lregress = ||θ − ˆθ||2 + |M(θ) − M(ˆθ)|,

(10)

LinearLinearLinearLinearLinearLinear10241024102415439301024BodySMPL-X1024Compositional Human-Scene Interaction Synthesis with Semantic Control

21

where θ and ˆθ are GT and predicted SMPL-X body parameters respectively, and
M denotes the SMPL-X model mapping parameters to mesh vertices.

The PelvisVAE is trained with the following losses:

Lpelvis = Ltransl + Lorient + LKL,

(11)

where Ltransl and Lorient denote the reconstruction loss of pelvis joint location
and orientation, and LKL is the robust Kullback-Leibler divergence loss from
Eq. (9).

A.3 Interaction-Based Optimization.

We use the sampled SMPL-X parameters as initialization and optimize body
translation t, global orientation R and pose θ. The optimization objective is
given by:

E(t, R, θ) = Linteraction + Lcoll + Lreg,

(12)

The interaction term Linteraction is defined in Eq. (2). The scene collision term
is defined as Lcoll = (cid:80)655
i=1 Ψ (vi), with Ψ (vi) denoting the signed distance of
vertex i to the scene. For computational efficiency, we use a precomputed SDF
grid for each scene and use interpolation to get SDF value for body vertices. The
regularization term Lreg = |t − tinit| + |R − Rinit| + ||θ − θinit||2 penalizes t, R
and θ deviating from their initialization.

A.4 Implementation Details

Our implementation is based on PyTorch [32]. We use the Adam optimizer [8]
with the learning rate 3e-4 and batch size of 8 for training all models. For
PelvisVAE, we use weights of 3, 1, 1 for {Ltransl, Lorient, LKL}. For Body-
VAE, we use weights of 1, 1, 0.1, 0.2, 0.05 for {Linteraction, Lvertex, Lnormal,
Ledge, Lconsistency}. We use weight 1 for LKL and apply the weight annealing
scheme [4]. For SMPL-X regressor, we use weights of 1 for vertex and body pose
reconstruction and 0.1 for shape parameter and hand PCA components recon-
struction. our contact features used a threshold object distance of 5cm. We use
latent dimensions of 6 and 128 for PelvisVAE and BodyVAE, respectively.

For the interaction-based optimization, we respectively use weights of 1 for
interaction term Linteraction, 32 for collision term Lcoll, 0.1 for translation and
orientation regularization and 32 for pose regularization.

For composite pelvis sampling, we use the Adam optimizer with a learning
rate of 0.1 and 100 optimization steps. We scale the sum of log probability of
latent codes with the weight of 0.05 to balance the influence of pelvis frame
difference and probability.

22

K. Zhao et al.

B Experiment Details

B.1 Dataset Collection

We extend the PROX-S dataset based on PROX [16], which contains 3D re-
constructions of 12 static scenes and RGB-D recordings of natural human-scene
interactions captured using a Kinect sensor. The pseudo-ground truth body fit-
tings of PROX recordings are estimated using [16,55]. To obtain object instance
segmentation and interaction semantics, we further process the PROX dataset
to get: (1) 3D instance segmentation in all the PROX scenes and (2) per-frame
interaction semantic labels in the form of action-object pairs.

We first conduct instance segmentation for the 12 scenes based on the scene
semantic annotation provided in the PROX-E [59] dataset. Specifically, we split
the scene into multiple possible over-segmented instances using connected com-
ponents analysis. Then we manually annotate the instances in the scenes. The
instance segmentation results of the 4 test scenes are visualized in Fig. S4.

To obtain interaction semantics, we densely annotated the PROX dataset
using the VIA video annotation tool [9]. The annotators are asked to label all
intervals containing interactions specified by the action-noun pairs. The anno-
tation tool is illustrated in Fig. S5. Note that multiple interaction labels can
exist in a single frame if the human interacts with multiple objects. We further
retrieve interaction object instance ID using the scene segmentation and object
category annotation. When there is more than one object instance of the anno-
tated category in one scene, we assign the object label to the instance with the
closest distance to action-related body parts.

The PROX-S dataset contains around 32K frames of human-scene interac-
tions from 43 sequences recorded in 12 indoor scenes. We follow the dataset split
in [59] to use ‘MPH16’, ‘MPH1Library’, ‘N0SittingBooth’, and ‘N3OpenArea’
as test scenes and the remaining eight scenes for training. The training data
comprises a total of 17 different actions and 42 categories of interactions defined
as action-object pairs. We evaluate interaction synthesis with semantic control
on about 150 different combinations of action and object instances in the four
test scenes.

B.2 Perceptual Study

We conduct binary perceptual studies to evaluate the interaction naturalness
and unary perceptual studies to evaluate the semantic accuracy of the generated
interactions on Amazon Mechanical Turk (AMT). The AMT interfaces of the two
perceptual studies are illustrated in Fig. S6. For the binary perceptual studies,
we uniformly select 160 samples from PROX pseudo ground truth with varying
semantic labels and respectively generate 160 random samples with the same
semantic labels using our method and two baselines. We render each interaction
with two different views and compare out method against the two baselines
and pseudo ground truth. During the study, participants are instructed to select
one sample that they think is more realistic from two samples generated with

Compositional Human-Scene Interaction Synthesis with Semantic Control

23

N3OpenArea

MPH1Library

MPH16

N0SittingBooth

Fig. S4: Visualization of instance segmentation results in the 4 test scenes. The
objects are colored according to their semantic categories.

different methods. Each comparison task is distributed to three participants for
evaluation.

For the unary perceptual studies, we sample and render one interaction for
the 155 combinations of actions and objects in our test scenes. These interaction
samples are shown to the participants with the interaction semantic labels and
the participants are instructed to rate the semantic accuracy from 1 (strongly
disagree) to 5 (strongly agree).

C Ablation Study

We investigate the influence of body representations and the two-stage genera-
tion design.

We compare three body representations of joint locations (JL), joints loca-
tions and orientations (JLO), and mesh vertices locations (VL) in BodyVAE
by evaluating the semantic contact score of sampled interactions without opti-
mization, and the consistency error between generated body and corresponding
SMPL-X body defined as:

Lbody = |B − M(θ, β)|,

(13)

where B denotes the generated body joints (JL and JLO) or vertices loca-
tions(VL), and M(·) denotes the SMPL-X body model that yields body joints
and vertices given body pose θ and shape parameter β. We use the pose and

24

K. Zhao et al.

Fig. S5: Our annotation tool for annotating interaction semantics. RGB record-
ings and the visualization of SMPL-X fitting are displayed side-by-side. Anno-
tators are instructed to label the intervals containing interactions using action-
noun pairs.

Table 1: Evaluation of body representation choices.

Semantic Contact ↑ Body Consistency (m) ↓

Vertex Location
Joint Location
Joint Location and Rotation

0.72
0.71
0.69

0.01
0.02
0.04

shape parameters predicted by the SMPL-X regressor for JL and VL. We do
not train a regressor for JLO and directly use the input shape parameter of the
template body and the generated joint orientations to pose the SMPL-X body.
Directly generating joint locations with orientations in JLO leads to a lower
semantic contact score of 0.69 and a significantly worse body consistency error
of 0.04m. This is because generating joint locations and orientations together
by networks does not directly yield valid SMPL-X bodies, as the bone lengths
defined by the generated joint locations may not correspond to valid human
skeletons. Regressing joint rotations from joint or vertex locations leads to bet-
ter performance and vertex location representation achieves the best semantic
contact score of 0.72 and body consistency error of 0.01m. Our result indicates
that regressing SMPL-X body parameters from body mesh vertices is easier than
from a skeleton of joints and generates slightly better human-object contact.

To quantify the importance of the two-stage design in interaction generation,
we train two models for one-stage generation which directly predicts the human
body given objects in the original scene coordinate system and re-centered scene
coordinate system, respectively. The re-centered scene coordinate system trans-
lates the origin to the average of interaction object points. The semantic contact
score of interactions generated from the one-stage models using original and re-

Compositional Human-Scene Interaction Synthesis with Semantic Control

25

(a) Binary perceptual study

(b) Unary perceptual study

Fig. S6: AMT User interfaces for the perceptual studies.

centered scene coordinates rapidly drops to 0.20 and 0.31 respectively, compared
to 0.72 of the two-stage method. Our result shows that learning to jointly pre-
dict global interaction location, orientation, and body pose is much more difficult
and the two-stage design is necessary for generating interactions with natural
human-scene contact.

D More Qualitative Results

D.1 Retarget Novel Objects.

Our method can naturally retarget learned interactions to unseen objects with
similar geometry and affordance because we use the point cloud object represen-
tation which encodes object shapes, instead of the object category in previous
works. Figure S7 shows some created novel interactions that are not seen in

26

K. Zhao et al.

“lie on the floor” “walk on the table” “step up the bed” “sit on the monitor”

Fig. S7: Novel combinations of actions and objects generated by our method that
were not part of the training data.

Fig. S8: Interaction synthesis results with explicitly controlled varying human
body shapes, where the extremely thin and heavy bodies are not seen during
training.

training. It demonstrates the generalization capability of our method and the
potential for synthesizing interactions with open-set objects beyond predefined
object categories. Moreover, our method also creates some interesting interac-
tions that are less possible in the real world, e.g., sitting on a monitor.

D.2 Explicit Body Shape Control.

Our method features explicit body shape control in interaction generation, which
is achieved by using personalized body templates. Figure S8 shows interaction
synthesis results where we control the SMPL-X body shape parameters changing
from -3 to 3. Note that the extremely thin and heavy bodies are not seen during
training and our method generalizes to such extreme shapes.

D.3 Random Interaction Samples

We show more random interaction samples from our method in Fig. S9. Our
method generates natural and diverse human-scene interactions.

D.4 Interaction Refinement

We demonstrate how the interaction-based optimization improves human-scene
penetration and contact in Fig. S10.

Compositional Human-Scene Interaction Synthesis with Semantic Control

27

D.5 Compositional Interaction Generation

We show composite interactions randomly generated by composing atomic inter-
actions in Fig. S11. Our method is capable of generating composite interactions
without corresponding training data.

D.6 Synthesis in Scenes with Noisy Segmentation

To show the possibility of generating interactions in scenes without ground truth
segmentation using our method, we generate interactions in scenes with noisy
segmentation obtained using off-the-shelf segmentation methods [6,31] where the
object geometry can be incomplete and noisier than the scene segmentation we
use. We show the generation results on noisily segmented PROX test scenes in
Fig. S12. Our method can generate reasonable interactions given noisy objects
as long as the object shape is not significantly different from training objects.

Regarding training with such noisy scene segmentation, we find it demands
prohibitively more effort in collecting interaction semantic annotation with noisy
segmentation and conclude that a clean scene segmentation that is consistent
with human perception is necessary.

D.7 Failure Cases and Limitation

We show typical failure cases of generating interactions from semantic specifica-
tions in Fig. S13 and failure cases of composing atomic interactions in Fig. S14.

Our method has some limitations. Firstly, our generative models currently
ignore scene objects that are not explicitly specified in the input, which can lead
to penetration with unspecified objects. We currently solve such penetration
using post-processing based on pre-computed scene SDF grids. It is possible to
get rid of the demand for scene SDF grid if we use recent human-occupancy
methods [28], and learning obstacle-aware generative models is an interesting
future direction.

Besides, we observe that hand-object contact in generated results of the
touching action are not accurate enough, an issue caused by the low-quality
hand estimation in used pseudo-ground truth data. Using hand-object inter-
action data with high-quality hand estimation can be future work to improve
hand-object contact in synthesis results.

In addition, the action semantics considered in this paper is relatively coarse-
grained due to the limited scale of interaction data in PROX. We do not distin-
guish left and right limbs in annotation due to limited data and can not generate
fine-grained composite semantics such as touching the chair with the left hand
while touching the table with the right hand. Given larger scale interaction data,
we expect to model more expressive interaction semantics and compose actions
corresponding to finer-grained body parts segmentation, e.g., put the left palm
on the table and lean on the right elbow on the table.

28

K. Zhao et al.

Moreover, we observe our mask-based composition method fails in two cases
as shown in Figure S14: 1) when physically impossible interaction combinations
are specified as input, such as sitting on a cabinet while touching a bed 10 meters
away. 2) when the data distributions of atomic actions have no intersection in
the training data, such as the combination of lying and touching since none of
the touching poses are simultaneously lying in our training data.

Compositional Human-Scene Interaction Synthesis with Semantic Control

29

a
f
o
s

n
o

e
i
l

a
f
o
s

n
w
o
d

e
i
l

r
i
a
h
c

h
c
u
o
t

r
i
a
h
c

n
w
o
d

t
i
s

r
o
t
i
n
o
m
h
c
u
o
t

t
e
n
i
b
a
c

n
o

t
i
s

Fig. S9: Randomly sampled interactions from our method. Each interaction is
rendered with two views.

30

K. Zhao et al.

(a) before

(b) after

(c) before

(d) after

Fig. S10: Illustration of interaction-based optimization where (b) and (d) are the
optimized results of (a) and (c). The human-scene penetration and contact are
improved after the optimization.

Compositional Human-Scene Interaction Synthesis with Semantic Control

31

e
l
b
a
t

h
c
u
o
t
+
a
f
o
s

n
o

t
i
s

d
e
b

h
c
u
o
t
+
r
o
o
l
f

n
o

d
n
a
t
s

g
n
i
t
a
e
s

h
c
u
o
t
+
e
l
b
a
t

n
o

t
i
s

l
l
a
w
h
c
u
o
t
+
d
e
b

n
o

t
i
s

t
h
g
i
l

h
c
u
o
t
+
a
f
o
s

n
o

d
n
a
t
s

Fig. S11: Randomly sampled composite interaction from our method. Note that
the model is not trained with the corresponding training data.

32

K. Zhao et al.

touch

sit on

sit on

sit on

Fig. S12: Synthesized interactions in PROX scenes with noisy object instance
segmentation. The noisy interaction objects are highlighted as red.

(a) Penetration

(b) Geometry Variance

(c) Local Minimum

Fig. S13: Typical failure cases in our results. Example (a) shows penetration
with thin-structure objects where the scene SDF is not effective in resolving
penetration. Example (b) shows the synthesis result of touching a table with
significantly different geometry from tables in training data, i.e., much lower
and smaller in size. Example (c) shows a failure case of being stuck in local
minimum in optimization where the human is blocked by the bed from touching
the wall.

Compositional Human-Scene Interaction Synthesis with Semantic Control

33

(a)

(b)

Fig. S14: Typical failure cases in generating novel interactions by semantic com-
position. Example (a) shows the failed composition of sitting on a cabinet and
touching a bed far away due to being physically impossible. Example (b) shows
the failed composition of lying on the floor and touching the wall.

