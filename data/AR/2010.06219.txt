INVESTIGATING THE SCALABILITY AND BIOLOGICAL
PLAUSIBILITY OF THE ACTIVATION RELAXATION ALGORITHM

0
2
0
2

t
c
O
3
1

]
I

A
.
s
c
[

1
v
9
1
2
6
0
.
0
1
0
2
:
v
i
X
r
a

Beren Millidge
School of Informatics
University of Edinburgh
beren@millidge.name

Alexander Tschantz
Sackler Center for Consciousness Science
School of Engineering and Informatics
University Sussex
tschantz.alec@gmail.com

Anil K Seth
Sackler Center for Consciousness Science
Evolutionary and Adaptive Systems Research Group
School of Engineering and Informatics
University of Sussex
A.K.Seth@sussex.ac.uk

Christopher L Buckley
Evolutionary and Adaptive Systems Research Group
School of Engineering and Informatics
University of Sussex
C.L.Buckley@sussex.ac.uk

October 14, 2020

ABSTRACT

The recently proposed Activation Relaxation (AR) algorithm provides a simple and robust approach
for approximating the backpropagation of error algorithm using only local learning rules. Unlike
competing schemes, it converges to the exact backpropagation gradients, and utilises only a single
type of computational unit and a single backwards relaxation phase. We have previously shown
that the algorithm can be further simpliﬁed and made more biologically plausible by (i) introducing
a learnable set of backwards weights, which overcomes the weight-transport problem, and (ii)
avoiding the computation of nonlinear derivatives at each neuron. However, tthe efﬁcacy of these
simpliﬁcations has, so far, only been tested on simple multi-layer-perceptron (MLP) networks. Here,
we show that these simpliﬁcations still maintain performance using more complex CNN architectures
and challenging datasets, which have proven difﬁcult for other biologically-plausible schemes to
scale to. We also investigate whether another biologically implausible assumption of the original AR
algorithm – the frozen feedforward pass – can be relaxed without damaging performance.

The backpropagation of error algorithm (backprop) has been the engine driving the successes of modern machine
learning with deep neural networks. Backprop solves the credit assignment problem, which is crucial for effectively
training such large and distributed networks. The credit assignment problem concerns how to correctly distribute credit
for a global outcome to each of the many parameters of the network. Backprop solves this problem by exploiting
the chain rule of calculus to recursively compute derivatives. The brain, as a distributed neural network comprising
trillions of synapses, also faces a formidable credit assignment problem. Since backprop is the optimal way to solve
this problem (Baldi & Sadowski, 2016), one might suspect that the brain has evolved to implement a backprop-like
mechanism. However, a canonical implementation of backprop is held to be biologically implausible (Crick, 1989;

 
 
 
 
 
 
A PREPRINT - OCTOBER 14, 2020

Lillicrap, Santoro, Marris, Akerman, & Hinton, 2020), due to entailing update rules that require information about the
global state of the network to be present at each synapse, as well as requiring symmetry of the forward weights, used to
compute predictions, and the backwards weights used to send back gradients, which is known as the ‘weight transport
problem’.

Recently, a variety of learning schemes have attempted to overcome this biological implausibility by using only local
learning rules (Lillicrap, Cownden, Tweed, & Akerman, 2016; Nøkland, 2016; Sacramento, Costa, Bengio, & Senn,
2018; Scellier & Bengio, 2017). These schemes broadly ﬁt into two categories. First, ‘target propagation’ schemes
which sequentially backpropagate targets which can be computed locally, and then rely on a layerwise minimization of
the difference between the activity and the local target (Lee, Zhang, Fischer, & Bengio, 2015; Ororbia & Mali, 2019).
These schemes typically do not exactly approximate the backprop gradients, and it has recently been shown they can
be understood in terms of second order optimization (Meulemans, Carzaniga, Suykens, Sacramento, & Grewe, 2020).
Secondly, ‘recurrent schemes’ exploit recurrent dynamics to enable information about the global output or target to ‘leak’
back through the system via local rules over the course of multiple iterations. Such schemes, which include predictive
coding (Millidge, Tschantz, & Buckley, 2020; Whittington & Bogacz, 2017) and equilibrium-prop (Scellier, Goyal,
Binas, Mesnard, & Bengio, 2018a, 2018b), have been shown to exactly approximate backprop using only local learning
rules. However, they typically require multiple dynamical iterations to converge. In a companion paper, we introduced
a novel recurrent scheme – Activation Relaxation (AR) – which eliminates much of the complexity of previous schemes
while preserving their asymptotic performance (Millidge, Tschantz, Buckley, & Seth, 2020). AR operates in two phases.
The ﬁrst ‘activation’ phase is a standard feedforward sweep through the network. In the second ‘relaxation’ phase, the
activities of each unit are dynamically updated according to a simple local learning rule. At convergence, the weights of
the network are updated using the activities computed in the relaxation phase. In the companion paper, we show that
AR converges rapidly and robustly to the exact backprop gradients and can be used to train deep neural networks with
equivalent performance to backprop. Moreover, unlike predictive coding, AR only utilizes one type of computational
unit. Unlike equilibrium-prop, AR does not require two separate backwards phases (a ’free phase’ and a ’clamped
phase’), nor the storage of information between backwards phases. Moreover, we described two simpliﬁcations of the
algorithm to enhance its biological plausibility – removing the symmetry between backwards and forwards weights
through a set of initially random learnable weights, and dropping the nonlinear derivative terms, which might be difﬁcult
for biological neurons to compute, and showed that these simpliﬁcations did not harm performance in simple MLP
models.

However, the previous paper left open two questions about the effectiveness and biological plausibility of the AR
algorithm which we investigate empirically here. The ﬁrst concerns an important assumption made in the original
algorithm, which we call the frozen forward pass assumption. The original AR algorithm requires that the values of
the forward pass be stored and used repeatedly in the relaxation phase updates. Speciﬁcally, the original value of the
feedforward pass activation and the derivative of the activation function with respect to the feedforward pass activation
need to be stored. Although local, this requirement poses a problem of biological plausibility since it is not clear how
these quantities could be maintained by the neuron and utilised in the learning rule during the relaxation phase. In this
paper, we empirically investigate the extent to which this assumption can be relaxed. We show that we can successfully
relax the requirement to store the nonlinear derivative of the feedforward pass value, but we cannot relax the assumption
for the activity value in the weight updates. This may limit the applicability of the algorithm, as is, in real neurons, or
else require that neurons possess some means of storing this information throughout the relaxation phase.

The second question concerns the scalability of the algorithm and especially the simpliﬁcations introduced in the
second part of the companion paper. Although the convergence to backprop of the AR scheme is shown on an
arbitrary computation graph (see Appendix A), and thus any machine learning architecture of any scale, the efﬁcacy
of the simpliﬁcations proposed to the algorithm were only tested on a simple MLP architecture on the MNIST and
Fashion-MNIST datasets. Here, we show that these simpliﬁcations remain applicable on larger scale CNN architectures

2

A PREPRINT - OCTOBER 14, 2020

and on more challenging datasets such as CIFAR10 and CIFAR100, which is notable since scaling up to CNNs has
historically been challenging for other similar schemes (Bartunov et al., 2018; Launay, Poli, & Krzakala, 2019).

1 Methods

The difﬁculty of neurally implementing backprop is computing the ∂L
∂xl term, where xl denotes the vector of activations
of a given layer l of a neural network, and L is the global loss function. The key idea of AR is to directly approximate
this quantity by deﬁning a dynamical system which converges to it over the course of a relaxation phase. A simple
system which converges to this quantity takes the form of a leaky integrator system, which requires only local quantities
and is biologically plausible

dxl
dt

= −xl +

∂L
∂xl

It can be seen that, at equilibrium, this system converges to the BP gradients,

Using the chain rule, we can write this dynamical system as:

dxl
dt

= 0 =⇒ xl∗

=

∂L
∂xl

dxl
dt

∂xl+1
∂xl

∂L
= −xl +
∂xl+1
≈ −xl + xl+1 ∂f (W l ¯xl)

(cid:12)
(cid:12)
(cid:12)xl=¯xl
∂xl W lT

= −xl + xl+1∗ ∂xl+1
∂xl

(cid:12)
(cid:12)
(cid:12)xl=¯xl

(1)

where ¯x is the value of x computed in the feedforward pass. In the second line, we make the crucial approximation
x∗ ≈ x which allows all layers to be updated in parallel without waiting for the layers above to converge to equilibrium.
Empirically, we ﬁnd that this assumption does not prevent convergence to the correct backprop gradients. The second
assumption, which we call the ‘frozen feedforward pass’ assumption, is that the derivative ∂xl+1
∂xl be computed at x = ¯x.
The weight update equation

∂L
∂xl+1

∂xl+1
∂W l

∂L
∂W l =

(cid:12)
(cid:12)
(cid:12)xl+1=¯xl+1
requires both the value of the activity and the nonlinear derivative to be ﬁxed at their feedforward pass values. We note
two additional implausibilities. The ﬁrst is the W T term in Equation 1 which furnishes the weight transport problem,
and the ∂f
∂x term which results in the backwards nonlinearity problem. We show in the companion paper that the ﬁrst
term can be overcome by replacing the backwards weights W T with a set of random backwards weights ψ, which are
learnt with the learning rule

¯xT ∂f (W l ¯xl)
∂ ¯x

= xl+1∗

(2)

T

∂f (W l ¯xl)
∂ ¯x
and we show that the second problem of backwards nonlinearities can be solved by simply dropping the nonlinearity
from the Equations 1 and 2, which gives the simple and highly plausible update rule,

¯xxl+1∗T

dψl
dt

(3)

=

dxl
dt

≈ −xl + xl+1ψl

(4)

2 Results

First, we investigate whether the frozen feedforward pass assumption can be relaxed. We train a 4-layer MLP network
identical to the one used in the companion paper on the MNIST and Fashion-MNIST (see Appendix B for details)
datasets under three different conditions. We evaluate whether the nonlinear derivative term can be unfrozen so that
it uses the current value of the activity in a.) the relaxation update (Equation 1), b.) in the weight update equation
(Equation 2), and c.) we investigate whether the activation value itself can be replaced in the weight update equation. In

3

A PREPRINT - OCTOBER 14, 2020

(a) MNIST nonlinear derivative relaxation update

(b) MNIST nonlinear derivative weight update

(c) MNIST both nonlinear derivative

(d) MNIST current x weight update

Figure 1: Assessing whether the frozen feedforward pass assumption can be relaxed. We show the resulting performance
(test accuracy) against baseline of relaxing this assumption on the MNIST dataset. See Appendix B for the Fashion-
MNIST results. All results averaged over 10 seeds.

Figure 1, we see that the frozen feedforward pass assumption can be relaxed in the case of the nonlinear derivatives for
both the AR update and the weight update equation. However, relaxing it in the case of the weight update equation
destroys performance. This means that ultimately, a direct implementation of AR in biological circuitry would require
neurons to store the feedforward pass value of their own activations. Secondly, we investigate the scalability of the
AR algorithm and the simpliﬁcations proposed in the companion paper – learnable backwards weights and dropping
nonlinear derivatives – by testing them on a CNN on more challenging datasets (SVHN, CIFAR, and CIFAR100; results
for CIFAR100 and SVHN are presented in Appendix C). The extension to CNNs is important because other biologically
plausible schemes such as feedback alignment (Lillicrap et al., 2016), and directed feedback alignment (Nøkland, 2016),
have been shown to struggle with the CNN architectures (Launay et al., 2019). We tested learning the feedback weights
of both the convolutional layers and the fully connected layers separately 1.

3 Discussion

In this paper, we empirically investigated the scalability and biological plausibility of AR. Firstly, we studied the
degree to which the assumption of the frozen feedforward pass can be relaxed. We showed that it can be relaxed on
the backwards nonlinearities in both the relaxation update equation (Equation 1) and in the weight update equation
(Equation 2). However, we found that the assumption cannot be relaxed for the activity values in the weight update
equation. To do so completely destroys performance. Secondly, we investigated whether the simpliﬁcations introduced
in the companion paper – using learnable backwards weights to address the weight-transport problem, and dropping the
nonlinear derivatives – are scalable to a more complex CNN architectures. We showed that these simpliﬁcations did

1All code to reproduce the experiments and the ﬁgures can be found online at https://github.com/BerenMillidge/Dynamical-

Activation-Relaxation

4

A PREPRINT - OCTOBER 14, 2020

(a) Conv backwards weights

(b) FC backwards weights

(c) Both backwards weights

(d) Conv no nonlinear derivative

(e) FC no nonlinear derivative

(f) Both no nonlinear derivative

Figure 2: Performance (test accuracy), averaged over 10 seeds, on CIFAR10 demonstrating the scalability of the
learnable backwards weights and dropping the nonlinear derivatives in a CNN architecture, compared to baseline AR
without simpliﬁcations. Performance is equivalent throughout.

indeed scale, both alone and in combination, to CNN architectures and more challenging datasets. These results are
highly promising for future studies aimed at investigating the potential of the AR algorithm to support biologically
plausible learning in complex architectures.

Our ﬁrst set of results showing that the frozen feedforward pass assumptions can be relaxed on the nonlinear derivative
terms but not the activities themselves are mixed. While it is important and interesting that they can be relaxed on
the nonlinear derivatives, this is not necessarily surprising given that the nonlinear derivatives can be dropped entirely.
More importantly, the fact that to maintain performance the activities in the weight updates must be ﬁxed at their
feedforward pass values raises a potential hurdle to the biological plausibility of AR, since this value must be stored
throughout the relaxation phase such that it can be utilized at the synapse for the weight update. However, this storage
may be potentially possible through the use of relatively short-lived synaptic traces, for which there is a fair amount of
neurobiological evidence (Bellec et al., 2020), as long as the relaxation phase is relatively short. Alternatively, using
more detailed multicompartment neuron models, the information may be stored in different neural compartments until it
is used. An alternative possibility is that the original activations may be maintained in a separate population of neurons
during the backwards relaxation phase and the retrieval of this information during the synaptic weight updates could be
coordinated by the regular oscillatory phase activity observed in the brain (Buzsaki, 2006).

4 Acknowledgements

BM is supported by an EPSRC funded PhD Studentship. AT is funded by a PhD studentship from the Dr. Mortimer
and Theresa Sackler Foundation and the School of Engineering and Informatics at the University of Sussex. CLB
is supported by BBRSC grant number BB/P022197/1 and by Joint Research with the National Institutes of Natural
Sciences (NINS), Japan, program No. 01112005. AT and AKSare grateful to the Dr. Mortimer and Theresa Sackler

5

A PREPRINT - OCTOBER 14, 2020

Foundation, which supports the Sackler Centrefor Consciousness Science. AKS is additionally grateful to the Canadian
Institute for AdvancedResearch (Azrieli Programme on Brain, Mind, and Consciousness).

References

Baldi, P., & Sadowski, P. (2016). A theory of local learning, the learning channel, and the optimality of backpropagation.

Neural Networks, 83, 51–74.

Bartunov, S., Santoro, A., Richards, B., Marris, L., Hinton, G. E., & Lillicrap, T. (2018). Assessing the scalability of
biologically-motivated deep learning algorithms and architectures. In Advances in neural information processing
systems (pp. 9368–9378).

Bellec, G., Scherr, F., Subramoney, A., Hajek, E., Salaj, D., Legenstein, R., & Maass, W. (2020). A solution to the

learning dilemma for recurrent networks of spiking neurons. bioRxiv, 738385.

Buzsaki, G. (2006). Rhythms of the brain. Oxford University Press.
Crick, F. (1989). The recent excitement about neural networks. Nature, 337(6203), 129–132.
Griewank, A., et al. (1989). On automatic differentiation. Mathematical Programming: recent developments and

applications, 6(6), 83–107.

Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735–1780.
Launay, J., Poli, I., & Krzakala, F. (2019). Principled training of neural networks with direct feedback alignment. arXiv

preprint arXiv:1906.04554.

Lee, D.-H., Zhang, S., Fischer, A., & Bengio, Y. (2015). Difference target propagation. In Joint european conference

on machine learning and knowledge discovery in databases (pp. 498–515).

Lillicrap, T. P., Cownden, D., Tweed, D. B., & Akerman, C. J. (2016). Random synaptic feedback weights support

error backpropagation for deep learning. Nature communications, 7(1), 1–10.

Lillicrap, T. P., Santoro, A., Marris, L., Akerman, C. J., & Hinton, G. (2020). Backpropagation and the brain. Nature

Reviews Neuroscience, 1–12.

Margossian, C. C. (2019). A review of automatic differentiation and its efﬁcient implementation. Wiley Interdisciplinary

Reviews: Data Mining and Knowledge Discovery, 9(4), e1305.

Meulemans, A., Carzaniga, F. S., Suykens, J. A., Sacramento, J., & Grewe, B. F. (2020). A theoretical framework for

target propagation. arXiv preprint arXiv:2006.14331.

Millidge, B., Tschantz, A., & Buckley, C. L.

(2020). Predictive coding approximates backprop along arbitrary

computation graphs. arXiv preprint arXiv:2006.04182.

Millidge, B., Tschantz, A., Buckley, C. L., & Seth, A. (2020). Activation relaxation: A local dynamical approximation

to backpropagation in the brain. arXiv preprint arXiv:2009.05359.

Nøkland, A. (2016). Direct feedback alignment provides learning in deep neural networks. In Advances in neural

information processing systems (pp. 1037–1045).

Ororbia, A. G., & Mali, A. (2019). Biologically motivated algorithms for propagating local target representations. In

Proceedings of the aaai conference on artiﬁcial intelligence (Vol. 33, pp. 4651–4658).

Sacramento, J., Costa, R. P., Bengio, Y., & Senn, W.

(2018). Dendritic cortical microcircuits approximate the

backpropagation algorithm. In Advances in neural information processing systems (pp. 8721–8732).

Scellier, B., & Bengio, Y. (2017). Equilibrium propagation: Bridging the gap between energy-based models and

backpropagation. Frontiers in computational neuroscience, 11, 24.

Scellier, B., Goyal, A., Binas, J., Mesnard, T., & Bengio, Y. (2018a). Extending the framework of equilibrium

propagation to general dynamics.

Scellier, B., Goyal, A., Binas, J., Mesnard, T., & Bengio, Y. (2018b). Generalization of equilibrium propagation to

vector ﬁeld dynamics. arXiv preprint arXiv:1808.04873.

Van Merriënboer, B., Breuleux, O., Bergeron, A., & Lamblin, P. (2018). Automatic differentiation in ml: Where we are
and where we should be going. In Advances in neural information processing systems (pp. 8757–8767).

6

A PREPRINT - OCTOBER 14, 2020

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., . . . Polosukhin, I. (2017). Attention is all

you need. In Advances in neural information processing systems (pp. 5998–6008).

Whittington, J. C., & Bogacz, R. (2017). An approximation of the error backpropagation algorithm in a predictive

coding network with local hebbian synaptic plasticity. Neural computation, 29(5), 1229–1262.

Xiao, H., Rasul, K., & Vollgraf, R. (2017). Fashion-mnist: a novel image dataset for benchmarking machine learning

algorithms.

Appendix A: The AR algorithm

In this appendix we present the detailed pseudocode of the AR algorithm and show how it can be extended to arbitrary
computation graphs. The AR algorithm operates in two phases. First there is a feedforward pass, as in the inference
phase of a standard neural network, which computes the network’s estimate of each activity. Then there is a relaxation
phase the activities of all layers are simultaneously updated with Equation 1. Upon convergence of the activities, the
weights are updated with Equation 2.

Algorithm 1: Activation Relaxation
Data: Dataset D = {X, T}, parameters Θ = {W 0 . . . W L}, inference learning rate ηx, weight learning rate ηθ.
/* Iterate over dataset
for (x0, t ∈ D) do

*/

/* Initial feedforward sweep
for (xl, W l) for each layer do

xl+1 = f (W l, xl)

/* Begin backwards relaxation
while not converged do

/* Compute final output error
(cid:15)L = T − xL
dxL = −xL + (cid:15)L ∂(cid:15)L
∂xL
for xl, W l, xl+1 for each layer do
/* Activation update
dxl = −xl + xl+1 ∂xl+1
∂xl
xlt+1

← xlt

+ ηxdxl

/* Update weights at equilibrium
for W l ∈ {W 0 . . . W L} do
← W lt

W lt+1

+ ηθxl ∂xl
∂W l

*/

*/

*/

*/

*/

Next we show that the AR algorithm can be extended to converge to the backprop gradients not just in MLP networks
composed of hierarchical layers but on arbitrary computation graphs. This extension enables AR to be applied to
essentially all modern machine learning architectures. A computation graph represents a complex function (such as
the forward pass of a complex NN architecture like a transformer (Vaswani et al., 2017), or an LSTM (Hochreiter
& Schmidhuber, 1997)) as a graph of simpler functions. Each function corresponds to a vertex of the graph, while
there is a directed edge between two vertices if the parent vertex is an argument to the function represented by the
child vertex. Because we only study ﬁnite feedforward architectures (and since it is assumed ﬁnite we can ‘unroll’ any
recurrent network into a long feedforward graph), we can represent any machine learning architecture as a directed
acyclic graph (DAG). Automatic Differentiation (AD) techniques, which are at the heart of modern machine learning
(Griewank et al., 1989; Margossian, 2019; Van Merriënboer, Breuleux, Bergeron, & Lamblin, 2018), can then be used
to compute gradients with respect to the parameters of any almost arbitrarily complex architecture automatically. This
allows machine-learning practitioners to derive models which encode complex inductive biases about the structure
of the problem domain, without having to be manually derive the expressions for the derivatives required to train the

7

A PREPRINT - OCTOBER 14, 2020

models. Here, we show that the AR algorithm can also be used to compute these derivatives along arbitrary DAGs,
using only local information in the dynamics,and requiring only the knowledge of the inter-layer derivatives.

Core to AD is the multivariate chain-rule of calculus. Given a node xl on a DAG, the derivative with respect to some
ﬁnal output of the graph can be computed recursively with the relation

∂L
∂xi =

(cid:88)

xj ∈Chi(xj )

∂L
∂xj

∂xj
∂xi

(5)

Where Chi(xi) represents all the nodes which are children of xi. In effect, this recursive rule states that the derivative
with respect to the loss of a point is equal to the sum of the derivatives coming from all paths from that node to the
output. For AR to apply to an arbitrary computation graph, all we require is that the activation update in the relaxation
phase (Equation 1) be replaced with the sum of the activities from all it’s child nodes.

dxi
dt

= −xi +

(cid:88)

xj ∈Chi(xj )

∂L
∂xj

= −xi +

(cid:88)

xj ∈Chi(xj )

≈ −xi +

(cid:88)

xj ∈Chi(xj )

xj ∗ ∂xj
∂xi

(cid:12)
(cid:12)
(cid:12)xl=¯xl

xj ∂xj
∂xi

(cid:12)
(cid:12)
(cid:12)xl=¯xl

(6)

(7)

The equilibrium of this dynamic will thus tend towards the correct backprop gradients, and since the relationship
between the gradient of the parent and the gradients of the children satisifes the same recursive relationship as the
multivariable chain rule (Equation 5) then each node of the graph, and thus the layer as a whole will converge during
the relaxation phase to the exact backprop gradients throughout the computation graph.

Appendix B: MLP architecture and FashionMNIST results

For these experiments, we used a simple MLP model identical to that used in the companion paper. Speciﬁcally,
we tested a four layer MLP where each layer consisted of 300,300,100, and 10 neurons respectively. All layers had
a hyperbolic tangent activation function except for the ﬁnal layer which was linear. For the relaxation phase in all
experiments throughout this paper we used a learning rate of 0.1. We used a minibatch size of 64. We ran the AR update
rule (Equation 2) for 100 iterations which we found empirically was sufﬁcient for convergence. After the relaxation
phase had completed, the weights were updated with a learning rate of 0.0005. The network was trained to predict the
correct one-hot label using a mean-square-error loss function.

The two datasets used were MNIST and Fashion-MNIST (Xiao, Rasul, & Vollgraf, 2017). The MNIST dataset consists
of 60000 training and 10000 test 28x28 grayscale images of handwritten digits. This task is generally considered
relatively easy to solve and the state of the art for simple MLP models is approximately 98%, which we obtain with our
network. The Fashion-MNIST dataset is desigend to be a ‘drop-in’ replacement for MNIST, but substantially more
challenging to solve. The Fashion-MNIST dataset consists of grayscale 28x28 images of clothing items which must be
classiﬁed into one of ten classes. All results presented were the average of 10 random seeds. We also plot the standard
error as error bars.

Here we show the results for the MLP scheme assessing whether the frozen feedforward pass assumption can be relaxed
on the more challenging Fashion-MNIST dataset. Importantly, performance is almost as unhindered compared to the
baseline even on this dataset. Trying to relax the frozen feedforward pass assumption for the activity term in the weight
update completely destroys performance as on the MNIST dataset.

8

A PREPRINT - OCTOBER 14, 2020

(a) Fashion nonlinear derivative relaxation update

(b) Fashion nonlinear derivative weight update

(c) Fashion both nonlinear derivative

(d) Fashion current x weight update

Figure 3: Test accuracy for the FashionMNIST dataset through different relaxations of the frozen feedforward pass
assumption. Similar to the MNIST results, we ﬁnd that relaxing the assumption on both x update (Equation 1) and
weight update (Equation 2) nonlinear derivatives does not affect performance, while relaxing the ¯x term in the weight
update equation destroys performance.

Appendix C: CNN Architecture and Results

For the CNN experiments, we used a simple CNN architecture which is designed simply to test these simpliﬁcations and
not necessarily obtain state of the art performance. This was due to the greater computational cost of the AR algorithm
compared to backprop since the AR algorithm contains a relaxation phase which requires multiple iterations which
each have the same cost as backprop. Our CNN consisted of a convolutional layer followed by a max-pooling layer,
followed by an additional convolutional layers, then two fully connected layers. The convolutional layers had 32 and 64
ﬁlters respectively, while the FC layers had 64,120,and 10 neurons respectively. For CIFAR100 there are 100 output
classes so the ﬁnal layer had 100 neurons. The labels were one-hot-encoded and fed to the network. All input images
were normalized so that their pixel values lay in the range [0, 1] but no other preprocessing was undertaken. We used
hyperbolic tangent activations functions at every layer except the ﬁnal layer which was linear. The network was trained
on a mean-square-error loss function.

We tested the simpliﬁcations to AR proposed in the companion paper – of removing the nonlinear derivatives and using
learnable backwards weights on this CNN architecture on three more challenging datasets (CIFAR, CIFAR100, and
SVHN). In the main text only the CIFAR results are presented. Here we present the SVHN and CIFAR100 results. We
tested the simpliﬁcations (dropping nonlinearity or learning backwards weights) on just the convolutional layers of the
network, just the fully-connected layers of the network, or both together. We found that ultimately performance was
largely maintained even when both convolutional and fully connected layers in the network used learnable backwards

9

A PREPRINT - OCTOBER 14, 2020

weights or had their nonlinear derivatives dropped from both the update and weight equations. These results speak to
the scalability and generalisability of the simpliﬁcations introduced in the companion paper and the general robustness
of the AR algorithm. We implemented the learnable backwards weights of the CNN by applying Equation 3 to the
ﬂattened form of the CNN ﬁlter kernel weights.

(a) Conv backwards weights

(b) FC backwards weights

(c) Both backwards weights

(d) Conv no nonlinear derivative

(e) FC no nonlinear derivative

(f) Both no nonlinear derivative

We see that the simpliﬁcations also scale to the CNN for the SVHN dataset, although, interestingly, performance is
degraded on this dataset when both convolutional and FC nonlinearities are dropped. However, since this does not occur
in the other, more challenging, CIFAR datasets, we take this result to be an anomaly.

(a) Conv backwards weights

(b) FC backwards weights

(c) Both backwards weights

(d) Conv no nonlinear derivative

(e) FC no nonlinear derivative

(f) Both no nonlinear derivative

Here we see that the simpliﬁcations also scale comparably to the full AR algorithm (and thus backprop) in the most
challenging CIFAR100 dataset.

10

