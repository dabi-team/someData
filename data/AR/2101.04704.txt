1
2
0
2

y
a
M
1
1

]

V
C
.
s
c
[

2
v
4
0
7
4
0
.
1
0
1
2
:
v
i
X
r
a

SUBMITTED TO IEEE XXXX

1

Boundary-Aware Segmentation Network for
Mobile and Web Applications

Xuebin Qin, Deng-Ping Fan, Chenyang Huang, Cyril Diagne, Zichen Zhang,
Adri `a Cabeza Sant’Anna, Albert Su `arez, Martin Jagersand, and Ling Shao, Fellow, IEEE

Abstract—Although deep models have greatly improved the accuracy and robustness of image segmentation, obtaining segmentation
results with highly accurate boundaries and ﬁne structures is still a challenging problem. In this paper, we propose a simple yet
powerful Boundary-Aware Segmentation Network (BASNet), which comprises a predict-reﬁne architecture and a hybrid loss, for
highly accurate image segmentation. The predict-reﬁne architecture consists of a densely supervised encoder-decoder network and
a residual reﬁnement module, which are respectively used to predict and reﬁne a segmentation probability map. The hybrid loss is
a combination of the binary cross entropy, structural similarity and intersection-over-union losses, which guide the network to learn
three-level (i.e., pixel-, patch- and map- level) hierarchy representations. We evaluate our BASNet on two reverse tasks including salient
object segmentation, camouﬂaged object segmentation, showing that it achieves very competitive performance with sharp segmentation
boundaries. Importantly, BASNet runs at over 70 fps on a single GPU which beneﬁts many potential real applications. Based on BASNet,
we further developed two (close to) commercial applications: AR COPY & PASTE, in which BASNet is integrated with augmented reality
for “COPYING” and “PASTING” real-world objects, and OBJECT CUT, which is a web-based tool for automatic object background
removal. Both applications have already drawn huge amount of attention and have important real-world impacts. The code and two
applications will be publicly available at: https://github.com/NathanUA/BASNet.

Index Terms—Boundary-aware segmentation, predict-reﬁne architecture, salient object, camouﬂaged object

(cid:70)

1 INTRODUCTION

I MAGE segmentation has been studied over many decades

using conventional methods, and in the past few years
using deep learning. Several different conventional ap-
proaches, such as interactive methods, active contour
(level-set methods), graph-theoretic approaches, perceptual
grouping methods and so on, have been studied for image
segmentation over the past decades. Yet automatic methods
fail where boundaries are complex. Interactive methods
let humans resolve the complex cases. Interactive meth-
ods, [99], [114], [8], [91], are usually able to produce accurate
and robust results, but with signiﬁcant time costs. Active
contour [82], [48], [1], [118], [6], [7], [124], graph-theoretic
[120], [16], [101], [100], [112] and perceptual grouping [47],
[20], [73], [2], [21], [97], [92], [88], methods require almost no

• Xuebin Qin is with the Department of Computing Science, University of

Alberta, Edmonton, AB, Canada, T6G 2R3.(Email: xuebin@ualberta.ca)

• Deng-Ping Fan and Ling Shao are with the Inception Institute of Artiﬁ-
cial Intelligence (IIAI), Abu Dhabi, UAE. (Email: dengpfan@gmail.com;
ling.shao@ieee.org)

• Chenyang Huang is with the Department of Computing Science, Univer-
sity of Alberta, Edmonton, AB, Canada. (Email: chuang8@ualberta.ca)

• Cyril Diagne is with Init ML (Email: cyril@initml.co)
• Zichen Zhang is with the Department of Computing Science, University

of Alberta, Edmonton, AB, Canada. (Email: vincent.zhang@ualberta.ca)

• Adri`a Cabeza Sant’Anna is with the Department of Computer Science,
Polytechnic University of Catalonia, BarcelonaTech, Barcelona, Spain.
(Email: adriacabezasantanna@gmail.com)

• Albert Su`arez is with the Department of Software Engineering, Poly-
technic University of Catalonia, BarcelonaTech, Barcelona, Spain. (Email:
alsumo95@gmail.com)

• Martin Jagersand is with the Department of Computing Science, Univer-

sity of Alberta, Edmonton, AB, Canada. (Email: mj7@ualberta.ca)

• A preliminary version of this work has appeared in CVPR [95].
• Corresponding author: Deng-Ping Fan.

Manuscript submitted December 6, 2020; revised xx xx, xx.

human interventions so that they are faster than interactive
methods. However, they are relatively less robust.

In recent years, to achieve accurate, robust and fast
performance, many deep learning models [77] have been
developed for image segmentation. Semantic image seg-
mentation [70], [40] is one of the most popular topics,
which aims at labeling every pixel in an image, with one
of the several predeﬁned class labels. It has been widely
used in many applications, such as scene understanding
[65], [145], autonomous driving [28], [15], etc. The targets in
these applications are usually large in size, so most existing
methods focus on achieving robustness performance with
high regional accuracy. Less attention has been paid to the
high spatial accuracy of boundaries and ﬁne structures.
However, many other applications, e.g. image segmenta-
tion/editing [90], [46], [31], [91] and manipulation [45], [76],
visual tracking [57], [89], [93], vision guided robot hand
manipulation [88] and so on, require highly accurate object
boundaries and ﬁne structures.

There are two main challenges in accurate image seg-
mentation: Firstly, large-scale features play important roles
in classifying pixels since they can provide more semantic
information compared with local features. However, large-
scale features are usually obtained from deep low-resolution
feature maps or by large size kernels and the spatial resolu-
tion is sacriﬁced. Simple upsampling of the low-resolution
feature maps to high resolution is not able to recover the ﬁne
structures [70]. Thus, many encoder-decoder architectures
[98] have been developed for segmenting edges or thin
structures. Their skip connections and gradual upsampling
operations play important roles in recovering the high res-
olution probability maps. Additionally, different cascaded

 
 
 
 
 
 
SUBMITTED TO IEEE XXXX

2

or iterative architectures [125], [115], [18], [113] have been
introduced to further improve the segmentation accuracy by
gradually reﬁning the coarse predictions, which sometimes
leads to complicated network architectures and computa-
tional bottleneck. Secondly, most of the image segmentation
models use cross entropy (CE) loss to supervise the training
process. CE loss usually gives greater penalties on these
seriously erroneous predictions (e.g. predict “1” as “0.1” or
predict “0” as “0.9”). Therefore, deep models trained with
CE loss compromise andes prefer to predict “hard” samples
with a non-committed “0.5”. In image segmentation tasks,
the boundary pixels of targets are usually the hard samples,
so this will lead to blurry boundaries in predicted segmen-
tation probability maps. Other losses, such as intersection-
over-union (IoU) loss [96], [75], [80], F-score loss [141] and
Dice-score loss [27], have also been introduced to image
segmentation tasks for handling biased training sets. These
are sometimes able to achieve higher (regional) evaluation
metrics, e.g. IoU, F-score, since their optimization targets
are consistent with these metrics. However, they are not
speciﬁcally designed for capturing ﬁne structures and often
produce “biased” results, which tend to emphasize the large
structures while neglecting ﬁne details.

To address the above issues, we propose a novel but
simple Boundary-Aware Segmentation Network (BASNet),
which consists of a predict-reﬁne network and a hybrid loss,
for highly accurate image segmentation. The predict-reﬁne
architecture is designed to predict and reﬁne the predicted
probability maps sequentially. It consists of a U-Net-like
[98] deeply supervised [56], [123] “heavy” encoder-decoder
network and a residual reﬁnement module with “light”
encoder-decoder structure. The “heavy” encoder-decoder
network transfers the input image to a segmentation prob-
ability map, while the “light” reﬁnement module reﬁnes
the predicted map by learning the residuals between the
coarse map and ground truth (GT). In contrast to [85], [43],
[18], which iteratively use reﬁnement modules on saliency
predictions or intermediate feature maps at multiple scales,
our reﬁnement module is used only once on the original
scale of the segmentation maps. Overall, our predict-reﬁne
architecture is concise and easy to use. The hybrid loss
combines the binary cross entropy (BCE) [17], structural
similarity (SSIM) [117] and IoU losses [75], to supervise the
training process in a three-level hierarchy: pixel-, patch- and
map- level. Instead of using explicit boundary loss (NLDF+
[72], C2S [61], BANet[104]), we implicitly inject the goal of
accurate boundary prediction in the hybrid loss, contem-
plating that it may help reduce spurious error from cross
propagating the information learned from the boundaries
and other regions on the image (see Fig. 1).

In addition to proposing novel segmentation techniques,
developing novel segmentation applications also plays a
very important role in advancing the segmentation ﬁeld.
Therefore, we developed two novel BASNet-based applica-
tions: AR COPY & PASTE1 and OBJECT CUT2. AR COPY
& PASTE is a mobile app built upon our BASNet model and
the Augmented Reality techniques. By using cellphones, it
provides a novel interactive user experience where users

1. https://clipdrop.co/
2. https://objectcut.com/

(a) Image

(b) GT

(c) BASNet

Fig. 1. Sample results of our BASNet on salient object
detection (Top) and camouﬂaged object detection (Bottom).

can “COPY” the real-world targets and “PASTE” them into
desktop software . Speciﬁcally, AR COPY & PASTE allows
users to take a photo of an object using a mobile device.
Then the background removed object returned by our re-
mote BASNet server will be shown in the camera view. In
this view, the “COPIED“ object is overlapped with real scene
video stream. Users can move and target the mobile camera
at a speciﬁc position on the desktop screen. Then tapping
the screen of the mobile device will trigger the “PASTE”
operation, which transmits the object from the mobile de-
vice to the software opened in the desktop. Meanwhile,
OBJECT CUT provides a web-based service for automatic
image background removal based on our BASNet model.
An image can be uploaded from a local machine or through
an URL. This application greatly facilitates the background
removal for users who have no image editing experience or
software. The main contributions can be summarized as:

• We develop a novel boundary-aware image segmen-
tation network, BASNet, which consists of a deeply
supervised encoder-decoder and a residual reﬁnement
module, and a novel hybrid loss that fuses BCE, SSIM,
and IoU to supervise the training process of accurate
image segmentation on three levels: pixel-level, patch-
level and map-level.

• We conduct thorough evaluations of the proposed
method including a comparison with 25 state-of-the-
art (SOTA) methods on six widely used public salient
object segmentation datasets, a comparison with 16
models on the SOC (Salient Object in Clutter) dataset
and a comparison with 13 camouﬂaged object detection
(COD) models on three public COD datasets. BAS-
Net achieves very competitive performance in terms
of regional evaluation metrics, while outperforms other
models in terms of boundary evaluation metrics.

• We develop two (close to) commercial applications,
AR COPY & PASTE and OBJECT CUT, based on our
BASNet. These two applications further demonstrate
the simplicity, effectiveness and efﬁciency of our model.

Compared with the CVPR version [95] of this work,
the following extensions are made. First, deeper theoretical
explanations of the hybrid loss design are added. Second,
more comprehensive and thorough experiments on different
datasets, including salient objects in clutter (SOC) and COD,
are included. Third, two (close to) commercial applications,
AR COPY & PASTE and OBJECT CUT, are developed.

SUBMITTED TO IEEE XXXX

3

2 RELATED WORKS
2.1 Traditional Image Segmentation Approaches

Watershed [108], graph cut [52], [53], active contour [82],
perceptual grouping [92] as well as the interactive methods
based on these approaches mainly rely on well-designed
handcrafted features, objective functions and optimization
algorithms. Watershed and graph cut approaches segment
images based on the regional pixel similarities, so they
are less effective in segmenting very ﬁne structures and
achieving smooth and accurate segmentation boundaries.
Active contour and perceptual grouping methods can be
considered as boundary based approaches. Active contour
methods represent the 2D segmentation contour by the level
set of a 3D function. Instead of directly evolving the 2D
contour, this group of approaches evolves the 3D function to
ﬁnd the optimal segmentation contour, which avoids com-
plicated 2D contour splitting and merging issues. Perceptual
grouping methods segment images by selecting and group-
ing subsets of the detected edge fragments or line segments
from given images to formulate closed or open contours
of the targets to be segmented. However, although these
methods are able to produce relatively accurate boundaries,
they are very sensitive to noise and local minima, which
usually leads to less robust and unreliable performance.

2.2 Patch-wise Deep Models

To improve the robustness and accuracy, deep learning
methods have been widely introduced to image segmen-
tation [87]. Early deep methods use existing image classi-
ﬁcation networks as feature extractors and formulate the
image segmentation tasks as patch-wise image pixel (super-
pixel) [58], [69], [109], [142], [60] classiﬁcation problems.
These models greatly improve the segmentation robustness
in some tasks due to the strong ﬁtting capability of deep
neural networks. However, they are still not able to produce
high spatial accuracy, let alone segmenting ﬁne structures.
The main reason is probably that the pixels in patch-wise
models are classiﬁed independently based on the local fea-
tures inside each patch and larger-scale spatial contexts are
not used.

2.3 FCN and its Variants

With the development of fully convolutional network (FCN)
[70], deep convolutional neural networks have become a
standard solution for image segmentation problems. Large
number of deep convolutional models [77] have been pro-
posed for image segmentation. FCN adapts classiﬁcation
backbones, such as VGG [102], GoogleNet [105], ResNet [35]
and DenseNet [40], by discarding the fully connected layers
and directly upsampling the output features of certain con-
volutional layers with speciﬁc scales to build a fully con-
volutional image segmentation model. However, the direct
upsampling from low resolution fails in capturing accurate
structures. Therefore, The DeepLab family [10], [11], [12]
replaces the pooling operations by atrous convolutions to
avoid degrading the feature map resolution. Besides, they
also introduce a densely connected Conditional Random
Field (CRF) to improve the segmentation results. However,
applying atrous convolutions on high-resolution maps leads

to larger memory costs and CRF usually yields noisy seg-
mentation boundaries. Holistically Edge Detection (HED)
[123], RCF [29] and CASENet [130] are proposed to directly
segment edges by making full use of the features from
both the shallow and deep stages of the image classiﬁcation
backbones. Besides, many variants [59], [50], [37] of FCN
have been proposed for salient object detection (binary-class
image segmentation) [116]. Most of these works are focusing
on either developing novel multi-scale feature aggregation
strategies or designing new multi-scale feature extraction
modules. Zhang et al. (Amulet) [136] developed a generic
framework for aggregating multi-level convolutional fea-
tures of the VGG backbone. Inspired by HED [123], Hou
et al. (DSS+) [36] introduced short connections to the skip-
layer structures of HED to better use the deep layer features.
Chen et al. (RAS) [13] developed a reverse attention model
to iteratively reﬁne the side-outputs from a HED-like ar-
chitecture. Zhang et al. (LFR) [135] designed a symmetrical
fully convolutional network which takes images and their
reﬂection as inputs to learn the saliency features from the
complementary input spaces. Instead of passing the infor-
mation with single direction (deep to shallow or shallow
to deep), Zhang et al. (BMPM) [133] proposed to have the
information passed between the shallow and deep layers by
a controlled bi-directional passing module.

2.4 Encoder-decoder Architectures

Rather than directly upsampling features from deep lay-
ers of the backbones, SegNet [3] and U-Net [98] employ
encoder-decoder like structures to gradually up-sample the
deep low-resolution feature maps. Combined with skip
connections, they are able to recover more details. One of
the main characteristics of these models is the symmetrical
downsampling and upsampling operations. To reduce the
checkerboard artifacts in the prediction, Zhang et al. (UCF)
[137] reformulated the dropout and developed a hybrid
module for the upsampling operation. To better use the
features extracted by backbones, Liu et al. (PoolNet) [66]
constructed the decoder part using their newly developed
feature aggregation, pyramid pooling and global guidance
modules. In addition, stacked HourglassNet [81], CU-UNet
[106], UNet++ [146] and U2-Net [94] further explore diverse
ways of improving the encoder-decoder architectures by
cascaded or nested stacking.

2.5 Deep Recurrent Models

Recurrent techniques have been widely applied in image
segmentation models. Kuen et al. [51] proposed to achieve
the completely segmented results by sequentially segment-
ing image sub-regions using a recurrent framework. Liu et
al. (PiCANetR) [68] deployed a bidirectional LSTM along the
row and column of the feature maps respectively to generate
the pixel-wise attention maps for salient object detection. Hu
et al. (SAC-Net) [38] developed similar strategies to [68] to
capture spatial attenuation context for image segmentation.
Zhang et al. (PAGRN) [138] proposed to transfers global
information from deep to shallower layers via a multi-
path recurrent connection. Wang et al. (RFCN) [111] built
a cascaded network by stacking multiple encoder-decoders

SUBMITTED TO IEEE XXXX

4

Fig. 2. Architecture of the proposed boundary-aware segmentation network: BASNet. See § 3 for details.

to recurrently correcting the prediction errors of the previ-
ous stages. Instead of iteratively reﬁning the segmentation
results [111], Hu et al. (RADF+) [39] recurrently aggregated
and reﬁned multi-layer deep features to achieve accurate
segmentation results. However, due to the serial connections
between each recurrent step, models using the “recurrent”
techniques are relatively less efﬁcient in terms of time costs.

2.6 Deep Coarse-to-Fine Models

This group of models aims at improving the segmentation
results by gradually reﬁning the coarse predictions. Lin et
al. (ReﬁneNet) [63] developed a multi-path reﬁnement seg-
mentation network, which uses long-range residual connec-
tions to exploit the information along the down-sampling
process. Liu et al. (DHSNet) [67] proposed a hierarchical
recurrent convolutional neural network (HRCNN), which
hierarchically and progressively reﬁnes the segmentation
results in a coarse-to-ﬁne manner. Wang et al. (SRM) [113]
developed a multi-stage framework for segmentation map
reﬁnement, in which each stage takes the input image and
the segmentation maps (lower resolution) from the last stage
to produce higher-resolution results. Deng et al. (R3Net+)
[18] proposed to alternatively reﬁne the segmentation re-
sults based on the shallow, high-resolution and deep low-
resolution feature maps. Wang et al. (DGRL) [115] developed
a global-to-local framework which ﬁrst localizes the to-be-
segmented targets globally and then reﬁnes these targets
using a local boundary reﬁnement module. The coarse to
ﬁne models reduce the probability of overﬁtting and show
promising improvements in accuracy.

2.7 Boundary-assisted Deep Models

Region and boundaries are mutually determined. Therefore,
many models introduce boundary information to assist seg-
mentation. Luo et al. (NLDF) [72] proposed to supervise a
4×5 grid structure adapted from VGG-16 by fusing the cross
entropy and the boundary IoU inspired by Mumford-Shah
[79]. Li et al. (C2S) [61] tried to recover the regional saliency
segmentation from segmented contours. Su et al. (BANet)
[104] developed a boundary-aware segmentation network
with three separate streams: a boundary localization stream,
an interior perception stream and a transition compensation
stream for boundary, region and boundary/region transi-
tion prediction, respectively. Zhao et al. (EGNet) [140] pro-
posed an edge guidance network for salient object segmen-
tation by explicitly modeling and fusing complementary
region and boundary information. Most of models in this
category explicitly use boundary information as either an
additional supervision loss or a assisting prediction stream
for inferring the region segments.

In this paper, we propose a simple predict-reﬁne archi-
tecture which takes advantage of both the encoder-decoder
architecture and the coarse-to-ﬁne strategy. Besides, instead
of explicitly using boundary loss or additional boundary
prediction streams, we design a simple hybrid loss which
implicitly describes the dissimilarity between the segmenta-
tion prediction and the ground truth at three levels: pixel-
, patch- and map-level. The predict-reﬁne architecture to-
gether with the hybrid loss provides a simple yet powerful
solution for image segmentation and some close to commer-
cial applications.

SUBMITTED TO IEEE XXXX

3 METHODOLOGY
3.1 Overview

Our BASNet architecture consists of two modules as shown
in Fig. 2. The prediction module is a U-Net-like densely
supervised Encoder-Decoder network [98], which learns to
predict segmentation probability maps from input images.
The multi-scale Residual Reﬁnement Module (RRM) reﬁnes
the resulting map of the prediction module by learning the
residuals between the coarse map and the GT.

3.2 Prediction Module

Share same spirit of U-Net [98] and SegNet [3], we design
our segmentation prediction module as an encoder-decoder
fashion, since this kind of architectures is able to capture
high-level global contexts and low-level details at the same
time. To reduce over-ﬁtting, the last layer of each decoder
stage is supervised by the GT, inspired by HED [123] (see
Fig. 2). The encoder has an input convolutional layer and
six stages comprised of basic res-blocks. The input convo-
lutional layer and the ﬁrst four stages are adopted from
ResNet-34 [35]. The difference is that our input layer has
64 convolutional ﬁlters with a size of 3×3 and stride of 1
rather than a size of 7×7 and stride of 2. Additionally, there
is no pooling operation after the input layer. This means
that the feature maps before the second stage have the same
spatial resolution as the input image. This is different from
the original ResNet-34, which has a quarter of the resolution
in the ﬁrst feature map. This adaptation enables the network
to obtain higher resolution feature maps in earlier layers,
while decreasing the overall receptive ﬁelds. To achieve
the same receptive ﬁeld as ResNet-34 [35], we add two
more stages after the fourth stage of ResNet-34. Both stages
consist of three basic res-blocks with 512 ﬁlters after a non-
overlapping max pooling layer of size 2. To further capture
global information, we add a bridge stage between the en-
coder and decoder. It consists of three convolutional layers
with 512 dilated (dilation=2) [129] 3×3 ﬁlters. Each of these
convolutional layers is followed by a batch normalization
[42] and a ReLU activation function [32].

Our decoder is almost symmetrical to the encoder. Each
stage consists of three convolution layers followed by a
batch normalization and a ReLU activation function. The
input of each stage is the concatenated feature maps of
the up-sampled output from its previous stage and its
corresponding stage in the encoder. To achieve the side-
output maps, the multi-channel output of the bridge stage
and each decoder stage is fed to a plain 3 × 3 convolution
layer followed by a bilinear upsampling and a sigmoid
function. Therefore, given an input image, our prediction
module produces seven segmentation probability maps in
the training process. Although every predicted map is up-
sampled to the same size with the input image, the last one
has the highest accuracy and hence is taken as the ﬁnal
output of the prediction module. This output is passed to
the reﬁnement module.

3.3 Residual Reﬁnement Module

Reﬁnement Modules (RMs) [43], [18] are usually designed
as a residual block [125] that reﬁnes the coarse segmentation

5

(b)

(d)

(a)

(c)

Fig. 3. Illustration of different aspects of coarse prediction
in one-dimension: (a) Red: probability plot of GT, (b) Green:
probability plot of coarse boundary not aligning with GT, (c)
Blue: coarse region having too low probability, (d) Purple:
real coarse predictions usually have both (b&c) problems.

maps Scoarse by learning the residuals Sresidual between the
coarse maps and the GT, as

Sref ined = Scoarse + Sresidual.

(1)

reﬁnement module,

Before introducing our
the term
“coarse” has to be determined. Here, “coarse” includes
two aspects. One is blurry and noisy boundaries (see the
one-dimension illustration in Fig. 3(b)).The other one is
the unevenly predicted regional probabilities (see Fig. 3(c)).
As shown in Fig. 3(d), real predicted coarse maps usually
contain both coarse cases.

The residual reﬁnement module based on local context
(RRM LC), Fig. 4(a), was originally designed for boundary
reﬁnement [85]. Since its receptive ﬁeld is small, Islam et
al. [43] and Deng et al. [18] iteratively or recurrently used
it for reﬁning segmentation probability maps on different
scales. Wang et al. [113] adopted the pyramid pooling mod-
ule from [34], in which three-scale pyramid pooling features
are concatenated. To avoid losing details caused by pooling
operations, RRM MS ( Fig. 4(b)) uses convolutions with
different kernel sizes and dilations [129], [133] to capture
multi-scale contexts. However, these modules are shallow
thus hard to capture high-level information for reﬁnement.
To reﬁne inaccuracies in coarse segmentation maps of
image reigons and boundaries, we develop a novel resid-
ual reﬁnement module. Our RRM employs the residual
encoder-decoder architecture, RRM Ours (see Figs. 2 and
4(c)). Its main architecture is similar but simpler than our
prediction module. It contains an input layer, an encoder, a
bridge, a decoder and an output layer. Different from the
prediction module, both the encoder and decoder have four
stages. Each stage only has one convolutional layer. Each
layer has 64 ﬁlters of size 3 × 3 followed by a batch normal-
ization and a ReLU activation function. The bridge stage
also has a convolutional layer with 64 ﬁlters of size 3 × 3
followed by a batch normalization and ReLU activation.
Non-overlapping max pooling is used for downsampling

SUBMITTED TO IEEE XXXX

6

in the encoder and bilinear interpolation is utilized for
upsampling in the decoder. The output of this RM module is
used as the ﬁnal generating segmentation results of BASNet.

3.4 Hybrid Loss

Our training loss is deﬁned as the summation over all
outputs:

L = (cid:80)K

k=1αk(cid:96)(k),

(2)

where (cid:96)(k) is the loss of the k-th side output, K denotes the
total number of the outputs and αk is the weight of each
loss. As described in Sec. 3.2 and Sec. 3.3, our segmentation
model is deeply supervised with eight outputs, i.e. K = 8,
including seven outputs from the prediction module and
one output from the reﬁnement module.

To obtain high quality regional segmentation and clear

boundaries, we propose to deﬁne (cid:96)(k) as a hybrid loss:

bce + (cid:96)(k)

ssim + (cid:96)(k)
iou,

(3)

(cid:96)(k) = (cid:96)(k)
ssim, and (cid:96)(k)

where (cid:96)(k)
[117] and IoU loss [75], respectively.

bce, (cid:96)(k)

iou denote BCE loss [17], SSIM loss

BCE [17] loss is the most widely used loss in binary

classiﬁcation and segmentation. It is deﬁned as:

(cid:96)bce=− (cid:80)
(r,c)

[G(r,c) log(S(r,c))+(1−G(r,c)) log(1−S(r,c))],

(4)

where G(r, c) ∈ {0, 1} is the GT label of the pixel (r, c) and
S(r, c) is the predicted probability of segmented object.

SSIM [117] was originally devised for image quality
assessment. It captures the structural information in an
image. Hence, we integrated it into our training loss to learn
the structural information of the GT. Let x = {xj : j =
1, ..., N 2} and y = {yj : j = 1, ..., N 2} be the pixel values
of two corresponding patches (size: N × N ) cropped from
the predicted probability map S and the binary GT mask G,
respectively. The SSIM of x and y is deﬁned as:

(cid:96)ssim = 1 −

(2µxµy + C1)(2σxy + C2)
x + µ2

y + C1)(σ2

x + σ2

y + C2)

(µ2

(5)

where µx, µy and σx, σy are the mean and standard devia-
tions of x and y respectively, σxy is covariance, C1 = 0.012
and C2 = 0.032 are used to avoid dividing by 0.

IoU was originally proposed for measuring the similarity
between two sets [44] and has become a standard evaluation
measure for object detection and segmentation. Recently,
it has been used as a training loss [96], [75]. To ensure its
differentiability, we adopted the IoU loss used in [75]:

(cid:96)iou = 1 −

H
(cid:80)
r=1

H
(cid:80)
r=1

W
(cid:80)
c=1

S(r,c)G(r,c)

W
(cid:80)
c=1

[S(r,c)+G(r,c)−S(r,c)G(r,c)]

(6)

(a) RRM LC

(b) RRM MS

(c) RRM Ours

Fig. 4. Illustration of different Residual Reﬁne Modules
(RRM): (a) local boundary reﬁnement module RRM LC; (b)
multi-scale reﬁnement module RRM MS; (c) our encoder-
decoder reﬁnement module RRM Ours.

and problems of these losses in the ﬁtting process. The (c),
(d), (e) and (f) columns show changes of the intermediate
probability maps as the training progresses.

The BCE loss is computed pixel-wise. It does not con-
sider the labels of the neighborhood and it weights both
the foreground and background pixels equally. This helps
with the convergence on all pixels and guarantee a relatively
good local optima. Since signiﬁcantly erroneous predictions
(predicting 0 as 0.9 or predicting 1 as 0.1) produce large
BCE loss, the models trained with BCE loss suppress these
errors by giving prediction values around 0.5 around the
boundaries, which often leads to blurred boundaries and
ﬁne structures, as we can see from the second row of column
(c), where the contour of the whole foreground region is
blurring, and the third row, in which the cable below the
backpack is with low probability values.

The SSIM loss is a patch-level measure, which considers
a local neighborhood of each pixel. It assigns higher weights
to pixels located in the transitional buffer regions between
foregrounds and backgrounds, e.g. boundaries, ﬁne struc-
tures, so that the loss is higher around the boundary, even
when the predicted probabilities on the boundary and the
rest of the foreground are the same. It is worth noting that
the loss for the background region is similar or sometimes
even higher than the foreground region. However, the back-
ground loss does not contribute to the training until the
prediction of background pixel becomes very close to the
GT, where the loss drops rapidly from one to zero. Because
µy, σxy, µxµy and σ2
y in the SSIM loss (Equ. 5) are all
zeros in the background regions, so the SSIM loss can be
approximated by:

(cid:96)bg
ssim = 1 −

C1C2
x + C1)(σ2

(µ2

x + C2)

.

(7)

where G(r, c) ∈ {0, 1} is the GT label of the pixel (r, c) and
S(r, c) is the predicted probability of segmented object.

Fig. 5 illustrated the impact of each of the three losses.
Fig. 5 (a) and (b) are the input image and its ground truth
segmentation mask. It is worth noting that the probability
maps in Fig. 5 are generated by ﬁtting a single pair of
image and its ground truth (GT) mask. Hence, after a certain
number of iterations, all the losses are able to produce
perfect results due to over-ﬁtting. Here, we ignore the ﬁnal
ﬁtting results and aim to observe the different characteristics

Since C1 = 0.012 and C2 = 0.032, only if the prediction
x is close to zero, the SSIM loss (Equ. 7) will become the
dominant term. The second and third rows of column (d) in
Fig. 5 illustrate that the model trained with the SSIM loss is
able to predict correct results on the foreground region and
boundaries while neglecting the background accuracy in the
beginning of the training process. This characteristic of the
SSIM loss helps the optimization to focus on the boundary
and foreground region. As the training progresses, the SSIM
loss for the foreground is reduced and the background

SUBMITTED TO IEEE XXXX

7

(a) Image

(b) GT

(c) (cid:96)bce

(d) (cid:96)ssim

(e) (cid:96)iou

(f) (cid:96)bsi

Fig. 5. Intermediate predictions of our BASNet when ﬁtting
with different losses.

loss becomes the dominant term. This is helpful since the
prediction typically goes close to zero only late in the
training process, where BCE loss becomes ﬂat. The SSIM
loss ensures that there is still enough gradient to drive the
learning process. Hence, the background prediction looks
cleaner since the probability is pushed to zero.

The IoU is a map-level measure. Larger ares contribute
more to the IoU, so models trained with IoU loss empha-
sizes more on the large foreground regions and are thus
able to produce relatively homogeneous and more conﬁ-
dent (whiter) probabilities for these regions. However, these
models often produce false negatives on ﬁne structures. As
shown in the column (e) of Fig. 5, the human head in the
second row and the backpack cord in both the second and
third rows are missing.

To take advantage of the above three losses, we combine
them together to formulate the hybrid loss. BCE is used
to maintain a smooth gradient for all pixels, while IoU is
employed to put more focus on the foreground. SSIM is
used to encourage the prediction to respect the structure
of the original image, by employing a larger loss near
the boundaries, as well as further push the backgrounds
predictions to zero.

4 EXPERIMENTS

In this paper, we are focusing on improving the spatial
accuracy of segmentation results. Therefore, experiments are
conducted on two reverse binary class image segmentation
tasks: salient object segmentation [110] and camouﬂaged
object segmentation [25]. Salient object segmentation is a
popular task in computer vision, which aims at segmenting
the salient regions against their backgrounds. In this task,
the targets are usually with high contrast against their back-
grounds. However, camouﬂaged object segmentation is the
most challenging one because the camouﬂaged objects usu-
ally have similar appearance to their backgrounds, which

means they are difﬁcult to be perceived and segmented.
In addition, many of the camouﬂaged objects have very
complex structures and boundaries.

4.1 Implementation and Setting

We implement our network using the publicly available Py-
torch 1.4.0 [84]. An 16-core PC with an AMD Threadripper
2950x 3.5 GHz CPU (with 64GB 3000 MHz RAM, ) and
an RTX Titan GPU (with 24GB memory) is used for both
training and testing. During training, each image is ﬁrst re-
sized to 320×320 and randomly cropped to 288×288. Some
of the encoder parameters are initialized from the ResNet-
34 model [35]. Other convolutional layers are initialized
by Xavier [30]. We use the Adam optimizer [49] to train
our network and its hyperparameters are set to the default
values, where the initial learning rate lr=1e-4, betas=(0.9,
0.999), eps=1e-8, weight decay=0. We train the network
until the loss converges, without using the validation set.
The training loss converges after 400k iterations with a batch
size of eight and the whole training process takes about
110 hours. During testing, the input image is resized to
320×320 and fed into the network to obtain its segmentation
probability map. Then, the probability map (320×320) is
resized back to the original size of the input image. Both
resizing processes use bilinear interpolation. The inference
for a 320×320 image only takes 0.015s (70 fps, different from
that reported in our CVPR version [95], in which IO time is
included).

4.2 Evaluation Metrics

Five measures are used to evaluate the performance of the
proposed model. (1) Weighted F-measure F w
β [74] gives a
comprehensive and balanced evaluation on both precision
and recall, which is able to better leverage the interpolation,
dependency and equal-importance ﬂaw. (2) Relax boundary
F-measure F b
β [19] is adopted to quantitatively evaluate the
boundary quality of the predicted maps. (3) Mean absolute
error M [86] reﬂects the average per-pixel difference be-
tween the probability map and the GT. (4) Mean structural
measure Sα [22] quantizes the structural similarity between
the predicted probability map and the GT mask. (5) Mean
enhanced-alignment measure Em
φ [23] takes both global
and local similarity into consideration. Evaluation code:
https://github.com/DengPingFan/CODToolbox.

4.3 Experiments on Salient Object Segmentation

4.3.1 Datasets
For salient object segmentation task3, we train our network
using the DUTS-TR [110] dataset, which has 10553 images.
Before training, the dataset is augmented by horizontal
ﬂipping to 21106 images. For salient object segmentation
tasks, we evaluate our method on six commonly used
salient object segmentation benchmark datasets: SOD [78],
ECSSD [127], DUT-OMRON [128], PASCAL-S [62], HKU-
IS [58], DUTS-TE [110]. DUT-OMRON has 5,168 images
with one or multiple objects. The majority of these ob-
jects are structurally complex. PASCAL-S was originally

3. The camouﬂaged object segmentation task use the same augmen-

tation strategies.

iter#1iter#20iter#40SUBMITTED TO IEEE XXXX

8

(a) Image

(b) GT

(c) U-Netbce

(d) EDbce

(e) EDSbce

(f) ED LCbce (g) ED MSbce(h) BASNetbce (i) BASNetbsi

(j) Image

(k) GT

(l) (cid:96)bce

(m) (cid:96)ssim

(n) (cid:96)iou

(o) (cid:96)bs

(p) (cid:96)bi

(q) (cid:96)si

(r) (cid:96)bsi

Fig. 6. Qualitative comparison of different conﬁgures in the ablation study. The ﬁrst row show the predicted probability
maps of different architectures trained with BCE loss and our BASNet trained with (cid:96)bsi loss. The second row show the
segmentation maps of our proposed prediction-reﬁnement architecture trained with different losses. The corresponding
quantitative results can be found in Table 1.

(a) Image

(b) GT

(c) BASNet

(d) PoolNet

(e) CPD

(f) AFNet

(g) PiCANet

(h) NLDF

Fig. 7. Qualitative comparison on salient object segmentation datasets.

created for semantic image segmentation and consists of
850 challenging images. DUTS is a relatively large salient
object segmentation dataset. It has two subsets: DUTS-TR
and DUTS-TE. There are 10,553 images in DUTS-TR for
training and 5,019 images in DUTS-TE for testing. In our
experiments, DUTS-TR is used for training the model for
salient object segmentation. HKU-IS contains 4,447 images,
many of which contain multiple foreground objects. ECSSD
contains 1,000 semantically meaningful images. However,
the structures of the foreground objects in these images are
complex. SOD contains 300 very challenging images. These
images have either single complicated large foreground ob-
jects which overlap with the image boundaries or multiple
salient objects with low contrast.

TABLE 1. Ablation study on different architectures (Arch.)
and losses: ED: encoder-decoder, EDS: encoder-decoder +
side output supervision; (cid:96)b, (cid:96)s and (cid:96)i denote the BCE, SSIM
and IoU loss, respectively, (cid:96)bi = (cid:96)b + (cid:96)i, (cid:96)bs = (cid:96)b + (cid:96)s, (cid:96)si =
(cid:96)s + (cid:96)i, (cid:96)bsi = (cid:96)b + (cid:96)s + (cid:96)i.

Ablation

.

h
c
r
A

s
s
o
L

F w
β ↑ F b

Conﬁgurations

β ↑ M ↓ Sα ↑ Em

φ ↑
U-Net [98] + (cid:96)b 0.827 0.669 0.064 0.867 0.897
ED + (cid:96)b 0.871 0.786 0.045 0.908 0.923
EDS + (cid:96)b 0.891 0.819 0.041 0.920 0.935
EDS+RRM LC + (cid:96)b 0.900 0.804 0.038 0.915 0.935
EDS+RRM MS + (cid:96)b 0.890 0.816 0.041 0.919 0.934
EDS+RRM Ours + (cid:96)b 0.900 0.827 0.037 0.923 0.943
EDS+RRM Ours + (cid:96)s
0.886 0.814 0.044 0.904 0.932
EDS+RRM Ours + (cid:96)i
0.902 0.820 0.037 0.911 0.943
EDS+RRM Ours + (cid:96)bs
0.903 0.823 0.037 0.920 0.942
EDS+RRM Ours + (cid:96)bi
0.909 0.832 0.035 0.921 0.947
EDS+RRM Ours + (cid:96)si
0.894 0.812 0.041 0.906 0.938
EDS+RRM Ours + (cid:96)bsi 0.912 0.840 0.034 0.925 0.947

4.3.2 Ablation Study

In this section, we validate the effectiveness of each key
components used in our model. The ablation study is di-
vided into two parts: an architecture ablation and loss ab-
lation. For simplify, the ablation experiments are conducted
on the ECSSD dataset. The same hyper-parameters to that
described in Sec. 4.1 are used here.

Architecture: To demonstrate the effectiveness of our
BASNet, we report quantitative comparison results of our
model against other related architectures. We take U-Net
[98] as our baseline network. Then we start with our pro-
posed encoder-decoder network and progressively extend
it with dense side output supervision and different resid-
ual reﬁnement modules, including RRM LC, RRM MS and

SUBMITTED TO IEEE XXXX

9

TABLE 2. Comparison of the proposed method and 25 other methods on three salient object segmentation datasets: DUT-
OMRON, DUTS-TE and HKU-IS. ↑ and ↓ indicate the higher the score the better and the lower the score the better,
respectively. “*” indicates results post-processed by CRF. Bold font denotes the best performance.

Models

CVPR17

MDFTIP16
UCFICCV17
AmuletICCV17
NLDF∗
CVPR17
DSS∗
LFRIJCAI18
C2SECCV18
RASECCV18
RADF∗
AAAI18
PAGRNCVPR18
BMPMCVPR18
PiCANetCVPR18
MLMSCVPR19
AFNetCVPR19
MSWSCVPR19
R3Net∗
IJCAI18
CapSalCVPR19
SRMICCV17
DGRLCVPR18
CPDCVPR19
PoolNetCVPR19
BANetICCV19
EGNetICCV19
MINetCVPR20
GateNetECCV20
BASNet (Ours)

F w
β ↑
0.565
0.573
0.626
0.634
0.697
0.647
0.661
0.695
0.723
0.622
0.681
0.691
0.681
0.717
0.527
0.728
0.482
0.658
0.697
0.719
0.729
0.719
0.728
0.719
0.703
0.760

DUT-OMRON[128]
F b
β ↑ M ↓
0.142
0.406
0.12
0.48
0.098
0.528
0.08
0.514
0.063
0.559
0.103
0.508
0.072
0.565
0.062
0.615
0.061
0.579
0.071
0.582
0.064
0.612
0.068
0.643
0.064
0.612
0.057
0.635
0.109
0.362
0.063
0.599
0.101
0.396
0.069
0.523
0.063
0.584
0.056
0.655
0.056
0.675
0.061
0.611
0.056
0.679
0.057
0.640
0.061
0.625
0.703
0.057

Sα ↑ Em
0.721
0.76
0.781
0.77
0.79
0.78
0.798
0.814
0.815
0.775
0.809
0.826
0.809
0.826
0.756
0.817
0.674
0.798
0.810
0.825
0.836
0.823
0.836
0.822
0.821
0.841

φ ↑
0.759
0.761
0.794
0.799
0.831
0.799
0.823
0.844
0.857
0.772
0.831
0.833
0.831
0.846
0.729
0.853
0.659
0.808
0.845
0.847
0.854
0.861
0.853
0.846
0.840
0.868

F w
β ↑
0.543
0.596
0.658
0.71
0.755
0.689
0.713
0.74
0.748
0.724
0.761
0.747
0.761
0.785
0.586
0.763
0.691
0.722
0.76
0.795
0.807
0.781
0.797
0.813
0.786
0.825

DUTS-TE[110]

HKU-IS[58]

F b
0.447
0.518
0.568
0.591
0.606
0.556
0.607
0.656
0.608
0.692
0.699
0.704
0.699
0.714
0.376
0.601
0.605
0.592
0.656
0.741
0.765
0.687
0.761
0.747
0.722
0.786

β ↑ M ↓
0.099
0.112
0.084
0.065
0.056
0.083
0.062
0.059
0.061
0.055
0.048
0.054
0.048
0.046
0.908
0.058
0.072
0.058
0.051
0.043
0.040
0.046
0.043
0.040
0.045
0.042

Sα ↑ Em
0.723
0.777
0.796
0.805
0.812
0.799
0.829
0.828
0.814
0.825
0.851
0.851
0.851
0.855
0.749
0.817
0.808
0.824
0.836
0.858
0.871
0.861
0.879
0.875
0.871
0.881

φ ↑
0.764
0.776
0.817
0.851
0.877
0.833
0.859
0.871
0.869
0.843
0.883
0.873
0.883
0.893
0.742
0.873
0.849
0.853
0.887
0.898
0.904
0.897
0.898
0.906
0.892
0.907

F w
β ↑
0.564
0.779
0.817
0.838
0.867
0.861
0.829
0.843
0.872
0.82
0.859
0.847
0.859
0.869
0.685
0.877
0.782
0.835
0.865
0.875
0.881
0.869
0.875
0.889
0.872
0.900

F b
0.594
0.679
0.716
0.694
0.706
0.731
0.717
0.748
0.725
0.762
0.773
0.784
0.773
0.772
0.438
0.74
0.654
0.68
0.744
0.795
0.811
0.760
0.802
0.799
0.783
0.821

β ↑ M ↓
0.129
0.062
0.051
0.048
0.04
0.04
0.048
0.045
0.039
0.048
0.039
0.042
0.039
0.036
0.084
0.036
0.062
0.046
0.037
0.034
0.033
0.037
0.035
0.032
0.036
0.030

Sα ↑ Em
0.81
0.875
0.886
0.879
0.878
0.905
0.883
0.887
0.888
0.887
0.907
0.906
0.907
0.905
0.818
0.895
0.85
0.887
0.897
0.905
0.917
0.902
0.910
0.912
0.910
0.918

φ ↑
0.742
0.887
0.910
0.914
0.925
0.934
0.859
0.92
0.935
0.900
0.931
0.923
0.931
0.935
0.787
0.939
0.883
0.913
0.939
0.939
0.940
0.938
0.938
0.944
0.934
0.948

TABLE 3. Comparison of the proposed method and 25 other methods on three salient object detection datasets: ECSSD,
PASCAL-S and SOD. See Table 2 for details.

Baseline Models

CVPR17

MDFTIP16
UCFICCV17
AmuletICCV17
NLDF∗
CVPR17
DSS∗
LFRIJCAI18
C2SECCV18
RASECCV18
RADF∗
AAAI18
PAGRNCVPR18
BMPMCVPR18
PiCANetCVPR18
MLMSCVPR19
AFNetCVPR19
MSWSCVPR19
R3Net∗
IJCAI18
CapSalCVPR19
SRMICCV17
DGRLCVPR18
CPDCVPR19
PoolNetCVPR19
BANetICCV19
EGNetICCV19
MINetCVPR20
GateNetECCV20
BASNet (Ours)

F w
β ↑
0.705
0.806
0.84
0.839
0.872
0.858
0.851
0.857
0.883
0.834
0.871
0.865
0.871
0.887
0.716
0.902
0.771
0.853
0.883
0.898
0.896
0.890
0.892
0.905
0.886
0.912

ECSSD[127]

PASCAL-S[62]

F b
0.472
0.669
0.711
0.666
0.696
0.694
0.708
0.741
0.720
0.747
0.770
0.784
0.770
0.776
0.411
0.759
0.574
0.672
0.753
0.811
0.813
0.758
0.814
0.805
0.782
0.840

β ↑ M ↓
0.105
0.069
0.059
0.063
0.052
0.052
0.055
0.056
0.049
0.061
0.045
0.046
0.045
0.042
0.096
0.040
0.077
0.054
0.042
0.037
0.039
0.041
0.041
0.037
0.042
0.034

Sα ↑ Em
0.776
0.884
0.894
0.897
0.882
0.897
0.893
0.893
0.894
0.889
0.911
0.914
0.911
0.914
0.828
0.910
0.826
0.895
0.906
0.918
0.921
0.913
0.920
0.920
0.917
0.925

φ ↑
0.796
0.891
0.909
0.900
0.918
0.923
0.917
0.914
0.929
0.895
0.928
0.924
0.928
0.936
0.791
0.944
0.849
0.913
0.938
0.942
0.940
0.940
0.936
0.943
0.933
0.947

F w
β ↑
0.589
0.694
0.734
0.737
0.759
0.737
0.766
0.736
0.755
0.738
0.779
0.772
0.779
0.798
0.614
0.761
0.786
0.758
0.787
0.800
0.798
0.792
0.793
0.813
0.803
0.808

F b
0.343
0.493
0.541
0.495
0.499
0.499
0.543
0.560
0.515
0.594
0.617
0.612
0.62
0.626
0.289
0.538
0.527
0.509
0.569
0.639
0.644
0.589
0.650
0.648
0.623
0.674

β ↑ M ↓
0.142
0.115
0.100
0.098
0.093
0.107
0.082
0.101
0.097
0.090
0.074
0.078
0.074
0.070
0.133
0.092
0.073
0.084
0.074
0.071
0.075
0.078
0.077
0.065
0.068
0.072

Sα ↑ Em
0.696
0.805
0.818
0.798
0.798
0.805
0.836
0.799
0.802
0.822
0.845
0.848
0.844
0.849
0.768
0.807
0.837
0.834
0.839
0.848
0.832
0.840
0.848
0.854
0.857
0.847

φ ↑
0.706
0.809
0.835
0.839
0.845
0.835
0.864
0.830
0.840
0.830
0.872
0.866
0.875
0.883
0.731
0.843
0.872
0.853
0.877
0.878
0.876
0.875
0.873
0.889
0.882
0.878

F w
β ↑
0.508
0.675
0.677
0.709
0.710
0.734
0.700
0.720
0.729
-
0.726
0.722
0.726
0.723
0.573
0.735
0.597
0.670
0.731
0.714
0.759
0.750
0.737
-
-
0.762

SOD[78]

F b
0.311
0.471
0.454
0.475
0.444
0.479
0.457
0.544
0.476
-
0.562
0.572
0.562
0.545
0.231
0.431
0.404
0.392
0.502
0.556
0.606
0.589
0.586
-
-
0.640

β ↑ M ↓
0.192
0.148
0.144
0.125
0.124
0.123
0.124
0.124
0.126
-
0.108
0.103
0.108
0.111
0.167
0.125
0.148
0.128
0.106
0.112
0.102
0.109
0.112
-
-
0.102

Sα ↑ Em
0.643
0.762
0.753
0.755
0.743
0.773
0.760
0.764
0.757
-
0.786
0.789
0.786
0.774
0.700
0.759
0.695
0.741
0.773
0.767
0.797
0.782
0.784
-
-
0.793

φ ↑
0.607
0.773
0.776
0.777
0.774
0.813
0.785
0.788
0.801
-
0.799
0.796
0.799
0.79
0.656
0.796
0.699
0.744
0.807
0.778
0.818
0.813
0.798
-
-
0.822

RRM Ours. The top part of Table 1 and the ﬁrst row of
Fig. 6 illustrate the qualitative and quantitative results of the
architecture ablation study, respectively. As we can see, our
BASNet architecture achieves the best performance among
all conﬁgurations.

Loss: To demonstrate the effectiveness of our proposed
fusion loss, we conduct a set of experiments over different
losses based on our BASNet architecture. The results in
Table 1 indicate that the proposed hybrid (cid:96)bsi loss greatly
improves the performance, especially in terms of the bound-
ary quality. It is clear that our hybrid loss achieves superior
qualitative results, as shown in the second row of Fig. 6.

4.3.3 Comparison with State-of-the-Arts

We compare our method with 25 state-of-the-art models, in-
cluding MDF [60], UCF [137], Amulet [136], NLDF [72], DSS

[36], LFR [135], C2S [61], RAS [13], RADF [39], PAGRN [138],
BMPM [133], PiCANet [68], MLMS [119], AFNet [26], MSWS
[131], R3-Net [18], CapSal [134], SRM [113], DGRL [115],
CPD [121], PoolNet [66], BANet [104], EGNet [140], MINet
[83] and GateNet [144], on the salient object segmentation
task. For fair comparison, we either use the segmentation
maps released by the authors or run their publicly available
models with their default settings.

Quantitative Evaluation: Tables 2 and 3 provide
quantitative comparisons on six salient object segmenta-
tion datasets. Our BASNet outperforms other models on
the DUT-OMRON, DUTS-TE, HKU-IS, ECSSD and SOD
datasets in terms of nearly all metrics, except for the M
measures on DUT-OMRON and DUTS-TE and the Sα on
SOD. On the PASCAL-S dataset, MINet performs the best in
terms of three metrics: F w
φ . It is worth noting

β , M AE and Em

SUBMITTED TO IEEE XXXX

10

(a) Image

(b) GT

(c) BASNet

(d) SCRN

(e) BANet

(f) EGNet

(g) PoolNet

(h) CPD

Fig. 8. Qualitative comparison on typical samples from the SOC dataset. Images from top to bottom are from attributes SO
(Small Object), OV(Out-of-View), OC (Occlusion) and SC (Shape Complexity) respectively.

where the scene contains multiple independent “salient”
targets, But only one of them is labeled. Our BASNet
sometimes fails in these cases due to lack of the ability of
recognizing the tiny saliency differences between multiple
connected targets. The recent uncertainty model [132] may
be one of the solutions.

4.3.5 Attribute-base Analysis

In addition to the most frequently used salient object seg-
mentation datasets, we also test our model on another
dataset, SOC [24]. The SOC dataset contains complicated
scenarios, which are more challenging than those in the
previous six SOD datasets. Besides, the SOC dataset cat-
egorizes images into nine different groups including AC
(Appearance Change), BO (Big Object), CL (Clutter), HO
(Heterogeneous Object), MB (Motion Blur), OC (Occlusion),
OV (Out-of-View), SC (Shape Complexity), and SO (Small
Object), according to their attributes. We train our BASNet
on both DUTS-TR and the training set (1,800 images with
salient objects) of SOC dataset [24] and evaluate their per-
formance on the testing set of SOC-Sal. There are totally 600
images with salient objects in the testing set. Each image
may be categorized into one or multiple attributes (e.g.AC
and BO).

Quantitative Evaluation: Tab. 4 illustrates a comparison
between our BASNet and 16 other state-of-the-art models,
including Amulet [136], DSS [36], NLDF [72], C2S-Net [61],
SRM [113], R3Net [18], BMPM [133], DGRL [115], PiCANet-
R (PiC(R)) [68], RANet [14], AFNet [26], CPD [121], PoolNet
[66], EGNet [140], BANet [104] and SCRN [122] in terms
of attribute-based performance. As we can see, our BAS-
Net achieves obvious improvements against the existing
methods. Particularly, our BASNet advances the boundary
measure F b
β by large margins (over 5% and sometimes over
10%) on different attributes.

Qualitative Evaluation: Fig. 8 provides a qualitative
comparison of our BASNet and other baseline models. As
we can see, BASNet is able to handle different challenges,

(a) Image

(b) GT
Fig. 9. Failure cases on salient object segmentation datasets.

(c) BASNet

that BASNet achieves the highest relax boundary F-measure
F b
β on all of the six datasets, which indicates its strong
capability in capturing boundaries and ﬁne structures.

Qualitative Evaluation: Fig. 7 shows the qualitative
comparison between our BASNet and 5 other typical mod-
els. As we can see, our BASNet is able to handle different
challenging cases, such as small target with relatively low
contrast (1st row), large object with complicated boundaries
(2nd row), object with hollow structures (3rd row) and target
with very ﬁne structures (4th row). The third and fourth row
of Fig. 7 show inspiring results, in which the segmentation
maps predicted by our BASNet contain more details than
the GT. These details reveals the possible inconsistency
between the labels of training and testing datasets. Although
detecting of these details usually leads to the deterioration
of the quantitative evaluation scores of our model, it is more
practically useful than good scores.

4.3.4 Failure Cases

Fig. 9 shows three typical failure cases of our BASNet on
SOD datasets. For instance, the model sometimes fails in
very complicated scenarios, in which there seems no salient
objects, as show in the ﬁrst row of Fig. 9. The second row
gives an exemplary failure case of “saliency confusing”,

SUBMITTED TO IEEE XXXX
11
TABLE 4. Comparison of the proposed method and other SOTA methods on the SOC test set. ↑ and ↓ indicate the higher
score the better and the lower score the better respectively. Bold font indicates the best performance. Avg. denotes the
average of all the attribute-based metric scores.

Attr Metr. Amulet
[136]

AC

BO

CL

HO

MB

OC

OV

SC

SO

Avg.

DSS
[36]

NLDF
[72]
F w
β ↑ 0.620 0.629 0.620
F b
β ↑ 0.448 0.384 0.374
M ↓ 0.120 0.113 0.119
Sα ↑ 0.752 0.753 0.737
Em
φ ↑ 0.791 0.788 0.784
F w
β ↑ 0.612 0.614 0.622
F b
β ↑ 0.274 0.213 0.218
M ↓ 0.346 0.356 0.354
Sα ↑ 0.574 0.561 0.568
Em
φ ↑ 0.551 0.537 0.539
F w
β ↑ 0.663 0.617 0.614
F b
β ↑ 0.374 0.275 0.292
M ↓ 0.141 0.153 0.159
Sα ↑ 0.763 0.722 0.713
Em
φ ↑ 0.789 0.763 0.764
F w
β ↑ 0.688 0.660 0.661
F b
β ↑ 0.465 0.347 0.378
M ↓ 0.119 0.124 0.126
Sα ↑ 0.791 0.767 0.755
Em
φ ↑ 0.810 0.796 0.798
F w
β ↑ 0.561 0.577 0.551
F b
β ↑ 0.435 0.396 0.397
M ↓ 0.142 0.132 0.138
Sα ↑ 0.712 0.719 0.685
Em
φ ↑ 0.739 0.753 0.740
F w
β ↑ 0.607 0.595 0.593
F b
β ↑ 0.395 0.310 0.335
M ↓ 0.143 0.144 0.149
Sα ↑ 0.735 0.718 0.709
Em
φ ↑ 0.763 0.760 0.755
F w
β ↑ 0.637 0.622 0.616
F b
β ↑ 0.405 0.311 0.339
M ↓ 0.173 0.180 0.184
Sα ↑ 0.721 0.700 0.688
Em
φ ↑ 0.751 0.737 0.736
F w
β ↑ 0.608 0.599 0.593
F b
β ↑ 0.481 0.407 0.414
M ↓ 0.098 0.098 0.101
Sα ↑ 0.768 0.761 0.745
Em
φ ↑ 0.794 0.799 0.788
F w
β ↑ 0.523 0.524 0.526
F b
β ↑ 0.386 0.325 0.341
M ↓ 0.119 0.109 0.115
Sα ↑ 0.718 0.713 0.703
Em
φ ↑ 0.745 0.756 0.747
F w
β ↑ 0.613 0.604 0.600
F b
β ↑ 0.407 0.330 0.343
M ↓ 0.156 0.157 0.161
Sα ↑ 0.726 0.713 0.700
Em
φ ↑ 0.748 0.743 0.739

R3Net
SRM
C2SNet
[18]
[61]
[113]
0.647 0.690 0.593
0.408 0.410 0.387
0.109 0.096 0.135
0.755 0.791 0.713
0.807 0.824 0.753
0.730 0.667 0.456
0.362 0.274 0.229
0.267 0.306 0.445
0.654 0.614 0.437
0.661 0.616 0.419
0.655 0.665 0.546
0.342 0.327 0.315
0.144 0.134 0.182
0.742 0.759 0.659
0.789 0.793 0.710
0.668 0.696 0.633
0.398 0.392 0.383
0.123 0.115 0.136
0.768 0.794 0.740
0.805 0.819 0.782
0.593 0.619 0.489
0.450 0.395 0.348
0.128 0.115 0.160
0.720 0.742 0.657
0.778 0.778 0.697
0.622 0.630 0.520
0.382 0.343 0.323
0.130 0.129 0.168
0.738 0.749 0.653
0.784 0.780 0.706
0.671 0.682 0.527
0.420 0.368 0.336
0.159 0.150 0.216
0.728 0.745 0.624
0.790 0.779 0.663
0.611 0.638 0.550
0.433 0.423 0.427
0.100 0.090 0.114
0.756 0.783 0.716
0.806 0.814 0.765
0.531 0.561 0.487
0.353 0.334 0.342
0.116 0.099 0.118
0.706 0.737 0.682
0.752 0.769 0.732
0.636 0.650 0.533
0.394 0.363 0.343
0.142 0.137 0.186
0.730 0.746 0.653
0.775 0.775 0.692

BMPM
[133]
0.680
0.531
0.098
0.780
0.815
0.670
0.400
0.303
0.604
0.620
0.678
0.432
0.123
0.761
0.801
0.684
0.496
0.116
0.781
0.813
0.651
0.561
0.105
0.762
0.812
0.644
0.456
0.119
0.752
0.800
0.701
0.494
0.136
0.751
0.807
0.677
0.561
0.081
0.799
0.841
0.567
0.442
0.096
0.732
0.780
0.661
0.486
0.131
0.747
0.788

DGRL
[115]
0.718
0.457
0.081
0.790
0.853
0.786
0.392
0.215
0.684
0.725
0.714
0.393
0.119
0.770
0.824
0.722
0.447
0.104
0.791
0.833
0.655
0.464
0.113
0.744
0.823
0.659
0.396
0.116
0.747
0.808
0.733
0.434
0.125
0.762
0.828
0.669
0.455
0.087
0.772
0.837
0.602
0.382
0.092
0.736
0.802
0.695
0.424
0.117
0.755
0.815

PiC(R)
[68]
0.682
0.489
0.093
0.792
0.815
0.799
0.466
0.200
0.729
0.741
0.692
0.420
0.123
0.787
0.794
0.704
0.462
0.108
0.809
0.819
0.637
0.520
0.099
0.775
0.813
0.638
0.439
0.119
0.765
0.784
0.721
0.490
0.127
0.781
0.810
0.627
0.492
0.093
0.784
0.799
0.566
0.417
0.095
0.748
0.766
0.674
0.466
0.117
0.774
0.793

RANet
[14]
0.603
0.448
0.132
0.708
0.765
0.453
0.231
0.454
0.421
0.404
0.542
0.344
0.188
0.624
0.715
0.626
0.425
0.143
0.713
0.777
0.576
0.476
0.139
0.696
0.761
0.527
0.382
0.169
0.641
0.718
0.529
0.383
0.217
0.611
0.664
0.594
0.504
0.110
0.724
0.792
0.518
0.412
0.113
0.682
0.759
0.552
0.401
0.185
0.647
0.706

AFNet
CPD
[26]
[121]
0.712 0.727
0.569 0.626
0.084 0.083
0.796 0.799
0.852 0.843
0.741 0.739
0.450 0.481
0.245 0.257
0.658 0.647
0.698 0.665
0.696 0.724
0.465 0.553
0.119 0.114
0.768 0.773
0.802 0.821
0.722 0.751
0.527 0.618
0.103 0.097
0.798 0.803
0.834 0.845
0.626 0.679
0.537 0.619
0.111 0.106
0.734 0.754
0.762 0.804
0.680 0.672
0.503 0.545
0.109 0.115
0.771 0.750
0.820 0.810
0.723 0.721
0.524 0.592
0.129 0.134
0.761 0.748
0.817 0.803
0.696 0.708
0.572 0.627
0.076 0.080
0.808 0.793
0.854 0.858
0.596 0.623
0.468 0.533
0.089 0.091
0.746 0.745
0.792 0.804
0.688 0.705
0.513 0.577
0.118 0.120
0.760 0.757
0.803 0.806

PoolNet
[66]
0.713
0.578
0.094
0.795
0.846
0.610
0.323
0.353
0.561
0.554
0.681
0.488
0.134
0.760
0.801
0.739
0.570
0.100
0.815
0.846
0.642
0.592
0.121
0.751
0.779
0.659
0.510
0.119
0.756
0.801
0.697
0.526
0.148
0.747
0.795
0.695
0.613
0.075
0.807
0.856
0.626
0.523
0.087
0.768
0.814
0.674
0.525
0.137
0.751
0.788

EGNet
[140]
0.731
0.597
0.085
0.806
0.854
0.585
0.319
0.373
0.528
0.528
0.677
0.493
0.139
0.757
0.790
0.720
0.560
0.106
0.802
0.829
0.649
0.584
0.109
0.762
0.789
0.658
0.505
0.121
0.754
0.798
0.707
0.541
0.146
0.752
0.802
0.678
0.597
0.083
0.793
0.844
0.594
0.494
0.098
0.749
0.784
0.667
0.521
0.140
0.745
0.780

BANet
[104]
0.740
0.562
0.086
0.806
0.858
0.720
0.360
0.271
0.645
0.650
0.726
0.461
0.117
0.784
0.824
0.754
0.542
0.094
0.819
0.850
0.672
0.539
0.104
0.764
0.803
0.678
0.466
0.112
0.765
0.809
0.752
0.509
0.119
0.779
0.835
0.706
0.562
0.078
0.807
0.851
0.621
0.457
0.090
0.755
0.801
0.708
0.495
0.119
0.769
0.809

SCRN
[122]
0.724
0.588
0.078
0.809
0.849
0.778
0.453
0.224
0.698
0.706
0.717
0.506
0.113
0.795
0.820
0.743
0.577
0.096
0.823
0.842
0.690
0.595
0.100
0.792
0.817
0.673
0.514
0.111
0.775
0.800
0.723
0.545
0.126
0.774
0.808
0.691
0.603
0.078
0.809
0.843
0.614
0.506
0.082
0.767
0.797
0.706
0.543
0.112
0.782
0.809

Ours
(DUTS)
0.735
0.659
0.087
0.805
0.844
0.747
0.519
0.253
0.666
0.677
0.700
0.552
0.121
0.774
0.807
0.746
0.627
0.099
0.809
0.843
0.678
0.635
0.115
0.755
0.805
0.672
0.573
0.115
0.760
0.806
0.730
0.617
0.132
0.764
0.809
0.728
0.654
0.074
0.812
0.861
0.634
0.551
0.092
0.758
0.800
0.708
0.599
0.121
0.767
0.806

Ours
(SOC)
0.792
0.696
0.060
0.831
0.885
0.808
0.572
0.166
0.723
0.775
0.730
0.579
0.110
0.785
0.826
0.764
0.639
0.093
0.814
0.850
0.725
0.674
0.072
0.797
0.836
0.707
0.601
0.101
0.780
0.829
0.749
0.630
0.114
0.781
0.828
0.746
0.672
0.072
0.820
0.872
0.684
0.612
0.075
0.787
0.835
0.745
0.631
0.096
0.791
0.837

including small objects (1st row), out-of-view objects (2nd
row), occluded targets (3rd row) and objects with compli-
cated shapes (4th row).

4.4 Experiments on Camouﬂaged Object Segmentation

To further evaluate the performance of the proposed BAS-
Net, we also tested it on the camouﬂaged object segmenta-
tion (COS) task [126], [55], [25]. Compared with salient ob-
ject segmentation, COS is a relatively newer and more chal-
lenging task. Because the contrast between the camouﬂaged
targets and their backgrounds is sometimes extremely low.
Besides, the targets usually have similar color and texture to
their backgrounds. In addition, their shape or structure of
these targets can sometimes be very complex.

4.4.1 Datasets

We test our model on the CHAMELEON [103], CAMO-Test
[54] and COD10K-Test datasets [25]. CHAMELEON [103]
contains 76 images taken by independent photographers.
These images are marked as good examples of camouﬂaged
animals by the photographers. CAMO [54] contains both
camouﬂaged and non-camouﬂaged subsets. We use the
camouﬂaged subset, which comprises two further subsets:
CAMO-Train (1,000 images) and CAMO-Test (250 images).
COD10K [25] is currently the largest camouﬂaged object
detection dataset. It comprises 10,000 images of 78 object
categories in various natural scenes. There are 5,066 images
densely labeled with accurate (matting-level) binary masks.
COD10K consists of 3,040 images for training (COD10K-
Train) and 2,026 images for testing (COD10K-Test). For fair

SUBMITTED TO IEEE XXXX

12

TABLE 5. Comparison of the proposed method and 13 other methods on three camouﬂaged object segmentation datasets:
CHAMELEON, CAMO-Test and COD10K-Test. ↑ and ↓ indicate the higher score the better and the lower the score the
better, respectively. Bold font indicates the best performance.

Baseline Models

FPNCVPR17
MaskRCNNCVPR17
PSPNetCVPR17
UNet++DLMIA18
PiCANetCVPR18
MSRCNCVPR19
PFANetCVPR19
HTCCVPR2019
PoolNetCVPR2019
ANet-SRMCVIU19
CPDCVPR2019
EGNetICCV19
SINetCVPR20
BASNet (Ours)

F w
β ↑ F b
0.590
0.518
0.555
0.501
0.536
0.443
0.378
0.204
0.555
-
0.706
0.702
0.740
0.866

CHAMELEON[103]
β ↑ M ↓
0.075
0.099
0.085
0.094
0.084
0.091
0.139
0.129
0.079
-
0.052
0.050
0.044
0.022

Sα ↑ Em
0.794
0.643
0.773
0.695
0.769
0.637
0.679
0.517
0.777
-
0.853
0.848
0.869
0.914

0.246
0.128
0.207
0.246
0.200
0.074
0.096
0.071
0.151
-
0.383
0.289
0.410
0.650

φ ↑ F w
0.784
0.778
0.756
0.763
0.749
0.686
0.648
0.489
0.779
-
0.868
0.871
0.893
0.954

β ↑ F b
0.483
0.430
0.455
0.392
0.356
0.454
0.391
0.174
0.494
0.484
0.550
0.583
0.606
0.646

CAMO-Test[54]
β ↑ M ↓
0.131
0.151
0.139
0.149
0.155
0.133
0.169
0.172
0.128
0.126
0.115
0.104
0.100
0.096

0.232
0.117
0.191
0.232
0.166
0.128
0.130
0.076
0.155
0.217
0.306
0.264
0.334
0.420

Sα ↑ Em
0.684
0.574
0.663
0.599
0.609
0.618
0.659
0.477
0.703
0.682
0.726
0.732
0.752
0.749

φ ↑ F w
0.677
0.715
0.659
0.654
0.584
0.669
0.622
0.442
0.698
0.686
0.730
0.768
0.772
0.796

β ↑ F b
0.411
0.402
0.377
0.350
0.322
0.419
0.286
0.221
0.416
-
0.508
0.509
0.551
0.677

COD10K-Test[25]
β ↑ M ↓
0.075
0.081
0.080
0.086
0.083
0.073
0.118
0.088
0.07
-
0.059
0.056
0.051
0.038

Sα ↑ Em
0.697
0.613
0.678
0.623
0.649
0.641
0.636
0.548
0.705
-
0.747
0.737
0.771
0.802

φ ↑
0.692
0.748
0.681
0.674
0.643
0.706
0.618
0.520
0.713
-
0.771
0.779
0.809
0.855

0.195
0.110
0.166
0.195
0.173
0.101
0.107
0.099
0.126
-
0.286
0.209
0.311
0.546

on CHAMELEON and COD10K-Test datasets) but there is
a 0.3% Sα decrease on CAMO-Test. Compared with SINet,
Em
φ ↑ takes both local and global structure similarity into
consideration. As we can see, our BASNet achieves even
greater improvements (6.1%, 2.4% and 4.6% on the three
datasets, respectively) against the second best model in
Em

φ ↑ than in Sα ↑.

Qualitative Evaluation: Qualitative comparisons against
several of the baseline models are illustrated in Fig. 11.
As we can see, our BASNet (the 3rd column) is able to
handle different types of challenging camouﬂaged cases
including complex foreground targets with low contrast
(the 1st row), targets with very thin foreground structures
(the 2nd and 5th row), targets occluded by ﬁne objects
(the 3rd row), targets with complicated boundaries (the
4th row), targets with extremely complex hollow structures
(the 5th row), multiple objects with low contrast (the 6th
row), etc. Compared with the results of other models, the
results of our BASNet demonstrates its excellent abilities
of perceiving ﬁne structures and complicated boundaries,
which also explains why our BASNet is able to achieve such
high boundary evaluation scores F b
β ↑ on camouﬂage object
segmentation datasets (see Table 5).

4.4.3 Failure Cases

Although our BASNet outperforms other camouﬂaged ob-
ject segmentation (COS) models and rarely produces com-
pletely incorrect results, there are still some false negative
(the 1st row in Figure 10) and false positive predictions (the
2nd row in Fig. 10) in many of the COS cases. It is worth not-
ing that other models usually have the same or even worse
results on these challenging cases. Although these failure
cases may not have a huge impact on evaluation metrics,
they will somehow limit the applications and degrade the
user experiences.

5 APPLICATIONS

Thanks to the high accuracy, fast speed and simple ar-
chitecture of our network, we developed two real-world
applications based on BASNet: AR COPY & PASTE and
OBJECT CUT. These two applications further demonstrate
the effectiveness and efﬁciency of our BASNet.

(a) Image

(b) GT

(c) BASNet

Fig. 10. Failure cases on camouﬂaged object segmentation
task. The ﬁrst row shows the typical false negative artifacts.
The second row illustrates the false positive phenomenon.

comparison, we use the same training sets as SINet [25].

4.4.2 Comparison with State-of-the-Arts

To validate the performance of the proposed BASNet on the
camouﬂaged object segmentation task, we compared BAS-
Net against 13 sate-of-the-art models, including FPN[64],
MaskRCNN[33], PSPNet[139], UNet++[146], PiCANet[68],
MSRCN[41], PFANet[143], HTC[9], PoolNet[66], ANet-
SRM[54], CPD[121], EGNet[140] and SINet[25]. For fair
comparison, the results of different models are either pro-
vided by the authors or obtained by re-training the model
with the default settings with same training data.

Quantitative Evaluation: The quantitative evaluation
results are illustrated in Table 5. As we can see that our
BASNet achieves the best performance in nearly all metrics
with great advantages. SINet is the second best model.
EGNet and CPD are competitive with each other and can
be ranked as the third and fourth best. Our BASNet im-
proves the weighted F-measure F w
β ↑ with large margins
(12.6%, 4.0% and 12.6% on CHAMELEON, CAMO-Test
and COD10K-Test respectively). Particularly, our BASNet
outperforms the second best model SINet by 24.0%, 8.6%
and 23.5% in terms of the relax boundary F-measure F b
β ↑
on CHAMELEON, CAMO-Test and COD10K-Test datasets.
This reveals the effectiveness and accuracy of our BASNet
in capturing boundaries and ﬁne structures. In terms of the
M ↓, our BASNet reduces the metric by 50.0%, 4.0% and
25.5% on the three datasets, respectively. For the structure
measures, Sα ↑, the improvements of our BASNet against
the second best model are also signiﬁcant (4.5% and 3.1%

SUBMITTED TO IEEE XXXX

13

(a) Image

(b) GT

(c) BASNet

(d) SINet

(e) EGNet

(f) PoolNet

(g) CPD

(h) PiCANet

Fig. 11. Qualitative comparison on camouﬂaged object segmentation datasets. See § 4.4.2 for details.

5.1 Application I: AR COPY & PASTE

Introduced in HCI by Larry Tesler in the early 70s [107],
cut/copy-paste has become essential to many applications
of modern computing. In this section, we explore how
BASNet can be used to apply this principle to the mobile
phone camera and seamlessly transfer visual elements be-
tween the physical space and the digital space. AR COPY
& PASTE is a prototype that we built upon our BASNet to
conveniently capture real-world items using the camera of
a mobile phone (e.g. objects, people, drawings, schematics,
etc), automatically remove the background, and insert the
result in an editing software on the computer by pointing
the camera at it, as shown in Fig. 12. Speciﬁcally, AR COPY
& PASTE ﬁrst removes the background of the photo and
only shows the foreground salient target on the mobile
phone screen. Then users can “paste” the segmented target
by moving the cellphone to point the mobile camera at a
speciﬁc location of a document opened on a computer. The
whole process of AR COPY & PASTE makes it seem like the
real-world target is “copied” and “pasted” into a document,
which provides a novel and inspiring interactive experience.
A demonstration video4 of the prototype has been released
along with the source code.5 Both have received world-wide
attention (millions of views for the video, tens of thousands
of Github stars for the source code). Hundreds of thousands
of people have subscribed to the private beta.

5.1.1 Workﬂow of AR COPY & PASTE

From the perspective of users, our AR COPY & PASTE only
consists of two main steps: “copy” and “paste”, as shown
in Fig. 13. (1) Copy. The ﬁrst step consists in pointing the

4. https://twitter.com/cyrildiagne/status/1256916982764646402
5. https://github.com/cyrildiagne/ar-cutpaste

(a) Copy

(b) Move

(c) Paste

Fig. 12. Screenshots from the video demonstration. (a) Copy:
Point and tap to “copy” the object by masking its back-
ground out using BASNet. (b) Move: Move the mobile
phone, where the “copied” object is shown, to target at
the computer screen at a speciﬁc location. (c) Paste: Tap to
“paste” the “copied” object into the current document.

mobile camera at a subject and tapping the screen to take
a picture. BASNet is then used to hide all the pixels that
are not part of the main foreground subject. The remaining
pixels keep ﬂoating on top of the camera and provide a
preview of the paste result. Compared to other methods
like image segmentation [77], BASNet can produce very
accurate segmentation results with sharp and high-quality
boundaries, which is essential in many image composition
workﬂows. (2) Paste. The second step consists of pointing
the mobile phone at a speciﬁc location on the computer
screen and tapping to “paste” the “copied” subject. SIFT
[71] (implemented in OpenCV [4]) is used to ﬁnd the
corresponding computer screen coordinates targeted by the
center of the mobile phone camera. The image containing
the background removed target is ﬁnally imported in an

SUBMITTED TO IEEE XXXX

14

TABLE 6. Number of operations of different methods for image capturing and masking out.

Methods

Cross-platform
(Android v11 and macOS v10.15)

Constructor-speciﬁc
(iOS v13 and macOS v10.15)

AR COPY & PASTE (Ours)

Number of steps (on mobile)

6
1(cid:13) Take a photo,
2(cid:13) Tap the thumbnail,
3(cid:13) Tap share,
4(cid:13) Tap more,
5(cid:13) Select Bluetooth,
6(cid:13) Tap the destination on a device.
5
1(cid:13) Take a photo,
2(cid:13) Tap the thumbnail,
3(cid:13) Tap share,
4(cid:13) Tap “AirDrop”,
5(cid:13) Tap the destination on a device.
2
1(cid:13) Take a photo,
2(cid:13) Tap toward the destination

software.

Number of steps (on desktop)

5
1(cid:13) Click “Open” on the Bluetooth notiﬁcation,
2(cid:13) Export image,
3(cid:13) Select destination software,
4(cid:13) Use “Select Object” Tool,
5(cid:13) Apply mask.

5
1(cid:13) Click “Open” on the AirDrop notiﬁcation,
2(cid:13) Export image,
3(cid:13) Select destination software,
4(cid:13) Use “Select Object” Tool,
5(cid:13) Apply mask.

0

Fig. 13. Schematic of the AR COPY & PASTE ﬂow.

image editing software at the computed screen coordinates
to create the ﬁnal composition.

Fig. 15. Sample results given by the OBJECT CUT API: the
ﬁrst row shows the input images and the second row shows
the background removed results.

5.1.2 Implementation Details

Fig. 14 illustrates the overall implementation pipeline of the
AR COPY & PASTE prototype, which consists of three main
parts: Kubernetes cluster, mobile application and desktop
application. The AR COPY & PASTE prototype was built
using our BASNet model trained on DUTS-TR [110]. To
make sure that it runs smoothly on mobile device, it has
been wrapped as an HTTP service/container image6 so that
it can be deployed easily on remote GPUs using Kubernetes
7. Hence, photos taken by mobile devices in AR COPY
& PASTE are sent to the remote server to obtain their
segmentation masks. The desktop application is a python
HTTP server which takes three ﬁles from the mobile ap-
plication as input (original picture, BASNet mask, photo of
the screen) and runs SIFT feature matching and perspective
transformation based on the photo of the screen and the
screenshot to determine the destination coordinates. Finally,
the desktop application is responsible for sending javascript
commands to desktop apps like Photoshop8 in order to
import the image into the document at the right position.

5.1.3 Comparison with Other Methods

Our AR COPY & PASTE prototype applies BASNet in
a novel human-computer-interaction setting, which makes
the process easier and faster than other methods (two oper-
ations for our method versus 10 or 11 operations for other
methods). Table 6 illustrates examples of typical user ﬂow

Fig. 14. Overall pipeline of the AR COPY & PASTE.

6. https://github.com/cyrildiagne/basnet-http
7. https://github.com/kubernetes/kubernetes
8. https://www.photoshop.com/en

SUBMITTED TO IEEE XXXX

15

Fig. 16. OBJECT CUT pipeline.

to clip and import an object from a mobile devices to an
desktop image editing software, such as Adobe Photoshop.
As we can see, our prototype greatly reduces the numbers
of operations and simpliﬁes the process. Besides, our AR
COPY & PASTE allows users to delegate some of the lower-
level decisions (how visible each pixel should be), and
focus on the higher-level objectives (how do they want the
object to look). Removing these tasks lowers the barrier to
entry (there is no need to learn how to paint masks in an
image editing software), saves a signiﬁcant amount of time,
and ultimately leads to better end results by removing the
cognitive load of the low-level tasks [5].

5.2 Application II: OBJECT CUT

OBJECT CUT is an online image background removal ser-
vice that uses BASNet. Removing the background from
an image is a common operation in the daily work of
professional photographers and image editors. This process
is usually a repeatable and manual task that requires a lot
of effort and time. However, thanks to BASNet, one of the
most robust and fastest performing deep learning models
for image segmentation, OBJECT CUT was able to turn
it into an easy and automatic process. The program was
built as an API to make it as easy as possible to integrate.
APIs, also known as Application Programming Interfaces,
are already commonly used to integrate different types of
solutions to improve systems without actually knowing
what is happening inside. For instance, RESTful APIs are
a standard in the software engineering ﬁeld for designing
and specifying APIs. Making it substantially easier to adapt
desired APIs to speciﬁc workﬂows.

Our system is based on three well-distinguished parts,
as shown in Fig. 16: 1) the API Handler, responsible for
receiving and validating clients requests, downloading and
preprocessing input images and sending those to the model;
2) BASNet, responsible for performing salient object detec-
tion. 3) Once the output from the network is generated,
the postprocessing module applies an unsharp masking
algorithm and morphological operations to improve the
quality of the output. Afterward, OBJECT CUT uploads the
cropped image to the Google Cloud Storage and returns its
public URL to the client. This is structured as-is in order
to isolate different parts, such as the BASNet component,
removing all the API parameter validations as well as image
download and upload processes, as much as possible. In this
scenario, OBJECT CUT maximizes the operations running
on the BASNet thread. The whole stack from the API is
running under Docker containers, all managed by the cloud
native application proxy called Traeﬁk. The usage of Traeﬁk
here allows us to have a production-ready deployment

making easy, from the containers’ perspective, to commu-
nicate with other processes. In addition, we have a Load
Balancer system in place to enable each of the components
to scale more easily. The source code for this pipeline
can be found under the OBJECT CUT GitHub repository:
https://github.com/AlbertSuarez/object-cut.

To ensure easy integration, it is publicly available at
RapidAPI9, the biggest API marketplace in the world, and it
has been effectively utilized by people and companies from
86 different countries around the globe, including China,
United States, Canada, India, Spain, Russia, etc. OBJECT
CUT was born to power up the designing and image edit-
ing process for the people who work with images daily.
Integrating the OBJECT CUT API removes the necessity of
understanding the complex inner workings behind it and
automates the process of removing the background from
images in a matter of seconds. See examples in Fig. 15.

6 CONCLUSION
In this paper, we proposed a novel end-to-end boundary-
aware model, BASNet, and a hybrid fusion loss for accurate
image segmentation. The proposed BASNet is a predict-
reﬁne architecture, which consists of two components: a pre-
diction network and a reﬁnement module. Combined with
the hybrid loss, BASNet is able to capture both large-scale
and ﬁne structures, e.g. thin regions, holes, and produce
segmentation probability maps with highly accurate bound-
aries. Experimental results on six salient object segmentation
datasets, one salient object in clutter dataset and three
camouﬂaged object segmentation datasets demonstrate that
our model achieves very competitive performance in terms
of both region-based and boundary-aware measures against
other models. Additionally, the proposed network architec-
ture is modular. It can be easily extended or adapted to
other tasks by replacing either the prediction network or
the reﬁnement module. The two (close to) commercial ap-
plications, AR COPY & PASTE and OBJECT CUT, based on
our BASNet not only prove the effectiveness and efﬁciency
of our model but also provide two practical tools for reduc-
ing the workload in real-world production scenarios. The
world-wide impacts of these two applications indicates the
huge demand for highly accurate segmentation approaches,
which motivates us to explore more accurate and reliable
segmentation models.

REFERENCES

[1]

[2]

[3]

[4]

[5]

[6]

Amir A Amini, Terry E Weymouth, and Ramesh C Jain. Using dy-
namic programming for solving variational problems in vision.
TPAMI, (9):855–867, 1990.
Arnon Amir and Michael Lindenbaum. A generic grouping
algorithm and its quantitative analysis. TPAMI, 20(2):168–185,
1998.
Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Seg-
net: A deep convolutional encoder-decoder architecture for im-
age segmentation. TPAMI, (12):2481–2495, 2017.
G. Bradski. The OpenCV Library. Dr. Dobb’s Journal of Software
Tools, 2000.
Shan Carter and Michael Nielsen. Using artiﬁcial intelligence to
augment human intelligence. Distill, 2(12):e9, 2017.
Vicent Caselles, Francine Catt´e, Tomeu Coll, and Franc¸oise Dibos.
A geometric model for active contours in image processing.
Numerische mathematik, 66(1):1–31, 1993.

9. https://rapidapi.com/objectcut.api/api/background-removal

SUBMITTED TO IEEE XXXX

16

[7]

[8]

[9]

Vicent Caselles, Ron Kimmel, and Guillermo Sapiro. Geodesic
active contours. IJCV, 22(1):61–79, 1997.
Lluis Castrejon, Kaustav Kundu, Raquel Urtasun, and Sanja
Fidler. Annotating object instances with a polygon-rnn. In CVPR,
pages 4485–4493, 2017.
Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaoxiao Li,
Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping Shi, Wanli
Ouyang, et al. Hybrid task cascade for instance segmentation.
In CVPR, pages 4974–4983, 2019.

[10] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin
Murphy, and Alan L. Yuille. Semantic image segmentation with
deep convolutional nets and fully connected crfs. 2015.

[11] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin
Murphy, and Alan L Yuille. Deeplab: Semantic image segmenta-
tion with deep convolutional nets, atrous convolution, and fully
connected crfs. TPAMI, 40(4):834–848, 2017.

[13]

[12] Liang-Chieh Chen, George Papandreou, Florian Schroff, and
Hartwig Adam. Rethinking atrous convolution for semantic
image segmentation. arXiv preprint arXiv:1706.05587, 2017.
Shuhan Chen, Xiuli Tan, Ben Wang, and Xuelong Hu. Reverse
In ECCV, pages 236–252,
attention for salient object detection.
2018.
Shuhan Chen, Xiuli Tan, Ben Wang, Huchuan Lu, Xuelong Hu,
and Yun Fu. Reverse attention-based residual network for salient
object detection. TIP, 29:3763–3776, 2020.

[14]

[15] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Re-
hfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan
Roth, and Bernt Schiele. The cityscapes dataset for semantic
urban scene understanding. In CVPR, pages 3213–3223, 2016.
Ingemar J Cox, Satish B Rao, and Yu Zhong. ” ratio regions”:
a technique for image segmentation. In ICPR, volume 2, pages
557–564, 1996.

[16]

[17] Pieter-Tjerk de Boer, Dirk P. Kroese, Shie Mannor, and Reuven Y.
Rubinstein. A tutorial on the cross-entropy method. Annals OR,
134(1):19–67, 2005.

[18] Zijun Deng, Xiaowei Hu, Lei Zhu, Xuemiao Xu, Jing Qin, Guo-
qiang Han, and Pheng-Ann Heng. R3net: Recurrent residual
reﬁnement network for saliency detection. pages 684–690. IJCAI,
2018.

[20]

[19] Marc Ehrig and J´er ˆome Euzenat. Relaxed precision and recall for
ontology matching. In Proc. K-Cap 2005 workshop on Integrating
ontology, pages 25–32. No commercial editor., 2005.
James Elder and Steven Zucker. The effect of contour closure
on the rapid discrimination of two-dimensional shapes. Vision
Research, 33(7):981–991, 1993.
James H. Elder, Amnon Krupnik, and Leigh A. Johnston. Contour
grouping with prior models. TPAMI, 25(6):661–674, 2003.
[22] Deng-Ping Fan, Ming-Ming Cheng, Yun Liu, Tao Li, and Ali Borji.
Structure-measure: A new way to evaluate foreground maps. In
ICCV, pages 4558–4567, 2017.

[21]

[23] Deng-Ping Fan, Cheng Gong, Yang Cao, Bo Ren, Ming-Ming
Cheng, and Ali Borji. Enhanced-alignment measure for binary
foreground map evaluation. In IJCAI, pages 698–704, 2018.
[24] Deng-Ping Fan, Ming-Ming Cheng, Jiang-Jiang Liu, Shang-Hua
Gao, Qibin Hou, and Ali Borji. Salient objects in clutter: Bringing
salient object detection to the foreground. In ECCV, 2018.
[25] Deng-Ping Fan, Ge-Peng Ji, Guolei Sun, Ming-Ming Cheng, Jian-
bing Shen, and Ling Shao. Camouﬂaged object detection.
In
CVPR, pages 2777–2787, 2020.

[26] Mengyang Feng, Huchuan Lu, and Errui Ding. Attentive feed-
back network for boundary-aware salient object detection.
In
CVPR, pages 1623–1632, 2019.

[27] Lucas Fidon, Wenqi Li, Luis C. Herrera, Jinendra Ekanayake, Neil
Kitchen, S´ebastien Ourselin, and Tom Vercauteren. Generalised
wasserstein dice score for imbalanced multi-class segmentation
using holistic convolutional networks. In MICCAI-W, pages 64–
76, 2017.

[28] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urta-
sun. Vision meets robotics: The kitti dataset. IJRR, 32(11):1231–
1237, 2013.

[29] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik.
Rich feature hierarchies for accurate object detection and seman-
tic segmentation. In CVPR, pages 580–587, 2014.

[30] Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty
of training deep feedforward neural networks. In AISTATS, pages
249–256, 2010.
Stas Goferman, Lihi Zelnik-Manor, and Ayellet Tal. Context-
aware saliency detection. TPAMI, 34(10):1915–1926, 2012.
[32] Richard HR Hahnloser and H Sebastian Seung. Permitted and
forbidden sets in symmetric threshold-linear networks. In NIPS,

[31]

pages 217–223, 2001.

[33] Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross Girshick.

Mask r-cnn. In ICCV, pages 2961–2969, 2017.

[34] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Spa-
tial pyramid pooling in deep convolutional networks for visual
recognition. In ECCV, pages 346–361. Springer, 2014.

[35] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep
residual learning for image recognition. In CVPR, pages 770–778,
2016.

[36] Qibin Hou, Ming-Ming Cheng, Xiaowei Hu, Ali Borji, Zhuowen
Tu, and Philip Torr. Deeply supervised salient object detection
with short connections. In CVPR, pages 5300–5309, 2017.
[37] Ping Hu, Bing Shuai, Jun Liu, and Gang Wang. Deep level sets

for salient object detection. In CVPR, volume 1, page 2, 2017.

[38] Xiaowei Hu, Chi-Wing Fu, Lei Zhu, Tianyu Wang, and Pheng-
Ann Heng. Sac-net: Spatial attenuation context for salient object
detection. TCSVT, 2020.

[39] Xiaowei Hu, Lei Zhu, Jing Qin, Chi-Wing Fu, and Pheng-Ann
Heng. Recurrently aggregating deep features for salient object
detection. In AAAI, pages 6943–6950, 2018.

[40] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q
In
Weinberger. Densely connected convolutional networks.
CVPR, pages 2261–2269, 2017.

[41] Zhaojin Huang, Lichao Huang, Yongchao Gong, Chang Huang,
and Xinggang Wang. Mask scoring r-cnn. In CVPR, pages 6409–
6418, 2019.
Sergey Ioffe and Christian Szegedy. Batch normalization: Acceler-
ating deep network training by reducing internal covariate shift.
ICML, pages 448–456, 2015.

[42]

[43] Md Amirul Islam, Mahmoud Kalash, Mrigank Rochan, Neil DB
Bruce, and Yang Wang. Salient object detection using a context-
aware reﬁnement network. In BMVC, 2017.

[44] Paul Jaccard. The distribution of the ﬂora in the alpine zone. 1.

New phytologist, 11(2):37–50, 1912.

[45] Martin J¨agersand. Saliency maps and attention selection in scale
and spatial coordinates: An information theoretic approach. In
ICCV, pages 195–202, 1995.
[46] Timor Kadir and Michael Brady.

Saliency, scale and image
description. International Journal of Computer Vision, 45(2):83–105,
2001.

[47] G. Kanizsa. Organization in Vision. New York: Praeger 1979.
[48] Michael Kass, Andrew Witkin, and Demetri Terzopoulos. Snakes:

Active contour models. IJCV, 1(4):321–331, 1988.

[49] Diederik P Kingma and Jimmy Ba. Adam: A method for stochas-

[50]

[51]

tic optimization. ICLR, 2015.
Srinivas SS Kruthiventi, Vennela Gudisa, Jaley H Dholakiya, and
R Venkatesh Babu. Saliency uniﬁed: A deep architecture for
simultaneous eye ﬁxation prediction and salient object segmen-
tation. In CVPR, pages 5781–5790, 2016.
Jason Kuen, Zhenhua Wang, and Gang Wang. Recurrent atten-
tional networks for saliency detection. In CVPR, pages 3668–3677,
2016.

[52] M Pawan Kumar, PHS Ton, and Andrew Zisserman. Obj cut. In

CVPR, volume 1, pages 18–25, 2005.

[53] Lubor Ladicky, Chris Russell, Pushmeet Kohli, and Philip HS
Torr. Graph cut based inference with co-occurrence statistics. In
ECCV, pages 239–253, 2010.

[54] Trung-Nghia Le, Tam V Nguyen, Zhongliang Nie, Minh-Triet
Tran, and Akihiro Sugimoto. Anabranch network for camou-
ﬂaged object segmentation. CVIU, 184:45–56, 2019.

[55] Trung-Nghia Le, Vuong Nguyen, Cong Le, Tan-Cong Nguyen,
Minh-Triet Tran, and Tam V Nguyen. Camouﬁnder: Finding
camouﬂaged instances in images. In AAAI, 2021.

[56] Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang,
and Zhuowen Tu. Deeply-supervised nets. In Artiﬁcial Intelligence
and Statistics, pages 562–570, 2015.

[57] Hyemin Lee and Daijin Kim. Salient region-based online object

tracking. In WACV, pages 1170–1177, 2018.

[58] Guanbin Li and Yizhou Yu. Visual saliency based on multiscale

deep features. In CVPR, pages 5455–5463, 2015.

[59] Guanbin Li and Yizhou Yu. Deep contrast learning for salient

object detection. In CVPR, pages 478–487, 2016.

[60] Guanbin Li and Yizhou Yu. Visual saliency detection based on
multiscale deep cnn features. TIP, 25(11):5012–5024, 2016.
[61] Xin Li, Fan Yang, Hong Cheng, Wei Liu, and Dinggang Shen.
Contour knowledge transfer for salient object detection. In ECCV,
pages 370–385, 2018.

[62] Yin Li, Xiaodi Hou, Christof Koch, James M Rehg, and Alan L
Yuille. The secrets of salient object segmentation. In CVPR, pages
280–287, 2014.

[63] Guosheng Lin, Anton Milan, Chunhua Shen, and Ian Reid.

SUBMITTED TO IEEE XXXX

17

Reﬁnenet: Multi-path reﬁnement networks for high-resolution
semantic segmentation. In CVPR, pages 1925–1934, 2017.
[64] Tsung-Yi Lin, Piotr Doll´ar, Ross Girshick, Kaiming He, Bharath
Hariharan, and Serge Belongie. Feature pyramid networks for
object detection. In CVPR, pages 2117–2125, 2017.

[66]

[65] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro
Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence Zitnick.
Microsoft coco: Common objects in context. In ECCV, pages 740–
755. Springer, 2014.
Jiang-Jiang Liu, Qibin Hou, Ming-Ming Cheng, Jiashi Feng, and
Jianmin Jiang. A simple pooling-based design for real-time
salient object detection. In CVPR, pages 3917–3926, 2019.
[67] Nian Liu and Junwei Han. Dhsnet: Deep hierarchical saliency
In CVPR, pages 678–686,

network for salient object detection.
2016.

[68] Nian Liu, Junwei Han, and Ming-Hsuan Yang. Picanet: Learning
pixel-wise contextual attention for saliency detection. In CVPR,
pages 3089–3098, 2018.

[69] Nian Liu, Junwei Han, Dingwen Zhang, Shifeng Wen, and Tian-
ming Liu. Predicting eye ﬁxations using convolutional neural
networks. In CVPR, pages 362–370, 2015.
Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully con-
volutional networks for semantic segmentation. In CVPR, pages
3431–3440, 2015.

[70]

[71] David G Lowe. Distinctive image features from scale-invariant

keypoints. IJCV, 60(2):91–110, 2004.

[72] Zhiming Luo, Akshaya Mishra, Andrew Achkar, Justin Eichel,
Shaozi Li, and Pierre-Marc Jodoin. Non-local deep features for
salient object detection. In CVPR, pages 6593–6601, 2017.
Shyjan Mahamud, Lance R. Williams, Karvel K. Thornber, and
Kanglin Xu. Segmentation of multiple salient closed contours
from real images. TPAMI, 25(4):433–444, 2003.

[73]

[74] Ran Margolin, Lihi Zelnik-Manor, and Ayellet Tal. How to

evaluate foreground maps. CVPR, pages 248–255, 2014.

[75] Gell´ert M´attyus, Wenjie Luo, and Raquel Urtasun. Deep-
roadmapper: Extracting road topology from aerial images.
In
ICCV, pages 3458–3466, 2017.

[76] Roey Mechrez, Eli Shechtman, and Lihi Zelnik-Manor. Saliency

[77]

driven image manipulation. In WACV, pages 1368–1376, 2018.
Shervin Minaee, Yuri Boykov, Fatih Porikli, Antonio Plaza,
Nasser Kehtarnavaz, and Demetri Terzopoulos.
Image seg-
arXiv preprint
mentation using deep learning: A survey.
arXiv:2001.05566, 2020.

[78] Vida Movahedi and James H Elder. Design and perceptual vali-
dation of performance measures for salient object segmentation.
In CVPR-W, pages 49–56, 2010.

[79] David Mumford and Jayant Shah. Optimal approximations by
piecewise smooth functions and associated variational problems.
CPAM, 42(5):577–685, 1989.

[80] Gattigorla Nagendar, Digvijay Singh, Vineeth N. Balasubrama-
nian, and C. V. Jawahar. Neuro-iou: Learning a surrogate loss for
semantic segmentation. In BMVC, page 278, 2018.

[81] Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hourglass
networks for human pose estimation. In ECCV, pages 483–499,
2016.
Stanley Osher and James A Sethian. Fronts propagating with
curvature-dependent speed: algorithms based on hamilton-jacobi
formulations. Journal of computational physics, 79(1):12–49, 1988.

[82]

[83] Youwei Pang, Xiaoqi Zhao, Lihe Zhang, and Huchuan Lu. Multi-
scale interactive network for salient object detection. In CVPR,
pages 9413–9422, 2020.

[84] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan,
Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison,
Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. In NIPS-W, 2017.

[85] Chao Peng, Xiangyu Zhang, Gang Yu, Guiming Luo, and Jian
Sun. Large kernel matters—improve semantic segmentation by
global convolutional network. In CVPR, pages 1743–1751, 2017.
[86] Federico Perazzi, Philipp Kr¨ahenb ¨uhl, Yael Pritch, and Alexander
Hornung. Saliency ﬁlters: Contrast based ﬁltering for salient
region detection. In CVPR, pages 733–740, 2012.

[87] Xuebin Qin. Visual salient object detection: Interactive, unsuper-
vised and supervised methods. Doctoral dissertation, 2020.
[88] Xuebin Qin, Shida He, Camilo Perez Quintero, Abhineet Singh,
Masood Dehghan, and Martin Jagersand. Real-time salient closed
boundary tracking via line segments perceptual grouping.
In
IROS, pages 4284–4289, 2017.

[89] Xuebin Qin, Shida He, Camilo Perez Quintero, Abhineet Singh,
Masood Dehghan, and Martin J¨agersand. Real-time salient closed
In
boundary tracking via line segments perceptual grouping.

IROS, pages 4284–4289, 2017.

[90] Xuebin Qin, Shida He, Xiucheng Yang, Masood Dehghan, Qim-
ing Qin, and Martin Jagersand. Accurate outline extraction of
individual building from very high-resolution optical images.
GRSL, (99):1–5, 2018.

[91] Xuebin Qin, Shida He, Zichen Zhang, Masood Dehghan, and
Martin Jagersand. Bylabel: A boundary based semi-automatic
image annotation tool. In WACV, pages 1804–1813, 2018.
[92] Xuebin Qin, Shida He, Zichen Vincent Zhang, and Masood
Dehghan. Real-time salient closed boundary tracking using
perceptual grouping and shape priors. In BMVC, 2017.

[93] Xuebin Qin, Shida He, Zichen Vincent Zhang, Masood Dehghan,
and Martin J¨agersand. Real-time salient closed boundary track-
ing using perceptual grouping and shape priors. In BMVC, 2017.
[94] Xuebin Qin, Zichen Zhang, Chenyang Huang, Masood De-
hghan, Osmar R Zaiane, and Martin Jagersand. U2-net: Going
deeper with nested u-structure for salient object detection. PR,
106:107404, 2020.

[95] Xuebin Qin, Zichen Zhang, Chenyang Huang, Chao Gao, Ma-
sood Dehghan, and Martin Jagersand. Basnet: Boundary-aware
salient object detection. In CVPR, pages 7479–7489, 2019.
[96] Md Atiqur Rahman and Yang Wang. Optimizing intersection-
over-union in deep neural networks for image segmentation. In
ISVC, pages 234–244. Springer, 2016.

[97] Xiaofeng Ren, Charless C. Fowlkes, and Jitendra Malik. Scale-
invariant contour completion using conditional random ﬁelds.
In ICCV, pages 1214–1221, 2005.

[98] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:
Convolutional networks for biomedical image segmentation. In
MICCAI, pages 234–241. Springer, 2015.

[99] Bryan C. Russell, Antonio Torralba, Kevin P. Murphy, and
William T. Freeman. Labelme: A database and web-based tool
for image annotation. IJCV, 77(1-3):157–173, 2008.

[100] Sudeep Sarkar and Padmanabhan Soundararajan. Supervised
learning of large perceptual organization: Graph spectral parti-
tioning and learning automata. TPAMI, 22(5):504–525, 2000.
[101] Jianbo Shi and Jitendra Malik. Normalized cuts and image

segmentation. Departmental Papers (CIS), page 107, 2000.

[102] Karen Simonyan and Andrew Zisserman. Very deep convolu-

tional networks for large-scale image recognition. ICLR, 2015.

[103] Przemysław Skurowski, Hassan Abdulameer, Jakub Błaszczyk,
Tomasz Depta, Adam Kornacki, and Przemysław Kozieł. Animal
camouﬂage analysis: Chameleon database. 2018.

[104] Jinming Su, Jia Li, Yu Zhang, Changqun Xia, and Yonghong
Tian. Selectivity or invariance: Boundary-aware salient object
detection. In ICCV, pages 3799–3808, 2019.

[105] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott
Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke,
and Andrew Rabinovich. Going deeper with convolutions.
In
CVPR, pages 1–9, 2015.

[106] Zhiqiang Tang, Xi Peng, Shijie Geng, Lingfei Wu, Shaoting
Zhang, and Dimitris Metaxas. Quantized densely connected u-
nets for efﬁcient landmark localization. In ECCV, 2018.

[107] Larry Tesler. A personal history of modeless text editing and

cut/copy-paste. interactions, 19(4):70–75, 2012.

[108] Luc Vincent and Pierre Soille. Watersheds in digital spaces: an
efﬁcient algorithm based on immersion simulations. TPAMI,
(6):583–598, 1991.

[109] Lijun Wang, Huchuan Lu, Xiang Ruan, and Ming-Hsuan Yang.
Deep networks for saliency detection via local estimation and
global search. In CVPR, pages 3183–3192, 2015.

[110] Lijun Wang, Huchuan Lu, Yifan Wang, Mengyang Feng, Dong
Wang, Baocai Yin, and Xiang Ruan. Learning to detect salient
objects with image-level supervision. In CVPR, pages 136–145,
2017.

[111] Linzhao Wang, Lijun Wang, Huchuan Lu, Pingping Zhang, and
Xiang Ruan. Salient object detection with recurrent fully convo-
lutional networks. TPAMI, 2018.

[112] Song Wang and Jeffrey Mark Siskind. Image segmentation with

ratio cut. TPAMI, 25(6):675–690, 2003.

[113] Tiantian Wang, Ali Borji, Lihe Zhang, Pingping Zhang, and
Huchuan Lu. A stagewise reﬁnement model for detecting salient
objects in images. In ICCV, pages 4039–4048, 2017.

[114] Tinghuai Wang, Bo Han, and John P. Collomosse. Touchcut: Fast
image and video segmentation using single-touch interaction.
CVIU, 120:14–30, 2014.

[115] Tiantian Wang, Lihe Zhang, Shuo Wang, Huchuan Lu, Gang
Yang, Xiang Ruan, and Ali Borji. Detect globally, reﬁne locally: A
novel approach to saliency detection. In CVPR, pages 3127–3135,
2018.

SUBMITTED TO IEEE XXXX

18

[116] Wenguan Wang, Qiuxia Lai, Huazhu Fu, Jianbing Shen, Haibin
Ling, and Ruigang Yang. Salient object detection in the deep
learning era: An in-depth survey. arXiv preprint arXiv:1904.09146,
2019.

[117] Zhou Wang, Eero P Simoncelli, and Alan C Bovik. Multiscale
In ACSSC,

structural similarity for image quality assessment.
volume 2, pages 1398–1402, 2003.

[118] Donna J. Williams and Mubarak Shah. A fast algorithm for active

contours. In ICCV, pages 592–595, 1990.

[119] Runmin Wu, Mengyang Feng, Wenlong Guan, Dong Wang,
Huchuan Lu, and Errui Ding. A mutual learning method for
salient object detection with intertwined multi-supervision.
In
CVPR, pages 8150–8159, 2019.

[120] Zhenyu Wu and Richard Leahy. An optimal graph theoretic
approach to data clustering: Theory and its application to image
segmentation. TPAMI, (11):1101–1113, 1993.

[121] Zhe Wu, Li Su, and Qingming Huang. Cascaded partial decoder
In CVPR, pages

for fast and accurate salient object detection.
3907–3916, 2019.

[122] Zhe Wu, Li Su, and Qingming Huang. Stacked cross reﬁnement
network for edge-aware salient object detection. In ICCV, pages
7263–7272, 2019.

[123] Saining Xie and Zhuowen Tu. Holistically-nested edge detection.

In ICCV, pages 1395–1403, 2015.

[124] Chenyang Xu, Jerry L Prince, et al. Snakes, shapes, and gradient

vector ﬂow. TIP, 7(3):359–369, 1998.

[125] Ning Xu, Brian Price, Scott Cohen, and Thomas Huang. Deep

image matting. In CVPR, pages 2970–2979, 2017.

[126] Jinnan Yan, Trung-Nghia Le, Khanh-Duy Nguyen, Minh-Triet
Tran, Thanh-Toan Do, and Tam V Nguyen. Mirrornet: Bio-
inspired adversarial attack for camouﬂaged object segmentation.
arXiv preprint arXiv:2007.12881, 2020.

[127] Qiong Yan, Li Xu, Jianping Shi, and Jiaya Jia. Hierarchical

saliency detection. In CVPR, pages 1155–1162, 2013.

[128] Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan, and Ming-
Hsuan Yang. Saliency detection via graph-based manifold rank-
ing. In CVPR, pages 3166–3173, 2013.

[129] Fisher Yu and Vladlen Koltun. Multi-scale context aggregation

by dilated convolutions. ICLR, 2016.

[130] Zhiding Yu, Chen Feng, Ming-Yu Liu, and Srikumar Rama-
lingam. Casenet: Deep category-aware semantic edge detection.
In CVPR, pages 5964–5973, 2017.

[131] Yu Zeng, Yunzhi Zhuge, Huchuan Lu, Lihe Zhang, Mingyang
Qian, and Yizhou Yu. Multi-source weak supervision for saliency
detection. In CVPR, pages 6074–6083, 2019.

[132] Jing Zhang, Deng-Ping Fan, Yuchao Dai, Saeed Anwar, Fatemeh
Saleh, Sadegh Aliakbarian, and Nick Barnes. Uncertainty in-
spired rgb-d saliency detection. CVPR, pages 8579–8588, 2020.

[133] Lu Zhang, Ju Dai, Huchuan Lu, You He, and Gang Wang. A bi-
directional message passing model for salient object detection. In
CVPR, pages 1741–1750, 2018.

[134] Lu Zhang, Jianming Zhang, Zhe Lin, Huchuan Lu, and You
He. Capsal: Leveraging captioning to boost semantics for salient
object detection. In CVPR, pages 6024–6033, 2019.

[135] Pingping Zhang, Wei Liu, Huchuan Lu, and Chunhua Shen.
In IJCAI,

Salient object detection by lossless feature reﬂection.
pages 1149–1155, 2018.

[136] Pingping Zhang, Dong Wang, Huchuan Lu, Hongyu Wang, and
Xiang Ruan. Amulet: Aggregating multi-level convolutional
features for salient object detection. In ICCV, pages 202–211, 2017.
[137] Pingping Zhang, Dong Wang, Huchuan Lu, Hongyu Wang,
and Baocai Yin. Learning uncertain convolutional features for
accurate saliency detection. In ICCV, pages 212–221, 2017.
[138] Xiaoning Zhang, Tiantian Wang, Jinqing Qi, Huchuan Lu, and
Gang Wang. Progressive attention guided recurrent network for
salient object detection. In CVPR, pages 714–722, 2018.

[139] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang,
and Jiaya Jia. Pyramid scene parsing network. In CVPR, pages
2881–2890, 2017.

[140] Jia-Xing Zhao, Jiang-Jiang Liu, Deng-Ping Fan, Yang Cao, Jufeng
Yang, and Ming-Ming Cheng. Egnet: Edge guidance network for
salient object detection. In ICCV, pages 8779–8788, 2019.
[141] Kai Zhao, Shanghua Gao, Qibin Hou, Dandan Li, and Ming-Ming
Cheng. Optimizing the f-measure for threshold-free salient object
detection. CoRR, abs/1805.07567, 2018.

[142] Rui Zhao, Wanli Ouyang, Hongsheng Li, and Xiaogang Wang.
In CVPR,

Saliency detection by multi-context deep learning.
pages 1265–1274, 2015.

[143] Ting Zhao and Xiangqian Wu. Pyramid feature attention network

for saliency detection. In CVPR, pages 3085–3094, 2019.

[144] Xiaoqi Zhao, Youwei Pang, Lihe Zhang, Huchuan Lu, and Lei

Zhang. Suppress and balance: A simple gated network for salient
object detection. 2020.

[145] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Bar-
Scene parsing through ade20k

riuso, and Antonio Torralba.
dataset. In CVPR, 2017.

[146] Zongwei Zhou, Md Mahfuzur Rahman Siddiquee, Nima
Tajbakhsh, and Jianming Liang. Unet++: A nested u-net archi-
tecture for medical image segmentation. In DLMIA, pages 3–11.
2018.

Xuebin QIN obtained his PhD degree from the University of Alberta,
Edmonton, Canada, in 2020. Since March, 2020, He is a Postdoctoral
Fellow in the Department of Computing Science and the Department of
Radiology and Diagnostic Imaging, University of Alberta, Canada. His
research interests include highly accurate image segmentation, salient
object detection, image labeling and detection. He has published about
10 papers in vision and robotics conferences such as CVPR, BMVC,
ICPR, WACV, IROS, etc.

Deng-Ping FAN received his PhD degree from the Nankai University in
2019. He joined Inception Institute of Artiﬁcial Intelligence (IIAI) in 2019.
He has published about 25 top journal and conference papers such as
CVPR, ICCV, ECCV, etc. His research interests include computer vision
and visual attention, especially on RGB salient object detection (SOD),
RGB-D SOD, Video SOD, Co-SOD. He won the Best Paper Finalist
Award at IEEE CVPR 2019, the Best Paper Award Nominee at IEEE
CVPR 2020.

Chenyang Huang obtained his M.Sc. degree from the University of
Alberta, Edmonton, Canada, in 2019. He is currently pursuing a Ph.D.
degree in the Department of Computing Science of the same university.
His research is mainly focusing on deep learning, natural language pro-
cessing, and computer vision. He has publications on some prestigious
conferences such as NAACL and CVPR.

Cyril Diagne is a designer and coder and a co-founder of Init ML, a
company that brings machine learning to production through practical
uses, such as ClipDrop. Cyril is a former Professor and Head of Media
& Interaction Design at ECAL (Lausanne University of Arts & Design,
Switzerland) where he continues to give regular workshops. In 2015, he
started a residency at Google Arts & Culture, where he helped kickstart
the Google Arts Experiments initiative and created multiple machine
learning projects such as the viral phenomenon Art Selﬁe.

Zichen Zhang is a Ph.D. student in Statistical Machine Learning at the
University of Alberta. He obtained his M.Sc degrees from Dalhousie
University and the University of Alberta and B.E degree from Huazhong
University of Science and Technology. He’s interested in machine learn-
ing and its applications in robotics perception and control.

Adri `a Cabeza Sant’Anna is a computer engineer who graduated at Uni-
versitat Polit `ecnica de Catalunya, BarcelonaTech in Computer Science.
His current position is Deep Learning Engineer at restb.ai, a Computer
Vision company for Real Estate based in Barcelona. Previously, he
worked as an Algorithmic Methods of Data Mining grader assistant
in the Department of Computer Science at Aalto University, Helsinki.
He was the president of the Student Representatives Association at
Barcelona School of Informatics. His research interests include machine
learning, computer vision, generative models, and data mining. He has
co-developed ObjectCut and participates in the organization of Hack-
UPC, the biggest student-run hackathon in Europe, located at Barcelona
School of Informatics.

Albert Su `arez is a software engineer who graduated at Universitat
Polit `ecnica de Catalunya, BarcelonaTech in Software Engineering. His
current position is Principal Software Engineer at restb.ai, a Computer
Vision company for Real Estate based in Barcelona, Spain. He was
the co-director of the biggest student-run hackathon in Europe, called
HackUPC, located at Barcelona School of Informatics.

Martin Jagersand’s research interests are in Robotics, Computer Vi-
sion, and Graphics, especially vision guided motion control and vision-
based human-robot interfaces. He studied physics at Chalmers Sweden
(MSc 1991). He was awarded a Fulbright fellowship for graduate studies
in the USA. He studied Computer Science at the Univ. of Rochester, NY
(MSc 1994, PhD 1997). He held an NSF CISE postdoc fellowship, at
Yale University, and then was a research faculty in the Engineering Re-
search Center for Surgical Systems and Technology at Johns Hopkins
University. He is now a faculty member at the University of Alberta.

SUBMITTED TO IEEE XXXX

19

Ling Shao is currently the CEO and the Chief Scientist of the Inception
Institute of Artiﬁcial Intelligence, Abu Dhabi, United Arab Emirates. He
is also the Executive Vice President and a Provost of the Mohamed bin
Zayed University of Artiﬁcial Intelligence. His current research interests
include computer vision, machine learning, and medical imaging. Dr.
Shao is a fellow of IAPR, IET, and BCS. He is an Associate Editor of the
IEEE TRANSACTIONS ON IMAGE PROCESSING, the IEEE TRANS-
ACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, and
several other top journals.

