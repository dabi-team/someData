1
2
0
2

n
a
J

0
3

]
I

N
.
s
c
[

1
v
5
4
1
0
0
.
2
0
1
2
:
v
i
X
r
a

Actor-Critic Learning Based QoS-Aware Scheduler
for Reconﬁgurable Wireless Networks

Shahram Mollahasani
School of Electrical Engineering and Computer Science
University of Ottawa
smollah2@uottawa.ca

Melike Erol-Kantarci, Senior Member, IEEE
School of Electrical Engineering and Computer Science
University of Ottawa
melike.erolkantarci@uottawa.ca

Mahdi Hirab
VMware Inc.
mhirab@vmware.com

Hoda Dehghan
VMware Inc.
hdehghan@vmware.com

Rodney Wilson, Senior Member, IEEE
Ciena Corp.
rwilson@ciena.com

School of Electrical Engineering

Abstract—The ﬂexibility offered by reconﬁgurable wireless
networks, provide new opportunities for various applications
such as online AR/VR gaming, high-quality video streaming and
autonomous vehicles, that desire high-bandwidth, reliable and
low-latency communications. These applications come with very
stringent Quality of Service (QoS) requirements and increase the
burden over mobile networks. Currently, there is a huge spectrum
scarcity due to the massive data explosion and this problem can
be solved by helps of Reconﬁgurable Wireless Networks (RWNs)
where nodes have reconﬁguration and perception capabilities.
Therefore, a necessity of AI-assisted algorithms for resource block
allocation is observed. To tackle this challenge, in this paper, we
propose an actor-critic learning-based scheduler for allocating
resource blocks in a RWN. Various trafﬁc types with different
QoS levels are assigned to our agents to provide more realistic
results. We also include mobility in our simulations to increase the
dynamicity of networks. The proposed model is compared with
another actor-critic model and with other traditional schedulers;
proportional fair (PF) and Channel and QoS Aware (CQA)
techniques. The proposed models are evaluated by considering
the delay experienced by user equipment
(UEs), successful
transmissions and head-of-the-line delays. The results show that
the proposed model noticeably outperforms other techniques in
different aspects.

Index Terms—5G, AI-enabled networks, Reinforcement learn-

ing, Resource allocation

I. INTRODUCTION

With the growing demand on social media platforms, Ultra-
High-Deﬁnition (UHD) video and Virtual Reality (VR), Aug-
mented Reality (AR) enabled applications, the ever-growing
data trafﬁc of Internet-of-Things (IoT), and the speedy ad-
vances in mobile devices and smartphones, have led to expo-
nential growth in the trafﬁc load over wireless networks [1].
Due to these new applications, availability of various trafﬁc
types, and unpredictability of physical channels,
including
fading, path loss, etc., maintaining quality of service (QoS) has
become more challenging than ever before. In recent years,
with the introduction of Open Radio Access Network (O-
RAN), Artiﬁcial Intellience (AI) and Machine Learning (ML)
have found applications in wireless networks, and Reconﬁg-
urable Wireless Networks (RWNs) have emerged. In RWNs,

local networking nodes are controlled by groups of communi-
cating nodes equipped with reconﬁgurable software, hardware,
or protocols. Software reconﬁguration is useful for updating,
inclusion, and exclusion of tasks while hardware reconﬁgura-
tion will enable manipulating physical infrastructure. AI and
ML techniques can provide automation at a higher degree than
before (further than self-organized networks (SON) concept of
3GPP) and manage the growing complexity of the RWNs [2].
Reinforcement learning (RL) is a machine learning tech-
nique that allows optimal control of systems by directing the
system to a desired state by interacting with the environment
and using feedback from the environment [3]. RL tech-
niques are widely used in cellular networks with various use
cases such as video ﬂow optimization [4], improving energy-
efﬁciency in mobile networks [5], and optimizing resource al-
location [6]. A majority of RL-based methods used in wireless
networks focus on Q-learning or Deep Q-learning. Although
promising results are obtained using these techniques, they
techniques offer a single level control. Instead, actor-critic
learning which is a type of RL, can be implemented in multiple
hierarchies and offer control from multiple points of views.

In this work, we propose an Actor-Critic Learning approach
[7], to allocate resource blocks in a way that the communica-
tion reliability is enhanced and the required QoS by each UE
is satisﬁed. In the proposed model, we formulate the choice
of the number of resource blocks (RBs) and the location
of them in the RBs’ map as a Markov Decision Process
(MDP) [8], and we solve this problem by using an actor-critic
model. We consider channel quality, packets priorities, and
the delay budget of each trafﬁc stream in our reward function.
We adopt two Advantage Actor-Critic (A2C) models [9]. The
ﬁrst technique solely schedules packets by giving priority to
their scheduling delay budget (called as D-A2C) while second
technique considers channel quality, delay budget, and packet
types (called as CDPA-A2C). We evaluate the performance of
the proposed models using NS3 [10] with ﬁxed and mobile
scenarios. Our results show that, in the ﬁxed scenario, the
proposed model can reduce the mean delay signiﬁcantly with

 
 
 
 
 
 
respect to proportional fair (PF) [11], Channel and QoS Aware
(CQA) [12] and D-A2C schedulers. Additionally, CDPA-A2C
can increase the packet delivery rate in the mobile scenario up
to 92% and 53% in comparison with PF and CQA.

The main contributions of this paper are:
• Proposing an actor-critic learning technique that can
be implemented on disaggregated RAN functions and
provide control in two levels.

• Proposing a comprehensive reward function which takes
care of channel quality, packet priorities, and the delay
budget of each trafﬁc stream.

• Proposing two A2C models, where the ﬁrst model solely
schedules packets by giving priority to their scheduling
delay budget (called as D-A2C) and the second technique
considers channel quality, delay budget, and packet types
(called as CDPA-A2C).

The rest of the paper is organized as follows. In Section II,
we summarized the related works. In Section III, the system
model is described. In Section IV, the proposed actor-critic
resource block scheduler is explained. Numerical results and
evaluation of the proposed model are presented in Section V,
and ﬁnally, we conclude the paper in Section VI.

II. RELATED WORK

Providing ubiquitous connectivity for various devices with
different QoS requirements is one of the most challenging
issues for mobile network operators [13]. This problem is
ampliﬁed in future 5G applications with strict QoS require-
ments [14]. Additionally, in order to be capable of handling
all
the new immersive applications (which are known for
their heterogeneous QoS properties), advanced techniques are
required to maintain quality of experience (QoE) among the
network’s entities. To this end, packet schedulers need to allow
sharing the network bandwidth dynamically among UEs in
a way that UEs achieve their target QoS. Many scheduling
algorithms have been introduced previously, which employs
QoS in their models. In [15], a scheduler is proposed, which
encapsulates different features of scheduling strategies for the
downlink of cellular networks to guarantee multi-dimensional
QoS for various radio channels and trafﬁc types. However,
most of the QoS-based schedulers are prioritizing some trafﬁc
types by ignoring the rest. For instance, in [16], a prioritized
trafﬁc scheduler named by frame level scheduler (FLS) is
introduced, which gives higher priority to real-time trafﬁcs in
comparison with elastic trafﬁcs (such as HTTP or ﬁle trans-
fer). Additionally, in [17] required activity detection (RADS)
scheduler is proposed, which prioritized UEs based on the
fairness and their packet delay. However, most of prioritizing
schedulers are not capable of quickly reacting to the dynamics
of cellular networks. Therefore, some trafﬁc classes may have
degradation in their QoS, while others can be over-provisioned.
RL-based models are also applied in different ways in
order to optimize resource allocation in networks. In [18], an
RL-based scheduler is presented for resource allocation in a
reliable vehicle-to-vehicle (V2V) network. The presented RL
scheduler interacts with the environment frequently to learn

and optimize resource allocation to the vehicles. In this work,
it is assumed that the whole network structure is connected to a
centralized scheduler. Additionally, in [19], resource allocation
and computation ofﬂoading in multi-channel multi-user mobile
edge cloud (MEC) systems are evaluated. In this work, the
authors presented a deep reinforcement network to jointly
optimize the total delay and energy consumption of all UEs.
Moreover, in [20], an RL controller is implemented to schedule
deadline-driven data transfers. In this paper, it is assumed
that the requests will be sent to a central network controller
where the ﬂows with respect to their pacing rates can be
scheduled. In [21], [22], authors introduce a trafﬁc predictor
for network slices with a low complexity, based on a soft
gated recurrent unit. They also use the trafﬁc predictor to feed
several deep learning models, which are trained ofﬂine to apply
end-to-end reliable and dynamic resource allocation under
dataset-dependent generalized service level agreement (SLA)
constraints. The authors have considered resource bounds-
based SLA and violation rate-based SLA in order to estimate
the required resources in the network.

Traditional radio resource management (RRM) algorithms
are not able to handle the stringent QoS requirements of users
while adapting to fast varying conditions of RWNs. Recently,
machine learning algorithms have been employed in sched-
ulers to beneﬁt from data in optimizing resource allocation,
as opposed to using models [23]–[26]. An ML-based RRM
algorithm is proposed in [27] to estimate the required resources
in the network for tackling trafﬁc on-demand trafﬁcs over
HTTP. ML has been used in resource allocation by considering
various QoS combinations objectives such as packet loss [23],
delay [24], and user fairness [25]. However, these models
are deﬁned for improving delay or enhancing throughput for
Ultra-Reliable and Low-Latency Communications (URLLC)
and throughput of enhanced Mobile Broadband (eMBB) UEs,
while trafﬁc types are considered homogeneous [26]. Hence,
they ignored the effect of trafﬁc types with various QoS
requirements in their model. In comparison with previous
works, we propose actor-critic learning for resource allocation
which can be used at different levels of disaggregated RANs.
We use a reward function that addresses channel quality,
packet priorities, and the delay budget of each trafﬁc stream.
We evaluate our proposed scheme under various trafﬁc types,
interference levels and mobility. We provide detailed results
on key performance indicators (KPIs) collected from NS3
simulator and integrated Open-AI Gym.

III. SYSTEM MODEL

We assume the overall downlink bandwidth is divided into
the total number of available RBs, and each RB contains
12 contiguous subcarriers. Moreover, a resource block group
(RBG) will be formed when consecutive RBs are grouped
together. In order to reduce the number of state’s in our
reinforcement learning approach, we consider RBG as a unit
for resource allocation in the frequency domain. The aim
of the proposed actor-critic model
is to assign RBGs by
considering trafﬁc types, their QoS requirement, and their

Fig. 1: A Distributed RL-based RB Scheduler.

priorities during each transmission time interval (TTI). Based
on our system model, Base Stations (BS) are actors and each
actor schedules the packets located in the transmission buffer
of its associated UEs during each time interval such that,
the amount of time that the packets stay in UEs buffers are
reduced. The scheduling decision will be made every TTI, by
considering the number of pending packets in the transmission
buffers of active UEs. The overall delay experienced by a
packet can be break down into three main factors as shown
below:

P acketLatency = THOL + Ttx + THARQ

(1)

where THOL is the time duration that a packet waits in the
queues to get a transmission opportunity (scheduling delay).
HOL stands for head-of-the-line delay. Ttx is communication
delay, and THARQ is a round-trip time which is required
to retransmit a packet. Ttx and THARQ are basically based
on the environment (path loss, shadowing, fading, etc.), UEs
locations (propagation distance) and channel condition (noise,
interference, etc.). In order to satisfy the packets with low
latency requirements (e.g., URLLC UEs), the scheduler needs
to handle those packets in UE buffers as soon as they arrive,
thus, minimizing HOL. We also need to limit the number
of HARQ to achieve lower delay during communication.
However,
limiting retransmissions can increase the packet
drop rate and reduce the reliability in the network [28]. Low
reliability can highly affect the UEs located at the edges. The
proposed RL-based scheduler aims to address this trade off by
enhancing reliability and meeting the required latency budget.

IV. ACTOR-CRITIC SCHEDULING FRAMEWORK

It

is well-known that due to the scheduler’s multi-
dimensional and continuous state space, we can not enumerate
the scheduling problem exhaustively [29]. We can tackle this

issue by employing RL and learning the proper scheduling
rule. In actor-critic learning, policy model and value function
are the two main parts of policy gradients. In order to reduce
the gradient variance in our policy, we need to learn the
value function for update and assist system policy during
each time interval, and this is known as the Actor-Critic
model. At the exploitation step, in order to make decisions,
the learnt actor function is used. In our model, we aim to
prioritize certain trafﬁc types and reduce their HOL during
scheduling decision. The obtained M dimensional decision
matrix is employed to schedule and prioritize the available
trafﬁc classes at each TTI. To do so, a neural network (NN)
framework is employed to tackle the complexity and obtain
an approximation for achieving the best possible prioritizing
decision during each time interval. At the learning state, the
weights of the neural network will be updated every TTI by
considering the interactions occurring between the actor and
the critic. Moreover, during the exploitation state, the value
of the updated weights is saved, and the NN is tuned as a
non-linear function.

An RL agent basically tries to achieve higher value from
each state st and it can be obtained through the state-value
(V (s)) and action-value functions Q(s, a). Using the action-
value function, we can estimate the output of action a in state
s during time interval t, and the average expected output of
state s can be obtained by using the state-value function. In
this work, instead of approximating both of action-value and
state-value functions, we estimate only V (s) by employing
the Advantage Actor-Critic (A2C) model, which simpliﬁes
the learning process and reduces the number of required
parameters. More speciﬁcally, advantage refer to a value which
determines how much the performed action is better than the
expected V (s) (A(st, at) = Q(st, at) − V (st)). Moreover,
the A2C model is a synchronous model and, with respect

to asynchronous actor-critic (A3C) [30], it provides better
consistency among agents, making it suitable for disaggregated
deployments.

The proposed A2C model contains two neural networks:
• A neural network in the critic for estimating the value
function to criticize the actors’ actions during each state.
• A neural network in the actor for approximating schedul-
ing rules and prioritizing packet streams during each state.
In the presented model, the critic is responsible for inspecting
the actors’ actions and enhance their decisions at each time
interval and its located at an edge cloud while the actor is at
the BS. This ﬂexible architecture is completed with recent O-
RAN efforts around disaggregation of network functions. The
high level perspective of the proposed model’s architecture is
presented in Fig.1.

In the following we present the actor-critic architecture step
by step: Actor: We employed actor as an agent to explore
the required policy π (θ is policy parameter) based on its
observation (O) to obtain and apply the corresponding action
(A).

πθ(O) = O → A

(2)

Therefore, the chosen action by an agent can be present as:

where a ∈ A. Actions are considered as choosing a proper
resource block in the resource block map of each agent. Due
to the discrete nature of actions, we employ softmax functions
at the last layer (output) of actor to obtain the corresponding
values of each actions. The summation of actions’ scores is
equal to 1 and they are presented as probabilities of achieving
a high reward value with respect to the chosen action.

Critic: We employed the critic for obtaining the value
function V (O). During each time interval t, after the agent
executed the chosen action by actor (at), it will send it to the
critic along with the current observation (Ot). Then, the critic
estimates the temporal difference (TD) by considering the next
state (Ot+1) and the reward value (Rt) as follows:

δt = Rt + γV (Ot+1) − (Ot).

(4)

Here, δt is TD error for the action-value at time t, and γ
is a discount factor. At the updating step, the least squares
temporal difference (LSTD) need to be minimized to update
the critic during each step:

is estimated per time step ((cid:53)θt) and
Here, The gradient
parameters will be updated in this gradient direction; also,
the learning rate is deﬁned as α, which is between 0 and 1.
Finally, the actor network, by using policy gradient can be
updated as follows:

θt+1 = θt + α (cid:53)θt logπθt(Ot, at)δt.

(8)

Our main goal is to provide a channel, delay and prior-
ity aware actor-critic learning based scheduler (CDPA-A2C).
Actors are located at BSs; therefore, during their observation,
they can access the channel condition through Channel Quality
Indicator (CQI) feedback of UEs from the control channels,
and the amount of time packets will remain in the buffer
in order to be scheduled. Moreover, actors can tune their
future actions with respect to the received reward value at
each iteration. In order to train the agents based on the
network requirements, we need to include information about
the channel condition, the load of transmission buffer, and
the priority of packets into our reward function. The reward
function in actor i (BSi) is designed as follows:

(cid:32)

(cid:32)

(cid:33)

, 0

(cid:80)K

cqik −

R = ϕR1 + τ R2 + λR3,
(cid:33)
j=0 cqij
K
(cid:26)1 P acketU RLLC
Otherwise
0
(cid:23)
(cid:22) P acketdelay
P acketbudget

)

R2 =

R3 = sinc(π

(9)

(10)

(11)

(12)

Here, cqik is the feedback send by U Ek to agent i at time
interval t, K is the total number of UEs associated with the
agent i, P acketU RLLC is an identiﬁer for packets with a low
delay budget, and it is associated to the QCI, P acketdelay
is the HOL delay at RLC buffer and P acketbudget is the
maximum tolerable delay for the corresponding packet type,
which is deﬁned based on the packets’ types, ϕ, τ and λ
are scalar weights to control the priority among trafﬁc types,
maintaining delay budget and UEs condition (CQI feedback)
[31], [32]. The reward function is tuned in a way that UEs
received signal strength alongside packet delivery ratio be
maximized while giving higher priority to the critical trafﬁc
load to increase QoS of URLLC users.

In this paper, to examine the effect of the proposed reward

a = πθ(O),

(3)

R1 = max

sgn

V ∗ = arg min

(δt)2,

V

(5)

function, we deﬁned two A2C models as follows:

Here, the optimal value function is presented as V ∗. The actor
can be updated by policy gradient which can be obtained by
using the TD error as follow:

(cid:53)θ J(θ) = Eπθ [(cid:53)θlogπθ(O, a)δt],

(6)

where, (cid:53)θJ(θ) is the gradient of the cost function with respect
to θ, and the value of action a under the current policy is shown
as πθ(O, a). Then, the difference of parameters’ weights at the
actor during time interval t can be calculated as:

∆θt = α (cid:53)θt logπθt(Ot, at)δt,

(7)

• In the ﬁrst one, the scheduler will schedule packets just
based on packets’ delay budget, and it
is named as
Delay aware A2C (D-A2C). In this model, the priority
of packets (in our scenario URLLC packets) are ignored
by setting τ = 0 in eq. (11).

• In the second one, the scheduler takes all performance
metrics into consideration by using eqs. (10),(11) and
(12). The scheduler is called as CDPA-A2C. In this
model, instead of giving priority to just one metric, the
scheduler is equipped with a more complex model to be
capable of handling RB allocation in different conditions.

V. SIMULATION ENVIRONMENT

In this work, we implemented the proposed algorithms in
ns3-gym [33], which is a framework for connecting ns-3 with
OpenAI Gym (a tool for integrating machine learning libraries)
[34]. The neural networks of CDPA-A2C and D-A2C were
implemented in Pytorch. In our simulations, three BSs are
considered, and the number of UEs varies between 30 to
90 UEs, which are distributed randomly in the environment.
Simulation results are based on 30 runs and each run contain
5000 iterations. To run the following model, we used a PC
equipped with CoreT M i7-8700 CPU and 32 GB of RAM.
The simulation time depends on the number of assigned UEs
and BSs, trafﬁc types, and UEs’ mobility. The simulation time
can be varied between 30-90 minutes for simulating 5000
TTI based on the deﬁned scenarios in this paper. In this
work numerology zero is employed with 15 KHz subcarrier
spacing, 12 subcarriers per resource block, and 14 symbols
per subframe and our scheduling interval is set to 1 ms [35].
We deployed two scenarios. In the ﬁrst scenario, we assume
the number of UEs vary between 30 to 90 and UEs are not
mobile. We distributed three trafﬁc types (voice, video, and
IP Multimedia Subsystem (IMS)) with different QoS require-
ments uniform randomly among UEs. The UE applications
generate trafﬁc with Poisson arrivals. Each trafﬁc type has its
own properties and different arrival time with respect to other
packets.

In our simulations, when the amount of time a packet stays
in a UE’s buffer is lower than its delay budget, we consider
it as a satisﬁed packet, and when this value is higher than the
delay budget, it will be evaluated as an unsatisﬁed packet. In
Table.I, we present the required QoS metrics for each trafﬁc
type considered in this paper, in detail.

TABLE I: The employed packet types and their properties
[36].

QCI

1
5
6
75

Resource
Type

GBR
Non-GBR
Non-GBR
GBR

Priority

2
1
6
2.5

Packet
Delay
Budget
100 ms
100 ms
300 ms
20 ms

Service
Example

Voice
IMS
Video
V2X

In the second scenario, we have 70-110 UEs in our vehicular
network, which 10% of them are vehicular UEs (U Ev) and
the rest are ﬁxed users (U Ec). The U Ec requires a high
capacity link, while U Ev demands need to be satisﬁed by a
low latency link. In this scenario, in addition to “Voice, Video,
and IMS” we have Vehicle-to-everything “V2X” packets that
are deﬁned based on 5GAA standards [37]. Table II presents
the assigned network parameters and neural network settings
in our simulations. ϕ, τ and λ

Before presenting network performance results, in Fig. 2
we show the convergence of the proposed reward. The ﬁgure
shows the behavior of the reward function (eq. 9) when the
number of UEs are 90 and 10% of them are mobile. In
this work, an epsilon-greedy policy is used, in which during

TABLE II: Simulation parameters.

Parameters
Number of neurons
Scheduler algorithm
Number of BSs
Number of UEs
Maximum Trafﬁc load per UE
(Downlink)
Trafﬁc types
Trafﬁc stream per TTI

D-A2C reward’s weights

CDPA-A2C reward’s weights

Discount factor
Actor learning-rate
Critic learning-rate

Value
256 x 3 layers (Actor + Critic)
CDPA-A2C, D-A2C, PF, CQA
3
30-110

256 kbps

Voice, Video, IMS, V2X
50
packetdelay
traf f icdelay−budget

ϕ = 1 −

τ = 0 and λ = 5

ϕ = 1 −

packetdelay
traf f icdelay−budget

τ = 5 and λ = 5
0.9
0.01
0.05

,

,

Fig. 2: The convergence performance of the CDPA-A2C
algorithm’s reward function when the number of UEs are 90
while 10% of them are mobile.

the exploration phase actors perform their actions either by
randomly or by choosing an RB with the highest weight
assigned by the proposed actor-critic model. As is shown in
Fig. 2, the exploration phase will end after almost 3700 rounds,
and the model will converge after that.

A. Simulation Results with no mobility

In this scenario, we assumed UEs are ﬁxed, and we have
three packet streams (voice, video, IMS) with different QoS
requirements (delay budget, packet loss ratio, etc.). As it is
mentioned previously, we considered two A2C models with
different reward functions named by D-A2C and CDPA-A2C,
respectively. In addition to these two models, we compare our
results with the traditional Proportional Fair (PF) scheduler
and Channel and QoS Aware (CQA) scheduler as described
in [12].

In Fig. 3, we present the mean delay by considering various
trafﬁc types and varying number of UEs. Although, when the
number of UEs below 50 (in case of CQA scheduler when the
number of UEs are below 70), PF and CQA can satisfy the
required delay budget for all types of trafﬁc, as we increase
the number of UEs the mean HOL delay will be signiﬁcantly
increased, and it becomes higher than the target delay for voice
and IMS trafﬁc types. In cases with higher number of UEs,

Fig. 3: The HOL delay for different number of UEs (with no mobility).

Fig. 4: Packet delivery ratio (with no mobility).

D-A2C results are better than the traditional PF and CQA
schedulers, while employing a more comprehensive reward
function such as CDPA-A2C can remarkably enhance network
performance. In general, The CDPA-A2C scheduler can reduce
the mean delay in comparison with D-A2C, PF and CQA
schedulers up to 100 ms, 225 ms and 375 ms, respectively.

In Fig. 4, we present the packet delivery ratio (the ratio
of the packets which satisfy the predeﬁned delay budget). As
it is shown, the packet delivery ratio of CDPA-A2C and D-
A2C can be up to 117% and 73% higher than PF and CQA, by
considering various numbers of UEs for different trafﬁc types.
Moreover, CDPA-A2C can considerably enhance the packet
delivery ratio in comparison to D-A2C (up to 63% for IMS
packets). Note that, we assume all applications are sensitive to
delay and will consider packets that are beyond service targets
as undelivered.

B. Simulation Results with mobility

In the second scenario, we consider a case when 10% of
UEs are mobile. We include V2X trafﬁc based on 5GAA
standards in addition to other trafﬁc types used in the previous
simulations. Due to the better performance of CDPA-A2C, we
omit the D-A2C from second scenario.

In Fig. 5, we evaluate the mean HOL delay for various
numbers of UEs (70-110) when we have mobile UEs in the
network. As we can see, although, PF and CQA schedulers
can maintain the mean HOL delay below the delay budget,
by increasing the number of UEs, the mean HOL delay will
increase dramatically. Moreover, due to the high sensitivity
of V2X packets to delay (V2X delay budget = 20 ms), PF

and CQA can not satisfy V2X packets in any of these cases.
However,
the proposed CDPA-A2C model, by scheduling
packets on time and preventing the congestion in the UEs
buffer, can provide a lower delay for the presented trafﬁc types
with respect to PF and CQA.

In Fig. 6, we present the packet delivery ratio (packets
that can satisfy delay target) of the proposed model with
respect to other schedulers. In Fig. 6a, we evaluate the effect
of increasing the number of UEs over the packet delivery
ratio when 10% of UEs are mobile. As shown, the packet
delivery ratio of CDPA-A2C for different trafﬁc types and
numbers of UEs up to 92% and 53% higher than PF and
CQA, respectively. Although CQA has a good performance
in the delivery of Voice, Video, and IMS packets, CDPA-
A2C can considerably enhance the packet delivery ratio for
delay-sensitive trafﬁcs such as V2X packets in comparison
with CQA. We also examined the effect of increasing the
mobile UEs’ ratio, on the packet delivery ratio, when the total
number of UEs is ﬁxed to 90, in Fig. 6b. As we can see, the
proposed model can signiﬁcantly enhance the packet delivery
ratio for different trafﬁc types with respect to PF and CQA.
Moreover, by increasing the density of mobile UEs, the packet
loss ratio of V2X packets when CQA and PF schedulers are
employed, will be increased up to 40% and 75%, respectively.
Therefore, the CDPA-A2C scheduler can noticeably enhance
QoS by reducing the HOL delay and increasing the packet
delivery ratio in comparison with PF and CQA schedulers.

Fig. 5: The HOL delay for different number of UEs (with mobility).

VI. CONCLUSION

VII. ACKNOWLEDGEMENT

In this paper, we propose two actor-critic learning-based
schedulers, namely Delay-aware actor-critic (D-A2C) and
channel, delay and priority-aware actor-critic (CDPA-A2C)
techniques, for reconﬁgurable wireless networks, where the
network is capable to autonomously learn and adapt itself
to the wireless environment’s dynamicity for optimizing the
utility of the network resources. Reconﬁgurable wireless net-
works play a vital role in network automation. Applying
machine learning algorithms can be an appropriate candidate
for making versatile models and making future networks
smarter. Here, the proposed model is constructed based on two
neural network models (actor and critic). Actors are employed
to apply actions (RB allocation); simultaneously, the critic is
used to monitor agents’ actions and tune their behaviors in the
following states to make the convergence faster and optimize
the model. The proposed comprehensive model (CDPA-A2C),
in addition of considering channel condition and delay budget
of each packet, prioritizes the received packets by considering
their types and their QoS requirements. We include ﬁxed and
mobile scenarios to evaluate the performance of the proposed
schemes. We compare the learning-based schemes to two
well-known algorithms: the traditional proportional fair and
a QoS-aware algorithm CQA. Our results show that CDPA-
A2C signiﬁcantly reduces the mean delay with respect to PF
and D-A2C schedulers. Additionally, CDPA-A2C can increase
the packet delivery rate in the mobile scenario up to 92% and
53% in comparison with PF and CQA, respectively.

This work is supported by Ontario Centers of Excellence

(OCE) 5G ENCQOR program.

REFERENCES

[1] J. Navarro-Ortiz, P. Romero-Diaz, S. Sendra, P. Ameigeiras, J. J. Ramos-
Munoz, and J. M. Lopez-Soler, “A Survey on 5G Usage Scenarios and
Trafﬁc Models,” IEEE Communications Surveys Tutorials, vol. 22, no. 2,
pp. 905–929, 2020.

[2] M. Polese, R. Jana, V. Kounev, K. Zhang, S. Deb, and M. Zorzi, “Ma-
chine learning at the edge: A data-driven architecture with applications to
5G cellular networks,” IEEE Transactions on Mobile Computing, 2020.
[3] N. C. Luong, D. T. Hoang, S. Gong, D. Niyato, P. Wang, Y.-C.
Liang, and D. I. Kim, “Applications of deep reinforcement learning
in communications and networking: A survey,” IEEE Communications
Surveys & Tutorials, vol. 21, no. 4, pp. 3133–3174, 2019.

[4] M. Polese, R. Jana, V. Kounev, K. Zhang, S. Deb, and M. Zorzi, “Ma-
chine learning at the edge: A data-driven architecture with applications to
5G cellular networks,” IEEE Transactions on Mobile Computing, 2020.
[5] R. Li, Z. Zhao, X. Zhou, G. Ding, Y. Chen, Z. Wang, and H. Zhang,
“Intelligent 5G: When cellular networks meet artiﬁcial intelligence,”
IEEE Wireless communications, vol. 24, no. 5, pp. 175–183, 2017.
[6] S. Chinchali, P. Hu, T. Chu, M. Sharma, M. Bansal, R. Misra, M. Pavone,
and S. Katti, “Cellular network trafﬁc scheduling with deep reinforce-
ment learning,” in Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, vol. 32, 2018.

[7] Y. Wei, F. R. Yu, M. Song, and Z. Han, “User scheduling and
resource allocation in HetNets with hybrid energy supply: An actor-
critic reinforcement learning approach,” IEEE Transactions on Wireless
Communications, vol. 17, no. 1, pp. 680–692, 2017.

[8] C. C. White, “A survey of solution techniques for the partially observed
Markov decision process,” Annals of Operations Research, vol. 32, no. 1,
pp. 215–230, 1991.

[9] O. Nachum, M. Norouzi, K. Xu, and D. Schuurmans, “Bridging the gap
between value and policy based reinforcement learning,” in Advances in
Neural Information Processing Systems, pp. 2775–2785, 2017.

(a) The effect of UEs’ number on packet delivery ratio when the ratio of mobile UEs is ﬁxed to 10%.

(b) The effect of percentage of mobile UEs on packet delivery ratio when the total number of UEs is 90.

Fig. 6: Packet delivery ratio (with mobility).

[30] M. Sewak, “Actor-Critic Models and the A3C,” in Deep Reinforcement

Learning, pp. 141–152, Springer, 2019.

[31] M. Elsayed and M. Erol-Kantarci, “Learning-based resource allocation
for data-intensive and immersive tactile applications,” in 2018 IEEE 5G
World Forum (5GWF), pp. 278–283, IEEE, 2018.

[32] M. Elsayed, M. Erol-Kantarci, B. Kantarci, L. Wu, and J. Li, “Low-
latency communications for community resilience microgrids: A re-
learning approach,” IEEE Transactions on Smart Grid,
inforcement
vol. 11, no. 2, pp. 1091–1099, 2019.

[33] P. Gawłowicz and A. Zubow, “NS-3 meets openai gym: The playground
for machine learning in networking research,” in Proceedings of the 22nd
International ACM Conference on Modeling, Analysis and Simulation of
Wireless and Mobile Systems, pp. 113–120, 2019.

[34] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schul-
man, J. Tang, and W. Zaremba, “Openai gym,” arXiv preprint
arXiv:1606.01540, 2016.

[35] J. Vihriälä, A. A. Zaidi, V. Venkatasubramanian, N. He, E. Tiirola,
J. Medbo, E. Lähetkangas, K. Werner, K. Pajukoski, A. Cedergren, et al.,
“Numerology and frame structure for 5G radio access,” in 2016 IEEE
27th annual international symposium on personal, indoor, and mobile
radio communications (PIMRC), pp. 1–5, IEEE, 2016.

[36] “Table 6.1.7-A: Standardized QCI characteristics from 3GPP TS

23.203V16.1.0.,”

[37] 5GAA: Paving the Way towards 5G, accessed on 03 September 2020.
Available online: https://5gaa.org/5g-technology/paving-the-way.

[10] A. K. Saluja, S. A. Dargad, and K. Mistry, “A Detailed Analogy of
Network Simulators—NS1, NS2, NS3 and NS4,” Int. J. Future Revolut.
Comput. Sci. Commun. Eng, vol. 3, pp. 291–295, 2017.

[11] M. T. Kawser, H. Farid, A. R. Hasin, A. M. Sadik, and I. K. Razu,
“Performance comparison between round robin and proportional fair
scheduling methods for LTE,” International Journal of Information and
Electronics Engineering, vol. 2, no. 5, pp. 678–681, 2012.

[12] B. Bojovic and N. Baldo, “A new channel and QoS aware scheduler to
enhance the capacity of voice over LTE systems,” in 2014 IEEE 11th
International Multi-Conference on Systems, Signals & Devices (SSD14),
pp. 1–6, IEEE, 2014.

[13] M. F. Audah, T. S. Chin, Y. Zulfadzli, C. K. Lee, and K. Rizaluddin,
“Towards Efﬁcient and Scalable Machine Learning-Based QoS Trafﬁc
Classiﬁcation in Software-Deﬁned Network,” in International Confer-
ence on Mobile Web and Intelligent Information Systems, pp. 217–229,
Springer, 2019.

[14] M. A. Habibi, M. Nasimi, B. Han, and H. D. Schotten, “A compre-
hensive survey of RAN architectures toward 5G mobile communication
system,” IEEE Access, vol. 7, pp. 70371–70421, 2019.

[15] S. Abedi, “Efﬁcient radio resource management for wireless multime-
dia communications: a multidimensional QoS-based packet scheduler,”
IEEE Transactions on Wireless Communications, vol. 4, no. 6, pp. 2811–
2822, 2005.

[16] G. Piro, L. A. Grieco, G. Boggia, R. Fortuna, and P. Camarda, “Two-
level downlink scheduling for real-time multimedia services in LTE
networks,” IEEE Transactions on Multimedia, vol. 13, no. 5, pp. 1052–
1065, 2011.

[17] G. Monghal, D. Laselva, P.-H. Michaelsen, and J. Wigard, “Dynamic
packet scheduling for trafﬁc mixes of best effort and VoIP users
in E-UTRAN downlink,” in 2010 IEEE 71st Vehicular Technology
Conference, pp. 1–5, IEEE, 2010.

[18] T. ¸Sahin, R. Khalili, M. Boban, and A. Wolisz, “Reinforcement learning
scheduler for vehicle-to-vehicle communications outside coverage,” in
2018 IEEE Vehicular Networking Conference (VNC), pp. 1–8, IEEE,
2018.

[19] S. Nath, Y. Li, J. Wu, and P. Fan, “Multi-user Multi-channel Computa-
tion Ofﬂoading and Resource Allocation for Mobile Edge Computing,”
in ICC 2020-2020 IEEE International Conference on Communications
(ICC), pp. 1–6, IEEE, 2020.

[20] G. R. Ghosal, D. Ghosal, A. Sim, A. V. Thakur, and K. Wu, “A
Deep Deterministic Policy Gradient Based Network Scheduler For
Deadline-Driven Data Transfers,” in 2020 IFIP Networking Conference
(Networking), pp. 253–261, IEEE, 2020.

[21] H. Chergui and C. Verikoukis, “Ofﬂine SLA-constrained deep learning
for 5G networks reliable and dynamic end-to-end slicing,” IEEE Journal
on Selected Areas in Communications, vol. 38, no. 2, pp. 350–360, 2019.
[22] H. Chergui and C. Verikoukis, “Big Data for 5G Intelligent Network
Slicing Management,” IEEE Network, vol. 34, no. 4, pp. 56–61, 2020.
[23] I.-S. Com¸sa, S. Zhang, M. E. Aydin, P. Kuonen, Y. Lu, R. Trestian, and
G. Ghinea, “Towards 5G: A reinforcement learning-based scheduling
solution for data trafﬁc management,” IEEE Transactions on Network
and Service Management, vol. 15, no. 4, pp. 1661–1675, 2018.
[24] I.-S. Coms, a, S. Zhang, M. Aydin, P. Kuonen, R. Trestian, and G. Ghinea,
“A comparison of reinforcement learning algorithms in fairness-oriented
OFDMA schedulers,” Information, vol. 10, no. 10, p. 315, 2019.
[25] M. Elsayed and M. Erol-Kantarci, “AI-enabled radio resource allocation
in 5G for URLLC and eMBB users,” in 2019 IEEE 2nd 5G World Forum
(5GWF), pp. 590–595, IEEE, 2019.

[26] M. Mohammadi and A. Al-Fuqaha, “Enabling cognitive smart cities
using big data and machine learning: Approaches and challenges,” IEEE
Communications Magazine, vol. 56, no. 2, pp. 94–101, 2018.

[27] A. Martin, J. Egaña, J. Flórez, J. Montalbán, I. G. Olaizola, M. Quartulli,
R. Viola, and M. Zorrilla, “Network resource allocation system for QoE-
aware delivery of media services in 5G networks,” IEEE Transactions
on Broadcasting, vol. 64, no. 2, pp. 561–574, 2018.

[28] X. Du, Y. Sun, N. B. Shroff, and A. Sabharwal, “Balancing Queueing
and Retransmission: Latency-Optimal Massive MIMO Design,” IEEE
Transactions on Wireless Communications, vol. 19, no. 4, pp. 2293–
2307, 2020.

[29] I.-S. Com¸sa, G.-M. Muntean, and R. Trestian, “An Innovative Machine-
Learning-Based Scheduling Solution for Improving Live UHD Video
Streaming Quality in Highly Dynamic Network Environments,” IEEE
Transactions on Broadcasting, 2020.

