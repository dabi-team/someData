1
2
0
2

b
e
F
5

]

R
P
.
h
t
a
m

[

4
v
7
9
3
5
0
.
5
0
8
1
:
v
i
X
r
a

Conditional Moments of Noncausal Alpha-Stable Processes
and the Prediction of Bubble Crash Odds

Sébastien Fries

Vrije Universiteit Amsterdam

Abstract

Noncausal, or anticipative, heavy-tailed processes generate trajectories featuring locally explosive episodes akin

to speculative bubbles in ﬁnancial time series data. For (Xt) a two-sided inﬁnite α-stable moving average (MA),

conditional moments up to integer order four are shown to exist provided (Xt) is anticipative enough, despite

the process featuring inﬁnite marginal variance. Formulae of these moments at any forecast horizon under any

admissible parameterisation are provided. Under the assumption of errors with regularly varying tails, closed-form

formulae of the predictive distribution during explosive bubble episodes are obtained and expressions of the ex ante

crash odds at any horizon are available. It is found that the noncausal autoregression of order 1 (AR(1)) with AR

coeﬃcient ρ and tail exponent α generates bubbles whose survival distributions are geometric with parameter ρα.

This property extends to bubbles with arbitrarily-shaped collapse after the peak, provided the inﬂation phase is

noncausal AR(1)-like. It appears that mixed causal-noncausal processes generate explosive episodes with dynamics

à la Blanchard and Watson (1982) which could reconcile rational bubbles with tail exponents greater than 1.

Applications of the conditional moments to bubble modelling by noncausal processes are discussed and the use of

the closed-form crash odds is illustrated on the Nasdaq and S&P500 series.

Keywords: Noncausal process, Conditional moments, Speculative bubble, Crashes, Prediction, Rational expectation

1

Introduction

Dynamic models often admit solution processes for which the current value of the variable is a func-

tion of future values of an independent error process. Such solutions, called anticipative or non-

causal, have attracted increasing attention in the ﬁnancial and econometric literatures.

In par-

ticular, noncausal processes have been found convenient for modelling locally explosive phenom-

ena in ﬁnancial time series such as speculative bubbles, while featuring heavy-tailed marginals and

Address for correspondence: Sébastien Fries, 1105 De Boelelaan, 1081HV, Amsterdam, the Netherlands. s.f.fries@vu.nl

1

 
 
 
 
 
 
conditional heteroscedastic eﬀects [Bec et al. (2020), Cavaliere et al. (2020), Fries and Zakoian (2019),

Gouriéroux and Jasiak (2018), Gouriéroux and Zakoian (2017), Hecq and Sun (2019), Hecq et al. (2016,

2017a,b), Hencic and Gouriéroux (2015)] (see also Chen et al. (2017), Lanne et al. (2012b), Lanne and

Saikkonen (2011, 2013)). Figure 1 depicts a typical simulated path of an elementary noncausal process,

the α-stable noncausal AR(1), featuring multiple bubbles. Noncausal processes, shown to be suitable

Figure 1: Sample path of an elementary bubble-generating noncausal process: the noncausal AR(1), strictly stationary

solution of Xt = ρXt+1 + εt, ρ = 0.95, with α-stable errors.

candidates for bubble components in rational expectation price models [Gouriéroux et al. (2020)], may

oﬀer a possibility to forecast the future trajectories of bubbles and to infer the odds of crashes. This

would enable for instance risk managers to assess large downside risks during prolonged bull markets and

the regulator to adjust requirements and restrictions to ensure resilience of the ﬁnancial system.

However, the limited knowledge about the predictive distribution of noncausal processes, especially

during explosive bubble events, is impeding the ability to forecast them, thus limiting their use in prac-

tical applications. Taking notice of the absence of closed-form formulae for conditional moments and

the predictive density except in special cases, two simulation- and sample-based methods have been

proposed in the noncausal literature to approximate the conditional distribution of noncausal processes

[Lanne et al. (2012a), Gouriéroux and Jasiak (2016)]. While oﬀering ﬂexible alternatives for forecasting

noncausal processes beyond the special cases, Hecq and Voisin (2020) ﬁnd that these methods can become

computationally intense for larger prediction horizons and that accurately capturing the dynamics during

explosive episodes may prove challenging [see also Gouriéroux et al. (2019)]. Partial results have been

obtained by Gouriéroux and Zakoian (2017) on the conditional moments of noncausal AR(1) processes

driven by independent and identically distributed (i.i.d.) α-stable errors, which have been extended to

mixed causal-noncausal AR processes with single ill-located root by Fries and Zakoian (2019). Despite

stable noncausal processes featuring inﬁnite marginal variance, their conditional moments may exist up

2

to integer order four. In special cases, expressions of the conditional expectation and variance have been

obtained, and revealed that noncausal processes can feature GARCH type eﬀects in calendar time despite

such eﬀects not being explicitly included in the modelling. Provided the expressions of the conditional

moments are derived, this suggests that point forecasts of noncausal processes based on their conditional

expectation, variance, skewness and kurtosis could be formulated -as opposed to other predictors speciﬁ-

cally introduced to circumvent the inﬁnite variance of α-stable processes, such as minimum Lα-dispersion

or maximum covariation (see Karcher et al. (2013) and the references therein).

The aim of this paper is to provide practical analytical results to compute the conditional moments

of α-stable noncausal processes and to compute the crash odds of bubbles that such processes gener-
ate. First, the paper extends the literature on the conditional moments E[X p

2 |X1] of arbitrary bivariate

α-stable random vectors (X1, X2) [Cioczek-Georges and Taqqu (1995a,b, 1998), Hardin et al. (1991),

Samorodnitsky and Taqqu (1994) (ST94 hereafter)] by providing formulae for the orders p = 2, 3, 4. We
then apply these results to derive a complete characterisation of the conditional moments E[X p

t+h|Xt],
p = 1, 2, 3, 4, h ≥ 1, for (Xt) an inﬁnite two-sided moving average process driven by i.i.d. α-stable errors

Xt = X
k∈Z

akεt+k,

(1.1)

where (ak) is a non-random coeﬃcients sequence satisfying mild conditions for (Xt) to be well deﬁned and

strictly stationary. Second, the conditional distribution of noncausal processes during explosive bubble

episodes is analysed. Provided the errors have probability tails similar to that of α-stable distributions,

in the sense that they also feature power-law tails, we obtain closed-form formulae valid during explosive

episodes. These expressions provide illuminating interpretations on the dynamics of the bubbles that

such models generate and a practical way to quantify the crash odds. Implications and parallels with

the literature on rational expectation bubble models in the line of Blanchard and Watson (1982) are

discussed.

The rest of the paper is organised as follows. Section 2 recalls properties of bivariate stable distribu-

tions and provides our results on the conditional moments up to order four of arbitrary bivariate α-stable

vectors. Applying these results to models of the form (1.1), Section 3 proposes a suﬃcient condition

on the coeﬃcients (ak) for the existence of conditional moments, characterises their expressions when

they exist, and discusses several examples and methodological aspects. Section 4 derives closed-form

formulae for the predictive distribution of noncausal processes during explosive bubble episodes. Section

5 proposes an application of the crash odds formulae on the Nasdaq and S&P500 series. Proofs and

complementary results are collected in a Supplementary File.

3

2 Conditional moments of bivariate α-stable vectors

We begin by recalling some properties of bivariate stable vectors (X1, X2) and then propose new ex-
pressions for their higher-order conditional power moments E[X p

2 |X1]. These expressions will apply to

(Xt, Xt+h) when considering α-stable noncausal processes in the next section. Letting α ∈ (0, 2), a ran-
dom vector X = (X1, X2) is said to be an α-stable random vector in R2 (see Theorem 2.3.1 in ST94) if
there exists a unique pair (Γ, µ0), where Γ is a ﬁnite measure on the Euclidean unit sphere S2 and µ0 a
vector in R2, such that, for any u ∈ R2, the characteristic function of X writes

Eh

eihu,Xii

= exp

(cid:26)

Z

−

(cid:18)

|hu, si|α

1 − i sign(hu, si)w(α, hu, si)

(cid:19)

Γ(ds) + i hu, µ0i

(cid:27)

,

(2.1)

S2

where h·, ·i is the canonical inner product, w(α, s) = tg (cid:0) πα
2

π ln |s| otherwise,
for s ∈ R. The measure Γ and the vector µ0 are respectively called the spectral measure and the shift

(cid:1), if α 6= 1, and w(1, s) = − 2

vector of X. The pair (Γ, µ0) is said to be the spectral representation of X. The spectral measure Γ

of a stable vector X in particular completely characterises the tail dependence between its components:

it holds that P(X/kXk ∈ A | kXk > x) → Γ(A)/Γ(S2) as x → +∞, for any continuity set A ⊂ S2

(Theorem 4.4.8 in ST94), where k · k denotes the Euclidean norm. Intuitively, the more mass Γ attributes

to some points of the unit sphere S2, the more likely (X1, X2) is to be colinear to these points when it
is large in norm. The counterpart of (2.1) for a univariate α-stable variable reads E[eiuX ] = exp
σα|u|α(cid:16)

, for some asymmetry β ∈ [−1, 1], scale σ > 0 and location µ ∈ R.

1 − iβ sign(u)w(α, u)

+ iuµ

−

o

n

(cid:17)

Stable distributions are known to have very few moments. However, the distribution of one component

conditionally on the other can have more moments according to the degree of dependence between them.

If the spectral measure Γ of an α-stable random vector X = (X1, X2) satisﬁes

Z

S2

|s1|−νΓ(ds) < +∞,

for some

ν ≥ 0,

(2.2)

then, E(cid:2)|X2|γ(cid:12)

(cid:12)X1 = x(cid:3) < +∞ for almost every x if 0 ≤ γ < min(α + ν, 2α + 1) < 5 (see Theorem 5.1.3

in ST94 for details), entailing that conditional moments up to integer order four may exist although
E(cid:2)|X2|α] = +∞. The conditional expectation of arbitrary α-stable bivariate vectors has been studied

in details and its expression is recalled in Theorem 2.1 below, while the conditional variance received

attention most exclusively in the symmetric α-Stable case.

We provide and prove new formulae for the conditional power moments of order 2, 3, and 4 of arbitrary

(not necessarily symmetric) α-stable bivariate vectors (X1, X2). The second order moment in the case

α = 1, which requires special treatment when not restricting to symmetric stable distributions, is also

considered. In the rest of this section, we assume without loss of generality that the shift vector µ0 =

4

1, µ0

(µ0
of order p exists, E(cid:2)X p
2

2) is zero. This can be done without loss of generality because, assuming the conditional moment
(cid:12)
(cid:12) ˜X1 = ˜x(cid:3)

(cid:12)
(cid:12)X1 = x(cid:3) = E(cid:2)(X2 − µ0

p(µ0

2 + µ0
1, X2 − µ0

(cid:12)X1 − µ0

1 = x − µ0
1

2)p(cid:12)
2) has the same spectral measure as (X1, X2) and zero

2)p−jE(cid:2) ˜X j

(cid:3) = Pp

j=0 Cj

1, and ( ˜X1, ˜X2) = (X1 − µ0

where ˜x = x − µ0

2

shift parameter. We ﬁrst consider the case α 6= 1 and introduce useful constants and functions which

generalise existing quantities in the literature. For p ∈ {1, 2, 3, 4}, when they exist, deﬁne

σα
1 =

Z

S2

|s1|αΓ(ds), β1 =

R
S2

s<α>
1
σα
1

Γ(ds)

, κp =

R
S2

( s2
s1

)p|s1|αΓ(ds)

σα
1

, λp =

R

( s2
s1

S2

)ps<α>
1
σα
1

Γ(ds)

,

(2.3)

where y<r> = sign(y)|y|r for any y, r ∈ R. For any n ∈ N, θ = (θ1, θ2) ∈ R2, x ∈ R, deﬁne H as

H(n, θ; x) =

Z +∞

0

e−σα

1 uα

un(α−1)(cid:16)

θ1 cos(ux − aβ1σα

1 uα) + θ2 sin(ux − aβ1σα

1 uα)

(cid:17)

du.

(2.4)

The quantities σ1 and β1 denote the scale and asymmetry parameters of the marginal distribution of X1,

whereas the constants κp’s and λp’s generalise standard dependence measures invoked in the literature.
Noticeably, κ1 = R

1 corresponds to the normalised covariation between X2 and

s2s<α−1>
1

Γ(ds)/σα

S2

X1. This dependence measure was been introduced by Miller (1978) and Cambanis and Miller (1981)

to replace the ill-deﬁned covariance between two symmetric α-stable random variables, and has been a

popular tool to formulate point forecasts of inﬁnite variance α-stable processes [see Karcher et al. (2013)

and the references therein]. The new constants κp and λp, p ≥ 2 introduced here, which intervene in the

expressions of the higher order conditional moments of (X1, X2), can be seen as extending this dependence

measure to higher powers of X1 and X2 in the asymmetric case. The new family of functions H introduced

contains functions related to the marginal density of the stable random variable X1 ∼ S(α, β1, σ1, 0),
α 6= 1: fX1(x) := 1
1 uα)du. The following result recalls the

1 uα cos(ux − aβ1σα

π H(cid:0)0, (1, 0); x(cid:1) = 1

R +∞
0

e−σα

π

expression of the conditional expectation in the case α 6= 1.

Theorem 2.1 (Theorem 5.2.2, ST94) Let (X1, X2) be an α-stable random vector with spectral repre-

sentation (Γ, 0). For α ∈ (0, 2) \ {1} and letting Γ satisfy (2.2) with ν > 1 − α if α ∈ (0, 1),

Eh

X2

(cid:12)
(cid:12)
(cid:12)X1 = x

i

= κ1x +

a(λ1 − β1κ1)
1 + a2β2
1

"
aβ1x +

1 − xH(x)
πfX1(x)

#

,

(2.5)

where a = tg(πα/2), β1, κ1 and λ1 are as in (2.3) and H( · ) := H(cid:0)0, (0, 1); · (cid:1).

We now state our result in the case α 6= 1 for the conditional moments of order two, three and four.

Theorem 2.2 Let (X1, X2) be an α-stable random vector with spectral representation (Γ, 0).

For α ∈ (1/2, 2) \ {1} and Γ satisfying (2.2) with ν > 2 − α,

5

Eh

X 2
2

(cid:12)
(cid:12)
(cid:12)X1 = x

i

= κ2x2 +

"
aβ1x +

#

1 − xH(x)
πfX1(x)

ax(λ2 − β1κ2)
1 + (aβ1)2
α2σ2α
1
πfX1(x)

H

(cid:16)

−

2, θ1; x

(cid:17)
.

For α ∈ (1, 2) and Γ satisfying (2.2) with ν > 3 − α,

Eh

X 3
2

(cid:12)
(cid:12)
(cid:12)X1 = x

i

= κ3x3 +

ax2(λ3 − β1κ3)
1 + (aβ1)2
(cid:20)
α2σ2α
1
xH
2πfX1(x)

−

(cid:20)
aβ1x +

(cid:21)

1 − xH(x)
πfX1(x)

(cid:16)

2, θ2; x

(cid:17)

+ ασα

1 H

(cid:16)
3, θ3; x

(cid:17)(cid:21)
.

For α ∈ (3/2, 2) and Γ satisfying (2.2) with ν > 4 − α,

Eh

X 4
2

(cid:12)
(cid:12)
(cid:12)X1 = x

i

= κ4x4 +

ax3(λ4 − β1κ4)
1 + (aβ1)2
(cid:20) x2
α2σ2α
1
2
πfX1(x)

−

(cid:20)
aβ1x +

(cid:16)

2, θ4; x

H

(cid:21)

1 − xH(x)
πfX1(x)
αxσα
1
6

+

(cid:17)

H

(cid:16)

3, θ5; x

(cid:17)

+

α2σ2α
1
3

(cid:16)

H

4, θ6; x

(cid:17)(cid:21)

.

(2.6)

(2.7)

(2.8)

Here, a = tg (πα/2), H( · ) = H(cid:0)0, (0, 1); · (cid:1), θ1 = (θ11, θ12) in (2.6) is given by

θ11 = κ2

1 − a2λ2

1 + a2β1λ2 − κ2,

θ12 = a(λ2 + β1κ2) − 2aλ1κ1,

and the remaining θi’s in (2.7)-(2.8), which depend only on α, β1, and the κp’s and λp’s in (2.3), are

given in (D.1)-(D.10) in the Supplementary File.

Proof. See Sections B, C and D in the Supplementary File.

Let us now turn to the case α = 1. The following result recalls the expression of the conditional

expectation in this case.

Theorem 2.3 (Theorem 5.2.3, ST94) Let (X1, X2) be α-stable, with α = 1 and spectral representa-

tion (Γ, 0), where Γ satisﬁes (2.2) with ν > 0. Then, for almost every x,

E[X2|X1 = x] = −aσ1q0 + κ1(x − µ1) +

λ1 − β1κ1
β1

(cid:20)
(x − µ1) − σ1

U (x)
πfX1(x)

(cid:21)
,

if β1 6= 0, and

E[X2|X1 = x] = −aσ1q0 + κ1(x − µ1) − aσ1λ1

V (x)
πfX1(x)

,

if β1 = 0. Here, a = 2/π, σ1, β1, κ1 and λ1 are as in (2.3), fX1 is the marginal density of X1 ∼

S(1, β1, σ1, µ1), q0 =

1
σ1

R
S2

s2 ln |s1|Γ(ds), µ1 = −a R

S2

(E.13) in the Supplementary File.

s1 ln |s1|Γ(ds), and U, V are given in (E.12)-

We next provide our result for the second order conditional moment when α = 1. As for the conditional

expectation, two diﬀerent expressions hold according to whether the marginal distribution of X1 is skewed

or symmetric.

6

Theorem 2.4 Let (X1, X2) be α-stable, with α = 1 and spectral representation (Γ, 0), where Γ satisﬁes

(2.2) with ν > 1. Then, for almost every x,

Eh

X 2
2

(cid:12)
(cid:12)
(cid:12)X1 = x

i

= σ2

1(a2q2

0 − κ2

1) +

2σ1λ1
β1

(cid:16)

(cid:17)
σ1κ1 − aq0(x − µ1)

+

(cid:16)

(cid:16)

+

+

aσ1q0(λ1 − β1κ1) + (κ1λ1 − λ2)(x − µ1)

λ2 + β1κ2 − 2κ1λ1 + a2σ1β1(λ2

1 − β1λ2)W (x)

(cid:17)

(cid:16)

(x − µ1)2 − σ2
1

λ2
β1
(cid:17) 2σ1U (x)
β1πfX1(x)
σ1
(cid:17)
β1πfX1(x)

,

if β1 6= 0, and

Eh

X 2
2

(cid:12)
i
(cid:12)
(cid:12)X1 = x

= σ2

1(κ2 + a2q2

0 − κ2
FX1(x) − 1/2
fX1(x)

1) − 2aσ1κ1q0(x − µ1) + κ2(x − µ1)2
(cid:20)
(cid:16)
2

(cid:17)
aσ1q0 − κ1(x − µ1)

+

aσ1λ1
πfX1(x)

+ aσ1(λ2 − 2λ1κ1)

V (x) + aσ1λ1W (x)

(cid:21)
,

if β1 = 0. Here, a = 2/π, σ1, β1, the κp’s and the λp’s are as in (2.3), fX1 and FX1 are respectively the

marginal density and cumulative distribution function of X1 ∼ S(1, β1, σ1, µ1), q0 =
µ1 = −a R

s1 ln |s1|Γ(ds), and U, V and W are given in (E.12)-(E.14) in the Supplementary File.

S2

1
σ1

R

S2

s2 ln |s1|Γ(ds),

Proof. See Section E in the Supplementary File.

The expressions of the conditional moments simplify when one considers the asymptotics with respect to

the conditioning variable, as X1 = x becomes large.

Proposition 2.1 Let p ∈ {1, 2, 3, 4} and let (X1, X2) be α-stable with α ∈ (0, 2), and spectral represen-

tation (Γ, 0) such that the conditional moment of order p exists. If |β1| 6= 1, then

x−p Eh

κp + λp
1 + β1
and if |β1| = 1 and β1x → +∞, then, x−p Eh

(cid:12)
(cid:12)
(cid:12)X1 = x

−→
x→+∞

X p
2

i

X p
2

(cid:12)
(cid:12)
(cid:12)X1 = x

i

−→κp.

,

x−p Eh

X p
2

(cid:12)
(cid:12)
(cid:12)X1 = x

i

−→
x→−∞

κp − λp
1 − β1

,

Proof. See Section F in the Supplementary File.

3 Conditional moments of noncausal α-stable processes

Operating the set of properties of bivariate α-stable distributions provided in the previous section, we

study the existence and expressions of the conditional moments of α-stable inﬁnite moving average

processes. Discussions on practical aspects as well as examples focusing on modelling practices of the

empirical noncausal literature follow the main result. Let us consider (Xt) a two-sided MA(∞) process

as in (1.1) with α-stable errors εt

i.i.d.∼ S(α, β, σ, µ) and coeﬃcients (ak) satisfying

|ak|s < +∞, for some s ∈ (0, α) ∩ [0, 1],

(3.1)

X

k∈Z

7

and in addition for α = 1, β 6= 0, X
k∈Z

(cid:12)
(cid:12)
(cid:12)ln |ak|
|ak|

(cid:12)
(cid:12)
(cid:12) < +∞.

(3.2)

Conditions (3.1)-(3.2) ensure that P

k∈Z akεt+k converges absolutely almost surely so that (Xt) is well

deﬁned and strictly stationary. A moving average process of the form (1.1) satisfying the above conditions

is said to be purely causal if ak = 0 for k > 0 and purely noncausal if ak = 0 for k < 0. Noncausality is

found to be crucial for the existence of conditional moments higher than order α. An important class of

models that we shall consider and which admits MA(∞) representations satisfying the above conditions is

the class of ARMA processes. General ARMA processes –causal, noncausal, invertible or non-invertible–

are strictly stationary solutions of stochastic recursive equations of the form

ψ(F )φ(B)Xt = Θ(F )H(B)εt,

(3.3)

where F (resp. B = F −1) denotes the forward (resp. backward) operator, ψ(z) := 1 − ψ1z − . . . − ψpzp
and φ(z) := 1 − φ1z − . . . − φqzq are polynomials of degrees p and q, and H and Θ are two polynomials

of respective degrees r and s with roots on or outside the unit circle. Equation (3.3) admits a unique

strictly stationary solution provided that ψ(z) 6= 0, φ(z) 6= 0 for |z| ≤ 1, and that ψ (resp. φ) has no

common root with Θ (resp. H). The stationary solution is noncausal if p ≥ 1.

3.1 Spectral representation of (Xt, Xt+h)

Because the error sequence (εt) is α-stable distributed, the bivariate vector (Xt, Xt+h), for (Xt) satisfying

(1.1), (3.1) and (3.2), is itself α-stable for any horizon h and the results from the previous section apply.

This is a consequence of the following lemma, which provides the spectral representation of discrete time

vectors of linear moving averages driven by α-stable i.i.d. errors.

Lemma 3.1 Let 0 < α < 2. For εt
both satisfying (3.1)-(3.2), let Xt = (X1,t, X2,t), with Xi,t = P
for k ∈ Z. Then, Xt is an α-stable random vector in R2, with spectral representation (Γ, µ0) given by

i.i.d.∼ S(α, β, σ, µ) and real deterministic sequences (ak,i)k, i = 1, 2,

k∈Z ak,iεt+k, and denote ak = (ak,1, ak,2)

Γ(A) = σα X
s=±1

X

k∈Z

1 + sβ
2

kakkαδ(cid:26) sak
kakk

(cid:27)(A),

µ0 = X
k∈Z

akµ − 1{α=1}

2
π

σβ X
k∈Z

ak ln kakk,

(3.4)

for any Borel set A ⊂ S2, where δ{x}(A) = 1 if x ∈ A, else δ{x}(A) = 0, is the Dirac measure at point
x ∈ R2, k · k stands for the Euclidean norm, and by convention, if for some k ∈ Z, ak = 0, i.e., kakk = 0,
then the kth term vanishes from the sums.

Proof. See Section G in the Supplementary File.

8

3.2 Conditional moments

The results on bivariate stable vectors immediately apply to X t = (Xt, Xt+h) with ak = (ak, ak−h). A

suﬃcient condition for the existence of conditional moments is given in the following proposition as well

as their expressions. Without loss of generality, we will assume in the rest of this section that the stable

errors have zero location parameter, i.e., µ = 0, unless stated otherwise.

Proposition 3.1 Let (Xt) be an α-stable two-sided MA(∞) process, 0 < α < 2, β ∈ [−1, 1], σ > 0,

satisfying (1.1), (3.1)-(3.2) and let h ≥ 1.

ι) Assume there is ν ≥ 0 such that

(cid:0)a2

k + a2

k−h

(cid:1) α+ν

2 |ak|−ν < +∞.

(3.5)

X

k∈Z

Then E[|Xt+h|γ|Xt] < +∞ for 0 ≤ γ < min(α + ν, 2α + 1).
ιι) For α 6= 1, the moments E[X p

1 = σα X
σα
k∈Z

|ak|α,

β1 = β

a<α>
k
|ak|α ,

P
k∈Z
P
k∈Z

t+h|Xt], p ≤ 4, when they exist, are given by Theorems 2.1-2.2 with
(cid:19)p

(cid:19)p

(cid:18) ak−h
ak
|ak|α

|ak|α

P
k∈Z

P
k∈Z

κp =

,

λp = β

(cid:18) ak−h
ak
|ak|α

P
k∈Z

a<α>
k

P
k∈Z

.

ιιι) For α = 1, let ( ˜Xt, ˜Xt+h) := (Xt, Xt+h) − µ0 where µ0 is the shift vector as in Lemma 3.1. Then,
the ﬁrst- and second-order moments of ˜Xt+h| ˜Xt are respectively given by Theorems 2.3-2.4 with the κp’s,

λp’s, σ1, β1 as in ιι) and

q0 = β X
k∈Z

ak−h ln

!

|ak|
a2
k + a2

k−h

/ X
k∈Z

|ak|,

µ1 = −

2σβ
π

X

k∈Z

ak ln

|ak|
a2
k + a2

k−h

!

.

By convention, in all the points above, if (ak, ak−h) = (0, 0), then the kth term vanishes from the sums.

Remark 3.1 (Existence of moments) Point ι) provides a suﬃcient condition for the existence of

conditional moments. Notice that the left-hand side of (3.5) is an increasing function of ν. Thus, if

(3.5) holds for some ν0 > 0, it then holds for any 0 ≤ ν ≤ ν0, and if it fails for ν0, it then fails for all
ν ≥ ν0. Causal processes, say of the form P
k≤0 akεt+k with a0 = 1, automatically fail condition (3.5) for
all ν > 0, as (ah, a0) = (0, 1) and the hth term of the sum is ﬁnite only if ν = 0. In the case of symmetric

errors (β = 0), Theorem 1.1 by Cioczek-Georges and Taqqu (1995b) allows to conclude that (3.5) is also

necessary and hence that causal processes do not have ﬁnite conditional moments for orders higher than

α. Conversely, (3.5) may hold for some ν > 0 for noncausal processes provided the coeﬃcients (ak) do not

decay too fast as k → +∞. In fact, the slower the decay of (ak) as k → +∞, the higher the values of ν

for which (3.5) will hold. In other terms, the stronger the dependence on «future» errors, the higher the

9

 
 
order at which conditional moments will exist: hence the intuition that higher-order conditional moments

may exist provided that the process is anticipative or noncausal enough. It is easy to show that (3.5)

holds for any ν ≥ 0 as soon as (ak) decays geometrically or hyperbolically, guaranteeing the existence of

conditional moments up to order 2α + 1 at all prediction horizons for noncausal ARMA and fractionally

integrated processes. Consider for instance a noncausal process (Xt) of the form (1.1) such that ak = 0

for k < 0, ak 6= 0 for k ≥ 0 and ak ∼

k→+∞

cλk, for some non-zero constant c and λ ∈ (−1, 1). Letting

ν ≥ 0,

(a2

k + a2

k−h)

α+ν

2 |ak|−ν = |ak|α

(cid:19) α+ν

2

(cid:18)

1 +

a2
k−h
a2
k

∼
k→+∞

|c|α(1 + λ−2h)

α+ν

2 |λ|αk,

and since |λ|α < 1, the summability condition (3.5) holds for any ν ≥ 0. In particular, it holds for ν = α+1

and therefore, Point ι of Proposition 3.1 ensures that (Xt) admits ﬁnite conditional moments up to order

2α + 1. It is possible to ﬁnd noncausal processes for which (3.5) holds only up to some ν ∈ [0, α + 1),

i.e., entailing that conditional moments are ﬁnite only up to order γ strictly within (α, 2α + 1), with γ

moreover depending on the prediction horizon. Such processes are necessarily noncausal and typically

feature extremely short range dependence on future errors. See Section A.1 in the Supplementary File

for an example.

Remark 3.2 (Computational aspects) From a computational perspective, the conditional moments

of Xt+h given Xt = x given in Proposition 3.1 can be inexpensively calculated for various horizons h and

conditioning values x. In the case α 6= 1, computing these moments requires evaluating the functions
H(cid:0)n, θ; x(cid:1), n = 2, 3, 4, appearing in Theorem 2.2, which depend both on x and on h through the κp’s

and λp’s given in point ιι of Proposition 3.1. These functions can be decomposed into ahun(x) + bhvn(x),

where ah and bh are constants depending only on h and ﬁxed parameters of the process, while un(x) =

H(n, (0, 1); x) and vn = H(n, (1, 0); x) are integrals of a single variable which need only to be computed

once for a given conditioning value x. Computing these integrals requires paying attention to two main

hurdles. First, these are improper integrals on (0, ∞), which requires truncating the integral using a high

enough cutoﬀ value U > 0. This will typically yield a good approximation as the integrand vanishes

at exponential speed. Notice that the speed of the decay does not depend on h nor x and a single

suﬃciently high threshold will do for all horizons and conditioning values. Second, the integrand contains

an oscillatory term, whose «frequency» increases with |x|. This requires choosing a suﬃciently ﬁne

subdivision of the truncated integration interval (0, U ). For lower magnitudes of |x|, coarser subdivisions

will suﬃce. As |x| grows larger, one might fear that the required ﬁneness of the subdivision will lead

to prohibitively expensive computational costs: in this large conditioning value regime, one can however

10

avoid the computation of the integral altogether and favour the asymptotic approximations given by

Proposition 2.1. Similar considerations hold for the moments in the case α = 1. More details can

be found in ST94 Section 5.5 on numerical techniques for computing the moment of order 1, which

recommendations are still relevant for higher orders.

3.3 Examples

3.3.1 Mixed ARMA processes

Mixed causal-noncausal AR (MAR) processes are often invoked in the empirical noncausal literature

for speculative bubble modelling. Their conditional distribution and moments are known analyti-

cally only in special cases [see Fries and Zakoian (2019) for details], and, beyond these special cases,

practical forecasting relies on the simulation- and sample-based methods by Lanne et al. (2012a) and

Gouriéroux and Jasiak (2016). Mixed causal-noncausal ARMA processes with in addition a possibly

non-invertible MA components (MARMA) as in (3.3) however, have not yet taken up as much as MAR

processes for speculative bubble modelling. This is probably due to the absence of analytical results

regarding their conditional distribution. Estimation procedures for such ARMA processes focus on pro-

viding estimators of the coeﬃcients of the AR and MA polynomials, whereas the results of Proposition
3.1 rely on the coeﬃcients of the MA(∞) representation Xt = P

k∈Z akεt+k. Fortunately, the coeﬃcients

(ak) can be recovered exactly from the AR and MA polynomials. For (Xt) a MARMA process solution

of Equation (3.3), we have from Gouriéroux and Jasiak (2016) Section 2.3 the following decomposition:

Xt = Bpb1(B)vt + b2(B)ut,

(3.6)

where b1(B) := Pq−1

i=0 b1,iBi and b2(B) := Pp−1

j=0 b2,jBj are the two polynomials resulting from the partial

fraction decomposition

1

(cid:16)

φ(B)

Bpψ(F )

(cid:17) =

b1(B)
φ(B)

+

b2(B)
Bpψ(F )

,

and where (vt) and (ut) are deﬁned by vt := ψ(F )Xt and ut := φ(B)Xt. Letting Zt := Θ(F )H(B)εt,

the processes (vt) and (ut) furthermore satisfy the recursions φ(B)vt = Zt and ψ(F )ut = Zt. When

Θ = H = 1, (Xt) reduces to a MAR process and (vt) and (ut) are respectively called the causal and

noncausal components of (Xt). Identifying the MA(∞) representations in (εt) of the left- and right-hand

side of (3.6) yields a general expression of the coeﬃcients (ak) as

∀k ∈ Z,

ak =

s
X

ϑ‘

q−1
X





‘=−r

i=0

b1,i c1,k+p+i−‘ +

b2,jc2,k+j−‘


 ,

p−1
X

j=0

(3.7)

11

where Ps

‘=−r ϑ‘F ‘ := Θ(F )H(B), (c1,k) and (c2,k) are the coeﬃcients of the Laurent expansions of
1/φ(z) and 1/ψ(z) [Conway (1978) p.107], which are such that c1,k = 0 for k > 0 ; c2,k = 0 for k < 0 ;

c1,0 = c2,0 = 1 and otherwise recursively obtained from the AR polynomials as

∀k < 0,

c1,k =

q
X

i=1

φic1,k+i,

∀k > 0,

c2,k =

p
X

j=1

ψjc2,k−j.

Proposition 3.1 then applies to the MARMA process (Xt) with coeﬃcients sequence (ak) as in (3.7). For

practical purposes, the inﬁnite sums in Proposition 3.1 can be truncated. For MARMA processes, (ak)

vanishes geometrically fast as |k| → ∞ and truncation will typically yield a good approximation.

A simulation experiment was conducted to illustrate the results of Proposition 3.1 in the case of a

MARMA process. The theoretical conditional moments are compared to model-free non-parametrically

estimated counterparts in order to assess the validity of the analytical formulae. Let us consider, for

the strictly stationary solution of (1−0.9F )(1+0.3B)Xt = (1+0.4F )(1−0.3B)εt, εt

expository purposes, that the price series (Xt) of an asset is modelled by the MARMA process deﬁned as
i.i.d.∼ S(1.8, 0.5, 0.2, 10).
Xt+h − Xt
Xt

We will focus on the conditional moments of the returns at horizon h of (Xt), denoted Rt+h =

.

On the one hand, we use the formulae of Proposition 3.1 to compute the theoretical expectation, standard

deviation, skewness and excess kurtosis of the returns, conditional on the level Xt = x:

µ(x, h) := E[Rt+h|Xt = x],

γ1(x, h) := E

(cid:20)(cid:18) Rt+h − µ(x, h)
σ(x, h)

(cid:19)3(cid:12)
(cid:12)
Xt = x
(cid:12)
(cid:12)

σ2(x, h) := Eh(cid:0)Rt+h − µ(x, h)(cid:1)2(cid:12)
(cid:12)
(cid:12)Xt = x
(cid:19)4(cid:12)
(cid:20)(cid:18) Rt+h − µ(x, h)
(cid:12)
(cid:12)
σ(x, h)
(cid:12)

(cid:21)
, γ2(x, h) := E

Xt = x

i
,

(3.8)

(cid:21)

− 3.

It is just a matter of expanding the powers in the deﬁnitions above to express the conditional moments
of Rt+h in terms of E[X p

t+h|Xt], p ∈ {1, 2, 3, 4}, where Xt = P

k∈Z akεt+k admits an α-stable MA(∞)

representation whose coeﬃcients are given by (3.7). On the other hand, we simulate M = 2000 trajectories
x(m)
1
and obtain model-free estimates of the conditional power moments E[X p

N , m = 1, . . . , M , with N = 107 observations of the aforementioned MARMA process
t+h|Xt = x] using Nadaraya-

, . . . , x(m)

, x(m)
2

Watson estimator

ˆE(m)(X p

t+h|Xt = x) :=

PN −h

i=1 Kw
PN −h

(cid:0)x − x(m)

i

(cid:1)(cid:0)x(m)
i+h
(cid:1)

(cid:1)p

,

j=1 Kw

(cid:0)x − x(m)

j

(x, h), ˆγ(m)

where Kw is the Gaussian kernel with bandwidth w. Empirical counterparts ˆµ(m)(x, h), ˆσ(m)(x, h),
ˆγ(m)
1
ing the non-parametric estimates ˆE(m)(X p

(x, h), m = 1, . . . , M , of µ(x, h), σ(x, h), γ1(x, h) and γ2(x, h) are obtained by substitut-
t+h|Xt = x) in (3.8) in place of E[X p

t+h|Xt = x]. We considered
prediction horizons h = 1, 3, 5, 10, conditioning values x in the interval (70, 85) –corresponding to the

2

12

0.0005 and 0.9995 quantiles of the marginal distribution of Xt: 99.9% of the probability mass of Xt is

supported on (70, 85)– and used a bandwidth of w = 0.1. Letting N W (m) denote generically any of ˆµ(m),
ˆσ(m), ˆγ(m)

, we compute for each quantity the point-wise average of Nadarya-Watson estimators as

m=1 N W (m)(x, h) as well as the point-wise 0.05 and 0.95 quantiles across simulations.
Figure 2 compares the theoretical conditional moments obtained using Proposition 3.1 and (3.7) with

PM

, γ(m)
2

1
N W (x, h) := 1
M

their empirical non-parametric counterparts. We notice that the average N W is very closely matching

the theoretical moments curves, and that the theoretical moments lie everywhere within the empirical

0.05-0.95 interquantile. This provides evidence for the sanity of Theorem 2.2 and Proposition 3.1. In ad-

dition, we notice that the dispersion of the model-free non-parametric estimators is rather important for

Xt = x far from central values, despite the length of the simulated trajectories (N = 107 observations).

This suggests that the analytical formulae can hardly be traded for purely data-driven methods when it

comes to estimating the dynamics during extreme events, even with massive amounts of data.

To compute the conditional moments in practice, one can now overlook the model-free non-parametric

approach and resort to a parametric plug-in strategy: e.g., estimate the MARMA and stable parame-

ters by maximum likelihood and plug the parameter estimates in the formulae of Proposition 3.1. An

additional experiment reported in Section A.2 of the Supplementary File illustrates the reliability of the

latter parametric plug-in strategy, and its ability to accurately recover the conditional moments curves

for practically-relevant sample sizes. Illustrations of the shape of the conditional moments for various

parameterisations of MARMA processes are also provided.

Remark 3.3 The asymptotic properties of the Nadaraya-Watson for strongly mixing sequences with

bounded second order marginal moment have been established by Hansen (2008). In our context, where

higher-order conditional moments may be bounded in spite of inﬁnite marginal variance, the validity of

the Nadarya-Watson estimator is an open issue. The agreement between the theoretical moment curves

and the empirical ones obtained with the Nadaraya-Watson estimator also suggests that the latter’s

validity may extend. This is left for further research.

3.3.2 Cauchy MA(∞) processes

MAR processes with Cauchy errors (stable with α = 1 and β = 0) are a popular benchmark for speculative

bubble modelling in the noncausal literature [e.g., Hencic and Gouriéroux (2015), Hecq et al. (2016),

Fries and Zakoian (2019), Gouriéroux et al. (2019), Cavaliere et al. (2020), Hecq and Voisin (2020)]. An

attractive feature of this class of models is that the Cauchy distribution is one of the special cases in

the stable family for which a closed-form density is available. For Cauchy MAR processes with a single

13

Figure 2: Conditional expectation, standard deviation, skewness and excess kurtosis (in rows) of the returns Rt+h =

(Xt+h −Xt)/Xt at horizons h = 1, 3, 5, 10 (in columns) of the ARMA process (1−0.9F )(1+0.3B)Xt = (1+0.4F )(1−0.3B)εt,
i.i.d.∼ S(1.8, 0.5, 0.2, 10) for conditioning values Xt = x ∈ (70, 85) (x-axis of each plot, 99.9% of the probability mass of the

εt

marginal distribution of Xt is supported on (70,85)). Black solid lines: theoretical moments (3.8) given by Proposition 3.1

and (3.7); Grey dotted lines: average of Nadaraya-Watson estimators (bandwidth=0.1) across 2000 simulated trajectories

of 107 observations each; Grey shaded areas: empirical 0.05-0.95 interquantile interval across simulations.

noncausal root, i.e., as in (3.3) with Θ = H = 1, p = 1 and q ≥ 0, the decomposition into causal

and noncausal components (3.6) allows to obtain the conditional moments and density in closed-form.

The techniques based on decomposition (3.6) do not extend however, and no result is available for more

general Cauchy noncausal processes.

14

Let us apply Proposition 3.1 to Xt = P

k∈Z akεt+k with εt

i.i.d.∼ S(1, 0, σ, 0) and (ak) such that Point

ι) guarantees the existence of the ﬁrst- and second-order moments (e.g., a Cauchy MARMA process).

Then, invoking Point ιιι) with β1 = λ1 = µ1 = q0 = 0 since β = 0, we have for any x ∈ R and h ≥ 1,

E[Xt+h|Xt = x] = κ1x,

V(Xt+h|Xt = x) = (κ2 − κ2

1)(x2 + σ2

1).

In particular, if ak ≥ 0 for all k, then κ1 = P

k∈Z |ak|(ak−h/ak)/ P

k∈Z |ak| = 1 and E[Xt+h|Xt] = Xt.

Remark 3.4 (Conditional heteroscedasticity of noncausal processes)

Gouriéroux and Zakoian (2017) and Fries and Zakoian (2019) highlighted that the Cauchy noncausal

AR(1) and MAR(1, q) processes exhibit GARCH eﬀects in calendar time, although seemingly deﬁned

based on i.i.d. errors. The above result shows that this property extends to Cauchy MA(∞) processes.

Figure 2 further illustrates that this is not a speciﬁc feature of the Cauchy distribution, and that mod-

elling prices with noncausal α-stable processes also induces conditional heteroscedasticity in the returns

for other values of α. In the Cauchy case, the conditional volatility is quadratic in the past values and the

authors underlined that (Xt) admits a semi-strong double autoregressive representation à la Ling (2007).

The conditional ﬁrst and second moments in Proposition 3.1 suggests that a more complex representation

may hold in general for α 6= 1. Proposition 2.1 ensures nevertheless that the variance of Xt+h|Xt is still

asymptotically quadratic in the conditioning value. This can be noticed in the example of the following

section.

3.3.3 α-stable noncausal AR(1)

Let (Xt) be the α-stable noncausal AR(1) solution of Xt = ρXt+1 + εt, εt
(for simplicity), β ∈ [−1, 1], σ > 0 and 0 < |ρ| < 1. Then E[|Xt+h|γ|Xt] < +∞ for 0 ≤ γ < 2α + 1 and

i.i.d.∼ S(α, β, σ, 0) with α 6= 1

h ≥ 1, and the conditional moments, when they exist, are given by Proposition 3.1 with

σα
1 =

σα
1 − |ρ|α ,

β1 = β

1 − |ρ|α
1 − ρ<α> ,

κp = |ρ|αhρ−hp,

λp = β1

(cid:0)ρ<α>(cid:1)hρ−hp,

for p ∈ {1, 2, 3, 4}. For ρ > 0, a clear interpretation of the distribution Xt+h|Xt = x appears during

bubble episodes, that is, as x becomes large relative to the central values of process (Xt). Letting µ(x, h),
σ2(x, h), γ1(x, h) and γ2(x, h) denote the conditional expectation, variance, skewness and excess kurtosis

of Xt+h given Xt = x respectively (as in (3.8) with Rt+h replaced by Xt+h), when they exist, we have

µ(x, h) ∼ (ρ−hx)ραh,

γ1(x, h) −→ s

σ2(x, h) ∼ (ρ−hx)2ραh(1 − ραh),

γ2(x, h) −→

15

1 − 2ραh

,

q

ραh(1 − ραh)
1

1 − ραh − 6,

1
ραh +

as β1x → +∞ if |β1| = 1, x → ±∞ if |β1| 6= 1, and s = 1 (s = −1) if x → +∞ (x → −∞). See Section

H in the Supplementary File for the proof.

4 Forecasting noncausal bubble crashes

For practical econometric purposes, ﬁnancial bubbles in stock prices, market indexes and price-dividend

ratios are typically characterised as short-lived explosive episodes followed by abrupt or gradual collapses,

and are analysed using reduced form models [Phillips and Shi (2018)]. In this section, we focus on the

dynamics of noncausal processes during such explosive episodes, that is, when the conditioning level of

the trajectory takes on large positive or negative values. The strikingly simplistic forms of the conditional

moments of the α-stable noncausal AR(1) during such events, as given in Section 3.3.3, are characteristic

of a weighted Bernoulli distribution charging probability ραh to the value ρ−hx and probability 1 − ραh to

0. In the framework of this model, it is thus natural to interpret ραh as the probability that the bubble

survives at least h more time steps, conditionally on having reached the level Xt = x.

Such simpliﬁcation of the dynamics during extreme events is actually not limited to the α-stable

noncausal AR(1). We derive here closed-form expressions of the ex ante crash odds of bubbles generated

by noncausal processes. We ﬁrst formally establish in the case of the noncausal AR(1) that the intuition

described above holds. We then show that this intuition non-trivially extends to processes featuring

noncausal AR(1)-type bubbles followed by almost arbitrarily shaped collapses after the peak. We end

this section by obtaining an expression of the crash odds in the case of noncausal MA(∞) processes.

As we focus on the extreme events, we do not need to fully specify a parametric distribution for the

errors (εt) as in Section 3, but only require that their probability tails are similar to those of an α-stable

distribution in that they decay as power-laws. Formally, we assume that (εt) is an i.i.d. error sequence

with regularly varying tails:

P(|ε0| > x) = x−αL(x),

P(ε0 > x)
P(|ε0| > x)

−→
x→+∞

1 + β
2

∈ [0, 1],

(4.1)

with tail parameter α > 0, asymmetry β ∈ [−1, 1] and L any slowly varying function at inﬁnity, i.e., such

that L(tx)/L(x) → 1 as x → +∞ for all t > 0. The α-stable distribution, with α ∈ (0, 2) and asymmetry

parameter β, is a typical example of distribution whose tails are power-law as in (4.1). However, the

more general assumption above and the results in the rest of this section encompass not only noncausal

processes with α-stable errors, for which we derived the moments in the previous section, but noncausal

processes with any power-law tailed errors, including (skewed) t-student errors often invoked in the

empirical noncausal literature. Note furthermore that the tail exponent α (or degrees of freedom in the

16

case of the t-student) is not restricted to be below 2 in this section but can take any positive value.

4.1 Crash odds of noncausal AR(1)-type bubbles

4.1.1 Purely noncausal AR(1) : exponential bubbles with instant collapses

The following proposition provides the conditional distribution of the noncausal AR(1) during explosive

bubble episodes.

Proposition 4.1 Let (Xt) be the noncausal AR(1) process solution of Xt = ρXt+1 + εt with 0 < ρ < 1,

i.i.d. errors (εt) satisfying (4.1) for some tail exponent α > 0 and asymmetry β ∈ [−1, 1]. Then, for any

h ≥ 1, any δ ∈ (0, ρ−h), we have as x → +∞

P

(cid:18) Xt+h
Xt

∈ [ρ−h − δ, ρ−h + δ]

(cid:19)

(cid:12)
(cid:12)
sXt > x
(cid:12)
(cid:12)

−→ ραh,

P

(cid:18) Xt+h
Xt

(cid:12)
(cid:12)
∈ [−δ, δ]
(cid:12)
(cid:12)

(cid:19)

sXt > x

−→ 1 − ραh,

for any s ∈ {−1, +1} if β ∈ (−1, 1), and s = β if |β| = 1.

Proof. See Section I in the Supplementary File.

The proposition formalises the intuition that bubbles generated by a noncausal AR(1) with regularly

varying errors feature a geometric survival distribution with probability parameter ρα. This interpretation

implies that the survival probability does not depend on the current scale of the bubble. Surprisingly,

given that the noncausal AR(1) is a Markov process, it further implies that the survival probability of

bubbles does not depend at all on the past history: such bubbles display a memory-less property. Several

statistics of interest can be easily computed to describe their survival distribution, e.g., crash probability

at horizon h, hazard rate, expected lifetime. As the bubbles are memory-less, their survival distribution

can be fully characterised by the so-called half-life, or median survival time: the duration h1/2 such that

the crash probability at horizon h1/2 is 1/2. More generally, one can be interested in the q-survival

quantile, q ∈ [0, 1], that is, the duration hq such that the survival probability at horizon hq is equal to

1 − q. Table 1 summarises the expressions of these descriptive survival statistics for bubbles generated

by a noncausal AR(1) model with regularly varing errors. Computing these statistics only requires the

Crash probability at hor. h Hazard rate Expected life

q-Survival quantile

1 − ραh

1 − ρα

1
1 − ρα

ln(1 − q)
α ln ρ

Table 1: Descriptive survival statistics of bubbles generated by a heavy-tailed noncausal AR(1) with AR coeﬃcient ρ ∈ (0, 1)

and tail exponent α > 0.

17

knowledge of the AR coeﬃcient ρ and of the tail exponent α. Typically, bubbles with smaller growth

rates (ρ closer to unity) and driven by heavier-tailed shocks (smaller α) are likely to last longer.

On the one hand, the memory-less property of these bubbles could be appealing from a ﬁnancial and

economic perspective as it implies that the crash date cannot be known with certainty by traders, hence

ensuring a form of no-arbitrage condition. Bubbles with crash dates arising according to a constant hazard

rate –another feature of the geometric distribution– appear moreover compatible with the implications of

game theoretic settings where arbitrageurs attempt to time exponentially increasing bubbles and induce

the crash at a random date when the selling pressure they exert is high enough [Matsushima (2013)].

On the other hand, the memory-less property also implies that no sophisticated method could allow a

forecaster to say anything more regarding the future of AR(1) bubbles than «growth or crash» with

the probabilities above. In the case of non-exponentially shaped bubbles or if the extreme errors driving

bubbles are assumed to be endogenous rather than i.i.d. (as in Blasques et al. (2018)), past history could

however play a more central role for prediction.

Remark 4.1 (Parallel with Blanchard and Watson (1982)) The

dynamics

of

the

non-

causal AR(1) during bubble

episodes

is

reminiscent of

the

classical model proposed by

Blanchard and Watson (1982):

Xt+1 = ρ∗ctXt + (cid:15)t,

X0 = 0,

(4.2)

where ρ∗ > 1, ((cid:15)t) is an i.i.d. zero-mean and ﬁnite variance error sequence, and (ct) are i.i.d. Bernoulli
distributed random variables such that P(ct = 1) = 1 − P(ct = 0) = p ∈ (0, 1). This model recurrently

generates exponentially-shaped explosive bubbles: the trajectory follows an explosive path while ct = 1

and ends in a crash when ct = 0.

In view of Proposition 4.1, the bubble episodes generated by a

noncausal AR(1) with regularly varying errors follow a dynamics à la Blanchard and Watson with ρ∗ =

ρ−1 and p = ρα.

Interestingly, while Blanchard and Watson’s model is explicitly designed to feature

successive bubble/bust cycles, where the bust probability is a free parameter, the noncausal AR(1)

generates trajectories where bubble events intersperse calmer periods. The dynamics (4.1) only emerges

during bubble events and the crash probability is rather a function of the other model parameters. The

structural constraint on the survival probability p = ρα –speciﬁc to the noncausal AR(1), a linear process

shown to be suitable to describe bubble components of solutions to rational expectation price models

[see Gouriéroux et al. (2020)]– has important statistical implications.

In the framework of Blanchard

and Watson’s model, statistical information about p can only be gathered from the observed durations

of past bubbles that have already collapsed. Assuming m bubbles of durations T1, . . . , Tm are observed

18

on a given time series, say, generated by (4.2), one could propose ˆp = m/ Pm

i=1 Ti as an estimator for the

parameter p of the Bernoulli variables (ct). In bubble modelling applications it is however not uncommon

to face very small m situations, or even m = 0 in cases where a single explosive and uncollapsed trend

is observed. This renders accurate estimation of p diﬃcult at best, and unfeasible at worst. West (1987)

even considered p not to be an identiﬁable parameter. In contrast, the estimation of ρα can exploit more

information present in the data: the sample autocorrelations of the time series and the bubble growth

rates provide information about ρ, while the tail heaviness of the time series and of the residuals (obtained

after estimation of ρ) provide information about α. A maximum likelihood estimation of the noncausal

AR(1) assuming a parametric distribution for the errors, such as α-stable or t-student, would suﬃce to

obtain an estimate of ρα. Semi-parametric approaches could be operative as well, e.g., estimating ρ by

Least Squares and α using the Hill estimator.

4.1.2 Mixed causal-noncausal AR(1) : exponential bubbles with arbitrary collapses

To encompass explosive exponential bubble patterns followed by more complex post-peak dynamics, the

noncausal literature considered adding a causal component to the noncausal AR(1), resulting in the

much-invoked MAR(1, q) processes [see for instance Hecq and Voisin (2020), Gouriéroux et al. (2019)].

We show here that whatever the form of the causal component adjoined to the noncausal AR(1), i.e.,

whatever the shape of the collapse after the exponential growth episode, the crash probability –or more

accurately, the probability of reaching the end of the exponential growth– still follows from a geometric

distribution with parameter ρα. We do not restrict to the case of MAR processes but actually consider

any process (Xt) satisfying (1.1) with ak = ρk for all k ≥ 0. Such a process satisﬁes the autoregression
Xt = ρXt+1 + Zt, where Zt := P
‘≤0 b‘εt+‘, with b‘ = a‘ − ρa‘−1 for all ‘ ≤ 0. Letting m ≥ 1, h ≥ 1,
and ρ := (ρm, ρm−1, . . . , ρ, 1), we will state our result in the context of a forecaster observing an ongoing

explosive exponential episode, that is, observing (Xt−m, . . . , Xt−1, Xt) being close to colinear with ρ, and

wishing to forecast the future path (Xt+1, . . . , Xt+h). The only restriction that we impose on (ak)k≤−1

is the one ruling out «collapses» that would be of similar shapes as the initial exponential growth. This

assumption is formalised below and the forecasting result follows.

Assumption 1 There is (cid:15) > 0 such that for all k ≤ −1 and λ ∈ R, (cid:12)
(cid:12)

(cid:12)
(cid:12)λ(ak+m, . . . , ak+1, ak) − ρ(cid:12)
(cid:12)
(cid:12) > (cid:15).
(cid:12)

Proposition 4.2 Let m ≥ 1, h ≥ 1, ρ ∈ (0, 1), α > 0 and (Xt) a two-sided MA(∞) process with i.i.d.
errors satisfying (1.1), (3.1)-(3.2) and (4.1) with ak = ρk for all k ≥ 0. Denote X t := (Xt−m, . . . , Xt),
X t+h := (Xt+1, . . . , Xt+h), Ak := (ak−1, . . . , ak−h)/|ak| = (ρ−1, . . . , ρ−k, ρ−ka−1, . . . , ρ−kak−h) for k ∈

19

{0, . . . , h}, k · k any norm and d := min

k,‘∈{0,...,h}
k6=‘

kAk − A‘k. If Assumption 1 holds for some (cid:15) > 0, we then

have that d > 0, and for any η ∈ (0, (cid:15)), δ ∈ (0, d),

P

 (cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

X t+h
|Xt|

− sAk

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

< δ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

|Xt| > x,

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

X t
|Xt|

(cid:12)
(cid:12)
− sρ
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

!

< η

−→
x→+∞






ραk(1 − ρα),

if k ∈ {0, . . . , h − 1},

ραh,

if k = h.

for any s ∈ {−1, +1} if β ∈ (−1, 1), and s = β if |β| = 1.

Proof. See Section J in the Supplementary File.

The above result enjoys a very intuitive pattern interpretation. We illustrate this on the example of the

MAR(1,1) below. Let us already highlight that the odds of reaching the end of an observed exponential

growth episode at some future horizon are of the same form as the crash odds of a purely noncausal

AR(1), i.e., geometric governed by ρα. This drastically simpliﬁes the peak-date prediction exercise for a

forecaster, who only has to estimate two parameters and can even aﬀord to stay agnostic as to whatever

form the collapse following the peak will take.

Example 4.1 (Forecasting MAR(1,1) bubbles) Consider the MAR(1,1) process deﬁned as the

strictly stationary solution of

(1 − ρF )(1 − φB)Xt = εt,

(4.3)

where |φ| < 1 and (εt) is an i.i.d. sequence of regularly varying errors as in (4.1) with tail index α > 0.
The process (Xt) admits the MA(∞) representation Xt = P
k∈Z akηt+k, with ak = ρk for k ≥ 0 ; ak = φ−k
for k ≤ −1 ; and ηt := εt/(1 − ρφ) for all t ∈ Z. Note that (ηt) is still an i.i.d. regularly varying sequence

with index α. Assumption 1 can be shown to hold and Proposition 4.2 applies to (Xt) with the sequence

(ak) as described above and

Ak =






(φ, φ2, . . . , φh),

(ρ−1, ρ−2, . . . , ρ−k
|
{z
}
k
(ρ−1, ρ−2, . . . , ρ−h),

, ρ−kφ, ρ−kφ2, . . . , ρ−kφh−k
}
{z
|
h−k

for k = 0,

),

for k ∈ {1, . . . , h − 1},

for k = h,

If a forecaster observes during an extreme event of process (Xt) that the recent past trajectory has
approximately an exponential shape of growth rate ρ−1, i.e., if one observes that (Xt−m, . . . , Xt) is
approximately colinear to (ρm, . . . , ρ, 1), then the forecaster may assert that the exponential growth has

probability ραh to continue at least until horizon h, and probability ραk(1 − ρα) to stop at an earlier date

k ∈ {0, . . . , h − 1}. Whenever the exponential growth will reach a peak, the trajectory will then enter

a phase of exponential decay, with decay rate φ. Figure 3 illustrates the forecast interpretation from a

trajectorial and probability tree perspectives.

20

Xt

Xt+1

Xt+2

ρ−1x

ρ α

x

1−ρα

φx

ρ−2x

ρ α

1−ρα

ρ−1φx

1

φ2x

Xt+3

ρ−3x

ρ α

1−ρα

ρ−2φx

1

1

ρ−1φ2x

φ3x

Figure 3: Illustration of the likely future paths of a bubble generated by a MAR(1,1) process with regularly varying errors

as in (4.3). Left panel: trajectorial interpretation with the past observed path in full points, the explosive exponential trend

in solid line, and projected likely future paths in dotted lines and circles (graph drawn using ρ = 0.8 and φ = 0.4). Right

panel: probability tree interpretation of the projected future paths with outcomes at the origins and ends of arrows, and

probabilities next to the arrows.

Remark 4.2 (Rational bubbles and fat tails) Lux and Sornette (2002) showed that the marginal

distribution of rational expectation bubble models à la Blanchard and Watson (1982) necessarily feature

regularly varying tails. They further established that a necessary condition for any bubble process (Bt) of
the form (4.2) to abide to the rational expectation condition Bt = aEt[Bt+1], 0 < a < 1, where Et denotes

the expectation conditional on all information available at date t, is that the tail index of the regular

variations be strictly smaller than 1. Invoking evidence gathered by the empirical literature, which does

not support such degrees of fat-tailedness, Lux and Sornette conclude that rational bubble models à la

Blanchard and Watson are incompatible with the observed statistical properties of ﬁnancial data.

Interestingly, it appears that MAR processes could reconcile the rational expectations condition with

tail indexes greater than 1. In the MAR(1,1) example above, we have that during the inﬂation phase of

a bubble generated by (4.3), the one-step ahead conditional distribution is approximately behaved as

Xt+1 =






ρ−1Xt, with probability ρα,

φXt,

with probability 1 − ρα.

Thus, during the inﬂation phase of a bubble, Et[Xt+1] ≈ ρα(ρ−1Xt) + (1 − ρα)φXt, and

Xt ≈ (ρα−1 + φ − ραφ)−1Et[Xt+1].

21

The rational expectations condition requires (ρα−1 + φ − ραφ)−1 < 1, which can be rewritten as

α < ln

(cid:18) 1 − φ
ρ−1 − φ

(cid:19)

/ ln(ρ) := fρ(φ).

A straightforward analysis shows that, for any ρ ∈ (0, 1), the function φ 7→ fρ(φ) is strictly increasing on

[0, 1) and that fρ(0) = 1, lim
φ→1

fρ(φ) = +∞. For φ = 0, one retrieves Lux and Sornette’s result. For φ > 0

however, values of α above 1 are admissible. This suggests that the MAR(1,1), as a process featuring

Blanchard/Watson-like bubbles followed by gradual decays, can reconcile the rational expectations condi-

tion with regular variation tail indexes above 1. In fact, tail indexes arbitrarily large could be admissible

provided the decay after the peak is slow enough (φ close enough to 1).

4.2 Crash odds of noncausal MA(∞) bubbles

Noncausal MA(∞) processes, which encompass general pre-peak bubble shapes, also feature a simpliﬁca-

tion of their dynamics during extreme events. The following result generalises the second convergence in

Proposition 4.1 to express the ex ante crash odds of bubbles generated by noncausal MA(∞) processes.

Proposition 4.3 Let (Xt) be a MA(∞) process with i.i.d. errors as in (1.1), (3.1)-(3.2) and (4.1), with

ak = 0 for all k < 0 and ak > 0 for all k ≥ 0, tail exponent α > 0 and asymmetry β ∈ [−1, 1]. Assume
also that there is some (cid:15) > 0 such ak/ak+1 > (cid:15) for all k ≥ 0. Then, for any h ≥ 1, δ ∈ (0, (cid:15)h),

P

(cid:18) Xt+h
Xt

(cid:12)
(cid:12)
∈ [−δ, δ]
(cid:12)
(cid:12)

(cid:19)

sXt > x

−→
x→+∞

h−1
X

‘=0

aα
‘ /

+∞
X

k=0

aα
k := p∞,h,

(4.4)

for any s ∈ {−1, +1} if β ∈ (−1, 1), and s = β if |β| = 1.

Proof. See Section K in the Supplementary File.

Similarly to the interpretation of the noncausal AR(1), one can notice that the crash probability of

bubbles does not depend on their current scale. Contrary to the noncausal AR(1) however, the survival

probabilities could in general be diﬀerent if the past history of the bubble was accounted for in the

conditioning. To investigate this question, one has to characterise the conditional distribution of Xt+h

given more past information, e.g., Xt, Xt−1... This problem is out of the scope of the current paper and

is addressed elsewhere [Fries (2018)]. To evaluate the asymptotic probability (4.4) in practice, only the

knowledge of the coeﬃcients (ak) and of α is needed, whereas asymmetry, scale or location have no role.

We illustrate through simulations that the probability on the left-hand side of (4.4) indeed converges to

the right-hand side limit as the conditioning value x grows larger. We simulated M = 2000 trajectories
, . . . , x(m)
of N = 107 observations of a noncausal AR(3) process. For each simulated trajectory x(m)
N ,

1

22

h = 1

h = 5

h = 10

Mean

95%-CI

Mean

95%-CI

Mean

95%-CI

8.37

9.12

17.7

18.5

20.5

20.5

20.7

20.7

20.7

(8.33 , 8.41)

(9.08 , 9.17)

(17.5 , 17.8)

(18.3 , 18.7)

(19.9 , 21.1)

(19.9 , 21.1)

(18.9 , 22.9)

(18.9 , 22.9)

–

33.1

30.7

65.9

69.1

75.2

75.6

76.2

76.2

76.2

(33.0 , 33.2)

(30.6 , 30.9)

(65.4 , 66.4)

(68.7 , 69.6)

(73.7 , 76.8)

(74.1 , 77.1)

(71.6 , 80.9)

(71.4 , 81.1)

–

42.5

37.6

83.4

87.9

94.4

94.9

95.4

95.5

95.5

(42.3 , 42.7)

(37.4 , 37.7)

(83.0 , 83.8)

(87.5 , 88.3)

(93.3 , 95.3)

(93.9 , 95.8)

(92.3 , 98.0)

(92.4 , 98.1)

–

ˆpq,h

q0.9

q0.99

q0.999

q0.9999

t1.5

S1.5

t1.5

S1.5

t1.5

S1.5

t1.5

S1.5

p∞,h ∞

Table 2: Comparison of theoretical and empirical crash probabilities at horizons h = 1, 5, 10 of bubbles generated by
i.i.d.∼ S(1.5, 1, 0.25, 0) (S1.5) and

the noncausal AR(3) Xt = 0.9Xt+1 + 0.04Xt+2 − 0.096Xt+3 + εt with 1.5-stable errors εt

t-student errors with 1.5 degrees of freedom (t1.5). The theoretical crash probabilities are computed using (4.4). Empirical

average (Mean) and 95% conﬁdence intervals (95%-CI) of the estimated probabilities are computed using (4.5) on M = 2000

simulated trajectories of N = 107 observations, with δ = 0.2 and for q = qa several a-quantiles of the marginal distribution

of Xt. The quantiles of the marginal of Xt in the case of t-student errors have been estimated by simulations.

1 ≤ m ≤ M , we computed the following estimator of the probability (4.4):

ˆp(m)
q,h :=

 N −h
X

t=1

1

{|x(m)

t+h/x(m)

t

|<δ}∩{x(m)

t >q}

!

N −h
X

/

t=1

1

{x(m)

t >q}

,

(4.5)

for several horizons h and several quantiles q of the marginal distribution of Xt. We perform this exercise

twice, ﬁrst assuming that the AR(3) process is driven by 1.5-stable errors, and then assuming t-student

errors with 1.5 degrees of freedom. As our result holds for any heavy-tailed errors in the sense of (4.1) and

the tail exponents of the error sequences are equal, the estimated crash probabilities should tend to the

same limit as q increases. Table 2 gathers the average 1
M

q,h of the empirical probabilities across
the M simulations along empirical 95% conﬁdence intervals. One notices that the empirical probabilities

PM

m=1 ˆp(m)

indeed come very close to the theoretical ones as q increases, both for α-stable and t-student errors. The

dispersion of the non-parametric estimators across simulations again indicates that estimating the crash

odds of bubble events by purely data-driven methods might be challenging, even with massive amount

of data. The expressions given by Propositions 4.1, 4.2 and 4.3 thus oﬀer the attractive alternative of

computing plug-in estimators of crash odds after having estimated the model parameters.

Remark 4.3 (Tail dynamics and GARCH eﬀects) We here propose some intuition highlighting

23

the connection between the tail dynamics derived in the previous propositions and the emerging GARCH
eﬀects of noncausal processes. Consider for simplicity a purely noncausal process Xt = P
ak > 0 for k ≥ 0 and ak = 0 for k < 0. The εt’s being heavy-tailed and i.i.d., if Xt = P

k∈Z akεt+k with

s∈Z as−tεs at some

date t is observed extreme, this likely results from one given ετ being extreme, for some random date τ

in the neighbourhood of t such that aτ −t 6= 0, i.e., τ ≥ t. Because of the i.i.d.-ness of the errors, it is

likely that the extreme error ετ is isolated and outweights the other neighbouring εs’s contributing to Xt

in the sense that εs/ετ ≈ 0 for all s 6= τ such that as−t 6= 0, i.e., s ≥ t. Thus, we have the approximation

Xt+1
Xt

=

aτ −t−1ετ + P
s6=τ
aτ −tετ + P
s6=τ

as−t−1εs

as−tεs

=

aτ −t−1 + P
s6=τ
aτ −t + P
s6=τ

as−t−1εs/ετ

as−tεs/ετ

≈

aτ −t−1
aτ −t

.

In the case of the noncausal AR(1), ak = ρk1k≥0, and aτ −t−1/aτ −t = ρ−11τ ≥t+1 (recall that the random
date τ satisﬁes 1τ ≥t = 1), which recovers the result of Proposition 4.1: the conditional distribution of
Xt+1/Xt during extreme events concentrates on the points 0 (crash) and ρ−1 (growth), and the random

date τ has to be interpreted as the peak date of the bubble. Given the information at t, which is assumed

to contain at least the value of Xt, the conditional variance of Xt+1 can now be approximated as

V(Xt+1|It) ≈ V

(cid:18) aτ −t−1
aτ −t

Xt

(cid:19)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

It

= X 2
t

V

(cid:18) aτ −t−1
aτ −t

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:19)

.

It

This analysis shows that provided the distribution of τ given It is not degenerate (note that the existence
of a non-zero constant such that ak−1/ak = const for all k ≥ 0 is ruled out), then V

> 0

It

(cid:19)

(cid:18) aτ −t−1
aτ −t

(cid:12)
(cid:12)
(cid:12)
(cid:12)

and (Xt) features GARCH eﬀects during extreme events. Continuing with the example of the noncausal

AR(1), the above writes (we recognise the asymptotic variance in Section 3.3.3)

V(Xt+1|It) = X 2
t

V(ρ−11τ ≥t+1|It) = (ρ−1Xt)2 P(τ ≥ t + 1|It)P(τ < t + 1|It),

highlighting that the conditional variance of Xt+1 stems from the uncertainty in the occurrence date of

the peak given the available information. Note that this heuristics does not necessarily presume that

It contains only information about the past values of (Xt). The set It could contain information about

other variables or noisy proxies of τ (insider information for instance). Section 3.3.3 and Proposition 4.1

leads us to conclude that observing the inﬁnite past of (Xt) (recall that the noncausal AR(1) is Markov)

does not induce τ |It to be degenerate and GARCH eﬀects emerge. Only in case of perfect foresight of τ ,

i.e., τ |It = const, does the GARCH phenomenon seem to vanish.

24

5 Evaluating the odds of crashes of real series

In this section, we consider two series commonly studied in the speculative bubble literature and

in which evidence of explosive behaviour has been exhibited: the Nasdaq and S&P500 indexes (see

e.g. Gouriéroux and Zakoian (2017), Phillips et al. (2015), Phillips et al. (2011)). Figure 4 displays the

monthly series of the Nasdaq and S&P500 real prices from February 1971 to September 2019 (N = 584

observations each), obtained by inﬂation-adjusting the nominal series using the Consumer Price Index

provided by the Federal Bank of Saint Louis (fred.stlouisfed.org/series/CPIAUCSL).

Figure 4: Monthly Nasdaq and S&P500 real price indexes from 02/1971 to 09/2019 (N = 584 observations each), in US

dollars of 09/2019.

The two series feature almost uninterrupted growth episodes since beginning of 2009 up to 2019.

Gouriéroux and Zakoian (2017) found evidence that a stable noncausal AR(1) bubble dynamics is com-

patible with the real Nasdaq trajectory. Unreported results using standard model selection methods from

the noncausal literature (e.g., information criteria [Lanne and Saikkonen (2011), Hecq et al. (2017b)], co-

eﬃcient testing [Cavaliere et al. (2020)]) further conﬁrm the reasonableness of this speciﬁcation for both

series of interest here. Starting on the premise that the explosive episodes in the data can be modelled

as ongoing realisations of noncausal AR(1) bubbles climbing towards exogenous power-law-scaled peaks,

we will be in the position to provide estimates of ex ante crash odds of the recent growth trends based

on the results of Section 4.

In particular, we have shown in Section 4.1 that bubbles generated by heavy-tailed noncausal AR(1)

models feature geometric survival distributions with probability parameter ρα. It thus suﬃces to provide

values for the AR coeﬃcient ρ and the tail parameter α. We obtain estimates of these parameters by α-

stable Maximum Likelihood, which has been shown to yield consistent estimators [Andrews et al. (2009)].

25

We use the fast implementation of Royuela-del-Val et al. (2017) to evaluate α-stable densities, available

in the R package libstableR. Contrary to ˆα which is asymptotically normal, the estimator ˆρ of the

AR coeﬃcient unfortunately features an intractable asymptotic distribution. We resort to a parametric

bootstrap procedure to approximate the ﬁnite sample distribution of the estimators and compute conﬁ-

dence intervals. Table 3 reports the stable noncausal AR(1) ﬁts and the value of the −log-likelihood

at optima. We note that the estimate of ρ for the Nasdaq series is close to the one obtained by

Gouriéroux and Zakoian (2017). The values of the −log-likelihood for ﬁtted stable causal AR(1) speci-

ﬁcations, which estimate the AR coeﬃcient to be 1 for both series, is provided for comparison purposes

and conﬁrm that the stationary noncausal options are to be preferred.

Under the stable noncausal AR(1) speciﬁcation, the survival distributions of the recent explosive

growth episodes can be completely estimated and characterised by plugging-in the obtained estimates ˆρ

and ˆα in the statistics of Table 1. For instance, an estimate of the crash probability at horizon h can be

computed as 1 − ˆρ ˆαh, while the q-life, that is, the duration hq such that the probability of an explosive

episode lasting as long as hq is equal to 1 − q, can be computed as ln(1 − q)/ˆα ln ˆρ. Table 4 displays

a summary of bubble survival statistics for both series.

In the case of the Nasdaq and S&P500, our

estimates indicate that bubbles generated by the corresponding stable noncausal AR(1) processes should

have q = 5% chance of lasting 8.3 and 10.6 years respectively, and q = 1% chance of lasting 12.7 and

16.3 years respectively. The observed durations of the growth episodes from 2009 to 2019 are therefore

not abnormally long in that respect and appear very well compatible with the implied model properties.

Irrespective of their past durations, as bubbles generated by such processes feature a memory-less prop-

erty, this analysis suggests relatively important crash probabilities within one year (h = 12) between 27.9

and 33.4% for the Nasdaq series, and between 18.6 and 34.8% for the S&P500. Note that these estimates

are robust to any behaviour the collapse after the peak may actually feature: as shown in Proposition

4.2, the shape of the collapse has no role in the ex ante probability of reaching the peak of a noncausal

AR(1)-type growth episode. Last, similar values of the survival statistics are obtained if instead of the

α-stable assumption one opts for t-student or skewed-t distributions, or if one proceeds to estimate ρ by

ordinary least squares and α by applying the Hill estimator to the residuals. The results of the latter

robustness checks are available in Section A.3 of the Supplementary File.

26

Stable Noncausal AR(1)

Stable Causal AR(1)

Nasdaq

α

1.01

ρ

−log-L

−log-L

0.971

3608.640

3642.348

(0.925 , 1.11)

(0.969 , 0.972)

S&P500

1.36

0.983

3067.574

3086.641

(1.25 , 1.48)

(0.975 , 0.987)

Table 3: Maximum likelihood estimates of the tail index α and the AR coeﬃcient ρ of stable noncausal AR(1) speciﬁcations

ﬁtted on the real Nasdaq and S&P500 series. The penultimate column reports the value of the −Log-likelihood (−log-L)

at optima of the stable noncausal AR(1) ﬁts (a lower value indicates a better ﬁt). The last column reports the value of the

−Log-likelihood at optima of stable causal AR(1) ﬁts as benchmark. Finite sample 95% conﬁdence intervals are reported in

parentheses below the point estimates and have been calculated by parametric bootstrap of the estimated stable noncausal

AR(1) processes with 2000 simulated trajectories each of 584 observations.

27

0
0
5
P
&
S

q
a
d
s
a
N

3
.
6
1

6
.
0
1

4
.
2

6
.
3

7
.
2
1

3
.
8

9
.
1

8
.
2

)
4
.
2
2

,

7
.
0
1
(

)
5
.
5
1

,

0
.
7
(

)
4
.
3

,

6
.
1
(

)
9
.
4

,

4
.
2
(

)
1
.
4
1

,

3
.
1
1
(

)
2
.
9

,

4
.
7
(

)
1
.
2

,

7
.
1
(

)
1
.
3

,

5
.
2
(

)
Y
(

e
f
i
l
–
%
9
9

)
Y
(

e
f
i
l
–
%
5
9

)
Y
(

e
f
i
l
-
f
l
a
H

)
Y
(

e
f
i
l

d
e
t
c
e
p
x
E

)
Y
(

e
f
i
l
–
%
9
9

)
Y
(

e
f
i
l
–
%
5
9

)
Y
(

e
f
i
l
-
f
l
a
H

)
Y
(

e
f
i
l

d
e
t
c
e
p
x
E

d
e
t
t
ﬁ

s
n
o
i
t
a
c
ﬁ
i
c
e
p
s

)
1
(
R
A

l
a
s
u
a
c
n
o
n

e
l
b
a
t
s

f
o

ρ

d
n
a
α

f
o

s
e
t
a
m

i
t
s
e

e
h
t

n
o

d
e
s
a
b

,
1

e
l
b
a
T
m
o
r
f

e
a
l
u
m
r
o
f

e
h
t

g
n
i
s
u

s
c
i
t
s
i
t
a
t
s

l
a
v
i
v
r
u
s

e
l
b
b
u
b

f
o

y
r
a
m
m
u
S

:
4

e
l

b
a
T

n
i

d
e
t
r
o
p
e
r

e
r
a

s
l
a
v
r
e
t
n
i

e
c
n
e
d
ﬁ
n
o
c
%
5
9

e
l
p
m
a
s

e
t
i
n
i
F

.
)
s
e
t
a
m

i
t
s
e

r
e
t
e
m
a
r
a
p

e
h
t

r
o
f

3

e
l
b
a
T
e
e
s
(

s
e
i
r
e
s

)
l
e
n
a
p

t
h
g
i
r
(

0
0
5
P
&
S

d
n
a

)
l
e
n
a
p

t
f
e
l
(

q
a
d
s
a
N

l
a
e
r

e
h
t

n
o

d
e
t
a
l
u
m

i
s

0
0
0
2

h
t
i
w
s
e
s
s
e
c
o
r
p

)
1
(
R
A

l
a
s
u
a
c
n
o
n

e
l
b
a
t
s

d
e
t
a
m

i
t
s
e

e
h
t

f
o

p
a
r
t
s
t
o
o
b

c
i
r
t
e
m
a
r
a
p

y
b

d
e
t
a
l
u
c
l
a
c

n
e
e
b

e
v
a
h

d
n
a

s
e
t
a
m

i
t
s
e

t
n
i
o
p

e
h
t
w
o
l
e
b

s
e
s
e
h
t
n
e
r
a
p

.
)

%

(

s
t
n
e
c
r
e
p

n
i

d
e
t
r
o
p
e
r

e
r
a

s
e
i
t
i
l
i
b
a
b
o
r
p

h
s
a
r
c

d
n
a

)
Y
(

s
r
a
e
y

n
i

d
e
t
r
o
p
e
r

e
r
a

e
f
i
l
-

%
9
9

,
e
f
i
l
-

%
5
9

,
e
f
i
l
-
f
l
a
h

,
e
f
i
l

d
e
t
c
e
p
x
E

.
s
n
o
i
t
a
v
r
e
s
b
o

4
8
5

f
o

h
c
a
e

s
e
i
r
o
t
c
e
j
a
r
t

)
8
.
4
3

,

6
.
8
1
(

)
3
.
9
1

,

8
.
9
(

)
2
.
0
1

,

0
.
5
(

)
5
.
3

,

7
.
1
(

)
5
3
0
.
0

,

7
1
0
.
0
(

)
4
.
3
3

,

9
.
7
2
(

)
4
.
8
1

,

1
.
5
1
(

)
7
.
9

,

9
.
7
(

)
3
.
3

,

7
.
2
(

)
3
3
0
.
0

,

7
2
0
.
0
(

)

%

(

s
h
t
n
o
m
h

n
i
h
t
i
w
h
s
a
r
c

f
o

y
t
i
l
i
b
a
b
o
r
P

)

%

(

s
h
t
n
o
m
h

n
i
h
t
i
w
h
s
a
r
c

f
o

y
t
i
l
i
b
a
b
o
r
P

2
1

7
.
4
2

6

2
.
3
1

3

8
.
6

3
.
2

3
2
0
.
0

1
=
h

e
t
a
r

d
r
a
z
a
H

2
1

3
.
0
3

6

6
.
6
1

3

7
.
8

0
.
3

0
3
0
.
0

1
=
h

e
t
a
r

d
r
a
z
a
H

28

6 Concluding remarks

By embedding α-stable two-sided MA(∞) processes into the framework of bivariate α-stable random

vectors, we described in detail the conditional dependence of Xt+h on Xt. We have shown that non-

causality plays a crucial role in the existence of conditional moments, and provided expressions for the

latter up to the fourth order, when they exist, as well as their asymptotic behaviours when the condition-

ing variable takes extreme values. We have detailed practical implementation aspects of the conditional

moments as well as the contribution of the results to current methodological practices of the empirical

noncausal literature. A future empirical investigation could determine whether corresponding patterns in

the conditional moments of real data can be identiﬁed. These results could serve as a basis to formulate

a higher-order moments portfolio allocation problem (e.g., following Jondeau and Rockinger (2006,2012))

where bubble-timing investors optimise over quantities of speculative and safer assets as well as over the

holding time through a bubble. Some limitations of the provided conditional moments formulae could

be addressed in further research. This includes expanding the conditioning to a set of past values or the

entire past, as opposed to conditioning only by the present level of the trajectory. Also, even though

Lemma 3.1 allows to extend the formulae of Proposition 3.1 to the conditional moments of, say, X2,t+h

given the present level of another process X1,t, obtaining a characterisation of the moments in the gen-

eral multivariate case remains an open issue. Furthermore, despite noncausal processes admitting more

conditional moments, higher-order conditional moments may nevertheless not exist for smaller values of

α -for instance, the conditional skewness and kurtosis when α = 1. Alternative dependence measures

capturing, say, conditional asymmetry and heavy-tailedness in such cases could be investigated.

Focusing on explosive bubble episodes generated by heavy-tailed noncausal MA(∞) processes, we

provided closed-form asymptotic formulae for the predictive distribution, which enjoy very intuitive pat-

terns and probability tree interpretations. This surprisingly revealed that the noncausal AR(1) bubbles

are memory-less with a dynamics à la Blanchard and Watson (1982). The survival distribution of such

bubbles is geometric and can be fully characterised by the given of the AR coeﬃcient ρ and the tail

exponent α, both of which can be estimated by classical methods from the data. Even more surprising is

the fact that the augmentation of a noncausal AR(1) bubble by an arbitrarily-shaped collapse after the

peak does not alter the survival distribution of the exponential growth phase of the bubble. From the

point of view of a forecaster observing that the past trajectory is approximately exponentially-shaped,

the likelihood of the peak being reached at some future horizon has the same simple expression in terms

of ρ and α whatever is bound to happen after the peak. Of course, the speed of the collapse still impacts

how much is at risk in case of downturn. Interestingly, bubbles generated by mixed causal-noncausal pro-

29

cesses, and those of a MAR(1,1) in particular, feature an extended Blanchard and Watson dynamics with

gradual collapse which appears able to reconcile rational expectation bubble models with tail exponents

greater than 1, a well-documented statistical property of ﬁnancial time series [Lux and Sornette (2002)].

We further demonstrated how the closed-form formulae of the predictive distribution and of crash odds

can be applied on growth episodes of real data. Statistical methods for agnostically estimating the coeﬃ-

cients (ak) of the MA representation, e.g., under low dimensional restrictions, and for robustly estimating

the tail index α in locally explosive events could enable more reﬁned evaluation of the crash odds.

Acknowledgments

The author is extraordinarily indebted to Jean-Michel Zakoïan, and further thanks Denisa-Georgiana

Banulescu, Jean-Marc Bardet, Frédérique Bec, Francisco Blasques, Ophélie Couperier, Gilles De Truchis,

Elena Dumitrescu, Christian Francq, Christian Gouriéroux, Alain Hecq, Jérémy Leymarie, Yang Lu,

Andre Lucas, Anders Rahbek, Li Sun, Sean Telg, Arthur Thomas and Elisa Voisin for insightful discus-

sions. The author gratefully acknowledges the support of the Groupe des Écoles Nationales d’Économie

et Statistique (GENES), the Agence Nationale de la Recherche (via the Project MultiRisk ANR CE26

2016 - CR), and the support of the European Commission (via the Project NONCAUSALBubble H2020-

MSCA-IF-2019 896504).

References

Andrews, B., Calder, M., and R., Davis. 2009. Maximum likelihood estimation for α-stable autoregressive process. Annals

of Statistics, 37, 1946-1982.

Bec, F., Nielsen, H. B., and S., Saïdi. 2020. Mixed causal-noncausal autoregressions: bimodality issues in estimation and

unit root testing. Forthcoming in Oxford Bulletin of Economics and Statistics.

Blanchard, O. and M., Watson. 1982. Bubbles, rational expectations, and ﬁnancial markets. National Bureau of Economic

Research, No. 0945.

Blasques, F., Koopman, S. J., and M., Nientker. 2018. A time-varying parameter model for local explosions. Tinbergen

Institute Discussion Paper, No. TI 2018-088/III.

Cambanis, S., and G., Miller. 1981. Linear problems in pth order and stable processes. SIAM Journal on Applied Mathe-

matics, 41, 43–69.

Cavaliere, G., Nielsen, H.B, and A. Rahbek. 2020. Bootstrapping non-causal autoregressions: with applications to explosive

bubble modelling. Journal of Business and Economic Statistics, 38, 55-67.

Chen, B., Choi, J., and J. C., Escanciano. 2017. Testing for fundamental vector moving average representations. Quantitative

Economics, 8, 149-180.

Cioczek-Georges, R., and M. S., Taqqu. 1995a. Form of the conditional variance for stable random variables. Statistica

Sinica, 351-361.

Cioczek-Georges, R., and M. S., Taqqu. 1995b. Necessary conditions for the existence of conditional moments of stable

random variables. Stochastic Processes and their Applications, 56, 233-246.

Cioczek-Georges, R., and M. S., Taqqu. 1998. Suﬃcient conditions for the existence of conditional moments of stable

random variables. Stochastic Processes and Related Topics, Birkhäuser Boston, 35-67.

Conway, J. B. 1978. Functions of one complex variable. Springer-Verlag, New York.

30

Fries, S. 2018. Path prediction of aggregated α-stable moving averages using semi-norm representations. ArXiv preprint

arXiv:1809.03631.

Fries, S., and J.-M., Zakoian. 2019. Mixed causal-noncausal AR processes and the modelling of explosive bubbles. Econom-

etric Theory, 35, 1234-1270.

Gouriéroux, C., Hencic, A., and J., Jasiak. 2019. Forecast performance and bubble analysis in noncausal MAR(1,1) pro-

cesses. Toulouse School of Economics. 10.13140/RG.2.2.25135.38568.

Gouriéroux, C., and J., Jasiak. 2016. Filtering, prediction and simulation methods for noncausal processes. Journal of

Time Series Analysis, 37, 405-430.

Gouriéroux, C., and J., Jasiak. 2018. Misspeciﬁcation of noncausal order in autoregressive processes. Journal of Economet-

rics, 205, 226-248.

Gouriéroux, C., Jasiak, J. and A., Monfort. 2020. Stationary bubble equilibria in rational expectation models. Forthcoming

in Journal of Econometrics.

Gouriéroux, C. and J.-M., Zakoian. 2017. Local explosion modelling by non-causal process. Journal of the Royal Statistical

Society: Series B. 79, 737-756.

Hansen, B. E. 2008. Uniform convergence rates for kernel estimation with dependent data. Econometric Theory, 726-748.

Hardin Jr, C. D., Samorodnitsky, G., and M. S., Taqqu. 1991. Nonlinear regression of stable random variables. The Annals

of Applied Probability, 582-612.

Hecq, A., and L., Sun. 2019.

Identiﬁcation of noncausal models by quantile autoregressions. arXiv preprint

arXiv:1904.05952.

Hecq, A., Lieb, L., and S. M., Telg. 2016. Identiﬁcation of mixed causal-noncausal models in ﬁnite samples. Annals of

Economics and Statistics, 123/124, 307-331.

Hecq, A., Telg, S., and L., Lieb. 2017. Do seasonal adjustments induce noncausal dynamics in inﬂation rates? Econometrics,

5, 48.

Hecq, A., Telg, S., and L, Lieb. 2017. Simulation, estimation and selection of mixed causal-noncausal autoregressive models:

the MARX package. Available at SSRN: https://ssrn.com/abstract=3015797.

Hecq, A., and E., Voisin. 2020. Forecasting bubbles with mixed causal-noncausal autoregressive models. Forthcoming in

Econometrics and Statistics.

Hencic, A., and C., Gouriéroux. 2015. Noncausal autoregressive model in application to Bitcoin/USD exchange rates.

Econometrics of Risk, Springer International Publishing, 17-40.

Jondeau, E., and M., Rockinger. 2006. Optimal portfolio allocation under higher moments. European Financial Manage-

ment, 12, 29-55.

Jondeau, E., and M., Rockinger. 2012. On the importance of time variability in higher moments for asset allocation. Journal

of Financial Econometrics, 10, 84-123.

Karcher, W., Shmileva, E., and E., Spodarev. 2013. Extrapolation of stable random ﬁelds. Journal of Multivariate Analysis,

115, 516-536.

Lanne, M., Luoto, J., and P., Saikkonen. 2012. Optimal forecasting of noncausal autoregressive time series. International

Journal of Forecasting, 28, 623-631.

Lanne, M., Nyberg, H., and E., Saarinen. 2012. Does noncausality help in forecasting economic time series? Economics

Bulletin, 32, 2849-2859.

Lanne, M., and P., Saikkonen. 2011. Noncausal autogressions for economic time series. Journal of Time Series Econometrics,

3.

Lanne, M., and P., Saikkonen. 2013. Noncausal vector autoregression. Econometric Theory, 29, 447-481.

Ling, S. 2007. A double AR (p) model: structure and estimation. Statistica Sinica, 17, 161-175.

Lux, T., and D., Sornette. 2002. On rational bubbles and fat tails. Journal of Money, Credit and Banking, 589-610.

Matsushima, H. 2003. Behavioral aspects of arbitrageurs in timing games of bubbles and crashes. Journal of Economic

Theory, 148, 858-870.

Miller, G. 1978. Properties of certain symmetric stable distributions. Journal of Multivariate Analysis, 8, 346–360.

Phillips, P. C., and S. P., Shi. 2018. Financial bubble implosion and reverse regression. Econometric Theory, 34, 705-753.

Phillips, P. C., Shi, S., and J., Yu. 2015. Testing for multiple bubbles: historical episodes of exuberance and collapse in

the S&P 500. International Economic Review, 56, 1043-1078.

31

Phillips, P. C., Wu, Y., and J., Yu. 2011. Explosive behavior in the 1990s Nasdaq: when did exuberance escalate asset

values? International Economic Review, 52, 201-226.

Royuela-del-Val J., Simmross-Wattenberg F., and C. Alberola López. 2017. libstable:

fast, parallel and high-precision

computation of alpha-stable distributions in R, C/C++ and MATLAB. Journal of Statistical Software, 78, 1-25.

Samorodnitsky, G., and M. S., Taqqu. 1994. Stable non-Gaussian random processes, Chapman & Hall, London, 516-536.

West, K. D. 1987. A speciﬁcation test for speculative bubbles. The Quarterly Journal of Economics, 102, 553-580.

32

Supplementary Materials
[For Online Publication only]

Conditional Moments of Noncausal Alpha-Stable Processes and the Prediction of Bubble Crash Odds

S. Fries

Contents

A Complementary result

A.1 Existence of moments and superexponential decay of (ak): a boundary case . . . . . . . .

A.2 Moments of MARMA processes : Complementary simulations and illustrations . . . . . .

A.2.1 Plug-in estimation of the conditional moments

. . . . . . . . . . . . . . . . . . . .

A.2.2 Illustrating the eﬀects of parameters on the conditional moments . . . . . . . . . .

2

2

4

4

8

A.3 Real series application: robustness checks of the crash odds estimation . . . . . . . . . . .

15

B Preliminary elements for the proof of the main results

B.1 Notations for the proofs of Theorem 2.2 and Proposition 2.1 . . . . . . . . . . . . . . . . .

B.2 Lemma B.1 for the proof of Theorem 2.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . .

C Proof of Lemma B.1

C.1 Justifying inversion of integral and derivation signs . . . . . . . . . . . . . . . . . . . . . .

C.1.1 Justifying inversion: First derivative . . . . . . . . . . . . . . . . . . . . . . . . . .

C.1.2 Justifying inversion: Second derivative . . . . . . . . . . . . . . . . . . . . . . . . .

C.1.3 Justifying inversion: Third derivative . . . . . . . . . . . . . . . . . . . . . . . . . .

C.1.4 A special manipulation to obtain the fourth derivative . . . . . . . . . . . . . . . .

C.1.5 Justifying inversion: Fourth derivative . . . . . . . . . . . . . . . . . . . . . . . . .

C.1.6 Lemmas for justifying the inversions in the proof of Lemma B.1 . . . . . . . . . . .

C.2 Computation of the derivatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

D Proof of Theorem 2.2

D.1 Proof of second order conditional moment (2.6) in Theorem 2.2 . . . . . . . . . . . . . . .

D.2 Proof of third order conditional moment (2.7) in Theorem 2.2 . . . . . . . . . . . . . . . .

D.3 Proof of fourth order conditional moment (2.8) in Theorem 2.2 . . . . . . . . . . . . . . .

D.4 Lemmas for the proof of Theorem 2.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

19

19

20

21

21

21

24

29

32

33

36

37

38

38

39

41

43

1

E Proof of Theorem 2.4

E.1 Justifying inversion of integral and derivative signs . . . . . . . . . . . . . . . . . . . . . .

E.2 Evaluating at r = 0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

E.3 Lemmas for the proof of Theorem 2.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

F Proof of Proposition 2.1

F.1 Case α 6= 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

F.2 Case α = 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

G Proof of Lemma 3.1

H Proof of the asymptotic moments in Section 3.3.3

I Proof of Proposition 4.1

J Proof of Proposition 4.2

K Proof of Proposition 4.3

A Complementary result

44

45

53

54

55

55

56

57

57

57

58

61

A.1 Existence of moments and superexponential decay of (ak): a boundary case

As pointed after Proposition 3.1, noncausal ARMA and fractionally integrated processes whose MA

coeﬃcients decay at geometric and hyperbolic speed satisfy condition (3.5) for all ν > 0 (provided

there are no index k such that ak−h 6= 0 and ak = 0). Such processes hence admit ﬁnite conditional

moments at least up to order 2α + 1. Theorem 5.1.3 by Samorodnitsky and Taqqu, Theorems 1.1, 1.2 in

Cioczek-Georges and Taqqu (1995b) however point to the fact that intermediate cases may arise where

moments are ﬁnite at most up to order α + ν for some value of ν such that α < α + ν < 2α + 1. We

propose here a noncausal MA(∞) process with super-exponentially decaying MA coeﬃcients which can

reach any intermediate value of the boundary. Consider the noncausal process deﬁned for all t ∈ Z by
Xt = P+∞

k=0 akεt+k with ak = exp{1 − eak}, a > 0, for all k ≥ 0, and let (εt) be an i.i.d. symmetrically
distributed α-stable error sequence. Letting ν ≥ 0, the general term of the series in (3.5) reads for all

k ≥ h

(a2

k + a2

k−h)

α+ν

2 |ak|−ν = (cid:0)1 + (ak−h/ak)2(cid:1) α+ν

2 |ak|α

2

(cid:16)

=

1 + exp{2eak(1 − e−ah)}

(cid:17) α+ν

2 exp{−α(1 − eak)}

n
eak(cid:2)(1 − e−ah)(α + ν) − α(cid:3) + α

o
,

exp

∼
k→+∞

which is the term of an absolutely convergent series if and only if (1 − e−ah)(α + ν) − α < 0, hence if and

only if

ν < α

(cid:18)

1

(cid:19)
1 − e−ah − 1

.

(A.1)

Because we

assume

(εt)

to

be

symmetrically

distributed, Theorems

1.1

and

1.2

in

Cioczek-Georges and Taqqu (1995b) allow to consider (3.5) and (A.1) as suﬃcient and necessary

conditions for the ﬁniteness of E[|Xt+h|γ|Xt], 0 ≤ γ < min(α + ν, 2α + 1), in most conﬁgurations of

α and ν (see within Cioczek-Georges and Taqqu (1995b) for details).

In particular, one can see that

for a ﬁxed prediction horizon h ≥ 1, the upper bound (A.1) on ν can lie anywhere between 0 and +∞

according to the parameter a. The smaller a > 0, i.e., the slower the decay, the higher the bound on

ν, and conversely, the greater a (faster decay), the smaller the upper bound on ν for the existence of

conditional moments.

Furthermore, contrary to the case where (ak) decays at geometric or hyperbolic speeds, the ﬁniteness
of E[|Xt+h|γ|Xt] also depends on the prediction horizon h. Most notably, for any ﬁxed decay speed a,

on can see that the bound (A.1) tends to 0 as h → +∞. For a decay parameter a small enough, the

moments E[|Xt+h|γ|Xt] may thus be ﬁnite up to order 2α + 1 for short-term prediction horizons while

being ﬁnite only up to order α for longer-term prediction horizons.

3

A.2 Moments of MARMA processes : Complementary simulations and illustrations

A.2.1 Plug-in estimation of the conditional moments

The simulation experiment of Section 3.3.1 and Figure 2 illustrated the validity of the formulae of Proposi-

tion 3.1 by showing their match with model-free, data-driven non-parametric estimates of the conditional

moments based on simulated trajectories. To compute the conditional moments in practice, one can now

overlook the model-free nonparametric approach and resort to a parametric plug-in estimation approach

based on the formulae of Proposition 3.1 as follows:

1. Estimate the parameters of the stable MARMA process, for instance by α-stable Maximum Like-

lihood [Andrews et al. (2009)] or M-estimation [Wu (2013)].

2. Plug the parameter estimates in the formulae of Proposition 3.1 and compute the conditional

moments.

Provided the estimators of step 1 are consistent, as is the case for the two mentioned above, the plug-in

estimators of the conditional moments will also be consistent. We provide here the methodology and

results of an additional simulation experiment designed to illustrate and gauge the parametric plug-in

estimation of the conditional moments. Consider again that the price (Xt) of an asset is modelled by a

MARMA process, say the noncausal-noninvertible solution of

Xt = ψ0Xt+1 + εt + θ0εt+1,

εt

i.i.d.∼ S(α0, β0, σ0, µ0),

(A.2)

where ϑ0 := (ψ0, θ0, α0, β0, σ0, µ0) = (0.9, 0.7, 1.8, 0.5, 0.1, 2) is the vector of true parameter values. We
simulate M = 2000 trajectories x(m)

N , m = 1, . . . , M from the above process for sample sizes

, . . . , x(m)

1

N = 1000, 2000 and 5000 and estimate all the parameters by maximum likelihood as follows.

For any candidate vector of parameters ϑ = (ψ, θ, α, β, σ, µ), we follow Wu (2013) to compute the residuals

and evaluate the likelihood. To ﬁx ideas, let us focus on simulation m. For any given candidate vector
ϑ, we compute the residuals z(m)

N (ϑ) as: ﬁrst compute v(m)

N (ϑ) according to

(ϑ), . . . , v(m)

(ϑ), . . . , z(m)

1

1

v(m)
t

(ϑ) = x(m)

t − ψx(m)
t+1,

for t = 1, . . . , N . Compute then the residuals backwards as

t−1 (ϑ) = θz(m)
z(m)

t

(ϑ) − ψv(m)

t

(ϑ),

t = N, N − 1, . . . , 2. In both steps, one can set initial (terminal) conditions, e.g., z(m)
and burn residuals close to the boundary. Based on residuals z(m)

(ϑ), . . . , z(m)

N (ϑ) = x(m)
N +1 := 0,
N (ϑ) (possibly accounting

1

4

for some burn), we can then compute the −log-likelihood

(cid:16)

L

ϑ = (ψ, θ, α, β, σ, µ); x(m)

1

, . . . , x(m)

N

(cid:17)

:= −

ln fα,β,σ,µ

(cid:16)

z(m)
t

(ϑ)

(cid:17)

,

N
X

t=1

where fα,β,σ,µ denotes the density of the stable distribution with corresponding parameters. For each

simulated trajectory m = 1, . . . , M and each sample size N = 1000, 2000, 5000, we compute the residuals
(m,N )
and the likelihood by following the steps above, and we numerically ﬁnd the vector of parameters ˆϑ
ML

minimising the −log-likelihood:

ˆϑ

(m,N )
ML

:= arg min

ϑ∈R6

(cid:16)
ϑ; x(m)
1

L

, . . . , x(m)

N

(cid:17)
.

(1,N )
ML , . . . , ˆϑ
The obtained estimators ˆϑ
compute the plug-in estimators ˆµ(m,N )

(M,N )
ML
(x, h), ˆσ(m,N )

2,ML (x, h), m = 1, . . . , M ,
N = 1000, 2000, 5000, of the conditional expectation, standard deviation, skewness and excess kurtosis

(x, h), ˆγ(m,N )

1,ML (x, h), ˆγ(m,N )

ML

ML

are then plugged in the formulae of Proposition 3.1 to

given in (3.8). Figure 5 represents the pointwise 0.05-0.95 interquantile intervals of the plug-in conditional

moments estimators across the M = 2000 simulations, alongside the conditional moments computed us-

ing the true values of the parameters ϑ0. Three interquantile intervals appear on Figure 5, one for each

sample size N = 1000, 2000, 5000. It can be noticed that even for the smallest sample size, the interquan-

tile interval is extremely narrow around most of the true conditional moments curves. For higher-order

moments and at furthest horizons, the interquantile intervals are slightly larger for N = 1000 but narrow

down fast as the sample size increases. For comparison purposes, the model-free non-parametric esti-

mators of the conditional moments for model (A.2) have been computed following the same procedure

as in Section 3.3.1 and Figure 2, using the Nadaraya-Watson estimator. The non-parametric estimators

have been computed based on M = 2000 simulated trajectories of N = 107 observations. Figure 6 repre-

sents the 0.05-0.95 interquantile intervals of the non-parametric estimator alongside the true conditional

moments. When comparing Figures 5 and 6, one can notice the dramatic eﬃciency gain from using

the parametric plug-in estimators compared to the model-free non-parametric approach. With sample

sizes four orders of magnitudes smaller, the parametric plug-in approach achieves a comparable or better

accuracy.

Importantly, the plug-in approach is able to extrapolate well the conditional moments for

conditioning values Xt = x far away from the central values of the process (Xt), while the error of the

model-free non-parametric approach explodes in these regions because of the scarcity of extreme-valued

data points.

5

Figure 5: Conditional expectation, standard deviation, skewness and excess kurtosis (in rows) of the

returns Rt+h = (Xt+h − Xt)/Xt at horizons h = 1, 3, 5, 10 (in columns) of the MARMA process Xt =
i.i.d.∼ S(1.8, 0.5, 0.1, 2), for conditioning values Xt = x ∈ (29, 43) (x-axis of each

0.9Xt+1 + εt + 0.7εt+1, εt

plot). Black solid lines: conditional moments (3.8) given by Proposition 3.1 computed using the true

parameter values. Grey shaded areas: 0.05-0.95 interquantile intervals across M = 2000 simulations of

the conditional moments (3.8) obtained by estimating the parameters of (Xt) by maximum likehood and

plugging-in the estimates into the formulae of Proposition 3.1. Estimation performed with sample sizes

N = 1000 (light grey), N = 2000 (middle grey), N = 5000 (dark grey).

6

Figure 6: Conditional expectation, standard deviation, skewness and excess kurtosis (in rows) of the

returns Rt+h = (Xt+h − Xt)/Xt at horizons h = 1, 3, 5, 10 (in columns) of the MARMA process Xt =
i.i.d.∼ S(1.8, 0.5, 0.1, 2), for conditioning values Xt = x ∈ (29, 43) (x-axis of each

0.9Xt+1 + εt + 0.7εt+1, εt

plot). Black solid lines: theoretical moments (3.8) given by Proposition 3.1. Grey dotted lines: average

of Nadaraya-Watson estimators (bandwidth=0.1) across 2000 simulated trajectories of 107 observations

each. Grey shaded areas: 0.05-0.95 interquantile interval across simulations.

7

A.2.2

Illustrating the eﬀects of parameters on the conditional moments

We provide here ﬁgures illustrating how the shape of the conditional moments of MARMA processes may

be aﬀected as we let parameter values vary. We introduce some notations and shorthands: in the rest of

the section, we will denote processes solution of

(1 − ψF )(1 − φB)Xt = (1 + θF )(1 + ηB)εt,

as MARMA(1, 1, 1, 1), and processes solution of

(1 − ψ1F )(1 − ψ2F )Xt = (1 + ηB)εt,

as MARMA(2, 0, 0, 1). For the errors, we assume as in the previous sections that εt

i.i.d.∼ S(α, β, σ, µ).

Figures 7-12 illustrate the eﬀects of the diﬀerent parameters on the shape of the conditional moments for

the two types of MARMA processes above.

8

Figure 7: Conditional moments of a stable MARMA(1,1,1,1) for diﬀerent values of α.

Conditional expectation, standard deviation, skewness and excess kurtosis (in rows) of Xt+h given Xt = x,

for horizons h = 1, 2, 3, 4, 5 (in columns) and conditioning values Xt = x ∈ (−5, 5) (x-axis of each

plot), computed using the formulae of Proposition 3.1, where (Xt) is the strictly stationary solution of

(1 − 0.8F )(1 + 0.3B)Xt = (1 + 0.4F )(1 − 0.3B)εt, εt

i.i.d.∼ S(α, 0.5, 0.1, 0), α ∈ {1.99, 1.9, 1.8, 1.7, 1.6}.

9

Figure 8: Conditional moments of a stable MARMA(1,1,1,1) for diﬀerent values of β.

Conditional expectation, standard deviation, skewness and excess kurtosis (in rows) of Xt+h given Xt = x,

for horizons h = 1, 2, 3, 4, 5 (in columns) and conditioning values Xt = x ∈ (−5, 5) (x-axis of each

plot), computed using the formulae of Proposition 3.1, where (Xt) is the strictly stationary solution of

(1 − 0.8F )(1 + 0.3B)Xt = (1 + 0.4F )(1 − 0.3B)εt, εt

i.i.d.∼ S(1.7, β, 0.1, 0), β ∈ {0.8, 0.4, 0, −0.2, −0.6}.

10

Figure 9: Conditional moments of a stable MARMA(1,1,1,1) for diﬀerent values of ψ.

Conditional expectation, standard deviation, skewness and excess kurtosis (in rows) of Xt+h given Xt = x,

for horizons h = 1, 2, 3, 4, 5 (in columns) and conditioning values Xt = x ∈ (−5, 5) (x-axis of each

plot), computed using the formulae of Proposition 3.1, where (Xt) is the strictly stationary solution of
i.i.d.∼ S(1.7, 0.5, 0.1, 0), ψ ∈ {0.95, 0.9, 0.85, 0.8, 0.75}.

(1 − ψF )(1 + 0.3B)Xt = (1 + 0.4F )(1 − 0.3B)εt, εt

11

Figure 10: Conditional moments of a stable MARMA(1,1,1,1) for diﬀerent values of φ.

Conditional expectation, standard deviation, skewness and excess kurtosis (in rows) of Xt+h given Xt = x,

for horizons h = 1, 2, 3, 4, 5 (in columns) and conditioning values Xt = x ∈ (−5, 5) (x-axis of each

plot), computed using the formulae of Proposition 3.1, where (Xt) is the strictly stationary solution of

(1 − 0.8F )(1 − φB)Xt = (1 + 0.4F )(1 − 0.3B)εt, εt

i.i.d.∼ S(1.7, 0.5, 0.1, 0), φ ∈ {0.7, 0.4, 0, −0.4, −0.7}.

12

Figure 11: Conditional moments of a stable MAR(2,0,0,1) for diﬀerent values of ψ2

Conditional expectation, standard deviation, skewness and excess kurtosis (in rows) of Xt+h given Xt = x,

for horizons h = 1, 2, 3, 4, 5 (in columns) and conditioning values Xt = x ∈ (−5, 5) (x-axis of each

plot), computed using the formulae of Proposition 3.1, where (Xt) is the strictly stationary solution of

(1 − ψ1F )(1 − ψ2F )Xt = (1 + 0.9B)εt, εt

i.i.d.∼ S(1.7, 0.5, 0.1, 0), ψ1 = 0.8, ψ2 ∈ {0.6, 0.3, 0, −0.3, −0.6}.

13

Figure 12: Conditional moments of a stable MAR(2,0,0,1) for diﬀerent values of η.

Conditional expectation, standard deviation, skewness and excess kurtosis (in rows) of Xt+h given Xt = x,

for horizons h = 1, 2, 3, 4, 5 (in columns) and conditioning values Xt = x ∈ (−5, 5) (x-axis of each

plot), computed using the formulae of Proposition 3.1, where (Xt) is the strictly stationary solution of

(1 − 0.4F )(1 − 0.8F )Xt = (1 + ηB)εt, εt

i.i.d.∼ S(1.7, 0.5, 0.1, 0), η ∈ {0.8, 0.4, −0.2, −0.4, −0.6}.

14

A.3 Real series application: robustness checks of the crash odds estimation

The crash odds estimates of Section 5 are obtained by maximum likelihood assuming α-stable distributed

errors. We here propose some additional empirical results assessing the robustness of these estimates to

alternative assumptions on the errors and diﬀerent ﬁtting methodologies. For the Nasdaq and S&P500

series as in Section 5, we ﬁt noncausal AR(1) models by:

1. t-student maximum likelihood using the function marx implemented in the R package MARX

[Hecq et al. (2017b)]. This approach assumes t-distributed errors instead of α-stable.

2. skewed-t regression using the function selm implemented in the R package sn [Azzalini (2018)].

This approach assumes skewed-t-distributed errors instead of α-stable.

3. OLS estimation of the AR coeﬃcient ρ, followed by Hill estimation of α on the residuals of the OLS

step, using the function hillplot implemented in the R package evmix [Hu and Scarrott (2018)].

This approach does not make any fully parametric assumption on the errors, but only assumes they

are heavy-tailed in the sense of Equation (4.1).

Note that in the three approaches above, the errors are power-law tailed and the results of Section 4

apply. Table 5 gathers the estimates of ρ and α, while Tables 6-7 display the survival statistics of bubbles

generated by the corresponding heavy-tailed noncausal AR(1) for the Nasdaq and S&P500. One can

notice that the survival statistics show similar values to those obtained in Table 4.

15

t-student

Nasdaq

skewed-t

OLS+Hill

t-student

S&P500

skewed-t

OLS+Hill

Noncausal AR(1)

α

1.22

1.18

1.80

2.02

2.12

2.50

ρ

0.979

0.972

0.988

0.987

0.983

0.992

Table 5: Estimates of the tail index α and the AR coeﬃcient ρ of heavy-tailed noncausal AR(1) speciﬁcations ﬁtted on the

real Nasdaq and S&P500 series using three methodologies: 1) t-student maximum likelihood (function marx implemented

in the R package MARX [Hecq et al. (2017b)]), 2) skewed-t regression (function selm implemented in the R package sn

[Azzalini (2018)]), 3) OLS estimation of ρ followed by Hill estimator of α on the residuals of the OLS step (function

hillplot implemented in the R package evmix [Hu and Scarrott (2018)]).

16

Nasdaq

Expected life (Y) Half-life (Y)

95%–life (Y)

99%–life (Y)

t-student

skewed-t

OLS+Hill

3.3

2.5

4.0

2.3

1.7

2.7

9.7

7.4

11.7

14.9

11.4

18.0

Probability of crash within h months (%)

Hazard rate

h = 1

t-student

skewed-t

OLS+Hill

0.025

0.033

0.021

2.5

3.3

2.1

3

7.4

9.6

6.2

6

14.3

18.3

12.0

12

26.5

33.3

22.5

Table 6: Nasdaq: Summary of bubble survival statistics using the formulae from Table 1, based on

estimates of α and ρ of noncausal AR(1) speciﬁcations ﬁtted using three methodologies: 1) t-student

maximum likelihood (function marx implemented in the R package MARX [Hecq et al. (2017b)]), 2) skewed-

t regression (function selm implemented in the R package sn [Azzalini (2018)]), 3) OLS estimation of

ρ followed by Hill estimator of α on the residuals of the OLS step (function hillplot implemented in

the R package evmix [Hu and Scarrott (2018)]). See Table 5 for the parameter estimates. Expected life,

half-life, 95%-life, 99%-life are reported in years (Y) and crash probabilities are reported in percents (%).

17

S&P500

Expected life (Y) Half-life (Y)

95%–life (Y)

99%–life (Y)

t-student

skewed-t

OLS+Hill

3.1

2.3

5.8

2.1

1.6

4.0

9.2

6.7

17.2

14.2

10.3

26.4

Probability of crash within h months (%)

Hazard rate

h = 1

t-student

skewed-t

OLS+Hill

0.027

0.037

0.014

2.7

3.7

1.4

3

7.8

10.6

4.3

6

15.0

20.0

8.4

12

27.8

36.0

16.0

Table 7: S&P500: Summary of bubble survival statistics using the formulae from Table 1, based on

estimates of α and ρ of noncausal AR(1) speciﬁcations ﬁtted using three methodologies: 1) t-student

maximum likelihood (function marx implemented in the R package MARX [Hecq et al. (2017b)]), 2) skewed-

t regression (function selm implemented in the R package sn [Azzalini (2018)]), 3) OLS estimation of

ρ followed by Hill estimator of α on the residuals of the OLS step (function hillplot implemented in

the R package evmix [Hu and Scarrott (2018)]). See Table 5 for the parameter estimates. Expected life,

half-life, 95%-life, 99%-life are reported in years (Y) and crash probabilities are reported in percents (%).

18

B Preliminary elements for the proof of the main results

B.1 Notations for the proofs of Theorem 2.2 and Proposition 2.1

The

proof

of Theorem 2.2

is

quite

involved

and

relies

on

techniques

used

in

[Cioczek-Georges and Taqqu (1994), Cioczek-Georges and Taqqu (1998)].

It consists in diﬀerenti-

ating the conditional characteristic function of X2|X1 up to the fourth derivation order and evaluating

the derivatives at 0 to obtain the conditional moments. Formal computation of the derivatives yields

divergent terms for the third and fourth order derivatives, as well as for the second order derivative

when 1/2 < α < 1 and special manipulations are needed (in particular the «appropriate integration by

parts» in Cioczek-Georges and Taqqu (1994) (p.106) as well as an additional manipulation to obtain the

fourth derivative). We ﬁrst introduce some notations to make the presentation of the proof as compact

as possible, then provide the derivatives in Lemma B.1 and ﬁnally show Theorem 2.2 by obtaining the

functional forms of the conditional moments.

Let X = (X1, X2) be an α-stable vector, with 0 < α < 2, α 6= 1, and spectral representation (Γ, 0).

Its characteristic function will be denoted ϕX (t, r) for any (t, r) ∈ R2, and reads

ϕX (t, r) = exp

Z

(cid:26)

−

S2

g1(ts1 + rs2)Γ(ds)

(cid:27)

,

(B.1)

where g1(z) = |z|α − iaz<α> for z ∈ R, and a = tg(πα/2). As we assume σ1 > 0 so that X1 is not
degenerate, the conditional characteristic function of X2 given X1 = x, denoted φX2|x(r) for r ∈ R,
equals

φX2|x(r) := 1 +

1
2πfX1(x)

Z

R

e−itx(cid:16)

(cid:17)
ϕX (t, r) − ϕX (t, 0)

dt.

(B.2)

where fX1 denotes the density of X1 ∼ S(α, β1, σ1, 0). The following notation of the H family function
for any y > −1 and θ = (θ1, θ2) ∈ R2, deﬁne the function

will be more handy than that in (2.4):

H(y, θ; · ) for x ∈ R as

H(y, θ; x) =

Z +∞

e−σα

1 uα

uy(cid:16)

0

θ1 cos(ux − aβ1σα

1 uα) + θ2 sin(ux − aβ1σα

(cid:17)
1 uα)

For z ∈ R, denote also,

g2(z) = z<α−1> − ia|z|α−1,

g3(z) = |z|α−2 − iaz<α−2>.

Often, we shall invoke functions of the form

r 7−→

Z

R

e−itxϕX (t, r)f p1

1 (t, r) . . . f pm

m (t, r)dt,

19

du,

(B.3)

(B.4)

(B.5)

(B.6)

where m ≤ 3 and the fi’s will be functions of the type fi(t, r) = R
2 Γ(ds), for ji = 2, 3,
ki, ‘i ∈ Z for which fi is well deﬁned and positive integer exponents pi’s. As a shorthand when no

gji(ts1 + rs2)ski

1 s‘i

S2

ambiguity is possible, we shall denote functions like (B.6) by

(cid:18) Z

Λ

S2

gj1sk1

1 s‘1
2

(cid:19)p1(cid:18) Z

S2

gj2sk2

1 s‘2
2

(cid:19)p2

. . .

up to the mth term.

B.2 Lemma B.1 for the proof of Theorem 2.2

Lemma B.1 Let (X1, X2) be an α-stable vector, 0 < α < 2,α 6= 1, with conditional characteristic
function φX2|x as given in (B.2). Let r ∈ R. If 1 < α < 2, or if 0 < α < 1 and (2.2) holds with ν > 1 − α,
the ﬁrst derivative of φX2|x is given by

φ(1)
X2|x(r) =

−α
2πfX1(x)

(cid:18) Z

Λ

S2

(cid:19)
.

g2s2

(B.7)

If 1/2 < α < 2 and (2.2) holds with ν > 2 − α, the second derivative is given by

φ(2)
X2|x(r) =

−α
2πfX1(x)

"

(cid:18) Z

ixΛ

S2

g2s2

2s−1
1

(cid:19)

+ α

(

(cid:18) Z

Λ

S2

g2s2

2s−1
1

(cid:19)(cid:18) Z

S2

(cid:19)

g2s1

(cid:18) Z

− Λ

S2

(cid:19)2)#
,

g2s2
2

(B.8)

If 1 < α < 2 and (2.2) holds with ν > 3 − α, the third derivative is given by

φ(3)
X2|x(r) =

−α
2πfX1(x)

(cid:18)

(cid:16)

ix

(α − 1)I1 − αI2

(cid:17)

+ α2(I3 − I4) + α(α − 1)(I5 + I6 − 2I7)

(cid:19)
,

(B.9)

with

I1 = Λ

I2 = Λ

I3 = Λ

I4 = Λ

(cid:18) Z

(cid:18) Z

(cid:18) Z

(cid:18) Z

S2

S2

S2

S2

g3s3

2s−1
1

(cid:19)
,

g2s2

(cid:19)(cid:18) Z

S2

g2s2

2s−1
1

(cid:19)
,

(cid:19)3

,

g2s2

g2s1

(cid:19)(cid:18) Z

S2

g2s2

(cid:19)(cid:18) Z

S2

g2s2

2s−1
1

(cid:19)
.

I5 = Λ

I6 = Λ

I7 = Λ

(cid:18) Z

(cid:18) Z

(cid:18) Z

g2s2

2s−1
1

(cid:19)(cid:18) Z

S2

(cid:19)

,

g3s2s1

g2s1

g2s2

(cid:19)(cid:18) Z

S2

(cid:19)(cid:18) Z

S2

g3s3

2s−1
1

(cid:19)

,

(cid:19)
,

g3s2
2

S2

S2

S2

If 3/2 < α < 2 and (2.2) holds with ν > 4 − α, the fourth derivative is given by

φ(4)
X2|x(r) =

"

−α
2πfX1(x)

(cid:18)

(cid:16)

3J1 − 2J2

(cid:17)

+ (α − 1)

(cid:16)

2J3 − 3J4 + J5

(cid:17)(cid:19)

+ αx2J6 − (α − 1)x2J7

α

iαx

+ α2(α − 1)

(cid:18)

J8 + J9 + J10 − 3

(cid:16)

2J11 + J12 − J13

(cid:17)(cid:19)

(B.10)

+ α(α − 1)2

(cid:18)

4J14 − 3J15 − J16

(cid:19)

(cid:18)

+ α3

3J17 − J18 − J19

(cid:19)#
,

20

with

J1 = Λ

J2 = Λ

J3 = Λ

J4 = Λ

J5 = Λ

J6 = Λ

J7 = Λ

J8 = Λ

J9 = Λ

(cid:16) Z

(cid:16) Z

(cid:16) Z

(cid:16) Z

(cid:16) Z

(cid:16) Z

(cid:16) Z

(cid:16) Z

(cid:16) Z

(cid:16) Z

S2

S2

S2

S2

S2

S2

S2

S2

S2

S2

g2s2

2s−1
1

g2s3

2s−2
1

g3s4

2s−2
1

g3s3

2s−1
1

g2s3

2s−2
1

g2s3

2s−2
1

(cid:17)(cid:16) Z

S2

(cid:17)(cid:16) Z

S2

(cid:17)(cid:16) Z

S2

(cid:17)(cid:16) Z

S2

(cid:17)(cid:16) Z

S2

(cid:17)(cid:16) Z

S2

g3s4

2s−2
1

(cid:17)
,

g2s3

2s−2
1

g2s3

2s−2
1

g3s4

2s−2
1

(cid:17)(cid:16) Z

S2

(cid:17)(cid:16) Z

S2

(cid:17)(cid:16) Z

S2

J10 = Λ

(cid:17)2

,

g2s2

g2s1

(cid:17)(cid:16) Z

S2

(cid:17)

,

g2s2

(cid:17)

(cid:17)

,

,

g2s1

g2s2

g3s2s1

(cid:17)

,

(cid:17)

,

g2s2

(cid:17)(cid:16) Z

g3s2
1

(cid:17)

,

g2s2

S2
(cid:17)(cid:16) Z

(cid:17)

,

g2s1

S2

g3s2s1

(cid:17)2

.

g2s1

J11 = Λ

g2s2

2s−1
1

(cid:16) Z

(cid:16) Z

(cid:16) Z

(cid:16) Z

(cid:16) Z

(cid:16) Z

(cid:16) Z

(cid:16) Z

(cid:16) Z

S2

S2

S2

S2

S2

S2

S2

S2

S2

J12 = Λ

J13 = Λ

J14 = Λ

J15 = Λ

J16 = Λ

J17 = Λ

J18 = Λ

J19 = Λ

(cid:17)(cid:16) Z

(cid:17)(cid:16) Z

(cid:17)(cid:16) Z

g3s2s1

(cid:17)

,

g2s2

S2

S2

g2s1

(cid:17)(cid:16) Z

S2

S2

(cid:17)

,

g2s2

(cid:17)2

,

g2s2

S2
(cid:17)(cid:16) Z

S2

(cid:17)(cid:16) Z

S2

(cid:17)(cid:16) Z

S2

g3s2s1

(cid:17)

,

g3s2
1

(cid:17)

,

g2s1

(cid:17)(cid:16) Z

S2

(cid:17)2

,

g2s2

g3s3

2s−1
1
(cid:17)(cid:16) Z

g3s2
2

g3s3

2s−1
1

(cid:17)2

,

g3s2
2

g3s4

2s−2
1

g2s2

2s−1
1

(cid:17)4

,

g2s2

g2s3

2s−2
1

(cid:17)(cid:16) Z

S2

g2s1

(cid:17)2(cid:16) Z

S2

(cid:17)

,

g2s2

C Proof of Lemma B.1

For each of the derivatives, the proof involves two main steps: 1) justifying inversion of integral and

derivation signs 2) computation of the derivative.

C.1 Justifying inversion of integral and derivation signs

C.1.1 Justifying inversion: First derivative

Case α ∈ (0, 1)

Assume α ∈ (0, 1). We begin with the ﬁrst derivative of the imaginary part of φX2|x.

(cid:16)

d
dr

ImφX2|x(r)

(cid:17)

=

−1
2πfX1(x)

lim
h→0

1
h

"

Z

R

− R
e

S2

|ts1+(r+h)s2|αΓ(ds)

(cid:18)

tx − a

sin

Z

S2

(ts1 + (r + h)s2)<α>Γ(ds)

(cid:19)

− R

S2

− e

|ts1+rs2|αΓ(ds)

(cid:18)

sin

tx − a

Z

S2

(ts1 + rs2)<α>Γ(ds)

(cid:19) #

dt

=

−1
2πfX1(x)

lim
h→0

1
h

"

Z

R

sin

(cid:18)

tx − a

Z

S2

(ts1 + (r + h)s2)<α>Γ(ds)

(cid:19)

21

(cid:18)

− sin

tx − a

Z

S2

(ts1 + rs2)<α>Γ(ds)

(cid:19) #

× exp

Z

n

−

S2

o
|ts1 + rs2|αΓ(ds)

dt

−

1
2πfX1(x)

lim
h→0

1
h

"

Z

R

n

−

exp

Z

S2

o
|ts1 + (r + h)s2|αΓ(ds)

− exp

Z

n

−

S2

|ts1 + rs2|αΓ(ds)

#

o

:= I1 + I2.

The integrand of I1 converges to

(cid:18)

tx − a

× sin

Z

S2

(ts1 + (r + h)s2)<α>Γ(ds)

(cid:19)

dt

(C.1)

−αa cos

Z

(cid:16)

tx − a

S2

(ts1 + rs2)<α>Γ(ds)

Z

(cid:17)

×

S2

|ts1 + rs2|α−1s2Γ(ds) × exp

Z

n

−

S2

o
|ts1 + rs2|αΓ(ds)

Using the mean value theorem, the triangle inequality and the inequality −|x + y|α ≤ −|x|α + |y|α when

0 < α < 1, the integrand of I1 can be bounded for any h, |h| < |r|, by

(cid:12)
(cid:12)

Z

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:18)(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)cos(y)

a
h
S2
2 e−σα
≤ 2|a|e|r|ασα

(cid:12)(ts1 + (r + h)s2)<α> − (ts1 + rs2)<α>(cid:12)
1 |t|α Z

|ts1 + rs2|α−1Γ(ds),

(cid:12)
(cid:12)Γ(ds)

S2

(cid:19)

exp

n Z

S2

−|ts1|α + |rs2|αΓ(ds)

o

(C.2)

where σ2 =

(cid:16) R

S2

|s2|αΓ(ds)

(cid:17)1/α

, y ∈ R, and we used the bound

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(ts1 + (r + h)s2)<α> − (ts1 + rs2)<α>
h

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ 2|ts1 + rs2|α−1|s2|,

(C.3)

for ts1 + rs2 6= 0, which is a consequence of ||1 + z|<α> − 1| ≤ 2|z|, for z ∈ R (see Lemma C.3 (ιι) below).

Bound (C.2) does not depend on h and is integrable with respect to t. Indeed, invoking Lemma C.5 with

η = α − 1, b = p = 0, and (2.2) with ν > 2 − α > 1 − α

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Z

R

1 |t|α Z

e−σα

S2

α−1

(cid:12)
(cid:12)
(cid:12)t + r

(cid:12)
(cid:12)
(cid:12)

s2
s1

|s1|α−1Γ(ds)dt −

Z

Z

R

S2

e−σα

1 |t|α

(cid:12)
(cid:12)
|t|α−1|s1|α−1Γ(ds)dt
(cid:12)
(cid:12)
(cid:12)

Z

≤

S2

|s1|α−1

Z

R

e−σα

1 |t|α

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)t + r

(cid:12)
(cid:12)
(cid:12)

s2
s1

α−1

− |t|α−1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

dtΓ(ds)

|s1|α−1+ν|s1|−νΓ(ds)

|s1|−νΓ(ds)

Z

S2

Z

S2

≤ const

≤ const

< +∞,

22

(C.4)

and the integrability with respect to t follows from the fact that R

R e−σα

1 |t|α|t|α−1dt < +∞. Hence the

Lebesgue dominated convergence theorem applies to I1 and we can invert integration and derivation.

Focusing on I2, its integrand tends to

Z

−α

S2

(ts1 + rs2)<α−1>s2Γ(ds) exp

Z

(cid:26)

−

S2

|ts1 + rs2|αΓ(ds)

(cid:27)

sin

(cid:18)

tx − a

Z

S2

|ts1 + rs2|<α>Γ(ds)

(cid:19)

.

Using the inequality

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(ts1 + (r + h)s2)α − (ts1 + rs2)α
h

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ |ts1 + rs2|α−1|s2|,

for ts1 + rs2 6= 0, which is a consequence of ||1 + z|α − 1| ≤ |z|, for z ∈ R (Lemma C.3 (ι) below) and the
inequality |e−x − e−y| ≤ e−ye|x−y||x − y|, for x, y ∈ R, we can bound the integrand of I2 for any |h| < |r|

by

Z

(cid:26)

−

exp

S2

|ts1 + rs2|αΓ(ds)

(cid:27)

exp

(cid:26)(cid:12)
(cid:12)
(cid:12)
(cid:12)

Z

S2

≤ e2|r|ασα

2 e−σα

1 |t|α Z

S2

α−1

(cid:12)
(cid:12)
(cid:12)t + r

(cid:12)
(cid:12)
(cid:12)

s2
s1

|s1|α−1Γ(ds).

|ts1 + (r + h)s2|α − |ts1 + rs2|αΓ(ds)

(cid:12)
(cid:27)
(cid:12)
(cid:12)
(cid:12)

×

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
h

Z

S2

(cid:12)
(cid:12)
|ts1 + (r + h)s2|α − |ts1 + rs2|αΓ(ds)
(cid:12)
(cid:12)

The integrability with respect to t is deduced as for (C.4) using Lemma C.5 with η = α − 1, b = p = 0.

Thus, the Lebesgue-dominated convergence theorem applies to I2 and we can invert integration and

derivation. The real part of φX2|x(r) can be treated in a similar way, allowing us to derivate under the

integral.

Case α ∈ (1, 2)

Assume α ∈ (1, 2). Just as for the case α ∈ (0, 1), the imaginary part of φX2|x is given by (C.1)

(cid:16)

(cid:17)
ImφX2|x(r)

d
dr

= I1 + I2.

The integrands of I1 and I2 still converges to the same limits, however a diﬀerent argument is needed to

bound them. For |h| < |r|, the mean value theorem, the triangle inequality and the inequality of Lemma

C.4, yield the following bound for the integrand of I1

(cid:18)(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

a
h

Z

S2

(cid:12)
(cid:12)

(cid:12)(ts1 + (r + h)s2)<α> − (ts1 + rs2)<α>(cid:12)

(cid:19)
(cid:12)
(cid:12)Γ(ds)

e|r|ασα

2 e−21−ασα

1 |t|α

,

(C.5)

where y ∈ R. By the triangle inequality and the mean value theorem, we have for some u ∈

(cid:18)

(cid:16)

min

ts1 +

(r + h)s2, ts1 + rs2

(cid:17)

, max

(cid:16)

ts1 + (r + h)s2, ts1 + rs2

(cid:17)(cid:19)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Z

S2

(cid:12)
(cid:12)
(ts1 + (r + h)s2)<α> − (ts1 + rs2)<α>Γ(ds)
(cid:12)
(cid:12)

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Z

S2

(cid:12)
(cid:12)
αhs2|u|α−1Γ(ds)
(cid:12)
(cid:12)

23

Thus, (C.5) can be bounded by

α|a|Γ(S2)e|r|ασα

2 e−21−ασα

1 |t|α

≤ α|h|

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Z

S2

|t|α−1 + 2|r|α−1Γ(ds)

≤ α|h|Γ(S2)(|t|α−1 + 2|r|α−1)

(C.6)

(|t|α−1 + 2|r|α−1),

which is certainly integrable with respect to t on R for α > 1. Let us now turn to I2. We have again by

the mean value theorem,

(cid:12)
(cid:12)
(cid:12)
(cid:12)

|ts1 + (r + h)s2|α − |ts1 + rs2|α
h

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ α(|t|α−1 + 2|r|α−1),

if |h| < |r|, and thus

− R

S2

e

(cid:12)
(cid:12)
(cid:12)
(cid:12)

|ts1+(r+h)s2|αΓ(ds)

− R

− e

S2

|ts1+rs2|αΓ(ds)

h

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ max

(cid:18)

− R
e

S2

|ts1+(r+h)s2|αΓ(ds)

− R

, e

S2

|ts1+rs2|αΓ(ds)(cid:19)

Z

×

S2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

|ts1 + (r + h)s2|α − |ts1 + rs2|α
h

(cid:12)
(cid:12)
Γ(ds)
(cid:12)
(cid:12)

≤ Γ(S2)e|2r|ασα

2 e−21−ασα

1 |t|α

α(|t|α−1 + 2|r|α−1),

(C.7)

by Lemma C.1 (C.18) and Lemma C.4. The latter bound is again integrable with respect to t on R.

Hence the dominated convergence theorem applies to I1, I2 and therefore to

(cid:16)

(cid:17)
ImφX2|x(r)

d
dr

and we can

invert the integration and derivation signs. Similar arguments show the dominated convergence theorem

applies to the real part of the conditional characteristic function as well.

C.1.2 Justifying inversion: Second derivative

Case α ∈ (1/2, 1)

In an expanded fashion, φ(1)

X2|x(r) can be written,

φ(1)
X2|x(r) =

h

−α
2πfX1(x)

J1 − aJ2 − i(J3 + aJ4)

i
,

(C.8)

with,

J1(r) =

− R

S2

e

Z

R

|ts1+rs2|αΓ(ds)

cos

tx − a

Z

S2

(ts1 + rs2)<α>Γ(ds)

! Z

S2

(ts1 + rs2)<α−1>s2Γ(ds)dt,

24

 
J2(r) =

J3(r) =

J4(r) =

− R

S2

e

− R
e

S2

− R
e

S2

Z

R

Z

R

Z

R

|ts1+rs2|αΓ(ds)

sin

tx − a

|ts1+rs2|αΓ(ds)

sin

tx − a

|ts1+rs2|αΓ(ds)

cos

tx − a

Z

S2

Z

S2

Z

(ts1 + rs2)<α>Γ(ds)

(ts1 + rs2)<α>Γ(ds)

! Z

S2

! Z

S2

|ts1 + rs2|α−1s2Γ(ds)dt,

(ts1 + rs2)<α−1>s2Γ(ds)dt,

(ts1 + rs2)<α>Γ(ds)

|ts1 + rs2|α−1s2Γ(ds)dt.

! Z

S2

S2

To obtain φ(2)

X2|x(r), we will show that the dominated convergence theorem applies to J 0

1. Let us consider,

J 0
1(r) = lim
h→0

"

Z

R

1
h

n

−

exp

Z

S2

|ts1 + (r + h)s2|αΓ(ds)

o

(cid:16)

cos

tx − a

Z

S2

(ts1 + (r + h)s2)<α>Γ(ds)

(cid:17)

Z

×

S2

(ts1 + (r + h)s2)<α−1>s2Γ(ds)

− exp

Z

n

−

S2

|ts1 + rs2|αΓ(ds)

o

(cid:16)

cos

tx − a

Z

S2

(ts1 + rs2)<α>Γ(ds)

(cid:17)

#
(ts1 + rs2)<α−1>s2Γ(ds)

dt

Z

×

S2

"

Z

R

1
h

= lim
h→0

n

−

exp

Z

S2

|ts1 + (r + h)s2|αΓ(ds)

o

− exp

Z

n

−

S2

o
|ts1 + rs2|αΓ(ds)

#

× cos

Z

(cid:16)

tx − a

(ts1 + rs2)<α>Γ(ds)

(ts1 + rs2)<α−1>s2Γ(ds)dt

(cid:17) Z

S2

S2

o
|ts1 + (r + h)s2|αΓ(ds)

(C.9)

+ lim
h→0

1
h

Z

R

n

−

exp

Z

S2
"

×

cos

Z

(cid:16)

tx − a

S2

(cid:17)
(ts1 + (r + h)s2)<α>Γ(ds)

− cos

Z

(cid:16)

tx − a

S2

(ts1 + rs2)<α>Γ(ds)

#

(cid:17)

+ lim
h→0

1
h

Z

R

n

−

exp

Z

S2

S2
o
|ts1 + (r + h)s2|αΓ(ds)

(cid:16)

cos

tx − a

Z

S2

(ts1 + (r + h)s2)<α>Γ(ds)

(cid:17)

Z

×

(ts1 + rs2)<α−1>s2Γ(ds)dt

" Z

×

S2

(ts1 + (r + h)s2)<α−1>s2Γ(ds) −

#
(ts1 + rs2)<α−1>s2Γ(ds)

dt

Z

S2

:= K1 + K2 + K3.

(C.10)

It can be shown that the dominated convergence theorem applies to K1 following the proof

in

Cioczek-Georges and Taqqu (1994) (p.105) for I1. Consider K2. The integrand converges to

  Z

αa

S2

|ts1 + rs2|α−1s2Γ(ds)

!  Z

S2

(ts1 + rs2)<α−1>s2Γ(ds)

!

25

 
 
 
Z

(cid:16)

tx − a

× sin

S2

(ts1 + rs2)<α>Γ(ds)

(cid:17)

n

−

exp

Z

S2

o
|ts1 + rs2|αΓ(ds)
.

Using the mean value theorem, (C.3) and the triangle inequality, we can bound the integrand for any

|h| < |r| by

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
h

Z

S2

(cid:12)
(cid:12)
(ts1 + (r + h)s2)<α> − (ts1 + rs2)<α>Γ(ds)
(cid:12)
(cid:12)
(cid:12)
α−1

Z

× | sin(y)|e2|r|ασα

2 e−|t|ασα

1

≤ 2e2|r|ασα

2

  Z

S2

α−1

(cid:12)
(cid:12)
(cid:12)t + r

(cid:12)
(cid:12)
(cid:12)

s2
s1

|s1|α−1Γ(ds)

e−|t|ασα

1

(cid:12)
(cid:12)
(cid:12)

s2
s1

(cid:12)
(cid:12)
(cid:12)t + r
!2

S2

|s2||s1|α−1Γ(ds)

(C.11)

where y ∈ R. The bound (C.11) does not depend on h and is integrable with respect to t: invoking (2.9)

Lemma 2.2 in Cioczek-Georges and Taqqu (1994),

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Z

Z

Z

R

S2

S2

e−σα

1 |t|α (cid:12)
(cid:12)
(cid:12)t + r

(cid:12)
(cid:12)
(cid:12)

α−1(cid:12)
(cid:12)
(cid:12)t + r

s2
s1

s0
2
s0
1

(cid:12)
(cid:12)
(cid:12)

α−1

|s0

1|α−1|s1|α−1Γ(ds)Γ(ds0)dt

(C.12)

−

Z

Z

Z

R

S2

S2

e−σα

1 |t|α

|t|2α−2dtΓ(ds)Γ(ds0)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Z

Z

S2

S2

|s0

1|α−1|s1|α−1

Z

R

e−σα

1 |t|α

"
(cid:12)
(cid:12)
(cid:12)t + r

s2
s1

(cid:12)
(cid:12)
(cid:12)

α−1(cid:12)
(cid:12)
(cid:12)t + r

s0
2
s0
1

(cid:12)
(cid:12)
(cid:12)

α−1

−

(cid:12)
(cid:12)
(cid:12)t + r

+

(cid:12)
(cid:12)
(cid:12)t + r

(cid:12)
(cid:12)
(cid:12)

α−1(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)t
(cid:12)

s2
s1

α−1

− |t|2α−2

(cid:12)
(cid:12)
(cid:12)

α−1

α−1(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)t
(cid:12)

s2
s1
#
dtΓ(ds)Γ(ds0)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Z

Z

≤

S2

S2

|s0

1|α−1|s1|α−1

Z

R

e−σα

1 |t|α

"(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)t + r

s0
2
s0
1

(cid:12)
(cid:12)
(cid:12)

α−1

− |t|α−1

α−1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)t + r

(cid:12)
(cid:12)
(cid:12)

s2
s1

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)t + r

(cid:12)
(cid:12)
(cid:12)

s2
s1

α−1

− |t|α−1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

#

|t|α−1

dtΓ(ds)Γ(ds0)

  Z

≤ const

S2

!2

|s1|α−1Γ(ds)

< +∞,

(C.13)

where const is a constant depending only on α and σα
fact that R

1 . The integrability of (C.11) follows from (C.13), the
1 |t|α|t|2α−2dt < +∞ and (2.2) with ν > 2 − α > 1 − α. Hence the dominated convergence

R e−σα

theorem applies to K2. Let us now turn to K3: «this is [a] case when appropriate "integration by parts"

is needed» (Cioczek-Georges and Taqqu (1994)). With the change of variable t0 = t +

hs0
2
s0
1

,

K3 = lim
h→0

" Z

R

1
h

n

−

exp

Z

S2

|ts1 + (r + h)s2|αΓ(ds)

o

(cid:16)

cos

tx − a

Z

S2

(ts1 + (r + h)s2)<α>Γ(ds)

(cid:17)

26

Z

×

(t +

hs0
2
s0
1

+

rs0
2
s0
1

S2
|ts1 + (r + h)s2|αΓ(ds)

−

Z

R

n

−

exp

Z

S2

2s0
1
Z

S2

)<α−1>s0

<α−1>Γ(ds0)dt

o

(cid:16)

cos

tx − a

(ts1 + (r + h)s2)<α>Γ(ds)

(cid:17)

Z

×

S2

(t +

rs0
2
s0
1

)<α−1>s0

2s0
1

<α−1>Γ(ds0)dt

#

= lim
h→0

1
h

Z

Z

"

R

S2

(cid:26)

Z

−

exp

S2

(cid:16)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

t −

s1 + (r + h)s2

(cid:27)

Γ(ds)

α

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:17)

hs0
2
s0
1
hs0
2
s0
1

(cid:18)(cid:16)

× cos

t −

(cid:17)

x − a

Z

(cid:18)(cid:16)

t −

S2

hs0
2
s0
1

(cid:17)
s1 + (r + h)s2

(cid:19)<α>

(cid:19)

Γ(ds)

− exp

Z

n

−

S2

|ts1 + (r + h)s2|αΓ(ds)

o

(cid:16)

cos

tx − a

Z

S2

(ts1 + (r + h)s2)<α>Γ(ds)

#

(cid:17)

(cid:18)

t + r

×

(cid:19)<α−1>

2s0
s0
1

<α−1>Γ(ds0)dt

s0
2
s0
1
hs0
2
s0
1

= lim
h→0

1
h

Z

Z

R

S2

1
hs0
2
s0
1

"

(cid:18)(cid:16)

t −

cos

(cid:17)

hs0
2
s0
1

x − a

Z

(cid:18)(cid:16)

S2

t −

(cid:17)

s1 + (r + h)s2

(cid:19)<α>

(cid:19)

Γ(ds)

− cos

Z

(cid:16)

tx − a

S2

(cid:17)
(ts1 + (r + h)s2)<α>Γ(ds)

#

× exp

Z

n

−

+ lim
h→0

1
h

Z

Z

R

S2

1
hs0
2
s0
1

"

(cid:26)

−

exp

S2
Z

S2

|ts1 + (r + h)s2|αΓ(ds)

o(cid:18)

t + r

(cid:19)<α−1>

s0
2

2|s0

1|α−2Γ(ds0)dt

(cid:16)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

t −

(cid:17)

hs0
2
s0
1

s1 + (r + h)s2

α

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Γ(ds)

− exp

Z

n

−

S2

o
|ts1 + (r + h)s2|αΓ(ds)

#

s0
2
s0
1
(cid:27)

(cid:18)(cid:16)

t −

× cos

Z

(cid:18)(cid:16)

t −

x − a

S2

hs0
2
s0
1

(cid:17)
s1 + (r + h)s2

(cid:19)<α>

(cid:19)

Γ(ds)

(cid:18)

t + r

(cid:19)<α−1>

s0
2

2|s0

1|α−2Γ(ds0)dt

(cid:17)

hs0
2
s0
1
s0
2
s0
1

×

= K31 + K32.

The case of K32 is similar to that of I22 in Cioczek-Georges and Taqqu (1994) (p.106-108), the dominated

convergence theorem applies. We focus on K31. Its integrand converges to
)

(

!

sin

tx − a

(ts1 + rs2)<α>Γ(ds)

exp

−

|ts1 + rs2|αΓ(ds)

Z

S2

Z

S2

×

x − αa

Z

S2

|ts1 + rs2|α−1s1Γ(ds)

!  Z

S2

(ts0

1 + rs0

2)<α−1>s0
2

!

−1Γ(ds0)

.

2s0
1

Using the mean value theorem and Lemma C.3 (ιι), we can bound the integrand of K31 for any |h| < |r|

by

| sin(y)|e2|r|ασα

2 e−|t|ασα

1

Z

S2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

t + r

α−1

s0
2
s0
1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

s0
2

2|s0

1|α−2

27

 
 
×

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
hs0
2
s0
1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

hs0
2
s0
1

Z

(cid:18)(cid:16)

t −

x − a

S2

(cid:17)

hs0
2
s0
1

s1 + (r + h)s2

(cid:19)<α>

− (ts1 + (r + h)s2)<α>Γ(ds)

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Γ(ds0)

≤ e2|r|ασα

2 e−|t|ασα

1

Z

S2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

t + r

α−1

s0
2
s0
1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

s0
2

2|s0

1|α−2

|x| + 2a

Z

S2

(cid:12)
(cid:12)
(cid:12)t + (r + h)

(cid:12)
(cid:12)
(cid:12)

s2
s1

α−1

!

|s1|Γ(ds)

Γ(ds0)

≤ |x|e2|r|ασα

2 e−|t|ασα

1

Z

+ 2ae2|r|ασα

2 e−|t|ασα

1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

t + r

Z

S2
Z

S2

S2

s0
2
s0
1
(cid:12)
(cid:12)
(cid:12)
(cid:12)

α−1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

s0
2

2|s0

1|α−2Γ(ds0)

t + r

α−1(cid:12)
(cid:12)
(cid:12)t + (r + h)

s0
2
s0
1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

α−1
(cid:12)
(cid:12)
(cid:12)

s2
s1

|s1|s0
2

2|s0

1|α−2Γ(ds)Γ(ds0).

The integrability with respect to t of the ﬁrst (resp. second) term is obtained in the same way as for

(C.4) (resp.

(C.13)) and concluding using (2.2) with ν > 2 − α. Thus, the dominated convergence

theorem applies to K31, which ﬁnally shows that the dominated convergence theorem applies to J 0

1. The

other J’s can be treated in a similar fashion.

Case α ∈ (1, 2)

After derivation, φ(1)

X2|x(r) is given by (C.8) with functions J’s of the form

− R
e

S2

Z

R

|ts1+rs2|αΓ(ds)

trig

tx − a

Z

S2

|ts1 + rs2|<α>Γ(ds)

! Z

S2

(ts1 + rs2)<α−1> or α−1s2Γ(ds)dt,

which are similar to deal with. Consider for instance J1(r). It’s derivative can be written as in (C.10)

For the integrand of K1, we can use (C.7) and the triangle inequality to bound it by

J 0
1(r) = K1 + K2 + K3.

Γ(S2)e|2r|ασα

2 e−21−ασα

1 |t|α

α(|t|α−1 + 2|r|α−1)

Z

S2

|ts1 + rs2|α−1|s2|Γ(ds).

Since 0 < α − 1 < 1, we can further bound it by

Γ(S2)e|2r|ασα

2 e−21−ασα

1 |t|α

α(|t|α−1 + 2|r|α−1)2,

which is integrable with respect to t. The same bound can be obtained for the integrand of K2 using

the mean value theorem, (C.6) and Lemma C.4. As for K3, there is no need to perform "appropriate

integration by parts" since 0 < α − 1 < 1. Its integrand converges to

(α − 1) exp

Z

n

−

S2

|ts1 + rs2|αΓ(ds)

o

(cid:16)

cos

tx − a

Z

S2

28

(ts1 + rs2)<α>Γ(ds)

(cid:17) Z

S2

|ts1 + rs2|α−2s2

2Γ(ds).

 
 
Using Lemmas C.4 and C.3 (ιι), it can be bounded for any |h| < |r| by

Γ(S2)e|2r|ασα

2
|h|
≤ Γ(S2)e|2r|ασα

2 e−21−ασα

1 |t|α Z

2 e−21−ασα

1 |t|α Z

|ts1 + rs2|α−2|hs2|Γ(ds),

S2
(cid:12)
(cid:12)
(cid:12)t +

S2

α−2

rs2
s1

(cid:12)
(cid:12)
(cid:12)

|s1|α−2Γ(ds).

We can show that this bound is integrable with respect to t using Lemma C.5 with η = α − 2, b = 0 and
p = 0, the fact that R
1 |t|α|t|α−2dt < +∞ for α ∈ (1, 2) and (2.2) with ν > 2 − α. The dominated

R e−21−ασα

convergence theorem thus applies and we get

φ(2)
X2|x(r) =

−α
2πfX1(x)

"

− α

Z

R

e−itxϕX (t, r)

(cid:16) Z

S2

g2(ts1 + rs2)s2Γ(ds)

(cid:17)2

dt

+ (α − 1)

Z

R

e−itxϕX (t, r)

(cid:16) Z

S2

g3(ts1 + rs2)s2

2Γ(ds)

(cid:17)

#

dt

,

(C.14)

with g3(z) = |z|α−2 − iaz<α−2> for z ∈ R. Integrating by parts the terms |ts1 + rs2|<α−2> or α−2 involved
in the expression R

dt yields the expression (B.8) obtained in the

g3(ts1 + rs2)s2

R e−itxϕX (t, r)

(cid:16) R

(cid:17)

2Γ(ds)

S2

case α ∈ (1/2, 1). Hence, the same functional form for the second order conditional moment (2.6) in

Theorem 2.2 holds when α > 1.

C.1.3 Justifying inversion: Third derivative

Let α ∈ (1, 2) and let (2.2) hold with ν > 3 − α. Starting from the second derivative of φ(2)

X2|x(r) given at

(B.8), with obvious notations

φ(2)
X2|x(r) =

−α
2πfX1(x)

h

i
ixI1(r) + α(I3(r) − I2(r))

On the one hand, it can be shown that the dominated convergence theorem applies to I 0

1 using the

usual arguments the fact that (2.2) holds with ν > 3 − α. On the other hand, after some elementary

manipulations, we get that

I3 − I2 =

−itx+ia R
e

S2

Z

R

(ts1+rs2)<α>Γ(ds)

− R
e

S2

|ts1+rs2|αΓ(ds)

Z

Z

×

S2

S2

(

(ts1 + rs2)<α−1>(ts0

1 + rs0

2)<α−1> − a2|ts1 + rs2|α−1|ts0

1 + rs0

2|α−1

(cid:18)

− ia

|ts1 + rs2|α−1(ts0

1 + rs0

2)<α−1> + (ts1 + rs2)<α−1>|ts0

1 + rs0

2|α−1

(cid:19))

h
2s−1
s2

1 s0

1 − s2s0
2

i
Γ(ds)Γ(ds0)dt

×

29

The previous expression can be decomposed into terms of the form

Z

Z

Z

R

S2

S2

(cid:18)

trig

− tx + a

Z

(ts1 + rs2)<α>Γ(ds)

(cid:19)

S2
|ts1+rs2|αΓ(ds)

− R

× e

S2

× |ts1 + rs2|<α−1> or α−1 × |ts0

1 + rs0

2|<α−1> or α−1

h
2s−1
s2

1 s0

1 − s2s0
2

×

i
Γ(ds)Γ(ds0)dt,

where «trig» is to be replaced by a sine or cosine function. Each of these terms can be treated in a similar

way to show that the dominated convergence theorem applies. We will consider

J(r) =

Z

Z

Z

R

S2

S2

(cid:18)

cos

tx − a

Z

S2

(ts1 + rs2)<α>Γ(ds)

(cid:19)
e

− R

S2

|ts1+rs2|αΓ(ds)

× |ts1 + rs2|α−1(ts0

1 + rs0

2)<α−1>h
2s−1
s2

1 s0

1 − s2s0
2

i

Γ(ds)Γ(ds0)dt.

We have

J 0(r) = lim
h→0

1
h

Z

Z

Z

R

S2

S2

"

(cid:18)

cos

tx − a

Z

S2

(ts1 + (r + h)s2)<α>Γ(ds)

(cid:19)

(cid:18)

tx − a

− cos

Z

S2

(ts1 + rs2)<α>Γ(ds)

(cid:19)#

|ts1+(r+h)s2|αΓ(ds)

|ts1 + (r + h)s2|α−1(ts0

1 + (r + h)s0

2)<α−1>

− R

S2

× e

×

+ lim
h→0

1
h

Z

Z

Z

R

S2

S2

1 s0

h
2s−1
s2
(cid:18)

1 − s2s0
2
Z

i
Γ(ds)Γ(ds0)dt

cos

tx − a

(ts1 + rs2)<α>Γ(ds)

(cid:19)

S2

− R

"
e

S2

×

|ts1+(r+h)s2|αΓ(ds)

− R

S2

− e

|ts1+rs2|αΓ(ds)

#

× |ts1 + (r + h)s2|α−1(ts0
Z

(cid:18)

Z

Z

1 + (r + h)s0

2)<α−1>h
2s−1
s2
− R
(cid:19)

cos

tx − a

(ts1 + rs2)<α>Γ(ds)

e

S2

R

S2

S2

S2

Z

+ lim
h→0

1
h

1 s0

1 − s2s0
2

i
Γ(ds)Γ(ds0)dt

|ts1+rs2|αΓ(ds)

"
|ts1 + (r + h)s2|α−1 − |ts1 + rs2|α−1

#

×

+ lim
h→0

1
h

Z

Z

Z

R

S2

S2

(cid:18)

cos

S2

× (ts0

1 + (r + h)s0
Z

2)<α−1>h

2s−1
1 s0
s2
(cid:19)

1 − s2s0
2
− R

tx − a

(ts1 + rs2)<α>Γ(ds)

e

S2

i

Γ(ds)Γ(ds0)dt

|ts1+rs2|αΓ(ds)

"
(ts0

1 + (r + h)s0

2)<α−1> − (ts0

1 + rs0

2)<α−1>

#

×

× |ts1 + rs2|α−1h

2s−1
s2

1 s0

1 − s2s0
2

i
Γ(ds)Γ(ds0)dt

30

:= K1 + K2 + K3 + K4.

We will show that we can apply the dominated convergence theorem to the Ki’s. Let us begin with K1.

Its integrand converges to

Z

αa

S2×S2×S2

(cid:18)

tx − a

sin

Z

S2

(ts1 + rs2)<α>Γ(ds)

(cid:19)
e

− R

S2

|ts1+rs2|αΓ(ds)

× |ts1 + rs2|α−1(ts0

1 + rs0

2)<α−1>|ts00

1 + rs00

2|α−1s00
2

h

2s−1
s2

1 s0

1 − s2s0
2

i
Γ(ds)Γ(ds0)Γ(ds00).

For any h, |h| < |r|, the integrand of K1 can be bounded using the mean value theorem on the cosine

and Lemma C.4 by

|a|
|h|

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Z

S2

(ts1 + (r + h)s2)<α> − (ts1 + rs2)<α>Γ(ds)

×

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Z

Z

S2

S2

|ts1 + (r + h)s2|α−1(ts0

1 + (r + h)s0

2 e−21−ασα

1 |t|α

e2α|r|ασα

(cid:12)
(cid:12)
(cid:12)
(cid:12)
2)<α−1>h
2s−1
s2

1 s0

1 − s2s0
2

i

Γ(ds)Γ(ds0)

(cid:12)
(cid:12)
.
(cid:12)
(cid:12)

(C.15)

Hence, by inequality (C.6) and given that 0 < α − 1 < 1, the quantity (C.15) can be bounded by

α|a|Γ(S2)e2α|r|ασα
(cid:12)
Z
(cid:12)
(cid:12)
(cid:12)

×

Z

S2

S2
≤ α|a|Γ(S2)e2α|r|ασα

2 e−21−ασα

1 |t|α

(|t|α−1 + 2|r|α−1)

|ts1 + (r + h)s2|α−1(ts0

1 + (r + h)s0

2 e−21−ασα

1 |t|α

(|t|α−1 + 2|r|α−1)3

2)<α−1>h
(cid:18)

Γ(S2) +

2s−1
s2

1 s0

1 − s2s0
2

i

(cid:12)
(cid:12)
Γ(ds)Γ(ds0)
(cid:12)
(cid:12)

(cid:19)

|s1|−1Γ(ds)

Z

S2

≤ const e−21−ασα

1 |t|α

(|t|α−1 + 2|r|α−1)3,

where const is a ﬁnite nonnegative constant because of (2.2) with ν > 3 − α > 1 and the fact that Γ is a

ﬁnite measure. This last bound, independent of h, is integrable with respect to t on R. The dominated

convergence theorem applies to K1. Consider now K2. Its integrand converges to

Z

α

(cid:18)

cos

tx − a

Z

(ts1 + rs2)<α>Γ(ds)

(cid:19)

− R
e

S2

|ts1+rs2|αΓ(ds)

S2×S2×S2

S2
1 + rs0
× |ts1 + rs2|α−1(ts0

2)<α−1>(ts00

1 + rs00

2)<α−1>s00
2

h

2s−1
s2

1 s0

1 − s2s0
2

(C.16)

i

Γ(ds)Γ(ds0)Γ(ds00)

By (C.7), the integrand of K2 can be bounded by

1 |t|α

α(|t|α−1 + 2|r|α−1)

Γ(S2)e|2r|ασα
2 e−21−ασα
(cid:12)
Z
Z
(cid:12)
(cid:12)
(cid:12)

S2

S2

|ts1 + (r + h)s2|α−1(ts0

1 + (r + h)s0

2)<α−1>h

2s−1
s2

1 s0

1 − s2s0
2

i

(cid:12)
(cid:12)
Γ(ds)Γ(ds0)
(cid:12)
(cid:12)

Which can be further bounded by an integrable function of t in a similar way as for the integrand of K1.

The dominated convergence theorem applies to K2. Consider now K3. Its integrand converges to

(α − 1)

Z

Z

S2

S2

(cid:18)

tx − a

cos

Z

S2

(ts1 + rs2)<α>Γ(ds)

(cid:19)

− R
e

S2

|ts1+rs2|αΓ(ds)

31

× (ts1 + rs2)<α−2>(ts0

1 + (r + h)s0

2)<α−1>s2

h

2s−1
s2

1 s0

1 − s2s0
2

i

Γ(ds)Γ(ds0)

Using Lemmas C.4, C.3 (ι) and the triangle inequality, the integrand of K3 can be bounded by

2 e−21−ασα

1 |t|α Z

Z

e|r|ασα

1
|h|
≤ e|r|ασα

2 Γ(S2)

Z

S2

S2
S2
1 |t|α
e−21−ασα

|hs2||ts1 + rs2|α−2|ts0

2|α−1(cid:12)
2s−1
(cid:12)s2
1 s0
(cid:12)
(cid:12)1 + |s1|−1(cid:12)
(cid:12)
|ts1 + rs2|α−2(|t|α−1 + 2|r|α−1)
(cid:12)

1 + (r + h)s0

(cid:12)
(cid:12)Γ(ds)

1 − s2s0
2

(cid:12)
(cid:12)Γ(ds)Γ(ds0)
(cid:12)

To show the integrability with respect to t of the last bound we make use of Lemma C.5 with η =
α − 2, b = 0, α − 1 and p = 0 and the fact that with 1 < α < 2, R
1 |t|α|t|α−2dt < +∞ and
R
R e−21−ασα

1 |t|α|t|2α−3dt < +∞

R e−21−ασα

e|r|ασα

2 Γ(S2)

Z

S2

(cid:12)
(cid:12)

(cid:12)1 + |s1|−1(cid:12)

(cid:12)
(cid:12)

Z

R

e−21−ασα

1 |t|α

|s1|α−2(cid:12)
(cid:12)
(cid:12)t + r
" Z

e−21−ασα

s2
α−2
(cid:12)
(cid:12)
(cid:12)
s1
1 |t|α(cid:12)
(cid:12)
(cid:12)
(cid:12)

R

(cid:12)
(cid:12)
(cid:12)t + r

(|t|α−1 + 2|r|α−1)dtΓ(ds)

α−2

(cid:12)
(cid:12)
(cid:12)

s2
s1

− |t|α−2 + |t|α−2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

|t|α−1dt

(cid:12)
(cid:12)

(cid:12)1 + |s1|−1(cid:12)

(cid:12)|s1|α−2
(cid:12)

≤ e|r|ασα

2 Γ(S2)

≤ e|r|ασα

2 Γ(S2)

Z

S2

Z

S2

+ 2|r|α−1

Z

R

e−21−ασα

1 |t|α(cid:12)
(cid:12)
(cid:12)
(cid:12)

α−2

(cid:12)
(cid:12)
(cid:12)

− |t|α−2 + |t|α−2

#

dt

Γ(ds)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)1 + |s1|−1(cid:12)

(cid:12)|s1|α−2
(cid:12)

" Z

R

e−21−ασα

α−2

− |t|α−2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

|t|α−1dt

(cid:12)
(cid:12)
(cid:12)t + r

s2
s1
1 |t|α(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)t + r

s2
(cid:12)
(cid:12)
(cid:12)
s1
1 |t|α(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ 2|r|α−1

Z

R

e−21−ασα

α−2

(cid:12)
(cid:12)
(cid:12)t + r

(cid:12)
(cid:12)
(cid:12)

s2
s1

− |t|α−2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

dt

+

Z

R

e−21−ασα

1 |t|α

|t|2α−3dt

+ 2|r|α−1

Z

R

e−21−ασα

1 |t|α

#
|t|α−2dt

Γ(ds)

Z

S2
(cid:16) Z

≤ const

≤ const

(cid:12)
(cid:12)

(cid:12)1 + |s1|−1(cid:12)

(cid:12)|s1|α−2Γ(ds)
(cid:12)
Z

|s1|α−2Γ(ds) +

|s1|α−3Γ(ds)

(cid:17)
,

S2

S2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

which is ﬁnite because of (2.2) with ν > 3 − α. Hence, the dominated convergence theorem applies to K3.
2)<α−2> −

The case of K4 is similar, using Lemma C.3 (ιι) instead of (ι) to bound the term

1 + (r + h)s0

(ts0

(ts0

1 + rs0

2)<α−2>

(cid:12)
(cid:12)
. The dominated convergence theorem applies to all the Ki’s and we can invert the
(cid:12)
(cid:12)

integration and derivation signs in J 0.

C.1.4 A special manipulation to obtain the fourth derivative

Before derivating φ(3)
integrate by parts the terms containing R

X2|x, we follow the advice stated in Cioczek-Georges and Taqqu (1998) (p.48) and
2Γ(ds), namely

1 Γ(ds) and R

g3(ts1 + rs2)s2

g3(ts1 + rs2)s3

2s−1

S2

S2

32

I1, I6 and I7. This is done in order to guarantee the validity of the representation of the fourth derivative

when (2.2) holds for any ν > 4 − α. If we did not do this step ﬁrst, the obtained fourth derivative would

be valid only when (2.2) holds with ν > 5 − α. We obtain

φ(3)
X2|x(r) =

−α
2πfX1(x)

"

(cid:16)

iαx

I11 − I2 + I62 − 2I72

(cid:17)

− x2I12

+ α2(cid:16)

I3 − I4 − 2I71 + I61

(cid:17)

+ α(α − 1)

(cid:16)

I5 − I63 + 2I73

#

(cid:17)

,

(C.17)

where, in addition to I2, I3, I4 and I5 deﬁned in the Lemma,

I11 = Λ

I61 = Λ

I62 = Λ

I63 = Λ

(cid:18) Z

(cid:18) Z

(cid:18) Z

(cid:18) Z

S2

S2

S2

S2

g2s3

2s−2
1

g2s3

2s−2
1

g2s3

2s−2
1

g2s3

2s−2
1

(cid:19)(cid:18) Z

S2

(cid:19)(cid:18) Z

S2

(cid:19)(cid:18) Z

S2

(cid:19)(cid:18) Z

S2

(cid:19)
,

(cid:19)2

,

g2s1

g2s1

(cid:19)
,

(cid:19)
,

g2s1

g3s2
1

I12 = Λ

I71 = Λ

I72 = Λ

I73 = Λ

(cid:18) Z

(cid:18) Z

(cid:18) Z

(cid:18) Z

S2

S2

S2

S2

g2s3

2s−2
1

(cid:19)
,

g2s2

2s−1
1

g2s2

2s−1
1

g2s2

2s−1
1

(cid:19)(cid:18) Z

S2

(cid:19)(cid:18) Z

S2

(cid:19)(cid:18) Z

S2

g2s1

(cid:19)(cid:18) Z

S2

(cid:19)

,

g2s2

(cid:19)
,

g2s2

(cid:19)
.

g3s2s1

Both justiﬁcation and computation of the fourth derivative are obtained by starting from the above

representation of the third derivative.

C.1.5 Justifying inversion: Fourth derivative

Showing that the dominated convergence theorem holds when diﬀerentiating (C.17) is the most delicate

for the terms: I5, I63 and I73 -the terms involving the function g3, that is, |ts1 + rs2| to the power α − 2.

Arguments and bounds that have already been encountered can be used for the other ones.

Let us show the dominated convergence theorem applies to I5. The cases of I63 and I73 are similar.

We decompose I5 into terms of the form

Z

Z

Z

R

S2

S2

(cid:18)

trig

− tx + a

Z

S2

(ts1 + rs2)<α>Γ(ds)

− R

(cid:19)
e

S2

|ts1+rs2|αΓ(ds)

× |ts1 + rs2|α−1 or <α−1>|ts0

1 + rs0

2|α−2 or <α−2>s2

2s−1

1 s0

2s0

1Γ(ds)Γ(ds0)dt.

Consider for instance

J(r) :=

Z

Z

Z

R

S2

S2

(cid:18)

cos

− tx + a

Z

(ts1 + rs2)<α>Γ(ds)

(cid:19)

− R
e

S2

|ts1+rs2|αΓ(ds)

S2
× |ts1 + rs2|α−1|ts0

1 + rs0

2|α−2s2

2s−1

1 s0

2s0

1Γ(ds)Γ(ds0)dt.

33

We have

J 0(r) = lim
h→0

1
h

Z

Z

Z

R

S2

S2

+ lim
h→0

1
h

Z

Z

Z

R

S2

S2

(cid:20)
|ts0

1 + (r + h)s0

2|α−2 − |ts0

1 + rs0

2|α−2

(cid:21)
|ts1 + (r + h)s2|α−1

(cid:18)

× cos

− tx + a

(ts1 + (r + h)s2)<α>Γ(ds)

(cid:19)

Z

S2

− R

× e

S2

|ts1+(r+h)s2|αΓ(ds)

2s−1
s2

1 s0

2s0

1Γ(ds)Γ(ds0)dt
(cid:21)

|ts0

1 + rs0

2|α−2

(cid:20)
|ts1 + (r + h)s2|α−1 − |ts1 + rs2|α−1

(cid:18)

× cos

− tx + a

(ts1 + (r + h)s2)<α>Γ(ds)

(cid:19)

Z

S2

− R

× e

S2

|ts1+(r+h)s2|αΓ(ds)

2s−1
s2

1 s0

2s0

1Γ(ds)Γ(ds0)dt

1
h
(cid:18)

+ lim
h→0
"

×

cos

Z

Z

Z

R

S2

S2

− tx + a

|ts0

1 + rs0

2|α−2|ts1 + rs2|α−1

(ts1 + (r + h)s2)<α>Γ(ds)

(cid:19)

Z

S2

(cid:18)

− cos

− tx + a

Z

S2

(ts1 + rs2)<α>Γ(ds)

(cid:19)#

− R

× e

S2

|ts1+(r+h)s2|αΓ(ds)

2s−1
s2

1 s0

2s0

1Γ(ds)Γ(ds0)dt

+ lim
h→0

1
h

Z

Z

Z

R

S2

S2

|ts0

1 + rs0

2|α−2|ts1 + rs2|α−1

(cid:18)

× cos

− tx + a

Z

S2

(ts1 + rs2)<α>Γ(ds)

(cid:19)

"

− R
e

S2

×

|ts1+(r+h)s2|αΓ(ds)

− R

S2

− e

|ts1+rs2|αΓ(ds)

#

:= K1 + K2 + K3 + K4

2s−1
s2

1 s0

2s0

1Γ(ds)Γ(ds0)dt

The integrand of K4 can be bounded using inequality (C.16), (C.7) and invoking Lemma C.5 and (2.2)

with ν > 4 − α. The integrand of K3 can be bounded using (C.6) Lemma C.4, and concluding with

Lemma C.5 and (2.2) with ν > 4 − α. Focus now on K2. Using Lemmas C.4 and C.3 (ι), its integrand

can be bounded by

e|2r|ασα

2 e−21−ασα

1 |t|α(cid:12)
(cid:12)
(cid:12)t +

rs0
2
s0
1

(cid:12)
(cid:12)
(cid:12)

α−2(cid:12)
(cid:12)
(cid:12)t +

rs2
s1

(cid:12)
(cid:12)
(cid:12)

α−2

2|s1|α−3|s0
s3

1|α−1|s0

2|.

The later bound does not depend on h and can be shown to be integrable with respect to t using (2.2)
with ν > 4 − α, Lemma C.6 with η = α − 2, z2 = z4 = 0, p = 0 and the fact that R
R e−c|t|α|t|2(α−2) < +∞

for α ∈ (3/2, 2). Let us now turn to the term K1 which is more intricate. Appropriate «integration by
parts» is required. With the change of variable t = t + hs0
s0
1

,

2

K1 = lim
h→0

1
h

Z

Z

Z

S2

S2

R

(cid:16)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

t−

(cid:17)

hs0
2
s0
1

"

− R
e

S2

s1+(r+h)s2

(cid:12)
α
(cid:12)
(cid:12)
(cid:12)

Γ(ds)

− R

− e

S2

|ts1+(r+h)s2|αΓ(ds)

#

34

× cos

(cid:17)

(cid:16)

t −

x − a

hs0
2
s0
1
hs0
2
s0
1
|ts1+(r+h)s2|αΓ(ds)

(cid:12)
(cid:16)
(cid:12)
(cid:12)
(cid:12)

t −

(cid:17)

×

Z

S2

(cid:18)(cid:16)

t −

(cid:17)

hs0
2
s0
1

s1 + (r + h)s2

(cid:19)<α>

!

Γ(ds)

s1 + (r + h)s2

α−1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

|ts0

1 + rs0

2|α−2s2

2s−1

1 s0

2s0

1dtΓ(ds)Γ(ds0)

+ lim
h→0

1
h

Z

Z

Z

S2

S2

R

− R

S2

e

× cos

(cid:16)

t −

(cid:17)

hs0
2
s0
1

Z

(cid:18)(cid:16)

t −

x − a

S2

(cid:17)

hs0
2
s0
1

s1 + (r + h)s2

(cid:19)<α>

!

Γ(ds)

"(cid:12)
(cid:12)
(cid:12)
(cid:12)

×

(cid:16)

t −

(cid:17)

hs0
2
s0
1

s1 + (r + h)s2

α−1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

ts1 + (r + h)s2

α−1#

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ lim
h→0

1
h

Z

Z

Z

S2

R

S2
"

×

cos

× |ts0

1 + rs0
|ts1+(r+h)s2|αΓ(ds)

2|α−2s2

2s−1

1 s0

2s0

1dtΓ(ds)Γ(ds0)

− R

S2

e

(cid:16)

t −

(cid:17)

hs0
2
s0
1

Z

(cid:18)(cid:16)

t −

x − a

S2

(cid:17)

hs0
2
s0
1

s1 + (r + h)s2

(cid:19)<α>

!

Γ(ds)

− cos

tx − a

ts1 + (r + h)s2

(cid:19)<α>

!#

Γ(ds)

Z

(cid:18)

S2

× (cid:12)

(cid:12)ts1 + (r + h)s2

(cid:12)
(cid:12)

:= K11 + K12 + K13.

α−1|ts0

1 + rs0

2|α−2s2

2s−1

1 s0

2s0

1dtΓ(ds)Γ(ds0)

It can be shown that the generalised Lebesgue convergence theorem applies to the terms K11 and K12

following the proof in Cioczek-Georges and Taqqu (1998) (p.50-52). Regarding the integrand of K13,

using the mean value theorem on the cosine, Lemma C.4 and (C.6), we get for |h| < |r|

e|2r|ασα

2 e−21−ασα

1 |t|α (cid:12)

(cid:12)ts1 + (r + h)s2

(cid:12)
(cid:12)

α−1|ts0

1 + rs0

2|α−2s2

2|s1|−1|s0

2|2

1
hs0
2
s0
1

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

×

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

hs0
2
s0
1

Z

(cid:18)(cid:16)

t −

x + a

S2

(cid:17)

hs0
2
s0
1

s1 + (r + h)s2

(cid:19)<α>

(cid:18)

−

ts1 + (r + h)s2

(cid:19)<α>

(cid:12)
(cid:12)
(cid:12)
Γ(ds)
(cid:12)
(cid:12)

e|2r|ασα

2 e−21−ασα

1 |t|α(cid:12)

(cid:12)ts1 + (r + h)s2

(cid:12)
(cid:12)

α−1|ts0

1 + rs0

2|α−2s2

2|s1|−1|s0

2|2

≤

1
hs0
2
s0
1

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

"

(cid:12)
(cid:12)
(cid:12)

×

x

(cid:12)
(cid:12)
(cid:12) +

hs0
2
s0
1
1 |t|α(cid:12)
(cid:12)
(cid:12)t +

(cid:12)
(cid:12)
(cid:12)a

hs0
2
s0
1
rs0
2
s0
1

≤ e|2r|ασα

2 e−21−ασα

α−2
(cid:12)
(cid:12)
(cid:12)

2|s1|−1s0
s2
2

2|s0

1|α−2

|s1||ts1 + (r + h)s2|α−1Γ(ds)

#

(cid:12)
(cid:12)
(cid:12)

Z

S2

× (cid:0)|t|α−1 + |2r|α−1(cid:1)

"
|x| + |a|Γ(S2)(|t|α−1 + |2r|α−1)

#
.

The last bound can be shown to be integrable with respect to t using Lemma C.7 with η = α − 2,

35

 
 
 
 
b = 0, α − 1, 2(α − 1), p = 0 and (2.2) with ν > 4 − α. We established that we can invert the derivation

and integration signs in all the Ki’s, hence in J 0.

C.1.6 Lemmas for justifying the inversions in the proof of Lemma B.1

The following elementary lemmas, stated without proof, are used to establish Lemma B.1.

Lemma C.1 For x, y ∈ R,

|e−x − e−y| ≤ e− min(x,y)|x − y|,

|e−x − e−y| ≤ e−ye|x−y||x − y|.

(C.18)

(C.19)

Lemma C.2 For α > 1 and x, y ∈ R,

max

(cid:16)

21−α|x|α − |y|α, 21−α|y|α − |x|α(cid:17)

≤ |x + y|α ≤ 2α−1(cid:16)

|x|α + |y|α(cid:17)

.

Lemma C.3 For z ∈ R and 0 < b ≤ 1,

(ι)

(ιι)

(cid:12)
(cid:12)
(cid:12)|1 + z|b − 1
(cid:12)
(cid:12)
(cid:12) ≤ |z|,
(cid:12)
(cid:12)
(cid:12)|1 + z|<b> − 1
(cid:12)
(cid:12)
(cid:12) ≤ 2|z|.

Lemma C.4 (Lemma 3.3, Cioszek-Georges and Taqqu (1998)) For α > 1 and t, r ∈ R,

n

−

exp

Z

S2

|ts1 + rs2|αΓ(ds)

o

≤ exp{|r|ασα

2 } exp{−21−ασα

1 |t|α}.

Lemma C.5 (Lemma 3.1, Cioszek-Georges and Taqqu (1998)) The following inequality holds

for c > 0, 0 < α < 2, −1 < η < 0 and −1 − η < b:

Z

R

exp(−c|t|α)

(cid:12)
(cid:12)

(cid:12)|t + z|η − |t|η(cid:12)

(cid:12)|t|bdt ≤ const. |z|p
(cid:12)

with

and

0 ≤ p < b + η + 1

for − 1 − η < b < 0,

0 ≤ p < η + 1

or

b ≤ p < b + η + η + 1, p ≤ 1

for

0 ≤ b.

const. depends only on c, α, η, b and p.

36

Lemma C.6 (Corollary 3.1, Cioszek-Georges and Taqqu (1998)) The following inequality holds

for c > 0, 0 < α < 2, −1/2 < η < 0 and 0 ≤ p < 2η + 1:

Z

R

exp(−c|t|α)

(cid:12)
(cid:12)

(cid:12)|t + z1|η|t + z3|η − |t + z2|η|t + z4|η(cid:12)

(cid:12)dt ≤ const. (|z1 − z2|p + |z3 − z4|p),
(cid:12)

where const depends only on c, α, η and p.

Lemma C.7 (Lemma 3.12, Cioszek-Georges and Taqqu (1998)) The following inequality holds

for c > 0, 0 < α < 2, −1 < η < 0, b ≥ 0 and 0 ≤ p < η + 1:

Z

R

(cid:12)
exp(−c|t|α)
(cid:12)

(cid:12)|t + z1|η − |t + z2|η(cid:12)

(cid:12)|t|bdt ≤ const. |z1 − z2|p,
(cid:12)

where const depends only on c, α, η, b and p.

C.2 Computation of the derivatives

We detail the computation of the second order derivative highlighting where appropriate integration by

parts intervenes. The computations are similar for the third and fourth order derivatives.

Note that if f (x) = |x|b, for x, b ∈ R, b 6= 0, then for x 6= 0, f 0(x) = bx<b−1> and if f : x 7−→ x<b>,

then f 0(x) = b|x|b−1. This can be shown by distinguishing the cases x > 0 and x < 0.

φ(2)
X2|x(r) =

∂
∂r

φ(1)
X2|x(r)

=

−α
2πfX1(x)

lim
h→0

1
h

" Z

Z

R

S2

e−itxϕX (t, r + h)g2(ts1 + (r + h)s2)s2Γ(ds)dt

−

Z

Z

R

S2

#
e−itxϕX (t, r)g2(ts1 + rs2)s2Γ(ds)dt

(cid:20)
ϕX (t, r + h) − ϕX (t, r)

(cid:21)
g2(ts1 + (r + h)s2)s2Γ(ds)dt

e−itxϕX (t, r)

(cid:20)
g2(ts1 + (r + h)s2) − g2(ts1 + rs2)

(cid:21)
s2Γ(ds)dt

Z

S2

=

−α
2πfX1(x)

lim
h→0

1
h

Z

Z

R

+

−α
2πfX1(x)

lim
h→0

:= A1 + A2.

e−itx

S2
Z
1
h

R

The ﬁrst limit can be straightforwardly obtained:

e−itxϕX (t, r)

(cid:18) Z

S2

g2(ts1 + rs2)s2Γ(ds)

(cid:19)2

dt

A1 =

=

α2
2πfX1(x)
α2
2πfX1(x)

Z

Λ

R
(cid:18) Z

S2

(cid:19)2

.

g2s2

37

The second one requires appropriate integration by parts. With the change of variable t0 = t +

hs2
s1

,

A2 =

−α
2πfX1(x)

lim
h→0

1
h

(cid:20) Z

Z

S2

R

e−itxϕX (t, r)g2(ts1 + (r + h)s2)s2dtΓ(ds)

−

Z

Z

S2

R

(cid:21)
e−itxϕX (t, r)g2(ts1 + rs2)s2dtΓ(ds)

=

−α
2πfX1(x)

lim
h→0

1
h

(cid:20) Z

Z

S2

R

(cid:16)

−i

t−

(cid:17)

x

hs2
s1

e

(cid:16)

t −

ϕX

(cid:17)

, r

hs2
s1

g2(ts1 + rs2)s2dtΓ(ds)

−

Z

Z

S2

R

e−itxϕX (t, r)g2(ts1 + rs2)s2dtΓ(ds)

(cid:21)

=

α
2πfX1(x)

Z

Z

S2

R

2s−1
s2

1 g2(ts1 + rs2) lim
h→0

1
− hs2
s1

(cid:16)

t−

(cid:17)

x

hs2
s1

(cid:20)
−i
e

(cid:16)

t −

ϕX

(cid:17)

, r

hs2
s1

(cid:21)
− e−itxϕX (t, r)
dtΓ(ds)

α
2πfX1(x)
−iαx
2πfX1(x)

Z

−

R
α2
2πfX1(x)
(cid:18) Z

=

=

A2 =

Z

Z

S2

R

(cid:20)
1 g2(ts1 + rs2)

2s−1
s2

− ixe−itxϕX (t, r) + e−itx ∂
∂t

(cid:21)
dtΓ(ds)
ϕX (t, r)

e−itxϕX (t, r)

(cid:18) Z

2s−1
s2

1 g2(ts1 + rs2)Γ(ds)

(cid:19)

dt

S2

Z

R

e−itxϕX (t, r)

(cid:18) Z

S2

s1g2(ts1 + rs2)Γ(ds)

(cid:19)(cid:18) Z

2s−1
s2

1 g2(ts1 + rs2)Γ(ds)

(cid:19)

dt

S2

−iαx
2πfX1(x)

Λ

g2s2

2s−1
1

(cid:19)

−

α2
2πfX1(x)

(cid:18) Z

Λ

S2

g2s2

2s−1
1

(cid:19)(cid:18) Z

S2

(cid:19)

g2s1

S2

Combining the expressions obtained for A1 and A2 yields the second derivative.

D Proof of Theorem 2.2

We here ﬁnally evaluate the derivatives of Lemma B.1 at r = 0 to obtain the functional forms of the

conditinal moments. These proofs yield in particular the expressions of the constants θi, i = 1, . . . , 6

which intervene in Theorem 2.2. Lemmas at the end of this section are used to regroup terms and simplify

as much as possible the functional forms.

D.1 Proof of second order conditional moment (2.6) in Theorem 2.2

The second order derivative of the characteristic function of X2|X1 = x is given by (B.8) in Lemma B.1.

Evaluating it at r = 0 yields

Eh

X 2
2

(cid:12)
(cid:12)
(cid:12)X1 = x

i

38

= −φ(2)
X2|x(0)
α
2πfX1(x)

=

Z

R

e−itx+iaσα

1 β1t<α>

e−σα

1 |t|α

"

×

ixσα

1 (κ2t<α−1> − iaλ2|t|α−1) − ασ2α

1 (κ1t<α−1> − iaλ1|t|α−1)2

#
1 (κ2t<α−1> − iaλ2|t|α−1)(t<α−1> − iaβ1|t|α−1)

+ ασ2α

dt

=

ασα
1
2πfX1(x)

Z

R

e−itx+iaσα

1 β1t<α>

e−σα

1 |t|α

"

×

xaλ2|t|α−1 + ασα

1 |t|2(α−1)

(cid:18)

κ2 − a2β1λ2 − κ2

1 + a2λ2
1

(cid:19)

+ ixκ2t<α−1> + iασα

1 t<2(α−1)>

(cid:18)

2aλ1κ1 − a(λ2 + β1κ2

(cid:19)#

dt

"

=

ασα
1
πfX1(x)

axλ2C1(x) + κ2xS1(x)

(cid:16)

− ασα
1

1 − a2λ2
κ2

1 + a2β1λ2 − κ2

(cid:17)

C2(x) − ασα
1

(cid:16)

a(λ2 + β1κ2) − 2aλ1κ1

(cid:17)

#
S2(x)

,

where the κi’s and λi’s are given in (2.3). Invoking Lemma D.1 (ιιι) yields

Eh

X 2
2

(cid:12)
(cid:12)
(cid:12)X1 = x

i

=

"
(a2λ2β1 + κ2)x + a(λ2 − κ2β1)

#

1 − xH(x)
πfX1(x)

x
1 + (aβ1)2
α2σ2α
1
πfX1(x)

−

(cid:16)

H

2(α − 1), θ1; x

(cid:17)

= κ2x2 +

ax(λ2 − β1κ2)
1 + (aβ1)2

"

aβ1x +

1 − xH(x)
πfX1(x)

#

−

α2σ2α
1
πfX1(x)

(cid:16)
2(α − 1), θ1; x

(cid:17)

,

H

where H is given in (B.3) with

θ11 = κ2

1 − a2λ2

1 + a2β1λ2 − κ2,

θ12 = a(λ2 + β1κ2) − 2aλ1κ1.

D.2 Proof of third order conditional moment (2.7) in Theorem 2.2

The third order derivative of the characteristic function of X2|X1 = x is given by (B.9) in Lemma B.1.

It can be shown that the I’s evaluated at r = 0 write

I1 = 2σα

1 H

(cid:16)

α − 2, θI

1; x

(cid:17)

,

I2 = 2σ2α

1 H

iI3 = 2σ3α

1 H

(cid:16)

(cid:16)

2(α − 1), θI

2; x

3(α − 1), θI

3; x

(cid:17)

(cid:17)

,

,

θI
1 =

θI
2 =

θI
3 =

39

(cid:18)

κ3, −aλ3

(cid:19)
,

(cid:18)

L, −aK

(cid:19)
,

(cid:18)

aλ1(3κ2

1 − a2λ2

1), κ3

1 − 3a2κ1λ2
1

(cid:19)

,

iI4 = 2σ3α

1 H

iI5 = iI7 = 2σ2α

1 H

iI6 = 2σ2α

1 H

(cid:16)

(cid:16)

(cid:16)

3(α − 1), θI

4; x

(cid:17)

,

2α − 3, θI

5; x

2α − 3, θI

6; x

(cid:17)

(cid:17)

,

,

(cid:16)

(cid:18)

a

θI
4 =

K + β1L

(cid:17)
, L − a2β1K

(cid:19)

,

θI
5 =

θI
6 =

(cid:18)

(cid:19)
,

aK, L

(cid:18)

a(λ3 + β1κ3), κ3 − a2β1λ3

(cid:19)

,

with K = κ1λ2 + λ1κ2 and L = κ1κ2 − a2λ1λ2. Hence,

Eh

X 3
2

(cid:12)
(cid:12)
(cid:12)X1 = x

i

= −iφ(3)

X2|x(0) =

"

(cid:16)

− x

α
πfX1(x)

(α − 1)K1 − αK2

(cid:17)

+ α2K3 + α(α − 1)K4

#
,

with

K1 = σα

1 H

(cid:16)

α − 2, θK

1 ; x

(cid:17)

,

K2 = σ2α

1 H

K3 = σ3α

1 H

K4 = σ2α

1 H

(cid:16)

(cid:16)

(cid:16)

2(α − 1), θK

2 ; x

3(α − 1), θK

3 ; x
(cid:17)

2α − 3, θK

4 ; x

,

(cid:17)

(cid:17)

,

,

with θK

1 = θI
1,

with θK

2 = θI
2,

with θK

3 = θI

3 − θI
4

with θK

4 = θI

6 − θI
5.

Invoking Lemma D.1 (ιι) for n = 1, 2 and regrouping the terms, we get

Eh

X 3
2

(cid:12)
(cid:12)
(cid:12)X1 = x

i

=

12C1(x) − θK
θK

11S1(x)

(cid:19)

(cid:18)

αx2σα
1
πfX1(x)
α
πfX1(x)

+

"

αxσ2α
1
2

(cid:18)

(cid:16)

− 2

C2(x)

11 + aβ1θK
θK
12

(cid:17)

+ 2θK

21 − θK
42

(cid:19)

+

+

+

αxσ2α
1
2
α2σ3α
1
2
α2σ3α
1
2

(cid:18)

(cid:16)

− 2

S2(x)

12 − aβ1θK
θK
11

(cid:17)

+ 2θK

22 + θK
41

(cid:19)

(cid:18)

C3(x)

2θK

31 + θK

41 + aβ1θK
42

(cid:19)

(cid:18)

S3(x)

2θK

32 + θK

42 − aβ1θK
41

(cid:19)#

.

Using Lemma D.1 (ιιι) yields the conclusion with θ2 = (θ21, θ22), θ3 = (θ31, θ32) such that

θ21 = 3(L + a2β1λ3 − κ3),

θ22 = 3a(λ3 + β1κ3 − K),

(cid:16)
θ31 = a

λ3(1 − a2β2

1) + 2β1κ3 + 2λ1(3κ2

1 − a2λ2

(cid:17)
1) − 3(K + β1L)

,

θ32 = κ3(1 − a2β2

1) − 2a2β1λ3 + 2(κ3

1 − 3a2κ1λ2

1) + 3(a2β1K − L),

with K = κ1λ2 + κ2λ1, L = κ1κ2 − a2λ1λ2.

40

(D.1)

(D.2)

(D.3)

(D.4)

D.3 Proof of fourth order conditional moment (2.8) in Theorem 2.2

The conditional moments are obtained by evaluating the derivatives of the conditional characteristic

function at r = 0. We provide here the proof for the fourth order, which yields the expressions of the

vectors θ4, θ5 and θ6 appearing in Equation (2.8) of Theorem 2.2. The fourth order derivative of the

characteristic function of X2|X1 = x is given by (B.10) in Lemma B.1. It can be shown that the J’s

evaluated at r = 0 write

iJ1 = 2σ3α

1 H

iJ2 = 2σ3α

1 H

iJ3 = 2σ2α

1 H

iJ4 = iJ5 = 2σ2α

1 H

(cid:16)

(cid:16)

(cid:16)

(cid:16)

3(α − 1), θJ

1 ; x

3(α − 1), θJ

(cid:17)

(cid:17)

,

,

2 ; x
(cid:17)

2α − 3, θJ

3 ; x

,

2α − 3, θJ

4 ; x

(cid:17)

,

(cid:16)

1 H
(cid:16)

J6 = 2σ2α

2(α − 1), θJ

J7 = 2σα

1 H

α − 2, θJ

7 ; x

(cid:17)

,

6 ; x
(cid:17)

,

J8 = J9 = J12 = 2σ3α

1 H

J10 = 2σ3α

1 H

(cid:16)

(cid:16)

3α − 4, θJ

8 ; x

(cid:17)

,

3α − 4, θJ

10; x

(cid:17)

,

where θJ

i = (θJ

i1, θJ

i2), for i = 1, . . . , 19,

J11 = J13 = 2σ3α

1 H

J14 = 2σ2α

1 H

J15 = 2σ2α

1 H

J16 = 2σ2α

1 H

J17 = 2σ4α

1 H

J18 = 2σ4α

1 H

J19 = 2σ4α

1 H

(cid:16)

(cid:16)

(cid:16)

(cid:16)

(cid:16)

(cid:16)

(cid:16)

3α − 4, θJ

11; x

2α − 4, θJ

14; x

2α − 4, θJ

15; x

2α − 4, θJ

16; x

(cid:17)

(cid:17)

(cid:17)

(cid:17)

,

,

,

,

4(α − 1), θJ

17; x

4(α − 1), θJ

18; x

4(α − 1), θJ

19; x

(cid:17)

(cid:17)

(cid:17)

,

,

,

(cid:16)
θJ
11 = a
(cid:16)
θJ
21 = a
(cid:16)
θJ
31 = a

1) + 2κ1κ2λ1

λ2(κ2

1 − a2λ2
(cid:17)
K + β1L

,

β1κ4 + λ4

(cid:17)

,

θJ
41 = aK,

θJ
61 = L,

θJ
71 = κ4,

(cid:17)
,

12 = κ2(κ2
θJ

1 − a2λ2

1) − 2a2κ1λ1λ2,

22 = L − a2β1K,
θJ

32 = κ4 − a2β1λ4,
θJ

θJ
42 = L,

θJ
62 = −aK,

θJ
72 = −aλ4,

81 = L − a2β1K,
θJ

101 = κ4(1 − a2β2
θJ

1) − 2a2β1λ4,

(cid:16)
θJ
82 = −a
(cid:16)
θJ
102 = −a

(cid:17)
K + β1L

,

λ4(1 − a2β2

1) + 2β1κ4

(cid:17)

,

111 = θJ
θJ
12,

θJ
141 = L,

151 = κ2
θJ

2 − a2λ2
2,

161 = κ4 − a2β1λ4,
θJ

112 = −θJ
θJ
11,

θJ
142 = −aK,

θJ
152 = −2aκ2λ2,

(cid:16)
θJ
162 = −a

λ4 + β1κ4

(cid:17)

,

41

171 = θJ
θJ

12 − aβ1θJ
11,

172 = −θJ
θJ

11 + aθJ
12,

181 = κ4
θJ

1 − 6a2κ2

1λ2

1 + a4λ4
1,

191 = L(1 − a2β2
θJ

1) − 2a2β1K,

and K = κ1λ3 + λ1κ3, L = κ1κ3 − a2λ1λ3. Hence,

182 = −4aκ1λ1(κ2
θJ
(cid:16)
θ192 = −a

K(1 − a2β2

1 − a2λ2

1),

1) + 2β1L

(cid:17)

,

X2|x(0)

= φ(4)
"

(cid:16)

Eh

X 4
2

(cid:12)
(cid:12)
(cid:12)X1 = x

i

=

−α
πfX1(x)

where

αx

αK1 + (α − 1)K2

(cid:17)

+ αx2K6 − (α − 1)x2K7 + α2(α − 1)K3 + α(α − 1)2K4 + α3K5

,

#

3(α − 1), θK

(cid:17)

,

with θK

1 = 3θJ

1 − 2θJ
2 ,

1 ; x
(cid:17)

2α − 3, θK

2 ; x

,

with θK

2 = 2(θJ

3 − θJ

4 ),

3α − 4, θK

3 ; x

2α − 4, θK

4 ; x

(cid:17)

(cid:17)

,

,

with θK

3 = θJ

10 − 3θJ

11 − θJ
8 ,

with θK

4 = 4θJ

14 − 3θJ

15 − θJ
16,

K1 = σ3α

1 H

K2 = σ2α

1 H

K3 = σ3α

1 H

K4 = σ2α

1 H

K5 = σ4α

1 H

(cid:16)

(cid:16)

(cid:16)

(cid:16)

(cid:16)

4(α − 1), θK

5 ; x

(cid:16)

1 H
(cid:16)

K6 = σ2α

2(α − 1), θK

K7 = σα

1 H

α − 2, θK

7 ; x

,

(cid:17)

(cid:17)

,

,

with θK

5 = 3θJ

17 − θJ

18 − θJ
19,

with θK

6 = θJ
6 ,

with θK

7 = θJ
7 .

6 ; x
(cid:17)

Invoking Lemmas D.1 (ιι) for n = 1, 2, 3 and D.2, we get

Eh

X 4
2

(cid:12)
(cid:12)
(cid:12)X1 = x

i

=

−α
πfX1(x)

"

(cid:16)

x3σα
1

72C1(x) − θK
θK

71S1(x)

(cid:17)

+

+

+

+

+

+

αx2σ2α
1
2
αx2σ2α
1
2

(cid:18)

C2(x)

− θK

22 + 2θK

61 − 2

(cid:16)
71 + aβ1θK
θK
72

(cid:17)

−

α − 1
2α − 3

(cid:18)

S2(x)

21 + 2θK
θK

62 − 2

(cid:16)

72 − aβ1θK
θK
71

(cid:17)

−

α − 1
2α − 3

θK
42

(cid:19)

θK
41

(cid:19)#

α2xσ3α
1
6
α2xσ3α
1
6
α3σ4α
1
3
α3σ4α
1
3

C3(x)

(cid:18)

6θK

11 + 3

(cid:16)

21 + aβ1θK
θK
22

(cid:17)

− 2θK

32 + 5

(cid:18)

S3(x)

(cid:16)

6θK

12 + 3

22 − aβ1θK
θK
21

(cid:17)

+ 2θK

31 + 5

α − 1
2α − 3
α − 1
2α − 3

(cid:16)

aβ1θK

41 − θK
42

(cid:17)(cid:19)

(cid:16)

41 + aβ1θK
θK
42

(cid:17)(cid:19)

C4(x)

(cid:18)

31 + aβ1θK
θK

32 +

(cid:16)

α − 1
2α − 3

41(1 − a2β2
θK

1) + 2aβ1θK
42

(cid:19)

(cid:17)

+ 3θK
51

(cid:18)

S4(x)

32 − aβ1θK
θK

31 +

(cid:16)

α − 1
2α − 3

42(1 − a2β2
θK

1) − 2aβ1θK
41

(cid:17)

+ 3θK
52

(cid:19)#

.

42

Using Lemma D.1 (ιιι) yields the conclusion. The coeﬃcients θ’s in the expression (2.8) are deduced

from the θK’s and θJ ’s as follows:

θ41 = −θK

22 + 2θK

61 − 2

(cid:16)

θ42 = θK

(cid:16)
62 − 2

21 + 2θK
(cid:16)

71 + aβ1θK
θK
72
(cid:17)

72 − aβ1θK
θK
71
(cid:17)

θ51 = 6θK

11 + 3

21 + aβ1θK
θK
22

− 2θK

32 + 5

(cid:17)

−

θK
41,

α − 1
2α − 3
θK
42,

α − 1
2α − 3

−

θ52 = 6θK

12 + 3

(cid:16)

θ61 = θK

31 + aβ1θK

θ62 = θK

32 − aβ1θK

(cid:17)

(cid:16)

32 +

22 − aβ1θK
θK
21
α − 1
2α − 3
α − 1
2α − 3

31 +

aβ1θK

41 − θK
42

(cid:16)

α − 1
2α − 3
α − 1
2α − 3
1) + 2aβ1θK
42

(cid:16)

41 + aβ1θK
θK
42
(cid:17)

+ 3θK
51,

(cid:17)

(cid:17)

,

,

+ 2θK

31 + 5

41(1 − a2β2
θK

(cid:16)

42(1 − a2β2
θK

1) − 2aβ1θK
41

(cid:17)

+ 3θK
52.

(D.5)

(D.6)

(D.7)

(D.8)

(D.9)

(D.10)

D.4 Lemmas for the proof of Theorem 2.2

The following elementary Lemmas, stated without proof, are used to establish Theorem 2.2.

Lemma D.1 Let α ∈ (1, 2), b > 0, c ∈ R. Deﬁne for n ≥ 1 and x ∈ R

Cn(x) =

Sn(x) =

Z +∞

0
Z +∞

0

e−btα

tn(α−1) cos(tx − ctα)dt,

e−btα

tn(α−1) sin(tx − ctα)dt,

Fn(x) =

Gn(x) =

Z +∞

0
Z +∞

0

e−btα

tn(α−1)−1 cos(tx − ctα)dt,

e−btα

tn(α−1)−1 sin(tx − ctα)dt.

ι) Then the following hold for any n ≥ 1 and x ∈ R

Fn(x) =

(cid:16)

α

bCn+1(x) − cSn+1(x)

(cid:17)

+ xSn(x)

n(α − 1)

,

Gn(x) =

(cid:16)

(cid:17)
cCn+1(x) + bSn+1(x)

α

− xCn(x)

n(α − 1)

.

ιι) For any n ≥ 1, θ1, θ2 ∈ R and x ∈ R:

θ1Fn(x) + θ2Gn(x) =

h

(cid:16)
Cn+1(x)

α

bθ1 + cθ2

(cid:17)

+ Sn+1(x)

(cid:16)

bθ2 − cθ1

(cid:17)i

+ x

h

i
− θ2Cn(x) + θ1Sn(x)

n(α − 1)

.

ιιι) We have for x ∈ R, b = σα

1 and c = aβ1σα
1 :

C1(x) =

aβ1xπfX1(x) + 1 − xH(x)
1 (1 + (aβ1)2)

ασα

,

S1(x) =

xπfX1(x) − aβ1(1 − xH(x))
1 (1 + (aβ1)2)

ασα

.

Lemma D.2 Let α ∈ (3/2, 2), b > 0, c ∈ R. Deﬁne for x ∈ R

hc(x) =

Z +∞

0

e−btα

t2α−4 cos(tx − ctα)dt,

hs(x) =

Z +∞

0

e−btα

t2α−4 sin(tx − ctα)dt.

Then for any θ1, θ2 ∈ R and x ∈ R,

θ1hc(x) + θ2hs(x) =

α2
3(2α − 3)(α − 1)

(cid:20)
(cid:16)
C4(x)

θ1(b2 − c2) + 2bcθ2

(cid:17)

(cid:16)
+ S4(x)

θ2(b2 − c2) − 2bcθ1

(cid:17)(cid:21)

43

+

−

5αx
6(2α − 3)(α − 1)
x2
2(2α − 3)(α − 1)

(cid:20)
(cid:16)
C3(x)

cθ1 − bθ2

(cid:17)

(cid:16)
+ S3(x)

bθ1 + cθ2

(cid:17)(cid:21)

(cid:21)
(cid:20)
.
θ1C2(x) + θ2S2(x)

E Proof of Theorem 2.4

Let X = (X1, X2) be an α-stable vector with α = 1 and spectral representation (Γ, 0). Its characteristic
function, denoted ϕX (t, r) for any (t, r) ∈ R2, reads

ϕX (t, r) = exp

Z

(cid:26)

−

S2

|ts1 + rs2| + ia(ts1 + rs2) ln |ts1 + rs2|Γ(ds)

(cid:27)

,

(E.1)

with a = 2/π. The conditional characteristic function of X2 given X1 = x, denoted φX2|x(r) for r ∈ R,
is still given by (B.2).

Lemma E.1 Let (X1, X2) be an α-stable random vector with α = 1 and spectral representation (Γ, 0).

If (2.2) holds with ν > 0, the ﬁrst derivative of φX2|x is given by

φ(1)
X2|x(r) =

(cid:16)

−1
2πfX1(x)

A1 + iaA2

(cid:17)

,

with

A1 =

A2 =

Z

R

Z

R

e−itxϕX (t, r)

e−itxϕX (t, r)

(cid:18) Z

(cid:18) Z

S2

S2

s2(ts1 + rs2)<0>Γ(ds)

(cid:19)

dt,

s2(1 + ln |ts1 + rs2|)Γ(ds)

(cid:19)

dt

If (2.2) holds with ν > 1, the second derivative of φX2|x is given by

φ(2)
X2|x(r) =

(cid:16)

−1
2πfX1(x)

− B1 + ixB2 + B3

(cid:17)

,

where,

B1 =

B2 =

B3 =

Z

R

Z

R

Z

R

e−itxϕX (t, r)

e−itxϕX (t, r)

e−itxϕX (t, r)

(cid:18) Z

(cid:18) Z

(cid:18) Z

S2

S2

S2

s2(ts1 + rs2)<0> + ias2(1 + ln |ts1 + rs2|Γ(ds)

(cid:19)2

dt,

(cid:16)

(cid:17)
(ts1 + rs2)<0> + ia(1 + ln |ts1 + rs2|

(cid:19)

2s−1
s2

1 Γ(ds)

dt,

s1(ts1 + rs2)<0> + ias1(1 + ln |ts1 + rs2|Γ(ds)

(cid:19)

(cid:18) Z

×

S2

(cid:16)

(cid:17)
(ts1 + rs2)<0> + ia(1 + ln |ts1 + rs2|

44

(cid:19)

2s−1
s2

1 Γ(ds)

(E.2)

(E.3)

(E.4)

dt.

E.1 Justifying inversion of integral and derivative signs

First derivative

The terms depending on r in the right-hand side of (E.1) are of the form (omitting the factor

1/2πfX1(x))

− R
e

S2

Z

R

|ts1+rs2|Γ(ds)

(cid:18)

trig

− tx − a

Z

S2

(ts1 + rs2) ln |ts1 + rs2|Γ(ds)

(cid:19)

dt.

Consider for instance the term obtained by replacing trig by the cosine function, denoted I1.

I 0
1(r) = lim
h→0

"
e

− R

S2

1
h

Z

R

|ts1+(r+h)s2|Γ(ds)

− R

− e

S2

|ts1+rs2|Γ(ds)

#

(cid:18)

tx + a

× cos

Z

S2

(ts1 + (r + h)s2) ln |ts1 + (r + h)s2|Γ(ds)

(cid:19)

dt

+ lim
h→0

− R
e

S2

1
h

Z

R

|ts1+rs2|Γ(ds)

"

cos

(cid:18)

tx + a

Z

S2

(ts1 + (r + h)s2) ln |ts1 + (r + h)s2|Γ(ds)

(cid:19)

(cid:18)

tx + a

− cos

Z

S2

(ts1 + rs2) ln |ts1 + rs2|Γ(ds)

(cid:19)#

dt

:= I11 + I12

The integrand of I11 converges to

− R

S2

−e

|ts1+rs2|Γ(ds)

(cid:18)

cos

tx + a

Z

S2

(ts1 + rs2) ln |ts1 + rs2|Γ(ds)

(cid:19) Z

S2

s2(ts1 + rs2)<0>Γ(ds).

Using (C.19) we can bound the integrand of I11 by

1
|h|

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Z

S2

(cid:12)
(cid:12)
(cid:12)
|ts1 + (r + h)s2| − |ts1 + rs2|Γ(ds)
(cid:12)
(cid:12)

e

− R

S2

|ts1+rs2|Γ(ds)

(cid:12)
(cid:12) R

e

S2

(cid:12)
(cid:12).
|ts1+(r+h)s2|−|ts1+rs2|Γ(ds)

By Lemma C.3 (ι) and the triangle inequality, we can further bound it for |h| < |r| by

σ2eσ2(1+|r|)−σ1|t|,

which does not depend on h and is integrable with respect to t on R. The dominated convergence theorem

applies to I11. Turning to I12, its integrand converges to

− R

S2

−ae

|ts1+rs2|Γ(ds)

(cid:18)

tx + a

sin

Z

S2

(ts1 + rs2) ln |ts1 + rs2|Γ(ds)

(cid:19) Z

S2

s2(1 + ln |ts1 + rs2|)Γ(ds).

Using the mean value theorem on the cosine, its integrand can be bounded by

− R

S2

a
|h|

e

|ts1+rs2|Γ(ds)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Z

S2

(cid:12)
(cid:12)
(cid:12)
(ts1 + (r + h)s2) ln |ts1 + (r + h)s2| − (ts1 + rs2) ln |ts1 + rs2|Γ(ds)
(cid:12)
(cid:12)

45

≤ aeσ2|r|−σ1|t| 1
|h|
:= aeσ2|r|−σ1|t|(cid:16)

Z

S2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(ts1 + (r + h)s2) ln |ts1 + (r + h)s2| − (ts1 + rs2) ln |ts1 + rs2|
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Γ(ds)

Q1 + Q2

(cid:17)

,

(E.5)

where the two terms Q1 and Q2 involve integrals over S2 ∩{s : |ts1 +rs2| ≥ 2|h|} and S2 ∩{s : |ts1 +rs2| <
2|h|}. Focus on Q2. Introduce the function f : R+ → R+ deﬁned for any z ≥ 0 by f (z) = z| ln z|. It is such
that f (0) = 0 and for z small enough (0 < z < e−1), f is monotone increasing. Since |ts1 + rs2| < 2|h|,
we also have |ts1 + (r + h)s2| < 3|h|. Thus, for 0 < |h| < (3e)−1, the integrand of Q2 can be bounded by

|h|−1

(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) +
(cid:12)f (|3h|)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)f (|2h|)
(cid:12)

(cid:19)

≤ 2|h|−1(cid:12)
(cid:12)
(cid:12)f (|3h|)

(cid:12)
(cid:12)
(cid:12) ≤ 6

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)ln|3h|
(cid:12)

Using Lemma E.2, we can bound the later quantity for any v > 0 by

6v−1(cid:16)

2 + |3h|v + |3h|−v(cid:17)

.

From |ts1 + rs2|/2 < |h| < (3e)−1, we deduce that |3h|−v <

(cid:16)

3|ts1 + rs2|/2

(cid:17)−v

and

6v−1(cid:16)

2 + |3h|v + |3h|−v(cid:17)

≤ 6v−1(cid:16)

2 + e−v +

(cid:16)

3|ts1 + rs2|/2

(cid:17)−v(cid:17)

≤ const1 + const2|ts1 + rs2|−v,

for some nonnegative constants const1 and const2. Hence, the term involving Q2 in E.5 can be further

bounded for any v > 0 by

aeσ2|r|−σ1|t|(cid:16)

const1 + const2

Z

S2

(cid:12)
(cid:12)
(cid:12)t +

rs2
s1

(cid:12)
(cid:12)
(cid:12)

−v

(cid:17)
|s1|−vΓ(ds)

.

(E.6)

The term with const1 is clearly integrable with respect to t on R. Letting (2.2) hold with ν > 0, choose

some v ∈ (0, min(ν, 1)). We show that the second term is bounded by an integrable function of t as we
did in Equation (C.4) using Lemma C.5 with η = v, b = 0, p = 0, the fact that R

R e−σ1|t||t|−vdt < +∞

and (2.2) with ν > v > 0. There remains to be bounded the part involving Q1 in (E.5). For this term,

we apply the mean value theorem to the function z 7−→ z ln |z| and get that

|h|−1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(ts1 + (r + h)s2) ln |ts1 + (r + h)s2| − (ts1 + rs2) ln |ts1 + rs2|

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
≤ |h|−1|hs2|
(cid:12)
(cid:12)
(cid:12)1 + ln |u|
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) ln |u|
(cid:12),

≤ 1 +

for some u ∈ [ts1 + (r + h)s2 ∧ ts1 + rs2, ts1 + (r + h)s2 ∨ ts1 + rs2]. Since Q1 is an integral over

S2 ∩ {s : |ts1 + rs2| ≥ 2|h|}, we have |u| ∈

h |ts1+rs2|
2

i
, and because of the quasi-convexity of
, 2|ts1 + rs2|

the function z 7−→

(cid:12)
(cid:12)
(cid:12) ln |z|
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1 +

(cid:12)
(cid:12)
(cid:12), we can bound the above term by
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
ln |2(ts1 + rs2)|
(cid:12)
(cid:12)

ts1 + rs2
2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

ln

≤ const + 2

(cid:12)
(cid:12)
(cid:12) ln |ts1 + rs2|

(cid:12)
(cid:12)
(cid:12).

46

Using Lemma E.2, we can bound this term for any v > 0 by

const + 2v−1(cid:16)

2 + |ts1 + rs2|v + |ts1 + rs2|−v(cid:17)

≤ const1 + const2|t|v + const3

−v

(cid:12)
(cid:12)
(cid:12)t +

rs2
s1

(cid:12)
(cid:12)
(cid:12)

|s1|−v

Hence, the term in (E.5) involving Q1 can be bounded for any v > 0 by

aeσ2|r|−σ1|t|(cid:16)

const1 + const2|t|v + const3

Z

S2

(cid:12)
(cid:12)
(cid:12)t +

rs2
s1

(cid:12)
(cid:12)
(cid:12)

−v

(cid:17)
|s1|−vΓ(ds)

.

(E.7)

which can be shown to be integrable with respect to t on R as we did above for the term with Q2. The

dominated convergence theorem applies to I12 and thus to I1. We can derivate φX2|x under the integral

sign.

Second derivative

Let us start with A2, which is the most delicate. It is composed of terms of the form

− R
e

S2

Z

R

|ts1+rs2|Γ(ds)

(cid:18)

trig

− tx − a

Z

S2

(ts1 + rs2) ln |ts1 + rs2|Γ(ds)

(cid:19)

(cid:18) Z

×

S2

s2(1 + ln |ts1 + rs2|)Γ(ds)

(cid:19)

dt,

where «trig» stands for sine or cosine. Denoting the one with cosine as K2, we have

K2 = lim
h→0

"

Z

R

1
h

− R
e

S2

|ts1+(r+h)s2|Γ(ds)

− R

S2

− e

|ts1+rs2|Γ(ds)

#

(cid:18)

tx + a

× cos

Z

S2

(ts1 + (r + h)s2) ln |ts1 + (r + h)s2|Γ(ds)

(cid:19)

+ lim
h→0

− R

S2

e

1
h

Z

R

|ts1+rs2|Γ(ds)

"

(cid:18)

cos

tx + a

Z

S2

(cid:18) Z

×

S2

s2(1 + ln |ts1 + (r + h)s2|)Γ(ds)

(cid:19)

dt

(ts1 + (r + h)s2) ln |ts1 + (r + h)s2|Γ(ds)

(cid:19)

(cid:18)

tx + a

Z

− cos

(ts1 + rs2) ln |ts1 + rs2|Γ(ds)

(cid:19)#

S2
(cid:18) Z

×

S2

s2(1 + ln |ts1 + (r + h)s2|)Γ(ds)

(cid:19)

dt

+ lim
h→0

− R

S2

e

1
h

Z

R

|ts1+rs2|Γ(ds)

(cid:18)

tx + a

cos

Z

S2

(ts1 + rs2) ln |ts1 + rs2|Γ(ds)

(cid:19)

#

s2 ln |ts1 + (r + h)s2| − s2 ln |ts1 + rs2|Γ(ds)

dt

" Z

×

S2

:= K21 + K22 + K23.

The integrand of K21 converges to

− R

S2

− e

|ts1+rs2|Γ(ds)

(cid:18)

tx + a

cos

Z

S2

(ts1 + rs2) ln |ts1 + rs2|Γ(ds)

(cid:19)

47

(cid:18) Z

×

S2

s2(ts1 + rs2)<0>Γ(ds)

(cid:19)(cid:18) Z

s2(1 + ln |ts1 + rs2|)Γ(ds)

(cid:19)
.

S2

Using (C.19), the triangle inequality and (C.4), it can be bounded by

σ2eσ2(1+|r|)−σ1|t|

Z

S2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)Γ(ds).
(cid:12)1 + ln |ts1 + (r + h)s2|
|s2|

(E.8)

The integrand of the above expression can be bounded using Lemma E.2 for any v > 0 by

1 + v−1(cid:16)

2 + |ts1 + (r + h)s2|v + |ts1 + (r + h)s2|−v(cid:17)

≤ const1 + const2|t|v + const3

(cid:12)
(cid:12)
(cid:12)t +

(r + h)s2
s1

(cid:12)
(cid:12)
(cid:12)

−v

|s1|−v,

hence, (E.8) is bounded by

σ2eσ2(1+|r|)−σ1|t|(cid:16)

const1 + const2|t|v + const3

Z

S2

(cid:12)
(cid:12)
(cid:12)t +

(r + h)s2
s1

(cid:12)
(cid:12)
(cid:12)

−v

|s1|−vΓ(ds)

(cid:17)
.

The terms involving const1 and const2 are clearly integrable with respect to t. The last term is more

intricate as it still depends on h. We will show that the generalised Lebesgue dominated convergence

theorem (Theorem 19, p.89 in Royden and Fitzpatrick (2010)) applies. Denoting

T (h) = e−σ1|t|(cid:12)
(cid:12)
(cid:12)t +

−v

(r + h)s2
s1

(cid:12)
(cid:12)
(cid:12)

|s1|−v,

it can be shown that T (0) is integrable with respect to t on R and Γ on S2 invoking the usual arguments.

Also, choosing some v ∈ (0, 1), with have by Lemma C.7 with η = −v, b = 0 and 0 < p < 1 − v,

Z

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
T (h) − T (0)
(cid:12)
(cid:12)

Z

≤

S2

|s1|−v

Z

R

e−σ1|t|

(cid:12)
(cid:12)
(cid:12)t +

−v

(r + h)s2
s1

(cid:12)
(cid:12)
(cid:12)

−

(cid:12)
(cid:12)
(cid:12)t +

rs2
s1

(cid:12)
(cid:12)
(cid:12)

−v(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

dtΓ(ds)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
hs2
s1

p

(cid:12)
(cid:12)
(cid:12)

Γ(ds)

Z

≤ const

S2
≤ const |h|p

|s1|−v(cid:12)
(cid:12)
(cid:12)
Z

|s1|−v−pΓ(ds) −→
h→0

0,

S2

R T (h) =
because (2.2) holds with ν > 1 and v + p < v + 1 − v < 1. Since T (0) is integrable and limh→0
R T (0), the generalised dominated convergence theorem applies to K21. We turn to K22. Its integrand

converges to

− R

S2

− ae

|ts1+rs2|Γ(ds)

(cid:18)

tx + a

sin

Z

S2

(ts1 + rs2) ln |ts1 + rs2|Γ(ds)

(cid:19)

(cid:18) Z

×

S2

s2(1 + ln |ts1 + rs2|)Γ(ds)

(cid:19)2

.

With the usual inequalities and Lemma E.2, it can be bounded for any v > 0 by

a
|h|

eσ2|r|−σ1|t|

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Z

S2

(cid:12)
(cid:12)
(cid:12)
(ts1 + (r + h)s2) ln |ts1 + (r + h)s2| − (ts1 + rs2) ln |ts1 + rs2|Γ(ds)
(cid:12)
(cid:12)

48

×

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Z

S2

s2(1 + ln |ts1 + (r + h)s2|)Γ(ds)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ aeσ2|r|−σ1|t|(cid:16)

Q1 + Q2

≤ aeσ2|r|−σ1|t|(cid:16)

Q1 + Q2

(cid:17)(cid:16)

(cid:17)(cid:16)

σ2 +

Z

S2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)Γ(ds)
(cid:12) ln |ts1 + (r + h)s2|
Z

(cid:17)

const1 + const2|t|v + const3

(cid:12)
(cid:12)
(cid:12)t +

(r + h)s2
s1

−v
(cid:12)
(cid:12)
(cid:12)

|s1|−vΓ(ds)

(cid:17)

,

S2

where, similarly to (E.5), the two terms Q1 and Q2 involve integrals over S2 ∩ {s : |ts1 + rs2| ≥ 2|h|} and

S2 ∩ {s : |ts1 + rs2| < 2|h|}. After expansion, the terms with const1 and const2 are readily dealt with by

following the method developed for (E.5). Focus on the remaining term

Z

a

S2

(cid:12)
eσ2|r|−σ1|t|(Q1 + Q2)
(cid:12)
(cid:12)t +

(r + h)s2
s1

(cid:12)
(cid:12)|s1|−vΓ(ds).
(cid:12)

In view of the bounds (E.6) and (E.7), the integrand can be bounded (up to a multiplicative constant)

by

U (h) = e−σ1|t|(cid:12)
(cid:12)
(cid:12)t +

rs2
s1

(cid:12)
(cid:12)
(cid:12)

−v(cid:12)
(cid:12)
(cid:12)t +

−v

(r + h)s0
2
s0
1

(cid:12)
(cid:12)
(cid:12)

|s1|−v|s0

1|−v.

Choosing some v ∈ (0, 1/2), we can invoke Lemma (C.6) with η = −v, p = 0 and the fact that

R
R e−σ1|t||t|−2vdt < +∞ to show that U (0) is integrable on the one hand. On the other hand we can

again invoke Lemma (C.6), this time with η = −v, 0 < p < 1 − 2v, and the fact that (2.2) holds with
ν > 1 > v + 1 − 2v > v + p to show that R U (h) → R U (0). The generalised dominated convergence

theorem applies to K12.

We turn to K23 for which «appropriate integration by parts» is required. After obvious manipulations,

K23 = lim
h→0

1
h

Z

Z

R

S2

2 ln |ts0
s0

"
1 + rs0
2|

− R
e

S2

(cid:17)

(cid:16)

(cid:12)
(cid:12)
(cid:12)

t−

hs0
2
s0
1

s1+rs2

(cid:12)
(cid:12)
(cid:12)Γ(ds)

− R

− e

S2

|ts1+rs2|Γ(ds)

#

× cos

(cid:16)

t −

(cid:17)

hs0
2
s0
1

Z

(cid:18)(cid:16)

t −

x + a

(cid:17)

hs0
2
s0
1

s1 + rs2

(cid:19)

ln

(cid:16)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

t −

(cid:17)

hs0
2
s0
1

s1 + rs2

!

Γ(ds)

Γ(ds0)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

S2
− R

2 ln |ts0
s0

1 + rs0

2|e

S2

|ts1+rs2|Γ(ds)

(cid:16)

t −

(cid:17)

hs0
2
s0
1

Z

(cid:18)(cid:16)

t −

x + a

(cid:17)

hs0
2
s0
1

s1 + rs2

(cid:19)

ln

(cid:12)
(cid:16)
(cid:12)
(cid:12)
(cid:12)

t −

s1 + rs2

!

Γ(ds)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:17)

hs0
2
s0
1
!#

− cos

tx + a

(ts1 + rs2) ln |ts1 + rs2|Γ(ds)

Γ(ds0)

S2

Z

S2

Z

Z

+ lim
h→0

1
h

R

S2
"

×

cos

:= L1 + L2.

Starting with L1, its integrand converges to

− R
e

S2

|ts1+rs2|Γ(ds)

(cid:18)

cos

tx + a

Z

S2

(ts1 + rs2) ln |ts1 + rs2|Γ(ds)

(cid:19)

49

 
 
 
(cid:18) Z

×

S2

s1(ts1 + rs2)<0>Γ(ds)

(cid:19)(cid:18) Z

S2

ln |ts1 + rs2|s2

2s1

−1Γ(ds)

(cid:19)

It can be bounded using (C.18) and Lemma C.3 (ι) by

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1 + rs0
2 ln |ts0
s0
2|
h

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(

exp

− min

(cid:18) Z

(cid:16)

t −

(cid:12)
(cid:12)
(cid:12)

(cid:17)

hs0
2
s0
1

s1 + rs2

(cid:12)
(cid:12)
(cid:12)Γ(ds),

Z

S2

|ts1 + rs2|Γ(ds)

(cid:19))

Z

S2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

S2

×

t −

(cid:16)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:19))

(cid:17)
s1 + rs2

hs0
2
s0
1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
− |ts1 + rs2|Γ(ds)
(cid:12)
(cid:12)

(

≤ eσ2|r| exp

− σ1 min

(cid:18)(cid:12)
(cid:12)
(cid:12)t −

hs0
2
s0
1

(cid:12)
(cid:12)
(cid:12), |t|

(cid:12)
2 ln |ts0
(cid:12)s0
(cid:12)

(cid:12)
1 + rs0
(cid:12)
2|
(cid:12)

1
|h|

Z

S2

(cid:12)
(cid:12)
(cid:12)

hs0
2
s0
1

(cid:12)
(cid:12)
(cid:12)Γ(ds)

s1

(

≤ σ1eσ2|r| exp

− σ1 min

(cid:18)(cid:12)
(cid:12)
(cid:12)t −

hs0
2
s0
1

(cid:19))

(cid:12)
(cid:12)
(cid:12), |t|

:= V (h).

(cid:12)
(cid:12) ln |ts0
(cid:12)

(cid:12)
(cid:12)|s0
1 + rs0
(cid:12)
2|

2|2|s0

1|−1

We follow a similar procedure as the one used in Cioczek-Georges and Taqqu (1998) (p.51) to deal with
hs2
s1

the min inside the exponential. Focus on the case

> 0 (the converse case is similar). We have

min

(cid:18)(cid:12)
(cid:12)
(cid:12)t −

hs0
2
s0
1

(cid:19)
(cid:12)
(cid:12)
(cid:12), |t|

=






(cid:12)
(cid:12)
(cid:12)t −

hs0
2
s0
1

(cid:12)
(cid:12)
(cid:12),

|t|,

if

t ≥ hs0

2/2s0
1,

if

t < hs0

2/2s0
1.

Thus, up to a multiplicative constant,

Z

R

V (h)dt =

=

=

Z +∞

hs2
2s1
Z +∞

− hs2
2s1

e−σ1|t− hs2

s1

|(cid:12)
(cid:12)
(cid:12) ln |ts1 + rs2|

(cid:12)
(cid:12)|s2|2|s1|−1dt +
(cid:12)

Z − hs2
2s1

−∞

e−σ1|t|(cid:12)
(cid:12)
(cid:12) ln |ts1 + rs2|

(cid:12)
(cid:12)|s2|2|s1|−1dt
(cid:12)

e−σ1|t|

(cid:12)
(cid:12)
(cid:12)
(cid:12)

ln

(cid:12)
(cid:12)
(cid:12)ts1 + rs2 +

hs2
s1

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

|s2|2|s1|−1dt +

Z − hs2
2s1

−∞

(cid:12)
e−σ1|t|(cid:12)
(cid:12)|s2|2|s1|−1dt
(cid:12)
(cid:12)
(cid:12) ln |ts1 + rs2|

Z

R

e−σ1|t|

(cid:20)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) ln |ts1 + (r + h)s2|
(cid:12)

1{t≥−hs2/2s1} +

(cid:12)
(cid:12)
(cid:12) ln |ts1 + rs2|

(cid:12)
(cid:12)
(cid:12)

1{t≤−hs2/2s1}

(cid:21)
|s2|2|s1|−1dt.

Thus, using Lemma E.2, we can bound the integrand for any v > 0 and |h| < |r| by

e−σ1|t|

(cid:20)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) ln |ts1 + (r + h)s2|
(cid:12) +

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) ln |ts1 + rs2|
(cid:12)

(cid:21)
|s2|2|s1|−1

≤ v−1e−σ1|t|

(cid:20)

const1 + const2|t|v

+ const3

−v

(cid:12)
(cid:12)
(cid:12)t +

rs2
s1

(cid:12)
(cid:12)
(cid:12)

|s1|−v + const4

(cid:12)
(cid:12)
(cid:12)t +

(r + h)s2
s1

(cid:12)
(cid:12)
(cid:12)

−v

|s1|−v

(cid:21)
|s2|2|s1|−1.

Clearly, the terms involving const1 and const2 are integrable with respect to t and Γ. Denoting the last
term as V4(h) := e−σ1|t|(cid:12)
|s2|2|s1|−1−v, we show that the generalised dominated convergence
(cid:12)
(cid:12)t+

(r + h)s2
s1
theorem applies. As (2.2) holds for some ν > 1, choose v =

ν − 1
2

> 0 if ν < 2, and some v ∈ (0, 1) if

−v

(cid:12)
(cid:12)
(cid:12)

50

ν ≥ 2. The integrability of V4(0) (and at the same time, of the term involving const3) is obtained from
Lemma C.5 with η = −v, b = 0, p = 0 and the fact that R
R e−σ1|t||t|−vdt < +∞. Doing so indeed yields

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Z

S2

|s2|2|s1|−1−v

Z

R

e−σ1|t|(cid:12)
(cid:12)
(cid:12)t +

rs2
s1

−v
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
− |t|−v|s2|2|s1|−1−vdt
(cid:12)
(cid:12)
(cid:12)

Γ(ds)

≤

Z

Z

S2

R

e−σ1|t|

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

−v

(cid:12)
(cid:12)
(cid:12)t +

rs2
s1

(cid:12)
(cid:12)
(cid:12)

− |t|−v

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

dtΓ(ds)

|s1|−ν|s1|ν−1−vΓ(ds)

|s1|−νΓ(ds)

Z

S2

Z

S2

≤ const

≤ const

< +∞,

since ν − 1 − v =
> 0 if ν ∈ (1, 2) and ν − 1 − v > ν − 2 > 0 if ν ≥ 2. The convergence
R V4(h) → R V4(0) can be obtained from Lemma C.7 with η = −v, b = 0 and 0 < p < v. The generalised

ν − 1
2

dominated convergence hence applies to L1.

We turn to L2. Its integrand converges to

− R

S2

e

|ts1+rs2|Γ(ds)

(cid:18)

sin

tx + a

Z

S2

(ts1 + rs2) ln |ts1 + rs2|Γ(ds)

(cid:19)

(cid:18)

×

x + a

Z

S2

s1(1 + ln |ts1 + rs2|)Γ(ds)

(cid:19)

ln |ts0

1 + rs0

2|s0
2

2s0
1

−1.

Applying the mean value theorem to the cosine function and the usual bounds, we can bound it by

ln |ts0

1 + rs0
2|

(cid:18)(cid:16)

t −

Z

S2

(cid:12)
(cid:12)
(cid:12)
hs0
2
s0
1

(cid:17)

s1 + rs2

(cid:19)

ln

(cid:16)

(cid:12)
(cid:12)
(cid:12)

t −

hs0
2
s0
1

(cid:17)
s1 + rs2

(cid:12)
(cid:12)
(cid:12) − (ts1 + rs2) ln |ts1 + rs2|Γ(ds)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

x + a

1
hs0
2
s0
1

eσ2|r|−σ1|t|(cid:12)
2 s0−1
(cid:12)s02
(cid:12)
1
(cid:12)
hs0
(cid:12)
2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
s0
(cid:12)
(cid:12)
(cid:12)
1
(cid:12)
(cid:12)
≤ eσ2|r|−σ1|t|(cid:12)
2 s0−1
(cid:12)s02
(cid:12)
1
(cid:12)
Z
a
(cid:12)
(cid:12)
hs0
(cid:12)
2
s0
1

|x| +

S2

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

ln |ts0

(cid:12)
1 + rs0
(cid:12)
2|
(cid:12)

(cid:18)(cid:16)

t −

(cid:17)

hs0
2
s0
1

s1 + rs2

(cid:19)

ln

(cid:16)

(cid:12)
(cid:12)
(cid:12)

t −

(cid:17)

hs0
2
s0
1

s1 + rs2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) − (ts1 + rs2) ln |ts1 + rs2|
(cid:12)
(cid:12)

!
.

Γ(ds)

(E.9)

The term involving |x| can be treated using the usual arguments. The one with the integral is of course

the most delicate. Let us split this integral into two parts as:

Z

S2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
hs0
2
s0
1

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:18)(cid:16)

t −

(cid:17)

hs0
2
s0
1

s1 + rs2

(cid:19)

ln

(cid:16)

(cid:12)
(cid:12)
(cid:12)

t −

(cid:17)

hs0
2
s0
1

s1 + rs2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) − (ts1 + rs2) ln |ts1 + rs2|
(cid:12)
(cid:12)

Γ(ds)

:= Q1 + Q2,

51

 
where Q1 and Q2 involve integrals over S2∩{s : |ts1+rs2| ≥ 2|hs0

2/s0

1|} and S2∩{s : |ts1+rs2| < 2|hs0

2/s0

1|}

respectively. We will ﬁrst majorise Q1 and Q2, and then use these bounds in inequality (E.9). Consider

Q2 and deﬁne the function g such that for any z > 0

g(z) =






f (z) = z| ln z|,

if 0 < z < e−1,

z(2 + ln z),

if

z ≥ e−1.

It is easily checked that g is continuous, strictly increasing and such that for any z > 0, 0 ≤ f (z) ≤ g(z).

The integrand of Q2 can be bounded as

 (cid:12)
(cid:12)
(cid:12)
(cid:12)

f

1
hs0
2
s0
1

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:18)(cid:16)

t −

(cid:17)

hs0
2
s0
1

s1 + rs2

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

(cid:16)

(cid:12)
(cid:12)
(cid:12)f

ts1 + rs2

!

(cid:17)(cid:12)
(cid:12)
(cid:12)

≤

≤

≤

 (cid:12)
(cid:12)
(cid:12)
(cid:12)

g

(cid:18)(cid:16)

t −

(cid:17)

hs0
2
s0
1

s1 + rs2

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

(cid:16)

(cid:12)
(cid:12)
(cid:12)g

ts1 + rs2

!

(cid:17)(cid:12)
(cid:12)
(cid:12)

 (cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:18)(cid:12)
(cid:12)
(cid:12)

g

3hs0
2
s0
1

(cid:12)
(cid:12)
(cid:12)

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

(cid:18)(cid:12)
(cid:12)
(cid:12)

g

(cid:12)
(cid:12)
(cid:12)
(cid:12)

2hs0
2
s1

(cid:12)
(cid:12)
(cid:12)

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)

!

g

(cid:16) 3hs0
2
s0
1

(cid:17)
.

1
hs0
2
s0
1

1
hs0
2
s0
1
2
hs0
2
s0
1

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

By Lemma (E.2), with bound further the right-hand side for any v > 0 by

(cid:17)

g

(cid:16) 3hs0
2
s0
1

2
hs0
2
s0
1

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

≤ const1 + const2

(cid:12)
(cid:12)
(cid:12)

3hs0
2
s0
1

v

(cid:12)
(cid:12)
(cid:12)

+ const3

(cid:12)
(cid:12)
(cid:12)

3hs0
2
s0
1

(cid:12)
(cid:12)
(cid:12)

−v

.

On the one hand if

(cid:12)
(cid:12)
(cid:12)

3hs0
2
s0
1

(cid:12)
(cid:12) < e−1, given that (3|ts1 + rs2|/2)−v > (3hs0
(cid:12)

2/s0

1)−v,

const1 + const2

(cid:12)
(cid:12)
(cid:12)

3hs0
2
s0
1

v

(cid:12)
(cid:12)
(cid:12)

+ const3

−v

(cid:12)
(cid:12)
(cid:12)

3hs0
2
s0
1

(cid:12)
(cid:12)
(cid:12)

≤ const1 + const2

−v

(cid:12)
(cid:12)
(cid:12)t +

rs2
s1

(cid:12)
(cid:12)
(cid:12)

|s1|−v.

On the other hand if

(cid:12)
(cid:12)
(cid:12)

3hs0
2
s0
1

(cid:12)
(cid:12) ≥ e−1, then for |h| < |r|,
(cid:12)

const1 + const2

(cid:12)
(cid:12)
(cid:12)

3hs0
2
s0
1

v

(cid:12)
(cid:12)
(cid:12)

+ const3

−v

(cid:12)
(cid:12)
(cid:12)

3hs0
2
s0
1

(cid:12)
(cid:12)
(cid:12)

≤ const1 + const2|s0

1|−v.

(E.10)

Focusing now on Q1, we can use the mean value theorem to bound its integrand by

|s1|

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12),
(cid:12)1 + ln |u|

for some u ∈

h

ts1 + rs2 − hs0

2|hs0

2/s0

1|, we have |u| ∈

2s1/s0
h |ts1+rs2|
2

1 ∧ ts1 + rs2, ts1 + rs2 − hs0
i
, 2|ts1 + rs2|

2s1/s0

1 ∨ ts1 + rs2

i
. Given that |ts1 + rs2| ≥

and thus, we further bound the above inequality using

Lemma E.2 for any v > 0 by

(cid:16)
|s1|

const1 + const2|ts1 + rs2|v + const3|ts1 + rs2|−v(cid:17)

52

≤ const1 + const2|t|v + const3

−v

(cid:12)
(cid:12)
(cid:12)t +

rs2
s1

(cid:12)
(cid:12)
(cid:12)

|s1|1−v.

Hence, using (E.10) and (E.11) in (E.9), and making use again of Lemma (E.2) to bound

we can bound integrand of L2 for any v > 0 by

(E.11)

(cid:12)
(cid:12) ln |ts0
(cid:12)

1 + rs0
2|

(cid:12)
(cid:12)
(cid:12),

(cid:18)

e−σ1|t|

const1 + const2|t|v + const3

−v(cid:19)

(cid:12)
(cid:12)
(cid:12)t +

rs0
2
s0
1

(cid:12)
(cid:12)
(cid:12)

|s0

1|−1−v

(cid:18)

×

|x| + const4 + const5|t|v + const6|s0

1|−v + const7

(cid:12)
(cid:12)
(cid:12)t +

rs2
s1

−v
(cid:12)
(cid:12)
(cid:12)

(cid:19)

|s1|1−v

It can be shown that all the terms obtained after expansion can be bounded by functions integrable

with respect to t and Γ using the usual combinations of either Lemma C.5 or Lemma C.6 with η = −v,
b = 0, p = 0, the fact that R

R e−σ1|t||t|−2v < +∞ for appropriately chosen values

R e−σ1|t||t|−v < +∞, R

v > 0, and (2.2) with ν > 1. The detail we have to pay attention to is precisely to chose an appropriate

exponent v > 0 so that it satisﬁes the constraint (2.2) and ensures the ﬁniteness of the two integrals in

t. The later imposes us to have v ∈ (0, 1/2). Regarding the former, we identify that the most negative

power of which |s1| appears in the above bound after expansion is −1 − 2v. We need ν − 1 − 2v > 0.

Choosing v = (ν − 1)/4 if 1 < ν < 3 and any v ∈ (0, 1/2) if ν ≥ 3 enables to satisfy both constraints,

validating the use of the dominated convergence theorem for L2, and ﬁnally, for B2 in (E.3).

The proof is essentially similar, somewhat easier, for B1 in (E.2) for which the only diﬃculty is

to perform the «appropriate integration by parts» when it comes to diﬀerentiating the term involving

(ts1 + rs2)<0>.

E.2 Evaluating at r = 0

Since Eh

X 2
2

(cid:12)
(cid:12)
(cid:12)X1 = x

i

= −φ(2)

X2|x(0), we evaluate (E.4) at r = 0 and get

ϕX (t, 0) = exp{−σ1|t| − iaσ1β1t ln |t| + itµ1},

A1/2 = σ2
1

(cid:16)

(κ2

1 − a2q2

(cid:17)

0)Hc(0) + 2aκ1q0Hs(0)
(cid:16)

+ 2aλ1σ2
1

− aq0Hc(1) + κ1Hs(1)

(cid:17)

− a2λ2

1σ2

1Hc(2),

iA2/2 = σ1

(cid:16)

(cid:17)
− ak1Hc(0) + κ2Hs(0)

− aλ2σ1Hc(1),

A3/2 = σ1

(cid:17)
(cid:16)
(σ1κ2 + aµ1k1)Hc(0) + (σ1ak1 − µ1κ2)Hs(0)

+ aσ1

(cid:16)

(cid:17)
(λ2µ1 − aσ1β1k1)Hc(1) + σ1(λ2 + β1κ2)Hs(1)

− a2σ2

1β1λ2Hc(2),

where k1 = σ−1
1

R
S2

(s2/s1)2s1 ln |s1|Γ(ds), and the Hc’s and Hs’s are deﬁned at Lemma E.3. Using the

result of the same Lemma under β1 6= 0 and β1 = 0, and regrouping the terms allows to retrieve the two

53

formulae of Theorem 2.4 with

U (x) =

V (x) =

W (x) =

Z +∞

0

Z +∞

0

Z +∞

0

e−σ1t sin

(cid:16)

(cid:17)
t(x − µ1) + aσ1β1t ln t

dt,

e−σ1t(1 + ln t) cos

(cid:16)
t(x − µ1) + aσ1β1t ln t

(cid:17)

dt,

e−σ1t(1 + ln t)2 cos

(cid:16)

(cid:17)
t(x − µ1) + aσ1β1t ln t

dt.

(E.12)

(E.13)

(E.14)

E.3 Lemmas for the proof of Theorem 2.4

Lemma E.2 For any x > 0 and v > 0

| ln x| ≤

1
v

(cid:16)

2 + xv + x−v(cid:17)

.

We provide here two Lemmas which are used in the proof of Theorem 2.4.

Lemma E.3 Let for any n ≥ 0,

Hc(n) =

Hs(n) =

Z +∞

0
Z +∞

0

e−σ1t(1 + ln t)n cos

(cid:16)

(cid:17)
t(x − µ1) + aσ1β1t ln t

dt,

e−σ1t(1 + ln t)n sin

(cid:16)

t(x − µ1) + aσ1β1t ln t

(cid:17)

dt.

Then, if β1 6= 0,

Hc(1) =

1
aσ1β1

(cid:16)

(cid:17)
σ1Hs(0) − (x − µ1)Hc(0)

,

Hs(1) =

(cid:16)

1
aσ1β1

1 − σ1Hc(0) − (x − µ1)Hs(0)

(cid:17)

.

If β1 = 0,

Hc(0) = πfX1(x),

Hs(0) =

Hs(1) −

x − µ1
σ1

Hc(1) =

πfX1(x),

x − µ1
σ1
πFX1(x)
σ1

.

Proof. The equalities of Lemmas D.1-E.3 can be obtained by integrating by parts. We provide details

for the last equality of Lemma E.3 when β1 = 0. Integrating the exponential by parts, we obtain

Hs(1) =

1
σ1

Z +∞

0

e−σ1tt−1 sin

(cid:16)
t(x − µ1)

(cid:17)

dt +

x − µ1
σ1

Hc(1)

Denote A(x) = R +∞

0

e−σ1tt−1 sin

(cid:16)

t(x−µ1)

(cid:17)

dt for x ∈ R (A is well deﬁned since e−σ1tt−1 sin

(cid:16)

(cid:17)
t(x−µ1)

→

x − µ1 as t → 0). It can be shown that we can derivate A under the integral sign and get

A0(x) =

Z +∞

0

e−σ1t cos

(cid:16)

(cid:17)

t(x − µ1)

dt = πfX1(x),

54

Since X1 is Cauchy distributed when α = 1 and β1 = 0,

A(x) = πFX1(x) + const = Arctg

(cid:16) x − µ1
σ1

(cid:17)

+

π
2

+ const,

and evaluating the integral form of A at µ1, we deduce that const = −π/2. Thus, A(x) = π

(cid:16)

FX1(x)−1/2

(cid:17)

.

F Proof of Proposition 2.1

F.1 Case α 6= 1

First assume that |β1| 6= 1. We will focus on the case x → +∞. The case x → −∞ can be obtained by
considering the vector (X1, X2), whose parameter are β∗
Eh

. For p = 1, the result is already known (see Hardin et al. (1991)).

1 = λ1 and noticing that

1 = −κ1 and λ∗

1 = −β1, κ∗

(cid:12)
(cid:12)
(cid:12)X1 = x

(cid:12)
(cid:12)
(cid:12) − X1 = −x
For p = 2, 3, 4, we have from the proofs of (2.6)-(2.8), that

= Eh

X p
2

X p
2

i

i

Eh

X p
2

(cid:12)
(cid:12)
(cid:12)X1 = x

i

=

ασα
1
πfX1(x)

(cid:20)
xp−1H

(cid:16)
α − 1, (aλp, κp); x

(cid:17)

+

p
X

i=2

bi,pxp−iH

(cid:16)

i(α − 1), νi; x

(cid:17)(cid:21)
,

for some coeﬃcients b’s. From the proof of Corollary 3.2 in Hardin et al. (1991), we deduce the following

limit:

We also have

Hence,

(cid:16)

xαH

α − 1, (aλp, κp); x

(cid:17)

−→
x→+∞

(cid:16)
κp + λp

(cid:17)

sin

(cid:17)

(cid:16) πα
2

Γ(α).

xα+1fX1(x) −→
x→+∞

1
π

σα
1 (1 + β1) sin

(cid:17)

(cid:16) πα
2

Γ(1 + α).

(F.1)

x−p ασα

1 xp−1
πfX1(x)

(cid:16)

(cid:17)
α − 1, (aλp, κp); x

H

−→

κp + λp
1 + β1

,

as x → +∞. It remains to be shown that

Titchmarsh (1948), for i = 2, 3, 4,

(cid:16)

Pp

i=2 bi,pxp−iH
(cid:16)

i(α − 1), νi; x
(cid:17)
α − 1, (aλp, κp); x

xp−1H

(cid:17)

−→
x→+∞

0. By Theorem 127 in

(cid:16)

H

i(α − 1), νi; x

(cid:17)

=
x→+∞

(cid:16)

x−i(α−1)−1(cid:17)

.

O

Hence,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

xp−iH
(cid:16)

(cid:16)

(cid:17)
i(α − 1), νi; x

xp−1H

α − 1, (aλp, κp); x

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:17)

=
x→+∞

(cid:16)

xα(1−i)(cid:17)

O

−→ 0.

Now assume that |β1| = 1. For instance if β1 = 1, the distribution of X1 is totally skewed to the right. On

the one hand, we have λp = β1κp. On the other hand, the right tail of fX1 still decays as (F.1), yielding

the conclusion.

55

F.2 Case α = 1

The form of the conditional second order moment when α = 1 requires to distinguish the cases β1 6= 0

and β1 = 0.

Case β1 6= 0 We only consider |β1| < 1 and x −→ +∞, the other cases being similar. Since |x| → +∞,
we have x−µ1 ∼ x and we may assume that µ1 = 0. From Hardin et al. (1991), we know that U (x) ∼ x−1.

Notice that

Z +∞

W (x) =

e−σ1t(1 + ln t)2 cos(aσ1β1t ln t) cos(tx)dt

0

−

Z +∞

0

e−σ1t(1 + ln t)2 sin(aσ1β1t ln t) sin(tx)dt.

Because the factors of cos(tx) and sin(tx) are integrable, we have by the Riemann-Lebesgue Lemma that

W (x) −→
x→+∞

0. Having also

we deduce the following limits

fX1(x) ∼

σ1(1 + β1)
π

x−2,

(cid:16)

2aσ1q0(λ1 − β1κ1) + 2(κ1λ1 − λ2)x

(cid:17) σ1U (x)

β1πfX1(x)

x−2 −→
x→+∞

(cid:16)

λ2 + β1κ2 − 2κ1λ1 + a2σ1β1(λ2

1 − β1λ2)W (x)

(cid:17) σ1x−2
πfX1(x)

Hence,

−→
x−→+∞

,

2(κ1λ1 − λ2)
(1 + β1)β1
λ2 + β1κ2 − 2κ1λ1
(1 + β1)β1

.

x−2Eh

X 2
2

(cid:12)
(cid:12)
(cid:12)X1 = x

i

−→
x→+∞

λ2
β1

+

2(κ1λ1 − λ2)
(1 + β1)β1

+

λ2 + β1κ2 − 2κ1λ1
(1 + β1)β1

=

κ2 + λ2
1 + β1

Case β1 = 0 From Hardin et al. (1991),

V (x) −→ −

π
2x

,

hence,

Moreover,

(cid:16)

2aσ1λ1

aσ1q0 − κ1(x − µ1)

(cid:17) V (x)

πfX1(x)

x−2 −→ aπλ1κ1.

FX1(x) − 1/2
fX1(x)
It can be shown that W (x) −→ 0. Therefore,

aσ1

x−2 −→

1
2

aπ(λ2 − 2κ1λ1).

x−2Eh

X 2
2

(cid:12)
i
(cid:12)
(cid:12)X1 = x

−→
x→+∞

κ2 +

1
2

aπ(λ2 − 2κ1λ1) + aπκ1λ1 = κ2 + λ2

56

G Proof of Lemma 3.1

The characteristic function of Xt reads, for any u = (u1, u2) ∈ R2:


exp

ϕXt(u) = E

ujXj,t


i

2
X









E

i


 = Y
k∈Z




 εt+k


 .

ujak,j

2
X

j=1



j=1

We obtain for α 6= 1,

ϕXt(u) = exp






− X
k∈Z

σα|

2
X

j=1


1 − iβsign

(cid:16) 2
X

ujak,j|α

j=1

ujak,j

(cid:17)

tg

(cid:18) πα
2

(cid:19)


 + i

2
X

j=1

uj

X

k∈Z

ak,jµ






.

(G.1)

And for α = 1,

ϕXt(u) = exp






− X
k∈Z

σ|

2
X

j=1

ujak,j|


1 + iβ

2
π

(cid:16) 2
X

sign

j=1

ujak,j

(cid:17)

ln

(cid:12)
(cid:12)
(cid:12)


 + i

(cid:12)
(cid:12)
(cid:12)

ujak,j

2
X

j=1

2
X

j=1

uj

X

k∈Z

ak,jµ






.

Replacing (3.4) in (2.1), we retrieve the two above formulae.

(G.2)

H Proof of the asymptotic moments in Section 3.3.3

The results in Section 3.3.3 follow from Proposition 3.1 applied to Xt = P

k∈Z ρk1{k≥0}εt+k. Regarding
the asymptotic behaviours of moments, we give the proof for the excess kurtosis. The other limits and

equivalents are obtained in a similar manner. Letting α ∈ (3/2, 2) ensures the existence of the fourth

order moment. Since we assume ρ > 0, it follows that λp = β1κp for p = 1, 2, 3, 4. Using Proposition 2.1,

one can show that as x tends to inﬁnity

γ2(x, h) −→

1κ2 − 3κ4
κ4 − 4κ1κ3 + 6κ2
1
(cid:17)2
(cid:16)

κ2 − κ2
1

− 3.

Substituting the κp’s by ρh(α−p) and rearranging terms yields the conclusion.

I Proof of Proposition 4.1

From Proposition 5.2.4 p.110 and Equation (15.3.9) p.438 in Kulik and Soulier (2020) applied to the
noncausal AR(1) process (Xt) with MA(∞) representation Xt = P
k∈Z ρk1{k≥0}εt+k, we have that,
(cid:18) Xt+h
, as x → +∞, for any continuity set A ⊂ R, and Θh is deﬁned
P
|Xt|
by Θh := Sρ−h1{h+τ ≤0}, with S a discrete random variable such that P(S = 1) = 1 − P(S = −1) = c :=
1 + β
, and τ a discrete random variable independent from S such that P(τ = k) = ρ−αk(1 − ρα)1{k≤0},
2

−→ P(cid:16)

|Xt| > x

Θh ∈ A

∈ A

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:19)

(cid:17)

57

for all k ∈ Z. For A = [ρ−h − δ, ρ−h + δ] with δ ∈ (0, ρ−h), we have [1 − ρhδ, 1 + ρhδ] ∩ {0, 1} = {1},

[−1 − ρhδ, −1 + ρhδ] ∩ {0, 1} = ∅ and

P(cid:16)

Θh ∈ A

(cid:17)

(cid:17)

Sρ−h1{h+τ ≤0} ∈ [ρ−h − δ, ρ−h + δ]

= P(cid:16)
= P(cid:16)
(cid:17)
S1{τ ≤−h} ∈ [1 − δρh, 1 + δρh]
= P(cid:16)1{τ ≤−h} ∈ [1 − δρh, 1 + δρh]
= P(cid:16)

τ ≤ −h

(cid:17)P(cid:16)

(cid:17)

c

(cid:17)

S = 1

+ P(cid:16)1{τ ≤−h} ∈ [−1 − δρh, −1 + δρh]

(cid:17)P(cid:16)

(cid:17)

S = −1

ρ−αk(1 − ρα)1{k≤0}

= c X
k≤−h

= cραh.

∈ [ρ−h − δ, ρ−h + δ]

(cid:18) Xt+h
|Xt|
(cid:12)
(cid:12)
∈ [−ρ−h − δ, −ρ−h + δ]
(cid:12)
(cid:12)

Thus, P
(cid:18) Xt+h
P
|Xt|
P(S = s) > 0,

(cid:19)

(cid:12)
(cid:12)
|Xt| > x
(cid:12)
(cid:12)
(cid:19)

−→ cραh as x → +∞. Similarly, it can be shown that

|Xt| > x

−→ (1 − c)ραh as x → +∞. Hence, for s ∈ {−1, +1}, provided

(cid:18) Xt+h
P
Xt

∈ [ρ−h − δ, ρ−h + δ]

(cid:19)

sXt > x

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= P

(cid:18) Xt+h
|Xt|
(cid:18) Xt+h
|Xt|

P

=

(cid:12)
(cid:12)
∈ [sρ−h − δ, sρ−h + δ]
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:17)
(cid:12)
|Xt| > x
sXt > x
(cid:12)
(cid:12)

∈ [sρ−h − δ, sρ−h + δ]

P(cid:16)

|Xt| > x, sXt > x

(cid:19)

(cid:19)

|Xt| > x

−→

ραhP(S = s)
P(S = s)

= ραh.

The proof for the limit of P

(cid:18) Xt+h
Xt

(cid:12)
(cid:12)
∈ [−δ, δ]
(cid:12)
(cid:12)

(cid:19)

sXt > x

is similar.

J Proof of Proposition 4.2

Assume Assumption 1 holds for some (cid:15) > 0. Let us ﬁrst show that d > 0. Ad absurdum, assume that

d = 0. Then there exists k, ‘ ∈ {0, . . . , h}, k 6= ‘, say k < ‘, such that Ak = A‘. This entails

, ρ−‘a−1, . . . , ρ−‘a−(h−‘)
) = (ρ−1, . . . , ρ−‘
, ρ−ka−1, . . . , ρ−ka−(h−k)
(ρ−1, . . . , ρ−k
).
}
}
{z
|
}
{z
|
}
{z
|
k
h−‘
h−k

{z
‘

|

Since k < ‘, the above equality implies that ρ−ka−1 = ρ−k−1, and hence a−1 = ρ−1. But then for k = −1

and λ = ρ,

λ(ak+m, . . . , ak+2, ak+1, ak) = ρ(ρm−1, . . . , ρ, 1, ρ−1) = ρ,

58

which violates Assumption 1. Hence, d > 0.

Let us now establish the main result of Proposition 4.2. For u > 0 and x0 ∈ Rn, denote generically
(cid:12)
(cid:12)
(cid:12) < u(cid:9). By Point ιι) Proposition 5.2.4 p.110 and Equation (15.3.9) p.438
(cid:12)

Bu(x0) = (cid:8)x ∈ Rn :

(cid:12)
(cid:12)
(cid:12)x − x0
(cid:12)

in Kulik and Soulier (2020),

P

X t+h
|Xt|

(cid:12)
(cid:12)
(cid:12)
∈ Bδ(sAk)
(cid:12)
(cid:12)

|Xt| > x,

∈ Bη

(cid:0)sρ(cid:1)

!

=

X t
|Xt|

P

(X t, X t+h)
|Xt|

∈ Bη

(cid:12)
(cid:12)
(cid:0)sρ(cid:1) × Bδ(sAk)
(cid:12)
(cid:12)
(cid:12)
!

∈ Bη

(cid:0)sρ(cid:1)

|Xt| > x

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

P

X t
|Xt|

!

|Xt| > x

(cid:18)

P

−→
x→+∞

(Θ−m, . . . , Θ0, Θ1, . . . , Θh) ∈ Bη

(cid:19)
(cid:0)sρ(cid:1) × Bδ(sAk)

(cid:18)

(Θ−m, . . . , Θ0) ∈ Bη

(cid:19)

(cid:0)sρ(cid:1)

P

,

(J.1)

|a−j|α
where for all k, Θk = S
‘∈Z |a‘|α for
all j ∈ Z, and S and N are independent. There is no issue with division by zero since a−N = 0 implies
P(N = j) = 0. Let us ﬁrst focus on the denominator in (J.1). We have

, with P(S = 1) = 1 − P(S = −1) = c :=

a−N −k
|a−N |

, P(N = j) =

1 + β
2

P

(cid:18)

P

(Θ−m, . . . , Θ0) ∈ Bη

(cid:19)

(cid:0)sρ(cid:1)

= P

(cid:18)

S(am−N , . . . , a1−N , a−N )/|a−N | ∈ Bη

(cid:19)

(cid:0)sρ(cid:1)

= X

(cid:18)

P

ζ∈{−1,+1}

ζ(am−N , . . . , a1−N , a−N )/|a−N | ∈ Bη

(cid:0)sρ(cid:1)

(cid:19)

P(S = ζ)

= X

X

ζ∈{−1,+1}

j∈Z

1n

ζ(am−j ,...,a1−j ,a−j )/|a−j |∈Bη

(cid:0)sρ(cid:1)oP(N = j)P(S = ζ)

= X

X

ζ∈{−1,+1}

j∈Jζ

|aj|α
‘∈Z |a‘|α

P

P(S = ζ),

(J.2)

where for ζ ∈ {−1, 1}

(

Jζ :=

j ∈ Z :

ζ

(am+j, . . . , a1+j, aj)
|aj|

)

∈ Bη(sρ)

.

By convention, if aj = 0, then the index j drops from the sum above and thus from Jζ. Let us show that

Js = {j ≥ 0} and J−s = ∅. Notice ﬁrst that for any ζ, s ∈ {−1, 1}, Assumption 1 guarantees that
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(am+j, . . . , a1+j, aj) − ρ
(cid:12)
(cid:12)

(am+j, . . . , a1+j, aj)
|aj|

(cid:12)
(cid:12)
− sρ
(cid:12)
(cid:12)

sζ
|aj|

> (cid:15) > η,

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

ζ

for all j ≤ −1 such that aj 6= 0. Thus, ζ(am+j, . . . , a1+j, aj)/|aj| 6∈ Bη(sρ) for j ≤ −1, and ζ, s ∈ {−1, 1}.
Hence, Jζ ⊂ {j ≥ 0}, for ζ ∈ {−1, 1}. Now, for j ≥ 0, since aj = ρj
(ρm+j, . . . , ρ1+j, ρj)
|ρj|

(am+j, . . . , a1+j, aj)
|aj|

= ζρ ∈ Bη(ζρ).

= ζ

ζ

59

 
 
 
Hence, {j ≥ 0} ⊂ Js which shows that Js = {j ≥ 0}. However, for j ≥ 0, −s(am+j, . . . , a1+j, aj)/|aj| =

−sρ, and k(−sρ) − sρk = 2kρk > 2(cid:15) > η by Assumption 1 with λ = 0, and −sρ 6∈ Bη(sρ). Thus,

J−s = ∅.

Therefore, (J.2) yields

X

X

ζ∈{−1,+1}

j∈Jζ

|aj|α
‘∈Z |a‘|α

P

|aj|α
‘∈Z |a‘|α

P

P(S = ζ) = X
j≥0
P(S = s)
P
‘∈Z |a‘|α

=

X

ρjα

P(S = s)

j≥0
P(S = s)

(1 − ρα) P

‘∈Z |a‘|α ,

which shows that:

(cid:18)

(Θ−m, . . . , Θ0) ∈ Bη

(cid:19)

(cid:0)sρ(cid:1)

=

P

=

P(S = s)

(1 − ρα) P

‘∈Z |a‘|α .

(J.3)

Let us now turn to the numerator in (J.1). Proceeding as above, we obtain that

(cid:18)

P

(Θ−m, . . . , Θ0, Θ1, . . . , Θh) ∈ Bη

(cid:0)sρ(cid:1) × Bδ(sAk)

(cid:19)

= X

X

ζ∈{−1,+1}

j∈Jζ

|aj|α
‘∈Z |a‘|α

P

P(S = ζ),

(J.4)

where for ζ ∈ {−1, 1}
(

Jζ :=

j ∈ Z :

ζ

(am+j, . . . , a1+j, aj, aj−1, . . . , aj−h)
|aj|

∈ Bη(sρ) × Bδ(sAk)

.

)

With similar considerations as above regarding the part ζ

(am+j, . . . , a1+j, aj)
|aj|

∈ Bη(sρ), we obtain that

Js ⊂ {j ≥ 0} whereas J−s = ∅. Also,
(

Js =

j ≥ 0 :

s

(am+j, . . . , a1+j, aj, aj, aj−1, . . . , aj−h)
|aj|

∈ Bη(sρ) × Bδ(sAk)

)

= {j ≥ 0 : Aj ∈ Bδ(Ak)} ,

where Aj = Ah for all j ≥ h. Since δ < d, we have by deﬁnition of d that Aj ∈ Bδ(Ak) if and only if

j = k in the case 0 ≤ k ≤ h − 1, and Aj ∈ Bδ(Ah) if and only if j ≥ h, that is

Js =






{k},

if k ∈ {0, . . . , h − 1},

{j ≥ h},

if k = h.

Therefore, (J.4) yields

(cid:18)

P

(Θ−m, . . . , Θ0, Θ1, . . . , Θh) ∈ Bη

(cid:0)sρ(cid:1) × Bδ(sAk)

(cid:19)

=






60

P(S = s)ραk
‘∈Z |a‘|α ,
P

if k ∈ {0, . . . , h − 1},

P(S = s)ραh

(1 − ρα) P

‘∈Z |a‘|α ,

if k = h.

Finally, combining this with (J.1) and (J.3), and provided P(S = s) > 0, we obtain that

P

X t+h
|Xt|

(cid:12)
(cid:12)
(cid:12)
∈ Bδ(sAk)
(cid:12)
(cid:12)

|Xt| > x,

∈ Bη

(cid:0)sρ(cid:1)

!

=

X t
|Xt|






ραk(1 − ρα),

if k ∈ {0, . . . , h − 1},

ραh,

if k = h.

which concludes the proof.

K Proof of Proposition 4.3

We ﬁrst have by Bayes formula that for any h ≥ 1, δ ∈ (0, (cid:15)h), s ∈ {−1, +1}:

P

(cid:18) Xt+h
Xt

(cid:12)
(cid:12)
∈ [−δ, δ]
(cid:12)
(cid:12)

(cid:19)

sXt > x

= P

(cid:18) Xt+h
Xt

(cid:12)
(cid:12)
∈ [−δ, δ]
(cid:12)
(cid:12)

|Xt| > x, sign(Xt) = s

(cid:19)

(cid:18) (Xt, Xt+h)
P
|Xt|
P(cid:16)

(cid:12)
(cid:12)
∈ {s} × [−δ, δ]
(cid:12)
(cid:12)
(cid:12)
(cid:17)
(cid:12)
(cid:12)|Xt| > x
sXt > x

=

|Xt| > x

(cid:19)

.

Letting S a random variable such that P(S = 1) = 1 − P(S = −1) = c :=

1 + β
2

, then (4.1) implies that

P(cid:16)

(cid:12)
(cid:17)
(cid:12)
(cid:12)|Xt| > x
sXt > x

−→
x→+∞

P(S = s).

Now, by Proposition 5.2.4 p.110 and Equation (15.3.9) p.438 in Kulik and Soulier (2020) applied to
Xt = P

k∈Z akεt+k, we have that:

P

(cid:18) (Xt, Xt+h)
|Xt|

∈ {s} × [−δ, δ]

(cid:19)

(cid:12)
(cid:12)
|Xt| > x
(cid:12)
(cid:12)

−→
x→+∞

P(cid:16)

(cid:17)
(Θ0, Θh) ∈ {s} × [−δ, δ]

,

where the Θj’s are random variables such that Θj := S
such that P(N = j) = |a−j|α/ P
of division by zero since for a−j = 0 we have P(N = j) = 0. Now, since |ak| = ak for all k ∈ Z,

k∈Z |ak|α, and with S and N furthermore independent. There is no issue

for all j ∈ Z, with N the random variable

a−N −j
|a−N |

P(cid:16)

(cid:17)
(Θ0, Θh) ∈ {s} × [−δ, δ]

(cid:18)

= P

(cid:16)

S

1,

∈ {s} × [−δ, δ]

(cid:19)

(cid:17)

a−N −h
a−N
(cid:18) a−N −h
a−N

= P(S = s)P

(cid:19)

∈ [−δ, δ]

= P(S = s) X
j∈Jh

|aj|α
k∈Z |ak|α ,

P

where Jh is the set of indexes deﬁned by

(cid:26)

Jh :=

j ∈ Z : aj 6= 0 and

(cid:27)

∈ [−δ, δ]

.

aj−h
aj

61

 
By assumption, aj = 0 for all j < 0, which implies that Jh ⊂ {j ≥ 0}. Also by assumption, we have that

for any j ≥ 0, aj/aj+1 > (cid:15) > 0, which implies that for all j ≥ h

aj−h
aj

> (cid:15)h > δ.

Hence, aj−h/aj 6∈ [−δ, δ] for all j ≥ h. Last, for j ∈ {0, . . . , h − 1}, we have that aj−h = 0 and thus

aj−h/aj ∈ [−δ, δ]. We deduce that Jh = {0, . . . , h − 1}, and therefore

Finally, we conclude that provided P(S = s) > 0

X

j∈Jh

|aj|α
k∈Z |ak|α =

P

Ph−1

j=0 |aj|α
k∈Z |ak|α .

P

P

(cid:18) Xt+h
Xt

(cid:12)
(cid:12)
∈ [−δ, δ]
(cid:12)
(cid:12)

(cid:19)

sXt > x

=

(cid:18) (Xt, Xt+h)
P
|Xt|
P(cid:16)

(cid:12)
(cid:12)
∈ {s} × [−δ, δ]
(cid:12)
(cid:12)
(cid:12)
(cid:17)
(cid:12)
(cid:12)|Xt| > x
sXt > x

(cid:19)

|Xt| > x

P(cid:16)

(Θ0, Θh) ∈ {s} × [−δ, δ]
P(S = s)

(cid:17)

−→
x→+∞

P(S = s)

Ph−1

j=0 |aj|α
k∈Z |ak|α

P
P(S = s)

Ph−1

j=0 |aj|α
k∈Z |ak|α .

P

=

=

Which concludes the proof.

Additional References

Azzalini, A. 2018. Package ‘sn’. Available at https://cran.r-project.org/web/packages/sn/vignettes/pkg-

overview.html

Cioczek-Georges, R., and M. S., Taqqu. 1994. How do conditional moments of stable vectors depend on

the spectral measure? Stochastic Processes and their Applications, 54, 95-111.

Hu, Y., and C., Scarrott. 2018. evmix: An R package for extreme value mixture modeling,
threshold estimation and boundary corrected kernel density estimation. Available at https://cran.r-
project.org/web/packages/evmix/index.html.

Kulik, R., and P., Soulier. 2020. Heavy-tailed time series. Springer Series in Operation Research and

Financial Engineering. Springer-Verlag.

Royden, H. L., and P. M., Fitzpatrick. 2010. Real analysis. Prentice Hall, fourth edition.

Titchmarsh, E. C. 1948. Introduction to the theory of Fourier integrals. Second edition, Oxford University

Press.

Wu, R. 2013. M-estimation for general ARMA processes with inﬁnite variance. Scandinavian Journal

of Statistics, 40, 571-591.

62

