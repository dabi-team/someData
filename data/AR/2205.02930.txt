YAN ET AL.: FISHEYEDISTILL

1

FisheyeDistill: Self-Supervised Monocular
Depth Estimation with Ordinal Distillation for
Fisheye Cameras

1 OPPO US Research Center
InnoPeak Technology, Inc.
Palo Alto, California,US
2 Wing LLC
Mountain View, California, US
(†Work done while at OPPO US Research
Center.)
3 ∗Corresponding author

]

V
C
.
s
c
[

Qingan Yan1
qingan.yan@innopeaktech.com
Pan Ji∗13
pan.ji@innopeaktech.com
Nitin Bansal1
nitin.bansal@innopeaktech.com
Yuxin Ma†2
yuxinma2005@gmail.com
Yuan Tian1
yuan.tian@innopeaktech.com
Yi Xu1
yi.xu@innopeaktech.com

2
2
0
2

y
a
M
5

1
v
0
3
9
2
0
.
5
0
2
2
:
v
i
X
r
a

Abstract

In this paper, we deal with the problem of monocular depth estimation for ﬁsheye
cameras in a self-supervised manner. A known issue of self-supervised depth estimation
is that it suffers in low-light/over-exposure conditions and in large homogeneous regions.
To tackle this issue, we propose a novel ordinal distillation loss that distills the ordinal
information from a large teacher model. Such a teacher model, since having been trained
on a large amount of diverse data, can capture the depth ordering information well, but
lacks in preserving accurate scene geometry. Combined with self-supervised losses, we
show that our model can not only generate reasonable depth maps in challenging envi-
ronments but also better recover the scene geometry. We further leverage the ﬁsheye
cameras of an AR-Glasses device to collect an indoor dataset to facilitate evaluation.

1

Introduction

Fisheye cameras have gradually gained popularity on Head Mounted Display (HMD), as it
offers a large ﬁeld-of-view (FoV) that is well suited for immersive AR/VR experience. To
estimate the depth map from a distorted ﬁsheye image, a typical prerequisite is to perform
image rectiﬁcation. Such a rectiﬁcation step, however, undermines the beneﬁts of using
ﬁsheye cameras because it will inevitably reduce the FoV of the camera. Moreover, image
rectiﬁcation itself is expensive on HMD as there are only very limited computational re-
sources available on the device. Therefore, it’s desirable to directly estimate a depth map
from the ﬁsheye image, which is the main theme of this paper.

© 2021. The copyright of this document resides with its authors.
It may be distributed unchanged freely in print or electronic forms.

 
 
 
 
 
 
2

YAN ET AL.: FISHEYEDISTILL

The past few years have seen a growing interests in self-supervised depth estimation [7,
10, 40] because it requires no groundtruth depth during training and yet achieves comparable
results with some of the well known supervised methods. However, due to dearth of effec-
tive benchmarking datasets, depth estimation for ﬁsheye cameras remains under-explored,
with only a few exceptions [18, 19]. Nonetheless, those ﬁsheye depth estimation meth-
ods [18, 19] are only evaluated on well-conditioned outdoor self-driving datasets, including
the WoodScape dataset [38], for which the groundtruth depths are not yet publicly available.
In this work, we follow this trend to take a self-supervised approach to estimating a depth
map from a distorted image, but under a more challenging environment, which is captured
by HMD.

Self-supervised depth estimation is not without challenges because the key supervision
signal of photo-consistency can become ineffective in low-light/over-exposure conditions
and in large areas of no/little textures as commonly seen in indoor environments. On the
other hand, there has been depth models that are trained on a variety of datasets and can gen-
eralize well to new unseen domains [27, 28, 36]. Although such depth models can generate
visually good-looking depth maps, the 3D scene geometry in those depth maps are often not
well recovered [36]. For example, when visualizing the depth maps of an indoor room in 3D,
the horizontal and vertical walls may not be perpendicular to each other, showing distorted
3D scene geometry [36].

In this work, we propose to combine the best of self-supervised depth estimation and
knowledge distillation [13] to build a robust depth model for ﬁsheye cameras especially in
challenging indoor environments. In particular, we adapt the photometric loss [10], which is
usually based on a pin-hole camera model, to a ﬁsheye camera model. On top of the self-
supervised losses, we further devise a novel ordinal distillation loss which aims to transfer
the depth ordering information from a teacher model into our target model. The intuition of
ordinal distillation is that the teacher model [27, 28] is usually good at predicting the depth
ordering relationships of pixels (e.g., a certain pixel is closer or farther than other pixels),
albeit not reﬂecting the exact 3D geometry. Our ﬁnal depth model becomes more robust and
accurate by (i) learning the ordering relationships of all pixels via ordinal distillation and (ii)
respecting the 3D geometry enforced by the photometric loss.

To perform quantitative evaluation, we collect a new dataset which contains stereo image
sequences captured by a pair of stereo ﬁsheye cameras mounted on an AR-Glasses device,
and generate the pseudo groundtruth depth maps using stereo matching [14]. On this dataset,
we show that our proposed model leads to signiﬁcant improvements over the baseline mod-
els.

2 Related Work

In this section, we brieﬂy review a few related works on self-supervised and supervised
monocular depth estimation.

Self-Supervised Depth Estimation

Most of self-supervised depth estimation methods assume that the camera is calibrated and
the images are rectiﬁed such that a photometric loss can be constructed via a combination
of unprojection and projection operations. Garg et al. [7] are the ﬁrst to leverage the photo-
consistency between stereo images to build a self-supervised loss for training a deep depth

YAN ET AL.: FISHEYEDISTILL

3

Figure 1: The system workﬂow of our FisheyeDistill. Our system consists of two main parts,
i.e., a student model trained with self-supervised losses and distillation losses, and a teacher
model (MiDaS) [27, 28] that predicts depths to guide the training of the student model. The
SID stands for the scale-invariant distillation and SIDM is the corresponding function module
described in Sec. 3.3. ORD and ORDM are the ordinal distillation loss and the module.

model. Zhou et al. [40] extend to using temporal sequences to train a depth model and a
pose model with a temporal photometric loss. A lot of follow-up methods are then proposed
to improve [7, 40] by new losses. Depth consistency losses are introduced to enforce the
network to predict consistent depth maps in stereo [9] or temporal [1, 23] images. Wang et
al. [33] observe a scale diminishing issue in monocular training and propose a depth nor-
malization method to counter this issue. A few methods [34, 42] employ recurrent neu-
ral networks to model long-term temporal dependencies for self-supervised training. Some
other methods [3, 37, 41] introduce an additional optical ﬂow network to build cross-task
consistencies in a monocular training setup. Godard et al. [10] comprehensively analyze
the challenges faced in self-supervised depth learning, including occlusion, static pixels,
and texture-copying artifacts, and propose a set of novel techniques to handle those chal-
lenges. Tiwari et al. [31] propose to combine self-supervised depth and geometric SLAM in
a self-improving loop. Ji et al. [16] observe the challenges for indoor self-supervised depth
estimation and come up with a depth factorization module and a residual pose estimation
module to improve the performance under indoor environments. Watson et al. [35] use mul-
tiple temporal images as input to the depth model and put forth a teacher-student framework
to deal with moving objects in the scene. Ji et al. [17] propose an online system for reﬁning
a depth model in a self-supervised manner, combining a robustiﬁed SLAM system and a
monocular depth model.

Supervised Depth Estimation

Ever since EigenDepth [5], many methods have been proposed to improve supervised monoc-
ular depth estimation on a speciﬁc dataset, either by using better loss terms [6, 22] or via
multi-task learning [11, 25, 26, 30, 32, 39]. Instead of focusing on the depth prediction of
one single dataset (or domain), some other methods seek to train a depth model that can learn

4

YAN ET AL.: FISHEYEDISTILL

across different domains. Along this line, Megadepth [20] exploits internet photos to train a
depth model with structure-from-motion reconstructions [29]. Li et al. [21] leverage internet
videos with frozen people to predict sharp depth maps for images with people. MiDaS [27]
proposes a scale-invariant loss to train the depth model on a large and diverse training sets,
which facilitates generality and cross-dataset learning by providing a dataset-agnostic depth
model. DPT [28] further improves MiDaS by using a vision transformer network [4]. Al-
though visually pleasing depth maps can be generated by those models [27, 28], their quan-
titative performance on a speciﬁc dataset is usually worse than those that are trained speciﬁ-
cally on the same dataset. This means that the accurate 3D geometry information is not well
preserved in those models [27, 28].

Our method tries to reap the beneﬁts of both self-supervised and supervised models in
the sense that it respects the 3D geometry via the use of self-supervised losses and enforces
ordering consistency between neighboring depth pixels by distilling information from a di-
versely trained teacher model.

3 Method

In this work, we aim at learning a self-supervised monocular depth estimation model for
ﬁsheye images (see Fig. 1). In all the previous works [7, 10, 40], view synthesis is usually
used in self-supervision by learning a depth and pose relationship to synthesize virtual target
images from neighboring views It−1 and It+1. For this, it requires a projection function
Φ that maps 3D points Pi in 3D space to image coordinates pi = Φ(Pi), and accordingly
the corresponding unprojection function Φ−1, which converts image pixels, based on the
estimated depth map D, into 3D space Pi = Φ−1(pi, D) in order to acquire color information
from other views.

3.1 Fisheye Geometry Model

For ﬁsheye cameras, given a 3D point Pi = (Xi,Yi, Zi)T in camera coordinates and with (xi =
Xi/Zi, yi = Yi/Zi), the projection function pi = Φ(Pi) from 3D point Pi to distorted image
pixel pi = (u, v)T can be obtained through the following mapping equation:

Φ(Pi) =

(cid:20)u = fx · xd + cx
v = fy · yd + cy

(cid:21)

,

(1)

xi
r

yi
r

φθ ) with r = (cid:112)xi

φθ , yd =

2, θ = arctan(r) is the angle of incidence,
2 + yi
where (xd =
φθ = θ · (1 + k1 · θ 2 + k2 · θ 4 + k3 · θ 6 + k4 · θ 8) is the polynomial radial distortion model
mapping the incident angle to the image radius, ( fx, fy) and (cx, cy) stand for the focal length
and the principal point derived from the intrinsic matrix K, and {ki} denote the set of ﬁsheye
distortion coefﬁcients.

For the unprojection part, due to the existence of ﬁsheye distortion, we are unable to
directly transform a pixel pi into camera coordinates via the pinhole model.
In order to
achieve that, we ﬁrst generate an intermediate rectiﬁed depth map ˆD from the depth map
D estimated from our network. This can be done by warping a pixel grid according to
Eq. (1). Then we leverage the rectiﬁed ˆD to unproject the grid into 3D by simply apply-
ing Φ−1(pi, ˆD) = ˆDK−1 · pi. 1 The view synthesis process in our ﬁsheye pipeline can be

1Here and below, we omit a necessary step of converting to the homogeneous coordinates for notation simplicity.

YAN ET AL.: FISHEYEDISTILL

5

summarized as: (i) Unproject a uniform pixel grid which has the same resolution with input
frames through Φ−1(pi, ˆD); (ii) Project those 3D points by Φ(Pi) and the associated pose
information from pose network module, to obtain distorted synthesis images. This process is
different from the FisheyeDistanceNet [18] which needs to numerically calculate θ out from
the 4th order polynomial.

3.2 Self-Supervised Losses

Following [10], we use a photometric reprojection loss between two neighboring ﬁsheye
images and an edge-aware depth smoothness loss as the self-supervised losses to train the
models. Speciﬁcally, given a target ﬁsheye image It and a source ﬁsheye image I(cid:48)
t , two
models (i.e., a depth model and a pose model) are jointly trained to predict a dense depth map
Dt and a relative transformation matrix Ttt(cid:48). The per-pixel minimum photometric reprojection
loss [10] can be computed as

and

Lph = min
t(cid:48)

ρ(It , Itt(cid:48)),

Itt(cid:48) = It(cid:48)

(cid:10)Φ(cid:0)Ttt(cid:48)Φ−1(pi, ˆD)(cid:1)(cid:11),

(2)

(3)

where t(cid:48) ∈ {t − 1,t + 1}, ρ(·) denotes a weighted combination of the L1 and Structured
SIMilarity (SSIM) losses [10], and (cid:10) · (cid:11) is the bilinear sampling operator. The edge-aware
smoothness loss is deﬁned as

Lsm = |∂xd∗

t |e−|∂xIt | + |∂yd∗

t |e−|∂yIt |,

(4)

t = d/ ¯dt is the mean-normalized inverse depth from [33]. An auto-masking mecha-

where d∗
nism [10] is also adopted to mask out static pixels when computing the photometric loss.

3.3 Distillation Losses

Self-supervised losses are volatile when the brightness or RGB values of a pixel become
indistinguishable with its neighboring pixels. Previously, such color ambiguity is implicitly
handled via the smoothness loss and a multi-scale strategy [10], but they do not work well
if the textureless regions are large. For example, in indoor environments, textureless walls
often occupy a large area in the image; or in low-light/over-exposure conditions, most of
the regions in an image are near-textureless. To provide supervision in those scenarios, we
apply knowledge distillation [13] to distill depth information from a diversely trained teacher
model [27, 28] into a smaller target model. In particular, we propose to combine a novel
depth ordinal distillation loss and a scale-invariant distillation loss [27], which we detail as
below.

Pretrained models such as MiDaS [27, 28] are good at predicting relative depth rela-
tionships as they are trained on large and diverse datasets. In light of this, we aim to distill
from the teacher model the ordinal information between neighboring pixels into our student
model. Inspired by [2], we propose to use a ranking loss to build an ordinal distillation loss
between the depth maps from the teacher model and the target student model.

Given a depth map DMONO from the student model and a depth map DMIDAS from the
teacher model, we compute a ranking loss from each pixel I(i, j) to its left neighbor I(i−1, j)

6

as follows,

Lod
l =






log (cid:0)1 + exp(−DMONO
log (cid:0)1 + exp(DMONO
|DMONO
|,
i, j

− DMONO
i−1, j

i, j

i, j

i−1, j )(cid:1),
+ DMONO
i−1, j )(cid:1),

− DMONO

YAN ET AL.: FISHEYEDISTILL

if DMIDAS
i, j
if DMIDAS
i, j
otherwise,

> αDMIDAS
i−1, j
< β DMIDAS
i−1, j

(5)

where α and β are hyper-parameters controlling the ranking gap and are empirically set to
α = 1.1 and β = 0.9 in our experiments. Similarly, we can compute a ranking loss from
each pixel I(i, j) to its neighbor above it: I(i, j − 1) as Lod
. The ﬁnal ordinal distillation loss
t
is then the sum of Lod

, i.e.,

l and Lod
t

Lod = Lod

l + Lod
t
Intuitively, if a pixel is predicted to be farther (or closer) than its neighbors by the teacher
model, by minimizing the ordinal distillation loss, the student model is encouraged to predict
If in the teacher model two neighboring pixels are
similar depth ordering relationships.
predicted to be close, a smoothness term is enforced for the student model.

(6)

.

We further add a scale-invariant distillation loss Lsd between DMONO and DMIDAS to
strengthen the supervision in textureless regions. Following [27], DMONO and DMIDAS are
respectively normalized to be scale- and shift-invariant (subtracted by the median scale and
then divided by the mean shift). The normalized versions are denoted as ˜DMONO and ˜DMIDAS.
The scale-invariant loss is then deﬁned as:

Lsd = | ˜DMONO − ˜DMIDAS|.

(7)

Our ﬁnal training loss is a weighted combination of the photometric loss, the smoothness

loss and the distillation losses, i.e.,

L = Lph + wsmLsm + wodLod + wsdLsd.

(8)

4 AR-Glasses Fisheye Dataset

In order to evaluate the model, we contribute a large ﬁsheye dataset for training and eval-
uation.The dataset contains 321, 300 raw ﬁsheye images collected from a well calibrated
AR-Glasses device in a stereo manner. All images are acquired in an indoor environment
with 8 different scenes. Besides various chairs, desks and decorations, there are also many
lights, texture-less walls and glasses therein. In addition, due to the computational limit on
the headset, all frames are captured under the resolution of 640 × 400 in gray-scale with a
sub-optimal auto-exposure mechanism. These make our dataset very challenging for the task
of self-supervised depth estimation as the photometric constraints would act poorly in such
scenes. We will have more discussion on it in Sec. 5.2.

More speciﬁcally, in our experiments, we use 238, 186 images for training and 42, 688
and 20, 213 images as the validation and test sets respectively. The 8 scenarios are randomly
shufﬂed and fed into the network while training. Similar to [10], we also apply the static
frame ﬁltering to clean our data and get a ﬁnal training and a validation set of 71, 688 and
6, 265 images. One major limitation of other ﬁsheye datasets [24] is that there is no easily
accessible ground truth for evaluation. Instead, we utilize the semi-global stereo matching
method [14] based on OpenCV’s implementation to calculate pseudo ground truth for our
dataset. In our experiments, we select 300 well calculated stereo depth maps from different
viewpoints as ground truth. We conduct all the experiments in the monocular mode.

YAN ET AL.: FISHEYEDISTILL

5 Experiments

5.1 Implementation Details

7

We implement our system using Pytorch and employ Adam optimizer to minimize the ob-
jective function in Eq. (8). We train the model for 20 epochs with a learning rate of 10−4
on a 12GB GeForce RTX. Due to the limitation of graphic memory, a small batch size of
6 is leveraged while training and batch normalization [15] layers are also frozen to avoid
unexpected degeneration. We convert the sigmoid output σ from the network to depth via
D = 1/(a · σ + b), where a and b are chosen to bound D between 0.1 and 100 units. As the
resolution of our AR-Glasses images is 640 × 400, we feed 640 × 384 pixels as the network
input to approximate the original aspect ratio. The smoothness loss weight wsm is set to
10−3 and distillation loss weights wod = wsd = 1.0. For generating the rectiﬁed intermediate
depth ˆD, we apply a sightly small focal length than the one given in K. This would help the
network to better learn the content on distorted image boundaries. In all our experiments, we
use 0.8 as the scale factor.

As previously discussed, while the MiDaS teacher model [28] provides a good guidance
for depth estimation even on challenging structures, it suffers from the limitation in keeping
true geometric relationship, due to the lack of cross-view photometric constraints. Therefore,
directly adopting a constant weight for the knowledge distillation through all epochs would
weaken the ability of learning better geometry information. To circumvent this issue, we
adopt a decay weighting strategy for the distillation losses in training. After each 10k steps,
we decay the weights wod and wsd by applying a factor s = 0.92λ , where λ =
. Such a
strategy enables a major contribution from the distillation losses when the uncertainty is high
and facilitates the convergence of photometric constraints in the later training stage. After
ﬁve epochs, photometric losses begin to play a more dominant role in model training, which
ensures the acquisition of geometric relationship between temporal frames. Similar to other
self-supervised works, we also set the length of the training snippets to 3 and build the losses
over 4 image scales.

steps
10000

5.2 Evaluation

We evaluate our model mainly on the proposed AR-Glasses ﬁsheye dataset. We do not
leverage KITTI [8] as it is not a ﬁsheye dataset. Woodscape [38] dataset could be a good
candidate, but it is still not fully released. Currently only around 8k images are available
and without ground truth for evaluation. While the image resolution of AR-Glasses dataset
is smaller than KITTI dataset, the ground truth is much denser. Moreover, since there is no
explicit sky region in the indoor environment, we do not perform any cropping for each test
image. We evaluate the models on all pixels whose depth values are not zero in the computed
groundtruth.

The quantitative results are reported in Table 5.2, which shows the performance of our
model on the AR-Glasses dataset. From the table, we can see that without considering
the ﬁsheye distortion, Monodepth2 [10] is unable to extract plausible depth outputs from
distorted images. MiDaS [28] shows a good generalization ability on our dataset, despite
being trained on rectiﬁed images. We are unable to compare with the ﬁsheye model [18],
as its source code is not publicly available. As ﬁsheye cameras are designed for near-ﬁeld
sensing and deployed on AR-Glasses for indoor applications, the groundtruth depth values
are capped at 20m for evaluation. In addition, we also demonstrate how the decay factor s

8

YAN ET AL.: FISHEYEDISTILL

Table 1: Quantitative results of different algorithms on AR-Glasses ﬁsheye dataset. All the
approaches are evaluated in the monocular mode and the estimated depths are scaled using
median scaling.

Method
Monodepth2 [10]
MiDaS [28]
Ours with s = 0.982λ
Ours with s = 0.92λ

Abs Rel
0.348
0.216
0.136
0.131

Sq Rel
0.108
0.131
0.022
0.020

RMSE
0.198
0.252
0.103
0.098

RMSE_log
0.398
0.267
0.183
0.177

δ < 1.25
0.449
0.756
0.843
0.854

δ < 1.252
0.749
0.918
0.971
0.973

δ < 1.253
0.895
0.965
0.991
0.992

Figure 2: Qualitative results on the AR-Glasses dataset. From left to right, the images are
respectively input ﬁsheye images, MiDaS estimated disparity maps, disparity maps from our
network without any distillation losses, and disparity maps with distillation losses.

affects the performance in training. We report the comparison results by employing s = 0.92λ
and ˆs = 0.982λ respectively, where ˆs enforces more emphasis on the distillation part. It can
be found that letting the network learn more geometric information through the temporal
photometric loss helps to improve the results.

We also render the qualitative results to demonstrate our advantages in challenging in-
door environments, as shown in Fig. 4. Directly training upon photometric constraints with-
out knowledge distillation is prone to producing halo and texture-copy artifacts. This is
because that large texture-less continuous surfaces, such as planar white walls and desks,
and the low light condition make photometric losses unable to well establish pixel corre-
spondences from the very beginning. In contrast, due to the good generalization ability on
different datasets, the MiDaS teacher model [28], provides a smooth and sharp depth order-
ing hint to let the network approximate plausible depth even on texture-less planes. However,
the geometric constraint within MiDaS is insufﬁcient. This is notable from Fig. 3, where we
visualize the 3D geometry unprojected from corresponding depth maps of different methods.
While the depth map is visually-pleasant, MiDaS suffers from severe distortion in 3D space.
Our method overcomes the limitations existing in both [10] and [28] and is able to give more
consistent results on the challenging indoor dataset.

YAN ET AL.: FISHEYEDISTILL

9

Figure 3: Visual comparison of the 3D point clouds generated from depth maps by MiDaS
(middle) and our method (right). Compared to MiDaS, our results generate more geometri-
cally correct point clouds.

Table 2: Ablation study by using different variants on the proposed AR-Glasses dataset.
Depths are all capped at 20m. SID, ORD, DW respectively represent the scale-invariant
distillation, ordinal distillation and decay weighting.

Method

Ours
Ours
Ours
Ours

SID
(cid:37)
(cid:33)
(cid:33)
(cid:33)

ORD
(cid:37)
(cid:33)
(cid:37)
(cid:33)

DW Abs Rel
(cid:37)
(cid:37)
(cid:33)
(cid:33)

0.151
0.134
0.134
0.131

Sq Rel

RMSE

RMSE_log

δ < 1.25

δ < 1.252

δ < 1.253

0.021
0.020
0.019
0.020

0.103
0.098
0.097
0.098

0.200
0.180
0.179
0.177

0.780
0.846
0.844
0.854

0.961
0.972
0.972
0.973

0.992
0.992
0.992
0.992

5.3 Ablation Study

We conduct an ablation study to illustrate the beneﬁts of different components used in our
system and show the results in Table 5.2. (i) Remove both distillation losses: The network
is trained only with distortion-based self-supervised photometric losses. As previously dis-
cussed, the proposed AR-Glasses dataset is very challenging, due to under-exposure and
textureless structures. Only relying on photometric constraints is prone to producing halo
and texture-copy artifacts. In contrast, the distillation strategy offers a huge quantitative im-
provement. Fig. 3 also provides a visual comparison for the sake of illustration. (ii) Remove
the ordinal distillation loss: Ordinal distillation loss is good at the supervision in textureless
regions. Removing the ordinal distillation loss from the network would diminish the per-
formance gain over the baseline. (iii) Utilize constant distillation loss weights: Apart from
the comparison of using different s as illustrated before, we also launch an experiment with
a constant distillation weighting strategy. For the trade-off between keeping depth ordering
and learning true geometry, we set the loss weights to wod = wsd = 0.3 without decaying
here for testing. It can be seen that using constant distillation loss weights decrease the per-
formance on our dataset, which veriﬁes the effectiveness of our decay-weighting strategy.

10

6 Conclusions

YAN ET AL.: FISHEYEDISTILL

In this work, we have presented FisheyeDistill, a self-supervised monocular depth estimation
method for ﬁsheye cameras. Our key insight is to combine self-supervised ﬁsheye photomet-
ric losses with a novel ordinal distillation loss to train a robust depth model that can work
well in challenging environments for ﬁsheye cameras.

Appendix

We adopt the same network structures with [10].
In all experiments we use a standard
ResNet18 [12] encoder for both depth and pose networks. The pose encoder takes ﬁsheye
frames as input, as our network is trained to learn the pose under the distortion circum-
stance. We also empirically found that using corresponding rectiﬁed images as input to the
pose network decreases its performance. For the distillation part, we leverage the pretrained
DPT [28] large model as our teacher model. For further demonstration, we also upload a
demo video, which respectively shows a portion of our contributed AR-glass ﬁsheye dataset
and the corresponding depth estimation results (with and without knowledge distillation). It
is evident that without the proposed distillation strategies, the results suffer from signiﬁcant
halo artifacts, as is also shown below.

Figure 4: Depth estimation results on the AR-Glasses dataset. From left to right are re-
spectively the ﬁsheye frames in our dataset, and the color-coded depth predictions from our
network without and with using the proposed distillation strategies.

References

[1] Jia-Wang Bian, Zhichao Li, Naiyan Wang, Huangying Zhan, Chunhua Shen, Ming-
Ming Cheng, and Ian Reid. Unsupervised scale-consistent depth and ego-motion learn-
ing from monocular video. arXiv preprint arXiv:1908.10553, 2019.

[2] Weifeng Chen, Zhao Fu, Dawei Yang, and Jia Deng. Single-image depth perception in

the wild. arXiv preprint arXiv:1604.03901, 2016.

YAN ET AL.: FISHEYEDISTILL

11

[3] Yuhua Chen, Cordelia Schmid, and Cristian Sminchisescu. Self-supervised learning
with geometric constraints in monocular video: Connecting ﬂow, depth, and camera.
In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages
7063–7072, 2019.

[4] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua
Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition
at scale. arXiv preprint arXiv:2010.11929, 2020.

[5] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from a single
image using a multi-scale deep network. arXiv preprint arXiv:1406.2283, 2014.

[6] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, and Dacheng Tao.
In CVPR, pages

Deep ordinal regression network for monocular depth estimation.
2002–2011, 2018.

[7] Ravi Garg, Vijay Kumar Bg, Gustavo Carneiro, and Ian Reid. Unsupervised cnn for
single view depth estimation: Geometry to the rescue. In ECCV, pages 740–756, 2016.

[8] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets
robotics: The kitti dataset. The International Journal of Robotics Research, 32(11):
1231–1237, 2013.

[9] Clément Godard, Oisin Mac Aodha, and Gabriel J Brostow. Unsupervised monocular

depth estimation with left-right consistency. In CVPR, pages 270–279, 2017.

[10] Clément Godard, Oisin Mac Aodha, Michael Firman, and Gabriel J Brostow. Digging

into self-supervised monocular depth estimation. In ICCV, pages 3828–3838, 2019.

[11] Xiaoyang Guo, Hongsheng Li, Shuai Yi, Jimmy Ren, and Xiaogang Wang. Learning
monocular depth by distilling cross-domain stereo networks. In ECCV, pages 484–500,
2018.

[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning
for image recognition. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 770–778, 2016.

[13] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural

network. arXiv preprint arXiv:1503.02531, 2015.

[14] Heiko Hirschmuller. Stereo processing by semiglobal matching and mutual informa-

tion. TPAMI, 30(2):328–341, 2007.

[15] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network

training by reducing internal covariate shift. In ICML, pages 448–456. PMLR, 2015.

[16] Pan Ji, Runze Li, Bir Bhanu, and Yi Xu. Monoindoor: Towards good practice of
self-supervised monocular depth estimation for indoor environments. In ICCV, pages
12787–12796, 2021.

[17] Pan Ji, Qingan Yan, Yuxin Ma, and Yi Xu. Georeﬁne: Self-supervised online depth
reﬁnement for accurate dense mapping. arXiv preprint arXiv:2205.01656, 2022.

12

YAN ET AL.: FISHEYEDISTILL

[18] Varun Ravi Kumar, Sandesh Athni Hiremath, Markus Bach, Stefan Milz, Christian
Witt, Clément Pinard, Senthil Yogamani, and Patrick Mäder. Fisheyedistancenet: Self-
supervised scale-aware distance estimation using monocular ﬁsheye camera for au-
tonomous driving. In ICRA, pages 574–581, 2020.

[19] Varun Ravi Kumar, Senthil Yogamani, Markus Bach, Christian Witt, Stefan Milz, and
Patrick Mader. Unrectdepthnet: Self-supervised monocular depth estimation using a
generic framework for handling common camera distortion models. arXiv preprint
arXiv:2007.06676, 2020.

[20] Zhengqi Li and Noah Snavely. Megadepth: Learning single-view depth prediction from

internet photos. In CVPR, pages 2041–2050, 2018.

[21] Zhengqi Li, Tali Dekel, Forrester Cole, Richard Tucker, Noah Snavely, Ce Liu, and
William T Freeman. Learning the depths of moving people by watching frozen people.
In CVPR, pages 4521–4530, 2019.

[22] Fayao Liu, Chunhua Shen, Guosheng Lin, and Ian Reid. Learning depth from single
monocular images using deep convolutional neural ﬁelds. TPAMI, 38(10):2024–2039,
2015.

[23] Xuan Luo, Jia-Bin Huang, Richard Szeliski, Kevin Matzen, and Johannes Kopf. Con-

sistent video depth estimation. TOG, 39(4):71–1, 2020.

[24] Hidenobu Matsuki, Lukas von Stumberg, Vladyslav Usenko, Jörg Stückler, and Daniel
Cremers. Omnidirectional dso: Direct sparse odometry with ﬁsheye cameras. IEEE
Robotics and Automation Letters, 3(4):3693–3700, 2018.

[25] Xiaojuan Qi, Renjie Liao, Zhengzhe Liu, Raquel Urtasun, and Jiaya Jia. Geonet: Geo-
metric neural network for joint depth and surface normal estimation. In CVPR, pages
283–291, 2018.

[26] Siyuan Qiao, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Vip-
deeplab: Learning visual perception with depth-aware video panoptic segmentation. In
CVPR, pages 3997–4008, 2021.

[27] René Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun.
Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-
dataset transfer. arXiv preprint arXiv:1907.01341, 2019.

[28] René Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense

prediction. arXiv preprint arXiv:2103.13413, 2021.

[29] Johannes L Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In

CVPR, pages 4104–4113, 2016.

[30] Zachary Teed and Jia Deng. Deepv2d: Video to depth with differentiable structure

from motion. arXiv preprint arXiv:1812.04605, 2018.

[31] Lokender Tiwari, Pan Ji, Quoc-Huy Tran, Bingbing Zhuang, Saket Anand, and Man-
mohan Chandraker. Pseudo rgb-d for self-improving monocular slam and depth pre-
diction. In ECCV, pages 437–455, 2020.

YAN ET AL.: FISHEYEDISTILL

13

[32] Benjamin Ummenhofer, Huizhong Zhou, Jonas Uhrig, Nikolaus Mayer, Eddy Ilg,
Alexey Dosovitskiy, and Thomas Brox. Demon: Depth and motion network for learn-
ing monocular stereo. In CVPR, pages 5038–5047, 2017.

[33] Chaoyang Wang, José Miguel Buenaposada, Rui Zhu, and Simon Lucey. Learning
depth from monocular videos using direct methods. In CVPR, pages 2022–2030, 2018.

[34] Rui Wang, Stephen M Pizer, and Jan-Michael Frahm. Recurrent neural network for
(un-) supervised learning of monocular video visual odometry and depth. In CVPR,
pages 5555–5564, 2019.

[35] Jamie Watson, Oisin Mac Aodha, Victor Prisacariu, Gabriel Brostow, and Michael
Firman. The temporal opportunist: Self-supervised multi-frame monocular depth. In
CVPR, pages 1164–1174, 2021.

[36] Wei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus, Long Mai, Simon Chen, and
Chunhua Shen. Learning to recover 3d scene shape from a single image. In CVPR,
pages 204–213, 2021.

[37] Zhichao Yin and Jianping Shi. Geonet: Unsupervised learning of dense depth, optical

ﬂow and camera pose. In CVPR, pages 1983–1992, 2018.

[38] Senthil Yogamani, Ciarán Hughes, Jonathan Horgan, Ganesh Sistu, Padraig Varley,
Derek O’Dea, Michal Uricár, Stefan Milz, Martin Simon, Karl Amende, et al. Wood-
scape: A multi-task, multi-camera ﬁsheye dataset for autonomous driving. In ICCV,
pages 9308–9318, 2019.

[39] Huizhong Zhou, Benjamin Ummenhofer, and Thomas Brox. Deeptam: Deep tracking

and mapping. In ECCV, pages 822–838, 2018.

[40] Tinghui Zhou, Matthew Brown, Noah Snavely, and David G Lowe. Unsupervised
learning of depth and ego-motion from video. In CVPR, pages 1851–1858, 2017.

[41] Yuliang Zou, Zelun Luo, and Jia-Bin Huang. DF-Net: Unsupervised joint learning of

depth and ﬂow using cross-task consistency. In ECCV, 2018.

[42] Yuliang Zou, Pan Ji, Quoc-Huy Tran, Jia-Bin Huang, and Manmohan Chandraker.
Learning monocular visual odometry via self-supervised long-term modeling. arXiv
preprint arXiv:2007.10983, 2020.

