8
1
0
2

n
a
J

7
1

]

R
S
.
h
p
-
o
r
t
s
a
[

1
v
4
4
7
5
0
.
1
0
8
1
:
v
i
X
r
a

Solar Physics
DOI: 10.1007/•••••-•••-•••-••••-•

Forecasting Solar Flares Using Magnetogram-based
Predictors and Machine Learning

·

Kostas Florios1,2
Ioannis Kontogiannis1
Sung-Hong Park3
Federico Benvenuto4
Shaun Bloomﬁeld5
K. Georgoulis1

·

· D.
· Manolis

· Jordan A. Guerra3

·

c(cid:13) Springer ••••

Abstract We propose a forecasting approach for solar ﬂares based on data from
Solar Cycle 24, taken by the Helioseismic and Magnetic Imager (HMI) on board
the Solar Dynamics Observatory (SDO) mission. In particular, we use the Space-
weather HMI Active Region Patches (SHARP) product that facilitates cut-out
magnetograms of solar active regions (AR) in the Sun in near-realtime (NRT),

B K. Florios

cﬂorios@aueb.gr

I. Kontogiannis
jkonto@noa.gr

S-H. Park
sunpark@tcd.ie

J.A. Guerra
guerraaj@tcd.ie

F. Benvenuto
benvenuto@dima.unige.it

D.S. Bloomﬁeld
shaun.bloomﬁeld@northumbria.ac.uk

M.K. Georgoulis
manolis.georgoulis@academyofathens.gr

1 Research Center for Astronomy and Applied Mathematics, Academy of Athens,

Greece

2 Department of Statistics, Athens University of Economics and Business, Greece

3

School of Physics, Trinity College Dublin, Ireland

4 Dipartimento di Matematica, Universit`a di Genova, Italy

5 Northumbria University, Newcastle upon Tyne, NE1 8ST, UK

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 1

 
 
 
 
 
 
K. Florios et al.

taken over a ﬁve-year interval (2012 – 2016). Our approach utilizes a set of thir-
teen predictors, which are not included in the SHARP metadata, extracted from
line-of-sight and vector photospheric magnetograms. We exploit several Machine
Learning (ML) and Conventional Statistics techniques to predict ﬂares of peak
magnitude >M1 and >C1, within a 24 h forecast window. The ML methods used
are multi-layer perceptrons (MLP), support vector machines (SVM) and random
forests (RF). We conclude that random forests could be the prediction technique
of choice for our sample, with the second best method being multi-layer percep-
trons, subject to an entropy objective function. A Monte Carlo simulation showed
that the best performing method gives accuracy ACC=0.93(0.00), true skill
statistic TSS=0.74(0.02) and Heidke skill score HSS=0.49(0.01) for >M1 ﬂare
prediction with probability threshold 15% and ACC=0.84(0.00), TSS=0.60(0.01)
and HSS=0.59(0.01) for >C1 ﬂare prediction with probability threshold 35%.

Keywords: Flares, Forecasting; Flares, Relation to Magnetic Field; Active
Regions, Magnetic Fields

1. Introduction

Solar ﬂares are sudden brightenings that occur in the solar atmosphere and
release enormous amounts of energy, over the entire electromagnetic spectrum.
Flares are quite prominent in X-rays, UV, and optical lines (Fletcher et al., 2011)
and they are often (but not always) accompanied by eruptions that eject solar
coronal plasma into the interplanetary space (coronal mass ejections, CMEs).
These very intense phenomena - the largest explosions in the solar system - are
associated with regions of enhanced magnetic ﬁeld, called active regions (AR)
and are associated, in white light, with sunspot groups. Depending on their peak
X-ray intensity, as recorded by the National Oceanic and Atmospheric Admin-
istration’s (NOAA) Geostationary Operational Environmental Satellite (GOES)
system, ﬂares are categorized in classes, the strongest and most important being
X, M and C (in decreasing order). Flare classiﬁcation is logarithmic, with a base
of 10, and is complemented by decimal sub-classes (e.g. M5.0, C3.2 etc.).

The solar ﬂare radiation may be detrimental to infrastructures, instruments
and personnel in space, therefore ﬂare forecasting is an integral part of con-
temporary space-weather forecasting. Forecast mainly employs measurements of
the AR magnetic ﬁeld in the solar photosphere. Magnetic-ﬁeld-based predictors
represent AR magnetic complexity or the energy budget available to power ﬂares.
Recent developments in instrumentation have led to a regular production of
such measurements oﬀering the opportunity to produce extensive databases with
properties suitable for solar ﬂare prediction.

On the other hand, machine learning in recent years has become an increas-
ingly popular approach for performing computer cognition tasks which were
inherently possible only using human intelligence. Thus, machine learning (ML)
is a subﬁeld of artiﬁcial intelligence (AI) and it aims at using past data in order
to train computers so that they can apply the accumulated knowledge to new,
previously unseen, data. The acquisition of knowledge is the training phase and

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 2

Forecasting Solar Flares Using Machine Learning

the application of what was learned to future scenarios is the prediction phase.
Typically, ML is more interested in prediction than conventional statistics. ML
can also interface with conventional statistics in a ﬁeld called statistical learning
(Hastie, Tibshirani, and Friedman, 2009). Learning is either called supervised or
unsupervised depending on whether it is done with a teacher or not. Supervised
learning comprises regression and classiﬁcation, while unsupervised learning is
also called clustering. In our study, we focus on classiﬁcation, where a set of input
variables or predictors belongs to one of two classes (binary classiﬁcation). ML
is more powerful than traditional statistical techniques such as, say, generalized
linear models that include probit, logit, etc. for binary classiﬁcation, because it
can help model more complex nonlinear relationships. An introduction to ML
research can be found in several textbooks (MacKay, 2003; Hastie, Tibshirani,
and Friedman, 2009).

Several researchers have recently used ML techniques to eﬀectively forecast
solar ﬂares. More often, the techniques used by researchers were: neural networks
(Wang et al., 2008; Yu et al., 2009; Colak and Qahwaji, 2009; Ahmed et al.,
2013), support vector machines (Li et al., 2008; Yuan et al., 2010; Bobra and
Couvidat, 2015; Boucheron, Al-Ghraibah, and McAteer, 2015), ordinal logistic
regression (Song et al., 2009), decision trees (Yu et al., 2009) and relevance vector
machines (Al-Ghraibah, Boucheron, and McAteer, 2015). Very recently, random
forests have also been used (Barnes et al., 2016; Liu et al., 2017).

We use predictors calculated from near-realtime (NRT) Space-weather HMI
Active Region Patches (SHARP) data combined with state-of-the-art ML and
statistical algorithms in order to eﬀectively forecast ﬂare events for an arbitrarily
chosen 24-hour forecast window. Flare magnitudes of interest are >M1 and >C1.
Prediction is binary, meaning that a given ﬂare class is considered to either hap-
pen or not within the next 24 hours after prediction. Our predictions are eﬀective
immediately, therefore with zero latency. Analysis involves a comprehensive NRT
SHARP sample including all calendar days between years 2012 and 2016, at a
cadence of 3 hours. Results in this work summarize the ﬁndings of the ﬁrst
eighteen months of the “Flare Likelihood And Region Eruption foreCASTing”
(FLARECAST) project and, while based on ongoing work, we took every eﬀort
to present robust and unbiased results.

The contribution of the present work is twofold:

•

•

The utilization of novel magnetogram-based predictors in a multi-parameter
solar ﬂare prediction model.
The utilization of classic and novel ML techniques, such as multi-layer
perceptrons (MLP), support vector machines (SVM) and especially, for
one of the ﬁrst times1, random forests (RF), for the forecasting of >M1
and >C1 ﬂares.

1 In June 2017, we noticed a manuscript by Liu et al. (2017) which also uses the random forest
algorithm for solar ﬂare prediction using SDO/HMI data. Nevertheless, the speciﬁc details in
that paper regarding the sampling strategy and the feature extraction are very diﬀerent from
our choices. For example, in Liu et al. (2017) only ﬂaring ARs (at the level >B1 class) were
considered and the sample size was N=845, while in our paper we consider both ﬂaring and
non-ﬂaring ARs with N=23,134.

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 3

K. Florios et al.

For the interested reader, the application code is available at http://dx.doi.org/
10.17632/4f6z2gf5d6.1, along with the benchmark dataset used in this work. The
run time for all methods is of the order of few minutes.

The analysis presented here is part of the EU Horizon 2020 FLARECAST
project, aiming to develop a NRT online forecasting system for solar ﬂares. The
study is organized as follows: Section 2 describes the data selected to train and
test the algorithms and presents the predictors used, together with background
information on the solar physics aspects of magnetogram-based calculations.
Section 3 describes the ML algorithms in terms of their core principles, along
with some additional remarks and comments. Section 4 is devoted to the forecast
experiments and a comparison with similar published results and statistics. Sec-
tion 5 presents the main conclusions and future integration of the present work
in the FLARECAST operational system. Four Appendices, describing multiple
complementary aspects of this work are also included.

2. Data and Classiﬁcation Predictors

2.1. Data

The Helioseismic and Magnetic Imager (HMI; Scherrer et al., 2012) on board
the Solar Dynamics Observatory (SDO; Pesnell, Thompson, and Chamberlin,
2012), provides regular full-disk solar observations of the three components of
the photospheric magnetic ﬁeld. The HMI team has created the Space Weather
HMI Active Region Patches (SHARPs), which are cut-outs of solar regions-of-
interest along with a set of parameters potentially useful for solar ﬂare prediction
(Bobra et al., 2014). For our analysis, we use the near-realtime (NRT), cylindrical
equal area (CEA) SHARP data to calculate a set of predictors.

To associate SHARPs with ﬂare occurrence we use the Geostationary Op-
erational Environmental Satellite (GOES) soft X-ray measurements. For each
SHARP we search for ﬂares within the next 24 hours by either matching the
NOAA AR numbers with those of the recorded ﬂares or by comparing the
corresponding longitude and latitude ranges, considering also the diﬀerential
solar rotation.

The algorithms of Section 3 are tested on a sample of the 2012-2016 SHARP
dataset. We consider all days in the period October 1, 2012 to January 13, 2016
and for every given day we compute the set of predictors (see Section 2.2) at
a cadence of 3 hours, starting at 00:00 UT. For our analysis, only SHARP cut-
outs that correspond to NOAA ARs are considered. In this way, we get a fairly
representative sample of the solar activity including several ﬂares of interest,
with a suﬃciently high sampling frequency.

2.2. Predictors

The set of thirteen predictors consists of both predictors already proposed in
the literature and new ones, and comprises a subset of the parameter set de-
veloped for the FLARECAST project. In Figure 1 we show two sample mag-
netograms to demonstrate how the predictors reﬂect the complexity and size of

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 4

Forecasting Solar Flares Using Machine Learning

Figure 1. Two SHARP frames depicting AR with very diﬀerent levels of ﬂaring activity.
NOAA AR 11875 (left) produced 7 C-, 0 M- and 0 X-class ﬂares within 24h while NOAA
AR 11923 (right) produced no ﬂares. The two AR are scaled so as to retain their original relative
size and, for comparison, vectors of the seven predictors used are included in the frames. The
names of all K = 7 predictors [logR, FSPI, TLMPIL, DI, WLSG, IsinEn1, IsinEn2] are deﬁned
in Section 2.2. High values of the predictors statistically indicate a powerful AR (left), with
low values indicating a quiescent, ﬂare-quiet AR (right).

the corresponding active region. The predictors utilized for this study are the
following:

2.2.1. Magnetic Polarity Inversion Line (TLMPIL)

A magnetic polarity inversion line (MPIL) in the photosphere of an AR separates
distinct patches of positive- and negative-polarity magnetic ﬂux. Several studies
have been carried out to investigate the relationship between ﬂare occurrence
and MPIL characteristics (Schrijver, 2007; Falconer et al., 2012). We determine a
speciﬁc subset of a MPIL, that has been also identiﬁed as MPIL*, with i) a strong
gradient in the vertical component of the ﬁeld across the MPIL and ii) a strong
horizontal component of the ﬁeld around the MPIL. MPIL* has been considered
as the single most likely place in AR where potential magnetic instabilities, such
as, say, magnetic ﬂux cancellation and/or magnetic ﬂux rope formation (Fang
et al., 2012) can take place. Such processes seem intimately related to ﬂares.
We use the total length Ltot of MPIL* segments in active regions as an MPIL
quantiﬁcation parameter.

2.2.2. Decay Index (DI)

The decay index is a quantitative measure for the torus magnetic instability
in a current-carrying magnetic ﬂux rope (Kliem and T¨or¨ok, 2006). It has been
found that the larger the value of decay index in AR magnetic ﬁelds, the more
likely it is to obtain a solar eruption involving a major solar ﬂare (Zuccarello,
Aulanier, and Gilchrist, 2015). We developed a decay index parameter derived
by the ratio Lhs/hmin, where Lhs is the length of a highly sheared portion of
a MPIL and hmin is the minimum height at which the decay index achieves a

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 5

K. Florios et al.

purported critical value of 1.5. This ratio can be used to measure the degree of
instability in a ﬂux rope. Notice that if there are more than one MPIL in an AR,
then we calculate the ratio Lhs/hmin for every MPIL and take the peak value
for a given time, that represents the highest eruptive potential of the AR.

2.2.3. Gradient-weighted integral length of the neutral line (W LSG)

The gradient-weighted integral length of neutral line, WLSG, is deﬁned in Fal-
coner, Moore, and Gary (2008) as,

WLSG =

Bz)dl ,

(
∇

Z

(1)

and corresponds to the line integral of the vertical-ﬁeld (Bz) horizontal gradient
over all neutral line (or MPIL) segments on which the potential horizontal ﬁeld
is greater than 150 G. This MPIL-related property has been reported to show a
useful empirical association with the occurrence of solar eruptions (ﬂares, CMEs,
SPEs; Falconer et al., 2011, 2014) and is the main predictor used in the Magnetic
Forecast (MAG4) forecasting service, developed in the University of Alabama
(http://www.uah.edu/cspar/research/mag4-page).

For these calculations of WLSG, two approximations of the vertical ﬁeld Bz
are used: Blos (line of sight; uncorrected) and Br, keeping in mind that in former
case, only values for regions located within 30o from the central meridian are
considered accurate. For each magnetogram, a MPIL mask is determined as in
the calculation of MPIL characteristics, described previously. In order to select
the strong-horizontal ﬁeld segments of MPILs, the potential ﬁeld extrapolation
method developed by Alissandrakis (1981) is used. Finally, the horizontal gradi-
ent of Bz is calculated numerically and integrated over all MPIL segments. The
accuracy of the calculated values was estimated by comparing ﬂare rates derived
from our calculations of WLSG (using Equation 4 along with Table 1 values in
Falconer et al., 2011) with the ﬂare rates from the text output of MAG4.

2.2.4. Ising Energy (IsinEn1, IsinEn2)

The Ising energy is a quantity that parameterizes the magnetic complexity of
an AR (Ahmed et al., 2010). For a two-dimensional distribution of positive and
negative interacting magnetic elements, the Ising energy is deﬁned as,

EIsing =

SiSj
d2

,

− Xij

(2)

where Si (Sj) equals to +1 (-1) for positive (negative) pixels and d is the distance
between opposite polarity pairs. The interacting magnetic elements can be either
the individual pixels with a minimum ﬂux density value as in Ahmed et al. (2010)
or the opposite-polarity partitions, produced using a ﬂux-partitioning scheme
(Barnes, Longcope, and Leka, 2005). The latter variation is introduced for the
ﬁrst time in the FLARECAST project, with promising results and an assessment

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 6

Forecasting Solar Flares Using Machine Learning

of its merit as a predictor is underway (Kontogiannis et al., in preparation).
The Ising energy calculation produces four predictors, two for the line-of-sight
magnetic ﬁeld and two for the radial magnetic ﬁeld component.

2.2.5. Fourier Spectral Power Index (FSPI)

The spectral power index, α, corresponds to the power-law exponent in ﬁtting
the one-dimensional power spectral density E(k) extracted from magnetograms
by the relation,

E(k)

−α .

k

(3)

∼
This index parameterizes the power contained in magnetic structures of spa-
tial scales l (= k−1) belonging to the inertial range of magnetohydrodynamic
(MHD) turbulence. Empirically, AR with spectral power index higher than 5/3
(Kolmogorov’s exponent for turbulence) are thought to display an overall high
productivity of ﬂares (e.g. see Guerra et al., 2015).

The spectral power index has been historically calculated from the vertical
component of the photospheric magnetic ﬁeld, as inferred from the line-of-sight
component assuming perfectly radial magnetic ﬁelds. First, the magnetogram
is processed using the fast Fourier transform (FFT). A two-dimensional power
spectral density (PSD) is then obtained as,

E(kx, ky) =

|

F F T [B(x, y)]

2 .

|

(4)

x + k2

In order to express E(kx, ky) from the Fourier kx and ky to the isotropic
y)1/2, it is necessary to calculate E(k)′ – the inte-
wavenumber k = (k2
grated PSD over angular direction in Fourier space. From this last step, the
one-dimensional PSD is obtained as E(k) = 2πkE(k)′. Finally, the power-law
ﬁt is performed as a linear ﬁt in a logarithmic representation of E(k) vs. k and
α is measured for the assumed turbulent inertial range of 2-20 Mm (i.e. 0.05-0.5
Mm−1).

2.2.6. Schrijver’s R value (logR)

The R-value property quantiﬁes the unsigned photospheric magnetic ﬂux near
strong MPILs. The presence of such MPILs indicates that twisted magnetic
structures carrying electrical currents have emerged into the AR through the
solar surface. Therefore, R represents a proxy for the maximum free magnetic
energy that is available for release in a ﬂare. This property and its usefulness in
forecasting was ﬁrst investigated by Schrijver (2007).

The algorithm for calculating R is relatively simple, computationally inex-
pensive, and was originally developed to use line-of-sight magnetograms from
the Michelson Doppler Imager (MDI) (Scherrer et al., 1995) on board the Solar
and Heliospheric Observatory (SoHO). First, a bitmap is constructed for each
polarity in a magnetogram, indicating where the magnitude of positive and
150 Mx cm−2.
negative magnetic ﬂux densities exceeds the threshold value of

±

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 7

K. Florios et al.

≈

×

These bitmaps are then dilated by a square kernel of 3
3 pixels and the areas
where the bitmaps overlap are deﬁned as strong-ﬁeld MPILs. This combined
bitmap is then convolved with a Gaussian ﬁlter of full width at half maximum
(FWHM)
15 Mm. This particular value is constrained by how far from MPILs
ﬂares are observed to occur in extreme ultraviolet images of the solar corona.
Finally, the convolved bitmap is multiplied by the absolute ﬂux value of the line-
of-sight magnetogram and R is calculated as the sum over all pixels. Notice that
since the R value was implemented by Schrijver (2007) for MDI magnetograms,
the SHARP magnetograms were resampled to the spatial scale of MDI, before
the kernel application and subsequent calculations.

3. Machine Learning Algorithms and Conventional Statistics

Models

The ML algorithms used in this study are MLPs, SVMs and RFs. Among the
hundreds of ML algorithms proposed for binary classiﬁcation (e.g., Fern´andez-
Delgado et al., 2014) these three categories of algorithms are representative of
three important approaches in ML: i) artiﬁcial neural networks (ANN), ii) kernel-
based methods and iii) classiﬁcation and regression trees. This is the reason why
they were used in the present study, in order to furthermore investigate whether
the usage of RFs could bring any improvements in ﬂare prediction in comparison
to SVMs and MLPs. The RFs belong to the category of ensemble methods
while the MLPs utilize unconstrained optimization and SVMs use constrained
optimization techniques (e.g., quadratic programming). In general, the working
principle of ML comprises the following steps: i) train the model using a training
set, ii) predict using the trained model and a testing set and iii) check whether
the algorithm predicted well, in what is called the validation of the overal ML
procedure. For further study, the reader is referred to Vapnik (1998), MacKay
(2003) and Hastie, Tibshirani, and Friedman (2009).

3.1. Multi-Layer Perceptrons

The MLP is a feed-forward network, thus it is described by the planar graph
shown in Figure 2. It contains an input layer, a hidden layer and an output layer
of neurons. By the term neuron, we denote a basic processing unit where inputs
are summed using speciﬁc weights and the result is squashed via an activation
function. The hidden layer might actually expand in a series of hidden layers.
Nevertheless, the simplest MLP networks have just one hidden layer. In principle
the term hidden describes every layer which is neither the input nor the output
layer, but resides in between, as presented in Figure 2. A suﬃcient number of
hidden nodes allows the MLP to approximate any continuous nonlinear function
of several inputs with a desired degree of accuracy (Hornik, Stinchcombe, and
White, 1989), which is what characterizes the MLPs as universal approximators.
It also holds that the greater the number of hidden nodes is, the more complex
the nonlinear function that can be approximated by the neural network with a
desired degree of accuracy. Usually, the number of hidden nodes does not have

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 8

Forecasting Solar Flares Using Machine Learning

B1

B2

I1

I2

I3

I4

I5

I6

H1

H2

H3

H4

H5

H6

H7

H8

H9

H10

H11

H12

O1

Figure 2. Example MLP neural network with 6 inputs, 12 hidden nodes, 1 output and 2
biases. Bold, darker lines indicate large positive weights ω.

to be more than twice the number of input nodes (or predictors). Actually, if
too many hidden nodes are utilized, then the overﬁtting problem arises, which
means that the MLP memorizes the sample observations and generalizes badly
in the prediction phase. Usually, and in this study, the optimal number of hidden
neurons (called size of the MLP) is determined with a ﬁne-tuning procedure (e.g.
cross-validation approach, see Section 4.2) before the training phase starts. The
tuning phase is relatively time consuming, so it need not be executed every time
the training starts. It can be conducted for a single realization of the training
set.

An MLP network is actually a kind of a nonlinear regression (classiﬁcation)
technique, equivalent to a nonlinear mapping from input I to an output O =
O(I; ω, A). The output is a continuous function of the input and of the weights
ω. The network is described by a given architecture A, which typically deﬁnes
the number of nodes in every layer (e.g. input, hidden and output). In general,
MLP networks can be used to solve regression and classiﬁcation problems. The
statistical model of a MLP neural network for binary outcome, as described in the
following, is based on MacKay (2003). For a recent survey on neural networks,
the interested reader is referred to Prieto et al. (2016).

3.1.1. Classiﬁcation Networks

We consider a MLP with l inputs called Il and bias B1. Also the network contains
a single hidden layer with j hidden nodes Hj and bias B2. We have in general i
outputs Oi, while typically a single output is all that is needed (i = 1).

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 9

In the case of a classiﬁcation problem, the propagation of the information

from the inputs I to the output O is described by,

K. Florios et al.

α(1)
j =

jl Il + B(1)
ω(1)

j

; Hj = f (α(1)

j ) ,

α(2)
i =

ij Hj + B(2)
ω(2)

i

; Oi = g(α(2)

i

) ,

(5)

L

Pl=1
J

Pj=1

1

1
(1+exp(−α)) .

where, for example, f (α) =

(1+exp(−α)) and g(α) =
The index l is used for the inputs I1, . . . , IL, the index j is used for the hidden
units and the index i is used for the outputs (i = 1). The weights ωjl, ωij and
biases Bj, Bi deﬁne the parameter vector ω to be estimated. The nonlinear
logistic function f at the hidden layer (also known as activation function) helps
the neural network approximate any generic continuous nonlinear function with a
desirable degree of accuracy (Hornik, Stinchcombe, and White, 1989). Visually, a
neural network can be represented as a series of layers consisting of nodes, where
every node is connected to nodes of the subsequent layer only (feed forward
networks).

In the case of binary classiﬁcation, the MLP is trained using a dataset of
by adjusting ω in order to minimize G(ω), the

I (n), T (n)

examples D =
{
negative log-likelihood function,

}

G(ω) =

N

−(cid:16)

Xn=1

T (n)ln(O(I (n); ω)) + (1

T (n))ln(1

O(I (n); ω))
(cid:17)

.

−

−

(6)

Notice that I(n) is the matrix of the predictors and T (n) is the vector of the
targets for observation n = 1, . . . , N . In Equation 6, T (n) is 0 (1) for the
negative (positive) class, respectively, and O(I (n); ω) is strictly between 0 and
1 (a probability) a fact that is ensured by Equations 5.

3.2. Support Vector Machines

The SVM variant we use is the C-Support Vector Classiﬁcation (C-SVC) accord-
ing to the widely used library LIBSVM (Chang and Lin, 2011; Meyer, Leisch,
and Hornik, 2003).

Let us assume a vector of K predictor values at observation i, xi ∈
i = 1, . . . , N , which belongs in one of two classes, and an indicator vector y
∈
1
such that yi ∈ {
}
negative class has label

RK,
RN
. Notice that the positive class has label +1 and the
1. Then the C-SVC solves the optimization problem:

1,

−

−

minimize 1

2 ωT ω + C

N
i=1 ξi ,

P

subject to

yi(ωT φ(xi) + b)
ξi ≥

ξi,
≥
i = 1, 2, . . . , N ,

0,

−

1

i = 1, 2, . . . , N,

(7)

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 10

Forecasting Solar Flares Using Machine Learning

where φ(xi) is an arbitrary unknown function which maps xi into a higher
dimensional space and C > 0 is the regularization parameter. The optimiza-
tion in C-SVC model is performed by changing the decision variables: ω, b,
ξ. Actually, LIBSVM solves the dual of C-SVC which depends on a quantity:
K(xi, xj) = φ(xi)T φ(xj), which is called the kernel function. While the φ(xi) is
unknown, the kernel function is known and is equal to the inner product of φ(xi)
with itself but for diﬀerent pairs of observations i and j. This is the so-called
kernel trick of SVMs. As seen below, the kernel is a similarity measure and takes
the maximum value of 1 when dist(xi, xj) = 0.

We have used the Radial Basis Function (RBF) (or Gaussian) kernel which
2). A variant of the C-SVC model has

is deﬁned as K(x, x′) = exp(
γ
−
been used for ﬂare prediction in Bobra and Couvidat (2015).

x′

−

x

||

||

For imbalanced datasets which account for rare events (e.g., in our case
the >M1 ﬂares) some researchers e.g. Bobra and Couvidat (2015) have used
two diﬀerent values for the regularization parameter C in Equation 7, thereby
penalizing more the constraint violations for the minority class. These authors
, where C1 is the coeﬃcient
have used C1 and C2 with a ratio C2/C1 ∈ {
for the majority class (no events) and C2 is the coeﬃcient for the minority class
(events). While we generally use the SVM in the original unweighted version in
Equation 7, in auxiliary runs we experimented also with using diﬀerent values
to account for the imbalanced nature
C1 and C2 with a ratio C2/C1 ∈ {
of the >M1 ﬂares dataset.

2, 15, 20

2, 15

}

}

3.3. Random Forests

The RF is a relatively recent ML methodology, introduced by Breiman (2001).
The RF approach is an ensemble of tree predictors, where we let each tree vote
for the most popular class. It has been reported (Fern´andez-Delgado et al., 2014)
that RF oﬀers signiﬁcant performance improvement over other classiﬁcation
algorithms. The RF approach relies on randomness and involves the concept
of split purity and the Gini index for variable selection (Breiman et al., 1984).
According to Hastie, Tibshirani, and Friedman (2009), the goal of the RF
algorithm is to randomly build a set (or ensemble) of trees, by repeating the
tree-formation process B times to create B trees. In particular, the algorithm:
i) chooses a bootstrap sample from the training data, ii) grows a tree Tb to
the bootstrapped sample by applying consequently the following two substeps:
Substep 1, select m variables randomly out of the M variables, and Substep 2,
split the current node into two children nodes, having picked the best variable
(node) from the m chosen ones. By repeating steps i) and ii) (where ii) consists
B
1 .
of Substeps 1 – 2), the algorithm creates a set (called ensemble) of trees
Then, in the classiﬁcation case studied in the present paper, a voting procedure
for every tree Tb is followed in order to obtain the class prediction of the random
forest.

Tb}

{

This is one of the ﬁrst times RF is used for ﬂare forecasting. Other related
works are Liu et al. (2017) and Barnes et al. (2016). Furthermore, three recent
applications of RF in astrophysics are by (Vilalta, Gupta, and Macri, 2013;
Schuh, Angryk, and Martens, 2015; Granett, 2017).

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 11

K. Florios et al.

3.4. Implementation of ML algorithms

3.4.1. Multi-layer Perceptrons

MLPs were implemented using the R programming language and the nnet pack-
age (Venables and Ripley, 2002). The options used were: linout=FALSE, to
ensure that sigmoid activation functions are used at the output node, entropy =
TRUE, to ensure that the negative log-likelihood objective function is minimized
during the training phase (and not the default Sum of Squares Error (SSE)
criterion), and size=iNode, where iNode for both >M1 ﬂares and for >C1 ﬂares
was chosen with a tuning procedure.

3.4.2. Support Vector Machines

SVMs were implemented using the R programming language and the e1071
package (Meyer et al., 2015). The options used were: probability=TRUE, in
order to obtain probability estimates for every element of the training set as
well as probability estimates for every element of the testing set.

3.4.3. Random Forests

RFs were implemented using randomForest package (Liaw and Wiener, 2002) in
the R programming language. The options used were: importance = TRUE, to
create importance information for every predictor, na.action=na.omit, to exclude
records of predictors with missing values appearing in preliminary versions of
the dataset (but lacking from the ﬁnal version of the dataset).

3.5. Conventional Statistics Models

Non-ML (or statistical) methods also considered are: i) linear regression (LM), ii)
probit regression (PR) and iii) logit regression (LG). Although multiple linear
regression is known to be redundant for binary outcomes, since it can yield
probabilistic predictions outside the interval [0, 1], we still include it in the array
of tested methods. The reason is that some practitioners still use it for binary
outcomes (calling it linear probability model (LPM), see Greene (2002)) and
there is always interest to consider ordinary least squares (OLS) as an entry-
level method for any regression analysis. An interesting article about the lack of
use of probit and logit in astrophysics modeling is de Souza et al. (2015). The
statistical algorithms were implemented in the statistical programming language
R using the lm and glm functions.

For a description of these well known methods the reader is referred to

(Greene, 2002; Winkelmann and Boes, 2006).

4. Data preparation, Results and Discussion

First, we implement ML predictions on >M1 ﬂares. Second, we use statistical
methods for the prediction of >M1 ﬂares. Third, we predict >C1 ﬂares with

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 12

Forecasting Solar Flares Using Machine Learning

ML algorithms. Finally, we predict >C1 ﬂares with the statistical algorithms.
The following subsections describe these four experiments, presenting at ﬁrst a
single combination of training/testing set for every ﬂare class and category of
techniques.

Results are presented for the prediction step in terms of: i) skill scores proﬁles
(SSP) of ACC, TSS and HSS as functions of the probability threshold, ii) ROC
curves, and iii) RD plots for all methods: (for the explanation of metrics ACC,
TSS, HSS as well as ROC curves and RD diagrams – see following Section 4.3).
Skill score proﬁles were created by a code we developed in R, ROC curves were
created using the ROCR package (Sing et al., 2005), while reliability diagrams
were created using the veriﬁcation package (Laboratory, 2015).

All algorithms were implemented and run using the R programming language

3.3.2 R Core Team (2016) and the RStudio 0.99 IDE.

4.1. Data Pre-processing

×

The data comprise the K = 7 predictors [logR, FSPI, TLMPIL, DI, WLSG,
IsinEn1, IsinEn2] described in Section 2.2 and computed using either the line-
of-sight magnetograms, Blos, of SHARP data or the respective radial component,
6 + 1 = 13 predictors2.
Br (Bobra et al., 2014). Hence, we test K = 2
The sample comprises N = 23, 134 observations, randomly split in half into
N1 = 11, 567 observations for the training, and N2 = 11, 567 observations for
the testing set. The random split is performed for 200 replications and all six
prediction algorithms (i.e. MLP, SVM, RF, LM, probit and logit) of Section 3
are trained and perform on identical training and test sets. The metrics ACC,
TSS and HSS of Section 4.3 are computed always for the testing (out-of-sample)
set. We have standardized all predictor variables to have mean equal to 0 and
standard deviation equal to 1, because several ML algorithms involve non-linear
optimization (e.g. MLPs). This helps to better train the ML algorithms and also
explains the eﬀect of every predictor variable on the studied outcome in the case
of the statistical models LM, probit and logit.

4.2. Tuning of ML algorithms

As with any parameterized algorithm (e.g. simulated annealing, evolutionary
algorithms, and other metaheuristics), the performance of ML algorithms de-
pends on a number of crucial parameters which need to be ﬁne tuned before
the application of the ML procedure (e.g. training, testing and validation steps).
The optimal tuning of ML algorithms is more or less still an open question in
the ML community and always poses a big challenge for any practitioner. This
choice of optimal options for the ML algorithms themselves is similar to the
choice of optimal parameters for other numerical models, (e.g. MHD models),
where the analyst also has to explore the optimal parameter space in several cru-
cial parameters before conducting numerical MHD simulations. The algorithms

2 This is because, for predictor WLSG, we considered only the Br version.

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 13

K. Florios et al.

∈ {

}

∈ {⌊

⌋

}

MLP, SVM and RF have their critical hyperparameters (e.g. parameters that are
critical for the forecasting performance of every algorithm) tuned via a 10-fold
cross-validation study exploiting only the training set at one of its realizations.
The set of plausible values for every ML algorithm is as follows: i) MLP: size
and decay (weight decay parameter)
(number of hidden neurons)
, ii) SVM: γ (parameter in the RBF (or Gaussian) kernel)
∈ {
10, 100
and cost (regularization parameter)
}
∈ {
and iii) RF: mtry (number of variables randomly sampled as candidates at each
and ntree (number of trees to grow)
split)

10−3, 10−2, 10−1
10−6, 10−5, 10−4, . . . , 10−1

4, 13, 26

√K

= 3

∈ {

500

∈ {

}

}

}

.

Actually, we have tuned only the MLP and SVM classiﬁers, because the
default RF values mtry=3 and ntree=500 immediately provided satisfactory
results. Tuning of the MLP and SVM was mostly needed in the >M1 ﬂares
case, that was found harder to predict than >C1 ﬂares, but was also performed
in the >C1 ﬂares case. Thus, the hyperparameters for MLP and SVM needed
tuning since, for example, the default values γ = 1 and cost = 1 for SVM provided
unsatisfactory results. We have used the tune.nnet and tune.svm functions of the
R package e1071 for tuning the MLP and SVM, respectively. After the tuning,
both MLP and SVM improved their performance signiﬁcantly.

For the >M1 ﬂares, the selected values are size = 26 and decay = 0.1 for the
MLP and γ = 0.1 and cost = 10 for the SVM. These values are used throughout
the remainder of this work. For the >C1 ﬂares case, the selected values are size
= 4 and decay = 0.1 for the MLP and γ = 0.001 and cost = 100 for the SVM.

4.3. Comparison Metrics

A wide variety of metrics exist in order to characterize the quality of binary
classiﬁcation. Among these, no single one is ﬁt for all purposes. There exist two
types of metrics, suitable for either categorical or probabilistic classiﬁcation. In
the former case a strict class membership is returned from the model and in the
latter case a probability of membership is returned. In this section we concentrate
on categorical forecast metrics for binary classiﬁcation. In what follows, let ACC
denote accuracy, TSS denote true skill statistic and HSS denote Heidke skill
score. The performance of algorithms is measured using a number of metrics.
These are derived from the so-called contingency table or confusion matrix, a
representation of which is provided in Table 1:

Table 1. 2×2 contingency table for
binary forecasting

ACTUAL

PREDICT
NO
YES

NO
TN
FP

YES
FN
TP

Table 1 includes true positives (TP; events predicted and observed), true neg-
atives (TN; events not predicted and not observed), false positives (FP; events

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 14

Forecasting Solar Flares Using Machine Learning

predicted but not observed) and false negatives (FN; events not predicted but
observed), where N = TP + FP + FN + TN is the sample size. From these
elements:
The meaning of ACC is the proportion correct, namely the number of correct
forecasts of both event and non-event, normalized by the total sample size,

ACC =

TP + TN
N

.

(8)

The TSS (Hanssen and Kuipers, 1965) compares the probability of detection
(POD) to the probability of false detection (POFD),

TSS = POD

POFD =

−

TP

TP + FN −

FP
FP + TN

.

(9)

Moreover, the TSS is the maximum vertical distance from the diagonal in the
ROC curve, that relates the POD and POFD for diﬀerent probability thresholds
– see Section 4. The TSS covers the range from
1 up to +1, while the value of
zero indicates lack of skill. Values below zero are linked to forecasts behaving in
a contrarian way, namely mixing the role of the positive class with the role of the
negative class. In any negative TSS value, by exchanging the roles of YES and
NO events, we can obtain the corresponding positive TSS value which would be
identical in absolute value terms with the negative TSS value.
The HSS (Heidke, 1926) measures the fractional improvement of the forecast
over the random forecast,

−

HSS =

2(TP

TN

FP

FN)

−
(TP + FN)(FN + TN) + (TP + FP)(FP + TN)

×

×

,

(10)

−∞

which ranges from
to 1. Any negative value means that the random forecast
is better, a zero value means that the method has no skill over the random
forecast, and an ideal forecast method provides a HSS value equal to 1.
The TSS and HSS metrics are among the most popular metrics for comparison
purposes in Meteorology and Space Weather and were conceptually compared
in Bloomﬁeld et al. (2012). In a probabilistic forecasting, such as the one for
solar ﬂares, they must be assigned a probability threshold, thus appearing as
functions of this threshold.

To summarize, ACC is the most popular classiﬁcation metric, but in rare
events such as ﬂares >M1, the ACC can be artiﬁcially high for the naive model
which will always predict the majority class (“no event”). Thus, TSS and HSS
are more suitable for ﬂare prediction. Moreover, TSS has the advantage of being
invariant to the frequency of events in a sample (e.g. see Bloomﬁeld et al.,
2012). Typically, both TSS and HSS need to be evaluated, for a given probability
threshold, in order to assess the merit of a given probabilistic forecasting model,
such as the ones we develop in this study.

Regarding the probabilistic assessment of classiﬁers, the present study utilizes
the visual approaches of Receiver Operating Characteristic (ROC) curves and

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 15

K. Florios et al.

Reliability Diagrams (RD) (e.g. see Section 4). The ROC describes the relation-
ship between the POD and the POFD for diﬀerent probability thresholds (e.g.
see Figure 3b). The Area Under the Curve (AUC) in the ROC has an ideal value
of one. The RD describes the relationship between the returned probabilities by
the model and the actual observed frequencies of the data. A binning approach
is used to construct the RD, in which probabilities are assigned to intervals of
arbitrary length (for example we use 20 bins of length 0.05 each). For an example
of RD, see Figure 3c. Also, to algebraically assess the probabilistic performance
of classiﬁers, we use the Brier Score (BS) (Brier, 1950) and Brier Skill Score
(BSS) (Wilks, 2011), as well as the AUC (Marzban, 2004).

4.4. Results on >M1 Flare Prediction

4.4.1. Prediction of >M1 Flare Events Using Machine Learning

Figure 3 shows the forecast performances of the three tested ML methods, using
both binary scores (SSP [left]; ROC [middle]) and probabilistic ones (RD [right]).
In particular:

i) Regarding the MLPs, we notice a wide plateau with more-or-less ﬂat proﬁle
for HSS and less so for TSS. This occurs because the number of hidden
neurons (size=26) is twice the number of input neurons, causing the MLP
to provide probability estimates clustered around 0 and 1. The ROC curve
is reasonably good, with maximum TSS=0.726. Moreover, the RD shows a
systematic over-prediction above a forecast probability of 0.4.

ii) For the SVMs, the SSP plateau noticed in case of the MLPs is not present
here, with nearly monotonically decreasing values of TSS and HSS ap-
pearing. The ROC curve shows a maximum TSS=0.629, while the RD
seems slightly better than for MLP, with some under-prediction below a
forecast probability of 0.4 and generally large uncertainties. When we use
the weighted version of the SVM, with a ratio of C2/C1 = 20, then the
ROC curve improves providing a maximum TSS= 0.718, but the overall
forecasting ability as measured by the SSP and RD remains worse than the
MLP.

iii) With respect to the RFs, the SSP behaviour is such that HSS shows a
plateau around its peak value, albeit smaller than in case of MLPs, while
TSS monotonically decreases. This said, notice that the peak HSS and TSS
values are higher in this case (e.g. TSS=0.780 and HSS=0.587). The ROC
curve is better than that of MLPs and SVMs with a maximum TSS=0.780.
The RD, ﬁnally, appears clearly better than those of MLPs and SVMs,
presenting some mild under-prediction, mainly within error bars, above a
forecast probability of 0.2.

4.4.2. Prediction of >M1 Flare Events Using Statistical Models.

Figure 4 shows the forecast performances of the three tested statistical methods,
for >M1 ﬂare prediction. In particular:

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 16

Forecasting Solar Flares Using Machine Learning

Regarding the LM, the SSP is diﬀerent between TSS and HSS, with TSS
peaking more impulsively and for smaller probabilities and then decreasing
nearly monotonically. The ROC curve shows also a signiﬁcant performance with
maximum TSS=0.744 that can also be seen in the RD, which shows a very good
behavior, albeit with error bars, for the entire range of forecast probabilities.

As far as the PR is concerned, a slightly improved behavior in comparison
with LM can be seen here, for the SSP, the ROC curves and the RD. The RD,
also, seems more reliable in this case compared to LM, although diﬀerences are
mostly within error bars.

For the LG, we notice a similar behavior as in the LM and especially PR

method, and the RD in this case appears as good as the PR RD.

4.4.3. Monte Carlo Simulation for >M1 Flares

±

±

0.00, TSS=0.63

In Table 2 we provide the average values of the skill scores ACC, TSS and HSS for
all prediction methods after the 200 replications of the Monte Carlo experiment
regarding >M1 ﬂares prediction. We notice from Table 2 that the maximum
HSS=0.57 is obtained with the RF method for a probability threshold of 25%.
The corresponding RF score values are ACC=0.96
0.02 and
0.02. The second best method in Table 2 for the same probability
HSS=0.57
threshold is MLP, with ACC=0.95
0.02.
Considering the threshold where the maximum TSS is observed, we get the
optimal results for method RF and threshold 10%, with values ACC=0.90
0.00,
0.01. The second best method may be con-
TSS=0.77
sidered the LM at 10% threshold with ACC=0.88
0.01 and
HSS=0.35
0.01. The diﬀerence between RF and LM is statistically signiﬁcant
at 0.01% level as shown in Table 4 at row 1. For the range of thresholds 10% to
25% the method RF yields increasing values of HSS and decreasing values of TSS.
For example, an appealing forecasting model could be RF with threshold 15%
0.01 in Table 2,
and metrics ACC=0.93
but this would depend on the needs and requirements of a given decision maker.

0.01 and HSS=0.42

0.02 and HSS=0.49

0.02 and HSS=0.50

0.00, TSS=0.73

0.00, TSS=0.74

0.00, TSS=0.56

±

±

±

±

±

±

±

±

±

±

±

±

±

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 17

MLP, SSP

MLP, ROC

MLP, RD

K. Florios et al.

s
e
u
a
v

l

s

l
l
i

k
s

s
e
u
a
v

l

s

l
l
i

k
s

s
e
u
a
v

l

s

l
l
i

k
s

s
e
u
a
v

l

s

l
l
i

k
s

0
1

.

8
0

.

6
0

.

4

.

0

2
0

.

0
0

.

0
1

.

8
0

.

6
0

.

4

.

0

2
0

.

0
0

.

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

acc
tss
hss

0.0

0.2

0.4

0.6

0.8

1.0

probability threshold

(a) MLP, SSP

SVM, SSP

acc
tss
hss

0.0

0.2

0.4

0.6

0.8

1.0

probability threshold

(d) SVM, SSP

SVM, SSP

acc
tss
hss

t

e
a
r

e
v
i
t
i
s
o
p
e
u
r
T

t

e
a
r

e
v
i
t
i
s
o
p
e
u
r
T

e
t
a
r
e
v
i
t
i
s
o
p

e
u
r
T

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

0
1

.

8
0

.

6
0

.

4
0

.

2
0

.

0
0

.

0.0

0.2

0.4

0.6

0.8

1.0

False positive rate

(b) MLP, ROC

SVM, ROC

0.0

0.2

0.4

0.6

0.8

1.0

False positive rate

(e) SVM, ROC

SVM, ROC

y
c
n
e
u
q
e
r
f

y
c
n
e
u
q
e
r
f

y
c
n
e
u
q
e
r
f

0
1

.

8
0

.

6

.

0

4

.

0

2
0

.

0
0

.

0
1

.

8
0

.

6
0

.

4

.

0

2
0

.

0
0

.

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

0.0

0.2

0.4

0.6

0.8

1.0

probability

(c) MLP, RD

SVM, RD

0.0

0.2

0.4

0.6

0.8

1.0

probability

(f) SVM, RD

SVM, RD

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

0.6

0.8

1.0

probability threshold

False positive rate

probability

(g) SVM weighted, SSP

(h) SVM weighted, ROC

(i) SVM weighted, RD

RF, SSP

RF, ROC

RF, RD

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

acc
tss
hss

e
t
a
r

e
v
i
t
i
s
o
p
e
u
r
T

0
1

.

8

.

0

6
0

.

4
0

.

2
0

.

0
0

.

y
c
n
e
u
q
e
r
f

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

0.6

0.8

1.0

probability threshold

(j) RF, SSP

False positive rate

(k) RF, ROC

probability

(l) RF, RD

Figure 3. ML methods comparison for >M1 GOES ﬂares prediction for (from top to bottom)
MLP, SVM, weighted SVM and RF. From left to right we present the corresponding SSP,
ROC and RD.

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 18

 
 
 
 
 
 
 
 
 
 
 
 
Forecasting Solar Flares Using Machine Learning

LM, SSP

LM, ROC

LM, RD

s
e
u
a
v

l

s

l
l
i

k
s

s
e
u
a
v

l

s

l
l
i

k
s

s
e
u
a
v

l

s

l
l
i

k
s

0
1

.

8
0

.

6
0

.

4

.

0

2
0

.

0
0

.

0
1

.

8
0

.

6
0

.

4

.

0

2
0

.

0
0

.

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

acc
tss
hss

0.0

0.2

0.4

0.6

0.8

1.0

probability threshold

(a) LM, SSP

PR, SSP

acc
tss
hss

0.0

0.2

0.4

0.6

0.8

1.0

probability threshold

(d) PR, SSP

LG, SSP

acc
tss
hss

t

e
a
r

e
v
i
t
i
s
o
p
e
u
r
T

t

e
a
r

e
v
i
t
i
s
o
p
e
u
r
T

e
t
a
r
e
v
i
t
i
s
o
p

e
u
r
T

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

0
1

.

8
0

.

6
0

.

4
0

.

2
0

.

0
0

.

0.0

0.2

0.4

0.6

0.8

1.0

False positive rate

(b) LM, ROC

PR, ROC

0.0

0.2

0.4

0.6

0.8

1.0

False positive rate

(e) PR, ROC

LG, ROC

y
c
n
e
u
q
e
r
f

y
c
n
e
u
q
e
r
f

y
c
n
e
u
q
e
r
f

0
1

.

8
0

.

6

.

0

4

.

0

2
0

.

0
0

.

0
1

.

8
0

.

6
0

.

4

.

0

2
0

.

0
0

.

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

0.0

0.2

0.4

0.6

0.8

1.0

probability

(c) LM, RD

PR, RD

0.0

0.2

0.4

0.6

0.8

1.0

probability

(f) PR, RD

LG, RD

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

0.6

0.8

1.0

probability threshold

(g) LG, SSP

False positive rate

(h) LG, ROC

probability

(i) LG, RD

Figure 4. Same as Figure 3, but for statistical methods: linear regression (LM; top), probit
regression (PR; middle), and logit regression (LG; bottom).

4.5. Results on >C1 Flare Prediction

4.5.1. Prediction of >C1 Flare Events Using Machine Learning

We continue our computational experiments by training and performing our
algorithms to the prediction of GOES >C1 ﬂares. Figure 5 shows the forecast
performances of the three tested ML methods, for >C1 ﬂare prediction. In
particular:

Regarding the MLP, we notice that since for the >C1 ﬂares the number of
hidden nodes selected is size=4, plateaus in HSS and TSS are not so eminent,
contrary to the case of >M1 ﬂare prediction. The ROC curve seems satisfac-

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 19

 
 
 
 
 
 
 
 
 
MLP, SSP

MLP, ROC

MLP, RD

K. Florios et al.

s
e
u
a
v

l

s

l
l
i

k
s

s
e
u
a
v

l

s

l
l
i

k
s

s
e
u
a
v

l

s

l
l
i

k
s

0
1

.

8
0

.

6
0

.

4

.

0

2
0

.

0
0

.

0
1

.

8
0

.

6
0

.

4

.

0

2
0

.

0
0

.

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

acc
tss
hss

0.0

0.2

0.4

0.6

0.8

1.0

probability threshold

(a) MLP, SSP

SVM, SSP

acc
tss
hss

0.0

0.2

0.4

0.6

0.8

1.0

probability threshold

(d) SVM, SSP

RF, SSP

acc
tss
hss

t

e
a
r

e
v
i
t
i
s
o
p
e
u
r
T

t

e
a
r

e
v
i
t
i
s
o
p
e
u
r
T

e
t
a
r
e
v
i
t
i
s
o
p

e
u
r
T

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

0
1

.

8
0

.

6
0

.

4
0

.

2
0

.

0
0

.

0.0

0.2

0.4

0.6

0.8

1.0

False positive rate

(b) MLP, ROC

SVM, ROC

0.0

0.2

0.4

0.6

0.8

1.0

False positive rate

(e) SVM, ROC

RF, ROC

y
c
n
e
u
q
e
r
f

y
c
n
e
u
q
e
r
f

y
c
n
e
u
q
e
r
f

0
1

.

8
0

.

6

.

0

4

.

0

2
0

.

0
0

.

0
1

.

8
0

.

6
0

.

4

.

0

2
0

.

0
0

.

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

0.0

0.2

0.4

0.6

0.8

1.0

probability

(c) MLP, RD

SVM, RD

0.0

0.2

0.4

0.6

0.8

1.0

probability

(f) SVM, RD

RF, RD

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

0.6

0.8

1.0

probability threshold

(g) RF, SSP

False positive rate

(h) RF, ROC

probability

(i) RF, RD

Figure 5. Same as Figure 3, but for >C1 ﬂare prediction.

tory with maximum TSS=0.574 and the RD is quite signiﬁcant, showing no
systematic over- or under-prediction.

With respect to the SVM, a purely monotonic decrease of TSS can be seen,
following an instantaneous peak. Some plateau in HSS is also noticed, followed
by a monotonic decrease. The ROC curve appears less satisfactory than in case
of MLPs with maximum TSS=0.566 and the RD shows some systematic under-
prediction for most of the forecast probabilities range.

For the RFs, one notices a relatively similar behavior with MLPs, albeit with
a slightly more pronounced HSS peak. The ROC curve seems better behaved
than in the previous two methods with maximum TSS=0.615 and the RD is
arguably the best achieved together with the MLP RD.

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 20

 
 
 
 
 
 
 
 
 
Forecasting Solar Flares Using Machine Learning

4.5.2. Prediction of >C1 Flare Events Using Statistical Models

Figure 6 shows the forecast performances of the three tested statistical methods,
for >C1 class ﬂare prediction. In particular:

LM, SSP

LM, ROC

LM, RD

s
e
u
a
v

l

s

l
l
i

k
s

s
e
u
a
v

l

s

l
l
i

k
s

s
e
u
a
v

l

s

l
l
i

k
s

0
1

.

8
0

.

6

.

0

4

.

0

2
0

.

0
0

.

0

.

1

8
0

.

6
0

.

4
.
0

2
.
0

0
.
0

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

acc
tss
hss

0.0

0.2

0.4

0.6

0.8

1.0

probability threshold

(a) LM, SSP

PR, SSP

acc
tss
hss

0.0

0.2

0.4

0.6

0.8

1.0

probability threshold

(d) PR, SSP

LG, SSP

acc
tss
hss

t

e
a
r

e
v
i
t
i
s
o
p
e
u
r
T

t

e
a
r

e
v
i
t
i
s
o
p

e
u
r
T

e
t
a
r

e
v
i
t
i
s
o
p
e
u
r
T

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

0
.
1

8
.
0

6
.

0

4
0

.

2
0

.

0
0

.

0
1

.

8
0

.

6
0

.

4
0

.

2
0

.

0
0

.

0.0

0.2

0.4

0.6

0.8

1.0

False positive rate

(b) LM, ROC

PR, ROC

0.0

0.2

0.4

0.6

0.8

1.0

False positive rate

(e) PR, ROC

LG, ROC

y
c
n
e
u
q
e
r
f

y
c
n
e
u
q
e
r
f

y
c
n
e
u
q
e
r
f

0
1

.

8
0

.

6
0

.

4

.

0

2
0

.

0
0

.

0
1

.

8
0

.

6
.
0

4
.
0

2
.
0

0
.
0

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

0.0

0.2

0.4

0.6

0.8

1.0

probability

(c) LM, RD

PR, RD

0.0

0.2

0.4

0.6

0.8

1.0

probability

(f) PR, RD

LG, RD

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

0.6

0.8

1.0

probability threshold

(g) LG, SSP

False positive rate

(h) LG, ROC

probability

(i) LG, RD

Figure 6. Same as Figure 4, but for >C1 ﬂare prediction.

For the LM, we notice a decrease in the ACC of the method and some more-
or-less similar behavior in the behaviour of HSS and TSS. The ROC curve
seems satisfactory with maximum TSS=0.562, while the RD appears to show a
systematic over-prediction below a forecast probability of 0.4 and a systematic
under-prediction above a forecast probability of 0.4 (excluding probabilities >
0.9).

Regarding the PR, similar behaviour with LM appears for the SSPs, while
the ROC curve seems slightly better with maximum TSS=0.566. The RD curve
shows some systematic under-prediction, although generally within error bars.

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 21

 
 
 
 
 
 
 
 
 
K. Florios et al.

Finally, for the LG, one notices a similar behaviour in the SSP, as in the
case of LM and PR, but arguably a better behaved ROC curve with maximum
TSS=0.567. The RD seems to bee the best behaved, compared to those of LM
and PR.

4.5.3. Monte Carlo Simulation for >C1 Flares

±

±

±

±

±

0.00, TSS=0.59

0.00, TSS = 0.54

In Table 3 we provide the average values of the skill scores ACC, TSS and HSS for
all prediction methods after the 200 replications of the Monte Carlo experiment
regarding >C1 ﬂares prediction. We notice from Table 3 that the maximum
HSS=0.60 is obtained with the RF method for a probability threshold of 40%.
0.01 and
The corresponding skill score values are ACC=0.85
0.01. The second best method in Table 3 for the same probability
HSS=0.60
threshold is obtained with the LG method, with ACC=0.83
±
0.01 and HSS=0.56
0.01. Considering again the probability threshold where the
maximum TSS is observed, we get the optimal results for the RF method and
threshold 30% with values ACC=0.82
±
±
0.01. The second best method may be considered the MLP (or the LG in a tie) at
30% threshold with ACC=0.81
0.01. For
0.00, TSS=0.57
a range of probability thresholds (30% – 40%) the method RF yields increasing
values of HSS and decreasing values of TSS. As a result, again it is not clear which
is the optimal value of the threshold probability, if we choose to simultaneously
optimize both TSS and HSS. For example, an appealing RF forecasting model
is with threshold 35% and skill scores ACC=0.84
0.01 and
0.01 in Table 3. These results are generally above those reported for
HSS=0.59
>C1 class ﬂares predictability, namely TSS
[0.40, 0.45]
(Al-Ghraibah, Boucheron, and McAteer, 2015; Boucheron, Al-Ghraibah, and
McAteer, 2015). In brief, we believe that our data samples, both training and
testing, are comprehensive and generally unbiased.

0.01 and HSS = 0.57

[0.50, 0.55] and HSS

0.01 and HSS=0.53

0.00, TSS=0.60

0.00, TSS=0.61

±

±

±

±

±

±

±

∈

∈

4.6. Assessment of Prediction Methods and Predictor Strength

Following the presentation of results in Tables 2 and 3, we can see that both
for >M1 and >C1 ﬂare prediction, RF delivers the best skill score metrics for a
wide range of probability thresholds. The second best method is MLP together
with LG. In this setting we perform some additional evaluation that conﬁrms
these results.

Regarding the predictors strength, we present analytical results in Appendix
A. It seems that logR and WLSG rank in the ﬁrst places both for >C1 and >M1
ﬂare prediction, closely followed by the Ising energy and the TLMPIL.

In order to investigate the robustness of our results, we present additional
results in Appendix C where we make predictions once a day (at 00:00 UT).
The mean evolution (over 200 Monte Carlo iterations) of ACC, TSS and HSS
with respect to the probability threshold is presented. Likewise, the BS, AUC
and BSS are presented. The main ﬁnding is that issuing forecasts once a day
keeps similar average skill scores with issuing forecasts eight times a day, but
the associated uncertainties (e.g. standard deviations) are higher in the case of
daily predictions.

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 22

Forecasting Solar Flares Using Machine Learning

A ﬁnal word for the comparison of ML algorithms vs. conventional statistics
models for this speciﬁc dataset and positive/negative class deﬁnitions is provided
in Appendix D. There, we have included auxiliary meta-analysis of the results in
Tables 2 and 3 in order to clearly show whether the ML category of prediction
algorithms does any better than the conventional statistics models in the >M1
and >C1 ﬂare prediction cases. A multicriteria analysis using the weighted-sum
(WS) method (Greco, Figueira, and Ehrgott, 2016) seems appropriate in order to
aggregate the performance metrics ACC, TSS and HSS of all classiﬁers as a func-
tion of the probability threshold (e.g. using equal weights for the aggregation). In
this way, a composite index (CI), as a measure of overall utility, is computed for
6 = 126
every algorithm and probability threshold combination. There exist 21
such alternatives when we use a 5% probability threshold grid, such as the grid in
Tables 2 and 3. The ranking, in non-increasing order, of the CI reveals the overall
merit of every probabilistic classiﬁer and also allows us to draw conclusions for
groups of classiﬁers, such as the group of ML methods (comprising RF, SVM
and MLP) and the group of conventional statistics methods (comprising LM,
PR and LG). Appendix D presents this multicriteria WS analysis, revealing that
overall, in >C1 ﬂare prediction ML outperforms conventional statistics methods
by 71% vs. 29% in the synthesis of the top 100(1/6) = 16.6% performing methods
(top 21 methods out of total 126 ones). Likewise, in the >M1 ﬂare prediction
case, ML outperforms conventional statistics methods by 62% vs. 38% in the
synthesis of the top 100(1/6) = 16.6% performing methods. So, it seems that
>C1 ﬂare prediction is more advantageous for ML versus statistical methods,
in comparison to the >M1 ﬂare case. This is due to the low performance of the
SVM in >M1 ﬂare prediction, which is due to the way we have implemented, for
simplicity, the SVM for a highly unbalanced sample in >M1 ﬂare prediction3,
using a single C constant and not two diﬀerent C1, C2 constants during the SVM
training with Equation 7.

×

In auxiliary runs (available upon request), we also noticed that when the
sample size is very low, using ML algorithms poses no advantage over conven-
tional statistics models. In order to have proper training, the ML algorithms
need N > 2, 000 for K = 13, especially for the >M1 ﬂare prediction.

4.7. Statistical Tests for Random Forest vs. MLP and Calculation of

AUC and Brier Skill Scores

In Section 4.7.1 we present results of a t-test between the two best performing
methods according to maximizer thresholds for either TSS or HSS for >M1
class and >C1 class ﬂares cases. Section 4.7.2 presents additional calculations
reporting on BS, BSS and AUC, used for assesing classiﬁcation in the prediction.

3Even by using the SVM weighted variant and recomputing the WS ranking using this variant,
(e.g. see Figure 3 and Table 5), the qualitative results of the presented ranking still hold.

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 23

K. Florios et al.

4.7.1. Unpaired t-tests to Compare Two Means for TSS and HSS of Random

Forest vs. MLP

A t-test compares the means of two groups. Here, we use the t-test to compare
the mean TSS (respectively HSS) of the RF method vs. those of the MLP method
(or in general the second best performing method). Means are considered with
respect to the Monte Carlo simulations performed on the 200 replications of the
previous section. The TSS- (respectively HSS-) values considered are those for
speciﬁc probability thresholds maximizing either TSS or HSS. Table 4 presents
the t-test results regarding the best and the second best methods with respect
to either TSS or HSS for these speciﬁc probability thresholds.
We ﬁnd that RF is always (i.e. 8/8 of times) statistically better than the second
best method (which is the MLP 4/8 of times), with respect to both TSS and
HSS.

4.7.2. Calculation of AUC and Brier Skill Scores

Tables 5 and 6 present the calculated mean values of BS, AUC and BSS for the
>M1 and >C1 ﬂare prediction cases, respectively.

For the >M1 ﬂare case (Table 5), results show that, on average, the best BS
and BSS results are achieved with the RF method (BS = 0.0266; BSS=0.4163).
The best AUC results are achieved with the RF method (AUC = 0.9556), but
also PR (AUC=0.9392) and LG (AUC=0.9391) methods.

For the >C1 ﬂare case (Table 6), results show that, on average, the best BS
and BSS results are achieved with the RF method (BS=0.1074; BSS=0.4426).
The best AUC results are also achieved with the RF method (AUC=0.8927),
with other methods (except SVM) following closely. The SVM probably needs
better ﬁne-tuning, given its sensitivity on γ and cost (see Section 4.2).

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 24

Table 2. Monte-Carlo scenario 1, based on 200 SHARP datasets, on >M1 GOES ﬂare prediction. Numbers in boldface correspond to the most
signiﬁcant results of a given method (MLP: multi-layer perceptron; LM: linear regression; PR: probit regression; LG: logit regression; RF: random
forest; SVM: support vector machine).

Par

%

ACC

val0
val5
val10
val15
val20
val25
val30
val35
val40
val45
val50
val55
val60
val65
val70
val75
val80
val85
val90
val95
val100

0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.55
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00

0.15
0.90
0.93
0.94
0.95
0.95

0.95
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.00

MLP
TSS

0.11
0.70
0.66
0.62
0.59
0.56

0.53
0.50
0.47
0.44
0.42
0.39
0.37
0.34
0.32
0.29
0.27
0.24
0.20
0.15
0.00

HSS

ACC

0.01
0.39
0.45
0.48
0.50
0.50

0.50
0.50
0.50
0.49
0.48
0.47
0.45
0.44
0.42
0.40
0.37
0.34
0.31
0.25
0.00

0.32
0.78
0.88

0.92
0.94
0.95
0.95
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.95
0.95
0.95
0.95
0.95

LM
TSS

0.27
0.70
0.73

0.65
0.54
0.45
0.38
0.31
0.26
0.21
0.18
0.16
0.14
0.11
0.09
0.08
0.06
0.05
0.03
0.02
0.02

HSS

ACC

0.04
0.22
0.35

0.43
0.46
0.45
0.44
0.39
0.35
0.31
0.28
0.25
0.23
0.19
0.16
0.13
0.12
0.09
0.06
0.04
0.03

0.00
0.84
0.90
0.93
0.94
0.95
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.95
0.95
0.00

PR
TSS

0.00
0.75
0.71
0.64
0.58
0.52
0.47
0.41
0.36
0.32
0.28
0.25
0.21
0.18
0.16
0.13
0.11
0.08
0.05
0.03
0.00

HSS

ACC

0.00
0.30
0.39
0.45
0.48
0.49
0.49
0.47
0.45
0.42
0.39
0.36
0.33
0.29
0.25
0.22
0.18
0.14
0.10
0.06
0.00

0.00
0.85
0.90
0.93
0.94
0.95
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.95
0.95
0.00

LG
TSS

0.00
0.75
0.69
0.63
0.58
0.52
0.48
0.43
0.39
0.35
0.31
0.27
0.24
0.21
0.18
0.15
0.12
0.09
0.06
0.03
0.00

HSS

ACC

0.00
0.31
0.40
0.45
0.48
0.49
0.50
0.49
0.47
0.45
0.42
0.39
0.36
0.32
0.28
0.25
0.21
0.16
0.10
0.05
0.00

0.51
0.85
0.90
0.93

0.95
0.96

0.96
0.96
0.97
0.97
0.97
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.95
0.92
0.00

RF
TSS

0.48
0.77
0.77
0.74

0.69
0.63

0.57
0.51
0.46
0.41
0.37
0.32
0.28
0.24
0.19
0.15
0.11
0.08
0.04
0.01
0.00

HSS

ACC

SVM
TSS

0.08
0.32
0.42
0.49

0.54
0.57

0.57
0.57
0.55
0.52
0.49
0.45
0.41
0.36
0.31
0.25
0.19
0.14
0.08
0.02
0.00

0.00
0.94
0.95
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.00

0.00
0.59
0.51
0.46
0.42
0.39
0.37
0.35
0.33
0.31
0.29
0.28
0.26
0.24
0.22
0.21
0.18
0.16
0.13
0.08
0.00

HSS

0.00
0.47
0.49
0.50
0.48
0.47
0.46
0.45
0.44
0.42
0.41
0.39
0.38
0.36
0.34
0.32
0.29
0.26
0.22
0.14
0.00

F
o
r
e
c
a
s
t
i
n
g

S
o
l
a
r

F
l
a
r
e
s
U
s
i
n
g
M
a
c
h
i
n
e

L
e
a
r
n
i
n
g

S
O
L
A
:

K
F
_
e
t
_
a
l
_
1
4
_
0
1
_
2
0
1
8
.
t
e
x
;

1
8

J
a
n
u
a
r
y

2
0
1
8
;

1
:
3
0
;

p
.

2
5

Table 3. Monte-Carlo scenario 2, based on 200 SHARP datasets, on >C1 GOES ﬂare prediction. Numbers in boldface correspond to the most
signiﬁcant results of a given method (MLP: multi-layer perceptron; LM: linear regression; PR: probit regression; LG: logit regression; RF: random
forest; SVM: support vector machine).

Par

%

ACC

val0
val5
val10
val15
val20
val25
val30
val35
val40
val45
val50
val55
val60
val65
val70
val75
val80
val85
val90
val95
val100

0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.55
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00

0.00
0.52
0.66
0.72
0.76
0.79
0.81

0.82
0.83
0.84
0.84
0.84
0.84
0.83
0.82
0.82
0.81
0.79
0.78
0.75
0.00

MLP
TSS

0.00
0.33
0.49
0.55
0.57
0.57
0.57

0.56
0.55
0.53
0.50
0.48
0.45
0.41
0.37
0.33
0.28
0.22
0.16
0.08
0.00

HSS

ACC

0.00
0.21
0.35
0.43
0.48
0.51
0.53

0.55
0.55
0.55
0.55
0.53
0.51
0.49
0.45
0.41
0.36
0.29
0.21
0.11
0.00

0.39
0.44
0.51
0.58
0.66
0.74
0.79
0.82
0.83
0.83
0.83
0.81
0.80
0.79
0.78
0.77
0.77
0.76
0.76
0.76
0.75

LM
TSS

0.16
0.23
0.32
0.40
0.48
0.55
0.57
0.55
0.52
0.47
0.40
0.34
0.28
0.22
0.18
0.15
0.12
0.10
0.08
0.07
0.06

HSS

ACC

0.09
0.14
0.20
0.27
0.35
0.45
0.51
0.54
0.54
0.52
0.47
0.41
0.35
0.29
0.24
0.20
0.17
0.14
0.11
0.10
0.08

0.00
0.47
0.58
0.66
0.73
0.78
0.80
0.82
0.83
0.84
0.84
0.83
0.82
0.82
0.81
0.80
0.79
0.78
0.77
0.76
0.00

PR
TSS

0.00
0.27
0.40
0.49
0.55
0.56
0.57
0.56
0.53
0.50
0.47
0.43
0.38
0.34
0.29
0.25
0.20
0.16
0.13
0.09
0.00

HSS

ACC

0.00
0.17
0.27
0.36
0.44
0.49
0.53
0.55
0.55
0.55
0.53
0.50
0.46
0.42
0.37
0.32
0.27
0.22
0.18
0.13
0.00

0.00
0.48
0.60
0.68
0.74
0.78
0.81

0.82
0.83

0.84
0.84
0.84
0.83
0.82
0.81
0.80
0.79
0.78
0.77
0.76
0.00

LG
TSS

0.00
0.28
0.42
0.50
0.55
0.57
0.57

0.56
0.54

0.51
0.48
0.45
0.41
0.37
0.32
0.27
0.22
0.18
0.14
0.09
0.00

HSS

ACC

0.00
0.17
0.29
0.38
0.45
0.50
0.53

0.55
0.56

0.55
0.53
0.52
0.48
0.44
0.40
0.35
0.29
0.24
0.19
0.13
0.00

0.28
0.52
0.64
0.71
0.76
0.79
0.82
0.84
0.85

0.85
0.85
0.85
0.85
0.84
0.83
0.82
0.81
0.79
0.78
0.76
0.00

RF
TSS

0.03
0.34
0.48
0.55
0.59
0.61
0.61
0.60
0.59

0.56
0.54
0.51
0.48
0.44
0.40
0.34
0.28
0.21
0.15
0.08
0.00

HSS

ACC

SVM
TSS

0.01
0.21
0.34
0.42
0.48
0.53
0.57
0.59
0.60

0.59
0.59
0.57
0.55
0.52
0.48
0.43
0.36
0.29
0.21
0.12
0.00

0.00
0.29
0.44
0.75
0.80
0.82
0.83
0.84
0.84
0.84
0.84
0.83
0.83
0.83
0.82
0.81
0.81
0.80
0.79
0.77
0.00

0.00
0.03
0.22
0.54
0.57
0.56
0.54
0.52
0.50
0.48
0.46
0.44
0.42
0.38
0.35
0.32
0.28
0.24
0.19
0.14
0.00

HSS

0.00
0.02
0.13
0.45
0.53
0.55
0.55
0.55
0.54
0.53
0.52
0.51
0.49
0.46
0.43
0.39
0.36
0.31
0.25
0.19
0.00

K

.

F
l
o
r
i
o
s

e
t

a
l
.

S
O
L
A
:

K
F
_
e
t
_
a
l
_
1
4
_
0
1
_
2
0
1
8
.
t
e
x
;

1
8

J
a
n
u
a
r
y

2
0
1
8
;

1
:
3
0
;

p
.

2
6

Forecasting Solar Flares Using Machine Learning

Table 4. Unpaired t-tests to compare the means of TSS and HSS
metrics (out-of-sample) for the best and the second best methods in
>M1 and >C1 ﬂare forecasting.

>M1 class ﬂares prediction

No. Metric Threshold (%)

1
2
3
4

TSS
HSS
TSS
HSS

10
10
25
25

Best
RF
RF
RF
RF

Second Best
LM
LM
MLP
MLP

p-value
< 10−4
< 10−4
< 10−4
< 10−4

>C1 class ﬂares prediction

No. Metric Threshold (%)

5
6
7
8

TSS
HSS
TSS
HSS

30
30
40
40

Best
RF
RF
RF
RF

Second Best
MLP
MLP
LG
LG

p-value
< 10−4
< 10−4
< 10−4
< 10−4

Table 5. Mean values for BS, BSS and AUC for all tested models on the prediction
of >M1 ﬂares. Means are obtained after 200 Monte-Carlo replications. Parentheses
underneath values denote standard deviations. Notice that smaller values indicate
better performance for BS, whereas higher values indicate better performance for
AUC and BSS.

BS

MLP
0.0324
(0.0013)

LM
0.0331
(0.0008)

PR
0.0305
(0.0008)

LG
0.0302
(0.0008)

RF
0.0266
(0.0008)

SVM
0.0327
(0.0012)

SVMweighted
0.0357
(0.0011)

AUC

MLP
0.9301
(0.0067)

LM
0.9278
(0.0043)

PR
0.9392
(0.0033)

LG
0.9391
(0.0033)

RF
0.9556
(0.0035)

SVM
0.8320
(0.0168)

SVMweighted
0.9175
(0.0059)

BSS

MLP
0.2903
(0.0267)

LM
0.2745
(0.0117)

PR
0.3323
(0.0128)

LG
0.3375
(0.0134)

RF
0.4163
(0.0126)

SVM
0.2829
(0.0159)

SVMweighted
0.2181
(0.0154)

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 27

K. Florios et al.

Table 6. Same as Table 5, but for the prediction of >C1 ﬂares.

BS

MLP
0.1167
(0.0014)

LM
0.1292
(0.0012)

PR
0.1201
(0.0012)

LG
0.1191
(0.0012)

RF
0.1074
(0.0012)

SVM
0.1226
(0.0015)

AUC

MLP
0.8731
(0.0029)

LM
0.8638
(0.0029)

PR
0.8665
(0.0027)

LG
0.8669
(0.0027)

RF
0.8927
(0.0026)

SVM
0.8466
(0.0033)

BSS

MLP
0.3940
(0.0069)

LM
0.3293
(0.0052)

PR
0.3767
(0.0055)

LG
0.3818
(0.0058)

RF
0.4426
(0.0056)

SVM
0.3636
(0.0063)

4.8. Related Published Work and Comparison To Our Results

Ahmed et al. (2013) presented prediction results for >C1 class ﬂares using cross-
validation with 60% training and 40% testing subsets, with 10 iterations in
operational and segmented mode. Since our analysis focuses in operational mode,
the golden standard for near-real-time operational systems such as FLARE-
CAST, we present here their results on the operational mode for the period Apr.
1996 - Dec. 2010: POD = 0.455 & POFD = 0.010 thus TSS = POD
POFD =
0.445 and HSS = 0.539. Hence, Ahmed et al. reported (using a variant of a
neural network, and threshold 50%) results for ﬂares >C1: TSS = 0.445 and
HSS = 0.539.

−

Li et al. (2008) presented results using a SVM coupled with k-nearest neighbor
(KNN) for ﬂare prediction >M1 in a way that, unfortunately, cannot be used
to recover TSS and HSS values. Instead, they report Equal = TN + TP, High
= FP, Low = FN. The accuracy achieved is only ACC=57.02% for SVM and
ACC=63.91% for SVM-KNN for the testing year 2002.

Song et al. (2009) presented results using an ordinal logistic regression model
classifying the C-, M- and X-class ﬂares with response values 1, 2 and 3, re-
spectively. The B-class ﬂares (or no ﬂares) category received class 0 (baseline).
Their sample contains 34 X-class ﬂares, 68 M-class ﬂares, 65 C-class ﬂares, and
63 B-class or no-ﬂare cases. A clear drawback of this sample is that it is not
taken using a random number generator but seems to be hand-picked aiming
at studying the considered 230 events during the period 1998-2005. As a result,
the sample is biased in that the occurrence rates of the various ﬂare classes
are not representative of an actual solar cycle. Perhaps not surprisingly, these
authors presented high TSS and HSS values that, given the sample, might be
taken with a conservative outlook. From the results of Model 4 in that study
(i.e. Table 8 of Song et al., 2009), we are able to infer that for C class ﬂares,

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 28

Forecasting Solar Flares Using Machine Learning

Song et al. computed values TSS=0.65 and HSS=0.623 (C1-C9 ﬂares). Moreover,
we maintain an impression that these numbers are obtained in-sample for the
dataset with 230 events in Song et al. (2009).

Yu et al. (2009) used a sliding window approach to account for the evolution of
three magnetic ﬂare predictors with importance index above 10 (for the deﬁnition
of the ﬂare importance index, see Yu et al. (2009)). The time period is 1996 to
2004, with a cadence of 96 minutes. The authors use the C4.5 decision tree
algorithm and the Learning Vector Quantization (LVQ) Neural network, both
implemented in WEKA (Witten et al., 2016; Hall et al., 2009). The authors
use a 10-fold cross-validation approach with 90% training and 10% testing sets
from the original sample. The sliding window size was 45 observations. Their
results showed that the sliding window versions of C4.5 and LVQ neural network
algorithms improved the results obtained with the same algorithms for sliding
window size equal to 0. Since the authors present only the TP rate and the TN
rate results, we are not able to recover their HSS value. Their recovered TSS is
TSS=0.651 for the C4.5 algorithm with a sliding window of 45 observations and
TSS=0.667 for the LVQ also with a sliding window of 45 observations.

Yuan et al. (2010) used the same dataset as in Song et al. (2009) and proposed
a cascading approach using, ﬁrst, an ordinal logistic regression model to produce
probabilities for GOES ﬂare classes B, C, M and X (associated with response
levels 0,1,2 and 3, respectively) and, second, feeding the probability values to an
SVM in order to obtain the ﬁnal class membership. Their results, according to
Yuan et al. (2010) improve the prediction especially for X-class ﬂares (response
level = 3 in the ordinal logistic regression) but, still, are not exceptionally high.
For example, for level = 1, therefore for C-class ﬂares, we were able to recover the
following TSS values for the used methods: Logistic Regression: TSS=0.22, SVM:
TSS=0.08, Logistic Regression + SVM: TSS=0.09, These rather fair results, as
can be seen from the contingency tables presented in Yuan et al. (2010), may be
due to the selection of a probability threshold value at 50% for levels 0, 1 and 3
in the ordinal logistic regression model and at 25% for the level 3 (X-class ﬂares)
in the same model. Choosing a threshold equal to 50% maximizes ACC but not
TSS / HSS, as can be seen both here and in Bloomﬁeld et al. (2012).

Colak and Qahwaji (2009) developed an online solar ﬂare forecasting system
called ASAP. Their prediction algorithm is a combination of two neural networks
with the Sum-of-Squared Error (SSE) objective function, where the ﬁrst neural
network predicts whether a ﬂare of all types (C, M or X) will occur and, if the
prediction is yes, the second neural network predicts whether a C-, M-, or X-
class ﬂare will occur. The ASAP system was developed in C++ and has been
validated with data from 1999 to 2002 (around the peak of Solar Cycle 23).
The predictors were the sunspot area and characteristics from the McIntosh
classiﬁcation of sunspots (Zpc scheme). They obtained HSS=49.3% (C-class
ﬂares) and HSS=47% (M-class ﬂares) for a forecast window of 24h.

Wang et al. (2008) developed a MLP neural network using three input vari-
ables for the prediction of solar ﬂares of class >M1. The predictors were the
, the length L of the neutral line and
maximum horizontal gradient
the number of singular points η. A limitation of the study is that only ﬂar-
ing active regions (at GOES C1 and above) are sampled and considered. The

gradh(Bz)
|

|

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 29

K. Florios et al.

forecast window is 48h. The authors presented prediction results for the period
1996-2002 (training set: Apr. 1996 to Dec. 2001, testing set: Jan 2002 to Dec
2002). The results were presented as plots of the X-ray ﬂux associated with the
predicted/observed ﬂares for the test year 2002, so comparison with the authors’
skill scores is not possible. This work reported ACC=69% for the test year.

≈

Bobra and Couvidat (2015) applied a SVM to a sample of 5,000 non-ﬂaring
and 303 ﬂaring (at the GOES >M1 level) AR. Those N = 5, 303 AR with N =
5, 000 negative examples and P = 303 positive examples (ratio N/P = 16.5),
were sampled from the
1.5 million patches of the SHARP product (Bobra et al.,
2014) between years 2010 and 2014. The authors selected 285 M-class ﬂares and
18 X-class ﬂares observed between 2010 May and 2014 May. By comparison,
our study herein relies on a representative sample of ﬂaring/non-ﬂaring AR in
the period 2012-2016 and for ﬂares >M1, with a ratio N/P = 19.9 (P = 1108
and N = 22, 026). By inspecting Table 3 of Bobra and Couvidat (2015) we
0.039 and
see that the authors report results: ACC=0.924
HSS2=0.517
0.02 and
±
HSS=0.49
0.01 (their deﬁnition of HSS2 is the same as the HSS deﬁnition in
Section 4.3). Thus, our results with random forests are competitive with those
of Bobra and Couvidat (2015). We note that we use a 50/50 rule for splitting
training/testing sets, while Bobra and Couvidat (2015) use a 70/30 rule. Also,
N/P in Bobra and Couvidat (2015) is 16.5 while in our case N/P is 19.9. Finally,
we use no ﬁne-tuning in the parameters of the Random Forest, while Bobra and
Couvidat (2015) carefully tune the C, γ and C1/C2 of their Equation 2, 5 and 6,
respectively. Regardless, we believe that Bobra and Couvidat (2015) represent
the state-of-the art in solar ﬂare forecasting so far.

0.035 while our results are ACC=0.93

±
0.00, TSS=0.74

0.007, TSS=0.761

±

±

±

±

Boucheron, Al-Ghraibah, and McAteer (2015) applied support vector regres-
sion (SVR) to 38 predictors characterizing the magnetic ﬁeld of solar AR in order
to predict: i) the ﬂare size and ii) the time-to-ﬂare using SVR modeling. The fore-
cast window used varies between 2 and 24 hours with a step of 2 hours (12 cases
of forecast windows). By using the size regression with appropriate thresholds
(diﬀerent to the usual probability thresholds, for example, in Bloomﬁeld et al.,
2012), the authors achieved prediction results for >C1 ﬂares with TSS=0.55 and
HSS=0.46, while reporting that using the same data, Al-Ghraibah, Boucheron,
and McAteer (2015) achieved TSS
0.40, respectively, for the
prediction of >C1 class ﬂares.

0.50 and HSS

≈

≈

Al-Ghraibah, Boucheron, and McAteer (2015) applied relevance vector ma-
chines (RVM), a technique that is a generalization of SVM, to a set of 38
magnetic properties characterizing 2124 AR in a total of 122,060 images across
diﬀerent time points for all AR. They predicted >C1 ﬂares using either the
full set of properties or suitable subsets thereof. The magnetic properties are
of three types: i) snapshots in space and time, ii) evolution in time and iii)
structures of multiple size scales. Al-Ghraibah, Boucheron, and McAteer (2015)
0.51
reported results (e.g., see their Table 5 and Figure 6) in the range TSS
and HSS
0.39, which is a baseline result for the literature when no temporal
information is included in the predictor set (i.e. static images are used).

≈

≈

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 30

Forecasting Solar Flares Using Machine Learning

5. Conclusions

We present a new approach for the eﬃcient prediction of >M1 and >C1 solar
ﬂares: classic and modern machine learning (ML) methods, such as multi-layer
perceptrons (MLP), support vector machines (SVM) and random forests (RF)
were used in order to build the prediction models. The predictor variables were
based on the SDO/HMI SHARP data product, available since 2012.

The sample was representative of the solar activity during a ﬁve-year period of
Solar Cycle 24 (2012 – 2016), with all calendar days within this period included
in the sample. The cadence of properties, or predictors, within the chosen days
was 3 hours.

We show that the RF methodology could be our prediction method of choice,
both for the prediction of >M1 ﬂares (with a relative frequency of 4.8%, or 1108
events) and for the prediction of >C1 ﬂares (with a relative frequency of 26.1%,
or 6029 events). In terms of categorical skill scores, a probability threshold of
15% for >M1 ﬂares gives rise to mean (after 200 replications) RF skill scores of
the order TSS=0.74
0.01, while a probability threshold
of 35% for >C1 ﬂares gives rise to mean TSS=0.60
±
0.01. The respective accuracy values are ACC=0.93 and ACC=0.84. In terms of
probabilistic skill scores, the ranking of the ML techniques with respect to their
BSS against climatology is RF (0.42), MLP (0.29) and SVM (0.28) for >M1
ﬂares and RF (0.44), MLP (0.39) and SVM (0.36) for >C1 ﬂares.

0.01 and HSS=0.59

0.02 and HSS=0.49

±

±

±

We further indicate that for >M1 ﬂare prediction, SVM and MLP need
additional tuning of their hyperparameters (Section 4.2) in order to produce com-
parable results with RF. Moreover, several statistical methods (linear regression,
probit, logit) produced acceptable forecast results when compared with the ML
methods. By increasing the number of hidden nodes, the MLP networks provide
ﬂatter skill scores proﬁles (i.e. ACC, TSS, HSS as a function of the threshold
probability), but the peak values of the corresponding curves are smaller than
those achieved by MLP networks with fewer hidden nodes. Regarding the >C1
ﬂares, all forecast methods work acceptably, although the best method is, again,
RF. A Monte Carlo experiment showed that results are robust with respect to
diﬀerent realizations of the training/testing pair, with diﬀerent random seeds.
Monte Carlo modeling also manages to decrease the amplitudes of the applicable
standard deviations of skill scores. Typically standard deviations are larger for
the >M1 ﬂare case compared to that of >C1 ﬂares. This is to be attributed to
the diﬀerent occurrence frequency of ﬂares in the two cases.

RF is a relatively new approach to solar ﬂare prediction. Nonetheless, it
may be preferable over other widely used ML algorithms, at least for the data
sets exploited so far, giving competitive results without much tuning of the
RF hyperparameters. This generates hope for future meaningful developments
in the formidable solar ﬂare prediction problem, at the same time aligning
with excellent performance for RF reported in several classiﬁcation benchmarks
(Fern´andez-Delgado et al., 2014). This important statement made, it appears
that even with the application of RF, solar ﬂare prediction in the foreseeable
future will likely continue to be probabilistic (i.e. 0.0 – 1.0, continuous), rather
than binary (i.e. 0 or 1).

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 31

K. Florios et al.

In terms of the predictors importance, Schrijver’s R is found to be among
the most statistically signiﬁcant predictors together with WLSG. Also, the Ising
energy and the TLMPIL are considered as important, ranking slightly below the
previous two predictors. This stems from the importance calculations according
to the Fisher score and random forest importance for the >C1 and >M1 ﬂare
cases in Appendix A. This result is also in line with the common knowledge that
ﬂares occur mostly when strong and highly sheared MPILs are formed. Other
MPIL-highlighting predictors, such as the eﬀective connected magnetic ﬁeld
strength, Beﬀ (Georgoulis and Rust, 2007) remain to be tested, in conjunction
with R and WLSG as their cadence was lower than 3 h at the time this study
was performed.

An interesting ﬁnding for the RF technique (Appendix B) is obtained by the
predictors’ ranking information according to their importance, as measured by
the Fisher score. Namely, when we create prediction models with a varying num-
ber of the most important predictors included, the RF prediction performance
(in terms of TSS and HSS) continues to improve monotonically with the number
of included parameters. On the contrary, the MLP and SVM algorithms achieve
only slight improvements in prediction results (again in terms of TSS and HSS)
by adding more than, say, the six most important predictors. This interesting
ﬁnding may further improve forecasting when more viable predictors become
available.

For future FLARECAST-supported research we plan to enlarge our analysis
sample by reducing the property cadence from 3 h to 1 h or even less (the limit
is the inherent cadence of SDO/HMI SHARP data, namely 12 min). Another
direction of future research is to investigate the robustness of our results for
samples created with a larger cadence of 12 h (24 h) coupled with a forecast
window of 12 h (24 h), respectively. Furthermore, we plan to exploit the sub-
stantial time-series aspect of our data using recurrent neural networks, possibly
trained with evolutionary algorithms. The present work, along with a series of
similar concluded or still ongoing studies are considered for possible integration
in the ﬁnal FLARECAST online system and forecasting tool, to be deployed by
early 2018.

Acknowledgments We would like to thank the anonymous referee for very helpful com-

ments that greatly improved the initial manuscript. This research has been supported by the

EU Horizon 2020 Research and Innovation Action under grant agreement No.640216 for the
“Flare Likelihood And Region Eruption foreCASTing” (FLARECAST) project. Data were

provided by the MEDOC data and operations centre (CNES / CNRS / Univ. Paris-Sud),

http://medoc.ias.u-psud.fr/ and the GOES team.

Disclosure of Potential Conﬂicts of Interest. The authors declare that they have
no conﬂicts of interest.

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 32

Forecasting Solar Flares Using Machine Learning

Appendix

A. Importance of predictors for ﬂare prediction

We computed the Fisher score (Bobra and Couvidat, 2015; Chang and Lin, 2008;
Chen and Lin, 2006) and the Gini importance (Breiman, 2001) for every predic-
tor in the case of >M1 and >C1 ﬂares. The obtained values for the importance
of several predictors are presented in Figures A.1 and A.2 for >C1 and >M1
ﬂare prediction, respectively. The Fisher score, F , is deﬁned for the j th predictor
as,

F (j) =

1
n+−1

n+

Pk=1

(¯x(+)

j −

¯xj)2 + (¯x(−)

¯xj )2

j −
n−

(x(+)

k,j −

¯x(+)
j

)2 + 1

n−−1

Pk=1

(x(−)

k,j −

¯x(−)

j

)2

.

(A.1)

and ¯x(−)

j

j

In Equation A.1, ¯xj , ¯x(+)

are the mean values for the j th predictor
over the entire sample, the positive class and the negative class, respectively.
Furthermore, n+ (n−) are the number of positive (negative) class observations.
Also, x(+)
k,j ) are the values for the k th observation of the j th predictor
belonging in the positive (negative) class. The higher the value of F (j) the more
important the j th predictor.

k,j (x(−)

The Gini importance is returned with the randomForest function of the ran-
domForest package in R. The higher the Gini importance of the j -th predictor
the more important this predictor is.

We note that the correlation between the two quantities (e.g. Fisher score
and Gini importance) is r = 0.7441 for >C1 sﬂares and r = 0.7535 for >M1
ﬂares, respectively. So, the two methods qualitatively agree on describing which
predictors are the most important regarding ﬂare prediction, in both classes of
ﬂare prediction. Also, by looking at Figure A.1 we see that for >C1 ﬂares, the
top three ranked predictors for both Fisher Score and Gini importance are: the
two versions of Schrijver’s R and WLSG. Regarding the >M1 ﬂares, from Figure
A.2 the top four ranked predictors for either Fisher score or Gini importance
are: the two versions of Schrijver’s R, WLSG and TLMPILBr. In Appendix A
the terminology for every predictor is explained in Table A1.

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 33

K. Florios et al.

Table A1. Abbreviations for predictors used in main text (Symbol1) and in Figures A.1
and A.2 (Symbol2).

Abbreviations for Predictors

Symbol2
r value logr
alpha exp ﬀt

Symbol1
logR
FSPI
TLMPIL mpil
DI
WLSG
IsinEn1
IsinEn2

decay index
wlsg
ising energy
ising energy part

Description
Schrijver’s R value
Fourier spectral power index
Magnetic polarity inversion line
Decay index
Gradient-weighted integral length of the neutral line
Ising Energy original
Ising Energy partitioned

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 34

Forecasting Solar Flares Using Machine Learning

Fisher Score Parameter Importance

Random Forest Parameter Importance

r_value_br_logr

r_value_blos_logr

wlsg_br

mpil_br

ising_energy_part_br

ising_energy_br

ising_energy_part_blos

mpil_blos

ising_energy_blos

decay_index_br

decay_index_blos

alpha_exp_fft_br

alpha_exp_fft_blos

r_value_blos_logr

wlsg_br

r_value_br_logr

ising_energy_blos

mpil_br

mpil_blos

ising_energy_part_blos

alpha_exp_fft_blos

ising_energy_br

alpha_exp_fft_br

ising_energy_part_br

decay_index_blos

decay_index_br

0.1

0.2

0.3

0.4

0.5

200

300

400

500

600

700

Fisher Score

Mean Decrease Gini

(a) Fisher score, >C1 ﬂares

(b) Gini importance, >C1 ﬂares

5
.

0

4

.

0

3

.

0

2
0

.

1
0

.

e
r
o
c
S

r
e
h
s
F
e

i

t

a
i
r
a
v
n
U

i

r_value_br_logr

r_value_blos_logr

wlsg_br

mpil_br

ising_energy_part_br
ising_energy_br

ising_energy_part_blos

mpil_blos

ising_energy_blos

R−squared=0.5537
correlation=0.7441

decay_index_br

decay_index_blos

alpha_exp_fft_br

alpha_exp_fft_blos

0

200

400

600

800

Gini Importance

(c) Correlation, >C1 ﬂares

Figure A.1. Importance of several predictors while predicting >C1 ﬂares.

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 35

 
 
K. Florios et al.

Fisher Score Parameter Importance

Random Forest Parameter Importance

r_value_br_logr

r_value_blos_logr

wlsg_br

mpil_br

ising_energy_part_br

ising_energy_part_blos

ising_energy_br

mpil_blos

alpha_exp_fft_br

ising_energy_blos

decay_index_blos

decay_index_br

alpha_exp_fft_blos

r_value_blos_logr

r_value_br_logr

wlsg_br

mpil_br

mpil_blos

ising_energy_blos

alpha_exp_fft_br

ising_energy_br

alpha_exp_fft_blos

ising_energy_part_blos

ising_energy_part_br

decay_index_blos

decay_index_br

0.2

0.4

0.6

0.8

1.0

1.2

60

80

100

120

140

160

180

Fisher Score

Mean Decrease Gini

(a) Fisher score, >M1 ﬂares

(b) Gini importance, >M1 ﬂares

r_value_br_logr

r_value_blos_logr

R−squared=0.5677
correlation=0.7535

wlsg_br

mpil_br

ising_energy_part_br

ising_energy_part_blos

ising_energy_br

mpil_blos

alpha_exp_fft_br

ising_energy_blos

decay_index_blos

decay_index_br

e
r
o
c
S

r
e
h
s
F
e

i

t

a
i
r
a
v
n
U

i

2
1

.

0

.

1

8
0

.

6

.

0

4

.

0

2
0

.

alpha_exp_fft_blos

50

100

150

200

Gini Importance

(c) Correlation, >M1 ﬂares

Figure A.2. Importance of several predictors while predicting >M1 ﬂares.

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 36

 
 
Forecasting Solar Flares Using Machine Learning

B. Prediction Models Resulting From Ranking the Predictors

We employed a backward elimination procedure, eliminating gradually predictors
according to their Fisher score rank, starting form the model with all K = 13
predictors included. In every step, we eliminated the least important predictor
from the set of currently included predictors. So, we obtained prediction results
for models with 2, 3, . . . , 11, 12 predictors included for the ML methods, RF,
SVM and MLP and the conventional statistics methods LM, PR and LG. The
results of this iterative procedure for ﬂares >C1 and >M1 are presented in
Figures B.1 and B.2.

Figure B.1 shows that there is a cut-oﬀ for the number of parameters included
in the RF equal to the 6 most important ones (according to Fisher score in Equa-
tion A.1) above which the RF is advantageous over the other two ML algorithms.
For low-dimensional prediction models (e.g. below 6 included parameters) there
is no special advantage in using RF, and MLP or SVM seem a better choice
then. This ﬁnding shows that among the highly correlated set of predictors, the
MLP and SVM perform well using only a handful of them (below 6), yet the RF
continues to improve its performance in higher-dimensional settings, when the
prediction model includes all 12 most important predictors. There is interest in
investigating the performance of RF when the number of (correlated) predictors
would be twice or three times that of the present study (24-36 predictors). Would
the upwards trend in Figure B.1a continue to hold when the number of included
parameters increases to 24 or 36? We note that RF is the only ML algorithm
in the present study which belongs in the category of “ensemble” methods.
Moreover, in Figure B.1 the performance of the three conventional statistics
methods LM, PR and LG is presented. Clearly, the LM presents the worst
forecasting ability and also we notice that in general the other two methods,
PR and LG, score similar values for the TSS and HSS. Also, it is noteworthy
that the proﬁles of PR and LG are pretty ﬂat as a function of the number of
included predictors, even ﬂatter than the proﬁles from SVM and MLP.

Likewise, Figure B.2 shows that for low-dimensional settings RF is worse than
MLP. The cut-oﬀ seems again to be 6 included parameters. Above this value,
the RF provides better out-of-sample TSS and HSS than MLP. There seems
to be a problematic region between 3 and 6 parameters included for the SVM,
where adding more parameters to the SVM degrades its performance. Above 6
parameters, the SVM performance again improves. Similarly to the >C1 class
ﬂares case, we again notice in Figure B.2 rather ﬂat proﬁles for the TSS and
HSS for the conventional statistics methods, with PR and LG showing better
behaviour than LM.

One general conclusion is that for very few predictors K < 6, all methods
work the same, so for parsimony the conventional statistics methods could be
preferred. This is also true for very small samples N < 2000 (results available
upon request). On the contrary, when K > 6 and N
10, 000 the ML methods
and especially the RF are better.

≥

We note that in Appendix B, the MLP has always 4 hidden nodes and the
SVM has γ and cost parameters analogously to the full K = 13 SVM model for
>C1 and >M1 ﬂares cases.

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 37

RF, RANKING

SVM, RANKING

K. Florios et al.

tss
hss

s
e
u
a
v

l

s

l
l
i

k
s

0
6
.
0

8
5
.
0

6
5
.
0

4
5
.
0

2
5
.
0

0
5
.
0

tss
hss

2

4

6

8

10

12

2

4

6

8

10

12

number of variables

number of variables

(a) RF, ranking, >C1 ﬂares

(b) SVM, ranking, >C1 ﬂares

MLP, RANKING

LM, RANKING

s
e
u
a
v

l

s

l
l
i

k
s

s
e
u
a
v

l

s

l
l
i

k
s

0
6
.
0

8
5

.

0

6
5
0

.

4
5
0

.

2
5
0

.

0
5
0

.

0
6

.

0

8
5

.

0

6
5

.

0

4
5

.

0

2
5
.
0

0
5
.
0

tss
hss

2

4

6

8

10

12

number of variables

(c) MLP, ranking, >C1 ﬂares

PR, RANKING

tss
hss

tss
hss

2

4

6

8

10

12

number of variables

(d) LM, ranking, >C1 ﬂares

LG, RANKING

tss
hss

s
e
u
a
v

l

s

l
l
i

k
s

s
e
u
a
v

l

s

l
l
i

k
s

s
e
u
a
v

l

s

l
l
i

k
s

2
6
.
0

0
6
.
0

8
5
.
0

6
5
.
0

4
5
.
0

2
5
.
0

0
5
.
0

8
4
.
0

0
6
.
0

8
5

.

0

6
5
0

.

4
5
0

.

2
5
0

.

0
5
0

.

0
6

.

0

8
5

.

0

6
5

.

0

4
5

.

0

2
5
.
0

0
5
.
0

2

4

6

8

10

12

2

4

6

8

10

12

number of variables

number of variables

(e) PR, ranking, >C1 ﬂares

(f) LG, ranking, >C1 ﬂares

Figure B.1. Out-of-sample skill scores (TSS and HSS) for the three ML prediction meth-
ods anf the three statistics methods during the ranking procedure for >C1 ﬂares. The thick
continuous lines depict the averages of the skill scores over 30 randomized runs.

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 38

 
 
 
 
 
 
Forecasting Solar Flares Using Machine Learning

RF, RANKING

SVM, RANKING

s
e
u
a
v

l

s

l
l
i

k
s

s
e
u
a
v

l

s

l
l
i

k
s

s
e
u
a
v

l

s

l
l
i

k
s

8
.
0

7
.
0

6
.
0

5
.
0

4
.
0

8
.
0

7

.

0

6

.

0

5
0

.

4
0

.

8
0

.

7
0

.

6

.

0

5

.

0

4
.
0

s
e
u
a
v

l

s

l
l
i

k
s

7
.
0

6
.
0

5
.
0

4
.
0

3
.
0

2
.
0

tss
hss

tss
hss

2

4

6

8

10

12

2

4

6

8

10

12

number of variables

number of variables

(a) RF, ranking, >M1 ﬂares

(b) SVM, ranking, >M1 ﬂares

MLP, RANKING

LM, RANKING

s
e
u
a
v

l

s

l
l
i

k
s

8
.
0

7

.

0

6

.

0

5
0

.

4
0

.

tss
hss

tss
hss

2

4

6

8

10

12

2

4

6

8

10

12

number of variables

number of variables

(c) MLP, ranking, >M1 ﬂares

(d) LM, ranking, >M1 ﬂares

PR, RANKING

LG, RANKING

s
e
u
a
v

l

s

l
l
i

k
s

8
0

.

7
0

.

6

.

0

5

.

0

4
.
0

tss
hss

tss
hss

2

4

6

8

10

12

2

4

6

8

10

12

number of variables

number of variables

(e) PR, ranking, >M1 ﬂares

(f) LG, ranking, >M1 ﬂares

Figure B.2. Same as in Figure B.1 but for >M1 ﬂares.

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 39

 
 
 
 
 
 
K. Florios et al.

C. Validation Results When Predictions Are Issued Only

Once a Day (at Midnight)

We present here forecasting results in the following scenario:

i) The training is perfomed as in the main scenario.

ii) The testing is performed only for the observations in the testing set of the
main scenario which correspond to a time of 00:00 UT. To achieve this,
we ﬁlter for the observations in the previous testing set with midnightSta-
tus=TRUE.

This method of training-testing is called the “hybrid method” where training is
done with a cadence of 3 h and a forecast window of 24 h, and testing is done
with a cadence of 24 h and a forecast window of 24 h. The hybrid method is
preferable over doing a training phase with cadence of 24 h, which would result
in under-trained models, due to the limited sample size during training.

Tables C1 and C2 are analogous to Tables 5 and 6 of the main scenario, but
for midnight (so once a day) only predictions. For completeness, we remind that
Table C1 is for >M1 ﬂare prediction and Table C2 is for >C1 ﬂare prediction.
By comparing Table C1 to Table 5 we see that BS and AUC do not change
much on average when we move from the baseline scenario to the midnight
prediction scenario. Nevertheless, the associated uncertainty increases in the
case of midnight only predictions, since the size of the testing set is smaller
(only one, rather than eight, predictions per day). More signiﬁcant diﬀerences are
observed for BSS since the associated climatology is also diﬀerent. Nevertheless,
the ﬁnding that RF is the best overall method continues to hold.

≈

Similar conclusions can be drawn for the >C1 ﬂare prediction case, so through
Table C2 and Table 6. Here, noticeably, not even the BSS changes signiﬁcantly,
since the underlying climatology seems similar in both cases. This is because,
contrary to >M1 class ﬂares, with a mean frequency of
5%, >C1 class ﬂares
show a mean frequency of

25%.

≈

Finally, Tables C3 and C4 present the skill scores ACC, TSS and HSS for
the midnight prediction scenario analogously to Tables 2 and 3 for the baseline
scenario. For completeness, we notice that Table C3 is for >M1 ﬂare prediction
and Table C4 is for >C1 ﬂare prediction. We see that on average the issuing of
midnight only predictions does not change much the ACC, TSS and HSS with
respect to the probability threshold. For example, on >M1 ﬂares midnight-only
predictions, the RF provides ACC=0.93
±
±
0.03 for probability threshold 15%. Also, for >C1 ﬂares midnight-only predic-
tions, the RF yields ACC=0.85
0.02 for
probability threshold 35%.

0.02 and HSS=0.61

0.04 and HSS=0.47

0.01, TSS=0.63

0.01, TSS=0.73

±

±

±

±

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 40

Forecasting Solar Flares Using Machine Learning

Table C1. Same as Table 5 but for predictions issued only at
midnight.

BS

MLP
0.0320
(0.0031)

LM
0.0328
(0.0025)

PR
0.0305
(0.0025)

LG
0.0305
(0.0025)

RF
0.0262
(0.0022)

SVM
0.0333
(0.0032)

AUC

MLP
0.9342
(0.0131)

LM
0.9245
(0.0111)

PR
0.9419
(0.0087)

LG
0.9412
(0.0089)

RF
0.9558
(0.0081)

SVM
0.8361
(0.0339)

BSS

MLP
0.2311
(0.0671)

LM
0.2122
(0.0316)

PR
0.2681
(0.0391)

LG
0.2686
(0.0421)

RF
0.3722
(0.0397)

SVM
0.2007
(0.0538)

Table C2. Same as Table 6, but for predictions issued only at
midnight.

BS

MLP
0.1142
(0.0041)

LM
0.1273
(0.0034)

PR
0.1181
(0.0037)

LG
0.1169
(0.0038)

RF
0.1023
(0.0037)

SVM
0.1187
(0.0041)

AUC

MLP
0.8771
(0.0078)

LM
0.8673
(0.0079)

PR
0.8696
(0.0079)

LG
0.8696
(0.0079)

RF
0.9004
(0.0074)

SVM
0.8620
(0.0086)

BSS

MLP
0.3970
(0.0190)

LM
0.3281
(0.0143)

PR
0.3767
(0.0163)

LG
0.3826
(0.0169)

RF
0.4597
(0.0169)

SVM
0.3735
(0.0172)

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 41

S
O
L
A
:

K
F
_
e
t
_
a
l
_
1
4
_
0
1
_
2
0
1
8
.
t
e
x
;

1
8

J
a
n
u
a
r
y

2
0
1
8
;

1
:
3
0
;

p
.

4
2

Table C3. Same as Table 2 but for predictions issued only at midnight.

Par

% ACC

MLP
TSS

HSS

ACC

val0
val5
val10
val15
val20
val25
val30
val35
val40
val45
val50
val55
val60
val65
val70
val75
val80
val85
val90
val95
val100

0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.55
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00

0.18
0.90
0.93
0.94
0.95
0.95
0.95
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.00

0.14
0.70
0.66
0.62
0.58
0.55
0.52
0.49
0.46
0.43
0.40
0.37
0.34
0.32
0.29
0.26
0.23
0.20
0.17
0.12
0.00

0.01
0.37
0.43
0.46
0.47
0.48
0.48
0.47
0.47
0.46
0.45
0.43
0.41
0.40
0.38
0.36
0.33
0.30
0.25
0.19
0.00

0.33
0.76
0.88
0.92
0.94
0.95
0.95
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.94
0.87
0.66
0.50
0.48
0.45

LM
TSS

0.29
0.69
0.72
0.59
0.45
0.36
0.30
0.22
0.19
0.16
0.13
0.09
0.07
0.06
0.05
0.04
0.02
0.01
0.01
0.01
0.01

HSS

ACC

0.04
0.19
0.32
0.38
0.38
0.37
0.36
0.29
0.27
0.23
0.21
0.16
0.11
0.10
0.10
0.08
0.04
0.02
0.01
0.01
0.01

0.00
0.84
0.90
0.93
0.94
0.95
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.95
0.91
0.54
0.00

PR
TSS

0.00
0.77
0.73
0.65
0.56
0.49
0.43
0.35
0.31
0.26
0.21
0.18
0.14
0.11
0.09
0.07
0.05
0.04
0.02
0.00
0.00

HSS

ACC

0.00
0.28
0.37
0.42
0.43
0.44
0.44
0.40
0.39
0.35
0.31
0.27
0.23
0.18
0.15
0.13
0.09
0.06
0.03
0.00
0.00

0.00
0.85
0.90
0.93
0.94
0.95
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.95
0.91
0.65
0.00

LG
TSS

0.00
0.77
0.71
0.64
0.54
0.50
0.45
0.38
0.33
0.30
0.26
0.21
0.17
0.14
0.11
0.09
0.07
0.04
0.02
0.00
0.00

HSS

ACC

0.00
0.30
0.38
0.43
0.42
0.45
0.45
0.43
0.39
0.39
0.35
0.31
0.26
0.22
0.18
0.15
0.12
0.06
0.03
0.00
0.00

0.50
0.85
0.91
0.93
0.95
0.96
0.96
0.96
0.96
0.97
0.97
0.97
0.96
0.96
0.96
0.96
0.96
0.95
0.93
0.53
0.00

RF
TSS

0.47
0.78
0.78
0.73
0.67
0.61
0.54
0.48
0.42
0.38
0.34
0.29
0.25
0.21
0.16
0.12
0.09
0.07
0.04
0.01
0.00

HSS

ACC

SVM
TSS

0.07
0.31
0.41
0.47
0.52
0.53
0.53
0.52
0.50
0.48
0.45
0.41
0.37
0.32
0.26
0.20
0.15
0.12
0.07
0.01
0.00

0.00
0.93
0.95
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.96
0.00

0.00
0.58
0.47
0.42
0.38
0.35
0.33
0.31
0.29
0.28
0.26
0.25
0.23
0.22
0.20
0.19
0.17
0.15
0.12
0.08
0.00

HSS

0.00
0.42
0.44
0.45
0.43
0.42
0.40
0.39
0.38
0.36
0.35
0.34
0.33
0.31
0.30
0.28
0.26
0.23
0.20
0.14
0.00

K

.

F
l
o
r
i
o
s

e
t

a
l
.

Table C4. Same as Table 3, but for predictions issued only at midnight.

Par

% ACC

MLP
TSS

HSS

ACC

val0
val5
val10
val15
val20
val25
val30
val35
val40
val45
val50
val55
val60
val65
val70
val75
val80
val85
val90
val95
val100

0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.55
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00

0.00
0.53
0.67
0.73
0.77
0.80
0.81
0.83
0.83
0.84
0.84
0.84
0.84
0.83
0.83
0.82
0.81
0.80
0.78
0.73
0.00

0.00
0.35
0.51
0.56
0.59
0.59
0.58
0.57
0.55
0.53
0.51
0.48
0.45
0.42
0.37
0.32
0.27
0.22
0.14
0.06
0.00

0.00
0.22
0.37
0.44
0.49
0.52
0.54
0.56
0.56
0.55
0.55
0.54
0.51
0.49
0.45
0.40
0.35
0.29
0.20
0.08
0.00

0.40
0.44
0.51
0.58
0.65
0.73
0.79
0.82
0.83
0.84
0.83
0.82
0.81
0.79
0.78
0.77
0.77
0.77
0.76
0.76
0.76

LM
TSS

0.18
0.23
0.32
0.40
0.49
0.55
0.57
0.56
0.52
0.48
0.42
0.33
0.28
0.20
0.15
0.12
0.10
0.09
0.07
0.06
0.05

HSS

ACC

0.10
0.14
0.20
0.26
0.35
0.44
0.50
0.53
0.54
0.53
0.48
0.41
0.35
0.27
0.21
0.17
0.14
0.13
0.10
0.08
0.07

0.00
0.48
0.59
0.66
0.73
0.78
0.80
0.82
0.84
0.84
0.84
0.84
0.83
0.82
0.81
0.80
0.78
0.78
0.77
0.76
0.00

PR
TSS

0.00
0.29
0.41
0.49
0.55
0.57
0.57
0.56
0.54
0.51
0.48
0.44
0.38
0.33
0.28
0.22
0.17
0.14
0.11
0.07
0.00

HSS

ACC

0.00
0.17
0.28
0.35
0.43
0.49
0.53
0.54
0.55
0.55
0.54
0.51
0.45
0.41
0.35
0.29
0.23
0.19
0.15
0.10
0.00

0.00
0.50
0.61
0.68
0.74
0.78
0.81
0.82
0.84
0.84
0.84
0.84
0.83
0.82
0.81
0.80
0.79
0.78
0.77
0.76
0.00

LG
TSS

0.00
0.31
0.44
0.51
0.56
0.58
0.57
0.56
0.54
0.52
0.50
0.46
0.41
0.36
0.31
0.26
0.20
0.15
0.11
0.07
0.00

HSS

ACC

0.00
0.19
0.30
0.38
0.45
0.50
0.53
0.54
0.56
0.56
0.55
0.52
0.48
0.43
0.39
0.33
0.26
0.21
0.16
0.10
0.00

0.27
0.52
0.65
0.72
0.77
0.81
0.83
0.85
0.86
0.86
0.86
0.86
0.85
0.85
0.84
0.82
0.81
0.80
0.78
0.76
0.00

RF
TSS

0.02
0.35
0.50
0.57
0.61
0.63
0.64
0.63
0.60
0.58
0.55
0.51
0.47
0.43
0.38
0.33
0.27
0.21
0.14
0.07
0.00

HSS

ACC

SVM
TSS

0.01
0.22
0.35
0.43
0.50
0.55
0.59
0.61
0.62
0.61
0.60
0.58
0.55
0.51
0.47
0.41
0.35
0.28
0.20
0.10
0.00

0.00
0.29
0.47
0.75
0.80
0.82
0.83
0.84
0.84
0.84
0.84
0.84
0.84
0.83
0.82
0.82
0.81
0.80
0.79
0.77
0.00

0.00
0.05
0.26
0.57
0.59
0.57
0.55
0.54
0.51
0.49
0.47
0.45
0.42
0.37
0.34
0.32
0.28
0.23
0.17
0.12
0.00

HSS

0.00
0.02
0.16
0.47
0.53
0.55
0.55
0.56
0.55
0.54
0.53
0.52
0.49
0.45
0.42
0.39
0.36
0.30
0.24
0.16
0.00

F
o
r
e
c
a
s
t
i
n
g

S
o
l
a
r

F
l
a
r
e
s
U
s
i
n
g
M
a
c
h
i
n
e

L
e
a
r
n
i
n
g

S
O
L
A
:

K
F
_
e
t
_
a
l
_
1
4
_
0
1
_
2
0
1
8
.
t
e
x
;

1
8

J
a
n
u
a
r
y

2
0
1
8
;

1
:
3
0
;

p
.

4
3

K. Florios et al.

D. Concluding Remarks on ML versus Statistical Methods for

Flare Forecasting

In order to assess the overall forecasting ability of ML vs. Statistical approaches
in our dataset and problem deﬁnition, we employ the weighted-sum (WS) mul-
ticriteria ranking approach (Greco, Figueira, and Ehrgott, 2016), using a com-
posite index (CI) deﬁned in Equation D.1:

(

{

×

+

+

1
3

CI =

TSSmin

TSSmin

HSSmin

HSSmin

) . (D.1)

ACCmin

ACCmin

TSS
−
TSSmax −

The CI value is computed for 6

ACC
−
ACCmax −

MLP, LM, PR, LG, RF, SVM
}

HSS
−
HSSmax −
21 = 126 probabilistic classiﬁers using the
set of methods
and a probability threshold
grid of 5%. Then, the 126 probabilistic classiﬁers are ranked in non-increasing
values of the CI index. Notice the normalization which is done for ACC, TSS
and HSS, so that each metric over the set of alternatives takes values in the
range [0, 1]. The normalization is useful because the range of values for ACC is
diﬀerent from the range of values for TSS and HSS. Also, notice that ACCmin
is the minimum of ACC over all 126 alternative models. Likewise, ACCmax is
the maximum ACC obtained over all 126 alternative models. Similar facts hold
for TSSmin, TSSmax, HSSmin and HSSmax. Analytically, Table D1 presents the
results of the multicriteria ranking approach for all methods used using various
probability thresholds, especially for the >C1 ﬂare forecasting case. Table D2
conveys a similar ranking of all methods developed in this paper, but for the
>M1 ﬂare prediction.

Figure D.1 summarizes the results shown in Tables D1 and D2, so that the
diﬀerences between ML and Statistical methods are highlighted (e.g. see Figure
D.1b and D.1d). Similarly, conclusions for the merit of all methods developed
in this paper can be drawn in Figures D.1a and D.1c. The top 100τ percentile
methods are the ones ranked in the corresponding positions of Tables D1 and
D2. For example, the top 16.6%(1/6) methods are the ones ranked in positions
1-21. For small values of τ one gets the best methods designated as the top
100τ % methods. From Figure D.1 we see that both for >C1 and >M1 ﬂares, the
RF has the greatest frequency in the top 16.6% percentile of methods, with a
frequency of 33.3%. This means, that in Tables D1 and D2, in positions 1-21, the
RF method appears 7 times, in each Table. Also, in Figure D.1b we see that for
>C1 ﬂares the top 16.6% methods are of type ML with a frequency 71% (versus
29% for Statistical Methods). Similarly, in Figure D.1d ML dominates in the top
16.6% methods with a frequency 62% (versus 38% for Statistical Methods).

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 44

Forecasting Solar Flares Using Machine Learning

Table D1. Ranking of all models with ML and statistical methods with the multicriteria WS
method with respect to the three criteria: ACC, TSS and HSS and using a weight vector equal
to w = [1/3, 1/3, 1/3] for >C1 ﬂare forecasting. The methods are ranked in decreasing order
of CI, for varying levels of probability thresholds used, using a grid of 5% for the probability
thresholds. We notice what the six best positions of the ranking are covered by the RF method
for various probability thresholds.

rank model

CI

rank model

CI

rank model

CI

PANEL A >C1 ﬂares

1 RF-val35
2 RF-val40
3 RF-val45
4 RF-val30
5 RF-val50
6 RF-val25
7
LG-val35
8 MLP-val35
9

PR-val35

10 MLP-val40
SVM-val25
11
12
LG-val40
13 RF-val55
14
SVM-val30
15 MLP-val30
16
17
18 MLP-val45
19
20
21

LG-val30
SVM-val35
SVM-val20

PR-val40
LM-val35

PR-val15

PR-val65
LM-val55

LM-val20
LG-val70
SVM-val75

64
65
66 MLP-val75
67
68 MLP-val10
69
70
71
72 RF-val10
73
PR-val70
74 RF-val80
SVM-val80
75
76 MLP-val80
LM-val60
77
LG-val75
78
PR-val75
79
LG-val10
80
81
SVM-val85
82 MLP-val85
83
84

PR-val10
LM-val65

0.983
0.983
0.971
0.970
0.953
0.938
0.935
0.934
0.933
0.932
0.932
0.930
0.929
0.928
0.925
0.925
0.923
0.923
0.921
0.920
0.920

0.738
0.735
0.727
0.724
0.720
0.718
0.715
0.709
0.699
0.682
0.671
0.667
0.666
0.664
0.657
0.622
0.621
0.613
0.596
0.594
0.593

PR-val45

PR-val30
LG-val45
LM-val40

22
23
24
25 MLP-val50
26
27 MLP-val25
SVM-val40
28
LM-val30
29
30 RF-val60
LG-val25
31
LG-val50
32
33
SVM-val45
PR-val25
34
35 RF-val20
36 MLP-val55
37
38 MLP-val20
LM-val45
39
SVM-val50
40
41
LG-val55
42 RF-val65

PR-val50

0.919
0.914
0.907
0.907
0.907
0.906
0.906
0.898
0.898
0.892
0.889
0.887
0.886
0.886
0.885
0.877
0.875
0.869
0.867
0.861
0.859

PANEL B >C1 ﬂares

LG-val80
85
86
LM-val15
87 RF-val85
PR-val80
88
SVM-val90
89
LM-val70
90
LG-val85
91
PR-val85
92
93 MLP-val90
94 RF-val5
95 MLP-val5
96 RF-val90
LM-val75
97
LM-val10
98
LG-val90
99
SVM-val95
100
PR-val90
101
LM-val80
102
LG-val5
103
LM-val85
104
PR-val5
105

0.593
0.592
0.587
0.566
0.552
0.537
0.533
0.515
0.510
0.506
0.501
0.500
0.494
0.488
0.484
0.484
0.476
0.461
0.441
0.431
0.427

43 MLP-val60
SVM-val55
44
SVM-val15
45
LG-val20
46
PR-val55
47
LM-val25
48
PR-val20
49
SVM-val60
50
51 MLP-val65
52 MLP-val15
53
LG-val60
54 RF-val15
55 RF-val70
LM-val50
56
SVM-val65
57
58
PR-val60
59 MLP-val70
LG-val65
60
SVM-val70
61
62
LG-val15
63 RF-val75

LM-val95
LM-val5
LM-val100
SVM-val10
LM-val0
SVM-val5

PR-val95
106
107
LG-val95
108 RF-val95
109
LM-val90
110 MLP-val95
111
112
113
114
115
116
117 RF-val0
118 MLP-val0
119 MLP-val100
PR-val0
120
PR-val100
121
LG-val0
122
123
LG-val100
124 RF-val100
SVM-val0
125
SVM-val100
126

0.856
0.849
0.843
0.842
0.840
0.835
0.828
0.824
0.821
0.821
0.818
0.810
0.809
0.803
0.789
0.787
0.778
0.768
0.751
0.748
0.747

0.420
0.417
0.410
0.405
0.400
0.387
0.372
0.370
0.363
0.291
0.142
0.131
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 45

Table D2. Same as Table D1 but for >M1 ﬂare forecasting.

rank model

CI

rank model

CI

rank model

CI

K. Florios et al.

1 RF-val20
2 RF-val25
3 RF-val15
4 RF-val30
5 RF-val10
6 RF-val35
7 MLP-val15
8 MLP-val20
9 MLP-val10
10 MLP-val25
PR-val15
11
PR-val20
12
LG-val15
13
14
LG-val20
15 RF-val40
16 MLP-val30
17
18
19
20
21

LM-val15
SVM-val5
PR-val10
LG-val10
LG-val25

SVM-val65
LG-val60

PR-val50
SVM-val55
LG-val55

64
65
66
67 MLP-val80
SVM-val60
68
PR-val55
69
LM-val40
70
71 RF-val65
72
73
74 MLP-val85
SVM-val70
75
PR-val60
76
LG-val65
77
SVM-val75
78
LM-val45
79
80 MLP-val90
81 RF-val70
82
83
84

SVM-val80
PR-val65
LG-val70

PANEL A >M1 ﬂares

PR-val30
SVM-val15

22
PR-val25
23 MLP-val5
24 MLP-val35
SVM-val10
25
LG-val30
26
LM-val20
27
28
LM-val10
29 MLP-val40
30
31
32 RF-val45
33 RF-val5
34 MLP-val45
LG-val35
35
LG-val5
36
37
SVM-val20
38 MLP-val50
PR-val5
39
LM-val25
40
41
PR-val35
42 RF-val50

0.839
0.836
0.836
0.834
0.827
0.823
0.822
0.820
0.817
0.816
0.815
0.809
0.805
0.799
0.796
0.792
0.790
0.786
0.786
0.782
0.778

PANEL B >M1 ﬂares

LM-val50
85
SVM-val85
86
PR-val70
87
88
LM-val55
89 RF-val75
90
LG-val75
91 MLP-val95
LM-val60
92
PR-val75
93
SVM-val90
94
95
LG-val80
96 RF-val80
LM-val65
97
PR-val80
98
LG-val85
99
LM-val70
100
SVM-val95
101
102
PR-val85
103 RF-val85
104
LM-val75
105 RF-val0

0.571
0.550
0.546
0.546
0.543
0.539
0.539
0.521
0.516
0.512
0.506
0.492
0.489
0.483
0.466
0.461
0.450
0.444
0.442
0.441
0.433

0.938
0.932
0.927
0.912
0.890
0.883
0.870
0.867
0.866
0.859
0.856
0.856
0.854
0.851
0.851
0.849
0.849
0.848
0.845
0.843
0.840

0.679
0.679
0.676
0.663
0.662
0.648
0.646
0.645
0.644
0.644
0.633
0.625
0.612
0.607
0.604
0.603
0.594
0.593
0.579
0.576
0.572

LM-val30
PR-val40
LG-val45
SVM-val35

43
SVM-val25
44 MLP-val55
LG-val40
45
46
SVM-val30
47 MLP-val60
48
49
50
51
52 RF-val55
53 MLP-val65
SVM-val40
54
55
PR-val45
56 MLP-val70
LG-val50
57
SVM-val45
58
LM-val5
59
60
LM-val35
61 RF-val60
62
SVM-val50
63 MLP-val75

LM-val80
106
LG-val90
107
PR-val90
108
109
LM-val85
110 RF-val90
LM-val90
111
PR-val95
112
LG-val95
113
LM-val95
114
115
LM-val100
116 RF-val95
LM-val0
117
118 MLP-val0
119 MLP-val100
PR-val0
120
PR-val100
121
LG-val0
122
123
LG-val100
124 RF-val100
SVM-val0
125
SVM-val100
126

0.774
0.773
0.770
0.756
0.754
0.749
0.749
0.743
0.741
0.736
0.735
0.725
0.713
0.713
0.709
0.709
0.699
0.694
0.693
0.692
0.689

0.424
0.413
0.410
0.404
0.392
0.381
0.378
0.372
0.364
0.352
0.334
0.251
0.108
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 46

Forecasting Solar Flares Using Machine Learning

(a) ALL methods, >C1 ﬂares

(b) ML vs. statistical, >C1 ﬂares

(c) ALL methods, >M1 ﬂares

(d) ML vs. statistical, >M1 ﬂares

Figure D.1. Descriptive statistics on the frequency with which every forecasting method for
any probability threshold presents itself to the top 100τ % percentile of the CI distribution.
Panels (a) and (c) describe frequencies for all methods and panels (b) and (d) group the results
by category of methods (e.g. ML vs. statistical methods). For example, for >C1 ﬂares in panel
(a), notice that the top 16.6% methods are dominated by RF with a frequency of 7/21=33%.
Likewise, for >M1 ﬂares in panel (c), notice that the top 16.6% methods are again dominated
by RF with a frequency of 7/21=33%.

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 47

K. Florios et al.

References

Ahmed, O.W., Qahwaji, R., Colak, T., Higgins, P.A., Gallagher, P.T., Bloomﬁeld, D.S.: 2013,
Solar ﬂare prediction using advanced feature extraction, machine learning, and feature
selection. Solar Phys. 283(1), 157. DOI.

Ahmed, O., Qahwaji, R., Colak, T., Dudok De Wit, T., Ipson, S.: 2010, A new technique
for the calculation and 3D visualisation of magnetic complexities on solar satellite images.
Visual Comput. 26, 385. DOI.

Al-Ghraibah, A., Boucheron, L.E., McAteer, R.T.J.: 2015, An automated classiﬁcation ap-
proach to ranking photospheric proxies of magnetic energy build-up. Astron. Astrophys.
579, A64. DOI.

Alissandrakis, C.E.: 1981, On the computation of constant alpha force-free magnetic ﬁeld.

Astron. Astrophys. 100, 197.

Barnes, G., Longcope, D.W., Leka, K.D.: 2005, Implementing a Magnetic Charge Topology

Model for Solar Active Regions. Astrophys. J. 629, 561. DOI.

Barnes, G., Schanche, N., Leka, K., Aggarwal, A., Reeves, K.: 2016, A comparison of classiﬁers
for solar energetic events. Proceedings of the International Astronomical Union 12(S325),
201. DOI.

Bloomﬁeld, D.S., Higgins, P.A., McAteer, R.T.J., Gallagher, P.T.: 2012, Toward reliable

benchmarking of solar ﬂare forecasting methods. Astrophys. J. Lett. 747(2). DOI.

Bobra, M.G., Couvidat, S.: 2015, Solar ﬂare prediction using SDO/HMI vector magnetic ﬁeld

data with a machine-learning algorithm. Astrophys. J. 798(2), 135. DOI.

Bobra, M.G., Sun, X., Hoeksema, J.T., Turmon, M., Liu, Y., Hayashi, K., Barnes, G., Leka,
K.D.: 2014, The Helioseismic and Magnetic Imager (HMI) Vector Magnetic Field Pipeline:
SHARPs - Space-Weather HMI Active Region Patches. Solar Phys. 289(9), 3549. DOI.
Boucheron, L.E., Al-Ghraibah, A., McAteer, R.T.J.: 2015, Prediction of solar ﬂare size and
time-to-ﬂare using support vector machine regression. Astrophys. J. 812(1), 51. DOI.

Breiman, L.: 2001, Random forests. Mach. Learn. 45(1), 5. DOI.
Breiman, L., Friedman, J., Stone, C.J., Olshen, R.A.: 1984, Classiﬁcation and regression trees.
Brier, G.W.: 1950, Veriﬁcation of forecasts expressed in terms of probability. Mon. Weather

Rev. 78, 1. DOI.

Chang, C.-C., Lin, C.-J.: 2011, LIBSVM: A library for support vector machines. ACM T. Intel.
Syst. Tec. 2, 27:1. Software available at http://www.csie.ntu.edu.tw/∼cjlin/libsvm. DOI.
Chang, Y.-W., Lin, C.-J.: 2008, Feature ranking using linear svm. In: Guyon, I., Aliferis,
C., Cooper, G., Elisseeﬀ, A., Pellet, J.-P., Spirtes, P., Statnikov, A. (eds.) Proceedings of
the Workshop on the Causation and Prediction Challenge at WCCI 2008, Proceedings of
Machine Learning Research 3, 53.

Chen, Y.-W., Lin, C.-J.: 2006, Combining svms with various feature selection strategies,

Springer Berlin Heidelberg, 315. DOI.

Colak, T., Qahwaji, R.: 2009, Automated solar activity prediction: A hybrid computer platform
using machine learning and solar imaging for automated prediction of solar ﬂares. Space
Weather 7(6), S06001. DOI.

de Souza, R.S., Cameron, E., Killedar, M., Hilbe, J., Vilalta, R., Maio, U., Biﬃ, V., Ciardi,
B., Riggs, J.D.: 2015, The overlooked potential of generalized linear models in astronomy,
i: Binomial regression. Astron. Comput. 12, 21 . DOI.

Falconer, D.A., Moore, R.L., Gary, G.A.: 2008, Magnetogram Measures of Total Nonpoten-
tiality for Prediction of Solar Coronal Mass Ejections from Active Regions of Any Degree
of Magnetic Complexity. Astrophys. J. 689, 1433. DOI.

Falconer, D.A., Moore, R.L., Barghouty, A.F., Khazanov, I.: 2014, Mag4 versus alternative
techniques for forecasting active region ﬂare productivity. Space Weather 12(5), 306. DOI.
Falconer, D., Barghouty, A.F., Khazanov, I., Moore, R.: 2011, A tool for empirical forecasting
of major ﬂares, coronal mass ejections, and solar particle events from a proxy of active-region
free magnetic energy. Space Weather 9(4), n/a. S04003. DOI.

Falconer, D.A., Moore, R.L., Barghouty, A.F., Khazanov, I.: 2012, Prior Flaring as a Com-
plement to Free Magnetic Energy for Forecasting Solar Eruptions. Astrophys. J. 757, 32.
DOI.

Fang, F., Manchester, W. IV, Abbett, W.P., van der Holst, B.: 2012, Buildup of Magnetic
Shear and Free Energy during Flux Emergence and Cancellation. Astrophys. J. 754, 15.
DOI.

Fern´andez-Delgado, M., Cernadas, E., Barro, S., Amorim, D.: 2014, Do we need hundreds of
classiﬁers to solve real world classiﬁcation problems? J. Mach. Learn. Res. 15, 3133.

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 48

Forecasting Solar Flares Using Machine Learning

Fletcher, L., Dennis, B.R., Hudson, H.S., Krucker, S., Phillips, K., Veronig, A., Battaglia, M.,
Bone, L., Caspi, A., Chen, Q., Gallagher, P., Grigis, P.T., Ji, H., Liu, W., Milligan, R.O.,
Temmer, M.: 2011, An Observational Overview of Solar Flares. Space Sci. Rev. 159, 19.
DOI.

Georgoulis, M.K., Rust, D.M.: 2007, Quantitative Forecasting of Major Solar Flares. Astro-

phys. J. Lett. 661, L109. DOI.

Granett, B.R.: 2017, Probing the sparse tails of redshift distributions with voronoi tessellations.

Astron. Comput. 18, 18 . DOI.

Greco, S., Figueira, J., Ehrgott, M.: 2016, Multiple criteria decision analysis, 2nd edn..
Greene, W.H.: 2002, Econometric analysis, 5th edn..
Guerra, J.A., Pulkkinen, A., Uritsky, V.M., Yashiro, S.: 2015, Spatio-Temporal Scaling of
Turbulent Photospheric Line-of-Sight Magnetic Field in Active Region NOAA 11158. Solar
Phys. 290, 335. DOI.

Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, P., Witten, I.H.: 2009, The weka
data mining software: an update. ACM SIGKDD explorations newsletter 11(1), 10. DOI.
Hanssen, A., Kuipers, W.: 1965, On the relationship between the frequency of rain and various

meteorological parameters:(with reference to the problem of objective forecasting).

Hastie, T., Tibshirani, R., Friedman, J.: 2009, The elements of statistical learning: data mining,

inference and prediction, 2nd edn..

Heidke, P.: 1926, Berechnung des erfolges und der g¨ute der windst¨arkevorhersagen im

sturmwarnungsdienst. Geograﬁska Annaler 8, 301. DOI.

Hornik, K., Stinchcombe, M., White, H.: 1989, Multilayer feedforward networks are universal

approximators. Neural Netw. 2(5), 359. DOI.

Kliem, B., T¨or¨ok, T.: 2006, Torus Instability. Phys. Rev. Lett. 96(25), 255002. DOI.
Laboratory, N.-R.A.: 2015, veriﬁcation: Weather forecast veriﬁcation utilities. R package

version 1.42.

Li, R., Cui, Y., He, H., Wang, H.: 2008, Application of support vector machine combined
with k-nearest neighbors in solar ﬂare and solar proton events forecasting. Adv. Space Res.
42(9), 1469. DOI.

Liaw, A., Wiener, M.: 2002, Classiﬁcation and regression by randomforest. R News 2(3), 18.
Liu, C., Deng, N., Wang, J.T.L., Wang, H.: 2017, Predicting solar ﬂares using sdo /hmi vector
magnetic data products and the random forest algorithm. Astrophys. J. 843(2), 104. DOI.

MacKay, D.J.C.: 2003, Information theory, inference, and learning algorithms.
Marzban, C.: 2004, The roc curve and the area under it as performance measures. Weather

and Forecasting 19(6), 1106. DOI.

Meyer, D., Leisch, F., Hornik, K.: 2003, The support vector machine under test. Neurocom-

puting 55(1), 169. DOI.

Meyer, D., Dimitriadou, E., Hornik, K., Weingessel, A., Leisch, F.: 2015, e1071: Misc functions
of the department of statistics, probability theory group (formerly: E1071), tu wien. R
package version 1.6-7.

Pesnell, W.D., Thompson, B.J., Chamberlin, P.C.: 2012, The Solar Dynamics Observatory

(SDO). Solar Phys. 275, 3. DOI.

Prieto, A., Prieto, B., Ortigosa, E.M., Ros, E., Pelayo, F., Ortega, J., Rojas, I.: 2016,
Neural networks: An overview of early research, current frameworks and new challenges.
Neurocomputing 214, 242. DOI.

R Core Team: 2016, R: A language and environment for statistical computing. R Foundation

for Statistical Computing, Vienna, Austria. R Foundation for Statistical Computing.

Scherrer, P.H., Bogart, R.S., Bush, R.I., Hoeksema, J.T., Kosovichev, A.G., Schou, J.et. al.
.: 1995, The Solar Oscillations Investigation - Michelson Doppler Imager. Solar Phys. 162,
129. DOI.

Scherrer, P.H., Schou, J., Bush, R.I., Kosovichev, A.G., Bogart, R.S., Hoeksema, J.T., Liu, Y.,
Duvall, T.L., Zhao, J., Title, A.M., Schrijver, C.J., Tarbell, T.D., Tomczyk, S.: 2012, The
Helioseismic and Magnetic Imager (HMI) Investigation for the Solar Dynamics Observatory
(SDO). Solar Phys. 275, 207. DOI.

Schrijver, C.J.: 2007, A Characteristic Magnetic Field Pattern Associated with All Major Solar

Flares and Its Use in Flare Forecasting. Astrophys. J. Lett. 655, L117. DOI.

Schuh, M.A., Angryk, R.A., Martens, P.C.: 2015, Solar image parameter data from the sdo:

Long-term curation and data mining. Astron. Comput. 13, 86 . DOI.

Sing, T., Sander, O., Beerenwinkel, N., Lengauer, T.: 2005, Rocr: visualizing classiﬁer

performance in r. Bioinformatics 21(20), 7881. DOI.

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 49

K. Florios et al.

Song, H., Tan, C., Jing, J., Wang, H., Yurchyshyn, V., Abramenko, V.: 2009, Statistical
assessment of photospheric magnetic features in imminent solar ﬂare predictions. Solar
Phys. 254(1), 101. DOI.

Vapnik, V.: 1998, Statistical learning theory.
Venables, W.N., Ripley, B.D.: 2002, Modern applied statistics with s, 4th edn., New York.

DOI.

Vilalta, R., Gupta, K.D., Macri, L.: 2013, A machine learning approach to cepheid variable
star classiﬁcation using data alignment and maximum likelihood. Astron. Comput. 2, 46 .
DOI.

Wang, H.N., Cui, Y.M., Li, R., Zhang, L.Y., Han, H.: 2008, Solar ﬂare forecasting model
supported with artiﬁcial neural network techniques. Adv. Space Res. 42(9), 1464 . DOI.

Wilks, D.S.: 2011, Statistical methods in the atmospheric sciences 100.
Winkelmann, R., Boes, S.: 2006, Analysis of microdata, Berlin and Heidelberg.
Witten, I.H., Frank, E., Hall, M.A., Pal, C.J.: 2016, Data mining: Practical machine learning

tools and techniques.

Yu, D., Huang, X., Wang, H., Cui, Y.: 2009, Short-term solar ﬂare prediction using a sequential

supervised learning method. Solar Phys. 255(1), 91. DOI.

Yuan, Y., Shih, F.Y., Jing, J., Wang, H.-M.: 2010, Automated ﬂare forecasting using a
statistical learning technique. Research in Astronomy and Astrophysics 10(8), 785. DOI.
Zuccarello, F.P., Aulanier, G., Gilchrist, S.A.: 2015, Critical Decay Index at the Onset of Solar

Eruptions. Astrophys. J. 814, 126. DOI.

SOLA: KF_et_al_14_01_2018.tex; 18 January 2018; 1:30; p. 50

