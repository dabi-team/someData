Parallel and High-Fidelity Text-to-Lip Generation

Jinglin Liu*1, Zhiying Zhu*1, Yi Ren*1,
Wencan Huang1, Baoxing Huai2, Nicholas Yuan2, Zhou Zhao†1
1Zhejiang University, China
2Huawei Cloud
{jinglinliu,zhyingzh,rayeren,huangwencan,zhaozhou}@zju.edu.cn,
{huaibaoxing,nicholas.yuan}@huawei.com

1
2
0
2
c
e
D
0
2

]

M
M

.
s
c
[

2
v
1
3
8
6
0
.
7
0
1
2
:
v
i
X
r
a

Abstract

As a key component of talking face generation, lip move-
ments generation determines the naturalness and coherence
of the generated talking face video. Prior literature mainly fo-
cuses on speech-to-lip generation while there is a paucity in
text-to-lip (T2L) generation. T2L is a challenging task and ex-
isting end-to-end works depend on the attention mechanism
and autoregressive (AR) decoding manner. However, the AR
decoding manner generates current lip frame conditioned on
frames generated previously, which inherently hinders the in-
ference speed, and also has a detrimental effect on the quality
of generated lip frames due to error propagation. This encour-
ages the research of parallel T2L generation. In this work, we
propose a parallel decoding model for fast and high-ﬁdelity
text-to-lip generation (ParaLip). Speciﬁcally, we predict the
duration of the encoded linguistic features and model the tar-
get lip frames conditioned on the encoded linguistic features
with their duration in a non-autoregressive manner. Further-
more, we incorporate the structural similarity index loss and
adversarial learning to improve perceptual quality of gener-
ated lip frames and alleviate the blurry prediction problem.
Extensive experiments conducted on GRID and TCD-TIMIT
datasets demonstrate the superiority of proposed methods.
Video samples are available via https://paralip.github.io/.

1

Introduction

In the modern service industries, talking face generation has
broad application prospects such as avatar, virtual assistant,
movie animation, teleconferencing, etc. (Zhu et al. 2020). As
a key component of talking face generation, lip movements
generation (a.k.a. lip generation) determines the naturalness
and coherence of the generated talking face video. Lip gen-
eration aims to synthesize accurate mouth movements video
corresponding to the linguistic content information carried
in speech or pure text.

Mainstream literature focuses on speech-to-lip (S2L) gen-
eration while there is a paucity in text-to-lip (T2L) genera-
tion. Even so, T2L generation is very crucial and has con-
siderable merits compared to S2L since 1) text data can be
obtained or edited more easily than speech, which makes
T2L generation more convenient; and 2) T2L extremely

*Equal contribution.
†Corresponding author

Copyright © 2022, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

Figure 1: The task description of T2L generation. The model
takes in an arbitrary source text sequence and a single iden-
tity lip image to synthesize the target lip movements video.
And in this ﬁgure we can see that the generated video loses
the linguistic information gradually and ﬁnally becomes
fuzzy and motionless (lip frames in the red box), which is
the intractable problem existing in AR T2L models due to
error propagation.

preserves privacy especially in the society where the deep
learning techniques are so developed that a single sentence
speech could expose an unimaginable amount of personal
information.

However, end-to-end T2L task (shown in Figure 1) is
challenging. Unlike S2L task where the mapping relation-
ship between the sequence length of source speech and tar-
get video is certain (according to audio sample rate and
fps), there is an uncertain sequence length discrepancy be-
tween source and target in T2L task. The traditional tempo-
ral convolutional networks become impractical. Hence, ex-
isting works view T2L as a sequence-to-sequence task and
tackle it by leveraging the attention mechanism and autore-
gressive (AR) decoding manner. The AR decoding manner
brings two drawbacks: 1) it inherently hinders the inference
speed since its decoder generates target lips one by one au-
toregressively with the causal structure. Consequently, gen-
erating a single sentence of short video consumes about 0.5-
1.5 seconds even on GPU, which is not acceptable for indus-
trial applications such as real-time interactions with avatar or
virtual assistant, real-time teleconferencing and document-
level audio-visual speech synthesis, etc. 2) It has a detrimen-
tal effect on the quality of generated lips due to error prop-

place white by u three againsilplacewhitebyuthreeagainsilground truthgenerated lip video( by AR T2L model )identity liparbitrary textrandom selection… 
 
 
 
 
 
agation1, which is frequently discussed in neural machine
translation and image caption ﬁeld (Bengio et al. 2015; Wu
et al. 2018). Worse still, error propagation is more obvious in
AR lip generation than in other tasks, because the mistakes
could take place at more dimensions (every pixel with three
channels in generated image) and there is information loss
during the down-sampling when sending the last generated
lip frame to predict current one. Although prior works al-
leviate the error propagation by incorporating the technique
of location-sensitive attention, it still has an unsatisfying per-
formance on long-sequence datasets due to accumulated pre-
diction error.

To address such limitations, we turn to non-autoregressive
(NAR) approaches. NAR decoding manner generates all the
target tokens in parallel, which has already pervaded multi-
ple research ﬁelds such as neural machine translation (Gu
et al. 2018; Lee, Mansimov, and Cho 2018; Ghazvinine-
jad et al. 2019; Ma et al. 2019), speech recognition (Chen
et al. 2019; Higuchi et al. 2020), speech synthesis (Ren
et al. 2019; Peng et al. 2020; Miao et al. 2020), image
captioning (Deng et al. 2020) and lip reading (Liu et al.
2020). These works utilize the NAR decoding in sequence-
to-sequence tasks to reduce the inference latency or generate
length-controllable sequence.

In this work, we propose an NAR model for parallel and
high-ﬁdelity T2L generation (ParaLip). ParaLip predicts the
duration of the encoded linguistic features and models the
target lip frames conditioned on the encoded linguistic fea-
tures with their duration in a non-autoregressive manner.
Furthermore, we leverage structural similarity index (SSIM)
loss to supervise ParaLip generating lips with better per-
ceptual quality. Finally, using only reconstruction loss and
SSIM loss is insufﬁcient to generate distinct lip images with
more realistic texture and local details (e.g.wrinkles, beard
and teeth), and therefore we adopt adversarial learning to
mitigate this problem.

Our main contributions can be summarized as follows: 1)
We point out and analyze the unacceptable inference latency
and intractable error propagation existing in AR T2L gen-
eration. 2) To circumvent these problems, we propose Par-
aLip to generate high-quality lips with low inference latency.
And as a byproduct of ParaLip, the duration predictor in Par-
aLip could be leveraged in an NAR text-to-speech model,
which naturally enables the synchronization in audio-visual
speech synthesis task. 3) We explore the source-target align-
ment method when the audio is absent even in the train-
ing set. Extensive experiments demonstrate that ParaLip
generates the competitive lip movements quality compared
with state-of-the-art AR T2L model and exceeds the base-
line AR model TransformerT2L by a notable margin. In the
meanwhile, ParaLip exhibits distinct superiority in inference
speed, which truly provides the possibility to bring T2L gen-
eration from laboratory to industrial applications.

1Error propagation means if a token is mistakenly predicted at
inference stage, the error will be propagated and the future tokens
conditioned on this one will be inﬂuenced (Bengio et al. 2015; Wu
et al. 2018).

2 Related Work

2.1 Talking Face Generation
Talking face generation aims to generate realistic talking
face video and covers many applications such as avatar and
movie animation. There is a branch of works in computer
graphics (CG) ﬁeld exploring it (Wang et al. 2011; Fan et al.
2015; Suwajanakorn, Seitz, and Kemelmacher-Shlizerman
2017; Yu et al. 2019; Abdelaziz et al. 2020) through hid-
den Markov models or deep neural network. These works
synthesize the whole face by generating the intermediate
parameters, which can then be used to deform a 3D face.
Thanks to the evolved convolutional neural network (CNN)
and high-performance computing resources, end-to-end sys-
tems which synthesize 2D talking face images by CNN
rather than rendering methods in CG, have been presented
recently in the computer vision (CV) ﬁeld (Kumar et al.
2017; Chung, Jamaludin, and Zisserman 2017; Chen et al.
2018; Vougioukas, Petridis, and Pantic 2019; Zhou et al.
2019; Zhu et al. 2020; Zheng et al. 2020; Prajwal et al.
2020). Most of them focus on synthesizing lip movements
images and then transforming them to faces. We mainly take
these works in CV ﬁeld into consideration and broadly di-
vide them into two streams as the following paragraphs.

Speech-to-Lip Generation Previous speech-driven works
e.g.Chung, Jamaludin, and Zisserman (2017) simply gen-
erate the talking face images conditioned on the encoded
speech and the encoded face image carrying the identity
information. To synthesize more accurate and distinct lip
movements, Chen et al. (2018) introduce the task of speech-
to-lip generation using lip image as the identity information.
Further, Song et al. (2019) add a lip-reading discriminator to
focus on the mouth region, and Zhu et al. (2020) add the dy-
namic attention on lip area to synthesize talking face while
keeping the lip movements realistic. Prajwal et al. (2020)
propose a pre-trained lip-syncing discriminator to synthesize
talking face with speech-consistent lip movements.

Text-to-Lip Generation The literature of direct text-to-
lip generation is rare. Some text-driven approaches ei-
ther cascade the text-to-speech and speech-to-lip generation
model(KR et al. 2019; Kumar et al. 2017), or combine the
text feature with speech feature together to synthesize lip
movements (Yu, Yu, and Ling 2019). Fried et al. (2019) edit
a given video based on pure speech-aligned text sequence.
Unlike the scenario where source speech or video is given,
the sequence length of target lip frames is uncertain with
only text input. Existing work (Chen et al. 2020) depends on
the attention mechanism and AR decoding method to gener-
ate the target lip frames until the stop token is predicted.

2.2 Non-Autoregressive Sequence Generation
In sequence-to-sequence tasks, an autoregressive (AR)
model takes in a source sequence and then generates to-
kens of the target sentence one by one with the causal struc-
ture at inference (Sutskever, Vinyals, and Le 2014; Vaswani
et al. 2017). Since the AR decoding manner causes the high
inference latency, many non-autoregressive (NAR) models,
which generate target tokens conditionally independent of

(a) ParaLip.

(b) Text Encoder with
Length Regulator.

(c) Motion De-
coder.

(d) Video Decoder with
multiple Image Decoders.

Figure 2: The overall architecture for ParaLip. In subﬁgure (a), Identity Encoder sends out residual information at every con-
volutional layer. In subﬁgure (b), Length Regulator expands the text sequence according to ground truth duration in training or
predicted duration in inference. In subﬁgure (c), Motion Decoder models lip movement information sequence from linguistic
information sequence. In subﬁgure (d), there are T Image Decoders placed parallel in Video Decoder. The τ -th Image Decoder
takes in motion information at τ time and generates lip image at τ time. T means total number of lip frames.

each other, have been proposed recently. Earliest in the NAR
machine translation ﬁeld, many works use the fertility mod-
ule or length predictor (Gu et al. 2018; Lee, Mansimov, and
Cho 2018; Ghazvininejad et al. 2019; Ma et al. 2019) to
predict the length correspondence (fertility) between source
and target sequences, and then generate the target sequence
depending on the source sequence and predicted fertility.
Shortly afterward, researchers bring NAR decoding manner
into heterogeneous tasks. In the speech ﬁeld, NAR-based
TTS (Ren et al. 2019; Peng et al. 2020; Miao et al. 2020)
synthesize speech from text with high speed and slightly
quality drop; NAR-based ASR (Chen et al. 2019; Higuchi
et al. 2020) recognize speech to corresponding transcription
faster. In the computer vision ﬁeld, Liu et al. (2020) propose
an NAR model for lipreading; Deng et al. (2020) present
NAR image caption model not only improving the decod-
ing efﬁciency but also making the generated captions more
controllable and diverse.

3 Method

3.1 Preliminary Knowledge
The text-to-lip generation aims to generate the sequence
of lip movement video frames L = {l1, l2, ..., lT }, given
source text sequence S = {s1, s2, ..., sm} and a single iden-
tity lip image lI as condition. Generally, there is a consid-
erable discrepancy between the sequence length of L and S
with uncertain mapping relationship. Previous work views
this as a sequence-to-sequence problem, utilizing attention
mechanism and AR decoding manner, where the conditional
probability of L can be formulated as:

P (L|S, lI ) =

T
(cid:89)

τ =0

P (lτ +1|l<τ +1, S, lI ; θ),

(1)

where θ denotes the parameters of the model.

To remedy the error propagation and high latency prob-
lem brought by AR decoding, ParaLip models the target se-
quence in an NAR manner, where the conditional probability
becomes:

P (L|S, lI ) =

T
(cid:89)

τ =1

P (lτ |S, lI ; θ).

(2)

3.2 Model Architecture of ParaLip
The overall model architecture and training losses are shown
in Figure 2a. We explain each component in ParaLip in the
following paragraphs.

Identity Encoder As shown in the right panel of Figure
2a, identity encoder consists of stacked 2D convolutional
layers with batch normalization, which down-samples the
identity image multiple times to extract features. The iden-
tity image is selected randomly from target lip frames, pro-
viding the appearance information of a speaker. It is worth
noting that the identity encoder sends out the ﬁnal encoded
hidden feature together with the intermediate hidden feature
of convolutional layers at every level, which provides the
ﬁne-grained image information.

Text Encoder As shown in Figure 2b, the text encoder
consists of a text embedding layer, stacked feed-forward
Transformer layers (TM) (Ren et al. 2019), a duration pre-
dictor and a length regulator. The TM layer contains self-
attention layer and 1D convolutional layer with layer nor-
malization and residual connection (Vaswani et al. 2017;
Gehring et al. 2017). The duration predictor contains two 1D
convolutional layers with layer normalization and one linear
layer, which takes in the hidden text embedding sequence

Image DecoderDeConv1DeConv2DeConvK…Identity InformationMotion Informationat τtime      Image at τtimeVideoDecoderT×Video framesMotion InformationLinguistic informationN×PositionalEncodingText TokensText EncoderMotion DecoderVideo Decoder…Conv1Conv2ConvKIdentity ImageIdentity Encoder…L1Loss & SSIM Loss & Adversarial LossText EncoderDurationPredictorN×TM BlockPositionalEncodingLayer NormLinear LayerMotion DecoderLinguistic informationText TokensText EmbeddingTM BlockLength ExpansionLayer Normbin| white | …Image DecoderDeConv1DeConv2DeConvK…Identity InformationMotion Informationat τtime      Image at τtimeVideoDecoderT×Video framesMotion InformationLinguistic informationN×PositionalEncodingText TokensText EncoderMotion DecoderVideo Decoder…Conv1Conv2ConvKIdentity ImageIdentity Encoder…L1Loss & SSIM Loss & Adversarial LossText EncoderDurationPredictorN×TM BlockPositionalEncodingLayer NormLinear LayerMotion DecoderLinguistic informationText TokensText EmbeddingTM BlockLength ExpansionLayer Normbin| white | …Image DecoderDeConv1DeConv2DeConvK…Identity InformationMotion Informationat τtime      Image at τtimeVideoDecoderT×Video framesMotion InformationLinguistic informationN×PositionalEncodingText TokensText EncoderMotion DecoderVideo Decoder…Conv1Conv2ConvKIdentity ImageIdentity Encoder…L1Loss & SSIM Loss & Adversarial LossText EncoderDurationPredictorN×TM BlockPositionalEncodingLayer NormLinear LayerMotion DecoderLinguistic informationText TokensText EmbeddingTM BlockLength ExpansionLayer Normbin| white | …Image DecoderDeConv1DeConv2DeConvK…Identity InformationMotion Informationat τtime      Image at τtimeVideoDecoderT×Video framesMotion InformationLinguistic informationN×PositionalEncodingText TokensText EncoderMotion DecoderVideo Decoder…Conv1Conv2ConvKIdentity ImageIdentity Encoder…L1Loss & SSIM Loss & Adversarial LossText EncoderDurationPredictorN×TM BlockPositionalEncodingLayer NormLinear LayerMotion DecoderLinguistic informationText TokensText EmbeddingTM BlockLength ExpansionLayer Normbin| white | …1, d∗

2, ..., d∗

and predicts duration sequence D∗ = {d∗
m},
where d∗
i means how many video frames the i-th text token
corresponding to. The length regulator expands the hidden
text embedding sequence according to ground truth dura-
tion D at training stage or predicted duration D∗ at infer-
ence stage. For example, when given source text and du-
ration sequence are {s1, s2, s3} and {2, 1, 3} respectively,
denoting the hidden text embedding as {h1, h2, h3}, the ex-
panded sequence is {h1, h1, h2, h3, h3, h3}, which carries
the linguistic information corresponding to lip movement
video at frame level. Collectively, text encoder encodes the
source text sequence S to linguistic information sequence
(cid:101)S = {(cid:101)s1, (cid:101)s2, ..., (cid:101)sT ∗ }, where T ∗ = (cid:80)m
i=1 di at training
stage, or T ∗ = (cid:80)m
i=1 d∗
i at inference stage.

the lip movement

Motion Decoder Motion decoder (Figure 2c) aims to
information sequence (cid:101)L =
model
{(cid:101)l1, (cid:101)l2, ..., (cid:101)lT ∗ } from linguistic information sequence (cid:101)S. It
utilizes the positional encoding and self-attention mecha-
nism in stacked TM blocks to enforce the temporal corre-
lation on the hidden sequence. There is a linear layer at the
end of this module convert the hidden states to an appropri-
ate dimension.

Video Decoder The video decoder generates the target lip
movement video L∗ conditioned on the motion information
sequence and identity information. As shown in Figure 2d,
the video decoder consists of multiple parallel image de-
coders with all parameters shared, each of which contains
stacked 2D deconvolutional layers, and there are skip con-
nections at every level between the identity encoder and
each image decoder. The skip connection is implemented
by concatenation. Then two extra 2D convolutional layers
are added at the end of each decoder for spatial coherence.
Finally, the τ -th image decoder takes in lip motion infor-
mation at τ time (cid:101)lτ and generates lip image l∗
τ at τ time in
corresponding shape.

3.3 Training Methods

In this section, we describe the loss function and training
strategy to supervise ParaLip. The reconstruction loss and
duration prediction loss endow the model with the funda-
mental ability to generate lip movement video. To generate
the lip with better perceptual quality and alleviate the “blurry
predictions” (Mathieu, Couprie, and LeCun 2016) problem,
the structural similarity index loss and adversarial learning
are introduced. We also explore the source-target alignment
method when the audio is absent even in the training set,
which will be introduced in Section 6.

Reconstruction Loss Basically, we optimize the whole
network by adding L1 reconstruction loss on generated lip
sequence L∗:

Duration Prediction Loss
In the training stage, we add
L1 loss on predicted duration sequence D∗ at token level2
and sequence level, which supervises the duration predictor
to make the precise ﬁne-grained and coarse-grained predic-
tions. Duration prediction loss Ldur can be written as:

Ldur =

m
(cid:88)

i=1

(cid:107)di − d∗

i (cid:107)1 + (cid:107)

m
(cid:88)

i=1

di −

m
(cid:88)

i=1

d∗
i (cid:107)1.

(4)

Structural Similarity Index Loss Structural Similarity
Index (SSIM) (Wang 2004) is adopted to measure the per-
ceptual image quality, which takes luminance, contrast and
structure into account, and is close to the perception of hu-
man beings. The SSIM value for two pixels at position (i, j)
in τ -th images l∗
τ and lτ can be formulated as:

SSIMi,j,τ =

τ

2µl∗
µ2
l∗
τ

µlτ + C1
+ C1

+ µ2
lτ

·

2σl∗

τ lτ + C2
+ σ2
lτ

+ C2

σ2
l∗
τ

,

where µl∗
τ and µlτ denotes the mean for regions in image
l∗
τ and lτ within a 2D-window surrounding (i, j). Similar,
σl∗
τ and σlτ are standard deviation; σl∗
τ lτ is the covariance;
C1 and C2 are constant values. To improve the perceptual
quality of the generated lip frames, we leverage SSIM loss in
ParaLip. Assuming the size of each lip frame to be (A × B),
the SSIM loss between generated L∗ and ground truth L
becomes:

Lssim =

1
T · A · B

T
(cid:88)

A
(cid:88)

B
(cid:88)

τ =1

i

j

(1 − SSIMi,j,τ )).

(5)

Adversarial Learning Through experiments, it can be
found that only using above losses is insufﬁcient to gener-
ate distinct lip images with more realistic texture and local
details (e.g.wrinkles, beard and teeth). Thus, we adopt ad-
versarial learning to mitigate this problem and train a quality
discriminator Disc along with ParaLip. The Disc contains
stacked 2D convolutional layers with LeakyReLU activation
which down-samples each image to 1 × 1 × H (H is hidden
size), and a 1 × 1 convolutional layer to project the hidden
states to a value of probability for judging real or fake. We
use the loss function in LSGAN (Mao et al. 2017) to train
ParaLip and Disc:

adv = Ex∼l∗ (Disc(x) − 1)2,
LG

(6)

adv = Ex∼l(Disc(x) − 1)2 + Ex∼l∗ Disc(x)2,
LD
(7)
where l∗ means lip images generated by ParaLip and l means
ground truth lip images.

To summarize, we optimize the Disc by minimizing
Equation (7), and optimize the ParaLip by minimizing
Ltotal:
Ltotal = λ1 · Lrec + λ2 · Ldur + λ3 · Lssim + λ4 · LG

adv, (8)

where the λ1, λ2, λ3 and λ4 are hyperparameters to trade off
the four losses.

Lrec =

T
(cid:88)

τ =1

(cid:107)lτ − l∗

τ (cid:107)1.

(3)

2Character level for GRID and phoneme level for TCD-TIMIT

following previous works.

4 Experimental Settings

4.1 Datasets
GRID The GRID dataset (Cooke et al. 2006) consists
of 33 video-available speakers, and each speaker utters
1,000 phrases. The phrases are in a 6-categories structure
following ﬁxed simple grammar: command4 + color4 +
preposition4 + letter25 + digit10 + adverb4 where the
number denotes how many choices of each category. Thus,
the total vocabulary size is 51, composing 64,000 possi-
ble phrases. All the videos last 3 seconds with frame rate
25 fps, which form a total duration of 27.5 hours. It is a
typical talking face dataset and there are a considerable of
lip-related works (Assael et al. 2016; Chung et al. 2017;
Afouras, Chung, and Zisserman 2018; Chen et al. 2018; Zhu
et al. 2020; Lin et al. 2021) conducting experiments on it.
Following previous works, we select 255 random samples
from each speaker to form the test set.

TCD-TIMIT The TCD-TIMIT dataset (Harte and Gillen
2015) is closer to real cases and more challenging than
GRID dataset, since 1) the vocabulary is not limited; 2) the
sequence length of videos is not ﬁxed and is longer than that
in GRID. We use the ‘volunteers’ subset of TCD-TIMIT fol-
lowing previous works, which consists of 59 speakers ut-
tering about 98 sentences individually. The frame rate is
29.97 fps and each video lasts 2.5∼8.1 seconds. The to-
tal duration is about 7.5 hours. We set 30% of data from
each speaker aside for testing following the recommended
speaker-dependent train-test splits (Harte and Gillen 2015).

4.2 Data Pre-processing
As for the video pre-processing, we utilize Dlib (King 2009)
to detect 68 facial landmarks (including 20 mouth land-
marks), and extract the face images from video frames. We
resize the face images to 256 × 256, and further crop each
face to a ﬁxed 160 × 80 size containing the lip-centered re-
gion. As for the text pre-processing, we encode the text se-
quence at the character level for GRID dataset and phoneme
level for TCD-TIMIT dataset. And for ground truth dura-
tion extraction, we ﬁrst extract the speech audio from video
ﬁles, and then utilize “Penn Phonetics Lab Forced Aligner”
(P2FA) (Yuan and Liberman 2008) to get speech-to-text
alignments, from which we obtain the duration of each text
token for training our duration predictor in ParaLip.

5 Results and Analysis
In this section, we present extensive experimental results to
evaluate the performance of ParaLip in terms of lip move-
ments quality and inference speedup. And then, we conduct
ablation experiments to verify the signiﬁcance of all pro-
posed methods in ParaLip.

5.1 Quality Comparison
We compare our model with 1) DualLip (Chen et al. 2020),
which is the state-of-the-art (SOTA) autoregressive text-
to-lip model based on RNN and location-sensitive atten-
tion (Shen et al. 2018). And 2) TransformerT2L, an au-
toregressive baseline model based on Transformer (Vaswani

et al. 2017) implemented by us, which uses the same model
settings with ParaLip3. The quantitative results on GRID
and TCD-TIMIT are listed in Table 1 and Table 2 respec-
tively4. Note that we do not add adversarial learning on any
model in Table 1 or Table 2, since there is no adversarial
learning in DualLip (Chen et al. 2020).

Methods

PSNR ↑

SSIM ↑

LMD ↓

AR Benchmarks

DualLip
TransformerT2L

29.13†
26.85

0.872†
0.829

1.809 †
1.980

Our Model

ParaLip

28.74

0.875

1.675

Table 1: Comparison with Autoregressive Benchmarks on
GRID dataset. † denotes our reproduction under the case w/o
GT duration at inference.

Methods

PSNR ↑

SSIM ↑

LMD ↓

AR Benchmarks

DualLip
TransformerT2L

27.38†
26.89

0.809†
0.794

2.351†
2.763

Our Model

ParaLip

27.64

0.816

2.084

Table 2: Comparison with Autoregressive Benchmarks on
TCD-TIMIT dataset. † denotes our reproduction under the
case w/o GT duration at inference.

Quantitative Comparison We can see that: 1) On GRID
dataset (Table 1), ParaLip outperforms DualLip on LMD
metric, and keeps the same performance in terms of PSNR
and SSIM metrics. However, on TCD-TIMIT dataset (Ta-
ble 2), ParaLip achieves a overall performance surpassing
DualLip by a notable margin, since autoregressive models
perform badly on the long-sequence dataset due to accu-
mulated prediction error; 2) ParaLip shows absolute supe-
riority over AR baseline TransformerT2L in terms of three
quantitative metrics on both datasets; 3) although DualLip
outperforms AR baseline by incorporating the technique of
location-sensitive attention, which could alleviate the error
propagation, it is still vulnerable on long-sequence dataset.

Qualitative Comparison We further visualize the qual-
itative comparison between DualLip, TransformerT2L and
ParaLip in Figure 3. It can be seen that the quality of lip
frames generated by DualLip and TransformerT2L become

3Most modules and the total number of model parameters in

ParaLip and TransformerT2L are similar.

4Note that the reported results are all under the case where the
ground truth (GT) duration is not provided at inference (denoted as
w/o duration in (Chen et al. 2020)) , since there is no GT duration
available in the real case.

Figure 3: The qualitative comparison among AR SOTA (DualLip), AR baseline (TransformerT2L) and our NAR method (Par-
aLip). We visualize two cases from GRID dataset and TCD-TIMIT dataset to illustrate the error propagation problem existing
in AR generation and verify the robustness of ParaLip. In the ﬁrst case, the lip sequence generated from AR baseline predicts
a wrong lip image (the 6-th frame with red box), and as a result, the subsequent lip images conditioned on that image becomes
out of synchronization with linguistic information and ends in chaos; DualLip alleviates the error propagation to some degree.
In the second case, both AR models perform poorly on the long-sequence dataset and generate the frames that look speechless
as the time goes further.

increasingly worse as the time goes further. Concretely, The
lip image becomes fuzzy and out of synchronization with
linguistic contents. We attribute this phenomenon to the rea-
son that: error propagation problem is serious in AR T2L
since the wrong prediction could take place at more di-
mensions (every pixel with three channels in generated im-
age) and there is information loss during the down-sampling
when sending the last generated lip frame to predict cur-
rent one. What’s worse, on TCD-TIMIT, a long-sequence
dataset, DualLip and TransformerT2L often generate totally
unsatisfying results that look like speechless video. By con-
trast, the lip frames generated by NAR model ParaLip main-
tain high ﬁdelity to ground truth all the while, which demon-
strates the effectiveness and robustness of NAR decoding.

5.2 Speed Comparison

In this section, we evaluate and compare the average infer-
ence latency of DualLip, TransformerT2L and ParaLip on
both datasets. Furthermore, we study the relationship be-
tween inference latency and the target video length.

Comparison of Average Inference Latency The average
inference latency is the average time consumed to generate
one video sample on the test set, which is measured in sec-
onds. Table 3 exhibits the inference latency of all systems.
It can be found that, 1) compared with DualLip, ParaLip
speeds up the inference by 13.09× and 19.12× on aver-
age on two datasets; 2) TransformerT2L has the same struc-
ture with ParaLip, but runs about 50% slower than Dual-
Lip, which indicates that it is NAR decoding manner in Par-
aLip speedups the inference, rather than the modiﬁcation of
model structure; 3) In regard to AR models, the time con-
sumption of a single sentence increases to 0.5-1.5 seconds
even on GPU, which is unacceptable for real-world appli-
cation. By contrast, ParaLip addresses the inference latency
problem satisfactorily.

Datasets

Methods

Latency (s)

Speedup

GRID

TIMIT

DualLip
TransformerT2L
ParaLip

DualLip
TransformerT2L
ParaLip

0.299
0.689
0.022

0.650
1.278
0.034

1.00 ×
0.43 ×
13.09 ×

1.00 ×
0.51 ×
19.12 ×

Table 3: The comparison of inference latency on GRID and
TCD-TIMIT dataset. The computations are conducted on a
server with 1 NVIDIA 2080Ti GPU.

Figure 4: Relationship between inference latency (seconds)
and predicted video length for DualLip, TransformerT2L
and ParaLip.

silbinblueatyninenowsilDualLipTransformerT2LParaLipsildhserpshzsilDualLipTransformerT2LParaLipuwwowaetahihwGround TruthGround Truth80100120140160180200220Predicted Lip Video Length0.080.501.001.502.00Inference Time(s)DualLipTransformerT2LParaLipRelationship between Inference Latency and Video
Length In this section, we study the speedup as the se-
quence length increases. The experiment is conducted on
TCD-TIMIT, since its videos are not in a ﬁxed length. From
Figure 4, it can be seen that 1) ParaLip model speeds up
the inference obviously due to high parallelization com-
pared with AR models; 2) ParaLip is insensitive to se-
quence length and almost holds a constant inference la-
tency, but by contrast, the inference latency of DualLip and
TransformerT2L increase linearly as the sequence length in-
creases. As a result, the speedup of ParaLip relative to Du-
alLip or TransformerT2L also increases linearly as the se-
quence length increases.

5.3 Ablation Study

Model

PSNR ↑

SSIM ↑

LMD ↓

FID ↓

Base model
+SSIM
+ADV
+SSIM+ADV

30.24
30.51
25.70
28.36

0.896
0.906
0.736
0.873

0.998
0.978
2.460
1.077

56.36
55.05
65.88
39.74

Table 4: The ablation studies on GRID dataset. Base model
is trained only with L1 loss; “+SSIM” means adding struc-
tural similarity index loss and “+ADV” means adding adver-
sarial learning to the base model. FID means Fr´echet Incep-
tion Distance metric. To focus on the frames quality, we pro-
vide the GT duration for eliminating the interference caused
by the discrepancy of predicted length.

We conduct ablation experiments on GRID dataset to an-
alyze the effectiveness of the proposed methods in our work.
All the results are shown in Table 4. Experiments show that:
• Adding only SSIM loss obtains the optimal score on

PSNR/SSIM/LMD (“+SSIM”);

• Adding only adversarial training causes performance
drop on PSNR/SSIM/LMD, which is consistent with pre-
vious works (Song et al. 2019) (“+ADV”);

• Adding SSIM to model with adversarial training can
greatly alleviate the detriment on PSNR/SSIM/LMD
brought by adversarial
training; make the GAN-
based model more stable; obtain the best FID score,
which means the generated lips look more realistic.
(“+SSIM+ADV”).

Previous works (Song et al. 2019) claim that 1) PSNR and
SSIM cannot well reﬂect some visual quality; 2) adversar-
ial learning encourages the generated face to pronounce in
diverse ways, leading to diverse lip movements and LMD
decrease. Although “+SSIM+ADV” causes marginally
PSNR/SSIM/LMD scores losses, “+SSIM+ADV” obtain the
best FID score and tends to generate distinct lip images with
more realistic texture and local details (e.g. wrinkles, beard
and teeth). The qualitative results are shown in Figure 5.

6 Further Discussions
In the foregoing sections, we train the duration predictor
using the “GT” duration extracted by P2FA, but it is not

Figure 5: The qualitative evaluation for adversarial learning.
“+ADV” tends to generate more realistic lip images.

applicable to the case where the audio is absent even in
the training set. Thus, to obtain the “GT” duration in this
case, we tried a lipreading model with monotonic alignment
searching (MAS) (Tillmann et al. 1997; Kim et al. 2020)
to ﬁnd the alignment between text and lip frames. Specif-
ically, we 1) ﬁrst trained a lipreading model by CTC loss
on the training set; 2) traveled the training set and for each
(L, S) pair, we extracted Octc ∈ Rm (the CTC outputs cor-
responding to the label tokens) from Octc ∈ RV (Octc is
the original CTC outputs; V is the vocabulary size); 3) ap-
plied softmax on the extracted Octc to obtain the probability
matrix P (A(si, lj)) = e
m eOctc,lj ; 4) conducted MAS by
(cid:80)
dynamic programming to ﬁnd the best alignment solution.
This method achieves similar results with P2FA on GRID
dataset, but could cause deterioration on TIMIT dataset:
PSNR:27.09, SSIM:0.816, LMD:2.313.

ctc,lj

Oi

Theoretically, obtaining the alignment between lip frames
and its transcript directly from themselves has more poten-
tial than obtaining this alignment indirectly from audio sam-
ple and its transcripts, since there are many cases when the
mouth is moving but the sound hasn’t come out yet (e.g. the
ﬁrst milliseconds of a speech sentence). This part of video
frames should be aligned to some words, but its correspond-
ing audio piece is silent, which will not be aligned to any
word, causing contradictions. We think it is valuable to try
more and better methods in this direction.

7 Conclusion
In this work, we point out and analyze the unacceptable in-
ference latency and intractable error propagation existing in
AR T2L generation, and propose a parallel decoding model
ParaLip to circumvent these problems. Extensive experi-
ments show that ParaLip generates lip movements with com-
petitive quality compared with the state-of-the-art AR T2L
model, exceeds the baseline AR model TransformerT2L by a
notable margin and exhibits distinct superiority in inference
speed, which provides the possibility to bring T2L genera-
tion from laboratory to industrial applications.

+ SSIM+ SSIM+ ADV+ SSIM+ SSIM+ ADV+ SSIM+ SSIM+ ADV8 Acknowledgments
This work was supported in part by the National Key
R&D Program of China under Grant No.2020YFC0832505,
No.62072397, Zhejiang Natural Science Foundation under
Grant LR19F020006.

References
Abdelaziz, A. H.; Kumar, A. P.; Seivwright, C.; Fanelli, G.;
Binder, J.; Stylianou, Y.; and Kajarekar, S. 2020. Audio-
visual Speech Synthesis using Tacotron2. arXiv preprint
arXiv:2008.00620.
Afouras, T.; Chung, J. S.; and Zisserman, A. 2018. Deep Lip
Reading: A Comparison of Models and an Online Applica-
tion. In Proc. Interspeech 2018, 3514–3518.
Assael, Y. M.; Shillingford, B.; Whiteson, S.; and De Fre-
itas, N. 2016. Lipnet: End-to-end sentence-level lipreading.
arXiv preprint arXiv:1611.01599.
Bengio, S.; Vinyals, O.; Jaitly, N.; and Shazeer, N. 2015.
Scheduled sampling for sequence prediction with recurrent
In Advances in Neural Information Pro-
neural networks.
cessing Systems, 1171–1179.
Chen, L.; Li, Z.; K Maddox, R.; Duan, Z.; and Xu, C. 2018.
Lip movements generation at a glance. In Proceedings of the
European Conference on Computer Vision (ECCV), 520–
535.
Chen, N.; Watanabe, S.; Villalba, J.; and Dehak, N. 2019.
Non-Autoregressive Transformer Automatic Speech Recog-
nition. arXiv preprint arXiv:1911.04908.
Chen, W.; Tan, X.; Xia, Y.; Qin, T.; Wang, Y.; and Liu, T.-Y.
2020. DualLip: A System for Joint Lip Reading and Gener-
ation. In Proceedings of the 28th ACM International Con-
ference on Multimedia, MM ’20, 1985–1993. New York,
NY, USA: Association for Computing Machinery.
ISBN
9781450379885.
Chung, J.; Jamaludin, A.; and Zisserman, A. 2017. You said
that? British Machine Vision Conference 2017, BMVC 2017.
Chung, J. S.; Senior, A.; Vinyals, O.; and Zisserman, A.
In 2017 IEEE
2017. Lip reading sentences in the wild.
Conference on Computer Vision and Pattern Recognition
(CVPR), 3444–3453. IEEE.
Cooke, M.; Barker, J.; Cunningham, S.; and Shao, X. 2006.
An audio-visual corpus for speech perception and automatic
Journal of the Acoustical Society of
speech recognition.
America, 120(5): 2421.
Deng, C.; Ding, N.; Tan, M.; and Wu, Q. 2020. Length-
Controllable Image Captioning. ECCV.
Fan, B.; Wang, L.; Soong, F. K.; and Xie, L. 2015. Photo-
In 2015
real talking head with deep bidirectional LSTM.
IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP), 4884–4888. IEEE.
Fried, O.; Tewari, A.; Zollh¨ofer, M.; Finkelstein, A.; Shecht-
man, E.; Goldman, D. B.; Genova, K.; Jin, Z.; Theobalt, C.;
and Agrawala, M. 2019. Text-based editing of talking-head
video. ACM Transactions on Graphics (TOG), 38(4): 1–14.

Gehring, J.; Auli, M.; Grangier, D.; Yarats, D.; and Dauphin,
Y. N. 2017. Convolutional sequence to sequence learning.
In Proceedings of the 34th International Conference on Ma-
chine Learning-Volume 70, 1243–1252.
Ghazvininejad, M.; Levy, O.; Liu, Y.; and Zettlemoyer,
L. 2019. Mask-Predict: Parallel Decoding of Conditional
Masked Language Models. In Proceedings of the 2019 Con-
ference on Empirical Methods in Natural Language Pro-
cessing and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP), 6114–6123.
Gu, J.; Bradbury, J.; Xiong, C.; Li, V. O.; and Socher, R.
2018. Non-Autoregressive Neural Machine Translation. In
International Conference on Learning Representations.
Harte, N.; and Gillen, E. 2015. TCD-TIMIT: An Audio-
IEEE Transactions
Visual Corpus of Continuous Speech.
on Multimedia, 17(5): 603–615.
Higuchi, Y.; Watanabe, S.; Chen, N.; Ogawa, T.; and
Kobayashi, T. 2020. Mask CTC: Non-Autoregressive End-
to-End ASR with CTC and Mask Predict. INTERSPEECH.
Kim, J.; Kim, S.; Kong, J.; and Yoon, S. 2020. Glow-TTS: A
Generative Flow for Text-to-Speech via Monotonic Align-
ment Search. Advances in Neural Information Processing
Systems, 33.
King, D. E. 2009. Dlib-ml: A Machine Learning Toolkit.
JMLR.org.
KR, P.; Mukhopadhyay, R.; Philip, J.; Jha, A.; Namboodiri,
V.; and Jawahar, C. 2019. Towards Automatic Face-to-Face
Translation. In Proceedings of the 27th ACM International
Conference on Multimedia, 1428–1436.
Kumar, R.; Sotelo, J.; Kumar, K.; de Br´ebisson, A.; and Ben-
gio, Y. 2017. Obamanet: Photo-realistic lip-sync from text.
arXiv preprint arXiv:1801.01442.
Lee, J.; Mansimov, E.; and Cho, K. 2018. Deterministic
Non-Autoregressive Neural Sequence Modeling by Iterative
Reﬁnement. In EMNLP, 1173–1182.
Lin, Z.; Zhao, Z.; Li, H.; Liu, J.; Zhang, M.; Zeng, X.; and
He, X. 2021. SimulLR: Simultaneous Lip Reading Trans-
ducer with Attention-Guided Adaptive Memory, 1359–1367.
New York, NY, USA: Association for Computing Machin-
ery. ISBN 9781450386517.
Liu, J.; Ren, Y.; Zhao, Z.; Zhang, C.; Huai, B.; and Yuan, J.
2020. FastLR: Non-Autoregressive Lipreading Model with
Integrate-and-Fire. In Proceedings of the 28th ACM Inter-
national Conference on Multimedia, 4328–4336.
Ma, X.; Zhou, C.; Li, X.; Neubig, G.; and Hovy, E. 2019.
FlowSeq: Non-Autoregressive Conditional Sequence Gen-
eration with Generative Flow. In EMNLP-IJCNLP, 4273–
4283.
Mao, X.; Li, Q.; Xie, H.; Lau, R. Y.; Wang, Z.; and
Paul Smolley, S. 2017. Least squares generative adversarial
networks. In Proceedings of the IEEE international confer-
ence on computer vision, 2794–2802.
Mathieu, M.; Couprie, C.; and LeCun, Y. 2016. Deep multi-
scale video prediction beyond mean square error. In 4th In-
ternational Conference on Learning Representations, ICLR
2016.

Miao, C.; Liang, S.; Chen, M.; Ma, J.; Wang, S.; and Xiao, J.
2020. Flow-TTS: A Non-Autoregressive Network for Text
to Speech Based on Flow. In ICASSP 2020-2020 IEEE Inter-
national Conference on Acoustics, Speech and Signal Pro-
cessing (ICASSP), 7209–7213. IEEE.
Peng, K.; Ping, W.; Song, Z.; and Zhao, K. 2020. Non-
Autoregressive Neural Text-to-Speech. ICML.
Prajwal, K.; Mukhopadhyay, R.; Namboodiri, V. P.; and
Jawahar, C. 2020. A Lip Sync Expert Is All You Need for
Speech to Lip Generation In The Wild. In Proceedings of the
28th ACM International Conference on Multimedia, 484–
492.
Ren, Y.; Ruan, Y.; Tan, X.; Qin, T.; Zhao, S.; Zhao, Z.; and
Liu, T.-Y. 2019. Fastspeech: Fast, robust and controllable
text to speech. In Advances in Neural Information Process-
ing Systems, 3165–3174.
Shen, J.; Pang, R.; Weiss, R. J.; Schuster, M.; Jaitly, N.;
Yang, Z.; Chen, Z.; Zhang, Y.; Wang, Y.; Skerrv-Ryan, R.;
et al. 2018. Natural tts synthesis by conditioning wavenet
In 2018 IEEE Interna-
on mel spectrogram predictions.
tional Conference on Acoustics, Speech and Signal Process-
ing (ICASSP), 4779–4783. IEEE.
Song, Y.; Zhu, J.; Li, D.; Wang, A.; and Qi, H. 2019. Talk-
ing Face Generation by Conditional Recurrent Adversarial
In Proceedings of the Twenty-Eighth Interna-
Network.
tional Joint Conference on Artiﬁcial Intelligence, IJCAI-19,
919–925. International Joint Conferences on Artiﬁcial Intel-
ligence Organization.
Sutskever, I.; Vinyals, O.; and Le, Q. V. 2014. Sequence
to sequence learning with neural networks. In Advances in
neural information processing systems, 3104–3112.
Suwajanakorn, S.; Seitz, S. M.; and Kemelmacher-
Shlizerman, I. 2017. Synthesizing obama: learning lip sync
from audio. ACM Transactions on Graphics (TOG), 36(4):
1–13.
Tillmann, C.; Vogel, S.; Ney, H.; and Zubiaga, A. 1997. A
DP-based search using monotone alignments in statistical
translation. In 35th Annual Meeting of the Association for
Computational Linguistics and 8th Conference of the Euro-
pean Chapter of the Association for Computational Linguis-
tics, 289–296.
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,
L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-
tention is all you need. In Advances in neural information
processing systems, 5998–6008.
Vougioukas, K.; Petridis, S.; and Pantic, M. 2019. End-to-
End Speech-Driven Realistic Facial Animation with Tempo-
ral GANs. In CVPR Workshops, 37–40.
Wang, L.; Han, W.; Soong, F. K.; and Huo, Q. 2011. Text
In Twelfth Annual
driven 3D photo-realistic talking head.
Conference of the International Speech Communication As-
sociation.
Wang, Z. 2004. Image Quality Assessment : From Error Vis-
ibility to Structural Similarity. IEEE Transactions on Image
Processing.

Wu, L.; Tan, X.; He, D.; Tian, F.; Qin, T.; Lai, J.; and Liu,
T.-Y. 2018. Beyond Error Propagation in Neural Machine
In
Translation: Characteristics of Language Also Matter.
Proceedings of the 2018 Conference on Empirical Methods
in Natural Language Processing, 3602–3611. Brussels, Bel-
gium: Association for Computational Linguistics.
Yu, C.; Lu, H.; Hu, N.; Yu, M.; Weng, C.; Xu, K.; Liu, P.;
Tuo, D.; Kang, S.; Lei, G.; et al. 2019. Durian: Duration
informed attention network for multimodal synthesis. arXiv
preprint arXiv:1909.01700.
Yu, L.; Yu, J.; and Ling, Q. 2019. Mining audio, text and vi-
sual information for talking face generation. In 2019 IEEE
International Conference on Data Mining (ICDM), 787–
795. IEEE.
Yuan, J.; and Liberman, M. 2008. Speaker identiﬁcation on
the SCOTUS corpus. Journal of the Acoustical Society of
America, 123(5): 3878.
Zheng, R.; Zhu, Z.; Song, B.; and Ji, C. 2020. Photorealis-
tic Lip Sync with Adversarial Temporal Convolutional Net-
works. arXiv preprint arXiv:2002.08700.
Zhou, H.; Liu, Y.; Liu, Z.; Luo, P.; and Wang, X. 2019.
Talking face generation by adversarially disentangled audio-
visual representation. In Proceedings of the AAAI Confer-
ence on Artiﬁcial Intelligence, volume 33, 9299–9306.
Zhu, H.; Huang, H.; Li, Y.; Zheng, A.; and He, R. 2020.
Arbitrary Talking Face Generation via Attentional Audio-
Visual Coherence Learning. In Bessiere, C., ed., Proceed-
ings of the Twenty-Ninth International Joint Conference on
Artiﬁcial Intelligence, IJCAI-20, 2362–2368. International
Joint Conferences on Artiﬁcial Intelligence Organization.
Main track.

A Implementation Detail
In our framework, for the TM blocks, we adopt Transformer
as the basic structure. Thereinto, the model hidden size
dhidden, number of stacked layers nT M , number of atten-
tion heads nhead and dropout rate are set to 256, 4, 2, and
0.2 respectively. For the convolutional layers, the number of
stacked layers and the kernel size are set to 4 and 5 by de-
fault. The window size for calculating SSIM is set to 11. As
for the identity lip image, we randomly select an identity lip
from target lip frames at the training stage, and use the ﬁrst
lip frame at inference stage.

To train ParaLip, the weights of loss functions λ1, λ2, λ3
and λ4 are set to 1, 1, 1 and 5. The batch size is set to 4 and
the total number of iterations is set to 110, 000. The learning
rate is set as 0.001 and 0.0001 for generator and discrimi-
nator respectively using Adam optimizer. The training is run
on one RTX 2080ti GPU and our implementation is based on
Pytorch Lightning. We will release our code once the paper
is published.

B Model Size
The model footprints of main systems for comparison in our
paper are shown in Table 5. It can be seen that ParaLip has
the similar learnable parameters of generator as other state-
of-the-art models.

Model

DualLip

TransformerT2L

ParaLip

Param(M)

41.483

37.780

35.224 (Generator)
14.114 (Discriminator)

Table 5: The model footprints. Param means the learnable
parameters.

C Model Details of Identity Encoder
The details of the identity encoder (in both ParaLip and
TransformerT2L) are given in Table 6. The ﬁlter is in the
format of ([h × w, channel] / stride, pad).

Layer Type

Filters

Output dimensions

Layer Type

Filters

Output dimensions

Conv 2D

[5 × 5, 16] /2, 2

T × 40 × 80 × 16

Conv 2D

[7 × 7, 32] /1, 3

T × 80 × 160 × 32

Conv 2D

[5 × 5, 32] /2, 2

T × 20 × 40 × 32

Conv 2D

[5 × 5, 64] /[1, 2], 2

T × 80 × 80 × 64

Conv 2D

[5 × 5, 64] /2, 2

T × 10 × 20 × 64

Conv 2D

[5 × 5, 64] /1, 2

T × 80 × 80 × 64

Conv 2D

[5 × 5, 128] /2, 2

T × 5 × 10 × 128

Conv 2D

[5 × 5, 128] /2, 2

T × 40 × 40 × 128

Conv 2D

[5 × 5, 128] /1, 2

T × 40 × 40 × 128

Conv 2D

[5 × 5, 256] /2, 2

T × 20 × 20 × 256

Conv 2D

[5 × 5, 256] /1, 2

T × 20 × 20 × 256

Conv 2D

[3 × 3, 512] /2, 1

T × 10 × 10 × 512

Conv 2D

[3 × 3, 512] /1, 1

T × 10 × 10 × 512

Conv 2D

[3 × 3, 512] /2, 1

T × 5 × 5 × 512

Conv 2D

[3 × 3, 512] /1, 0

T × 3 × 3 × 512

Conv 2D

[3 × 3, 512] /1, 0

T × 1 × 1 × 512

Conv 2D

[1 × 1, 512] /1, 0

T × 1 × 1 × 512

Conv 2D

[1 × 1, 1] /1, 0

T × 1 × 1 × 1

Table 7: Model details for Discriminator. There is a
LeakyReLU activation function after each Conv 2D layer.

Linear

-

T × 256

Table 6: Model details for Identity Encoder. There is a
BatchNorm layer and a ReLU activation function after each
Conv 2D layer.

D Model Details of Discriminator in ParaLip
The details of the discriminator in ParaLip for adversarial
learning are given in Table 7. The ﬁlter is in the format of
([h × w, channel] / stride, pad).

E Evaluation Metrics
Following previous T2L generation work, we adopt PSNR,
SSIM (Wang 2004) and Landmark Distance (LMD) (Chen
et al. 2018; Song et al. 2019) for quantitative evaluation of
lip quality. PSNR and SSIM are the classical reconstruction
metrics to evaluate the images quality in generated video
(Mathieu, Couprie, and LeCun 2016). LMD measures lip
movement accuracy from pixel-level (Song et al. 2019). Fol-
lowing Chen et al. (2020), we calculate the euclidean dis-
tance between corresponding lip landmarks of the generated
lip movements and ground truth, instead of all the facial
landmarks.

