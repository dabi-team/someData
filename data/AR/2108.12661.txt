SceneAR: Scene-based Micro Narratives for Sharing and Remixing in
Augmented Reality

Mengyu Chen*
University of California Santa Barbara

Andr ´es Monroy-Hern ´andez†
Snap Inc.

Misha Sra‡
University of California Santa
Barbara

1
2
0
2

g
u
A
8
2

]

C
H
.
s
c
[

1
v
1
6
6
2
1
.
8
0
1
2
:
v
i
X
r
a

ABSTRACT

Short-form digital storytelling has become a popular medium for
millions of people to express themselves. Traditionally, this medium
uses primarily 2D media such as text (e.g., memes), images (e.g.,
Instagram), GIFs (e.g., Giphy), and videos (e.g., TikTok, Snapchat).
To expand the modalities from 2D to 3D media, we present SceneAR,
a smartphone application for creating sequential scene-based micro
narratives in augmented reality (AR). What sets SceneAR apart
from prior work is its ability to share the scene-based stories as
AR content. No longer limited to sharing images or videos, users
can now experience narratives in their own physical environments.
Additionally, SceneAR affords users the ability to remix AR content,
empowering them to collectively build on others’ creations. We
asked 18 people to use SceneAR in a three-day study, and based
on user interviews, analyses of screen recordings, and the stories
they created, we extracted three themes. From these themes and the
study overall, we derived six strategies for designers interested in
supporting short-form AR narratives.

Index Terms: Human-centered computing—Visualization—Visu-
alization techniques—Treemaps; Human-centered computing—
Visualization—Visualization design and evaluation methods

1 INTRODUCTION

Examples of early forms of storytelling with sequences of pictures
can be found in Egyptian hieroglyphs, limestone carvings of Buddha,
Greek friezes, Japanese scrolls, and illustrated Christian manuscripts,
to name a few [10]. Over time, using picture panels and text for
storytelling evolved into comics. More recently, with digital tools,
everyday people can tell their stories through not only pictures and
text but also sound and video (e.g., micro stories told with memes,
GIFs, Snapchat lenses, TikTok videos etc.). SceneAR adds to this
list of storytelling media by enabling the creation and consumption
of micro narratives using smartphone-based augmented reality (AR).
Unlike popular 2D media, creating micro narratives with AR and
viewing them in the physical environment is an under-explored area
and presents a rich set of research opportunities as well as unique
challenges.

Although AR technologies have become more broadly available,
tools for novices to quickly and easily create shareable narratives in
AR are fairly limited. Mainstream development tools for creating
AR experiences, such as Unity and A-Frame, often require extensive
technical skills and programming knowledge [34]. The complexity
of tackling AR-speciﬁc issues, such as surface detection, object
registration, and tracking, can hold creators back despite help from
ARCore [2] and ARKit [3] SDKs. Integrated solutions like Spark
AR Studio by Facebook [44] or Lens Studio by Snap Inc. [31] are
not as complex as Unity, but they are limited to desktop computers,

*e-mail: mengyuchen@ucsb.edu
†e-mail: amh@snap.com
‡e-mail: sra@cs.ucsb.edu

require the use of additional 3D software tools (e.g., Maya and
Blender) for creating 3D models, and expect the user to be familiar
with programming. On the other hand, apps like Apple’s Reality
Composer [38] and Adobe Aero [1] have simpliﬁed the AR creation
process for novices with the added ability to share an AR scene.
But even these apps do not support creating or stitching together
sequences of AR scenes nor do they allow remixing of AR content.
In this work, we present SceneAR, a smartphone-based appli-
cation that allows people to easily and quickly create and share
micro narratives in AR. The stories are composed of sequences of
AR scenes that contain 3D objects and text to form something like
an AR comic strip. Although most mobile AR apps compress 3D
space into 2D for sharing (i.e., turning an AR scene into an image
or video), SceneAR enables viewing of the AR scene sequences by
placing them in any physical environment (Figure 1). Users can
freely explore the 3D models (characters, objects) in the scenes from
any angle or distance by moving around and can interact with them
using select, rotate, move, and zoom options. Furthermore, unlike
existing AR apps, users can also modify and remix [39] AR scenes or
entire narratives and share the updated versions. Remixing is deﬁned
as the “reworking and combination of existing creative artifacts, usu-
ally in the form of music, video, and other interactive media” [12].
Table 1 presents a summary of how SceneAR compares with exist-
ing AR apps and prior research that supports some form of story
creation and consumption. As a smartphone-based AR application,
SceneAR not only enables “immersive authoring” [30], namely, the
creation process that occurs in an immersive AR environment, but
also in situ authoring [47], namely, creation that happens in the same
app used for consuming the content. We envision SceneAR could
be used in areas such as storyboarding, stage design, social media,
education, parent-children interactions, and creative arts. The main
contributions of this work are:

• A smartphone-based app that enables non-expert users to easily
create, share, consume, and remix scene-based micro narratives
in AR.

• A new Micro AR packaging format that integrates AR scene

layout and content into a single shareable form.

• Three themes derived from analysis of three-day user experi-
ence of SceneAR on creativity, spatial interaction, and remix-
ing.

• Six design strategies for designers and practitioners exploring

AR for new forms of narratives.

2 RELATED WORK

SceneAR is inspired by prior work in short-form digital storytelling
(e.g., memes, GIFs, TikTok videos), AR storytelling, and mobile
AR authoring applications.

2.1 Digital Storytelling

Digital storytelling is a combination of narrative and technology
used in many areas like entertainment, education, healthcare, and
communication [40]. It emerged with the advent of accessible hard-
ware and software, making it easy and fast for anyone to create

 
 
 
 
 
 
Creating Platform

Consuming Platform

In Situ
Authoring

AR Sharing
Format

Programming
Required

AR Remixing
by Others

Community
Repository

Lens Studio + Snapchat
SparkAR + Instagram

computer
computer

Apple Reality Composer

smartphone

Adobe Aero
Wonderscope
MagicBook (2001)
Magic Story Cube (2004)
ComposAR-Mobile (2009)
Educ-AR (2011)
MARAT + ARCO (2013)

PintAR (2019)

StoryMakAR (2020)

yes

no
no

Snapchat app
Facebook AR Player
Apple Reality
Composer app
yes
Adobe Aero app
Wonderscope app
N/A
HMD + physical book N/A
N/A
mobile SceneAR app
no
smartphone AR player
no
desktop Educ-AR
yes

smartphone
pre-authored
pre-authored
pre-authored
smartphone and desktop
desktop
smartphone and desktop MARAT app
specialized hardware +
tablet
smartphone + browser +
specialized hardware

PintAR Hololens app

StoryMakAR app

yes

no

yes

SceneAR (ours)

smartphone

SceneAR app

image or video
image or video

some
some

video

single AR scene
N/A
N/A
N/A
N/A
N/A
no

video

N/A

no

no
N/A
N/A
N/A
yes
no
no

no

yes

no
no

no

no
no
no
no
no
no
no

no

no

yes, Snapchat lenses
yes, Instagram ﬁlters

no

no
no
no
no
no
no
no

no

no

multi-scene AR no

yes

yes, cloud-based
repository

Table 1: A summary of consumer applications and prior research focused on AR storytelling, both pre-authored and user created. The table
shows how SceneAR differs from existing work in its ability to allow sharing sequences of AR scenes as micro narratives and its ability to
remix shared AR stories.

and share their story. Digital storytelling is largely either interac-
tive (e.g., video games, web documentaries [15, 37], ﬁlm [7]) or
non-interactive (e.g., memes, gifs, comics, short videos) with some
viral memes seeing recent sales as NFTs [13]. Most non-interactive
digital stories support meta-interactivity in the form of sharing, lik-
ing, and commenting. Comics have been a powerful medium of
non-interactive storytelling for almost a century, exploring a variety
of topics from daily life and political humor to tales of heroism and
fantasy. SceneAR translates features of 20th century comics like
panels, drawings, and dialog balloons [33] into AR as scenes, 3D
models, and dialog bubbles. Each scene acts as an attention unit,
and narrative structure emerges when these scenes are placed in a
deliberate sequence by the creator [11]. Taking inspiration from
the French bandes dessin´ees to the Japanese manga and the Ameri-
can superhero comics, SceneAR proposes the idea of an AR comic
experienced scene by scene in the user’s physical environment.

2.2 AR Storytelling

Prior work has explored AR for a variety of use cases like edu-
cation [4], creativity [52], museum exhibits [6], animation [51],
location-based experiences [29, 35], tourism/heritage [22], collabo-
ration [8], journalism [36], and audio-only experiences [45]. While
there is an abundance of research on how to use AR, to our knowl-
edge, a general purpose AR app for everyday use that allows users
to create, share, and remix short-form narratives has not yet been
explored. Early works look at augmented print media that uses AR
interfaces like MagicBook [5] and edutainment [21]. Magic Story
Cube [53] introduced a tangible AR interface in the form of a folded
paper cube where each unfolding produced different pieces of a story
with non-animated visuals, voice, sound and music. ARFacade [14]
presented an interactive AR drama enabled by conversations with the
characters. More recently, Kljun et al. [27] explored the design space
of AR-augmented comic books and examined how digital content
can inﬂuence the behaviors of child readers. Wonderscope [50] in-
troduced an interactive storybook app for kids with integrated voice
recognition to encourage reading. StoryMakAR [18] is a recently in-
troduced system that merges electro-mechanical devices and virtual
characters that allow users to create stories with a browser-based
block programming interface. MARAT [41] is an mobile authoring
application that works with a back end tool that museums can use to
design virtual exhibits. Of these prior works, both StoryMakAR [18]
and MARAT [41] allow users to author AR content, though only
the former allows the creation of a story using one of four virtual
characters available in the app and custom hardware. Almost all

other projects provide AR content already created by experts for
consumption by non-experts. In contrast, SceneAR is an app that
everyday users can use to create, view, share, and remix scene-based
micro narratives with smartphone-based AR.

2.3 AR Authoring

Prior work in AR authoring can be classiﬁed into two main types:
low-level approaches that provide toolkits, libraries, or frameworks
like ARToolkit, Studierstube, or Vuforia [26, 42, 48] and high-level
approaches that are desktop-based authoring environments like Lens
Studio, MARS, DART, or ComposAR [23, 31, 32, 43, 44, 46]. More
recently, a third approach has become feasible with improvements
in tracking and registration technologies, supporting AR content cre-
ation and consumption within the same AR system [1, 38]. Feiner et
al. [17] developed one of the earliest mobile AR computing systems
to allow spatially registered information for both indoor and outdoor
locations to be accessed and managed by users through a combina-
tion of a see-through, head-worn display and a hand-held pen-based
computer. Guven et al. [24] proposed the Freeze-Frame technique
that enables the user to capture a snapshot of the environment and
author AR content directly on top of it. Hengel et al. [47] presented
a form of in situ authoring where 3D models generated from im-
ages could be inserted into the video stream. Langlotz et al. [28]
presented an on-site authoring system that allows users to add 2D
drawings and simple 3D objects in AR, though they do not offer any
ability to scale, rotate, or move the virtual objects. Location-based
game Tidy City [49] allows end users to record and upload images
and GPS locations to create AR scavenger hunt missions for other
players. Zhu et al. [54] proposed a bi-directional authoring tool that
allows desktop users as well as maintenance technicians to author
content on-site. Both of these approaches are similar in that they uti-
lize pre-created AR environments that users can modify in a limited
manner by taking new images or adding new text. None of these in
situ authoring systems are real-time or immersive, namely, they do
not allow for “developing and experiencing the content concurrently
throughout the creation process” [30]. In contrast, SceneAR is an
immersive and in situ authoring tool that enables non-experts to
create and experience AR scene-based narratives in a smartphone
AR app. By providing access to thousands of 3D models, SceneAR
makes authoring simpler and faster, much like Snapchat allows users
to add AR lenses to their faces without having to create each lens
from scratch.

3 SYSTEM DESIGN

We built SceneAR in Unity as a mobile Android application using
Google’s ARCore API [2]. ARCore allows SceneAR to track the
position of the phone in physical space and build an understand-
ing of the user’s surroundings by detecting planar surfaces in the
environment. Figure 2 shows an overview of the SceneAR system.

3.1 Narrative Components

Scenes A micro narrative in SceneAR is deﬁned as a collection
of AR scenes, each composed of 3D models that the creator or viewer
can place on surfaces in their physical environment. Each scene is
modiﬁed individually, and users can select, add, or delete scenes as
needed with no limitation on the number of scenes in a narrative.
Inspired by comic strips, our scene-based design enables a sequential
narrative in AR.

Interaction Interaction with SceneAR has two elements: touch
and speech. Touch-based interaction is used for AR object manip-
ulation and for interaction with the app UI. SceneAR recognizes
the following touch gestures: 1) tap for object placement and selec-
tion, 2) drag for object translation, 3) twist for object rotation, 4)
pinch for object scaling, and 5) two-ﬁnger drag for object elevation.
Speech-based input is used for 3D object search and for creating
dialogues as an alternative to text-based search and dialog creation.

Authoring and Presentation Users have access to a large set
of 3D models in the SceneAR app through the Google Poly 3D
object library [20]. These objects can be accessed via voice or
text search (Figure 2). We included speech input because users are
already familiar with using Siri or the Google Assistant to interact
with their smartphones. Google Cloud Speech API parses speech
data to text, which is then used to return a set of 3D models from
Google Poly (Figure 3). The app also includes a smaller set of
animated humanoid models to allow creation of scenes if the user is
temporarily ofﬂine. Figure 3 shows the main steps for creating an
AR scene. There is a separate module to add dialog balloons to any
object in a scene. The user can choose from pre-existing dialogue
or create their own using speech or text. The dialog balloons can be
removed or edited for remixing.

3.2 Application Elements

Publishing, Viewing, and Remixing a Story Before publish-
ing (Figure 2), users are asked for a title and a description, including
any physical requirements (e.g., best placed on the bath tub) that
potential story viewers, who can freely place the scenes anywhere in
their environment, should know. Once a scene is placed, the viewer
can edit it directly by adding/deleting 3D objects, dialog balloons, or

Figure 1: Concept design of a micro story told in three scenes where
the user can view each scene on surfaces in their environment as a
sequential 3D comic–like story made of three panels of 3D objects
and dialog balloons.

entire scenes to modify or expand upon the narrative. This modiﬁed
or remixed story can be published with a new title and description.

Micro Narrative Packaging SceneAR allows publishing and
sharing 3D comic-style stories. To enable this, we created Micro
AR, a new container format for packaging scene-based AR stories.
Inspired by the open-source OpenDocument Format (ODF) that
packages a text document from different components like metadata,
content, and formatting [25], the Micro AR format is designed to
package three main components of the AR story: 1) metadata, 2)
story content, and 3) spatial layout.
Metadata The Micro AR format contains app-generated and
user-created metadata about the AR story. It includes ﬁelds such
as creator, title, description/viewing guidance, original creator,
creation timestamp, and viewing statistics. The metadata deﬁnes
and differentiates each AR comic as a unique document. Metadata
allows the user to browse through a large library of stories before
deciding which one to load and view.

Content Content contains both local and remote data related
to each story. Local data are mainly dialog texts created for each
scene in a story. These are small in size and packaged for sharing
directly. Remote data are content-speciﬁc metadata that include
3D asset names and unique keys. Because these assets are large
in size, the app uses keys to retrieve the actual 3D models from
cloud storage when needed. The assets are downloaded in the
background as the user scans their environment to make the story
appear immediately upon plane detection and selection for an
improved user experience.

Spatial Layout All content
in an AR story is placed on a
physical surface for creating and for viewing. Thus, the Micro AR
package needs to track the user-deﬁned spatial layouts in each scene
of a story. Positions, rotations, scales, and groupings of 3D objects
relative to each other in each scene are recorded and packaged so
that each reconstruction and viewing of an AR story has the same
spatial layout as the original creator intended.

4 THREE-DAY USER STUDY

We evaluated SceneAR in a three-day ﬁeld study with 18 remotely
located participants. Before the study, we ran a pilot study with two
remote participants as a dry run of the study and to get early feedback
on the app’s functionality for identifying and ﬁxing technical issues.

4.1 Participants

Eighteen participants were recruited through word of mouth and
mailing lists of various academic departments around the United
States. Because we developed the SceneAR app using Google’s AR-
Core SDK, we looked for participants who owned phones ofﬁcially
supported by ARCore. Participants were located in six different
states of the United States, Brazil, India, and China. Participants
included graduate and undergraduate students, engineers, software
developers, and a chemist (12 male, 6 female), aged between 20 and
44.

Participants rated, on average, 4.1 on a 7-point Likert scale (1 =
never before, 7 = a great deal) their familiarity with AR. Pok´emon
Go was the most common AR experience cited. Participants re-
ported spending an average of 6.7 hours per week using personal
devices like phones, tablets, or computers and 1.7 hours a week
playing games on the phone or tablet. Participants also reported
posting visual content frequently on WeChat (2), Snapchat (7), and
Instagram (9), including memes (6), GIFs (3), comic strips (1), and
YouTube videos (2). Two participants reported rarely using social
media.

Figure 2: Overview of the SceneAR system (from left to right). Users search for 3D objects to add to an AR scene along with dialog bubbles.
Multiple scenes create a sequential story. Published stories are downloaded and viewed in AR. Anyone can remix any story by adding or
removing 3D objects, scenes, and dialog bubbles.

4.2 Study Procedure

4.2.1 Pre-study

Because ARCore is only supported on recent devices, ﬁnding par-
ticipants with ofﬁcial ARCore-approved devices was challenging.
Therefore, potential participants were asked to install Measure (an
AR app) and AZ (a screen-recording app) from the Google Play
Store, record a session running Measure for 3 to 5 minutes, and re-
port on device overheating, excessive battery drain, and any Measure
app crashes. Our goal was to ensure ARCore could run on devices
at least well enough for participants to successfully complete the
SceneAR study tasks, even if they were not ofﬁcially supported.
Despite this test, some participants had severe issues with ARCore
surface detection (see Results 6.1).

4.2.2 Onboarding

Each participant was onboarded in an online meeting using Zoom
video conferencing software. This meeting lasted 40 to 50 minutes.
At the beginning of the video call, we emailed the participants a
consent form (study protocol approved by our ofﬁce of research), a
participant ID, a pre-study questionnaire, the app installer (APK),
instructions on how to install the app, a document with information
about the study tasks and how to use the app, and a drive link to
upload their screen recordings. During the video call, we walked
participants through all these items. Participants also screen recorded
themselves creating a story and remixing a story during the Zoom

Figure 3: Figure showing the main SceneAR scene-creation ele-
ments. From left to right: 3D object search results displayed as
a scrollable list of preview icons at the bottom. A custom dialog
can be added via typing or speech-to-text input. Stories can be pub-
lished or loaded for viewing/remixing. Browsable list of all stories
is available for viewing and/or remixing.

meeting. At the end of the Zoom session, participants were asked
to use SceneAR as they pleased with a study task requirement to
create a total of nine stories over the three days. We closed by
scheduling a follow-up Zoom meeting three days later for ﬁlling
out a post-study questionnaire, conducting a short interview, and
showing participants how to remove the app from their devices.

4.3 Data Collection

Data were collected through screen recordings of app usage sessions,
published stories, pre- and post-study questionnaires, and Zoom
recordings of interviews. Because of the open and remote nature
of the study, we asked participants to screen record all their app
usage sessions and upload them to a secure folder at the end of
the study. After the three-day study, participants were interviewed
using a semi-structured interview schedule, which lasted an average
of 20 minutes. Before that, participants were asked to ﬁll out a
post-study questionnaire with two 7-point Likert scale (1 = strongly
disagree, 7 = strongly agree) questions, along with questions about
what they liked/disliked about SceneAR, the types of stories they
enjoyed creating, barriers to using the app, and other open-ended
feedback.

5 DATA ANALYSIS

We wanted to identify and quantify SceneAR usage, so the data
were thematically coded by two researchers independently to de-
termine characteristics and functionalities. We employed an induc-
tive thematic analysis approach to the data, as described by Braun
and Clarke [9]. Participant interviews were transcribed from the
Zoom recordings using an online service (sonix.ai), and the text
transcripts were exported for qualitative analysis. Screen recordings
were matched with interview transcripts using the participant IDs.
Two researchers independently reviewed these transcripts and record-
ings. To make meaning from the screen recordings and the inter-
views, each researcher created their own codes. Following that, the
researchers met over Zoom to discuss and reﬁne their codes, which
resulted in an agreement on 12 codes. These codes were further
analyzed and referenced with the screen recordings and feedback in
the questionnaires and then reviewed again by both researchers in
another meeting. This approach identiﬁed three overarching themes,
as discussed below.

6 RESULTS

In this section, we present the responses to the post-study question-
naire and describe the three themes we derived from our analysis of
the data: Diversity in Creativity; Spatial Creation and Viewing; and
Sharing, Remixing, and Collaborating.

Figure 4: Participant (N = 18) responses to two questions in the
post-study questionnaire asking 1) how likely are they to share a
created micro story (1 = not at all, 7 = extremely likely), and 2) was
the app easy to use (1 = very easy, 7 = very hard).

6.1 Questionnaire Responses
Likert responses are illustrated in Figure 4. Most of the participants
found our app easy to use (1 = very easy, 7 = very hard), with a
positive response median (M) of 3 and a median absolute deviation
(MAD) of 1. Participants also agreed that they were likely to share
their creations with others (1 = not at all, 7 = super excited to share)
with M = 6, MAD = 0. To understand the reasons for these scores in
the interview data, we found that two participants who rated our app
as being not very easy to use had frequent ARCore-related surface
detection issues on their devices. One user had an old phone model
that did not provide a stable frame rate, and the other user was
detecting surfaces under extremely poor lighting. On likelihood to
share the creations with others, two participants rated it low, which
matched their prior report on rarely using social media, with one
saying, “[Do] not post very often and pretty much never publicly
share things like gifs or memes.”

6.2 SceneAR Usage Summary
A summary of how participants used the app is presented in Table 2.
Participants used SceneAR in bursts throughout the study, averag-
ing about 20 minutes of app usage per participant per day. The
minimum time spent in one session was about 10 minutes, whereas
the maximum time was about 34 minutes. The minimum total app
usage time over the three days was about 30 minutes, whereas the
maximum was over 3 hours.

Across all 18 participants, a total of 194 stories were created, of
which 25% (48) were remixes. Of the original stories, 26% had one
scene, 32% had two, and 42% had three or more scenes. “Mary
had a little lamb” by P17 had the most scenes of any story at 10,
whereas “Lonely Cat” by P5 was the second longest at 8 scenes.
The longest remixed story had six scenes, titled “remix don quixote”
(Figure 7b), originally created by P5 with ﬁve scenes and remixed by

App Usage Data Summary

Published stories
Remixes
Self remixes
App usage time per user (average)
Sessions per user (average)
Session duration (average)
Unique 3D objects used
Total 3D objects

194
48
11
67 min
11
20 min
325
1,204

Table 2: Summary of SceneAR usage during the study.

(a)

(b)

Figure 5: a) Scene one shows two virtual bees arguing about who
gets the ﬂowers on the physical rug because of the shortages created
by the COVID-19 pandemic. A third bee joins them in Scene 2 to
end their argument by saying the ﬂowers are infected by the virus.
b) Scene one shows an art history professor lecturing. The two
scene screenshots show how the creator framed the professor, by
scaling the model instead of moving the camera, inspired by their
background in ﬁlm making.

P14 with a new scene. Of the remixed stories, 22% were remixes of
stories created by the participant themselves, whereas the rest were
remixes of other people’s stories. The most used search keyword
was “person” for a total of 142 times not including other related
searches for “old man, girl, black woman, athlete, drummer, artist,
Trump, etc.” and ﬁctional character searches like “Luke Skywalker,
Iron Man, Mario, Captain Marvel, Darth Vader, Gandalf, etc.” The
top three story themes were COVID-19, music, and space.

6.3 Theme 1: Diversity in Creativity

This theme describes how participants were able to create the stories
from various sources by using different narrative techniques. It is
divided into four categories: Moments from Life, Media Inspiration,
Improvisation on Visual Search, and Technique Transfer. We found
that participants were eager to discuss the types of stories they
created and why, requiring little to no prompting to describe their
experience. Several participants expressed a desire to continue using
the app after the study.

Moments from Life
A large number of stories created by participants depict moments
from life, also commonly seen in digital storytelling media like
memes, GIFs, and short-form videos. They cover topics such as
home, relationships, recreation, politics, games, nature, etc. Among
these daily life–themed stories, a variety of ongoing social events
including the COVID-19 pandemic, space travel, and government
policies (the international student ban, shelter-at-home orders,
etc.) can be observed. COVID-19 stories speciﬁcally focused on
the economic crisis, social distancing, mask wearing, Christmas
without Santa, travel bans, and loneliness. For example, P8 created
a digital-physical story in three scenes called “bees in covid” that
showed virtual bees standing on a physical ﬂoral print rug and
having an argument about who should get the ﬂowers (Figure 5a) in
this time of crisis and shortage.

Media Inspiration
Participants narrated how much they enjoyed re-creating content
from other media like movies and storybooks. The tortoise and hare
story created by P4 was remixed by two others with new scenes
and dialogues. P6 said, “We can recreate remixes of different, like,
stories that we’ve read before like Cinderella comes and picks up

her shoe. Having a twist, maybe something is not similar, at the
same time it’s a visual depiction of that [something familiar]”. P6
was interested in creating old stories that teach philosophy and
morality. P7 created a space battle based on the movie franchise
Star Wars, showing virtual ships and two 3D-printed ships, namely
the Millennium Falcon and a TIE Fighter. The Don Quixote story
was created and remixed in a playful way by several participants
(Figure 7b).

Improvisation on Visual Search
Participants improvised based on the app’s responses to their search
results supported by access to thousands of 3D models, though
sometimes even that large set did not seem enough. We noticed that,
during an object search, the selected 3D object does not always
match perfectly with the intended keyword input. Some cases were
due to the absence of a 3D model that matches the participant’s
expectations to use as a story component, which is a limitation
of keyword-based search methods. However, in some cases, the
participants were inspired by the visual results of a search and
decided to dedicate the story plot to the found 3D object. This can
be particularly observed in the story creation process by P13 where
the story was originally intended to be a basketball game between
Pikachu and Godzilla. P13 initially fetched a 3D model of a regular
basketball but later found a spherical-shaped ball-like Pikachu while
trying to fetch a regular Pikachu 3D model. This made P13 change
the story plot into an ending where Pikachu becomes the ball in
the basketball game, thus deleting the original basketball. Similar
behaviors can also be observed in P16’s creation process where one
of the main characters of the story switched from a giraffe to a fox
during the visual search of 3D objects.

Technique Transfer
Participants used their prior experience with other forms of media to
help guide their 3D creative experience. A story by P5 composed of
seven scenes titled “On Art History” is a monologue by a professor
of Western Art speaking to his audience (presumably lecturing
in class) (Figure 5b). We noticed each scene was explored in a
manner similar to framing shots in a ﬁlm, something P5 expressed
during the interview. About framing scenes, P5 said, “Even though
I did not use any real world objects, I was still able to enjoy the
functionality of using different angles and sometimes making it big,
sometimes make it small, sometimes make it distance, sometimes
make it close.” P4 said, “I’m into short ﬁlms. This app, there are
more 3D models and objects. I felt like it´s more useful for future
ﬁlmmakers and all to set their scene in this AR thing so they can
show it to their crew and team member to like better understand
director’s vision.” P1 enjoyed placing objects in scenes and said,
“You are actually constantly a theater designer or stage designer.
That’s already like a whole another set of entertainment for me.” P4,
P12, and P7 used onomatopoeic words commonly seen in comics
(e.g., ’Raawr’, ’VROOM’, ’Zzzz’, ’SCREECH’, ’pew pew’) and
emoticons seen in text messaging in their dialog balloons to express
emotions.

6.4 Theme 2: Spatial Creation and Viewing

This theme focuses on creating and consuming stories in a 3D
spatial environment. We divided it into four categories:
In
Situ Authoring for Creativity, Spatially Stimulated Imagination,
Story Setting, and Physical and Virtual Object Interplay. We
found that our users speciﬁcally enjoyed the simplicity and
speed of the scene creation process and did not remark upon
or ask for inclusion of other features (sound, animation control,
events, etc.) as necessary elements for telling their Micro AR stories.

Figure 6: Three micro stories created by P18 that connect virtual
objects with the real world. Left: A 10-scene narrative that teaches
how to play ”Mary had a little lamb” on the piano. Middle: A story
set outdoors where the character explains how to recognize different
tomato plants. Right: The dog being mildly annoyed by virtual bugs.

In Situ 3D Authoring for Creativity
Participants expressed surprise and excitement about being able
to create 3D content using their mobile devices. They reported
that SceneAR enabled free-form creativity in AR, with 3D adding
another dimension and enabling new possibilities that they had
not yet seen in any other app. P5 said, “It is just so cool, like
you create some 3D scenes with dialog with different scenes, on
a mobile device, in 20 minutes.” Some found 3D to be the next
obvious step saying (P5),“You create from drawings to moving
frames, movies, then there are just more things to do. Right? I
think there will be more storytelling ways, possibilities in this new
platform.” Participants found the ability to express themselves
in 3D liberating. P6 said, “Having it in 3D just expands the
scope. In 2D its just an image and it is quite constrained but in
3D like we can have like different scenes so it just expands the
scope.” P12 remarked upon how stories in 3D “become part of
your world” while P15 said the 3D aspect was “deﬁnitely part of
the appeal. There isn’t really anything that is creating in the app
in 3D and viewing in the app in 3D.” On 3D authoring enabling
greater creativity, P4 said, “We are going from 2D world to a 3D
world and creating stuff. In 3D world a lot of other stuff can be
include which we can’t put on paper. It will help people be more
creative and like convey the message that they want a little more
clearly to others.” P17 said 2D has barriers but with SceneAR
everything became much more limitless, though P3 worried that
there may come a time when there are “too many layers on top
of what you’re seeing and you are losing sight of what’s really there.”

Spatially Stimulated Imagination
The ability to create virtual content with changing perspectives and
realistic life-size scales seemed to have presented the participants
with a new mental model for thinking about stories. In AR, stories
and storytellers inhabit the same space, which is a blend of physical
and virtual elements. Some found this new relationship helped
them think of new ways to tell their story. P7 said, “I found
myself thinking of special stories content that I could create that I
wouldn’t otherwise be able to create using other platforms.” P8
expressed a similar idea about the scale of objects: “This is like an
environment where you can make anything kind of happen, like
you can make robots talk or like ﬂies be bigger than dinosaurs.”
Participants found the ability to change perspectives a new way of
experiencing a story. P3 said, “[O]nce I realize I can move and
see it from different angles and its like that, that helped coz you
can be inside it from different angles where you sorta like can’t
with other ways like drawing or whatever, you can’t be like inside it!”

6.5 Theme 3: Sharing, Remixing, and Collaborating

This theme relates to participants being able to share and remix
stories made by others in AR, unlike anything they reported having
seen or tried before. Our new Micro AR story packaging method
enables easy sharing and remixing of the user-created AR scenes.
The four categories are: Stories for Sharing, Remixing, Inspiration
from Community, and Communication and Collaboration.

Stories for Sharing
A fundamental aspect of storytelling is the audience. All participants
in our study enjoyed creating and sharing stories as reported on the
post-study questionnaire. P7 likened people using SceneAR to those
creating short-form video stories on TikTok 1, saying “[P]eople that
are good at making shorter funny content that would have a blast
with adding these [3D] new elements into the scene.” The desire to
include friends in stories both directly as avatars and indirectly as
people to share stories with was expressed by several participants.
Some participants added characters that resembled their friends, and
some expressed a desire for the app release to include a feature
that would allow them to import photos of their friends into the 3D
scenes.

Remixing
The idea of collaborative remixing [16] is evident in participants
creating stories specﬁcally for others to remix. For example, P8 says,
“I made it [a story about wormholes] so that other people would have
like good remix ideas for it.” All participants remarked upon the
novelty of remixing AR scenes and how much they enjoyed it. P11
was the only participant who did not create any remixes, being one
of two people (other being P3) who also mentioned not posting
on social media. Nevertheless, P11 created the most stories (13)
of anyone in the study. We would like to believe that they found
their medium of creativity in SceneAR. P1 also did not create any
remixes saying, “I never felt like I have a motivation to trying to
do it. Like storyline is so complete a lot of time that it’s better to
just start over sometimes.” Six participants explicitly mentioned
enjoying seeing remixes of their own stories or creating their own
remixes, which we call self-remixes. P2’s remixed story presented
a continuation of one idea from the ﬁrst story to the fourth one
with each new story adding new scenes, characters, and objects
(Figure 7b) They said, “[Remixing] is really cool, like seeing other
people part of your story.” Of those who enjoyed remixing, P8 said,
“Remixing stories was like a lot of fun, yeah, that was deﬁnitely very
interesting.”

Inspiration from Community
During the interview, participants highlighted that the ability to
remix made creating stories much easier. One participant contrasted
the mindless ease of taking and sharing a photo on Instagram with
putting a lot more care and purpose into the design of their scenes.
P5 said, “Most of the time I would start from seeing somebody else’s
work. It would give me inspiration. If it’s really good I would just
expand on that. I see it as a source of motivation, giving you ideas.”
P10 said, “We can follow the idea from other people and then build
on top from it.” Three participants said coming up with creative or
clever ideas was the hardest part of using the app. P4 said, “For me,
was the creativity to create the stories.” P14 also said the creativity
was the hardest part for them and being able to remix was helpful:
“You can pick idea of another person and improve this, so I think
is very nice.” P1 thought the app allowed open-ended creativity
like Lego blocks but, unlike Legos, where you are constrained by
the pieces you have, SceneAR gives you “unlimited 3D objects,
unlimited resources to create your own work.” P17 wanted the app
to give them suggestions for objects “based on the physical space

1https://www.tiktok.com/en/

(a)

(b)

Figure 7: a) Left: two penguins debating who jumps ﬁrst while
standing on the edge of a bath tub with the water running. Right:
a re-creation of the Super Mario video game with Princess Peach
hidden behind the physical toy castle and virtual Bowser defending
it. b) This set of two screenshots shows a scene from the original
story ”Don Quixote” by P5 on the left and the same scene remixed
by P14 with the addition of a robot cat on the right.

Story Setting
We noticed participants created their stories in different settings
around their homes—from bathrooms, kitchens, and hallways to
bedrooms and front yards. P15 created a story called “penguins go
swimming” where virtual penguins debate who will jump ﬁrst while
standing on the edge of a physical bath tub (Figure 7a left). P15
created another story on the kitchen counter introducing a virtual
cookie to a physical one. Understandably, these stories require the
viewer to have similar physical conditions to experience the story as
designed. When asked about blending the real and the virtual, P15
said, “[It] is a really fun and exciting new mechanism for people to
share their creativity.” P14 echoed that sentiment, saying, “I think
it’s the opportunity to mix real world with ﬁction AR.” P18 was the
only participant who made stories outdoors, one story showing their
vegetable garden, another one teaching the viewer how to play a
song on the piano, and yet another one attaching virtual bugs to their
dog (Figure 6). When asked, P18 remarked, “Every new tool is a
new way to think about things and do things in different ways.”

Physical and Virtual Object Interplay
In addition to setting a story in a physical place (e.g., kitchen
counter), we noted that participants created stories with physical
objects. P7 created a ﬁctional story based on the popular video game
franchise Super Mario using the game characters and a physical
toy castle (Figure 7a, right) as the main elements. P6 expressed a
desire for more interaction with physical objects saying, “If I could
use dialogues on physical objects, even though there was no object
detection, I could just put a dialog.” P7 said their favorite part
was “making the digital and physical worlds act with each other.”
These participants demonstrated a different type of relationship
between the physical and the virtual, one where the setting has no
relationship to the story but the story itself integrates physical and
virtual elements, for example, a toy castle with the virtual Mario
(Figure 7a, right). P7 created two stories (“Mario” and “Star
Wars”) that incorporated physical objects like toys and 3D-printed
space ships, but not the physical location, into the stories.

recognition” to make the experience more intuitive, such as creating
stories related to living rooms if they are in the living room.

Communication and Collaboration
We noted that remixing afforded a novel form of back-and-forth
communication between participants where one person expressed
their thoughts and emotions through a story and another person
remixed it only to be remixed again by the ﬁrst person. P2 remarked,
“We don’t want to build everything from scratch.
I need some
existing background that I can put my people and you know start
like conversation.” P12 found remixing a good way to “share
imagination and creativeness with each other.” P12 said that seeing
other people modify your stories gives you an understanding of how
they have interpreted your work. During the interviews, participants
expressed a desire to have a social network and the ability to create
these back-and-forth narrative-based dialogues with friends. P6 said,
“if I could have friends on this app and I can share the story and they
could render it in their surroundings, like they could load it in their
surroundings, then I can like ask them to edit, so it would be like a
collaborative creation.”

7 DISCUSSION: STRATEGIES FOR SUPPORTING AR STO-

RYTELLING

Here we articulate six design strategies that we derived from our
data analysis and participant feedback. These strategies can serve
as a guide for the development and design of future AR short-form
storytelling systems that allow users to create, share, and remix 3D
scene-based narratives on their mobile devices.

7.1 Consider Spatial Dependencies

One of the challenges with sharing AR stories is that the creator’s
physical environment is likely to differ from the consumer’s. For
example, an AR story can have contextual dependencies, such as
the size of the detected plane, which might impact the position and
rotation of AR objects. This could happen if, for example, the creator
uses a large dining table to create the story and the viewer tries to
render it on a small coffee table. Similarly, a creator might compose
a story on multiple detected planes. For example, a creator might
create one scene on the ﬂoor and a subsequent one on a table. If
the person viewing the story does not have an environment with
two planes at different relative heights, some virtual objects might
appear to be ﬂoating. These differences can create a disconnect
between the virtual objects and physical surfaces, which can break
the illusion of realism for the viewer. Until smartphone devices
with LIDAR sensing become commonplace and users are willing
and able to share their entire physical environment, one way of
mitigating the challenge of spatial dependencies is to guide the story
consumer to arrange their physical environment to match what the
creator had in mind more closely, something people usually do for
watching football games together. SceneAR does this through the
story description.

7.2 Include Spatial Navigation

The story consumer has complete control over how and where they
view the AR scenes. However, this can negatively impact the viewing
experience owing to accidental camera movements. For example, if
the person places a scene on the ﬂoor and happens to turn around,
that scene will fall out of the camera’s viewport. In such scenarios,
locating the scene again with the camera’s viewﬁnder, without virtual
visual guidance, may be difﬁcult. One way to resolve this is to
provide directional arrows that point the viewer toward the center of
the scene, especially if they are noticeably off-track from ﬁnding the
scene with their camera. More complex 3D navigation information
is needed to guide the viewer in multi-plane AR stories.

7.3 Recognize Camera Clutter
The ease with which creators can add virtual objects to a scene
can lead them to crowd the camera’s viewport and make it difﬁcult
to interact with the objects in the scene (e.g., selecting, scaling,
moving). Some of our participants reported this problem. Designers
can mitigate this problem by exploring different narrative structures,
such as timed objects or plane-to-plane movement, to prevent AR
objects from overcrowding a single viewport.

7.4 Enable Support for Contextual Constraints
The differences between the creator’s and viewer’s environment
introduce some semantic challenges in addition to the spatial ones
described above. For example, a viewer would miss some of the
story’s context if they saw the story about the two penguins (Fig-
ure 7a) in a place without a bathtub. It would not present exactly as
the creator intended. Again, once LIDAR sensors in mobile devices
become commonplace, sharing the story context in 3D will become
simpler but might introduce its own set of challenges like privacy.
Meanwhile, a simple way to alleviate this challenge is by listing
physical dependencies of the stories and leaving the consumer in
charge of their AR experience of that story. Effective storytelling
in AR truly augments a user’s reality; hence, describing the type of
reality the creator intended to augment is necessary until we are able
to easily create and share the 3D context with each AR narrative.

7.5 Assess Surface Detection Limitations
Unlike traditional touchscreen apps, creating and viewing stories in
AR requires physical movement to scan the physical space, detect
planes, and move around for an immersive experience. Several
factors can impact surface detection, including the user’s device,
lighting, clutter, and textures (or lack thereof). Repeated surface
detection failures can make the AR experience feel onerous. One
way to mitigate this issue, for the creator and the viewer, is to
often save or cache the story content to prevent the story from
disappearing when the detected surface is lost. While newer Android
devices can support Depth APIs [3, 19] and newer Apple devices
have LIDAR sensors, both of which can improve the user’s AR
experience, most people do not own these devices yet, especially if
we want to consider bringing AR to the billions of mobile devices in
use worldwide.

7.6 Offer Help with Writer’s Block
People can feel intimidated when creating a story from scratch, as
some of our participants reported. Although this is not unique to AR,
the spatial nature and even the novelty of AR might exacerbate this
problem. One way to mitigate this is to scaffold new stories through
previous stories, such as by allowing people to remix other people’s
stories as supported in SceneAR. Other options include offering
in-app suggestions or templates. For example, the app can detect the
user’s location (e.g., a public park) and offer a story template based
on it, or, as one participant suggested, the app can offer suggestions
for virtual objects based on physical object recognition in their im-
mediate environment. Although suggestions can be helpful, systems
that scaffold (e.g., for novices) and support open-ended explorations
for experts would allow us to build on decades of user-centered
research in the design of web and mobile interfaces. Interestingly,
participants commented that SceneAR could be the next AR meme
generation app because of its rich data set of models and the freedom
to create, share, and modify combinations of visuals and text.

8 LIMITATIONS
Our research provides insights for using AR as a storytelling plat-
form for creating scene-based micro narratives. However, our work
has some limitations. We focused on understanding how people
would use an AR storytelling app and the opportunities and chal-
lenges that would arise. We chose to evaluate SceneAR as a minimal

viable prototype, directing the user’s efforts toward the creating,
publishing, viewing, and remixing of micro stories rather than opti-
mizing elements of the UI or adding more complex features. With
informal testing, and later with the pilot study, we realized that users
are not familiar with AR; namely, they do not understand how to
scan, what types of surfaces make good candidates for plane de-
tection (e.g., not blank walls or ﬂoors or white tables), nor do they
realize the impact of low lighting levels on surface detection and
tracking, to name a few. Therefore, while our original application
had complex elements like hand gesture recognition and ﬁnger-based
drawing in space, we chose to keep things simpler for the evaluation.
We plan to investigate the multi-modal input modalities (gesture,
speech, and touch) in a more controlled lab environment once we
are allowed to do so in the fall. Although conducting the study with
remotely located participants increased the complexity of the study
design, we believe it also helped us learn things about AR use we
may not have learned in an in-lab study, as mentioned above.

Additionally, the demographics of our sample may have biased
our results. While the average age of our participants might align
with those who are more likely to use creative or social media apps,
our sample does not include older adults or children (i.e., those 18
years and below). A future study of two larger groups of users—
with and without storytelling or creative backgrounds—may help
us better understand narrative strategies and functionalities of our
system beyond the end user social scenario considered in this work.

9 CONCLUSION

We presented SceneAR, a mobile AR app that enables users to create,
publish, view, and remix scene-based micro narratives in AR. We
detailed the design and implementation of SceneAR and presented
ﬁndings from a three-day ﬁeld study with 18 participants. Remixing
enabled a new form of visual communication between participants,
both by modifying a story and watching others modify and remix
their stories. Conducting the study outside the lab environment
revealed design tensions in AR apps primarily due to ARCore issues
related to reliable surface detection and environmental conditions
like lighting, surface textures, and clutter. Participants expressed a
desire for a social network where they can create longer story-based
communication threads, in both private and public conversations, or
use the app to create AR memes. Based on the study, we outlined
design strategies for AR scene-based narrative systems, highlighting
characteristics unique to smartphone-based AR.

REFERENCES

[1] Adobe Aero: Augmented reality. Now a reality., 2020.
[2] ARCore: Build the future, 2020.
[3] ARKit: Dive into the world of augmented reality., 2020.
[4] M. Billinghurst and A. Duenser. Augmented reality in the classroom.

Computer, 45(7):56–63, 2012.

[5] M. Billinghurst, H. Kato, and I. Poupyrev. The magicbook-moving
seamlessly between reality and virtuality. IEEE Computer Graphics
and applications, 21(3):6–8, 2001.

[6] O. Bimber, L. M. Encarnac¸ ˜ao, and D. Schmalstieg. The virtual show-
case as a new platform for augmented reality digital storytelling. In
Proceedings of the workshop on Virtual environments 2003, pp. 87–95,
2003.

[7] Black Mirror: Bandersnatch, 2018.
[8] N. Braun. Storytelling in collaborative augmented reality environments.

2003.

[9] V. Braun and V. Clarke. Using thematic analysis in psychology. Quali-

tative research in psychology, 3(2):77–101, 2006.

[10] A history of storytelling through pictures, 2020.
[11] N. Cohn. The Visual Language of Comics: Introduction to the Structure

and Cognition of Sequential Images. A&C Black, 2013.

[12] S. Dasgupta, W. Hale, A. Monroy-Hern´andez, and B. M. Hill. Remix-
ing as a pathway to computational thinking. In Proceedings of the 19th

ACM Conference on Computer-Supported Cooperative Work & Social
Computing, pp. 1438–1449, 2016.

[13] ‘disaster girl’ has sold her popular meme as an nft for $500,000, 2020.
[14] S. Dow, M. Mehta, B. MacIntyre, and M. Mateas. Ar fac¸ade: an
augmented reality interactve drama. In Proceedings of the 2007 ACM
symposium on Virtual reality software and technology, pp. 215–216,
2007.

[15] J. Ducasse, M. Kljun, and K. Pucihar. Interactive web documentaries:
A case study of audience reception and user engagement on iotok.
International Journal of Human-Computer Interaction, 36:1–28, 05
2020. doi: 10.1080/10447318.2020.1757255

[16] B. Dybwad. Approaching a deﬁnition of web 2.0. Retrieved March,

19:2009, 2005.

[17] S. Feiner, B. Macintyre, T. H¨ollerer, and A. Webster. A touring ma-
chine: Prototyping 3d mobile augmented reality systems for exploring
the urban environment. vol. 1, pp. 74–81, 12 1997. doi: 10.1007/
BF01682023

[18] T. Glenn, A. Ipsita, C. Carithers, K. Peppler, and K. Ramani. Story-
makar: Bringing stories to life with an augmented reality & physical
prototyping toolkit for youth. In Proceedings of the 2020 CHI Confer-
ence on Human Factors in Computing Systems, pp. 1–14, 2020.

[19] Google Depth API, 2020.
[20] Google Poly: Explore the world of 3D, 2020.
[21] R. Grasset, A. D¨unser, and M. Billinghurst. Edutainment with a mixed
reality book: a visually augmented illustrative childrens’ book. In Pro-
ceedings of the 2008 international conference on advances in computer
entertainment technology, pp. 292–295, 2008.

[22] F. Guimar˜aes, M. Figueiredo, and J. Rodrigues. Augmented reality
and storytelling in heritage application in public gardens: Caloust
gulbenkian foundation garden. In 2015 Digital Heritage, vol. 1, pp.
317–320. IEEE, 2015.

[23] S. Guven and S. Feiner. Authoring 3d hypermedia for wearable aug-
mented and virtual reality. In Seventh IEEE International Symposium
on Wearable Computers, 2003. Proceedings., pp. 118–126, 2003. doi:
10.1109/ISWC.2003.1241401

[24] S. Guven, S. Feiner, and O. Oda. Mobile augmented reality interaction
techniques for authoring situated media on-site. In 2006 IEEE/ACM
International Symposium on Mixed and Augmented Reality, pp. 235–
236, 2006. doi: 10.1109/ISMAR.2006.297821

[25] Information technology — open document format for ofﬁce applica-
tions (opendocument) v1.2 - part1: Opendocument schema. Standard,
International Organization for Standardization, July 2015.

[26] H. Kato. Artoolkit: library for vision-based augmented reality. IEICE,

PRMU, 6(79-86):2, 2002.

[27] M. Kljun, M. ˇCelar, K. Pucihar, J. Alexander, M. Weerasinghe, C. Cam-
pos, J. Ducasse, B. Kopaˇcin, J. Grubert, and P. Coulton. Augmentation
not duplication: Considerations for the design of digitally-augmented
comic books. In Proceedings of the 2019 CHI Conference on Hu-
man Factors in Computing Systems, pp. 1–12, 04 2019. doi: 10.1145/
3290605.3300333

[28] T. Langlotz, S. Mooslechner, S. Zollmann, C. Degendorfer, G. Reit-
mayr, and D. Schmalstieg. Sketching up the world: in situ authoring
for mobile augmented reality. Personal and ubiquitous computing,
16(6):623–630, 2012.

[29] G. A. Lee, A. D¨unser, S. Kim, and M. Billinghurst. Cityviewar: A
mobile outdoor ar application for city visualization. In 2012 IEEE
International Symposium on Mixed and Augmented Reality-Arts, Media,
and Humanities (ISMAR-AMH), pp. 57–64. IEEE, 2012.
[30] G. A. Lee, C. Nelles, M. Billinghurst, and G. J. Kim.

Immersive
authoring of tangible augmented reality applications. In Third IEEE
and ACM International Symposium on Mixed and Augmented Reality,
pp. 172–181. IEEE, 2004.

[31] Lens Studio: Creativity Powered by AR, 2020.
[32] B. MacIntyre, M. Gandy, S. Dow, and J. D. Bolter. Dart: a toolkit
for rapid design exploration of augmented reality experiences.
In
Proceedings of the 17th annual ACM symposium on User interface
software and technology, pp. 197–206, 2004.

[33] S. McCloud. Understanding comics: The invisible art. Northampton,

Mass, 1993.

[34] M. Nebeling and M. Speicher. The trouble with augmented real-

ity/virtual reality authoring tools. In 2018 IEEE International Sympo-
sium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct), pp.
333–337. IEEE, 2018.

[35] J. Paay, J. Kjeldskov, A. Christensen, A. Ibsen, D. Jensen, G. Nielsen,
and R. Vutborg. Location-based storytelling in the urban environment.
In Proceedings of the 20th Australasian Conference on Computer-
Human Interaction: Designing for Habitus and Habitat, pp. 122–129,
2008.

[36] J. V. Pavlik and F. Bridges. The emergence of augmented reality (ar)
as a storytelling medium in journalism. Journalism & Communication
Monographs, 15(1):4–59, 2013.

[37] A. Podara, D. Giomelakis, C. Nicolaou, M. Matsiola, and R. Kotsakis.
Digital storytelling in cultural heritage: Audience engagement in the
interactive documentary new life. Sustainability, 13, 01 2021. doi: 10.
3390/su13031193

[38] Apple Reality Composer: Build Augmented Reality, 2020.
[39] M. Resnick and B. Silverman. Some reﬂections on designing construc-
tion kits for kids. In Proceedings of the 2005 conference on Interaction
design and children, pp. 117–122, 2005.

[40] M. Rossiter and P. A. Garcia. Digital storytelling: A new player on
the narrative ﬁeld. New directions for adult and continuing education,
126:37–48, 2010.

[41] D. Rumi´nski and K. Walczak. Creation of interactive ar content on
mobile devices. In International Conference on Business Information
Systems, pp. 258–269. Springer, 2013.

[42] D. Schmalstieg, A. Fuhrmann, G. Hesina, Z. Szalav´ari, L. M.
Encarnac¸ao, M. Gervautz, and W. Purgathofer. The studierstube aug-
mented reality project. Presence: Teleoperators & Virtual Environ-
ments, 11(1):33–54, 2002.

[43] H. Seichter, J. Looser, and M. Billinghurst. Composar: An intuitive tool
for authoring ar applications. In 2008 7th IEEE/ACM International
Symposium on Mixed and Augmented Reality, pp. 177–178. IEEE,
2008.

[44] Spark AR: Brand new features.All-new possibilities., 2020.
[45] M. Sra. Spellbound: An activity-based outdoor mobile multiplayer
game. PhD thesis, Massachusetts Institute of Technology, 2013.

[46] Unity MARS, 2020.
[47] A. van den Hengel, R. Hill, B. Ward, and A. Dick. In situ image-based
modeling. In 2009 8th IEEE International Symposium on Mixed and
Augmented Reality, pp. 107–110. IEEE, 2009.
[48] Vuforia: Market-Leading Enterprise AR, 2020.
[49] R. Wetzel, L. Blum, and L. Oppermann. Tidy city: a location-based
game supported by in-situ and web-based authoring tools to enable
user-created content. In Proceedings of the international conference
on the foundations of digital games, pp. 238–241, 2012.

[50] Wonderscope: An iOS app for kids, 2020.
[51] H. Ye, K. C. Kwan, W. Su, and H. Fu. Aranimator: in-situ charac-
ter animation in mobile ar with user-deﬁned motion gestures. ACM
Transactions on Graphics (TOG), 39(4):83–1, 2020.

[52] R. M. Yilmaz and Y. Goktas. Using augmented reality technology in
storytelling activities: examining elementary students’ narrative skill
and creativity. Virtual Reality, 21(2):75–89, 2017.

[53] Z. Zhou, A. D. Cheok, J. Pan, and Y. Li. Magic story cube: an
interactive tangible interface for storytelling. In Proceedings of the
2004 ACM SIGCHI International Conference on Advances in computer
entertainment technology, pp. 364–365, 2004.

[54] J. Zhu, S. K. Ong, and A. Y. Nee. An authorable context-aware aug-
mented reality system to assist the maintenance technicians. The
International Journal of Advanced Manufacturing Technology, 66(9-
12):1699–1714, 2013.

