1
2
0
2

c
e
D
1

]
T
S
.
h
t
a
m

[

1
v
3
9
6
0
0
.
2
1
1
2
:
v
i
X
r
a

AUTO-REGRESSIVE APPROXIMATIONS TO NON-STATIONARY TIME
SERIES, WITH INFERENCE AND APPLICATIONS

BY XIUCAI DING1 AND ZHOU ZHOU2,*

1Department of Statistics, University of California, Davis xcading@ucdavis.edu

2Department of Statistical Sciences, University of Toronto *zhou@utstat.utoronto.ca

Understanding the time-varying structure of complex temporal systems
is one of the main challenges of modern time series analysis. In this pa-
per, we show that every uniformly-positive-deﬁnite-in-covariance and suf-
ﬁciently short-range dependent non-stationary and nonlinear time series can
be well approximated globally by a white-noise-driven auto-regressive (AR)
process of slowly diverging order. To our best knowledge, it is the ﬁrst time
such a structural approximation result is established for general classes of
non-stationary time series. A high dimensional L2 test and an associated
multiplier bootstrap procedure are proposed for the inference of the AR
approximation coefﬁcients. In particular, an adaptive stability test is pro-
posed to check whether the AR approximation coefﬁcients are time-varying,
a frequently-encountered question for practitioners and researchers of time
series. As an application, globally optimal short-term forecasting theory and
methodology for a wide class of locally stationary time series are established
via the method of sieves.

1. Introduction. The Wiener-Kolmogorov prediction theory [25, 26, 41] is a fundamen-
tal result in time series analysis which, among other ﬁndings, guarantees that a weakly sta-
tionary time series can be represented as a white-noise-driven auto-regressive (AR) process
of inﬁnite order under some mild conditions. The latter structural representation result has
had profound inﬂuence in the development of the classic linear time series theory. Later, [1, 2]
studied the truncation error of AR prediction of stationary processes when ﬁnite many past
values, instead of the inﬁnite history, were used in the prediction. Nowadays, as increasingly
longer time series are being collected in the modern information age, it has become more
appropriate to model many of those series as non-stationary processes whose data generat-
ing mechanisms evolve over time. Consequently, there has been an increasing demand for
a systematic structural representation theory for such processes. Nevertheless, it has been a
difﬁcult and open problem to establish linear structural representations for general classes
of non-stationary time series. The main difﬁculty lies in the fact that the profound spectral
domain techniques which were essential in the investigation of the AR(
) representation
for stationary sequences are difﬁcult to apply to non-stationary processes where the spectral
density function is either difﬁcult to deﬁne or only deﬁned locally in time.

∞

The ﬁrst main purpose of the paper is to establish a uniﬁed AR approximation theory for
a wide class of non-stationary time series. Speciﬁcally, we shall establish that every short
memory and uniformly-positive-deﬁnite-in-covariance (UPDC) non-stationary time series
n
i=1 can be well approximated globally by a non-stationary white-noise-driven AR pro-
xi,n}
{
cess of slowly diverging order; see Theorem 2.5 for a more precise statement. Similar to the
spirit of the Wiener-Kolmogorov prediction theory, the latter structural approximation result
connects a wide range of fundamental problems in non-stationary time series analysis such
as optimal forecasting, dependence quantiﬁcation, efﬁcient estimation and adaptive boot-
strap inference to those of AR processes and ordinary least squares (OLS) regression with
diverging number of dependent predictors. In fact, the very reason for us to consider the AR

1

 
 
 
 
 
 
2

approximation instead of a moving average approximation or representation (c.f. Wold de-
composition [42]) to non-stationary time series is due to its close ties with the OLS regression
and hence the ease of practical implementation. Our proof of the structural approximation re-
sult resorts to modern operator spectral theory and classical approximation theory [16] which
control the decay rates of inverse of banded matrices. Consequently the decay speed of the
best linear projection coefﬁcients of the time series can be controlled via the Yule-Walker
equations; see Theorem 2.4 for more details.

The last two decades have witnessed the rapid development of locally stationary time se-
ries analysis in statistics. Locally stationary time series refers to the subclass of non-stationary
time series whose data generating mechanisms evolve smoothly or slowly over time. See [12]
for a comprehensive review. For locally stationary processes, we will show that the UPDC
condition is equivalent to the uniform time-frequency positiveness of the local spectral den-
n
i=1 (c.f. Proposition 2.9) and the approximating AR process has smoothly time-
sity of
varying coefﬁcients (c.f. Theorem 2.11).

xi,n}
{

L

In practice, one may be interested in testing various hypotheses on the AR approximation
such as whether some approximation coefﬁcients are zero or whether the approximation coef-
ﬁcients are invariant with respect to time. The second main purpose of the paper is to propose
2 test and an associated multiplier bootstrap procedure for the inference
a high-dimensional
of the AR approximation coefﬁcients of locally stationary time series. For the sake of brevity
we concentrate on the test of stability of the approximation coefﬁcients with respect to time
for locally stationary time series (c.f. (3.2)). It is easy to see that similar methodologies can be
developed for other problems of statistical inference such as tests for parametric assumptions
on the approximation coefﬁcients. Our test is shown to be adaptive to the strength of the time
series dependence as well as the smoothness of the underlying data generating mechanism;
see Propositions 3.6 and 3.7 and Algorithm 1 for more details. The theoretical investigation
of the test critically depends on a result on Gaussian approximations to quadratic forms of
high-dimensional locally stationary time series developed in the current paper (c.f. Theo-
rem 3.4). In particular, uniform Gaussian approximations over high-dimensional convex sets
[8, 22] as well as m-dependent approximations to quadratic forms of non-stationary time
series are important techniques used in the proofs.

Interestingly, the test of stability for the AR approximation coefﬁcients is asymptotically
equivalent to testing correlation stationarity in the case of locally stationary time series; see
Theorem 3.1 for more details. Here correlation stationarity means that the correlation struc-
ture of the time series does not change over time. As a result, our stability test can also be
viewed as an adaptive test for correlation stationarity. In the statistics literature, there is a re-
cent surge of interest in testing covariance stationarity of a time series using techniques from
the spectral domain. See, for instance, [17, 20, 29, 30]. But it seems that the tests for cor-
relation stationarity have not been discussed in the literature. Observe that the time-varying
marginal variance has to be estimated and removed from the time series in order to apply the
aforementioned tests to checking correlation stationarity. However, it is unknown whether
the errors introduced in such estimation would inﬂuence the ﬁnite sample and asymptotic
behaviour of the tests. Furthermore, estimating the marginal variance usually involves the
difﬁcult choice of a smoothing parameter. One major advantage of our test when used as a
test of correlation stationarity is that it is totally free from the marginal variance as the latter
quantity is absorbed into the errors of the AR approximation and hence is independent of the
AR approximation coefﬁcients.

Historically, the Wiener-Kolmogorov prediction theory was motivated by the optimal fore-
casting problem of stationary processes. Analogously, the AR approximation theory estab-
lished in this paper is directly applicable to the problem of optimal short-term linear fore-
casting of non-stationary time series. For locally stationary time series, thanks to the AR

AR APPROXIMATIONS TO NON-STATIONARY TIME SERIES

3

approximation theory, the optimal short-term forecasting problem boils down to that of efﬁ-
ciently estimating the smoothly-varying AR approximation coefﬁcient functions at the right
boundary. We propose a nonparametric sieve regression method to estimate the latter coef-
ﬁcient functions and the associated MSE of forecast. Contrary to most non-stationary time
series forecasting methods in the literature where only data near the end of the sequence are
used to estimate the parameters of the forecast, the nonparametric sieve regression is global
in the sense that it utilizes all available time series observations to determine the optimal
forecast coefﬁcients and hence is expected to be more efﬁcient. Furthermore, by controlling
the number of basis functions used in the regression, we demonstrate that the sieve method
is adaptive in the sense that the estimation accuracy achieves global minimax rate for non-
parametric function estimation in the sense of [36] under some mild conditions; see Theorem
4.3 for more details. In the statistics literature, there have been some scattered works dis-
cussing non-stationary time series prediction from some different angles. See for instance
[13, 18, 23, 24, 34], among others. With the aid of the AR approximation, we are able to
establish a uniﬁed globally-optimal short-term forecasting theory for a wide class of locally
stationary time series asymptotically.

The rest of the paper is organized as follows. In Section 2, we introduce the AR approxi-
mation results for both general non-stationary time series and locally stationary time series.
2 statistics of the esti-
In Section 3, we test the stability of the AR approximation using
mated AR coefﬁcient functions for locally stationary time series. A multiplier bootstrapping
procedure is proposed and theoretically veriﬁed for practical implementation. In Section 4,
we provide one important application of our AR approximation theory in optimal forecasting
of locally stationary time series. In Section 5, we use extensive Monte Carlo simulations to
verify the accuracy and power of our proposed methodologies. In Section 6, we conduct anal-
ysis on a ﬁnancial real data set using our proposed methods. Technical proofs are deferred to
the supplementary material.

L

x
k

0 as n

1, denote by
bn}
{

Convention. Throughout the paper, we will consistently use the following notation. For a
matrix Y or vector y, we use Y ∗ and y∗ to stand for their transposes. For a random variable
kq = (E
q)1/q its Lq norm. For two sequences
x and some constant q
x
|
|
≥
, the notation an = O(bn) means that
of real numbers
for some
and
an| ≤
an}
bn|
C
{
|
|
for some positive sequence
ﬁnite constant C > 0, and an = o(bn) means that
an| ≤
bn|
cn|
|
, we
and positive real values
. For a sequence of random variables
xn}
cn ↓
{
use the notation xn = OP(an) to state that xn/an is stochastically bounded. Similarly, we use
the notation xn = oP(an) to say that xn/an converges to 0 in probability. Moreover, we use
the notation xn = Oℓq (an) to state that xn/an is bounded in Lq norm; that is,
C
for some ﬁnite constant C. Similarly, we can deﬁne xn = oℓq (an). We will always use C as
a genetic positive and ﬁnite constant independent of n whose value may change from line to
line.

xn/ankq ≤
k

an}
{

→ ∞

2. Auto-Regressive Approximations to Non-stationary Time Series .
In this section,
we establish a general AR approximation theory for a non-stationary time series
under
xi,n}
{
mild assumptions related to its covariance structure. Speciﬁcally, in Section 2.1, we study
general non-stationary time series. In Section 2.2, we investigate the special case of locally
stationary time series where the covariance structure is assumed to be smoothly time-varying.
Before proceeding to our main results, we pause to introduce two mild assumptions. Till
the end of the paper, unless otherwise speciﬁed, we omit the subscript n and simply write
xi ≡
First, in order to avoid erratic behavior of the AR approximation, the smallest eigenvalue
of the time series covaraince matrix should be bounded away from zero. For stationary time
series, this is equivalent to the uniform positiveness of the spectral density function which is

xi,n.

4

widely used in the literature. Further note that the latter assumption is mild and frequently
used in the statistics literature of covariance and precision matrix estimation; see, for instance,
[6, 11, 46] and the references therein. In this paper we shall call this uniformly-positive-
deﬁnite-in-covariance (UPDC) condition and formally summarize it as follows.

ASSUMPTION 2.1 (UPDC). For all n

N, we assume that there exists a universal con-

∈

stant κ > 0 such that

(2.1)

λn(Cov(x1,

, xn))

κ,

· · ·

≥

) is the covariance
) is the smallest eigenvalue of the given matrix and Cov(
where λn(
·
·
matrix of the given vector.

As discussed earlier, the UPDC is a mild assumption and is widely used in the litera-
ture. Moreover, for locally stationary time series, we will provide a necessary and sufﬁcient
condition from spectral domain (c.f. Proposition 2.9) for practical checking.

Second, we impose the following assumption to control the covariance decay speed of the

non-stationary time series.

ASSUMPTION 2.2. For 1

such that

k, l

≤

≤

n, we assume that there exists some constant τ > 1

(2.2)

−
where C > 0 is some universal constant independent of n.

| ≤

max
k,l

Cov(xk, xl)
|

C

k
|

l

τ ,

−
|

Assumption 2.2 states that the covariance structure of

decays polynomially fast and it
can be easily satisﬁed for many non-stationary time series; see Example 2.7 for a demonstra-
tion. Note that τ > 1 amounts to a short range dependent requirement for
in the sense
is bounded above by a ﬁxed ﬁnite constant for all k and n while the
that
latter sum may diverge if τ

n
l=1 Cov(xk, xl)
|

xi}
{

xi}
{

1.

|

P

≤

REMARK 2.3.

In (2.2), we assume a polynomial decay rate. We can easily obtain analo-
gous results to those established in this paper when the covariance decays exponentially fast;
i.e.,

(2.3)

max
k,l

Cov(xk, xl)
|

| ≤

Ca|

k

l

−

|, 0 < a < 1.

For the sake of brevity, we focus on reporting our main results under the polynomial decay
Assumption 2.2. From time to time, we will brieﬂy mention the results under the exponential
decay assumption (2.3) without providing extra details.

2.1. AR approximation for general non-stationary time series.

In this subsection, we
satisfying
establish an AR approximation theory for general non-stationary time series
xi}
{
Assumptions 2.1 and 2.2. Denote by b
b(n) a generic value which speciﬁes the order of the
AR approximation. In what follows, we investigate the accuracy of an AR(b) approximation
to
and provide the error rates using such an approximation. Observe that for theoretical
and practical purposes b is typically required to be much smaller than n in order to achieve
a parsimonious approximating model. For i > b, the best linear prediction (in terms of the
mean squared prediction error)
1, is
denoted as

xi of xi which utilizes all its predecessors x1,

xi}
{

, xi

· · ·

≡

−

b

xi = φi0 +

b

1

i

−

Xj=1

φijxi

−

j, i = b + 1,

, n,

· · ·

AR APPROXIMATIONS TO NON-STATIONARY TIME SERIES

5

φij}
{

where
n
i=1 is a time-varying white noise process, i.e.,
ǫi}
{

are the prediction coefﬁcients. Denote ǫi := xi −

Eǫi = 0 , Cov(ǫi, ǫj) = 1(i = j)σ2
i .

xi. It is well-known that

b

Armed with the above notation, we write

(2.4)

1

i

−

xi = φi0 +

φijxi

−

j + ǫi, i = b + 1,

, n.

· · ·

Xj=1

To provide an AR approximation of order b, where b may be much smaller than n, we need
to examine the theoretical properties of the coefﬁcients φij . We summarize the results in
Theorem 2.4.

THEOREM 2.4.

xi}
{
exists some constant C > 0, when n is sufﬁciently large, we have that

Suppose Assumptions 2.1 and 2.2 hold for

. For τ in (2.2), there

(2.5)

max
i

φij| ≤
|

C

(cid:18)

Moreover, analogously to (2.4), denote by
based on xi
, xi

b, i.e.,

1,

−

· · ·

−

τ

1

−

log j + 1
j

(cid:19)
φb
ij }
{

,

for all j

1.

≥

the best linear forecast coefﬁcients of xi

(2.6)

Then we have that

(2.7)

xi = φb

i0 +

b

Xj=1

φb
ijxi

−

j + ǫb

i , i > b.

max
i>b

max
j
1
≤
≤
max
i>b |

φb
ij| ≤

φij −
b |
φb
φi0 −
i0| ≤

C(log b)τ

1b−

−

(τ

3),

−

C(log b)τ

1b−

−

(τ

3.5).

−

xi}
{

On the one hand, Theorem 2.4 is general and only needs mild assumptions on the covari-
ance structure of
. On the other hand, all error bounds in Theorem 2.4 are adaptive to the
decay rate of the temporal dependence and the order of the AR approximation. Particularly,
by (2.5), we only need τ > 1 to ensure a polynomial decay of the coefﬁcients φij as a func-
tion of j. Meanwhile, (2.7) establishes that the best linear forecast coefﬁcients of xi based on
xi

1,
−
Based on Theorem 2.4, we establish an AR approximation theory for the time series

b are close provided that τ and b are sufﬁciently large.

, x1 and xi

, xi

· · ·

· · ·

1,

−

−

xi}
{

in Theorem 2.5. Denote the process

by

x∗i }
{

(2.8)

x∗i =

xi,
φi0 +

(

b
j=1 φijx∗i
−

j + ǫi,

i
b;
≤
i > b.

Since
ǫi}
{
AR(b) process.

is a time-varying white noise process, by construction, we have that

P

x∗i }i
{

≥

1 is an

THEOREM 2.5.

Suppose the assumptions of Theorem 2.4 hold. Then we have that for all

1

i

≤

≤

n

(2.9)

xi = φi0 +

Furthermore, we have

(2.10)

min

b,i

{

1

}

−

Xj=1

φijxi

−

j + ǫi + Oℓ2

(log b)τ

1b−

−

(τ

−

1.5)

.

(cid:16)

(cid:17)

xi −

x∗i = Oℓ2

(cid:16)

(log b)τ

1b−

−

(τ

−

1.5)

.

(cid:17)

6

Recall from convention in the end of Section 1 that the Oℓ2 notation means bounded in
the L2 norm. Note that the AR approximation error diminishes as b
if τ > 1.5.. Theo-
rem 2.5 demonstrates that every sufﬁciently short-range dependent and UPDC non-stationary
time series can be efﬁciently approximated by an AR process of slowly diverging order (c.f.
(2.10)). Furthermore, the approximation error is adaptive to the decay rate of the time series
covariance as well as the approximation order b.

→ ∞

REMARK 2.6. Our results can be easily extended to the case when the temporal de-
pendence is of exponential decay, i.e., (2.3) holds true. In this case and under the UPDC
condition, (2.5) can be updated to

φij| ≤
|

C max

n−
{

2, aj/2

, j > 1, C > 0 is some constant,
}

and the magnitude of the error bounds in equations (2.7), (2.9) and (2.10) can all be changed
to max

1, ab/2

.

b3/2/n2, n−
{

}

Before concluding this subsection, we provide an example of a general class of non-
stationary time series using their physical representations [43, 48] and illustrate how to check
the short range dependence assumption 2.2 for this class of non-stationary processes.

lowing form

(2.11)

EXAMPLE 2.7. For a non-stationary time series

xi,n}
{

, we assume that it has the fol-

xi,n = Gi,n(

Fi), i = 1, 2,

· · ·

, n,

, ηi

Fi := (
· · ·
1, 2,
{

Z are i.i.d. random variables and the sequence of func-
where
tions Gi,n :
Fi)
is a properly deﬁned random variable. The above representation is very general since any
n
i=1 can be represented in the form of (2.11) via the Rosen-
non-stationary time series
blatt transform [33].

1, ηi) and ηi, i
−
R∞ →
, n
xi,n}
{

∈
R are measurable such that for all 1

n, Gi0,n(

i0 ≤

} ×

· · ·

≤

Under the representation (2.11), temporal dependence can be quantiﬁed using physical
0, we deﬁne

be an i.i.d. copy of

. For j

dependence measures [43, 47, 49]. Let
the physical dependence measure of

ηi}
{

≥

η′i}
{
by
xi,n}
{
sup
δ(j, q) := sup
||
i

n

Gi,n(

F0)

Gi,n(

F0,j)

||q,

−

(2.12)

where

j, η
−
From Lemma S.6.7, Assumption 2.2 will be satisﬁed if

F0,j := (

1, η′
−

, η0).

j+1,

F−

· · ·

−

j

(2.13)

δ(j, 2)

≤

Cj−

τ .

Hence (2.2) can be directly checked by the physical dependence measures of a non-
stationary time series. For a more speciﬁc example, consider the following non-stationary
linear processes

xi,n =

∞

Xj=0

aij,nzi

j,

−

zi}
{

where
(2.13) is satisﬁed if supn supi |
refer the readers to [43] and [19, Section 2.1].

are i.i.d. random variables with ﬁnite variance. In this case, it is easy to see that
τ . For more examples in the form of (2.11), we

aij,n| ≤

Cj−

AR APPROXIMATIONS TO NON-STATIONARY TIME SERIES

7

2.2. AR approximation for locally stationary time series. From the discussion of Section
2.1, we have seen that every UPDC and sufﬁciently short-range dependent non-stationary
time series can be well approximated by an AR process with diverging order (c.f. (2.8) and
Theorem 2.5). However, from an estimation viewpoint, since we assume that only one re-
alization of the time series is observed, the Yule-Walker equations by which the AR coefﬁ-
cients (c.f. (2.4) or (2.6)) are governed are clearly underdetermined linear systems (i.e. there
are more unknown parameters than the number of equations). Therefore, additional con-
straints/assumptions on the non-stationary temporal dynamics have to be imposed in order
to estimate the AR approximation coefﬁcients consistently. In this section, we shall con-
sider an important subclass of non-stationary time series, the locally stationary time series
[12, 17, 18, 39, 48]. This class of non-stationary time series is characterized by assuming that
the underlying data generating mechanism evolves smoothly over time.

In this subsection, we will establish an AR approximation theory for locally stationary
time series under certain smoothness assumptions of their covariance structure. We start with
the following deﬁnition.

DEFINITION 2.8 (Locally stationary time series). A non-stationary time series
locally stationary time series (in covariance) if there exists a function γ(t, k) : [0, 1]
such that

is a
R

xi}
{
N
→
×

(2.14)

Cov(xi, xj) = γ(ti,

i
|

−

j

) + O
|

+ 1

i
|

−

j
|
n

, ti =

i
n

.

(cid:19)

(cid:18)

Moreover, we assume that γ is Lipschitz continuous in t and for any ﬁxed t
the autocovariance function of a stationary process.

∈

[0, 1], γ(t,

) is
·

. In particular, (2.14) means that the covariance structure of

Our Deﬁnition 2.8 only imposes a smoothness assumption on the covariance structure
of
in any small time
xi}
{
segment can be well approximated by that of a stationary process. Deﬁnition 2.8 covers
many locally stationary time series models used in the literature [12, 17, 18, 39, 48]. For
more discussions, we refer the readers to Example 2.13 below.

xi}
{

Equipped with Deﬁnition 2.8, we ﬁrst provide a necessary and sufﬁcient condition for the
UPDC assumption in the case of locally stationary time series. For stationary time series,
Herglotz’s theorem asserts that UPDC holds if the spectral density function is bounded from
below by a constant; see [5, Section 4.3] for more details. Our next proposition extends such
results to locally stationary time series with short-range dependence.

PROPOSITION 2.9.

If

and Deﬁnition 2.8, and there exists some constant κ > 0 such that f (t, ω)
ω, where

≥

xi}
{

is locally stationary time series satisfying Assumption 2.2
κ for all t and

(2.15)

f (t, ω) =

∞

Xj=
−∞

γ(t, j)e−

ijω, i = √

1,

−

satisﬁes UPDC in Assumption 2.1. Conversely, if

then
xi}
{
and 2.2 and Deﬁnition 2.8, then there exists some constant κ > 0, such that f (t, ω)
all t and ω.

satisﬁes Assumptions 2.1
κ for

xi}
{

≥

Note that f (t, w) is the local spectral density function. Proposition 2.9 implies that the
veriﬁcation of UPDC reduces to showing that the local spectral density function is uniformly
bounded from below by a constant, which can be easily checked for many non-stationary
processes. We refer the readers to Example 2.13 below for a demonstration.

8

Next, we establish an AR approximation theory for locally stationary time series. As men-
tioned earlier, we will impose some smoothness assumptions such that the AR approximation
coefﬁcients in (2.6) can be calculated consistently. Till the end of the paper, unless otherwise
speciﬁed, we shall impose the following Assumption 2.10, which states that the mean and
covariance functions of xi are d-times continuously differentiable, for some positive integer
d.

ASSUMPTION 2.10. For some given integer d > 0, we assume that there exists a smooth
C d([0, 1]), where C d([0, 1]) is the function space on [0, 1] of continuous func-

function µ(
)
·
tions that have continuous ﬁrst d derivatives, such that

∈

Moreover, we assume that γ(t, j)

Exi = µ(ti), ti =

i
n
C d([0, 1]) for any j

.

0.

≥

∈

We now proceed to state the AR approximation theory for locally stationary time series
Rb

(c.f. Theorem 2.11). We ﬁrst prepare some notation. Denote φ(t) := (φ1(t),
such that

, φb(t))∗ ∈

· · ·

(2.16)

where Γ(t)

Rb

×

∈

b and γ(t)

∈
Γij(t) = γ(t,

φ(t) = Γ(t)−

1γ(t),

Rb are deﬁned as

i
|

−

j

), γi(t) = γ(t, i), i, j = 1, 2,
|

· · ·

, b.

Here for any matrix A, Aij denotes its entry at the ith row and jth column. For a vector V , Vi
denotes its ith entry. As we will see in the proof of Theorem 2.11, Γ(t) is always invertible
under the UPDC assumption. With the above notation, we further deﬁne φ0(t) as

(2.17)

φ0(t) = µ(t)

Analogous to (2.8), denote

b

−

Xj=1

φj(t)µ(t).

x∗∗i =

xi,
φ0( i

n ) +

(

b
j=1 φj( i

n )x∗∗i
−

j + ǫi,

b;
i
≤
i > b.

THEOREM 2.11. Consider the locally stationary time series from Deﬁnition 2.8. Suppose

P

Assumptions 2.1, 2.2 and 2.10 hold true. Then we have that

Furthermore, there exists some constant C > 0, such that

φj(t)

∈

C d([0, 1]), 0

j

≤

≤

b.

(2.18)

max
b
j
1
≤
≤

Moreover, we have that

(2.19)

max
i>b

Finally, we have for b + 1

max
i>b

φij −
(cid:12)
(cid:12)
(cid:12)
(cid:12)
φi0 −
(cid:12)
(cid:12)
i
(cid:12)
(cid:12)

φ0(

≤

≤

n

b

φj(

i
n

i
n

≤

)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

C

(cid:18)

C

(log b)τ

1b−

−

(τ

3) +

−

(cid:18)

b2
n

.

(cid:19)

(log b)τ

1b−

−

(τ

3.5) +

−

b2.5
n

.

(cid:19)

(2.20)

i
n

) +

xi −

φ0(

(cid:16)

Xj=1

φj(

i
n

)xi

−

j + ǫi

= Oℓ2

(log b)τ

1b−

−

(τ

3.5) +

−

(cid:17)

(cid:18)

b2.5
n

,

(cid:19)

AR APPROXIMATIONS TO NON-STATIONARY TIME SERIES

9

and for all 1

n

i

≤

≤

(2.21)

xi −

x∗∗i = Oℓ2

(log b)τ

1b−

−

(τ

4) +

−

(cid:18)

b3
n

.

(cid:19)

Theorem 2.11 establishes that a locally stationary time series can be well approximated
by an AR process of smoothly time-varying coefﬁcients and a slowly diverging order under
mild conditions. In particular, the AR coefﬁcient functions φj(
) has the same degree of
·
Z. Observe that the smooth
smoothness as the time-varying covariance functions γ(
, k), k
·
) can be well approximated by models of small number of parameters using,
functions φj(
·
for example, the theory of basis function expansion or local Taylor expansion. Therefore
Theorem 2.11 implies that the approximating AR model x∗∗i can be consistently estimated
using various popular nonparametric methods such as the local polynomial regression or
the method of sieves provided that the underlying data generating mechanism is sufﬁciently
smooth and the temporal dependence is sufﬁciently weak.

∈

REMARK 2.12. As we can see from Theorem 2.11, the approximation error for the lo-
cally stationary AR approximation comprises of two parts. The ﬁrst part is the truncation
error, i.e., using an AR(b) approximation instead of an AR(n) approximation to
. This
xi}
{
part of the error is represented by the ﬁrst term on the right hand side of equations (2.18)
to (2.21). The second part is the error caused by using the smooth AR coefﬁcients φj(
) to
·
approximate φij. This part of the error is represented by the second term on the right hand
side of equations (2.18) to (2.21). In order to balance the aforementioned two types of errors,
an elementary calculation shows that the choice of b should satisfy that

(2.22)

b
log b

= O(n

1

τ −1 ).

Before concluding this subsection, we provide an example to illustrate two frequently-
used models of locally stationary time series in the literature and how the assumptions in this
subsection can be veriﬁed for those models.

EXAMPLE 2.13. We shall ﬁrst consider the locally stationary time series model in [48,
49] using a physical representation. Speciﬁcally, the authors deﬁne locally stationary time
series

as follows

xi}
{

(2.23)

xi = G(

i
n

,

Fi), i = 1, 2,

· · ·

, n,

×

R∞ →

R is a measurable function such that ξi(t) := G(t,

where G : [0, 1]
Fi) is a prop-
[0, 1]. In (2.23), by allowing the data generating
erly deﬁned random variable for all t
mechanism G depending on the time index t in such a way that G(t,
Fi) changes smoothly
with respect to t, one has local stationarity in the sense that the subsequence
xi, ..., xi+j
1}
{
is approximately stationary if its length j is sufﬁciently small compared to n. Analogous to
(2.12), one can deﬁne the physical dependence measure for (2.23) as follows

∈

−

(2.24)

δ(j, q) := sup

G(t,

[0,1] ||
t
∈

F0)

−

G(t,

F0,j )

||q.

Moreover, the following assumptions are needed to ensure local stationarity.

ASSUMPTION 2.14. G(
,
·

) deﬁned in (2.23) satisﬁes the property of stochastic Lipschitz
·

continuity, i.e., for some q > 2 and C > 0,

(2.25)

G(t1,

||

Fi)

−

G(t2,

Fi)

||q ≤

C

t1 −
|

,
t2|

10

where t1, t2 ∈
(2.26)

[0, 1]. Furthermore,

sup
[0,1]

t
∈

G(t,

max
i
1
≤
≤

n ||

Fi)

||q <

.

∞

It can be shown that time series

xi}
{
2.14 satisﬁes Deﬁnition 2.8. In particular, for each ﬁxed t
can be found easily using the following

with physical representation (2.23) and Assumption
[0, 1], γ(t, j) in Deﬁnition 2.8

∈

(2.27)

γ(t, j) = Cov(G(t,

F0), G(t,

Fj)).

Note that the assumptions (2.25) and (2.26) ensure that γ(t, j) is Lipschiz continuous in t.
Moreover, for each ﬁxed t, γ(t,
, which is a
)
}
·
stationary process.

) is the autocovariance function of
·

G(t,
{

The physical representation form (2.23) includes many commonly used locally stationary
be zero-mean i.i.d. random variables with variance

time series models. For example, let
σ2. We also assume aj(
), j = 0, 1,
·

zi}
{
be C d([0, 1]) functions such that
· · ·

(2.28)

G(t,

Fi) =

∞

ak(t)zi

k.

−

Xk=0
(2.28) is a locally stationary linear process. It is easy to see that Assumptions 2.2, 2.10 and
2.14 will be satisﬁed if

sup
[0,1] |
t
∈

aj(t)

| ≤

Cj−

τ , j

1;

≥

∞

Xj=0

sup
[0,1] |
t
∈

a′j(t)
|

<

,

∞

and

a(d)
j (t)

Cj−

τ , j

1.

≥

| ≤

sup
[0,1] |
t
∈

Furthermore, we note that the local spectral density function of (2.28) can be written as

f (t, w) = σ2

ψ(t, e−
|

2,
ijω)
|

) is deﬁned such that G(t,
·

where ψ(
Fi) = ψ(t, B)zi with B being the backshift operator.
,
·
By Proposition 2.9, the UPDC is satisﬁed if
κ for all t and ω, where κ > 0
ψ(t, e−
|
is some universal constant. For more examples of locally stationary time series in the form of
(2.23), we refer the readers to [43] and [19, Section 2.1].

2
ijω)
|

≥

For a second example, note that in [17, 39], the locally stationary time series is deﬁned as
is locally stationary time series if for each scaled

follows (see Deﬁnition 2.1 of [39]).
time point u

xi}
{
[0, 1], there exists a strictly stationary process

hi(u)
}
{

such that

xi −
|

hi(u)

| ≤

ti −
|
(cid:18)

+

u
|

1
n

Ui(u), a.s,

(cid:19)
Lq([0, 1]) for some q > 0. By similar arguments as those of model (2.23),
where Ui(u)
Deﬁnition 2.8 as well as assumptions of this subsection can be veriﬁed for (2.29). We shall
omit the details here.

∈

∈

(2.29)

AR APPROXIMATIONS TO NON-STATIONARY TIME SERIES

11

L

3. A Test of Stability for AR Approximations.

In this section, we study a class of
statistical inference problems for the AR approximation of locally stationary time series using
2 test. We point out here that, thanks to the AR approximation (2.20), a
a high dimensional
wide class of hypotheses on the structure of φj(
) can be performed using the aforementioned
·
testing procedure. On the other hand, For the sake of brevity, in this paper we concentrate on
the test of stability of the AR approximating coefﬁcients with respect to time for locally
stationary time series (c.f. (3.2)). The latter is an important problem as in practice one is
usually interested in checking whether the time series can be well approximated by an AR
process with time-invariant coefﬁcients.

In order to theoretically investigate the test, time series dependence measures should be
deﬁned and controlled for the locally stationary time series
. Throughout this section, we
assume that the locally stationary time series admits the representation as in (2.23) equipped
with physical dependence measures (2.24). On the other hand, note that in Section 2.2, all
our AR approximation results are established under smoothness and fast decay assumptions
of the covariance structure of
without the need of any speciﬁc time series dependence
measures. Therefore, we believe that the theoretical results of this section can be easily estab-
lished using other measures of time series dependence such as the strong mixing conditions.
For the sake of brevity, we shall concentrate on establishing results using the physical depen-
dence measures in this paper.

xi}
{

xi}
{

3.1. Problem setup and test statistics.

In this subsection, we formally state the hypoth-
esis testing problems and propose our statistics based on nonparametric sieve estimators of
.
)
φj (
}
·
{
) is related to the trend of the time series and in many real applications the trend
Since φ0(
·
is removed via differencing or subtraction, we focus our discussion on the test the stability of
1. For the test of stability including the trend, we refer the readers to Remark 3.2.
φj(
), j
·
Formally, the null hypothesis we would like to test is

≥

H0 : φj(
) is a constant function on [0, 1], for all j
·

≥

1.

diverges to inﬁnity at the rate such that

e

Let b

∗

(3.1)

b
∗
log b

∗

1

τ −1 .

n

≍

By Remark 2.12, the AR approximation for locally stationary time series at order b
achieves
∗
the smallest error. According to (2.5) and (2.18), when τ is sufﬁciently large, we have that
) for j > b
. Therefore, from an inferential viewpoint, φj(
supt |
·
∗
can be effectively treated as zero. Together with the approximation (2.20), it sufﬁces for us
to test

1/2) for j > b
∗

φj(t)
|

= o(n−

H0 : φj(
(3.2)
) is a constant function on [0, 1], j = 1, 2,
·
Before providing the test statistic for H0, we shall ﬁrst investigate the interesting insight that
H0 is asymptotically equivalent to testing whether
n
i=1 is correlation stationary, i.e., there
exists some function ̺ such that

xi}
{

, b
∗

· · ·

.

(3.3)

H′0 : Corr(xi, xj) = ̺(
i
|

j

),
|

−

where Corr(xi, xj) stands for the correlation between xi and xj. We formalize the above
statements in Theorem 3.1 below.

12

THEOREM 3.1.

Suppose Assumptions 2.1, 2.2, 2.10 and 2.14 hold true. For j

one hand, when H′0 holds true, there exists some constants φj such that

, on

b

∗

≤

(3.4)

φj(

i
n

) = φj + O

(log b

)τ

−

1b−
∗

∗

(τ

3)

−

+

(cid:18)

b2
∗
n

.

(cid:19)

On the other hand, when H0 holds true that φj( i
smooth function ̺, such that

n ) = φj, j = 1, 2,

, b
∗

· · ·

, there exists some

(3.5)

i
Corr(xi, xi+j) = ̺(
|

j

) + O
|

−

b
∗
n

(cid:18)

+ (log b
∗

)τ

−

1b−
∗

(τ

2)

−

.

(cid:19)

Note that the right hand sides of (3.4) and (3.5) are of the order o(n−

1/2) if τ is sufﬁciently
large. Hence Theorem 3.1 establishes the asymptotic equivalence between H′0 and H0 for
short range dependent locally stationary time series.

For the rest of this subsection, we shall propose a test statistic for H0 in (3.2). We start
, b, for a generic order b.
C d([0, 1]), it is natural for us to approximate it via a ﬁnite and diverging term

with the estimation of the coefﬁcient functions φj(
), j = 0, 1, 2,
·
Since φj(t)
basis expansion (method of sieves [9]). Speciﬁcally, by [9, Section 2.3], we have that

· · ·

∈

(3.6)

φj(

i
n

c

) =

ajkαk(

i
n

) + O(c−

d), 0

j

≤

≤

b, i > b,

Xk=1

αk(t)
}
{

where
are some pre-chosen basis functions on [0, 1] and c is the number of basis
functions. For the ease of discussion, throughout this section, we assume that c is of the
following form

(3.7)

c = O(na), 0 < a < 1.

Moreover, for the reader’s convenience, in Section S.7, we collect the commonly used basis
functions.

In view of (3.6), we need to estimate the ajk’s in order to get an estimation for φj(t). For

i > b, by (2.20), write

b

c

(3.8)

xi =

ajkzkj + ǫi + Oℓ2

(log b)τ

1b−

−

(τ

3.5) +

−

Xj=0

Xk=1

(cid:18)
j for j

zkj(i/n) := αk(i/n)xi

1 and zk0 = αk(i/n). By (3.8), we can esti-
where zkj ≡
≥
mate all the a′jks using only one ordinary least squares (OLS) regression with a diverging
, c as a
number of predictors. In particular, we write all ajk, j = 0, 1, 2
R(b+1)c, then the OLS estimator for β can be written as
1Y ∗x, where
vector β
∈
b and Y is the design matrix. After estimating a′jks, φj(i/n) is
x = (xb+1,
, xn)∗ ∈
· · ·
estimated using (3.6) as

· · ·
β = (Y ∗Y )−

, b, k = 1, 2,

Rn

· · ·

−

−

b

(3.9)

φj(

i
n

) =

β∗Bj(

i
n

),

where Bj(i/n) := Bj,b(i/n)
, αc(i/n))∗ ∈
(α1(i/n),
the following statistic in terms of (3.9)

· · ·

∈

b

Rc, j = 0, 1, 2,

With the estimation (3.9), we proceed to provide the

· · ·

L

b
, b, and zeros otherwise.

R(b+1)c has (b + 1) blocks and the j-th block is B( i

2 test statistic. To test H0, we use

n ) =

(3.10)

T =

b∗

1

0
Xj=1 Z

φj(t)
(

−

φj)2dt,

φj =

1

0
Z

φj(t)dt.

b

b

b

b

b2.5
n

+ bc−

d

,

(cid:19)

AR APPROXIMATIONS TO NON-STATIONARY TIME SERIES

13

The heuristic behind T is that H0 is equivalent to φj(t) = φj for j = 1, 2,
2 test statistic T should be small under the null.
φj =

, b
∗

· · ·

, where

1
0 φj(t)dt. Hence the
R

L

REMARK 3.2. We remark that in some cases practitioners and researchers may be inter-
) do
ested in testing whether all optimal forecast coefﬁcient functions including the trend φj(
·
not change over time. That is equivalent to testing whether both the trend and the correlation
structure of the time series stay constant over time. In this case, one will test

(3.11)
Similar to (3.10), for the test of H0,g, we shall use

H0,g : φj(
) is a constant function on [0, 1], j = 0, 1,
·

.

, b
∗

· · ·

(3.12)

Tg =

b∗

1

0

Xj=0 Z

φj(t)
(

−

φj)2dt,

φj =

1

0

Z

φj(t)dt.

b

b

b

b

3.2. High dimensional Gaussian approximation and asymptotic normality.

In this sub-
section, we prove the asymptotic normality of the statistic T. The key ingredient is to establish
Gaussian approximation results for quadratic forms of high dimensional locally stationary
time series.

We ﬁrst show that the study of the statistic T reduces to the investigation of a weighted
quadratic form of high dimensional locally stationary time series. We prepare some notation.
¯B ¯B∗. Let W be a (b
Denote ¯B =
+ 1)c dimensional
(b
∗
∗
diagonal block matrix with diagonal block W and Ib∗c be a (b
+ 1)c dimensional
(b
∗
diagonal matrix whose non-zero entries are ones and in the lower b
c major part. Recall
b
xi = (1, xi

B(t)dt and W = I

+ 1)c
+ 1)c
c

b∗)∗ and set

×
×
×

, xi

1
0

1,

−

R

∗

∗

∗

−

· · ·

−

(3.13)

p = (b
∗

+ 1)c.

Recall ǫi in (2.4). We denote the sequence of p-dimensional vectors zi by

(3.14)

zi = hi ⊗

B(

i
n

)

∈

Rp, hi = xiǫi,

where
a locally stationary time series. For notational convenience, we denote

is the Kronecker product. We point out that when i > b

⊗

∗

, it is easy to see that hi is

(3.15)

hi = U(

Recall (2.27). We also denote the b

ij (t)) such that

,

i
Fi), i > b
n
matrix Σb∗(t) = (Σb∗

∗

.

b
∗ ×
∗
ij (t) = γ(t,

Σb∗

i
|

j

).
|

−

LEMMA 3.3. Denote X = 1
√n

where

n
i=b∗+1 z∗i , and the p

×

p matrix Γ by Γ = Σ−

1Ib∗cWΣ−

1

,

(3.16)

Σ =

P

, Σ =

Ic 0
0 Σ

(cid:18)

(cid:19)

Z

1

0

Σb∗(t)

⊗

(B(t)B∗(t))dt.

Suppose Assumptions 2.1, 2.10, 2.14 and S.3.1 hold true. Moreover, we assume that the phys-
ical dependence measure δ(j, q), q > 2, in (2.24) satisﬁes

(3.17)

δ(j, q)

Cj−

τ , j

1,

≥

≤

for some constant C > 0 and τ > 4.5 + ̟, where ̟ > 0 is some ﬁxed small constant. Then
for c in the form of (3.7) and b
∗

satisfying (3.1), when n is sufﬁciently large, we have that

(3.18)

nT = X∗ΓX + oP(1).

14

Based on Lemma 3.3, for the purpose of statistical inference, it sufﬁces to establish
the distribution of X∗ΓX, which is a high dimensional quadratic form of
since p
is divergent as n
. To this end, we shall establish a Gaussian approximation result
for the latter quadratic form. Speciﬁcally, choose a sequence of centered Gaussian ran-
n
i=b∗+1 and deﬁne
dom vectors
gi = vi ⊗

→ ∞
n
i=b∗+1 which preserves the covariance structure of

vi}
{
B( i
n ). Denote

hi}
{

zi}
{

Y =

1
√n

n

g∗i .

Xi=b∗+1

We shall establish a Gaussian approximation result by controlling the Kolmogorov distance

(3.19)

(X, Y) = sup
R
∈

x

K

P

X∗ΓX

x

≤

−

(cid:17)

P

Y∗ΓY

(cid:16)

≤

(cid:12)
(cid:12)
(cid:12)
and a small constant δ > 0, such that

(cid:16)

x

.

(cid:17)(cid:12)
(cid:12)
(cid:12)

THEOREM 3.4. Under the assumptions of Lemma 3.3, there exist some constant C > 0

Θ(Mz, M, p, ξc, τ, q, δ) is deﬁned as

(X, Y)

K

CΘ.

≤

Here Θ

≡

Θ(Mz, M, p, ξc, τ, q) := log n

ξc
Mz

+ p

7

4 n−

1/2M 3

z M 2 + M

−qτ +1

2q+1 ξ(q+1)/(2q+1)

c

q+1

2q+1 n

p

δq
2q+1

(3.20)

where Mz, M

→ ∞

when n

→ ∞

+ p1/2ξ1/2

c

(pξcM −

τ +1 + pξ2

c nM −
z

(q

2)

)

−

1/2

+ n−

δ,

(cid:17)
, p is deﬁned in (3.13), q > 2 is from (2.25) and

(cid:16)

(3.21)

ξc := sup
c
1
≤

≤

i

sup
[0,1]

t
∈

.

αi(t)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

REMARK 3.5. We remark that ξc in (3.21) can be well controlled for many commonly
used basis functions. For instance, ξc = O(1) for the Fourier basis and the normalized orthog-
onal polynomials and ξc = O(√c) for orthogonal wavelet; see Section S.7 for more details.
Moreover, it is easy to see that the approximation rate in Theorem 3.4 converges to 0 under
mild conditions. In particular, when τ > 0 is large enough, ξc = O(1) and q > 2 is sufﬁ-
ciently large, the dimension p can be as large as O(n2/7
δ1) in order for the approximation
rate to vanish, where δ1 > 0 is a sufﬁciently small constant. This is, to date, the best known
dimension setting for high dimensional convex Gaussian approximation [22].

−

By Theorem 3.4, the asymptotic normality of nT can be readily obtained as in Proposition

3.6 below. Denote the long-run covariance matrix for

at time t as

(3.22)

Ω(t) =

∞

Cov

U(t,

Xj=
−∞

(cid:16)

1
and the aggregated covariance matrix as Ω =
0 Ω(t)
zi}
as the integrated long-run covariance matrix of
R
{
Tr[Ω1/2ΓΩ1/2]k

(3.23)

fk =

) is the trace of the given matrix.
where Tr(
·

(cid:16)

hi}
{
F0), U(t,

,

Fj )
(cid:17)
B(t)B∗(t)
⊗
. For k
(cid:17)
(cid:16)

∈

1/k

,

(cid:17)

dt. Ω can be regarded

N and Γ in (3.18), we deﬁne

AR APPROXIMATIONS TO NON-STATIONARY TIME SERIES

15

PROPOSITION 3.6. Under the assumptions of Lemma 3.3, assuming that Θ in (3.20)

satisfying Θ = o(1), when H0 in (3.2) holds true, we have

nT

f1

−
f2 ⇒ N

(0, 2).

Next, we discuss the power of the test under a class of local alternatives. For a given α, set

∞

1

Ha :

φj(t)

¯φj

−

dt > Cα

c

,

√b
∗
n

2

(cid:17)

0

(cid:16)

Xj=1 Z
0 φj(t)dt and Cα ≡

1

Cα(n)

→ ∞

as n

.

→ ∞

where ¯φj =

R

PROPOSITION 3.7. Under the assumptions of Lemma 3.3, assuming that Θ in (3.20)

satisfying Θ = o(1), when Ha holds true, we have

nT

n

f1 −

−

∞j=1

1
0
f2
R

P

φj(t)

−

(cid:16)

¯φj

2

dt

(cid:17)

⇒ N

(0, 2).

Consequently, under Ha, the power of our test will asymptotically be 1, i.e.,
f1

nT

−
f2

√2

≥

α

Z1
−

→

1, n

,

→ ∞

(cid:17)

α)th quantile of the standard Gaussian distribution.

P

(cid:16) (cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

where

Z1
−

α is the (1

−

Proposition 3.7 implies that our test T can detect local alternatives when the

2 distance
L
p1/4/√n. Observe
c)1/4/√n
n2/7. Therefore p1/4/√n converges to 0 faster than

) dominates (b
∗

· · ·

≍

between (φ1(t), φ2(t),
that Proposition 3.6 requires that p
n−

3/7.

· · ·

) and ( ¯φ1, ¯φ2,

≪

REMARK 3.8.

In this remark, we discuss how to deal with Tg in (3.12). By a discussion

similar to Lemma 3.3, Tg can also be expressed as a quadratic form
1WΣ−

nTg = X∗ΓgX + oP(1), Γg = Σ−

1

,

where we recall (3.16). Consequently, the only difference lies in the deterministic weight
matrix of the quadratic form. By Theorem 3.4, we can prove similar results to nTg as in
Propositions 3.6 and 3.7. We omit further details.

3.3. Multiplier bootstrap procedure.

In this subsection, we propose a practical procedure

to implement the stability test based on a multiplier bootstrap procedure.

On one hand, it is difﬁcult to directly use Proposition 3.6 to carry out the stability test since
the quantities f1 and f2 rely on Ω which is hard to estimate in general. On the other hand,
the high-dimensional Gaussian quadratic form Y∗ΓY converges at a slow rate. To overcome
these difﬁculties, we extend the strategy of [47] and use a high-dimensional mulitplier boot-
strap statistic to mimic the distributions of nT. Note that (3.18) can be explicitly written
as

(3.24)

nT =

1
√n

(cid:16)

n

z∗i

Γ

Xi=b∗+1

(cid:17)

(cid:16)

1
√n

n

zi

+ oP(1).

Xi=b∗+1

(cid:17)

16

Recall (3.14). For some positive integer m, denote

(3.25)

Φ =

√n

m

+ 1√m

1
b

m

n

−

i+m

hj

B(

i
n

⊗

)

Ri,

−

∗
m, are i.i.d. standard Gaussian random variables. Φ is an
where Ri, i = b
∗
important statistic since the covariance of Φ is close to Ω conditional on the data; see (S.39)
for a more precise statement.

+ 1,

· · ·

, n

−

(cid:17)i

(cid:17)

(cid:16)

Xi=b∗+1 h(cid:16)

Xj=i

−

Since
residuals

hi}
{

is based on

ǫi}
{

which cannot be observed directly, we shall instead use the

(3.26)

ǫb∗
i

:= xi −

φ0(

i
n

)

−

b∗

φj(

i
n

)xi

j.

−

Xj=1
ǫi}
{

b
with

ǫb∗
i }
{

hi}
Denote
{
as in (3.25) using
b

similarly as in (3.14) by replacing

Φ
. With the above notations, we denote the bootstrap quadratic form as
b

. Accordingly, we denote

b

b

b

(3.27)

where

Γ :=

Σ = 1
In Theorem 3.9 below, we prove that the conditional distribution of

1 with

Σ−

Σ−

b

b

Γ is a consistent estimator of Γ.

Γ

Φ,

:=

Φ∗
T
n Y ∗Y. Note that
b

b

hi}
{
b
1Ib∗cW

b
nT asymptotically. Denote

b

b

b

b

(3.28)

ζc := sup

B(t)
.
k

t k

can mimic that of

T

b

THEOREM 3.9.

Suppose the assumptions of Lemma 3.3 hold and

b

1/2

(3.29)

ζ 2
c c−

m
n
Furthermore, we assume that Assumption 2.14 holds with q > 4. When H0 holds true, there
An, we have that condi-
exists some set
tional on the data

n
i=b∗+1, assuming that Θ in (3.20) satisfying Θ = o(1),

p
An such that P(
xi}
{

o(1) and under the event

An) = 1

= o(1).

(cid:16)r

1
m

+

−

(cid:17)

∗

P

sup
R (cid:12)
x
∈
(cid:12)
(cid:12)
(cid:12)
(cid:12)

f1
T −
√2f2 ≤
b

x

! −

P (Ψ

x)

≤

= o(1),

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

where Ψ

∼ N

(0, 1) is a standard normal random variable.

REMARK 3.10. First, ζc can be well controlled by the commonly used sieve basis func-
tions. For example, we have ζc = O(√c) for the Fourier basis and orthogonal wavelets, and
ζc = O(c) for the Legendre polynomials; see Section S.7 for more details. Second, in the
scenario where ζc = O(√c), (3.29) is equivalent to

√p

m
n

+

1
m

= o(1).

(cid:16)r
Hence, in the optimal case when m = O(n1/3), we are allowed to choose p
n2/3 if ζc =
O(√c). In this regime, Theorem 3.4 still holds true. Third, for the detailed construction of
An, we refer the reader to (S.40). Finally, a theoretical discussion of the accuracy of the
bootstrap can be found in Section S.3 and the choices of the hyperparameters m, c, b
, are
discussed in Section S.5.

≪

(cid:17)

∗

Based on Theorem 3.9, we can use the following Algorithm 1 for practical implementation

to calculate the p-value of the stability test.

 
AR APPROXIMATIONS TO NON-STATIONARY TIME SERIES

17

Algorithm 1 Multiplier Bootstrap

1 and the residuals

, c and m chosen by the data-drive procedure demonstrated in
∗
, and sieve basis functions.

Inputs: tuning parameters b
Section S.5, time series
xi}
{
1 using n(Y ∗Y )−
Step one: Compute
Σ−
(3.26). Step two: Generate B (say 1000) i.i.d. copies of
Tk, k = 1, 2,
· · ·
Step three: Let
b
H0 at the level α if nT >
b
equal to x. Let B∗ = max
Output: p-value of the test can be computed as 1

ǫb∗
n
i=b∗+1 according to
i }
{
B
Φ(k)
k=1. Compute
}
{
b
T(B) be the order statistics of
T(2) ≤ · · · ≤
), where
α)
B(1
T(
⌋
⌋
⌊
b
.
nT
r :
}
{
b

Tk, k = 1, 2,
b

denotes the largest integer smaller or

, B, correspondingly as in (3.27).

−
Tr ≤

T(1) ≤

x
⌊

· · ·

b

b

b

, B. Reject

B∗
B .

−

b

4. Applications to globally optimal forecasting.

In this section, independent of Section
3, we discuss an application of our AR approximation theory in optimal global forecasting
for locally stationary time series. We ﬁrst introduce the notion of asymptotically optimal
predictor.

DEFINITION 4.1. A linear predictor

z of a random variable z based on x1,

called asymptotically optimal if

(4.1)

E(z

e
z)2

−

≤

σ2
n + o(1),

, xn, is

· · ·

and the predictor is called strongly asymptotically optimal if

(4.2)

where σ2
x1,

, xn.

· · ·

E(z

−

σ2
n + o(1/n),

≤

e
z)2

e

n is the mean squared error (MSE) of the best linear predictor of z based on

The rationale for the deﬁnition of strong asymptotic optimality is that, in practice, the
MSE of forecast can only be estimated with a smallest possible error of O(1/n) when the
time series length is n. Speciﬁcally, it is well-known that the parametric rate for estimating
1/2). When one uses the estimated coefﬁcients
the coefﬁcients of a time series model is O(n−
to forecast the future, the corresponding inﬂuence on the MSE of forecast is O(1/n) (at
best). Therefore, if a linear predictor achieves an MSE of forecast within o(1/n) range of the
optimal one, it is practically indistinguishable from the optimal predictor asymptotically.

In what follows, we shall focus on the discussion of one-step ahead prediction. The general
case can be handled similarly. In order to make the forecasting feasible, we assume that
the smooth data generating mechanism extends to time n + 1. That is, we assume that the
satisﬁes the locally stationary assumptions imposed in the paper.
time series
, xn+1}
Naturally, we propose the following estimate for ˆxn+1, the best linear predictor of xn+1 based
on its predecessors xn, xn

x1,
{

, x1,

· · ·

1,

−

· · ·

(4.3)

xb
n+1 = φ0(1) +

b

Xj=1

φj(1)xn+1

−

j, n > b.

b

b+1 (instead of
Observe that (4.3) is a truncated linear predictor where xn, xn
, x1) is used to forecast xn+1. Note that here b is a generic order which may be dif-
xn,
xb
ferent from the order b
n+1 is an
asymptotic optimal predictor satisfying (4.1) or (4.2) in Deﬁnition 4.1 under mild conditions.

used in the test of stability. The next theorem shows that

, xn

· · ·

· · ·

1,

−

−

∗

b

18

THEOREM 4.2.

Suppose Assumptions 2.1, 2.2, 2.10 and 2.14 hold true. Then for sufﬁ-

ciently large n,

(4.4)

E(xn+1 −

xb
n+1)2

E(xn+1 −

≤

xn+1)2 + O

(log b)τ

1b−

−

(τ

3.5) +

−

 (cid:18)

b2.5
n

2

.

!

(cid:19)

b

b

It is easy to see that the order b which minimizes the right hand side of (4.4) is of the same
order as that in (2.22). When n is sufﬁciently large, the corresponding error on the right-hand
side of (4.4) equals

Hence Theorem 4.2 states that the estimator (4.3) is an asymptotic optimal one-step ahead
forecast if τ > 3.5 and it is asymptotically strongly optimal if τ > 6.

O(n−

2+ 5

τ −1 ).

Theorem 4.2 veriﬁes the asymptotic global optimality of truncated linear predictors for
locally stationary time series under mild conditions. For general stationary processes, [1] and
[2], among others, established profound theory on the decay rate of the AR approximation
coefﬁcients as well as the magnitude of the truncation error. As we mentioned in the Intro-
duction, those results are derived using sophisticated spectral domain techniques which are
difﬁcult to extend to non-stationary processes. In this section, using the AR approximation
theory established in this paper, we are able to establish a global optimal forecasting theory
for the truncated linear predictors for a general class of locally stationary processes.
In practice, one needs to estimate the optimal forecast coefﬁcients φj(1), j = 0,
, b as
well as the MSE of the forecast E(xn+1 −
n+1)2. To investigate the estimation accuracy of
xb
those parameters, we need to impose certain dependence measures for the series. Therefore,
for the rest of this subsection, we shall focus on the physical representation as well as depen-
dence measures as in (2.23) and (2.24). To obtain an estimation for the predictor, in light of
(3.9), based on (4.3), we shall estimate

xb
n+1, or equivalently, forecast xn+1 using

· · ·

b

(4.5)

xb
n+1 =

b
φ0(1) +

b

φj(1)xn+1

j .

−

Xj=1
b
Next, we discuss the estimation of the MSE of the forecast. Denote the series of estimated
b
ǫb
forecast error
by
i := xi −
j=1
σ2
. Recall the deﬁnition of ǫi in (3.8). According to [19, Lemma 3.11], we ﬁnd that there
i }
{
exists a smooth function ϕ(
)
·

C d([0, 1]) such that for some constant C > 0,

j and the variance of

φj(i/n)xi

φ0(i/n)

ǫb
i }
{

ǫi}
{

P

by

−

∈

b

b

b

b

b

b

−

(4.6)

sup
i>b |

σ2
i −

ϕ(

i
n

C

)
| ≤

(log b)τ

1b−

−

(τ

3.5) +

−

(cid:18)

b2.5
n

.

(cid:19)

Therefore the estimation of σ2

i reduces to the estimation of the smooth function ϕ as
the estimation error of (4.6) is sufﬁciently small for appropriately chosen b. Similar to the
estimation of the smooth AR approximation coefﬁcients, one can again use the method of
). Speciﬁcally, similar to (3.6), write
sieves to estimate the smooth function ϕ(
·

ϕ(

i
n

) =

bkαk(

i
n

) + O(c−

d).

c

Xk=1

Furthermore, by equation (3.14) of [19], we write

c

i )2 =
ǫb
(

bkαk(

Xk=1

b

i
n

) + νi + OP

b(ζc

(cid:16)

log n
√n

+ c−

, i > b,

d)
(cid:17)

· · ·

b

and use

AR APPROXIMATIONS TO NON-STATIONARY TIME SERIES

19

νi}
{
n ), k = 1,

where
2.1, 2.2, 2.10 and 2.14. Consequently, we can use an OLS with (
αk( i
bk, k = 1, 2,

is a centered sequence of locally stationary time series satisfying Assumptions
i )2 being the response and
ǫb
, c, being the explanatory variables to estimate bk’s, which are denoted as

, c. Finally, we estimate

· · ·

b

c

ϕ(i/n) =

bkαk(i/n)

ϕ(1) to estimate the MSE of the forecast. We now state the asymptotic behaviour

b

Xk=1

b

of the MSE of (4.5) in Theorem 4.3 below. Recall (3.28).

THEOREM 4.3.

b

Suppose Assumptions 2.1, 2.2, 2.10, 2.14 and (1) and (2) of Assumption

S.3.1 hold true. We have

σ2
n+1 −

(cid:12)
(cid:12)

ϕ(1)

= OP

b(ζc

(cid:16)

(cid:12)
(cid:12)

log n
n

r

+ c−

d) + (log b)τ

1b−

−

(τ

3.5) +

−

b2.5
n

.

(cid:17)

b

We point out that the error term on the right-hand side of the above equation vanishes
asymptotically under mild conditions. For instance, assuming that d is sufﬁciently large, b
(for example, as in (2.22)) and the temporal dependence de-
slowly diverges as n
cays fast enough (i.e. τ is some large constant), the leading error term in Theorem 4.3 is
log n/n. Moreover, if we assume exponential decay of temporal dependence as in Re-
bζc
) is inﬁnitely differentiable, then the error almost achieves the parametric
mark 2.6 and φ(
·
n−

p
1/2 rate except a factor of logarithm.

→ ∞

5. Simulation studies.

In this section, we perform a small Monte Carlo simulation to
study the ﬁnite-sample accuracy and power of the multiplier bootstrap Algorithm 1 for the
test of stability of AR approximation coefﬁcients and compare it with some existing methods
on testing covariance stationarity in the literature.

5.1. Simulation setup. We consider four different types of non-stationary time series
models: two linear time series models, a two-regime model, a Markov switching model and
a bilinear model.

1. Linear AR model: Consider the following time-varying AR(2) model

2

xi =

aj(

i
n

−

)xi

j + ǫi, ǫi =

0.4 + 0.4

sin(2π

)

)ηi,

where ηi, i = 1, 2,
when we ﬁnish introducing the models. It is elementary to see that when aj( i
are constants, the prediction is stable.

Xj=1
, n, are i.i.d. random variables whose distributions will be speciﬁed
n ), j = 1, 2,

· · ·

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:16)

2. Linear MA model: Consider the following time-varying MA(2) model

2

xi =

aj(

i
n

)ǫi

−

Xj=1

j + ǫi, ǫi =

0.4 + 0.4

sin(2π

)

)ηi.

(cid:16)

(cid:12)
(cid:12)
(cid:12)

3. Two-regime model: Consider the following self-exciting threshold auto-regressive (SE-

i
n

i
n

(cid:12)
(cid:12)
(cid:12)

TAR) model [21, 38]

xi =

a1( i
a2( i

n )xi
n )xi

(

1 + ǫi, xi
1 + ǫi, xi

0,
1 ≥
1 < 0.

−

−

−

−

It is easy to check that the SETAR model is stable if aj( i
bounded by one.

ǫi =

0.4 + 0.4

sin(2π

)

)ηi.

i
n

(cid:16)

(cid:12)
(cid:12)
n ), j = 1, 2, are constants and
(cid:12)

(cid:12)
(cid:12)
(cid:12)

20

4. Markov two-regime switching model: Consider the following Markov switching AR(1)

model

xi =

a1( i
a2( i

n )xi
n )xi

(

−

1 + ǫi, si = 0,
1 + ǫi, si = 1.

ǫi =

0.4 + 0.4

sin(2π

)

)ηi,

i
n

−

(cid:12)
(cid:12)
where the unobserved state variable si is a discrete Markov chain taking values 0 and 1,
(cid:12)
with transition probabilities p00 = 2
3 , p10 = p11 = 1
2 . It is easy to check that the
above model is stable if the functions aj(
), j = 1, 2, are constants and bounded by one
·
[32]. In the simulations, the initial state is chosen to be 1.
5. Simple bilinear model: Consider the ﬁrst order bilinear model

3 , p01 = 1

(cid:12)
(cid:12)
(cid:12)

(cid:16)

i
n

i
n

xi =

a1(

)ǫi

1 + a2(

)

xi

1 + ǫi, ǫi =

0.4 + 0.4

sin(2π

)

)ηi.

−
It is known from [21] that when the functions aj(
), j = 1, 2, are constants and bounded
·
by one, xi has an ARMA representation and hence stable.

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:16)

(cid:16)

(cid:17)

−

i
n

In the simulations below, we record our results based on 1,000 repetitions and for Algo-
, we set
rithm 1, we choose B = 1, 000. For the choices of random variables ηi, i = 1, 2,
ηi to be student-t distribution with degree of 5, i.e., t(5), for models 1-2 and standard normal
random variables for models 3-5.

· · ·

5.2. Accuracy and power of the stability test.

In this subsection, we study the perfor-
mance of the proposed test (3.2). First, we study the ﬁnite sample accuracy of our test under
the null hypothesis that

(5.1)

a1(

i
n

) = a2(

i
n

)

≡

0.4.

Observe that the simulated time series are not covariance stationary as the marginal variances
change smoothly over time. We choose the values of b
, c and m according to the methods
described in Section S.5. It can be seen from Table 1 that our bootstrap testing procedure is
reasonably accurate for all three types of sieve basis functions even for a smaller sample size
n = 256.

∗

Second, we study the power of the tests and report the results in Table 2 when the un-
derlying time series is not correlation stationary, i.e., the AR approximation coefﬁcients are
time-varying. Speciﬁcally, we use

(5.2)

a1(

i
n

)

≡

0.4, a2(

i
n

) = 0.2 + δ sin(2π

i
n

), δ > 0 is some constant,

for the models 1-5 in Section 5.1. It can be seen that the simulated powers are reasonably
good even for smaller values of δ and sample sized, and the results will be improved when
δ and the sample size increase. Additionally, the power performances of the three types of
sieve basis functions are similar in general.

5.3. Comparison with tests for covariance stationarity.

In this subsection, we compare
2 distance
our method with some existing works on the tests of covariance stationarity: the
method in [17], the discrete Fourier transform method in [20] and the Haar wavelet peri-
odogram method in [29]. The ﬁrst method is easy to implement; for the second method, we
use the codes from the author’s website (see https://www.stat.tamu.edu/~suhasini/test_papers/DFT_covariance_lagl.R);
and for the third method, we employ the R package locits, which is contributed by the
author. For the purpose of comparison of accuracy, besides the ﬁve models considered in
Section 5.1, we also consider two strictly stationary time series models, model 6 for a sta-
tionary ARMA(1,1) and model 7 for a stationary SETAR. Furthermore, for the comparison

L

AR APPROXIMATIONS TO NON-STATIONARY TIME SERIES

21

Basis/Model

1

2

α = 0.1
3

4

5

1

2

n=256

α = 0.05
3

4

5

Fourier
Legendre
Daubechies-9

Fourier
Legendre
Daubechies-9

0.132
0.091
0.132

0.09
0.09
0.091

0.11
0.136
0.12

0.13
0.094
0.11

0.12
0.13
0.11

0.13
0.12
0.133

0.11
0.13
0.132

0.067
0.06
0.063

0.11
0.092
0.098

0.13
0.12
0.11

n=512

0.127
0.118
0.118

0.05
0.04
0.048

0.07
0.059
0.067

0.06
0.058
0.052

0.06
0.041
0.059

0.067
0.07
0.054

0.04
0.07
0.068

0.068
0.043
0.053

0.06
0.07
0.065

0.069
0.057
0.054

TABLE 1
Simulated type I errors using the setup (5.1). The models are listed in Section 5.1 and the basis functions can be
found in Section S.7. The results are reported based on 1,000 simulations. We can see that our multiplier
bootstrap procedure is reasonably accurate for both α = 0.1 and α = 0.05.

Basis/Model

1

δ = 0.2/0.5
3

4

2

δ = 0.35/0.7
3

2

4

5

1
n=256

Fourier
Legendre
Daubechies-9

Fourier
Legendre
Daubechies-9

0.84
0.8
0.81

0.91
0.9
0.87

0.86
0.806
0.81

0.9
0.91
0.88

0.84
0.81
0.86

0.96
0.92
0.93

0.837
0.84
0.81

0.9
0.893
0.91

0.94
0.83
0.81

0.97
0.97
0.97

0.97
0.968
0.96

n=512

0.93
0.91
0.91

0.96
0.94
0.96

0.97
0.95
0.99

0.96
0.95
0.983

0.973
0.98
0.97

0.99
0.97
0.98

0.98
0.97
0.97

5

0.98
0.91
0.98

0.97
0.96
0.96

TABLE 2
Simulated power under the setup (5.2) using nominal level 0.1. For models 1-2, we consider the cases δ = 0.2
and δ = 0.35, whereas for models 3-5, we use δ = 0.5 and δ = 0.7. The results are based on 1,000 simulations.

of power, we consider two non-stationary time series models whose errors have constant
variances, denoted as models 6# and 7#. The detailed setups of those models can be found
in Section S.2 of our supplement.

In the simulations below, we report the type I error rates under the nominal levels 0.05 and
0.1 for the above seven models in Table 3, where for models 1-5 we use the setup (5.1). Our
2 distance method,
simulation results are based on 1,000 repetitions, where
DFT 1-3 refer to the approaches using the imagery part, real part, both imagery and real parts
of the discrete Fourier transform method, respectively, HWT is the Haar wavelet periodogram
method and MB is our multiplier bootstrap method Algorithm 1 using orthogonal wavelets
constructed by (S.2) with Daubechies-9 wavelet.

2 refers to the

L

L

Since HWT needs the length to be a power of two, we set the length of time series to be
2 test, we use M = 8, N = 32 for n = 256, and
256 and 512. For the parameters of the
M = 8, N = 64 for n = 512. For the DFT, we choose the lag to be 0 as suggested by the
authors in [20]. Since the mean of model 5 is non-zero, we test its ﬁrst order difference for
the methods mentioned above. Moreover, we report the power of the above tests under certain
alternatives in Table 4 for models 6#

7# and models 1-5 under the setup (5.2).

L

We ﬁrst discuss the results for models 6-7 since they are not only correlation stationary
but also covariance stationary. It can be seen from Table 3 that all the methods including our
MB achieve a reasonable level of accuracy for the linear model 6. However, for the nonlinear
2 method tends to be over-conservative due to
model 7, we conclude from Table 3 that the
L
the fact that the latter test is designed only for linear models driven by independent errors.

−

22

Model

L2

DFT1 DFT2 DFT3 HWT MB

α = 0.1

α = 0.05

DFT1 DFT2 DFT3 HWT MB

L2

n=256

1
2
3
4
5
6
7

1
2
3
4
5
6
7

0.08
0.081
0.171
0.2
0.46
0.11
0.051

0.087
0.051
0.26
0.287
0.64
0.11
0.051

0.148
0.097
0.183
0.163
0.293
0.105
0.097

0.127
0.096
0.16
0.167
0.303
0.093
0.087

0.057
0.068
0.04
0.05
0.077
0.096
0.08

0.03
0.085
0.04
0.027
0.087
0.084
0.113

0.13
0.12
0.137
0.12
0.19
0.09
0.092

0.13
0.093
0.117
0.09
0.283
0.088
0.083

0.18
0.085
0.227
0.176
0.153
0.087
0.085

0.237
0.075
0.243
0.247
0.35
0.088
0.093

0.132
0.12
0.11
0.133
0.132
0.088
0.127

0.024
0.038
0.087
0.077
0.29
0.047
0.018

n=512

0.091
0.11
0.098
0.11
0.118
0.092
0.092

0.023
0.026
0.127
0.177
0.413
0.035
0.013

0.067
0.04
0.103
0.087
0.21
0.053
0.04

0.1
0.036
0.1
0.103
0.26
0.046
0.037

0.017
0.07
0.011
0.013
0.03
0.053
0.06

0.02
0.067
0.007
0.013
0.063
0.047
0.047

0.063
0.057
0.033
0.034
0.14
0.039
0.047

0.043
0.044
0.037
0.073
0.167
0.048
0.043

0.083
0.028
0.093
0.113
0.12
0.052
0.038

0.137
0.033
0.14
0.163
0.23
0.053
0.04

0.063
0.067
0.059
0.068
0.065
0.057
0.061

0.048
0.052
0.054
0.053
0.054
0.048
0.051

TABLE 3
Comparison of accuracy for models 1-7 using different methods. We report the results based on 1,000
simulations.

Regarding the power in Table 4, we shall ﬁrst discuss the results for models 6# and 7# where
the errors of the models are i.i.d. For model 6#, when the sample size and δ are smaller (n =
256, δ = 0.2 or 0.35), our MB method is signiﬁcantly more powerful than the other methods.
2 test starts to become similarly powerful. Further,
When n = 256 and δ increases, the
when both the sample size and δ increase, the HWT method becomes similarly powerful.
Similar discussion holds for model 7#. Therefore, we conclude that, when the variances of
the AR approximation errors stay constant, other methods in the literature are accurate for the
purpose of testing for correlation stationarity (which is equivalent to covariance stationarity
in this case). On the other hand, in this case the MB method is more powerful when the
sample size is moderate and/or the departure from covariance stationarity is small for the
alternative models experimented in our simulations.

L

Next, we study models 1-5 from Section 5.1. None of these models is covariance station-
ary. For the type I error rates, we use the setting (5.1) where all the models are correlation
stationary. For the power, we use the setup (5.2). We ﬁnd that DFT-3 is accurate for models
2 test seems to have a high
1-4 but with low power across all the models. Moreover, the
power for models 3-5. But this is at the cost of blown-up type I error rates. This inaccuracy
in Type-I error increases when the sample size becomes larger. For the HWT method, even
though its power becomes larger when the sample size and δ increase, it also loses its accu-
racy. Finally, for all the models 1-5, our MB method obtains both reasonably high accuracy
and power. In summary, most of the existing tests for covariance stationarity are not suitable
for the purpose of testing for correlation stationarity. Of course, the latter is expected as those
tests are designed for testing covariance stationarity which is surely a different problem from
correlation stationarity or stability of AR approximation. From our simulation studies, our
multiplier bootstrap method Algorithm 1 performs well for the latter purpose.

L

6. An empirical illustration.

In this section, we illustrate the usefulness of our results
by analyzing a ﬁnancial data set. We study the stock return data of the Nigerian Breweries
(NB) Plc. This stock is traded in Nigerian Stock Exchange (NSE). Regarding on market re-
turns, the brewery industry in Nigerian has done pretty well in outperforming Brazil, Russia,

AR APPROXIMATIONS TO NON-STATIONARY TIME SERIES

23

Model

L2

DFT1 DFT2 DFT3 HWT MB

δ = 0.2/0.5

δ = 0.35/0.7

DFT1 DFT2 DFT3 HWT MB

L2
n = 256

1
2
3
4
5
6#
7#

1
2
3
4
5
6#
7#

0.263
0.183
0.44
0.603
0.92
0.697
0.463

0.477
0.51
0.657
0.84
0.963
0.847
0.69

0.14
0.497
0.153
0.16
0.243
0.12
0.137

0.173
0.297
0.24
0.23
0.297
0.147
0.14

0.03
0.08
0.04
0.04
0.143
0.093
0.107

0.04
0.082
0.05
0.043
0.127
0.087
0.13

0.07
0.092
0.16
0.203
0.24
0.11
0.133

0.08
0.092
0.083
0.143
0.263
0.103
0.217

0.3
0.585
0.393
0.44
0.57
0.327
0.273

0.52
0.385
0.61
0.773
0.87
0.67
0.383

0.81
0.81
0.86
0.81
0.81
0.86
0.85

0.503
0.68
0.7
0.86
0.997
0.923
0.81

n = 512

0.87
0.88
0.93
0.91
0.91
0.88
0.91

0.857
0.918
0.96
0.987
0.983
0.95
0.953

0.113
0.14
0.14
0.2
0.347
0.16
0.193

0.137
0.24
0.17
0.293
0.523
0.13
0.3

0.053
0.06
0.05
0.07
0.193
0.15
0.203

0.03
0.06
0.24
0.053
0.24
0.09
0.313

0.089
0.047
0.09
0.12
0.397
0.15
0.223

0.1
0.047
0.113
0.19
0.478
0.133
0.383

0.4
0.38
0.64
0.647
0.797
0.563
0.483

0.75
0.838
0.95
0.97
0.994
0.963
0.823

0.97
0.96
0.983
0.98
0.98
0.94
0.96

0.96
0.99
0.97
0.97
0.96
0.95
0.943

TABLE 4
Comparison of power at nominal level 0.1 using different methods. We report the results based on 1,000
simulations.

India, and China (BRIC) and emerging markets by a wide margin over the past ten years.
Nigerian Breweries Plc is the largest brewing company in Nigeria, which mainly serves the
Nigerian market and also exports to other parts of West Africa. The data can be found on the
website of morningstar (see http://performance.morningstar.com/stock/performance-return.action?p=price_history_page&t=NIBR&region=nga&culture=en-US).
We are interested in understanding the volatility of the NB stock. We shall study the absolute
value of the daily log-return of the stock for the latter purpose.

Nigerian Breweries Stock price: 2008−2014

e
c
i
r
p

k
c
o
t
S

n
r
u
t
e
r
−
g
o
L

0
5
1

0
0
1

0
5

5
0
.
0

0
0
0

.

.

5
0
0
−

.

0
1
0
−

2008

2009

2010

2011

2012

2013

2014

Fig 1: Nigerian Breweries stock return from 2008 to 2014. The upper panel is the original stock price
and the lower panel is the log-return.

We perform our analysis on the time period 2008-2014 (Figure 1). This time series con-
tains the data of the 2008 global ﬁnancial crisis and its post period. As said in the report from
the Heritage Foundation [35], "the economy is experiencing the slowest recovery in 70 years"
and even till 2014, the economy does not fully recover.

 
24

Then we apply the methodologies described in Sections 3 for the absolute values of log-
return time series. It is clear that we need to ﬁt a mean curve for this model. Then we test
the stability of the AR approximation as described in Section 3 using Algorithm 1. For the
sieve basis functions, we use the orthogonal wavelets constructed by (S.2) with Daubechies-9
, c and m based on the discussion of Section S.5 which
wavelet. We choose the parameters b
yields b
= 7, Jn = 5 (i.e., c = 32) and m = 18. We apply the bootstrap procedure described
in Algorithm 1 and ﬁnd that the p-value is 0.0825. We hence conclude that the prediction is
likely to be unstable during this time period.

∗

∗

Next, we use the time series 2008-2014 as a training dataset to study the forecast perfor-
mance over the ﬁrst month of 2015 using (4.5). We employ the data-driven approach from
Section S.5 to choose b = 5 and Jn = 3. The averaged MSE is 0.189. We point out that this
leads to a 20.9% improvement compared to simply ﬁtting a stationary ARMA model using
all the data from 2008 to 2014 where the MSE is 0.239, and leads to a 24.7% improvement
compared to the benchmark of simply using ˆxn+1 = xn where the MSE is 0.251.

Finally, we study the absolute value of the stock return from 2012 to 2014. We apply our
bootstrap procedure Algorithm 1 to test correlation stationarity of the sub-series. We select
= 6, Jn = 4 (i.e., c = 16) and m = 12 for this sub-series and ﬁnd that the p-value is 0.599.
b
∗
We hence conclude that the prediction is stable during this time period. Therefore, we ﬁt
a stationary ARMA model to this sub-series and do the prediction. This yields an MSE of
0.192 which is close to 0.189, the MSE when we use the whole non-stationary time series and
the methodology proposed in Section 4. The result from this sub-series shows an interesting
trade-off between forecasting using a shorter and correlation-stationary time series and a
longer but non-stationary series. The forecast model of the shorter stationary period can be
estimated at a faster rate but at the expense of a smaller sample size. The opposite happens
to the longer non-stationary period. Note that 2012-2014 is nearly half as long as 2008-
2014 and hence the length of the shorter stationary period is substantial compared to that of
the long period. In this case we see that the forecasting accuracy using the shorter period
is comparable to that of the longer period. In many applications where the data generating
mechanism is constantly changing, the stable period is typically very short and in this case
the methodology proposed in Section 4 is expected to give better forecasting results under the
assumption that the time series is locally stationary. Finally, we emphasize that the correlation
stationarity test proposed in this paper is an important tool to determine a period of prediction
stability.

Supplementary material

This supplementary material contains further explanation, auxiliary lemmas and technical
proofs for the main results of the paper.

APPENDIX S.1: SOME CONVENTIONS

x
k

bn if an = O(bn) and bn = O(an).

k2 to denote the L2 norm of x. For two sequences of positive real values

Throughout the supplement, we consistently use the conventions listed in the end of Sec-
R, we simply write
and

tion 1 of the main manuscript. Additionally, for any random variable x
x
:=
k
k
, we write an ≍
bn}
{
Moreover, with a little bit abuse of notation, we use
if A is a positive deﬁnite matrix and the L2 norm if A is a vector. Consequently, if
is a sequence of deterministic matrices and
an}
{
notation
= O(an) means that there exists some constant C > 0 so that
Ank
k
Moreover, if
An}
{
that the operator norm of An is stochastically bounded by an.

to denote the operator norm
An}
{
is a sequence of positive real values, the
Can.
Ank ≤
k
= OP(an) means

is a sequence of random matrices, the notation

Ank
k

an}
{

A
k
k

∈

AR APPROXIMATIONS TO NON-STATIONARY TIME SERIES

25

APPENDIX S.2: ADDITIONAL INFORMATION ON THE SIMULATION SETUP

In this section, we provide the detailed simulation setups used in Section 5.3 with the

following four time series models.

6. Linear time series: stationary ARMA(1,1) process. We consider the following process

where ǫi are i.i.d.

xi −
(0, 1) random variables.

0.5xi

−

1 = ǫi + 0.5ǫi

1,

−

7. Nonlinear time series: stationary SETAR. We consider the following model

xi =

0.4xi
0.5xi

−

−

(

1 + ǫi, xi
1 + ǫi, xi

0,
1 ≥
1 < 0,

−

−

(0, 1) random variables.

where ǫi are i.i.d.

N

N

6#. Non-stationary linear time series. We consider the following process

i
n
, n, are i.i.d. standard normal random variables.

xi = δ sin(4π

1 + ǫi,

)xi

−

where ǫi, i = 1, 2,

· · ·

7#. Piece-wise locally stationary linear time series. We consider the following process

δ sin(4π i
0.4xi
0.3xi

n )xi
1 + ǫi,
1 + ǫi,

−

−

xi = 


1 + ǫi,

0.75n,

≤

≤

i
1
0.75n < i
0.75n < i

n and xi
n and xi

−

0,
1 ≥
1 < 0,

≤
≤

where ǫi, i = 1, 2,

−

−
, n, are i.i.d. standard normal random variables.

· · ·

APPENDIX S.3: A FEW FURTHER REMARKS

First, we will need the following further assumption in the paper.

ASSUMPTION S.3.1. We assume that the following assumptions hold true for the sieve

basis functions and parameters:
(1). For any k = 1, 2,
j

, b
∗

· · ·

) where γ(
,
·
|

, denote Σk(t)

Rk

×

k whose (i, j)-th entry is Σk

ij(t) = γ(t,

∈
) is deﬁned in (2.27), we assume that the eigenvalues of
·

i
|

−

Σk(t)

⊗

(B(t)B∗(t)) ,

1

0

Z

sup
t

are bounded above and also away from zero by a universal constant κ > 0.
(2). There exist constants ω1, ω2 ≥

0, for some constant C > 0, we have
Cnω1cω2.

B(t)

|| ≤
(3). We assume that for τ deﬁned in (3.17), d deﬁned in Assumption 2.10 and a deﬁned in
(3.7), there exists a large constant C > 2, such that

||∇

C
τ

+ a < 1 and da > 2.

We mention that the above assumptions are mild and easy to check. First, (1) of Assump-
tion S.3.1 guarantees the invertibility of the design matrix Y and the existence of the OLS
solution. It can be easily veriﬁed, for example, for the linear non-stationary process (2.28),
where (1) will be satisﬁed if supt
< 1. Second, (2) is a mild regularity condition on

aj(t)
|
|

P

26

the sieve basis functions and is satisﬁed by many commonly used basis functions. We refer
the readers to [10, Assumption 4] for further details. Finally, (3) can be easily satisﬁed by
choosing C < τ and a accordingly. When the physical dependence is of exponential decay,
we only need da > 2. We refer the readers to [19, Assumption 3.5] for more details.

Second, the accuracy of the multiplier bootstrap in Section 3.3 is determined by the close-
ness of its conditional covariance structure to that of Ω. Following [47, Section 4.1.1], we
shall use

(S.1)

(m) =

Ω

Ω

,

−

L

(cid:12)
(cid:12)
(cid:12)
(cid:12)
Ω is deﬁned in (S.1), to quantify the latter closeness. The following theorem establishes
(cid:12)
(cid:12)
(m). Its proof follows from (S.39) below and the assumptions of Theorem

(cid:12)
(cid:12)
(cid:12)b

(cid:12)
(cid:12)
(cid:12)

where
the bound for
L
3.9. We omit the details here.
b

THEOREM S.3.2 (Optimal choice of m). Under the assumptions of Theorem 3.9, we

have

(cid:17)(cid:19)
Consequently, the optimal choice of m is of the order O(n1/3).

(cid:16)r

(m) = OP

L

ζ 2
c

b
(cid:18)

∗

m
n

+

1
m

.

Note that compared to [47, Theorem 4], the difference from Theorem S.3.2 is that we get
ζ 2
c due to the high dimensionality. For instance, when we use the Fourier
an extra factor b
basis, normalized Chebyshev orthogonal polynomials and orthogonal wavelets, we shall have
ζ 2
c = p, which is the dimension of zi deﬁned in (3.14). However, it will not inﬂuence
that b
the optimal choice of m.

∗

∗

APPENDIX S.4: TECHNICAL PROOFS

This section is devoted to the technical proofs of the paper.

S.4.1. Proofs of the main results of Section 2.

Proof of Theorem 2.4. We start with the proof of (2.5). Till the end of the proof, we
1)∗. By the Yule-Walker’s

focus our discussion on each ﬁxed j. Denote φi = (φi1,
equation, we have

, φi,i

· · ·

−

(S.1)

φi = Γ−

1
i γi,

where Γi = Cov(xi
for some constants C1, C2 > 0,

1, xi

−

−

1) and γi = Cov(xi, xi) with xi

1 = (xi

1,

−

· · ·

−

, x1)∗. Note that

(S.2)

φi
k

k ≤

1
λmin(Γi) k

γik ≤

C1κ

τ

k−

C2,

≤

1

i

−

Xk=1

where in the second inequality we used the UPDC condition in Assumption 2.1 and Assump-
tion 2.2 that τ > 1.

Note that when j = O(1), the result holds immediately according to (S.2). We next focus
our discussion on the case when j diverges with n. Since i > j, i also diverges with n. We
denote the (i

1) symmetric banded matrix Γs

1)

(i

Γs
i (j) by

−

×

−

(Γs

i )kl =

(Γi)kl,
0,

(

l
k
|
otherwise.

| ≤

−

i ≡
j
K log j ;

AR APPROXIMATIONS TO NON-STATIONARY TIME SERIES

27

Here K > 0 is some large constant. By Lemma S.6.1, Assumption 2.2 and the UPDC condi-
tion in Assumption 2.1, we have for some constant C > 0,

λmin(Γs
i )

κ

≥

−

Cj1
−

τ (K log j)τ

1,

−

for all i > j. Similarly, we can show that λmax(Γs
C for some constant C > 0. Since
i )
n is sufﬁciently large and j diverges with n, the above arguments show that the support of
the spectrum of Γs
i is bounded from both above and below by some constants. Therefore, by
Lemma S.6.3, we conclude that for some δ

(0, 1) and some constant C > 0, we have

≤

(S.3)

(Γs

1
i )−
kl

≤

∈
Cδ(K

k

l

|

log j)/j.

−

|

By Cauchy-Schwarz inequality and Lemma S.6.1, when n is large enough, for some constant
C > 0, we have that

(cid:12)
(cid:12)

(cid:12)
(cid:12)

1

i γi −
Γ−

(Γs

i )−

1γi

≤ k

Γi −
Cj1
−

Γ−

Γs
1
i kk
i kk
τ (K log j)τ
−

1,

(Γs

i )−

1

γik

kk

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(S.4)

(cid:12)
(cid:12)
≤
where we used (2.2), the UPDC in Assumption 2.1 and the conclusion λmin(Γs
i )
i = (φs
some constant C1 > 0. Denote φs
i1,
· · ·
i = (Γs
φs

1) such that
−
1γi.

, φs
i,i

(S.5)

i )−

(cid:12)
(cid:12)

C1, for

≥

Then we get immediately from (S.4) that

(S.6)

Hence, it sufﬁces to control φs
recall that γik = Cov(xi, xi

1
jk γik, where we
ij =
k). By (2.2) and (S.3), we have that for some constants C, C1 > 0

i
1
k=1(Γs
−

i )−

Cj1
−

φs
ij| ≤

φij −
|
ij. By (S.5), we note that φs

τ (K log j)τ

1.

−

−

φs
ij| ≤
|

C

1

i

−

Xk=1

(S.7)

δ(K

|

k

j

log j)/jk−

τ = C

|

−





C1

≤

j

1

−

δ(K(j

k) log j)/jk−

τ +

−

1

−

δ(K(k

j) log j)/jk−

τ

−

P
i

Xk=j

δK log j(j

k)/jk−

τ + j1
−

τ

−

.

!

Xk=1
1
j

−

Xk=1





where in the second inequality we used the fact that δ(K(k
j.
Furthermore, to control the ﬁrst summation of the right-hand side of (S.7), since j diverges
with n, we see that

j) log j)/j is bounded for k

≥

−

j

1

−

δK log j(j

k)/jk−

τ

−

j

1

−

Xk=1

≍

j−

K(1

−

k/j)k−

τ .

Xk=1
k/j)k−

K(1

Let f (k) = j−
is decreasing between 1 and τ j/(K log j) and increasing between τ j/(K log j) and j
As a result, since j−

τ . By an elementary derivative argument, it is easy to see that f (k)
1.
k/j) is bounded when k < j, for some constants C, C1 > 0, we have

K(1

−

−

−

j

1

−

Xk=1

j−

K(1

−

k/j)k−

τ

τ j
K log j

(cid:19)

≤

(cid:18)

j−

K(1

−

1/j) + C

j

1

−

τ

k−

Xk=τ j/(K log j)

C1(j/ log j)−

τ +1,

≤

where in the second inequality we used the fact that K is a large constant and j diverges.
Together with (S.6), we conclude our proof of (2.5).

 
28

Then we proceed to prove the ﬁrst equation of (2.7) using Lemma S.6.4. For the conve-
nience of our discussion, we denote the (k, l)-entry of Γi as Γi(k, l). For i > b, we denote the
i and the block vector γb
(i

1) block matrix Γb

1 via

Ri

1)

(i

−

−

×

−

i ∈

i , xi), 0),

Γb

i =

i , xb

Cov(xb
E3

i ) E1
E2(cid:21)
(cid:20)
b)∗ and Ei, i = 1, 2, are deﬁned as

i = (Cov(xb

, γb

· · ·

, xi
−
i , xb
i , xm
i )
i )
R(i
, x1)∗. Moreover, E3 = (E3(k, l))
1,
b. For some constant ς > 2, E3 is denoted as

1), E2 = Cov(xm

b
−

· · ·

Rb

∈

∈

−

×

(i

−

where xb

i = (xi

−

1,

(S.8)

E1 = Cov(xm

and xm
i
k

i = (xi
1, 1

≤

−

−
l

b
−
≤

≤

(S.9)

E3(k, l) =

Γi(k, l)
0
(

b/ς,

l
k
|
otherwise.

| ≤

−

Denote φb

i = (φb
i1,

, φb

ib, 0)

∈

· · ·

(S.10)

where ∆γi is deﬁned as

Ri

1. We have that

−
i φb
Γb

i = γb
i

∆γi,

−

∆γi = (E3

φb

i , 0),

φb

i = (φb
i1,

, φb

ib)∗.

· · ·

R(i

b
−

−

1)

×

(i

b
−

1),

−

1)

×

b. Note that b + 1

≤

∈
b
−

Since

(S.11)

e
e
φb
φi −
φij −
ij| ≤ k
b |
φb
φi −
. Now we employ Lemma S.6.4 with
it sufﬁces to provide an upper bound for
i k
k
A = Γi, ∆A = Γb
φi, v = γi, ∆v = γb
Γi, x = φi, ∆x = φb
∆γi to the systems
i −
(S.1) and (S.10). By the UPDC in Assumption 2.1, for some constant C > 0, we ﬁnd that
C. By Lemma S.6.1 and (2.2), we ﬁnd that for some constant C > 0, we have
κ(A)

max
j
1
≤
≤

φb
,
i k

γi −

i −

i −

≤

Moreover, note that

∆A
k

k ≤

(b/ς)−

τ +1

Cb−

τ +1.

≤

∆v
k

γb
i −

k ≤ k

γik

+

∆γik
.
k

The ﬁrst term of the right-hand side of the above equation can be bounded by Cb−
(2.2). For the second term, by a discussion similar to (2.5), we ﬁnd that
1)/j)τ

1. Using the deﬁnition of E3 in (S.9), we obtain that for some constants C, C1 > 0

φb
ij| ≤
|

τ +1 using
C((log j +

−

∆γik ≤
k

C min

b

b/ς, i
{
−
τ +3(log b)τ

−
1.
−

(b/ς)−
1
}

(S.12)

C1b−

≤

Consequently, we have that

(τ

1)+1(log(b/ς))τ

−

−

1

(S.13)

φi −
k

φb

i k ≤

Cb−

τ +3(log b)τ

1.

−

This ﬁnishes our proof of the ﬁrst equation of (2.7).

Finally, we prove the second equation of (2.7). Note that

(S.14)

φi0 = µi −

1

i

−

Xj=1

φijµi

j, φb

i0 = µi −

−

b

Xj=1

φb
ijµi

j,

−

AR APPROXIMATIONS TO NON-STATIONARY TIME SERIES

29

where µi = Exi, i = 1, 2,

· · ·

, n, is the sequence of trends of

. We have

φi0 −

φb
i0 =

b

Xj=1

(φb

ij −

φij)µi

j −

−

Xj=b+1

φijµi

j.

−

xi}
{
1

i

−

The ﬁrst term of the right-hand side of the above equation is bounded by C(log b)τ
using (S.13) and Cauchy-Schwarz inequality and the second term can be bounded by
C(log b)τ

2) using (2.5). This concludes our proof.

1b−

3.5)

(τ

(τ

−

−

1b−

−

−

Proof of Theorem 2.5. We start with the ﬁrst part. First of all, when i

b, it holds by
setting φij to be the coefﬁcients of best linear prediction. When i > b, by (2.4), we decompose
that

≤

xi = φi0 +

b

Xj=1

φijxi

−

j + ǫi +

1

i

−

Xj=b+1

φijxi

j.

−

By (2.2) of the main article, we have that

1

i

−

E

j(cid:12)
(cid:12)
(cid:12)
(cid:12)
Together with Theorem 2.4, we ﬁnd that
(cid:12)
(cid:12)

(cid:12)
(cid:12)
Xj=b+1
(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

φijxi

2

C

≤

Xk Xj

φi,jφi,j+kk−

τ .

1

i

−

Xj=b+1

φijxi

−

j = Oℓ2(b−

(τ

1.5)(log b)τ

1).

−

−

This concludes our proof of the ﬁrst part.
Next, we prove the second part. Recall (2.8). Clearly,

For i = b + 1, we have that

is an AR(b) process when i > b.

x∗i }
{

Suppose (2.10) holds true for k > b + 1, then for k + 1, we have

xi −

x∗i = 0.

b

x∗k+1 =

φij(xk+1

xk+1 −

j −

x∗k+1

−

j) +

k

Xj=b+1

φijxk+1

−

j

−

Xj=1
= Oℓ2(b−

(τ

1.5)(log b)τ

1),

−

−

where in the second step we used induction and Theorem 2.4.

Proof of Proposition 2.9. Denote the covariance matrix of (x1,

a given truncation level dn = O(nf ), 0 < f < 1

Σn. For
· · ·
2 , we deﬁne the banded matrix Σdn such that

, xn) as Σ

≡

Σdn

ij =

Σij,
0,

(

j

i
|

if
−
| ≤
otherwise.

dn;

Throughout the proof, we let λn be the smallest eigenvalue of Σ and µn be that of Σdn. Under
Assumption 2.2, when n is large enough, by Lemma S.6.1, we have

(S.15)

λn = µn + o(1),

30

where we used the assumption that τ > 1 in (2.2). Therefore, it is equivalent to study the
UPDC for Σdn. We now consider a longer time series
, where we use the conven-
tion xi = G(0,
Fi) if i > n. We will need the following lemma to
Fi) if i < 0 and xi = G(1,
prove the sufﬁciency.

xi}
{

n+dn
i=

dn

−

LEMMA S.4.1. Let Σdn
n, let λdn(Σdn
dn ≤
−
(2.15) is bounded from below, we have that for some constant ς > 0,

i be the covariance matrix of (xi, xi+1,
i ) be the smallest eigenvalue of Σdn

≤

i

i

, xi+dn). Then for all
. Then if the spectral density

· · ·

λdn (Σdn
i )

≥

ς > 0, for all i.

PROOF. Without loss of generality, we set i = 0. Consider the stationary process

x0
i }
{
such that γ(0,
) is its autocovariance function. By Lemma S.6.2, when the spectral density is
·
bounded below, we ﬁnd that

(S.16)

, x0

dn))

≥

ς > 0,

λdn(Cov(x0
i ,
i, j

· · ·

≤

≤
i , x0
j )

for any dn. On the other hand, when 1

dn, for some constant C > 0, we have

Cov(xi, xj)

−

Cov(x0

C min

≤

(cid:18)

max(i, j)
n

,

i
|

−

τ

j

−
|

,

(cid:19)

where the ﬁrst bound comes from the (2.14) and the Lipschitz continuity of γ in t, and the
second bound is due to (2.2). As a consequence, by Lemma S.6.1, we ﬁnd that

(cid:12)
(cid:12)

(cid:12)
(cid:12)

λdn (Σdn
i )
|

−

λdn (Cov(x0
i ,

· · ·

Together with (S.16), we ﬁnish the proof.

, x0

dn))

| ≤

C

d2
n
n

.

With the above preparation, we proceed with the ﬁnal proof. We start with the sufﬁciency
, ai+2dn)∗ ∈

part. For any non-zero vector a = (a1,

Ri+2dn , i =

, n, denote

dn,

· · ·

· · ·

−

F (a, i) :=

By Lemma S.4.1, we ﬁnd that

i+dn

i+dn

Xk=1

Xl=1

ak(Σdn

i )k,lal.

(S.17)

F (a, i)

ς

≥

i+dn

a2
l .

Xl=i

Now we let the ﬁrst and last dn entries of a be zeros. Then using a discussion similar to
(S.17), we ﬁnd that

(S.18)

1
dn

n

F (a, i)

n

ς

a2
l .

≥

dn
Xi=
−
Furthermore, by Lemma S.6.1, it is easy to see that for some constant C > 0,

Xl=1

1
dn

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n

n

n

F (a, i)

−

akΣdn

kl al

Xk=1

Xl=1

dn
Xi=
−

C
dn

≤

n

a2
k.

Xk=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Together with (S.18), we ﬁnd that

n

n

Xk=1

Xl=1

akΣdn

kl al ≥

ς
2

n

Xl=1

a2
l ,

AR APPROXIMATIONS TO NON-STATIONARY TIME SERIES

31

when n is large enough. This shows that Σdn satisﬁes PDC and hence ﬁnishes the proof of
the sufﬁcient part.

Next we brieﬂy discuss the proof of necessity. We make use of the structure of Σdn. For

any given ti := i

n and ω, denote

fn(ti, ω) =

1
2πn

n

Xk,l=1

e−

ikωγ(ti, k

l)eilω.

−

It is easy to see that (for instance see a similar discussion in [5, Corollary 4.3.2])

(S.19)

fn(ti, ω) = f (ti, ω) + o(1).

Furthermore, we denote

gn(ti, ω) =

1
2πn

n

Xk,l=1

e−

ikωΣdn
k
i,
|

l

|

−

eilω.

By the assumption (2.14) and Assumption 2.2, we ﬁnd that

(S.20)

gn(ti, ω) = fn(ti, ω) + o(1).
Using the structure of Σdn and the assumption that Σdn satisﬁes UPDC, we ﬁnd that for some
constant κ > 0,

≥
In light of (S.19) and (S.20), we ﬁnd that f (ti, ω)
using the continuity of f (t, ω) in t. This concludes our proof.

≥

gn(ti, ω)

κ.

κ. Finally, we can conclude our proof

j

≤

≤

Proof of Theorem 2.11. For the ﬁrst statement regarding the smoothness of φj(t), under
Assumption 2.10, the case 1
b follows from Lemma 3.1 of [19]. When j = 0, i.e., φ0(t)
deﬁned in (2.17), the smoothness can be easily proved using term by term differentiation,
C d([0, 1]), 1
Assumption 2.10 and the results φj(t)
We then prove (2.18). Since ηi’s in the ﬁltration
j

of φj(t), we can equivalently write φj(i/n), 1
n ))∗ via φ( i
(φ1( i
the discussion of (S.11), it sufﬁces to offer an upper bound for
φb

≤
Fi are i.i.d., in light of the deﬁnition
b in the following way. Let φ( i
n ) =
≤
1γ(i/n) as in (2.16). In view of (2.7), similar to
φ( i
. Note that
n )
k
Rb is governed by the following Yule-Walker’s equation

n ) = Γ(i/n)−

, φb( i

φb
k

i −

n ),

· · ·

≤

≤

b.

∈

j

, φb

i = (φb
i1,

· · ·

ib)∗ ∈

φb

1
ib γib,
i = Γ−
i , xi) with xb

where Γib = Cov(xb
have that

i , xb

i ), γib = Cov(xb

i = (xi

1,

−

· · ·

, xi

−

b)∗. Consequently, we

φb
k

i −

φ(

i
n

)
k ≤ k

γib −

γ(i/n)
k

Γ−

1
ib kk
Γ(i/n)−
k

1

+

(S.21)

kk
By (2.14) and Cauchy-Schwarz inequality, it is easy to see that for some constant C > 0

−

Γ−

1
ib kk

Γ(i/n)

Γibkk

γ(i/n)
.
k

Similarly, together with Lemma S.6.1, we see that

γib −
k

γ(i/n)

k ≤

C

b1.5
n

.

Γ(i/n)
k

Γibk ≤

C

−

b2
n

.

32

Under Assumption 2.1, combing with (S.21), we conclude that

(S.22)

φb
k
This completes our proof for (2.18).

i −

φ(

i
n

C

)
k ≤

b2
n

.

For the proof of (2.19), (2.17) implies that

φ0(

i
n

) = µ(

i
n

)

−

b

Xj=1

φj(

i
n

)µ(

i
n

).

By Assumption 2.10 that
C1b/n when
i
|
C1 > 0, (S.14) and (S.22), we ﬁnd that for some constant C > 0,

µ(i/n)
|

j)/n)

µ((i

| ≤

−

−

j

−

| ≤

b for some constant

i
n

)

−

φb
i0

b2.5
n

,

C

≤

φ0(
(cid:12)
(cid:12)
(cid:12)
(cid:12)

where we used Cauchy-Schwarz inequality. Then we can prove (2.19) using (2.7).

(cid:12)
(cid:12)
(cid:12)
(cid:12)
Further, invoking (2.4), (2.20) follows from (2.18), (2.19) and (2.26). Finally, the proof of
(2.21) is similar to those of Theorem 2.5 using Theorems 2.11 and 2.5 and we omit further
details here.

S.4.2. Proofs of the main results of Section 3. We point out that throughout this sub-

section, we use the choice of b

as in (3.1).

∗

Proof of Theorem 3.1. By Assumptions 2.2 and 2.14, we ﬁnd that there exists some con-

stant C > 0, such that

sup
i

|

Corr(xi, xj)

| ≤

C

i
|

j

−
|

−

τ , i

= j.

Therefore, we only consider the correlation when
2.14, for some constant C > 0, we have

j

i
|

−

| ≤

b
∗

. Indeed, due to Assumption

Corr(xi, xj)

−

b∗ |

Corr(xb+i, xb+j)

| ≤

C

b
∗
n

.

sup
i,j

≤

1

≤

Therefore, it sufﬁces to test the stationarity for the correlation of xi and xj, where i, j > b
∗
and
, we can
i
|
write

. First, by the smoothness of G(
,
·

), we observe that for any i > b
·

| ≤

−

b

j

∗

∗

Var

G(

i
n

,

Fi

−

k)

= σ2(

i
n

), 1

k

.

b
∗

≤

≤

(cid:18)
As a consequence, using Yule-Walker’s equation, we have

(cid:19)

) = P−

1ρ,

φ(

i
n
xi

where P is the correlation matrix of
xi
vector of
xi
where
part using (2.18).

xi and
−
1,k is the k-th entry of
−
e

e

xi
1,
−
xi
1. Here use the notation that
e

b∗)∗ and ρ is the correlation
xi
· · ·
−
1,k = G( i
,
k), k = 1, 2,
n ,
, b
∗
−
1. Hence, under H′0, we conclude the proof of the ﬁrst
e

1 = (

Fi

· · ·

xi

e

) are constant func-
For the second part of the statements, under the assumption that φj(
·

e

e

e

−

−

−

,

tions, we have that

Corr(xi, xi+j) =

b∗
k=1 φk Cov(xi, xi+j
Var xi+j

P

k)

−

+ O

b
∗
n

(cid:18)

+ (log b
∗

)τ

−

1b−
∗

(τ

2)

−

,

(cid:19)

6
AR APPROXIMATIONS TO NON-STATIONARY TIME SERIES

33

where we use the stochastic Lipschitz continuity, the expression (2.4) of xi+j and (2.5). Using
the fact that

Cov(xi, xi+j

k) = η
|

−

k

j

|

−

Var xi+j + O

b
∗
n

(cid:18)

+ (log b
∗

)τ

−

1b−
∗

(τ

2)

−

,

(cid:19)

j

k

= Corr

where η
|

G( i+j
n ,
ary time series. By Theorem 2.4, we can choose ̺j =
proof.

Fi), G( i+j
n ,

Fi+j

k)

(cid:17)

(cid:16)

−

−

|

is the correlation function for a station-

b∗
j=k φkη
|

k

j

|

−

. This concludes our

P

Proof of Lemma 3.3. Under the null assumption H0 that φj(t) are identical in t, we have

φj(t)
(

−

φj)2 =

φj(t)

1

φj(t)

−

−

0

(cid:18)Z

φj(s)
(

−

φj(s))ds

2

.

(cid:19)(cid:19)

b

(cid:18)

b

By (3.6), we can write

b

b

b∗

W

β∗j

T =

β∗j −

βj −
b
Rc satisﬁes that βjk = βjc+k, 1
≤

Xj=1 (cid:16)

βj

(cid:16)

(cid:17)

b

(cid:17)
k

+ O(b
∗

c−

d), W =

I

−

¯B ¯B∗

,

(cid:17)

(cid:16)

c. It is well-known that the OLS estimator

≤

where βj ∈
satisﬁes

(S.23)

β = β +

Y ∗Y
n

(cid:18)

(cid:19)

−

1 Y ∗ǫ
n

, ǫ = (ǫb∗+1,

, ǫn)∗.

· · ·

By (S.23), we ﬁnd that nT is a quadratic form in terms of 1
√n

b

n
i=b∗+1 z∗i . We ﬁnd that

nT = X∗

1

−

Ib∗cW

Y ∗Y
n

(cid:18)

(cid:19)

Y ∗Y
n

(cid:18)

(cid:19)

1

−

P
X + OP(b

c−

d).

∗

By (2) of Lemma S.6.7 and (1) and (3) of Assumption S.3.1, we can conclude our proof.

Proof of Theorem 3.4. Denote

Ax :=

W

Rp : W∗ΓW

∈

≤

n

It is easy to check that Ax is convex as Γ is positive semi-deﬁnite. By deﬁnition, we have

(S.24)

where

A

(X, Y) = sup
x

P

X

Ax

P

Y

Ax

P

X

A

P

Y

K
(cid:17)
(cid:16)
is the collection of all convex sets in Rp. Given a large constant M

≤

−

∈

∈

∈

(cid:16)

−

A

,

∈

(cid:16)
M (n), denote

(cid:17)(cid:12)
(cid:12)
(cid:12)

≡

(cid:16)

(cid:17)

(cid:12)
(cid:12)
(cid:12)
i = E(hi|
hM
ηi
M ,
−
∗
, zM
n ) = (zM
ip )∗. Recall p = (b
i1 ,
· · ·
∗

, ηi), i = b

(cid:17)(cid:12)
(cid:12)
(cid:12)

· · ·

, n,

· · ·

B( i

i ⊗

i = hM

+ 1)c. Then we can deﬁne XM
and zM
accordingly and then YM can be deﬁned similarly. Note that in Lemma S.6.5, we have n1 =
n2 = n3 = M. Next we provide a truncation for the M -dependent sequence. Now we choose
Mz for γ

(0, 1), such that

∈

x

.

o

sup
A

∈A (cid:12)
(cid:12)
(cid:12)
+ 1,

Denote the set

P

(cid:16)

max
i
≤
≤

b∗+1

n

max
j
1
≤
≤

p |

zM
ij | ≥

Mz

γ.

≤

(cid:17)

(Mz) :=

B

max
i
≤
≤

b∗+1

n

n

max
j
1
≤
≤

p |

zM
ij | ≤

Mz

,

o

34

and X = (X1,
truncated version as XM

· · ·

, Xp). Similarly, we can deﬁne its M -dependent approximation as XM and

. We decompose the probability by

(S.25)

(XM , Y) =

K

K

≤ K

(Mz)) +

(XM , Y

∩ B
, Y) + Cγ,

(XM

(XM , Y

K

∩ B

c(Mz))

where C > 0 is some constant. Note that on

(Mz),

1
√n

zM
i

=

B
hM

i ⊗

1
√n

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

B(

i
n

)

C√pMz
√n

.

≤

YM as the Gaussian random vector with the same covariance structure with XM

Denote
whose Gaussian part is the same as Y. By Lemma S.6.5, we conclude that

e

7

(XM

YM )

,

K

Cp
In light of (S.25), it sufﬁces to control the difference of the covariance matrices between XM
and X. First, we show that the covariance matrices between XM
and XM are close. We
= Xi1(
emphasize that X
Mz). We need to conduct a more careful analysis. For
Xi| ≤
|
i = 1, 2,

4 n−

, p,

M
i

≤

e

1/2M 3

z M 2.

M
i )

−

Var(XM

i ) = E(X

M
i )2

E(XM

i )2 + (E(X

−

M
i −

XM

i ))(E(X

M
i + XM

i )).

· · ·

(S.26) Var(X

Note that

=

1
√n

E(X

M
i −

XM
i )
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
where we used the fact
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n

Xk=b∗+1

E(zM

ki −

=

zM
ki )
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1

zM
ki |
|

> Mz

≤

n

1
√n (cid:12)
Xk=b∗+1
(cid:12)
(cid:12)
(cid:12)
(cid:12)
q
zM
ki |
|
M q
−
z

−
1

1

,

√nξcM −
z

q+1

,

EzM
ki

1(
zM
ki |
|

≤

> Mz)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:1)
and Markov inequality. By Cauchy-Schwarz inequality, we can show analogously that for
some constant C > 0

(cid:0)

This implies that for some constant C > 0

Similarly, we can show that

E(X

M
i )2

−

E(XM

i )2

Cξ2

c nM −
z

(q

−

1)

.

≤

(cid:12)
(cid:12)
(cid:12)
Var(X

M
i )

−

(cid:12)
(cid:12)
(cid:12)

Cov(X

M
i

, X

M
j )

−

(cid:12)
(cid:12)
(cid:12)

≤

(cid:12)
(cid:12)
(cid:12)
Var(XM
i )
(cid:12)
(cid:12)
(cid:12)
, XM
j )
(cid:12)
(cid:12)
(cid:12)
Cξ2

Cov(XM
i

Cov(XM

≤

Cnξ2

c M −
z

(q

2)

.

−

Cnξ2

c M −
z

(q

−

1)

.

Together with Lemma S.6.1, we ﬁnd that

)
k ≤
Second, we control the difference between XM and X. By [27, Lemma A.1] (or Lemma

c pnM −
z

−

k

−

.

Cov(XM )

(q

2)

S.6.8), we have

(S.27)

By (3.17), we conclude that

(S.28)

E

Xj −
|
(cid:0)

XM
j

q
|

2/q

≤

CΘ2

M,j,q.

(cid:1)

ΘM,j,q ≤

CξcM −

τ +1.

6
AR APPROXIMATIONS TO NON-STATIONARY TIME SERIES

35

Consequently, by Jenson’s inequality, we have that

(S.29)

E

Xj −
|

XM
j

| ≤

CξcM −

τ +1, E

Xj −
|

XM
j

2
|

≤

Cξ2

c M −

2τ +2.

Therefore, we have that for some constant C > 0,

Var(XM
i )

−

Var(XM
i )

≤

CξcM −

τ +1,

where we use a discussion similar to (S.26). Similarly, we can show that

−
Together with Lemma S.6.1, we ﬁnd that

(cid:12)
(cid:12)

(cid:12)
(cid:12)
Cov(XM
i

(cid:12)
(cid:12)

, XM
j )

Cov(Xi, Xj)

CξcM −

τ +1.

≤

Cov(X)

k

−

Cov(XM )

k ≤

(cid:12)
(cid:12)

CpξcM −

τ +1.

As a result, we conclude that

(S.30)

Cov(X)

k

−

Cov(XM

)
k ≤

C(pξcM −

τ +1 + pnξ2

c M −
z

(q

2)

).

−

We decompose that
(S.31)
P(YΓY∗

x)

−

≤
(Y,

where

D

P(

YM Γ(

YM )∗

Y) is deﬁned as
e
(Y,

e

e

D

x) = P(YΓY∗

x)

−

≤

P(YΓY∗

x +

D

≤

(Y,

YM )),

≤

e

YM ) :=

−

YM Γ(

YM )∗ + YΓY∗.

By (S.30), a decomposition similar to (S.34) below and Bernstein’s inequality (see Example
2.11 of [40]), for some small constant δ > 0, we have with 1

δ) probability

e

e

e

O(n−

By Lemma S.6.6 and (S.31), we ﬁnd that with 1

(Y,

kD

YM )

k ≤

Cpnδξc(pξcM −

e
YM )

≤

(Y,

K

C

pξc(pξcM −

−
τ +1 + pξ2
c nM −
z
δ) probability

O(n−

(q

−

2)

).

−
τ +1 + pξ2

c nM −
z

(q

2)

)

−

1/2

.

Therefore, using the deﬁnition of
δ > 0,

e

(cid:16)
,
(
·
K

) in (S.24), we conclude that for some small constant
·

(cid:17)

(XM , Y)

K

≤

C

γ + p

7

4 n−

1/2M 3

z M 2 +

pξc(pξcM −

τ +1 + pξ2

c nM −
z

(q

2)

)

−

1/2

+ n−

δ.

(cid:18)
(X, XM ) to ﬁnish our
It is clear that we can choose γ = O
proof. We ﬁrst introduce some notations. Denote the physical dependence measure for zkl as
(cid:17)
δz
kl(s, q) and

. Finally, we control

(cid:16)
log n ξc
Mz

K

(cid:19)

(cid:16)

(cid:17)

θk,j,q = sup

k

δz
kl(s, q), Θs,l,q =

By (3.17), we conclude that

θo,l,q.

∞

o=s
X

(S.32)

Denote the set

sup
p
l
1

≤

≤

Θs,l,q < ξc,

∞

s=1
X

sup
p
l
1

≤

≤

sθs,l,3 < ξc.

(∆M ) :=

I

max
p
j
1
≤
≤

n

Xj −
(cid:12)
(cid:12)
(cid:12)

X (M )
j

∆M

.

o

≤

(cid:12)
(cid:12)
(cid:12)

36

We claim that for arbitrary small δ > 0, we can decompose the probability by
c(∆M ))

(XM , X) =

(XM , X

(XM , X

(∆M )) +

(S.33)

K

K
C

≤

∩ I
p∆M ξcnδ + n−

K
δ + P(
I

∩ I
c(∆M ))
(cid:17)

,

) to control the second term of the right-hand side of
·

(cid:16)p
where we use the deﬁnition of
,
(
·
K
(S.33). For the ﬁrst term, note that
P (XΓX

(XM )∗ΓXM

P

x

≤

−

(XM , M) is deﬁned as
(cid:1)

(cid:0)
where

D

x) = P

≤

(XM )∗ΓXM

x

≤

−

P

(XM )∗ΓXM

x +

D

≤

(XM , X)

,

(cid:0)
X∗ΓX + (XM )∗ΓXM .

(cid:1)

(cid:0)

(cid:1)

(XM , X) =

D

−

Further, we have

(S.34)

kD

(XM , M)

k ≤ k

(XM )∗Γ(XM

X)
k

+

(XM
k

X)∗ΓX

.
k

−

−
(∆M ), by Cauchy-Schwarz inequality, the fact Γ is

Recall (S.27) and (S.28). Restricted on
bounded, Lemma S.6.8 with (S.32), we ﬁnd that for some constant C > 0,

I

kD
Therefore, conditional on

P

(XM )∗ΓXM

(cid:0)

(cid:12)
(cid:12)

x

≤

−

(cid:1)

Moreover, we have

(XM , X)

k ≤

C√pξc(√p∆M ) = Cp∆M ξc.

(∆M ), for some constant C > 0, we have
δ

I
P (XΓX

Cn−

x)

≤

≤
+

(cid:12)
(cid:12)

P

(XM )∗ΓXM

(cid:0)

(cid:12)
(cid:12)
(cid:12)

x

≤

−

(cid:1)

P

(XM )∗ΓXM
(cid:16)

≤

x + nδp∆M ξc

(XM , Y) + P

2
K
≤

x + nδp∆M ξc

.

(cid:17)(cid:12)
(cid:12)
(cid:12)

x + nδp∆M ξc

(cid:17)

Y∗ΓY
(cid:16)
x) .

≤

P

(XM )∗ΓXM

x

≤

−

P

(XM )∗ΓXM
(cid:16)

≤

(cid:0)

(cid:1)

(cid:12)
(cid:12)
(cid:12)
Since Γ is positive deﬁnite and bounded, by Lemma S.6.6 and the rotation invariance property
of Gaussian random vectors, we obtain the bound for the ﬁrst term of the right-hand side of
(S.33). Next, by Markov inequality and a simple union bound, we have that

P ((Y∗ΓY

(cid:17)(cid:12)
(cid:12)
(cid:12)

≤

−

c(∆M ))

P(
I

C

≤

Θq
M,j,q
∆q
M

.

p

Xj=1

C

p∆M ξcnδ + n−

δ + pξcM −

qτ +1/∆q
M

.

Consequently, we can control

(XM , X)

K

≤

(cid:16)p
By optimizing ∆M , we conclude that

(XM , X)

K

≤

C

M

−qτ +1

2q+1 ξ(q+1)/(2q+1)

c

This ﬁnishes our proof using triangle inequality.

(cid:16)

(cid:17)

q+1

2q+1 n

p

δq

2q+1 + n−

δ

,

(cid:17)

Proof of Proposition 3.6. Denote r = Rank(Ω1/2ΓΩ1/2) and the eigenvalues of Ω1/2ΓΩ1/2

dr. Under (1) of Assumption S.3.1, the deﬁnition of W and the fact that

as d1 ≥

d2 >

· · · ≥
λmin(A)λmin(B)

λmin(AB)

≤

≤

λmax(AB)

≤

λmax(A)λmax(B),

AR APPROXIMATIONS TO NON-STATIONARY TIME SERIES

37

for any given positive semi-deﬁnite matrices A and B, we conclude that di = O(1), i =
1, 2,

, r. For the basis functions we used, we have that r = O(b
∗

c). Therefore, we have

· · ·

d1
f2 →

0.

Hence, by Theorem 3.4 and Lindeberg’s central limit theorem, we ﬁnish our proof.

Proof of Proposition 3.7. Denote the statistic

as

T

b∗

1

:=

φj(t)

T

φj(t)

−

−

0
Xj=1 Z
One one hand, by Proposition 3.6, we have that

(cid:16)

b

(cid:16) Z

2

dt.

(cid:17)(cid:17)

1

0

φj(s)

−

φj(s)ds

b

(0, 2).

n

T −

f1

f2 ⇒ N

On the other hand, by an elementary computation, we have

= nT + n

n

T

b∗

1

φj(t)

−

0

(cid:16)

Xj=1 Z

¯φj

2

dt

(cid:17)

2n

−

b∗

1

φj(t)

0

(cid:16)

Xj=1 Z

Furthermore, we can rewrite the above equation as

¯φj

−

φj(t)

(cid:17)(cid:16)

b

dt.

−

¯
φj

b

(cid:17)

n

T

= nT

n

−

By (3.6), we ﬁnd that

b∗

1

φj(t)

−

0

(cid:0)

(cid:1)

Xj=1 Z

¯φj

2 dt+2n

b∗

1

0

Xj=1 Z

(cid:0)

φj(t)

¯φj

−

(cid:1) (cid:16)

φj(t)

−

φj(t)

( ¯φj −

−

b

¯
φj)

dt.

(cid:17)

b

1

φj(t)

¯φj

−

0

Z
B is deﬁned as

(cid:0)

(cid:1) (cid:16)

where

φj(t)

φj(t)

−

( ¯φj −

−

b

¯
φj)

(cid:17)

b

dt = β∗j

B(βj −
b

βj) + O(b

c−

d),

∗

b

b

B =

1

0

(B(t)

−

¯B)(B(t)

¯B)∗dt.

−

Z

It is easy to see that
that

B
k

k

= O(1). Therefore, under the alternative hypothesis Ha, we ﬁnd

b

1

b
¯φj

φj(t)

( ¯φj −
where we use Theorem 4.3 and Assumption S.3.1. This concludes our proof of part one. For
part two, it follows directly from part one.

dt = OP

φj(t)

φj(t)

log n

(cid:1) (cid:16)

p

−

−

−

(cid:18)

(cid:19)

(cid:17)

b

b

Z

(cid:0)

0

∗

,

¯
φj)

(b

c)1/4
n

Proof of Theorem 3.9. We divide our proofs into two steps. In the ﬁrst step, we show that

the result holds for

deﬁned as

T

In the second step, we control the closeness between
with the ﬁrst step following the proof strategy of [47, Theorem 3]. Denote

and

b

T

T

deﬁned in (3.27). We start

:= Φ∗

ΓΦ,

T

Λ =

1
m

(n

−

m

n

−

)

b

∗

−

Xi=b∗+1

b
Υi,mΥ∗i,m,

38

where we use

Υi,m =

1
√m

Hi ⊗

B(

i
n

i+m

), Hi =

hj

.

(cid:17)
We ﬁrst propose and prove the following Lemmas S.4.2 and S.4.3.

(cid:16)

Xj=i

LEMMA S.4.2. Under the assumptions of Theorem 3.9, we have that for all b
∗

+ 1

i

≤

≤

n

m

−

PROOF. Using the basic property of Kronecker product, we ﬁnd

E

Υi,mΥ∗i,m

Υi,mΥ∗i,m −
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:16)

(cid:17)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= OP

b

∗

(cid:16)

.

ζ 2
c √m
(cid:17)

Υi,mΥ∗i,m =

As a consequence, we have that

1
m

[HiH ∗i ]

i
n

B(
(cid:20)

⊗

)B∗(

i
n

.

)
(cid:21)

(S.35)

E

Υi,mΥ∗i,m

Υi,mΥ∗i,m −
(cid:12)
(cid:12)
(cid:12)

(cid:16)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:17)(cid:12)
where we use the property of the spectrum of Kronecker product and the fact B( i
n )B∗( i
(cid:12)
(cid:12)
n )
(cid:12)
(cid:12)
is a rank-one matrix. Now we focus on studying the ﬁrst entry of HiH ∗i , which is of the
form w =
. We ﬁrst study its physical dependence measure. Note that w is
Fi+m,l). By (3.17) and
Fi+m measurable and can be written as fi(
Lemma S.6.8, we conclude that

Fi+m). Denote w(l) = fi(

i+m
j=i xj

(cid:16) P

(cid:17)(cid:12)
(cid:12)
(cid:12)

1ǫj

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:17)

(cid:16)

−

2

E

HiH ∗i

ζ 2
c
m

,

≤

HiH ∗i −
(cid:12)
(cid:12)
(cid:12)

i+m

xj

−

(S.36)

Xj=i
Recall that by Jensen’s inequality, if x
E

(S.37)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

q

1ǫj(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(E

Lq, q > 4, we have
2
|

q)2/q.
|

x
|

≤

∈
x
|

= O(√m).

Therefore, by (S.36), (S.37) and Minkowski’s inequality, we have

w

||

−

w(l)

||

By Lemma S.6.8 and (3.17), we have

l

= O(√m)

δ(j, q)

.

(cid:16)

m
Xj=l
−

(cid:17)

Therefore, by (S.35) and Lemma S.6.1, we conclude our proof.

Ew

= O(m3/2).

w

||

−

||

Using a discussion similar to the lemma above and by (3.29), it is easy to conclude that

Λ

||

−

E(Λ)
||

= OP

ζ 2
c

b

∗

m/n

.

(cid:16)
Next, we show that the covariance of a stationary time series can be used to approximate
, where the stationary time series can closely preserve the long-run covariance
HjH ∗j

p

(cid:17)

matrix (3.22). Recall (3.15). Denote the stationary time series as

(cid:17)

(S.38)

E

(cid:16)

hi,j = U(

i
n

,

Fj), i

≤

j

≤

i + m.

e

AR APPROXIMATIONS TO NON-STATIONARY TIME SERIES

39

Correspondingly, we can deﬁne

Υi,m =

1
√m

i+m

),

Hi =

hi,j.

B(

i
n

Xj=i

e

e

Hi ⊗
e

e

LEMMA S.4.3. Under the assumptions of Theorem 3.9, we have that for all b
∗

+ 1

i

≤

≤

n

m

−

E

Υi,mΥ∗i,m

E

Υi,m

Υ∗i,m

= O

−

(cid:16)
(cid:16)
e
PROOF. Similar to (S.35), we have

(cid:17)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

e

(cid:17)(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

2/τ

1

−

mb2
∗
n

(cid:17)

(cid:16)(cid:16)

ζ 2
c

b

∗

.

(cid:17)

ζ 2
c
m

.

−

E(HiH ∗i )
(cid:12)
(cid:12)
(cid:12)
(cid:12)
HiH ∗i , which is of the form
(cid:12)
(cid:12)

We also focus on studying the ﬁrst entry of

(cid:16)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
ǫj

1

i+m
j=i

xj

−

(cid:16) P

e

e

2

−

(cid:16) P
i+m

(cid:17)

E

Υi,mΥ∗i,m

E

Υi,m

−

(cid:17)

(cid:16)

e

i+m
j=i xj

1ǫj

−

2

(cid:17)

H ∗i )

≤

Hi

Υ∗i,m

E(
(cid:12)
(cid:12)
(cid:12)
(cid:12)
Hi
H ∗i −
e
e
(cid:12)
(cid:12)
. We ﬁrst observe that
e

(cid:17)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

e

e

i+m

xj

1

ǫj −

−

xj

1ǫj

−

= O



Xj=i
Hence, by Lemma S.6.8 and Assumption 2.14, we have

Xj=i (cid:16)



(cid:17)

e

e

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∞

xj

ǫj −
1(

−

ǫj)



.

e



i+m

xj

1

ǫj −

xj

1ǫj

−

−

Xj=i (cid:16)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:17)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
where we use the fact δ(j, 2)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
≤
(cid:12)
(cid:12)
(cid:12)
(cid:12)
have that

e

e

= O

√m

min
{

, δ(j, 2)
}
(cid:17)
δ(j, q). Hence, by (S.37) and Minkowski’s inequality, we

Xj=0

= O

(cid:16)

(cid:16)

(cid:17)

(cid:17)

(cid:16)

√m

,

1

2/τ

−

m
n

m
n

i+m

xj

2

ǫj

1

−

−

i+m

xj

−

Xj=i
This concludes our proof using Lemma S.6.1.

Xj=i

(cid:16)

(cid:17)

(cid:16)

e

e

= O

m

(cid:16)

(cid:16)

2/τ

1

−

m
n

(cid:17)

.

(cid:17)

2

1ǫj

(cid:17)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Furthermore, by [47, Lemma 4] and a discussion similar to (S.35), we have

Υi,m

Υ∗i,m

Ω(

−

i
n

)

⊗

B(

)B(

)∗

= O

i
n

i
n

(cid:19)(cid:12)
(cid:12)
Hence, by Assumption 2.10 and [37, Theorem 1.1], we have
(cid:12)
(cid:12)

(cid:18)

(cid:17)

(cid:16)

e

e

(cid:12)
(cid:12)
(cid:12)
(cid:12)

ζ 2
b
c
∗
m

.

(cid:17)

(cid:16)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
m
−

n

1
m

E

Υi,m

Υ∗i,m

Ω(t)

(B(t)B(t)∗) dt

= O

∗

b

n

−

−

b
(cid:13)
∗
(cid:13)
(cid:13)
We now come back to our proof of Theorem 3.9. Under (3.29), by Lemmas S.6.8, S.4.2 and
(cid:13)
(cid:13)
S.4.3, we have that

Xi=b∗+1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(n

−

⊗

−

−

(cid:16)

(cid:16)

(cid:17)

Z

e

e

0

)2

.

(cid:17)

ζ 2
b
c
∗
m

+

1
m

1

(S.39)

Λ

||

−

Ω

||

= OP

θ(m)

, θ(m) = b

It is easy to check that as τ > 4,

(cid:16)

(cid:17)

ζ 2
c

∗

(cid:18)r

m
n

+

1
√n

mb2
∗
n

(cid:16)

2/τ

1

−

+

(cid:17)

1
m

.

(cid:19)

1
√n

m
n

(cid:16)

(cid:17)

2/τ

1

−

1
m

,

≤

40

where we use the assumption that m
≪
mally distributed. Hence, we may write

n. By deﬁnition, conditional on the data, Φ is nor-

(0, Ip) and

where G
means that they have the same distribution. Deﬁne r =
Rank(Λ1/2
λr > 0. By (S.38)
and Assumption S.3.1, it is easy to see that λi = O(1) when conditional on the data. There-
fore, by Lindeberg’s central limit theorem, we have

≡
∼ N
ΓΛ1/2) and the eigenvalues of Λ1/2

λ2 ≥ · · · ≥

b

Λ1/2G,

Φ

≡

ΓΛ1/2 as λ1 ≥
b
r
i=1 λi

(0, 2).

⇒ N

dr > 0 are the eigenvalues of Ω1/2ΓΩ1/2 and note di = O(1).

ΓΛ1/2G
G∗Λ1/2
r
i=1 λ2
(

−
i )1/2
P

b
P

Recall that d1 ≥
Recall that r = O(b

d2 ≥ · · · ≥

c) and denote the set
r

∗

(S.40)

A ≡ An :=

|

(λi −

di)

| ≤

n

Xi=1
where bn, cn = o(1). On the event
A
ΓΛ1/2G
G∗Λ1/2
(

ΓΛ1/2G
f2

G∗Λ1/2

f1

−

=

p
, we have that

A ≡ An as
bn
c,
b

∗

b

(S.41)

=

b
ΓΛ1/2G
G∗Λ1/2
r
i=1 λ2
(

P
−
i )1/2
P

b
P

r

|

Xi=1

(λ2

i −

d2
i )

| ≤

cn

c

b

∗

,

p

o

r

i=1 λi −

f1

(

i )1/2

r
i=1 λ2
f2

!

P

r
i=1 λi +
i )1/2

−
r
i=1 λ2
P
r
i=1 λi

P

+ o(1).

Therefore, we have shown that Theorem 3.9 holds true on the event
discussion similar to (S.39) 1 and (2) of Lemma S.6.7, we ﬁnd that

. Under (3.29), using a

A

Consequently, we have that

b

Σ

||

−

Σ

||

= OP

ζc log n
√n

.

(cid:17)

(cid:16)

) = 1

o(1).

−

P(

A

T

Hence, we can conclude our proof for

using Theorem 3.4.

For the second step, by Theorems 2.4 and a discussion similar to [19, Theorem 3.7 and

Corollary 3.8], we conclude that

sup
i>b |

ǫi −

ǫb
i |

= OP(ϑ(n)), ϑn = n1/q

log n
n

ζc

b

∗

r

(cid:16)

da

+ n−

.

(cid:17)

Υi,m by replacing hi with

b

hi. By a discussion similar to Lemma S.4.3, we conclude

Denote
that

b

Hence, we have

b∗+1

sup
n
i

≤

≤

b
Υi,mΥ∗i,m −

m

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Υi,m

Υ∗i,m

= OP(b

ζ 2
c ϑn).

∗

b

b

1
√n

b

∗

(cid:16)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
ζ 2
c ϑn

.

(cid:17)

Λ

||

Λ

||

−

= OP

Using a discussion similar to (S.41), we can conclude our proof.

b

1The operator norm and the difference of trace share the same order as we apply Lemma S.6.1.

 
AR APPROXIMATIONS TO NON-STATIONARY TIME SERIES

41

S.4.3. Proofs of the main results of Section 4.

Proof of Theorem 4.2. Note that by adding and subtracting

xn+1, we have

n+1)2 = E(xn+1 −
xb

E(xn+1 −
xn+1)2 + E(
It sufﬁces to control the second and third terms of the above equations. First,
b

n+1)2 + 2E(xn+1 −
xb
b

xn+1 −

b

b

b

b
b

n

xn+1)(

xn+1 −

xb
n+1).

b

b

(S.42)

xn+1 −

xb
n+1 =

(φnj −

φj(1))xn+1

−

j +

φnjxn+1

−

j.

Xj=b+1

Xj=1

Therefore, by Theorem 2.11, (2.26) and (2.5), we ﬁnd that there exists some constant C > 0
such that

b

b

2

.

b2.5
n
(cid:19)
, then xn+1 −
, xn}

(S.43)

E(

xn+1 −

xb
n+1)2

≤

C

(log b)τ

1b−

−

(τ

3.5) +

−

(cid:18)

Second, since
b
uncorrelated with any linear combination of
obtain that

xn+1 is the best linear forecasting based on
xn+1 is
x1,
{
. Together with (S.42), we readily
, xn}

x1,
{

· · ·

· · ·

b

b

b

This completes our proof.

E(xn+1 −

xn+1)(

xn+1 −

xb
n+1) = 0.

b

b

b

Proof of Theorems 4.3. By a discussion similar to [19, Theorem 3.7 and Corollary 3.8],

we ﬁnd that

i
n

i
n

)

≤

≤

−

(S.44)

bζc

i>b,0

ϕj(

ϕj(

= OP

sup
j

)
(cid:12)
(cid:12)
(cid:12)
In fact, the only difference of the proof is that our design matrix Y is the (n
(b +
b
(cid:12)
×
Rb+1,
1)c rectangular matrix whose i-th row is xi ⊗
, xi
∈
B(i/n) = (α1( i
is the Kronecker product. Then it is easy to see
∈
that the proof follows from (S.44), (4.6) and the smoothness of ϕ(
). Together with (4.6), we
·
can conclude our proof.

n ). Here xi = (1, xi

Rc and

, αc( i

b)
b)

+ bc−

b (cid:12)
(cid:12)
(cid:12)
(cid:12)

B( i

n ))

−
−

n ),

· · ·

· · ·

r

!

1,

⊗

−

.

d

log n
n

APPENDIX S.5: CHOICES OF TUNING PARAMETERS

l

xi}
{

In this section, we discuss how to choose the parameters. As we have seen from (4.3) and
(3.6), we need to choose two important parameters in order to get an accurate prediction: b
and c. We use a data-driven procedure proposed in [4] to choose such parameters.
, we divide the time series into two parts: the
For a given integer l, say l =
3 log2 n
⌋
⌊
n
training part
l+1. With some preliminary initial pair
i=1 and the validation part
xi}
−
{
(b, c), we propose a sequence of candidate pairs (bi, cj), i = 1, 2,
, v, in
an appropriate neighbourhood of (b, c) where u, v are some given integers. For each pair of
the choices (bi, cj), we ﬁt a time-varying AR(bi) model (i.e., b = bi in (4.3)) with cj sieve
basis expansion using the training data set. Then using the ﬁtted model, we forecast the
time series in the validation part of the time series. Let
xn,ij be the forecast
of xn
l+1, ..., xn, respectively using the parameter pair (bi, cj). Then we choose the pair
(bi0, cj0) with the minimum sample MSE of forecast, i.e.,
b

, u, j = 1, 2,

l+1,ij,

n
i=n

xn

· · ·

· · ·

· · ·

b

−

−

−

,

(i0, j0) :=

((i,j):1

argmin
u,1
i

≤

≤

≤

1
l

v)

j

≤

n

(xk −

Xk=n
−

l+1

xk,ij)2.

b

 
42

Then we discuss how to choose m for practical implementation. In [47], the author used
the minimum volatility (MV) method to choose the window size m for the scalar covariance
function. The MV method does not depend on the speciﬁc form of the underlying time series
dependence structure and hence is robust to misspeciﬁcation of the latter structure [31]. The
Ω becomes stable when the block
MV method utilizes the fact that the covariance structure of
size m is in an appropriate range, where
(x1,

, xn)] is deﬁned as

(S.1)

Ω :=

1

−

(n

m

−

b + 1)m

m

n

−

Xi=b+1 h(cid:16)

Xj=i

b

· · ·
b

Ω = E[ΦΦ∗|
i+m
b

hi

⊗

(cid:16)

(cid:17)

B(

i
n

i+m

)

(cid:17)i

×

h(cid:16)

Xj=i

hi

⊗

(cid:17)

i
n

B(
(cid:16)

)

∗.

(cid:17)i

Therefore, it desires to minimize the standard errors of the latter covariance structure in a
suitable range of candidate m’s.

In detail, for a give large value mn0 and a neighborhood control parameter h0 > 0, we can
< mn0+h0
, n0 + h0. For each
Ωmj in the h0-neighborhood,

choose a sequence of window sizes m
and obtain
mj, j = 1, 2,
b
i.e.,

, mn0, we calculate the matrix norm error of

Ωmj by replacing m with mj in (3.25), j =

· · ·
h0 + 1, 2,

< m1 < m2 <

< mn0 <

h0+1 <

· · ·

· · ·

· · ·

· · ·

−

−

se(mj) := se(
Ωmj+k }
{
b

h0
k=

) =

h0

−

1
2h0

"

h0

h0
Xk=
−

b
Ωmj −
k
b

Ωmj+kk
b

2

1/2

,

#

where

Ωmj =

h0
k=

h0

−

P

b

b

Ωmj +k/(2h0 + 1). Therefore, we choose the estimate of m using
se(m).

m := argmin
m1

m

mn0

≤
Note that in [47] the author used h0 = 3 and we also adopt this choice in the current paper.

≤

b

APPENDIX S.6: SOME AUXILIARY LEMMAS

In this section, we collect some preliminary lemmas which will be used for our technical
proofs. First of all, we collect a result which provides a deterministic bound for the spectrum
n, let Ri =
of a square matrix. Let A = (aij) be a complex n
×
be the sum of the absolute values of the non-diagonal entries in the i-th row.
C be a closed disc centered at aii with radius Ri. Such a disc is called a

n matrix. For 1

=i |

≤

≤

i

j

aij|
Let D(aii, Ri)
P
⊆
Gershgorin disc.

LEMMA S.6.1 (Gershgorin circle theorem). Every eigenvalue of A = (aij) lies within at

least one of the Gershgorin discs D(aii, Ri), where Ri =

j

aij|

=i |

.

The next lemma provides a lower bound for the eigenvalues of a Toeplitz matrix in terms
of its associated spectral density function. Since the autocovariance matrix of any stationary
time series is a Toeplitz matrix, we can use the following lemma to bound the smallest eigen-
value of the autocovariance matrix. It will be used in the proof of Proposition 2.9 and can be
found in [44, Lemma 1].

P

LEMMA S.6.2. Let h be a continuous function on [

minimum and maximum, respectively. Deﬁne ak =
ΓT = (as

T . Then

t)1

s,t

−

≤

≤

π, π]. Denote by h and h its
T matrix

ikθdθ and the T

−
π
π h(θ)e−

×

2πh

≤

λmin(ΓT )

≤

2πh.

≤

−
R
λmax(ΓT )

6
6
AR APPROXIMATIONS TO NON-STATIONARY TIME SERIES

43

The following lemma indicates that, under suitable condition, the inverse of a banded
matrix can also be approximated by another banded-like matrix. It will be used in the proof
of Theorem 2.4 and can be found in [16, Proposition 2.2]. We say that A is m-banded if

Aij = 0, if

i
|

−

j

|

> m/2.

LEMMA S.6.3. Let A be a positive deﬁnite, m-banded, bounded and bounded invertible
matrix. Let [a, b] be the smallest interval containing the spectrum of A. Set r = b/a, q =
1)/(√r + 1) and set C0 = (1 + r1/2)2/(2ar) and λ = q2/m. Then we have
(√r
1)ij| ≤

(A−
|

Cλ|

−

|,

−

j

i

where

C := C(a, r) = max

a−
{

1, C0}
.

The following lemma provides an upper bound for the error of solutions of perturbed
linear system. It can be found in the standard numerical analysis literature, for instance see
[7]. It will be used in the proof of Theorem 2.4. Recall that the conditional number of a
diagonalizable matrix A is deﬁned as

κ(A) =

λmax(A)
λmin(A)

.

LEMMA S.6.4. Consider a matrix A and vectors x, v which satisfy the linear system

Ax = v.

Suppose that we add perturbations on both A and v such that

(A + ∆A)(x + ∆x) = v + ∆v.

Assuming that there exists some constant C > 0, such that

holds. Then we have that

κ(A)

κ(A) ||

∆A
||A

≤

1

−

C,

∆x
x

C

||
||
The following lemma provides Gaussian approximation result on convex sets for the sum
of an m-dependent sequence, which is [22, Theorem 2.1]. It will be used in the proof of
Theorem 3.4.

||
|| (cid:19)

||
||

||
||

||
||

≤

(cid:18)

.

∆A
A

∆v
+ ||
v
||

LEMMA S.6.5. Let W =

n

i=1 Xi be a sum of d-dimensional random vectors such that

i

E(Xi) = 0 and Cov(W ) = Σ. Suppose W can be decomposed as follows:
1.
2.
3.

P
[n], such that W
Nij ⊂
Ni ⊂
Nij,
∃
∈

−
[n], such that W
Nijk ⊂
Nij ⊂

[n] such that W

i
[n],
Ni ⊂
∀
∈
∃
∈
i
[n], j
Ni,
∀
∈
∈
∃
Ni, k
i
[n], j
∀
∈
∈
.
Xi, Xj, Xk}
{

XNij is independent of

XNi is independent of Xi, where [n] =

−

−

1,
· · ·
{
.
Xi, Xj}
{

, n

.
}

XNijk is independent of

44

Suppose further that for each i
Xi| ≤
|

∈
β,

[n], j
∈
Ni| ≤
|

Ni, k

Nij,
∈
Nij| ≤
|

n1,

n2,

Nijk| ≤
|

n3,

where
that

| · |

is the Euclidean norm of a vector. Then there exists a universal constant C such

P(W

A)

P(Σ1/2Z

A)

Cd1/4n

1/2

Σ−

∈

≤
where Z is a d-dimensional Gaussian random vector preserving the covariance structure of
W and where

denotes the collection of all the convex sets in Rd. .

∈A (cid:12)
(cid:12)
(cid:12)

−

(cid:12)
(cid:12)
(cid:12)

∈

||

3β3n1(n2 +
||

n3
d ),

sup
A

A

The following lemma offers a control for the summation of Chi-square random variables,

which will be employed in the proof of Theorem 3.4. It can be found in [45, Lemma S.2].

LEMMA S.6.6. Let a1 ≥

ap ≥
random variables. Then for all h > 0, we have

a2 ≥ · · · ≥

0 such that

p
i=1 a2

i = 1; let ηi be i.i.d. χ2
1

P

P(t

sup
t

≤

p

Xk=1

akηk ≤

t + h)

≤

√h

4/π.

p

Next, we collect some preliminary results. The ﬁrst part of the following lemma shows that
the covariance function (2.27) decays polynomially fast under suitable assumptions. It can
be found in [19, Lemma 2.6]. The second part shows that the sample covariance matrix and
its inverse will converge to some deterministic limits. Its proof is similar to equation (B.6) in
the supplementary ﬁle of [19] and we omit the details here.

LEMMA S.6.7.

(1). Suppose (3.17) and Assumptions 2.1, 2.10 and 2.14 hold true. Then

there exists some constant C > 0, such that

sup
t

γ(t, j)
|

| ≤

Cj−

τ , j

1.

≥

(2). Recall (3.16). Suppose (3.17) and Assumptions 2.1, 2.10, 2.14 and S.3.1 hold true. Then
we have that

Σ

||

−

Σ

||

= OP

where

Σ = n−

1Y ∗Y.

b

ζc log n
√n

,

(cid:17)

(cid:16)

b

Finally, we collect the concentration inequalities for non-stationary process using the phys-
ical dependence measure. It is the key ingredient for the proof of most of the theorems and
lemmas. It can be found in [47, Lemma 6].

LEMMA S.6.8. Let xi = Gi(

i

∈

q <

1, ηi) and ηi,
(
, ηi
· · ·
−
maxi E
xi|
∞
|
where
1, η′i
k := (
Fi,i
−
δx(k) = 0 if k < 0. Write γk =
(i). For q′ = min(2, q),

−

−

−

k

Fi =
Z are i.i.d random variables. Suppose that Exi = 0 and
Gi(
Fi,i
−
−
. We further let

) is a measurable function and
Fi), where Gi(
·
for some q > 1. For some k > 0, let δx(k) := max1
η′i}
Fi
{
i
j=1 xj.

· · ·
k
i=0 δx(i). Let Si =

, ηi) for an i.i.d copy

n k
≤
ηi}
{

Fi)

Gi(

≤
of

k,

i

P

P

q′
Snk
q ≤
k

Cq

∞

n
Xi=
−

(γi+n −

γi)q′

.

k)

kq ,

AR APPROXIMATIONS TO NON-STATIONARY TIME SERIES

45

(ii). If ∆ :=

∞j=0 δx(j) <

, we then have

∞

P

q ≤
In (i) and (ii), Cq are generic ﬁnite constants which only depend on q and can vary from
place to place.

(cid:13)
(cid:13)
(cid:13)
(cid:13)

n |

max
i
1
≤
≤

Si|
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Cqn1/q′

∆.

APPENDIX S.7: EXAMPLES OF SIEVE BASIS FUNCTIONS

In this section, we provide a list of some commonly used basis functions. We also refer to

[9, Section 2.3] for a more detailed discussion.
(1). Normalized Fourier basis. For x
als

∈

[0, 1], consider the following trigonometric polynomi-

1, √2 cos(2kπx), √2 sin(2kπx),

· · ·

, k

N.

∈

n

We note that the classical trigonometric basis function is well suited for approximating peri-
odic functions on [0, 1].
(2). Normalized Legendre polynomials [3]. The Legendre polynomial of degree n can be
obtained using Rodrigue’s formula

o

−
In this paper, we use the normalized Legendre polynomial

−

Pn(x) =

1
2nn!

dn
dxn (x2

1)n,

1

1.

x

≤

≤

P ∗n (x) =

1,

(

2n+1

2 Pn(2x

n = 0;

1),

, n > 0.

−

The coefﬁcients of the Legendre polynomials can be obtained using the R package mpoly
and hence they are easy to implement in R.
(3). Daubechies orthogonal wavelet [14, 15]. For N
class D

N, a Daubechies (mother) wavelet of

L2(R) deﬁned by

N is a function ψ

∈

q

−

∈

2N

1

−

ψ(x) := √2

Xk=1

1)kh2N

(
−

kϕ(2x

1

−

−

k),

−

where h0, h1,
conditions

, h2N
1 ∈
· · ·
1
N
k=0 h2k = 1
−
√2

−

R are the constant (high pass) ﬁlter coefﬁcients satisfying the
=

N
k=0 h2k+1, as well as, for l = 0, 1,

, N

1

−

1

· · ·

−

P

P
2N
−

1+2l

hkhk

2l =

−

1,
0,

(

l = 0,
= 0.
l

Xk=2l

1) and satis-
And ϕ(x) is the scaling (father) wavelet function is supported on [0, 2N
2N
ﬁes the recursion equation ϕ(x) = √2
k), as well as the normalization
k=0 hkϕ(2x
−
= l. Note that the ﬁlter coefﬁcients can
R ϕ(x)dx = 1 and
l)dx = 0, k
−
P
be efﬁciently computed as listed in [15]. The order N , on the one hand, decides the support
R
of our wavelet; on the other hand, provides the regularity condition in the sense that

R ϕ(2x

k)ϕ(2x

−

−

−

R

1

xjψ(x)dx = 0, j = 0,

R

, N, where N

d.

≥

· · ·

Z

We will employ Daubechies wavelet with a sufﬁciently high order when forecasting in our
simulations and data analysis. The basis functions can be either generated using the library

6
6
46

PyWavelets in Python 2 or the wavefun in the Wavelet Toolbox of Matlab. In the
present paper, to construct a sequence of orthogonal wavelet, we will follow the dyadic con-
struction of [14]. For a given Jn and J0, we will consider the following periodized wavelets
on [0, 1]

(S.1)

ϕJ0k(x), 0

n

2J0

k

≤

≤

1; ψjk(x), J0 ≤

j

Jn −

≤

−

1, 0

≤

k

2j

≤

−

1

, where

k), ψjk(x) = 2j/2

−

Z
Xl
∈

o
ψ(2j x + 2jl

k),

−

ϕJ0k(x) = 2J0/2

ϕ(2J0x + 2J0l

Z
Xl
∈

or, equivalently [28]

(S.2)

ϕJnk(x), 0

k

≤

≤

n

2Jn

−

1

.

o

REFERENCES

[1] BAXTER, G. (1962). An Asymptotic Result for the Finite Predictor. Mathematica Scandinavica 10 137-144.
[2] BAXTER, G. (1963). A norm inequality for a “ﬁnite-section” Wiener-Hopf equation. Illinois J. Math. 7

97–103.

[3] BELL, W. W. (2004). Special Functions for Scientists and Engineers (Dover Books on Mathematics). Dover

Publications.

[4] BISHOP, C. M. (2013). Pattern Recognition and Machine Learning. Information science and statistics.

Springer.

[5] BROCKWELL, P. and DAVIS, R. (1987). Time series: Theory and Methods. Springer-Verlag.
[6] CAI, T. T., LIU, W. and ZHOU, H. H. (2016). Estimating sparse precision matrix: Optimal rates of conver-

gence and adaptive estimation. Ann. Statist. 44 455–488.

[7] CHANDRASEKARAN, S. and IPSEN, I. C. F. (1995). On the Sensitivity of Solution Components in Linear

Systems of Equations. SIAM Journal on Matrix Analysis and Applications 16 93-112.

[8] CHEN, L. and FANG, X. (2011). Multivariate Normal Approximation by Stein’s Method: The Concentration

Inequality Approach. arXiv preprint arXiv:1111.4073.

[9] CHEN, X. (2007). Large Sample Sieve Estimation of Semi-nonparametric Models. Chapter 76 in Handbook

of Econometrics, Vol. 6B, James J. Heckman and Edward E. Leamer.

[10] CHEN, X. and CHRISTENSEN, T. M. (2015). Optimal uniform convergence rates and asymptotic normality
for series estimators under weak dependence and weak conditions. Journal of Econometrics 188 447 -
465.

[11] CHEN, X., XU, M. and WU, W. B. (2013). Covariance and precision matrix estimation for high-

dimensional time series. Ann. Statist. 41 2994–3021.

[12] DAHLHAUS, R. (2012). Locally stationary processes. In Handbook of statistics, 30 351–413. Elsevier.
[13] DAS, S. and POLITIS, D. N. (2021). Predictive Inference for Locally Stationary Time Series With an

Application to Climate Data. J. Amer. Statist. Assoc. 116 919–934.

[14] DAUBECHIES, I. (1988). Orthonormal bases of compactly supported wavelets. Commun. Pure Appl. Math.

41 909-996.

[15] DAUBECHIES, I. (1992). Ten Lectures on Wavelets. SIAM series: CBMS-NSF Regional Conference Series

in Applied Mathematics.

[16] DEMKO, S., MOSS, W. and SMITH, P. (1984). Decay Rates for Inverses of Band Matrices. Math. Comput.

43 491-499.

[17] DETTE, H., PREUBB, P. and VETTER, M. (2011). A Measure of Stationarity in Locally Stationary Pro-

cesses With Applications to Testing. J. Am. Stat. Assoc. 106 1113-1124.

[18] DETTE, H. and WU, W. (2020). Prediction in locally stationary time series. Journal of Business & Economic

Statistics 1–12.

[19] DING, X. and ZHOU, Z. (2020). Estimation and inference for precision matrices of nonstationary time

series. Ann. Statist. 48 2455–2477.

2For

functions, we
http://wavelets.pybytes.com, where the library PyWavelets is also introduced there.

of Daubechies wavelet

visualization

families

the

for

refer

to

AR APPROXIMATIONS TO NON-STATIONARY TIME SERIES

47

[20] DWIVEDI, Y. and RAO, S. S. (2011). A test for second–order stationarity of a time series based on the

discrete Fourier transform. Journal of Time Series Analysis 32 68-91.

[21] FAN, J. and YAO, Q. (2003). Nonlinear Time Series: Nonparametric and Parametric Methods. Springer.
[22] FANG, X. (2016). A Multivariate CLT for Bounded Decomposable Random Vectors with the Best Known

Rate. Journal of Theoretical Probability 29 1510–1523.

[23] FRYZLEWICZ, P., VAN BELLEGEM, S. and VON SACHS, R. (2003). Forecasting non-stationary time series
by wavelet process modelling. Annals of the Institute of Statistical Mathematics 55 737–764.
[24] KLEY, T., PREUSS, P. and FRYZLEWICZ, P. (2019). Predictive, ﬁnite-sample model choice for time series

under stationarity and non-stationarity. Electron. J. Stat. 13 3710–3774.

[25] KOLMOGOROV, A. N. (1941). Stationary Sequences in Hilbert Space. Bull. Moscow State U. Math. 2 1–40.
[26] KOLMOGOROV, A. N. (1941). Interpolation and extrapolation of stationary random sequences. Izv. Akad.

Nauk SSSR., Ser. Mat. 5 3–14.

[27] LIU, W. and LIN, Z. (2009). Strong approximation for a class of stationary processes. Stochastic Process.

Appl. 119 249-280.

[28] MEYER, Y. (1990). Ondelettes et opérateurs. I. Actualités Mathématiques. Hermann, Paris Ondelettes.
[29] NASON, G. (2013). A test for second–order stationarity and approximate conﬁdence intervals for localized

autocovariances for locally stationary time series. J.R. Statist. Soc. B 75 879-904.

[30] PAPARODITIS, E. (2010). Validating Stationarity Assumptions in Time Series Analysis by Rolling Local

Periodograms. Journal of the American Statistical Association 105 839-851.

[31] POLITIS, D. N., WOLF, D. N. P. J. P. R. M., ROMANO, J. P., WOLF, M., BICKEL, P. J., DIGGLE, P. and

FIENBERG, S. (1999). Subsampling. Springer Series in Statistics. Springer New York.

[32] QUANDT, R. (1972). A new approach to estimating switching regressions. J. Am. Stat. Assoc. 67 306-310.
[33] ROSENBLATT, M. (1952). Remarks on a Multivariate Transformation. Ann. Math. Statist. 23 470–472.
[34] ROUEFF, F. and SANCHEZ-PEREZ, A. (2018). Prediction of weakly locally stationary processes by auto-

regression. Lat. Am. J. Probab. Math. Stat. 15 1215–1239.

[35] SHERK, J. (2014). Not Looking for Work: Why Labor Force Participation Has Fallen During the Recovery.

https://www.heritage.org/jobs-and-labor/report/not-looking-work-why-labor-force-participation-has-fallen-during-the-recovery.

[36] STONE, C. J. (1982). Optimal Global Rates of Convergence for Nonparametric Regression. Ann. Statist. 10

1040–1053.

[37] TASAKI, H. (2009). Convergence rates of approximate sums of Riemann integrals. J. Approx. Theory 161

477-490.

[38] TONG, H. (2011). Threshold models in time series analysis - 30 years on. Stat. Interface 4 107-118.
[39] VOGT, M. (2012). Nonparametric regression for locally stationary time series. Ann. Statist. 40 2601–2633.
[40] WAINWRIGHT, M. J. (2019). High-dimensional statistics. Cambridge Series in Statistical and Probabilistic

Mathematics 48. Cambridge University Press, Cambridge A non-asymptotic viewpoint.

[41] WIENER, N. (1949). Extrapolation, interpolation and smoothing of stationary time series. New York, Wi-

ley.

[42] WOLD, H. (1954). A study in the analysis of stationary time series. Almqvist and Wiksell, Stockholm 2d

ed, With an appendix by Peter Whittle.

[43] WU, W. (2005). Nonlinear system theory: Another look at dependence. Proc Natl Acad Sci U S A. 40

14150-14151.

[44] XIAO, H. and WU, W. B. (2012). Covariance matrix estimation for stationary time series. Ann. Statist. 40

466–493.

[45] XU, M., ZHANG, D. and WU, W. B. (2019). Pearson’s chi-squared statistics: approximation theory and

beyond. Biometrika 106 716-723.

[46] YUAN, M. (2010). High Dimensional Inverse Covariance Matrix Estimation via Linear Programming. J.

Mach. Learn. Res. 11 2261–2286.

[47] ZHOU, Z. (2013). Heteroscedasticity and Autocorrelation Robust Structural Change Detection. J. Am. Stat.

Assoc. 108 726-740.

[48] ZHOU, Z. and WU, W. (2009). Local linear quantile estimation for non-stationary time series. Ann. Stat. 37

2696-2729.

[49] ZHOU, Z. and WU, W. (2010). Simultaneous inference of linear models with time varying coefﬁcents. J.R.

Statist. Soc. B 72 513-531.

