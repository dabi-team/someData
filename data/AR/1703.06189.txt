TURN TAP: Temporal Unit Regression Network for Temporal Action Proposals

Jiyang Gao1∗

Zhenheng Yang1∗ Chen Sun2 Kan Chen1 Ram Nevatia1

1University of Southern California

2Google Research

{jiyangga, zhenheny, kanchen, nevatia}@usc.edu, chensun@google.com

7
1
0
2

g
u
A
4

]

V
C
.
s
c
[

2
v
9
8
1
6
0
.
3
0
7
1
:
v
i
X
r
a

Abstract

Temporal Action Proposal (TAP) generation is an im-
portant problem, as fast and accurate extraction of se-
mantically important (e.g. human actions) segments from
untrimmed videos is an important step for large-scale video
analysis. We propose a novel Temporal Unit Regression
Network (TURN) model. There are two salient aspects of
TURN: (1) TURN jointly predicts action proposals and re-
ﬁnes the temporal boundaries by temporal coordinate re-
gression; (2) Fast computation is enabled by unit feature
reuse: a long untrimmed video is decomposed into video
units, which are reused as basic building blocks of tempo-
ral proposals. TURN outperforms the previous state-of-the-
art methods under average recall (AR) by a large margin
on THUMOS-14 and ActivityNet datasets, and runs at over
880 frames per second (FPS) on a TITAN X GPU. We fur-
ther apply TURN as a proposal generation stage for existing
temporal action localization pipelines, it outperforms state-
of-the-art performance on THUMOS-14 and ActivityNet.

1. Introduction

We address the problem of generating Temporal Action
Proposals (TAP) in long untrimmed videos, akin to gener-
ation of object proposals in images for rapid object detec-
tion. As in the case for objects, the goal is to make action
proposals have high precision and recall, while maintaining
computational efﬁciency.

There has been considerable work in action classiﬁca-
tion task where a “trimmed” video is classiﬁed into one of
speciﬁed categories [24, 29]. There has also been work
on localizing the actions in a longer, “untrimmed” video
[6, 23, 35, 33], i.e. temporal action localization. A straight-
forward way to use action classiﬁcation techniques for lo-
calization is to use temporal sliding windows, however there
is a trade-off between density of the sliding windows and
computation time. Taking cues from the success of pro-

∗ indicates equal contributions.

Figure 1.
Temporal action proposal generation from a long
untrimmed video. We propose a Temporal Unit Regression Net-
work (TURN) to jointly predict action proposals and reﬁne the
location by temporal coordinate regression.

posal frameworks in object detection tasks [7, 21], there has
been recent work for generating temporal action proposals
in videos [23, 4, 2] to improve the precision and accelerate
the speed of temporal localization.

State-of-the-art methods [23, 2] formulate TAP gener-
ation as a binary classiﬁcation problem (i.e.
action vs.
background) and apply sliding window approach as well.
Denser sliding windows usually would lead to higher re-
call rates at the cost of computation time. Instead of bas-
ing on sliding windows, Deep Action Proposals (DAPs) [4]
uses a Long Short-term Memory (LSTM) network to en-
code video streams and infer multiple action proposals in-
side the streams. However, the performance of average re-
call (AR), which is computed by the average of recall at
temporal intersection over union (tIoU) between 0.5 and 1,
suffers at small number of predicted proposals compared
with the sliding window based method [23] 1.

To achieve high temporal localization accuracy and efﬁ-
cient computation cost, we propose to use temporal bound-
ary regression. Boundary regression has been a successful
practice for object localization, as in [21]. However, tempo-
ral boundary regression for actions has not been attempted
in the past work.

We present a novel method for fast TAP genera-
tion: Temporal Unit Regression Network (TURN). A long

groundtruthslidingwindowpredictionlocationrefinementTimeline 
 
 
 
 
 
untrimmed video is ﬁrst decomposed into short (e.g. 16
or 32 frames) video units, which serve as basic process-
ing blocks. For each unit, we extract unit-level visual fea-
tures using off-the-shelf models (C3D and two-stream CNN
model are evaluated) to represent video units. Features from
a set of contiguous units, called a clip, are pooled to cre-
ate clip features. Multiple temporal scales are used to cre-
ate a clip pyramid. To provide temporal context, clip-level
features from the internal and surrounding units are con-
catenated. Each clip is then treated as a proposal candidate
and TURN outputs a conﬁdence score, indicating whether
it is an action instance or not. In order to better estimate
the action boundary, TURN outputs two regression offsets
for the starting time and ending time of an action in the
clip. Non-maximum suppression (NMS) is then applied to
remove redundant proposals. The source code is available
at https://github.com/jiyanggao/TURN-TAP.
DAPs [4] and Sparse-prop [2] use Average Recall vs. Aver-
age Number of retrieved proposals (AR-AN) to evaluate the
TAP performance. There are two issues with AR-AN met-
ric: (1) the correlation between AR-AN of TAP and mean
Average Precision (mAP) of action localization was not ex-
plored ; (2) the average number of retrieved proposals is
related to average video length of the test dataset, which
makes AR-AN less reliable when evaluating across differ-
ent datasets. Spatio-temporal action detection [34, 30] used
Recall vs. Proposal Number (R-N), however this metric
does not take video lengths into consideration.

There are two criteria for a good metric: (1) it should be
capable of evaluating the performance of different methods
on the same dataset effectively; (2) it should be capable of
evaluating the performance of the same method across dif-
ferent datasets (generalization capability). We should ex-
pect better TAP would lead to better localization perfor-
mance, using the same localizer. We propose a new metric,
Average Recall vs. Frequency of retrieved proposals (AR-
F), for TAP evaluation. In Section 4.2, we validate that the
proposed method satisﬁes the two criteria by quantitative
correlation analysis between TAP performance and action
localization performance.

We test TURN on THUMOS-14 and ActivityNet for
TAP generation. Experimental results show that TURN out-
performs the previous state-of-the-art methods [4, 23] by a
large margin under AR-F and AR-AN. For run-time perfor-
mance, TURN runs at over 880 frames per second (FPS)
with C3D features and 260 FPS with ﬂow CNN features on
a single TITAN X GPU. We further plug TURN as a pro-
posal generation step in existing temporal action localiza-
tion pipelines, and observe an improvement of mAP from
state-of-the-art 19% to 25.6% (at tIoU=0.5) on THUMOS-
14 by changing only the proposals. State-of-the-art local-

1Newly released evaluation results from DAPs authors show that

SCNN-prop [23] outperforms DAPs.

ization performance is also achieved on ActivityNet. We
show state-of-the-art performance on generalization capa-
bility by training TURN on THUMOS-14 and transfer it to
ActivityNet without ﬁne-tuning, strong generalization capa-
bility is also shown by test TURN across different subsets
in ActivityNet without ﬁne-tuning.

In summary, our contributions are four-fold:
(1) We propose a novel architecture for temporal action

proposal generation using temporal coordinate regression.
(2) Our proposed method achieves high efﬁciency (>800
fps) and outperforms previous state-of-the-art methods by a
large margin.

(3) We show state-of-the-art generalization performance
of TURN across different action datasets without dataset
speciﬁc ﬁne-tuning.

(4) We propose a new metric, AR-F, to evaluate the per-
formance of TAP and compare AR-F with AR-AN and AR-
N by quantitative analysis.

2. Related Work

Temporal Action Proposal. Sparse-prop [2] proposes
the use of STIPs [15] and dictionary learning for class-
independent proposal generation. S-CNN [23] presents a
two-stage action localization system, in which the ﬁrst stage
is temporal proposal generation, and shows the effective-
ness of temporal proposals for action localization. S-CNN’s
proposal network is based on ﬁne-tuning 3D convolutional
networks (C3D) [27] to binary classiﬁcation task. DAPs [4]
adopts LSTM networks to encode a video stream and pro-
duce proposals inside the video stream.

Temporal Action Localization. Based on the progress
of action classiﬁcation, temporal action localization has
been received much attentions recently. Ma et al.
[17]
address the problem of early action detection. They pro-
pose to train a LSTM network with ranking loss and merge
the detection spans based on the frame-wise prediction
scores generated by the LSTM. Singh et al.
[25] extend
two-stream [24] framework to multi-stream bi-directional
LSTM networks and achieved state-of-the-art performance
on MPII-Cooking dataset [22]. Sun et al.
[26] transfer
knowledge from web images to address temporal localiza-
tion in untrimmed web videos. S-CNN [23] presents a two-
stage action localization framework: ﬁrst using proposal
networks to generate temporal proposals and then score the
proposals with localization networks, which is trained with
classiﬁcation and localization loss.

Spatio-temporal Action Localization. A handful of ef-
forts have been seen in spatio-temporal action localization.
Gkioxari et al. [9] extract proposals from RGB images with
SelectiveSearch [28] and then apply R-CNN [8] on both
RGB and optical ﬂow images for action detection. Wein-
zaepfel et al. [31] replace SelectiveSearch [28] with Edge-
Boxes [36]. Mettes et al. [18] propose to use sparse points

Figure 2. Architecture of Temporal Unit Regression Network (TURN). A long video is decomposed into short video units, and CNN
features are calculated for each unit. Features from a set of contiguous units, called a clip, are pooled to create clip features. Multiple
temporal scales are used to create a clip pyramid at an anchor unit. TURN takes a clip as input, and outputs a conﬁdence score, indicating
whether it is an action instance or not, and two regression offsets of start and end times to reﬁne the temporal action boundaries.

as supervision for action detection to save tedious annota-
tion work.

Object Proposals and Detection. Object proposal gen-
eration methods can be classiﬁed into two types based the
features they use. One relies on hand-crafted low-level vi-
sual features, such as SelectiveSearch [28] and Edgebox
[36]. R-CNN [8] and Fast R-CNN [7] are built on this type
of proposals. The other type is based on deep ConvNet fea-
ture maps, such as RPNs [21], which introduces the use of
anchor boxes and spatial regression for object proposal gen-
eration. YOLO [20] and SSD [16] divide images into grids
and regress object bounding boxes based on the grid cells.
Bounding box coordinate regression is a common design
shared in second type of object proposal frameworks. In-
spired by object proposals, we adopt temporal regression in
action proposal generation task.

3. Methods

In this section, we will describe the Temporal Unit Re-

gression Network (TURN) and the training procedure.

3.1. Video Unit Processing

As we discussed before, the large-scale nature of video
proposal generation requires the solution to be computa-
tionally efﬁcient. Thus, extracting visual feature for the
same window or overlapped windows repeatedly should be
avoided. To accomplish this, we use video units as the ba-
sic processing units in our framework. A video V contains
T frames, V = {ti}T
1 , and is divided into T /nu consecu-
tive video units , where nu is the frame number of a unit.
A unit is represented as u = {ti}sf +nu
, where sf is the
starting frame, sf + nu is the ending frame. Units are not

sf

overlapped with each other.

Each unit is processed by a visual encoder Ev to get a
unit-level representation fu = Ev(u). In our experiments,
C3D [27], optical ﬂow based CNN model and RGB image
CNN model [24] are investigated. Details are given in Sec-
tion 4.2.

3.2. Clip Pyramid Modeling

su−nctx and {uj}eu+nctx

A clip (i.e. window) c is composed of units, c =
{uj}su+nc
, where su is the index of starting unit and nc
su
is the number of units inside c. eu = su + nc is the index
of ending unit, and {uj}eu
su is called internal units of c. Be-
sides the internal units, context units for c are also modeled.
{uj}su
are the context before and af-
ter c respectively, nctx is the number of units we consider
for context. Internal feature and context feature are pooled
from unit features separately by a function P . The ﬁnal fea-
ture fc for a clip is the concatenation of context features and
the internal features; fc is given by
fc = P ({uj}su

) (cid:107) P ({uj}eu+nctx

) (cid:107) P ({uj}eu
su

su−nctx

eu

eu

)

where (cid:107) represents vector concatenation and mean pooling
is used for P . We scan an untrimmed video by building
window pyramids at each unit position, i.e. an anchor unit.
A clip pyramid p consists of temporal windows with differ-
ent temporal resolution, p = {cnc}, nc ∈ {nc,1, nc,2, ...}.
Note that, although multi-resolution clips would have tem-
poral overlaps, the clip-level features are computed from
unit-level features, which are only calculated once.

3.3. Unit-level Temporal Coordinate Regression

The intuition behind temporal coordinate regression is
that human can infer the approximate start and end time

of an action instance (e.g. shooting basketball, swing golf)
without watching the entire instance, similarly, neural net-
works might also be able to infer the temporal boundaries.
Speciﬁcally, we design a unit regression model that takes a
clip-level representation fc as input, and have two sibling
output layers. The ﬁrst one outputs a conﬁdence score in-
dicating whether the input clip is an action instance. The
second one outputs temporal coordinate regression offsets.
The regression offsets are

L1 distance is adopted. l∗
i is the label, 1 for positive sam-
ples and 0 for background samples. Npos is the number of
positive samples. The regression loss is calculated only for
positive samples.

During training, the background to positive samples ratio
is set to be 10 in a mini-batch. The learning rate and batch
size are set as 0.005 and 128 respectively. We use the Adam
[14] optimizer to train TURN.

os = sclip − sgt, oe = eclip − egt

(1)

4. Evaluation

where sclip, eclip is the index of starting unit and ending
unit of the input clip; sgt, egt is the index of starting unit
and ending unit of the matched ground truth.

There are two salient aspects in our coordinate regres-
sion model. First, instead of regressing the temporal coordi-
nates at frame-level, we adopt unit-level coordinate regres-
sion. As the basic unit-level features are extracted to encode
nu frames, the feature may not be discriminative enough
to regress the coordinates at frame-level. Comparing with
frame-level regression, unit-level coordinate regression is
easier to learn and more effective. Second, in contrast to
spatial bounding box regression, we don’t use coordinate
parametrization. We directly regress the offsets of the start-
ing unit coordinates and the ending unit coordinates. The
reason is that objects can be re-scaled in images due to cam-
era projection, so the bounding box coordinates should be
ﬁrst normalized to some standard scale. However, actions’
time spans can not be easily rescaled in videos.

3.4. Loss Function

For training TURN, we assign a binary class label (of be-
ing an action or not) to each clip (generated at each anchor
unit). A positive label is assigned to a clip if: (1) the win-
dow clip with the highest temporal Intersection over Union
(tIoU) overlaps with a ground truth clip; or (2) the window
clip has tIoU larger than 0.5 with any of the ground truth
clips. Note that, a single ground truth clip may assign pos-
itive labels to multiple window clips. Negative labels are
assigned to non-positive clips whose tIoU is equal to 0.0
(i.e. no overlap) for all ground truth clips. We design a
multi-task loss L to jointly train classiﬁcation and coordi-
nates regression.

L = Lcls + λLreg

(2)

where Lcls is the loss for action/background classiﬁcation,
which is a standard Softmax loss. Lreg is for temporal co-
ordinate regression and λ is a hyper-parameter. The regres-
sion loss is

Lreg =

1
Npos

N
(cid:88)

i=1

i |(os,i − o∗
l∗

s,i) + (oe,i − o∗

e,i)|

(3)

In this section, we introduce the evaluation metrics, ex-

perimental setup and discuss the experimental results.

4.1. Metrics

We consider three different metrics to assess the quality
of TAP, the major difference is in the way to consider the
retrieve number of proposals: Average Recall vs. Number
of retrieved proposals (AR-N) [34, 11], Average Recall vs.
Average Number of retrieved proposals (AR-AN) [4], Av-
erage Recall vs. Frequency of retreived proposals (AR-F).
Average Recall (AR) is calculated as a mean value of recall
rate at tIoU between 0.5 and 1.

AR-N curve.

In this metric, the numbers of retrieved
proposals (N) for all test videos are the same. This curve
plots AR versus number of retrieved proposals.

(cid:80)n

AR-AN curve.

In this metric, AR is calculated as a
function of average number of retrieved proposals (AN).
AN is calculated as: Θ = ρΦ, ρ ∈ (0, 1].
In which,
Φ = 1
i=1 Φi is the average number of all proposals of
n
test videos. ρ is the ratio of picked proposals to evaluate. n
is the number of test videos and Φi is the number of all pro-
posals for each video. By scanning the ratio ρ from 0 to 1,
the number of retrieved proposals in each video varies from
0 to number of all proposals and thus the average number of
retrieved proposals also varies.

AR-F curve. This is the new metric that we propose. We
measure average recall as a function of proposal frequency
(F), which denotes the number of retrieved proposals per
second for a video. For a video of length li and proposal
frequency of F , the retrieved proposal number of this video
is Ri = F li.

We also report Recall@X-tIoU curve: recall rate at X
with regard to different tIoU. X could be number of re-
trieved proposals (N), average number of retrieved propos-
als (AN) and proposal frequency (F).

For the evaluation of temporal action localization, we
follow the traditional mean Average Precision (mAP) metric
used in THUMOS-14 and ActivityNet. A prediction is re-
garded as positive only when it has correct category predic-
tion and tIoU with ground truth higher than a threshold. We
use the ofﬁcial evaluation toolkit provided by THUMOS-14
and ActivityNet.

4.2. Experiments on THUMOS-14

Datasets.

The temporal action localization part of
THUMOS-14 contains over 20 hours of videos from 20
sports classes. This part consists of 200 videos in valida-
tion set and 213 videos in test set. TURN model is trained
on the validation set, as the training set of THUMOS-14
contains only trimmed videos.

Experimental setup. We perform the following exper-
iments: (1) different temporal proposal evaluation metrics
are compared; (2) the performance of TURN and other TAP
generation methods are compared under evaluation metrics
(i.e AR-F and AR-AN) mentioned above; (3) different TAP
generation methods are compared on the temporal action
localization task with the same localizer/classiﬁer. Speciﬁ-
cally, we feed the proposals into a localizer/classiﬁer, which
outputs the conﬁdence scores of 21 classes (20 classes
of action plus background). Two localizer/classiﬁers are
adopted: (a) SVM classiﬁers: one-vs-all linear SVM clas-
siﬁers are trained for all 21 classes using C3D fc6 features;
(b) S-CNN localizer: the pre-trained localization network
of S-CNN [23] is adopted.

For TURN model, the context unit number nctx is 4, λ
is 2.0, the dimension of middle layer fm is 1000, temporal
window pyramids is built with {1, 2, 4, 8, 16, 32} units. We
test TURN with different unit sizes nu ∈ {16, 32}, and dif-
ferent unit features, including C3D [27], optical ﬂow based
CNN feature and RGB CNN feature [24]. The NMS thresh-
old is set to be 0.1 smaller than tIoU in evaluation. We im-
plement TURN model in Tensorﬂow [1].

Comparison of different evaluation metrics. To val-
idate the effectiveness of different evaluation metrics, we
compare AR-F, AR-N, AR-AN by a correlation analysis
with localization performance (mAP). We generate seven
different sets of proposals, including random proposals, sli-
dinig windows and variants of S-CNN [23] proposals (de-
tails are given in the supplementary material). We then test
the localization performance using the proposals, as shown
in Figure 3 (a)-(c). SVM classiﬁers are used for localiza-
tion.

A detailed analysis of correlation and video length is
given in Figure 3 (d). The test videos are sorted by video
lengths and then divided evenly into four groups. The aver-
age video length of the group is the x-axis, and y-axis repre-
sents the correlation coefﬁcient between action localization
performance and TAP performance of the group. Each point
in 3 (d) represents the correlation of TAP and localization
performance of one group under different evaluation met-
rics. As can be observed in Figure 3, the correlation coef-
ﬁcient between mAP and AR-F is consistently higher than
0.9 at all video lengths. In contrast, correlation of AR-N
and mAP is affected by video length distribution. Note that,
AR-AN also shows a stable correlation with mAP, this is
partially because the TAP generation methods we use gen-

Figure 3. (a)-(c) show the correlation between temporal action
localization performance and TAP performance under different
metrics.
(d) shows correlation coefﬁcient between temporal ac-
tion localization and TAP performance versus video length on
THUMOS-14 dataset.

erate proportional numbers of proposals to video length.

To assess generalization, assume that we have two differ-
ent datasets, S0 and S1, whose average number of all pro-
posals are Φ0 and Φ1 respectively. As introduced before,
average number of retrieved proposals Θ = ρΦ, ρ ∈ (0, 1]
is dependent on Φ. When we compare AR at some certain
AN = Θx between S0 and S1, as Φ0 and Φ1 are different,
we need to set different ρ0 and ρ1. It means that the ratios
between retrieved proposals and all generated proposals are
different for S0 and S1, which make the AR calculated for
S0 and S1 at the same AN = Θx can not be compared
directly. For AR-F, the number of proposals retrieved is
based on “frequency”, which is independent with the aver-
age number of all generated proposals.

In summary, AR-N cannot evaluate TAP performance
effectively on the same dataset, as number of retrieved
proposals should vary with video lengths. AR-AN can-
not be used to compare TAP performance among different
datasets, as the retrieval ratio depends on dataset’s video
length distribution, which makes the comparison unreason-
able. AR-F satisﬁes both requirements.

Comparison of visual features. We test TURN with
three unit-level features to assess the effect of visual fea-
tures on AR performance: C3D [27] features, RGB CNN
features with temporal mean pooling and dense ﬂow CNN
[32] features. The C3D model is pre-trained on Sports1m
[13], all 16 frames in a unit are input into C3D and the
output of f c6 layer is used as unit-level feature. For RGB
CNN features, we uniformly sample 8 frames from a unit,
extract “Flatten 673” features using a ResNet [10] model

(a)(b)(c)(d)Figure 4. Comparison of TURN variants on THUMOS-14 dataset

(pre-trained on training set of ActivityNet v1.3 dataset [32])
and compute the mean of these 8 features as the unit-level
feature. For dense ﬂow CNN features, we sample 6 con-
secutive frames at the center of a unit and calculate optical
ﬂow [5] between them. The ﬂows are then fed into a BN-
Inception model [32, 12] that is pre-trained on training set of
ActivityNet v1.3 dataset [32]. The output of “global pool”
layer of BN-Inception is used as the unit-level feature.

As shown in Figure 4, dense ﬂow CNN feature (TURN-
FL) gives the best results, indicating optical ﬂow can cap-
In contrast,
ture temporal action information effectively.
RGB CNN features (TURN-RGB) show inferior perfor-
mance and C3D (TURN-C3D) gives competitive perfor-
mance.

Temporal context and unit-level coordinate regres-
sion. We compare four variants of TURN to show the effec-
tiveness of temporal context and unit regression: (1) binary
cls w/o ctx: binary classiﬁcation (no regression) without the
use of temporal context, (2) binary cls w/ ctx: binary clas-
siﬁcation (no regression) with the use of context, (3) frame
reg w/ ctx: frame-level coordinate regression with the use
of context and (4) unit reg w/ ctx: unit-level coordinate re-
gression with the use of context (i.e. our full model). The
four variants are compared with AR-F curves. As shown
in Figure 4, temporal context helps to classify action and
background by providing additional information. As shown
in AR-F curve, unit reg w/ ctx has higher AR than the
other variants at all frequencies, indicating that unit-level re-
gression can effectively reﬁne the proposal location. Some
TURN proposal results are shown in Figure 6.

Comparison with state-of-the-art. We compare TURN
with the state-of-the-art methods under AR-AN, AR-F,
Recall@AN-tIoU, Recall@F-tIoU metrics. The TAP gener-
ation methods include DAPs [4], SCNN-prop [23], Sparse-
prop [2], sliding window, and random proposals. For DAPs,
Sparse-prop and SCNN-prop, we plot the curves using the
proposal results provided by the authors. “Sliding window
proposals” include all sliding windows of length from 16
to 512 overlapped by 75%, each window is assigned with
a random score. “Random proposals” are generated by as-
signing random starting and ending temporal coordinates
(ending temporal coordinate is larger than starting temporal

Figure 5. Proposal performance on THUMOS-14 dataset under 4
metrics: AR-F, AR-AN, Recall@F-tIoU, Recall@AN-tIoU. For
AR-AN and Recall@AN-tIoU, we use the codes provided by [4]

coordinate), each random window is assigned with a ran-
dom score. As shown in Figure 5, TURN outperforms the
state-of-the-art consistently by a large margin under all four
metrics.

How unit size affects AR and run-time performance?
The impact of unit size on AR and computation speed is
evaluated with nu ∈ {16, 32}. We keep other hyper-
parameters the same as in Section 4.2. Table 1 shows
comparison of the three TURN variants (TURN-FL-16,
TURN-FL-32, TURN-C3D-16) and three state-of-the-art
TAP methods, in terms of recall (AR@F=1.0) and run-time
(FPS) performance. We randomly select 100 videos from
THUMOS-14 validation set and run TURN-FL-16, TURN-
FL-32 and TURN-C3D-16 on a single Nvidia TITAN X
GPU. The run-time of DAPs [4] and SCNN-prop [23] are
provided in [4], which were tested on a TITAN X GPU and
a GTX 980 GPU respectively. The hardware used in [2] is
not speciﬁed in the paper.

Table 1. Run-time and AR Comparison on THUMOS-14.

method
DAPs [4]
SCNN-prop [23]
Sparse-prop [2]
TURN-FL-16
TURN-FL-32
TURN-C3D-16

AR@F=1.0 (%)
35.7
38.3
33.3
43.5
42.4
39.3

FPS
134.3
60.0
10.2
129.4
260.6
880.8

As can be seen, there is a trade-off between AR and FPS:
smaller unit size leads to higher recall rate, and also higher
computational complexity. We consider unit size as tempo-

AR-FAR-ANRecall@F-tIoURecall@AN-tIoUral coordinate precision, for example, unit size of 16 and 32
frames represent approximately half second and one second
respectively. The major part of computation time comes
from unit-level feature extraction. Smaller unit size leads to
more number of units, which increases computation time;
on the other hand, smaller unit size also increases temporal
coordinate precision, which improves the precision of tem-
poral regression. C3D feature is faster than ﬂow CNN fea-
ture, but with a lower performance. Compared with state-
of-the-art methods, we can see that TURN-C3D-16 outper-
forms current state-of-the-art AR performance, but acceler-
ates computation speed by more than 6 times. TURN-FL-
16 achieves the highest AR performance with competitive
run-time performance.

TURN for temporal action localization. We feed pro-
posal results of different TAP generation methods into the
same temporal action localizers/classiﬁers to compare the
quality of proposals. The value of mAP@tIoU=0.5 is re-
ported in Table 2. TURN outperforms all other methods in
both the SVM classiﬁer and S-CNN localizer. Sparse-prop,
SCNN-prop and DAPs all use C3D to extract features. It
is worth noting that the localization results of four differ-
ent proposals suit well with their proposal performance un-
der AR-F metric in Figure 5: the methods that have better
performance under AR-F achieve higher mAP in temporal
action localization.

Table 2. Temporal action localization performance (mAP %
@tIoU=0.5) evaluated on different proposals on THUMOS-14.
DAPs SVM[4] Our SVM S-CNN

Sparse-prop[2]
DAPs[4]
SCNN-prop[23]
TURN-C3D-16
TURN-FL-16

7.8
13.9
7.62
-
-

8.1
9.5
14.0
16.4
17.8

15.3
16.3
19.0
22.5
25.6

A more detailed comparison of state-of-the-art localiza-
tion methods is given in Table 3. It can be seen that, by
applying TURN with linear SVM classiﬁers for action lo-
calization, we achieve comparable performance with the
state-of-the-art methods. By further incorporating S-CNN
localizer, we outperform all other methods by a large mar-
gin at all tIoU thresholds. The experimental results prove
the high-quality of TURN proposals.

TURN helps action localization on two aspects:
(1)
TURN serves as the ﬁrst stage of a localization pipeline
(e.g. S-CNN, SVM) to generate high-quality TAP, and thus
increases the localization performance; (2) TURN acceler-
ates localization pipelines by ﬁltering out many background
segments, thus reducing the unnecessary computation.

2 This number should be higher, as DAPs authors adopted an incorrect

frame rate when using S-CNN proposals.

Table 3. Temporal action localization performance (mAP %) com-
parison at different tIoU thresholds on THUMOS-14.

0.1
tIoU
Oneata et al.[19]
36.6
Yeung et al.[33]
48.9
Yuan et al. [35]
51.4
S-CNN [23]
47.7
46.4
TURN-C3D-16 + SVM
TURN-FL-16 + SVM
48.3
TURN-C3D-16 +S-CNN 48.8
54.0
TURN-FL-16 + S-CNN

0.2
33.6
44.0
42.6
43.5
41.5
43.2
45.5
50.9

0.3
27.0
36.0
33.6
36.3
34.3
35.1
40.3
44.1

0.4
20.8
26.4
26.1
28.7
24.9
26.2
31.5
34.9

0.5
14.4
17.1
18.8
19.0
16.4
17.8
22.5
25.6

Figure 6. Qualitative examples of retrieved proposals by TURN on
THUMOS-14 dataset. GT indicates ground truth. TP and FP in-
dicate true positive and false positive respectively. “reg prop” and
“cls prop” indicate regression proposal and classiﬁcation proposal.

4.3. Experiments on ActivityNet

Datasets. ActivityNet datasets provide rich and diverse
action categories. There are three releases of ActivityNet
dataset: v1.1, v1.2 and v1.3. All three versions deﬁne a
5-level hierarchy of action classes. Nodes on higher level
represent more abstract action categories. For example,
the node “Housework” on level-3 has child nodes “Interior
cleaning”, “Sewing, repairing, & maintaining textiles” and
“Laundry” on level-4. From the hierarchical action cate-
gories deﬁnition, a subset can be formed by including all
action categories that belong to a certain node.

Experiment setup. To compare with previous work,
we do experiments on v1.1 (on subsets of “Works” and
“Sports”) for temporal action localization [3, 33], v1.2 for
proposal generalization capability following the same eval-
uation protocol as in [4]. On v1.3, we design a different ex-
perimental setup to test TURN’s cross-domain generaliza-
tion capability: four subsets having distinct semantic mean-
ings are selected, including “Participating in Sports, Exer-
cise, or Recreation”, “Vehicles”, “Housework” and “Arts
and Entertainment”. We also check that the action cat-
egories in different subsets are not semantically related:
for example, ”archery”, ”dodge ball” in “Sports” subset,

GTTP regpropTP clspropFP regpropFP clspropTimeTimeTime57.263.163.98.610.411.112.914.233.334.034.937.238.429.933.135.737.4115.2116.7123.9121.6122.557.661.9Table 4. Proposal generalization performance (AR@F=1.0 %) of
TURN-C3D-16 on different subsets of ActivityNet.
Arts
24.44
44.30
40.27
38.43
43.26
45.30

Sliding Windows
Arts (23; 685)
Housework (10; 373)
Vehicles (5; 238)
Sports (26; 1294)
Ensemble (64; 2590)

Housework
27.63
44.38
44.30
40.05
43.58
48.12

Vehicles
27.59
40.85
38.65
42.22
41.40
42.33

Sports
25.72
38.43
36.54
30.70
46.62
46.72

dows (lengths of 32, 64, 128, 256, 512, 1024 and 2048,
overlap 50% ) in each subset. Average recall at frequency
1.0 (AR@F=1.0) are reported in Table 4. The left-most col-
umn lists subsets used for training. The numbers of action
classes and training videos with each subset are shown in
brackets. The top row lists subsets for test. The off-diagonal
elements indicate that the training data and test data are
from different subsets; the diagonal elements indicate the
training data and test data are from the same subsets.

As can be seen in Table 4, the overall generalization ca-
pability is strong. Speciﬁcally, the generalization capabil-
ity when training on “Sports” subset is the best compared
with other subsets, which may indicate that more training
data would lead to better generalization performance. The
“Ensemble” row shows that using training data from other
subsets would not harm the performance of each subset.

TURN for temporal action localization. Temporal ac-
tion localization performance is evaluated and compared on
“Works” and “Sports” subsets of ActivityNet v1.1. TURN
trained with dense ﬂow CNN features is used for compari-
son. On v1.1, TURN-FL-16 proposal is fed into one-vs-all
SVM classiﬁers which trained with two-stream CNN fea-
tures. From the results shown in Table 5, we can see that
TURN proposals improve localization performance.

Table 5. Temporal action localization performance (mAP%
@tIoU=0.5) on ActivityNet v1.1
[3]
33.2
31.1

Sliding Windows
27.3
29.6

TURN-FL-16
37.1
41.2

Subsets
Sports
Work

[33]
36.7
39.9

5. Conclusion

We presented a novel and effective Temporal Unit Re-
gression Network (TURN) for fast TAP generation. We
proposed a new metric for TAP: Average Recall-Proposal
Frequency (AR-F). AR-F is robustly correlated with tem-
poral action localization performance and it allows perfor-
mance comparison among different datasets. TURN can
runs at over 880 FPS with the state-of-the-art AR perfor-
mance. TURN is robust on different visual features, in-
cluding C3D and dense ﬂow CNN features. We showed
the effectiveness of TURN as a proposal generation stage in
localization pipelines on THUMOS-14 and ActivityNet.

Figure 7. Comparison of generalizability on ActivityNet v1.2
dataset

”changing car wheels”, ”ﬁxing bicycles” in “Vehicles” sub-
set, ”vacuuming ﬂoor”, ”cleaning shoes” in “Housework”
subset, ”ballet”, ”playing saxophone” in “Arts” subset.

The evaluation metrics include AR@AN curve for tem-
poral action proposal and mAP for action localization.
AR@F=1.0 is reported for comparing proposal perfor-
mance on different subsets. The validation set is used for
testing as the test set is not publicly available.

To train TURN, we set the number of frames in a unit nu
to be 16, the context unit number nctx to be 4, L to be 6 and
λ to be 2.0. We build the temporal window pyramid with
{2, 4, 8, 16, 32, 64, 128} number of units. The NMS thresh-
old is set to be 0.1 smaller than tIoU in evaluation. For the
temporal action localizer, SVM classiﬁers are trained with
two-stream CNN features in “Sports” and “Works” subsets.
Generalization capability of TURN. One important
property of TAP is the expectation to generalize beyond the
categories it is trained on.

On ActivityNet v1.2, we follow the same evaluation pro-
tocol from [4]: model trained on THUMOS-14 validation
set and tested in three different sets of ActivityNet v1.2: the
whole set of ActivityNet v1.2 (all 100 categories), Activi-
tyNet v1.2 ∩ THUMOS-14 (on 9 categories shared between
the two) and ActivityNet v1.2 (cid:54) 1024 frames (videos with
unseen categories with annotations up to 1024 frames). To
avoid any possible dataset overlap and enable direct com-
parison, we use C3D (pre-trained on Sports1M) as feature
extractor, the same as DAPs did. As shown in Figure 7,
TURN has better generalization capability in all three sets.
On ActivityNet v1.3, we implement a different setup for
evaluating generalization capability on subsets that contain
semantically distinct actions: (1) we train TURN on one
subset and test on the other three subsets, (2) we train on the
ensemble of all 4 subsets and test on each subset. TURN
is trained with C3D unit features, to avoid any overlap of
training data. We also report performance of sliding win-

101102103Average number of retrieved proposals0.00.20.40.60.8Average RecallSliding WindowDAPs ActivityNetDAPs ActivityNet  1024 framesDAPs ActivityNet ∩ THUMOS-14TURN-C3D-16 ActivityNetTURN-C3D-16 ActivityNet  1024 framesTURN-C3D-16 ActivityNet ∩ THUMOS-14[21] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards
real-time object detection with region proposal networks. In
NIPS, 2015.

[22] M. Rohrbach, S. Amin, M. Andriluka, and B. Schiele. A
database for ﬁne grained activity detection of cooking activ-
ities. In CVPR, 2012.

[23] Z. Shou, D. Wang, and S.-F. Chang. Temporal action local-
ization in untrimmed videos via multi-stage cnns. In CVPR,
2016.

[24] K. Simonyan and A. Zisserman. Two-stream convolutional

networks for action recognition in videos. In NIPS, 2014.

[25] B. Singh, T. K. Marks, M. Jones, O. Tuzel, and M. Shao. A
multi-stream bi-directional recurrent neural network for ﬁne-
grained action detection. In CVPR, 2016.

[26] C. Sun, S. Shetty, R. Sukthankar, and R. Nevatia. Tempo-
ral localization of ﬁne-grained actions in videos by domain
transfer from web images. In ACM MM, 2015.

[27] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri.
Learning spatiotemporal features with 3d convolutional net-
works. In ICCV, 2015.

[28] J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W.
Smeulders. Selective search for object recognition. Interna-
tional journal of computer vision, 104(2):154–171, 2013.
[29] L. Wang, Y. Qiao, and X. Tang. Action recognition with
trajectory-pooled deep-convolutional descriptors. In CVPR,
2015.

[30] L. Wang, Y. Qiao, X. Tang, and L. Van Gool. Actionness esti-
mation using hybrid fully convolutional networks. In CVPR,
2016.

[31] P. Weinzaepfel, Z. Harchaoui, and C. Schmid. Learning to
track for spatio-temporal action localization. In ICCV, 2015.
[32] Y. Xiong, L. Wang, Z. Wang, B. Zhang, H. Song, W. Li,
D. Lin, Y. Qiao, L. Van Gool, and X. Tang. Cuhk & ethz &
siat submission to activitynet challenge 2016. arXiv preprint
arXiv:1608.00797, 2016.

[33] S. Yeung, O. Russakovsky, G. Mori, and L. Fei-Fei. End-
to-end learning of action detection from frame glimpses in
videos. In CVPR, 2016.

[34] G. Yu and J. Yuan. Fast action proposals for human action

detection and search. In CVPR, 2015.

[35] J. Yuan, B. Ni, X. Yang, and A. A. Kassim. Temporal action
localization with pyramid of score distribution features. In
CVPR, 2016.

[36] C. L. Zitnick and P. Doll´ar. Edge boxes: Locating object

proposals from edges. In ECCV, 2014.

References

[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen,
C. Citro, G. Corrado, A. Davis, J. Dean, M. Devin, et al.
Tensorﬂow: Large-scale machine learning on heterogeneous
distributed systems. 2015.

[2] F. Caba Heilbron, J. Carlos Niebles, and B. Ghanem. Fast
temporal activity proposals for efﬁcient detection of human
actions in untrimmed videos. In CVPR, 2016.

[3] F. Caba Heilbron, V. Escorcia, B. Ghanem, and J. Car-
los Niebles. Activitynet: A large-scale video benchmark for
human activity understanding. In CVPR, 2015.

[4] V. Escorcia, F. C. Heilbron, J. C. Niebles, and B. Ghanem.
Daps: Deep action proposals for action understanding.
In
ECCV, 2016.

[5] G. Farneb¨ack. Two-frame motion estimation based on poly-
In Scandinavian conference on Image

nomial expansion.
analysis. Springer, 2003.

[6] C. Gan, N. Wang, Y. Yang, D.-Y. Yeung, and A. G. Haupt-
mann. Devnet: A deep event network for multimedia event
detection and evidence recounting. In CVPR, 2015.

[7] R. Girshick. Fast r-cnn. In ICCV, 2015.
[8] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-
ture hierarchies for accurate object detection and semantic
segmentation. In CVPR, 2014.

[9] G. Gkioxari and J. Malik. Finding action tubes. In CVPR,

2015.

[10] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016.

[11] J. Hosang, R. Benenson, P. Doll´ar, and B. Schiele. What
makes for effective detection proposals? IEEE transactions
on pattern analysis and machine intelligence, 38(4):814–
830, 2016.

[12] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift.
2015.

[13] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,
and L. Fei-Fei. Large-scale video classiﬁcation with convo-
lutional neural networks. In CVPR, 2014.

[14] D. Kingma and J. Ba. Adam: A method for stochastic opti-

mization. In ICLR, 2015.

[15] I. Laptev. On space-time interest points. International Jour-

nal of Computer Vision, 64(2-3):107–123, 2005.

[16] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, and S. Reed.

Ssd: Single shot multibox detector. In ECCV, 2016.

[17] S. Ma, L. Sigal, and S. Sclaroff. Learning activity progres-
sion in lstms for activity detection and early detection.
In
CVPR, 2016.

[18] P. Mettes, J. C. van Gemert, and C. G. Snoek. Spot on:
Action localization from pointly-supervised proposals.
In
ECCV, 2016.

[19] D. Oneata, J. Verbeek, and C. Schmid. The lear submission

at thumos 2014. In ECCV Workshop, 2014.

[20] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi. You
only look once: Uniﬁed, real-time object detection.
In
CVPR, 2016.

