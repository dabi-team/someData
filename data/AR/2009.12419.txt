1
2
0
2

b
e
F
9

]

V
C
.
s
c
[

2
v
9
1
4
2
1
.
9
0
0
2
:
v
i
X
r
a

Towards General Purpose, Geometry Preserving Single-View Depth Estimation

Mikhail Romanov
m.romanov@samsung.com

Nikolay Patakin
n.patakin@samsung.com

Anna Vorontsova
a.vorontsova@samsung.com

Sergey Nikolenko
s.nikolenko@samsung.com

Dmitriy Senyushkin
d.senyushkin@samsung.com

Anton Konushin
a.konushin@samsung.com

Figure 1: Left to right: original image (from the Depth-in-the-Wild dataset, unseen during training), corresponding up-to-
scale inverse depth map predicted with the proposed B5-LRN model, and a point cloud built directly from this depth map.

Abstract

Single-view depth estimation (SVDE) plays a crucial role
in scene understanding for AR applications, 3D modeling,
and robotics, providing the geometry of a scene based on
a single image. Recent works have shown that a success-
ful solution strongly relies on the diversity and volume of
training data. This data can be sourced from stereo movies
and photos. However, they do not provide geometrically
complete depth maps (as disparities contain unknown shift
value). Therefore, existing models trained on this data are
not able to recover correct 3D representations. Our work
shows that a model trained on this data along with con-
ventional datasets can gain accuracy while predicting cor-
rect scene geometry. Surprisingly, only a small portion of
geometrically correct depth maps are required to train a
model that performs equally to a model trained on the full
geometrically correct dataset. After that, we train compu-

tationally efﬁcient models on a mixture of datasets using
the proposed method. Through quantitative comparison on
completely unseen datasets and qualitative comparison of
3D point clouds, we show that our model deﬁnes the new
state of the art in general-purpose SVDE.

1. Introduction

Single-view monocular depth estimation (SVDE) is an
essential problem of computer vision and visual under-
standing. It has numerous important applications in such
areas as simultaneous localization and mapping (SLAM),
navigation, computational photography,
robotics, aug-
mented reality, and many others. Approaches capable
of predicting geometrically correct depth maps are espe-
cially interesting for industrial applications as they would
potentially enable the users to construct a reliable three-
dimensional point cloud based on a single image. Unfor-

1

 
 
 
 
 
 
tunately, even state-of-the-art approaches cannot either pro-
vide correct scene geometry or perform robustly on arbi-
trary images.

Nevertheless, recent developments based on deep neural
networks have led to signiﬁcant progress in this area over
the last few years; see, e.g., the survey [48]. Up until very
recently, deep learning models had usually been trained and
tested either on synthetic datasets such as SYNTHIA [30]
or SunCG [35] or datasets with sensor-based or geometry-
based depth supervision, such as KITTI [22], NYU [23],
and others (we discuss existing depth datasets in detail in
Section 3).

From the other side, several recent works improved
depth estimation accuracy through ﬁnding new data
sources. Among them, stereo movies [29, 42] and photos
[44] are one of the most diverse and accessible at-scale data
sources. The depth data available in stereo movies is volu-
minous and diverse, but in order to derive from them geo-
metrically complete depth data one would have to know pre-
cise intrinsic and extrinsic parameters of the stereo cameras.
These parameters are not needed to consume stereo movies
and thus are usually not provided. Without them, disparity
from a stereo pair can be computed up to unknown shift and
scale coefﬁcients (UTSS). Such information can be used as
a good proxy for depth, but is insufﬁcient to restore geom-
etry. Due to this fact, state-of-the-art models that are cur-
rently trained using data from stereo movies can show good
performance and generalization properties but do not pro-
vide geometrically correct predictions [29, 42, 44],

In this work, we show that it is possible to use stereo
movies data even without precise camera parameters in
training networks that predict geometrically correct depth
maps. Moreover, we show that use of this large-scale
source of depth data from stereo movies alongside with a
small quantity of geometrically correct depth data (provided
by sensors or structure-from-motion reconstructions) in the
training process of an SVDE model is equivalent to training
the same model using a dataset of the same size but con-
taining only geometrically correct data. Inspiring by Eigen
et al. [9], we introduce a new scale-invariant pairwise loss
function, which outperforms existing data terms on NYU
[23].

Since depth estimation problem has many industrial ap-
plications, which means that a real-life depth estimation
network should be not only accurate but also computa-
tionally efﬁcient, we construct our solutions using Mo-
bileNet [13, 31] and EfﬁcientNet [38] networks as back-
bones and a modiﬁed Light-Weight Reﬁne Net [24] as a
decoder.

Following [29], we train our models on a mixture of
datasets, including DIML [15], MegaDepth [19], RedWeb
[44] and stereo movies, but preserve geometrically cor-
rect predictions. We test the resulting models on datasets

that have not been used for training (NYU[23], TUM [36],
ETH3D [33], DIW [4]). Our most accurate model (B5-
LRN) ourperforms MIDAS [29], while having 3.6x less pa-
rameters. Our fastest model (based on MobilenetV2 [31])
with only 2.4 million parameters can produce plausible 3D
geometry on a wide range of scenarios (see Fig. 4).

Our contribution. Firstly, we propose how models can
be trained on both geometrically complete and geometri-
cally incomplete data sources without loss of ability to pre-
dict correct scene geometry. Secondly, we propose a new
loss function, which outperforms existing on a NYU [23]
dataset. We train a set of scalable by computational com-
plexity models. The most accurate one (B5-LRN) out-
performs competitors, being a new state-of-the-art in the
general-purpose SVDE. Also, we show that the small net-
work based on MobilenetV2[31] can still generalize well in
a wide variety of scenarios.

The paper is organized as follows. Section 2 discusses
related work. In Section 3, we discuss existing datasets for
depth estimation and three different kinds of available depth
maps. In Section 4 we propose geometry-preserving train-
ing method and a new loss function, Section 5 presents the
experimental results, and Section 6 concludes the paper.

2. Related Work

Single-View Depth Estimation (SVDE) was studied for
decades. Early methods of general purpose depth estima-
tion from single RGB image applied complicated heuristic
algorithms with hand-crafted features [32, 12].

Recently, deep learning-based approaches were adopted
for solving various computer vision tasks including depth
estimation. The majority of modern approaches formulate
depth estimation as a dense labelling in continuous space
[8, 17, 3, 24]. In that case, L1- or L2-based regression loss
functions are used in different domains (depth, log-depth,
disparity).
[9] it is proposed to compare
pairwise differences of ground-truth and prediction. Since
it computed in log-depth domain, loss is invariant to scal-
ing. However, alternative formulations have also been con-
sidered: for instance, Fu et al. [10] proposed to discretize
depth and to interpret depth estimation as ordinal regression
problem.

In Eigen et al.

While the aforementioned works mainly deal with train-
ing models using sensor-measured depth, some other works
employ hand-labeled data [5], processed videos [29, 42] and
stereo movies [29] data for training. While being trained on
such data, models learn only ordinal depth rankings [4, 44]
or predicts disparity up to unknown scale and shift coefﬁ-
cients (UTSS) [42, 29].

2

Figure 2: Point clouds constructed with depth estimation models. Top row: an image from NYU. Second and third rows:
images from the DIW dataset [4]; the ground truth is not available in this case so it is impossible to align shift of MiDaS
predictions.

3. Depth Data and Depth Predictions

Datasets with absolute depth measurements. There are
many datasets and data sources for the SVDE problem,
and they present different kinds of data with different re-
strictions. First, several datasets provide images accompa-
nied by absolute depth measurements, usually taken with
special depth sensors such as LiDARs, time-of-ﬂight cam-
eras, structured light sensors, and others. Datasets that pro-
vide such depth data include KITTI [22], NYUv2 [23],
DIML [15], ETH3D [33], Standord 2D3DS [1], Scan-
Net [7], Sun3D [45], SunRGBD [34] and others. Such
datasets usually either do not provide diverse data (cap-
tures only indoor environments due to the limited sensing
range) or are expensive to collect at-scale and sparse (laser
or LiDAR-based scanners).

Another possible source of depth data are synthetic
datasets.
If the 3D model of a synthetic scene is avail-
able, precise depth measurements can be generated together
with rendered images at virtually no additional cost. There-
fore, almost all modern synthetic datasets for computer
vision include depth maps for their images; examples of
such datasets include Sintel [2] and SYNTHIA [30], Falling
Things [40] for basic objects, SunCG [35] and Habitat [21]
for indoor scenes, DeepDrive [28] and ProcSy [14] for out-
door scenes, PHAV [6] and SURREAL [41] for synthetic
humans, and many more; we refer to [26] for a detailed
overview.

Up-to-scale data. Up-to-scale (UTS) data is a different
type of data for the SVDE problem, where the depth mea-
surements are known up to an unknown constant (scale).
This means that a UTS depth map for an image d and an ab-
solute depth map d∗ are related as d∗−1 = C1d−1, where
C1 is an unknown constant. Note that UTS depth map rep-
resents the overall geometry of the scene, which means that
it is sufﬁcient to know the UTS depth map of a scene to
construct a reliable point cloud.

The most popular dataset that provides such depth data
is MegaDepth [19]. This dataset was collected using the
Structure-from-Motion (SfM) method [43] from crowd-
sourced images of architectural sights.

Up-to-shift-and-scale data (UTSS) can be derived from
stereo movies and stereo photos using state of the art opti-
cal ﬂow algorithms; this has been done in datasets such as
MiDaS [29], RedWeb[44] and WSVD [42].

For an aligned stereo pair, the optical ﬂow is usually re-
ferred to as disparity, and the disparity map is related to the
absolute depth map as follows:

d∗−1 = C1(D + C2),

where D is the disparity map and C1, C2 are unknown co-
efﬁcients. C2 is only known if we know the displacement
of the principal point between the left and right frames of a
stereo pair. Note that it is impossible to get absolute or UTS

3

Ours,B5-LRNMIDAS(w/o alignment)MIDAS,(w/ alignment)ImageVNL(trained on NYU)MegaDepthImpossibledue to GTabscenceImpossibledue to GTabscencedepth maps without knowing C2. Hence, it is impossible
to build a geometrically correct point cloud from this data
without a correct C2. More details on this can be found in
supplementary.

can be rewritten as

LSI = −

2
N 2

N
(cid:88)

i=1

R{i} (N − 1 − 2(i − 1)) ,

(2)

From UTSS to UTS. As we have seen above, if one needs
geometrically correct predictions one has to predict either
absolute or UTS depth maps. On the other hand, the prob-
lem of predicting absolute depth is ill-posed: it is impossi-
ble to detect the scale of a scene using only a single frame,
since inﬁnite number of scenes with different scale can be
projected into the same image. To correctly estimate scene
scale models are likely to learn size for different types of
objects. Unfortunately, absolute depth datasets for a wide
range of outdoor datasets are not available.

Since our goal is to learn geometrically correct depth es-
timations, we train our models in UTS mode. UTSS data
lacks only one value (C2) per image to be transformed to
UTS data. Using an incorrect value of C2 leads to incorrect
reconstruction of straight lines and angles between planes.
Thus, we may try to use UTSS data along with UTS data
during training, with the expectation that the neural network
will learn correct 3D geometry patterns from UTS data and
reuse them.

4. Loss functions

Scale Invariant Loss. There are several loss functions
commonly used for the depth estimation problem. One of
the most popular ones is the L2 pairwise loss function [8].
Interesting feature of this loss is that it takes into account all
the pairs of pixels on the image. In the same time pointwise
L1 loss functions are more robust as they pay less atten-
tion to outliers and known to work better in SVDE then the
pointwise L2 loss. We introduce the proposed pairwise L1
loss function below and then discuss its properties.

The pairwise L1 loss function for a single image can be

formulated as follows:

LSI =

1
N 2

N
(cid:88)

i,j=1

(cid:12)
(cid:12)(log di − log dj) − (log d∗

j )(cid:12)
i − log d∗
(cid:12) ,

(1)
where d is the predicted depth, d∗ is the ground truth depth,
i and j are pixel indices, and N is the number of pixels in
the image. Note that since we are subtracting logarithms
of depth values, the proposed pairwise L1 loss is scale-
invariant (SI) so it can be used for training on both absolute
and UTS inverse depth maps.

A direct calculation of this loss function would require a
summation of N 2 terms. However, it can be computed more
efﬁciently, in time O(N log N ). To do so we ﬁrst introduce
the differences between logarithmic depths Ri = log di −
log d∗
i . In the terms of the differences Ri, loss function (1)

where R{i} is a sorted list of Ri: R{i} ≥ R{j} if i > j. To
sort the list of differences, we need O(N log N ) operations,
and then LSI is computed in linear time, getting the over-
all computational cost for calculating the pairwise L1 loss
of O(N log N ). A full derivation of this loss function is
shown in supplementary. Despite the increased asymptotic
complexity of the loss function computation, in practice we
observe no more than 5% increase in training time.

Shift-and-Scale Invariant Loss. A scale-invariant (SI)
loss can be easily converted into a shift-and-scale invariant
(SSI) loss. We can replace the logarithm of the depth with
normalized disparity:

˜Di =

Di − µ
σ

,

where Di is the disparity value at pixel i, and µ and σ are
the mean and standard deviation for the image’s disparity
map:

µ =

1
N

N
(cid:88)

i=1

Di;

σ =

1
N − 1

N
(cid:88)

i=1

(Di − µ)2.

The ground truth disparity map is deﬁned by the optical ﬂow
between left and right frames, while the predicted disparity
map can be computed from the depth map as Di = 1/di.

With this in mind, we can deﬁne the SSI pairwise L1

loss:

LSSI =

1
N 2

N
(cid:88)

i,j=1

|( ˜Di − ˜Dj) − ( ˜D∗

i − ˜D∗

j )|.

loss

This
O(N log N ) with formula (2), substituting Di − D∗
of Ri.

function also can be computed in time
i instead

Mixing datasets. To train on UTS, absolute, and UTSS
data at the same time, we propose to use a mixture of SSI
and SI loss functions, using the latter when it is available.
Formally, we train our models with the following loss func-
tion:

LM ixture = IU T SLSI + LSSI ,
(3)
where IU T S = 1 for images with UTS or absolute data
and 0 for images with UTSS data. The SSI loss forces the
model to generalize and work well for the large-scale UTSS
datasets, while the SI loss forces the model to produce un-
biased estimates of inverse depth with correct geometry.

4

Dataset
DIML Indoor [15]
MegaDepth [19]
ReDWeb [44]
3D Movies [29]
Sintel [2]
NYUv2 Raw [23]
TUM-RGBD [36]
DIW [4]

Scene
type
indoor
general
general
general
general
indoor
indoor
general

Depth
type
absolute
UTS
UTSS
UTSS
absolute
absolute
absolute
ordinal

#Samples
220K
130K
3600
500K
1064
407K
80K
496K

Table 1: Overview of the datasets used in our experiments.
Top: training datasets, bottom: test datasets.

Since SI loss function (1) requires model to predict in log-
depth domain and UTSS disparities can not be converted
to depth without shift adjustment, our models make predic-
tions in log-disparity domain, which is a negative value of
log-depth.

5. Experimental evaluation

Network architectures.
In this work we use a modiﬁed
Light-Weight Reﬁne Net (LRN) architectures for our ex-
periments. We change the number of channels in CRP and
Fusion blocks according to the corresponding output of the
backbone in each layer for scalability purposes. As a back-
bone, as the model optimized for efﬁciency we use Mo-
bileNetV2 [31], and to increase accuracy we change the
backbone to a set of EfﬁcientNet architectures [38]. To
compare the efﬁciency, we compute the number of param-
eters and multiply-addition operations required to infer one
sample (in 384 × 384 resolution). In our approach, models
predict the logarithms of inverse depth log d−1. More de-
tails of the architecture can be found in the Supplementary.

Implementation details.
In all experiments, we use the
same set of augmentations. We apply resizing, padding, and
taking random crops to obtain samples of size 384×384,
and then use geometrical transformations (rotation, hori-
zontal ﬂip) and apply color distortions to images (gamma,
noise, brightness and contrast). The models were trained
using the Ranger optimizer, which is a combination of
Radam [20] with LookAhead [47]. We set the learning
rate to 10−3 and use batches of size 32 (except for mod-
els based on B3-B5, where we used batches of size 16),
with each batch consisting of as equal as possible number
of images from each training dataset. We deﬁne an epoch
as 10,000 training steps and train our models for 40 epochs.
We implemented all models in Python and PyTorch [27],
using EfﬁcientNets from the Segmentation Models Pytorch
library [46]. Experiments were run with an NVIDIA Tesla
P40 GPU.

Datasets. Following MIDAS [29], we train our models
on a mixture of four datasets, speciﬁcally MegaDepth [19]
(≈100k samples), DIML [15] (≈220k samples), which are
datasets with geometrically complete depth, RedWeb[44]
(3600 samples), and stereo movies; characteristics of all
datasets are summarized in Table 1. We extend the list of
stereo movies used in MIDAS [29] from 23 to 49. Ad-
ditionally, we process stereo images with a current state
of the art optical ﬂow estimation method, namely RAFT
[39]. This allows us to acquire more accurate disparities
with sharp edge boundaries. In total, we use ≈500k sam-
ples from stereo movies. A comparison of disparity maps
and the full list of movies are shown in the Supplementary.
We test our models on several datasets that were com-
pletely unseen during training. This test set includes the
NYU test set (654 images), the split of the TUM RGBD
dataset proposed by Li et al. [18] (1815 images), Sintel
dataset (1045 images), and DIW (74441 images). We ren-
der dense ground truth depth maps for ETH3D from recon-
structed point clouds, similar to [29]. Unfortunately, the
authors do not provide the resulting depth maps for this
dataset, so we have recomputed the metric for their model
in our version of ETH3D.

Metrics. To evaluate our method, we use standard metrics
for depth estimation. To evaluate the results on NYU and
TUM datasets, we use the δ1.25 error:

δ1.25 =

1
N

N
(cid:88)

I

i=1

(cid:20)

max

(cid:27)

(cid:26) d∗
i
di

,

di
d∗
i

(cid:21)

> 1.25

,

where I [x] = 1 if x is true and 0 otherwise. This metric can
be interpreted as the percentage of pixels where the depth
deviation from the target exceeds 25%.

For Sintel and ETH3D datasets, we use the rel metric:

rel =

1
M

M
(cid:88)

i=1

|d∗

i − di|
|d∗
i |

.

where d∗
i denotes ground truth depth in a pixel i and di de-
notes predicted depth in a pixel i. Lower values of this met-
ric indicate better prediction quality.

The DIW dataset contains only one pair of depth ordinal
ranking points per image. We evaluate WHDR (Weighted
Human Disagreement Rate), i.e., the percentage of incor-
rectly predicted depth ordinal rankings. Before computing
the metrics, we cap maximal depth in datasets to 10, 10, 80,
and 72 meters respectively. Before computing UTS metrics
for our approach, we align the median of the model’s log-
disparity prediction to match the ground truth. To compute
UTSS metrics, we use an approach similar to MIDAS [29],
aligning predictions using MSE criteria. We infer the depth

5

Loss function Predictions
L1-pair
L2-pair [9]
L1-point
L1-point
L2-point

UTS
UTS
UTS
absolute
absolute

log10
δ1.25
rel
0.0388
9.16
9.27
0.0402
9.47
9.97
0.0397
9.36
9.65
0.0419
9.91
10.36
11.37 10.43 0.0446

Table 2: Quantitative comparison of commonly used loss
functions (data terms) with the proposed L1-pairwise loss
on NYU [23] using MN-LRN model.

loss functions that we will use in large-scale training. Ex-
perimental results are presented in Table 2. These experi-
ments have shown that our new loss function yields better
results on all metrics.

This proof of concept experiment demonstrates that a
lack of geometrically correct data can be compensated for
by using extra UTSS data. Moreover, the proportion of
UTS and UTSS data does not affect the ﬁnal result much.
This means that one can use a relatively small amount
of geometrically correct data (provided, e.g., in NYUv2
or MegaDepth dataset) and accompany it with large-scale,
diverse UTSS datasets that can be found, e.g., in stereo
movies in virtually unlimited amounts. This experiment
suggests that a model trained with this mixture might be-
have as well as a model trained on a similar amount of data
all supplied with geometrically correct depth maps.

Along with an ablation study of the loss functions that
can be found in Table 2, we also perform yet another proof-
of-concept experiment. We compare two identical mod-
els, one trained to predict the absolute depth map (L1-
pointwise, absolute) and the other trained to predict the UTS
depth map (L1-pointwise, UTS). The latter model achieves
better values of UTS metrics.

Training on a mixture of datasets We train our net-
works on several datasets at the same time. Among train-
ing datasets used in this experiment, only MegaDepth and
DIML contain UTS data (see also a full comparison in Ta-
ble 1). Consequently, we can train on these two datasets in
UTS mode and on all the training datasets in UTSS mode.
In Table 3 we compare our models with commonly used
popular depth estimation models using UTS metrics. We
also compare our solution with the results of [29] in Ta-
ble 5. In Table 4 we compare the performance of models
with and without UTSS data.

In Fig. 2, we compare point clouds that are generated
using our B5-LRN model with point clouds built using other
popular models. We also show some point clouds generated
by our most efﬁcient network MN-LRN in Fig. 4. Note that
the images that we are using in these demonstrations are
taken from datasets unseen during training, showcasing the

Figure 3: We renormalize depth in the NYU dataset and
divide it into two parts: up-to-scale (geometrically com-
plete) and up-to-shift-and-scale (geometrically incomplete).
Trained with the proposed method, the model reaches the
same quality as the one trained on the full geometrically
complete dataset while using only 10-20% of UTS data.

on test images by ﬁrst resizing their smaller side to 384 pix-
els (e.g., 512 × 384 for the NYU and TUM datasets) and
upscaling them back before computing the metrics.

Ablation study. First, as a proof of concept we train a
UTS model on a mixture of UTS and UTSS data from the
NYUv2 Raw dataset. Since NYUv2 Raw contains absolute
depth, we can convert it to either UTS or UTSS. To convert
absolute depth to UTS, we multiply it by a random positive
coefﬁcient, and to obtain UTSS data we multiply the inverse
depth by a random scale and then shift it by a random addi-
tive value. To obtain comprehensive results, we have tested
our approach with several mixtures where UTS and UTSS
data are present in different proportions, that is, p% of the
source absolute depth data is converted to UTS data and the
rest is converted to UTSS data for a given p.

For this proof-of-concept experiment, we use a light-
weight model that consists of the MobileNet encoder and
LRN decoder. We train it on each UTS/UTSS mixture using
the loss function (3) and then evaluate it against the same
model that was trained only on the UTS part of data.

Results of this experiment are shown in Fig. 3. The
model trained on UTS data demonstrates expected be-
haviour: the more training data we use, the better results we
obtain. At the same time, the model trained on a mixture
of UTS and UTSS data shows similar results for all values
of p. In other words, it performs as if it was trained on the
dataset fully supplied with UTS data.

Loss functions. We have conducted a set of experiments
on the NYUv2 dataset in order to ﬁnd the best out of the

6

0%20%40%60%80%100%UTS data percentage0.100.150.200.250.300.35Relative errortrained on UTS datatrained on UTS+UTSS dataFigure 4: Sample images from NYU and DIW datasets (ﬁrst and third rows) and their corresponding 3D reconstructions
(second and fourth rows), obtained with the MN-LRN model. The model was not trained on these datasets yet is able to
produce plausible 3D geometry on a wide range of visual scenarios (from indoor to arbitrary outdoor images).

Method

NYU
(δ1.25)
Li et al. [19]
34,39
Mannequin [18]
23.42
Tiefenrausch [16]
32.8
14.64
MN-LRN
EfﬁcientNet-Lite0-LRN 14.15
13.84
EfﬁcientNet-B0-LRN
EfﬁcientNet-B1-LRN
12.80
13.04
EfﬁcientNet-B2-LRN
12.35
EfﬁcientNet-B3-LRN
EfﬁcientNet-B4-LRN
11.92
10.64
EfﬁcientNet-B5-LRN

TUM
(δ1.25)
33,11
22,39
35.4
15.13
14.41
15.95
15.03
15.36
14.38
13.55
13.05

ETH3D
(rel)
0.276
0.249
0.314
0.191
0.177
0.168
0.179
0.168
0.176
0.164
0.154

Sintel
(rel)
0.490
0.431
0.497
0.360
0.354
0.330
0.315
0.304
0.343
0.346
0.328

DIW
(WHDR)
24,55
26.52
25.54
15.02
14.59
13.15
12.71
13.06
12.95
12.81
12.56

Params
(mln)
5.4
5.4
3.6
2.4
3.6
4.2
6.7
8
11
18
29

MAdds
(109)
91.1
91.1
7
1.17
1.29
1.66
2.22
2.5
3.61
5.44
8.07

Table 3: Results of the UTS models trained on the datasets mixtures of UTS and UTSS data compared to other UTS single-
view depth estimation methods.

generalization capabilities of our models. The point clouds
produced using our method look competitive compared to
the other methods. More visualizations of the results are
provided in the Supplementary.

Discussion. Acquisition of UTS depth data is often a bot-
tleneck. At the same time, sources of UTSS stereo data

In our evaluation
are accessible, diverse, and plentiful.
study, we have seen that we can train a UTS model bet-
ter by supplementing the dataset with UTSS data. This
broadens the horizons for production-ready solutions. We
argue that stereo data gathered from multiple sources to-
gether with existing UTS and absolute depth datasets pro-
vide a solid basis for a versatile and robust SVDE method.

7

Model
MN-LRN
MN-LRN
EfﬁcientNet-Lite0-LRN
EfﬁcientNet-Lite0-LRN UTS+UTSS

Data
UTS
UTS+UTSS
UTS

NYU TUM ETH3D Sintel DIW
21.49
0.193
15.38
15.02
0.191
14.64
14.15
19.70
0.191
14.59
0.177
14.15

17.78
15.13
16.99
14.41

0.432
0.360
0.428
0.354

Table 4: Results of the models trained on UTS and combined UTS and UTSS data. For both models, UTS+UTSS data
combination signiﬁcantly improves prediction quality. At the same time, the models retain the ability to compensate for C2
since only scale-invariant metrics were used.

NYU TUM ETH3D Sintel DIW Params
Model name
105.4
MIDAS [29]
9.55
2.4
Ours, MN-LRN 10.97
29
Ours, B5-LRN

12.46
15.02
12.56

0.167
0.177
0.145

0.327
0.292
0.253

14.29
14.22
9.86

7.4

FLOPS
103.9
1.17
8.07

Table 5: Comparison of the proposed models with the MiDaS model [29]. For this comparison, UTSS metrics were used.

Figure 5: Qualitative comparison of depth maps produced by our models and existing competitors. Images are taken from
the DIW dataset and were not seen during training.

Obtaining depth maps via stereo matching has certain lim-
itations. First, precise disparities for distant objects can be
obtained only from a very wide stereo baseline. Second, the
method depends massively on the model used to estimate
the disparity. To estimate depth in the distance, it should be
sensitive enough to capture small displacements. Accord-
ingly, SVDE is not yet applicable for large-scale scenarios
such as outdoor landscapes.

Furthermore, our results indicate that lightweight models
are not yet able to produce sharp depth maps. Still, there
is a trade-off between the accuracy of depth estimates and
the complexity of the model that determines speed of the
inference, memory requirements, and power consumption.

8

6. Conclusion

In this work, we have shown that training geometry-
preserving SVDE models beneﬁts signiﬁcantly from the use
of voluminous data from stereo pairs. The resulting models
exhibit strong generalization capabilities and produce ro-
bust depth maps that can be used to generate natural point
clouds from a single input image. Although the results of
these models are still imperfect, they already yield quality
sufﬁcient for many practical applications.

We have also presented a family of models that produce
state of the art results for geometrically preserving depth
estimation on the majority of existing datasets. One of these
models is light-weight, computationally efﬁcient, and based
on a mobile-oriented backbone architecture. This enables
general purpose SVDE to be used on consumer devices.

InputLi et al. [4]Li & Snavely [5]MiDaS [6]Ours, MN-LRNOurs, B5-LRNReferences

[1] Iro Armeni, Sasha Sax, Amir R Zamir, and Silvio Savarese.
Joint 2d-3d-semantic data for indoor scene understanding.
arXiv preprint arXiv:1702.01105, 2017.

[2] Daniel Butler, Jonas Wulff, Garrett Stanley, and Michael
Black. A naturalistic open source movie for optical ﬂow
evaluation. pages 611–625, 10 2012.

[3] Yuanzhouhan Cao, Zifeng Wu, and Chunhua Shen. Esti-
mating depth from monocular images as classiﬁcation using
deep fully convolutional residual networks. IEEE, 2017.
[4] Weifeng Chen, Zhao Fu, Dawei Yang, and Jia Deng. Single-
image depth perception in the wild.
In D. D. Lee, M.
Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, edi-
tors, Advances in Neural Information Processing Systems 29,
pages 730–738. Curran Associates, Inc., 2016.

[5] Weifeng Chen, Zhao Fu, Dawei Yang, and Jia Deng. Single-
image depth perception in the wild. In Advances in neural
information processing systems, pages 730–738, 2016.
[6] C. R. d. Souza, A. Gaidon, Y. Cabon, and A. M. L´opez. Pro-
cedural generation of videos to train deep action recognition
networks. In 2017 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pages 2594–2604, July 2017.

[7] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Hal-
ber, Thomas Funkhouser, and Matthias Nießner. Scannet:
Richly-annotated 3d reconstructions of indoor scenes.
In
Proc. Computer Vision and Pattern Recognition (CVPR),
IEEE, 2017.

[8] David Eigen and Rob Fergus. Predicting depth, surface nor-
mals and semantic labels with a common multi-scale con-
In Proceedings of the IEEE Inter-
volutional architecture.
national Conference on Computer Vision, pages 2650–2658,
2015.

[9] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map
prediction from a single image using a multi-scale deep net-
work, 2014.

[10] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Bat-
manghelich, and Dacheng Tao. Deep ordinal regression net-
work for monocular depth estimation, 2018.

[11] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we
ready for autonomous driving? the kitti vision benchmark
suite. In Conference on Computer Vision and Pattern Recog-
nition (CVPR), 2012.

[12] Derek Hoiem, A.A. Efros, and M. Hebert. Geometric context
from a single image. volume 1, pages 654– 661 Vol. 1, 11
2005.

[13] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry
Kalenichenko, Weijun Wang, Tobias Weyand, Marco An-
dreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolu-
tional neural networks for mobile vision applications, 2017.
[14] Samin Khan, Buu Phan, Rick Salay, and Krzysztof Czar-
necki. Procsy: Procedural synthetic dataset generation to-
wards inﬂuence factor studies of semantic segmentation net-
In The IEEE Conference on Computer Vision and
works.
Pattern Recognition (CVPR) Workshops, June 2019.

[15] Youngjung Kim, Hyungjoo Jung, Dongbo Min,

and
Kwanghoon Sohn. Deep monocular depth estimation via in-

tegration of global and local predictions. IEEE Transactions
on Image Processing, PP:1–1, 05 2018.

[16] Johannes Kopf, Kevin Matzen, Suhib Alsisan, Ocean
Quigley, Francis Ge, Yangming Chong, Josh Patterson, Jan-
Michael Frahm, Shu Wu, Matthew Yu, Peizhao Zhang, Zi-
jian He, Peter Vajda, Ayush Saraf, and Michael Cohen. One
shot 3d photography. 39(4), 2020.

[17] Iro Laina, Christian Rupprecht, Vasileios Belagiannis, Fed-
erico Tombari, and Nassir Navab. Deeper depth prediction
In 3D Vision
with fully convolutional residual networks.
(3DV), 2016 Fourth International Conference on, pages 239–
248. IEEE, 2016.

[18] Zhengqi Li, Tali Dekel, Forrester Cole, Richard Tucker,
Noah Snavely, Ce Liu, and William T. Freeman. Learning the
depths of moving people by watching frozen people. CoRR,
abs/1904.11111, 2019.

[19] Zhengqi Li and Noah Snavely. Megadepth: Learning single-
In Computer

view depth prediction from internet photos.
Vision and Pattern Recognition (CVPR), 2018.

[20] Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen,
Xiaodong Liu, Jianfeng Gao, and Jiawei Han. On the vari-
ance of the adaptive learning rate and beyond, 2019.

Savva*,

[21] Manolis

Abhishek Kadian*,

Oleksandr
Maksymets*, Yili Zhao, Erik Wijmans, Bhavana Jain,
Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi
Parikh, and Dhruv Batra. Habitat: A Platform for Embodied
AI Research. arXiv preprint arXiv:1904.01201, 2019.
[22] Moritz Menze and Andreas Geiger. Object scene ﬂow for
In Conference on Computer Vision

autonomous vehicles.
and Pattern Recognition (CVPR), 2015.

[23] Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob
Indoor segmentation and support inference from

Fergus.
rgbd images. In ECCV, 2012.

[24] Vladimir Nekrasov, Thanuja Dharmasiri, Andrew Spek, Tom
Drummond, Chunhua Shen, and Ian Reid. Real-time joint se-
mantic segmentation and depth estimation using asymmetric
annotations. arXiv preprint arXiv:1809.04766, 2018.
[25] Vladimir Nekrasov, Chunhua Shen, and Ian Reid. Light-
weight reﬁnenet for real-time semantic segmentation, 2018.
[26] Sergey I. Nikolenko. Synthetic data for deep learning. CoRR,

abs/1909.11512, 2019.

[27] Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-
ban Desmaison, Luca Antiga, and Adam Lerer. Automatic
differentiation in pytorch. 2017.
[28] Craig Quiter and Maik Ernst.

deepdrive/deepdrive: 2.0,

March 2018.

[29] Ren´e Ranftl, Katrin Lasinger, David Hafner, Konrad
Schindler, and Vladlen Koltun. Towards robust monocular
depth estimation: Mixing datasets for zero-shot cross-dataset
transfer, 2019.

[30] German Ros, Laura Sellart, Joanna Materzynska, David
Vazquez, and Antonio M. Lopez. The synthia dataset: A
large collection of synthetic images for semantic segmenta-
tion of urban scenes. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2016.

9

[45] Jianxiong Xiao, Andrew Owens, and Antonio Torralba.
Sun3d: A database of big spaces reconstructed using sfm
and object labels. In Proceedings of the IEEE international
conference on computer vision, pages 1625–1632, 2013.
[46] Pavel Yakubovskiy. Segmentation models pytorch. https:
//github.com/qubvel/segmentation_models.
pytorch, 2020.

[47] Michael R. Zhang, James Lucas, Geoffrey Hinton, and
Jimmy Ba. Lookahead optimizer: k steps forward, 1 step
back, 2019.

[48] ChaoQiang Zhao, QiYu Sun, ChongZhen Zhang, Yang Tang,
and Feng Qian. Monocular depth estimation based on deep
learning: An overview. Science China Technological Sci-
ences, 63(9):1612–1627, Jun 2020.

[31] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-
moginov, and Liang-Chieh Chen. Mobilenetv2: Inverted
residuals and linear bottlenecks, 2018.

[32] A. Saxena, M. Sun, and A. Y. Ng. Make3d: Learning 3d
scene structure from a single still image. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 31(5):824–
840, 2009.

[33] Thomas Sch¨ops, Johannes L. Sch¨onberger, Silvano Galliani,
Torsten Sattler, Konrad Schindler, Marc Pollefeys, and An-
dreas Geiger. A multi-view stereo benchmark with high-
resolution images and multi-camera videos. In Conference
on Computer Vision and Pattern Recognition (CVPR), 2017.
[34] Shuran Song, Samuel P Lichtenberg, and Jianxiong Xiao.
Sun rgb-d: A rgb-d scene understanding benchmark suite. In
Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 567–576, 2015.

[35] Shuran Song, Fisher Yu, Andy Zeng, Angel X Chang, Mano-
lis Savva, and Thomas Funkhouser. Semantic scene comple-
tion from a single depth image. Proceedings of 30th IEEE
Conference on Computer Vision and Pattern Recognition,
2017.

[36] J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cre-
mers. A benchmark for the evaluation of rgb-d slam systems.
In Proc. of the International Conference on Intelligent Robot
Systems (IROS), Oct. 2012.

[37] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz.
Pwc-net: Cnns for optical ﬂow using pyramid, warping, and
In Proceedings of the IEEE conference on
cost volume.
computer vision and pattern recognition, pages 8934–8943,
2018.

[38] Mingxing Tan and Quoc V. Le. Efﬁcientnet: Rethinking
model scaling for convolutional neural networks, 2019.

[39] Zachary Teed and Jia Deng.

pairs ﬁeld transforms for optical ﬂow.
arXiv:2003.12039, 2020.

Raft: Recurrent all-
arXiv preprint

[40] J. Tremblay, T. To, and S. Birchﬁeld. Falling things: A syn-
thetic dataset for 3d object detection and pose estimation. In
2018 IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition Workshops (CVPRW), pages 2119–21193,
June 2018.

[41] G¨ul Varol, Javier Romero, Xavier Martin, Naureen Mah-
mood, Michael J. Black, Ivan Laptev, and Cordelia Schmid.
Learning from synthetic humans. CoRR, abs/1701.01370,
2017.

[42] Chaoyang Wang, Simon Lucey, Federico Perazzi, and Oliver
Wang. Web stereo video supervision for depth prediction
from dynamic scenes, 2019.

[43] Matthew J Westoby, James Brasington, Niel F Glasser,
Michael J Hambrey, and Jennifer M Reynolds.
‘structure-
from-motion’photogrammetry: A low-cost, effective tool
for geoscience applications. Geomorphology, 179:300–314,
2012.

[44] Ke Xian, Chunhua Shen, Zhiguo Cao, Hao Lu, Yang Xiao,
Ruibo Li, and Zhenbo Luo. Monocular relative depth percep-
tion with web stereo data supervision. In The IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
June 2018.

10

A. Depth uncertainty impact on geometry esti-

mation

For several datasets, sourced from stereo ﬁlms [29] and
arbitrary stereo photos from the internet [44], ground-truth
disparity can be obtained only up to unknown scale and shift
coefﬁcients. Without knowing the correct disparity shift
value scene 3D geometry can not be reconstructed properly.
To illustrate this, suppose a 3D line that is not aligned with
the optical axis of a pinhole camera. Depth for the 3D line
projection point x, y can be expressed as:

d = ax + by + c,

(4)

where a, b, c are some coefﬁcients and x, y belongs to
the point set on the camera matrix containing the 3D line
projection. Suppose that an inverse depth (disparity) of this
3D line is deﬁned up to unknown shift and scale:

1
˜d

=

C1
ax + by + c

+ C2,

or equivalently

˜d =

ax + by + c
C1 + C2(ax + by + c)

.

(5)

(6)

This expression denotes a line iff C2 is zero. Therefore,
to obtain predictions useful for 3D scene reconstruction, a
neural network should evaluate the C2 coefﬁcient explicitly.
Though the C2 coefﬁcient signiﬁcantly affects the point
cloud’s geometry, C1 affects only the global scene scale. To
illustrate that, we can consider mapping from the pinhole
camera plane point (x, y) and its corresponding depth d to
the 3D scene point:







 (cid:55)→






(x−cx)d
fx
(y−cy)d
fy
d




 .

x
y
d

Suppose the original depth map is scaled by a factor C1.
According to 7, all the 3D point coordinates are also mul-
tiplied by C1. Thus the overall scene is just scaled by C1
without affecting the correctness of the geometry (e.g. an-
gles and curvatures).

B. Efﬁcient computation of the proposed loss

function

Inspired by the loss function of Eigen et al. [9], we pro-

pose up-to-scale pairwise L1 loss function:

L =

1
N 2

N
(cid:88)

i,j=1

|(log di − log dj) − (log d∗

i − log d∗

j )|. (8)

This loss function is scale-invariant since the difference
of log-depth values eliminates unknown depth scale coefﬁ-
cients.

As the original expression requires to sum up O(N 2)
terms, we propose an efﬁcient way of computing it. Using
a substitution Ri = log di − log d∗

i we get:

L =

1
N 2

N
(cid:88)

i,j=1

|Ri − Rj|,

(9)

or equivalently:

L =

1
N 2

N
(cid:88)

i,j=1

|R{i} − R{j}|,

(10)

where R{i} is an i-th element in ascending order. By
changing the order of summation, one can get the following
expression:

L =

2
N 2

N
(cid:88)

N
(cid:88)

j=1

i=j+1

(cid:0)R{i} − R{j}

(cid:1)

(11)

since the expression in the brackets is always positive. In
this summation, term Ri occurs i − 1 times with a positive
sign and N − i times with a negative sign. Hence, sum
of these terms is equal to (2i − N − 1)R{i}. This can be
rewritten as

L = −

2
N 2

N
(cid:88)

i=1

(N − 1 − 2(i − 1))R{i}.

(12)

Thus, L1 pairwise loss can be computed in O(N log N )
time, which is a complexity of sorting operation. However,
in practice, we faced only negligible training time increase
compared to the training time with conventional loss func-
tions.

(7)

C. Overview of alternative loss functions

There are several other functions that can be used for ge-
ometry preserving SVDE task. First of all there are absolute
loss functions that are designed for absolute depth predic-
tion. First one may be called L2 pointwise loss function:

L =

1
N

N
(cid:88)

(log di − log d∗

i )2.

i=1

(13)

In this loss function the discrepancy between the loga-
rithms of target and predicted depths is penalized in each of
the pixels.

Secondly, there is an L1 pointwise loss function that is

known to be more robust to outliers:

L =

1
N

N
(cid:88)

i=1

| log di − log d∗
i |.

(14)

11

Both loss functions, L1 pointwise and L2 pointwise, can
be modiﬁed to become scale-invariant and to be able to
work with UTS data. L2 pointwise loss function should
me modiﬁed as follows:

L =

1
N

N
(cid:88)

i=1

(log di − log d∗

i − µ)2,

(15)

where

µ =

1
N

N
(cid:88)

i=1

(log di − log d∗

i ).

(16)

L1 pointwise loss function can be modiﬁed similarly to
become a scale-invariant loss, the only difference is that in-
stead of mean value µ, the median value should be used.

Finally, there is L2 pairwise loss function [8]:

N
(cid:88)

(cid:0)(log di − log dj) − (log d∗

i − log d∗

j )(cid:1) .

L =

1
N 2

i=1

(17)
This loss function may be computed in O(N ) time using

the formula:

L =

1
N

N
(cid:88)

i=1

R2

i −

1
N 2

(cid:32) N
(cid:88)

(cid:33)2

Ri

.

i=1

(18)

L2 pairwise loss function is scale-invariant and, thus, can
be used for training on UTS data. It compares all the pairs
of the pixels on the image.

D. Architecture modiﬁcations

Following the approach from [29], we use a ReﬁneNet
architecture to address the depth estimation problem. For
the sake of efﬁciency, we use Light-Weight ReﬁneNet
(LRN) [25].

The encoders in our experiments are based either on Mo-
bileNetv2 [31] or on architectures from the EfﬁcientNet
family [38], namely EfﬁcientNet-Lite0 and a set of Efﬁ-
cientNet backbones (B0-B5), pre-trained on the ImageNet
classiﬁcation task.

We introduce two modiﬁcations to improve efﬁciency
and address stability issues further. Firstly, we replace the
layer that maps the encoder output to 256 channels in the
Instead, we use 1x1 convolu-
original LRN architecture.
tions that do not change the number of channels.
In this
block, the output of the encoder layer is fused with the
features coming from a deeper layer (see Fig. 6, top) into
the same amount of channels as the encoder yields. Sec-
ondly, we noticed that the original Chained Residual Pool-
ing (CRP) blocks cause instabilities in the training process.
We ﬁx this issue by replacing summation with averaging
by the number of chains (Fig. 6, bottom). Predictions of

Figure 6: Top: Architecture used in our method. The
number of channels in fusion convolutions is equal to the
number of channels in the corresponding backbone level to
provide decoder scalability. Bottom: updated CRP block.
If the CRP block has N CRP modules, the output signal
should be divided by N + 1.

our LRN decoder modiﬁcation are twice as small as target
depth maps, so we upscale them to the original resolution
via bilinear interpolation.

E. Stereo movies data

Across all the experiments, we use the same collection of
datasets for training and testing unless otherwise stated. We
train our models on a mixture of RedWeb [44], DIML [15],
3D Movies [29], and MegaDepth [19] datasets and evalu-
ate them on previously unseen NYUv2 [23] (654 images),
TUM-RGBD [36] (1815 images), DIW (74,441 images)[4],
and KITTI [11] (161 images) datasets. The large-scale
DIML dataset covers more than 200 indoor environments
and provides absolute depth since it was captured with the
Kinect sensor.
In contrast with DIML and MegaDepth
datasets [19] focus mostly on outdoor static environments,
such as architecture and landscapes. MegaDepth was ac-
quired using the SfM technique from crowd-sourced inter-
net images and provides UTS depth. The ReDWeb dataset

12

Chained Residual PoolingConvPool(N times)ConvPool1/(N+1)Layer 1Layer 2Layer 3Layer 4Layer 5Layer 6Layer 7x/424x/832x/1664x/1696x/32160x/32320Conv96Conv320Conv32Conv24++++CRPCRPConv24CRPConv32CRPConv96x/216BackboneConv16Conv24+CRPoutput (x/2)Convolution 1x1(N channels)Backbone layer(MobileNetV2)Chained Residual Pooling+Add-ReLU operationName
3-D Sex and Zen: Extreme Ecstasy
A Very Harold &
Kumar 3D Christmas
Battle of the Year
Cirque du Soleil: Journey of Man
Creature from the Black Lagoon
Dark Country
Dolphin Tale

Drive Angry
Exodus: Gods and Kings
Final Destination 5
Flying Swords of Dragon Gate
Galapagos: The Enchanted Voyage
Ghosts of the Abyss
Hugo
Into the Deep
Jack the Giant Slayer
Journey 2: The Mysterious Island
Journey to the Center of the Earth
Life of Pi
My Bloody Valentine
Oz the Great and Powerful
Pina
Piranha 3DD
Pirates of the Caribbean:
On Stranger Tides
Pompeii
Prometheus
Sanctum
Saw 3D: The Final Chapter
Sea Rex 3D:
Journey to a Prehistoric World

Year
2011

2011

2013
2000
1954
2009
2011
2009
2011
2014
2011
2011
1999
2003
2011
1994
2013
2012
2008
2012
2009
2013
2011
2012

Frames
12201

6418

10992
3654
7266
7657
11536
3077
10679
14855
9009
13301
1787
6300
12852
1564
10020
9923
9472
9926
10275
11087
10674
7718

2011

12914

2014
2012
2011
2010

2011

9178
11114
9682
8632

4130

Table 6: Stereo movies used in our experiments, part 1

[44] consists of 3600 stereo RGB-D images covering both
indoor and outdoor scenarios. It is a small but highly di-
verse dataset with dynamic scenes, constructed using stereo
photos from Flickr. The authors of MIDAS [29] proposed
to use stereo movies for depth estimation models training.

The original dataset consists of 23 stereo movies and fea-
tured video frames from various non-static environments.
We use similar data acquisition and processing pipeline, yet
we use RAFT [39] instead of PWCNet [37] to estimate dis-
parities. Additionally, we extend the list of ﬁlms with 26
additional stereo movies, totaling 49 movies overall. We
sample one frame per second from these movies. We leave
ﬁrst and the last 10% of frames out as they usually belong
to opening and closing credits. The disparity is considered

13

Name
Silent Hill: Revelation 3D
Sin City: A Dame to Kill For
Space Station 3D
Stalingrad
Step Up 3D
Step Up Revolution
Texas Chainsaw 3D
The Amazing Spider-Man
The Child’s Eye
The Darkest Hour
The Final Destination
The Great Gatsby
The Hobbit:
An Unexpected Journey
The Hobbit:
The Battle of the Five Armies
The Hobbit:
The Desolation of Smaug
The Hole
The Martian
The Three Musketeers
The Ultimate Wave Tahiti
Ultimate G’s
Underworld: Awakening
X-Men: Days of Future Past
Overall

Year
2012
2014
2002
2013
2010
2012
2013
2012
2010
2011
2009
2013

2012

Frames
8533
10788
4169
10315
11051
10064
6893
8585
7746
8034
8403
14295

8810

2014

14021

2013

2010
2015
2011
2010
2000
2012
2014

15462

8765
14075
9976
4083
3851
7391
12916
473042

Table 7: Stereo movies used in our experiments, part 2

valid only in pixels where the discrepancy between left to
right and right to left disparities is less than 8 pixels. We
leave only those images in the dataset, where the dispar-
ity is correct for more than 80% of pixels and the range of
disparity (the difference between maximal and minimal dis-
parities) is more than 8 pixels. Acquired images are highly
diverse and contain landscapes, architecture, humans in ac-
tion, and other scene types. Refer to the tables 6 and 7 for
detailed ﬁlms list.

For our ablation studies, we use the NYUv2 raw [23]
data. We subsample the training set to approximately 150
thousand images and use the original test set of images (654
images).

F. Metrics

In this work we use δ1.25 and rel metrics. These met-
rics are designed to be used with absolute depth predic-
tions. As our models yield up-to-scale depth predictions,
we choose an appropriate scene scale before metrics com-
putation. We select the value that minimizes L1 difference
between ground truth and predicted log-depth maps.

Figure 7: Point cloud 3D reconstructions of images from Microsoft COCO dataset with the use of B5-LRN model.

14

Figure 8: Point cloud 3D reconstructions of images from Microsoft COCO dataset with the use of B5-LRN model.

15

Figure 9: Point cloud 3D reconstructions of paintings with the use of B5-LRN model.

Figure 10: Failure cases for B5-LRN model: reﬂective and glass surfaces, mirrors, objects with thin edges.

16

