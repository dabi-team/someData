1

A Deep Perceptual Measure
for Lens and Camera Calibration

Yannick Hold-Geoffroy1, Dominique Pich ´e-Meunier3, Kalyan Sunkavalli1,
Jean-Charles Bazin2, Franc¸ois Rameau2, and Jean-Franc¸ois Lalonde3
Adobe Research1, KAIST2, Universit ´e Laval3

2
2
0
2

g
u
A
5
2

]

V
C
.
s
c
[

1
v
0
0
3
2
1
.
8
0
2
2
:
v
i
X
r
a

Abstract—Image editing and compositing have become ubiquitous in entertainment, from digital art to AR and VR experiences. To
produce beautiful composites, the camera needs to be geometrically calibrated, which can be tedious and requires a physical calibration
target. In place of the traditional multi-images calibration process, we propose to infer the camera calibration parameters such as pitch,
roll, ﬁeld of view, and lens distortion directly from a single image using a deep convolutional neural network. We train this network using
automatically generated samples from a large-scale panorama dataset, yielding competitive accuracy in terms of standard (cid:96)2 error.
However, we argue that minimizing such standard error metrics might not be optimal for many applications. In this work, we investigate
human sensitivity to inaccuracies in geometric camera calibration. To this end, we conduct a large-scale human perception study where
we ask participants to judge the realism of 3D objects composited with correct and biased camera calibration parameters. Based on this
study, we develop a new perceptual measure for camera calibration and demonstrate that our deep calibration network outperforms
previous single-image based calibration methods both on standard metrics as well as on this novel perceptual measure. Finally, we
demonstrate the use of our calibration network for several applications, including virtual object insertion, image retrieval, and compositing.
A demonstration of our approach is available at https://lvsn.github.io/deepcalib.

Index Terms—single image camera calibration, human perception, horizon estimation, lens distortion

(cid:70)

1 INTRODUCTION

Most image understanding and editing tasks—ranging
from 3D scene reconstruction to image metrology to photo-
graphic editing—require some degree of camera calibration
to be performed. For example, performing virtual object
insertion involves constructing a virtual scene and camera
that mimic the background image. To do so, estimating the
scene local geometry in addition to the camera parameters—
such as focal length, orientation and lens distortion—is of
paramount importance.

Many methods have been proposed in the literature to
estimate the camera parameters from images. In particular,
methods focusing on metrology aim to perform absolute
metric estimations that typically achieve subpixel repro-
jection accuracy [1], [2], [3]. However, the usefulness of
these approaches comes at a high cost, as they often require
multiple captures of speciﬁc calibration objects [1]. Some
methods relax these requirements to a single capture [4], but
still need known objects such as markers to be present in
the image. These restrictions make those methods typically
tedious, and impractical to perform on images obtained “in
the wild” where access to the camera is impossible.

One key observation is that a high degree of accuracy in
camera calibration may not be necessary for all applications.
Consider, for example, image editing such as object copy-
paste [5] or virtual object insertion [6]. It is well-known
that humans have a very low sensitivity to deviations from
realism in some cases, for example in paintings [7] and digital
composites [8]. Is subpixel-precise camera calibration needed
in these cases, or do we need to be accurate only up to
human perception? This raises the important question how do
humans perceive inaccuracies in geometric camera calibration?
In this work, we investigate this topic by conducting a

large-scale user study on the human perception of errors
in camera calibration. We asked participants to select the
most plausible virtual object insertion between pairs of
images, using either ground-truth calibration or perturbed
parameters to perform the insertions. Using the answers from
this user study, we build a human perception measure that
estimates how sensitive humans are to speciﬁc combinations
of geometric camera calibration errors.

We also develop a single image calibration approach
that generalizes to a large diversity of both natural and
urban scenes, and which spans a large set of cameras. We
parameterize the camera by its ﬁeld of view, lens distortion,
and horizon position in the image, from which can be
derived pitch and roll angles. We evaluate our method on
a challenging test set and demonstrate that it quantitatively
outperforms the state of the art in terms of raw parameter
error and are more pleasing to the human eye according
to our novel human perception measure when used for
applications such as virtual insertion.

This work uniﬁes and signiﬁcantly extends on contri-
butions introduced in [9] (ﬁeld of view, distortion estima-
tion) and [10] (ﬁeld of view, pitch, roll estimation). First,
we perform two large-scale perceptual experiments which
evaluate the sensitivity of human observers to inaccuracies
in geometric camera calibration, in terms of ﬁeld of view,
roll and pitch angles, as well as lens distortion. Second, from
these experiments is derived a novel quantitative measure of
observer sensitivity to geometric camera calibration errors.
Third, we present a method for automatically estimating
both intrinsic and extrinsic parameters of a camera from a
single image, which works on a wide range of parameters
including very high lens distortion. Finally, we present an

 
 
 
 
 
 
extensive set of experiments to benchmark our method, both
in terms of absolute error and human perception.

2 RELATED WORK

Geometric camera calibration
is a widely studied topic
that has a signiﬁcant impact on a variety of applications
including metrology [11], 3D inference [12], [13] and aug-
mented reality, both indoor [14], [15] and outdoor [16]. As
such, many techniques have been developed to perform
precise geometric calibration using a calibration target in-
serted in the image [1], [3], [17], [18], [19], [20], [21], [22], [23],
[24], [25], [26]. Despite their great accuracy, these methods
require a cumbersome acquisition process, precluding them
from being applied on images in the wild. To alleviate
this limitation, work has been done to directly use the
features present in urban scenes to perform calibration,
typically using either lines [27], [28], [29], [30], vanishing
points [31], [32], higher-level geometric cues [33], or speciﬁc
objects typically present in human-made environments [34],
[35]. While greatly simplifying the calibration process, these
methods are limited to structured man-made scenes, and thus
cannot deal with general environments such as landscapes
or natural scenes.

Other work has proposed to take advantage of lighting
cues for camera calibration [36], [37] from the sky and from
rainbows, circumventing the need to detect vanishing lines.
However, these techniques often fail on complex scenes
where semantic reasoning is required to discard misleading
textures and visual cues. To solve the need for high-level
reasoning, convolutional neural networks (CNNs) were used
to bring camera calibration on single images to a wider
variety of scenes. One of such ﬁrst attempts is the approach of
Mendonc¸a et al. [38], which uses neural network to compute
the camera parameters given 3D point locations from a
calibration target and their respective 2D observations. More
recently, deep learning-based methods were proposed to
estimate individual camera parameters from a single image
without calibration target. For example, DeepFocal [39] pre-
dicts the camera’s focal length, DeepHorizon [28] estimates
the camera’s pose using the horizon line, and Rong et
al. [40] estimate the radial distortion. Similar techniques
were explored for 360 panorama upright alignment [41], [42].
Hold-Geoffroy et al. [10] estimate the camera’s focal
length and orientation. They train a CNN on images gen-
erated from panoramas using standard pinhole model, and
thus, are limited to perspective cameras. Latest work on
geometric calibration estimation propose optimization-based
methods that simultaneously solves for lens distortion and
camera pose [43], combining circular vanishing lines estima-
tion [32], [44] with the detection of repeated patterns [45].
Closer to our work, [46] presents an end-to-end learned
approach to estimate camera parameters of large ﬁeld-of-
view images trained on pixel ray direction error. In our work,
we draw inspiration from [9] and use a uniﬁed spherical
model [22], [47] to represent the lens distortion.

Our method holistically estimates the camera parameters,
providing both camera intrinsics and its pose at the same
time. Its output is readily usable for image undistortion and
3D reconstruction and supports a large diversity of lens
types, from perspective (pinhole) to ﬁsheye.

2

Fig. 1. The uniﬁed spherical
model [22], [47]. A 3D point pc =
Rpw (see sec. 3.3) is ﬁrst pro-
jected onto the unit sphere with
ps = pc/(cid:107)pc(cid:107), and then onto
the image plane pim using a line
starting from a point Oc located
at [0, 0, ξ] below the sphere cen-
ter O. Distances not to scale to
simplify visualization.

Image undistortion
A common task when using
wide FOV cameras is image undistortion—also called
rectiﬁcation—which undoes the warping caused by the lens,
effectively straightening the lines in an image. To do this,
we need to calibrate the camera’s lens distortion. For this
purpose, various distortion models have been developed.
One of the most popular representation is the Brown-
Conrady model [48] which models the radial and tangential
lens distortion via a polynomial function. While this model
approximates reasonably low distortions, it is not suitable for
thicker lenses with larger ﬁelds of view which exhibit large
geometric distortions (e.g. ﬁsheye cameras) [49]. To cope
with this limitation, appropriate distortion models for large
ﬁeld-of-view cameras have been developed. A representative
example is the division model proposed by Fitzgibbon [50],
which also represents non-linear distortion via a radially-
symmetric polynomial equation. These models have proven
effective for self-calibration [50] and robotics navigation with
wide FoV cameras [51], [52]. Unfortunately, they require
multiple unbounded parameters and they are difﬁcult to
invert, which make them both less suited for training a neural
network and generating training data, respectively. Once
geometrically calibrated, many image processing libraries
such as OpenCV [53], ImageMagick [54], Adobe Photoshop,
PTLens [55] and Hugin [56] propose algorithms to project
captured images to a ﬂat surface.

For large ﬁeld of view capture setups, the Fisheye-
Hemi plug-in for Photoshop [57], Carroll et al. [58] and the
FOVO projection [59] aim to produce aesthetically pleasing
visualizations, used notably in architecture model display.
Perception Understanding the limits of the human visual
system has also received signiﬁcant attention, with studies
quantifying color sensitivity [60], how reliably we can detect
photo manipulations artifacts [8] and how people perceive
distortion in street-level image-based rendering [61]. More
recently, perceptual studies were performed to assess hu-
man appreciation on tasks like super-resolution [62], image
caption generation [63] and video temporal alignment [64].
In this work, we go one step further by studying human
sensitivity to camera calibration errors (pitch, roll, ﬁeld of
view and distortion) by using virtual object insertion as a
test scenario.

3 IMAGE FORMATION MODEL

3.1 Projection model

We begin by reviewing the main parameters of the image
formation model. The pixel coordinates pim of a 3D point
pw in world coordinates are given by

pim = [u, v]T = P(Rpw + t) ,

(1)

OOcpcpspimξ13

(a)

(b)

Fig. 2. Example results of horizon line estimation on our 360Cities test set, for images both (a) with and (b) without clear vanishing lines. We provide
the ground truth ﬁeld of view to UprightNet [33]. Only our method and SVA [43] allow for curved horizon lines (due to large distortions). Note how
Upright [41] and SVA perform well when sharp human-made objects are present in the scene, whereas deep learning methods offer a more robust
performance across all scenes. Note also that SVA fails on 49% of our test images. More examples available in the supplementary material.

where P is a 3D-to-2D projection operator, including conver-
sion from homogeneous coordinates to 2D. Here, R and t
are the camera rotation and translation respectively in the
world reference frame. Under the pinhole camera model and
with the common assumptions that the principal point is
at the image center, negligible skew and a unit pixel aspect
ratio [17], the projection operator P would take the form
of a matrix K = diag([f f 1]), where f is the focal length
in pixels and the diag operator builds a matrix with the
speciﬁed elements on its diagonal and 0 elsewhere.

The uniﬁed spherical model relies on a stereographic
projection (ﬁg. 1), which makes the basis for our projection
operator P from eq. 1. First, the 3D point pc = Rpw (in
camera coordinates, see sec. 3.3) is projected onto the unit
sphere with ps = pc/(cid:107)pc(cid:107). Then, we project the spherical
point ps onto the image plane, using Oc = [0, 0, ξ] as
the center of projection. The distance ξ models the radial
distortion of the camera. The projection operator P, which
projects a 3D point in camera coordinates pc = [x, y, z]T to
pixel coordinates pim = [u, v]T can therefore be expressed as

3.2 Intrinsic parameters

Instead of following these typical assumptions, we model P
by including lens distortion and employ the uniﬁed spherical
model [22], [47] since it features several advantages over the
models discussed in sec. 2. First, the radial distortion can be
represented by a single, bounded parameter ξ ∈ [0, 1]1. Second,
this model is fully invertible which make it practical to both
add/remove distortion to/from an image. Third, both the
projection and back-projection operations admit closed-form
solutions that can be computed very efﬁciently. Finally, unlike
polynomial-based models, the spherical model is compatible
with the entire family of single view points sensors, including
perspective, ﬁsheye and catadioptric cameras.

[u, v] = P(pc) ≡

(cid:20) xf
ξα + z

+ u0,

yf
ξα + z

(cid:21)

,

+ v0

(2)

where α = ||pc|| and [u0 v0] is the principal point, assumed
in this work to be the image center.

As mentioned, one of the advantages of the spherical
model is the closed-form solution of the inverse projection
equation. This inverse projection is especially useful when
applying the spherical lens model to panoramic images (see
sec. 4.1). Given a 2D image point pim = [u, v]T, the back-
projection from the image to the sphere is computed by
ps = [ωu, ωv, ω−ξ]T, where

ω =

ξ + (cid:112)1 + (1 − ξ2)(u2 + v2)
u2 + v2 + 1

.

(3)

1. ξ can be slightly greater than 1 for certain types of catadioptric

cameras [65] but we ignore this scenario in this work.

We can also derive the equation for the effective horizontal

Ground truthOursUprightNetUprightDeephorizonSVAParameter

Distribution Values

Lognormal µ = 14, σ = 16

Focal length (mm)
Horizon (image units) Normal
Roll (◦)
Cauchy
Varying
Aspect ratio
Triangular
Distortion

µ = 0.523, σ = 0.3
x0 = 0, γ ∈ {0.001, 0.1}
{1:1, 5:4, 4:3, 3:2, 16:9}
c ∈ {0.3, 1}

TABLE 1
Sampling of camera parameters used to generate the dataset for the
human sensitivity study.

ﬁeld of view hθ, noting ˆu = −u0/f :

hθ = 2 arccos

(cid:32)

ξ + (cid:112)1 + (1 − ξ2)ˆu2
ˆu2 + 1

(cid:33)

− ξ

.

(4)

Please refer to the supplementary material for more details.

3.3 Extrinsic parameters

The rotation matrix R can be parameterized by roll ψ, pitch
θ, and yaw ϕ angles. Since there exists no natural reference
frame to estimate ϕ (left vs right) from an arbitrary image,
we therefore constrain the rotation to only pitch and roll
components, simplifying the extrinsic rotation matrix to R =
Rz(ψ)Rx(θ). For the same reason, we take the origin to be
at the camera with t = 0.

The camera extrinsic parameters are expressed by using
the horizon line as an intuitive representation for these angles.
As in [28], we parameterize the horizon line by its midpoint
vm and its angle ψ. The latter coincides with the camera
roll, which we deﬁne as the angle between the horizon line
in the image and an virtual horizontal line located at the
midpoint. We deﬁne the midpoint vm as the v-coordinate
of its intersection with the vertical axis in the center of the
image. The midpoint vm can be derived from θ and f as

vm =

2f sin θ
h (ξ + cos θ)

,

where h is the image height. In this normalized image
units representation, the top and bottom of the image have
coordinates 1 and −1, respectively.

4 PROPOSED APPROACH

Our goal is to train a deep network to estimate the horizon
line (parametrized by ψ and vm), ﬁeld of view hθ and
distortion ξ from a single image. In this section, we present
our CNN architecture for single image calibration and
compare it to state-of-the-art estimation methods. To train
this model, we need a large number of images and their
corresponding camera parameters. In the literature, [28]
provides images with ground truth horizon lines, a subset of
which also have ﬁeld of view annotations [66]. However,
these images are captured with cameras sharing similar
parameters, with low ﬁelds of view and distortions.

4.1 Dataset

4

dataset of 30,000 360◦ panoramas obtained from 360Cities2.
We extract 7 rectiﬁed images from each panorama using our
projection model from sec. 3.2. To obtain reasonable camera
parameters, the following sampling strategies, summarized
in table 1, were employed. Note that for the camera roll,
two different Cauchy distributions {0.001, 0.1} are sampled
with 0.33 and 0.66 probability respectively. This was done
to model the fact that many photos typically have a roll
close to 0. Additionally, the distortion ξ was sampled from
a triangular distribution with low and high parameters
of 0 and 1, and modes {0.03, 0.} sampled with 0.8 and
0.2 probability, respectively. Those two modes are used to
generate a long-tail distribution bounded in [0, 1], yielding a
distribution of distortion values empirically close to images
found in the wild. The aspect ratios correspond to popular
image formats on Flickr and ImageNet images. Note that a
larger probability (0.66) was given to the 4:3 aspect ratio as
it is the most common. The other aspect ratios are given
a probability of 0.11. We resize the extracted images to
224 × 224 to ﬁt the neural network input size. This results in
a dataset of 205,865 pairs of photos and their corresponding
camera parameters which we split into a training set of
186,690 pairs, a validation set of 2,000 pairs and a test set of
17,175 pairs. We made sure that no training panorama was
used to compute a crop in the test set.

4.2 Network architecture

We adopt a DenseNet [68] model pretrained on ImageNet [69]
and replace the last layer with four separate heads which
estimate horizon angle ψ, the horizon distance to the center
of the image vm, the ﬁeld of view of the image hθ, and the
distortion ξ respectively. Instead of predicting f and ξ, our
network is trained to output hθ and ξ. We experimented
with all possible pairs of the three parameters and found that
estimating ξ and hθ gave overall the most accurate results.
We hypothesize that the dynamic range and distributions
of those two parameters better ﬁt the network initialization
and training procedure. All output layers use the softmax
activation function, which was also used in [28]. We adopt a
range of [−π/2, π/2] for ψ and [−1.6, 1.6] for vm. For roll ψ, we
use smaller bins around 0 for ﬁner estimations around those
values with the relationship a − b exp (cid:0)
. We empirically
choose a = 0.044 and b = 0.04. For hθ and ξ, we use 256
uniform bins on the intervals [0.33, 2.6] rad and [0, 1],
respectively. We employ the Kullback-Leibler loss on all
outputs and train the model using the Adam optimizer [70].
Please see our supplementary material for training details.

-2ψ2(cid:1)

5 RESULTS
5.1 Comparison to existing methods

We now compare our method against multiple single-
image camera parameters and horizon estimation methods,
including Single View Autocalibration [43], Upright [30], Up-
rightNet [33], and DeepHorizon [28]. Qualitative results are
shown in ﬁg. 2. Note that [43] and [30] are not learning-based
and [33] requires ground truth per-pixel surface normals for
training—as such, we do not retrain on our training set.

To obtain a dataset of images and their ground truth camera
parameters, we take inspiration from [28], [67] and leverage a

2. https://www.360cities.net, obtained through a license allowing for

publication but not redistribution.

5

(a)

(b)

(c)

(d)

Fig. 3. Quantitative comparison of (a) roll, (b) pitch, (c) focal length, (d) distortion. Note that SVA [43] fails on 49% of the images, when there are not
enough distinguishable edges to be detected with conﬁdence.

We also observe that optimization-based methods [30], [43]
provide good accuracy on urban scenes, where visual cues
for vanishing points are abundant.

Quantitative results are shown on ﬁg. 3. We note that our
method provides state-of-the-art accuracy on our test set. We
hypothesize that our larger and more diverse training dataset
and newer architecture provide enhanced generalization
capabilities compared to training on fewer scenes [28] or
indoor rooms only [33].

5.2 Feature analysis

We use guided backpropagation [71] to understand the image
features our CNN-based method focuses on to perform its
estimation. We use the smoothgrad [72] version of guided
backpropagation (SGB) to obtain a more stable analysis.
Qualitative results are shown in ﬁg. 4. Note how edges
representing vanishing lines are highlighted by SGB in accor-
dance to the features used by geometry-based approaches
such as Upright. When no clear vanishing line is detected
in the image, the CNN model tends to focus on boundaries
between sky and land, as the horizon typically lies on or
below this boundary.

5.3 Comparison to state-of-the-art calibration methods

We now compare our method to state-of-the-art multi-image
calibration methods. This comparison if offered for the sake
of completeness, as we target a single-image method with
much higher ﬂexibility than those methods at the expense
of accuracy. In table 2, we evaluate the following calibra-
tion methods: Mei’s toolbox [22] (based on the spherical
model), Brown [1], Fitzgibbon’s division model [50] and
Scaramuzza’s Toolbox [3]. Details on the camera setups
are provided in the supplementary material. Our method
provides competitive results on wide angle lenses. We also
notice that the Brown model is not suitable to handle high
distortion [73], leading to many failure cases.

We emphasize that our work does not aim to compete
with these approaches which typically obtain subpixel re-
projection accuracy. These methods require multiple images
of a reference object or manual human annotations to work,
which typically takes a long time to be performed. In contrast,
our method can be executed very quickly on a generic scene
(e.g., image from the internet). Despite this, our method
still proposes visually pleasing results for most tasks related
to image editing such as virtual object insertion, which we
demonstrate in the rest of the paper.

Avenir Avenir
2.8mm 4mm

GoPro Fisheye

Ours
Mei [22]
Division [50]
Brown [1]
Scaramuzza [3]

0.57
0.64
0.12
0.13
N/A N/A
0.29
0.20
0.54
1.01

1.25
2.74
0.10
0.10
0.7
1.05
N/A N/A
1.48
0.82

TABLE 2
Camera calibration results for different cameras (columns) and
calibration methods (rows). All compared methods use multiple
checkerboard pictures, while ours require a single picture of a general
scene. Failures cases are noted N/A. Please see the supplementary
materials for the estimated parameters.

6 APPLICATIONS

We now demonstrate different uses for camera parameter
estimation from a single image. Please consult the supple-
mentary material for additional results.
3D reconstruction We acquired roughly 300 images of a
building from a PointGrey Flea3 camera with an Avenir 4mm
lens. We show the resulting 3D reconstruction obtained from
COLMAP [74], [75] in ﬁg. 5, using both undistorted images
using our method (top) and the original images (bottom). Our
method successfully corrects the large distortions present in
the input images and allows for a robust 3D reconstruction of
the scene, while the original sequence yields curved surfaces.
Undistortion of images in the wild
To challenge the
robustness of our algorithm, we propose to undistort a
set of wide FoV images downloaded from the Internet
for which no ground truth distortion parameters nor any
information about the cameras or lenses are available. To
generate undistorted images, we undo the lens distortion
estimated by our method using eq. 3 and reproject the image
on a ﬂat plane. A set of representative results is available
in ﬁg. 6, showcasing our method robustness to diverse
environments and lighting conditions.
Image retrieval Our technique can be used to match im-
ages in large databases based on their camera parameters. To
demonstrate this, we estimated the camera parameters using
our technique on 10,000 images randomly selected from the
Places2 dataset [76] and computed the intersection of the
horizon line with the left/right image boundaries (which
works better than matching the parameters themselves since
this metric directly expresses camera parameters as pixel-
based measurements). Fig. 7 presents the 4 closest matches in
the dataset (according to the L2 distance) for 2 query images.
Geometrically-consistent object transfer
Transferring ob-
jects from one image to another requires matching the camera
parameters. While previous techniques required the use of

402002040Roll value [°]0.02.55.07.510.012.515.017.520.0Error [°]OursSVAUprightDeepHorizonUprightNet0.40.20.00.20.4Pitch value [image units]0.000.050.100.150.200.250.30Error [image units]OursSVAUprightDeepHorizonUprightNet7501250175022502750Focal length value [px]050010001500200025003000Error [px]OursSVAUpright0.10.30.50.70.9Distortion value [-]01020304050Mean reprojection error [px]OursSVA6

(a)

(b)

Fig. 4. Analysis of the neural network focus. The result of smoothed guided backpropagation is displayed as a jet overlay, and the estimated horizon
line in blue. (a) When present, edges corresponding to important vanishing lines are highlighted while other edges are discarded. (b) When no clear
horizontal vanishing lines are detected, the neural network seems to look for the boundaries of either sky or land textures while dismissing the clouds
or objects like trees, probably hinting bounds on horizon location in the image. Refer to the supplementary material for more examples and a
comparison of the network focus before and after training.

(a)

(b)

Fig. 5. 3D reconstruction results: (a) front and (b) top views. Top row:
results obtained with our parameters used for initialization step. Bottom:
results obtained with the automatic initialization of intrinsic parameters.

objects of known height in the image in order to infer camera
parameters [5], our approach estimates them from the image
itself, and can be used to realistically transfer objects from
one image to another, as shown in the supp. material.
Virtual object insertion Our approach also enables the
realistic insertion of 3D objects in 2D images. As discussed
in sec. 7, camera parameters are needed to plausibly align
the virtual object with the background image. Given our
automatic estimates, the user only needs to select an insertion
point in the image and to specify the virtual camera height.
Assuming the local scene around the object is a ﬂat plane
aligned with the horizon, we can automatically insert a
virtual object and demonstrate several such examples in
ﬁg. 8. For these results, the camera height was set to 1.6m
and the lighting was automatically estimated with [67], [77].

7 HUMAN PERCEPTION OF CALIBRATION

In this work, we aim to understand the sensitivity of humans
to camera calibration errors. For this, we leverage virtual
object insertion as a tool for evaluating whether an error
in camera calibration can be detected by a human observer.
Since the space of parameters is large, we separate it into two
different experiments on the sensitivity of human observers

to the studied camera parameters: Experiment 1 evaluates
sensitivity with respect to camera pitch, roll, and ﬁeld of view
(FoV) in the case where the FoV is narrow; and Experiment
2 studies sensitivity to lens distortion for images with wide
FoVs, where it is most apparent.

7.1 Method

We ﬁrst proceed to describe aspects that are common to both
experiments, and follow with per-experiment speciﬁcs.
Stimuli
To understand human perception of geometric
camera calibration errors, the stimuli generated for both
experiments consists of two versions of the same real image
in which a virtual object has been inserted. The virtual object
is inserted according to: the ground truth camera parameters
for the ﬁrst version; and to perturbed camera parameters
in the second. To generate this dataset, we ﬁrst use the
same process as described in sec. 4.1 to extract an image
with ground truth camera calibration from a panorama. The
Cycles rendering engine [78] is used to insert a single virtual
3D object at a manually-selected point in the image on a
ﬂat horizontal surface. To generate believable cast shadows
around the object, a ﬂat virtual ground plane3 was set at
y = 0. The techniques of [77] (indoor) and [67] (outdoor)
were used to produce the virtual light sources. The virtual
camera is placed at a height of 1.6m.

To ﬁrst image is obtained by setting the parameters of
the Cycles camera to the ground truth parameters of the
background image. The second is obtained by distorting the
Cycles camera parameters, yielding a virtual object that does
not have the same camera parameters as the background. In
the distorted renders, the virtual object was, after rendering,
translated and scaled in order to appear at the same location
and have the same size in the image as the ground truth
render. This step is needed as apparent size is a function

3. Set in Cycles as a “shadow catcher” which only shows the shadows

cast upon it but not the plane itself.

hθ = 145◦, ξ = 0.93

hθ = 143◦, ξ = 0.91

hθ = 149◦, ξ = 0.81

7

hθ = 115◦, ξ =0.85

hθ = 149◦, ξ = 0.77

hθ = 145◦, ξ = 0.98

hθ = 143◦, ξ = 0.87

hθ = 149◦, ξ = 0.79

hθ = 149◦, ξ = 0.90

Fig. 6. Examples of automatic undistortion results on images in the wild, with the estimated ﬁeld of view hθ and distortion ξ. Left: Original image.
Right: output of our algorithm. Our approach works on a variety of images, including close/far objects, indoor/outdoor, true color/heavily edited,
vertical/tilted viewpoint, and ground/aerial views. See the supplementary material for more results.

Query

NN1

NN2

NN3

NN4

Fig. 7. Examples of image retrieval by horizon location on Places2. The
horizon line is estimated using our method from the query image, and
used to ﬁnd closest matches in a 10k random subset from Places2. The
top-4 matches are shown on the right.

Fig. 8. Examples of virtual object insertions using our estimations.

of ﬁeld of view and/or lens distortion. The virtual objects
were laid vertically on the ground plane and were randomly
rotated about their vertical axis. To limit biases caused by the
virtual object inserted, different virtual objects were used in
both experiments.
Procedure Recruited subjects observed both images dis-
played side by side horizontally (the order being randomly
permuted) on their own (uncontrolled) monitor. After being
shown the stimuli, the subjects were asked the question
“select the image where the orientation of the virtual object
looks most natural to you” and responded by clicking on the

Fig. 9. Results of Preliminary Experiment 2a: sensitivity of observers to
lens distortion as a function of horizontal position of the virtual object, for
three scene types: artiﬁcial, urban and nature. Sensitivity is quantiﬁed
as the percentage of users choosing the ground truth image, i.e., where
the object has the same amount of distortion as the background. A
sensitivity of 50% means that humans are not sensitive to errors in
distortion (confusion), whereas a sensitivity of 100% means that humans
are very sensitive to errors. The shaded areas correspond to the 95%
conﬁdence intervals.

image of their choice. Subjects were speciﬁcally instructed to
ignore the color, texture, shadows or lighting on or around
the object and to focus on its coherence with the background.

7.2 Experiment 1 design: pitch, roll, (narrow) FoV

Stimuli For Experiment 1, 530 panoramas (79 indoor, 451
outdoor) were randomly selected and 20 crops were extracted
for each (see sec. 4.1), resulting in 10,368 images. The camera

for roll and [5, 55]◦

parameters were altered by randomly adding or subtracting
values sampled from a uniform distribution in [1, 30]◦
for
pitch, [0.5, 20]◦
for ﬁeld of view. No lens
distortion was used, i.e., ξ = 0.
Procedure Experiment 1 employed 8 different virtual objects
for insertion, ranging from simple geometric primitives
(sphere, cone), real objects with clear vertical directions (toy
rocket, metal barrel, the Eiffel tower) and objects with a
somewhat organic shape (the Stanford bunny and a horse
statue). Several examples of images generated using this
process are shown in ﬁg. 11 and in the supp. material.

Participants for this study were recruited on the Mechan-
ical Turk platform. In total, 376 subjects provided 145,720
submissions, from which 124,740 were accepted, leading to
11 different subjects annotating each image on average. 4,319
of those submissions had a single distorted parameter, while
the remaining 5,947 had two or more distorted parameters.
To handle inattentive subjects, we use the sentinels system
of Gingold et al. [79]. In this system, subjects are shown
some “sentinels” to perform, which consist of images with
easily identiﬁable ground-truth. Subjects were presented sets
of 20 image pairs at a time which contained 2 sentinels. 9
subjects were removed since they failed the sentinel tasks.
An additional 520 annotations were rejected for inattentive
subjects who failed some sentinels over a small time lapse.
The median time spent on a single pair of images was 4
seconds.
Observations We found no statistical difference in the
results obtained across the virtual objects, except for two: the
sphere and the Stanford bunny where subjects were unable
to identify the ground truth image except for signiﬁcant ﬁeld
of view variations (at least 30◦). The sphere being rotationally
symmetric, it is unsurprising that it does not serve as a good
instrument to determine object insertion errors. We theorize
that the bunny, with its round shape, shares similarities to
the sphere in that respect.

We also found no statistical difference in the results
when analyzing the impact of the object size, computed
as the relative height of the object with respect to the image.
Participants showed similar accuracy regardless of object size
(ranging from 10% to 85% image height in our dataset). We
manually labeled the images into pairs of outdoor-indoors
and built-natural, and found no statistical difference between
either subset. Finally, we analyzed the impact of the mean
image brightness, hypothesizing that a mismatch in camera
parameters may be more easily observable in brightly-lit
images. We found no statistical differences between images
of different mean intensities.

7.3 Experiment 2 design: distortion, (wide) FoV

As opposed to errors in pitch, roll or ﬁeld of view, human
perception of errors in lens distortion seems to be highly
dependent on background image content and on the position
of the virtual object in the image. We hypothesize that: 1)
humans perceive distortion better on urban scenes, where
there are a multitude of straight lines, than on scenes in
nature; and 2) the sensitivity to distortion is reduced when
virtual objects are inserted near the image center.
Preliminary Experiment 2a
To evaluate the signiﬁcance
of these two effects on our perception of errors in distortion,

8

Fig. 10. Human sensitivity to errors in (a) pitch, (b) roll, (c) ﬁeld of view
and (d) distortion. We use the percentage of users choosing the image
with an object inserted with the ground truth calibration as a measure of
human sensitivity. 50% represents perfect confusion: users are as likely
to choose the image with a distorted calibration than the ground truth,
and 100% means that all humans could detect the ground truth image.
The solid line represents the median of the percentage of people who
pick the ground truth human for each image, and the light shaded regions
show the ﬁrst and third quartile.

we begin by conducting preliminary Experiment 2a on
27 images of three different scene types: artiﬁcial (cube
with graduated lines, see ﬁg. 9), urban and nature. This
preliminary Experiment 2a aims to answer the question
“How does the image content affects human sensitivity to lens
distortion?”. The position of the object varies horizontally
while the error on distortion is kept at a constant high value
(|∆ξ| = 1). 18 participants were recruited from research labs,
and each observed 54 image pairs corresponding to a total
of 972 pairs analyzed.

The preliminary Experiment 2a results, displayed in ﬁg. 9,
show that observers are much less sensitive to errors in
distortion for nature scenes (average accuracy of 43%) than
for artiﬁcial (75%) or urban (74%) scenes (see ﬁg. 9). Based on
this observation, only urban images were used in Experiment
2. These results also show that the position of the object in
the image does not have an impact on human sensitivity
to errors in distortion, except when the object is inserted
exactly in the center of the image. In this case, accuracy
falls to nearly 50%, meaning humans can not differentiate
between the ground truth and distorted objects. Following
these results, the objects were not inserted in the middle 20%
of the image in Experiment 2.
Stimuli We then conduct the full Experiment 2, where
the error on distortion is varying. This experiment explores
“How sensitive are humans to calibration errors in camera
rotation, ﬁeld of view and lens distortion?”. To answer
this, we generate 200 images (56 indoors and 144 outdoor),
with distortion errors ∆ξ uniformly sampled from -1 to 1,
while keeping the value ξ ∈ [0, 1] and keep all the other
parameters ﬁxed. To remove the variance related to shape in

(a)(b)(c)(d)9

Fig. 11. Human sensitivity to errors in ﬁeld of view (top left), roll (top right), pitch (bottom left) and distortion (bottom right) as a function of individual
parameter values, along with examples of image pairs shown to the users. We bin the percentage of people choosing the ground truth per image of
the user study. The colors in the plot represents the median over all values in each bin. Note the strong relation between the roll value and its human
sensitivity to error. Some combinations of parameters and errors makes it impossible to perform insertion leading to missing values in the ﬁgure (e.g.,
the ground is not visible anymore in the image (bottom left in pitch), the resulting ﬁeld of view would be negative (bottom left in ﬁeld of view) or the
resulting distortion would be negative (bottom left and top right in distortion). See the supplementary material for more analysis, including joint
modeling of distortions on multiple parameters.

our distortion analysis, we only insert the square prism in all
of the images since round shapes provide cues that are too
ambiguous to assess distortion (see Experiment 1 in sec. 7.2).
Procedure
122 participants, different than for Experiment
2a, were recruited from research labs. Each participant was
shown a randomly-selected subset of up to 50 images,
corresponding to 4843 pairs of images.

7.4 Experimental results

We now report on the results of Experiments 1 and 2 jointly.
We report on the observers sensitivity to errors in camera
parameters ﬁrst as a function of the ground truth value, and
second as a function of both the ground truth and the error.

7.4.1 Error in camera parameters
We evaluate the sensitivity of human observers to errors in
camera parameters for each investigated parameter indepen-

dently and illustrate the results in ﬁg. 10. This curve was
generated by computing the percentage of times observers
preferred the ground truth over the distorted image, and
compute the median and percentiles across all images (and
different virtual objects) that share the same amount of
distortion. The higher the percentage in the y-axis, the more
humans are prone to detect errors. Conversely, 50% indi-
cates perfect confusion: subjects were unable to distinguish
between the ground truth and the distorted version.

First, we note that when the error in camera parameters
is close to 0, confusion nears 50%, which is expected. What is
interesting is how quickly sensitivity rises when increasing
the error. For example, subjects could tolerate an error in
pitch up to 0.2 in rescaled image units (see sec. 3), but
beyond this threshold, subjects started to distinguish the
distortion (ﬁg. 10-a). The high sensitivity to roll errors is

ground truthground truthdistorteddistortedground truthdistortedground truthdistortedground truthdistortedground truthdistortedground truthdistortedground truthdistortedground truthdistortedground truthdistortedmost prominent (ﬁg. 10-b), where errors of 12◦ and more
are almost systematically detected and only a small range of
approximately ±2.5◦ roll error go unnoticed.

We note a large tolerance to negative errors in ﬁeld of
view (ﬁg. 10-c). Large positive errors (right side of the plot)
translate to rendering an object with a ﬁeld of view that is
larger than that of the background. This results is increased
perspective effects on the object, which tend to be visible. On
the other hand, negative errors indicate that the perspective
effect is not as pronounced on the object as it should be with
respect to the background image. In this scenario, subjects
seemed to have been unable to differentiate between the
ground truth and the distorted object. A similar trend for
errors in lens distortion is observed in ﬁg. 10-d). On images
with negative errors, meaning the object has less distortion
(i.e. is straighter) than the background, subjects could not
reliably identify the object with correct calibration. However,
subjects were increasingly sensitive to positive errors in
distortion, where the object has a greater distortion than
the background.

7.4.2 Joint space of error in camera parameters and abso-

lute parameter value

To evaluate if there are regions in parameter space where
errors are more noticeable, we plot observer sensitivity
to errors in camera parameters according to the joint 2D
space of errors and absolute parameter value in ﬁg. 11.
Note that white squares in these plots indicate impossible
parameter/error conﬁgurations (e.g., ground plane is not
visible in the image and thus no virtual object can be inserted,
lens distortion values outside its admissible range, etc.).

The results suggest that observer sensitivity to pitch error
in the inserted object does not correlate strongly with pitch
of the background image: larger error is more noticeable
across the board. Similarly, sensitivity to errors in ﬁeld of
view seems to be constant across all ﬁelds of view. However,
camera roll appears to have an inﬂuence on the perception of
roll error: images with high roll (positive or negative) allow
more room for roll estimation errors. The lens distortion also
has an inﬂuence on the perception of its error: only when the
background has relatively little distortion do observers notice
the discrepancy when the inserted object is signiﬁcantly
curved, but not the other way around.

7.4.3 Summary of ﬁndings

In summary, our Experiments reveal that: 1) larger absolute
errors in pitch and roll are more noticeable; 2) larger positive
errors in ﬁeld of view and roll are more noticeable; 3) large
roll, or low distortion, allow for errors to be less perceivable.

7.5 Evaluation on a perceptual measure

We use the perception data gathered during these experi-
ments to build a quantitative perceptual measure for evalu-
ating and comparing approaches. For this, the data is binned
in a 7 × 7 histogram on the 2D space of errors and absolute
parameter (as in ﬁg. 11), and piece-wise linear functions
are ﬁtted on the bins. The resulting function indicates, for
a particular set of camera parameters and errors on the
estimation of these parameters, whether humans would
perceive that error (value of 100%) or not (value of 50%).

10

(a)

(c)

(b)

(d)

Fig. 12. Quantitative comparison of (a) roll, (b) pitch, (c) horizontal
effective ﬁeld of view, (d) distortion on our perceptual measure (lower
is better, 50% means the network can fool a human). SVA [43] could
not be evaluated on our perceptual measure for the last two parameters,
because they use a different distortion model (division model) than ours
(spherical model). Note that SVA fails on 49% of the test images.

Fig. 12 compares approaches in how humans would perceive
their errors, and shows that our approach yields errors that
are least noticeable amongst all approaches according to a
human perceptual measure.

8 DISCUSSION
Limitations Despite providing competitive accuracy, our
method bears some limitations. Notably, the uniﬁed spherical
model has an ambiguity between the focal length and distor-
tion parameter [80], [81], [82], leading to multiple parameters
yielding the same (or very similar) projection of a world
point in the image. A promising extension of our approach is
to remove this ambiguity in a more compact representation.
In addition, images with extreme roll (greater than 60◦) or
pitch (looking straight up/down) angles are not properly
estimated due to the way we perform our data sampling. If
the image is cropped, the principal point is no longer at the
center of the image, which breaks the radial symmetry and
leads to errors in distortion estimation. Finally, images devoid
of textures or recognizable scene elements (e.g. looking at a
ﬂat wall) do not possess sufﬁcient cues for accurate camera
calibration. Please see the supp. mat. for visual examples.
Lastly, we experimented with our perceptual measure to
train our network. Unfortunately, doing so did not improve
performance improve over regular training. We hypothesize
that since the optimum is at the same location for both
perceptual and regular losses—i.e., when the errors are
0—the neural network is optimized to a similar minima.
Furthermore, the DenseNet architecture used as backend is
not speciﬁcally tailored for the camera calibration task. We
are hopeful that injecting domain knowledge into the neural
network architecture will prove to be a fruitful direction for
research in the near future.
Conclusion
In this paper, we present a method to auto-
matically estimate camera parameters from a single image.
Our method is the ﬁrst to jointly estimate camera pitch,
roll, ﬁeld of view, and lens distortion—and can be applied

0.40.20.00.20.4Pitch value [image units]5060708090100Perceptual measure (%)OursSVAUprightDeepHorizonUprightNet1680816Roll value [°]5060708090100Perceptual measure (%)OursSVAUprightDeepHorizonUprightNet2741556983Horizontal effective field of view value [°]5060708090100Perceptual measure (%)OursUpright0.10.30.50.70.9Distortion value [-]5060708090100Perceptual measure (%)Oursto both narrow and wide-angle ﬁelds of view. We then
establish a link between the performance of our approach
and human sensitivity to camera calibration errors by using
virtual object insertion as a test bed and by running two
large scale perceptual experiments. In this sense, our work
aligns with the recent relationships drawn between human
perception and deep learning on 2D images [83], [84], and
extend those ideas to 3D and the camera itself.

In our experiments, we ﬁnd that humans are sensitive
to calibration errors in a non-uniform way: there are conﬁg-
urations for which small errors are perceptible and others
where large errors go unnoticed. Notably, we found that
humans are particularly sensitive to errors in roll only when
the image is level with the ground. When in doubt, humans
tend to prefer images with lower perspective distortion. The
human perception data can be aggregated into a quantitative
perceptual measure which shows that our approach also
yields the best results according to human perception.

ACKNOWLEDGMENTS
This research was partially funded by the NSERC Discov-
ery Grant RGPIN-2014-05314, the Korea NRF grant NRF-
2017R1C1B5077030, an NSERC USRA to Dominique Pich´e-
Meunier, generous gift funding from Adobe to JC Bazin and
JF Lalonde, and Nvidia who donated GPUs. We are grateful
to Michel Antunes and Jo˜ao P. Barreto for running their
line-based method on our image dataset.

Yannick Hold-Geoffroy is a Senior Research
Scientist and Engineer with Adobe in San Jos ´e.
He received a Ph.D. degree in Electrical Engineer-
ing with a mention on the dean’s honor roll from
Universit ´e Laval, Canada, in 2018, for which he
was awarded the CIPPRS Doctoral Dissertation
Award in 2019. His research interests lie in the
understanding of natural images through learned
priors, aiming to help artists and engineers alike.

Dominique Pich ´e-Meunier has completed his
undergraduate degree in Engineering Physics at
Universit ´e Laval in May of 2021. He is pursuing
his M.S. in Electrical Engineering at the same
institution since the Fall of 2021, working on
camera parameters estimation from images.

Kalyan Sunkavalli is a Principal Scientist at
Adobe Research. He received his Ph.D. from
Harvard University in 2012, and his Masters in
2006 from Columbia University. He graduated
from the Indian Institute of Technology, Delhi in
2003. His research interests lie at the intersection
of computer vision and graphics; in particular,
his work focuses on understanding visual ap-
pearance in images and videos—illumination,
geometry, reﬂectance, etc.—and building tools
that enable users to edit and enhance them.

11

Jean-Charles Bazin is now at Meta. He re-
ceived his MS from the Universit ´e de Technologie
de Compi `egne, France, in 2006, and his PhD
from KAIST in 2011. He worked as Associate
Research Scientist at Disney Research Zurich,
Switzerland (2014-2016). Before joining Disney
Research, he was a postdoc at ETH Zurich,
Switzerland (2011-2014). He also worked as
Postdoctoral Fellow at the Computer Vision Lab
of Prof. Katsushi Ikeuchi, University of Tokyo,
Japan (2010-2011).

Franc¸ ois Rameau received his Master degree in
Vision and Robotics (VIBOT, Erasmus mundus)
from the University of Burgundy in 2011. He
completed his PhD degree in 2014 under the
joined supervision of Prof. David Foﬁ, Prof. Cedric
Demonceaux and Prof. Desire Sidibe. In 2015, he
worked as a full time postdoctoral researcher in
KAIST (Daejeon, South Korea). He is nowadays
an Associate Research Professor in the Robotics
and Computer Vision (RCV) laboratory at KAIST.

Jean-Franc¸ ois Lalonde is a Professor at Uni-
versit ´e Laval. Previously, he was a Post-Doctoral
Associate at Disney Research, Pittsburgh. He
received a Ph.D. in Robotics from Carnegie Mel-
lon University in 2011. His research interests lie
at the intersection of computer vision, computer
graphics, and machine learning. In particular,
he is interested in exploring how physics-based
models and learning techniques can be uniﬁed to
better understand, model, interpret, and recreate
the richness of our visual world.

REFERENCES

[1] Z. Zhang, “A ﬂexible new technique for camera calibration,”

[2]

TPAMI, 2000.
J. Heikkila, “Geometric camera calibration using circular control
points,” TPAMI, vol. 22, no. 10, pp. 1066–1077, 2000.

[3] D. Scaramuzza, A. Martinelli, and R. Siegwart, “A toolbox for easily

calibrating omnidirectional cameras,” in IROS, 2006.

[4] R. A. Boby and S. K. Saha, “Single image based camera calibration
and pose estimation of the end-effector of a robot,” in ICRA.
IEEE,
2016, pp. 2435–2440.
J.-F. Lalonde, D. Hoiem, A. A. Efros, C. Rother, J. Winn, and
A. Criminisi, “Photo clip art,” SIGGRAPH, vol. 26, no. 3, p. 3,
2007.

[5]

[6] P. Debevec, “Rendering synthetic objects into real scenes : Bridging
traditional and image-based graphics with global illumination
and high dynamic range photography,” in Proceedings of ACM
SIGGRAPH, 1998, pp. 189–198.

[7] P. Cavanagh, “The artist as neuroscientist,” Nature, vol. 434, 2005.
[8] H. Farid and M. J. Bravo, “Image forensic analyses that elude the
human visual system,” in Media forensics and security II, vol. 7541.
International Society for Optics and Photonics, 2010, p. 754106.
[9] O. Bogdan, V. Eckstein, F. Rameau, and J.-C. Bazin, “DeepCalib: a
deep learning approach for automatic intrinsic calibration of wide
ﬁeld-of-view cameras,” in CVMP, 2018.

[10] Y. Hold-Geoffroy, K. Sunkavalli, J. Eisenmann, M. Fisher, E. Gam-
baretto, S. Hadap, and J.-F. Lalonde, “A perceptual measure for
deep single image camera calibration,” in CVPR, 2018.

[11] A. Criminisi, I. Reid, and A. Zisserman, “Single view metrology,”

IJCV, vol. 40, no. 2, pp. 123–148, 2000.

[12] A. Criminisi and A. Zisserman, “Shape from texture: homogeneity

revisited,” in BMVC, 2000.

[13] D. F. Fouhey, A. Gupta, and M. Hebert, “Data-driven 3D primitives

for single image understanding,” in ICCV, 2013, pp. 3392–3399.

[14] V. Hedau, D. Hoiem, and D. A. Forsyth, “Recovering the spatial

layout of cluttered rooms,” in ICCV, 2009, pp. 1849–1856.

[15] H. Izadinia, Q. Shan, and S. M. Seitz, “IM2CAD,” in CVPR, 2017,

pp. 2422–2431.

[16] D. Hoiem, A. A. Efros, and M. Hebert, “Putting objects in perspec-

tive,” in CVPR, 2006, pp. 2137–2144.

[17] R. I. Hartley and A. Zisserman, Multiple View Geometry in Computer

Vision, 2nd ed. Cambridge University Press, 2004.

[18] P. Sturm and S. Maybank, “On plane-based camera calibration: A
general algorithm, singularities, applications,” in CVPR, 1999, pp.
1432–1437.

[19] F. Remondino and C. Fraser, “Digital camera calibration methods:
considerations and comparisons,” International Archives of the
Photogrammetry, Remote Sensing and Spatial Information Sciences,
vol. 36, no. 5, pp. 266–272, 2006.

[20] J. Heikkil¨a and O. Silv´en, “A four-step camera calibration procedure
with implicit image correction,” in CVPR, 1997, pp. 1106–1112.
[21] Q. Chen, H. Wu, and T. Wada, “Camera calibration with two

arbitrary coplanar circles,” in ECCV, 2004, pp. 521–532.

[22] C. Mei and P. Rives, “Single view point omnidirectional camera

calibration from planar grids,” in ICRA, 2007.

[23] S. Gasparini, P. Sturm, and J. P. Barreto, “Plane-based calibration of

central catadioptric cameras,” in ICCV, 2009.

[24] S. Shah and J. K. Aggarwal, “A simple calibration procedure for

ﬁsh-eye (high-distortion) lens camera,” in ICRA, 1994.

[25] X. Ying and H. Zha, “Identical projective geometric properties of
central catadioptric line images and sphere images with applica-
tions to calibration,” IJCV, 2008.

[26] V. Larsson, T. Sattler, Z. Kukelova, and M. Pollefeys, “Revisiting
Radial Distortion Absolute Pose,” in ICCV, Oct. 2019, pp. 1062–
1071.

[27] M. Zhang, J. Yao, M. Xia, K. Li, Y. Zhang, and Y. Liu, “Line-based
multi-label energy optimization for ﬁsheye image rectiﬁcation and
calibration,” in CVPR, 2015.

12

[31] C. Hughes, P. Denny, M. Glavin, and E. Jones, “Equidistant Fish-
Eye Calibration and Rectiﬁcation by Vanishing Point Extraction,”
TPAMI, 2010.

[32] M. Antunes, J. P. Barreto, D. Aouada, and B. Ottersten, “Unsu-
pervised vanishing point detection and camera calibration from a
single manhattan image with radial distortion,” in CVPR, 2017.

[33] W. Xian, Z. Li, M. Fisher, J. Eisenmann, E. Shechtman, and
N. Snavely, “Uprightnet: Geometry-aware camera orientation
estimation from single images,” in ICCV, 2019, pp. 9974–9983.
[34] C. Rother, “A new approach for vanishing point detection in

architectural environments,” in BMVC, 2000, pp. 1–10.

[35] R. Melo, M. Antunes, J. P. Barreto, G. Falc˜ao, and N. Gonc¸alves,
“Unsupervised intrinsic calibration from a single frame using a
”plumb-line” approach,” in ICCV, 2013.

[36] J.-F. Lalonde, S. G. Narasimhan, and A. A. Efros, “What do the
sun and the sky tell us about the camera?” IJCV, vol. 88, no. 1, pp.
24–51, 2010.

[37] S. Workman, R. P. Mihail, and N. Jacobs, “A pot of gold: Rainbows

as a calibration cue,” in ECCV, 2014.

[38] M. Mendonc¸a, I. N. D. Silva, and J. E. Castanho, “Camera calibra-

tion using neural networks,” WSCG, 2002.

[39] S. Workman, C. Greenwell, M. Zhai, R. Baltenberger, and N. Jacobs,
“DeepFocal: a method for direct focal length estimation,” in ICIP,
2015.

[40] J. Rong, S. Huang, Z. Shang, and X. Ying, “Radial lens distortion
correction using convolutional neural networks trained with
synthesized images,” in ACCV, 2016.

[41] R. Jung, A. Lee, A. Ashtari, and J.-C. Bazin, “Deep360Up: deep
learning-based approach for automatic VR image upright adjust-
ment,” in IEEE VR, 2019.

[42] J. Jung, B. Kim, J.-Y. Lee, B. Kim, and S. Lee, “Robust upright
adjustment of 360 spherical panoramas,” The Visual Computer,
vol. 33, no. 6-8, pp. 737–747, 2017.

[43] Y. Lochman, O. Dobosevych, R. Hryniv, and J. Pritts, “Minimal
solvers for single-view lens-distorted camera auto-calibration,” in
WACV, January 2021, pp. 2887–2896.

[44] H. Wildenauer and B. Micusik, “Closed form solution for radial
distortion estimation from a single vanishing point,” in BMVC,
vol. 1, 2013, p. 2.

[45] J. Pritts, Z. Kukelova, V. Larsson, and O. Chum, “Radially-distorted
conjugate translations,” in Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 2018, pp. 1993–2001.
[46] M. L ´opez-Antequera, R. Mar´ı, P. Gargallo, Y. Kuang, J. Gonzalez-
Jimenez, and G. Haro, “Deep single image camera calibration with
radial distortion,” in CVPR, 2019.

[47] J. P. Barreto, “A unifying geometric representation for central

projection systems,” CVIU, 2006.

[48] D. C. Brown, “Close-range camera calibration,” Photogrammetric

Engineering, 1971.

[49] P. Sturm, S. Ramalingam, J.-P. Tardif, S. Gasparini, and J. P. Barreto,
“Camera models and fundamental concepts used in geometric
computer vision,” Foundations and Trends in Computer Graphics and
Vision, 2011.

[50] A. Fitzgibbon, “Simultaneous linear estimation of multiple view

geometry and lens distortion,” in CVPR, 2001.

[51] O. Pizarro and H. Singh, “Toward large-area mosaicing for under-
water scientiﬁc applications,” IEEE journal of oceanic engineering,
vol. 28, no. 4, pp. 651–672, 2003.

[52] Y. Lin, V. Larsson, M. Geppert, Z. Kukelova, M. Pollefeys, and
T. Sattler, “Infrastructure-based multi-camera calibration using
radial projections,” in ECCV. Springer, 2020, pp. 327–344.
[53] G. Bradski, “The OpenCV Library,” Dr. Dobb’s Journal of Software

Tools, 2000.

[54] The ImageMagick Development Team, “Imagemagick.” [Online].

Available: https://epaperpress.com/ptlens/

[55] Tom Niemann and the PTLens Development Team, “Ptlens.”

[Online]. Available: https://imagemagick.org

[56] Pablo d’Angelo and the Hugin developers, “Hugin.” [Online].

Available: https://http://hugin.sourceforge.net/

[28] S. Workman, M. Zhai, and N. Jacobs, “Horizon lines in the wild,”

[57] Fisheye-Hemi, “https://imadio.com/products/prodpage hemi.

in BMVC, 2016.

aspx,” 2015.

[29] J. P. Barreto and H. Ara ´ujo, “Geometric properties of central
catadioptric line images and their application in calibration,”
TPAMI, 2005.

[30] H. Lee, E. Shechtman, J. Wang, and S. Lee, “Automatic upright
adjustment of photographs with robust camera calibration,” TPAMI,
vol. 36, no. 5, pp. 833–844, 2014.

[58] R. Carroll, M. Agrawala, and A. Agarwala, “Optimizing content-

preserving projections for wide-angle images,” TOG, 2009.

[59] R. Pepperell, A. Burleigh, N. Ruta, and T. Langford, “Fovo: A
ﬂexible real-time computer graphics rendering process,” Proceedings
of EVA London 2019, pp. 237–238, 2019.
[60] M. D. Fairchild, Color appearance models.

John Wiley & Sons, 2013.

13

[61] P. Vangorp, C. Richardt, E. A. Cooper, G. Chaurasia, M. S. Banks,
and G. Drettakis, “Perception of perspective distortions in image-
based rendering,” TOG, vol. 32, no. 4, pp. 58:1–58:12, 2013.

[62] C. Ledig, L. Theis, F. Huszar, J. Caballero, A. Cunningham,
A. Acosta, A. P. Aitken, A. Tejani, J. Totz, Z. Wang, and W. Shi,
“Photo-realistic single image super-resolution using a generative
adversarial network,” in CVPR, 2017, pp. 105–114.

[63] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, “Show and tell: A

neural image caption generator,” in CVPR, 2015, pp. 3156–3164.

[64] A. Papazoglou, L. D. Pero, and V. Ferrari, “Video temporal
alignment for object viewpoint,” in ACCV, 2016, pp. 273–288.
[65] X. Ying and Z. Hu, “Can we consider central catadioptric cameras
and ﬁsheye cameras within a uniﬁed imaging model,” in ECCV,
2004.

[66] K. Wilson and N. Snavely, “Robust global translations with 1dsfm,”

in ECCV, 2014.

[67] Y. Hold-Geoffroy, K. Sunkavalli, S. Hadap, E. Gambaretto, and J.-F.
Lalonde, “Deep outdoor illumination estimation,” in CVPR, 2017.
[68] G. Huang, Z. Liu, K. Q. Weinberger, and L. van der Maaten,
“Densely connected convolutional networks,” in IEEE Conference on
Computer Vision and Pattern Recognition, 2016.

[69] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. S. Bernstein, A. C. Berg, and
F. Li, “Imagenet large scale visual recognition challenge,” IJCV, vol.
115, no. 3, pp. 211–252, 2015.

[70] D. Kingma and J. Ba, “ADAM: A Method for Stochastic Opti-
mization,” in International Conference for Learning Representations,
2015.

[71] J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller,
“Striving for simplicity: The all convolutional net,” International
Conference on Learning Representations, 2015.

[72] D. Smilkov, N. Thorat, B. Kim, F. Vi´egas, and M. Wattenberg,
“Smoothgrad: removing noise by adding noise,” arXiv preprint
arXiv:1706.03825, 2017.

[73] J. Kannala and S. S. Brandt, “A generic camera model and
calibration method for conventional , wide-angle, and ﬁsh-eye
lenses,” TPAMI, 2006.

[74] J. L. Sch ¨onberger and J. Frahm, “Structure-from-motion revisited,”

in CVPR, 2016.

[75] J. L. Sch ¨onberger, E. Zheng, M. Pollefeys, and J.-M. Frahm,
“Pixelwise view selection for unstructured multi-view stereo,” in
ECCV, 2016.

[76] B. Zhou, `A. Lapedriza, A. Khosla, A. Oliva, and A. Torralba, “Places:
A 10 million image database for scene recognition,” TPAMI, vol. 40,
no. 6, pp. 1452–1464, 2018.

[77] M.-A. Gardner, K. Sunkavalli, E. Yumer, X. Shen, E. Gambaretto,
C. Gagn´e, and J.-F. Lalonde, “Learning to predict indoor illu-
mination from a single image,” ACM Transactions on Graphics
(SIGGRAPH Asia), vol. 9, no. 4, 2017.

[78] Blender Online Community, Blender - a 3D modelling and rendering
package, Blender Foundation, Blender Institute, Amsterdam, 2021.
[Online]. Available: http://www.blender.org

[79] Y. Gingold, A. Shamir, and D.-O. Cohen, “Micro perceptual human
computation for visual tasks,” ACM Transactions on Graphics, vol. 31,
no. 5, 2012.

[80] R. Hartley and S. B. Kang, “Parameter-free radial distortion
correction with center of distortion estimation,” TPAMI, 2007.
[81] H. Li and R. Hartley, “A non-iterative method for correcting lens
distortion from nine point correspondences,” OMNIVIS, 2005.
[82] K. Cornelis, M. Pollefeys, and L. V. Gool, “Lens distortion recovery
for accurate sequential structure and motion recovery,” in ECCV,
2002.

[83] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, “The
unreasonable effectiveness of deep features as a perceptual metric,”
in Proceedings of the IEEE conference on computer vision and pattern
recognition, 2018, pp. 586–595.

[84] J. Johnson, A. Alahi, and L. Fei-Fei, “Perceptual losses for real-
time style transfer and super-resolution,” in European conference on
computer vision. Springer, 2016, pp. 694–711.

