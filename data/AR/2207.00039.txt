2
2
0
2

n
u
J

0
3

]
E
M

.
t
a
t
s
[

1
v
9
3
0
0
0
.
7
0
2
2
:
v
i
X
r
a

K-ARMA Models for Clustering Time Series Data

Derek O. Hoare
Department of Statistics and Data Science
Cornell University
doh24@cornell.edu

David S. Matteson
Department of Statistics and Data Science
Cornell University
dm484@cornell.edu

Martin T. Wells
Department of Statistics and Data Science
Cornell University
mtw1@cornell.edu

Abstract

We present an approach to clustering time series data using a model-based
generalization of the K-Means algorithm which we call K-Models. We
prove the convergence of this general algorithm and relate it to the hard-EM
algorithm for mixture modeling. We then apply our method ﬁrst with an
AR(p) clustering example and show how the clustering algorithm can be
made robust to outliers using a least-absolute deviations criteria. We then
build our clustering algorithm up for ARMA(p, q) models and extend this
to ARIMA(p, d, q) models. We develop a goodness of ﬁt statistic for the
models ﬁtted to clusters based on the Ljung-Box statistic. We perform
experiments with simulated data to show how the algorithm can be used
for outlier detection, detecting distributional drift, and discuss the impact
of initialization method on empty clusters. We also perform experiments
on real data which show that our method is competitive with other existing
methods for similar time series clustering tasks.
Keywords: Clustering Algorithms, Time Series Analysis, ARMA Models

1

Introduction

Today, an abundance of information is collected over time. This ranges from climate data
to server metrics, biometric to sensor data, and digital signals to logistics information.
With technological advances, we are also capable of collecting more time series data than
ever before. Large quantities of time series data, however, come with the inability for an
individual to inspect all of the data. As a consequence, we need methods for identifying
patterns in large quantities of time series data that require only minimal manual overhead.
One of the ways we can do this is by clustering the time series data and performing analysis
at the cluster level.

Several realizations of time series from the same generative process may look very different
from each other yet still have the same underlying process. In these cases, a model based
clustering approach can do a much better job of clustering time series that were generated
from the same process than traditional point-based clustering methods like the K-Means
algorithm. Model-based time series clustering methods have proven to be useful in many

Preprint.

 
 
 
 
 
 
real-world situations. They have been used to cluster gene expression data [1], to understand
within-person dynamics [2], to better understand currency exchange dynamics [3], to cluster
input signals [4], and more. We study the time series clustering problem using new model-
based clustering methods focusing on the general class of Autoregressive Moving Average
models.

Autoregressive Moving Average (ARMA) processes and their Integrated extension (ARIMA)
can be used to describe many observed time series, so we may know or strongly suspect that
a collection observed time series come from an ARMA process. In such a case, we should
take advantage of this underlying ARMA structure when clustering time series. This has led
to several methods for clustering ARMA time series including approaches using features
derived from the time series, mixture modeling approaches, methods which cluster the raw
time series data [5], and models ﬁt to entire clusters of data [2, 6, 7]. Surveys of time series
clustering methods can be found in [8, 9, 10].

The method we explore in this paper uses a model-based generalization of the K-Means
algorithm which we call K-Models. We apply this general framework to clustering ARMA
time series by ﬁtting ARMA models to entire clusters of data during the clustering process.
The approach we use is most similar in ﬂavor to the ARMA mixture modeling approach [11].
However, where they use an expectation maximization (EM) algorithm to ﬁt mixtures of
ARMA models to their time series and identify clusters based on membership probabilities,
the K-Models approach takes a hard thresholding approach with potential convergence
advantages.

1.1 A Motivating Example

Our main reason for studying model-based clustering is that it is able to capture similarities
that distance-based clustering methods cannot. As a motivating example, consider the three

Figure 1: Three AR(1) time series. The Euclidean distances between the time series are
d(Series 1, Series 3) = 1.049016, d(Series 2, Series 3) = 1.215401, d(Series 1, Series 2) =
1.462759. Looking at the autocorrelation plots we see that Series 1 and Series 2 appear more
similar to each other than to Series 3.

2

Time SeriesSeries 1−0.20.00.2AR(1), f = −0.5Series 2−0.20.00.2AR(1), f = −0.5TimeSeries 301020304050−0.20.00.2AR(1), f = 0.5−0.50.00.51.0ACF Plots−0.50.51.0051015−0.40.20.8Lagtime series in Figure 1. Series 1 and Series 2, shown in red and blue were both generated
from the exact same AR(1) process with coefﬁcient φ1 = −0.5. Series 3, shown in black,
was generated from an AR(1) process with coefﬁcient φ1 = 0.5. When we look at the sums
of squared differences between the AR(1) time series, however, we would conclude that the
Series 1 (red) is more similar to Series 3 (black) than it is to Series 2 (blue), even though
Series 1 and Series 2 were generated from the same AR(1) process. This suggests that
traditional clustering methods that rely on distance measure between the time series such as
the Euclidean distance are unable to capture similarities in the generative processes of time
series. On the other hand, if we look at the ﬁtted AR(1) coefﬁcients we have ˆφ red
1 = −0.4403,
ˆφ blue
1 = −0.6039, and ˆφ black
= 0.6449. Looking at these model coefﬁcients, Series 1 and
Series 2 appear much more similar. Clustering these series using the model-based approach
that we develop in this paper using 2 clusters also identiﬁes Series 1 and Series 2 as belonging
to one cluster and the black as belonging to another.

1

Being able to identify time series that come from the same process is important in the
applications we have discussed. For clustering input signals, the clusters represent that
the inputs come from the same process, so the underlying process is more important than
the speciﬁc realizations of that process. For clustering gene expressions in response to a
treatment, it is the trend in how a gene is regulated in response to that treatment that is
important, not the magnitude of the response.

1.2 Organization

In Section 2 we describe a general algorithm for model-based clustering which we call K-
Models, and prove a condition for the convergence of the K-Models algorithm. In Section 3
use this K-Models framework to cluster time series using AR(p) models using least squares
and least absolute deviations approaches. In Section 4 we explore the use of the K-Models
framework for clustering ARMA/ARIMA time series. In doing so, we study methods for
ﬁtting a single ARMA model to a collection of time series which are used in our clustering
algorithm. In Section 5 we develop and use an extension of the Ljung-Box test to answer
questions about the goodness of ﬁt of ARMA models to our clusters to detect outliers and
distributional drift in our time series data. In Section 6, we study different initialization
methods for our algorithm. In doing so, we ﬁnd that certain initialization methods lead
to many empty clusters in cases where the number of clusters we ﬁt is greater than the
true number of underlying clusters. In Section 7, we test our algorithm on a few open
source datasets for comparison to other clustering methods. Finally, in Section 8 we discuss
limitations and possible extensions of our clustering approach.

2 K-Means and K-Models

The K-Means algorithm is one of the most widely used unsupervised clustering algo-
rithms. For a given set of data points X1, . . . , Xn ∈ Rd, the K-Means clustering algorithm
seeks to minimize the within-groups sums of squared error over all possible cluster centers
{m1, . . . , mk} ⊆ Rd by using an iterative algorithm of assigning points to clusters and then
updating the cluster centers [12, 13]. If we view the cluster centers mi as a constant function
representation of a cluster, then we can view K-Means as a model-based clustering algorithm
where we assign each point to the constant function that best represents that point.

This observation about the K-Means algorithm has led to several derivative model-based
clustering algorithms including Clusterwise Linear Regression where the models used are
linear models [14], K-Random Forests where the models used are Random Forests [15],
K-Means based kernel clustering algorithms [16], and has been described for some general
dissimilarity measures [17, 18]. We call the model-based generalization of the K-Means
algorithm K-Models (see Algorithm 1).

3

Algorithm 1: The K-Models Clustering Algorithm
Data: X1, . . . , Xn, a model class M , a loss function L, and a number of clusters k.
Result: A set of clusters C1, . . . ,Ck and a set of models M1, . . . , Mk ∈ M

Randomly initialize Mi ∈ M for i = 1, . . . , k with Mi (cid:54)= M j for i (cid:54)= j.
while not converged do
for i=1 to k do

Ci ← (cid:8)Xl : L(Xl, Mi) ≤ L(Xl, M j)∀ j (cid:54)= i(cid:9)

// Assignment Step

// Update Step

end
for i=1 to k do
Mi ← ˆM(Ci)

end

end

We point out that the update step in Algorithm 1, typically the most computationally intensive
step, can be computed in parallel across the clusters of data. Furthermore, we are able to
guarantee the convergence of the K-Models algorithm to a local optimum for a large class of
models.
Theorem 1. Let M be a family of models. Let X1, . . . , Xn ∈ Rd. Let L : Rd × M → R≥0 be
a loss function. If ˆM(Ci) = arg minM∈M ∑X∈Ci L(X, M) then Algorithm 1 converges.

Proof. This proof follows the proof of convergence for the K-Means algorithm. At a given
step in the algorithm, let C = {Ci} and M = {Mi} be our candidate clusters and their
corresponding models. Consider the global loss function

E(C, M) =

k
∑
i=1

∑
X∈Ci

L(X, Mi).

At the assignment step we assign each point Xi to the cluster in which they have the smallest
loss to get a set of clusters C(cid:48) = {C(cid:48)

i} which by construction satisfy the relationship

E(C(cid:48), M) =

k
∑
i=1

∑
X∈C(cid:48)
i

L(X, Mi) ≤

k
∑
i=1

∑
X∈Ci

L(X, Mi) = E(C, M).

For the update step, we construct M(cid:48) = {M(cid:48)
∑X∈C(cid:48)
i

L(X, Mi). Thus

i } so that M(cid:48)

i satisﬁes ∑X∈C(cid:48)

L(X, M(cid:48)

i ) ≤

i

E(C(cid:48), M(cid:48)) =

k
∑
i=1

∑
X∈C(cid:48)
i

L(X, M(cid:48)

i ) ≤

k
∑
i=1

∑
X∈C(cid:48)
i

L(X, Mi) = E(C(cid:48), M).

So we have that the total loss E is monotonically decreasing with each step of the iteration
in the K-Models algorithm. Since E is bounded below by 0, this implies the convergence of
E to a local minimum. Therefore Algorithm 1 converges.

Theorem 1 says is that if we use the same loss function for both ﬁtting the models and
performing the cluster assignment, then Algorithm 1 converges to a local optimum solution.
We can see that Theorem 1 implies the convergence of the K-Means algorithm since both the
assignment and the update steps use the euclidean distance as the loss L. Additionally, we
will use Theorem 1 to imply the convergence of our clustering algorithms by using a least
squares criteria in the update step when we ﬁt our time series models using least squares
methods, and by using a least absolute deviation criteria in the update step when we ﬁt
models using median regression techniques.

4

2.1 Relationship to Mixture Modeling and Expectation Maximization Algorithms

The K-Models framework presents a hard clustering approach to model-based clustering
which can accommodate many model classes. One of the competitors to this framework is
the mixture modeling approach which has a nice probabilistic framework and a general ﬁtting
procedure, namely the EM algorithm [19]. This mixture modeling framework appears to be
more commonly used even though it has many of the same ﬂaws as the K-Models framework
including convergence to a local optimum solution and convergence rates dependent on the
initialization choice.

As a part of the EM algorithm for mixture modeling, during the E-step class membership
probabilities are computed. These are usually of the form

∑k
where fθi is the density of the ith mixture component with parameters θi. By taking the
densities to a power α > 0, we get

P(X ∈ Ci) =

fθi(X)
j=1 fθ j (X)

,

Pα (X ∈ Ci) =

fθi(X)α
j=1 fθ j (X)α

.

∑k
This Pα is often used for annealing the EM-algorithm by slowly increasing α from 0 → 1 to
help it get away from local solutions and reach a globally optimum solution [20]. Assuming
no ties, taking a limit as α → ∞ yields

P∞(X ∈ Ci) =

(cid:26)1
0

if i = arg max j fθ j (X);
otherwise.

By adding the parameter α and taking the limit as α → ∞, we have essentially converted
the EM algorithm into a hard clustering algorithm since during the E-step we would be
assigning elements to the class in which they have the highest likelihood, and during the
M-step we would compute the parameters based on those class assignments. Using P∞ for
the cluster membership probabilities is sometimes called hard-EM. The E-step is analogous
to the assignment step in Algorithm 1 and the M-step is analogous to the update step.

We point this out because increasing α to be greater than 1 has been used to hasten the
convergence rate of the EM algorithm [21], and K-Means algorithms have in several cases
been shown to have better convergence rates than EM algorithms [22, 23]. This also helps
explain why using K-Means can be useful for initializing EM runs for Gaussian mixture
models, and how we can extend this idea to using K-Models to initialize EM Algorithms for
more complicated mixture modeling problems.

The relationship between K-Models and EM algorithms for mixture modeling can be sum-
marized with the following theorem.
Theorem 2. Hard-EM for mixture modeling is a special case of K-Models if the likelihood
function is bounded above.

Proof. Consider data X = {X1, . . . , Xn} and k mixture components of the densities fθ . If the
likelihood function is bounded above, then loglik(θ , X) < C for all θ , so we can consider
M(θ , X) = C −loglik(θ , X) > 0. Finding the value of θ that minimizes M(θ , X) is equivalent
to ﬁnding the value of θ that maximizes lik(θ , X), and M(θ , X) satisﬁes the properties of a
loss function for Algorithm 1.

Equivalence of hard E-step and K-Models assignment Step: In the hard-EM algorithm, we
have P∞(X ∈ C j) = 1 if and only if fθ(cid:96)(X) < fθ j (X) for all (cid:96) (cid:54)= j. This happens if and only if
C − log fθ(cid:96)(X) > C − log fθ j (X) for all (cid:96) (cid:54)= j. The likelihood lik(θ , X) = fθ (X), so we have
M(θ(cid:96), X) > M(θ j, X) for all (cid:96) (cid:54)= j and thus we have that P∞(X ∈ C j) = 1 is equivalent to the

5

assignment rule X ∈ C j if and only if M(θ j, X) < M(θ(cid:96), X) which is the assignment rule we
have in the K-Models algorithm.

Equivalence of hard M-step and K-Models update Step: The M-step in the hard-EM algorithm
takes the form ˆθ j = arg maxθ ∏X∈C j fθ (X). Computing, we have
ˆθ j = arg max

fθ (X)

X∈C j

θ ∏
θ ∑

X∈C j

θ ∑
θ ∑

= arg max

log( fθ j (X))

= arg min

θ

= arg min

X∈C j
|C j| ·C − ∑
X∈C j
(C − log fθ (X))

log fθ (X)

= arg min

M(θ , X)

X∈C j
M(θ ,C j).

= arg min

θ

So the set of parameters we get for the model ﬁt in the hard-EM algorithm is the same
as the set of parameters we get using the K-Models approach using the cluster-wide loss
M(θ ,C j) = ∑X∈C j (C − log fθ (X)).
So, we have that the equivalence between the E-step and the K-Models assignment step, and
the equivalence of the M-step and the K-Models update step which together give use the
equivalence of hard-EM and K-Models for the given choice of loss function.

Several mixture modeling approaches have been developed for time series clustering and
classiﬁcation tasks for which, to our knowledge, hard-clustering analogues have not been
developed. Some examples include the work of Xiong and Yeung who develop an EM
algorithm for clustering ARMA models [11]; the work of Ren and Barnett who develop an
EM algorithm for a Wishart mixture model for clustering time series based on their autoco-
variance matrices [24]; and the work of Coke and Tsao who develop an EM algorithm for a
random effects model for clustering electricity consumers based on their consumption [25].
Theorem 2 says that each of these mixture models has a hard-clustering K-Models analogue.
The corresponding K-Models algorithm can then potentially be used for initialization or
as its own clustering algorithm. We show in the coming sections how this can be done for
clustering ARMA models.

3 K-AR(p) Models

We start our time series clustering approach by discussing AR(p) models, a special case
of the more general ARMA(p, q) model. Unlike general ARMA models and MA models,
AR(p) models do not have any unobserved covariates. This allows us to take a more direct
approach to their estimation and more easily consider robust regression techniques for their
estimation.

Recall that a time series Xt is AR(p) if it takes the form

Xt = at +

p
∑
i=1

φiXt−i,

where the at form a white noise process. If we have several time series X1,t, . . . , Xn,t that
follow the same AR(p) process, then we can ﬁt the parameters φ using the method of

6

conditional least squares:

ˆφLS = arg min
φ

n
∑
j=1

T
∑
t=p+1

(cid:32)

X j,t −

p
∑
i=1

(cid:33)2

φiX j,t−i

,

and in fact, this approach is used by [2] for clustering VAR(1) models and by [6] for
clustering VAR(p) models.

We extend this method and make it more robust to outlier time series by using least absolute
deviations (LAD) instead of least squares regression:

ˆφLAD = arg min
φ

n
∑
j=1

T
∑
t=p+1

(cid:12)
(cid:12)
(cid:12)
X j,t −
(cid:12)
(cid:12)

p
∑
i=1

φiX j,t−i

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

(1)

This can be written as a regression problem in cannonical form as

ˆφLAD = arg min
φ ∈Rp

||Y − Xφ ||1,

where Y(cid:48) = (Y1, Y2, . . . , Yn) and Y(cid:48)

X =













X1
X2
...
Xn



i = (Xi,p+1, . . . , Xi,T ), and
Xi,p−1
Xi,p
Xi,p
Xi,p+1
...
...
Xi,T −1
Xi,T





, where Xi =

Xi,1
Xi,2
...

· · ·
· · ·
. . .
· · · Xi,T −p







.

This reformulation of the problem allows us to use existing packages to solve the L1 regres-
sion problem such as L1pack in R. If we use the sum of absolute deviations as our criteria in
the assignment step and use Equation (1) in the update step of Algorithm 1, then Theorem 1
tells us that Algorithm 1 will converge to a locally optimal solution.

For a simulated example using 10 clusters of AR(2) time series and the LAD criteria, we
were able to correctly recover all 10 clusters when we had 1000 time points for each time
series. Lowering the number of time points to 100 for each time series reduces the recovery
rate slightly. See Appendix C for details.

4 K-ARMA(p,q) Models

Autoregressive Moving Average (ARMA) models are a well studied time series model. A
time series Xt is said to be ARMA(p, q) if it has the following form:

Xt = at +

p
∑
i=1

φiXt−i +

q
∑
j=1

θ jat− j,

where the at form a white noise process with constant variance σa.
In order to use the K-Models paradigm, however, we need to be able to ﬁt a single
ARMA(p, q) model to a corpus of time series data. One standard method for ﬁtting
ARMA(p, q) is the method of maximum likelihood under the additional assumption that
at ∼ N(0, σa). We can extend the use of maximum likelihood for ﬁtting a single ARMA
model to a cluster of time series data (X j,t) which amounts to minimizing the sums of squares
criterion

( ˆφ , ˆθ ) = arg min
φ ,θ

n
∑
j=1

T
∑
t=1

p
∑
i=1

q
∑
k=1

(X j,t −

φiX j,t−i −

θka j,t−k)2,

(2)

where a j,t is the white noise process associated with the time series X j,t (see Appendix A for
a derivation of Equation 2).

7

Algorithm 2: Algorithm for ﬁtting an ARMA(p, q) model to a cluster of time series.
Data: Time series X1,t, . . . , Xn,t of length T , AR order p and MA order q
Result: Parameters φ = (φ1, . . . , φp) and ψ = (ψ1, . . . , ψq) = −θ = (−θ1, . . . , −θq)

// Initialization
Choose starting values φ (0) and ψ (0)
Set u j,t = v j,t = ε j,t = 0 for t < max(p, q) + 1
k ← 0
while not converged do
ε (k)
j,t ← X j,t − ∑
i=1 ψ (k)
q
u j,t ← ∑
i=1 ψ (k)
q
v j,t ← ∑
(∆φ , ∆ψ) ← arg min
∆φ ,∆ψ

i=1 φ (k)
i u j,t−i + X j,t
i v j,t−i − ε (k)

q
i X j,t−i + ∑

j,t
j=1 ∑T

i=1 ψiε (k)

t=max p,q

∑n

p

j,t−i

(cid:16)
ε (k)
j,t − ∑

q
p
i=1 ∆ψiv j,t−i
i=1 ∆φiu j,t−i − ∑

(cid:17)2

φ (k+1) ← φ (k) + ∆φ
ψ (k+1) ← ψ (k) + ∆ψ
k ← k + 1

end

We can turn this into a conditional sums of squares problem by conditioning on X j,1, . . . , X j,p
for j = 1, . . . , n, and assuming that the ﬁrst q innovations are 0. That is a j,1 = · · · = a j,q = 0
for j = 1, . . . , n. The conditional sums of squared criterion can be minimized using a
generalized least-squares procedure, which we modify from the standard procedure1 in [26]
for ﬁtting ARMA(p, q) models to get Algorithm 2. Algorithm 2 allows us to ﬁt ARMA(p, q)
models to a cluster of time series in the update step of Algorithm 1, and Theorem 1 tells
us that we should use the conditional sums of squares criteria for the assignment step to
guarantee convergence of the K-Models algorithm for clustering ARMA(p, q) time series.

While our primary purpose in deriving Algorithm 2 is for its use in clustering, we point
out that it can be used any time we want to ﬁt a single ARMA(p,q) model to a corpus of
time series. In particular, this has been noted to be useful when studying sparse time series
[27, 28].

We present Algorithm 2 for ﬁtting an ARMA(p,q) model where all of the time series are
assumed to be the same length. This can be easily extended to time series of different lengths
by computing all of the values of ε (k)
j,t , u j,t and v j,t up to time Ti, where Ti is the length of
time series Xi and then computing the updates as

(∆φ , ∆ψ) ← arg min
∆φ ,∆ψ

n
∑
j=1

Ti
∑
t=max p,q

(cid:32)

ε (k)
j,t −

p
∑
i=1

∆φiu j,t−i −

q
∑
i=1

(cid:33)2

∆ψiv j,t−i

.

(3)

It is also possible to weight the time series during this computation by 1/Ti so that the longer
time series don’t have an outsize inﬂuence on the ARMA parameters for a cluster, or by
another value of interest such as an estimate of the variance for time series σεi.
The extension of the clustering algorithm from ARMA(p, q) models to ARIMA(p, d, q)
models is straightforward when d is known. During the model ﬁtting process we start by
taking d differences of all of our time series X1,t, . . . , Xn,t to get time series Y1,t, . . . ,Yn,t. We
then run the K-ARMA(p, q) algorithm on the resulting differenced time series (Yi,t) to cluster
time series.

1See Section 7.2.4 in [26] for a nice overview of the algorithm for a single time series.

8

Figure 2: Each point represents the ﬁtted ARMA(1,1) parameters colored by the clusters the
K-Models algorithm identiﬁed.

4.1 An ARMA(1,1) Example

As an example we consider 50 time series of length 200 generated from two ARMA(1,1)
processes. The ﬁrst 25 time series come from an ARMA(1,1) process with φ1 = −0.4 and
θ = −0.2. The rest of the time series come from an ARMA(1,1) process with φ1 = 0.4 and
θ = 0.4. Running the K-ARMAs algorithm on this dataset with k = 2 and correctly speciﬁed
model parameters p = q = 1 we are able to completely recover the two clusters of models
used in our data generation process (see Figure 2).

5 Cluster Diagnostics

Since the K-ARMA(p, q) algorithm is unsupervised, it is useful to ask questions about the
resulting clusters. The two questions that we focus on are:

1. Is the model Mi ﬁtted to cluster Ci appropriate for a time series X ∈ Ci?
2. Is the model Mi ﬁtted to cluster Ci appropriate for the entire cluster Ci?

These questions are particularly important to answer if we want to use the models beyond
just clustering the data such as for making forecasts for the time series in a cluster. We
answer both of these questions using the Ljung-Box test [29]. For the ﬁrst question, we can
apply the Ljung-Box test directly, where the residuals are calculated using the model Mi. For
the second question, however, we need to extend the Ljung-Box test to work with models ﬁt
to clusters of data. We do this by establishing asymptotic distributions for statistics derived
from the Ljung-Box statistic. The proofs of the results presented in this section extend the
original proof of Box and Pierce [30] and are detailed in Appendix B.
Let ˆM be an ARMA model ﬁtted to a set of time series C = {X1,t, . . . , Xn,t} each of length T .
Denote the residuals of the time series Xi,t under ˆM as ˆai,t. We deﬁne the Grouped-Ljung-Box
statistic as:

Qgroup(ˆr) = T (T + 2)

n
∑
i=1

m
∑
l=1

(T − l)−1 ˆr2
i,l,

(4)

9

where m is the number of lags to consider, and ˆri,l is the estimated l-lag auto-correlation of
the residuals for time series Xi,t,

ˆri,l =

(cid:32) T
∑
t=l+1

(cid:33)

ˆai,t ˆai,t−l

/

(cid:33)

ˆa2
i,t

.

(cid:32) T
∑
t=1

(5)

Note that (4) is the sum of Ljung-Box statistics for the residuals of each individual time
series in C, and in the case that a cluster C has only one time series, Qgroup(ˆr) is exactly
equal to the Ljung-Box statistic. Using the prior of work of Box and Pierce, we can derive
the asymptotic distribution of this statistic.
Theorem 3. For n independent time series X1,t, . . . , Xn,t each of length T with the ﬁtted
ˆM, the Grouped-Ljung-Box statistic Qgroup(ˆr) computed with m lags
ARMA(p, q) model
asymptotically follows a χ 2
(nm−p−q) distribution if the model is correctly identiﬁed.

The proof of Theorem 3, while quite similar to the derivation of Box and Pierce [30], is
nontrivial because the l-lag autocorrelations in the residuals of different time series ˆri,l and
ˆr j,l are not independent. See Appendix B for details.
Figure 3 shows Monte-Carlo simulations of the Qgroup(ˆr) statistic plotted with its correspond-
ing asymptotic χ 2 distribution. Visually, we see that as T increases from 200 to 2000 the
asymptotic approximation gets better, as we might expect for a limiting result. Additionally,
a hypothesis test performed using the asymptotic result appears to be a conservative test
when the value of T is smaller.

The asymptotic result presented in Theorem 3 suggests a portmanteau test for the goodness
of ﬁt of model Mi to cluster Ci. That is, we can test the hypotheses

H0 : The are no autocorrelations in the residuals,
H1 : There are autocorrelations in the residuals.

by rejecting H0 at the α-level of signiﬁcance if and only if Qgroup > χ 2
(nm−p−q)(1 − α).
Autocorrelations in the residuals would indicate that the model chosen for a cluster is not
adequately explaining the behavior of the time series in that cluster. This may indicate model
misspeciﬁcation for a cluster, which in turn may indicate the grouping of multiple distinct
clusters into a single cluster during the K-ARMA clustering process.

We can also generalize Theorem 3 to a goodness of ﬁt test about all of the ﬁtted models
and their parameters. Let C = {C1,C2, . . . ,Ck} be a set of clusters of sizes n1, n2, . . . , nk

Figure 3: Distribution of 10000 Monte-Carlo estimates of Qgroup(ˆr) using m = 15 lags for 10
AR(1) time series of length T = 200 (left) and for 10 AR(1) time series of length T = 2000
(right).

10

Figure 4: Distribution of 10000 Monte-Carlo estimates of Qgroup( ˆπ) using m = 15 lags
for 10 AR(1) time series of length T = 200 (left) and for 10 AR(1) time series of length
T = 2000 (right).

respectively and ﬁtted ARMA(pi, qi) models ˆMi. Denote the time series in cluster Ci =
{Xi,1,t, Xi,2,t, . . . Xi,ni,t}. Let ˆri, j,l be the l-lag autocorrelation of the residuals for time series
Xi, j,t under its appropriate model ˆMi. Then we can deﬁne the statistic

Qtotal(ˆr) = T (T + 2)

k
∑
i=1

ni
∑
j=1

m
∑
l=1

(T − l)−1 ˆr2

i, j,l

Corollary 4. The statistic Qtotal(ˆr) asymptotically follows a χ 2
degrees of freedom ν = ∑k

i=1(nim − pi − qi) if the models ˆMi are correctly identiﬁed.

ν distribution where the

Monti showed in [31] that if we have a time series Xt and a proposed ARMA(p, q) model,
then we can replace the ﬁtted autocorrelations in the Ljung-Box statistic with the partial
autocorrelations, denoted ˆπ = ( ˆπ1, . . . , ˆπm) and the resulting statistic

Q( ˆπ) = T (T + 2)

m
∑
l=1

(T − l)−1 ˆπ 2
l

m−p−q distribution. We can use this result to extend the grouped

asymptotically follows a χ 2
version of the Ljung-Box statistic to use the partial autocorrelations as well.
If we let ˆπi,k denote the estimated k-lag auto-correlation of the residuals of time series Xi,t,
then we can use the statistic Qgroup( ˆπ) to perform a statistical test.
Corollary 5. For independent time series X1,t, . . . , Xn,t each of length T with the ﬁtted
ARMA(p, q) model ˆM, the statistic

Qgroup( ˆπ) = T (T + 2)

n
∑
i=1

m
∑
k=1

(T − k)−1 ˆπ 2
i,k

asymptotically follows a χ 2

(nm−p−q) distribution if the model is correctly identiﬁed.

Figure 4 shows Monte-Carlo simulations of the Qgroup( ˆπ) statistic plotted with its correspond-
ing asymptotic χ 2 distribution. These plots appear very similar to the ones we constructed
for Qgroup(ˆr) in that as T increases the approximation gets better, and the hypothesis test
based on the asymptotic result appears to be a conservative test when the value of T is
smaller.

The extension also works when we have time series of different lengths and when we want
to perform a goodness of ﬁt test across multiple clusters of time series with different models
yielding a partial correlation analogue to Corollary 4.

11

Corollary 6. Under the same conditions as Corollary 4, and denoting the estimated l-lag
partial autocorrelation of time series Xi, j,t as ˆπi, j,t, the statistic

Qtotal( ˆπ) = T (T + 2)

k
∑
i=1

ni
∑
j=1

m
∑
l=1

(T − l)−1 ˆπ 2

i, j,l

is asymptotically χ 2

ν , where ν = ∑l

i=1(nim − pi − qi).

We remark that Qtotal(ˆr) and Qtotal( ˆπ) can be used for simultaneous hypothesis testing.

5.1 ARMA(1,1) Example with an Outlier

We illustrate the use of Qgroup(ˆr) for assessing the ﬁt of a model Mi to a cluster of time
series Ci with the ARMA(1,1) example from the previous section. We do this by running the
K-ARMA(1,1) algorithm for two clusters with an additional time series that was generated
from an ARMA(1,1) process with φ = 0.2 and θ = −0.2. When we plot the ﬁtted parameters
to each time series and color them by the ﬁtted cluster, we see that added outlier is far from
any other time series (See Figure 5).

While at a glance, the ﬁtted cluster models are still able to separate the two clusters, the
Qgroup(ˆr) statistics tell us a different story about the ﬁt of the models. For 20 lags, the
red cluster has Qred
group(ˆr) = 750.0753. Under the Grouped-
Ljung-Box test, these yield p-values of pred = 0.8717 and pblue < 10−11. This indicates that
the clustering model for the blue cluster is poorly speciﬁed, likely as a result of the outlier.

group(ˆr) = 462.4082, whereas Qblue

Looking at the individual Ljung-Box tests for the blue cluster, we identify a time series with
a statistic Qoutlier = 239.61314, far above the rest of the values, and indicating a poor ﬁt for
that time series. Identifying the outlier, removing it, and reﬁtting our model on the remaining
data in the cluster yields slightly shifted parameters and test statistic Qblue(cid:48)
group = 496.7404 with
corresponding p-value pblue(cid:48) = 0.5075057 indicating a better model ﬁt.

Figure 5: Each point represents the ARMA(1,1) parameters ﬁtted to an individual time series.
The points are colored according to the clusters assigned by the K-ARMA algorithm on data
generated from 2 independent ARMA(1,1) processes with an outlier.

12

5.2 Data Drift Application

In the machine learning literature there is a general interest in studying and identifying
distributional drift (also called concept drift) [32]. Distributional drift occurs when the data
that a model is trained on is different from subsequently observed data, usually a testing set
[33]. Since the models developed using our clustering method can be used for classifying
new time series we may want to keep an eye out for distributional drift which can occur in a
few ways:

1. If a new cluster appears.

2. If new observations in a cluster’s parameters change.

3. If the proportion of time series in the different clusters changes.

As the third point doesn’t affect the clustering models, the Ljung-Box statistic is most useful
in identifying the ﬁrst two types of drift. If a new cluster appears, we would be able to
identify this much in the same way we identiﬁed the outlier in our ARMA(1,1) example
(which can be viewed as a new cluster). That is, we would ﬁnd that the time series belonging
to an unobserved cluster would have very large Ljung-Box statistics. If a clusters parameters
drift with newly observed data, this would be more difﬁcult to spot and would likely require
many observations or signiﬁcant drift before this becomes readily apparent in the Ljung-Box
statistic unless the drift is happening quickly.

6 Empty Clusters and Model Vanishing

With a K-Models approach there can be instances where we are unable to ﬁt a model to a
cluster of data. This can occur when we have more parameters than data in a cluster making
us unable to ﬁt a model to a cluster. As a subset of this, we have the special case when
no data points are assigned to a cluster of data, which is called the empty cluster problem.
In either case we are left unable to ﬁt a model for the cluster assignment step. We call
this phenomenon model vanishing. This has been observed for k-means algorithms, and
mitigation techniques have been developed including empty cluster reassignment [34, 18].

In our K-ARMA clustering algorithm, we can ﬁt an ARMA model to a single time series, so
in practice we only run into model vanishing when no time series are assigned to a cluster in
the assignment step. Empirically we have found that model vanishing occurs more frequently
when the true number of clusters is less than the number of clusters learned by the K-ARMA
algorithm. We also found that it occurs more frequently when we use random partitioning
to initialize the clusters than when we use a prototype initialization method. Of all the
factors considered in our experiments, the initialization method seemed to have the biggest
impact on the model vanishing behavior. See Table 1 for a summary of our simulations and
Appendix C for a more detailed description of the experiments.

Something we found particularly interesting is that when we selected a number of clusters
larger than the true number of underlying generative processes, the partitioning initialization
often reduced the number of non-empty clusters to the number of underlying generative pro-
cesses. For example, when we clustered 4-AR(2) models with k = 10 using the partitioning
initialization and least squares parameter estimation, we got clusters in the range 3 − 6, 40%
of which had 4 clusters and only 7 of which had 3 clusters. This seems to suggest that if we
have a known number of underlying clusters then we should go with the prototype method
of initialization and if we have an unknown number of underlying clusters then we should
choose a large starting number of clusters and use the random partitioning method to get
something close to the true number of clusters.

13

Table 1: Average number of resulting clusters when 100 K-ARMA clusters are formed for
various values of k. For Initialization, Pr = Prototype method, and Pa = Random Partitioning.
For details see Appendix C

s
s
o
Actual Model L

2-AR(2)
2-AR(2)
2-AR(2)
2-AR(2)
4-AR(2)
4-AR(2)
4-AR(2)
4-AR(2)
10-AR(2)
10-AR(2)
10-AR(2)
10-AR(2)
2-ARMA(1,1)
2-ARMA(1,1)
4-ARMA(1,1)
4-ARMA(1,1)

L1
L2
L1
L2
L1
L2
L1
L2
L1
L2
L1
L2
L2
L2
L2
L2

.
t
i
n
I

Pr
Pr
Pa
Pa
Pr
Pr
Pa
Pa
Pr
Pr
Pa
Pa
Pr
Pa
Pr
Pa

2
=
k

2.00
2.00
2.00
2.00
2.00
2.00
2.00
2.00
2.00
2.00
2.00
2.00
2.00
2.00
2.00
1.52

3
=
k

3.00
2.98
2.38
2.29
3.00
3.00
3.00
2.98
3.00
3.00
3.00
3.00
2.88
2.04
2.88
2.36

4
=
k

4.00
4.00
2.61
2.55
3.99
3.97
3.34
3.33
4.00
4.00
4.00
4.00
3.76
2.05
3.89
3.07

5
=
k

4.99
5.00
2.71
2.62
4.97
4.98
3.73
3.77
5.00
5.00
5.00
5.00
4.37
2.08
4.76
3.64

7
=
k

7.00
6.97
2.74
2.73
6.97
6.97
4.07
4.08
7.00
6.99
6.79
6.69
5.65
2.24
6.64
4.17

0
1
=
k

10.00
9.70
3.33
3.10
9.96
9.95
4.63
4.46
9.98
9.98
8.19
8.04
7.28
2.90
8.93
4.59

7 Real Data Experiments

We now show how our method can be used to cluster real world data. We do this using three
open source datasets2 compiled by Kalpakis et al. for their paper on clustering ARIMA time
series using cepstral coefﬁcients [35] and subsequently used by Xiong and Yeung in their
paper exploring ARMA mixture modeling [11]. In order to compare the ﬁtted clusters to
expected clusters we use the same cluster similarity measure as was used by Kalpakis and
Xiong which is deﬁned as

Sim(A, B) =

1
k

k
∑
i=1

max
1≤ j≤k

Sim(Ai, B j), where

Sim(Ai, B j) =

2 · |Ai ∩ B j|
|Ai| + |B j|

,

where A = {A1, . . . , Ak} and B = {B1, . . . , Bk} are two clusterings of a set of data. The
similarity measure Sim(A, B) falls in the range [0, 1], with a 1 indicating that the clusterings
A and B are identical, and lower scores indicating weaker overlap. The similarity measure
Sim(A, B) is not symmetric so we will use the ﬁrst input in Sim(A, B) to always be the
"known" or "expected" clusters and the second input to be the clusters ﬁtted by our method.
Because we are only guaranteed a local optimum by the K-ARMA algorithm, we will run it
10 times for each dataset with different random initializations and select a clustering that
achieves the best score.

7.1 Personal Income Data

The Bureau of Economic Analysis maintains data on the personal income levels of each
state in the USA [36]. Looking at the per capita income between the years 1929-1999, it has
been suggested that the east coast states, California, and Illinois form a set of "high income
growth" states and that the midwest states form a set of "low growth" states [35].

For the purposes of comparison, we took the same preprocessing approach as [35], where
the authors ﬁrst take a rolling mean of the time series with a window size of 2 to smooth the
data, followed by taking a log transform of the resulting data, and then using ARIMA(1,1,0)

2Downloaded from https://www.csee.umbc.edu/ kalpakis/TS-mining/ts-datasets.html on 10 April 2022.

14

models to cluster the processed data. This left us with Fitted Clusters 1A and 2A in Table 2,
which had a similarity score of 0.78 when compared to the expected clusters.

Additionally, because taking the rolling average removes some of the signal in the time
series, we clustered the log-transformation of the data without taking the rolling average and
using a higher order model instead. We did this using ARIMA(5,1,0) models which yielded
the Fitted Clusters 1B and 2B in Table 2 which had a similarity score of 0.90 when compared
to the expected clusters. This is the same clustering result that was achieved by Xiong and
Yeung [11], and a better clustering result than the clustering of Kalpakis et al. [35].

Table 2: The expected clusters of states for the personal income data as suggested by [35, 11]
and the ﬁtted clusters given by the K-ARMA algorithm. The states that did not end up in the
"expected" cluster are in red.

States

Expected Cluster 1 CT, DC, DE, FL, MA, ME, MD, NC, NJ, NY, PA, RI, VA, VT, WV, CA, IL
Expected Cluster 2

ID, IA, IN, KS, ND, NE, OK, SD

Fitted Cluster 1A
Fitted Cluster 2A

CT, DC, DE, FL, MA, ME, MD, NC, NJ, NY, PA, RI, VA, VT, WV, CA, IL, IN, KS, NE, OK
ID, IA, ND, SD

Fitted Cluster 1B
Fitted Cluster 2B

CT, DC, DE, FL, MA, ME, MD, NC, NJ, NY, PA, RI, VA, VT, WV, CA, IL, KS, OK
ID, IA, IN, ND, NE, SD

7.2 Temperature Dataset

The temperature dataset that we consider was compiled by the National Oceanic and At-
mospheric Administration (NOAA) and contains daily temperatures for the year 2000 at
locations in Florida, Tennessee, and Cuba. In Kalpakis et al. they group these locations into
two groups [11]. The ﬁrst, contains locations for Northern Florida and Tennessee, and the
second contained Southern Florida and Cuba.

Using the model speciﬁcations of Kalpakis et al., we ﬁt 2 clusters using ARIMA(2,1,0)
models to the temperature data which resulted in clusters. Unlike Kalpakis et al., we decided
not to preprocess the data using a moving window. The resulting two clusters are shown in
Figure 6, and had a similarity of 0.933 with the expected clusters. The two locations that did
not fall into their expected cluster were in northern Florida which is geographically on the
border between the clusters. We also decided to ﬁt 3 clusters to the temperature data, and
when we did this all of locations in Cuba ended up in their own cluster.

Figure 6: Locations of the temperature collection sites colored by expected cluster (left), by
ﬁtted cluster using two clusters (center), and by ﬁtted cluster using three different clusters
(right).

15

7.3 Population Dataset

The population of each states in the USA is well documented by the US Census Bureau
and forms a time series. Between the years 1900-1999, some states saw their population
grow rapidly, while other states saw their populations stagnate. In their paper, Kalpakis et al.
make an argument that a subset of 20 states can be reasonably separated into exponential
population growth states and states with stabilizing populations. These are listed as Expected
Cluster 1 and Expected Cluster 2 in Table 3, respectively.

For comparison, we again used the same preprocessing as Kalpakis et al. and took a similar
approach as for the personal income dataset to cluster these states based on their population
trend by ﬁrst taking a rolling mean of the time series with a window size of 2 to smooth the
data, followed by taking a log transform of the resulting data, and then using ARIMA(1,1,0)
models to cluster the processed data. This left us with Fitted Clusters 1A and 2A in Table 2,
which had a similarity score of 0.85 when compared to the expected clusters which showed
signiﬁcantly better performance than either the ARMA mixture modeling approach or the
cepstral coefﬁcient clustering approach.

Table 3: The expected clusters of states for the population data as suggested by [11, 35] and
the ﬁtted clusters given by the K-ARMA algorithm. The states that did not fall into their
"expected" cluster are in red.

States

Expected Cluster 1 CA, CO, FL, GA, MD, NC, SC, TN, TX, VA, WA
Expected Cluster 2

IL, MA, MI, NJ, NY, OK, PA, ND, SD

Fitted Cluster 1
Fitted Cluster 2

CA, CO, FL, GA, MD, NC, SC, TX, VA, WA, MI, NJ
IL, MA, NY, OK, PA, ND, SD, TN

7.4 Comparison to Other Methods

Because we use the same similarity score and the same data as that used by [35], and
presumably that used by [11], we can compare the similarity scores of the clusters that we ﬁt
using the K-ARMA method to the clusters ﬁt using a mixture modeling approach and the
cepstral coefﬁcient clustering method. This is done in Table 4. We see that the K-ARMA
method performs comparably to the other methods on the personal income dataset and the
temperature dataset and quite a bit better than the other two methods on the population
dataset.

Table 4: The similarity scores of the clusters ﬁtted using the K-ARMA method, an ARMA
mixture modeling EM algorithm, and using cepstral coefﬁcients. The highest similarity
scores are in bold.

K-ARMA ARMA Mixture Model [11]

Cepstral Coefﬁcients [35]

Personal Income
Temperature Dataset
Population Dataset

0.90
0.93
0.85

0.90
1.00
0.64

0.84
0.93
0.74

8 Discussion

We presented a new general framework for clustering time series data using a model-based
generalization of the K-Means algorithm and showed how it can be used to cluster ARMA
models. We use our method on three real world data sets and show that it is competitive with
similar clustering methods. Additionally we show the importance of initialization method on

16

empty clusters and model vanishing. There are several avenues we see for extending these
results:

• All of our experiments and algorithms were conducted using a ﬁxed order for
our ARMA models across all of the time series. It would be ideal to incorporate
mixed architecture model-based clustering where we allow different clusters to
have different order ARMA models. One difﬁculty with this, however, would be
the selection of the model order during the clustering process. One method worth
exploring for solving this may be using higher order models and a Lasso-like penalty
to zero-out some of the coefﬁcients to select the order.

• We only considered time series of the same length in our simulations and examples.
It would be good to incorporate time series of different lengths and explore weighting
based on the length of the time series. One way this can be done is by weighting the
Conditional Sums of Squares criterion by the inverse length of the time series.
• We can also extend our results for a variety of different loss functions. For ARMA
clustering we used a conditional sum of squares estimate derived from the likelihood
function, however this has been shown to be biased for short time series [37].
Additionally the conditional sums of squares estimate is not robust to outliers.
Because of these issues, we may want to consider the unconditional MLE, and more
robust ﬁtting procedures such as least absolute deviations for the ARMA ﬁtting
process.

• Although the Grouped-Ljung-Box test statistic that we developed for diagnosing
the models ﬁt to the clusters gives us an indication of the goodness of ﬁt for the
clusters, it is not on its own comprehensive. Other measures of goodness of ﬁt and
model appropriateness should be explored for analyzing the clusters produced by
our algorithm such as the Peña Rodríguez Portmanteau test [38].

• It would also be interesting to incorporate the Grouped-Ljung-Box statistic into the
clustering process, perhaps by using it to dynamically add and remove clusters for
the automatic selection of k, the number of clusters.

• We saw that the initialization method had a large impact on the resulting clusters.
Using a random partition initialization appears to do some form of regularization
with respect to the number of clusters but the mechanism that causes this behavior is
not well understood and should be explored further.

Acknowledgements

Derek Hoare and Martin Wells were partially supported by NIH grant # R01GM135926-03,
and David Matteson gratefully acknowledges ﬁnancial support from the National Science
Foundation Awards 1934985, 1940124, 1940276, and 2114143.

References

[1] Sara Venkatraman, Sumanta Basu, Andrew G Clark, Soﬁe Delbare, Myung Hee Lee,
and Martin T Wells. An empirical Bayes approach to estimating dynamic models of
co-regulated gene expression. bioRxiv, 2021.

[2] Kirsten Bulteel, Francis Tuerlinckx, Annette Brose, and Eva Ceulemans. Cluster-
ing vector autoregressive models: Capturing qualitative differences in within-person
dynamics. Frontiers in Psychology, 7, 10 2016.

[3] Minakhi Rout, Babita Majhi, Ritanjali Majhi, and Ganapati Panda. Forecasting of
currency exchange rates using an adaptive ARMA model with differential evolution
based training. Journal of King Saud University - Computer and Information Sciences,
26(1):7–18, 2014.

17

[4] Hag ElAmin and Khalid Abd El Mageed. Clustering input signals based identiﬁcation
algorithms for two-input single-output models with autoregressive moving average
noises. Complexity, 2020.

[5] Roberto Baragona. A simulation study on clustering time series with metaheuristic

methods. Quaderni di Statistica, 3:1–26, 2001.

[6] Keisuke Takano, Mina Stefanovic, Tabea Rosenkranz, and Thomas Ehring. Cluster-
ing individuals on limited features of a vector autoregressive model. Multivariate
Behavioral Research, 56(5):768–786, 05 2020.

[7] Ville Hautamaki, Pekka Nykanen, and Pasi Franti. Time-series clustering by approxi-
mate prototypes. In 2008 19th International Conference on Pattern Recognition, pages
1–4. IEEE, 2008.

[8] Saeed Aghabozorgi, Ali Seyed Shirkhorshidi, and Teh Ying Wah. Time-series clustering

– a decade review. Information Systems, 53:16–38, 2015.

[9] Elizabeth Ann Maharaj, Pierpaolo D’Urso, and Jorge Caiado. Time Series Clustering

and Classiﬁcation. Chapman and Hall/CRC, 2019.

[10] Sangeeta Rani and Geeta Sikka. Recent techniques of clustering of time series data: A
survey. International Journal of Computer Applications, 52(15):1–9, 08 2012.
[11] Yimin Xiong and Dit-Yan Yeung. Time series clustering with ARMA mixtures. Pattern

Recognition, 37(8):1675–1689, 2004.

[12] Hugo Steinhaus. Sur la division des corps matériels en parties. Bulletin de L’Académie

Polonaise de Sciences, 4(12):801–804, 1956.

[13] James MacQueen. Some methods for classiﬁcation and analysis of multivariate obser-
vations. Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and
Probability, 1(14):281–297, 1967.

[14] H. Späth. Algorithm 39 clusterwise linear regression. Computing, 22(4):367–373, 12

1979.

[15] Manuele Bicego. K-random forests: A k-means style algorithm for random forest
clustering. 2019 International Joint Conference on Neural Networks (IJCNN), 07 2019.
[16] Francesco Camastra and Alessandro Verri. A Novel Kernel Method for Clustering,

pages 245–250. Springer Netherlands, 2005.

[17] F. E. Maranzana. On the location of supply points to minimize transportation costs.

IBM Systems Journal, 2(2):129–135, 1963.

[18] Hans-Hermann Bock. Origins and extensions of the k-means algorithm in cluster anal-
ysis. Journal Electronique d’Histoire des Probabilités et de la Statistique, December
2008.

[19] Arthur P. Dempster, Nan M. Laird, and Donald B. Rubin. Maximum likelihood from
incomplete data via the EM algorithm. Journal of the Royal Statistical Society: Series
B (Methodological), 39(1):1–22, 1977.

[20] Naonori Ueda and Ryohei Nakano. Deterministic annealing EM algorithm. Neural

networks, 11(2):271–282, 1998.

[21] Iftekhar Naim and Daniel Gildea. Convergence of the EM algorithm for gaussian
mixtures with unbalanced mixing coefﬁcients. In Proceedings of the 29th International
Coference on International Conference on Machine Learning, 2012.

[22] Leon Bottou and Yoshua Bengio. Convergence properties of the k-means algorithms.

Advances in Neural Information Processing Systems, 7, 1994.

[23] Ting Su and Jennifer G Dy. In search of deterministic methods for initializing k-means
and Gaussian mixture clustering. Intelligent Data Analysis, 11(4):319–338, 2007.

18

[24] Benny Ren and Ian Barnett. Autoregressive mixture models for clustering time series.

Journal of Time Series Analysis, 2022.

[25] Geoffrey Coke and Min Tsao. Random effects mixture models for clustering electrical

load series. Journal of Time Series Analysis, 31(6):451–464, 2010.

[26] George E. P. Box, Gwilym M. Jenkins, Gregory C. Reinsel, and Greta M. Ljung. Time
Series Analysis : Forecasting and Control. John Wiley & Sons, Inc, Hoboken, New
Jersey, 2016.

[27] Abhay Jha, Shubhankar Ray, Brian Seaman, and Inderjit S Dhillon. Clustering to
forecast sparse time-series data. In 2015 IEEE 31st International Conference on Data
Engineering. IEEE, 2015.

[28] Luca Barbaglia, Ines Wilms, and Christophe Croux. Commodity dynamics: A sparse

multi-class approach. Energy Economics, 60:62–72, 2016.

[29] G. M. Ljung and G. E. P. Box. On a Measure of Lack of Fit in Time Series Models.

Biometrika, 65(2):297–303, 08 1978.

[30] G. E. P. Box and David A. Pierce. Distribution of residual autocorrelations in
autoregressive-integrated moving average time series models. Journal of the American
Statistical Association, 65(332):1509–1526, 1970.

[31] Anna Clara Monti. A proposal for a residual autocorrelation test in linear models.

Biometrika, 81(4):776–780, 1994.

[32] Jie Lu, Anjin Liu, Fan Dong, Feng Gu, Joao Gama, and Guangquan Zhang. Learning
under concept drift: A review. IEEE Transactions on Knowledge and Data Engineering,
31(12):2346–2363, 2018.

[33] João Gama, Indrundeﬁned Žliobaitundeﬁned, Albert Bifet, Mykola Pechenizkiy, and
Abdelhamid Bouchachia. A survey on concept drift adaptation. ACM Comput. Surv.,
46(4), 2014.

[34] Chun Hua, Feng Li, Chao Zhang, Jie Yang, and Wei Wu. A genetic xk-means algorithm

with empty cluster reassignment. Symmetry, 11(6), 2019.

[35] Konstantinos Kalpakis, Dhiral Gada, and Vasundhara Puttagunta. Distance measures
for effective clustering of arima time-series. Proceedings 2001 IEEE International
Conference on Data Mining, 2001.

[36] Bureau of Economic Analysis Personal Income by State. https://www.bea.gov/

data/income-saving/personal-income-by-state. Accessed: 10 April 2022.

[37] Ching-Fan Chung and Richard T Baillie. Small sample bias in conditional sum-of-
squares estimators of fractionally integrated ARMA models. Empirical Economics,
18(4):791–806, 1993.

[38] Daniel Peña and Julio Rodríguez. A powerful portmanteau test of lack of ﬁt for time
series. Journal of the American Statistical Association, 97(458):601–610, 2002.

[39] Ronald L Anderson. Distribution of the serial correlation coefﬁcient. The Annals of

Mathematical Statistics, 13(1):1–13, 1942.

[40] Theodore W. Anderson and A.M. Walker. On the asymptotic distribution of the auto-
correlations of a sample from a linear stochastic process. The Annals of Mathematical
Statistics, 35(3):1296–1303, 1964.

[41] A. I. McLeod and W. K. Li. Diagnostic checking ARMA time series models using
squared-residual autocorrelations. Journal of Time Series Analysis, 4(4):269–273,
1983.

19

A Cluster-wide ARMA(p,q) MLE Criterion

If we assume that the innovations follow a normal distribution with a constant variance σε ,
then we can use maximum likelihood estimation for ﬁtting ARMA parameters to a cluster of
time series X1,t, . . . , Xn,t.
For the ARMA(p, q) model, we have

X j,t = ε j,t +

p
∑
i=1

φiXt−i +

q
∑
k=1

θkεt−k

where

ε j,t

i.i.d.∼ N(0, σ 2

ε ).

Under this model we can write the likelihood function as

L(φ , θ , σε ) =

n
∏
j=1

T
∏
t=1

√

1
2πσε

exp

(cid:26) −(X j,t − ∑

q
p
k=1 θkεt−k)2
i=1 φiXt−i − ∑
2σ 2
ε

(cid:27)

which yields the log likelihood

(cid:96)(φ , θ , σε ) = −

nT
2

log(2π) −

nT
2

log(σ 2
ε )

−

1
2σ 2
ε

n
∑
j=1

T
∑
t=1

(X j,t −

p
∑
i=1

φiXt−i −

q
∑
k=1

θkεt−k)2.

Maximizing the log-likelihood function then amounts to ﬁnding the parameters φ and θ that
minimize the sums of squares criterion

SSE =

n
∑
j=1

T
∑
t=1

(X j,t −

p
∑
i=1

φiXt−i −

q
∑
k=1

θkεt−k)2.

We can turn this into a conditional sums of squares problem by conditioning on X1, . . . , Xp,
and assuming that the ﬁrst q innovations are 0. This leaves us with the conditional sums of
squares criterion

cSSE =

n
∑
j=1

T
∑
t=max(p,q)+1

(X j,t −

p
∑
i=1

φiX j,t−i −

q
∑
i=1

θiε j,t−i)2

which is what we minimize in Algorithm 2.

B Additional Proofs

Theorem 3. For n independent time series X1,t, . . . , Xn,t each of length T with the ﬁtted
ARMA(p, q) model ˆM, the Grouped-Ljung-Box statistic Qgroup(ˆr) computed with m lags
asymptotically follows a χ 2

(nm−p−q) distribution if the model is correctly identiﬁed.

Proof. The proof generally follows the proof of the asymptotic distribution of the Ljung-
Box Statistic in [29]. We start by considering the true white noise process and the true
correlations. If the ai,t are iid Normal with mean 0, then according to [39, 40], the limiting
distribution of the autocorrelations in the white noise process for Xi,t, which we denote
r(m)
i = (ri,1, . . . , ri,m)(cid:48), is a normal distribution with mean 0 and diagonal covariance matrix
where

Var(ri,l) =

T − l
T (T + 2)

.

20

i

j

and r(m)

are independent for i (cid:54)= j. Letting rlong = (r(m)(cid:48)

as Σ(m). Our assumptions also imply that the vectors
)(cid:48) be the stacked

Denote this covariance matrix of r(m)
, r(m)(cid:48)
r(m)
i
2
vector of the autocorrelations, we have rlong ∼ N(0nm, In ⊗ Σ(m)).
Box and Pierce showed in [30] that there exists a matrix A such that ˆr(m)
i +
Op(1/T ), where A is an idempotent matrix of rank p + q. In particular, [41] shows in detail
that the matrix A takes the form A = X(X (cid:48)X)−1X (cid:48), where X is the m × (p + q) matrix

d= (Im − A)r(m)

, . . . , r(m)(cid:48)
n

1

i

X =









0
1
φ (cid:48)
1
...

1
φ (cid:48)
1
φ (cid:48)
2
...
m−1 φ (cid:48)
φ (cid:48)

m−2

· · ·
· · ·
· · ·
. . .
· · · φ (cid:48)

0
0
0
...
m−p θ (cid:48)

1
θ (cid:48)
1
θ (cid:48)
2
...
m−1 θ (cid:48)

0
1
θ (cid:48)
1
...

m−2

0
0
0
...

· · ·
· · ·
· · ·
. . .
· · · θ (cid:48)

m−q









,

(6)

with the φ (cid:48) and θ (cid:48) are constructed from the ARMA(p,q) polynomials

φ (B) = 1 − φ1B − · · · − φpBp

and θ (B) = 1 − θ1B − · · · − θqBq

so that

1/φ (B) =

n
∑
i=1

i Bi
φ (cid:48)

and

1/θ (B) =

n
∑
i=1

i Bi.
θ (cid:48)

We will construct an idempotent matrix C of rank p + q so that
d= (Inm −C)rlong + Op(1/T )

ˆrlong

which will yield the desired asymptotic result. From Box and Pierce [30] we have that
i + X(β − ˆβ ) + Op(1/T )

1 ≤ i ≤ n,

d= r(m)

ˆr(m)
i

where β = (φ1, . . . , φp, θ1, . . . , θq)(cid:48), and ˆβ = ( ˆφ1, . . . , ˆφp, ˆθ1, . . . , ˆθq)(cid:48) is our set of ﬁtted pa-
rameters. Consequently, we have that

(7)
Consider the matrix D = 1n ⊗ X, and let C = D(D(cid:48)D)−1D(cid:48). Computing, we have that
C = 1

d= rlong + 1n ⊗ X(β − ˆβ ) + Op(1/T ).

n ) ⊗ X(X (cid:48)X)−1X (cid:48).

ˆrlong

n (1n1T

We now left multiply both sides of Equation (7) by C, performing a similar computation to
Box and Pierce [30]. When we do this, we get that

C ˆrlong

d= 0 + Op(1/T ).

Additionally we have that

C(rlong + 1n ⊗ X(β − ˆβ ) = Crlong +C1n ⊗ X(β − ˆβ )

(cid:19) (cid:16)

(cid:17)
1n ⊗ X(β − ˆβ )

(cid:17)
(cid:16)
X(X (cid:48)X)−1X (cid:48)X(β − ˆβ )

(1n1T

n ) ⊗ X(X (cid:48)X)−1X (cid:48)

= Crlong +

= Crlong +

(cid:18) 1
n
(cid:18) 1
n
= Crlong + 1n ⊗ X(β − ˆβ )
d= Crlong + ˆrlong − rlong + Op(1/T ).

(1n1T

n 1n)

⊗

(cid:19)

Together, we get

0

ˆrlong

d= Crlong + ˆrlong − rlong + Op(1/T )
d= (Inm −C)rlong + Op(1/T ).

=⇒

21

Consequently, ˆrlong is asymptotically N(0nm, (Inm − C)(In ⊗ Σ(m))). Call the covariance
matrix of the asymptotic distribution Γ = (Inm −C)(In ⊗ Σ(m)). We now observe that

Qgroup(ˆr) = T (T + 2)

n
∑
i=1

m
∑
l=1

(T − l)−1 ˆr2

i,l = ˆr(cid:48)

long(In ⊗ Σ(m))−1 ˆrlong

is a quadratic form. Denoting M = (In ⊗Σ(m))−1, a little algebra yields that ΓMΓMΓ = ΓMΓ,
a result of which is that ˆr(cid:48)
longM ˆrlong ∼ χ 2(df = tr(ΓM)). Some more algebra yields that
tr(ΓM) = rank(Inm −C) = nm − p − q. Thus, Qgroup(ˆr) is asymptotically χ 2

nm−p−q.

Corollary 4. The statistic Qtotal(ˆr) asymptotically follows a χ 2
degrees of freedom ν = ∑k

i=1(nim − pi − qi) if the models ˆMi are correctly identiﬁed.

ν distribution where the

j=1 ∑m

l=1(T − l)−1 ˆr2

Proof. Let Qi = T (T + 2) ∑ni
i, j,l be the Grouped-Ljung-Box statistic for
the time series in cluster Ci under the ARMA(pi, qi) model ˆMi. Then by Theorem 3 we have
that Qi asymptotically follows a χ 2
(nim−pi−qi) distribution. Furthermore, we have that the Qi
are all mutually independent since the time series Xi, j,t are independent and the ﬁtted model
ˆMi only depends on the time series in Ci. So

Qtotal(ˆr) = T (T + 2)

k
∑
i=1

ni
∑
j=1

m
∑
l=1

(T − l)−1 ˆr2

i, j,l =

k
∑
i=1

Qi

is asymptotically a sum of independent χ 2
ν where ν = ∑k
χ 2

i=1(nim − pi − qi).

(nim−pi−qi) distributions and is hence asymptotically

Corollary 5. For independent time series X1,t, . . . , Xn,t each of length T with the ﬁtted
ARMA(p, q) model ˆM, the statistic

Qgroup( ˆπ) = T (T + 2)

n
∑
i=1

m
∑
k=1

(T − k)−1 ˆπ 2
i,k

computed using m lags asymptotically follows a χ 2
correctly identiﬁed.

(nm−p−q) distribution if the model is

Proof. The proof of this result is very similar to the proof of Theorem 3. Let ri, ˆri, rlong,
ˆrlong, X, A,C, Σ(m) be as in the proof of Theorem 3. Let ˆπi = ( ˆπi,1, . . . , ˆπi,m)(cid:48) denote the ﬁtted
partial autocorrelations in the white noise process for the time series Xi,t. In [31], Monti
shows that ˆπ (m)
i + Op(1/T ), and, as a consequence of this, ˆπi = (Im − A)ri + Op(1/T ).
This result of Monti together with our construction in the proof of Theorem 3 implies that
ˆπlong = ( ˆπ (cid:48)

d= ˆr(m)

n)(cid:48) satisﬁes

1, . . . , ˆπ (cid:48)

i

ˆπlong

d= ˆrlong + Op(1/T )
d= (Inm −C)rlong + Op(1/T )
d= N

(cid:17)
(cid:16)
0nm, (Inm −C)(In ⊗ Σ(m))

+ Op(1/T ).

That is, ˆπlong is asymptotically N (cid:0)0nm, (Inm −C)(In ⊗ Σ(m))(cid:1). Observing that Qgroup( ˆπ) =
long(In ⊗ Σ(m))−1 ˆπlong, it follows that Qgroup( ˆπ) is asymptotically χ 2
ˆπ (cid:48)

nm−p−q.

22

Corollary 6. Under the same conditions as Corollary 4, and denoting the estimated l-lag
partial autocorrelation of time series Xi, j,t as ˆπi, j,t, the statistic

Qtotal( ˆπ) = T (T + 2)

k
∑
i=1

ni
∑
j=1

m
∑
l=1

(T − l)−1 ˆπ 2

i, j,l

is asymptotically χ 2

ν , where ν = ∑l

i=1(nim − pi − qi)

Proof. This follows from Corollary 5 and the argument we used to prove Corollary 4.

C K-ARMA Simulation Experiments

In this appendix we discuss in depth some experiments we conducted to better understand
model-vanishing, cluster identiﬁcation, and the need for random restarts. All of the time
series used in our clustering experiments were generated using the arima.sim function in R.

C.1 AR(p) Models

We start by considering clusters of AR(p) models. In this section we describe how the AR(2)
models were generated for the experiments conducted in Table 1.

C.1.1 2-AR(2) Models

We ﬁrst consider clusters of 2 − AR(2) models generated from the parameters (φ1 = 0.7, φ2 =
.25) and (φ1 = −0.3, φ2 = 0.2). We generated 25 models from each of these parameter sets
to get 2 clusters of time series. Each time series was a ﬁxed length of 100 time units.

C.1.2 4-AR(2) Models

We next consider 4−AR(2) models generated from the following parameters: (φ1 = 0.7, φ2 =
.2), (φ1 = −0.3, φ2 = .2), (φ1 = 0.4, φ2 = −.2), and (φ1 = −0.2, φ2 = −.5).

C.1.3 10-AR(2) Models

Ten clusters of AR(2) models were constructed from the parameters in Table 5. We generated
25 time series for each cluster with 1000 time points each.

Table 5: Parameters for 10 AR(2) models.

r
e
t
e
m
a
r
a
P
φ1
φ2

1

r
e
t
s
u
l
C

2

r
e
t
s
u
l
C

3

r
e
t
s
u
l
C

4

r
e
t
s
u
l
C

5

r
e
t
s
u
l
C

6

r
e
t
s
u
l
C

7

r
e
t
s
u
l
C

8

r
e
t
s
u
l
C

9

r
e
t
s
u
l
C

−0.097
−0.945

−0.215
−0.463

0.419
0.206

−0.237
0.135

0.273
0.640

0.403
−0.497

0.281
0.500

0.144
0.824

0.105
−0.550

0
1

r
e
t
s
u
l
C
0.861
−0.520

23

Figure 7: 10 correctly identiﬁed clusters of AR(2) time series. The ﬁtted parameters are in
blue.

Figure 8: Two clusterings of the time series with only 100 time points for each series. The
left clusters have Sim(A, B) = 0.9, whereas on the right the algorithm converged to a local
solution and Sim(A, B) = 0.82

If we look at fewer time points for each of these time series, the separation is less clear. This
makes it more difﬁcult to cluster the time series which can be seen in Figures 8. Figure 8
also shows the importance of cluster initialization.

C.2 ARMA(1,1) Models

We also considered ARMA(1,1) models in this paper which we also generated using
arima.sim in R.

C.2.1 2-ARMA(1,1)

For the clusters generated using 2 clusters of ARMA(1,1) models we used the parameters
(φ1 = −0.4, θ1 = 0.2) and (φ1 = −0.2, θ1 = 0.4). We generated 25 time series from each of
these parameter sets where each time series had 1000 time points.

C.2.2 4-ARMA(1,1)

For the clusters generated using 4 clusters of ARMA(1,1) models we used the parameters
(φ1 = −0.4, θ1 = 0.2), (φ1 = −0.2, θ1 = 0.4), (φ1 = 0.2, θ1 = 0.4), and (φ1 = −0.2, θ1 =
−0.4). We generated 25 time series from each of these parameter sets where each time series
had 1000 time points.

24

