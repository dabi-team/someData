8
1
0
2

b
e
F
8
2

]
T
S
.
h
t
a
m

[

1
v
9
9
2
0
1
.
2
0
8
1
:
v
i
X
r
a

Limit theory for an AR(1) model with intercept and a possible
inﬁnite variance

Qing Liu1,2 and Xiaohui Liu1,2

Abstract. In this paper, we derive the limit distribution of the least squares estimator for

an AR(1) model with a non-zero intercept and a possible inﬁnite variance. It turns out that the

estimator has a quite diﬀerent limit for the cases of

nα for some
(0, 1], and whether or not the variance of the model errors is inﬁnite

> 1, and ρ = 1 + c

< 1,

ρ
|
|

ρ
|
|

constant c

R and α

∈

∈

also has a great impact on both the convergence rate and the limit distribution of the estimator.

Key words and phrases: Limit distribution; Autoregressive model; Inﬁnite variance.

1

Introduction

As a simple but useful tool, the auto-regression (AR) models have been widely used in economics

and many other ﬁelds. Among them, the most simplest one is the autoregressive process with

order 1, i.e., an AR(1) model, which is usually deﬁned as

yt = µ + ρyt

−

1 + et,

t = 1,

, n.

· · ·

(1)

where y0 is a constant and et’s are identically and independent distributed random errors with
yt}
{
unit root if ρ = 1, iii) near unit root if ρ = 1 + c/n for some nonzero constant c, iv) explosive if

mean zero and ﬁnite variance. The process

< 1 independent of n, ii)

is i) stationary if

ρ
|
|

ρ
|
|
nonzero constant c and a sequence

> 1 independent of n, and v) moderate deviation from a unit root if ρ = 1 + c/kn for some
satisfying kn → ∞
When µ = 0 and the error variance in model (1) is ﬁnite, it is well known in the literature

and kn/n

kn}
{

0 as n

→ ∞

→

.

that the least squares estimator for ρ has a quite diﬀerent limit distribution in cases of station-

ary, unit root and near unit root; see Phillips (1987). The convergence rate of the correlation

coeﬃcient is √n, n for cases i)-iii), respectively, and may even be (1 + c)n in the case of v)

for some c > 0 as stated in Phillips and Magdalinos (2007). More studies on this model can

be found in Dickey and Fuller (1981), Dios-Palomares and Roldan (2006), Mikusheva (2007),

1School of Statistics, Jiangxi University of Finance and Economics, Nanchang, Jiangxi 330013, China

2Research Center of Applied Statistics, Jiangxi University of Finance and Economics, Nanchang, Jiangxi 330013,

China

 
 
 
 
 
 
Andrews and Guggenberger (2009, 2014), So and Shin (2007), Chan et al. (2012), Hill et al.

(2016) and references therein.

When µ

= 0 with ﬁnite variance, Wang and Yu (2015) and Fei (2018) studied the limit

theory for the AR(1) for cases of iv) and v), respectively.

It is shown that the inclusion of

a nonzero intercept may change drastically the large sample properties of the least squares

estimator compared to Phillips and Magdalinos (2007). More recently, Liu and Peng (2017)

studied how to construct a uniform conﬁdence region for (µ, ρ) regardless of i)-v) based on the

empirical likelihood method.

Observe that et may have an inﬁnite variance in practice (Phillips, 1990; Cavaliere and Taylor,

2009), and most of the aforementioned researches were focused on the case of et having a ﬁnite

variance. In this paper, we consider model (1) when µ

= 0 and the variance of et may possibly

be inﬁnite. We will derive the limit distribution of the least squares estimator of (µ, ρ) for the

following cases:

P1)

P2)

ρ
|
|

ρ
|
|

< 1 independent of n;

> 1 independent of n;

P3) ρ = 1;

P4) ρ = 1 + c

n for some constant c

= 0;

P5) ρ = 1 + c

nα for some constants c < 0 and α

P6) ρ = 1 + c

nα for some constants c > 0 and α

(0, 1);

(0, 1).

∈

∈

•

•

•

•

•

•

Since the current paper allows for the inclusion of both the intercept and a possible inﬁnite

variance,

it can be treated as an extension of the existing literature,

i.e., Phillips (1987);

Phillips and Magdalinos (2007); Huang et al. (2014); Wang and Yu (2015); Fei (2018), among

others.

We organize the rest of this paper as follows. Section 2 provides the methodology and main

limit results. Detailed proofs are put in Section 3.

2 Methodology and main results

Under model (1), by minimizing the sum of squares:

n

(yt −

t=1
X

µ

ρyt

−

−

1)2,

2

6
6
6
with respect to (µ, ρ)⊤, we get the least squares estimator for (µ, ρ)⊤ as follows

n

n

ys

ˆµ =

Ps=1

Pt=1
n
n

y2
t−1−

n

Ps=1

n

ys−1

ytyt−1

2

Pt=1

y2
t−1−(cid:18)
n

n

Pt=1

ytyt−1

ys−1

−

Ps=1
n

Pt=1
yt−1(cid:19)

n

yt

Pt=1
2

.

y2
t−1−(cid:18)

yt−1

(cid:19)

Pt=1

n

n

Pt=1
n
n

Pt=1

ˆρ =

(2)






Here A⊤ denotes the transpose of the matrix or vector A. In the sequel we will investigate the

limit distribution of (ˆµ

µ, ˆρ

−

−

ρ)⊤.

To derive the limit distribution of this least squares estimator, we follow Phillips and Magdalinos

(2007) by assuming that

C1) The innovations

et}
{

are iid with E[et] = 0;

C2) The process is initialized at y0 = Op(1).

•

•

Observing that the variance of et’s may not exist, we use the slowly varying function l(x) =

E[e2

et| ≤
t I(
|

x)] instead as did in Huang et al. (2014) to characterize the dispersion property of

the random errors, which is supposed to satisfy

C3) l(nx)/l(n)

1 as n

→

→ ∞

for any x > 0.

•

An example of slowly varying function is when l(x) has a limit, say lim
→∞

x

having a ﬁnite variance σ2. Another example is l(x) = log(x), x > 1, which implies that

et}
{
the variance of et’s does not exist. One known property of l(x) is that l(x) = o(xε) as x

→ ∞
for any ε > 0. More properties on l(x) can be found in Embrechts et al. (1997). To deal with

,

l(x) = σ2, which implies

the possibly inﬁnite variance, we introcude the following sequence

bk}∞k=0, where
{

b0 = inf

x
{

1 : l(x) > 0
}

≥

and

bj = inf

s : s

which imply directly nl(bn)

(cid:26)

≤

b2
n for all n

≥

b0 + 1,

≥

l(s)
s2 ≤

1
j

, for j = 1, 2, . . . , n,

(cid:27)

1; see also Gin´e et al. (1997).

For convenience, in the sequel we still call

< 1 the stationary case, ρ = 1 the unit root

φ
|
|
= 0 the near unit root case, ρ = 1 + c

nα for some c

= 0 the moderate

deviation case and

> 1 the explosive case, even when the variance of vt is inﬁnite. We will

case, φ = 1 + c

n for some c
ρ
|
|

divide the theoretical derivations into four separate subsections.

3

6
6
2.1 Limit theory for the stationary case

We ﬁrst consider the stationary case

yt = µ + ρyt

−

1 + et =

ρ
|
|
µ

We write ¯yt = yt −

µ

1

−

ρ , and then have

< 1, which is independent of n. Observe that

y0 −

1

+

(cid:18)

µ

−

ρ

(cid:19)

t

ρt +

ρt
−

jej.

Xj=1

ρ

1

−

To prove the main result for this case, we need the following preliminary lemma.

¯yt = ρ¯yt

−

1 + et.

Lemma 1. Suppose conditions C1)-C3) hold. Under P1), as n

1
n

n

Xt=1

yt

−

1

p
−→

1

µ

−

,

ρ

, we have

→ ∞

1
nl(bn)

and

1

1
ρ2 ,
ρ2 + µ2

σ2(1

−
1

1

−

−

if

if

lim
m
→∞
lim
m
→∞

ρ)2 ,

,

l(bm) =

∞
l(bm) = σ2,

n

Xt=1

n

y2
t
−

1

p
−→ 




1
√nl(bn)
n

et



0

0

Based on Lemma 1, we can show the following theorem.

1
√nl(bn)

t=1
P
¯yt

−→ 

t=1
P









W1

W2

1et















N

∼

−

d

1

0

1
ρ2

1

−

,





0

.









(3)

Theorem 1. Under conditions C1)-C3), as n

, we have under P1) that

→ ∞

n
l(bn) (ˆµ

q

√n(ˆρ





−

d

−→ 

µ)
−
ρ) 


ρ2)W2 if

−

X1

X2

;



lim
m
→∞


l(bm) = σ2, and X1 = W1 and

where X1 = W1 −
ρ2)W2 if
X2 = (1

−

µ(1+ρ)

σ W2 and X2 = (1

lim
m
→∞

l(bm) =

.

∞

Remark 1. Theorem 1 indicates that the possible inﬁnite variance may aﬀect both the conver-

gence rate and the limit distribution of the least squares estimator of the intercept, but has no

impact on those of ρ.

Remark 2. When lim
m
→∞
2 with σ2

12 = σ2
limit distribution reduces to the ordinary case; see Liu and Peng (2017) and references therein

l(bm) exists and is equal to σ2, we have (X1, X2)⊤ ∼
11 = 1 + µ2(1+ρ)
21 =
−

ρ2. That is, the

N (0, Σ1), where

Σ1 = (σ2

µ(1+ρ)
σ2

ρ) , σ2

and σ2

22 = 1

ij)1
≤

σ4(1

−

i,j

≤

−

for details.

4

2.2 Limit theory for the explosive case

For this case, let ˜yt =

t
i=1 ρt
−

iei + ρty0, then

P
yt = µ

ρt
ρ

1
1

−
−

t

+ ρty0 +

ρt
−

iei = µ

ρt
ρ

1
1

−
−

+ ˜yt.

Xi=1
Along the same line as Section 2.1, we derive a preliminary lemma ﬁrst as follows.

Lemma 2. Suppose conditions C1)-C3) hold. Under P2), as n

, we have

→ ∞












and

1
√l(bn)

ρ
√l(bn)

t=1
P
1
−

n

t=1
P

1
√nl(bn)

n

et

n

t=1
P
(n
ρ−

t)et

−

ρ−

tet + ρy0

d
−→

W1

U1

U2










,

















(n

ρ−

−

2)

1

l(bn)
{

}−



(ρ2

1)ρ−

2(n

1)

−

−

l(bn)
{




1
√l(bm)

m

n

t=1
P
}−

˜yt

1et

−

n

1

t=1
P
ρy0 + lim
→∞

m

˜y2
t
−

1

U1U2







d
−→ 


m
−

,





tet, and W1 are mutually

U 2
2

1

ρ−

where U1 ∼
independent random variables. W1 is speciﬁed in Lemma 1.

lim
m
→∞

t)et, U2 ∼

t=1
P

ρ−

(m

−

ρ
√l(bm)

t=1
P

Using this lemma, we can obtain the following theorem.

Theorem 2. Under conditions C1)-C3), as n

, we have

→ ∞

n
l(bn) (ˆµ
ρn(ˆρ

−

µ)
−
ρ) 


d

−→ 





q



(ρ2

1)

−

under P2).

W1

U1

U2+µρ/(ρ

,





1)

−

Remark 3. Similar to the case with ﬁnite variance, the least square estimator of the intercept

is asymptotically normal, regardless of the error distribution. While the limit distribution of ˆρ

depends on the distribution of et’s, hence no invariance principle is applicable.

Remark 4. Theorem 2 indicates that the possible inﬁnite variance only aﬀects the conver-

gence rate of ˆµ. The joint limit distribution reduces to that obtained in Wang and Yu (2015) if

l(bm) is ﬁnite.

lim
m
→∞

5

2.3 Limit theory for the unit root and near unit root cases

In these two cases, ρ =: ρn = 1 + c

n , c

Let ˜yt =

t
i=1 ρt
−

iei, then

∈

R. (c = 0 corresponds to ρ = 1, i.e., the unit root case.)

P

yt = µ + ρyt

−

1 + et = µ

We have the following Lemma.

1

t
−

ρj





Xj=0

+ ρty0 + ˜yt.





Lemma 3. 1) Let En(t) =

[ns]

ei

Pi=1
√nl(bn)

, s

∈

[0, 1]. Then

En(s)

D
−→

W (s), in D[0, 1] as n

,

→ ∞

where

W (s), s
{

0
}

≥

the weak convergence. Moreover, deﬁne Jc(s) = lim
c
→

f

a

−
−

] is the ﬂoor function, and D
is a standard Brownian process, [
−→
·
eas
, we have under P3)
a , then as n

denotes

f

1

→ ∞

and P4) that

and in turn






n

2

n−

3

n−

n

t=2  
P
n

t=2  
P

n

2

t
−

j=0
P
2
t
−

ρj

ρj

j=0
P
t
−

2

1
0 Jc(s) ds,

! →

2

R

1
0 J 2

c (s) ds,

→

!

R

3/2

n−

n

˜y2
t

k=1
P
n2l(bn)
n

t=2  
P

j=0
P

ρj

!

et
√l(bn)

d
−→

1
0 Jc(s) d

W (s).

R

f

d
−→

1

0
Z

e−

2c(1

−

s)

W 2(Bc(s)) ds,

˜yt

k=1
P

n3/2

l(bn)

d
−→

1

0
Z

f

e−

c(1

−

s)

W (Bc(s)) ds,

˜yt

1et

k=1
P

−
nl(bn)
2cs

p

c

d

−→ −

Z
2c).

−

1)/(

−

f

1

0

e−

2c(1

−

s)

W 2(Bc(s)) ds +

f

W 2(Bc(1))
2

1
2

,

−

f

where Bc(s) = e2c(e−

This lemma can be showed easily by using similar techniques as in Chan and Wei (1987)

based on the fact that

1

n3/2

l(bn)

Using this lemma, it is easy to check the following theorem.

p

ρj



e(2)
t

p
−→

0, n

.

→ ∞

n

t=2
X

2

t
−

Xj=0







6

Theorem 3. Under the conditions C1)-C3), as n

, we have

→ ∞

n
l(bn) (ˆµ

n3
l(bn) (ˆρ

µ)

ρ)

−

−







q

q

under P3) and P4), where

d

−→ 



;

Y1/d
Y2/(µd) 








1

d =

1

0

Z

J 2
c (s) ds

Jc(s) ds

−

0
(cid:18)Z

2

,

(cid:19)

Y1 =

W (1)

1

0

Z

J 2
c (s) ds

−

0
Z

1

1

Jc(s) ds

Jc(s) d

W (s),

0

Z

Y2 =

f
1

0

Z

Jc(s) d

W (s)

−

1

W (1)

Jc(s) ds.

0

Z

f

2.4 Limit theory for the moderate deviation cases

f

f

As stated in Phillips and Magdalinos (2007), the moderate deviation cases bridge the diﬀerent

convergence rates of cases P1)-P4). That is, the case P5) bridges the stationary case and the

near unit root case, while Case P6) bridges the explosive case and the near unit root case. And

the derivation of these two cases need to be handled diﬀerently, because for the c > 0 case

central limit theorem for martingales fails to hold. Following Phillips and Magdalinos (2007),

we consider them separately.

The following lemma is useful in deriving the limit distribution of the least square estimator

under cases P5)-P6).

Lemma 4. Suppose conditions C1)-C3) hold.

i) Under P5), as n

, we have

→ ∞

•















1
√n1+α

where Σ2 = diag(1,

−

n

1
√n

1
√nα

1
√nα
n

et
√l(bn)
ρt−1et
√l(bn)
ρn−tet
√l(bn)
t
−

1

t=1
n
P
t=1
n
P
t=1
P
et
√l(bn)

t=2
P
1
2c ,

1
2c ,

−

−

d
−→

N (0, Σ2),

∼

V11



V12



V13

V14

















ρt−1−jej
√l(bn)

j=1
P
1
2c ), which implies that V1i’s are independent;















7

ii) Under P6), as n

, we have

→ ∞

•

1
√n

1
√nα

1
√nα

n

t=1
n
P
t=1
n
P
t=1
P










et
√l(bn)
ρ−tet
√l(bn)
ρt−1−net
√l(bn)

V21










,

V22

V23






d
−→










and

1
ρnnα

where (V21, V22, V23)⊤ ∼
independent.

n

1

t
−

t=2  
X

Xi=1

ρt

1

iei
−
−
l(bn) !

et
l(bn)

d
−→

V22V23,

N (0, Σ3) with Σ3 = diag(1, 1
p

p

2c , 1

2c ), which implies that V2i’s are

Theorem 4. Suppose conditions C1)-C3) hold.

1) Under P5), we have as n

→ ∞

where





an(ˆµ
−
annα(ˆρ

µ)
ρ) 


−

⊤

Z;

d

−→ 



µ
cd

1
d





an =

Z =

d =

if

lim
m
→∞

l(bm) =

, and

∞

nα/l(bn)I(α > 1/2) + √n1
−

αI(α

1/2),

≤

µ
p
c

V12I(α > 1/2) + V14I(α

1/2),

≤

µ2
2c3 I(α > 1/2) +

−

1
2c

−

I(α

≤

1/2),

α/2,

Z =

an = nmax(α,1/2)
−
µσ
c
µ2
2c3 I(α

V12I(α

d =

≥

≥

−

1/2) + σ2V14I(α
σ2
2c

I(α

1/2) +

≤

1/2),

≤

1/2),

−

l(bm) = σ2.

if

lim
m
→∞

2) Under P6), we have as n

→ ∞

µ)

n
l(bn) (ˆµ
−
q
n3α
l(bn) ρn(ˆρ

−






q

ρ)

8

d

−→ 








V21

2c2
µ V23

.





Remark 5. Similar to Fei (2018), the least squares estimators under P6) are asymptotically

normal, in contrast to the Cauchy distribution in Phillips and Magdalinos (2007). Moreover,

the joint limit distribution is still degenerated under P5), but slightly diﬀerently, we obtain the

exact limit distribution in this case.

Remark 6. As can be seen from Theorem 4, under P5), the possible inﬁnite variance has no

impact on the asymptotic behavior of estimators when α < 1
2 , and the limit distribution when α = 1
2 .

when α > 1

2 , but aﬀects the convergence rate

Remark 7. Under some mild conditions, it is possible to extend the current result under P6)

into to the cases that ρ = 1+ c
kn

by using similar arguments for some general sequence

kn}
{

such

that kn = o(n) and kn/n

0 as n

→

→ ∞

as studied in Phillips and Magdalinos (2007). But it

is impossible to do such an extension under P5) because the derivation of the limit distribution

involving the order comparison between √n and kn, while the limit of √n/kn as n

is

→ ∞

unclear without any further information of kn.

3 Detailed proofs of the main results

In this section, we provide all detailed proofs of the lemmas and theorems stated in Section 2.

Proof of Lemma 1. To handle the possible inﬁnite invariance, we use the truncated random

variables. Let

e(1)
et| ≤
t = etI(
|

bn)

et| ≤
E[etI(
|

−

bn)],

e(2)
et|
t = etI(
|

> bn)

et|
E[etI(
|

−

> bn)],




(4)

) denotes the indicative function. The key step is to show that the diﬀerence of replacing
where I(
·
et by e(1)



t

in the summations is negligible.
¯y(1)
t }
{

¯y(2)
t }
{

and

Let

be two time series satisfying

t = ρ¯y(k)
¯y(k)

1 + e(k)

t

t
−

,

k = 1, 2.

Obviously,

e(1)
t /
{

l(bn) : t

1
}

≥

are iid, and under P1), it is easy to check that

is a martingale diﬀerences sequence with respect to

p

Ft
−

¯y(1)
t
{
−
es : s
1 = σ(
{

≤

1e(1)
t

−

t /l(bn) :

) for
1
}

, n, which satisfy the Lindeberg condition. Hence, by the Cram´er-Wold device and

· · ·

9

t

≥

1
}
t = 1, 2,

the central limit theorem for martingale diﬀerence sequences, we have

1
√nl(bn)
n

1
√nl(bn)

n

e(1)
t

t=1
P
¯y(1)
t
−

1e(1)

t













d

W1

−→ 



W2

.

(5)

t=1
P
Next, under condition C3), it follows from Cs¨org˝o et al. (2003) that





et|
E[
|

et|
I(
|

> bn)] = o(l(bn)b−

n ), n

1

.

→ ∞

Then by nl(bn)

≤

b2
n and the Markov inequality, we have, for any ε > 0,

e(2)
t

P

n

 (cid:12)
t=1
(cid:12)
X
(cid:12)
(cid:12)
(cid:12)

≥

p

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

nl(bn)ε

! ≤

≤

n

E

t=1
P

e(2)
t
|
|
nl(bn)ε
n
p
t=1
P

2

> bn)]

et|
I(
et|
E[
|
|
nl(bn)ε

= o

nl(bn)
p
bn !

  p

= o(1), as n

,

→ ∞

That is,

1
nl(bn)

p
Furthermore, note that ¯y(k)
t
−

1 = ρt
−

we have

P

t=1
X
1 ¯y(k)

0 +

n

e(2)
t = op(1), n

,

→ ∞

(6)

t
1
i=1 ρt
−
−

1

−

ie(k)
i

, k = 1, 2. By the H¨older inequality,

Using the Markov inequality, we have

e(1)
t
! ≤ 
l(bn) (cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)



E

e(1)
t
l(bn) !

p

1.

≤

1/2

2






E

 (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

p

n

1e(2)

t

¯y(1)
t
−

√nl(bn)ε

!

≥

(cid:12)
(cid:12)
(cid:12)
(cid:12)
¯y(1)
E(
(cid:12)
1|
t
|
−

e(2)
)E(
t
|

)
|

P

≤

≤

 (cid:12)
t=1
(cid:12)
X
(cid:12)
1
(cid:12)
(cid:12)
√nl(bn)ε
1
√nl(bn)ε

n

t=1
X
1

1

−

¯y(1)
E(
0 |
|

e(1)
) + nE(
t
|

ρ {

e(2)
E(
)
t
|
}
|

)
|

= o(n−

i.e.,

1/2b−

n ) + o

1

nl(bn)
bn !

  p

= o(1),

1
√nl(bn)

n

t=1
X

1e(2)

t = op(1),

¯y(1)
t
−

as n

.

→ ∞

(7)

10

 
Similarly we can show

n

1
√nl(bn)

1e(j)

¯y(2)
t
−

t = op(1), n

, j = 1, 2.

→ ∞

(8)

t=1
X
This, together with (6)-(7), shows

1
√nl(bn)
n

1
√nl(bn)

n

et

t=1
P
¯yt

−

t=1
P









= 

1et









while combined with (5) shows (3).

1
√nl(bn)
n

1
√nl(bn)

n

e(1)
t

t=1
P
¯y(1)
t
−

1e(1)

t

t=1
P



+ op(1),

as n

,

→ ∞





Note that ¯yt = ρ¯yt

−

summation, we have

1 + et. Multiplying both sides with ¯yt and ¯yt

−

1 respectively, and taking

Since

and

we have

¯y2
t = ρ

n

t=1
P

¯yt ¯yt

−

1 = ρ

n

1 +

¯ytet,

¯yt ¯yt

−

t=1
P
1 +

n

t=1
P

1et.

¯yt

−

¯y2
t
−

n

t=1
P

n

t=1
P
n

t=1
P






1
√nl(bn)

1et

¯yt

−

d
−→

W2,

n

t=1
X

n

n

Xt=1

¯ytet = ρ

1et +

¯yt

−

Xt=1

n

e2
t ,

Xt=1

n
t=1 ¯ytet
nl(bn)

P

p
−→

1, n

,

→ ∞

1 (see (3.4) in Gin´e et al. (1997)). Hence,

by noting that

n
t=1 e2
t
nl(bn)
P

p
−→

n

¯y2
t
Pt=1
nl(bn) = ρ

n

¯yt ¯yt−1
nl(bn) + 1 + op(1)

Pt=1

n

n

¯yt ¯yt−1
nl(bn) = ρ

Pt=1

¯y2
t−1

Pt=1
nl(bn) + op(1),






→ ∞

which implies that as n






n

¯y2
t

Pt=1
nl(bn)

n

p
−→

1
ρ2

1

−

¯yt ¯yt−1

Pt=1

nl(bn)

ρ
ρ2 .

1

−

p
−→

11

Note that

and

1
nl(bn)

1
n

n

Xt=1

1 =

yt

−

1
n

n

Xt=1

1 +

¯yt

−

1

µ

−

=

1

ρ

µ

−

ρ

+ op(1),

y2
t
−

1 =

1
nl(bn)

n

t=1
X

n

t=1
X

¯y2
t
−

1 +

1

1
nl(bn)

ρ

2µ

−

n

t=1
X

1 +

¯yt

−

1
l(bn)

µ2

ρ)2 .

(1

−

Using these, the rest proof of this lemma follows directly by the law of large numbers.

Proof of Theorem 1. For the least squares estimator, it is easy to check that

=

ˆµ



ˆρ



−

−

µ
ρ


For convenience, hereafter write












etyt−1

2

n

Pt=1

y2
t−1

n

n

n

n

es

−

ys−1

n

Ps=1

Ps=1
n
y2
t−1−(cid:18)
Pt=1
n

Pt=1
n

Pt=1
yt−1(cid:19)

n

yt−1et

yt−1

Pt=1
n
n

Pt=1

−

Pt=1
n

y2
t−1−(cid:18)

yt−1

(cid:19)

Pt=1

n

es

Ps=1
2

.












n

n

n

n

∆1 =

1

y2
t
−

Xt=1
n

∆2 = n

∆3 = n

t=1
X
n

t=1
X

es −

Xs=1
1et −

yt

−

n

Xs=1
yt

1

−

ys

1

−

n

1et,

yt

−

Xt=1
es,

s=1
X
2

y2
t
−

1 −  

yt

−

1

!

.

t=1
X
n

t=1
X

Observe that

n

n

n

n

∆1 =

y2
t
−

Xt=1
n

1

Xs=1

es −

ys

1

−

Xt=1

1et

yt

−

n

n

n

=

y2
t
−

1 −

1

t=1
X

yt

−

1

!

s=1
X

es −

s=1
X

ys

1

−

t=1
X

1et.

¯yt

−

Xs=1
n

ρ

t=1
X

µ

−

,

→ ∞
1

ρ2 W1 −

1

−

Hence, by Lemma 1 we have, as n

1
(nl(bn))3/2

∆1

d
−→

Next, relying on

µ
σ2(1

−

W2I

ρ)

lim
m
→∞

(cid:16)

l(bm) = σ2

.

(cid:17)

∆2 = n

= n

n

t=1
X
n

t=1
X

yt

−

1et −

¯yt

−

1et −

n

n

yt

−

1

¯yt

−

1

es

es

s=1
X
n

s=1
X

t=1
X
n

t=1
X

12

 
we obtain

= n

n

t=1
X

1et

¯yt

−

,
1 + op(1)
}

! {

1
n3/2l(bn)

∆2

d
−→

W2.

Following a similar fashion, we have

1
n2l(bn)

∆3 =

1
nl(bn)

n

Xt=1

y2
t
−

1 −

1
l(bn)  

1
n

2

yt

−

1

!

n

Xt=1

p
−→

1

1

−

ρ2 .

Then this theorem follows immediately by using Slutsky’s theorem.

Proof of Lemma 2. For the ﬁrst part, by following a similar fashion to Lemma 1, we can show

that



1
√nl(bn)

n

et

n

t=1
P
(n
ρ−

1
√l(bn)










t=1
P
1
−

n

t=1
P





t)et

−

=



















1
√nl(bn)

n

e(1)
t

n

t=1
P
(n
ρ−

t)e(1)
t

−

1
√l(bn)

t=1
P
1
−

n

t=1
P

ρ
√l(bn)

ρ−

tet + ρy0

ρ
√l(bn)

ρ−

te(1)

t + ρy0

+ op(1).












The rest proof is similar to Anderson (1959) and Wang and Yu (2015). We omit the details.

For the second part, we only prove the case of

k = 1, 2, t = 1, 2,

· · ·

, n. Similar to Lemma 1, it is easy to verify that

lim
m
→∞

l(bm) =

∞

. Let ˜y(k)

t =

ρt

−

ie(k)

i + ρty0,

t

i=1
P

n

ρ−

1

l(bn)
−
}
{

2n

ρ−

1

l(bn)
−
}
{

n

E

t=1
X
n

E

˜y(k)
t
|
−

1e(j)

t

p
−→

|

0,

˜y(k)
t
|
−

1 ˜y(j)
1|

t
−

p
−→

0,

t=1
X
, and in turn we can obtain that

for (k, j)

(1, 2), (2, 1), (2, 2)

∈ {

, as n
}

→ ∞

(n

ρ−

−

2)

1

l(bn)
{

}−

(ρ2

−

1)ρ−

2(n

1)

−

l(bn)
{







n

t=1
P
}−

˜yt

1et

−

n

1

t=1
P

(n

ρ−

−

2)

1

l(bn)
{

}−



= 

˜y2
t
−

1





(ρ2





−

1)ρ−

2(n

1)

−

l(bn)
{

1e(1)

t

˜y(1)
t
−
n

1

n

t=1
P
}−

t=1
P

(˜y(1)
t
−

1)2



+ op(1).





Then the conclusion follows.

Proof of Theorem 2. Using the same arguments as Wang and Yu (2015), it follows from Lemma

13

 
2 that

Then as n

→ ∞






, we have

(n

ρ−

−

1)

l(bn)
{

}−

1/2yn

d
−→

U2 + µρ
1 ,
−

ρ

n

1

t=1
P
l(bn)
{

}−

(n

ρ−

−

2)

l(bn)
{

}−

1et

yt

−

d
−→

U1(U2 + µρ
−

ρ

1 ),

1)ρ−

(n

1)

−

(ρ

−

(ρ2

−

1)ρ−

2(n

1)

−

1

l(bn)
{

}−

n

1/2

t=1
P
n

t=1
P

yt

−

1

d
−→

U2 + µρ
1 ,
−

ρ

y2
t
−

1

d
−→

(U2 + µρ
−

ρ

1 )2.

(cid:18)

n

t=1
X

µρ

.

n

n1/2ρ2n

1
3/2
l(bn)
}
{

∆1 = ρ−

2n

1

l(bn)
−
}
{

n

t=1
X

y2
t
−

1 ×

1
nl(bn)

et + op(1)

n

t=1
X

d
−→

1
ρ2(ρ2

1)

−

W1

U2 +

p

µρ

2

ρ

1

(cid:19)

−

1
nρnl(bn)

∆2 = ρ−

n

1

l(bn)
−
}
{

1et + op(1)

yt

−

d
−→

1
ρ2 U1

U2 +

(cid:18)

and

ρ

1

(cid:19)

−

1
nρ2nl(bn)

∆3 = ρ−

2n

1

l(bn)
−
}
{

y2
t
−

1 + op(1)

t=1
X

Then the theorem has been proved.

d
−→

1
ρ2(ρ2

U2 +

(cid:18)

1)

−

µρ

2

ρ

1

(cid:19)

−

.

Proof of Theorem 3. Similar to the proof of Theorem 1, by Lemma 3, we have, as n

,

→ ∞

n

n

n

n

∆1 =

y2
t
−

1

n

t=1
X

= µ2

which implies

Xt=2










Xj=0

et −

2

ρj

t=1
X
t
−

1et

yt

−

yt

−

1

n

t=1
X
2

t=1
X





Xs=1

es − 


n

2

s

−

ρj

Xs=2

Xj=0

1
n7l(bn)

∆1

d
−→

µ2

p

1

0

Z

W (1)

(cid:18)

f

J 2
c (s) ds

14

n

2

t
−





Xt=2

Xj=0





1

,
1 + op(1)
}
{

ρjet







1

−

0

Z

Jc(s) ds

Jc(s) d

W (s)

,

0

Z

(cid:19)

f

and

which leads

∆2 = n

= µ

n

t=1
X

n






n

n

yt

1

et

yt

−

1et −

−

t=1
X
2
ρjet


t=1
X

− 



n

t
−





Xt=2

Xj=0

n

2

t
−

ρj

Xt=2

Xj=0

1
n5l(bn)

∆2

d
−→

µ

and

p

1

0
(cid:18)Z

Jc(s) d

W (s)

−

W (1)

Jc(s) ds

,

0

Z

(cid:19)

f

f

2

n

∆3 = n

n

t=1
X

y2
t
−

1 −  

yt

−

t=1
X

2

= µ2n

n

2

t
−

ρj



−







Xj=0

1

!

µ2

n

2

t
−





Xt=2

Xj=0

+ op(n4),

2

ρj





which results in

Xt=2

,
1 + op(1)
}
{

n





Xt=1

et



1

1
n4 ∆3 →

µ2

1

J 2
c (s) ds

1

Jc(s) ds

2

!

(cid:19)

−

0
(cid:18)Z

 Z

0

, n

.

→ ∞

Then the theorem has been proved.

Proof of Lemma 4. i) Similar to Lemma 1, under P5), by the Markov inequality and the fact

nl(bn)

≤

b2
n, we have for any ε > 0

P

n

t=1
X

 (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ρt
−

1e(2)
t

≥

p

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

nαl(bn)ε

! ≤

≤

P

2

|

n
t=1 ρt
p
−

e(2)
n
1E
t=1 ρt
−
t
|
nαl(bn)ε
1E[
et|
et|
I(
|
|
nαl(bn)ε
ρn
ρ

1
p
−
1
−

l(bn)
bn (cid:19)
nαl(bn)
bn

p

! →

P

(cid:18)

  p

= o

= o

> bn)]

1
nαl(bn)ε

0, as n

.

→ ∞

This implies that

Similarly we can show that

1
√nα

n

Xt=1

ρt

1e(2)
−
t
l(bn)

p
−→

0 as n

.

→ ∞

p

1
√n

n

t=1
X

e(2)
t
l(bn)

p
−→

0 and

p

1
√nα

15

n

ρn

te(2)
−
t
l(bn)

p
−→

0, n

.

→ ∞

t=1
X

p

Next, if (i, j)

, it follows from Lemma A.2 of Huang et al. (2014) that

∈ {

(1, 2), (2, 1), (2, 2)
}
t
−

n

1
√n1+α

t=2 (
X

Xk=1

1

ρt

−

1

ke(i)
−
k
l(bn) )

e(j)
t
l(bn)

p
−→

0, n

.

→ ∞

We actually obtain

p

p

n

1
√n

1
√nα

1
√nα
n

et
√l(bn)
ρt−1et
√l(bn)
ρn−tet
√l(bn)
t
−

1

t=1
n
P
t=1
n
P
t=1
P
et
√l(bn)

j=1
P

t=2
P



















=

























n

1
√n

1
√nα

1
√nα
n

t=1
n
P
t=1
n
P
t=1
e(1)
P
t
√l(bn)

e(1)
t
√l(bn)
ρt−1e(1)
t
√l(bn)
ρn−te(1)
t
√l(bn)
1
t
−

t=2
P

j=1
P















1
√n1+α

ρt−1−j ej
√l(bn)

1
√n1+α

ρt−1−j e(1)
j
√l(bn)

+ op(1).

(9)

Then, based on the Cram´er-Wold device and central limit theorem for martingales diﬀerences

sequence, the Lindeberg condition for the ﬁrst part of the right side of (9) can be proved by

using the same arguments as Phillips and Magdalinos (2007) and Huang et al. (2014). We omit

the details.

ii) The proof of the case under P6) is similar to that of i) and Phillips and Magdalinos (2007),

thus is omitted.

Proof of Theorem 4. i) Under P5), observe that ρn = o(n−

α), y0 = Op(1), and

yt = µ + ρyt

−

1 + et

=

µ

−

1

+

ρ

µ
c

(cid:16)

nα + y0

ρt +

ρt
−

iei,

(cid:17)

Xi=1

t

which implies

yn =

Note that yt −

yt

−

1 = µ + (ρ

1)yt

−

−

µ

n

+

ρn

iei

−

1

ρ

−

Xi=1
1 + et and

.
1 + op(1)
}

! {

y2
t −

y2
t
−

1 = µ2 + (ρ2

1)y2
t
−

−

it is easy to verify that

1 + e2

t + 2µρyt

−

1 + 2µet + 2ρyt

−

1et,

n

t=1
X
n

t=1
X

1 =

yt

−

y2
t
−

1 =

1

−
1

−

1

1

nµ

−

ρ

(cid:16)

ρ2

nµ2
(cid:16)

−

n

yn +

et

t=1
X
n + y2
y2

0 +

,
1 + op(1)
}
{
(cid:17)

n

e2
t + 2µρ

t=1
X

16

n

t=1
X

n

n

1 + 2µ

yt

−

et + 2ρ

1et

yt

−

(cid:17)

t=1
X

t=1
X

 
and

Hence,

∆1 =

=

=

=

=

=

1

ρ2

1

−

nµ2
(cid:16)

−

n

y2
n +

e2
t + 2µρ

t=1
X

n

t=1
X

yt

−

1

,
1 + op(1)
}
{
(cid:17)

n

t=1
X

1et =

yt

−

n

(

t=1  
X

µ

−

1

+

ρ

1

t
−

Xi=1

ρt
−

1

−

iei

et

!

.
1 + op(1)
}

) {

n

n

n

n

y2
t
−

1 ·

et −

Xt=1

1

(

1

−

Xt=1
nµ2
(cid:16)

Xt=1
y2
n +

1 ·

yt

−

n

yt

−

1et

n

Xt=1
e2
t + 2µρ

t=1
X
n

t=1
X

n

n

n

n

1 + 2µ

yt

−

et + 2ρ

1et

yt

−

et

·

(cid:17)

t=1
X

t=1
X

t=1
X

ρ2

1

−

−

−

nµ

yn +

et

−

1

ρ

t=1
(cid:17)
X
n
t=1 e2
t

P

n

µ

ρ

−

Xt=1
n
1et −

(cid:16)
nµ2

−

y2
n +
2

1

−

1

ρ

n(cid:16)
µ

n

ρ

−

1

h
nµ

Xt=1
µ

et −

1

n

1

1

ρ

n

−

−

ρ

t=1
X

n1+5α/2

(

l(bn)

p

ρt
−

µ2
c2

n3(1+α)/2l(bn)

µ
c 

1
√n1+α

1et

yt

−

1 + op(1)
}

) {

n

n

n

·

t=1
X

+ µρ

Xt=1
n

yt

1 + µ

et

−

1

t
−

Xt=1
ρt
−

et

ρt
−

1et +

·

(cid:17)

Xt=1

et −

(cid:16)

nµ

yn

−

×

(cid:17)

1

−

jej

(cid:17)io

1 + op(1)
}
{

Xt=1 (cid:16)
1
ρt
−

−

1

t
−

et

Xj=0
jej

t=1 (cid:16)
X
n

Xj=0
1

ρt
−

1
√nα

et
l(bn) !

+

1 + op(1)
}
{
(cid:17)o

Xt=1
n

t=1
X

p

p

et
l(bn)

1

t
−

Xj=0

ρt
−

1

−

j

ej
l(bn) 


p






,
1 + op(1)
}
{

and

∆2 =

=



n

Xt=1

yt

−

1 ·

n

et + n

1et

yt

−

n

n

Xt=1
et

yn +

−

nµ

(cid:16)

n

·

t=1
X
n

(cid:17)

ρt
−

1et +

et

et + n

t=1
X
1
t
−

ρt
−

1

−

jej

µ

−

ρ

1

h

n

et

t=1
X

n

−

Xt=1
1

−

1

−

1

ρ

ρ

−
µ

−

t=1
X

=

n1+3α/2

(

l(bn)

·

p

Xj=0
n

t=1 (cid:16)
X
1
√nα

µ
c ·  

+ n3/2+α/2l(bn)

1
√n1+α





n

t=1
X

(cid:17)i
et
l(bn) !

ρt
−

1

−

j

1

t
p
−

Xj=0

,
1 + op(1)
}
{

ej
l(bn) 







p

1

ρt
−

t=1
X

et
l(bn)

p

17

 
and

∆3 = n

+

nµ2 +
(cid:16)

n

n

e2
t

Xt=1

(cid:17)

y2
n −

−

2µρ
ρ
1

−

yn +

4ρµ
ρ
1

−

n

et

Xt=1

i

n

yt

−

2

1

(cid:17)

t=1
X
2nµ2ρ
ρ
1

−

n

t=1
X
n

y2
t
−

1 −

(cid:16)

1

−

ρ

1

−

1

−

n

ρ

n

1
1 + ρ

h

nµ2

−

h

1
1 + ρ

=

=

=

2µyn + 2µ

n

t=1
X

e2
t −

y2
n −

+

et

yn

1 + op(1)
}
{
io
2µρ
1
ρ
+ n1+3α µ2
2c3
−

2µyn
ρ
1

) {

−

−

i

1

(

ρ

−
n
n2+αl(bn)

h

2c  

−

t=1
X
n
1
n

t=1
X

e2
t
l(bn) !

1 + op(1)
}
{

o

.
1 + op(1)
}

These, together with Lemma 4, lead directly to i).

ii)Under P6), it follows from Lemma 4 that

yn =

µ
c

nαρn + nα/2ρn

l(bn)V22{

, n
1 + op(1)
}

,

→ ∞

p

y2
n =

µ2
c2 n2αρ2n + 2

µ
c

n3α/2ρ2n

l(bn)V22{

1 + op(1)
}

p

which implies that

and

n

t=1
X

1 =

yt

−

=

1
c

1
c

nαyn −
µ
c2 n2αρn +

nαy0 −
1
c

n3α/2ρn

µ
c

nα+1

1
c

−

n

nα

et

t=1
X
.
1 + op(1)
}

l(bn)V22{

Again by Lemma 4, we can show that

p

n

Xt=1

1et =

yt

−

=

=

n

t=1
X

and

n

µ
c

n

Xt=1  −
nα

µ
c

−
µ
c

Xt=1
n3α/2ρn

nα +

µ
c

nαρt
−

1 + y0ρt
−

1 +

1

t
−

Xi=1
n

ρt
−

1

−

iei

et

!
n

n

et +

µ
c

nα

ρt
−

1et + y0

ρt
−

1et +

Xt=1
,
1 + op(1)
}

l(bn)V23{

Xt=1

1

t
−

Xt=1  

Xi=1

ρt
−

1

−

iei

et

!

p

y2
t
−

1 =

µ2
2c3 n3αρ2n +

µ
c2 n5α/2ρ2n

l(bn)V22{

.
1 + op(1)
}

p

18

Then as n

, we have

→ ∞

∆1 =

∆2 =

∆3 =

l(bn)V21{

µ2
2c3 n3α+1/2ρ2n
µ
p
n3α/2+1ρn
l(bn)V23{
c
µ2
p
2c3 n3α+1ρ2n
.
1 + op(1)
}
{

,
1 + op(1)
}

,
1 + op(1)
}

Therefore, the result holds.

4 Concluding remarks

In this paper, we investigated the limit distribution of the least squares estimator of (µ, ρ) for the

ﬁrst-order autoregression model whit µ

= 0. The discussions were took under the assumption

that the error variance may be inﬁnite. The existing results fail to hold under this assumption.

Our results show that the possible inﬁnite variance aﬀects the convergence rate of the estimator

of the intercept in all cases, but only in some cases for the correlation coeﬃcient; see Sections 3.3

and 3.4 for details. Based on the current results, one could build some testing procedures, e.g.,

t-statistics. However, their limit distributions may be quite complex because the least squares

estimator has a diﬀerent limit distribution in diﬀerent cases, and even is degenerated in the

moderate deviations from a unit root cases. Hence, it is interesting to construct some uniform

statistical inferential procedures, e.g., conﬁdence region for (µ, ρ)⊤, which are robust to all cases

above. Nevertheless, this topic is beyond the scope of the current paper, and will be pursued in

the future.

Acknowledgements

Xiaohui Liu’s research was supported by NSF of China (Grant No.11601197, 11461029), China

Postdoctoral Science Foundation funded project (2016M600511, 2017T100475), the Postdoctoral

Research Project of Jiangxi (2017KY10), NSF of Jiangxi Province (No.20171ACB21030).

References

Andrews, D.W.K. and Guggenberger, P. (2009). Hybrid and size-corrected subsampling meth-

ods. Econometrica 77, 721–762.

19

6
Andrews, D.W.K. and Guggenberger, P. (2014). A conditional-heteroskedasticity-robust conﬁ-

dence interval for the autoregressive parameter. The Review of Economics and Statistics 96,

376–381.

Anderson, T.W. (1959). On asymptotic distributions of estimates of parameters of stochastic

diﬀerence equations. The Annals of Mathematical Statistics 30, 676–687.

Chan, N.H., Li, D. and Peng, L. (2012). Toward a uniﬁed interval estimation of autoregressions.

Econometric Theory 28, 705–717.

Cavaliere, G. and Taylor, A.M.R. (2009). Heteroskedastic time series with a unit root. Econo-

metric Theory 25: 1228–1276.

Chan, N.H. and Wei, C.Z. (1987). Asymptotic inference for nearly nonstationary AR(1) pro-

cesses. The Annals of Statistics 15, 1050–1063.

Cs¨org˝o, M., Szyszkowicz, B., and Wang, Q.Y., (2003). Donskers theorem for self-normalized

partial sums processes. The Annals of Probability 31, 1228–1240.

Dickey, D.A. and Fuller, W.A. (1981). Likelihood ratio statistics for autoregressive time series

with a unit root. Econometrica 49, 1057–1072.

Dios-Palomares, R. and Roldan, J.A. (2006). A strategy for testing the unit root in AR(1) model

with intercept: a Monte Carlo experiment. Journal of Statistical Planning and Inference 136,

2685–2705.

Embrechts, P., Kl¨uppelberg, K., and Mikosch, T., (1997). Modelling Extremal Events:

for

Insurance and Finance. Springer.

Fei, Y., (2018). Limit theory for mildly interated process with intercept, Economics Letters 163

98–101.

Gin´e, E., G¨otze, F., and Mason, D.M. (1997). When is the student t-statistic asymptotically

standard normal? The Annals of Probability 25, 1514–1531.

Hill, J.B., Li, D. and Peng, L. (2016). Uniform Interval Estimation for an AR(1) Process with

AR Errors. Statistica Sinica 26, 119–136.

20

Huang, S.H., Pang, T.X., and Weng, C. (2014). Limit theory for moderate deviations from a

unit root under innovations with a possibly inﬁnite variance. Methodology & Computing in

Applied Probability 16, 187–206.

Liu, X., and Peng, L., (2017). Asymptotic Theory And Uniform Conﬁdence Region For An

Autoregressive Model. Technical report.

Mikusheva, A. (2007). Uniform inference in autoregressive models. Econometrica 75, 1411–1452.

Phillips, P.C.B. (1987). Towards a uniﬁed asymptotic theory for autoregression. Biometrika 74,

535-547.

Phillips, P.C.B. (1990). Time series regression with a unit root and inﬁnite variance errors.

Econometric Theory 6: 44–62.

Phillips, P.C.B., and Magdalinos, T. (2007). Limit theory for moderate deviations from a unit

root. Journal of Econometrics 136, 115–130.

So, B.S. and Shin, D.W. (1999). Cauchy estimators for autoregressive processes with applications

to unit root tests and conﬁdence intervals. Econometric Theory 15, 165–176.

Wang, X., and Yu, J., (2015). Limit theory for an explosive autoregressive process. Economics

Letters 126, 176–180.

21

