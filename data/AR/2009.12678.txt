I Like to Move It: 6D Pose Estimation as an Action Decision Process

Benjamin Busam Hyun Jun Jung Nassir Navab
Technical University of Munich
{b.busam,hyunjun.jung,nassir.navab}@tum.de

0
2
0
2

v
o
N
0
3

]

V
C
.
s
c
[

2
v
8
7
6
2
1
.
9
0
0
2
:
v
i
X
r
a

Abstract

Object pose estimation is an integral part of robot vision
and AR. Previous 6D pose retrieval pipelines treat the prob-
lem either as a regression task or discretize the pose space
to classify. We change this paradigm and reformulate the
problem as an action decision process where an initial pose
is updated in incremental discrete steps that sequentially
move a virtual 3D rendering towards the correct solution.
A neural network estimates likely moves from a single RGB
image iteratively and determines so an acceptable ﬁnal
pose. In comparison to other approaches that train object-
speciﬁc pose models, we learn a decision process. This al-
lows for a lightweight architecture while it naturally gener-
alizes to unseen objects. A coherent stop action for process
termination enables dynamic reduction of the computation
cost if there are insigniﬁcant changes in a video sequence.
Instead of a static inference time, we thereby automatically
increase the runtime depending on the object motion. Ro-
bustness and accuracy of our action decision network are
evaluated on Laval [22] and YCB [81] video scenes where
we signiﬁcantly improve the state-of-the-art.

1. Introduction

We live in a 3D world. Every object with which we interact
has six degrees of freedom to move freely in space, three
for its orientation and three for its translation. Thus, the
question to determine these parameters naturally arises
whenever we include a vision system observing the scene.
A single camera will only capture a projection of this
world. Thus, recovering such information constitutes an
inherently ill-posed problem which has drawn attention of
many vision experts in the past [34, 42, 25, 52, 81]. The
motives for this can be different: One may want to extract
scene content for accurate measurements [4], camera
localization [53] or 3D reconstruction [40]. Another driver
can be geometric image manipulation [29, 8] or sensor
fusion [17]. Also human-robot interaction [6] and robot
grasping [16] require estimation of 6D poses.
The rise of low-cost RGBD sensors helped development of

Figure 1. Method Overview. A virtual object is rendered with an
initial pose on top of a real object (top). Both image and rendering
are cropped. A lightweight action decision network determines an
incremental move to bring the rendering closer to the real observa-
tion. The updated pose is used to iteratively modify the rendering.

6D pose detectors [5, 36, 75] and trackers [70, 22]. More
recently, the ﬁeld also considered methods with single RGB
image input. The best performing methods for this task are
all data-driven and thus require a certain amount of training
images. Annotating a large body of data for this kind of
task is cumbersome and time-intensive which yields to
either complex acquisition setups [22] or diverse annotation
quality. The majority of pose estimation pipelines train
on these annotations, usually one network per object.
Besides difﬁcult data annotation and the requirement for
annotations for each new object, this brings two further
drawbacks. On one hand,
the networks adjust to the
individual sensor noise of the acquisition hardware which
drastically hampers the generalization capabilities [33]. On
the other hand, every real annotation has its own errors
introduced either by the used ground truth sensor system
or by the human annotator, which propagate to models
trained on it. Modern 3D renderers, however, can produce
photorealistic images in high quantity with pixel-perfect

1

Action Vector:Discrete Actions forPose Steps and StopProcess IterationAugmented SceneReal Object+Rendered Object[ Initial Pose ]Observation:Cropped & Resized Patcharound RenderingPose Update:Modify RenderingActionDecisionNetworktxtytzrxryrzs3D ModelProcess Start 
 
 
 
 
 
ground truth. Some recent scholars therefore propose to
leverage such data [35, 69, 84] and fully train on synthetic
images. Most widely used evaluation datasets [25, 5]
provide single image acquisitions and only recently video
sequences [81, 22] with pose annotations are available even
though video data is the natural data source in applications.

Contributions and Outline. We leverage the temporal
component in video data to accelerate our pose estimation
performance and propose an RGB pose estimation pipeline
by taking inspiration from the reinforcement learning ap-
proach proposed for 2D bounding box tracking [83] where
the authors frame the problem with consecutive discrete ac-
tions for an agent. We frame 6D pose estimation as an ac-
tion decision process realized by applying a network that
determines a sequence of likely object moves as shown
in Fig. 1. At ﬁrst, an initial pose is used to render the
3D model. Both the rendering and the current image are
cropped around the virtual pose and fed to a lightweight
CNN. The network predicts a pose action to move the ren-
dering closer to the real object. The pose is then modiﬁed
according to the action and a new rendering is fed back into
the pipeline with a new crop to move the estimation incre-
mentally closer to the observation. This goes on until ei-
ther a stop criterion ﬁres or the maximum number of itera-
tions is reached. If our input is a video stream, we can use
the pose retrieved at frame t as an initial pose for frame
t + 1 which can greatly reduce the computation time as
the amount of iterations is determined by the pose actions
needed between the initial pose and the result. Improving
pose estimation with iterative inference has previously been
explored by [45] where a reﬁnement network is iteratively
applied to reﬁne a pose predicted by an estimator such as
PoseCNN [81]. However, the performance of their method
actually decreases if more than two iterations are used.
In summary, our contributions in this work are fourfold:

1. We reformulate 6D pose estimation as an action deci-
sion process and design a lightweight CNN architec-
ture for this task that generalizes to unseen objects.

2. We iteratively apply a shallow network to optimize the
pose and deploy a change-aware dynamic complexity
reduction scheme to improve inference cost.

3. We propose an RGB-only method able to improve
video pose estimation while being able to track ob-
jects in presence of noise and clutter.

4. We provide a data augmentation scheme to render
high-quality images of 3D models on real backgrounds
under varying clutter and occlusion.

In the remainder of the paper, we ﬁrst review the related lit-
erature in Section 2 before we present our method and net-

work architecture(Sec. 3). An extensive analysis and evalu-
ation of our method is reported in Sec. 4.

2. Related Work

Vision system acquire imagery of our 3D world. In order to
interact with objects in this world it is crucial to understand
relative position and orientation which has been addressed
in different ways in the literature.
From Markers to Features. Early works apply marker
based systems to track objects. Typical augmented reality
applications are driven by markers such as AR-Tag [19],
ArUcO [23], ARToolkit [34] or AprilTag [55]. These are
also used for sensor fusion [17] and extended to high accu-
racy systems [4]. Reliable and robust detection is of particu-
lar interest in the medical domain [18], where self-adhesive
markers allow ﬂexible usage [6].
Object-marker calibration can be intricate and time-
consuming in practice and feature extractors are a practi-
cable alternative. Methods such as SIFT [48], SURF [2],
BRISK [43], ORB [65] etc. are utilized for camera [52, 53]
and object [80, 44] pose estimation. Tracking applications
beneﬁt from the rotation accuracy of such systems in inside-
out camera setups [9]. The Perspective-n-Point (PnP) algo-
rithm and its successor EPnP [42] are still utilized to re-
cover 6D poses from 2D-3D correspondences.
Pose Regression and Classiﬁcation. Rotations densely
populate a non-Euclidean space and there are multiple
parametrization for the Riemannian manifold described by
them [7]. The geodesic distance on the unit quaternions
hypersphere is not compliant with the Euclidean L-p norm
in its 4D-embedding impeding 6D pose regression net-
works [86]. Some works therefore discretize the prob-
lem and classify [35]. Hinterstoisser [25] uses a template
matching strategy for viewpoint estimation and [10, 38, 28]
achieve a sub-linear matching complexity in the number of
objects by hashing.
Others train a regressor for RGBD [5, 71, 79, 36, 75] pose
estimation. Some scholars recently also report methods that
solely rely on RGB [12, 35, 60, 14, 81, 69] input without
the need of additional depth. To realize a 6D pose estima-
tion pipeline, these methods are usually separated into three
stages [35, 69, 60]: 2D detection, 2D keypoint extraction,
6D pose estimation. Tekin [72] is based on YOLO [62]
and thus provides a single shot method. After bounding
box corner or keypoint detection, the 6D pose is estimated
with PnP. Other approaches [30, 81, 14, 56] utilize multi-
modalities or multi-task training. More recently, pixel-
wise object correspondences [84, 57, 46] use robust PnP
within a RANSAC loop to improve the results. The model
performance is mostly hampered by the domain gap cre-
ated through synthetic-only data training which is addressed
for depth renderings by [61]. Further works address oc-
clusion [54, 20] and ambiguous [49] cases. To improve

2

upon the estimated pose, Li et al. [45] propose an RGB-
based reﬁnement strategy. Many methods, however, reﬁne
their RGB results with additional depth information using
ICP [85]. All the core networks usually require to train one
network per object. If training is done for multiple objects,
the resulting predictions become unreliable [33]. The re-
cent CorNet [59] focuses on objects geometry instead and
detects object-agnostic corners. While this is more robust,
it is in spirit similar to early pose estimation approaches
that detect signiﬁcant points. Our model is different as we
learn a discrete set of decisions that gradually lead to the
correct pose.
Iterative processes are also used in a rein-
forcement learning (RL) context for pose estimation with
weak supervision by [68] while [41] uses RL for efﬁcient
computation. Both methods train object-speciﬁc networks
on real data while we leverage synthetic training. Some
recent works also address joint pose estimation and shape
retrieval [76, 51]. While being instance-agnostic, they are
however bound to objects from the same class.

Temporal Tracking. Tracking of 3D objects using tempo-
ral information has been presented with the help of depth
maps and point clouds. It can be realized with ICP [3] and
its variants [66, 67]. These methods highly rely on an initial
pose close to the correct prediction and fail in the presence
of heavy noise and clutter [22]. To stabilise tracking with
depth maps, additional intensity information [24, 82, 31, 37]
or a robust learning procedure [70] helps. The current meth-
ods need one CNN trained per objects [21] or are bound to
speciﬁc geometrical constraints such as planar objects [77].
The recent PoseRBPF [13] presents as an efﬁcient RGB-
only tracker using a particle ﬁlter setting state-of-the-art re-
sults on the YCB dataset [81]. Although our approach may
appear similar to a classical temporal tracker whose opti-
mization procedure usually includes incremental pose up-
dates and requires initialization close to the correct pose
in order not to fail or drift [85], the convergence basin of
our method is much wider (see Sec. 4.3). While we largely
beneﬁt from temporal information in terms of computation
time, our method can also be used to detect the pose with
multiple seeds intuitively.
Pose Datasets. To compare different tracking and detec-
tion method for 6D pose estimation, different datasets exist.
LineMOD [25] and its occlusion extension [5] are arguably
the most widely used ones for detection. More recently
HomebrewedDB [33] uses three of the LineMOD objects
and adds 30 higher quality 3D models. The scenes are more
cluttered and acquired under different illumination condi-
tions. Other datasets focus on textureless industrial [26, 16]
and household [63, 15, 71] objects. A dataset summary is
given in the BOP 6D Pose Benchmark [27]. While the dif-
ferent setups are diverse and the ground truth labels often of
very high quality, objects are usually acquired from individ-
ual acquisitions that are not temporally connected making

Figure 2. Pose Update Actions. One of 13 possible actions (top)
determines the incremental rendering update (bottom).

tracking evaluation difﬁcult. The more recent YCB-Video
dataset [81], however, includes 92 video sequences of 21
household objects and 12 test videos with annotated ground
truth poses and detailed 3D models. Several RGBD trackers
also evaluate on the dataset of Garon et al. [22] that includes
severe occlusion and clutter.

3. Method

Our target is to optimize an action decision CNN to decide
for iterative discrete actions to move a rendered 3D model
to the observed position of the according object in an image
sequence as shown in Fig. 1. An initial pose is used to crop
the image with the projected bounding box of the object.
We discretize the set of possible actions to move or not to
move a 3D object depending on the six degrees of freedom
for rigid displacement in space. The 13 possible actions
are divided into six pose actions for positive parameter ad-
justment, six for negative changes and an action to stop the
process (i.e. not to move the object). An action vector reads
as

a = [tx, ty, tz, rx, ry, rz] × s ∈ {−1, 0, 1}6 × {0, 1}, (1)

where each action is associated with a positive or negative
direction. The stop action is s = 1 while s = 0 indi-
cates process continuation. Associating all possible discrete
actions with an element, we can describe the action vec-
tor also by v ∈ {0, 1}13 as shown in Fig. 2. A sequence
vn, n ∈ {1, . . . N } determines a result

V =

N
(cid:88)

i=1

vi =

N
(cid:88)

13
(cid:88)

i=1

j=1

vi (cid:12) ej =

(cid:88)

i,j

vij,

(2)

where one can split the action vector at step i into its deci-
sion components vij by element-wise multiplication with
the discrete basis vector ej whose entries are δjk with
k ∈ {1, . . . 13}. We aim to learn the next best decision
vij and iteratively sum these decisions where we can lever-
age an intermediate rendering of (cid:80)
i,j vij to compare with
the currently observed image.

3

Figure 3. Architecture Overview. The input is the RGB video frame cropped and concatenated with the rendered RGB, rendered depth
and rendered segmentation mask. A series of convolutional layers with activations are used to extract an embedding. An unsupervised
attention mask is concatenated with it before an global average pooling layer. Two fully connected layers extract the set of action logits
from which the most probable is selected with argmax.

A synthetic dataset provides training images with the best
decision deﬁned as the one that minimizes the pose error

way during training to mask the embedded feature tensor
and realize a weighted global average pooling.

vgt = arg min

ej

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Pgt − P

(cid:32) m
(cid:88)

i=1

vi + vm+1 (cid:12) ej

(cid:33)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(3)

for a pose P. This serves as supervision to train with the loss

L = (cid:107)v − vgt(cid:107)

(4)

minimizing the Hamming distance between the predicted
action vector and the best decision.
For each of these actions, we set units depending on an im-
age and a current crop: While movements for tx, ty are
measured in pixels and determine movements of the bound-
ing box as such, rx, ry, rz are measured in degrees and tz is
determined as the diameter in pixels of the current bound-
ing box. While an action can change the position and size
of the crop, the image crop is always rescaled to a quadratic
n × n patch of the same size as the rendering.
We decide to implement the action decision CNN with a
lightweight architecture that allows for training on a con-
sumer laptop. The architecture details are shown in Fig. 3.
An attention mechanism is implemented as guidance for the
network to focus on relevant image regions and ignore oc-
clusions. This attention map is learnt in an unsupervised

Stopping Criteria & Tracking Mode. Usually the itera-
tion process is stopped with the stop action in frame t and
the last pose is used to initialize the process in frame t + 1.
As we discretize the pose steps, the stop criterion, however,
may not always be hit perfectly. Moreover, the decision
boundary between the stop criterion and some close action
may lead to oscillations between two or multiple predictions
close to the correct result. To cope with this in practice, we
can also stop the process early if we encounter oscillations
and if an intermediate pose has been predicted before in the
same loop or if a maximum number of iterations is reached.

Initialization & Detection. We observe that the model
tends to ﬁrst align the rendering for the translation and per-
forms rotation actions afterwards. We make use of this ob-
servation and run the network with multiple seeds as a pose
detection pipeline omitting the use of another model as nec-
essary for [50, 45, 22]. For this, we randomly chose an
object pose and seed the image at different locations by
changing tx and ty for the pose. We then run one itera-
tion of the network in every location and record the values
for tx and ty. We normalize the 2-vector given these in-
puts and generate a sparse vector ﬁeld V on top of the im-
age as shown in Fig. 7 where we place these vectors at the

4

RGBRGBMASKDEPTHInput128 x 128 x 8Feature ExtractionConvolutional Layers with ReLu Activation[ K - S - C ][ IN ]AttentionModuleKernel Size, Stride, ChannelsInstance NormalizationAction Detection[ GAP][ FC ][ AM ]Global Average PoolingFully Connected Layers wit ReLu+ w/o ActivationArg Max3-1-32IN3-1-32IN3-2-64IN3-1-64IN3-1-128IN3-2-128IN4-1-128IN4-1-256IN4-2-512IN4-1-1GAP512FC256FC13AMseed centres. This vector ﬁeld is rather random for non-
overlapping regions while its ﬂux points toward the projec-
tion centre of the object if visible. Applying a divergence
operation W = ∇·V on the smoothed vectors allows to ﬁnd
the object centre as the maximum of W while analyzing W
helps also to determine a valuable bounding box size for
a ﬁrst crop. Running the method on a coarsely discretized
rotation space in this crop allows to ﬁnd an initial rotation
where the minimum number of iteration positively corre-
lates with a possible starting rotation. As the initial seeds
can be calculated independent from each other, this process
can be parallelized.

4. Experiments

We implemented the model using the 3D renderer from
unity [74] with a customized version of the ML-agent
toolkit [32]. We combined it with TensorFlow and train
with a batch size of 32 using the ADAM [39] optimizer
with a learning rate of 10−4 and exponential decay of 5%
every 1k iterations. We trained all our models until con-
vergence (i.e. 25k iterations for object-speciﬁc training and
50k for multi-object training). For all our experiments as
well as training and dataset creation, we used a consumer
laptop with an Intel Xeon E3-1505Mv6 CPU and an Nvdia
Quadro P5000 mobile GPU.

4.1. Training on Synthetic Data

We propose a synthetic dataset generation pipeline where
we render the 3D models with changing backgrounds and
varying poses in clutter and occlusion on top of real images.
Following [35] we use images from MS COCO [47] as
background. We randomly pick 40k images from [47] and
use the high quality 3D models from YCB [81] and the
models from Linemod [25] to render the objects during
training in various poses as shown in Fig. 4.

Data Augmentation. We augment
the renderings in
different ways with occluders, crops, image blur as well as
material and light changes before placing it on top of the
real images. As our network operates on cropped image
patches of size 128 × 128 pixels, we perform the augmen-
tation on these patches, too. We synthetically generate
50k images for each YCB [81] object and 50k images for
each Linemod [25] model. The augmentation pipeline
is described in detail in the supplementary material. We
consider these images as our synthetic ground truth.
To simulate also the initial pose seeds, we produce a variety
of 3D renderings without any augmentation a set of actions
away from the related synthetic ground truth patch. We
want our method to work particularly well close to the
correct result where it is crucial to take the right decisions
in order to converge. For this reason instead of rendering
random seeds evenly distributed in pose space, we pay

Figure 4. Dataset Creation. Synthetic 3D models are rendered in
various poses on top of real 2D images. Augmentation in form of
blur, light changes and occlusions is added. A comparison image
from the real dataset is shown on the right.

close attention near the ground truth by providing more
training data in this region. We group the pose seeds in
ﬁve clusters: 10k each for YCB and Linemod. The ﬁrst
cluster contains small misalignment in only one action
direction, where each action has an equal chance of 1/13
to be picked, also the stop-action. For the step size it holds
tx, ty ∈ [1, 5], tz, ri ∈ [1, 4] ∀i. The second group consists
of larger misalignment in only one direction with equal
chance. For this we chose tx, ty ∈ [5, 30], tz ∈ [1, 15],
ri ∈ [4, 20] ∀i. The third group is mixed where we have
one larger misalignment in one direction and the remaining
tx = 10
actions are random small misalignment (e.g.
and all other directions are randomly chosen as in group
one). The fourth and ﬁfth groups are a random small and a
random large mix of misalignments from groups one and
two.

Training. We train networks for each YCB [81] model
(object-speciﬁc training) and one network with mixed train-
ing including all YCB and Linemod models (multi-object
training). Fig. 5 shows the unsupervised training of our at-
tention map on the same image after different number of
iterations for training with cracker box. It can be seen, that
after attention on high gradient object regions (250 itera-
tions), the mask emphasizes on the overall object geome-
try excluding big occlusion patches (6k iterations) before it
learns to exclude the ﬁner occluder details such as the front
part of the drill (15k iterations).

4.2. Pose Estimation & Dataset Quality

Datasets.
High quality pose annotations are usually
acquired with ﬁducial markers, manual annotation or a
combination of both [25, 5]. This process is very time-
consuming and video annotations for 6D pose estimation
are difﬁcult to retrieve. In order to produce the marker-free
video pose dataset YCB [81], the authors manually anno-

5

Figure 5. Unsupervised Training of Attention Map. The input
RGB and input renderings are shown together with the results of
the unsupervised training of the attention map after different num-
bers of training steps.

tated only the poses of all the objects in the ﬁrst frame of a
sequence. The ground truth labels for the rest of the frames
within the sequences are retrieved by camera trajectory
estimation. Larger frame sets are possible, however, the
quality of the annotations can vary. The Laval [22] video
dataset circumvents this issue through the use of a motion
capture system and retroreﬂective markers attached to the
objects. We test our models on these two datasets and
evaluate both quantitatively and qualitatively. The models
in the YCB dataset are part of our training, while the
objects from Laval are entirely unseen.

Quantitative & Qualitative Evaluation. For all quanti-
tative experiments, we follow the protocol of [21, 22] and
reset the pose estimation with the annotated pose every 15
frames. The maximum number of action steps per frame
is set to 30. At ﬁrst, we test our networks trained on in-
dividual YCB models and compare with their ground truth
poses [81]. The result is reported in comparison with the
state-of-the-art [81, 20, 30, 54] in Tab. 1 column two to six.
We utilize the 3D metrics for ADD and ADI (for symmetric
objects) relative to the object diameter as proposed in [25].
We can note an average improvement of 9.94% compared
to [54] for our method and investigated the failure cases.
While most of them seem visually plausible, we still ob-
serve a signiﬁcant accuracy variance between the video se-
quences in YCB which we further analyzed. It turns out that
the annotations for some of the objects are slightly shifted
as shown in Fig. 6. Our method – in contrast to others with
which we compare in Tab. 1 – is fully trained on synthetic
data. Thus, we cannot learn an annotation offset during
training time due to the fact that our training setup provides
pixel-perfect ground truth. Further investigations revealed
that the ground truth annotation quality is a common is-
sue amongst multiple videos sequences in this dataset. We
believe that the main source for this is a slightly incorrect

6

Figure 6. Annotation Quality for YCB. The input image is shown
together with our prediction and the ground truth annotations. Ar-
rows and 3D visualization are added to detail the difference in
these cases where our estimation is considered incorrect.

annotation in the ﬁrst frame that propagates through the
whole sequence, as the manual label is only given in frame
one [81]. We correct this shift by a single, constant transla-
tion delta for each sequence and rerun the evaluation. The
results are shown in the last column of Tab. 1, where the
accuracy of our method improves signiﬁcantly to a margin
of 28.64% over the state-of-the-art.
Generalization and Ablation Study. Given these prob-
lematic initial annotations, we refrain form further inter-
pretation of the results and investigate another dataset [22].
To the best of our knowledge, we are the ﬁrst RGB-only
method to report object-speciﬁc results on the challenging
sequences of Laval [22] where we test the generalization
capabilities of our multi-object model. Please note that the
objects of the dataset have not been seen during training.
The results are summarized in Tab. 2 and Fig. 9 shows an
example scenario. To ablate our small network with a sin-
gle loss term, we also provide the corresponding result for
a model trained without the synthetic depth input channel.
We follow the evaluation protocol of [22] and report sepa-
rately the average error for translation and rotation. Tab. 2
shows that our multi-object model generalizes well on this
dataset where the ground truth is acquired with a profes-
sional tracking system. Both models are able to track the
unseen object in translation. While the full model provides
close results both for translation and rotation, the ablated
model focuses only on the translation component and pre-
dicts stop once the object centre is aligned with only weak
corrections for the rotation. Without the depth rendering,
the rotational error is signiﬁcantly larger. Rendering the
synthetic depth helps with respect to the rotational accuracy.
This can be explained by the fact that moving the object in a
close proximity to the observation does not require detailed
understanding of depth while rotating it correctly is more
intricate.

4.3. Robustness & Convergence

The performance of conventional trackers largely depends
on the difference between the correct pose and the ini-

GTOursOursGTRGBRGB3D3DModel

PC [81]

HMP [20]

SD [30]

HM [54]

Ours OS

Ours + Shift

002 master chef can
003 cracker box
004 sugar box
005 tomato soup can
006 mustard bottle
007 tuna ﬁsh can
008 pudding box
009 gelatin box
010 potted meat can
011 banana
019 pitcher base
021 bleach cleanser
024 bowl
025 mug
035 power drill
036 wood block
037 scissors
040 large marker
051 large clamp
052 extra large clamp
061 foam brick

Average

3.60
25.10
40.30
25.50
61.90
11.40
14.50
12.10
18.90
30.30
15.60
21.20
12.10
5.20
29.90
10.70
2.20
3.40
28.50
19.60
54.50

21.26

40.10
69.50
49.70
36.10
57.90
9.80
67.20
59.10
42.00
19.30
58.50
69.40
27.70
12.90
51.80
35.70
2.10
3.60
11.20
30.90
55.40

38.57

33.00
46.60
75.60
40.80
70.60
18.10
12.20
59.40
33.30
16.60
90.00
70.90
30.50
40.70
63.50
27.70
17.10
4.80
25.60
8.80
34.70

39.07

75.80
86.20
67.70
38.10
95.20
5.83
82.20
87.80
46.50
30.80
57.90
73.30
36.90
17.50
78.80
33.90
43.10
8.88
50.10
32.50
66.30

53.11

7.70
88.36
58.35
38.23
87.74
47.90
58.68
37.08
45.99
74.02
99.40
95.04
99.44
45.35
52.77
52.28
63.33
39.53
64.01
88.02
80.83

63.05

91.88
97.76
91.95
57.99
98.49
52.89
76.00
89.20
60.61
90.43
100.00
95.30
99.44
76.59
97.35
63.48
81.11
41.73
82.83
91.37
80.83

81.75

Table 1. Evaluation on the YCB dataset with our object-speciﬁc models. We compare the percentage of frames for which the 3D AD{D|I}
error is < 10% of the object diameter [25]. Symmetric objects are shown in italic letters.

Ours full

Ours w/o D

Occlusion

0% 15% 30%

0% 15% 30%

Turtle

T[mm]
R[deg]

Walkman

T[mm]
R[deg]

5.92
7.09

9.91
14.87

12.91
14.87

5.53
18.31

6.37
20.13

16.14
26.03

8.74
6.97

18.93
11.33

31.98
21.17

11.63
40.68

15.63
44.47

20.12
50.18

Table 2. Evaluation result on Laval dataset for different levels of
noise. We compare the full model to a model without rendered
depth input. More objects are investigated in the suppl. material.

tialization [1]. As their paradigm is temporally consis-
tent motion in videos, oftentimes close-to-correct poses are
available from the result of the previous frame or from re-
initialization with another algorithm [13] while we can use
our framework for intrinsic initialization as shown in Fig. 7.
Recent methods severely suffer if the initialization is too far
off [21, 22]. Moreover, most conventional 3D trackers are
not able to detect whether their estimation is correct or not.
In contrast to them, we propose a pipeline with a large con-
vergence basin able to detect its own drift by analysing the
number of steps and our stopping criterion.
We test the convergence radius of our model by providing
different initial poses with gradually increasing deviation
from the correct result. After manually checking the ground
truth poses of the YCB dataset [81], we decided to test with
power drill on all keyframes from video sequence 50 which
provides reliable annotations. We prepare initial poses by

Figure 7. Initial Point & Rotation Seeding. The predictions for
tx and ty generate a vector ﬁeld over the image (top left) whose
divergence (top right) determines the initial point. Seeding a ran-
dom rotation at this point allows to calculate the initial pose. The
necessary number of iterations is plotted (bottom) against differ-
ent seeds at a certain deviation from this rotation in just one action
parameter (in this case rz). A good initialization in the example is
+5 actions away where the curve has its minimum.

deteriorating the ground truth annotations with increasing
noise from the correct result to an initialization which is
270 actions apart. This is done by adding actions to the GT
pose with the state [tx, ty, tz, rx, ry, rz] in the form of:

∆ · [m(tx), m(ty), m(tz), m(rx), m(ry), m(rz)] ,

where m(q) = m · sgn (X) ,

(5)

(6)

for all state variables q. We vary the value m ∈ {0, ..., 45}
and X is drawn from the uniform distribution U (−1, 1)

7

Figure 8. Robustness Test. The average ADD score is shown for
increasing deviations (top) from the ground truth (orange) while
the average number of steps the method needed for convergence is
illustrated in blue. For deviations with m ≥ 43 the method did not
converge within 200 steps.

and determines the direction of corruption. The parameter
∆ = 6 sets the deviation for our test.
We use the individually trained model and set the stepsize
for all actions to three. Then we run the method and record
the average ADD accuracy score as well as the average
number of steps in case the model converges to the correct
solution. We randomly reduce the amount of keyframes for
m ∈ {25, ..., 30} to 25% and for m ∈ {31, ..., 45} to 10%
to avoid unreasonably long computations. If convergence
is not reached within 200 steps, we treat the run as a fail.
The results are summarized in Fig. 8. Note that even for a
large deviation of m = 12 which is signiﬁcantly larger then
the deviation found in the video sequence, our accuracy
is ADD = 73.8%. Moreover, we can also see reasonable
convergence in cases with 50% or fewer bounding box
overlap where other methods [21] struggle and drift.
We use this wide convergence basin to show that our
framework can be modiﬁed without retraining to also
provide an initial pose close to the correct one in Fig. 7.

Failure Cases.
Even though the convergence of our
method is reliable, the network capacity is limited. This re-
sults in pose estimation failures in case of heavy occlusions
and ﬁne detailed geometry. Moreover, we share the issue
with other RGB-only methods that low-textured objects are
difﬁcult to estimate reliably which results in drift in some
cases as depicted in Fig. 9 together with further examples.

Runtime.
Due to the stop action, a dynamic runtime
improvements can be reported depending on the motion
present in the scene. Since the number of iteration steps is
non-static and the 3D rendering is negligible for this com-
parison, the overall runtime depends on two parameters: the
action decision cycle and the number of actions. In our im-

8

Figure 9. Estimation Examples. We show some prediction ex-
amples from the Laval [22] dataset. Self-occluded ﬁne details
(Dragon), low texture (Clock) and occlusions (Shoe) can cause
pose estimation failure for unseen objects.

plementation, the runtime for one loop in the cycle breaks
down in the image preprocessing on CPU and the inference
on the GPU. We performe a runtime test averaging 512 it-
erations. The results are shown in Tab. 3.

Average Runtime on

CPU

GPU

Total

Average Runtime in ms

14.6

5.2

19.8

Table 3. Average Runtime of Action Decision Process Cycle.

Given the average of 4.2 actions on our YCB tests, we re-
port an overall average runtime of 83.16 ms or 12 FPS. The
runtime can be further increased if the image processing
was also ported to the GPU.

5. Conclusion

We reformulated 6D pose estimation as an action decision
process and presented a pipeline to solve it as a generic task
without the need for object-speciﬁc training. The method
implements a dynamic runtime complexity depending on
the inter-frame motion to increase performance and gen-
eralizes to unseen objects. However, while improving the
state-of-the-art for RGB-based video pose estimation, it still
struggles in challenging cases for unseen objects. Currently
we search for the next best pose in every step. An interest-
ing direction for future research could be to integrate built
up knowledge over time leveraging a more complex deci-
sion embedding space.

6. Supplementary Material

In this additional section, we detail
the augmentation
pipeline in Sec. 6.1 and report additional results on the YCB
(Sec. 6.2) and Laval (Sec. 6.3) datasets. Please also note the
supplementary video that provides qualitative examples of
our pipeline.1

6.1. Data Augmentation Details

We simulate two different blurs to augment the data with
TensorFlow.
In 75% of the cases, we randomly add mo-
tion blur and in 25% of training scenarios a radial blur.
Both are generated with a mean of µ = 0 and σ = 0.05
standard deviation for all three colour channels. Vari-
ety in the exposures are augmented through changes of
brightness, contrast and saturation values in the range of
[0.95, 1.25]. For object material and light augmentation,
we leverage the unity engine and simulate 20% of un-
lit material and 80% of standard material (i.e. metal-
lic with [0, 0.85] and glossiness/smoothness with [0, 0.8]).
Light is augmented with ﬁve point lights at random posi-
tions with an intensity drawn from [0.5, 1.5]. We change
the light colour randomly by picking one colour from
C = {blue, cyan, green, magenta, red, yellow, white} at
every capture and set the same colour for all ﬁve lights.
The colour brightness for the light is randomly enhanced
offering subtle additional variation in contrast to the inten-
sity changes. Then we randomly crop the rendering patch
with 128 × 128 pixels to a height and width within [96, 128]
and resize the patch to a value within [32, 64]. To simulate
occlusion, we render 20k patches from YCB and Linemod
models with random poses from which we pick four sam-
ples at each training step. Firstly, they all are processed by
the aforementioned blur and colour augmentation scheme.
In 50% of the cases, we do not occlude the patch. In the
other cases we use these four samples for occlusion. With a
12.5% chance we respectively select either one, two or three
occluders at random or use all four. Finally, we crop the en-
tire masked region of the augmentation pipeline in 25% of
the cases to simulate another occlusion scenario where we
select the cropped region patch height and width randomly
from [72, 96]. We apply this procedure to generate 50k im-
ages for each YCB [81] object and 50k images for each
Linemod [25] model.

6.2. Additional YCB Comparison

The main paper shows a quantitative evaluation on the stan-
dard ADD metric [25] relative to the object diameter where
a pose estimate is considered successful if its ADD value is
below 10% of the object diameter. The ﬁnal ADD score is
calculated by the percentage of frames with such a success-
ful estimation. Tables 4 and 5 additionally compare the area

1Link to additional material.

under the ADD threshold curve (AUC) for varying absolute
thresholds from zero to 0.1 m [81]. The extensive study in
comparison with the state-of-the-art shows that our method
compares favourable on the standard benchmark (Ours OS)
and signiﬁcantly better with the shift-correction.

6.3. Additional Laval Results

The main paper shows the results for the error on the Laval
dataset [22] for two objects. Table 6 shows additional oc-
clusion levels and the results for the remaining objects of
the dataset which further support the claims from the main
paper.

References

[1] Sharath Akkaladevi, Martin Ankerl, Christoph Heindl, and
Andreas Pichler. Tracking multiple rigid symmetric and non-
In 2016
symmetric objects in real-time using depth data.
IEEE International Conference on Robotics and Automation
(ICRA), pages 5644–5649. IEEE, 2016. 7

[2] Herbert Bay, Tinne Tuytelaars, and Luc Van Gool. Surf:
Speeded up robust features. In European conference on com-
puter vision, pages 404–417. Springer, 2006. 2

[3] Paul J Besl and Neil D McKay. Method for registration of
3-d shapes. In Sensor fusion IV: control paradigms and data
structures, volume 1611, pages 586–606. International Soci-
ety for Optics and Photonics, 1992. 3

[4] Tolga Birdal, Ievgeniia Dobryden, and Slobodan Ilic. X-tag:
A ﬁducial tag for ﬂexible and accurate bundle adjustment. In
3D Vision (3DV), 2016 Fourth International Conference on,
pages 556–564. IEEE, 2016. 1, 2

[5] Eric Brachmann, Alexander Krull, Frank Michel, Stefan
Gumhold, Jamie Shotton, and Carsten Rother. Learning
6d object pose estimation using 3d object coordinates.
In
European conference on computer vision, pages 536–551.
Springer, 2014. 1, 2, 3, 5

[6] Benjamin Busam, Marco Esposito, Simon Che’Rose, Nas-
sir Navab, and Benjamin Frisch. A stereo vision approach
for cooperative robotic movement therapy. In Proceedings
of the IEEE International Conference on Computer Vision
Workshops, pages 127–135, 2015. 1, 2

[7] Benjamin Busam, Marco Esposito, Benjamin Frisch, and
Nassir Navab. Quaternionic Upsampling: Hyperspherical
In 2016 Fourth In-
Techniques for 6 DoF Pose Tracking.
ternational Conference on 3D Vision (3DV), pages 629–638.
IEEE, 10 2016. 2

[8] Benjamin Busam, Matthieu Hog, Steven McDonagh, and
Gregory Slabaugh. Sterefo: Efﬁcient image refocusing with
stereo vision. In Proceedings of the IEEE International Con-
ference on Computer Vision Workshops, pages 0–0, 2019. 1
[9] Benjamin Busam, Patrick Ruhkamp, Salvatore Virga, Beat-
rice Lentes, Julia Rackerseder, Nassir Navab, and Christoph
Hennersperger. Markerless inside-out tracking for 3d ultra-
sound compounding. In Simulation, Image Processing, and
Ultrasound Systems for Assisted Diagnosis and Navigation,
pages 56–64. Springer, 2018. 2

9

Model

3DC [81]

PC [81]

CPC [11]

PRBPF [13]

RKF [64]

HM [54]

Ours OS

+ Shift

002 master chef can
003 cracker box
004 sugar box
005 tomato soup can
006 mustard bottle
007 tuna ﬁsh can
008 pudding box
009 gelatin box
010 potted meat can
011 banana
019 pitcher base
021 bleach cleanser
024 bowl
025 mug
035 power drill
036 wood block
037 scissors
040 large marker
051 large clamp
052 extra large clamp
061 foam brick

Average

12.30
16.80
28.70
27.30
25.90
5.40
14.90
25.40
18.70
3.20
27.30
25.20
2.70
9.00
18.00
1.20
1.00
0.20
6.90
2.70
0.60

13.02

50.90
51.70
68.60
66.00
79.90
70.40
62.90
75.20
59.60
72.30
52.50
50.50
6.50
57.70
55.10
31.80
35.80
58.00
25.00
15.80
40.40

51.74

62.32
66.69
67.19
75.52
83.79
60.98
62.17
83.84
65.86
37.74
62.19
55.14
3.55
45.83
76.47
0.12
56.42
55.26
29.73
21.99
51.80

53.55

63.30
77.80
79.60
73.00
84.70
64.20
64.50
83.00
51.80
18.40
63.70
60.50
28.40
77.90
71.80
2.30
38.70
67.10
38.30
32.30
84.10

58.35

54.60
57.60
57.60
68.30
79.00
43.50
65.90
74.80
50.30
8.20
77.80
59.30
-
69.10
71.40
-
-
-
-
-
-

60.59

81.90
83.60
83.60
79.80
91.50
48.70
69.12
93.70
79.10
51.70
69.40
76.20
3.60
53.90
82.90
0.00
65.30
56.50
57.20
23.60
32.10

62.05

65.61
84.34
78.43
66.83
86.05
65.90
79.00
82.92
75.21
84.99
85.14
89.27
85.89
78.95
76.56
48.62
79.78
73.27
56.09
67.31
86.52

76.03

91.15
90.74
91.05
76.06
94.03
69.12
83.01
92.78
79.44
90.19
94.22
90.68
87.03
87.83
91.95
53.52
83.99
75.31
65.97
78.06
86.70

83.47

Table 4. Evaluation on the YCB dataset with our object-speciﬁc models. We compare the area under the ADD threshold curve (AUC) for
varying thresholds from zero to 0.1 m. Symmetric objects are shown in italic letters.

Model

R&C [58]

Dope [73]

HMP [20] MT [78]

D-IM [45]

PV-N [57]

Ours OS

+ Shift

002 master chef can
003 cracker box
004 sugar box
005 tomato soup can
006 mustard bottle
007 tuna ﬁsh can
008 pudding box
009 gelatin box
010 potted meat can
011 banana
019 pitcher base
021 bleach cleanser
024 bowl
025 mug
035 power drill
036 wood block
037 scissors
040 large marker
051 large clamp
052 extra large clamp
061 foam brick

Average

76.70
82.90
86.40
57.40
86.70
69.70
68.80
73.00
74.60
68.80
83.80
78.30
1.50
57.90
81.50
0.00
75.40
59.80
75.30
20.40
37.00

62.66

-
55.90
75.70
76.10
81.90
-
-
-
39.40
-
-
-
-
-
-
-
-
-
-
-
-

65.80

75.80
78.00
76.50
72.10
78.90
51.60
85.60
86.70
70.10
47.90
71.80
69.10
-
43.40
76.80
-
42.90
47.60
-
-
-

67.18

62.70
80.90
83.80
60.40
85.10
75.40
17.70
79.90
55.00
59.60
96.10
89.40
49.50
87.70
96.40
43.80
60.20
87.50
90.70
88.10
26.30

70.30

71.20
83.60
94.10
86.10
91.50
87.70
82.70
91.90
76.20
81.20
90.10
81.20
8.60
81.40
85.50
60.00
60.90
75.60
48.40
31.00
35.90

71.66

81.60
80.50
84.90
78.20
88.30
62.20
85.20
88.70
65.10
51.80
91.20
74.80
-
81.50
83.40
-
54.80
35.80
-
-
-

74.25

65.61
84.34
78.43
66.83
86.05
50.30
79.00
82.92
75.21
84.99
85.14
89.27
85.89
78.95
76.56
48.62
79.78
73.27
56.09
67.31
86.52

76.03

91.15
90.74
91.05
76.06
94.03
90.20
83.01
92.78
79.44
90.19
94.22
90.68
87.03
87.83
91.95
53.52
83.99
75.31
65.97
78.06
86.70

83.47

Table 5. Evaluation on the YCB dataset with our object-speciﬁc models. We compare the area under the ADD threshold curve (AUC) for
varying thresholds from zero to 0.1 m. Symmetric objects are shown in italic letters.

[10] Hongping Cai, Tom´aˇs Werner, and Jiˇr´ı Matas. Fast detection
of multiple textureless 3-d objects. In International Confer-
ence on Computer Vision Systems, pages 103–112. Springer,
2013. 2

[11] Catherine Capellen, Max Schwarz, and Sven Behnke. Con-
vposecnn: Dense convolutional 6d object pose estimation.
arXiv preprint arXiv:1912.07333, 2019. 10

tation of parts for accurate 3d object detection and tracking in
monocular images. In Proceedings of the IEEE international
conference on computer vision, pages 4391–4399, 2015. 2

[13] Xinke Deng, Arsalan Mousavian, Yu Xiang, Fei Xia, Timo-
thy Bretl, and Dieter Fox. Poserbpf: A rao-blackwellized
particle ﬁlter for 6d object pose tracking. arXiv preprint
arXiv:1905.09304, 2019. 3, 7, 10

[12] Alberto Crivellaro, Mahdi Rad, Yannick Verdie, Kwang
Moo Yi, Pascal Fua, and Vincent Lepetit. A novel represen-

[14] Thanh-Toan Do, Ming Cai, Trung Pham, and Ian Reid.
Deep-6DPose: Recovering 6D Object Pose from a Single

10

Occlusion

0%

15%

30%

45%

0%

15%

30%

45%

Ours full

Ours w/o D

Clock

T[mm]
R[deg]

Cookie Jar

T[mm]
R[deg]

Dog

T[mm]
R[deg]

Dragon

T[mm]
R[deg]

Shoe

T[mm]
R[deg]

Turtle

T[mm]
R[deg]

Walkman

T[mm]
R[deg]

14.02
9.40

3.82
6.48

12.09
11.70

22.47
3.34

9.72
5.84

5.92
7.09

8.74
6.97

Watering Can

T[mm]
R[deg]

14.67
11.89

20.54
10.84

5.99
17.82

28.37
14.21

29.39
4.89

17.91
9.26

9.91
14.87

18.93
11.33

21.66
19.80

25.85
12.74

9.52
18.22

55.48
22.43

36.37
11.65

24.33
17.89

12.91
14.87

31.98
21.17

18.68
23.43

51.92
17.05

15.18
15.89

77.91
23.80

40.06
13.39

37.34
16.91

23.92
14.11

45.13
22.26

33.26
33.54

9.39
29.15

1.79
28.77

6.10
20.75

25.69
27.16

44.61
62.78

5.53
18.31

11.63
40.68

11.61
38.89

9.96
27.92

2.75
18.18

10.76
26.81

25.13
36.40

19.90
39.47

6.37
20.13

15.63
44.47

20.54
40.85

32.58
30.72

11.62
24.30

33.89
24.22

27.71
37.61

38.04
43.50

16.14
26.03

20.12
50.18

20.96
36.30

15.91
28.40

5.95
19.02

15.62
22.53

30.65
30.94

41.90
24.73

12.63
24.97

31.30
45.14

26.10
35.23

Table 6. Evaluation error on Laval dataset for different levels of noise. We compare the full model to a model without rendered depth input.

RGB Image. arXiv preprint arXiv:1802.10367, 2018. 2
[15] Andreas Doumanoglou, Rigas Kouskouridas, Sotiris Malas-
siotis, and Tae-Kyun Kim. Recovering 6d object pose and
predicting next-best-view in the crowd. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 3583–3592, 2016. 3

[16] Bertram Drost, Markus Ulrich, Paul Bergmann, Philipp
Introducing mvtec itodd-a
Hartinger, and Carsten Steger.
dataset for 3d object recognition in industry. In Proceedings
of the IEEE International Conference on Computer Vision,
pages 2200–2208, 2017. 1, 3

[17] Marco Esposito, Benjamin Busam, Christoph Hen-
nersperger, Julia Rackerseder, An Lu, Nassir Navab, and
Benjamin Frisch. Cooperative robotic gamma imaging:
In International
Enhancing us-guided needle biopsy.
Conference on Medical Image Computing and Computer-
Assisted Intervention, pages 611–618. Springer, 2015. 1,
2

[18] Marco Esposito, Benjamin Busam, Christoph Hen-
nersperger, Julia Rackerseder, Nassir Navab, and Benjamin
Frisch. Multimodal US–gamma imaging using collaborative
robotics for cancer staging biopsies. International journal of
computer assisted radiology and surgery, 11(9):1561–1571,
2016. 2

[19] Mark Fiala. ARTag, a ﬁducial marker system using digital
In Computer Vision and Pattern Recognition,
techniques.
2005. CVPR 2005. IEEE Computer Society Conference on,
volume 2, pages 590–596. IEEE, 2005. 2

[20] Mingliang Fu and Weijia Zhou. Deephmap++: Combined
projection grouping and correspondence learning for full dof
pose estimation. Sensors, 19(5):1032, 2019. 2, 6, 7, 10
[21] Mathieu Garon and Jean-Franc¸ois Lalonde. Deep 6-dof
tracking. IEEE transactions on visualization and computer
graphics, 23(11):2410–2418, 2017. 3, 6, 7, 8

[22] Mathieu Garon, Denis Laurendeau, and Jean-Franc¸ois
Lalonde. A framework for evaluating 6-dof object trackers.
In Proceedings of the European Conference on Computer Vi-
sion (ECCV), pages 582–597, 2018. 1, 2, 3, 4, 6, 7, 8, 9

[23] Sergio Garrido-Jurado, Rafael Mu˜noz-Salinas,

Fran-
cisco Jos´e Madrid-Cuevas, and Manuel Jes´us Mar\’\in-
Jim´enez. Automatic generation and detection of highly
reliable ﬁducial markers under occlusion. Pattern Recogni-
tion, 47(6):2280–2292, 2014. 2

[24] Robert Held, Ankit Gupta, Brian Curless, and Maneesh
Agrawala. 3d puppetry: a kinect-based interface for 3d ani-
mation. In UIST, pages 423–434. Citeseer, 2012. 3

[25] Stefan Hinterstoisser, Vincent Lepetit, Slobodan Ilic, Ste-
fan Holzer, Gary Bradski, Kurt Konolige, and Nassir Navab.

11

Model based training, detection and pose estimation of
texture-less 3d objects in heavily cluttered scenes. In Asian
conference on computer vision, pages 548–562. Springer,
2012. 1, 2, 3, 5, 6, 7, 9

[26] Tom´aˇs Hodan, Pavel Haluza, ˇStep´an Obdrˇz´alek, Jiri Matas,
Manolis Lourakis, and Xenophon Zabulis. T-less: An rgb-
d dataset for 6d pose estimation of texture-less objects. In
2017 IEEE Winter Conference on Applications of Computer
Vision (WACV), pages 880–888. IEEE, 2017. 3

[27] Tomas Hodan, Frank Michel, Eric Brachmann, Wadim Kehl,
Anders GlentBuch, Dirk Kraft, Bertram Drost, Joel Vidal,
Stephan Ihrke, Xenophon Zabulis, et al. Bop: Benchmark
for 6d object pose estimation. In Proceedings of the Euro-
pean Conference on Computer Vision (ECCV), pages 19–34,
2018. 3

[28] Tom´aˇs Hodaˇn, Xenophon Zabulis, Manolis Lourakis, ˇStˇep´an
Obdrˇz´alek, and Jiˇr´ı Matas. Detection and ﬁne 3d pose es-
In 2015
timation of texture-less objects in rgb-d images.
IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS), pages 4421–4428. IEEE, 2015. 2
[29] Aleksander Holynski and Johannes Kopf. Fast depth densiﬁ-
cation for occlusion-aware augmented reality. In SIGGRAPH
Asia 2018 Technical Papers, page 194. ACM, 2018. 1
[30] Yinlin Hu, Joachim Hugonot, Pascal Fua, and Mathieu Salz-
mann. Segmentation-driven 6d object pose estimation.
In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 3385–3394, 2019. 2, 6, 7
[31] David Joseph Tan, Federico Tombari, Slobodan Ilic, and
Nassir Navab. A versatile learning-based 3d temporal
tracker: Scalable, robust, online. In Proceedings of the IEEE
International Conference on Computer Vision, pages 693–
701, 2015. 3

[32] Arthur Juliani, Vincent-Pierre Berges, Esh Vckay, Yuan Gao,
Hunter Henry, Marwan Mattar, and Danny Lange. Unity:
A general platform for intelligent agents. arXiv preprint
arXiv:1809.02627, 2018. 5

[33] Roman Kaskman, Sergey Zakharov, Ivan Shugurov, and Slo-
bodan Ilic. Homebreweddb: Rgb-d dataset for 6d pose esti-
mation of 3d objects. In The IEEE International Conference
on Computer Vision (ICCV) Workshops, Oct 2019. 1, 3
[34] Hirokazu Kato and Mark Billinghurst. Marker tracking and
hmd calibration for a video-based augmented reality confer-
encing system. In Augmented Reality, 1999.(IWAR’99) Pro-
ceedings. 2nd IEEE and ACM International Workshop on,
pages 85–94. IEEE, 1999. 1, 2

[35] Wadim Kehl, Fabian Manhardt, Federico Tombari, Slobodan
Ilic, and Nassir Navab. SSD-6D: Making RGB-based 3D
detection and 6D pose estimation great again. In Proceedings
of the International Conference on Computer Vision (ICCV
2017), Venice, Italy, pages 22–29, 2017. 2, 5

[36] Wadim Kehl, Fausto Milletari, Federico Tombari, Slobo-
dan Ilic, and Nassir Navab. Deep learning of local RGB-D
patches for 3D object detection and 6D pose estimation. In
European Conference on Computer Vision, pages 205–220.
Springer, 2016. 1, 2

single cpu core. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 745–753,
2017. 3

[38] Wadim Kehl, Federico Tombari, Nassir Navab, Slobodan
Ilic, and Vincent Lepetit. Hashmod: A hashing method for
scalable 3d object detection. In BMVC, volume 1, page 2,
2015. 2

[39] Diederick P Kingma and Jimmy Ba. Adam: A method
for stochastic optimization. In International Conference on
Learning Representations (ICLR), 2015. 5

[40] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen
Koltun. Tanks and temples: Benchmarking large-scale scene
reconstruction. ACM Transactions on Graphics, 36(4), 2017.
1

[41] Alexander Krull, Eric Brachmann, Sebastian Nowozin,
Frank Michel,
and Carsten Rother.
Jamie Shotton,
Poseagent: Budget-constrained 6d object pose estimation via
In Proceedings of the IEEE Con-
reinforcement learning.
ference on Computer Vision and Pattern Recognition, pages
6702–6710, 2017. 3

[42] Vincent Lepetit, Francesc Moreno-Noguer, and Pascal Fua.
EPnP: An Accurate O(n) Solution to the PnP Problem. In-
ternational Journal of Computer Vision, 81(2):155–166, 2
2009. 1, 2

[43] Stefan Leutenegger, Margarita Chli, and Roland Y Siegwart.
BRISK: Binary robust invariant scalable keypoints. In Com-
puter Vision (ICCV), 2011 IEEE International Conference
on, pages 2548–2555. IEEE, 2011. 2

[44] Yunpeng Li, Noah Snavely, and Daniel P Huttenlocher. Lo-
cation recognition using prioritized feature matching.
In
European conference on computer vision, pages 791–804.
Springer, 2010. 2

[45] Yi Li, Gu Wang, Xiangyang Ji, Yu Xiang, and Dieter Fox.
Deepim: Deep iterative matching for 6d pose estimation. In
Proceedings of the European Conference on Computer Vi-
sion (ECCV), pages 683–698, 2018. 2, 3, 4, 10

[46] Zhigang Li, Gu Wang, and Xiangyang Ji.

Cdpn:
Coordinates-based disentangled pose network for real-time
rgb-based 6-dof object pose estimation. In The IEEE Inter-
national Conference on Computer Vision (ICCV), October
2019. 2

[47] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context.
In
European conference on computer vision, pages 740–755.
Springer, 2014. 5

[48] David G Lowe. Distinctive image features from scale-
International journal of computer vi-

invariant keypoints.
sion, 60(2):91–110, 2004. 2

[49] Fabian Manhardt, Diego Martin Arroyo, Christian Rup-
precht, Benjamin Busam, Tolga Birdal, Nassir Navab, and
Federico Tombari. Explaining the ambiguity of object de-
tection and 6d pose from visual data. In Proceedings of the
IEEE International Conference on Computer Vision, pages
6841–6850, 2019. 2

[37] Wadim Kehl, Federico Tombari, Slobodan Ilic, and Nassir
Navab. Real-time 3d model tracking in color and depth on a

[50] Fabian Manhardt, Wadim Kehl, Nassir Navab, and Federico
Tombari. Deep model-based 6d pose reﬁnement in rgb. In

12

Proceedings of the European Conference on Computer Vi-
sion (ECCV), pages 800–815, 2018. 4

IEEE Robotics and Automation Letters, 1(2):1179–1185,
2016. 3

[51] Fabian Manhardt, Gu Wang, Benjamin Busam, Manuel
Nickel, Sven Meier, Luca Minciullo, Xiangyang Ji, and
Nassir Navab.
Improving class-level 6d pose
and shape estimation from monocular images with self-
supervised learning, 2020. 3

Cps++:

[52] Raul Mur-Artal, Jose Maria Martinez Montiel, and Juan D
Tardos. Orb-slam: a versatile and accurate monocular slam
IEEE transactions on robotics, 31(5):1147–1163,
system.
2015. 1, 2

[53] Raul Mur-Artal and Juan D Tard´os. Orb-slam2: An open-
source slam system for monocular, stereo, and rgb-d cam-
IEEE Transactions on Robotics, 33(5):1255–1262,
eras.
2017. 1, 2

[54] Markus Oberweger, Mahdi Rad, and Vincent Lepetit. Mak-
ing deep heatmaps robust to partial occlusions for 3d object
pose estimation. In Proceedings of the European Conference
on Computer Vision (ECCV), pages 119–134, 2018. 2, 6, 7,
10

[55] Edwin Olson. AprilTag: A robust and ﬂexible visual ﬁdu-
cial system. In Robotics and Automation (ICRA), 2011 IEEE
International Conference on, pages 3400–3407. IEEE, 2011.
2

[56] Georgios Pavlakos, Xiaowei Zhou, Aaron Chan, Konstanti-
nos G Derpanis, and Kostas Daniilidis. 6-dof object pose
In Robotics and Automation
from semantic keypoints.
(ICRA), 2017 IEEE International Conference on, pages
2011–2018. IEEE, 2017. 2

[57] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hu-
jun Bao. Pvnet: Pixel-wise voting network for 6dof pose esti-
mation. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 4561–4570, 2019. 2,
10

[58] Arul Selvam Periyasamy, Max Schwarz, and Sven Behnke.
Reﬁning 6d object pose predictions using abstract render-
and-compare. arXiv preprint arXiv:1910.03412, 2019. 10

[59] Giorgia Pitteri, Slobodan Ilic, and Vincent Lepetit. Cor-
net: Generic 3d corners for 6d pose estimation of new ob-
jects without retraining. In Proceedings of the IEEE Inter-
national Conference on Computer Vision Workshops, pages
0–0, 2019. 3

[60] Mahdi Rad and Vincent Lepetit. BB8: A scalable, accu-
rate, robust to partial occlusion method for predicting the 3D
poses of challenging objects without using depth. In ICCV,
2017. 2

[61] Mahdi Rad, Markus Oberweger, and Vincent Lepetit. Do-
main transfer for 3d pose estimation from color images with-
out manual annotations. In Asian Conference on Computer
Vision, pages 69–84. Springer, 2018. 2

[62] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali
Farhadi. You only look once: Uniﬁed, real-time object de-
tection. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 779–788, 2016. 2
[63] Colin Rennie, Rahul Shome, Kostas E Bekris, and Alberto F
De Souza. A dataset for improved rgbd-based object de-
tection and pose estimation for warehouse pick-and-place.

[64] Jesse Richter-Klug and Udo Frese. Towards meaningful un-
certainty information for cnn based 6d pose estimates. In In-
ternational Conference on Computer Vision Systems, pages
408–422. Springer, 2019. 10

[65] Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary
Bradski. ORB: An efﬁcient alternative to SIFT or SURF. In
2011 International Conference on Computer Vision, pages
2564–2571. IEEE, 11 2011. 2

[66] Szymon Rusinkiewicz and Marc Levoy. Efﬁcient variants of
the icp algorithm. In 3dim, volume 1, pages 145–152, 2001.
3

[67] Aleksandr Segal, Dirk Haehnel, and Sebastian Thrun.
In Robotics: science and systems, vol-

Generalized-icp.
ume 2, page 435. Seattle, WA, 2009. 3

[68] Jianzhun Shao, Yuhang Jiang, Gu Wang, Zhigang Li, and
Xiangyang Ji. Pfrl: Pose-free reinforcement learning for
6d pose estimation. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pages
11454–11463, 2020. 3

[69] Martin Sundermeyer, Zoltan-Csaba Marton, Maximilian
Durner, Manuel Brucker, and Rudolph Triebel. Implicit 3D
orientation learning for 6D object detection from RGB im-
In European Conference on Computer Vision, pages
ages.
712–729. Springer, 2018. 2

[70] David J. Tan and Slobodan Ilic. Multi-forest tracker: A
In The IEEE Conference on Com-
chameleon in tracking.
puter Vision and Pattern Recognition (CVPR), June 2014. 1,
3

[71] Alykhan Tejani, Danhang Tang, Rigas Kouskouridas, and
Tae-Kyun Kim. Latent-class hough forests for 3D object
detection and pose estimation. In European Conference on
Computer Vision, pages 462–477. Springer, 2014. 2, 3
[72] Bugra Tekin, Sudipta N Sinha, and Pascal Fua. Real-time
seamless single shot 6D object pose prediction. CVPR, 2018.
2

[73] Jonathan Tremblay, Thang To, Balakumar Sundaralingam,
Yu Xiang, Dieter Fox, and Stan Birchﬁeld. Deep object pose
estimation for semantic robotic grasping of household ob-
jects. arXiv preprint arXiv:1809.10790, 2018. 10

[74] Unity Technologies. Unity. https://unity3d.com/
unity/whats-new/2018.3.6. Accessed: 2019-06-
10. 5

[75] Chen Wang, Danfei Xu, Yuke Zhu, Roberto Mart´ın-Mart´ın,
Cewu Lu, Li Fei-Fei, and Silvio Savarese. Densefusion: 6d
object pose estimation by iterative dense fusion. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 3343–3352, 2019. 1, 2

[76] He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin,
Shuran Song, and Leonidas J Guibas. Normalized object co-
ordinate space for category-level 6d object pose and size esti-
mation. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 2642–2651, 2019. 3

[77] Tao Wang and Haibin Ling. Gracker: A graph-based planar
IEEE transactions on pattern analysis and

object tracker.
machine intelligence, 40(6):1494–1501, 2017. 3

13

[78] Yurui Wang, Shaokun Jin, and Yongsheng Ou. A multi-
task learning convolutional neural network for object pose
In International Conference on Robotics and
estimation.
Biomimetics (ROBIO), pages 284–289. IEEE, 2019. 10
[79] Paul Wohlhart and Vincent Lepetit. Learning descriptors for
object recognition and 3d pose estimation. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 3109–3118, 2015. 2

[80] Changchang Wu, Friedrich Fraundorfer, Jan-Michael Frahm,
and Marc Pollefeys. 3D model search and pose estimation
In Computer Vi-
from single images using VIP features.
sion and Pattern Recognition Workshops, 2008. CVPRW’08.
IEEE Computer Society Conference on, pages 1–8. IEEE,
2008. 2

[81] Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, and
Dieter Fox. Posecnn: A convolutional neural network for 6d
object pose estimation in cluttered scenes. Robotics: Science
and Systems (RSS), 2018. 1, 2, 3, 5, 6, 7, 9, 10

[82] Carl Yuheng Ren, Victor Prisacariu, David Murray, and Ian
Reid. Star3d: Simultaneous tracking and reconstruction of
In Proceedings of the IEEE
3d objects using rgb-d data.
International Conference on Computer Vision, pages 1561–
1568, 2013. 3

[83] Sangdoo Yun, Jongwon Choi, Youngjoon Yoo, Kimin Yun,
and Jin Young Choi. Action-decision networks for visual
In The IEEE
tracking with deep reinforcement learning.
Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017. 2

[84] Sergey Zakharov, Ivan Shugurov, and Slobodan Ilic. Dpod:
In The IEEE Inter-
6d pose object detector and reﬁner.
national Conference on Computer Vision (ICCV), October
2019. 2

[85] Zhengyou Zhang.

of free-form curves and surfaces.
computer vision, 13(2):119–152, 1994. 3

Iterative point matching for registration
International journal of

[86] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and
Hao Li. On the continuity of rotation representations in
In Proceedings of the IEEE Conference
neural networks.
on Computer Vision and Pattern Recognition, pages 5745–
5753, 2019. 2

14

