Noname manuscript No.
(will be inserted by the editor)

Joint Surgical Gesture and Task Classiﬁcation with

Multi-Task and Multimodal Learning

Duygu Sarikaya · Khurshid A. Guru ·

Jason J. Corso

Received: date / Accepted: date

Abstract

Purpose

We propose a novel multi-modal and multi-task architecture for simultaneous low

level gesture and surgical task classiﬁcation in Robot Assisted Surgery (RAS)

videos.

Methods

Our end-to-end architecture is based on the principles of a long short-term memory

network (LSTM) that jointly learns temporal dynamics on rich representations of

visual and motion features, while simultaneously classifying activities of low-level

gestures and surgical tasks.

Results

D. Sarikaya
Department of Computer Science and Engineering, SUNY Buﬀalo, NY 14260-1660 USA
E-mail: duygusar@buﬀalo.edu

K. A. Guru
Applied Technology Laboratory for Advanced Surgery, Roswell Park Cancer Institute, Buﬀalo,
NY 14263 USA

J. J. Corso
Department of Electrical Engineering and Computer Science, University of Michigan, Ann
Arbor, MI 48109 USA

8
1
0
2

y
a
M
2

]

V
C
.
s
c
[

1
v
1
2
7
0
0
.
5
0
8
1
:
v
i
X
r
a

 
 
 
 
 
 
2

Duygu Sarikaya et al.

Our experimental results show that our approach is superior compared to an ar-

chitecture that classiﬁes the gestures and surgical tasks separately on visual cues

and motion cues respectively. We train our model on a ﬁxed random set of 1200

gesture video segments and use the rest 422 for testing. This results in around

42,000 gesture frames sampled for training and 14,500 for testing. For a 6 split

experimentation, while the conventional approach reaches an Average Precision

(AP) of only 29% (29.13%), our architecture reaches an AP of 51% (50.83%) for

3 tasks and 14 possible gesture labels, resulting in an improvement of 22% (21.7%).

Conclusions

Our architecture learns temporal dynamics on rich representations of visual and

motion features that compliment each other for classiﬁcation of low-level gestures

and surgical tasks. Its multi-task learning nature makes use of learned joint re-

lationships and combinations of shared and task-speciﬁc representations. While

benchmark studies focus on recognizing gestures that take place under speciﬁc

tasks, we focus on recognizing common gestures that reoccur across diﬀerent tasks

and settings and signiﬁcantly perform better compared to conventional architec-

tures.

Keywords Robot-Assisted Surgery · Surgical Gesture Classiﬁcation · Multi-task

Learning · Multimodal Learning · Long Short-term Recurrent Neural Networks ·

Convolutional Neural Networks

1 Introduction

Video understanding of robot-assisted surgery (RAS) videos is an active research

area. Modeling the gestures and skill level of surgeons presents an interesting

problem for the needs addressed by the community such as automation and early

identiﬁcation of technical competence for surgeons in training. We approach the

problem of video understanding of RAS videos as modeling the motions of sur-

geons. By analyzing the scene and object features, motion, low-level surgical ges-

Title Suppressed Due to Excessive Length

3

tures and the transitions among the gestures, we could model the activities taking

place during surgical tasks [1]. The insights drawn may be applied in eﬀective

skill acquisition, objective skill assessment, real-time feedback, and human-robot

collaborative surgeries [2].

We propose to model the low-level surgical gestures; recurring common activity

segments as described by Gao et al. [3] and the surgical tasks that are composed of

gestures with a multimodal and multi-task learning approach. We use the diﬀerent

modalities of the visual features and motion cues (optical ﬂow), and learn the

temporal dynamics jointly. We argue that surgical tasks are better modeled by

the visual features that are determined by the objects in the scene, while the

low-level gestures are better modeled with motion cues. For example, the gesture

Positioning the needle might take place in diﬀerent tasks of Suturing and Needle

Passing [3]. Or, on a higher level of actions, placing a Tie Knot might occur

during a task of suturing on a tissue and a more speciﬁc and challenging task of

Urethrovesical Anastomosis (UVA) [2] that involves stitching and reconnecting two

parts together. If we rely on visual features of the object and the scenes only, these

smaller segments of activities would have very diﬀerent representations. Motion

cues of low-level gestures, on the other hand, are independent of object and scene

features. This helps us to classify common low-level gestures that are generic in

nature and that reoccur in diﬀerent tasks with diﬀerent objects and under diﬀerent

settings better, and also helps reduce overﬁtting. However, we also believe that

visual features are complementary to the motion cues, which are independent of

object and scene features. The gesture “Positioning the needle” might take place

in diﬀerent tasks of Suturing and Needle Passing, however is not likely to take

place in Knot Tying tasks as a needle is not used for this task.

In this paper, we address the problem of simultaneous classiﬁcation of com-

mon low-level gestures and surgical tasks. While surgical task recognition, which

is characteristically deﬁned by the scene and the objects, is an easy task, the

low-level gesture recognition remains a challenge. In our work, we focus on this

4

Duygu Sarikaya et al.

challenging task by making use of complex relationships between the visual and

motion cues, and the relationship between surgical activities at diﬀerent levels

of complexity. We propose a novel architecture that simultaneously classiﬁes the

common low-level gestures and the surgical tasks by making use of these joint re-

lationships, combining shared and task-speciﬁc representations with a multi-task

learning approach. Moreover, our architecture supports multimodal learning and

is trained on both visual features and motion cues to achieve better performance.

An overview of our method is shown in Figure 1. We use the inputs of RGB frames

for visual features and the RGB representation of the optical ﬂow information of

the same frames to refer to motion cues. We extract the high level features of

these inputs using the convolutional neural networks we have trained on each task

separately and use them as input to our recurrent joint model. After we convolve

the two streams of input modality pairs, we concatenate the higher level features

and use it as an input to our recurrent model that learns temporal dynamics of

consequent frames.

2 Related Work

Surgical activity recognition is an active research area that gets a lot of attention

from the computer vision and medical imaging communities. The potential of au-

tonomous or human-robot collaborative surgeries, automated real-time feedback,

guidance and navigation during surgical tasks is exciting for the community. There

are studies that address this open research problem. The methods range from us-

ing SVM classiﬁcation with Linear Discriminant Analysis (LDA) [4, 5], Gaussian

mixture models (GMMs) [6], variations of Hidden Markov Models (HMM) that

represents probability distributions over sequences of observations, [7, 8] and more

recently, to convolutional neural networks and recurrent neural networks [9, 10, 11].

Ahmidi et al. [12] do a comparative benchmark study on the recognition of

gestures on JIGSAWS dataset. In this study, in order to classify surgical gestures,

three main methods are chosen: Bag of Spatio-Temporal Features (BoF), Lin-

Title Suppressed Due to Excessive Length

5

Fig. 1: We propose a novel architecture that is based on the principles of LRCN, a
specialized LSTM network that jointly learns temporal dynamics and visual fea-
tures by convolutional network models. Our architecture however, jointly learns
on the two modalities of the video: RGB frames and the RGB representation of
the optical ﬂow information. We extract higher level features using CNNs that we
train on separate tasks. After we convolve the two streams of input pairs, we con-
catenate the CNN features and use it as an input to our recurrent joint model that
learns temporal dynamics of consequent frames. Our architecture simultaneously
classiﬁes the common low-level gestures and the surgical tasks.

ear Dynamical System (LDS) and a composite Gaussian Mixture Model- Hidden

Markov Model: GMM-HMM. HMMs are often used in studies that use additional

modalities of various sensors [13, 14], and to classify high level surgical activities

[15].

Deep neural networks have been introduced to the activity recognition tasks

[16, 17]. However, in the medical ﬁeld these advances are only very recently being

6

Duygu Sarikaya et al.

explored. DiPietro et al. [10] propose using Recurrent Neural Networks (RNN)

trained on kinematic data for surgical gesture classiﬁcation on JIGSAWS dataset.

Some other recent studies focus on classifying surgery phases; a higher level of

surgical activity that includes a sequence of diﬀerent tasks. A recent work by

Cad´ene et al. [9] uses deep residual networks to extract visual features of the

video frames and then applies temporal smoothing with averaging. The authors

of this work ﬁnally model the transitions between the surgery phase steps with

an HMM. Twinanda et al. [11] oﬀer a study on classiﬁcation of surgery phases by

ﬁrst extracting visual features of video frames via a CNN and then passing them

to an SVM to compute conﬁdences of a frame belonging to a surgery phase. These

conﬁdences are used as inputs to an HMM and also to an LSTM network.

HMMs are often used in surgical activity recognition as they address the tem-

poral dynamics of the gestures and relationships between surgical phases. Although

there are studies that rely on visual features of the surgical video frames, a large

number of promising studies rely highly on kinematic data. The problem with cap-

turing kinematic data is that; it requires additional equipment, and it is possible

that the kinematic data is sampled over periods and might be missing important

motion information. For the latter case, we need to preprocess the kinematic data

by interpolation to estimate the motion information of these missing parts. Kine-

matic data also requires preprocessing based on speciﬁc attributes by the equip-

ment that has captured it. Visual features on the other hand, when used alone,

are highly dependent on the objects and scenes, and might be missing important

motion cues.

In this paper, we address the video understanding problem of activity classiﬁca-

tion in Robot-assisted Surgery (RAS) videos. We simultaneously classify common

low-level gestures and surgical tasks. We focus on modeling the common low-level

gestures that are generic and that reoccur in diﬀerent tasks. We believe that the

low-level gestures are better modeled by motion cues, while the surgical tasks are

better modeled by the visual features that are characteristically deﬁned by the ob-

Title Suppressed Due to Excessive Length

7

jects in the scene. However, we also believe that visual features are complementary

to the motion cues, which are independent of object and scene features.

Many recent studies have shown that exploiting the relationship across dif-

ferent tasks, jointly reasoning multi-tasks [20, 21, 22] and taking advantage of a

combination of shared and task-speciﬁc representations [23] perform astonishingly

better than their single-task counterparts.

We propose using recurrent neural networks [19], speciﬁcally long term short

memory (LSTM) neural networks [24], in order to model the complex temporal

dynamics and sequences. We aspire to model the sequences of gestures occurring

in surgical-task videos with a model that is deep over temporal dimensions. Our

architecture is based on the principles of LRCN [17], a specialized LSTM network

that jointly learns temporal dynamics and visual features by convolutional network

models. Our model supports multimodal learning, and uses the RGB frames and

the RGB representation of the optical ﬂow information relating the motion cues in

the same frames as input. Our architecture simultaneously classiﬁes the low-level

gestures and surgical tasks by a multi-task learning approach making use of joint

relationships, combining shared and task-speciﬁc representations. In this sense,

our model is truly end-to-end and novel in the way that it supports multimodal

and multi-task learning for surgical low-level gesture classiﬁcation.

3 Dataset

The JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS) [3] provides a

public benchmark surgical activity dataset. In this video dataset, 8 surgeons with

varying expertise perform 3 surgical tasks on the daVinci Surgical System (dVSS

R(cid:13)) (Figure 2). The dataset includes video data recorded during the performance

of the tasks: Suturing, Needle Passing and Knot Tying and provide 15 low-level

gesture labels, which are the smallest action units where the movement is inten-

tional and is carried on towards achieving a speciﬁc goal. The gestures reoccur in

various tasks. A surgical gesture is a small, however a meaningful and purposeful

8

Duygu Sarikaya et al.

Fig. 2: A temporal sequence of surgical activities during a Suturing task are shown
(from left to right, top to bottom). The surgeon does a suture on the tissue fol-
lowing the guide landmarks.

segment of activity that, the tasks could be broken into. For example, the gesture

of G2: Positioning needle could take place in both Suturing and Needle Passing

tasks. However, the objects and the scene in these two tasks are quite diﬀerent;

while Suturing involves a task of placing sutures on a tissue model, Needle Passing

takes place on a set of metal hoops [3]. The gestures form a common vocabulary

for small action segments that reoccur in diﬀerent tasks. We focus on recognizing

these low-level gestures, even when they reoccur across diﬀerent tasks with varying

accompanying objects and are performed on diﬀerent sets.

We ﬁrst clipped task videos into gesture clips using the annotation ﬁles pro-

vided with the dataset. We have converted the frame information to time and

clipped the tasks into gesture segments. Then we extracted 8 frames per second

from these videos. We resized these frames to 640x480. We computed the optical

Title Suppressed Due to Excessive Length

9

Gesture vocabulary

Gesture index Gesture description
G1
G2
G3
G4
G5
G6
G7
G8
G9
G10
G11
G12
G13
G14
G15

Reaching for needle with right hand
Positioning needle
Pushing needle through tissue
Transferring needle from left to right
Moving to center with needle in grip
Pulling suture with left hand
Pulling suture with right hand
Orienting needle
Using right hand to help tighten suture
Loosening more suture
Dropping suture at end and moving to end points
Reaching for needle with left hand
Making C loop around right hand
Reaching for suture with right hand
Pulling suture with both hands

Table 1: The gestures form a common vocabulary for small action segments that
reoccur in diﬀerent tasks.

ﬂow information of the clipped videos with the method suggested by Thomas et.al

[25] and transformed this information into RGB representation images. Although

there are 15 gestures in the gesture vocabulary deﬁned, there is no video data

available for the G7 : Pulling Suture with right hand, so we only have 14 gesture

action labels. Most studies on JIGSAWS dataset use the gestures through G1 to

G11 only [10, 12], resulting in 10 gestures, however, we experiment on all of the ges-

ture labels available. We rename our data of 14 gestures excluding the empty set of

G7. For the remaining of this paper we will address them as < G1, G2, ..., G14 >.

4 Material and Methods

Neural networks typically assume that all inputs are independent of each other.

However, with sequential tasks and time-series, in order to make a better pre-

diction, we should consider the previous computations and temporal dynamics

between the sequential inputs. Recurrent Neural Networks [19], feed-forward net-

works that unroll over time where each time step is equivalent to a layer, are able

to make use of sequential information. In these networks, the hidden unit activa-

10

Duygu Sarikaya et al.

tions of former inputs of a sequence feeds into the network along with the inputs,

making it possible for these networks to learn sequential, time-series models and

temporal dynamics. In RNNs, learning is done through back-propagation on the

unfolded network. This is called Back Propagation Through Time (BPTT). The

total cost function then, is the sum of the error function over time, and at each

time step, weights are adjusted according to this error rate. As with feed-forward

neural networks, vanishing gradients becomes a problem, and naturally, for the

Recurrent Neural Network, this problem leads to exponentially small gradients

through time. Recurrent neural networks are diﬃcult to train for this reason. In

our work, we speciﬁcally use Long Short Term Memory Networks (LSTM) [24],

a type of a Recurrent Neural Networks. LSTMs bring a solution to the vanishing

gradients problem by introducing the concept of a memory unit. LSTMs have a

gate mechanism that decides when to forget and when to remember hidden states

for future time steps. With this feature, LSTMs are able to train models on long

term dependencies.

LRCN introduced by Donahue et al. [17] proposes a model that incorporates

a deep hierarchical visual feature extractor and a model that can learn long-term

temporal dynamics. In this model, a deep hierarchical visual feature extractor,

speciﬁcally, a Convolutional Neural Network (CNN), takes each visual input xt

and extracts its features. This feature transformation of CNN activations produces

a ﬁxed length vector representation of the visual inputs: φV (xt) which are then

passed into a recurrent learning module based on an LSTM. This recurrent model

maps inputs to a sequence of hidden states ht and outputs yt. In order to predict

the distribution at time step t, we obtain a vector of class probabilities across our

vocabulary by applying softmax to a linear prediction layer on the outputs of the

sequential model.

In the LRCN model for activity recognition, the sequential visual inputs <

x1, x2, ..., xT > are ﬁrst individually processed with Convolutional Networks (CNN),

convolutional layer activations are then fed into a single-layer LSTM with 256

Title Suppressed Due to Excessive Length

11

hidden units that learns the frames' temporal dynamics. Predictions based on av-

eraging scores across all frames from a ﬁxed vocabulary of actions are made. In

their study, Donahue et al. [17] train two separate LRCN networks on RGB frames

and RGB representations of the ﬂow information in the same frames. Decisions

are made at a ﬁnal stage where the network predictions are averaged with ﬁxed

weights.

4.1 Our Model

We propose a novel architecture that is based on the principles of LRCN, how-

ever our arcchitecture supports multimodal learning and jointly learns temporal

dynamics on rich representations of visual features and motion cues. We use the

two modalities of the video as input: RGB frames and the RGB representation of

the optical ﬂow information in the same frames. We extract higher level features

of the individual frames using CNNs that we train on separate tasks. After we

convolve the two streams of input pairs, we concatenate the CNN features and use

it as an input to our recurrent model that learns temporal dynamics of consequent

frames. Our architecture simultaneously classiﬁes the common low-level gestures

and the surgical tasks with a multi-task learning approach. Please see Figure 1 for

an overview of our model.

We draw our input data of RGB and RGB optical ﬂow representations of

the same frames from a joint data layer and convolve both these inputs in parallel

convolutional neural network (CNN) streams. We concatenate the activations after

the 5th convolutional layers. Before pooling this fusion of representations, we apply

another convolutional layer and decrease the dimension of the representations. We

then apply a fully convolutional layer and feed its output to the LSTM layer

along with the sequence clip markers. In our model, we deﬁne two diﬀerent tasks

and losses, one for the 3 surgical task labels which are Knot Tying, Suturing and

Needle Passing and another for the 14 diﬀerent common low-level gesture labels

< G1, G2, G3, ...G14 >.

12

Duygu Sarikaya et al.

The convolutional body of our architecture is similar to the architecture pro-

posed by Zeiler and Fergus [26] and AlexNet [27] by Krizhevsky et al., with 5

convolutional layers, then another one that comes after fusing the activations of

these convolutional layers, followed by a fully convolutional layer. We ﬁrst train

two CNNs for a maximum of 40k iterations, one for each task; on RGB and ﬂow,

that classiﬁes the visual inputs based on individual frames without the notion of

temporal dynamics and recurrent networks. We initialize these networks by trans-

ferring weights from a pre-trained model on the 1.2M image ILSVRC-2012 dataset

[28] for a strong initialization and also to prevent overﬁtting. We then train two

LSTM networks, one for each task; on RGB and ﬂow, with the weights transferred

from the individual frame models. We train the ﬂow LSTM and RGB LTSM

networks for a maximum iterations of 60k. Finally, we deﬁne our joint model,

combining the weights from the convolutional and fully connected layers of the

two separate LSTM models we have trained, and transferring them to our model.

We perform stochastic gradient descent optimization. We set our learning rate at

0.001, weight decay at 0.005, and our learning policy to dynamically decrease the

learning rate by a factor of 0.1 for every 20k iterations, for all training models. We

train the LSTM network with a maximum iteration number of 90k. Additionally,

we set a threshold (15) based clipping gradient for the LSTM models. The weight

for both tasks are equal, so they both contribute to the prediction equally. With

time and resource concerns, we chose all mentioned hyperparameters based on our

experimental observations with a base of earlier works of Zeiler and Fergus [26],

Krizhevsky [27] and Donahue et al. [17]. Using grid-search, further optimization

of these hyperparameters could be possible.

We train our joint model end-to-end with video clips of length greater than

8 frames. We augment our data by clipping, mirroring, and cropping the frames

in order to prevent overﬁtting. At each time step, our model predicts both the

common low-level gesture and surgical task labels across our vocabulary of gestures

and tasks. In order to classify a whole segment of video, we average the predictions

Title Suppressed Due to Excessive Length

13

of each consequent frame for each task. The averaging is done to agree on a single

label for the whole video segment. To elaborate, we test our trained model on the

multiple clips of 8 frame length that we extract with a stride of 4 frames from

each video. In order to label the whole video segment, we average the predictions

of individual frames and then we average across all clips extracted from the video

segment.

5 Experiments and Evaluation

We evaluate our novel architecture on JIGSAWS dataset customized to our prob-

lem as discussed in the Dataset section. We create 6 splits for both the individual

frame and LSTM models. We ﬁrst create randomized lists of videos for training

and testing the LSTM models, and then we extract the frames of these videos and

randomize them again for the individual frame models. We train our model on a

ﬁxed random set of 1200 gesture video segments and use the rest 422 for testing.

This results in around 42, 000 gesture frames sampled for training and 14, 500 for

testing. The video segments have varying sizes from greater than just 8 frames to

535, however, most range around 10 to 90. We have set our clip frame length on

the base of the shorter videos, however, there is room for improvement for this

selection. During training we resize both RGB and ﬂow frames to 240x320 and

we augment the data by taking 227x227 crops and mirroring. We use a setting

of multiple 2.62GHz CPU processors and Geforce GTX1080 with computation

capability of 6.1.

Our experimental results show that our multi-modal and multi-task approach

is superior compared to an architecture that classiﬁes the surgical tasks and the

common low-level gestures separately on visual cues and motion cues respectively

(Table 2). While the conventional approach reaches a Mean Average Precision

(MAP) of only 29.13% , our architecture reaches a MAP of 50.83% for 3 tasks

and 14 possible gesture labels, resulting in an improvement of 22% (21.7%). We

14

Duygu Sarikaya et al.

Table 2: For a split of six experiment sets, accuracy for both the single-task, single-
modality model and our proposed model are shown. A video segment is deﬁned as
accurate only and only when both the gesture and surgical task labels are predicted
correctly.

Split Set

Accuracy for single-task,
single-modality model

Accuracy for
Joint Learning Model
(multi-task, multi-modal)

Diﬀerence

1

2

3

4

5

6

Average

26%

30%

29%

29%

30%

30%

29%

47%

54%

47%

47%

56%

55%

51%

21%

24%

18%

18%

26%

25%

22%

observed robust improvements of a median of 22.5%, ranging from 18% to 25.1%

for all six split experiments.

In training, every 5000k iterations take about 2 hours and 10 minutes. For

a video of frame size 30, the test time is around 7-8 seconds (excluding the

preprocessing time required to extract optical ﬂow, which takes a few seconds for

a generic image as recorded by Thomas et al. [25]). Our code does not take full

advantage of the GPU and parallelization. For a more eﬃcient study, our code

could be modiﬁed to fully take advantage of the GPU and parallel computation.

JIGSAWS dataset benchmark studies [12, 10] train and test on gestures that

only belong to one type of surgical task. They train on a set of gestures that occur

only during a speciﬁc surgical task (e.g. Suturing) and also test on this same task.

Their experimentation is limited as it trains models only on the gestures that

take place in a speciﬁc task and not the whole gesture vocabulary of 14 gestures.

Their Suturing experiments are done on 10 gesture labels, while Knot Tying and

Needle Passing are done on only 4 gestures. Our approach diﬀers greatly; we

focus on recognizing a speciﬁc gesture whether it takes place in Suturing or Knot

Tying. Therefore, our experiments are done on the whole dataset, across multiple

Title Suppressed Due to Excessive Length

15

surgical tasks and recognizes all 14 common low-level gesture labels in our gesture

vocabulary.

6 Conclusion

We propose a novel architecture as a solution to simultaneously classify common

low-level gestures and surgical tasks in Robot-assisted Surgery (RAS) video seg-

ments. Our architecture simultaneously classiﬁes the gestures and surgical tasks

with a multi-task learning approach by making use of joint relationships, combin-

ing shared and task-speciﬁc representations. Our architecture is multimodal and

uses two modalities of the video as input: RGB frames and the RGB representa-

tion of the optical ﬂow information in the same frames referring to motion cues.

After we convolve the two streams of input pairs, we concatenate the CNN features

that we extract and use this fusion of higher level representations as an input to

our recurrent model that learns temporal dynamics of the gestures and surgical

tasks. We focus on recognizing common low-level gestures even when they occur

across diﬀerent tasks. In this manner, it diﬀers greatly compared to the benchmark

studies on JIGSAWS [12, 10]. Our multi-modal and multi-task learning approach

is superior compared to an architecture that classiﬁes the tasks and the gestures

separately on visual cues and motion cues respectively.

Compliance with ethical standards

Conﬂict of interest The authors declare that they have no conﬂict of interest.

Ethical approval This article does not contain any studies with human partici-

pants or animals performed by any of the authors.

Informed consent This articles does not contain patient data.

References

1. Tao L, Elhamifar E, Khudanpur S, Hager GD, Vidal R (2012) Sparse hidden markov models

for surgical gesture classiﬁcation and skill evaluation. In: Proc. International Conference on

16

Duygu Sarikaya et al.

Information Processing in Computer-Assisted Interventions (IPCAI) 167-177.

2. Sarikaya D, Corso JJ, Guru KA (2017) Detection and localization of robotic tools in robot-

assisted surgery videos using deep neural networks for region proposal and detection. IEEE

Transactions on Medical Imaging 36(7):1542-1549

3. Gao Y, Vedula SS, Reiley CE, Ahmidi N, Varadarajan B, Lin HC, Tao L, Zappella L, Bejar

B, Yuh DD, Chen CCG, Vidal R, Khudanpur S, Hager GD (2014) The JHU-ISI gesture

and skill assessment working set (JIGSAWS): a surgical activity dataset for human motion

modeling. In: Proc. Modeling Monitor. Comput. Assist. Interventions (MCAI)

4. Lin HC, Shafran I, Murphy TE, Okamura AM, Yuh DD, Hager GD (2005) Automatic detec-

tion and segmentation of robot-assisted surgical motions. In: Proc. Medical Image Computing

and Computer-Assisted Intervention (MICCAI) 3749:802-810

5. Lin HC, Shafran I, Yuh D, Hager GD (2006) Towards automatic skill evaluation: detection

and segmentation of robot-assisted surgical motions. Computer Aided Surgery 11:220-230

6. Leong JJH, Nicolaou M, Atallah L, Mylonas GP, Darzi AW, Yang GZ (2006) HMM as-

sessment of quality of movement trajectory in laparoscopic surgery. In: Proc. Medical Image

Computing and Computer-Assisted Intervention (MICCAI) 4190:752-759

7. Yang GZ , Varadarajan B, Reiley C, Lin H, Khudanpur S, Hager G (2009) Data-derived

models for segmentation with application to surgical assessment and training. In: Proc.

Medical Image Computing and Computer-Assisted Intervention (MICCAI) 5761:426-434

8. Balakrishnan Varadarajan (2011) Learning and inference algorithms for dynamical system

models of dextrous motion. PhD thesis Johns Hopkins University

9. Cad´ene R, Robert T, Thome N, Cord M (2016) M2CAI workﬂow challenge: convolutional

neural networks with time smoothing and hidden markov model for video frames classiﬁca-

tion. Computing Research Repository (CoRR) abs:1610.05541

10. DiPietro R, Lea C, Malpani A, Ahmidi N, Vedula SS, Lee GI, Lee MR, Hager HG (2016)

Recognizing surgical activities with recurrent neural networks. In: Proc. Medical Image Com-

puting and Computer-Assisted Intervention (MICCAI) 551-558

11. Twinanda AP, Mutter D, Marescaux J, Mathelin M, Padoy N (2016) Single and multi-

task architectures for surgical workﬂow challenge. In: Proc. Workshop and Challenges on

Modeling and Monitoring of Computer Assisted Interventions (M2CAI) at Medical Image

Computing and Computer-Assisted Intervention (MICCAI)

12. Ahmidi N, Tao L, Sefati S, Gao Y, Lea C, Haro BB, Zappella L, Khudanpur S, Vidal R,

Hager GD (2017) A dataset and benchmarks for segmentation and recognition of gestures

in robotic surgery. Transaction of Biomedical Engineering

13. Meiner C, Meixensberger J, Pretschner A, Neumuth T (2014) Sensor-based surgical ac-

tivity recognition in unconstrained environments. Minimally Invasive Therapy and Allied

Technologies 23(4)

Title Suppressed Due to Excessive Length

17

14. Bardram JE, Doryab A, Jensen RM, Lange PM, Nielsen KLG, Petersen ST (2011) Phase

recognition during surgical procedures using embedded and body-worn Sensors. In: Proc.

IEEE International Conference on Pervasive Computing and Communications (PerCom)

45-53

15. Bouarfa L, Jonker PP, Dankelman J (2011) Discovery of high-level tasks in the operating

room. Journal of Biomedical Informatics 44(3)455:462

16. Simonyan K, Zisserman A (2014) Two-Stream Convolutional Networks for Action Recog-

nition in Videos. In: Proc. Neural Information Processing Systems (NIPS) 568-576

17. Donahue J, Hendricks LA, Rohrbach M, Venugopalan S, Guadarrama S, Saenko K, Darrell

T (2017) Long-Term Recurrent Convolutional Networks for Visual Recognition and Descrip-

tion. IEEE Transactions on Pattern Analysis and Machine Intelligence 39(4):677-691

18. Graves A, Jaitly N (2014) Towards End-To-End Speech Recognition with Recurrent Neural

Networks. In: Proc. International Conference on Machine Learning 1764-1772

19. Graves A (2013) Generating Sequences With Recurrent Neural Networks. Computing Re-

search Repository (CoRR) abs:1308.0850

20. Kokkinos I (2016) UberNet: Training a ’Universal’ Convolutional Neural Network for Low-

, Mid-, and High-Level Vision using Diverse Datasets and Limited Memory. Computing

Research Repository (CoRR), abs:1609.02132

21. Long M, Wang J (2015) Learning Multiple Tasks with Deep Relationship Networks. Com-

puting Research Repository (CoRR) abs:1506.02117

22. Yao J, Fidler S, Urtasun R (2012) Describing the scene as a whole: Joint object detec-

tion, scene classiﬁcation and semantic segmentation. In: Proc. Computer Vision and Pattern

Recognition (CVPR)

23. Misra I, Shrivastava A, Gupta A, Hebert M (2016) Cross-stitch Networks for Multi-Task

Learning. In: Proc. Computer Vision and Pattern Recognition (CVPR)

24. Hochreiter S, Schmidhuber J (1997) Long short-term memory. Neural Computation

9(8):1735-1780

25. Thomas B, Bruhn A, Papenberg N, Weickert J (2004) High accuracy optical ﬂow estima-

tion based on a theory for warping. In: Proc. Eur.Conf. Comput. Vis. (ECCV) 25-36

26. Zeiler MD and Fergus R (2014) Visualizing and understanding convolutional networks. In:

Proc. Eur. Conf. Comput. Vis. (ECCV) 818-833

27. Krizhevsky A, Sutskever I, Hinton GE (2012) ImageNet classiﬁcation with deep convolu-

tional neural networks. In: Proc. Adv. Neural Information Processing Systems (NIPS) 1-2

28. Russakovsky O, Deng J, Su H, Krause J, Satheesh S, Ma S, Huang Z, Karpathy A,

Khosla A, Bernstein M, Berg AC, Fei-Fei L (2015) ImageNet Large Scale Visual Recognition

Challenge. In: Proc. International Journal of Computer Vision (IJCV) 115(3)

