2
2
0
2

r
p
A
4
1

]
S
A
.
s
s
e
e
[

2
v
4
9
8
3
0
.
0
1
1
2
:
v
i
X
r
a

A Study of Low-Resource Speech Commands Recognition based on
Adversarial Reprogramming

Hao Yen1,2

Pin-Jui Ku1,2

Chao-Han Huck Yang1

Hu Hu1

Sabato Marco Siniscalchi1,3

Pin-Yu Chen4

Yu Tsao2

1Georgia Institute of Technology, USA
2Research Center for Information Technology Innovation, Academia Sinica, Taiwan
3Kore University of Enna, Italy and 4IBM Research, USA
rick.yen@gatech.edu, pku9@gatech.edu, huckiyang@gatech.edu, huhu@gatech.edu

Abstract

In this study, we propose a novel adversarial reprogramming
(AR) approach for low-resource spoken command recognition
(SCR) and build an AR-SCR system. The AR procedure aims
at repurposing a pretrained SCR model (from the source do-
main) to modify the acoustic signals (from the target domain).
To solve the label mismatches between source and target do-
mains and further improve the stability of AR, we propose a
novel similarity-based label mapping technique to align classes.
In addition, the transfer learning (TL) technique is combined
with the original AR process to improve the model adaptation
capability. We evaluate the proposed AR-SCR system on three
low-resource SCR datasets, including Arabic, Lithuanian, and
dysarthric Mandarin speech. Experimental results show that
with a pretrained AM trained on a large-scale English dataset,
the proposed AR-SCR system outperforms the current state-
of-the-art results on Lithuanian and Arabic speech commands
datasets, with only a limited amount of training data.
Index Terms: Low-resource speech processing, adversarial re-
programming, spoken command recognition, transfer learning,
label mapping.

1. Introduction

The aim of spoken command recognition (SCR) is to identify a
target command out of a set of predeﬁned candidates, based on
an input utterance [1, 2]. Owing to its wide applicability to var-
ious domains, such as smart home devices [3] or crime defec-
tions [4], SCR has long been an important research topic in the
speech processing ﬁeld [5, 6]. Due to recent advances of deep
learning (DL) algorithms, the performance of SCR systems has
been signiﬁcantly enhanced [7, 8, 9]. However, a common re-
quirement to build a high-performance DL-based SCR system
is to prepare a large amount of labeled training data (speech
utterances and corresponding transcriptions of the commands).
Such requirement is not always realizable in real-world scenar-
ios. In fact, it is generally favorable to build a SCR system with
only a limited amount of training data. Such a scenario is often
referred to as a low-resource training scenario [10].

Numerous algorithms have been developed to train DL-
based systems under low-resource scenarios. A well-known
category of approaches is transfer learning (TL) [11], which
aims to use a small amount of training data to ﬁne-tune a pre-
trained model, where the pretrained model is generally trained
on a large-scale dataset. Prior arts have demonstrated the ef-
fectiveness of TL in the speech processing tasks. For example,
in [12], an English acoustic model (AM) pretrained on a large-
scale training set is ﬁne-tuned with limited training data to ob-

Figure 1: Illustration of the proposed AR-SCR system. The
acoustic signals of a Lithuanian command (“ne”) is repro-
grammed to English commands (“nine” and “no”) and mapped
to its ﬁnal prediction with a pretrained English acoustic model.

tain a Spanish AM. Meanwhile in [13], a multilingual bottle-
neck feature extractor is pretrained on a large-scale training set
and ﬁne-tuned to form a keyword recognizer on a low-resourced
Luganda corpus. The study of [14] collected 200-million pieces
of 2-second data from Youtube to pretrain a speech embedding
model to extract useful features for a downstream keyword spot-
ting task. Although these TL approaches show promising re-
sults, the ﬁne-tuning process (which is often done in the online
mode) requires large training resources and thus is only feasi-
ble for applications where sufﬁcient computation resources are
available.

Another category of approaches is to adopt a pretrained
model to extract representative features to facilitate efﬁcient
and effective training for the SCR systems. The pretrained
model is generally trained on a large-scale dataset with ei-
ther a supervised or self-supervised training manner.
In [15]
and [16], the SCR systems were established by adopting rep-
resentative features extracted from pretrained sound event de-
tector and phone classiﬁer, respectively. Both were trained on
large-scale labeled data. Meanwhile, several methods adopt
self-supervised models, such as PASE+ [17, 18] , wav2vec [19],
and wav2vec 2.0 [20], as feature extractors to build the SCR
systems [21, 22, 23]. A notable drawback of this category of ap-
proaches is that an additional large-scale DL-based model (ac-
cordingly with increased hardware) is required.

Adversarial reprogramming (AR), as an alternative model
adaptation technique, has been conﬁrmed to provide satisfac-
tory results in numerous machine learning tasks [24]. In [25],
AR adopts a trainable layer to generate additive noise (e.g.,
as additional information) on input acoustic signals (source-
domain data) to guide an AM to recognize electrocardiography
(ECG) signals (target-domain data). Along with the success
of [25], our study investigates whether AR can be applied to
domain adaptation and accordingly building a SCR system in a
low-resource training scenario. Different from [26, 27], which

Reprogram LayerPretrained AMLabel MappingXtXt’YsYtLithuanian “ne”reprogrammed as English “no” source class probabilitiestarget class probabilitiestrainable noise θninenone 
 
 
 
 
 
Figure 2: Frameworks studied in this work. The ”AM” block
refers to acoustic model. In (a), the baseline system is trained
from scratch on the target-domain data. In (b), AM is pretrained
and then ﬁne-tuned on the target-domain data. In (c), AM is pre-
trained on source domain and then ﬁxed; a reprogram layer is
then placed before the pretrained AM model to modify the input
signals. In (d), we combine AR and TL to train the reprogram
layer and ﬁne-tune AM simultaneously.

perform acoustic model adaptation by passing input through
a neural network layer for transformation, AR aims to add a
trainable noise directly to the input sequence without modify-
ing the model. Fig. 1 shows the design concept of the proposed
AR-SCR system, which consists of a reprogram layer and a
pretrained AM. The reprogram layer ﬁrst generates trainable
noises, θ, to modify the original signals before passing them
into the pretrained AM. The AM will then output class prob-
abilities corresponding to the source classes. A label mapping
technique is adopted to map the probabilities of source classes
to the target class by aggregating probabilities over the assigned
source labels. Based on the aggregated probabilities, the repro-
gram layer is further trained to generate noises and thus modify
the input signals, so that the pretrained AM can be repurposed
to perform recognition in the target task.

The proposed AR-SCR system adopts two additional tech-
niques to further improve the model adaptation capability: (1) a
novel similarity-based label mapping strategy that aims to align
the target and source classes and (2) a ﬁne-tuning process that
adjusts the AM with the AR-generated signals. Experiment
results on three low-resource SCR datasets, including Arabic,
Lithuanian, and dysarthric Mandarin speech command datasets,
demonstrate that the proposed AR-SCR system can yield bet-
ter a performance than other state-of-the-art methods. In sum-
mary, the major contribution of the present work is twofold: 1)
This is the ﬁrst study that investigates the applicability of AR
to low-resource SCR tasks with promising results. 2) We verify
that AR has the ﬂexibility to combine with the TL technique to
achieve a better model adaptation performance.

2. The Proposed AR-SCR System

2.1. AR-SCR System

Fig. 2 illustrates the overﬂow of a SCR system with AR and TL
model adaptation techniques. In Fig. 2 (a), the AM is directly
trained by data from the target-domain task. When the train-
ing data from the target domain is limited, the model cannot be
trained well and thus may result in unsatisfactory recognition
performance. In Fig. 2 (b), the TL technique is applied on the
pretrained AM to establish a new SCR system that matches the
target domain. In Fig. 2 (c), the pretrained AM is ﬁxed, and a re-
program layer is trained to transform the input signals in order
to reduce the distance between the target and source distribu-
tions. In Fig. 2 (d), the reprogram layer is treated as a front-end
processor, and the TL technique is applied to further ﬁne-tune
AM with the reprogrammed signals. We expect the combina-
tion of AR (as a front-end processing) and TL (as a back-end

processing) can reach better model adaptation capability due to
their complementary abilities.

2.2. Acoustic Signal Reprogramming

The concept of AR was ﬁrst introduced in [24], and its aim was
to determine a trainable input transformation function H to re-
purpose a pretrained model from the source domain to a target
task. The authors in [24] showed that by the AR process, a pre-
trained ImageNet model trained on the image classiﬁcation task
can solve a square-counting task with high accuracy. A later
study [28] demonstrated that a reliable classiﬁcation system can
be established using AR and a black-box pretrained model with
scarce data and limited resources. Meanwhile, the Voice2Series
method [25] is proposed to transfer time series data (e.g., ECG
or Earthquake) xt, as the target domain, XT ⊆ RdT from the
source domain XS ⊆ Rds , where dT < dS . For these AR
approaches, a reprogrammed sample x(cid:48)
t can be formulated as:

x(cid:48)
t = H (xt; θ) := Pad (xt) + M (cid:12) θ
(cid:124) (cid:123)(cid:122) (cid:125)
δ

,

(1)

where Pad (xt) generates a zero-padded time series of dimen-
sion dS . The binary mask M ∈ {0, 1}dS indicates the indexes
that are not occupied and reprogrammable. θ ∈ RdS is a set of
trainable parameters for aligning source and target domain data
distributions. The term δ denotes the trainable additive input
transformation for reprogramming. In the original AR method,
the target sequence must be shorter than the source counterpart.
To overcome this limitation, we design to add trainable noises
to the whole sequence, and thus Eq. (1) becomes

x(cid:48)
t = H (xt; θ) := xt + θ.

(2)

In this work, we focus on applying AR as a model adapta-

tion technique to effectively ﬁne-tune a pretrained model.

2.3. Pretrained Acoustic Model

In the Voice2Series study [25], the authors have compared sev-
eral well-known AMs as pretrained models and provided the
ﬁrst theoretical justiﬁcations by optimal transport for repro-
gramming general time-series signals to acoustic signals. Based
on the provided justiﬁcations, in this study, we established AM
with two layers of fully convolutional neural networks, followed
by two bidirectional recurrent neural networks, which are then
combined with an attention layer. To train the model, we use the
Google Speech Commands dataset [29], which is a large-scale
collection of spoken command words, containing 105,829 ut-
terances of 35 words from 2,618 speakers; all the utterances are
recorded in a 16 kHz sampling-rate format. The pretrained AM
has 0.2M parameters and yields 96.90% recognition accuracy
rate on the testing set of Google Speech Commands.

2.4. Similarity Label Mapping

As illustrated in Fig. 1, a label mapping function is adopted
to map the probabilities of the source class to that of the tar-
get class. The results in [25] show that a many-to-one label
mapping strategy (randomly mapping multiple classes from the
source task to an arbitrary target class) yields a better perfor-
mance as compared to the one-to-one mapping strategy. In this
study, we attempt to improve the random mapping process with
a similarity mapping that considers the relationships of data
in the source and target domains. In [30], the structural rela-
tionships between acoustic scene classes are explored and uti-

Pretrained (Frozen)Non-TrainableLabel MappingTrainablePretrained (Fine-tune)(a) BaselineInputOutputAM(b) Transfer Learning (TL)InputOutputAM(d) Reprogram + Transfer Learning (AR+TL)InputOutputAMReprogramInputOutputAMReprogram(c) Reprogram (AR)Table 1: Testing results (average and std of the accuracy scores)
of three conditions (3, 10, and 20 training samples) for the
Lithuanian speech commands dataset. The results of the state-
of-the-art system [22] are also reported for comparison.

Limit

System Avg. Acc. (%) Rel. Imp. (%)

Std.

3

10

20

Baseline
AR
TL
AR+TL

Baseline
AR
TL
AR+TL

Baseline
AR
TL
AR+TL

33.8
25.1
52.8
58.9

63.7
34.4
80.7
81.9

70.3
34.6
86.8
88.6

–
-25.7
56.2
72.8

–
-46.0
26.7
28.6

–
-51.4
23.5
26

±8.82
±2.27
±7.69
±4.84

±5.35
±4.1
±5.65
±4.44

±6.8
±3.0
±2.92
±1.7

designed to allow dysarthric patients to control web browsers
via speech. By removing very long recordings, we select 13
short commands in our experiments, and the duration of each
command is around one second. We follow the setting in [33]
to split the whole dataset into 70% and 30% to form training
and testing tests.
Arabic Speech Commands: The Arabic speech commands
including 6 control
dataset [34] consists of 16 commands,
words and 10 digits (0 through 9), and each command have 100
samples, amounting 1600 utterances in total. 40 speakers are
involved in preparing the dataset. The speech utterances are
recorded at a sampling rate of 48 kHz and then converted into
16 kHz in our experiments. We follow the setting in [34] and
split the dataset into 80% for training and 20% for testing. In
addition, we randomly excerpt 20% of the training data to form
a validation set.

3.2. Experimental Results

For each of the three datasets, we compare the SCR results of
the four systems, as shown in Fig. 2. The results of the baseline
system (denoted as Baseline), stands for an AM trained from
scratch on the training set of the three SCR datasets. The re-
sults of the systems with AR and TL model adaptations (Fig. 2
(b) and (c)) are denoted as AR and TL, respectively. The re-
sults of combining the AR and TL methods, referred to Fig. 2
(d), are denoted as AR+TL. In our preliminary experiments, we
have tested the performance of several different setups. We
have compared the domains for signal reprogramming by di-
rectly modifying the input waveforms or modifying the input
spectral features. Meanwhile, we evaluated different label map-
ping techniques, i.e., one-to-one versus many-to-one, as well as
random mapping versus similarity mapping. In the following
discussions, we only report the results of the best setups. For
all the experiments in this study, we tested each SCR system
10 times and the average accuracy and standard deviation (std.)
values are reported in each table.

Table 1 lists the testing results of the Lithuanian speech
commands dataset. We follow the setting in [22] to conduct
experiments using different amounts of training data and re-
port the results of three conditions: each keyword has 3, 10,
and 20 training samples. From Table 1, we observe that TL

Figure 3: PCA plots of average representations of several
source-target pairs for (a) English-Lithuanian and (b) Emglish-
Arabic datasets. A target class (star point) is mapped to two or
three source classes (circle points) with higher cosine similarity
(marked with same colors).

lized to address the domain mismatch issue, while the authors
in [31] proposed an unsupervised learning method that maps
sentences in two different languages into the same latent space
for the Machine Translation task. Inspired by the prior art, we
propose to investigate the similarity of the labels between the
source and target domains and determine the optimal many-to-
one label mapping strategy. To compute the similarity, we ﬁrst
feed source and target data to the pretrained AM and calculate
the average representations of all classes and compute the co-
sine similarity between each of them. Based on the similarity
of classes, each target class is mapped to two or three source
classes in our AR-SCR system. Fig. 3 (a) and (b), respectively,
show the PCA plots of representation vectors for the English-
Lithuanian and English-Arabic datasets. Interestingly, we can
observe that command words with similar acoustic character-
istics are mapped to the same target word. For example, in
Fig. 3 (a), the source English source classes ”nine, no, learn”
are mapped to the target Lithuanian class ”ne”; whereas in Fig. 3
(b), the source English classes ”right, eight” are mapped to the
target Arabic class ”Takeed”. In our experiment, the AR system
with similarity mapping strategy outperforms the one with ran-
dom mapping [25] by increasing the average testing accuracy
by 2.1%, 13.4%, 9.2% on the Arabic, Lithuanian, and Mandarin
datasets respectively.

3. Experiments

3.1. Speech Commands Dataset

We use the Google Speech Commands dataset [29] as the
source domain data to pretrain our AM. Three low-resource
SCR datasets, including the Lithuanian, dysarthric Mandarin,
and Arabic speech command datasets, are used as the target do-
main tasks.
Lithuanian Speech Commands: The content of the Lithua-
nian speech commands dataset [22] is created by translating
20 keywords from Google Speech Commands [29] into the
Lithuanian language. The dataset consists of recordings from
28 speakers, with each speaker pronouncing 20 words on a mo-
bile phone. We follow the setting in [22] and [32], and chose
15 target classes: 13 command words, 1 unknown word, and 1
silent class. The resulting dataset consists of 326 recordings for
training, 75 for validation, and 88 for testing.
Dysarthric Mandarin Speech Commands: The dataset con-
tains 19 Mandarin commands, each uttered 10 times from 3
dysarthric patients [33], with 16kHz sampling rate. These 19
commands include 10 action commands and 9 digits, which are

(a) English-Lithuanian (b) English-Arabiccosine similarity label mapping★   target classsource classTable 2: Testing results (average and std of the accuracy scores)
of the dysarthric Mandarin dataset.

Table 3: Testing results (average and std of the accuracy scores)
of two conditions (3, and 10 training samples) for the Lithua-
nian speech commands dataset.

System Trainable para. Avg. Acc. (%) Rel. Imp. (%)

Std.

Baseline
AR
TL
AR+TL

200.4k
16k
200.4k
217.2k

64.0
33.2
78.1
82.3

–
-48.12
22.03
28.59

±16.43
±2.88
±3.18
±2.56

consistently outperforms Baseline for the three conditions. No-
tably, although AR alone cannot attain improved performance,
AR+TL can yield the average accuracy of 58.9%, 81.9%, and
88.6% for 3, 10, and 20 training samples conditions, respec-
tively, which are notably better than TL with the average ac-
curacy of 52.8%, 80.7%, and 86.7%. Moreover, the std values
in Table 1 show that AR can improve the performance stabil-
ity when combined with the TL technique. Fig. 4 also com-
pares our best AR+TL system with the state-of-the-art SCR sys-
tem [22], where the best-1 accuracy results are reported for the
3, 10, and 20 conditions. From the ﬁgure, AR+TL consistently
outperforms the state-of-the-art SCR system [22] on the Lithua-
nian speech commands dataset.

Table 2 shows the results of the dysarthric Mandarin
dataset. A similar observation can be obtained as those from
Table 1. First, TL yields an improved performance as compared
to Baseline. Next, although AR alone can not provide better
results than Baseline, AR+TL achieves the best performance
among the four systems. As compared to Baseline, AR+TL
yields notable improvements of 28.59% (from 64.0 to 82.3).
Moreover, the low std values of ±2.56 suggests that AR can
improve the stability for the SCR systems.

We also conducted experiments on the Arabic speech com-
mands dataset. From the testing results, we ﬁrst note that the
three adaptation systems (AR, TL, and AR+TL) all yield im-
proved accuracy rates with lower std as compared to Base-
line. Next, we note that AR+TL can achieve the highest ac-
curacy of 98.9%, which outperforms individual AR (94.8%)
and TL (98.6%). Based on our literature survey, AR+TL also
outperforms the state-of-the-art SCR system using LSTM [34]
(98.13%) on this Arabic speech commands dataset.

3.3. Ablation Study: Combining Adversarial Reprogram-
ming with Data Augmentation

As mentioned in Section 2, AR could be easily combined
with TL and achieve the highest performance. It is thus very

Figure 4: The best-1 accuracy values of Baseline, the state-of-
the-art wav2vec model [22], and the proposed AR+TL system
on the Lithuanian speech command dataset. We follow the same
evaluation metrics in [22] and only reported the best-1 test ac-
curacy values.

Limit

System

Avg. Acc. (%) Rel. Imp. (%)

Std.

3

10

Baseline+Aug
TL+Aug
AR+TL+Aug

Baseline+Aug
TL+Aug
AR+TL+Aug

39.5
65.9
70.2

65.3
81.7
87.4

-
66.8
77.7

-
25.1
33.8

±8.65
±4.52
±2.72

±8.80
±2.64
±2.63

natural assume that we can combine AR with another low-
resource training techniques and further improve the perfor-
mance of the whole system. To verify whether AR has such
ﬂexibility, we combine AR with Data Augmentation (Aug), an-
other standard approach when training DL-based systems with
In our experiments, we utilize
very limited amount of data.
SpecAugment [35], a common data augmentation method in
speech processing ﬁeld which randomly masks input features
by zero along both time and frequency axes. We dynamically
perform augmentation to generate additional data during train-
ing. We compare the results by combining data augmentation
with the Baseline, TL and AR+TL systems mentioned above.
We respectively denoted the three systems as Baseline+Aug,
TL+Aug, and AR+TL+Aug.

Table 3 shows the testing results of the Lithuanian Speech
command dataset when each keyword has 3 or 10 training sam-
ples. Compared with the results in Table 1, data augmenta-
tion can improve and stabilize the performance of the AR and
TL systems by a large margin. A similar results can be ob-
served as those from Table 1. First, TL+Aug improves the per-
formance by 66.8% and 33.0% as compared to Baseline+Aug.
Then, AR+TL+Aug again achieves the highest average accu-
racy among the three systems, with 70.2% and 87.4% accuracy,
which is notably better than TL+Aug (65.9% and 81.7%) and
Baseline+Aug (39.5% and 61.2%), under the conditions of 3
and 10 training samples. Moreover, the lower std values also
show that AR combined with data augmentation can improve
the stability of the systems. The results show that AR could
be successfully combining with data augmentation schemes and
further improves the SCR system under low-resource scenarios.

4. Conclusion

In this paper, we introduce the an AR approach to establish
a SCR system with a very limited amount of training data.
Experimental results show that the proposed AR-SCR system
can yield better performance as compared with state-of-the-art
SCR methods on the Lithuanian and Arabic speech command
datasets [34, 22]. The results also demonstrate that AR can ef-
fectively improve the accuracy over the baseline system with
few trainable parameters on the Arabic dataset. Meanwhile, we
also show AR has a great ﬂexibility to combine with different
low-resource training techniques, such as transfer learning and
data augmentation. In our future work, we will explore to fur-
ther improve the AR-SCR performance by combining with self-
supervised learning methods. Meanwhile, we will investigate to
the application of proposed AR approach to improve the model
safety [36, 37] for both classiﬁcation and regression tasks.
The codes used in this study will be released at https://
github.com/dodohow1011/SpeechAdvReprogram.

31020Number of samples per command020406080100Accuracy (%)50.077.680.656.978.590.866.388.891.8Baselinewav2vecAR+TL5. References
[1] T. Zeppenfeld and A.H. Waibel, “A hybrid neural network, dy-
namic programming word spotter,” in Proc. ICASSP, 1992.

[2] J.R. Rohlicek, W. Russell, S. Roukos, and H. Gish, “Continuous
hidden markov modeling for speaker-independent word spotting,”
in Proc. ICASSP, 1989.

[3] Sakshi Bajpai and D. Radha, “Smart phone as a controlling device
for smart home using speech recognition,” in Proc. ICCSP, 2019.

[4] H.P. Kavya and Veena Karjigi, “Sensitive keyword spotting for

crime analysis,” in Proc. NCCSN, 2014.

[5] Thilini Dinushika, Lakshika Kavmini, Pamoda Abeyawardhana,
Uthayasanker Thayasivam, and Sanath Jayasena, “Speech com-
mand classiﬁcation system for sinhala language based on auto-
matic speech recognition,” in Proc. IALP, 2019.

[6] Deokjin Seo, Heung-Seon Oh, and Yuchul Jung,

“Wav2kws:
Transfer learning from speech representations for keyword spot-
ting,” IEEE Access, pp. 1–1, 2021.

[7] Douglas Coimbra de Andrade, Sabato Leo, Martin Loesener
“A neural atten-
arXiv preprint

Da Silva Viana, and Christoph Bernkopf,
tion model for speech command recognition,”
arXiv:1808.08929, 2018.

[8] Somshubra Majumdar and Boris Ginsburg,

“Matchboxnet:
1d time-channel separable convolutional neural network archi-
arXiv preprint
tecture for speech commands recognition,”
arXiv:2004.08531, 2020.

[9] Roman Vygon and Nikolay Mikhaylovskiy, “Learning efﬁcient
arXiv

representations for keyword spotting with triplet loss,”
preprint arXiv:2101.04792, 2021.

[10] Laurent Besacier, Etienne Barnard, Alexey Karpov, and Tanja
Schultz, “Automatic speech recognition for under-resourced lan-
guages: A survey,” Speech Communication, vol. 56, pp. 85–100,
01 2014.

[11] Sinno Jialin Pan and Qiang Yang, “A survey on transfer learning,”
IEEE Transactions on Knowledge and Data Engineering, vol. 22,
no. 10, pp. 1345–1359, 2010.

[12] Ella Tetariy, Yossi Bar-Yosef, Vered Silber-Varod, Michal Gishri,
Ruthi Alon-Lavi, Vered Aharonson, and Ami Moyal,
“Cross-
language phoneme mapping for phonetic search keyword spotting
in continuous speech of under-resourced languages,” Artiﬁcial In-
telligence Research, vol. 4, no. 2, 2015.

[13] Raghav Menon, Herman Kamper, Ewald van der Westhuizen,
John Quinn, and Thomas Niesler, “Feature exploration for al-
most zero-resource asr-free keyword spotting using a multilingual
bottleneck extractor and correspondence autoencoders,” arXiv
preprint arXiv:1811.08284, 2019.

[14] James Lin, Kevin Kilgour, Dominik Roblek, and Matthew Shariﬁ,
“Training keyword spotters with limited and synthesized speech
data,” arXiv preprint arXiv:2002.01322, 2020.

[15] Brian McMahan and Delip Rao, “Listening to the world improves

speech command recognition,” in AAAI, 2018.

[16] Samuel Myer and Vikrant Singh Tomar,

spotting using time delay neural networks,”
arXiv:1807.04353, 2018.

“Efﬁcient keyword
arXiv preprint

[17] Santiago Pascual, Mirco Ravanelli, Joan Serra, Antonio Bona-
fonte, and Yoshua Bengio, “Learning problem-agnostic speech
arXiv
representations from multiple self-supervised tasks,”
preprint arXiv:1904.03416, 2019.

[18] Mirco Ravanelli, Jianyuan Zhong, Santiago Pascual, Pawel Swi-
etojanski, Joao Monteiro, Jan Trmal, and Yoshua Bengio, “Multi-
task self-supervised learning for robust speech recognition,” in
Proc. ICASSP, 2020.

[19] Steffen Schneider, Alexei Baevski, Ronan Collobert, and Michael
Auli, “wav2vec: Unsupervised pre-training for speech recogni-
tion,” in Proc. Interspeech, 2019.

[20] Alexei Baevski, Henry Zhou, Abdel rahman Mohamed, and
Michael Auli,
“wav2vec 2.0: A framework for self-
supervised learning of speech representations,” arXiv preprint
arXiv:2006.11477, 2020.

[21] Ashish Mittal, Samarth Bharadwaj, Shreya Khare, Saneem Chem-
mengath, Karthik Sankaranarayanan, and Brian Kingsbury, “Rep-
resentation based meta-learning for few-shot spoken intent recog-
nition,” arXiv preprint arXiv:2106.15238, 2021.

[22] Aliaksei Kolesau and Dmitrij ˇSeˇsok, “Unsupervised pre-training

for voice activation,” Applied Sciences, 2020.

[23] Deokjin Seo, Heung-Seon Oh, and Yuchul Jung,

“Wav2kws:
Transfer learning from speech representations for keyword spot-
ting,” IEEE Access, vol. 9, pp. 80682–80691, 2021.

[24] Gamaleldin F. Elsayed,

Ian Goodfellow, and Jascha Sohl-
Dickstein, “Adversarial reprogramming of neural networks,” in
Proc. ICLR, 2019.

[25] Chao-Han Huck Yang, Yun-Yun Tsai, and Pin-Yu Chen,
“Voice2series: Reprogramming acoustic models for time series
classiﬁcation,” in Proceedings of the 38th International Confer-
ence on Machine Learning. 2021, pp. 11808–11819, PMLR.

[26] J. Neto, L. Almeida, M. Hochberg, C. Martins, Nunes L., S. Re-
nals, and T. Robinson, “Speaker-adaptation for hybrid hmm-ann
continuous speech recognition system,” in Fourth European Con-
ference on Speech Communication and Technology, 1995.

[27] Kaisheng Yao, Dong Yu, Frank Seide, Hang Su, Li Deng, and
Yifan Gong, “Adaptation of context-dependent deep neural net-
works for automatic speech recognition,” in 2012 IEEE Spoken
Language Technology Workshop (SLT), 2012, pp. 366–369.

[28] Yun-Yun Tsai, Pin-Yu Chen, and Tsung-Yi Ho, “Transfer learning
without knowing: Reprogramming black-box machine learning
models with scarce data and limited resources,” in International
Conference on Machine Learning. PMLR, 2020, pp. 9614–9624.

[29] Pete Warden,

“Speech commands:

limited-vocabulary speech recognition,”
arXiv:1804.03209, 2018.

A dataset
for
arXiv preprint

[30] Hu Hu, Sabato Marco Siniscalchi, Yannan Wang, and Chin-Hui
Lee, “Relational teacher student learning with neural label embed-
ding for device adaptation in acoustic scene classiﬁcation,” arXiv
preprint arXiv:2008.00110, 2020.

[31] Guillaume Lample, Alexis Conneau, Ludovic Denoyer, and
Marc’Aurelio Ranzato, “Unsupervised machine translation us-
ing monolingual corpora only,” arXiv preprint arXiv:1711.00043,
2017.

[32] Aliaksei Kolesau and Dmitrij ˇSeˇsok, “Voice activation for low-
resource languages,” Applied Sciences, vol. 11, no. 14, 2021.

[33] Yu-Yi Lin, Wei-Zhong Zheng, Wei Chung Chu, Ji-Yan Han,
Ying-Hsiu Hung, Guan-Min Ho, Chia-Yuan Chang, and Ying-Hui
Lai, “A speech command control-based recognition system for
dysarthric patients based on deep learning technology,” Applied
Sciences, vol. 11, no. 6, 2021.

[34] Lina Benamer and Osama Alkishriwo,

“Database for arabic

speech commands recognition,” in Proc. CEST, 2020.

[35] Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu,
Barret Zoph, Ekin D. Cubuk, and Quoc V. Le, “Specaugment: A
simple data augmentation method for automatic speech recogni-
tion,” in Proc. Interspeech, 2019.

[36] Chao-Han Yang, Jun Qi, Pin-Yu Chen, Xiaoli Ma, and Chin-Hui
“Characterizing speech adversarial examples using self-

Lee,
attention u-net enhancement,” in Proc. ICASSP, 2020.

[37] Chao-Han Huck Yang, Sabato Marco Siniscalchi, and Chin-Hui
Lee, “PATE-AAE: Incorporating Adversarial Autoencoder into
Private Aggregation of Teacher Ensembles for Spoken Command
Classiﬁcation,” in Proc. Interspeech 2021, 2021, pp. 881–885.

