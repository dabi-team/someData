VARA-TTS:Non-AutoregressiveText-to-SpeechSynthesisbasedonVeryDeepVAEwithResidualAttentionPengLiu1YuewenCao*2SongxiangLiu*2NaHu1GuangzhiLi1ChaoWeng1DanSu1AbstractThispaperproposesVARA-TTS1,anon-autoregressive(non-AR)end-to-endtext-to-speech(TTS)modelusingaverydeepVariationalAutoencoder(VDVAE)withResidualAttentionmechanism,whichreﬁnesthetextual-to-acousticalignmentlayer-wisely.Hierarchicallatentvari-ableswithdifferenttemporalresolutionsfromtheVDVAEareusedasqueriesfortheresid-ualattentionmodule.Byleveragingthecoarseglobalalignmentfrompreviousattentionlayerasanextrainput,thefollowingattentionlayercanproduceareﬁnedversionofalignment.Thisamortizestheburdenoflearningthetextual-to-acousticalignmentamongmultipleattentionlay-ersandoutperformstheuseofonlyasingleat-tentionlayerinrobustness.Anutterance-levelspeakingspeedfactoriscomputedbyajointly-trainedspeakingspeedpredictor,whichtakesthemean-pooledlatentvariablesofthecoarsestlayerasinput,todeterminenumberofacousticframesatinference.ExperimentalresultsshowthatVARA-TTSachievesslightlyinferiorspeechqualitytoanARcounterpartTacotron2butanorder-of-magnitudespeed-upatinference;andoutperformsananalogousnon-ARmodel,BVAE-TTS,intermsofspeechquality.1.IntroductionInrecentyears,text-to-speech(TTS)synthesistechnologieshavemaderapidprogresswithdeeplearningtechniques.Previousattention-basedencoder-decoderTTSmodelshaveachievedstate-of-the-artresultsinspeechqualityandintel-ligibility(Wangetal.,2017;Shenetal.,2018;Gibianskyetal.,2017;Pingetal.,2018b;Lietal.,2019).Theyﬁrst*WorkdoneduringinternshipatTencentAILab.1TencentAILab2Human-ComputerCommunicationsLaboratory,TheChi-neseUniversityofHongKong.Correspondenceto:PengLiu<laupeng1989@gmail.com>.1Codeswillbereleasedsoon.generateacousticfeature(e.g.,mel-spectrogram)autore-gressivelyfromtextinputusinganattentionmechanism,andthensynthesizeaudiosamplesfromtheacousticfeaturewithaneuralvocoder(Oordetal.,2016;Kalchbrenneretal.,2018).However,theautoregressive(AR)structurehastwomajorlimitations:(1)itgreatlylimitstheinferencespeed,sincetheinferencetimeofARmodelsgrowslinearlywiththeoutputlength;(2)theARmodelsusuallysufferfromrobustnessissues,e.g.,wordskippingandrepeating,duetoaccumulatedpredictionerror.ToavoidtheaforementionedlimitationsoftheARTTSmod-els,researchershaveproposedvariousnon-autoregressive(non-AR)TTSmodels(Renetal.,2019;Pengetal.,2020;Renetal.,2020;Kimetal.,2020;Miaoetal.,2020;Leeetal.,2021).ThesemodelscansynthesizeacousticfeatureswithsigniﬁcantlyfasterspeedthanARmodelsandreducerobustnessissues,whileachievingcomparablespeechqual-itytotheirARcounterparts.However,thesemodelsusuallycontainaseparatedurationmodulethatdoesnotpropagateinformationtotheacousticmodule,whichmayleadtothetraining-inferencemismatchissue.Moreover,thedurationmoduleneedsdurationlabelsassupervision,whichmaycomefrompre-trainedARTTSmodels(Renetal.,2019;Pengetal.,2020),forcedaligner(Renetal.,2020),dy-namicprogrammingbacktrackpath(Kimetal.,2020)orjointly-trainedattentionmodules(Miaoetal.,2020;Leeetal.,2021).Inattention-basedencoder-decoderTTSmodels,thekeysandvaluesarecalculatedfromtext,whilethequeriesaredifferentlyconstructedinvariousmodels.ForARTTSmodelslikeTacotron(Wangetal.,2017),thequeriesareconstructedfromtheARhiddenstates.ItisanaturalchoicesincetheARhiddenstatescontaintheacousticinformationuptothecurrentframe.Fornon-ARTTSmodelslikeFLOW-TTS(Miaoetal.,2020),onlypositioninformationisusedasquery.ForBVAE-TTS,thequeryisconstructedformVAElatentvariables.Ifthecorrelationbetweenthequeriesandkeysarenotstrongenough,itisdifﬁcultforattentionmoduletolearnthealignmentbetweenqueriesandkeys.Therefore,thekeytobuildanon-ARTTSmodelisconstructingthequeriesthatarehighlycorrelatedtokeysandalsoenableparallelattentioncalculation.arXiv:2102.06431v1  [cs.SD]  12 Feb 2021VARA-TTS:Non-AutoregressiveText-to-SpeechSynthesisbasedonVeryDeepVAEwithResidualAttentionThecorrespondingtexttranscriptioncanbeconsideredasalossycompressionoftheacousticfeatures.Meanwhile,thelatentvariablesfromhierarchicalvariationalautoencoder(VAE)arealsolossycompressionsofacousticfeatureswhentrainedonacousticdata.Theselatentvariablesmaybecor-relatedwiththetextandsuitabletoconstructthequeries.Moreover,hierarchicalVAEprovideslatentvariablesofdif-ferentresolutions,whichcanbeusedasthequeriestobuildacoarse-to-ﬁnelayer-wisereﬁnedresidualattentionmodule.Speciﬁcally,theattentionlayercanusethealignmentpro-ducedbythepreviouslayerasanextrainputandproduceareﬁnedversionofthealignment.Inthisway,theburdenofpredictingthetextual-to-acousticalignmentcouldbere-ducedandamortizedbythecoarse-to-ﬁneattentionlayers.Fortheﬁstattentionlayer,wecanusea“nearlydiagonal”initialalignmentasagoodinstructivebias,consideringthemonotonicpropertiesoftextandacousticfeatures.Inthiswork,wepresentanovelnon-ARTTSmodelbasedonaspeciﬁchierarchicalVAEmodelcalledVeryDeepVAE(VDVAE)andacoarse-to-ﬁnelayer-wisereﬁnedresidualattentionmodule.Ourmaincontributionsareasfollows:•WeadoptVDVAEtomodelmel-spectrogramswithbottom-upandtop-downpaths.Thehierarchicallatentvariablesserveasqueriestotheattentionmodules.•Weproposeanovelresidualattentionmechanismthatlearnslayer-wisealignmentsfromcoarsegranularitytoﬁnegranularityinthetop-downpaths.•WeproposedetailedKullback-Leibler(KL)gainforVDVAEtoavoidposteriorcollapseinsomehierarchi-callayers.•Theproposedmodelisfullyparallelandtrainedinanend-to-endmanner,whileobtaininghighspeechquality.Inferencespeedoftheproposedmodelis16timesfasterthantheARmodel,Tacotron2,onasingleNVIDIAGeForceRTX2080TiGPU.Also,ourpro-posedmodeloutperformsananalogousnon-ARmodel,BVAE-TTS,intermsofspeechquality.Therestofthepaperisorganizedasfollows.Section2discussestherelatedwork.TheVAEsareintroducedinSection3.WepresentthemodelarchitectureinSection4.ExperimentalresultsandanalysesarereportedinSection5andSection6.TheconclusionisdrawninSection7.2.RelatedWork2.1.Text-to-SpeechModelsThegoaloftext-to-speech(TTS)synthesisistoconvertaninputtextsequenceintoanintelligibleandnatural-soundingspeechutterance.Mostpreviousworkdividesthetaskintotwosteps.Theﬁrststepistext-to-acoustic(e.g.mel-spectrograms)modeling.Tacotron1&2(Wangetal.,2017;Shenetal.,2018),DeepVoice2&3(Gibianskyetal.,2017;Pingetal.,2018b),TransformerTTS(Lietal.,2019)andFlowtron(Valleetal.,2020)areARmodelsamongthebestperformingTTSmodels.Thesemodelsemployanencoder-decoderframeworkwithattentionmechanism,wheretheencoderconverttheinputtextsequencetohid-denrepresentationsandthedecodertakesaweightedsumofthehiddenrepresentationstogeneratetheoutputacous-ticfeaturesframebyframe.Itischallengingtolearnthealignmentbetweentextsequenceandacousticfeatures(e.g.mel-spectrograms)forTTSmodels.Variousattentionmech-anismsareproposedtoimprovethestabilityandmonotonic-ityofthealignmentinARmodels,suchaslocation-sensitiveattention(Chorowskietal.,2015),forwardattention(Zhangetal.,2018),multi-headattention(Lietal.,2019),step-wisemonotonicattention(Heetal.,2019)location-relativeattention(Battenbergetal.,2020).However,thelowinfer-enceefﬁciencyofARmodelshinderstheirapplicationinreal-timeservices.Recently,non-ARmodelsareproposedtosynthesizetheoutputinparallel.Thekeytodesignanon-ARacousticmodelistheparallelalignmentprediction.ParaNet(Pengetal.,2020)alsoadoptsalayer-wisereﬁnedattentionmechanism,wherethequeriesareonlypositionalencodingintheﬁrstattentionlayerandpreviousattentionlayeroutputprocessedbyaconvolutionblockinthefollow-ingattentionlayers.However,attentiondistillationfrompre-trainedARTTSmodelarestillneededtoguidethetrainingofalignments.Fastspeech(Renetal.,2019)alsorequiresknowledgedistillationfrompre-trainedARTTSmodeltolearnalignments,whileFastspeech2bypassestherequirementofteachermodelthroughanexternalforcealignerfordurationlabels(Renetal.,2020).Glow-TTS(Kimetal.,2020)andFlow-TTS(Miaoetal.,2020)arebothﬂow-basednon-ARTTSmodels.Glow-TTSenforceshardmonotonicalignmentsthroughthepropertiesofﬂowsanddynamicprogramming.Flow-TTSadoptspositionalatten-tiontolearnthealignmentduringtraininganduseslengthpredictortopredictspectrogramlengthduringinference.Thesecondstepisacousticfeaturestotime-domainwave-formsamplesmodeling.WaveNet(Oordetal.,2016)istheﬁrstoftheseARneuralvocoders,whichproducedhighqualityaudio.SinceWaveNetinferenceiscomputation-allychallenging.SeveralARmodelsareproposedtoim-provetheinferencespeedwhileretainingquality(Ariketal.,2017;Jinetal.,2018;Kalchbrenneretal.,2018).Non-ARvocodershavealsoattractedincreasingresearchinterest(Oordetal.,2018;Pingetal.,2018a;Prengeretal.,2019;Yamamotoetal.,2020),whichgeneratehighﬁdelityspeechmuchfasterthanreal-time.Recently,end-to-endgenerationofaudiosamplesfromtextsequencehasbeenproposedin(Donahueetal.,2020;RenVARA-TTS:Non-AutoregressiveText-to-SpeechSynthesisbasedonVeryDeepVAEwithResidualAttentionetal.,2020;Weissetal.,2020).Wave-TacotronextendsTacotronbyincorporatinganormalizingﬂowintotheARdeocoderloop.BothEATS(Weissetal.,2020)andFast-speech2s(Renetal.,2020)arenon-ARmodels,wherevar-iousadversarialfeedbacksandauxiliarypredictionlossesareusedrespectively.2.2.VAE-basedGenerativeModelsVAE(Kingma&Welling,2013;Rezendeetal.,2014)isawidelyusedgenerativemodel.BothvariationalRNN(VRNN)(Chungetal.,2015)andvectorquantised-VAE(VQ-VAE)(VanDenOordetal.,2017)adoptARstructuretomodelthegenerativeprocessofaudiosamples.(Child,2020)veriﬁesthatVDVAEoutperformstheARmodelPix-elCNN(VandenOordetal.,2016)inlog-likelihoodonallnaturalimagebenchmarks,whileusingfewerparametersandgeneratingsamplesthousandsoftimesfaster.InspiredbythesuccessoftheVDVAEarchitecture,weemployitforparallelspeechsynthesistask.Inparalleltoourwork,BVAE-TTS(Leeetal.,2021)hasbeenproposed,wherebidirectionalhierarchicalVAEarchi-tectureisalsoadoptedfornon-ARTTS.However,onlyla-tentvariablesfromthetoppestlayerareusedasqueriesandthereisagapbetweentheattention-basedmel-spectrogramgenerationduringtrainingandtheduration-basedmel-spectrogramgenerationduringinferenceinBVAE-TTS.Therefore,variousempiricalandcarefully-designedtech-niquesareneededtobridgethisgap.Byemployingaresid-ualattentionmechanism,ourproposedmodelcaneliminatethistrainingandinferencemismatch.3.VariationalautoencodersVAEsconsistofthefollowingparts:ageneratorp(x|z),apriorp(z)andaposteriorq(z|x)approximator.Typically,theposteriorsandpriorsinVAEsareassumednormallydistributedwithdiagonalcovariance,whichallowsfortheGaussianreparameterizationtricktobeused(Kingma&Welling,2013).Thegeneratorp(x|z)andapproximatorq(z|x)arejointlytrainedbymaximizingtheevidencelowerbound(ELBO):logp(x)≥Ez∼q(z|x)logp(x|z)−KL[q(z|x)kp(z)](1)wheretheﬁrsttermoftherighthandsideofthisinequalitycanbeseenastheexpectationofnegativereconstructionerrorandthesecondKLdivergencetermcanbeseenasaregularizer.HierarchicalVAEscangainbettergenerativeperformancethanpreviousVAEswherefully-factorizedposteriorsandpriorsareincorporated.OnetypicalhierarchicalVAEisintroducedin(Sønderbyetal.,2016),whereboththepriorp(z)andposteriorapproximatorq(z|x)areconditionallydependent:p(z)=p(z0)p(z1|z0)...p(zN|z<N)(2)q(z|x)=q(z0|x)q(z1|z0,x)...q(zN|z<N,x)(3)whereNisthenumberofhierarchicalgroups.Thishierar-chicalVAEﬁrstextractslatentvariablesfrominputdataxalongthebottom-uppath,thenprocessesthelatentvariablesalongthetop-downpathtogeneratex.Theequation(1)ischangedasfollows:logp(x)≥Ez∼q(z|x)logp(x|z)−KL[q(z0|x)kp(z0)]−NXn=1Eq(z<n|x)[KL[q(zn|z<n,x)kp(zn|z<n))]](4)whereq(z<n|x):=Qn−1i=1q(zi|z<i,x)istheapproxi-mateposterioruptothe(n−1)thgroup.4.ModelArchitectureWeadopttheVDVAE(Child,2020)withanovelresidualattentionmechanismfornon-ARTTS.Theoverallarchi-tectureisshowninFigure1.Hierarchicallatentvariablesatdecreasingtimescaleareextractedfromtheinputmel-spectrogramsalongthebottom-uppath.Thesehierarchicallatentvariablesareprocessedfromtoptobottomandservedasqueries(Q).AtextencodertakesphonemesequenceandoptionalspeakerIDasinputandoutputstextencodingaskey(K)andvalue(V).Q,K,Vandattentionweightfrompreviousattentionblock(Aprev)aresenttothefollow-ingresidualattentionmoduletoproduceareﬁnedattentionweightandcontextvector.ThecontextvectorisaweightedaverageofV,andispassedtotop-downblockforvaria-tionalinferenceandmel-spectrogramreconstructionalongwithstoredhierarchicallatentvariables.Aspeakingspeedpredictortakesthemean-pooledlatentvariablesofthecoars-estlayerasinputandpredictstheutterance-levelaveragespeakingspeedfactortodeterminethenumberofacousticframesatinference.Weusetheβ-VAEtrainingobjectiveandamel-spectrogramlossinspiredby(Rezende&Viola,2018).ThehierarchicalVAE,speakingspeedpredictor,textencoderandresidualattentionmodulesarejointlytrainedwiththefollowingobjective:L=αLspeakingspeed+Lrecon+βLKL(5)where,Lspeakingspeed=E(cid:16)d−ˆd(cid:17),(6)VARA-TTS:Non-AutoregressiveText-to-SpeechSynthesisbasedonVeryDeepVAEwithResidualAttentionres blockres blockres blockpoolpoolunpooltopdown blocktopdown blocktopdown blockunpoolBottom-up pathTop-down pathText EncoderInitialAttentionResidualAttentionResidualAttentionPhonemesSpeakingspeedpredictorSpeakerIDmean poolrepeatmean poolpreviousblock outputq(zi|x,z<i,t)p(zi|z<i,t)zres blockfrombottom-up4 convlayersconcat4 convlayersconv layercontextpreviousblock output4 convlayersA0An-1K, VAnQQFigure1.OverallmodelarchitectureLrecon=kx−ˆxk2kxk2+klogx−logˆxk1,(7)LKL=KL[q(z0|x,t)kp(z0,t)]+NXn=1Eq(z<n|x,t)[KL[q(zn|x,z<n,t)kp(zn|z<n,t)]],(8)andα,βareweightsofspeakingrateloss,KLlossandrespectively.d,ˆd,x,ˆx,tandzdenoteground-truthspeakingspeed,predictedspeakingspeed,ground-truthmel-spectrogrammagnitudes,predictedmel-spectrogrammag-nitudes,phonemesequences,andlatentvariablesrespec-tively.WeintroducedetailedimplementationsofVDVAE,textencoder,speakingratepredictorandresidualattentionmodulesinthefollowingsubsections.4.1.VerydeepVAEAsshowinthecentralpartofFigure1,theverydeepVAEisstackedofresidualblockgroupsalongthebottom-uppathandtopdownblockgroupsalongthetop-downpath.Thetimedimensionofinputmel-spectrogramisreducedatthepoolinglayerofeachbottom-upgroupusingaveragepool-ing.TheresidualblockcontainsfourconvolutionlayersandaresidualconnectionasshownintheleftpartofFigure1.EachconvolutionoutputisprecededbytheGELUnonlin-earity(Hendrycks&Gimpel,2016).Thetopdownblocktakesbottom-upoutput,previoustopdownblockoutputandcontextvectorfromresidualattentionmoduleasinput.Theprevioustopdownblockoutputisunpooledatthebottomlayerofeachtop-downgroupusingnearest-neighborupsam-plingalongtemporalaxis,andthenaddedtocontextvectorasblockbiasforpriorp(·)andtheposteriorapproximatorq(·).Thebottom-upgroupoutputisconcatenatedwiththisblockbiasandprocessedbyfourconvolutionlayerstopredictq(zi|x,z<i,t).Anotherpathofcontextvectorisusedtopredictthepriorp(zi|z<i,t)throughanotherfourconvolutionlayers.Bothq(·)andp(·)areisotropicGaussiandistributions.Intrainingstage,zissampledfromq(·).Ininferencestage,thebottom-uppathisdiscardedandzissampledfromp(·).zpassesaconvolutionlayerandisaddedtothesummationoftheblockbiasforposteriorapproximatorandtheoutputofthefourconvolutionlayers,beforebeingsenttoaresidualblock.Theresidualblockcontainsthreeconvolutionlayers,onedilatedconvolutionlayerandaresidualconnection.zisalsousedasthequeryinthesucceedingresidualattentionmodule.AlthoughresidualconnectionsinVDVAEareabletoalle-viateposteriorcollapsetosomedegree,weobservesomehorizontalsegmentsinthecumulativeKLcurveasshowninFigure3.Thisindicatesthatposteriorcollapseoccursintheselayersandlatentvariablesfromtheselayersen-codesmallamountofinformation.Therefore,weproposeadetailedKLgainmechanism,inspiredby(Alemietal.,2018;Burgessetal.,2018).InhierarchicalVAE,theKLVARA-TTS:Non-AutoregressiveText-to-SpeechSynthesisbasedonVeryDeepVAEwithResidualAttentiontermistheaccumulationoftheKLtermsfromdifferenthierarchicallayers.WecansetareferencevalueforeachlayerliketheKLreferencetermin(Alemietal.,2018):KLref=cNNXi=1KLi(9)LdetailedKLgain=NXi=1|max(KLi,KLref)−KLref|(10)wherethereferenceKLrefistheaveragevalueofKLtermsfromdifferenthierarchicallayersmultipliedbyaconstantKLgainfactorc(c=0.5isusedinthispaper).WhenKLiislargerthanthereference,theKLgaintermis0.WhenKLiissmallerthanthereference,thegaintermisnon-zeroandthegradientofthistermwouldenlargetheKLi,thentheineffectivelatentvariableswouldbeeliminated.Therefore,thetrainingobjectiveofthewholemodelismodiﬁedasfollows:L=αLspeakingspeed+Lrecon+βLKL+λLdetailedKLgain(11)whereλistheweightofthedetailedKLgain.4.2.SpeakingspeedpredictorSincenumberofmel-spectrogamframestogenerateduringinferenceisunknownapriori,wetrainanauxiliaryspeakingspeedpredictor.Atwo-dimensionalmel-spectrogramisre-ducedintime-scalehierarchicallyalongthebottom-uppathandeventuallytoavectorx0byatemporalglobalmeanpoolingoperation;anothervectort0isobtainedfromthetextencoderoutputbytemporalglobalmeanpooling.x0andt0areusedtocalculatethepriorandposteriordistribu-tionsforthetop-mostlatentvariablesz0.Wesamplealatentvariablevectorz0fromthelatentspace,whichisfedintothespeakingspeedpredictorasinput.Duringtraining,z0issampledfromtheposteriorq(z0|x0,t0),whileatinference,z0issampledfromthepriorp(z0|t0).AccordingtotheInfoGANderivation(Chenetal.,2016),minimizingthepredictionerrorforspeakingspeedfromthetop-mostlatentvariablespavesawaytomaximizingalowerboundoftheirmutualinformation(proofinAppendixA).Ifthespeakingspeedpredictoriswell-trained,latentvariablesfromtheﬁrsttop-downgroup(i.e.,thetop-mostlatentvariablez0)wouldencoderichspeakingspeedin-formation.Sincelatentvariables(thez’s)inVARA-TTSareconditionallydependentinachainfromtop-to-bottom,theyalsocontainspeakingspeedinformation,whichfacili-tatesthealignmentlearningattheircorrespondingattentionmodules.Thisalleviatestheproblemofinconsistencyofacousticmodelandaseparatelytraineddurationmodel.Forexample,thedurationmoduleofBVAE-TTSdoesnotprop-agateinformationtotheacousticmoduleandonlyusesthePhonemesequenceSpeaker IDText embedding tableFClayers4 convlayersPositional encodingFeature wise affineSpeaker embedding tableFiLM blockFigure2.Networkdetailoftextencoderalignmentobtainedfromtheacousticmoduleandtexten-codingmoduleassupervision.Thismayleadtotheissuethattheacousticmoduleisnotadaptedwelltothedurationmodule.InVARA-TTS,thespeakingspeedpredictoradoptstwofully-connected(FC)layers,betweenwhichweaddaReLUactivationfunction,alayer-normalizationlayerandadropoutlayerinasequence(moredetailsarepresentedinAppendixD).Unlikemostexistingnon-ARTTSmodels,whichrequiretoken-leveldurationinformationassupervi-sionfordurationmodeltraining,weuseareadily-computedspeakingratedforeachtrainingutteranceasthetargetofthespeakingspeedpredictor:d=Normalizemin-max(TmelLtext)∈[0,1],(12)whereNormalizemin-max(·)denotesmin-maxnormaliza-tionacrossthetrainingset,TmelrepresentsthenumberofframesinamelspectrogramandLtextisthenumberofto-kensinaphonemesequence.Weaddasigmoidfunctiononthespeakingspeedpredictoroutputtoobtainthepredictedspeakingrateˆd.AnMSElossappliedondandˆdisusedastrainingsignalforthespeakingspeedpredictor.4.3.TextencoderAsshowninFig2.phonemesequenceandspeakeridaretransformedtophonemeembeddingandspeakerembeddingthroughtwolookuptables.Thefeature-wiselinearmodu-lation(FiLM)(Perezetal.,2018)isusedtofusespeakerembeddingwithphonemeembedding.TwoFClayersareusedtocomputethescaleandshiftvectorsfromthespeakerembeddingvector,respectively.Thefeature-wiseafﬁneoperationisconductedas:γspk×Uphn+ξspk(13)VARA-TTS:Non-AutoregressiveText-to-SpeechSynthesisbasedonVeryDeepVAEwithResidualAttentionwhereγspkandξspkrepresentthescaleandshiftvectorsandUphnrepresentsphonemeembedding.Thefusedoutputpassesfourconvolutionlayers.Theconvolutionoutputandsinuouspositionalencoding(Vaswanietal.,2017)areaddedastextencoderoutput.4.4.ResidualattentionFortheinitialattentionmoduleinFigure1,wesimplysettheattentionweightmatrixA∈RTmaxred×Ltexttobe“nearlydiagonal”.Tmaxredisthenumberoffeatureframesbeforethemeanpoolinglayeratbottom-uppath.This“nearlydi-agonal”attentionmatrixisagoodinstructivebias,sincethealignmentbetweentextandacousticfeaturesaremonotonicandalmostdiagonal.Inspiredby(Tachibanaetal.,2018),wesettheattentionweightmatrixasfollows:Sl=exp(cid:2)−(t/Tmaxred−l/Ltext)/2g2(cid:3)(14)Atl=Stl/Xl0Stl0(15)where,listextpositionindexandtismel-spectrogramposi-tionindex.Inthispaper,wegeneratefourattentionmatricesusingg∈[0.01,0.05,0.1,0.2]andcomputefourcontextvectorsbymultiplyingthefourgeneratedattentionmatriceswithtextencoderoutputrespectively.Thenthefourcontextvectorsareconcatenatedandprojectedtoonecontextvec-toralongtemporalaxis.Duringtrainingstage,Tmaxredisobtainedfromthenumberofground-truthmel-spectrogramframesdividedbythemaximumreductionfactor.Maxi-mumreductionfactoristheproductofthereductionfactorsbetweenneighbouringresidualblockgroups.Duringinfer-encestage,itisobtainedbymultiplyingpredictedspeakingspeedˆdtoLtextandthenbeingdividedbythemaximumreductionfactor.Fortheremainingresidualattentionmodules,theytaketextencoderoutput(K,V),attentionweightfrompreviousattentionmodule(Aprev)andlatentvariablesfromprevioustopdowmgroup(Q)asinput.Multi-headattentionmecha-nism(Vaswanietal.,2017)isused,whereAprevisaddedasanadditionalinput:ResidualMultiHead(Q,K,V,Aprev)=Concat(head1,...,headh)WO(16)whereQ,K,Varematriceswithdimensiondk,dkanddvrespectively.headi=ResidualAttention(QWQi,KWKi,VWVi,Aprev).WOisthetransformationmatrixthatlin-earlyprojectstheconcatenationofallheadsoutput.WQi,WKiandWViareprojectionmatricesofthei-thhead.Aprevistheaveragedattentionweightsfrompreviousattentionheads.Weusethescaleddot-productmechanismbasedresidualattention:ResidualAttention(Q,K,V,Aprev)=Softmax(cid:16)QKT√dk+Aprev(cid:17)V(17)WeﬁndthatusingattentionweightsAprevmakestrainingprocessmorestablethanusingbefore-softmaxattentionscores,whichisusedinRealFormer(Heetal.,2020).ThepreviousattentionweightAprevisﬁrstupsampledtoﬁtthetimedimensionandthenprocessedbyaconvolutionlayerbeforebeingsenttothenextlayer.Differentfromthelocationsensitiveattention(Chorowskietal.,2015)thatonlytakesintoaccountattentionweightatpreviousdecodertimestep,ourresidualattentionmechanismincorporatesattentionweightsofalltimestepsfromlastresidualattentionmodule,whichprovidesaglobalperspectiveofattentionhistory.5.Experiments5.1.DataWeconductbothsingle-speakerandmulti-speakerTTSex-perimentstoevaluatetheproposedVARA-TTSmodel.Forsingle-speakerTTS,theLJSpeechcorpus(Ito&Johnson,2017)isused,whichcontains13100speechsampleswithtotaldurationofabout24hours.Weup-samplethespeechsignalsfrom22.05kHzto24kHzinsamplingrate.Thedatasetisrandomlypartitionedintotraining,validationandtestingsetsaccordingtoa12900/100/100scheme.Formulti-speakerTTS,weuseaninternalMandarinChinesemulti-speakercorpus,whichcontains55hoursofspeechdatafrom7femalespeakers.Inallexperiments,mel-spectrogramsarecomputedwith1024windowlengthand256frameshift.WeconverttextsentencesintophonemesequenceswithFestivalforEn-glishandwithaninternalgrapheme-to-phonemetoolkitforChinese.5.2.ModelconﬁgurationsWecompareVARA-TTSwithawell-knownARTTSmodel,Tacotron2(Shenetal.,2018),andananalogousnon-ARTTSmodel,BVAE-TTS(Leeetal.,2021),undersingle-speakerTTSsetting.Weuseanopen-sourceTacotron2im-plementation2andtheofﬁcialBVAE-TTSimplementation3.SomekeyhyperparametersinVARA-TTSarepresentedinAppendixC.Tomakethecomparisonfair,weusethesameneuralvocoder,HiFi-GAN(Kongetal.,2020)withHiFi-GAN-V1conﬁguration,toconvertmelspectrogramsintowaveformforallthreecomparedmodels.WetraintheVARA-TTSmodelwithabatchsizeof32withtwoNVIDIAV100GPUs.Andthemodelforevaluationaretrainedfor90kiterations.TheAdamoptimizerwithβ1=0.9,β2=0.999isadoptedforparameterupdating,wherethemaximumoflearningrateis1.5e-4andscheduled2https://github.com/NVIDIA/tacotron23https://github.com/LEEYOONHYUNG/BVAE-TTSVARA-TTS:Non-AutoregressiveText-to-SpeechSynthesisbasedonVeryDeepVAEwithResidualAttentioninthesamemannerasin(Vaswanietal.,2017)with10kstepstowarm-up.6.ResultsandAnalysis6.1.SpeechqualityMeanopinionscore(MOS)testisconductedtoevaluatethespeechnaturalness.Weinvite10raterstotaketheMOStest,whereparticipantsareaskedtogiveascorefrom1to5(leastnaturaltomostnatural)with0.5pointincrementstoeachstimulipresented.ThesentencesforevaluationaresampledfromLibriTTS(Zenetal.,2019)testset.TheMOStestresultsarepresentedinTable1.WecanseethatourproposedVARA-TTSobtainshigherMOSscorethanBVAE-TTS,whileisinferiortoTacotron2innaturalness.WealsoconductaMOStestforamulti-speakerVARA-TTStrainedwiththemulti-speakerMandarincorpus.TheMOSresultis4.49±0.11.WecanseethattheMOSscoreonmulti-speakerMandarincorpusismuchhigherthantheresultonLJSpeech.Onepossiblereasonmaybethatthemulti-speakerMandarincorpusisofhigherqualityandcontainslargeramountofdata.VARA-TTSisdata-hungryandtendstoover-ﬁtonLJSpeech.Audiosamplescanbefoundonline4.6.2.InferencespeedBeneﬁtingfromitsnon-ARstructure,VARA-TTSenjoysfastinferencespeed.Weusethetestsettocomparetheinferencespeedofthethreecomparedmodels.Averageinferencespeedofthethreecomparedmodelsobtainedfrom10differentrunsononeNVIDIAGeForceRTX2080TiGPUispresentedinTable1.WecanseethatinferencespeedofourVARA-TTSis16xfasterthanthatofTacotron2,andatthesamescaleasthatofBVAE-TTS.6.3.AlignmentreﬁnementprocessFigureB.1inAppendixBshowsanexampleofthealign-mentreﬁnementprocessforanutterancebythewell-trainedmulti-speakerVARA-TTSmodel.Intheﬁgure,thebottomrowshowstheinitialalignments,whichshowdiagonalpat-ternsgeneratedbyrule.Inthesecondrow,abluralignmentisgenerated.Theblurrinessindicatesthatthealignmentisnotreliable.However,VARA-TTSlearnstoreﬁnethecoarsealignmentandobtaincleareralignmentsduringthesucceedinghierarchies,ascanbeseenfromtheupperplotsinFigureB.1.Moreover,weobservethatthealignmentre-ﬁnementprocessvarieswiththeβvalueused.Forβ=1.0,thealignmentsdisperseinupperlayers,andremainclearforβ=1.8.Thisindicatesthathierarchicallatentvariablesfromthesamelayersencodedifferentinformationwhen4https://vara-tts.github.io/VARA-TTS/Table1.MOSwith95%conﬁdenceandinferencespeedresults.ModelMOSInferencespeed(ms)Tacotron24.11±0.22526.52BVAE-TTS3.33±0.1818.06VARA-TTS3.88±0.2032.010102030405060Layer index0.000.010.020.030.04Cumulative KL=1.0,=0.0=1.8,=0.0=1.8,=1.0Figure3.CumulativeKLfordifferentβandλ.They-axisisthecumulativeKLvalueperdatapointbylayersandthey-axisisthelayerindex.Flathorizontallinesegmentindicatesthatposteriorcollapseoccursinthecorrespondinglayers.trainedwithdifferentvaluesofβ.6.4.AblationstudiesToshowtheeffectivenessofmodeldesigninVARA-TTS,weconductthefollowingablationstudies:(i)TrainwithoutdetailedKLgain;(ii)UsedifferentvaluesofβinEquation11duringtraining;(iii)Separatelytrainthespeakingspeedpredictor.DetailedKLgain.Figure3showsthecumulativeKLdiver-gencecurveswithdifferentvaluesofβandλ.WecanseethattherearesomehorizontalsegmentsinthecumulativeKLdivergencecurvewhenλ=0.ThisindicatesKLval-uesare0andposteriorcollapseoccursinthecorrespondinglayers.WhendetailedKLgainisapplied(λ=1.0),thecumulativeKLincreasessmoothlywiththelayerindexandcontainsnohorizontalsegments.TheKLvalueofλ=1.0islargerthanthatofλ=0inFigure3.ThisisbecausedetailedKLgainmechanismenlargesthehierarchicalKLvaluessmallerthanKLref,whichcanbecompensatedbyalargerβ.WealsoobservethatwhentrainedwithdetailedKLgain,VARA-TTScanlearncleareralignmentsincoarsehierarchicallayerasshowninAppendixB.Thisindicatesthatposteriorcollapsedoesnothappenintheselayersandthelatentvariablesencodemeaningfulinformation.Differentvaluesofβ.Attraining,thequeriesaresampledfromposterior,butsampledfromprioratinference.SmallerKLvalueindicatessmallergapbetweenpriorandposte-riorforVDVAE.Thesmallerthegapbetweenpriorandposterior,themorereliablethequeriescanbeatinference.ThehpyerparameterβinEquation11adjuststherelativeweightbetweenthereconstructionerrorandKLterm.TheVARA-TTS:Non-AutoregressiveText-to-SpeechSynthesisbasedonVeryDeepVAEwithResidualAttention020406080100k step0.2000.2250.2500.2750.3000.3250.3500.3750.400feature reconstruction error=1.0,=0.0=1.8,=0.0=1.8,=1.0=2.2,=1.0=2.6,=1.0=3.0,=1.0(a)featurereconstructionerror020406080100k step0.000.020.040.060.080.10KL=1.0,=0.0=1.8,=0.0=1.8,=1.0=2.2,=1.0=2.6,=1.0=3.0,=1.0(b)KL020406080100k step0.0000.0020.0040.0060.0080.010speaking speed error=1.0,=0.0=1.8,=0.0=1.8,=1.0=2.2,=1.0=2.6,=1.0=3.0,=1.0(c)speakingspeederrorFigure4.Thefeaturereconstructionerror,KL,andspeakingspeederrortrainingcurvesonvalidationsetwithdifferentvaluesofβandλ.Thex-axisistrainingstepintheunitofthousandandthey-axisisthecorrespondingvalueoffeaturereconstructionerror,KL,andspeakingspeederror.020406080100k step0.0000.0050.0100.0150.0200.0250.030speaking speed errorjointly-trainedseparately-trained(a)speakingspeederror020406080100k step0.260.280.300.320.340.360.380.40-ELBOjointly-trainedseparately-trained(b)-ELBOFigure5.Speakingspeederrorand-ELBOformodelstrainedwithajointspeakingspeedpredictorandaseparateonecurvesoffeaturereconstructionerrorandKLvaluewithdifferentvaluesofβandλonvalidationsetareshowninFigure4(a)and(b)respectively.Ascanbeseen,enlargingβresultsinlargerfeaturereconstructionerrorandsmallerKLvalue.However,weﬁndthatsmallchangesinfeaturereconstructionerrordonotinﬂuenceperpetualresultalot.Figure4(c)showsthespeakingspeederroronvalidationset.Wecanseethatdifferentvaluesofβhavelittleeffectonspeakingspeederror.Weuseβ=1.8andλ=1.0formodelevaluation.Jointspeakingspeedmodeling.Theseparatespeakingspeedpredictorhasthesamestructureasthejointone,ex-ceptthatitsinputisthemean-pooledtextembeddinganddetachedfromthecomputationalgraphbyastopgradientoperation.ThestopgradientoperationisalsoappliedinBVAE-TTSandGLOW-TTStoavoidaffectingthetrainingobjective.AsshowinFigure5,thejointtrainingstrategyobtainssimilarELBOastheseparatetrainingcounterpart,butattainsmuchsmallerspeakingspeederroronvalidationset.Thisvalidatestheeffectivenessofjointtrainingthespeakingspeedpredictorandthewholemodel.7.ConclusionInthiswork,weproposeanovelnon-ARend-to-endTTSmodel,VARA-TTS,generatingmel-spectrogramfromtextwithVDVAEandresidualattentionmechanism.Thehier-archicallatentvariablesfromVDVAEareusedasqueriesfortheresidualattentionmodule.Theresidualattentionmoduleisabletogeneratethetextual-to-acousticalignmentinalayer-by-layercoarse-to-ﬁnemanner.Experimentalre-sultsshowthatVARA-TTSattainsbetterperceptualresultsthanBVAE-TTSatasimilarinferencespeed,andis16xspeed-upforinferenceoverTacotron2withslightlyinfe-riorperformanceinnaturalness.Wealsodemonstrateitsextensibilitytoamulti-speakersetting.VARA-TTSshouldbeeasilyextendedtotext-to-waveformbyaddingmorelayers.However,weﬁndthatitishardtooptimizeinourpreliminaryexperiments.Themodelisabletolearnclearalignmentbetweentextandwaveform,butitcannotgenerateintelligiblewaveform.Thisisconsistentwiththeresultsreportedin(Child,2020)whereVDVAEcannotgenerateconsistentandsharp1024×1024images.WeproposedetailedKLgainforVDVAE,whichcanavoidnon-informativehierarchicallatentvariables.Itisinterestingtoanalyzeittheoreticallyandweleavethisforthefuturework.VARA-TTS:Non-AutoregressiveText-to-SpeechSynthesisbasedonVeryDeepVAEwithResidualAttentionReferencesAlemi,A.A.,Poole,B.,Fischer,I.,Dillon,J.V.,Saurous,R.A.,andMurphy,K.FixingabrokenELBO.InDy,J.G.andKrause,A.(eds.),Proceedingsofthe35thInter-nationalConferenceonMachineLearning,ICML2018,Stockholmsm¨assan,Stockholm,Sweden,July10-15,2018,volume80ofProceedingsofMachineLearningResearch,pp.159–168.PMLR,2018.Arik,S.O.,Chrzanowski,M.,Coates,A.,Diamos,G.,Gib-iansky,A.,Kang,Y.,Li,X.,Miller,J.,Ng,A.,Raiman,J.,etal.Deepvoice:Real-timeneuraltext-to-speech.arXivpreprintarXiv:1702.07825,2017.Battenberg,E.,Skerry-Ryan,R.,Mariooryad,S.,Stanton,D.,Kao,D.,Shannon,M.,andBagby,T.Location-relativeattentionmechanismsforrobustlong-formspeechsynthesis.InICASSP2020-2020IEEEInter-nationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),pp.6194–6198.IEEE,2020.Burgess,C.P.,Higgins,I.,Pal,A.,Matthey,L.,Watters,N.,Desjardins,G.,andLerchner,A.Understandingdisentan-glinginβ-vae.CoRR,abs/1804.03599,2018.Chen,X.,Duan,Y.,Houthooft,R.,Schulman,J.,Sutskever,I.,andAbbeel,P.Infogan:Interpretablerepresentationlearningbyinformationmaximizinggenerativeadversar-ialnets.InNIPS,2016.Child,R.Verydeepvaesgeneralizeautoregressivemodelsandcanoutperformthemonimages.arXivpreprintarXiv:2011.10650,2020.Chorowski,J.K.,Bahdanau,D.,Serdyuk,D.,Cho,K.,andBengio,Y.Attention-basedmodelsforspeechrecogni-tion.Advancesinneuralinformationprocessingsystems,28:577–585,2015.Chung,J.,Kastner,K.,Dinh,L.,Goel,K.,Courville,A.C.,andBengio,Y.Arecurrentlatentvariablemodelforse-quentialdata.Advancesinneuralinformationprocessingsystems,28:2980–2988,2015.Donahue,J.,Dieleman,S.,Bi´nkowski,M.,Elsen,E.,andSi-monyan,K.End-to-endadversarialtext-to-speech.arXivpreprintarXiv:2006.03575,2020.Gibiansky,A.,Arik,S.,Diamos,G.,Miller,J.,Peng,K.,Ping,W.,Raiman,J.,andZhou,Y.Deepvoice2:Multi-speakerneuraltext-to-speech.InAdvancesinneuralinformationprocessingsystems,pp.2962–2970,2017.He,M.,Deng,Y.,andHe,L.Robustsequence-to-sequenceacousticmodelingwithstepwisemonotonicattentionforneuraltts.Proc.Interspeech2019,pp.1293–1297,2019.He,R.,Ravula,A.,Kanagal,B.,andAinslie,J.Real-former:Transformerlikesresidualattention.CoRR,abs/2012.11747,2020.Hendrycks,D.andGimpel,K.Gaussianerrorlinearunits(gelus).arXivpreprintarXiv:1606.08415,2016.Ito,K.andJohnson,L.Theljspeechdataset.https://keithito.com/LJ-Speech-Dataset/,2017.Jin,Z.,Finkelstein,A.,Mysore,G.J.,andLu,J.Fftnet:Areal-timespeaker-dependentneuralvocoder.In2018IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),pp.2251–2255.IEEE,2018.Kalchbrenner,N.,Elsen,E.,Simonyan,K.,Noury,S.,Casagrande,N.,Lockhart,E.,Stimberg,F.,Oord,A.v.d.,Dieleman,S.,andKavukcuoglu,K.Efﬁcientneuralaudiosynthesis.arXivpreprintarXiv:1802.08435,2018.Kim,J.,Kim,S.,Kong,J.,andYoon,S.Glow-tts:Agen-erativeﬂowfortext-to-speechviamonotonicalignmentsearch.arXivpreprintarXiv:2005.11129,2020.Kingma,D.P.andWelling,M.Auto-encodingvariationalbayes.arXivpreprintarXiv:1312.6114,2013.Kong,J.,Kim,J.,andBae,J.Hiﬁ-gan:Generativead-versarialnetworksforefﬁcientandhighﬁdelityspeechsynthesis.InLarochelle,H.,Ranzato,M.,Hadsell,R.,Balcan,M.,andLin,H.(eds.),AdvancesinNeuralIn-formationProcessingSystems33:AnnualConferenceonNeuralInformationProcessingSystems2020,NeurIPS2020,December6-12,2020,virtual,2020.Lee,Y.,Shin,J.,andJung,K.Bidirectionalvariationalinferencefornon-autoregressivetext-to-speech.InIn-ternationalConferenceonLearningRepresentations,2021.URLhttps://openreview.net/forum?id=o3iritJHLfO.Li,N.,Liu,S.,Liu,Y.,Zhao,S.,andLiu,M.Neuralspeechsynthesiswithtransformernetwork.InProceedingsoftheAAAIConferenceonArtiﬁcialIntelligence,volume33,pp.6706–6713,2019.Miao,C.,Liang,S.,Chen,M.,Ma,J.,Wang,S.,andXiao,J.Flow-tts:Anon-autoregressivenetworkfortexttospeechbasedonﬂow.InICASSP2020-2020IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),pp.7209–7213.IEEE,2020.Oord,A.,Li,Y.,Babuschkin,I.,Simonyan,K.,Vinyals,O.,Kavukcuoglu,K.,Driessche,G.,Lockhart,E.,Cobo,L.,Stimberg,F.,etal.Parallelwavenet:Fasthigh-ﬁdelityspeechsynthesis.InInternationalconferenceonmachinelearning,pp.3918–3926.PMLR,2018.VARA-TTS:Non-AutoregressiveText-to-SpeechSynthesisbasedonVeryDeepVAEwithResidualAttentionOord,A.v.d.,Dieleman,S.,Zen,H.,Simonyan,K.,Vinyals,O.,Graves,A.,Kalchbrenner,N.,Senior,A.,andKavukcuoglu,K.Wavenet:Agenerativemodelforrawaudio.arXivpreprintarXiv:1609.03499,2016.Peng,K.,Ping,W.,Song,Z.,andZhao,K.Non-autoregressiveneuraltext-to-speech.InInternationalConferenceonMachineLearning,pp.7586–7598.PMLR,2020.Perez,E.,Strub,F.,DeVries,H.,Dumoulin,V.,andCourville,A.Film:Visualreasoningwithageneralcon-ditioninglayer.InProceedingsoftheAAAIConferenceonArtiﬁcialIntelligence,volume32,2018.Ping,W.,Peng,K.,andChen,J.Clarinet:Parallelwavegenerationinend-to-endtext-to-speech.InInternationalConferenceonLearningRepresentations,2018a.Ping,W.,Peng,K.,Gibiansky,A.,Arik,S.O.,Kannan,A.,Narang,S.,Raiman,J.,andMiller,J.Deepvoice3:2000-speakerneuraltext-to-speech.Proc.ICLR,pp.214–217,2018b.Prenger,R.,Valle,R.,andCatanzaro,B.Waveglow:Aﬂow-basedgenerativenetworkforspeechsynthesis.InICASSP2019-2019IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),pp.3617–3621.IEEE,2019.Ren,Y.,Ruan,Y.,Tan,X.,Qin,T.,Zhao,S.,Zhao,Z.,andLiu,T.-Y.Fastspeech:Fast,robustandcontrollabletexttospeech.AdvancesinNeuralInformationProcessingSystems,32:3171–3180,2019.Ren,Y.,Hu,C.,Qin,T.,Zhao,S.,Zhao,Z.,andLiu,T.-Y.Fastspeech2:Fastandhigh-qualityend-to-endtext-to-speech.arXivpreprintarXiv:2006.04558,2020.Rezende,D.J.andViola,F.Tamingvaes.CoRR,abs/1810.00597,2018.Rezende,D.J.,Mohamed,S.,andWierstra,D.Stochasticbackpropagationandapproximateinferenceindeepgen-erativemodels.InInternationalConferenceonMachineLearning,pp.1278–1286,2014.Shen,J.,Pang,R.,Weiss,R.J.,Schuster,M.,Jaitly,N.,Yang,Z.,Chen,Z.,Zhang,Y.,Wang,Y.,Skerrv-Ryan,R.,etal.Naturalttssynthesisbyconditioningwavenetonmelspectrogrampredictions.In2018IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),pp.4779–4783.IEEE,2018.Sønderby,C.K.,Raiko,T.,Maaløe,L.,Sønderby,S.K.,andWinther,O.Laddervariationalautoencoders.InAdvancesinneuralinformationprocessingsystems,pp.3738–3746,2016.Tachibana,H.,Uenoyama,K.,andAihara,S.Efﬁcientlytrainabletext-to-speechsystembasedondeepconvolu-tionalnetworkswithguidedattention.In2018IEEEIn-ternationalConferenceonAcoustics,SpeechandSignalProcessing,ICASSP2018,Calgary,AB,Canada,April15-20,2018,pp.4784–4788.IEEE,2018.Valle,R.,Shih,K.,Prenger,R.,andCatanzaro,B.Flowtron:anautoregressiveﬂow-basedgenerativenetworkfortext-to-speechsynthesis.arXivpreprintarXiv:2005.05957,2020.VandenOord,A.,Kalchbrenner,N.,Espeholt,L.,Vinyals,O.,Graves,A.,etal.Conditionalimagegenerationwithpixelcnndecoders.Advancesinneuralinformationpro-cessingsystems,29:4790–4798,2016.VanDenOord,A.,Vinyals,O.,etal.Neuraldiscreterep-resentationlearning.InAdvancesinNeuralInformationProcessingSystems,pp.6306–6315,2017.Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,Kaiser,L.,andPolosukhin,I.Attentionisallyouneed.InGuyon,I.,vonLuxburg,U.,Bengio,S.,Wallach,H.M.,Fergus,R.,Vishwanathan,S.V.N.,andGarnett,R.(eds.),AdvancesinNeuralInformationProcessingSystems30:AnnualConferenceonNeuralInformationProcessingSystems2017,December4-9,2017,LongBeach,CA,USA,pp.5998–6008,2017.Wang,Y.,Skerry-Ryan,R.,Stanton,D.,Wu,Y.,Weiss,R.J.,Jaitly,N.,Yang,Z.,Xiao,Y.,Chen,Z.,Bengio,S.,etal.Tacotron:Towardsend-to-endspeechsynthesis.Proc.Interspeech2017,pp.4006–4010,2017.Weiss,R.J.,Skerry-Ryan,R.,Battenberg,E.,Mariooryad,S.,andKingma,D.P.Wave-tacotron:Spectrogram-freeend-to-endtext-to-speechsynthesis.arXivpreprintarXiv:2011.03568,2020.Yamamoto,R.,Song,E.,andKim,J.-M.Parallelwavegan:Afastwaveformgenerationmodelbasedongenerativeadversarialnetworkswithmulti-resolutionspectrogram.InICASSP2020-2020IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),pp.6199–6203.IEEE,2020.Zen,H.,Dang,V.,Clark,R.,Zhang,Y.,Weiss,R.J.,Jia,Y.,Chen,Z.,andWu,Y.Libritts:Acorpusderivedfromlibrispeechfortext-to-speech.CoRR,abs/1904.02882,2019.Zhang,J.-X.,Ling,Z.-H.,andDai,L.-R.Forwardattentioninsequence-to-sequenceacousticmodelingforspeechsynthesis.In2018IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),pp.4789–4793.IEEE,2018.SupplementMaterialsA.AuxiliaryspeakingspeedpredictorproofMinimizingthepredictionerrorforspeakingspeeddfromthetop-mostlatentvariablesz0pavesthewaytomaximizingalowerboundoftheirmutualinformation.ProofI(z0;d)=−H(d|z0)+H(d)=Ez0∼p(z0)(cid:2)Ed∼p(d|z0)[logp(d|z0)](cid:3)+H(d)=Ez0∼p(z0)(cid:2)DKL(p(d|z0)||q(d|z0))+Ed∼p(d|z0)[logq(d|z0)](cid:3)+H(d)≥Ez0∼p(z0)(cid:2)Ed∼p(s|z0)[logq(d|z0)](cid:3)+H(d)=Ed∼p(d)(cid:2)Ez0∼p(z0|d)[logq(d|z0)](cid:3)+H(d)I(z0;d)isthemutualinformationbetweenz0andd.H(d|z0)istheconditionalentropyofdgivenz0andH(d)istheentropyofd.Eachpieceofdatum(xandt)hasitscorrespondingd.p(z0)isthepriordistributionofz0givent,whichomittedforconciseness.p(d)isthepriorofd.Itisnotactuallysampled.dissampledwhenapieceofdatumissampled.q(d|z0)istheoutputdistributionofanauxiliaryspeakingspeedpredictor.p(z0|d)isactuallytheposteriorofz0givenxandt.B.Alignmentsfordifferentvaluesofβandγ.FigureB.1showsalignmentsfordifferentvaluesofβandγ.C.DetailedmodelconﬁgurationDetailsofsomekeyhyperparametersinVARA-TTSarelistedinTable1.Fortheremainingmodelconﬁguration,pleaserefertothesourcecodeaccompaniedwiththismanuscript.D.SpeakingspeedpredictornetworkstructureNetworkstructureofthespeakingspeedpredictorusedinVARA-TTSisillustratedinFigureD.1.arXiv:2102.06431v1  [cs.SD]  12 Feb 2021VARA-TTS:Non-AutoregressiveText-to-SpeechSynthesisbasedonVeryDeepVAEwithResidualAttention0204060800204060800204060800204060800100200300400500020406080Frame indexPhoneme indexAlignments for =1.0,=0.00204060800204060800204060800204060800100200300400500020406080Frame indexPhoneme indexAlignments for =1.8,=0.00204060800204060800204060800204060800100200300400500020406080Frame indexPhoneme indexAlignments for =1.8,=1.0FigureB.1.Alignmentsfordifferentvaluesofβandλ.Allthealignmentsareinterpolatedtothehighesttemporalresolution.Theﬁguresfrombottomrowtotoprowcorrespondtocoarsetoﬁnehierarchicallayers.Thealignmentsonthebottomrowarediagonalalignmentsgeneratedaccordingtolengthsoftextandacousticfeatures.Thefollowingalignmentsaregeneratedbyresidualattentionmechanism.Thealignmentsbecomecleareratﬁnerhierarchicallayerswhenβ=1.8.However,whenβ=1.0,thealignmentsatﬁnerhierarchicallayersbecomeblurry.Whenλ=1.0,thealignmentsatcoarselayersisclearerthanthosewhenλ=0.Table1.HyperparametersinVARA-TTS.HyperparameternameVARA-TTS(EN)VARA-TTS(ZH)Numberofmelbanksinmelspectrogram8080Melspectrogrampre-convlayerConv1Dwithk=11andc=384Conv1Dwithk=11andc=512Numberofphonemetokens55148Textembeddingdimension384384Totalnumberofbottom-upstacks67Numberofresblocksineachbottom-upstack4/6/8/12/9/56/12/16/10/8/8/5Temporalreductionrateineachbottom-upstackrepeat/2/2/2/2/1repeat/2/2/2/2/2/1Bottom-upresidualblockconvdimensions384/96/96/384512/128/128/512Numberofheadsinmultiheadattentionmodule88Attentiondimension384384Latentvariabledimension1616Textencoderconvdimensions384/96/96/384384/96/96/384Numberofspeakers17Speakerembeddingdimension-384VARA-TTS:Non-AutoregressiveText-to-SpeechSynthesisbasedonVeryDeepVAEwithResidualAttentionLinearSpeaking rate ReLUDropoutLinearSigmoidTrainingInferenceFigureD.1.NetworkdetailofspeakingspeedpredictorinVARA-TTS