Leveraging recent advances in Pre-Trained Language Models for
Eye-Tracking Prediction

Varun Madhavan
IIT Kharagpur
varun.m.iitkgp
@gmail.com

Aditya Girish Pawate
IIT Kharagpur
adityagirish
pawate@gmail.com

Shraman Pal
IIT Kharagpur
shramanpal12345
@gmail.com

Abhranil Chandra
IIT Kharagpur
abhranil.chandra
@gmail.com

Abstract

Cognitively inspired Natural Language Pro-
cessing uses human-derived behavioral data
like eye-tracking data, which reﬂect the seman-
tic representations of language in the human
brain to augment the neural nets to solve a
range of tasks spanning syntax and semantics
with the aim of teaching machines about lan-
guage processing mechanisms. In this paper,
we use the ZuCo 1.0 and ZuCo 2.0 dataset con-
taining the eye-gaze features to explore differ-
ent linguistic models to directly predict these
gaze features for each word with respect to its
sentence. We tried different neural network
models with the words as inputs to predict the
targets. And after lots of experimentation and
feature engineering ﬁnally devised a novel ar-
chitecture consisting of RoBERTa Token Clas-
siﬁer with a dense layer on top for language
modeling and a stand-alone model consisting
of dense layers followed by a transformer layer
for the extra features we engineered. Finally,
we took the mean of the outputs of both these
models to make the ﬁnal predictions. We eval-
uated the models using mean absolute error
(MAE) and the R2 score for each target.

1

Introduction

When reading, we humans process language “au-
tomatically” without reﬂecting on each step — we
string words together into sentences, understand the
meaning of spoken and written ideas, and process
language without thinking too much about how the
underlying cognitive process happens. This pro-
cess generates cognitive signals that could poten-
tially facilitate natural language processing tasks.
Our understanding of the ﬁeld of language process-
ing is highly dependent on accurately modeling
eye-tracking features (example of cognitive data)
as they provide millisecond-accurate records of
where humans look while reading, thus shedding
some light on human attention mechanisms and lan-
guage comprehension skills. Eye movement data

is used extensively in various ﬁelds including but
not limited to Natural Language Processing and
Computer Vision. The recent introduction of stan-
dardized datasets has enabled the comparison of
the capabilities of different cognitive modeling and
linguistically motivated approaches to model and
analyze human patterns of reading.

2 Problem Description

Feature
Name
nFix

FFD
TRT

GPT
ﬁxProp

Feature Description

Number of ﬁxations on the
word
Duration of the ﬁrst ﬁxation
Total ﬁxation duration, includ-
ing regressions
Sum of all prior ﬁxations
Proportion of participants that
ﬁxated the current word

Table 2.1: Target Variables

Our brain processes language and generates cog-
nitive processing data such as gaze patterns and
brain activity. These signals can be recorded while
reading. The problem is formulated as a regression
task to predict the ﬁve eye-tracking features for
each token of a sentence in the test set. Each train-
ing sample consists of a token in a sentence and the
corresponding features. We are required to predict
the ﬁve features: nFix, FFD, TRT, GPT, and ﬁx-
Prop. We were given a CSV ﬁle with sentences to
train our model. Tokens in the sentences were split
in the same manner as they were presented to the
participants during the reading experiments. Hence,
this did not necessarily follow linguistically correct
tokenization. Sentence endings were marked with
an <EOS> symbol. A prediction had to be made
for each token for all the ﬁve target variables. The
evaluation metric used was the mean absolute error.

1
2
0
2

t
c
O
9

]
L
C
.
s
c
[

1
v
5
7
4
4
0
.
0
1
1
2
:
v
i
X
r
a

 
 
 
 
 
 
• Number of characters in the lemmatized word
subtracted from the number of characters in
the word. Obtained using NLTK package
(Bird and Loper, 2004) (lemwordlen)

• If a word was a stopword or not, we appended
1 if stopword and -1 otherwise(stopword)

• If a word was a number, we appended 1 if it
was a number and -1 otherwise(number)

• If a word was the at end of the sentence or
not, we appended 1 if last word and -1 other-
wise (endword). Our hypothesis for adding
the endword was that it had a strong cor-
relation with GPT as endwords had signiﬁ-
cantly higher GPT values compared to the
other words.

• Part of Speech Tags for each word in a sen-
tence (postag). We one hot encoded the POS
TAGS which gave us a sparse matrix of 20
features. The POS tags have a high statistical
correlation with all the target variables as they
represent the semantic structure of a sentence.
Obtained using NLTK package.

• TF-IDF value of each word by considering
each sentence as a separate document and
each word as a separate term(tf idf ). For the
tﬁdf score, words unrecognized by the in-built
tokenizer due to being single digit numbers,
having “-” etc were replaced by 0.01 if it was
among “a”, “A” or “I” and 0.1 otherwise. This
provides a hyperparameter(TF-IDF Error) that
could be tuned for better results. Obtained us-
ing sklearn package. (Pedregosa et al., 2011)

Scatter plots of word length(number of charac-
ters) and the 5 target variables have been shown.
It is observed that there is quite a positive correla-
tion between them. we expected the last word of
the sentence to have an impact on the GPT score
which is evident from the barplot as shown in ﬁgure
2f. The GPT score for a endword is usually quite
high compared to a non endword. We also created
barplots of stopword and the target variables. It is
evident that for stopwords the values of all the tar-
gets variables are comparitively lower which points
to stopword being a signiﬁcant feature. The effect
of adding these features is further explored as an
ablation study in section 6.3.

Figure 3.1: Correlation Heatmap of Targets

3 Dataset

We use the eye-tracking data of the Zurich Cogni-
tive Language Processing Corpus (ZuCo 1.0 and
ZuCo 2.0) (Hollenstein et al., 2020) recorded dur-
ing normal reading. The training and test data con-
tains 800 and 191 sentences respectively.In recent
years, collecting these signals has become increas-
ingly easy and less expensive (Papoutsaki et al.,
2016); as a result, using cognitive features to im-
prove NLP tasks has become more popular. The
data contains features averaged over all readers and
scaled in the range of 0 to 100, facilitating eval-
uation via mean absolute average (MAE). The 5
eye-tracking features included in the dataset are
shown in 2.1. The heatmap of the correlation ma-
trix of the 5 target variables have been shown in
Figure 1. It is observed that apart from GPT all the
features are correlated with each other to a large
extent.

4 Preprocessing

We ﬁrst removed the <EOS> token at the end of
each sentence. We looked at similar word (Clifton
et al., 2007), (Demberg and Keller, 2008) and (Just
and Carpenter, 1980) to ﬁnd features that we could
add. As GPT consistently performed the worst in
all our initial experiments, we tried replacing GPT
with a new feature (TRT - GPT) during training.
The results improved slightly and we decided to
keep this change for the remaining experiments. To
incorporate word level characteristics (i.e character-
istics that belong to a word; eg: word length) and
syntactic information we introduced the following
features -

• Number of characters in a word (wordlen)

Figure 4.1: nFix vs

Figure 4.2: FFD vs

wordlen

wordlen

Figure 4.5: GPT vs
endword

Figure 4.6: nFix vs
stopword

Figure 4.3: GPT vs

Figure 4.4: TRT vs

wordlen

wordlen

Figure 4.7: FFD vs
stopword

Figure 4.8: GPT vs
stopword

5 Model Description

The model has two major components, i.e.
the
Feature Model and the Language Model, which
encode the features and the sentence respectively.
The resulting encoded representations are mean
pooled and passed to a fully connected layer and
sigmoid activation to output a (5x1) score vector
for each token in the sentence. The score vector
represents the values of nFix, FFD, TRT, GPT and
ﬁxProp respectively.

6 Experiments

6.1 Evaluation Metric

We utilised the R2 score metric instead of the Mean
Absolute Error score because R2 provides a better
understanding of the model’s performance in ab-
sence of a baseline score. As the R2 score has an
upper bound of 1 it is possible to understand how
good the model is performing for a particular value
of R2 score.

R2(y, ˆy) = 1 −

(cid:80)n
(cid:80)n

i=1(yi − ˆyi)2
i=1(yi − ¯y)2

6.2 Models

We experimented with a variety of models, in par-
ticular recent advances in attention-based(Vaswani
et al., 2017) pre-trained language models like
RoBERTa (Liu et al., 2019), ELECTRA (Clark
et al., 2020), BERT (Devlin et al., 2019), Distil-
BERT(Sanh et al., 2020). Each language model
was used to encode text as described in section 5
and the rest of the pipeline was kept the same. We
compare this method with (Hollenstein et al., 2019),
which employed GloVe(200d) (Pennington et al.,

2014) and a Bidirectional LSTM (Hochreiter and
Schmidhuber, 1997) to encode sentences. In addi-
tion to GloVe, we also tried the more recent Con-
ceptNet Numberbatch embeddings (300D) (Speer
et al., 2018) that have been shown to outperform
GloVe on a variety of tasks. The results for all
experiments are tabulated in Table 6.3.

6.3 Features

In addition to various architectures, we tried a
number of feature engineering techniques with in-
ferred/transformed features as described in section
4. We performed an ablation study with each fea-
ture to judge the importance/beneﬁt of each adding
each of the new features.

We note that increasing the number of fea-
tures does not always help, so analysing the data
we saw that most of the POS tags rarely oc-
curred and there were 6-7 frequently occurring
tags. We decided to club the remaining signiﬁ-
cant tags under their respective larger categories
(eg: noun phrases(NNP) were clubbed into noun
category(NN)) and marked the remaining small
number of categories as <UNK> while also one-
hot encoding all of them. Lastly we standardized
the targets for better training stability and it was

Figure 4.9: TRT vs
stopword

Figure 4.10: ﬁxProp vs
stopword

word len

is stopword

is number

Figure 5.1: Feature Model Figure 5.2: Language Head

lem word len

applied after transforming GPT to TRT-GPT.

• Effect of scaling: Due to a large number
of features with a large difference in their
means, it is helpful to scale the features to the
same range/same mean and standard deviation.
Hence we compared two scaling methods -

is endword

– Min-max scaling:

Xnorm = (X − Xmin)/(Xmax − Xmin)

tﬁdf score

Feature Name

Feature
Description

Average
Score
and Std
Deviation

0.6145
0.07363

0.6475
0.06043

0.6194
0.06731

0.6125
0.06649

0.5918
0.10606

0.6265
0.06367

0.6284
0.059786

Number of
characters in
the word
Whether a
word is a
stopword
Whether a
word is a
number
Difference in
the number of
characters in
the word and
its lemmatized
form
Whether the
word is the last
word of its
sentence
TF-IDF score
of the word in
the
corresponding
sentence
Part of Speech
Tag of the word

– Standard scaling:

Xnorm = (X − Xmean)/(Xstd)

With the same model (multi-target with
RoBERTa encoder), using standard scaling
gave an R2 score of 0.62 while min-max scal-
ing gave a score of 0.81. The reason for min-
max scaling giving signiﬁcantly better perfor-
mance might be because the targets in this
dataset are always positive.

• Single Target vs. Multi-Target models:
With a large number of input features and a
relatively small datasets, we hypothesized that
large pre-trained models may fail to converge
to a suitable minima in the limited ﬁne-tuning.
Hence, we trained separate instances of the
same model on each of nFix, FFD, TRT, GPT
and ﬁxProp. This further increased the R2
score on the validation set to 0.88.

7 Results

In 6.3, we tabulate the R2 scores of each of our
models on the validation set.

pos tag

Table 6.1: Features and their Description and the Mean and
Std Deviation of R2 scores of target variables

Hyperparameter
Batch Size
Num Warm Up Steps
Learning Rate
Max epochs
Beta 1
Delta
Beta 2
Early stopping patience
Weight Decay
TF-IDF Error
Validation Split Ratio
Training Split Ratio

Value
4
4
3e-5
120
0.91
0.0001
0.998
8
1e-5
0.1
0.2
0.8

Table 6.2: Hyperparameter Values

Figure 5.3: Complete Model Flow

Model
RoBERTa
RoBERTa
RoBERTa
BERT
BiLSTM
Minibatch
BiLSTM
Minibatch
RoBERTa
Token
BERT
Token
BERT
BiLSTM
MiniBatch
RoBERTa
Token
RoBERTa
BiLSTM
Stochastic
RoBERTa
Token
RoBERTa
Token
BERT
Token
BERT
BERT
DistilBert

Feature Norm Extra Features

Std Scl
Min Max
Min Max
Min Max

STT
All
STT
All

FFD

TRT

nFix
ﬁxProp
GPT
0.8842 0.9246 0.7343 0.8823 0.9509
0.8835 0.9132 0.7383 0.8542 0.9461
0.8417 0.9090 0.6456 0.8152 0.9492
0.7433 0.8528 0.4574 0.6925 0.9173

Min Max

All + GloVE

0.6595 0.7275 0.6281 0.5988 0.7583

Min Max

All + NB1

0.6379 0.7203 0.5992 0.5388 0.7592

Std Scl

Std Scl

Std Scl

Std Scl

Std Scl

Std Scl

Std Scl

Std Scl

Std Scl

Std Scl

Std Scl
Std Scl
No Scaling

All

All

All

0.7038 0.6512 0.4697 0.6877 0.7146

0.7231 0.7107 0.3440 0.6376 0.7635

0.6497 0.6456 0.5409 0.6088 0.7171

All+GloVE

0.6850 0.5976 0.5267 0.5940 0.7158

All

All

0.6483 0.5861 0.5475 0.6280 0.6566

0.6360 0.5869 0.4798 0.6061 0.6623

All+GloVE

0.6282 0.6247 0.4394 0.5860 0.6893

All

All

All

All
All
All

0.6349 0.5914 0.4767 0.6123 0.6452

0.5991 0.5847 0.5417 0.5686 0.6524

0.6182 0.5677 0.4495 0.6026 0.6960

0.6019 0.5847 0.4159 0.5771 0.6404
0.5681 0.5437 0.4891 0.5170 0.6371
0.4804 0.5292 0.0983 0.4183 0.6324

Table 6.3: Compiled results of all the top performing models. STT means Single Target Training (one model to predict one
feature). Numberbatch refers to the ConceptNet Numberbatch word embeddings used (Speer et al., 2017). The architecture was
kept same (except for input dimensions), the learning rate was reduced for smaller number of features to achieve optimal values
which we observed through trial and error.

D. Cournapeau, M. Brucher, M. Perrot, and E. Duch-
Scikit-learn: Machine learning in
esnay. 2011.
Journal of Machine Learning Research,
Python.
12:2825–2830.

Jeffrey Pennington, Richard Socher, and Christopher D.
Manning. 2014. Glove: Global vectors for word rep-
resentation. In Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 1532–1543.

Victor Sanh, Lysandre Debut, Julien Chaumond, and
Thomas Wolf. 2020. Distilbert, a distilled version of
bert: smaller, faster, cheaper and lighter.

Robyn Speer, Joshua Chin, and Catherine Havasi. 2017.
ConceptNet 5.5: An open multilingual graph of gen-
eral knowledge. pages 4444–4451.

Robyn Speer, Joshua Chin, and Catherine Havasi. 2018.
Conceptnet 5.5: An open multilingual graph of gen-
eral knowledge.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need.

References

Steven Bird and Edward Loper. 2004. NLTK: The nat-
ural language toolkit. In Proceedings of the ACL In-
teractive Poster and Demonstration Sessions, pages
214–217, Barcelona, Spain. Association for Compu-
tational Linguistics.

Kevin Clark, Minh-Thang Luong, Quoc V. Le, and
Christopher D. Manning. 2020.
Electra: Pre-
training text encoders as discriminators rather than
generators.

Charles Clifton, Adrian Staub, and Keith Rayner. 2007.
Chapter 15 - eye movements in reading words and
sentences.
In Roger P.G. Van Gompel, Martin H.
Fischer, Wayne S. Murray, and Robin L. Hill, edi-
tors, Eye Movements, pages 341–371. Elsevier, Ox-
ford.

Vera Demberg and Frank Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syntactic
processing complexity. Cognition, 109(2):193–210.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Neural computation,

Long short-term memory.
9(8):1735–1780.

Nora Hollenstein, Maria Barrett, Marius Troendle,
Francesco Bigiolli, Nicolas Langer, and Ce Zhang.
2019. Advancing nlp with cognitive language pro-
cessing signals.

Nora Hollenstein, Marius Troendle, Ce Zhang, and
Nicolas Langer. 2020. Zuco 2.0: A dataset of physi-
ological recordings during natural reading and anno-
tation.

Marcel Adam Just and Patricia A. Carpenter. 1980. A
theory of reading: from eye ﬁxations to comprehen-
sion. PSYCHOLOGICAL REVIEW, 87(4).

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized BERT pretraining ap-
proach. CoRR, abs/1907.11692.

Alexandra Papoutsaki, Patsorn Sangkloy,

James
Laskey, Nediyana Daskalova, Jeff Huang, and James
Hays. 2016. Webgazer: Scalable webcam eye track-
In Proceedings of the
ing using user interactions.
Twenty-Fifth International Joint Conference on Arti-
ﬁcial Intelligence, IJCAI 2016, New York, NY, USA,
9-15 July 2016, pages 3839–3845. IJCAI/AAAI
Press.

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,

