Variable selection for the prediction of C[0,1]-valued
AR processes using RKHS

Beatriz Bueno-Larraz∗†

Johannes Klepsch‡

March 2018

Abstract

A model for the prediction of functional time series is introduced, where observa-

tions are assumed to be continuous random functions. We model the dependence of

the data with a nonstandard autoregressive structure, motivated in terms of the Re-

producing Kernel Hilbert Space (RKHS) generated by the auto-covariance function

of the data. The new approach helps to ﬁnd relevant points of the curves in terms

of prediction accuracy. This dimension reduction technique is particularly useful

for applications, since the results are usually directly interpretable in terms of the

original curves. An empirical study involving real and simulated data is included,

which generates competitive results. Supplementary material includes R-Code, ta-

bles and mathematical comments.

Keywords: Functional data analysis (FDA); Functional linear process; AR; Time

series; RKHS; Dimension reduction

MSC 2010: Primary: 62M10, 62M15, 62M20; Secondary: 62H25, 60G25

1 Introduction

Functional data analysis (FDA) is one of the answers to the recent rise of complex data.
It consists in viewing observations as entire curves instead of individual data points. For
a recent overview of the literature of FDA we refer to Cuevas (2014).

8
1
0
2

l
u
J

2

]
E
M

.
t
a
t
s
[

4
v
0
6
6
6
0
.
0
1
7
1
:
v
i
X
r
a

∗Universidad Aut´onoma

Departamento
beatriz.bueno@uam.es & beatriz.bueno.larraz@gmail.com

de Madrid,

de Matem´aticas,

Spain,

email:

†Corresponding author
‡Center for Mathematical Sciences, Technische Universit¨at M¨unchen, 85748 Garching, Boltz-

mannstraße 3, Germany, email: j.j.klepsch@gmail.com

1

 
 
 
 
 
 
Functional time series and variable selection
In many applications the data consists of a single curve z(s) sequentially recorded in
time, up to an instant point s ∈ (−∞, N ] (or [0, N ]). Typically these curves present a
periodical behavior. Common examples are daily ﬁnancial or meteorological records. This
directly suggests to split the curve z into pieces of the same length, sharing a common
structure. These new curves are denoted as xn ≡ xn(·), for n ≤ N , and are usually
rescaled to the interval [0, 1] in order to make them independent of the time unit of
measure. Each of these curves xn are understood to be a realization of the corresponding
random process Xn, which is a random variable whose samples are functions instead of
real numbers. The set of these processes Xn, indexed by n ∈ Z, is known as functional
time series (see Alvarez-Li´ebana (2017) for details). In this work we focus on predicting
the curve xn given the previous ones.

In practice, these curves are usually recorded as high-dimensional vectors with highly
correlated entrances. Then, the need of dimension reduction techniques that take into
account the continuous nature of the data arises. We propose to replace the whole curves
xn(·) with the p most relevant evaluations xn(t1), . . . , xn(tp) for the prediction of xn+1(·)
(in a similar sense as in Kargin and Onatski (2008) and Mokhtari and Mourid (2003)).
This is equivalent to directly select the points t1, . . . , tp in [0, 1] under a suitable opti-
mality criterion. Although we end up with a ﬁnite dimensional vector, the problem is
fully functional, since the deﬁnition of this criterion is based on the whole curves. This
technique is commonly know as variable selection. Its main advantage in comparison with
other dimension reduction techniques is the interpretability in terms of the original data,
which is usually desired in real applications. For instance, one of the real data sets tested
is the daily indoor temperature of a house which uses solar energy. With our technique
we detect that one of the most relevant points to predict the temperature one day is 8:00
of the previous morning, which is not evident at ﬁrst sight.

In the present paper we introduce a predictor based on a functional autoregressive
(AR) model which will be especially suitable for variable selection purposes. Bosq (2000)
gives a good introduction into the ﬁeld of linear processes in function spaces and introduces
functional autoregressive processes in depth. AR models have shown to be a valuable tool
in functional time series analysis since they combine computational tractability with suf-
ﬁcient generality (e.g. Besse and Cardot (1996); Kargin and Onatski (2008); Didericksen
et al. (2012); Aue et al. (2015)). In particular, we adapt the methodology introduced by
Berrendero et al. (2018) for the problem of scalar regression for independent data. We
deﬁne a new functional autoregressive model based on the Reproducing Kernel Hilbert
Space (RKHS) generated by the auto-covariance function of the time series.

Classical approaches
The standard assumption in the literature is that L2[0, 1], the space of square-integrable
functions on [0, 1], is the space into which the curves fall. This is a sensible choice, since
L2[0, 1] is a separable Hilbert space and oﬀers desirable geometric properties through the
deﬁnition of the natural scalar product. However, considering our variable selection pur-
pose, the main drawback of L2[0, 1] is that, strictly speaking, it consists of equivalence
classes of functions. That is, two functions represent the same L2-function if the set where
they diﬀer has measure zero. In other words, for any particular point s ∈ [0, 1], the value
f (s) is not well-deﬁned for any function f in this space.

2

Since we require pointwise evaluations xn(s) of the curves, the space of continuous
functions on [0, 1], C[0, 1], which is a Banach space with the supremum-norm, is a more
natural space to work in. In addition, the subsequent change of norm allows us to obtain
uniform convergence results. The problem of estimating AR models in C[0, 1] has been
already addressed in the literature (e.g. Ruiz-Medina and ´Alvarez-Li´ebana (2018) and
references therein). The usual methodology is to project the curves onto a ﬁnite dimen-
sional subspace of C[0, 1], spanned by some eigenfunctions of the covariance operator of
the data, as in Pumo (1998). Some limitations of this principal component approach have
been discussed extensively in the literature. For instance, the resulting space is shown
to be optimal in order to represent the variability of the process, but the dependence
might be lost by the dimension reduction (Kargin and Onatski (2008) and H¨ormann et al.
(2015)). Furthermore, Bernard (1997) indicates the sensitivity of the proposal to small
errors in the estimation of small eigenvalues.

Some relevant results of this work
In this paper the projection on a ﬁnite dimensional space is replaced by the choice of
the evaluations xn(t1), . . . , xn(tp). In the cases relevant for variable selection, it is shown
that the new model falls into the wide class of Banach space–valued processes (ARB(q))
introduced in Bosq (2000), which directly gives us suﬃcient conditions for the existence
of a unique stationary solution. Notably, for instance the well-known Ornstein-Uhlenbeck
process satisﬁes our model. In these cases we are able to prove, under some standard
conditions, almost sure convergence of the estimated points and also of the estimated
curves, both uniformly and in L2[0, 1]. In this setting our predictor coincides with the
optimal one, in the sense that it is the best probabilistic predictor, as stated in Mokhtari
and Mourid (2003). In addition, we develop a consistent estimator for the number p of
relevant variables to select.

The advantages of predicting autoregressive processes with this new approach are
numerous, besides the ones already mentioned. From an applied perspective, the proposed
method is ﬂexible concerning the structure of the data: whether it is recorded on a grid or
available as continuous functions - the methodology remains similar with slight technical
diﬀerences. Nevertheless, for theoretical considerations the data is assumed to be given
in a fully functional fashion. Besides, the use of this RKHS based model avoids the need
of inverting the covariance operator since this is carried out, in some sense, by the inner
product of the space.

In order to show the practical relevance of the method, a simulation study is con-
ducted. To evaluate the performance in the real world four real data sets are studied.
The execution times of the tested methods are also analyzed. Our proposal is competitive
both in prediction accuracy and computational eﬃciency.

Related literature
The literature in the ﬁeld of functional time series analysis is developing quickly. Re-
cent publications include time-domain methods like H¨ormann and Kokoszka (2010), where
a weak dependence concept is introduced, Aue et al. (2015), Klepsch and Kl¨uppelberg
(2017) and Klepsch et al. (2017), where prediction methodologies based on linear mod-
els are developed, and Aue and Klepsch (2017), where an estimator of functional linear
processes based on moving average model ﬁtting is derived. Besides, another examples of

3

statistical papers taking advantage of the usefulness of Reproducing Kernel Hilbert Spaces
are, among others, Hsing and Eubank (2015); Kadri et al. (2015); Berrendero et al. (2017,
2018) and Yang et al. (2018).

Some interesting variable selection techniques for functional regression with indepen-
dent observations are Aneiros and Vieu (2014); Delaigle et al. (2012); McKeague and Sen
(2010); Ferraty et al. (2010) and (Shi and Choi, 2011, Sec. 4.3). See also Section 7.3
of Cuevas (2014) for a brief review. There are also some proposals for feature selection
on multivariate time series, like Fan and Lv (2010); Lam et al. (2012); Liu (2014); Tran
et al. (2015) and the references therein. However, as far as we know, there are no previ-
ous approaches to variable selection for continuous time series in the same sense as it is
presented here.

Organization of the document
The paper is organised as follows. After introducing the notation and some back-
ground on RKHS theory, we deﬁne in Section 3 the new functional autoregressive model
for variable selection.
In Appendix A the mathematical properties of such model are
carefully studied. The asymptotic properties of the sample estimators are shown in Sec-
tion 4. In this section the estimator for the number p of relevant variables is presented.
Sections 5 and 6 include the experimental study along with some practical consideration
for the implementation. Some proofs, which are mainly based on the theory developed in
Berrendero et al. (2018), are included in Appendix B. Appendix C includes some tables
and pseudo-codes.

2 Methodology

2.1 Notation

Our object of interest is a random curve observed over (−∞, N ], for N ∈ Z (or over
[0, N ]).
In order to better handle its behavior when N increases, the whole curve is
usually split into intervals of the same length. Then, each piece is a function rescaled
to [0, 1], randomly generated from a stochastic process Xn, for n ∈ Z and n ≤ N . A
stochastic process can be thought as a random variable whose samples, denoted as xn,
are functions. Such a set of stochastic processes is known as a functional time series.

We assume that the curves inside each interval of length one are continuous, that is,
each Xn takes values in C[0, 1] (the space of continuous functions over [0, 1]). We denote
by (cid:107) · (cid:107) the supremum norm in this space,

(cid:107)f (cid:107) = sup
s∈[0,1]

|f (s)|,

for f ∈ C[0, 1].

A standard assumption is that the random variable sups∈[0,1] |Xn(s)| has ﬁnite variance.
In this case each evaluation Xn(s), for s ∈ [0, 1], also has ﬁnite variance. A functional time
series satisfying this condition is said to be stationary if its mean functions E[Xn] do not
change with n and Cov(cid:0)Xn+r(s), Xn(t)(cid:1) equals Cov(cid:0)Xr(s), X0(t)(cid:1), for every s, t ∈ [0, 1],
n, r ∈ {0} ∪ N. Under stationarity the lagged covariance function, Cov(cid:0)Xr(s), X0(t)(cid:1),
is denoted as cr(s, t). For the sake of clarity in the equations, and since the time series
throughout this work are stationary, we make the following abuse of notation: we assume
to work with the centered processes Xn − E[Xn], denoted simply by Xn.

4

We denote the vectors of points as Tp = (t1, . . . , tp) ∈ [0, 1]p, where the subindex
indicates the dimension. The covariance matrix of the random variables Xn(t1), . . . , Xn(tp)
indexed by Tp is ΣTp. Moreover, for a general function f : [0, 1] → R, the evaluation f (Tp)
is understood to be the column vector with entries f (tj), tj ∈ Tp. Similarly for functions
in two variables.

As usual in statistics, we use a hat to denote the estimations derived from the samples.

In addition, an asterisk will indicate the optimal quantities under some criterion.

2.2 Some background on Reproducing Kernel Hilbert Spaces

The following discussion, although interesting, is not directly needed in order to apply
the proposed method in practice. Therefore, a more applied reader could directly skip to
Section 3 if desired.

Let X(·) ≡ Xn(·) be a centered stochastic process in [0, 1] such that X(s) has ﬁnite
variance for all s ∈ [0, 1]. Being c0(s, t) the auto-covariance function (or covariance kernel)
of the process, we can deﬁne the auxiliary space

H0(X) := (cid:8)f ∈ L2[0, 1]

: f (·) =

p
(cid:88)

i=1

aic0(ti, ·), ai ∈ R, ti ∈ [0, 1], p ∈ N(cid:9)

(1)

with inner product (cid:104)f, g(cid:105)H0 = (cid:80)
i aic0(ti, ·) and g(·) =
(cid:80)
j bjc0(sj, ·). The RKHS associated with c0, H(X), is the completion of this space; all
the functions of H0(X) plus their limits when p → ∞ with respect to the norm (cid:107)·(cid:107)H0. The
norm in the RKHS is the continuous extension of the norm in H0(X) and it is denoted
as (cid:107) · (cid:107)H. Thus, (cid:107)f (cid:107)H coincides with (cid:107)f (cid:107)H0 for all function f ∈ H0(X).

i,j aibjc0(ti, sj), where f (·) = (cid:80)

Reproducing Kernel Hilbert Spaces appear occasionally in the literature of functional
data and machine learning, since they are useful to impose smoothness conditions and help
to reduce noise and irrelevant information. However we use them with a diﬀerent goal,
since our interest lies in variable selection. Therefore, the so-called reproducing property
of these spaces is particularly useful.
It states that, for all f ∈ H(X) and s ∈ [0, 1],
(cid:104)f, c0(s, ·)(cid:105)H = f (s). That is, the auto-covariance function behaves, in some sense, like
Dirac’s delta. We are interested in selecting variables on the trajectories drawn from the
process X(·). However, in general, the realizations of the process do not belong to H(X)
with probability one (e.g., Theorem 11 of Pillai et al. (2007)). Thus, this reproducing
property can not be directly applied to the trajectories.

In order to circumvent this issue, we use another Hilbert space also closely related to

the process,

L0(X) = (cid:8)U : U =

p
(cid:88)

i=1

aiX(ti), ai ∈ R, ti ∈ [0, 1], p ∈ N(cid:9).

As before, L(X) is the closure of this space. All the random variables in this space have
ﬁnite variance, which coincides with their norms.

Remark 1. By deﬁnition of both spaces, the ﬁnite sums (cid:80)p
and the ﬁnite sums (cid:80)p
every function of these spaces can be arbitrarily well approximated with a ﬁnite sum).

i=1 aic0(ti, ·) are dense in H(X)
i=1 aiX(ti) are dense in L(X), with their corresponding norms (i.e.

5

These two spaces L(X) and H(X) can be connected using the following congruence
(bijective transformation preserving the inner product), named Lo`eve’s isometry (Berlinet
and Thomas-Agnan (2004, Theorem 35) and Luki´c and Beder (2001, Lemma 1.1))

ΨX : L(X) → H(X)

U

(cid:55)→ E[U X(·)].

(2)

If the random variable U is an element of L0(X), i.e. U = (cid:80)p
isometry is given by

i=1 aiX(ti), its image by the

(cid:104)
(cid:0)ΨX(U )(cid:1)(·) = E

X(·)

(cid:105)
aiX(ti)

=

p
(cid:88)

i=1

p
(cid:88)

i=1

aiE[X(·)X(ti)] =

p
(cid:88)

i=1

aic0(ti, ·).

(3)

Therefore, when applying the inverse of ΨX to a function in H0(X) we obtain a ﬁnite
combination of evaluations X(ti). That is, replacing the inner product in H(X) with
Ψ−1
X (c0(s, ·)) = X(s).
Next we see how to use this isometry to perform variable selection in AR processes.

X we recover the Dirac’s delta behavior for the trajectories, since Ψ−1

3 Model deﬁnition and variable selection

Given xn ∈ C[0, 1], n ∈ Z, trajectories drawn from a functional time series Xn, the
standard autoregressive model is of the form (see Chapter 6 Bosq (2000))

xn = ρ(xn−1) + εn,

n ∈ Z,

(4)

for some white noise process εn, n ∈ Z, in C[0, 1] and some bounded linear operator ρ (i.e.
supf (cid:107)ρ(f )(cid:107) ﬁnite for f ∈ C[0, 1] and (cid:107)f (cid:107) ≤ 1). In this section we propose a particular
functional AR model “customized” to give a well-founded framework for variable selection.

We work with a centered stationary time series Xn in C[0, 1] such that E(cid:2)(cid:0) sups |Xn(s)|(cid:1)2(cid:3)

is ﬁnite. In order to perform variable selection we propose to deﬁne the model

Xn(·) =

p
(cid:88)

j=1

αj(·)Xn−1(tj) + εn(·),

(5)

where αj(·) are continuous functions in [0, 1] and εn is a strong C[0, 1]-white noise point-
wisely uncorrelated with Xn. That is, all the curves depend on the same set of points
t1, . . . , tp, regardless of the index n. Comparing this expression with the standard AR
model of Equation (4), we are using ρ(f )(·) = (cid:80)p
j=1 αj(·)f (tj), for f ∈ C[0, 1]. The
following example, taken from Bosq (2000), shows that this model class has good amount
of generality.

Example 1. Let Z be a continuous version of the Ornstein-Uhlenbeck process,
(cid:90) s

Z(s) =

e−θ(s−t)dB(t),

−∞

where B is a standard Brownian motion. If we deﬁne Xn(s) = Z(n + s) for s ∈ [0, 1],
Xn(s) can be rewritten as e−θsXn−1(1) + εn(s), where now εn is a white noise given by
(cid:82) n+s
e−θ(n+s−t)dB(t). That is, Equation (5) is fulﬁlled with p = 1, t1 = 1 and α1(s) =
n
e−θs.

6

This is reasonable, since O.U. processes are Markov, which means that the probability
events involving Xn(·) only depend on Xn−1(1). For real data sets one can not ensure
whether model (5) holds, but the results in Section 6 reveal that it is, at least, a good
approximation of reality. In any case, it is important to emphasize that in order to carry
out the variable selection we do not assume that the process ﬁts model (5). We merely
search the points t1, . . . , tp such that an expression as in (5) approximates the real process
Xn according to some criterion.

Then, we propose the following deﬁnition.

Deﬁnition 1. A functional time series Xn such that E(cid:2)(sups |Xn(s)|)2(cid:3) < ∞, n ∈ Z, is
called Functional Continuous Autoregressive process of order 1 (FCAR(1)) if it is station-
ary and it can be expressed as in Equation (5).

As pointed out in Appendix A, a suﬃcient condition for a FCAR(1) process to have
i=1 (cid:107)αi(cid:107) < 1, which is an easily veriﬁable

a unique strictly stationary solution is that (cid:80)p
condition.
Assumption 1. Xn, n ∈ Z is a FCAR process such that (cid:80)p

i=1 (cid:107)αi(cid:107) < 1.

An extension of model FCAR(1) to FCAR(q) can be carried out whenever Xn(s) =
Z(s + n) for s ∈ [0, 1] and Z is a stationary process with continuous trajectories. All the
theory presented in the paper remains valid in this case, with some intuitive additional
assumptions on the model parameters. For the sake of clarity we restrict ourselves to
q = 1. Nevertheless, we include some comments along the document to clarify the changes
due to this extension. One of the necessary generalizations is to split the process as
Zn,q(s) = Z(s + n − q + 1), s ∈ [0, q]. Now L(Zn,q) is the space generated by the random
variables Xn(s), . . . , Xn−q(s) and H(Zn,q) is the RKHS associated with this lagged process,
whose reproducing kernel c0(s, t) for s, t ∈ [0, q] is the auto-covariance function of Zn,q.
Thus, Equation (5) is rewritten as

Xn(·) =

p(1)
(cid:88)

j=1

α(1)

j (·) Xn−1

(cid:0)t(1)

j − q + 1(cid:1) + . . . +

p(q)
(cid:88)

j=1

α(q)

j (·) Xn−q

(cid:0)t(q)

j

(cid:1).

An extension of model (5) is analyzed hereunder, making clear that this approach
includes a wide class of processes, and not only the ﬁnite approximations of Equation (5).

Model (5) is a particular case of the more general (and fully functional) model,

Xn(·) = Ψ−1

Xn−1

(φ(·, (cid:63))) + εn(·),

n ∈ Z,

(6)

Xn−1

where φ ∈ H(X) and Ψ−1
is the inverse of the Lo`eve’s isometry deﬁned in Equation (2).
This more general formulation is useful when trying to prove asymptotic results on the
model, although it is not directly used in practice. As it is sometimes the case, a slight
generalization helps to make theory simpler. Another beneﬁt of changing the work space
to H(X) is that ﬁnding a solution of this model does not require to invert the covariance
operator. It could be understood as if the “inversion” was intrinsically carried out by the
Lo`eve’s isometry. By means of Equation (3), it is easy to see that model (5) is recovered
for

φ(s, ·) =

αj(s)c0(tj, ·) ∈ H(X).

(7)

p
(cid:88)

j=1

7

In fact, both models are almost equivalent since, as pointed out in Remark 1, these
ﬁnite linear combinations are dense in H(X) (at the end of Appendix A it is included a
convergence result in this regard). Therefore, any possible function φ(s, ·) in (6) can be
approximated arbitrarily well by just increasing the number of points p. Besides, in most
cases H(X) is a dense subspace of L2[0, 1]. The practical examples displayed in Section
6 demonstrate that a small number p is usually enough to obtain good approximations.
These experimental results also support the claim that any function can be approximated
as in (7), since the method performs well even when the model is not satisﬁed.

3.1 Optimality criteria

We now focus on the optimality criterion for variable selection. The main objective of the
regression method is to predict Xn(·) given Xn−1(·). Since each Xn(s) for s ∈ [0, 1] has
ﬁnite variance, the ﬁrst approach could be to minimize the variances of the residuals

q(Tp ; α1, . . . , αp)(s) = E

(cid:104)(cid:16)

Xn(s) −

αj(s)Xn−1(tj)

(cid:17)2(cid:105)

,

p
(cid:88)

j=1

in the same spirit as in Berrendero et al. (2018), where the coeﬃcients αj(s) (which are
just real numbers) depend on the points t1, . . . , tp. Due to the stationarity of the process,
q does not change with n.

In model (5) all processes Xn(·) depend on the same set of points t1, . . . , tp, indepen-
dently of s. Therefore, we have to ﬁnd the functions αj(·) such that for each s ∈ [0, 1]
the evaluations αj(s) give the best approximation of Xn(s) for a given set of points Tp.
Then, integrating q2 over s leads to

Q(Tp) :=

(cid:90) 1

0

min
αj (s)∈R

q(Tp ; α1, . . . , αp)2(s) ds.

(8)

This function Q can now be minimized with respect to Tp. From Berrendero et al. (2018)
we know that q2(s) is a convex function in αj(s) for each s ∈ [0, 1]. Thus, we can obtain
an explicit expression of the minimizing functions, denoted by α∗
j (s), pointwise for each
s ∈ [0, 1]. We see in the proof of Proposition 1 that these optimal functions are given by

(α∗

1(s), . . . , α∗

p(s)) = Σ−1
Tp

c1(s, Tp),

where c1(·, Tp) = (c1(·, t1), . . . , c1(·, tp))(cid:48) is the vector of lagged-covariances.

Up to now we have assumed that all points tj belong to [0, 1]. However, if we want
to have identiﬁability of the set Tp ∈ [0, 1]p, we need to restrict the search to a compact
subset of this space. This problem has been solved in diﬀerent ways in the literature. The
chosen solution is to work, for some δ > 0, in

Θp = {Tp = (t1, . . . , tp) ∈ [0, 1]p : ti+1 − ti ≥ δ, for i = 1, . . . , p},

which is the space proposed in Berrendero et al. (2018). Other possibilities can be studied,
for instance the space used by Ji and M¨uller (2016). The choice of a value δ > 0 is mainly
technical, to avoid problems with the invertibility of the covariance matrices. In addition
it allows us to obtain meaningful sets Tp, since it discards reoccuring points. It is also not

8

a strong restriction in practice, since usually the data is given in a discretized fashion,
and the value δ can be chosen as small as desired. However, as pointed out in Berrendero
et al. (2018), all the theory remains valid for δ = 0, by simply adjusting everywhere the
value p to the dimension of the vector without repeated entries.

Although the optimality criterion deﬁned by Q is theoretically sound, it lacks an easily
computable expression. We see in the following result that it can be rewritten in a more
feasible way.

Proposition 1. Let Xn, n ∈ Z be a FCAR(1) process satisfying Assumption 1 and with
E(cid:107)ε2

n(cid:107) < ∞. Then,

arg min
Tp∈Θp

Q(Tp) = arg max
Tp∈Θp

Q0(Tp),

where Q0(Tp) := (cid:82) 1

0 c1(s, Tp)(cid:48)Σ−1
Tp

c1(s, Tp)ds.

The proof of this result is an extension to the current setting of the proof of Proposition

1 in Berrendero et al. (2018) and can be found in Appendix B.

For the criterion of the F CAR(q) with q > 1 we have to substitute in this proposition

the function c1(s, t) by the continuous picewise-deﬁned function

c(s, t) = ci(s, t − q + i) for t ∈ (q − i, q − i + 1] and s ∈ [0, 1].

(9)

We need the additional assumption that the random variables Zn,q(s) for each s are all
linearly independent, to ensure the invertibility of the covariance matrices of Zn,q evaluated
in (t(1)
p(q)) ∈ [0, q]p. This assumption introduces some further restrictions to the
model. For instance, the functions α(i)

j (s) can not vanish for s ∈ [0, 1] and 1 < i ≤ q.

1 , . . . , t(q)

3.2 Estimation from the sample

As mentioned, the optimality criterion deﬁned by Q0 is simple to implement in practice.
In this section we study the asymptotic properties of the natural estimator of Q0. These
results are useful when studying the asymptotic behavior of the selected points and the
estimated trajectories in the next section. We work with a sample x1, . . . , xm of size m
drawn from a FCAR(1) process satisfying Assumption 1. Then, for a given number of
points p, the natural estimator for the functions Q0(Tp) is

(cid:98)Q0

m(Tp) =

(cid:90) 1

0

(cid:98)c1(s, Tp)(cid:48) (cid:98)Σ−1

Tp (cid:98)c1(s, Tp)ds,

(10)

where (cid:98)c1(·, Tp)(cid:48) = ((cid:98)c1(·, t1), . . . , (cid:98)c1(·, tp)) and (cid:98)c1 is the usual estimator of the covariance
function

(cid:98)c1(s, tj) =

1
m − 1

m−1
(cid:88)

i=1

xi+1(s)xi(tj).

The entries of the sample covariance matrix, (cid:98)c0(ti, tj), ti, tj ∈ Tp, are computed equiv-
alently. According to this criterion, we propose to select as the most relevant points

(cid:98)Tp,m = arg max
Tp∈Θp

(cid:98)Q0,m(Tp).

(11)

9

In Section 4 we prove some consistence results for this estimator, under the assumption
that the ﬁnite dimensional model deﬁned by Equation (5) holds. To this end, we ﬁrst
need convergence results of the sample covariances involved in the expression of (cid:98)Q0
m. The
main one is based on a result of Pumo (1998).

Lemma 1. Assume that Xn, n ∈ Z is a FCAR(1) process satisfying Assumption 1 and:

H1. The process Xn(t)Xn(s) for t, s ∈ [0, 1] is uniformly geometrically strong mixing.

H2. (Cramer conditions) For every t, s ∈ [0, 1] there exist d > 0 and D < ∞ such that

• d ≤ E[X 2

0 (t)X 2
• E|X0(t)X0(s)|k ≤ Dk−2k! E[X 2

0 (s)] ≤ D

and

0 (t)X 2

0 (s)]

for k ≥ 3.

Then,

sup
t,s∈[0,1]

sup
t,s∈[0,1]

|(cid:98)c0(s, t) − c0(s, t)| a.s.→ 0
|(cid:98)c1(s, t) − c1(s, t)| a.s.→ 0.

and

(12)

(13)

Proof. By Lemma 1 of Pumo (1998) we know that for some positive constants A1, A2, A3,

(cid:32)

P

sup
t,s∈[0,1]

|(cid:98)c0(s, t) − c0(s, t)| ≥ ε

≤ (2

(cid:33)

m + A1) exp (cid:0)−A2ε2√
√
√

5 m exp (cid:0)−log(r−1)

2

+A3ε

m(cid:1)

m(cid:1) ,

where m is the sample size and 0 < r < 1 is given by assumption H1. By Borel-Cantelli,
if the sums over m of these probabilities are ﬁnite for every ε > 0, we get the almost sure
convergence stated in Equation (12). The sum is of order

∞
(cid:88)

P

m=1

(cid:32)

sup
t,s∈[0,1]

|(cid:98)c0(s, t) − c0(s, t)| ≥ ε

∼ 2

(cid:33)

√

eCε

∞
(cid:88)

m=1

m
√
m

+

∞
(cid:88)

m=1

A1
√

eCε

m

+ Dε

∞
(cid:88)

m=1

m
√
eCr

,

m

where Cε, Cr, Dε > 0 and these three series converge, for example by the limit comparison
test with (cid:80) m−γ, γ > 1. Concerning (13), the same Lemma 1 of Pumo (1998) states that
the bounds for these probabilities are equivalent but with m − 1 in place of m.

These Cramer conditions appear often in the literature related with limit theorems
for AR processes in Banach spaces. For instance, all bounded processes satisfy them, and
also the Ornstein-Uhlenbeck process of Example 1. In the latter case, |Xn(s)Xn(t)| =
e−k(t+s)Xn−1(1)2, then, e−k(t+s)EX0(1)2k ≤ Dk−2k!e−2(t+s)EX0(1)4, where X0(1) follows
a N (0, 0.5). Using the expression for the moments of a Gaussian variable,

e−k(t+s) (2k)!
22kk!

≤ Dk−2e−2(t+s) 3k!
4

,

which is satisﬁed, for instance, for D ≥ 5e−2/12.

Given the previous result, we show the uniform convergence of the sample criterion

function to its population counterpart and that both functions are continuous on Θp.

10

Lemma 2. Assume that Xn satisﬁes the same hypotheses as in Lemma 1. Let p ≥ 1 be
such that the covariance matrices ΣTp are invertible for all Tp ∈ Θp. Then Q0 and (cid:98)Q0,m
are continuous on Θp and supTp∈Θp | (cid:98)Q0

m(Tp) − Q0(Tp)| a.s.→ 0.

pointwise properties of the integrands of Q0 and (cid:98)Q0

The proof of this result can be found in Appendix B, and it is partially based on the
m shown in Berrendero et al. (2018).
For the case of greater order F CAR(q) with q > 1, this result, and therefore all the
results of the next section, hold whenever the process Zn,q fulﬁls assumptions H1 and H2
of Lemma 1. This is equivalent to suppose that all the products Xi(t)Xj(s) satisfy H1
and H2 for 0 ≤ i, j ≤ q − 1.

4 Asymptotic results

In the previous section we introduced the variable selection method in a general context.
We have presented an optimality criterion that can be used to select the p most relevant
points, without imposing additional restrictions to the model that generates the data. If
we assume that the data is generated by the ﬁnite dimensional model of Equation (5),
additional asymptotic results for the estimator (cid:98)Tp,m and for the estimated curves can be
derived. Therefore, in this section we assume that the kernel depends only on p∗ points:

φ(s, ·) =

p∗
(cid:88)

j=1

αj(s)c0(t∗

j , ·),

(14)

for all s ∈ [0, 1], where p∗ is the minimum integer such that this expression holds. We
denote by T ∗ = T ∗
p∗ ∈ Θp∗ the set of points that generate the model. Two questions
arise; how good is our estimator (cid:98)Tp∗,m when searching the real points T ∗, and how can we
estimate the real number of points p∗.

4.1 Estimated points and trajectories

From the expression of Q given in Equation (8), one sees that given (14), the set T ∗ is
a global minimum of Q on Θp∗, and therefore a global maximum of Q0. Assuming for
now that p∗ is known, we can prove that this optimum is unique and the estimated points
(cid:98)Tp∗,m converge to the real ones T ∗. This result is an extension of Theorem 1 of Berrendero
et al. (2018) and its proof is included in Appendix B.

Theorem 2. Under the assumptions of Lemma 2 for p = p∗, whenever (14) holds and
the covariance matrices ΣTp∗ ∪Sp∗ are invertible for all Tp∗, Sp∗ ∈ Θp∗ with Tp∗ (cid:54)= Sp∗, then:

(a) The vector T ∗ ∈ Θp∗ is the only global maximum of Q0 on this space.

(b) (cid:98)Tp∗,m

a.s.→ T ∗ with the sample size m → ∞, where (cid:98)Tp∗,m is given in Eq.(11) with

p = p∗.

(c) (cid:98)Tp∗,m converges to T ∗ in quadratic mean when m → ∞.

11

Once that we have selected the most relevant points from the sample, we want to

estimate the trajectories of the process. That is, we want to approximate

Xn(·)T ∗ = α1(·)Xn−1(t∗

1) + . . . + αp∗(·)Xn−1(t∗

p∗).

(15)

In the proof of Proposition 1 we have seen that the functions (α1(·), . . . , αp∗(·))(cid:48) used
to carry out this projection are given by Σ−1
p∗)). Therefore, we can
construct the estimated curves as (cid:98)Xn(·)
as (cid:98)α1(·)Xn−1((cid:98)t1) + . . . + (cid:98)αp∗(·)Xn−1((cid:98)tp∗),
(cid:98)Tp∗,m
where now the functions ((cid:98)α1(·), . . . , (cid:98)αp∗(·))(cid:48) are computed using the sample version of the
covariances as (cid:98)Σ−1
( (cid:98)c1(·, (cid:98)t1), . . . , (cid:98)c1(·, (cid:98)tp∗)). Thus our proposed estimator for Xn(·)T ∗ is

1), . . . , c1(·, t∗

Tp∗ (c1(·, t∗

(cid:98)Tp∗ ,m

(cid:98)Xn(·)

(cid:98)Tp∗,m

= (cid:98)c1(·, (cid:98)Tp∗,m)(cid:48) (cid:98)Σ−1

(cid:98)Tp∗,m

Xn−1( (cid:98)Tp∗,m).

(16)

Under the same conditions of the previous theorem, we can see that this estimator con-
verges to Xn(·)T ∗ uniformly a.s. and in quadratic mean.

Theorem 3. Under the same assumptions of Theorem 2 and when the sample size m →
∞,

(a) (cid:98)Xn(·)

(cid:98)Tp∗,m

converges to Xn(·)T ∗ a.s. in C[0, 1]:

sups

(cid:12)
(cid:12) (cid:98)Xn(s)

(cid:98)Tp∗,m

− Xn(s)T ∗

(cid:12)
(cid:12)

a.s.→ 0.

(b) If, in addition, there exists η > 1 such that E(cid:107)|Xn|2η(cid:107) < ∞, then one also gets

E[(sups | (cid:98)Xn(s)

(cid:98)Tp∗,m

− Xn(s)T ∗|)2] → 0.

The proof of this theorem is based on the one of Theorem 2 of Berrendero et al.
(2018) and can be found in Appendix B. In fact, as shown in Proposition 2 of Mokhtari
and Mourid (2003), if the true kernel of the model is as in Equation (7), the best linear
predictor based on Xn−1(t1), . . . , Xn−1(tp) is the best probabilistic predictor of (Xn − εn).

4.2 Number of relevant points

We are left with deriving an estimator for p∗, the number of points to select. Notice
that the optimality criterion (Equation (8)) does not reach its minimum value for p < p∗.
Additionally, there is no room for improvement using p > p∗. Therefore, the number p∗
would be the smallest p such that the minimum value of Q(Tp) (or the maximum of Q0)
remains unchanged when increasing p.

The sample version of this idea would be as follows. Deﬁning

∆ = min
p<p∗

and ﬁxing some 0 < (cid:15) < ∆, we set

(Q0(T ∗

p+1) − Q0(T ∗

p )) > 0

(cid:110)

(cid:98)pm = min

p : max

Tp+1∈Θp+1

(cid:8)

m(Tp+1)(cid:9) − max
(cid:98)Q0

Tp∈Θp

(cid:8)

m(Tp)(cid:9) < (cid:15)
(cid:98)Q0

(cid:111)
.

(17)

The value ∆ is merely theoretical, and should be understood as taking an (cid:15) small enough.
The issue of choosing the threshold (cid:15) in practice is tackled in the following section.

This estimator is a.s. consistent for the real number of relevant variables.

12

Theorem 4. Suppose that assumptions of Lemma 2 hold for p ≤ p∗ and that p∗ is the
smallest integer such that Equation (14) is satisﬁed. Then the estimator given by Equation
(17) fulﬁlls (cid:98)pm
Proof. We can prove similar results as the ones given in Lemma 4 of Berrendero et al.
(2018) using the same reasoning, with the only diﬀerence that now Q0(T ∗) = (cid:82) 1
0 (cid:107)φ(s, ·)(cid:107)2
(as in the proof of Proposition 1).

a.s.→ p∗.

Hds

5 Experimental setting

In this section we introduce the data sets which appear along the experiments (both sim-
ulated and real), as well as other methods of the literature used for comparison. We start
making a couple of theoretical comments that ease the implementation of the method. A
general pseudo-code is provided in Appendix C.

5.1 Practical considerations

Greedy approximation

In order to obtain the most relevant points in practice, we maximize the expression
of (cid:98)Q0
m (given in Equation (10)) in the p-dimensional space Θp. However, due to compu-
tational limitations, this optimization is not feasible even for relatively small values of p.
Therefore, a greedy approximation is carried out. We can decompose the function Q0 in a
way that directly suggests an iterative approximation to this optimization problem. If the
vector Tp+1 ∈ θp+1 is such that it contains all the entries of Tp plus a new one tp+1 ∈ [0, 1],
using Equation (16) of Berrendero et al. (2018) we can write,

Q0(Tp+1) =

(cid:90) 1

0

c1(s, Tp+1)(cid:48) Σ−1
Tp+1

c1(s, Tp+1) ds

= Q0(Tp) +

0 Cov(cid:0)Xn(s) − Xn(s)Tp, Xn−1(tp+1)(cid:1)2 ds
(cid:82) 1
Var(cid:0)Xn−1(tp+1), Xn−1(tp+1)Tp

(cid:1)

,

where Xn(·)Tp and Xn−1(·)Tp are the same kind of projections deﬁned in Equation (15). For
each ﬁxed value s ∈ [0, 1], the integrand of this quotient recalls the classical multivariate
“forward selection” method. The crucial diﬀerence here is that the variables are not
selected among a ﬁxed ﬁnite set but among the whole interval [0, 1]. This derivation can
be also done using the sample counterpart of Q0,

(cid:98)Q0

m(Tp+1) = (cid:98)Q0

m(Tp) +

0 (cid:100)Cov(cid:0)Xn(s) − Xn(s)Tp, Xn−1(tp+1)(cid:1)2 ds
(cid:82) 1
(cid:100)Var(cid:0)Xn−1(tp+1), Xn−1(tp+1)Tp

(cid:1)

.

Then the proposed algorithm selects at each step the point tp+1 that maximizes this quo-
tient. The starting point would be the one that maximizes (cid:98)Q0
0 (cid:98)c1(s, t)2ds.
As usual when dealing with greedy algorithms, this approximation does not guaranty that
the global maximum of (cid:98)Q0

m is reached. However it performs well in practice.

m(t) = (cid:98)c0(t, t)−1 (cid:82) 1

In order to compute this quotient we can use a similar reasoning as in the proof of

Proposition 2 of Berrendero et al. (2018) and rewrite the previous equation as,
Tp (cid:98)c0(tp+1, Tp) − (cid:98)c1(s, tp+1)(cid:1)2 ds
Tp (cid:98)c0(tp+1, Tp)

(cid:98)c0(tp+1, tp+1) − (cid:98)c0(tp+1, Tp)(cid:48) (cid:98)Σ−1

(cid:0)
(cid:98)c1(s, Tp)(cid:48) (cid:98)Σ−1

m(Tp+1) = (cid:98)Q0

m(Tp) +

(cid:82) 1
0

(cid:98)Q0

,

(18)

13

where (cid:98)c1(s, Tp) is the vector whose entries are given by the sample covariances (cid:100)Cov(Xn(s),
Xn−1(tj)), and equivalently for (cid:98)c0.

Grid and covariance matrices
Due to computational limitations, the search of the most relevant points should be
done on a grid of [0, 1].
If the data is given in a discretized fashion, then the grid is
directly given by the data. However, if it is fully functional, the grid can be deﬁned
(theoretically) arbitrarily ﬁne. Under the assumption that all the covariance matrices
ΣTp are invertible for Tp in this grid, the quotient of Equation (18) is easy to compute. In
addition, depending on the nature of the data, the estimations of the covariances (cid:98)c1(·, Tp)
can be made fully functional or using the values on the ﬁxed grid. Both possibilities are
implemented for the numerical study.

However, for some real data sets the condition of the invertibility of ΣTp may not be
satisﬁed. For instance, if the curves are represented using a Fourier basis, this condition
is rarely satisﬁed, since periodicity is introduced on the data. Thus, other representations
should be used, like splines. If the data is in any case not invertible, it can be always
preprocessed to remove the conﬂictive points of the grid. It would not aﬀect the eﬃciency
of the method, since these points would be linearly dependent of the others, so their
information would be redundant. In addition, if the data set contains outliers, there exist
several well-known methods to robustly estimate the covariance matrices. Furthermore,
if we have some additional information about the covariance structure of the data, it can
be directly incorporated to the method.

For the model of order greater than one, we substitute the sample lagged covariance
function (cid:98)c1(s, t) by the sample version of the picewise-deﬁned function of Equation (9).

Number of selected points
In order to apply the estimator of Equation (17), some value for (cid:15) must be ﬁxed. In
other words, it should be determined for which value of p the quotient of Equation (18)
has converged to zero. The standard approach to this problem is to ﬁx the parameter
by cross-validation. Additionally we test the proposal given in Berrendero et al. (2018);
to apply the usual k-means with k = 2 to the logarithms of the values of the quotient
in Equation (18).
If we denote by Lm(p) the values of these logarithms, (cid:98)pm would be
the minimum p such that all the Lm(p) for p > (cid:98)pm do not belong to the same cluster as
Lm(1). This is equivalent to ﬁx the parameter (cid:15) of Equation (17) to Lm(q) where q is the
largest value such that Lm(q) belongs to the same cluster as Lm(1).

5.2 Methodology

We compare the eﬃciency of the proposal with two other recent methods. Both of them
carry out the dimension reduction using functional principal components. For the forecast-
ing experiments we also compare with two “base” methods that do not reduce dimension.
We indicate in brackets the names used in the tables for each method, which are included
in Appendix C.

• The method proposed in this paper (RKHS) has been implemented in four dif-
ferent ways. As mentioned in the previous section, we use two approaches to se-
lect the number of relevant variables; doing clustering on the maximum values of

14

the (cid:98)Q0
m functions (CL) and by cross-validation (CV). In addition, the points can
be selected by using covariance vectors on a grid or computing purely functional
lagged-covariance functions. We use one or the other depending on the nature of
the data.

• The method proposed in Aue et al. (2015) (fFPE). This proposal uses a dimension
reduction method based on functional principal components analysis to ﬁnd a ﬁnite
dimensional space on which the prediction is performed using a vector autoregressive
model. The model order and dimension of the ﬁnite dimensional space are chosen by
the fFPE criterion. For details, see Aue et al. (2015), where the empirical properties
of the approach are demonstrated in depth. The R-code of this method was provided
by the authors.

• The method proposed in Bosq (2000) and Kokoszka and Reimherr (2013) (KR). This
prediction method by Bosq is the one known as the standard prediction method
for functional autoregressive processes. To determine the order of the functional
autoregressive model to be ﬁtted, we use the multiple testing procedure of Kokoszka
and Reimherr (2013).

• Exact and Naive methods are implemented in order to provide some bounds on
the errors. These methods are also used, for instance, in Horv´ath and Kokoszka
(2012). The exact prediction consists in “predicting” the response directly as
ρ(xn−1). Therefore, it can be only applied for simulated data, since the opera-
tor ρ is unknown for real data sets. It is not really a prediction method but gives
us an idea of the minimum error that we can achieve. The Naive approach simply
predicts (cid:98)xn as xn−1.

Both the maximum number of points to select and the number of principal components
are always limited to 10. For the simulated data all methods are tested using a sample of
size n = 115, where 100 realizations are used for training and the remaining 15 for test.
Each experiment has been replicated 100 times. For the real data sets we use a window
moving approach with ﬁve blocks to obtain several measures of the errors. The size of the
windows is adjusted depending on the sample size of each set. The order of the process is
always limited to 3 for the methods. However, for our proposal we have to set it to order
1 whenever the curves can not be interpreted as xn(s) = z(s + n), with z continuous.

Usually the functional data sets are given in a discretized fashion. Some of the tested
methods require to transform previously the data to truly functional. However, our dis-
crete proposal can also deal with discretized data. In addition, when the data is irregular,
some information could be lost when transforming the data to functional. This compli-
cates the comparison between the diﬀerent methods. Therefore, for this kind of discretized
data sets we measure two diﬀerent types of errors.

• Discrete errors: The error is measured using the original discretized data. The
discrete version of the proposal (the one that uses covariance vectors) is tested. The
predictions returned by the methods that use fully functional data are evaluated on
the same grid given by the data.

• Functional errors: The data are transformed to functions using a Bsplines basis
before applying the methods. We have found that using Bsplines is more suitable

15

in this setting, since the standard Fourier basis introduces periodicity in the data.
For the displayed results 10 functions of the basis are used, but diﬀerent numbers
have been tested without signiﬁcant changes. The functional implementations of the
RKHS proposal, which estimate the whole lagged-covariance functions in a purely
functional way, are tested now.

As we see in the following subsection, one of the simulated sets is purely functional. In this
case only the functional errors are measured. Two diﬀerent norms are used to measure the
error: the standard L2[0, 1] norm and the supremum norm of C[0, 1] that has been used
along the paper. Each of these norms measure diﬀerent characteristics of the predictions.
We also measure two diﬀerent relative errors, given a sample of curves x1, . . . , xm,

e1 =

m
(cid:88)

i=1

(cid:107)xi − (cid:98)xi(cid:107)
(cid:107)xi(cid:107)

,

e2 =

(cid:80)m

i=1 (cid:107)xi − (cid:98)xi(cid:107)
(cid:80)m
i=1 (cid:107)xi(cid:107)

.

(19)

The ﬁrst one gives the same importance to all curves regardless of their norm, while the
second one place more importance to the errors in the curves of biggest norms, since it is
just a scaling of the absolute error.

5.3 Simulated data

We test the diﬀerent methods using simulated sets that fulﬁl the sparsity assumption of
Equation (14) as well as some which not. Most of them are inspired by other data sets
used in the literature. Some realizations of these processes can be found in Figure 1.

• Two data sets satisfying the sparsity assumption with standard Brownian innova-
tions. The real points are T ∗ = (0.3, 0.5, 0.9) with two diﬀerent sets of functions αj.
The ﬁrst ones are logarithms, log((1 + s)j−1) for j = 1, 2, 3 and s ∈ [0, 1], similar to
the function used for the simulated data in Berrendero et al. (2018). The second set
of functions is sin(30πj−1s), since we also want to incorporate a data set with high
variation. When transforming this last data set to purely functional, 30 Bspline
functions are used instead of 10, to be able to capture most of the variation.

• Ornstein-Uhlenbeck process introduced in Example 1. This is the only simulated

set for which Xn(s) = Z(s + n), so that we can use the model F CAR(3).

• FAR process with linearly decaying eigenvalues of the covariance operator (s =
1, . . . , 15). Following the simulation example used in Aue et al. (2015), this set
consists of spanning a D-dimensional space by the ﬁrst D Fourier basis functions,
and then generate random D × D parameter matrices and a D-dimensional noise
process, where the construction ensures a linear decay of the eigenvalues of the
covariance operator. The slow decay of these eigenvalues makes sure that problems
with PCA based methods due to non-invertibility of the covariance operator are
avoided. In this example only the functional errors are measured since it is purely
functional by construction.

16

(a) Sparse with logs.

(b) Sparse with sins.

(c) O.U.

(d) FAR

Figure 1: 25 trajectories of each of the simulated data sets.

5.4 Real data sets

We test also some real data sets, a couple of them already used in other recent papers.

• Particulate matter concentrations (PM10). This data set is used, for instance, in
Aue et al. (2015) and consists on 175 samples. It contains the µgm−1 concentration
in air of a particular substance with aerodynamic diameter less than 10 µm. The
measures were taken each half hour from from October 1, 2010 to March 31, 2011
in Austria. The data is preprocessed in the same way as suggested in Aue et al.
(2015). For the ﬁve windows we take blocks of 115 observations, 100 for training
and 15 for test.

• Vehicle traﬃc data (Traﬃc) presented in Aue and Klepsch (2017). The original data
set was provided by the Autobahndirektion S¨udbayern.
It contains the amount
of vehicles traveling each ﬁve minutes on the highway A92 in Southern Bavaria,
Germany, from January 1 to June 30, 2014. Retaining only working days, we work
with 119 samples divided into 5 windows of size 99; 94 for train and 5 for test.

• Indoor temperature of a “solar house” (Temp). This data set consist in temperature
measures each 15 minutes during 42 days in the living room of a SMLsystem solar
house. The whole data set (which contains other diﬀerent attributes) is studied in
Zamora-Martnez et al. (2014) and it is available in http://archive.ics.uci.edu/
ml/datasets/SML2010. This is the smallest set, so it is divided into 5 windows of
size 34, from which just 2 curves are used for test. For this set we were forced to use
at most 9 PCA components for the fFPE method, in order to avoid computational
errors.

• Utility demand data (Utility) which appears in the book Hyndman et al. (2008) and
is available in the R package “expsmooth”. The original set is made of 126 curves
of hourly utility demand from a company of the the Midwestern United States,
starting on January 2003. Since this work is focused on variable selection for data
sampled on a ﬁne grid, the curves have been sub-sampled to simulate observations
each 15 minutes. The ﬁve windows into which the curves are splited consist in 100
samples for train and 5 for test.

A few curves of these data sets are included in Figure 2.

17

(a) Functional PM10.

(b) Original PM10.

(c) Functional traﬃc.

(d) Original traﬃc.

(e) Functional temp.

(f) Original
ture.

tempera-

(g) Functional utility.

(h) Original utility.

Figure 2: 25 trajectories of the real data sets, both discrete and functional.

6 Experiments and results

In this section we present the results of the experiments, conducted in order to check the
performance of the proposal. The code used to run these simulations is provided and the
tables containing the results can be found in Appendix C. The entries marked with bold
letters there correspond to the best performance in each case. We want to emphasize
that we do not intend to obtain deﬁnitive and general conclusions from these results. We
believe that only real applications of the techniques lead to safe conclusions.

6.1 Forecasting

The main goal for which our proposal is designed is the prediction of time series. Accord-
ingly, the greatest part of the experiments is devoted to forecasting.

Simulated data sets
Table 1 of Appendix C summarizes the measurements for the simulated data sets of
the two types of errors e1 and e2 (Equation (19)). Regarding our two proposals, there
is not a method that uniformly outperform the other one. That is, both cluster and
cross-validation perform well when it comes to select the number of points. In general,
our proposals are mainly the victors, closely followed by the FPCA approach with the
fFPE criterion. These are the expected results, since three out of the four data sets fulﬁll
the sparse model of Equation (5). In any case, our proposal also slightly outperforms the
others for the FAR data, where this sparsity assumption is far from being satisﬁed.

Real data sets
In Table 2 of Appendix C we summarize the diﬀerent error measurements for the
four real data sets tested. Taking these results into account, it is even less clear which
implementation of our proposal, the cross-validation one or the cluster one, is the best
choice. For the two ﬁrst data sets it seems that the FPCA approach with fFPE slightly

18

outperforms the other methods. However, the diﬀerences between it and our proposals
are in general small, even achieving the same error, or improving it, in about half of the
It is
measures. By comparison, our proposal is the victor for the last two data sets.
particularly noteworthy the diﬀerences obtained for the temperature data set, which is
the smallest one with only 32 curves for training in each window. The error measurements
of our proposals for this set fall in the interval [0.24, 0.82], while the measurements for
fFPE are in [1.95, 5.3]. This could be due to the simplicity of our proposal, which just
relies on the computation of the covariance matrix of (at most) 10 real random variables.
This simplicity is also reﬂected in the execution time presented later.

In addition, for these real data sets we have also obtained the selected points, which
are shown in Figure 3 (these curves are centered versions of the ones in Figure 2). It is
diﬃcult to reach meaningful conclusions for the four sets altogether, but we can make a
couple of interesting observations. For instance, the points selected for the discrete data
sets are more “precise” (in some sense) than the ones for the functional version, which look
more equispaced. This could lead to think that we are “dispersing” the dependence of the
data when representing them on a functional basis. In addition, the points selected with
cluster and cross-validation for the discrete sets are similar, although it seems that the
cross-validation implementation selects more points than needed (in view of the prediction
performance).

We can also analyze the points selected for each data set separately. We mainly focus
on the points selected for the discrete versions of the data. For the pollution data set, it
seems that the last few hours of the day are the most informative when predicting the
pollution of the following day, which seems reasonable. But it seems also important to
measure the pollution early in the morning (since all the methods select at least one point
in the interval [0.2, 0.4], which would correspond to between 5:00 and 9:00). With regard
to the traﬃc data, we can identify the most relevant time interval around 17:00, which
coincide with one of the moments of greatest traﬃc volume. All the methods select one
point around 13:00 as well, which correspond to the local minimum in the original curves.
For the temperature it looks like almost the only relevant hours for prediction purposes
are between 0:00 and 2:00 of the previous day (since the blue points correspond to Xn−2),
along with around 8:00 in the morning. We ﬁnd this result remarkable, since it is not
completely intuitive. Finally, for the utility data set the most noteworthy fact is that the
cluster implementation for the discrete version does not select any point for Xn−1, which
would mean that the dependence lies more backward in time.

6.2 Execution time

We measured the execution times of all the previous forecasting experiments. Both the
functional (funct) and the discrete (disc) implementations of our proposal are measured.
Table 3 of Appendix C shows the mean execution times of each of the methods for the
real data sets. It seems that working with the transformed functional data is slower in
general, and that our two discrete implementations are considerably faster than the other
methods. The traﬃc data set is the only one for which our proposal is not the fastest one.
This is due to the larger size of the grid, since the curves are sampled every ﬁve minutes.
Since our procedure checks almost all the points of the grid at each step, the grid size
notably aﬀects the execution time.

We also measure how the sample size aﬀects the execution time, increasing it from

19

(a) Selected points for PM10.

(b) Selected points for traﬃc.

(c) Selected points for temperature.

(d) Selected points for utility.

Figure 3: Selected points in Xn−1 (solid), Xn−2 (dashed) and Xn−3 (dotted).

20

(a) Sparse with logs.

(b) Sparse with sins.

(c) O.U.

(d) FAR.

Figure 4: Execution times for the simulated data sets when increasing the sample size.

50 to 250 samples for the four simulated data sets. The obtained results are available in
Table 4 of Appendix C and they are also plotted in Figure 4. We see that our two discrete
implementations are almost not aﬀected by the change on the sample size, compared with
the other methods. The execution times for the functional implementations are also
almost constant with the sample size, although we can see that for the O.U. the execution
times are quite high. This is due to the use of the model F CAR(3) for this data set
instead of F CAR(1). Therefore, we analyze also the impact of the order of the model on
the execution time. We use the values q = 1, . . . , 5 for this same data set. The results
are summarized in Table 5 of Appendix C. We can see that the value of this parameter
signiﬁcantly aﬀects the execution time of the functional implementations.

6.3 Kernel approximation

As mentioned when the general model was ﬁrst introduced in Equation (6), any function
φ(s, ·) ∈ H(X) can be arbitrarily well approximated by ﬁnite linear combinations as in
Equation (7), which we denote here as φp(s, ·). We quantify how good this approximation
is when increasing the number of elements p in this sum.

In order to avoid the use of samples, which introduces noise to the measurements, we

21

work with the RKHS associated to the standard Brownian Motion, since in this case we
know explicitly the space H(X),

H(X) = {f ∈ L2[0, 1]

: f (0) = 0, f absolutely continuous and f (cid:48) ∈ L2[0, 1]},

where f (cid:48) denotes the derivative. The inner product of this space is given by

(cid:104)f, g(cid:105)H =

(cid:90) 1

0

f (cid:48)(s)g(cid:48)(s)ds,

for f, g ∈ H(X).

Since the diﬀerent norms in function spaces are not equivalent, it is not obvious which
norm should we use to measure the errors. We decided on the H(X)-norm, since it is
the one that appears in the theoretical results. In addition, the L2-norm is always less
than the RKHS-norm. Thus, the diﬀerences shown here are greater than the diﬀerences
in L2[0, 1].

We approximate diﬀerent kernels in H(X), increasing p from 1 to 3. First, we use the
sparse kernel function with logarithms previously used for the forecasting experiments.
Then we try the functions

φ1(s, t) = cos(2πs) sin(2πt),

φ2(s, t) = sin(2πst) and φ3(s, t) = − log(5st + 1). (20)

Diﬀerent continuous functions can be tried using the provided code, as long as they fulﬁll
φ(s, 0) = 0 for all s ∈ [0, 1] and the derivatives of φ(s, ·) lie in L2[0, 1].

Figure 5 shows the approximated kernels. In Figure 6 we plot the distances (cid:107)φ(s, ·) −
φp(s, ·)(cid:107)H for s ∈ [0, 1], for p from 1 to 20. We can see that these distances go to zero for
every s, although this convergence does not seem to be at the same rate for every point.
As expected, the distances for the sparse representation are all zero for p = 3 (the real
model).

7 Conclusions

In the present paper we have fundamentally extended the theory developed for regression
with scalar response and independent data, introduced by Berrendero et al. (2018), to
the setting of prediction of functional time series, whose dependence is modeled using an
autoregressive structure. That is, a variable selection technique for prediction is devel-
oped, based on the theory of Reproducing Kernel Hilbert Spaces. This variable selection
approach helps to overcome some of the usual problems coming from the use of other
dimension reduction techniques. Besides, the change of environment from the standard
L2[0, 1] to C[0, 1] allows us to prove uniform almost sure convergence of the estimations.
We also provide an a.s. consistent estimator for the number of relevant variables involved
in the model, which is computationally eﬃcient.

When compared with other prediction methods of the literature, our proposal is quite
competitive. The results obtained for the real data sets tested are encouraging. In addi-
tion, the execution times of our implementation are smaller than the competitors. That
is, our proposals, particularly the discrete approaches, are more suitable for large data
sets. Furthermore, the proposed estimators can be directly adapted to discrete or fully
functional data sets.

22

(a) Function φ1 of Equation (20).

(b) Function φ2 of Equation (20).

(c) Function φ3 of Equation (20).

(d) Sparse kernel with logarithms.

Figure 5: Approximations of the functions φ(s, ·) ∈ H(X) when increasing p in Equation (7).

23

(a) φ1 of Eq. (20).

(b) φ2 of Eq. (20).

(c) φ3 of Eq. (20).

(d) Sparse with logs.

Figure 6: Distances (cid:107)φ(s, ·) − φp(s, ·)(cid:107)H for s ∈ [0, 1].

Acknowledgements

We are very grateful to Prof. Dr. Claudia Kl¨uppelberg, Antonio Cuevas, Jos´e Ram´on
Berrendero and Alexander Aue and two referees for their help, suggestions and criti-
cisms. We furthermore thank Siegfried H¨ormann and the Autobahndirektion S¨uedbayern
for providing the datasets. This work has been partially supported by Spanish Grant
MTM2016-78751-P and the European Social Fund (“Ayudas para contratos predoctor-
ales para la formaci´on de doctores 2015” and “Ayudas a la movilidad predoctoral para
la realizaci´on de estancias breves en centros de I+D 2016”, BES-2014-070460). The pa-
per was partly written during the visit of Beatriz Bueno-Larraz as an academic guest
researcher at the Technical University of Munich.

References

J. Alvarez-Li´ebana. A review and comparative study on functional time series techniques

(under review). arXiv preprint arXiv:1706.06288, 2017.

G. Aneiros and P. Vieu. Variable selection in inﬁnite-dimensional problems. Statistics

and Probability Letters, 94:12–20, 2014.

A. Aue and J. Klepsch. Estimating functional time series by moving average model ﬁtting.

preprint at arXiv:1701.00770[ME], 2017.

A. Aue, D. Dubart Norinho, and S. H¨ormann. On the prediction of stationary functional
time series. Journal of the American Statistical Association, 110(509):378–392, 2015.

A. Berlinet and C. Thomas-Agnan. Reproducing kernel Hilbert spaces in probability and
statistics. Kluwer Academic, Boston, 2004. ISBN 1-4020-7679-7. URL http://opac.
inria.fr/record=b1128469.

P. Bernard. Analyse de signaux physiologiques. M´emoire Universit´e Catholique Angers,

1997.

J. R. Berrendero, A. Cuevas, and J. L. Torrecilla. On the use of reproducing kernel Hilbert
spaces in functional classiﬁcation. Journal of the American Statistical Association, page
(to appear), 2017. doi: 10.1080/01621459.2017.1320287. URL http://dx.doi.org/
10.1080/01621459.2017.1320287.

24

J. R. Berrendero, B. Bueno-Larraz, and A. Cuevas. An RKHS model for variable selection
in functional linear regression. Journal of Multivariate Analysis, 2018. ISSN 0047-259X.
doi: https://doi.org/10.1016/j.jmva.2018.04.008. URL http://www.sciencedirect.
com/science/article/pii/S0047259X17305596.

P. Besse and H. Cardot. Approximation spline de la pr´evision d’un processus fonctionnel

autor´gressif d’ordre 1. Canadian Journal of Statistics, 24:467–487, 1996.

D. Bosq. Linear Processes in Function Spaces: Theory and Applications. Springer, New

York, 2000.

S. Cambanis. Sampling designs for time series. Handbook of Statistics, 5:337–362, 1985.

A. Cuevas. A partial overview of the theory of statistics with functional data. Journal of

Statistical Planning and Inference, 147:1–23, 2014.

A. Delaigle, P. Hall, and N. Bathia. Componentwise classiﬁcation and clustering of func-

tional data. Biometrika, 99(2):299, 2012.

D. Didericksen, P. Kokoszka, and X. Zhang. Empirical properties of forecasts with the

functional autoregressive model. Computational Statistics, 27:285–298, 2012.

J. Fan and J. Lv. A selective overview of variable selection in high dimensional feature

space. Statistica Sinica, 20(1):101, 2010.

F. Ferraty, P. Hall, and P. Vieu. Most-predictive design points for functional data predic-
tors. Biometrika, 97(4):807–824, 2010. URL http://EconPapers.repec.org/RePEc:
oup:biomet:v:97:y:2010:i:4:p:807-824.

S. H¨ormann and P. Kokoszka. Weakly dependent functional data. Annals of Statistics,

38(3):1845–1884, 2010.

S. H¨ormann, L. Kidzi´nski, and M. Hallin. Dynamic functional principal components.

Journal of the Royal Statistical Society: Series B, 77(2):319–348, 2015.

L. Horv´ath and P. Kokoszka. Inference for functional data with applications, volume 200.

Springer Science & Business Media, 2012.

T. Hsing and R. Eubank. Theoretical Foundations of Functional Data Analysis, with an

Introduction to Linear Operators. Wiley, West Sussex, UK, 2015.

R. Hyndman, A. B. Koehler, J. K. Ord, and R. D. Snyder. Forecasting with exponential

smoothing: the state space approach. Springer Science & Business Media, 2008.

S. Janson. Gaussian Hilbert spaces, volume 129. Cambridge university press, 1997.

H. Ji and H.-G. M¨uller. Optimal designs for longitudinal and functional data. J. Roy.

Statist. Soc., B, to appear, 2016.

H. Kadri, E. Duﬂos, P. Preux, S. Canu, A. Rakotomamonjy, and J. Audiﬀren. Operator-
valued kernels for learning from functional response data. Journal of Machine Learning
Research, 16:1–54, 2015.

25

V. Kargin and A. Onatski. Curve forecasting by functional autoregression. Journal of

Multivariate Analysis, 99:2508–2526, 2008.

J. Klepsch and C. Kl¨uppelberg. An Innovations Algorithm for the prediction of functional

linear processes. Journal of Multivariate Analysis, 155:252–271, 2017.

J. Klepsch, C. Kl¨uppelberg, and T. Wei. Prediction of functional ARMA processes with

an application to traﬃc data. Econometrics and Statistics, 1:128–149, 2017.

P. Kokoszka and M. Reimherr. Determining the order of the functional autoregressive

model. Journal of Time Series Analysis, 34:116–129, 2013.

C. Lam, Q. Yao, et al. Factor modeling for high-dimensional time series:
the number of factors. The Annals of Statistics, 40(2):694–726, 2012.

inference for

Z. Z. Liu. The doubly adaptive LASSO methods for time series analysis. PhD dissertation,

University of Western Ontario, 2014.

M. N. Luki´c and J. H. Beder. Stochastic processes with sample paths in reproducing
kernel hilbert spaces. Transactions of the American Mathematical Society, 353(10):pp.
3945–3969, 2001. ISSN 00029947. URL http://www.jstor.org/stable/2693779.

I. W. McKeague and B. Sen. Fractals with point impact in functional linear regression.
Ann. Statist., 38(4):2559–2586, 08 2010. doi: 10.1214/10-AOS791. URL http://dx.
doi.org/10.1214/10-AOS791.

F. Mokhtari and T. Mourid. Prediction of continuous time autoregressive processes via the
reproducing kernel spaces. Statistical Inference for Stochastic Processes, 6(3):247–266,
2003.

N. S. Pillai, Q. Wu, F. Liang, S. Mukherjee, and R. L. Wolpert. Characterizing the
function space for bayesian kernel models. Journal of Machine Learning Research, 8
(Aug):1769–1797, 2007.

B. Pumo. Prediction of continuous time processes by C[0, 1]–valued autoregressive pro-
cess. Statistical Inference for Stochastic Processes, 1(3):297–309, 1998. doi: 10.1023/A:
1009951104780.

M. Ruiz-Medina and J. ´Alvarez-Li´ebana. Strong-consistent autoregressive predictors in
abstract Banach spaces (under minor review in J. Multivariate Anal.). arXiv preprint
arXiv:1801.08817, 2018.

J. Q. Shi and T. Choi. Gaussian process regression analysis for functional data. CRC

Press, 2011.

H. Tran, N. Muttil, and B. Perera. Selection of signiﬁcant input variables for time series

forecasting. Environmental Modelling & Software, 64:156–163, 2015.

Y. Yang, T. Zhang, and H. Zou. Flexible expectile regression in reproducing kernel Hilbert
spaces. Technometrics, 60(1):26–35, 2018. doi: 10.1080/00401706.2017.1291450. URL
https://doi.org/10.1080/00401706.2017.1291450.

26

F. Zamora-Martnez, P. Romeu, P. Botella-Rocamora, and J. Pardo. On-line learning of
indoor temperature forecasting models towards energy eﬃciency. Energy and Buildings,
83:162 – 172, 2014. ISSN 0378-7788. doi: https://doi.org/10.1016/j.enbuild.2014.04.
034.

27

APPENDIX A: Theoretical model justiﬁcation

In this appendix we include the discussion about model of Equation (6) of the main

document,

Xn(·) = Ψ−1

Xn−1

(cid:0)φ(·, (cid:63))(cid:1) + εn(·),

n ∈ Z.

We work with Xn, n ∈ Z a centered stationary process taking values in C[0, 1] such
that E(cid:2)(cid:0) sups |Xn(s)|(cid:1)2(cid:3) < ∞. Note that, since the process is stationary, its covariance
structure remains invariant and then the space H(X), on which the Lo`eve’s isometry
is deﬁned, does not depend on n. In addition, at the end of this document we include
a result concerning the convergence of the projections when increasing the number of
selected points.

We start analyzing the pointwise deﬁnition of the previous model, for each s ∈ [0, 1],

Xn(s) = Ψ−1

Xn−1

(cid:0)φ(s, ·)(cid:1) + εn(s),

n ∈ Z.

(21)

This deﬁnition can be applied for any process Xn, n ∈ Z, with E(cid:2)(cid:0) sups |Xn(s)|(cid:1)2(cid:3) < ∞,
since then Xn(s) has ﬁnite variance and the Lo`eve’s isometry can be applied. Moreover,

φ(s, ·) = ΨXn−1 (Xn(s) − εn(s)) = E [(Xn(s) − εn(s)) Xn−1(·)] = c1(s, ·),

(22)

and then

(cid:107)c1(s, ·)(cid:107)H = (cid:107)φ(s, ·)(cid:107)H = Var(cid:0)Xn(s) − εn(s)(cid:1) < ∞.

That is, the pointwise evaluations Xn(s) can be written as Ψ−1
(c1(s, ·)) + εn(s), which
is always well-deﬁned. From Equation (22) we see that, when changing the working space
from L2[0, 1] to H(X), the solution of the model does not require to invert the covariance
operator.

Xn−1

As pointed out in the main document, the ﬁnite FCAR model depending on p points
is recovered when φ is as in Equation (7). We focus now on this ﬁnite family, since
we are mainly interested in variable selection. In order to make sense of the functional
deﬁnition of FCAR models (in the same vein as the general AR model (4) of the main
document) and to be able to obtain some properties about the process, we make use of a
more general family. In view of Deﬁnition 1, one can understand that each realization xn
equals ρ(xn−1) + εn, where ρ is the operator:

ρ(f ) =

p
(cid:88)

j=1

αj(·)f (tj)

for f ∈ C[0, 1].

(23)

We prove with Proposition 2 that this interpretation is well founded. Note that this
operator depends on the covariance function c1(s, t), since it uses the same set of points
tj and functions αj that deﬁne it (by Equation (22)).
Proposition 2. Let Xn follow a FCAR(1) model of Deﬁntion 1 such that (cid:80)p
then it has a unique strictly stationary solution given by

j=1 (cid:107)αj(cid:107) < 1,

xn =

∞
(cid:88)

j=0

ρj(εn−j), n ∈ Z,

28

where ρ is deﬁned in Equation (23), the series converges almost surely and besides

(cid:104)(cid:16)

E

(cid:12)
(cid:12)
(cid:12)Xn(s) −

sup
s

p
(cid:88)

j=0

(cid:12)
ρj(εn−j)(s)
(cid:12)
(cid:12)

(cid:17)2(cid:105)

→ 0

as p → ∞.

Proof. This proof relies on the theory of Banach space valued autoregressive (ARB) pro-
cesses (introduced in Deﬁnition 6.1 of Bosq (2000)) with B = C[0, 1]. First we check
that our model follows an ARB model as in (4) of the main document. That is, that the
operator of Equation (23) is bounded. It follows from the deﬁnition of the norm in the
space of linear operators,

(cid:107)ρ(cid:107)L = sup
(cid:107)f (cid:107)≤1

(cid:13)
(cid:13)
(cid:13)

p
(cid:88)

j=1

αj(·)f (tj)

(cid:13)
(cid:13)
(cid:13) = sup

(cid:107)f (cid:107)≤1

(cid:12)
(cid:12)
(cid:12)

sup
s∈[0,1]

p
(cid:88)

(cid:12)
(cid:12)
αj(s)f (tj)
(cid:12)

≤ sup
(cid:107)f (cid:107)≤1

sup
s∈[0,1]

p
(cid:88)

j=1

|αj(s)| |f (tj)| ≤

(cid:16)

sup
s∈[0,1]

(cid:17)(cid:16)

|αj(s)|

(cid:17)

|f (t)|

sup
(cid:107)f (cid:107)≤1

sup
t∈[0,1]

j=1
p
(cid:88)

j=1

≤

p
(cid:88)

j=1

sup
s∈[0,1]

|αj(s)| =

p
(cid:88)

j=1

(cid:107)αj(cid:107) < 1.

The operator norm satisﬁes (cid:107)AB(cid:107)L ≤ (cid:107)A(cid:107)L(cid:107)B(cid:107)L, for A, B operators in this space. Then
the result follows from the corollary of Theorem 6.1 in Bosq (2000) since

(cid:107)ρj0(cid:107)L ≤ (cid:107)ρ(cid:107)j0

L < 1,

for any ﬁnite positive j0.
The condition (cid:80)p

j=1 (cid:107)αj(cid:107) < 1 may be relaxed, as it is deduced from the proof, since
it is enough to have (cid:107)ρj0(cid:107)L < 1. Proving this same result for a general kernel function
φ is not straightforward, although possible. Each φ(s, ·) can be written as a pointwise
limit of functions in H0(c0), and then the operator ρ of Equation (23) would be deﬁned
as a limit of operators of ﬁnite rank. But the main issue is that one needs to impose
the condition that this limit operator is bounded, which is usually unknown. Besides,
additional smoothness conditions on the auto-covariance function are required.

Convergence when increasing the number of points

In Section 5.1 of Cambanis (1985) the author studied the convergence of the ﬁnite di-
mensional approximations of the functions in H(X). Given a function f ∈ H(X), let us
denote the projection deﬁned by the points in Tp as

f (·)Tp =

p
(cid:88)

j=1

aj(s)c0(tj, ·).

In Equation (5.3) of Cambanis (1985), it is stated that if f ∈ H(X) and

T ∗
p = arg sup
Tp∈Dp

f (Tp)(cid:48)ΣTpf (Tp) = arg sup
Tp∈Dp

(cid:107)fTp(cid:107)2
H,

29

H as p goes to inﬁnity.

where Dp = {Tp ∈ [0, 1]p : t1 < t2 < . . . < tp}, then the square norm (cid:107)fT ∗
H converges
to (cid:107)f (cid:107)2
In addition, if the points Tp which generate the space
onto which we project are not these optimal ones, the projected function fTp could still
converge to the real function f under some conditions. For instance, in Equation (5.4) of
the same book it is claimed that whenever qp ∈ [0, 1]p form a regular sequence of points
generated by a density h (that is, qp are the quantiles of h), then

p (cid:107)2

(cid:107)f (cid:107)2

H − (cid:107)fqp(cid:107)2

H = (cid:107)f − fqp(cid:107)2

H → 0 as p → ∞.

H + (cid:107)f −
H. The author also give some convergence rates depending on how smooth the func-

Note that we are projecting onto a closed subspace of H(X), so (cid:107)f (cid:107)2
fqp(cid:107)2
tion c0 is.

H = (cid:107)fqp(cid:107)2

Now, in the model that we have rewritten at the beginning of this appendix, the process
Xn is generated throughout a continuous function φ on [0, 1]2 such that φ(s, ·) ∈ H(X)
for all s ∈ [0, 1]. We will denote as T ∗

p the points that minimize, for Tp ∈ Θp

Q1(Tp) =

(cid:90) 1

0

(cid:13)
(cid:13)
(cid:13)φ(s, ·) −

min
αj (s)∈R

p
(cid:88)

j=1

αj(s)c0(tj, ·)

(cid:13)
2
(cid:13)
(cid:13)

H

ds.

This criterion to select the points is equivalent to optimize Q or Q0 of Proposition 1 of
the main document (as mentioned in the proof of that same proposition). Then, using a
similar notation for the projection of real random variables,

Xn(s)Tp = Ψ−1

Xn−1

(cid:0)φ(s, ·)Tp

(cid:1) =

p
(cid:88)

j=1

αj(s)Xn−1(tj),

we can prove the following result.

Proposition 3. If Xn is given by Equation (6) of the main document, where φ is a
continuous function in [0, 1]2 and εn(s) is uncorrelated with Xn−1(s) for every s ∈ [0, 1],
then

E

(cid:104)(cid:13)
(cid:0)Xn(·) − εn(·)(cid:1) − Xn(·)T p
(cid:13)

(cid:105)

(cid:13)
2
(cid:13)
2

→ 0 as p → ∞,

where (cid:107) · (cid:107)2 is the norm in L2[0, 1].

Proof. We denote as qp ∈ [0, 1]p the quantiles of a ﬁxed density h. By Tonelli’s Theorem
we can change the order of the integrals

(cid:90) 1

E

0

(cid:0)(Xn(s) − εn(s)) − Xn(s)Tp

(cid:1)2ds =

=

(cid:90) 1

0

(cid:90) 1

0

E(cid:2)(Xn(s) − εn(s)) − Xn(s)Tp

(cid:3)2ds

Var(cid:0)(cid:0)Xn(s) − εn(s)(cid:1) − Xn(s)Tp

(cid:1)ds

= Q1(T ∗

p ) − E(cid:2)(cid:107)εn(cid:107)2

2

(cid:3) = Q1(T ∗
p )

≤ Q1(qp) =

(cid:90) 1

0

(cid:13)
(cid:13)φ(s, ·) − φ(s, ·)qp

(cid:13)
2
Hds.
(cid:13)

30

That is, we have to see that the sequence of functions gp(s) = (cid:107)φ(s, ·) − φ(s, ·)qp(cid:107)2
H
converges to zero in L1[0, 1]. Since the points qp are a regular sequence, we know that
gp(s) → 0 as p → ∞ for each s ∈ [0, 1] (Equation (5.4) of Cambanis (1985)). Besides,
these functions are bounded,

|gp(s)| = gp(s) = (cid:107)φ(s, ·)(cid:107)2

H − (cid:107)φ(s, ·)qp(cid:107)2

H ≤ (cid:107)φ(s, ·)(cid:107)2
H,

0 (cid:107)φ(s, ·)(cid:107)2

and since (cid:82) 1
tion of two continuous functions s (cid:55)→ φ(s, ·) and f (cid:55)→ (cid:107)f (cid:107)2
obtain the result using the dominated convergence theorem.

Hds ≤ sups (cid:107)φ(s, ·)(cid:107)2

H < ∞ (because s (cid:55)→ (cid:107)φ(s, ·)(cid:107)2

H is the composi-
H over the compact [0, 1]) we

31

APPENDIX B: Additional proofs

In this appendix we include the proofs that are mainly based on the pointwise results

of Berrendero et al. (2018).

Proof of Proposition 1
This proof is inspired by the one of Proposition 1 of Berrendero et al. (2018). Pointwise

for each s ∈ [0, 1], using Lo`eve’s isometry we have that

(cid:16)

Var

Xn(s) −

p
(cid:88)

j=1

αj(s)Xn−1(tj)

(cid:17)

= (cid:13)

(cid:13)φ(s, ·) −

p
(cid:88)

j=1

αj(s)c0(tj, ·)(cid:13)
2
H + σ(s),
(cid:13)

where σ(s) = Var(εn(s)) ≤ (cid:107)E[ε2
n(cid:107) < ∞, so the minimizing values αj(s) are
the same for both sides of the equality. Again pointwise for each s ∈ [0, 1], using the
reproducing property of H(X),

n](cid:107) ≤ E(cid:107)ε2

(cid:13)
(cid:13)
(cid:13)φ(s, ·) −

p
(cid:88)

j=1

αj(s)c0(tj, ·)

(cid:13)
2
(cid:13)
(cid:13)

H

= (cid:107)φ(s, ·)(cid:107)H +

p
(cid:88)

i,j=1

αi(s)αj(s)c0(ti, tj) − 2

p
(cid:88)

j=1

αj(s)φ(s, tj).

Since c0 is a positive-deﬁnite function, this last function is convex in αj(s) for each
s ∈ [0, 1]. Therefore we can compute its minimum pointwisely, which is achieved at
(α∗
c1(·, Tp), since c1(s, t) = φ(s, t) for each s (Equation (22) of Ap-
pendix A). Then if we substitute this optimum in the previous equation we get

p(·))(cid:48) = Σ−1
Tp

1(·), . . . , α∗

min
αj (s)∈R

(cid:13)
(cid:13)φ(s, ·) −

p
(cid:88)

j=1

αj(s)c0(tj, ·)(cid:13)
2
H = (cid:107)φ(s, ·)(cid:107)2
(cid:13)

H − c1(s, Tp)(cid:48)Σ−1
Tp

c1(s, Tp).

Hence, integrating over s ∈ [0, 1],

Q(Tp) =

(cid:90) 1

0

σ(s)ds +

(cid:90) 1

0

(cid:107)φ(s, ·)(cid:107)2

Hds − Q0(Tp) = C − Q0(Tp).

This constant C is ﬁnite since the integral of σ(s) is bounded by (cid:107)E[ε2
(cid:82) 1
0 (cid:107)φ(s, ·)(cid:107)2
is the composition of two continuous functions, s (cid:55)→ φ(s, ·) = c1(s, ·) and f (cid:55)→ (cid:107)f (cid:107)H).

n](cid:107) < ∞ and
H a continuous function on [0, 1] (it

Hds ≤ sups∈[0,1] (cid:107)φ(s, ·)(cid:107)2

H being (cid:107)φ(s, ·)(cid:107)2

Proof of Lemma 2
Denoting

Q0(Tp) =

(cid:98)Q0

m(Tp) =

(cid:90) 1

0

(cid:90) 1

0

c1(s, Tp)(cid:48)Σ−1
Tp

c1(s, Tp) ds =

(cid:98)c1(s, Tp)(cid:48) (cid:98)Σ−1

Tp (cid:98)c1(s, Tp) ds =

(cid:90) 1

0

(cid:90) 1

0

q0(Tp; s)ds,

(cid:98)q0
m(Tp; s)ds,

we can extend the proofs of Lemmas 2 and 3 of Berrendero et al. (2018) to our setting.
m(Tp; s) are
continuous in Tp for each s ∈ [0, 1], using the same reasoning as in the proof of Lemma

For the continuity of the functions, it can be shown that q0(Tp; s) and (cid:98)q0

32

2 of Berrendero et al. (2018) but using now Lemma 1. Then if (cid:107)Tp − Sp(cid:107)Rp < δ, where
(cid:107) · (cid:107)Rp is the usual vector norm,

|Q0(Tp) − Q0(Sp)| =

(cid:12)
(cid:12)
(cid:12)

(cid:90) 1

0

(cid:0)q0(Tp; s) − q0(Sp; s)(cid:1)ds

(cid:12)
(cid:12)
(cid:12) ≤

(cid:90) 1

0

|q0(Tp; s) − q0(Sp; s)|ds < ε.

And equivalently to see that (cid:98)Q0

m is continuous with probability one.

The uniform convergence is straightforward in view of the deﬁnition of Q0. By Lemma
1, the vector (cid:98)c1(s, Tp) converge uniformly a.s. to c1(s, Tp). Besides, by the same reasoning
as in Lemma 3 of Berrendero et al. (2018), using Lemma 1 now, one gets the uniform
convergence of the inverse covariance matrices. Then, we get

sup
(s,Tp)∈[0,1]×Θp

m(Tp; s) − q0(Tp; s)(cid:12)
(cid:12)
(cid:12)(cid:98)q0
(cid:12)

a.s.→ 0,

which implies the convergence of the integral over s ∈ [0, 1].

Proof of Theorem 2
(a) Because of the equivalence of the criteria proved in Proposition 1, it is enough to
see that T ∗ is the only global minimum of Q(Tp∗) in Θp∗. From Equation (8) it is clear
that T ∗ minimizes Q since

Q(Tp∗) =

(cid:90) 1

0

Var(cid:0)Xn(s) − Xn(s)Tp∗

(cid:1) ds =

(cid:90) 1

0

Var(cid:0)Xn(s)T ∗ − Xn(s)Tp∗

(cid:1) ds + (cid:107)Var(εn)(cid:107)2,

where (cid:107) · (cid:107)2 is the norm in L2[0, 1], the space of square integrable functions over [0, 1].
Therefore, its minimum value is (cid:107)Var(εn)(cid:107)2 = (cid:107)σ(cid:107)2. If there exists another vector S∗ (cid:54)= T ∗
which also achieves this value, it must be Var(cid:0)Xn(s)T ∗ − Xn(s)S∗
(cid:1) = 0 for almost every
s ∈ [0, 1] (except on a set of measure zero with regard to the Lebesgue measure). However,
it is enough to have one point s0 in which this equality holds. For this point we have that
Xn(s0)T ∗ = Xn(s0)S∗ a.s., which contradicts the assumption that the covariance matrix
ΣT ∗∪S∗ is invertible, and then T ∗ = S∗.

(b) The result follows from the fact that (cid:98)Q0

m and Q0 are continuous functions such
m tends uniformly a.s. to Q0 (Lemma 2) and Q0 has a unique maximum in Θp∗

that (cid:98)Q0
(part (a)).

(c) The same argument as in the proof of Theorem 1.c of Berrendero et al. (2018).

Proof of Theorem 3
(a) We derive a proof similar to the proof of Theorem 2 of Berrendero et al. (2018).

(cid:98)Tp∗,m

(cid:13)
(cid:13) (cid:98)Xn(·)

− Xn(·)T ∗

We can decompose the norm in the statement as
(cid:13) = (cid:13)
(cid:13)
(cid:13)(cid:98)c1(·, (cid:98)Tp∗,m)(cid:48) (cid:98)Σ−1
≤ (cid:13)
(cid:98)c1(·, (cid:98)Tp∗,m) − c1(·, T ∗)(cid:1)(cid:48)
(cid:0)
(cid:13)
+ (cid:13)
(cid:13)c1(·, T ∗)(cid:48) (cid:0)
(cid:98)Σ−1
≤ (cid:13)
(cid:13)Σ−1

T ∗ Xn−1(T ∗) + (cid:15) (cid:13)

(cid:98)Tp∗,m

(cid:98)Tp∗,m

T ∗ Xn−1(T ∗)(cid:13)
(cid:13)

(cid:98)Σ−1

Xn−1( (cid:98)Tp∗,m) − c1(·, T ∗)(cid:48) Σ−1
Xn−1( (cid:98)Tp∗,m)(cid:13)
(cid:13)
T ∗ Xn−1(T ∗)(cid:1)(cid:13)
(cid:13)
(cid:13)(cid:98)c1(s, (cid:98)Tp∗,m) − c1(s, T ∗)(cid:13)
(cid:13)

Xn−1( (cid:98)Tp∗,m) − Σ−1

(cid:98)Tp∗,m

(cid:13)Rp∗ sup
s∈[0,1]

(cid:13)Rp∗

(A)

33

+ (cid:13)

(cid:13)(cid:98)Σ−1

Xn−1( (cid:98)Tp∗,m) − Σ−1

T ∗ Xn−1(T ∗)(cid:13)

(cid:98)Tp∗,m

(cid:13)Rp∗ sup
s∈[0,1]

(cid:13)c1(s, T ∗)(cid:13)
(cid:13)

(cid:13)Rp∗ , (B)

where (cid:107) · (cid:107)Rp∗ denotes the Euclidean norm in Rp∗. For the term in Equation (A), it is
enough to see the convergence of each entry of the covariance vector. For any 1 ≤ j ≤ p∗,

sup
s∈[0,1]

j )(cid:12)
(cid:12)
(cid:12)(cid:98)c1(s, (cid:98)tj) − c1(s, t∗
(cid:12) ≤ sup
s∈[0,1]

≤ sup

(cid:12)(cid:98)c1(s, (cid:98)tj) − c1(s, (cid:98)tj)(cid:12)
(cid:12)
(cid:12)(cid:98)c1(s, t) − c1(s, t)(cid:12)
(cid:12)

j )(cid:12)
(cid:12)
(cid:12)c1(s, (cid:98)tj) − c1(s, t∗
(cid:12)

(cid:12) + sup
s∈[0,1]
j )(cid:12)
(cid:12)
(cid:12)c1(s, (cid:98)tj − t∗
(cid:12).

(cid:12) + sup
s∈[0,1]

s,t∈[0,1]

to zero by Lemma 1, and the second addend due to the
The ﬁrst addend goes a.s.
a.s. convergence of (cid:98)Tp∗,m to T ∗ and the continuity of both c1 and the norm in C[0, 1]
(continuous mapping theorem).

The ﬁrst term in Equation (B) is bounded by

(cid:13)
(cid:13)(cid:98)Σ−1

(cid:98)Tp∗,m

Xn−1( (cid:98)Tp∗,m) − Σ−1

T ∗ Xn−1(T ∗)(cid:13)

(cid:13)Rp∗ ≤ (cid:13)

(cid:13)(cid:98)Σ−1
+ (cid:13)

(cid:98)Tp∗,m
(cid:13)Σ−1

Xn−1( (cid:98)Tp∗,m) − Σ−1

(cid:98)Tp∗,m
Xn−1( (cid:98)Tp∗,m) − Σ−1

(cid:98)Tp∗,m

Xn−1( (cid:98)Tp∗,m)(cid:13)

(cid:13)Rp∗

T ∗ Xn−1(T ∗)(cid:13)

(cid:13)Rp∗ .

(cid:13)
(cid:0)
(cid:13)

(cid:1)Xn−1(T )(cid:13)

(cid:98)Σ−1

T − Σ−1

The ﬁrst term is bounded by supT ∈Θp∗
(cid:13)Rp∗ . Using Lemma 1 we
show that the sample version of the inverse of the covariance matrix, (cid:98)Σ−1
T , as a function on
Θp∗, converges uniformly with probability one to its population counterpart Σ−1
(using a
T
similar reasoning as in the proof of Lemma 3 of Berrendero et al. (2018)). Since Xn−1 has
continuous trajectories on a compact space, the product goes almost sure to zero. The
second addend also goes to zero due to Theorem 1(b) and the continuity of the involved
functions.

T

(cid:98)Tp∗,m

− Xn(·)T ∗

(b) The statement is equivalent to see that the real valued random variables Zm =
(cid:13)
(cid:13)
2 converge to 0 in the L1-norm. From part a) we know that they
(cid:13) (cid:98)Xn(·)
(cid:13)
converge a.s.
to zero, so it only remains to check that the sequence Zm is uniformly
integrable (Vitali’s convergence theorem). In order to do this, it is enough to check that
m] < ∞ for some η > 1 (de la Vall´ee-Poussin’s theorem). Since the function
supm
f (y) = y2η is convex in this case,

E[Z η

m ≤ (cid:0)(cid:13)
Z η

(cid:13) (cid:98)Xn(·)

(cid:98)Tp∗,m

(cid:13) + (cid:13)
(cid:13)

(cid:13)Xn(·)T ∗

(cid:13)
(cid:1)2η ≤ 0.5(cid:2)(cid:0)2(cid:13)
(cid:13)

(cid:13) (cid:98)Xn(·)

(cid:98)Tp∗,m

(cid:13)
(cid:1)2η + (cid:0)2(cid:13)
(cid:13)

(cid:13)Xn(·)T ∗

(cid:13)
(cid:1)2η(cid:3).
(cid:13)

Thus, we have to verify that

22η−1(cid:16)

sup
m

E(cid:2)(cid:13)

(cid:13) (cid:98)Xn(·)

(cid:98)Tp∗,m

2η(cid:3) + E(cid:2)(cid:13)
(cid:13)
(cid:13)

(cid:13)Xn(·)T ∗

2η(cid:3)(cid:17)
(cid:13)
(cid:13)

< ∞.

Let us start with the ﬁrst addend. Using Lemma 1 the supremum of (cid:107)(cid:98)c1(s, Tp∗)(cid:48) (cid:98)Σ−1
c1(s, Tp∗)(cid:48)Σ−1

Tp∗ −
Tp∗ (cid:107)Rp∗ for (s, Tp∗) ∈ [0, 1] × Θp∗ goes a.s to zero, then we bound the norm as

(cid:13)
(cid:13) (cid:98)Xn(·)

(cid:98)Tp∗,m

(cid:13)
2η =
(cid:13)

(cid:16)

sup
s∈[0,1]

(cid:12)
(cid:12)(cid:98)c1(s, (cid:98)Tp∗,m)(cid:48) (cid:98)Σ−1

(cid:98)Tp∗,m

Xn−1( (cid:98)Tp∗,m)(cid:12)
(cid:12)

(cid:17)2η

≤ (cid:13)

(cid:13)Xn−1( (cid:98)Tp∗,m)(cid:13)
(cid:13)

2η
Rp∗

(cid:16)

sup
s∈[0,1]

(cid:13)
(cid:13)(cid:98)c1(s, (cid:98)Tp∗,m)(cid:48) (cid:98)Σ−1

(cid:98)Tp∗,m

(cid:13)
(cid:13)Rp∗

(cid:17)2η

34

≤ (cid:107)Xn−1( (cid:98)Tp∗,m)(cid:107)2η
Rp∗

(cid:16)

sup
s∈[0,1],Tp∗ ∈Θp∗

(cid:13)
(cid:13)c1(s, Tp∗)(cid:48) Σ−1
Tp∗

(cid:17)2η

(cid:13)
(cid:13)Rp∗ + (cid:15)

≤ C (cid:107)Xn−1( (cid:98)Tp∗,m)(cid:107)2η

Rp∗ ,

where C < ∞, since the function involved in the supremum is a continuous function on
the compact space [0, 1] × Θp∗. To conclude we can use the same reasoning as in the proof
of Theorem 2 of Berrendero et al. (2018).

For the second addend, which does not depend on the sample size,

(cid:104)(cid:16)

E

sup
s∈[0,1]

(cid:12)
(cid:12)Xn(s)T ∗

(cid:12)
(cid:12)

(cid:17)2η(cid:105)

(cid:104)(cid:16)

= E

sup
s∈[0,1]

(cid:12)
(cid:12)c1(s, T ∗)(cid:48) Σ−1

T ∗ Xn−1(T ∗)(cid:12)
(cid:12)

(cid:17)2η(cid:105)

E

(cid:104)(cid:13)
(cid:13)Xn−1(T ∗)(cid:13)
(cid:13)

2η
Rp∗

(cid:105)

,

(cid:16)

≤

sup
s∈[0,1]

(cid:13)
(cid:13)c1(s, T ∗)(cid:48) Σ−1
T ∗

(cid:13)
(cid:13)Rp∗

(cid:17)2η

where the expectation is ﬁnite by hypothesis.

35

APPENDIX C: Pseudocodes and tables

0 (cid:98)c1(s, t)2ds, ∀t ∈ grid

Quot(grid) ← (cid:98)c0(t, t)−1 (cid:82) 1
M (1) ← max(Quot)
pts(1) ← which Quot == M (1)

Algorithm 1 Variable selection for a given p
1: procedure RKHS variable selection
2: First point:
3:
4:
5:
6: Rest of the points:
7:
8:
9:
10:
11: Return pts (points) and M

for i in 2 to p do

Quot(grid\pts) ← Quotient of Eq. (18) ∀tp+1 ∈ {grid and not in pts}
M (i) ← max(Quot)
pts(i) ← which Quot == M (i)

M, points ← RKHS method for P points
Lm ← log(M )

Algorithm 2 Cluster approximation to estimate (cid:98)p
1: procedure Number of points
2:
3:
4: k-means procedure with k = 2:
5:
6:
7:

clusters ← kmeans(Lm)
cl1 ← clusters(1)
(cid:98)p ← tail of clusters == cl1

36

Sparse
with log.

Sparse
with sins

O.U.

FAR

ε1

ε2

ε1

ε2

ε1

ε2

ε1

ε2

Disc.

Funct.

Disc.

Funct.

Disc.

Funct.

Disc.

Funct.

Disc.

Funct.

Disc.

Funct.

L2
sup
L2
sup
L2
sup
L2
sup
L2
sup
L2
sup
L2
sup
L2
sup
L2
sup
L2
sup
L2
sup
L2
sup
L2
sup
L2
sup

RKHS+cl RKHS+CV fFPE KR Exact Naive
3.24
1.64
3.32
1.67
1.32
0.80
2.63
1.57
2.42
1.50
2.60
1.53
1.10
0.75
2.19
1.47
2.33
1.35
2.49
1.34
0.65
0.65
1.26
1.21
2.20
1.45
1.96
1.39

0.71
0.71
0.64
0.65
0.18
0.32
0.32
0.55
0.74
0.73
0.81
0.77
0.38
0.37
0.79
0.75
1.01
0.88
1.00
0.91
0.31
0.44
0.66
0.85
1.01
1.00
0.96
0.98

0.65
0.68
0.67
0.65
0.17
0.30
0.33
0.56
0.83
0.84
0.81
0.77
0.42
0.42
0.80
0.75
1.00
0.93
1.05
0.92
0.35
0.47
0.68
0.86
1.01
1.00
0.97
0.98

0.64
0.68
0.65
0.65
0.16
0.30
0.32
0.55
0.72
0.71
0.78
0.77
0.38
0.36
0.78
0.75
1.07
0.88
1.00
0.91
0.31
0.44
0.66
0.85
1.00
1.00
0.96
0.98

0.55
0.65
0.56
0.62
0.14
0.29
0.28
0.53
0.60
0.67
0.65
0.69
0.33
0.34
0.69
0.68
0.83
0.88
0.85
0.86
0.30
0.44
0.59
0.81
0.85
0.91
0.81
0.89

0.66
0.82
0.67
0.83
0.18
0.39
0.34
0.77
0.94
0.95
0.94
0.95
0.48
0.49
0.91
0.94
1.15
0.98
1.20
0.98
0.42
0.50
0.83
0.94
1.13
1.04
1.08
1.02

Table 1: Errors for the simulated data sets (e1 and e2 errors of Eq. (19))

37

PM10

Traﬃc

Temp

Utility

ε1 error

ε2 error

ε1 error

ε2 error

ε1 error

ε2 error

ε1 error

ε2 error

Disc.

Func.

Disc.

Func.

Disc.

Func.

Disc.

Func.

Disc.

Func.

Disc.

Func.

Disc.

Func.

Disc.

Func.

L2
sup
L2
sup
L2
sup
L2
sup
L2
sup
L2
sup
L2
sup
L2
sup
L2
sup
L2
sup
L2
sup
L2
sup
L2
sup
L2
sup
L2
sup
L2
sup

RKHS+cl RKHS+CV fFPE KR
1.48
0.74
1.06
0.86
1.59
0.72
1.08
0.84
0.86
0.47
0.98
0.81
0.85
0.48
0.99
0.79
1.04
1.02
1.01
1.01
1.42
1.21
1.11
0.98
0.95
0.89
0.99
0.98
0.92
0.80
1.00
0.89
1.11
0.53
1.08
0.67
1.12
0.82
1.07
0.72
1.03
0.24
1.05
0.54
1.03
0.37
1.04
0.60
1.16
0.09
1.02
0.34
1.17
0.08
1.01
0.30
0.93
0.07
0.98
0.32
0.92
0.06
0.98
0.28

0.82
0.86
0.82
0.85
0.50
0.80
0.48
0.78
1.00
1.01
1.49
1.05
0.83
0.98
0.75
0.90
5.28
2.10
5.30
2.11
2.87
1.95
2.85
1.96
0.10
0.34
0.09
0.31
0.07
0.32
0.06
0.30

0.97
0.92
0.70
0.84
0.56
0.85
0.48
0.79
1.00
1.02
1.37
1.04
0.90
0.99
0.83
0.91
0.45
0.66
0.63
0.66
0.25
0.59
0.37
0.58
0.11
0.35
0.09
0.29
0.08
0.33
0.06
0.28

Naive
1.65
1.15
1.68
1.11
0.80
1.02
0.76
0.97
67.48
4.70
240.57
10.22
40.57
4.26
62.07
6.82
37.78
3.55
38.20
3.54
14.25
2.98
14.23
2.98
18.72
3.33
18.94
3.37
15.08
3.19
15.16
3.24

Table 2: Errors for real data sets (e1 and e2 of Eq. (19))

RKHS+cl RKHS+CV RKHS+cl RKHS+CV

PM10
Traﬃc
Temp
Utility

(disc)
0.09
2.13
0.27
0.23

(disc)
0.09
2.14
0.20
0.23

(funct)
0.89
32.66
3.49
4.38

(funct)
1.08
32.06
3.80
4.05

fFPE KR

0.71
0.52
0.37
0.56

2.62
1.04
0.51
1.02

Table 3: Execution times (secs) for the real data sets.

38

Sparse
with log.

Sparse
with sins

O.U

FAR

n

50
100
150
200
250
50
100
150
200
250
50
100
150
200
250
50
100
150
200
250

RKHS+cl RKHS+CV RKHS+cl RKHS+CV

(disc)
0.05
0.05
0.04
0.05
0.05
0.06
0.05
0.05
0.05
0.05
0.26
0.26
0.28
0.27
0.27
0.05
0.05
0.05
0.05
0.05

(disc)
0.05
0.06
0.05
0.05
0.06
0.05
0.06
0.06
0.06
0.06
0.23
0.25
0.26
0.27
0.25
0.05
0.05
0.05
0.05
0.06

(funct)
0.34
0.34
0.33
0.33
0.34
0.85
0.81
0.84
0.86
0.86
5.07
5.04
5.14
5.07
4.94
0.53
0.54
0.53
0.54
0.54

(funct)
0.37
0.39
0.40
0.39
0.38
0.98
1.03
1.05
1.06
1.07
5.36
5.25
5.51
5.34
5.23
0.59
0.59
0.63
0.63
0.63

fFPE KR

0.57
0.63
0.84
1.00
1.24
0.62
0.78
0.96
1.22
1.44
0.55
0.72
0.93
1.24
1.58
0.51
0.72
1.03
1.35
1.63

1.50
2.60
3.79
5.09
6.37
2.26
4.07
5.85
7.94
9.96
1.47
2.59
3.73
5.05
6.30
1.71
3.05
4.37
5.88
7.40

Table 4: Execution times (secs) for the simulated data sets.

q RKHS+cl (funct) RKHS+CV (funct)
1
2
3
4
5

0.36
1.71
5.04
8.86
13.92

0.38
1.82
5.16
9.07
14.25

Table 5: Execution times (secs) for FCAR(q) with functional implementation.

39

