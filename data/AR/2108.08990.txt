Few Shot Activity Recognition Using Variational Inference

Neeraj Kumar1

∗ , Siddhansh Narang2

1Indian Institute of Technology, Delhi, India
2RAeS (Royal Aeronautical Society), UK
neerajkr2k14@gmail.com, siddhansh.narang@gmail.com

1
2
0
2

g
u
A
0
2

]

V
C
.
s
c
[

1
v
0
9
9
8
0
.
8
0
1
2
:
v
i
X
r
a

Abstract

There has been a remarkable progress in learning
a model which could recognize novel classes with
only a few labeled examples in the last few years.
Few-shot learning (FSL) for action recognition is a
challenging task of recognizing novel action cate-
gories which are represented by few instances in the
training data.
We propose a novel variational inference based ar-
chitectural framework (HF-AR) for few shot activ-
ity recognition. Our framework leverages volume-
preserving Householder Flow to learn a ﬂexible pos-
terior distribution of the novel classes. This results
in better performance as compared to state-of-the-art
few shot approaches for human activity recognition.
approach consists of base model and an adapter
model. Our architecture consists of a base model
and an adapter model. The base model is trained on
seen classes and it computes an embedding that rep-
resent the spatial an temporal insights extracted from
the input video, e.g. combination of Resnet-152 and
LSTM based encoder-decoder model. The adapter
model applies a series of Householder transforma-
tions to compute a ﬂexible posterior distribution that
lends higher accuracy in the few shot approach.
Extensive experiments on three well-known datasets:
UCF101, HMDB51 and Something-Something-V2,
demonstrate similar or better performance on 1-shot
and 5-shot classiﬁcation as compared to state-of-the-
art few shot approaches that use only RGB frame
sequence as input. To the best of our knowledge, we
are the ﬁrst to explore variational inference along
with householder transformations to capture the full
rank covariance matrix of posterior distribution, for
few shot learning in activity recognition.

1 Introduction
Action recognition in videos has received increasing attention
from the deep learning community[39, 29, 37, 3] in recent
years, due to its great potential value in industry and real-
world applications. Convolutional Neural Network (CNN)

∗Contact Author

based methods [3, 37] have achieved tremendous success in
recognizing actions from videos in the supervised learning
paradigm.

However, the performance of these methods deteriorates
drastically [42] when recognizing action classes that are not
adequately represented in the training data. This problem lim-
its the deployment of these methods in real world applications
where the number of action classes to be recognized increases
rapidly with use cases. In certain cases, the number of samples
for novel classes are quite few for even traditional data aug-
mentation techniques [19] to work. Therefore, systems with
more advanced learning paradigm like Few Shot Learning
[10], which learns to recognize novel classes from only a few
examples (few-shots), have come into prominence.

Videos are more complex than images as one requires the
complete information of the temporal aspects in recognizing
some speciﬁc actions, such as opening the door. In the pre-
vious literature of video classiﬁcation, 3D convolution and
optical ﬂow are two of the most popular methods to model
temporal relations. The direct output of neural network en-
coders is always a temporal sequence of deep encoded features.
Loss of information is even more severe for few-shot learn-
ing. It is hard to learn the local temporal patterns which are
useful for few-shot classiﬁcation with limited amount of data.
Utilizing the long-term temporal ordering information, which
is often neglected in previous works on video classiﬁcation,
might potentially help with few-shot learning. For example,
if the model could verify that there is an act of playing long
jump before a close-up view of a jumping person, the model
will then become quite conﬁdent about predicting the class of
this query video to be playing long jump, rather than some
other potential predictions like jumping exercises or warm-up
activities. In addition, for two videos in the same class, even
though they both contain the act of jumping, the exact tempo-
ral duration of each atomic step can vary dramatically. These
non-linear temporal variations in videos pose a signiﬁcant
challenge to few-shot video activity classiﬁcation.

Prior research on few-shot learning for action recogni-
tion [22, 9, 46, 4, 45, 2] leverage a variety of approaches, but ei-
ther suffer from high computational complexity or are not able
to deliver sufﬁcient accuracy on datasets such as Something-
Something-V2. [22], projects the the action class to visual
space using word2vec, while [9] learns a graph based repre-
sentation to classify the actions and [46] uses multi-saliency

 
 
 
 
 
 
based approach to learn the video representation. [4] lever-
ages a class-prototype vector with conditional GAN based
approach to recognize the activity, while [45] uses C3D based
features along with spatial and temporal network to detect
unseen classes. Recently, [2] used a temporal alignment net-
work to generate the score used for few shot prediction. In
an attempt to improve accuracy, we explore a novel approach
using variational inference (HF-AR: Householder Flow [36]
for Activity Recognition) with normalizing ﬂows and study
the performance in detail on multiple well-known datasets.
We demonstrate close or superior performance using a two
stage variational inference architecture with a base and adapter
model. The ﬁrst stage consists of base model having pretrained
Resnet-152[12] and stacked LSTM[13] layers to capture the
spatial and temporal information and generate a relevant video
embedding. The second stage is an adapter model that uses a
series of householder transformations.

Speciﬁcally, we make the following contributions:

• We propose an architectural framework (HF-AR) that
uses a base and an adapter model. The adapter model
uses variational inference with volume preserving House-
holder Flow[36].

• This approach learns a full rank covariance matrix of
the posterior distribution of datasets having novel classes
with few samples. This ﬂexible posterior distribution
helps in delivering higher accuracy in few shot video
classiﬁcation.

• Detailed experimental analysis on three datasets namely:
UCF101, HMDB51 and Something-Something-V2
demonstrates superior results for 5-shot classiﬁcation
as compared to state-of-the-art few shot approaches.

• Ablation study on series of householder transformation
demonstrates that the length of the householder transfor-
mation, T , depends on k in the k-shot setting . Higher
the value of k, more is the length required, T to learn the
posterior distribution to prevent over-ﬁtting. .

2 Related Work
Few Shot Learning Few shot learning in image classiﬁca-
tion is predominantly based on meta-learning [26, 44, 5].
Under the umbrella of meta-learning, various approaches like
metric learning [30, 38, 33], learning optimization [25] and
learning to initialize and ﬁne tune [5] are proposed.

Metric learning methods learn the similarity between im-
ages [38] to classify samples of novel classes at inference
based on nearest neighbors with labelled samples. Convolu-
tional Siamese Net [18] employs a unique structure to naturally
rank similarity between inputs. Once a network has been tuned,
it can then capitalize on powerful discriminative features to
generalize the predictive power of the network not just to new
data, but to entirely new classes from unknown distributions.
Learning optimization involves memory based networks like
LSTM [41] or memory augmented networks [23] which gen-
erate updates for the classiﬁer rather than using gradients. The
meta-learner also learns a task-common weight initialization
which captures shared knowledge across tasks. Learning to
initialize and ﬁne-tune aims to make minimal changes in the

network when adapting to the new task of classifying novel
classes with few examples. Graph Neural Networks [6, 15, 7]
construct a weighted graph to represent all the data and mea-
sure the similarity between data.

Other methods use data augmentation, which learns to
augment labeled data in unseen classes for supervised train-
ing [11, 40]. In this paper, we have propose a novel two-stage
architecture, HF-AR, with a base and adapter model, in which
the base model learns the knowledge (via embedding) from
seen classes. This is then used in the adapter model that uses
variational inference with a series of householder transforma-
tions, in the ﬁne tuning phase to classify the novel classes.

Few shot Action Recognition:
In contrast to few shot
learning in images, few shot approaches in video classiﬁcation
have gotten less attention.
In [22] each class attribute is
projected to the visual space. In the visual space each class
is represented by a Gaussian distribution. Our approach
uses pre-trained models and variational inference based
architecture to generate embedding to predict the novel
classes. Neural graph matching network [9] used graph
generator to leverage the structure of 3D data through a
graphical representation.
[42] combines dilated temporal
convolution and densely layer connection to perform video
action recognition.

Compound Memory Networks [46] introduces a multi-
saliency embedding algorithm to encode video into a ﬁxed-size
matrix representation. Then, they propose a compound mem-
ory network (CMN) to compress and store the representation
and classify videos by matching and ranking. Our method uses
pretrained Resnet-152 and LSTM layers to capture the spatial
and temporal information to be used at ﬁne tuning stage using
variational inference. Attribute-based feature generation for
unseen classes from GAN by using Fisher vector representa-
tion was explored in zero-shot learning in [43]. It proposes
multi-level semantic inference to boost video feature synthesis,
which captures the discriminative information implied in the
joint visual-semantic distribution via feature-level and label-
level semantic inference along with matching-aware Mutual
Information Correlation to overcome information degradation
issue. This captures seen-to-unseen correlation in matched
and mismatched visual-semantic pairs by mutual information,
providing the zero-shot synthesis procedure with robust guid-
ance signals. Our approach, HF-AR, uses only RGB frames as
input and captures the posterior distribution of novel classes,
with few samples, using variational inference.

ProtoGAN [4] proposes a GAN model which conditions
CGAN on a class-prototype vector which is learnt through
Class Prototype Transfer Network (CPTN) to synthesize ad-
ditional video features for the action recognition classiﬁer.
Action Relation Network [45] contains C3D-based encoder, re-
lation network and spatial and temporal attention units which
reﬁne the aggregation step.
[2] uses Temporal Alignment
Module (TAM), a novel few-shot learning framework that can
learn to classify a previous unseen video. It computes tem-
poral alignment score for each potential query support pair
by averaging per-frame distances along a temporal alignment
path, which enforces the score used to make prediction to

preserve temporal ordering. Our approach (HF-AR) uses pre-
trained Resnet-152 [12] and stacked LSTM [13] to capture the
spatial and temporal information during pre-training on seen
classes. During the ﬁne tuning stage on novel classes, we use
a series of householder transformations to capture the full rank
covariance matrix based distribution of the dataset.

3 Mathematical background

3.1 Variational Autoencoder

Let us consider some dataset X = {x(i)}N
i=1 consisting of N
i.i.d. samples of some continuous or discrete variable x. We
assume that the data are generated by some random process,
involving an unobserved continuous random variable z. The
process consists of two steps: (1) a value z(i) is generated
from some prior distribution p(z); (2) a value x(i) is generated
from some conditional distribution p(x|z). Given N data
points X = {x1, . . . , xN } we typically aim at maximizing
the marginal log-likelihood:

ln p(X) =

N
(cid:88)

i=1

ln p(xi),

(1)

with respect to parameters. This task could be trouble-
some because the integral of the marginal likelihood p(x) =
(cid:82) p(z)p(x|z) dz is intractable (so we cannot evaluate or dif-
ferentiate the marginal likelihood). These intractabilities are
quite common and appear in cases of moderately complicated
likelihood functions p(x|z), e.g. a neural network with a non-
linear hidden layer. To overcome this issue one can introduce
an inference model (an encoder) q(z|x) and optimize the vari-
ational lower bound:

ln p(x) ≥ Eq(z|x)[ln p(x|z)] − KL(cid:0)q(z|x)||p(z)(cid:1),

(2)

where p(x|z) is called a decoder and p(z) = N (z|0, I) is the
prior. There are various ways of optimizing this lower bound
but for continuous z this could be done efﬁciently through a
re-parameterization of q(z|x) [17, 28]. Then the architecture
is called a variational auto-encoder (VAE).

Typically, a diagonal covariance matrix of the encoder is as-
sumed, i.e., q(z|x) = N (cid:0)z|µ(x), diag(σ2(x))(cid:1), where µ(x)
and σ2(x) are parameterized by the NN. However, this as-
sumption can be insufﬁcient and not ﬂexible enough to match
the true posterior.

3.2 Householder Flow

A normalizing ﬂow, formulated by [34, 35, 27] is a frame-
work for designing ﬂexible posterior distribution by starting
with an initial random variable with a simple distribution for
generating z(0) and then applying a series of invertible trans-
formations f (t), for t = 1, . . . , T . Due to this , the last random
variable z(T ) has more richer distribution. The aim of is to
optimize the following objective function once we choose the
transformations f (t) for which we can compute the jacobian
determinant.

ln p(x) ≥ E

q(z(0)|x)

(cid:104)

ln p(x|z(T )) +

T
(cid:88)

t=1

ln

(cid:12)
(cid:12)
(cid:12)det

∂f (t)
∂z(t−1)

(cid:12)
(cid:105)
(cid:12)
(cid:12)

− KL(cid:0)q(z(0)|x)||p(z(T ))(cid:1).

(3)

We have used the volume preserving ﬂows that apply series
of Householder transformations that we refer to as the House-
holder ﬂow. The volume preserving ﬂows provide the series
of transformation such that the jacobian determinant equals
one. This helps in reducing the computation complexity while
successfully providing a ﬂexible posterior distribution.
Motivation: Any full-covariance matrix Σ can be repre-
sented by the eigenvalue decomposition using eigenvectors
and eigenvalues:

Σ = UDU(cid:62),
(4)
where U is an orthogonal matrix with eigenvectors in columns,
D is a diagonal matrix with eigenvalues. The goal is to model
the matrix U to obtain a full-covariance matrix. This process
requires a linear transformation of a random variable using an
orthogonal matrix U. Since the absolute value of the Jacobian
determinant of an orthogonal matrix is 1, for z(1) = Uz(0) one
gets z(1) ∼ N (Uµ, U diag(σ2) U(cid:62)). If diag(σ2) coincides
with true D, then it would be possible to resemble the true
full-covariance function.

Generally, the task of modelling an orthogonal matrix is
rather non-trivial. However, ﬁrst we notice that any orthogonal
matrix can be represented in the following form [1, 32]:
Theorem 1. (The Basis-Kernel Representation of Orthogonal
Matrices)
For any M × M orthogonal matrix U there exist a full-rank
M × K matrix Y (the basis) and a nonsingular (triangular)
K × K matrix S (the kernel), K ≤ M , such that:

U = I − YSY(cid:62).

(5)

The value K is called the degree of the orthogonal matrix.
Further, it can be shown that any orthogonal matrix of de-
gree K can be expressed using the product of Householder
transformations [1, 32], namely:
Theorem 2. Any orthogonal matrix with the basis acting on
the K-dimensional subspace can be expressed as a product of
exactly K Householder transformations:

U = HKHK−1 · · · H1,

(6)

where Hk = I − SkkY·k(Y·k)(cid:62), for k = 1, . . . , K.

Theoretically, Theorem 2 shows that we can model any

orthogonal matrix using K Householder transformations.

Figure 1: a. Figure shows the general architecture of VAE+HF b.
Figure shows the series of housefolder transformation[36]

(a)encodernetwork+HouseholderFlow(b)onestepoftheHouseholderFlowNNHFxz(0)z(T)v0Htz(t−1)z(t−1)z(t)vt−1vtDeﬁnition For a given vector z(t−1) the reﬂection hyper-
plane can be deﬁned by a vector (a Householder vector)
vt ∈ RM that is orthogonal to the hyperplane, and the re-
ﬂection of this point about the hyperplane is [14]:

z(t) =

(cid:16)

I − 2

(cid:17)

vtv(cid:62)
t
||vt||2

z(t−1)

= Htz(t−1),

(7)

(8)

t

such

∂z(t−1)

results

where Ht = I − 2 vtv(cid:62)
The
series
(cid:12)
(cid:12)det ∂Htz(t−1)
(cid:12)

||vt||2 is called the Householder matrix.
in
transformation
of
(cid:12)
(cid:12)
(cid:12) = 0, for t = 1, . . . , T . We start
ln
with the simple diagonal covariance matrix for z(0) and after
series of householder transformation results in vectors vt,
t = 1, . . . , T , which leads to learn full covariance matrix for
more ﬂexible posterior distribution. Figure 1 shows the idea
of householder ﬂow where where T operations are required to
generate ﬂexible posterior distribution with approximated full
covariance matrix.

4 Architecture
Our proposed architecture (HF-AR) consists of two stages:
base model and adapter model (Fig. 2). The ﬁrst stage (Fig. 3)
is an encoder-decoder based architecture to generate video
embeddings on seen classes. The second stage is an adapter
model which takes as input the embedding generated by the
base model and applies a series of householder transformations
to ﬁnally predict the novel classes in a few shot fashion.

Figure 2: 2-stage Proposed Architecture (HF-AR): Base and Adapter
model. Base model with RGB frames as input and Adapter model
with embeddings as input and applies a series of householder trans-
formations.

Figure 3: Base Model: takes video (RGB) frames as input. These
are fed to pretrained Resenet-152 in the encoder followed by stacked
LSTM layers in the decoder.

4.1 Notation
Let x, ¯x, y represent real video frames, video embeddings,
class labels respectively. Let S = {xs, ys|xs ∈ X s, ys ∈ Y s}
be the training set for seen classes where xs denotes the video
frames , ys denotes the class labels in Y s = {ys
S}
with S seen classes. The video embeddings ¯x are calculated

1, . . . , ys

from the base model with xs video frames as an input. Ad-
ditionally, N = {xn, yn)|xn ∈ X n, yn ∈ Y n} is available
during adapter training from novel categories where yn is a
class from a disjoint label set Y n = {yn
N } of N novel
labels. During the adaptor training , the video embeddings ¯x
are fed to the pretrained base model with multiple householder
transformations to predict the novel classes in few shot setting.

1 , . . . , yn

4.2 Base Model
The base model has encoder−decoder architecture in which
the encoder takes video frames(xs) and feed it to the pretrained
Resnet-152 [12] model followed by the 3 linear layers to gen-
erate the 512 dimensional video embedding( ¯xs). The decoder
takes the embedding and send it to the stacked LSTM [13]
layer followed by linear layers to predict the seen classes.
When the number of samples in the seen classes(ys) are high
during the training, the encoder generates effective embed-
dings which helps the decoder to predict the seen classes with
high accuracy. Pretrained Resnet-152 helps in exploiting the
higher level features in the frames which compresses the infor-
mation into small sized embeddings. The stacked LSTM helps
in extracting the temporal aspects of the given input video
which helps the base model to generate better embeddings.

4.3 Adapter Model
The adapter model takes the video embedding as input and
applies a series of volume preserving householder transfor-
mations to predict the novel classes. The householder trans-
formation help the model to learn a more ﬂexible posterior
distribution of dataset with novel classes in few shot setting,
with full rank covariance matrix .

4.4 Losses
The optimisation function for variational inference is given in
Equation 9:

ln p(x) ≥ E

q(z(0)|x)

(cid:104)

ln p(x|z(T )) +

T
(cid:88)

t=1

ln

(cid:12)
(cid:12)
(cid:12)det

∂f (t)
∂z(t−1)

(cid:12)
(cid:105)
(cid:12)
(cid:12)

− β · KL(cid:0)q(z(0)|x)||p(z(T ))(cid:1).
(9)
Cross Entropy loss
In multi class classiﬁcation, the class
label(ys) follows the multinomial distribution given in Equa-
tion 10. So the ﬁrst part of Equation 9 i.e. ln p(x|z(T )) is the
cross entropy loss in our set up. The cross entropy loss(CE) is
given by Equation 11 .

N
(cid:88)

j=1

y(j) = 1

CE(ˆy, y) =

N
(cid:88)

y(j) log ˆy(j)

(10)

(11)

j=1
where y(j) is the true label i.e 0 or 1 and ˆy(j) is the predicted

probability of a class coming from softmax operation.
Jacobian Determinant based loss The second part,
(cid:12)
(cid:12)
ln
(cid:12) becomes zero as we are using the house-
holder ﬂow which is volume preserving based ﬂow, whose
jacobian determinant is 1.

(cid:12)
(cid:12)det ∂Htz(t−1)
(cid:12)

∂z(t−1)

Model

[21]

GenApp
ProtoGAN [4]
ARN [45]
TAM [2]
HF-AR

HMDB51[20]

UCF101[31]

Something-SomethingV2 [8]

1-shot
−
34.7 ± 9.20
44.6 ± 0.9
−
43.4 ± 0.6

5-shot
52.5 ± 3.10
54.0 ± 3.90
59.1 ± 0.8
−
62.2 ± 0.9

1-shot
−
57.8 ± 3.0
62.1 ± 1.0
−
58.6 ± 1.2

5-shot
78.6 ± 2.1
80.2 ± 1.3
84.8 ± 0.8
−
86.4 ± 1.6

1-shot
−
−
−
42.8
43.1 ± 1.7

5-shot
−
−
−
52.3
55.13 ± 3.1

Table 1: Comparison of accuracy between our model (HF-AR) and state-of-the-art approaches on HMDB51, UCF101 and Something-Something
V2 datasets

Kullback–Leibler(KL) divergence loss The third part,
KL(cid:0)q(z(0)|x)||p(z(T ))(cid:1) is the KL loss between with q(z(0)|x)
and p(z(T )) where z(T ) is the last random variable obtained
from householder ﬂow. The KL loss acts as the regularizer
which prevents the model from overﬁtting and helps in learn-
ing a ﬂexible posterior distribution. β value is used to to
control the effect of KL loss. The value of β decreases linearly
with every epoch.

5 Experimental Section
In this work, our task is few-shot video classiﬁcation, where
the objective is to classify novel classes with only a few exam-
ples from the support set.

5.1 Datasets
We evaluate our method (HF-AR) on three publicly available
datasets for action recognition. On these datasets, we measure
the action recognition performance with respect to top-1 and
top-5 recognition accuracy.

• UCF101[31]: contains 13320 videos spanning over 101
classes. For our experiments, we randomly split the data
into 80 seen and 21 novel classes.

• HMDB51[20]: contains 6766 videos spanning over 51
classes. For our experiments we randomly split the data
into 41 seen and 10 novel classes.

• Something-Something V2 [8]: We randomly selected
100 classes from the whole dataset. For our experiments,
we randomly split this data into 76 seen and 24 novel
classes.

Implementation Details

5.2
Pre-Processing Steps
: The videos frames are resized into
224*224 and then normalized before being fed to the activity
recognition model. We have selected 50 frames of the video
to be used in the proposed model. The training procedure
uses the batch of 150 during base model training and 4 during
adapter training . We have used Adam optimizer [16] with the
initial learning rate of 1e − 3 and dynamic adjustment. We
have used ReduceLROnPlateau scheduler [24] with patience
equals to 10.

5.3 Model Details
Base model consists of encoder decoder framework . The
encoder uses the pretrained Resnet 152 which gives 2048

features for every frame and ﬁnally all the 50 frames are con-
catenated to generate the tensor of size (batchsize, 50, 2048).
Three linear layers are used with channel length of (2048 →
300) , (512 → 512), (512 → 512). The 300 embeddings of
every frames are fed into the decoder architecture of the base
model.

The decoder takes the tensor of size (batchsize, 50, 512)
and feeds this to 3 stacked LSTM layers. The output of the last
layer of LSTM is used as an input to the following layers. Two
linear layers having channel length (512 → 256) ,(256 →
classes) are then used to predict the classes.

The adapter model which is used to predict the classes with
few samples uses the pretrained base model as the source of
its input. The pretrained model gives the output of 256 fea-
tures (output of second last layer of base model) is fed to the
householder ﬂow network which predicts the classes. The
householder network uses linear layers to predict the µ and
σ and which generates the vector using the reparametrization
trick, z(0)= (µ + σ (cid:12) (cid:15)). The z(0) goes into series of house-
holder transformations to generate the ﬁnal z(T ) whose feature
size is 256. This goes into linear layer to predict the novel
classes having few samples.

5.4 Comparative Analysis
Table 1 presents the comparison of accuracy obtained between
our approach (HF-AR) vs. state-of-the-art approaches on
the three datasets with 1-shot and 5-shot classiﬁcation. For
5-shot classiﬁcation, HF-AR delivers better accuracy on all
the 3 datasets: 62.2% vs 59.1% (ARN [45]) for HMDB51
dataset, 86.4% vs 84.8% (ARN [45]) for UCF101 dataset,
and 55.13% vs 52.3% (TAM []) for Something-Something V2
dataset. This improvement is due to ﬂexbile posterior model
learned by Householder Flow in our adapter model. For 1-
shot classiﬁcation, HF-AR delivers slightly better accuracy
for Something-Something V2 dataset: 43.1% vs 42.8% ( [2]);
slightly lower accuracy for HMDB51 dataset: 43.4% vs 44.6%
(ARN [45]); and, lower accuracy for UCF101 dataset: 58.6%
vs 62.1% ( [2]). For very limited data as in 1-shot approach,
we believe it becomes harder for our variational inference
based approach to attain the right generalization ability.

5.5 Ablation Study
Effect of length of householder ﬂow : We examined the
behaviour of the length of the series of householder transfor-
mations in the few shot setting. We have done experiments
with HMDB51 dataset to study this in detail. Table 2 shows
that in 1-shot setting as we increase the length, the accuracy

dips as the model is overﬁtting. In 5-shot setting the optimum
length increases as compared to 1-shot setting.

References
[1] C. Bischof and X. Sun. On orthogonal block elimination.

Length
T=1
T=3
T=10
T=20

1-shot
35.1 ± 0.6
43.4 ± 0.6
39.8 ± 0.4
39.2 ± 0.4

5-shot
55.6 ± 0.8
59.9 ± 0.8
62.2 ± 0.9
60.8 ± 0.7

Table 2: Effect of changing the length of householder ﬂow in HF-AR
on accuracy with HMDB51 daatset.

Length
3D-CNN [37]
CompHF
HF-AR

1-shot
45.4 ± 1.6
48.4 ± 2.3
58.6 ± 1.2

5-shot
70.5 ± 1.3
72.2 ± 2.1
86.4 ± 1.6

Table 3: Effect of various Base Model with UCF101 dataset on
accuracy.

Effect of various base models
: We studied the effect of
various base models in few shot setting with UCF101 dataset.
Table 3 shows superior performance of our proposed model
with respect to other base models. 3D-CNN [37] based
base model is not able to predict high quality video embed-
dings which leads to lower accuracy on both seen and unseen
classes. However, our proposed method (HF-AR) uses pre-
trained ResNet-152 which is able to learn good image features
along with LSTM which captures the temporal aspects. The
CompHF base model uses householder ﬂow in the base model
itself to predict the seen classes. It gives lesser accuracy on
seen and unseen classes because the posterior distribution
generated from householder ﬂow fails to capture the more
complex multi-modal distribution of the dataset with larger
number of seen classes. This needs further investigation with
more variety of base models.

6 Conclusions

We propose a variational inference based architectural frame-
work for few shot activity recognition. The proposed archi-
tecture does not require any optical ﬂow for input or super-
vision and provides a uniﬁed and efﬁcient method to predict
unseen classes. This method learns the full rank covariance
matrix of the posterior distribution of datasets with novel
classes. The idea relies on the observation that the true full-
covariance matrix can be decomposed to the diagonal matrix
with eigenvalues on the diagonal and an orthogonal matrix of
eigenvectors that could be further modeled using a series of
Householder transformations. Experimental study on multiple
datasets demonstrates that variational inference with House-
holder ﬂows could perform better than other state of the art
approaches for few shot activity recognition. This makes this
approach a very promising direction for further investigation
in this and other related areas in computer vision.

1996.

[2] Kaidi Cao, Jingwei Ji, Zhangjie Cao, C. Chang, and
Juan Carlos Niebles. Few-shot video classiﬁcation via
temporal alignment. 2020 IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR), pages 10615–
10624, 2020.

[3] João Carreira and Andrew Zisserman. Quo vadis, action
recognition? a new model and the kinetics dataset. 2017
IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), pages 4724–4733, 2017.

[4] Sai Kumar Dwivedi, Vikram Gupta, Rahul Mitra, Shuaib
Ahmed, and Arjun Jain. Protogan: Towards few shot learn-
ing for action recognition. arXiv preprint, 2019.

[5] Chelsea Finn, P. Abbeel, and Sergey Levine. Model-
agnostic meta-learning for fast adaptation of deep networks.
In ICML, 2017.

[6] Victor Garcia and Joan Bruna. Few-shot learning with

graph neural networks. 2018.

[7] Spyros Gidaris and Nikos Komodakis. Generating classi-
ﬁcation weights with gnn denoising autoencoders for few-
shot learning. 2019.

[8] R. Goyal, S. Kahou, Vincent Michalski, Joanna Materzyn-
ska, S. Westphal, Heuna Kim, Valentin Haenel, Ingo Fründ,
P. Yianilos, Moritz Mueller-Freitag, F. Hoppe, Christian
Thurau, I. Bax, and R. Memisevic. The “something some-
thing” video database for learning and evaluating visual
common sense. 2017 IEEE International Conference on
Computer Vision (ICCV), pages 5843–5851, 2017.

[9] Michelle Guo, Edward Chou, De-An Huang, Shuran Song,
Serena Yeung, and Li Fei-Fei. Neural graph matching
networks for fewshot 3d action recognition. 2018.

[10] Bharath Hariharan and Ross B. Girshick. Low-shot vi-
sual object recognition. ArXiv, abs/1606.02819, 2016.
[11] Bharath Hariharan and Ross B. Girshick. Low-shot vi-
sual recognition by shrinking and hallucinating features.
2017 IEEE International Conference on Computer Vision
(ICCV), pages 3037–3046, 2017.

[12] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. 2016 IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 770–778, 2016.

[13] S. Hochreiter and J. Schmidhuber. Long short-term mem-

ory. Neural Computation, 9:1735–1780, 1997.

[14] A. Householder. Unitary triangularization of a nonsym-

metric matrix. J. ACM, 5:339–342, 1958.

[15] Jongmin Kim, Taesup Kim, Sungwoong Kim, and
Chang D. Yoo. Edge-labeling graph neural network for
few-shot learning. 2019.

[16] Diederik P. Kingma and Jimmy Ba. Adam: A method
for stochastic optimization. CoRR, abs/1412.6980, 2015.

[34] E. Tabak and C. Turner. A family of nonparametric
density estimation algorithms. Communications on Pure
and Applied Mathematics, 66:145–164, 2013.

[35] Esteban Tabak and Eric Vanden-Eijnden. Density estima-
tion by dual ascent of the log-likelihood. Communications
in Mathematical Sciences - COMMUN MATH SCI, 8, 03
2010.

[36] Jakub M. Tomczak and M. Welling.

Improving vari-
ational auto-encoders using householder ﬂow. ArXiv,
abs/1611.09630, 2016.

[37] Du Tran, Lubomir D. Bourdev, R. Fergus, L. Torresani,
and Manohar Paluri. Learning spatiotemporal features
with 3d convolutional networks. 2015 IEEE International
Conference on Computer Vision (ICCV), pages 4489–4497,
2015.

[38] Oriol Vinyals, C. Blundell, T. Lillicrap, K. Kavukcuoglu,
and Daan Wierstra. Matching networks for one shot learn-
ing. In NIPS, 2016.

[39] Heng Wang, Alexander Kläser, C. Schmid, and C. Liu.
Action recognition by dense trajectories. CVPR 2011, pages
3169–3176, 2011.

[40] Yu-Xiong Wang, Ross B. Girshick, M. Hebert, and
Bharath Hariharan. Low-shot learning from imaginary
data. 2018 IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 7278–7286, 2018.

[41] Zhongwen Xu, Linchao Zhu, and Yi Yang. Few-shot
object recognition from machine-labeled web images. 2017
IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), pages 5358–5366, 2017.

[42] Baohan Xu, Hao Ye, Yingbin Zheng, Heng Wang, Tianyu
Luwang, and Yu-Gang Jiang. Dense dilated network for
few shot action recognition. In ICMR, 2018.

[43] Chenrui Zhang and Y. Peng. Visual data synthesis via
gan for zero-shot video classiﬁcation. In IJCAI, 2018.
[44] R. Zhang, Tong Che, Zoubin Ghahramani, Yoshua Ben-
gio, and Y. Song. Metagan: An adversarial approach to
few-shot learning. In NeurIPS, 2018.

[45] Hongguang Zhang, Liyong Zhang, Xiaojuan Qi, Hong-
dong Li, Philip H. S. Torr, and Piotr Koniusz. Few-shot
action recognition with permutation-invariant attention. In
ECCV, 2020.

[46] Linchao Zhu and Y. Yang. Compound memory networks

for few-shot video classiﬁcation. In ECCV, 2018.

[17] Diederik P. Kingma and M. Welling. Auto-encoding

variational bayes. CoRR, abs/1312.6114, 2014.

[18] Gregory R. Koch. Siamese neural networks for one-shot

image recognition. 2015.

[19] A. Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works. Communications of the ACM, 60:84 – 90, 2012.
[20] Hilde Kuehne, Hueihan Jhuang, Estíbaliz Garrote,
T. Poggio, and Thomas Serre. Hmdb: A large video
database for human motion recognition. 2011 Interna-
tional Conference on Computer Vision, pages 2556–2563,
2011.

[21] Ashish Mishra, V. Verma, M. K. Reddy, Arulkumar Sub-
ramaniam, Piyush Rai, and Anurag Mittal. A generative ap-
proach to zero-shot and few-shot action recognition. 2018
IEEE Winter Conference on Applications of Computer Vi-
sion (WACV), pages 372–380, 2018.

[22] Ashish Mishra, Vinay Kumar Verma, M Shiva Krishna
Reddy, S Arulkumar, Piyush Rai, and Anurag Mittal. A
generative approach to zero-shot and few-shot action recog-
nition. 2018.

[23] Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and
P. Abbeel. A simple neural attentive meta-learner. In ICLR,
2018.

[24] K. Mukherjee, Alind Khare, and A. Verma. A simple dy-
namic learning rate tuning algorithm for automated training
of dnns. ArXiv, abs/1910.11605, 2019.

[25] S. Ravi and H. Larochelle. Optimization as a model for

few-shot learning. In ICLR, 2017.

[26] Mengye Ren, Eleni Triantaﬁllou, Sachin Ravi, Jake Snell,
Kevin Swersky, Joshua B. Tenenbaum, Hugo Larochelle,
and Richard S. Zemel. Meta-learning for semi-supervised
few-shot classiﬁcation. CoRR, abs/1803.00676, 2018.
[27] Danilo Jimenez Rezende and S. Mohamed. Variational

inference with normalizing ﬂows. In ICML, 2015.

[28] Danilo Jimenez Rezende, S. Mohamed, and Daan Wier-
stra. Stochastic backpropagation and approximate inference
in deep generative models. In ICML, 2014.

[29] K. Simonyan and Andrew Zisserman. Two-stream con-
volutional networks for action recognition in videos. In
NIPS, 2014.

[30] J. Snell, Kevin Swersky, and R. Zemel. Prototypical

networks for few-shot learning. In NIPS, 2017.

[31] K. Soomro, A. Zamir, and M. Shah. Ucf101: A dataset of
101 human actions classes from videos in the wild. ArXiv,
abs/1212.0402, 2012.

[32] X. Sun and C. Bischof. A basis-kernel representation of
orthogonal matrices. SIAM J. Matrix Anal. Appl., 16:1184–
1196, 1995.

[33] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip
H. S. Torr, and Timothy M. Hospedales. Learning to
compare: Relation network for few-shot learning. CoRR,
abs/1711.06025, 2017.

