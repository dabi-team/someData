2
2
0
2

g
u
A
2
1

]

G
L
.
s
c
[

1
v
3
8
1
6
0
.
8
0
2
2
:
v
i
X
r
a

Non-Autoregressive Sign Language Production
via Knowledge Distillation

Eui Jun Hwang, Jung Ho Kim, Suk Min Cho, and Jong C. Park

Korea Advanced Institute of Science
and Technology (KAIST)
Daejeon, Korea {ejhwang,jhkim,park}@nlp.kaist.ac.kr

Abstract. Sign Language Production (SLP) aims to translate expres-
sions in spoken language into corresponding ones in sign language, such
as skeleton-based sign poses or videos. Existing SLP models are either
AutoRegressive (AR) or Non-Autoregressive (NAR). However, AR-SLP
models suffer from regression to the mean and error propagation during
decoding. NSLP-G, a NAR-based model, resolves these issues to some
extent but engenders other problems. For example, it does not consider
target sign lengths and suffers from false decoding initiation. We propose
a novel NAR-SLP model via Knowledge Distillation (KD) to address
these problems. First, we devise a length regulator to predict the end
of the generated sign pose sequence. We then adopt KD, which distills
spatial-linguistic features from a pre-trained pose encoder to alleviate
false decoding initiation. Extensive experiments show that the proposed
approach significantly outperforms existing SLP models in both Fr√©chet
Gesture Distance and Back-Translation evaluation.

Keywords: Sign Language Production; Knowledge Distillation; Non-
Autoregressive Generation; Deep Learning

1

Introduction

Sign language (SL), a rich multi-channel language, is the primary form of com-
munication in the Deaf communities. However, unlike spoken languages, manual
and non-manual components such as hands, face, and body play essential roles
in making effective communication [24]. This aspect of SL gives rise to a com-
munication gap between SL users and spoken language users. Therefore, many
researchers have proposed various methods to convert expressions in spoken lan-
guage into corresponding ones in sign language to fill this gap.

Sign Language Production (SLP) aims to translate expressions in spoken
language into corresponding ones in SL, such as sign skeleton-based [25,27,14],
avatar-based [16] sign poses, and videos [28]. As shown in Figure 1, recent SLP
works have been carried out either gloss-to-pose (G2P) or text-to-pose (T2P).
G2P does not use text but only glosses as input to produce the sign pose se-
quence. On the other hand, T2P produces the sign pose sequence directly from

 
 
 
 
 
 
2

Hwang et al.

Fig. 1: An overview of Sign Language Production (SLP). There are two different tasks:
Gloss-to-Pose (G2P) and Text-to-Pose (T2P). Recent SLP works demonstrate that T2P
performs better than G2P. Hence, we focus on the T2P task in this work.

the given text without intermediate glosses1. According to recent SLP stud-
ies [27,14], T2P has achieved better performance than G2P, suggesting that
gloss representation may be losing some rich information in sign language ex-
pressions. Therefore, in this work, we focus on T2P to overcome the limitations
of the gloss representation.

Most existing approaches to T2P use an AutoRegressive (AR) model based
on direct mapping to generate the current pose depending on the previous one.
However, these approaches often regress to the mean, propagate errors during
decoding and suffer from the slow decoding procedure [13]. To address these lim-
itations, Gaussian Mixture Network [26] and Mixture of Motion Primitives [27]
have been proposed. While these efforts have mitigated the limitations to some
extent, the fundamental problem posed by AR approaches has not yet been fully
resolved.

To solve the problems in AR models, Huang et al. [13] and Hwang et al. [14]
proposed NAT-EA and NSLP-G, respectively, which work in a NAR manner.
NAT-EA is designed for the G2P task and consists of a transformer-based en-
coder, spatial-temporal graph pose generator, and a length regulator. NSPL-G
has achieved the goal with two learning steps: learning the spatial-linguistic as-
pect of SL using Variational Autoencoder (VAE) [17] and mapping text into the
spatial linguistic feature using NAR Transformer. Note that NAT-EA is designed
for the G2P task only and is not considered in this paper.

Although NSLP-G [14] outperforms the previous AR models, it still has sev-
eral limitations. (1) The model does not include a component to predict the
target length but instead introduces a masked loss as a partial solution. (2) It

1 Glosses are a simplified notation that connects spoken language and SL.

Spoken LanguageSign GlossSign Pose SequenceGloss-to-Pose (G2P)Text-to-Pose (T2P)Text-to-Gloss (T2G)DIENSTAG(TUESDAY)HAUPTSAECHLICH(MAINLY)SONNE(SUN)NORDOST(NORTHEAST)WOLKE(CLOUD)MOEGLICH(POSSIBLE)REGEN(REGEN)am dienstagistes meistfreundlichimnordostenhaltensichdichterewolkenmitetwasregen(on Tuesday it is mostly friendly in the northeast there are thicker clouds with a little rain)Abbreviated paper title

3

Fig. 2: An overview of the proposed NAR-SLP model. To apply Knowledge Distilla-
tion (KD), we pre-trained a sign pose network based on VAE. The trained encoder
and decoder are used as Latent Guiding Module (LGM) and Pose Generating Mod-
ule (PGM), respectively. Latent Learning Module (LLM) generates a length-regulated
sequence of latent representation. ‚äï denotes a length regulating operation. LLM con-
sists of three sub-modules: BERT as a language embedding module, Transformer-based
NAR decoder, and Length Regulator. During training, as a pose distillation process,
LGM acts as a teacher network, teaching LLM to generate zlang similar to zpose.

performs worse in a short range of prediction. We call it ‚Äúfalse decoding ini-
tiation‚Äù. (3) VAE has been optimized with Mean Squared Error (MSE) loss.
However, MSE is not relative and tends to lose its signal when processing per-
ceptually important signals [31], making it difficult to produce detailed outputs
such as hand representations in SL.

We propose a novel NAR-SLP model via Knowledge Distillation (KD) [12]
as shown in Figure 2. To resolve (1), our model is designed to generate length-
regulated sign poses by introducing a length regulator. A pre-trained language
model such as BERT [5] is employed for increased modality in our model. To
resolve (2), we introduce KD, which aligns language features directly with sign
pose features. This alleviates the false decoding initiation by a large margin
throughout experiments. To resolve (3), we assume that the final output of the
model follows a multivariate Bernoulli distribution. The sign poses can there-
fore be interpreted as an estimate of the probability that a particular point is
attended by an observer. Hence, we optimize VAE using Binary Cross Entropy
(BCE) loss [4].

The difference from the previous NAR model is the exact formulation of
the length regulator and application of KD. In particular, KD addresses the
performance degradation due to increased modality in our model as well as false
decoding initiation. Our main contributions are summarized as follows:

BERTSpoken Language SentenceNAR Transformer DecoderPretrained & FreezeLearnableLength RegulatorPE‚Ä¶‚Ä¶Pose Distillationùëç!"#$ùëç%&‚Äô(‚Ä¶Pose Generating ModuleLatent Learning Module (Student Model)Sign Pose SequenceFCFCFCGenerated Sign Pose SequenceLatent Guiding Module(Teacher Model)Only TrainingFCFCFCFCFCFC4

Hwang et al.

‚ó¶ We introduce a novel Non-Autoregressive Sign Language Production (NAR-
SLP) model via Knowledge Distillation (KD) for the T2P task. This is the
first SLP model to apply the KD approach.

‚ó¶ We design the proposed model to generate length-regulated outputs to fill

the gap in the previous NAR-SLP model.

‚ó¶ Our approach improves performance by addressing the limitations of previ-

ous SLP models.

‚ó¶ Extensive experiments demonstrate the effectiveness of the proposed model

on the PHOENIX-2014T dataset.

2 Related Work

Non-Autoregressive Models. AR-based models have achieved great success
in machine translation [29]. Typically, they generate outputs sequentially where
the current output is dependent on the previous one. In the case of predicting
the human pose, however, AR models are prone to converge to a mean pose,
which hinders predictions of realistic poses [19]. Several efforts have been made
to avoid AR modeling. Gu et al. [10] propose Non-Autoregressive Transformer
(NAT) that uses a fertility predictor to represent the number of times each
source token is copied to predict the target length. Li et al. [18] use the NAR
decoder to independently generate each human pose given context features from
the encoder and positional information.

Knowledge Distillation. KD is a method of distilling knowledge from a
teacher model to a student for model compression and acceleration [12], a method
frequently used in various fields, such as natural language processing and visual
recognition [9]. Wang et al. [30] exploit KD for guiding the pose estimator to
learn clean pose knowledge during training on hard samples with noise. Also,
Khan et al. [23] propose a KD framework for transferring the knowledge from a
pre-trained linguistic vision models to multilingual BERT.

Sign Language Production. Previous SLP works have started with avatar-
based approaches. They can produce human-like signs but rely on phrase look-
ups and predefined motion dictionaries [21], or require expensive motion capture
or pre-recorded phrases [20]. With recent advances in deep learning, Stoll et
al. [28] propose the first SLP model to translate text into glosses and map them
to corresponding sign poses. Zelinka and Kanis [36] propose the first end-to-end
SLP model with a fixed length. They also propose a gradient descent method
for skeletal refinement. Recently, Jiang et al. [15] propose a transformer-based
skeletal refinement model that refines the 3D lifted skeleton with a BERT-like
training approach. Saunders et al. [25] propose Progressive Transformer (PT),
which uses a counter-encoding scheme to directly learn the mapping between
spoken language and sign pose sequence. As follow-up studies, a Mixed Density
network [26] and a Mixture of Experts [27] are applied to PT. For the G2P

Abbreviated paper title

5

task, Huang et al. [13] propose a NAR-SLP model with an External Aligner for
sequence alignment learning. Hwang et al. [14] propose a NAR-SLP model for
both G2P and T2P, which adopts indirect mapping using pre-trained VAE.

3 Approach

Our proposed approach is shown in Figure 2. We utilize a VAE model trained
in individual sign poses, similar to NSLP-G. However, while NSLP-G only uses
a pre-trained decoder to align the outputs of a NAR Transformer, both encoder
and decoder are used in our model. The trained encoder is used as a teacher
network called Latent Guiding Module (LGM), and the trained decoder is used
as Pose Generating Module (PGM). Latent Learning Module (LLM), a student
network, generates a length-regulated sequence of latent representation from the
given spoken language sentence. LLM learns how to generate an appropriate
latent representation for sign pose generation in the teacher network across the
different modalities between spoken language and sign poses.

In the following subsections, we will first define the problem of SLP and then

cover each module in detail, followed by a training scheme.

3.1 Problem Definition

Given a sequence of spoken words X = (x1, x2, ..., xu), the goal of SLP is to
generate a consecutive sign pose sequence with length T , Y = (y1, y2, ..., yT ) ‚àà
RT √óJ√óK, where each frame yt = {yj
j=1 represents a single sign pose, containing
J joint data. yj
t ‚àà RK is a minimal per-joint representation at the t-th frame
and the j-th joint, of which K is the feature dimension which presents the sign
pose joint data.

t }J

Our goal is to train a NAR Transformer to generate the realistic sign pose
sequence Y with a corresponding length T from the given words X. We introduce
two different conditional probabilities, P (Z|X) and g(Y |Z), as in the previous
work [14]. However, their work does not truly satisfy P (Z|X) because there is no
part for the target length T . Therefore, we introduce revised P (Z|X) to achieve
our goal. More details are covered in Section 3.3. We stipulate that the model
has Z = (z1, z2, ..., zT ) as an intermediate representation between spoken words
X and sign pose sequence Y .

3.2 Latent Guiding and Pose Generating Modules

We first train VAE on individual sign poses. It can be represented as:

enc(y) = q(z|y), dec(z) = p(y|z),

(1)

where y denotes a single sign pose skeleton and z denotes an encoded sign pose
latent.

More specifically, y is encoded as sign pose distribution parameters ¬µ and
œÉ of the sign pose distribution. We use reparameterization [17] to sample a

6

Hwang et al.

latent space z ‚àà Rd from this distribution. The decoder is used to reconstruct
the original sign pose y, given the corresponding z. The objective of VAE is
formulated as:

Lvae(y) = ‚àíEq(z|y)[logp(y|z)] + ŒªKL(q(z|y)||p(z)),

(2)

where p(z) is the prior distribution, KL is the Kullback-Leibler divergence, and Œª
is the hyper-parameter to control the balance of losses. The first term allows the
model to encode the sign pose y into the latent space z for reconstruction. The
second term pushes posterior distribution to be close to the prior distribution.
After the learning process, the trained encoder and decoder are used as the
teacher network LGM and PGM, respectively.

3.3 Latent Learning Module

We propose LLM to model P (Z|X). As shown in Figure 2, it has different sub-
modules: a spoken language encoder, length regulator, and NAR decoder. The
previous work [14] achieves the SLP model non-autoregressively but does not
include a conditional probability of the target length. Instead, they introduce a
masked loss to solve this problem partially. To fill this gap, we redefine P (Z|X)
as follows:

pN A(Z|X) = pL(T |x1:U ) ¬∑

T
(cid:89)

t=1

p(zt|x1:U ),

(3)

where pL(T |x1:U ) is a conditional probability of a target length T . More details
of each sub-module are described in the following paragraphs.

Language Encoder and Length Regulator. We employ BERT [5] as the
spoken language encoder. BERT is trained on a very large unlabeled corpus and
is successfully applied to the SLT task [22].

To satisfy the first term pL(T |x1:U ) in Equation 3, we employ a simple net-
work consisting of multiple fully-connected (FC) layers with a Sigmoid activation
at the end of the layer to adjust the length of the generated sequence of latent
representation. Specifically, we design the length regulator to generate the ra-
tio of target sign length to maximum target length using [CLS] token, which is
widely used in BERT-based classification tasks [35,8].

Non-AutoRegressive Decoder. Our model uses a NAR Transformer decoder
but removes the AR connection. It is based solely on an attention mechanism
to generate representations of entire sequences with global dependencies. It is
formulated as:

Attention(Q, K, V ) = sof tmax(

QK T
‚àö
dk

)V,

(4)

where Q, K and V are query, key and value, respectively. Finally, Multi-Head
Attention (MHA) can be formulated as:

Abbreviated paper title

7

M HA(Q, K, V ) = Concat(headi, ..., headn)WO,
headi = Attention(QW i

Q, KW i

K, V W i

V ),

(5)

(6)

where WQ,WK, and WV are weights related to each input.

Given the embedded input and temporal information, the NAR decoder gen-
erates a sequence of latent Zlang of the given spoken words X. Specifically, it
takes Positional Encoding (PE) as Q and the encoder output as K and V , re-
spectively.

3.4 Training

We define several loss terms to train our model in this section and present an
ablation study in Section 5.4.

Pose Loss. Previous work [14] uses MSE loss to train the VAE. However, MSE
tends to lose its signal when processing perceptually important signals [31]. As
a result, the model does not effectively capture detailed outputs such as hand
movement and facial expressions, which are relatively a small variance compared
to body movements within the dataset.

To address this problem, we normalize the ground-truth joints so that each
value is in the range [0, 1]. The sign poses can therefore be interpreted as an
estimate of the probability that a particular joint is attended by an observer. As
with the work presented in [17], it is tempting to induce a multivariate Bernoulli
distribution. Thus, Sigmoid has been applied element-wise and BCE loss is used
to measure the difference between the ground-truth sign pose sequence Y and
generated sign pose sequence ÀÜY , defined as:

Lpose = ‚àí

1
N

N
(cid:88)

i=1

(yilog(ÀÜyi) + (1 ‚àí yi)log(1 ‚àí ÀÜyi)),

(7)

where ÀÜyi denotes the predicted joint and yi denotes the corresponding target
joint from the ground-truth.

On the other hand, MSE loss is used to optimize LLM through the pre-trained
pose generator PGM indirectly. This is because we assume that the NAR decoder
does not follow the Bernoulli distribution. Hence, it can be defined as:

Lpose =

1
N

N
(cid:88)

(yi ‚àí ÀÜyi)2.

i=1

(8)

Distillation Loss. We argue that input language features should be directly
aligned with the sign pose features to generate a sign pose sequence correctly.

8

Hwang et al.

This helps the model to represent the cross-modality between language and sign
poses. We compute a pose distillation loss, which directly measures the difference
between Zlang and Zpose. It plays a significant role in our model throughout
experiments and can be defined as:

Ldistil =

1
N

N
(cid:88)

(zpose
i

i=1

‚àí zlang
i

)2.

(9)

Length Loss. As described in Section 3.3, our model is designed to generate
length-regulated outputs. Specifically, the regulator determines how many frames
should be discarded from the generated latent sequence. Therefore, BCE loss is
used to measure the difference between the ground-truth length ratio L, and the
predicted length ratio ÀÜL, defined as:

Llength = ‚àí

1
N

N
(cid:88)

i=1

(lilog(ÀÜli) + (1 ‚àí li)log(1 ‚àí ÀÜli)).

(10)

The resulting total loss is defined as the summation of different terms, as:

L = Lpose + Ldistil + Llength,

(11)

where the loss terms are assigned the same weight empirically.

4 Experimental Settings

In this section, we describe the dataset and prepossessing, followed by the imple-
mentation details of our model. We then introduce baselines in our experiments.

4.1 Dataset and Preprocessing

We use the publicly available RWTH-PHOENIX-Weather 2014T dataset [1]. To
the best of our knowledge, the current trend in SLP works is using one method
on a single dataset due to the scarcity of SLP datasets. The PHOENIX-2014T
dataset was chosen because it has been extensively and actively used in sign
language research. This dataset contains 8,257 pairs of German and German
Sign Language (DGS) videos with word-level annotations, collected from weather
forecast of PHOENIX TV station. It includes 2,887 different German words
and 1,066 different DGS glosses. We use OpenPose [3] to extract manual and
non-manual features. The manual features are lifted into 3D using the skeletal
correction model [36]. Recently, Duarte et al. [6] introduced a new benchmark
dataset for SL field. However, since the dataset is not yet fully released, we do
not include results on the dataset.

Abbreviated paper title

9

4.2

Implementation Details

For all our experiments, LGM and PGM have 3 linear layers with ReLU and
PGM has Sigmoid at the end of the layer, with Œª set to 0.0001. In LLM, we
set the embedding dimension to 768, the number of layers to 8, the number of
heads in multi-head attention to 8, the dropout rate to 0.1, and the dimension
of the intermediate feedforward network to 2,048. All parts of our network are
trained with Xavier initialization and AdamW optimization, with a learning rate
of 0.0002. Our model is implemented using PyTorch Lightning [7]. Training takes
24 hours for 500 epochs on a single Tesla V100 GPU, using 40GB GPU memory
with batch size 40.

4.3 Baselines

We compare our approach to several other methods, including AR and NAR
state-of-the-art models and other baseline models. Note that these models are
trained for 500 epochs for a fair comparison. Overall, more epochs improve per-
formance, but we stop training to keep computational costs low.

Ground Truth (GT). We use sign poses of human signers in the PHOENIX-
2014T dataset. Note that we considered the sign poses extracted from Openpose
as the ground truth.

Mean and Random. To set the bottom baselines, we employ two different
models, Mean and Random, each generating the mean pose and a randomly
selected pose sequence from the training set.

Progressive Transformer (PT). Since PT [25] is the only publicly available
AR-SLP model, we compare the T2P model of PT with our approach. Note that
the results they presented are not comparable to ours as the authors did not
release a pre-trained BT model and a full sign pose joint data, including facial
landmarks. Therefore, we reproduce the results of the T2P models using base,
and Future Prediction and Gaussian noise (FP&GN) settings, which shows the
best performance in their paper.

NSLP-G. NSLP-G [14] is the first NAR-based model to apply an unsupervised
learning approach. We compare our approach with their T2P models using base,
gloss supervision (GS), and fine-tuning (ft) settings. The results presented in
their work are not the same as ours. This is because, for FGD, we use BCE loss
to optimize the evaluation model for the reasons stated in Section 3.

10

Hwang et al.

5 Experimental Results

We conduct extensive experiments to evaluate our approach and compare it
against the aforementioned baselines. The experiments are designed to validate
the proposed solutions to the limitations described in Section 1. In this section,
we describe the evaluation metrics, followed by quantitative and qualitative re-
sults.

5.1 Evaluation metrics

Fr√©chet Gesture Distance. We evaluate the visual quality and realism of the
generated sign pose sequence using Fr√©chet Gesture Distance (FGD) [34]. It is
based on the concept of Fr√©chet Inception Distance (FID) [11], and measures
how close the distribution of generated sign pose sequence ÀÜY is to real sign pose
sequence Y . It can be formulated as:

F GD(Y, ÀÜY ) = ‚à•¬µr ‚àí ¬µg‚à•2 + T r(Œ£r + Œ£g ‚àí 2(Œ£rŒ£g)

1
2 ),

(12)

where ¬µr and Œ£r are the first and second moments of the latent feature distri-
bution Zr of real sign poses Y , respectively, and ¬µg and Œ£g are the first and
second moments of the latent feature distribution Zg of generated sign poses ÀÜY ,
respectively. Transformer-based Autoencoder (TAE) [14] is used as an evalua-
tion model for FGD. It is trained on the PHOENIX-2014T dataset with a fixed
length of sign pose sequences. We set it to 34.

Back-Translation. We apply the Back-Translation (BT) method proposed
in [25] as a means of BLEU and ROUGE evaluation. BT uses pre-trained Sign
Language Translation (SLT) model to translate the generated sign pose sequence
into spoken language or sign glosses. We use a state-of-the-art SLT model [2]
modified to take a sign pose sequence as an input. It is trained on the PHOENIX-
2014T dataset.

5.2 Quantitative Results

Overall Assessment. As shown in Table 1, our method shows the best per-
formance on all criteria. FGD of ours is 0.15, which is significantly lower than
other methods. This indicates that the model can generate outputs much closer
to a real sign pose sequence. It also achieves 13.90 and 36.89 in BLEU-4 and
ROUGE, respectively, which is a significant improvement over all other methods.
However, without KD, our model shows worse performance than NSLP-G in all
criteria. These results clearly demonstrate that KD has a significant impact on
our method.

Abbreviated paper title

11

FGD‚Ä† BLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE FGD‚Ä† BLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE

DEV

TEST

0.0

0.49
0.44

32.28

13.93
17.36

19.51

13.93

10.96

3.07
5.04

0.98
1.94

0.40
0.91

32.47

14.28
16.40

0.0

0.56
0.48

32.48

13.25
18.13

19.57

13.83

10.71

2.84
5.75

0.84
2.20

0.29
0.92

32.22

14.40
17.46

Autoregressive Model

Models

GT

Mean
Random

PT

Base
FP&GN

1.01
0.33

18.47
17.28

5.07
5.49

1.90
2.65

0.97
1.63

17.26
16.26

0.97
0.32

17.55
16.52

4.59
4.24

1.62
1.40

0.73
0.58

17.38
16.30

Non-Autoregressive Model

NSLP-G*
Base
GS
Base (+ft)
GS (+ft)

Ours

0.24
0.25
0.22
0.25

28.24
25.97
33.01
30.62

15.01
13.19
19.67
16.75

9.85
8.62
14.02
10.92

7.35
6.45
10.97
8.13

27.19
25.32
32.21
29.41

0.26
0.28
0.23
0.27

28.60
26.92
32.63
29.89

15.89
14.04
19.42
16.57

10.78
9.30
13.47
11.03

8.23
7.03
10.39
8.26

27.94
26.92
32.24
29.16

w/o KD
w/ KD

0.32
0.15

25.54
36.81

12.82
23.90

8.70
17.50

6.62
13.90

25.87
36.89

0.36
0.16

25.91
37.43

13.56
24.50

9.14
17.83

6.90
13.92

26.24
37.26

Table 1: We compare the recent SLP work on the PHOENIX-2014T dataset using
FGD (lower is better), BLEU and ROUGE metrics. Note that, due to differences in
implementation (lifted sign pose data, number of epochs used), the metrics for the base-
lines differ from those reported in their paper. * indicates that we test the model under
a fair setting, and ‚Ä† indicates scaled 10√ó for better readability. ft denotes finetuning of
the NSLP-G model, and KD denotes Knowledge Distillation of our model.

Models

0‚àº50
(0‚àº2s)

Intervals (Seconds*)
100‚àº150
50‚àº100
(4‚àº6s)
(2‚àº4s)

Short Intervals (Seconds*)

150‚àº200
(6‚àº8s)

‚àº20
(‚àº0.8s)

‚àº40
(‚àº1.6s)

‚àº60
(‚àº2.4s)

‚àº80
(‚àº3.2s)

PT
PT(FN&GN)

0.35
0.24

NSLP-G (base+ft)
Ours (w/ KD)

0.45
0.21

Autoregressive

1.17
0.36

1.18
0.37

Non-Autoregressive

0.16
0.12

0.11
0.09

0.91
0.29

0.28
0.18

0.04
0.05

0.50
0.09

0.23
0.25

0.60
0.24

0.46
0.23

0.42
0.22

0.60
0.23

0.39
0.20

Table 2: FGD results of comparison with state-of-the-art models according to each
frame length interval. * indicates that the generated frames are assumed to be 25 FPS.
Lower numbers indicate better performance. Bold is the best, and underlined is second.

Frame Length Assessment. The outputs of each model are further evaluated
over a range of frame lengths (or duration) to verify for error propagation [18]
and false decoding initiation described in Section 1. Specifically, FGD of the
generated sign pose sequences is measured and averaged over each frame length
interval. In terms of error propagation, FGD of PT starts at 0.24 and eventually
increases to 0.37, as shown in Table 2. The data augmentation options (FN&GN)
alleviate this, but it still exists. This demonstrates that error propagation exists
in AR-based models as FGD keeps increasing with a longer frame length. On
the other hand, NAR-SLP models resolve this issue and perform better at longer
frame lengths.

We also measure FGD of each model at an early stage of prediction to verify
false decoding initiation in NAR-based models. As shown in Table 2, FGD of

12

Hwang et al.

Fig. 3: Visual comparison of generated sign poses over all methods.

NSLP-G is much larger than that of other models, showing the negative effect
of the NAR method to some extent. However, our method alleviates the prob-
lem dramatically by reducing it from 0.50 to 0.09. This fact verifies that KD
effectively resolves the false decoding initiation. Furthermore, both PT model
outperforms the NAR-based model by achieving 0.04 and 0.05, respectively, be-
cause the AR-based models take the first sign pose and timing information from
their ground truth during decoding [13].

5.3 Qualitative Results

The qualitative results of our model against other state-of-the-art models are
shown in Figure 3. The first frame shows that PT starts with a given ground-
truth frame but shows less dynamic and detailed outputs than other models.
In contrast, NSLP-G is better at capturing frame-level movements without the
error propagation problem. However, it starts with a poor sign pose and lacks
detailed hand expressions within the overall sequence. Our approach shows a
better start sign pose and more detailed hand expressions than NSLP-G.

5.4 Ablation Study

To better understand our model in detail, we performed an ablation study to
access several components, including KD and PLM, of our model in a controlled
setting.

Loss study. We investigate the influence of the proposed loss formulation as
in Equation 11. We measure FGD and Duration Accuracy (DurACC) for the

OursGTInputNSLP-GOriginal Videoam dienstag ist es meist freundlich im nordosten halten sich dichtere wolken mit etwas regen(on Tuesday it is mostly friendly in the northeast there are thicker clouds with a little rain)PTAbbreviated paper title

13

Models

GT

Lpose + Llength
Lpose + Llength + Ldistil

DEV

DurACC
(<1s)

1.0

0.26
0.76

FGD‚Ä†

0.0

0.32
0.15

TEST

DurACC
(<1s)

1.0

0.23
0.76

FGD‚Ä†

0.0

0.36
0.16

Models

GT

w/o BERT
w/ BERT

FGD‚Ä†

0.0

0.27
0.15

DEV

DurACC
(<1s)

1.0

0.50
0.76

TEST

DurACC
(<1s)

1.0

0.52
0.76

FGD‚Ä†

0.0

0.30
0.16

(a) Effect of KD

(b) Effect of PLM

Table 3: Performance comparison between different loss types and architecture choices.
Note that Llength is a mandatory loss of our model to regulate output length.

generated sign pose sequences and target frame length with the models trained
with Lpose and Lpose +Ldistil, respectively. Note that Llength is a mandatory loss
of our model, as we design our model to generate both sign poses and length reg-
ulating ratio. DurACC assumes that the generated sign pose output is recorded
at 25 FPS and measures the accuracy with less than a second margin of error.
As shown in Table 3a, a single loss is insufficient to optimize the model properly.
On the other hand, using combined loss significantly improves the results in both
FGD and DurACC. Thus, we conclude that Ldistil plays a significant role in the
multi-modal prediction of our model.

Architecture design. We further experiment with different architectural choices
to determine the impact of using PLM for multi-modal prediction. First, we im-
plement a simple transformer as a baseline. Specifically, to implement the base-
line, we use the [BOS] token for a pooling purpose to predict the target frame
lengths of the given spoken language. Moreover, we use the same number of lay-
ers and dimensions as the BERT-based model for a fair comparison. As shown
in Table 3b, the performance results indicate that the model without BERT
is far behind the model with BERT. Therefore, we conclude that the simple
Transformer insufficient enough to learn two different modalities from the given
input.

6 Discussion

6.1 Error Propagation Problem

According to [18], AR-based models such as PT can easily propagate decoding
errors to the next prediction. As shown in Figure 4, the FGD scores of PT models
keep increasing with a longer frame length because the decoding errors continue
to propagate. The data augmentation options (FN&GN) alleviate this problem,
but the problem still exists. This is because, to the best of our knowledge, AR-
based models use a greedy decoding strategy to obtain sign pose sequences. On
the other hand, NAR-based models such as NSLP-G and ours show consistent
scores at longer frame lengths as the NAR decoder outputs sign pose sequence
in parallel.

14

Hwang et al.

Fig. 4: FGD line graph for each model according to frame length. The red dotted box
indicates false decoding initiation in NAR models.

6.2 Effect of Knowledge Distillation

We further verify the false decoding initiation in the previous NAR model, which
performs worse at shorter frame lengths. As shown in Figure 4, NSLP-G shows
the worst performance in the early stage of prediction (red dotted box), which
leads to overall performance degradation. In NSLP-G and our model, the pre-
trained VAE is responsible for the spatial-linguistic features of SL and flows
this information to the NAR Transformer. The previous model uses only a pre-
trained decoder to align zlang indirectly, resulting in false decoding initiation.
On the other hand, the proposed model alleviates this problem by a large margin
by introducing KD that aligns directly with zpose. Therefore, we conclude that
KD is effective in NAR-based SLP models.

6.3 Limitation of Back-Translation Evaluation

Surprisingly, as shown in Table 1, our best performance surpasses the GT per-
formance in the BT evaluation setting. It demonstrates that BT is limited in
measuring the performance of the SLP models. BT also does not give consistent
results when comparing PT with Mean and Random models as PT performs
better in BLEU but worse in FGD. This is because BT strongly depends on the
performance of the SLT model, and the translation performance is not yet sta-
ble enough to warrant the SLP models [14]. The evaluation results may become
more accurate with the recent SLT model such as STMC [33], suggesting that
BT cannot be an absolute metric for SLP. However, at least BT serves as an
indicator of how well the sign poses are generated in the evaluation results.

7 Conclusions

In this work, we proposed a novel Non-AutoRegressive Sign Language Produc-
tion (NAR-SLP) via Knowledge Distillation (KD) to address the limitations of

Abbreviated paper title

15

the existing SLP models, i.e., absence of target length prediction, error propa-
gation, false decoding initiation, and less detailed outputs. We design our model
to predict a sign pose sequence with its length. Furthermore, KD is adopted to
directly align the outputs of a NAR Transformer with the pre-trained pose en-
coder. As a result, it alleviates the problem of false decoding initiation as well as
the performance degradation due to the increased modality in our model. More-
over, our model introduces a multivariate Bernoulli distribution for sign poses,
achieving a more detailed output. Extensive experiments have demonstrated the
superior performance of the proposed approach. One of the particularly attrac-
tive properties of our method is that it learns the spatial and temporal aspects
of Sign Language (SL) separately. Future work may therefore exploit our model
to further improve the spatial aspect by introducing a graph representation [32].
We also plan to apply our model to a new benchmark SL dataset [6]. The code
will be made publicly available.

References

1. Camg√∂z, N.C., Hadfield, S., Koller, O., Ney, H., Bowden, R.: Neural Sign Language
Translation. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition. pp. 7784‚Äì7793 (2018)

2. Camg√∂z, N.C., Koller, O., Hadfield, S., Bowden, R.: Sign Language Transform-
ers: Joint End-to-end Sign Language Recognition and Translation. In: Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.
10023‚Äì10033 (2020)

3. Cao, Z., Hidalgo, G., Simon, T., Wei, S.E., Sheikh, Y.: OpenPose: Realtime Multi-
person 2D Pose Estimation using Part Affinity Fields. IEEE transactions on pat-
tern analysis and machine intelligence 43(1), 172‚Äì186 (2019)

4. De Boer, P.T., Kroese, D.P., Mannor, S., Rubinstein, R.Y.: A tutorial on the cross-

entropy method. Annals of operations research 134(1), 19‚Äì67 (2005)

5. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of Deep Bidi-
rectional Transformers for Language Understanding. In: Proceedings of the 2019
Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers).
pp. 4171‚Äì4186. Association for Computational Linguistics, Stroudsburg, PA, USA
(2019). https://doi.org/10.18653/v1/N19-1423, http://aclweb.org/anthology/
N19-1423

6. Duarte, A., Palaskar, S., Ventura, L., Ghadiyaram, D., DeHaan, K., Metze, F.,
Torres, J., Giro-i Nieto, X.: How2sign: a large-scale multimodal dataset for con-
tinuous american sign language. In: Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition. pp. 2735‚Äì2744 (2021)

7. Falcon, W.F.: Pytorchlightning/pytorch-lightning: The lightweight pytorch wrap-
per for high-performance ai research. scale your models, not the boilerplate.,
https://github.com/PyTorchLightning/pytorch-lightning

8. Gao, Z., Feng, A., Song, X., Wu, X.: Target-dependent sentiment classification

with bert. Ieee Access 7, 154290‚Äì154299 (2019)

9. Gou, J., Yu, B., Maybank, S.J., Tao, D.: Knowledge distillation: A survey. Inter-

national Journal of Computer Vision 129(6), 1789‚Äì1819 (2021)

16

Hwang et al.

10. Gu, J., Bradbury, J., Xiong, C., Li, V.O., Socher, R.: Non-autoregressive Neural

Machine Translation. arXiv preprint arXiv:1711.02281 (2017)

11. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: Gans trained
by a two time-scale update rule converge to a local nash equilibrium. Advances in
neural information processing systems 30 (2017)

12. Hinton, G., Vinyals, O., Dean, J., et al.: Distilling the knowledge in a neural net-

work. arXiv preprint arXiv:1503.02531 2(7) (2015)

13. Huang, W., Pan, W., Zhao, Z., Tian, Q.: Towards fast and high-quality sign lan-
guage production. In: Proceedings of the 29th ACM International Conference on
Multimedia. pp. 3172‚Äì3181 (2021)

14. Hwang, E.J., Kim, J.H., Park, J.C.: Non-Autoregressive Sign Language Production
with Gaussian Space. In: Proceedings of the British Machine Vision Conference
(BMVC) (2021)

15. Jiang, T., Camgoz, N.C., Bowden, R.: Skeletor: Skeletal transformers for robust
body-pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. pp. 3394‚Äì3402 (2021)

16. Kacorri, H., Huenerfauth, M.: Selecting Exemplar Recordings of American Sign
Language Non-manual Expressions for Animation Synthesis based on Manual Sign
Timing. In: Proceedings of the 7th Workshop on Speech and Language Processing
for Assistive Technologies, INTERSPEECH (2016)

17. Kingma, D.P., Welling, M.: Auto-encoding Variational Bayes. arXiv preprint

arXiv:1312.6114 (2013)

18. Li, B., Tian, J., Zhang, Z., Feng, H., Li, X.: Multitask Non-Autoregressive Model
for Human Motion Prediction. IEEE Transactions on Image Processing 30, 2562‚Äì
2574 (2021). https://doi.org/10.1109/TIP.2020.3038362

19. Li, C., Zhang, Z., Lee, W.S., Lee, G.H.: Convolutional Sequence to Sequence Model
for Human Dynamics. In: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition. pp. 5226‚Äì5234 (2018)

20. Lu, P., Huenerfauth, M.: Data-driven Synthesis of Spatially Inflected Verbs for
American Sign Language Animation. ACM Transactions on Accessible Computing
(TACCESS) 4(1), 1‚Äì29 (2011)

21. McDonald, J., Wolfe, R., Schnepp, J., Hochgesang, J., Jamrozik, D.G., Stumbo,
M., Berke, L., Bialek, M., Thomas, F.: An Automated Technique for Real-time
Production of Lifelike Animations of American Sign Language. Universal Access
in the Information Society 15(4), 551‚Äì566 (2016)

22. Miyazaki, T., Morita, Y., Sano, M.: Machine translation from spoken language to
sign language using pre-trained language model as encoder. In: Proceedings of the
LREC2020 9th Workshop on the Representation and Processing of Sign Languages:
Sign Language Resources in the Service of the Language Community, Technological
Challenges and Application Perspectives. pp. 139‚Äì144 (2020)

23. Raj Khan, H., Gupta, D., Ekbal, A.: Towards developing a multilingual and
code-mixed visual question answering system by knowledge distillation. In: Find-
ings of the Association for Computational Linguistics: EMNLP 2021. pp. 1753‚Äì
1767. Association for Computational Linguistics, Punta Cana, Dominican Re-
public (Nov 2021). https://doi.org/10.18653/v1/2021.findings-emnlp.151, https:
//aclanthology.org/2021.findings-emnlp.151

24. Sandler, W., Lillo-Martin, D.: Sign language and linguistic universals. Cambridge

University Press (2006)

25. Saunders, B., Camg√∂z, N.C., Bowden, R.: Progressive Transformers for End-to-
End Sign Language Production. In: Proceedings of the European Conference on
Computer Vision (ECCV) (2020)

Abbreviated paper title

17

26. Saunders, B., Camg√∂z, N.C., Bowden, R.: Continuous 3D Multi-Channel Sign Lan-
guage Production via Progressive Transformers and Mixture Density Networks.
International Journal of Computer Vision pp. 1‚Äì23 (2021)

27. Saunders, B., Camgoz, N.C., Bowden, R.: Mixed signals: Sign language production
via a mixture of motion primitives. In: Proceedings of the IEEE/CVF International
Conference on Computer Vision. pp. 1919‚Äì1929 (2021)

28. Stoll, S., Camg√∂z, N.C., Hadfield, S., Bowden, R.: Sign Language Production using
Neural Machine Translation and Generative Adversarial Networks. In: Proceedings
of the 29th British Machine Vision Conference (BMVC) (2018)

29. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,
u., Polosukhin, I.: Attention is All You Need. In: Proceedings of the 31st Interna-
tional Conference on Neural Information Processing Systems. p. 6000‚Äì6010 (2017)
30. Wang, J., Jin, S., Liu, W., Liu, W., Qian, C., Luo, P.: When human pose estimation
meets robustness: Adversarial algorithms and benchmarks. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).
pp. 11855‚Äì11864 (June 2021)

31. Wang, Z., Bovik, A.C.: Mean squared error: Love it or leave it? a new look at signal

fidelity measures. IEEE signal processing magazine 26(1), 98‚Äì117 (2009)

32. Yan, S., Xiong, Y., Lin, D.: Spatial temporal graph convolutional networks for
skeleton-based action recognition. In: Thirty-second AAAI conference on artificial
intelligence (2018)

33. Yin, K., Read, J.: Better sign language translation with stmc-transformer. In:
Proceedings of the 28th International Conference on Computational Linguistics.
pp. 5975‚Äì5989 (2020)

34. Yoon, Y., Cha, B., Lee, J.H., Jang, M., Lee, J., Kim, J., Lee, G.: Speech Gesture
Generation from the Trimodal Context of Text, Audio, and Speaker Identity. ACM
Transactions on Graphics (TOG) 39(6), 1‚Äì16 (2020)

35. Yu, S., Su, J., Luo, D.: Improving bert-based text classification with auxiliary

sentence and domain knowledge. IEEE Access 7, 176600‚Äì176612 (2019)

36. Zelinka, J., Kanis, J.: Neural Sign Language Synthesis: Words are our Glosses. In:
Proceedings of the IEEE/CVF Winter Conference on Applications of Computer
Vision. pp. 3395‚Äì3403 (2020)

