2
2
0
2

y
a
M
9
2

]

G
L
.
s
c
[

1
v
3
8
6
4
1
.
5
0
2
2
:
v
i
X
r
a

The impact of memory on learning sequence-to-sequence tasks

Alireza Seif∗1, Sarah A.M. Loos2, Gennaro Tucci3, Édgar Roldán2 and Sebastian Goldt†4

1Pritzker School of Molecular Engineering, University of Chicago, USA
2ICTP – The Abdus Salam International Centre for Theoretical Physics, Trieste, Italy
3Max Planck Institute for Dynamics and Self-Organization, Göttingen, Germany
4International School of Advanced Studies (SISSA), Trieste, Italy

Abstract

The recent success of neural networks in machine translation and other fields has drawn renewed
attention to learning sequence-to-sequence (seq2seq) tasks. While there exists a rich literature that
studies classification and regression using solvable models of neural networks, learning seq2seq
tasks is significantly less studied from this perspective. Here, we propose a simple model for a
seq2seq task that gives us explicit control over the degree of memory, or non-Markovianity, in the
sequences – the stochastic switching-Ornstein-Uhlenbeck (SSOU) model. We introduce a measure
of non-Markovianity to quantify the amount of memory in the sequences. For a minimal auto-
regressive (AR) learning model trained on this task, we identify two learning regimes corresponding
to distinct phases in the stationary state of the SSOU process. These phases emerge from the interplay
between two different time scales that govern the sequence statistics. Moreover, we observe that
while increasing the memory of the AR model always improves performance, increasing the non-
Markovianity of the input sequences can improve or degrade performance. Finally, our experiments
with recurrent and convolutional neural networks show that our observations carry over to more
complicated neural network architectures.

1 Introduction

The recent success of neural networks on problems like machine translation and text summarisation
has rekindled interest in tasks that require transforming a sequence of tokens into another sequence.
Transformers [1] and other models for sequence-to-sequence (seq2seq) learning now also provide
promising results in areas beyond natural language processing, such as symbolic regression [2–4], which
were previously in the realm of genetic algorithms, and image captioning [5–7].

At the same time, these practical successes highlight the relative lack of theoretical understanding
of learning seq2seq tasks with neural networks (NN), especially compared to the recent progress that
was made on standard regression or classification tasks. There, a fruitful line of research focused on the
importance of data structure for the success of neural networks. The idea there is to develop synthetic
models of data and to analyse the dynamics and the performance of neural networks trained on that
data. This approach was initiated in the early days of neural network theory, where data was modelled
as random functions of Gaussian i.i.d. inputs, and the performance of a simple neural network, called
the student, was studied [8–11]. The successes of neural networks in image classification [6, 12–14]
then inspired a new generation of data models that take the effective low-dimensionality of images [15]

∗seif@uchicago.edu
†sgoldt@sissa.it

1

 
 
 
 
 
 
into account, such as object manifolds [16], the hidden manifold [17, 18], spiked covariates [19, 20],
or low-dimensional mixture models embedded in high dimensions [21–23]. Meanwhile, other works
focused on the impact of translation invariance [24–26]. By deriving learning curves for neural networks
on these and other data models [27–30], these works clarified the role of data structure for the success
of neural networks, for example by showing how the performance gap between neural networks and
random features [31] is exacerbated by data with low intrinsic dimension [19, 21, 22, 32].

Since all these works consider classification or regression, the aforementioned recent successes of
neural networks on seq2seq tasks make it an important challenge to extend this approach to this domain,
too. What are key properties of sequences, akin to the low intrinsic dimension of images, that need to
be modelled? How do these properties interact with different neural network architectures?

t=0

t=0

Here, we make a first step in this direction by proposing to use a minimal, solvable latent variable
model for time-series data that we call the stochastic switching Ornstein-Uhlenbeck process (SSOU). The
input data consists of a sequence x = (xt)T
, which depends on a latent, unobserved stochastic process
. The learning task is to reconstruct the unobserved sequence c from the input sequence x,
c = (ct)T
cf. fig. 1. The key data property we aim to describe and control is the memory within the input sequence
x. In the simplest scenario, the sequence xt is memoryless: given the value of the present token xt, the
value of the next token xt+1 is statistically dependent solely on xt but not on previous tokens xt(cid:48)<t.
Such a sequence can be described by a Markov process. By tuning the dynamics of the latent process ct,
we can control the memory of the sequence, i.e. we can increase the statistical dependence of future
tokens on their full history xt(cid:48)<t (we will introduce a precise, quantitative measure for the memory
below). Adding memory thus makes the process non-Markovian and allows us to model richer statistical
dependencies between tokens. At the same time, the presence of memory in a processes makes the
mathematical analysis generally harder. Here, we use techniques introduced by Tucci et. al. [33] to show
that statistical properties of the model such as its correlation functions, can be computed exactly even
in the non-Markovian case and give important insights into the performance of various students trained
on the reconstruction task.

We will contrast the memory of the sequence with the memory of various neural networks that
we train on this task. We begin with a simple auto-regressive model, whose basic structure – one
layer of weights followed by a non-linear activation function – makes it the seq2seq analogue of the
famous perceptron model that has been the object of a large literature in the theory of neural networks
focused on classification [8–11]. We also train simple convolutional (CNN) and (gated) recurrent neural
networks (RNN) on the SSOU task. The memory of these students ranges from finite integration window
in auto-regressive models and the kernel size of a CNN to the – in principle – unbounded memory of
(gated) recurrent neural networks with internal hidden states [34].

The goal of our study is to understand the interplay between the memory of the data and the memory
of the student when learning the seq2seq task x → c. Our main contributions can be summarised as
follows:

1. We introduce the stochastic switching Ornstein-Uhlenbeck process as a latent variable model for

seq2seq tasks (section 2)

2. We describe how to tune the memory of the input and target sequences, and introduce a measure

of non-Markovianity that quantifies this memory (section 4.1)

3. We identify two regimes in the performance of an auto-regressive model. These regimes corres-
pond to distinct phases in the stationary state of the SSOU and they emerge from the interplay of
two different time scales that govern the sequence statistics (section 4.2)

4. We show that the task difficulty is non-monotonous in the sequence memory for all three models

2

Figure 1: A flexible, minimal model for sequence-to-sequence learning tasks with varying de-
grees of memory. Left: The motion of a Brownian particle (black filled circle) in a switching parabolic
potential “trap” yields dynamics as given by the SSOU (stochastic switching Ornstein Uhlenbeck) model
given by eq. (1). Example trajectories for sequences of the particle’s position Xt (black line) and the
trap center Ct (blue line) as shown as a function of time t. The blue dashed line is set at the trap
centers C0 = ±1. Middle: We train three types of models to reconstruct the trap positions Ct from
the particle trajectories Xt: auto-regressive models (AR) as well as convolutional and recurrent neural
networks (CNN and RNN, resp.) Right: Sample reconstruction of the particle trap positions ˆyt (red
dashed line) for one example input sequence (Xt, black solid line) compared with the actual hidden trap
position (Ct, blue solid line). The sequence is a zoomed-in view of the sample sequence in the left panel.
Parameters: κ = 10, k = 1, D = 0.5, simulation time step ∆t = 0.02, AR model window size W = 2.
For this example, we used 5000 training samples, evaluated them over 5000 test samples, a mini-batch
size 32 and 5 epochs.

(auto-regressive, convolutional and recurrent): compared to the memoryless case, the task is
harder with weak memory, but easier with strong memory. We explain the mechanism behind
this effect in terms of the sequence statistics (section 4.3)

5. We finally find that increasing the memory of auto-regressive models and CNN improves their
performance, while increasing the dimension of the memory of a gated RNN achieves only minimal
improvements (section 4.3)

Reproducibility We provide code to sample from the SSOU model and reproduce our experiments at
https://github.com/alirezaseif/nonmarkovian_learning.

2 A model for sequence-to-sequence tasks

We first describe a latent variable model for seq2seq tasks, which we call the Stochastic Switching-
Ornstein-Uhlenbeck (SSOU) model. This model was introduced recently in biophysics [33] to describe
the spontaneous oscillations of the tip of hair-cell bundles in the ear of the bullfrog. We consider
observable sequences X = (Xt) which are described by a one-dimensional stochastic process whose
dynamics is driven by an autonomous latent stochastic process C = (Ct) and a Gaussian white noise
that is independent to Ct. Here and in the following we index sequences by a time variable t ≥ 0, and
use bold letters such as X to denote sequences, or trajectories. A sequence of length T can be sampled
from the stochastic differential equation

Xt+1 − Xt = −κ(Xt − Ct)dt +

√

2DdBt,

(1)

with t ∈ [0, T − 1]. Here, dBt is the increment of the Wiener process in [t, t + dt], with (cid:104)dBt(cid:105) = 0 and
(cid:104)(dBt)(dBt(cid:48))(cid:105) = δ(t − t(cid:48))dt. The angled brackets (cid:104)·(cid:105) indicate an average over the noise process. We

3

Network architectureSample sequencesReconstructed sequences2100123-1-210-1051015ARCNNRNNdenote the parameters D > 0 and κ > 0 as diffusion coefficient and trap stiffness for reasons described
below.

A well-known application of eq. (1) in statistical physics is to describe the trajectories of a small
particle (e.g. a colloid) in an aqueous solution undergoing Brownian motion [35] in a parabolic potential
with time-dependent center Ct. Such physical model is often realized with microscopic particles
immersed in water and trapped with optical tweezers [36–38]. When κ = 0, the particle is driven only
by the noise term dWt and performs a one-dimensional free diffusion along the real line with diffusion
coefficient D. By choosing κ positive, the particle experiences a restoring force −κ(Xt − Ct) towards
the instantaneous center of the potential. Such force F (Xt, Ct) = − ∂X V (X, C)|X=Xt, C=Ct
tends to
confine the particle to the vicinity of the point Ct, and is therefore often called a particle trap, which is
modelled by a harmonic potential centred on Ct

V (Xt, Ct) =

κ
2

(Xt − Ct)2.

(2)

This motivates the name “stiffness” for κ; the higher κ, the stronger the restoring force that confines the
particle to the origin of the trap Ct.

The dynamics of Xt given by eq. (1) could be in principle driven by any stochastic process Ct. Here
we focus on a minimal model where Ct = {−1, 1} is a dichotomous noise that changes sign after random
waiting times that are drawn from a prescribed distribution. In particular, we will use a one-parameter
family of gamma distributions (defined in eq. (6) below), which allows us to quantitatively control the
degree of memory in the sequence of tokens C in a simple manner. From a physical point of view, the
dynamics of Xt consists in the alternate relaxation towards the two minima of the potential between
consecutive switches, cf. fig. 1.

In what follows, we employ this setup as a seq2seq learning task where we aim to reconstruct the
hidden sequence of trap positions C given a sequence of particle positions X, see fig. 1 for an illustration.

The latent dynamics: switching the trap positions The key idea in our model is to let the location
of the potential Ct alternate in a stochastic manner between the two positions C0 = {−1, 1}. The
waiting time τ spend in each of these two positions is drawn from the waiting-time distribution ψk(τ ).
For a generic choice of ψk(τ ), the process Ct – and hence Xt – is non-Markovian and has a memory;
we will discuss this in detail in section 4.1.

3 Methods

Auto-regressive model We first consider the arguably simplest machine-learning model that can be
trained on sequential data, an auto-regressive model of order W , AR(W ), see fig. 1. The output ˆy = (ˆyt)
of the model for the trap position given the sequence x is given by

ˆyt = σ

(cid:32) W
(cid:88)

τ =1

(cid:33)

wτ xt−τ +1 + b

(3)

where σ(x) = 1/(1 + e−x) is the sigmoidal activation function, and the weights wτ and the bias b are
trained. We will also refer to the window size W ≥ 1 as the number of “memory units”, as it governs
the number of tokens accessible to the model. We do not pad the input sequence, so the output sequence
is shorter than the input sequence by W − 1 steps.

4

Convolutional two-layer network The auto-regressive model can also be thought of as a single
layer 1D convolutional neural network [39, 40] with a single filter and a kernel of size W with sigmoid
activation. We also consider the natural extension of this model, CNN(W ), a 1D convolutional neural
network with two layers (see fig. 1). The first layer has the same kernel size (W ), but contains f filters
with rectified linear activation [41]. In the second layer, we apply another convolution with a single
filter and a kernel size of 1 with sigmoid activation. In this way, we can compare a CNN and an AR
model with the same memory, i.e., same number of memory units (W ), and investigate the effect of
additional nonlinearities of the second layer of the CNN on the prediction accuracy.

Recurrent neural network We also apply a recurrent network to this task that takes xt as the input
at step t, followed by a layer of Gated Recurrent Units [34] with a d-dimensional hidden state, followed
by a fully connected layer with a single output and the sigmoid activation function (see fig. 1). We refer
to this family of models by GRU(d).

Generating the data set We train all the models on a training set with N sequences {x(n)}N
,
n=1
which can in principle be of different lengths. To obtain a sequence, we first generate a trajectory C for
the latent variable by choosing an initial condition at random from C0 ∈ {−1, 1} and then drawing a
sequence of waiting times from the distribution ψk(τ ). Using the sampled trap positions, C, we then
sample the trajectory X from eq. (1). Since in practice we do not have access to the full trajectory, as any
experiment has a finite sampling rate, we subsample the process Xt by taking every sth element of the
,
sequence. That is, for a given sequence {X (cid:48)
t=1
where xt = Xs×t and T = (cid:98)T (cid:48)/s(cid:99). The subsampled sequence x is then used as an input sequence.
We verified that the finite time step for the integration of the SDE and the subsampling preserve the
statistics of the continuum description that we use to derive our theoretical results, cf. appendix B.2.2.
For convenience, we also introduce y = (1 + c)/2 to shift the trap position to 0 or 1. We then use this
subsampled and shifted sequence of trap positions as the target.

of length T (cid:48), we construct a new sequence x = (xt)T

t}T (cid:48)

t(cid:48)=0

Training and evaluation We train the models by minimising the mean squared loss function using
Adam [42] with standard hyperparameters. For each sample sequence, the loss is defined as

(cid:96)(yt, ˆyt) =

1
T − τ0

T
(cid:88)

τ =τ0

(yτ − ˆyτ )2,

(4)

where the offset τ0 in the lower limit of the sum is necessary for the AR and CNN models since their
output has a different length than the input sequence. For those models we choose τ0 = W − 1.
For RNN models, however, we use τ0 = 1 as the input and out sequence lengths are the same. We
assess the performance of the trained models by first thresholding their outputs ˆyt at 0.5 to obtain the
sequence ˆct ∈ {±1}, since the location of the centre of the potential has two possible values ±1. We
then use the misclassification error (cid:15) defined as

(cid:15) =

1
NsamplesNτ

(cid:88)

δ(cτ (cid:54)= ˆcτ ),

samples,τ >τh

(5)

where Nτ is the length of the test sequence and δ(x) = 1 if the condition x is true and it is 0 otherwise, as
the figure of merit throughout this work. Unless otherwise specified, we used 50000 training samples with
mini-batch size 32 to train the models, and we evaluated them over 10000 test samples. To consistently
compare different models with different output lengths and to reduce the boundary effects we only
consider the predictions after an initial time offset τh.

5

Figure 2: Quantifying the memory of non-Markovian input sequences. Left: waiting-time dis-
tribution ψk(τ ) given by eq. (6) for three choices of k. Right: measure M (t) (see eq. (8)) associated with
the parameters in the left panel, which quantifies the memory of the past time t1 in the sequence. We
obtain a memoryless sequence Xt with M (t) = 0 by choosing k = 1 which creates an exponential
waiting-time distribution (6) (red curves). As the value of k increases, the memory becomes stronger
(blue and green curves). Parameters: D = 0.5, κ = 2, Ω1 = Ω2 = [0.5, 1.5], t3 − t2 = 0.5, by generating
N = 105 trajectories via the Euler–Maruyama method [43] with simulation time step ∆t = 5 × 10−3.

4 Results

4.1 The waiting time distribution of the trap controls the memory of the sequence

The key data property we would like to control is the memory of the sequence X. In our SSOU model,
the memory is controlled by tuning the memory in the latent sequence C, which in turn depends on the
distribution of the waiting time τ . To fix ideas, we choose gamma-distributed waiting times,

ψk(τ ) =

1
τ

(τ k)ke−kτ
Γ(k)

,

(6)

where we set k ≥ 1, while Γ(k) = (cid:82) ∞
0 dx xk−1e−x denotes the Gamma function. Note that for any
choice of k, the normalization condition is (cid:82) ∞
0 ψk(τ )dτ = 1 and the average waiting time between two
consecutive switches is (cid:104)τ (cid:105)k = (cid:82) ∞
0 τ ψk(τ ) dτ = 1. We can control the shape of the distribution by
changing the value of k: the variance of the waiting time for example is k-dependent, (cid:104)τ 2(cid:105)k −(cid:104)τ (cid:105)2
k = 1/k
(cf. fig. 2). Furthermore, k also controls the “degree of non-Markovianity” of the latent sequence C as
follows.

For the choice k = 1, the waiting-time distribution is exponential, making the process Ct Markovian.
This result, together with the fact that eq. (1) is linear, implies that the observable process Xt is Markovian.
Thus the probability distribution for the tokens xt obeys the Markov identity [35]

p1|2(xt3|xt2; xt1) = p1|1(xt3|xt2),

(7)

where t1 < t2 < t3 represent different instants in time, and p1|m denotes the conditional probability
density (at one time instant) with m conditions (at m previous time instants). Equation (7) is the
mathematical expression of the intuitive argument that we gave earlier: in a Markovian process, the
future state xt3
at time t3 given the state x2 at an earlier time t2 < t3 is conditionally independent of
the previous tokens x1 for all times t1 < t2.

However, most of the sequences analysed in machine learning are non-Markovian: in written
language for example, the next word will not just depend on the previous word, but instead on a

6

01234τ0.00.51.01.5k()k=15k=5k=105101520t2−t11.00.50.00.51.0M(t1)k=15k=5k=1large number of preceding words. We can generate non-Markovian sequences by choosing k > 1. To
systematically investigate the impact of memory on the learning task, it is crucial to quantify the degree
of non-Markovianity of the sequence for a given value of k beyond the binary distinction between
Markovian and non-Markovian. Yet defining a practical measure that quantifies conclusively the degree
of non-Markovianity is a non-trivial task [44, 45] and subject of ongoing research mainly done in the
field of open quantum systems [46–50].

Here, we introduce a simple quantitative measure of the degree of non-Markovianity of the input

sequence motivated directly by the Markov property (7),

M (t1) ≡

(cid:104)xt3| xt2 ∈ Ω2, xt1 ∈ Ω1(cid:105)
(cid:104)xt3| xt2 ∈ Ω2(cid:105)

(cid:82)

− 1 =

X xt3 p1|2(xt3| xt2 ∈ Ω2, xt1 ∈ Ω1)dxt3
X xt3 p1|1(xt3| xt2 ∈ Ω2)dxt3

(cid:82)

− 1.

(8)

Equation (8) involves crucially conditional expectations1, where the expectation of the future system
state (at time t3) is conditioned on the present state (at time t2) or on the present and past state (at
times t2 and t1). We have further introduced the notion of state space regions Ω1 ⊆ X and Ω2 ⊆ X
which are subsets of the entire state space X = R that is accessible by the process X. From eq. (7) it
follows that for any Markovian process the measure M (t1) (8) vanishes at all times, whereas M (t1) (cid:54)= 0
reveals the presence of non-Markovianity in the form of memory of the past time t1. In a stationary
process, M generally depends on the two time differences t3 − t2 and t2 − t1, and on the choices of Ω1
and Ω2. Here, we fix t2,3, Ω1,2, and vary t1. To spot the non-Markovianity, t3 − t2 should be comparable
to the other time scales of the process. The choice of Ω1 and Ω2 is in principle arbitrary, but, for practical
purposes, they should correspond to regions in the state space that are frequently visited.

We plot M (t1) in fig. 2 for three different values of k obtained from numerical simulations of the
SSOU. As expected, for a Markovian switching process with k = 1, M (t1) vanishes at all times t1, while
for k > 1, M (t1) displays non-zero values. The non-Markovianity measure M (t1) defined in eq. (8)
generally captures different facets of non-Markovianity. On the one hand, the decay with t1 measures
how far the memory reaches into the past. On the other hand, the magnitude of M (t1) tells us how much
the predictability of the future given the present state profits from additionally knowing a past state.
For the SSOU model, we observe in fig. 2 that increasing k increases both the decay time of M (t1) and
its amplitude, showing that that the parameter k controls conclusively the degree of non-Markovianity.
We further note that M (t1) displays oscillations for k > 1, reflecting the oscillatory behaviour of C
and X, and that M (t1) always decays to zero for sufficiently large values of t2 − t1, indicating the finite
persistence time of the memory in the system. In conclusion of this analysis, we can in the following
simply use k as control parameter of the degree of non-Markovianity of X.

4.2 The performance of auto-regressive models

The interplay between two time scales determines prediction accuracy To gain some intuition,
we first consider the simplest possible students, namely auto-regressive models AR(2) (3) with window
size W = 2. The student predicts the value of ct only using information about the present and the
previous tokens xt and xt−1, giving it access to the current particle position and allowing it in principle
also to estimate the velocity of the particle. We show the performance of this model obtained numerically
in fig. 3. The accuracy of AR(2) varies significantly from less than 5% to essentially random guessing

1We denote (cid:104)X|Y ∈ Ω(cid:105) the conditional expectation of the random variable X given that the random variable Y satisfies a
certain criterion, symbolyzed here as belonging to the set Ω. Note that in general X and Y are statistically dependent, and
that the conditioning may be done with respect to more than one random variables satisfying prescribed criteria.

7

Figure 3: Performance of auto-regressive AR(2) models for Markovian and non-Markovian
sequences of the SSOU model across its “learnability phase diagram”. (Left and middle) Recon-
struction accuracy of the AR(2) model (in %) for different values of the diffusive time scale tdiﬀ = 1/2D
and the relaxation time tκ = 1/κ, see eq. (9). We show these “phase diagrams” for Markovian sequences
(k = 1) and for non-Markovian sequences (k = 5). In these diagrams, the two axes represent ratios of
time scales, tdiﬀ /tκ (vertical axis) vs tκ/(cid:104)τ (cid:105)k = 1/κ (horizontal axis), since (cid:104)τ (cid:105)k = 1 for all k. (Right)
Scatter plot of the error of several AR(2) models trained on Markovian sequences versus the Sarle
coefficient (11), a measure of the bimodality of the distribution p(xt). Parameters: total simulation time
for each parameter value τ = 30, simulation time step ∆t=0.01, AR(W ) window size W = 2, κ varying
from 0.1 to 0.6, τh = 2, remaining training parameters as in fig. 1.

at 50% error as we vary the two time scales that influence the sequence statistics,

tκ = 1/κ

and

tdiﬀ = C2

0 /2D = 1/2D

(9)

the relaxation time and the diffusive time scale. The relaxation time tκ determines how quickly the
average position of the particle is restored to the centre of the trap when the the average waiting
time (cid:104)τ (cid:105)k = 1. A faster relaxation time means that the particle responds more quickly to a change
in ct, making reconstruction easier. The diffusive time scale tdiﬀ sets the typical time that the system
elapses to randomly diffuse a distance ∼ |C0|. The larger this time scale, the slower the particle moves
randomly, as opposed to movement that follows the particle trap, hence improving the accuracy.

A “phase diagram” for learnability For memoryless sequences (k = 1), we can explain this per-
formance more quantitatively by solving for the data distribution p(xt). In particular, we observe that
reconstructing the trap position is simplified when the marginal distribution of the particle distribu-
tion p(xt) is bimodal, i.e. when it presents two well defined peaks around the trap centres ±C0 (see fig. 3
top left). On the contrary, when the distribution p(xt) is unimodal (corresponding to the case of fast
relaxation times tκ) the learning performance worsens as the data is not sufficiently informative about
the latent state C. Interestingly, we observe that along the “phase boundary” between unimodal and
bimodal the error is roughly homogeneous and approximately equal to 25%. The presence of two
“phases” in terms of the bimodality/unimodality of the distribution p(xt) is rationalised using recent
analytical results [33] for the loci of the phase boundary in the the case k = 1, which is given by

tdiﬀ
tκ

=

(tκ + 1/2) 1F1 (1/2, tκ + 1/2, −tdiﬀ /tκ)
1F1 (3/2, tκ + 3/2, −tdiﬀ /tκ)

.

(10)

This analytical result is obtained by finding the parameter values at which the derivative of p(xt) at xt = 0
changes sign, which corresponds to a transition between unimodal and bimodal (see appendix A.1 for

8

0.51.01.51/01234567/2DAR(2), Markovian (k=1)xtp(xt)Bimodalxtp(xt)Unimodal0.51.01.51/01234567/2DAR(2), non-Markovian (k=5)0.40.60.8Sa1020304050†(%)AR(2), Markovian (k=1)1020304050Figure 4: The trade-off between student memory and sequence memory in recurrent neural
(Left) Analytical predictions for the covariance of the
networks and convolutional networks.
particle and trap position (12) in the non-Markovian case as a function of k. As we increase the non-
Markovianity, correlations between xt and ct decay, complicating the reconstruction task. (Right)
Prediction error (cid:15) for students with different architectures: auto-regressive (AR), convolutional (CNN)
and gated recurrent neural network (GRU). The memory units here correspond to the size of the kernel
(W ) in the AR and the first layer of CNN models, and the number of units (d) in the GRU models. The
CNN models here all have f = 10 filters in their first layer. Parameters: Similar to fig. 3 with κ = 2.

additional details). The line separating the two phases is drawn in black in the density plot of fig. 3. If
we add memory to the sequence by choosing k > 1, an analytical solution for the phase boundary is
challenging, yet we can explore the performance of the model with numerical simulations. Interestingly,
when increasing the degree of memory to k = 5 (see fig. 3 top right for k = 5), we find that the AR(2)
model yields a larger error than for k = 1 Markovian sequences for all the parameter values explored
in the learnability phase diagram. This means that not only that AR(2) is not able to extract all the
available information from the sequence, but that the task has become harder by increasing k. We will
explore further how the degree of non-Markovianity makes the task harder in the next section.

Further statistical characterisation of the accuracy We can further quantify the shape of the
distribution by measuring the degree of bimodality using the Sarle coefficient [51],

Sa = (σ2 + 1)/K

(11)

with σ and K the skewness and kurtosis of xt, respectively. The Sarle coefficient takes values from 0 to 1
and is equal to 5/9 for the uniform distribution. Higher values may indicate bi-modality. We can see a
clear correlation between the Sarle coefficient and the accuracy of the AR(2) model from the scatter plot
on the right of fig. 3. Indeed, there appears to exist an upper bound on the accuracy in terms for the
Sarle coefficient.

4.3 The interplay between sequence memory and model memory

The statistical properties of the input sequence x with memory We saw in fig. 3 that increasing
the sequence memory by increasing k makes the problem harder: the accuracy of the same student de-
creased. We can understand the root of this difficulty by studying the variance of the distribution p(xt, ct).
Using the tools introduced by Tucci et. al. [33], we can calculate these correlations analytically for any

9

100101102k0.30.40.50.60.7CovarianceCov(xt,ct)Var(xt)100101102k101520253035†(%)ModelARCNNGRUMemory Units124816integer k. As we describe in more detail in appendix A.2, we find that

Var (xt) = (cid:104)x2

t (cid:105) =

D
κ

C2

0 + κ

k−1
(cid:88)

n=0

Qn

k qn − κ
(k qn)2 − κ2 ,

Cov (xt, ct) = (cid:104)xtct(cid:105) = 1 −

2
κ

(1 + κ/k)k − 1
(1 + κ/k)k + 1

,

(12a)

(12b)

where Qn ≡ −4(1 − qn)/(k qn)2, and qn ≡ 1 − eπ(1+2n)/k with n = 0, . . . , k − 1; we recall that (cid:104)xt(cid:105) =
(cid:104)ct(cid:105) = 0. We plot both correlation functions as a function of k on the right of fig. 4. First, we note
that the variance of the particle distribution decreases with k (red line). In other words, as we increase
the memory of the sequence, the particle spends on average more time around the origin, making
reconstruction harder. This is also born out by a decrease in the correlation between xt and ct (blue line).
This loss in correlations is closely related to the error of the simplest reconstruction algorithm, where
we estimate the particle positions c by thresholding the particle position, ct = sign(xt). As shown
by a light blue line in the right panel of fig. 4, the error of this parameter-free, memoryless algorithm
increases monotonically with k.

Increasing memory in autoregressive models and convolutional neural networks The exist-
ence of memory in the sequences suggests the use of students with larger memory to exploit these
additional statistical dependencies. We first studied the performance of AR(W ) models with varying W .
As shown in fig. 4, we observed that for a fixed k increasing W generally helps with reducing the error (cid:15)
(solid lines), so models with larger memory can take advantage of the additional temporal correlations in
the sequence and make better predictions in the large k limit. For the convolutional networks described
in section 3 with f = 10 filters (dashed lines), we observe that the additional filters and non-linearities
in the CNN do not help with the predictions. We conclude that it is truly the memory that makes
a difference on this data set and that even a simple AR model can completely take advantage of the
information in a given time window.

Recurrent neural networks We numerically studied the performance of a gated recurrent unit
GRU(d) [34] with hidden state size d, which can be thought of as a slightly more streamlined variant
of the classic long short-term memory [52]. Although such an RNN reads in the input sequence one
token at a time, it can build up long-time memory by continuously updating its internal state h ∈ Rd,
cf. fig. 1. We show the test performance of GRU models with various internal state sizes d in fig. 4.
The GRU display a good performance: as soon as the GRU(d) has a hidden state with a hidden state of
dimension d = 2, it captures all the memory of the sequence at all k, achieving the limiting performance
of AR(W ) models with the largest window size. Indeed, the error curves for GRUs with d ≥ 2 all
coincide. This observation, together with the limited performance gain from increasing W in the AR(W )
and CNN(W ) models suggest that the error is approaching the Bayes limit. Note that the rate of this
convergence depends on k, since the student memory required to achieve the optimal performance
depends on the degree of non-Markovianity of the sequence (see Fig. 4).

The interplay between sequence memory and model memory Finally, we found that for all
three architectures – AR, CNN and GRU – there is a peak in the reconstruction error, usually around
k ≈ 5. This peak can be understood by noting that the tokens xt get more concentrated around the
origin as k is increased, as we have shown in eq. (12). Therefore, xt is less correlated with ct in the highly
non-Markovian limit. However, as the degree of non-Markovianity is increased through increasing k,
the history of the tokens position xt can help with predicting the trap’s position ct more accurately.

10

The trade-off between these two phenomena is evident in the non-monotonous dependency of the
error as a function of k for models with larger memory in fig. 4. For small k, the correlation between
different times in the sequence is not strong enough to compensate the loss of information about the
trap’s position in the signal, but for larger k the trap’s position is more regular and the history of xt can
be used to infer the ct more efficiently.

5 Concluding perspectives

We have introduced the stochastically switching Ornstein-Uhlenbeck process (SSOU) to model seq2seq
machine learning tasks. This model gives us precise control over the memory of the generated sequences,
which we used to study the interaction between this memory and various machine learning models
trained on this data, ranging from simple auto-regressive models to convolutional and recurrent neural
networks. We found that the accuracy of these models is governed by the interaction of the different
time scales of the data, and we discovered an intriguing interplay between the memory of the students
and the sequence which leads to non-monotonic error curves.

An important limitation of the SSOU model considered here is that it only allows one to study
correlations that decay exponentially fast. As a next step it would be interesting to explicitly consider the
impact of long-range memory in the data sequence. This could be implemented in the SSOU model e.g.
by choosing a long-ranged waiting time distribution, such as a power law. Another limitation concerns
the linearity of the model analysed here. We speculate that a nonlinear model could provide insights
into the different ways AR and CNN utilise the data. In our model, one could implement a tunable
nonlinearity by adding a non-quadratic term of increasing amplitude to the trap potentials, eq. (2).
Similarly, to enrich the statistical dependencies within the sequence, one could generate non-symmetric
sequences with respect to a x → −x sign inversion, by implementing two different waiting times. It
would also be intriguing to analyse the impact of sequence memory on the dynamics of simple RNN
models [53, 54], such as those with low-rank connectivity [55, 56].

Finally, it would be intriguing to apply some of the neural networks studied here to noisy non-
Markovian signals extracted from experiments in physical or biological systems [57–63]. Examples
include the recent application of the SSOU to infer the thermodynamics of spontaneous oscillations of
the hair-cell bundles in the ear of the bullfrog [33] or the characterisation and mitigation the effects of
non-Markovian noise [64, 65].

Acknowledgements

We thank Roman Belousov, Alessandro Ingrosso, Stéphane d’Ascoli, Andrea Gambassi, Florian Berger,
Gogui Alonso, AJ Hudspeth, Aljaz Godec and Jyrki Piilo for stimulating discussions. A.S. is supported
by a Chicago Prize Postdoctoral Fellowship in Theoretical Quantum Science.

11

References

1. Vaswani, A. et al. Attention is All you Need in Advances in Neural Information Processing Systems

(eds Guyon, I. et al.) 30 (2017).

2. Valipour, M., You, B., Panju, M. & Ghodsi, A. SymbolicGPT: A Generative Transformer Model for

Symbolic Regression. arXiv preprint arXiv:2106.14131 (2021).

3. Kamienny, P.-A., d’Ascoli, S., Lample, G. & Charton, F. End-to-end symbolic regression with

transformers. arXiv preprint arXiv:2204.10532 (2022).

4. Biggio, L., Bendinelli, T., Neitz, A., Lucchi, A. & Parascandolo, G. Neural Symbolic Regression that

Scales in International Conference on Machine Learning (2021), 936–945.

5. He, S. et al. Image captioning through image transformer in Proceedings of the Asian Conference on

Computer Vision (2020).

6. Dosovitskiy, A. et al. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale in

International Conference on Learning Representations (2021).

7. Touvron, H. et al. Training data-efficient image transformers & distillation through attention in

International Conference on Machine Learning (2021), 10347–10357.

8. Gardner, E. & Derrida, B. Three unfinished works on the optimal storage capacity of networks.

9.

Journal of Physics A: Mathematical and General 22, 1983 (1989).
Seung, H. S., Sompolinsky, H. & Tishby, N. Statistical mechanics of learning from examples. Physical
review A 45, 6056 (1992).

10. Engel, A. & Van den Broeck, C. Statistical mechanics of learning (Cambridge University Press, 2001).
11. Carleo, G. et al. Machine learning and the physical sciences. Reviews of Modern Physics 91, 045002

(2019).

12. Krizhevsky, A., Sutskever, I. & Hinton, G. Imagenet classification with deep convolutional neural

13.

networks in Advances in neural information processing systems (2012), 1097–1105.
Simonyan, K. & Zisserman, A. Very Deep Convolutional Networks for Large-Scale Image Recognition
in International Conference on Learning Representations (2015).

14. He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image recognition in Proceedings of the

IEEE conference on computer vision and pattern recognition (2016), 770–778.

15. Pope, P., Zhu, C., Abdelkader, A., Goldblum, M. & Goldstein, T. The Intrinsic Dimension of Images
and Its Impact on Learning in International Conference on Learning Representations (2021).
16. Chung, S., Lee, D. D. & Sompolinsky, H. Classification and Geometry of General Perceptual

Manifolds. Phys. Rev. X 8, 031003 (3 2018).

17. Goldt, S., Mézard, M., Krzakala, F. & Zdeborová, L. Modeling the influence of data structure on
learning in neural networks: The hidden manifold model. Phys. Rev. X 10, 041044 (2020).
18. Goldt, S. et al. The Gaussian equivalence of generative models for learning with shallow neural
networks in Proceedings of the 2nd Mathematical and Scientific Machine Learning Conference (eds
Bruna, J., Hesthaven, J. & Zdeborová, L.) 145 (PMLR, 2022), 426–471.

19. Ghorbani, B., Mei, S., Misiakiewicz, T. & Montanari, A. When do neural networks outperform kernel

methods? in Advances in Neural Information Processing Systems 33 (2020).

12

20. Richards, D., Mourtada, J. & Rosasco, L. Asymptotics of Ridge(less) Regression under General Source
Condition in Proceedings of The 24th International Conference on Artificial Intelligence and Statistics
(eds Banerjee, A. & Fukumizu, K.) 130 (PMLR, 2021), 3889–3897.

21. Chizat, L. & Bach, F. Implicit bias of gradient descent for wide two-layer neural networks trained with

the logistic loss in Conference on Learning Theory (2020), 1305–1338.

22. Refinetti, M., Goldt, S., Krzakala, F. & Zdeborová, L. Classifying high-dimensional Gaussian mixtures:
Where kernel methods fail and neural networks succeed in Proceedings of the 38th International
Conference on Machine Learning (eds Meila, M. & Zhang, T.) 139 (PMLR, 2021), 8936–8947.
23. Loureiro, B. et al. Learning Gaussian Mixtures with Generalized Linear Models: Precise Asymptotics

in High-dimensions. Advances in Neural Information Processing Systems 34 (2021).

25.

24. Harsh, M., Tubiana, J., Cocco, S. & Monasson, R. ‘Place-cell’emergence and learning of invariant data
with restricted Boltzmann machines: breaking and dynamical restoration of continuous symmetries
in the weight space. Journal of Physics A: Mathematical and Theoretical 53, 174002 (2020).
Favero, A., Cagnetta, F. & Wyart, M. Locality defeats the curse of dimensionality in convolutional
teacher-student scenarios. Advances in Neural Information Processing Systems 34 (2021).
Ingrosso, A. & Goldt, S. Data-driven emergence of convolutional structure in neural networks.
arXiv preprint arXiv:2202.00565 (2022).
Spigler, S., Geiger, M. & Wyart, M. Asymptotic learning curves of kernel methods: empirical data
versus teacher–student paradigm. Journal of Statistical Mechanics: Theory and Experiment 2020,
124001 (2020).

27.

26.

28.

d’Ascoli, S., Gabrié, M., Sagun, L. & Biroli, G. On the interplay between data structure and loss function
in classification problems in Advances in Neural Information Processing Systems (eds Ranzato, M.,
Beygelzimer, A., Dauphin, Y., Liang, P. & Vaughan, J. W.) 34 (Curran Associates, Inc., 2021), 8506–
8517.

29. Benna, M. K. & Fusi, S. Place cells may simply be memory cells: Memory compression leads
to spatial tuning and history dependence. Proceedings of the National Academy of Sciences 118,
e2018422118 (2021).

30. Gerace, F., Saglietti, L., Mannelli, S. S., Saxe, A. & Zdeborová, L. Probing transfer learning with a
model of synthetic correlated datasets. Machine Learning: Science and Technology 3, 015030 (2022).
31. Bach, F. Breaking the curse of dimensionality with convex neural networks. The Journal of Machine

Learning Research 18, 629–681 (2017).

32. Ghorbani, B., Mei, S., Misiakiewicz, T. & Montanari, A. Limitations of Lazy Training of Two-layers
Neural Network in Advances in Neural Information Processing Systems 32 (2019), 9111–9121.
33. Tucci, G. et al. Modelling Active Non-Markovian Oscillations. arXiv preprint arXiv:2201.12171

(2022).

34. Cho, K., van Merriënboer, B., Bahdanau, D. & Bengio, Y. On the Properties of Neural Machine
Translation: Encoder–Decoder Approaches in Proceedings of SSST-8, Eighth Workshop on Syntax,
Semantics and Structure in Statistical Translation (2014), 103–111.

35. Van Kampen, N. G. Stochastic processes in physics and chemistry (Elsevier, 1992).
36. Martinez, I. A. & Petrov, D. Force mapping of an optical trap using an acousto-optical deflector in a

time-sharing regime. Applied optics 51, 5522–5526 (2012).

13

37. Pietzonka, P., Ritort, F. & Seifert, U. Finite-time generalization of the thermodynamic uncertainty

relation. Physical Review E 96, 012101 (2017).

38. Martínez, I. A., Roldán, E., Parrondo, J. M. & Petrov, D. Effective heating to several thousand kelvins

of an optically trapped sphere in a liquid. Physical Review E 87, 032159 (2013).

39. LeCun, Y. et al. Backpropagation Applied to Handwritten Zip Code Recognition. Neural Computation

1, 541–551 (1989).

40. Goodfellow, I., Bengio, Y. & Courville, A. Deep learning (MIT press, 2016).
41.

Fukushima, K. Visual Feature Extraction by a Multilayered Network of Analog Threshold Elements.
IEEE Transactions on Systems Science and Cybernetics 5, 322–333 (1969).

42. Kingma, D. P. & Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980

(2014).

43. Kloeden, P. E. & Platen, E. in Numerical Solution of Stochastic Differential Equations 103–160

(Springer, 1992).

44. Lapolla, A. & Godec, A. Toolbox for quantifying memory in dynamics along reaction coordinates.

Physical Review Research 3, L022018 (2021).

45. Lapolla, A. & Godec, A. Manifestations of projection-induced memory: General theory and the

tilted single file. Frontiers in Physics 7, 182 (2019).

46. Laine, E.-M., Piilo, J. & Breuer, H.-P. Measure for the non-Markovianity of quantum processes.

Physical Review A 81, 062115 (2010).

47. Hall, M. J., Cresser, J. D., Li, L. & Andersson, E. Canonical form of master equations and character-

ization of non-Markovianity. Physical Review A 89, 042120 (2014).

48. Rivas, Á., Huelga, S. F. & Plenio, M. B. Entanglement and non-Markovianity of quantum evolutions.

Physical review letters 105, 050403 (2010).

49. Huang, Z., Guo, X.-K. et al. Quantifying non-Markovianity via conditional mutual information.

50.

Physical Review A 104, 032212 (2021).
Strasberg, P. & Esposito, M. Response functions as quantifiers of non-Markovianity. Physical review
letters 121, 040601 (2018).

51. Ellison, A. M. Effect of seed dimorphism on the density-dependent dynamics of experimental
populations of Atriplex triangularis (Chenopodiaceae). American Journal of Botany 74, 1280–1288
(1987).

52. Hochreiter, S. & Schmidhuber, J. Long short-term memory. Neural computation 9, 1735–1780 (1997).
Sompolinsky, H., Crisanti, A. & Sommers, H.-J. Chaos in random neural networks. Physical review
53.
letters 61, 259 (1988).
Sussillo, D. & Abbott, L. F. Generating coherent patterns of activity from chaotic neural networks.
Neuron 63, 544–557 (2009).

54.

55. Mastrogiuseppe, F. & Ostojic, S. Linking connectivity, dynamics, and computations in low-rank

56.

recurrent neural networks. Neuron 99, 609–623 (2018).
Schuessler, F., Mastrogiuseppe, F., Dubreuil, A., Ostojic, S. & Barak, O. The interplay between
randomness and structure during learning in RNNs. Advances in neural information processing
systems 33, 13352–13362 (2020).

57. Mindlin, G. B. Nonlinear dynamics in the study of birdsong. Chaos 27, 092101 (2017).

14

58. Vettoretti, G. & Peltier, W. R. Fast physics and slow physics in the nonlinear Dansgaard–Oeschger

relaxation oscillation. J. Clim. 31, 3423 (2018).

59. Cavallaro, M. & Harris, R. J. Effective bandwidth of non-Markovian packet traffic. Journal of

Statistical Mechanics: Theory and Experiment 2019, 083404 (2019).

60. Roldán, É., Barral, J., Martin, P., Parrondo, J. M. & Jülicher, F. Quantifying entropy production in
active fluctuations of the hair-cell bundle from time irreversibility and uncertainty relations. New
Journal of Physics 23, 083013 (2021).

61. Belousov, R., Berger, F. & Hudspeth, A. Volterra-series approach to stochastic nonlinear dynamics:
Linear response of the Van der Pol oscillator driven by white noise. Phys. Rev. E 102, 032209 (2020).
62. Brückner, D. B. et al. Stochastic nonlinear dynamics of confined cell migration in two-state systems.

63.

Nat. Phys. 15, 595 (2019).
Skinner, D. J. & Dunkel, J. Estimating Entropy Production from Waiting Time Distributions. Phys.
Rev. Lett. 127, 198101 (19 2021).

64. Mavadia, S., Frey, V., Sastrawan, J., Dona, S. & Biercuk, M. J. Prediction and real-time compensation

of qubit decoherence via machine learning. Nature communications 8, 1–6 (2017).

65. Majumder, S., Andreta de Castro, L. & Brown, K. R. Real-time calibration with spectator qubits. npj

Quantum Information 6, 1–9 (2020).

66. Glorot, X. & Bengio, Y. Understanding the difficulty of training deep feedforward neural networks in
Proceedings of the thirteenth international conference on artificial intelligence and statistics (2010),
249–256.

15

A Analytical details

A.1 Details on the calculation of the phase diagram

As anticipated in section 4.1, the process Ct becomes Markovian in the case of exponentially distributed
waiting time distribution; in our units this coincides with ψk=1(τ ) = e−τ . For this simple case, one can
characterise analytically the mono–bistable transition of the stationary density p(xt). This can be done
by looking at the behaviour of p(xt) at the origin: if xt = 0 is a point of maximum, p(xt) is unimodal,
bimodal otherwise. We report from [33] the explicit expression of p(xt), which reads

p(xt) =

1
√
π

Γ (cid:0)ζ + 1
2
Γ (cid:0)ζ − 1
2

(cid:1)
(cid:1)

(cid:90) +1

−1

dz

e−χ(xt−z)2
(cid:112)π/χ

(1 − z2)ζ−1,

(A.1)

where we have set C0 = 1, and we have defined the dimensionless parameters ζ = tκ/(cid:104)τ (cid:105)k = 1/κ
and χ = tdiﬀ /tκ = κ/(2D). One finds that xt = 0 is an extremum point for p(xt), coinciding with the
condition p(cid:48)(0) = 0. The nature of xt = 0 is understood by looking at the second derivative of p(xt),
that is

p(cid:48)(cid:48)(0) =

2
(cid:112)π/χ3

(cid:20)

χ

ζ + 1/2 1F1

(cid:18) 3
2

, ζ +

3
2

(cid:19)

, −χ

− 1F1

(cid:18) 1
2

, ζ +

1
2

(cid:19)(cid:21)

, −χ

,

(A.2)

where 1F1 denotes the confluent hypergeometric function. The mono-bimodal transition occurs upon
crossing the critical value χ∗ satisfying the condition p(cid:48)(cid:48)(0) = 0, or equivalently, eq. (10). For χ < χ∗,
the second derivative p(cid:48)(cid:48)(0) is negative and p(x) is unimodal, while it is bimodal otherwise.

A.2 Computation of the correlation functions in the non-Markovian case

In this section, we report the analytical expression for the (stationary) auto-correlation function CX (t)
of the process Xt in eq. (1), and its Fourier transform, i.e., the power spectral density (PSD) SX (ω). For
Gamma-distributed waiting-time ψk(τ ) as given in eq. (6), the PSD SX (ω) of the process Xt is given
by [33]

where SC(ω) denotes the PSD of the process Ct, whose explicit expression reads

SX (ω) =

2D + κ2SC(ω)
κ2 + ω2

,

SC(ω) =

4C2
0
ω2

R2(ω) − 1
R2(ω) + 1 + 2R(ω) cos φ(ω)

,

(A.3)

(A.4)

with R(ω) = (cid:2)1 + (ω/k)2(cid:3)k/2, and φ(ω) = k arctan(ω/k). According to the Wiener-Khinchin theorem
[35], the inverse Fourier transform of SC(ω) and SX (ω) coincides with the auto-correlation functions
CC(t) ≡ limτ →∞(cid:104)Ct+τ Cτ (cid:105) and CX (t) ≡ limτ →∞(cid:104)Xt+τ Xτ (cid:105). In the case of integer k, their expressions
can be calculated explicitly according to

CC(t) = C2
0

k−1
(cid:88)

n=0

Qn e−t k qn,

CX (t) =

D
κ

e−κ t + C2

0 κ

k−1
(cid:88)

n=0

Qn

k qn e−κ t − κ e−k qn t
(k qn)2 − κ2

,

(A.5)

(A.6)

where Qn ≡ −4(1 − qn)/(k qn)2, and qn ≡ 1 − eπ(1+2n)/k with n = 0, . . . , k − 1; in fig. 5, we compare
eq. (A.6) with the numerical estimate of CX for three different choices of k. Note that the value of the
sum (cid:80)k−1
. Moreover, one can deduce the

n=0 Qn = 1 implies the correct initial condition Cc(0) = C2
0

16

Figure 5: Autocorrelation function of the process Ct. We compare the analytical formula of CX (t)
in eq. (A.5) (solid line) with its numerical prediction (circles), calculated by generating N = 5 × 104
trajectories via Euler–Maruyama method with ∆t = 0.01, κ = 2, D = 0.5 and various values of k.

t (cid:105) of the process Xt from the initial value CX (0).
stationary expression of the stationary variance (cid:104)x2
We conclude this section by reporting the value of the covariance (cid:104)xtct(cid:105), which we derive here using
similar methods as those in ref. [33], and is given by

(cid:104)xtct(cid:105) = 1 −

2
κ

(1 + κ/k)k − 1
(1 + κ/k)k + 1

.

(A.7)

B Additional experimental results

We use a fixed number of 40 epochs and do not perform any hyperparameter tuning (with the exception
of the GRU(1) model as detailed in Appendix B) as the models are rather simple and varying these
parameters have negligible effects on the results.

B.1 Additional details on creating the figures

In this section, we provide additional details on the generation of some of the figures of the main text.

Figure 3 The parameter sweep in the top left and top right panels was done by inspecting the intervals
κ ∈ [2/3, 10] and D ∈ [1/21, 50] in a regular grid of size 20×20. We used a finite-sample estimate for
Sarle’s coefficient given by (σ2 + 1)/(3 + ωK), with σ the skewness and K the kurtosis of the sequence
X, and ω = (T − 1)2/(T − 2)(T − 3) and T = τ /∆t = 3 × 103.

B.2 Ensuring the consistency between continuous-time theory and sampled sequences

Here we discuss how we ensured that our theory that is based on the continuous-time description of
the stochastic processes Xt and Ct (cf. section 2) and the sequences we find into the machine learning
models are consistent. If the sequence is sampled with a very small time step, the student almost never
sees a jump of the trap, and effectively samples from an (equilibrium) distribution in one trap. On
the other hand, in the limit of very large time steps, the temporal correlations in the sequence is not
visible, e.g., recall that the non-Markovianity parameter eventually decays to zero for infinitely large
time differences. Thus, it is crucial to sample with a time step that is smaller but comparable to the

17

0246810t0.00.20.40.60.8CX(t)k=15k=5k=1characteristic time scales of the data sequence. In practice, one would thus need to estimate those time
scales in a preceding data analysis step, before feeding the data into the network. For this purpose, a
suitable method is to compute the autocorrelation function of the input sequence data which reveals the
time scales. For the SSOU model, we provide the analytical expression for the correlation function in
eq. (12). For the SSOU, the characteristic time scales are given by the oscillation period and the decay
time of the autocorrelation function, which are of order of magnitude 1 for our parameter choice (see
fig. 5 ).

B.2.1 Correlations function and integration time step

To generate trajectories with correct statistics we vary the time differences ∆t, which corresponds to
the discrete version of dt in numerically integrating eq. (1). We find that ∆t = 0.005 reproduces the
correct statistics in fig. 5.

B.2.2 The role of subsampling

As mentioned earlier the trajectories are first generated by numerically integrating eq. (1) by choosing a
sufficiently small dt such that the statistical properties of the trajectories match their theoretical values,
which we verified by checking the correlation functions estimated analytically match our theoretical
result (see fig. 5).

As we discuss in the main text, we subsample the full trajectories before training the machine learning
models. This subsampling also simplifies the optimization and accelerates the training. However, in
doing so, we need to make sure that the qualitative properties of the model remain unchanged. Therefore,
we compare AR models trained on the original trajectories with those trained on subsampled trajectories.
We observe that the key properties of the problem, namely the strictly increasing errors with k in the
memoryless learning (W = 1), and non-monotonic behaviour of errors due to the trade-off between the
memory (W ) and the non-Markovianity (k) in more complicated models are preserved.

As mentioned in the main text we subsample the sequences by taking every sth element of the
original sequence in the dataset when training the models in Sec. 4.3. To investigate the effect of this
subsampling we compare AR models trained on original dataset with those trained on subsampled
sequences with s = 30, and show the results in fig. 6. The qualitative features of the error curves as a
function of non-Markovianity k are preserved. The monotonicity of errors for memoryless models and
the trade-off between improvements by memory and the difficulty of correlating the particle’s position
xt to the trap’s position ct, the two main features of fig. 4, are present in both the original and the
subsampled cases.

B.3 The thresholding algorithm

As a baseline benchmark, we consider a simple thresholding algorithm, where ˆct = sgn(xt). Put simply,
this algorithm infers the potential’s location by only considering the position of the particle at a single
point xt and choose the closest configuration of potential ct to that position. In fig. 7 we compare the
performance of the AR(1) model with the thresholding algorithm and observe that AR(1) matches the
performance of the the thresholding algorithm.

B.4 Selecting the best GRU(1) model

To find the best performing GRU(1) we train model for all k values shown in fig. 4 we train five different
instance of the network initialized randomly using as suggested in Ref. [66]. We choose the best

18

Figure 6: Subsampling effects. We compare the error of AR models trained on original raw dataset
and the subsampled one with a subsampling factor of s = 30. The key qualitative features of the error
curves as function of the non-Markovianity k are not affected by subsampling.

Figure 7: Comparing the AR(1) and the thresholding algorithm. Both the AR(1) model and the
thresholding algorithm only have access to the position of the particle at a single point. As a consistency
check we compare the performance of the two and observe that they match.

19

100101102k101520253035†(%)Subsampled, W=1Subsampled, W=2Subsampled, W=8Raw, W=1Raw, W=60Raw, W=240100101102k242628303234†(%)AR(1)ThresholdingFigure 8: Validating the GRU(1) model. The accuracy of the best performing GRU(1) models over
the cross validation and test sets match.

performing model over 10000 samples for each k, and report the accuracy on a new set of 10000 samples.
As shown in fig. 8 the test and cross validation accuracy are in excellent agreement.

20

100101102k212223242526†(%)TestCross