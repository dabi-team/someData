LEE et al.: DIFFERENTIABLE ARTIFICIAL REVERBERATION

1

Differentiable Artiﬁcial Reverberation

Sungho Lee, Hyeong-Seok Choi, and Kyogu Lee

2
2
0
2

l
u
J

0
2

]

D
S
.
s
c
[

3
v
0
4
9
3
1
.
5
0
1
2
:
v
i
X
r
a

Abstract—Artiﬁcial reverberation (AR) models play a central
role in various audio applications. Therefore, estimating the AR
model parameters (ARPs) of a reference reverberation is a crucial
task. Although a few recent deep-learning-based approaches have
shown promising performance, their non-end-to-end training
scheme prevents them from fully exploiting the potential of deep
neural networks. This motivates the introduction of differentiable
artiﬁcial reverberation (DAR) models, allowing loss gradients to
be back-propagated end-to-end. However, implementing the AR
models with their difference equations “as is” in the deep learning
framework severely bottlenecks the training speed when executed
with a parallel processor like GPU due to their inﬁnite impulse
response (IIR) components. We tackle this problem by replacing
the IIR ﬁlters with ﬁnite impulse response (FIR) approximations
with the frequency-sampling method. Using this technique, we
implement
three DAR models—differentiable Filtered Velvet
Noise (FVN), Advanced Filtered Velvet Noise (AFVN), and
Delay Network (DN). For each AR model, we train its ARP
estimation networks for analysis-synthesis (RIR-to-ARP) and
blind estimation (reverberant-speech-to-ARP) task in an end-to-
end manner with its DAR model counterpart. Experiment results
show that the proposed method achieves consistent performance
improvement over the non-end-to-end approaches in both objec-
tive metrics and subjective listening test results. Audio samples
are available at https://sh-lee97.github.io/DAR-samples/.

Index Terms—Digital Signal Processing, Acoustics, Reverbera-

tion, Artiﬁcial Reverberation, Deep Learning.

I. INTRODUCTION

Reverberation is ubiquitous in a real acoustic environment.
It provides the listeners psychoacoustic cues for spatial char-
acteristics. Therefore, adding an appropriate reverberation to a
dry audio is desirable for plausible listening [1]–[3]. Artiﬁcial
reverberation (AR), efﬁcient digital ﬁlters that may be used
to mimic real-world reverberation, have been developed to
achieve such auditory effects [4]–[6] and applied to room
acoustic enhancement [7], auditory scene generation [8], [9],
post-production [10], and many more.

Manuscript received October 21, 2021; revised March 27, 2022 and June
10, 2022; accepted July 15, 2022. This work was supported by Institute of
Information & communications Technology Planning & Evaluation (IITP)
grant funded by the Korea government (MSIT) (No.2022-0-00641). The
associate editor coordinating the review of this manuscript and approving
it for publication was Cecchi, Stefania (Corresponding Author: Kyogu Lee).
Sungho Lee is with the Music and Audio Research Group, Graduate School
of Convergence Science and Technology, Seoul National University, Seoul,
Republic of Korea (e-mail: sh-lee@snu.ac.kr).

Hyeong-Seok Choi was with the Music and Audio Research Group, Graduate
School of Convergence Science and Technology, Seoul National University,
Seoul, Republic of Korea. He is now with the Institute of New Media and
Communications, Seoul National University, Seoul, Republic of Korea (e-mail:
kekepa15@snu.ac.kr).

Kyogu Lee is with the Music and Audio Research Group, Graduate School
of Convergence Science and Technology, Seoul National University, Seoul,
South Korea, and also with the Advanced Institutes of Convergence Technology,
Suwon, South Korea (e-mail: kglee@snu.ac.kr).

(a) Training ARP estimation network with DAR models.

(b) Inference-time usage of the trained ARP estimation network.

Fig. 1. The proposed ARP estimation framework. (a) With the DAR models,
loss gradients ∂L/∂P1, · · · , ∂L/∂Pn can be back-propagated through the
DAR models. Hence, the ARP estimation network can be trained in an end-to-
end manner. We can train the network to perform the analysis-synthesis (RIR
h input), blind estimation (reverberant speech h ∗ x input), or even both. The
network has an AR-model-agnostic encoder so that using a different DAR
model only requires changing the tiny projection layers Proj1, · · · , Projn. (b)
Each DAR model generate an FIR approximation of its original AR model’s
IIR. After the training, estimated ARPs can be plugged in to the AR model
which is highly efﬁcient and real-time controllable in CPU.

Nevertheless, estimating the AR model parameters (ARPs)
that match the reference reverberation remains challenging. This
is because the mapping from the ARPs to the reverberation is
highly nontrivial. We refer to this task as analysis-synthesis
when the reference is a room impulse response (room IR, RIR).
When the reference is indirectly provided with a reverberant
signal, i.e., an RIR convolved with a dry signal, we refer to
this task as blind estimation. We limit the scope to reverberant
speech signals in this paper, yet the framework can easily be
extended to other types of signals, such as musical ones.

ARP estimation is a classical problem, and various attempts
have been made [11]–[14]. However, they are only applicable
to a speciﬁc AR model (AR-model-dependent) and the analysis-
synthesis task (task-dependent). This inﬂexibility is problematic
since every application has its own suitable AR models and
reference forms (e.g., RIR and reverberant speech).

We can overcome such limitations with differentiable arti-
ﬁcial reverberation (DAR) models. Each DAR model takes
ARPs as input and generates its corresponding AR model’s IR
(see Figure 1a). After computing a loss between the generated
IR and the reference RIR, they allow the loss gradients to be
back-propagated through themselves. As a result, we can train
a deep neural network (DNN) to match the reference RIR in
an end-to-end manner. By leveraging the DNN’s ﬂexibility and
expressiveness, we can train the network to perform any task
with any AR model with minimal architecture change.

 
 
 
 
 
 
LEE et al.: DIFFERENTIABLE ARTIFICIAL REVERBERATION

2

Since the AR models are exactly or close to linear time-
invariant (LTI), we can implement them differentiably “as
is” with their difference equations [15]–[18]. However, this
approach has been impractical because the training speed is
limited by the recurrent sample generation of their IIR ﬁlter
components when executed on a parallel processor, such as
a GPU. In this context, we aim to modify the vanilla DAR
models to sidestep the aforementioned problem. Inspired by
recent works [19], [20], we replace the IIR ﬁlters with ﬁnite
impulse response (FIR) approximations using the frequency-
sampling method [21]. This way, each DAR model becomes an
FIR ﬁlter whose IR can be generated without any recurrent step.
Using this technique, we present differentiable Filtered Velvet
Noise (FVN) [13], Advanced Filtered Velvet Noise (AFVN),
and Delay Network (DN) [22], [23]. Note that the AR-to-
DAR conversion can be applied systematically to any other
AR model with IIR components. While the DAR models are
not identical to their originals, the differences are little in
most cases, allowing us to train the ARP estimation networks
reliably with them. Furthermore, even when differences exist,
the beneﬁts of the end-to-end learning enabled by the DAR
models outweigh them; we show that the proposed approach
brings consistent performance improvement over non-end-to-
end baselines where the difference is only in the training
scheme. After the training, we can revert to the original AR
models for efﬁcient computation (see Figure 1b).

II. RELATED WORKS

A. Differentiable Digital Signal Processing

Digital signal processing (DSP) and deep learning are closely
related. For example, a temporal convolutional network [24] is
a stack of dilated one-dimensional convolutional layers with
nonlinear activation layers and it has been used for modeling
dynamic range compression [25] and reverberation [26]. From
the DSP viewpoint, it is FIR ﬁlter banks serially connected with
memoryless nonlinearities, i.e., an extension of the classical
Wiener-Hammerstein model [27].

If we look for more explicit DSP-deep learning relationships,
so-called “differentiable digital signal processing (DDSP)” ap-
proaches exist which aim to import DSP components into deep
learning frameworks as they provide strong structural priors and
interpretable representations. For example, a harmonic-plus-
noise model [28] was implemented in such a manner; then, a
DNN was trained to estimate its parameters to synthesize a
monophonic signal and used for various applications, including
controllable synthesis and timbre transfer [29].

In the case of LTI ﬁlters, it is well-known that both an FIR
and IIR ﬁlter are differentiable and can be directly imported
into the deep learning framework as a convolutional layer and
recurrent layer, respectively [15]–[18]. Since the IIR ﬁlter, e.g.,
parametric equalizer (PEQ), bottlenecks the training speed, the
frequency-sampling method [21] was utilized [19], [20]. Upon
these works, we investigate the reliability of this technique,
present a general differentiable IIR ﬁlter based on state-variable
ﬁlter (SVF) [30], [31], and use it to implement the DAR models.

B. ARP Estimation

1) Analysis-synthesis: Various methods have been proposed
to systematically and automatically estimate ARPs that mimic
certain characteristics of a given RIR. Some works ﬁrst extract
perceptually relevant features from the RIR, then estimate the
ARPs from them. For example, FDN parameters were derived
from the RIR’s energy decay relief (EDR) representation which
encodes the frequency-dependent reverberation decay [11].
Along with the FVN and AFVN, their parameter estimation
algorithm based on linear prediction were proposed together
[13]. Another thread of work utilizes optimization. A genetic
algorithm was proposed to ﬁnd the FDN parameters that achieve
the desired “ﬁtness” to the reference RIR [12]. Since all these
methods assume availability of the RIR, they cannot be applied
to the blind estimation task.

2) Blind Estimation: End-users (e.g., audio engineers) often
interact with the AR models provided as audio plug-ins. As
a result, several attempts have been made to implement blind
estimation algorithms for the plug-ins. For example, a preset
recommendation method based on the Gaussian mixture model
was proposed [14]. More recently, DNNs were trained for the
plug-in parameter estimation and preset recommendation [10].
While these methods have shown promise, there is still room
for improvement. First, they generated the training data using
the plug-ins to train their models. However, this procedure
may be time-consuming, and relying on data generated with a
single plug-in may increase the generalization error. Second,
their training objectives could be suboptimal. When the task
is deﬁned as parameter regression [10], one must weigh each
parameter’s perceptual importance and apply it to the loss
function design, which is a highly challenging task. When the
task is deﬁned as classiﬁcation [10], [14], it could suffer from
scalability problems. Finally, most plug-ins provide only a few
parameters, and each of them controls multiple internal ARPs
simultaneously. This might limit the performance due to the
reduced degree of freedom.

We tackle these problems with the DAR models as follows.
First, we can use a training objective that directly compares the
estimated IR with ground-truth RIR. Therefore, we can use any
RIR for training and bypass the need to obtain the ground-truth
ARPs. Second, we replace the parameter-matching loss with
the end-to-end IR-matching loss and show that it improves the
estimation performance by a large margin in both objective
metrics and subjective listening test scores. Finally, we estimate
the ARPs directly instead of relying on the plug-in parameters.
3) Two-Stage Approaches: Note that both analysis-synthesis
and blind estimation tasks can be divided into two subtasks.
First, we can obtain reverberation parameters, e.g., reverberation
time, from a given reference (reference-to-reverberation param-
eters). We can directly compute the parameters from the RIR
[32] for the analysis-synthesis case. For the blind estimation,
various methods have been proposed [33]–[36]. Then, we can
tune the ARPs to match the desired reverberation parameter
values (reverberation parameters-to-ARPs) [37]–[39]. This two-
stage approach can be a possible alternative to our single-stage
end-to-end estimation method. However, perceptually different
reverberations can have the same reverberation parameters,
resulting in information loss.

LEE et al.: DIFFERENTIABLE ARTIFICIAL REVERBERATION

3

C. Modeling Reverberation in the Deep Learning Framework

An alternative to the proposed DAR approach could be using
existing deep learning modules to build another reverberation
model. The most simplest approach could be using a high-order
FIR directly as an RIR model [29]. While it is viable when the
objective is to learn a single RIR, training a DNN to generate
various raw RIRs is highly challenging and inefﬁcient.

Instead, recent approaches either use a DNN as a reverberator
and control them via conditioning methods [26], [40] or design
a differentiable RIR generator and use a DNN as a parameter
estimator [41]. Our method is closely related to the latter since
the DAR model and the proposed ARP estimation network
act in that way. However, the key difference is that our
DAR models are direct imports of the AR models, which
bring the following beneﬁts. First, they are more directly
interpretable and controllable, allowing end-users to further tune
the parameters. Second, their structure can prevent unexpected
artifacts. Finally, they use sparse IIR and FIR ﬁlters, enabling
efﬁcient real-time computation in CPU.

III. FREQUENCY-SAMPLING METHOD FOR
DIFFERENTIABLE IIR FILTERS
For any real IIR ﬁlter H, its frequency response H(ejω) is
continuous, 2π-periodic, and conjugate symmetric. To obtain
its FIR approximation, we frequency-sample H(ejω) at angular
frequencies ωk = 2πk/N where k = 0, · · · , (cid:98)N/2(cid:99) [21]. We
denote the frequency-sampled ﬁlter with subscript N , i.e., HN .
HN [k] = H(ejωk ).

(1)

Note that the frequency-sampling can also be deﬁned on transfer
functions, e.g., H(z) with z = ejωk .

Since each sampling is independent, HN can be generated
simultaneously with a parallel processor like GPU. Furthermore,
the order of frequency-sampling (·)N and other basic arithmetic
operations does not matter. Therefore, we can frequency-sample
an IIR ﬁlter by combining frequency-sampled m-sample delays
(z−m)N ∈ C(cid:98)N/2(cid:99)+1 as follows,

(cid:33)

(cid:32) (cid:80)M
(cid:80)M

(cid:80)M

HN [k] =

m=0 βmz−m
m=0 αmz−m

m=0 βm(z−m)N [k]
m=0 αm(z−m)N [k]
(2)
Since (z−m)N [k] = e−j2πkm/N , using the widely-used deep
learning libraries, we obtain (z−m)N as a tensor z_m by

[k] =

(cid:80)M

N

.

angle = 2*pi*arange(N//2+1)/N,

z_m = e**(-1j*angle*m).

(3a)

(3b)

Then, we apply equation (2) to z_0, · · · , z_M with basic tensor
operations and obtain every sample of HN in parallel. A time-
domain representation of HN is a length-N FIR hN [n], inverse
discrete Fourier transform (inverse DFT) of HN [k], i.e.,

hN [n] =

1
N

N −1
(cid:88)

k=0

HN [k]ejωkn(u[n] − u[n − N ])

(4)

Fig. 2. The frequency-sampling method with a various number of sampling
points N . Blue curves represent its magnitude response |H(ejω)| (top row)
and IR h[n] (bottom row). Red curves are the magnitude response and IR of
the frequency-sampled ﬁlter HN .

hN [n] whose convolution can be performed efﬁciently via Fast
Fourier Transform (FFT). In this paper, the “differentiable IIR”
ﬁlter refers to HN , which is in fact an FIR ﬁlter.

A. Reliability of the Frequency-Sampling Method

Every IIR ﬁlter’s frequency response has “inﬁnite bandwidth”
(in the time domain) so that the frequency-sampling causes time-
aliasing [42]. Speciﬁcally, the frequency-sampled IR hN [n] is
sum of length-N segments of the original IR h[n].

hN [n] =

∞
(cid:88)

m=0

h[mN + n](u[n] − u[n − N ]).

(5)

Since the energy of every stable IIR ﬁlter’s IR decays over time,
more frequency-sampling points N give less time-aliasing, i.e.,
a closer approximation (see Figure 2). Therefore, we can use
the differentiable ﬁlter HN in place of the original H to match
a reference response HRef. We elaborate this with following
analytical results.

1) Time-aliasing Error: If H has M distinct poles where
each of them νi ∈ C has multiplicity ri ∈ N, the time-aliasing
error asymptotically decreases as N increases as follows,

(cid:107)H − HN (cid:107)2 =

M
(cid:88)

i=1

O(N ri−1|νi|N ).

(6)

2) Loss Error: The triangle inequality indicates that the loss

error is bounded to the time-aliasing error.

(cid:12)
(cid:12)(cid:107)HRef − H(cid:107)2

2 − (cid:107)HRef − HN (cid:107)2
2

(cid:12)
(cid:12) ≤ (cid:107)H − HN (cid:107)2
2.

(7)

3) Loss Gradient Error: The loss gradient error has similar

asymptotic behavior to the time-aliasing error as follows,

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂
∂p

(cid:107)HRef − H(cid:107)2

2 −

∂
∂p

(cid:107)HRef − HN (cid:107)2
2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

M
(cid:88)

=

O(N 3ri−1|νi|N )

(8)
where p is a parameter of H. See Appendix C for the proofs.

i=1

where u[n] is a unit step function.

B. Differentiable Dense IIR Filter with State-variable Filters

Therefore, using the frequency-sampling method, we can
replace any IIR ﬁlter H that causes the bottleneck with an FIR

Since arbitrary IIR ﬁlter can be expressed as serially cascaded
biquads (second-order IIR ﬁlters), we can obtain a differentiable

Magnitude(dB)Original FilterFrequency-SampledN=10Waveform (amplitude)N=20N=40N=80LEE et al.: DIFFERENTIABLE ARTIFICIAL REVERBERATION

4

IIR ﬁlter ˜H as a product of frequency-sampled biquads (Hi)N .
(cid:32) (cid:80)2
(cid:80)2

(Hi)N [k] =

˜H[k] =

m=0 βi,mz−m
m=0 αi,mz−m

[k].

(cid:89)

(cid:89)

(9)

(cid:33)

i

i

N

Moreover, we use the state variable ﬁlter (SVF) parameters
fi, Ri, mLP
to express each biquad as follows,
i

, and mHP

, mBP
i
βi,0 = f 2
βi,1 = 2f 2
βi,2 = f 2
αi,0 = f 2
αi,1 = 2f 2
αi,2 = f 2

i + mHP
i

i
i + fimBP
i mLP
i − 2mHP
i mLP
i
i + mHP
i − fimBP
i mLP
i
i + 2Rifi + 1,
i − 2,
i − 2Rifi + 1.

,

,

,

(10a)

(10b)

(10c)

(10d)

(10e)

(10f)

We prefer to use the SVF parameters rather than the biquad

coefﬁcients due to the following reasons.

i

, and highpass ﬁlter H HP

• Interpretability. Each SVF has lowpass H LP

, bandpass
i
H BP
. They share a resonance Ri
i
and cutoff parameter fi = tan(πωi/ωs) where ωi is their
cutoff frequency and ωs is the sampling rate. The ﬁlter
outputs are multiplied with gains mLP
, and mHP
i
and then summed. This interpretability comes without any
generality loss since an SVF can express any biquad [31].
• Simple Activation Functions. The SVF parameters have
simpler stability conditions, Ri > 0 and fi > 0, than the
biquad coefﬁcients [18], which leads to simpler activation
function design. Refer to Section V-C for details.

, mBP
i

i

• Better Performance. We found that our ARP estimation
networks performed better when they estimate the SVF
parameters rather than the biquad coefﬁcients. Refer to
Appendix A-B for the comparison results.

From now on, we denote each SVF-parameterized biquad

with a superscript H SVF

i

and call it simply “SVF”.

C. Differentiable Parametric Equalizer

A low-shelving H LS, peaking H Peak, and high-shelving ﬁlter
H HS are widely used audio ﬁlters. Each of them can be obtained
using the SVF H SVF(f, R, mLP, mBP, mHP) as follows [30],

H LS(f, R, G) = H SVF(f, R, G, 2R
H Peak(f, R, G) = H SVF(f, R, 1, 2RG, 1),
H HS(f, R, G) = H SVF(f, R, 1, 2R

√

G, G).

G, 1),

(11a)

(11b)

(11c)

√

Here, G is a new parameter that gives 20 log10(G) dB gain. A
parametric equalizer (PEQ) is serial composition of such ﬁlters.
Following the recent works [19], [20], we use one low-shelving,
one high-shelving, and K − 2 peaking ﬁlters.

H PEQ(z) = H LS(z)H HS(z)

K−2
(cid:89)

i=1

H Peak
i

(z).

(12)

Same as the IIR ﬁlter case, frequency-sampling the components
and multiplying them results in a differentiable PEQ H PEQ
N .

IV. DIFFERENTIABLE ARTIFICIAL REVERBERATION

In this section, we derive differentiable FVN, AFVN, and
DN. We ﬁrst brieﬂy review the original AR models. Then,

Fig. 3. Modiﬁed FVN, AVFN’s RIR approximation, and their differentiable
implementation strategy. FVN and AVFN divide a target RIR h[n] into
segments hi[n] and approximate each segment with a velvet noise vi ﬁltered
with an allpass ﬁlter Ui and a coloration ﬁlter Ci. We obtain the differentiable
FVN and AVFN by converting the IIR ﬁlters Ui and Ci into FIR ﬁlters ˜ui
and (ci)N . Each IIR ﬁlter is emphasized with a double border box.

we obtain their DAR counterparts by frequency-sampling the
IIR components, replacing them with FIRs. Also, we slightly
modify the original models for efﬁcient training, plausible
reverberation, and better overall estimation performance.

A. Differentiable Filtered Velvet Noise

1) Filtered Velvet Noise: One can divide an RIR h[n] into
S segments (see Figure 3). As reverberation can be regarded
as mostly stochastic, we can model each length-Li segment
hi[n] with a source noise signal si[n] “colored with” a ﬁlter
Ci. When we use velvet noise for each source, this source-
ﬁlter model becomes Filtered Velvet Noise (FVN) [13], [43],
[44]. The velvet noise vi[n] is sparse; in every length-Ti
interval, it contains a single nonzero sample ±1 with random
sign/position such that its time-domain convolution is highly
efﬁcient. Moreover, an allpass ﬁlter Ui is introduced to smooth
each source and make plausible sound. Finally, an deterministic
(bypass) FIR ˆh0[n] models the direct arrival and early reﬂection.
Then, an IR of FVN ˆh[n] becomes

ˆh[n] = ˆh0[n] +

S
(cid:88)

i=1

si[n−di]
(cid:125)(cid:124)

(cid:123)

(cid:122)
vi[n − di] ∗ ui[n] ∗ ci[n]
(cid:123)(cid:122)
(cid:125)
(cid:124)
ˆhi[n−di]

(13)

where each di is a delay for the segment alignment (d1 = 0). In
practice, each allpass ﬁlter Ui is composed with Schroeder all-
i,j (z) = (1 + γi,jz−τi,j )/(γi,j + z−τi,j ),
pass ﬁlters (SAPs), U SAP
where τi,j is a delay-line length and γi,j is a feed-forward/back
gain) for efﬁciency [45]. A low-order dense IIR ﬁlter is used
as each coloration ﬁlter Ci.

𝑥𝑛⋯𝑦𝑛=&ℎ𝑛∗𝑥𝑛𝑈!"#$𝑉!+𝐻%𝑈&"#$𝑉&𝑈’"#$𝑉’⋯&ℎ!𝑛∗𝑥𝑛&ℎ&𝑛−𝑑&∗𝑥𝑛&ℎ’𝑛−𝑑’∗𝑥𝑛𝐶!𝐶&𝐶’𝑥𝑛𝐶!⋯𝑦𝑛=&ℎ𝑛∗𝑥𝑛𝑈!"#$𝑉!+𝐻%𝐶(&𝑈&"#$𝑉&𝐶(’𝑈’"#$𝑉’⋯&ℎ!𝑛∗𝑥𝑛&ℎ&𝑛−𝑑&∗𝑥𝑛&ℎ’𝑛−𝑑’∗𝑥𝑛0ℎ&=0ℎ!=0ℎ’=0ℎ𝐿!𝐿"𝐿#ℎ%≈&ℎ%ℎ𝑣&𝑐&𝑈!"#$𝑢&"#$ℎ&≈&ℎ&=𝑣&𝑐&*𝑈!"#$5𝑢&0ℎ&=≈𝑣!𝑐!𝑢!"#$ℎ!≈&ℎ!=𝑣!𝑐!*5𝑢!0ℎ!=≈frequency-samplecopyconvolve&crop𝑣’𝑐’𝑢’"#$⋮ℎ’≈&ℎ’=𝑣’𝑐’*5𝑢’0ℎ’=≈⋯⋯⋯⋯⋯⋯𝑢’𝑢&𝑢!∗∗∗∗∗∗∗∗∗∗∗∗𝑐(&𝑐&*𝑐!𝑐!*frequency-sample𝑐(’𝑐’*⋯⋮⋮FVNAFVN𝑐’𝑐&LEE et al.: DIFFERENTIABLE ARTIFICIAL REVERBERATION

5

(a) Approximation error of differentiable FVN in EDR.

(a) Modiﬁed FVN in the time domain.

(b) Approximation error of differentiable AFVN in EDR.

Fig. 4. EDR of IRs generated by differentiable FVN, AFVN, their original AR
counterparts, and EDR errors introduced during the AR-to-DAR conversion
process. Because the errors are small and mostly in the low-energy region,
they are imperceptibly small (note that the z axis scale is different for the
EDR plots and the EDR error plots). We obtained the FVN/AFVN parameters
with the trained blind estimation networks discussed below.

2) Differentiable Implementation: We modify each FVN

component to obtain differentiable FVN.

• Velvet ﬁlters. To batch the IR segments, we use the same
segment lengths L1, · · · , LS and average pulse distance
values T1, · · · , TS for every reference RIR.

• Allpass ﬁlters. We tune and ﬁx the allpass ﬁlter parameters
jointly with the velvet ﬁlter parameters to avoid perceptual
artifacts, e.g., discontinuous and rough sound, while being
computationally efﬁcient. Since we ﬁxed the ﬁlter, instead
of the frequency-sampling method, we simply crop its IR
to obtain an FIR ˜ui[n].

• Coloration ﬁlters. We model Ci with K serial SVFs, i.e.,
Ci(z) = (cid:81)K
i,k (z). We frequency-sample each ﬁlter
Ci(z) and convert it into a length-N FIR (ci)N [n] so that
we can train our network to estimate its parameters.

k=1 C SVF

Therefore, IR of the differentiable FVN ˜h[n] becomes

(cid:122)
(cid:123)
˜h[n] = ˆh0[n] + overlap-add{
}.
vi[n] ∗ ˜ui[n] ∗(ci)N [n]
(cid:125)
(cid:123)(cid:122)
(cid:124)
˜hi[n]

˜si[n]
(cid:125)(cid:124)

(14)

Since every component vi[n], ˜ui[n], and (ci)N [n] is an FIR,
we can compute each segment ˜hi[n] = vi[n] ∗ ˜ui[n] ∗ (ci)N [n]
efﬁciently by convolving the FIRs in the frequency domain,
i.e., zero-padding followed by FFT, multiplication, then IFFT.
Finally, we overlap-add all the segments and add the determin-
istic FIR ˆh0[n] to obtain the full IR ˜h[n]. The overlap-add can
be implemented with a transposed convolution layer.

While the modiﬁcations for the differentiable FVN introduce
approximation error, it is negligible in practice. We explain
this with the EDR H EDR, a reverse cumulative sum of an IR’s
short-time Fourier transform (STFT) energy [11].

H EDR[k, n] =

∞
(cid:88)

m=n

|H STFT[k, m]|2.

(15)

(b) Modiﬁed AVFN in the time domain.

(c) Each velvet ﬁlter Vi.

(d) Each coloration ﬁlter Ci.

Fig. 5. Modiﬁcations to the original FVN and AFVN. (a) Modiﬁed FVN. To
retain a sharp transient, we insert each SAP before its corresponding velvet
ﬁlter. For each velvet ﬁlter Vi, the output denoted with a down arrow is a
convolution of the input and the velvet noise vi[n]. The other output denoted
with a right arrow is a Li-sample-delayed input. Refer to [13] for more details.
(b) The same modiﬁcation is performed to the AVFN model. (c) For both FVN
and AVFN, each velvet ﬁlter Vi is divided into smaller ﬁlters Vi,1, · · · , Vi,M
assigned with gains gi,1, · · · , gi,M . This modiﬁcation is equivalent to dividing
each velvet segment vi[n] into smaller segments vi,1[n], · · · , vi,M [n] and
multiplying them with gi,1, · · · , gi,M . (d) The ith coloration ﬁlter Ci in the
time domain, which is serially connected SVFs CSVF

i,1 , · · · , CSVF
i,K .

By comparing the FVN’s EDR and its differentiable counterpart,
we can analyze their differences in reverberation characteristics.
Speciﬁcally, we obtain the EDR error EEDR by calculating the
difference between the two log-magnitude EDRs.

EEDR = 10 log10

ˆH EDR − 10 log10

˜H EDR.

(16)

Figure 4a shows the EDRs and the EDR error. It reveals that
the error is small (at most 3dB) and mostly in the low-energy
region (below −40dB). Also, there is little EDR error at the ﬁrst
time index, i.e., |EEDR[k, 0]| (cid:28) 1, indicating that the overall
magnitude response remains the same during the conversion
process. In conclusion, the error is hardly noticeable, and the
differentiable FVN can replace the original FVN for the training
(we use a loss function related to the EDR; see Section V-D).
3) Modiﬁcations to the Original Model: FVN was initially
designed for the analysis-synthesis of late reverberation; it used
the direct arrival and early reﬂections cropped from the RIR
and modeled only the late part. However, we also aim to solve
the blind estimation problem where we must estimate the entire
RIR. To this end, we modify the original model as follows.

• Finer segment gain. We divide each velvet segment vi[n]
into M smaller segments vi,1[n], · · · , vi,M [n] multiplied
with gains gi,1, · · · , gi,M . Figure 5c shows the resulting

Frequency (Hz)601255001k4k16kTime (s)00.511.522.5   0   -20   -40   -60 Differentiable FVNFrequency (Hz)601255001k4k16kTime (s)00.511.522.5   0   -20   -40   -60 FVNFrequency (Hz)601255001k4k16kTime (s)00.511.522.5Error (dB)   -12   -6   0   6   12Frequency (Hz)601255001k4k16kTime (s)00.511.522.5   0   -20   -40   -60 Differentiable AFVNFrequency (Hz)601255001k4k16kTime (s)00.511.522.5   0   -20   -40   -60 AFVNFrequency (Hz)601255001k4k16kTime (s)00.511.522.5Error (dB)   -12   -6   0   6   12𝑥𝑛⋯𝑦𝑛=&ℎ𝑛∗𝑥𝑛𝑈!"#$𝑉!+𝐻%𝑈&"#$𝑉&𝑈’"#$𝑉’⋯&ℎ!𝑛∗𝑥𝑛&ℎ&𝑛−𝑑&∗𝑥𝑛&ℎ’𝑛−𝑑’∗𝑥𝑛𝐶!𝐶&𝐶’𝐿!𝐿"𝐿#ℎ%≈&ℎ%ℎ𝑣&𝑐&𝑈!"#$𝑢&"#$ℎ&≈&ℎ&=𝑣&𝑐&)𝑈!"#$4𝑢&5ℎ&=≈𝑣!𝑐!𝑢!"#$ℎ!≈&ℎ!=𝑣!𝑐!)4𝑢!5ℎ!=≈frequency-samplecopyconvolve&crop𝑣’𝑐’𝑢’"#$⋮ℎ’≈&ℎ’=𝑣’𝑐’)4𝑢’5ℎ’=≈⋯⋯⋯⋯⋯⋯𝑢’𝑢&𝑢!𝑥𝑛𝐶!⋯𝑦𝑛=&ℎ𝑛∗𝑥𝑛𝑈!"#$𝑉!+𝐻%𝐶;&𝑈&"#$𝑉&𝐶;’𝑈’"#$𝑉’⋯&ℎ!𝑛∗𝑥𝑛&ℎ&𝑛−𝑑&∗𝑥𝑛&ℎ’𝑛−𝑑’∗𝑥𝑛5ℎ&=5ℎ!=5ℎ’=5ℎ∗∗∗∗∗∗∗∗∗∗∗∗𝑐;&𝑐&)𝑐!𝑐!)frequency-sample𝑐;’𝑐’)⋯⋮⋮FVNadv-FVN𝑐’𝑐&𝑥𝑛⋯𝑦𝑛=&ℎ𝑛∗𝑥𝑛𝑈!"#$𝑉!+𝐻%𝑈&"#$𝑉&𝑈’"#$𝑉’⋯&ℎ!𝑛∗𝑥𝑛&ℎ&𝑛−𝑑&∗𝑥𝑛&ℎ’𝑛−𝑑’∗𝑥𝑛𝐶!𝐶&𝐶’𝐿!𝐿"𝐿#ℎ%≈&ℎ%ℎ𝑣&𝑐&𝑈!"#$𝑢&"#$ℎ&≈&ℎ&=𝑣&𝑐&)𝑈!"#$4𝑢&5ℎ&=≈𝑣!𝑐!𝑢!"#$ℎ!≈&ℎ!=𝑣!𝑐!)4𝑢!5ℎ!=≈frequency-samplecopyconvolve&crop𝑣’𝑐’𝑢’"#$⋮ℎ’≈&ℎ’=𝑣’𝑐’)4𝑢’5ℎ’=≈⋯⋯⋯⋯⋯⋯𝑢’𝑢&𝑢!𝑥𝑛𝐶!⋯𝑦𝑛=&ℎ𝑛∗𝑥𝑛𝑈!"#$𝑉!+𝐻%𝐶;&𝑈&"#$𝑉&𝐶;’𝑈’"#$𝑉’⋯&ℎ!𝑛∗𝑥𝑛&ℎ&𝑛−𝑑&∗𝑥𝑛&ℎ’𝑛−𝑑’∗𝑥𝑛5ℎ&=5ℎ!=5ℎ’=5ℎ∗∗∗∗∗∗∗∗∗∗∗∗𝑐;&𝑐&)𝑐!𝑐!)frequency-sample𝑐;’𝑐’)⋯⋮⋮FVNadv-FVN𝑐’𝑐&𝑥𝑛⋯𝑦𝑛=𝑣!𝑛∗𝑥𝑛𝑉!,#𝑉!,$𝑉!,%⋯𝑔!,#𝑔!,$𝑔!,%𝑥𝑛−𝑙!𝑥𝑛𝐶!,#&’(⋯𝑦𝑛=-ℎ𝑛∗𝑥𝑛𝐶!,$&’(𝑉#/𝐻)𝐶*$𝑈$&+,𝑉$𝐶*-𝑈-&+,𝑉-⋯-ℎ#𝑛∗𝑥𝑛-ℎ$𝑛−𝑑$∗𝑥𝑛-ℎ-𝑛−𝑑-∗𝑥𝑛𝑥𝑛𝐶!,#&’(𝐶!,$&’(𝐶!,.&’(⋯𝑐!𝑛∗𝑥𝑛𝑥𝑛⋯𝑦𝑛=𝑣!𝑛∗𝑥𝑛𝑉!,#𝑉!,$𝑉!,%⋯𝑔!,#𝑔!,$𝑔!,%𝑥𝑛−𝑙!𝑥𝑛𝐶!,#&’(⋯𝑦𝑛=-ℎ𝑛∗𝑥𝑛𝐶!,$&’(𝑉#/𝐻)𝐶*$𝑈$&+,𝑉$𝐶*-𝑈-&+,𝑉-⋯-ℎ#𝑛∗𝑥𝑛-ℎ$𝑛−𝑑$∗𝑥𝑛-ℎ-𝑛−𝑑-∗𝑥𝑛𝑥𝑛𝐶!,#&’(𝐶!,$&’(𝐶!,.&’(⋯𝑐!𝑛∗𝑥𝑛LEE et al.: DIFFERENTIABLE ARTIFICIAL REVERBERATION

6

velvet ﬁlter Vi in the time domain.

• Cumulative allpass. The original model smoothes every
velvet noise with the same allpass ﬁlter. This is undesirable
in our context since it could smear the direct arrival and
early reﬂections. Instead, we gradually cascade the SAPs
to obtain each allpass ﬁlter Ui(z) = (cid:81)i
(z) so the
earlier segments are less smeared than the later ones. As
shown in Figure 5a, this modiﬁcation can be implemented
in the time domain by inserting each SAP U SAP
before
the corresponding velvet ﬁlter Vj.

j=1 U SAP
j

j

• Shorter deterministic FIR. With the above changes, the
stochastic segments can ﬁt the early RIR to some extent.
Therefore, we shorten the early deterministic FIR ˆh0[n].
Refer to Appendix A-A for the evaluation of the modiﬁcations.
4) Model Conﬁguration Details: Exact conﬁgurations of

our FVN used for the experiments are as follows.

j

We generated 2.5 seconds of IR (48kHz sampling rate, total
L = 120k samples) with FVN. We used S = 20 non-uniform
velvet segments whose lengths Li are L/40, L/20, and L/10
for 10, 5, and 5 segments, respectively, and their average pulse
distances were set to T = [10, 20, 35, 50, 65, 90, 120, 135,
180, 220, 270, 320, 370, 420, 480, 540, 610, 680, 750, 820].
The SAPs U SAP
were forced to have gains γj = 0.75 + 0.01j
and delay lengths τ = [23, 48, 79, 109, 113, 127, 163, 191,
229, 251, 293, 337, 397, 421, 449, 509, 541, 601, 641, 691].
Each velvet segment had M = 4 sub-segment gains and ﬁltered
with K = 8 SVFs. We frequency-sampled the coloration ﬁlters
with N = 4k points. The length of the deterministic FIR h0[n]
was set to 50. We set the gain g ∈ RS×M , SVF parameters
f , R, mLP, mBP, mHP ∈ RS×K, and the bypass FIR h0 ∈ RZ
to estimation targets (total 930 ARPs). The time-domain FVN
ˆH requires 2166 ﬂoating point operations per sample (FLOPs).

B. Differentiable Advanced Filtered Velvet Noise

Since frequency-dependent decay of reverberation is gradual,
one can model each coloration ﬁlter Ci as an initial coloration
ﬁlter C1 cascaded with delta-coloration ﬁlters C∆2, · · · , C∆i.

Ci(z) = C1(z)

i
(cid:89)

j=2

C∆j(z).

(17)

FVN with this modiﬁcation is called Advanced Filtered Velvet
Noise (AFVN) [13]. The delta ﬁlters’ orders ∆K2, · · · , ∆KS
are set lower than the initial ﬁlter’s K1 for the efﬁciency. See
Figure 5b for its time-domain implementation. We used K1 = 8
and K∆ = K∆2 = · · · = K∆S = 2 in the experiments. This
results in a total of ¯K = K1 + (S − 1)K∆ = 46 SVFs. The
other settings are the same as the FVN. This AFVN has 360
ARPs to estimate and its time-domain model ˆH requires 1511
FLOPs. While order of the coloration ﬁlters are higher than the
FVN’s counterparts, the introduced frequency-sampling error
remains approximately the same in practice (see Figure 4b).

C. Differentiable Delay Network

1) Delay Network: By interconnecting multiple delay lines
in a recursive manner, one can obtain a Delay Network (DN)
[22], [23] structure. A difference equation of the DN can be

Fig. 6. Our DN in the time domain. We restrict the general ﬁlter matrices
to accelerate the training and obtain an efﬁcient and controllable model. We
simplify the pre-ﬁlters B and the post-ﬁlters C to have only one coloration
ﬁlter C1 and use the same C∆ for all feedback loops. A time-varying mixing
matrix Qn and allpass ﬁlters U are introduced to use DN with small M (i.e.,
fast training) with minimal quality loss.

written in a general form as follows (we omit the bracket
notation for the ﬁltering),

y[n] = CT ¯y[n] + ˆH0x[n],

¯y[n + d] = A¯y[n] + Bx[n].

(18a)

(18b)

That is, an input x[n] is distributed and ﬁltered (or simply
scaled) with B, then go through d-sample parallel delay lines
which are recursively interconnected to themselves through a
mixing ﬁlter matrix A. The delay line outputs ¯y[n] are ﬁltered
and summed with C. We add a bypass signal ﬁltered with ˆH0,
resulting in an output y[n]. Transfer function of the DN is
ˆH(z) = C(z)T (D−1(z) − A(z))−1B(z) + ˆH0(z).

(19)

Here, D(z) = diag(z−d) is an M ×M transfer function matrix
for the delay lines, i.e., Dii(z) = z−di. B(z), C(z), and A(z)
are input, output, and feedback transfer function matrices of
shape M × 1, 1 × M , and M × M , respectively.

2) Differentiable Implementation: The DN components are
recursively interconnected such that one cannot divide its IR
into independent segments and generate them in parallel like
we did with the FVN and AFVN. Instead, we frequency-sample
its entire transfer function to obtain differentiable DN, which is
equivalent to a composition of individually frequency-sampled
transfer function matrices.
N (D−1

N − AN )−1BN + ( ˆH0)N

˜h[n] = IFFT

(20)

CT

(cid:110)

(cid:111)

.

Here, DN = diag((z−d)N ) ∈ CM ×M ×(cid:98)N/2+1(cid:99) is a frequency-
sampled version of the delay line transfer function matrix
D, i.e., (DN )ii = (z−di)N ∈ C(cid:98)N/2+1(cid:99). Likewise, we can
frequency-sample the transfer function matrices B, C, and A
to obtain their approximations BN , CN , and AN , respectively.
Each frequency-sampled transfer function matrix is a batch of
(cid:98)N/2 + 1(cid:99) matrices, the matrix multiplications and inversions
of equation (20) can be performed in parallel.

<04?!["];!<01!<01"+,%;,)!/(/(),3!3,=2⋮⋮⋮⋮⋮⋮⋮$"=+,23-!"/!>$!">$,"@ALEE et al.: DIFFERENTIABLE ARTIFICIAL REVERBERATION

7

3) Restrictions to the General Model: The presented DN
structure is fully general and able to express many variants,
including the Feedback Delay Network (FDN) [46]. Therefore,
we can also derive their differentiable versions following
the proposed method. However, when following the general
practice that uses many delay lines (e.g., M = 16) for a
high-quality reverberation, its differentiable model’s frequency-
sampled transfer function matrices consume too much memory,
and their multiplications and inversions bottleneck the training
speed. To tackle this, we reduce M and modify the components
as follows to make DN plausible with low M (see Figure 6).
• Pre and Post Filter Matrix. Following most previous works
[11], [47], [48], the pre ﬁlter matrix B is set to a (constant)
gain vector b. The post ﬁlter matrix C is a combination
of a gain vector c and a common ﬁlter C1, i.e., C = C1c.
The common ﬁlter C1 is composed of serial KC1 SVFs.
• Feedback Filter Matrix. While most DN model combines
channel-wise parallel delta-coloration (absorption) ﬁlters
C∆ and an inter-channel mixing matrix Q to compose the
feedback ﬁlter matrix A = QC∆. Additionally, we insert
channel-wise allpass ﬁlters U and introduce time-variance
by modulating the mixing matrix Q. This results in the
time-varying feedback ﬁlter matrix An = QnUC∆.
• Allpass Filters. To achieve faster echo density build-up
without adding more delay lines, we insert serial KU
SAPs in each feedback path. Unlike the FVN and AFVN
cases, we estimate the SAP gains γ ∈ RM ×KU with the
estimation network and frequency-sample the allpass ﬁlter
matrix U for the differentiable model.

• Time-varying Mixing Matrix. We further reduce the audible
“ringing” artifacts by modulating the stationary poles with
the time-varying mixing matrix, which is set to Qn =
Q0Rn where Q0 is a Householder matrix and R is a
tiny rotational matrix constructed from a random matrix
[48], [49]. Since the frequency-sampling method is only
applicable to LTI ﬁlters, we ﬁx the mixing matrix to Q0
when obtaining the differentiable model.

• Absorption Filters. Unlike the conventional FDN [46], we
use the same absorption ﬁlter C∆ for every channel. Since
the mixing matrix Qn is always unitary, our DN becomes
stable when C∆ has magnitude response less than 1 [48].
We achieve this by using a PEQ with additional constraints
(see Section V-C) as C∆.

• Bypass FIR. We use a length-Z FIR h0[n] for the bypass.
We summarize the modiﬁcations. The difference equation

of our time-varying DN ˆH TV

n is

y[n] = C1cT ¯y[n] + H0x[n],
¯y[n + d] = QnUC∆ ¯y[n] + bx[n].

(21a)

(21b)

The transfer function of its LTI approximation ˆH(z) is given as
equation (19) with the restrictions B(z) = b, C(z) = C1(z)c,
and A(z) = Q0 diag(UN (z) (cid:12) C∆(z)). Hence, the IFFT of

(C1)N cT (D−1

N − Q0 diag(UN (cid:12) (C∆)N ))−1b

(22)
summed with the bypass FIR ˆh0[n] results in our differentiable
DN’s IR ˜h[n].

Figure 7 shows the EDR errors between the three different

Fig. 7. EDR of differentiable DN, its original DN with two versions (LTI
and time-varying), and EDR errors between those three. Same as the FVN
and AFVN, the frequency-sampling introduce little error, as shown in the
upper right plot. However, rotating the mixing matrix introduces a considerable
amount of EDR error since it modulates the pole positions. The error is mostly
in the low-frequency region, where the poles are sparsely located.

DN modes, i.e., differentiable DN, original LTI, and the time-
varying one. The only difference between the ﬁrst two is the
frequency-sampling; unless the DN decays very slowly, there
is little time-aliasing and EDR error. However, due to the pole
modulation, the EDR error increases signiﬁcantly when the
time-varying mixing matrix is applied. Despite this, we ﬁnd
that other perceptually important factors, such as reverberation
time, remain largely unchanged (see Section VI-A).

4) Model Conﬁguration Details: We used M = 6 delay
lines with delay lengths d = [233, 311, 421, 461, 587, 613]
in samples. We used KU = 4 SAPs for each delay line. We
ﬁxed the SAP delay lengths to τ = [[131, 151, 337, 353],
[103, 173, 331, 373], [89, 181, 307, 401], [79, 197, 281, 419],
[61, 211, 257, 431], [47, 229, 251, 443]]. Note that the SAPs
introduce additional delays so that the effective delay lengths
are [1205, 1291, 1399, 1437, 1547, 1583] or about [25.1, 26.9,
29.1, 29.9, 32.2, 33.0] in milliseconds. Our rotational matrix
satisﬁes R30k = I so that the mixing matrix Qn has a period
of 0.625 second. Both post C1 and absorption ﬁlter C∆ have
KC1 = Kc∆ = 8 components. We used N = 120k frequency-
sampling points. The order of the bypass FIR is Z = 100. The
∈ R1×KC1 ,
post ﬁlter parameters fC1 , RC1 , mLP
C1
feedback ﬁlter parameters fC∆ , RC∆ , GC∆ ∈ RKC∆ , pre
and post gain vectors b, c ∈ RM ×1, SAP gain vector γ ∈
RM ×KU , and the bypass FIR h0 ∈ RZ are the estimation
targets (total 200 ARPs). The resulting time-domain model
ˆH and the time-varying model ˆH TV
n consume 889 and 1285
FLOPs, respectively.

, mHP
C1

, mBP
C1

V. ARTIFICIAL REVERBERATION PARAMETER ESTIMATION
WITH A DEEP NEURAL NETWORK

The details of our ARP estimation network are as follows.
As shown in Figure 8, it transforms single channel audio input
(either RIR or reverberant speech) into a shared latent z with
the model/task-agnostic encoder. Then, each ARP-groupwise
layer projects the latent z into an ARP tensor Pi.

Frequency (Hz)601255001k4k16kTime (s)00.511.522.5   0   -20   -40   -60 Differentiable DNFrequency (Hz)601255001k4k16kTime (s)00.511.522.5   0   -20   -40   -60 DNFrequency (Hz)601255001k4k16kTime (s)00.511.522.5Error (dB)   -12   -6   0   6   12Frequency (Hz)601255001k4k16kTime (s)00.511.522.5   0   -20   -40   -60 DN (Time-Varying)Frequency (Hz)601255001k4k16kTime (s)00.511.522.5   -12   -6   0   6   12Frequency (Hz)601255001k4k16kTime (s)00.511.522.5Error (dB)   -12   -6   0   6   12LEE et al.: DIFFERENTIABLE ARTIFICIAL REVERBERATION

8

A. AR-Model/Task-Agnostic Encoder

We ﬁrst transform the reference RIR or reverberant speech
into a log-frequency log-magnitude spectrogram. Then, the
spectrogram goes through ﬁve two-dimensional convolutional
layers, each followed by a rectiﬁed linear unit (ReLU) activa-
tion. We apply gated recurrent unit (GRU) layers [50] along
with the frequency axis regarding the channels as features
and the time axis regarding the channels and frequencies as
features, widening the receptive ﬁeld. Finally, we apply two
linear layers followed by layer normalization [51] and ReLU,
resulting in the two-dimensional shared latent z. The detailed
network conﬁgurations are shown in Figure 8, and the encoder
has about 7.3M parameters.

B. ARP-Groupwise Projection Layers

Now we aim to transform the two-dimensional shared latent
z into each ARP tensor Pi which is at most two-dimensional.
To achieve this, we use two axis-by-axis linear layers. We ﬁrst
transpose and apply a linear layer to the latent to match the
ﬁrst axis shape. Then, another transposition and a linear layer
match the other axis. This approach factorizes a single large
layer into two smaller ones, reducing the number of parameters.

C. Nonlinear Activation Functions and Bias Initialization

Each ARP group has different stability conditions and desired
distribution. To satisfy these, we attach a nonlinear activation
to each projection layer and initialize the last linear layer’s
bias as follows. Here, x denotes a pre-activation element.

1) Nonlinear Activation Functions:
• For the resonance R of the SVF, we use a scaled softplus
ζ(x) = log(1+ex)/ log(2) to center the initial distribution
at 1 and satisfy the stability condition R > 0.

• For cutoff f we also have the same stability condition f >
0. However, instead of the softplus, we use tan(πσ(x)/2)
where σ(x) = 1/(1 + e−x) is a logistic sigmoid. With
this activation, σ(x) = 0 and 1 represent cutoff frequency
of 0Hz and half of the sampling rate, respectively.

• The delta-coloration PEQ C∆ inside the DN must satisfy
the stability condition |C∆(ejω)| < 1. To achieve this, we
use 10−ζ(x) for the component gain G. Also, the shelving
ﬁlters’ resonances should satisfy R >
2/2 to prevent
their magnitude responses to “spike” over 1. Therefore,
we add

2/2 after the softplus activation.
• For the feed-forward/back gains γ of channel-wise allpass
ﬁlters U inside the DN, we use sigmoid activation σ(x).

√

√

2) Bias Initialization:
• We initialize bias of the f projection layer to σ−1(2ωk/ωs)
where each ωk is desired initial cutoff frequency of kth
SVF. We set ωk = ωmin(ωmax/ωmin)(k−1)/(K−1) such that
the frequencies are equally spaced in the logarithmic scale
from ωmin = 40Hz to ωmax = 12kHz. To the SVFs inside
the AFVN’s initial and delta-coloration ﬁlters, we perform
a random permutation to the frequency index k and obtain
a new index ¯k. This prevents SVFs for the latter segments
having higher initial cutoff frequencies.

Fig. 8. Architecture of the ARP estimation network. For each two-dimensional
convolutional (Conv2D) layer, c, k, and s denote number of output channel,
kernel size, and stride, respectively. For each gated reccurent unit (GRU) layer,
n and h denote number of layers and hidden features. Shape and size of the
intermediate output tensors are provided in the parentheses, where F , T , and
C denote frequency, time, and channel axis, respectively. Z1, and Z2 denote
each ARP tensor’s ﬁrst and second axis. We omitted the batch axis.

• Decoder biases for the SVF mixing coefﬁcients mLP, mBP,
and mHP are initialized to 1, 2, and 1, respectively, so that
each SVF’s initial magnitude response slightly deviates
from 1. This prevents the coloration ﬁlters’ responses and
the estimation network loss gradients to vanish or explode.
• For the PEQ gains G of the DN’s delta-coloration ﬁlters,
we set each of their biases to −10 such that the initially
generated IRs have long enough reverberation time.

D. Loss Function

1) Match Loss: We utilize a multi-scale spectral loss [29]

and deﬁne a match loss as follows,
LMATCH(h, ˜h) =

(cid:88)

(cid:13)
(cid:13)|H STFT
(cid:13)

i

i

| − | ˜H STFT
i

|

(cid:13)
(cid:13)
(cid:13)1

.

(23)

i denotes STFT with different FFT sizes. We use the FFT sizes
of 256, 512, 1024, 2048, and 4096, hop sizes of 25% of the
respective FFT sizes, and Hann windows. Each spectrogram’s
frequency axis is log-scaled like the encoder’s spectrogram.

2) Regularization: We additionally apply a regularization to
reduce time-aliasing of the estimation. As shown in equation
(6) and (8), pole radii of each SVF affects the reliability of the
frequency-sampled one (hence DAR model) and its parameter

LEE et al.: DIFFERENTIABLE ARTIFICIAL REVERBERATION

9

estimator. Since each SVF’s IR (cSVF
i,k )N [n] can be computed,
we encourage reducing its pole radii by penalizing the IR’s
decreasing speed. Each decreasing speed γi,k is calculated by
the ratio of the average amplitude of ﬁrst and last n0 samples
of (cSVF
i,k )N [n]. We ﬁx n0 to N/8. Then, each γi,k is weighted
with a softmax function along the SVF-axis to penalize higher
γi,k more. Sum of the weighted decreasing speed values results
in the regularization loss LREG as follows,

(cid:80)N

γi,k =

LREG =

S
(cid:88)

i=1

i,k )N [n]|

|(cSVF
i,k )N [n]|

n=N −n0
(cid:80)n0

n=0 |(cSVF
(cid:80)K

k=1 γi,keγi,k
(cid:80)K
j=1 eγi,j

,

(24a)

.

(24b)

We omit the regularization loss for the DN networks since
with DN the time-aliasing is inevitable to match the reference
with reverberation time longer than the number of frequency-
sampling points N . Therefore, our full loss function L is
L(h, ˜h) = LMATCH(h, ˜h) + βLREG(cSVF
N )

(25)

where β = 1 for the FVN and AFVN and β = 0 for the DN.

VI. EXPERIMENTAL SETUP

A. Data

1) Room Impulse Response: We collected 1835 real-world
RIR measurements from various datasets including OpenAIR
[52] and ACE Challenge [53]. The amount of the RIRs were
insufﬁcient for the training purpose, so we used them only
for the validation and test (836 and 999 RIRs, respectively),
ensuring that each set consists of RIRs from different rooms.
For the training, we synthesized 200k RIRs with shoebox
room simulations using the image-source method [54], [55].
To reﬂect various acoustic environments, we randomized simu-
lation parameters for every RIR synthesis, which include room
size, each wall’s frequency-dependent absorption coefﬁcient,
and source/microphone positions. We tuned the randomization
scheme to match the training dataset to the validation set in
terms of reverberation parameter statistics.

In addition, we pre-processed each RIR as following.
• We removed the pre-onset part of the RIR. We detected
the onset by extracting a local energy envelope of the RIR
then ﬁnding its maximum point [56].

• Following the recent RIR augmentation method [57], we
multiplied random gain sampled from U(−12dB, 3dB) to
the ﬁrst 5ms of each train RIR. This slightly reduces the
average direct-to-reverberant ratio (DRR) of the training
set and matches the validation set. At the evaluation/test,
we omitted this procedure.

• Finally, we normalized the RIR to have the energy of 1.
2) Reverberant Speech: We used VCTK [58] for dry speech.
We split it into the train (61808), validation (21608), and test
(4912) set so that each set is composed of the speech recordings
from different speakers. We sampled an RIR and a dry speech
sample from their respective datasets and convolved them to
generate reverberant speech. We random-cropped 2.5-second
segment from it for the input.

B. Training

We trained each network with every proposed DAR model
for the three tasks: analysis-synthesis, blind estimation, and
both tasks. For the both-performing ones, we fed an RIR or
reverberant speech in a 50-50% probability.

We set the initial learning rate to 10−4 for FVN and AFVN
and 10−5 for DN. We used Adam optimizer [59]. For DN,
gradients were clipped to ±10 for stable training. After 250k
steps, we performed the learning rate decay. We multiplied
the learning rate by 10−0.2 and 10−0.1 every 50k step for the
analysis-synthesis networks and the others, respectively. We
ﬁnished the training if the validation loss did not improve
after 50k steps, which took no more than 500k steps for the
analysis-synthesis and 1M steps for the others.

C. Evaluation Metrics

We evaluated our networks with the match loss LMATCH
and EDR distance, deﬁned as an average of the absolute
EDR error. Furthermore, we evaluated reverberation parameter
differences, i.e., reverberation time T30, DRR, and clarity C50
difference, denoted as ∆T30, ∆DRR, and ∆C50, respectively
[32], [60]. We calculated both full-band differences and average
of differences measured at octave bands of center frequencies
of 125, 250, 500, 1k, 2k, 4k and 8kHz. Then, we obtained
their respective median values.

We can interpret the reverberation parameter differences by
comparing them with their respective just noticeable differences
[9]. Reported just noticeable differences of T30 from previous
works vary from 5% [61] up to about 25% [62]. For C50, 1dB
[63] and 1.1dB [64] were reported. For DRR it differs for
various range, e.g., 6dB at −10dB, 2dB at 0dB, 3dB at 10dB,
and 8dB at 20dB [65].

D. Subjective Listening Test

Following the previous researches [10], [41], we conducted a
modiﬁed MUltiple Stimuli with Hidden Reference and Anchor
(MUSHRA) test [66]. We asked subjects to score similarities
of reverberation between a given reference reverberant speech
h ∗ x and followings (h and x denote a test RIR and dry speech
signal, respectively).

• A hidden reference h ∗ x (exactly the same audio).
• A lower anchor h(cid:48) ∗ x obtained by averaging the test set

RIRs [41].

• Another anchor h(cid:48)(cid:48) ∗ x obtained by random-sampling an

RIR from the test set [10].

• Estimations to evaluate ˆH1(x), · · · , ˆHn(x). We used the
AR models ˆH1, · · · , ˆHn to obtain the reverberant speech.
The similarity score ranged from 0 to 100, where we provided
text descriptions for 5 equally-divided ranges (0-20: totally
different, 20-40: considerably different, 40-60: slightly different,
60-80: noticeable, but not much, and 80-100: imperceptible).
While we asked the participants to score both analysis-synthesis
and blind estimation results, we divided them into two separate
pages. As a result, a total of 10 stimulus (3 proposed models, 4
baselines which we will explain in Section VI-E, 2 anchors, and
1 hidden reference) were scored for each task. We conducted

LEE et al.: DIFFERENTIABLE ARTIFICIAL REVERBERATION

10

(a) ARP estimation framework without the DAR models.

(b) RIR estimation with CNN decoder model.

Fig. 9. Baseline methods for the experiments.

the test using the webMUSHRA API [67]. A total of 24 sets
were scored for each task, where the ﬁrst 3 sets were used
for training. A total of 12 people participated where 7 people
were audio engineers and the others were instrument players.
We discarded 1 subject’s results since he scored the hidden
references lower than 90 for more than 15% of the entire sets.

E. Baselines

We compared our framework with following baselines. Refer

to Appendix B for further details.

1) Parameter-matching Networks: Assuming that the DAR
models are not available, similar to the previous non-end-to-end
approach [10], we generated the training data (reference-and-
ARPs pair) with the DAR models and trained the same networks
with a ARP-matching loss (see Figure 9a). Note that we used
the DAR models simply for the convenience of on-the-ﬂy data
generation in GPU, but the same procedure can be performed
with the AR models.

2) CNN Decoder: We trained the same estimation network
but instead of the DAR model we attached a decoder composed
of one-dimensional transposed convolutional layers and TCNs.
With this decoder, the entire network resembles an autoencoder
(see Figure 9b). From now, we denote this model as CNN. We
trained two CNNs, one for each of the two tasks.

Note that we did not compare the our methods with possible
two-stage solutions [32]–[39] (see Section II-B3) since each
DAR model’s architecture became different from its original.

VII. EVALUATION RESULTS

A. Comparison with the Baseline Models

1) Advantage of the End-to-end Learning: Figure 10 com-
pares the proposed end-to-end approach with the baselines on
objective metrics and subjective scores. By a large margin,
the end-to-end model outperformed the non-end-to-end ARP-
matching baselines. The baselines struggled to match the rever-
beration decay, reporting noticeably large reverberation time
differences ∆T30 ≥ 20%. A two-sided t-test for each AR/DAR
model and task showed that the performance difference was
statistically signiﬁcant (all p < 10−5). Subjective listening test
results also agreed with the objective metrics; median scores of
the end-to-end approaches were from 70 to 80, showing close
matches to the reference, while the parameter matching models
scored less than 60. Also, refer to Figure 13 that visualizes
each estimation result with an EDR and EDR error plots. This

Fig. 10. ARP estimation results on various tasks with the AR/DAR models.
Here, AS and BE denote analysis-synthesis and blind estimation, respectively.
The Full and Freq denote the full-band reverberation parameter difference
and average of octave-band reverberation parameter differences, respectively.
Along with the proposed approaches and baselines, we additionally report the
lower anchor’s evaluation results and the (hidden) reference’s MUSHRA score
result. The both-performing networks’ performance are denoted as small dots
in the ﬁgure.

shows that the end-to-end learning enabled by the DAR models
gives a huge performance gain relative to the non-end-to-end
parameter-matching scheme.

2) Advantage of the DSP Prior: Since the CNN baselines
have more trainable parameters and complex architecture, they
reach lower losses than the DAR-equipped networks. However,
they produce audible artifacts. While being more restrictive, the
AR models avoid such artifacts and achieve higher subjective
listening scores than the CNN baselines. Again, these subjective
score differences were statistically signiﬁcant (all p < 10−5).

0.10.2Match LossASBEASBEASBEASBEFVNAFVNFDNCNNEnd-to-EndParam. MatchCNN DecoderAverage RIRRandom RIRReference05EDR Difference125102040T30 (Full)25102040T30 (Freq)1248DRR (Full)248DRR (Freq)1248C50 (Full)1248C50 (Freq)020406080MUSHRA ScoreLEE et al.: DIFFERENTIABLE ARTIFICIAL REVERBERATION

11

Fig. 12. Magnitude responses of the coloration ﬁlters of the AR models. We
used the proposed analysis-synthesis networks to obtain the ﬁlter parameters
for this plot. Each AR model has a different approach to model the frequency-
dependent characteristic of given reverberation.

reverberation parameter difference only slightly increased (less
than the just noticeable differences), which hint at a possibility
of a universal reference-form-agnostic ARP estimation network.

D. AR Model Efﬁciency and Estimation Performance

Each AR model approximates reference reverberation dif-
ferently, and consequently, its computational efﬁciency and its
estimation networks’ performance varies.

1) Filtered Velvet Noise: FVN ﬁlters each source segment
independently (see Figure 12). Such ﬂexibility enables its
parameter estimators to perform better than the other AR
models’ counterparts experimented in this paper. However,
at the same time, it also has the largest number of ARPs (930),
which makes it cumbersome to control manually. Also, it could
be computationally expensive to use multiple FVN instances
in real-time (2546 FLOPs for each).

2) Advanced Filtered Velvet Noise: Instead, AVFN optimizes
each coloration ﬁlter by decomposing it
into the initial
coloration and delta-coloration ﬁlters, making the model more
controllable and efﬁcient (360 ARPs and 1511 FLOPs) than the
FVN. Such simpliﬁcation costs estimation performance since
the underlying assumption of the optimization that coloration
of real-world RIRs changes gradually does not necessarily hold.
In addition, the AFVN networks could be more challenging
to optimize than the FVN’s since each segment’s coloration
depends on the previous ones’. Indeed, we empirically observed
that their training losses decreased slower than the FVN’s.

3) Delay Network: In DN, we simpliﬁed the ﬁlter structure
even further by using a single absorption ﬁlter for each feedback
loop. Similar to the AVFN case, DN trade-offs its estimators’
performance with its controllability and efﬁciency (200 ARPs
and 1285 FLOPs). Again, the estimation performance degra-
dation comes from the DN’s restricted expressibility (forced
exponential decay) and training difﬁculty.

Fig. 11. Reliability of the DAR models. Diff, LTI, and LTV denote the
differentiable, original LTI, and the time-varying model, respectively. The
hatched results are the same values of Figure 10 (analysis-synthesis).

B. Reliability of the DAR Models

We demonstrated in previous sections that each DAR model
is close to the original AR model, making them reliable to use
as an alternative for the training. Here, we empirically validate
this argument again. Figure 11 summarizes the objective metrics
calculated with the DAR and AR models. We used the trained
analysis-synthesis network for this evaluation. First, metrics
calculated between the AR model IR and the DAR model IR
were orders of magnitude smaller than the metrics calculated
with the ground truth. DN was the only exception, showing
more deviation than the other AR models due to the time
variance. Nevertheless, the reverberation parameter differences
between the DN IR and differentiable model IR were less than
the just noticeable difference values (for example, ∆T30 < 3%).
Second, as expected with equation (7), each metric difference
was smaller than the corresponding AR-to-DAR conversion
error. As a result, overall evaluation results were very similar
regardless of the used model. In short, the DAR models are
reliable for training in practice.

C. Performance Difference Between Target Tasks

Without surprise, the analysis-synthesis networks performed
better in most metrics than the blind estimation networks since
the reference reverberation is directly given, i.e., the former
ones have fewer burdens than the latter. Likewise, the both-
performing networks (shown as dots in Figure 10) show slightly
degraded results than their single-task counterparts. Yet, the

101102103104Match LossFVNAFVNDNBetween AR/DARvs. Groud-TruthMetric Differences101100EDR Difference101100101T30 (Freq)100101102103104DRR (Freq)Diff vs. LTIDiff LossLTI LossDiff Loss vs. LTI LossDiff vs. LTIDiff LossLTI LossDiff Loss vs. LTI LossDiff vs. LTILTI vs. LTVDiff vs. LTVDiff LossLTI LossLTV LossDiff Loss vs. LTI LossLTI Loss vs. LTV LossDiff Loss vs. LTV Loss100101102103104C50 (Freq)5025025FVNMagnitude (dB)|C1(ej)||C6(ej)||C11(ej)||C16(ej)|5025025AFVNMagnitude (dB)|C1(ej)||C6(ej)|Delta SVFsFull SVFsPreviousDeltaFull|C11(ej)||C16(ej)|2501k4k16kFrequency (Hz)5025025DNMagnitude (dB)|C1(ej)|2501k4k16kFrequency (Hz)|C(ej)|LEE et al.: DIFFERENTIABLE ARTIFICIAL REVERBERATION

12

Fig. 13. EDR plots of the reference RIRs and their estimations with the trained networks. For each reference, a total of twelve networks’ estimations and their
errors are visualized where each network is trained for one of the three AR models (FVN, AFVN, and DN), one of the two tasks (analysis-synthesis and blind
estimation), and one of the two training approaches (end-to-end learning with the proposed DAR models and the baseline non-end-to-end parameter-matching
learning). The number provided above each plot is the EDR distance of the estimation.

Frequency (Hz)601255001k4k16kTime (s)00.511.522.51.6781.4942.9791.8712.9392.080Magnitude (dB)   0   -20   -40   -601.259Error (dB)   -36   -24   -12   0   12   24   363.2532.0643.6821.9462.4873.113Magnitude (dB)   0   -20   -40   -603.082Error (dB)   -36   -24   -12   0   12   24   36Frequency (Hz)601255001k4k16kTime (s)00.511.522.51.9855.3101.8847.8983.1915.198Magnitude (dB)   0   -20   -40   -601.568Error (dB)   -36   -24   -12   0   12   24   363.3396.0113.5215.5044.0238.095Magnitude (dB)   0   -20   -40   -603.189Error (dB)   -36   -24   -12   0   12   24   36Frequency (Hz)601255001k4k16kTime (s)00.511.522.51.4084.6831.7067.7671.8585.725Magnitude (dB)   0   -20   -40   -601.402Error (dB)   -36   -24   -12   0   12   24   362.5604.4612.4463.6402.5157.189Magnitude (dB)   0   -20   -40   -602.609Error (dB)   -36   -24   -12   0   12   24   36ReferenceFiltered Velvet NoiseAdvanced Filtered Velvet NoiseDelay NetworkCNN DecoderEnd-to-endParam. MatchEnd-to-endParam. MatchEnd-to-endParam. MatchAnalysis-SynthesisBlind EstimationAnalysis-SynthesisBlind EstimationAnalysis-SynthesisBlind EstimationLEE et al.: DIFFERENTIABLE ARTIFICIAL REVERBERATION

13

VIII. CONCLUSION

We proposed differentiable artiﬁcial reverberation (DAR)
models that can be integrated with deep neural networks
(DNNs). Among numerous pre-existing artiﬁcial reverberation
(AR) models, we selectively implemented Filtered Velvet
Noise (FVN), Advanced Filtered Velvet Noise (AFVN), and
Delay Network (DN) differentiably. Nonetheless, the proposed
method, which replaces the inﬁnite impulse response (IIR)
components with ﬁnite impulse response (FIR) approximations
via frequency-sampling, is applicable to any other AR model.
Then, using the DAR models, we trained the proposed artiﬁcial
reverberation parameter (ARP) estimation networks end-to-end.
The evaluation results showed that our networks captured the
target reverberation accurately in both analysis-synthesis and
blind estimation tasks. We showed, in particular, that the end-to-
end training signiﬁcantly improves the estimation performance
at the tolerable cost of the approximation errors caused by the
frequency-sampling. Additionally, we demonstrated that the
structural priors of the AR models avoid perceptual artifacts
that a DNN as a room impulse response (RIR) generator might
produce. Thus, our framework successfully combined powerful
and fully general deep learning techniques with well-established
domain knowledge of reverberation.

Here, we outline the remaining challenges and future work.
First, we used the shoe-box simulation to obtain a large amount
of training data, which is slightly different from the real-world
RIRs. Interestingly, energy decay relief (EDR) errors of the
end-to-end models showed similar patterns regardless of the
equipped AR/DAR model (see Figure 13). This suggests that
the data characteristics discrepancy might be the cause of the
performance degradation. Therefore, collecting more real-world
RIRs and using powerful augmentation methods could replace
the simulation-based data and improve the performance. Second,
we only used the RIRs and the reverberant speech signals as
references. However, other possible applications with different
reference types exist (for example, automatic mixing of musical
signals). Finally, we investigated how each AR model can be
compared in terms of its expressive power and difﬁculty of
parameter optimization under the deep learning environment.
The evaluation results revealed a trade-off relationship; when a
more compact AR model is used, the estimation performance
was upper-bounded by its limited expressive power. Also, the
proposed modiﬁcations to the original AR models improved
estimation performance and training speed. All of these indicate
that other than the presented AR models and estimation
networks, there may be more “deep-learning-friendly” models
and “AR-model-friendly” DNNs that yield better performance-
efﬁciency trade-offs, and seeking those is left as future work.

APPENDIX A
ABLATIONS AND COMPARISONS OF THE AR/DAR MODELS

TABLE I
ABLATION ON THE AR/DAR MODEL CONFIGURATIONS.

MODEL

LMATCH ∆T30 (%) ∆DRR (dB) ∆C50 (dB)
(×10−1) FULL FREQ FULL FREQ FULL FREQ

FVN
Ti = 10
∆li = L/S
−h0[n]
M = 1

AFVN
−C1
−∆Cj
−C1, ∆Cj

DN
−C1
−C∆
−C1, C∆
−Qn
−U
−U, Qn

1.236
1.257
1.255
1.250
1.261

1.277
1.446
1.324
1.695

1.334
1.446
1.351
1.729
1.307
1.535
1.251

1.89
2.74
2.56
2.23
2.53

4.77
6.75
7.12
8.91

4.20
6.14
5.43
10.71
4.42

0.94 3.39 0.60 1.95
4.21 0.48 2.25
1.56
2.29
1.48
0.89
3.68
9.88
3.17 11.94 0.81
2.21
0.82
3.65
1.51

9.96
12.60 1.46 8.43
5.01
2.99
32.16
6.62
2.15
28.21

1.59 3.64 0.61 2.06
3.64
2.62
5.89

1.76
0.92
2.64

2.30
1.71
8.47
11.59
3.03
3.69
10.47 20.75
2.38
2.22
31.22
7.83
4.54
4.00
11.62 31.20
2.39
5.93
1.81
12.48
21.10 10.76 1.03
2.28
7.43 10.79 1.00 3.39 0.70 2.09

1.08
2.31
0.94
2.03
1.08
0.88

3.83
4.55
4.71
6.68
3.93
3.44

TABLE II
DIFFERENTIABLE FVN WITH VARIOUS COLORATION FILTERS AND
PARAMETERIZATION APPROACHES.

FILTER

LMATCH ∆T30 (%) ∆DRR (dB) ∆C50 (dB)
(×10−1) FULL FREQ FULL FREQ FULL FREQ

SSVF
PSVF
PEQ
SBIQ
PBIQ
FIR
LFIR

1.236
1.235
1.254
1.322
1.337
1.370
1.384

1.89 4.20 0.94 3.39 0.60
4.52
0.96
1.95
5.47
0.97
2.50
3.83
1.55
9.44
4.88 11.77 1.57
2.72 10.10 1.80
3.54 11.84 1.64

1.95
0.53 1.86
3.64
1.89
0.62
3.61
2.38
0.59
4.61
4.01
2.19
0.62
3.73 0.51 2.16
2.65
0.62
4.31

Table I summarizes the results. FVN, AFVN, and DN denote
the proposed models. Ti = 10, ∆li = L/S, −h0[n], and
M = 1 denote the proposed FVN model without non-uniform
pulse distance and allpass ﬁlters, non-uniform segmentation, a
deterministic FIR, and ﬁner segment gains. Their results show
that the proposed modiﬁcations contribute to better performance.
We omitted the same ablations for the AFVN. Instead, we
discarded the initial ﬁlter C1, delta ﬁlter ∆Cj, or both. We
also evaluated the DN without the initial ﬁlter C1 or delta
ﬁlter C∆. The results reveal the importance of modelling the
frequency-dependent nature of reverberation.

For DN, −Qn, −U, and −U, Qn denote DN without the
time-varying mixing matrix Qn, the allpass ﬁlters U, and both,
respectively. While −U, Qn reports best evaluation metrics, its
low echo density produced unrealistic sound. Using both U and
Qn improved plausibility of reverberation with an acceptable
performance degradation. Refer to the provided audio samples.

A. AR/DAR Model Ablations

B. Comparisons on Different Coloration Filters

We conducted ablation studies to verify that the modiﬁcations
to the original AR models improve the estimation performance.
We evaluated the modiﬁed models with the analysis-synthesis
task since their performance differences remained the same
regardless of the task in our initial experiments.

We compared different IIR/FIR ﬁlters and parameterization
approaches. We evaluated them with the analysis-synthesis task
using FVN. Table II summarizes the results. SSVF, PSVF, PEQ,
SBIQ, PBIQ, FIR, and LFIR denote serial SVF, parallel SVF,
PEQ, serial biquad, parallel biquad, FIR, and linear-phase FIR

LEE et al.: DIFFERENTIABLE ARTIFICIAL REVERBERATION

14

TABLE III
EFFECTS OF FREQUENCY-SAMPLING RESOLUTION AND
REGULARIZATION ON THE MATCH LOSS LMATCH (×10−1).

FREQUENCY-SAMPLING RESOLUTION

MODEL

×1 (FULL)

×0.5

×0.25

×0.125

DAR

AR

DAR AR DAR AR DAR AR

FVNβ=1 1.236 1.236 1.259 1.259 1.252 1.249 1.574 1.570
FVNβ=0 1.231 1.237 1.283 1.282 1.264 1.263 1.280 1.773
− 1.328 − 1.361 − 1.393

1.277 1.307

DN

from [29]. The order of every ﬁlter was set to 40. The activation
functions and bias initialization for each ﬁlter’s decoder are
as follows. PSVF has the same setting as SSVF. For SBIQ
and PBIQ decoders, we used activations from [20], and their
biases are initialized to match that of SSVF and PSVF. For
PEQ, unlike DN, we used 10x for the activation of the gain G.
FIR/LFIR have no activation and custom bias initialization.

In spite of having identical expressive power, SSVF/PSVF
performed better than SBIQ/PBIQ by large margins. PEQ
performed slightly worse than SSVF due to its restricted degree
of freedom. FIR/LFIR also performs worse than SSVF/PSVF
and PEQ because they cannot change their frequency responses
radically as other IIR ﬁlters could.

C. Effects of the Frequency-Sampling Resolution

Table III demonstrates the effects of the frequency-sampling
resolution and the regularization loss LREG. From the table,
×x denotes the relative sampling resolution compared to the
default conﬁguration, e.g., ×0.5 denotes N = 2k for the FVN
and N = 6k for the DN. For each resolution and regularization
setup, we report match loss with the DAR and AR models.
Results of the differentiable DNs with lower resolutions are
omitted since they are calculated with shorter IRs.

As expected, higher sampling resolution led to smaller loss
difference. Also, introducing the regularization term (β = 1) re-
duced the difference further. As a result, the ﬁnal differentiable
FVN model (with full resolution and regularization) showed
little loss difference. Moreover, higher resolution led to better
performance, which might be because increased resolution
helped each network to ﬁnd better ARPs.

APPENDIX B
DETAILS ON THE BASELINE METHODS

A. ARP Match Models

1) Training: We trained the ARP match baseline models as
follows. First, we randomized ARPs P1, · · · , Pn and generated
an IR with using the DAR model. Then, each baseline network
estimated ARPs ˆP1, · · · , ˆPn from the given IR. We evaluated
the estimation and trained the baseline using an ARP-match
loss LARP deﬁned as follows,
(cid:88)

αi

(cid:13)
(cid:13)
(cid:13)fi(Pi) − fi( ˆPi)
(cid:13)
(cid:13)
(cid:13)1

.

LARP(Pi, ˆPi) =

(26)

i

Here, the index i denotes a different ARP tensor, e.g., P0 = g
and P1 = h0. fi(·) and αi are an elementwise function and a

constant, respectively. We set αi = 10 for the all DAR models’
bypass FIRs and the DN’s absorption ﬁlter parameters. We
used fi(x) = log10(x) for the FVN and AFVN segment gains.
We chose fi(x) = x and αi = 1 otherwise.

2) Network Architecture: We trained an almost identical
network to the proposed network for each model/task. The only
difference is that we added an activation 10−ζ(x)/20 to the g
decoders of FVN and AFVN to improve their performance.

√

3) Data Generation: We tuned each DAR model’s ARP ran-
domization scheme to match the synthesized IRs’ reverberation
parameter statistics to the validation set. For the FVN baselines,
we ﬁrst sampled a reverberation time T30 ∼ Ulog[50ms, 8s)
then generated the segment gains g that match the T30. Here,
Ulog[·, ·) denotes a uniform distribution in log scale. In this
procedure, we also compensated the average pulse distance by
dividing each gain gi by
Ti. For the initial coloration ﬁlter C1,
we ﬁrst generated PEQ parameters as ωi ∼ Ulog[40Hz, 16kHz),
Ri ∼ Ulog[0.2, 5), and Gi ∼ U[−18dB, 18dB) (we sorted the
cutoff frequencies after the sampling) and derived their SVF
parameters. After that, we perturbed each parameter’s value
slightly. This ARP generation method was motivated by the
observation that the differentiable SVFs act like a relaxed PEQ
(see Figure 12). We gradually changed the parameters of C1 to
obtain C2, · · · , CK, modeling the frequency-dependent decay.
Finally, we set h0[n] as an uniform noise with a gain sampled
from U[−24dB, 0dB). We followed similar procedures for the
AFVN baselines. One difference is that the ﬁrst delta ﬁlter
C∆2 is randomized and the delta ﬁlters afterwords are slight
deviation of the ﬁrst one. For the DN baselines, we sampled
each absorption PEQ’s gain with Gi ∼ U[−2.8dB, 0dB).

B. DNN Decoder Models

Regarding the ﬁrst and second axis of the shared latent z as
a time and channel axis, respectively, our decoder upsamples
z with seven one-dimensional transposed convolution layers.
Conﬁgurations of these layers are as follows. Channels: 128,
128, 64, 64, 64, 64, then 64. Kernel sizes: 5, 5, 5, 5, 5, 5,
then 3. Strides: 3, 3, 3, 3, 3, 3, then 2. Furthermore, after
each upsampling, we inserted a small temporal convolutional
network with conﬁgurations as follows. Channels: 128, 128, 64,
64, 64, 64, and 64. The number of layers: 3, 3, 2, 2, 2, 2, and
2. Kernel sizes: all 7. Finally, we added 1 × 1 convolution as
the last layer to mix all channels. We used the ReLU activation
between the layers. This decoder has about 2.5M parameters.

APPENDIX C
DETAILS ON THE FREQUENCY-SAMPLING METHOD

A. Proof of Equation 6

With equation (5), the time-aliasing error can be written as

(cid:107)H − HN (cid:107)2

2 =

N −1
(cid:88)

∞
(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
h[mN + n]
(cid:12)
(cid:12)

2

+

∞
(cid:88)

|h[n]|2 . (27)

m=1

n=0
With the triangle inequality and (cid:107)X(cid:107)2
1, we can upper-
bound (cid:107)H − HN (cid:107)2
n=N |h[n]| is an
absolute sum of the tail of the original IR h[n] (n ≥ N ). Next,
we expand h[n] with partial fraction expansion [68], [69]. For

n=N
2 ≤ (cid:107)X(cid:107)2
N where SN = (cid:80)∞

2 with 2S2

LEE et al.: DIFFERENTIABLE ARTIFICIAL REVERBERATION

15

H(z) with M distinct poles where each of them νi ∈ C has
multiplicity ri ∈ N, we can express h[n] as a sum of an FIR
hFIR[n] and IIRs hIIR

i,k[n] where following holds,

H(z) = H FIR(z) +

M
(cid:88)

ri(cid:88)

i=1

k=1

ζi,k
(1 − νiz−1)k
(cid:124)
(cid:125)
(cid:123)(cid:122)
H IIR
i,k(z)

.

(ζi,k ∈ C) (28)

Then, applying the triangle inequality to the summation gives
an upper bound UN , an absolute sum of the IIRs’ tails.

SN ≤

M
(cid:88)

ri(cid:88)

∞
(cid:88)

i=1

k=1

n=N

(cid:12)
(cid:12)hIIR

i,k[n](cid:12)

(cid:12) = UN .

(29)

From above equation, each IIR, ﬁrst sum, and second sum show
O(nk−1|νi|n), O(N k−1|νi|N ), and O(N ri−1|νi|N ) asymp-
totic behavior, respectively [69]. We conclude the proof:

(cid:107)H − HN (cid:107)2 ≤

√

2SN ≤

√

2UN =

M
(cid:88)

i=1

O(N ri−1 |νi|N ).

(30)

B. Proof of Equation 8

We denote H(ejω) and ∂X/∂p with H and X (cid:48). X (cid:48)

N denotes
(XN )(cid:48) = (X (cid:48))N . We upper-bound the gradient difference ∆G
using integral inequalities as

(cid:90) 2π

∆G ≤

|HRef(H (cid:48) − H (cid:48)
(cid:123)(cid:122)

≤2(cid:107)HRef(cid:107)2(cid:107)H−H (cid:48)

N )|

N (cid:107)2

0

(cid:124)

(cid:90) 2π

+

dω
π
(cid:125)

0

(cid:124)

|HH (cid:48) − HN H (cid:48)

N |

(cid:123)(cid:122)

≤2(cid:107)HH (cid:48)−HN H (cid:48)

N (cid:107)2

dω
π
(cid:125)

(31)
where the under-braced ones are the Cauchy-Schwartz inequal-
ities. Considering the ﬁrst integral’s upper bound, (cid:107)HRef(cid:107)2 is a
ﬁnite constant. In (cid:107)H (cid:48) − H (cid:48)
N (cid:107)2, H (cid:48) is another stable LTI ﬁlter
if p is well-deﬁned. If νi is a function of p, its multiplicity ri is
doubled in H (cid:48). In the worst case, p controls every pole of H and
i=1 O(N 2ri−1|νi|N ) holds. Similarly, the
(cid:107)H (cid:48) − H (cid:48)
upper bound of the second integral consists an l2 distance be-
N . Since HH (cid:48) has 2ri or 3ri multiplicity
tween HH (cid:48) and HN H (cid:48)
for each pole νi, (cid:107)HH (cid:48) − HN H (cid:48)
i=1 O(N 3ri−1|νi|N )
holds at the worst case, which concludes the proof.

N (cid:107)2 = (cid:80)M

N (cid:107)2 = (cid:80)M

REFERENCES

[1] J. W. Bayless, “Innovations in studio design and construction in the capitol
tower recording studios,” Journal of the Audio Engineering Society, vol. 5,
no. 2, pp. 71–76, April 1957.

[2] B. A. Blesser, “An interdisciplinary synthesis of reverberation viewpoints,”
Journal of the Audio Engineering Society, vol. 49, no. 10, pp. 867–903,
October 2001.

[3] J. Traer and J. H. McDermott, “Statistics of natural reverberation enable
perceptual separation of sound and space,” Proceedings of the National
Academy of Sciences, vol. 113, no. 48, pp. E7856–E7865, 2016.

[4] V. V¨alim¨aki, J. D. Parker, L. Savioja, J. O. Smith, and J. S. Abel, “Fifty
years of artiﬁcial reverberation,” IEEE Transactions on Audio, Speech,
and Language Processing, vol. 20, no. 5, pp. 1421–1448, 2012.

[5] V. V¨alim¨aki, J. Parker, L. Savioja, J. O. Smith, and J. Abel, “More than
50 years of artiﬁcial reverberation,” Journal of the Audio Engineering
Society, January 2016.

[6] J. Dattorro, “Effect design, part 1: reverberator and other ﬁlters,” Journal
of the Audio Engineering Society, vol. 45, no. 9, pp. 660–684, 1997.
[7] W. G. Gardner, “A real-time multichannel room simulator,” Journal of
the Acoustical Society of America, vol. 92, pp. 2395–2395, 1992.

[8] E. De Sena, H. Hacιhabibo˘glu, Z. Cvetkovi´c, and J. O. Smith, “Efﬁcient
synthesis of room acoustics via scattering delay networks,” IEEE/ACM
Transactions on Audio, Speech, and Language Processing, vol. 23, no. 9,
pp. 1478–1492, 2015.

[9] N. Agus, H. Anderson, J.-M. Chen, S. Lui, and D. Herremans, “Minimally
simple binaural room modeling using a single feedback delay network,”
Journal of the Audio Engineering Society, vol. 66, no. 10, pp. 791–807,
October 2018.

[10] A. Sarroff and R. Michaels, “Blind arbitrary reverb matching,” in
Proceedings of the 23rd International Conference on Digital Audio
Effects, 2020.

[11] J.-M. Jot, “An analysis/synthesis approach to real-time artiﬁcial reverbera-
tion,” in [Proceedings] ICASSP-92: 1992 IEEE International Conference
on Acoustics, Speech, and Signal Processing, vol. 2, 1992, pp. 221–224.
[12] J. Coggin and W. Pirkle, “Automatic design of feedback delay network
reverb parameters for impulse response matching,” Journal of the Audio
Engineering Society, September 2016.

[13] V. V¨alim¨aki, B. Holm-Rasmussen, B. Alary, and H.-M. Lehtonen, “Late
reverberation synthesis using ﬁltered velvet noise,” Applied Sciences,
vol. 7, no. 5, 2017.

[14] N. Peters, J. Choi, and H. Lei, “Matching artiﬁcial reverb settings to
unknown room recordings: a recommendation system for reverb plugins,”
Journal of the Audio Engineering Society, October 2012.

[15] F. Gao and W. Snelgrove, “An adaptive backpropagation cascade IIR
ﬁlter,” IEEE Transactions on Circuits and Systems II: Analog and Digital
Signal Processing, vol. 39, no. 9, pp. 606–610, 1992.

[16] A. D. Back and A. C. Tsoi, “FIR and IIR synapses, a new neural network
architecture for time series modeling,” Neural Computation, vol. 3, no. 3,
pp. 375–385, 1991.

[17] P. Campolucci, A. Uncini, and F. Piazza, “Fast adaptive IIR-MLP neural
networks for signal processing applications,” in 1996 IEEE International
Conference on Acoustics, Speech, and Signal Processing Conference
Proceedings, vol. 6, 1996, pp. 3529–3532 vol. 6.

[18] J. D. P. Boris Kuznetsov and F. Esqueda, “Differentiable IIR ﬁlters for
machine learning application,” in Proceedings of the 23rd International
Conference on Digital Audio Effects, 2020.

[19] S. Nercessian, “Neural parametric equalizer matching using differentiable
biquads,” in Proceedings of the 23rd International Conference on Digital
Audio Effects, 2020.

[20] S. Nercessian, A. Sarroff, and K. J. Werner, “Lightweight and inter-
pretable neural modeling of an audio distortion effect using hypercondi-
tioned differentiable biquads,” arXiv preprint arXiv:2103.08709, 2021.
[21] L. Rabiner, B. Gold, and C. McGonegal, “An approach to the approxi-
mation problem for nonrecursive digital ﬁlters,” IEEE Transactions on
Audio and Electroacoustics, vol. 18, no. 2, pp. 83–106, 1970.

[22] J. Stautner and M. Puckette, “Designing multichannel reverberators,”

Computer Music Journal, vol. 3, pp. 569–582, 1989.

[23] M. Gerzon, “Synthetic reverberation, part I,” Studio Sound, vol. 13, pp.

632–635, 1971.

[24] S. Bai, J. Z. Kolter, and V. Koltun, “An empirical evaluation of generic

convolutional and recurrent networks for sequence modeling,” 2018.

[25] C. J. Steinmetz and J. D. Reiss, “Efﬁcient neural networks for real-time
analog audio effect modeling,” arXiv preprint arXiv:2102.06200, 2021.
[26] C. J. Steinmetz, J. Pons, S. Pascual, and J. Serr`a, “Automatic multitrack
mixing with a differentiable mixing console of neural audio effects,”
in ICASSP 2021 - 2021 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), 2021, pp. 71–75.

[27] S. Billings and S. Fakhouri, “Identiﬁcation of systems containing linear
dynamic and static nonlinear elements,” Automatica, vol. 18, no. 1, pp.
15–26, 1982.

[28] X. Serra and J. Smith, “Spectral modeling synthesis: A sound analy-
sis/synthesis based on a deterministic plus stochastic decomposition,”
Computer Music Journal, vol. 14, pp. 12–24, 1990.

[29] J. Engel, L. H. Hantrakul, C. Gu, and A. Roberts, “DDSP: differentiable
digital signal processing,” in International Conference on Learning
Representations, 2020.

[30] V. Zavalishin, The Art of VA Filter Design. Native Instruments, 2020.
[31] A. Wishnick, “Time-varying ﬁlters for musical applications,” in Pro-
ceedings of the 17th International Conference on Digital Audio Effects,
2014.

[32] “Acoustics — Measurement of room acoustic parameters — Part 2:
Reverberation time in ordinary rooms,” International Organization for
Standardization, Geneva, CH, Standard, Mar. 2008.

[33] R. Ratnam, D. L. Jones, B. C. Wheeler, W. D. O’Brien, C. R. Lansing,
and A. S. Feng, “Blind estimation of reverberation time,” The Journal of
the Acoustical Society of America, vol. 114, no. 5, pp. 2877–2892, 2003.

LEE et al.: DIFFERENTIABLE ARTIFICIAL REVERBERATION

16

[60] C. Hak, R. Wenmaekers, and L. Luxemburg, “Measuring room impulse re-
sponses: Impact of the decay range on derived room acoustic parameters,”
Acta Acustica united with Acustica, vol. 98, 11 2012.

[61] Z. Meng, F. Zhao, and M. He, “The just noticeable difference of noise
length and reverberation perception,” in 2006 International Symposium
on Communications and Information Technologies, 2006, pp. 418–421.
[62] M. G. Blevins, A. Buck, Z. E. Peng, and L. Wang, “Quantifying the
just noticeable difference of reverberation time with band-limited noise
centered around 1000 hz using a transformed up-down adaptive method,”
June 2013.

[63] J. Bradley, R. Reich, and S. Norcross, “A just noticeable difference in
c50 for speech,” Applied Acoustics, vol. 58, no. 2, pp. 99 – 108, 1999.
[64] F. Martellotta, “The just noticeable difference of center time and clarity
index in large reverberant spaces,” The Journal of the Acoustical Society
of America, vol. 128, pp. 654–63, 08 2010.

[65] E. Larsen, N. Iyer, C. Lansing, and A. Feng, “On the minimum audible
difference in direct-to-reverberant energy ratio,” The Journal of the
Acoustical Society of America, vol. 124, pp. 450–61, 07 2008.

[66] “Method for the subjective assessment of intermediate quality level of
audio systems,” International Telecommunication Union Radio Commu-
nication Assembly, Geneva, CH, Standard, Mar. 2014.

[67] M. Schoefﬂer, S. Bartoschek, F.-R. St¨oter, M. Roess, S. Westphal,
B. Edler, and J. Herre, “webMUSHRA — a comprehensive framework
for web-based listening tests. journal of open research software,” Journal
of Open Research Software, vol. 6, p. 8, 2018.

[68] J. Liski, B. Bank, J. Smith, and V. V¨alim¨aki, “Converting series biquad
ﬁlters into delayed parallel form: Application to graphic equalizers,”
IEEE Transactions on Signal Processing, vol. PP, pp. 1–1, 05 2019.
[69] J. O. Smith, Introduction to Digital Filters with Audio Applications.

W3K Publishing, 2007.

[34] J. Y. C. Wen, E. A. P. Habets, and P. A. Naylor, “Blind estimation of
reverberation time based on the distribution of signal decay rates,” in
2008 IEEE International Conference on Acoustics, Speech and Signal
Processing, 2008, pp. 329–332.

[35] H. Gamper and I. J. Tashev, “Blind reverberation time estimation using
a convolutional neural network,” in 2018 16th International Workshop
on Acoustic Signal Enhancement (IWAENC), 2018, pp. 136–140.
[36] S. Li, R. Schlieper, and J. Peissig, “A hybrid method for blind estimation
of frequency dependent reverberation time using speech signals,” in
ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), 2019, pp. 211–215.

[37] S. Schlecht and E. Habets, “Accurate reverberation time control in
feedback delay networks,” in Proceedings of the 20th International
Conference on Digital Audio Effects, September 2017.

[38] K. Prawda, V. V¨alim¨aki, and S. Schlecht, “Improved reverberation
time control for feedback delay networks,” in Proceedings of the 22nd
International Conference on Digital Audio Effects, September 2019.

[39] E. T. Chourdakis and J. D. Reiss, “A machine-learning approach to
application of intelligent artiﬁcial reverberation,” Journal of the Audio
Engineering Society, vol. 65, no. 1/2, pp. 56–65, January 2017.
[40] A. Richard, D. Markovic, I. D. Gebru, S. Krenn, G. A. Butler, F. Torre,
and Y. Sheikh, “Neural synthesis of binaural speech,” in International
Conference on Learning Representations, 2021.

[41] C. J. Steinmetz, V. K. Ithapu, and P. Calamia, “Filtered noise shaping for
time domain room impulse response estimation from reverberant speech,”
arXiv preprint arXiv:2107.07503, 2021.

[42] J. O. Smith, Mathematics of the Discrete Fourier Transform (DFT).

http://www.w3k.org/books/: W3K Publishing, 2007.

[43] V. V¨alim¨aki, H.-M. Lehtonen, and M. Takanen, “A perceptual study
on velvet noise and its variants at different pulse densities,” IEEE
Transactions on Audio, Speech, and Language Processing, vol. 21, no. 7,
pp. 1481–1488, 2013.

[44] H. J¨arvel¨ainen and M. Karjalainen, “Reverberation modeling using velvet

noise,” journal of the audio engineering society, March 2007.

[45] M. R. Schroeder and B. F. Logan, “‘colorless’ artiﬁcial reverberation,”
Journal of the Audio Engineering Society, vol. 9, no. 3, pp. 192–197,
July 1961.

[46] J.-M. Jot and A. Chaigne, “Digital delay networks for designing artiﬁcial
reverberators,” Journal of the Audio Engineering Society, February 1991.
[47] D. Rocchesso and J. O. Smith, “Circulant and elliptic feedback delay
networks for artiﬁcial reverberation,” IEEE Transactions on Speech and
Audio Processing, vol. 5, no. 1, pp. 51–63, 1997.

[48] S. J. Schlecht and E. A. P. Habets, “Time-varying feedback matrices in
feedback delay networks and their application in artiﬁcial reverberation,”
The Journal of the Acoustical Society of America, vol. 138, no. 3, pp.
1389–1398, 2015.

[49] S. Schlecht, “Fdntb: The feedback delay network toolbox,” in Proceedings

of the 23rd International Conference on Digital Audio Effects, 2020.

[50] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Empirical evaluation of
gated recurrent neural networks on sequence modeling,” arXiv preprint
arXiv:1412.3555, 2014.

[51] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” arXiv

preprint arXiv:1607.06450, 2016.

[52] S. Shelley and D. Murphy, “Openair: An interactive auralization web
resource and database,” 129th Audio Engineering Society Convention
2010, vol. 2, pp. 1270–1278, 01 2010.

[53] J. Eaton, N. D. Gaubitch, A. H. Moore, and P. A. Naylor, “Estimation of
room acoustic parameters: The ace challenge,” IEEE/ACM Transactions
on Audio, Speech, and Language Processing, vol. 24, no. 10, pp. 1681–
1693, 2016.

[54] P. Svensson and U. R. Kristiansen, “Computational modelling and
simulation of acoutic spaces,” Journal of the Audio Engineering Society,
June 2002.

[55] R. Scheibler, E. Bezzam, and I. Dokmanic, “Pyroomacoustics: A python
package for audio room simulations and array processing algorithms,”
arXiv preprint arXiv:1710.04196, 2017.

[56] G. Defrance, L. Daudet, and J.-D. Polack, “Finding the onset of a room
impulse response: straightforward?” The Journal of the Acoustical Society
of America, vol. 124, no. 4, pp. EL248–EL254, 2008.

[57] N. J. Bryan, “Impulse response data augmentation and deep neural
networks for blind room acoustic parameter estimation,” in 2020 IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP), 2020, pp. 1–5.

[58] J. Yamagishi, C. Veaux, K. MacDonald et al., “Cstr vctk corpus: English
multi-speaker corpus for cstr voice cloning toolkit (version 0.92),” 2019.
[59] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”

arXiv preprint arXiv:1412.6980, 2017.

