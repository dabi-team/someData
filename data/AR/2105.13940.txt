LEE et al.: DIFFERENTIABLE ARTIFICIAL REVERBERATION

1

Differentiable Artiï¬cial Reverberation

Sungho Lee, Hyeong-Seok Choi, and Kyogu Lee

2
2
0
2

l
u
J

0
2

]

D
S
.
s
c
[

3
v
0
4
9
3
1
.
5
0
1
2
:
v
i
X
r
a

Abstractâ€”Artiï¬cial reverberation (AR) models play a central
role in various audio applications. Therefore, estimating the AR
model parameters (ARPs) of a reference reverberation is a crucial
task. Although a few recent deep-learning-based approaches have
shown promising performance, their non-end-to-end training
scheme prevents them from fully exploiting the potential of deep
neural networks. This motivates the introduction of differentiable
artiï¬cial reverberation (DAR) models, allowing loss gradients to
be back-propagated end-to-end. However, implementing the AR
models with their difference equations â€œas isâ€ in the deep learning
framework severely bottlenecks the training speed when executed
with a parallel processor like GPU due to their inï¬nite impulse
response (IIR) components. We tackle this problem by replacing
the IIR ï¬lters with ï¬nite impulse response (FIR) approximations
with the frequency-sampling method. Using this technique, we
implement
three DAR modelsâ€”differentiable Filtered Velvet
Noise (FVN), Advanced Filtered Velvet Noise (AFVN), and
Delay Network (DN). For each AR model, we train its ARP
estimation networks for analysis-synthesis (RIR-to-ARP) and
blind estimation (reverberant-speech-to-ARP) task in an end-to-
end manner with its DAR model counterpart. Experiment results
show that the proposed method achieves consistent performance
improvement over the non-end-to-end approaches in both objec-
tive metrics and subjective listening test results. Audio samples
are available at https://sh-lee97.github.io/DAR-samples/.

Index Termsâ€”Digital Signal Processing, Acoustics, Reverbera-

tion, Artiï¬cial Reverberation, Deep Learning.

I. INTRODUCTION

Reverberation is ubiquitous in a real acoustic environment.
It provides the listeners psychoacoustic cues for spatial char-
acteristics. Therefore, adding an appropriate reverberation to a
dry audio is desirable for plausible listening [1]â€“[3]. Artiï¬cial
reverberation (AR), efï¬cient digital ï¬lters that may be used
to mimic real-world reverberation, have been developed to
achieve such auditory effects [4]â€“[6] and applied to room
acoustic enhancement [7], auditory scene generation [8], [9],
post-production [10], and many more.

Manuscript received October 21, 2021; revised March 27, 2022 and June
10, 2022; accepted July 15, 2022. This work was supported by Institute of
Information & communications Technology Planning & Evaluation (IITP)
grant funded by the Korea government (MSIT) (No.2022-0-00641). The
associate editor coordinating the review of this manuscript and approving
it for publication was Cecchi, Stefania (Corresponding Author: Kyogu Lee).
Sungho Lee is with the Music and Audio Research Group, Graduate School
of Convergence Science and Technology, Seoul National University, Seoul,
Republic of Korea (e-mail: sh-lee@snu.ac.kr).

Hyeong-Seok Choi was with the Music and Audio Research Group, Graduate
School of Convergence Science and Technology, Seoul National University,
Seoul, Republic of Korea. He is now with the Institute of New Media and
Communications, Seoul National University, Seoul, Republic of Korea (e-mail:
kekepa15@snu.ac.kr).

Kyogu Lee is with the Music and Audio Research Group, Graduate School
of Convergence Science and Technology, Seoul National University, Seoul,
South Korea, and also with the Advanced Institutes of Convergence Technology,
Suwon, South Korea (e-mail: kglee@snu.ac.kr).

(a) Training ARP estimation network with DAR models.

(b) Inference-time usage of the trained ARP estimation network.

Fig. 1. The proposed ARP estimation framework. (a) With the DAR models,
loss gradients âˆ‚L/âˆ‚P1, Â· Â· Â· , âˆ‚L/âˆ‚Pn can be back-propagated through the
DAR models. Hence, the ARP estimation network can be trained in an end-to-
end manner. We can train the network to perform the analysis-synthesis (RIR
h input), blind estimation (reverberant speech h âˆ— x input), or even both. The
network has an AR-model-agnostic encoder so that using a different DAR
model only requires changing the tiny projection layers Proj1, Â· Â· Â· , Projn. (b)
Each DAR model generate an FIR approximation of its original AR modelâ€™s
IIR. After the training, estimated ARPs can be plugged in to the AR model
which is highly efï¬cient and real-time controllable in CPU.

Nevertheless, estimating the AR model parameters (ARPs)
that match the reference reverberation remains challenging. This
is because the mapping from the ARPs to the reverberation is
highly nontrivial. We refer to this task as analysis-synthesis
when the reference is a room impulse response (room IR, RIR).
When the reference is indirectly provided with a reverberant
signal, i.e., an RIR convolved with a dry signal, we refer to
this task as blind estimation. We limit the scope to reverberant
speech signals in this paper, yet the framework can easily be
extended to other types of signals, such as musical ones.

ARP estimation is a classical problem, and various attempts
have been made [11]â€“[14]. However, they are only applicable
to a speciï¬c AR model (AR-model-dependent) and the analysis-
synthesis task (task-dependent). This inï¬‚exibility is problematic
since every application has its own suitable AR models and
reference forms (e.g., RIR and reverberant speech).

We can overcome such limitations with differentiable arti-
ï¬cial reverberation (DAR) models. Each DAR model takes
ARPs as input and generates its corresponding AR modelâ€™s IR
(see Figure 1a). After computing a loss between the generated
IR and the reference RIR, they allow the loss gradients to be
back-propagated through themselves. As a result, we can train
a deep neural network (DNN) to match the reference RIR in
an end-to-end manner. By leveraging the DNNâ€™s ï¬‚exibility and
expressiveness, we can train the network to perform any task
with any AR model with minimal architecture change.

 
 
 
 
 
 
LEE et al.: DIFFERENTIABLE ARTIFICIAL REVERBERATION

2

Since the AR models are exactly or close to linear time-
invariant (LTI), we can implement them differentiably â€œas
isâ€ with their difference equations [15]â€“[18]. However, this
approach has been impractical because the training speed is
limited by the recurrent sample generation of their IIR ï¬lter
components when executed on a parallel processor, such as
a GPU. In this context, we aim to modify the vanilla DAR
models to sidestep the aforementioned problem. Inspired by
recent works [19], [20], we replace the IIR ï¬lters with ï¬nite
impulse response (FIR) approximations using the frequency-
sampling method [21]. This way, each DAR model becomes an
FIR ï¬lter whose IR can be generated without any recurrent step.
Using this technique, we present differentiable Filtered Velvet
Noise (FVN) [13], Advanced Filtered Velvet Noise (AFVN),
and Delay Network (DN) [22], [23]. Note that the AR-to-
DAR conversion can be applied systematically to any other
AR model with IIR components. While the DAR models are
not identical to their originals, the differences are little in
most cases, allowing us to train the ARP estimation networks
reliably with them. Furthermore, even when differences exist,
the beneï¬ts of the end-to-end learning enabled by the DAR
models outweigh them; we show that the proposed approach
brings consistent performance improvement over non-end-to-
end baselines where the difference is only in the training
scheme. After the training, we can revert to the original AR
models for efï¬cient computation (see Figure 1b).

II. RELATED WORKS

A. Differentiable Digital Signal Processing

Digital signal processing (DSP) and deep learning are closely
related. For example, a temporal convolutional network [24] is
a stack of dilated one-dimensional convolutional layers with
nonlinear activation layers and it has been used for modeling
dynamic range compression [25] and reverberation [26]. From
the DSP viewpoint, it is FIR ï¬lter banks serially connected with
memoryless nonlinearities, i.e., an extension of the classical
Wiener-Hammerstein model [27].

If we look for more explicit DSP-deep learning relationships,
so-called â€œdifferentiable digital signal processing (DDSP)â€ ap-
proaches exist which aim to import DSP components into deep
learning frameworks as they provide strong structural priors and
interpretable representations. For example, a harmonic-plus-
noise model [28] was implemented in such a manner; then, a
DNN was trained to estimate its parameters to synthesize a
monophonic signal and used for various applications, including
controllable synthesis and timbre transfer [29].

In the case of LTI ï¬lters, it is well-known that both an FIR
and IIR ï¬lter are differentiable and can be directly imported
into the deep learning framework as a convolutional layer and
recurrent layer, respectively [15]â€“[18]. Since the IIR ï¬lter, e.g.,
parametric equalizer (PEQ), bottlenecks the training speed, the
frequency-sampling method [21] was utilized [19], [20]. Upon
these works, we investigate the reliability of this technique,
present a general differentiable IIR ï¬lter based on state-variable
ï¬lter (SVF) [30], [31], and use it to implement the DAR models.

B. ARP Estimation

1) Analysis-synthesis: Various methods have been proposed
to systematically and automatically estimate ARPs that mimic
certain characteristics of a given RIR. Some works ï¬rst extract
perceptually relevant features from the RIR, then estimate the
ARPs from them. For example, FDN parameters were derived
from the RIRâ€™s energy decay relief (EDR) representation which
encodes the frequency-dependent reverberation decay [11].
Along with the FVN and AFVN, their parameter estimation
algorithm based on linear prediction were proposed together
[13]. Another thread of work utilizes optimization. A genetic
algorithm was proposed to ï¬nd the FDN parameters that achieve
the desired â€œï¬tnessâ€ to the reference RIR [12]. Since all these
methods assume availability of the RIR, they cannot be applied
to the blind estimation task.

2) Blind Estimation: End-users (e.g., audio engineers) often
interact with the AR models provided as audio plug-ins. As
a result, several attempts have been made to implement blind
estimation algorithms for the plug-ins. For example, a preset
recommendation method based on the Gaussian mixture model
was proposed [14]. More recently, DNNs were trained for the
plug-in parameter estimation and preset recommendation [10].
While these methods have shown promise, there is still room
for improvement. First, they generated the training data using
the plug-ins to train their models. However, this procedure
may be time-consuming, and relying on data generated with a
single plug-in may increase the generalization error. Second,
their training objectives could be suboptimal. When the task
is deï¬ned as parameter regression [10], one must weigh each
parameterâ€™s perceptual importance and apply it to the loss
function design, which is a highly challenging task. When the
task is deï¬ned as classiï¬cation [10], [14], it could suffer from
scalability problems. Finally, most plug-ins provide only a few
parameters, and each of them controls multiple internal ARPs
simultaneously. This might limit the performance due to the
reduced degree of freedom.

We tackle these problems with the DAR models as follows.
First, we can use a training objective that directly compares the
estimated IR with ground-truth RIR. Therefore, we can use any
RIR for training and bypass the need to obtain the ground-truth
ARPs. Second, we replace the parameter-matching loss with
the end-to-end IR-matching loss and show that it improves the
estimation performance by a large margin in both objective
metrics and subjective listening test scores. Finally, we estimate
the ARPs directly instead of relying on the plug-in parameters.
3) Two-Stage Approaches: Note that both analysis-synthesis
and blind estimation tasks can be divided into two subtasks.
First, we can obtain reverberation parameters, e.g., reverberation
time, from a given reference (reference-to-reverberation param-
eters). We can directly compute the parameters from the RIR
[32] for the analysis-synthesis case. For the blind estimation,
various methods have been proposed [33]â€“[36]. Then, we can
tune the ARPs to match the desired reverberation parameter
values (reverberation parameters-to-ARPs) [37]â€“[39]. This two-
stage approach can be a possible alternative to our single-stage
end-to-end estimation method. However, perceptually different
reverberations can have the same reverberation parameters,
resulting in information loss.

LEE et al.: DIFFERENTIABLE ARTIFICIAL REVERBERATION

3

C. Modeling Reverberation in the Deep Learning Framework

An alternative to the proposed DAR approach could be using
existing deep learning modules to build another reverberation
model. The most simplest approach could be using a high-order
FIR directly as an RIR model [29]. While it is viable when the
objective is to learn a single RIR, training a DNN to generate
various raw RIRs is highly challenging and inefï¬cient.

Instead, recent approaches either use a DNN as a reverberator
and control them via conditioning methods [26], [40] or design
a differentiable RIR generator and use a DNN as a parameter
estimator [41]. Our method is closely related to the latter since
the DAR model and the proposed ARP estimation network
act in that way. However, the key difference is that our
DAR models are direct imports of the AR models, which
bring the following beneï¬ts. First, they are more directly
interpretable and controllable, allowing end-users to further tune
the parameters. Second, their structure can prevent unexpected
artifacts. Finally, they use sparse IIR and FIR ï¬lters, enabling
efï¬cient real-time computation in CPU.

III. FREQUENCY-SAMPLING METHOD FOR
DIFFERENTIABLE IIR FILTERS
For any real IIR ï¬lter H, its frequency response H(ejÏ‰) is
continuous, 2Ï€-periodic, and conjugate symmetric. To obtain
its FIR approximation, we frequency-sample H(ejÏ‰) at angular
frequencies Ï‰k = 2Ï€k/N where k = 0, Â· Â· Â· , (cid:98)N/2(cid:99) [21]. We
denote the frequency-sampled ï¬lter with subscript N , i.e., HN .
HN [k] = H(ejÏ‰k ).

(1)

Note that the frequency-sampling can also be deï¬ned on transfer
functions, e.g., H(z) with z = ejÏ‰k .

Since each sampling is independent, HN can be generated
simultaneously with a parallel processor like GPU. Furthermore,
the order of frequency-sampling (Â·)N and other basic arithmetic
operations does not matter. Therefore, we can frequency-sample
an IIR ï¬lter by combining frequency-sampled m-sample delays
(zâˆ’m)N âˆˆ C(cid:98)N/2(cid:99)+1 as follows,

(cid:33)

(cid:32) (cid:80)M
(cid:80)M

(cid:80)M

HN [k] =

m=0 Î²mzâˆ’m
m=0 Î±mzâˆ’m

m=0 Î²m(zâˆ’m)N [k]
m=0 Î±m(zâˆ’m)N [k]
(2)
Since (zâˆ’m)N [k] = eâˆ’j2Ï€km/N , using the widely-used deep
learning libraries, we obtain (zâˆ’m)N as a tensor z_m by

[k] =

(cid:80)M

N

.

angle = 2*pi*arange(N//2+1)/N,

z_m = e**(-1j*angle*m).

(3a)

(3b)

Then, we apply equation (2) to z_0, Â· Â· Â· , z_M with basic tensor
operations and obtain every sample of HN in parallel. A time-
domain representation of HN is a length-N FIR hN [n], inverse
discrete Fourier transform (inverse DFT) of HN [k], i.e.,

hN [n] =

1
N

N âˆ’1
(cid:88)

k=0

HN [k]ejÏ‰kn(u[n] âˆ’ u[n âˆ’ N ])

(4)

Fig. 2. The frequency-sampling method with a various number of sampling
points N . Blue curves represent its magnitude response |H(ejÏ‰)| (top row)
and IR h[n] (bottom row). Red curves are the magnitude response and IR of
the frequency-sampled ï¬lter HN .

hN [n] whose convolution can be performed efï¬ciently via Fast
Fourier Transform (FFT). In this paper, the â€œdifferentiable IIRâ€
ï¬lter refers to HN , which is in fact an FIR ï¬lter.

A. Reliability of the Frequency-Sampling Method

Every IIR ï¬lterâ€™s frequency response has â€œinï¬nite bandwidthâ€
(in the time domain) so that the frequency-sampling causes time-
aliasing [42]. Speciï¬cally, the frequency-sampled IR hN [n] is
sum of length-N segments of the original IR h[n].

hN [n] =

âˆ
(cid:88)

m=0

h[mN + n](u[n] âˆ’ u[n âˆ’ N ]).

(5)

Since the energy of every stable IIR ï¬lterâ€™s IR decays over time,
more frequency-sampling points N give less time-aliasing, i.e.,
a closer approximation (see Figure 2). Therefore, we can use
the differentiable ï¬lter HN in place of the original H to match
a reference response HRef. We elaborate this with following
analytical results.

1) Time-aliasing Error: If H has M distinct poles where
each of them Î½i âˆˆ C has multiplicity ri âˆˆ N, the time-aliasing
error asymptotically decreases as N increases as follows,

(cid:107)H âˆ’ HN (cid:107)2 =

M
(cid:88)

i=1

O(N riâˆ’1|Î½i|N ).

(6)

2) Loss Error: The triangle inequality indicates that the loss

error is bounded to the time-aliasing error.

(cid:12)
(cid:12)(cid:107)HRef âˆ’ H(cid:107)2

2 âˆ’ (cid:107)HRef âˆ’ HN (cid:107)2
2

(cid:12)
(cid:12) â‰¤ (cid:107)H âˆ’ HN (cid:107)2
2.

(7)

3) Loss Gradient Error: The loss gradient error has similar

asymptotic behavior to the time-aliasing error as follows,

(cid:12)
(cid:12)
(cid:12)
(cid:12)

âˆ‚
âˆ‚p

(cid:107)HRef âˆ’ H(cid:107)2

2 âˆ’

âˆ‚
âˆ‚p

(cid:107)HRef âˆ’ HN (cid:107)2
2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

M
(cid:88)

=

O(N 3riâˆ’1|Î½i|N )

(8)
where p is a parameter of H. See Appendix C for the proofs.

i=1

where u[n] is a unit step function.

B. Differentiable Dense IIR Filter with State-variable Filters

Therefore, using the frequency-sampling method, we can
replace any IIR ï¬lter H that causes the bottleneck with an FIR

Since arbitrary IIR ï¬lter can be expressed as serially cascaded
biquads (second-order IIR ï¬lters), we can obtain a differentiable

Magnitude(dB)Original FilterFrequency-SampledN=10Waveform (amplitude)N=20N=40N=80LEE et al.: DIFFERENTIABLE ARTIFICIAL REVERBERATION

4

IIR ï¬lter ËœH as a product of frequency-sampled biquads (Hi)N .
(cid:32) (cid:80)2
(cid:80)2

(Hi)N [k] =

ËœH[k] =

m=0 Î²i,mzâˆ’m
m=0 Î±i,mzâˆ’m

[k].

(cid:89)

(cid:89)

(9)

(cid:33)

i

i

N

Moreover, we use the state variable ï¬lter (SVF) parameters
fi, Ri, mLP
to express each biquad as follows,
i

, and mHP

, mBP
i
Î²i,0 = f 2
Î²i,1 = 2f 2
Î²i,2 = f 2
Î±i,0 = f 2
Î±i,1 = 2f 2
Î±i,2 = f 2

i + mHP
i

i
i + fimBP
i mLP
i âˆ’ 2mHP
i mLP
i
i + mHP
i âˆ’ fimBP
i mLP
i
i + 2Rifi + 1,
i âˆ’ 2,
i âˆ’ 2Rifi + 1.

,

,

,

(10a)

(10b)

(10c)

(10d)

(10e)

(10f)

We prefer to use the SVF parameters rather than the biquad

coefï¬cients due to the following reasons.

i

, and highpass ï¬lter H HP

â€¢ Interpretability. Each SVF has lowpass H LP

, bandpass
i
H BP
. They share a resonance Ri
i
and cutoff parameter fi = tan(Ï€Ï‰i/Ï‰s) where Ï‰i is their
cutoff frequency and Ï‰s is the sampling rate. The ï¬lter
outputs are multiplied with gains mLP
, and mHP
i
and then summed. This interpretability comes without any
generality loss since an SVF can express any biquad [31].
â€¢ Simple Activation Functions. The SVF parameters have
simpler stability conditions, Ri > 0 and fi > 0, than the
biquad coefï¬cients [18], which leads to simpler activation
function design. Refer to Section V-C for details.

, mBP
i

i

â€¢ Better Performance. We found that our ARP estimation
networks performed better when they estimate the SVF
parameters rather than the biquad coefï¬cients. Refer to
Appendix A-B for the comparison results.

From now on, we denote each SVF-parameterized biquad

with a superscript H SVF

i

and call it simply â€œSVFâ€.

C. Differentiable Parametric Equalizer

A low-shelving H LS, peaking H Peak, and high-shelving ï¬lter
H HS are widely used audio ï¬lters. Each of them can be obtained
using the SVF H SVF(f, R, mLP, mBP, mHP) as follows [30],

H LS(f, R, G) = H SVF(f, R, G, 2R
H Peak(f, R, G) = H SVF(f, R, 1, 2RG, 1),
H HS(f, R, G) = H SVF(f, R, 1, 2R

âˆš

G, G).

G, 1),

(11a)

(11b)

(11c)

âˆš

Here, G is a new parameter that gives 20 log10(G) dB gain. A
parametric equalizer (PEQ) is serial composition of such ï¬lters.
Following the recent works [19], [20], we use one low-shelving,
one high-shelving, and K âˆ’ 2 peaking ï¬lters.

H PEQ(z) = H LS(z)H HS(z)

Kâˆ’2
(cid:89)

i=1

H Peak
i

(z).

(12)

Same as the IIR ï¬lter case, frequency-sampling the components
and multiplying them results in a differentiable PEQ H PEQ
N .

IV. DIFFERENTIABLE ARTIFICIAL REVERBERATION

In this section, we derive differentiable FVN, AFVN, and
DN. We ï¬rst brieï¬‚y review the original AR models. Then,

Fig. 3. Modiï¬ed FVN, AVFNâ€™s RIR approximation, and their differentiable
implementation strategy. FVN and AVFN divide a target RIR h[n] into
segments hi[n] and approximate each segment with a velvet noise vi ï¬ltered
with an allpass ï¬lter Ui and a coloration ï¬lter Ci. We obtain the differentiable
FVN and AVFN by converting the IIR ï¬lters Ui and Ci into FIR ï¬lters Ëœui
and (ci)N . Each IIR ï¬lter is emphasized with a double border box.

we obtain their DAR counterparts by frequency-sampling the
IIR components, replacing them with FIRs. Also, we slightly
modify the original models for efï¬cient training, plausible
reverberation, and better overall estimation performance.

A. Differentiable Filtered Velvet Noise

1) Filtered Velvet Noise: One can divide an RIR h[n] into
S segments (see Figure 3). As reverberation can be regarded
as mostly stochastic, we can model each length-Li segment
hi[n] with a source noise signal si[n] â€œcolored withâ€ a ï¬lter
Ci. When we use velvet noise for each source, this source-
ï¬lter model becomes Filtered Velvet Noise (FVN) [13], [43],
[44]. The velvet noise vi[n] is sparse; in every length-Ti
interval, it contains a single nonzero sample Â±1 with random
sign/position such that its time-domain convolution is highly
efï¬cient. Moreover, an allpass ï¬lter Ui is introduced to smooth
each source and make plausible sound. Finally, an deterministic
(bypass) FIR Ë†h0[n] models the direct arrival and early reï¬‚ection.
Then, an IR of FVN Ë†h[n] becomes

Ë†h[n] = Ë†h0[n] +

S
(cid:88)

i=1

si[nâˆ’di]
(cid:125)(cid:124)

(cid:123)

(cid:122)
vi[n âˆ’ di] âˆ— ui[n] âˆ— ci[n]
(cid:123)(cid:122)
(cid:125)
(cid:124)
Ë†hi[nâˆ’di]

(13)

where each di is a delay for the segment alignment (d1 = 0). In
practice, each allpass ï¬lter Ui is composed with Schroeder all-
i,j (z) = (1 + Î³i,jzâˆ’Ï„i,j )/(Î³i,j + zâˆ’Ï„i,j ),
pass ï¬lters (SAPs), U SAP
where Ï„i,j is a delay-line length and Î³i,j is a feed-forward/back
gain) for efï¬ciency [45]. A low-order dense IIR ï¬lter is used
as each coloration ï¬lter Ci.

ğ‘¥ğ‘›â‹¯ğ‘¦ğ‘›=&â„ğ‘›âˆ—ğ‘¥ğ‘›ğ‘ˆ!"#$ğ‘‰!+ğ»%ğ‘ˆ&"#$ğ‘‰&ğ‘ˆâ€™"#$ğ‘‰â€™â‹¯&â„!ğ‘›âˆ—ğ‘¥ğ‘›&â„&ğ‘›âˆ’ğ‘‘&âˆ—ğ‘¥ğ‘›&â„â€™ğ‘›âˆ’ğ‘‘â€™âˆ—ğ‘¥ğ‘›ğ¶!ğ¶&ğ¶â€™ğ‘¥ğ‘›ğ¶!â‹¯ğ‘¦ğ‘›=&â„ğ‘›âˆ—ğ‘¥ğ‘›ğ‘ˆ!"#$ğ‘‰!+ğ»%ğ¶(&ğ‘ˆ&"#$ğ‘‰&ğ¶(â€™ğ‘ˆâ€™"#$ğ‘‰â€™â‹¯&â„!ğ‘›âˆ—ğ‘¥ğ‘›&â„&ğ‘›âˆ’ğ‘‘&âˆ—ğ‘¥ğ‘›&â„â€™ğ‘›âˆ’ğ‘‘â€™âˆ—ğ‘¥ğ‘›0â„&=0â„!=0â„â€™=0â„ğ¿!ğ¿"ğ¿#â„%â‰ˆ&â„%â„ğ‘£&ğ‘&ğ‘ˆ!"#$ğ‘¢&"#$â„&â‰ˆ&â„&=ğ‘£&ğ‘&*ğ‘ˆ!"#$5ğ‘¢&0â„&=â‰ˆğ‘£!ğ‘!ğ‘¢!"#$â„!â‰ˆ&â„!=ğ‘£!ğ‘!*5ğ‘¢!0â„!=â‰ˆfrequency-samplecopyconvolve&cropğ‘£â€™ğ‘â€™ğ‘¢â€™"#$â‹®â„â€™â‰ˆ&â„â€™=ğ‘£â€™ğ‘â€™*5ğ‘¢â€™0â„â€™=â‰ˆâ‹¯â‹¯â‹¯â‹¯â‹¯â‹¯ğ‘¢â€™ğ‘¢&ğ‘¢!âˆ—âˆ—âˆ—âˆ—âˆ—âˆ—âˆ—âˆ—âˆ—âˆ—âˆ—âˆ—ğ‘(&ğ‘&*ğ‘!ğ‘!*frequency-sampleğ‘(â€™ğ‘â€™*â‹¯â‹®â‹®FVNAFVNğ‘â€™ğ‘&LEE et al.: DIFFERENTIABLE ARTIFICIAL REVERBERATION

5

(a) Approximation error of differentiable FVN in EDR.

(a) Modiï¬ed FVN in the time domain.

(b) Approximation error of differentiable AFVN in EDR.

Fig. 4. EDR of IRs generated by differentiable FVN, AFVN, their original AR
counterparts, and EDR errors introduced during the AR-to-DAR conversion
process. Because the errors are small and mostly in the low-energy region,
they are imperceptibly small (note that the z axis scale is different for the
EDR plots and the EDR error plots). We obtained the FVN/AFVN parameters
with the trained blind estimation networks discussed below.

2) Differentiable Implementation: We modify each FVN

component to obtain differentiable FVN.

â€¢ Velvet ï¬lters. To batch the IR segments, we use the same
segment lengths L1, Â· Â· Â· , LS and average pulse distance
values T1, Â· Â· Â· , TS for every reference RIR.

â€¢ Allpass ï¬lters. We tune and ï¬x the allpass ï¬lter parameters
jointly with the velvet ï¬lter parameters to avoid perceptual
artifacts, e.g., discontinuous and rough sound, while being
computationally efï¬cient. Since we ï¬xed the ï¬lter, instead
of the frequency-sampling method, we simply crop its IR
to obtain an FIR Ëœui[n].

â€¢ Coloration ï¬lters. We model Ci with K serial SVFs, i.e.,
Ci(z) = (cid:81)K
i,k (z). We frequency-sample each ï¬lter
Ci(z) and convert it into a length-N FIR (ci)N [n] so that
we can train our network to estimate its parameters.

k=1 C SVF

Therefore, IR of the differentiable FVN Ëœh[n] becomes

(cid:122)
(cid:123)
Ëœh[n] = Ë†h0[n] + overlap-add{
}.
vi[n] âˆ— Ëœui[n] âˆ—(ci)N [n]
(cid:125)
(cid:123)(cid:122)
(cid:124)
Ëœhi[n]

Ëœsi[n]
(cid:125)(cid:124)

(14)

Since every component vi[n], Ëœui[n], and (ci)N [n] is an FIR,
we can compute each segment Ëœhi[n] = vi[n] âˆ— Ëœui[n] âˆ— (ci)N [n]
efï¬ciently by convolving the FIRs in the frequency domain,
i.e., zero-padding followed by FFT, multiplication, then IFFT.
Finally, we overlap-add all the segments and add the determin-
istic FIR Ë†h0[n] to obtain the full IR Ëœh[n]. The overlap-add can
be implemented with a transposed convolution layer.

While the modiï¬cations for the differentiable FVN introduce
approximation error, it is negligible in practice. We explain
this with the EDR H EDR, a reverse cumulative sum of an IRâ€™s
short-time Fourier transform (STFT) energy [11].

H EDR[k, n] =

âˆ
(cid:88)

m=n

|H STFT[k, m]|2.

(15)

(b) Modiï¬ed AVFN in the time domain.

(c) Each velvet ï¬lter Vi.

(d) Each coloration ï¬lter Ci.

Fig. 5. Modiï¬cations to the original FVN and AFVN. (a) Modiï¬ed FVN. To
retain a sharp transient, we insert each SAP before its corresponding velvet
ï¬lter. For each velvet ï¬lter Vi, the output denoted with a down arrow is a
convolution of the input and the velvet noise vi[n]. The other output denoted
with a right arrow is a Li-sample-delayed input. Refer to [13] for more details.
(b) The same modiï¬cation is performed to the AVFN model. (c) For both FVN
and AVFN, each velvet ï¬lter Vi is divided into smaller ï¬lters Vi,1, Â· Â· Â· , Vi,M
assigned with gains gi,1, Â· Â· Â· , gi,M . This modiï¬cation is equivalent to dividing
each velvet segment vi[n] into smaller segments vi,1[n], Â· Â· Â· , vi,M [n] and
multiplying them with gi,1, Â· Â· Â· , gi,M . (d) The ith coloration ï¬lter Ci in the
time domain, which is serially connected SVFs CSVF

i,1 , Â· Â· Â· , CSVF
i,K .

By comparing the FVNâ€™s EDR and its differentiable counterpart,
we can analyze their differences in reverberation characteristics.
Speciï¬cally, we obtain the EDR error EEDR by calculating the
difference between the two log-magnitude EDRs.

EEDR = 10 log10

Ë†H EDR âˆ’ 10 log10

ËœH EDR.

(16)

Figure 4a shows the EDRs and the EDR error. It reveals that
the error is small (at most 3dB) and mostly in the low-energy
region (below âˆ’40dB). Also, there is little EDR error at the ï¬rst
time index, i.e., |EEDR[k, 0]| (cid:28) 1, indicating that the overall
magnitude response remains the same during the conversion
process. In conclusion, the error is hardly noticeable, and the
differentiable FVN can replace the original FVN for the training
(we use a loss function related to the EDR; see Section V-D).
3) Modiï¬cations to the Original Model: FVN was initially
designed for the analysis-synthesis of late reverberation; it used
the direct arrival and early reï¬‚ections cropped from the RIR
and modeled only the late part. However, we also aim to solve
the blind estimation problem where we must estimate the entire
RIR. To this end, we modify the original model as follows.

â€¢ Finer segment gain. We divide each velvet segment vi[n]
into M smaller segments vi,1[n], Â· Â· Â· , vi,M [n] multiplied
with gains gi,1, Â· Â· Â· , gi,M . Figure 5c shows the resulting

Frequency (Hz)601255001k4k16kTime (s)00.511.522.5   0   -20   -40   -60 Differentiable FVNFrequency (Hz)601255001k4k16kTime (s)00.511.522.5   0   -20   -40   -60 FVNFrequency (Hz)601255001k4k16kTime (s)00.511.522.5Error (dB)   -12   -6   0   6   12Frequency (Hz)601255001k4k16kTime (s)00.511.522.5   0   -20   -40   -60 Differentiable AFVNFrequency (Hz)601255001k4k16kTime (s)00.511.522.5   0   -20   -40   -60 AFVNFrequency (Hz)601255001k4k16kTime (s)00.511.522.5Error (dB)   -12   -6   0   6   12ğ‘¥ğ‘›â‹¯ğ‘¦ğ‘›=&â„ğ‘›âˆ—ğ‘¥ğ‘›ğ‘ˆ!"#$ğ‘‰!+ğ»%ğ‘ˆ&"#$ğ‘‰&ğ‘ˆâ€™"#$ğ‘‰â€™â‹¯&â„!ğ‘›âˆ—ğ‘¥ğ‘›&â„&ğ‘›âˆ’ğ‘‘&âˆ—ğ‘¥ğ‘›&â„â€™ğ‘›âˆ’ğ‘‘â€™âˆ—ğ‘¥ğ‘›ğ¶!ğ¶&ğ¶â€™ğ¿!ğ¿"ğ¿#â„%â‰ˆ&â„%â„ğ‘£&ğ‘&ğ‘ˆ!"#$ğ‘¢&"#$â„&â‰ˆ&â„&=ğ‘£&ğ‘&)ğ‘ˆ!"#$4ğ‘¢&5â„&=â‰ˆğ‘£!ğ‘!ğ‘¢!"#$â„!â‰ˆ&â„!=ğ‘£!ğ‘!)4ğ‘¢!5â„!=â‰ˆfrequency-samplecopyconvolve&cropğ‘£â€™ğ‘â€™ğ‘¢â€™"#$â‹®â„â€™â‰ˆ&â„â€™=ğ‘£â€™ğ‘â€™)4ğ‘¢â€™5â„â€™=â‰ˆâ‹¯â‹¯â‹¯â‹¯â‹¯â‹¯ğ‘¢â€™ğ‘¢&ğ‘¢!ğ‘¥ğ‘›ğ¶!â‹¯ğ‘¦ğ‘›=&â„ğ‘›âˆ—ğ‘¥ğ‘›ğ‘ˆ!"#$ğ‘‰!+ğ»%ğ¶;&ğ‘ˆ&"#$ğ‘‰&ğ¶;â€™ğ‘ˆâ€™"#$ğ‘‰â€™â‹¯&â„!ğ‘›âˆ—ğ‘¥ğ‘›&â„&ğ‘›âˆ’ğ‘‘&âˆ—ğ‘¥ğ‘›&â„â€™ğ‘›âˆ’ğ‘‘â€™âˆ—ğ‘¥ğ‘›5â„&=5â„!=5â„â€™=5â„âˆ—âˆ—âˆ—âˆ—âˆ—âˆ—âˆ—âˆ—âˆ—âˆ—âˆ—âˆ—ğ‘;&ğ‘&)ğ‘!ğ‘!)frequency-sampleğ‘;â€™ğ‘â€™)â‹¯â‹®â‹®FVNadv-FVNğ‘â€™ğ‘&ğ‘¥ğ‘›â‹¯ğ‘¦ğ‘›=&â„ğ‘›âˆ—ğ‘¥ğ‘›ğ‘ˆ!"#$ğ‘‰!+ğ»%ğ‘ˆ&"#$ğ‘‰&ğ‘ˆâ€™"#$ğ‘‰â€™â‹¯&â„!ğ‘›âˆ—ğ‘¥ğ‘›&â„&ğ‘›âˆ’ğ‘‘&âˆ—ğ‘¥ğ‘›&â„â€™ğ‘›âˆ’ğ‘‘â€™âˆ—ğ‘¥ğ‘›ğ¶!ğ¶&ğ¶â€™ğ¿!ğ¿"ğ¿#â„%â‰ˆ&â„%â„ğ‘£&ğ‘&ğ‘ˆ!"#$ğ‘¢&"#$â„&â‰ˆ&â„&=ğ‘£&ğ‘&)ğ‘ˆ!"#$4ğ‘¢&5â„&=â‰ˆğ‘£!ğ‘!ğ‘¢!"#$â„!â‰ˆ&â„!=ğ‘£!ğ‘!)4ğ‘¢!5â„!=â‰ˆfrequency-samplecopyconvolve&cropğ‘£â€™ğ‘â€™ğ‘¢â€™"#$â‹®â„â€™â‰ˆ&â„â€™=ğ‘£â€™ğ‘â€™)4ğ‘¢â€™5â„â€™=â‰ˆâ‹¯â‹¯â‹¯â‹¯â‹¯â‹¯ğ‘¢â€™ğ‘¢&ğ‘¢!ğ‘¥ğ‘›ğ¶!â‹¯ğ‘¦ğ‘›=&â„ğ‘›âˆ—ğ‘¥ğ‘›ğ‘ˆ!"#$ğ‘‰!+ğ»%ğ¶;&ğ‘ˆ&"#$ğ‘‰&ğ¶;â€™ğ‘ˆâ€™"#$ğ‘‰â€™â‹¯&â„!ğ‘›âˆ—ğ‘¥ğ‘›&â„&ğ‘›âˆ’ğ‘‘&âˆ—ğ‘¥ğ‘›&â„â€™ğ‘›âˆ’ğ‘‘â€™âˆ—ğ‘¥ğ‘›5â„&=5â„!=5â„â€™=5â„âˆ—âˆ—âˆ—âˆ—âˆ—âˆ—âˆ—âˆ—âˆ—âˆ—âˆ—âˆ—ğ‘;&ğ‘&)ğ‘!ğ‘!)frequency-sampleğ‘;â€™ğ‘â€™)â‹¯â‹®â‹®FVNadv-FVNğ‘â€™ğ‘&ğ‘¥ğ‘›â‹¯ğ‘¦ğ‘›=ğ‘£!ğ‘›âˆ—ğ‘¥ğ‘›ğ‘‰!,#ğ‘‰!,$ğ‘‰!,%â‹¯ğ‘”!,#ğ‘”!,$ğ‘”!,%ğ‘¥ğ‘›âˆ’ğ‘™!ğ‘¥ğ‘›ğ¶!,#&â€™(â‹¯ğ‘¦ğ‘›=-â„ğ‘›âˆ—ğ‘¥ğ‘›ğ¶!,$&â€™(ğ‘‰#/ğ»)ğ¶*$ğ‘ˆ$&+,ğ‘‰$ğ¶*-ğ‘ˆ-&+,ğ‘‰-â‹¯-â„#ğ‘›âˆ—ğ‘¥ğ‘›-â„$ğ‘›âˆ’ğ‘‘$âˆ—ğ‘¥ğ‘›-â„-ğ‘›âˆ’ğ‘‘-âˆ—ğ‘¥ğ‘›ğ‘¥ğ‘›ğ¶!,#&â€™(ğ¶!,$&â€™(ğ¶!,.&â€™(â‹¯ğ‘!ğ‘›âˆ—ğ‘¥ğ‘›ğ‘¥ğ‘›â‹¯ğ‘¦ğ‘›=ğ‘£!ğ‘›âˆ—ğ‘¥ğ‘›ğ‘‰!,#ğ‘‰!,$ğ‘‰!,%â‹¯ğ‘”!,#ğ‘”!,$ğ‘”!,%ğ‘¥ğ‘›âˆ’ğ‘™!ğ‘¥ğ‘›ğ¶!,#&â€™(â‹¯ğ‘¦ğ‘›=-â„ğ‘›âˆ—ğ‘¥ğ‘›ğ¶!,$&â€™(ğ‘‰#/ğ»)ğ¶*$ğ‘ˆ$&+,ğ‘‰$ğ¶*-ğ‘ˆ-&+,ğ‘‰-â‹¯-â„#ğ‘›âˆ—ğ‘¥ğ‘›-â„$ğ‘›âˆ’ğ‘‘$âˆ—ğ‘¥ğ‘›-â„-ğ‘›âˆ’ğ‘‘-âˆ—ğ‘¥ğ‘›ğ‘¥ğ‘›ğ¶!,#&â€™(ğ¶!,$&â€™(ğ¶!,.&â€™(â‹¯ğ‘!ğ‘›âˆ—ğ‘¥ğ‘›LEE et al.: DIFFERENTIABLE ARTIFICIAL REVERBERATION

6

velvet ï¬lter Vi in the time domain.

â€¢ Cumulative allpass. The original model smoothes every
velvet noise with the same allpass ï¬lter. This is undesirable
in our context since it could smear the direct arrival and
early reï¬‚ections. Instead, we gradually cascade the SAPs
to obtain each allpass ï¬lter Ui(z) = (cid:81)i
(z) so the
earlier segments are less smeared than the later ones. As
shown in Figure 5a, this modiï¬cation can be implemented
in the time domain by inserting each SAP U SAP
before
the corresponding velvet ï¬lter Vj.

j=1 U SAP
j

j

â€¢ Shorter deterministic FIR. With the above changes, the
stochastic segments can ï¬t the early RIR to some extent.
Therefore, we shorten the early deterministic FIR Ë†h0[n].
Refer to Appendix A-A for the evaluation of the modiï¬cations.
4) Model Conï¬guration Details: Exact conï¬gurations of

our FVN used for the experiments are as follows.

j

We generated 2.5 seconds of IR (48kHz sampling rate, total
L = 120k samples) with FVN. We used S = 20 non-uniform
velvet segments whose lengths Li are L/40, L/20, and L/10
for 10, 5, and 5 segments, respectively, and their average pulse
distances were set to T = [10, 20, 35, 50, 65, 90, 120, 135,
180, 220, 270, 320, 370, 420, 480, 540, 610, 680, 750, 820].
The SAPs U SAP
were forced to have gains Î³j = 0.75 + 0.01j
and delay lengths Ï„ = [23, 48, 79, 109, 113, 127, 163, 191,
229, 251, 293, 337, 397, 421, 449, 509, 541, 601, 641, 691].
Each velvet segment had M = 4 sub-segment gains and ï¬ltered
with K = 8 SVFs. We frequency-sampled the coloration ï¬lters
with N = 4k points. The length of the deterministic FIR h0[n]
was set to 50. We set the gain g âˆˆ RSÃ—M , SVF parameters
f , R, mLP, mBP, mHP âˆˆ RSÃ—K, and the bypass FIR h0 âˆˆ RZ
to estimation targets (total 930 ARPs). The time-domain FVN
Ë†H requires 2166 ï¬‚oating point operations per sample (FLOPs).

B. Differentiable Advanced Filtered Velvet Noise

Since frequency-dependent decay of reverberation is gradual,
one can model each coloration ï¬lter Ci as an initial coloration
ï¬lter C1 cascaded with delta-coloration ï¬lters Câˆ†2, Â· Â· Â· , Câˆ†i.

Ci(z) = C1(z)

i
(cid:89)

j=2

Câˆ†j(z).

(17)

FVN with this modiï¬cation is called Advanced Filtered Velvet
Noise (AFVN) [13]. The delta ï¬ltersâ€™ orders âˆ†K2, Â· Â· Â· , âˆ†KS
are set lower than the initial ï¬lterâ€™s K1 for the efï¬ciency. See
Figure 5b for its time-domain implementation. We used K1 = 8
and Kâˆ† = Kâˆ†2 = Â· Â· Â· = Kâˆ†S = 2 in the experiments. This
results in a total of Â¯K = K1 + (S âˆ’ 1)Kâˆ† = 46 SVFs. The
other settings are the same as the FVN. This AFVN has 360
ARPs to estimate and its time-domain model Ë†H requires 1511
FLOPs. While order of the coloration ï¬lters are higher than the
FVNâ€™s counterparts, the introduced frequency-sampling error
remains approximately the same in practice (see Figure 4b).

C. Differentiable Delay Network

1) Delay Network: By interconnecting multiple delay lines
in a recursive manner, one can obtain a Delay Network (DN)
[22], [23] structure. A difference equation of the DN can be

Fig. 6. Our DN in the time domain. We restrict the general ï¬lter matrices
to accelerate the training and obtain an efï¬cient and controllable model. We
simplify the pre-ï¬lters B and the post-ï¬lters C to have only one coloration
ï¬lter C1 and use the same Câˆ† for all feedback loops. A time-varying mixing
matrix Qn and allpass ï¬lters U are introduced to use DN with small M (i.e.,
fast training) with minimal quality loss.

written in a general form as follows (we omit the bracket
notation for the ï¬ltering),

y[n] = CT Â¯y[n] + Ë†H0x[n],

Â¯y[n + d] = AÂ¯y[n] + Bx[n].

(18a)

(18b)

That is, an input x[n] is distributed and ï¬ltered (or simply
scaled) with B, then go through d-sample parallel delay lines
which are recursively interconnected to themselves through a
mixing ï¬lter matrix A. The delay line outputs Â¯y[n] are ï¬ltered
and summed with C. We add a bypass signal ï¬ltered with Ë†H0,
resulting in an output y[n]. Transfer function of the DN is
Ë†H(z) = C(z)T (Dâˆ’1(z) âˆ’ A(z))âˆ’1B(z) + Ë†H0(z).

(19)

Here, D(z) = diag(zâˆ’d) is an M Ã—M transfer function matrix
for the delay lines, i.e., Dii(z) = zâˆ’di. B(z), C(z), and A(z)
are input, output, and feedback transfer function matrices of
shape M Ã— 1, 1 Ã— M , and M Ã— M , respectively.

2) Differentiable Implementation: The DN components are
recursively interconnected such that one cannot divide its IR
into independent segments and generate them in parallel like
we did with the FVN and AFVN. Instead, we frequency-sample
its entire transfer function to obtain differentiable DN, which is
equivalent to a composition of individually frequency-sampled
transfer function matrices.
N (Dâˆ’1

N âˆ’ AN )âˆ’1BN + ( Ë†H0)N

Ëœh[n] = IFFT

(20)

CT

(cid:110)

(cid:111)

.

Here, DN = diag((zâˆ’d)N ) âˆˆ CM Ã—M Ã—(cid:98)N/2+1(cid:99) is a frequency-
sampled version of the delay line transfer function matrix
D, i.e., (DN )ii = (zâˆ’di)N âˆˆ C(cid:98)N/2+1(cid:99). Likewise, we can
frequency-sample the transfer function matrices B, C, and A
to obtain their approximations BN , CN , and AN , respectively.
Each frequency-sampled transfer function matrix is a batch of
(cid:98)N/2 + 1(cid:99) matrices, the matrix multiplications and inversions
of equation (20) can be performed in parallel.

<04?!["];!<01!<01"+,%;,)!/(/(),3!3,=2â‹®â‹®â‹®â‹®â‹®â‹®â‹®$"=+,23-!"/!>$!">$,"@ALEE et al.: DIFFERENTIABLE ARTIFICIAL REVERBERATION

7

3) Restrictions to the General Model: The presented DN
structure is fully general and able to express many variants,
including the Feedback Delay Network (FDN) [46]. Therefore,
we can also derive their differentiable versions following
the proposed method. However, when following the general
practice that uses many delay lines (e.g., M = 16) for a
high-quality reverberation, its differentiable modelâ€™s frequency-
sampled transfer function matrices consume too much memory,
and their multiplications and inversions bottleneck the training
speed. To tackle this, we reduce M and modify the components
as follows to make DN plausible with low M (see Figure 6).
â€¢ Pre and Post Filter Matrix. Following most previous works
[11], [47], [48], the pre ï¬lter matrix B is set to a (constant)
gain vector b. The post ï¬lter matrix C is a combination
of a gain vector c and a common ï¬lter C1, i.e., C = C1c.
The common ï¬lter C1 is composed of serial KC1 SVFs.
â€¢ Feedback Filter Matrix. While most DN model combines
channel-wise parallel delta-coloration (absorption) ï¬lters
Câˆ† and an inter-channel mixing matrix Q to compose the
feedback ï¬lter matrix A = QCâˆ†. Additionally, we insert
channel-wise allpass ï¬lters U and introduce time-variance
by modulating the mixing matrix Q. This results in the
time-varying feedback ï¬lter matrix An = QnUCâˆ†.
â€¢ Allpass Filters. To achieve faster echo density build-up
without adding more delay lines, we insert serial KU
SAPs in each feedback path. Unlike the FVN and AFVN
cases, we estimate the SAP gains Î³ âˆˆ RM Ã—KU with the
estimation network and frequency-sample the allpass ï¬lter
matrix U for the differentiable model.

â€¢ Time-varying Mixing Matrix. We further reduce the audible
â€œringingâ€ artifacts by modulating the stationary poles with
the time-varying mixing matrix, which is set to Qn =
Q0Rn where Q0 is a Householder matrix and R is a
tiny rotational matrix constructed from a random matrix
[48], [49]. Since the frequency-sampling method is only
applicable to LTI ï¬lters, we ï¬x the mixing matrix to Q0
when obtaining the differentiable model.

â€¢ Absorption Filters. Unlike the conventional FDN [46], we
use the same absorption ï¬lter Câˆ† for every channel. Since
the mixing matrix Qn is always unitary, our DN becomes
stable when Câˆ† has magnitude response less than 1 [48].
We achieve this by using a PEQ with additional constraints
(see Section V-C) as Câˆ†.

â€¢ Bypass FIR. We use a length-Z FIR h0[n] for the bypass.
We summarize the modiï¬cations. The difference equation

of our time-varying DN Ë†H TV

n is

y[n] = C1cT Â¯y[n] + H0x[n],
Â¯y[n + d] = QnUCâˆ† Â¯y[n] + bx[n].

(21a)

(21b)

The transfer function of its LTI approximation Ë†H(z) is given as
equation (19) with the restrictions B(z) = b, C(z) = C1(z)c,
and A(z) = Q0 diag(UN (z) (cid:12) Câˆ†(z)). Hence, the IFFT of

(C1)N cT (Dâˆ’1

N âˆ’ Q0 diag(UN (cid:12) (Câˆ†)N ))âˆ’1b

(22)
summed with the bypass FIR Ë†h0[n] results in our differentiable
DNâ€™s IR Ëœh[n].

Figure 7 shows the EDR errors between the three different

Fig. 7. EDR of differentiable DN, its original DN with two versions (LTI
and time-varying), and EDR errors between those three. Same as the FVN
and AFVN, the frequency-sampling introduce little error, as shown in the
upper right plot. However, rotating the mixing matrix introduces a considerable
amount of EDR error since it modulates the pole positions. The error is mostly
in the low-frequency region, where the poles are sparsely located.

DN modes, i.e., differentiable DN, original LTI, and the time-
varying one. The only difference between the ï¬rst two is the
frequency-sampling; unless the DN decays very slowly, there
is little time-aliasing and EDR error. However, due to the pole
modulation, the EDR error increases signiï¬cantly when the
time-varying mixing matrix is applied. Despite this, we ï¬nd
that other perceptually important factors, such as reverberation
time, remain largely unchanged (see Section VI-A).

4) Model Conï¬guration Details: We used M = 6 delay
lines with delay lengths d = [233, 311, 421, 461, 587, 613]
in samples. We used KU = 4 SAPs for each delay line. We
ï¬xed the SAP delay lengths to Ï„ = [[131, 151, 337, 353],
[103, 173, 331, 373], [89, 181, 307, 401], [79, 197, 281, 419],
[61, 211, 257, 431], [47, 229, 251, 443]]. Note that the SAPs
introduce additional delays so that the effective delay lengths
are [1205, 1291, 1399, 1437, 1547, 1583] or about [25.1, 26.9,
29.1, 29.9, 32.2, 33.0] in milliseconds. Our rotational matrix
satisï¬es R30k = I so that the mixing matrix Qn has a period
of 0.625 second. Both post C1 and absorption ï¬lter Câˆ† have
KC1 = Kcâˆ† = 8 components. We used N = 120k frequency-
sampling points. The order of the bypass FIR is Z = 100. The
âˆˆ R1Ã—KC1 ,
post ï¬lter parameters fC1 , RC1 , mLP
C1
feedback ï¬lter parameters fCâˆ† , RCâˆ† , GCâˆ† âˆˆ RKCâˆ† , pre
and post gain vectors b, c âˆˆ RM Ã—1, SAP gain vector Î³ âˆˆ
RM Ã—KU , and the bypass FIR h0 âˆˆ RZ are the estimation
targets (total 200 ARPs). The resulting time-domain model
Ë†H and the time-varying model Ë†H TV
n consume 889 and 1285
FLOPs, respectively.

, mHP
C1

, mBP
C1

V. ARTIFICIAL REVERBERATION PARAMETER ESTIMATION
WITH A DEEP NEURAL NETWORK

The details of our ARP estimation network are as follows.
As shown in Figure 8, it transforms single channel audio input
(either RIR or reverberant speech) into a shared latent z with
the model/task-agnostic encoder. Then, each ARP-groupwise
layer projects the latent z into an ARP tensor Pi.

Frequency (Hz)601255001k4k16kTime (s)00.511.522.5   0   -20   -40   -60 Differentiable DNFrequency (Hz)601255001k4k16kTime (s)00.511.522.5   0   -20   -40   -60 DNFrequency (Hz)601255001k4k16kTime (s)00.511.522.5Error (dB)   -12   -6   0   6   12Frequency (Hz)601255001k4k16kTime (s)00.511.522.5   0   -20   -40   -60 DN (Time-Varying)Frequency (Hz)601255001k4k16kTime (s)00.511.522.5   -12   -6   0   6   12Frequency (Hz)601255001k4k16kTime (s)00.511.522.5Error (dB)   -12   -6   0   6   12LEE et al.: DIFFERENTIABLE ARTIFICIAL REVERBERATION

8

A. AR-Model/Task-Agnostic Encoder

We ï¬rst transform the reference RIR or reverberant speech
into a log-frequency log-magnitude spectrogram. Then, the
spectrogram goes through ï¬ve two-dimensional convolutional
layers, each followed by a rectiï¬ed linear unit (ReLU) activa-
tion. We apply gated recurrent unit (GRU) layers [50] along
with the frequency axis regarding the channels as features
and the time axis regarding the channels and frequencies as
features, widening the receptive ï¬eld. Finally, we apply two
linear layers followed by layer normalization [51] and ReLU,
resulting in the two-dimensional shared latent z. The detailed
network conï¬gurations are shown in Figure 8, and the encoder
has about 7.3M parameters.

B. ARP-Groupwise Projection Layers

Now we aim to transform the two-dimensional shared latent
z into each ARP tensor Pi which is at most two-dimensional.
To achieve this, we use two axis-by-axis linear layers. We ï¬rst
transpose and apply a linear layer to the latent to match the
ï¬rst axis shape. Then, another transposition and a linear layer
match the other axis. This approach factorizes a single large
layer into two smaller ones, reducing the number of parameters.

C. Nonlinear Activation Functions and Bias Initialization

Each ARP group has different stability conditions and desired
distribution. To satisfy these, we attach a nonlinear activation
to each projection layer and initialize the last linear layerâ€™s
bias as follows. Here, x denotes a pre-activation element.

1) Nonlinear Activation Functions:
â€¢ For the resonance R of the SVF, we use a scaled softplus
Î¶(x) = log(1+ex)/ log(2) to center the initial distribution
at 1 and satisfy the stability condition R > 0.

â€¢ For cutoff f we also have the same stability condition f >
0. However, instead of the softplus, we use tan(Ï€Ïƒ(x)/2)
where Ïƒ(x) = 1/(1 + eâˆ’x) is a logistic sigmoid. With
this activation, Ïƒ(x) = 0 and 1 represent cutoff frequency
of 0Hz and half of the sampling rate, respectively.

â€¢ The delta-coloration PEQ Câˆ† inside the DN must satisfy
the stability condition |Câˆ†(ejÏ‰)| < 1. To achieve this, we
use 10âˆ’Î¶(x) for the component gain G. Also, the shelving
ï¬ltersâ€™ resonances should satisfy R >
2/2 to prevent
their magnitude responses to â€œspikeâ€ over 1. Therefore,
we add

2/2 after the softplus activation.
â€¢ For the feed-forward/back gains Î³ of channel-wise allpass
ï¬lters U inside the DN, we use sigmoid activation Ïƒ(x).

âˆš

âˆš

2) Bias Initialization:
â€¢ We initialize bias of the f projection layer to Ïƒâˆ’1(2Ï‰k/Ï‰s)
where each Ï‰k is desired initial cutoff frequency of kth
SVF. We set Ï‰k = Ï‰min(Ï‰max/Ï‰min)(kâˆ’1)/(Kâˆ’1) such that
the frequencies are equally spaced in the logarithmic scale
from Ï‰min = 40Hz to Ï‰max = 12kHz. To the SVFs inside
the AFVNâ€™s initial and delta-coloration ï¬lters, we perform
a random permutation to the frequency index k and obtain
a new index Â¯k. This prevents SVFs for the latter segments
having higher initial cutoff frequencies.

Fig. 8. Architecture of the ARP estimation network. For each two-dimensional
convolutional (Conv2D) layer, c, k, and s denote number of output channel,
kernel size, and stride, respectively. For each gated reccurent unit (GRU) layer,
n and h denote number of layers and hidden features. Shape and size of the
intermediate output tensors are provided in the parentheses, where F , T , and
C denote frequency, time, and channel axis, respectively. Z1, and Z2 denote
each ARP tensorâ€™s ï¬rst and second axis. We omitted the batch axis.

â€¢ Decoder biases for the SVF mixing coefï¬cients mLP, mBP,
and mHP are initialized to 1, 2, and 1, respectively, so that
each SVFâ€™s initial magnitude response slightly deviates
from 1. This prevents the coloration ï¬ltersâ€™ responses and
the estimation network loss gradients to vanish or explode.
â€¢ For the PEQ gains G of the DNâ€™s delta-coloration ï¬lters,
we set each of their biases to âˆ’10 such that the initially
generated IRs have long enough reverberation time.

D. Loss Function

1) Match Loss: We utilize a multi-scale spectral loss [29]

and deï¬ne a match loss as follows,
LMATCH(h, Ëœh) =

(cid:88)

(cid:13)
(cid:13)|H STFT
(cid:13)

i

i

| âˆ’ | ËœH STFT
i

|

(cid:13)
(cid:13)
(cid:13)1

.

(23)

i denotes STFT with different FFT sizes. We use the FFT sizes
of 256, 512, 1024, 2048, and 4096, hop sizes of 25% of the
respective FFT sizes, and Hann windows. Each spectrogramâ€™s
frequency axis is log-scaled like the encoderâ€™s spectrogram.

2) Regularization: We additionally apply a regularization to
reduce time-aliasing of the estimation. As shown in equation
(6) and (8), pole radii of each SVF affects the reliability of the
frequency-sampled one (hence DAR model) and its parameter

LEE et al.: DIFFERENTIABLE ARTIFICIAL REVERBERATION

9

estimator. Since each SVFâ€™s IR (cSVF
i,k )N [n] can be computed,
we encourage reducing its pole radii by penalizing the IRâ€™s
decreasing speed. Each decreasing speed Î³i,k is calculated by
the ratio of the average amplitude of ï¬rst and last n0 samples
of (cSVF
i,k )N [n]. We ï¬x n0 to N/8. Then, each Î³i,k is weighted
with a softmax function along the SVF-axis to penalize higher
Î³i,k more. Sum of the weighted decreasing speed values results
in the regularization loss LREG as follows,

(cid:80)N

Î³i,k =

LREG =

S
(cid:88)

i=1

i,k )N [n]|

|(cSVF
i,k )N [n]|

n=N âˆ’n0
(cid:80)n0

n=0 |(cSVF
(cid:80)K

k=1 Î³i,keÎ³i,k
(cid:80)K
j=1 eÎ³i,j

,

(24a)

.

(24b)

We omit the regularization loss for the DN networks since
with DN the time-aliasing is inevitable to match the reference
with reverberation time longer than the number of frequency-
sampling points N . Therefore, our full loss function L is
L(h, Ëœh) = LMATCH(h, Ëœh) + Î²LREG(cSVF
N )

(25)

where Î² = 1 for the FVN and AFVN and Î² = 0 for the DN.

VI. EXPERIMENTAL SETUP

A. Data

1) Room Impulse Response: We collected 1835 real-world
RIR measurements from various datasets including OpenAIR
[52] and ACE Challenge [53]. The amount of the RIRs were
insufï¬cient for the training purpose, so we used them only
for the validation and test (836 and 999 RIRs, respectively),
ensuring that each set consists of RIRs from different rooms.
For the training, we synthesized 200k RIRs with shoebox
room simulations using the image-source method [54], [55].
To reï¬‚ect various acoustic environments, we randomized simu-
lation parameters for every RIR synthesis, which include room
size, each wallâ€™s frequency-dependent absorption coefï¬cient,
and source/microphone positions. We tuned the randomization
scheme to match the training dataset to the validation set in
terms of reverberation parameter statistics.

In addition, we pre-processed each RIR as following.
â€¢ We removed the pre-onset part of the RIR. We detected
the onset by extracting a local energy envelope of the RIR
then ï¬nding its maximum point [56].

â€¢ Following the recent RIR augmentation method [57], we
multiplied random gain sampled from U(âˆ’12dB, 3dB) to
the ï¬rst 5ms of each train RIR. This slightly reduces the
average direct-to-reverberant ratio (DRR) of the training
set and matches the validation set. At the evaluation/test,
we omitted this procedure.

â€¢ Finally, we normalized the RIR to have the energy of 1.
2) Reverberant Speech: We used VCTK [58] for dry speech.
We split it into the train (61808), validation (21608), and test
(4912) set so that each set is composed of the speech recordings
from different speakers. We sampled an RIR and a dry speech
sample from their respective datasets and convolved them to
generate reverberant speech. We random-cropped 2.5-second
segment from it for the input.

B. Training

We trained each network with every proposed DAR model
for the three tasks: analysis-synthesis, blind estimation, and
both tasks. For the both-performing ones, we fed an RIR or
reverberant speech in a 50-50% probability.

We set the initial learning rate to 10âˆ’4 for FVN and AFVN
and 10âˆ’5 for DN. We used Adam optimizer [59]. For DN,
gradients were clipped to Â±10 for stable training. After 250k
steps, we performed the learning rate decay. We multiplied
the learning rate by 10âˆ’0.2 and 10âˆ’0.1 every 50k step for the
analysis-synthesis networks and the others, respectively. We
ï¬nished the training if the validation loss did not improve
after 50k steps, which took no more than 500k steps for the
analysis-synthesis and 1M steps for the others.

C. Evaluation Metrics

We evaluated our networks with the match loss LMATCH
and EDR distance, deï¬ned as an average of the absolute
EDR error. Furthermore, we evaluated reverberation parameter
differences, i.e., reverberation time T30, DRR, and clarity C50
difference, denoted as âˆ†T30, âˆ†DRR, and âˆ†C50, respectively
[32], [60]. We calculated both full-band differences and average
of differences measured at octave bands of center frequencies
of 125, 250, 500, 1k, 2k, 4k and 8kHz. Then, we obtained
their respective median values.

We can interpret the reverberation parameter differences by
comparing them with their respective just noticeable differences
[9]. Reported just noticeable differences of T30 from previous
works vary from 5% [61] up to about 25% [62]. For C50, 1dB
[63] and 1.1dB [64] were reported. For DRR it differs for
various range, e.g., 6dB at âˆ’10dB, 2dB at 0dB, 3dB at 10dB,
and 8dB at 20dB [65].

D. Subjective Listening Test

Following the previous researches [10], [41], we conducted a
modiï¬ed MUltiple Stimuli with Hidden Reference and Anchor
(MUSHRA) test [66]. We asked subjects to score similarities
of reverberation between a given reference reverberant speech
h âˆ— x and followings (h and x denote a test RIR and dry speech
signal, respectively).

â€¢ A hidden reference h âˆ— x (exactly the same audio).
â€¢ A lower anchor h(cid:48) âˆ— x obtained by averaging the test set

RIRs [41].

â€¢ Another anchor h(cid:48)(cid:48) âˆ— x obtained by random-sampling an

RIR from the test set [10].

â€¢ Estimations to evaluate Ë†H1(x), Â· Â· Â· , Ë†Hn(x). We used the
AR models Ë†H1, Â· Â· Â· , Ë†Hn to obtain the reverberant speech.
The similarity score ranged from 0 to 100, where we provided
text descriptions for 5 equally-divided ranges (0-20: totally
different, 20-40: considerably different, 40-60: slightly different,
60-80: noticeable, but not much, and 80-100: imperceptible).
While we asked the participants to score both analysis-synthesis
and blind estimation results, we divided them into two separate
pages. As a result, a total of 10 stimulus (3 proposed models, 4
baselines which we will explain in Section VI-E, 2 anchors, and
1 hidden reference) were scored for each task. We conducted

LEE et al.: DIFFERENTIABLE ARTIFICIAL REVERBERATION

10

(a) ARP estimation framework without the DAR models.

(b) RIR estimation with CNN decoder model.

Fig. 9. Baseline methods for the experiments.

the test using the webMUSHRA API [67]. A total of 24 sets
were scored for each task, where the ï¬rst 3 sets were used
for training. A total of 12 people participated where 7 people
were audio engineers and the others were instrument players.
We discarded 1 subjectâ€™s results since he scored the hidden
references lower than 90 for more than 15% of the entire sets.

E. Baselines

We compared our framework with following baselines. Refer

to Appendix B for further details.

1) Parameter-matching Networks: Assuming that the DAR
models are not available, similar to the previous non-end-to-end
approach [10], we generated the training data (reference-and-
ARPs pair) with the DAR models and trained the same networks
with a ARP-matching loss (see Figure 9a). Note that we used
the DAR models simply for the convenience of on-the-ï¬‚y data
generation in GPU, but the same procedure can be performed
with the AR models.

2) CNN Decoder: We trained the same estimation network
but instead of the DAR model we attached a decoder composed
of one-dimensional transposed convolutional layers and TCNs.
With this decoder, the entire network resembles an autoencoder
(see Figure 9b). From now, we denote this model as CNN. We
trained two CNNs, one for each of the two tasks.

Note that we did not compare the our methods with possible
two-stage solutions [32]â€“[39] (see Section II-B3) since each
DAR modelâ€™s architecture became different from its original.

VII. EVALUATION RESULTS

A. Comparison with the Baseline Models

1) Advantage of the End-to-end Learning: Figure 10 com-
pares the proposed end-to-end approach with the baselines on
objective metrics and subjective scores. By a large margin,
the end-to-end model outperformed the non-end-to-end ARP-
matching baselines. The baselines struggled to match the rever-
beration decay, reporting noticeably large reverberation time
differences âˆ†T30 â‰¥ 20%. A two-sided t-test for each AR/DAR
model and task showed that the performance difference was
statistically signiï¬cant (all p < 10âˆ’5). Subjective listening test
results also agreed with the objective metrics; median scores of
the end-to-end approaches were from 70 to 80, showing close
matches to the reference, while the parameter matching models
scored less than 60. Also, refer to Figure 13 that visualizes
each estimation result with an EDR and EDR error plots. This

Fig. 10. ARP estimation results on various tasks with the AR/DAR models.
Here, AS and BE denote analysis-synthesis and blind estimation, respectively.
The Full and Freq denote the full-band reverberation parameter difference
and average of octave-band reverberation parameter differences, respectively.
Along with the proposed approaches and baselines, we additionally report the
lower anchorâ€™s evaluation results and the (hidden) referenceâ€™s MUSHRA score
result. The both-performing networksâ€™ performance are denoted as small dots
in the ï¬gure.

shows that the end-to-end learning enabled by the DAR models
gives a huge performance gain relative to the non-end-to-end
parameter-matching scheme.

2) Advantage of the DSP Prior: Since the CNN baselines
have more trainable parameters and complex architecture, they
reach lower losses than the DAR-equipped networks. However,
they produce audible artifacts. While being more restrictive, the
AR models avoid such artifacts and achieve higher subjective
listening scores than the CNN baselines. Again, these subjective
score differences were statistically signiï¬cant (all p < 10âˆ’5).

0.10.2Match LossASBEASBEASBEASBEFVNAFVNFDNCNNEnd-to-EndParam. MatchCNN DecoderAverage RIRRandom RIRReference05EDR Difference125102040T30 (Full)25102040T30 (Freq)1248DRR (Full)248DRR (Freq)1248C50 (Full)1248C50 (Freq)020406080MUSHRA ScoreLEE et al.: DIFFERENTIABLE ARTIFICIAL REVERBERATION

11

Fig. 12. Magnitude responses of the coloration ï¬lters of the AR models. We
used the proposed analysis-synthesis networks to obtain the ï¬lter parameters
for this plot. Each AR model has a different approach to model the frequency-
dependent characteristic of given reverberation.

reverberation parameter difference only slightly increased (less
than the just noticeable differences), which hint at a possibility
of a universal reference-form-agnostic ARP estimation network.

D. AR Model Efï¬ciency and Estimation Performance

Each AR model approximates reference reverberation dif-
ferently, and consequently, its computational efï¬ciency and its
estimation networksâ€™ performance varies.

1) Filtered Velvet Noise: FVN ï¬lters each source segment
independently (see Figure 12). Such ï¬‚exibility enables its
parameter estimators to perform better than the other AR
modelsâ€™ counterparts experimented in this paper. However,
at the same time, it also has the largest number of ARPs (930),
which makes it cumbersome to control manually. Also, it could
be computationally expensive to use multiple FVN instances
in real-time (2546 FLOPs for each).

2) Advanced Filtered Velvet Noise: Instead, AVFN optimizes
each coloration ï¬lter by decomposing it
into the initial
coloration and delta-coloration ï¬lters, making the model more
controllable and efï¬cient (360 ARPs and 1511 FLOPs) than the
FVN. Such simpliï¬cation costs estimation performance since
the underlying assumption of the optimization that coloration
of real-world RIRs changes gradually does not necessarily hold.
In addition, the AFVN networks could be more challenging
to optimize than the FVNâ€™s since each segmentâ€™s coloration
depends on the previous onesâ€™. Indeed, we empirically observed
that their training losses decreased slower than the FVNâ€™s.

3) Delay Network: In DN, we simpliï¬ed the ï¬lter structure
even further by using a single absorption ï¬lter for each feedback
loop. Similar to the AVFN case, DN trade-offs its estimatorsâ€™
performance with its controllability and efï¬ciency (200 ARPs
and 1285 FLOPs). Again, the estimation performance degra-
dation comes from the DNâ€™s restricted expressibility (forced
exponential decay) and training difï¬culty.

Fig. 11. Reliability of the DAR models. Diff, LTI, and LTV denote the
differentiable, original LTI, and the time-varying model, respectively. The
hatched results are the same values of Figure 10 (analysis-synthesis).

B. Reliability of the DAR Models

We demonstrated in previous sections that each DAR model
is close to the original AR model, making them reliable to use
as an alternative for the training. Here, we empirically validate
this argument again. Figure 11 summarizes the objective metrics
calculated with the DAR and AR models. We used the trained
analysis-synthesis network for this evaluation. First, metrics
calculated between the AR model IR and the DAR model IR
were orders of magnitude smaller than the metrics calculated
with the ground truth. DN was the only exception, showing
more deviation than the other AR models due to the time
variance. Nevertheless, the reverberation parameter differences
between the DN IR and differentiable model IR were less than
the just noticeable difference values (for example, âˆ†T30 < 3%).
Second, as expected with equation (7), each metric difference
was smaller than the corresponding AR-to-DAR conversion
error. As a result, overall evaluation results were very similar
regardless of the used model. In short, the DAR models are
reliable for training in practice.

C. Performance Difference Between Target Tasks

Without surprise, the analysis-synthesis networks performed
better in most metrics than the blind estimation networks since
the reference reverberation is directly given, i.e., the former
ones have fewer burdens than the latter. Likewise, the both-
performing networks (shown as dots in Figure 10) show slightly
degraded results than their single-task counterparts. Yet, the

101102103104Match LossFVNAFVNDNBetween AR/DARvs. Groud-TruthMetric Differences101100EDR Difference101100101T30 (Freq)100101102103104DRR (Freq)Diff vs. LTIDiff LossLTI LossDiff Loss vs. LTI LossDiff vs. LTIDiff LossLTI LossDiff Loss vs. LTI LossDiff vs. LTILTI vs. LTVDiff vs. LTVDiff LossLTI LossLTV LossDiff Loss vs. LTI LossLTI Loss vs. LTV LossDiff Loss vs. LTV Loss100101102103104C50 (Freq)5025025FVNMagnitude (dB)|C1(ej)||C6(ej)||C11(ej)||C16(ej)|5025025AFVNMagnitude (dB)|C1(ej)||C6(ej)|Delta SVFsFull SVFsPreviousDeltaFull|C11(ej)||C16(ej)|2501k4k16kFrequency (Hz)5025025DNMagnitude (dB)|C1(ej)|2501k4k16kFrequency (Hz)|C(ej)|LEE et al.: DIFFERENTIABLE ARTIFICIAL REVERBERATION

12

Fig. 13. EDR plots of the reference RIRs and their estimations with the trained networks. For each reference, a total of twelve networksâ€™ estimations and their
errors are visualized where each network is trained for one of the three AR models (FVN, AFVN, and DN), one of the two tasks (analysis-synthesis and blind
estimation), and one of the two training approaches (end-to-end learning with the proposed DAR models and the baseline non-end-to-end parameter-matching
learning). The number provided above each plot is the EDR distance of the estimation.

Frequency (Hz)601255001k4k16kTime (s)00.511.522.51.6781.4942.9791.8712.9392.080Magnitude (dB)   0   -20   -40   -601.259Error (dB)   -36   -24   -12   0   12   24   363.2532.0643.6821.9462.4873.113Magnitude (dB)   0   -20   -40   -603.082Error (dB)   -36   -24   -12   0   12   24   36Frequency (Hz)601255001k4k16kTime (s)00.511.522.51.9855.3101.8847.8983.1915.198Magnitude (dB)   0   -20   -40   -601.568Error (dB)   -36   -24   -12   0   12   24   363.3396.0113.5215.5044.0238.095Magnitude (dB)   0   -20   -40   -603.189Error (dB)   -36   -24   -12   0   12   24   36Frequency (Hz)601255001k4k16kTime (s)00.511.522.51.4084.6831.7067.7671.8585.725Magnitude (dB)   0   -20   -40   -601.402Error (dB)   -36   -24   -12   0   12   24   362.5604.4612.4463.6402.5157.189Magnitude (dB)   0   -20   -40   -602.609Error (dB)   -36   -24   -12   0   12   24   36ReferenceFiltered Velvet NoiseAdvanced Filtered Velvet NoiseDelay NetworkCNN DecoderEnd-to-endParam. MatchEnd-to-endParam. MatchEnd-to-endParam. MatchAnalysis-SynthesisBlind EstimationAnalysis-SynthesisBlind EstimationAnalysis-SynthesisBlind EstimationLEE et al.: DIFFERENTIABLE ARTIFICIAL REVERBERATION

13

VIII. CONCLUSION

We proposed differentiable artiï¬cial reverberation (DAR)
models that can be integrated with deep neural networks
(DNNs). Among numerous pre-existing artiï¬cial reverberation
(AR) models, we selectively implemented Filtered Velvet
Noise (FVN), Advanced Filtered Velvet Noise (AFVN), and
Delay Network (DN) differentiably. Nonetheless, the proposed
method, which replaces the inï¬nite impulse response (IIR)
components with ï¬nite impulse response (FIR) approximations
via frequency-sampling, is applicable to any other AR model.
Then, using the DAR models, we trained the proposed artiï¬cial
reverberation parameter (ARP) estimation networks end-to-end.
The evaluation results showed that our networks captured the
target reverberation accurately in both analysis-synthesis and
blind estimation tasks. We showed, in particular, that the end-to-
end training signiï¬cantly improves the estimation performance
at the tolerable cost of the approximation errors caused by the
frequency-sampling. Additionally, we demonstrated that the
structural priors of the AR models avoid perceptual artifacts
that a DNN as a room impulse response (RIR) generator might
produce. Thus, our framework successfully combined powerful
and fully general deep learning techniques with well-established
domain knowledge of reverberation.

Here, we outline the remaining challenges and future work.
First, we used the shoe-box simulation to obtain a large amount
of training data, which is slightly different from the real-world
RIRs. Interestingly, energy decay relief (EDR) errors of the
end-to-end models showed similar patterns regardless of the
equipped AR/DAR model (see Figure 13). This suggests that
the data characteristics discrepancy might be the cause of the
performance degradation. Therefore, collecting more real-world
RIRs and using powerful augmentation methods could replace
the simulation-based data and improve the performance. Second,
we only used the RIRs and the reverberant speech signals as
references. However, other possible applications with different
reference types exist (for example, automatic mixing of musical
signals). Finally, we investigated how each AR model can be
compared in terms of its expressive power and difï¬culty of
parameter optimization under the deep learning environment.
The evaluation results revealed a trade-off relationship; when a
more compact AR model is used, the estimation performance
was upper-bounded by its limited expressive power. Also, the
proposed modiï¬cations to the original AR models improved
estimation performance and training speed. All of these indicate
that other than the presented AR models and estimation
networks, there may be more â€œdeep-learning-friendlyâ€ models
and â€œAR-model-friendlyâ€ DNNs that yield better performance-
efï¬ciency trade-offs, and seeking those is left as future work.

APPENDIX A
ABLATIONS AND COMPARISONS OF THE AR/DAR MODELS

TABLE I
ABLATION ON THE AR/DAR MODEL CONFIGURATIONS.

MODEL

LMATCH âˆ†T30 (%) âˆ†DRR (dB) âˆ†C50 (dB)
(Ã—10âˆ’1) FULL FREQ FULL FREQ FULL FREQ

FVN
Ti = 10
âˆ†li = L/S
âˆ’h0[n]
M = 1

AFVN
âˆ’C1
âˆ’âˆ†Cj
âˆ’C1, âˆ†Cj

DN
âˆ’C1
âˆ’Câˆ†
âˆ’C1, Câˆ†
âˆ’Qn
âˆ’U
âˆ’U, Qn

1.236
1.257
1.255
1.250
1.261

1.277
1.446
1.324
1.695

1.334
1.446
1.351
1.729
1.307
1.535
1.251

1.89
2.74
2.56
2.23
2.53

4.77
6.75
7.12
8.91

4.20
6.14
5.43
10.71
4.42

0.94 3.39 0.60 1.95
4.21 0.48 2.25
1.56
2.29
1.48
0.89
3.68
9.88
3.17 11.94 0.81
2.21
0.82
3.65
1.51

9.96
12.60 1.46 8.43
5.01
2.99
32.16
6.62
2.15
28.21

1.59 3.64 0.61 2.06
3.64
2.62
5.89

1.76
0.92
2.64

2.30
1.71
8.47
11.59
3.03
3.69
10.47 20.75
2.38
2.22
31.22
7.83
4.54
4.00
11.62 31.20
2.39
5.93
1.81
12.48
21.10 10.76 1.03
2.28
7.43 10.79 1.00 3.39 0.70 2.09

1.08
2.31
0.94
2.03
1.08
0.88

3.83
4.55
4.71
6.68
3.93
3.44

TABLE II
DIFFERENTIABLE FVN WITH VARIOUS COLORATION FILTERS AND
PARAMETERIZATION APPROACHES.

FILTER

LMATCH âˆ†T30 (%) âˆ†DRR (dB) âˆ†C50 (dB)
(Ã—10âˆ’1) FULL FREQ FULL FREQ FULL FREQ

SSVF
PSVF
PEQ
SBIQ
PBIQ
FIR
LFIR

1.236
1.235
1.254
1.322
1.337
1.370
1.384

1.89 4.20 0.94 3.39 0.60
4.52
0.96
1.95
5.47
0.97
2.50
3.83
1.55
9.44
4.88 11.77 1.57
2.72 10.10 1.80
3.54 11.84 1.64

1.95
0.53 1.86
3.64
1.89
0.62
3.61
2.38
0.59
4.61
4.01
2.19
0.62
3.73 0.51 2.16
2.65
0.62
4.31

Table I summarizes the results. FVN, AFVN, and DN denote
the proposed models. Ti = 10, âˆ†li = L/S, âˆ’h0[n], and
M = 1 denote the proposed FVN model without non-uniform
pulse distance and allpass ï¬lters, non-uniform segmentation, a
deterministic FIR, and ï¬ner segment gains. Their results show
that the proposed modiï¬cations contribute to better performance.
We omitted the same ablations for the AFVN. Instead, we
discarded the initial ï¬lter C1, delta ï¬lter âˆ†Cj, or both. We
also evaluated the DN without the initial ï¬lter C1 or delta
ï¬lter Câˆ†. The results reveal the importance of modelling the
frequency-dependent nature of reverberation.

For DN, âˆ’Qn, âˆ’U, and âˆ’U, Qn denote DN without the
time-varying mixing matrix Qn, the allpass ï¬lters U, and both,
respectively. While âˆ’U, Qn reports best evaluation metrics, its
low echo density produced unrealistic sound. Using both U and
Qn improved plausibility of reverberation with an acceptable
performance degradation. Refer to the provided audio samples.

A. AR/DAR Model Ablations

B. Comparisons on Different Coloration Filters

We conducted ablation studies to verify that the modiï¬cations
to the original AR models improve the estimation performance.
We evaluated the modiï¬ed models with the analysis-synthesis
task since their performance differences remained the same
regardless of the task in our initial experiments.

We compared different IIR/FIR ï¬lters and parameterization
approaches. We evaluated them with the analysis-synthesis task
using FVN. Table II summarizes the results. SSVF, PSVF, PEQ,
SBIQ, PBIQ, FIR, and LFIR denote serial SVF, parallel SVF,
PEQ, serial biquad, parallel biquad, FIR, and linear-phase FIR

LEE et al.: DIFFERENTIABLE ARTIFICIAL REVERBERATION

14

TABLE III
EFFECTS OF FREQUENCY-SAMPLING RESOLUTION AND
REGULARIZATION ON THE MATCH LOSS LMATCH (Ã—10âˆ’1).

FREQUENCY-SAMPLING RESOLUTION

MODEL

Ã—1 (FULL)

Ã—0.5

Ã—0.25

Ã—0.125

DAR

AR

DAR AR DAR AR DAR AR

FVNÎ²=1 1.236 1.236 1.259 1.259 1.252 1.249 1.574 1.570
FVNÎ²=0 1.231 1.237 1.283 1.282 1.264 1.263 1.280 1.773
âˆ’ 1.328 âˆ’ 1.361 âˆ’ 1.393

1.277 1.307

DN

from [29]. The order of every ï¬lter was set to 40. The activation
functions and bias initialization for each ï¬lterâ€™s decoder are
as follows. PSVF has the same setting as SSVF. For SBIQ
and PBIQ decoders, we used activations from [20], and their
biases are initialized to match that of SSVF and PSVF. For
PEQ, unlike DN, we used 10x for the activation of the gain G.
FIR/LFIR have no activation and custom bias initialization.

In spite of having identical expressive power, SSVF/PSVF
performed better than SBIQ/PBIQ by large margins. PEQ
performed slightly worse than SSVF due to its restricted degree
of freedom. FIR/LFIR also performs worse than SSVF/PSVF
and PEQ because they cannot change their frequency responses
radically as other IIR ï¬lters could.

C. Effects of the Frequency-Sampling Resolution

Table III demonstrates the effects of the frequency-sampling
resolution and the regularization loss LREG. From the table,
Ã—x denotes the relative sampling resolution compared to the
default conï¬guration, e.g., Ã—0.5 denotes N = 2k for the FVN
and N = 6k for the DN. For each resolution and regularization
setup, we report match loss with the DAR and AR models.
Results of the differentiable DNs with lower resolutions are
omitted since they are calculated with shorter IRs.

As expected, higher sampling resolution led to smaller loss
difference. Also, introducing the regularization term (Î² = 1) re-
duced the difference further. As a result, the ï¬nal differentiable
FVN model (with full resolution and regularization) showed
little loss difference. Moreover, higher resolution led to better
performance, which might be because increased resolution
helped each network to ï¬nd better ARPs.

APPENDIX B
DETAILS ON THE BASELINE METHODS

A. ARP Match Models

1) Training: We trained the ARP match baseline models as
follows. First, we randomized ARPs P1, Â· Â· Â· , Pn and generated
an IR with using the DAR model. Then, each baseline network
estimated ARPs Ë†P1, Â· Â· Â· , Ë†Pn from the given IR. We evaluated
the estimation and trained the baseline using an ARP-match
loss LARP deï¬ned as follows,
(cid:88)

Î±i

(cid:13)
(cid:13)
(cid:13)fi(Pi) âˆ’ fi( Ë†Pi)
(cid:13)
(cid:13)
(cid:13)1

.

LARP(Pi, Ë†Pi) =

(26)

i

Here, the index i denotes a different ARP tensor, e.g., P0 = g
and P1 = h0. fi(Â·) and Î±i are an elementwise function and a

constant, respectively. We set Î±i = 10 for the all DAR modelsâ€™
bypass FIRs and the DNâ€™s absorption ï¬lter parameters. We
used fi(x) = log10(x) for the FVN and AFVN segment gains.
We chose fi(x) = x and Î±i = 1 otherwise.

2) Network Architecture: We trained an almost identical
network to the proposed network for each model/task. The only
difference is that we added an activation 10âˆ’Î¶(x)/20 to the g
decoders of FVN and AFVN to improve their performance.

âˆš

3) Data Generation: We tuned each DAR modelâ€™s ARP ran-
domization scheme to match the synthesized IRsâ€™ reverberation
parameter statistics to the validation set. For the FVN baselines,
we ï¬rst sampled a reverberation time T30 âˆ¼ Ulog[50ms, 8s)
then generated the segment gains g that match the T30. Here,
Ulog[Â·, Â·) denotes a uniform distribution in log scale. In this
procedure, we also compensated the average pulse distance by
dividing each gain gi by
Ti. For the initial coloration ï¬lter C1,
we ï¬rst generated PEQ parameters as Ï‰i âˆ¼ Ulog[40Hz, 16kHz),
Ri âˆ¼ Ulog[0.2, 5), and Gi âˆ¼ U[âˆ’18dB, 18dB) (we sorted the
cutoff frequencies after the sampling) and derived their SVF
parameters. After that, we perturbed each parameterâ€™s value
slightly. This ARP generation method was motivated by the
observation that the differentiable SVFs act like a relaxed PEQ
(see Figure 12). We gradually changed the parameters of C1 to
obtain C2, Â· Â· Â· , CK, modeling the frequency-dependent decay.
Finally, we set h0[n] as an uniform noise with a gain sampled
from U[âˆ’24dB, 0dB). We followed similar procedures for the
AFVN baselines. One difference is that the ï¬rst delta ï¬lter
Câˆ†2 is randomized and the delta ï¬lters afterwords are slight
deviation of the ï¬rst one. For the DN baselines, we sampled
each absorption PEQâ€™s gain with Gi âˆ¼ U[âˆ’2.8dB, 0dB).

B. DNN Decoder Models

Regarding the ï¬rst and second axis of the shared latent z as
a time and channel axis, respectively, our decoder upsamples
z with seven one-dimensional transposed convolution layers.
Conï¬gurations of these layers are as follows. Channels: 128,
128, 64, 64, 64, 64, then 64. Kernel sizes: 5, 5, 5, 5, 5, 5,
then 3. Strides: 3, 3, 3, 3, 3, 3, then 2. Furthermore, after
each upsampling, we inserted a small temporal convolutional
network with conï¬gurations as follows. Channels: 128, 128, 64,
64, 64, 64, and 64. The number of layers: 3, 3, 2, 2, 2, 2, and
2. Kernel sizes: all 7. Finally, we added 1 Ã— 1 convolution as
the last layer to mix all channels. We used the ReLU activation
between the layers. This decoder has about 2.5M parameters.

APPENDIX C
DETAILS ON THE FREQUENCY-SAMPLING METHOD

A. Proof of Equation 6

With equation (5), the time-aliasing error can be written as

(cid:107)H âˆ’ HN (cid:107)2

2 =

N âˆ’1
(cid:88)

âˆ
(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
h[mN + n]
(cid:12)
(cid:12)

2

+

âˆ
(cid:88)

|h[n]|2 . (27)

m=1

n=0
With the triangle inequality and (cid:107)X(cid:107)2
1, we can upper-
bound (cid:107)H âˆ’ HN (cid:107)2
n=N |h[n]| is an
absolute sum of the tail of the original IR h[n] (n â‰¥ N ). Next,
we expand h[n] with partial fraction expansion [68], [69]. For

n=N
2 â‰¤ (cid:107)X(cid:107)2
N where SN = (cid:80)âˆ

2 with 2S2

LEE et al.: DIFFERENTIABLE ARTIFICIAL REVERBERATION

15

H(z) with M distinct poles where each of them Î½i âˆˆ C has
multiplicity ri âˆˆ N, we can express h[n] as a sum of an FIR
hFIR[n] and IIRs hIIR

i,k[n] where following holds,

H(z) = H FIR(z) +

M
(cid:88)

ri(cid:88)

i=1

k=1

Î¶i,k
(1 âˆ’ Î½izâˆ’1)k
(cid:124)
(cid:125)
(cid:123)(cid:122)
H IIR
i,k(z)

.

(Î¶i,k âˆˆ C) (28)

Then, applying the triangle inequality to the summation gives
an upper bound UN , an absolute sum of the IIRsâ€™ tails.

SN â‰¤

M
(cid:88)

ri(cid:88)

âˆ
(cid:88)

i=1

k=1

n=N

(cid:12)
(cid:12)hIIR

i,k[n](cid:12)

(cid:12) = UN .

(29)

From above equation, each IIR, ï¬rst sum, and second sum show
O(nkâˆ’1|Î½i|n), O(N kâˆ’1|Î½i|N ), and O(N riâˆ’1|Î½i|N ) asymp-
totic behavior, respectively [69]. We conclude the proof:

(cid:107)H âˆ’ HN (cid:107)2 â‰¤

âˆš

2SN â‰¤

âˆš

2UN =

M
(cid:88)

i=1

O(N riâˆ’1 |Î½i|N ).

(30)

B. Proof of Equation 8

We denote H(ejÏ‰) and âˆ‚X/âˆ‚p with H and X (cid:48). X (cid:48)

N denotes
(XN )(cid:48) = (X (cid:48))N . We upper-bound the gradient difference âˆ†G
using integral inequalities as

(cid:90) 2Ï€

âˆ†G â‰¤

|HRef(H (cid:48) âˆ’ H (cid:48)
(cid:123)(cid:122)

â‰¤2(cid:107)HRef(cid:107)2(cid:107)Hâˆ’H (cid:48)

N )|

N (cid:107)2

0

(cid:124)

(cid:90) 2Ï€

+

dÏ‰
Ï€
(cid:125)

0

(cid:124)

|HH (cid:48) âˆ’ HN H (cid:48)

N |

(cid:123)(cid:122)

â‰¤2(cid:107)HH (cid:48)âˆ’HN H (cid:48)

N (cid:107)2

dÏ‰
Ï€
(cid:125)

(31)
where the under-braced ones are the Cauchy-Schwartz inequal-
ities. Considering the ï¬rst integralâ€™s upper bound, (cid:107)HRef(cid:107)2 is a
ï¬nite constant. In (cid:107)H (cid:48) âˆ’ H (cid:48)
N (cid:107)2, H (cid:48) is another stable LTI ï¬lter
if p is well-deï¬ned. If Î½i is a function of p, its multiplicity ri is
doubled in H (cid:48). In the worst case, p controls every pole of H and
i=1 O(N 2riâˆ’1|Î½i|N ) holds. Similarly, the
(cid:107)H (cid:48) âˆ’ H (cid:48)
upper bound of the second integral consists an l2 distance be-
N . Since HH (cid:48) has 2ri or 3ri multiplicity
tween HH (cid:48) and HN H (cid:48)
for each pole Î½i, (cid:107)HH (cid:48) âˆ’ HN H (cid:48)
i=1 O(N 3riâˆ’1|Î½i|N )
holds at the worst case, which concludes the proof.

N (cid:107)2 = (cid:80)M

N (cid:107)2 = (cid:80)M

REFERENCES

[1] J. W. Bayless, â€œInnovations in studio design and construction in the capitol
tower recording studios,â€ Journal of the Audio Engineering Society, vol. 5,
no. 2, pp. 71â€“76, April 1957.

[2] B. A. Blesser, â€œAn interdisciplinary synthesis of reverberation viewpoints,â€
Journal of the Audio Engineering Society, vol. 49, no. 10, pp. 867â€“903,
October 2001.

[3] J. Traer and J. H. McDermott, â€œStatistics of natural reverberation enable
perceptual separation of sound and space,â€ Proceedings of the National
Academy of Sciences, vol. 113, no. 48, pp. E7856â€“E7865, 2016.

[4] V. VÂ¨alimÂ¨aki, J. D. Parker, L. Savioja, J. O. Smith, and J. S. Abel, â€œFifty
years of artiï¬cial reverberation,â€ IEEE Transactions on Audio, Speech,
and Language Processing, vol. 20, no. 5, pp. 1421â€“1448, 2012.

[5] V. VÂ¨alimÂ¨aki, J. Parker, L. Savioja, J. O. Smith, and J. Abel, â€œMore than
50 years of artiï¬cial reverberation,â€ Journal of the Audio Engineering
Society, January 2016.

[6] J. Dattorro, â€œEffect design, part 1: reverberator and other ï¬lters,â€ Journal
of the Audio Engineering Society, vol. 45, no. 9, pp. 660â€“684, 1997.
[7] W. G. Gardner, â€œA real-time multichannel room simulator,â€ Journal of
the Acoustical Society of America, vol. 92, pp. 2395â€“2395, 1992.

[8] E. De Sena, H. HacÎ¹habiboË˜glu, Z. CvetkoviÂ´c, and J. O. Smith, â€œEfï¬cient
synthesis of room acoustics via scattering delay networks,â€ IEEE/ACM
Transactions on Audio, Speech, and Language Processing, vol. 23, no. 9,
pp. 1478â€“1492, 2015.

[9] N. Agus, H. Anderson, J.-M. Chen, S. Lui, and D. Herremans, â€œMinimally
simple binaural room modeling using a single feedback delay network,â€
Journal of the Audio Engineering Society, vol. 66, no. 10, pp. 791â€“807,
October 2018.

[10] A. Sarroff and R. Michaels, â€œBlind arbitrary reverb matching,â€ in
Proceedings of the 23rd International Conference on Digital Audio
Effects, 2020.

[11] J.-M. Jot, â€œAn analysis/synthesis approach to real-time artiï¬cial reverbera-
tion,â€ in [Proceedings] ICASSP-92: 1992 IEEE International Conference
on Acoustics, Speech, and Signal Processing, vol. 2, 1992, pp. 221â€“224.
[12] J. Coggin and W. Pirkle, â€œAutomatic design of feedback delay network
reverb parameters for impulse response matching,â€ Journal of the Audio
Engineering Society, September 2016.

[13] V. VÂ¨alimÂ¨aki, B. Holm-Rasmussen, B. Alary, and H.-M. Lehtonen, â€œLate
reverberation synthesis using ï¬ltered velvet noise,â€ Applied Sciences,
vol. 7, no. 5, 2017.

[14] N. Peters, J. Choi, and H. Lei, â€œMatching artiï¬cial reverb settings to
unknown room recordings: a recommendation system for reverb plugins,â€
Journal of the Audio Engineering Society, October 2012.

[15] F. Gao and W. Snelgrove, â€œAn adaptive backpropagation cascade IIR
ï¬lter,â€ IEEE Transactions on Circuits and Systems II: Analog and Digital
Signal Processing, vol. 39, no. 9, pp. 606â€“610, 1992.

[16] A. D. Back and A. C. Tsoi, â€œFIR and IIR synapses, a new neural network
architecture for time series modeling,â€ Neural Computation, vol. 3, no. 3,
pp. 375â€“385, 1991.

[17] P. Campolucci, A. Uncini, and F. Piazza, â€œFast adaptive IIR-MLP neural
networks for signal processing applications,â€ in 1996 IEEE International
Conference on Acoustics, Speech, and Signal Processing Conference
Proceedings, vol. 6, 1996, pp. 3529â€“3532 vol. 6.

[18] J. D. P. Boris Kuznetsov and F. Esqueda, â€œDifferentiable IIR ï¬lters for
machine learning application,â€ in Proceedings of the 23rd International
Conference on Digital Audio Effects, 2020.

[19] S. Nercessian, â€œNeural parametric equalizer matching using differentiable
biquads,â€ in Proceedings of the 23rd International Conference on Digital
Audio Effects, 2020.

[20] S. Nercessian, A. Sarroff, and K. J. Werner, â€œLightweight and inter-
pretable neural modeling of an audio distortion effect using hypercondi-
tioned differentiable biquads,â€ arXiv preprint arXiv:2103.08709, 2021.
[21] L. Rabiner, B. Gold, and C. McGonegal, â€œAn approach to the approxi-
mation problem for nonrecursive digital ï¬lters,â€ IEEE Transactions on
Audio and Electroacoustics, vol. 18, no. 2, pp. 83â€“106, 1970.

[22] J. Stautner and M. Puckette, â€œDesigning multichannel reverberators,â€

Computer Music Journal, vol. 3, pp. 569â€“582, 1989.

[23] M. Gerzon, â€œSynthetic reverberation, part I,â€ Studio Sound, vol. 13, pp.

632â€“635, 1971.

[24] S. Bai, J. Z. Kolter, and V. Koltun, â€œAn empirical evaluation of generic

convolutional and recurrent networks for sequence modeling,â€ 2018.

[25] C. J. Steinmetz and J. D. Reiss, â€œEfï¬cient neural networks for real-time
analog audio effect modeling,â€ arXiv preprint arXiv:2102.06200, 2021.
[26] C. J. Steinmetz, J. Pons, S. Pascual, and J. Serr`a, â€œAutomatic multitrack
mixing with a differentiable mixing console of neural audio effects,â€
in ICASSP 2021 - 2021 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), 2021, pp. 71â€“75.

[27] S. Billings and S. Fakhouri, â€œIdentiï¬cation of systems containing linear
dynamic and static nonlinear elements,â€ Automatica, vol. 18, no. 1, pp.
15â€“26, 1982.

[28] X. Serra and J. Smith, â€œSpectral modeling synthesis: A sound analy-
sis/synthesis based on a deterministic plus stochastic decomposition,â€
Computer Music Journal, vol. 14, pp. 12â€“24, 1990.

[29] J. Engel, L. H. Hantrakul, C. Gu, and A. Roberts, â€œDDSP: differentiable
digital signal processing,â€ in International Conference on Learning
Representations, 2020.

[30] V. Zavalishin, The Art of VA Filter Design. Native Instruments, 2020.
[31] A. Wishnick, â€œTime-varying ï¬lters for musical applications,â€ in Pro-
ceedings of the 17th International Conference on Digital Audio Effects,
2014.

[32] â€œAcoustics â€” Measurement of room acoustic parameters â€” Part 2:
Reverberation time in ordinary rooms,â€ International Organization for
Standardization, Geneva, CH, Standard, Mar. 2008.

[33] R. Ratnam, D. L. Jones, B. C. Wheeler, W. D. Oâ€™Brien, C. R. Lansing,
and A. S. Feng, â€œBlind estimation of reverberation time,â€ The Journal of
the Acoustical Society of America, vol. 114, no. 5, pp. 2877â€“2892, 2003.

LEE et al.: DIFFERENTIABLE ARTIFICIAL REVERBERATION

16

[60] C. Hak, R. Wenmaekers, and L. Luxemburg, â€œMeasuring room impulse re-
sponses: Impact of the decay range on derived room acoustic parameters,â€
Acta Acustica united with Acustica, vol. 98, 11 2012.

[61] Z. Meng, F. Zhao, and M. He, â€œThe just noticeable difference of noise
length and reverberation perception,â€ in 2006 International Symposium
on Communications and Information Technologies, 2006, pp. 418â€“421.
[62] M. G. Blevins, A. Buck, Z. E. Peng, and L. Wang, â€œQuantifying the
just noticeable difference of reverberation time with band-limited noise
centered around 1000 hz using a transformed up-down adaptive method,â€
June 2013.

[63] J. Bradley, R. Reich, and S. Norcross, â€œA just noticeable difference in
c50 for speech,â€ Applied Acoustics, vol. 58, no. 2, pp. 99 â€“ 108, 1999.
[64] F. Martellotta, â€œThe just noticeable difference of center time and clarity
index in large reverberant spaces,â€ The Journal of the Acoustical Society
of America, vol. 128, pp. 654â€“63, 08 2010.

[65] E. Larsen, N. Iyer, C. Lansing, and A. Feng, â€œOn the minimum audible
difference in direct-to-reverberant energy ratio,â€ The Journal of the
Acoustical Society of America, vol. 124, pp. 450â€“61, 07 2008.

[66] â€œMethod for the subjective assessment of intermediate quality level of
audio systems,â€ International Telecommunication Union Radio Commu-
nication Assembly, Geneva, CH, Standard, Mar. 2014.

[67] M. Schoefï¬‚er, S. Bartoschek, F.-R. StÂ¨oter, M. Roess, S. Westphal,
B. Edler, and J. Herre, â€œwebMUSHRA â€” a comprehensive framework
for web-based listening tests. journal of open research software,â€ Journal
of Open Research Software, vol. 6, p. 8, 2018.

[68] J. Liski, B. Bank, J. Smith, and V. VÂ¨alimÂ¨aki, â€œConverting series biquad
ï¬lters into delayed parallel form: Application to graphic equalizers,â€
IEEE Transactions on Signal Processing, vol. PP, pp. 1â€“1, 05 2019.
[69] J. O. Smith, Introduction to Digital Filters with Audio Applications.

W3K Publishing, 2007.

[34] J. Y. C. Wen, E. A. P. Habets, and P. A. Naylor, â€œBlind estimation of
reverberation time based on the distribution of signal decay rates,â€ in
2008 IEEE International Conference on Acoustics, Speech and Signal
Processing, 2008, pp. 329â€“332.

[35] H. Gamper and I. J. Tashev, â€œBlind reverberation time estimation using
a convolutional neural network,â€ in 2018 16th International Workshop
on Acoustic Signal Enhancement (IWAENC), 2018, pp. 136â€“140.
[36] S. Li, R. Schlieper, and J. Peissig, â€œA hybrid method for blind estimation
of frequency dependent reverberation time using speech signals,â€ in
ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), 2019, pp. 211â€“215.

[37] S. Schlecht and E. Habets, â€œAccurate reverberation time control in
feedback delay networks,â€ in Proceedings of the 20th International
Conference on Digital Audio Effects, September 2017.

[38] K. Prawda, V. VÂ¨alimÂ¨aki, and S. Schlecht, â€œImproved reverberation
time control for feedback delay networks,â€ in Proceedings of the 22nd
International Conference on Digital Audio Effects, September 2019.

[39] E. T. Chourdakis and J. D. Reiss, â€œA machine-learning approach to
application of intelligent artiï¬cial reverberation,â€ Journal of the Audio
Engineering Society, vol. 65, no. 1/2, pp. 56â€“65, January 2017.
[40] A. Richard, D. Markovic, I. D. Gebru, S. Krenn, G. A. Butler, F. Torre,
and Y. Sheikh, â€œNeural synthesis of binaural speech,â€ in International
Conference on Learning Representations, 2021.

[41] C. J. Steinmetz, V. K. Ithapu, and P. Calamia, â€œFiltered noise shaping for
time domain room impulse response estimation from reverberant speech,â€
arXiv preprint arXiv:2107.07503, 2021.

[42] J. O. Smith, Mathematics of the Discrete Fourier Transform (DFT).

http://www.w3k.org/books/: W3K Publishing, 2007.

[43] V. VÂ¨alimÂ¨aki, H.-M. Lehtonen, and M. Takanen, â€œA perceptual study
on velvet noise and its variants at different pulse densities,â€ IEEE
Transactions on Audio, Speech, and Language Processing, vol. 21, no. 7,
pp. 1481â€“1488, 2013.

[44] H. JÂ¨arvelÂ¨ainen and M. Karjalainen, â€œReverberation modeling using velvet

noise,â€ journal of the audio engineering society, March 2007.

[45] M. R. Schroeder and B. F. Logan, â€œâ€˜colorlessâ€™ artiï¬cial reverberation,â€
Journal of the Audio Engineering Society, vol. 9, no. 3, pp. 192â€“197,
July 1961.

[46] J.-M. Jot and A. Chaigne, â€œDigital delay networks for designing artiï¬cial
reverberators,â€ Journal of the Audio Engineering Society, February 1991.
[47] D. Rocchesso and J. O. Smith, â€œCirculant and elliptic feedback delay
networks for artiï¬cial reverberation,â€ IEEE Transactions on Speech and
Audio Processing, vol. 5, no. 1, pp. 51â€“63, 1997.

[48] S. J. Schlecht and E. A. P. Habets, â€œTime-varying feedback matrices in
feedback delay networks and their application in artiï¬cial reverberation,â€
The Journal of the Acoustical Society of America, vol. 138, no. 3, pp.
1389â€“1398, 2015.

[49] S. Schlecht, â€œFdntb: The feedback delay network toolbox,â€ in Proceedings

of the 23rd International Conference on Digital Audio Effects, 2020.

[50] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, â€œEmpirical evaluation of
gated recurrent neural networks on sequence modeling,â€ arXiv preprint
arXiv:1412.3555, 2014.

[51] J. L. Ba, J. R. Kiros, and G. E. Hinton, â€œLayer normalization,â€ arXiv

preprint arXiv:1607.06450, 2016.

[52] S. Shelley and D. Murphy, â€œOpenair: An interactive auralization web
resource and database,â€ 129th Audio Engineering Society Convention
2010, vol. 2, pp. 1270â€“1278, 01 2010.

[53] J. Eaton, N. D. Gaubitch, A. H. Moore, and P. A. Naylor, â€œEstimation of
room acoustic parameters: The ace challenge,â€ IEEE/ACM Transactions
on Audio, Speech, and Language Processing, vol. 24, no. 10, pp. 1681â€“
1693, 2016.

[54] P. Svensson and U. R. Kristiansen, â€œComputational modelling and
simulation of acoutic spaces,â€ Journal of the Audio Engineering Society,
June 2002.

[55] R. Scheibler, E. Bezzam, and I. Dokmanic, â€œPyroomacoustics: A python
package for audio room simulations and array processing algorithms,â€
arXiv preprint arXiv:1710.04196, 2017.

[56] G. Defrance, L. Daudet, and J.-D. Polack, â€œFinding the onset of a room
impulse response: straightforward?â€ The Journal of the Acoustical Society
of America, vol. 124, no. 4, pp. EL248â€“EL254, 2008.

[57] N. J. Bryan, â€œImpulse response data augmentation and deep neural
networks for blind room acoustic parameter estimation,â€ in 2020 IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP), 2020, pp. 1â€“5.

[58] J. Yamagishi, C. Veaux, K. MacDonald et al., â€œCstr vctk corpus: English
multi-speaker corpus for cstr voice cloning toolkit (version 0.92),â€ 2019.
[59] D. P. Kingma and J. Ba, â€œAdam: A method for stochastic optimization,â€

arXiv preprint arXiv:1412.6980, 2017.

