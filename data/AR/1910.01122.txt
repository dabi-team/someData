OpenVSLAM: A Versatile Visual SLAM Framework

Shinya Sumikura
Nagoya University
Aichi, Japan
sumikura@ucl.nuee.nagoya-u.ac.jp

Mikiya Shibuya
Tokyo Institute of Technology
Tokyo, Japan
shibuya.m.ab@m.titech.ac.jp

Ken Sakurada
National Institute of Advanced
Industrial Science and Technology
Tokyo, Japan
k.sakurada@aist.go.jp

9
1
0
2

t
c
O
0
1

]

V
C
.
s
c
[

2
v
2
2
1
1
0
.
0
1
9
1
:
v
i
X
r
a

Figure 1: One of the noteworthy features of OpenVSLAM: 3D scene mapping with various types of camera models.

ABSTRACT
In this paper, we introduce OpenVSLAM, a visual SLAM framework
with high usability and extensibility. Visual SLAM systems are
essential for AR devices, autonomous control of robots and drones,
etc. However, conventional open-source visual SLAM frameworks
are not appropriately designed as libraries called from third-party
programs. To overcome this situation, we have developed a novel
visual SLAM framework. This software is designed to be easily used
and extended. It incorporates several useful features and functions
for research and development. OpenVSLAM is released at https:
//github.com/xdspacelab/openvslam under the 2-clause BSD license.

CCS CONCEPTS
• Software and its engineering → Software libraries and repos-
itories; • Computing methodologies → Scene understanding;
Vision for robotics.

KEYWORDS
Visual SLAM; Visual Odometry; Scene Modeling; Scene Mapping;
Localization; Open Source Software; Computer Vision

ACM Reference Format:
Shinya Sumikura, Mikiya Shibuya, and Ken Sakurada. 2019. OpenVSLAM: A
Versatile Visual SLAM Framework. In Proceedings of the 27th ACM Interna-
tional Conference on Multimedia (MM ’19), October 21–25, 2019, Nice, France.
ACM, New York, NY, USA, 6 pages. https://doi.org/10.1145/3343031.3350539

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
MM ’19, October 21–25, 2019, Nice, France
© 2019 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-6889-6/19/10.
https://doi.org/10.1145/3343031.3350539

1 INTRODUCTION
Simultaneous localization and mapping (SLAM) systems have ex-
perienced a notable and rapid progression through enthusiastic
research and investigation conducted by researchers in the fields
of computer vision and robotics. In particular, ORB–SLAM [10, 11],
LSD–SLAM [4], and DSO [3] constitute major approaches regarded
as de facto standards of visual SLAM, which performs SLAM pro-
cessing using imagery. These approaches have achieved state-of-
the-art performance as visual SLAM. In addition, researchers can
reproduce the behavior of these systems on their computers be-
cause their source code is open to the public. However, they are not
appropriately designed in terms of usability and extensibility as vi-
sual SLAM libraries. Thus, researchers and engineers have to make
a great effort to apply those SLAM systems to their applications. In
other words, it is inconvenient to use existing open-source software
(OSS) for visual SLAM as the basis of applications derived from 3D
modeling and mapping techniques, such as autonomous control
of robots and unmanned aerial vehicles (UAVs), and augmented
reality (AR) on mobile devices. Therefore, it is definitely valuable
to provide an open-source visual SLAM framework that is easy to
use and to extend by users of visual SLAM.

In this paper, we present OpenVSLAM, a monocular, stereo, and
RGBD visual SLAM system that comprises well-known SLAM ap-
proaches, encapsulating them in several separated components with
clear application programming interfaces (APIs). We also provide
extensive documentation for it, including sample code snippets.
The main contributions of OpenVSLAM are

• It is compatible with various types of camera models and

can be customized for optional camera models.

• Created maps can be stored and loaded, then OpenVSLAM

can localize new images using prebuilt maps.

• A cross-platform viewer running on web browsers is pro-

vided for convenience of users.

fisheyeequirectangularperspectivevariouscameratypes360m180m(captured with a hand-held camera)Outdoor 3D mapwithanequirectangularcamera 
 
 
 
 
 
Table 1: Comparison of several open-source visual SLAM frameworks.

OSS license
SLAM method

ORB–SLAM2 [11]
GPLv3
indirect

LSD–SLAM [4]
GPLv3
direct

DSO [3]
GPLv3
direct

ProSLAM [15]
3-clause BSD
indirect

UcoSLAM [9]
GPLv3
indirect + marker

camera model

perspective

perspective

perspective

perspective

perspective

monocular,
stereo, RGBD

setup

map store/load
customizability

monocular

monocular

stereo, RGBD

monocular,
stereo, RGBD
✓

✓

OpenVSLAM (ours)
2-clause BSD
indirect
perspective, fisheye,
equirectangular
monocular,
stereo, RGBD
✓
✓

One of the noteworthy features of OpenVSLAM is that the system
can deal with various types of camera models, such as perspective,
fisheye, and equirectangular, as shown in Figure 1. AR on mobile
devices such as smartphones needs a SLAM system with a regular
perspective camera. Meanwhile, fisheye cameras are often mounted
on UAVs and robots for visual SLAM and scene understanding be-
cause they have a wider field of view (FoV) than perspective cameras.
OpenVSLAM can be used with almost the same implementation
between perspective and fisheye camera models. In addition, it is a
significant contribution that equirectangular images can constitute
inputs to our SLAM system. By using cameras that can capture
omnidirectional imagery, the tracking performance of SLAM can
be improved. Our efforts to make use of equirectangular images for
visual SLAM enable tracking and mapping not to depend on the di-
rection of a camera. Furthermore, OpenVSLAM provides interfaces
that can be employed for applications and researches that use visual
SLAM. For example, our SLAM system incorporates interfaces to
store and load a map database and a localization function based on
a prebuilt map.

We contribute to the community of computer vision and robotics
by providing this SLAM framework with a more lax OSS license
than most of the conventional visual SLAM frameworks, as shown
in Table 1. Additionally, we continuously maintain the software so
that researchers can jointly contribute to its development. Our code
is released at https://github.com/xdspacelab/openvslam.

2 RELATED WORK
2.1 OSS for Scene Modeling
In this section, mapping and localization techniques whose pro-
grams are released as OSS are briefly described. Such techniques are
essential in a wide variety of application scenarios for autonomous
control of UAVs and robots, AR on mobile devices, etc. Some OSS
packages for those tasks using images have been open to the public.
Structure from motion (SfM) and visual SLAM are often em-
ployed as scene modeling techniques based on imagery. Regard-
ing SfM, it is usually assumed that the entire image set is pre-
pared in advance. Then the algorithm performs 3D reconstruction
via batch processing. Concerning visual SLAM, 3D reconstruction
is processed in real-time. Therefore, it assumes that images are
sequentially input. OpenMVG [8], Theia [18], OpenSfM [1], and
COLMAP [16] are well-known OSS packages for SfM. Some SfM
frameworks [1, 8] are capable of dealing with fisheye and equirect-
angular imagery. The compatibility with such images has improved
the performance and usability of SfM packages as 3D modeling

frameworks. Meanwhile, researchers often use visual SLAM, such
as ORB–SLAM [10, 11], LSD–SLAM [4], and DSO [3], for real-time
3D mapping. Unlike some SfM frameworks, most of the visual
SLAM software programs can only handle perspective imagery. In
our case, inspired by the aforementioned SfM frameworks, we do
provide a novel visual SLAM framework compatible with various
types of camera models. We thus aim at improving usability and
extensibility of visual SLAM for 3D mapping and localization.

2.2 Visual SLAM
Some visual SLAM programs are introduced and some of their fea-
tures are explained in this section. Table 1 compares characteristics
of well-known visual SLAM frameworks with our OpenVSLAM.

ORB–SLAM [10, 11] is a kind of indirect SLAM that carries
out visual SLAM processing using local feature matching among
frames at different time instants. In this approach, the FAST algo-
rithm [12, 13] is used for keypoint detection. The binary vector [14]
is then used for its descriptor. Quick methods that can extract key-
points and match feature vectors enable visual SLAM algorithms
to be processed in real-time. Similar approaches are employed in
ProSLAM [15], which is the simple visual SLAM framework for
perspective stereo and RGBD camera systems. UcoSLAM [9] adopts
an algorithm that combines artificial landmarks, such as squared
fiducial markers, and binary descriptor used by ORB–SLAM and
ProSLAM. Meanwhile, LSD–SLAM [4] and DSO [3] are two dif-
ferent approaches of direct SLAM, which realizes visual SLAM
processing directly exploiting brightness information of each pixel
in images. It should be noted that the direct method does not have
to explicitly extract any keypoints from images. Unlike the indi-
rect method, the direct method can be correctly operated in more
texture-less environments because it utilizes whole information
from images. However, the direct method presents more suscepti-
bility to changes in lighting conditions. Additionally, it has been re-
ported that the direct method achieves lower performance than the
indirect one when using rolling shutter cameras [3, 4]. Given that
image sensors in smartphones and consumer cameras are rolling
shutter, OpenVSLAM adopts the indirect method for visual SLAM.
Most of the visual SLAM frameworks cannot store and load
map databases, as highlighted in Table 1. Localization based on
a prebuilt map is important in practical terms for a lot of appli-
cation. Accordingly, it is clear that the ability to store and load
created maps improves the usability and extensibility of a visual
SLAM framework. Therefore, functions for I/O of map databases
are implemented in OpenVSLAM.

users can easily add new camera models (e.g., dual fisheye and cata-
dioptric) by implementing new camera model classes derived from
a base class camera::base. This is a great advantage compared
to other SLAM frameworks because new camera models can be
implemented easily.

It is a noteworthy point that OpenVSLAM can perform SLAM
with an equirectangular camera. Equirectangular cameras, such as
RICOH THETA series, insta360 series, and Ladybug series, have
been recently used to capture omnidirectional images and videos.
In regard to visual SLAM, being compatible with equirectangular
cameras implies a significant benefit for tracking and mapping
because they have omnidirectional view, unlike perspective ones.
To the best of our knowledge, this is the first open-source visual
SLAM framework that can accept equirectangular imagery.

3.3 Map I/O and Localization
As opposed to most of the visual SLAM frameworks, OpenVSLAM
has functions to store and load map information, as shown in Ta-
ble 1. In addition, users can localize new frames based on a prebuilt
map. The map database is stored in MessagePack format, hence the
map information can be reused for any third-party applications in
addition to OpenVSLAM.

4 QUANTITATIVE EVALUATION
In this section, tracking accuracy of OpenVSLAM is evaluated using
EuRoC MAV dataset [2] and KITTI Odomery dataset [5], both of
which have ground-truth trajectories. ORB–SLAM2 [11], the typical
indirect SLAM, is selected for comparison. In addition to tracking
accuracy, tracking times are also compared.

Absolute trajectory error (ATE) [17] is used for evaluation of
estimated trajectories. To align an estimated trajectory and the
corresponding ground-truth, transformation parameters between
the two trajectories are estimated using Umeyama’s method [19].
Sim(3) transformation is estimated for monocular sequences be-
cause tracked trajectories are up-to-scale. On the contrary, SE(3)
transformation is used for stereo sequences. The laptop computer
used for the evaluations equips a Core i7-7820HK CPU (2.90GHz,
4C8T) and 32GB RAM.

4.1 EuRoC MAV Dataset
Figure 3 shows ATEs on the 11 sequences of EuRoC MAV dataset.
From the graph, it is found that OpenVSLAM is comparable to ORB–
SLAM with respect to tracking accuracy for UAV-mounted cameras.
Concerning the sequences including dark scenes (MH_04 and MH_05),
the trajectories estimated with OpenVSLAM are more accurate
than that with ORB–SLAM. This is mainly because frame tracking
method based on robust matching is additionally implemented in
OpenVSLAM.

Subsequently, tracking times measured using the MH_02 sequence
of EuRoC MAV dataset are shown in Figure 4. Mean and median
tracking times are presented in the table as well. From the table,
it is found that OpenVSLAM consumes less tracking time than
ORB–SLAM. This is mainly because the implementation of ORB
extraction in OpenVSLAM is more optimized than that in ORB–
SLAM. In addition, it should be noted that OpenVSLAM requires
less tracking time than ORB–SLAM in later parts of the sequence

Figure 2: Main modules of OpenVSLAM: tracking, mapping,
and global optimization modules.

3 IMPLEMENTATION
OpenVSLAM is mainly implemented with C++. It includes well-
known libraries, such as Eigen1 for matrix calculation, OpenCV2
for I/O operation of images and feature extraction, and g2o [7] for
map optimization. In addition, extensive documentation including
sample code snippets is provided. Users can employ these snippets
for their programs.

3.1 SLAM Algorithm
In this section, we present a brief outline of the SLAM algorithm
adopted by OpenVSLAM and its module structure. As in ORB–
SLAM [10, 11] and ProSLAM [15], the graph-based SLAM algo-
rithm [6] with the indirect method is used in OpenVSLAM. It adopts
ORB [14] as a feature extractor. The module structure of OpenVS-
LAM is carefully designed for the customizability.

The software of OpenVSLAM is roughly divided into three mod-
ules, as shown in Figure 2: tracking, mapping, and global optimiza-
tion modules. The tracking module estimates a camera pose for
every frame that is sequentially inputted to OpenVSLAM via key-
point matching and pose optimization. This module also decides
whether to insert a new keyframe (KF) or not. When a frame is
regarded as appropriate for a new KF, it is sent to the mapping and
the global optimization modules. In the mapping module, new 3D
points are triangulated using the inserted KFs; that is, the map is cre-
ated and extended. Additionally, the windowed map optimization,
called local bundle adjustment (BA), is performed in this module.
Loop detection, pose-graph optimization, and global BA are carried
out in the global optimization module. Trajectory drift, which often
becomes a problem in SLAM, is resolved via pose-graph optimiza-
tion implemented with g2o [7]. Scale drift is also canceled in this
way, especially for monocular camera models.

3.2 Camera Models
OpenVSLAM can accept images captured with perspective, fisheye,
and equirectangular cameras. In regard to perspective and fish-
eye camera models, the framework is compatible not only with
monocular but also with stereo and RGBD setups. Additionally,

1C++ template library for linear algebra: http://eigen.tuxfamily.org/
2Open Source Computer Vision Library: http://opencv.org/

KF†creationlocal BA‡global maplocal mapposeoptimizationtriangulationof 3D pointspose-graphoptimizationkeypointdetectionmatching withlocal maploop detectionglobal BA‡inputframeKF†decision† KeyFrame‡ Bundle Adjustmenttrackingmoduleglobaloptimizationmodulemappingmoduleoptimize a global mapvia pose-graph optimizationand global BAcreate 3D pointsand optimize a mapnearthe current KFestimatea camera poseof every framea whole map created so fara partialmap around thecurrent KF Figure 3: Absolute trajectory errors on the 11 sequences in
EuRoC MAV dataset (monocular). Lower is better.

Figure 5: Absolute trajectory errors on the 11 sequences in
KITTI Odometry dataset (stereo). Lower is better.

ORB–SLAM OpenVSLAM

mean [ms/frame]
median [ms/frame]

27.96
24.97

23.84
23.38

ORB–SLAM OpenVSLAM

mean [ms/frame]
median [ms/frame]

68.78
66.78

56.32
54.45

Figure 4: Tracking times on the MH_02 sequence of EuRoC
MAV dataset (monocular). The table shows mean and me-
dian tracking times on each of the two frameworks. The
graph shows the change in tracking times. Lower is better.

Figure 6: Tracking times on the sequence number 05 of
KITTI Odometry dataset (stereo). The table shows mean and
median tracking times on each of the two frameworks. The
graph shows the change in tracking times. Lower is better.

as shown in the graph. This is because OpenVSLAM efficiently
prevents a local map from being enlarged in the tracking module
when a global map is expanded.

4.2 KITTI Odometry Dataset
Figure 5 shows ATEs on the 11 sequences of KITTI Odometry
dataset. From the graph, it is found that OpenVSLAM has compara-
ble performance to ORB–SLAM with respect to tracking accuracy
for car-mounted cameras.

Subsequently, tracking times measured using the sequence num-
ber 05 of KITTI Odometry dataset are shown in Figure 6. Mean and
median tracking times are also presented in the table. OpenVSLAM
consumes less tracking time than ORB–SLAM for the same rea-
son described in Section 4.1. The difference in the tracking times
shown in Figure 6 is bigger than that in Figure 4 because 1) image
size of KITTI Odometry dataset is larger than that of EuRoC MAV
dataset, and 2) stereo-matching implementation in OpenVSLAM is
also more optimized than that in ORB–SLAM.

5 QUALITATIVE RESULTS
5.1 Fisheye Camera
In this section, experimental results of visual SLAM with a fisheye
camera are presented both outdoors and indoors. A LUMIX DMC-
GX8 which equips an 8mm fisheye lens (Panasonic Corp.) is used
for capturing image sequences. The FPS of the camera is 30.0.

The 3D map shown in Figure 7 is created with OpenVSLAM
using a fisheye video captured outdoor. The number of frames is

about 6400. The difference in elevation is observed from the side-
view of the 3D map. Also, it should be noted that camera pose
tracking succeeded even in high dynamic range scenes.

Figure 8 presents the 3D map built from an indoor fisheye video.
The number of frames is about 6700. It is found that the shape of
the room is reconstructed clearly. In addition, tracking succeeded
even in areas that have less common views (e.g., the right-bottom
image of Figure 8).

These results allow us to conclude that visual SLAM with fisheye

cameras is correctly performed both outdoors and indoors.

5.2 Equirectangular Camera
In this section, experimental results of visual SLAM with a THETA
V (RICOH Co., Ltd.), a consumer equirectangular camera, are pre-
sented.

The 3D map shown in the right half of Figure 1 is created with
OpenVSLAM using an equirectangular video captured outdoor.
The FPS is 10.0 and the number of frames is 15000. It is found that
tracking of camera movement, loop-closing, and global optimization
work well even for the large-scale sequence.

Meanwhile, Figure 9 presents the 3D map based on an indoor
equirectangular video. In this case, the FPS is 10.0 and the number
of frames is 1430. It should be noted that the camera poses are cor-
rectly tracked even in texture-less areas thanks to omnidirectional
observation.

These results allow us to conclude that visual SLAM with equirect-
angular cameras is correctly performed both outdoors and indoors.

00.020.040.060.080.1MH01MH02MH03MH04MH05V101V102V103V201V202V203ATE [m]sequence nameORB-SLAMOpenVSLAMTRACKING LOST1020304050150110011501200125013001tracking time [ms/frame]frame number ORB-SLAM OpenVSLAM00.40.81.21.6200030405060710ATE [m]sequence numberORB-SLAMOpenVSLAM02468100102080940608010012015011001150120012501tracking time [ms/frame]frame number ORB-SLAM OpenVSLAMFigure 7: Mapping result using the outdoor fisheye video.
The top image depicts side-view of the 3D map, while the
center image is top-view. The difference in elevation can be
observed from the side-view.

6 CONCLUSION
In this project, we have developed OpenVSLAM, a visual SLAM
framework with high usability and extensibility. The software is
designed to be easily used for various application scenarios of visual
SLAM. It incorporates several useful functions for research and de-
velopment. In this paper, the quantitative performance is evaluated
using the benchmarking dataset. In addition, experimental results
of visual SLAM with fisheye and equirectangular camera models
are presented. We will continuously maintain this framework for
the further development of computer vision and robotics fields.

ACKNOWLEDGMENTS
The authors would like to thank Mr. H. Ishikawa, Mr. M. Ichihara,
Dr. M. Onishi, Dr. R. Nakamura, and Prof. N. Kawaguchi, for their
support for this project.

REFERENCES
[1] Mapillary AB. 2019. OpenSfM. https://github.com/mapillary/OpenSfM.
[2] Michael Burri, Janosch Nikolic, Pascal Gohl, Thomas Schneider, Joern Rehder,
Sammy Omari, Markus W Achtelik, and Roland Siegwart. 2016. The EuRoC
micro aerial vehicle datasets. International Journal of Robotics Research (IJRR) 35,
10 (2016), 1157–1163. https://doi.org/10.1177/0278364915620033

[3] Jakob Engel, Vladlen Koltun, and Daniel Cremers. 2018. Direct Sparse Odometry.
IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 40, 3
(2018), 611–625. https://doi.org/10.1109/TPAMI.2017.2658577

[4] Jakob Engel, Thomas Schöps, and Daniel Cremers. 2014. LSD–SLAM: Large-Scale
Direct Monocular SLAM. In Proceedings of European Conference on Computer
Vision (ECCV). 834–849. https://doi.org/10.1007/978-3-319-10605-2_54

[5] Andreas Geiger, Philip Lenz, and Raquel Urtasun. 2012. Are we ready for
Autonomous Driving? The KITTI Vision Benchmark Suite. In Proceedings of

Figure 8: Mapping result using the indoor fisheye video. The
center image depicts the top-view of the 3D map. It is found
that the shape of the room is reconstructed well.

Figure 9: Mapping result using the indoor equirectangular
video. Tracking succeeds even in the texture-less areas.

IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 3354–3361.
https://doi.org/10.1109/CVPR.2012.6248074

[6] Giorgio Grisetti, Rainer Kümmerle, Cyrill Stachniss, and Wolfram Burgard. 2010.
A Tutorial on Graph-Based SLAM. IEEE Transactions on Intelligent Transportation
Systems Magazine 2, 4 (2010), 31–43. https://doi.org/10.1109/MITS.2010.939925
[7] Rainer Kümmerle, Giorgio Grisetti, Hauke Strasdat, Kurt Konolige, and Wolfram
Burgard. 2011. g2o: A general framework for graph optimization. In Proceedings
of IEEE International Conference on Robotics and Automation (ICRA). 3607–3613.
https://doi.org/10.1109/ICRA.2011.5979949

[8] Pierre Moulon, Pascal Monasse, Renaud Marlet, et al. 2019. OpenMVG: An Open
Multiple View Geometry library. https://github.com/openMVG/openMVG.

top viewside viewstairshigh dynamic rangestartendloop-closure pointtexture-lessscene[9] Rafael Muñoz-Salinas and Rafael Medina Carnicer. 2019. UcoSLAM: Simultaneous
Localization and Mapping by Fusion of KeyPoints and Squared Planar Markers.
https://doi.org/10.13140/RG.2.2.31751.65440 arXiv:1902.03729

[10] Raúl Mur-Artal, J. M. M. Montiel, and Juan D. Tardós. 2015. ORB–SLAM: a
Versatile and Accurate Monocular SLAM System. IEEE Transactions on Robotics
31, 5 (2015), 1147–1163. https://doi.org/10.1109/TRO.2015.2463671

[11] Raúl Mur-Artal and Juan D. Tardós. 2017. ORB–SLAM2: an Open-Source SLAM
System for Monocular, Stereo and RGB-D Cameras. IEEE Transactions on Robotics
33, 5 (2017), 1255–1262. https://doi.org/10.1109/TRO.2017.2705103

[12] Edward Rosten and Tom Drummond. 2006. Machine Learning for High-Speed
Corner Detection. In Proceedings of European Conference on Computer Vision
(ECCV). 430–443. https://doi.org/10.1007/11744023_34

[13] Edward Rosten, Reid Porter, and Tom Drummond. 2010. Faster and Better: A
Machine Learning Approach to Corner Detection. IEEE Transactions on Pattern
Analysis and Machine Intelligence (TPAMI) 32, 1 (2010), 105–119. https://doi.org/
10.1109/TPAMI.2008.275

[14] Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary Bradski. 2011. ORB:
An efficient alternative to SIFT or SURF. In Proceedings of IEEE International
Conference on Computer Vision (ICCV). 2564–2571. https://doi.org/10.1109/ICCV.

2011.6126544

[15] Dominik Schlegel, Mirco Colosi, and Giorgio Grisetti. 2018. ProSLAM: Graph
SLAM from a Programmer’s Perspective. In Proceedings of IEEE International
Conference on Robotics and Automation (ICRA). 1–9. https://doi.org/10.1109/
ICRA.2018.8461180

[16] Johannes Lutz Schönberger and Jan-Michael Frahm. 2016. Structure-from-Motion
Revisited. In Proceedings of IEEE Conference on Computer Vision and Pattern
Recognition (CVPR). 4104–4113. https://doi.org/10.1109/CVPR.2016.445

[17] Jrgen Sturm, Nikolas Engelhard, Felix Endres, Wolfram Burgard, and Daniel
Cremers. 2012. A Benchmark for the Evaluation of RGB-D SLAM Systems. In
Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS). 573–580. https://doi.org/10.1109/IROS.2012.6385773

[18] Christopher Sweeney, Tobias Hollerer, and Matthew Turk. 2015. Theia: A Fast
and Scalable Structure-from-Motion Library. In Proceedings of the 23rd ACM
International Conference on Multimedia. 693–696. https://doi.org/10.1145/2733373.
2807405

[19] Shinji Umeyama. 1991. Least-Squares Estimation of Transformation Parameters
Between Two Point Patterns. IEEE Transactions on Pattern Analysis and Machine
Intelligence (TPAMI) 13, 4 (1991), 376–380. https://doi.org/10.1109/34.88573

