Teaching a Robot to Walk
Using Reinforcement Learning

Jack Dibachi and Jacob Azoulay
Stanford University
AA228: Decision Making under Uncertainty
dibachi@stanford.edu — jazoulay@stanford.edu

1
2
0
2

c
e
D
3
1

]

G
L
.
s
c
[

1
v
1
3
0
7
0
.
2
1
1
2
:
v
i
X
r
a

Abstract

Classical control techniques such as PID and LQR have
been used effectively in maintaining a system state,
but these techniques become more difﬁcult to imple-
ment when the model dynamics increase in complex-
ity and sensitivity. For adaptive robotic locomotion
tasks with several degrees of freedom, this task be-
comes infeasible with classical control techniques. In-
stead, reinforcement learning can train optimal walk-
ing policies with ease. We apply deep Q-learning and
augmented random search (ARS) to teach a simu-
lated two-dimensional bipedal robot how to walk us-
ing the OpenAI Gym BipedalWalker-v3 environ-
ment. Deep Q-learning did not yield a high reward
policy, often prematurely converging to suboptimal lo-
cal maxima likely due to the coarsely discretized ac-
tion space. ARS, however, resulted in a better trained
robot, and produced an optimal policy which ofﬁcially
“solves” the BipedalWalker-v3 problem. Various
naive policies—including a random policy, a manually
encoded inch forward policy, and a stay still policy—
were used as benchmarks to evaluate the proﬁciency of
the learning algorithm results.

Problem Statement
Creating feedback controllers capable of maintaining stabil-
ity in a bipedal robot is a complicated task, particularly be-
cause of model complexity and sensitivity. Even when con-
trollers can be made, they are extremely sensitive to varia-
tions in the environment, such as irregularities in the walk-
ing surface or unexpected obstacles (Morimoto et al. 2004).
Moreover, mitigating state uncertainty by including range
sensors and contact sensors requires the control designer to
account for any number of possible environments. In short,
creating a multi-input-multi-output feedback controller to
address this control task is intractable.

Instead, the robot’s dynamics can be modeled and tested
in numerous simulation trials, and use reinforcement learn-
ing to determine the optimal policy mapping the robot’s state
to the action required to walk forward. In the OpenAI Gym’s
BipedalWalker-v3 environment, the reward system is
deﬁned by incrementally gaining 300 points if the ﬁnish line
is reached, losing 100 points if robot falls over, and grad-
ually losing points based on the amount of control effort
exerted from the motors. Solving the problem is deﬁned

by collecting over 300 reward points for 100 consecutive
episodes. We use OpenAI Gym’s BipedalWalker-v3
for the robot dynamics and environment, and an augmented
random search (ARS) strategy for determining the optimal
policy.

Other Work
Robot locomotion has been treated as a learning task more
frequently in the past few years. Morimoto et al. apply a
model-based approach to a simple ﬁve-link bipedal robot
using Poincare maps. The policy, however, is only deter-
mined using the robot’s intrinsic state, such as the joint an-
gles, positions, and velocities, rather than including mea-
surements such as foot contacts and range measurements.
This approach, then, cannot be used in environments with
surface irregularities (BipedalWalker-v3) or obstacles
(BipedalWalkerHardcore-v3). Li et al. effectively
implement reinforcement learning to teach the Cassie robot
to walk, though their approach uses a gait library as input
to a deep reinforcement learning network (Li et al. 2021).
While effective, their objective is to enable robust online
planning for environmental uncertainty on physical robotic
platforms. Mania, Guy, and Recht outline an ARS strategy,
validated in a number of MuJoCo locomotion environments
via OpenAI Gym with promising results. ARS is a partic-
ularly useful learning strategy for locomotion tasks, espe-
cially BipedalWalker-v3, as the policy space is ﬁlled
with locally optimal locomotion strategies (Mania, Guy, and
Recht 2018). Other model-free learning strategies require
many more episodes to solve these kinds of locomotion
tasks, if they solve them at all. For this reason, we use ARS
to train the bipedal walker.

Approach
OpenAI Gym’s BipedalWalker-v3 environment pro-
vides a model of a ﬁve-link bipedal robot, depicted in Fig-
ure 1. The robot state is a vector with 24 elements: θ, ˙x, ˙y, ω
of the hull center of mass (white), θ, ω of each joint (two
green, two orange), contacts with the ground (red), and 10
lidar scans of the ground (red line). The action space, then,
is a four element vector representing the torque command to
the motor at each of the four joints (Brockman et al. 2016).
This robot has to navigate through the environment, which is

 
 
 
 
 
 
Figure 1: The robot in the BipedalWalker-v3 environ-
ment. Red ﬂag indicates starting position.

a randomly seeded ground with an uneven surface. The lidar
scans can help detect this unevenness, though they are most
useful in the hardcore version of the environment where ob-
stacles and stairs are present. To ﬁnd the optimal policy,
which maps the robot states to the robot actions yielding the
highest reward, we try both deep Q-learning and ARS.

Deep Q-Learning
Traditional Q-learning techniques incrementally estimate
the action value function Q(s, a), using information at each
step to update the Q-function. This populates a lookup ta-
ble containing action values for every possible state-action
pair. This works well for Markov decision process (MDP)
problems with discrete state and action spaces, however is
insufﬁcient for problems with large continuous state and ac-
tion spaces. Fortunately, the Q-learning algorithm can be
adapted to problems with continuous spaces by using a para-
metric approximation of the action value function Qθ(s, a)
such as a neural network with the weights and biases of the
network serving as the function hyperparameters. The input
layer consists of one node for each state variable and the
output layer consists of nodes representing utilities for each
discretized action.

Deep Q-learning relies on the minimization of the loss
between the approximate action value function Qθ(s, a) and
the true optimal action value function Q∗(s, a) denoted as

l(θ) =

1
2

E

(s,a)∼π∗ [(Q∗(s, a)

Qθ(s, a))2]

−

Applying gradient descent to minimize this loss and using
samples to approximate the expectation results in the update
rule

θ

←

θ + α(Q∗(s, a)

Qθ(s, a))

θQθ(s, a)

−

∇

where α is the learning rate which determines the size
of each update step. Lastly, because the true optimal policy
Q∗(s, a) is unknown, it can be approximated, yielding the
ﬁnalized update rule:

θ

θ + α(r + γ max

Qθ(s(cid:48), a(cid:48))

Qθ(s, a))

θQθ(s, a)

a(cid:48)

←
To implement this algorithm for the bipedal walker, the
continuous action space is ﬁrst discretized into a predeter-
mined number of bins. The continuous action space consists

∇

−

Figure 2: Q-learning neural network structure.

of four action variables (one for each joint), each taking a
1 and 1. These were discretized and con-
value between
1, 0, and 1, which
strained to only take one of three values
results in 34 = 81 possible actions represented as a vector
of four numbers.

−

−

The neural network representation has 24 input nodes,
each representing one of the 24 state variables observed by
the agent. The output layer has 81 nodes (one per possi-
ble action) which correspond to the approximated utilities
of taking an action given the observed state. The output
layer uses a linear activation function in order to produce
unbounded linear utility outputs. There are two hidden lay-
ers, each with 55 nodes using tanh activation functions. A
visual representation of the neural network structure is de-
picted in Figure 2.

To ﬁnd the optimal walking policy, we encourage explo-
ration of the action space using (cid:15)-greedy exploration, where
the robot takes some random action with probability (cid:15). The
amount of exploration needed decreases as the robot im-
proves its gait, so we decay the (cid:15) term over time. Further-
more, to minimize unstable learning, training episodes are
run in batches, with each batch using a static copy of the cur-
rent estimation of the action value function (represented as
a neural network) when computing r + γ maxa(cid:48) Qθ(s(cid:48), a(cid:48)).
The neural network representations and gradient descent
loss minimization are implemented using the Python module
pytorch (Paszke et al. 2019).

Augmented Random Search (ARS)
While Q-learning required the discretization of the action
space, ARS allows for both a continuous state space and
action space, often resulting in efﬁcient optimal policies in
fewer training iterations (2018). ARS explores the policy pa-
rameter space, as opposed to the action and state space, by
taking a set of parameter samples with zero mean Gaussian
noise and executing rollouts for each of those samples.
The parameters of the model θ take the form of an n
m
matrix where n is the number of action variables and m
is the number of state variables. A state observation is ma-
trix multiplied to produce a desired action to take. In ARS,
several parameter noise matrices δ are sampled, resulting in

×

• •• Observation inputlayer with 24 nodes• •• • •• • •• Action-value outputlayer with 81 nodescorresponding to allpossible discretizedactionsTwo hidden layersusing tanh activation,each with 55 nodes.Algorithm 1 Deep Q-Learning Algorithm

Ensure: Initialize hyperparameters: α, γ, action bin size

Initialize neural net Q with weights θ

for batch in training duration do

primary difference between basic random search (BRS) and
augmented random search (ARS). The reward standard de-
viation σ overtime is plotted in Figure 3.

decay((cid:15))

←

(cid:15)

Q

Q

←

for episode in batch size do
reset environment

s

←

while episode is not done do

a =

(cid:26)arg maxa(cid:48) Q(s, a(cid:48)), with prob 1
with prob (cid:15)
random action,

(cid:15)

−

s(cid:48), r = observed new state, reward

Qtarget = r + (1

−
Qoutput = Q(s, a)

done)γ maxa(cid:48) Q (s(cid:48), a(cid:48))

Figure 3: ARS reward standard deviation σ over 1500
episodes.

θ updated with gradient descent:
dθ + ∂(Q target−Q output)2
←
s(cid:48)

dθ

∂θ

s

←

end while

end for

end for

−

θ + vδ and θ
vδ pairs where θ is the current parameteriza-
tion, v is a step size scalar that determines how much noise
is introduced to the model, and δ is a random noise matrix
of the same dimensions as θ. If in total there are h number
of random noise matrices δ then there are 2h matrices, each
of which are used to conduct rollouts. These noisy parame-
terizations all lie within a hypersphere of a speciﬁed radius
around the current parameterization θ (2018).

Rewards for each rollout are collected, and their stan-
dard deviation σ is computed and used to normalize the
ﬁnal update step. The h vδ matrices are then sorted in
descending order of maximum reward using the criteria
max(rθ+vδi , rθ−vδi ) for each parameterization pair i, where
rθ+vδi denotes the reward received from the rollout using a
model parameterization θ + vδi, and rθ−vδi denotes the re-
ward received from the rollout using a model parameteriza-
tion θ
vδi. The top m pairs are then used to update θ using
the following update rule:

−

θ +

θ

←

α
mσ

m
(cid:88)

i=1

[(rθ+vδi −

rθ−vδi)

δi]

∗

where α represents the learning rate.
As the agent learns and the average reward size increases,
rθ−vδi will increase as well,
the summation over rθ+vδi −
resulting in proportionately larger step sizes as the policy
improves overtime. To mitigate this inconsistency in step
size, the learning rate α is normalized by the standard de-
viation of the rollout rewards σ. This normalization is the

Lastly, each state observation is normalized before it is
fed through the model to determine an optimal action, elim-
inating unintentional bias towards state variables with larger
absolute bounds.

Algorithm 2 ARS Algorithm

Ensure: Initialize hyperparameters: α, θ, number of noise
matrices h, noise scalar v, number of best performing
noise matrices m

for episode in training duration do

δ = [δ1 : δh] where each δi is a randomly sampled
zero mean noise sample matrix the size of θ

for each δi in δ do

rθ+vδi = reward collected from rollout using
θ + vδi to determine action a at each step
observation s

rθ−vδi = reward collected from rollout using
vδi to determine action a at each step
θ
observation s

−

r list

end for

←

store (rθ+vδi, rθ−vδi, vδi) in list

σ = std of

rθ+vδ1, rθ−vδ1, ..., rθ+vδh, rθ−vδh }
{
sort r list using criterion max(rθ+vδi, rθ−vδi)

discard h
rewards

−

m worst performing tuples with lowest

θ + α
mσ

(cid:80)m

i=1[(rθ+vδi −

rθ−vδi)

δi]

∗

θ

←
end for

Analysis
The primary metric indicating how well the agent learns is
the change in the average collected reward per episode. The

02004006008001,0001,2001,400020406080100120EpisodeRewardStandardDeviationStandardDeviationofRolloutBatchRewardvsEpisodeExperiment1Experiment2AverageStandardDeviationultimate goal is to achieve high reward collecting perfor-
mance in as few training iterations as possible. The perfor-
mance of each algorithm is also evaluated compared to three
naive approaches: a random policy model, a manually en-
coded policy which consists of a set of actions that inch the
walker forward, and a policy that has the walker take one
step forward and stay still for the duration of the episode
iteration.

With both Q-learning and ARS, there are many hyper-
parameters that can be adjusted when training the bipedal
robot how to walk. For instance, in the above implementa-
tion of Q-learning the following need to be speciﬁed: learn-
ing rate α, exploration parameter (cid:15), discount factor γ, ac-
tion space discretization bin sizes, neural network structure
(number and size of hidden layers), layer activation func-
tions, batch size, and total number of episode iterations.
With ARS, the following hyperparameters must be speci-
ﬁed: learning rate α, noise scalar v, the number of random
noise matrices h, the number of highest ranked matrices to
use m, and the total number of episode iterations. It is infea-
sible to comprehensively explore all combinations of hyper-
parameters. Through experimental trial runs, however, ad-
justing the learning rate proved to result in large improve-
ments in learning.

Results

As benchmarks for the ARS performance, we compare accu-
mulated rewards per episode with other agents. The random
agent selects a random sample along a uniform distribution
from the action space, regardless of the state. Evaluating the
random agent for 1000 episodes gives clusters of accumu-
lated reward just below 0 and just above -100 in Figure 4.
Thus, the agent either falls over after some effort forward, or
gradually loses points by twitching about, but stays upright.
The random agent rarely accumulates a net positive reward
in any individual episode, let alone the success threshold of
300 reward points.

Figure 4: Random policy rolled out for 1000 episodes.

However, the random agent still outperforms the predeter-
mined agent shown in Figure 5, which periodically repeats
an inching forward action regardless of state. This agent has
fewer episodes with zero reward and a higher percentage

Figure 5: Predetermined crawling agent rolled out for 1000
episodes.

Figure 6: Deep Q-Network performance over 10000
episodes with decaying (cid:15)-greedy exploration and learning
rate α = 0.001.

of episodes with approximately -100 reward, as the peri-
odic agent either falls over or expends excess control effort.
Because the periodic agent tends to stand for longer than
the random agent, it spends longer expending control effort
without making up for it by covering linear distance, thus
resulting in a lower average reward than the random agent.
A more advanced benchmark is the deep Q-learning agent
with decaying (cid:15)-greedy exploration. Although this algorithm
takes signiﬁcantly more episodes (and thus run-time) to ob-
tain results, Figure 6 shows the majority of episodes accu-
mulate rewards closer to zero than the random or periodic
agents, outperforming both these agents.

Implementing ARS for the same learning rate gives bet-
ter performance than the Q-learning approach, in that the
average reward is greater than zero, and the robot rarely ac-
cumulates rewards less than zero, as shown in two separate
experiments in Figure 7. This is indicative of the robot taking
a single step to stay upright, then staying still to avoid losing
reward points. However, there is not enough exploration in
the policy space for the agent to take sequential steps. Ag-
gressively increasing the learning rate should remedy this
issue.

The ARS agent with a learning rate of α = 0.02 demon-
strates signiﬁcantly better performance in accumulated re-

02004006008001,000−1000100200300EpisodeRewardAccumulatedRewardvsEpisodeRolloutDataAverageRewardRewardThreshold02004006008001,000−1000100200300EpisodeRewardAccumulatedRewardvsEpisodeRolloutDataAverageRewardRewardThreshold00.20.40.60.81·104−200−1000100200300EpisodeRewardAccumulatedRewardvsEpisodeRolloutDataAverageRewardRewardThresholdFigure 7: ARS learning curve with α = 0.001, averaged
over two separate experiments.

Figure 9: ARS learning curve with α = 0.06, averaged over
two separate experiments.

Algorithms and Run Times

Algorithm
Random
Periodic
Stationary
Deep Q
ARS α = 0.001
ARS α = 0.02
ARS α = 0.06

Total Run Time (s) Episodes
181
298
354
3861
1212
8428
6864

1000
1000
1000
10000
1500
1500
1500

Discussion
When choosing a reinforcement learning method to imple-
ment in a system, there is no all-encompassing algorithm
that universally produces the best results. It is necessary to
not only consider the structure of the problem, but also ex-
periment with various algorithms and various hyperparame-
ters for those algorithms.

In context of the bipedal walker, the realm of possible al-
gorithms to choose from is narrowed due to the large con-
tinuous state space and action space, producing the best re-
sults with a model-free method. While with some continu-
ous problems it is possible to discretize the state space and
action space, doing so is computationally intractable for the
problem at hand since producing a sufﬁciently granular state
space and action space would result in memory overﬂow as
the discretized space scales exponentially with bin size. This
motivated the decision to use deep Q-learning and ARS.

The advantage of deep Q-learning lies in the ability to
have a continuous state space; however, our implementation
still requires discretizing the action-space. The robot did not
yield successful learning results likely because of this dis-
cretization. Due to the exponential growth in discretized ac-
tion space with each additional bin, the possible actions the
agent could execute were very limited compared to the full
continuous action space. Many of the trial runs produced
policies which have the robot take one step forward and stay
still for the duration of the episode. The robot learned that
attempting to move posed a high risk of falling (which is pe-
nalized with a large negative reward), and as a result chooses
to remain still and receive a higher reward, albeit still nega-
tive or close to zero.

Perhaps a more aggressive exploration strategy or a ﬁner

Figure 8: ARS learning curve with α = 0.02, averaged over
three separate experiments.

ward than the other agents we have covered so far. Results
over three experiments are shown in Figure 8, illustrating
the agent’s ability to take multiple steps and maintain bal-
ance. The agent’s learning curve varies broadly over these
experiments because the learning process is stochastic, so
the episode in which the agent learns to take a certain stride
can happen in a wide range of possible times.

Increasing the learning rate to α = 0.06 demonstrates
even higher performance, making sharp increases in reward
accumulation in approximately 100 episodes, and clearing
the reward threshold in less than 400 episodes. Results from
two experiments are shown in Figure 9.

02004006008001,0001,2001,4000100200300EpisodeRewardAccumulatedRewardvsEpisodeRolloutDataRolloutDataAverageRewardRewardThreshold02004006008001,0001,2001,4000100200300EpisodeRewardAccumulatedRewardvsEpisodeRolloutDataRolloutDataRolloutDataAverageRewardRewardThreshold02004006008001,0001,2001,400−1000100200300EpisodeRewardAccumulatedRewardvsEpisodeRolloutDataRolloutDataAverageRewardRewardThresholdReferences
[2016] Brockman, G.; Cheung, V.; Pettersson, L.; Schneider,
J.; Schulman, J.; Tang, J.; and Zaremba, W. 2016. Openai
gym.
[2017] Kuhnle, A.; Schaarschmidt, M.; and Fricke, K. 2017.
Tensorforce: a tensorﬂow library for applied reinforcement
learning. Web page.
[2021] Li, Z.; Cheng, X.; Peng, X. B.; Abbeel, P.; Levine, S.;
Berseth, G.; and Sreenath, K. 2021. Reinforcement learn-
ing for robust parameterized locomotion control of bipedal
robots. CoRR abs/2103.14295.
[2018] Mania, H.; Guy, A.; and Recht, B. 2018. Simple ran-
dom search provides a competitive approach to reinforce-
ment learning. CoRR abs/1803.07055.
[2004] Morimoto, J.; Cheng, G.; Atkeson, C.; and Zeglin, G.
2004. A simple reinforcement learning algorithm for biped
walking. In IEEE International Conference on Robotics and
Automation, 2004. Proceedings. ICRA ’04. 2004, volume 3,
3030–3035 Vol.3.
[2019] Paszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury,
J.; Chanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga,
L.; Desmaison, A.; Kopf, A.; Yang, E.; DeVito, Z.; Raison,
M.; Tejani, A.; Chilamkurthy, S.; Steiner, B.; Fang, L.; Bai,
J.; and Chintala, S. 2019. Pytorch: An imperative style,
high-performance deep learning library.
In Wallach, H.;
Larochelle, H.; Beygelzimer, A.; d'Alch´e-Buc, F.; Fox, E.;
and Garnett, R., eds., Advances in Neural Information Pro-
cessing Systems 32. Curran Associates, Inc. 8024–8035.
[2018] Skow, C. 2018. Coding demos from the school of ai’s
move37 course. Web page.

action space discretization could nudge the agent out of this
local optimum. Furthermore, future work could involve ex-
perimenting with different training batch sizes and overall
length of training.

ARS, however, yielded consistently reliable learning. Ac-
cording to OpenAI, the BipedalWalker-v3 is consid-
ered “solved” when the agent obtains an average reward of
at least 300 over 100 consecutive episodes. Several runs of
ARS in this study achieved this metric, in particular the runs
using learning rates α in the range of [0.01, 0.06]. One of the
beneﬁts of ARS is that it allows for a continuous state space
and continuous action space, an advantage likely contribut-
ing to the remarkable learning.

Future work could involve applying the algorithm to
the BipedalWalkerHardcore-v3 environment, which
poses a more challenging problem introducing obstacles and
pitfalls. Because the hardcore version of the problem still re-
quires the robot to learn how to walk on ﬂat ground, it may
be beneﬁcial to initialize future training models with the out-
put model parameterization θ of the original non-hardcore
version of the problem. Doing so will allow the walker to
spend more time exploring the expanded state space im-
posed by the new obstacles rather than relearning how to
walk as well.

Conclusion
This study presented an application of deep Q-learning and
ARS to control a bipedal robot in a simulated environment.
While deep Q-learning yielded poor results, ARS demon-
strated quick and efﬁcient learning for this application. ARS
can be extended beyond a two-dimensional simulated en-
vironment, and can ultimately be deployed for real world
applications of robot autonomy.

Group Member Contributions

Jacob:
• Deep Q-Learning code using pytorch (2019)
• ARS code (Skow 2018)
• Collected raw training data
• Wrote the following sections of this paper:

Abstract
Approach
Analysis
Discussion
Conclusion

Jack:
• Deep Q-Learning code using TensorForce (2017)
• ARS code (Skow 2018)
• Produced data visualizations using pgfplots
• Wrote the following sections of this paper:

Problem Statement
Other Work
Results
References

