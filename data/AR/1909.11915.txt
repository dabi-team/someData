Unsupervised Image Translation using Adversarial 
Networks for Improved Plant Disease Recognition 

A Preprint 

Haseeb Nazki 
Department of Electronics Engineering 
Chonbuk National University 
Jeonju, South Korea 

Sook Yoon 
Department of Computer Engineering 
Mokpo National University 
Muan, South Korea 

Alvaro Fuentes 
College of Artificial Intelligence 
Tianjin University of Science and 
Technology 
Tianjin, China 

Dong Sun Park 
IT Convergence Research Centre 
Department of Electronics Engineering 
Chonbuk National University 
Jeonju, South Korea 

Abstract 

Acquisition of data  in task-specific applications of machine  learning like  plant disease recognition is a costly endeavor 
owing  to  the  requirements  of  professional  human  diligence  and  time  constraints.  In  this  paper,  we  present  a  simple 
pipeline that uses GANs in an unsupervised image translation environment to improve learning with respect to the data 
distribution in a plant disease dataset, reducing the partiality introduced by acute class imbalance and hence shifting the 
classification decision boundary towards better performance. The empirical analysis of our method is demonstrated on a 
limited dataset of 2789 tomato plant disease images, highly corrupted with an imbalance in the 9 disease categories. First, 
we extend the state of the art for the GAN-based image-to-image translation method by enhancing the perceptual quality 
of the generated images and preserving the semantics. We introduce AR-GAN, where in addition to the adversarial loss, 
our  synthetic  image  generator  optimizes  on  Activation  Reconstruction  loss  (ARL)  function  that  optimizes  feature 
activations  against  the  natural  image.  We  present  visually  more  compelling  synthetic  images  in  comparison  to  most 
prominent existing models and evaluate the performance of our GAN framework in terms of various datasets and metrics. 
Second, we evaluate the performance of a baseline convolutional neural network classifier for improved recognition using 
the resulting synthetic samples to augment our training set and compare it with the classical data augmentation scheme. 
We observe a significant improvement in classification accuracy (+5.2%) using generated synthetic samples as compared 
to (+0.8%) increase using classic augmentation in an equal class distribution environment. 

Keywords:  Generative  Adversarial  Networks;  Image  Translation;  Data  Augmentation;  Convolutional  Neural  Network; 
Plant Disease Recognition; Class Imbalance; Activation Reconstruction loss 

1. Introduction 

Plant pests and diseases affect food crops and hence play a considerable role in the annual yield and economic 
losses in the agricultural sector, threatening the food security of a country (Strange et al., 2005). For instance, during the 
ages,  tomato  production  has  been  experiencing  a  considerable  increase  worldwide,  hence  representing  the  most 
economically  important  vegetable  worldwide  (Bergougnox,  2014).  However,  tomato  plants  are  vulnerable  to  numerous 
syndromes and attacks caused by diseases and pests. There are several reasons that contribute to these effects on the crops: 
(1)  environmental  conditions,  such  as  humidity,  temperature,  light,  nutritional  excess  (fertilizer),  and  species  of  plant 
constituting  the  abiotic  disorders;  (2)  pests,  such  as  whiteflies,  leaf  miners,  worms, bugs,  etc.  that  inhabit  and  colonize 
from plant to plant; and (3) bacteria, viruses, and fungi that constitute to the most common diseases  (Bergougnox, 2014). 
These diseases and pests show different physical characteristics with the plant, such as changes in shape, color, form, etc. 

 
 
 
 
 
         
 
 
 
 
 
of  distinct  parts  in  the  plant.  Therefore,  these  variations  are  hard  to  be  distinguished,  which  furthermore  makes  their 
recognition a challenge for an early detection and treatment that could help avoid several losses in the whole crop. These 
effects are becoming a demanding menace nowadays and need to be approached in the best way and with special attention. 
Furthermore, monitoring the plants for the  symptoms continuously, can prove to be a tedious  task. Hence, efforts have 
been devoted to come up with the approach that could automate the process of recognition of diseases using image data. 
An early recognition of diseases and a proper treatment in due course can be helpful in dealing with huge damages and 
improve the crop yield.  

In  recent  years,  machine  learning-based  methods  have  attained  state-of-the-art performance  in  many  computer 
vision  tasks.  Hence,  several  of  the  recent  approaches  towards  this  plant  disease  recognition  challenge  are  based  on 
training a deep neural network associated with a classifier for the identification of diseases using the images of the plant 
parts (Akhtar et al., 2013; Al-Hiary et al., 2011; Brahimi et al., 2017; Atabay et al., 2017). One of the advantages of using 
deep neural networks is its capability to exploit the raw data directly without using the hand-crafted features. The success 
of machine learning is largely related to the access to enormous amounts of data for training an algorithm and the high 
computing  power  provided  by  graphics  processing  units  (GPUs)  which  makes  it  possible  to  train  these  deep  neural 
networks and enforce the parallelism in data  computing. Compared to shallow learning methods which include support 
vector  machines  (SVM),  decision  trees  and  naÃ¯ve  bayes,  deep  learning  models  exhibit  more  representative  power  by 
passing input data through several non-linearity functions to produce robust descriptive features and perform recognition 
based on those features.  

One of the main challenges in machine learning related to the agricultural imaging domain is how to cope with 
the  small  datasets  and  limited  number  of  annotated  samples,  specifically  when  employing  supervised  machine  learning 
algorithms  that  need  labeled  data  and  large  number  of  training  examples.  Collecting  plant  disease  related  data  is  a 
complex  and  expensive  procedure  and  requires  the  collaboration  of  people  from  different  fields  at  contrasting  stages. 
Although public datasets are  available, most datasets are  still limited in  size  and applicable to specific problems. Also, 
researchers often come across the challenge of class imbalance which has been a general problem in machine learning. 
Using classic data augmentation for enlarging training set and balancing classes has been reported in various literatures 
(Perez et al., 2017; Wong et al., 2016). However, the diversity and variation that can be gained from such modifications of 
the images (such as rotation, translation, flip and scale) is relatively small. This motivates the use of synthetic data, where 
the generated samples introduce more variability and can enrich the dataset further, to improve the recognition training 
process and accuracy.  

Generative Adversarial Networks (GANs) introduced by Goodfellow et al., (2014), have been found successful 
at  various  tasks  of  generating  synthetic  images.  The  major  goal  is  to  generate  synthetic  samples  with  the  same 
characteristics as the given training distribution. Motivated by the success, GANs used in image-to-image translation i.e. 
translation of one possible representation of a scene to another, further proves its representational power (Isola et al., 2017; 
Zhu et al.,2017; Yi et al., 2017). Though, existing state-of-the-art methods (Zhu et al., 2017; Yi et al., 2017) eliminate the 
need  for  paired  data  for  image-to-image  translation,  the generated  images  often  depict  visible  flaws  that  lack  structural 
definition for an object of interest. 

We begin by extending the state of the art for GAN-based image-to-image synthesis to improve the perceptual 
quality  of  the  generated  images  by  preserving  the  structure  of  the  scene.  The  main  reason  to  use  GANs  in  an  image 
translation setting is the effective synthesis of viable output, given a limited amount of input data. Our approach builds on 
CycleGAN and DualGAN (Zhu et al., 2017), where they propose a self-consistency or reconstruction loss that preserves 
the  input  image  after  the  translation  cycle.  Though,  these  approaches  combine  the  adversarial  loss  and  the  cycle-
consistency loss (Zhu et al., 2017) to preserve the semantic affinity and avoid mode collapse but an explicit regularization 
term that could penalize the perceptual quality between the images from the two domains is absent.  

In  this  work  we  introduce  AR-GAN  that  differs  from  the  previous  approaches  by  optimizing  on  an  activation 
reconstruction loss (Johnson et al., 2016; Cha et al. 2017) in addition to regularizing the original GAN objective function 
and cycle-consistency optimizations to present visually more compelling synthetic images on an unaligned dataset. The 
main focus of this work is to analyze the performance of plant disease recognition systems using synthetically generated 
image data. We synthesize high-quality plant disease samples from a finite number of raw images using AR-GAN. We 
further use these synthetic samples for augmentation and balancing the data in the training set. As shown in Fig. 1, our 
pipeline feeds the generated samples into another learning machine (i.e. a Convolutional Neural Network).  Finally, we 
compare the classical and synthetic data augmentation schemes using various metrics. To the best of our knowledge, this 
is the first work that uses GANs to synthetically augment the dataset to improve the plant disease recognition performance. 
Our main contributions are as follows: 

â€¢  We  formulate  unsupervised  AR-GAN  to  learn  one-to-one  mapping  for  high  quality  image  translation  by 

aggregating cycle-consistency and activation reconstruction loss. 

 
Fig.1.  Our  proposed  pipeline  containing  two  components:  a  Synthetic  Data  Augmentation  module  with  AR-GAN  for 
unsupervised learning and a Recognition System with a Convolutional Neural Network for supervised learning. We aim 
to learn more discriminative embeddings with the â€œtraining dataâ€ containing both â€œreal dataâ€ and â€œgenerated dataâ€. 

â€¢  We propose a simple pipeline for synthetic augmentation of plant disease datasets using AR-GAN to improve the 

plant disease recognition performance in a data deficient environment. 

â€¢  We introduce our limited dataset of tomato plant disease images to validate the effectiveness of our pipeline to 
prove  or  disprove  the  hypothesis:  (i)  Does  synthetic  augmentation  improve  the  performance  of  a  deep 
convolutional  neural  network  for  plant  disease  recognition?  (ii)  How  does  synthetic  augmentation  compare  to 
classic augmentation in terms of performance in a plant disease recognition system? 

 2. Materials and Methods 

2.1. Data Augmentation and Generation 

The biggest limitation with machine learning algorithms is that they require huge amounts of training data before 
they become effective. One of the more common and effective ways to obtain more data  is classical data augmentation 
(Perez  et  al.,  2017).  There  are  multiple  ways  to  augment  images,  but  commonly  used  ones  include  traditional 
transformations  like  flipping  images  horizontally,  flipping  images  vertically,  random  crops,  zooms,  rotations,  color 
perturbation,  and  translation.  Interestingly,  augmentation  in  data-space  using  elastic  deformations  has  been  reported  to 
give better improvement in error % than augmentation in feature space (Wong et al., 2016) in some fields like medical 
images. 

Generative network approaches have been extensively used to generate samples in recent years. (Kingma et al., 
2013; Sohn et al., 2015; Mirza et al., 2014; Arjovsky et al., 2017; Goodfellow et al., 2014; Yi et al., 2017) have produced 
nice  samples  on  various  image  datasets  (Radford  et  al.,  2015;  Nguyen  et  al.,  2017;  Yi  et  al.,  2017).  All  the  generative 
models ğº are parameterized by ğœƒ and take as input a random noise ğ‘§ and output a sample ğº(ğ‘§; ğœƒ), such that the output can 
be considered as a sample drawn from a distribution: ğº(ğ‘§; ğœƒ)âˆ¼  ğ‘ğ‘”. The training objective for the generative model ğº is to 
approximate  ğ‘ğ‘‘ using  ğ‘ğ‘”, given the training data x drawn from ğ‘ğ‘‘. We limit our discussion to GANs in the next section. 
The main reason for using GAN based image translation in our study is that in practice, other generative models tend to 
produce blurry images relative to GANs. 

2.2. Generative Adversarial Networks (GANs) 

GANs (Goodfellow et al., 2014) consist of two separate neural networks: a generator G that takes a random noise 
vector  z  and  generates  synthetic  data ğº(ğ‘§);  a  discriminator  D  that  takes  an  input  x  or ğº(ğ‘§) and  output  a  probability 
ğ·(ğ‘¥) or ğ·(ğº(ğ‘§)) to  indicate  whether  the  input  derived  from  the  synthetic  distribution  of ğº(ğ‘§) or  from  the  true  data 
distribution.  

 
 
 
 
 
 
 
GANs (Mirza et al., 2014; Arjovsky et al., 2017; Goodfellow et al., 2014) are a framework of models that learn 
by a two-player game between two different networks: a generator that learns to produce images from a distribution  ğ‘ğ‘‘ 
and a discriminator that learns to discriminate between the generated and the real images. The generator wants to fool the 
discriminator and the discriminator wants to beat the generator. The value function devised to be optimized is represented 
as follows: 

min
ğº

 max
ğ·

 ğ‘‰(ğ·, ğº)

(1) 

=   ğ”¼ğ‘¥~ğ‘ğ‘‘(ğ‘¥)[log ğ·(ğ‘¥)]
+   ğ”¼ğ‘§~ğ‘ğ‘§(ğ‘§)[log(1 âˆ’ ğ·(ğº(ğ‘§)))] 

where ğ‘ğ‘‘(ğ‘¥) denotes the true data distribution and ğ‘ğ‘§(ğ‘§) denote the noise distribution. If the discriminator is trained much 
better than the generator, it can discard the samples from generator with a high confidence (close to 1), and thus the loss 
ğ‘™ğ‘œğ‘”(1  âˆ’  ğ·(ğº(ğ‘§))) would saturate and ğº would not learn anything from zero gradient. To avoid this, instead of training 
ğº to minimizeğ‘™ğ‘œğ‘”(1  âˆ’  ğ·(ğº(ğ‘§))), we train it to maximize ğ‘™ğ‘œğ‘”ğ·(ğº(ğ‘§)). This new adversarial loss function for ğº provides 
the same direction of gradient and does not saturate (Goodfellow et al., 2014).  

The first model (Goodfellow et al., 2014) proposed, uses fully connected layer as its building block. More recent 
approaches (Radford et al., 2015; Dumoulin et al., 2016) successfully demonstrated the use of fully convolutional neural 
networks to achieve better performance, and since then convolution and transposed convolution layers (Dumoulin et al., 
2016) have become the fundamental components in many models.  

While GAN is known to be very effective in image synthesis, its training process is very unstable and requires 
following many guidelines to obtain satisfying results (Radford et al., 2015; Goodfellow et al., 2014). GAN also suffers 
from the problem of mode collapse (Radford et al., 2015; Denton et al., 2015; Goodfellow et al., 2014)] where all points 
collapse to a common single space. Many methods have been proposed to address this problem  (Salimans et al., 2016; 
Che  et  al.,  2016;  Donahue  et  al.  2016).  WGAN  (Arjovsky  et  al.,  2017)  proposes  to  use  the  Wasserstein  distance  to 
measure the similarity between true and the learned data distribution, instead of using Jensen-Shannon divergence as in 
the original GAN model (Goodfellow et al., 2014). While it theoretically avoids mode collapse, it results in the model to 
take  longer  time  to  converge  than  previous  approaches.  To  avoid  this  problem,  WGAN-GP  (Gulrajani  et  al.,  2017) 
suggests using gradient penalty, instead of weight clipping used in WGAN. It produces better images and significantly 
avoids  mode  collapse  and  is  also  easy  to  apply  to  the  training  framework  of  other  models.  GANs  have  demonstrated 
potential in generating images for specific fields like image in-painting, image super resolution, text-to-image synthesis, 
image-to-image translation (Creswell et al., 2018). In our work, we do not focus on investigating all these sophisticated 
GAN algorithms. Instead, we use GANs for image translation to generate samples from the training data. 

2.3. Image-to-Image Translation 

Image-to-Image  translation  is  defined  as  the  task  of  translation  of  one  possible  representation  of  a  scene  to 
another, such as mapping grayscale images to RGB or the other way around (Isola et al., 2017). GANs have been used for 
image-to-image  translation  in  both  supervised  as  well  as  unsupervised  settings.  By  supervised  we  mean  that  there  is  a 
corresponding ground-truth image in the target domain (Isola et al., 2017). Our approach builds on unsupervised image-
to-image  translation  GAN  architectures  where  they  propose  a  self-consistency  or  reconstruction  loss  that  preserves  the 
input  image  after  the  translation  cycle  (Zhu  et  al.,  2017;  Yi  et  al.,  2017;  Kim  et  al.,  2017).  They  all  share  the  same 
framework  with  two  generators  (ğºğ´ğµ and ğºğµğ´)  doing  opposite  transformations,  which  can  be  seen  as  a  kind  of  dual 
learning (He et al., 2016) and two discriminators ğ·ğ´ and ğ·ğµ  that predict whether an image belongs to that domain or not. 
Among  recent  models  that  use  unpaired  data  (Aytar  et  al.,  2017;  Liu  et  al.,  2016)  a  weight-sharing  strategy  to  learn 
representation  across  domains  is  reported.  Concurrently,  other  works  (Taigman  et  al.,  2016;  Shrivastava  et  al.  2017; 
Bousmalis et al., 2017) encourage the input and output to share some specific feature contents. These methods use GANs 
with additional terms to enforce a close association between input and output in terms of image pixel space, class label 
space, and image feature space. 

2.4. Plant disease recognition 

Plant  diseases  recognition  is  a  critical  topic  that  is  being  studied  through  the  years  and  is  motivated  by  the 
requirement of a healthy agricultural output  (Nutter et al., 2016; Bergougnox, 2014; Strange et al., 2005). The research 
associated can be broadly categorized as (i) Shallow learning approaches that use hand crafted features (ii) Deep learning 
approaches using deep convolutional neural networks to learn the feature representations. In disease recognition, shallow 
learning approaches have been applied for classifying tomato powdery mildew against healthy leaves by means of thermal 

 
and  stereo  images  (Prince  et  al.,  2015),  detecting  yellow  leaf  curl  virus  in  tomatoes  by  using  a  set  of  classic  feature 
extraction  steps,  classified  by  SVM  (Mokhtar  et  al.,  2015),  recognition  of  tomato  diseases  in  a  greenhouse  (Chai  and 
Wang, 2013) etc. Moreover, there has been a lot of research going on in this field using plant leaf analysis  together with 
machine  learning  (Singh et al., 2016). Deep learning approaches were  introduced in plant image  recognition as its first 
application  came  up  with  the  identification  of  plant  based  on  leaf  vein  patterns  (Grinblat  et  al.,  2016).  They  classified 
three leguminous plant species: white bean, red bean and soybean using CNNs and experimented using 3-6 layers. Ghazi 
et al., (2017) used three popular CNN architectures, AlexNet (Krizhevsky et al., 2012), VggNet (Simonyan and Zisserman, 
2014),  and  GoogLeNet  (Szegedy  et  al.,  2015),  and  evaluated  the  numerous  factors  affecting  the  performance  of  these 
networks  on  task  of  plant  species  identification.  They  use  Transfer  learning  to  fine-tune  the  pre-trained  models,  using 
LifeCLEF plant dataset (GoÃ«au et al., 2014) and applied classic data augmentation techniques based on image transforms 
such as rotation, translation, reflection, and scaling to decrease the chance of overfitting.  

Other applications of the deep CNNs in plant disease recognition include the  diagnosis of crop leaf disease in 
(Mohanty et al., 2016), where they used a deep convolutional neural network to identify 26 diseases and 14 crop species 
using  a  publicly  available  PlantVillage  dataset  (Hughes  et  al.,  2015)  with  over  50,000  images.  Among  the  recently 
published  works  in  the  literature  [(Brahimi  et  al.,  2017;  Atabay  et  al.,  2017)],  they  use  transfer  learning  of  pre-trained 
models and introduce deep residual learning (He et al., 2016) to identify tomato plant diseases of the PlantVillage dataset. 

2.5. Network Overview and Architecture 

As mentioned earlier, the goal of our proposed method is to  observe the improvement in the accuracy of plant 
disease  recognition  systems  suffering  with  class  imbalance  and  sample  deficiency  in  the  training  data.  To  enlarge  the 
training dataset while keeping classes balanced, the system requires an additional data augmentation measure. We propose 
to synthetically generate additional training data using GAN and further train the recognition network using that data in 
addition  to  the  original  data. Further,  we  also  investigate  the  effectiveness  of  the  synthetic  data  augmentation  by  GAN 
over classical data augmentation for improved recognition.  

In this section we  elaborate on the pipeline of the proposed method.  As shown in Fig. 1, our proposed system 
consists of two blocks: a) Synthetic Data Augmentation block using AR-GAN for data synthesis and class balancing b) 
Recognition System that uses a deep convolutional neural network for image recognition. The real data is used to train the 
AR-GAN model which will generate more training data. The generated data from  AR-GAN is added to the real data to 
yield training data for the CNN. In the following section, we introduce the architecture for our AR-GAN, followed by the 
baseline CNN architecture used to analyze the recognition performance. 

2.5.1. AR-GAN 

A framework of our AR-GAN is depicted in  Fig. 2.  The AR-GAN improves on  CycleGAN  (Zhu et al.,2017), 
unsupervised  image-to-image  translator,  by  introducing  an  activation  reconstruction  module  consisting  of  a  feature 
extraction network to calculate Activation Reconstruction Loss (ARL) between an image and its translation, in addition to 
the cycle-consistency loss and adversarial loss of CycleGAN. 

The  objective  of  unsupervised  image-to-image  translation  is  to  unveil  the  mappings  between  two  distinct 
domains ğ´ to ğµ and vice versa, using unpaired samples from distributions ğ‘ğ‘‘(ğ‘) and ğ‘ğ‘‘(ğ‘) in each domain. This can be 
expressed  as  a  conditional  generative  modeling  task  where  we  try  to  approximate  the  true  conditionals  ğ‘(ğ‘|ğ‘) and 
ğ‘(ğ‘|ğ‘) using  samples  from  the  true  marginals.  A  significant  postulation  here  is  that  elements  in  domains ğ´ and ğµ are 
highly  correlated;  otherwise,  it  is  questionable  that  the  model  would  uncover  a  meaningful  relationship  without  any 
pairing  information.  The  CycleGAN  model  (Zhu  et  al.,2017)  approximates  these  conditionals  using  two  mappings  
ğºğ´ğµ: ğ´ â†’ ğµ and ğºğµğ´: ğµ â†’ ğ´, parameterized by deep networks, which satisfy the following constraints: 1) The output of 
each  mapping  must  match  the  empirical  distribution  of  the  target  domain,  when  focused  over  the  source  domain.  2) 
Mapping an element from one domain to the other, and then back, should deliver a sample close to the element that we 
initially started with, thus showing a cyclic-consistency. The first constraint is satisfied using the GAN (Goodfellow et al., 
2014).  Applying  this  matching  on  target  domain  ğµ ,  marginalized  over  source  domain  A,  includes  minimizing  an 
adversarial objective with respect to ğºğ´ğµ: 

â„’ğºğ´ğ‘(ğºğ´ğµ, ğ·ğµ) =   ğ”¼ğ‘~ğ‘ğµ(ğ‘)[ğ‘™ğ‘œğ‘” ğ·ğµ(ğ‘)]

(2) 

+ ğ”¼ğ‘~ğ‘ğ´(ğ‘)[ğ‘™ğ‘œğ‘”(1 âˆ’ ğ·ğµ(ğºğ´ğµ(ğ‘))] 

as  the  discriminator  DB  is  trained  to  maximize  it.  A  parallel  adversarial  loss â„’ğºğ´ğ‘(ğºğµğ´, ğ·ğ´) is  defined  in  the  opposite 
direction. 

 
Fig. 2. Framework of AR-GAN. A and B are two unaligned domains. Two generators (ğºğ´ğµ, ğºğµğ´) translate an image from 
one domain to another. There is a discriminator for each domain (ğ·ğ´,ğ·ğµ) that judges if an image belongs to that domain. 
Two cycles of data flow, the green one performs a domain transfer A â†’ B â†’ A, while the red one is B â†’ A â†’ B. L1 
loss  is  applied  on  the  input  a  (or  b)  and  the  reconstructed  input    ğºğµğ´(ğºğ´ğµ(ğ‘)) (or   ğºğ´ğµ(ğºğµğ´(ğ‘)) )  to  enforce  self-
consistency. In addition to that, a common feature extraction network calculates activation reconstruction loss between  a 
and ğºğ´ğµ(ğ‘): â„’ğ´ğ‘…ğ¿ğ‘“ or b and ğºğµğ´(ğ‘): â„’ğ´ğ‘…ğ¿ğ‘ or both. 

Cycle-consistency  imposes  that,  when  initializing  with  a  sample  â€œğ‘ â€  from ğ´,  the  reconstructed ğ‘â€² = ğºğµğ´ (ğºğ´ğµ(ğ‘)) 
remains  close  to  the  original  â€œğ‘â€.  For  image  domains,  closeness  between ğ‘ and ğ‘â€²  is  typically  measured  with ğ¿1 or 
ğ¿2 norms. When using the ğ¿1 norm, cycle-consistency starting can be formulated as: 

â„’ğ‘ğ‘¦ğ‘   (ğºğ´ğµ, ğºğµğ´) = ğ”¼ğ‘~ğ‘ğ´(ğ‘)   âƒ¦   ğºğµğ´(ğºğ´ğµ(ğ‘)) âˆ’ ğ‘      âƒ¦
+ ğ”¼ğ‘~ğ‘ğµ(ğ‘)  âƒ¦   ğºğ´ğµ(ğºğµğ´(ğ‘)) âˆ’ ğ‘     âƒ¦ 

(3) 

Finally, the full objective is given by: 

â„’ğ‘ğ‘¦ğ‘ğ‘™ğ‘’ğºğ´ğ‘(ğºğ´ğµ, ğºğµğ´, ğ·ğ´, ğ·ğµ)

(4) 

= â„’ğºğ´ğ‘  (ğºğ´ğµ, ğ·ğµ) 
+ â„’ğºğ´ğ‘  (ğºğµğ´, ğ·ğ´)
+ Î±â„’ğ‘ğ‘¦ğ‘(ğºğ´ğµ, ğºğµğ´) 

where Î± is a hyper-parameter that stabilizes the two constraints. The adversarial terms encourage generation of realistic 
samples in either domain while cycle-consistency ensures a strong relationship between these domains. 

 
 
 
 
 
 
 
We  adopt  the  activation  reconstruction  loss â„’ğ‘ğ‘ğ‘¡ (Johnson  et  al.,  2016;  Cha  et  al.  2017)  as  a  perception  loss 
trained along with the other loss functions aiming to enforce perceptual realism between the real and the generated images 
and increasing the stability of the model. â„’ğ‘ğ‘ğ‘¡ encourages high-level feature representations of the images to be alike. Let 
ğ‘› be  the  activation  outputs  of  the  nth  layer  within  any  convolutional  recognition  network  (feature  extraction 
ğ‘› and ğ´ğ‘
ğ´ğ‘
network) where a and b are used as inputs respectively. Then â„’ğ´ğ‘…ğ¿ is defined as:  

â„’ğ´ğ‘…ğ¿ =

1
ğ‘š

â€–ğ´ğ‘

ğ‘› âˆ’ ğ´ğ‘

ğ‘›â€–ğ¹
2  

(5) 

where â€–Â·â€–ğ¹ represents  the  Frobenius  norm, ğ‘š is  the  shape  of  the  feature  map  and ğ‘› is  nth  layer  used  from  the  feature 
extraction network. From equations (4) and (5) our total objective function can be summed up as: 

â„’ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ =    â„’ğ‘ğ‘¦ğ‘ğ‘™ğ‘’ğºğ´ğ‘ (ğºğ´ğµ,  ğºğµğ´, ğ·ğ´, ğ·ğµ) +  Î» â„’ğ´ğ‘…ğ¿ 

(6) 

where  Î»  is  the  hyper-parameter  to  regulate  the â„’ğ´ğ‘…ğ¿term.  The  resemblance  to  the  new  domain  and  faithfulness  to  the 
original image is a trade-off determined by the weight ğœ† of the ARL term relative to the image adversarial term. If ğœ† is set 
too large, the translated images are close to the input but cannot capture the features of the other domain and if ğœ† is set too 
small, the translated images fail to pertain the perception value of the input. In our experiments, we start by setting ğœ†  =
 0, which means we only use the cycle-adversarial constraint to train the generator. Then we gradually increment ğœ† which 
leads to the increase in the aesthetic and perceptive quality until a threshold is reached. The value of ğœ† is largely a domain 
dependent hyper-parameter. 

2.5.1.1. Architecture 

The architectures for the generator and discriminator are similar to (Johnson et al., 2016; Zhu et al., 2017). ğºğ´ğµ 
and  ğºğµğ´ contain  9-residual  blocks,  pair  of  convolution  block  with  stride  2  and  another  pair  of  fractionally  strided 
convolution block with stride 1/2. For ğ·ğ´, ğ·ğµ we use 70 Ã— 70 PatchGAN (Isola et al., 2017; Zhu et al., 2017) discriminator, 
which aims at classifying whether 70 Ã— 70 overlapping image patches are real or fake. Such a discriminator architecture 
working at a patch-level, has fewer parameters than any full-image discriminator and can work on arbitrarily-sized images 
in a fully convolutional fashion (Isola et al., 2017). The feature extraction network, to calculate â„’ğ´ğ‘…ğ¿, consists of a down 
convolution layer followed by 4-residual blocks.  
Let: - 

â€¢  C7-k-n: 7Ã—7 Convolution Instance Norm ReLU layer with n filters and stride k. 
â€¢  D3-2-n: 3Ã—3 Convolution Instance Norm ReLU layer with n filters and stride 2. 
â€¢  R3-n: Residual block that contains two 3 Ã— 3 Convolutional layers with the same n number of filters on both 

layers.  

â€¢  U3-n: 3Ã—3 fractional strided Convolution Instance Norm ReLU layer with n filters and stride 1/2. 
â€¢ 

P4-2-n: 4 Ã— 4 Convolution Instance Norm Leaky ReLU layer with n filters and stride 2.  

Following (Zhu et al., 2017) the Generator network with 9 blocks consists of:  
C7-1-32, D3-2-64, D3-2-128, R3-128, R3-128, R3-128, R3-128, R3-128, R3-128, R3-128, R3-128, R3-128, U3-64, U3-32, 
C7-1-3 

The discriminator architecture is:  
P4-2-64, P4-2-128, P4-2-256, P4-2-512 
where we again follow [(Zhu et al.,2017)] and for P4-2-64 circumvent instance normalization. 

The feature extraction network to calculate â„’ğ´ğ‘…ğ¿ is given by: 
C7-2-64, R3-64, R3-128, R3-256, R3-512 

The  feature  extraction  network  is  warm-started  with  pre-trained  ImageNet  weights.  We  did  some  extensive 
experiments  to  try  different  combinations  of  feature  layers  to  obtain  the  best  results.  Using  all  layers  ensures  the 
preservation of low-level as well as high level traits resulting in best performance. The network architecture trains on and 
outputs  images  of  size  256Ã—256.  For  calculating  â„’ğ‘ğ‘¦ğ‘ğ‘™ğ‘’ğºğ´ğ‘ (ğºğ´ğµ, ğºğµğ´, ğ·ğ´, ğ·ğµ),  we  use  least  squared  loss  instead  of  log 
likelihood for generating higher quality results. Weights were initialized from a Gaussian distribution with 0 mean and 
0.02 standard deviation. We train for 200â€“300 epochs for different datasets where we keep the same learning rate for the 

 
 
 
 
 
initial  100  epochs  and  linearly  decay  this  learning  rate  to  zero  over  the  next  remaining  epochs.  For  most  of  the 
experiments with Tomato Plant Disease Dataset, we find it suitable to use Î± =10 in equation (4) and Î» = 1 in equation (6), 
which are the terms that largely depend on the nature of the two domains. All models are trained within 3 days on a single 
NVIDIA Titan X GPU with Adam optimizer and a batch size between  1 and 3. The network has approximately 11.3M 
parameters  and  the  generation  of  synthetic  images  takes  about  0.0182  ms  per  image.  We  implemented  our  generative 
model using the Pytorch framework. 

2.5.2. CNN for Plant disease Recognition 

CNNs are extensively used for solving image recognition tasks in computer vision. CNN architectures for tomato 
plant disease recognition used in (Sladojevic et al., 2016; Lu et al., 2017; Brahimi et al., 2017) usually either contain few 
convolution layers because of the small datasets or many convolution layers with large datasets. We use a simple baseline 
Resnet50 model for various experiments to analyze the effectiveness of classical and synthetic augmentation techniques 
using different instances of our tomato plant disease dataset. The model takes in fixed 3-channel input RoI of 256 Ã— 256. 
The architecture remains the same, consisting of 34-layers of 3-layer bottleneck block where each block contains 1 Ã— 1, 3 
Ã— 3, and 1 Ã— 1 convolutions. The layers with 1 Ã— 1 convolution are responsible for reducing and then restoring (increasing) 
the  dimensions,  leaving  the  3Ã—3  layer  as  a  bottleneck  with  smaller  input/output  dimensions  (He  et  al.,  2016).  We  use 
ReLU  as  activation  functions  and  with  the  network  having  approximately  23M  trainable  parameters.  We  fine-tune  our 
recognition networks from ImageNet pre-trained weights (Chai and Wang, 2013). For training, we use a batch size of 32 
with a learning rate of 0.001 for 300 epochs. We use stochastic gradient descent optimization with Nesterov momentum 
updates (Nesterov, 1983). For the implementation of the recognition CNN architecture we used the Keras framework.  

3. Results  

We first introduce our limited tomato plant disease recognition dataset along with other datasets used to evaluate 
the performance of our GAN  network. We compare  our AR-GAN  against recent methods for unpaired image-to-image 
translation  on  both  paired  as  well  as  unpaired  datasets.  We  study  the  importance  of  ARL  term  and  compare  our  final 
method against several variants. Finally, we evaluate the proposed pipeline on our tomato plant disease dataset. The main 
challenge here is to manage with the small amount of available data for training the final classifier network. This setup 
can be imitated by other applications facing the problem of discrete training data e.g. medical images.  

3.1. Datasets 

In  this  work,  we  evaluate  in  terms  of  two  tasks  a)  our  generative  model  and  b)  the  recognition  system  of  the 
proposed  method.  We  experiment  on  several  datasets.  To  evaluate  the  performance  of  our  generative  model  we  use 
Cityscapes dataset (Cordts et al., 2016), our tomato plant disease dataset and several other datasets provided in (Isola et al., 
2017;  Zhu  et  al.,  2017).  The tomato  plant  disease  dataset  is  also  used  to  investigate  the  effectiveness  of  synthetic  data 
augmentation in the recognition system of our proposed method.  

3.1.1. Tomato Plant Disease Dataset 

We  collected  2,789  tomato  plant  images  with  9  identifiable  disease  classes  (Fig.  3(ii))  for  our  dataset  under 
different circumstances depending on the illumination, season, temperature, humidity, and places where they were taken 
using simple camera devices. For that purpose, we have supported our dataset with images having distinct features and 
conditions  like  (i)  Samples  at  different  infection  status.    (ii)  Varied  sizes  of  the  plant.  (iii)  Images  containing  different 
infected regions of the plant (e.g., stem, leaves, fruits, etc.). (iv) Images with diverse resolutions. (v) Objects surrounding 
the plant, etc. These conditions help to approximate the infection process and deduce how a plant is affected by the pest or 
disease (origin or possible developing cause). 

After collecting images, we manually annotate by capturing the RoIs in the samples as the main part of images 
and  categorizing  each  image containing  the  disease  or  pest  into  its  respective  class,  as  shown  in  Fig.  3(i).  The  opinion 
from the experts in the area was a must, depending on the infection status in different diseases that look similar, where the 
knowledge for identifying the disease was required. This serves as our ground truth.  

 
 
 
 
 
 
 
Fig. 3. ROI extraction (i) RoI sample extraction process. (ii) Examples of extracted ROIs with diseases and pests affecting 
tomato plants from our tomato plant disease dataset. (a) Nutritional excess or deficiency, (b) Powdery mildew, (c) Gray 
mold, (d) Plague, (e) Canker, (f) Whitefly, (g) Leaf mold, (h) Low temperature, (i) Miner. 

3.2. Metrics 

We  adopt  FCN  (Isola  et  al.,  2017),  FID  (Heusel  et  al.,  2017)  and  NIMA  (Talebi  and  Milanfar,  2018)  as  the 

evaluation criterion for our generative model. 

3.2.1. FCN score 

We use FCN score (Isola et al., 2017) for evaluating our generative model as an automatic quantitative measure 
that does not require any human experiments. We adopt this metric to evaluate on the Cityscapes labelsâ†’photo task. The 
FCN metric evaluates the comprehensibility of the generated photos in accordance to a semantic segmentation algorithm 
(the fully-convolutional network, FCN [(Long et al., 2015)]). The FCN predicts a label map for the generated photos. The 

 
 
 
 
 
label maps are  then compared to the  input ground truth  labels using standard semantic segmentation metrics like mean 
Intersection-Over-Union (Class IoU), per-class accuracy (pca), and per-pixel accuracy (ppa).  

3.2.2. FID score 

We also evaluate our results with the metric FrÃ©chet Inception Distance (FID) score [(Heusel et al., 2017)] that 
correlates well with human judgment. FID score is known to capture the similarity of generated images to real ones better 
than the Inception Score (Salimans et al., 2016). Smaller FID value denotes better quality. Even though this metric allows 
us  to  avoid  depending  on  human  evaluations  as  they  associate  well  with  our  subjective  judgment  of  image  quality 
(Salimans et al., 2016), it is recommended to use a huge sample size to calculate this metric otherwise the true realization 
of the generator is underestimated. For our tomato plant disease dataset, we use 10,564 random crops of healthy tomato 
plant  leaves  and  translate  them  to  leaves  infected  with  powdery  mildew  to  evaluate  using  FID  score.  Considering  that 
there are not many samples in the cityscapes dataset to evaluate and easily tell the difference, our results are mainly based 
on manual inspection of the visual fidelity of the generated images. 

3.2.3. NIMA score 

Apart from the metrics mentioned above, we also evaluate our model using the state-of-the art evaluation model 
namely neural image assessment (NIMA) (Talebi and Milanfar, 2018). The NIMA estimates aesthetic qualities in aspects 
of photography skills and perceptual relevance. Larger values of NIMA denote better aesthetic quality of an image. We 
calculate the average of the NIMA score for the images in the test set of the datasets used for evaluation of the generative 
model. 

We  adopt  the  class  precision  and  total  accuracy  metrics  as  the  evaluation  criterion  for  our  recognition  task. 
Accuracy has often been adopted as a major metric to evaluate recognition algorithms while the example-based evaluation 
metric like class precision is better at capturing the consistency of predictions on a given diseased class image. 

3.3. Results 

We compare the results of AR-GAN with various approaches including supervised pix2pix  (Isola et al., 2017). 
We present sample results from AR-GAN across given datasets to demonstrate the performance in effective translation of 
features  between  two  domains.  Further,  we  analyze  our  recognition  CNN  to  evaluate  the  performance  of  classical  and 
synthetic augmentation using various instances of our tomato plant disease data. 

3.3.1. Comparing ARL-GAN against other GAN models 

We  first  compare  our  AR-GAN  against  several  recently  proposed  models  for  unpaired  image-to-image 
translation on paired cityscapes dataset with 2975 training images. To perform the image-to-image translation task, all the  

Table 1. FCN-scores for different methods, evaluated on Cityscapes labelsâ†’photos. 

Loss 

CoGAN [9] 
BiGAN [26] 
SimGAN [3] 
CycleGAN [32] 
ARL-GAN 
pix2pix* [18] 
Cityscapes test set* 

Per-pixel 
accuracy 
0.40 
0.19 
0.20 
0.52 
0.68 
0.71 
0.80 

Per-class 
accuracy 
0.10 
0.06 
0.10 
0.17 
0.20 
0.25 
0.26 

Class IoU 

0.06 
0.02 
0.04 
0.11 
0.15 
0.18 
0.21 

models are required to capture the semantic information from the input image and generate the corresponding transformed 
image. For a fair comparison as reported in Table 1, all the methods use the same architecture and details as reported in 
(Zhu et al., 2017). Here, our AR-GAN uses 9 residual blocks and Activation Reconstruction Loss in the forward direction: 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
â„’ğ´ğ‘…ğ¿ğ‘“. We assess their performance on Cityscapes labelsâ†’photo task and observe that our method outperforms them in 
terms of FCN metrics. We observe that the CycleGAN model based on cycle reconstruction and adversarial loss can be 
improved with our proposed addition of ARL between the real and fake image pairs. The translations produced using our 
approach are often of similar quality to the fully supervised pix2pix. 

3.3.2. Comparing AR-GAN against CycleGAN variants 

In Tables 2 and 3, we evaluate different generator architectural variants and compare their performance in terms 
of FID and NIMA using powdery mildew class from the tomato plant disease dataset and cityscapes dataset, respectively. 
We  also  evaluate  our  method  with  the  ARL  in  both  directions:    â„’ğ´ğ‘…ğ¿ğ‘“  between  ğ‘ and  ğºğ´ğµ(ğ‘)  and    â„’ğ´ğ‘…ğ¿ğ‘ 
between ğ‘ and  ğºğµğ´(ğ‘).  In  the  tables,  we  use  res*,  where  *  denotes  the  number  of  residual  blocks  in  the  generator 
architecture. Last model in Table 2 uses AR-GAN with 9  residual blocks in the generator and ARL in both directions: 
â„’ğ´ğ‘…ğ¿ğ‘“ + â„’ğ´ğ‘…ğ¿ğ‘ . Our method outperforms all these variants with a significant difference between the resulting scores. Figs. 
4 and 5 show samples generated from generators of different models in Tables 2 and 3 on powdery mildew class from the 
tomato disease dataset. We are unable to achieve compelling results with CycleGAN. AR-GAN, on the other hand, can 
produce translations with higher fidelity and aesthetic value. It generates compositions that are not only more faithful to 
the input, but also have fewer artifacts. CycleGAN often confuses between the foreground and the background, while AR-
GAN explicitly deals with it by getting sharper edges of the target, without treating the whole image as one object. As can 
also  be  seen  from  Table  1,  CycleGAN  finds  it  hard  to  capture  perceptual  information  from  the  input  image  in  the 
cityscapes dataset, resulting in a poor FCN score, when compared to AR-GAN.  

Table 2. FID and NIMA scores on healthy tomato leavesâ†’leaves infected with powdery mildew at 256 Ã— 256 resolution. 

Model 
CycleGAN (res6) 
CycleGAN (res9) 
CycleGAN ( res12) 
CycleGAN (U-net[31]) 
AR-GAN( res9, â„’ğ´ğ‘…ğ¿ğ‘“) 
AR-GAN(U-net[31], â„’ğ´ğ‘…ğ¿ğ‘“) 
AR-GAN( res9, â„’ğ´ğ‘…ğ¿ğ‘“ + â„’ğ´ğ‘…ğ¿ğ‘) 

FID 
(lower is better) 
75.81 
92.26 
88.86 
86.16 
55.18 

47.15 

54.65 

NIMA 
(higher is better) 
4.842 Â± 1.841 
4.584 Â± 1.860 
4.804 Â± 1.831 
4.612 Â± 1.859 
4.894 Â± 1.806 

4.909 Â± 1.803 

4.911 Â± 1.809 

Table 3. FID and NIMA on cityscapes labelsâ†’photos at 256 Ã— 256 resolution. 

Model 
CycleGAN (res6) 
CycleGAN (res9) 
CycleGAN (res12) 
AR-GAN( res9, â„’ğ´ğ‘…ğ¿ğ‘“) 

FID 
(lower is better) 
66.95 
74.25 
66.68 
56.23 

NIMA 
(higher is better) 
4.639 Â± 1.697 
4.588 Â± 1.672 
4.623 Â± 1.681 
4.643 Â± 1.652 

3.3.3. Analysis of loss function 

In image translation methods, the fidelity and resemblance between images generated by GANs and real images 
is  a  trade-off.  In  our  model,  it  is  determined  by  the  weight Î» of  the  ARL  relative  to  the  cycle-adversarial  term.  In  our 
experiments, we start by setting Î» = 0 and gradually increase its value in regular increments until a threshold is reached 
where the translated image looks closer to the input but fails to pertain the visual traits from the target domain. This effect 
is demonstrated in Fig. 6, on horseâ†’zebra translation task on unpaired Horse2Zebra dataset (Zhu et al., 2017). Tuning Î»  

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Input 

CycleGAN(res6) 

CycleGAN(res9) 

CycleGAN(res12) 

CycleGAN(U-net[31]) 

Fig.  4.  Different  variants  of  CycleGAN  for  mapping  healthy  leavesâ†’leaves  infected  with  powdery  mildew  from  our 
unpaired tomato plant disease data. From left to right: input, CycleGAN with resnet6 generator, CycleGAN with resnet9 
generator, CycleGAN with resnet12 generator and CycleGAN with Unet generator. 

Input 

CycleGAN(res9) 

AR-GAN(res9, â„’ğ´ğ‘…ğ¿ğ‘“) 

AR-GAN(U-net, â„’ğ´ğ‘…ğ¿ğ‘“)  AR-GAN(res9, â„’ğ´ğ‘…ğ¿ğ‘“+ â„’ğ´ğ‘…ğ¿ğ‘) 

Fig.  5.  Different  variants  of  our  method  for  mapping  healthy  leavesâ†’leaves  infected  with  powdery  mildew  from  our 
unpaired tomato plant disease data. From left to right: input,  CycleGAN with resnet9 generator, AR-GAN  with resnet9 
generator, AR-GAN with Unet generator, AR-GAN with Unet generator and ARL between real and fake images in both 
directions: â„’ğ´ğ‘…ğ¿ğ‘“ + â„’ğ´ğ‘…ğ¿ğ‘.  

 
 
 
 
 
 
 
 
Input 

AR-GAN (Î»=0.5) 

AR-GAN (Î»=0.75) 

AR-GAN (Î»=1) 

Fig. 6.  Mapping horsesâ†’zebras using 1171 images from zebra class and 939 images from wild horse class of ImageNet. 
From left to right: input, AR-GAN (ğ›Œ=0.5), AR-GAN (ğ›Œ=0.75) and AR-GAN (ğ›Œ=1). While ğ›Œ=0.75 gives the best results 
for  the  Horse2Zebra  dataset,  in  our  experiments  with  Tomato  Plant  Disease  Dataset  we  find  it  suitable  to  use ğ›Œ = ğŸ, 
which depicts ğ›Œ as a hyperparameter depending on the nature of data in the two domains while training the model. 

for each specific task and dataset gives results that are both faithful to the original image and similar to the target domain, 
and thus penalizes the perceptual quality between the images from the two domains. 

3.3.4. Data augmentation 

After  extracting  the  RoIs,  the  number  of  plant  disease  images  in  our  dataset  is  5,318  with  class  distribution 
portrayed as ğ‘‹ in Fig. 7. As evident from Fig. 7, the available limited training data ğ‘‹ suffers from heavy class imbalance. 
Among 9 classes, two classes of  â€˜Leaf Moldâ€™ and â€˜Whiteflyâ€™ occupy over 50% of the dataset  while â€˜Low Temperatureâ€™ 
and â€˜Powdery Mildewâ€™ occupy less than 5%.  To address this problem of data insufficiency  and heavy class imbalance, 
we  augment  dataset ğ‘‹ using  two  kinds  of  data  augmentation  methods:  classical  data  augmentation  and  synthetic  data 
augmentation  using  AR-GAN.  Through  these  data  augmentation  processes,  the  numbers  of  data  instances  can  be 
increased  by  more  than  double,  for  classes  with  minimum  samples.  It  can  also  be  seen  from  Fig.  7  that  using  data 
augmentation,  we  have  a  relatively  better  balance  between  classes.  In  Fig.  7, ğ‘‹ represents  the  original  dataset  obtained 
after  RoI  extraction,  ğ‘‹ + ğ‘‹ğ¶ represents  the  dataset  augmented  by  classical  augmentation  of  X,  and   ğ‘‹ + ğ‘‹ğ‘†   represents 
dataset augmented by synthetic augmentation of  X. Note that the number of samples per class in ğ‘‹ + ğ‘‹ğ¶ and  ğ‘‹ + ğ‘‹ğ‘† is 
kept  proportional  for  fair  evaluation.  We  do  not  use  any  augmented  data  for  classes  â€˜Leaf  Moldâ€™  and  â€˜Whiteflyâ€™  that 
means ğ‘‹, ğ‘‹ + ğ‘‹ğ¶ and  ğ‘‹ + ğ‘‹ğ‘† for these two classes are the same.  

 
 
 
 
 
 
 
 
 
 
Fig. 7.   Class distributions of Tomato Plant Disease Dataset instances under different data augmentation methods, (i) ğ‘‹: 
Original  dataset  after  RoI  extraction,  (ii) ğ‘‹ + ğ‘‹ğ¶ :  Dataset  augmented    classically  from ğ‘‹,  and  (iii)     ğ‘‹ + ğ‘‹ğ‘†:  Dataset 
augmented synthetically from  ğ‘‹. 

Fig.  8.      Synthetic  images  from  AR-GAN  model.  The  real  images  shown  at  the  leftmost  column  are  inputs  that  the 
synthetic  images  are  based  on.  The  2nd,  3rd,  4th,  5th,6th  and  7th    column  represent  the  translated  images  to  the  domains: 
canker, miner, whitefly, powdery mildew, plague and low temperature respectively. 

Fig.  8  shows  samples  of  images  generated  by  our  AR-GAN  for  each  class.  For  classical  augmentation,  the 
extracted RoIs are first induced with random distortions like tilting, skewing and shearing to produce ğ‘‹ğ‘‘ğ‘–ğ‘ ğ‘¡ samples with a 
probability of 1 each. Each image is also rotated ğ‘‹ğ‘Ÿğ‘œğ‘¡ times at random angles with Î¸ = [0â—¦, ..., 180â—¦] with probability of 1 
and  flipped  up-down  and  left-right ğ‘‹ğ‘“ğ‘™ğ‘–ğ‘ times  with  probability  of  0.5.  Elastic  augmentation, ğ‘‹ğ‘’ğ‘‘ is  then  applied  with  a 
grid  size  of  16  and  probability  of  1  to  all  the  resultant  images.  An  example  of  an  extracted  RoI  and  its  corresponding 
augmentation are shown in Figure 3.3. This augmentation process results in a total number of augmentations ğ‘‹ğ‘, such that: 

ğ‘‹ğ‘ =   (ğ‘‹ğ‘Ÿğ‘œğ‘¡ +   ğ‘‹ğ‘‘ğ‘–ğ‘ ğ‘¡   +   ğ‘‹ğ‘“ğ‘™ğ‘–ğ‘   +  ğ‘‹) Ã—   ğ‘‹ğ‘’ğ‘‘ 

(7) 

All the resulting samples were resized to 256 Ã— 256 using bicubic interpolation. 

3.3.5. Analysis of classic and synthetic augmentation techniques 

To analyze the results of our baseline recognition CNN  - ResNet50 and show the performance of classical and 

synthetic data augmentation, we use three instances of our tomato plant disease dataset (ğ‘‹, ğ‘‹ + ğ‘‹ğ¶, ğ‘‹ + ğ‘‹ğ‘†) as training  

 
 
 
 
 
 
 
 
 
Precision (X)

Precision (X+Xc)

Precision (X+Xs)

5
2
9
0

.

6
9
0

.

4
9
0

.

5
0
8
0

.

5
9
7
0

.

3
8
0

.

5
0
8
0

.

5
4
7
0

.

5
5
7
0

.

7
9
0

.

8
9
0

.

6
9
0

.

5
3
8
0

.

5
4
8
0

.

7
8
0

.

4
6
0

.

4
6
0

.

2
6
0

.

5
2
9
0

.

5
8
8
0

.

5
2
8
0

.

5
8
9
0

.

6
9
0

.

7
9
0

.

1
6
8
0

.

9
0
8
0

.

7
1
8
0

.

5
5
7
0

.

5
1
6
0

.

5
5
0

.

Precision (X)

Precision (X+Xc)

Canker

0.805

0.795

Precision (X+Xs)

0.83

Gray
mold

0.745

0.755

0.805

Leaf mold

0.925

0.94

0.96

Low
Temp.

0.55

0.615

0.755

Miner

Nutrition
al excess

0.96

0.97

0.98

0.62

0.64

0.64

Plague

0.835

0.845

0.87

Powdery
mildew

Whitefly

Total
Accuracy

0.885

0.825

0.925

0.96

0.97

0.985

0.809

0.817

0.861

Fig.  9.     Class  Precision  and total  accuracy of  the  baseline  recognition  CNN  at  dataset  instances: ğ‘‹, ğ‘‹ + ğ‘‹ğ¶, ğ‘‹ + ğ‘‹ğ‘† of 
tomato plant disease dataset. We observe a significant improvement in class and final average precision from ğ‘‹ â†’ ğ‘‹ +
ğ‘‹ğ¶ â†’ ğ‘‹ + ğ‘‹ğ‘†. The final column represents the total accuracy for each data instance on the validation data. 

data.  To  evaluate  the  recognition  system,  we  use  200  test  samples  from  each  class,  which  remain  unseen  to  this 
recognition system while training. From Fig. 9, we observe that using classical augmentation to increase the size of the 
training set, shows an increase from 80.9% to 81.7% (+0.8) in terms of accuracy. Compared to classical augmentation, our 
method of synthetically generating tomato plant disease images to aid the training set shows an increase from 80.9% to 
86.1% (+5.2%). 

For further analysis, we investigate the changes in predicted results of 200 positives from each class used to test 
our recognition system using the three different training dataset instances: ğ‘‹, ğ‘‹ + ğ‘‹ğ¶, and ğ‘‹ + ğ‘‹ğ‘†. Fig. 10 shows relative 
changes  of  predicted  true  positives  of  two  augmented  datasets ğ‘‹ + ğ‘‹ğ¶,  and ğ‘‹ + ğ‘‹ğ‘† based  on ğ‘‹.  For  all  classes,  using 
synthetically  augmented  dataset ğ‘‹ + ğ‘‹ğ‘†  as  a  training  dataset,  increases  the  portion  of  true  positives  compared  to  the 
original  dataset ğ‘‹.  However,  in  the  case  of  training  with  dataset  instance ğ‘‹ + ğ‘‹ğ¶ ,  it  shows  worse  results  for  classes 
â€˜Cankerâ€™ and â€˜Powdery mildewâ€™. The average rate of increase in the portion of true positives remains much higher when 
using synthetically augmented dataset instance as compared to the dataset instance using classic augmentation techniques. 
We  also  compare  the  relative  change  rate  of  false  positives  for  each  tomato  plant  disease  when  using  data 
instances ğ‘‹ + ğ‘‹ğ¶ and ğ‘‹ + ğ‘‹ğ‘† against the original dataset in Fig. 11 and illustrate the detailed changes in the false positive 
rates  leading  to  the  improvement  in  the  final  accuracy.    Three  horizontal  bars  in  each  row  of  the  corresponding  class 
represent the ratios of the amount of false positives between various instances of our dataset. The first two bars represent 
the ratio of amount of false positives using dataset instances ğ‘‹ + ğ‘‹ğ‘ and ğ‘‹ + ğ‘‹ğ‘  to the amount of false positives from the 
original dataset ğ‘‹, respectively. The third bar represents the ratio of false positives using dataset instances ğ‘‹ + ğ‘‹ğ‘  and ğ‘‹ +
ğ‘‹ğ‘. A negative rate displayed on the left side of the graph represents the decrease in false positives while the positive rate 
displayed  on  the  right  side  represents  the  increase  in  false  positives  of  the  corresponding  class.  Comparing  dataset 
instances ğ‘‹ + ğ‘‹ğ‘  and ğ‘‹ + ğ‘‹ğ‘, as  represented  in  the  third  horizontal  bar  for  each  row,  we  can  identify  longer  stacked 
negative rate bars than positive ones for all classes resulting in an improved overall accuracy. As can be seen from Fig. 10, 
there  is  a  significant  increase  in  the  relative  change  of  rate  in  true  positives,  especially  in  classes  gray  mold,  low 
temperature and powdery mildew. This effect can also be seen in Fig. 11 showing that these classes have relatively more 
decrease in the relative rate of change in false positives. We can also observe that the classes with a relative increment in 
class distribution (as shown in Fig. 7) due to data augmentation, e.g. â€˜Gray moldâ€™, â€˜Low temperatureâ€™, â€˜Nutritional excessâ€™, 
and â€˜Powdery mildewâ€™ show relatively exorbitant changes in false positives. These classes show more negative changes 
(reduction  of  false  positives)  than  the  other  classes  when  using  synthetic  augmentation.      This  comparative  illustration 
shows that the diversity and variation that can be gained from classical modifications of the images for augmentation is 
relatively smaller when compared to synthetic augmentation. These results also indicate that using generated tomato plant  

 
 
 
25.0%

20.0%

15.0%

10.0%

5.0%

0.0%

-5.0%

-10.0%

20.5%

14.0%

6.0%

5.0%

6.5%

3.5%

2.5%

1.0%

3.5%

1.5%

2.0%

-1.0%

10.0%

2.0%

2.0%

2.0%

1.0%

1.0%

0.0%

4.0%

3.5%

2.5%

1.0%

2.5%

1.0%

1.5%

X vs. X+Xc

X vs.X+Xs

X+Xc vs. X+Xs

-6.0%

Fig. 10. Relative change rate of true positives for two augmented training datasets ğ‘‹ + ğ‘‹ğ¶, and ğ‘‹ + ğ‘‹ğ‘† against the original 
dataset ğ‘‹. 

Fig. 11. Relative change rate of false positives for each tomato plant disease when using two training dataset 
instances ğ‘‹ + ğ‘‹ğ¶, and ğ‘‹ + ğ‘‹ğ‘† against the original dataset ğ‘‹. 

 
 
 
 
 
disease images together with real images yield improvements in the performance for the recognition task over the limited 
tomato plant disease data. 

4. Discussion 

This work focused on using GANs in an image translation setting to synthetically augment plant disease dataset 
and further improve the performance on the plant disease recognition task using deep CNN. Our relatively small dataset 
reflects the size of datasets available to most researchers in the field of plant disease detection and recognition. We first 
extend the state-of-the-art for GAN-based image-to-image synthesis to improving the perceptual quality of the generated 
images.  In  addition  to  a  cycle  consistent  and  adversarial  term,  our  GAN  based  image-to-image  translation  framework 
(AR-GAN) optimizes on Activation Reconstruction loss function that measures feature activation against the real image. 
We present visually more convincing synthetic images in comparison to one of the most prominent existing models and 
evaluate  the  performance of our AR-GAN  framework in terms of different datasets and metrics. Further, we  tested our 
hypothesis that adding synthetic samples would improve classification results. We analyze a baseline convolutional neural 
network classifier for improved recognition using the synthetic samples for augmentation of our training set and compare 
it with the classical data augmentation scheme. We observe a significant improvement in classification accuracy (+5.2%) 
using  the  synthetic  samples  generated  by  AR-GAN  framework  as  compared  to  (+0.8%)  increase  using  classic 
augmentation strategy.  

Our results show that the images generated using AR-GAN have meaningful features and visualizations that can 
be incorporated into other computer aided algorithms. In the future, we plan to extend our work to additional computer 
vision domains that can yield better performance using synthetic plant disease generation. We believe that problems like 
detection and recognition of plant diseases can benefit from using synthetic augmentation, and that the presented approach 
can lead to a stronger and more robust plant disease detection systems. 

Acknowledgements 

This  research  was  supported  by  Basic  Science  Research  Program  through  the  National  Research  Foundation  of 
Korea (NRF)  funded by the Ministry of Education (No. 2019R1A6A1A09031717). This work was carried out with the 
support  of  "Cooperative  Research  Program  for  Agriculture  Science  and  Technology  Development  (Project  No. 
PJ01389105)" Rural Development Administration, Republic of Korea. 

References 

Akhtar, A., Khanum, A., Khan, S.A. and Shaukat, A., 2013, December. Automated Plant Disease Analysis (APDA): 
Performance comparison of machine learning techniques. In 2013 11th International Conference on Frontiers of 
Information Technology (pp. 60-65). IEEE. 

Al-Hiary, H., Bani-Ahmad, S., Reyalat, M., Braik, M. and ALRahamneh, Z., 2011. Fast and accurate detection and 
classification of plant diseases. International Journal of Computer Applications, 17(1), pp.31-38. 

Arjovsky, M., Chintala, S. and Bottou, L., 2017, July. Wasserstein generative adversarial networks. In International 
conference on machine learning (pp. 214-223). 

Atabay, H.A., 2017. Deep residual learning for tomato plant leaf disease identification. Journal of Theoretical & Applied 
Information Technology, 95(24). 

Aytar, Y., Castrejon, L., Vondrick, C., Pirsiavash, H. and Torralba, A., 2017. Cross-modal scene networks. IEEE 
transactions on pattern analysis and machine intelligence, 40(10), pp.2303-2314. 

Bergougnoux, V., 2014. The history of tomato: from domestication to biopharming. Biotechnology advances, 32(1), 
pp.170-189. 

 
 
 
 
 
 
 
 
 
 
 
 
Bousmalis, K., Silberman, N., Dohan, D., Erhan, D. and Krishnan, D., 2017. Unsupervised pixel-level domain adaptation 
with generative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern 
recognition (pp. 3722-3731). 

Brahimi, M., Boukhalfa, K. and Moussaoui, A., 2017. Deep learning for tomato diseases: classification and symptoms 
visualization. Applied Artificial Intelligence, 31(4), pp.299-315. 

Cha, M., Gwon, Y. and Kung, H.T., 2017, September. Adversarial nets with perceptual losses for text-to-image synthesis. 
In 2017 IEEE 27th International Workshop on Machine Learning for Signal Processing (MLSP) (pp. 1-6). IEEE. 

Chai, Y. and Wang, X., 2013. Recognition of greenhouse tomato disease based on image processing technology. Pattern 
Recognition and Simulation, 9, pp.83-89. 

Che, T., Li, Y., Jacob, A.P., Bengio, Y. and Li, W., 2016. Mode regularized generative adversarial networks. arXiv 
preprint arXiv:1612.02136. 

Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S. and Schiele, B., 2016. 
The cityscapes dataset for semantic urban scene understanding. In Proceedings of the IEEE conference on computer 
vision and pattern recognition (pp. 3213-3223). 

Creswell, A., White, T., Dumoulin, V., Arulkumaran, K., Sengupta, B. and Bharath, A.A., 2018. Generative adversarial 
networks: An overview. IEEE Signal Processing Magazine, 35(1), pp.53-65. 

Denton, E.L., Chintala, S. and Fergus, R., 2015. Deep generative image models using aï¿¼ laplacian pyramid of 
adversarial networks. In Advances in neural information processing systems (pp. 1486-1494). 

Donahue, J., KrÃ¤henbÃ¼hl, P. and Darrell, T., 2016. Adversarial feature learning. arXiv preprint arXiv:1605.09782. 

Dumoulin, V. and Visin, F., 2016. A guide to convolution arithmetic for deep learning. arXiv preprint arXiv:1603.07285. 

Ghazi, M.M., Yanikoglu, B. and Aptoula, E., 2017. Plant identification using deep neural networks via optimization of 
transfer learning parameters. Neurocomputing, 235, pp.228-235. 

GoÃ«au, H., Joly, A., Bonnet, P., Selmi, S., Molino, J.F., BarthÃ©lÃ©my, D. and Boujemaa, N., 2014.  

Lifeclef plant identification task 2014. In CLEF2014 Working Notes. Working Notes for CLEF 2014 Conference, 
Sheffield, UK, September 15-18, 2014 (pp. 598-615). CEUR-WS. 

Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A. and Bengio, Y., 2014. 
Generative adversarial nets. In Advances in neural information processing systems (pp. 2672-2680). 

Grinblat, G.L., Uzal, L.C., Larese, M.G. and Granitto, P.M., 2016. Deep learning for plant identification using vein 
morphological patterns. Computers and Electronics in Agriculture, 127, pp.418-424. 

Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V. and Courville, A.C., 2017. Improved training of wasserstein gans. 
In Advances in neural information processing systems (pp. 5767-5777). 

He, D., Xia, Y., Qin, T., Wang, L., Yu, N., Liu, T.Y. and Ma, W.Y., 2016. Dual learning for machine translation. 
In Advances in Neural Information Processing Systems (pp. 820-828). 

He, K., Zhang, X., Ren, S. and Sun, J., 2016. Deep residual learning for image recognition. In Proceedings of the IEEE 
conference on computer vision and pattern recognition (pp. 770-778). 

Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B. and Hochreiter, S., 2017. Gans trained by a two time-scale update 
rule converge to a local nash equilibrium. In Advances in Neural Information Processing Systems (pp. 6626-6637). 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Hughes, D. and SalathÃ©, M., 2015. An open access repository of images on plant health to enable the development of 
mobile disease diagnostics. arXiv preprint arXiv:1511.08060. 

Isola, P., Zhu, J.Y., Zhou, T. and Efros, A.A., 2017. Image-to-image translation with conditional adversarial networks. 
In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1125-1134). 

Johnson, J., Alahi, A. and Fei-Fei, L., 2016, October. Perceptual losses for real-time style transfer and super-resolution. 
In European conference on computer vision (pp. 694-711). Springer, Cham. 

Kim, T., Cha, M., Kim, H., Lee, J.K. and Kim, J., 2017, August. Learning to discover cross-domain relations with 
generative adversarial networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 
70 (pp. 1857-1865). JMLR. org. 

Kingma, D.P. and Welling, M., 2013. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114. 

Krizhevsky, A., Sutskever, I. and Hinton, G.E., 2012. Imagenet classification with deep convolutional neural networks. 
In Advances in neural information processing systems (pp. 1097-1105). 

Liu, M.Y. and Tuzel, O., 2016. Coupled generative adversarial networks. In Advances in neural information processing 
systems (pp. 469-477). 

Long, J., Shelhamer, E. and Darrell, T., 2015. Fully convolutional networks for semantic segmentation. In Proceedings of 
the IEEE conference on computer vision and pattern recognition (pp. 3431-3440). 

Lu, Y., Yi, S., Zeng, N., Liu, Y. and Zhang, Y., 2017. Identification of rice diseases using deep convolutional neural 
networks. Neurocomputing, 267, pp.378-384. 

Mirza, M. and Osindero, S., 2014. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784. 

Mohanty, S.P., Hughes, D.P. and SalathÃ©, M., 2016. Using deep learning for image-based plant disease 
detection. Frontiers in plant science, 7, p.1419. 

Mokhtar, U., Ali, M.A., Hassanien, A.E. and Hefny, H., 2015. Identifying two of tomatoes leaf viruses using support 
vector machine. In Information Systems Design and Intelligent Applications (pp. 771-782). Springer, New Delhi. 

Nazki H, Park DS, Park J. â€œLearning from Imbalanced Data Sets with Synthetically Generated Samples.â€ In International 
Symposium on Information Technology Convergence, 2018. 

Nazki, H., Lee, J., Yoon, S. and Park, D.S. â€œImage-to-Image Translation with GAN for Synthetic Data Augmentation in 
Plant Disease Datasets.â€ Smart Media Journal Vol.8 No.22019. 46~57, 2019. 

Nazki, H., Lee, J., Yoon, S. and Park, D.S., 2018. Synthetic Data Augmentation for Plant Disease Image Generation using 
GAN. In Proceedings of the Korea Contents Association Conference (pp. 459-460). The Korea Contents Association. 

Nesterov, Y., 1983. A method for unconstrained convex minimization problem with the rate of convergence O (1/k^ 2). 
In Doklady AN USSR (Vol. 269, pp. 543-547). 

Nguyen, A., Clune, J., Bengio, Y., Dosovitskiy, A. and Yosinski, J., 2017. Plug & play generative networks: Conditional 
iterative generation of images in latent space. In Proceedings of the IEEE Conference on Computer Vision and Pattern 
Recognition (pp. 4467-4477). 

Nutter, F.W., Esker, P.D. and Netto, R.A.C., 2006. Disease assessment concepts and the advancements made in 
improving the accuracy and precision of plant disease data. European Journal of Plant Pathology, 115(1), pp.95-103. 

Pan, S.J. and Yang, Q., 2009. A survey on transfer learning. IEEE Transactions on knowledge and data 
engineering, 22(10), pp.1345-1359. 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Perez, L. and Wang, J., 2017. The effectiveness of data augmentation in image classification using deep learning. arXiv 
preprint arXiv:1712.04621. 

Prince, G., Clarkson, J.P. and Rajpoot, N.M., 2015. Automatic detection of diseased tomato plants using thermal and 
stereo visible light images. PloS one, 10(4), p.e0123262.z 

Radford, A., Metz, L. and Chintala, S., 2015. Unsupervised representation learning with deep convolutional generative 
adversarial networks. arXiv preprint arXiv:1511.06434. 

Ronneberger, O., Fischer, P. and Brox, T., 2015, October. U-net: Convolutional networks for biomedical image 
segmentation. In International Conference on Medical image computing and computer-assisted intervention (pp. 234-241). 
Springer, Cham. 

Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A. and Chen, X., 2016. Improved techniques for 
training gans. In Advances in neural information processing systems (pp. 2234-2242). 

Shrivastava, A., Pfister, T., Tuzel, O., Susskind, J., Wang, W. and Webb, R., 2017. Learning from simulated and 
unsupervised images through adversarial training. In Proceedings of the IEEE conference on computer vision and pattern 
recognition (pp. 2107-2116). 

Simonyan, K. and Zisserman, A., 2014. Very deep convolutional networks for large-scale image recognition. arXiv 
preprint arXiv:1409.1556. 

Singh, A., Ganapathysubramanian, B., Singh, A.K. and Sarkar, S., 2016. Machine learning for high-throughput stress 
phenotyping in plants. Trends in plant science, 21(2), pp.110-124. 

Sladojevic, S., Arsenovic, M., Anderla, A., Culibrk, D. and Stefanovic, D., 2016. Deep neural networks based recognition 
of plant diseases by leaf image classification. Computational intelligence and neuroscience, 2016. 

Sohn, K., Lee, H. and Yan, X., 2015. Learning structured output representation using deep conditional generative models. 
In Advances in neural information processing systems (pp. 3483-3491). 

Strange, R.N. and Scott, P.R., 2005. Plant disease: a threat to global food security. Annu. Rev. Phytopathol., 43, pp.83-
116. 

Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V. and Rabinovich, A., 2015. 
Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 
1-9). 

Taigman, Y., Polyak, A. and Wolf, L., 2016. Unsupervised cross-domain image generation. arXiv preprint 
arXiv:1611.02200. 

Talebi, H. and Milanfar, P., 2018. Nima: Neural image assessment. IEEE Transactions on Image Processing, 27(8), 
pp.3998-4011. 

Wong, S.C., Gatt, A., Stamatescu, V. and McDonnell, M.D., 2016, November. Understanding data augmentation for 
classification: when to warp? In 2016 international conference on digital image computing: techniques and applications 
(DICTA) (pp. 1-6). IEEE. 

Yi, Z., Zhang, H., Tan, P. and Gong, M., 2017. Dualgan: Unsupervised dual learning for image-to-image translation. 
In Proceedings of the IEEE international conference on computer vision (pp. 2849-2857). 

Zhu, J.Y., Park, T., Isola, P. and Efros, A.A., 2017. Unpaired image-to-image translation using cycle-consistent 
adversarial networks. In Proceedings of the IEEE international conference on computer vision (pp. 2223-2232). 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
